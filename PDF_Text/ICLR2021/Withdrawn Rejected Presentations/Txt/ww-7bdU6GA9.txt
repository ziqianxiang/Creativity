Under review as a conference paper at ICLR 2021
Near-Optimal Linear Regression under Dis-
tribution Shift
Anonymous authors
Paper under double-blind review
Ab stract
Transfer learning is an essential technique when sufficient data comes from the
source domain, while no or scarce data is from the target domain. We develop
estimators that achieve minimax linear risk for linear regression problems under
the distribution shift. Our algorithms cover different kinds of settings with
covariate shift or model shift. We also consider when data are generating from
either linear or general nonlinear models. We show that affine minimax rules are
within an absolute constant of the minimax risk even among nonlinear rules for
various source/target distributions.
1	Introduction
The success of machine learning crucially relies on the availability of labeled data. The data
labeling process usually requires much human labor and can be very expensive and time-consuming,
especially for large datasets like ImageNet (Deng et al., 2009). On the other hand, models trained on
one dataset, despite performing well on test data from the same distribution they are trained on, are
often sensitive to distribution shifts, i.e., they do not adapt well to related but different distributions.
Even small distributional shift can result in substantial performance degradation (Recht et al., 2018;
Lu et al., 2020).
Transfer learning has been an essential paradigm to tackle the challenges associated with insufficient
labeled data (Pan & Yang, 2009; Weiss et al., 2016; Long et al., 2017). The main idea is to make
use of a source domain with a lot of labeled data (e.g. ImageNet), and to try to learn a model that
performs well on our target domain (e.g. medical images) where few or no labels are available.
Despite the lack of labeled data, we may still use unlabeled data from the target domain, which
are usually much easier to obtain and can provide helpful information about the target domain.
Although this approach has been integral to many applications, many fundamental questions are
left open even in very basic settings.
In this work, we focus on the setting of linear regression under distribution shift and ask the
fundamental question of how to optimally learn a linear model for a target domain, using labeled
data from a source domain and unlabeled data (and possibly some labeled data) from the target
domain. For various settings, including covariate shift (i.e., when p(x) changes) and model shift
(i.e., when p(y|x) changes), we develop estimators that achieve near minimax risk (up to universal
constant factors) among all linear estimation rules. Here linear estimators refer to all estimators
that depend linearly on the label vector; these include almost all popular estimators known in
linear regression, such as ridge regression and its variants. When the input covariances in source
and target domains commute, we prove that our estimators achieve near minimax risk among all
possible estimators.
1
Under review as a conference paper at ICLR 2021
A key insight from our results is that, when covariate shift is present, we need to apply data-
dependent regularization that adapts to changes in the input distribution. For linear regression, this
can be given by the input covariances of source and target tasks, which can be estimated using
unlabeled data. Our experiments verify that our estimator has significant improvement over ridge
regression and similar heuristics.
1.1	Related work
Different types of distribution shift are introduced in (Storkey, 2009; Quionero-Candela et al.,
2009). Specifically, covariate shift occurs when the marginal distribution on P (X) changes from
source to target domain (Shimodaira, 2000; Huang et al., 2007). Wang et al. (2014); Wang &
Schneider (2015) tackle model shift (P(Y |X)) provided the change is smooth as a function of X.
Sun et al. (2011) design a two-stage reweighting method based on both covariate shift and model
shift. Other methods like the change of representation, adaptation through prior, and instance
pruning are proposed in (Jiang & Zhai, 2007). In this work, we focus on the above two kinds of
distribution shift. For modeling target shift (P(Y )) and conditional shift (P(X|Y )), Zhang et al.
(2013) exploits the benefit of multi-layer adaptation by some location-scale transformation on X .
Transfer learning/domain adaptation are sub-fields within machine learning to cope with distri-
bution shift. A variety of prior work roughly falls into the following categories. 1) Importance-
reweighting is mostly used in the covariate shift. (Shimodaira, 2000; Huang et al., 2007; Cortes
et al., 2010); 2) One fruitful line of work focuses on exploring robust/causal features or domain-
invariant representations through invariant risk minimization (Arjovsky et al., 2019), distributional
robust minimization (Sagawa et al., 2019), human annotation (Srivastava et al., 2020), adversarial
training (Long et al., 2017; Ganin et al., 2016), or by minimizing domain discrepancy measured
by some distance metric (Pan et al., 2010; Long et al., 2013; Baktashmotlagh et al., 2013; Gong
et al., 2013; Zhang et al., 2013; Wang & Schneider, 2014) ; 3) Several approaches seek gradual
domain adaptation (Gopalan et al., 2011; Gong et al., 2012; Glorot et al., 2011; Kumar et al., 2020)
through self-training or a gradual change in the training distribution.
Near minimax estimations are introduced in Donoho (1994) for linear regression problems with
Gaussian noise. For a more general setting, Juditsky et al. (2009) estimate the linear functional
using convex programming. Blaker (2000) compares ridge regression with a minimax linear
estimator under weighted squared error. Kalan et al. (2020) considers a setting similar to this work
of minimax estimator under distribution shift, but focuses on computing the lower bound for linear
and one-hidden-layer neural network under distribution shift. A few more interesting results are
derived on the generalization lower bound for distribution shift under various settings (David et al.,
2010; Hanneke & Kpotufe, 2019; Ben-David et al., 2010; Zhao et al., 2019).
2	Preliminary
We formalize the setting considered in this paper for transfer learning under the distribution shift.
Notation and setup. Let pS(x) and pT (x) be the marginal distribution for x in source and target
domain. The associated covariance matrices are ΣS, and ΣT . We assume to have sufficient
unlabeled data to estimate ΣT accurately. We observe nS , nT labeled samples from source and
target domain. Data is scarce in target domain: nS nT and nT can be 0. Specifically, XS =
[x>∣x> | …∣x>s]> ∈ Rn×d, With x” i ∈ [ns] drawn fromPS, noise Z = [z1,z2,…ZnS]>,zi 〜
N(0,σ2). ys = [y1,y2,…，yns]> ∈ RnS, with each y = f*(xi) + Zi (XT ∈ RnT×d and
2
Under review as a conference paper at ICLR 2021
yT ∈ RnT are similarly defined). Denote by ΣS = XS>XS/nS the empirical covariance matrix
(Throughout the paper we assume data is centered: EpS [x] = EpT [x] = 0). The positive part of
a number is denoted by (x)+. We consider both linear (f *(x) = x>β*) and general nonlinear
ground truth models. When the optimal linear model changes from source to domain we add a
subscript for distinction, i.e., βS and βT. We use bold (x) symbols for vectors, lower case letter
(x) for scalars and capital letter (A) for matrices.
Minimax (linear) risk. In this work, we focus on designing linear estimators β = AyS 1 for
parameter β* ∈ B. Our estimator is evaluated by the excess risk on target domain, with the worst
case β* in some set B: LB(β) =maxβ*∈B EyS k∑TZ2(β(ys) - β*)k2. Minimax linear risk and
minimax risk among all estimators are respectively defined as:
RL(B) ≡ ʌ mm	LB(β); RN(B) ≡ minLB(β).
β linear in ys	β
The subscript “N" or “L" is a mnemonic for “non-linear" or “linear" estimators. RN is the
optimal risk with no restriction placed on the class of estimators. RL only considers the linear
function class for β. Minimax linear estimator and minimax estimator are the estimators that
respectively attain RL and RN within universal multiplicative constants. Normally we only
consider B = {β∣∣∣β∣∣2 ≤ r}. When there is no ambiguity, We simplify β(ys) by β.
Our meta-algorithm. Our paper considers different settings with distribution shift. Our methods
are unified under the following meta-algorithm:
Step 1: Find an unbiased sufficient statistic 6ss2 for the unknown parameter.
step 2: Find βMM, a linear operator applied to βss that minimizes LB(βMM).
For each setting, We will show that ∕3mm achieves linear minimax risk RL (asymptotically or in
fixed design). Furthermore, under some conditions, the minimax risk RN is uniformly lower
bounded by a universal constant times LB(βMM).
Outline. In the sections below, we tackle the problem in different settings. In section 3 we design
algorithms with only covariate shift: 1) nτ = 0 and f*(x) is linear (Section 3.1); 2) nτ = 0
and f *(x) is a general nonlinear function (Section 3.2); 3) nτ > 0 and f * is linear (Section 3.3).
Finally, we cope with the model shift for linear models (βS = βT) in Section 4.
3	Minimax estimator with covariate shift
In this section, we consider the setting with only covariate shift. That is, only ΣS (marginal
distribution pS (x)) changes to ΣT (pT (x)), but f* = E[y|x] (conditional distribution p(y|x)) is
shared. We first consider the case when f* is a linear map: x → x>β* and then consider the
problem with approximation power.
1A ∈ Rd×n may depend in an arbitrary way on XS , nS, or ΣT . The estimator is linear in the observation yS.
2With samples ys, a statistic t = T(ys) is sufficient for the underlying parameter β* if the conditional
probability distribution of the data ys, given the statistic t = T(ys), does not depend on the parameter β*.
3
Under review as a conference paper at ICLR 2021
3.1	Covariate shift with linear models
We observe ns samples from source domain: ys = XSβ* + z, Z 〜N(0, σ2I) and no labeled
samples from the target domain. Our goal is to find the minimax linear estimator βMM(yS) = AyS
with some linear mapping A that attains RL (B).
Following our meta-algorithm, let βss = n^Σ-1X>ys3 be an unbiased sufficient statistic for β*:
^ — 1 ^ — 1 ¥>” — 1 ^ — 1 v> v ∕3* _i_ 1 ^ — 1 ¥> ,
∣βSS - ςs XS yS -  ςs XS XSβ + ςs XS z.
nS	nS	nS
1σ
=β* + —∑—1 X>z 〜N β*,-∑—1 .	(1)
nS S S	nS S
The fact that βSS(yS) is a sufficient statistic is proven in Claim 3.7 for a more general case, using
the Fisher-Neyman factorization theorem. Here we consider XS as fixed values, and randomness
only comes from noise z . We prove that the minimax linear estimator is of the form βMM = CβSS
and then design algorithms that calculate the optimal C .
Claim 3.1. The minimax linear estimator is OfthefOrm βMM = Cβssfor some C ∈ Rd×d.
Warm-up: commutative covariance matrices. In order to derive the minimax linear estimator,
We first consider the simple case when ∑t and ΣS are simultaneously diagonalizable. We apply
Pinsker’s Theorem (Johnstone, 2011) and get:
Theorem 3.2 (Linear Minimax Risk with Covariate Shift). Suppose the observations follow se-
quence model y$ = XSβ* + z, Z 〜N(0, σ2In). If ∑t = Udiag(t)U> andΣS ≡ XSXS/n =
U diag(s)U >, then the minimax linear risk
RL(B) ≡ βmAys m∈B Ek* (β -β*)k2=xχ n 1 (1 - √⅛)+,
where B = {β∣kβk ≤ r}, and λ = λ(r) is determined by ^∣ Pd=I /(√ti∕λ - 1)+ = r2. The
linear minimax estimator is given by:
βmm = ∑T1∕2U(I - diag(λ∕√t)) + U>∑1∕2βSS, where βss = -1∑— 1X>yS.	⑵
nS
Since r is unknown in practice, we could simply view either r or directly λ as the tuning parameter.
We compare the functionality of λ with that of ridge regression: 6Rr = arg mi% E * kXSβ —
ySk2 + λkβk2 = (∑S + λI) —1X>yS/n$. For both algorithms, λ is to balance the bias and
variance: λ = 0 gives an unbiased estimator, and a big λ gives a (near) zero estimator with no
variance. The difference is, our estimator shrinks some signal directions based on the value of
ti. The estimator tends to sacrifice the directions of signal where ti is smaller. Ridge regression,
however, respects the value of si. A natural counterpart is for ridge to also regularize based on
t: let 6Rrt = arg min1 k∑g2(β - ∑—1X>yS )k2 + λkβk2 = (∑τ + λI) -1 ∑t 6ss. We will
compare their performances in the experimental section.
3Throughout the paper Σ-1 could be replaced by pseudo-inverse and our algorithm also applies when n < d.
4
Under review as a conference paper at ICLR 2021
Non-commutative covariance matrices. For non-commutative covariate shift, we follow the
same procedure. Our estimator is achieved by optimizing over C: βMM = CβSS:
RL (B) ≡
βmns m∈BE 田2(e - β*)k2
min
β=Cβss
(Claim 3.1)
min
τ,C
r2τ + 士Tr(Σ着CΣ-IC>∑*)) , s.t. (C — I)>∑t(C — I) W τI. (3)
nS	T	T
Unlike the commutative case, this problem doesn’t have a closed form solution, but is still solvable:
Proposition 3.3. Problem (3) is a convex program and thus solvable.
We achieve near-optimal minimax risk among all estimators under some conditions:
Theorem 3.4 (Near minimaxity of linear estimators). When ΣS , ΣT commute, or ΣTis rank 1, the
best linear estimator from (2) or (3) achieves near-optimal minimax risk: LB(βMM) = RL (B) ≤
1.25RN (B).
Note that RN ≤ RL by definition. Therefore 1) our estimator 6mm is near-optimal, and 2) our
lower bound for RN is tight. Lower bounds (without matching upper bounds) for general non-
commutative problem is presented in (Kalan et al., 2020) and we improve their result for the
commutative case and provide a matching algorithm. Their lower bound scales with -d- mini ti
nS	si
for large r, while ours becomes	Pisi.OUr lower bound is always larger and thus tighter, and
potentially arbitrarily larger when maxiti and miniti are very different. We defer our proof to
si	si
the appendix.
Remark 3.1 (Benefit of minimax linear estimator). Consider estimators from ridge regres-
sion: 8Rr = argmin§ E *∣∣Xsβ 一 ys∣∣2 + 2∣∣β∣∣2. There is an example that RL(B) ≤
O(d-1/4LB(βRR)) even with the optimal hyperparameter λ. 4
Remark 3.2 (Incorporating the randomness of source and target features). For clean presentation
purposes, in the main text we assume to have access to ΣT. In practice, we will need to estimate
ΣTby finite unlabeled samples from target domain. In Appendix C.1 we show that our estimator
remains near-optimal if we have d unlabeled target samples under some standard light-tail
assumptions.
Theorem 3.4 is comparing our estimator with the optimal nonlinear estimator using the same data
Xs from the source domain. In appendix C we compare our estimator with a stronger notion of
linear estimator with infinite access to ps and show that our estimator is still within multiplicative
factor of it.
3.2	Linear minimax estimator with approximation error
Now we consider observations coming from nonlinear models: ys = f *(Xs) + z. Let βS =
argmi□β Ex〜PS,z〜N(o,σ2)[(f *(x) + Z 一 β>x)2], and similarly for βT. Notice now even with
f * unchanged across domains, the input distribution affects the best linear model. Approximation
error is as(x) = f *(x) 一 x>βS and vice versa for aτ.
4 Note this goes without saying that our method can also be order-wise better than ordinary least square, which
is a special case of ridge regression by setting λ = 0.
5
Under review as a conference paper at ICLR 2021
Define the reweighting vector w ∈ Rn as wi = pT (xi)/pS(xi). We form unbiased estimator via
β^LS =argmin{XPT(Xi) (β>Xi — yi)2} = (X>diag(w)Xs)-1(X>diag(w)ys).
β i pS (xi )
Claim 3.5. βLS is asymptotically unbiased and normally distributed:
√ns(8ls - βT) → N(0, ∑T1 Ex〜PT [Pt(x)∕Ps(x)(aτ(x)2 + σ2)xx>]∑T1).
Denote by m(x) = aT (x) + z. We want to minimize the worst case risk:
,min maχEk动/2(6 -βTτ)k2
β = CβLS βT ∈B
→d min max \ kΣT((C — I)βTk2 + — Tr(C∑tE EpT ['，,_ɪm(x)xx^]∑-ICT∑t))
CkeTk≤r V1 T	T1 112	ns	T	PT LPS(x)	T J
= min {k(C - I)>∑t(C - I)k2r2 + :Tr(C∑T1 EpT[PT(X)m(x)2xx>]∑T1C>∑τ)}
EI	∕'	. ∙	.	∙ A	∕γ X	∖	∕γ C 1
Therefore our estimator Is βMM J CβLs, where C finds
C J arg min (r2τ + -1 /ɪ X PT(X)(y - x>βLs)2XiX>, ∑T1C >∑τ C ∑T1”	(4)
τ,C	ns ns i P2s (X)	i	i T	T
s.t. (C-I)>ΣT(C-I)	τI.
Claim 3.6. Let B = {βlkβk ≤ r}, and f * ∈ F is some compact symmetric function class:
f ∈ F ⇔ -f ∈ F. Then linear minimax estimator is of the form CβLS for some C. When C
solves Eqn. (4), LB (βMM) asymptotically matches RL (B), the linear minimax risk.
By reducing from ys to βLS we eliminate n - d dimensions, and this claim says that Xs>ys is
sufficient to predict βT* . We note that f* is more general than a linear function and therefore the
lower bound could only be larger than RN (B) defined in the previous section.
3.3	Utilize source and target labeled data jointly
In some scenarios we have moderate amount of labeled data from target domain as well. Then
it is important to utilize the source and target labeled data jointly. Let ys = Xsβ* + zs, yT =
XTβ* + ZT. We consider XS, XT as deterministic variables, Σ-1X>ys∕ns 〜N(β*, ^2∑-1)
and ΣTIXTyT∕nτ 〜 N(β*, %∑T1). Therefore conditioned on the observations ys, yτ, a
sufficient statistic for β* is βSS ：= (ns∑S + nτ∑T)-1(X>ys + XTyT).
Claim 3.7. βss is an unbiased sufficient statistic of β With samples ys, yτ.	βss 〜
N(β ,σ2(ns∑S + nτ∑T)-1).
Algorithm: First consider the estimator βSS = (ns∑S + nτ∑T)-1(X>ys + XTyT). Next find
the best linear function of βSS:
βMM = arg min r2τ + σ2Tr((ns∑S + nτ∑T)-1C>∑τC), s.t. (C - I)>∑τ(C - I) Y τ.
C,τ
ι⅛ _ . . . _	_ r c τ-7	■	, ■	, A ■	.» r	CXr	C ττr?	1	∙ C
Proposition 3.8. The minimax estimator βMM is of the form Cβss for some C. When choosing C
with our proposed algorithm and when Σs commutes with ΣT and ΣT, we achieve the minimax
risk RL (B) ≤ 1.25RN (B).
6
Under review as a conference paper at ICLR 2021
4	Near minimax estimator with model shift
The general setting of transfer learning in linear regression involves both model shift and covariate
shift. Namely, the generative model of the labels might be different: ys = XSβS + ZS, and
yτ = XTβT + zτ. Denote by δ := βS 一 βT as the model shift. We are interested in the minimax
linear estimator when ∣∣δk ≤ Y and ∣∣βT∣∣ ≤ r. Thus our problem becomes to find minimax
estimator for βT ∈ B = {β∣kβk ≤ r} from ys, yT
Algorithm:	First consider a sufficient statistic (βs, βτ) for (βT, δ).	Here βs =
22
Σs1 XSnSlnS 〜N(βT + δ, nsΣs1), and βτ = ΣTIXTyT/nT 〜N(βT, %∑t1). Then
consider the best linear estimator on top of it: β = AιβS + A2βτ. Write ∆ = {δ∣∣δ∣ ≤ γ} and
LB,∆(β) := maxβT∈B,δ∈∆ ∣∣∑T2(∕3 一 βT)k2.
RL(B, δ^I	min _ LB,∆(β
β=A1βs+A2βτ
≤ min 四 ImaxSv {2k∑T"((Ai + A2 - I)βTk2 +2∣∑T2Aιδ∣2
A1,A2 kβT k≤r,kδk≤γ I
22
+—Tr(Aι∑-1A>) + — Tr(A2∑ -1A>)
nS	nT
=Amn {2k∑1∕2((Aι + A2 - I)∣2r2 + 2k∑1∕2Aιk2γ2
22
+--Tr(AIςS1A>) +-Tr(A2ςT1A>) =: rB,∆(A1, A2) \ .
nS	S 1	nT	T 2
(5)
(AM-GM)
Therefore we optimize over this upper bound and reformulate the problem as a convex program:
22
(A1,A2) - arg min	< 2ar2 + 2bγ2 +-Tr(Aι∑-1A>) +-Tr(A2∑T1A>) >
A1,A2,a,b	nS	S	nT	T
s.t.	(Ai + A2 一 I)t∑t(Ai + A2 一 I) W aI, A>∑tAi W bI. (6)
Our estimator is given by: βMM = AiβS + A2βT. Since βMM is a relaxation of the linear minimax
estimator, it is important to understand how well ∕3mm performs on the original objective:
Claim 4.1. Rl(B, ∆) ≤ Lb,δ(3mm) ≤ 2Rl(B, ∆).
Finally we show with the relaxation we still achieve a near-optimal estimator even among all
nonlinear rules.
Theorem 4.2. When ΣT commutes with ΣS, it satisfies:
Lb,Δ (βMM) := a” max ʌ k∑7 2 (βMM 一 βT )k2 ≤ 27Rn (B, ∆).
β ∈B,δ∈∆
Here RN(B, ∆) := minβ3 y『)maxe”∈B,δ∈∆ k∑T∕2(β — βT)k is the minimax risk.
Proof sketch of Theorem 4.2. For the ease of understanding, we provide a simple proof sketch
when ΣS = ΣT are diagonal. We first define the hardest hyperrectangular subproblem. Let
B(T) = {b : ∣βi∣ ≤ 丁力 be a subset of B and similarly for ∆(Z). We show that Rl(B, ∆)=
7
Under review as a conference paper at ICLR 2021
maxτ∈B,ζ∈∆ RL(B(τ), ∆(ζ)), and clearly RN(B, ∆) ≥ maxτ∈B,ζ∈∆ RN (B(τ), ∆(ζ)). Mean-
while we show when the sets are hyperrectangles the minimax (linear) risk could be decomposed
to 1-d problems: RL(B(τ), ∆(ζ)) = i RL(τi, ζi). Each RL(τi, ζi) is the linear minimax risk
to estimate βi from X 〜N(βi + δi, 1) and y 〜N(βi, 1) where ∣βi| ≤ Ti and ∣δi| ≤ Zi. This 1-d
problem for linear risk has a closed form solution, and the minimax risk can be lower bounded
using Le Cam’s two point lemma. We show RL(τi, ζi) ≤ 13.5RN(τi, ζi) and therefore:
1	Claim 4.1	Lemma B 2
3Lb,δ(βMM)	≤	RL(B, △) Lem= B.2 max ʌ RL(B(T), ∆(Z))
2	τ ∈B,ζ∈∆
Prop B.4.a	Lemma B.6
= max	RL (τi, ζi)	≤ max 13.5	RN (τi, ζi)
τ∈B,ζ∈∆	L i i	τ∈B,ζ∈∆	N i i
ii
Prop=B.4.b13.5 max RN(B(τ),∆(ζ)) ≤ 13.5RN (B, ∆).
□
(a) covariate eigen-spectrum
(b) signal strength
(c) model shift
Figure 1: Performance comparisons. (a): The x-axis α defines the spread of eigen-spectrum of ΣS :
Si Y 1∕iα ,ti Y 1/i. (b) x-axis is the normalized value of signal strength: ∣∣Σt β * ||/r. (C) X-axis
is the model shift measured by γ∕r. Performance with standard error bar is from 40 runs.
5	Experiments
Our estimators are provably near optimal for the worst case β * . However, it remains unknown
whether on average they outperform other baselines. With synthetic data we explore the per-
formances with random β * . We are also interested to investigate the conditions when we win
more.
Setup. We set ns = 2000, d = 50, σ = 1,r = drd. For each setting, we sample βT from standard
normal distribution and rescale it to be norm r. We assume to know ΣT . We compare our estimator
with ridge regression (S-ridge) and a variant of ridge regression transformed to target domain
(T-ridge): 6Rr,t = argmin 1 ∣∣Σ%2(β - Σ-1 X>ys)∣∣2 + λ∣∣β∣∣2 = (∑τ + λI)-1∑t∕3ss.
Covariate shift. In order to understand the effect of covariate shift on our algorithm, we consider
three types of settings, each with a unique varying factor that influences the performance: 1) covari-
ate eigenvalue shift with shared eigenspace; 2) covariate eigenspace shift with fixed eigenvalues5;
5We leave this result in appendix since performance appears invariant to this factor.
8
Under review as a conference paper at ICLR 2021
3) signal strength change. We also have an additional 200 labeled data from target domain as
validation set only for hyper-parameter tuning.
Model shift. Next We consider the problem with model shift. We sample a random δ with norm Y
varying from 0 to r = √d and observe data generated by ys = XS(βT + δ) + ZS ∈ R2000, ZS 〜
N(0, I) and yτ = XTβT + ZT ∈ R500, ZT 〜N(0, I). We compare our estimator with two
baselines: "ridge-source" denotes ridge regression using only source data, and "ridge-target" is
from ridge regression with target data.
Figure 1 demonstrates the better performance of our estimator in all circumstances. From (a) we
see that with more discrepancy between ΣS and ΣT , our estimator tends to perform better. (b)
shows our estimator is better when the signal is relatively stronger. From (c) we can see that with
the increasing model shift measured by γ∕r, ridge-source becomes worse and is outperformed by
ridge-target that remains unchanged. Our estimator becomes slightly worse as well due to the less
utility from source data, but remains the best among others. When γ∕r ≈ 0.2, our method has the
most improvement in percentage compared to the best result among ridge-source and ridge-target.
References
Martin Arjovsky, Lean Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Mahsa Baktashmotlagh, Mehrtash T Harandi, Brian C Lovell, and Mathieu Salzmann. Unsupervised
domain adaptation by domain invariant projection. In Proceedings of the IEEE International
Conference on Computer Vision,pp. 769-776, 2013.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Helge Blaker. Minimax estimation in linear regression under restrictions. Journal of statistical
planning and inference, 90(1):35-55, 2000.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In Advances in neural information processing systems, pp. 442-450, 2010.
Shai Ben David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain adaptation.
In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,
pp. 129-136, 2010.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pp. 248-255. Ieee, 2009.
David L Donoho. Statistical estimation and optimal recovery. The Annals of Statistics, pp. 238-270,
1994.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FrangOiS
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural
networks. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
9
Under review as a conference paper at ICLR 2021
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In ICML, 2011.
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsupervised
domain adaptation. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp.
2066-2073. IEEE, 2012.
Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks: Discrimina-
tively learning domain-invariant features for unsupervised domain adaptation. In International
Conference on Machine Learning, pp. 222-230, 2013.
Raghuraman Gopalan, Ruonan Li, and Rama Chellappa. Domain adaptation for object recognition:
An unsupervised approach. In 2011 international conference on computer vision, pp. 999-1006.
IEEE, 2011.
Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. In Advances
in Neural Information Processing Systems, pp. 9871-9881, 2019.
JiayUan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex J Smola.
Correcting sample selection bias by unlabeled data. In Advances in neural information processing
systems, pp. 601-608, 2007.
Jing Jiang and ChengXiang Zhai. Instance weighting for domain adaptation in nlp. In Proceedings
of the 45th annual meeting of the association of computational linguistics, pp. 264-271, 2007.
Iain M Johnstone. Gaussian estimation: Sequence and wavelet models. Unpublished manuscript,
2011.
Anatoli B Juditsky, Arkadi S Nemirovski, et al. Nonparametric estimation by convex programming.
The Annals of Statistics, 37(5A):2278-2300, 2009.
Seyed Mohammadreza Mousavi Kalan, Zalan Fabian, A Salman Avestimehr, and Mahdi
Soltanolkotabi. Minimax lower bounds for transfer learning with linear and one-hidden layer
neural networks. arXiv preprint arXiv:2006.10581, 2020.
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct
importance estimation. The Journal of Machine Learning Research, 10:1391-1445, 2009.
Ananya Kumar, Tengyu Ma, and Percy Liang. Understanding self-training for gradual domain
adaptation. arXiv preprint arXiv:2002.11361, 2020.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S Yu. Transfer feature
learning with joint distribution adaptation. In Proceedings of the IEEE international conference
on computer vision, pp. 2200-2207, 2013.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In International conference on machine learning, pp. 2208-2217. PMLR,
2017.
Shangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi, Yair Carmon, and
Ludwig Schmidt. Harder or different? a closer look at distribution shift in dataset reproduction.
In ICML Workshop on Uncertainty and Robustness in Deep Learning, 2020.
10
Under review as a conference paper at ICLR 2021
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer
component analysis. IEEE Transactions on Neural Networks, 22(2):199-210, 2010.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence.
Dataset shift in machine learning. 2009.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers
generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.
Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generaliza-
tion. arXiv preprint arXiv:1911.08731, 2019.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the
log-likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Megha Srivastava, Tatsunori Hashimoto, and Percy Liang. Robustness to spurious correlations via
human annotations. arXiv preprint arXiv:2007.06661, 2020.
Amos Storkey. When training and test sets are different: characterizing learning transfer. Dataset
shift in machine learning, pp. 3-28, 2009.
Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye. A two-stage weighting
framework for multi-source domain adaptation. In Advances in neural information processing
systems, pp. 505-513, 2011.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridge University Press, 2019.
Xuezhi Wang and Jeff Schneider. Flexible transfer learning under support and model shift. In
Advances in Neural Information Processing Systems, pp. 1898-1906, 2014.
Xuezhi Wang and Jeff G Schneider. Generalization bounds for transfer learning under model shift.
In UAI, pp. 922-931, 2015.
Xuezhi Wang, Tzu-Kuo Huang, and Jeff Schneider. Active transfer learning under model shift. In
International Conference on Machine Learning, pp. 1305-1313, 2014.
Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journal of
Big data, 3(1):9, 2016.
Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, pp. 819-827,
2013.
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. arXiv preprint arXiv:1901.09453, 2019.
11
Under review as a conference paper at ICLR 2021
A Omitted proof for minimax estimator with covariate shift
A.1 Pinsker’s Theorem and covariate shift with linear model
Theorem A.1 (Pinsker's Theorem). Suppose the obervations follow sequence model y% = θ* +
izi, i > 0, i ∈ [d], and Θ is an ellipsoid in Rd: Θ = Θ(a, C) = {θ : Pi ai2θi2 ≤ C2}. Then the
minimax linear risk
RL(Θ) := min max E ∣∣θ(y) — θ ∣∣2
θ linear θ* ∈θ
=	i2 (1 ―ai∕μU,
i
where μ = μ(C) is determined by
d
ɪ2 e2ai(μ - ai)+ = C2.
i=1
The linear minimax estimator is given by
O , ,	,	,	,
θi (y) = Ciyi = (I - ai∕μ)+yi,	G)
and is Bayes for a Gaussian prior ∏c having independent components θi 〜 N(0, τi2 ) with
τi = e23∕ai — 1)+.
Our theorem 3.2 is to connect our parameter β* to the θ* in pinsker's theorem. First We show that
reformulating the problem from a linear map of n dimensional observations yS to a linear map on
the d-dimensional statistic βSS is sufficient, i.e., Claim 3.1:
Proof of Claim 3.1. This is to show that if β(yS) := AyS is a minimax linear estimator, each
row vector of A ∈ Rd×n is in the column span of XS. Write A = A1XS> + A2W> where
W ∈ Rn×(n-d), columns of which forms the orthonormal complement for the column space of
XS. Equivalently we want to show A2 = 0. We have
RL(B) ≡ min max EkςT"Ge — β*)k2
β=Ayβ*∈B
=min max E ∣∑T∕2((AιX> + A2W>)ys — β*)∣∣2
A1,A2 β*∈B	T
=min max E k∑1∕2(A1X>(XSβ* + z) + A2W>z — β*)∣2 (Since W>Xs = 0)
=min max nk∑1∕2(A1X>XS — I)β*∣2 + E∣∣∑1∕2A1X>zk2
A1,A2 β*∈B	T 1 S S	T 1 S
+ E k∑1?A2W>z∣2 + E D∑1∕2AιX>z, ∑1∕2A2W>z)}
(Other cross terms vanish since E[z] = 0)
=min max {k∑k(AιX>Xs — I)β*k2 + Ek∑1∕2AιX>z∣2 + E k∑f A2W>z∣2, }
A1,A2 β*∈B
where the last equation is because
E D∑*AιX>z, Σ1^2A2W>z)= E [Tr [∑TZ2AιX>zzWA>∑τ口
12
Under review as a conference paper at ICLR 2021
= TrIΣ^2A1X> E[zzτ]WA>Στ∣ = σ2Tr [∑1∕2A1X>WA>Σt∣ =0.
Clearly, at min-max point, without loss of generality We can take A2 = 0.	□
Formally the proof for Theorem 3.2 is presented here:
Proof of Theorem 3.2. To use Pinsker’s theorem to prove Theorem 3.2, we simply need to trans-
form the problem match its setting. Let yτ = ∑TZ2Σ-1 XTyS/ns = θτ + ZT, where θ^ =
Uτ∑T∕2β* and zτ 〜N（0,σ2diag（[ti∕si]d=ι）∕ns）. The setfor θ^ is Θ = {θ∣k∑τ"Uθ∣∣ ≤ r},
ie, θ = {θιpiθ27ti ≤ r2}.
Now with Pinsker S theorem, θ（yτ）i = （1 一 1∕（μ√ti））+（yτ）i IS the best linear estimator for θT,
where μ = μ（r） solves
k 2 d n~. 1
ns X W （μ-√t= ）+ = r2.	⑻
i=1
Connecting to the original problem, we get that the best estimator for Σ^2β* is U（I 一
μ diagαu√ti]d=I））yτ = U （I- μ diagαv√ti]d=I））U >夕产夕-IXTyS/ns.
□
A.2 Omitted proof for noncommute covariance matrices
Convex program. Our estimator for β* can be achieved through convex programming:
Proof of Proposition 3.3. First note the objective function is quadratic in C and linear in τ , there-
fore we only need to prove the constraint S = {（C, τ ）|（C 一 I ）τ∑τ （C 一 I） W TI} is a convex set.
Notice for （C1, τ1）, （C2, τ2） ∈ S, i.e., （Ci 一 I）τΣτ（Ci 一 I） τiI, i ∈ {1, 2}. We simply need
to prove for Cα := αC1 + （1 一 α）C2, τα := τ1α + τ2（1 一 α）, （Cα 一 I）τΣτ（Cα 一 I） W ταI for
any α ∈ [0, 1]. First, notice （C1 一 C2）τΣτ（C1 一 C2） 0. Next,
（Ca 一 I）>∑τ（Cα - I）
=α（C1 一 I）τ Στ （C1 一 I） + （1 一 α）（C2 一 I）τ Στ （C2 一 I）
一 α（1 一 α）（C1 一 C2 ）τ Στ （C1 一 C2 ）
Wα（C1 一 I）τ Στ （C1 一 I） + （1 一 α）（C2 一 I）τ Στ （C2 一 I）
WταI.
□
Benefit of our estimator. Compared to ridge regression, our estimator could possibly achieve
much better （d-1/4） improvements:
ProofofRemark 3.1. We consider diagonal covariance matrices ΣS = diag（s）, ∑τ = diag（t）,
σ = 1. First we calculate the expected risk obtained with ridge regression: ∣3Rr = （XTXS/n +
13
Under review as a conference paper at ICLR 2021
λI)-1X>ys/ns 〜N((ΣS + λI)-1∑sβ*,∖∕ns(∑s + λI)-2∑s).
LB(QRr) =maχEyS k∑^(βRR(ys)-β*)k2
β* ∈B
=max k∑g2((Σs + λI)-1Σs - I)β*∣∣2 + T⅛(L(∑S + λI)-2ΣS∑t)
β*∈B	T	nS
2 √ √tisi	∕j-∖ l 1	tisi
= maxr -------- - √ti	+ 〉 ——7----^.
i si + λ	i nS (si + λ)2
Compared to our risk:
RL(B) = xχ nS si (1-√1μ)+,
Where n Pi=I ~~ (μ—√1r)+=r2.Let r2 = nd, si =ι, ∀i,ti =ι, ∀i ∈ [do], ti=d-1/2, do <
i ≤ d, where do = #√-] ≈ d1/4. Then μ = 1, and RL(B) = dn/4. In this case,
.	2 √ √tisi	∕T~∖ i X-''' 1	tisi
mm max r --------- - √ti	+ 〉 ——7------
λ i	si +λ i i nS (si + λ)2
= minmax √d f ^√t-	- √t∖	+ X ɪ /	ti、?	≥ min	√l-λ^-	+ √d 1
λ i n \ 1 + λ J nS ns (1	+ λ)2 λ	n (1 + λ)2 n	(1 + λ)2
√d
≥ 2n.
Therefore minλ LB(QRr) ≥ d1/4RL(B)/2.	□
Near minimax risk. Even among all nonlinear estimators, our estimator is within 1.25 of the
minimax risk:
Proof of Theorem 3.4. First we note that for both linear and nonlinear estimators, it is sufficient to
use βSS instead of the original observations yS. See Lemma A.2 and its corollary. Therefore it
suffices to do the following reformulations of the problem.
When ΣS and ΣT commute, we formulate the problem as the following Gaussian sequence model.
Recall ΣS = Udiag(s)U>, ∑t = Udiag(t)U>. Let θ* = U>∑T∕2β*, and y = U>∑^2βss 〜
N(θ*, nσ2diag(t/s)). Our objective of minimizing k∑TZ2(β(yS) - β*)k from linear estimator is
equivalent to minimizing ∣∣U(θ(y) - θ*)k = ∣∣θ(y) - θ*k from linear estimator.
The set for the parameter that satisfies θ* = U τ∑TZ2β*, kβ*k ≤ r is equivalent to ∣∑T1'U θ*k ≤
r ⇔ ∣∣θ*∕√ti∣ ≤ r is an axis-aligned ellipsoid. Then we could directly derive our result from
Corollary 4.26 from Johnstone (2011). Note that this result is a special case of Theorem 4.2 and
we have provided a detailed proof in Section B. Therefore here we save further descriptions.
For the case when ΣT = aaτ is rank-1, the objective function becomes:
RL(B) = Cmin maxE(ατ(β(yS) - β*))2.
β* linear β∈B
14
Under review as a conference paper at ICLR 2021
Then the result could be derived from Corollary 1 of Donoho (1994), which reformulate the problem
to the hardest one-dimensional problem which becomes tractable.
□
In the proof above, we equate the best nonlinear estimator on yS as the best nonlinear estimator on
βSS. The reasoning is as follows:
Lemma A.2 (Sufficient statistic is enough to achieve a best estimator). Consider the statistical
problem of estimating β* ∈ B from observations y ∈ Y. B '-compact. If S(y) is a sufficient
statistic of β , then the best estimator that achieves min^^ maxB '(β, β*) IS of the form β =
f (S(y)) with some function f, for any loss ` : Y → [0, ∞).
This Lemma is restated from Proposition 3.13 from Johnstone (2011).
Corollary A.3 (Corollary of Lemma A.2). Under the same setting of Lemma A.2, RN (B) is
achieved with the form β = f(S(y)).
A.3 Omitted proof with approximation error
Unbiased estimator for βT.
Proof of Claim 3.5.
6ls - βT =(X>diag(w)Xs)-1(X>diag(w)y) - βT
=(X>diag(w)Xs)-1(X>diag(w)(XsβT + aτ + Z))- @T
=(XS>diag(w)XS)-1(XS>diag(w)(aT + z))
Notice Eχ~ps[xaτ(x) PT(X) ] = Eχ~pτ [xaτ(x)] = 0. This is due to the KKT condition for
the minimizer of l(β) := Eχ~pτ Ilf*(x) - β>x∣∣2 at βT: Vef (β*) = 0 → Eχ~pτ[x(f* -
x>βT)] = 0, i.e., Eχ~p"xaτ(x)] = 0. Next We have: Ex_p, [X>diag(w)Xs] =
Ex―ps Pn=ι ppS(xi)Xix> = Eχj~pτ Pn=ι[xjx>] = ns∑t∙ Therefore
βLS - βT → N(0, -1∑T1 Eχ~pτ[pτ(x)∕ps(x)(aτ(x)2 + σ2)xxτ]∑T1).
nS
□
ProofofClaim 3.6. Recall XS = [x>∣x>∣ …∣x>]τ ∈ Rn×d, with Xi,∀i ∈ [n] drawn fromPS,
and aτ = [aτ(xι),aτ(x2),…aτ(Xn)IT ∈ Rn, y = [y(xι),y(x2),…，y(xn)]τ ∈ Rn, noise
z = y - f* (X). w = [pT(xi)∕pS(xi)]τ.
To prove the, we only need to show the minimax linear estimator Ay is achieved of the form
A1Xτdiag(w), i.e., the row span ofA is in the row span of Xτdiag(w).
RL (B) ≡min
eτ mX∙ ∈f Ex-" [H"(Ay - βT )k2]
min
A
max
βT ∈B,aτ ∈F
E k∑T/2((AX - I)βT + AaT + Az)Il2
15
Under review as a conference paper at ICLR 2021
min βτ W ∈f{田2((E[ax]
- I )βT + E[Aaτ ])k2
+ E∣∣Σ*(AX - E[AX])βTk2 + E∣∣∑TZ2(Aaτ - E[Aaτ])∣∣2 + E∣∣∑TZ2Az∣∣2}
Write A = A1X>diag(w) + A2W >, where X ∈ Rn×d and W ∈ Rn×(n-d) forms the orthogonal
complement for the column span of diag(w)X. Therefore X >diag(w)W = 0, and W>W =
Iη-d. Also, notice Exi〜ps[X>diag(w)aτ] = n Ex〜PT [xaτ(x)] = 0. Therefore plugging it in
RL(B), we have:
RL(B)=叫n	maχ	{k∑^2((Aι EpSX>diag(w)X] - I)βT + A2 E[W>aτ])k2
A βτ ∈B,f ∈F <
+ E k∑*Aι(X>diag(w)X - E[X>diag(w)X])βT∣∣2
+ E k∑T∕2A2(W>aτ - E[W>aτ])∣2
+σ2 E k∑TZ2AιX>diag(w)∣∣2 + σ2 E∣∣∑TZ2A2∣∣2}
min max	[∣∑T/2((AInS ∑t
A1,A2 βτ∈B,f*∈FU T
-I)βT + A2 E[W>aτ])k2
+ E k∑T/2Ai(X>diag(w)X - ∑t)βT∣∣2 + E k∑^2A2(W>aτ - E[W>aτ])∣2
+σ2 E k∑TZ2AιX>diag(w)∣2 + σ2 E∣∣∑TZ2A2∣∣2}
We could view E[W> aT] and W>aT - E[W> aT] separately. First notice at min-max point,
if E[W> aT] = 0, the minimizer A2 should be 0 since it only appears in the third and last non-
negative terms. If E[W>aT] 6= 0, the cross term of the bias should be non-negative, or otherwise
since both f * and -f * are in the set, aτ, βT could be replaced by -aτ, 一βT and the loss increases.
Clearly in this case A2 should also be 0 at min-max point.	□
A.4 Omitted proof for utilizing source and target data jointly
Sufficient statistic.
ProofofClaim 3.7. Denote by βs := Σ-1X>ys/ns 〜 N(β*, n∣Σ-1) and βτ :=
∑ T1 XTyT/nτ 〜N(β*, nσ7∑T1). We use the Fisher-Neyman factorization theorem to derive
the sufficient statistics. The likelihood of observing βs, βτ from parameter β* is:
p(βS, βT; β*)=ce-σ2 a-Q*)ς S (8s-β*)-W (β-β*A T (8τ-β*)
=cg(β*,T(β*))h(βs, βτ),
where g(β*,T(β*)) = e-(β*-βsS)>(σ2ςs+σTςt)i(β*-βSS), and C is some constant. Therefore
it S easy to see that T(β*) = βss is the sufficient statistic for β*.	□
Proof of Claim 3.8. With similar procedure as before, and notice zS and zT are independent,
we could first conclude that the optimal estimator is of the form β = AΣ-1X>ys/ns +
B∑T1X>yτ/nT 〜N((A + B)β*,怨A∑-1A> + *B∑T1B>).
Rl(B) =min max Ez |加；/2(6 - β*)∣2
A,B β* ∈B
16
Under review as a conference paper at ICLR 2021
= min max {|同1/2(A + B - I)β*k2
+σ2Tr((° AΣ-1A> + -1B Σ-1B>)Σt ))
nS S	nT	T
= min { |同1/2(A + B - I)kθpr2 + σ2Tr((n1-AΣ-1A> + n1-BΣ-1 B>)∑t)
Take gradient w.r.t A and B respectively we have:
2
Va(∣∣∑”(A + B - I)k2pr2) + — ΣτAΣ-1 = 0
2
=VB(|加1/2(A + B - I)k2pr2) + — ∑tB∑T1 = 0
TL T . •	.1 f- . .	- 1	El	1 Λ ŋ ——1	1 7-1 ŋ ——1 .1	.1	. •	1 A •	,1
Notice the first terms are equivalent. Therefore n1s aς- = nTbςt thus the optimalβ is of the
form C(XS>yS + XT>yT) for some matrix C, thus finishing the proof.
□
B Omitted Proof with Model S hift
Definition B.1 (Orthosymmetry). A set Θ is said to be solid and orthosymmetric if θ ∈ Θ and
|Zi | ≤ ∣θi∣ for all i implies that Z ∈ Θ.Ifa solid, orthoSymmetric Θ contains a point T, then it
contains the entire hyperrectangle that T defines: Θ(τ) ≡ {θ∣∣θi∣ ≤ Ti, ∀i} ⊂ Θ.
ProofofClaim 4.1. First notice for any estimator β, it all satisfies
LB,∆(β) ≤ rB,∆(β) ≤ 2LB∆(β).	⑼
The first inequality is straightforward with the same reasoning of AM-GM as the derivation of
(5). AS for the second inequality, we take a closer look at (5). Notice that when maxβ帝∈B,δ∈∆
is achieved, the cross term has to be non-negative, or otherwise one could flip the sign of βT
to make the value larger. Therefore at maximum ∣∣∑T/2((A1 + A2 一 I)βT∣∣2 + ∣∣∑Tz2A1δk2 ≤
∣∣∑TZ2((A1 + A2 一 I)βT + ∑TZ2A1 δ∣2, and notice the remaining parts are all non-negative.
Therefore rB,∆(β) ≤ 2Lb,δ(β).
Now let β* = argmιnz^=A1 ys+A2yS Lb,δ(β). Wehave:
(a)
Rl(B, ∆) =LB,∆(β*) ≤ LB,δ(6mm)
(9)	(b)	(9)
≤rB,∆ (∕3mm) ≤ rB,∆(β*) ≤ 2Lb,δ(∕3*) = 2Rl(B, ∆).
The inequality (a) is by definition of β* while (b) is from the definition of 6mm.	□
B.1	Lower Bound with Model Shift
In order to derive the lower bound, we abstract the problem to the following more general one:
17
Under review as a conference paper at ICLR 2021
Problem 1. For arbitrary diagonal matrix D ∈ Rd×d, two '2-compact, solid, orthoSymmetric,
and quadratically convex sets Θ, ∆ ⊂ Rd, let
D	f""Dθ + δ] 「I 0]∖ A	AI
PΘ,∆,D =俨 I θ , -0--T~ ) θ ∈ θ, δ ∈ δ)
Let RL(Θ, ∆, D) and RN (Θ, ∆, D) be the minimax linear risk and minimax risk respectively for
estimating θ within the distribution class PΘ,∆,D:
RL (Θ, ∆, D) = min max rP (θ),
θ=Rd→Θ linear P∈PΘ,∆,D
RN (Θ, ∆, D) = min max rP (θ).
θiRd→Θ P∈PΘ,∆,D
Here rP (θ) := Eχ~P ∣∣θ(x) - θ(Ρ )k2. We want to derive a uniform lower bound for RN with
RL, i.e., RN ≥ μ*Rl, where μ* is universal and doesn't depend on the choices of D, Θ or ∆.
Before proving the lower bound, we establish its connection to our considered problem:
Remark B.1. Suppose ΣS = U diag(s)U > and ΣT = U diag(t)U > share the same eigenspace.
Recall our samples a 〜 N(∑S∕2(βT + δ), σ2I), b ~ N(∑T2βT, σ2I). Our goal to uniformly
lower bound RN (r, γ) by RL (r, γ) is essentially Problem 1, where
Rl(t-, γ) := min max	E k∑11/2(β(a, b) — β*)k2,
β linear kβT k≤r,kδ∣∣≤γ "	1	' '
RN(r, γ) := min	max	E k∑T2(∣3(a, b) — β*)∣∣2.
β kβT k≤r,kδk≤γ u 1	' '
Proof of Remark B.1. Our target considers samples drawn from distributions x ~N (∑S∕2(βT +
δ),σ2I), y ~N(∑1∕2βT,σ2I).
^⇒
U >a∕σ
U >b∕σ
一 Udiag(s1/2)U>(βT + δ)
U diag(t1/2)U >βT
diag(s1/2)U >(βT + δ)
diag(t1/2)U >βT
σ2I
σ2I
,θ∈Θ,δ∈∆
,kβTk≤r,kδk∈ Y
a
b
〜N
〜N
0
0
I
0
0
I
Let a = U>a∕σ, b = U>b∕σ, Θ = {θ∣kdiag(t-1∕2)θk ≤ r}, ∆ = {kdiag(s-1∕2)δk ≤ γ}.
θ = U>∑1Z2βT, δ = U>夕%/26, and D = diag(s1/2t-l/2). Weget:
U>a∕σ ] ~ “(J diag(s1/2)U>(βT + δ)
U >b∕σ	I	diag(t1/2)U > βT
^⇒
"]),kβτ k≤r,kδk∈ γ
ab
bb
〜Pθ,δ,D ：= N
Dθb+δb
θb
I 0
0^ T
,θb∈ Θ,δb∈ ∆.
Let Pθ,δ,d ：= {Pθ,δ,DI θ ∈ Θ, δ ∈ ∆}. Since U is an invertible matrices, observing
U> a∕σ, U> b∕σ instead of a, b has no affect on the performance of the best estimator. Also
Θ, ∆ are axis-aligned ellipsoid and thus satisfy orthosymmetry. Therefore our problem is essen-
tially reduced to Problem 1.	口
18
Under review as a conference paper at ICLR 2021
Lemma B.2. Let Θ(τ) = {θ∣θi ≤ Ti, ∀i, θ ∈ Θ} and similarly for ∆(Z) = {δ∣δi ≤ Zi, δ ∈ ∆},
D is some diagonal matrix.
RL(Θ, ∆, D) = sup RL(Θ(τ), ∆(ζ), D), and
τ ∈Θ,ζ∈∆
RN(Θ,∆,D) ≥ sup RN (Θ(τ), ∆(ζ), D).
τ ∈Θ,ζ ∈∆
Write samples drawn from some Pθ,δ,D ∈ Pθ,δ,d as (x, y) : X 〜N(Dθ + δ, I), y 〜N(θ,I).
Lemma B.3. The minimax linear estimator θ : (x, y) → Ax + By has the form θa,b(x, y) =
i ai xi + i biyi for some a, b ∈ Rd. Namely,
Rl(Θ, ∆,D) = inf max rp(θa,b).
θa,b P∈pθ,δ,d
Proof. According to the proof of Proposition B.4.a, by discarding off-diagonal terms, the maximum
risk of any linear estimator Θa,b over any hyperrectangles Θ(τ), ∆(Z) is reduced.
max
θ∈Θ(τ),δ∈∆(ζ)
rPθ,δ,D (θA,B) ≥
max
θ∈Θ(τ),δ∈∆(ζ)
^
rPθ,δ,D (θdiag(A),diag(B) ).
Further we have:
♦	/ ∙A ∖ 、	♦	/ A	∖
min max rP	(θAB) ≥ min max max rP	(θdia (A) dia (B))
A,B θ∈Θ,δ∈∆ Pθ,δ,D A,B A,B τ∈Θ,ζ∈∆ θ∈Θ(τ),δ∈∆(ζ) Pθ,δ,D diag(A),diag(B)
= min max rPθ δ D (θa,b)
a,b θ∈Θ,ζ∈∆	θ,δ,D a,
≥ min max rPθ δ (θAB ).
C θ∈Θ,δ∈∆ Pθ,δ,D A,B
Therefore all four terms have to be equal, thus finishing the proof.
□
Notice Θ(τ) and ∆(ζ) are hyperrectangles in Rd. Therefore we could decompose the problem to
some 2-d problems:
Proposition B.4. Under the same setting as Problem 1,
a)	. RL(Θ(τ), ∆(ζ), D) =XRL(τi,ζi,Dii).
i
If θA,B (x, y) = Ax + By is minimax linear estimator over PΘ(τ),∆(ζ),D, then necessarily A, B
must be diagonal.
b)	. RN(Θ(τ),∆(ζ),D) =XRN(τi,ζi,Dii).
i
Proof of Proposition B.4.a . First review our notation:
rPθ,δ,D (θA,B ) = E(x,y)~Pθ,δ,D kθA,B(X, y) -θk2
=Ex〜N(Dθ+δ,I),y〜N(θ,I) IIAx + By - θk2
=kA(Dθ + δ) + Bθ - θk2 +Tr(AA>) + Tr(BB>)
=I(AD + B -I)θ+AδI2 +Tr(AA>) +Tr(BB>).
19
Under review as a conference paper at ICLR 2021
Our objective is
一 ，—，、・，,、一、 . , ʌ 、
Rl(Θ(t), ∆(ζ),D) :=min max	rPθ δ D (θA,B)
A,B θ∈Θ(τ),δ∈∆(Z)	θ,δ,	，
We will show that restricting A, B to be diagonal will not include the RHS value.




For any τ ∈ Θ(τ), C ∈ ∆(ζ), letset V(τ, ζ[= {(θ, b)|R,&) ∈ {(Ti,ζi), (-Ti,-ζi)}} be the
subset of vertices of Θ(τ) × △(4). Let π(τ, ¢) be uniform distribution on this finite set. Due to
the symmetry of this distribution, we have
E,
E,
⅛(τ,C) θi =0,i ∈ [d],
,π(τ,Z) δi = 0,i ∈ [d],
Eπ(T,Z) θiθj = 1i=jτi2,i ∈ [d],
Eπ(T,Z) δiδj = 1i=jζ2,i ∈ [d],
E∏(t,g θiδj = 1i=jTiZi,i ∈ [d].
We utilize the distribution to find the explicit^value of the maximum (in fact the maximum will
only be obtained inside the vertices set V(τ, ¢)):
,0	. 一
O
SKmax - 八 F,* (θ A,B)>Emt© rpθ,δ,D (θA,B)
(θ,δ)∈V (τ,ζ)
=E∏(t,g k(AD + B - I)θ + Aδ∣∣2 + Tr(AAT)+ Tr(BBT)
=Tr((AD + B - I) E[θθτ](AD + B - I)t) + Tr(AE[δδτ]Aτ)+
2Tr((AD + B - I) E[θδτ]Aτ) + Tr(AAτ) + Tr(BBτ)
=Tr((AD + B - I)τ(AD + B - I)diag(τ2)) + Tr(AτAdiag(ζ2))
+ Tr((AD + B - I)τAdiag(τG) + Tr(AAτ) + Tr(BBτ)
=^X Il(AD + B - i):,iTi + A:,iZk2 + Tr(AAT) + Tr(BBT)
i
> ^X((AiiDii + Bii- I)Ti + AiiZi )2 + A+ Bi2
i
= ∣(diag(A)D + diag(B) - I)θ + diag(A)δ∣2 + Tr(diag(A)2) + Tr(diag(B)2),	_
(∀(θ, δ) ∈ V(τ, G)
=max ∣∣(diag(A)D + diag(B) — I)θ + diag(A)δk2 + Tr(diag(A)2) + Tr(diag(B)2)
V (τ,Z)
Therefore we have:
一 ，—，、・，,、—、	.	，个	、
RL （θ（T）, △（"D）=m^ …泮蕖△© rκ,D（*，B）
min max
max
a,b T∈Θ(τ),Z∈∆(Z) θ∈V(τ,Z)
/ A ∖
rPθ,δ,D (θA,B )
> min max
max
a,b T∈Θ(τ),ζ∈∆(ζ)(θ,δ)∈V(T,G
丫Pθ,δ,D (θdιag(A),dιag(B))
min
max
a∈Rd,b∈Rd θ∈Θ(τ),δ∈∆(Z)
rpθ,δ,D (θα,b)∙
20
Under review as a conference paper at ICLR 2021
Next, since the optimal solution on the minimizer is always obtained by diagonal A, B , it becomes
straightforward that each axis could be viewed in separation, thus finishing the proof for part a.
The nonlinear part is a straightforward extension of Proposition 4.16 from Johnstone (2011).
□
Theorem B.5 (Restated Le Cam Two Point Theorem Wainwright (2019)). Let P be a family of
distribution, and θ : P → Θ is some associated parameter. Let ρ : Θ × Θ → R+ be some metric
defined on Θ and Φ : R+ → R+ is a monotone non-decreasing function with Φ(0) = 0. For any
α ∈ (0, 1),
11
inf suP [φ(P(θ,θ(P)))] ≥ PmaxQ 9φ(5P(WPI),θ(PZ)))(I- α),
θ p∈ρ	P1,P2∈P 2	2
s.t. kP1n - P2nkTV ≤ α.
LemmaB.6. Consideraclassofdistribution Pτ,ζs = {Pθ,δ,s∣Pθ,δ,s ：= N([sθ + δ,θ]>,I2), ∣θ∣ ≤
τ, ∣δ∣ ≤ ζ}. Define
2
RL(τ,ζ,s) = min Smax」,Eχ~Pθ,δ,s (θ(x) - θ) ,
θ linear lθl≤τJδl≤ζ
2
and RN(τ,ζ,s) =minlθlmax≤ζ Ex~Pθ,δ,s (Wx) - θ)
We have
RL (τ, ζ, s) ≤ 27/2RN (τ, ζ, s), ∀ζ, s > 0, τ > 0.
Proof of Lemma B.6. We first calculate an upper bound of RL and connect it to a lower bound of
RN.
RL (τ, ζ, s) = min max [(as + b - 1)θ + aδ]2 + a2 + b2
a,b ∣θ∣≤τ,∣δ∣≤ζ
=min(∣as + b — 1∣τ + |a|Z )2 + a2 + b2
a,b
≤ min 2(as + b - 1)2τ2 + 2a2 ζ2 + a2 + b2.
a,b
By some detailed calculations, we get the RHS is equal to:
2τ 2(2Z2 + 1)
2τ2(s2 +2Z2 + 1)+2Z2 + 1
≤ min{1, 2τ2, ：+；； }.
For simplify this form, we could see that
Next, we use Le cam two point theorem to lower bound RN(τ, ζ, s) where the metric ρ is Euclidean
distance and Φ is squared function. Therefore
RN(τ,ζ,s) ≥ max -( 0 (θ1 — θ2))2(1 — α)
∣θi∣≤τ,∣δi∣≤ζ,i∈{i,2} 2'2'
s.t. kN ([sθ1 + δ1, θ1]>, I2), N ([sθ2 + δ2, θ2]>, I2)kTV ≤ α.
21
Under review as a conference paper at ICLR 2021
Since the total variation distance is related to Kullback-Leibler divergence by Pinsker's inequality:
l∣∙, ∙kτv ≤ 1DDkl(∙∣∣∙), it S sufficient to replace the constraint as:
DKL (N([sθι + δ1,θ1]>,I2)∣∣N([sθ2 + δ2,θ2]>,I2)) ≤ 2α2.
max	ɪ(θɪ - θ2)2(l - α)
∣θi∣≤τ,∣δi∣≤Z,i∈{1,2} 8
s.t. (sθ1	+ δ1	- (sθ2 + δ2))2 + (θ1	- θ2)2 ≤ 2α2
c2
⇔ max —(1 — α)
∣c∣≤2τ,∣d∣≤2Z 8
s.t. (sc + d)2 + c2 ≤ 2α2.
Recall RL ≤ miη{1,2τ2, S++4Z}.
We first note that c2 ≤ 4τ2 and setting α = 0 We have RN ≥ T2/2 ≥ 1∕4Rl. For In the following
we look at other cases when the bound for c2 is smaller.
When 2Z ≥ sc, will set d = -Sc and c2 = 2α2. Let α = 2/3 for large T we get: c2(1 — α)∕8 =
2/27 ≥ 2/27RL.
When 2ζ ≤ sc we set d = -2ζ and require (sc - 2ζ)2 + c2 ≤ 2α2 . We have (sc - 2ζ)2 +
c2 = s2c2 + 4ζ2 - 4ζsc + c2 ≤ s2c2 + 4ζ2 - 8ζ2 + c2 = (s2 + 1)c2 - 4ζ2. Therefore as
we set c2 = 2彳；+；，2, the original inequality is satisfied. Again by setting α = 2/3 we have
c2 ≥ 8/91+；；? ≥ 8∕9Rl. Therefore in this case RN ≥ 227Rl.
□
C Discussions on Random Design under Covariate Shift.
In the main text, we present the results where we consider XS as fixed and ΣT to be known. In this
section, we view both source and target input data as random, and generalize the results of Section
3 while training is on finite observations and testing is on the (worst case) population loss, under
some light-tail properties of the input data samples.
C.1 Random design on target covariance matrix
In Section 3, we consider the case when ΣT is known exactly. This could be viewed as the fixed
design setting where training and testing are on the same set of data. In this section, our analysis
will include the estimation error on observing finite unlabeled samples of target domain. Let
XT = [x1, ∙ ∙ ∙ XnU]> ∈ RnU ×d be nu (Here U stands for unlabeled data and is used to distinguish
from nτ labeled target samples) data samples where Xi 〜PT, and we will use the unlabeled target
samples to conduct estimation. We let ΣT = XT XT/nu.
Let LB to denote the worst case excess risk measured on the observed target samples: LB(β) =
maxβ*∈B EyS n^ ∣∣Xt(β(ys) - β*)∣∣2. To find the best linear estimator that minimizes Lb, our
22
Under review as a conference paper at ICLR 2021
proposed algorithm becomes:
C 一 min (r2τ + QTr(ΣT/2CΣ-1C>ΣT/2)l , s.t. (C — I)>ΣT(C — I) W τI. (10)
τ,C	nS
And set β = CΣS-1XS>yS /nS . We want to show that in spite of the existence of estimation error
due to the replacement of ΣT with ΣT , our generated β performs well on the worst-case population
♦ t τ / A∖	ττn ττn	Il T / r> /	∖ zι⅛ ∖ 11 9 t t ∙	♦♦	t ∙	∙ ι /	.
risk LB(β) := maxβ* EyS Ex〜PT ∣∣x 1 (β(ys) 一 β*)∣∣2 and achieves minimax linear risk (UP to
constant multiplicative error).
In this section we assUme that the data samPles is light tail:
Definition C.1 (ρ2-sUbgaUssian distribUtion). We call a distribution p to be ρ2-subgaussian when
there exists ρ > 0 such that the random vector X 〜P is ρ2-Subgaussian. P is the Whitening of P
such that X 〜P is equivalent to X = Σ1/2x 〜P, where Σ is the covariance matrix of P. 6
Notice that here the sUbgaUssian Parameter is defined on the whitening of the data, and ρ doesn’t
dePend on how large ∣Σ∣op is.
Theorem C.2. Fix a failure probability δ ∈ (0, 1). Suppose target distribution PT is ρ2-
subgaussian, and the sample size in target domain satisfies nυ > ρ4(d + log δ). Let
1
β : ys → C Σs-1 Xs> ys where C is defined from Eqn. 10. Then with probability at least
1 一 δ over the unlabeled samples from target domain, and for each fixed Xs from source domain,
our learned estimator β(ys) satisfies:
LB(β) ≤ (1 + O(rρ4(d + lθg(1⅞))Rl(B).	(11)
n
SPeCifiCally, when ∑t commutes with ΣS or is rank 1, we have:
LB(β) ≤ (1.25 + O(rρ4(d +log(ɪ⅞))RN(B).	(12)
Similarly all other resUlts in the PaPer coUld be extended to random design with finite samPles XT .
Proof of Theorem C.2. The Proof relies on the two technical claims C.3, C.4.
Let βR be the oPtimal linear estimator on LB, i.e., LB(βR) = minβ linear in yS LB(β) = RL(B).
LB(β) ≤ (1 + O(JPYd + log≡δ)2))LB(β)	(ClaimC.4)
≤(1 + O(J ρ( + Iog(I/HI)) LB (βκ)	(from definition of β)
n
≤(1 + O(j PYd + log(区))2Lb (βR	(ClaimC.4)
n
≤(1 + O(J P4(d + log(10)))LB (β) = (1 + O(J P4(d + log(10)))RL(B).
nn
(from P (d+lng⑴δ”《1, and definition of βR
6A random vector x is called ρ2-sUbgaUssian if for any fixed Unit vector v of the same dimension, the random
variable v>x is ρ2-subgaussian, i.e., E[es∙v>(x-E[x])] ≤ es2ρ2/2 (∀s ∈ R).
23
Under review as a conference paper at ICLR 2021
From Theorem 3.4 we know RL (B) ≤ 1.25RN (B) when ΣT is rank-1 matrix or commute with
Σs which further finishes the whole proof.	□
Claim C.3 (Restated Claim A.6 from Du et al. (2020)). Fix a failure probability δ ∈ (0, 1), and
assume n>ρ4(d + log(1∕δ)) 7. Then with probability at least 1 一 条 over the inputs xι,..., Xn,
if Xi 〜P and P is a ρ2-SUbgaUSSian distribution, we have
r园"+log(10)	W iX>χ W(1+o(rp4(d+log(IA⅞)∑,	(13)
nn	n
where Σ = Ex〜p[xx>].
With the help of Claim C.3 we directly get:
Claim C.4. Fix a failure probability δ ∈ (0,1), and assume nu》ρ4(d + log(1∕δ)), XT =
[xι,…，XnU]> ∈ RnU×d satisfies Xi 〜 pτ where PT is ρ2 -subgaussian. We have for any
estimator β:
(1 -O(Sρ4(d + log(10)))LB(β) ≤ LB(β) ≤ (1 + O(Sρ4(d + log(10)))LB(β),
nU	nU
with high probability 1 — δ∕10 over the random samples XT.
Proof of Claim C.4. Recall
1
LB (β) = maχEys — kXτ(β(ys) — β*)k2,
β*∈B	nu
LB(β) = maχEyS k∑g2(β(ys) — β*)k2.
β* ∈B
Therefore for any estimator β, it satisfies
LB (例 一 LB (例
=(β(ys) — β*)>(∑s — ∑ s )(β(ys) — β*)
.O(S 因 d + log≡δ)⅛(β(ys) — β*)> ∑s (β(ys) — β*)
nU
S ρ4(d + log(1∕δ))	ʌ
=O(U----------------)LB (β),
nU
which finishes the proof.	□
7 When this is not satisfied the result is still satisfied by replacing O
ρ4(d+log(1∕δ))
) with
n
O(max{ J ρ4(d+lng(WC
simplify the results.
P (d+lng(1∕δ)) }). For cleaner presentation, we assume n is large enough and
24
Under review as a conference paper at ICLR 2021
C.2 Random design on source domain.
In the main text or the subsection above, the worst case excess risk is upper bounded by 1.25RN,
which is achieved by best estimator that is using the same set of training data (XS , yS). Here we
would like to take into consideration the randomness of XS and compare the worst case excess risk
using our estimator with a stronger notion of linear estimator.
For this purpose, We consider estimators that are linear functionals of yR := ∑S/2β* + Z ∈
Rd, z 〜N(0, σ2∕nsId) (this σ2∕ns is the correct scaling since X>XsInS is comparable to ∑s).
We consider the minimax linear estimator With yR and With access to ΣS, and We compare our
estimator against this oracle linear estimator. This estimator is not computable in practice since ΣS
must be estimated, but We Will shoW that our estimator is Within an absolute multiplicative constant
in minimax risk of the oracle linear estimator.
To recap the notations and setup, let
1
LB(β) ：=maxEyS — kXτ(β(ys) - β*)k2,
β*	nu
La(β)=maxEyS Ex〜p7∣∣x>(β(ys) - β*)∣∣2,
β*
LB,R(β)=maxEyR Ex〜PT ∣∣x>(β(yκ) - β*)∣∣2∙
β*
Our target is to find the best linear estimator using LB(β) (trained With XT) and prove its per-
formance on the population (Worst-case) excess risk LB(β) is no much Worse compared to the
minimax linear risk trained on yR and ΣS.
Theorem C.5. Fix a failure probability δ ∈ (0, 1). Suppose both target and source distributions
pS and pT are ρ2 -subgaussian, and the sample sizes in source domain and target domain satisfies
11
ns ,nu》ρ4(d + log 1). Let C be the solution for Eqn. (10), and set β(ys) - C ∑s'Xgys.
Then with probability at least 1 - δ over all the unlabeled samples from target domain and all
the labeled samples XS from source domain, our estimator β(yR) yields the worst case expected
excess risk that satisfies:
一 ^	S P4(d + log(1∕δ)) - Cf S P4(d +log(1∕δ))、∖	∙	, 「冷
LB(β) ≤	1 + O(I/	) + O(I/	) α Lmin	LR,B(β).
nU	nT	β linear in yR
Proof of Theorem C.5. For each matrix C ∈ Rd×d, We first conduct bias-variance decomposition
and reWrite each Worst-case risk With linear estimator in terms of a matrix C. When β(yS) =
C∑-1X>ys, We have:
2
LB(β) =k∑T/2(C - I)k2pr2 + 晟Tr(∑TC∑-1C>)=: "。)，
2
LB(β)=k∑T7C - I)k2pr2 + -nτr(∑τC∑-1C>) =: l(C),
Similarly, when ∕3r = CΣ-"2yR, we have:
2
Lr,b(β) =k∑^2(C - I)k2pr2 + nTr(∑τC∑-1C>) = IR(C).
25
Under review as a conference paper at ICLR 2021
ClaimC.6. Fix afailure probability δ ∈ (0,1), and assume nu ,ns》ρ4(d + log(1∕δ)), XS ∈
RnS ×d, XT ∈ RnU ×d are respectively from pS pT which are both ρ2-subgaussian. We have for
any matrix C ∈ Rd×d :
(1 -O(Sρ4(d +lθg(Uδ))))j(C) ≤ l(C) ≤ (1 + O(Sρ4(d + lθg(Uδ))))j(C),
nU	nU
with high probability 1 一 δ∕10 over the random samples XT.
(1 -O(Sp4(d + lθg(1⅞))l(C) ≤ IR(C) ≤ (1 + O(Sp4(d + lθg(I⅞))l(C),
nS	nS
with high probability 1 — δ∕10 over the random samples XS.
Proof of Claim C.6. We omit the proof of the first inequality since it’s exactly the same as proof of
Claim C.4.
For the second line, we have:
2
IR (C) — I(C) =—Tr(∑τ C (∑-1 — ∑-1)C>)
nS
≤O(S ρ4(d + lOg(I外)))σ2 Tr(∑T C ∑-IC >)
nS	nS	S
≤O(S ρ4(d + lθg(UδI)l(C).
nS
Therefore we prove the RHS of the second inequality. The LHS follows with the same proof
techniques.	口
■‰ τ 1	. /r ι .ι	♦	♦	♦	/`	1 / ^--t∖	ι /-« -t . ι	♦	♦	♦	/`	7	/ ^~ι∖
Now let C be the minimizer for l(C), and CR be the minimizer for lR(C).
I(C) ≤(1 + O(I∕ρ4(d + log(1∕δ))))f(C)
nU
≤(1 + O(S P4(d + log(10)))^(CR)
nU
≤(1 + O(∣ "≡正I))2l(CR)
nU
=(1 +O(S P4(d +”就⑷%心口)
nU
(w.p. 1 — δ∕10; due to Claim C.6)
(DUe to the definition of C)
(w.p. 1 — δ∕5; due to Claim C.6)
(since nU is large enough)
≤(1 + O([∕ρ4(d + l°g(10)))(1 + O([> +bgM∖)lR(CR)
nU	nT
(w.p. 1 — 3δ∕10; due to Claim C.6)
26
Under review as a conference paper at ICLR 2021
=1 + O (ʌ ∕p4≡⅛≡) + O (ʌ ∕p4≡M≡) min Ir (C).
nU	nT	C
This finishes the proof.	口
D More empirical results
We include some more empirical studies. In the main text our results have small noise. Here
we show some more results with larger noise, and also the case with varied eigenspace. For the
following results, We use σ = 10 and r = 0.2y∕d. Other meta data remains the same as presented
in the main text. Figure 2 (a)(b) show similar phenomenon as the small noise setting presented in
(a) covariate eigen-spectrum	(b) signal strength
(c) covariate eigenspace
Figure 2: (a): The x-axis α defines the spread of eigen-spectrum of ΣS: Si H 1∕iα ,ti H 1/i. (b)
x-axis is the normalized value of signal strength: ∣∣Σtβ* k∕r. (c) X-axis is the covariate shift due
to eigenspace shift measured by kUS - UT kF .
the main text. From Figure 2 (c) we see no particular relationship between the performance of each
algorithm with eigenspace shift.
D.1 Experiments with approximation error
Finally, we conduct empirical studies with nonlinear models.
Setup. We choose nS = 2000, d = 50. Let
XS ∈ R2000×50 be generated randomly under
Gaussian distribution N(0, ΣS). We also gener-
ate a small validation dataset from target domain:
XCV ∈ R500×50, sampled from N(0, ΣT), yCV =
f * (XCV) + zCV, with ZCV 〜N(0, σ21). We choose
λi(ΣS) H i, λi(ΣT) H 1∕i, and the eigenspace
for both ΣS and ΣT are random orthonormal ma-
trices. (∣ΣS ∣2F = ∣ΣT ∣2F = d.) The ground
truth model is a one-hidden-layer ReLU network:
f* (x) = 1∕da> (Wx)+, where W and a are ran-
domly generated from standard Gaussian distribution.
We observe noisy labels: yS = f* (x) + z, where
ZiZ N(0,σ2).
Figure 3: The x-axis is noise level σ and y-
axis is the excess risk (with approximation
error).
27
Under review as a conference paper at ICLR 2021
Estimating weights PT(x)∕ps(x). Since the generated data samples are Gaussian, the absolute
weights for PT(x)∕ps(x) = JIexp( 1 x>(Σ-1 - ∑T1)x). However, this absolute value has
an exponential factor and can amplify the noise level. Meanwhile, when one multiplies both
XS, yS by 10, the ground truth β doesn’t change but the absolute value for PT (x)∕PS (x) will
change drastically. This discrepancy highlights the importance of relative magnitudes (among
samples) instead of the absolute value Kanamori et al. (2009).
To obtain a relative score, we first estimate the absolute value of PT (x)∕PS (x) by l(x) :=
11
x>(ΣS-1 - ΣT-1)x. We then uniformly assign the weight for each sample by 10 discrete values
1,2,3 …10 based on their scoring l(x) and then rescale the reweighting vector properly.
We implement our method (Eqn. 4) using the estimated weights as above. Refer to Figure 3
for the results. The baselines we choose are ordinary least square ("OLS" in Figure (3)), ridge
regression (Legend is "Ridge") and classic weighted least square Kanamori et al. (2009) (Legend
is "Reweighting"; βLS in our main text). For both ridge regression and our methods, we tune
hyperparameters through cross-validation. All results are presented from 40 runs where the
randomness comes from f * and the eigenspaces of ∑s , ∑t .
28