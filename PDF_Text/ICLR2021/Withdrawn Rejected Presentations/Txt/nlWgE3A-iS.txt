Under review as a conference paper at ICLR 2021
ReaPER: Improving Sample Efficiency in
Model-Based Latent Imagination
Anonymous authors
Paper under double-blind review
Abstract
Deep Reinforcement Learning (DRL) can distill behavioural policies from sen-
sory input to solve complex tasks, however, the policies tend to be task-specific
and sample inefficient, requiring a large number of interactions with the environ-
ment that may be costly or impracticed for many real world applications. Model-
based DRL (MBRL) can allow learned behaviours and dynamics from one task
to be translated to a new task in a related environment, but still suffer from low
sample efficiency. In this work we introduce ReaPER, an algorithm that addresses
the sample efficiency challenge in model-based DRL, and illustrate the power of
the proposed solution on the DeepMind Control benchmark. Our improvements
are driven by incorporating sparse, self-supervised, contrastive model represen-
tations and efficient use of past experience. We empirically analyze each novel
component of ReaPER and analyze how they contribute to overall sample effi-
ciency. We also illustrate how other standard alternatives fail to improve upon
previous methods. Code for the plug-and-play tools introduced here will be made
available.
1	Introduction
Real world agents are able to efficiently achieve goals in complex, partially-controllable environ-
ments even in situations the agent has never experienced before. This capacity can be attributed to
a robust model of how the agent5s actions affect their surroundings, model that is distilled from past
experience, allowing the agent to estimate the effect of their actions in the pursuit of a goal in a novel
situation. In practice, learning an explicit world model as in Hafner et al. (2019a); Sekar et al. (2020)
is challenging in terms of sample efficiency (number of environment interactions), since the agent
needs to simultaneously Ieam good reward-seeking behaviours and refine a generalizeable model of
the environment.
This sample-efficiency challenge has been empirically observed to be more pronounced on agents
that act based on sensory input (e.g., raw pixels) instead of proprioception or latent states Lake et al.
(2017); Kaiser et al. (2019); Tassa et al. (2018). Since the latter is often inferrable based on raw
pixels, this indicates that pixel-based pipelines have room for improvement. This is an important
issue since many real-world applications are more naturally solved and articulated in terms of pixels
or sensory input.
In ⅛is work we investigate several potential sources of sample-inefficiency in model-based re-
inforcement learning (MBRL) methods, addressing each source separately using simple tech-
niques. We propose ReaPER (Regularized Contrastive Model-Based DRL Widl Prioritized Episodic
Replay), an MBRL algorithm built upon Dreamer Hafner et al. (2019a) that addresses the identified
sources of sample-inefficiency and improves results over the DMControl suite Tassa et al. (2020).
We provide ablation experiments for all the introduced components, interestingly, we show that tech-
niques that individually improve sample efficiency can sometimes show detrimental results when
combined, but can be made to work together by the introduction of prioritized replay. Although cur-
rent state of the art methodologies for single-task agents in the context of, for example, DMControl
suite are geared towards model free learning, we specifically choose to address MBRL as a stepping
stone towards continual learning in RL. An efficient Pytorch implementation of this code will be
made available.
1
Under review as a conference paper at ICLR 2021
Main Contributions. We introduce ReaPER, aɪi MBRL agent for control tasks in visual envi-
ronments that outperforms the previous state-of-the-art MBRL agent (Hafner et al. (2019a)) on the
DMControl benchmark Tassa et al. (2020). We empirically study key sources of sample inefficiency
in MBRL methods, and address each individually using simple approaches, the combination of all
successful approaches is integrated onto ReaPER. We experimentdly show that individually promis-
ing approaches can fail to compose when combined simply, but that the introduction of prioritized
replay can improve the combined approach beyond what each technique could individually achieve.
This provides a self-contained ablation study of the effects of each component of the proposed
ReaPER architecture.
We empirically show how ReaPER constructs a coarser but more robust world model, and how the
sample efficiency improvement holds throughout model training. Ideas to improve sample efficiency
that were ultimately unsuccessful such as exploration via latent disagreement Sekar et al. (2020) and
bisimulation metrics Fems et al. (2011); Fems & Precup (2014); van der Pol et al. (2020) are also
shown and discussed due to their value to improve the understanding of MBRL. Code will be made
available.
2	Agent description
We model the environment as a partially observable Markov decision process (POMDP) with dis-
crete time steps i ∈ [1 : T], high-dimensional observations and rewards produced by the en-
vironment ot, rt 〜 p(ot,rt I oi:t-i,ri;t-i,ɑɪ:i), and real-valued actions selected by the agent
at 〜p(at I oɪ:t, ri：t, ɑɪ:t-i). The goal of the agent is to maximize the expected discounted reward
¾(∑tΞzi 7t-ɪrt), with discount factor zγ ∈ (0,1].
We introdice Regularized Contrastive Model Based DRL with Prioritized Episodic Replay
(ReaPER), a latent dynamics arquitecture extending the work of Dreamer Hafner et al. (2019a),
with the following key components, which will be detailed and exploited in the coining sections:
Sequence Augmentation Image encoders Representation Transition Image Decoder Reward predictor Policy Value	{°t∙>ot}rt=τ =S∕({s}S⅛ ht = ʃə(θt),喟=fa ⑹, Pθ(st I Sf—i, tZ⅛-χ, ∕i⅛)j Qθ(st I	门] 明(或I比)，	⑴ Qθ(rt I st), ”力(α± I St), uψ(st)∙
We use the letters p, q to denote true and estimated distributions respectively. Ib interact with the
environment, observations ot are perturbed and then passed through the image encoder ht = fθ(ot),
the current model state is sampled from the representation Pe(St ∖ st-ι,at-ι,ht), and the action is
selected from policy at ~ ττφ(at ∖ si). The full algorithm is presented below, an explanation of each
component and the reasoning behind its inclusion is provided in the following sections.
2.1	Model learning
The model is trained on batches of sequential, observable past experience {{(4,门,4)}；+工}区,
where t indicates time from start of episode, L is sequence length, & is a batch indicator, and B is
the total number of batches. The modeΓs parameters, denoted by θ, are trained on a variety of com-
plementary objectives, some of these objectives are computed independently for each time sequence
(i.e.5 one per batch element), while others include comparisons against elements in different batches.
Ib simplify the notation, we denote each model loss by a super-index t or t,b to indicate if these
losses are computed across time or across time and batch respectively. The losses are described next.
Reconstruction loss. We follow the derivation in Hafner et al. (2019a) to increase the variational
lower bound of the model (ELBO, Jordan et al. (1999)). The resulting reconstruction loss is
2
Under review as a conference paper at ICLR 2021
Algorithm 1 ReaPER
Require: Hyper-parameters: Seed Episodes S, Collect interval C, Batch size B, Sequence length L, Imagi-
nation horizon H, learning rate Ir, Contrastive and Sparsity bonuses λc, λ£1, Contrastive momentum a
Initialize Priority Episodic replay buffer D with S random episodes
Initialize network parameters 仇 仇 ιψ
while training do
for step = 1,..., C do
Model Learning
Draw B sequential samples {(θt,Γt, αt)}J+i ~ D
Do sequence-consistent contrastive image augmentation {(o⅛ jo⅛)}J+z = <SΛ({ot}t+r)
Compute model states St ~ P(St ∣ o^,at-ι,st-ι)
Compute model component Crec,^e1 , as in equations 2 3 4.
Compute model loss Cm = Crec + 儿Ial + XC
Update buffer sample priorities according to Cm
Update model parameters on model loss θ — 9 — ∕rVβ(jCjvf)
Update key encoder parameters θ, — aθ, + (1- a)θ
Policy Learning
Imagine latent trajectories {(sτg)}* for each state st
Compute value targets H(ST) as in Eq 8
Compute policy and value losses £* £V as in Eq 7
Update φ φ- ZrVφ(jC7r);协一协一IzW(CV
end for
Data Collection
Oi — env.reset()
for Z = 1,... ,71 do
Sample s±iat ~ pe(st ∖ ot,at-ι, st-ι)7Vφ(at ∖ St)
Add exploration noise at ― clamp(at + e≡⅛; —1,1), z方 ~ ΛΓ(0,1)
Observe ot,rt — env.step(αt)
end for
Add to buffer D ■<_ _D ㊉{(θi,αt,Γi)}‰ι
end while
>ɛo = Inqe(Ot ∣ st), Z⅛ = Inge(r⅛ ∣ st),
IKL = -KL(p0(si I st-ι,αt-ι,∕ιt) H Qθ(st ∣ st-ι,at-ι)),	(2)
^tREC = ◎ + 盘 + ^KL-
The reconstruction loss £REC is intuitively interpretable, the state st needs to encode sufficient
information about the environment to reconstruct the current observation and reward ot, rt (>Cθ, £%),
it also needs to be compact enough to be predictable given the past state and action (£%兀). The state
then should capture the minimum amount of information of the past trajectory needed to reconstruct
current and future observations and rewards.
State-space compression. The state-space representation of the model may attempt to capture
irrelevant or non-generalizeable details of the environment. These details may still be consistent
across the training dataset, and thus are not eliminated by the state prediction loss KL. We therefore
add a sparsity prior to the model state to alleviate this issue,
= llst∣lli 匚瓜=EP[ɪ2dj	⑶
t
This prior is directly applied to the state-space representations, and not to the model weights as in
standard & regularization.
Robustness to visual perturbations. Contrastive learning has proven an effective technique to
Ieam robust representations from visual input Oord et al. (2018); He et al. (2020); Chen et al.
(2020a;b); Srinivas et al. (2020). We follow Srinivas et al. (2020), where a batch of sequential
observations {{ot}*+ly}^1 drawn from the dataset are augmented independently into query and
3
Under review as a conference paper at ICLR 2021
key observations {{(of,	(denoted by the superscript ql and k respectively), these aug-
mented pairs are fed through their corresponding encoders ʌ( ), ∕ρ>(∙) to produce paired encodings
{{(⅛f, ^t)}t+i}^=ι∙ The objective of the contrastive loss is to match the encoded query ⅛fb (where
we index the query both by time and sequence indicator) to its corresponding encoded key through
a bilinear log-loss classifier,
9((碍尸卬脸ɔ
E,∕,exp((%)"%,)∙	U
Here W ∈ R-*" is a learned bi-linear matrix, (∙)τ denotes transposition, and key parameters θt
are updated via exponential moving average θ, αθ, + (1- 0)θ.
The augmentations used to generate the key-query pairs define the invariance class we induce into
our image encodings. Since ReaPER is a model-based architecture that predicts future states based
an current model state, it is imperative for these augmentations to be temporally consistent, oth-
CTwise the augmentation procedure is ɪɑ direct conflict to the observation reconstruction loss £%
and the prediction loss since it introduces an unpredictable perturbation in successive obser-
vations that makes pixel-wise prediction impossible. We follow Srinivas et al. (2020) and chose
time-consistent random cropping as our augmentation class, where all observations of the same
sequence are cropped using the same mask. Randomized temporalIy-ConSiStent CrOpPing has the
added benefit of having a low computational budget, since it can be computed as a tensorized view
of the observation matrix, and requires no extra post-processing. An example is shown on Figure 1.
Figure 1: Random crop augmentation. Top row shows original observations £rom a single episode
on the walker run eπviromnent, middle and bottom row show corresponding key and query observa-
tions. Note that the crop is consistent across time on both key and quray. CMLginal image is 80 × 80,
cropped images are 64 × 64.
Per-sample model loss. The resulting pωr-sample model loss is a linear combination of the previ-
ous objectives,
^ = ^‰ + ¼416 + λ^.	(5)
Since this model loss contains both a state sparsity teπn and a contrastive robustness term, we
expect the model to produce coarser and more robust reconstructions of its environment, this simpli-
fied and robust reconstruction yields faster policy improvement than the baseline, improvmg sample
efficiency. Figure 2 shows a comparison between Dreamer and ReaPER reconstructions on 4 envi-
ronments. In this figure we can observe that ReaPER input reconstructions have diminished fidelity
on non-critical components of the image, such as the background splatter pattern, but still accurately
reconstruct both the agent and the fl∞r, the latter of which can be used as a proxy for relative motion
in these environments.
ITD be consistent with the lit0"ature, q is used both for the estimated distributions and for the query, the
context making the distinction clear.
4
Under review as a conference paper at ICLR 2021
Figure 2: Comparison of observation versus reconstructed observation in Dreamer vs ReaPER for
the walker run, hopper hop, finger spin, and cartpole two poles environments. ReaPER builds coarse
and robust reconstructions compared to Dreamer, where irrelevant background details are blurred
but important elements in the environment are well modeled.
2.2	Prioritized Episodic Replay
Prioritized experience replay Schaul et al. (2015) has been extensively used in off-policy, model-
free Ieaniing (Schaul et (2015); Wang et al. (2016); Hessel et al. (2017), as a way to enable the
learning agent to focus on high error samples from the replay buffer, and. thus converge 也Ster to a
good quality agent. We extend this notion to episodic (temporalLy-SeqUential) sampling, where a
sequence {(c⅛, c⅛, ∕)}各K ɪs sampled from the buffer with probability
tq+∙L
Pt0 (× ∑
t=to
(6)
Since policy and. value learning are peɪformed exclusively on imagined trajectories, we chose model
loss as our base priority measure, since experience is primarily used to train model components.
Standard prioritized replay utilizes a sum-tree for efficient sampling. We extend this algorithm
to temporally-sequential sampling via a tuple-valued sum-tree, where each leaf in the tree (sample)
contains both its own sample loss	and the episodic sample loss	珪 £%)； whenever a sample
is fed through the model, we register <5i, the change in the sample loss, and add this sample to an
update queue. Before sampling £rom the prioritized replay buKer, all samples in the update queue
update their own	value and the cumulative E；比 values of all leaves up to distance M with
change δt, the non-leaf nodes are then updated based on die cumulative loss as usual. This update is
computationally efficient and only introduces a small O(L) Iinear overhead w.r.t the standard sum-
tree algorithm. Td our understanding, this is the first application of prioritized replay in MBRL.
2.3	Policy learning
Im^ined trajectories. The latent dynamic components qe(st ∖ 为_1,/一1),私(> ∣ st) define
a fully ObSa^abIe Markov decision process (MDR Sutton & Barto (2018)); we denote tiιe time
indices drawn from these imaginary trajectories by τ. Imagiaaiy trajectories start at true model
states st and then follow the NTOP defined below, the policy and value functions are trained jointly
on imagined trajectories on the objectives
呼 Eqe ,”/£3 匕⑶)],minEββjπ	方| %®) -匕®)胤	(7)
φ	ψ
where 匕(St) is an exponentially-weighted average of the b∞tstrapped value target as defined in
Hafher et al. (2019a),
唆(ST) = EqHJWyI+√lf %(sQ, h = min(τ + fc,t + H)],
v}M =(i-λ) * *F(sτ) + *fFGτ)∙
Under review as a conference paper at ICLR 2021
Note that we can differentiate through model transitions and through the reward function, so the
policy can propagate gradients throughout the transitions and Ieam directly via gradient descent,
rather than relying on policy gradients. Likewise for the value function.
2.4	Architecture
We emulate the architecture of Hafner et al. (2019a), our observation encoder and decoder and
qθ(ot I st) are implemented with a CNN and a transposed CNN respectively, the representation and
transition functions pe(st ∖ St-ι5 ɑi-i, Qθ(st ∖ st-ι,dt-ι) are jointly implemented with an
RSSM Hafher et al. (2019b), which splits the state st into a deterministic component and a stochas-
tic component, the deteπninistic component is shared for the representation and transition function,
and does not depend on the current image embedding ht. All other components are implemented
as MLPs. We note that only the contrastive component introduces an additional architectural com-
ponent compared to baseline Dreamer, and thus ReaPER incurs very limited additional processing
overhead compared to the former method. The experiments with each ReaPER component were de-
signed so that minimal or no changes to the architecture were made whenever possible. For details
on architecture and hypeιparameters refer to section 6.1.
3	Related Work
In this work we combine and extend ideas from diverse sources to address sample efficiency in
MBRL. The works of Sekar et al. (2020); Hafner et al. (2019a) propose the use of analytic gradients
through latent dynamics to train the policy, making efficient use of the world model to reduce the
number of environment interactions needed to recover good behaviours from the available environ-
ment interactions.
Schaul et al. (2015) proposed the use of prioritized memory replay to enable the agent to focus on
high loss samples in the experience buffer. This ensures that examples from rare or novel transitions
are adequately explored by the model, reducing the need to collect a large number of transitions
before these peculiarities of the environment can be learnt. While their work focused on Q-leaming,
here we extend their work to episodic sampling.
The use of sparse priors and data augmentation to improve performance in machine learning al-
gorithms is well documented. Recent works in contrastive learning in particular He et al. (2020);
Henaff et al. (2019); Chen et al. (2020a) have yielded impressive results in other areas of machine
learning, and recently Srinivas et al. (2020) applied these notions to augment a SAC-Iike agent with
contrastive learning. The use of data augmentation in RL has also proven effective in Laskin et al.
(2020), who also identify temporally-consistent random cropping as a particularly effective type of
data augmentation for image-based pipelines in RL.
The use of alternative metrics to improve representation learning has been extensively studied Oord
et al. (2018); Lee et al. (2019); Hafner et al. (2019b); one approach to avoid pixel reconstruction
relies on the use of bisimulation distance Fems et al. (2011); Fems & Precup (2014); van der Pol
et al. (2020), where two states are taken to be similar if for any action sequence effected on these
states, the rewards are similar. The work of Zhang et al. (2020) addressed the use of bisimulation
in DRL, and their experiments show that their agent is robust to large nuisance perturbations in the
visual input. In section 6.2 we adapt this idea to latent bisimulation distance, but we observe no
improvement in terms of sample-efficiency.
The work of Sekar et al. (2020) utilizes an ensemble of embedding predictors to estimate model
uncertainty, this uncertainty is used as an unsupervised reward to Ieam a robust world model without
extrinsic rewards. In section 6.3 we utilize latent disagreement to train an auxiliary exploration
policy for data collection. Our results indicate that this did not improve sample efficiency in the low
sample regime, further motivating our approach.
4	Experiments and Results
We experimentally show the improvement in sample efficiency over a range of tasks from the Deep-
mind control suite. All environments feature an articulated robot that the agent controls, with contin-
6
Under review as a conference paper at ICLR 2021
uous actions in the range [—1,1], instantaneous rewards line in the range rt ∈ [0,1], the maximum
episode length is 1000, so all environments can accrue at most 1000 reward per episode. To iso-
late the contribution of each novel component of ReaPER, we first conduct an ablation study over
a smaller subset of tasks, to avoid architecture changes between contrastive and non-contrastive
implementations, we render observations on non-contrastive experiments at 64 × 64 pixels, and ex-
periments using contrastive loss and random cropping are rendered at 80 x 80 pixels then cropped to
ht original 64 × 64 resolution. The results, shown in Table 1 for the best hyperpaπneters, indicate that
contrastive augmentation and sparse priors can both improve sample efficiency over the benchmark,
achieving higher rewards for the same number of environment interactions. Surprisingly, prioritized
episodic replay by itself seemed to reduce the performance when compared against the baseline.
Prioritized experience replay was, however, effective at improving sample efficiency when paired
with the contrastive and sparsity auxiliary losses
Table 1: Episodic reward average for Dreamer, ReaPER, and the individual components of ReaPER (Con-
trastive loss, Ll and Prioritized Episodic replay, noted Contrast, Ll and PER respetively), as a function of
environment steps, the conjunction of Ll regularization and Contrastive loss is also shown (LlContrast). Re-
wards are averaged across the cartpole balance, cartpole swingup, reacher easy, cup catch, finger spin, walker
walk, walker run, and cheetah run environments in DMControl. ReaPER consistently outperforms the other
options. Prioritized episodic replay by itself does not improve upon the Dreamer baseline, but is effective in
conjunction with the rest of the ReaPER pipeline. Conversely, LlContrast under-performs both Ll and Contrast
me⅛ods over all tested hyperparameters, showing that these methods do not compose well unless paired with
prioritized episodic replay.
Steps	Dre∑ιmer	Contrast	Ll	LlContrast	PER	ReaPER
IOOK	309	349	358	262	263	374
200K	549	663	563	619	452	670
300K	636	714	693	691	562	756
400K	682	732	752	758	595	780
500K	707	760	774	741	611	787
We further validate these observations on a significantly larger set of experiments. We compare
results across 18 environments and observe that, in general, ReaPER is able to obtain the same or
greater rewards than Dreamer asymptotically, but it achieves these results in a smaller number of
environment interactions. Figure 3a shows how the average reward over the benchmark increases as
a function of environment interactions, and how this metric is improved by the addition of the regu-
larization proposed in ReaPER. Figure 3b highlights these results by showing the average number of
interactions required to achieve or surpass a given reward target for the first time over the baseline.
Table 2 shows the number of steps required to reach a reward target for Dreamer and ReaPER over
the same environments. We observe ⅛at ReaPER achieves better results on the benchmark across
the training procedure, it also requires less samples to achieve these targets; this is especially pro-
nounced for higher values of target rewards, since a good model is required for consistently high
rewards.
Figure 4 shows the reward vs environment step curves for the best 8 environments. These environ-
ments show a marked improvement by the proposed ReaPER from the Dreamer baseline, having
overall better results at each timestep. Some of the environments can continue improving with ad-
ditional training time, but the results here highlight that the auxiliary objectives in ReaPER can
provide a marked improvement on environments of varying complexity, making model based RL to
be more effective at learning the task with less environmental samples. We also highlight that this
performance increase is also marked by significantly decreased inter-seed variance.
7
Under review as a conference paper at ICLR 2021
Oooo
70503010
(upo∞d山
Q.400K
300K
200K
200	400	600	800	1000
Episode Rewards

(a) Average reward across environment steps
(b) Average tɪme-to-ɪewaɪd
Figure 3: Performance CamPariSon across 18 environments of the DMControl benchmark. Left
Figure shows average agent reward as a function of environment steps. Across all environments,
ReaPER achieves consistently better episode rewards for the same number of environment steps.
Right figure compares the average number of steps needed to first meet or exceed a reward target on
the benchmark; in this metric Reaper also outperforms Dreamer across the benchmark.
Figure 4: Penonnance companson between Dreamer and the proposed ReaPER across 8 of the best
p^fbπning environments in the benchmark, mean and standard deviations are computed across 3
seeds. These environments show a marked ∞ntrast in sample efficiency between the two methods,
highlighting the benefit of selecting the appropriate auxiliary objectives during training.
Table 2: Time-to-reward comparison between Dreamer and ReaPER on 8 environments in DMCon-
trol benchmark. Values indicate the ratio between average number of steps required to reach reward
target (expressed as percentage of maximum reward achieved by the RemER agent). Average num-
ber of steps computed over the seeds that reached the target, so the estimate is Optunistic. Larger
numbers indicate longer times for Dreamer to reach the target reward, numbers PreCeeded by >
indicate that Dreamer did not achieve the target before 500⅛ timesteps.
R d 夕 Cartpole	Reacher	Finger	Walker	Walker	Pendulum	Quadruped	Hopper
°	Two-poles easy spin	walk run swingup walk hop
%%%%%
Ooooo
13 5 7 9
1	.70	2.21	.87	2.69
1.11	.76	1.68	.90	1.26
2.47	1.15	1.35	.95	1.49
2.15	> 2.46	1.42	1.01	1.43
> 1.01	1.64	1.62	1.49	> 1.03
83.69.75.84,76
♦ 1 1 1 1
1.10	1.5
1.13	1.98
1.55	1.37
1.77	1.64
> 1.09	1.81
8
Under review as a conference paper at ICLR 2021
5	Discusion
We study sample efficiency in the context of model based RL and propose ReaPER, an agent that
Ieams behaviours on latent imagination and improves upon Dreamer by explicitly introducing aux-
iliary objectives that enable the agent to only encode and Ieam the parts of the environment which
are relevant for behaviour learning. We ablate each novel component of ReaPER to show how each
contributes to the overall agent, and experimentally validate our findings on visual control tasks in
the DMControl environment suite.
Our experiments indicate that methods that have individually been shown to be effective outside of
model-based RL do not necessarily translate to MBRL. Such was the case of latent disagreement
for exploration and bisimulation5 which have been successfully used for other purposes in RL, but
proved to be ineffective at addressing sample efficiency in this context. Furthermore, components
that individually do improve sample efficiency can, Counterintutively, reduce performance when
combined; in the case of ReaPER, this apparent anti-synergy was resolved Wi出 the addition and
adaptation of prioritized episodic replay.
This paper focuses heavily on model-based architectures since we believe MBRL is better suited to
handle curriculum learning and continual learning, and to environments where reward sparsity is in
itself a major hurdle to overcome; a potential future direction of our research. We wish to extend
this approach for other visual-based tasks that may rely on discrete control.
References
Ting Chen, Simon Komblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Djork-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511,07289, 2015.
Norm Fems, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov
decision processes. SIAM Journal on Computing, 40(6):1662-1714, 2011.
Norman Fems and Doina Precup. Bisimulation metrics are optimal value functions. In UAI, pp.
210-219. Citeseer, 2014.
David Ha and Jiirgen Schmidhuber. World models. arXivpreprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv: 1912. Ol603, 2019a.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Wlegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565. PMLR, 2019b.
Kaiming He3 Haoqi Fan, Yiixin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE∕CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Olivier J Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and
Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. arXiv
preprint arXiv:1905,09272, 2019.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan5 Bilal Piot, Mohammad Azar5 and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. arXiv preprint arXiv:1710.02298, 2017.
Zhimin Hou, Kuangen Zhang, Yi Wan, Dongyu Li, Chenglong Fu, and Haoyong Yu. Off-policy
maximum entropy reinforcement learning: Soft actor-critic with advantage weighted mixture pol-
icy (sac-awmp). arXivpreprint arXiv:2002.02829, 2020.
9
Under review as a conference paper at ICLR 2021
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Lukasz Kaiser5 Mohammad Babaeizadeh5 Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum5 and Samuel J Gershman. Building
machines that Ieam and think like people. Behavioral and brain sciences, 40, 2017.
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Re-
inforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.
Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953,
2019.
Aaron van den Oord, Yazhe Li, and Qriol Vinyals. Representation learning with contrastive predic-
tive coding. arXivpreprint arXiv:1807.03748, 2018.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.
Planning to explore via self-supervised world models. arXiv preprint arXiv:2005.05960, 2020.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez5 Yazhe Li3 Diego de Las Casas, David Bud-
den, Abbas Abdolmaleki, Josh Merel5 Andrew Lefrancq5 et al. Deepmind control suite. arXiv
preprint arXiv:1801.00690, 2018.
Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal5 Ybtam Doron, Siqi Liu, Steven Bohez5 Josh
Merel, Tom Erez, Timothy Lillicrap, and Nicolas Heess. dm control: Software and tasks for
continuous control, 2020.
Elise van der Pol, Thomas Kipf, Frans A Oliehoek, and Max Welling. Plannable approximations to
mdp homomorphisms: Equivariance under actions. arXiv preprint arXiv:2002.11963, 2020.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In International conference on machine
learning, pp. 1995-2003, 2016.
Amy Zhang3 Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning
invariant representations for reinforcement learning without reconstruction. arXiv preprint
arXiv:2006.10742, 2020.
10