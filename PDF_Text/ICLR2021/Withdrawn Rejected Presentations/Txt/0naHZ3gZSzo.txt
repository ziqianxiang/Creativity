Under review as a conference paper at ICLR 2021
Optimizing Large-Scale Hyperparameters via
Automated Learning Algorithm
Anonymous authors
Paper under double-blind review
Ab stract
Modern machine learning algorithms usually involve tuning multiple (from one to
thousands) hyperparameters which play a pivotal role in terms of model generaliz-
ability. Black-box optimization and gradient-based algorithms are two dominant
approaches to hyperparameter optimization while they have totally distinct advan-
tages. How to design a new hyperparameter optimization technique inheriting all
benefits from both approaches is still an open problem. To address this challenging
problem, in this paper, we propose a new hyperparameter optimization method with
zeroth-order hyper-gradients (HOZOG). Specifically, we first exactly formulate
hyperparameter optimization as an A-based constrained optimization problem,
where A is a black-box optimization algorithm (such as deep neural network).
Then, we use the average zeroth-order hyper-gradients to update hyperparameters.
We provide the feasibility analysis of using HOZOG to achieve hyperparameter op-
timization. Finally, the experimental results on three representative hyperparameter
(the size is from 1 to 1250) optimization tasks demonstrate the benefits of HOZOG
in terms of simplicity, scalability, flexibility, effectiveness and efficiency compared
with the state-of-the-art hyperparameter optimization methods.
1	Introduction
Modern machine learning algorithms usually involve tuning multiple hyperparameters whose size
could be from one to thousands. For example, support vector machines (Vapnik, 2013) have the regu-
larization parameter and kernel hyperparameter, deep neural networks (Krizhevsky et al., 2012) have
the optimization hyperparameters (e.g., learning rate schedules and momentum) and regularization
hyperparameters (e.g., weight decay and dropout rates). The performance of the most prominent
algorithms strongly depends on the appropriate setting of these hyperparameters.
Traditional hyperparameter tuning is treated as a bi-level optimization problem as follows.
min f(λ) = E (w(λ), λ), s.t. w(λ) ∈ arg minw∈RdL(w, λ)	(1)
λ∈Rp
where w ∈ Rd are the model parameters, λ ∈ Rp are the hyperparameters, the outer objective E
1 represents a proxy of the generalization error w.r.t. the hyperparameters, the inner objective L
represents traditional learning problems (such as regularized empirical risk minimization problems),
and w(λ) are the optimal model parameters of the inner objective L for the fixed hyperparameters λ.
Note that the size of hyperparameters is normally much smaller than the one of model parameters (i.e.,
p d). Choosing appropriate values of hyperparameters is extremely computationally challenging
due to the nested structure involved in the optimization problem. However, at the same time both
researchers and practitioners desire the hyperparameter optimization methods as effective, efficient,
scalable, simple and flexible1 2 as possible.
Classic techniques such as grid search (Gu & Ling, 2015) and random search (Bergstra & Bengio,
2012) have a very restricted application in modern hyperparameter optimization tasks, because they
only can manage a very small number of hyperparameters and cannot guarantee to converge to
1The choice of objective function E depends on the specified tasks. For example, accuracy, AUC or F1 can
be used for binary classification problem. Square error loss or absolute error loss can be used as the objective of
E for regression problems on validation samples.
2“effective”: good generalization performance. “efficient”: running fast. “scalable”: scalable in terms of
the sizes of hyperparameters and model parameters. “simple”: easy to be implemented. “flexible”: flexible to
various learning algorithms.
1
Under review as a conference paper at ICLR 2021
Table 1: Representative black-box optimization and gradient-based hyperparameter optimization
algorithms. (“BB” and “G" are the abbreviations of black-box and gradient respectively, and “帛”
denotes that the property holds for a small number of hyperparmaters or medium-sized training set.
“Scalable-H” and “Scalable-P” denotes scalability in terms of hyperparameters and model parameters
respectively.)
Algorithm	Type	Properties					
		Effective	Efficient	Scalable-H	Simple	Flexible	Scalable-P
GPBO (Snoek et al., 2012)	BB	事	~~JΓ~	X	-X	X	X
BOHB (Falkner et al., 2018)	BB	事	帛	X	X	X	X
HOAG (Pedregosa, 2016)	G	X	X	X	X	X	X
RMD (Maclaurin et al., 2015)	G	X	X	X	X	X	X
RFHO (Franceschi et al., 2017; 2018)	G	X	X	X	X	X	X
HOZOG	BB+G	X	X	X	X	X	X
local/global minima. For modern hyperparameter tuning tasks, black-box optimization (Snoek et al.,
2012; Falkner et al., 2018) and gradient-based algorithms (Maclaurin et al., 2015; Franceschi et al.,
2018; 2017) are currently the dominant approaches due to the advantages in terms of effectiveness,
efficiency, scalability, simplicity and flexibility which are abbreviated as E2S2F in this paper. We
provide a brief review of representative black-box optimization and gradient-based hyperparameter
optimization algorithms in §2.1, and a detailed comparison of them in terms of the above properties
in Table 1.
Table 1 clearly shows that black-box optimization and gradient-based approaches have totally distinct
advantages, i.e., black-box optimization approach is simple, flexible and salable in term of model pa-
rameters, while gradient-based approach is effective, efficient and scalable in term of hyperparmeters.
Each property of E2S2F is an important criterion to a successful hyperparameter optimization method.
To the best of our knowledge, there is still no algorithm satisfying all the five properties simultane-
ously. Designing a hyperparameter optimization method having the benefits of both approaches is
still an open problem.
To address this challenging problem, in this paper, we propose a new hyperparameter optimization
method with zeroth-order hyper-gradients (HOZOG). Specifically, we first exactly formulate hyper-
parameter optimization as an A-based constrained optimization problem, where A is a black-box
optimization algorithm (such as the deep neural network). Then, we use the average zeroth-order
hyper-gradients to update hyperparameters. We provide the feasibility analysis of using HOZOG to
achieve hyperparameter optimization. Finally, the experimental results of various hyperparameter
(the size is from 1 to 1250) optimization problems demonstrate the benefits of HOZOG in terms of
E2S2F compared with the state-of-the-art hyperparameter optimization methods.
2	Hyperparameter Optimization based on Zeroth- Order
Hyper-Gradients
In this section, we first give a brief review of black-box optimization and gradient-based algorithms,
and then provide our HOZOG algorithm. Finally, we provide the feasibility analysis of HOZOG.
2.1	B rief Review of Black-Box Optimization and Gradient-based Algorithms
Black-box optimization algorithms: Black-box optimization algorithms view the bilevel optimiza-
tion problem f as a black-box function. Existing black-box optimization methods (Snoek et al.,
2012; Falkner et al., 2018) mainly employ Bayesian optimization (Brochu et al., 2010) to solve (1).
Black-box optimization approach has good simplicity and flexibility. However, a lot of references
have pointed out that it can only handle hyperparmeters from a few to several dozens (Falkner et al.,
2018) while the number of hyperparmeters in real hyperparameter optimization problems would
range from hundreds to thousands. Thus, black-box optimization approach has weak scalability in
term of the size of of hyperparmeters.
Gradient-based algorithms: The existing gradient-based algorithms can be divided into two parts
(i.e., inexact gradients and exact gradients). The approach of inexact gradients first solves the inner
problem approximately, and then estimates the gradient of (1) based on the approximate solution
by the approach of implicit differentiation (Pedregosa, 2016). Because the implicit differentiation
involves Hessian matrices of sizes of d × d and d × p where p d, they have poor scalability. The
2
Under review as a conference paper at ICLR 2021
approach of exact gradients3 treats the inner level problem as a dynamic system, and use chain rule
(Rudin et al., 1964) to compute the gradient. Because the chain rule highly depends on specific
learning algorithms, this approach has poor flexibility and simplicity. Computing the gradients
involves Hessian matrices of sizes of p × p and d × p. Thus, the approach of exact gradients has
better scalability than the approach of inexact gradients because normally we have p d.
* Enlightenment: As introduced in (Nesterov & Spokoiny, 2017; Gu et al., 2018), zeroth-order
gradient (also known as finite difference approximation (Cui et al., 2017)) technique is a black-box
optimization method which estimates the gradient only by two function evaluations. Thus, zeroth-
order gradient technique belongs both to black-box optimization and gradient-based optimization.
We hope that the hyperparameter optimization method bases on zeroth-order hyper-gradients4 can
inherit all benefits as described in Table 1.
2.2	HOZOG Algorithm
I Principle: Instead of directly computing the hyper-gradient as in (Pedregosa, 2016; Maclaurin
et al., 2015; Franceschi et al., 2017; 2018), we use two function evaluations (i.e., the zeroth-order
hyper-gradient technique (Nesterov & Spokoiny, 2017; Gu et al., 2018)) to estimate the hyper-gradient,
and update hyperparameters with hyper-gradients which derives our HOZOG algorithm.
Before presenting HOZOG algorithm in detail, we first clarify what problem we are solving exactly.
I What problem we are solving exactly? As mentioned in (1), the inner level problem in the
traditional hyperparameter tuning is finding the model parameters that minimize the inner objective
L, (i.e., w(λ) ∈ arg minw∈Rd L(w, λ)). However, in the real-world hyperparameter tuning problems,
we are usually trying to find an approximate minimum solution of L by an optimization algorithm if
the inner level problem L in convex. If the inner level problem L in non-convex, we usually try to
find an approximate local solution or a stationary point. Thus, we replace the inner level problem by
w(λ) = A(λ) where A is an optimization algorithm which approximately solves the inner objective
L. Further, we replace the bi-level optimization problem (1) by the following A-based constrained
optimization problem (2).
min f(λ) = E(w(λ), λ),	s.t. w(λ) = A(λ)	(2)
where w(λ) are the values returned by the optimization algorithm A.
• Hyperparameters: Hyperparameters can be divided into two types, i.e., problem-based hyperparam-
eters and algorithm-based hyperparameters.
1.	Problem-based hyperparameters: The problem-based hyperparameters are the hyperparame-
ters involved in learning problems such as the regularization parameter and the architectural
hyperparameters in deep neural networks.
2.	Algorithm-based hyperparameters: These are the hyperparameters involved in optimization
algorithms such as the learning rate, momentum and dropout rates.
The traditional bi-level optimization problem (1) can only formulate the problem-based hyperpa-
rameters. However, our A-based constrained optimization problem (2) can formulate both types of
hyperparameters.
I Algorithm: To solve the A-based constrained optimization problem (2), we propose HOZOG
algorithm in Algorithm 1, where the “for” loop is referred to as “meta-iteration”. We describe the
two key operations of Algorithm 1 (i.e., estimating the function value and average zeroth-order
hyper-gradient) in detail as follows.
•	Estimating the function value: We treat the optimization algorithm A as a black-box oracle. Given
hyperparameters λ, the black-box oracle A returns model parameters w(λ). Based on the pair of λ
and w(λ), the function value can be estimated as E(w(λ), λ).
•	Computing the average zeroth-order hyper-gradient: Zeroth-order hyper-gradient can be computed
as ▽ f(λ) = μ (f(λ + μu) - f (λ)) U based on the two function evaluations f(λ + μu) and f (λ),
3Although the inner-problem is usually solved approximately e.g. by taking a finite number of steps of
gradient descent, we still call this kind of methods as exact gradients throughout this paper to avoid using too
complex terminology.
4We call the gradient w.r.t. hyperparameter as hyper-gradient in this paper.
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Hyperparameter optimization method with zeroth-order hyper-gradients (HOZOG)
Input: Learning rate Y, approximate parameter μ, size of directions q and black-box inner solver A.
1:	Initialize λ0 ∈ Rp .
2:	fort = 0, 1,2,. . . ,T - 1 do
3:	Generate U = [uι,..., Uq], where Ui 〜 N(0; Ip).
4:	Compute the average zeroth-order	hyper-gradient Vf (λt)	=
μPq Pq=ι (f (λt + μui) — f (λt)) Ui, where f (λt) is estimated based on the solution
returned by the black-box inner solver A.
5:	Update λt+ι J λt — γVf(λt).
6:	end for
Output: λτ.
where U 〜N(0, Ip) is a random direction drawn from a uniform distribution over a unit sphere, and
μ is an approximate parameter. Vf (λ) has a large variance due to single direction u. To reduce the
variance, we use the average zeroth-order hyper-gradient (3) by sampling a set of directions {Ui}iq=1.
q
Vf(λ) = p- E(f(λ + μUi) — f(λ)) Ui	(3)
μq i=1
Based on the average zeroth-order hyper-gradient Vf (λ), we update the hyperparameters as follows.
λ 一 λ — YV f (λ)	(4)
Note that Vf (λ) is a biased approximation to the true gradient Vf (λ). Its bias can be reduced by
decreasing the value of μ. However, in a practical system, μ could not be too small, because in that
case the function difference could be dominated by the system noise (or error) and fails to represent
the function differential (Lian et al., 2016).
• Parallel acceleration. Because the average zeroth-order hyper-gradient involves q + 1 function
evaluations as shown in (3), we can use GPU or multiple cores to compute the q + 1 function
evaluations in parallel to accelerate the computation of average zeroth-order hyper-gradients.
2.3 Feasibility Analysis
I Challenge: In treating the optimization algorithm A(λ) as a black-box oracle that maps λ to w,
the most important problem is whether the mapping function A(λ) is continuous which is the basis
of using the zeroth-order hyper-gradient technique to optimize (2).
• Continuity: Before discussing the continuity of the A-based constrained optimization problem
f (λ), we first give the definitions of iterative algorithm and continuous function in Definitions 1 and
2 respectively.
Definition 1 (Iterative algorithm). Assume the optimization algorithm A(λ) can be formulated as
a nested function as A(λ) = wT and wt = Φt(wt-1, λ) for t = 1, . . . , T, where T is the number
of iterations, w0 is an initial solution, and, for every t ∈ {1, . . . , T}, Φt : (Rd × Rp) → Rd is
a mapping function that represents the operation performed by the t-th step of the optimization
algorithm. We call the optimization algorithm A(λ) as an iterative algorithm.
Definition 2 (Continuous function). For all λ ∈ Rp, if the limit of f(λ + δ) as δ ∈ Rp approaches 0
exists and is equal to f (λ), we call the function f(λ) is continuous everywhere.
Based on Definitions 1 and 2, we give Theorem 1 to show that the A-based constrained optimization
problem f(λ) is continuous under mild assumptions. The proof is provided in Appendix.
Theorem 1. If the hyperparameters λ are continuous and the mapping functions Φt(wt-1, λ) (for
every t ∈ {1, . . . , T }) are continuous, the mapping function A(λ) is continuous, and the outer
objective E is continuous, we have that the A-based constrained optimization problem f(λ) is
continuous w.r.t. λ.
We provide several popular types of optimization algorithms to show that almost existing iterative
algorithms are continuous mapping functions which would make f (λ) continious.
1. Gradient descent algorithms: If A is a gradient descent algorithm (such as SGD (Ghadimi &
Lan, 2013), SVRG (Reddi et al., 2016; Allen-Zhu & Hazan, 2016), SAGA (Defazio et al., 2014),
4
Under review as a conference paper at ICLR 2021
SPIDER (Fang et al., 2018)), the updating rules can be formulated as W J W - Y0v, where V is
a stochastic or deterministic gradient estimated by the current w, and γ0 is the learning rate. To
accelerate the training of deep neural networks, multiple adaptive variants of SGD (e.g., Adagrad,
RMSProp and Adam (Goodfellow et al., 2016)) have emerged.
2. Proximal gradient descent algorithms: If A is a proximal gradient descent algorithm (Zhao
et al., 2014; Xiao & Zhang, 2014; Gu & Huo, 2018), the updating rules should be the form of
W J Prox(W - γ0v), where Prox is a proximal operator (such as the soft-thresholding operator
for Lasso (Tibshirani, 1996)) which is normally continuous (Bredies & Lorenz, 2007; Zou, 2006).
It is easy to verify that the mapping functions A(λ) corresponding to these iterative algorithms are
continuous according to Theorem 1.
For a continuous function f (λ), there exists a Lipschitz constant L (see Definition 3) which upper
bounds f(λλ)-fλ2), ∀λ1,λ2 ∈ Rp. Unfortunately, exactly calculating the LiPsChitz constant of
f (λ) is NP-hard problem (Virmaux & Scaman, 2018). We provide an upper bound5 to the Lipschitz
constant of f(λ) in Theorem 2.
Definition 3 (Lipschitz continuous constant). For a continuous function f (λ), there exists a constant
L such that, ∀λ1, λ2 ∈ Rp, we have kf(λ1) - f(λ2)k ≤ Lkλ1 - λ2 k. The smallest L for which the
inequality is true is called the Lipschitz constant of f (λ).
Theorem 2. Given the continuous mapping functions Φt(Wt-1, λ) where t ∈ {1, . . . , T}), At
dφ∂Wt-1,λ)， Bt = dφ"WT ,λ' , Given the continuous objective function E(WT,λ), AT+1
—∂wT，) and BT +1 =	"『)∙ Let LAt = supλ∈RP,w∈Rd k At+1 k2, LBt = supλ∈RP,w∈Rd IIBtk2 ∙
Let L(f) denote the Lipschitz constant of the continuous function f (λ), we can upper bound L(f) by
PtT=+11 LBt LAt+1 .
. . LAT+1,
I Conclusion: Because the A-based constrained optimization problem f(λ) is continuous, we
can use the zeroth-order hyper-gradient technique to optimize f(λ) (Nesterov & Spokoiny, 2017).
Nesterov & Spokoiny (2017) provided the convergence guarantee of zeroth-order hyper-gradient
method when f(λ) is Lipschitz continuous as defined in Definition 3.
3	Experiments
We conduct the hyperparameter optimization experiments on three representative learning problems
(i,e,, l2-regularized logistic regression, deep neural networks (DNN) and data hyper-cleaning), whose
sizes of hyperparameters are from 1 to 1250. We also test the parameter sensitivity analysis of
HOZOG under different settings of parameters q, μ and γ, which are included in Appendix due to the
page limit. All the experiments are conducted on a Linux system equipped with four NVIDIA Tesla
P40 graphic cards.
•	Compared algorithms: We compare our HOZOG with the representative hyperparameter
optimization approaches such as random search (RS) (Bergstra & Bengio, 2012), RFHO with forward
(FOR) or reverse (REV) gradients (Franceschi et al., 2017) 6, HOAG (Pedregosa, 2016)7, GPBO
Snoek et al. (2012) 8 and BOHB (Falkner et al., 2018) 9. Most of them are the representative black-box
optimization and gradient-based hyperparameter optimization algorithms as presented in Table 1. We
implement our HOZOG in Python10.
•	Evaluation criteria: We compare different algorithms with three criteria, i.e., ∣Vf (λ)∣∣2, sub-
optimality and test error, where “suboptimality” denotes f(λ) - f(λ) and f(λ) is the minimum
5Although the upper bound is related to T , our simulation results show that it does not grow exponentially
with T because LAt or LBt is not larger than one at most times.
6The code of RFHO is is available at https://github.com/lucfra/RFHO.
7The code of HOAG is available at https://github.com/fabianp/hoag.
8The code of GPBO is available at http://github.com/fmfn/BayesianOptimization/.
9The code of BOHB is available at https://github.com/automl/HpBandSter. Note that BOHB
is an improved version of Hyperband (Li et al., 2017). Thus, we do not compare HOZOG with Hyperband.
10We will release the code of HOZOG and the experiments after the paper is accepted.
5
Under review as a conference paper at ICLR 2021
2*103
W,
βxlβ,
*Xlβ,
Time (in
—HOTOG
——BOHB
—BO
—WMe
——Fon
——REV
RS
(a) News20
aχw∙
7.tβX10*
7.32 κ 10«
w ..30 49. so «
Time (∣n seconds；
一£■-8p
MXWs-
1.7Xlβ5-
W .. 15 29. 25 »
Time (∣n seconds)
♦ ， ∙ a
⅛-≡E-as3Λ
Time (in
(e) Real-sim
7.88X1«*
1.4X1
(b) Covtype	(c) Real-sim	(d) News20
W M .<«. 5« «0	W. . 12 U 1«	w .. IS B. ≈ 8	g 量..3⅛ Λ M «
T∣me(∣nseconds)	Time (in seconds)	Time (in seconds)	Time (in seconds)
(f) CoVtyPe	(g) News20	(h) Real-Sim	(i) CoVtyPe
Figure 1: Comparison of different hyperparameter optimization algorithms for l2-regularized logistic
regression sharing the same legend. (a)-(c): Test error. (d)-(f): Suboptimality. (g)-(i): kVf (λ)k2.
(Larger figures can be found in the suPPlement material.)
Value of f(λ) for all λ which haVe been exPlored, and test error is the aVerage loss on the testing set.
Note the hyPer-gradients Vf (λ) for all method excePt for FOR and REV are comPuted by Eq. (3).


• Datasets: The datasets used in exPer-
iments are News20, CoVtyPe, Real-sim,
CIFAR-10 and Mnist datasets from
LIBSVM rePository, which is aVailable at
https://www.csie.ntu.edu.tw/
~cjlin∕libsvmtools∕datasets∕.
EsPecially, for News20 and Mnist two
multi-class datasets, we transform them to
binary classification Problems by randomly
Partitioning the data into two grouPs.
Experiment	#HP	Dataset	q	μ	Y
		News20	1	0.01	0.05
l2-regularized logistic regression	1	Covtype	1	0.01	0.03
		Real-sim	1	0.01	0.005
2-layer CNN	-100-		1	丁	-~
Deep Neural Networks VGG-16	20	CIFAR-10	3	1	1
ResNet-152	10		3	1	1
Data hyper-cleaning	500/1250	Mnist	5	丁	~~
Table 2: The Parameter settings of HOZOG in the ex-
Periments. (“# HP” is the abbreViation of the number of
hyPerParameters.)
•	Parameters of HOZOG: The values of parameters q, μ and Y in HOZOG are given in Table 2.
EsPecially, q Plays an imPortant role to HOZOG because it determines the accuracy and the running
time of estimating the gradients. We empirically observe that q ≤ 5 has a good balance between the
two objectives.
3.1	l2 -REGULARIZED LOGISTIC REGRESSION
Experimental setup: We consider to estimate the regularization parameter in the l2-regularized
logistic regression model. We split one data set into three subsets (i.e., the train set Dtr, validation
set Dval and test set Dt) with a ratio of 2:1:1. We use the logistic loss l(t) = log(1 + e-t) as the
loss function. The hyperparameter optimization problem for l2-regularized logistic regression is
formulated as follows.
arg min	l(yihxi, w(λ)i),	s.t. w(λ) ∈ arg min	l(yihxi, w(λ)i) + eλkwk2 (5)
λ∈[-10,10] i∈Dval	w∈Rd i∈Dtr
The solver used for solving the inner objective is L-BFGS11 (Liu & Nocedal, 1989) for HOAG and
Adam (Kingma & Ba, 2014) for the others.
Results and discussions: Figure 1 presents the convergence results of suboptimality, kVf (λ)k2 and
test error vs. the running time for different methods. Note that we take same initial values of λ and
w for all gradient-based methods, while the black-box methods naturally start from different points.
Because HOAG works with tolerances and warm start strategy, HOAG has a fast convergence at
the early stage but a slow convergence at the late stage as shown in Figures 1g-1i. We observe that
HOZOG runs faster than other gradient-based methods. This is because that FOR and REV need
much time to compute hyper-gradients. Figures 1g-1i provide kVf (λ)k2 of different methods as
functions of running time. We can see that the black-box methods (i.e., BOHB and GPBO) spend
much time on exploring because kVf (λ)k2 of these methods didn’t strictly go down in the early
stage. Overall, all the results show that HOZOG has a faster convergence than other methods.
3.2	Deep Neural Networks
Experimental setup: We validate the advantages of HOZOG on optimizing learning rates of DNN
which is much more complicated in both structure and training compared to l2-regularized logistic
regression.
11The implementation is available at https://github.com/fabianp/hoag.
6
Under review as a conference paper at ICLR 2021
-R∙,18F
-R∙,18F
-RS
-G阳。
-BOHB
一 HOZOG
5¾> ,?«» 15000 JOOO
Tlmetln seconds)
(C) ReSNet-152	(d) 2-layer CNN (e) VGG-16
(a) 2-layer CNN (b) VGG-16
t=eE-=oqns

(f) ResNet-152 (g) 2-layer CNN (h) VGG-16 (i) ResNet-152
Figure 2: CompariSon of different hyperparameter optimization algorithmS for 2-layer CNN, VGG-
16 and ReSNet-152 Sharing the Same legend. (a)-(C): TeSt error. (d)-(f): Suboptimality. (g)-(i):
∣∣Vf (λ) ∣∣2. (Larger figures can be found in the supplement material.)
SpeCifiCally, the training of modern DNN iS uSually an intriguing proCeSS, involving multiple heuriStiC
hyperparameter schedules, e.g. learning rate with exponential weight decay. Instead of intuitive
settings, we propose to apply epoch-wise learning rates and jointly optimize these hyperparameters.
The experiments are conducted on CIFAR-10 dataset with 50,000 samples. To demonstrate the
scalability of HOZOG, three deep neural networks with various structure are used, including (1)
two layers DNN (2-layer CNN) with convolutional, max pooling, and normalizing layers; (2) VGG-
16 (Simonyan & Zisserman, 2014), (3) ResNet-152 (He et al., 2016). The initialization of inner
problem is randomized for different meta-iterations to avoid the potential dependence on the quirks
of particular settings. In detail, for all experiments we apply 50 meta-iterations and optimize inner
problems using stochastic gradient descent, with batch size of 256. On CNN, 100 epochs for inner
problem are used, which indicates 100 hyperparameters are involved. On VGG-16, the original model
takes 224 × 224 images as inputs, and we adjust the size of the first fully-connected layer from 7 × 7
convolution to 1 × 1 to fit CIFAR-10 inputs. Here 20 epochs for inner are used. On ResNet-152,
similar processing is exploited and the inner epoch is 10.
Results and discussions: The results are summarized in Figure 2. The experimental results show
that the learning rates computed by HOZOG achieve the lowest test error and the fastest descending
speed compared to baselines on all tasks. Moreover, the proposed method requires much less time to
attain the best hyperparameters, and tends to have smaller variances in gradients. It is noteworthy that,
some state-of-the-art hyperparameter optimization approaches (including HOAG, REV and FOR)
are missing in this setting, due to the algorithms of REV and FOR are limited to smooth functions
and the implementation of HOAG is limited to the hyperparameter optimization problems with a
small number of hyperparamters. However, these difficulties are avoided by our HOZOG, which also
demonstrates the flexibility of HOZOG. Moreover, as a brutal search method, the performance of RS
is very unstable, which can be identified from the hyper-gradients. For BO and BOHB, the instability
also exists, potentially due to the highly complexity of the network structure. Another noteworthy
problem with respect to BO and BOHB is the computational overhead in sampling, which make the
meta-iteration extremely time consuming, compared to other methods.
We observe that the difficulty of this problem mainly comes from model complexity, instead of
hyper-parameter numbers. For CNN with 100 hyper-parameters, HOZOG shows advantages in both
time and suboptimality, although baselines can also efficiently find a reasonable solution. For VGG-16
and ResNet-152, we notice that though the size of hyperparameters is reduced, it takes baselines
longer time to find acceptable results. Instead, HOZOG still shows fast convergence empirically. This
observation indicates that HOZOG is potentially more suitable for hyperparameter optimization in
large DNN.
3.3	Data Hyper-Cleaning
Experimental setup: We evaluate HOZOG on tuning the hyperparameters of data hyper-cleaning
task. Compared with the preceding problems, the data cleaning task is more challenging, since it has
more hyperparameters (hundreds or even thousands).
Assuming that we have a label noise dataset, with only limited clean data provided. The data hyper-
cleaning task is to allocate a hyperparameter weight λi to a certain data point or a group of data
points to counteract the influence of noisy samples. We split a certain data set into three subsets: Dtr
7
Under review as a conference paper at ICLR 2021
(a) 500 HP (b) 1250 HP (c) 500 HP (d) 1250 HP (e) 500 HP (f) 1250 HP
Figure 3: Comparison of different hyperparameter optimization algorithms for data hyper-cleaning
sharing the same legend, where “HP” is the abbreviation of hyperparameters (a)-(b): Suboptimality.
(c)-(d): ∣Nf (λ)k2. (e)-(f): Test error. (Larger figures can be found in the supplement material.)
of Ntr training samples, Dval of Nval validation samples and a test set Dt containing the Nt samples.
We set random labels to10.5 * Ntre training examples, and select a random subset Df from Dtr.
Similar to Franceschi et al. (2017), we considered a plain softmax regression model with parameters
W (weights) and b (bias). The error of a model (W, b) on an example (x, y) was evaluated by using
the cross-entropy l(W, b, (x, y)) both in the training objective function, L, and in the validation one, E.
We added in L an hyperparameter vector λ ∈ RNh that weights each group of examples in the train-
ing phase through sigmoid function, i.e. L(W, b)=看 PggEQ PiEg Sigmoid(λg)l(W, b, (xi, yi)),
where G contain Nh groups random select from Dtr . Thus, we have the hyperparameter optimization
problem as follows.
arg min E (W (λ), b(λ)), s.t. [W (λ), b(λ)] ≈ arg min L(W, b)	(6)
λ∈RNh	W,b
We instance two subset dataset for the MNIST dataset, with Ntr = 5000, Nval = 5000, Nt = 10000,
Nh = 1250 and Ntr = 1000, Nval = 1000, Nt = 4000, Nh = 500. We use a standard gradient
descent method for the inner problem with fixed learning rate 0.05 and 4000 iteration. RS is used as
baseline method, and BOHB and REV are used as comparison.
Results and discussions: Figure 3 presents the results of HOZOG, BOHB, REV and RS for data
hyper-cleaning. Note that the methods of GPBO, FOR and HOAG are missing here, because the
hyperparameter size is beyond the capability of their implementations. The results show that HOZOG
can beat RS and BOHB easily, while not perform completely as good as REV in the long run.
This is because REV is an exact gradient method whose convergence rate is faster than the one of
zeroth-order gradient method (i.e., HOZOG) by a constant whose value is depending on p (Nesterov
& Spokoiny, 2017). However, computing the exact gradients in REV is costly. Specifically, REV
takes about 40 seconds to finish the computation of one hyper-gradient under the setting of 1250
hyperparameters, which is only about 24 seconds for HOZOG. This is the reason why our method
converges faster than REV in the early stage of training. Importantly, the application scenarios of
REV are limited to smooth functions, e.g., not suitable for the experimental settings of convolutional
neural networks and deeper neural networks. However, our HOZOG can be utilized to a broader class
of functions (i.e., continuous functions).
3.4	Discussion: Importance of HOZOG
The experimental results show that the black-box optimization methods have a weak performance for
the high-dimensional hyperparameter optimization problems which is also verified in a large number
of existing references (Brochu et al., 2010; Snoek et al., 2012), while they have the advantages of
simplicity and flexibility. On the other hand, the existing gradient-based methods (Franceschi et al.,
2017; 2018) need experienced researchers to provide a customized program against the optimization
algorithm and sometime it would fail, while they have the advantages of scalability and efficiency.
HOZOG inherits all the benefits from both approaches in that, the gradients are computed in a black-
box manner, while the hyperparameter search is accomplished via gradient descent. Especially, for
high-dimensional hyperparameter optimization problems which have no customized RFHO algorithm,
HOZOG currently is the only choice for this kind of problems to the best of our knowledge.
4 Conclusion
Effectiveness, efficiency, scalability, simplicity and flexibility (i.e., E2S2F) are important evaluation
criteria for hyperparameter optimization methods. In this paper, we proposed a new hyperparameter
optimization paradigm with zeroth-order hyper-gradients (HOZOG) which is the first method having
all these benefits to the best of our knowledge. We proved the feasibility of using HOZOG to achieve
hyperparameter optimization under the condition of Lipschitz continuity. The experimental results on
three representative hyperparameter (the size is from 1 to 1250) optimization tasks not only verify
the result in the feasibility analysis, but also demonstrate the benefits of HOZOG in terms of E2S2F,
compared with the state-of-the-art hyperparameter optimization methods.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International conference on machine learning, pp. 699-707, 2016. 4
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research, 13(Feb):281-305, 2012. 1, 5
Kristian Bredies and Dirk A Lorenz. Iterative soft-thresholding converges linearly. arXiv preprint
arXiv:0709.1598, 2007. 5
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010. 2, 8
Jianbo Cui, Jialin Hong, and Zhihui Liu. Strong convergence rate of finite difference approxi-
mations for stochastic cubic Schrodinger equations ?? Journal OfDifferential Equations, 263:
S002203961730253X, 2017. 3
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014. 4
Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter opti-
mization at scale. In International Conference on Machine Learning, pp. 1436-1445, 2018. 2,
5
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti-
mization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018. 5
Herbert Federer. Geometric measure theory. Springer, 2014. 11
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 1165-1173. JMLR. org, 2017. 2, 3, 5, 8
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference on
Machine Learning, pp. 1563-1572, 2018. 2, 3, 8
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013. 4
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. 5
Bin Gu and Zhouyuan Huo. Asynchronous doubly stochastic group regularized learning. In
International Conference on Artificial Intelligence and Statistics (AISTATS 2018), 2018. 5
Bin Gu and Charles Ling. Anew generalized error path algorithm for model selection. In International
Conference on Machine Learning, pp. 2549-2558, 2015. 1
Bin Gu, Zhouyuan Huo, Cheng Deng, and Heng Huang. Faster derivative-free stochastic algorithm
for shared memory machines. In International Conference on Machine Learning, pp. 1807-1816,
2018. 3
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016. 7
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014. 6
9
Under review as a conference paper at ICLR 2021
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
Honal neural networks. In Advances in neural information processing Systems, pp. 1097-1105,
2012. 1
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning
Research, 18(1):6765-6816, 2017. 5
Xiangru Lian, Huan Zhang, Cho-Jui Hsieh, Yijun Huang, and Ji Liu. A comprehensive linear speedup
analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order. In
Advances in Neural Information Processing Systems, pp. 3054-3062, 2016. 4
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989. 6
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization
through reversible learning. In International Conference on Machine Learning, pp. 2113-2122,
2015. 2, 3
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527-566, 2017. 3, 5, 8
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International
Conference on Machine Learning, pp. 737-746, 2016. 2, 3, 5
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp.
314-323, 2016. 4
Walter Rudin. Principles of mathematical analysis. 1976. 12
Walter Rudin et al. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1964. 3
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014. 7
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
2, 5, 8
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267-288, 1996. 5
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media, 2013.
1
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Advances in Neural Information Processing Systems, pp. 3835-3844, 2018.
5
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.
SIAM Journal on Optimization, 24(4):2057-2075, 2014. 5
Tuo Zhao, Mo Yu, Yiming Wang, Raman Arora, and Han Liu. Accelerated mini-batch randomized
block coordinate descent method. In Advances in neural information processing systems, pp.
3329-3337, 2014. 5
Hui Zou. The adaptive lasso and its oracle properties. Journal of the American statistical association,
101(476):1418-1429, 2006. 5
10