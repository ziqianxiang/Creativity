Under review as a conference paper at ICLR 2021
Double Generative Adversarial Networks
for Conditional Independence Testing
Anonymous authors
Paper under double-blind review
Ab stract
In this article, we consider the problem of high-dimensional conditional indepen-
dence testing, which is a key building block in statistics and machine learning. We
propose a double generative adversarial networks (GANs)-based inference proce-
dure. We first introduce a double GANs framework to learn two generators, and
integrate the two generators to construct a doubly-robust test statistic. We next
consider multiple generalized covariance measures, and take their maximum as
our test statistic. Finally, we obtain the empirical distribution of our test statistic
through multiplier bootstrap. We show that our test controls type-I error, while the
power approaches one asymptotically. More importantly, these theoretical guar-
antees are obtained under much weaker and practically more feasible conditions
compared to existing tests. We demonstrate the efficacy of our test through both
synthetic and real data examples.
1	Introduction
Conditional independence (CI) is a fundamental concept in statistics and machine learning. Testing
conditional independence is a key building block and plays a central role in a wide variety of sta-
tistical learning problems, for instance, causal inference (Pearl, 2009), graphical models (Koller &
Friedman, 2009), dimension reduction (Li, 2018), among others. In this article, we aim at testing
whether two random variables X and Y are conditionally independent given a set of confounding
variables Z. That is, we test the hypotheses:
H0 : X ⊥ Y | Z versus	H1 : X 6⊥ Y | Z,
(1)
given the observed data of n i.i.d. copies {(Xi, Yi, Zi)}1≤i≤n of (X, Y, Z). For our problem, X, Y
and Z can all be multivariate. However, the main challenge arises when the confounding set of
variables Z is high-dimensional. As such, we primarily focus on the scenario with a univariate X
andY , and a multivariate Z. Meanwhile, our proposed method can be extended to the multivariate X
and Y scenario as well. Another challenge is the limited sample size compared to the dimensionality
ofZ. As a result, many existing tests are ineffective, with either an inflated type-I error, or not having
enough power to detect the alternatives. See Section 2 for a detailed review.
We propose a double generative adversarial networks (GANs, Goodfellow et al., 2014)-based infer-
ence procedure for the CI testing problem (1). Our proposal involves two key components, a double
GANs framework to learn two generators that approximate the conditional distribution of X given
Z and Y given Z , and a maximum of generalized covariance measures of multiple combinations of
the transformation functions of X and Y . We first establish that our test statistic is doubly-robust,
which offers additional protections against potential misspecification of the conditional distributions
(see Theorems 1 and 2). Second, we show the resulting test achieves a valid control of the type-I er-
ror asymptotically, and more importantly, under the conditions that are much weaker and practically
more feasible (see Theorem 3). Finally, we prove the power of our test approaches one asymptoti-
cally (see Theorem 4), and demonstrate it is more powerful than the competing tests empirically.
2	Related works
There has been a growing literature on conditional independence testing in recent years; see (Li &
Fan, 2019) for a review. Broadly speaking, the existing testing methods can be cast into four main
categories, the metric-based tests, e.g., (Su & White, 2007; 2014; Wang et al., 2015), the conditional
randomization-based tests (Candes et al., 2018; Bellot & van der Schaar, 2019), the kernel-based
1
Under review as a conference paper at ICLR 2021
tests (Fukumizu et al., 2008; Zhang et al., 2011), and the regression-based tests (Hoyer et al., 2009;
Zhang et al., 2018; Shah & Peters, 2018). There are other types of tests, e.g., Bergsma (2004); Doran
et al. (2014); Sen et al. (2017; 2018); Berrett et al. (2019), to mention a few.
The metric-based tests typically employ some kernel smoothers to estimate the conditional charac-
teristic function or the distribution function of Y given X and Z. Kernel smoothers, however, are
known to suffer from the curse of dimensionality, and as such, these tests are not suitable when the
dimension of Z is high. The conditional randomization-based tests require the knowledge of the
conditional distribution of X|Z (Candes et al., 2018). If unknown, the type-I error rates of these
tests rely critically on the quality of the approximation of this conditional distribution. Kernel-based
test is built upon the notion of maximum mean discrepancy (MMD, Gretton et al., 2012), and could
have inflated type-I errors. The regression-based tests have valid type-I error control, but may suffer
from inadequate power. Next, we discuss in detail the conditional randomization-based tests, in
particular, the work of Bellot & van der Schaar (2019), the regression-based and the MMD-based
tests, since our proposal is closely related to them.
2.1	Conditional randomization-based tests
The family of conditional randomization-based tests is built upon the following basis. If the condi-
tional distribution Pχ∣z of X given Z is known, then one can independently draw XiD 〜Pχ∣z=Zi
for i = 1, . . . , n, and these samples are independent of the observed samples Xi’s and Yi ’s. Write
X = (X1,...,Xn)>,X(1) = (X1(1),...,Xn(1))>,Y = (Y1,...,Yn)>,andZ = (Z1,...,Zn)>.
Here we use boldface letters to denote data matrices that consist of n samples. The joint distri-
butions of (X, Y , Z) and (X(1), Y , Z) are the same under H0. Any large difference between
the two distributions can be interpreted as the evidence against H0 . Therefore, one can repeat
the process M times, and generate Xlm) 〜 Pχ∣z=Zi, i = 1,...,n, m = 1,...,M. Write
X(m) = (X1(m), . . . , Xn(m))>. Then, for any given test statistic ρ = ρ(X, Y , Z), its associated
p-value is P = [1 + PM= I{ρ(X(m), Y, Z) ≥ P(X, Y, Z)}]∕(1 + M), where I(∙) is the indi-
cator function. Since the triplets (X, Y, Z), (X(1), Y, Z), . . . , (X(M), Y, Z) are exchangeable
under Ho, thep-value is valid, and it satisfies that Pr(P ≤ α∣Ho) ≤ α + o(1) for any 0 < α < 1.
In practice, however, PX∣z is rarely known, and Bellot & van der Schaar (2019) proposed to approx-
imate it using GANs. Specifically, they learned a generator GX(∙, ∙) from the observed data, then
took Zi and a noise variable vi(,mχ) as input to obtain a sample Xei(m), which minimizes the divergence
between the distributions of (Xi, Zi) and (Xei(m), Zi). The p-value is then computed by replacing
X(m) by Xf(m) = (Xe1(m), . . . , Xen(m))>. They called this test GCIT, short for generative conditional
independence test. By Theorem 1 of Bellot & van der Schaar (2019), the excess type-I error of this
test is upper bounded by
Pr(p ≤ α∣Ho) - α ≤ EdTV(PXIZ,Pχ∣z)= ESUp ∣Pr(X ∈ A|Z) - Pr(f(m) ∈ A|Z)| ≡ D, (2)
A
where dTV is the total variation norm between two probability distributions, the supremum is taken
over all measurable sets, and the expectations in (2) are taken with respect to Z.
By definition, the quantity D on the right-hand-side of (2) measures the quality of the conditional
distribution approximation. Bellot & van der Schaar (2019) argued that this error term is negligible
due to the capacity of deep neural nets in estimating conditional distributions. To the contrary, we
find this approximation error is usually not negligible, and consequently, it may inflate the type-I
error and invalidate the test. We consider a simple example to further elaborate this.
Example 1.	Suppose X is one-dimensional, and follows a simple linear regression model, X =
Z>βo + ε, where the error ε is independent of Z and ε 〜 N(0,σ0) for some σ2 > 0.
Suppose we know a priori that the linear regression model holds. We thus estimate β0 by ordinary
least squares, and denote the resulting estimator by βb. For simplicity, suppose σ02 is known too. For
this simple example, we have the following result regarding the approximation error term D.
Proposition 1 Suppose the linear regression model holds. The derived distribution PeX|Z is
N(Zβb, σ02In), where In is the n × n identity matrix. Then D is not o(1).
2
Under review as a conference paper at ICLR 2021
To facilitate the understanding of the convergence behavior of D, we sketch a few lines of the proof
of Proposition 1. A detailed proof is given in Appendix F.1. Let PX|Z=Zi denote the conditional
distribution of Xei(m) given Zi, which is N (Zi> βb, σ02) in this example. If D = o(1), then,
De ≡ n1∕2JEdTV(PX∣Z=Zi,PX∣Z=Zi) = O(I).	(3)
In other words, the validity of GCIT requires the root mean squared total variation distance in (3)
to converge at a faster rate than n-1/2 . However, this rate cannot be achieved in general. In our
simple Example 1, we have D ≥ c for some universal constant c > 0. Consequently, D in (2) is
not o(1). Proposition 1 shows that, even if we know a priori that the linear model holds, D is not
to decay to zero as n grows to infinity. In practice, we do not have such prior model information.
Then it would be even more difficult to estimate the conditional distribution PX|Z . Therefore, using
GANs to approximate PX |Z does not guarantee a negligible approximation error, nor the validity of
the test.
2.2 Regression-based tests
The family of regression-based tests is built upon akey quantity, the generalized covariance measure,
n
GCM(X,Y) = — X {Xi - E(Xi∣Zi)} {匕-E(K∣Zi)},
ni=1
1	ə / I Γ7∖	-I ə /-τ	Γ7∖	.ι	-ι∙	. -ι	t∙ .∙	τ-, / I Γ7∖	-ι τ-1 /t I Γ7∖	J ι
where E(X|Z) and E(Y|Z) are the predicted condition mean E(X|Z) and E(Y|Z), respectively,
by any supervised learner. When the prediction errors of E(X|Z) and E(Y|Z) satisfy certain con-
vergence rates, Shah & Peters (2018) proved that GCM is asymptotically normal. Under H0, the
asymptotic mean of GCM is zero, and its asymptotic standard deviation can be consistently esti-
mated by some standard error estimator, denoted by sb(GCM). Therefore, at level α, we reject H0,
if |GCM|/b(GCM) exceeds the upper ɑ∕2th quantile of a standard normal distribution.
Such a test is valid. However, it may not have sufficient power to detect H1. This is because the
asymptotic mean of GCM equals GCM* (X, Y) = E{X -E(X∣Z)}{Y -E(Y|Z)}. Theregression-
based tests require |GCM* | to be nonzero under Hi to have power. However, there is no guarantee
of this requirement. We again consider a simple example to elaborate.
Example 2.	Suppose X*, Y and Z are independent random variables. Besides, X* has mean zero,
and X = X*g(Y) for some function g.
For this example, we have E(X|Z) = E(X), since both X* and Y are independent of Z, and so is
X. Besides, E(X) = E(X*)E{g(Y)} = 0, since X* is independent ofY andE(X*) = 0. As such,
GCM* (X, Y) = E{X - E(X)}{Y - E(Y|Z)} = 0 for any function g. On the other hand, X and
Y are conditionally dependent given Z, as long as g is not a constant function. Therefore, for this
example, the regression-based tests would fail to discriminate between H0 and Hi .
2.3 MMD-based tests
The family of kernel-based tests often involves the notion of maximum mean discrepancy as a mea-
sure of independence. For any two probability measures P, Q and a function space F, define
MMD(P, Q|F) = SUPf∈f {Ef (Wi) - Ef (W2)} , Wi ~ P, W2 ~ Q.
Let Hi, H2 be some function spaces of X and Y. Define φχγ = MMD(PXY, Qχγ∣Hι 0 H2),
where 0 is the tensor product, PXY is thejoint distribution of (X, Y), and QXY is the conditionally
independent distribution with the same X and Y margins as PXY . Then following the calculations
in Appendix D, we have, φXY = suph1∈H1,h2∈H2 E[hi (X) -E{hi (X)|Z}][h2 (Y) -E{h2(Y)|Z}].
We see that φXY measures the average conditional association between X and Y given Z. Under
H0, it equals zero, and hence an estimator of this measure can be used as a test statistic for H0.
3	A new double GANs-based testing procedure
We propose a double GANs-based testing procedure for the conditional independence testing prob-
lem (1). Conceptually, our test integrates GCIT, regression-based and MMD-based tests. Mean-
while, our new test addresses the limitations of the existing ones. Unlike GCIT that only learned the
3
Under review as a conference paper at ICLR 2021
Figure 1: Illustration of the conditional independence test with double GANs.
conditional distribution ofX|Z, we learn two generators GX and GY to approximate the conditional
distributions of both X |Z and Y |Z. We then integrate the two generators in an appropriate way to
construct a doubly-robust test statistic, and we only require the root mean squared total variation
norm to converge at a rate of n-κ for some κ > 1/4. Such a requirement is much weaker and
practically more feasible than the condition in (3). The notion of doubly-robustness property comes
from the classical semiparametric theory in statistics (Tsiatis, 2007). Specifically, a doubly robust
procedure applies both types of models simultaneously, and produces a consistent estimateif either
of the two models has been consistently estimated.
Moreover, to improve the power of the test, we consider a set of the GCMs, {GCM(h1(X), h2(Y )) :
h1, h2}, for multiple combinations of transformation functions h1 (X) and h2(Y ). We then take the
maximum of all these GCMs as our test statistic. This essentially yields φXY , which is connected
with the notion of MMD. To see why the maximum-type statistic can enhance the power, we quickly
revisit Example 2. When g is not a constant function, there exists some nonlinear function h1 such
that h；(Y) = E{hι(X)|Y} is not a constant function of Y. Set h2 = h↑. We have GCM* =
E[hι{X*g(Y)}{Y - E(Y)}] = Var{h*(Y)} > 0. This enables Us to discriminate HIfrom H0.
We next detail our testing procedure. A graphical overview is given in Figure 1.
3.1	Test statistic
We begin with two function spaces,	Hi	=	{h1,θ1	:	θι	∈ Rd1 }	and H2	=	{h2,θ2	:	θ2	∈ Rd2},
indexed by some parameters θ1 and θ2, respectively. In our implementation, we set H1 and
H2 to the classes of neural networks with a single-hidden layer, finitely many hidden nodes,
and the sigmoid activation function. We then randomly generate B functions, h1,1, . . . , h1,B ∈
H1, h2,1, . . . , h2,B ∈ H2, where we independently generate i.i.d. multivariate normal variables
θ1,1,..., θi,B 〜N(0, 2Idι∕dι), and θ2,1,∙∙∙, θ2,B 〜N(0,2Id2/d2). We then set hi,b =%八,
and h2,b = h2,θ2,b, b = 1, . . . , B. Consider the following maximum-type test statistic,
max	σb-1
b1,b2∈{1,...,B} b1,b2
n
n X [hi,bι (Xi)- E{hi,bι (Xi)IZi}] [h2,b2 (Yi)- E{h2,b2 (Yi)IZi}]
i=1
(4)
where 欧 b2 is a consistent estimator for √nGCM(hι(X),h2(Y)). See Section A for definition. To
compute (4), however, we need to estimate the conditional means E{h1,b1 (X)IZ}, E{h2,b2 (Y)IZ}
for b1 , b2 = 1, ∙ ∙ ∙ , B . In theory, B should diverge to infinity to guarantee the power property of
the test. Separately applying supervised learning algorithms 2B times to compute these means is
computationally very expensive. Instead, we propose to implement this step based on the generators
GX and GY estimated using GANs, which is computationally much more efficient.
Specifically, for i = 1, ∙ ∙ ∙ , n, we randomly generate i.i.d. random vectors {vi(,mX)}mM=1, {vi(,mY)}mM=1
and output the pseudo samples Xei(m) = GX(Zi, vi(,mX)), Yei(m) = GY (Zi, vi(,mY)), for m =
1, ∙ ∙ ∙ , M , to approximate the conditional distributions of Xi and Yi given Zi . We then compute
Eb{h1,b1(Xei)IZi} = M-1PmM=1h1,b1(Xi(m)),andEb{h2,b2(Yi)IZi} = M -1 PmM=1 h2,b2 (Yei(m)),
for b1 , b2 = 1, ∙ ∙ ∙ , B . Plugging those estimated means into (4) produces our test statistic,
T ≡ maxb1,b2 n-1/2 Pin=1 ψb1,b2,i,where
1M
ψb1,b2,i = bb-,b2 jh1,bι (Xi)- ME h1,bι
4
Under review as a conference paper at ICLR 2021
Input: Number of functions B, number of pseudo samples M, and number of data splits L.
Step 1: Divide {1,..., n} into L folds I ⑴，...，I (L). Let I (-') = {1,..., n}-1 (').
Step 2: For ' = 1,... ,L, train two generators GX) and GY) based on {(Xi, Zi)}i∈ι(-') and
{(Yi, Zi)}i∈ι(-'), to approximate the conditional distributions of X|Z and Y|Z.
Step 3: For ' = 1,..., L and i ∈ l`, generate i.i.d. random noises Wm)}
Set Xy= GX)(Zi, v(,m)), and Y" = GY)(Zi, VW, m = 1,..., M.
Step 4: Randomly generate h1,1, . . . , h1,B ∈ H1 and h2,1, . . . , h2,B ∈ H2.
Step 5: Compute the test statistic T .
m=1
v(m)
, vi,Y
M
m=1
M
Algorithm 1: Algorithm for computing the test statistic.
To help reduce the type-I error of our test, we further employ a data splitting and cross-fitting strat-
egy, which is commonly used in statistical testing (Romano & DiCiccio, 2019). That is, we use
different subsets of data samples to learn GANs and to construct the test statistic. We summarize
our procedure of computing the test statistic in Algorithm 1.
3.2	B OOTSTRAPPING THE p-VALUE
Next, we propose a multiplier bootstrap method to approximate the distribution of √nT under Ho
to compute the corresponding p-value. The key observation is that ψb1 ,b2 = n-1/2 Pin=1 ψb1,
b2,i is
asymptotically normal with zero mean under H0 ; see the proof of Theorem 3 in Appendix F.3 for
details. As such, √nT = maxb1,b2 |n-1/2 Pn=ι Ψb1,b2,i∣ is to converge to a maximum of normal
variables in absolute values.
To approximate this limiting distribution, we first estimate the covariance matrix of a B2-
dimensional vector formed by {ψb1,b2}b1,b2 using the sample covariance matrix Σ. We then generate
i.i.d. random vectors with the covariance matrix equal to Σ, and compute the maximum elements
of each of these vectors in absolute values. Finally, we use these maximum absolute values to
approximate the distribution of T under the null. We summarize this procedure in Algorithm 2.
3.3	Approximating conditional distribution via GANs
We adopt the proposal in Genevay et al. (2017) to learn the conditional distributions PX|Z and PY|Z.
Recall that PX|Z is the distribution of pseudo outcome generated by the generator GX given Z. We
consider estimating PX|Z by optimizing minGX maxc Dc,(PX|Z, PX|Z), where Dc, denotes the
Sinkhorn loss function between two probability measures with respect to some cost function c and
some regularization parameter > 0. A detailed definition of Dc, is given in Appendix B. Intu-
itively, the closer the two probability measures, the smaller the Sinkhorn loss. As such, maximizing
the loss with respect to the cost function learns a discriminator that can better discriminate the sam-
ples generated between PX|Z and PX|Z. On the other hand, minimizing the maximum cost with
respect to the generator GX makes it closer to the true distribution PX|Z. This yields the minimax
Input: Number of bootstrap samples J, and {ψb1,b2,i}b1,b2,i.
Step 1: Compute a B2 × B2 matrix Σb whose {b1 + B(b2 - 1), b3 + B(b4 - 1)}th entry is given
by (n - 1)-1 Pin=1(ψb1,b2,i - ψb1,b2 )(ψb3,b4,i - ψb3,b4 ).
Step 2: Generate i.i.d. standard normal variables Zj,b for j = 1, . . . , J, b = 1, . . . , B2. Set
Zj = (Zj,1,...,Zj,B2)>, and Tj = ∣∣Σ1/2Zj ∣∣∞, where Σ1/2 is a positive semi-definite matrix
that satisfies Σ 1∕2Σ1/2 = Σ, and ∣∣ ∙ ∣∞ is the maximum element of a vector in absolute values.
Step 3: Compute the p-value, p = J-1 PjJ=1 I(T ≥ Tej).
Algorithm 2: Algorithm for computing the p-value.
5
Under review as a conference paper at ICLR 2021
formulation minGX maxc Dc,(PX|Z, PX|Z) that we target. In practice, we approximate the cost
and the generator based on neural networks. Integrations in the objective function Dc,(PX|Z, PX|Z)
are approximated by sample averages. A pseudocode detailing our learning procedure is given in
Appendix B. The conditional distribution PY |Z is estimated similarly.
We make two remarks. First, our proposed framework is general, and any GANs learning procedure
can be applied. In our implementation, we have chosen the Sinkhorn GANs because of its compet-
itive performance. We did not use the original GANs due to its instability and the mode collapse
issue. We did implement WGANs, but found the estimated distributions may suffer from the large
bias resulting from the gradient penalty enforced in WGANs. Second, it is important to check the
“goodness-of-fit” of the generator. In practice, this could be achieved by comparing the conditional
histogram of the generated samples to that of the true samples. See Appendix C for details.
4	Asymptotic theory
To derive the theoretical properties of the test statistic T, we first introduce a concept of the “oracle”
test statistic T*. If Pχ∣z and Pγ∣z were known a priori, then one can draw {X(m)}m and {Yi(m)}m
from PX|Z=Zi and PY|Z=Zi directly, and can compute the test statistic T by replacing {Xei(m)}m
and {匕(m)}m with {X(m)}m and {Yi(m)}m. We call the resulting T* an “oracle” test statistic.
We next establish the double-robustness property of T, which helps us better understand why our
proposed test can relax the requirement in (3). Informally speaking, the double-robustness means
that T is asymptotically equivalent to T* when either the conditional distribution of X |Z, or that
of Y |Z is well approximated by GANs. It guarantees T converges to T* at a faster rate than the
estimated conditional distribution. In contrast, the convergence rate of the GCIT test statistic is the
same as the estimated conditional distribution. As such, our procedure requires a weaker condition.
Theorem 1 (Double-robustness) Suppose M is proportional to n, and B = c0nc for some con-
Stants c,co > 0. Then T 一 T* = Op(1), when either (E[d2v{QX)(∙∣Z),Qχ(∙∣Z)}])1/2 =
o(logT∕2 n), or (E[dTv{Q>Y'l(-∣Z),Qγ(∙∣Z)}])1/2 = o(log-1/2 n).
The conditions on M and B are mild, as these are user-specified parameters. As we have discussed,
when both total variation distances converge to zero, the test statistic T converges at a faster rate
than those total variation distances. Therefore, we can greatly relax the condition in (3), and replace
it with, for any ` = 1, . . . , L,
[E{dTV(PX‰, Pχ∣z)}]1/2 = O(n-κ), and 但w前巨院,Pγ∣z)}]1/2 = O(n-κ),	(5)
for some constant 0 < κ < 1/2, where PXIZ and P^Z denote the conditional distributions approx-
imated via GANs trained on the `-th subset. The next theorem summarizes this discussion.
Theorem 2 Suppose the conditions in Theorem 1 and 5 holds. Then T 一 T* = Op(n-2κ log n).
Since κ > 0, the convergence rate of (T 一 T*) is faster than that in (5). To ensure √n(T 一 T*)=
op(1), it suffices to require κ > 1/4. In contrast to (3), this rate is achievable. We consider three
examples to illustrate, while the condition holds in a wide range of settings.
Example 3	(Parametric setting). Suppose the parametric forms of Qχ and QY are correctly spec-
ified. Then the requirement κ > 1/4 holds if k = O(nt0 ) for some t0 < 1/4, where k is the
dimension of the parameters defining the parametric model.
Example 4.	(Nonparametric setting with binary data). Suppose X, Y are binary variables. Then
it suffices to estimate the conditional means ofX and Y given Z. The requirement κ > 1/4 holds if
the mean squared prediction errors of both nonparametric estimators are O(nκ0) for some κ0 > 1/4.
Example 5.	(Nonparametric setting with general data). Suppose X, Y are continuous variables.
We apply GANs to learn the condition distributions. Chen et al. (2020) established the statistical
properties of GANs, and one can apply their technical tools to check the convergence rates in (5). In
general, the convergence rate depends on the smoothness of the conditional density function. The
smoother the conditional density, the faster the rate.
Next, we establish the size and the power properties of our proposed test.
6
Under review as a conference paper at ICLR 2021
Theorem 3	Suppose the conditions in Theorem 1 hold. Suppose (5) holds for some κ > 1/4. Then
the p-value from Algorithm 2 satisfies that Pr(p ≤ α∣H0) = α + o(1).
Theorem 3 shows that our proposed test can control the type-I error. Next, to derive the power
property, we introduce the pair of hypotheses based on the notion of weak CI (Daudin, 1980):
H0 ： ECov(f(X),g(Y)|Z) = 0,∀f ∈ LX,g ∈ LY versusH ： ECov(f(X),g(Y)|Z) = 0,∃f,g,
where L2X and L2Y denote the class of all squared integrable functions of X and Y, respectively. We
note that conditional independence implies weak conditional independence, i.e., H0 implies H0.
Consequently, H implies H1. The next theorem shows that our proposed test is consistent against
the alternatives in H；, but not against all alternatives in H1.
Theorem 4	Suppose the conditions in Theorem 3 hold. Then the p-value from Algorithm 2 satisfies
that Pr(p ≤ a|H；) → 1, as n → ∞.
Finally, we remark that our test is constructed based on φXY . Meanwhile, we may consider another
test based on φXYZ = MMD(PXYZ, QXYZIHI 0 H2 0 H3), where PXYZ is the joint distribution
of (X, Y, Z), QXYZ = PX|ZPY|ZPZ, and H3 is a neural network class of functions of Z. This
type of test is consistent against all alternatives in H1. However, in our numerical experiments, we
find it less powerful compared to our test. This agrees with the observation by Li & Fan (2019) in
that, even though the tests based on weak CI cannot fully characterize CI, they potentially benefit
from an improved power.
5 Numerical studies
We give the implementation details of our testing procedure in Appendix A. The time complexity
of our test is dominated by Step 2 of Algorithm 1, where we use GANs to estimate the conditional
distributions PX|Z and PY|Z. The complexity of each SGD iteration is O(RN2); see Appendix B.
All experiments were run on 16 N1 CPUs on Google Cloud Computing platform. The wall clock
time for computing a single test statistic was about 2.5 minutes.
5.1	Synthetic data example
We generate synthetic data following the post non-linear noise model similarly as in Zhang et al.
(2011); Doran et al. (2014); Bellot & van der Schaar (2019),
X = sin(af>Z + εf) and Y = cos(ag>Z + bX + εg).
The entries of af , ag are randomly and uniformly sampled from [0, 1], then normalized to unit
norm. The noise variables ε are independently sampled from a normal distribution with mean zero
Figure 2: Top panels: the empirical type-I error rate of various tests under H0 . From left to right:
normal Z with α = 0.1, normal Z with α = 0.05, Laplacian Z with α = 0.1, and Laplacian Z
with α = 0.05. Bottom panels: the empirical power of various tests under H1 . From left to right:
dZ = 100,α=0.1,dZ = 100, α = 0.05, dZ = 200, α = 0.1, and dZ = 200, α = 0.05.
7
Under review as a conference paper at ICLR 2021
Table 1: The variable importance measures of the elastic net and random forest models, versus the
P-ValUes of the GCIT and DGCIT tests for the anti-cancer drug example.
	BRAF.V600E	BRAF.MC	HIP1	FTL3	CDC42BPA	THBS3	DNMT1	PRKD1	PIP5K1A	MAP3K5
EN	1	3	4	5	7	8	9	10	19	78
RF	1	2	3	14	8	34	28	18	7	9
GCIT	<0.001	<0.001	0.008	0.521	0.050	0.013	0.020	0.002	0.001	<0.001
DGCIT	0	0	0	0	0	0	0	0	0	0.794
and variance 0.25. In this model, the parameter b determines the degree of conditional dependence.
When b = 0, H0 holds, and otherwise H1 holds. The sample size is fixed at n = 1000.
We call our test DGCIT, short for double GANs-based conditional independence test. We compare
it with the GCIT test of Bellot & van der Schaar (2019), the regression-based test (RCIT) of Shah
& Peters (2018), the kernel MMD-based test (KCIT) of Zhang et al. (2011), the classifier CI test
(CCIT) of Sen et al. (2017) and the deep learning-based CI test (DL-CIT) of Sen et al. (2018).
Type-I error under H0. We vary the dimension of Z as dZ = 50, 100, 150, 200, 250, and consider
two generation distributions. We first generate Z from a standard normal distribution, then from
a Laplace distribution. We set the significance level at α = 0.05 and 0.1. Figure 2 top panels
report the empirical size of the tests aggregated over 500 data replications. We make the following
observations. First, the type-I error rates of our test and RCIT are close to or below the nominal
level in nearly all cases. Second, KCIT and DL-CIT fail in that their type-I errors are considerably
larger than the nominal level in all cases. Third, GCIT and CCIT have inflated type-I errors in some
cases. Take GCIT as an example. When Z is normal, dZ = 250 and α = 0.1, its empirical size is
close to 0.15. This is consistent with our discussion in Section 2.1, as GCIT requires a very strong
condition to control the type-I error.
Powers under H1. We generate Z from a standard normal distribution, with dZ = 100, 200, and
vary the value of b = 0.3, 0.45, 0.6, 0.75, 0.9 that controls the magnitude of the alternative. Figure 2
bottom panels report the empirical power of the tests over 500 data replications. We observe that our
test is the most powerful, and the empirical power approaches 1 as b increases to 0.9, demonstrating
the consistency of the test. Meanwhile, both GCIT and RCIT have no power in all cases. We did
not report the power of KCIT and DL-CIT, because as we show earlier, they can not control the size,
and thus they empirical powers are meaningless.
5.2	Anti-cancer drug data example
We illustrate our proposed test with an anti-cancer drug dataset from the Cancer Cell Line Ency-
clopedia (Barretina et al., 2012). We concentrate on a subset, the CCLE data, that measures the
treatment response of drug PLX4720. It is well known that the patient’s cancer treatment response
to drug can be strongly influenced by alterations in the genome (Garnett et al., 2012) This data mea-
sures 1638 genetic mutations of n = 472 cell lines, and the goal of our analysis is to determine
which genetic mutation is significantly correlated with the drug response after conditioning on all
other mutations. The same data was also analyzed in Tansey et al. (2018) and Bellot & van der
Schaar (2019). We adopt the same screening procedure as theirs to screen out irrelevant mutations,
which leaves a total of 466 potential mutations for our conditional independence testing.
The ground truth is unknown for this data. Instead, we compare with the variable importance mea-
sures obtained from fitting an elastic net (EN) model and a random forest (RF) model as reported
in Barretina et al. (2012). In addition, we compare with the GCIT test of Bellot & van der Schaar
(2019). Table 1 reports the corresponding variable importance measures and the p-values, for 10
mutations that were also reported by Bellot & van der Schaar (2019). We see that, the p-values of
the tests generally agree well with the variable important measures from the EN and RF models.
Meanwhile, the two conditional independence tests agree relatively well, except for two genetic mu-
tations, MAP3K5 and FTL3. GCIT concluded that MAP3K5 is significant (p < 0.001) but FTL3
is not (p = 0.521), whereas our test leads to the opposite conclusion that MAP3K5 is insignificant
(p = 0.794) but FTL3 is (p = 0). Besides, both EN and RF place FTL3 as an important mutation.
We then compare our findings with the cancer drug response literature. Actually, MAP3K5 has not
been previously reported in the literature as being directly linked to the PLX4720 drug response.
Meanwhile, there is strong evidence showing the connections of the FLT3 mutation with cancer re-
sponse (Tsai et al., 2008; Larrosa-Garcia & Baer, 2017). Combining the existing literature with our
theoretical and synthetic results, we have more confidence about the findings of our proposed test.
8
Under review as a conference paper at ICLR 2021
References
Jordi Barretina, Giordano Caponigro, Nicolas Stransky, Kavitha Venkatesan, Adam A Margolin,
Sungjoon Kim, Christopher J Wilson, Joseph Lehar, Gregory V Kryukov, Dmitriy Sonkin, et al.
The cancer cell line encyclopedia enables predictive modelling of anticancer drug sensitivity.
Nature, 483(7391):603-607, 2012.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information theory, 39(3):930-945, 1993.
Alexis Bellot and Mihaela van der Schaar. Conditional independence testing using generative adver-
sarial networks. In Advances in Neural Information Processing Systems, pp. 2199-2208, 2019.
Wicher Pieter Bergsma. Testing conditional independence for continuous random variables. Euran-
dom, 2004.
Thomas B Berrett, Yi Wang, Rina Foygel Barber, and Richard J Samworth. The conditional permu-
tation test for independence while controlling for confounders. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), accepted, 2019.
Emmanuel Candes, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold:model-xknockoffs
for high dimensional controlled variable selection. Journal of the Royal Statistical Society: Series
B (Statistical Methodology), 80(3):551-577, 2018.
Minshuo Chen, Wenjing Liao, Hongyuan Zha, and Tuo Zhao. Statistical guarantees of generative
adversarial networks for distribution estimation. arXiv preprint arXiv:2002.03938, 2020.
Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Gaussian approximation of suprema of
empirical processes. The Annals of Statistics, 42(4):1564-1597, 2014.
Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Comparison and anti-concentration
bounds for maxima of Gaussian random vectors. Probability Theory and Related Fields, 162
(1-2):47-70, 2015.
Victor Chernozhukov, Denis Chetverikov, and Kengo Kato. Detailed proof of nazarov’s inequality.
arXiv preprint arXiv:1711.10696, 2017.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
JJ Daudin. Partial association measures and an application to qualitative regression. Biometrika, 67
(3):581-590, 1980.
Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-
dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018.
Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Scholkopf. A permutation-based kernel
conditional independence test. In UAI, pp. 132-141, 2014.
Kenji Fukumizu, Arthur Gretton, Xiaohai Sun, and Bernhard Scholkopf. Kernel measures of condi-
tional dependence. In Advances in neural information processing systems, pp. 489-496, 2008.
Mathew Garnett, Elena Edelman, Sonja Gill, Chris Greenman, Anahita Dastur, King Lau, Patricia
Greninger, Richard Thompson, Xi Luo, Jorge Soares, Qingsong Liu, Francesco Iorio, Didier
Surdez, Li Chen, Randy Milano, Graham Bignell, Ah Tam, Helen Davies, Jesse Stevenson, and
Cyril Benes. Systematic identification of genomic markers of drug sensitivity in cancer cells.
Nature, 483:570-5, 03 2012. doi: 10.1038/nature11005.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn diver-
gences. arXiv preprint arXiv:1706.00292, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
9
Under review as a conference paper at ICLR 2021
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. Journal ofMachine Learning Research,13(Mar):723-773, 2012.
Patrik O Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard Scholkopf. Nonlin-
ear causal discovery with additive noise models. In Advances in neural information processing
systems, pp. 689-696, 2009.
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. Adaptive
computation and machine learning. MIT Press, 2009.
Maria Larrosa-Garcia and Maria R. Baer. Flt3 inhibitors in acute myeloid leukemia: Current status
and future directions. Molecular Cancer Therapeutics, 16(6):991-1001, 2017.
Bing Li. Sufficient Dimension Reduction: Methods and Applications with R. CRC Press, 2018.
Chun Li and Xiaodan Fan. On nonparametric conditional independence tests for continuous vari-
ables. Wiley Interdisciplinary Reviews: Computational Statistics, pp. e1489, 2019.
Judea Pearl. Causality: Models, Reasoning and Inference. Cambridge: Cambridge University Press,
2nd Edition, 2009.
J Romano and Cyrus DiCiccio. Multiple data splitting for testing. Technical report, Technical report,
2019.
Rajat Sen, Ananda Theertha Suresh, Karthikeyan Shanmugam, Alexandros G Dimakis, and Sanjay
Shakkottai. Model-powered conditional independence test. In Advances in neural information
processing systems, pp. 2951-2961, 2017.
Rajat Sen, Karthikeyan Shanmugam, Himanshu Asnani, Arman Rahimzamani, and Sreeram Kan-
nan. Mimic and classify: A meta-algorithm for conditional independence testing. arXiv preprint
arXiv:1806.09708, 2018.
Rajen D Shah and Jonas Peters. The hardness of conditional independence testing and the gener-
alised covariance measure. arXiv preprint arXiv:1804.07203, 2018.
Liangjun Su and Halbert White. A consistent characteristic function-based test for conditional in-
dependence. Journal of Econometrics, 141(2):807-834, 2007.
Liangjun Su and Halbert White. Testing conditional independence via empirical likelihood. Journal
of Econometrics, 182(1):27-44, 2014.
Wesley Tansey, Victor Veitch, Haoran Zhang, Raul Rabadan, and David M Blei. The hold-
out randomization test: Principled and easy black box feature selection. arXiv preprint
arXiv:1811.00645, 2018.
James Tsai, John T. Lee, Weiru Wang, et al. Discovery of a selective inhibitor of oncogenic b-raf
kinase with potent antimelanoma activity. Proceedings of the National Academy of Sciences, 105
(8):3041-3046, 2008. doi: 10.1073/pnas.0711741105.
Anastasios Tsiatis. Semiparametric theory and missing data. Springer Science & Business Media,
2007.
Xueqin Wang, Wenliang Pan, Wenhao Hu, Yuan Tian, and Heping Zhang. Conditional distance
correlation. Journal of the American Statistical Association, 110(512):1726-1734, 2015.
Hao Zhang, Shuigeng Zhou, and Jihong Guan. Measuring conditional independence by indepen-
dent residuals: Theoretical results and application in causal discovery. In Thirty-Second AAAI
Conference on Artificial Intelligence, 2018.
K Zhang, J Peters, D Janzing, and B Scholkopf. Kernel-based conditional independence test and
application in causal discovery. In 27th Conference on Uncertainty in Artificial Intelligence (UAI
2011), pp. 804-813. AUAI Press, 2011.
10
Under review as a conference paper at ICLR 2021
A Additional details about the proposed testing procedure
We first discuss the computation time. In our synthetic data example, it took about 2.5 minutes to
compute our test, 2 minutes to compute CCIT, and 20 seconds to compute GCIT and DL-CIT.
We next discuss some implementation details. For the number of functions B in Algorithm 1, it
represents a trade-off. By Theorem 4, B should be as large as possible to guarantee a good power.
In practice, the computation complexity increases as B increases. Our numerical studies suggest that
the value of B between 30 and 50 achieves a good balance between the power and the computational
cost, and we fix B = 30. For the number of pseudo samples M, and the number of sample splittings
L, we find the results are not overly sensitive to their choices, and thus we fix M = 100 and L = 3.
For the GANs, we use a single-hidden layer neural network to approximate both the discriminator
and generator. The number of nodes in the hidden layer is set at 128. The dimension of the input
noise νi(,mX) and νi(,mY) is set at 10. The performance of GANs is largely affected by the regularization
parameter and the number of Sinkhorn iterations R. In our experiments, we set = 0.8 and
R = 30. In practice, we suggest to tune these parameters by investigating the goodness-of-fit of the
resulting generator. This could be achieved by comparing the conditional histogram of the generated
samples to that of the true samples. See Appendix C for details. We use stochastic gradient descent
(SGD) to update the parameters in GANs. The batch size N is set to 64. We find the resulting GANs
work reasonably well in our experiments.
Finally, we introduce the variance estimator. Define
b2ι,b2 =max{ n--1 X ([hι,bι (Xi)- E{h1,b1 (Xi) |%}] [〃2耳(K)- E{h2,b2 (匕)邑}]
i=1
-GCM{h1,b1 (X), h2,b2 (Y)}	,0,
for any b1 and b2, where 0 denotes some sufficiently small constant. The constant 0 is to guarantee
that the denominator of ψb1 ,b2 ,i is strictly greater than zero, such that the proposed test has the
desired size and power properties.
B Additional details for conditional distribution
approximation using GANs
We first introduce the notion of optimal transport (OT). Let X and Y be some closed subsets of RN,
μ and V be the probability measures on X and Y, respectively. The Kantorovich formulation of
optimal transport is defined by,
Dc(μ, v )：
inf	c(x, y)π(dx, dy),
where D is the OT of μ into V with respect to a cost function c(∙, ∙), and Π(μ, V) is a set containing
all probability measures ∏ whose marginal distributions on X and Y correspond to μ and V.
Notably, when the cost function is the Euclidean distance between the two arguments, D is better
known as the Wasserstein distance. To facilitate the computation of evaluating OT losses, Cuturi
(2013) and Genevay et al. (2017) suggested to add an entropic regularization to Dc. This yields the
objective function,
Dc,e(μ, V)
∏∈f)L…-
EH (π∣μ 0 V)}π(dx, dy),
where H denotes the Kullback-Leibler divergence, and μ 0 V is the product measure of μ and V.
Such an objective function can be efficiently evaluated by the Sinkhorn algorithm (Cuturi, 2013;
Genevay et al., 2017). To alleviate the bias resulting from the entropic regularization term, Cuturi
(2013) and Genevay et al. (2017) further considered using the Sinkhorn loss function, defined as,
Dc,e(μ, V) = 2Dc,e (μ, V) - Dc,e(μ, μ) - Dc,e (v, V).
Next, We present our algorithm for learning the generator GX) in Algorithm 3, where, XN and yN
denote the empirical distributions of {xj}jN=1 and {yθj}jN=1, respectively.
11
Under review as a conference paper at ICLR 2021
Input: data {Xi}i∈ι('), {Zi}i∈ι('), probability distribution Z on the latent space V, initial
values of θo and 夕0, batch size N, regularization parameter e, number of Sinkhorn iterations R,
and learning rate α.
Repeat:
Train discriminator:
Sample {xj}jN=1 from {(Xi, Zi)}i∈I (`);
Sample {zj}N=ι from {。力记工⑷；
Sample {vj}jN=1 from ζ;
yj - {(Gθ(zj,vj),zj)} for all j;
中一中 + aV/c,e(XN, ^N)).
Train generator:
Sample {xj}jN=1 from {(Xi, Zi)}i∈I (`);
Sample {zj}?= from {Zi}i∈ι(');
Sample {vj}jN=1 from ζ;
yj - {(Gθ(ZjJj),zj)} for all j;
θ 一 θ - αVθDc,e(χN, ^N)).
until convergence.
Algorithm 3: Algorithm for training Sinkhorn GANs by stochastic gradient descent.
C Additional details for GANs model checking
Let dz denote the dimension of Z, and bz the sample average n-
denote a simulated sample to approximate the distribution of Y |Z
Pi Zi. Let Yei = GY (Zi,vi,Y )
= Zi obtained by the generator
〜
GY . When GY is accurate, we expect the conditional distribution of Yi and Yi given Zi are similar.
As such, for any dz-dimensional vector a, the histograms {匕 ：a > (Zi — μz) > 0} and {匕 ：
aT(Zi - bz) > 0} should be similar. We sample i.i.d. normal vectors {ag}g from N(0, IdZ). For
each g, We plot the histogram {Yi : a;(Zi 一 bz) > 0} and {Yi(m) : a;(Zi — bz) > 0}. See
Figures 3 (a) and (b) for the conditional histograms with two choices of ag , respectively. It can be
seen that the Sinkhorn GANs fit the conditional density reasonably Well.
The fitted conditional distribution for PX|z could be checked in a similar fashion as Well.
▲ ACS
Heal 5^mp∣ES
Sinkham Samples
Heal Samples
Siπkhotjπ Samples
(a)
(b)
Figure 3: Conditional Histograms. GANs are trained using data generated from the simulation study
(see Section 5.1).
We next comment on the performance of the WGANs using Bellot & van der Schaar (2019)’s code.
Specifically, We apply their code to their simulation setting as Well as our experiments to train the
WGAN. We then plot the histograms of the real samples and the WGAN samples scaled betWeen
0 and 1 in Figure 4. It can be seen that the WGANs perform poorly in both settings. For instance,
12
Under review as a conference paper at ICLR 2021
Figure 4: The line plots and histograms of real samples and GCIT WGAN samples (scaled between
0 and 1). The top two panels are the results under Bellot & van der Schaar (2019)’s setup with three
different simulations and the bottom top panels, are the results under our setup with three different
simulations.
under Bellot & van der Schaar (2019)’s setup, the WGAN samples are very close to zero and do not
have much variability. This is very different from the distribution of the real samples.
D Additional details for MMD
Let X0 and Y 0 be independent copies of X and Y such that they are conditionally independent given
Z . Note that,
E{h1(X0)h2(Y0)} = E[E{h1(X0)|Z}E{h2(Y0)|Z}],
Henceforth,
Φxy = MMD(PXY,Qχγ∣Hι ③ H2) = Sup	[E{hι(X)h2(Y)}- E{hι(X0)h2(Y0)}]
h1∈H1,h2∈H2
=	sup	E{h1(X)h2(Y)}-E[E{h1(X)|Z}E{h2(Y)|Z}]
h1∈H1,h2∈H2
=	sup	E{h1(X)h2(Y)}-E[h1(X)E{h2(Y)|Z}] -E[{h1(X)|Z}h2(Y)]
h1∈H1,h2∈H2
+ E[E{h1(X)|Z}E{h2(Y)|Z}] = sup Ehh1(X) -E{h1(X)|Z}ihh2(Y) - E{h2 (Y)|Z}i.
h1∈H1,h2∈H2
E Additional numerical results
In our simulation settings, we find that DL-CIT, CCIT and KCIT cannot control the type-I error
in finite samples. In particular, the type-I errors of DL-CIT are larger than 0.2 in almost all cases.
We first use DL-CIT as an example to investigate its type-I error with a larger sample size. We fix
dZ = 150, and generate Z from a standard normal distribution. It can be seen that DL-CIT requires
a very large sample size, e.g., n = 5000, in order to control the type-I error.
We next conduct additional experiments to investigate the performance of the proposed test with a
small sample size, i.e., when n = 500. We fix the dimension d = 100, generate Z from a standard
normal distribution, and set δ to 0.9 under H1. We did not implement DL-CIT and KCIT, because
they are no longer valid even when n = 1000. Table 2 report the results. It is seen that the proposed
test is consistent under this small sample size setting too. Meanwhile, GCIT and RCIT do not have
powers under H1 . CCIT has inflated type-I error under H0 . Specifically, its type-I errors under
13
Under review as a conference paper at ICLR 2021
Figure 5: Type-I errors of DL-CIT with larger sample sizes.
Ho						H1		
		DGCIT	GCIT	RCrr	CCIT	DGCrT	GCrT	RCrT
α=	0.05	0.04	0.06	0.05	0.08	0.22	0.07	0.07
α	= 0.1	0.06	0.13	0.11	0.13	0.35	0.10	0.10
Table 2: Empirical type-I error rate and power of various tests under H0 and H1. n = 500, d = 100,
normal Z and δ = 0.9 under H1.
α = 0.05 and 0.1 are 0.08 and 0.13, respectively. The differences 0.08 — 0.05 and 0.13 — 0.1 are
statistically significant as they exceed the Monte Carlo error 1.96 * ,0.05 * 0.95/500 = 0.019 and
1.96,0.1 * 0.9/500 = 0.026. Thus its power becomes meaningless.
F Proofs
We provide the proofs of Proposition 1, and Theorems 2, 3, and 4. We omit the proof of Theorem 1,
since it is similar as that of Theorem 2. Theorems 1-4 are established under our choice of the function
classes H1 andH2, which are set to the classes of neural networks with a single-hidden layer, finitely
many hidden nodes, and the sigmoid activation function, as used in our implementation. Meanwhile,
the results can be extended to more general cases.
F.1 Proof of Proposition 1
Note that the total variation distance is bounded by 1. Suppose EdTV(PX|Z, PX|Z) = o(1).
Then we have dTV(PeX|Z, PX|Z) = op(1). By the dominated convergence theorem, we have
EdTV (PeX |Z, PX|Z) = o(1).
By Theorem 1.2 of Devroye et al. (2018), we have dTV(PX|Z, PX|Z) is proportional to
1
——E
σ0
n
X{Zi> (βb — β0)}2 = o(1).
i=1
It follows that
14
Under review as a conference paper at ICLR 2021
A ∣.	El	YCCC	.	1	/…C、	^	一 ♦ T /U	C	、.
Applying Theorem 1.2 of Devroye et al. (2018) again, we obtain dTV(PX|Z=Zi, PX|Z=Zi) is pro-
portional to
min [l,σ-1∣Z>(b - βo)∣].
Thus, we obtain
n
XEd2TV(PeX|Z=Zi, PX|Z=Zi) = o(1).
i=1
Since the data is exchangeable, we have
Ed2TV(PeX|Z=Zi, PX|Z=Zi) = o(n-1).
This shows that when RHS of (2) is o(1), (6) automatically holds.
(6)
Next, we show (6) is violated in the linear regression example. By the data exchangability, it suffices
n
to show	i=1 Ed2TV{PX|Z=Zi, PX|Z=Zi} is not o(1). With some calculations, we obtain
nn
XEmin [l,σ-2∣Z>(b - βo)∣2] = XEσ-2∣Z>(b - βo)∣2I{σ-2∣Z>(b - βo)∣2 ≤ 1}
nn
+ XEI{σ-2∣Z>(b - βo)|2 > 1} = XEσ-2∣Z>(b-βo)|2
i=1	i=1
n
-XE[σ-2∣Z>(b - βo)∣2 - 1]I{σ-2∣Z>(b - βo)∣2 > 1}.
i=1
By the definition of β, we have
n1	1
VEσ-2∣Z>(β - βo)∣2 = FE(β - β)>Z>Z(β - β) = FEε>Z(Z>Z)-1 Zτε,
i=1	σ0	σ0
where ε = (ε1,…，εn)τ consist of i.i.d. copies of ε defined in Example 1. It follows that
XEσ-2∣Z>(b- βo)∣2 = 42EετZ(ZτZ)τZτε = ^2 trace {EεετZ(ZτZ)-1Zτ}
i=1	σ0	σ0
= trace {EZ(ZτZ)-1Zτ } = dZ,
where dZ is the dimension of Z.
(7)
(8)
In the following, we show
n
XEσ-2∣Zj(b-eo^Rb—BzXb-βo)∣2 ≥ 1} = o(1).	(9)
i=1
Combining this together with (7) and (8) yields
n
XEmin [l,σ-2∣Zτ(b- βo)∣2] ≥ dz - o⑴ ≥ 1 - o(1),
i=1
and hence PZi EdTγ{Pex ∣z=Zi, Qχn)(∙∣Zi)} ≥ 1 - o(1). The proof is hence completed.
It remains to show (9), or equivalently,
Enσ-2∣Zj(b- βo)∣2I{σ-2∣Zj(b- βo)∣2 ≥ 1} = o(1).
We have already shown that Enσ-2∣Zτ (β - β0)∣2 = dz. By dominated convergence theorem, it
suffices to show
nσ-2∣Zτ(b - eo)fim-vzj(b - βo)∣2 ≥ 1} = 0p(1).
By definition, it suffices to show
Pr k-2∣Zj(b - βo)∣2 ≥ 1)→ 0.
However, this is immediate to see by Markov’s inequality as
Eσ-2lZJ (β - βO) |2 = —z → 0.
n
This completes the proof of Proposition 1.
15
Under review as a conference paper at ICLR 2021
F.2 Proof of Theorem 2
We begin by providing an upper bound for the function classes H1 and H2. Recall that θ1,b and θ2,b
are generated according to N(0, 2Id1 /d1) and N(0, 2Id2/d2), respectively. Since H1 and H2 are
classes of neural networks with a single hidden layer and finitely many hidden nodes. Both di and
d2 are finite. All the elements {√diθι,b,j}b,j and {√d2θ2,b,j}b,j are i.i.d. standard normal. For any
standard normal variable Z, we have for any t ≥ 1 that
Pr(|Z| > t) ≤ 2 Z	φ(z)dz ≤ 2 Z	zφ(z)dz = 2 exp (-g
where φ(∙) denotes the standard normal density function. Setting t = C √2 log n for some constant
c* > 0, We obtain Pr(∣Z∣ > t) ≤ n-c*. It follows from Bonferroni,s inequality that
Pr Qmax ∣pdιθι,b,j∣ > t} \ !max ∣pd2θ2,b,j| > t}) ≤ Bdιd2n-cc'
(10)
Since d1 and d2 are finite and B = O(nc) for some constant c > 0, the right-hand-side converges
to zero with proper as long as C > c. Consequently, all the weights can be uniformly bounded
by O(√logn). Since the sigmoid function is set to the activation function, the function classes are
bounded by O(√log n) with probability tending to 1. Without loss of generality, we assume this
event holds throughout the proofs of Theorems 2-4.
Define a test statistic
n
M
M
max σb
b1,b2
-1
b1,b2
n
^X h1,b1 (Xi)- M X h1,b1 (Xiim) ) h2,b2 (Yi) - M X h2,b2 (Yim )l,
n
i=1
m=1
m=1
where the σbb1,b2 is constructed based on {Xei(m) }m and {Yei(m)}m, instead of {Xi(m) }m and
{Yi(m)}m. Thus, it suffices to show |T - T** | = Op(n-2κ) and |T* - T**| = Op(n-2κ).
Consider the difference |T - T** |. For any sequences {an}n, {bn}n, we have
| max |an| - max |bn|| ≤ max |an - bn|.
nn	n
(11)
Consequently, the difference |T - T** | is upper bounded by I1 + I2 + I3 where
I1
I2
-i
max σb-
b1,b2 b1,b2
-i
max σb-
b1,b2 b1,b2
-i
I3 = max σ -
3 = mb1,abx2 σb1,b2
nM	M	l
^ X M X {h1,bι (Xiim)) - h1,bι (Xiim))}	h2,b2 (Yi)- M X h2,b2 (Yim ) I ,
nM	M
i=1
i=1
m=1
m=1
n
-X M X {hi,bι (Xi(m)) - hi,bι (Xi(m))}
i=1
m=1
By definition, we have minb也也 ≥ √ for some constant E
Op(n-2κ log n) for j = -, 2, 3 where
Ii* = max
i	b1,b2
I2* = max
2	b1,b2
I3* = max
3 b1 ,b2
m=1
M X {h2,b2 (Yi(m)) - h2,b2 (Yi(m))}"∣ । ,
m=i	l
M X {h2,b2 (匕(m)) - h2,b2 (Yi(m))}"∣ l .
m=i	l
>	0. It suffices to show Ij* =
nM
-	X M X {hi,bι (Xi(m)) - hi,bι (X")}
i=1
m=1
h2,b2 (Yi)- M X h2,b2 (Yi(m))J,
n
-	X hi,bι (Xi) - M X hi,bι (Xi(m))
i=1
m=1
n
n X M X {he (Ximb-him Km))}
i=i
m=i
MM X {h2,b2 (Yi(m)) - h2,b2 (Yi(m))}]l,
m=i	l
M X {h2,b2 (Yi(m)) - h2,b2 (Yi(m))}j .
n
n
n
n
n
n
M
M
M
M
M
M
M
16
Under review as a conference paper at ICLR 2021
The number of folds L is finite, as such, it suffices to show I(') = Op(n-2κ log n) for j = 1,2,3
and ' = 1,…，L, where
I x
M
M
max
b1,b2
i∈I(')
max
b1,b2
i∈I⑶
1 上	〜	1 产
M X {h1,bι (X(m)) - h1,bι (Xi(m))}	h2,b2 (Yi)- M X h2,b2 (Y^m)) I ,
m=1
m=1
m=1
m=1
M
M
ι3')
max
b1 ,b2
11	1
^ X M X {h1,bι (Xi	) - h1,bι (Xi	)} M X {h2,b2 (Yi	) - h2,b2 (Yi	)} I .
nM	M
i∈I⑼
m=1
m=1
The rest of the proof is divided into four steps. In the first three steps, we show I(')
Op(n-2κ logn) for j = 1, 2, 3. n the last step, We show T* - T** = Op(n-2κ logn).
Step 1. Without loss of generality, suppose functions in H1 and H2 are bounded by logn in absolute
values. By Bernstein’s inequality, we have
Pr (归…叫-M Emb(Xi)|Zi}| ≥ t) ≤ 2exp {-2(M log n+2t 即/3) },
for any b and i. Set t = /3(c + 2)M log n where the constant C is defined in the statement of
Theorem 1. For sufficiently large n, we have t√log n/3 ≤ M log n/2. It follows that
Pr	III XM h1,b(Xi(m)) -ME{h1,b(Xi)|Zi}
Im=1
≥，3(c + 2)M log n
By Bonferroni’s inequality, we obtain
Pr max max
[b∈{1,…，B} i∈{1,…，n}
M
X h1,b(Xi(m))-ME{h1,b(Xi)|Zi}
m=1
≥，3(c + 2)M log n
Bn max max Pr II X h1 b(Xi(m)) - M E{h1 b(Xi)|Zi}
b∈{1,…，B} i∈{1,…，n}	—
m=1
Under the condition B = O(nc), we obtain with probability 1 - O(n-1) that
≥，3(c + 2)M log n
M
max max I X h1,b(Xi(m)) - ME{h1,b(Xi)|Zi}I ≤ O(1)n-1/2 log n, (12)
b∈{1,∙∙∙ ,B} i∈{1,…，n}
as M is proportional to n, and O(1) denotes some positive constant.
Similarly, we can show
max max
b∈{1,…，B} i∈I(')
M
X hι,b(X(m)) - M h hι,b(x)PXz=zi(dx)
m=1	x
≤ O(1)√n log n,
with probability 1 - O(n-1), as well. Combining this together with (12), we obtain with probability
1 - O(n-1) that
ι x
)
≤
max
b∈{1,…，B}
i∈I(')
M
X {h1,b(Xi(m)) - h1,b(Xei(m))} -
m=1
(`)
h1,b (x){PX |Z =Zi (dx) - PX|Z=Zi (dx)}
(13)
M
x
≤ O(1)√n log n.
Conditional on Zi, the expectation of h2,b2(Yi) - M-1PmM=1h2,b2(Yi(m)) equals zero. Under
the null hypothesis, the expectation of M-1 PmM=1{h1,b1 (Xi(m)) - h1,b1 (Xei(m))}{h2,b2 (Yi) -
17
Under review as a conference paper at ICLR 2021
M-1 PmM=1 h2,b2 (Yi(m))} equals zero as well. Applying Bernstein’s inequality again, we can sim-
ilarly show with probability tending to 1 that I(') can be upper bounded by
O(1)(σn-1/2 log3/2 n + n-1 log2 n),
(14)
where O(1) denotes some positive constant and
σ2 = max E
b1 ,b2
MM
M X {h1,bι (X(m)) - h1,bι (X(m))}{h2,b2 (Yi)- M X h2,b2 (Yi(m))}
m=1	m=1
1 M
≤ max E M £{h1,bi (X(m)) - h1,bι (Em))} log n.
1	m=1
Let A denote the event in (13). The last term on the second line can be bounded from above by
max E
b1 ,i
1M
M X{h1,b1 (Xim)) - h1,b1 (X(m))} I(A)logn
m=1
max E
b1 ,i
M2
M X{h1,b1 (XIrh- h1,b1 (X(m))} I(Ac)logn.
m=1
Since M is proportional to n, by (11), the term on the first line of (15) is upper bounded by
h h1,b(x){PXZ=Zi(dx) - px∖z=Zi(dx)}
x
n + max E
b∈{1,…，B}
i∈I(')
By the boundedness of the function class H1 , it can be further bounded from above by
O(1) {n-1 log3 n + EdTV(PeX)z, px∖z) log2 n}.
2I
log n.
(15)
(16)
(17)
+
Under the current conditions, the above quantity is of the order O(n-2κ log2 n). Consequently, (15)
is or the order O(n-2κ log2 n).
Note that the event A occurs with probability at least 1 - O(n-1). By the boundedness of the
function class H1, (16) is of the order O(n-1 log2 n).
To summarize, We have further that σ2 is of the order O(n-2κ log2 n). This implies I(') can be
bounded from above by O(n-1∕2-κ log5/2 n). This yields I(') = Op(n-2κ logn).
Step 2. Step 2 can be proven in a similar manner as Step 1 and is thus omitted.
Step 3. Under H0, the expectation of
M
M
11	1
lɪ(')l X M X {h1,bι (Xi	)- h1,b1 (Xi	)} M X {h2,b2 (Yi	) - h2,b2 (Yi	)}
i∈I(') L	m=1	」L m=1	.
equals
E h h1,b1(X){PX|)Z(dx) - PX∖z(dx)} h h2,b2 (y){peY∖z(dy) - PY∖z(dy)}.
xy
Similar to (17), its absolute value can be upper bounded by
EdTV{PXZ=Zi ,PX ∖Z }dTV{PeY∣Z=Zi ,py∖Z } log n.
Under the given conditions, it follows from Cauchy-Schwarz inequality that
EdTV {Px ∖Z =Zi, Px∖Z}dTV{PY∖Z=Zi, PY∖Z}
≤	2 EdTV{PXZ=Zi ,px∖z } + 2 EdTV{P，Z=Zi ,py ∖z } = O(n-2κ).
18
Under review as a conference paper at ICLR 2021
This yields that
max IE
b1 ,b2 I
h h1,b1 (χ){PXz(dχ)- Pχ∣z(dx)} h h2,b2(y){PY⅞(dy) - Py∖z(dy)}
xy
O(n-2κ log n).
Using similar arguments in Step 1, we can show that
I(') 一 max E
3 b1,b2 I
/ h1,b1 (x){pXz(dx)- Pχ∣z(dx)} / h2,b2 (y){PY¾(dy)- Pγ∣z(dy)} = Op(n-2κ log n).
xy
Thus, We obtain IS) = Op(n-2κ log n). This completes the proof of Step 3.
Step 4. Denote by σbb*2,b the variance estimator with {Xei(m)}m and {Yei(m)}m replaced by {Xi(m)}m
and {Yi(m)}m. Using (11), the difference betWeen T* and T** is upper bounded by
n
M
M
-1
max ∣σ- 卜
b1,b2 b1,b2
i=1
m=1
m=1
Under H0 , similar to (13), we can show
n
M
M
max |
b1,b2
n-1
n
X h1,bι (Xi)- M X h1,bι (X(m))	h2,b2 (Yi)- M X h2,b2 (Y^m))
i=1
m=1
m=1	I
Op(n-1/2 log3/2 n)∙
To ShoW |T* 一 T**| = Op(n-2κ), it suffices to show maxb1,b2 找-兀
^*
-σ
-1
b1,b2
| = Op(n-c) for
some constant c > 0. Since both b-兀 and 方也也 are well-bounded away from zero. It suffices to
show maχb1,b2 |b2i,b2 - b*2,b2 | = Opin-c).
Using similar arguments in Steps 1 and 3, we can show
max
b1 ,b2
σbb21,b2
n-1 Var([h1,b1 (X) - E{h1,b1 (X)∣Z}][h2,b2(Y) - E{h2,b2(Y)|Z}] ) ∣ = Op(n-c),
and
max
b1,b2
,b2 - n - 1
Var [him (X) - E{h1,b1 (X)[Z}]%也(Y) - E{h2,b2(Y)[Z}] I = Op(n-c)∙
—
n
This completes the proof of Theorem 2.
F.3 Proof of Theorem 3
In the proof of Theorem 2, We have shoWn T - T* = Op(n-2κ log n). Using similar arguments in
Step 4 of the proof of Theorem 2, We can shoW T* - T*** = Op(n-2κ log n) Where
n
M
M
T*** = max
b1,b2
i=1
m=1
m=1
where
σl,b2 =max{ n-1 Var([h1,b1 (X) - E{h^γ (X )∣Z }]也m(丫) - E{h2,b2 (Y )∣Z }]), J∙
By (12), using similar arguments in I1 in the proof of Theorem 2, we can show T*** - T ****
Op(n-2κ log n) where
T **** = maxσ-1 |
b1,b2 b1,b2
n-1
n	1
E [h1,bι (Xi) - E{h1,bι (Xi)IZiH h2,b2 (Yi)- M E h2,b2 Mm)) 小
i=1
m=1
n
M
19
Under review as a conference paper at ICLR 2021
Similarly, We can show T**** - To = Op(n-2κ logn) where
n
To = maxσ-1,b2∣ n-1 X [h1,b1 (Xi)- E{h1,b1 区)邑}]也m(匕)-E{h2,b2 (匕)∣Zi}].
b1 ,b2
i=1
To summarize, we have shown T - To = Op(n-2κ log n). Since we require κ > 1/4, we obtain
√n(T - To) = θp(log-1/2 n).	(18)
Define a B2 × B2 matrix Σo whose {b1 + B(b2 - 1), b3 + B(b4 - 1)}th entry is given by
Cov σb-11,b2 [h1,b1(Xi)-E{h1,b1(Xi)|Zi}][h2,b2(Yi) -E{h2,b2(Yi)|Zi}],
σb-31,b4 [h1,b3(Xi) - E{h1,b3(Xi)|Zi}] [h2,b4(Yi) - E{h2,b4(Yi)|Zi}] .
In the following, we show
sup ∣Pr(√nTo ≤ t|Ho) - Pr(∣∣N(0, ∑o)k∞ ≤ t)| = o(1).	(19)
t
When B is finite, this is implied by the classical weak convergence results. When B diverges with
n, we require B = O(nc) for some constant c > 0. By the definition of σb1,b2, the variance of
σb-11,b2 [h1,b1(Xi)-E{h1,b1(Xi)|Zi}][h2,b2(Yi)-E{h2,b2(Yi)|Zi}]
is bounded from above by (n - 1)/n. Moreover, combining the boundedness assumption on the
function spaces H1 and H2 together with the definition of σb1,b2 yields that
{σ-1b2 [h1，bi (Xi)- E{h1,bι (Xi)IZi}]也也(Yi)- E{h2,b2 (YiXZiH : b1, b2 ∈ {1,…，B}}
are uniformly bounded away from infinity. Similar to Corollary 4.1 of Chernozhukov et al. (2014),
we can show that (19) holds. Suppose B = 1. Then N(0, Σo) is a single normal variable. This
implies that
n
σb-11,b2 n-1/2 X [h1,b1 (Xi) - E{h1,b1 (Xi)|Zi}] [h2,b2(Yi) - E{h2,b2(Yi)|Zi}]
i=1
is asymptotically normal with zero mean.
Combining (19) together with (18) yields
Pr(√nT ≤ t|Ho) ≥ Pr(kN(0, ∑o)k∞ ≤ t - 6o log-1/2 n) - o(1),
Pr(√nT ≤ t|Ho) ≤ Pr(kN(0, ∑o)k∞ ≤ t + co log-1/2 n) + o(1),
for any sufficiently small o > 0, where the little-o terms are uniform in t.
(20)
Using similar arguments in Step 4 and Step 5 of the proof of Theorem 2, we can show that kΣ -
∑ok∞,∞ = Op(n-c) for some constant c > 0. Using similar arguments for (20) and also Lemma
3.1 of Chernozhukov et al. (2015), we have that
Pr(√nT ≤ t|Ho) ≥	Pr(kN(0,	∑)k∞ ≤ t -	2eo log-1/2	n∣∑)-。⑴，
Pr(√nT ≤ t|Ho) ≤	Pr(kN(0,	∑)k∞ ≤ t +	2e0 log-1/2	n∣Σ) +	o(1),
for any sufficiently small o > 0. Since the little-o terms are uniform in t ∈ R, we obtain
SUp ∣Pr(√nT ≤ t|Ho) - Pr(kN(0, Σ)k∞ ≤ t∣Σ)∣ ≤ o(1)
t
+ sup ∣Pr(kN(0, ∑)k∞ ≤ t + 2elog-1/2 n∣Σ) - Pr(∣∣N(0, Σ)k∞ ≤ t - 2elog-1/2 n∣Σ)∣.
t
The term on the second line can be bounded by O(1) log1/2 B log-1/2 n where O(1) denotes
some positive constant, by Theorem 1 of Chernozhukov et al. (2017). Since B = O(nc),
log1/2 B log-1/2 n = O(1). As grows to zero, this term becomes negligible. Consequently,
we obtain
SUP ∣Pr(√nT ≤ t|Ho) - Pr(∣∣N(0, Σ)k∞ ≤ t∣Σ)∣ ≤ o(1).
t
As such, the distribution of our test statistic can be well-approximated by that of the bootstrap
samples. This completes the proof of Theorem 3.
20
Under review as a conference paper at ICLR 2021
F.4 Proof of Theorem 4
According to the universal approximation theorem (Barron, 1993), neural networks with a single
hidden layer and the sigmoid activation function are universal approximators. Under H；, there exist
two neural networks functions f(X) andg(Y ), with a single hidden layer and the sigmoid activation
function, such that
E[f(X) - E{f(X)|Z}][g(Y) - E{g(Y)|Z}] 6=0.
Note that f (g) can be represented by linear combinations of functions in H1 (H2). It implies that
there exists some constant c； > 0 such that GCM；(h1；(X), h2； (Y )) > c； for some h1； ∈ H1 and
h； ∈ H2. Let θ1 and θg be the corresponding parameters such that h； = hι,θ*, h； = h2,θ*.
We next ShoWthatGCM;(hι,θι (X),h2,θ2 (Y)) is a Lipschitz continuous function of (θ1,θ2). Note
that h1 θ1 (X) and h2,θ2 (Y) are Lipschitz continuous functions of θ1 and θ2, respectively. For any
θ1,1 , θ1,2 ∈ Rd1, θ2,1 , θ2,2 ∈ Rd2, We have
∣GCM*(h1,θ1 (X),h2,θ2(Y))- GCM*(h1,θ1 (X),h;^(Y))|
≤ |E[h1,1(X)-E{h1,1(X)|Z}-h1,2(X)+E{h2,1(X)|Z}][h2,1(Y)-E{h2,1(Y)|Z}]| (21)
+|E[h1,2(X)-E{h1,2(X)|Z}][h2,1(Y)-E{h2,1(Y)|Z}-h2,2(Y)+E{h2,2(Y)|Z}]|. (22)
Since H2 is bounded function class, RHS of (21) is bounded from above by
。⑴E∣h1,1(X) - E{h1,1(X)|Z} - h1,2(X) + E{h2,1(X)|Z}∣piθgn,
Where O(1) denotes some positive constant. By Jensen’s inequality, the above quantity can be
further bounded from above by
O(1)E∣hι,ι(X) - h1,2(X)∣2Pl0gn ≤ L∣∣θ1,1 - θ1,2k2PlOgn,
for some constant L > 0. Using similar arguments, We can shoW RHS of (22) is bounded from
above by Lkθ2,1 - θ2,2 k2. To summarize, We have shoWn that
|GCM；(hi,©](X), h2,θ2(Y))- GCM；(him(X), h2,θ2(Y))|
≤ L(∣∣θ1,1 — θ1,2∣∣2 + ∣∣θ2,1 — θ2,2∣∣2)plogn.
As such, for any sufficiently small e > 0, there exists a neighborhood N = {(θι, θ2) : ∣θj 一 θ*∣2 ≤
δlog-1/2 n} for some constant δ > 0 around (θɪ, θ2) such thatGCM；(hi,©i (X),九2/(Y)) ≥ E for
any (θi, θ2) that belongs to this neighborhood.
Since We use multivariate normal to generate (θi,b, θ2,b) and the dimensions di and d2 are finite,
the probability that (θi,b, θ2,b) belongs to this neighborhood is strictly greater than O(log-c1 n)
for some constant ci > 0. Since B = c0nc, the probability that at least one pair of parameters
(θi,b1 , θ2,b2) belongs to this neighborhood approaches one. Consequently, We have
maxGCM；(hi,b1(X),h2,b2(Y)) ≥E,
b1,b2
With probability tending to 1.
Using similar arguments in the proof of Theorems 2 and 3, We can shoW that |T -
maxb1,b2 GCM；(hi,b1 (X), h2,b2 (Y))| = op(1) and Tej = op(1). Consequently, both probabili-
ties Pr(T < E/2) and Pr(Tj ≥ E/2) converge to zero. As such, the probability that the p-value is
greater than α is bounded by the probability that Pr(T < E/2), and hence converges to zero. This
completes the proof of Theorem 4.
21