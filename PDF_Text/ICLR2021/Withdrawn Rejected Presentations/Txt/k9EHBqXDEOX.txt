Under review as a conference paper at ICLR 2021
Asynchronous Advantage Actor Critic:
Non-asymptotic Analysis and Linear Speedup
Anonymous authors
Paper under double-blind review
Ab stract
Asynchronous and parallel implementation of standard reinforcement learning
(RL) algorithms is a key enabler of the tremendous success of modern RL. Among
many asynchronous RL algorithms, arguably the most popular and effective one is
the asynchronous advantage actor-critic (A3C) algorithm. Although A3C is becom-
ing the workhorse of RL, its theoretical properties are still not well-understood,
including the non-asymptotic analysis and the performance gain of parallelism
(a.k.a. speedup). This paper revisits the A3C algorithm with TD(0) for the critic
update, termed A3C-TD(0), with provable convergence guarantees. With linear
value function approximation for the TD update, the convergence of A3C-TD(0) is
established under both i.i.d. and Markovian sampling. Under i.i.d. sampling, A3C-
TD(0) obtains sample complexity of O(-2.5/N) per worker to achieve accuracy,
where N is the number of workers. Compared to the best-known sample complex-
ity of O(-2.5) for two-timescale AC, A3C-TD(0) achieves linear speedup, which
justifies the advantage of parallelism and asynchrony in AC algorithms theoretically
for the first time. Numerical tests on synthetically generated instances and OpenAI
Gym environments have been provided to verify our theoretical analysis.
1	Introduction
Reinforcement learning (RL) has achieved impressive performance in many domains such as robotics
[1, 2] and video games [3]. However, these empirical successes are often at the expense of significant
computation. To unlock high computation capabilities, the state-of-the-art RL approaches rely on
sampling data from massive parallel simulators on multiple machines [3, 4, 5]. Empirically, these
approaches can stabilize the learning processes and reduce training time when they are implemented in
an asynchronous manner. One popular RL method that often achieves the best empirical performance
is the asynchronous variant of the actor-critic (AC) algorithm, which is referred to as A3C [3].
A3C builds on the original AC algorithm [6]. At a high level, AC simultaneously performs policy
optimization (a.k.a. the actor step) using the policy gradient method [7] and policy evaluation (a.k.a.
the critic step) using the temporal difference learning (TD) algorithm [8]. To ensure scalability,
both actor and critic steps can combine with various function approximation techniques. To ensure
stability, AC is often implemented in a two time-scale fashion, where the actor step runs in the slow
timescale and the critic step runs in the fast timescale. Similar to other on-policy RL algorithms, AC
uses samples generated from the target policy. Thus, data sampling is entangled with the learning
procedure, which generates significant overhead. To speed up the sampling process of AC, A3C
introduces multiple workers with a shared policy, and each learner has its own simulator to perform
data sampling. The shared policy can be then updated using samples collected from multiple learners.
Despite the tremendous empirical success achieved by A3C, to the best of our knowledge, its
theoretical property is not well-understood. The following theoretical questions remain unclear: Q1)
Under what assumption does A3C converge? Q2) What is its convergence rate? Q3) Can A3C obtain
benefit (or speedup) using parallelism and asynchrony?
For Q3), we are interested in the training time linear speedup with N workers, which is the ratio
between the training time using a single worker and that using N workers. Since asynchronous
parallelism mitigates the effect of stragglers and keeps all workers busy, the training time speedup
1
Under review as a conference paper at ICLR 2021
can be measured roughly by the sample (i.e., computational) complexity linear speedup [9], given by
Speedup(N)
sample complexity when using one worker
average sample complexity per worker when using N workers
(1)
If Speedup(N) = Θ(N), the speedup is linear, and the training time roughly reduces linearly as
the number of workers increases. This paper aims to answer these questions, towards the goal of
providing theoretical justification for the empirical successes of parallel and asynchronous RL.
1.1	Related works
Analysis of actor critic algorithms. AC method was first proposed by [6, 10], with asymptotic
convergence guarantees provided in [6, 10, 11]. It was not until recently that the non-asymptotic
analyses of AC have been established. The finite-sample guarantee for the batch AC algorithm
has been established in [12, 13] with i.i.d. sampling. Later, in [14], the finite-sample analysis was
established for the double-loop nested AC algorithm under the Markovian setting. An improved
analysis for the Markovian setting with minibatch updates has been presented in [15] for the nested
AC method. More recently, [16, 17] have provided the first finite-time analyses for the two-timescale
AC algorithms under Markov sampling, with both O(E-2.5) sample complexity, which is the best-
known sample complexity for two-timescale AC. Through the lens of bi-level optimization, [18] has
also provided finite-sample guarantees for this two-timescale Markov sampling setting, with global
optimality guarantees when a natural policy gradient step is used in the actor. However, none of the
existing works has analyzed the effect of the asynchronous and parallel updates in AC.
Empirical parallel and distributed AC. In [3], the original A3C method was proposed and became
the workhorse in empirical RL. Later, [19] has provided a GPU-version of A3C which significantly
decreases training time. Recently, the A3C algorithm is further optimized in modern computers
by [20], where a large batch variant of A3C with improved efficiency is also proposed. In [21], an
importance weighted distributed AC algorithm IMPALA has been developed to solve a collection of
problems with one single set of parameters. Recently, a gossip-based distributed yet synchronous AC
algorithm has been proposed in [5], which has achieved the performance competitive to A3C.
Asynchronous stochastic optimization. For solving general optimization problems, asynchronous
stochastic methods have received much attention recently. The study of asynchronous stochastic
methods can be traced back to 1980s [22]. With the batch size M, [23] analyzed asynchronous
SGD (async-SGD) for convex functions, and derived a convergence rate of O(K- 2 M- 2) if delay
Ko is bounded by O(K 1 M-4). This result implies linear speedup. [24] extended the analysis of
[23] to smooth convex with nonsmooth regularization and derived a similar rate. Recent studies
by [25] improved upper bound of Ko to O(K 1 M-2). However, all these works have focused on
the single-timescale SGD with a single variable, which cannot capture the stochastic recursion of
the AC and A3C algorithms. To best of our knowledge, non-asymptotic analysis of asynchronous
two-timescale SGD has remained unaddressed, and its speedup analysis is even an uncharted territory.
1.2	This work
In this context, we revisit A3C with TD(0) for the critic update, termed A3C-TD(0). The hope is to
provide non-asymptotic guarantee and linear speedup justification for this popular algorithm.
Our contributions. Compared to the existing literature on both the AC algorithms and the async-
SGD, our contributions can be summarized as follows.
c1) We revisit two-timescale A3C-TD(0) and establish its convergence rates with both i.i.d. and
Markovian sampling. To the best of our knowledge, this is the first non-asymptotic convergence
result for asynchronous parallel AC algorithms.
c2) We characterize the sample complexity of A3C-TD(0). In i.i.d. setting, A3C-TD(0) achieves a
sample complexity of O(E-2.5/N) per worker, where N is the number of workers. Compared to the
best-known complexity of O(E-2.5) for i.i.d. two-timescale AC [18], A3C-TD(0) achieves linear
speedup, thanks to the parallelism and asynchrony. In the Markovian setting, if delay is bounded, the
sample complexity of A3C-TD(0) matches the order of the non-parallel AC algorithm [17].
2
Under review as a conference paper at ICLR 2021
c3) We test A3C-TD(0) on the synthetically generated environment to verify our theoretical guarantees
with both i.i.d. and Markovian sampling. We also test A3C-TD(0) on the classic control tasks and
Atari Games from OpenAI Gym. Code is available in the supplementary material.
Technical challenges. Compared to the recent convergence analysis of nonparallel two-timescale
AC in [16, 17, 18], several new challenges arise due to the parallelism and asynchrony.
Markovian noise coupled with asynchrony and delay. The analysis of two-timescale AC algorithm is
non-trivial because of the Markovian noise coupled with both the actor and critic steps. Different from
the nonparallel AC that only involves a single Markov chain, asynchronous parallel AC introduces
multiple Markov chains (one per worker) that mix at different speed. This is because at a given
iteration, workers collect different number of samples and thus their chains mix to different degrees.
As we will show later, the worker with the slowest mixing chain will determine the convergence.
Linear speedup for SGD with two coupled sequences. Parallel async-SGD has been shown to achieve
linear speedup recently [9, 26]. Different from async-SGD, asynchronous AC is a two-timescale
stochastic semi-gradient algorithm for solving the more challenging bilevel optimization problem
(see [18]). The errors induced by asynchrony and delay are intertwined with both actor and critic
updates via a nested structure, which makes the sharp analysis more challenging. Our linear speedup
analysis should be also distinguished from that of mini-batch async-SGD [27], where the speedup is
a result of variance reduction thanks to the larger batch size generated by parallel workers.
2	Preliminaries
2.1	Markov decision process and policy gradient theorem
RL problems are often modeled as an MDP described by M = {S, A, P, r, γ}, where S is the state
space, A is the action space, P(s0|s, a) is the probability of transitioning to s0 ∈ S given current
state s ∈ S and action a ∈ A, and r(s, a, s0) is the reward associated with the transition (s, a, s0),
and γ ∈ [0, 1) is a discount factor. Throughout the paper, we assume the reward r is upper-bounded
by a constant rmax. A policy π : S → ∆(A) is defined as a mapping from the state space S to the
probability distribution over the action space A.
Considering discrete time t in an infinite horizon, a policy π can generate a trajectory of state-action
pairs (so, ao, sι,αι,...) with at 〜∏(∙∣st) and st+ι 〜P(∙∣st, at). Given a policy π, We define the
state and state action value functions as
∞
Vπ(s) := E	γtr(st, at, st+1) | s0 = s
t=0
∞
Qπ (s, a) := E	γ r(st, at, st+1) | s0 = s, a0 = a (2)
t=0
where E is taken over the trajectory (s0, a0, s1, a1, . . .) generated under policy π. With the above
definitions, the advantage function is Aπ(s, a) := Qπ(s, a) - Vπ(s). With η denoting the initial
state distribution, the discounted state visitation measure induced by policy π is defined as dπ (s) :=
(1 - γ) t∞=0 γtP(st = s | s0 〜η,∏), and the discounted state action visitation measure is
d∏(s, a) = (I-Y) P∞=o YtP(St = S | so ~ η, n)n(a|s).
The goal of RL is to find a policy that maximizes the expected accumulative reward J(π) :=
Es〜η [V∏(s)]. When the state and action spaces are large, finding the optimal policy ∏ becomes
computationally intractable. To overcome the inherent difficulty of learning a function, the policy
gradient methods search the best performing policy over a class of parameterized policies. We
parameterize the policy with parameter θ ∈ Rd, and solve the optimization problem as
max J(θ) with J(θ) := E [Vπθ (s)].
θ∈Rd	S〜η
(3)
To maximize J(θ) with respect to θ, one can update θ using the policy gradient direction given by [7]
VJ (θ) =	E	[A∏θ (s,a)ψθ (s,a)],
s,a 〜dθ
(4)
where ψθ(s,a) := V log ∏θ(a|s), and d% := (1 一 Y) P∞=0 YtP(St = S | so,∏θ)∏θ(a|s). Since
computing E in (4) is expensive if not impossible, popular policy gradient-based algorithms iteratively
update θ using stochastic estimate of (4) such as REINFORCE [28] and G(PO)MDP [29].
3
Under review as a conference paper at ICLR 2021
2.2	Actor-critic algorithm with value function approximation
Both REINFORCE and G(PO)MDP-based policy gradient algorithms rely on a Monte-Carlo estimate
of the value function V∏θ (S) and thus V J(θ) by generating a trajectory per iteration. However,
policy gradient methods based on Monte-Carlo estimate typically suffer from high variance and large
sampling cost. An alternative way is to recursively refine the estimate of Vπθ (s). For a policy πθ, it is
known that Vπθ (s) satisfies the Bellman equation [30], that is
Vπθ (s) =	E0	r(s, a, s0) +γVπθ(s0) , ∀s ∈ S.	(5)
a〜ne(.|s), s0〜Pgs,。)
In practice, when the state space S is prohibitively large, one cannot afford the computational and
memory complexity of computing Vπθ (s) and Aπθ (s, a). To overcome this curse-of-dimensionality,
a popular method is to approximate the value function using function approximation techniques.
Given the state feature mapping φ(∙) : S → Rd0 for some d0 > 0, we approximate the value function
linearly as V∏θ (S) ≈ 匕(s) ：= φ(s)>ω, where ω ∈ Rd0 is the critic parameter.
Given a policy ∏θ, the task of finding the best ω such that V∏θ (S) ≈ 匕(S) is usually addressed by
TD learning [8]. Defining the kth transition as xk := (Sk, ak, Sk+1) and the corresponding TD-error
as δ(xk,ωk):= r(sk, ak, sk+ι) + γφ(sk+ι)>ωk — φ(sk)>ωk, the parameter ω is updated via
ωk+ι = ∏Rω(ωk + βkg(xk,ωk)) with g(xk,ωk):= δ(xk,ωk)Vωfc匕七(Sk)	(6)
where βk is the critic stepsize, and ΠRω is the projection with Rω being a pre-defined constant. The
projection step is often used to control the norm of the gradient. In AC, it prevents the actor and critic
updates from going a too large step in the ‘wrong’ direction; see e.g., [6, 16, 17].
Using the definition of advantage function A∏θ (s, a)=旧§，〜P [r(S, a, s0) + γV∏θ (s0)] — V∏θ (s), we
can also rewrite (4) as VJ(θ) = Es,。〜dθ/，〜P [(r(S,a, s0) + γV∏ (s0) — V∏ (S)) ψθ(S,a)]. LeVer-
aging the value function approximation, we can then approximate the policy gradient as
▽ J(θ) = (r(s,a,s0) + γVω(s0) - Vω (S)) ψθ(s,a) = δ(x,ω)ψθ(s,a)	(7)
which gives rise to the policy update
θk+ι = θk + αkv(xk,θk,3k) with v(xk,θ,ωk) := δ(xk,3卜)ψθfc(Sk,ak)	(8)
where αk is the stepsize for the actor update.
To ensure convergence when simultaneously performing critic and actor updates, the stepsizes αk
and βk often decay at two different rates, which is referred to the two-timescale AC [17, 18].
3	Asynchronous Advantage Actor Critic with TD(0)
To speed up the training process, we implement AC over N workers in a shared memory setting
without coordinating among workers — a setting similar to that in A3C [3]. Each worker has its
own simulator to perform sampling, and then collaboratively updates the shared policy πθ using AC
updates. As there is no synchronization after each update, the policy used by workers to generate
samples may be outdated, which introduces staleness.
Notations on transition (S, a, S0). Since each worker will maintain a separate Markov chain, we
thereafter use subscription t in (St, at, St+1) to indicate the tth transition on a Markov chain. We
use k to denote the global counter (or iteration), which increases by one whenever a worker finishes
the actor and critic updates in the shared memory. We use subscription (k) in (S(k), a(k) , S0(k)) to
indicate the transition used in the kth update.
Specifically, we initialize θ0, ω0 in the shared memory. Each worker will initialize the simulator
with initial state S0. Without coordination, workers will read θ, ω in the shared memory. From each
worker,s view, it then generates sample (St, at, St+ι) by either sampling St from μθ(∙), where μθ(∙)
is the stationary distribution of an artificial MDP with transition probability measure Pe(∙∣St, at):=
YP(∙∣St, at) + (1 — Y)η(∙), or, sampling St from a Markov chain under policy ∏. In both cases,
each worker obtains at 〜 ∏(∙∣s. and St+ι 〜 P(∙∣St, a.. Sampling St+ι from P(∙∣St, a. can be
achieved by sampling St+ι from η(∙) with probability 1 — Y and from P(∙∣St, at) otherwise. Once
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Asynchronous advantage AC with TD(0): each worker’s view.
1:	Global initialize: Global counter k = 0, initial θ0, ω0 in the shared memory.
2:	Worker initialize: Local counter t = 0. Obtain initial state s0 .
3:	for t = 0,1, 2,•…do
4:	Read θ, ω in the shared memory.
5:	Option 1 (i.i.d. sampling):
6:	Sample St 〜μθ(∙), at 〜∏(∙∣s), st+ι 〜Pe(∙∣st,at).
7:	Option 2 (Markovian sampling):
8:	Sample at 〜∏θ(∙∣st), st+ι 〜Pe(∙∣st, at).
9:	Compute δ(xt, ω) = r(st, at, st+1) + γVω(st+1) - Vω(st).
10:	Compute g(xt,ω) = δ(xt,ω)Vω匕(st).
11:	Compute v(xt, θ, ω) = δ(xt, ω)ψθ(st, at).
12:	In the shared memory, perform update (9).
13:	end for
obtaining xt := (st, at, st+1), each worker locally computes the policy gradient v(xt, θ, ω) and the
TD(0) update g(xt, ω), and then updates the parameters in shared memory asynchronously by
ωk+1 = ΠRω ωk + βkg(x(k), ωk-τk) ,	(9a)
θk+1 = θk + αk v(x(k) , θk-τk , ωk-τk ),	(9b)
where τk is the delay in the kth actor and critic updates. See the A3C with TD(0) in Algorithm 1.
Sampling distributions. Since the transition kernel required by the actor and critic updates are
different in the discounted MDP, it is difficult to design a two-timescale AC algorithm. To address
this issue, we adopt the sampling method introduced in the seminal work [6, 31] and the recent work
[15, 16], which inevitably introduces bias by sampling from the artificial transition P instead of P .
However, as we will mention later, this extra bias is small when the discount factor γ is close to 1.
Parallel sampling. The AC update (6) and (8) uses samples generated “on-the-fly” from the target
policy πθ, which brings overhead. Compared with (6) and (8), the A3C-TD(0) update (9) allows
parallel sampling from N workers, which is the key to linear speedup. We consider the case where
only one worker can update parameters in the shared memory at the same time and the update cannot
be interrupted. In practice, (9) can also be performed in a mini-batch fashion.
Minor differences from A3C [3]. The A3C-TD(0) algorithm resembles the popular A3C method
[3]. With nmax denoting the horizon of steps, for n ∈ {1, ..., nmax}, A3C iteratively uses n-step TD
errors to compute actor and critic gradients. In A3C-TD(0), we use the TD(0) method which is the
1-step TD method for actor and critic update. When nmax = 1, A3C method reduces to A3C-TD(0).
The n-step TD method is a hybrid version of the TD(0) method and the Monte-Carlo method. The
A3C method with Monte-Carlo sampling is essentially the delayed policy gradient method, and thus
its convergence follows directly from the delayed SGD. Therefore, we believe that the convergence
of the A3C method based on TD(0) in this paper can be easily extended to the convergence of the
A3C method with n-step TD. We here focus on A3C with TD(0) just for ease of exposition.
4 Convergence Analysis of Two-Timescale A3C-TD(0)
In this section, we analyze the convergence of A3C-TD(0) in both i.i.d. and Markovian settings.
Throughout this section, the notation O(∙) contains constants that are independent of N and K0.
To analyze the performance of A3C-TD(0), we make the following assumptions.
Assumption 1. There exists K0 such that the delay at each iteration is bounded by τk ≤ K0, ∀k.
Assumption 1 ensures the viability of analyzing the asynchronous update; see the same assumption in
e.g., [5, 25]. In practice, the delay usually scales as the number of workers, that is K0 = Θ(N).
With P∏θ (s0∣s) = Ea∈∕ P(s0∣s, a)∏θ(a|s), We define that:
Aθ,φ := E	[φ(s)(γφ(s0) - φ(s))>],	bθ,φ := E	[r(s, a, s0)φ(s)].	(10)
S 〜μθ ,s0 〜P∏θ	S〜μθ ,。〜∏θ ,s0 〜P
5
Under review as a conference paper at ICLR 2021
It is known that for a given θ, the stationary point ω^ of the TD(0) update in Algorithm 1 satisfies
Aθ,φω3 + bθ,φ = 0.	(11)
Assumption 2. For all s ∈ S, the feature vector φ(s) is normalized so that kφ(s)k2 ≤ 1. For all
γ ∈ [0, 1] and θ ∈ Rd, Aθ,φ is negative definite and its max eigenvalue is upper bounded by -λ.
Assumption 2 is common in analyzing TD with linear function approximation; see e.g., [17, 32, 33].
With this assumption, Aθ,φ is invertible, so We have ω^ = -A-φbθ,φ. Define Rω := rmax∕λ, then
we have ∣∣ω^k2 ≤ Rω. Itjustifies the projection introduced in Algorithm 1. In practice, the projection
radius Rω can be estimated online by methods proposed in [32, Section 8.2] or [34, Lemma 1].
Assumption 3. For any θ, θ0 ∈ Rd, s ∈ S and a ∈ A, there exist constants such that: i) ∣ψθ (s, a)∣2 ≤
Cψ； ii) ∣ψθ(s, a) - ψθo(s, a)∣∣2 ≤ Lψ∣∣θ 一 θ0∣∣2；iii) ∣∏θ(a|s) - ∏θo(a|s)| ≤ Ln∣∣θ 一 θ0∣∣2∙
Assumption 3 is common in analyzing policy gradient-type algorithms which has also been made by
e.g., [34, 35, 36]. This assumption holds for many policy parameterization methods such as tabular
softmax policy [36], Gaussian policy [37] and Boltzmann policy [31].
Assumption 4. For any θ ∈ Rd, the Markov chain under policy ∏ and transition kernel P (∙∣s, a)
or P(∙∣s, a) is irreducible and aperiodic. Then there exist constants κ > 0 and P ∈ (0,1) such that
SuP dτv (P(St ∈ ∙∣so = s,∏θ),μθ) ≤ κρt, Vt	(12)
s∈S
where μθ is the stationary state distribution under ∏θ , and St is the state of Markov chain at time t.
Assumption 4 assumes the Markov chain mixes at a geometric rate; see also [32, 33]. The stationary
distribution μθ of an artificial Markov chain with transition P is the same as the discounted visitation
measure dθ of the Markov chain with transition P [6]. This means that if we sample according to
at 〜∏θ (∙∣st), st+1 〜Pe (∙∣ st, at) ,the marginal distribution of (st,at) will converge to the discounted
state-action visitation measure d0θ(s, a), which allows us to control the gradient bias.
4.1	Linear speedup result with i.i.d. sampling
In this section, we consider A3C-TD(0) under the i.i.d. sampling setting, which is widely used for
analyzing RL algorithms; see e.g., [13, 18, 38].
We first give the convergence result of critic update as follows.
Theorem 1	(Critic convergence). Suppose Assumptions 1-4 hold. Consider Algorithm 1 with
i.i.d. sampling and Vω(S) =	φ(s)>ω.	Select step size	ak	= .+力严,βk	=	(i+2σ2,	where
0 < σ2 < σ1 < 1 and c1 , c2 are positive constants. Then it holds that
K X Ellωk - ωθkll2 = O (K1-2)+O ( K2(σ1-σ2) )+O (K⅛)+O (KK0Γ)+O (K12) . (13)
Different from async-SGD (e.g., [9]), the optimal critic parameter ωθ is constantly drifting as θ
changes at each iteration. This necessitates setting σ1 > σ2 to make the policy change slower than
the critic, which can be observed in the second term in (13). If σ1 > σ2, then the policy is static
relative to the critic in an asymptotic sense.
To introduce the convergence of actor update, we first define the critic approximation error as
^app := maχ JE | V∏θ (S) - Vωi (s) |2 ≤ 6fa + ^sp ,
θ∈Rd V S〜μθ	θ
(14)
where μθ is the stationary distribution under ∏θ and P. The error eapp captures the quality of the critic
approximation under Algorithm 1. It can be further decomposed into the function approximation error
fa, which is common in analyzing AC with function approximation [14, 15, 17], and the sampling
error sp = O(1 - γ), which is unique in analyzing two-timescale AC for a discounted MDP. The
error app is small when the value function approximation is accurate and the discounting factor γ is
close to 1; see the detailed derivations in Lemma 7 of supplementary material. Now we are ready to
give the actor convergence.
6
Under review as a conference paper at ICLR 2021
Theorem 2	(Actor convergence). Under the same assumptions of Theorem 1, select step size αk =
q+k)σι, βk = (1+N., where 0 < σ2 < σι < 1 and ci, c2 are positive constants. Then it holds that
1
K
K
X EkVJ (θk )k2
k=1
(K-στ )+O
磊)+O
ωθk k2) +O&pP).
(15)
O
Different from the analysis of async-SGD, in actor update, the stochastic gradient v(x, θ, ω) is biased
because of inexact value function approximation. The bias introduced by the critic optimality gap
and the function approximation error correspond to the last two terms in (15).
In Theorem 1 and Theorem 2, optimizing σ1 and σ2 gives the following convergence rate.
Corollary 1 (Linear speedup). Given Theorem 1 and Theorem 2, select σι = 3 and σ? = ∣. Ifwe
further assume Ko = O(K1), then it holds that
1K
K XEkVJ(θk)k2 = O (K-2) + O(Capp)	(16)
k=1
where O(∙) contains constants that are independent of N and Ko.
By setting the first term in (16) to , we get the total iteration complexity to reach -accuracy is
O(-∣.5). Since each iteration only uses one sample (one transition), it also implies a total sample
complexity of O(e-∣.5). Then the average sample complexity per worker is O(e-∣.5∕N) which
indicates linear speedup in (1). Intuitively, the negative effect of parameter staleness introduced by
parallel asynchrony vanishes asymptotically, which implies linear speedup in terms of convergence.
4.2	Convergence result with Markovian sampling
Theorem 3 (Critic convergence). Suppose Assumptions 1-4 hold. Consider Algorithm 1 with
cc
Markovian sampling and Vω (S) = φ(s) > ω. Select step size ɑk =(1十力严,βk =(［十月产,where
0 < σ∣ < σ1 < 1 and c1 , c∣ are positive constants. Then it holds that
1K
K XEllωk
k=1
-ω^k∣l2
+o( K(∖j
+O
+O
(Ko log K
+ ∖	Kσ2
(17)
The following theorem gives the convergence rate of actor update in Algorithm 1.
Theorem 4 (Actor convergence). Under the same assumptions of Theorem 3, select step size αk =
(i+k)σι, βk = (i+k)σ2, where 0 < σ∣ < σι < 1 and ci, c∣ are positive constants. Then it holds that
K XEkVJ(θk)k2 = O(K⅛στ卜0(k⅛⅛k)+O(K⅛)+O(KXe kωk
*
- ωθk
Assume Ko = O(K5). Given Theorem 3, select σι = 3 and σ∣ = ∣, then it holds that
k22 +O(Capp).
(18)
1K
K X E kVJ (θk )k2 =O(KoK- 5) + O(Capp),
k=1
(19)
where O(∙) hides constants and the logarithmic order of K.
With Markovian sampling, the stochastic gradients g(x, ω) and v(x, θ, ω) are biased, and the bias
decreases as the Markov chain mixes. The mixing time corresponds to the logarithmic term log K
in (17) and (18). Because of asynchrony, at a given iteration, workers collect different number of
samples and their chains mix to different degrees. The worker with the slowest mixing chain will
determine the rate of convergence. The product of Ko and log K in (17) and (18) appears due to the
slowest mixing chain. As the last term in (17) dominates other terms asymptotically, the convergence
rate reduces as the number of workers increases. While the theoretical linear speedup is difficult to
establish in the Markovian setting, we will empirically demonstrate it in Section 5.2.
7
Under review as a conference paper at ICLR 2021
d<33-do⅛2o
100	200	300
Training Tims (sec.)
5 0 5 0
7 7 6 6
---- workers = 1
workers = 2
---- workers = 4
workers = 8
workers = 16
100	200	300
Training Time (sec.)
5 0 5 0 5 0
7 7 6 6 5 5
Figure 1: Convergence results of A3C-TD(0) with i.i.d. sampling in synthetic environment.
d<33-3⅞20
---- workers = 1
workers = 2
----workers = 4
---- workers = 8
workers = 16
100	200	300
Training Tims (sec.)
Figure 2: Convergence results of A3C-TD(0) with Markovian sampling in synthetic environment.
5	Numerical Experiments
We test the speedup performance of A3C-TD(0) on both synthetically generated and OpenAI Gym
environments. The settings, parameters, and codes are provided in supplementary material.
5.1	A3C-TD(0) in synthetic environment
To verify the theoretical result, we tested A3C-TD(0) with linear value function approximation
in a synthetic environment. We use tabular softmax policy parameterization [36], which satisfies
Assumption 3. The MDP has a state space |S| = 100, an discrete action space of |A| = 5. Each state
feature has a dimension of 10. Elements of the transition matrix, the reward and the state features are
randomly sampled from a uniform distribution over (0, 1). We evaluate the convergence of actor and
critic respectively with the running average of test reward and critic optimality gap ∣∣ωk - ωθj∣2.
Figures 1 and 2 show the training time and sample complexity of running A3C-TD(0) with i.i.d.
sampling and Markovian sampling respectively. The speedup plot is measured by the number of
samples needed to achieve a target running average reward under different number of workers. All
the results are average over 10 Monte-Carlo runs. Figure 1 shows that the sample complexity of
A3C-TD(0) stays about the same with different number of workers under i.i.d. sampling. Also,
it can be observed from the speedup plot of Figure 1 that the A3C-TD(0) achieves roughly linear
speedup with i.i.d. sampling, which is consistent with Corollary 1. The speedup of A3C-TD(0) with
Markovian sampling shown in Figure 2 is roughly linear when number of workers is small.
5.2	A3C-TD(0) in OpenAI Gym environments
We have also tested A3C-TD(0) with neural network parametrization in the classic control (Carpole)
environment and the Atari game (Seaquest and Beamrider) environments. In Figures 3-5, each curve
is generated by averaging over 5 Monte-Carlo runs with 95% confidence interval. Figures 3-5 show
the speedup of A3C-TD(0) under different number of workers, where the average reward is computed
by taking the running average of test rewards. The speedup and runtime speedup plots are respectively
measured by the number of samples and training time needed to achieve a target running average
reward under different number of workers. Although not justified theoretically, Figures 3-5 suggest
that the sample complexity speedup is roughly linear, and the runtime speedup slightly degrades when
the number of workers increases. This is partially due to our hardware limit. Similar observation has
also been obtained in async-SGD [9].
8
Under review as a conference paper at ICLR 2021
Average Reward	Arerage Reward	Average Reward
---- workers ≡ 1
——workers ≡ 2
---- workers = 4
---- workers = β
workers ≡ 16
25	50	75	100
TraInIngTme(Set)
Training Samples
1e4
8	16
N u mber of Wo rkers
NumberofWortors
Figure 3: Speedup of A3C-TD(0) in OpenAI gym classic control task (Carpole).
---- workers ≡ 1
---- workers = 2
---- workers = 4
workers = 8
workers =Λf
Training Time (hrs.)
EeMsαα>6ejα)>q
0.0	0.2	0-4	0-6	0.8	1.0
Training Samples Ie6
β	16
Number OfWorkars
0.2	0.4 OS OA
Training Samples
Figure 4: Speedup of A3C-TD(0) in OpenAI Gym Atari game (Seaquest).
Figure 5: Speedup of A3C-TD(0) in OpenAI Gym Atari game (Beamrider).
6	Conclusions
This paper revisits the A3C algorithm with TD(0) for the critic update, termed A3C-TD(0). With linear
value function approximation, the convergence of the A3C-TD(0) algorithm has been established
under both i.i.d. and Markovian sampling settings. Under i.i.d. sampling, A3C-TD(0) achieves linear
speedup compared to the best-known sample complexity of two-timescale AC, theoretically justifying
the benefit of parallelism and asynchrony for the first time. Under Markov sampling, such a linear
speedup can be observed in most classic benchmark tasks.
References
[1]	T. P Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra,
“Continuous control with deep reinforcement learning,” in Proc. OfInternational Conference on
Learning Representations, 2016.
[2]	V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves,
M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep rein-
forcement learning,” Nature, vol. 518, no. 7540, p. 529, 2015.
[3]	V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and
K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning.” in Proc. ofInterna-
tional Conference on Machine Learning, 2016.
9
Under review as a conference paper at ICLR 2021
[4]	A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. De Maria, V. Panneershelvam,
M. Suleyman, C. Beattie, S. Petersen et al., “Massively parallel methods for deep reinforcement
learning,” arXiv preprint:1507.04296, 2015.
[5]	M. Assran, J. Romoff, N. Ballas, J. Pineau, and M. Rabbat, “Gossip-based actor-learner
architectures for deep reinforcement learning,” in Proc. of Advances in Neural Information
Processing Systems, 2019.
[6]	V. Konda, Actor-critic algorithms. PhD thesis, Department of Electrical Engineering and
Computer Science, Massachusetts Institute of Technology, 2002.
[7]	R. Sutton, D. McAllester, S. Singh, and Y. Mansour, “Policy gradient methods for reinforcement
learning with function approximation.” in Proc. of Advances in Neural Information Processing
Systems, 2000.
[8]	R. Sutton, “Learning to predict by the methods of temporal differences,” Machine Learning,
vol. 3,pp. 9-44,1988.
[9]	X. Lian, H. Zhang, C. Hsieh, Y. Yijun Huang, and J. Liu, “A comprehensive linear speedup
analysis for asynchronous stochastic parallel optimization from zeroth-order to first-order,” in
Proc. of the Advances in Neural Information Processing Systems, 2016.
[10]	V. Borkar and V. Konda, “The actor-critic algorithm as multi-time-scale stochastic approxima-
tion,” Sadhana, vol. 22, no. 4, pp. 525-543, 1997.
[11]	S. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee, “Natural actor critic algorithms,”
Automatica, vol. 45, pp. 2471-2482, 2009.
[12]	Z. Yang, K. Zhang, M. Hong, and T. BaSar,“A finite sample analysis of the actor-critic algorithm,”
in Proc. of IEEE Conference on Decision and Control, 2018, pp. 2759-2764.
[13]	H. Kumar, A. Koppel, and A. Ribeiro, “On the sample complexity of actor-critic method for
reinforcement learning with function approximation,” arXiv preprint:1910.08412, 2019.
[14]	S. Qiu, Z. Yang, J. Ye, and Z. Wang, “On the finite-time convergence of actor-critic algorithm,”
in Optimization Foundations for Reinforcement Learning Workshop at Advances in Neural
Information Processing Systems, 2019.
[15]	T. Xu, Z. Wang, and Y. Liang, “Improving sample complexity bounds for (natural) actor-critic
algorithms,” in Proc. of Advances in Neural Information Processing Systems, 2020.
[16]	——, “Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms,”
arXiv preprint:2005.03557, 2020.
[17]	Y. Wu, W. Zhang, P. Xu, and Q. Gu, “A finite time analysis of two time-scale actor critic
methods,” in Proc. of Advances in Neural Information Processing Systems, 2020.
[18]	M. Hong, H.-T. Wai, Z. Wang, and Z. Yang, “A two-timescale framework for bilevel opti-
mization: Complexity analysis and application to actor-critic,” arXiv preprint:2007.05170,
2020.
[19]	M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, “Reinforcement learning through
asynchronous advantage actor-critic on a gpu,” in Proc. of International Conference on Learning
Representations, 2017.
[20]	A. Stooke and P. Abbeel, “Accelerated methods for deep reinforcement learning,” arXiv
preprint:1803.02811, 2019.
[21]	L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
I. Dunning, S. Legg, and K. Kavukcuoglu, “Impala: Scalable distributed deep-rl with importance
weighted actor-learner architectures,” arXiv preprint:1802.01561, 2018.
[22]	D. Bertsekas and J. Tsitsiklis, Parallel and distributed computation: numerical methods.
Prentice-Hall, 1989.
10
Under review as a conference paper at ICLR 2021
[23]	A. Agarwal and J. Duchi, “Distributed delayed stochastic optimization,” in Proc. of Advances in
Neural Information Processing Systems, 2011.
[24]	H. Feyzmahdavian, A. Aytekin, and M. Johansson, “An asynchronous mini-batch algorithm for
regularized stochastic optimization,” arXiv preprint:1505.04824, 2015.
[25]	X. Lian, Y. Huang, Y. Li, and J. Liu, “Asynchronous parallel stochastic gradient for nonconvex
optimization.” in Proc. of Advances in Neural Information Processing Systems, 2015.
[26]	T. Sun, R. Hannah, and W. Yin, “Asynchronous coordinate descent under more realistic assump-
tions,” in Proc. of Advances in Neural Information Processing Systems, 2017.
[27]	X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu, “Can decentralized algorithms
outperform centralized algorithms? a case study for decentralized parallel stochastic gradient
descent,” in Proc. of Advances in Neural Information Processing Systems, 2017.
[28]	R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement
learning,” Machine Learning, vol. 8, no. 3-4,pp. 229-256, May 1992.
[29]	J. Baxter and P. L. Bartlett, “Infinite-horizon policy-gradient estimation,” J. Artificial Intelligence
Res., vol. 15, pp. 319-350, 2001.
[30]	R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT Press, 2018.
[31]	V. Konda and V. Borkar, “Actor-critic-type learning algorithms for markov decision processes,”
SIAM Journal on Control and Optimization, vol. 38, no. 1, pp. 94-123, 1999.
[32]	J. Bhandari, D. Russo, and R. Singal, “A finite time analysis of temporal difference learning
with linear function approximation.” in Proc. of Conference on Learning Theory, 2018.
[33]	T. Xu, Z. Wang, Y. Zhou, and Y. Liang, “Reanalysis of variance reduced temporal difference
learning,” in Proc. of International Conference on Learning Representations, 2020.
[34]	S. Zou, T. Xu, and Y. Liang, “Finite-sample analysis for SARSA with linear function approxi-
mation,” in Proc. of Advances in Neural Information Processing Systems, 2019.
[35]	K. Zhang, A. Koppel, H. Zhu, and T. BaSar,“Global convergence of policy gradient methods to
(almost) locally optimal policies,” arXiv preprint:1906.08383, 2019.
[36]	A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan, “Optimality and approximation with
policy gradient methods in markov decision processes.” in Proc. of Thirty Third Conference on
Learning Theory, 2020.
[37]	K. Doya, “Reinforcement learning in continuous time and space,” Neural Computation, vol. 12,
no. 1, pp. 219-245, 2000.
[38]	R. Sutton, H. Maei, D. Precup, S. Bhatnagar, D. Silver, and E. Szepesvari, C.and WieWiOra,“Fast
gradient-descent methods for temporal-difference learning with linear function approximation,”
in Proc. of International Conference on Machine Learning, 2009.
[39]	A. Y. Mitrophanov, “Sensitivity and convergence of uniformly ergodic markov chains,” Journal
of Applied Probability, vol. 42, no. 4, pp. 1003-1014, 2005.
[40]	Dgriff, “Pytorch implementation of a3c,” https://github.com/dgriff777/rl_a3c_pytorch, 2018.
11
Under review as a conference paper at ICLR 2021
Supplementary Material
A Preliminary Lemmas
A.1 Geometric Mixing
The operation P 0 q denotes the tensor product between two distributions p(χ) and q(y), i.e. (P 0
q)(χ,y) = P(X) ∙ q(y).
Lemma 1. Suppose Assumption 4 holds for a Markov chain generated by the rule at 〜∏θ (∙∣st),
st+1 〜Pe(∙∣st, at). For any θ ∈ Rd, we have
sup dτv (P((St,at,st+ι) ∈ ∙∣so,∏θ),μθ 0 ∏θ 0pe) ≤ κρt.	(20)
where μθ (∙) is the stationary distribution with policy ∏θ and transition kernel Pe(∙∣s, a).
Proof. We start with
sup dTV P((st, at, st+1)
s0∈S
∙∣so,∏θ),μθ 0 ∏θ 0
Pe
sup dTV
s0∈S
St = ∙∣S0,∏θ) 0 ∏θ 0 P,μθ 0 ∏θ 0
sup2/ x/
s0∈S 2 s∈S a∈A s0∈S
00
∣P(st = ds∣so, ∏θ)∏θ(a∣s)P(ds0∣s, a) — μθ(ds)∏θ(a∣s)P(ds0∣s, a)
sup ʒ I	∣P(st = ds∣so,∏θ) — μθ(ds)| X ∏θ(a|s) ∣	P(ds0∣s,a)
s0∈S 2 s∈S	a∈A	s0∈S
=sup dτv (P(St ∈ ∙∣so,∏θ),μθ)
s0∈S
≤ κPt,
which completes the proof.	□
For the use in the later proof, given K > 0, we first define mK as:
mκ := min { m ∈ N+ | κρm-^1 ≤ min{ak, βk}} ,	(21)
where κ and ρ are constants defined in (4). mK is the minimum number of samples needed for the
Markov chain to approach the stationary distribution so that the bias incurred by the Markovian
sampling is small enough.
A.2 Auxiliary Markov Chain
The auxiliary Markov chain is a virtual Markov chain with no policy drifting — a technique developed
in [34] to analyze stochastic approximation algorithms in non-stationary settings.
Lemma 2. Under Assumption 1 and Assumption 3, consider the update (9) in Algorithm 1 with
Markovian sampling. For a given number of samples m, consider the Markov chain of the worker
that contributes to the kth update:
θk-dm	Pe	θk-dm-1	θk-d1	Pe	θk-d0	Pe
st-m -------→ at-m -→ st-m+1 ---------→ at-m+1 •…st-1 --------→ at-1 —→ St -----→ at -→ st+1,
where (St, at, St+1) = (S(k), a(k), S0(k)), and {dj}jm=0 is some increasing sequence with d0 := τk.
Given (St-m, at-m, St-m+1) and θk-dm, we construct its auxiliary Markov chain by repeatedly
applying πθk-dm:
θk-dm	Pe	θk-dm	θk-dm	Pe	θk-dm	Pe
st-m -------→ at-m	-→ st-m+1 -------→	at-m+1	•…st-1 -------→	at-1	-→ st -----→	at	-→	st+1∙
Define xt := (St, at, St+1), then we have:
dTV (P(Xt ∈ 1θk-dm, st-m+1), P(Xt ∈ 1θk-dm, st-m+1))
1	dm
≤ 5|/|Ln X E [kθk-i — θk-dm ∣∣2∣θk-dm,, st-m+1 ] ∙	(22)
i=τk
12
Under review as a conference paper at ICLR 2021
Proof. Throughout the lemma, all expectations and probabilities are conditioned on θk-dm and
st-m+ι. We omit this condition for convenience.
First we have
dτv (P(St+1 ∈∙),P(Rt+ι ∈∙))
=I /	∣p(st+ι = as0)一 p(et+i = ds0)∣
2 JSgS
=ɔ / I X ): P(St = ds, at = a, st+1 = ds ) - P(St = ds, at = a, st+1 = ds )
2 ∕s'∈s Ps∈s aeA
≤ J /	/ X	IP(St = ds,	at	= a, st+ι =	ds0)	- P(et = ds,	at
=2 /	X /	∣P(st = ds,	at	= a, st+ι =	ds0)	- P(et = ds, Rt
=dτv (P(Xt ∈∙),P(Rt ∈∙)),
a,Rt+ι = ds0)∣
a,Rt+ι = ds0)∣
(23)
where the last second equality is due to Tonelli,s theorem. Next we have
dτv (P(Xt ∈∙),P(Rt ∈∙))
=2 J): J	IP(St = ds, at = a, st+ι = ds) - P(st = ds, at = a, st+ι
ds0)∣
∣P(st = ds,	at	= a) —	P(Rt	= ds, Rt	= a)∣ /	Pa(st+ι	=	ds0∣st = ds, at	=	a)
J s0 ∈S
∣P(st = ds, at = a) - P(sat = ds, aat = a)∣
dτv (P ((St ,at) ∈∙), P (Rt, Rt) ∈∙)) ∙
(24)
Due to the fact that θk-τk is dependent on st, we need to write P(st, at) as
P(st,at) = /	P(st ,θk-τk ,at)
Jθk-τk ∈Rd
2 LS X
2 L X
P	P(st )P(θk-Tk
√θ∈Rd
P(St) I	P(θk-Tk
√θ∈Rd
dθ∣st)πθ 一k (at∣st)
dθ∣st)πθ 一k (at∣st)
P(St)E[πθ -k (at∣st)∣st]∙
Then we have
dτv (P((st,at) ∈∙),P((Rt,Rt) ∈ ∙))
1 /es X IP(St = dS)E[πθk-τk (at
a∣st = ds)∣st = ds] - P(Rt = ds)πek-dm (at = a∣Rt = ds)
≤ 1 /GS X I P(St = dS)E[πθk-τk (at
a∣st = ds)∣st = ds] - P(St = ds)∏θfc-dm (at = a∣St = ds)
+ 1 /WS X I P(St = ds)πθk-dm (et = a∣et = ds) - P(Rt = ds)πθk-dm (et = a∣et = ds) I
1	I	P(St = ds) X IE[πθk-τk (at = a∣st = ds)∣st = ds] - πθk-dm (at
2	g M 1
+ 1 / ∣P(st = ds) - P(Rt = ds)∣ ∙
2 Js∈S
a∣st = ds)
(25)
13
Under review as a conference paper at ICLR 2021
Using Jensen’s inequality, we have
dτv(P((St,at) ∈∙),P((et,et) ∈∙))
≤ 2 I	P(St	=	dS)	X Eh 卜θk-τk (at	= a|St =	dS)-	πθk-dm (at
s∈S	a∈A
a|st = ds) st = ds
+ 2/ JP(St = ds) — P(et = ds)∣
11
≤ 2 J SP(St = dS)E E [kθk-τk - θk-dm l∣2 | St= ds] + 2
=2 lAlLπ E kθk-τk - θk-dm k2 + dTV (P(St ∈ ∙), P(st ∈ ∙))
|P(st =ds) - P(set = ds)|
s∈S
(26)
where the last inequality follows Assumption 3.
Now we start to prove (22).
dτv(P(xt ∈ ∙),P(et ∈ ∙)) (=)dτv(P((St,at) ∈ ∙),P((et,et) ∈ ∙))
(25)	1
≤ dTV (P(St ∈ ∙), P(St ∈ ∙)) + 2 IAlLn E kθk-Tk - θk-dm k2
(23)	1
≤ dTV (P(Xt-I ∈ ∙), P(Xt-I ∈ ∙)) + 2 |A|Ln E kθk-τk - θk-dm k2 ∙
Now we have
dτv (P(χt ∈∙),P(et ∈∙)) ≤ dτv (P(χt-ι ∈∙),P(et-ι ∈ ∙)) + 1 ∣a∣l∏ El%-τk - θk-dmk2.
(27)
Since dτv (P(Xt-m ∈ ∙), P(xt-m ∈ ∙)) = 0, recursively applying (27) for {t 一 1,..., t 一 m} gives
1m
dτv (P(Xt ∈ ∙), P(Xt ∈ ∙)) ≤ 5 |A|Ln X E kθk-dj — θk-dm k2
2	j=0
1 dm
≤ X |A|Ln X E kθk-i 一 θk-dm k2,
i=τk
which completes the proof.	□
A.3 Lipschitz Continuity of Value Function
Lemma 3. Suppose Assumption 3 holds. For any θ1 , θ2 ∈ Rd and S ∈ S, we have
∣∣VV∏θι (S)k2 ≤ Lv,
IVπθ1 (S) 一 Vπθ2 (S)I ≤ LV lθ1 一 θ2l2,
(28a)
(28b)
where the constant is LV := Cψ rmax /(1 一 γ) with Cψ defined as in Assumption 3.
Proof. First we have
∞
Qπ (S, a) = E	γtr(St, at, St+1)IS0 = S, a0 = a
t=0
∞
≤	γtrmax
t=0
rmax
1 - Y
14
Under review as a conference paper at ICLR 2021
By the policy gradient theorem [7], we have
l∣vV∏θ1 (s)k2 = ∣∣E [Q∏θ1 (S, a)ψθι(S, a)]||2
≤ E Qπθ1 (s, a)ψθ1 (s, a)2
≤ EIjQnθ1 (s, a)lkψθι Ga)∣3
≤
rmax
1 - Y
Cψ,
where the first inequality is due to Jensen’s inequality, and the last inequality follows Assumption 3
and the fact that Q∏ (s, a) ≤ r1m-γx. By the mean value theorem, We immediately have
∣V∏θι (S)- V∏θ2 (s)l ≤ sup ∣∣vv∏θι (s)∣∣2 kθ1-θ2k2 = LVkθι - Θ2k2,
1	2	θ1 ∈Rd	1
Which completes the proof.
□
A.4 Lipschitz Continuity of Policy Gradient
We give a proposition regarding the LJ -Lipschitz of the policy gradient under proper assumptions,
Which has been shoWn by [35].
Proposition 1. Suppose Assumption 3 and 4 hold. For any θ,θ0 ∈ Rd, we have ∣∣VJ(θ) 一
VJ(θ0)∣2 ≤ LJ∣∣θ 一 θ0∣2, where LJisapositive constant
A.5 Lipschitz Continuity of Optimal Critic Parameter
We provide ajustifiCation for LiPschitz continuity of ω3 in the next proposition.
Proposition 2. Suppose Assumption 3 and 4 hold. For any θ1 , θ2 ∈ Rd, we have
lω或一 ωθ2 ∣2 ≤ Lω kθ1 一 θ2∣∣2,
where Lω := 2rmax∣A∣L∏(λ-1 + λ-2(1 + γ))(1+logρ KT + (1 — P)T).
Proof. We use A1 , A2, b1 and b2 as shorthand notations of Aπθ , Aπθ , bπθ and bπθ respectively.
By Assumption 2, Aθ,φ is invertible for any θ ∈ Rd, so we can write ωθ = -A-φbθ,φ. Then we have
∣∣ωl - ω2∣2 = k - A-1b1 + A-Ib2 k2
= ∣ 一 A1-1 b1 一 A1-1 b2 + A1-1 b2 + A2-1 b2 ∣2
= ∣ 一 A1-1 (b1 一 b2) 一 (A1-1 一 A2-1 )b2 ∣2
≤ ∣A1-1 (b1 一	b2)∣2 + ∣(A1-1 一 A2-1)b2∣2
≤ ∣A1-1 ∣2 ∣b1	一 b2 ∣2 +	∣A1-1 一 A2-1 ∣2 ∣b2 ∣2
= ∣A1-1 ∣2∣b1	一 b2∣2 +	∣A1-1 (A2 一 A1 )A2-1 ∣2∣b2∣2
≤ ∣A1-1 ∣2 ∣b1	一 b2 ∣2 +	∣A1-1 ∣2 ∣A2-1 ∣2 ∣b2 ∣2 ∣(A2	一 A1 )∣2
≤ λ-1 ∣b1 一 b2 ∣2 + λ-2rmax ∣A1 一 A2 ∣2 ,	(29)
where the last inequality follows Assumption 2, and the fact that
kb2∣2 = kE[r(S,a,sO)φ(S)]k2 ≤ Ellr(S,a, SO)φ(S)Il2 ≤ E[∣r(s,a,SO)Ii∣φ(S)II2] ≤ rmax.
Denote (s1, a1, so1) and (S2,a2, so2) as samples drawn with θ1 and θ2 respectively, i.e. s1 〜 μθι,
a1 〜 ∏θι, S01 〜 P and s2 〜μe2, a2 〜∏e?, so2 〜Pe. Then we have
∣bι - b2k2 = ∣∣E [r(S1,a1, s")Φ(s1)] - E [r(S2, a2, s02)Φ(s2)] ∣∣2
≤ sup ∣∣r(s,a,s0)φ(s)k2∣∣P((sl,al,s01) ∈∙) — P((s2,a2,so2) ∈∙)∣∣tv
s,a,s0
≤ rmax∣P((s1, a1, S01) ∈∙) 一 P((s2,a2, S02) ∈ ∙)∣tv
=2rmaχdτv (μθι 0 ∏θι 0 P, μθ2 0 ∏θ2 0 P)
≤ 2rmaxIAILπ (1 + logρ κ-1 + (1 一 ρ)-1)∣θ1 一 θ2 ∣2,	(30)
15
Under review as a conference paper at ICLR 2021
where the first inequality follows the definition of total variation (TV) norm, and the last inequality
follows Lemma A.1. in [17]. Similarly we have:
∣∣Aι - A2k2 ≤ 2(1 + γ)dτv (μθι 0 ∏θι,μθ2 0 ∏θ2)
=(1 + γ)∣A∣L∏(1+logρ κ-1 + (1- ρ)-1)kθ1 -Θ2k2.	(31)
Substituting (30) and (31) into (29) completes the proof.	□
B Proof of Main Theorems
B.1 Proof of Theorem 1
For brevity, we first define the following notations:
x := (s, a, s0),
δ(x,ω) := r(s, a, s0) + γφ(s0)>ω — φ(s)>ω,
g(x, ω) := δ(x, ω)φ(s),
g(θ,ω) :=	E , ~[g(X,M].
S 〜μθ,a 〜∏θ,s0 〜P
We also define constant Cδ := rmax + (1 + Y) max{rjm-γx, Rω}, and We immediately have
kg(x,ω)∣2 ≤ |r(x) + γφ(s0)>ω - φ(s)>ω∣ ≤ rmax +(1 + γ)Rω ≤ Cδ	(32)
and likewise, we have ∣∣g(χ,ω)∣2 ≤ Cδ.
The critic update in Algorithm 1 can be Written compactly as:
ωk+1 = ΠRω ωk + βkg(x(k), ωk-τk) ,	(33)
where τk is the delay of the parameters used in evaluating the kth stochastic gradient, and x(k) :=
(s(k), a(k), s0(k)) is the sample used to evaluate the stochastic gradient at kth update.
Proof. Using ωk as shorthand notation of ωθfc, we start with the optimality gap
kωk+1 - ωk+1k2
=kπRω (ωk + βkg(x(k), ωk-τk )) - ωfe+1k2
≤ ∣ωk + βk g(x(k) , ωk-τk ) - ωk+1 ∣2
kωk- ωk	∣∣2	+ 2βk〈叫	- ωk, g(X(k),ωk-τk)) +2(叫-ωk,ωk-以+J + ||以-ωt+ι + β g(X(防,叫-丁广	)∣∣2
kωk	- ωk	∣∣2	+ 2βk (ωk	- ωk, g(x(k), ωk-Tk	) - g(x(k),ωk)) + 2fk (ωk - ωk, g(x(k), ωk ) - g(θk, ωk ))
I CQ /	* —(Γ∖ ∖∖∣c∕	**	*∖ Ill *	* IQ ~/	∖ll2
+ 2βk hωk - ωk, g(θk, ωk )i + 2 ∖ωk - ωk ,ωk - ωk+l∕ + ∣∣ωk - ωk+1 + Bk g(x(k),ωk-τk )∣∣2
≤ kωk - ωk k2 + 2βk (ωk - ωk, g(x(k), ωk-Tk ) - g(x(k),ωk)) + 2βk <ωk - ω*, g(x(k), ωk ) - g(θk, ωk ))
+ 2βk hωk - ω*,g(θk, ωk )i + 2 <ωk - ωk,ωk - ωk+l) + 2 ∣∣ωk - ω*+i∣∣2 + 2C2 β2.
We first bound M - ω*,g(θk, ωk)〉in (34) as
hωk—ωk,g(θk ,ωk)i = hωk—ω*,g(θk ,ωk)—g(θk ,ωk)i
= Dωk - ωk*, E h(γφ(s0) - φ(s))> (ωk - ωk*)φ(s)iE
= Dωk - ωk*, E hφ(s) (γφ(s0) - φ(s))>i (ωk - ωk*)E
= ωk - ωk*, Aπθk (ωk - ωk*)
≤ -λ∣ωk - ωk* ∣22 ,
(34)
(35)
where the first equality is due to g(θ,ωθ) = Aθ,φMθ + b = 0, and the last inequality follows
Assumption 2. Substituting (35) into (34), then taking expectation on both sides of (34) yield
E ∣ωk+1 - ωk*+1 ∣22 ≤ (1 - 2λβk) E ∣ωk - ωk* ∣22 + 2βk E ωk - ωk*, g(X(k), ωk-τk) - g(X(k), ωk))
+ '28k E <ωk - ωk, g(X(k),ωk) - g(θk, ωk)) + 2 E (ωk - ωk,ωk - ωk + l)
+2E∣∣ωk* -ωk*+1∣∣22+2Cδ2βk2.	(36)
16
Under review as a conference paper at ICLR 2021
We then bound the term E Qk - ω^,g(x(k),ωk-τk) - g(x(k),ωk)〉in (36) as
E(ωk-ωk,g(X(k),ωk-Tk) -g(X©,心))= E(心-ωk, (γφ(Sjk)) - φ(s(k))	(ωk-τk-ωk)φ(s(k))
≤ (1 + Y) E [kωk - ωkk2kωk-τk - ωkk2]
≤ (I + Y) E llωk - ωkk2 X (ωi+1 - ωi)ll
i=k-τk	2
k-1
≤ (1 + Y)E kωk - ωfck2 X βikg(xi,ωi-Ti )k2
i=k-τk
k-1
≤ (1 + Y)E kωk - ωfck2 X βk-Ko kg(xi,ωi-Ti )k2
i=k-τk
≤ Cδ (1 + Y)KOek-Ko E kωk - ωkk2,	(37)
where the second last inequality is due to the monotonicity of step size, and the last inequality follows
the definition of Cδ in (32).
Next we jointly bound the fourth and fifth term in (36) as
2 E (ωk- ωk,ωk- ωk+ι) +2 EII媒-媒+1∣I2
≤ 2 E [kωk - ωkk2 Hωk - ωk + i∣2 ] + 2 EII 媒一媒+ill2
≤ 2Lω E [∣∣ωk - ωk k2 kθk - θk+1k2] + 2L： Ekθk - θk + 1k2
=2Lω αk E [kωk - ωkk2 IIδ(x(k),ωk-τk )ψθk-τk (S (k) a(k))I□ + 2Lω α2 E Hδ(x(k),ωk-Tk )ψθk-τk (S (k) a(k))I2
≤ 2LωCpak Ellωk - ωkk2 + 2L：Cpαk,	(38)
where constant Cp := Cδ Cψ. The second inequality is due to the Lω -LiPSchitz of ω^ shown in
Proposition 2, and the last inequality follows the fact that
..ʌ, . . , ... ....
(39)
lδ(X(k) , ωk-τk )ψθk-τk (s(k) , a(k) )l2 ≤ CδCψ = Cp .
Substituting (37) and (38) into (36) yields
ElIωk + 1 - ωk + 1k2 ≤ (1 - 2λβk) e kωk - ωk k2 + 2βk (C1-^ + CzKoBk-Ko ) e kωk - ω和2
βk
+ 2βk E <ωk - ωk ,g(x(k), ωk ) - g(θk, ωk )) + Cq βk ,	(4O)
22
where Ci ：= LωCp, C ：= Cδ(1 + Y) and Cq := 2C2 + 2比禺 max(k)βk = 2C2 + 2比玛∣2.
For brevity, we use X 〜θ to denote S 〜μθ, a 〜∏ and s0 〜P in this proof. Consider the third
term in (40) conditioned on θk, ωk, θk-τk. We bound it as
E [<ωk - ωk, g(x(k), ωk ) - g(θk ,ωk ) ) | θk ,ωk, θk-Tk ]
=13k - ωk E [g(x(k),ωk)∣ωk] - g(θk,ω)
∖	x(k)~θk-τk
=Ik - ωk,~g(θk-Tk , ωk ) - g(θk, ωk),
≤ kωk - ωkk2k9(θk-Tk , ωk) - g(θk ,ωk)∣∣2
≤ 2Rω I E	[g(X, ωk)] - E [g(X, ωk)]I
X 〜θk-τk	X 〜θk	2
≤ 2Rω sup kg(x,3k)∣∣2 IIμθk-τk 0 ∏θk-τk XP - μθk 0 ∏θk xPIItv
≤ 4RωCδdτv(μθk-τk 0 ∏θk-τk 0 P, μθk 0 ∏θk 0 P),	(41)
17
Under review as a conference paper at ICLR 2021
where second last inequality follows the definition of TV norm and the last inequality uses the
definition of Cδ in (32).
Define constant C3 := 2RωCδ∣A∣L∏(1 + logρ KT + (1 - P)T). Then by following the third item
in Lemma A.1 shown by [17], we can write (41) as
E [<ωk - ωk, g(x(k), ωk ) - g(θk ,ωk » | θk ,ωk, θk-τk ]
≤ 4RωCδdτv(μθk-τk X πθk-τk X P, μθk X πθk X P)
≤ C3 kθk-τk - θk k2
k-1
≤ C3	αikg(xi, ωi-τi)k2
i=k-τk
≤ C3CδK0αk-K0 ,	(42)
where we used the monotonicity of αk and Assumption 1.
Taking total expectation on both sides of (42) and substituting it into (40) yield
E kωk+1 - ωk+1 k2 ≤ (I - '2λ8k) E kωk - ωkk2 + 2βk (CI 些 + C2K0βk-Ko) E kωk - ωkk2
βk
+ 2C3CδK0βkαk-K0 +Cqβk2.	(43)
Taking summation on both sides of (43) and rearranging yield
KK	K
2λ X E kωk - ω和 2 ≤ X ɪ (E∣∣ωk - ω和 2 - E∣∣ωk+ι - ωQ 1 肩 +Cq X βk
k 2	k 2	+	k+1 2 q
k=K0	k=K0	k=K0
I	I	I	I
Il	I2
K	Kα
+ 2 ^X 2C3CδKOak-K0 +2 ^X (CI ~β^ + C2K0βk-Ko ) E kωk - ωkk2 .
k=K0	k=K0	k
I	I I	I
I3	I4
(44)
We bound I1 as
I1
K1
X β (Ekωk-
k=MK	k
ω
kk2 - E ∣∣ωk+1 -煤+I∣∣2)
=kXK& -
≤ X (β-
k=MK	k
βk-1
E kωk -媒k2 +
βMK -1
E ∣ωMK
-ωM k∣∣2 - ； E∣∣
ωK+1 - ωK +1∣∣2
E---) E kωk - ωk I∣2 + 飞----E ∣∣ωMκ
βk-1	βMK -1
-ωM κ∣∣2
≤ 4Rω (XK& - +)+ 3
4Rω
βk
O(Kσ2),
(45)
1
1
where the last inequality is due to the fact that
kωk - ωθk2 ≤ kωk k2 + ∣∣ωθ∣∣2 ≤ 2Rω∙
We bound I2 as
KK
X βk = X
k=MK k=MK
C
(1 + k)σ2
O(K1-σ2)
(46)
where the inequality follows from the integration rule Pbk =ak-σ ≤ b-σ,
18
Under review as a conference paper at ICLR 2021
We bound I3 as
K	K-K0
I3 = X 2C3CδK0αk-K0 =2C3Cδc1K0 X (1 + k)-σ1 = O(K0K1-σ1).
k=K0	k=0
(47)
For the last term I4, we have
I4
+C + GKofk-Ko)
βk	0
E kωk - ωkk2
≤t
≤t
K α	2 u
X Sβk + c2κ0βk-κ0) t
κ α	、2
kX°(°1 βk+c2κ0βk-κ0) t
K
X (E∣M-ωkk2)2
k=K0
K
X E∣M-ωkk2,
k=K0
(48)
where the first inequality follows CaUchy-SchWartz inequality, and the second inequality follows
Jensen’s inequality. In (48), we have
K	2 K-K0	2
SX(Ci	—k	+	C2K0βk-K0) ≤	SX(Ci	—k	+	C2K0βk)
k=K0	βk	k=0 βk
K-K0 2	K-K0	K-K0
C2 X α +2C1C2Ko X αk + C2κ2 X e2
k=0	k	k=0	k=0
O (K2S2-σI)+1) + O (K0K-σ1 + 1) + O (K0K 1-2σ2) (49)
where the first inequality is due to the fact that αβk and βk-K are monotonically decreasing.
Substituting (49) into (48) gives
_________________________________________ K
I4 ≤ Jo (K2S2-σI)+1)+ o (KoK-σι+1) + O(KOK 1-2σ2). X E∣M - “kk；.
k=MK
(50)
Substituting (45), (46), (47) and (50) into (44), and dividing both sides of (44) by K - Ko + 1 give
1K
2λK- KO + 1 X Ekωk-ωkk2
o k=K0
/O (K2(σ2-σι)+1) + O (KoK-σι + 1) + O (K2K1-2σ2)
≤ X--------------------- ----------------------------
+O
K1-σ2
K-Ko+ 1
+ o (Kσ ) + o (KKh
K
X E kωk - ω*k2
k=K0
(51)
∖
1
We define the following functions:
T1(K)
:= K - Ko + 1
K
X Ekωk-ωkk2 ,
k=K0
T2(K)
:= O
K1-σ2
+o (焉)+ o (K
T3(K)
O (K2(σ2-σι)+1) + O (K0K-σ1 + 1) + O (K2K 1-2σ2
K-Ko+1
1
1
19
Under review as a conference paper at ICLR 2021
Then (51) can be written as:
TI(K)- ɪpTWpT3(K) ≤ ɪTzg.
2λ	2λ
Solving this quadratic inequality in terms of T1(K), we obtain
TI(K) ≤ λT2(K) + 2λ2 T3(K),
(52)
which implies
1K
K-E X Ek"-2
k=K0
1
O
K1-σ2
+ O ( K2(σ1-σ2) ) + O (j⅛) + O (1K01) + O (Kσ2
We further have
1 K	1
Ek EE kωk - ωk k2 ≤
KK
k=1
1K
4Rω + X E kωk—ωkk2
k=K0
KK14Rω+
K-K0+1
K K - K0 +1
K
X E∣M-ωkk2
k=K0
1
O (K) + O (K - Ko + 1 Xo E k"k - 媒k2
1K
O	K-E X EkωTk2
k=K0
which completes the proof.
□
B.2 Proof of Theorem 2
We first clarify the notations:
x := (s, a, s0),
δ(x, ω) := r(s, a, s0) + γφ(s0)>ω — φ(s)>ω,
δ(x, θ) := r(s, a, s0) + γVπθ (s0) - Vπθ (s).
The update in Algorithm 1 can be written compactly as:
θk+1 = θk + αk δ(x(k) , ωk-τk )ψθk-τk (s(k) , a(k) ).
For brevity, We use ω% as shorthand notation of ω旋.Then We are ready to give the proof.
Proof. From LJ -Lipschitz of policy gradient shown in Proposition 1, we have:
J (θk+1) ≥ J (θk ) + hV J (θk), θk + 1 - θk i-2 kθk+1 - θk k2
(53)
(54)
J(θk) + αk〈VJ(θk),(，
ʌ
ʌ
，/一	、	，/一	「*、\」. /一
δ(x(k) , ωk-τk ) - δ(x(k) , ωk ) ψθk-τk (s(k) , a(k)
+αk DvJ (θk),δ(x(k),ω 却他-飞(S (k),。⑻)〉—LJ akkδ(x(k),ωk-τk )ψθk-τk (S W ,a(k))k2
≥ J(θk) + αk NJ(θk), j
ʌ
ʌ
，/一	、	，/一	「*、\」. /一
δ(x(k) , ωk-τk ) - δ(x(k) , ωk ) ψθk-τk (s(k) , a(k)
+ αk DVJ (θk),δ(x(k),ωk')ψθk-τk (s(k), a(k))E-----J Cpak ,
where the last inequality follows the definition of Cp in (39).
20
Under review as a conference paper at ICLR 2021
Taking expectation on both sides of the last inequality yields
E[J (θk+ι)] ≥ E[J (θk)] + αk E WJ (θk), G(X(k) ,ωk-τk) — <^x(k),ωk" ψθk-τk (S(k),a(kD)
I	I
Il
+αk E NJ (θk), $(X(k),ωk)ψθk-τk(s(k), a(k) )Ε —2 Cpa2.	(55)
I	I
I2
We first decompose I1 as
I1
E (VJ (θk), (δ(χ(k),ωk-τk)- j(χ(k),ωk)) ψθk-τk(s(k),a(k)))
E (VJ (θk ), G(X(k),ωk-Tk ) — δ(x(k),ωk )) ψθk-τk (S (k),a(k)
1	I11
+ E (VJ (θk ), (δ(x(k),ωk) — δ(x(k),ωk)) ψθk-τk (S (k),a(k)
l12)
We bound I1(1) as
I1(1) = E VJ(θk), γφ(S0(k)) — φ(S(k))	(ωk-τk — ωk)ψθk-τk (S(k), a(k))
≥ — E hkVJ(θk)k2kγφ(S0(k)) — φ(S(k))k2kωk — ωk-τkk2kψθk-τk (S(k), a(k))k2i
≥ -2Cψ E [kVJ(θk)∣∣2kωk - ωk-Tkk2]
≥ -2CψCδK0βk-1 EIlVJ(θk)k2,
where the last inequality follows
k-1
Iωk — ωk-τk I2 =	(ωi+1 — ωi)
i=k-τk	2
k-1
≤	Iβig(Xi, ωi-τi )I2
i=k-τk
k-1
≤ βk-1	Ig(Xi, ωi-τi )I2
i=k-τk
≤ βk-1K0Cδ,
where the second inequality is due to the monotonicity of step size, and the third one follows (32).
Then we bound I1(2) as
I(2) = E (VJ(θk), G(X(k),ωk) — i(x(k),ωk)) ψθk-τk (s(k),a(k)))
=- E (VJ(θk), (γφ(s(k)) - φ(s(k)))	(ωk — ωk)ψθk-τk (s(k),a(k)),
≥ - E h∣VJ (θk)k2kγφ(s(k)) — Φ(s(k))k2 kωk — ωk∣2∣ψθk-τk (S(k),a(k))k2i
≥ -2Cψ E[∣∣VJ(θk)∣2∣ωk — ωk∣2] .
Collecting the lower bounds of I1(1) and I1(2) gives
I1 ≥ -2cψ E [∣∣vj(θk)k2 (CδKOβk-1 + kωk — ωk∣∣2)] .	(56)
21
Under review as a conference paper at ICLR 2021
Now we consider I2. We first decompose I2 as
I2 = E (VJ (θ k), δ (X (k), ωk)ψ θk - Tk(S (k), α(k)),
=E (VJ ®), G(X(k),ωk)- δ(X(k),ωk-Tk)) ψθk-Tk(S(k), a(k)))
I	I
i2I)
+ E (VJ (θk), (j(χ(k),ωk-Tk)- δ(X(k),θk-Tk )) ψθk-τk (s(k),α(k)),
I	'	I
以
+ E DVJ(θk), δ(x(k),θk-Tk)ψθk-τk (s(k),α(k)) - VJ(θk))+kVJ(θk)k2∙
We bound l21) as
i2I) = E (VJ (θk), G(X(k), ωk)- δ(χ(k),ωk-Tk)) ψθk-τk (s(k), a*)))
=E (VJ(θk), (γφ(s(k))- φ(s(k)“ 3k -4-Tk) ψθk-Tk(S(k), α(k)),
T
>-E kVJ (θk )k2 k (γφ(s(k)) - φ(s(k)) k2 ∣∣ωk - ωk-Tk∣∣2 I*%』(s(k),α(k))k2
> -LV Cψ (I+ Y) EII 媒-ωk-Tk∣∣2
> -LV* Lω Cψ (I + Y) ElI θk - θk-Tk ∣∣2
> -LV Lω Cψ Cp(I + Y )K0ak-K0 ,
where the second last inequality follows from Proposition 2 and the last inequality uses (39) as
k-1
∣∣θk - θk-Tkk2 ≤ X ∣∣θi+1 - θi∣∣2
i=k-Tk
k-1
= E αiM(Xi,"-Ti)ψθi-Ti(Si, Oi)∣∣2
i=k-Tk
k-1
≤ E αk-Tk Cp
i=k-Tk
≤ CpK0αk-K0∙	(57)
We bound I2(2) as
I22) = E (VJ (θk ), (i(X(k),媒-Tk)- S(X(k)/k-Tk )) ψθk-τk (s(k),α(k)))
≥
>
>
>
>
-	E [∣∣VJ(θk)∣∣2 I δ(x(k),ω^-Tk) - δ(x(k),θk-Tk) I kψθk-Tk (S(k),α(k))k2]
-	Cψ E IVJ(θk)∣∣2 δ(x(k),媒-Tk)- δ(x(k), θk-τk) I ]
-	Cψ E [∣∣VJ(θk)∣∣2
-	Cψ E [∣∣VJ(θk)∣∣2
-	Cψ E [∣∣VJ(θk)∣∣2
咚%』(加))) + ‰fc-τfc(s(k))- Φ(s(k))τωk-τk ]
v∏θk-τk (s(k))∣ + IVπ%-τk (s(k)) - φ(s(k))>ωJτk )]
-	V∏θk-TJS(k))| + I v∏θfc-τfc (s(k)) - φ(s(k))>媒-TJ I θk, θk-Tk ]]
-	2CψLVefa - 2Cψesp EkVJ(θk)∣∣2
(58)

2
22
Under review as a conference paper at ICLR 2021
where the second last inequality follows from the fact that
E [γ I φ(s(k))τωk-τk - ‰fc-τfc (s(k)) I + "θ-k (s(k)) - φ(s(k))>ωk-τJ]
≤ Y JE 1 φ(s(k))>ωk-Tk -咚 j (s(k))∣ + JE 1 咚θk-τk (S(k)) - φ(s(k))>ωk-Tk 1
≤ 25pp.
Define artificial transition X(k) ：= (s(k), α(k), %)〜 P), then I(3) can be bounded as
i(3) = E NJ(Ok), δ(X(k),θk-τk )ψθk-τk (s(k),a(k)) - VJ(Ok))
=E IE KVJ (Ok ),δ(X(k),θk-Tk )ψθk-Tk(S(k),a(k)) -VJ (Ok ))I
Ok-Tk ,θk]]
=E DVJ (Ok ), E [ (δ(x(k), Ok-Tk ) - δ(X(k), Ok-Tk )) ψθk-τk (S(k), a(k)) | Ok-τ%，Ok])
+ E (VJ(Ok),E [δ(X(k),Ok-Tk)Ψθk-τk (S(k),a(k)) ∣ Ok-Tk,Ok] -VJ(Ok))
≥ - E UVJ (Ok )k2 IlE [(S(X(k),Ok-Tk ) - S(X(k),Ok-Tk )) ψθk-τk (S(k),α(k) ) l>k-Tk ,Ok ] ∣l2]
-E [∣∣VJ (Ok )∣2∣∣E [δ(X(k), Ok-Tk )Ψθj (S(k),a(k)) ∣ Ok-Tk ,Ok] -VJ (Ok )∣∣J .	(59)
The first term in the last inequality can be bounded as
E [(S(X(k), Ok-Tk) - S(X(k), Ok-Tk)) ψθk-Tk(S(k), a(k)) l Ok-Tk , Ok]
=E [(δ(X(k),Ok-Tk )-6(X(k), Ok-Tk))ψθk-Tk(S(k),a(k)) l Ok-Tk , Ok
=E [(r(X(k)) + YE[r(Sk, a0, S)] - (r(X(k)) + YE[r国,a0, s")])) ψθk-Tk(S(k), a(k)) | 0k-Tk, Ok]
≤ 2Cψrmax∣∣P - PIlTV
≤ 8cψrmax(l - y),
(60)
where the last inequality follows
IIP - PIlTV = 2 f	IP(Sls,a)	—p(Sls,a)∣ =	2(I-	Y)	[	IP(Sls,a) — η(s0)∣ ≤	4(I-	γ).
s0∈S	s0∈S
(61)
The second term in (59) can be rewritten as
EIJ(X(k),Ok-Tk )ψθk-τk(S(k) ,α(k))∣ Ok-Tk , Ok]
Sg-E5	(r(X(k)) + YV∏θk-τ1. (5(k))-咚θk-τli,(S(k))) ψθk-τk(S(k), a(k)) Ok-Tk ,Ok
∖k) μθk- Tk I ×	k	k	/
α(k)〜πθk-τk
[ k)~P
E
S(k)〜μθk-Tk
α(k)~πθk-Tk
(S(k), a(k) ) - Vπθk-Tfc (S(k))J ψθk-Tk(S(k), a(k)) Ok-Tk , Ok
E
S(k)〜μθk-Tk
α(k)~πθk-Tk
Aπθk-Tfc (S(k), a(k))ψθk-Tk(S(k), a(k)) Ok-Tk , Ok
E
S(k)~dθk-Tk
α(k)~πθk-Tk
Aπθk-Tfc (S(k), a(k))ψθk-Tk (S(k), a(k)) Ok-Tk , Ok = VJ (Ok-Tk )
(62)
where the second last equality follows μθ(∙) = dθ(∙) with dθ being a shorthand notation of dπ0 [6].
23
Under review as a conference paper at ICLR 2021
Substituting (60) and (62) into (59) yields
l23)	≥ -8Cψ rmax(1 - Y)EkVJ (θk )k2 - E[kVJ (θk)∣∣2∣∣VJ (θj) -VJ (θk )|3
≥ -8Cψrmax(1 - Y)EkVJ(θk)k2 - LVLJ E∣∣θi - θk k2
≥ -8Cψrmax(1 -γ)EkVJ(θk)k2 -LVLJCpK0αk-K0,	(63)
where the second last inequality is due to LJ -Lipschitz of policy gradient shown in Proposition 1,
and the last inequality follows (57).
Collecting lower bounds ofI2(1), I2(2) and I2(3) gives
I2 ≥ -D1K0αk-K0 - (2Cψsp + 8Cψrmax(1 -Y))EkVJ(θk)k2 -2CψLVfa+ kVJ(θk)k22,
(64)
where the constant is D1 := LVLωCψCp(1 + Y) + LVLJCp.
Substituting (56) and (64) into (55) yields
E[J (θk+1)] ≥ E[J (θk )] - 2αk Cψ (QP + 4rmax(I- Y) + Cδ KOek-I + kωk - ωk k2) EkVJ (Ek)Il 2
-αk D1K0αk-Ko - 2αkCψ LV efa + αk kVJ (θk )k2-2 Cpak ∙	(65)
By following Cauchy-Schwarz inequality, the second term in (65) can be bounded as
(esp + 4rmax(* 1 - Y) + CδKOek-I + kωk - ωk k2) EkVJ(θk) k2
≤ JEkVJ(θk)k2 E [(esp + 4rmaχ(1 - Y) + CδKoβk-i + |距一“就⑻2]
≤ ,EkVJ(θk)k2 Je 性C2K2β2-i + 4kωk - ω和2 + 4e2p + 64小视乂(1 - y)2]
=2,EkVJ(θk)k2qCK2β2-1 + Ekωk- ω"∣2 + O(e2p),	(66)
where the last inequality follows the order of esp in Lemma 7.
Collecting the upper bound gives
E[J (θk+ι)] ≥ E[J (θk)] - 4ak Cψ ,E∣∣VJ (θk)∣∣2 JCiK耐T+ E∣∣ωk-ωk ∣∣2 + O(e∖p)
-ak DIKO ak-Ko - 2ak Cψ LV efa + akkVJ (θk )∣∣2-Cpak .	(67)
Dividing both sides of (67) by ak, then rearranging and taking summation on both sides give
K	K1	K	L
EEkVJ(θk)k2 ≤ E 一 (E[J(Θk+1)]- E[J(θk)])+£	DiKOak-K0 + -JCpak
k=K0	k=K0 ak	k=K0
I	I I	I
I3	I4
K _____________ _______________________________
+ 4Cψ X EkkVJ(θk)k2 Jc2κ2e2-ι + Ekωk- ω∕k2 + O(e2p)
k=K0
] ]
I5
+2CψLV(K-KO+1)efa.	(68)
We bound I3 as
K
I3 = X
k=K0
:(E [J(θk+ι)]- E[J(θk)])
K
X
k=K0
ak-1	ak
-)E[J (θk)]——1
aMK -1
EJ (θMK)] + OK E[J (θK+1)]
1
—
≤ --E [J(θK+l)]
aK
≤ -^maxɪ =O(Kσι),
- Y aK
(69)
24
Under review as a conference paper at ICLR 2021
where the first inequality is due to the αk is monotonic decreasing and positive, and last inequality is
due to V∏θ (S) ≤ rmx for any S ∈ S and ∏θ.
We bound I4 as
K
I4 = X
k=K0
αk-Ko +-2 Cpak) ≤ ^X (DIKOak +--2 Cpak )=O(KOK 1-01 )∙
We bound I5 as
K ____________
I5= X JEkVJ(θk)k2,CsKses-I + Ekωk- ωkks + O(esp)
k=K0
K
K
≤t
EEkVJ(θk)kst E (CsKsβs-i + Ekωk-ω和s + O(esp))
K
k=K0
K
E kVJ(θk)ksstCδsKOs	β
k=K0
k=K0
K
K- + X E kωk - ωkk2 + O(Ke2p),
k=K0
(70)
where the first inequality follows Cauchy-Schwartz inequality.
In (70), we have
K
X βk2-1
k=K0
K -K0	K -K0
≤ X βk2 = X c22(1 + k)-2σ2 =O(K1-2σ2).
k=0
k=0
Substituting the last equality into (70) gives
I5 ≤ t
E E kVJ (θk )kst O(KsK 1-2σ2)+ E E kωk - ω* ks + O(Kesp).
k=MK
k=MK
(71)
∖
K
K
Dividing both sides of (67) by K - K0 + 1 and collecting upper bounds of I3, I4 and I5 give
1K
K-E X EkVJ(θk)k2
k=K0
≤	4Cψ
≤ K - Ko + 1 ∖
Ku
X EkVJ(θk)k2t
k=K0
K
O(K0K 1-2σ2)+ X Ekωk-ωkk2 + O(Kesp)
k=K0
1
+O
K1-σ1
+ OIK0"
+ O(fa).
(72)
Define the following functions
T4(K)
1K
K-E X EkVJ(θk)k2,
k=K0
T5(K)
K - K +1 (θ(K2K 1-2σ2)+ X E kωk - ωkk2 + O(Ke2p)),
0	k=K0


1
+ O(K
T6(K)
:= O
K1-σ1
+ O(fa).
Then (72) can be rewritten as
T4(K) ≤ T6(K) + √2(1 + Y)CψPT4(K)√T5(K).
25
Under review as a conference paper at ICLR 2021
Solving this quadratic inequality in terms of T4 (K), we obtain
T4(K) ≤ 2T6(K)+4(1+γ)2Cψ2T5(K),
(73)
which implies
1K
K-K0+1 X EkVJ(θk)k2
k=K0
=O (K1σι) + O (KK0) + O (-K02) + O (K-K + ι AXcEkωk-说k2) + O&pP).
We further have
1 K	1 K0-1	K
K XEkVJ(θk)k2 ≤ K X LV + X EkVJ(θk)k2
k=1	k=1	k=K0
=KK1LV + K-K0+1 K-⅛Γ X EkVJ(θk)k2
0	k=K0
+ O K - Ko + 1 X EkVJ(%)k2
0	k=K0
=O K-K0+1 X EkVJ(θk)k2
0	k=K0
which completes the proof.
(74)
□
B.3 Proof of Theorem 3
Given the definition in Section B.1, we now give the convergence proof of critic update in Algorithm
1 with linear function approximation and Markovian sampling.
By following the derivation of (40), we have
Ekωk + 1 - ωk + 1k2 ≤ (I - 2λβk) e kωk - ωk k2 + 2βk (C1-^- + C2K0βk-Κo ) e kωk - ωk k2
βk
+ 2βk E (ωk - ωk , g(x(k),ωk ) - g(θk, ωk ))+ Cq βk,	(75)
22
where Ci ：=	CpLω, C2	：=	Cδ(1 +	Y)	and	Cq	：= 2C2 +	2LCp	max(k)徭=2C + 2L3Cp.
Now we consider the third item in the last inequality. For some m ∈ N+ , we define M :=
(K0 + 1)m +K0. Following Lemma 4 (to be presented in Sec. C.1), for some dm ≤ M and positive
constants C4, C5 , C6 , C7, we have
E gk — ωk,g(x(k),ωk) — g(θk,ωk))
dm
≤ C4 E kθk — θk-dm k2 + C5 X E kθk-i — θk-dm k2 + C6 E kωk — 3k-dm ∣g + C7κρm-i
i=τk
k-i	dm -i k-i-i	k-i
≤ C4 X E kθi+i — θi k2 + C5 X X E kθj+i — θj k2 + C6 X E k3i+i — 3i k2 + C7κρm-i
i=k-dm	i=τk j=k-dm	i=k-dm
k-i	dm-i k-i-i	k-i
≤C4	X	αiCp	+	C5	X X	αjCp+C6	X	βiCδ+C7κρm-i
i=k-dm	i=τk j=k-dm	i=k-dm
k-i	dm-i k-i-i	k-i
≤ C4αk-dm X Cp + C5αk-dm X X Cp + C6βk-dm X Cδ + C7κρm-i
i=k-dm	i=τk j=k-dm	i=k-dm
≤ C4dmCpαk-dm +C5(dm — τk)2Cpαk-dm + C6dmCδβk-dm + C7κρm-i
≤ (C4M + C5M2) Cpak-M + C6MCδβk-M + C7κρm-i,	(76)
26
Under review as a conference paper at ICLR 2021
where the third last inequality is due to the monotonicity of step size, and the last inequality is due to
τk ≥ 0 and dm ≤ M .
Further letting m = mK which is defined in (21) yields
E l∖3k - ωl, g(x(k),ωk ) - g(θk, ωk »
=(C4Mκ + C5MK) Cpαk-MK + C6Cδ MK βk-MK + C7κρmK -1
≤ (C4MK + C5MK) Cpαk-MK + C6Cδ MK βk-MK + C7αK ,	(77)
where MK = (K0 + 1)mK + K0, and the last inequality follows the definition of mK.
Substituting (77) into (75), then rearranging and summing up both sides over k = MK, ..., K yield
KK	K
2λ X Ek3k-ωM∣2 ≤ X Ir (Ekωk-ωkk2 - EM+ι-ωk+ι∣∣2)+Cq X βk
k=MK	k=MK k	k=MK
I	I	I	I
Iι	I2
K
+ 2 ^X ((C4MK + C5MK) Cpak-MK + C6CδMKβk-Mκ + C7αK)
k=MK
I	I
I3
Kα
+ 2 ^X (CI 停 + C2K0βk-Ko
k=MK	βk
E k3k - ωkk2 .
(78)
I4
where the order of I1 , I2 and I4 have already been given by (45), (46) and (50) respectively.
We bound I3 as
K	KK
I3 =	(。4MK	+ C5MK2)	Cp	X ak +	C6CδMK	X	βk +	C7aκ	X 1
k=MK	k=MK	k=MK
K 1-σ1	K 1-σ2
≤ (。4MK + C5MK) Cpci 1——+ °6°δMKc21——+ C7c1K(1 + K)-σ1
1 - σ1	1 - σ2
=O ((K2 log2 K)K1-σ1) + O ((K0 log K)K1-σ2),	(79)
where the last inequality follows from the integration rule Pbk=a k-σ ≤ b-σ, and the last equality
is due to O(MK) = O(K0mK) = O(K0logK).
Collecting the bounds of I1, I2, I3 and I4, and dividing both sides of (78) by K - MK + 1 yield
1K
2λK - MK + 1 X Ek31 k2
K	k=MK
∖ O (K2(σ2-σι)+i) + O (KoK-σι+i) + O (K0K 1-2σ2)
≤ Y---------------------------------------------- A
≤	K - MK + 1	∖
K
X Ek3k-3kk2
k=MK
+O
K1-σ2
+O
K)+O(K⅛手
(80)
1
Similar to the derivation of (52), (80) implies
K - MK + 1
K
X Ek3k-3kk2
k=MK
1
O
K1-σ2
+ O (忌F ) + o ( K⅛ )+o
+ O fKologK
+ I K σ
1
27
Under review as a conference paper at ICLR 2021
Similar to (53), we have
KK XXEM-ωkk2 = O (K0KgK)+O (K - M XX EM-ωkk2
k=1	k=MK
=O ( K-MK + 1 XX Ekωk-4 H
K	k=MK
which completes the proof.
(81)
□
B.4 Proof of Theorem 4
Given the definition in section B.2, we now give the convergence proof of actor update in Algorithm
1 with linear value function approximation and Markovian sampling method.
By following the derivation of (55), we have
E[J (θk+ι)] ≥ E[J (θk 力 + αk E WJ (θk), G(X(k) ,ωk-τk) — <^x(k),ωk" ψθk-τk (S(k),a(kp)
1 ' 1
Il
+ αk E NJ (θk ), ^(x(k), ωk)ψθk-τk (s(k), a(k) )Ε-----2 Cipak .	(82)
I	I
I2
The item I1 can be bounded by following (56) as
I1 ≥ -2Cψ E [kVJ(θk)k2 (cδK0βk-1 + kωk - ωk k2)] ∙	(83)
Next we consider I2. We first decompose it as
I2 = E NJ(θk),δ(x(k),ωk)ψθk-τk (S(k), a(k)))
=E〈vJ (θk), G(X(k),ωk)- δ(x(k),θk)) ψθk-τk (S(k),a(k)),
I	I
ι21)
+ E DVJ(θk), δ(x(k), θk)ψθk-τk (S(k), a(k)) - VJ(θk)E + EkVJ(θk)k2.	(84)
For some m ∈ N+, define M := (K0 + 1)m + K0. Following Lemma 5, for some dm ≤ M and
positive constants D2, D3, D4, D5, I2(1) can be bounded as
I21)	= E (VJ (θk), G(X(k), ωk) —δ(X(k), θk)) ψθk-τk (S(k), a(k)))
k-τk
≥ -D2	E kθk-τk	- θk-dm k2 -	D3 E kθk	-	θk-dm k2 -	D4	X	E kθi	- θk-dm	k2
i=k-dm
-D5κρm-1 -2CψLVfa-2CψspEkVJ(θk)k2
≥ -D2 (dm - τk )Cpαk-dm - D3dmCpαk-dm - D4 (dm - τk)2Cpαk-dm
-D5κρm-1 -2CψLVfa-2CψspEkVJ(θk)k2,	(85)
where the derivation of the last inequality is similar to that of (76).
By setting m = mK in (85), and following the fact that dmK ≤ MK and τk ≥ 0, we have
I2 ≥ -D2MK Cpαk-MK - D3MKCpαk-MK - D4MK2 Cpαk-MK - D5 κρmK -1
- 2Cψ LV fa - 2Cψ sp E kVJ (θ)k2
=—((Dk + D3 )Cp MK + D4CpMK) αk-MK -D5κρmK-1 -2CψLVfa-2CψspEkVJ(θk)kk
≥ — ((D2 + D3 )Cp MK + D4CpMK) αk-MK - D5αK - 2Cψ LV fa - 2Cψ sp E kVJ (θk)kk,
(86)
28
Under review as a conference paper at ICLR 2021
where the last inequality is due to the definition of mK .
Following Lemma 6, for some positive constants D6 , D7, D8 and D9, we bound I2(2) as
I22)	= E NJ(θk), δ(x(k), θQψθk-τk (s(k), a(k)) - NJ(θk)E
dm
≥ -D6 E kθk-τk - θk-dm k2 - D7 E kθk - θk-dm k2 - D8 X E kθk-i - θk-dm k2
i=τk
-D9κρm-1 -8Cψrmax(1 -γ)EkNJ(θk)k2.
Similar to the derivation of (86), we have
I2(2) ≥ - (D6 + D7 + D8MK) CpMKαk-MK -D9αK - 8Cψrmax(1 -γ)EkNJ(θk)k2. (87)
Collecting the lower bounds of I2(1) and I2(2) yields
I2 ≥ -2CψLV fa - 2Cψ (sp + 4rmax(1 -γ))EkNJ(θk)k2 +EkNJ(θk)k22
-	DK αk-MK - (D5 + D9)αK,	(88)
where we define DK := (D4 + D8)CpMK2 + (D2 + D3 + D6 + D7)CpMK for brevity.
Substituting (83) and (88) into (82) yields
E[J(θk+ι)] ≥ E[J(θk)] - 2αkCψ E [∣∣NJ®)∣∣2 (QP + 4小(1 - γ) + CδKoβk-ι + |皿-ωk∣∣2)]
-	ak (DK ak-Mκ + (D5 + D9)αK ) - 2Cψ LV efaαk + αk EkNJ (θk )k2-2j Cpa2 .
Similar to the derivation of (67), the last inequality implies
E[J (θk+ι)] ≥ E[J (θk)] - 4akCψ qE kVJ (θk )k2 qC2K2β2-i + Ekωk-ωk∣∣2 + O(⅛J
-	ak (DKak-Mκ + (D5 + D9)aK) - 2CψLV∈faak + ak E kVJ(Ok)Il2-----2CPak∙
Rearranging and dividing both sides by αk yield
EkVJ(θk)k2 ≤ O- (E[J(Θk+1)] - E[J(θk)]) + DKαk-MK + (D5 + D9 )aK + -JCpak
+	4CΨ qE kVJ (θk )k2 qC2K2β2-1 + E kωk - ωkk2 + O(E2p) + 2Cψ LV efa∙
Taking summation gives
KK
X EkVJ(θk)k2 ≤ X 屋(E[J(θk+ι)] -E[J(θk)])
k=MK	k=MK k
I	I
I3
KL
+ ɪ2 ( DKak-Mκ + C2apc^+ + (D5 + D9)aK I
k=MK
I	I
I4
K _________ ____________________
+ 4Cψ X JE kVJ (θk)k2 JC2K2 β2-i + E kωk - ωM∣2 + O(e2p)
k=MK
I	I
I5
+ 2CψLV(K - MK + 1)efa.	(89)
in which the upper bounds of I3 and I5 have already been given by (69) and (71) respectively.
29
Under review as a conference paper at ICLR 2021
We bound I4 as
I4
KL
=ɪ2 ( DKαk-Mκ +--Cpαk + (D5 + D9)αK )
k=MK
KL
≤ ɪ2 ( DKαk-Mκ +-2Cipak-MK + (D5 + D9)αK
k=MK
DK +
DK +
≤	DK +
K
X
k=MK
K-MK
X
αk-MK + (D5 + D9)(K - MK + 1)αK
αk + (D5 + D9)(K - MK + 1)αK
k=0
Tɪ K 1-σ1 + cι(D5 + D9)(K + 1)1-σ1
1 - σ1
O ((K0 log2 K)K 1-σ1)
(90)
where the last inequality uses Pk=a k-σ ≤ b11-σ, and the last equality is due to the fact that
O(DK) = O(MK2 +MK) = O((K0mK)2 + K0mK) = O(K02 log2 K).
Substituting the upper bounds of I3, I4 and I5 into (89), and dividing both sides by K - MK + 1 give
1K
K - MK + 1 X EkVJ(θk)k2
K	k=MK
≤	4Cψ
≤ K - MK + 1 ∖
E EkVJ (θk)k2t O(K02K 1-2σ2)+ E Ekωk-ωkk2 + O(K⅛J
k=MK
k=MK
+O
K1-σ1
+O
K + O(fa).
(91)
1
K
K
Following the similar steps of those in (73), (91) essentially implies
1K
K - MK + 1 X EkVJ(θk)k2
K	k=MK
O
K1-σ1
+O
2K
+O
+o	K-IMKTI
XX E kωk - ωθk k2) +O(≡app).
k=MK
1
Similar to (74), we have
ɪXXEkVJ(Ok)k2 = O(K0詈K) + O(K「］	XX EkVJ(θk)k2
K	K	K - MK + 1
k=1	K	k=MK
O (K-MKTI jχjkVJ (θk )k2
which completes the proof.
□
30
Under review as a conference paper at ICLR 2021
C S upporting Lemmas
C.1 Supporting Lemmas for Theorem 3
Lemma 4. For any m ≥ 1 and k ≥ (K0 + 1)m + K0 + 1, we have
dm
E (ωk - ωθk , g(x(k), ωk ) - g(θk, ωk )) ≤ C4 E kθk - θk-dm ∣∣2 + C5 X E kθk-i - θk-dm ∣∣2
i=τk
+ C6 E ∣ωk - ωk-dm ∣2 + C7 κρm-1,
where dm ≤ (Ko + 1)m + Ko, and C4 := 2CδLω + 4RωCδ∣A∣L∏(1 + logρ KT + (1 — P)T),
C5 ：= 4RωCδ ∣A∣L∏ and。6 ：= 4(1 + γ)Rω + 2Cδ, C7 ：= 8RωCδ.
Proof. Consider the collection of random samples {x(k-K0-1), x(k-K0), ..., x(k)}. Suppose x(k) is
sampled by worker n, then due to Assumption 1, {x(k-K0-1), x(k-K0), ..., x(k-1)} will contain at
least another sample drawn by worker n. Therefore, {x(k-(K0+1)m), x(k-(K0+1)m+1), ..., x(k-1)}
will contain at least m samples from worker n.
Consider the Markov chain formed by m + 1 samples in {x(k-(K0+1)m), x(k-(K0+1)m+1), ..., x(k)}:
θk-dm	Pe	θk-dm-1	θk-d1	Pe	θk-d0	Pe
st-m -------→ at-m -→ st-m+1 --------→ at-m+1 •…st-1 -------→ at-1 -→ st -----→ at -→ st+1,
where (st, at, st+1) = (s(k), a(k), s0(k)), and {dj}jm=o is some increasing sequence with do ：= τk.
Suppose θk-dm was used to do the kmth update, then we have xt-m = x(km). Following Assumption
1, we have τkm = km — (k — dm) ≤ Ko. Since x(km) is in {x(k-(K0+1)m), ..., x(k)}, we have
km ≥ k — (Ko + 1)m. Combining these two inequalities, we have
dm ≤ (Ko + 1)m + Ko .	(92)
Given (st-m, at-m, st-m+1) and θk-dm, we construct an auxiliary Markov chain as that in Lemma
2:
θk-dm	P	θk-dm	θk-dm	P	θk-dm	P
st-m -------→	at-m	-→	st-m+1 ------→	at-m+1	•…st-1 ------→ at-1	-→	st -----→	at	-→	st+1.
For brevity, we define
∆ι(x, θ, ω) := hω — ωθ,g(x, ω) — g(θ, ω)i .
Throughout this proof, we use θ, θ0, ω, ω0, x and xe as shorthand notations of θk, θk-dm, ωk, ωk-dm,
xt and xet respectively.
First we decompose ∆1(x, θ, ω) as
∆1(x, θ, ω) = ∆1(x, θ, ω) — ∆1(x, θ0, ω) + ∆1(x, θ0, ω) — ∆1(x, θ0, ω0)
I	I I	I
Il	I2
+ ∆1(x, θ0, ω0) — ∆1(xe, θ0, ω0) + ∆1(xe, θ0, ω0).	(93)
I	I I	I
I3	I4
We bound I1 in (93) as
δi(x, θ, ω) ― δi(x, θ0, ω) = hω ― ωθ,g(x, ω) ― g(θ, ω)i - hω ― ωθo,g(x, ω) ― g(θ0, ω)i
≤ lhω 一 ωθ,g(x,ω) 一g(θ,ω)i 一 hω — ωθ0,g(x,ω) 一g(θ,ω)il
+ lhω — ωθ)0 ,g(x,ω) - g(θ,ω)i - hω - ωθo ,g(x,ω) - g(θ0,ω)il .
(94)
For the first term in (94), we have
lhω — ωθ,g(x,ω) — g(θ,ω)i — hω — ωθo ,g(x,ω) — g(θ,ω)il = lhωθ — ωθo ,g(x,ω) — g(θ,ω)il
≤ kωθ — ωθo l∣2kg(x,ω) — g(θ,ω)k
≤ 2Cδ ∣ωθθ — ωθθ0 ∣2
≤ 2CδLω∣θ — θ0∣2,
31
Under review as a conference paper at ICLR 2021
where the last inequality is due to Proposition 2.
We use X 〜θ0 as shorthand notations to represent that S 〜μgo, a 〜∏θo, s0 〜P. For the second
term in (94), we have
lhω - ωθ0,g(x,ω) -g(θ,ω)i - hω - ωθ0,g(x,ω) -g(θ0,ω)il
=lhω - ωθ)0,g(θ0,ω) - g(θ,ω)il
≤ kω - ωθ0 l∣2kg(θ0, ω) - g(θ, ω)k2
≤ 2Rωkg(θ0,ω) - g(θ,ω)∣2
= 2Rω	E [g(x, ω)] - E [g(x, ω)]
X〜θ0	X〜θ	2
≤ 2Rω sup kg(x,ω)∣2 kμθ0 0 ∏θo 0P - μθ 0 ∏θ XPkTV
X
≤ 2Rω Cδ kμθ0 0 ∏θ0 0P- μθ0 πθ 0 P kTV
=4RωCδdτv (μθo 0 ∏θo 0P,μθ 0 ∏θ 0 P)
≤ 4RωCδ∣A∣L∏(1 + logρ KT + (1 - ρ)-1)kθ - θ0∣∣2,
where the third inequality follows the definition of TV norm, the second last inequality follows (32),
and the last inequality follows Lemma A.1. in [17].
Collecting the upper bounds of the two terms in (94) yields
I1 ≤ [2Cδ Lω + 4Rω Cδ IAlLn(I + logρ K 1 + (I- P)I)] kθ - θ0∣∣2.
Next we bound E[I2] in (93) as
E[I2] = E[∆1 (x, θ0, ω) - ∆1(x, θ0, ω0)]
=E hω — ωθo ,g(x,ω) — g(θ0,ω)i — hω0 — ωθo ,g(x,ω) — g(θ0,ω0)i
≤ E lhω — ωθ,g(χ,ω) — g(θ0,ω)i -hω — ωθ,g(χ,ω) — g(θ0,ω0)il
+ E lhω - ωθθ ,g(X, ω0 ) - g(θ0, ω' )i - hω0 - ωθo,g(X, ωO)- g(θ0, ω0)il .	(95)
We bound the first term in (95) as
E lhω - ωθ0,g(x, ω) - g(θ0, ω)i -hω - ωθ0,g(x, ω0) - g(θ0, ω0)il
=E lhω — ωθo ,g(x,ω) — g(X,ωO) + g(θ0,ω0) — g(θ0,ω)il
≤ 2Rω (Ekg(X,ω) - g(x,ω0)k2 + Ekg(θ0,ω0) - g(θ0,ω)∣∣2)
≤ 2Rω E kg(X, ω) - g(X, ωO)k2 + E E [g(X, ωO)] - E [g(X, ω)]
∖	X〜θ0	X〜θ0	2 J
= 2Rω E k(γφ(sO) - φ(s))>(ω - ωO)k2 + E E [(γφ(sO) - φ(s))> (ωO - ω)
∖	x~θ'	2
≤ 2Rω ((1 + γ)Ekω - ωOk2 + (1 + γ)Ekω - ωOk2)
= 4Rω(1 + γ) E kω - ωOk2.
We bound the second term in (95) as
E lhω - ωθo ,g[x, ωO) - g(θ0, ωO)i - hω0 - ωθo ,g(X, ωO) - g(θ0, ω0)il
=E lhω — ω, g(X, J)-或θ, ω')il
≤ 2CδEkω - ωOk2.
Collecting the upper bounds of the two terms in (95) yields
E[I2] ≤ (4(1+γ)Rω+2Cδ)Ekω-ωOk2.
We first bound I3 as
E[I3IθO, ωO, st-m+1] = E [∆1(X, θO, ωO) - ∆1(Xe, θO, ωO)IθO, ωO, st-m+1]
≤ IE [∆1(X, θO, ωO)IθO, ωO, st-m+1] - E [∆1(Xe, θO, ωO)IθO, ωO, st-m+1]I
≤ SUP @小/”, ω') IkP(X ∈ lθ',j, st-m+1 ) - P(X ∈ "θ',j, st-m+1)kτv
X
≤ 8RωCδdTV (P(X ∈ 1θl st-m+1 ), P(X ∈ lθl st-m+1)) ,	(96)
32
Under review as a conference paper at ICLR 2021
where the second last inequality follows the definition of TV norm, and the last inequality follows the
fact that
∣∆ι(x,θ0,ω0)∣ ≤ ∣∣ω0 - ω&∣∣2kg(x,ω0) - g(θ0,ω0)∣∣2 ≤ 4RωCδ.
By following (22) in Lemma 2, we have
1	dm
dTV (P(X ∈ ∙∣θ0, st-m+1), P(x ∈ ∙∣θ0, st-m+1)) ≤ j∣ AlLn X E [∣∣θk-i - θk-dm ∣∣2 | θ0, st-m+1 ] .
i=τk
Substituting the last inequality into (96), then taking total expectation on both sides yield
dm
E[I3] ≤ 4RωCδlAlLπ X E ∣θk-i - θk-dm ∣2.
i=τk
Next We bound I4. Define x := (s, a, s0) where S 〜μjo, a 〜∏qo and s0 〜P. It is immediate that
E[∆ι(X, θ0, ω0)∣θ0, ω0, st-m+1] = (ω0 - ω], E[g(x, ω0)∣θ0, ω0, St-m+1] - g(θ0, ω0))
=hω — ωθ0,9(θ,ω) - g(θ0,ω0)i = 0∙	M)
Then we have
E[I4∣θ0,ω0,St-m+1] = E [∆ι(ee, θ0,ω0) - ∆ι(x, θ0,ω0)∣θ0,ω0, St-m+1 ]
≤ ∣E∆1 (X, θ0,ω0)∣θ0,ωf, St-m+1] - E∆1 (X, θ0,ω0)∣θ0,ω0,St-m+1]∣
≤ sup ∣∆1(x,θ0, ω0)∣∣P(X ∈∙∣θ0, st-m+1) - P(X ∈∙∣θ0, st-m+1)kτv
x
≤ 8Rω Cδ dτv (P(X ∈∙∣θ0, St-m+1), P(X ∈ ∙∣Θ0, St-m+1))
=8RωCδdτv (P(X ∈ ∙∣θ0, St-m+1), μθ0 0 ∏θ0 0 P) ,	(98)
where the second inequality follows the definition of TV norm, and the third inequality follows (97).
The auxiliary Markov chain with policy πθ0 starts from initial state St-m+1, and SXt is the (m - 1)th
state on the chain. Following Lemma 1, we have:
dτv (P(X ∈ ∙∣θ0, St-m+1), μθo 0 ∏θo 0 P)
=dτv (P ((Xt,Xt,Xt+1) ∈∙∣θ0 ,St-m+1) ,μθ0 0 ∏Θ0 0p) ≤ KPm-1.
Substituting the last inequality into (98) and taking total expectation on both sides yield
E[I4] ≤ 8RωCδκPm-1.
Taking total expectation on (93) and collecting bounds of I1, I2, I3, I4 yield
dm
E [∆1(x, θ, ω)]	≤C4E∣θk	-θk-dm∣2+C5	XE∣θk-i-θk-dm∣2
i=τk
+ C6 E ∣ωk - ωk-dm ∣2 + C7 κPm-1,
where C4 := 2CδLω + 4Rω Cδ lAlLπ (1 + logρ κ-1 + (1 - P)-1), C5 := 4RωCδlAlLπ, C6 :=
4(1 + Y)Rω + 2Cδ and C7 := 8RωCδ.	□
C.2 Supporting Lemmas for Theorem 4
Lemma 5. For any m ≥ 1 and k ≥ (K0 + 1)m + K0 + 1, we have
E NJ(θk), G(X(k), ωk) - δ(X(k), θk)) ψθk-τk (S(k), α(k))) ≥ -D2 E kθk-τfc - θk-dm ∣∣2
dm
-D3 E ∣θk - θk-dm∣2 - D4 X E∣θk-i- θk-dm∣2 - D5κPn- - 2CψLVefa - 2Cψe$p EkVJ(θ)∣2,
i=τk
where D2 := 2LV Lψ Cδ, D3 := (2Cδ Cψ LJ + LVCψ (Lω + LV)(1 + γ) + 2Cψ LJ eapp), D4 :=
2LV Cψ Cδ lAlLπ and D5 := 4LVCψCδ.
33
Under review as a conference paper at ICLR 2021
Proof. For the worker that contributes to the kth update, we construct its Markov chain:
θk-dm	Pe	θk-dm-1	θk-d1	P	θk-d°	P
st-m --------→	at-m	-→	st-m+1 ---------→	at-m+1	+…st-1 ------→ at-1	-→	St ----→ at	—→ st+1,
where (st, at, st+1) = (s(k),α(k), s(k)), and {dj}mL0 is some increasing sequence with do := Tk. By
(92) in Lemma 4, we have dm ≤ (Ko + 1)m + Ko.
Given (st-m, αt-m, st-m+ι) and θk-dm, we construct an auxiliary Markov chain:
C	θk-dm ∖ r	P ∖ C
St-m	→ at-m -→ St-m+1
θk-dm
e at-m+1 ... st-1
θk — d∙m, ʌ-	Pe ʌ- θk — d ∙m, ʌ- Pe ʌ-
--------→ at-1 -→ St -----------→ at —→ St+1 ∙


First we have
(VJ(Ok), (S(X(k),“却一S(X(k)/k)) ψθk-τk (S(k),a(k)),
=(VJ (Ok) , (S(X(k), ωk ) - S(X(k),Ok)) (ψθ%-τk (S(k),a(k)) - ψθk-dm (S(k), a(k)
+ (VJ (Ok), G(X(k),ωk) - S(X(k), Ok)) ψθk-dm (S(k),a(k))) .
We first bound the fist term in (99) as
(VJ (Ok ), G(X(k),ω勃-S(X(k),Ok )) (ψθk-τk (S(k),a(k)) - ψθkf (S(k),a(k))))
(99)
〉
〉
〉
Il τ∕n ∖ιι ∣3∕	*∖ c/ n ∖ιιι /	/	∖	/	/	∖ιι
-IJ(^k)k2|S(X(k),Wfe) - S(X(k),θk)l∣ψθfc-τfc (S(k),a(k)) - ψθk-dm (S(k),a(k))∣2
-∣J(θk)∣2 (∣S(X(k),说)∣ + ∣δ(X(k),θk)∣) Wθj (S(k),a(k)) - ψθk-dm (S(k),a(k))∣2
/. ʌ , .,.. . 、-， , . . ∖ ,.. , . , ...
-	LV (砥/⑹心却1 + |S(X(k),θk )lJ Iψθk-τk (S(k),a(k)) - ψθk-dm (S(k) ,a(k))k2
-	2LV cδ ∣ψθk-τk (S(k), a(k)) - ψθk-dm (S(k), a(k))∣∣2
-	2LVLψC6 ∣∣θk-τfc - θk-dm ∣∣2,
〉
〉
(100)
where the last inequality follows Assumption 3 and second last inequality follows
.ʌ, ... .. ......................... ................
M(X,ωθ )1 ≤ Ir(X)I + γ∣φ (S )k2kωθk2 + ∣φ (S)II2∣ωθ∣2 ≤ rmax + (I + Y)Rω ≤ Cδ,
..................................   ...	.riV
|S(x, θ)∣ ≤ Ir(X) I + 7∣Vπθ (s )∣ + ∣v∏θ (S)I ≤ rmax + (1+ Y)---- ≤ C6 ∙
1 - Y
Substituting (100) into (99) gives
(VJ(θk), (δ(X(k),说)一S(X(k), θk)) ψθk-τk (S(k), α(k)),
≥ 一2LVLψC6 ∣∣θk-τfc - θk-dm ∣∣2 + (VJ(θk), 0(χ(k),ωk) - S(X(k),θk)) ψθk-dm (S(k), a(k))) .
(101)
Then we start to bound the second term in (101). For brevity, we define
△2(X,O) := DVJ(O), (s(x,"3) - S(x,O)) ψθk-dm (S,a)〉.
In the following proof, we use O, O0, ωθ,娓,,X and x as shorthand notations for Ok, Ok-dm, ωk,
ωk-dιn, Xt and Xt respectively. We also define X := (s, a, s0), where S 〜μθz, a 〜∏θz and s0 〜P.
We decompose the second term in (101) as
∆2(x,O) = ∆2(x, O) - △ (x, O0) + ∆2(x, O0) - ∆2(X, O0) + ∆2(X, O0) - ∆2(X, O0) + ∆2(x, O0).
I	I I	I I	I I	I
Ii	12	I3	I4
We bound the term I1 as
I1 = DVJ(O), (S(x,ωk) - δ(x, O)) M(s,。))- DVJ(O0), ^δ(x, ωk) - δ(x, O0)) ψθf (s,。))
=DVJ(O), (δ(x, ωk) - S(x, O)) ψθ,(s, a)) - DVJ(O0), (S(x, ωk) - S(x, O)) ψθ,(s, a))
+ DVJ(O0), (S(x,ωk) - S(x,O)) ψθ,(s,a)) - DVJ(O0), (S(x,ωk,) - S(x,O0))他夕(s,a)).
34
Under review as a conference paper at ICLR 2021
For the first term in I1, we have
(▽J (θ), (δ(x,ω^) - δ(x,θ)) ψ (s,a))-(VJ (θ0), (δ(x,ω^) - δ(x, θ))@夕 Ga)E
=(VJ(θ) - VJ(θ0), (6(x,ωθ) - δ(x,θ)) ψθ(s,α)E
≥ -kVJ(θ) - VJ(θ)k2kδ(x,ωθ) - δ(x,ff)Mψθ,(s,a)k2
≥ -2CδCψ∣∣VJ(θ) - VJ(θ0)∣∣2
≥ -2CδCψLj∣∣θ - θ'∣∣2,
where the last inequality is due to the Lj-Lipschitz of policy gradient shown in Proposition 1.
For the second term in I1, we have
(vj(θ0), (6(x,ωθ) - δ(x,θ)) ψθ0(s,α)E -(VJ(θ0), (δ(x,ω^ - δ(x,θ0))死，(s,a)E
=(VJ(θ0), (δ(x,ω^) - δ(x,ωθJ + δ(x,θ0) - δ(x, θ)) ≠θz(s, a))
≥ -LVCψ ∣δ(x, ω^) — δ(x, ωθdo) + δ(x,θ0) — δ(x, θ) ∣
≥ -LV cψ ∣ γφ (SZ) >(ω^- ωθ0)+ φ (S)T 3θo - ω& + γvπθ0(SZ) - γVπθ(SZ) + Vπθ(S)- vπθ0 (S)I
≥ -LV cψ (γ∣∣ωθ- ωθ01∣2+ ι∣ωθ0- ω莉 2+ Y ιVπθ∕(SZ)- Vπθ(SZ) I+ ιVπθ(S)-咚即(S)D
≥ -lVcψ (YLω ∣∣θ - θ'∣∣2 + Lω ∣∣θ - θ'∣∣2 + YLV ∣∣θ - θ'∣∣2 + LV∣∣θ - θ1∣2)
=-LV cψ (Lω + lV )(1 + γ)∣∣θ - θ1∣2,
where the last inequality is due to the Lω -Lipschitz continuity of ωθ shown in Proposition 2 and
LV-Lipschitz continuity of V∏θ (s) shown in Lemma 3. Collecting the upper bounds of Ii yields
I1 ≥ - (2cδ cψ lj + lv cψ (Lω + lV )(1 + Y)) ∣∣θ - θ1∣2.
First we bound I2 as
E[I2∣θz, St-m+1] = E [∆2(x, θz) - ∆2(≡,θz)∣θz, St-m+1]
≥-∣E [∆2(x,θz)∣ θz, St-m+1] - E [∆2(e, θz)∣ θz, St-m+1]∣
≥ -SUP ld(/, θ')lllP(X ∈ ∙∣θ1 st-m+1) - P(X ∈ ∙∣θ', st-m+I)IlTV
x
≥ -4LvCψCδdTV (P(X ∈ ∙∣θ',St-m+1),P(e ∈ ∙∣θ',St-m+1 ))
dm
≥ -2LVcψcδ IAILn X E [∣∣θk-i - θk-dm ∣∣2 ∣ θl st-m+1] ,	(102)
i=τk
where the second inequality is due to the definition of TV norm, the last inequality follows (22) in
Lemma 2, and the second last inequality follows the fact that
∣∆2(x, θz)∣ ≤ IlVJ(θz)∣∣2∣δ(x,ω:o)- δ(x,θ∕)∣∣∣ψθ0(s,a)∣∣2 ≤ 2LvCCψ.	(103)
Taking total expectation on both sides of (102) yields
dm
E[I2] ≥ -2LV cψ cδ IAILn X EIl θk-i - θk-dm ∣∣2∙
i 二 τk
Next we bound I3 as
E[I3∣θz, St-m+1] = E [∆2(e, θz) - ∆2(X,θ z)l θz, St-m+1]
≥ - IE [∆2(e,θz)∣ θz, St-m+1] - E [∆2(X, θ z)l θz, St-m+1]∣
≥ -SUp ^2(/, θZ)||IP(X ∈ "θ Z, st-m+1) - P(X ∈ 1θ Z, st-m+I)IlTV
x
≥ -4LVcψcδdTV (P(e ∈ "θ Z, st-m+1), Mθz 区 πθ0 0 Pe) ,	(104)
where the second inequality is due to the definition of TV norm, and the last inequality follows (103).
35
Under review as a conference paper at ICLR 2021
The auxiliary Markov chain with policy ∏θ，starts from initial state st-m+1, and Iet is the (m - 1)th
state on the chain. Following Lemma 1, we have:
dτv(P(e ∈ ∙∣θ0, St-m+1), μθ，0 ∏θ，③ P)
=dτv (P((≡t, at,st+ι) ∈ ∙∣θ0,st-m+ι) ,〃/ 0 ∏θ，0P) ≤ KPm-1.
Substituting the last inequality into (104) and taking total expectation on both sides yield
E[I3] ≥ -4LvCψC6KPmT
We bound I4 as
E[l4∣θ0] = E KVJ(θ0), G@, ωθ,) - δ(X,θ0)) ψθ,(s, a))∣ θ,]
≥ -Cψ kVJ (θ0)k2 E [∣δ(x,ω^,) - δ(X,θ0) M
=-Cψ IIVJ(町2 E [∣ γ (。0)>ω孤-%,⑺)+ 咚“⑶-。⑶>ωθ, ∣ ∣ θ']
≥ -Cψ kVJ (θ 0)∣∣2 (Y E [∖φ(s' )τω^, - L ⑺∣ ∣θ 0] + E [∣Js) - -, ∣ ∣θ 0])
≥ -Cψ∣VJ(θ')∣∣2 (YqE [∣φ⑺τω，，- %,⑺∣2 ∣θ0] + JE [%“ ⑶一。⑶τωθ,∣2 ∣ θ0]
=-CψIIVJ(θ0)∣∣2 L/，E	∣φ⑺τωθ,一 %，⑺∣2 + IN Γ‰，⑸一Vs)Tωθ,∣2
S，〜"e，	S 〜"e，
≥-2Cψ IlVJ (θ0)∣∣2.pp,
where the second last inequality follows Jensen,s inequality.
The last inequality further implies
E[I4] ≥ -2Cψ EkVJ(θ0) - VJ(θ)+ VJ(θ)∣∣2eapp
≥ -2CψeappEkVJ(θ0) -VJ(θ)∣∣2 - 2Cψ6appEkVJ(θ)∣∣2
≥ -2cΨeapp EkVJ(θO)-VJ(θX∣2 - 2cΨefa EkVJ(θ)∣2 - 2CψLVesp
≥ -2CψLjeapp E kθ - θ0k2 - 2Cψefa EkVJ(θ)∣∣2 - 2CψLVesp,
where the last inequality follows Proposition 1.
Taking total expectation on both sides of (101), and collecting lower bounds of Iι, I2, I3 and I4 yield
E (VJ (θk ∖ G(X(k), ω却一δ(x(k), θk)) ψθjk (s(k), α(k)))
dm
≥ -D2 Ekθk-τ⅛ - θk-dm ∣∣2 - D3 Ekθk - θk-dm ∣∣2 - D4 ^X Ekθk-i - θk-dm ∣∣2
i=τk
-D5κρm-1 - 2CψLVefa - 2Cψesp EkVJ(θk)∣∣2,
where D2 ：= 2LvLψCs, D3 ：= (2C(5CψLJ + LVCψ(Lω + LV)(1 + γ) + 2CψLJeapp), D4 ：=
2LvCψCS∣A∣L∏ and D5 := 4LvCψCS.
Lemma 6. For any m ≥ 1 and k ≥ (K0 + 1)m + K0 + 1, we have
E DVJ (θk ),δ(x(k),θk )ψθk-τk (s(k),α(k)) -VJ (θk)) ≥ -D6 E kθk-τk - θk-dm k2
dm
-D7 E kθk - θk-dm k2 -D8 X E kθk-i - θk-dm k2 -D9κρm-1 - 8Cψrmax(1 - Y)EkVJ(θk)k2 ,
i二τk
where Dβ := LVCSLψ, D7 ：= CPLJ + (1 + Y)LVCψ + 2LvLJ + 8CψrmaχLj(1 — Y),。8 :=
Lv(CP + Lv)∣A∣L∏, D9 ：= 2Lv(CP + LV).
Proof. For the worker that contributes to the kth update, we construct its Markov chain:
θk-dm	P	θk-dm-1	θk-d∖	P	θk-d°	P
st-m ------→ at-m	-→	st-m+1 ------→ αt-m+1 ∙ ∙ ∙ st-1 --→ αt-1	—→	st ---→	αt	-→	st+1,
36
Under review as a conference paper at ICLR 2021
where (st, at, st+1) = (s(k), a(k), s0(k)), and {dj}jm=0 is some increasing sequence with d0 := τk. By
(92) in Lemma 4, we have dm ≤ (K0 + 1)m + K0.
Given (st-m, at-m, st-m+1) and θk-dm, we construct an auxiliary Markov chain:
st-m
θk-dm	Pe
----→ at-m -→ st-m+1
-dm
----→ at-m+1 ∙ ∙ ∙ st-1
θk-dm
-----→ at-1
st
θk-dm
-----→ at
st+1.
First we have
(VJ(θk), δ(x(k),θk)ψθk-τk (s(k),a(k)) - VJ(θk»
=(VJ(θk), δ(x(k), θk) (ψθk-τk (s(k), a(k)) - ψθk-dm (s(k), a(k)")
+ VJ(θk), δ(x(k), θk)ψθk-dm (s(k), a(k)) -VJ(θk).	(105)
We bound the first term in (105) as
VJ(θk), δ(x(k), θk) ψθk-τk (s(k), a(k)) - ψθk-dm (s(k), a(k))
≥ - kVJ(θk)k2 kδ(x(k), θk)k2kψθk-τk (s(k), a(k)) - ψθk-dm (s(k), a(k))k2
≥ -LV kδ(x(k), θk)k2kψθk-τk (s(k), a(k)) - ψθk-dm (s(k), a(k))k2
≥ -LV Cδ kψθk-τk (s(k), a(k)) - ψθk-dm (s(k), a(k))k2
≥ -LV CδLψ kθk-τk -θk-dmk2,	(106)
where the last inequality follows Assumption 3, and the second last inequality follows the fact that
r
lδ(X,θ)∣ ≤ Ir(X) I + γ∣v∏θ (S )1 + lV∏θ (S)I ≤ rmax + (I + Y)-.- ≤ Cδ .
1-γ
Substituting (106) into (105) gives
VJ(θk), δ(X(k), θk)ψθk-τk (S(k), a(k)) - VJ(θk)
≥ -LVCδLψkθk-τk - θk-dmk2 + VJ(θk), δ(X(k), θk)ψθk-dm (S(k), a(k)) -VJ(θk). (107)
Then we start to bound the second term in (107). For brevity, we define
∆3(X, θ) := VJ (θ), δ(X, θ)ψθk-dm (S, a) -VJ(θ) .
Throughout the following proof, we use θ, θ0, X and Xe as shorthand notations of θk, θk-dm, Xt and
Xet respectively.
We decompose ∆3 (X, θ) as
∆3(x, θ) = ∆3(x,θ) - ∆3(x,θ0)+∆3(x,θ0) - ∆3(e, θ0) + ∆3(e, θ0).
I1	I2	I3
We first bound I1 as
II1I = I∆3(X, θ) - ∆3(X, θ0)I
= hVJ (θ), δ(X, θ)ψθ0 (S, a)i - kVJ(θ)k22-hVJ(θ0),δ(X,θ0)ψθ0(S,a)i+kVJ(θ0)k22
≤ IhVJ(θ),	δ(X, θ)ψθ0(S,	a)i	- hVJ(θ0),	δ(X, θ0)ψθ0(S, a)iI	+ kVJ(θ0)k22 - kVJ(θ)k22
≤ IhVJ(θ),	δ(X, θ)ψθ0(S,	a)i	- hVJ(θ0),	δ(X, θ0)ψθ0(S, a)iI	+ kVJ(θ0) + VJ(θ)k2kVJ(θ0)	-
≤ IhVJ(θ), δ(X, θ)ψθ0(S, a)i - hVJ(θ0), δ(X, θ0)ψθ0(S, a)iI + 2LV LJkθ - θ0k2,	(108)
VJ(θ)k2
37
Under review as a conference paper at ICLR 2021
where the last equality is due to LV -Lipschitz of value function and Lj-Lipschitz of policy gradient.
We bound the first term in (108) as
KVJ (θ),δ(x,θ)ψθ(s,α))-hVJ (θ'∙),δ(x,θ'∙)ψθ' (s,a)i∣
≤ KVJ(ff),δ(x,ff)ψθ'(s,a)}-(VJ(θ),δ(x,θ0)ψθo(s,α)i∣
+ KVJ(θ),δ(x,θ 0)%，(s,a)〉一 hVJ(θ 0),δ(x,θ0)ψθo(s,a)i∣
=KVJ (θ), (δ(x,θ) - δ(x,θ0)) ψθο (s, a))| + KVJ (θ) - VJ (θ0), δ(x,θ0)ψθ0 (s, a))|
≤ LvCψ ∣δ(x,θ) - δ(x,θ0)∣ + CPkVJ(θ) - VJ(θ')g
=Lv Cψ ∣7(V∏θ (s0)-咚“(s'))+ VπΘ,(s)-咚 ° (s)∣ + CPkVJ ⑹-VJ (θ‰
≤ LvCψ (Y ∣ ‰(s') - V∏θ, (s')∣ + ∣ V∏Θ,(s) - V∏°(s) ∣ ) + CPkVJ(θ) - VJ(θ')k2
≤ LvCψ (YLVkθ - θ'k2 + Lvkθ' - θ∣∣) + CPLJ∣∣θ - θ'k2
=(CPLJ + (1+ Y)LVcψ) llθ - θ'k2∙
Substituting the above inequality into (108) gives the lower bound of Ii：
I1 ≥ - (CPLJ + (I + Y)LVcψ + 2LVLJ) ∣∣θ - θ,∣∣2 ∙
First we bound I2 as
E[l2∣θ', st-m+1] = E [∆3(x, θ') - ∆3(e,θ')∣θ', st-m+1 ]
≥-∣E [∆3(x,θ')∣θ', st-m+1 ] - E [∆3(e,θ')∣θ', st-m+ι]∣
≥ -SUP |∆3(x, θO)IkP(X ∈ ∙∣θ0, st-m+1) - P(X ∈ ∙∣θ', st-m+I)IlTV
x
≥ -2LV(Cp + LVIdTV (P(X ∈ ∙∣θ0, st-m+1), P(X ∈ ∙∣θ', st-m+1))
dm
≥ -LV(CP + LV)IAILπ ^X E [∣∣θk-i - θk-dm ∣∣2 ∣θ', st-m+1] ,	(109)
i二τk
where the second inequality is due to the definition of TV norm, the last inequality is due to (22) in
Lemma 2, and thesecond last inequality follows the fact that
∣∆3(x,θ')∣ ≤ kVJ(θ)k2 (kδ(x,θ)ψθk.dm(s,a)k2 + kVJ(θ)k2) ≤ Lv(CP + LV)∙	(110)
Taking total expectation on both sides of (109) yields
dm
E[I2] ≥ -Lv(CP + Lv)∣A∣L∏ EEkθ1- θk-dm k2∙
i=τk
Define X := (s, a, s'), where s 〜dg；, a 〜∏θ z and s/〜P. Then we have
E[I3] = E [∆3(e, θ') - ∆3(X,θ')] + E [∆3(X, θ')] ∙	(111)
We bound the first term in (111) as
E [∆3(e, θ') - ∆3(X,θ')∣θ',st-m+ι]
≥ -∣E [∆3(e, θ')∣θ',st-m+1] - E [∆3(X, θ')∣θ',st-m+1]∣
≥ -SUp [△six”)∣kP(X ∈ ∙∣θ0, st-m+1) - P(X ∈ ∙∣θ0, st-m+1)∣∣τv
x
≥ -2Lv(CP + Lv)dτv (P(≡ ∈ ∙∣θ', st-m+i), P(x ∈ ∙∣θ', st-m+i))
=-2Lv(CP + Lv)dτv (p(≡ ∈ ∙∣θ', st-m+i), dg，乳 ∏ 乳 P)
=-2Lv(CP	+ LV)dτv(P(e	∈	∙∣θ',	st-m+ι),	μg0	乳 ∏θ，乳 P)	(112)
where the second inequality follows the definition of total variation norm, and the third inequality
follows (110). The last equality is due to the fact shown by [6] that μg，(∙) = dg，(∙), where μg，is the
stationary distribution of an artificial MDP with transition kernel P(∙∣s,a) and policy ng，.
38
Under review as a conference paper at ICLR 2021
The auxiliary Markov chain with policy πθ0 starts from initial state st-m+1, and set is the (m - 1)th
state on the chain. Following Lemma 1, we have:
dτv(P(e ∈∙∣θ0, st-m+1),μθ0 0 ∏θ0 ③ Pe) = dτv (P ((e,e,et+ι) ∈∙∣θ0, st-m+ι), Mθ0 0 ∏θ0 0 Pe)
≤ κρm-1.
Substituting the last inequality into (112) and taking total expectation on both sides yield
E [∆3(e, θ0) - ∆3(x, θ0)] ≥ -2Lv(Cp + LV)κρm-1.
Consider the second term in (111). Note its form is similar to (59), so by following the derivation of
(63), we directly have
E[∆3(x,θ0)] = E NJ(θ0),δ(x,θ0)ψθo(s,a) -VJ(θ0)i ≥ -8Cψrmaχ(1 - Y) Ell VJ(θ0)∣∣2 ,
which further implies
E[∆3(x,θ0)] ≥ -8Cψrmaχ(1 - Y)EkVJ(θ0)k2
≥ -8Cψrmax(1 -γ)ElVJ(θ0) -VJ(θ)l2 -8Cψrmax(1 -γ)ElVJ(θ)l2
≥ -8Cψ rmax LJ (1 - Y) E kθ0 - θ k2- 8Cψ rmax (1 - Y) E kVJ (θ)k2
where the last inequality follows from Proposition 1.
Collecting the lower bounds gives
E[I3] ≥ -2LV (Cp + LV)κρm-1 -8Cψrmax(1 -Y)(LJEkθ0-θk2 -EkVJ(θ)k2).
Taking total expectation on ∆3(x, θ) and collecting lower bounds of I1, I2, I3 yield
e[δ3(x, θ)] ≥ -(CpLJ + (1 + Y)LV Cψ + 2LV LJ + 8Cψr maxLJ (1 - Y)) E kθk - θk-dm k2
dm
-LV(Cp + LV)∣A∣L∏ E E kθk-i - θk-dm k2 - 2Lv(Cp + LV)κρm-1 - 8Cψrmaχ(1 - Y) E ∣VJ®m.
i=τk
Taking total expectation on (107) and substituting the above inequality into it yield
E DVJ(θk), δ(x(k), θk)ψθk-τk (s(k), a(k)) -VJ(θk)E ≥ -D6 E kθk-τk -θk-dmk2
dm
-D7Ekθk -θk-dmk2 -D8 X E kθk-i -θk-dmk2 -D9κρm-1 -8Cψrmax(1 -Y)EkVJ(θk)k2,
i=τk
where D6 := LVCδLψ, D7 := CpLJ + (1 + Y)L2VCψ + 2LV LJ + 8CψrmaxLJ(1 - Y), D8 :=
LV(Cp + LV)∣A∣L∏, D9 := 2Lv(Cp + LV).	□
C.3 Explanation of the approximation error
In this section, we will provide a justification for the circumstances when the approximation error
app defined in (14) is small.
Lemma 7. Suppose Assumption 2 and 4 hold. Then it holds that
eapp ≤ m∈RX /殳 ％θ (S)- V⅛ (S) |2 +4rmax(λ-1 + λ-2rmax) (1 + lθgP	+	) (1 -Y)
(113)
where ωθ the critic stationary point oforiginal Markov chain with policy ∏θ and transition kernel P.
In (113), the first term captures the quality of critic function parameterization method which also
appears in previous works [14, 15, 17]. When using linear critic function approximation, it becomes
zero when the value function Vπθ belongs to the linear function space for any θ. The second term
corresponds to the error introduced by sampling from the artificial transition kernel Pe(∙∣s, a)=
(1 - Y)P(∙∣s, a) + Yη(∙). For a large Y close to 1, the artificial Markov chain is close to the original
39
Under review as a conference paper at ICLR 2021
one. In this case, the second error term is therefore small. This fact also consists with practice where
large γ is commonly used in two time-scale actor critic algorithms [3].
Before going into the proof, we first define that:
Aθ,φ :=	Ee [φ(S)(Yφ(SO)- φ(S))>],	bθ,φ := E	,JrGa, s1φ(S)],
S 〜μθ,s0 〜P∏θ	S 〜μθ,a 〜∏θ,s0 〜P
where μθ as the stationary distribution of the original Markov chain with ∏θ and transition kernel P.
Proof. Recall the definition of the approximation error:
app
m∈Rχ ∕χlV∏θ(S)- %(s)|2，
where μθ is the stationary distribution of the artificial Markov chain with ∏θ and transition kernel P,
and ω^ is the stationary point of critic update under the artificial Markov chain.
We decompose app as
eapp = maχ ./ E ∣Vπ°(S)-匕)⑶ + 匕已⑶一V)θ(s)∣2
θ∈Rd V S〜μθ	θ	θ	θ
≤ maχ ./ E ∣V∏θ(S) - V⅛>θ(s)∣2 + maχ ./ E ∣V¾(s) - V⅛(s)∣2,	(114)
θ∈Rd V S〜μθ	θ	θ∈Rd V S〜μθ	θ	θ
I	I I	I
fa	sp
where the first term corresponds to the function approximation error fa, and second term corresponds
to the sampling error sp .
With A, b and A, b as shorthand notations for Aθ,ψ, bθ,ψ and Aθ,ψ, bθ,ψ respectively, We bound the
second term in (114) as
忆力(S)- Vωθ (S)I = ∣φ(S)>ωθ - φ(S)>ωM
≤ ∣∣A-1b - ATbll2
=∣∣A-1b - ATb + ATb - A-1b∣∣2
≤llAT(b-b)∣∣2 + ∣∣(AT-AT)b∣∣2
≤ λ 1kb - bk2 + rmax ∣∣A 1 - A 1 ∣∣2
=λ-1kb - b∣∣2 + rmax ∣∣A-1(A - A)AT∣∣2
≤ λ-1kb - b∣∣2 + λ-2rmaχ ∣∣A - A∣∣2 .	(115)
We bound the first term in last inequality as
kb - bk2 =	E 」r(S, a, S0)。(S)] - E 0」r(S, a, S0)。(S)]
S 〜μθ ,a 〜∏θ ,s0 〜P	S 〜Ma 〜πθ,s 〜P
0
≤ SuP ∣∣r(S,a, S )。一)|图心③ ∏θ ③ P- μθ ③ ∏θ ③ PkTV
≤ 2rmaχdτv(μθ ③ ∏θ ③ P, μθ ③ ∏θ ③ P).
We now bound the divergence term in the last inequality as
dτv (μθ ③ ∏θ 0 P, μθ ③ ∏θ ③ P)
=	£	lμθ(s)∏θ(a∣S)P(S0∣S,a) - μθ(s)∏θ(a∣S)P(S0∣S,a)
S∈S a∈A S0∈S
X	Iμθ(S)πθ(aIS)Pe(S0IS, a) - μθ(S)πθ(aIS)P(S0IS, a)
+ μθ(s)∏θ(a∣s)P(s0∣s,a) - μθ(s)∏θ(a∣s)P(s0∣s,a)∣
≤Xμθ(S)πθ(aIS)	lllPe(S0IS, a) - P(S0IS, a)
(116)
(117)
Iμθ(s) - μθ(s)| .
40
Under review as a conference paper at ICLR 2021
We bound the first term in (117) as
s0∈
Pe(Sls,a) -P(Sls,a)| = (I- Y)∕, JP(S0|s,a) - η(SO)I ≤ 2(I- Y).	(118)
Following [39, Theorem 3.1], the second term in (117) can be bounded as
/	lμθ(S)- μθ(S)I ≤
s∈S
κ-1 +
1
1 - P
1
1 - P
1
)SuPLS X∏θ(a∣s) C
Pe(s0|s, a) - P (s0 |s,
suPX πθ (aIS)	Pe(S0IS, a) - P (S0IS, a)
s a	s0∈S
+ τ-7
where the last inequality follows (118).
Substituting (118) and (119) into (117) gives
(1 - γ),
(119)
a
dτv(μθ ③ ∏θ 0pe, μθ ③ ∏θ ③。)≤ 2(1 + log。KT + ɪɪ^) (1 - Y).
Substituting the above inequality into (116) gives
kb
-b∣∣2 ≤ 4rmax ( 1 + logρ KT +
T⅛ )(1-Y).
(120)
Similarly, we also have
kA
-A∣∣2 ≤ 4rmax ( 1 + logp KT +
E )(1 - γ).
(121)
Substituting (120) and (121) into (115), then substituting (115) into (114) completes the proof. □
D Experiment Details
Hardware device. The tests on synthetic environment and CartPole was performed in a 16-core
CPU computer. The test on Atari game was run in a 4 GPU computer.
Parameterization. For the synthetic environment, we used linear value function approximation and
tabular softmax policy [36]. For CartPole, we used a 3-layer MLP with 128 neurons and sigmoid
activation function in each layer. The first two layers are shared for both actor and critic network. For
the Atari seaquest game, we used a convolution-LSTM network. For network details, see [40].
Hyper-parameters	Value
Number of workers	-16-
Optimizer	Adam
Step size	0.00015
Batch size	20
Discount factor	0.99
Entropy coefficient	0.01
Frame size	80 X 80
Frame skip rate	4
Grayscaling	Yes
Training reward clipping	[-1,1]
Table 1: Hyper-parameters of A3C-TD(0) in the Atari seaquest game.
Hyper-parameters. For the synthetic environment tests, we run Algorithm 1 with actor step size
αk = (J.05o.6 and critic step size βk = (JOjoj. In tests of CartPole, We run Algorithm 1 with a
minibatch of 20 samples. We update the actor network with a step size of ɑk =(以储 and critic
network with a step size of βk = (J,；。/. See Table 1 for hyper-parameters to generate the Atari
game results in Figure 4.
41