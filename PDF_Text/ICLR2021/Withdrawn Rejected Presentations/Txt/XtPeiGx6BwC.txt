Under review as a conference paper at ICLR 2021
Robustness against Relational Adversary
Anonymous authors
Paper under double-blind review
Ab stract
Test-time adversarial attacks have posed serious challenges to the robustness
of machine-learning models, and in many settings the adversarial perturbation
need not be bounded by small p-norms. Motivated by the semantics-preserving
attacks in vision and security domain, we investigaterelational adversaries, a broad
class of attackers who create adversarial examples that are in a reflexive-transitive
closure of a logical relation. We analyze the conditions for robustness and propose
normalize-and-Predict - a eearning rramework Wtth PrOVaIɔe o)bɔusttiess guaaaneee.
We compare our approach with adversarial training and derive an unified framework
that providis )iiifits of )oth approachis. Guidid )y our thioriticalfiidiigs, wi
apply our framiwork to imagi classificatioi aid malwari ditictioi. Risults of
)oth tasks show that attacks usiig rilatioial advirsariis friquiitly fool ixistiig
modils, )ut our uiifiid framiwork cai sigiificaitly iihaici thiir ro)ustiiss.
1	Introduction
Thi ro)ustiiss of machiii liariiig (ML) systims has )iii challiigid )y tist-timi attacks usiig
advirsarial ixamplis (Szigidy it al., 2013). Thisi advirsarial ixamplis ari iitiitioially maiipulatid
iiputs that prisirvi thi issiitial charactiristics of thi origiial iiputs, aid thus ari ixpictid to havi
thi sami tist outcomi as thi origiials )y humai staidard; yit thiy sivirily affict thi pirformaici
of maiy ML modils across diffiriit domaiis (Moosavi-Dizfooli it al., 2016; Eykholt it al., 2018;
Qii it al., 2019). As modils ii high-staki domaiis such as systim sicurity ari also uidirmiiid )y
attacks (Grossi it al., 2017; Rosii)irg it al., 2018; Hu & Tai, 2018), ro)ust ML ii advirsarial tist
iiviroimiit )icomis ai impirativi task for thi ML commuiity.
Existiig work oi tist-timi attacks pridomiiatily coisidirs p-iorm )ouidid advirsarial maiip-
ulatioi (Goodfillow it al., 2014; Carliii & Wagiir, 2017). Howivir, ii maiy sicurity-critical
sittiigs, thi advirsarial ixamplis iiid iot rispict thi p -iorm coistraiit as loig as thiy prisirvi thi
malicious simaitics. Ii malwari ditictioi, for ixampli, a malwari author cai implimiit thi sami
fuictioi usiig diffiriit APIs, or )iid a malwari withii )iiigi softwaris liki vidio gamis or offici
tools. Thi modifiid malwari prisirvis thi malicious fuictioiality dispiti thi drastically diffiriit
syitactic fiaturis. Hiici, focusiig oi advirsarial ixamplis of small p -iorm ii this sittiig will fail
to addriss a siza)li attack surfaci that attackirs cai ixploit to ivadi ditictors.
Ii additioi to sicurity thriats, aiothir risiig coiciri oi ML modils is thi spurious corrilatiois thiy
could havi liariid ii a )iasid data sit. Ri)iiro it al. (2016) show that a highly accurati wolf-vs-
husky-dog classifiir iidiid )asis its pridictioi oi thi prisiici/a)siici of siow ii thi )ackgrouid.
A rilia)li modil, ii coitrast, should )i ro)ust to chaigis of this iaturi. Although du))id as simaitic
pirtur)atioi or maiipulatioi (Mohapatra it al., 2020; Bhattad it al., 2019), thisi chaigis do iot
altir thi cori of thi simaitics of iiput data, thus, wi still coisidir thim to )i simaitics-prisirviig
pirtaiiiig to thi classificatioi task. Siici such simaitics-prisirviig chaigis oftii risultid ii largi
p-iorms, thiy ari likily to riidir thi ixistiig p-iorm )asid difiisis iiiffictivi.
Ii this papir, wi coisidir a giiiral attack framiwork ii which attackirs criati advirsarial ixamplis
)y traisformiig thi origiial iiputs via a sit of rulis ii a simaitics-prisirviig maiiir. Uiliki thi
prior works (Rosii)irg it al., 2018; Hu & Tai, 2018; Hossiiii it al., 2017; Hossiiii & Pooviidrai,
2018) which iivistigati spicific advirsarial sittiigs, our papir ixtiids thi scopi of attacks to giiiral
logical traisformatioi: wi uiify thi thriat modils iito a powirful rilatioial advirsary, which cai
riadily iicorporati mori complix iiput traisformatiois.
1
Under review as a conference paper at ICLR 2021
From the defense perspective, recent work has started to look beyond p -norm constraints, including
adversarial training (Grosse et al., 2017; Rosenberg et al., 2019; Lei et al., 2019), verification-
loss regularization (Huang et al., 2019) and invariance-induced regularization (Yang et al., 2019).
Adversarial training in principle can achieve high robust accuracy when the adversarial example in
the training loop maximizes the loss. However,finding such adversarial examples is in general NP-
hard (Katz et al., 2017), and we show in Sec 4 that it is even PSPACE-hard for semantics-preserving
attacks that are considered in this paper. Huang et al. (2019) and Yang et al. (2019) add regularizers
that incorporate model robustness as part of the training objective. However, such regularization can
not be strictly enforced in training, and neither can the model robustness. These limitations still cause
vulnerability to semantics-preserving attacks.
Normalize-and-Predict Learning Framework This paper attempts to overcome the limitations of
prior work by introducing a learning framework that guarantees robustness by design. In particular,
we target arelationaladversary, whose admissible manipulation is specified by a logical relation.
A logical relation is a set of input pairs, each of which consists of a source and target of an atomic,
semantics-preserving transformation. We consider a strong adversary who can apply an arbitrary
number of transformations. Our paper makes the following contribution towards the theoretical
understanding of robust ML against relational adversaries:
1.	We formally describe admissible adversarial manipulation using logical relations, and
characterize the necessary and sufficient conditions for robustness to relational adversaries.
2.	We proposenormalize-and-predict(hereinafter abbreviated asN &P), a learning framework
thatfirst converts each data input to a well-defined and unique normal form and then trains
and classifies over the normalized inputs. We show that our framework has guaranteed
robustness, and characterize conditions to different levels of robustness-accuracy trade-off.
3.	We compareN &Pto the popular adversarial training framework, which directly optimizes
for accuracy under attacks. We show thatN &Phas the advantage in terms of explicit
robustness guarantee and reduced training complexity, and in certain cases yields the same
model accuracy as adversarial training. Motivated by the comparison, we propose a unified
framework, which selectively normalizes over relations that tend to preserve the model
accuracy and adversarially trains over the rest. Our unified approach gets the benefits from
both frameworks.
We then apply our theoreticalfindings to malware detection and image classification. For the former,
first, we formulate two types of common program transformation — (1) addition of redundant libraries
and API calls, and (2) substitution of equivalent API calls — as logical relations. Next, we instantiate
our learning framework to these relations, and propose two generic relational adversarial attacks to
determine the robustness of a model. Finally, we perform experiments overSleipnir, a real-world
WIN32 malware data set. Regarding image classification, we reused an attack method proposed by
the prior work (Hosseini & Poovendran, 2018) — shifting of the hue in the HSV color space — that
can be deemed as a specific instantiation of our attack framework. We then compare the accuracy and
robustness of ResNet-32 (He et al., 2016), a common image classification model, trained with the
unified framework against the standard adversarial training on CIFAR-10 (Krizhevsky et al., 2009).
The results we obtained in both tasks show that:
1.	Attacks using addition and substitution suffice to evade existing ML malware detectors.
2.	Our unified approach using input normalization and adversarial training achieves highest
robust accuracy among all baselines in malware detection. The drop in accuracy on clean
inputs is small and the computation cost is lower than pure adversarial training.
3.	When trained with the unified learning framework, ResNet-32 achieves similar clean accu-
racy but significantly higher robust accuracy than adversarial training alone.
Finally, based on our theoretical and empirical results, we conclude that input normalization is vital
to robust learning against relational adversaries. We believe techniques that can improve the quality
of normalization are promising directions for future work.
2
Under review as a conference paper at ICLR 2021
2	Related Work.
Test-time attacks using adversarial examples have been extensively studied in the past several
years. Research has shown ML models are vulnerable to such attack in a variety of application
domains (Moosavi-Dezfooli et al., 2016; Chen et al., 2017; Papernot et al., 2017; Eykholt et al.,
2018; Ebrahimi et al., 2018; Qin et al., 2019; Yang et al., 2020) including system security where
reliable defense is absolutely essential. For instance, Grosse et al. (2017) and Al-Dujaili et al. (2018)
evade API/library usage based malware detectors by adding redundant API calls; Rosenberg et al.
(2018), Hu & Tan (2018), and Rosenberg et al. (2019) successfully attack running-time behavior
based detectors by adding redundant execution traces; Pierazzi et al. (2020) extend the attacks from
feature-space to problem-space, propose a framework to describe real-world attacker’s constraints
and create realistic attack instances using automated software transplantation.
On the defense end, the work closest to ours in spirit is Yang et al. (2019), which adds invariance-
induced regularizers to the training process. Their work however differs from ours in two major ways.
First, their work considers a specific spatial transformation attack in image classification; our work
considers a general adversary based on logic relations. Second, their regularizer may not enforce the
model robustness onfinite samples as they are primarily interested in enhancing the model accuracy.
In contrast, our framework emphasizes robustness which is enforced by design. Grosse et al. (2017);
Al-Dujaili et al. (2018); Rosenberg et al. (2019) improve robustness via adversarial training; we show
such approach is hard to optimize. Incer et al. (2018); Kouzemtchenko (2018) enforce monotonicity
over model outputs so that the addition of feature values always increase the maliciousness score.
These approaches are limited to guarding against the addition attacks, thus lacks generality. Last, Xu
et al. (2017) use feature squeezing, which quantizes the feature values in order to reduce the number
of adversarial choices. However, their defense is for p-norm adversaries and thus inapplicable for
relational attacks.
Normalization is a technique to reduce the number of syntactically distinct instances. First introduced
to network security in the early 2000s in the context of intrusion detection systems (Handley et al.,
2001), it was later applied to malware detection (Christodorescu et al., 2007; Coogan et al., 2011;
Bichsel et al., 2016; Salem & Banescu, 2016; Baumann et al., 2017). Our work addresses the open
question whether normalization is useful for ML under relational adversary by investigating its impact
on both model robustness and accuracy.
3	Background
In this section, wefirst describe the learning task, then formalize the potential adversarial manipulation
as logical relations, and eventually derive the notion of robustness to relational adversaries.
Learning Task. We consider a data distribution D over a input space X and categorical label space
Y. We use bold face letters, e.g. x, for input vectors and y for the label. Given a hypothesis class
H, the learner wants to learn a classifier f x → → ʃ in H that minimizes the risk over the data
distribution. In non-adversarial settings, the learner solves minf ∈h E(χ,y)〜D 2(f, x, y∕), where Q is a
loss function. For classification,(f,x, y) =1(f(x)=y).
Logical Relation. A relation R is a set of input pairs, where each pair (x,z) specifies a transformation
of input X to output z. We write X →R Z iff (x, Z) ∈R R. We write X →R Z iff x = Zor there exists
zo, zι,…)Zk (k > 0) such that X = zo, Zi →r Zi+ι (0 ≤ < V AO and Zk = Z. In other words, →R
is the reflexive-transitive closure of→R . We describe an example relation as follows:
Example 1 (Hue Shifting). Let Xh, Xs, Xv denote the hue, saturation and value components of an
image X. In a hue shifting relation R, X→R Z iff Zh = (Xh+δ) % 1 where δ is a scalar, Zs =X s,
Zv =X v. Since Xh changes in a circle, i.e., hue of 1 is equal to hue of 0. Hence, we compute the
modulo of the hue component with 1 to map Zh within [0,1] (Appendix B gives the background of
HSV).
In this paper, we also consider unions of relations. Notice that afinite union R of m relations
Ri,…R m is also a relation, and x → R Ziff X TRi Z for any ∈ ∈ {L∙∙∙)m}.
3
Under review as a conference paper at ICLR 2021
Table 1: Comparison of training objective and test output for standard risk minimization learning
scheme, N&P and adversarial trainnng; ∕* is the minimizer of the training objective.
	No Defense	Normalize-and-Predict	Adversarial Training
Train	min	(f,X,y) f (x,y)∈D	min	Σ	Kf,NM),y) f (x,y)∈D		min maχ Σ ，(f,A(x),y) f AG) (x,y)∈D	
Test	f*(x)	f *(Mx))		f*(X	
Threat Model. A test-time adversary replaces a clean test input x with an adversarially manipulated
input A(x), where A(∙) represents the attack algorithm. We consider an adversary who wants to
maximize the classification error rate: E(χ,y)〜D 1(f(A(x))= y).
We assumewhite-boxattacks 1, i.e. the adversary has total access to f , including its structures, model
parameters and any defense mechanism in place. To maintain the malicious semantics, the adversarial
input A(x) should belong to a feasible set T(x) . In this paper, we focus on T(x) that is described by
relation. We consider a logical relation R that is known to both the learner and the adversary, and we
define a relational adversary as the following.
Definition 1 (relational adversary). An adversary is said to be R-relational if T(x) = {z ∣ → TR z},
i.e. each element in R represents an admissible transformation, and the adversary can apply arbitrary
number of transformation specified byR.
We can then define the robustness of a classifier f by how often its prediction is consistent under
attack, and robust accuracy as the fraction of predictions that are both robust and accurate.
Definition 2 (Robustness and robust accuracy). Let Q(R, f ,x) be the following statement:
∀z((x fR z) ⇒f X)X= = /(z)). Then, a classifier f is robust at X if Q(R,f, x) is true, and
the robustness of f to an R-relational adversary is: Ex〜DX IQ(Rf ,x), where 1(.)indicates the truth
value of a statement and DX is the marginal distribution over inputs. The robust accuracy of f w.r.t.
an R-reaaiioncd adversary is then: E(x,y)〜D IQ(Rf ,x)∧f(x)=y ∙
Notice that the robust accuracy of a classifier is no more than the robustness in value because of the
extra requirement of f(X) =y . Meanwhile, a classifier with the highest robustness accuracy may not
always have the highest robustness and vice versa: an intuitive example is that a constant classifier is
always robust but not necessarily robustly accurate. In Sec 4, we will discuss both objectives and
characterize the trade-off between them.
4	N&P — A PROVABLY Robust Learning Framework
In this section, we introduceN &P, a learning framework which learns and predicts over normalized
training and test inputs. Wefirst identify the necessary and sufficient condition for robustness, and
propose a normalization procedure that makesN &Pprovably robust to R-relational adversaries.
Finally, we analyze the performance ofN &P: sinceN &Pguarantees robustness, the analysis will focus
on robustness-accuracy trade-off and provide an in-depth understanding to causes of such trade-off.
4.1	An Overview of the N&PF ramework
In N&P, the learner first SPeCifieS a IIomlaiiZer N x → X ɪ. We call N(x) the 'normal form, of input
X. The learner then both trains the classifier and predicts the test label over the normal forms instead
of the original inputs. Let D denote the training set. In the empirical risk minimization learning
scheme, for example, the learner will now solve the following problem
fm∈iHn	(f,N(X),y),	(1)
(x,y)∈D
and use the minimizer f * as the classifier. During test-time, the model will predict f * (N(X)), Table 1
compares theN &Plearning pipeline to normal risk minimization and adversarial training.
1We consider a strong white-box attacker to avoid interference from security by obscurity, which is shown
fragile in various other adversarial settings (Carlini & Wagner, 2017).
4
Under review as a conference paper at ICLR 2021
Figure. 1: Relations with different robustness-accuracy trade-off. Different node colors indicate
different most likely labels. Appendix A.7 gives a detailed explanation on why semantics-preserving
transformation can still change the labels of data. Left: N&Ppreserves natural accuracy; Middle:
N&P PreSerVeS robust accuracy; Right: N&P CaUSeS SUbOPiimal robust accuracy: SUPPOSe μ(x)=
0.02, μ(zι) = μ(z2)=0.49, and η is deterministic. N&P Prediet the Same Iabel and thus has accuracy
at most0.49, while the highest robust accuracy is0.98by predicting the true label forz 1 andz 2.
4.2	Finding the Normalizer
Tte normalizer N is crucial for actieving robustness: intuitively, if x and its adversarial example
xadv stare tte same normal form, tten tte prediction will be robust. Meanwtile, a constant N is
robust, but has no utility as f (N^(∙)) is also constant. Therefore, we seek an N that perform only the
necessary normalization for robustness and tas minimal impact on accuracy.
Wefirst construct the relational graphG R ={V, E} of R: the vertex set V contains all elements
in X; the edge set E contains an edge (x,z) iff (x,z)∈R . Then, a directed path exists from x to z
iff X →R z. We derive the following necessary and sufficient condition for robustness under N&P in
Observation 1, and thus obtain a normalizerNin Proposition 1 that guarantees robustness.
Observation 1 (Condition forRobustness). Let Ci,…，Ck denote the weakly connected components
(WCC) inG R. A classifierfis robust for allX∈C i ifff(X)returns the same label for allX∈C i.
Proposition 1 (Choice of Normalizer). Let N be a function that maps an input X∈C i to any
deterministic element in Cii. Then f(Nl)) isbubust R R-reionaπalvdrearαries.2
4.3	Robustness-Accuracy Trade-off
Optimal Accuracy under N&P. Let μ(x) denote the probability mass of x.	The label
of an input X may also be probabilistic in nature, therefore we use η(X, l) = Pr(y=l|X)
to denote the probability that x has label l. 3 Then the optimal robust accuracy using
N&P, denoted by AccR, is ECi maxι∈γ Eχ∈Ci μ(x)η(x,l), which happens when f (N^(x))=
argmaxι∈γ Eχ∈∕ μ(x)η(x,l) for X ∈ Ci. Intuitively, f shall assign the most likely label of
random samples inC i to allx∈C i .
Price of Robustness. InN &P, the optimal robust accuracy depends on R. We then observe the
following fundamental robustness-accuracy trade-off: as the relation becomes more complicated, we
may lose accuracy for enforcing invariant model predictions, and such loss is the price of robustness.
Observation 2 (Robustness-accuracy trade-off). Let R and R be two relations s.t. R =
R {(x,z)} , i.e. R allows an extra transformation from x to z than R. Let Cx,R denote the WCC in
GR that contains X, and lc be the most likely label ofinputr in a WCC C. Then AccRz 一 AICCR ≤ O
for allR,R pairs, and the equality only holds whenl Cx,R =l Cz,R.
The intuition is that the extra edge on the relation graph may join two connected components which
are otherwise separate. As a result, a model underN &Pwill predict the same label for the two
components, thus the accuracy on one component will drop if two components have different labels.
We further characterize three different levels of trade-offs (Figure 1). First, if two inputs X,z have the
same most likely label on D, then the optimal accuracy underN &Pis the same as before normalization,
in other words, robustness is obtainedfor free. Second, if both (X,z) and (z,X) are in R but X,z
have different most likely labels, then the model with the highest natural accuracy, which predicts the
most likely label of X and z respectively, do not have any robustness. In contrast,N &Pachieves the
optimal robust accuracy by predicting asinglelabel — the most likely label of samples in {X,z} —
for both X and z. Third, if X can only be one-way transformed to two inputs z1,z 2 with different
2Appendix C.1 shows a decidable algorithm of realizing such anNgivenG R.
3For example, a ransomware and a zip tool may have the same static feature vector x. The label of a randomly
drawnxis probabilistic, and the probability depends on the frequency that each software appears.
5
Under review as a conference paper at ICLR 2021
most likely labels, thenN &Pmay have suboptimal robust accuracy. An absolutely robust classifier
need to predict the same label for x,z 1 and z2, while the classifier with the highest robust accuracy
should predict the mostly likely labels forz 1 andz 2 ifz 1,z 2 appear more frequently thanx.
5	Comparing and Unifying N&P with Adversarial Training
N&Pdiffers from the adversarial training — the most widely acknowledged defense mechanism
against test-time adversary — in its objective and procedure. While each approach has its own
limitation against relational adversaries, we show that they can complement each other and be unified
into one framework that enjoys the benefits from both worlds.
Comparative Advantages. The performance of adversarial training depends on the quality of the
adversarial examples. However, we show in Proposition 2 that the inner maximization problem is in
general computationally infeasible for relational adversaries.
Proposition 2 (Hardness of Inner Maximization). The inner optimization problem of adversarial
training is PSPACE-hard for relational adversaries.
Intuitively, the search space of a relational adversary can grow combinatorially with the number of
transformations, and the proposition follows the classical results of reachability analysis in model
checking (Kozen, 1977). TheN &Pframework, in contrast, solves a typical minimization problem,
and thus reduces the computation complexity if an efficient normalizer exists. Meanwhile, we show
in Appendix A.4 that robust accuracy can be achieved with a simpler model class on normalized
inputs than on original inputs; reduced model complexity may also improve the sample efficiency of
the underlying learning algorithm. On the other hand,N &Pmay incur excessive loss in accuracy to
enforce robustness, for example, the last scenario in Figure 1, in which case, adversarial training will
be a better choice for overall utility.
A Unified Framework. Motivated by the above observations, we propose a unified framework: for a
relation R, we strategically select a subset R ⊂R to normalize inputs, and adversarially train on
the normalized inputs. LetN R denote the normalizer forR . Formally, the learner solves
fmiHn max	(f, A(N R (x)), y),	(2)
(x,y)∈D
during training to obtain a minimizer f *, and predicts f *(NR∕ (x)) at test-time. The classifier f *
will be robust to R-relational adversary, and have potentially higher robust accuracy than usingN &P
alone. In particular, if R isreversibleby Definition 3, then our unified framework preserves the
optimal robust accuracy as shown in Theorem 1.
Definition 3. re eιlatθnR R' is reversible iff x →r,* Z impiies ZTr,* X and i^cee VerS(L
Theorem 1 (Preservation of robust accuracy). Let f* be the classifier that minimizes the objective of
tur unified framewtrk tver data distributitn D, and let fa*dv minimize the tbjective tf adversarial
training over D. Then, in principle, f * (NR (∙)) and 于；壮嵬 have the same optimal robust accuracy if
R is reversible.
The proof can be found in Appendix A.5. In essence, Theorem 1 is a generalization of the second
scenario in Figure 1, in particular, we extend the same principle applied to (x,z) to all possible pairs
of inputs in the relational graph induced by R . Note that reversible relation is also common: if z
is x’s adversarial example, then x is also likely to be an adversarial choice of z. Observation 2 and
Theorem 1 provide a general guideline for selectingR : choose the reversible subset ofRfirst, and
then consider transformations that cause little drop in optimal robust accuracy.
Regarding the efficiency of normalization, we show in Appendix A.6 that the strongest adversarial
example satisfies the requirment of Proposition 1, and thus can be used as the normal form. Therefore,
in theory,N &Pis at least as efficient as the optimal adversarial training. In practice, the normalizer
we use in our empirical evaluation are all more efficient than adversarial training.
6	Experiment
We now evaluate the effectiveness of our unified framework against relational attacks. In particular,
we seek answers to the following questions:
6
Under review as a conference paper at ICLR 2021
Table 2: Malware Detection: False Negative Rate (FNR) and False Positive Rate (FPR) onSleipnir.
	Unified (Ours)Adv		-Trained	Al-Duja		ili et al. (2018)		Natural	
	FNR(%)	FPR(%)	FNR(%)	FPR(%)	FNR(%)	FPR(%)	FNR(%)	FPR(%)
Natural	5.0±0.4	11.9±1.2	5.8±0.9	12.1±1.2	6.4±0.5	10.7±0.3	6.2±0.6	10.0±0.6
Adversarial	5.5±0.5	11.9±1.2	27.9±8.2	12.1±1.2	89.9±7.8	10.7±0.3	100±0.0	10.0±0.6
1.	Do relational attacks pose real threats to existing ML models?
2.	How effective is our unified framework in enhancing robustness, and do the results corrobo-
rate with the theory?
We investigate these aspects over two real world tasks — malware detection and image classification.
For each task, we identify relations that do not alter the essential semantics of the inputs. Our result
shows that the models obtained from our unified framework has the highest robust accuracy compared
to adversarially trained models and unprotected models.
6.1	Malware Detection
We evaluate a malware detection task onSleipnir, a data set containing Windows binary API usage
features of 34,995 malware and 19,696 benign software, extracted from their Portable Executable
(PE)files using LIEF (Thomas, 2017). The detection is exclusively based on the API usage of a
malware. There are 22,761 unique API calls in the data set, so each PEfile is represented by a binary
indicator vector x∈{0,1} m, where m= 22,761 . Note that this is the same encoding scheme
adopted by Al-Dujaili et al. (2018). We sample 19,000 benign PEs and 19,000 malicious PEs to
construct the training (60%), validation (20%), and test (20%) sets.
Existing p norm based attacks are not applicable for relational adversaries. Meanwhile, exhaustive
search over adversarial choices may be computationally prohibitive. Therefore, we propose two
heuristic attack algorithms - GREEDYBYGROUP and GREEDYBYGRAD - to validate models' robust
accuracy. Both algorithms are greedy and iterative in nature. Detailed algorithm descriptions are in
Appendix C.2.
GreedyByGroup takes a test input vector x and a maximum number of iterations K. In each
iteration, it partitions R into subsets of relations Ri,…RQm, and fiinds hie insθance Wthlin hie
transitive closure of each Ri that maximizes the loss. These instances from all Ri s are combined to
create the new version ofx adv. Notice the attack reduces to exact search ifRis not partitioned.
GreedyByGrad takes a test input vector x, a maximum number m of transformation to apply in
each iteration, and a maximum number of iteration K . In each iteration, it makes afirst-order approx-
imation of the change in test loss caused by each transformation, and then applies the transformations
with topmapproximated increases in test loss to create the new version ofx adv .
Relation and Attacks. The goal of an adversary is to evade a malware detector. A common strategy
that (Al-Dujaili et al., 2018) also adopts is adding redundant API calls. This strategy can be described
by an additive relation: (x,z)∈R iff z is obtained byflipping some x’s feature values from
0 to 1. We also consider a new attacking strategy, which substitutes API calls with functionally
equivalent counterparts. This strategy can be described by an equivalence relation: (x,z)∈R iff
z is obtained by changing some of x’s feature values from 1 to 0 in conjunction with some of x’s
other feature values changed from 0 to 1. With expert knowledge, we extract nearly 2,000 equivalent
API groups described in Appendix C.3. We use three attack algorithms — GreedyByGrad,
GREEDYBYGROUP and the rfgsm_k additive attack presented by Al-Dujaili et al. (2018) — and
consider the attack to be successful if any algorithm fools the detector.
Model and Baselines. We compare four ML detectors. The Unified detector is realized using our
unified framework in Sec 5: we normalize over the equivalence relation based on the functionally
equivalent API groups, and then adversarially trains over the additive relation. The Adv-Trained
detector is adversarially trained with the best adversarial example generated using GreedyByGrad
and the rfgsm_k additive attack (Al-Dujaili et al., 2018) as GREEDYBYGROUP is too computation-
ally expensive to be included in the training loop. We also include the model proposed by Al-Dujaili
et al. (2018), which is adversarially trained against only the rfgsm_k additive attack, and a Natural
7
Under review as a conference paper at ICLR 2021
Table 3: Image Classification: Classification accuracy on CIFAR10, same relation in training and
testing. Thefirst column specifies the attack parameters used in test-time. The parameters are in the
form of (∞-norm, PGD step size, PGD steps, number of hue-shifts). The models are adversarially
trained using(4/255,2/255,3,20).
	Unified (Ours)A	dv-Trained (Combined) A	dv-Trained (PGD only)
Natural	73.4±1.0	73.7±1.2	78.6±3.7
(4/255,2/255,3,20)54.9±	1.2	5	0.0±0.8	2	8.5±3.6
(4/255,2/255,3,200)54.9	±1.2	49.1±0.6	28.3±3.6
(4/255,2/255,15,20)54.3	±1.1	48.6±0.9	27.9±3.6
(4/255,2/255,200,20)54.2	±1.2	48.2±0.9	27.8±3.6
(6/255,2/255,15,20)42.4	±1.4	34.5±1.1	17.4±3.8
(6/255,2/255,200,200)42	.3±1.5	33.8±L0	17.2±3.8
Table 4: Image Classification: Classification accuracy on CIFAR10, relation in training is a subset of
relation in testing. The attacker uses a 15-step PGD attack with ∞-norm 4/255 and step size 2/255,
and randomly samples500combinations of hue, brightness and constrast adjustment factors.
	Unified (Ours)A	AdVaTnein(C(Cbmbined)	dv-Trained (PGD only)
Natural	73.4±1.3	73.7±1.2	78.6±3.7
Adversarial	47.3±1.1	41.7±1.1	19.7±3.8
model with no defense. We use the same network architecture as Al-Dujaili et al. (2018), a fully-
connected neural net with three hidden layers, each with 300 ReLU nodes, to set up a fair comparison.
We train each baseline to minimize the negative log-likelihood loss for 20 epochs, and pick the model
with the lowest validation loss. We runfive different data splits.
Results. As Table 2 shows, relational attacks are overwhelmingly effective to detectors that are
oblivious to potential transformations. Adversarial examples almost always (>99% FNR) evade the
naturally trained model, and also evade the detector in Al-Dujaili et al. (2018) most of the time (>89%
FNR) as it does not consider API substitution. On the defense end, Unified achieves the highest robust
accuracy: the evasion rate (FNR) only increases by 0.5% on average. Adv-Trained comes second but
the evasion rate is still 22.1% higher. The evasion is mostly caused by GreedyByGroup, the attack
that is too computationally expensive to be included in the training loop. This result corroborates
with the theoretical advantage ofN &P: its robustness guarantee is independent of training algorithms.
Last, all detectors using robust learning techniques have higher FPR compared to Natural, which is
expected because of the inevitable robustness-accuracy trade-off. However, the difference is much
smaller compared to the cost due to attacks, and thus the trade-off is worthwhile.
6.2	Image Classification
We evaluate the effectiveness of our unified framework on CIFAR10 containing 50,000 training and
10,000 test images of size 32x32 pixels. We randomly sample 5,000 images for validation and train
on the remaining 45,000 images.
Relation and Attacks. We consider a relation induced by hue shifting specified in Example 1. Due
to the shape bias property (Landau et al., 1988), humans can still correctly classify most images after
the adjustment of color hue. Therefore, we consider this relation to be semantics-preserving. The
attacker uses a combination of ∞ and relational attacks: itfirst shifts the color hue of the image,
and then generates ∞ adversarial example using PGD attack. For each image, the attacker tries
different hue adjustments, which evenly split the hue space. In addition, we consider an attacker that
can also adjust the brightness and contrast of the image by a factor in [0.8,1.2] . It tries 500 random
combination of hue, brightness and contrast adjustments followed by PGD attack.
Model and Baselines. The Unified classifier is obtained with our unified framework in Sec 5: we
adjust hue of the input such that the pixel at the top-left corner has hue value 1, and then adversarially
8
Under review as a conference paper at ICLR 2021
train against the PGD attack. 4 We also consider two adversarial training baselines: thefirst uses the
combined attack (PGD and hue adjustment) in training, while the second only uses the PGD attack.
We train a ResNet32 network for 100 epochs in all configurations, and pick the model with the lowest
validation loss. We also runfive different data splits.
Results. Table 3 shows the results against attackers using hue-shift and ∞ perturbation. Although
adversarial training against only the PGD attack has higher clean input accuracy, the combined attack
heavily reduces its test accuracy, indicating again the effectiveness of simple relational attack to
unprotected models. Unified achieves the highest robust accuracy against the combined attack -
≥4.8% higher compared to adversarial training with the combined attack over all attack parameters.
This result shows the advantage of normalization over reversible relations, as projected by our analysis
in Sec 5. In addition, Table 4 shows the results against an attacker using more transformations than the
ones normalized in training. Our unified approach still achieves the highest accuracy with a substantial
margin over the baselines. Although the attacker may use more transformations, normalization can
still reduce the search space of adversarial examples and increase robustness.
7 Conclusion and Future Work
In this work, we set thefirst step towards robust learning against relational adversaries: we theoreti-
cally characterize the conditions for robustness and the sources of robustness-accuracy trade-off, and
propose a provably robust learning framework. Our empirical evaluation shows that a combination of
input normalization and adversarial training can significantly enhance model robustness. For future
work, we see automatic detection of semantics-preserving transformation as a promising addition to
our current expert knowledge approach, and plan to extend the normalization approach to deal with
other kinds of attacks beyond relational adversaries.
4Given an input image in RGB format, wefirst convert the image to HSV format, and then add a scalar to the
hue of all pixels. The scalar is determined by 1 - (the hue of the pixel on thefirst row andfirst column). The hue
values are then projected back to the [0,1] interval by taking the remainder over 1. Finally, we convert the image
back to RGB for classification.
9
Under review as a conference paper at ICLR 2021
References
Abdullah Al-Dujaili, Alex Huang, Erik Hemberg, and Una-May O’Reilly. Adversarial deep learning
for robust detection of binary encoded malware. In2018 IEEE Security and Privacy Workshops
(SPW),pp. 76-82. IEEE, 2018.
Richard Baumann, Mykolai Protsenko, and Tilo Muller. Anti-proguard: Towards automated deobfus-
cation of android apps. InProceedings of the 4th Workshop on Security in Highly Connected IT
Systems, SHCIS ’17, pp. 7-12, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-5271-0. doi:
10.1145/3099012.3099020. URL http://doi.acm.org/10.1145/3099012.3099020.
Anand Bhattad, Min Jin Chong, Kaizhao Liang, Bo Li, and David A Forsyth. Unrestricted adversarial
examples via semantic manipulation.arXiv preprint arXiv:1904.06347, 2019.
Benjamin Bichsel, Veselin Raychev, Petar Tsankov, and Martin Vechev. Statistical deobfuscation
of android applications. InProceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’16, pp. 343-355, New York, NY, USA, 2016. ACM. ISBN 978-
1-4503-4139-4. doi: 10.1145/2976749.2978422. URL http://doi.acm.org/10.1145/
2976749.2978422.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
InProceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26. ACM,
2017.
Mihai Christodorescu, Somesh Jha, Johannes Kinder, Stefan Katzenbeisser, and Helmut Veith.
Software transformations to improve malware detection.Journal in Computer Virology, 3:253-265,
10 2007. doi: 10.1007/s11416-007-0059-8.
Kevin Coogan, Gen Lu, and Saumya Debray. Deobfuscation of virtualization-obfuscated software:
A semantics-based approach. InProceedings of the 18th ACM Conference on Computer and
Communications Security, CCS ’11, pp. 275-284, New York, NY, USA, 2011. ACM. ISBN 978-
1-4503-0948-6. doi: 10.1145/2046707.2046739. URL http://doi.acm.org/10.1145/
2046707.2046739.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotflip: White-box adversarial exam-
ples for text classification. InProceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pp. 31-36, 2018.
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul
Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning
visual classification. InProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1625-1634, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples.arXiv preprint arXiv:1412.6572, 2014.
Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and Patrick McDaniel.
Adversarial examples for malware detection. InEuropean Symposium on Research in Computer
Security, pp. 62-79. Springer, 2017.
Mark Handley, Vern Paxson, and Christian Kreibich. Network intrusion detection: Evasion, traffic
normalization, and end-to-end protocol semantics. InProceedings of the 10th Conference on
USENIX Security Symposium - Volume 10, SSYM’01, Berkeley, CA, USA, 2001. USENIX
Association. URLhttp://dl.acm.org/citation.cfm?id=1251327.1251336.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Hossein Hosseini and Radha Poovendran. Semantic adversarial examples. InProceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 1614-1619, 2018.
10
Under review as a conference paper at ICLR 2021
Hossein Hosseini, Baicen Xiao, Mayoore Jaiswal, and Radha Poovendran. On the limitation of
convolutional neural networks in recognizing negative images. In2017 16th IEEE International
Conference on Machine Learning and APPlications (ICMLA), pp. 352-358. IEEE, 2017.
Weiwei Hu and Ying Tan. Black-box attacks against rnn based malware detection algorithms. In
WorkshoPs at the Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishna-
murthy Dvijotham, and Pushmeet Kohli. Achieving verified robustness to symbol substitutions via
interval bound propagation. pp. 4074-4084, 2019.
Inigo Incer, Michael Theodorides, Sadia Afroz, and David Wagner. Adversarially robust malware
detection using monotonic classification. Inthe Fourth ACM International WorkshoP on Security
and Privacy Analytics (IWSPA), Tempe, AZ, USA, Mar. 2018.
G. Katz, C. Barrett, D.L. Dill, K. Julian, and M.J. Kochenderfer. Reluplex: An efficient smt solver
for verifying deep neural networks. InInternational Conference on ComPuter Aided Verification,
2017.
A. Koschan, M. Abidi, and M.A. Abidi.Digital Color Image Processing. Wiley, 2008. ISBN
9780470147085.
Alex Kouzemtchenko. Defending malware classification networks against adversarial perturbations
with non-negative weight restrictions.arXiv PrePrint arXiv:1806.09035, 2018.
Dexter Kozen. Lower bounds for natural proof systems. InFOCS, 1977.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Barbara Landau, Linda B Smith, and Susan S Jones. The importance of shape in early lexical learning.
Cognitive develoPment, 3(3):299-321, 1988.
Qi Lei, Lingfei Wu, Pin-Yu Chen, Alexandros G Dimakis, Inderjit S Dhillon, and Michael Witbrock.
Discrete adversarial attacks and submodular optimization with applications to text classification.
Systems and Machine Learning (SysML), 2019.
Jeet Mohapatra, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Towards verifying
robustness of neural networks against a family of semantic perturbations. InProceedings of the
IEEE/CVF Conference on ComPuter Vision and Pattern Recognition, pp. 244-252, 2020.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. InProceedings of the IEEE conference on comPuter
vision and Pattern recognition, pp. 2574-2582, 2016.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. InProceedings of the 2017 ACM on
Asia conference on comPuter and communications security, pp. 506-519, 2017.
Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. Intriguing properties
of adversarial ml attacks in the problem space. In2020 IEEE SymPosium on Security and Privacy
(SP), pp. 1308-1325. IEEE Computer Society, 2020. doi: 10.1109/SP40000.2020.00073. URL
https://doi.ieeecomputersociety.org/10.1109/SP40000.2020.00073.
Yao Qin, Nicholas Carlini, Garrison Cottrell, Ian Goodfellow, and Colin Raffel. Imperceptible, robust,
and targeted adversarial examples for automatic speech recognition. InInternational Conference
on Machine Learning, pp. 5231-5240, 2019.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. InProceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016.
Ishai Rosenberg, Asaf Shabtai, Lior Rokach, and Yuval Elovici. Generic black-box end-to-end attack
against state of the art api call based malware classifiers. InInternational SymPosium on Research
in Attacks, Intrusions, and Defenses, pp. 490-510. Springer, 2018.
11
Under review as a conference paper at ICLR 2021
Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Defense methods against adversarial
examples for recurrent neural networks.arXiv preprint arXiv:1901.09963, 2019.
Aleieldin Salem and Sebastian Banescu. Metadata recovery from obfuscated programs using machine
learning. InProceedings of the 6th Workshop on Software Security, Protection, and Reverse
Engineering, SSPREW '16, pp. 1:1-1:11, New York, NY, USA, 2016. ACM. ISBN 978-1-
4503-4841-6. doi: 10.1145/3015135.3015136. URL http://doi.acm.org/10.1145/
3015135.3015136.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks.arXiv preprint arXiv:1312.6199, 2013.
Romain Thomas. Lief - library to instrument executable formats. https://lief.quarkslab.com/, April
2017.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks.arXiv preprint arXiv:1704.01155, 2017.
Fanny Yang, Zuowen Wang, and Christina Heinze-Deml. Invariance-inducing regularization using
worst-case transformations suffices to boost accuracy and spatial robustness. InAdvances in Neural
Information Processing Systems, pp. 14757-14768, 2019.
Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael I Jordan. Greedy attack and
gumbel attack: Generating adversarial examples for discrete data.Journal of Machine Learning
Research, 21(43):1-36, 2020.
12