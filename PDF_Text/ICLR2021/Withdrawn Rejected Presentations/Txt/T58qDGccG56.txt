Under review as a conference paper at ICLR 2021
On the Stability of Multi-branch Network
Anonymous authors
Paper under double-blind review
Ab stract
Multi-branch architectures are widely used in state-of-the-art neural networks.
Their empirical success relies on some design wisdom, like adding normalization
layers or/and scaling down the initialization. In this paper, we investigate the
multi-branch architecture from the stability perspective. Specifically, we establish
the forward/backward stability of multi-branch network, which leads to several
new findings. Our analysis shows that only scaling down the initialization may
not be enough for training multi-branch network successfully because of the un-
controllable backward process. We also unveil a new role of the normalization
layer in terms of stabilizing the multi-branch architectures. More importantly, we
propose a new design “STAM aggregation” that can guarantee to STAbilize the
forward/backward process of Multi-branch networks irrespective of the number of
branches. We demonstrate that with STAM aggregation, the same training strategy
is applicable to models with different numbers of branches, which can reduce the
hyper-parameter tuning burden. Our experiments verify our theoretical findings
and also demonstrate that the STAM aggregation can improve the performance of
multi-branch networks considerably.
1	Introduction
Multi-branch architecture is a building block in state-of-the-art neural network models for many tasks,
e.g., the ResNeXt (Xie et al., 2017) for computer vision and the Transformer (Vaswani et al., 2017)
for machine translation. It has been pointed out that the benefit of multi-branch architecture is the
parameter efficiency (Xie et al., 2017). The number of parameters grows linearly with the number
of branches but quadratically with the width (the number of neurons in one layer). It has also been
argued that the multiple branches can bring diversity if branches are composed of sub-networks with
different filters and depths (Huang et al., 2017; Li et al., 2019).
To train multi-branch networks successfully, it usually requires careful designs and some hyper-
parameter tuning such as adding normalization layers, scaling down the initialization, and adjusting
learning rate. As a verifying example, for a trainable single-branch network, simply adding branches
multiple times and aggregating their outputs together often do not work as expected, e.g., the training
instability of sum aggregation in Figure 4. This demonstrates the difficulty of training multi-branch
network and also motivates us to do this study.
In this paper, we try to understand the behavior of training multi-branch network. Specifically, we
study the forward and backward process of multi-branch networks, which is believed to govern
whether the network is easy to optimize by gradient-based methods. We find out that the aggregation
scheme, i.e., “the way of combining the multi-branch outputs” plays a central role in determining the
behavior of training multi-branch network. We show that the sum aggregation would become unstable
as the number of branches grows, which explains the bad performance of simply adding branches.
Moreover, we characterize the condition on the aggregation scheme under which the forward and
backward stability is guaranteed.
Inspired by the theoretical analysis, we propose a “STAM” aggregation, that can STAbilize Multi-
branch network, which scales the sum of the branch outputs by a branch-aware factor α (see the later
part of Section 3.1 for details). We argue the benefit of STAM aggregation over the sum and average
aggregations by analyzing the Hessian of the multi-branch network. We show that STAM permits the
same gradient-based optimizer works for different settings, which could reduce lots of tuning burden
for training network with flexible number of branches.
1
Under review as a conference paper at ICLR 2021
We further examine the usual design wisdom through the stability lens. As a result, we find that
scaling down initialization may control the forward or backward stability but not necessarily the both,
which is verified in experiment. We also unveil a new role of normalization layer that it can stabilize
the forward and backward process of multi-branch network besides the many wanted and unwanted
properties that have been argued before (Ioffe & Szegedy, 2015; Yang et al., 2018; Santurkar et al.,
2018; Xiong et al., 2020).
Apart from the usual feedforward multi-branch architecture, we analyze the multi-head attention
layer, a multi-branch architecture widely used in natural language processing. We give an upper
bound on the multi-head representations when the softmax operator is replaced with max operation.
The upper bound unveils the relation between the head dimension and the length of the sequence,
which interprets empirical observation well. This relation cannot be discovered if assuming softmax
outputs equal probability as in Xiong et al. (2020).
Overall, our contribution can be summarized as follows.
•	We analyze the forward/backward stability of multi-branch network, under which we can
clearly interpret the benefit and potential problem of the practical wisdom, i.e., scaling down
initialization and adding normalization layer.
•	We propose a theoretically inspired STAM aggregation design for multi-branch network,
which can handle arbitrary number of branches with a same optimizer.
•	We also analyze the forward/backward process of multi-head attention layer and identify its
special property that has not been characterized before.
1.1	Related Work
Multi-branch architecture, also known as split-transform-merge architecture, has been widely used in
computer vision task, namely Inceptions (Szegedy et al., 2017; Chollet, 2017), ResNeXt (Xie et al.,
2017), and many others (Abdi & Nahavandi, 2016; Ahmed & Torresani, 2017). In fact, the models
in natural language tasks have also leveraged the multi-branch architecture including the BiLSTM
(Wu et al., 2016; Zhou et al., 2016) and the multi-head attention layer in Transformer (Vaswani et al.,
2017; Anonymous, 2020). Apart from the sum or average aggregation, recent works (Li et al., 2019;
Zhang et al., 2020) integrate the attention mechanism with the aggregation scheme, i.e., the attentive
aggregation, although only a small number (2 〜3) of parallel branches are considered.
Theoretically, Zhang et al. (2018) interpret the benefit of multi-branch architecture from reducing
the duality gap or the degree of non-convexity. The theory of training general deep neural network
has been widely studied, via the stability analysis (Arpit et al., 2019; Zhang et al., 2019a;c; Yang &
Schoenholz, 2017; Zhang et al., 2019b; Yang, 2019; Lee et al., 2019), neural tangent kernel (Jacot
et al., 2018; Allen-Zhu et al., 2018; Du et al., 2018; Chizat & Bach, 2018; Zou et al., 2018; Zou & Gu,
2019; Arora et al., 2019; Oymak & Soltanolkotabi, 2019; Chen et al., 2019; Ji & Telgarsky, 2019). In
contrast, we focus on the multi-branch network, which has not been studied theoretically before.
2 Model Description and Notations
In practice, the multi-branch architecture is often used as a building block in a whole network. In this
paper, we describe a multi-branch architecture/network N(∙) as follows (see Figure 1).
•	N(∙) has C branches {Bk}C=ι, input hn ∈ Rp and output h0u ∈ Rd;
•	The aggregation is parameterized with a vector α = (α1, . . . , αC )T :
C
hout := NEin)= X αk ∙Bk (hin).	(1)
k=1
Each branch Bk often consists of multiple layers with various structures and flexible configuration:
depth, width, kernel size for convolution layer, activation functions and normalization layer. The
aggregation weight is given by α. Such description covers popular multi-branch architectures in
state-of-the-art models, e.g., Inception, ResNeXt and Transformer, if specifying Bk and α properly.
2
Under review as a conference paper at ICLR 2021
Figure 1: A multi-branch network.
Figure 2: Output norm after first aggregation
in multi-branch ResNets at initialization. The
norm ratio is with respect to the case of C = 4.
Throughout the paper, We use k ∙ k to denote the l2 norm of a vector. We further use ∣∣ ∙ k and ∣∣∙∣∣f
to denote the spectral norm and the Frobenius norm of a matrix, respectively. We denote a set of
naturals With [n] := {1, . . . , n} and [c : d] = {c, c + 1, . . . , d}. We use bold small case letters, e.g.,
v, to denote vectors and bold capital case letters, e.g., M, to denote matrices. Moreover, Id×d is the
d × d identity matrix, 1d is d dimensional vector With all 1’s, and Vec(M) stacks the roW vectors of
matrix M as a long vector.
3 Stability of Multi-branch Feedforward Network
To theoretically study the multi-branch netWork, We introduce a simplified multi-branch feedforWard
netWork. Specifically, We assume that each branch is a b-layer fully connected netWork With ReLU
activation, and each layer has the same Width m. Branches share the same structure and they differ
from each other by the random initialization. One branch is given by
Bk(hin) = WkΦ(Wk-1 …Φ(Wkhin)),	(2)
where Wk ∈ Rm×p, WIk ∈ Rd×m and φ(∙) is the ReLU activation φ(∙) := max{0, ∙}. We further
introduce -W→k := (W1k, . . . , Wbk) that collects parameters of Bk, and -W→ := (-W→1 , . . . , -W→C).
Next we analyze the forward and backward propagation of the multi-branch network given by
(Equation 1) and (Equation 2), and characterize a condition that guarantees forward/backward
stability. Based on the theoretical analysis, we propose the STAM aggregation that can stabilize the
forward/backward process of multi-branch networks. We further argue why the practical wisdom of
scaling down initialization and adding normalization layer works and when it could fail.
3.1	Forward and backward process
We assume that the feedforward multi-branch network given by (Equation 1) and (Equation 2) adopts
the Kaiming’s initialization (He et al., 2016): entries of Wa for a ∈ [1 : b - 1] are independently
sampled from N(0,号),and entries of Wb are independently sampled from N(0, d). Then the
forward norm is well concentrated around its mean value as follows.
Theorem 1.	Suppose the multi-branch network N(∙) is given by Equation 1 and Equation 2 with
Kaiming's initialization. For an input hin, the following holds with probability at least 1 一 O(bC) ∙
e-O(me2/b) over the initialization randomness of W
∣N(hin)∣ ∈ (1 ±)
JX a llhink,
k∈[C]
(3)
where C is the number of branches, and m, b are the width and depth of each branch, respectively.
Proof. The proof is based on the Gaussianness of Bk (hin) and a concentration property, whose full
version is deferred to Appendix A.2.	□
3
Under review as a conference paper at ICLR 2021
Theorem 1 is presented for one input sample. If we want such a result holds for all the training
samples, the probability loses an n factor by the union bound and becomes 1 - O(nbC) ∙ e-Q(me2/b).
Remark 1. With the same assumption as Theorem 1, we have
EkN(hin)k
/X
k∈[C]
α2kkhink,
(4)
where C is the number of branches and the expectation is over the random initialization.
These results are based on the bound of the forward propagation of one feed-forward branch given by
(Equation 2), which is studied in Allen-Zhu et al. (2018) and restated in Appendix A. Furthermore, if
the weight matrices follows the Gaussian distribution, then Bk (hin) is also roughly Gaussian and
jointly Gaussian for different input samples, as the width m goes to infinity. Then the aggregation
(Equation 1) can be viewed as a sum of weighted Gaussian vectors. Hence at the initialization, we
can characterize the aggregation of multiple branches as above.
We next analyze the backward propagation of multi-branch feedforward network. We abuse “gradient”
to refer to the values computed through back-propagation even for non-smooth function. We assume
the loss function '(∙, ∙) is quadratic, i.e., '(hout, y*) = 1 ∣∣hout - y*∣∣2. Hence, the objective function
is L(W) = 1 Pi=ι Li(W), where Li(W) = '(N(xi), y*). We next show the backward
process is bounded for each individual sample for the multi-branch network N(∙).
Theorem 2.	With probability at least 1 — (nb) ∙ exp(一Ω(m)) over the randomness of W, it satisfies
for every a ∈ [b] and k ∈ [C], every i ∈ [n],
kWkLi(W)IIF ≤ o (Li(W)αk×m), ∣∣vwLi(W)∣∣F ≤ o(Li(w)× mdb X a)∙⑸
F	F	k∈[C]
Proof. We can compute the gradient with respect to intermediate layer outputs via the backward
propagation procedure. The gradient upper bound is guaranteed if the intermediate layer outputs and
their gradients are bounded across layers. The full proof is relegated to Appendix A.3.	□
If further assuming the gradient independence condition: weights in backward process can be assumed
to be independent from weights in the forward pass (Yang, 2019), we can estimate the expectation of
the gradient norm as follows.
Remark 2. Assuming the gradient independence, we have for every a ∈ [b] and k ∈ [C], every
i ∈ [n],
E∣∣VwaLi(W)∣∣2 = Li(W)αk×m, E∣∣VWLi(W)∣∣2 = Li(W) × mdb X α∙	(6)
F	F	k∈[C]
With Theorem 1 and 2 and two remarks, we can discuss the property of the forward and backward
process of the multi-branch network. We can see that both the output of multi-branch network and
the gradient are under control if P α2k ≤ O(1). Specifically, for the sum aggregation, we have
P α2k = C which grows unbounded with the number of branches C. For the average aggregation,
we have P α2k = 1/C which diminishes with the number of branches C.
There exists a better choice of ɑk: αk = 1∕√C for k ∈ [C] that keeps P Ok = 1 constant
as the number of branches varies. We call it “STAM” aggregation, abbreviating STAble Multi-
branch aggregation. We plot the output norm of the first residual block in multi-branch ResNets at
initialization in Figure 2. Multi-branch ResNets are generated by varying the number of branches in
the residual block with batch normalization removed. We can see that the forward norm of STAM
aggregation roughly remains the same, while that of the sum aggregation explodes and that of the
average aggregation diminishes, as the number of branches grows.
We also analyze the Hessian of different aggregation schemes. We find that the spectral norm of the
Hessian, which determines the smoothness of the objective, proportionally scales with the square
root of the number of branches for the sum aggregation, while reciprocally scales with the square
root of the number of branches for the average aggregation. In contrast, the Hessian for the STAM
aggregation keeps unchanged as the number of branches varies. Hence with STAM, the same learning
rate works for network with different number of branches. We present the details in Appendix B.
4
Under review as a conference paper at ICLR 2021
3.2	Understanding the Practical Wisdom
In practice people have design wisdom to stabilize the multi-branch network. We next analyze them
through the stability lens.
One can scale down the initialization for multi-branch network. For example, one can initialize each
Wk with a scaling-down factor C- 21b for all l ∈ [b] and k ∈ [C] so that the forward norm of N(hin)
has a bound around khink, irrelevant with C. However, with this initialization, the norm of the output
update induced by one gradient descent step, scales with bC b. Alternatively, we suggest initializing
—	—1
each Wlk with a scaling-down factor (bC)2(I) for all l ∈ [b] and k ∈ [C] to stabilize the backward
process such that the expected update on the output is irrelevant with b and C. This indicates that a
constant learning rate can train multi-branch network with any b and C, although the forward process
1	— 1
has a diminishing output norm scaling with b-E (bC)2(I). More detailed discussion is presented in
Appendix A.4. We empirically verify this in Section 5.1.
Another widely used technique is adding normalization layers. It is easy to see that normalization
layer can stabilize the forward process as it normalizes the output to be with mean 0 and variance 1.
Moreover, the normalization layer can also stabilize the backward process as the error signal is divided
by the standard deviation which is proportional to CC when propagating backward. Therefore, we
show a new role of the normalization layer that it automatically stabilizes the forward and backward
process of the multi-branch network or in general stabilizes the architecture, besides the previous
understanding of increasing the smoothness (Santurkar et al., 2018) or handling the covariance shift
(Ioffe & Szegedy, 2015) or implicit structure bias (De & Smith, 2020).
We next examine concrete structures. One popular aggregation scheme for multi-branch outputs is
concatenation followed by a linear transformation. This is equivalent to a sum aggregation. The
default Xavier initialization (Glorot & Bengio, 2010) would scale down the initialization of the
linear transformation by roughly 1 / √C to stabilize the forward process but the training could still be
unstable because of the unbounded backward process as discussed above and verified in Section 5.1.
A well-known multi-branch structure is ResNeXt for image classification task. It uses batch normal-
ization on the output of the residual branch to stabilize the forward and backward processes. However,
if we use the pre-act form, i.e., remove the batch normalization on the output of the residual branch
and add a batch normalization on the input of the residual branch, the training becomes unstable as
the number of branches increases (see Section 5.2). With the pre-act residual block, one needs a
normalization layer on the final output of the network, which has been studied in (Xiong et al., 2020).
It could still be unstable for the deep half-precision ResNet, as the output may overflow before the
final normalization layer. This is interesting but beyond the scope of this paper.
Transformer also uses multi-branch structure in the attention layer and fully-connected layer. It does
not use normalization layer on the output of the residual block because of the practical performance
concerns. Hence it is observed unstable training or deteriorating performance when adding the
number of heads in attention layer or adding fully-connected branches (see Section 5.3).
4	Forward and backward process of multi-head attention layer
In this section, we analyze the multi-head attention layer, a multi-branch structure used in Transformer
(Vaswani et al., 2017) with each head viewed as one branch. It is worthy to note that the multi-head
attention layer behaves differently from the feed-forward network and at the same time it is in general
very hard to analyze, because of the softmax operator and the inter-symbol dependence. Previous
work assumes that the softmax outputs equal weights Xiong et al. (2020) and ignores the inter-symbol
dependence, which does not fully reflect the attention behavior.
Suppose that the input is a sequence of symbols X = (x1, . . . , xn)T and each row of X is a symbol
representation xi ∈ Rd . The multi-head self-attention layer, is given by
MultiHead(Q, K, V) = COnCat(hi,…，he)0,	(7)
hc = softmax QQf ) V, for C ∈ [C],	(8)
dk
5
Under review as a conference paper at ICLR 2021
where Q := XQc, K := XKc, V := XVC and QC ∈ Rd×dk, KC ∈ Rd×dk, VC ∈ Rd×dv, OC ∈
Rdv ×d are projection (trainable) matrices and C is the number of heads, dv , dk are dimensions.
The multi-head attention computes each head representation (with dimension dv) independently, and
then concatenates these heads (C in total) together to form a large representation (Cdv dimensional
vector), finally uses a fully-connected layer O ∈ Rd× (Cdv) projecting back to Rd (d is the model
embedding dimension). This indicates that the original multi-head attention uses the sum aggregation.
It is convenient to first consider the case of one head with dimension dv for studying the multi-head
attention layer. We give an upper bound on the forward process when the softmax behaves more like
a “max” operator. The softmax output could be rather spreadout even after training which is usually
the case after initial training.
Proposition 1. Suppose a self-attention layer as described above and assume ∣∣Xik = √d for i ∈ [n].
Ifthe parameters Q, K, V are randomly initialized with N(0, d), then for symbol Xi and its head
representation hi ∈ Rdv , we have
Ekhik2 . dv + (logn - 1 + √2(2dv — 1)logn),	(9)
where “.” means that the bound holds asymptotically and the expectation is over the initialization.
Proof. The proof is based on estimating the extreme Chi-square values, deferred to Appendix C. □
We can see that besides dv, the norm also scales with log n where n is the number of symbols. In
the Transformer literature, one usually fixes C ∙ dv = d, and then the upper bound of head norm
grows with C because of the second term in the right hand side of Equation 9. This term is ignored
in previous analysis, which can dominate when dv < Ω(log n). We plot the norms of self attention
layer (in the first decoder block) in Transformers with varying number of heads on IWSLT de-en task
in Figure 3 to illustrate this point. We can see at initialization the norm does not change with number
of heads because of the expected behavior at initialization (Xiong et al., 2020). However, this is not
true after training (see the curve at Epoch 10). The assumption that softmax outputs uniform simplex
does not hold. With a large C and a small dv , the second term in Equation 9 dominates, which may
hurts the forward/backward stability and the training process, matching the observation in practice.
For the backward process, we can estimate the gradient at initialization with the gradient independence
assumption (Yang, 2019). Suppose the backward signal is {e1 , e2, ..., en}, the gradient on O is
VO = Pi∈[n] eihτ and EkVOkF = Pi∈[n](Ekh∙ik2)keik2, which scales with the head norm.
Large forward process results in large gradients, which may lead to unstable training procedure.
5	Experiments
In this section, we first verify the theoretical claims in Section 3. We then apply the STAM aggregation
on the popular ResNeXt (Xie et al., 2017) model for image classification and on Transformer (Vaswani
et al., 2017) to demonstrate its efficacy.
5.1	Theoretical verification
We conduct some experiments to verify the theoretical claims in Section 3. Specifically, we use
a Fixup ResNet20 (Zhang et al., 2019a) as a backbone network, which uses Fixup initialization
without all batch normalization (BN) layers and can be trained to a decent accuracy. The multi-branch
ResNets are generated by varying the number of branches in each residual block (Figure 1). We train
these networks on the CIFAR10/CIFAR100 classification task (Krizhevsky & Hinton, 2009) and
record the top 1 validation error in Figure 4 with a standard training procedure, i.e., 128 batch size,
200 epoch, initial learning rate 0.1 and step-wise learning rate decay by 1/10 at epoch 100 and 150
respectively.
We compare several approaches: the STAM aggregation with α = 1∕√C ;thesum aggregation with
α = 1; the average aggregation with α = 1/C; the backward scaling-down initialization Y = 1/√C,
referred to as “Back-scaled init” in Figure 4; the concatenation then aggregation by a linear layer
with scaling-down initialization Y = 1/√C, referred to as “linear layer aggregation” in Figure 4. As
6
Under review as a conference paper at ICLR 2021
Figure 3: Head representation norm of the
self attention layer in the first decoder block.
We fix hdv = d and vary h. When dv is
small, the representation norm grows with h.
Figure 4: Top1 validation error of multi-
branch ResNets on CIFAR10. Models
trained with STAM aggregation consis-
tently benefit from increasing branches.
Table 1: Top1 validation accuracy (in %) of ResNeXt models. The numbers with * are reported in
(Xie et al., 2θ17) and other numbers are our implementation (averaged over 3 repeats).
Dataset	Setting	Param.	Basline	+STAM
	8×64d	34.4M	96.35*	96.51(0.07)
CIFAR10	16×64d	68.1M	96.42*	96.57(0.12)
	32×64d	135.9M	96.37(0.11)	96.73(0.13)
	8×64d	34.4M	82.23*	83.17(0.11)
CIFAR100	16×64d	68.1M	82.69*	83.63(0.09)
	32×64d	135.9M	83.26(0.14)	83.86(0.12)
shown in Figure 4, the performance of model trained with STAM aggregation consistently improves
as the number of branches C grows. In contrast, the models with sum aggregation fail to converge for
C > 4, as predicted in Theorem 1 and 2. The performance of average aggregation degrades as the
number of branches is beyond a certain value. In addition, the backward scaling-down initialization
works well in terms of stability as we argued in Section 3.2. However, the scheme of concatenation
then aggregation with a linear transformation becomes unstable as the number of branches increases
even with a scaled-down initialization.
5.2	Apply STAM aggregation to train ResNeXt
We next investigate a well-known multi-branch network, the ResNeXt (Xie et al., 2017) model, which
divides the bottleneck residual block into multiple branches. It uses 1x1 convolution to aggregate
the branch outputs, which is equivalent to sum aggregation if taking the 1x1 convolution as part of
each branch (see Figure 3 in Xie et al. (2017)). ResNext uses a BN layer to normalize the aggregated
result. We have argued how normalization layer can stabilize the forward and backward process in
Section 3. It can be observed that ResNeXt fails to converge for C > 2 without such BN layer.
To verify the efficacy of STAM aggregation on ResNeXt, we use separate BN layer for each branch,
equivalent to viewing the BN layer a part of a branch, and then apply the STAM aggregation on these
outputs. The hyperparameters and the data augmentation are the same as in Xie et al. (2017). From
Table 1, we see that our STAM-aggregation is applicable to the cases with many branches (more
than the original paper) and improves over the strong baselines on both CIFAR10/CIFAR100. More
details about the experiments can be found in Appendix D.
5.3	Apply STAM aggregation to train Transformer with many branches
In this section, we investigate the aggregation scheme on Transformer models (Vaswani et al., 2017)
with machine translation tasks. A Transformer model consists of several encoder and decoder layers,
with each layer stacking one or two multi-head attention blocks and one fully connected (FC) block.
7
Under review as a conference paper at ICLR 2021
Table 2: BLEU scores on IWSLT14 de-en test sets. The higher, the better. * is baseline setting.				Table 3: BLEU scores on newstest2014 for WMT En-De. The higher, the better. The number with * is reported in (Ott et al., 2018).			
h/d/df c	Param.	BLEU	BLEU (+STAM)				
				h/d/dfc	Param.	BLEU	BLEU (+STAM)
4/512/1024	36.7M	35.21*	N/A				
8/256/1024	18.4M	34.96	35.30	16/1024/4096	209.8M	"293*	N/A
12/256/1536	26.3M	ɪŋ^^	35.79	16/512/4096	105.1M	186	28.8
16/256/2048	34.2M	34.74	36.05	32/512/8192	193.2M	24.3	29.4
32/256/4096	65.7M	34.30	36.09	36/512/9216	215.2M	23.5	29.8
There are several hyperparameters about the model setting: model/embedding dimension d, head
dimension dh, number of heads h, intermediate FC dimension dfc. More detailed introduction on
Transformer can be found in Appendix C.
We conduct two sets of experiments: IWSLT14 German-English (de-en) task and the WMT16
English-German (en-de) task. For baseline models, we choose the default Transformer and the
big Transformer for IWSLT14 de-en and WMT16 en-de, respectively. For multi-branch versions
of Transformer, we first construct a mini model, and then generate multi-branch Transformers by
copying mini-models multiple times. This can explore many branches while controlling model size.
For IWSLT14 de-en task, the baseline Transformer configuration is d = 512, dh = 128, dfc =
1024, h = 4. We choose the mini model for IWSLT14 de-en with configuration d = 256, dh =
64, h = 4, dfc = 512, and the generated multi-branch Transformers are with configurations
(d, dh, h, dfc) = (256, 64, 8, 1024), (256, 64, 12, 1536),   These models are trained with the
Adam (Kingma & Ba, 2014) optimizer with initial learning rate 1e-3, β1 = 0.9, β2 = 0.98 and the
inverse Sqrt learning rate scheduler, which are standard hyperparemeter choices following Ott et al.
(2018). For the STAM aggregation, we set α = 1∕√C as C is the equivalent number of mini models.
We evaluate the translation quality by BLEU scores by using multi-bleu.perl. We train every model
on a single Tesla P40 GPU for 200 epochs. We test on a single model that achieves best BLEU score
on the validation set. Other detailed configuration can be found in the code.
For the WMT16 English-German (en-de) task, the baseline big Transformer model has configuration
d = 1024, dh = 64, dfc = 4096, h = 16. We choose the mini model for WMT16 en-de task with
configuration d = 512, dh = 64, h = 8, dfc = 2048, and the generated multi-branch Transform-
ers are with configurations (d, dh, h, dfc,) = (512, 64,16,4096), (512,64,32,8192),... . For the
STAM aggregation, we set α = 1∕√C where C is the equivalent number of mini models. All
models are trained for 150 epochs on a single node with 8 Tesla V100 GPUs. We average the model
parameters of last 5 checkpoints for evaluation.
We report the BLEU scores of all the models on test set in Table 2 and 3. We can see that the
performance of original architecture degrades as the number of branches increases, while with STAM
aggregation the performance gradually improves as the number of branches increases.
6	Conclusion
In this paper, we study the training process of multi-branch network, especially the forward and
backward stability. The theory tells that the sum aggregation is not stable as the number of branches
increases. Motivated by the theoretical analysis of the multi-branch network, we propose the STAM
aggregation that can not only guarantee the forward/backward stability but also allows a same training
strategy works for networks with different number of branches. We show that with STAM aggregation,
the models can consistently benefit from increasing the number of branches. We believe that our
analysis and the proposed STAM aggregation gives practitioners a new direction to design new
multi-branch models.
8
Under review as a conference paper at ICLR 2021
References
Masoud Abdi and Saeid Nahavandi. Multi-residual networks: Improving the speed and accuracy of
residual networks. arXiv preprint arXiv:1609.05672, 2016.
Karim Ahmed and Lorenzo Torresani. Connectivity learning in multi-branch networks. arXiv preprint
arXiv:1709.09582, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962v5, 2018.
Anonymous Anonymous. Hybrid attentive transformer. 2020.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems, 2019.
Devansh Arpit, Victor Campos, and Yoshua Bengio. How to initialize your network? robust
initialization for weightnorm & resnets. In Advances in Neural Information Processing Systems,
2019.
ZD BAI and YQ YIN. Limit of the smallest eigenvalue of a large dimensional sample covariance
matrix. Annals of probability, 21(3):1275-1294,1993.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is
sufficient to learn deep ReLU networks? arXiv preprint arXiv:1911.12360, 2019.
LenaIc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in Neural Information Processing Systems 31. 2018.
Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 1251-1258, 2017.
S De and SL Smith. Batch normalization biases residual blocks towards the identity function in deep
networks. arXiv preprint arXiv:2002.10444, 2020.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, Proceedings of Machine Learning Research, pp. 249-256, Chia Laguna Resort, Sardinia,
Italy, 13-15 May 2010. PMLR.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June
2016.
Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), volume 1, pp. 3, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning (ICML), pp.
448-456, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, pp.
8571-8580, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow ReLU networks. arXiv preprint arXiv:1909.12292, 2019.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
9
Under review as a conference paper at ICLR 2021
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing Systems, pp. 8570-8581,
2019.
Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 510-519, 2019.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation. In
Proceedings of the Third Conference on Machine Translation (WMT), 2018.
Samet Oymak and Mahdi Soltanolkotabi. Overparameterized nonlinear learning: Gradient descent
takes the shortest path? In International Conference on Machine Learning (ICML), 2019.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-2493,
2018.
Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception-
resnet and the impact of residual connections on learning. In Thirty-first AAAI conference on
artificial intelligence, 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Eukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation sys-
tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5987-5995. IEEE, 2017.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture.
arXiv preprint arXiv:2002.04745, 2020.
Ge Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. In Advances
in Neural Information Processing Systems, pp. 7103-7114, 2017.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S Schoenholz. A
mean field theory of batch normalization. In International Conference on Learning Representations,
2018.
Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong
He, Jonas Mueller, R Manmatha, et al. Resnest: Split-attention networks. arXiv preprint
arXiv:2004.08955, 2020.
Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-branch
architectures are less non-convex. arXiv preprint arXiv:1806.01845, 2018.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. In International Conference on Learning Representations (ICLR), 2019a.
Huishuai Zhang, Da Yu, Mingyang Yi, Wei Chen, and Tie-Yan Liu. Stability and convergence theory
for learning resnet: A full characterization. arXiv preprint arXiv:1903.07120, 2019b.
10
Under review as a conference paper at ICLR 2021
Jingfeng Zhang, Bo Han, Laura Wynter, Kian Hsiang Low, and Mohan Kankanhalli. Towards robust
resnet: A small step but a giant leap. In International Joint Conferences on Artificial Intelligence
(IJCAI), 2019c.
Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward
connections for neural machine translation. Transactions of the Association for Computational
Linguistics, 4:371-383, 2016.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
11
Under review as a conference paper at ICLR 2021
A Proofs in Section 3
For feed-forward branches given by equation 2, we adopt the Kaiming’s initialization He et al. (2016):
entries of W a for a ∈ [1 : b - 1] are independently sampled from N (0,m),and entries of W b are
independently sampled from N(0, d).
A. 1 One branch result
Lemma 1. [Lemma 7.1 in Allen-Zhu et al. (2018)] For one branch B(hin) given by equation 2
with Kaiming’s initialization, there exists some small constant such that with probability at least
1 — O(b) ∙ e-Q(m‘2/b) over the initialization randomness of W ∈ (Rm×m)b thefollowing holds
∀a ∈ [b - 1] : kha k ∈ (1 ± )khin k,	(10)
where ha := φ(Wa ha-1), for a = 1, ..., b - 1 with h0 := hin.
We note that khak ∈ (1 ± )khink me-a→ns (1 - )khink ≤ khak ≤ (1 + )khink. The proof of this
lemma is based on the randomness of Wk and the concentration property.
A.2 Proof of Theorem 1
Theorem 1. Suppose the multi-branch network N(∙) is given by Equation 1 and Equation 2 with
Kaimingis initialization. For an input hin, thefollowing holds with probability at least 1 — O(bC) ∙
e-Q(me2/b) over the initialization randomness of W
kN(hin)k ∈
(1 ± X
k∈[C]
α2kkhink,
(3)
where C is the number of branches, and m, b are the width and depth of each branch, respectively.
Proof. In the following derivation, we fix a specific sample i ∈ [n] and the input is hi,in. For notation
simplicity, we ignore the i index. We define an event E := {khbk-1 k ∈ (1 ± )khink, ∀k ∈ [C]}.
By Lemma 1, We have that E holds with probability 1 - O(bC)e-Q(me2/b).On the event E, using
only the randomness of Wbk ∈ Rd×m , hbk = Wbkhbk-1 are independent Gaussian across k and
E[hbk] =0,Var[hbk] = khbk-1k2Id×d for k ∈ [C].
Hence, we have N(hin) = Pk∈[C] Bk (hin) is Gaussian with mean 0 and covariance matrix
Pk∈[C] α2kkhbk-1k2Id×d. By law of large number, we have
kN(hin)k2∈ (1±)2	αk2khink2.	(11)
k
The claim is proved.	□
Proof of Remark 1
The proof of Remark 1 is straightforward. In expectation we always have E[hlk|hlk-1] =
0, E	khlk k2hlk-1	= khlk-1 k2 for k ∈	[C]	and l ∈	[b].	Furthermore as	hbk’s	are independent
across k, thenEkN(hin)k2 = Pk α2kkhink2.
A.3 Proof of Theorem 2
Given the branch equation 2, we further introduce notations to denote intermediate representation:
gak :=Wakhka-1, and hak := φ(gak), fora= 1,..., b.
with h0k := hin and hbk := gbk for all k ∈ [C]. Let Dak be a diagonal matrix representing the activation
state with (Dak)jj = 1, if(gak)j > 0;0, otherwise. Let -→gb := (gb1,gb2, . . . ,gbk) ∈ Rd×b.
12
Under review as a conference paper at ICLR 2021
)∙ exp(一Ω(m)) over the randomness of W, it satisfies
Theorem 2. With probability at least 1 - (nb
for every a ∈ [b] and k ∈ [C], every i ∈ [n],
kWaLi(W)IIF ≤ O(L(W)αk×m)，IIvWLi(W)IlF ≤ o{Li(W) × mdb X αk).	(5)
F	F	k∈[C]
Deep neural networks are often trained with gradient based optimization algorithms. The gradient
with respect to the parameter is computed through back-propagation, e.g., ∂Wι = ∂hι ∙ gT-ι, where
∂∙ represents the gradient of the objective with respect to ∙. Therefore, the gradient upper bound is
guaranteed if gl-1 and ∂hl are bounded across layers and iterations. We next show the backward
process is bounded for each individual sample for the multi-branch network N(∙).
Proof. The gradient is computed as follows via back-propagation. We ignore the sample index i for
notational simplicity.
hout = → α,	dhoUt = αk Id×d,	(12)
∂gbk
∂gbk =
∂hk-i = (Wk)T∂gk, ∂Wk = ∂gk ∙ (hk-i)T	(14)
∂gk-i = DkIdhb-i,…	(15)
∂hk = (Wk)t∂gk, ∂wk = ∂gk ∙ (hk)T.	(16)
dh0ut) ∙ ∂hout = αk∂hout,
∂gbk
(13)
For quadratic loss and sample i ∈ [n], we have that k∂hi,out k2
2Li(W).
We have
∣∣Vw a Li(W)kF = k∂gik,a ∙ hk,a-ikF
=kDk (Wk+ι)τ ∙∙∙ Db-ι(Wb )t ∂hi,outk ∙ khk,a-ιk
≤ O(pm/d) ∙αk JLi(W) ∙ O(1)kxik,	(17)
where the last inequality is due to Theorem 1 and the (Allen-Zhu et al., 2018, Lemma7.4b).
Thus, with high probability 1 一 O(nbC) ∙ exp(-Ω(m)) for all i ∈ [n], a ∈ [b] and k ∈ [C], we have
IlVWkLi(W)kF ≤ O (Li(W) αk X m) under the assumption ∣∣xik = 1.	□
Proof of Remark 2
To establish the expectation value on the backward process, it requires the gradient independence
assumption (Yang, 2019), which can be verified and argued for certain cases. With this assumption,
we can take the expectation on the forward pass and the backward pass independently in (Equation 17).
It is easy to obtain the expectation estimation in Remark 2.
A.4 More discussion on practical wisdom
We argue the choices of scaling down initialization in detail.
Without loss of generality, we first initialize the parameters following the Kaiming’s scheme, the
same as in Section 3. Then we multiply the parameters in multi-branch block by a same scaling down
factor γ. We use the sum aggregation αk = 1 for k ∈ [C].
For forward stability, we need EkN(hin)k2 ≤ O(1), which requires Pk∈[C] (γb)2 ≤ 1. Therefore
we obtain Y ≤ C-*.
For the backward process, we focus on the output update induced by one gradient descent step. That
is N(hin； W + ηVW) -N(hin； W). Suppose the backward error signal on hout is e with ∣∣e∣ = 1.
13
Under review as a conference paper at ICLR 2021
Then following the proof of Theorem 2, we have
EkVWlk kF = Y b-1 ∙ VZm/d ∙ kek ∙ khin k = ω(YbT),	(18)
EkBk(hin； Wlk + ηVWk) - Bk(hin; Wιk)k = Ω(ηγ2(I)),	(19)
where the second equality is due to Taylor expansion and the forward step has a factor Yb-1. Then
the forward output update of branch k is
Bk (hin； Wk + ηVWk) - Bk (hin ； Wk) ≈ X Bk (hin； Wlk + ηVwk) - Bk (hin； Wlk),
l∈[b]
where the "≈" ignores second order perturbation. Hence
Bk(hin； Wk)k ≈ Ω(ηbγ2(b-D). Summing
EkBk(hin； -W→k + ηV-W→k) -
over C branches, we have
EkN(hin； W + ηVw) -N(hin； W)k ≈ Ω(ηbCγ2(I)).	(20)
Hence, to make the expected output update irrelevant with C and b, it requires Y = (bC)-2(b-1)
With γ = (bC)-2(bτ), the forward process will not explode but obtain a diminishing output norm
1	-1
Ω(b-ι (bC)2(b-1)).
B Hessian analysis
Next to obtain a refined understanding on the property of different aggregation schemes, we analyze
the Hessian of a multi-branch network. Specifically for simplicity, we assume that Bk has only one
linear layer: Bk(hin) = Wkhin. We can compute the Hessian of the objective with respect to the
input hin and the learnable parameter -W→,
Hhin = (X αkWk)T(X akWk), HW = (αατ)和 Id×d 和(EhinhT) ,	(21)
k
k
whereH-W→ := HVec(-W→) and Vec(-W→) := (Vec(W1)T,Vec(W2)T, . . .,Vec(Wk)T)T is the long
vector stacking W, 0 is the Kronecker product, and E is average over the training samples.
Fact 1. For Bk(hin) = Wkhin and α = α1C, we have the spectral norm of Hessian,
EkHhink=Ω(α2C), and ∣∣H→∣∣ = α2C∣∣EhinhTn∣∣.
(22)
where the first expectation is over the randomness at initialization and the second expectation is over
the training samples.
Proof. The Hessians are written as
H hin = (X ɑkW k )T (X ɑk W k ) ɑk=α ɑ2
XWk!TXWk!
k
k
HW = (αατ)乳 Id×d 乳(EhinhTn) α”α α2 (1c×c)乳(Ib×b 氧(EhinhTn)),
where 0 represents the Kronecker product and HW has dimension Cbp X Cbp.
Because entries of Wk are initialized with Gaussian distribution N(0, 1/d), Pk Wk follows Gaus-
sian distribution N(0, C/d). The singular value of Gaussian matrix is given by the following
Bai-Yin’s law (BAI & YIN, 1993).
Lemma 2. Let A ∈ RN ×n, and entries of A are independent standard Gaussian random variables.
Suppose that dimensions N and n grow to infinity while the aspect ratio n/N converges to a constant
in [0, 1], one has
SmaX(A) = VN + √n + o(√n) almost surely.
where smax(A) are the largest singular value of A.
(23)
14
Under review as a conference paper at ICLR 2021
Hence EkHhink = α2C(1 +，m/d)2 = Ω(ɑ2C) if viewing m/d some fixed hyper-parameter.
For the spectral norm of Hw，we use the property of Kronecker product λmaχ(A 0 B)= λmaχ(A) ∙
λmaχ(B), where λmaχ(∙) is the largest eigenvalue of the positively semi-definite matrix ∙. Hence
∣∣Hwk = α2λmaχ(1c×c) ∙ 1 ∙ λmaχ(EhinhTn) = α2CkEhinhTTnk,	(24)
where the second equality is due to the fact λmaχ(1c×c) = C and λmaχ(Ic×c) = 1.	□
The spectral norm of the Hessian corresponds to the smoothness or the Lipschitz coefficient of the
gradient. It determines the largest allowable learning rate for gradient descent algorithm. For the
sum aggregation, the spectral norm ∣∣H自/| = Ω(C) and ∣∣HWk = C∣∣EhinhTnk. Hence they
both are C times larger than those in the case with only one branch. This indicates the learning
rate for the gradient descent needs properly scaling down with the number of branches to guarantee
convergence. For the average aggregation, at initialization, the spectral norm ∣∣Hhin ∣ = Ω(1∕C) and
∣∣HWk = C ∣EhinhTn∣. They both are shrunk by 1/C compared to the case with only one branch.
This indicates the learning rate or the gradient needs properly scaling up with the number of branches.
Thus one cannot apply the same training strategy to train network with varying number of branches
and the sum or average aggregation.
In contrast, for the STAM aggregation, the spectral norm ∣∣H hin ∣ = Ω(1) and ∣∣H Wk = ∣∣EhinhTnk.
They both remain the same when the number of branches varies. This indicates that we can apply the
same training strategy for varying number of branches.
C More discussion on Transformer model
C.1 Forward process
Proof of Proposition 1
Without loss of generality, we assume that there are only one head and ignore the head subscript
for simplicity. For a specific symbol representation x, the query is q = xTQ, the keys are kι =
TT	TT
x1TK, ..., kn = xTnK (i.e., K = XK ), the values are v1 = x1TV , ..., vn = xTn V (i.e., V = XV .
We note that q, kj, Vj are row vectors. Let (pi,…,Pn) = softmax(√√^, √√d^,…,√n).
We note that for symbol x, h = (p1, . . . ,pn)V and then we have an upper bound on the head
representation ∣h∣ = ∣(pi, . . . ,pn)XV ∣ ≤ maxi∈[n] ∣xiV ∣.
Suppose k Xi k = √d for all i ∈ [n], which is reasonable assumption because of the layer normaliza-
tion. Then ∣∣XiV∣∣2 is a Chi-square XdO variable. Next we estimate the maximum of n independent
Chi-square variables by using the following asymptotic relation. For a random variable U 〜XV and
large ν, we have √2U 一 √2ν — 1 〜N(0,1). We use the estimation of extreme value of Gaussian
variables. For Z = maxi∈[n] Vi and Vi 〜N(0,1), we have E(Z) ≤ √2 log n. Based on the
above estimation, we have asymptotically maxi∈[n] ∣XiV∣∣2 . dv 一 1 + logn + ,2(2dv - 1)logn.
We can also estimate the maximum of a sequence of X2 variables by using the relation that the
extreme value of χ2 distribution (a case of Gamma distribution) asymptotically converges to Gumbel
distribution and get a similar result. Hence, we have ∣∣h∣2 . dv + (logn 一 1 + ,2(2dv - 1) logn).
C.2 Backward process
We focus on the case with one head and omit the subscript c. We assume dv = dk = dq =: dh .
We rewrite the forward process as follows. Given the parameters Q ∈ Rd×dh, K ∈ Rd×dh, V ∈
15
Under review as a conference paper at ICLR 2021
Rd×dh , O ∈ Rdh ×d.
X = (x1,  , xn)T, with each row being a symbol embedding, ∈ Rn×d	(25)
V = XV, ∈ Rn×dh	(26)
Q = XQ, ∈ Rn×dh	(27)
K = XK, ∈ Rn×dh	(28)
P = QK T / p∕dh	(29)
P = softmax(P ), with each row being a probability simplex, ∈ Rn×n	(30)
H = P V, with each row being the head representation, ∈ Rn×dh	(31)
O = HO , with each row being the new representation of a symbol, ∈ Rn×d	(32)
For the backward process, it can be written as follows. Suppose the error signals on the output O is
E = (e1, ..., en)T, with each row being an error signal on a symbol representation.
∂O = E, ∈ Rn×d	(33)
T
∂H = ∂O ∙ O , ∂O = HT ∙ ∂O,	(34)
∂P = ∂H ∙ VT, ∂V = PT ∙ ∂H,	(35)
T
∂v X = ∂V ∙ V , ∂V = XT ∙ ∂V,	(36)
∂ P = ∂PJ T,	(37)
∂Q = ∂P ∙ K/∕dh, ∂K = ∂PTQ∕∕dh,	(38)
T
∂qX = ∂Q ∙ Q , ∂Q = XT ∙ ∂Q,	(39)
T
∂k X = ∂K ∙ K , ∂K = XT ∙ ∂K,	(40)
∂X = ∂VX + ∂QX + ∂KX	(41)
We can further compute the expected norm of the gradient as in the proof of Proposition 1 under the
gradient independence assumption (Yang, 2019).
C.3 More about Transformer
Apart from the multi-head attention block, the fully-connected (FC) block is also a multi-branch
architecture with sum aggregation. We use dfc to denote the intermediate dimension of fully con-
nected block. The fully connected block consists of two FC layers with weights W1fc ∈ Rd×dfc and
W2fc ∈ Rdfc×d. In practice, dfc is several times larger than d. For example, the “big” Transformer
in Vaswani et al. (2017) uses dfc = 4d. These sum aggregation operations restrict practitioners to
explore Transformer models with large h and dfc .
D More details about experiments
The training procedure of ResNeXt model in Section 5 is the same as Xie et al. (2017). Specifically,
we train the models on 8 GPUs with batch size 128, total number of epochs 300 and weight decay
0.0005. We use Nesterov momentum with coefficient 0.9. The learning rate is multiplied by 0.1 at
the 150-th and 225-th epoch. For data augmentation, we take a random crop with size 32x32 from a
zero-padded 40x40 image or its horizontal flipping.
Now we introduce more details of Transformers on translation tasks in Section 5. The IWSLT14
De-En dataset is collected from Fairseq official site1, which contains 156K/7K/7K sentence pairs for
training/validation/test, respectively. For WMT English-German task, we use the same data setup as
Ott et al. (2018). Specifically, we train the model using the training data of WMT16 , which contains
4.5M sentence pairs. The validation and test sets are newstest13 and newstest14, respectively.
1https://github.com/pytorch/fairseq/blob/v0.9.0/examples/translation/README.md
16
Under review as a conference paper at ICLR 2021
We use head-level dropout which drop heads representation (before the output projection of attention
block). The head-level dropout probability is p. We setp = 0.3 for IWSLT14 De-En task andp = 0.1
for WMT16 En-De task, respectively. Our source code is available at anonymous GitHub page2.
2https://github.com/AnonymousAKES/STAM-aggregation
17