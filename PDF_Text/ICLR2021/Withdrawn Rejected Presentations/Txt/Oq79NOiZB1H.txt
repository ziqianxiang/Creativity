Under review as a conference paper at ICLR 2021
On the Importance of Sampling in Training
GCNs: Convergence Analysis and Variance
Reduction
Anonymous authors
Paper under double-blind review
Ab stract
Graph Convolutional Networks (GCNs) have achieved impressive empirical ad-
vancement across a wide variety of graph-related applications. Despite their great
success, training GCNs on large graphs suffers from computational and memory
issues. A potential path to circumvent these obstacles is sampling-based methods,
where at each layer a subset of nodes is sampled. Although recent studies have em-
pirically demonstrated the effectiveness of sampling-based methods, these works
lack theoretical convergence guarantees under realistic settings and cannot fully
leverage the information of evolving parameters during optimization. In this pa-
per, we describe and analyze a general doubly variance reduction schema that
can accelerate any sampling method under the memory budget. The motivating
impetus for the proposed schema is a careful analysis for the variance of sampling
methods where itis shown that the induced variance can be decomposed into node
embedding approximation variance (zeroth-order variance) during forward prop-
agation and layerwise-gradient variance (first-order variance) during backward
propagation. We theoretically analyze the convergence of the proposed schema
and show that it enjoys an O(1/T) convergence rate. We complement our theo-
retical results by integrating the proposed schema in different sampling methods
and applying them to different large real-world graphs.
1	Introduction
In the past few years, graph convolutional networks (GCNs) have achieved great success in many
graph-related applications, such as semi-supervised node classification (Kipf & Welling, 2016), su-
pervised graph classification (Xu et al., 2018), protein interface prediction (Fout et al., 2017), and
knowledge graph (Schlichtkrull et al., 2018; Wang et al., 2017). However, most works on GCNs fo-
cus on relatively small graphs, and scaling GCNs for large-scale graphs is not straight forward. Due
to the dependency of the nodes in the graph, we need to consider a large receptive-field to calculate
the representation of each node in the mini-batch, while the receptive field grows exponentially with
respect to the number of layers. To alleviate this issue, sampling-based methods, such as node-wise
sampling (Hamilton et al., 2017; Ying et al., 2018; Chen et al., 2017), layer-wise sampling (Chen
et al., 2018; Zou et al., 2019), and subgraph sampling (Chiang et al., 2019; Zeng et al., 2019) are
proposed for mini-batch GCN training.
Although empirical results show that sampling-based methods can scale GCN training to large
graphs, these methods suffer from a few key issues. First, the theoretical understanding of sampling-
based methods is still lacking. Second, the aforementioned sampling strategies are only based on the
structure of the graph. Although most recent works (Huang et al., 2018; Cong et al., 2020) propose
to utilize adaptive importance sampling strategies to constantly re-evaluate the relative importance
of nodes during training (e.g., current gradient or representation of nodes), finding the optimal adap-
tive sampling distribution is computationally inadmissible, as it requires to calculate the full gradient
or node representations in each iteration. This necessitates developing alternative solutions that can
efficiently be computed and that come with theoretical guarantees.
In this paper, we develop a novel variance reduction schema that can be applied to any sampling
strategy to significantly reduce the induced variance. The key idea is to use the historical node
1
Under review as a conference paper at ICLR 2021
864
..
11
SSoL gniniar
1.8
1.6
1.4
-----LADIES ------------- LADIES (FirSt-order VR) ---------- LADIES (Zeroth-order VR) ------------ LADIES (DoublyVR)
04
0	50	100	150	0	50	100	150	.	0	50	100	150
Epoch	Epoch	Epoch
Figure 1: The effect of doubly variance reduction on training loss, validation loss, and mean-square
error (MSE) of gradient on Flickr dataset using LADIES proposed in Zou et al. (2019).
embeddings and the historical layerwise gradient of each graph convolution layer as control vari-
ants. The main motivation behind the proposed schema stems from our theoretical analysis of the
sampling methods’ variance in training GCNs. Specifically, we show that due to the composite struc-
ture of training objective, any sampling strategy introduces two types of variance in estimating the
stochastic gradients: node embedding approximation variance (zeroth-order variance) which results
from embeddings approximation during forward propagation, and layerwise-gradient variance (first-
order variance) which results from gradient estimation during backward propagation. In Figure 1,
we exhibit the performance of proposed schema when utilized in the sampling strategy introduced
in (Zou et al., 2019). The plots show that applying our proposal can lead to a significant reduction
in variance; hence faster convergence rate and better test accuracy. We can also see that both zeroth-
order and first-order methods are equally important and demonstrate significant improvement when
applied jointly (i.e, doubly variance reduction).
Contributions. We summarize the contributions of this paper as follows:
•	We provide the theoretical analysis for sampling-based GCN training (SGCN) with a non-
asymptotic convergence rate. We show that due to the node embedding approximation
variance, SGCNs suffer from residual error that hinders their convergence.
•	We mathematically show that the aforementioned residual error can be resolved by em-
ploying zeroth-order variance reduction to node embedding approximation (dubbed as
SGCN+), which explains why VRGCN (Chen et al., 2017) enjoys a better convergence than
GraphSAGE (Hamilton et al., 2017), even with less sampled neighbors.
•	We extend the algorithm from node embedding approximation to stochastic gradient
approximation, and propose a generic and efficient doubly variance reduction schema
(SGCN++). SGCN++ can be integrated with different sampling-based methods to signif-
icantly reduce both zeroth- and first-order variance, and resulting in a faster convergence
rate and better generalization.
•	We theoretically analyze the convergence of SGCN++ and obtain an O(1/T) rate, which
significantly improves the best known bound O(1∕√T). We empirically verify SGCN+ +
through various experiments on several real-world datasets and different sampling methods,
where it demonstrates significant improvements over the original sampling methods.
2	Related works
Training GCNs via sampling. The full-batch training of a typical GCN is employed in Kipf &
Welling (2016) which necessities keeping the whole graph data and intermediate nodes’ represen-
tations in the memory. This is the key bottleneck that hinders the scalability of full-batch GCN
training. To overcome this issues, sampling-based GCN training methods (Hamilton et al., 2017;
Chen et al., 2017; Chiang et al., 2019; Chen et al., 2018; Huang et al., 2018) are proposed to train
GCNs based on mini-batch of nodes, and only aggregate the embeddings of a sampled subset of
neighbors of nodes in the mini-batch. For example, GraphSAGE (Hamilton et al., 2017) restricts
the computation complexity by uniformly sampling a fixed number of neighbors from the previous
layer nodes. However, a significant computational overhead is introduced when GCN goes deep.
VRGCN (Chen et al., 2017) further reduces the neighborhood size and uses history activation of the
previous layer to reduce variance. However, they require to perform a full-batch graph convolu-
tional operation on history activation during each forward propagation, which is computationally
expensive. Another direction applies layerwise importance sampling to reduce variance. For exam-
ple, FastGCN (Chen et al., 2018) independently sample a constant number of nodes in all layers
using importance sampling. However, the sampled nodes are too sparse to achieve high accuracy.
2
Under review as a conference paper at ICLR 2021
LADIES (Zou et al., 2019) further restrict the candidate nodes in the union of the neighborhoods of
the sampled nodes in the upper layer. However, significant overhead may be incurred due to the ex-
pensive sampling algorithm. In addition, subgraph sampling methods such as GraphSAINT (Zeng
et al., 2019) constructs mini-batches by importance sampling, and apply normalization techniques to
eliminate bias and reduce variance. However, the sampled subgraphs are usually sparse and requires
a large sampling size to guarantee the performance.
Theoretical analysis. Despite many algorithmic progresses over the years, the theoretical under-
standing of the convergence for SGCNs training method is still limited. VRGCN provides a conver-
gence analysis under a strong assumption that the stochastic gradient due to sampling is unbiased
and achieved a convergence rate of O(1∕√T). However, the convergence analysis is limited to
VRGCN, and the assumption is not true due to the composite structure of training objective as will
be elaborated. Chen & Luss (2018) provides another convergence analysis for FastGCN under a
strong assumption that the stochastic gradient of GCN converges to the consistent gradient exponen-
tially fast with respect to the sample size, and results in the same convergence rate as unbiased ones,
i.e., O(1∕√T). Most recently, Sato et al. (2020) provides PAC learning-style bounds on the node
embedding and gradient estimation for SGCNs training. Another direction of theoretical research
focuses on analyzing the expressive power of GCN (Garg et al., 2020; Chen et al., 2019; Zhang
et al., 2020), which is not the focus of this paper and omitted for brevity.
Connection to composite optimization. The proposed doubly variance reduction algorithm shares
the same spirit with the variance reduced composite optimization problem considered in Zhang &
Xiao (2019a); Hu et al. (2020); Tran-Dinh et al. (2020); Zhang & Xiao (2019c;b), but with two
main differences. Firstly, the objective function is different. In composite optimization, an objective
function that only the first composite layer has the trainable parameters is considered, where the
output of the lower-level function is acting as the parameter for the higher-level function. However,
in GCN model, the output of one graph convolutional layer is the input node embedding matrix for
the next layer. As a result, when analyzing the convergence of variance reduced neural network
model, we have to explicitly handle both the evolving node embedding matrices and the trainable
parameters at all layers. Secondly, the data points in these works are sampled independently, but the
data points (nodes) in SGCN are sampled node- or layer-dependent according to the graph structure.
In our analysis, we provide a sampled-graph structure dependent convergence rate by bridging the
connection between the convergence rate of GCN to the graph Laplacian matrices.
3	SGCN: A tighit analysis of SGD for GCN training
Full-batch GCN training. We begin by introducing the basic mathematical formulation of training
GCNs. In this paper, we consider training GCNs in semi-supervised multi-class classification set-
ting. Given an undirected graph G = (V, E) with N = |V| and |E| edges and the adjacency matrix
A ∈ {0, 1}N×N, we assume that each node is associated with a feature vector xi ∈ Rd and label
yi. We use X = [x1, . . . , xN] ∈ RN×d and y = [y1, . . . , yN] ∈ RN to denote the node feature
matrix and label vector, respectively. The Laplacian matrix is calculated as L = D-1/2AD-1/2 or
L = D-1A where D ∈ RN×N is the degree matrix. We use θ = {W(1), . . . , W(L)} to denote
the stacked weight parameters of a L-layer GCN. The training of full-batch GCN (FullGCN) as an
empirical risk minimization problem aims at minimizing the loss L(θ) over all training data
L(θ) = NN PLLOSS(h(L),yi), H(L)= σ(L ...σ^Lσ(LXW} )W⑵)... W(L))
Z⑴
where h(') is the ith row of node embedding matrix H(') = σ(Z(')), Z(') = LH('-1)W(') that
corresponds to embedding of ith node at 'th layer (hop), Loss(∙, ∙) is the loss function (e.g., cross-
entropy loss) to measure the discrepancy between the prediction of the GCN and its ground truth
label, and σ(∙) is the activation function (e.g., ReLU function).
Sampling-based GCN training. When the graph is large, the computational complexity of forward
and backward propagation could be very high. One practical solution to alleviate this issue is to
sample a subset of nodes and construct a sparser normalized Laplacian matrix Le (`) for each layer
with supp(L⑶)《 supp(L), and perform forward and backward propagation only based on the
sampled Laplacian matrices. The sparse Laplacian matrix construction algorithms can be roughly
3
Under review as a conference paper at ICLR 2021
classified as nodewise sampling, layerwise sampling, and subgraph sampling. A detailed discussion
of different sampling strategies can be found in Appendix D. To apply Stochastic Gradient Descent
(SGD) (Bottou et al., 2018) to train GCN (SGCN), we sample a mini-batch of nodes VB ⊆ V from
all nodes with size B = |Vb|, and construct the set of sparser Laplacian matrices {L(')}L==ι based
on nodes sampled at each layer and compute the stochastic gradient to update parameters as 1
VLe(θ) = B Pi∈Vβ VLoss (h (L),yi), HI(L) = σ(L (L) ...σ(L ⑵ σ(g (I)XW(I), )W⑵)... W(L))
e⑴
Key challenges. Compared to vanilla SGD, the key challenge of theoretical understanding for
SGCN training is the biasedness of stochastic gradient due to sampling of nodes at inner layers. Let
denote FullGCN's full-batch gradient as VL(θ) = {G(') = ∂WL(θ) }L=ι and SGCN's stochastic
e
gradient as VLe(θ) = {G(') = aWc') }L=ι. By the chain rule, We can compute the full-batch
gradient G(') w.r.t. the 'th layer weight matrix W(') as
Gt') = [LHt'τ)]>(D('+1) ◦ σ0(zf)), D；')= L>(Df+1) ◦ σ0(zf)) W,，DtL+1) = IH⅛ ⑴
and compute stochastic gradient G(') utilized in SGCN for the 'th layer w.r.t. W(E) as
Gt') = [L⑷H('T)]>(D('+1)。σ0(Z('))), Df = [L⑷]>(Dt'+1)◦ σ0(z(')))W('), D尸1)= ⅛ (2)
(`)	(`)
For any layer ' ∈ [L], the stochastic gradient Gt is a biased estimator of full-batch gradient Gt ,
as it is computed from H(tL) and z(t`), which are not available in SGCN since He t(L) and ze t(`) are
used as an approximation during training. Recently, Chen et al. (2017) established a convergence
rate under the strong assumption that the stochastic gradient of SGCN is unbiased and Chen & Luss
(2018) provided another analysis under the strong assumption that the stochastic gradient converges
to the consistent gradient exponentially fast as the number of sampled nodes increases. While both
studies establish the same convergence rate of O(1∕√T), however, these assumptions do not hold
in reality due to the composite structure of the training objectives and sampling of nodes at inner
layers. Motivated by this, we aim at providing a tight analysis without the aforementioned strong
assumptions on the stochastic gradient. Our analysis is inspired by the bias and variance decompo-
sition of the mean-square error of stochastic gradient, which has been previously used in Cong et al.
(2020) to analysis the stochastic gradient in GCN. Formally, we can decompose mean-square error
of stochastic gradient as
E[kVLe(θ) - VL(θ)k2F] = E[kE[VLe(θ)] - VL(θ)k2F] + E[kVLe(θ) - E[VLe(θ)]k2F]	(3)
、	一V-	J 、	' -V--	J
Bias E[kbk2F]	Variance E[knk2F]
where the bias terms E[kbk2F] is mainly due to the node embedding approximation variance (zeroth-
order variance) during forward propagation and the variance term E[knk2F] is mainly due to the
layerwise gradient variance (first-order variance) during backward propagation. Before proceeding
to analysis, we make the following standard assumptions on the Lipschitz-continuity and smoothness
of the loss function Loss(∙, ∙) and activation function σ(∙).
Assumption 1. The lossfunction Loss(∙, ∙) is C卜SS-LiPschitz continuous and Lloss-smoothness w.r.t.
to the input node embedding vector, i.e., kLoss(h(L), y) - Loss(h0(L), y)k2 ≤ Closskh(L) - h0(L) k2
and kVLoss(h(L), y) - VLoss(h0(L), y)k2 ≤ Llosskh(L) - h0(L)k2.
Assumption 2. The activation function σ(∙) is Cσ-Lipschitz continuous and Lσ-smoothness, i.e.,
kσ(z⑶)一σ(z0('))k2 ≤ Cσkz(') - z0(')k2 and ∣∣σ0(z⑶)一σ0(z0('))∣∣2 ≤ Lσ∣∣z⑶-z0(')∣∣2.
We also make the following assumptions on the norm of weight matrices, Laplacian matrices, and
node feature matrix, which are used in the generalization analysis of GNNs Garg et al. (2020).
Assumption 3. For any ' ∈ [L], the norm of weight matrices, Laplacian matrices, input node
feature matrix are bounded: kW(') ∣∣f ≤ BW, kL⑶∣f ≤ BLA, ∣L∣f ≤ BLA ,and ∣X∣f ≤ BH.
1We use a tilde symbol e for their stochastic form
4
Under review as a conference paper at ICLR 2021
Proposition 1. For any ` ∈ [L], there exist constants BH and BD such that the norm of node
embedding matrices and the gradient with respect to the input node embedding matrices satisfy
kH(')kF ≤ BH，kH(')kF ≤ BH，k dσ(LH'-11w(')) kF ≤ Bd, and k 的叫、(':：，')) Hf ≤ Bd.
Before presenting the convergence of SGCN, we introduce the notation of propagation matrices
{P"ι, which are defined as the column-wise expectation of the sparser Laplacian matrices.
Note that this notation is only for presenting the theoretical results, and are not used in the practical
training algorithms. By doing so, we can decompose the difference between Le (`) and L as the
summation of column-wise difference ∣∣L(') 一 P(')kF and row-wise difference ∣∣P(') 一 L∣∣F.
In the following theorem, we show that the upper bound of the bias and variance of stochastic
gradient is closely related to the expectation of column-wise difference E[∣L (') 一 P(') ∣∣F] and row-
wise difference E[∣P(') 一 L∣F] which can significantly impact the convergence of SGCN.
Theorem 1 (Convergence of SGCN). Suppose Assumptions 1, 2, 3 hold and apply SGCN with learn-
ing rate chosen as η = min{1∕Lp, 1∕√T} where LF is the smoothness constant. Let ∆n and ∆b
denote the upper bound on the variance and bias of stochastic gradients as:
LL
∆n = X O(E[∣L(') - P(')kF]) + O(E[∣P') - LkF]),	∆b = X O(E[∣P') - LkF])	(4)
'=1	'=1
Then, the output of SGCN satisfies
min E[∣VL(θt)kF] ≤ 2(L(θ1) ~L(θ?)) + L√∆n + ∆b.	(5)
t∈[T]	T	T
The exact value of key parameters LF, ∆n, and ∆b are computed in Lemma 1, Lemma 2, and
Lemma 3 respectively and can be found in Appendix G. Theorem 1 implies that after T iterations
the gradient norm of SGCN is at most O(∆n∕√T) + ∆b, which suffers from a constant residual
error ∆b that is not decreasing as the number of iterations T increases. Without the bias2 we recover
the convergence of vanilla SGD. Of course, this type of convergence is only useful if ∆b and ∆n
are small enough. We note that existing SGCN algorithms propose to reduce ∆b by increasing the
number of neighbors sampled at each layer (e.g., GraphSAGE), or applying importance sampling
(e.g., FastGCN, LADIES and GraphSAINT).
4 SGCN+: Zeroth-order Variance Reduction
An important question to answer is: can we eliminate the residual error without using all neigh-
bors during forward-propagation? A remarkable attempt to answer this question has been recently
made in VRGCN (Chen et al., 2017) where they propose to use historical node embeddings as an
approximation to estimate the true node embeddings. More specifically, the graph convolution in
VRGCN is defined as H,) = σ(LH(—7LI)W(E) + L(')(je('-1) 一 H('^l1))W(')). Taking advantage
of historical node embeddings, VRGCN requires less sampled neighbors and results in significant less
computation overhead during gradient computation. Although VRGCN achieves significant speed up
and better performance compared to other SGCNs, it involves using the full Laplacian matrix at each
iteration, which can be computationally prohibitive. Moreover, since both SGCNs and VRGCN are
approximating the exact node embeddings calculated using all neighbors, it is still not clear why
VRGCN achieves a better convergence result than SGCNs using historical node embeddings.
To fill in these gaps, we introduce zeroth-order variance reduced sampling-based GCN training
method dubbed as SGCN+. As shown in Algorithm 1, SGCN+ has two types of forward propagation:
the forward propagation at the snapshot steps and the forward propagation at the regular steps. At
the snapshot step (t mod K = 0), a full Laplacian matrix is utilized:
Zl = LH('T)W化	H(') = σ(Z(')),	Z(') 一 Z(')	⑹
During the regular steps (t mod K 6= 0), the sampled Laplacian matrix is utilized:
Z(') = Z巴 + L(')Ht'τ)w(') - L(')Htt11)w(')1, H(') = σ(Z⑶)	(J)
2We have ∆b = 0 if all neighbor are used to calculate the exact node embeddings, i.e., P(') = L, ∀' ∈ [L].
5
Under review as a conference paper at ICLR 2021
Algorithm 1 SGCN+: Zeroth-order variance reduction (Detailed version in Algorithm 4)
1:	Input: Learning rate η > 0, snapshot gap K > 0
2:	for t = 1, . . . , T do
3:	if t mod K = 0 then
4:	Calculate node embeddings using Eq. 6
5:	Calculate full-batch gradient VL(θt) as Eq. 1 and update as θt+ι = θt — ηVL(θt)
6:	else
7:	Calculate node embeddings using Eq. 7
8:	Calculate stochastic gradient VL(θt) as Eq. 2 and update as θt+1 = θt — ηVL(θt)
9:	end if
10:	end for
11:	Output: Model with parameter θT +1
Algorithm 2 SGCN++: Doubly variance reduction (Detailed version in Algorithm 5)
1:	Input: Learning rate η > 0, snapshot gap K > 0
2:	for t = 1, . . . , T do
3:	if t mod K = 0 then
4:	Calculate node embeddings using Eq. 6
5:	Calculate full-batch gradient VL(θt) usng Eq. 1 and update as θt+1 = θt — ηVL(θt)
6:	Save the per layerwise gradient Gt') - G('), D(') - D('), ∀' ∈ [L]
7:	else
8:	Calculate node embeddings using Eq. 7
9:	Calculate stochastic gradient VL(θt) usng Eq. 10 and update as θt+1 = θt — ηVL(θt)
10:	end if
11:	end for
12:	Output: Model with parameter θT +1
Comparing with VRGCN, the proposed SGCN+ only requires one full Laplacian graph convolution
operation every K iterations, where K > 0 is an additional parameter to be tuned.
In the following theorem, we introduce the convergence result of SGCN+. Recall that the node em-
bedding approximation variance (zeroth-order variance) determines the bias of stochastic gradient
E[kbk2F]. Applying SGCN+ can significantly reduce the bias of stochastic gradient, such that its
value is small enough that will not deteriorate the convergence.
Theorem 2 (Convergence of SGCN+). Suppose Assumptions 1, 2, 3 hold and apply SGCN+ with
learning rate chosen as η = min{1∕Lp, 1∕√T} where LF is the smoothness constant. Let ∆n and
∆b+ denote the upper bound for the variance and bias of stochastic gradient as:
L
∆n = X O(E[kL(D- P(')kF]) + O(E[kP(') - LkF]),
'=1	L	(8)
△+ = η2∆+0 where ∆+0 = O(KX ∣E[∣∣P叫F] -∣∣L∣∣F∣)
'=1
Then, the output of SGCN+ satisfies
minE[NL(θt)kF] ≤ 2(Wθ*)) + L√∆n + δT0.	(9)
t∈[T]	T	T T
The exact value of key parameters LF, ∆n, and ∆b+ are computed in Lemma 1, Lemma 2, and
Lemma 5 respectively, and can be found in Appendix G, H. Theorem 2 implies that after T iterations
the gradient norm of SGCN+ is at most O(∆n∕√T) + O(∆+0∕T). When using all neighbors for
calculating the exact node embeddings, We have P(') = L such that ∆40 = 0, which leads to con-
vergence rate of SGD. Comparing with vallina SGCN, the bias of SGCN+ is scaled by learning rate
η. Therefore, we can reduce the negative effect of bias by choose learning rate as η = O(1∕√T).
This also explains why SGCN+ achieves a significantly better convergence rate compared to SGCN.
6
Under review as a conference paper at ICLR 2021
5 SGCN++: Doubly Variance Reduction
Algorithm 1 applies zeroth-order variance reduction on node embedding matrices and results in a
faster convergence. However, both SGCN and SGCN+ suffer from the same stochastic gradient vari-
ance ∆n, which can be only reduced either by increasing the mini-batch size of SGCN or applying
variance reduction on stochastic gradient. An interesting question that arises is: can we further
accelerate the convergence by simultaneously employing zeroth-order variance reduction on node
embeddings and first-order variance reduction on layerwise gradient? To answer this question, we
propose doubly variance reduction algorithm SGCN++, that extends the variance reduction algo-
rithm from node embedding approximation to layerwise gradient estimation.
As shown in Algorithm 2, the main idea of SGCN++ is to use the historical gradient as control
variants for current layerwise gradient estimation. More specifically, similar to SGCN+ that has
two types of forward propagation steps, SGCN++ also has two types of backward propagation:
at the snapshot steps and at the regular steps. The snapshot steps (t mod K = 0) backward
propagation are full-batch gradient computation as is defined in Eq. 1, and the computed full-batch
gradient are saved as control variants for the following regular steps. The backward propagation (t
mod K 6= 0) at the regular steps are defined as
G(') = G(')ι + [L(')Ht'T)]>(D('+1) ◦ σ0(Zt)) - [e(')H匕1)]>回”)◦ σ0(Z-))
Dt') = Dt-1 + [L(')]> (Dt'+1) ◦ σ0(Zt)) [W(')] - [e⑹]>(D(")◦ σ0(et-i)) [W(')ι]
(10)
Next, in the following theorem, we establish the convergence rate of SGCN++. Recall that the
mean-square error of the stochastic gradient can be decomposed into bias E[kbk2F] that is due to
node embedding approximation and variance E[knk2F] that is due to layerwise gradient estimation.
Applying doubly variance reduction on node embedding and layerwise gradient simultaneously can
significantly reduce mean-square error of stochastic gradient and speed up convergence.
Theorem 3 (Convergence of SGCN++). Suppose Assumptions 1, 2, 3 hold, and denote LF as the
smoothness constant and ∆n+++b as the upper-bound of mean-square error of stochastic gradient
∆++b = η2∆++b = η2o(κ PL=1 ∣E[kL ⑹ kF] -[旧图)	(ii)
Apply SGCN++ in Algorithm 2 with learning rate as η
-------/ 2 一 . Then it holds that
LF + L2F +4∆n+++b0
T	ι	___________
T X E[kVL(θt)k2] ≤ T (LF + √LF +4∆++b) (L(θ1) -L(θ?)).
(12)
t=1
The exact value of key parameter LF and ∆n+++b0 are computed in Lemma 1 and Lemma 12 re-
spectively, and can be found in Appendix G, I. Theorem 2 implies that applying doubly variance
reduction can scale the mean-square error O(η2K) times smaller. As a result, after T iterations the
norm of gradient of solution obtained by SGCN++ is at most O(∆n+++b0 /T), which enjoys the same
rate as vanilla variance reduced SGD (Reddi et al., 2016; Fang et al., 2018).
Scalability of SGCN++. One might doubt whether the computation at the snapshot step with full-
batch gradient will hinder the scalability of SGCN++ for extremely large graphs? Heuristically,
we can approximate the full-batch gradient with the gradient calculated on a large-batch using all
neighbors. The intuition stems from matrix Bernstein inequality (Gross, 2011), where the probabil-
ity of the approximation error violating the desired accuracy decreases exponentially as the number
of samples increase. Please refer to Algorithm 6 for the full-batch free SGCN++ and explanation
on why large-batch approximation is feasible using tools from matrix concentration. Moreover, we
provide the empirical evaluation on the large-batch size instead of full-batch in Appendix C. We
remark that large-batch approximation can also be utilized in SGCN+ to further reduce the memory
requirement for historical node embeddings.
Connection to composite optimization. Although we formulate sampling-based GCNs as a spe-
cial case of the composite optimization problem, it is worth noting that compared to the classical
composite optimization, there are a few key differences that make the utilization of variance reduc-
tion methods for composite optimization non-trivial: (a) different objective function that makes the
GCN analysis challenging; (b) different gradient computation, analysis, and algorithm which make
7
Under review as a conference paper at ICLR 2021
Table 1: Comparison of the accuracy (F1-score) of SGCN, SGCN+, and SGCN++.
Method / VR		PPI	PPI-Large	Flickr	Reddit	Yelp
Exact	SGCN	77.61	78.10	52.30	95.07	59.99
	SGCN++ (Doubly)	82.18	88.98	52.88	95.17	62.09
VRGCN	SGCN+ (Zeroth)	77.65	77.82	52.57	95.17	61.29
	SGCN++ (Doubly)	82.50	88.65	52.53	95.17	62.64
	SGCN	71.40	69.51	51.13	94.73	59.58
GraphSAGE	SGCN+ (Zeroth)	72.16	69.67	51.13	94.89	58.62
	SGCN++ (Doubly)	79.63	85.41	52.65	95.18	61.75
	SGCN	63.51	59.60	50.74	87.36	55.75
FastGCN	SGCN+ (Zeroth)	72.32	72.30	51.07	94.54	56.86
	SGCN++ (Doubly)	81.92	85.04	52.57	94.99	60.63
	SGCN	62.46	60.74	50.29	94.11	59.84
LADIES	SGCN+ (Zeroth)	72.16	74.03	51.83	94.39	57.06
	SGCN++ (Doubly)	81.58	84.18	52.09	95.05	60.96
	SGCN	61.51	38.68	50.10	93.68	54.65
GraphSAINT	SGCN+ (Zeroth)	66.94	41.40	50.66	84.61	55.42
	SGCN++ (Doubly)	79.63	79.71	50.94	94.18	57.03
FULLGCN	N/A	82.14	90.62	52.99	95.15	62.77
directly applying multi-level variance reduction methods such as SPIDER (Zhang & Xiao, 2019b)
nontrivial; (c) different theoretical results and novel intuition for sampling-based GCN training. Due
to the space limit, we defer the detail discussion to the Appendix A.
6 Experiments
Experimental setup. We evaluate our proposed methods in semi-supervised learning setting on
various classification datasets, summarized in Appendix B. In addition to different sampling mech-
anisms, we introduce Exact sampling that takes all neighbors for node embedding computation
during mini-batch training (no zeroth-order variance), which can be used to explicitly test the im-
portance of first-order variance reduction. We add SGCN+(Zeroth) and SGCN++(Doubly) on
top of each sampling method to illustrate how zeroth-order and doubly variance reduction affect
GCN training. All implementation details are deferred to Appendix B. By default, we train 2-layer
GCNs with hidden dimension of 256 and snapshot gap K = 10. We use all nodes for snapshot
computation on Flickr, PPI, PPI-large datasets. To scale variance reduced algorithm on large-graph,
we employ snapshot large-batch approximation for both SGCN+ and SGCN++ by randomly selecting
50% of nodes for Reddit and 15% of nodes for Yelp dataset. We update the model with a mini-batch
size of B = 512 and Adam optimizer with a learning rate of η = 0.01. We conduct training 3 times
for 200 epochs and report the average results. We choose the model with the lowest validation error
as the convergence point. A summary of experiment configurations and data statistic can be found
in Table 2 and Table 3 in Appendix B. Due to the space limit, more experiments can be found in
Appendix C.
Experiment results. In Table 1 and Figure 2, we show the accuracy and convergence comparison
of SGCN, SGCN+, and SGCN++. We remark that multi-class classification tasks prefer a more sta-
ble node embedding and gradient than single-class classification tasks. Therefore, even the vanilla
Exact, GraphSAGE and VRGCN already outperforms other baseline methods on PPI, PPI-large,
and Yelp. Applying variance reductions can further improve its performance. In addition, we ob-
serve that the effect of variance reduction depends on its base sampling algorithms. Even though
the performance of base sampling algorithm various significantly, the doubly variance reduction
can bring their performance to a similar level. Moreover, we can observe from the loss curves that
SGCNs suffers an residual error as discussed in Theorem 1, and the residual error is proportional
to node embedding approximation variance (zeroth-order variance), where VRGCN has less vari-
ance than GraphSAGE because of its zeroth-order variance reduction, and GraphSAGE has less
variance than LADIES because more nodes are sampled for node embedding approximation.
8
Under review as a conference paper at ICLR 2021
Figure 2: Comparing the validation loss of SGCN and SGCN++ on real world datasets.
7 Conclusion
In this work, we develop a theoretical framework for analyzing the convergence of sampling based
mini-batch GCNs training. We show that the node embedding approximation variance and layerwise
gradient variance are two key factors that slow down the convergence of these methods. Further-
more, we propose doubly variance reduction schema and theoretically analyzed its convergence.
Experimental results on benchmark datasets demonstrate the effectiveness of proposed schema to
significantly reduce the variance of different sampling strategies to achieve better generalization.
References
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SiamReview, 60(2):223-311, 2018.
Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu, and Liwei Wang. Graphnorm: A prin-
cipled approach to accelerating graph neural network training. arXiv preprint arXiv:2009.03294,
2020.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. arXiv preprint arXiv:1710.10568, 2017.
Jie Chen and Ronny Luss. Stochastic gradient descent with biased but consistent gradient estimators.
arXiv preprint arXiv:1807.11880, 2018.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. arXiv preprint arXiv:1801.10247, 2018.
Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds ofa family of recurrent neural
networks. arXiv preprint arXiv:1910.12947, 2019.
Tianyi Chen, Yuejiao Sun, and Wotao Yin. Solving stochastic compositional optimization is nearly
as easy as solving stochastic optimization. arXiv preprint arXiv:2008.10847, 2020.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn:
An efficient algorithm for training deep and large graph convolutional networks. arXiv preprint
arXiv:1905.07953, 2019.
Weilin Cong, Rana Forsati, Mahmut Kandemir, and Mehrdad Mahdavi. Minimal variance sampling
with provable guarantees for fast training of graph neural networks. In Proceedings of the 26th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1393-
1403, 2020.
Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In NIPS, 2017.
Vikas K Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. arXiv preprint arXiv:2002.06157, 2020.
9
Under review as a conference paper at ICLR 2021
David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions
on Information Theory, 57(3):1548-1566, 2011.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Yifan Hu, Siqi Zhang, Xin Chen, and Niao He. Biased stochastic gradient descent for conditional
stochastic optimization. arXiv preprint arXiv:2002.10790, 2020.
Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in Neural Information Processing Systems, pp. 4558-4567,
2018.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Sashank J Reddi, Ahmed Hefny, SUVrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323, 2016.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Constant time graph neUral networks, 2020.
URL https://openreview.net/forum?id=rkgKW64FPH.
Michael SchlichtkrUll, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolUtional networks. In European Semantic Web
Conference, pp. 593-607. Springer, 2018.
QUoc Tran-Dinh, Deyi LiU, and Lam M NgUyen. Hybrid variance-redUced sgd algorithms for
nonconvex-concave minimax problems. arXiv preprint arXiv:2006.15266, 2020.
QUan Wang, Zhendong Mao, Bin Wang, and Li GUo. Knowledge graph embedding: A sUrvey of
approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):
2724-2743, 2017.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018.
ShUogUang Yang, Mengdi Wang, and Ethan X Fang. MUltilevel stochastic gradient methods for
nested composition optimization. SIAM Journal on Optimization, 29(1):616-659, 2019.
Rex Ying, RUining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and JUre Leskovec.
Graph convolUtional neUral networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974-
983. ACM, 2018.
Hanqing Zeng, HongkUan ZhoU, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based indUctive learning method. arXiv preprint arXiv:1907.04931, 2019.
JUnyU Zhang and Lin Xiao. A composite randomized incremental gradient method. In International
Conference on Machine Learning, pp. 7454-7462, 2019a.
JUnyU Zhang and Lin Xiao. MUlti-level composite stochastic optimization via nested variance re-
dUction. arXiv preprint arXiv:1908.11468, 2019b.
JUnyU Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance
redUction. In Advances in Neural Information Processing Systems, pp. 9078-9088, 2019c.
YUyU Zhang, Xinshi Chen, YUan Yang, ArUn RamamUrthy, Bo Li, YUan Qi, and Le Song. Efficient
probabilistic logic reasoning with graph neUral networks. arXiv preprint arXiv:2001.11850, 2020.
Difan ZoU, ZiniU HU, Yewen Wang, Song Jiang, YizhoU SUn, and QUanqUan GU. Layer-dependent
importance sampling for training deep and large graph convolUtional networks. In Advances in
Neural Information Processing Systems, pp. 11249-11259, 2019.
10
Under review as a conference paper at ICLR 2021
Appendix
Table of Contents
A	Connection to composite optimization	12
B	Experiment configurations	13
C	Additional experiments	15
D	Different sampling strategies	19
E	Detailed algorithms	21
E.1	Description .............................................................. 21
E.2	SGCN ..................................................................... 21
E.3	SGCN+ .................................................................... 21
E.4	SGCN++ ................................................................... 22
E.5	SGCN++ without full-batch ................................................ 23
F	Notations, important propositions and lemmas	24
F.1	Notations for gradient computation ....................................... 24
F.2	Upper-bounded on the node embedding matrices and layerwise gradients ..... 25
F.3	Lipschitz continuity and smoothness property of graph	convolution layers . 26
F.4	Lipschitz continouity of the gradient of graph convolutional	network ..... 27
G Proof of Theorem 1	30
G.1	Supporting lemmas ........................................................ 31
G.2	Remaining steps toward Theorem 1 ......................................... 34
H Proof of Theorem 2	36
H.1	Supporting lemmas ........................................................ 36
H.2	Remaining steps toward Theorem 2 ......................................... 40
I	Proof of Theorem 3	41
I.1	Supporting lemmas ........................................................ 41
I.2	Remaining steps toward Theorem 3 ......................................... 52
J Reproducing experiment results	53
11
Under review as a conference paper at ICLR 2021
A Connection to composite optimization
In this section, we formally compare the optimization problem in training GCNs to the standard
composite optimization and highlight the key differences that necessitates developing a completely
different variance reduction schema and convergence analysis compared to the composite optimiza-
tion counterparts (e.g., see Fang et al. (2018)).
Different objective function. In composite optimization, the output of the lower-level function is
treated as the parameter of the outer-level function. However in GCN, the output of the lower-level
function is used as the input of the outer-level function, and the parameter of the outer-level function
is independent of the output of the inner-layer result.
More specifically, a two-level composite optimization problem can be formulated as
1N 1M
F(θ) = N X fi (M Xgj(W)), θ = {w},	(13)
i=1	j=1
where fi(∙) is the outer-level function computed on the ith data point, gj (∙) is the inner-level function
computed on the jth data point, and W is the parameter. We denote Vfi(∙) and Ngj(∙) as the
gradient. Then, the gradient for Eq. 13 is computed as
NM	M
VF(θ) = [N X Vfi(M Xgj(W))] (M X Vgj(w)} θ = {w},	(14)
where the dependency between inner- and outer-level sampling are not considered. One can indepen-
dently sample inner layer data to estimate e ≈ 吉 PM=ι gj(W) and Ve ≈ 吉 PM=ι Vgj (w), sam-
ple outer layer data to estimate Vf ≈ N PN=I Vfi(e), then estimate VF(θ) by using [Vf]>Ve.
By casting the optimizaion problem in GCN as composite optimization problem in Eq. 13, we have
L(θ) = BB X Loss(hL,yi), θ = {W(1)}
i∈VB
H(L) = σ(Le(L)XWf(L)), Wf(L) = σLe(L-1)XσLe(L-2)X. . . σLe(1)XW(1) . . .,	(15)
'---------V----------}
Wf(2)
which is different from the vanilla GCN model. To see this, we note that in vanilla GCNs, since
the sampled nodes at the `th layer are dependent from the nodes sampled at the (` + 1)th layer, we
have E[e⑶]=P⑶=L. However in Eq. 15, since the sampled nodes have no dependency on
the weight matrices or nodes sampled at other layers, We can easily obtain E[L(`)] = L. These key
differences makes the analysis more involved and are reflected in all three theorems, that give us
different results.
Different gradient computation and algorithm. The stochastic gradients to update the parameters
in Eq. 15 are computed as
∂L(θ) _ ∂L(θ) / YY ∂W⑶ ∖
∂W ⑶=∂ W(L)	`l I ∂WjT)J
j='+1
(16)
However in GCN, there are two types of gradient at each layer (i.e., De (`) and Ge (`)) that are fused
with each other (i.e., D(') is a part of G('-1) and D(') is a part of D(ET)) but with different
functionality. De (`) is passing gradient between different layers, Ge (`) is passing gradient to weight
matrices.
These two types of gradient and their coupled relation make both algorithm and analysis different
from Zhang & Xiao (2019b). For example in Zhang & Xiao (2019b), the zeroth-order variance
(`)
reduction is applied to Wt in Eq. 13 (please refer to Algorithm 3 in Zhang & Xiao (2019b)),
12
Under review as a conference paper at ICLR 2021
where W(')ι is used as a control variant to reduce the variance of W('), i.e.,
WW('+1) = W(-11) + σ(Lt')xW(')) - σ(Lt')xW(')ι).	(17)
However in SGCN++, the Zeroth-Order variance reduction is applied to Ht'). Because the node
sampled at the tth and (t-1)th iteration are unlikely the same, We cannot directly use H(')1 to reduce
(`)
the variance of Ht . Instead, the control variant in SGCN++ is computed by applying historical
weight Wq-I on the historical node embedding from previous layer Ht-L1), i.e.,
H (') = H (-1 + σ(L(')H 尸)W(')) - σ(L (')H('J11)W(')1).	(18)
These changes are not simply heuristic modifications, but all reflected in the analysis and the result.
Different theoretical results and intuition. The aforementioned differences further result in a
novel analysis of Theorem 1, where we show that the vanilla sampling-based GCNs suffer a residual
error ∆b that is not decreasing as the number of iterations T increases, and this residual error is
strongly connected to the difference between sampled and full Laplacian matrices. This is one of
our novel observations for GCNs, when compared to (1) multi-level composite optimization with
layerwise changing learning rate Yang et al. (2019); Chen et al. (2020), (2) variance reduction based
methods Zhang & Xiao (2019b), and (3) the previous analysis on the convergence of GCNs Chen
et al. (2018); Chen & Luss (2018). Our observation can be used as a theoretical motivation on using
first-order and doubly variance reduction, and can mathematically explain why VRGCN outperform
GraphSAGE, even with fewer nodes during training. Furthermore, as the algorithm and gradient
computation are different, the theoretical results in Theorems 2 and 3 are also different.
B	Experiment configurations
Hardware specification and environment. We run our experiments on a single machine with Intel
i5-7500, NVIDIA GTX1080 GPU (8GB memory) and, 32GB RAM memory. The code is written in
Python 3.7 and we use PyTorch 1.4 on CUDA 10.1 to train the model on GPU. During each epoch,
we randomly construct 10 mini-batches in parallel.
Implementation details. To demonstrate the effectiveness of doubly variance reduction, we mod-
ified the PyTorch implementation of GCN (Kipf & Welling, 2016)3 to add LADIES (Zou et al.,
2019), FastGCN (Chen et al., 2018), GraphSAGE (Hamilton et al., 2017), GraphSAINT (Zeng
et al., 2019), VRGCN (Chen et al., 2017), and Exact sampling mechanism. Then, we implement
SGCN+ and SGCN++ on the top of each sampling method to illustrate how zeroth-order variance
reduction and doubly variance reduction help for GCN training.
By default, we train 2-layer GCNs with hidden state dimension of 256, element-wise ELU as the
activation function and symmetric normalized Laplacian matrix L = D-1/2AD-1/2. We use
mean-aggregation for single-class classification task and concatenate-aggregation for multi-class
classification. The default mini-batch batch size and sampled node size are summarized in Table 2.
We update the model using Adam optimizer with a learning rate of 0.01. For SGCN++, historical
node embeddings are first calculated on GPUs and transfer to CPU memory using PyTorch com-
mand Tensor.to(device). Therefore, no extra GPU memory is required when training with
SGCN++. To balance the staleness of snapshot model and the computational efficiency, as default
we choose snapshot gap K = 10 and early stop inner-loop if the Euclidean distance between current
step gradient to snapshot gradient is larger than 0.002 times the norm of snapshot gradient.
Table 2: Configuration of different sampling algorithms during training
	GraphSAGE	VRGCN	Exact	FastGCN	LADIES	GraphSAINT
Mini-batch size	512	512	512	512	512	2048
Sampled neighbors	5	2	All			
Samples in layerwise				4096	512	2048
3https://github.com/tkipf/pygcn
13
Under review as a conference paper at ICLR 2021
Figure 3: Comparison of doubly variance reduction and vanilla sampling-based GCN training on
PPI dataset with SGD (learning rate 0.1) and Adam optimizer (learning rate 0.01). All other Config-
urations are as default.
During training, for each epoch we construct 10 mini-batches in parallel using Python package
multiprocessing and perform training on the sampled 10 mini-batches. To achieves a fair
comparison of different sampling strategies in terms of sampling complexity, we implement all
sampling algorithms using numpy.random and scipy.sparse package.
We have to emphasize that, in order to better observe the impact of sampling on convergence, we
have not use any augmentation methods (e.g., “layer normalization”, “skip-connection”, and “atten-
tion”), which have been proven to impact the GCN performance in Cai et al. (2020); Dwivedi et al.
(2020). Notice that we are not criticizing the usage of these augmentations. Instead, we use the
most primitive network structure to better explore the impact of sampling and variance reduction on
convergence.
Comparison of SGD and Adam. It is worth noting that Adam optimizer is used as the default op-
timizer during training. We choose Adam optimizer over SGD optimizer for the following reasons:
(a)	Baseline methods training with SGD cannot converge when using a constant learning rate
due to the bias and variance in stochastic gradient (Adam has some implicit variance re-
duction effect, which can alleviate the issue). The empirical result of SGD trained baseline
models has a huge performance gap to the one trained with Adam, which makes the com-
parison meaningless. For example in Figure 3, we compare Adam and SGD optimizer on
PPI dataset. For Adam optimizer we use PyTorch’s default learning rate 0.01, and for SGD
optimizer we choose learning rate as 0.1, which is selected as the most stable learning rate
from range [0.01, 1] for this dataset. Although the SGD is using a learning rate 10 times
larger than Adam, it requires 100 times more iterations than Adam to reach the early stop
point (valid loss do not decrease for 200 iterations), and suffers a giant performance gap
when comparing to Adam optimizer.
(b)	Most public implementation of GCNs, including all implementations in PyTorch Geometric
and DGL packages, use Adam optimizer instead of SGD optimizer.
(c)	In this paper, we mainly focus on how to estimate a stabilized stochastic gradient, instead
of how to take the existing gradient for weight update. We employ Adam optimizer for all
algorithms during experiment, which lead to a fair comparison.
14
Under review as a conference paper at ICLR 2021
Dataset statistics. We summarize the dataset statistics in Table 3.
Table 3: Summary of dataset statistics. m stands for multi-class classification, and s stands for
single-class.
Dataset	Nodes	Edges	Degree	Feature	Classes	Train/Val/Test
PPI	14, 755	225, 270	15	50	121(m)	66%/12%/22%
PPI-Large	56, 944	818, 716	14	50	121(m)	79%/11%/10%
Flickr	89, 250	899, 756	10	500	7 (s)	50%/25%/25%
Reddit	232, 965	11, 606, 919	50	602	41(s)	66%/10%/24%
Yelp	716, 847	6, 977, 410	10	300	100(m)	75%/10%/15%
C Additional experiments
Effective of variance reduction. In Figure 4, we empirically evaluate the effectiveness of variance
reduction by comparing the mean-square error of the stochastic gradient and training loss curve of
different sampling strategies on Reddit dataset.
Because Exact sampling is using all neighbors for the node embedding approximation, it is only
affected by layerwise gradient variance (first-order variance). Therefore, employing first-order vari-
ance reduction on Exact sampling can significantly reduce the mean-square error of stochastic
gradient to full-gradient, and speed up the convergence.
Different from Exact sampling that the exact node embeddings are available during training, layer-
wise sampling algorithm LADIES, and nodewise sampling GraphSAGE are approximating the true
node embeddings using a subset of nodes (neighbors). Therefore, these methods both suffer from
node embedding approximation variance (Zeroth-order variance) and layerwise gradient variance
(first-order variance). As a result, applying Zeroth-Order variance reduction and first-order variance
reduction simultaneously is necessary to reduce the mean-square error of the stochastic gradient and
speed up the convergence.
Figure 4: Comparing the mean-square error of stochastic gradient to full gradient and training loss
of SGCN, SGCN+, SGCN++ in the first 200 iterations of training process on Reddit dataset.
GPU memory usage. In Figure 5, We compare the GPU memory usage of SGCN and SGCN+ + .
We calculate the allocated memory by torch.cuda.memory_allocated, which is the current
GPU memory occupied by tensors in bytes for a given device. We calculate the maximum allocated
memory by torch.cuda.max_memory_allocated, which is the maximum GPU memory oc-
cupied by tensors in bytes for a given device.
15
Under review as a conference paper at ICLR 2021
From Figure 5, We observe that neither running full-batch GCN nor saving historical node embed-
dings and gradients will significantly increase the computation overhead during training. Besides,
since all historical activations are stored outside GPU, we see that SGCN++ only requires several
megabytes to transfer data between GPU memory to the host, which can be ignored compared to the
memory usage of calculation itself.
Fllckr - Memory allocated / Epoch
Figure 5: Comparison of GPU memory usage of SGCN and SGCN++ on Flickr and PPI dataset.

Evaluation of total time. In Table 4 and Table 5, we report the average time of doubly variance re-
duced ladies + + and vanilla ladies. We classify the wall clock time during the training process
into five categories:
•	Snapshot step sampling time: The time used to construct the snapshot full-batch or the
snapshot large-batch. In practice, we directly use full-batch training for the smaller datasets
(e.g., PPL PPI-large, and Flickr) and use sampled snapshot large-batch for large datasets
(e.g., Reddit and Yelp). When constructing snapshot large-batch, the Exact sampler has
to go through all neighbors of each node using for-loops based on the graph structure, such
that it is time-consuming.
•	Snapshot step transfer time: The time required to transfer the sampled snapshot batch
nodes and Laplacian matrices to the GPUs.
•	Regular step sampling time: The time used to construct the mini-batches using layerwise
LADiES sampler.
•	Regular step transfer time: The time required to transfer the sampled mini-batch nodes
and Laplacian matrices to GPUs, and the time to transfer the historical node embeddings
and the stochastic gradient between GPUs and CPUs.
•	Computation time: The time used for forward- and backward-propagation.
Notice that we are reporting the total time per iteration because the vanilla sampling-based method
cannot reach the same accuracy as the doubly variance reduced algorithm (due to the residual error
as shown in Theorem 1).
From Table 4 and Table 5, we can observe that the most time-consuming process in sampling-
based GCN training is data sampling and data transfer. The extra computation time introduces by
employing the snapshot step is negligible when comparing to the mini-batch sampling time during
each regular step. Therefore, a promising future direction for large-scale graph training is developing
a provable sampling algorithm with low sampling complexity.
Evaluation of snapshot gap for SGCN+ and SGCN++. Doubly variance reduced SGCN++ requires
performing full-batch (large-batch) training periodically to calculate the snapshot node embeddings
and gradients. A larger snapshot gap K can make training faster, but also might make the snapshot
node embeddings and gradients too stale for variance reduction. In this experiment, we evaluate the
effect of snapshot grap on training by choosing mini-batch size as B = 512 and change the inner-
loop intervals from K = 5 mini-batches to K = 20 mini-batches. In Figure 6 and Figure 7, we show
16
Under review as a conference paper at ICLR 2021
Table 4: Comparison of average time (1 snapshot step and 10 regular steps) of doubly variance
reduced LADIES++ with regular step batch size as 512. Full-batch is used for snapshot step on PPI,
PPI-Large, and Flickr. 50% training set nodes are sampled for the snapshot step on Reddit, and 15%
training set nodes are sampled for the snapshot step on Yelp.
Time (second)	PPI	PPI-Large	Flickr	Reddit	Yelp
Snapshot step sampling	0.182	0.355	0.221	18.446	21.909
Snapshot step transfer	0.035	0.070	0.036	0.427	0.176
Regular step sampling	1.128	1.322	0.899	9.499	9.102
Regular step transfer	0.393	0.459	0.250	0.550	0.372
Computation	0.215	0.196	0.136	0.399	0.139
Total time	1.954	2.377	1.442	29.321	31.697
Table 5: Comparison of average time (10 regular steps) of LADIES with regular step batch size as
512	______________________________________________________________
Time (second)	PPI	PPI-Large	Flickr	Reddit	Yelp
Regular step sampling	1.042	1.077	0.977	9.856	9.155
Regular step transfer	0.036	0.047	0.016	0.496	0.041
Computation	0.077	0.068	0.034	0.029	0.082
Total time	1.156	1.192	1.028	10.381	9.278
the comparison of training loss and validation loss with different number of inner-loop intervals for
SGCN++ and SGCN+ on Reddit dataset, respectively. We can observe that the model with a
smaller snapshot gap requires less number of iterations to reach the same training and validation
loss, and gives us a better generalization performance (FI-Score).
■■■■■■
Figure 6: Comparison of training loss, validation loss, and F1-score of SGCN++ with different
snapshot gap on Reddit dataset.
Evaluation of large-batch size for SGCN+ and SGCN++. The full-batch gradient calculation at
each snapshot step is computationally expensive. Heuristically, We can approximate the full-batch
gradient by using the gradient computed on a large-batch of nodes. Besides, it is worth noting that
large-batch approximation can be also used for the node embedding approximation in zeroth-order
variance reduction. In SGCN+, saving the historical node embeddings for all nodes in an extreme
large graph can be computationally prohibitive. An alternative strategy is sampling a large-batch
during the snapshot step, computing the node embeddings for all nodes in the large-batch, and
saving the freshly computed node embeddings on the storage. After that, mini-batch nodes are
sampled from the large-batch during the regular steps. Let denote B0 as the snapshot step large-
batch size and B denote the regular step mini-batch size. By default, we choose snapshot gap as
17
Under review as a conference paper at ICLR 2021
Figure 7: Comparison of training loss, validation loss, and F1-score of SGCN+ with different snap-
shot gap on Reddit dataset.
K = 10, fix the regular step batch size as B = 512, and change the snapshot step batch size B0
from 20, 000 (20K) to 80, 000 (80K). In Figure 8 and Figure 9, we show the comparison of training
loss and validation loss with different snapshot step large-batch size B0 for SGCN++ and SGCN+,
respectively.
Figure 8: Comparison of training loss, validation loss, and F1-score of SGCN++ with different
snapshot large-batch size on Reddit dataset.
The effect of mini-batch size. In Figure 10, We show the comparsion of training loss and validation
loss with different regular step mini-batch size. By default, We choose the snapshot gap as K = 10
,fix the snapshot step batch size as B0 = 80,000, and change the regular step mini-batch size B
from 256 to 2,048. Besides, we note that subgraph sampling algorithm GraphSAINT requires an
extreme large mini-batch size every iterations. In Figure 11, we explicitly compare the effectiveness
of mini-batch size on doubly variance reduced GraphSAINT++ and vanilla GraphSAINT, and
show that a smaller mini-batch is required by GraphSAINT+ + .
Evaluation of increasing snapshot gap. Snapshot gap size K serves as a budge hyper-parameter
that balances between training speed and the quality of variance reduction. During training, as the
number of iterations increases, the GCN models convergences to a saddle point. Therefore, it is
interesting to explore whether increasing the snapshot gap K during the training process can obtain
18
Under review as a conference paper at ICLR 2021
Reddit: Training loss / Iterations
iterations
Reddit: Validation loss / Iterations
Figure 9: Comparison of training loss, validation loss, and F1-score of SGCN+ with different snap-
shot large-batch size on Reddit dataset.
Figure 10: Comparison of training loss, validation loss, and F1-score of SGCN++ with different
mini-batch size on Reddit dataset.
a speed boost. In Figure 12, We show the comparison of validation loss of fixed snapshot gap K = 10
and gradually increasing snapshot gap K = 10 + 0.1 X s,s = 1, 2,..., where S is the number of
snapshot steps has been computed. Recall that the key bottleneck for SGCN++ is memory budget
and sampling complexity, rather than snapshot computing. Dynamically increasing snapshot gap
can reduce the number of snapshot steps, but cannot significantly reduce the training time but might
lead to a performance drop.
D Different sampling strategies
In this section, we highlight the difference between node-wise sampling, layer-wise sampling, and
subgraph sampling algorithms.
Node-wide sampling. The main idea of node-wise sampling is to first sample all the nodes needed
for the computation using neighbor sampling (NS), then train the GCN based on the sampled nodes.
For each node in the `th GCN layer, NS randomly samples s of its neighbors at the (` - 1)th GCN
layer and formulate Le (`) by
LS L
X Lij,	if j ∈Ne(')(i)
otherwise
(19)
19
Under review as a conference paper at ICLR 2021
Figure 11: Comparing the validation loss and F1-score of GraphSAINT and GraphSAINT+ +
with different mini-batch size on Reddit dataset
Figure 12: Effectiveness of gradually increasing snapshot gap K during training on wallclock
time (second) and accuracy on PPI dataset. We choose snapshot gap K = 10 for fixed-K. For
increasing-K, we choose snapshot gap K = 10 + 0.1 × s, s = 1, 2, . . ., where s is the number of
snapshot steps.
where N(i) is the full set of ith node neighbor, Ne (`) (i) is the sampled neighbors of node i for `th
GCN layer. GraphSAGE Hamilton et al. (2017) follows the spirit of node-wise sampling where
it performs uniform node sampling on the previous layer neighbors for a fixed number of nodes to
bound the mini-batch computation complexity.
Layer-wise sampling. To avoid the neighbor explosion issue, layer-wise sampling is introduced
to controls the size of sampled neighborhoods in each layer. For the `th GCN layer, layer-wise
sampling methods sample a set of nodes B(') ⊆ V of size S under the distribution P to approximate
the Laplacian by
L(')= S s×1pj X Lij，	if j ∈ B⑻
i,j	0,	otherwise
(20)
Existing work FastGCN Chen et al. (2018) and LADIES Zou et al. (2019) follows the spirit of
layer-wise sampling. FastGCN performs independently node sampling for each layer and applies
important sampling to reduce variance and results in a constant sample size in all layers. However,
mini-batches potentially become too sparse to achieve high accuracy. LADIES improves FastGCN
by layer-dependent sampling. Based on the sampled nodes in the upper layer, it selects their neigh-
borhood nodes, constructs a bipartite subgraph, and computes the importance probability accord-
ingly. Then, it samples a fixed number of nodes based on the calculated probability, and recursively
conducts such a procedure per layer to construct the whole computation graph.
Subgraph sampling. Subgraph sampling is similar to layer-wise sampling by restricting the sam-
pled Laplacian matrices at each layer are identical
L(L) = S s×1pj X Li,j，	if j ∈ B⑹
i,j	0,	otherwise
(21)
For example, GraphSAINT Zeng et al. (2019) can be viewed as a special case of layer-wise sam-
pling algorithm FastGCN by restricting the nodes sampled at the 1-st to (L - 1)th layer the same as
the nodes sampled at the Lth layer. However, GraphSAINT requires a significant large mini-batch
size compared to other layer-wise sampling methods. We leave this as a potential future direction to
explore.
20
Under review as a conference paper at ICLR 2021
E Detailed algorithms
E.1 Description
In order to help readers better compare the difference of different algorithms, we summarize the
vanilla sampling-based GCN training algorithm SGCN in Algorithm 3, zeroth-order variance re-
duced algorithm SGCN+ in Algorithm 4, and doubly variance reduced algorithm SGCN++ in Al-
gorithm 5. In addition, we illustrate in Figure 13 the relationship between node embedding ap-
proximation variance and layerwise gradient variance to the forward- and backward-propagation.
We remark that zeroth-order variance reduction SGCN+ is only applied during forward-propagation,
and doubly (zeroth- and first-order) variance reduction SGCN++ is applied during forward- and
backward-propagation, simultaneously.
Forward-propagation (Node embedding approximation variance)
Istlayer	2nd layer	Lth layer
(	I ɪɪ VLoSS(胪〉,yl)
Backward-propagation (Layerwise gradient variance)	ι∈v≡
Figure 13: Relationship between the two types of variance with the training process, where em-
bedding approximation variance (zeroth-order variance) happens during forward-propagation and
layerwise gradient variance (first-order variance) happens during backward-propagation.
E.2 SGCN
Algorithm 3 SGCN: Vanilla sampling-based GCN training method
1:	Input: Learning rate η > 0
2:	for t = 1 ,...,T do
3:	Sample mini-batch VB ⊂ V
4:	Calculate node embeddings using
HH(') = σ(L⑶HH('T)W⑶)，where He⑼=X,	(22)
5:	Calculate loss as Le(θt) = B Pii∈vβ Loss(h(L),yi)
6:	Calculate stochastic gradient VL(θt) = {G(')}L=ι as
G(') := [L(')H('T)]>(D('+1) °Vσ(Z('))),
(23)
D(') := [L(')]>(D('+1) °Vσ(Z(')))w('), DtL+1) = ∂^
7:	Update parameters as θt+1 = θt - ηVL(θt)
8:	end for
9:	Output: Model with parameter θT+1
E.3 SGCN+
Algorithm 4 SGCN+: Zeroth-order variance reduction (Detailed version of Algorithm 4)
1:	Input: Learning rate η > 0, snapshot gap K > 0
2:	for t = 1, . . . , T do
3:	if t mod K = 0 then
4:	% Snapshot steps
21
Under review as a conference paper at ICLR 2021
5:	Calculate node embeddings and update historical node embeddings using
z(') = LHtj)W('), H(') = σ(Z(')), ZF - Z(')	(24)
6:	Calculate loss as L(θt) = N PN=ILoss(h(L) 11,yi)
7:	Calculate full-batch gradient VL(θt) = {G(')}L=ι as
G(') ：= [LH('T)]>(D('+1) oVσ(Z(')))
D(') := L>(D('+1) ◦ Vσ(z(')))w('), D(L+1) = dL(θt)
t	t	It t , t	∂H(L)
8:	Update parameters as θt+1 = θt - ηVL(θt)
9:	else
10:	% Regular steps
11:	Sample mini-batch VB ⊂ V
12:	Calculate node embeddings using
Z(') = Zt-11 + e(')H尸IWt' - L(')H(':ι1)w(')1, H(') = σ(Z⑶)
13:	Calculate loss as L(θt) = -g Pi∈vi3 Loss(h(L), yi)
14:	Calculate the stochastic gradient VL(θt) = {Gt')}L=ι as
G(') ：= [Lt')Ht'-1)]>(D('+1) °Vσ(Z('))),
D(') ：= [Lt')]>(Dt'+1) °Vσ(Z(')))w('), DtL+1) = dLθ)
15:	Update parameters as θt+1 = θt - ηVL(θt)
16:	end if
17:	end for
18:	Output: Model with parameter θT+1
(25)
(26)
(27)
E.4 SGCN++
Algorithm 5 SGCN++: Doubly variance reduction (Detailed version of Algorithm 5)
1: Input: Learning rate η > 0, snapshot gap K > 0
2: for t = 1, . . . , T do
3: if t mod K = 0 then
4:	% Snapshot steps
5:	Calculate node embeddings and update historical node embeddings using
z(') = LHt'-1)w('), H(') = σ(z(')), Zt') — z(')	(28)
6:	Calculate loss as L(θt) = N PN=I Loss(h(L),y∕
7:	Calculate the full-batch gradient VL(θt) = {Gt')}L=ι as
G(') ：= [LH('T)]>(D(')°Vσ(Zt'))),
Dt') •— τ > fDt'+1) c V Zt')、)Wt') DtL+1) - dL(θt)
Dt ：= L (Dt °Vσ(Zt )J wt , Dt = ∂h(l)
8:	Save the per layerwise gradient G(') - G('), Dt') - D(') for all' ∈ [L]
9:	Update parameters as θt+1 = θt - ηVL(θt)
10: else
11:	% Regular steps
12:	Sample mini-batch VB ⊂ V
(29)
22
Under review as a conference paper at ICLR 2021
13:	Calculate node embeddings using
Zt') = Z(')ι + L(')HH('-1)w(') - L(')He('-L1) 11w(')i, He(') = σ(Z⑶)	(30)
14:	Calculate loss as Le(θt) = -B Pi∈vj3 Loss(h(L), yi)
15:	Calculate the stochastic gradient VL(θt) = {G(')}L=ι as
G(') = Gg + [L('亩t'-1)]> (Dt'+1) ◦ Vσ(Zt))
-[L('后储1)]>(D付1) °Vσ(Z，")
D (') = ]ɔ (-1 + [L⑶]> (D('+1) ◦ Vσ(Z t)) [w(')]	(31)
-[L(')]>(D叶1) oVσ(Zt-1))wg], D(L+1) = IL(L)
∂Ht
r /	TT 1 .	,	Λ	Λ	X-Z 7/ 八 \
16:	Update parameters as θt+1 = θt - ηVL(θt)
17:	end if
18:	end for
19:	Output: Model with parameter θT+1
E.5 SGCN++ without full-batch
Furthermore, in Algorithm 6, we provide an alternative version of SGCN++ that does not require full-
batch forward- and backward-propagation at the snapshot step. The basic idea is to approximate the
full-batch gradient by sampling a large mini-batch VB0 of size B0 = |VB0 | using Exact sampling,
then compute the node embedding matrices and stochastic gradients on the sampled large-batch VB0 .
Algorithm 6 SGCN++ (without full-batch): Doubly variance reduction
1:
2:
3:
4:
5:
Input: Learning rate η > 0, snapshot gap K > 0
for t = 1, . . . , T do
if t mod K = 0 then
% Snapshot steps
Sample a large-batch VB of size B0 and construct the Laplacian matrices L(') for each layer
using all neighbors, i.e.,
L(') = (Lij,	if j ∈N(')(i)
i,j 0, otherwise
(32)
6:	Calculate node embeddings and update historical node embeddings using
Zf) = L⑶H('-1)w('), H(') = σ(zt')), Z(') J z(')	(33)
7:	Calculate loss as L(θt) = B Pi∈v，Loss(h(L),yi)
8:	Calculate the approximated snapshot gradient VL(θt) = {G(')}L=1 as
G(') := [L(')H('-1)]>(D('+1) ◦ Vσ(Z('))),
D(')	L(')]> (D('+1) CV Z(')、)W(') D(L+1) - dαθt)
Dt ：= [L ] (Dt	◦ vσ(Zt ) Wt , Dt = ∂h(l)
9:	Save the per layerwise gradient Ge t(`) J Gt'), D(') J D('), ∀' ∈ [L]
10:	Update parameters as θt+1 = θt - ηVL(θt)
11: else
12:	% Regular steps
13:	Sample mini-batch VB ⊂ VB0
14:	Calculate node embeddings using
Z(') = Z(')1 + L(')H('-1)w(') - L(')H('-11)w(')1, H(') = σ(Z('))
(34)
(35)
23
Under review as a conference paper at ICLR 2021
15:	Calculate loss as Le(θt) = -^ Pi∈^is Loss(h(L), yi)
16:	Calculate the stochastic gradient VL(θt) = {G(')}L=1 as
G(') = G化 + [L(')H ('T)]> (D ('+1) ◦ vσ(Z t))
-[L⑶Ht-ι1)]>(D ('+1) °Vσ(ZI))
D (') = D t')1 + [L ⑶]> (D ('+1) ◦ Vσ(Z t)) [w(')]
-[L(叩(Dt-11) °Vσ(ZI))W㈡,
+ 1)_ ∂Le(θt)
---------:—:-
∂H HtL)
17:	Update parameters as θt+1 = θt - ηVL(θt)
18:	end if
19:	end for
20:	Output: Model with parameter θT+1
(36)
The intuition of snapshot step large-batch approximation stems from matrix Bernstein inequal-
ity Gross (2011). More specifically, suppose given Ge i ∈ Rd×d be the stochastic gradient computed
by using the ith node with Exact sampling (all neighbors are used to calculate the exact node em-
beddings). Suppose the different between Gi and full-gradient E[Gi] is uniformly bounded and the
variance is bounded:
kGi - EGi]kF ≤ μ, E[kGi - E[Gi]kF] ≤ σ2
(37)
Let Ge 0 as the snapshot step gradient computed on the sampled large batch
G 0=B XG 4	(38)
i∈VB0
By matrix Bernstein inequality, we know the probability of kG0-E[Gi]kF larger than some constant
decreases exponentially as the size of the sampled large-batch size B 0 increase, i.e.,
Pr(kGe 0 - E[Ge i]kF ≥ ) ≤ 2d exp
(39)
Therefore, by choosing a large enough snapshot step batch size B 0 , we can obtain a good approxi-
mation of full-gradient.
F Notations, important propositions and lemmas
F.1 Notations for gradient computation
We introduce the following notations to simplify the representation and make it easier for readers to
understand. Let formulate each GCN layer in FullGCN as a function
H(') = [f(')(Ht'T), W(')) := σ(LH('-1)W('))] ∈ RN×d'
'---------------------------------------}''
(40)
z(`)
and its gradient w.r.t. the input node embedding matrix Dt') ∈ RN ×d'-1 is computed as
D(')= IVH f (')(D('+1), H('-1), W(')) ：= [L]>(D('+1) ◦ σ0(LH('-1)W(')))[W(')]>i
and its gradient w.r.t. the weight matrix Gt') ∈ Rd'-1×d' is computed as
Gt') = ∣Vwf(')(D('+1), Ht'-1), Wt')):= [LH('-1)]>(D('+1) ◦ σ0(L(')H('-1)W(')))]
(41)
(42)
24
Under review as a conference paper at ICLR 2021
Similarly, We can formulate the calculation of node embedding matrix H (') ∈ RN ×d' at each GCN
layer in SGCN as
H(') = [fe⑶(H('T), W⑶):=σ(je(')lH('T)W⑶,)]	(43)
"^^^^^^^^{^^^^^^^^^
e(')
and its gradient w.r.t. the input node embedding matrix D (') ∈ RN ×d'-1 is computed as
De (`)
IVHf(')(D('+1),H('-1),w('))：= [L(')]>(D('+1) ◦ b0(L(')H('T)w(')))[w(')]>]
(44)
,_
and its gradient w.r.t. the weight matrix G(') ∈ Rd'-1×d' is computed as
G(') = [vw严(D('+1),H('-1), w('))：= [L(')H('T)]>(D('+1) ◦ σ0(L(')HH('-1)w('))^i
(45)
Let us denote the gradient of loss w.r.t. the final node embedding matrix as
D(L+i) = dLOSS(H(L), y)
D = ~∂H(L-
D (L+1) = d LoSS(H (L), y)
一	∂H(L)
∈ RN×dL, [D(L+1)]i = N dLoSS(h()) ,yi) ∈ RdL
∈ RN "'[D S" = B i{i∈VB}d y,yi)
B	∂h
∈ RdL
(46)
Notice that De (L+1) is a N × dL matrix with the row number i ∈ VB are non-zero vectors. Then we
can write the gradient for the `th weight matrix in FullGCN and SGCN as
G(') = VW f (')(VH f ('+1)(... VH f (L)(D(L+1), H(LT), W(L))..., H('), W('+1)), H('-1), W('))
G (') = VW f(')(VH f('+1)(... VH f(L)(D(L+1), H (LT), W(L))..., H('), w('+1)), H ('-1), w('))
(47)
F.2 Upper-bounded on the node embedding matrices and layerwise gradients
Based on the Assumption 3, we first derive the upper-bound on the node embedding matrices and
the gradient passing from `th layer node embedding matrix to the (` - 1)th layer node embedding
matrix.
Proposition 2 (Detailed version of Proposition 1). For any ` ∈ [L], the Frobenius norm of node
embedding matrices, gradient passing from the `th layer node embeddings to the (` - 1)th are
bounded
∂σ(LH('-1)W('))	∂σ(L (')H ('-1)W('))
旧勺山 ≤ Bh, ∣∣H(')∣∣f ≤ BH, k (海	-)∣∣f ≤ Bd, k ( Q三二丁_)∣∣f ≤ BD
∂H''	)	∂H('-1)
(48)
where
BH = CσBLABW BH, BD = BLACσBW	Closs	(49)
Proof.
∣∣H(')∣∣f = kσ(LH('τ) W('))||f
≤ CσBLABWkH('-1)kF
≤ (CσBLABW)'kXkF ≤ (CσBLABWYBH
恒(')∣∣f = ∣∣σ(L (')HH ('-1)W('))kF
≤ CσBLABWkH('T)∣∣f
≤ (CσBLABW)'kX∣F ≤ (CσBLABW)'Bh
(50)
(51)
25
Under review as a conference paper at ICLR 2021
∣∣D 叫F = ∣∣[L]τ(D('+1) o σ0(LH(J)W ⑶))[W ⑹]>k
≤ BLACσBw||D('+I)IlF	(52)
L—'	L—'
≤ (BLAaBW)	kD(L+∖∣F ≤ (BLAaBW)	Closs
∣∣DqF = WL⑶]T(D('+1) o σ,(L⑶H(J)W⑶))[w⑶]t∣∣f
≤ BlaCσBw∣D('+I)IlF	(53)
≤ (BLACσBw)L-'kD(L+1)|f ≤ (BLACσBW)LjClOSS
□
F.3 Lipschitz continuity and smoothness property of graph convolution
LAYERS
Then, we derive the Lipschitz continuity of /(`)(•,•) and its gradient. Notice that the following result
hold for deterministic function f ⑶(∙, ∙) as well.
Proposition 3. f(')(•, ∙) is CH-Lipschitz COntinUOUS w.r.t. the input node embedding matrix where
CH = Cσ BLABW
Proof.
kf(')(HsT), W('))-产(H尸),W('))||f
=∣∣σ(L⑶H,-1)W⑹)-σWH尸)W('))kF
≤ CσIlLqFkHST)-短'—1)|周|亚叫F	(54)
≤ CσBLABWkHsT)-短'-1)庙
□
Proposition 4. f(')(•, ∙) is CW-Lipschitz continuous w.r.t. the weight matrix where CW
Cσ BlaBh
Proof.
f)(HJ), WS))-产(H('-1), W*)∣∣F
=∣∣σ(L⑶H(J)W，)- σ(e⑶H(J)Wy))IlF
≤ Cσ ∣∣L(')∣∣f∣∣h('T)IlFllWf)- WyIIF
≤ CσBlaBhIWf)- WylIF
□
Proposition 5. NH f ⑹(∙, ∙, ∙) is LH-Lipschitz COntinUOUS where
LH = max nBii^AC;crBw, BLABDBWLσ, BlaBdCσ + BLABDBWLσBh}
(55)
(56)
Proof.
∣∣Vh 尹)(DS+1), H('—1), w('))-VH f')(D2'+1), H('—1), w('))∣∣f
≤ k[e(')]T(DS+1) o σ0(L(')H('τ)w(')))[w(')]τ
-	[L(')]τ (D2'+1) o σ0(L(')H('τ)w(')))[w(')]TkF
≤ BlaCσBWkDf+1) - D2'+1)||f
(57)
26
Under review as a conference paper at ICLR 2021
∣∣vh 尹)(D ('+1), H ST), W⑶)-VH 兴)(D ('+1), H 2'-1), W ⑶)∣∣F
≤ Ime)]>(D'+1) O σ00)]⅞ST)W⑶))[W(')]T
-	[M)]> (6('+1) o σ0(M)]⅞尸)w(')”[W(')]TkF
≤ BLABdBWLσ恒ST)-前T)IIF
(58)
∣∣vh f(e∖De+1∖ H(J), Wf))- vh 尹)($('+1), H(J), w2e))|F
≤ 口区⑶]T(De('+1) o『0亩('T)Wf))) [w(e)]T
-	[Le(')]τ ($('+1) O σ,(M)H(J)Wg)))[w2e)]TkF	(59)
≤ (BLABDCσ + BLABDBWLσBh)∣Wf)- W*∣f
□
Proposition 6. VW/(')(•, ∙, ∙) is Lw-Lipschitz continuous where
Lw =max [blj^BhCσ,BLa^HHdLσ,BlaBdCσ + BLABHBDLσ}	(60)
Proof.
∣∣Vw f(e)(DS+1), H('-1), W⑶)一Vw f(')(De2'+1), H('-1), W⑶)∣f
≤ k[L(')H('τ)]T(De('+1) o σ0(L⑶H('t)w(')”
-[je(')H('T)]τ (De2'+1) O σ0(L⑶H('T)w('))) kF
≤ BLABHCσ kD 1'+1)- D'+1)|f
∣∣Vw兴)(D(e+1), H 1eT), W⑶)-Vw尹)(D(e+1), H2eT), W(e))kF
≤ Il [L(e)H 1'T)]t(D(e+1) O σ,(je(')H 1'T)W⑶))
-[je(')H尸)]τ (De('+1) O σ,0)H尸)W('))) kF
≤	(BLABDCσ + BLABHBdLσBW)恒 1'T)- H2'T)∣f
∣∣Vw 尹)(D ('+1), H('-1), Wf))-VW 尹)(D ('+1), H('-1), w2'))∣f
≤	k[L')H('τ)]T(De('+1) o σ0(L')H('T)w(')))
-[L(')H('t)]t (De('+1) o σ0(L(')H('τ)w2')” ∣f
≤ BLABHHdLσ∣W1') - w2')||f
(61)
(62)
(63)
□
F.4 Lipschitz CONTINOUITY OF THE gradient of graph convolutional network
Let first recall the parameters and gradients of a L-layer GCN is defined as
θ = {W(I),…,W(L)}, VL(θ) = {G(I),..., G(L)}	(64)
where G(')is defined as the gradient w.r.t. the 'th layer weight matrix. Let US slight abuse of
notation and define the distance between two set of parameters θ1, θ2 and its gradient as
LL
kθ1 - θ2∣F = X kW(') - w2')∣∣f, kVL(θ1) - VL(θ2)kF = X ∣∣G1') - g2')∣∣f	(65)
'=1	'=1
Then, we derive the Lipschitz continuous constant of the gradient of a L-layer graph convolutonal
network. Notice that the above result also hold for sampling-based GCN training.
27
Under review as a conference paper at ICLR 2021
Lemma 1. The gradient of an L-layer GCN is LF -Lipschitz continuous with LF
maxLUmax C + Umax L)) ie.,
kVL(θ1) -VL(θ2)kF ≤ Lf∣∣Θi- θ2∣∣F
(66)
where
UmaX C
Umax L
max	CH Cw ,
j∈{o,…,L—1}
min	LW LH × max41, Lloss ∖
Lj∈{0,...,L-g-1}	HJ	I	J
(67)
Proof. We first consider the gradient w.r.t. the 'th graph convolutional layer weight matrix
IlGS)-g2')∣∣f
=∣Vw f ⑶(VH f ('+1)(... Vh f (L)(DIL+1), HILT), WIL))..., H，, W，+1)), HfT), Wf))
-Vw f ⑶(VH f ('+1)(... Vh f (L)(D2L+1),虱LT), w2L))..., h2')w2'+1)), h2'-1), w2'))||f
≤ ∣Vw f ⑶(VH f ('+1)(... Vh f (L)(DIL+1), HILT), WIL))…,HR W，+1)), HST), Wf))
-	VW f ⑶(VH f ('+1)(... Vh f (L)(D2L+1), HILT), W(L))…,HR W('+1)), HfT), Wf))IlF
+ ∣Vw f ⑶(VH f ('+1)(... Vh f (L)(D2L+1), HILT), WIL))…，HR W，+1)), HST), WS))
-	Vw f ⑶(VH f ('+1)(... Vh f (L)(D2L+1),短LT), W(L))…,HR W，+1)), HST), Wf))IIF
+ ∣Vw f ⑶(VH f ('+1)(... Vh f (L)(D2L+1), h2LT), WIL))…,H，, W，+1)), HfT), Wf))
-	Vw f ⑶(VH f ('+1)(... Vh f (L)(D2L+1),短LT), w2L))…,HR W，+1)), HST), Wf))IIF + …
+ ∣Vw f ⑶(d2'+1), HST), WS))-VW f ⑶(d2'+1), h2'-1), Wf))IIF
+ IVw f ⑶(d2'+1), h2'-1), Wf))-VW f ⑶(d2'+1), h2'-1), w2'))||f
(68)
By the Lipschitz continuity of VWf ⑶(∙) and VHf(')(•)
∣∣G? - g2')∣∣f ≤ LwlH-'Tl∣oss∣ML)-短L)IIF
+ LwLHTT(IlHILT)- H2LT)IlF + ∣W(L)- w2L)IlF) + …
+ LwLh(IlHS)-短‘)|山 + ∣∣wf+1)- w2'+I)IIF)
+ Lw(∣H1'T)- H'T)IIF + IlWf)- w2%f)
Let define Umax L as
Umax L
min	LW LH
L j∈{0,…,L-2-1}	」
× max {1, Lloss }
(70)
then we can rewrite the above equation as
LL
IlGs)- G2%F ≤ Umax L (X IIHIj) - Hj" f) + Umax l( X |Wj)-亚初山)(71)
j=ι	j=ι
Then, let consider the upper bound of IlHS) - h2')∣∣f
IIHf)- h2')∣∣f = If (')(f *”...f(I)(X, WII))…，WfT)), Wf))
-	f ⑶(f (J)(... f(I)(χ, w21))…,w2'-1)), w2'))∣f
≤ If⑶(f(J)(...f(I)(X,WII))…,WfT)),Wf))
-	f ⑶(f ('-1)(...f(I)(X, W21))…，WfT)), Wf))IF + ...
+ If ⑶(f (J)(... f(I)(X, W21))…,w2'-1)), Wf))
-	f ⑶(f (J)(... f(I)(X, W21))…,w2'-1)), Wf))IF
(72)
28
Under review as a conference paper at ICLR 2021
By the LiPschitz continuity of f(')(∙, ∙) We have
kH') - h2')∣∣f ≤ CHTCWkW11)- w21)kF + ... + CwkW(') - w2')∣∣f
L
≤ j∈{0maxL-1}CHjCW	X kW1(j) -W(2j)kF
,...,	j=1
Let define Umax C is defined as
Umax C =	max	CHj CW
j∈{0,...,L-1}
Plugging it back We have
L
kGl') - G2')∣∣f ≤ (LUmax LUmax C + Umax L)( X ∣Wj) — 亚2叫山)
j=1
Summing both size from ` = 1 to ` = L We have
L
kVL(θι) - VL(θ2)kF = X kGl') - g2')∣∣f
'=1
L
≤ L(LUmaxLUmaxC+UmaxL) XkW(1j) -W(2j)kF
j=1
≤ L(LUmaxLUmaxC + UmaxL)kθ1 - θ2kF
(73)
(74)
(75)
(76)
□
29
Under review as a conference paper at ICLR 2021
G Proof of Theorem 1
By bias-variance decomposition, we can decompose the mean-square error of stochastic gradient as
L
XE[kGG⑶-G⑶
'=1
L
kF] = Xh E[kE[G ⑶]-G⑶
'=1	'	〜―
=	bias E[kbkF]
眩}+E[kG ⑶-e[G ⑶]kF]]
variance E[knk2F]
(77)
Therefore, we have to explicitly define the computation of E[Ge (`)], which requires computing
D(L+I) = E[D(L+1)], D(') = E[DD(')], and G⑶=E[G(')].
Let defined a general form of the sampled LaPlaCian matrix L(') ∈ RN×N as
L(')= ( Lij	if i ∈B⑶ and j ∈ B('-1)
i,j 0 otherwise
where αi,j is the weighted Constant depends on the sampling algorithms.
The expectation of L(j is computed as
Ele(('j] = Ei∈B(') hEj∈B('-1) Rfj) 1 i ∈ BRi
(78)
(79)
In order to compute the expectation of SGCN’s node embedding matrices, let define the propagation
matrix P⑶ ∈ RN×N as
Ps) = Ei∈B(') [L(') I i ∈ B(')i
(80)
where the expectation is taken over row indices i. The above equation implies that under the condi-
tion that knowing the ith node is in B('), We have Pj) = Lij, ∀j = {1,...,Ν}. Let consider the
mean-aggregation for the ith node as
N
x(')=σ(X Lfj)Xj'T))	(81)
j=1
Then, under the condition ith node is in B('), we can replace L，j by Pij, which gives US
N
x(')=σ(X Pij)X 尸))	(82)
j=1
As a result, we can write the expectation of x(') with respect to the indices i as
N
Ei∈B(') [xi') I i ∈ B(')] = Ei∈B(') [σ(X e('j)xj'τ)) I i ∈ B(')]
j=1
N
=Ei∈B(') [σ ( X Pij)X尸)) I i ∈ B(')i	(83)
j=1
N
= σ(X PSXjj))
j=1
Then define H(') ∈ RN ×d' as the node embedding of using full-batch but a subset of neighbors for
neighbor aggregation, i.e.,
H (') = σ(p(')H('T)W⑶)	(84)
where all rows in H(') are non-zero vectors.
30
Under review as a conference paper at ICLR 2021
Using the notations defined above, we can compute D(L十1)∈ RN×dL, G(') ∈ Rd'-1×d', and
D(') ∈ RN×d'-ι as
D(L+1) = e[
∂Loss(H (L))
∂H(L)
]∈ RN×dL , di = N dLOSdg)，yi) ∈ RdL
(85)
and
D⑶
and
G(')
IVH产)(D('+1), H('-1), Wg) := [L]τ(D('+1) o σ0(p(')H('τ)w(')))[W(')]τ]
(86)
IVW/⑶(D('+1), H('-1), W(')) := [LH('T)]τ(D('+1)◦ σ,(P(')Il('-1)W(`)))] (87)
As a result, we can represent G⑶=E[G(`)] as
G ⑶=VW 于⑻ EH /('+1)(∙∙∙ Vh f(L)(D(L+1), H (LT), W(L))..., H ⑶,W('+1)), H ('-1), W(')))
(88)
G.1 Supporting lemmas
We derive the upper-bound of the bias and variance of the stochastic gradient in the following lem-
mas.
Lemma 2 (Upper-bound on variance). We can upper-bound the variance of stochastic gradient in
SGCN as
LL
XE[∣∣GC)- E[G⑶]kF] ≤ XO(E[kLe)- p(')∣∣F]) + O(IlPe)- LkF)	(89)
'=1	'=1
Proof. By definition, we can write the variance in SGCN as
e[∣∣G⑻-e[G(')]∣∣F]
=e[∣vw f⑶(VH 尹+1)(…VH f(L)(D(L+1), H (LT)W(L)) ∙∙∙, H ⑶,w('+1)), H ('-1), W⑶)
-VW K(VH f'+1)(∙ ∙ ∙ VH f(L)(D(L+1), H (LT), W(L)) ∙∙∙, H('), W('+1)), H ('-1)), W(')kF]
≤ (L + 1)E[kVw f(')( VH f('+1)(…VH f(L) (D(L+1), H(LT)W(L)) ∙∙∙, H('), w('+1) ), H('-1), W⑶)
-	VW f)(VH f+1)(∙ ∙ ∙ VH f(L)(D(L+1), H(LT)W(L)) ∙∙∙, H ⑶,W('+1)), H('-1), W⑶)kF]
+ (L + 1)E[∣∣Vw 尹)(VH 尹+1)(∙ ∙ ∙ Vh f(L)(D(L+1), H(LT)W(L)) ∙∙∙, H('), W('+1)), H('-1), W⑶)
-	VW 尹)(VH 兴+1)(∙ ∙ ∙ Vh f(L)(D(L+1), H (LT)W(L)) ∙∙∙, H('), W('+1)), H('-1), W('))kF] + ∙∙∙
+ (L + 1)E[∣Vwf(')(VHf('+I)(D('+2), H(e), W('+1)), H('-1), W('))
-	VW尹)(VH尹+1)(D('+2), H(e), W('+1)),H('T), W('))kF]
+ (L + 1)E[∣Vwf(')(D('+1),H('-1), W(')) -VW产)(D('+1), H('-1), W('))kF]
≤ (L + 1)LWLHL-'T)E[∣D(L+1) - D(L+1)kF]
+ (L + 1)LWLHL-'-2)E[kVHf(L)(D(L+1),H(LT)W(L)) - Vhf(L)(D(L+1),H(LT)W(L))kF] + ∙∙∙
+ (L + 1)LWE[∣Vh尹+1)(D('+2), H('), W('+1)) -Vh尹+1)(D('+2), H⑶,W('+1))kF]
+ (L + 1)E[∣Vw尹)(D('+1),H('-1), W(')) -Vw尹)(D('+1),H('-1), W('))kF]
(90)
From the previous equation, we know that there are three key factors that will affect the variance:
•	The difference of gradient with respect to the last layer node representations
E[∣∣D(L+1) - D(L+1)kF]	(91)
31
Under review as a conference paper at ICLR 2021
•	The difference of gradient with respect to the input node embedding matrix at each graph
convolutional layer
E[kVH尹+1)(D('+2)e(`), W('+1)) - VHf('+1)(D('+2), 口⑶,W(`+1))kF]	(92)
•	The difference of gradient with respect to the weight matrix at each graph convolutional
layer
E[kVwf⑹(D('+1),用'T), W⑶)-VW/⑶(D('+1), H('T), W⑶)kF]	(93)
First, Let consider the upper-bound ofEq. 91.
E[kD(L+1) - D(L+1)kF] = E[k Wl产-也* AkF]
≤ LLSE[恒(L)- H(L)kF]
≤	L2ossE[∣∣σ(L(L)H(LT)W(L)) - bp')：H(LT)W(L))kF]	(94)
≤	LIOSSCBWE[kL(L)H(LT) - P(L)H(LT)kF]
≤	LjossCiBWvBj1E[kL(L) - P(L)kF]
Then, let consider the upper-bound of Eq. 92.
E[kVHf(')(D('+1), H('-1), W(')) - VH/(')(D('+1), H('-1), W('))]kF]
=E[k[L(')]>(D('+I)O σ0(L(')H('T)W(')))[W(')]> - [L]>(D('+1) o σ,(P0H('-1)W('))) [W(')]>kF]
≤ 2E[k[L(')]>(D('+1) o σ0(L(')H('T)W(')))[W(')]> - [L(')]>(D('+1) o σ0(p(')H('T)W(')))[W(')]>kF]
+ 2E[k [L(')]>(D('+1) o σ,(p(')H('T) W('))) [W(')]> - [L]τ(D('+1) o σ,(p(')H('T)W('))) [W(')]> kF]
≤ 2BLaBDbWL*E[kL(')H('T)- p(')H('τ)∣∣F] + 2BVciBWE[kL⑶-LkF]
≤ 2BLaBDbHbWL*E[kL⑶-P(')kF]+2BDdBWE[kL⑹一p(') + p(') - LkF]
≤ 2(BLABDBHBWL + 2BVcσBW)E[kL(') - P(')kF]+4B2CiBWE[kP(') - L∣∣F]
≤ O(E[ke⑶-p(')kF]) + O(E[kP(') - LkF])
(95)
Finally, let consider the upper-bound of Eq. 93.
E[kVWf(')(D('+1),H('-1),W(')) -VW/(')(D('+1),H('-1), W('))kF]
≤ E[k[L(')H('T)]T(D('+1) o σ,(L(')H('T)W('))) - [LH('T)]T(D('+1) o，田⑶m'T)W('))) kF]
≤ 2E[k[L(')H('T)]τ (D('+1) o σ,(L(')H('T)W('))) - [L(')H('T)]τ (D('+1) o σ,(p(')H('T)W(')))kF]
+ 2E[k[L⑶前'-1)]〒(D('+1) o σ,(p(')H('T)W('))) - [LH('-1)]T(D('+1) o σ'(P(')I!('-1)W('))) kF]
≤ 2BLABHBDBWLiE[kL(')H('T) - P(')H('-1)kF] + 2BVCiE[kL(')H('T) - LH('-1)kF]
≤ 2(BLABHBDBWL + BDC)E[kL⑶H('T)- P(')H('T)kF]
+ 2BVBHCiE[kP⑶-LkF]
≤ 2(BLABHBDBWL + BHBDC)E[kL(') - P(')kF]
+ 2BDBHCiE[kP⑶-LkF]
≤ O(E[ke⑶-p(')kF]) + O(E[kP(') - LkF])
(96)
32
Under review as a conference paper at ICLR 2021
Combining the result from Eq. 91, 92, 93 we have
e[∣∣G⑻-e[G(')]kF] ≤ o(e[∣∣L⑶-P叫F]) +... + o(e[∣∣L(L) -P(L)IIF])
+ O(E[∣∣P⑶-LkF]) + ... + O(E[kP(L)- LkF])
(97)
□
Lemma 3 (Upper-bound on bias). We can upper-bound the bias ofstochastic gradient in SGCN as
LL
XE[kE[G⑶]-G(')kF] ≤ XO(kP⑶-LkF)	(98)
'=1	'=1
Proof. By definition, we can write the bias of stochastic gradient in SGCN as
E[kE[忑⑶]-G⑶kF]
≤ E[∣∣Vw科)IyH产+1)(…VHf(L)(D(L+1), H(LT), W(L))…,H⑶,W('+1)), H('-1), W⑶)
-	VW f ⑶(VH f ('+1)(…VH f (L)(D(L+1), H(LT), W(L))..., H(‘), W('+1)), H('-1), W('))kF ]
≤ (L + 1)E[∣∣VwK(VHf('+1)(…VH f(L)(D(L+1), H(LT), W(L))…，H⑶,W('+1)), H('-1), W('))
-	VW 2(VH f('+1)(…VH f(L)(D(L+1), H(LT), W(L))..., H(‘), W('+1)), H('-1), W('))kF ]
+ (L + 1)E[∣∣Vw f(')(VH f('+1)(... VH f(L)(D(L+1), H (LT), W(L))…，H('), W('+1)), H ('-1), W('))
-	VW f ⑶(VH f('+1)(... VH f (L)(D(L+1), H(LT), W(L))..., H('), W('+1)), H('-1), W('))kF ] + ...
+ (L + 1)E[∣∣Vwfw(VHf('+1)(D('+2), H('), W('+1)), H ('-1), W('))
-	VW/(')(VHf ('+1)(D('+2), H(`), W('+1)), H ('-1), W⑶)kF]
+ (L + 1)E[∣∣Vwf(')(D('+1), H('-1), W(')) - VWf ⑶(D('+1), H('-1), W⑶)kF]
≤ (L + 1)LWLHLTT)E[kD(L+1) - D(L+1)kF]
+ (L + 1)LWLHL-'-2)E[kVHf(L)(D(L+1), H(LT), W(L)) - VHf (L)(D(L+1), H(LT), W(L))kF] + …
+ (L + 1)LWE[∣Vh尹+1)(D('+2), H('), W('+1)) - Vhf('+1)(D('+2), H('), W('+1))kF]
+ (L + 1)E[∣∣Vwf(')(D('+1), H ('-1), W(')) - VWf ⑶(D('+1), H('-1), W('))kF]
(99)
From the previous equation, we know that there are three key factors that will affect the bias:
•	The difference of gradient with respect to the last layer node representations
E[kD(L+1) - D(L+1)kF]	(100)
•	The difference of gradient with respect to the input node embedding matrix at each graph
convolutional layer
E[∣Vhf+1)(D('+2), H(`), W('+1)) -Vhf ('+I)(DC+2), H(`), W('+1))kF]	(101)
•	The difference of gradient with respect to the weight matrix at each graph convolutional
layer
E[∣Vwf')(D('+1),H ('-1),W(')) -Vwf⑶(D('+1),H('-1), W('))kF]	(102)
Firstly, let consider the upper-bound of Eq. 100.
E[kD(L+1) - D(L+1)kF] = E[kdLOS熏LL), y) - dLos凿LL),y) kF]	(103)
≤ L2ossE[kH(L)- H(L)kF]
33
Under review as a conference paper at ICLR 2021
The upper-bound for E[∣∣H(') - H(')∣∣∣] as
∣∣H⑶一H⑶kF = ∣σ(P⑶H('T)W⑶)-b(LH('T)W⑶)∣F
≤ CBW∣P⑶H(J) - LH(J) + LH('T)- LH(J)∣F
≤ 2CσBWBH∣P⑶-LkF + 2CσBWBLAkH(J)- H(J)IIF
≤O(∣P (I)- LkF) + ... + O(kP⑶-LkF)
Therefore, we have
E[kD(L+1) - D(L+1)kF] ≤ O(kP (I)- LkF) + ... + O(kP(L)- LkF)
(104)
(105)
Then, let consider the upper-bound of Eq. 101.
E[∣Vh/⑶(D('+1), H('-1), W(')) - VHf ⑶(D('+1), H('-1), W('))kF]
=E[∣[L]t(D('+1) ◦ σ'(P(')H-1)w(')” [WRI - [L]τ(D('+1) o σ,(LH('T)W('))) [W(')]>kF]
≤ BLABDBWLσE[kP⑶H('T) - LH('T) + LH('T) - LH('-1)kF]
≤ 2BLaBDBWLBHE[∣∣P⑶-LkF]+ 2B=aBDBWLE[||H('T)- H('-1)kF]
≤O(kP ⑴-LkF) +... + O(kP⑶一LkF)
(106)
Finally, let consider the upper-bound ofEq. 102.
E[∣Vwf(')(D('+1),H('-1),W(')) -Vwf⑶(D('+1),H('-1), W('))kF]
=E[∣[LH('T)]t(D('+1) o σ,(P(')H('T)W('))) - [LH('T)]τ(D('+1) o σ,(L(')H('-1)W('))) kF]
≤ 2E[k [LH('-1)]t(D('+1) o σ,(P(')H('T)W⑹))-[LH('-1)]τ(D('+1) o σ,(P(')II('-1) W⑹))kF]
+ 2E[k[LH('T)]T(D('+1) o σ,(P(')H('T)W('))) - [LH('-1)]t(D('+1) o σ0(L(')H('-1)W('))) kF]
≤ 2BDcσBLaE[∣∣H('T)- H('T)kF]
+ 2BLABHBDLBWE[kP⑶H('T) - LH('T) + LH(J) - LH('-1)kF]
≤ 2(bD C BLA + BLABH BD L BW )e[∣H(J)- HCT)kF ]
+ 2BLABHBDLBWE[kP⑶-LkF]
≤O(kP ⑴-LkF) +... + O(kP⑶一LkF)
(107)
Combining the result from Eq. 100, 101, 102 we have
E[kE[G ⑶]-G(')kF] ≤ O(E[kP ⑴-LkF]) + ... + O(E[kP(L)- L∣∣F])	(108)
□
G.2 Remaining steps toward Theorem 1
By the smoothness of L(θt), we have
L(θt+1) ≤ L(Ot) + hvL(θt), θt+ι - θti +—f ∣∣θt+ι - θt∣∣F
〜	2	〜	(109)
=L(θt) - ηhVL(θt), ve(θt)i + ητf kve(θt)kF
Let Ft = {{B(')}L=i,..., {bR}L=J. Note that the weight parameters θt is a function of history
of the generated random process and hence is random. Taking expectation on both sides condition
34
Under review as a conference paper at ICLR 2021
on Ft and using η < 1/Lf we have
E[L(θt+ι)∣Ft]
≤ L(θt) - ηhVL(θt), E[V](θt)∣Ft]i + η22Lf (E[kvZ(θt) - E[VΓ(θt)∣Ft]kF∣Ft] + E[kE[Vl(θt)∣Ft]kF|Ft]
=L(θt) - ηhVL(θt), VL(θt) + E[bt∣Ft]i + η22Lf (E[kntkF∣Ft] + kVL(θt) + E[bt|Ft]kF)
≤ L(θt) + 2 ( - 2hVL(θt), VL(θt) + E[bt∣Ft]i + kVL(θt) + E[bt|Ft]kF)+ *E[kntkFFt]
=L(θt) + 2 ( - kVL(θt)kF + E[∣∣btkF∣Ft]) + η22Lf E[kntkF∣Ft]
(110)
Denote ∆b as the upper bound of bias of stochasitc gradient as shown in Lemma 3 and Denote ∆n
as the upper bound of bias of stochastic gradient as shown in Lemma 2. Plugging in the upper bound
of bias and variance, taking expectation over Ft , and rearranging the term we have
E[kVL(θt)kF] ≤ 2 (E[L(θt)] - E[L(θt+ι)]) + ηLf∆ + ∆b
Summing up from t = 1 to T, rearranging we have
1T	2T
T fE[kVL(θt)kF] ≤ — E(E[L(θt)] - E[L(θt+ι)]) + ηLf∆n + ∆b
t=1	η t=1
2
≤ ~7^(L(θ1) - L(θ )) + ηLf ∆n + ∆b
(a) ηT
where the inequality (a) is due to L(θ?) ≤ E[L(θT+1)].
By selecting learning rate as η = 1/√T, We have
(111)
(112)
1T
T fE[kVL(θt)kF ] ≤
t=1
2(L(θι)-L(θ?))	Lf ∆n
--------------+ F + ∆b
Tt
(113)
35
Under review as a conference paper at ICLR 2021
H Proof of Theorem 2
H.1 Supporting lemmas
In the following lemma, we derive the upper-bound on the node embedding approximation error of
each GCN layer in SGCN+. This upper-bound plays an important role in the analysis of the upper-
bound of the bias term for the stochastic gradient. Suppose the input node embedding matrix for the
' GCN layer as H('_1), the forward propagation for the 'th layer in SGCN+ is defined as
产(H尸),W⑶)=IZF) := P⑶HF)W叫	(114)
and the forward propagation for the 'th layer in FullGCN is defined as
f ⑶(H 尸,W⑶)=LH t'T) W⑶	(115)
In the following, we derive the upper-bound of
E[k尹)(H('T), W⑶)-f ⑶(H('-1), W⑶)kF] = E[∣∣LHFT)Wy)- Z*IF]	(116)
Lemma 4. Let denote E ∈ [0, T/K — 1] as the current epoch, let t be the current step. Therefore,
for any t ∈ {EK +1,..., EK + K}, we have
E[∣∣LH('T)Wf)- Z僧∣F]
≤ η2K X o(∣E[∣∣P⑶kF] -IILkFI +... + ∣E[∣∣P⑴kF] -∣∣l∣∣F∣))
(117)
Proof.
kLHFT)Wf)- ZFkF
行('-l)^w(')	6 ('-1)^W(')	⅛ ('-1)^W(')	7(')	7(')	7(')	2
=k[LHt	Wt - LHt-I Wt-1] + [LHt-I Wt-I - Zt-I] - [Zt - Zt-1]kF
L H(2-1)W(')	L H('-1)W(') 2	L H('-1)W(')	Z(') 2	Z(')	Z(`) 2
一k LHt	W t - LHt-I	Wt-IkF 十 k LHt-I	W t-1	-	Zt-IkF 十 kZt	- Zt-IkF	(118)
7(g-i)^w(')	7(`-1)^w(')	7('-i)^w(')	7(')
+ 2hLHt	Wt - LHt-1 Wt-1, LHt-I Wt-1 - Zt-Ii
-2(LHt'T)Wy) - LHt-υW(∖, Z(')- Z(')1i
-2(LH储I)Wg-Za ZF)- Zgi
Recall that by the update rule, we have
Z⑶-Z⑶ =P(')H('-I)W⑶-P(')H('-I)W⑶	(119)
Zt	Zt-1 = p Ht Wt P Ht-1 Wt-I	(119)
and
EZ(')	Z(') F	L H('-1)W(')	L H('-1)W(')	120、
E[Zt - Zt-IIFt] = LHt Wt - LHt-i Wt-1	(120)
Taking expectation on both side condition on Ft, we have
E[kLH(J)Wy)- Z(')kF∣Ft] ≤ kLH匕I)Wa
-ZgkF + E[kZ F)- ZgkFIFt]
-kLH (J)Wy)
-LH 储 I)W2 kF
(121)
Then take the expectation over Ft, we have
E[kLH(J)Wy)- Z(')kF] ≤ kLH归I)Wg
-Z (-ikF + E[kZ F)- Z (-ιkF]
-kLH (J)Wy) - LH ('-I)W(%k2
t	t	t— 1	t— 1 F
(122)
36
Under review as a conference paper at ICLR 2021
Since we know t ∈ {EK + 1,..., EK + K}, we can denote t = EK + k, k ≤ K such that
E[kLHt'T)Wf)- Zt')kF]= E[kLH'^kW('K+fc - ZEK+kkF]
=E[kLHEKI)WEK - ZEKkF]
I
,
(A)
EK+K
+ X	(E[kZt') - Z(—)ιkF]
t=EK+1
Knowing that we are using all neighbors at the snapshot step (t
As a result, we have
E[kLHEκ+kWEK+k - zEK+k ∣∣F]
-kLH('T)Wy)- LH('-I)Wf)IkF
t	t	t — 1 t — Illr
(123)
mod K) = 0, we have (A) = 0.
EK+K
≤ X	(E[kZF)- Z2kF]-kLH
t=EK+1	|---------------
yτ)Wy)
^{^^^^^^^^≡
(B)
-LH匕I)WakF
(124)
Let take a closer look at term (B).
E[kZF)-%gkF] -kLH(J)Wy)
=E[kP⑶H尸)Wf)- P⑶H储I)
-LH 储I)
WgkF
W?kF] -kLHt'T)
Wy)
≤ E[kP⑶(H尸)Wf) - H储I)
Wg)kF] -kL(H(J)WF)
-LH 储I)WgkF
-H 储I)Wg)kF
(125)
(')	l∣2
—
≤
P(2)∣∣2] IlLu2、E∣^∣∣H('—1)W(') H('—1)W(') l∣2 1
P If]- kLkF) E[kHt Wt - Ht -1 Wt—1If]
/ X---------------V----------------'
(C)
Let take a closer look at term
e[∣h 尸)Wy)
(c).
-H 归I)W2 kF]
E[kH 尸)Wf)
—
H ('T)WA + H t'T)W(—1
—
H kI)WakF]
(126)
≤ 2BHE[kWF)
By induction, we have
e[∣h (`T)Wy)
-WgkF] + 2E[kH (J)W2
-H 归I)W2 kF]
-H 归I)W2 kF]
≤ 2BHE[kWf)- WAkF] + 22BHE[∣∣W('T)
+ 2'BH⅛[kWr)- W(1)1 kF]
-W 匕1)∣∣F] +...
(127)
By the update rule of weight matrices, we know
E[kWF)- WgkF] = η2E[kG2kF]
(128)
Therefore, we have
e[∣lh EK+k WEk+
EK+K
_ Z(')	∣∣2 ]
ZEK+k IIf]
≤ X η2θ(∣E[kP(')∣∣F] -kLkFl × E[kG(GIkF] +... + ∣E[kP⑴kF] - kLkFl ×
t=EK+1
(129)
By the definition of Gg, we have that
E[kG2ιkF] ≤ BLABHBDcσ
(130)
37
Under review as a conference paper at ICLR 2021
Plugging it back, we have
E[kLH('-1)W(') - Z(')kF]
：=E[kLHEκ+kW(EK+k -ZEK+kk2F]	(131)
≤ η2K X o(∣E[∣P')kF] -kLkF∣ +... + ∣E[kP⑴kF] -kLkF∣))
□
Based on the upper-bound of node embedding approximation error of each graph convolutional
layer, we derived the upper-bound on the bias of stochastic gradient in SGCN.
Lemma 5 (Upper-bound on bias). We can upper-bound the bias of stochastic gradient in SGCN+ as
LL
XE[kE[G(')] - G(')kF] ≤ η2KXθ(∣E[∣P')kF]-国图)	(132)
'=1	'=1
Proof. From the decomposition of bias as shown in previously in Eq. 108, we have
E[kE[G ⑶]—G⑶kF]
≤ (L + 1)LWLHl-'T)E[∣∣D(L+1)- D(L+1)kF]
+ (L + 1)LW LHL-'-2)E[∣∣VH /(L)(D(L+1), H(LT), W(L)) - VH f (L)(D(L+1), H(LT), W(L))|图十...
+ (L + i)lW E[kvH 尹+1)(d('+2), H('), w('+1))- VH f ('+1)(d('+2), h('), W('+1))kF]
+ (L + 1)E[kVwf(')(D('+1), H('-1), W(')) - VWf⑶(D('+1), H('-1), W('))||F]
(133)
From the previous equation, we know that there are three key factors that will affect the bias:
•	The difference of gradient with respect to the last layer node representations
E[kD(L+1) - D(L+1)|图	(134)
•	The difference of gradient with respect to the input node embedding matrix at each graph
convolutional layer
E[kVHf('+1)(D('+2), HR W('+1)) -VHf('+1)(D('+2), HQ W('+1))kF]	(135)
• The difference of gradient with respect to the weight matrix at each graph convolutional
layer
E[kVwf(')(D('+1),H('-1),W(')) -VWf⑶(D('+1),H('-1), W('))kF]	(136)
Firstly, let consider the upper-bound of Eq. 134.
E[kD (L+1) - D(L+1)∣∣F]=E[kd L喘 LL)，y) -dLosdHLL)，豆 kF]
≤ L2ossE[kH(L)-H(L)kF]	(137)
≤ L2ossCσ E[kZ(L) - Z(L) kF]
We Can decompose E[∣∣Z(L) - Z(L)kF] as
E[kZ(L) - Z(L)kF]
=E[kZ(L) - LH(LT)W(L)kF]
≤ 2E[kZ(L) - LH(LT)W(L)kF] + 2E[∣∣LH(LT)W(L) - LH(LT)W(L)|图
≤ 2E[kZ(L) - LH(LT)W(L)kF] + 2BLABWCσE[kZ(LT)- Z(LT)kF]	(138)
L
≤ X ο(E[kZ(')- lH('t)w(')∣∣F ])
'=1
38
Under review as a conference paper at ICLR 2021
Using result from Lemma 4, we have
L
E[∣∣D5 - D(L包陶 ≤ Xη2K × θ(∣E[∣∣PqF] -IlLkF|)
2=1
(139)
Then, let consider the upper-bound of Eq. 135.
E[∣VH 严(D('+1), H(J), W⑶)-VH f ⑶(D('+1), H(J), W ⑶)|图
=E[∣∣[L]>(D('+I)O σ'(P⑶H('T)W⑶))[W⑶]> -[L]τ (D('+1) o b0(LH('T)W⑶))[W⑶]>∣∣F]
≤ BLABDBWLlE[∣ZF)- LHCT)WC)IIF]
≤ 2BLaBDBWLlE[∣ZF)- LHCT)WqF]
+ 2BLABDBWLl E[∣∣LH(J)W⑶-LH('-1)W(')kF]
、------------V--------------}
(A)
(140)
where Z(` = ZR + P(')HL)Wy) - P(')H,> W；，.
t	t—1	t	t	t—1	t—1
Let take a closer look at term (A), we have
E[∣∣LH(`t)w(') - LHrT)W(')∣∣F] ≤ BLABWCE[∣Z('-2) - LH('-2)W('-1)kF]
≤ 2BLABWCle[∣z('-2) - lH('-2)w('T)∣F]
+ 2BLABWClE[∣LH('-2)W('T) - LH('-2)W('-1)|F]
(141)
Therefore, by induction we have
E[∣VHf(')(D('+1), H('T), W(')) - VHf(')(D('+1), H('-1), W('))∣F]
≤ O(E[∣Z(')- LH('T)W(')∣F]) + O(E[∣Z('T) - LH ('-2)W('-1) ∣∣F]) + ...	(142)
+ O(E[∣Z⑵-LH(I)W(2)kF]) + O(E[∣Z(I) - LXW(I)|图)
Using result from Lemma 4, we have
E[∣VHf(`(D('+1), H('-1), W(')) - VHf(')(D('+1), H('-1), W('))kF]
≤ η2K X O(∣E[∣PqF] -∣L∣F∣) + ... + η2K X θ(∣E[∣P(I)kF] -∣L∣F∣)
(143)
Finally, let consider the upper-bound ofEq. 136.
e[∣vw /(')(d('+1), H('-1), w(')) - VW f (')(d('+1), h('-1), w('))∣F]
=E[k [LH('T)]τ(D('+1) O σ0(Z('))) - [LH('T)]τ(D('+1) o σ0(L(')H('-1)W('))) ∣F]
≤ 2E[∣ [LH('T)]τ(D('+1) o σ0(Z('))) - [LH('-1)]τ(D('+1) o σ0(Z⑶))|图
+ 2E[k[LH('T)]T(D('+1) o σ0(Z⑹))-[LH('-1)]τ(D('+1) o σ0(L(')H('-1)W(')))∣F]
≤ 2BDClBLA e[∣h('T) - H('T)kF] +4BLABHBDLE[∣Z⑶-LH('-1)W(')∣F]
`---------------------V--------}
(B)
+ 4BL a BH BD L E[∣∣LH ('T)W⑶-LH('T)W(')∣∣F ]
(144)
By definition, we can write the term (B) as
E[|H('T)- H('T)kF] ≤ ClE[∣Z⑶-LH('-2)W('-1)∣F]
≤ 2ClE[∣Z(')- LH('-2)W('T)∣F]	(145)
+ 2ClE[∣LH('-2)W(')- LH('-2)W('-1)|F]
39
Under review as a conference paper at ICLR 2021
Plugging term (B) back and using Eq. 141 and Lemma 4, we have
E[kVW/⑶(D('+1), H('T), W⑶)-VWf (')(D('+1), H('-1), W⑶)kF]
≤ η2κ X o(∣E[∣P')kF] -IILkFl)+... + η2κ X o(∣e[∣∣p⑴kF]-内图)
Combining the result from Eq. 134, 135, 136 we have
L
E[kE[G⑶]-G⑹kF] ≤ η2KXθ(∣E[∣∣P⑶kF] -kLkF|)
'=1
(146)
(147)
□
H.2 Remaining steps toward Theorem 2
Now we are ready to prove Theorem 2. By the smoothness of L(θt), we have
L(θt+ι) ≤ L(θt) + hVL(θt), θt+ι - θti + LFkθt+ι - θtk2
=L(θt) - ηhVL(θt), Ve(θt)i + η22LFkVL(θt)k2
Let Ft = {{B(')}L=ι,..., {B(1)ι}L=ι}∙ Note that the weight parameters θt is a function of history
of the generated random process and hence is random. Taking expectation on both sides condition
on Ft and using η < 1/LF we have
E[VL(θt+ι)∣Ft]
≤ L(θt) - ηhVL(θt), E[VΓ(θt)∣Ft]i + η22LF (E[kVΓ(θt) - E[VΓ(θt)∣Ft]k2∣Ft]+ E[∣∣E[g∣Ft]k2∣Ft]
L(θt) - ηhVL(θt), VL(θt) + E[bt|Ft]i +
η22LF (E[kntk2∣Ft] + kVL(θt) + E[bt∣Ft]k2
≤ L(θt) + η ( - 2hVL(θt), VL(θt) + E[bt∣Ft]i + ∣∣VL(θt) + E[b|"『)+ η22LFE[kntk2F
≤ L(θt) + 2 ( - kVL(θt)k2 + E[kbtk2∣Ft]) + η22LFE[kntk2∣Ft]
(149)
Plugging in the upper bound of bias and variance, taking expectation over Ft, and rearranging the
term we have
E[kVL(θt)k2] ≤ 2 (E[L(θt)] - E[VL(θt+ι)]) + ηLf∆n + η2∆+0
(150)
Summing up from t = 1 to T , rearranging we have
1T	2T
T fE[kVL(θt)k2] ≤ — E(E[L(θt)] - E[L(θt+ι)]) + ηLf∆n + η2∆+0
t=1	η t=1	(151)
2
≤ —(L(θ1) - L(θ )) + ηLF∆n + η δJ
(a) ηT	b
where the inequality (a) is due to L(θ?) ≤ E[L(θT+1)].
By selecting learning rate as η = 1/√T, we have
ɪ XE[kVL(θt)k2] ≤ 2≡⅛L≡ + 悖 + 率	(152)
T t=1	T	T T
40
Under review as a conference paper at ICLR 2021
I	Proof of Theorem 3
I.1	Supporting lemmas
In the following lemma, we decompose the mean-square error of stochastic gradient at the 'th layer
E[∣∣G(')- G(')||F2] as the summation of
•	The difference between the gradient with respect to the last layer node embedding matrix
E[∣∣D(L+1) - D(L+1)kF]	(153)
•	The difference of gradient passing from the (' + 1)th layer node embedding to the 'th layer
node embedding
E[∣∣Vh尹+1)(D('+2),苴⑶,W('+1)) -VHf ('+1)(D('+2), H('), W('+1))kF]	(154)
• The difference of gradient passing from the 'th layer node embedding to the 'th layer
weight matrix
E[kVw尹)(D('+1), H('-1), W⑶)-VWf⑶(D('+1), H('-1), W⑶)∣∣F]	(155)
Lemma 6. The mean-square error ofstochastic gradient at the 'th layer can be decomposed as
E[∣∣G⑶-G⑶∣∣F2]
≤ O(E[∣∣D(L+1) - D(L+1)kF])
+ O(E[∣∣VH f(L)(D(L+1), H (LT)W(L)) - Vh f(L) (D(L+1), H(LT)W(L))kF]) + ...
+ O(E[∣∣ VHf('+I)(D('+2), H('), W('+1)) -Vhf ('+1)(D('+2), H('), W('+1))kF])
+ O(E[∣∣Vwf⑶(D('+1),H('-1),W(')) -VWf⑶(D('+1),H('-1),W⑹)kF])
(156)
Proof. By definition, we can write down the mean-square error of stochastic gradient as
E[kG⑶-G⑶kF]
=e[∣∣vw f')(VH f('+1)(... vh f(L)(D(L+1), H (LT)W(L))..., H('), w('+1)), H ('-1), W⑶)
-VWf⑶(VHf('+1)(…Vhf (L)(D(L+1), H(LT), W(L))…,H(`), W('+1)), H('-1)), W⑶kF]
≤ (L + 1)E[∣∣Vw 产(VH f('+1)(... Vh f(L) (D(L+1), H(LT)W(L))…，H('), W('+1)), H('-1), W⑶)
-VW fw(VH 兴+1)(... Vh f(L)(D(L+1), H(LT)W(L))…,H('), W('+1)), H('-1), W⑶)kF]
+ (L + 1)E[∣∣Vw f(')(VH f('+1)(... Vh f(L)(D(L+1), H(LT)W(L))..., H('), W('+1)), H('-1), W⑶)
-VW fw(VH f('+1)(... Vh f (L)(D(L+1), H(LT)W(L))…,H('), W('+1)), H('-1), W('))kF] + …
+ (L + 1)E[kVWf(')(VHf('+I)(D('+2), H('), W('+1)), H('-1), W('))
-VWfw(VHf ('+1)(D('+2), H('), W('+1)),H('t), W('))kF]
+ (L + 1)E[∣∣Vwf(')(D('+1), H('-1), W(')) - VWf(')(D('+1), H('-1), W⑶)kF]
≤ O(E[kD(L+1) - D(L+1)kF])
+ O(E[k VHf(L)(D(L+1), H(LT)W(L)) - Vhf (L)(D(L+1), H(LT)W(L))kF]) + …
+ O(E[k VHf('+I)(D('+2),HQ W('+1))- Vhf('+1)(D('+2), H('), W('+1))kF])
+ O(E[∣∣Vwf⑶(D('+1), H('-1), W(')) -Vwf⑶(D('+1), H('-1), W⑶)kF])
(157)
□
(2)、
Recall the definition of stochastic gradient for all model parameters VL(仇)={Gt 73 where
(`)
Gt，is the gradient for the 'th weight matrix, i.e.,
Wf) = WrI- ηGt-1	(158)
41
Under review as a conference paper at ICLR 2021
In the following lemma, we derive the upper-bound on the difference of the gradient passing from
the 'th to (' - 1)th layer given the same inputs D('+1), H('-1), where the backward propagation
for the 'th layer in SGCN++ is defined as
VH 产(d('+1),H 尸),Wf))
=D3 + [L(叩(d£+1) O σ0(ZF)))Wy) - [L(')]T(D(") O σ0(Z3))Wg]
and the backward propagation for the 'th layer in FullGCN is defined as
Vhf ⑶(dy+1), H(`t), Wy))= L>(D('+1) o σ0(ZF)))W(')
(159)
(160)
Lemma 7. Let suppose t ∈ {EK + 1,..., EK + K}. The upper-bound on the difference of the
gradient with respect to the input node embedding matrix at the 'th graph convolutional layer given
the same input D，+1) and HFT) is defined as
E[HVhf')(D,+1), H('T, Wc))- Vhf(')(D('+1), H('-1), WC))|阂
tj- t	t t t t t t	t t t t t t t F
eK+k „	/	„ λ	(161)
≤ X η2θ(∣E[∣∣LC)∣∣F] -IlLkF∣× E[kVL(仇-1幅)
t=EK+1
Proof. To simplify the presentation, let US denote Df) = VHf(')(D('+1), HfT), Wy)). Then,
by definition we have
D('):= Da + [L(')]T(D('+1) O σ0(Z(')))Wy)- [L(')]T(DT) O σ0(Zt-1))W(-1	(162)
Therefore, we know that
IlLT(Dt'+1) o σ0(Z(')))Wf - D*IF
=k[LT(Dt'+1) o σ0(Zt')))Wf)- LT(DT) ◦ ^①公心㈡
+ [Lτ(D叶I) o σ0(Zg))W(-1 - D3] - [D(') - D(-1]kF
≤ k LT(Dt'+1) o σ0(Z(')))Wy)-LT(D(")o σ0(Z2))W(-1 ∣∣F
X---------------------------------------------/
(AI )
+ k lt(d叶I) oσ0(Z3))W(-1 - Dt-1 kF + k D(') - D(-1 kF
X------------------------------' X------------'
-z
A2
+ 2hA1,A2i - 2〈A1,A3〉一 202,43)
{z^
A3
(163)
Taking expectation condition on Ft on both side, and using the fact that
e[D(')- D(-1∣Ft]
=E[[L(')]t(d('+I) o σ0(Z(')))W(') - L')]T(D(")o σ'(Z(-1))W(-1 ∣Ft]	(164)
=Lτ(D('+1) o σ0(Z(')))W(') - LT(D(")o σ0(Z(')1))W(')1
the following inequality holds
E[∣Lτ(D('+1) o σ0(Zt')))W(') - D(')kF|Ft]
≤ kLT(D(")o σ0(Z3))W(') - DgkF +E[kD(')- Dg∣∣F∣Ft]	(165)
-∣∣Lτ(Dt'+1) o σ0(Z(')))W(')-LT(D(")o σ0(Z2))W(-1kF
Then, taking expectation over Ft, we have
E[kLτ(D('+1) o σ0(Z(')))W(') - D(')kF]
≤ E[kLT(D叶I) o σ0(Z3))Wg - Dt-IkF] + E[∣∣Dt') - D(-1kF]	(166)
-[kLτ(Dt'+1) o σ0(Z(')))Wy)-LT(D(")o σ0(Z(-)1 ))W(')1kF]
42
Under review as a conference paper at ICLR 2021
Let suppose t ∈ {EK +1,..., EK + K}. Then we can denote t = EK + k for some k ≤ K such
that
E[kLτ(D('+1) o σ0(ZF)))Wf)- D功图
E[∣∣L>(D媒％ o σ0(Z队"WK+k- D‰fckF]
EKKK
≤ E[∣∣LT(D(IKI) oσ0(Z乳))W吼-D乳kF]+ X	(E[kD，')-D2常
t=EK+1
-[kLT(D('+1) o σ0(ZF)))Wf)-LT(D(-11) o σ0(Z&心㈡阂)
≤ E[kLT(DEKI) o σ0(ZEK))WEK - DEKkF]
EK+K
+ X	(E[k[LC)]τ(Df+1) o σ0(ZF)))Wy) - [L⑶]T(D叶I) o σ0(Z3))Wg∣∣F]
t=EK+1
-E[kLT(Df+1) o σ0(ZF)))Wy)- LT(Dt-1I)。σ0(Z2))WAkF])
(167)
Knowing that we are taking full-batch gradient descent when (t mod K) = 0, we have
E[kLT(Dt'+1) o σ0(ZF)))Wy)- D舟图
EK+K
≤ X	(∣E[kLR∣F] -kLkFl) × E[k(D尸I) o σ0(ZF)))Wy)-(Dt”) o σ0(Z2))WAkF]
t=EK+1、	X---------------------V----------------------'
(B)
(168)
Let take closer look at term (B).
E[k(D('+1) o σ0(ZF)))WF)-(D叶I) o σ0(Zg))WgkF]
≤ 3CσBW kD('+1)- D叶 1)∣∣F +3BDBWL E[∣∣ZF)- Z(-/^+3^BDE[kW(') - WgkF]
'------V-------'	'-----V-------'
(。1)	(。2)
(169)
For term (C1) by definition we know
kD('+1)- D(")kF
=k (LT(D('+2) o σ0(Z('+I)))W") - (LT(D叶2) o σ0(Z(-V)))W叶I)) kF
≤ 3k (LT(D('+2) o σ0(Z('+I)))Wy+1)) - (LT(D(32) o σ'(Z('+I)))Wy+1)) kF
-I-	311(T T (d('+2) C ∕τ' (Z('+1)) [W('+1))	(TT(d('+2) C ∕τ' (Z('+1)) [W('+I))U2
+ 3k L (Dt-1 oσ (Zt ))Wt	- L (Dt-1 oσ (Zt-1	))Wt kF
J 3U(TT(d('+2) C ∕τ'(Z('+1))[W('+1))	(TT(d('+2) C ∕τ'(Z('+1))[W('+I))U2
+ 3k L (Dt-1 oσ (Zt-1 ))Wt	- L (Dt-1 oσ (Zt-1	))Wt-1 kF
V O ||D('+2) D('+2)l|2] _|_ o( Il z('+1)	z('+1)∣∣2] ∣ o/||W('+1)	W('+1)l|2]
≤ O(kDt	- Dt-IkF) + O(kZt	- Zt-1 l∣F) + O(k Wt - Wt-I kF)
(170)
By induction, we have
∣∣D(1+1) d('+1) Il 2 < O( IID(L+1) D(L+I)U2、∣ o( ∣∣ z(1+1) z('+1) ∣∣2 ∖ ∣	_|_o“ IZ(L) Z(L)Il2]
kDt	- Dt-1	IIf ≤o(IlDt	- Dt-1	kF) + O(kZt	-	Zt-IkF) + …+ O(kZt	-	Zt-IkF)
`----------V-----------' `---------V---------'
(D1)	(D2)
+ o(kW('+1)- W(")kF) +... + o(kW(L)- W(L)IkF)
(171)
43
Under review as a conference paper at ICLR 2021
For term (D1) we have
kDtL+1)- D(III)kF = k
∂ L(θt) ∂ L(θt-ι)
----;----------;-:-
----，-、--------，-、
∂w(L)	∂w(L1
kF
(172)
≤ LOSS cσ kz(L)- z(L1kF
For term (D2) we have
kz/I)- Z叶I)kF ≤ cσkLH(')wf+I)-LHgW!")kF
≤ C2BLAkHf)w('+1) - H(')1 w('+1) + H(')1 w('+1) - H(')1 w('+1)kF
σ LA t t	t—1	t	t—1 t	t — 1	t—1 IIF
≤ 20BLAkHf)W('+I)-Ht—1W('+1)kF + 2CσBLAkHgW('+I)-HgW"kF
≤ 2CBLABWkz. - z(—1 kF + 2cσBLABHIW-1)- W"kF
(173)
By induction we have
kZ('+1)- ZT)kF ≤ O(kW('+1)- W"kF) + ... + O(kW(I)- W(—)1kF)	(174)
For term (C2) by definition we have
E[kZ衿-ZgkF] = E[kL⑶H (J)Wf)- L C)H 储 I)WgkF]
≤ 2BLaBWcσE[kZt'T)- Z匕I)∣∣F]+2BLaBHE[kWf)- WakF]
(175)
By induction we have
E[kZ F)- Z ,kF] ≤ O(E[kWf)- WAkF]) + ... + O(E[kW(I)- W(—)1kF])	(176)
Plugging (D1), (D2) back to (C1) and (C1), (C2), (C3) back to (B), we have
E[k(D('+1) ◦ σ0(ZF)))Wf)-(DT) 0 σ0(Z㈡即2 |图
EK+K
≤ X	(θ(E[kW(I)- W(—)1kF]) + ... + O(E[kW(L)- W(L1kF]))
t=EK+1	t	t—1 F	t	t—1 F	(177)
EK+K
=X η2o(E[kv2®—1)kF])
t=EK+1
Then plugging term (B) back to Eq. 169 we conclude the proof.
□
Using the previous lemma, we provide the upper-bound of Eq. 154, which is one of the three key
factors that affect the mean-square error of stochastic gradient at the 'th layer.
Lemma 8. Let suppose t ∈ {EK + 1,..., EK + K}. The upper-bound on the difference of the
gradient with respect to the input node embedding matrix at the 'th graph convolutional layer given
the same input Dt'+1) but different input H('—1), H('—1) is defined as
E[∣Vh产(D，+1), H('-1), Wy))- VHf⑶(D，+1), H('-1), Wy))kF]
EK+K
≤ X	η2o(∣E[kL⑶kF] -kLkFl× E[kvL®—1)kF])
t=EK+1
(178)
44
Under review as a conference paper at ICLR 2021
Proof. For the gradient w.r.t. the node embedding matrices, we have
E[∣Nhf(')(D('+1),H('-1), Wy))- VHf⑶(D('+1),h(`t), Wy))kF]
t	t	t	t	t	t	Γ,
=E[k (D3 + 吁叩(D('+1) O σ0(ZF)))Wy) - [LC)]T(D*) o σ'(Zg))wg])
-(LT(Df+1) o σ0(Zf)))Wy)) kF]
≤ 2 E[k(Dg + IL⑶]T(D，'+1) o σ0(ZF)))W『-[L⑶]T(D叶I) o σ0(Z2))W㈡)
S------------------------------V---------------------------------'
(A)
-(LT(D('+1) o σ0(ZF)))Wy)) kF]
、--------------V--------------Z
(A)
+ 2E[k(Lτ(D-I) o σ0(ZF)))Wy))-(LT(Dt'+1) o σ0(Zf)))Wy)) [商
V---------------------------------V-----------------------Z
(B)
(179)
Let first take a closer look at term (A). Let suppose t ∈ {EK + 1,..., EK + K}, where E = t
mod K is the current epoch number and K is the inner-loop size. By the previous lemma, term (A)
can be bounded by
ek+k
(A) ≤ X η2θ(∣E[厄⑶kF] -kLkFl× E[kVE(θ-)kF])	(180)
t=EK+1
Then we take a closer look at term (B).
E[k(LT(D('+1) o σ0(ZF)))Wy))-(LT(D/I) o σ0(ZF)))Wf)) kF]
≤ BLABD BW L E[kZ F)- Z(')kF]	(181)
X------V------Z
(C)
The term (C) can be decomposed as
e[∣∣Z F)- Z 舟 ∣F]
=E[kZF)- LH(J)W(')kF]
7(')	e('-1)w^(') Il2	e('-1)AV，'，	('-1)^W(') l∣2 I
≤ 2E[kZt - LHt	wt kF]+2E[kLHt	Wt - LHt	Wt IIf]
≤ 2E[kZF)-LH(J)Wy)kF] + 2BLaBWCE[kZFT)- ZfT)kF]
(182)
By induction, we have
E[kZF)-Zf)kF] ≤o(E[kZF)-LHHfT)Wy)kF])+...+o(e[∣∣Z,t1)-LXW(I)kF]) (183)
X{
(D)
The upper-bound for term (D) is similar to one we have in the proof of Lemma 4.
kLH FT)Wf)- Z t')kF
7 (g-1)^w(')	e('-I)W(')	H'('-1)^W(')	7(')	7(')	7(') 2
=k[LHt Wt - LHt-1 Wt-1] + [LHt-I Wt-1 - Zt-1] - [Zt - Zt-1]kF
τ H('-1)W(')	T H('-1)W(') 2 t H('-1)W(')	7(`) 2	7(')	7(') 2 (184)
一k LHt	Wt - LHt-1 Wt-IkF 十 kLHt-I W t-1 - Zt-IkF 十 kZt - Zt-IkF (184)
+ 2<lH('T)Wl), lH储I)Wgi - 2(lHt'T)Wy), Zt') - Z3)
-2hZ F)- Z a LH 匕 I)WAi
Taking expectation condition on Ft and using
ElZ(`) 7(') F	τ H(`-1)w(`) τ H('-1)w(')
EIZt - Zt-IIFt] = LHt Wt - LHt-i Wt-1
(185)
45
Under review as a conference paper at ICLR 2021
we have
E[∣∣lH(J)WF- Z僧图Ft]
_ IlLH('-1)w⑶ 7⑶ Il2 -I- FΓIIe⑶ Z⑶ Il2 IFl IlLH('-1)w⑹ LH('-1)w⑶ Il2
一IILHt-I	Wt-I	-	Zt-IkF + E[kZt	- Zt-IkFIFt]	-IILHt	Wt - LHt-I	Wt-IkF
(186)
Take expectation over Ft we have
e[∣∣lH (J)Wy)- Z 舟图
_ IITR('-D^w⑶	7⑶ l∣2 I ιpr∣∣ 7(')	7(') Il2 ^∣	∣∣τ7('-D^w⑹	T 7(`-1)^W(') Il2
-kLHt-I Wt-I - Zt-IkF + E[kZt - Zt-1kF]-kLHt	Wt - LHt-I Wt-IkF
(187)
Let suppose t ∈ {EK +1,..., EK + K}. Then we can denote t = EK + k for some k ≤ K such
that
E[kLH t'T)Wf)- Z t')kF]
=E[kLHEK+kW 2+k- Z EK+® kF]
EK+K
—∣∣T 7 (Z-I)W"⑶	7 (Z) Il 2 I ∖、 jpr∣∣ 7 (Z)	7 (Z) ∣∣2^∣	∣∣τ7 (Z-I)W"(Z)	T 7 (Z-I)W(Z) Il 2
一 HlhEK WEK - ZEK kF ÷ 匚 E[kZt - Zt-1kF]-kLHt	Wt - LHt-1 Wt-IkF
t=EK+1
EK+K
≤ X o(∣E[kL(Z)kF]-kLkFI×E[kw(Z)-W(Z)IkF])
t=EK+1
(188)
Plugging (D) to (C) and (C) to (B), we have
Z EK+K
(B) ≤ X X o(∣E[kL(j)kF] -kLkF∣ × E[kWj)- w2LkF])	(189)
j=1 t=EK+1
Combing with term (A) we have
E[∣∣Vh产(DtZ+1), H(ZT), W(Z))- VHf(Z)(DtZ+1), H(ZT), W(Z))kF]
L EK+K
≤ XX o(∣E[kL(Z)kF] -kLkF∣ × E[kw(Z)- W(Z)IkF])
Z=1 t=EK+1
EK+K
= X η2o(∣E[kL(Z)kF] -kLkF∣ × E[kvZ(θt-ι)kF])
t=EK+1
(190)
□
In the following lemma, we derive the upper-bound on the difference of the gradient with respect to
the weight matrix at each graph convolutional layer. Suppose the input node embedding matrix for
the 'th GCN layer is defined as H (ZT),the gradient calculated for the 'th weight matrix in SGCN+ +
is defined as
VW 严(D(z+1),H (ZT), W(Z))
=G(Z)I ÷ IL(Z)HtZ-1)]T(D(Z+1) O σ0(Z(Z)))- [L(Z)H匕 1)]T(Dt31) O σ0(Z(Z)I))
and the backward propagation for the 'th layer in FullGCN is defined as
VWf(Z)(DtZ+1), H(ZT), W(Z)) = [lH(ZT)]t(d(z+I) O σ0(Z(Z)))	(192)
Lemma 9. Let suppose t ∈ {EK ÷ 1,..., EK ÷ K}. The upper-bound on the difference of the
gradient with respect to the 'th graph convolutional layer g^ven the same input D(Z+1) and H(Z-I)
46
Under review as a conference paper at ICLR 2021
is defined as
IlVW尹)(D('+1),H，T), WF)-VWf (')(D('+1), Hh，t), WF)IIF
EK+K
≤ X η2o(∣E[IwkF] -IlLkF∣× E[∣∣vZ(θ"∣∣F])
t=EK+1
(193)
Proof. To simplify the presentation, let US denote GF) = VWf ⑶(D,+ 1∖ HF-1), Wy)). Then,
by definition, we have
GF) = G(-1 + [L(')H'T)]T(D('+1) O σ0②为)-[L⑶H储I)]T(D(3I) O σ0(Z(-ι)) (194)
Therefore, we know that
∣∣[LHt'T)]τ(D('+1) O σ0(Z('))) - Gf)IlF
=W[LHt'T)]τ(D('+1) O σ0(Z('))) - [LH匕1)]T(D(-V)O σ0(Z2))]
I hL HH('-1)^∣τ「d('+I)C ∏j(Z(')口 G(')i l^G(') G(')i l∣2
+ [[LHt-1 ] (Dt-I o σ (Zt-I))- Gt-1] - [Gt - Gt-1] Up
≤ Il [LH (J)]τ(Dt'+1) O σ0(Z F)))- [LH 储I)]τ(D¾υ O σ0(Z (-J), ||：
X---------------------------/ F
(195)
{z
(A1)
+ k[LH储1)]>(D(-V) o σ ②-I))- G
X------------------------------
^{z
A2
(-1 IlF +1G (')- G(-1 IIF
—} x----------V----}
A3
+ 2〈A1,A2〉一 2〈A1,A3〉一 202,43)
Taking expectation condition on Ft on both side, and using the fact that
E[G(') - G (-1∣Ft] = E[[e(')H (J)]τ (D('+1) O σ0(Z F))) - [L(')H 储I) ]τ(D 叶I) o σ0(Z g))∣Ft ]
=[LHt'-1)]T(Dte+1) o σ0(Zt'))) - [LH3n]T(DT) o σ0(Z2))
(196)
we have the following inequality holds
E[∣∣[lHt')]τ(D('+1) Oσ0(Z(J)))- GF)IIF∣Ft]
≤ I[LH储1)]T(Dt”) oσ0(Zt-1)) - G3∣∣f + E[∣∣G(') - G(-1IF∣Ft]	(197)
-I[lH尸)]τ(Df+1) O σ0(ZF)))- [LH匕1)]T(D(")O σ0(Z2))WaIF
Then, taking expectation over Ft, we have
E[∣∣[lH尸)]τ(D('+1) O σ0(ZF)))- G河图
≤ e[I[lH储1)]T(D(")oσ0(Zt-1))W(-1 - Gt-1IF] + E[∣∣G(') - G(-1IF]	(198)
Γ∣∣ L HH('-1)]T (D('+I)C H(Z⑶口	l^τH('-1)]TrD('+I)C H(Z(') '1'1∣∣21
- [I[LHt	] (Dt Oσ (Zt )) - [LHt-1 ] (Dt-1 Oσ (Zt-1))IF]
47
Under review as a conference paper at ICLR 2021
Let suppose t ∈ {EK +1,..., EK + K}. Then we can denote t = EK + k for some k ≤ K such
that
E[∣∣ [lH('-1)]τ(D('+1) o σ0(ZF)))- Gf)kF]
fγii rτ ft('-[)^]T(n('+1) C zτ"e⑶	、、 e(') ι∣21
E[k[LHEK+k] (DEK+k 0 σ (ZEK+k)) - GEK+k IIf]
EK+K
≤ E[k[LHE1K1)]τ (DEKI) 0 σ0(Z EIK))- G EK kF]+	X(E[kG F)- G&F]
t=EK+1
-[k[LH尸)]τ(D('+1) 0 σ0(ZF)))- [LH归°]T(D叶I) o σ0(Z?))|图)
≤ E[kLT(DEKI) 0 σ0(ZEIK))WEK - DEIKkF]
EK+K
+ X (E[k[M)H尸)]τ(D尸 o σ0(ZC))) - [L⑶H储 1)]T(D叶I) o σ0(Z3))|阂
t=EK+1
EI^HL H('-1)]T(d('+I) C H(Z(')}}	ΓL H('-1)]T(d('+I) C H(Z⑶ '1'1ll2 Λ
-E[HLHt	] (Dt	o σ (Zt )) - [LHt-I ] (Dt-I o σ (Zt-I))kF]J
(199)
Knowing that we are taking full-batch gradient descent when (t mod K) = 0, we have
E[kLT(Dt'+1) o σ0(ZF)))Wy)- D舟图
EK+K
≤ X (∣E[kL⑶kF] -kLkFl)× E[k[Ht'-1)]T(D尸I) o σ0(ZF)))- [H忆I)]t(dT) o σ0(Z2))|同
t=EK+1	X---------------------------V----------------------------'
(B)
(200)
Let take closer look at term (B).
E∣ΨI^H('-1)]TrD('+I)C H(Z⑶口	∣^H('-1)]TrD('+I)C H(Z(')'1'1∣∣2l
E[k[Ht	] (Dt o σ (Zt )) - [Ht-I ] (Dt-I o σ (Zt-I))kF]
≤ 3(BDcσ + BHBDL)E[kZt'T)- Zt-υkF]+3BHC E[kD('+1)- D(-υ∣∣F]	(201)
x-------V---------' x-------------------V------'
(C1)	(C2)
For term (Ci) by definition we know
∣∣D('+1) d('+1) Il2 — Il/LT/D('+2) c H<z('+1)>]W('+1)、 (LT(D('+2) c H/z('+1)]]W('+1)、U2
kDt	- Dt-1 kF = k L	(Dt	oσ	(Zt	))Wt	- L (Dt-1 oσ	(Zt-1	))Wt-1	kF
≤ 3k (LT(D('+2) o σ0(Z('+I)))W('+1)) -(LT(D(-+2) o σ0(Z('+1)))W('+1)) kF
-I-	3U/LT(d('+2) C ∕τz(Z('+1))[W('+1))	(LT(d('+2) C H<Z('+1)A[W('+1)) l∣2
+ 3k L (Dt-1 oσ (Zt ))Wt	- L	(Dt-1 oσ (Zt-1	))Wt kF
-I-	3U(LT(d('+2) C ∕τz(Z('+1))[W('+1))	(L T(d('+2) C H<Z('+1)A[W('+1)) l∣2
+ 3k L (Dt-1 oσ (Zt-1 ))Wt	- L	(Dt-1 oσ (Zt-1	))Wt-1 kF
VO(IlD('+2) d('+2) l∣2 si _i_ O( ||z('+1)	z('+1)∣∣2∖ ∣ o/||W('+1)	w('+1) l∣2 ʌ
≤ O(kDt	- Dt-IkF) + O(HZt	- Zt-IkF) + O(k Wt - wt-1 kF)
(202)
By induction, we have
||D('+1)	d('+1) Il 2 < O(||D(L+1)	D(L+I)U2) ∣ O(Il z('+1)	z('+1) ∣∣2 ∖ ∣	∣ o(||Z(l)	Z(L)Il2、
kDt	- Dt-1 IIf ≤ O(IlDt	- Dt-1 l∣F) + O(MZt - Zt-IkF) + …+ O(HZt	- Zt-IkF)
X} X}
(D1)	(D2)
I O(Ilw('+1)	w('+1) l∣2 ʌ -L _L O(IlW(L)	W(L)U2、
+ O(kWt	- Wt-1 kF) + . . . + O(kWt	- Wt-1kF)
(203)
For term (D1) we have
kDtL+1)- D(-+1)kF = k ∂L⅛)
∂Wt ,
∂ L(θt-1), ,2
-IWLrkF
≤ LOSS cσ k Z(L)- Z(L)IkF
(204)
48
Under review as a conference paper at ICLR 2021
For term (D2) we have
IlZ尸-ZT)IlF ≤ cσkLH(')w('+I)-LHaW以I)IIF
<	C2bLa∣∣h(')w('+1) - Hy)Iw('+1) + Hy)Iw('+1) - Hy)Iw('+1)∣∣F
σ LA t t	t—1 t	t—1	t	t—1	t—1 IIF
≤	2CσBLAkHf)W-I)- Ht-1W('+1)IlF + 2CσBLAkHaW-I)- HaW印kF
<	2不BLABWEe)- Z(-1 ∣F + 2CσBLABHIW-I)- WBI)IlF
(205)
By induction we have
E'+1)- ZT)IIF ≤ O(∣W('+1)- WBI)IIF) + ... + O(IW(I)- W乜监	(206)
For term (C2) by definition we have
E[∣∣Z衿-Z (—1IF] = E[∣∣L ⑹ H (J)Wf)- L⑶H 储 I)WgkF]
≤ 2BLaBW QE[∣∣Z (J)- Z 匕 I)IlF]+2BLaBH E[∣ Wf)- Wr∕∣F]
(207)
By induction we have
E[∣∣Z衿-ZRkF] ≤ O(E[∣Wf)- WgkF]) + ... + O(E[∣W(I)- W乜常)(208)
Plugging (D1), (D2) to (C2) and C1, (C2) to (B), we have
EK+K
e[∣∣[lH(J)]>(Dt'+%σ0(ZF)))-G僧IF] ≤	X η2o(旦IlLqFHlLkFl×E[∣∣vE(θt-1)kF])
t=EK+1
(209)
□
Using the previous lemma, we provide the upper-bound of Eq. 155, which is one of the three key
factors that affect the mean-square error of stochastic gradient at the 'th layer.
Lemma 10. Let suppose t ∈ {EK +1,..., EK + K}. The upper-bound on the difference of the
gradient with respect to the weight ofthe 'th graph Convolutionallayergiven the same input D(e+1)
but different input HFT), HFT) is defined as
E[∣Vw产(D(e+1), H(e-1), Wy))-VWf ⑶(Dt'+1), Hte-1), Wy))kF]
EK+K
≤ X η2o(∣E[kLC)∣∣F]-ILkFl×E[kvL(θt-1 )kF])
t=EK+1
(210)
49
Under review as a conference paper at ICLR 2021
Proof. For the gradient w.r.t. the weight matrices, we have
E[∣∣Vw产(D('+1),H尸),Wy))-VWf⑶(D('+1), Hf-1), Wy))|图
=E[∣∣(Gg + [ILC)H尸)]>(D尸。σ0(ZF)))- [L⑹H归0]T(DT)。/(ZR)))
-([LH尸]>(D尸。σ0(Zf))))总
≤ 2E[k(Gt-1 + [L⑶H尸)]t(D尸。σ0(ZF)))- [L⑶H尸)]T(DW)。σ0(ZR)))
S----------------------------V---------------------'
(A)
-([LH (J)]t(D尸。σ0(Z F)))) kF]
、--------------V---------------Z
(A)
+ 2 E[k([LH 尸)]t(D尸。σ0(Z F))))- ([LH(J)]T(Dt'+1)。σ0(Zf)))) kF ]
V---------------------------------V-------------------------Z
(B)
(211)
Let first take a closer look at term (A). Let suppose t ∈ {EK + 1,..., EK + K}, where E = t
mod K is the current epoch number and K is the inner-loop size. By the previous lemma, term (A)
can be bounded by
ek+k
(A) ≤ X	(O(kW(I)- W(1)ιkF) + ... + O(kW(L)-W(L)IkF))	(212)
t=EK+1
Then we take a closer look at term (B).
E[k ([lH (J)]t(d£+1)。σ0(Z F))))- ([LH(J)]T(D('+1)。σ0(Zf)))) kF]
≤ 2(BLABD cσ + BL aBH BD )E[kZ F)- Z*∣F]	(213)
X------V-------Z
(C)
Let suppose t ∈ {EK + 1,..., EK + K}. Then we can denote t = EK + k for some k ≤ K.
From Eq. 220, we know that
(C) = E[kLH (J) Wf)- Z t')kF ]
EK+K
≤ X	o(∣E[kL⑶kF] -kLkFl× E[kWf)-WgkF])
t=EK+1
Plugging (C) back to (B), combing with (A) we have
E[kVw 产(D('+1),H (J), Wy))-VW f ⑶(Dt'+1), Ht'-1), W('))kF]
EK+K
≤ X η2o(∣E[kL⑶kF]-kLkFl×E[kvE(θt)kF])
t=EK+1
(214)
(215)
□
In the following lemma, we provide the upper-bound ofEq. 153, which is one of the three key factors
that affect the mean-square error of stochastic gradient at the 'th layer.
Lemma 11. Let suppose t ∈ {EK + 1,..., EK + K}. Then the upper bound of
EK+K
E[kD尸I)- D(L+1)kF] ≤ X η2 xo(|E[kL(‘)kF] -kLkFl× E[kvZ(θt-ι)kF]) (216)
t=EK+1
50
Under review as a conference paper at ICLR 2021
Proof. By definition we have
〜
2
kDe (tL+1) - Dt(L+1)
2 Ii ∂L(θt)	∂L(θt)
kF ≤ii膏 -
≤ Ll2osskHet(L)
∂h(l1 "F
- H(tL)k2F
(217)
Let take closer look at term (A):
≤Ll2ossCσ2kZet(L) -Zt(L)k2F
'----------
(A)
kZe(tL) -Zt(L)k2F = kfe(L)(He t(L-1), W(L)) - f (L)(Ht(L-1), W(L))k2F
≤ 2kfe(L)(He t(L-1), W(L)) - f (L)(He (tL-1), W(L))k2F
+2kf(L)(He(tL-1),W(L)) - f (L)(Ht(L-1), W(L))k2F
= 2kfe(L)(He t(L-1), W(L)) - f (L)(He (tL-1), W(L))k2F	(218)
+ 2kσ(LHe t(L-1)W(L)) - σ(LHt(L-1)W(L))k2F
≤ 2kfe(L)(He t(L-1), W(L)) - f (L)(He (tL-1), W(L))k2F
+2Cσ4BL2ABW2 kZet(L-1) -Zt(L-1)k2F
By induction, we have
L
kZ(L) - z(L)kF ≤ XO(kf(')(H('-1), w('))-f(')(H('-1), w('))kF)	(219)
'=1
Let suppose t ∈ {EK + 1, . . . , EK + K}. Then we can denote t = EK + k for some k ≤ K.
From Eq. 220, we know that
k尹)(H t'-1), W⑶)-f ⑶(H ('-1), w('))kF
EK+K
≤ X	o(∣E[kL(')kF] -kLkFl×E[kW(') -WgkF])
t=EK+1
(220)
Therefore, we know
kDet(L+1)-D(tL+1)k2F≤Ll2ossCσ2kZet(L)-Zt(L)k2F
L EK+K
≤ XX	O(|E[kL(')kF]-kLkF|x E[kW(') - W(-)ιkF])
'=1 t=EK+1	F	F	t	t-1 F (221)
EK+K
= X η2 xθ(∣E[kL⑶kF] -kLkF∣x E[kVΓ(θt-ι)kFf)
t=EK+1
which conclude the proof.	□
Combing the upper-bound of Eq. 153, 154, 155, we provide the upper-bound of mean-suqare error
of stochastic gradient in SGCN++.
Lemma 12. Let suppose t ∈ {EK + 1, . . . , EK + K}. Then we can denote t = EK + k for some
k ≤ K such that
EK+K
E[kVf(θt) - VL(θt)kF] ≤ X η2θ(∣E[ke⑶kF] -kLkF| X E[kVf(θt-ι)[图)(222)
t=EK+1
}
51
Under review as a conference paper at ICLR 2021
Proof. From Lemma 6, we have
E[kGG⑶-G(')kF2]
≤ O(E[kDe (L+1) - D(L+1)k2F])
+ O(E[kVH fe(L)(D(L+1), HH (LT)W(L)) - VH f (L) (D(L+1), H(LT)W(L))kF]) + ...
+ O(E[kVHfe('+1)(D('+2), H⑶,W('+1)) -VHf ('+1)(D('+2), H('), W('+1))kF])
+ O(E[kVWfe(')(D('+1),H('-1),W(')) -VWf⑶(D('+1),H('-1),W('))kF])
(223)
Plugging the result from support lemmas, i.e., Lemma 8, 10, 11 and using the definition of stochastic
gradient for all model parameters VL(θt) = {G(')}L=ι We have
EK+K	L
E[kVLe(θt) -VL(θt)kF] ≤ X η2O(X ∣E[kL⑹kF] -IILkFlx E[∣∣VLe(θt-ι)kF]) (224)
t=EK+1	'=1
□
I.2 Remaining steps toward Theorem 3
By the smoothness of L(θt), We have
L(θτ +1) ≤ L(Ot) + hVL(Ot), θt+ι - θti + -f kθt+ι - θtkF
=L(θt) - ηhVL(θt), VLe(θt)i + η2Lf kVLe(θt)kF
=L(θt) - 2kVL(θt)kF + 2kVL(θt) - VLe(θt)kF - (2 - Lf2η2) kVLe(θt)kF
(225)
Where equality (a) is due to the fact 2hx, yi = kxk2F + kyk2F - kx - yk2F for any x, y.
Take expectation on both sides, We have
E[L(θτ +ι)] ≤ E[L(θt)]-2E[kVL(θt)kF]+2E[kVL(θt)-VLe(θt)kF]-(2- f )E[kVLe(θt)kF]
(226)
By summing over t = 1, . . . , T Where T is the inner-loop size, We have
T
XE[kVL(Ot)k2F]
t=1
T
≤ 2 (E[L(θι)] - E[L(θτ +ι)]) + X[E[kVL(θt) - VLe(θt)kF] -(1 - Lf η)E[kVLe(θt)kF]]
η	t=1
T
≤ 2 (E[L(θι)] - E[L(θ?)]) + X[E[kVL(θt) - VLe(θt)kF] -(1 - Lf η)E[kVLe(θt)kF]]
η	t=1
Where L(O? ) is the global optimal solution.
(227)
52
Under review as a conference paper at ICLR 2021
Let us consider each inner-loop with E ∈ [0, T/K - 1] and t ∈ {EK + 1, . . . , EK + K}. Using
Lemma 12 we have
T/K-1 EK+K	T/K-1 EK+K
X X E[kL(θt)-v2(θt)kF]-(ι-Lfη) X X E[kv2(θt幅
E=0 t=EK+1	E=0 t=EK+1
L	T/K-1 EK+K t-EK
≤o(X∣E[kL⑹|图-国图)(X X X η2E[kvL(θ)j-ιkF])
'=1	E=0 t=EK+1 j=2
T/K-1 EK+K
- 1 - Lfη	X X E[kvLe(θt)k2F]	(228)
E=0 t=EK+1
L	T/K-1 EK+K
≤ [η2K xθ(X ∣E[kL(')kF] -IILkFl) - (1-Lfη)] X X E[kVφt)kF]
'=1	E=0 t=EK+1
T/K-1 EK+K
= [η2∆b+++n0 -(1 - Lfη)] X X E[kvLe(θt)k2F]
E=0 t=EK+1
Notice that η = -----/ 2	= is a root of equation η2∆++0 — (1 一 Lf η) = 0. Therefore We
Lf+ L2f+4∆b+++n0	b+n
have
T
XE[kVL(θt)kF] ≤ 2 (E[L(θι)] - E[L(θ?)])	(229)
t=1	η
Which implies
T
T XE[kVL(θt)kF] ≤ T (Lf +
t=1
- E[L(θ?)]	(230)
J Reproducing experiment results
To reproduce the results reported in the paper, We provide link for dataset doWnload, the bash script
to reproduce the experiment results, and a jupyter notebook file for a quick visualization and GPU
utilization calculation. Itis Worth noting that due to the existence of randomness, the obtained results
(e.g., loss curve) may be slightly different. HoWever, it is not difficult to find that the overall trend
of loss curves and conclusions Will remains the same.
This implementation is based on 4PyTorch using Python 3. We notice that Python 2 might results in
a Wrong gradient update, even for vanilla SGCNs.
Install dependencies:
#	create virtual environment
$ virtualenv env
$ source env/bin/activate
#	install dependencies
$ pip install -r requirements.txt
Experiments are produced on PPI, PPI-Large, Flickr, Reddit, and Yelp datasets. The utilized datasets
can be doWnloaded from 5Google drive.
#	create folders that save experiment results and datasets
$ mkdir ./results
$	mkdir ./data # please download the dataset and put them inside this folder
4https://pytorch.org/
5https://drive.google.com/drive/folders/15eP7OHiHQUnDrHKYh1YPxXkiqGoJhbis?
usp=sharing
53
Under review as a conference paper at ICLR 2021
To reproduce the results, please run the following commands:
$ python
$ python
$ python
$ python
$ python
$ python
train.py
train.py
train.py
train.py
train.py
train.py
--sample_method
--sample_method
--sample_method
--sample_method
--sample_method
--sample_method
ladies' --dataset 'reddit'
fastgcn' --dataset 'reddit'
graphsage' --dataset 'reddit'
vrgcn' --dataset 'reddit'
graphsaint' --dataset 'reddit
exact' --dataset 'reddit'
$ python
$ python
$ python
$ python
$ python
$ python
train.py
train.py
train.py
train.py
train.py
train.py
--sample_method
--sample_method
--sample_method
--sample_method
ladies' --dataset 'ppi'
fastgcn' --dataset 'ppi'
graphsage' --dataset 'ppi
vrgcn' --dataset 'ppi'
--sample_method
--sample_method
'graphsaint' --dataset 'ppi'
'exact' --dataset 'ppi'
$ python
$ python
$ python
$ python
$ python
$ python
train.py
train.py
train.py
train.py
train.py
train.py
--sample_method 'ladies' --dataset 'flickr'
--sample_method 'fastgcn' --dataset 'flickr'
--sample_method 'graphsage' --dataset 'flickr'
--sample_method 'vrgcn' --dataset 'flickr'
--sample_method 'graphsaint' --dataset 'flickr
--sample_method 'exact' --dataset 'flickr'
$ python
$ python
$ python
$ python
$ python
$ python
train.py
train.py
train.py
train.py
train.py
train.py
--sample_method 'ladies' --dataset 'ppi-large'
--sample_method 'fastgcn' --dataset 'ppi-large'
--sample_method 'graphsage' --dataset 'ppi-large'
--sample_method 'vrgcn' --dataset 'ppi-large'
--sample_method 'graphsaint' --dataset 'ppi-large
--sample_method 'exact' --dataset 'ppi-large'
$ python
$ python
$ python
$ python
$ python
$ python
train.py
train.py
train.py
train.py
train.py
train.py
--sample_method
--sample_method
--sample_method
'ladies' --dataset 'yelp'
'fastgcn' --dataset 'yelp'
'graphsage' --dataset 'yelp'
--sample_method 'vrgcn' --dataset 'yelp'
--sample_method 'graphsaint' --dataset 'yelp'
--sample_method 'exact' --dataset 'yelp'
54