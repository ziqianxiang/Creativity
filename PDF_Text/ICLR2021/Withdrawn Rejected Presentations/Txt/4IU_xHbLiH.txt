Under review as a conference paper at ICLR 2021
Fast and Differentiable Matrix Inverse and
Its Extension to SVD
Anonymous authors
Paper under double-blind review
Ab stract
Matrix inverse (Minv) and singular value decomposition (SVD) are among the
most widely used matrix operations in massive data analysis, machine learning,
and statistics. Although well-studied, they still encounter difficulties in practical
use due to inefficiency and non-differentiability. In this paper, we aim at solving
efficiency and differentiability issues through learning-based methods. First of all,
to perform matrix inverse, we provide a differentiable yet efficient way, named
LD-Minv, which is a learnable deep neural network (DNN) with each layer be-
ing an L-th order matrix polynomial. We show that, with proper initialization,
the difference between LD-Minv’s output and exact pseudo-inverse is in the order
O exp -LK , where K is the depth of the LD-Minv. Moreover, by learning
from data, LD-Minv further reduces the difference between the output and the
exact pseudo-inverse. We prove that gradient descent finds an -error minimum
within O(nKL log 1/) steps for LD-Minv, where n is the data size. At last,
we provide the generalization bound for LD-Minv in both under-parameterized
and over-parameterized settings. As an application of LD-Minv, we provide a
learning-based optimization method to solve the problem with orthogonality con-
straints and utilize it to differentiate SVD (D-SVD). We also provide a theoretical
generalization guarantee for D-SVD. Finally, we demonstrate the superiority of
our methods on the synthetic and real data in the supplementary materials.
1	Introduction
Matrix inverse (including matrix pseudo-inverse) and singular value decomposition are fundamental
linear algebra operations, ubiquitous in machine learning, statistics, signal processing, and other
fields. In general, solving scientific computing or optimization problems often need to perform
these two operators, such as matrix (pseudo) inverse for least squares regression, singular value
decomposition (SVD) for dimensionality reduction (PCA), the low-rank related problems (Liu et al.,
2010; Zhang et al., 2018; Liu et al., 2013), the graph-based issues (Wu et al., 2020a; Von Luxburg,
2007), and even for training deep neural networks (DNNs) with structured layers (Ionescu et al.,
2015). Nevertheless, Minv and SVD appear less and less often in the modern machine learning tasks.
One reason is inefficiency. Computing the SVD and the Minv can be extremely time-consuming for
large-scale problems; however, efficiency is a significant concern in the current big data and deep
learning era. Besides, non-differentiability is considered another reason that blocks the use of SVD
and Minv. Usually, most prevalent methods for training DNNs are first-order and are based on the
backpropagation; however, Minv and SVD are not necessarily continuous functions of the matrix
entries (Stewart, 1969). Therefore, derivatives are not always existent. Although Minv and SVD may
be backprop-able due to some specific implementation, they are unstable and are essentially non-
differentiable. Thus the gradients cannot pass through them when backpropagating. Considering the
above problems, one natural question emerges.
Does there exist an efficient and differentiable way to perform Minv and SVD?
Over the last decade, many sketches-based methods have been developed, e.g., Nelson & Nguyen
(2013); Meng & Mahoney (2013); Cohen et al. (2015); Indyk et al. (2019). The main idea of the
sketches-based techniques is to use random projections, which is efficiently computable, to reduce
the problem size before performing SVD and Minv. However, they do not solve the problem of
non-differentiability as smaller-sized SVD and Minv still needs to be computed.
1
Under review as a conference paper at ICLR 2021
Recently, “differentiable learning-based” methods (D-LbM) have attracted considerable atten-
tion (Chen et al., 2018; Indyk et al., 2019; Liu et al., 2019; Wu et al., 2020b; Xie et al., 2019). These
methods usually unroll the classical optimization or numerical iterative algorithms and introduce the
learnable parameters to obtain a learnable DNN. In general, the iterative algorithms inspired DNNs
consist of differentiable operators such as matrix polynomials. Benefiting from training on the data,
D-LbMs can execute in much fewer iterations with similar per-iteration cost as original algorithms
but obtain much better performance. Many empirical results, e.g., Gregor & LeCun (2010); Yang
et al. (2016); Peng et al. (2018), show that a well-trained DNN provided by D-LbM—compared
with the original optimization algorithm—can obtain an almost equally good solution using one
or even two order-of-magnitude fewer iterations. Based on these observations, we aim to find a
learning-based iterative way to differentiate Minv and SVD in this paper.
However, all these D-LbMs suffer from two common problems. One is the convergence guarantee.
The forward process of D-LbMs may diverge, even if the original unrolled iterative algorithm has a
well-behaved convergence guarantee. In fact, most D-LbM methods have little or no convergence
guarantees. To the best of our knowledge, there is no work to reveal the behavior of D-LbM during
training. How does the training loss decrease? How big is the gap between the output of D-LbM
and that of the original algorithm? All these questions are still open. Another problem is, both
for D-LbM and the original algorithm, a limited theory exists when the input data obey a common
distribution (e.g., data drawn from a low-dimensional manifold). Essentially, by learning from data,
D-LbMs can obtain a problem-dependent parameter bias. It can help D-LbMs get the better perfor-
mance on a specific data distribution within much fewer computation cost, instead of simply fixing
the parameter ahead of time like traditional iterative methods. But there is no mathematical result to
describe this phenomenon strictly. Moreover, it is unknown whether the trained D-LbMs generalize
well on data from the same distribution.
Remarkably, in this paper we provide a learnable, differentiable and efficient way to perform Minv,
named Learnable Differentiable Matrix Inverse (LD-Minv) and solve the above two problems by our
proposed LD-Minv. First of all, LD-Minv is a DNN with each layer being an L-th order matrix poly-
nomial; and the coefficient of each power is learnable. We show that LD-Minv converges to the Minv
or pseudo-inverse with order L if the coefficients of polynomial are set properly. Namely, the differ-
ence between the output of the DNN and the exact pseudo-inverse is in the order O exp -LK ,
where K is the depth of LD-Minv. Secondly, by learning from data, LD-Minv can further improve
the and precision on some specific data distribution. Namely, the distance between the output and
the exact pseudo-inverse can be arbitrarily small. Specifically, the training loss converges to zero
exponentially fast, i.e., gradient descent (GD) finds an -error minimum in `2 regression using at
most O(nKL log 1/) iterations, where n is the data size. Finally, we provide the generalization
bound for LD-Minv in both under-parameterized and over-parameterized settings.
With LD-Minv at hand, as a direct application, we propose a learning-based optimization to solve
any convex problems with non-convex orthogonality constraints. Then we use it to differentiate
SVD. Note that we also provide a generalization guarantee for our D-SVD. Unlike the previous
work on differentiable SVD (Indyk et al., 2019), which is based on the power method and needs an
assumption on the gap between singular values to ensure convergence, our method can converge to
the solution without any gap assumption. In summary, our main contributions include:
•	We propose a differentiable method, LD-Minv, to perform matrix inverse efficiently. LD-
Minv, inspiring a DNN with each layer being a learnable L-th order matrix polynomial,
would be useful for accelerating and differentiating general matrix decompositions. We
show that LD-Minv can converge to the matrix pseudo-inverse with order L.
•	By learning, LD-Minv can further improve the approximation performance on some under-
lying data distribution. We prove that GD finds an -error minimum in `2 regression using
at most O(nKL log 1/) iterations under mild conditions. Moreover, we also provide the
generalization bound for LD-Minv. We reveal that the empirical Rademacher complexity
of the loss function class is bounded by Oe(min{d3L∕K1/2, ,d3K∕n}), where Oe(∙) hides
log factors, and K and d are the depth and the width of LD-Minv, respectively.
•	As a direct application, we further provide a learning-based general framework to solve
the problems with orthogonality constraints. This D-LbM helps us to differentiate SVD.
Finally, we also provide a generalization guarantee for our D-SVD.
2
Under review as a conference paper at ICLR 2021
2	Differentiable Matrix Inverse
We introduce the proposed D-Minv in this section. We first present an intuitive idea to show how
to approximate the matrix inverse iteratively. Then we generalize the iterative method to obtain our
fixed and learnable D-Minv, separately
2.1	Fixed Differentiable Matrix Inverse
Given a matrix A ∈ Rd×d, we want to approximate its inverse A-1 by the matrix X ∈ Rd×d, i.e.,
AX ≈ I, where I ∈ Rd×d is the identity matrix. Ideally, X is the fixed point of the following
equation:
X=X(AX)-1=X X(I-AX)l ,	(1)
where the last equality follows from the Neumann series when AX ≈ I and (I - AX)l is the
l-order matrix polynomial.
Inspired by the above iterative process, it is obvious that we can obtain the matrix X iteratively.
Hence, we consider the following higher-order iterative method, which is an L-th order matrix
polynomial in each iteration, where L ≥ 4:
Xk+1 =Xk LX-1 Elk , Ek =I-AXk.	(2)
Note that DNNs can easily implement this method: one layer corresponds to one iterative step.
Since there are no parameters to learn in Eq. (2), and it only involves the matrix multiplication
(thus differentiable), we name it fixed D-Minv 1. One may doubt that the computational cost of each
iteration is high in Eq. (2). However, as shown in Lemma 1, higher-order polynomial usually implies
a faster convergence rate. For obtaining the same precision with different L, the total computational
cost is in the order of O(L/ ln(L)), which is a very slow growth rate w.r.t. L. Although simple,
fixed D-Minv converges to the inverse AT or At extremely fast.
Lemma 1 (Approximation Speed). The generated sequence {Xk ∈ Rd1 ×d2} by Eq. (2) converges
to the Moore-Penrose inverse At with order L provided that X。=d A>, co > 111∣ A∣∣2, i.e., we
can conclude:
AAt - AXk = e0Lk, in which e0 = AAt - AX0 < 1,
where ∣∣∙∣∣ is the spectral norm.
We can see that fixed D-Minv converges with order L, i.e., a shallow D-Minv can approximate the
matrix inverse very well. Moreover, provided with the X0 given in Lemma 1, the column space and
the row space of the matrix Xk are exactly correct from the beginning.
Proposition 1 (Invariant Column and Row Spaces). With the same setting in Lemma 1, for any
k ≥ 0, it holds that:
XkAAt = Xk,	AtAXk = Xk.
By Proposition 1, it is easy to see that the zero singular values remain unchanged during computing,
which indicates that D-Minv works consistently on the full and rank-deficient square and rectangle
matrices. Note that this proposition also holds for the upcoming LD-Minv.
2.2	Learnable Differentiable Matrix Inverse
We consider the learnable D-Minv (LD-Minv) with the following iterative formula:
Xk+1 =Xk LX-1C{k,l}Elk ,	Ek=I-AXk, L≥4,	(3)
1Higher-order method Eq. (2) is already well-known in the applied mathematics community in another
equivalent form, see Climent et al. (2001); Amat et al. (2003); Li et al. (2011).
3
Under review as a conference paper at ICLR 2021
where (k,l + 1) ∈ [K] X [L], C{k,i} ∈ R are learnable coefficients, and |卜|恨 is the Frobenius norm.
LD-Minv is also in the category of LbMs that unroll the conventional numerical iteration methods.
As discussed in the introduction, two problems, i.e., no convergence analysis for the training proce-
dure and no generalization guarantee for the trained DNN, block the further theoretical exploration
of these LbMs. However, in contrast to the previous works, we obtain favorable theoretical results
on both of the two problems for our LD-Minv. First, although fixed D-Minv already converges
extremely fast, LD-Minv can still perform much better on the training data under mild conditions.
SPecificanyJXK - Aql converges to zero exponentially fast during training, i.e., GD finds an E-
error minimum in `2 regression using at most O(nKL log 1/) iterations 2. Second, LD-Minv has a
tight generalization bound to guarantee the performance on the unseen matrix when it comes from
the same distribution as the training instances. We provide the rigorous theoretical results in Sec. 4.
Discussion 1. One can find that the final iterate XK can be written as matrix polynomial of A,
and may argue that approximating the Minv by the polynomial is well-studied. Zero training can
be obtained when the polynomial’s order is large enough, and the approximation error controls the
generalization error. For the sake of distinction, we make three clarifications. First of all, most
traditional results only describe the behavior in the worst case, which holds for any input matrix and
may easily fail in some extreme cases. However, LD-Minv focuses on a more realistic situation, i.e.,
the input matrices obey some common distribution, such as the affinity matrices of Facebook users
or the sampled CT and MRI images. How to learn from the data and design a more efficient method
is still open, not to mention the theoretical generalization guarantee on specific data distribution.
Secondly, the traditional polynomial approximation method is susceptible to the polynomial order
and is very easy to over-fitting when the order is over-parameterized, say order nd. Without
precise data distribution density, it is impossible to choose the proper order to ensure zero training
and guarantee the small generalization error, simultaneously. To solve this, we may need some
complex and data-driven regularization strategy. In sharp contrast, LD-Minv is robust to the order,
and our theoretical results (i.e., zero training and small generalization error) hold well from under-
parameterized to over-parameterized cases. Thirdly, the exact polynomial fitting can only imply the
existence of zero training solution, while our results describe the convergence behavior (i.e., which
solution we will choose) . Obviously, there exists abig gap between the convergence of training and
the existence of a solution.
Note that we have considered a much easier matrix polynomial: XK = PiL=0 CiAi . Learning the
coefficients {Ci}iL=1 becomes an easy-to-solve convex regression problem unlike the convex one in
Eq. (3). Unfortunately, we failed due to some stability issues, e.g, the coefficients in different powers
vary significantly which cause the poor generalization performance.
Discussion 2. Most matrix iterative methods suffer from the ill-condition problem, which usually
brings instability and makes the analysis and calculation hard to carry on, e.g., for D-Minv, large
condition number may significantly slow the convergence speed (see Lemma 1). However, learning-
based method can greatly alleviate this ill-condition problem by learning from data. LD-Minv can
beat D-Minv easily when the condition number of the input matrix is large and the learned coeffi-
cients for LD-Minv are also valid on the unseen data. See the experimental part for more details.
2.3	Training Settings
We now describe some training settings for D-Minv. Denote by {Ai}in=1 the training set consisting
of n samples. Let X{k,i} ∈ Rd1 ×d2 be the output of the k-th layer on the i-th training sample
Ai ∈ Rd2×d1 . We consider a typical regression loss:
nK
Ln(C) := 2nK XX IIAiX{k,i}Ai - AiIIF,	(4)
i=1 k=1
where C = {C{k,i}, (k, l + 1) ∈ [K] × [L]} is the collection of all learnable parameters. Note
that when X = A*, the properties of the Moore-Penrose inverse indicate the equation AXA = A,
which can further indicates the equation XAX = X in our invariant space case, see Proposition 1.
Therefore, we only use the difference term (AXA - A) in the training loss.
2Please distinguish the two “convergence” here. One describes Xk approaching At w.r.t. k and L, and the
other refers to the behavior of LD-Minv’s loss w.r.t. training iteration.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Gradient descent (GD) with proper initialization for LD-Minv
Input: Training data {Ai}in=1, number of iterations T, step size η.
1:	Generate each C{k,i} ∈ C such that ∣C{k,i} - 1| = O(K-α), where α > 1.
2:	Set X{o,i} = cl A> or X i, where co > 11lAik2, X i is the output of a fixed D-Minv.
3:	for t = 1,…，T do
4:	Update C(t+1) = C⑴一η ∙ D[C=C(t)]Ln.
5:	end for
Output: C(0), . . . , C(T).
Thanks to the differentiability of LD-Minv, we adopt GD to minimize the training loss to obtain
the proper coefficients C. We present the initialization strategy and training process in Algorithm
1, where the first derivative of differentiable function Ln(C) : RK×L → R at C(t) is denoted
by D[c=c(t)]Ln(∙) := (∂Ln(C(t'))∕∂C) ∈ Rk×* l *. When K is large, one may notice that our
coefficient is close to the oracle value 1, which may provide a relatively good initial loss L C(0) .
Notably, this does NOT mean that we will treat the learning and initialization as a perturbation
of the fixed D-Minv and bound the loss by the good-enough fixed D-Minv’s performance plus the
perturbation error. On the contrary, as we will show in theoretical results part, learnability indeed
matters, and LD-Minv can obtain better performance than the fixed one by training. The real purpose
of this initialization is to take advantage of D-Minv’s local Lipschitz continuity near C = 1, where
1 is the all one vector. The continuity reduces the complexity of optimization and will benefit the
generalization of D-Minv.
3 Learning-based Optimization with Orthogonality Constraints
Considering the following general orthogonality constrained optimization problem:
min f (U),	s.t. U>U = I,	(5)
where the objective function f(U) : Rm×r → R is differentiable and convex. The usual way
to solve Eq. (5) is to perform the manifold GD on the Stiefel manifold, which evolves along the
manifold geodesics. Specifically, manifold GD first updates the variable in the manifold tangent
space along the objective function’s projected gradient. Then, map the updated variable in the tan-
gent space to a feasible point on the geodesic, and repeat these two steps until convergence (Edel-
man et al., 1998). Usually, the mapping step is non-differentiable and inefficient. Fortunately, the
work (Wen & Yin, 2013; Ren & Lin, 2013) develops a technique to solve the optimization problem
with orthogonal constrains approximately, which only involves matrix multiplication and inverse.
We let U ∈ Rm×r, where r is the Stiefel manifold’s dimension. Denote by G ∈ Rm×r the gradient
of the objective function f(U) in Eq. (5) w.r.t. U at Uk, then the projection ofG in the tangent space
of the Stiefel manifold at Uk is PUk3, where P = GUk> - UkG> and P ∈ Rm×m. Instead of
parameterizing the geodesic of the Stiefel manifold along direction P using the exponential function,
we generate the feasible points to update Uk by the following Cayley transform (Wen & Yin, 2013):
Uk+1 = U(t) = C(t)Uk,
where
C⑴=(I+tpΓ(I- tP),
(6)
where I is the identity matrix and t ∈ R is the step size used for updating the current Uk. In other
words, U(t) is a re-parameterized local geodesic w.r.t. t on the Stiefel manifold. One can easily
verify that U(t) has the following properties, given Uk> Uk = I:
⑴ ddtU(0) = -PUk, (2) U(t) is smooth in t, (3) U(0) = Uk, (4) U (t)[ U (t) = I, ∀t ∈ R.
It is evident that, if t is in a proper range, Uk+1 can lead to a lower objective function value than
U(0) = Uk on the Stiefel manifold. Besides, computing Uk+1 only involves the matrix multiplica-
tion and matrix inverse, which can be easily performed by our LD-Minv in Eq. (3. Therefore, we let
C(t) = LD-Minv (I + tP/2) in Eq. (6). Then we can obtain a learning-based optimization method
for the problem with orthogonality constraints in Eq. (5).
3We choose the canonical metric on the tangent space as the equipped Riemannian metric.
5
Under review as a conference paper at ICLR 2021
3.1 Application: Differentiable SVD by LD-Minv
In this part, we show how to utilize the previous general learning-based optimization framework to
differentiate SVD. Given an arbitrary matrix M ∈ Rm×n and its skinny SVD M = USV>, the Von
_________________________________ _____ ____________ , _ ., — ʌ..、
Neumann S trace inequality,(M, Xi ≤ Pi σi(M)σi(X), implies that {U ∈ Rm×r, V ∈ Rn×r} is
an optimal solution of the following optimization problem:
min f (U, V)=(I ∣∣Dc (Λ)kF -〈M, UV>〉)， s.t. U> U = V>V = I,	⑺
where Λ := U>MV, Diag(∙) returns the collection of the diagonal entries of the input matrix, and
Dc (Λ) := (Id - Diag)(Λ) := Λ - Diag (Λ) for any input matrix Λ.
Note that solving the problem in Eq. (7) is equivalent to performing SVD. Let {M, U0, V0} be
the inputs of our Differentiable SVD (D-SVD). We adopt the Cayley transform in Eq. (6) to solve
the problem in Eq. (7), and replace the matrix inverse in it by our proposed LD-Minv in Eq. (3).
W.l.o.g, in the following, we only focus on the updating strategy of U due to the similarity between
U and V. We first compute the gradient of the objective function f(U, V) w.r.t. U at {Uk, Vk},
which we shall denote by G = MVk Dc Vk>M>Uk - I . Then we find a geodesic curve
U(t) along the gradient on the Stiefel manifold for updating Uk. Referring to Eq. (6), the curve is
U(t) = (I + tP)T (I — tP)Uk, where P = GU> — UkG>. Given the step size t, we can use
LD-Minv to perform the matrix inverse on the factor(I + 2 P). In summery, D-SVD consists of
two steps: (1) find a proper t; (2) update {Uk, Vk} by the Cayley transform.
For finding a proper step size t, we consider the following problem:
tU = argmin f (t) := f(U(t), Vk),	⑻
0≤t≤ε
where ε is a given parameter to ensure a small enough magnitude of tU. Notice that if t is small
enough to hold ∣∣2P∣∣ < 1, then We have (I + tP)T = I + P∞=ι (—tP)l, which implies that
U(t) =(I + 2 P∞=ι (—tP)l)Uk. Considering that t* in Eq. (8) is small, We can approximate
f(t) via its second order Taylor expansion at t = 0:
f (t) = f (0) + f0(0) ∙ t + 1 f00(0) ∙ t2 + O(t2),	(9)
where f0(0) and f00 (0) are the first and the second order derivatives of f(t) evaluated at 0, respec-
tively. These two derivatives have closed form and can be computed efficiently. Consequently, we
can obtain an approximated optimal solution t* via:
tU = min {ε,s},	where ε < 2∕∣∣Pk, and S= -f0(0)∕f00(O),	(10)
provided that:
(f0(0) =〈Dc (U>MVk), Dc (U>PMVk)〉+ (MV®, PUk),
If00(O) = ∣∣Dc (U>PMVk)∣∣F +〈Dc (U>MVk), Dc (U>P2MVk)) - MVk, P2Uk〉.
(11)
Then we can update U by U(t*U), i.e., Uk+1 = U(t*U). Thanks to the cyclic property of trace
operator, Vk+1 shares a similar update strategy with Uk+1.
Provided the input {M, Uk, Vk}, the complete iterative steps for our D-SVD are as follows4:
GU = MVk(Dc (V>M>Uk) - I),
产 _ ∙ 2	2	f0(U(t), Vk) [ s
U = min IkPuk, f00(U(t),Vk)∫+ tUk,
GV = M>Uk+ι(Dc (U>+ιMVk) - I),
t* =mi" ɪ	f'(Uk+ι, Mt)) 1 + s
V = IkPvk, f00(Uk+ι, V(t))∫+ Vk
PU = GUUk> - UkGU>,
Uk+1 = Cayley(t*U,CUk,PU,Uk),
PV = GV Vk> - VkGV> ,
Vk+1 = Cayley (t*V, CVk,PV,Vk),
4For convenience and clearing writing, we omit the superscript in the updating rules.
6
Under review as a conference paper at ICLR 2021
Algorithm 2 Forward Propagation for D-SVD
Input: Training data {Mi, Ui,0, Vi,0}in=1, depth Ksvd of D-SVD, number of iterations Tinv, step
size ηinv and D-SVD’s current parameters t.
1:	for k = 0, . . . , Ksvd do
2:	Calculate Pu,i and t^ with {Mi, Ui,k, Vi,k}n=ι byEq.(12).
3:	Train LD-Minv with parameters CUk by Algorithm 1 with the data {Ai}in=1, number of
iterations Tinv and step size 力口丫，where Ai = (I + 坨正小力2).
4:	Repeat Steps 2 and 3 by switching the roles of U and V with the input {Ui,k+1, Vi,k}in=1.
5:	end for
Output: {CUk,CVk}kK=sv1dand{Ui,k+1,Vi,k}in=,K1,skv=d1.
Algorithm 3 Joint Training for D-SVD and LD-Minv
Input: Training data {Mi}in=1, number of iterations Tinv and Tsvd, step sizes ηsvd and ηinv.
1:	Generate {Ui,o, Vi,0}n=1 randomly from the r-dimensional Stiefel manifold. Set t(0) = 0.
2:	for t =1, ∙ ∙∙ , Tsvd do
3:	Forward propagate by Algorithm 2.
4:	Update t(t+1) =t(t) - %vd ∙ D[t=t(t)]Mn, where Mn ：= nK1svd Pn=I PKvd f (Ui,kVi,k).
5:	end for
Output: {Ui,k+1, Vi,k}in=,K1,skv=d 1. * 4
where
Cayley (t, C, P, U) ：= LD-Minv (I + tP/2, C)(I - tP/2)U,
tt ：= {(ttUk, ttVk ) ∈ R2}kK=sv1d is the collection of learnable parameters for D-SVD, and
LD-Minv(∙, C) is a LD-Minv module with parameters C. Note that a DNN can easily implement
our D-SVD. Each layer implements the steps in Eq. (12), which are the specific iterative procedures
of the previous general learning-based optimization with orthogonality constraints. We adopt differ-
ent LD-Minvs for different layers, and provide the training process for D-SVD in Algorithms 2 and
3. By introducing LD-Minv into the Cayley transform, we bypass the non-differentiable exact Minv,
and solve the problem in Eq. (7) (i.e., perform SVD) in a differentiable and learning-based way.
4 Main Results
In this section, we provide the main theoretical results of D-Minv, including the linear convergence
of GD during training and the generalization performance. All the detailed proofs of these theorems
are provided in the supplementary material. The theoretical results for D-SVD are presented in the
supplementary material due to limited space.
4.1 Convergence Rate of GD
We consider the general LD-Minv and large training data size n in this section. We first make the
following assumption on the training data.
Assumption 1 (Bounded Singular Values). Given the training matrices Ai ∈ Rd1 ×d2 in=1, we
assume the training matrices’ positive singular value vectors are d-dimensional, where d ≤
min{d1, d2}. Assume these positive singular values have lower and upper bounds, i.e.,
IlXiII∞ ≤ 1 and	min[xij	≥	ba	> 0, where	Xi	=	σ+(Ai)	∈ Rd,	∀i	∈	[n],
j∈[d]
and σ+(∙) extract the positive singular values.
In general, the boundedness assumption for the singular values the is weak and easy to satisfy. See
the discussion in the supplementary material (Section C.1.2).
With this assumption, we can obtain the convergence rate of our Algorithm 1.
7
Under review as a conference paper at ICLR 2021
Theorem 1 (Convergence Rate). Suppose Assumption 1 holds and let d = min{d1 , d2}. For any
t, e > 0, we let K = Ω (dba), then with probability at least 1 一 exp (-O (K/bT)), GD in Algorithm
1 with SteP size η = Θ(1∕dL) can find a collection of coefficients such that:
Ln(C(T)) <e, for T = θ(dLKnb- ln(1∕e)),
where ba is the lower bound for the positive singular values oftraining data Ai.
This is known as the linear convergence rate because drops exponentially fast in T. Recall that K
and L are from the definition of our LD-Minv.
Remark 1. Different from the traditional theoretical results of DNNs, the randomness in our results
mainly comes from a re-weighting strategy (see Sec. E) rather than initialization. In general, our
results hold for any initialization strategy as long as the condition in Algorithm 1 is met. Notably,
our results do not require the depth or the width to be in the polynomial order of data size n, which
is a prevalent over-parameterization assumption for the current deep learning theories.
Remark 2. To present our convergence result most simply, we choose to focus on GD method
with fixed step size mainly. It shall be easy to extend our results to more general settings, such as
stochastic GD and dynamic step size.
4.2 Generalization
We characterize the generalization performance of LD-Minv trained by GD in this theorem.
Theorem 2 (Generalization Bound). Denote by PA the distribution of Ai in Assumption 1. Suppose
that ɑ ≥ 1 and K2α-1/2 = Ω(√n), then with probability at least 1 — δ, the iterate C(t) ofAlgorithm
1 holds that:
AJuLn(C(t))]≤ Ln 6 )+ 0 (mm]⅜d3L,
for t = 0,1,…，T, where EA 〜PA [Ln] is the expected value of Ln under the probability measure
PA and O(∙) hides log factors.
We adopt two ways to upper bound the empirical Rademacher complexity, which corresponds to the
cases K < n and K > n, respectively. The final bound is the minimum of them (middle term in the
above upper bound).
Remark 3. The second term in our bound distinguishes our result from most previous work (Allen-
Zhu et al., 2019; Arora et al., 2019; Yehudai & Shamir, 2019; Cao & Gu, 2019; Chen et al., 2019;
Xie et al., 2020) on the generalization bounds of over-parameterized neural networks. Specifically,
most over-parameterized work mainly focus on establishing the bound that does not explode when
the network width goes to infinity. However, their bounds are exponential in the depth, which may
not hold when the depth is large. In contrast, our result covers a wider range of depth K: it covers
the case of both K < n and K > n. One may wonder whether our bound will explore when K
approaches infinity. The answer is no. We can observe a “double descent” trend to certain extent:
the generalization error bound first increases with the network depth K when K < n, then it starts
to decrease when K becomes larger even for K n.
5 Conclusion
We provide a differentiable and learnable way, named LD-Minv, to perform the matrix inverse
by using a L-th order matrix polynomials. Then, we offer some theoretical guarantees about the
convergence during training and generalization ability in both under-parameterization and over-
parameterization setting for LD-Minv. Remarkably, we are the first to provide the strict analysis
for the training process of the differentiable learning-based methods. As an application of LD-Minv,
we propose a learning-based optimization method to solve the problems with non-convex orthogo-
nality constraints, and then utilize it to differentiate SVD. A generalization guarantee for D-SVD is
also provided.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems,pp. 6155-6166, 2019.
Sergio Amat, Sonia BUsqUier, and JM Gutierrez. Geometric constructions of iterative functions to
solve nonlinear equations. Journal of Computational and Applied Mathematics, 157(1):197-205,
2003.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10835-10845,
2019.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of
unfolded ISTA and its practical weights and thresholds. In Advances in Neural Information Pro-
cessing Systems, pp. 9061-9071, 2018.
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suffi-
cient to learn deep ReLU networks? arXiv preprint arXiv:1911.12360, 2019.
Joan-Josep Climent, Nestor Thome, and Yimin Wei. A geometrical approach on generalized inverses
by neumann-type series. Linear Algebra and its Applications, 332:533-540, 2001.
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimen-
sionality reduction for k-means clustering and low rank approximation. In Proceedings of the
forty-seventh annual ACM symposium on Theory of computing, pp. 163-172, 2015.
R Brent Dozier and Jack W Silverstein. Analysis of the limiting spectral distribution of large dimen-
sional information-plus-noise type matrices. Journal of Multivariate Analysis, 98(6):1099-1122,
2007.
Richard M Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian pro-
cesses. Journal of Functional Analysis, 1(3):290-330, 1967.
Alan Edelman, Tomas A Arias, and Steven T Smith. The geometry of algorithms with orthogonality
constraints. SIAM Journal on Matrix Analysis and Applications, 20(2):303-353, 1998.
Gene H Golub and Victor Pereyra. The differentiation of pseudo-inverses and nonlinear least squares
problems whose variables separate. SIAM Journal on numerical analysis, 10(2):413-432, 1973.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of
the International Conference on Machine Learning, pp. 399-406, 2010.
Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances
in Neural Information Processing Systems, pp. 7400-7410, 2019.
Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation for deep net-
works with structured layers. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 2965-2973, 2015.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow relu networks. arXiv preprint arXiv:1909.12292, 2019.
Hou-Biao Li, Ting-Zhu Huang, Yong Zhang, Xing-Ping Liu, and Tong-Xiang Gu. Chebyshev-type
methods and preconditioning techniques. Applied Mathematics and Computation, 218(2):260-
270, 2011.
9
Under review as a conference paper at ICLR 2021
Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold
via the cayley transform. In International Conference on Learning Representations, 2019.
Zhouchen Lin, Risheng Liu, and Zhixun Su. Linearized alternating direction method with adap-
tive penalty for low-rank representation. In Proceedings of the Advances in Neural Information
Processing Systems, pp. 612-620, 2011.
Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank repre-
sentation. In Proceedings of the International Conference on Machine Learning, pp. 663-670,
2010.
Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. Robust recovery
of subspace structures by low-rank representation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 35(1):171-184, 2013.
Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Alista: Analytic weights are as good
as learned weights in lista. In International Conference on Learning Representations, 2019.
Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input-sparsity
time and applications to robust linear regression. In Proceedings of the forty-fifth annual ACM
symposium on Theory of computing, pp. 91-100, 2013.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
2018.
Jelani Nelson and Huy L Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser
subspace embeddings. In 2013 ieee 54th annual symposium on foundations of computer science,
pp. 117-126. IEEE, 2013.
Xi Peng, Joey Tianyi Zhou, and Hongyuan Zhu. K-meansNet: When K-means meets differentiable
programming. arXiv preprint arXiv:1808.07292, 2018.
Xiang Ren and Zhouchen Lin. Linearized alternating direction method with adaptive penalty and
warm starts for fast solving transform invariant low-rank textures. International Journal of Com-
puter Vision, 104(1):1-14, 2013.
Mark Rudelson and Roman Vershynin. The Littlewood-Offord problem and invertibility of random
matrices. Advances in Mathematics, 218(2):600-633, 2008.
Uwe Schmidt and Stefan Roth. Shrinkage fields for effective image restoration. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2774-2781, 2014.
Jianhong Shen. On the singular values of gaussian random matrices. Linear Algebra and its Appli-
cations, 326(1-3):1-14, 2001.
GW Stewart. On the continuity of the generalized inverse. SIAM Journal on Applied Mathematics,
17(1):33-45, 1969.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17(4):395-416,
2007.
Zhangyang Wang, Ding Liu, Shiyu Chang, Qing Ling, Yingzhen Yang, and Thomas S Huang. D3:
Deep dual-domain based fast restoration of jpeg-compressed images. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2764-2772, 2016.
Colin Wei and Tengyu Ma. Data-dependent sample complexity of deep neural networks via lipschitz
augmentation. In Advances in Neural Information Processing Systems, pp. 9722-9733, 2019.
Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints.
Mathematical Programming, 142(1-2):397-434, 2013.
10
Under review as a conference paper at ICLR 2021
Jianlong Wu, Xingyu Xie, Liqiang Nie, Zhouchen Lin, and Hongbin Zha. Unified graph and low-
rank tensor learning for multi-view clustering. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI2020, pp. 6388-6395, 2020a.
Kailun Wu, Yiwen Guo, Ziang Li, and Changshui Zhang. Sparse coding with gated learned ISTA.
In International Conference on Learning Representations, 2020b.
Xingyu Xie, Jianlong Wu, Guangcan Liu, Zhisheng Zhong, and Zhouchen Lin. Differentiable lin-
earized ADMM. In International Conference on Machine Learning, pp. 6902-6911, 2019.
Xingyu Xie, Hao Kong, Jianlong Wu, Guangcan Liu, and Zhouchen Lin. Maximum-and-
concatenation networks. In International Conference on Machine Learning, pp. 1358-1368,
2020.
Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for compressive sensing MRI.
In Proceedings of the Advances in Neural Information Processing Systems, pp. 10-18, 2016.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. In Advances in Neural Information Processing Systems, pp. 6594-6604, 2019.
Xiao Zhang, Lingxiao Wang, Yaodong Yu, and Quanquan Gu. A primal-dual analysis of global op-
timality in nonconvex low-rank matrix recovery. In Proceedings of the International Conference
on Machine Learning, pp. 5857-5866, 2018.
Appendix
A Experimental Results
A.1 Effectiveness Validation
We conduct experiments on synthetic data to validate the effectiveness of our proposed LD-Minv
and D-SVD. We run our all experiments for 10 times and report the average results.
(b)
Figure 1: Experimental results of LD-Minv and D-Minv. (a) Test loss of LD-Minv with different
K and L; (b) Comparision between LD-Minv and D-Minv with different K and fixed L = 4; (c)
Comparison between LD-Minv and D-Minv with different L and fixed K = 5.
ziD-Mi∏y
-D-Minv
4 5 6 7 8 9 10 11 12 13 14 15
(c)
A.1.1 Experiments Results for LD-Minv
Settings. For LD-Minv, we adopt the square and rectangle matrix to test its performance. It should
be mentioned that our LD-Minv is adaptive to the shape of the input matrix. In general, the square
matrix can better test the performance of the algorithm, since the minimum singular value is near 0
in this case (Vershynin, 2010). And a large condition number of the input may make most numerical
matrix calculation methods unstable. For the square matrix A ∈ Rd×d , its elements are sampled
from i.i.d. Gaussian, namely Ai,j 〜N(0,1 /ʌ/d). For the rectangle matrix A ∈ Rm×n, its elements
11
Under review as a conference paper at ICLR 2021
Table 1: Inference time (seconds) of D-Minv/LD-Minv and exact Minv for different matrix size with
batch size 100. The efficiency advantage of our LD-Minv/D-Minv is obvious through all cases.
Matrix Size			10 X 15	50 × 75	100 × 150	200 × 300	500 × 750	1000 × 1500
L=	4, K =	4	0.0030	0.0070	0.010	0.025	0.40	1.23
L=	4, K =	8	0.0052	0.0088	0.016	0.047	0.56	2.22
L=	4, K =	12	0.0083	0.0133	0.019	0.055	0.64	3.08
L=	4, K =	5	0.0035	0.0050	0.013	0.025	0.38	1.38
L=	8, K =	5	0.0058	0.0076	0.014	0.051	0.55	2.42
L=	12,K	=5	0.0084	0.013	0.020	0.059	0.68	3.53
Exact Pseudo Minv			0.46	0.58	1.21	2.60	11.39	39.49
are also sampled from Aij 〜N(0,1∕√d), where d = max{m, n}. We adopt the SGD to optimize
the parameters. Note that our theoretical results in Sec. 4 hold for GD, but they can easily extend to
the SGD case. The learning rate is set to 1e-4, and it is reduced by half every 300 iterations. We also
fix the batch size as 100, while the number of iteration is set to 1200. Note that, in the experiments,
the adopted exact Pseudo matrix inverse (Minv) is implemented by Pytorch.
We adopt the following test loss to measure the performace:
1n	2
Test Loss :=最£ ∣∣AiX{Kmv,i}Ai - AillF,
n i=1
where X{Kinv,i} is the final output of D-Minv or LD-Minv. Here n = 100 is the size of test data.
With d = 100, we first test the influence of L and K, which correspond to the order of matrix
polynomial and the number of layers in our LD-Minv, respectively. Due to high precision of LD-
Minv and D-Minv, for visualization, we take a base-10 logarithm on loss in this experiment.
Results. From Fig. 1(a), we can see that the number of layer K plays a more critical role for LD-
Minv than the order L of polynomial. When setting K = 7, the log of the test error drops from -1.0
for L = 4 to -4.6 for L = 9. However, the log of the test error drops from 0.32 for K = 2 to -4.6
for K = 7, when fixing L = 9. Hence, for efficiency, we suggest utilizing large K and small L for
LD-Minv.
Compared with D-Minv, according to Fig. 1(b) and Fig. 1(c), LD-Minv can obtain a better test
performance even for large K and L, although D-Minv converges to Minv extremely fast in this case.
Our result demonstrates that learning can help LD-Minv obtain better performance on a problem-
dependent data distribution rather than merely fixing the coefficients ahead for time. We notice
that LD-Minv and D-Minv obtain a similar precision when K > 15, and the reason is that the
approximation error for D-Minv is in the order of 1e - 8 in this case, which lifts very limited space
for the improvement by learning.
Fig. 1(b) further verifies the importance of K . As shown in our theoretical results (Theorem 1 and
Theorem 2); large K indicates smaller generalization bound and higher probability to obtain the
linear convergence rate for Algorithm 1.
Another significant advantage of our proposed method is efficiency. D-Minv and LD-Minv can in-
verse the input matrix with high precision meanwhile with the low computational cost. The benefits
of D-Minv and LD-Minv are apparent when the matrix dimension is large. As shown in Table 1,
even for large K and L, the inference time of LD-Minv is less than a tenth of the time of exact
matrix inverse when the size is 1000 × 1500. It is well-known that the Minv and matrix multipli-
cation (MatMul) share the same computation complexity theoretically, while their speeds vary a lot
numerically. The computation of our method is mostly spent on MatMul, which can be easily accel-
erated by GPU and is much cheaper than Minv. MatMul and Minv only share the same complexity
asymptotically. The constants in the order O(∙) vary significantly, especially when matrix dimension
d ∞. Moreover, the general MatMul algorithm with a complexity of O d3 isn’t optimal, and
there exist algorithms that own a complexity of only O d2.3 .
To further investigate the importance of the learning procedure, we conduct the experiments on the
matrices with different condition numbers. Given the condition number κ ≥ 1, we generate the
data matrix by Ai = UiSiVi, where Ui and Vi are randomly sampled from a 100-dimensional
12
Under review as a conference paper at ICLR 2021
Table 2: - log10(Test Loss) (the bigger the better) on the test data for LD-Minv and D-Minv with
various condition number κ. D-Minv performs better when κ = 1. However, LD-Minv shows its
strong robustness in the more complex case κ 1. The network parameters. e.g., K and L, mainly
determines the final results of LD-MinV rather than the Condition number.___________________
Condition Number κ	1	2	3	4	5	10	50	100	1000
D-Minv	7.64	2.83	2.49	2.36	2.30	2.27	2.26	2.26	2.25
LD-Minv	5.95	5.01	4.19	3.62	3.49	3.30	3.21	3.22	3.21
Stiefel manifold. Here Si is a diagonal matrix whose diagonal entries are i.i.d. and obey a uniform
distribution U(1∕κ, 1) with distribution boundaries being [1∕κ, 1]. We let K = 4,L = 10 and
the batch size be 100. We show the results in Table 2. Not surprisingly, D-Minv suffers from
ill-condition problems, e.g., the large condition number deteriorates the performance of D-Minv.
However, LD-Minv is not sensitive to the condition number, and it can beat D-Minv easily when the
condition number is large. Note that all the results are reported on the unseen data.
A.1.2 Experimental Results for D-SVD
Settings. For D-SVD, we adopt the rectangle matrix to run the experiments. Similarly, our algo-
rithms also Work for the square matrix. For the rectangle matrix A ∈ Rm×n, its elements are also
sampled from i.i.d. Gaussian, namely Ai,j 〜N(0,1∕√d), where d = max{m, n}. We let m = 50
and n = 100, and utilize Algorithm 3 to solve our learning problem5. Similarly, we also perform
the batch training with batch size 30. The learning rate is set to 1e - 2 for D-SVD and 1e - 4 for
LD-Minv contained in D-SVD. We do not decay the learning rate during training, and let the number
of iterations for D-SVD and LD-Minv be 200 and 100, respectively. For LD-Minv in each layer, we
let L = 4 and K = 6.
Same as Sec. 3, we adopt the following loss to train our D-SVD:
1 n Ksvd	1
Training Loss :=	— XX
2 l∣Λi,k - Diag (Λi,k)kF —〈Mi, Ui,kV>®))，
where Ui,k and Vi,k are the k-th layer’s output of D-SVD, and Λi,k := Ui>,kMiVi,k. The rea-
son that we introduce the regularization term Ui>,kMiVi,k - Diag Ui>,kMiVi,k	is the non-
uniqueness. In general, any Pair (U, V) ∈ {(UR, VR) : M = USV>, R>R = RR> = l} can
maximize the function M, UV> . However, in order to obtain the exact singular vectors of M,
the matrix Ui>,kMiVi,k should have zero off-diagonal entries. Thus, the regularization term is
necessary for the training of D-SVD.
We use the following test loss to report the results:
1n 1	2
TeStLoSS := n £ (2 kAi,Ksvd — Diag (Ai,Ksvd)kF
n i=1
Mi,Ui,KsvdVi>,Ksvd
—
where Ui,Ksvd and Vi,Ksvd is the final layer’s output of D-SVD, and Λi,Ksvd := Ui>,Ksvd MiVi,Ksvd.
Note that we also utilize the following quantity to measure the test performance.
1n	2
SVDMSE := — £ 悒rt(Diag(U>KsvdMiVi,κ,vd)) - SVD(MiMF,
n i=1
where Sοrt(∙) is the operator that sort the input elements in the descending order, Diag(∙) returns
the collection of the entries at the diagonal of the input matrix, and SVD(∙) gives out the singular
values.
For convenience, we add the mean of the sum of SVD(Mi) to the loss in Table 3 to make the loss
non-negative. Although increasing Ksvd also can improve the performance in the non-learning case,
5We replace the GD part by SGD for the generality of the experiment.
13
Under review as a conference paper at ICLR 2021
Table 3: Training loss and SVD MSE on the test data for D-SVD with various depth Ksvd . The first
two rows are the quantities that measured without training which, to certain extent, are equivalent to
the results of the original optimization algorithm (Wen & Yin, 2013; Li et al., 2019). The last two
rows are the quantities measured after training on the same test data. The decline of the loss and the
MSE is obvious, which demonstrates the matters of learning.
Ksvd	1	5	10	20	30	40	60
Init Test Loss	44.9983	35.8998	26.6607	15.6132	10.3556	7.7057	5.0146
Init SVD MSE	0.8782	0.5721	0.3373	0.1502	0.0879	0.0568	0.0274
Final Test Loss	23.1838	0.9565	0.4043	0.2694	0.2018	0.1243	0.0305
Final SVD MSE	0.2528	0.0711	0.0626	0.0167	0.0064	0.0052	0.0029
learning can strengthen the role of Ksvd . Without training, the test loss for Ksvd = 60 is one eighth
of the loss for Ksvd = 1; in sharp contrast, the reduction can reach one thousandth after training.
Due to the introduced regularizer for the training loss, we can obtain a small SVD MSE rather than
the small difference between the sum of singular values.
During the training of LD-SVD, we observe that our results are robust to the setting of K and L.
In general, the forward process can converge even with very inexact matrix inverse (Li et al., 2019).
Moreover, our introduced learnable step size {Uu,,干Vk} further provides the freedom to adjust the
update direction for Uk and Vk. Thus, for efficiency, a relative small K and L is enough for each
layer’s LD-Minv.
A.1.3 Plug-and-Play: Differentiable Non-blind Deconvolution
Non-blind deconvolution (NbD) aims to restore the latent image z from corrupted observation y
with known blur kernel b. In this experiment, we consider a well-known sparse coding formulation:
y = b 0 Z + n = BW>x + n,
where 0 is the convolution operator, X and n are the sparse code and unknown noises, respectively.
B is the matrix form of kernel b, and W> is the inverse of the wavelet transform W (i.e., x = Wz
and z = W>X). We utilize the following optimization to perform the non-blind deconvolution:
12	>
m,ing(X⑶ :=2ky -Bzk2+λkχkι,	s.t. Z = W x,
where λ > 0 is the balance parameter. A usual way to solve this problem is linearized ADMM
(L-ADMM) (Lin et al., 2011), read as:
ak = Zk — αk(λk + β(zk — W>Xk)),	Zk+1 = (αk BB> + I)1 由 + αk B>y),
bk = Xk - YkW>(λk + β(zk+1 — W>Xk)), Xk+1 = sgn(bk) Θ max (bk — γk, 0),
λk + 1 = λk + β(zk + 1 - W>Xk+1),
(13)
where αk > 0, γk > 0 and β > 0 are penalty parameters, Θ is the Hadamard product, and λ is the
Lagrange multiplier.
Differentiable NbD. Inspired by the Eq. (13), we can easily obtain the learning-based non-blind
deconvolution method:
ak = Zk - αkek(λk + β(zk — W>Xk)),	Tk =LD-Minv (αkBB> + LCk)
zk+1 = Tk (ak + αkBTy),
>
bk = Xk - γkWk λk + β Zk+1 - W>Xk ,
Xk+1
ReLU (bk — ɪ)
- ReLU (-bk - ɪ),
∖λk+1 = λk + β(zk + 1 - W>Xk+1),
(14)
K
where SNbD := {Ik, Wk, αk, γk, β}kK=N1bD is the collection of all learnable parameters, Ik and Wk
are introduced learnable matrices (in the implementation, we choose them as the convolution layer
14
Under review as a conference paper at ICLR 2021
Algorithm 4 Joint Training for D-NbD and LD-Minv
Input: Training data {yi}in=1, number of iterations TNbD and Tinv, step sizes ηNbD and ηinv.
1:	Set xi,0 = yi,zi,0 = W>xi,0 and λ0 = 0.
2:	for t =1,…，Tsvd do
3:	Fix the learnable parameters SN(tb)D and train each layer’s LD-Minvs by Algorithm 1.
4：	Update SNbDI) = SNbD — ηNbD ∙。<⑴ _c 1Qn, where Qn is presented in Eq.(15).
[SNbD =SNbD]
5: end for
Output: {zi,k}in=,K1,Nkb=D1.
Table 4: Averaged PSNR and Inference Time(s) on the test images randomly selected from Schmidt
& Roth (2014). Here fL-ADMM denote the L-ADMM given in Eq. (13) with fixed αk, γk for all k,
aL-ADMM represents the L-ADMM with adaptive αk = max{0.98k α0, }, γk = max{0.98k γ0, }
with =1e-4. When D-NbD in Eq. (14) utilizes D-Minv and LD-Minv to perform Minv during the
forward propagation, we denote them by D-NbD(D-Minv) and D-NbD(LD-Minv), respectively.
Noise level	Metric	fL-ADMM	aL-ADMM	D-NbD(D-Minv)	D-NbD(LD-Minv)
1%	PSNR	26.13	27.50	28.97	30.00
	Time(s)	7.83	21.48	1.27	1.20
2%	PSNR	24.06	25.33	27.82	28.11
	Time(s)	5.26	14.35	1.28	1.19
3%	PSNR	23.38	24.67	26.94	27.13
	Time(s)	5.72	13.90	1.28	1.20
with compatible size), Ck is the learnable parameters for each layer’s LD-Minv, and ReLU(a) =
max{a, 0} is the Rectified Linear Unit (ReLU) function. Note that
sgn (b)	max (b — γ, 0) = ReLU (b — γ) — ReLU (—b — γ).
Eq. (14) can be implemented by a DNN. Given the training images {yi}in=1, we adopt the following
loss to train our D-NbD:
1 n KNbD
Qn (SNbD) := 4E E (ox,®,w>~,k)),
nKNbD
i=1 k=1
(15)
where xi,k is the k-th layer’s output of D-NbD. The training process for D-NbD is given in Algo-
rithm 4.
Settings. The training and test images come from Schmidt & Roth (2014), in which 300 (random
selected) have been used for training and the others are used for test. We add different levels of the
Gaussian noise to generate our corrupted observations {yi}in=1. We set the patch size as 160 × 160,
and let the batch size be 15. Finally, Adam is adopted and executed 1000 epochs for learning rate
ranging from 1e-4 to 1e-6 (it is reduced by half every 140 iterations). The observations {yi} are
corrupted by blur kernel with size ranging from 17 × 17 to 57 × 57 and the levels of Gaussian noise
varying from 1% to 3%.
Results. From Table 4, we find that fL-ADMM is much more efficient than aL-ADMM. fL-ADMM
does not need to perform the inverse balbala since αk, γk are fixed, which however is also why fL-
ADMM performs poorly. It can not find the universal αk, γk for all test images. aL-ADMM can
perform better with adaptive step size, but it needs exact Minv in each iteration. Compared to them,
D-NbD outperforms L-ADMM by a large margin in terms of speed and performance. Note that, for
one iteration, the computation complexity of fL-ADMM is much lower than the other three since
it only involves the MatMul. However, it consumes more than 600 iterations to obtain a relatively
good solution. In fact, just comparing the Minv time in one iteration for aL-ADMM and D-NbD, our
methods’ time is only almost half of that of exact Minv in PyTorch. Fortunately, the learnability ofD-
NbD helps us obtain a better or equally good solution using one order-of-magnitude fewer iterations.
Hence, the total time can only be one-tenth of that of aL-ADMM. Moreover, the learnability of LD-
Minv further introduces more flexibility, which allows D-NbD(LD-Minv) to achieve better results
within fewer layers. Thus, D-NbD(LD-Minv) is more efficient due to less calculation.
15
Under review as a conference paper at ICLR 2021
A.2 Discussion
Not only D-NbD, based on the LD-Minv and D-SVD, but there are also rich potential applica-
tions, including image recovery or denoising, numerical algorithm acceleration, blind deconvolu-
tion, sparse coding, subspace clustering, etc. All these compelling applications may be beyond this
paper’s scope, and we leave them as future work. Moreover, due to the differentiability, D-SVD can
help us design more abundant singular-value-related training loss for DNN, which is impossible in
the previous non-differentiable situation.
B Notation
Positive definite matrix A is denoted by A 0. Transpose of the matrix A ∈ Rm×n is A> . We de-
note by AT and At the matrix inverse and Moore-Penrose inverse, respectively. The Euclidean in-
ner product between two matrices A ∈ Rm×n and B ∈ Rm×n is defined ashA, Bi := Tr(A>B),
where Tr(∙) is the trace of a matrix. κ(A) := ∣∣AtkkAk is the generalized condition number of a
matrix, where ∣∣∙∣ is the spectral norm. We denote by σ(A) the singular values vectors (no need to
be ordered) of the matrix A. Similarly, we denote by σ+(A) the positive singular values vectors.
Let ∣∣∙∣f be Frobenius norm. Given a differentiable function the first and second derivative of F
at Y are denoted by: linear operator Dχ=γ]F(∙) : Rm → Rn := (dFXY)) (∙) and quadratic
operator D^=Y]F(∙, ∙) : Rm X Rm → Rn := (IXFi(Y)) (∙, ∙), respectively, where Xi is the i-th
entry of X. We also denote by F-1 the inverse of the function F and let the composition map
f ◦ g(χ) := f (g(χ)). Θ is the Hadamard product and [K] = {1,2,…，K}.
C Additional Theoretical Results
In this section, we provide the additional theoretical results of D-Minv and D-SVD. We start by a
warm-up case for D-Minv.
C.1 Results for LD-Minv
C.1.1 Warm-up: One Parameter for One Layer
As a warm-up case, we consider only to learn one coefficient for each layer, as a special case of
learnable D-Minv:
Xk+1 =Xk LX-1 Elk + CkEkL , Ek =I-AXk,	(16)
where Ck ∈ R are the learnable coefficients. By setting X。= *A>, where c0 > ∣∣ A∣2, and let
Ck = 0 for all k, by Lemma 1, we know that AAt - AXk = e0Lk. One important question is
if we let the coefficient of EkL to be learnable, by training, will D-Minv obtain a faster convergence
rate?
In general, previous works show that learning-based methods share the same convergence order with
their fixed version but with a better statistical constant. Interestingly, beyond the constant, we found
that D-Minv can improve the order of convergence by learning.
Lemma 2 (Learnability Matters). Assume these is only one training data A. Provided that X0 =
Cl A>, where c。> ∣∣ A∣2 ,then we have:
ek+1 = (1 - Ck) ∙ eL + Ck ∙ eL+1, where e® = IlAAt- AXk∣∣, cmd k ∈ [K],L ≥ 4.
Moreover, the gradient of L w.r.t. Ck is negative, i.e., D[Ck]L < 0 for all Ck ∈ [0, 1 + ], where
> 0 depends on ek-1. Additionally, D[Ck]L ≈ 0 for k ∈ [K - 1] when Ck ∈ [1 - , 1 + ].
Starting from 0, according to the gradient’s negativeness, the coefficients Ck for k ∈ [K - 1] will
converge to the range [1 - , 1 + ] when adopting a proper step size for GD. In this case, i.e., Ck
16
Under review as a conference paper at ICLR 2021
are around 1 for all k, the sequence {Xk} converges to the Moore-Penrose inverse m L-th order,
where L < L ≤ L + 1. In other words, by learning from one data, D-Minv finds a way to improve
the order of convergence for any input. Remarkably, Ck = 1 for all k is not a global or even local
minimum for the training loss L. For the learnable coefficient CK at the last layer, satisfying its first
order condition usually implies obtaining the exact solution, i.e., eK+1 = 0. In summary, not only
improve the convergence speed, learnable D-Minv makes the exact fitting come true.
C.1.2 Discussion for Assumption 1
In general, the bounded assumption for the singular values the is weak and easy to satisfy. For ex-
ample, singular values of the matrices with i.i.d. zero mean and unit variance entries asymptotically
obey the Quadrant LaW (Dozier & Silverstein, 2007; Shen, 2001), i.e., σ 〜∏√4 - σ2, σ ∈ [0,2],
which implies the singular values have an upper bound. On the other hand, a well-known fact from
the random matrix theory is a zero mean and unit variance random matrix is non-singular with
high probability. Even for the square matrix A, with probability at least 1 - δ, we still we have
σmin(A) ≥ δd-1/2 (Rudelson & Vershynin, 2008), where σmin(A) is the smallest singular value of
the matrix A.
C.1.3 Lipschitz Smoothness before Generalization
We would also like to remark that our generalization result can easily be extended to the stochastic
case. The proof is the same as the Lemma 4.3 in Ji & Telgarsky (2019) or Theorem 3.3 in Cao &
Gu (2019).
In general, smoothness plays an important role in the generalization analysis. The covering number
of a smooth function usually is small. We start by showing the local smoothness of learnable D-
Minv.
Proposition 2 (Local Lipschitz smoothness). Define a set of coefficients collection:
C* = C∣∀C{k,i} ∈ C,
(k, l) ∈ [K] × [L],	L ≥4.
Let PA = AAl Given any coefficients collection C ∈ C, we denote by Gk(∙) the mapfrom PAEk
to PAEk+1 in(3), i.e., Gk (E) ：= PA-PA(I - E)(PL=01 C{k,i}El), and let Hk (∙) ：= R◦Gk-1 ◦
…。Gk (∙), where R(E)= P^L ∣∣W^E^ AIlF. Suppose ∣∣A∣∣2 ≤ 1 and ∀k IlWk II= O(1) ,then
we have:
Hk	Eb	- Hk	Ee	≤	2IIPA Eb	-	PA EeII	,	∀k	∈	[K],
where E and E are with the same size and same norm upper bound as Ek, d is the dimension of
positive singular value vector σ+(A), and ∣∣∙∣* is the matrix nuclear norm.
In general, the composition of two Lipschitz continuous functions leads to a worse Lipschitz
constant. However, our LD-Minv shares a consistent constant for all layers. This is important
since the layer-wisely local smoothness usually implies a small covering number of the whole D-
Minv (Bartlett et al., 2017; Wei & Ma, 2019), which results in a tight generalization bound.
C.2 Results for D-SVD
We characterize the generalization performance of D-SVD trained by GD in this theorem. First of
all, we present several definitions. Let
.~ .
Mn(t,C)
1
nKsvd

n Ksvd
X X Mi,Ui,kVi>,k,
i=1 k=1
1	/ T	Z∙—< ∖	. 1	1-∣	i` . Λ	.	i` 1 ʌ	n X ʃ-ɪ-ʌ	1 .1	T 1 ʌ ɪ	r ∙	♦	1	1	I ʌ	.
where (t, C) are the collection of the parameters of D-SVD and the LD-Minvs in each layer. Denote
by f (U | V, M,tk, Ck) the operations in Eq. (12) that map Uk to Uk+ι, i.e.,
. . ~ .
Uk+1 := f (Uk | Vk, M, tk, Ck),
(17)
17
Under review as a conference paper at ICLR 2021
where Ck, Ck) ∈ R X RLKinv are the learnable parameters ofD-SVD and LD-Minv at the k-th layer
of D-SVD. We define the coefficients set:
where
1
min-----
i kPik,
in which Pi = MiVkUk> - UkV>Mi>,	∀i ∈ [n],
Proposition 3 (Covering Number of Single Layer). Denote F the class of function in Eq. (17), i.e.,
Fk := {f (Uk | Vk, M, ik, Ck) ： (tk, Ck) ∈ Ck}∙
Then we have:
+ ln(㈣
lnN(Fk,3k∙k)) = O LKinv ln

Theorem 3 (Generalization Bound for D-SVD). Denote by PM the distribution of Mi and let
d := max{m,n}. Suppose that kMik ≤ 1, ∀i ∈ [n] and Kinv = Ω(max{nd, K2vd}), then with
probability at least 1 一 δ ,the iterate t(t) ofAlgorithm 3 holds that:
M% hMn W)i ≤ Mn W)) + 0 (rKsndL) + θ rn^
for t = 0,1,…，Tsvd, where EM〜PM [Mn] is the expected value of Mn under the probability
measure PM and O(∙) hides log factors.
D Omitted Proof for Fixed D-Minv in Section 2
D.1 Proof of Lemma 1
We first verify that e0 < 1, suppose rank(A) = r.
e0 =IlAAt 一 Aχ0 J J = uu> _ 1 U∑2U> =	Ir - 1 ∑2 < 1,
where A = UΣV>, Ir ∈ Rr×r is the identity matrix, and the last inequality comes from the fact
C > 1 k Ak2. Form Eq. (2), we have
AAt- AXk+ι = AAt(I- AXk+ι) = AAt (I- AXk(X Ek))
L-1	L-1	L-1
I-(I-Ek) XElk	AAt I-XElk+XElk+1
l=0	l=0	l=0
=AAt(EL) = AAt(I - AXk)L = (AAt- AXk)L,
i.e.,
JJAAt -AXkJJ = JJAAt -AXk-1JJL =e0Lk.
We finish the proof.
D.2 Proof of Proposition 1
Note that for any matrix X, the matrix polynomial of X share the same column and row space
with X. In general, the iterative equation (2) does not change the column and row space from the
begining. We finish the proof by induction. The conclusion is obvious for X。due to X。= ɪ A>.
c0
We assume the proposition holds for Xk, then:
AtAXk+1 = AtAXk LX-1 Elk
l=。
Xk X Elk = Xk+1,
18
Under review as a conference paper at ICLR 2021
and
Xk+ιAAt = Xk(X Ek) AAt = Xk(X (I - AXk)) AAt
=XkAAt (X (I - AX®)) = Xk+ι,
Hence, we can conclude that:
AtAXk = Xk, XkAAt,	∀k.
We finish the proof.
E	Omitted Proof for Learnable D-Minv in Sections 4 and C
In the theoretical part, we consider a more general training regression loss:
nK
Ln(C)= 2nK XX ||V{k,i}(AiX{k,i}Ai - Ai)IIF，	(18)
i=1 k=1
where C = {C{k,l} , (k, l + 1) ∈ [K] × [L]} is the collection of all learnable parameters, V{k,i}
is a diagonal matrix whose diagonal entries are i.i.d. and obey a zero mean bounded distribution
(w.l.o.g. We let the bound be ±bv).
Note that V{k,i} ’s are fixed during training and testing. Introducing V{k,i} has two advantages.
One is re-weighting the error term. As we can see from the fixed D-Minv, the error term decays
exponentially. In that way, the smallest positive singular value dominate the gradient flow. However,
for LD-Minv, we hope that the gradient flow comes from all positive singular values’ loss during
training. The random re-weighting strategy is a good choice for numerical calculation when the
order of singular values is unknown. Another advantage is that the introduced V{k,i}s, with very
high probability (see Claim 1 in Sec. E.2), make the landscape of the training loss locally strongly
convex, which is an excellent benefit from the optimization perspective.
Notably, by setting the diagonal entries of V{k,i} as the independent symmetric Bernoulli random
variables, the training loss here in Eq. (18) will degenerate into the loss given in Eq. (4). Hence, all
the theoretical results in this section proven for Eq. (18) also hold for Eq. (4).
It is worth mentioning that the weight matrices {V{k,i} } here has nothing to do with the learned
singular vectors {Vi,k} for D-SVD. Please distinguish them.
E.1	General Analysis
Before start the proof, we first make some general analysis. It is easy to see that Proposition 1 also
holds for learnable D-Minv. Thus, we can observe that:
XkElk = XkAAtElk,	∀(k, l).
Hence, we obtain an equivalent form of Eq. (3):
Xk+1 = Xk X C{k,l} Ee lk , Eek = AAt - AXk, L ≥ 4,	(19)
Due to the invariance of the row and column space, we can only concern on the singular values
vectors and rewrite Eq. (19) as :
ek = 1 - a xk, L ≥ 4,	(20)
where 1 ∈ Rd is the all-one vector (d is the dimension of the positive singular vector σ+ (A)), and:
a = σ+(A), Xk = σ+(XQ, e& = σ+ (Ek),	或=^⅛ 0 ∙： ∙ 0 e% ;	(21)
l
recall that σ(A) the singular values vectors (no need to
19
Under review as a conference paper at ICLR 2021
Remark 4. We only utilize Eq. (19), the equivalent form of Eq. (3), in this section for the conve-
nience of theoretical analysis. For implementation, we still use Eq. (3) and do NOT calculate the
pseudo inverse At throughout learning.
Now we continue the derivation:
ek+1 = 1 — aΘ Xk+1 = 1 — aΘXk Θ
1 —(1 -ek) Θ (X C{k,i}
l=0
L-1
— X C{k,l}elk+1
l=0
(22)
eL + (I- Ck,0)1 + (X (C{k,l-1} - C{k,l})ek) + (C{k,L-1} - 1)eL
= ek + Ek ck ,
where e0k = 1,
Ebk ∈ Rd×(L+1)
(23)
and
ck ∈ RL+1 := 1 - Ck,0, C{k,0} - C{k,1} ,
We define:
1K	1
'(C, A) :=2 E ∣Vk(AXkA - A)kF = 2
k=1
…，C{k,L-2} - C{k,L-1}, C{k,L-1} - 1]	∙
K
X Vj Θ (a2 Θ Xk — a)
k=1
1K
2 X kvk
j=1
(24)
Θ a Θ ek ∣22 ,
(25)
where C = {C{k,l}, (k, l+ 1) ∈ [K] × [L]} is the collection of all learnable parameters for D-Minv,
Vk is a diagonal matrix whose diagonal entries vk = diag (Vk) are i.i.d. and obey a zero mean
bounded distribution (w.l.o.g. We let the bound be ±bv), and ∣∣∙∣2 is the vector '2 norm; recall the
definitions of a, ek and Xk from Eq. (21). Note that vk , ∀k is fixed during training.
From Lemma 1, we know that AAt - AXk decays extremely fast. We can run fixed D-Minv for
several times before training learnable D-Minv. In this case, X0 is the output of the K-layer fixed
D-Minv. Hence, without loss of generality (w.l.o.g.), we suppose the following assumption holds in
this section.
Assumption 2 (Well-Bounded E0). Assume ∣∣E o∣∣ = ιιeok∞ ≤1.
We first provide several propositions of learnable D-Minv.
Proposition 4 (Upper Bound for Perturbation). Suppose that ∣c{k,l} - 1I = δ < 8, ∀ (k, l + I) ∈
[K] × [L], then we have:
IIE k∣∣ = kek k∞ ≤ eLk + 18δ,	∀k ∈ [K].
2
2
Proof. We first show that ∣ek ∣∣∞ < 11 for all k by induction. By Eq. (22), for k = 0, we know that:
keι∣∞ ≤ ∣eok∞ + ∣∣E0C0∣∣∞ ≤ keok∞ + 卜/£ | [Eo]i,j gok∞ ≤ ∣eok∞ + 2δ < ∣.
Now, we assume ∣∣ek∣∣∞ < 2 holds. Then,
Ilek+1k∞ ≤ llekk∞ + ∣∣E k ck∣∣∞ ≤ kek k∞ + (maxd XIhEkiijj kck k∞ ≤ kek k∞ + 2δ < 1.
Since δ < ɪ, we have 2δ < ɪ .By Lemma 3, we can conclude that:
Lk	1
Ilekk∞ ≤ eL +(1 + e)2δ, where " > e → 0 as k →∞.
We finish the proof.
□
20
Under review as a conference paper at ICLR 2021
Proposition 5 (Upper Bound First-Order Derivative). Suppose that ∣C{k,i} 一 1∣ = δ < ɪ, ∀ (k,l +
1) ∈ [K] × [L] and ∣∣a∣∣∞ ≤ 1, then we have:
∂'(C, A)
∂"
≤ 4 (3) bVd.
Proof. Note that:
∂'(C, A)
∂C{k,l}
X
k=k ∖
∂'(C, A)	∂e^
de^	, de^-1
。…。普Χ∖
dC{k,l}/
By Eq. (22), we also have:
∂ek+ι
∂ek
Diag (LeLT + Ekc,,
where Diag(e) is a diagonal matrix with the diagonal entries being e, and:
E'k ∈ Rd×(L+1) := [0; ek;…；Lef-1];
here 0 ∈ Rd is the all zero vector. Hence, it holds that:
∂ ek+ι
∂ ek
=∣∣LeLT + EkCkL ≤ LeLT +
ek
(1 一 ek )2
• δ,
(26)
where <¾ = Ilek∣∣∞. By Proposition 4, we know that ek ≤ eL + l7δ < 3. Thus, we have:
∂ek+ι
∂ek
1
≤ 4,
(27)
It is easy to see:
∂'(C, A)
∂ek
江
k=k ∖
∂'(C, A)
de^
dek	∂ek+ι ∖
,de^-1 ° "0	dek / .
Hence, we have
∂'(C, A)
de^
K
¾ / I I	/	∖ I I ∖	1. i `.	一c I-
E(M Θ a Θ (Vk ® a © e^) ∣ ∣ 2)∙ 4 - <2bv√d.
ʌ
k=k
According to Eq. (22), we notice:
∂ek+ι
∂C{k~}
= l∣ek+1 - ek∣∣2 ≤ 2∣∣ek∣∣2.
2
Combing all things together, we conclude that:
∂'(C, A)
∂C{k,l}
κ	/ 1 ∖ 1
≤ X4I(4bV√d∣∣ek∣∣2) ≤4(1)bVd.
k = k
We finish the proof.
Proposition 6 (Second Order Approximation). Define a set ofcoefficient collection around 1:
{c∣∀ C{k,i} ∈ C, ∣C{k,i}
(k,l) ∈ [K] × [L], L ≥ 4.
≤
2
□
Forany Ci, C2 ∈ C*, we have:
'(C1, A) = '(C2, A) +(阳CC A), Ci - C2,+ O(bvdKL)∣Ci - C212.
21
Under review as a conference paper at ICLR 2021
Proof. Note that for any C ∈ C*,we have:
U24(C, A)	/ U'(C, A)
UC{k,l}UC{k∖l,} = ∖
_ / U'(C, A)	Ueκ
∖	Ueκ	, UeK—1 °
/ Ueκ	U2'(C, A)
+ \西启'UeKUeK UC{k,,，,}/
_ X	/ U'(C, A) Ueκ
^ r	u U eK	,U eK—1
k=max{k,k0} '
∂ 2eκ
UeK	, ∂C{k,i}
U2eκ-1
UC{k,ιyUC{k ,10}
U eκ	∖
dC{k0,l0}
/	Ueκ	U24(C, A)	Ueκ
+ ∖UC{k,i}, UeKUeK UCf
∂'(C, A)	∂2eκ
∂eκ-i	∂ eκ—ι
UeK	，UeK— 1Ueκ-ι ∖βc{k,i}' UC{k0,l0}
U2ek l 1 ( Ue ^	U e^ ∖∖ z . m
。…。τHτ1	,μ k )+ ∆(k,k',l,l'),
UekUek \UC{k,l} UC{k',l'}J /
+
where we let ek+ι := '(C, A) with a little abuse of notation, and:
(	0
∆(k,k∖l,l∖
∂'(C,A)	∂eκ C C d(ek+1—ek)
∂eκ , ∂eκ-ι c，，，c ∂C{k,ιo}
if (k,l) = (k Y),
otherwise.
Hence we can get the upper bound for the entry dc]：'(∂⅛AL 门.From the proof in Proposition 5,
we know that:
U ek + 1
U ek
Note that:
Γ. J. I J
CU ≤ d(1)	，and∣⅛⅛
ʌ --
k —k+l
∂2ek+ι
∂ek∂ek
TDiag (L(L - 1)eL-2 + E"
1
≤ 4,
where TDiag(e) is a three-dimensional diagonal tensor with the diagonal entries being e, and:
EIk ∈ Rd×(L+1) := [0; 0; ek；…；L(L - 1)eL-2].
Hence, similar to Eq. (26), it holds that:
U 2ek+1
UekUek
=∣∣L(L - 1)eL-2 + Ek Ck ∣∣	≤ L(L - 1)ef-2 + 1 ：2ek -；ek ∙ δ< 3,
11	ll∞	(1 - ek)
where ek
∣∣efc∣∣∞ < ɪ by Proposition 4. Moreover, we also note that:
∂2'(C, A)
∂eκ ∂eκ
IlDiag (v Θ a)k2 ≤ K.
We also have:
Hence, we have
∂'(C, A)
κ
≤ X (llv^ θ a θ (v^ θ aθ ek) L) ∙ 4k-k < 2bU.
ʌ
k=k
U24(C, A)
UC{k,l}UC{k,,l,}
/ 1 ∖ 2K —(k+k0) + l+l0	/ 1 ∖ 2K —(k + k0)+l + l0-1
≤ 4bUd (4)	+ 3 ∙ 2b24d (4)	+ ∙ ∙ ∙
/1、2K—(k+k0) + l+l0	K —max{k,k0} / 、2K—(k + kz)+l + lz —k
= 4bU d(4)	+24b2 d	X (4)
k = 1
< 29bU d,
where we omit the term ∆(k, k 0, l, l 0) since it is much smaller than the other sum term. Thus, we
can conclude that
∂ e ʌ
U ek
∞
U24(C, A, V)
-UCUC-
U22(C, A, V)、
UC{k,l}UC{k0,l0}) (kMl,l,)
≤ 29b2 KLd.
-- U
F
(28)
F
22
Under review as a conference paper at ICLR 2021
Combing all things together, we can see:
'(Cι, A) = '(C2, A) +(阳* A), Ci - C2)+(Ci - C2, UCCA) (Ci - C2)),
∂C	∂C∂C
where the first equality comes from the second-order Taylor expansion, Cδ := Ci + δ(C2 - Ci)
and δ ∈ (0, 1) depends on Ci and C2 . Due to Eq. (28), we conclude:
'(Ci, A) = '(C2, A) +(d'(CC A), Ci - C2,+ O(bVdKL)∣∣C1 - C2|直
We finish the proof.	□
E.2 Proof of Theorem 1: Convergence Rate
Proof. Let Us assume that C(t) ∈ C* for all t ∈ [T], where
C* ：= {c∣∀ C{k,i} ∈ C, ∣C{k,i} - 1∣ ≤ 8},	(k,l) ∈ [K] × [L], L ≥ 4.
We will verity the boundness at the end.
We first provide the lower bound for the gradients.
Claim 1. Under the same setting of this theorem, given:
1n K
Ln(C)：= 2 XX ∣∣V{k,i} (AiX{k,i}Ai- Ai) ∣∣F.
i=i k=i
Suppose K = Ω (nd£%), then with probability at least 1 一 exp (-O (K∕(bVba))), We have:
∣∣D[C=C(t)]Ln∣∣2 ≥ 等Ln(C).
Proof. First of all, we let
υ{k,i} ：= v{k,i}	ai	e({tk),i},
where e({tk),i} is the k-th layer error term of learnable D-Minv with the equivalent form Eq. (20), the
subscript {k, i} indicates the input is Ai and k-th layer, and ai = σ+ (Ai). We already have:
D[C=C(t)]Ln = Xn XK *
U{k,i}, "ɪf" + .
i=i k=i
Derivative for the last layer: We now consider the derivative for C{K,0}, the first entry of
D[C=C(t)]Ln. According to Eq. (22), we notice:
d ek+i = el+i
∂C{k,l} = k
—
Hence, we have
d (υ{K,i}) —v	CfIC (α (t)	lʌ URd
∂C{K,0} = v{K,i} θ ai θ (e{K-i,i} - U ∈ R .
Then, we can get
D[C{κ,0)=c(K,o)]Ln = X 卜KE ¾⅞): = X Mκ,i})>(υ{κ,i} ® ai ® (e{K-i,i} - 1))
where υ{K,i} = v{K,i}	ai θ e({tK) ,i} . Note that each entry of v{j,i} obeys mean-zero bounded
distribution; hence, it is a mean-zero b2 -SUb-GaUssian variable (subG(0, b)). Then, we have
工， 、>/	/	、、	， 一	C	4b2b2 /ʌ,,	“2
XMK,i}) (υ{K,i}θaiθ(e{K-i,i}- 1))~subG(o,σK),	where σK>X∣∣υ{K,i}∣∣2
i=i	i=i
23
Under review as a conference paper at ICLR 2021
where the lower bound comes from the assumption Ilail∣∞ > ba, and ∣∣e{K-ι 讣 一 1∣∣	> 3 (by
Propostion 4). Hence, we have:	∞
D[C{κ,0)=c(t)c,}]Ln 〜SUbG (0,σK),	Where σK > 4b9ba (XMK,iW2
and
(d[C(k 0)=C(t)0)1Ln) ~subE (σK，(°e)k), where (σE)κ ≥ σK,
here subE(a, b) represents a sub-exponential variable with mean being a and sub-exponential norm
being b. The above conclusion comes from the fact “sub-exponential is sub-gaussian squared” (see
Lemma 5.14 in Vershynin (2010)).
■ -¼	♦	C 八	八	■	TL T	∙ 1	.1	1	i' /-«	i' 7^	T7 TL T .	. 1	.
Derivative for the other layer: Now, we consider the derivative for C{^ °} for k < K, Note that:
nK
D[C{k,0} =c(k),0}]Ln = XX V{k,i},
{ ,}	i=1 k=k ∖
d(υ{k,i}) ∖ X X /
"/=S SI,
∂(υ{k,i})	d (eU,i}) ∖
◦ ∙ ∙ ∙ ◦	.
de{k-1,i}	dC{k,0}	/
We note that:
∂ θυ{j,i})	= UDiag (v{j,i} θ ai) Il < bv,
e{j-1,i}
and ll ⅞+1 ll ≤ 1
The shrink property comes from Proposition 5.
leading term of the derivative for C{^ 0} is
Due to the exponential decay term H d∂e+1 ll, the
X (υ{^,i},
i=1
dC{^,0}
,
1	∙ . . 1	CIl 7 - f T T	∙ ∙ 1 . .1	∙ .1 1 . 1	1
where we omit the term for all k > k. Hence, similar to the case in the last layer, we have:
D	(t) Ln 〜SubG (θ,σ∣), where σ∣
[C*,0}=C{^,0}]	' k	k
›也
9


and
.[c{£0}=c(k),0}]
∣
~subE 卜∣, (σE Q ,
where (σE )^ ≥ σ∣.
Sum the square of derivative: Note that, we have:
D[C=C(t)]
∣
Then by Bernstein-type inequality for the sub-exponential random variable, with probability at least
1 一 exp (-O(K/(b∣ba))), it holds that
K
j=1	D[C{j,0}=C{(tj),0}]
IJvba
> ɪ
Ln(C).
Till now we have completed the proof for one particular vector collection {υ{k,i}}. In general, when
(K/(b∣ba)) is large enough, we can prove that the above inequality holds for arbitrary {υ{k,i}}
with high probability. Taking -net over all possible vectors {υ{k,i} }, then applying union bound,
the above inequality holds with probability at least:
1 一 exp (-O(K∕(b∣ba) 一 d ln(d))),
where the e-net has nothing to do with K and n since any υ{k,i} is contained in a '∞ ball with the
radius less than I)V/3. Due to the assumption that K = Ω (db∣ba), we can conclude that the desired
result holds for all choices of {υ{k,i}}. Now, we finish the proof.	□
24
Under review as a conference paper at ICLR 2021
We continue the proof for the convergence. Recall that
nK
Ln(C) := 2nκ X X Il v{k,i} (AiX{k,i} Ai- Ai) ∣∣F,
i=1 k=1
By Proposition 6, we have:
Ln(C(M) = Ln(C⑴) +( dLn(C㈤), c(t+1) - C⑴)+ o(bV dL)∣∣c(t+1) - C㈤ ||2,
where the order of K is 0 than 1 since We have a factor -^ compared to the function in Proposition
6. Since we adopt the GD method, we have
i.e.,
C(t) - C(t+1) = η
∂Ln(C(t))
∂C
Ln(C(t+1)) = Ln(C⑴) - η∣∣ dLn(Ctt) ∣∣2 + η2o(bVdL)∣∣ dLnCP
By Claim 1, when K = Ω(d∑V%
have:
(29)
,with probability at least 1 - exp (-O(K/ (b2ba))), we already
∂Ln(C(t))
-∂C-
2b2ba
3nK
Ln(C).
2
≥
2
By our choice of η = θ(l∕bVdL), we conclude
Ln(C(t+1)) =	1
笫 A").
-Ω
Hence, in order to achieve accuracy , we require
T
给 ln(1)Ln(C⑼) = OfdL⅛Kn
bv2 b2aη	b2a
ln(，) = θ(dLKnb-2 ln(1∕e)),
where the first equality comes from our choice of η.
Verify the boundness: At last, we verify that C(t) ∈ C * for all t ∈ [T ]. Form Eq. (29), we have:
η dLnCC^	≤ 2(Ln(C(t))-Ln(C(t+1))),
then:
C(T) - C⑼∣∣	= ηX dL*㈤)
∞	d C
t=1
≤η
∞
X ∂Ln(C(t))
t=1 ~^C~
T
≤ηX
∂Ln(C㈤)
-∂C-
T ___________________ ____________
≤ p2ηX JLn(C⑴)-Ln(C(t+1)) ≤ ,2ηTLn(C(I)) ≤
t=1
2nK
VW
Ln(C(0))Ln(C(1))
O d d√n ln(1/e)A
Ol K 2α-1∕2「
where the last equality is from the fact ∣∣C(0) - 1∣∣∞ < K-α, which implies Ln(C(O)) = O(-K2a)
according to Proposition 4. By setting large enough K, we can easily have
∣∣C(T)- C(O) ∣∣∞≤ 116.
Recall that our setting for C(O) in Algorithms 1, we have ∣∣C(0) - 1∣∣	< 击. Hence, we can
c°ncludethat	∣∣C(T)-1∣∣∞ ≤ ∣∣C(T)- C(O) ∣∣∞+1∣C(O)-1∣∣∞ ≤ 1.
We now can conclude C(t) ∈ C * for all t ∈ [T ], and finish the whole proof.	口
25
Under review as a conference paper at ICLR 2021
E.3 Proof of Proposition 2: Local Lipschitz Smoothness


Proof. Note that PAEk = Ek, where Ek comes from Eq. (19). By Proposition 4, we know
∣∣E k∣∣ < 3, hence, by the “same norm upper bound” assumption, we have:
PAE卜 3, and ∣∣PAE卜 1.
By the mean value theorem, we have:
^ r≤
^

Hk E -Hk E =D[Ek=Eδ]Hk E-E ,
r≤ _ / r≤ ^
where Eδ = E + δ E - E for some δ ∈ (0, 1). Then
Hk (E)I ≤llD[Ek=EMlHl(E - E)IL
where |卜|| * is the nuclear norm. Hence, the LiPschitz constant of Hk is bounded by l∣D[Efc=e^ ]Hk∣l.
By the chain rule, we have:
D[Ek=Eδ]Hk =
k=k
K ∂ R
dGk-ι
dGk-I (G^-2,δ)	∂Gk(Eδ)
----TΓ÷----- ◦ ∙ ∙ ∙ ◦—————
d G^-2
∂E
where G^ § ：= Gk ◦ ∙ ∙ ∙ ◦ Gk(Eδ). Then, it holds that:
D[Ek=Eδ]H
ʌ
k = k
(G^,δ)	dG^-1 (G^-2,δ)
d G^-ι
dG^-2
∂Gk(Eδ)
∂E
K ∂ R
On one hand, for any k ≥ k, similar to Eq.(26) and Eq. (27), we have:
dG^+ι (G^,δ)
dG^
≤ LgLT +
g^
~2 ∙ δ <
4,
1
Given kPAEδ k < 3, it is easy to see that ∣∣G^ §|| < ∣ for all k ≥ k by Proposition 4, then:
dR (G^,δ)
dG^-ι
= ∣∣v^ © a © (v^ © a © σ+(G^,δ))∣∣∞ = O⑴.
where g^ = ∣∣G^ §||. Combing all things together, we can conclude that:
K
D[Ek=Eδ]Hkll* ≤ CX
ʌ
k = k
K-k
≤ 2,
where C > 0 is some proper constant. We finish the proof.
□
E.4 Proof of Theorem 2: Generalization
Proof. When let ∣∣C(t) 一 1∣∣ = O(K-α), we can easily conclude that Ln(C(O)) = O(小2一) by
Proposition 4. By the last part of Theorem 1's proof, We know that C(t) ∈ Cd for all t ≤ T, where:
Cd* ：= C： ∣∣C 一 C(0) ∣∣F ≤Oe
K 2α-1∕2
26
Under review as a conference paper at ICLR 2021
Part One: We first find the upper bound for the empirical Rademacher complexity Radn(L) of our
loss function class L (see Section G for definitions), where
Ln := {2'(C, A): C ∈Cd}.
See Eq. (25) for the definition of '(C, A). To bound the Rademacher complexity of L, We invoke
covering numbers by Lemma 5. First of all, we can easily verify the condition of Lemma 5 by
Proposition 4 and Proposition 2. Due to Lemma 5, We have:
ln Na, X 2ek ,L2(Pn, k∙k) ) ≤ X ln N(Gk,q ,L2(Pn, k∙k))+ln N(R, e®, L2(Pn, ∣∣∙∣∣)),
k=1	k=1
Where We refer to Proposition 2 for the definition of Gk and R. We first note that the term
lnN(R, ek, L2(Pn, k∙k)) is simply 0 because there is no learnable parameters for the output func-
tion RO By our general analysis in Eq. (22), we have:
lnN(Gk,ek,L2(Pn, k∙k)) = lnN({E®c : C ∈ Rl, ∣∣c∣∣∞ ≤ 1 },e®, |卜||尸
Note that ∣∣Ek∣∣ = O(d). Moreover, by proof in Claim 1, if K2α-1/2 = Ω(√n), similar to the
proof of the boundness verification, it holds that
kck22 < Oe(d),
where O(∙) hides log factors, and then according to Lemma 4, we have:
d3
lnN(Gk , ek, L2 (PnJ ∙∣I)) ≤ F ln (2L),
ek
By setting ek = e, we have PK=I Kek = e, where the factor * is from the fact Proposition 2.
Then, according to Lemma 5, we have:
lnN伍n,e,L2(Pn,k∙∣∣)) ≤O
where O(∙) hides log factors. By Dudley,s entropy theorem (Lemma 7), we can get:
Radn (Ln)
Thus, applying the standard Rademacher generalization bound (Lemma 6), with probability 1 - δ
over the training data, it holds that:
E
A〜PA
ln(1∕δ)
n
Part Two: We now ready to provide the generalization bound for the case K n, w.o.l.g. we let
K = Ω(n). When let ∣∣C(t) 一 1∣∣ = O(K-α), we can easily conclude that Ln(C(O)) = O(^2α)
by Proposition 4. Recall the last part of Theorem 1’s proof (boundness verification), we have:
∣∣c⑴一C(O)HF=O(κ√√nd-2), ∀t ∈ [T].
Hence, by Proposition 6, with /(•):= 春'(∙), we have:
*
*
∂“C(O)) C(t)
∂C ,
-C(O)) + O(bVdL)
- C(0) ∣∣∣22
∂“C(O)) C(t)
∂C ,
∂“C(O)) C(t)
∂C ,
-	C(0)	+ Oe
-	C(0)	+ Oe
b2 d3nL
K 4a-1
bV d3L
K α∕2
27
Under review as a conference paper at ICLR 2021
where O(∙) hides log factors, and the last equality is due to K = Ω(n) and ɑ ≥ 1. Thus, it holds
that:
Radn(Ln) ≤ Radn	d'∂CP, C - C(O); : C ∈Cd}) + 0(号)，
where Ln = {2(C, A) : C ∈ Cd }, and we omit the term 2(C(O)) due to Radn 仅C(O))) = 0. By
Cauchy-Schwarz inequality, we have:
Rad
d'∂CC0)), C - C⑼;:C ∈C')
≤0 (K 2α-1∕2 J E
1 S ∂2(C(O), Ai)
n" -∂C—
i=1
≤ Oe
√nκ2α-1∕2 ) t
E X ai
∂2(C(O), Ai)
∂C
F
d
2
F
where we apply Jensen’s inequality to obtain the last inequality. Notice that:
2
∖
E
αi
n
αi
i=1
∂'(C(O), Ai)
∂C
n
X
i=1
∂'(C(O), Ai)
∂C
F
∖
2
F
By Proposition 5, we have:
2
∂“C(O), Ai)
∂C
O(bV d2),
F
which implies:
Hence, we have:
n
X
i=1
∂'(C(O), Ai)
∂C
2
=o(√nb2 d).
F
∖
Rad
(d'∂CP，C — C(O); ： C ∈ cd}) = O
b2 d2
K2α-1∕2 广
Combing all things together, we conclude:
Radn(Ln)= θ(普
Kα∕2
Similar to the part one, we have:
E
A〜PA
C町 ≤Ln(c㈤)+O( H)十。(rln≡).
We now finish the whole proof.
F Omitted Proof for D-SVD in Sections C
F.1 Proof of Proposition 3
Proof. First of all, we provide the Lipschitz constant w.r.t. t2k , Ck . Note that:
□
(30)
where
Uk+1 =	(I	+	-2UPU)	(I-	EKinv)(I-	-2UPU)Uk
28
Under review as a conference paper at ICLR 2021
On one hand, by Eq. (26) in Proposition 5,we have:
∂Ek+1
∂Ek
O (Kb),	∀k ∈ [Kinv]
On the other hand, we get:
∂EK
inv
∂EKinv ∂EKinv -1	∂E1
∂tU
∂EKinv-1 ∂EKinv-2	∂tU .
When Kinv is large enough, we can ignore EKinv’s impact, and conclude that the Lipschitz constant
of f w.r.t. tk dominate by the other term in Eq. (30) rather than EKinv . Recall that by Proposition
1, EKinv share the same row and the same column space with I + tUPU. W.o.l.g. we can assume
EKinv is diagonal, and by Proposition 4, we have EKi
have
O (l∕√Kinv). Thus, for large K* We
inv
/
∂Uk+1
∂Uk+1
况Uk
∂tu
∂tu
O

∖
From the spectral theorem, for a real skew-symmetric matrix the nonzero eigenvalues are all pure
imaginary and thus are of the form λ1i, -λ1i, λ2i, -λ2 i, . . .λ1 i, -λ1i, λ2i, -λ2i, . . . where each of
the λi are real. Hence, let PU = UPΛUp, where the entries of the diagonal of Λ are ±λii, and
UP is the unitary matrix, we have
then
(I + tU Pu
U) Uk = UP((I + t2U Λ
UP Uk,
-2 UPUk.
∂(i + tU PU )-1(i-tU PU)Uk
∂tu
-UP Λ(I+
Hence, we can easily obtain that:
(
∂Uk+1
∂tu
∂tu
O(kΛk) = O(kMk),
O

∖
where the second and the third equations come from the facts:
λi
< ∣λ∣ and ∣λι(P)∣≤ σι(P),
where ∣λι(∙)∣ and σι(∙) are the largest scale eigenvalue and singular value, respectively. Hence, we
can conclude that the Lipschitz constant of f w.r.t. tUk is in the order of O(kMk).
By the proof in Proposition 5, we have
∂Ek+1
∂C{k,l}
Elk+1-Elk ≤2Elk, ∀k∈ [Kinv].
Hence, we get:
∂EK
inv
{k,l}
O Kinv(k-Kinv)2Elk	= O Kinv(k-Kinv-l/2)
where the last equation is from kEk k = O
∂Uk+1
∂C{k,l}
≤
. Then, we can get:
O Kinv(k-Kinv-l/2)
{k,l}
29
Under review as a conference paper at ICLR 2021
where this equation is from the fact that EKinv share the same row and the same column space with
I ± tUPU, and:
Kinv L
XX
k=1 l=0
∂ Uk+ι
∂C{k,i}
Thus, we can easily verify that:
O(1).
Hence, we can conclude that the Lipschitz constant of f w.r.t. C is in the order of O(1).
Because of the Lipschitz condition, we can have:
lnN(Fk,e,k∙k) ≤ lnN Ck,τA^,'∞ ,
Lip(f)
where Lip(f) is the LiPschitz constant of f w.r.t. the spectral norm |卜||. Note that:
ln N(Ck，Lipf)，
O lnN
,血,'∞) +lnN({C},epKinV,
Note the bounds on covering number:
Finally, we get:
We now finish the proof.
□
F.2 Proof of Theorem 3
Proof. Before adopting Lemma 5, we first verify the conditions of this Lemma. The boundness
condition of the intermediate results is obvious since {Uk,i} and {Vk,i} are both from the Stiefel
manifold.
On the other hand, we can easily verify the Lipschitz condition when Kinv ≥ Ksvd . Let d
max{m, n}. Recall the last part of Theorem 1's proof (boundness verification), we have:
By the assumption K
By Eq. (30) in the proof of Proposition 3, we have the Lipschitz constant of f(Uk | Vk, M, tk, Ck)
w.r.t. Uk is bounded by:
≤ 1 + kEKinv k.
where this inequality is from the fact that EKinv share the same row and the same column space with
I± tUPU,and:
1.
30
Under review as a conference paper at ICLR 2021
DUeto IlC㈤—C(O)IlF = O(√kU), we have:
∣(I+tUPU) (I-EKinv)(I-tUPU)I=Oe(1+√Knv).
ThUs, the Lipschitz constant from any intermediate layer to oUtpUt layer of D-SVD is boUnded by:
where we use the assumption that √KnV = Ω(Ksvd). We have verified the two conditions in
Lemma 5.
By setting k = /Ksvd, we have PkK=sv1d k = . Due to Lemma 5 and Proposition 3, we obtain:
ln N(M n,e,L2(2j∙k))
O (KsVdLKmV ln( -Kvd-
where Oe(∙) hides some log factors.
By Dudley’s entropy theorem (Lemma 7), we can get:
where we utilize the fact that the integral of
K Ksvd ∖
∖√κinv√
Radn(M n)
evaluated near zero is in the order of
Thus, applying the standard Rademacher generalization bound (Lemma 6), with prob-
ability 1 - δ over the training data, it holds that:
O
m% hMn (沙))i ≤ Mn(t(t))+Oe (rKndL)+O (rιn≡).
We now finish the proof.
□
G Auxiliary Lemmas
Lemma 3 (Bounded Iterates). For q ∈ N ≥ 4, given xo, e ∈ R, such that xo ≤ 1 and e ≤ 1 ,let
xk = xqk-1 + e, then:
qk	1
Xk ≤ xO +(1 + -)e, ∀k ≥ 0, where - → 0 as k → ∞, and - ≤ -.
Proof. W.l.o.g, in this proof, we assume xqOk	e, otherwise, the conclusion is obvious. We prove
0
this lemma by induction. For k = 0, we have xO = xqO , the conclusion is obvious. Now, we assume
the inequality holds for xk , then:
q	qk	17e q	(a) qk+1	17e q 1 qk
xk+ι = Xk + e ≤ (xO +16 ) + e ≤ xO + 2q(ɪ6)ρ xO +e,
where (a) Come from the Binomial theorem for (x0 + ^)" with Xq 《e. It is easy to verify
that:
0.8 ∙	(	)	<	■, and 1 >	1.25	∙	—— q	∙	xO	-> 0	as k	-→	∞,
16	16	8 O
hence
xk+1 ≤ xqOk+1 + e + e 4q(2e)q-2xqOk < xqOk+1 + (1 + -)e.
By the induction, we finish the proof.
□
31
Under review as a conference paper at ICLR 2021
Let N(U, e, ∣∣∣∙∣∣∣) denote the least cardinality of any subset V ⊆ U that covers U at scale E with
norm ∣∣∣∙∣∣∣, meaning:
sup min |||A - B||| ≤ E.
A∈U B∈V
Then we have the following lemma.
Lemma 4 (Matrix Covering, Bartlett et al. (2017) Lemma 3.2). Let matrix X ∈ Rd×L be given with
kXkF ≤ b. Then
lnN({XC: C ∈ RL×m, kCk2,1 ≤ a},e,k∙∣∣F) ≤ * ln(2Lm),
where kCk2,1 = Pim=1 kC:,ik2.
Let Pn be a uniform distribution over n points data Xn := {xι, x2, ∙∙∙ , xn}. Then the L2(Pn, ∣∣∣∙∣∣∣)
norm of the function f is defined as
1
kfkL2(Pn,∣H∣∣)：=(1 X|||f1『).
Let Fι,…，Fk bea sequence of families of functions (which corresponds to families of single layer
neural nets in the deep learning setting) and ` be a Lipschitz loss function. We study the family of
compositions of ` and functions in Fi ’s:
L ：= ' ◦Fk oFk-ι …。Fi = {' ◦ fk ◦ fk-ι。…。fι ： ∀i,fi ∈Fi}∙
Lemma 5 (Abstraction of Techniques in Bartlett et al. (2017); Wei & Ma (2019)). Assume:
1.	Given the data set Xn and norm ∣∣∣∙∣∣∣, for any X ∈ X, |||fi。…。fι(x)∣∣∣ ≤ Si.
2.	'。fk。•…。fi+i is Ki-Lipschitzfor all i.
Then, we have the following covering number bound for L (for any choice E1, . . . , Ek > 0):
kk
L, X KiEi, L2(Pn, ∣∣∣∙∣∣∣)	≤ XlnN(Fi, Ei, L2(Pn, I∣∣∙∣∣∣))∙
i=1	i=1
The above lemma lemma says that if the intermediate variable (or the hidden layer) f。…。f i (x) is
bounded, and the composition of the rest of the functions'。fk。•…。fi+ι(x) is Lipschitz, then small
covering number of local functions imply small covering number for the composition of functions.
Finally, we present two classical results about Rademacher complexity. For a class of real-valued
functions L and dataset Xn with data size n, define the empirical Rademacher complexity of the
function class L by:
Radn (L) = E sup
αi
l∈L
1 X αil(xi),
n i=1
where Xi ∈ Xn, and ai's are independent uniform ±1 random variables. Now We present the
Rademacher complexity regression bounds.
Lemma 6 (Rademacher complexity regression bounds, Mohri et al. (2018) Theorem 11.3 ). Let
` : U ⊂ Rd → R be a nonnegative loss upper bounded by M > 0, and ' is μ-Lipschitz for
some μ > 0, then for any δ > 0, with probability at least 1 一 δ over the draw of an i.i.d. sample
Xn = {xι, X2, ∙∙∙ , Xn } of size n according the distribution Pr, the following holdsfor all' ∈ L:
E ['(x)] ≤
x~P
1n
1 £'(χi) +
n i=1
2μ Radn(L) + 3M
Jln(2∕δ)
V	2n
Lemma 7 (Dudley’s entropy theorem, Dudley (1967)). For any function class L containing func-
tions ` : X → R, where ∀X ∈ X is bounded, we have that:
Radn(L) ≤ ɪnf (4α +12广皿 ^^ rln N(L, e, L2(Pn, 1M))!.
32
Under review as a conference paper at ICLR 2021
Corollary 1. Let US use O(∙) to hide anyfactor that is sub-polynomial. Then we basically have that
if lnN(L,e,L2(PUI∙∣ll))= O*(*), then:
一| O((I
[O(n1p)
P< 2,
P = 2,
p > 2.
33