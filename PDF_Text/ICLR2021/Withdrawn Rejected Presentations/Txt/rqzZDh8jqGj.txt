Under review as a conference paper at ICLR 2021
Experimental Design for Overparameterized
Learning with Application to Single Shot Deep
Active Learning
Anonymous authors
Paper under double-blind review
Ab stract
The impressive performance exhibited by modern machine learning models hinges
on the ability to train such models on a very large amounts of labeled data. However,
since access to large volumes of labeled data is often limited or expensive, it is
desirable to alleviate this bottleneck by carefully curating the training set. Optimal
experimental design is a well-established paradigm for selecting data point to
be labeled so to maximally inform the learning process. Unfortunately, classical
theory on optimal experimental design focuses on selecting examples in order
to learn underparameterized (and thus, non-interpolative) models, while modern
machine learning models such as deep neural networks are overparameterized, and
oftentimes are trained to be interpolative. As such, classical experimental design
methods are not applicable in many modern learning setups. Indeed, the predictive
performance of underparameterized models tends to be variance dominated, so
classical experimental design focuses on variance reduction, while the predictive
performance of overparameterized models can also be, as is shown in this paper,
bias dominated or of mixed nature. In this paper we propose a design strategy that is
well suited for overparameterized regression and interpolation, and we demonstrate
the applicability of our method in the context of deep learning by proposing a new
algorithm for single shot deep active learning.
1	Introduction
The impressive performance exhibited by modern machine learning models hinges on the ability
to train the aforementioned models on a very large amounts of labeled data. In practice, in many
real world scenarios, even when raw data exists aplenty, acquiring labels might prove challenging
and/or expensive. This severely limits the ability to deploy machine learning capabilities in real
world applications. This bottleneck has been recognized early on, and methods to alleviate it have
been suggested. Most relevant for our work is the large body of research on active learning or
optimal experimental design, which aims at selecting data point to be labeled so to maximally inform
the learning process. Disappointedly, active learning techniques seem to deliver mostly lukewarm
benefits in the context of deep learning.
One possible reason why experimental design has so far failed to make an impact in the context of
deep learning is that such models are overparameterized, and oftentimes are trained to be interpola-
tive (Zhang et al., 2017), i.e., they are trained so that a perfect fit of the training data is found. This
raises a conundrum: the classical perspective on statistical learning theory is that overfitting should
be avoided since there is a tradeoff between the fit and complexity of the model. This conundrum is
exemplified by the double descent phenomena (Belkin et al., 2019b; Bartlett et al., 2020), namely
when fixing the model size and increasing the amount of training data, the predictive performance
initially goes down, and then starts to go up, exploding when the amount of training data approaches
the model complexity, and then starts to descend again. This runs counter to statistical intuition which
says that more data implies better learning. Indeed, when using interpolative models, more data can
hurt (Nakkiran et al., 2020a)! This phenomena is exemplified in the curve labeled “Random Selection”
in Figure 1. Figure 1 explores the predictive performance of various designs when learning a linear
regression model and varying the amount of training data with responses.
1
Under review as a conference paper at ICLR 2021
The fact that more data can hurt further moti-
vates experimental design in the interpolative
regime. Presumably, if data is carefully cu-
rated, more data should never hurt. Unfortu-
nately, classical optimal experimental design fo-
cuses on the underparameterized (and thus, non-
interpolative) case. As such, the theory reported
in the literature is often not applicable in the
interpolative regime. As our analysis shows (see
Section 3), the prediction error of interpolative
models can either be bias dominated (the first
descent phase, i.e., when training size is very
small compared to the number of parameters),
variance dominated (near equality of size and
parameters) or of mixed nature. However, prop-
erly trained underparameterized models tend to
have prediction error which is variance domi-
nated, so classical experimental design focuses
on variance reduction. As such, naively using
Figure 1: MSE of a minimum norm linear inter-
polative model. We use synthetic data of dimen-
sion 100. The full description is in Appendix E.
classical optimality criteria, such as V-optimality (the one most relevant for generalization error) or
others, in the context of interpolation, tends to produce poor results when prediction error is bias
dominated or of mixed nature. This is exemplified in the curve labeled “Classical OED” in Figure 1.
The goal of this paper is to understand these regimes, and to propose an experimental design strategy
that is well suited for overparameterized models. Like many recent work that attempt to understand the
double descent phenomena by analyzing underdetermined linear regression, we too use a simple linear
regression model in our analysis of experimental design in the overparameterized case (however,
we also consider kernel ridge regression, not only linear interpolative models). We believe that
understanding experimental design in the overparameterized linear regression case is a prelude to
designing effective design algorithms for deep learning. Indeed, recent theoretical results showed
a deep connection between deep learning and kernel learning via the so-called Neural Tangent
Kernel (Jacot et al., 2018; Arora et al., 2019a; Lee et al., 2019). Based on this connection, and as a
proof-of-concept, we propose a new algorithm for single shot deep active learning.
Let us now summarize our contributions:
•	We analyze the prediction error of learning overparameterized linear models for a given fixed
design, revealing three possible regimes that call for different design criteria: bias dominated,
variance dominated, and mixed nature. We also reveal an interesting connection between
overparameterized experimental design and the column subset selection problem (Boutsidis
et al., 2009), transductive experimental design (Yu et al., 2006), and coresets (Sener &
Savarese, 2018). We also extend our approach to kernel ridge regression.
•	We propose a novel greedy algorithm for finding designs for overparameterized linear
models. As exemplified in the curve labeled “Overparameterized OED”, our algorithm is
sometimes able to mitigate the double descent phenomena, while still performing better than
classical OED (though no formal proof of this fact is provided).
•	We show how our algorithm can also be applied for kernel ridge regression, and report
experiments which show that when the number of parameters is in a sense infinite, our
algorithm is able to find designs that are better than state of the art.
•	We propose a new algorithm for single shot deep active learning, a scaracly treated problem
so far, and demonstrate its effectiveness on MNIST.
Related Work. The phenomena of benign overfitting and double descent was firstly recognized in
DNNs (Zhang et al., 2017), and later discussed and analyzed in the context of linear models (Zhang
et al., 2017; Belkin et al., 2018; 2019a;b; Bartlett et al., 2020). Recently there is also a growing
interest in the related phenomena of “more data can hurt” (Nakkiran et al., 2020a; Nakkiran, 2019;
Nakkiran et al., 2020b; Loog et al., 2019). A complementary work discussed the need to consider
zero or negative regularization coefficient for large real life linear models (Kobak et al., 2020).
2
Under review as a conference paper at ICLR 2021
Experimental design is an well established paradigm in statistics, extensively covered in the literature
for the linear case (Pukelsheim, 2006) and the non linear case (Pronzato & Pdzman, 2013). The
application of it to pool based active learning with batch acquisitions was explored by Yu et al. (2006)
for linear models and by Hoi et al. (2006) for logistic regression. It was also proposed in the context
of deep learning (Sourati et al., 2018). Another related line of work is recent work by Haber and
Horesh on experimental design for ill-posed inverse problems(Haber et al., 2008; 2012; Horesh et al.,
2010). Active learning in the context of overparameterized learning was explored by Karzand &
Nowak (2020), however their approach differs from ours significantly since it is based on artificially
completing the labels using a minimax approach.
I the context of Laplacian regularized Least Squares (LapRLS), which is a generalization of ridge
regression, Gu et al. (2012) showed rigorously that Yu et al. (2006) criterion is justified as a bound
for both the bias and variance components of the expected error. We farther show that this bound
is in some sense tight only if the parameter norm is oneand the noise variance equals the l2 penalty
coefficient. In addition we postulate and show experimentally that in the overparameterized case
using a bias dominant criterion is preferable. Another case in which the bias term idoes not vanish is
when the model is misspecified. For linear and generalized linear models this case has been tackled
with reweighing of the loss function.
A popular modern approach for pool based active learning with batch acquisition is coresets (Sener &
Savarese, 2018; Geifman & El-Yaniv, 2017; Ash et al., 2019; Pinsler et al., 2019). This approach has
been used in the context of active learning for DNNs.
2	Underparameterized V-Optimal Experimental Design
Consider a noisy linear response model y = XT W + e, where E 〜N(0, σ2) and W ∈ Rd and assume
we are given with some data points x1, . . . , xn, for which we obtained independent responses,
yi = XiTW + Ei . Consider the underparameterized case, i.e. n ≥ d, and furthermore assume that
the set {xι,..., Xn} contains at least d independent vectors. The best linear unbiased estimator W
of w according to the Gauss-Markov theorem is given by: W = argminw ∣∣Xw - y∣∣2 = X+y
where X ∈ Rn×d is a matrix whose rows are X1 , . . . , Xn, y = [y1 . . . yn]T ∈ Rn and X+ is the
Moore-Pensrose pseudoinverse of X. It is well known that W - W is a normal random vector with
zero mean and covariance matrix σ2M-1, where M = XTX is the Fisher information matrix. This
implies that y(x) - y(x) is also a normal variable with zero mean and variance equal to σ2xtM-1x.
Assume also that X comes from a distribution ρ. With that we can further define the excess risk
R(W) = Ex〜P [(xtw - xtW)2] and its expectation:
Ee [R(W)] = Ex〜P [Vare [y(x) - y(x)]] = Ex〜P [σ2xτM-1x] = Tr (σ2M-1Cρ)	(1)
where CP is the uncentered second moment matrix of ρ: CP := Ex〜P [xxt].
Eq. (1) motivates the so-called V-optimal design criterion: select the dataset X1, . . . , Xn so that
夕(M) := Tr (M-ICP) is minimized (if we do not have access to CP then it is possible to estimate
it by drawing samples from ρ). In doing so, we are trying to minimize the expected (with respect to
the noise E) average (with respect to the data X) prediction variance, since the risk is composed solely
from it (due to the fact that the estimator is unbiased). As we shall see, this is in contrast with the
overparameterized case, in which the estimator is biased.
V-optimality is only one instance of various statistical criteria used in experimental design. In general
experimental design, the focus is on minimizing a preselected criteria 夕(M) (Pukelsheim, 2006). For
example in D-optimal design,夕(M) = det(M-1) and in A-optimal design 夕(M) = Tr (M-I).
However, since minimizing the V-optimality criterion corresponds to minimizing the risk, it is more
appropriate when assessing the predictive performance of machine learning models.
3	Overparameterized Experimental Design Criteria
In this section we derive an expression for the risk in the overparameterized case, i.e. like Eq. (1)
but also for the case that n ≤ d (our expressions also hold for n > d). This, in turn, leads to an
3
Under review as a conference paper at ICLR 2021
experimental design criteria analogous to V-optimality, but relevant for overparamterized modeling
as well. We design a novel algorithm based on this criteria in subsequent sections.
3.1	Overparameterized Regression and Interpolation
When n ≥ d there is a natural candidate for w^: the best unbiased linear estimator X+y 1. However,
when d > n there is no longer a unique minimizer of kXw - yk22 as there is an infinite amount
of interpolating w’s, i.e. w’s such that Xw = y (the last statement makes the mild additional
assumption that X has full row rank). One natural strategy for dealing with the non-uniqueness is to
consider the minimum norm interpolator:
W = argmin ∣∣wk2 s.t. Xw = y
It is still the case that W = X+ y. Another option for dealing with non-uniqueness of the minimizer is
to add a ridge term, i.e., add and additive penalty λ∣w∣22 . Let:
Wλ = arg min ∣∣Xw — y∣2 + λ∣w∣2
One can show that
W λ = X+y	(2)
where for λ ≥ 0 we define X+ = (XTX + λId)+ XT (see also Bardow (2008)). Note that Eq. (2)
holds both for the overparameterized (d ≥ n) and underparameterized (d < n) case.
Proposition 1. The function λ 7→ Xλ+ is continuous for all λ ≥ 0.
The proof, like all of our proofs, is delegated to the appendix. Thus, we also have that the minimum
norm interpolator W is equal to Wo, and that λ → W人 is continuous. This implies that the various
expressions for the expected risk of W入 hold also when λ = 0. So, henceforth we analyze the
expected risk of Wλ and the results also apply for W.
3.2	Expected Risk of W入
The following proposition gives an expression for the expected risk of the regularized estimator Wλ.
Note that it holds both for the overparameterized (d ≥ n) and underparameterized (d < n) case.
Proposition 2. We have
E[R(W λ)]
∣cp/2 (i — m+m) w∣2 +
X-------------{z--------------}
bias
σ2Tr CρMλ+2M
X-------------}
variance
where Mλ := XT X + λId = M + λId. The expectation is with respect to the training noise .
The last proposition motivates the following design criterion, which can be viewed as a generalization
of classical V-optimality:
C(M) = ∣C1/2 (I — M+M) w∣2 + σ2Tr (CρM+2M).
For λ = 0 the expression simplifies to the following expression:
中0 (M) = ∣Cp/2 (I — PM) w∣2 + σ2Tr (CρM+)
where PM = M+ M is the projection on the row space of X. Note that when n ≥ d and X has full
column rank,夕o(M) reduces to the variance of underparameterized linear regression, so minimizing
夕λ(M) is indeed a generalization of the V-optimality criterion.
Note the bias-variance tradeoff in 夕λ(M). When the bias term is much larger than the variance,
something we should expect for small n, then it make sense for the design algorithm to be bias
oriented. When the variance is larger, something we should expect for n ≈ d or n ≥ d, then the
design algorithm should be variance oriented. It is also possible to have mixed nature in which both
bias and variance are of the same order.
1In practice, when n is only mildly bigger than d it is usually better to regularize the problem.
4
Under review as a conference paper at ICLR 2021
3.3	Practical Criterion
As is,夕λ is problematic as an experimental design criterion since it depends both on W and on Cρ.
We discuss how to handle an unknown Cρ in Subsection 3.5. Here we discuss how to handle an
unknown W. Note that obviously W is unknown: it is exactly what we want to approximate! If we
have a good guess W for the true value of w, then We can replace W with W in 夕λ. However, in many
cases, such an approximation is not available. Instead, we suggest to replace the bias component with
an upper bound: ∣∣C/2(I - M+M)w∣∣2 ≤ ∣∣Wk2 ∙∣cp∕2 (I- m+m) kF.
Let us now define a new design criterion which has an additional parameter t ≥ 0:
…)=∣Cρ/2 (I-M+M) kF+ 色C巴I
bias bound (divided by kwk22)	variance (divided by
The parameter t captures an a-priori assumption on the tradeoff between bias and variance: if we
have t = σ2∕∣w∣∣2, then 夕λ(M) ≤ ∣∣w∣2 ∙ Oλ,t(M) . Thus, minimizing Oλ,t(M) corresponds to
minimizing an upper bound of 夕λ, if t is set correctly.
Another interpretation of Oλ,t(M) is as follows. If we assume that W 〜N(0, γ2Id), then
Ew S(M)] = Y2kCl/2 (I- M+M) kF + σ2Tr (CρM+2M)
so if we set t = σ2∕γ2 then Y20λ,t(M) = EW [夕λ(M)], so minimizing Oλ,t(M) corresponds to
minimizing the expected expected risk if t is set correctly. Again, the parameter t captures an a-priori
assumption on the tradeoff between bias and variance.
Remark 1. One alternative strategy for dealing with the fact that w is unknown is to consider a
sequential setup where batches are acquired incrementally based on increasingly refined approxima-
tions of w. Such a strategy falls under the heading of Sequential Experimental Design. In this paper,
we focus on single shot experimental design, i.e. examples are chosen to be labeled once. We leave
sequential experimental design to future research. Although, we decided to focus on the single shot
scenario for simplicity, the single shot scenario actually captures important real-life scenarios.
3.4	Comparison to Other Generalized V-Optimality Criteria
Consider the case of λ = 0. Note that we can write: Oo,t(M) = ∣∣Cp/2 (I 一 PM) kF + tTr (CρM+).
Recall that the classical V-optimal experimental design criterion is Tr CρM-1 , which is only
applicable if n ≥ d (otherwise, M is not invertible). Indeed, if n ≥ d and M is invertible, then
PM = Id and Oo,t(M) is equal to Tr(CPMT) up to a constant factor. However, M is not
invertible if n < d and the expression Tr CρM-1 does not make sense.
One naive generalization of classical V-optimality for n < d would be to simply replace the inverse
with pseudoinverse, i.e. Tr CρM+ . This corresponds to minimizing only the variance term, i.e.
taking t → ∞. This is consistent with classical experimental design which focuses on variance
reduction, and is appropriate when the risk is variance dominated.
Another generalization of V-optimality can be obtained by replacing M with its regularized (and
invertible) version Mμ = M + μId for some chosen μ > 0, obtaining Tr (CPM-I). This is exactly
the strategy employed in transductive experimental design (Yu et al., 2006), and it also emerges in a
Bayesian setup (Chaloner & Verdinelli, 1995). One can try to eliminate the parameter μ by taking the
limit of the minimizers when μ → 0. The following proposition shows that this is actually equivalent
to taking t = 0.
Proposition 3. For a compact domain Ω ⊂ Rd×d ofsymmetricpositive Semidefinite matrices:
lim argmin Tr (CPM-I) ⊆ argminTr (CP (I — PM)).
μ→0 M∈Ω	M∈Ω
We see that the aforementioned generalizations of V-optimality correspond to either disregarding the
bias term (t = ∞) or disregarding the variance term (t = 0). However, using Oo,t(M) allows much
better control over the bias-variance tradeoff (see Figure 1.)
Let us consider now the case of λ > 0. We now show that the regularized criteria Tr (CPM-I) used
in transductive experimental design (See Proposition 3) when μ = λ corresponds to also using t = λ.
5
Under review as a conference paper at ICLR 2021
Proposition 4. For any matrix space Ω, λ > 0: argminχ∈Ω Tr (CPM-I) = argminχ∈Ω Oλ,λ(M)
So, transductive experimental design corresponds to a specific choice of bias-variance tradeoff.
Another interesting relation with transductive experimental design is given by next proposition which
is a small modification of Theorem 1 due to Gu et al. (2012) .
Proposition 5. For any λ > 0 andt ≥ 0: 0λ,t(M) ≤ (λ +1) Tr (CPM-1)
In the absence of a decent model of the noise, which is a typical situation in machine learning, Prop.
5 suggests the to perhaps minimize only Tr CPMλ-1 without need to set t. However, this approach
may be suboptimal in the overparameterized regime. This approach implicitly considers t = λ (see
Prop. 4) which in a bias dominated regime can put too much emphasis on minimizing the variance. A
sequential approach for experimental design can lead to better modeling of the noise, thereby assisting
in dynamically setting t during acquisition-learning cycles. However, in a single shot regime, noise
estimation is difficult. Arguably, there exists better values for t than using a default rule-of-thumb
t = λ. In particular, we conjecture that t = 0 is a better rule-of-thumb then t = λ for severely
overparameterized regimes as it suppresses the potential damage of choosing a too large λ and it
is reasonale also if λ is small (since anyway we are in a bias dominated regime), so we can focus
on minimizing the bias only. In the experiment section we show an experiment that supports this
assumption. Notice that t = ∞ corresponds to minimizing the variance, while t = 0 corresponds to
minimizing the bias.
3.5	APPROXIMATING CP
Our criteria so far depended on CP . Oftentimes CP is unknown. However, it can be approximated
using unlabeled data. Suppose we have m unlabeled points (i.e. drawn form ρ), and suppose we
write them as the rows of V ∈ Rm×d. Then E m-1VTV = CP. Thus, we can write
mφλ(M) ≈ ψλ(M) := ∣∣V (Id - M+M) w∣∣2 + σ2Tr(VM+2MVT) , λ ≥ 0.
and use ψλ(M) instead of 夕λ(M). For minimum norm interpolation We have
Ψo(M) = kV (Id- PM) wk2 + σ2Tr (VM+Vt).
Again, let us turn this into a practical design criteria by introducing an additional parameter t:
ψλ,t(M) ：= kV (Id - M+ m) kF + tTr(VM+2MVT).	⑶
4 Pool-based Overparameterized Experimental Design
In the previous section We defined design criteria Oλ,t and ψλ,t that are appropriate for Overparame-
terized linear regression. While one can envision a situation in Which such We are free to choose X
so to minimize the design criteria, in much more realistic pool-based active learning We assume that
We are given in advance a large pool of unlabeled data x1 , . . . , xm . The training set is chosen to be
a subset of the pool. This subset is then labeled, and learning performed. The goal of pool-based
experimental design algorithms is to chose the subset to be labeled.
We formalize the pool-based setup as folloWs. Recall that to approximate CP We assumed We have a
pool of unlabeled data Written as the roWs of V ∈ Rm×d . We assume that V serves also as the pool
of samples from Which X is selected. For a matrix A and index sets S ⊆ [n], T ⊆ [d], let AS,T be
the matrix obtained by restricting to the roWs Whose index is in S and the columns Whose index is in
T. If : appears instead of an index set, that denotes the full index set corresponding to that dimension.
Our goal is to select a subset S of cardinality n such that ψλ,t(VS^Vs,:) is minimized (i.e., setting
X = VS,:). Formally, We pose folloWing problem:
Problem 1. (Pool-based Overparameterized V-Optimal Design) Given a pool of unlabeled examples
V ∈ Rm×d , a regularization parameter λ ≥ 0, a bias-variance tradeoff parameter t ≥ 0, and a design
size n, find a minimizer of
min
S ⊆[m], |S |=n
Ψλ,t(vS,: Vs,：).
6
Under review as a conference paper at ICLR 2021
Problem 1 is a generalization of the Column Subset Selection Problem (CSSP) (Boutsidis et al.,
2009). In the CSSP, we are given matrix U ∈ Rd×m and target number of columns n, and our goal is
to select a subset T which is a minimizer of
min	k(Id - U:,T U:+T)Uk2F
T ⊆[m], |T |=n	:,	:,T F
When λ = 0 and t = 0, Problem 1 reduces to the CSSP for U = VT . The λ = t = 0 case is also
somewhat related to the coreset approach for active learning Sener & Savarese (2018); Pinsler et al.
(2019); Ash et al. (2019); Geifman & El-Yaniv (2017). See Appendix B.
5	Optimization Algorithm
In this section we propose an algorithm for overparameterized experimental design. Our algorithm is
based on greedy minimization of a kernalized version of ψλ,t(VS,：Vs,：). Thus, before presenting
our algorithm, we show how to handle feature spaces defined by a kernel.
Kernelization. If |S|	≤ d and Vs,： has full row rank We have (Vs,：)；	=
VST,: VS,:VST,：+ λl∣s∣)	which allows US to write
Ψλ,t(VSM,:) = Tr(V I - 2VS,： (Vs,：VS,： + λI∣s∣)-1 VSJ VT)
+Tr(VVS,： (Vs,：VS,： + λI∣s∣)-1 Vs,：VS,：(Vs,：VS,： + λI∣s∣)-1 Vs,：VT)
+tTr(VVS,： (Vs,：vS,： + λI∣S∣)-2 Vs,：VT)
Let now K := VVT ∈ Rm×m. Then Vs,：VsT,： = Ks,s and VVTs,： = K：,s . Since Tr (K) is
constant, minimizing ψλ,t(vS,：Vs,：) is equivalent to minimizing
Jλ,t(S) ：= Tr(K：,s [(Ks,s + λI∣s∣)-1 (-2I∣s∣ + Ks,s (Ks,s + λI∣s∣)-1) + t(Ks,s + λl∣s∣)-[ KTS).
(4)
For λ = 0 we have a simpler form: J0,t(S) = Tr K：,s -Ks-,1s + tKs-,2s K：T,s .
Interestingly, when λ = 0 and t = 0, minimizing J0,0(S) is equivalent to maximizing the trace of
the Nystrom approximation of K. Another case for which Eq. (4) simplifies is t = λ (this equation
was already derived in Yu et al. (2006)):
Jλ,λ(S) = Tr (-K：,S (Ks,s + λI∣s∣)-1 KTs).
Eq. (4) allows us, via the kernel trick, to perform experimental design for learning of nonlinear models
defined using high dimensional feature maps. Denote our unlabeled pool of data by z1, . . . , zm ∈ RD,
and that we are using a feature map φ : Rd → H where H is some Hilbert space (e.g., H = Rd), i.e.
the regression function is y(z) = hφ(z), wiH. We can then envision the pool of data to be defined by
xj = φ(zj), j = 1, . . . , m. If we assume we have a kernel function k : RD × RD → RD such that
k(x, z) = hφ(x), φ(z)iH then Jλ,t(S) can be computed without actually forming x1, . . . , xm since
entries in K can be computed via k. If H is the Reproducing Kernel Hilbert Space of k then this is
exactly the setting that corresponds to kernel ridge regression (possibly with a zero ridge term).
Greedy Algorithm. We now propose our algorithm for overparameterized experimental design,
which is based on greedy minimization of Jλ,t(S). Greedy algorithms have already been shown to
be effective for classical experimental design (Yu et al., 2006; Avron & Boutsidis, 2013; Chamon &
Ribeiro, 2017), and it is reasonable to assume this carries on to the overparameterized case.
Our greedy algorithm proceeds as follows. We start with S⑼ = 0, and proceed in iteration.
At iteration j, given selected samples S(j-1) ⊂ [m] the greedy algorithm finds the index i(j) ∈
[m] — S(j-1) that minimizes J；,t (S(j-1) ∪ {i(j)}) . We set S(j) J S(j-1) ∪ {i(j)}. We continue
iterating until S(j) reaches its target size and/or Jλ,t(S) is small enough.
7
Under review as a conference paper at ICLR 2021
The cost of iteration j in a naive implementation is O (m - j) mj2 + j3 . Through careful matrix
algebra, the cost of iteration j can be reduced to O((m - j)(mj + j2)) = O(m2j) (since j ≤ m).
The cost of finding a design of size n is then O(m2 (n2 + D)) assuming the entire kernel matrix K is
formed at the start and a single evaluation of k takes O(D). Details are delegated to Appendix C.
6	Single Shot Deep Active Learning
There are few ways in which our proposed experimental design algorithm can be used in the context
of deep learning. For example, one can consider a sequential setting where current labeled data are
used to create a linear approximation via the Fisher information matrix at the point of minimum
loss (Sourati et al., 2018). However, such a strategy falls under the heading of Sequential Experimental
Design, and, as we previously stated, in this paper we focus on single shot active learning, i.e. no
labeled data is given neither before acquisition nor during acquisition (Yang & Loog, 2019).
In order to design an algorithm for deep active learning, we leverage a recent breakthrough in
theoretical analysis of deep learning - the Neural Tangent Kernel (NTK) (Jacot et al., 2018; Lee et al.,
2019; Arora et al., 2019a). A rigorous exposition of the NTK is beyond the scope of this paper, but a
short and heuristic explanation is sufficient for our needs.
Consider a DNN, and suppose the weights of the various layers can be represented in a vector
θ ∈ Rd. Given a specific θ, let fθ(∙) denote the function instantiated by that network when the
weights are set to θ. The crucial observation is that when the network is wide (width in convolutional
layers refers to the number of output channels) enough, we use a quadratic loss function (i.e.,
l(fθ(x),y) = 1∕2(fθ(x) - y)2), and the initial weights θo are initialized randomly in a standard way,
then when training the DNN using gradient descent, the vector of parameters θ stays almost fixed.
Thus, when we consider θ1, θ2, . . . formed by training, a first-order Taylor approximation is:
fθk (X) ≈ fθ0 (X) + Vθfθ0 (x)T(θk - θ0)
Informally speaking, the approximation becomes an equality in the infinite width limit. The Taylor
approximation implies that if we further assume that θ0 is such that fθ0 (X) = 0, the learned prediction
function of the DNN is well approximated by the solution of a kernel regression problem with the
(Finite) Neural Tangent Kernel, defined as
kf,θ0 (x, z) := Vθfθo (X)TVθfθ0 (Z)
We remark that there are few simple tricks to fulfill the requirement that fθ0 (X) = 0.
It has also been shown that under certain initialization distribution, when the width goes to infinity,
the NTK kf,θ0 converges in probability to a deterministic kernel kf - the infinite NTK. Thus, in a
sense, instead of training a DNN on a finite width network, we can take the width to infinity and
solve a kernel regression problem instead.
Although, it is unclear whether the infinite NTK can be an effective alternative to DNNs in the
context of inference, one can postulate that it can be used for deep active learning. That is, in order
to select examples to be labeled, use an experimental design algorithm for kernel learning applied
to the corresponding NTK. Specifically, for single shot deep active learning, we propose to apply
the algorithm presented in the previous section to the infinite NTK. In the next section we present
preliminary experiments with this algorithm. We leave theoretical analysis to future research.
7	Empirical Evaluation
Transductive Vs ψλ,0 Criterion (i.e., variance-oriented vs. bias-oriented designs) ψλ,0 and
ψλ,λ are simplified version of ψλ,t criterion. Our conjecture is that in the overparameterized regime
ψλ,0 is preferable, at least for relatively large λ. Table 1. empirically supports our conjecture. In
this experiment, we performed an experimental design task on 112 classification datasets from UCI
database (similar to the list that was used by Arora et al. (2019b) ). Learning is performed using
kernel ridge regression with standard RBF kernel. We tried different values of λ and checked which
criterion brings to a smaller classification error on a test set when selecting 50 samples. Each entry
in Table 1 counts how many times ψλ,λ , won ψλ,0 won or the error was the same. We consider an
equal error when the difference is less the 5%.
8
Under review as a conference paper at ICLR 2021
Table 1: ψλ,0 VS ψλ,λ on UCI datasets. We generated designs on 112 classification datasets. Each cell
details the number of datasets in which that selection of t was clearly superior to the other possible
choice, or the same (for the “SAME” column).
λ	ψλ,λ is better	ψλ,o is better	SAME
0.001	5	8	99
0.01	7	9	96
0.1	16	16	80
1.0	21	43	48
10.0	19	68	25
Deep Active Learning Here we report preliminary experiments with the proposed algorithm for
single shot deep actiVe learning (Section 6). Additional experiments are reported in the appendix. We
used the MNIST dataset, and used the square loss for training. As for the network architecture, we
used a Version of LeNet5 (LeCun et al., 1998) that is widen by a factor of 8. we refer to this network
as “Wide-LeNet5”.
The setup is as follows. We use Google’s open source neural tangents library (NoVak et al., 2020)
to compute Gram matrix of the infinite NTK using 59,940 training samples (we did not use the
full 60,000 training samples due to batching related technical issues). We then used the algorithm
proposed in Section 5 to incrementally select greedy designs of up to 800 samples, where we set the
parameters to λ = t = 0. We now trained the original neural network with different design sizes,
each design with fiVe different random initial parameters. Learning was conducted using SGD, with
fixed learning rate of 0.1, batch size of 128, and no weight decay. Instead of counting epochs, we
simply capped the number of SGD iterations to be equiValent to 20 epochs of the full trainning set.
We computed the accuracy of the model predictions on 9963 test-set samples (again, due to technical
issues related to batching).
Figure 2 report the mean and standard deVia-
tion (oVer the parameters initialization) of the
final accuracy. We see a consistent adVantage
in terms of accuracy for designs selected Via
our algorithm, though as expected the adVantage
shrinks as the training size increase. Notice, that
comparing the accuracy of our design with 400
training samples, random selection required as
many as 600 for Wide-LeNet5 to achieVe the
same accuracy!
Two remarks are in order. First, to preVent oVer-
fitting and reduce computational load, at each
iteration of the greedy algorithm we computed
the score for only on a subset of 2000 samples
from the pool. Second, to keep the experiment
simple we refrained from using tricks that en-
sure fθ0 = 0.
0.96-
0.94-
0.92-
0.90-
0.88-
0.86-
χ0.84-
£ 0.82 -
3 0.80 -
(o 0.78-
100	200	300	400	500	600	700	800
training set size
Figure 2: Single shot actiVe learning with Wide-
LeNet5 model on MNIST.
9
Under review as a conference paper at ICLR 2021
References
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems,pp. 8141-8150, 2019a. 1, 6
Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu. Har-
nessing the power of infinitely wide deep nets on small-data tasks. arXiv preprint arXiv:1910.01663,
2019b. 7
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671,
2019. 1,4
Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applications. SIAM
Journal on Matrix Analysis and Applications, 34(4):1464-1499, 2013. 5
Andre Bardow. Optimal experimental design of ill-posed problems: The METER approach. Comput-
ers & Chemical Engineering, 32(1-2):115-124, 2008. 3.1
Peter L. Bartlett, Philip M. Long, GgbOr Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 2020. ISSN 0027-8424. doi:
10.1073/pnas.1907378117. URL https://www.pnas.org/content/early/2020/04/
22/1907378117. 1,1,G
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to
understand kernel learning. volume 80 of Proceedings of Machine Learning Research, pp.
541-549, Stockholrnsmassan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http:
//proceedings.mlr.press/v80/belkin18a.html. 1
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias-variance trade-off. Proceedings of the National Academy of
Sciences, 116(32):15849-15854, 2019a. ISSN 0027-8424. doi: 10.1073/pnas.1903070116. URL
https://www.pnas.org/content/116/32/15849. 1
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019b. 1, 1
Christos Boutsidis, Michael W. Mahoney, and Petros Drineas. An improved approximation algorithm
for the column subset selection problem. In Proceedings of the Twentieth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA ’09, pp. 968-977, USA, 2009. Society for Industrial
and Applied Mathematics. 1, 4
Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical
Science, pp. 273-304, 1995. 3.4
Luiz Chamon and Alejandro Ribeiro. Approximate supermodularity bounds for experimental design.
In Advances in Neural Information Processing Systems, pp. 5403-5412, 2017. 5
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
arXiv preprint arXiv:1703.02910, 2017. G
Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint
arXiv:1711.00941, 2017. 1,4
Quanquan Gu, Tong Zhang, Jiawei Han, and Chris Ding. Selective labeling via error bound mini-
mization. Advances in neural information processing systems, 25:323-331, 2012. 1, 3.4
Eldad Haber, Lior Horesh, and Luis Tenorio. Numerical methods for experimental design of large-
scale linear ill-posed inverse problems. Inverse Problems, 24(5):055012, 2008. 1
Eldad Haber, Zhuojun Magnant, Christian Lucero, and Luis Tenorio. Numerical methods for
A-optimal designs with a sparsity constraint for ill-posed inverse problems. Computational
Optimization and Applications, 52(1):293-314, 2012. 1
10
Under review as a conference paper at ICLR 2021
Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch mode active learning and its
application to medical image classification. In Proceedings of the 23rd International Conference
on Machine Learning,pp. 417-424. ACM, 2006. 1
Lior Horesh, Eldad Haber, and Luis Tenorio. Optimal experimental design for the large-scale nonlinear
ill-posed problem of impedance imaging. Large-Scale Inverse Problems and Quantification of
Uncertainty, pp. 273-290, 2010. 1
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, pp.
8571-8580, 2018. 1, 6
Mina Karzand and Robert D Nowak. Maximin active learning in overparameterized model classes.
IEEE Journal on Selected Areas in Information Theory, 2020. 1
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. Optimal ridge penalty for real-world high-
dimensional data can be zero or negative due to the implicit ridge regularization. Journal of
Machine Learning Research, to appear, 2020. 1
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. 7
Yann LeCun, Corinna Cortes, and CJ Burges. MNIST handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010. D
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, pp. 8572-8583,
2019. 1,6
Marco Loog, Tom Viering, and Alexander Mey. Minimizers of the empirical risk and risk monotonic-
ity. In Advances in Neural Information Processing Systems, pp. 7476-7485, 2019. 1
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019. 1
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020a. URL https://openreview.net/forum?id=B1g5sA4twr. 1, 1
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. arXiv preprint arXiv:2003.01897, 2020b. 1
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in Python. In
International Conference on Learning Representations, 2020. URL https://github.com/
google/neural-tangents. 7
Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hernandez-Lobato. Bayesian batch
active learning as sparse subset approximation. In Advances in Neural Information Processing
Systems, pp. 6356-6367, 2019. 1,4
Luc Pronzato and Andrej Pazman. Design of experiments in nonlinear models. Lecture Notes in
Statistics, 212, 2013. 1
Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006. 1, 2
R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009. 1
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL
https://openreview.net/forum?id=H1aIuk-RW. 1, 1, 4, B, D
11
Under review as a conference paper at ICLR 2021
Jamshid Sourati, Ali Gholipour, Jennifer G Dy, Sila Kurugol, and Simon K Warfield. Active deep
learning with Fisher information for patch-wise semantic segmentation. In Deep Learning in
Medical Image Analysis and Multimodal Learning for Clinical Decision Support, pp. 83-91.
Springer, 2018. 1, 6
Yazhou Yang and Marco Loog. Single shot active learning using pseudo annotators. Pattern
Recognition, 89:22-31, 2019. 6
Kai Yu, Jinbo Bi, and Volker Tresp. Active learning via transductive experimental design. In
Proceedings of the 23rd International Conference on Machine Learning, pp. 1081-1088. ACM,
2006. 1, 1, 3.4,5,5,D
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017. URL https://openreview.net/forum?id=Sy8gdB9xx. 1, 1
A Proofs
A.1 Proof of Proposition 1
Proof. We prove the case of d ≥ n (for X ∈ Rn×d). The proof for d < n is similar. It is enough to
show that limλ→0 Xλ+=X+ . For a scalar γ let
γ 6= 0
γ=0
LetX = UΣVT be the SVD of X with
σι	0 …0
Σ =	..	.	.
..	.
σd	0	…0
∈ Rd
×n
where σ1 , . . . , σd are the singular values of X. We have X+ = VΣ+UT where we have
「σ+
0	…	0
On the other hand, simple matrix algebra shows that
-(σ2 + λ)+σι	-
.
.
.
Xλ = V	C	(σ2+ λ)+σd	UT	(5)
0	♦…	0
..
..
..
_	0	…	0
Now clearly for i = 1, . . . , d,
lim+ (σi2 + λ)+σi = σi+
So the limit of the diagonal matrix in Eq. (5) when λ → 0+ is Σ+. Since matrix product is a linear,
and thus continuous function, the proposition follows.	□
12
Under review as a conference paper at ICLR 2021
A.2 Proof of Proposition 2
Proof. Let us write
1
E ：=	.
.
En
so
y = Xw + E .
Thus,
W λ = Xλ y = Xλ Xw + Xλ E = Mλ Mw + Xλ E
and
XTw - XTWλ = XT(Id - MλM)w + xτXλE
For brevity we denote Pλ⊥X = Id - Mλ+ M. Note that this is not really a projection, but rather
(informally) a “soft projection”. So:
(xτW - xτWλ)2 = WτP⊥X(xxτ)P⊥χW + wτP⊥χ(xxτ)X；E + et(X；)τ(xxτ)X；E
Finally,
E [R(W；)] = Eχ,e [(XTw - xτW；)2]
=EehExh(XTW - XTWλ)2 | Eii
= E hEx hWτPλ⊥X(XXτ)Pλ⊥XW + WτPλ⊥X(XXτ)X+λE + Eτ(Xλ+)τ(XXτ)X+λE | Eii
= Ee hWτPλ⊥XCρPλ⊥XW +WτPλ⊥XCρXλ+E+Eτ(Xλ+)τCρXλ+Ei
=wτP⊥XCρP⊥χW + σ2Tr ((X；)τCpX；)
=kCp∕2P⊥χwk2+ σ2Tr (CpX；(X；)T)
=kCp/2 (I - M+M) wk2+ σ2Tr (CρM+2M)
□
A.3 Proof of Proposition 3
Before proving Proposition 3 we need the following definition and theorem.
Definition 1. For a family of sets {A；}；*r, A ⊂ Rd We write lim；7入 A; = A if W ∈ A if and
only if there exists sequence λn → λ and a sequence Wn → W where Wn ∈ A；n for sufficiently
large n.
Theorem 1. (A restricted version of Theorem 1.17 in Rockafellar & Wets (2009)) Consider f :
Ω X Ψ → R where Ω ⊆ Rd and Ψ ⊆ R are compact and f is continuous. Then
lim argmin f (w, λ) ⊆ argmin f (w, λ).
λ—→λ	W	W
Proof. Suppose W ∈ lim；.入 argmi□w f (w, λ). The implies that there exits λn → λ such that
Wn ∈ argmi□w f (w, λn) and Wn → W. From the continuity of f we have that f (wn, λn) →
f (W, λ) . Now suppose for the sake of contradiction that W ∈ argmi□w f (w, λ) . So there is U
such that f (u, λ) < f (W, λ). From the continuity of f in λ there is no such that for all n > no
f (u, λn) < f (W, λ). Then from the continuity of f in w, and Wn → W, for sufficiently large n,
f (Wn, λn) > f(u, λn), which contradicts Wn ∈ argminw f (W, λn).
□
We are now ready to prove Proposition 3.
13
Under review as a conference paper at ICLR 2021
Proof. Consider the function f (M, μ) = Tr (CPM-I)
f (M,μ)
JTr (μCρ(M + μI)-1)
Tr (Cρ (I - M+M))
μ > 0
μ = 0
defined over Ω X R≥o where R≥o denotes the set of non-negative real numbers. Note that this
function is well-defined since Ω is a set of positive semidefinite matrices.
We now show that f is continuous. For μ > 0 it is clearly continuous for every M, so we focus
on the case that μ = 0 for an arbitrary M. Consider a sequence R>o 3 μn → 0 (where R>o is
the set of positive reals) and Ω 3 Mn → M. Since Ω is compact, M ∈ Ω. Let us write a spectral
decomposition of Mn (recall that Ω is a set of symmetric matrices)
Mn = UnΛnUnT
where Λn is diagonal with non-negative diagonal elements (recall that Ω is a set of positive definite
matrices). Let M = UΛUT be a spectral decomposition of M. Without loss of generality we may
assume that Un → U and Λn → Λ. Now note that
(Mn + μnI)-1Mn = Un(Λn + μnI)-1ΛnUT
One can easily show that (Λn + μnI)-1Λn → sign(Λ) where sign is taken entry wise, which implies
that (Mn + μnI)-1Mn → U sign(Λ)UTSinCe matrix multiplication is continuous. Next, note that
M+M = UΛ+ ΛUt = U sign(Λ)UT so (Mn + μnI)-1Mn → M+M. The Woodbury formula
implies that
μnCP(Mn + μnI)I = Cp (I -(Mn + MnI)IMn)
so the continuity of the trace operator implies that
Tr (μnCρ (Mn + μnI)-1) = Tr (Cρ (I -(Mn + μnI)-1Mn)) → Tr (Cρ (I - M+ M))
which shows that f is continuous.
Theorem 1 now implies the claim since for μ > 0 we have
argmin Tr (CPM-I) = argmin Tr (μCρM-1).
M∈Ω	M∈Ω
□
A.4 Proof of Proposition 4
Proof. Let A = (I — (M + λId)-1M)2 + λ(M + λId)-2M ,so Q,λ(M) = Tr (CρA). Wenow
have for λ > 0:
A =	(I - (M + λId)-1(M + λId) + λ(M + λId)-1)2 + λ(M + λId)-2M
= λ2 (M + λId)-2 + λ(M + λId)-2M
= λ(M + λId)-2 (M + λId)
= λ(M+λI)-1
so:
Tr (CPA) = λTr CP (M + λI)-1 .
Since λ > 0 it doesn,t affect the minimizer.	□
B Relation to Coresets
The idea in the coreset approach for active learning is to find an S such that
C(S)
1m	1
m X l("S)-同 X l(xl,yllS S
14
Under review as a conference paper at ICLR 2021
is minimized. In the above l(x, y | S) is a loss function, and the conditioning on S denotes that the
parameters of the loss function are the ones obtained when training only using indices selected in
S . For linear regression the conditioning on S is not relevant (since the parameters do not affect
the loss). The motivation for minimizing C(S) is that the expected test loss can be broken to the
generalization loss on the entire dataset (which is fixed), the training loss (which is 0 in the presence
of overparameterization) and the coreset loss.
One popular approach to active learning using coresets is to find a coverset. A δ-coverset of a set of
points A is a set of points B such that for every x ∈ A there exists a y ∈ B such that kx - yk2 ≤ δ
(other metrics can be used as well). Sener and Savarese Sener & Savarese (2018) showed that under
suitable Lipschitz and boundness conditions, if {xi }i∈S is a δ-coverset of {xi}i∈[m] then
C(S) ≤ O(δ + m-1/2)
which motivates finding a S that minimizes δS, where δS denotes the minimal δ for which {xi }i∈S
is a δ-coverset of {xi}i∈[m] .
Since for a x in the training set (which is a row of V) kx(Id - PM)k22, for M = VST,:VS,: is the
minimal distance from x to the span of {xi}i∈S, and as such is always smaller than the distance
between x and it’s closest point in {xi}i∈S, it is easy to show that
n-1ψo,o(VS,：Vs,：) ≤ δS.
Thus, minimizing δS can be viewed as minimizing an upper bound on the bias term when λ = 0.
Under the setup of the experiment in Section 7 we tried to replace our design with k-centers algorithm,
which often used as approximated solution for the problem of finding S that minimizes δS . How ever
the result we got were much worse then random design, probably due to the problem of outliers. We
did not try more sophisticated versions of the k-center algorithm that tackle the problem of outliers.
C Details on the Algorithm
We discuss the case of λ = 0. The case of λ > 0 requires some more careful matrix algebra, so we
omit the details.
Let us define
Aj := KS-(j),S(j) ,	Bj := K:,S(j) K:,S(j)
and note that Jλ,t(S(j)) = -Tr (Bj(Aj- tA；)). We also denote by Aj and Bj the matrices
obtained from Aj and Bj (respectively) by adding a zero row and column.
Our goal is to efficiently compute Jλ,t(S(j-1) ∪ {i}) for any i ∈ [m] - S(j-1) so find i(j) and form
S(j). We assume that at the start of iteration j we already have in memory Aj-1 and Bj-1. We show
later how to efficiently update Aj and Bj once we have found i(j). For brevity, let us denote
Si(j)	:=	S(j-1)	∪	{i},	Aji	:=	KS-(1j),S(j),	Bji :=K:T,S(j)K:,S(j)
Let us also define
22
Cj-1 := Bj-1Aj-1, Dj-1 := Bj-1Aj-1,	Ej-1 := Aj-1
Again, we assume that at the start of iteration j we already have in memory Cj-1, Dj-1 and Ej-1,
and show how to efficiently update these.
Let
0j-1	K:T,S(j-1)K:,i
K:T,iK:,S(j-1)	K:T,iK:,i
and note that
Bji = B j-1 + Wji.
Also important is the fact that Wji has rank 2 and that finding the factors takes O(mj) discounting
the cost of computing columns of K. Next, let us denote
_	1
j	(Kii- KS(j),iAj-1 KS(j),i)
Wji :
15
Under review as a conference paper at ICLR 2021
and
Qji := rji
Aj-1KS(j),iKST (j),iAj--11	-Aj-1KS(j) ,i
-KTS(j) ,iAj-1	1
A well known identity regarding Schur complement implies that
Aji= A j-1 + Qji
Also important is the fact that Qji
cost of computing entries of K.
has rank 2 and that finding the factors takes O(j2) discounting the
So
Jλ,t(Sj) = -Tr (Bji(Aji- tA*))
=-Tr ((Bj-ι + Wji)(Aj-1 + Qji- t(Aj-1 + Qji)2))
2
=-Tr	((Bj-1 + Wji)(Aj-1	+	Qji)- t(Bj-1	+ Wji)(Aj-1 +	Qji	+ Aj-1 Qji +	QjiAj-1 J
=-Tr (Cj-1 + Bj-1Qji + Wji(Aj-1 + Qji))
+tTr(Dj-1 + Bj(Aj-1Qji + QjiAj-1 + Q2i))
+Tr (Wi(Ej-1 + Q2i + Aj-1 Qj, + QjiAj-1)
Now, Cj-1 is already in memory so Tr (Cj-1) can be computed in O(j), Qji has rank 2 and Bj-1
is in memory so Tr (Bj-1Qj∙J can be compute in O(j2), and Wji has rank 2 and Aj-1 is in
memory so Tr Wi (A j-1 + Qji) can be computed in O(j2 ). Using a similar rationale, all the
other terms of Jλ,t(Si(j)) can also be computed in O(j) or O(j2), and overall Jλ,t(Si(j)) can be
computed in O(j2). Thus, scanning for i(j) takes O((m - j)j2).
Once i(j) has been identified, We set S(j) = Sij), Aj = Ajij) = Aj7 + Qjij) and Bj = Bjij)=
Bj-1 + Wjij). The last two can be computed in O(j2) once we form Qij) and Wij). Computing
the factors of these matrices takes O(mj). As for updating Cj-1, We have
Cj = Cj-I + Bj-IQjij) + Wjij)A j-1 + Wjij)Qjij)
1 ʃ-⅛ ♦ 1 . IC	X-1 Ill ♦	1 1	C ♦ X-1 ♦ ♦	1
where Cj-1 is obtained from Cj-1 be adding a zero row and column. Since Cj-1 is in memory and
both Qji(j) and Wi(j) have rank O(1), we can compute Cj is O(j2). Similar reasoning can be used
to show that Dj and Ej can also be computed in O(j2 ).
Overall, the cost of iteration j is O((m - j )(mj + j2)) = O(m2j) (since j ≤ m). The cost of
finding a design of size n is O(m2 (n2 + D)) assuming the entire kernel matrix K is formed at the
start and a single evaluation of k takes O(D).
D Experimental Parameters Exploration and Comparison to
Transductive Experimental Design
In this subsection we report a set of experiments on a kernel ridge regression setup (though in one
experiment we set the ridge term to 0, so we are using interpolation). We use the MNIST handwriting
dataset (LeCun et al., 2010), where the regression target response was computed by applying one-hot
function on the labels 0-9. Nevertheless, we still measure the MSE, and do not use the learnt models
as classifiers. We use the RBF kernel k(x, z) = exp(-γkx - zk22) with parameter γ = 1/784. From
the dataset, we used the standard test set of 10000 images and selected randomly another 10000
images from the rest of the 60000 images as a pool. We used our proposed greedy algorithm to select
a training set of sizes 1 to 100. We use two values of λ: λ = 0 (interpolation), and λ = 0.752. The
optimal λ according to cross validation was the smallest we checked so we just used λ = 0. However,
in some cases having a λ > 0 is desirable from a computational perspective, e.g. it caps the condition
16
Under review as a conference paper at ICLR 2021
Figure 3: Kernel regression experiments on MNIST.
number of the kernel matrix, making the linear system easier to solve. Furthermore, in real world
scenarios, oftentimes we do not have any data before we start to acquire labels, and if we do, it is not
always distributed as in the test data, so computing the optimal λ can be a challenging.
Results are reported in Figure 3. The left panel show the results for λ = 0. We report results for
t = 0 and t = 0.5. The choice of t = 0 worked better. Kernel models with the RBF kernel are highly
overparameterized (the hypothesis space is infinite dimensional), so we expect the MSE to be bias
dominated, in which case a small t (or t = 0) might work best. Recall that the option of λ = t = 0 is
equivalent to the Column Subset Selection Problem, is the limit case of transductive experimental
design (Yu et al., 2006), and can be related to the coreset approach (specifically Sener & Savarese
(2018)).
The case of λ = 0.752 is reported in the right panel of Figure 3. We tried t = 0 and t = λ = 0.752.
Here too, using a purely bias oriented objective (i.e., t = 0) worked better. Note that this is in contrast
with classical OED which use variance oriented objectives. The choice of t = λ worked well, but not
optimally. In general, in the reported experiments, and other experiments conducted but not reported,
it seems that the choice of t = λ, which is, as we have shown in this paper, equivalent to transductive
experimental design, usually works well, but is not optimal.
E Experimental Setup for Res ult Reported in Figure 1
First, w ∈ R100 was sampled randomly from N(0, I) . Then a pool (the set from which we
later choose the design) of 500 samples and a test set of 100 samples were randomly generated
according to X 〜N(0, ∑), e 〜N(0, σ2I) and y = XTW + e, where Σ ∈ R100×100 is diagonal with
Σii = exp(-2.5i/100), and σ = 0.2. We then created three incremental designs (training sets) of size
120 according to three different methods:
•	Random design - at each iteration we randomly choose the next training sample from the
remaining pool.
•	Classical OED (variance oriented) - at each iteration we choose the next training sample
from the remaining pool with a greedy step that minimizes the variance term in Eq. (3).
•	Overparameterized OED - at each iteration we chose the next training sample from the
remaining pool with a greedy step that minimizes Eq. (3), with λ = 0 and t = σ2 .
With the addition of each new training sample we computed the new MSE achieved on the test set
with minimum norm linear regression.
F Experiment： Single Shot Active Learning for Narrow Networks
In Figure 4 We compare the result of our method
on LeNet5 with the result of our method on
Wide-LeNet5. We see that while the result on
17
0.84
Under review as a conference paper at ICLR 2021
the wide version are generally better, both for
random designs and our design, our method
brings a consistent advantage over random de-
sign. In both the narrow and the wide versions
it requires about 600 training samples for the
random design to achieve the accuracy achieved
using our algorithm with only 400 training sam-
ples!
The parameters used by our algorithm to select
the design are λ = t = 0. For the network train-
ing we used SGD with batch size 128, leaning
rate 0.1 and no regularization. The SGD number
iterations is equivalent to 20 epochs of the full
trainning set.
G Sequential vs S ingle Shot Active Learning
While in this work focus on the single shot active learning, an interesting question is how does it
compare to sequential active learning. In sequential active learning we alternate between a model
improving step and a step of new labels acquisition,. This obviously gives an advantage to sequential
active learning over single shot active learning, as the latter is a restricted instance of the former.
As we still do not have a sequential version of our algorithm to compare with, we chose to experimen-
tally compare our single shot algorithm with the classical method of uncertainty sampling (Bartlett
et al., 2020). This method has proved to be relatively efficient for neural networks (Gal et al., 2017).
Uncertainty sampling based active learning requires computing the uncertainty of the updated model
regarding each sample in the pool. As such, this approach is sequential by nature.
Usually uncertainty sampling is derived in connection to the cross entropy since in that case the
network output after the softmax layer can be interpreted as a probability estimation of y = i given
x, which we symbolize as pi(x). The uncertainty score (in one common version) is then given by
1 - maxpi(x).
Because we use the square lose, we need to make some adaptation for the way of pi (x) is computed.
Considering the fact that the square loss is an outcome of a maximum likelihood model that given x
assumes y 〜N(f (x), Il), it make sense to use
Pi(X) = (2π)-2 e- 1 kyi-f(X)k2,
where yi is the onehot vector of i.
Figure 5. shows a comparison between the ac-
curacy achieved with our single shot algorithm
and the sequential active learning on MNIST
with LeNet5. The acquisitions batch size of the
sequential active learning were set to 100. Our
algorithm ran with λ = t = 0. For the network
training we used SGD with batch size 128, lean-
ing rate 0.1 and no l2 regularization. The SGD
number iterations is equivalent to 20 epochs of
the full train set.
Initially, our selection procedure shows a clear
advantage. However, once the training set grows
large enough, the benefit of a sequential setup
starts to kick-in, the sequential algorithm starts
to show superior results. This experiment moti-
vates further development of sequential version
of our algorithm.
Figure 5: Single shot active learning vs sequential
active learning. MNIST and (standard) LeNet5
18