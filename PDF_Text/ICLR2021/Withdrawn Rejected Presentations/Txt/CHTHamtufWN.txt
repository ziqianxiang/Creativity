Under review as a conference paper at ICLR 2021
Continual Invariant Risk Minimization
Anonymous authors
Paper under double-blind review
Ab stract
Empirical risk minimization can lead to poor generalization behaviour on unseen
environments if the learned model does not capture invariant feature represen-
tations. Invariant risk minimization (IRM) is a recent proposal for discovering
environment-invariant representations. It was introduced by Arjovsky et al. (2019)
and extended by Ahuja et al. (2020). The assumption of IRM is that all environ-
ments are available to the learning system at the same time. With this work, we
generalize the concept of IRM to scenarios where environments are observed se-
quentially. We show that existing approaches, including those designed for contin-
ual learning, fail to identify the invariant features and models across sequentially
presented environments. We extend IRM under a variational Bayesian and bilevel
framework, creating a general approach to continual invariant risk minimization.
We also describe a strategy to solve the optimization problems using a variant of
the alternating direction method of multiplier (ADMM). We show empirically us-
ing multiple datasets and with multiple sequential environments that the proposed
methods outperforms or is competitive with prior approaches.
1	Introduction
Empirical risk minimization (ERM) is the predominant principle for designing machine learning
models. In numerous application domains, however, the test data distribution can differ from the
training data distribution. For instance, at test time, the same task might be observed in a differ-
ent environment. Neural networks trained by minimizing ERM objectives over the training distri-
bution tend to generalize poorly in these situations. Improving generalization of learning systems
has become a major research topic in recent years, with many different threads of research includ-
ing, but not limited to, robust optimization (e.g., Hoffman et al. (2018)) and domain adaptation
(e.g., Johansson et al. (2019)). Both of these research directions, however, have their own intrin-
sic limitations (Ahuja et al. (2020)). Recently, there have been proposals of approaches that learn
environment-invariant representations. The motivating idea is that the behavior of a model being
invariant across environments makes it more likely that the model has captured a causal relationship
between features and prediction targets. This in turn should lead to a better generalization behavior.
Invariant risk minimization (IRM, Arjovsky et al. (2019)), which pioneered this idea, introduces a
new optimization loss function to identify non-spurious causal feature-target interactions. Invari-
ant risk minimization games (IRMG, Ahuja et al. (2020)) expands on IRM from a game-theoretic
perspective.
The assumption of IRM and its extensions, however, is that all environments are available to the
learning system at the same time, which is unrealistic in numerous applications. A learning agent
experiences environments often sequentially and not concurrently. For instance, in a federated learn-
ing scenario with patient medical records, each hospital’s (environment) data might be used to train
a shared machine learning model which receives the data from these environments in a sequential
manner. The model might then be applied to data from an additional hospital (environment) that
was not available at training time. Unfortunately, both IRM and IRMG are incompatible with such a
continual learning setup in which the learner receives training data from environments presented in
a sequential manner. As already noted by Javed et al. (2020), “IRM Arjovsky et al. (2019) requires
sampling data from multiple environments simultaneously for computing a regularization term per-
tinent to its learning objective, where different environments are defined by intervening on one or
more variables of the world.” The same applies to IRMG (Ahuja et al. (2020))
1
Under review as a conference paper at ICLR 2021
To address the problem of learning environment-invariant ML models in sequential environements,
we make the following contributions:
•	We expand both IRM and IRMG under a Bayesian variational framework and develop novel
objectives (for the discovery of invariant models) in two scenarios: (1) the standard multi-
environment scenario where the learner receives training data from all environments at the same
time; and (2) the scenario where data from each environment arrives in a sequential manner.
•	We demonstrate that the resulting bilevel problem objectives have an alternative formulation,
which allows us to compute a solution efficiently using the alternating direction method of multi-
pliers (ADMM).
•	We compare our method to ERM, IRM, IRMG, and various continual learning methods (EWC,
GEM, MER, VCL) on a diverse set of tasks, demonstrating comparable or superior performance
in most situations.
2	Background: Offline Invariant Risk Minimization
We consider a multi-environment setting where, given a set of training environments E =
{eι ,e2,…，em},the goal is to find parameters θ that generalize well to unseen (test) environments.
Each environment e has an associated training data set De and a corresponding risk Re
Re(W ◦ O)= E(χ,y)〜De'e((W ◦ O)(X), y),	⑴
where fθ = w ◦ φ is the composition of a feature extraction function φ and a classifier (or regression
function) W. Empirical Risk Minimization (ERM) minimizes the average loss across all training
examples, regardless of environment:
RERM(θ) = E(x,y)〜∪e∈E De '(fθ (x),y)∙	⑵
ERM has strong theoretical foundations in the case of iid data (Vapnik (1992)) but can fail dramat-
ically when test environments differ significantly from training environments. To remove spurious
features from the model, Invariant Risk Minimization (IRM, Arjovsky et al. (2019)) instead aims to
capture invariant representations O such that the optimal classifier W given O is the same across all
training environments. This leads to the following multiple bi-level optimization problem
min	Re(W ◦ O) s.t. W ∈ arg min Re(We ◦ O), ∀e ∈ E,	(3)
φ∈Hφ,w∈Hw	we∈Hw
e∈E
where Hφ , Hw are the hypothesis sets for, respectively, feature extractors and classifiers. Unfortu-
nately, solving the IRM bi-level programming problem directly is difficult since solving the outer
problem requires solving multiple dependent minimization problems jointly. We can, however, relax
IRM to IRMv1 by fixing a scalar classifier and learning a representation O such that the classifier is
“approximately locally optimal” (Arjovsky et al. (2019))
min X Re(φ) + λ∣∣Vw∣w=1.0Re(wφ)∣∣2,∀e ∈ E,	(4)
φ∈Hφ e∈E
where W is a scalar evaluated in 1 and λ controls the strength of the penalty term on gradients on
W. Alternatively, the recently proposed Invariant Risk Minimization Games (IRMG) (Ahuja et al.
(2020)) proposes to learn an ensemble of classifiers with each environment controlling one compo-
nent of the ensemble. Intuitively, the environments play a game where each environment’s action is
to decide its contribution to the ensemble aiming to minimize its risk. Specifically, IRMG optimizes
the following objective:
min	Re(W ◦	φ)	s.t.	We	= argminRe	(	ɪ-(w +	w—) ◦ φ)	, ∀e	∈ E, (5)
φ∈Hφ e∈E	w∈Hw	|E|
where W=告 Pg∈e We is the average and w-e = Pe∈p,H=已 wr the complement classifier.
3	Continual IRM by Approximate Bayesian Inference
Both IRM and IRMG assume the availability of training data from all environments at the same
time, which is impractical and unrealistic in numerous applications. A natural approach would be
2
Under review as a conference paper at ICLR 2021
to combine principles from IRM and continual learning. Experience replay, that is, memorizing ex-
amples of past environments and reusing them later, could be possible in some scenarios but it is
often difficult to estimate a-priori the extend of replay necessary to achieve satisfactory general-
ization capabilities. Here, we propose to adopt a probabilistic approach, exploiting the propagation
of the model distribution over environments using Bayes’ rule. We integrate both IRM and IRMG
with stochastic models, introducing their variational counterparts that admit a continual extension.
In addition, our approach is justified by the property of the KUllback-Leibler (KL) divergence that
promotes invariant distributions when used in sequential learning (as shown in Theorem 3).
3.1	Variational Continual Learning
Following prior work in continual learning (Nguyen et al. (2018)), let Dt be the training data from
the t-th environment et , let D1t be the cumulative data up to the t-th environment, and let θ be the
parameters of the feature extractor. When each environment is given in a sequential manner, we can
use Bayes’ rule and we have (all proofs are provided in the supplementary material)
p(θ∣Dt) H p(θ∣Dt-1)p(Dt∣θ),	(6)
that is, once We have the posterior distribution p(θ∣Dt-1) at time t 一 1, we can obtain, by apply-
ing Bayes rule, the posterior p(θ |D1t) at time t up to a normalization constant. This is achieved by
multiplying the previous posterior with the current data likelihoodp(Dt∣θ). The posterior distribu-
tion is in general not tractable and we use an approximation. With the variational approximation,
p(θ∣Dt) ≈ qt(θ), it is thus possible to propagate the variational distribution from one environment
to the next. From Corollary 14 (in the supplementary material) we can write the continual variational
Bayesian inference objective as
qt(θ) = argmin E(x,y)〜Dt Eθ^q(θ){'(y,fθ (x))} + DκL(q(θ)∣∣qt-ι(θ)),	⑺
q(θ)
from the variational distribution at step qt-1(θ), with fθ = w ◦ φ, a function with parameters θ.
3.2	Equivalent formulation of IRM as a Bilevel Optimization Problem (BIRM)
In order to extend the IRM principle of Equation 3 using the principle of approximate Bayesian
inference, by applying Lemma 5 (in supplementary material), we first introduce the following new
equivalent definition of IRM (equation 3).
Definition 1 (Bilevel IRM (BIRM)). Let Hφ be a set of feature extractors and let
Hw be the set of possible classifiers. An invariant predictor w ◦ φ on a set of en-
vironments E is said to satisfy the Invariant Risk Minimization (IRM) property, if it
is the solution to the following bi-level Invariant Risk Minimization (BIRM) problem
min	SX Re(W ◦ φ)	(8a) s.t. NwRe(W ◦ φ) = 0,∀e ∈ E. (8b)
φ∈Hφ,w∈Hw
e∈E
This formulation results from substituting the minimization conditions in the constraint set of the
original IRM formulation with the Karush-Kuhn-Tucker (KKT) optimality conditions. This new
formulation allows us to introduce efficient solution methods and simplifies the conditions of IRM.
It also justifies the IRMv1 model; indeed, when the classifier is a scalar value and the equality
constraint is included in the optimization cost function, we obtain Equation 4. To solve the BIRM
problem, we propose to use the Alternating Direction Method of Multipliers (ADMM) (Boyd et al.
(2011)). ADMM is an alternate optimization procedure that improves convergence and exploits the
decomposability of the objective function and constraints. Details of the BIRM-ADMM algorithm
are presented in the supplementary material.
3.3	Bilevel Variational IRM
At this point, we cannot yet directly extend the IRM principle using variational inference. That is
because if we observe all environments at the same time, the prior of the single environment is data
independent. Therefore, we substitute qt-1(θ) from Equation 7 with priors pφ(θ) andpw(ω), where
θ and ω are now the parameters of the two functions φ and W. We also substitute qt(θ) with the
variational distributions qφ(θ) and qw(ω).
3
Under review as a conference paper at ICLR 2021
Definition 2 (Bilevel Variational IRM (BVIRM)). Let Pφ be a family of distributions
over feature extractors, and let Pw be a family of distributions over classifiers. A vari-
ational invariant predictor on a set of environments E is said to satisfy Bilevel Varia-
tional Invariant Risk Minimization (BVIRM) if it is the solution to the following problem:
min X Qφ(qw,qφ)	(9a)	s∙t. VqwQw(qw,qφ) =0, Ve ∈ E, (9b)
qφ ∈Pφ
qw∈Pw e∈E
with	Qφ(qw,qφ) = EwFw Re(W ◦ Φ) + βDκL(qφ∣∣Pφ)+βDκL(qw∣∣pw),	(l0a)
φ~q⅛
and	Qw(qw,qφ) = EwFw Re(W ◦ Φ) + βDκL(qw||pw),	(10b)
φ~q⅛
and where pφ and pw are the priors of the two distributions. β is a hyper-parameter balancing the
ERM and closeness to the prior.
Definition 2 extends Definition 1 with the objective of Eq.7, where the parameters φ and W are
substituted by their distributions qφ and qw . The gradient of the cost in the inner problem is taken
with respect to the distribution qw . When we parameterize qφ with θ and qw with ω, the gradient is
evaluated with respect to these parameters1, since the condition implies that the solution is locally
optimal. If Q(p, q) is convex in the first argument, then the solution is globally optimal. This defini-
tion extends the IRM principle to the case where we use approximate Bayes inference, shaping the
variational distributions qw and qφ, to be, in expectation, invariant and optimal across environments.
3.4	The BVIRM ADMM Algorithm
As noted for the BIRM definition, the solution of the variational BVIRM formulation can be ob-
tained by using ADMM (Boyd et al. (2011)). While in general there are no convergence results
of ADMM methods for this problem, for local minima, under proper conditions 2, the stochastic
version of ADMM converges with rate O(1∕√t) for convex functions and O (log t/t) for strongly
convex functions (Ouyang et al. (2013)). We are now in the position to write the BVIRM-ADMM
formulation of the BVIRM problem. ADMM is defined by the update Eq.11, where we denote with
the apexes - and + the value of any variable before and after the update. Moreover, we abbreviate
as follows Q(ω, θ) = Q(q(ω), q(θ)).
ωe+	arg min Lρ(ωe, ue-, ω-, ve-), ∀e ∈ E, ωe	(11a)
ω+	1/|E| X(ωe + ue) e	(11b)
ue	ue- + (ωe+ - ω+)	(11c)
ve+	ve- + Vq(ω)Qew(ωe+, θ)	(11d)
with
LP(We,ue,w,Ve)	= Qφ(ωe, θ) + ^0 kωe - ω + uek2 + ɪ kvq(ω)Qw(ωe ◦ φ) + ve∣∣2∙ (12)
Here, φ is fixed and θ is updated in an external loop or given (e.g. the identity function). In the
experiment we use stochastic Gradient Descent (SGD) to update both the model parameters We and
the feature extractor parameters φ. The result follows by applying Lemma 11 in the supplementary
material and substituting Xi J (We), fi(xi) J Qφ(ωe,θ) and gi(xi) J Vq(ω)Qw(ω+,θ). We
provide a pseudo-code implementation leveraging Equation 11 as Algorithm 1. One of the advan-
tages of the ADMM formulation of BVIRM of Eq.11, is that it can be computed in parallel, where
only Eq.11b requires synchronization among environments, while the other steps can be computed
independently.
1 Implementation detail using the mean field parameterization and reparametrization trick is provided in the
Supplementary Material 2 These conditions are specific bounds on the magnitude and variance of the
(sub-)gradients of the stochastic function (Ouyang et al. (2013)). We used ELU ∈ C∞ in the experiments.
4
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 1: w,φ - BVIRM-
ADMM(E, Re) ADMM version of the
Bilevel Variational IRM Algorithm
Result: w ◦ φ : feature extraction and
classifier for the environment E
// Randomly initialize the
variables
ω,ωe,Ue,Ve,θ — Init();
// Outer loop (on θ) and
Inner loop (on ω)
while not converged do
// Update φ using SGD
θ=SGDθ(Pe∈EQeφ(qw,qφ));
for k = 1, . . . , K do
for e ∈ E do
ωe = SGDωeLρ(ωe, ue, ω, ve) ;
ω = 1∕∣E∣ Pe(ωe + Ue);
ue = ue + (ωe - ω) ;
Ve = Ve + VωQeM/)；
end
end
end
Algorithm 2: w,φ - C-BVIRM-
ADMM(E, Re) ADMM version of the
Bilevel Variational IRM Algorithm
Result: wω ◦ φ - θ : feature extraction and
classifier for the environment E
// Randomly initialize the
variables
1 ω,ωe,Ue,Ve,θ — Init();
2	ω 二 0 ；
3	for e ∈ E do
4
5
6
7
8
9
10
11
12
for k = 1, . . . , K do
θ=SGDθ(Qeφ(qw,qφ)) ;
while not converged do
// Update ω using SGD
and ADMM
ωe = SGDωeLρ(ωe, ue, ω, Ve) ;
ω = 1∕2(ω. + Ue + ω);
ue = ue + (ωe - ω) ;
Ve = Ve + VωQe(ωe, θ) ;
end
end
13	0 = 3e ；
14 end
3.4.1 The Continual BVIRM ADMM Algorithm
In presence of sequential environments, the priors for the new environment are given by the pre-
vious environment’s distributions qφ- and qw-, this is obtained by comparing the BVIRM defini-
tion in Eqs. (9) with the continual Bayesian learning Equation (7). In Equation 10 we thus now
have Qφ(qw,qφ) = Ew〜q.,φ〜qφ Re(W ◦ φ) + βDκL(qφ∣∣q-)+β°κL(qw||q-) and Qw(qw,qφ)=
Ew〜qw,Φ〜qφ Re(W ◦ φ) + βDκL(qw||q-) Algorithm 2 presents an example implementation of
ADMM3 applied to the continual BVIRM formulation.
3.5 Information-Theoretic interpretation of C-BVIRM
The KL divergence provides an additional motivation for the meth-
ods We propose. Indeed, for causal discovery Peters et al. (2015)
suggests a discovery mechanism for causal variables as the intersec-
tion of the invariant conditional distributions across environments
subject to interventions. The KL divergence is asymmetric and only
components present in the first argument distribution are evaluated.
This implies that by using the KL divergence we can compute the
intersection of the distributions, even when these are observed se-
quentially. This can be made more explicit by the property of the
information projection Cover (1999)
Theorem 3 (Information Projection). If P and Q are two fam-
ilies of distributions with partially overlapping support, 0 ⊂
supp(P) supp(Q), andq ∈ Q, then
P* = arg min Dkl(p∣W)
p∈P
Figure 1: Sequential pro-
jection of distributions
p1, . . .pt, where pi+1 =
arg minp∈Pi+ι DκL(P∣∣Pi)
has support in the intersection for the support ofP and q, or supp(p*) ⊆ supp(P) supp(q).
3 In Algorithm 1 the ADMM update equation is implemented from line 6 to line 9, while in Algorithm 2, from
line 7 to line 10.
5
Under review as a conference paper at ICLR 2021
Figure 2: The two color models (on the left b01, on the right b11) for the train (upper row) and test
(lower row) of the MNIST (left) and FashionMNIST (right) datasets.
Therefore, if we have a sequence of sets of distributions of models from intervention environments
and we compute the projection in sequence, the final projected distribution has support on the inter-
section of all previous distribution families, or supp(Pt) = Tit=1 supp(Pi) (see Figure 1) , since at
each step pi+1 = arg minp∈Pi+1 DKL(p||pi) .
4	Related Work
Generalization Domain adaptation (Ben-David et al., 2007; Johansson et al., 2019) aims to learn
invariant features or components φ(x) that have similar P (φ(x)) on different (but related) domains
by explicitly minimizing a distribution discrepancy measure, such as the Maximum Mean Discrep-
ancy (MMD) (Gretton et al., 2012) or the Correlation Alignment (CORAL) (Sun & Saenko, 2016).
The above condition, however, is not sufficient to guarantee successful generalization to unseen do-
mains, even when the class-conditional distributions of all covariates changes between source and
target domains (Gong et al., 2016; Zhao et al., 2019). Robust optimization (Hoffman et al., 2018;
Lee & Raginsky, 2018), on the other hand, minimizes the worst performance over a set of possi-
ble environments E, that is, maxe∈E Re(θ). This approach usually poses strong constraint on the
closeness between training and test distributions (Bagnell, 2005) which is often violated in practical
settings (Arjovsky et al., 2019; Ahuja et al., 2020).
Incorporating the machinery of causality into learning models is a recent trend for improving gener-
alization. (Bengio et al., 2019) argued that causal models can adapt to sparse distributional changes
quickly and proposed a meta-learning objective that optimizes for fast adaptation. IRM, on the other
hand, presents an optimization-based formulation to find non-spurious actual causal factors to target
y. Extensions of IRM include IRMG and the Risk Extrapolation (REx) (Krueger et al., 2020). Our
work’s motivation is similar to that of online causal learning (Javed et al., 2020), which models the
expected value of target y given each feature as a Markov decision process (MDP) and identifies the
spurious feature xi if E[y|xi] is not consistent to temporally distant parts of the MDP. The learning
is implemented with a gating model and behaves as a feature selection mechanism and, therefore,
can be seen as learning the support of the invariant model. The proposed solution, however, is only
applicable to binary features and assumes that the aspect of the spurious variables is known (e.g. the
color). It also requires careful hyper-parameter tuning. In the cases where data is not divided into
environments, Environment Inference for Invariant Learning (EIIL) classification method (Creager
et al. (2020)) aims at splitting the samples into environments. This method proves to be effective
also when the environment label is present.
Continual Learning Kirkpatrick et al. (2017); De Lange et al. (2019) addresses the problem of
learning one classifier that performs well across multiple tasks given in a sequential manner. The
focus is on the avoidance of catastrophic forgetting. With our work, we shift the focus of continual
learning to the study of a single task that is observed in different environments.
5	Experimental Evaluation
5.1	Datasets and Experiment Setup
Colored MNIST Figure 2 (left) shows a sample of train (upper) and test (lower) samples. In
each training environment, the task is to classify whether the digit is, respectively, even or odd.
As in prior work, we add noise to the preliminary label by randomly flipping it with a probability
6
Under review as a conference paper at ICLR 2021
Table 1: Mean accuracy (N = 5) on train and test environments when training on 2 consecutive
environments on MNIST and the b01 color correlation.
	/ 标	£ O	6		/	/	Q 4	3 宇	/	W	O W
			G								
train	71.3	69.1	51.4	86.4	87.4	87.4	86.3	85.3	87.3	89.3	89.3
	(4.2)	(2.8)	(3.4)	(1.2)	(2.7)	(2.7)	(1.2)	(0.8)	(1.7)	(0.6)	(0.6)
test	29.6	27.9	46.0	12.7	15.7	15.7	12.8	9.9	14.8	24.9	24.9
	(3.3)	(8.5)	(2.1)	(2.7)	(4.2)	(4.2)	(2.6)	(0.2)	(3.8)	(1.9)	(1.9)
of 0.25. The color of the image is defined by the variable z, which is the noisy label flipped with
probability pc ∈ [0.1, 0.2]. The color of the digit is green if z is even and red if z is odd. Each train
environment contains 30, 000 images of size 28 × 28 pixels, while the test environment contains
10, 000 images where the probability pc = 0.9. The color of the digit (b01) or the background
(b11) is thus generated from the label but depends on the environment. Figure 3 depicts the causal
graph (the hammer indicating the effect of the intervention) of the environment. The variable “Color”
is inverted when moving from the training to test environment.
Colored FashionMNIST, KMNIST, and EMNIST Figure 2 (right)
shows the Fashion-MNIST dataset, where the variable z defines the back-
ground color. Again, we add noise to the preliminary label (y = 0 for
“t-shirt”, “pullover”, “coat”, “shirt”, “bag” and y = 1 for “trouser”,
“dress”, “sandal”, “sneaker”, “ankle boots”) by flipping it with 25 per-
cent probability to construct the final label. Besides, we also consider
Kuzushiji-MNIST dataset Clanuwat et al. (2018)4 and the EMNIST Let-
ters dataset Cohen et al. (2017)5. The former includes 10 symbols of Hi-
ragana, whereas the latter contains 26 letters in the modern English alpha- Figure 3: Causal re-
bet. For EMNIST, there are 62, 400 training samples per environment and lationships of colored
20, 300 test samples. We set y = 0 for letters ‘a’, ‘c’, ‘e’, ‘g’, ‘i’, ‘k’, ‘m’, MNIST.
‘o’, ‘q’, ‘s’, ‘u’, ‘v’, ‘y’ and y = 1 for remaining ones.
Reference Methods. We compare with a set of popular reference methods in order to show the ad-
vantage of the variational Bayesian framework in learning invariant models in the sequential environ-
ment setup. For completeness, we also evaluate the performances of four reference continual learn-
ing methods. These include Elastic Weight Consolidation (EWC, Kirkpatrick et al. (2017)), Gra-
dient Episodic Memory (GEM, (Lopez-Paz & Ranzato, 2017))6, Meta-Experience Replay (MER,
Riemer et al. (2018))7, and Variational Continual Learning (VCL, Swaroop et al. (2019); Nguyen
et al. (2018))8. ERM is the classical empirical risk minimization method; we always use the cross-
entropy loss. IRMv1 enforces the gradient of the model with respect to a scalar to be zero. IRMG
models the problem as a game among environments, where each environment learns a separate
model. EWC imposes a regularization cost on the parameters that are relevant to the previous task,
where the relevance is measured by Fisher Information (FI); GEM uses episodic memory and com-
putes the updates such that accuracy on previous tasks is not reduced, using gradients stored from
previous tasks; MER uses an efficient replay memory and employs the meta-learning gradient up-
date to obtain a smooth adaptation among tasks; VCL and Variational Continual Learining with
coreset VCLC apply variational inference to continual learning. C-VIRMv1 and C-VIRMG refer
to, respectively, our proposed variational extensions of IRMv1 and IRGM in sequential environ-
ments. C-BVIRM is the implementation with ADMM.
All hyper-paramter optimization strategies and simulation configurations are discussed in detail in
the supplementary material.
4 https://github.com/rois-codh/kmnist 5 https://www.nist.gov/itl/products-and-services/emnist-dataset
6 https://github.com/facebookresearch/GradientEpisodicMemory 7 https://github.com/mattriemer/mer
8 https://github.com/nvcuong/variational- continual- learning
7
Under review as a conference paper at ICLR 2021
Table 2: Mean accuracy (over 5 runs) and standard deviation at test time for (n) 2, 6, 10 environ-
ments, (d) across datasets, and (c) for the two color correlations (b01,b11).
	标	G	3		/	/		/	/	W	O W
			U	/							
n2	29.6	27.9	46.0	12.7	15.7	15.7	12.8	9.9	14.8	24.9	24.9
	(3.3)	(8.5)	(2.1)	(2.7)	(4.2)	(4.2)	(2.6)	(0.2)	(3.8)	(1.9)	(1.9)
6	28.8	27.5	47.1	11.1	15.6	15.6	12.2	9.7	15.3	15.4	15.4
	(4.1)	(2.6)	(3.0)	(2.9)	(5.7)	(5.7)	(2.8)	(0.3)	(4.4)	(1.1)	(1.1)
10	21.8	25.2	31.0	10.2	17.7	17.7	12.4	10.2	15.5	10.8	10.8
	(2.4)	(4.5)	(7.1)	(0.2)	(5.5)	(5.5)	(2.1)	(0.2)	(4.1)	(0.2)	(0.2)
d	MNIST	29.6	27.1	46.8	12.7	15.7	15.7	12.8	9.9	14.8	24.9	24.9
	(3.3)	(7.6)	(2.6)	(2.7)	(4.2)	(4.2)	(2.6)	(0.2)	(3.8)	(1.9)	(1.9)
Fa-MNIST	36.3	26.7	48.2	10.7	15.4	15.3	10.8	9.9	13.2	24.9	24.9
	(4.3)	(8.7)	(3.6)	(1.5)	(5.1)	(5.4)	(1.4)	(0.2)	(2.8)	(2.0)	(2.0)
KMNIST	32.8	24.2	46.5	12.0	14.0	14.0	12.1	9.9	15.6	24.9	24.9
	(4.6)	(6.0)	(1.9)	(2.1)	(3.5)	(3.5)	(2.4)	(0.2)	(4.1)	(2.0)	(2.0)
EMNIST	32.1	25.0	45.9	10.8	15.3	14.8	10.8	10.0	12.6	24.9	24.9
	(4.6)	(7.5)	(2.5)	(1.0)	(3.6)	(3.7)	(1.2)	(0.2)	(2.3)	(2.0)	(2.0)
c	b01	29.6	27.1	46.8	14.9	18.8	18.8	14.6	9.8	18.0	24.9	24.9
	(3.3)	(7.6)	(2.6)	(0.8)	(1.3)	(1.3)	(0.8)	(0.1)	(1.0)	(2.0)	(2.0)
b11	38.8	23.9	43.3	9.9	12.5	12.5	9.8	9.9	11.5	24.9	24.9
	(4.2)	(6.9)	(4.5)	(0.2)	(3.6)	(3.6)	(0.1)	(0.2)	(2.0)	(2.0)	(2.0)
5.2	Results
Table 1 lists the training and test accuracy on the MNIST dataset with the color correction b01 (see
Figure 2 left). Since we introduced label noise by randomly flipping 25 percent of the given la-
bels, a hypothetical optimal classifier would be able to achieve an accuracy of 75% in both training
and test environments. ERM, IRMv1, and IRMG perform poorly in the setup where environments
are given sequentially. Similarly, reference continual learning methods also fail to learn invariant
representation in the new environment. As these models are learning to mainly use spurious fea-
tures for the classification problems at hand, here: the colors of the digits (red〜odd; green〜even),
they perform poorly (much worse than a random baseline) when the spurious feature properties are
inverted (green〜odd; red〜even). In contrast, our variational extensions to both IRM and IRMG
achieve a classification accuracy higher than 45% on the test data. This implies that our model is
not relying exclusively on spurious correlations present in the color of digits. By comparing the
performance between C-VIRMv1 and C-BVIRM, we conclude that (1) our proposed bilevel invari-
ant risk minimization framework (i.e., the BIRM in Definition 1) is an effective alternative to the
original formulation Arjovsky et al. (2019); and (2) ADMM is effective in solving the BIRM op-
timization problem and has the potential to improve the generalization performance. In addition,
one can observe that the KL divergence term in VCL and our framework significantly improves the
test accuracy with respect to the baseline counterparts. This result further justifies our motivation
of using a variational Bayesian framework for the problem of continual invariant risk minimization.
Table 2 lists the accuracy on the test environment for: (n) (upper rows) an increasing number of se-
quential environments (d) (central rows) different datasets, and (c) (lower rows) the two given color
correlation schemes. We can observe that there is a general trend in the results. IRMG and IRM,
with an accuracy of less than 10%, are not able to learn invariant models. Similarily, the continual
learning reference methods (MER, EWC, MER, VCL, VCLC) also fail with a test accuracy of under
25%. The proposed methods on the other hand provide mechanism to learn more robust features and
classification models. The higher variance of the accuracy is caused by the stochastic nature of the
variational Bayesian formulation.
5.3	Environment Inference for Continual invariant learning
In practical applications, the environmental labels are usually unavailable, which means that it is
difficult or impossible to manually partitioning the training set into “domains” or “environments”.
8
Under review as a conference paper at ICLR 2021
Table 3: Mean accuracy (over 10 runs) on train and test environments when training off-line on 2
environments on Colored-MNIST, with the EIIL. (pc1 = 0.2, pc2 = 0.1, 500000 samples)
	IRMv1 Train	Test	C-VIRMv1 Train	Test
No EIIL	70.73 (1.16)	67.48 (1.96)	70.99 (0.90)	66.60 (2.66)
EIIL	73.78 (0.61)	67.96 (3.01)	75.29 (0.53)	68.40 (1.11)
Table 4: Mean accuracy (over 5 runs) on train and test environments when training on 1 environment
on Colored-MNIST, with and without EIIL. (pc1 = 0.1)
Without Environment Inference	With Environment Inference (EIIL)
Ns	IRMv1 Train	Test	C-VIRMv1 Train	Test	IRMv1 Train	Test	C-VIRMv1 Train	Test
1’000	93.7 (0.7)	13.5 (1.7)	94.1 (1.1)	13.7(1.5)	95.5 (0.3)	12.7 (2.0)	96.0 (0.4)	16.5 (6.0)
2’000	91.5 (0.4)	12.7 (0.9)	91.1 (0.7)	11.9 (0.9)	92.6 (0.4)	27.8 (2.8)	93.3 (0.7)	29.3 (3.4)
5’000	90.2 (0.4)	10.5 (1.1)	90.1 (0.4)	10.6 (0.7)	91.6 (0.4)	29.6 (4.8)	91.6 (0.9)	30.6 (3.2)
10’000	89.9 (0.3)	10.1 (0.5)	90.0 (0.2)	10.1 (0.1)	85.3 (1.0)	42.9 (3.9)	83.7 (1.2)	50.4 (2.3)
20’000	90.0 (0.2)	10.1 (0.2)	90.1 (0.2)	10.1 (0.0)	77.2 (1.2)	57.4 (2.2)	77.9 (1.1)	57.6 (2.0)
50’000	90.1 (0.1)	9.7 (0.4)	90.0 (0.1)	10.0 (0.4)	73.9 (0.5)	67.2 (1.2)	74.0 (0.5)	67.3 (1.0)
In order to generalize our continual invariant learning models to an environment-agnostic setting, we
leverage the recently proposed Environment Inference for Invariant Learning (EIIL) by Creager et al.
(2020) to automatically infer environment partitions from observational training data, and integrate
EIIL into our continual invariant learning models.
We take our proposed C-VIRMv1 as an example. According to Table 3, it is easy to observe that
inferring environments directly from observational data (using EIIL) has the potential to improve
(continual) invariant learning relative to using the hand-crafted environments. Moreover, C-VIRMv1
with EIIL improves both training and test accuracy, compared with IRMv1 with EIIL. In fact, this
environment partition strategy also enables invariant learning with only one environmental data.
Table 4 further suggests that the generalization accuracy improves for both IRMv1 and C-VIRMv1
as the number of training samples increases. Again, we observed that, when combined with EIIL,
C-VIRMv1 always outperforms IRMv1.
6	Conclusions
We aim to broaden the applicability of IRM to settings where environments are observed sequen-
tially. We show that reference approaches fail in this scenario. We introduce a variational Bayesian
approach for the estimation of the invariant models and a solution based on ADMM. We evaluate
the proposed approach with reference models, including those from continual learning, and show a
significant improvement in generalization capabilities.
References
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk min-
imization games. arXiv preprint arXiv:2002.04692, 2020.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David LoPez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
J Andrew Bagnell. Robust suPervised learning. In Proceedings of the national conference on artifi-
cial intelligence, volume 20, PP. 714, 2005.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of rePresentations
for domain adaptation. In Advances in neural information processing Systems, pp. 137-144,2007.
9
Under review as a conference paper at ICLR 2021
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sebastien Lachapelle, Olexa Bila-
niuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle
causal mechanisms. arXiv preprint arXiv:1901.10912, 2019.
Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge uni-
versity press, 2004.
Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the
alternating direction method of multipliers. Now Publishers Inc, 2011.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Elliot Creager, Jorn-Henrik Jacobsen, and Richard ZemeL Environment inference for invariant learn-
ing. In ICML Workshop on Uncertainty and Robustness, 2020.
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory
Slabaugh, and Tinne Tuytelaars. Continual learning: A comparative study on how to defy forget-
ting in classification tasks. arXiv preprint arXiv:1909.08383, 2019.
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Scholkopf. Domain adaptation with conditional transferable components. In International Con-
ference on machine learning, pp. 2839-2848, 2016.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. In Advances in Neural Information Processing Systems, pp. 8246-8256, 2018.
Khurram Javed, Martha White, and Yoshua Bengio. Learning causal models online. arXiv preprint
arXiv:2006.07461, 2020.
Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-
invariant representations. arXiv preprint arXiv:1903.03448, 2019.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le
Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). arXiv
preprint arXiv:2003.00688, 2020.
Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In Ad-
vances in Neural Information Processing Systems, pp. 2687-2696, 2018.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, pp. 6467-6476, 2017.
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learn-
ing. In International Conference on Learning Representations, 2018.
Hua Ouyang, Niao He, Long Tran, and Alexander Gray. Stochastic alternating direction method of
multipliers. In International Conference on Machine Learning, pp. 80-88, 2013.
10
Under review as a conference paper at ICLR 2021
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Peter Buhlmann, and NicoIai Meinshausen. Causal inference using invariant Predic-
tion: identification and confidence intervals. arXiv preprint arXiv:1501.01332, 2015.
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald
Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing interfer-
ence. arXiv preprint arXiv:1810.11910, 2018.
Baochen Sun and Kate Saenko. DeeP coral: Correlation alignment for deeP domain adaPtation. In
European conference on computer vision, pp. 443-450. Springer, 2016.
Siddharth SwarooP, Cuong V. Nguyen, Thang D. Bui, and Richard E. Turner. ImProving and
understanding variational continual learning. arXiv:1905.02099 [cs, stat], 2019. URL http:
//arxiv.org/abs/1905.02099.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in neural infor-
mation processing systems, pp. 831-838, 1992.
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. arXiv preprint arXiv:1901.09453, 2019.
11
Under review as a conference paper at ICLR 2021
A Supplementary Material
A. 1 Variational Invariant Risk Minimization games
We now consider the IRMG objective and extend it with the variational Bayesian inference. If we
observe all environment at the same time, the prior of the single environment is data independent.
From Equation 7, we thus substitute qt-1 (θ) with a priors pφ(θ) and qw (ω), where θ and ω are
now the parameters of the two functions φ and w. While we substitute qt (θ), with the variational
distributions qφ(θ) and qw(ω). The outer problem is now
min
qφ
s.t.
Eφ〜q(φ)Re(W ◦ Φ)+ βDκL(qφ∣∣Pφ)
(13a)
qwe = arg min Ew〜qwe Re(看(W + w-e) ◦ Φ) + βDκL(qwe ∣∣Pw )∀e ∈ Etr (13b)
where W =	|E1 Pe∈Etr We, We 〜 qwe (w) is the average classifier and w-e =
Pe0∈Etr e0=e We，,比匕，〜 qwe0(w) is the complement classifier. In the reformulation of the IRMG
model, we weight the distance of the varional distribution to the prior with β. We notice how the
difference of the variational formulation of IRMG differs on the presence of the mean on the distri-
bution of the function over the variational distributions and the KL term.
We can now finally extend IRMG when the environments are observed sequentially. Combining
the definition of IRMG Eqs. (5) with the continual bayesian learning Equation (7), we obtain the
variational objective of IRMG in sequential environment case.
min	Eφ〜q(φ){'(y,W ◦ φ)} + βDκL(qφ|%一1)	(14a)
qφ
s.t.	W = 2(w + Wt-1 ),W 〜qw (w), Wt-1 〜qtt-1 (W)	(14b)
qw = argmmin Ew〜qwe ,φ〜qφ {'(y, 2(w + Wt-1) ◦ Φ} + βDκL(qwe ||qw-1)	(14c)
We can similarly extend the definition of IRMv1 when all environments are seen at the same time
and sequentially.
A.2 Mean Field Parametrization and reparametrization trick
When we want to implement Equation 11 and Equation 12 and the different variation, we use
the mean field approximation and the reparametrization trick Kingma & Welling (2013). In this
case the density function of our model is parameterized by θ and ω and constraints becomes
Vq(ω)QW(ω+, θ) = 0 → VωQw(ω, θ) = 0. If We then parametrize μ(ω*) and σ(ωσ) the mean
and standard deviation and model the distribution as qω (w) = μ(ω*) + eσ(ωσ), with E 〜N(0,1)
We now want to compute the gradient (in the following we ignore the dependence on the φ and its
parameters)
VωQ(ω) = VωEw〜q(ω)R(W ◦ φ) + βVDκL(q(ω)∣∣p)
The second term is
Vω DκL(q∣∣P) = Vμ DκL(q∣∣P)Vω μ + VσDκL(q∣∣PM σ
with
Vω μ =1, Vω σ =1
E
▽xDHLgIlP) = -σp l(μp - μq )
Vσ DκL(q∣∣p) = — diag(σq )-1 + diag(σp)-1
where we assume σp , σq to be diagonal, in this way the previous equation can be evaluated element-
wise and where the DκL (qIIp) is defined as
DκL(qllp) = ln π=rr — n + 证{夕-1n} + (μp — μq)Tς-1Jp — μq)
IΣqI
12
Under review as a conference paper at ICLR 2021
The first term is evaluated by Monte Carlo sampling
1N
Vω Ew 〜q(ω)R(W) ≈ R ω NfR(Wi)
N i=1
with
Wi = μ(ω) + Ei Θ σ(ω)
and Wi 〜N(0,1). Also in this case
1N	1N	1N
Vω NER(Wi) = Vμ NER(WiW ω 〃 + R σ NER(WiW ω σ
i=1	i=1	i=1
A.3 THE BIRM-ADMM ALGORITHM
We observe that to solve BIRM we can use Lemma 11 and write the following algorithm
We+	arg min Lρ(We, ue-, W- , ve- ), ∀e ∈ E we	(15a)
W+	1/|E| X(We +ue) i	(15b)
ue+	ue- + (We+ - W+ )	(15c)
ve+	ve- + VwRe(We+ ◦ φ)	(15d)
where
Lρ(We,Ue,W,Ve) = Re(We ◦ φ) + P IlWe - W + Ue∣∣2 + p2 ∣∣Vw Re (We ◦ φ) + Ve || 2 (16)
We denote .+, .- the values of the variable after and before the update. In order to implement the
method we use the SGD to update the model We and in a outer loop updating for φ.
1
2
3
4
5
6
7
8
9
10
11
12
Algorithm 3: w, φ - BIRM-ADMM(E, Re) ADMM version of the Bilevel IRM Algorithm
Result: w ◦ φ : feature extraction and classifier for the environment E
// Randomly initialize the variables
W, We,Ue, Ve, Φ - I□it()；
// Outer (on φ) and Inner loop (on W)
while not converged do
// Update φ using stochastic gradient descent(SGD)
φ = SGDφ(Pe Re(W ◦ φ)) ;
for k = 1, . . . , K do
for e ∈ E do
We = SGDweLρ(We, ue, W, ve) ;
W = 1/|E| Pe(We +ue) ;
ue = ue + (We - W) ;
ve = ve + Vw R (We ◦ φ) ;
end
end
end
A.4 Variational Invariant Risk Minimization
Definition 4 (VIRM). Give a set of distribution over the mapping Pφ and a distribution over the set
of classifier Pw, a variational invariant predictor on a set of environments E is said to satisfy the
13
Under review as a conference paper at ICLR 2021
Variational Invariant Risk Minimization (VIRM) if it is the solution of the following problem
min	Qeφ(qw, qφ)	(17a)
qφ∈Pφ
qw ∈Pw	e∈E
s.t.	qw ∈ arg min Qew(qw, qφ),∀e ∈ E	(17b)
qwe ∈Pw
where	Qφ(qw,qφ) = Ew〜qw Re(W ◦ Φ) + βDκL(qφ∣∣Pφ)+βDκL(qw∣∣Pw),	(17c)
φ~qφ
Qw(qw,qφ) = Ews Re(W ◦ Φ) + BDkl(qw||pw)	(I7d)
φ~qφ
andpφ,pw are the priors of the two distributions.
A.5 Bilevel Alternative Formulation
We state here a general result on solving Bilevel Optimization Problems
Lemma 5 (Bilevel Reformulation).
min	F(x, y x	)|G(x, y(x)) ≤ 0	(18a)
s.t.	y(x) ∈	arg min f (x, y)|g(x, y) ≤ 0 y	(18b)
then we can solve the equivalent problem		
min	F (x, y)|G(x, y(x)) ≤ 0,	(19a)
x,y,u		
	▽ y L(X,y,u) = 0,	(19b)
	u ≥ 0,	(19c)
	g(x, y) ≤ 0,	(19d)
	uT g(x, y) = 0	(19e)
L(x, y, u) =	f(x, y) + uT g(x, y)	(19f)
Proof of Lemma 5 . Lemma 5 follows by applying the Karush-Kuhn-Tucker conditions (Chapter 5
Boyd et al. (2004)) to Eq.18, where the Lagrangian function is L(x, y, U) = f (x, y)+uTg(x, y). □
Lemma 6 (Equivalence of Definition 1). Definition 1 is equivalent to Eq. 3, the Invariant Risk
Minimization.
ProofofLemma 6. The result follows by apply Lemma 5 to Eq.3.	□
Lemma 7 (Definition 2). Definition 2 is the extension of Eq. 8, the Bilevel Invariant Risk Minimiza-
tion, when the function is described by the distributions of their variable φ and W.
Proof of Lemma 7 . The result follows by inspecting Eq. 8. The equation requires the minimisation
of the aggregated loss function, which is now, from Eq.7:
Qφ(qw,qφ) = Ew~qw Re(W ◦ Φ) + βDκL(qφ∣∣Pφ) + BDkl(qw||pw),	(20)
φ~qφ
where we have separated the two contributions in φ and W, and used genetic prior distributions pφ
andpw. This is by the additive property of KL divergence:
DκL(qφqw∣∣PφPw) = DκL(qφ∣∣Pφ) + Dkl (qw ||pw),	(21)
since we model the two distributions independently, i.e. qφ,w = qφqw and qφ,w = pφpw . Since the
classifiers’ losses shall be minimal for all environments, this condition is substituted by requiring
the gradient with respect to qw to be zero, ∀e. The gradient w.r.t. qw of the second term of Eq.20 is
zero.	□
14
Under review as a conference paper at ICLR 2021
A.6 Theorem 3 and IRM connection
A.6.1 Sequential Information Projection
In Theorem.3, we show that the Information Projection (IP) shrinks the support of the output distri-
bution.
Lemma 8. If we have a sequence of families of distributions Pi. Let p1 ∈ P1 and
pi+1 = arg min DKL (p, pi)
p∈Pi+1
then
supp pi ⊆	supp(Pj )
j≤i
Proof of Lemma 8. We have that ∀i, supp pi ⊆ supp(Pi) T supp(Pi-1), where the first condition
follows from pi ∈ Pi in the minimization and the second from Theorem 3. The results follows by
iterating the property.	□
A.6.2 IRM and Information Projection
We show now two Ways to state the connection of the
IRM principle and the sequential IP. Let q- be the dis-
tribution of the previous environment and R(q) the loss
function of the current environment, where q denotes the
distribution of the network parameters. Let
q* = arg min R(q)
q
be the optimal distribution for the current environment.
We can then consider the Taylor expansion of the param-
eters distribution around the optimal distribution as
Figure 4: Sequential IRM projec-
tion of distributions, where q+ =
arg min Dkl(p∣∣9-) + Dkl(p∣∣9*)
R(q) = R(q*)+∆qT Vq R(q*)
we can compute the new distribution as
∆q*	= arg min DklS* +∆q∣∣q-)
∆q
s.t.	∆qTVqR(q*) ≤
and then
p+ = q* + ∆q*
Or alternatively
p*, q* = argminDKL(p||q-) + DKL(p||q)
p,q
s.t.	VqR(q) = 0
and then
q+ = p*
Or more simply
q+ = arg min DKL(p||q-) + DKL(p||q*)	(22)
p
This last equation, shows how the new distribution is the intersection of the optimal distribution
at the previous step q- and the current optimal distribution q*. Fig 8 shows visually, how the new
distribution is the result of projecting into two distributions q* and q-.
15
Under review as a conference paper at ICLR 2021
A.7 Out of distribution Generalization
The question arises if the property of generalization to out of distributions given by Theorem 9 in
Arjovsky et al. (2019) also holds for BIRM and BVIRM.
Lemma 9. If φ and w are linear functions and w ◦ φ = ΦTw is a solution of Eq.8 it then satisfies
ΦEXe XeXeT ΦTw = ΦEXe,Y e XeYeT
Proof. Lemma 9 follows from the fact that
VwRe(w ◦ φ) = ΦEχe [XeXeT] ΦTW - ΦEχe,Ye [xeY]	(23)
= 0	(24)
□
For BIRM thus Theorem 9 of Arjovsky et al. (2019) applies directly. A similar results holds for the
the BVIRM model
Lemma 10. If φ 〜pφ and W 〜pφ are linear functions and W ◦ φ = ΦT W is a solution of Eq.9,
with β = 0, it then satisfies
Eφ〜qφ {ΦEχe [XeXeT] ΦT } W = ΦEχe,Ye [XeY]
where Φ = Eφ〜？Φ[Φ] and W = Ew〜Pw [w] are the mean values.
Proof. Lemma 10 follows from the fact that
Vqw Qw (qw,qφ )∣β=0 = VqwEgqw Re(W ◦ φ)	(25)
φ~qφ
= 0	(26)
We now take the Frechet directional derivative in the η direction that is the limit of
δqw,η Ew-qw Re(W ◦ φ) = lim 1(Ew〜qw Re(((W + eη) ◦ φ) - Ew^w Re(w ◦ φ))
φ 〜qφ	e→ e φ 〜qφ	φ 〜qφ
which is obtained when we differentiate the distribution qw → qw + η. Since
δqw,η Egqw φ 〜qφ Re(W ◦ φ) = Egqw ¢y2^ ΦEχe [X eX eT] ΦT W - 2ηT ΦEχe,γe [X eY eT ]
we can factorize for the direction η and obtain
δqw Re (W ◦ φ) =2 Egqw ΦEχe [X eX eT ] ΦT W - ΦEχe,Y e [X eY eT ]
φ~qφ
We can now derive the Lemma by requiring δqw Re (w ◦ φ) = 0	□
Theorem 9 of Arjovsky et al. (2019) now holds when φ has rank r > 0 in expectation with respect
to the invariant distribution qφ, i.e. Eg” rank(Φ) = r.
A.8 Generalized ADMM
The following generalization of ADMM holds:
Lemma 11 (GADMM). Suppose we want to minimized
min	Xfi(x)|gi(x) = 0,∀i ∈ I	(27)
x
i
we can equivalently solve the following problem
min	fi(xi)|xi = z,gi(xi) = 0,∀i ∈ I	(28)
xi,z
i
using the following update role (scaled ADMM)
16
Under review as a conference paper at ICLR 2021
xi+ = argminLρ(xi, x--i, ui-, z-, vi-),∀i ∈ I	(29a)
xi
z+	=	1/N X(xi + ui)	(29b)
i
ui+	=	ui- + (xi+ - z+ )	(29c)
vi+	=	vi- + gi (xi+ )	(29d)
where the augmented Lagrangian
Lρ(xi, ui, z, vi)	=	X^ fi(xi)	+ 与 X^	Ilxi-Z + Uik2 + ^21 X^	Ilgi(Xi)	+	vik2	(3O)
ii	i
and x-i = {xj,j 6= i} is the set of all other variable, expect the i-th.
A.9 Continual Variational Inference
Following Nguyen et al. (2018) we can state the following lemma.
Lemma 12 (Variational Continual Learning). Suppose we have a sequence of datasets Di , i =
1, . . . , t drown i.i.d, then the variational estimation of the distribution qt at step t is given as projec-
tion on KL divergence
qt(θ) = arg min DKL (q(θ)∣∣ -1 qt-i(θ)p(Dt 屈
q(θ)	Zt
with Zt = J qt-ι(θ)p(Dt∣θ)dθ the normalization factor, which does not depends on q.
Proof of Lemma 12. Let denote D1t = Sit=1Di, from i.i.d. p(D1t ) = Qit=1p(Di). We are interested
to maximase the a posteriori probability of the Paramters give the data p(θ∣Dt)
P(θ∣Dt) = M P(θ∣Dt-1)p(Dt∣θ)
1	p(Dt )	1
since
p(θ,Dt) = p(θD1 )p(Dt)
=p(θ)p(Dt ∣θ)
t
=p(θ) Yp(Di∣θ)
i=1
=p(θ)p(Dt-1∣θ)p(Dt∣θ)
=p(θ,Dt-1)p(Dt∣θ)
=p(θD1-1)p(DtT)p(Dt∣θ)
thus
p(θ∣Dt) = PDy p(θ∣Dt-1)p(DtT)p(Dt∣θ)
=ppD-y p(θ∣DtT)p(Dt∣θ)
=p(D) p(θ∣Dt-1)p(Dt∣θ)
We now use a probability distribution which approximates the distribution at step t - 1
qt-i(θ) ≈ p(θ∣Dt-1)
when then want to approximate at time t
qt(θ) ≈ PDyqt-ι(θ)p(Dt∣θ)
17
Under review as a conference paper at ICLR 2021
This can be obtain by minimizing the KL divergence of the variational distribution qt and the distri-
bution induced by the previous step approximation, thus
qt(θ) = arg min DKL (q(θ)∣∣ -1 qt-ι(θ)p(Dt 屈
q(θ)	Zt
□
Lemma 13 (VCLv2). The minimization of the VCL defined in Lemma 12, is equivalent to solve the
following minimization
qt(θ) = arg max Eθ〜q(θ){log p(Dt∣θ)} - DκL(q(θ)∣∣qt-i(θ))
q(θ)
with Nt i.i.d. samples
1 Nt	t t
Eθ〜q(θ){lθgP(Dt∣θ)}=歹 EEθ〜q(θ){lθgp(y外θ,xt)}
Nt i=1
Where the second term can be computed in closed form for known distribution as for example with
the Gaussian distributions, whereas the expectation can be approximated by Monte Carlo sampling.
For a general loss function we can substitute the reconstruction probability with the loss function
associated with a neural network parametrized by θ
logp(y外θ,χt) — '(yt, (w ◦ φ)θ(Xt))
1 Nt
Eθ〜q(θ){logp(Dt∣θ)} 一 NE Eθ〜q(θ)'(yt,(w ◦ φ)θ(xt))}
Nt i=1
Proof of Lemma 13. The Lamma follows from the definition of the KL diveregnce
Dkl (q(θ)∣∣ Wqt-ι(θ)p(Dt∣θ))	= Eq(lnq(θ) - lnqt-ι(θ) - lnp(Dt∣θ)+lnZt)
=Eq(lnq(θ) - lnqt-i(θ)) - Eqlnp(Dt∣θ) + EqlnZt
=Dkl(q(θ)∣∣qt-ι(θ))- Eq lnp(Dt∣θ)+ln Zt
The last term does not depend on q. Thus the result follows.	□
If we substitute the log of the posterior probability with a specific loss function we obtain the fol-
lowing Corollary.
Corollary 14 (Continual Variational Bayesian Inference). Given a loss function '(y, y), the Varia-
tional continual learning is formulated as
qt (θ) = argmin E(x,y)〜DtEθ 〜q(θ){'(y,fθ (x))} + DκL(q(θ)∣∣qt-ι(θ)),	(31)
q(θ)
with fθ = (w ◦ φ)θ
A.10 Proofs
Proofof Theorem 3. Let first first recall that DκL(p∣∣q) = Rp(x)ln P(X)dx. If q(x) = 0 then
p(x) = 0 otherwise the distance is infinite. Second if p(x) = 0, then the contribution of q(x) is
not considered since the integral is taken of the support of p, thus, since the intersection is not null
and p is the result of an optimization, the support of p is the intersection of the support of q and the
support of P.	□
A.11 Datasets and color correction
We here visualize few of the dataset and color correlations. Figure 5 shows Fashion-MNIST and
the b11 color correlation. In the test environment the background color of each class is inverted.
In Figure 6 we show the dataset as generated from Ahuja et al. (2020). In Figure 7 we show the
EMINST (letter) and KMNIST dataset.
18
Under review as a conference paper at ICLR 2021
□ ΠB□T≡ΓKE
□[WM Γl∣ E≡ΠF?
Dfl LHIEl □rai H= Γ S
(a)
(b)
Figure 5: Fashion MNIST dataset training (a) and testing (b) environments; the color is inverted
based on the b11 color correlation scheme, where the background color depends on the class of the
image. In the test environment the dependency is inverted.
3 R M
7 I 3
N 2 4
O 6 Z
A 7 X匕
G(Z。6
/ a。1
夕(
(b)
(a)
Figure 6: MNIST dataset (a) and Fashion MNIST (b) environments as defined in Ahuja et al. (2020)
KV^YKS二乙
Z √ 5 <fr y( ⅛
WXfHJg 口3
(a)
Figure 7: Examples of the EMNIST dataset (a) and of the KMNIST (b).
19
Under review as a conference paper at ICLR 2021
A.12 Hyper-parameter Search and Experimental Setup
We performed hyper-parameter search around the suggested values from the original works and the
values selected based on the best performance on the test environment. To implement a complete
comparison we used for training 10000 samples randomly drawn from each environment. All meth-
ods were trained on the same data, using random seed reset. We trained all method with 100 epochs
on a batch size of 256.
•	IRM: γ = 91257, threshold = 1/2 epochs, learning rate 2.5e-4
•	IRMG: warm start=300, termination accuracy 0.6, learning rate 2.5e-4, dropout probabil-
ity 75%, weight decays = .00125
•	ERM: learning rate 1e-3, dropout probability 75%, weight decays = .00125
•	MER: memory size 100 (10% of the samples), learning rate 1e-3, replay batch size =5,
β = .03,γ= 1.0
•	GEM: memory size 100 (10% of the samples), learning rate 1e-3
•	EWC: memory size 100 (10% of the samples), learning rate 1e-3, regularization 0.1
•	VCL,VCLC: learning rate = 5e-3, corset size 100 (10% of the samples),
•	C-BVIRM, C-VIRMG, C-VIRMv1: weight decays = .00125, β = 1., number evaluations
5, ρ0 = ρ1 = 10, step threshold =1/2 epochs, δρ = 100, learning rate 1e-3
The neural network architecture is composed of 2 non-linear Exponential Linear Unit (ELU) acti-
vated Full Connected layers of size 100, followed by a linear full connected layer. Each layer with
dropout. Dropout is not present in VCL/VCLC since not implemented in the original work. Training
loss is the Cross Entropy. We tested also with the feature extraction layer separated, but with no
advantage, since the test set-up only consist of one task.
The IRMv1, IRMG and ERM methods, similarly to the other methods, are trained sequentially as
data from each new environment arrives. The Continual Learning methods are allowed to have a
limited memory of samples from previous environments.
A.13 Hyper-parameters of environment inference for continual invariant
LEARNING
We list below the values of hyper-parameters in EIIL for continual invariant learning:
2 layers,
390 hidden neurons,
501 epochs,
l2 regularizer weight: 0.00110794568,
learning rate: 0.0004898536566546834,
numbber of runs: 10,
number of EIIL iterations: 100000,
number Monte Carlo evaluations: 3,
penalty anneal iterations: 190,
penalty weight: 191257.18613115902,
prior weight: 1e-6
A.13.1 Synthetic Dataset
The Synthetic Dataset is described in (Arjovsky et al. (2019)) for testing IRM and it is defined by a
Structural Causal Model (Pearl (2009)), where a variable y ∈ RN is generated by x1 ∈ RN, while
x2 ∈ RN is generated by y. The observed variable is x = (x1 , x2). The structural equations are
xι	=	eι, eι 〜N(0,σ2)	(32)
y	=	xι + ey, ey 〜N(0, σ2	(33)
X1	=	y + e2, e2 〜N(0,1)	(34)
with σ1 fixed and σe dependent on the environment. We compared with ERM, IRM (Arjovsky et al.
(2019)), IPC (Invariant Prediction), which is the method proposed in Peters et al. (2015), and EIIL (
20
Under review as a conference paper at ICLR 2021
Table 5: Mean accuracy (over 5 runs) on Synthetic Dataset (Arjovsky et al. (2019),Creager et al.
(2020)). BIRM refers to our bilevel objective Eq. (8) optimized with ADMM.
	Causal MSE	Noncausal MSE
ERM	0.827 ± 0.016	0.824 ± 0.015
ICP	1.000 ± 0.000	0.756 ± 0.423
IRM	0.280 ± 0.006	0.290 ± 0.009
BIRM	0.183 ± 0.005	0.184 ± 0.002
EIIL(IRM)	0.180 ± 0.026	0.188 ± 0.033
Creager et al. (2020)). We use a similar set up of Creager et al. (2020), with N = 4. The invariant
model is given by w = (1, 0).
21