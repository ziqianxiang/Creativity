Under review as a conference paper at ICLR 2021
Multi-Head Attention:
Collaborate Instead of Concatenate
Anonymous authors
Paper under double-blind review
Ab stract
Attention layers are widely used in natural language processing (NLP) and are be-
ginning to influence computer vision architectures. Training very large transformer
models allowed significan improvement in both fields, but once trained, these
networks show symptoms of over-parameterization. For instance, it is known that
many attention heads can be pruned without impacting accuracy. This work aims
to enhance current understanding on how multiple heads interact. Motivated by
the observation that trained attention heads share common key/query projections,
we propose a collaborative multi-head attention layer that enables heads to learn
shared projections. Our scheme decreases the number of parameters in an attention
layer and can be used as a drop-in replacement in any transformer architecture. For
instance, by allowing heads to collaborate on a neural machine translation task,
we can reduce the key dimension by 4× without any loss in performance. We
also show that it is possible to re-parametrize a pre-trained multi-head attention
layer into our collaborative attention layer. Even without retraining, collaborative
multi-head attention manages to reduce the size of the key and query projections
by half without sacrificing accuracy. Our code is public.1
1	Introduction
Since the invention of attention (Bahdanau et al., 2014) and its popularization in the transformer
architecture (Vaswani et al., 2017), multi-head attention (MHA) has become the de facto architecture
for natural language understanding tasks (Devlin et al., 2019) and neural machine translation. Atten-
tion mechanisms have also gained traction in computer vision following the work of Ramachandran
et al. (2019) and Bello et al. (2019). Nevertheless, despite their wide adoption, we currently lack
solid theoretical understanding of how transformers operate. In fact, many of their modules and
hyperparameters are derived from empirical evidences that are possibly circumstantial.
The uncertainty is amplified in multi-head attention, where both the roles and interactions between
heads are still poorly understood. Empirically, it is well known that using multiple heads can improve
model accuracy. However, not all heads are equally informative, and it has been shown that certain
heads can be pruned without impacting model performance. For instance, Voita et al. (2019) present
a method to quantify head utility and prune redundant members. Michel et al. (2019) go further to
question the utility of multiple heads by testing the effect of heavy pruning in several settings. On
the other hand, Cordonnier et al. (2020) prove that multiple heads are needed for self-attention to
perform convolution, specifically requiring one head per pixel in the filter’s receptive field. Beyond
the number of heads, finding the adequate head dimension is also an open question. Bhojanapalli
et al. (2020) finds that the division of the key/query projection between heads gives rise to a low-rank
bottleneck for each attention head expressivity that can be fixed by increasing the head sizes. In
contrast, our approach increases heads expressivity by leveraging the low-rankness accross heads to
share common query/key dimensions.
This work aims to better detect and quantify head redundancy by asking whether independent heads
learn overlapping or distinct concepts. This relates to the work on CNN compression that factorizes
common filters in a trained convolutional network (Kim et al., 2016) using Tucker decomposition. In
attention models, we discover that some key/query projected dimensions are redundant, as trained
1https://github.com/...
1
Under review as a conference paper at ICLR 2021
concatenated heads tend to compute their attention patterns on common features. Our finding implies
that MHA can be re-parametrized with better weight sharing for these common projections and a
lower number of parameters. This differs from concurrent work (Shazeer et al., 2020) that orchestrate
collaboration between heads on top of the dot product attention scores.
Contribution 1: Introducing the collaborative multi-head attention layer. Section 3 describes a
collaborative attention layer that allows heads to learn shared key and query features. The proposed
re-parametrization significantly decreases the number of parameters of the attention layer without
sacrificing performance. Our Neural Machine Translation experiments in Section 4 show that the
number of FLOPS and parameters to compute the attention scores can be divided by 4 without
affecting the BLEU score on the WMT14 English-to-German task.
Contribution 2: Re-parametrizing pre-trained models into a collaborative form renders them more
efficient. Pre-training large language models has been central to the latest NLP developments. But
pre-training transformers from scratch remains daunting for its computational cost even when using
more efficient training tasks such as (Clark et al., 2020). Interestingly, our changes to the MHA layers
can be applied post-hoc on pre-trained transformers, as a drop-in replacement of classic attention
layers. To achieve this, we compute the weights of the re-parametrized layer using canonical tensor
decomposition of the query and key matrices in the original layer. Our experiments in Section 4 show
that the key/query dimensions can be divided by 3 without any degradation in performance.
As a side contribution, we identify a discrepancy between the theory and some implementations
of attention layers and show that by correctly modeling the biases of key and query layers, we can
clearly differentiate between context and content-based attention.
2	Multi-Head Attention
We first review standard multi-head attention introduced by Vaswani et al. (2017).
2.1	Attention
Let X ∈ RT ×Din and Y ∈ RT ×Din be two input matrices consisting of respectively T and T 0
tokens of Din dimensions each. An attention layer maps each of the T query token from Din to Dout
dimensions as follows:
Attention(Q, K, V) = SoftmaX QQK>) V, With Q = XWQ, K = YWk, V = YWv (1)
dk
The layer is parametrized by a query matrix WQ ∈ RDin ×Dk , a key matrix WK ∈ RDin ×Dk and a
value matrix Wv ∈ RDin ×Dout. Using attention on the same sequence (i.e. X = Y) is knoWn as
self-attention and is the basic building block of the transformer architecture.
2.2	Content vs. Context
Some re-implementations of the original transformer architecture2 use biases in the linear layers. This
differs from the attention operator defined in eq. (1) Where the biases bQ and bK ∈ RDk are ommited.
Key and query projections are computed as K = XWK + 1T×1bK and Q = YWQ + 1T×1bQ,
respectively, Where 1a×b is an all one matrix of dimension a × b. The exact computation of the
(unscaled) attention scores can be decomposed as folloWs:
QK>= (XWQ+1T×1b>Q)(YWK+1T×1b>K)>	(2)
= XWQWK>Y>+1T×1b>QWK>Y>+XWQbK11×T +1T×Tb>QbK	(3)
X{z}	{}
context	content
As the last tWo terms of eq. (3) have a constant contribution over all entries of the same roW, they do not
contribute to the computed attention probabilities (softmax is shift invariant and softmaX(x + c) =
softmaX(x), ∀c). On the other hand, the first tWo terms have a clear meaning: X WQ WK>Y>
2For instance: the BERT orignal implementation, its HuggingFace re-implementation and FairSeq encoder-
decoder transformer.
2
Under review as a conference paper at ICLR 2021
5
-
O
9uBA p9,mqdB0
0.0
1	8	16
PCA components (heads separately)
9uαB'aBA pə,maʤ
0. 5 -
W I I I	- Wk
..…WQ
0. 0 -1--1-----1------------1------
1 64 128	256	512
PCA components (heads concatenated)
Figure 1: Cumulative captured variance of the key query matrices per head separately (left) and per
layer with concatenated heads (right). Matrices are taken from a pre-trained BERT-base model with
Nh = 12 heads of dimension dk = 64. Bold lines show the means. Even though, by themselves,
heads are not low rank (left), the product of their concatenation WQ WK> is low rank (right, in red).
Hence, the heads are sharing common projections in their column-space.
considers the relation between keys and query pairs, whereas 1T×1bQ>WK>Y > computes attention
solely based on key content.
The above findings suggest that the bias bK of the key layer can be always be disabled without any
consequence. Moreover, the query biases bQ play an additional role: they allow for attention scores
that are content-based, rather than solely depending on key-query interactions. This could provide
an explanation for the recent success of the Dense-Synthesizer (Tay et al., 2020), a method that
ignores context and computes attention scores solely as a function of individual tokens. That is,
perhaps context is not always crucial for attention scores, and content can suffice.
2.3 Multi-Head Attention
Traditionally, the attention mechanism is replicated by concatenation to obtain multi-head attention
defined for Nh heads as:
MultiHead(X, Y) = concat H(i) WO	(4)
i∈[Nh]
H(i) = Attention(XWQ(i),YWK(i),YWV(i)),	(5)
where distinct parameter matrices WQ(i), WK(i) ∈ RDin×dk and WV(i) ∈ RDin×dout are learned for each
head i ∈ [Nh] and the extra parameter matrix WO ∈ RNhdout×Dout projects the concatenation of the
Nh head outputs (each in Rdout) to the output space RDout . In the multi-head setting, we call dk the
dimension of each head and Dk = Nhdk the total dimension of the query/key space.
3	Improving the Multi-Head Mechanism
Head concatenation is a simple and remarkably practical setup that gives empirical improvements.
However, we show that another path could have been taken instead of concatenation. As the multiple
heads are inherently solving similar tasks, they can collaborate instead of being independent.
3.1	How much do heads have in common?
We hypothesize that some heads might attend on similar features in the input space, for example
computing high attention on the verb of a sentence or extracting some dimensions of the positional
encoding. To verify this hypothesis, it does not suffice to look at the similarity between query (or key)
matrices {WQ(i)}i∈[Nh] of different heads. To illustrate this issue, consider the case where two heads
are computing the same key/query representations up to a unitary matrix R ∈ Rdk ×dk such that
WQ(2) = WQ(1)R and WK(2) = WK(1)R.
3
Under review as a conference paper at ICLR 2021
Even though the two heads are computing identical attention scores, i.e. WQ(1)RR>WK(1)> =
WQ(1)WK(1)>, they can have orthogonal column-spaces and the concatenation [WQ(1) , WQ(2)] ∈
RDin ×2dk can be full rank.
To disregard artificial differences due to common rotations or scaling of the key/query spaces, we
study the similarity of the product WQ(i)WK(i)> ∈ RDin×Din across heads. Figure 1 shows the captured
energy by the principal components of the key, query matrices and their product. It can be seen on
(i)	(i)>
the left that single head key/query matrices WQ WK are not low rank on average. However, as
seen on the right, even if parameter matrices taken separately are not low rank, their concatenation is
indeed low rank. This means that heads, though acting independently, learn to focus on the same
subspaces. The phenomenon is quite pronounced: one third of the dimensions suffices to capture
almost all the energy of WQ WK>, which suggests that there is inefficiency in the way multi-head
attention currently operate.
3.2	Collaborative Multi-Head Attention
Following the observation that heads’ key/query projections learn redundant projections, we propose
to learn key/query projections for all heads at once and to let each head use a re-weighting of these
projections. Our collaborative head attention is defined as follows:
CollabHead(X,Y) = concat H(i) WO	(6)
i∈[Nh]
H⑴=Attention(XWQ diag(mi), YWK, YWVi)).	(7)
The main difference with standard multi-head attention defined in eq. (5) is that we do not duplicate
the key and query matrices for each head. Instead, each head learns a mixing vector mi ∈ RDk that
defines a custom dot product over the Dk projected dimensions of the shared matrices WQ and WK
of dimension Dn X Dk. This approach leads to:
(i)	adaptive head expressiveness, with heads being able to use more or fewer dimensions according
to attention pattern complexity;
(ii)	parameter efficient representation, as learned projections are shared between heads, hence stored
and learned only once.
It is instructive to observe how standard multi-head attention (where heads are simply concatenated)
can be seen as a special case of our collaborative framework (with D k = Nhdk). The left of Figure 2
displays the standard attention computed between xn and ym input vectors with the mixing matrix
M := concat mi ∈ RNh×Dk ,	(8)
i∈[Nh]
laying out the mixing vectors mi as rows. In the concatenated MHA, the mixing vector mi for the
i-th head is a vector with ones aligned with the dk dimensions allocated to the i-th head among the
Dk = Nhdk total dimensions.
Some alternative collaborative schema can be seen on the right side of Figure 2. By learning the
mixing vectors {mi}i∈[Nh] instead of fixing them to this “blocks-of-1” structure, we increase the
expressive power of each head for a negligible increase in the number of parameters. The size dk of
each head, arbitrarily set to 64 in most implementations, is now adaptive and the heads can attend to
a smaller or bigger subspace if needed.
3.3	Head Collaboration as Tensor Decomposition
As we show next, there is a simple way to convert any standard attention layer to collaborative
attention without retraining. To this end, we must extract the common dimensions between query/key
matrices {WQ(i)WK(i)> ∈ RDin×Din}i∈[Nh] across the different heads. This can be solved using the
Tucker tensor decomposition (Tucker, 1966) of the 3rd-order tensor
WQK := stack hWQ(i)WK(i)>i ∈ RNh×Din×Din .	(9)
4
Under review as a conference paper at ICLR 2021
Nh
^in
⅞∕m
Θ
111111
111111
111111
=
attention
scores
哦）哦）哦）
Dk = N}ldk
Other possible mixing matrices M
111111
(a)	111111
111111
(b)	1 1 1 111111 . IargeWead
11111111
(C)
(d)
五 '
X
æn
M
×
×
Figure 2: Left: computation of the attention scores between tokens xn and ym using a standard
concatenated multi-head attention with Nh = 3 independent heads. The block structure of the mixing
matrix M enforces that each head dot products non overlapping dimensions. Right: we propose to
use more general mixing matrices M than (a) heads concatenation, such as (b) allowing heads to
have different sizes; (c) sharing heads projections by learning the full matrix; (d) compressing the
number of projections from Dk to Dk as heads can share redundant projections.
Following the notation3 of Kolda & Bader (2009), the Tucker decomposition of a tensor T ∈ RI ×J ×K
is written as
PQR
T≈G×1A×2B×3C=ΣΣΣgpqr ap ◦ bq ◦ cr =: qG; A, B , Cy,	(10)
p=1 q=1 r=1
with A ∈ RI×P, B ∈ RJ×Q, and C ∈ RK×R being factor matrices, whereas G ∈ RP ×Q×R is the
core tensor. Intuitively, the core entry gpqr = Gp,q,r quantifies the level of interaction between the
components ap , bq, and cr .
In the case of attention, it suffices to consider the dot product of the aligned key/query components
of the Q and K matrices, which means that the core tensor is super-diagonal (i.e. gpqr 6= 0 only
if q = r). We further simplify the Tucker decomposition by setting the factors dimensions P, Q
and R to Dk, a single interpretable hyperparameter equal to the dimension of the shared key/query
space that controls the amount of compression of the decomposition into collaborative heads. These
changes lead to a special case of Tucker decomposition called the canonical decomposition, also
known as CP or PARAFAC (Harshman, 1970) in the literature (Kolda & Bader, 2009). Fix any
positive rank R. The decomposition yields:
R
T ≈ X ar。br ◦ Cr =: qA, B, Cy ,	(11)
r=1
with A ∈ RI×R, B ∈ RJ×R andC ∈ RK×R.
What is remarkable is that the above can be used to express any (trained) attention layer parametrized
by{WQ(i),b(Qi),WK(i),b(Ki)}
i∈[Nh] as a collaborative layer. In particular, ifwe apply the decomposition
,	,1	,	1	F 1 Hf	1 .	∙	.1	.1	. ∙	ΓΓ K Λ" ɪɪ T ɪɪ T Tl , 1	, FC	11 1	, ∙
to the stacked heads WQK we obtain the three matrices JM , WQ , WKK that define a collaborative
Γ,
attention layer: the mixing matrix M ∈ RNh ×Dk, as well as the key and query projection matrices
WQ, WK ∈ RD i ×D k .
3 ◦ represents the vector outer product
5
Under review as a conference paper at ICLR 2021
On the other hand, biases can be easily dealt with based on the content/context decomposition of
eq. (3), by storing for each head the vector
vi = WK(i)b(Qi) ∈ RDin.
(12)
With this in place, the computation of the (unscaled) attention score for the i-th head is given by:
(XWQi) + 1t×ιbQ)(YWi) + 1t×ιbK)> ≈ XWQ diag(mi)W>Y> + 1丁×ιv>Y>, (13)
where mi is the i-th row of M. If Dk ≥ Dk the decomposition is exact (eq. (11) is an equality)
and our collaborative heads layer can express any concatenation-based attention layer. We also
note that the proposed re-parametrization can be applied to the attention layers of many transformer
architectures, such as the ones proposed by Devlin et al. (2019); Sanh et al. (2019); Lan et al. (2020).
3.4	Parameter and Computation Efficiency Table 1: Comparison ofa layer of con-
Collaborative MHA introduces weight sharing across the
key/query projections and decreases the number of parame-
catenate vs. collaborative MHA with
chosen Dk to give negligible Perfor-
mance difference. T=128.
ters and FLOPS. While the size of the heads in the standard
attention layer is set to dk = 64 and the key/query layers
Project into a sPace of dimension Dk = Nhdk, the shared
key/query dimension Dk of collaborative MHA can be set
freely. According to our exPeriments in Section 4 (summa-
rized in Table 1), a good rule of thumb when transforming
a trained MHA layer to collaborative is to set Dk to half
~
	train FairSeq §4.1		re-param. HuggingFace §4.2	
	concat. collab.		concat. collab.	
Dk → Dk	512 → 128		768 → 256	
Params (×106)	1.05	0.66	2.36	1.58
FLOPS (×108)	1.51	1.09	3.27	2.65
inference (ms)	0.99	0.81	1.71	1.65
or one third of Dk. When training from scratch, Dk can even be set to 1/4-th of Dk.
Parameters. Collaborative heads use (2Din + Nh)Dk parameters, as compared to 2D加Dk in
the standard case (ignoring biases). Hence, the compression ratio is ≈ Dk/Dk, controlled by the
shared key dimension Dk. The collaborative factorization introduces a new matrix M of dimension
Nh × Dk. Nevertheless, as the number of heads is small compared to the hidden dimension (in
BERT-base Nh = 12 whereas Din = 768), the extra parameter matrix yields a negligible increase as
compared to the size of the query/key/values matrices of dimension Din × Dk.
Computational cost. Our layer decomposes two matrices into three, of modulable dimensions. To
compute the attention scores between T tokens for all the Nh heads, collaborative MHA requires
2T(Din + Nh)Dk + T2NhDk FLOPS, while the concatenation-based MHA uses 2TDnDk + T2Dk
FLOPS. Assuming that Din Nh = O(1) (as is common in most implementations), we obtain a
theoretical speedup of Θ(Dk/Dk). However in practice, having two matrix multiplications instead
of a larger one makes our implementation slightly slower, if larger multiplications are supported by
the hardware.
4	Experiments
The goal of our experimental section is two-fold. First, we show that concatenation-based MHA is
a drop-in replacement for collaborative MHA in transformer architectures. We obtain a significant
reduction in the number of parameters and number of FLOPS without sacrificing performance on a
Neural Machine Translation (NMT) task with an encoder-decoder transformer. Secondly, we verify
that our tensor decomposition allows one to reparametrize pre-trained transformers, such as BERT
(Devlin et al., 2019) and its variants. To this end, we show that collaborative MHA performs on par
with its concatenation-based counter-part on the GLUE benchmark (Wang et al., 2018) for Natural
Language Understanding (NLU) tasks, even without retraining.
The NMT experiments are based on the FairSeq (Ott et al., 2019) implementation of transformer-base
by Vaswani et al. (2017). For the NLU experiments, we implemented the collaborative MHA layer as
an extension of the Transformers library (Wolf et al., 2019). The flexibility of our layer allows it to
be applied to most of the existing transformer architectures, either at pre-training or after fine-tuning
6
Under review as a conference paper at ICLR 2021
Dk	BLEU		params (×106)		time (h)	
	concat.	collab.	concat.	collab.	concat.	collab.
512	27.40	27.58	60.9	61.0	18.0	21.0
256	27.10	27.41	56.2	56.2	17.3	19.0
128	26.89	27.40	53.8	53.8	17.3	18.4
64	26.77	27.31	52.6	52.7	16.9	17.9
27.6
27.4
w 27.2
m
27.0
26.8
512
-∣∣128	∣4-256 I I x 512
+64	,
χl28
×256
+ collab.
× concat.
54	56	58	60
Number of parameters ×106
Training time on 4xV100 (hours)
Figure 3: Comparison of the BLEU score on WMT14 EN-DE translation task for an encoder-decoder
transformer-base (Vaswani et al., 2017) using collaborate vs. concatenate heads with key/query
dimension Dk . We visualize performence as a function of number of parameters (middle) and
training time (right). Collaborative attention consistently improves BLEU score, Dk can be decreased
by a factor of 4 without drop in performance.
using tensor decomposition. We use the tensor decomposition library Tensorly (Kossaifi et al., 2019)
with the PyTorch backend (Paszke et al., 2017) to reparameterize pre-trained attention layers. Our
code and datasets are publicly available4 and all hyperparameters are specified in the Appendix.
4.1	Collaborative MHA for Neural Machine Translation
We replace the concatenation-based MHA layers of an encoder-decoder transformer by our collabora-
tive MHA and evaluate it on the WMT14 English-to-German translation task. Following (Vaswani
et al., 2017), we train on the WMT16 train corpus, apply checkpoint averaging and report compound
split tokenized BLEU. We use the same hyperparameters as the baseline for all our runs. Results
are shown in Figure 3. Our run of the original base transformer with Nh = 8 heads and Dk = 512
key/query total dimensions achieves 27.40 BLUE (instead of 27.30).
As observed in the original paper by Vaswani et al. (2017), decreasing the key/query head size
dk degrades the performance (× in Figure 3). However, with collaborative heads (+ in Figure 3),
the shared key/query dimension can be reduced by 4× without decreasing the BLEU score. As
feed-forward layers and embeddings are left untouched, this translates to a 10% decrease in number
of parameters for a slight increase in training time. When setting a total key/query dimension of
Dk = 64, corresponding to dk = 8 dimensions per head, the classic MHA model suffers a drop of
0.6 BLEU points, meanwhile the collaborative MHA stays within 0.1 point of the transformer-base
model using concatenation.
We conclude that sharing key/query projections across heads allows attention features to be learned
and stored only once. This weight sharing enables decreasing Dk without sacrificing expressiveness.
4.2	Re-parametrize a Pre-trained MHA into Collaborative MHA
We turn to experiments on Natural Language Understanding
(NLU) tasks, where transformers have been decisive in im-
proving the state-of-the-art. As pre-training on large text cor-
pora remains an expensive task, we leverage the post-hoc re-
parametrization introduced in Section 3.3 to cast already pre-
trained models into their collaborative form. We proceed in
3 steps for each GLUE task (Wang et al., 2018). First, we
take a pre-trained transformer and fine-tune it on each task
individually. Secondly, we replace all the attention layers by
our collaborative MHA using tensor decomposition to com-
pute WQ , WK and M and re-parametrize the biases into v.
This step only takes a few minutes as shown in Figure 4. Fi-
nally, we fine-tune the compressed model again and evaluate
its performance.
Dk
Figure 4: Time to decompose
_____- _ _____________ ~
BERT-base from Dk = 768 to Dk.
We experiment with a pre-trained BERT-base model (Devlin et al., 2019). We also repurpose two
variants of BERT designed to be more parameter efficient: ALBERT (Lan et al., 2020), an improved
4https://github.com/...
7
Under review as a conference paper at ICLR 2021
Table 2: Performance of collaborative MHA on the GLUE benchmark (Wang et al., 2018). We
report the median of 3 runs for BERT (Devlin et al., 2019), DistilBERT (Sanh et al., 2019) and
ALBERT (Lan et al., 2020) with collaborative heads and different compression controlled by Dk.
Comparing the original models (Dk = 768) with their compressed counter part shows that the number
of parameters can be decreased with less than 1.5% performance drop (gray rows).
Model	D k	params	CoLA	SST-2	MRPC	STS-B	QQP	MNLI	QNLI	RTE	Avg.
BERT-base	-	108.3M	54.7	91.7	88.8/83.8	88.8/88.7	87.6/90.8	84.1	90.9	63.2	83.0
	768	108.5M	56.8	90.1	89.6/85.1	89.2/88.9	86.8/90.2	83.4	90.2	65.3	83.2
	384	101.4M	56.3	90.7	87.7/82.4	88.3/88.0	86.3/90.0	83.0	90.1	65.3	82.5
	256	99.0M	52.6	90.1	88.1/82.6	87.5/87.2	85.9/89.6	82.7	89.5	62.5	81.7
	128	96.6M	43.5	89.5	83.4/75.2	84.5/84.3	81.1/85.8	79.4	86.7	60.7	77.6
DistilBERT	-	66.4M	466	898	87.0/82.1	84.0/83.7	86.2/89.8	819	88.1	60.3	80.0
	384	62.9M	45.6	89.2	86.6/80.9	81.7/81.9	86.1/89.6	81.1	87.0	60.7	79.1
ALBERT^^	-	11.7M	58.3	907	90.8/87.5	91.2/90.8	87.5/90.7	852	917	737	85.3
	512	11.3M	51.1	86.0	91.4/88.0	88.6/88.2	87.2/90.4	84.2	90.2	69.0	83.1
	384	11.1M	40.7	89.6	82.3/71.1	86.0/85.6	87.2/90.5	84.4	90.0	49.5	77.9
transformer with a single layer unrolled, and DistilBERT (Sanh et al., 2019) a smaller version of
BERT trained with distillation. We report in Table 2 the median performance of 3 independent runs
of the models on the GLUE benchmark (Wang et al., 2018).
We first verify that tensor decomposition without compression (Dk = Dk = 768) does not alter
performance. As shown in Table 2, both BERT-base and its decomposition performs similarly with an
average score of 83.0% and 83.2% respectively. We should clarify that, for consistency, we opted to
re-finetune the model in all cases (even when Dk = Dk), and that the slight score variation disappears
without re-finetuning. Nevertheless, even with re-finetuning, reparametrizing the attention layers into
collaborative form is beneficial in 4 out of the 8 tasks, as well as in terms of the average score.
We then experiment with compressed decomposition using a smaller Dk. Comparing the original
models with their well-performing compressed counterpart (gray rows) shows that the key/query
dimension of BERT and DistilBERT can be reduced by 2× and 3× respectively without sacrificing
more than 1.5% of performance. This is especially remarkable given that DistilBERT was designed
to be a parameter-efficient version of BERT. It seems that ALBERT suffers more from compression,
but the dimension can be reduced by a factor 1.5× with minor performance degradation. We suspect
that unrolling the same attention layer over the depth of the transformer forces the heads to use
different projections and decreases their overlap, decreasing the opportunity for weight-sharing. Our
hypothesis is that better performance may be obtained by pre-training the whole BERT architecture
variants from scratch.
64 128	256	384	512	640	768
Dk
16 64 128	256	384	512	640	768
Dk
16 64 128	256	384	512	640	768
Dk
Figure 5: Performance on MNLL MRPC and STS-B datasets of a …fine-tuned BERT-base model,
—decomposed with collaborative heads of compressed dimension Dk (horizontal axis). — Repeating
fine-tuning after compression can make the model recover the original performance when compression
was drastic. The GLUE baseline gives a reference for catastrophic failure.
Recovering from compression with fine-tuning. We further investigate the necessity of the sec-
ond fine-tuning—step 3 of our experimental protocol—after the model compression. Figure 5 shows
the performance of BERT-base on 3 GLUE tasks for different compression parameters Dk with
8
Under review as a conference paper at ICLR 2021
and without the second fine-tuning. We find that for compression up to 1.5× (from Dk = 768 to
Dk = 512), the re-parametrization is accurate and performance is maintained without fine-tuning
again. Further compressing the model starts to affect performance. Nevertheless, for compression by
up to 3× (to Dk = 256), this loss can readily be recovered by a second fine-tuning (in orange).
5	Conclusion
This work showed that trained concatenated heads in multi-head attention models can extract redun-
dant query/key representations. To mitigate this issue, we propose to replace concatenation-based
MHA by collaborative MHA. When our layer is used as a replacement for standard MHA in en-
coder/decoder transformers for Neural Machine Translation, it enables the decrease of effective
individual head size from dk = 64 to 8 without impacting performance. Further, without pre-training
from scratch, switching a MHA layer to collaborative halves the number of FLOPS and parameters
needed to compute the attentions score affecting the GLUE score by less than 1.5%.
Our model can impact every transformer architecture and our code (publicly available) provides
post-hoc compression of already trained networks. We believe that using collaborative MHA in
models pre-trained from scratch could force heads to extract meaningful shared query/key features.
We are curious if this would translate to faster pre-training, better performance on downstream tasks
and improved interpretability of the attention mechanism.
9
Under review as a conference paper at ICLR 2021
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate, 2014. URL http://arxiv.org/abs/1409.0473.
Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention augmented
convolutional networks. In The IEEE International Conference on Computer Vision (ICCV),
October 2019.
Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar.
Low-rank bottleneck in multi-head attention models, 2020.
Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.wandb.
com/. Software available from wandb.com.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations, 2020. URL https://openreview.net/forum?id=r1xMH1BtvB.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-
attention and convolutional layers. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=HJlnC1rKPB.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of
deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and
Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT
2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171-
4186. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1423. URL
https://doi.org/10.18653/v1/n19-1423.
Richard A. Harshman. Foundations of the PARAFAC procedure: Models and conditions for an
"explanatory" multi-modal factor analysis. UCLA Working Papers in Phonetics, 16:1-84, 1970.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Compres-
sion of deep convolutional neural networks for fast and low power mobile applications, 2016.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Review, 51
(3):455-500, 2009. ISSN 00361445. doi: 10.1137/07070111X. URL http://dx.doi.org/10.
1137/07070111X.
Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. Tensorly: Tensor learning in
python. Journal of Machine Learning Research, 20(26):1-6, 2019. URL http://jmlr.org/
papers/v20/18-277.html.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
Albert: A lite bert for self-supervised learning of language representations. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=
H1eA7AEtvS.
Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than
one? In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6 Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
14014-14024. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
9551- are- sixteen- heads- really- better- than- one.pdf.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
10
Under review as a conference paper at ICLR 2021
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.
Stand-alone self-attention in vision models. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Informa-
tion Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada ,pp. 68-80, 2019. URL http：//
papers.nips.cc/paper/8302- stand- alone- self- attention- in- vision- models.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://arxiv.
org/abs/1910.01108.
Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, and Le Hou. Talking-heads attention,
2020.
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:
Rethinking self-attention in transformer models, 2020.
Ledyard Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279-
311, 1966. URL https://EconPapers.repec.org/RePEc:spr:psycho:v:31:y:1966:i:
3:p:279-311.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg,
Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.),
Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL
http://papers.nips.cc/paper/7181- attention- is- all- you- need.
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head
self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics, pp. 5797-5808, Florence,
Italy, July 2019. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/P19-1580.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A multi-task benchmark and analysis platform for natural language understanding. In Proceedings
of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for
NLP, pp. 353-355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/W18-5446.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s
transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
11
Under review as a conference paper at ICLR 2021
Supplementary Material
A Hyperparameters for Neural Machine Translation Experiments
Our implementation is based on Fairseq implementation Ott et al. (2019). We report in the following
tables the specification of the architecture. We used the default hyperparameters if they are not
specified below.
Transformer architecture parameters
dataset	wmt16_en_de_bpe32k
architecture	transformer_wmt_en_de
layers	6
heads	8
hidden-dim	512
collaborative-heads	"encoder_cross_decoder" or "none"
key-dim	64, 128, 256, 512
share-all-embeddings	True
optimizer	adam
adam-betas	(0.9, 0.98)
clip-norm	0.0
lr	0.0007
min-lr	1e-09
lr-scheduler	inverse_sqrt
warmup-updates	4000
warmup-init-lr	1e-07
dropout	0.1
weight-decay	0.0
criterion	label_smoothed_cross_entropy
label-smoothing	0.1
max-tokens	3584
update-freq	2
fp16	True
Table 3: Hyperparameters for the NMT experiment.
B Hyperparameters for Natural Language Understanding
Experiments
We use standard models downloadable from HuggingFace repository along with their configuration.
Models
BERT-base	Devlin et al. (2019)	bert-base-cased
DistilBERT	Sanh et al. (2019)	distilbert-base-cased
ALBERT	Lan et al. (2020)	albert-base-v2
We use HuggingFace default hyperparameters for GLUE fine-tuning in all our runs. We train with
a learning rate of 2 ∙ 10-5 for 3 epochs for all datasets except sSt-2 and RTE where We train
for 10 epochs. In preliminary experiments, we tried to tune the tensor decomposition tolerance
hyperparameter among {10-6, 10-7, 10-8} but did not see significant improvement and kept the
default 10-6 for all our experiments.
12
Under review as a conference paper at ICLR 2021
GLUE fine-tuning hyperparameters	
Number of epochs	3 for all tasks but 10 for SST-2 and RTE
Batch size	32
Learning rate	2e-5
Adam	1e-8
Max gradient norm	1
Weight decay	0
Decomposition tolerance	1e-6
13