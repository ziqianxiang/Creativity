Under review as a conference paper at ICLR 2021
Policy Optimization in Zero-Sum Markov
Games: Fictitious Self-Play Provably Attains
Nash Equilibria
Anonymous authors
Paper under double-blind review
Ab stract
Fictitious Self-Play (FSP) has achieved significant empirical success in solving
extensive-form games. However, from a theoretical perspective, it remains un-
known whether FSP is guaranteed to converge to Nash equilibria in Markov games.
As an initial attempt, we propose an FSP algorithm for two-player zero-sum Markov
games, dubbed as smooth FSP, where both agents adopt an entropy-regularized pol-
icy optimization method against each other. Smooth FSP builds upon a connection
between smooth fictitious play and the policy optimization framework. Specifically,
in each iteration, each player infers the policy of the opponent implicitly via policy
evaluation and improves its current policy by taking the smoothed best-response via
a proximal policy optimization (PPO) step. Moreover, to tame the non-stationarity
caused by the opponent, we propose to incorporate entropy regularization in PPO
for algorithmic stability. When both players adopt smooth FSP simultaneously, i.e.,
with self-play, in a class of games with Lipschitz continuous transition and reward,
we prove that the sequence of joint policies converges to a neighborhood of a Nash
equilibrium at a sublinear O(1/T) rate, where T is the number of iterations. To
our best knowledge, we establish the first finite-time convergence guarantee for
FSP-type algorithms in zero-sum Markov games.
1 Introduction
Multi-agent reinforcement learning (MARL) (Bu et al., 2008; Sutton & Barto, 2018) has achieved
great empirical success, e.g., in playing the game ofGo (Silver et al., 2016; 2017), Dota 2 (Berner et al.,
2019), and StarCraft 2 (Vinyals et al., 2019), which are all driven by policy optimization algorithms
which iteratively update the policies that are parameterized using deep neural networks. Empirically,
the popularity of policy optimization algorithms for MARL is attributed to the observations that they
usually converges faster than value-based methods that iteratively update the value functions (Mnih
et al., 2016; O’Donoghue et al., 2016).
Compared with their empirical success, the theoretical aspect of policy optimization algorithms in
MARL setting (Littman, 1994; HU & Wellman, 2003; Conitzer & Sandholm, 2007; Perolat et al.,
2016; Zhang et al., 2018) remains less understood. Although convergence guarantees for various
policy optimization algorithms have been established Under the single-agent RL setting (SUtton et al.,
2000; Konda & Tsitsiklis, 2000; Kakade, 2002; Agarwal et al., 2019; Wang et al., 2019), extending
those theoretical gUarantees to argUably one of the simplest settings of MARL, two-player zero-sUm
Markov game, sUffers from challenges in the following two aspects. First, in sUch a Markov game,
each agent interact with the opponent as well as the environment. Seen from the perspective of each
agent, it belongs to an environment that is altered by the actions of the opponent. As a resUlt, dUe
to the existence of an opponent, the policy optimization problem of each agent has a time-varying
objective fUnction, which is in stark contrast with the valUe-based methods sUch as valUe-iteration
Shapley (1953); Littman (1994), where there is a central controller which specifies the policies
of both players. When the joint policy of both players are considered, the problem of solving the
optimal valUe fUnction corresponds to finding the fixed point of the Bellman operator, which is
defined independently of the policy of the players. Second, when viewing the policy optimization
in zero-sUm Markov game as an optimization problem for both players together, althoUgh we have
1
Under review as a conference paper at ICLR 2021
a fixed objective function, the problem is minimax optimization with a non-convex non-concave
objective. Even for classical optimization, such a kind of optimization problem remains less less
understood (Cherukuri et al., 2017; Rafique et al., 2018; Daskalakis & Panageas, 2018; Mertikopoulos
et al., 2018). It is observed that first-order methods such as gradient descent might fail to converge
(Balduzzi et al., 2018; Mazumdar & Ratliff, 2018).
As an initial step to study policy optimization for MARL, we propose a novel policy optimization
algorithm for any player of a multi-player Markov game, which is dubbed as smooth fictitious self-
play (FSP). Specifically, when a player adopts smooth FSP, in each iteration, it first solves a policy
evaluation problem that estimates the value function associate with the current joint policy of all
players. Then it update its own policy via an entropy-regularized proximal policy optimization (PPO)
Schulman et al. (2017) step, where the update direction is obtained from the estimated value function.
This algorithm can be viewed as an extension of the fictitious play (FP) algorithm that is designed
for normal-form games (Von Neumann & Morgenstern, 2007; Shapley, 1953) and extensive-form
games (Heinrich et al., 2015; Perolat et al., 2018) to Markov-games. FP is a general algorithmic
framework for solving games where an agent first infer the policy of the opponents and then adopt
a policy that best respond to the inferred opponents. When viewing our algorithm as a FP method,
instead of estimating the policies of the opponents directly, the agent infers the opponent implicitly
by estimating the value function. Besides, policy update corresponds to a smoothed best-response
policy Swenson & Poor (2019) based on the inferred value function.
To examine the theoretical merits of the proposed algorithm, we focus on two-player zero-sum
Markov games and let both players follow smooth FSP, i.e., with self-play. Moreover, we restrict to a
class of Lipschitz games (Radanovic et al., 2019) where the impact of each player’s policy change
on the environment is Lipschitz continuous with respect to the magnitude of policy change. For
such a Markov game, we tackle the challenge of non-stationarity by imposing entropy regularization
which brings algorithmic stability. In addition, to establish convergence to Nash equilibrium, we
explicitly characterize the geometry of the policy optimization problem from a functional perspective.
Specifically, we prove that the objective function, as a bivariate function of the two players’ policies,
despite being non-convex and non-concave, satisfies a one-point strong monotonicity condition
(Facchinei & Pang, 2007) at a Nash equilibrium. Thanks to such benign geometry, we prove that
smooth FSP converges to a neighborhood of a Nash equilibrium at a sublinear O(1/T) rate, where
E J	I	1 六…I	JC.	一	I	I	C
T is the number of policy iterations and O hides logarithmic factors. Moreover, as a byproduct of
our analysis, if any of the two players deviates from the proposed algorithm, it is shown the other
player that follows smooth FSP exploits such deviation by finding the best-response policy at a same
sublinear rate. Such a Hannan consistency property exhibited in our algorithm is related to Hennes
et al. (2020), which focus on normal-form games. Thus, our results also serve as a first step towards
connecting regret between minimization in normal-form/extensive-form games and Markov games.
Contribution. Our contribution is two-fold. First, we propose a novel policy optimization algorithm
for Markov games, which can be viewed as a generalization of FP. Second, when applied to a class
of two-player zero-sum Markov games satisfying a Lipschitz regularity condition, our algorithm
provably enjoys global convergence to a neighborhood of a Nash equilibrium at a sublinear rate.
To the best of our knowledge, we propose the first provable FSP-type algorithm with finite time
convergence guarantee for zero-sum Markov games.
Related Work. There is a large body of literature on the value-based methods to zero-sum Markov
games (LagoUdakis & Parr, 2012; Perolat et al., 2016; Zhang et al., 2018; Zou et al., 2019). More
recently, Perolat et al. (2018) prove that actor-critic fictitious play asymptotically converges to the
Nash equilibrium, while our work provides finite time convergence guarantee to a neighborhood of a
Nash equilibrium. In addition, Zhang et al. (2020) study the sample comlexity of planning algorithm
in the model-based MARL settting as opposed to the model-free setting with function approximation
in this paper.
Closely related to smooth FSP proposed in this paper, there is a line of work in best-response
algorithms (Heinrich et al., 2015; Heinrich & Silver, 2016), which have also shown great empirical
performances (Dudziak, 2006; Xiao et al., 2013; Kawamura et al., 2017). However, they are only
applicable to extensive-form games and not directly applicable to stochastic games. Also, our smooth
FSP is related to Swenson & Poor (2019), which focus on the potential games. It does not enforce
entropy-regularization and only provides asymptotic convergence guarantee to a neighborhood of the
2
Under review as a conference paper at ICLR 2021
Nash equilibrium for smooth fictitious play in multi-player two-action potential games. Moreover, our
work also falls into the realm of regularizing and smoothing techniques in reinforcement learning (Dai
et al., 2017; Geist et al., 2019; Shani et al., 2019; Cen et al., 2020), which focus on the single-agent
setting.
2 Background
In this section, we briefly introduce the general setting of reinforcement learning for two-player
zero-sum Markov games.
Zero-Sum Markov Games. We consider the two-player zero-sum Markov game
(S, A1, A2, P, r, γ), where S ⊂ Rd is a compact state space, A1 and A2 are finite action spaces of
Player 1 and Player 2, respectively, P : S × S × A1 × A2 → [0, 1] is the Markov transition kernel,
r : S × A1 × A2 → [-1, 1] is the reward function of Player 1, which implies that the reward function
of Player 2 is -r, and γ ∈ (0, 1) is the discount factor. Let r1 = r and r2 = -r be the reward
functions of Player 1 and Player 2, respectively. For notational simplicity, throughout this paper, we
write Player -i as Player i’s opponent, where i ∈ {1, 2}. In the rest of this paper, we omit i ∈ {1, 2}
where it is clear from the context. Also, We denote by E∏i,∏-i [ ∙ ] the expectation over the trajectory
induced by the policy pair [πi ; π-i].
Given a policy π-i : A-i × S → [0, 1] of Player -i, the performance of a policy πi : Ai × S → [0, 1]
of Player i is evaluated by its state-value function (Vi-function) Viπi,π-i : S → R, which is defined
as
∞
Viπ ,π (S)= E∏i,∏-i X Yt ∙ ri(st, ai,a-i) s0 = s .	(2.I)
Correspondingly, the performance of a policy πi : Ai × S → [0, 1] of Player i is evaluated by its
πi π-i
action-value function (Qi-function) Qiπ ,π : S × Ai × A-i → R, which is defined by the following
Bellman equation,
i -i	i -i
Qi	,	(s,ai,a-i) =	ri(s,ai,a-i) + Y ∙	Es，〜P(∙∣	§a,a-)[匕,(s	)].
We denote by νπi,π-i (S) and σπi,π-i (S, ai, a-i) = πi(ai | S) ∙ π-i(a-i | S) ∙ νπi,π-i (S) the station-
ary state distribution and the stationary state-action distribution associated with the policy pair
[πi; π-i], respectively. Correspondingly, we denote by Eσ i -i [ ∙ ] and Eν i -i [ ∙ ] the expectations
E(s,ai,a-i)〜σ i -i [ ∙ ] and Es 〜ν7r i π-i [ ∙ ] ,respectively. Throughout this paper, we denote by h∙, •〉the
inner product between vectors.
Let [∏1,∏2] be a Nash equilibrium of the two-player zero-sum Markov game (S, A1, A2, P, r, γ),
which exists (Shapley, 1953) and satisfies
J(∏1,∏2) ≤ J(∏1,∏2) ≤ J(∏1,∏2)
for all policy pairs [π1; π2]. Here we define the performance function as
J (∏1,∏2) = Eν*[V1π1,π2 (s)],	(2.2)
where V is the stationary distribution σ∏ι ∏.
*,*
Regularized Markov Games. Based on the definition of the two-player zero-sum Markov game
(S, A1, A2, P, r, Y), we define its entropy-regularized counterpart (S, A1, A2, P, r, Y, λ1, λ2), where
λ1, λ2 ≥ 0 are the regularization parameters. Specifically, (S, A1, A2, P, r, Y, λ1, λ2) is defined
as the two-player general-sum Markov game with the reward function of Player i replaced by its
πi π-i
entropy-regularized counterpart r(πi),π : S × Ai × A-i → R, which is defined as
i -i
r(i),	(s, ai, a-i) = ri(s, ai, a-i) - λi ∙ logπi(ai | s).
With a slight abuse of notation, we write
i -i
ri ,	(s) = Eπi,π-i ri(s,ai,a-i)],
r∏i),π i(S) = E∏i,∏-i [r∏i),π i(s,ai,a-i)] = TTM-(S) + λi ∙ H(πi(∙ | S))
(2.3)
3
Under review as a conference paper at ICLR 2021
as the state-reward function and the entropy-regularized state-reward function, respectively. Here
H(πi(∙ | S)) 二 - Pai∈Ai πi(ai | S) ∙ log πi(ai | S) is the Shannon entropy. For Player i, the entroPy-
regularized state-value function (V(i)-function) V(πi)i,π-i : S → R and the entropy-regularized
πi π-i
action-value function (Q(i)-function) Q(πi),π	: S × Ai × A-i → R are defined as
i -i	i -i
V(i) ,	(S)= E∏i,∏-i E 7^ r(i),	(st, at, a- ) s0 = S ,	(2.4)
i -i	i -i
Q(t),	(s,ai,a-t)	=	rt(s,at,a-t) + Y ∙ Es，〜P(∙∣ s,ai,a-i)[V(t)	,	(s	)]	,	(2.5)
πi π-i
respectively. By the definition of r(πt),π	in (2.3), we have that, for all policy pairs [πt; π-t] and
S∈S,
∣E∏i,∏-i [rπi),π-i(s, at, a-t)] I ≤ 1 + λt ∙ log |Ai|,
which, by (2.4) and (2.5) implies that, for all policy pairs [πt; π-t] and (S, at, a-t) ∈ S × At × A-t,
1 + λt ∙ log |Ai|
1 - Y
(2.6)
IQ*，"-'(s,at,a-t)∣ ≤ Qmax = 1 + Y Y1+1-, lAil).	(2.7)
3	Fictitious S elf-Play for Zero-Sum Markov Games
In this section, we introduce smooth fictitious self-play (FSP) for two-player zero-sum Markov games.
3.1	FSP: From Matrix Games to Markov Games
FSP is an algorithmic framework for finding the Nash equilibria of games. It consists of two building
blocks: (I) inferring the opponent’s policy by playing against each other, namely fictitious play, and
(II) improving the two players’ policies with symmetric updating rules, namely self-play. Specifically,
Player i best responds to a mixed policy of Player -i, which is a weighted average of Player -i’s
historical policies. Here playing a mixed policy π-t = α ∙ π-t + (1 - α) ∙ π-t0 means that, at the
beginning of the game, the player chooses to play the policy π-t with probability α and play the
policy π-t0 with probability 1 - α.
FSP is originally developed for normal-form games (Von Neumann & Morgenstern, 2007; Shapley,
1953) and extensive-form games (Heinrich et al., 2015; Heinrich & Silver, 2016). In (entropy-
regularized) two-player zero-sum matrix games, which are the special cases of (entropy-regularized)
two-player zero-sum Markov games with |S | = 1 and no state transition, mixing two policies π-t
and π-t0 with probabilities α and 1 - α, respectively, is equivalent to averaging the corresponding
Qt-functions, i.e.,
Qπi,α∙π-i+(1-α)∙π-i0 = α ∙ Qπi,π-i + (1 - α) ∙ Qπi,π-i0.
In other words, in a two-player zero-sum matrix game, Player i is equivalently best responding to
a weighted average of the historical Qt-functions by taking the corresponding greedy action. To
generalize FSP to the two-player zero-sum Markov game (S, A1, A2, P, r, Y), we propose to let
Player i best respond to the following weighted average of the historical marginalized Q(t)-functions
at the t-th iteration,
， A、	，一	∖	：=：：	/ 八	丈∏i .∏∙ i / 八
Qt+1,(t)(s, a ) = (1 - at,(i)) ∙ Qt,(t)(s, a ) + αt,(t) ∙ Q(t)	(s, a ),
(3.1)
■〜∏i ∏-i	.
where at，(t)∈ [0,1] is the mixing rate. Here the marginalized Q(t)-function Q3	(s, at) is defined
as
i -i	i -i
Qe(πt),π	(S, at) =Eπ-i[Q(πt),π (S, at, a-t)]
(3.2)
4
Under review as a conference paper at ICLR 2021
Recursively applying the symmetric updating rule in (3.1), we obtain
Qt+ι,(i)(s,ai) = XX {",(i)∙ Y! (I- ɑk,(i)) ∙ QniT,π-i (s,ai)),
τ=0	k=τ +1
which is the weighted average of the historical marginalized Q(i)-functions. Here we use the
convention that Qk=t+ι(1 一 百3⑶)=1. Correspondingly, (3.1) induces the following symmetric
policy updating rule,
∏t+best (ai | S) = l(ai = argmax{Qt+ι,(i) (s, ai0) }^,	(3.3)
where the obtained policy ∏t+best best responds to Qt+i,(i)defined in (3.1) by taking the correspond-
ing greedy action.
3.2	Markov Games: From FSP to Smooth FSP
FSP is only known to converge asymptotically even in two-player zero-sum matrix games (Robinson,
1951). Instead, we consider smooth FSP, which uses the following smoothed best-response,
∏i+ι(ai | S) ∞ exp{Et+ι,(i)(s,ai)}.	(3.4)
Here the ideal energy function Et+ι,(i)(s, ai)= 勺+:⑴∙ Qt+ι,(i)(s, ai) is proportional to the
weighted average of the historical marginalized Q(i)-functions defined in (3.1) with the normalization
parameter κt+1,(i) > 0.
In the sequel, we simplify the symmetric updating rules in (3.1) and (3.4). Let the stepsizes be
αt,(i) = κt+1,(i) ∙ αt,(i),	αt,(i) = Kt+1,(i)/Kt,(i) ∙ (1 - αt,(i)).	(3.5)
πi π-i
Recall that Q(it), t , which is the marginalized Q(i)-function, is defined in (3.2). Corresponding to
(3.1), we have the following symmetric updating rule for the energy functions,
Et+1,(i)(s,ai) = αt,(i) ∙ Et,(i)(s,aD + αt,(i) ∙ Qnt)E(S,ai),	(3.6)
which gives the following symmetric policy updating rule equivalent to (3.4),
∏i+ι(ai | S) B (∏i(ai | s)『t⑸∙ exp{αt,(i) ∙ Qnit),"t (s,ai)}.
We call Et+1,(i) the ideal energy function, since it is directly obtained from the symmetric updating
rule in (3.3), which operates in the functional space given the marginalized Q(i)-functions.
3.3	Implementing Smooth FSP
In practice, it remains to approximate the ideal energy function Et+1,(i) within a parameterized
function class, which is further used to parameterize the policy πti+1. For notational simplicity, we
concatenate the parameters of the policies πti+1 and πt-+i1 into a single parameter θt+1 ∈ Θ, which
gives the parameterized policy pair [πθi ; πθ-i]. Meanwhile, we need to estimate the marginalized Q(i)-
function Qe(niθ)t,nθt (S, ai) defined in (3.2). In practice, the parameterization of the energy function
and the marginalized Q(i)-function are set to be neural networks, which means that Θ = RN with
N being the size of the neural network. To implement smooth FSP, given θt ∈ Θ, we find the best
parameter θt+1 ∈ Θ that minimizes the mean squared error (MSE),
Eσt	E (Eθt+ι,(i)(S,ai)-
i∈{1,2}
i0
where Et+1,(i)(S,ai) = α0t,(i)
Ebt+1,(i)(S,ai)2 ,
• Eθt,(i)(S, ai) + αt,(i) ∙ Qb(i)t，θt (S, ai)
(3.7)
(3.8)
is the estimated ideal energy function. Here Qb(niθ)t,nθt (S, ai) is the estimator of the marginalized Q(i)-
function Qe(niθ)t,nθt (S, ai). Such an estimator is obtained based on the data generated by smooth FSP
via policy evaluation (Sutton et al., 2000). For notational simplicity, in (3.7) and the rest of the paper,
5
Under review as a conference paper at ICLR 2021
we write the stationary state-action distribution σπi ,π-i and the stationary state distributionνπi ,π-i
associated with the policy pair [πθi ; πθ-i] as σt and νt, respectively.
We define the bounded function class FR with the radius R > 0 as FR = {f : kf k∞ ≤ R}.
Algorithm 1 gives the implementation of smooth FSP for two-player zero-sum Markov games.
Algorithm 1 Smooth FSP for Two-Player Zero-Sum Markov Games
1:	Require Two-player zero-sum Markov game (S, A1, A2, P, r, γ), number of iterations T, reg-
ularization parameters {λi}i∈{1,2}, truncation parameters {Q(mi)ax, E(mi)ax}i∈{1,2}, and stepsizes
{αt,(i) , α0t,(i) }0≤t≤T -1,i∈{1,2}
2:	Initialize the energy function Eθ0,(i)(s, ai) J 0 (i ∈ {1,2})
3:	Fort = 0, . . .,T - 1 andi ∈ {1, 2} do
4： Set the policy ∏θt (∙ | S) H exp{Eθ%(i)(s, ∙)}
5:	Generate the marginalized Q(i)-function estimator Qb(πiθ)t,πθt (s, ai) ∈ FQmax using the data
generated by fictitious play with the policy pair [πθi ; πθ-i]
6:	Update the estimated ideal energy function
E-t+1,(i)(4 s, ai) J ɑt,(i) . Eθt,(i)(S, ai) + αt,(i) ∙ Q (i)，θt (S, ai)
7:	Minimize (3.7) to obtain the energy function Eθt+1,(i) (s, ai) ∈ FEmax
8:	End
9:	Output: {πθit }0≤t≤T -1,i∈{1,2}
4 Main Results
In this section, we establish the convergence of smooth FSP for two-player zero-sum Markov games
by casting it as regularized proximal policy optimization (PPO).
4.1 Smooth FSP as Regularized PPO
In the sequel, we connect the energy function update in (3.8) with regularized PPO. Corresponding
to the estimated ideal energy function updates Et+1,(i) in (3.8), we define the estimated ideal policy
update as
„ √	...	-O	,	....
∏t+ι(∙ I S) H exp{Et+ι,(i)(s, ∙)}.	(4.1)
We have the following proposition states the equivalence between smooth FSP and regularized PPO.
Proposition 4.1. For all 0 ≤ t ≤ T - 1, let the stepsizes αt,(i) and α0t,(i) of Algorithm 1 satisfy
λi = (I- at,(i)”αt,(i) > 0.
At the t-th iteration of Algorithm 1, the policy update in (4.1) is equivalent to solving the regularized
PPO subproblem,
πbti+1
argmax
πi
) - λi ∙ logπθt(∙ | s),πi(Ts) - πθt(∙ |s)〉
(4.2)
—KL(πi(∙ I S)
Here Qb (πiθ)t,πθt (S, ai) is the estimator of the marginalized Q(i)-function Qe(πiθ)t,πθt (S, ai).
Proof. See Appendix A for a detailed proof.
□
6
Under review as a conference paper at ICLR 2021
Proposition 4.1 implies that smooth FSP proximally improves the policy πi based on the regularized
performance function,
J(i)(∏i,∏-i)= Eν* [V∏i,π-i(s)].	(4.3)
Proposition C.1 implies that, the smaller the regularization parameter λi is, the closer the regularized
performance function J(i) is to the performance function J. In the rest of the paper, we show that,
with a proper choice of λi, smooth FSP converges to a neighborhood of a Nash equilibrium [∏1; ∏2]
at a sublinear rate of O(1/T ).
4.2 Convergence to Nash Equilibrium
Let P(St = S | ∏i, ∏-i, so 〜 V) be the probability that the trajectory, which is generated by the
policy pair [∏i; ∏-i] with the initial state distribution so 〜ν, reaches the state S at the timestep t.
Correspondingly, let
∞
PV'," '(S) = (I-Y) ∙ X γt ∙ P(St = S | πi,π-i,so 〜v)
(4.4)
t=o
be the visitation measure of [∏i; ∏-i] with the initial state distribution So 〜v. Also, for notational
simplicity, we define
∞
ρπ,∏∏π0,∏-i0(S) = (I-Y) ∙ X Yt ∙ P(st+1 = SIni,π-i, (So,a0,a-i) 〜IVnlLI)	(4.5)
t=o
as the visitation measure of the policy pair [πi ; π-i] with the initial state-action distribution vπilπ-il.
We lay out the following assumption on the concentrability coefficient. With a slight abuse of notation,
we write v and πil in the subscripts as S and ai , respectively, when they are point masses.
Assumption 4.2 (Concentrability Coefficient). We assume that for the two-player zero-sum Markov
game (S, A1, A2, P, r, Y), there exists ζ > 0 such that
Eν* h∣dρ∏iaπ⅛i /dv *∣2]1/2 ≤ Z
for all S ∈ S, ai ∈ Ai , and πi = πθi generated by the policy update in Line 4 of Algorithm 1. Here
πi π-i	πi π-i
dρ , .* -i/dv* is the Radon-Nikodym derivative, where P ,. * — is defined in (4.5).
s,a",π*	s,a",π*
The notion of concentrability coefficient in Assumption 4.2 is commonly used in the literature (Munos
& Szepesvari, 2008; Antos et al., 2008; Farahmand et al., 2010; Tosatto et al., 2017; Yang et al.,
2019).
For all policy pairs [πi; π-i], we define the Markov state transition kernel as
P πi,π-i (∙∣ S)= E∏i,∏-i[P (∙∣ S,ai,a-i)].
(4.6)
With a slight abuse of notation, we write P πi,π-i as the Markov state transition operator induced by
the Markov state transition kernel defined in (4.6), such that
[Pπi,π-i ◦ h](S) =	h(Sl)Pπi,π-i(dSl | S),
s0∈S
(4.7)
where h : S → R is an L1 -integrable function and the Lebesgue measure over S ⊂ Rd is used.
Correspondingly, we define the operator norm of an operator O as
kOkop = sup kO ◦ hkL1(S)khkL1(S) = sup	kO ◦ hkL1(S),
h	khkL1(S)≤1
where ∣∣ ∙ ||li(s)is the Lι-norm over the state space S. The following assumption characterizes the
Lipschitz continuity of P πi,π-i and rπi,π-i with respect to π-i .
Assumption 4.3 (Lipschitz Game). We assume that for the two-player zero-sum Markov game
(S, A1, A2, P, r, Y), there exists ιi > 0 such that for all S ∈ S and [πi; π-i],
kPπi,π-i - Pπi,π-ikop ≤	∣i∙	Eν* [κL(∏-i(∙ | S) Il π-i(∙ | S))i1/2,	(4.8)
∣rπi,π*i(s) - rπi,π i(s)∣ ≤	∣i	∙	KL(π-i(∙ | s) ∣∣ π-i(∙ | s))1/2.	(4.9)
7
Under review as a conference paper at ICLR 2021
The Lipschitz coefficient ιi in (4.8) of Assumption 4.3 quantifies to the influence of Player -i
on the nonstationary environment that Payer i faces. Such a notion of influence is proposed by
Radanovic et al. (2019) in the tabular setting. In particular, the expected KL-divergence between
the policies is used in place of the distance maxs∈s ∣∣∏-i(∙ | S) - ∏-i0(∙ | s)kι in Radanovic et al.
(2019). Such an assumption is also related to the linear-quadratic game (LQG) literature (see, e.g.,
Zhang et al. (2019)), where the Lipschitz continuity is established based on the special structure in
the LQG model. In Lemma C.2, we show that such a Lipschitz coefficient ιi quantifies the Lipschitz
continuity of the marginalized Q(i)-function of the entropy-regularized two-player Markov game
(S,A1,A2,P,r,γ,λ1,λ2).
i
Recall that ∏i+ι 8 exp{Et+ι,(i)} is defined in (4.1), where Et+ι,(i) is defined in (3.8). Also, recall
that ∏θ叶ɪ 8 exp{Eθt+ι,(i)} is defined in Line 4 of Algorithm 1, where Eg=+1,⑴ is obtained by
minimizing (3.7) in Line 7 of Algorithm 1. Meanwhile, we define the ideal policy update as
πi+ι(∙ | S) (X exp{Et+ι,(i)(S, ∙)},
where Et+ι,(i)(s, ai)=。[⑴∙ E»,(i)(s, ai) + αt,(i) ∙ Q；；“* (s,ai)	(4.10)
is the corresponding ideal energy function update.
We lay out the following assumption on the errors that arise from the estimation of the marginalized
Q(i)-function Qe(πiθ)t,πθt and the minimization of the MSE in (3.7).
Assumption 4.4 (Estimation Error). We assume that there exist t , 0t > 0 such that for all 0 ≤ t ≤
T - 1,
Eν* h∣∣Eθt+ι,(i)(S,∙)-Ebt+ι,(i)(s, ∙)∣∣∞i ≤ et,	(4/I)
Eν* [<Eθt+ι,(i)(s, B - E t+1,(i)(s, ∙),π* (∙ | S)- πθ t (∙ | S))i ≤4.	(4.12)
Assumption 4.4 characterizes the estimation error through the policy updates in Line 7 of Algorithm
1. In particular, (4.11) upper bounds the errors arising from the minimization of the MSE in (3.7),
which is zero as long as the representation power of the parameterized class of the energy functions
is sufficiently strong. Meanwhile, by (3.7) and (4.10), the gap between £6叶1,(办 and Et+ι,(i) involves
(I) the gap between Et+ι,(i) and Et+ι,(i), which arises from the gap between Q；：) ,πθt and Qen) ,πθt ,
and (II) the gap between Eθt+1,(i) and Et+1,(i), which arises the minimization of the MSE in (3.7).
πi π-i	πi π-i
Hence, 0t in (4.12) is zero as long as the estimator Qb(iθ)t, θt of Qe(iθ)t, θt is accurate and t is zero.
We summarize t and 0t into the following total error σ,
T-1
σ = X (t + 1) •& + G	(4.13)
t=0
As discussed in Lemmas 4.7 and 4.8 of Liu et al. (2019), under Assumption 4.2, when we use
sufficiently deep and wide neural networks equipped with the rectified linear unit (ReLU) activation
function to parameterize the marginalized Q(i)-functions and the energy functions, Assumption 4.4
can be satisfied with σ = Oe(1). See Appendix B for a detailed discussion.
We are now ready to present the following theorem on the convergence of the policy sequence
{[∏θt; ∏θt]}o≤t≤τ-ι to a neighborhood of a Nash equilibrium [∏1; ∏2]. Recall that Qmax and V(max
are defined in (2.6) and (2.7), respectively. Also, recall that ζ is the concentrability coefficient in
Assumption 4.2, ιi is the Lipschitz coefficient in Assumption 4.3, and σ is defined in (4.13).
Theorem 4.5 (Convergence of Smooth FSP to Nash Equilibrium). Suppose that Assumptions 4.2-4.4
hold. We set the regularization parameter λi ≥ 2Mi, where
Mi=	[2	+ Pi∈{i,2}(V(max +	Qmax	∙	Z)/(1 -	γ)]	∙	∣i.	(4.14)
In Algorithm 1, we set Emax = Qmax/(λi - Mi) and
t，(i)	(t +	1)	∙	mini∈{1,2}{λi	- Mi}，	t，(i)	(t +	1)	∙ mini∈{1,2}{λi	- Mi},,
8
Under review as a conference paper at ICLR 2021
For the policy sequence {[πθ1 ; πθ2 ]}0≤t≤T -1 generated by the policy update in Line 7 of Algorithm
1, we have
1	X-1Γ7 (箱 Q 7/ 1	2)] Pi∈{1,2}[2 + 2λ"Ci- Mi)2] ∙ (Qmax)2	log T
T ∙	t=0 JK,πθt)-J3,π*)]	≤ —(1-γ) ∙ mmi∈{i,2}{λi - Mi}---F	(4.16)
+
2σ ∙ mini∈{i,2}{λi - Mi}
(1- Y) ∙ T
+ E λi ∙ log |Ai|.
i∈{1,2}
Proof. See Appendix C for a detailed proof. The key to our proof is the convergence of infinite-
dimensional mirror descent with the primal and dual errors. In particular, the errors are characterized
in Appendix B.	□
Recall that the Lipschitz coefficient ιi is defined in Assumption 4.3. In Lemma C.2, we interpret ιi
as the Lipschitz coefficient of the marginalized Q(i)-function. Meanwhile, recall that Theorem 4.5
requires λi ≥ 2Mi , where Mi scales linearly with ιi . Hence, the smaller the Lipschitz coefficient ιi
is, the smaller the regularization parameter λi can be, which in turn leads to a smaller regularization
bias characterized in Proposition C.1. Thus, the policy sequence {[πθ1 ; πθ2 ]}0≤t≤T -1 generated by
Algorithm 1 converges to a smaller neighborhood of aNash equilibrium [∏1; ∏2].
We give the following two sufficient conditions for the Lipschitz coefficients. (I) The two players have
similar influence to the game, i.e., ∣1∕∣2 = O(1): a sufficient requirement on both of the Lipschitz
coefficients is
Ii ≤ (1 - Y)2∕[8(1 + Y) ∙ log |Ai|], i ∈{1, 2}.
(II)	One of the two players (without loss of generality, we assume it is Player 2) has dominant
influence to the game compared to the other: let ∣1∕∣2 = z > 0, in which case We set Mi in (4.14) as
Mi = √2z ∙ [2 + Pi∈{i,2}(V(max + Qmax ∙Z)∕(1-Y)] ∙ ∣2, {1,2}.
Then one sufficient requirement on the ratio z is
Z ≤ (1 - Y)4∕[l6(1 + Y)∣2 ∙ Iog(IA1HA2I)].
As z moves towards zero, the convergence guarantee approaches those for single-controller case.
Please see Appendix I for a more detailed illustration on case(II).
We remark in the following that, with stronger assumptions, we can strengthen Theorem 4.5 to satisfy
Hannan consistency.
Remark 4.6 (Hannan Consistency). When Assumptions 4.2-4.4 hold for any policy [πi0; π-i0]
instead of only a Nash equilibrium [n：; ∏-i], we can prove that, when one of the player does not
update the policy as described in Algorithm 1, the opposing player can exploit the strategies it plays.
Specifically, for example, when Player 2 plays the policy sequence {πet2}0≤t≤T -1 while Player 1
updates its policy according to Algorithm 1, we have
T-1
• EJ (∏1,e2) -J (∏1t,e2)]	(4.17)
t=0
sup{ T
1•	(λι	-	Mi)	[2+2λ2∕(λι	-	M1)2]	• ©max)2	logT
≤ (1-γ) ・ T +—(1 - Y) • (λi - Mi)-----------F + λ1 •log |A 1，
which implies that the policy sequence {πθi }0≤t≤T -i converges to the best policy in hindsight with
respect to {πet2}0≤t≤T -i. As a consequence, we can also replace the left-hand side of (4.16) by the
following duality gap,
1 T-i
sup( T ∙ EJ (πi,π2t)
π1 T t=0
-in2f{ T
π2 T
T-i
X J(πθit,π2)
t=0
(4.18)
See Appendix J for a more detailed illustration on Remark 4.6.
9
Under review as a conference paper at ICLR 2021
References
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261,
2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918, 2018.
Andras Antos, Csaba Szepesvari, and Remi Munos. Fitted Q-iteration in continuous action-space
mdps. In Advances in Neural Information Processing Systems, pp. 9-16, 2008.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel.
The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemySIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale
deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Lucian Bu, Robert Babu, Bart De Schutter, et al. A comprehensive survey of multiagent reinforcement
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),
38(2):156-172, 2008.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning
converges to global optima. arXiv preprint arXiv:1905.10027, 2019.
Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, and Yuejie Chi. Fast global convergence of
natural policy gradient methods With entropy regularization. arXiv preprint arXiv:2007.06558,
2020.
Ashish Cherukuri, Bahman Gharesifard, and Jorge Cortes. Saddle-point dynamics: conditions for
asymptotic stability of saddle points. SIAM Journal on Control and Optimization, 55(1):486-511,
2017.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
Vincent Conitzer and Tuomas Sandholm. AWesome: A general multiagent learning algorithm that
converges in self-play and learns a best response against stationary opponents. Machine Learning,
67(1-2):23-43, 2007.
Bo Dai, Albert ShaW, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song.
SBEED: Convergent reinforcement learning With nonlinear function approximation. arXiv preprint
arXiv:1712.10285, 2017.
Amit Daniely, Roy Frostig, and Yoram Singer. ToWard deeper understanding of neural netWorks:
The poWer of initialization and a dual vieW on expressivity. In Advances in Neural Information
Processing Systems, pp. 2253-2261, 2016.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9236-9246,
2018.
William Dudziak. Using fictitious play to find pseudo-optimal solutions for full-scale poker. In IC-AI,
pp. 374-380, 2006.
Francisco Facchinei and Jong-Shi Pang. Finite-Dimensional Variational Inequalities and Comple-
mentarity Problems. Springer Science & Business Media, 2007.
10
Under review as a conference paper at ICLR 2021
Amir-massoud Farahmand, Csaba Szepesvari, and Remi Munos. Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems, pp. 568-576,
2010.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision
processes. arXiv preprint arXiv:1901.11275, 2019.
Johannes Heinrich and David Silver. Deep reinforcement learning from self-play in imperfect-
information games. arXiv preprint arXiv:1603.01121, 2016.
Johannes Heinrich, Marc Lanctot, and David Silver. Fictitious self-play in extensive-form games. In
International Conference on Machine Learning, pp. 805-813, 2015.
Daniel Hennes, Dustin Morrill, Shayegan Omidshafiei, R’emi Munos, Julien Perolat, Marc Lanctot,
Audrunas Gruslys, Jean-Baptiste Lespiau, Paavo Parmas, Edgar Du‘e nez-Guzm’an, et al. Neural
replicator dynamics: Multiagent learning via hedging policy gradients. In Proceedings of the 19th
International Conference on Autonomous Agents and MultiAgent Systems, pp. 492-501, 2020.
Junling Hu and Michael P Wellman. Nash Q-learning for general-sum stochastic games. Journal of
Machine Learning Research, 4(Nov):1039-1069, 2003.
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang. Momentum-based policy gradient methods.
arXiv preprint arXiv:2007.06680, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, pp.
8571-8580, 2018.
Sham Kakade. A natural policy gradient. In Advances in Neural Information Processing Systems, pp.
1531-1538, 2002.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
International Conference on Machine Learning, pp. 267-274, 2002.
Keigo Kawamura, Naoki Mizukami, and Yoshimasa Tsuruoka. Neural fictitious self-play in imperfect
information games with many players. In Workshop on Computer Games, pp. 61-74. Springer,
2017.
Valentin Khrulkov, Alexander Novikov, and Ivan Oseledets. Expressive power of recurrent neural
networks. arXiv preprint arXiv:1711.00811, 2017.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems, pp. 1008-1014, 2000.
Michail Lagoudakis and Ron Parr. Value function approximation in zero-sum Markov games. arXiv
preprint arXiv:1301.0580, 2012.
Andrzej Lasota and Michael C Mackey. Chaos, Fractals, and Noise: Stochastic Aspects of Dynamics,
volume 97. Springer Science & Business Media, 2013.
Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.
arXiv preprint arXiv:1902.06720, 2019.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine Learning Proceedings 1994, pp. 157-163. Elsevier, 1994.
Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural proximal/trust region policy optimization
attains globally optimal policy. arXiv preprint arXiv:1906.10306, 2019.
Eric Mazumdar and Lillian J Ratliff. On the convergence of competitive, multi-agent gradient-based
learning. arXiv preprint arXiv:1804.05464, 2018.
11
Under review as a conference paper at ICLR 2021
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar,
and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra
(gradient) mile. arXiv preprint arXiv:1807.02629, 2018.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning, pp. 1928-1937, 2016.
Remi Munos and Csaba Szepesvari. Finite-time bounds for fitted value iteration. Journal ofMachine
Learning Research, 9(May):815-857, 2008.
Arkadi S Nemirovski and David B Yudin. Problem Complexity and Method Efficiency in Optimization.
Springer, 1983.
Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Springer
Science & Business Media, 2013.
Brendan O’Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih. Combining policy
gradient and Q-learning. arXiv preprint arXiv:1611.01626, 2016.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochas-
tic variance-reduced policy gradient. In ICML 2018-35th International Conference on Machine
Learning, volume 80, pp. 4026-4035, 2018.
Julien Perolat, Bilal Piot, Bruno Scherrer, and Olivier Pietquin. On the use of non-stationary
strategies for solving two-player zero-sum markov games. In International Conference on Artificial
Intelligence and Statistics, pp. 893-901, 2016.
Julien Perolat, Bilal Piot, and Olivier Pietquin. Actor-critic fictitious play in simultaneous move
multistage games. 2018.
Goran Radanovic, Rati Devidze, David C Parkes, and Adish Singla. Learning to collaborate in
Markov decision processes. arXiv preprint arXiv:1901.08029, 2019.
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv preprint arXiv:1810.02060, 2018.
Julia Robinson. An iterative method of solving a game. Annals of Mathematics, pp. 296-301, 1951.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized MDPs. arXiv preprint arXiv:1909.02769, 2019.
Lloyd S Shapley. Stochastic games. Proceedings of the National Academy of Sciences, 39(10):
1095-1100, 1953.
Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy
gradient. In International Conference on Machine Learning, pp. 5729-5738, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354-359, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in Neural Information
Processing Systems, pp. 1057-1063, 2000.
12
Under review as a conference paper at ICLR 2021
Brian Swenson and H Vincent Poor. Smooth fictitious play in N × 2 potential games. In 2019 53rd
Asilomar Conference on Signals, Systems, and Computers ,pp.1739-1743. IEEE, 2019.
Samuele Tosatto, Matteo Pirotta, Carlo D’Eramo, and Marcello Restelli. Boosted fitted Q-iteration.
In International Conference on Machine Learning, pp. 3434-3443, 2017.
Oriol Vinyals,Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung
Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in
StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior (commemora-
tive edition). Princeton university press, 2007.
Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. arXiv preprint arXiv:1909.01150, 2019.
Nan Xiao, Xuehe Wang, Tichakorn Wongpiromsarn, Keyou You, Lihua Xie, Emilio Frazzoli, and
Daniela Rus. Average strategy fictitious play with application to road pricing. In 2013 American
Control Conference, pp. 1920-1925. IEEE, 2013.
Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive
variance reduction. arXiv preprint arXiv:1909.08610, 2019.
Pan Xu, Felicia Gao, and Quanquan Gu. An improved convergence analysis of stochastic variance-
reduced policy gradient. In Uncertainty in Artificial Intelligence, pp. 541-551. PMLR, 2020.
Zhuora Yang, Yuchen Xie, and Zhaoran Wang. A theoretical analysis of deep Q-learning. arXiv
preprint arXiv:1901.00137, 2019.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Bayar. Fully decentralized multi-
agent reinforcement learning with networked agents. arXiv preprint arXiv:1802.08757, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Policy optimization provably converges to Nash
equilibria in zero-sum linear quadratic games. In Advances in Neural Information Processing
Systems, pp. 11602-11614, 2019.
Kaiqing Zhang, Sham Kakade, Tamer Basar, and Lin Yang. Model-based multi-agent RL in zero-sum
Markov games with near-optimal sample complexity. Advances in Neural Information Processing
Systems, 33, 2020.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear
function approximation. In Advances in Neural Information Processing Systems, pp. 8668-8678,
2019.
13