Under review as a conference paper at ICLR 2021
Gradient descent temporal
difference-difference learning
Anonymous authors
Paper under double-blind review
Ab stract
Off-policy algorithms, in which a behavior policy differs from the target policy
and is used to gain experience for learning, have proven to be of great practical
value in reinforcement learning. However, even for simple convex problems such
as linear value function approximation, these algorithms are not guaranteed to be
stable. To address this, alternative algorithms that are provably convergent in such
cases have been introduced, the most well known being gradient descent tempo-
ral difference (GTD) learning. This algorithm and others like it, however, tend to
converge much more slowly than conventional temporal difference learning. In
this paper we propose gradient descent temporal difference-difference (Gradient-
DD) learning in order to improve GTD learning by introducing second-order dif-
ferences in successive parameter updates. We investigate this algorithm in the
framework of linear value function approximation, analytically showing its im-
provement over GTD learning. Studying the model empirically on the random
walk and Boyan-chain prediction tasks, we find substantial improvement over
GTD learning and, in several cases, better performance even than conventional
TD learning.
1	Introduction
Off-policy algorithms for value function learning enable an agent to use a behavior policy that dif-
fers from the target policy in order to gain experience for learning. However, because off-policy
methods learn a value function for a target policy given data due to a different behavior policy, they
often exhibit greater variance in parameter updates. When applied to problems involving function
approximation, off-policy methods are slower to converge than on-policy methods and may even
diverge (Baird, 1995; Sutton & Barto, 2018).
Two general approaches have been investigated to address the challenge of developing stable and
effective off-policy temporal-difference algorithms. One approach is to use importance sampling
methods to warp the update distribution back to the on-policy distribution (Precup et al., 2000;
Mahmood et al., 2014). This approach is useful for decreasing the variance of parameter updates,
but it does not address stability issues. The second main approach to addressing the challenge of
off-policy learning is to develop true gradient descent-based methods that are guaranteed to be stable
regardless of the update distribution. Sutton et al. (2009a;b) proposed the first off-policy gradient-
descent-based temporal difference (GTD and GTD2, respectively) algorithms. These algorithms are
guaranteed to be stable, with computational complexity scaling linearly with the size of the function
approximator. Empirically, however, their convergence is much slower than conventional temporal
difference (TD) learning, limiting their practical utility (Ghiassian et al., 2020; White & White,
2016). Building on this work, extensions to the GTD family of algorithms (see Ghiassian et al.
(2018) for a review) have allowed for incorporating eligibility traces (Maei & Sutton, 2010; Geist
& Scherrer, 2014), non-linear function approximation such as with a neural network (Maei, 2011),
and reformulation of the optimization as a saddle point problem (Liu et al., 2015; Du et al., 2017).
However, due to their slow convergence, none of these stable off-policy methods are commonly used
in practice.
In this work, we introduce a new gradient descent algorithm for temporal difference learning with
linear value function approximation. This algorithm, which we call gradient descent temporal
difference-difference (Gradient-DD) learning, is an acceleration technique that employs second-
1
Under review as a conference paper at ICLR 2021
order differences in successive parameter updates. The basic idea of Gradient-DD is to modify
the error objective function by additionally considering the prediction error obtained in last time
step, then to derive a gradient-descent algorithm based on this modified objective function. In ad-
dition to exploiting the Bellman equation to get the solution, this modified error objective function
avoids drastic changes in the value function estimate by encouraging local search around the current
estimate. Algorithmically, the Gradient-DD approach only adds an additional term to the update
rule of the GTD2 method, and the extra computational cost is negligible. We show mathematically
that applying this method significantly improves the convergence rate relative to the GTD2 method
for linear function approximation. This result is supported by numerical experiments, which also
show that Gradient-DD obtains better convergence in many cases than conventional TD learning.
1.1	Related Work
In related approaches to ours, some previous studies have attempted to improve Gradient-TD al-
gorithms by adding regularization terms to the objective function. Liu et al. (2012) have used l1
regularization on weights to learn sparse representations of value functions, and Ghiassian et al.
(2020) has used l2 regularization on weights. Unlike these references, our approach modifies the
error objective function by regularizing the evaluation error obtained in the most recent time step.
With this modification, our method provides a learning rule that contains second-order differences
in successive parameter updates.
Our approach is similar to trust region policy optimization (Peters & Schaal, 2008; Schulman et al.,
2015) or relative entropy policy search (Peters et al., 2010), which penalize large changes being
learned in policy learning. In these methods, constrained optimization is used to update the policy
by considering the constraint on some measure between the new policy and the old policy. Here,
however, our aim here is to look for the optimal value function, and the regularization term uses the
previous value function estimate to avoid drastic changes in the updating process.
2	Gradient descent method for off-policy temporal difference
LEARNING
2.1	Problem definition and background
In this section, we formalize the problem of learning the value function for a given policy under
the Markov Decision Process (MDP) framework. In this framework, the agent interacts with the
environment over a sequence of discrete time steps, t = 1, 2,   At each time step the agent
observes a partial summary of the state st ∈ S and selects an action at ∈ A. In response, the
environment emits a reward rt ∈ R and transitions the agent to its next state st+1 ∈ S. The
state and action sets are finite. State transitions are stochastic and dependent on the immediately
preceding state and action. Rewards are stochastic and dependent on the preceding state and action,
as well as on the next state. The process generating the agent’s actions is termed the behavior policy.
In off-policy learning, this behavior policy is in general different from the target policy π : S → A.
The objective is to learn an approximation to the state-value function under the target policy in a
particular environment:
∞
V (S)= En X YtTrtISI = S ,	⑴
t=1
where γ ∈ [0, 1) is the discount rate.
In problems for which the state space is large, it is practical to approximate the value function. In
this paper we consider linear function approximation, where states are mapped to feature vectors
with fewer components than the number of states. Specifically, for each state S ∈ S there is a
corresponding feature vector x(S) ∈ Rp, with p ≤ |S|, such that the approximate value function is
given by
Vw(S) := w>x(S).	(2)
The goal is then to learn the parameters w such that Vw(S) ≈ V(S).
2
Under review as a conference paper at ICLR 2021
2.2 Gradient temporal difference learning
A major breakthrough for the study of the convergence properties of MDP systems came with the
introduction of the GTD and GTD2 learning algorithms (Sutton et al., 2009a;b). We begin by briefly
recapitulating the GTD algorithms, which we will then extend in the following sections. To begin,
we introduce the Bellman operator B such that the true value function V ∈ R|S| satisfies the Bell-
man equation:
V =R+γPV=: BV,
where R is the reward vector with components E(rn+1 |sn = s), and P is a matrix of state transition
probabilities. In temporal difference methods, an appropriate objective function should minimize
the difference between the approximate value function and the solution to the Bellman equation.
Having defined the Bellman operator, we next introduce the projection operator Π, which takes
any value function V and projects it to the nearest value function within the space of approximate
value functions of the form (2). Letting X be the matrix whose rows are x(s), the approximate value
function can be expressed as Vw = Xw. We will also assume that there exists a limiting probability
distribution such that ds = limn→∞ p(sn = s) (or, in the episodic case, ds is the proportion of time
steps spent in state s). The projection operator is then given by
Π = X(X>DX)-1X>D,
where the matrix D is diagonal, with diagonal elements ds .
The natural measure of how closely the approximation Vw satisfies the Bellman equation is the
mean-squared Bellman error:
MSBE(w) = kVw-BVwk2D,	(3)
where the norm is weighted by D, such that kVk2D = V>DV. However, because the Bellman
operator follows the underlying state dynamics of the Markov chain, irrespective of the structure
of the linear function approximator, BVw will typically not be representable as Vw for any w.
An alternative objective function, therefore, is the mean squared projected Bellman error (MSPBE),
which we define as
J(w) = kVw -ΠBVwk2D.	(4)
Following (Sutton et al., 2009b), our objective is to minimize this error measure. As usual in stochas-
tic gradient descent, the weights at each time step are then updated by ∆w = -KwJ(w), where
α > 0, and
-1 Nw J (w) = - E[(γ Xn+1 - Xn)x>][E(XnX>)]-1E(δnXn)
≈ - E[(γxn+1 - xn)xn>]η.	(5)
For notational simplicity, we have denoted the feature vector associated with sn as xn = x(sn).
We have also introduced the temporal difference error δn = rn + (γxn+1 - xn)>wn, as well as
η, a linear predictor to approximate [E(xnxn>)]-1E(δnxn). Because the factors in Eqn. (5) can be
directly sampled, the resulting updates in each step are
δn =rn + (γ xn+1 - xn ) wn
ηn+1 =ηn + βn (δn - xn>ηn)xn
wn+1 =wn - αn (γxn+1 - xn)(xn>ηn).	(6)
These updates define the GTD2 learning algorithm, which we will build upon in the following
section.
3 Gradient descent temporal difference-difference learning
In order to improve the GTD2 algorithm described above, in this section we modify the objective
function via additionally considering the approximation error Vw - Vwn-1 given the previous time
step n - 1. Specifically, we modify Eqn. (4) as follows:
JGDD(w|wn-1) = J(w) + κkVw - Vwn-1k2D,	(7)
3
Under review as a conference paper at ICLR 2021
Figure 1: Schematic diagram of Gradient-DD
learning with w ∈ R2 . Rather than updating
w directly along the gradient of the MSPBE
(arrow), the update rule selects wn that min-
imizes the MSPBE while satisfying the con-
StraintkVW - VWn-IIID ≤ μ (shaded ellipse).
where κ ≥ 0 is a parameter of the regularization.
Minimizing Eqn. (7) is equivalent to the following optimization
arg min J(W) s.t. ∣∣Vw - VWn-IkD ≤ μ
W
(8)
where μ > 0 is a parameter which becomes large when K is small, so that the MSPBE objective is
recovered as μ → ∞, equivalent to K → 0 in Eqn. (7). We show in the Appendix that for any μ > 0,
there exist κ ≥ 0 such that the solution of Eqn. (7) and that of Eqn. (8) are the same.
Eqns. (7) and (8) represent a tradeoff between minimizing the MSPBE error and preventing the
estimated value function from changing too drastically. Rather than simply minimizing the optimal
prediction from the projected Bellman equation, the agent makes use of the most recent update to
look for the solution. Figure 1 gives a schematic view of the effect of the regularization. Rather
than directly following the direction of the MSPBE gradient, the update chooses a W that minimizes
the MSPBE while following the constraint that the estimated value function should not change too
greatly. In effect, the regularization term encourages searching around the estimate at previous time
step, especially when the state space is large.
With these considerations in mind, the negative gradient of JGDD(W|Wn-1) is
— 1 Vw JGDD(WWn-1)
= - E[(γxn+1 - xn)xn>][E(xnxn>)]-1E(δnxn) - KE[(xn>Wn - xn>Wn-1)xn]
≈ - E[(γxn+1 - xn)xn>]ηn - KE[(xn>Wn - xn>Wn-1)xn].	(9)
Because the terms in Eqn. (9) can be directly sampled, the stochastic gradient descent updates are
given by
δn =rn + (γxn+1 - xn) Wn
ηn+1 =ηn +βn(δn - xn>ηn)xn
Wn+1 =Wn - Kn(xn>Wn - xn>Wn-1)xn - αn(γxn+1 - xn)(xn>ηn).	(10)
These update equations define the Gradient-DD method, in which the GTD2 update equations (6)
are generalized by including a second-order update term in the third update equation, where this
term originates from the squared bias term in the objective (7). In the following sections, we shall
analytically and numerically investigate the convergence and performance of Gradient-DD learning.
4 Improved convergence rate
In this section we analyze the convergence rate of Gradient-DD learning. Note that the second-order
update in the last line in Eqn. (10) can be rewritten as a system of first-order difference equations:
(I + Knxnxn>)(Wn+1 - Wn) =Knxnxn>(un+1 - un) - αn(γxn+1 - xn)(xn>ηn);
un+1 =Wn+1 - Wn.	(11)
4
Under review as a conference paper at ICLR 2021
Let βn = ζαn, ζ > 0. We consider constant step sizes in the updates, i.e., κn = κ and αn = α. De-
note Hn =	0	> and Gn
0 xn xn
√ζxnx>
-(xn - γxn+1)xn>
xn(xn -0γxn+1)	. We rewrite
the update rules of two iterations in Eqn. (11) as a single iteration in a combined parameter vector
with 2n components, Pn = (η>/√Z, w>)>, and a new reward-related vector with 2n components,
gn+1 = (rnxn> , 0> )>, as follows:
ρn+1 =Pn - KHn(Pn - Pn-I) + V^。(GnPn + gn+1),
(12)
Denoting ψn+1 = α-1(Pn+1 - Pn), Eqn. (12) is rewritten as
Pn+1 - Pn
ψn+1 - ψn
—	——	——	-∣ —1 Γ r， .∖^l
I + KHn	-KaHn	-√Z(GnPn - gn+l)
I -αI	ψn
-√ζ Gn	-KHn	] J Pn	+。	√ζgn+1
-√ζa-1Gn	-α-1(I + KHn) _| ψn	Λ√ζa^-1gn+1
(13)
where the second step is from
note Jn
-√ζ Gn
-√ζa-1Gn
I + KHn
I
-KαHn
-αI
-KHn
-α-1(I + KHn)
I	-KHn
α- I -α- (I + KHn)
. De-
. Eqn. (13) tells us that Jn is the update matrix of
the Gradient-DD algorithm. (Note that Gn is the update matrix of the GTD2 algorithm.) Therefore,
assuming the stochastic approximation in Eqn. (13) goes to the solution of an associated ordinary
differential equation (ODE) under some regularity conditions (a convergence property is provided in
the appendix by following Borkar & Meyn (2000)), we can analyze the improved convergence rate
of Gradient-DD learning by comparing the eigenvalues of the matrices E(Gn) denoted by G, and
E(Jn ) denoted by J (Atkinson et al., 2008). Obviously, J
-√ζ G
-√ζa-1G
where H = E(Hn). To simplify, we consider the case that the matrix E(xnxn>)
-KH
-α-1(I+KH)
= I.
α
α
Let λG be a real eigenvalue of the matrix √ζG. (Note that G is defined here with opposite sign
relative to G in Maei (2011).) From Maei (2011), the eigenvalues of the matrix -G are strictly
negative. In other words, λG > 0. Let λ be an eigenvalue of the matrix J, i.e. a solution to the
equation
∣λI - J| =(λ + λG)(λ + α-1) + Ka-1λ = λ2 + [a-1 (1 + k) + λg]λ + a-1λG =0.	(14)
The smaller eigenvalues λm of the pair solutions to Eqn. (14) are
λm < -λG,
where details of the above derivations are given in the appendix. This explains the enhanced speed
of convergence in Gradient-DD learning. We shall illustrate this enhanced speed of convergence in
numerical experiments in Section 5.
Additionally, we also show a convergence property of Gradient-DD under constant step sizes by
applying the ordinary differential equation method of stochastic approximation (Borkar & Meyn,
2000). Let the TD fixed point be w*, such that Vw* = ΠBVw*. Under some conditions, we
prove that, for any e > 0, there exists bi < ∞ such that limsupP(IlWn - w*k > e) ≤ bια.
n→∞
Details are provided in the appendix. For tapered step sizes, which would be necessary to obtain
an even stronger convergence proof, the analysis framework in Borkar & Meyn (2000) does not
apply into the Gradient-DD algorithm. Although theoretical investigation of the convergence under
tapered step sizes is a question to be studied, we find empirically in numerical experiments that the
algorithm does in fact converge with tapered step sizes and even obtains much better performance
in this case than with fixed step sizes.
5	Empirical S tudy
In this section, we assess the practical utility of the Gradient-DD method in numerical experiments.
To validate performance of Gradient-DD learning, we compare Gradient-DD learning with GTD2
5
Under review as a conference paper at ICLR 2021
learning, TDC learning (TD with gradient correction (Sutton et al., 2009b)), TD learning, and Em-
phatic TD learning (Sutton & Mahmood, 2016) in tabular representation using a random-walk task
and in linear representation using the Boyan-chain task. For each method and each task, we per-
formed a scan over the step sizes αn and the parameter κ so that the comprehensive performance of
the different algorithms can be compared. We considered two choices of step size sequence {αn}:
•	(Case 1) αn is constant, i.e., αn = α0.
•	(Case 2) The learning rate αn is tapered according to the schedule αn = α0 (103 +
1)/(103 + n).
We set the κ = cα0 where c = 1, 2, 4. Additionally, we also allow κ dependent on n and consider
Case 3: αn is tapered as in Case 2, but κn = cαn . In order to simplify presentation, the results of
Case 3 are reported in the Appendix. To begin, we set βn = αn , then later allow for βn = ζαn
under ζ ∈ {1/4, 1/2, 1, 2} in order to investigate the effect of the two-timescale approach of the
Gradient-based TD algorithms on Gradient-DD. In all cases, we set γ = 1.
5.1 Random walk task
As a first test of Gradient-DD learning, we conducted a simple random walk task (Sutton & Barto,
2018) with tabular representation of the value function. The random walk task has a linear arrange-
ment ofm states plus an absorbing terminal state at each end. Thus there are m+2 sequential states,
So, Si,…，Sm, Sm+ι, where m = 20, 50, or 100. Every walk begins in the center state. At each
step, the walk moves to a neighboring state, either to the right or to the left with equal probability.
If either edge state (S0 or Sm+1) is entered, the walk terminates. A walk’s outcome is defined to
be r = 0 at S0 and r = 1 at Sm+1. Our aim is to learn the value of each state V (s), where the
true values are (1,…，m)/(m + 1). In all cases the approximate value function is initialized to the
intermediate value V0(s) = 0.5. In order to investigate the effect of the initialization V0(s), we also
initialize V0(s) = 0, and report the results in Figure 7 of the Appendix, where its performance is
very similar as the initialization V0(s) = 0.5.
We first compare the methods by plotting the empirical RMS error from the final episode during
training as a function of step size α in Figure 2, where 5000 episodes are used. From the figure, we
can make several observations. (1) Emphatic TD works well but is sensitive to α. It prefers very
small α even in the tapering case, and this preference becomes strong as the state space becomes
large in size. (2) Gradient-DD works well and is robust to α, as is conventional TD learning. (3) TDC
performs similarly to the GTD2 method, but requires slightly larger α than GTD2. (4) Gradient-DD
performs similarly to conventional TD learning and better than the GTD2 method. This advantage
is consistent in different settings. (5) The range of α leading to effective learning for Gradient-DD
is roughly similar to that for GTD2.
9.0 寸.0 NO
1e-05	1e-03	1e-01
α
1e-05	1e-03	1e-01
α
」。」」①s≡α
1e-05	1e-03	1e-01
α
1e-05	1e-03	1e-01
α
(a)	Constant step size.
(b)	Tapering step size.
Figure 2:	Performance in the random walk task depends on step size. (a), Constant step size αn =
α0. (b), Tapering step size αn = α0(103 + 1)/(103 + n). In (a) and (b), state space size 10 (left)
or 20 (right). GDD(c) denotes the Gradient-DD with c. The curves are averaged over 20 runs, with
error bars denoting standard deviations across runs.
Next we look closely at the performance during training, which we show in Figure 3, where each
method and parameter setting was run for 5000 episodes. From the observations in Figure 2, in order
to facilitate comparison of these methods, we set α0 = 0.1 for 10 spaces, α0 = 0.2 for 20 spaces,
and α0 = 0.5 for 50 spaces. Because Emphatic TD requires the step size α to be especially small
6
Under review as a conference paper at ICLR 2021
as shown in Figure 2, the plotted values of α0 for Emphatic TD are tuned relative to the values used
in the algorithm defined in Sutton & Mahmood (2016), where the step sizes of Emphatic TD α(0ETD)
are chosen from {0.5%, 0.1%, 0.05%, 0.01%} by the smallest area under the performance curve.
Additionally we also tune α0 for TDC because TDC requires αn larger a little than GTD2 as shown
in Figure 2. The step sizes for TDC are set as α
by the smallest area under the performance curve.
(TDC)
n
aαn, where a is chosen from {1, 1.5, 2, 3}
From the results shown in Figure 3a, we draw several observations. (1) For all conditions tested,
Gradient-DD converges much more rapidly than GTD2 and TDC. The results indicate that Gradient-
DD even converges faster than TD learning in some cases, though it is not as fast in the beginning
episodes. (2) The advantage of Gradient-DD learning over other methods grows as the state space
increases in size. (3) Gradient-DD learning is robust to the choice of c, which controls the size κ
of the second-order update, as long as c is not too large. (Empirically c = 2 is a good choice.)
(4) Gradient-DD has consistent and good performance under both the constant step size setting and
under the tapered step size setting. In summary, compared with GTD2 learning and other methods,
Gradient-DD learning in this task leads to improved learning with good convergence.
Po`o
rorre SMR laciripm
0 1000	3000
episode
5000
PO`o
rorre SMR laciripm
5000
0 1000	3000
episode
PO`o
rorre SMR laciripm
0 1000	3000	5000
episode
PO`o
rorre SMR laciripm
(a) Constant step size αn = α0.
PO`o
rorre SMR laciripm
PO`o
rorre SMR laciripm
0 1000	3000	5000	0 1000	3000	5000	0 1000	3000	5000
episode	episode	episode
(b) Tapering step size αn = α0 (103 + 1)/(103 + n).
Figure 3:	Performance of Gradient-DD in the random walk task. From left to right in each subfigure:
the size of state space is 10 (α0 = 0.1), 20 (α0 = 0.2), 50 (α0 = 0.5). The curves are averaged over
20 runs, with error bars denoting standard deviations across runs.
In addition to investigating the effects of the learning rate, size of the state space, and magnitude
of the regularization parameter, we also investigated the effect of using distinct values for the two
learning rates, αn and βn. To do this, we set βn = ζαn with ζ ∈ {1/4, 1/2, 1, 2} and report
the results in Figure 8 of the appendix. The results show that comparably good performance of
Gradient-DD is obtained under these various βn settings.
5.2 B oyan-chain task
We next investigate Gradient-DD learning on the Boyan-chain problem, which is a standard task for
testing linear value-function approximation (Boyan, 2002). In this task we allow for 4p - 3 states,
with p = 20, each of which is represented by a p-dimensional feature vector. The p-dimensional
representation for every fourth state from the start is [1,0, ∙∙∙ , 0] for state si, [0,1,0, ∙∙∙ , 0] for s5,
7
Under review as a conference paper at ICLR 2021
…，and [0,0,…，0,1] for the terminal state s4p-3. The representations for the remaining states are
obtained by linearly interpolating between these. The optimal coefficients of the feature vector are
(-4(p - 1), -4(p - 2),…，0)/5. Simulations with P = 50 and 100 give similar results to those
from the random walk task, and hence are not shown here. In each state, except for the last one
before the end, there are two possible actions: move forward one step or move forward two steps
with equal probability 0.5. Both actions lead to reward -0.3. The last state before the end just has
one action of moving forward to the terminal with reward -0.2. As in the random-walk task, α0 used
in Emphatic TD is tuned from {0.5%, 0.2%, 0.1%, 0.05%}.
We report the results in Figure 4, which leads to conclusions similar to those already drawn from
Figure 3. (1) Gradient-DD has much faster convergence than GTD2 and TDC, and generally con-
verges to better values despite being somewhat slower than TD learning at the beginning episodes.
(2) Gradient-DD is competitive with Emphatic TD. The improvement over other methods grows as
the state space becomes larger. (3) As κ increases, the performance of Gradient-DD improves. Ad-
ditionally, the performance of Gradient-DD is robust to changes in κ as long as κ is not very large.
Empirically a good choice is to set κ = α or 2α. (4) Comparing the performance with constant step
size versus that with tapered step size, the Gradient-DD method performs better with tapered step
size than it does with constant step size.
S 9°Scm°
e SMR lacirip
0	20000	60000
episode
SIARd
0	5000	15000
episode
S 9°Scm°
e SMR lacirip
0	5000	15000
episode
(a) Constant step size αn = α0, where α0 = 0.05, 0.1, 0.2 from left to right.
S 9°Scm°
e SMR lacirip
0	20000	60000
episode
S 9°Scm°
e SMR lacirip
0	5000	15000
episode
S 9°Scm°
e SMR lacirip
0	5000	15000
episode
(b) Tapering step size αn
α0 (103 + 1)/(103 + n) , where α0
0.3, 0.5, 0.8 from left to right.
Figure 4:	Performance of Gradient-DD in the Boyan Chain task with 20 features. Note that the case
Gradient-DD(4), i.e. c = 4, is not shown when it does not converge.
5.3 Baird’s counterexample
We also verify the performance of Gradient-DD on Baird’s off-policy counterexample (Baird, 1995),
for which TD learning famously diverges. We consider three cases: 7-state, 100-state and 500-state.
Wesetα = 0.02 (but α = 10-5 for ETD), β = αandγ = 0.99. Wesetκ = 0.2 for GDD1, κ = 0.4
for GDD2 and K = 0.8 for GDD3. For the initial parameter values (1, ∙∙∙ , 1,10,1)>. We measure
the performance by the empirical RMS errors as function of sweep, and report the results in Figure
5. The figure demonstrates that Gradient-DD works as well on this well-known counterexample
as GTD2 does, and even works better than GTD2 for the 100-state case. We also observe that the
performance improvement of Gradient-DD increases as the state spaces increases. We also note that,
because the linear approximation leaves a residual error in the value estimation due to the projection
8
Under review as a conference paper at ICLR 2021
0	100	300	500
Sweep
0	100	300	500
Sweep
Figure 5: Bairds off-policy counterexample. From left to right: 7-state, 100-state, and 500-state.
TDC is not reported here due to its similarity to GTD2. We set α = 0.02 (but α = 10-5 for ETD),
β = α, and κ = 0.02c. GDD(c) denotes the Gradient-DD with c.
gL。Lg O
SMR laciripm
0	100	300	500
Sweep
error, the RMS errors in this task do not go to zero. Interestingly, Gradient-DD reduces this residual
error as the size of the state space increases.
6 Conclusion and discussion
In this work, we have proposed Gradient-DD learning, a new gradient descent-based TD learning
algorithm. The algorithm is based on a modification of the projected Bellman error objective func-
tion for value function approximation by introducing a second-order difference term. The algorithm
significantly improves upon existing methods for gradient-based TD learning, obtaining better con-
vergence performance than conventional linear TD learning.
Since GTD learning was originally proposed, the Gradient-TD family of algorithms has been ex-
tended for incorporating eligibility traces and learning optimal policies (Maei & Sutton, 2010; Geist
& Scherrer, 2014), as well as for application to neural networks (Maei, 2011). Additionally, many
variants of the vanilla Gradient-TD methods have been proposed, including HTD (Hackman, 2012)
and Proximal Gradient-TD (Liu et al., 2016). Because Gradient-DD just modifies the objective er-
ror of GTD2 by considering an additional squared-bias term, it may be extended and combined with
these other methods, potentially broadening its utility for more complicated tasks.
In this work we have focused on value function prediction in the two simple cases of tabular rep-
resentations and linear approximation. An especially interesting direction for future study will be
the application of Gradient-DD learning to tasks requiring more complex representations, including
neural network implementations. Such approaches are especially useful in cases where state spaces
are large, and indeed we have found in our results that Gradient-DD seems to confer the greatest
advantage over other methods in such cases. Intuitively, we expect that this is because the differ-
ence between the optimal update direction and that chosen by gradient descent becomes greater in
higher-dimensional spaces (cf. Fig. 1). This performance benefit in large state spaces suggests that
Gradient-DD may be of practical use for these more challenging cases.
References
K.	Atkinson, W. Han, and D. Stewart. Numerical Solution of Ordinary Differential Equations. JOHN
WILEY & SONS, 2008.
L.	C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Pro-
Ceedings of the 12 th International Conference on Machine Learning, pp. 30-37, 1995.
V.	S. Borkar and S.P. Meyn. The ODE method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.
Justin A. Boyan. Technical update: least-squares temporal difference learning. Machine Learning,
49:233-246, 2002.
S. S. Du, J. Chen, L. Li, L. Xiao, and D. Zhou. Stochastic variance reduction methods for policy
evaluation. In Proceedings of the 34 th International Conference on Machine Learning, 2017.
9
Under review as a conference paper at ICLR 2021
M. Geist and B. Scherrer. Off-policy learning with eligibility traces: A survey. Journal of Machine
Learning Research,15:289-333, 2014.
S. Ghiassian, A. Patterson, M. White, R.S. Sutton, and A. White. Online off-policy prediction.
arXiv:1811.02597, 2018.
S. Ghiassian, A. Patterson, S. Garg, D. Gupta, A. White, and M. White. Gradient temporal-
difference learning with regularized corrections. In International Conference on Machine Learn-
ing, 2020.
L. Hackman. Faster gradient-TD algorithms. Master’s thesis, University of Alberta, Edmonton,
2012.
B. Liu, S. Mahadevan, and J. Liu. Regularized off-policy TD-learning. In Advances in Neural
Information Processing Systems, 2012.
B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Finite-sample analysis of proximal
gradient TD algorithms. In Proceedings of the 31st International Conference on Uncertainty in
Artificial Intelligence, pp. 504-513, 2015.
B. Liu, J. Liu, M. Ghavamzadeh, S. Mahadevan, and M. Petrik. Proximal gradient temporal differ-
ence learning algorithms. In The 25th International Conference on Arti cial Intelligence (IJCAI-
16),, 2016.
H.R. Maei. Gradient temporal-difference learning algorithms. PhD thesis, University of Alberta,
Edmonton, 2011.
H.R. Maei and R.S. Sutton. GQ(λ): A general gradient algorithm for temporal-difference predic-
tion learning with eligibility traces. In Proceedings of the 3rd Conference on Artificial General
Intelligence, pp. 91-96, 2010.
A. R. Mahmood, H. van Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy
learning with linear function approximation. In Advances in Neural Information Processing Sys-
tems 27, 2014.
J. Peters, K. Mulling, and Y. AltUn. Relative entropy policy search. In AAAI Conference OnArtificial
Intelligence, 2010.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180-1190, 2008.
D. Precup, R. S. Sutton, and S. Singh. Eligibility traces for off-policy policy evaluation. In Proceed-
ings of the 17 th International Conference on Machine Learning, 2000.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning,
2015.
R. S. Sutton, Cs. Szepesvari, and H. R. Maei. A convergent O(n) algorithm for off-policy tempo-
ral difference learning with linear function approximation. In Advances in Neural Information
Processing Systems 21, 2009a.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
second edition edition, 2018.
R.S. Sutton and M. Mahmood, A.R. amd White. An emphatic approach to the problem of off-policy
temporal difference learning. Journal of Machine Learning Research, 17(73):1-20, 2016.
R.S. Sutton, H.R. Maei, D. Precup, S. Bhatnagar, D. Silver, Cs. Szepesvari, and E. Wiewiora. Fast
gradient-descent methods for temporal-difference learning with linear function approximation. In
Proceedings of the 26th International Conference on Machine Learning, 2009b.
A. White and M. White. Investigating practical linear temporal difference learning. In International
Conference on Autonomous Agents and Multi-Agent Systems, 2016.
10
Under review as a conference paper at ICLR 2021
Appendix
6.1 ON THE EQUIVALENCE OF EQNS. (7) & (8)
The Karush-Kuhn-Tucker conditions ofEqn. (8) are the following system of equations
(dW J (W) + K 急(IlVW - VWn-IkD - μ) = 0；
J K(IlVW - VWn-IkD - μ) = 0；
I llVW - VWn-IkD ≤ μ;
IK ≥ 0.
These equations are equivalent to
(焉 J(W) +
l 忌 J(W) =
Thus, for any μ > 0, there exists a
6.2 Eigenvalues OF J
K dW IIVW - VWn-IlID
if IIVW -
0 and K = 0, if IlVW -
Γl 1
WW
VV
;.
μμ
=<
2D2
K ≥ 0 such that dW J(W) + μdWkVW - VWn-JD = 0.
Let λ be an eigenvalue of the matrix J. We have that
IN—Jl= λI + √ζ G	KH
l J| =	√ζα-1G	λI + α-1(I + kH)
_ λI + √Z G	kH
=	-λα-1I	λI + QTI
_ λI + √Z G	kH
=	0 λI + QTI + κa-1λ(λI + √Z G)-1H
= ∣(λI + √ζ G)(λI + Q-1I)+ κɑ-1λH∣.
From the assumption E(Xnx>) = I and the definition of H, some eigenvalues of the matrix J, λ,
are solutions to
∣λI - J∣ =(λ + λg)(λ + Q 1) = 0;
and other eigenvalues of the matrix J, λ, are solutions to
∣λI - J∣ =(λ + λc)(λ + QT) + KQ-1λ
=λ2 + [q-1(1 + κ) + λg]λ + Q-1λG = 0.
Note λG > 0. the pair solutions to the equation above are
λ = - 2[q-1(1 + κ) + λg] ± 2√[q-1(1 + κ) + λg]2 - 4Q-1λG
=- 2[q 1 (1 + κ) + λg] ± 2√[q-1(1 + K) - λg]2 + 4Q-1λgκ.
Thus, the smaller eigenvalues of the pairs are
λm = - 2 [q 1(1 + κ) + λg] - 2 √[q-1(1 + K) - λg]2 + 4q-1 λgK
< - 1[q-1(1 + κ) + λg] - 2 √[q-1(1 + K) - λg]2,
where the inequality is from λσ > 0. When q-1(1 + k) - λσ > 0, then
λm < - 2∣Q-I(I + k) + λG] - $(Q-I(I + k) - λG)
= - Q-1(1 + K)
< - λG,
When q-1(1 + κ) - λ^ ≤ 0, then
λm < - 2∣Q-I(I + k) + λG] + 2(Q-I(I + k) - λG)
=- λc,
11
Under review as a conference paper at ICLR 2021
Convergence with constant step sizes
At last we apply the ODE method of stochastic approximation to obtain the convergence perfor-
mance.
Theorem 1 Consider the update rules (10) with constant step size sequences κ, α and β satisfying
K ≥ 0, β = Zα, Z > 0, α ∈ (0,1) and β > 0. Let the TD fixed point be w*, such that Vw* =
ΠBVw*. Suppose that (A1) (xn, rn, Xn+ι) is an i.i.d. Sequence with uniformly bounded second
moments, and (A2) E[(xn - γ xn+1)xn>] and E(xnxn>) are non-singular. Then for any > 0, there
exists b1 < ∞ such that
lim sup P (kwn - w* k > ) ≤ b1α.
n→∞
Proof From the constant step sizes in the conditions, we denote κn = κ and αn = α. Thus,
Eqn. (12) equals
(I + κHn)(ρn+1 - ρn) - κHn (ρn+1 - 2ρn + ρn-1)
=-V^a(GnPn - gn+1).	(A∙D
Denoting ψn+1 = α-1(ρn+1 - ρn), Eqn. (A.1) is rewritten as
ρn+1 - ρn
ψn+1 - ψn
α
α
—	——	——	-∣ — 1 Γ r-r /	、
I + KHn	-KaHn	— √Z(GnPn - gn+l)
I	-αI	ψn
-√ζ Gn	-KHn	] ∣" ρn
-√ζa-1 Gn	-a-1(I + KHn)	ψn
+ a I- V?gn+1
√Za-1gn+ι
(A.2)
where the second step is from
I + KHn
I
-KaHn
-aI
I
a-1I
-KHn
-a-1(I + KHn)
Denoting G = E(Gn), g = E(gn) and H = E(Hn), then the TD fixed point of Eqn. (A.1) is given
by
-GP+g=0	(A.3)
We apply the ordinary differential equation approach of the stochastic approximation in Theorem
1 (Theorem 2.3 of (Borkar & Meyn, 2000)) into Eqn. (A.2). Note that (Sutton et al., 2009a) and
(Sutton et al., 2009b) also applied Theorem 2.3 of (Borkar & Meyn, 2000) in using the gradient-
descent method for temporal-difference learning to obtain their convergence results. For simplifying
notation, denote
-√ζ Gn	-KHn
-√ζα 1Gn —a 1 (I + KHn)
-√ζ G	-kH	一
-√Za-1G -a-1(I + kH)
yn
Pn
ψn
√Zgn+1
√Za-1gn+1
, and h
√Zg
√Za-1g
. Eqn. (A.2) is rewritten as
yn+1 = yn +a(f(yn) +h+Mn+1),	(A.4)
where f(yn) = Jyn and Mn+1 = (Jn - J)yn + hn - h.
Now we verify the conditions (c1-c4) of Lemma 1. Firstly, Condition (c1) is satisfied under the
assumption of constant step sizes. Secondly, f(y) is Lipschitz and f∞(y) = Gy. Following Sutton
et al. (2009a), the Assumption A2 implies the real parts of all the eigenvalues of G are positive.
Therefore, Condition (c2) is satisfied.
12
Under review as a conference paper at ICLR 2021
Because E(Mn+1 |Fn) = 0andE(kMn+1k2|Fn) ≤ c0(1+kynk2), where Fn = σ(yi, Mi, i ≤ n),
is a martingale difference sequence, we have that
kMn+1k2 ≤ 2(kJn - Jk2kynk2 + khn - hk2).	(A.5)
From the assumption A1, Eqn. (A.5) follows that there are constants cj and ch such that
E(kJn - Jk2|Fn) ≤ cj;
E(khn+1 - hk2) ≤ ch.
Thus, Condition (c3) is satisfied.
Finally, Condition (c4) is satisfied by noting that y* = G-1g is the unique globally asymptotically
stable equilibrium.	■
Theorem 1 bounds the estimation error of w in probability. Note that the convergence of Gradient-
DD learning provided in Theorem 1is a somewhat weaker result than the statement that Wn → W
with probability 1 as n → ∞. The technical reason for this is the condition on step sizes. In
Theorem 1, we consider the case of constant step sizes, with αn = α and κn = κ. This restriction
is imposed so that Eqn. (12) can be written as a system of first-order difference equations, which
cannot be done rigorously when step sizes are tapered as in (Sutton et al., 2009b). As shown below,
however, we find empirically in numerical experiments that the algorithm does in fact converge with
tapered step sizes and even obtains much better performance in this case than with fixed step sizes.
An ODE result on stochastic approximation
We introduce an ODE result on stochastic approximation in the following lemma, then prove Theo-
rem 1 by applying this result.
Lemma 1 (Theorem 2.3 of Borkar & Meyn (2000)) Consider the stochastic approximation algo-
rithm described by the d-dimensional recursion
yn+1 = yn + an [f(yn) + Mn+1].
Suppose the following conditions hold: (c1) The sequence {αn} satisfies for some constant 0 <
α < α < 1, α<αn < α; (c2) Thefunction f is Lipschitz, and there exists a function f∞ such that
limr→∞ fr (y) = f∞ (y), where the scaled function fr : Rd → Rd is given by fr (y) = f (ry)/r.
Furthermore, the ODE y = f∞(y) has the origin as a globally asymptotically stable equilibrium;
(c3) The sequence {Mn , Fn}, with Fn = σ(yi , Mi, i ≤ n), is a martingale difference sequence.
Moreover, for some c0 < ∞ and any initial condition y0, E(kMn+1 k2|Fn) ≤ c0(1 + kyn k2). (c4)
The ODE
y⑴=f (y⑴)
has a unique globally asymptotically stable equilibrium y*. Thenfor any E > 0, there exists bi < ∞
such that limsupP(kyn 一 y* k > e) ≤ bιa.
n→∞
6.3 Additional empirical results
13
Under review as a conference paper at ICLR 2021
PO`o
rorre SMR laciripm
5000
0 1000	3000
episode
Po
■0
rorre SMR laciripm
0 1000	3000	5000
episode
PO`o
rorre SMR laciripm
0 1000	3000	5000
episode
Figure 6:	Performance of Gradient-DD in the random walk task under Case 3. Tapering step size
αn = α0(103 + 1)/(103 + n) and κ is allowed to be dependent on n: κn = cαn. From left to right
in each subfigure: the size of state space is 10 (α0 = 0.1), 20 (α0 = 0.2), 50 (α0 = 0.5). Results
are averaged over 20 runs, with error bars denoting standard deviations across runs.
0 1000	3000	5000
episode
Figure 7:	Performance of Gradient-DD in the random walk task when the initial values are set 0.
The size of state space is 20, with tapering step size αn = 0.2(103 + 1)/(103 + n). Results are
averaged over 20 runs, with error bars denoting standard deviations across runs.
寸°70 0.0
rorre SMR laciripmE
GTD2
TDC
GDD(1)
GDD(2)
GDD(4)
寸°NO
rorre SMR laciripmE
寸.0 NO 0.0
rorre SMR laciripmE
1000	3000	5000
episode
0 1000	3000	5000
episode
1000	3000	5000
episode
寸°NO
rorre SMR laciripmE
0 1000	3000	5000
episode
Figure 8:	Performance under various βn in the random walk task with 20 states. αn = α0(103 +
1)/(103 + n) with α0 = 0.2. From left to right in each subfigure: the size of state space is βn =
an∕4, βn = a.n/2, βn = α. and βn = 2αn. Results are averaged over 20 runs, with error bars
denoting standard deviations across runs.
14
Under review as a conference paper at ICLR 2021
二cq0 9.0 PO-o
orre SMR lacirip
0 20000	60000
episode
二cq0 9.0 PO-o
orre SMR lacirip
0	5000	15000
episode
二cq0 9.0 PO-o
orre SMR lacirip
0	5000	15000
episode
Figure 9:	Performance of Gradient-DD in the Boyan Chain task with 20-features under Case 3.
Tapering step size an = αo(103 + 1)/(103+n), where α° = 0.3,0.5,0.8,, where α° = 0.3,0.5,0.8
from left to right, and κ is allowed to be dependent on n: κn = cαn Note that the case GDD(4), i.e.
c = 4, is not shown when it does not converge.
15