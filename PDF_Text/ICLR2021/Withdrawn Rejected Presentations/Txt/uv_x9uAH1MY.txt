Under review as a conference paper at ICLR 2021
Rethinking Graph Neural Networks
for Graph Coloring
Anonymous authors
Paper under double-blind review
Ab stract
The development of graph neural networks (GNNs) stimulated the interest in
GNNs for NP-hard problems, while most works apply GNNs for NP-hard prob-
lems by empirical intuition and experimental trials and improve the results with
the help of some heuristic designs. We start from a simple contradiction that a
graph coloring problem requires two connected and symmetric nodes to be as-
signed different colors while an expressively powerful GNN always maps the two
nodes to the same node embeddings. To characterize the power of GNNs for the
graph coloring problem, we first formalize the discrimination power of GNNs as
the capability to assign nodes different colors. We then study a popular class of
GNNs, called AC-GNN, and identify the node pairs that AC-GNNs fail to discrim-
inate and provide corresponding solutions to discriminate them. We show that any
AC-GNN is a local coloring method and any local coloring method is non-optimal.
Moreover, we discuss the color equivalence of graphs and give a condition for the
color equivalent AC-GNN theoretically. Following the approaches which prove
to enhance the discriminative power, we develop a simple architecture for the
graph coloring problem without heuristic pre-processing and post-processing pro-
cedures. We empirically validate our theoretical findings and demonstrate that
our model is discriminatively powerful and even comparable with state-of-the-art
heuristic algorithms.
1	Introduction
Graph neural networks (GNNs) have shown overwhelming success in various fields, such as
molecules, social networks and web pages, by learning the representation of graph structured data
(Hamilton et al., 2017b). The main idea behind GNNs is a neighborhood aggregation scheme (or
called message passing), where each node aggregates feature vectors from its neighbors and com-
bines them with its own feature vector producing a new one. A GNN following such scheme is
also called aggregation-combination GNN (AC-GNN) (Barcelo et al., 2019). After finite iterations
of such aggregation and combination, the corresponding feature vector of each node is called node
embedding to represent the node.
The development of GNNs stimulated the interest in GNNs for NP-hard problems (Li et al., 2018;
Lemos et al., 2019; Huang et al., 2019), e.g. graph coloring problem. Although most works apply
GNNs with the help of empirical intuition and some heuristic designs, some recent works (Loukas,
2019; Barcelo et al., 2019; XU et al., 2018) explore the theoretical properties of the general GNNs.
These theoretical studies help to understand the GNNs and investigate their properties, but unfor-
tUnately they do not pay special attention to the application for the NP-hard problems and some
objectives are even contrary thUs resUlt in poor resUlts in some cases. One example is shown in
FigUre 1(a), where an expressively powerfUl GNN maps the symmetric nodes c and d to the same
embedding while they are expected to be colored differently in the graph coloring problem.
In this paper, we stUdy the capability of AC-GNNs especially for the graph coloring problem throUgh
a theoretical framework. Generally, we aim to answer qUestions of whether an AC-GNN can be an
optimal coloring method for all graphs, when a GNN cannot be optimal and what properties a
GNN shoUld possess to avoid non-optimal cases. To answer these qUestions, we first formalize the
discrimination power of an AC-GNN as the capability to assign nodes different colors. A coloring
method can be then decided as non-optimal when it fails to discriminate some distinct node pairs
in which the node colors are different in all optimal solUtions. We then prove that AC-GNN is not
1
Under review as a conference paper at ICLR 2021
Figure 1: Examples of graph structures in which AC-GNNs fail to discriminate the node pair {c, d}.
(a) left: the input graph with the same node attribute; right: the coloring results by the most powerful
AC-GNN. (b) left: the input graph with different node attributes (represented by the gray scale);
right: the coloring results by the most powerful integrated AC-GNN.
optimal if the attributes are the same among nodes or the aggregation and combination steps are
integrated. Furthermore, We follow the study of locality in AC-GNN (Barcelo et al.,2019; XU et al.,
2018) to show that AC-GNN is a local coloring method and derive a lower bound of the chromatic
number of any local coloring method, which proves to be strictly larger than the optimal one and
thus confirms the non-optimality of AC-GNN for the graph coloring problem. Besides the analysis
of the optimality, we also investigate the color equivalence of graphs and give a condition for the
color equivalent AC-GNN. Considering all propositions above, we propose a simple variation of
AC-GNN, Graph Discrimination Network (GDN). Also, we provide a simple version of GDN by
assigning suitable values to the variables to deepen the network.
Overall, in this work, (1) we identify the node pairs that cannot be discriminated by AC-GNN and
provide solutions to discriminate them; (2) we show that any AC-GNN is a local coloring method
and any local coloring method is non-optimal, and thus prove the non-optimality of AC-GNN; (3)
we discuss the color equivalence of graphs and gives a condition for the color equivalent AC-GNN;
(4) we propose a simple variation of AC-GNN, graph discrimination network (GDN) which avoids
the non-optimal cases discussed.
2	Preliminaries
We leave the basic notations and graph terminology in Appendix A.
Graph coloring. The graph coloring problem is to assign colors to the vertices of a graph, such
that the color assigned to each vertex is as different as possible from its neighbors. To be more
specific, let G = (V, E) represent the input graph and each vertex v ∈ V is associated with an
attribute xv, a function fk : (v, G, xv) → {1, ..., k} for graph coloring returns a color of v indexed
by colv ∈ {1, ..., k}. Given a graph G colored by f, a conflict function c : (u, v, f) → {0, 1} is used
to measure the performance of f on G. Specifically, c(u, v, f) = 1 when u and v are connected and
assigned the same color:
c(u, v, f) = 01,,
if f(v,G,xv) = f(u,G,xu) and {u, v} ∈ E;
otherwise.
(1)
The node pair {u, v} is called a conflict if c(u, v, f) = 1. The objective of the graph coloring prob-
lem is widely formulated in two ways: 1) Given an available color number constraint k, minimize
the number of conflicts as in Equation (2); 2) Given a conflict constraint cmax, minimize the number
of used colors as in Equation (3).
min	c(u, v, fk).	(2) min k, s.t. X c(u, v, fk) ≤ cmax.	(3)
{u,v}∈E	{u,v}∈E
When we set cmax as 0, i.e., no conflict is introduced by fk, we call the obtained minimum color
number as the chromatic number of G . In this paper, we focus on the k-coloring problem, in which
k is the chromatic number of G. In the following pages, we say a coloring function is optimal if it
colors the graph without conflict in the k-coloring problem.
Graph neural networks. GNNs are to learn the embedding of a node or the entire graph based
on the graph G = (V, E) and node features {xv : v ∈ V}. We follow the same notations in a
2
Under review as a conference paper at ICLR 2021
previous work Barcelo et al. (2019) to formally define the basics for GNNs. Let {AGG(i)}L=ι and
{COM(i)}iL=1 be two sets of aggregation and combination functions. An aggregation-combine GNN
(AC-GNN) computes the feature vectors h(vi) for every node v ∈ V by:
h(vi) = COM(i)(h(vi-1),AGG(i)({h(ui-1) : u ∈ N(v)})),	(4)
where N(v) denotes the neighborhood of v, i.e., N (v) = {u : {u, v} ∈ E} and h(v0) is the
node attribute Xv. Finally, each node V is classified by a node classification CLS(∙) applied to
the node embedding h(vL) . When the AC-GNN is used as the coloring function for the graph col-
oring problem, CLS(∙) returns a Colv ∈ {1,…，k}. Then, an AC-GnN A with L layers is defined
as A = ({AGG(i)}L=ι, {COM(i)}L=ι, CLS(∙)). Here, We define A(v, G, Xv) as the color of V
assigned by A.
The properties of aggregation, combination and classification functions are widely studied in recent
years (Barcelo et al., 2019; XU et al., 2018; Li et al., 2020; Morris et al., 2019; KiPf & Welling, 2016)
and many variations of these functions are proposed. Among various function architectures, we say
an AC-GNN is simple as in Barcelo et al. (2019) if the aggregation and combination functions are
defined as follows:
AGG(i) (X) =	X,	(5)
x∈X
COM(i) (X, y) = σ(XC(i) + yA(i) +b(i)),	(6)
where C(i), A(i) and b(i) are trainable parameters, σ is an activation function.
The aggregation and combination functions can also be integrated such as the networks studied in
Kipf & Welling (2016); Loukas (2019). We say that such AC-GNN is integrated and the aggregation
and combination functions are integrated as follows:
h(ui) = COM(i)(AGG(i)({h(wi-1) : w ∈ N(u) ∪ {u}})).	(7) 3
3 Connection between GNNs and graph coloring
GNNs have been applied to solve a set of graph-related problems, and a few trials have been made
for the graph coloring problem. However, GNNs are designed in a local aggregation scheme, which
has a totally different objective from the graph coloring context, i.e., GNNs often map connected
nodes into similar node embeddings while the graph coloring function assigns the connected nodes
to different colors. In this section, we theoretically study the connection between GNNs and the
graph coloring problem through the discussions and proofs of several distinct characteristics. We
first formalize the power of GNNs for the graph coloring problem. We then exploit the locality of
AC-GNN and give conditions on how the power of AC-GNN can be enhanced for the graph coloring
problem. Furthermore, we study the color equivalence of the graph coloring problem and, in reverse,
seek the ways to make the AC-GNN color equivalent.
3.1 Discrimination power
Recent works (XU et al., 2018; Barcelo et al., 2019; Morris et al., 2019) study the expressive power
of a GNN by analyzing when a GNN maps two nodes to the same node embedding. In the study
of the expressive power, a GNN is called maximally powerful if it maps two nodes to the same
node embedding only if they are local equivalent. Nevertheless, the analysis of the expressiveness
power is not suitable for the graph coloring problem. On the contrary, a GNN with the maximum
expressiveness power may harm the results of graph coloring: Two local equivalent nodes are often
assigned different colors in the optimal solution. To stress the distinction and study the power of a
GNN for the graph coloring problem, we introduce the discrimination power of GNNs. Ideally, a
GNN with a strong discrimination power maps two connected nodes to node embeddings as different
as possible. Formally, we define that a coloring method f discriminates a node pair (u, V) as follows:
Definition 1 (discriminate). A coloring method f discriminates a node pair (u, V) if the color of
each node in the pair assigned by f is different, i.e., f (u, G, Xu) 6= f(V, G, Xv ).
Definition 2 (distinct node pair). A node pair (u, V) is a distinct node pair if the colors of u and V
are different in any optimal solution.
3
Under review as a conference paper at ICLR 2021
According to the definitions above, we can study the discrimination power by analyzing the distinct
node pair, i.e., a GNN with stronger discrimination power discriminates more distinct node pairs.
Clearly, a connected node pair is also distinct and an optimal coloring method always discriminates
the distinct node pair. One may try to build an optimal AC-GNN which colors all graphs without
conflict; however, the following Proposition 1 refutes the existence of such a “perfect” AC-GNN:
Proposition 1. All AC-GNNs cannot discriminate any equivalent node pair.
It is not difficult to see that an equivalent node pair can be connected, which makes the node pair
distinct. The node pair {c, d} in Figure 1(a) is one example of such node pairs. Hence, AC-GNN is
not optimal for any graph that contains these node pairs, i.e., connected and also equivalent.
Many previous works use a constant value or degree value as the node attribute, which makes any
topologically equivalent node pair also equivalent. To enhance the discrimination power of an AC-
GNN, one may impose more difference for each node like a random feature (Sato et al., 2020) or
one-hot vector (Barcelo et al., 2019).
Actually, this distinct node attribute solution also fits the conclusion in Loukas (2019), which proves
that with discriminative attributes GNNs become significantly more powerful. Indeed, the handling
of node attribute strengthens the AC-GNN by eliminating the equivalent node pairs (the topological
equivalence preserves). However, the superficial methods on the node attributes cannot influence and
solve the underlying defects of specific AC-GNNs, for example, the integrated AC-GNN. Intuitively,
integrated AC-GNN is reasonable in the clustering tasks, while it may not fit for the graph coloring
problem due to its aggregation scheme. Considering that an integrated AC-GNN treats the features
of each node and features of its neighbors equally, i.e., all of them are aggregated into the same
multi-set, the integrated AC-GNN is more difficult to discriminate the node against its neighbors.
Proposition 2 points out that an integrated AC-GNN cannot be optimal since there always exists a
graph in which at least one distinct node pair is not discriminated by the integrated AC-GNN.
Proposition 2. There is a graph in which at least one distinct node pair is not discriminated by any
integrated AC-GNN.
3.2	Locality
With a random feature vector and separated aggregation and combination functions, one may ask
whether it is possible to build an optimal AC-GNN. To answer the question, we first introduce the
definition of locality in the graph coloring problem:
Definition 3 (local method). A coloring method f is r-local if it fails to discriminate any r-local
equivalent node pair. A coloring method f is local if f is r-local for at least one positive integer r.
Local methods are widely used in combinatorial optimization such as maximum independent set
(MIS) and graph coloring. Along with the study for the local methods, the upper bound of the
power of a local method for the MIS problem are investigated. For example, Gamarnik & Sudan
(2014) gives an upper bound 1/2 + 1∕(2√2) of the size of a MIS produced by any local method
in the random d-regular graph as d → ∞ and Rahman et al. (2017) enhances the bound to 1/2. A
random d-regular graph is a graph with n nodes and the nodes in each node pair are connected with
a probability d/n. Starting from the upper bound of a local method for the MIS problem, Corollary
1 states the non-optimality of a local method for the graph coloring problem:
Corollary 1. A local coloring method is non-optimal in the random d-regular tree as d → ∞.
Due to the localized nature of the aggregation function in GNNs, an AC-GNN with a fixed number of
layers, say L layers, cannot see the structure or information of nodes at a distance further than L, i.e.,
an AC-GNN maps each node u to the node embedding using the information of the subtree T(u, L)
instead of the global graph structure. Following the non-optimality ofa local coloring method stated
in Corollary 1 and the localized nature of GNNs, we can reduce our analysis of whether there exists
an optimal AC-GNN for graph coloring to the question whether an AC-GNN is a local coloring
method. Corollary 2 answers the latter question as yes:
Corollary 2. AC-GNN is a local coloring method.
Corollary 1 and Corollary 2 direct give the following theorem:
Theorem 1. AC-GNN is not optimal.
4
Under review as a conference paper at ICLR 2021
To obtain a more discriminatively powerful GNN for the graph coloring problem, the locality of
AC-GNN should be broken, i.e., the AC-GNN should be able to discriminate r-local equivalent node
pairs. In fact, the locality constraint of AC-GNNs has been widely investigated in previous works
and corresponding global GNNs (or at least the GNNs that break the local equality) are proposed
(Barcelo et al., 2019; Li et al., 2020; You et al., 2019; Pei et al., 2020; Rong et al., 2019). Among
these explorations, global features can be introduced to make the whole graph structure visible to
the GNN. Nevertheless, these variations of AC-GNN are still local for the graph coloring problem
if the global features applied also fail to discriminate the local equivalent node pairs. For example,
(Barcelo et al., 2019) uses a readout operation that assigns each node a global feature calculated
by the summation of all node features. The summation feature, however, keeps the same for all
nodes and thus fails to help discriminate the local equivalent node pair. On the contrary, some
methods succeed in distinguishing the local equivalent node pair without a global mechanism such
as DropEdge (Rong et al., 2019) which drops edge randomly in each layer and thus breaks the local
equality.
3.3	Color equivariance
Equivariance is an important property for a function if it is defined on sets that are equivariant to the
permutation of the set elements. Many works studied the equivariance of a neural network (Zaheer
et al., 2017; Qi et al., 2017). For the network on a graph, the node equivariance in a GNN for the
node embeddings are studied (Maron et al., 2018; Xu et al., 2018), that is, when the input node set
is permuted, the GNN should produce node embeddings with the same permutation.
When we adopt GNN for the graph coloring problem, we should consider not only the order equiv-
ariance, but also the color equivariance when the node attribute of each node is the probability
distribution of colors as in Braunstein et al. (2006); Lemos et al. (2019). One may argue that colors
are treated equally in the graph coloring problem, i.e., the solution of an optimal coloring function
is already optimal with no need to follow the same permutation on the input coloring distribution.
In fact, this claim holds for the pure graph coloring problem but may not fit for the practical ap-
plication. For example, in the layout decomposition problem (Jiang & Chang, 2017), each color
represents a mask and some features (nodes) are pre-assigned to some specific masks, which makes
the color equivariance necessary for the coloring function. To investigate the type of functions to be
color equivariant, we first formalize the definition of an equivariant function:
Definition 4 (Maron et al. (2018); Zaheer et al. (2017)). A function f : Rk → Rk is equivariant if
f (h)P = f(hP) for any permutation matrix P ∈ Rk×k and feature vector h ∈ Rk.
Similarly, in the graph coloring problem, we say a function is color equivariant if it is equivariant
when the node attribute is the probability distribution of colors. A GNN A with L layers is color
equivalent if and only if all functions in {AGG(i), COM(i) : i ∈ 1, ..., L} are color equivalent. Then,
the following theorem states the sufficient and necessary conditions for A to be color equivalent:
Theorem 2. Let A be a simple AC-GNN and both input and output be the probability distribution
h ∈ Rk of k colors, A is color equivariant if and only if the following conditions hold:
•	For any layer i, all the off-diagonal elements of C(i) are tied together and all the diagonal
elements are equal as well. That is,
C(i) = λ(Ci)I + γC(i)(11) λ(Ci),γC(i) ∈R 1= [1,..., 1]> ∈Rk.	(8)
•	For any layer i, all the off-diagonal elements of A(i) are also tied together and all the
diagonal elements are equal as well. That is,
A(i) = λ(Ai)I + γA(i)(11) λ(Ai),γA(i) ∈R 1= [1,..., 1]> ∈Rk.	(9)
•	For any layer i, all elements in b(i) are equal. That is,
b(i) = β(i)1 β(i) ∈ R 1 = [1,..., 1]> ∈ Rk.	(10)
The theorem above is actually an extension of (Zaheer et al. (2017), Lemma 3) from a standard
neural network layer f = (Θx) to a simple AC-GNN. Similarly, when λ(Ci) , γC(i) , λ(Ai) , γA(i) are
the matrices, the result above can be also easily extended to a higher dimension, i.e., the node
attribute is not a probability distribution h ∈ Rk but a feature with higher arbitrary dimensional
h ∈ Rd : d > k.
5
Under review as a conference paper at ICLR 2021
4 Our Method
Having developed the conditions for a GNN which is less discriminative powerful and not color
equivalent, we summarize a series of rules that make a GNN A color equivalent and improve its
discrimination power as follows: (1) The input graph contains no equivalent node pair (Proposition
1); (2) A does not integrate the aggregation and combination function (Proposition 2); (3) A is able
to discriminate the local equivalent node pairs (Theorem 1); (4) A is color equivalent. If A follows
the form of a simple AC-GNN, it should satisfy the conditions in Theorem 2.
We then propose a simple architecture, Graph Discrimination Network (GDN), which satisfies all the
rules above and follows the form of the simple AC-GNN. We describe GDN for the graph coloring
problem as follows:
Forward Computation For a k-coloring problem, given a graph G = {V, E}, each node v ∈ V is
assigned an discriminative attribute xv ∈ Rk . xv is normalized to represent the probability distri-
bution of k colors and is initialized randomly to eliminate the equivalent node pairs as in Braunstein
et al. (2006); Lemos et al. (2019).
In the aggregation function, let m(vi) ∈ Rk be the result returned by AGG(i) for the node v in the
i-th layer, the aggregation layer is organized as follows:
m(vi) =	h(ui-1) ,
u∈N0 (v)
(11)
whereN0(v) is defined as the subset of N (v), in which only the nodes with degree larger or equal to
k are preserved with the probability p. A degree constraint follows the fact that a node with degree
less than k is always colorable without conflict. We also use a random dropout mechanism proposed
in Rong et al. (2019) so that GDN can discriminate the local equivalent node pair by removing
edges randomly in each layer. Meanwhile, the global graph structure is still invisible to our model.
It would be interesting to characterize how a global feature can be applied in the discrimination of
the local equivalent node pair. We leave the question for future work.
In the combination function, we define the COM(i) following Theorem 2 to make GDN color equiv-
alent:
h(vi) =σ(h(vi-1)λ(Ci) +h(vi-1)γC(i)(11)+m(vi)λ(Ai) + m(vi)γA(i)(11) + β(i)1).	(12)
Generally, there exist many other GNNs which satisfy the rules above and GDN is only one possible
example.
Loss Function Considering that an optimal color solution is still optimal after the permutation of
colors, i.e., there exist many different optimal solutions for each graph, itis not an easy job to develop
a supervised training scheme. Li et al. (2018) uses supervised learning followed by a heuristic tree
search to alleviate the multi-solution issue in the MIS problem. However, the complexity of the tree
search explodes when the number of colors k and the number of nodes n become large. Here, we use
a self-supervised loss which mimics the cost of the graph coloring problem specified in Equation (2).
The objective function is motivated by the intuition that the product of probability distributions of
connected nodes should be minimized and formulated by:
minhu ∙ h ∙	(13)
{u,v}∈E
where hu is the probability distribution obtained by A when hu ∈ Rk. If the dimension is not k, hu
is normalized to measure the similarity.
Dealing with Depth Here GDN is still based on a neighbor aggregation-based scheme, that is,
the structural information is constrained by the depth of GDN , i.e., the number of layers (Loukas,
2019; Xu et al., 2018). We set the depth of GDN following the Proposition proven by Loukas (2019);
Abboud et al. (2016), which gives the lower bound of the depth for a GNN to be optimal in the graph
coloring problem:
Proposition 3 (Loukas (2019); Abboud et al. (2016)). There exists a graph G on which every AC-
GNN of width W = O(1) requires depth at lest d = Ω(n2 / log2 n) to solve the coloring problem
optimally.
6
Under review as a conference paper at ICLR 2021
Graph	Nodes	Edges	GNN-GCP		Tabucol		Ours	
			Cost	Time(s)	Cost	Time(s)	Cost	Time(s)
Cora	2708	5429	1291	3.90	31	15410	3~	4.80
Citeseer	3327	4732	1733	2.74	6	44700	3	4.88
Pubmed	19717	44338	4393	4.50	NA	>24h	35	21.71
Power Law Tree	7856	7756	4519	10.47	33	4417	465	36.76
Small World	7856	29716	5563	7.56	64	2021	235	171.5
Holme and Kim	7856	15712	8443	7.99	87	5317	506	48.46
Layout	641202	787242	386009	3896	2392	82301	4626	8474
Table 1: Results of citation datasets, random dataset and the layout dataset by different coloring
methods.
Here n is the number of nodes in G and w is the dimension of the features. According to the
lower bound of the depth d = Ω(n2/ log2 n), the required number of layers explodes as the graph
goes larger. However, deep AC-GNN suffers from the severe vanishing gradient problem (Li et al.,
2019). To solve this problem, we propose a simpler version of GDN : we assign constant values
to λ(Ci) , γC(i) , λ(Ai) , γA(i) and β(i), which makes GDN free of training and thus free of the vanishing
gradient problem. The values are assigned as follows:
λC) = 1,γC) = 0,λA) = -δ,γ(A) = 0,β(i) = δk	(14)
where δ is the manually-defined step rate and k is the number of available colors to ensure that
the summation of h(vi+1) remains 1. We then prove that such value assignment, at least, always
decreases the cost (although it may not be optimal):
Proposition 4. The cost monotonically decreases under specific parameters λ(Ci) = 1, γC(i) =
0, λ(Ai) = -δ, γA(i) = 0.
There are no trainable parameters in the simpler version, which avoids the time-consuming training
procedures but may not be the best assignment scheme for these parameters. An interesting direc-
tion for future work is to seek the solution for the vanishing gradient problem and keep the model
trainable.
5	Experimental Results
Datasets We test our models and baselines on four datasets: (1) The Layout dataset from the layout
decomposition problem (Jiang & Chang, 2017); (2) The citation datasets (Cora, Citeseer, Pubmed)
(Sen et al., 2008) that are widely used in node classification tasks, and we here regard them as
the coloring scenario for large but sparse graphs, hence dismissing their original node attributes;
(3) Three random graph distributions namely: random power-law tree, Watts-Strogatz small-world
and Holme and Kim model (Watts & Strogatz, 1998; Holme & Kim, 2002); (4) COLOR dataset1
containing small and medium sized graphs.
Baselines We compare our models with two previous works which focus on the graph coloring
problem and four state-of-the-art variants of AC-GNNs: (1) GNN-GCP (Lemos et al., 2019), comb-
ing GNN, RNN, and MLP to obtain the node embedding and using a k-means method to color the
node; (2) Tabucol (Hertz & de Werra, 1987), a well-known heuristic algorithm using Tabu search.
We also compare different variants of AC-GNN in previous works: GCN (Kipf & Welling, 2016),
GAN (Velickovic et al., 2017), GIN (XU et al., 2018) and GraPhSAGE (Hamilton et al., 2017a).
More details on the datasets and model configurations can be found in Appendix H.
5.1	Comparison with other AC-GNN variations
The comparison with other trainable AC-GNN variations is conducted on the layout dataset. Specif-
ically, we only evaluate the AC-GNN based models on the layout dataset since other datasets are
composed of either (1) a single graph, or (2) graphs with varying chromatic numbers, which are not
1https://mat.tepper.cmu.edu/COLOR02/
7
Under review as a conference paper at ICLR 2021
Graph	k	GNN-GCP CoSt Time		Tabucol Cost Time		Ours Cost Time	
jean	10	76	0.06	0	0.95	0	3.77
anna	11	87	0.08	0	3.23	1	5.61
huck	11	117	0.05	0	0.15	0	4.17
david	11	NA	NA	0	4.83	1	5.27
homer	13	1628	1.09	0	274	1	12.60
myciel5	6	35	0.04	0	0.20	0	1.57
myciel6	7	94	4.33	0	0.79	0	3.12
games120	9	301	0.07	0	0.93	0	7.28
Mug88」	4	146	0.33	0	0.12	0	0.39
I-InSertionS_4	5	42	0.05	0	0.16	0	0.82
2-Insertions_4	4	360	0.09	1	255	1	0.75
QUeen5 _5	5	37	0.03	0	0.13	0	0.68
QUeen6 _6	7	290	0.38	0	4.93	4	0.87
QUeen7 _7	7	126	0.04	10	36.9	15	1.03
QUeen8 _8	9	188	0.05	8	61.3	7	1.31
QUeen9 _9	10	296	0.07	5	97.8	13	1.99
QUeen8_12	12	260	0.10	10	139	7	3.02
QUeenILII	11	396	0.10	33	213	33	2.54
QUeen13_13	13	728	0.20	42	401	40	3.70
Table 2: Results of COLOR dataset by different color-
ing methods.
Figure 2: Solved ratio by different
AC-GNN variations.
suitable for training. The results are shown in Figure 2, where the value of each bar represents the
solved ratio, defined as the ratio between the number of edges without introducing conflicts and the
number of total edges. For example, given a graph with 100 edges, if a coloring function colors the
graph with 10 conflicts, then the solved rate is calculated by (100 - 10)/100 = 0.9. “GDN-Train-2”
represents the trainable GDN with a depth of 2. According to the results, we observe the following:
(1) The trainable AC-GNNs, such as GCN and GDN, face a severe performance degradation when
the model becomes deep. The more complex the model is (the larger number of variables the model
has), the more severe the degradation happens; (2) The variant of the simple AC-GNN, GCN, has the
poorest coloring performance, which verifies our theoretical analysis; (3) Our model demonstrates
the best discriminative power with the fewest number of parameters, as expected.
5.2	Comparison with other graph coloring methods
The comparison with other graph coloring methods is conducted on all collected datasets. The
results are shown in Table 1 and Table 2, where k is the number of available colors and cost is
the number of conflicts in the coloring result. GNN-GCP gives NA if it fails to find a chromatic
number prediction, while Tabucol gives NA if it fails to color the graph within 24 hours. According
to the results, we observe the following: (1) Our model is much more discriminatively powerful than
GNN-GCP, with an acceptable sacrifice of efficiency. (2) Our model, which is designed as a pure
end-to-end network without any heuristic preprocess or post-process methods, shows a competitive
coloring results even compared to the state-of-the-art heuristic algorithm, especially on relatively
large or complex graphs.
6	Conclusion
In this paper, we developed theoretical foundations for reasoning about the discriminative power
of GNNs for the graph coloring problem. We further identified the node pairs that a popular class
of GNNs, AC-GNNs fail to discriminate and gave conditions on how an AC-GNN can be more
discriminative powerful. Moreover, we analyzed the color equivalence in the graph coloring problem
and proposed a scheme to make AC-GNN color equivalent. Considering the conditions and schemes
that are demonstrated to help enhance the discriminative power theoretically, we designed a simple
variant of AC-GNN for the graph coloring problem. To complete the picture, it would be interesting
to analyze the global terms for the discriminative power of GNNs.
8
Under review as a conference paper at ICLR 2021
References
Amir Abboud, Keren Censor-Hillel, and Seri Khoury. Near-linear lower bounds for distributed dis-
tance computations, even in sparse networks. In International Symposium on Distributed Com-
Puting,pp. 29-42. Springer, 2016.
Dimitris Achlioptas and Assaf Naor. The two possible values of the chromatic number of a random
graph. In ACM Symposium on Theory of computing (STOC), pp. 587-593, 2004.
Pablo Barcelo, Egor V Kostylev, Mikael Monet, Jorge Perez, Juan Reutter, and JUan Pablo Silva.
The logical expressiveness of graph neural networks. In International Conference on Learning
Representations (ICLR), 2019.
Alfredo Braunstein, Marc Mezard, Martin Weigt, and Riccardo Zecchina. Constraint satisfaction by
survey propagation., 2006.
David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random graphs. In
Proceedings on Innovations in theoretical computer science, pp. 369-376, 2014.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Annual Conference on Neural Information Processing Systems (NIPS), pp. 1024-1034, 2017a.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584, 2017b.
Alain Hertz and Dominique de Werra. Using tabu search techniques for graph coloring. Computing,
39(4):345-351, 1987.
Petter Holme and Beom Jun Kim. Growing scale-free networks with tunable clustering. Physical
review E, 65(2):026107, 2002.
Jiayi Huang, Mostofa Patwary, and Gregory Diamos. Coloring big graphs with alphagozero. arXiv
preprint arXiv:1902.10162, 2019.
Iris Hui-Ru Jiang and Hua-Yu Chang. Multiple patterning layout decomposition considering com-
plex coloring rules and density balancing. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems (TCAD), 36(12):2080-2092, 2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Henrique Lemos, Marcelo Prates, Pedro Avelar, and Luis Lamb. Graph colouring meets deep learn-
ing: Effective graph neural network models for combinatorial problems. In IEEE International
Conference on Tools with Artificial Intelligence (ICTAI), pp. 879-885. IEEE, 2019.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. DeepGCNs: Can GCNs go as deep
as CNNs? In IEEE International Conference on Computer Vision (ICCV), pp. 9267-9276, 2019.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding-design provably
more powerful GNNs for structural representation learning. Annual Conference on Neural Infor-
mation Processing Systems (NeurIPS), 2020.
Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Combinatorial optimization with graph convolu-
tional networks and guided tree search. In Annual Conference on Neural Information Processing
Systems (NIPS), pp. 539-548, 2018.
Andreas Loukas. What graph neural networks cannot learn: depth vs width. arXiv preprint
arXiv:1907.03199, 2019.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In AAAI Conference on Artificial Intelligence, volume 33, pp. 4602-4609, 2019.
9
Under review as a conference paper at ICLR 2021
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-GCN: Geo-
metric graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point
sets for 3d classification and segmentation. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR),pp. 652-660, 2017.
Mustazee Rahman, Balint Virag, et al. Local algorithms for independent sets are half-optimal. The
Annals of Probability, 45(3):1543-1577, 2017.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations (ICLR), 2019.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. arXiv preprint arXiv:2002.03155, 2020.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
DUncan J Watts and Steven H Strogatz. Collective dynamics of ‘small-world’networks. nature, 393
(6684):440-442, 1998.
Boris Weisfeiler and Andrei A Lehman. A redUction of a graph to a canonical form and an algebra
arising dUring this redUction. Nauchno-Technicheskaya Informatsia, 2(9):12-16, 1968.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018.
JiaxUan YoU, Rex Ying, and JUre Leskovec. Position-aware graph neUral networks. arXiv preprint
arXiv:1906.04817, 2019.
Manzil Zaheer, Satwik KottUr, Siamak Ravanbakhsh, Barnabas Poczos, RUss R SalakhUtdinov, and
Alexander J Smola. Deep sets. In Annual Conference on Neural Information Processing Systems
(NIPS), pp. 3391-3401, 2017.
A Graph Terminology
Here we list the following graph theoretic terms encoUntered in oUr work:
Basic graph teminology. Let G = (V, E) and G0 = (V0 , E0) be graphs on vertex set V and V0, we
define
•	isomorphism: we say that a bijection π : V → V0 is an isomorphism if any two vertices
u, v ∈ V are adjacent in G if and only if π(u), π(v) ∈ V0 are adjacent in G0, i.e., {u, v} ∈ E
iff {π(u), π(v)} ∈ E0.
•	isomorphic nodes: If there exists the isomorphism between G and G0, we say that G and G0 are
isomorphic.
•	automorphism: When π is an isomorphism of a vertex set onto itself, i.e., V = V0, π is called an
automorphism of G .
•	topologically equivalent: We say that the node pair {u, v} is topologically equivalent if there is
an aUtomorphism mapping one to the other, i.e., v = π(u).
•	equivalent: {u, v} is equivalent if it is topologically eqUivalent by π and xw = xπ(w) holds for
every w ∈ V, where xw is the node attribUte of node w.
10
Under review as a conference paper at ICLR 2021
Local graph teminology. For every positive integer r and every node u ∈ V, we define TG (u, r) as
the depth-r neighborhood of u in G, which is induced by a subtree of height r rooted at u. Consider
two nodes u, v ∈ V , we define
•	r-local automorphism: A bijection πr : V → V is an r-local automorphism mapping u to v.
•	r-local topologically equivalent: the node pair {u, v} is r-local topologically equivalent if πr is
an isomorphism from TG (u, r) to TG (v, r).
•	r-local equivalent: Similarly, {u, v} is r-local equivalent if it is r-local topologically equivalent
by πr and xw = xπr (w) holds for every w ∈ TG (u, r).
B Proof of Proposition 1
We first recall the proposition.
Proposition 1. All AC-GNNs cannot discriminate any equivalent node pair.
Proof. Let π be the automorphism mapping u to v, here, we propose a stronger proposition:
Proposition 5. Given an AC-GNN and an equivalent node pair {u, v} by π, hiw = hiπ(w) holds for
any iteration i and any node w ∈ V.
This apparently holds for i = 0 since xw = xπ(w), ∀w ∈ V. Suppose this holds for iteration j, i.e.,
hjw = hjπ(w), ∀w ∈ V. By definition, AC-GNN A produces the feature vector hjv+1 of node v in the
(j + 1)th iteration as follows:
h(vj+1) = COM(j+1)(h(vj),AGG(j+1)({h(uj) : u ∈ N (v)})).	(15)
Since an automorphism π remains the set of edges, i.e., {u, v} ∈ E iff {π(u), π(v)} ∈ E,
the connection relation between two neighbors is preserved after the permutation by π, that is,
N (π(v)) = {π(u), u ∈ N(v)} for any v ∈ V. Then, the input of AGG(j+1) for π(v) is given by
{h(uj) : u ∈ N (π(v))}, which is {h(πj()u) : u ∈ N (v)}. Since hjw = hjπ(w), ∀w ∈ V, the input of
AGG(j+1) for v is equal to the one of AGG(j+1) for v, i.e., {h(πj()u) : u ∈ N(v)} = {h(πj()u) : u ∈
N(v)} and makes their output equal, i.e., mjv+1 = mjπ+(v1). Therefore, the input of COM(j+1) for v,
(h(vj), mjv+1), is also equal to the one of COM(j+1) for π(v), which makes the vector features of v
and π(v) equal after (j + 1)th iteration for any node v ∈ V and proves the proposition 8. Thus, the
AC-GNN A always produces the same node embeddings for the nodes in the equivalent node pair,
which results in the same color.
□
C Proof of Proposition 2
We first recall the proposition.
Proposition 2. There is a graph in which at least one distinct node pair is not discriminated by any
integrated AC-GNN.
Proof. The proof starts with a simple fact: a classifier CLS(∙) always assign two nodes with the
same node embedding to the same category.
Consider the following graph G = (V, E), in which two connected nodes u and v share the same
neighborhood except each other, i.e., N (u)\{v} = N (v)\{u}. The node pair {u, v} is distinct
since they are connected. It follows that the inputs for the node features of u and v after iteration k
are exactly the same since N (u) ∪ {u} = N (u)\{v} ∪ {u, v} = N (v)\{u} ∪ {u, v} = N(v) ∪ {v}.
(j)	(j)
Therefore, the outputs are the same, which means that hu = hv holds for any iteration k and any
aggregation and combine functions AGG(∙), COM(∙). Combining with the fact that CLS(hu)=
CLS(hv) if hu = hv, the proof is finished.	□
11
Under review as a conference paper at ICLR 2021
D	Proof of Corollary 1
We first recall the corollary.
Corollary 1. A local coloring method is non-optimal in the random d-regular graph as d → ∞.
Proof. A random d-regular graph Gdn is a graph with n nodes and each node pair is connected with
a probability d/n. We start the proof from the following non-trivial proposition:
Proposition 6 (Rahman et al. (2017)). The largest density of factor of i.i.d. independent sets in a
random d-regular graph is asymptotically at most (log d)/d as d → ∞. The density of the largest
independent sets in these graphs is asymptotically 2(log d)/d.
The proposition above limits the size of an independent set produced by a local method for the
random d-regular graph with an upper bound, n(log d)/d as d → ∞. Given an upper bound of the
independent set, the following corollary on the graph coloring problem is introduced:
Corollary 2. The lower bound of k with a zero conflict constraint obtained by a local coloring
method for the random d-regular graph is d/ log d as d → ∞.
The proof is based on the Proposition 6: if a local coloring method f obtains a smaller k0, s.t.
k0 < d/ log d by coloring Gdn without conflict using k0 colors, all node sets classified by the node
color will be independent sets and the size of the maximum one will be larger than (n log d)/d, a
contradiction with Proposition 6.
The Corollary 2 reveals the lower bound of k by local methods for a random d-regular graph. An-
other important observation of k by Achlioptas & Naor (2004) specifies that exact value of the
chromatic number (i.e., the minimum k) of a random d-regular graph. The proposition is described
as follows:
Proposition 7 (Achlioptas & Naor (2004)). Let td be the smallest integer t such that d < 2t log t.
The chromatic number of a random d-regular graph is either td or td + 1.
It follows directly from Corollary 2 and Proposition 7 that, we can finish the proof of Corollary 1
by showing that the lower bound of k by local methods is always greater than the exact chromatic
number:
d/ log d > td + 1 for d → ∞.	(16)
Let f(t) = 2tlogt and define t0 s.t. d = f(t0) = 2t0 logt0. Since td is the smallest integer t such
that d < f (t), we have f(t0) = d ≥ f(td - 1). Since f is monotonically increasing, t0 ≥ td - 1
and thus d/ log d- td - 1 ≥ d/ logd- t0 - 2 always holds. Let d = 2t0 logt0, we further transform
the objective to:
d/ log d — td — 1 ≥ d/ log d — to — 2 = -—\、—力。—2 > 0, for d,to → ∞.
log(2t0 logt0)
we first prove that
_logto_ > 2/3
log(2to log to)
⇒3 log t0 > 2 log(2t0 log t0)
⇒3 log to > 2(1 +logto + log(log to))
⇒ log to > 2 + 2 log(log to) when to → ∞.
The above inequality holds obviously. Following the objective, we have:
2to log to	4
———1——V — to — 2 > —to — to - 2 > 0 When to → ∞.
log(2to logto)	3
Therefore, We finish the proof.
□
12
Under review as a conference paper at ICLR 2021
E Proof of Corollary 2
We first recall the corollary.
Corollary 2. AC-GNN is a local coloring method.
Proof. Given an AC-GNN A with L layers, let’s consider a L-local equivalent node pair {u, v} in
G by an L-local automorphism πL, which means that two rooted subtrees TG (u, L) and TG (v, L)
are isomorphic and xw = xπr(w) holds for every w ∈ TG (u, r). Since two rooted subtrees are iso-
morphic, the WL test (Weisfeiler & Lehman, 1968) decides TG (u, L) and TG (v, L) are isomorphic
and assigns the same color to w and πL (w) for any w ∈ TG (u, L). To connect the WL test with
AC-GNN, the following proposition is used:
Proposition 8 (Morris et al. (2019); Barcelo et al. (2019); XU et al. (2018)). Ifthe WL test assigns
the same color to two nodes in a graph, then every AC-GNN maps the two nodes into the same node
embedding.
Therefore, A maps the u and v into the same node embedding. It follows that A is L-local and thUs
local.	□
F	Proof of Theorem 2
We first recall the theorem.
Theorem 2. Let A be a simple AC-GNN and both input and output of each layer in A be the
probability distribution h ∈ Rk of k colors, A is color equivariant if and only if the following
conditions hold:
•	For any layer i, all the off-diagonal elements of C(i) are tied together and all the diagonal
elements are equal as well. That is,
C(i) = λ(Ci)I + γC(i)(11) λ(Ci),γC(i) ∈ R 1 = [1,..., 1]T ∈ Rk.	(17)
•	For any layer i, all the off-diagonal elements of A(i) are also tied together and all the
diagonal elements are equal as well. That is,
A(i) = λ(Ai)I + γA(i)(11) λ(Ai),γA(i) ∈R 1 = [1,..., 1]T ∈ Rk.	(18)
•	For any layer i, all elements in b(i) are equal. That is,
b(i) = β(i)1 β(i) ∈ R 1 = [1,..., 1]T ∈ Rk.	(19)
Proof. Let AGG(i) and COM(i) be the aggregation and combination fUnctions in the ith layer of
A. A is color eqUivalent if and only if all fUnctions in {AGG(i) , COM(i) : i ∈ 1, ..., L} are color
eqUivalent. the aggregation fUnction is color eqUivalent clearly and thUs we are left to consider
the color eqUivalence of combination fUnctions. Considering the definition of color eqUivalent in
Definition 4, the color eqUivalence of combination fUnction COM(i) = σ (xC (i) + yA(i) + b(i)) is
given by:
σ(xC(i) + yA(i) +b(i))P = σ(xPC(i) + yP A(i) +b(i)).	(20)
COM(i) is color eqUivalent if and only if the eqUation above holds for any permUtation matrix P ∈
Rk×k and x, y. We first find three special cases, which correspond to three conditions respectively:
Case 0. When y = ~0, we have that σ(xC(i))P = σ(xP C(i)) holds for any P and x. That is,
x(C(i)P - PC(i)) = ~0 always holds, which reveals that C(i)P = PC(i). C(i)P = PC(i) holds
for any P follows that Cm(i),m = Cn(i,)n and Cm(i),n = Cm(i),n for any m, n ∈ {1, ..., k}. Therefore, all
the off-diagonal elements of C(i) are tied together and all the diagonal elements are eqUal as well.
Case 1.	When x = ~0, we can prove that all the off-diagonal elements of A(i) are tied together and
all the diagonal elements are eqUal as well following the similar indUction in case 1.
13
Under review as a conference paper at ICLR 2021
Case 2.	When x = y = ~0, we have that σ(b(i) )P = σ(b(i)) holds for any P . Therefore, all
elements in b(i) are equal.
After proving that these conditions are necessary for a color equivalent A, we proceed to prove that
the conditions are already sufficient. Let C(i) = λ(Ci)I + γC(i) (11), A(i) = λ(Ai)I + γA(i) (11) and
b(i) = β(i) 1, COM(i) is then calculated by:
COM(i)P = σ(xC(i) + yA(i) + b(i))P
= σ(xλ(Ci)IP +xγC(i)(11)P +yλ(Ai)IP +yγA(i)(11)P +β(i)1P)
= σ(xP λ(Ci)I + xP γC(i)(11) + yP λ(Ai)I + yP γA(i)(11) +β(i)1)
= σ(xPC(i) + yPA(i) + b(i)).	(21)
Therefore, COM(i) is color equivalent if and only if the conditions hold, which completes the proof.
□
G	Proof of Proposition 4
We first recall the proposition.
Proposition 4. The cost monotonically decreases under specific parameters λ(Ci) = 1, γC(i) =
0, λ(Ai) = -δ, γA(i) = 0.
Proof. Consider the derivatives of the cost function with respect to time t. Let Cv,j denote the cost
induced by node v and j .
∂E	∂Ev ∂hv	1	∂Cv j ∂hv
——= ’  ---------- --- = — 〉	∙	------ •---
∂t	∂h ∂hv	∂t 2 乙 ∂h	∂hv	∂t
v∈V	v∈V j∈N 0(v)
=1 X X h •也
222	3	dt
v∈V j∈N 0(v)
=2 XX hj [(λC)- 1)hv + YC) (11)hv + mVi)λA) + mVi)YA) (11)]
v∈V j∈N 0(v)
=2 X mVi)[(λC)- 1)hv + YC)(11)hv + mVi)λA + mVi)γAi)(11)].	(22)
2 V∈V
To ensure the function E to monotonically decrease, a straightforward way is to guarantee the coeffi-
cients of the polynomial above satisfy a set of constraints. Obviously, the choice of λ(Ci) = 1, λ(Ai) =
-δ, YC) = YS) = 0 monotonically decreases the cost function.	□
H Details on the experimental setting and results
H.1 Models
GDN. We select a depth following the trend of the graph size, as stated in Loukas (2019). Specif-
ically, the layer number is 10 for the layout testing dataset and 100 for other larger datasets. The
trainable GDN is represented by ”GDN-train” following a same training strategy with other AC-
GNN variations. If not specified, the result of GDN shown in Section 5 stands for the training-free
GDN . The step-rate δ is 0.5 and dropedge rate p is 0.5.
GNN-GCP. We trained the GNN-GCP model for 4300 epochs achieving around 70% training
accuracy which showed no significant discrepancies from the results in the original paper, in terms
of chromatic number predictions.
Tabucol. Following the original configuration, the Tabucol algorithm is assigned with iteration
limit of 1000 (or the time limit of 24 hours) and the number of miscolored node pairs is returned if
the algorithm fails to find a perfect coloring assignment within the limit.
14
Under review as a conference paper at ICLR 2021
Other AC-GNN variations. If not specified, all AC-GNN variations have 2 layers and the hidden
dimension is 64. We trained the models for 200 epochs and the 10-layer GCN is trained for 30
epochs, in which the epoch number is selected as the one with the best validation results. We use
the SGD optimizer with learning rate 0.005 and momentum 0.1 under the layout training dataset.
H.2 Datasets
Layout. The number of available colors is set to 3. In the comparison with the AC-GNN variations,
80% randomly selected samples is separated into the training dataset (for trainable models) and the
testing dataset contains the remaining samples.
Random graphs. The number of available colors is set from 2 to 5 since it is randomly generated.
The settings to generate the random graphs are: random power-law tree (γ=3), Watts- Strogatz
small-world (k = 4, p = 0.25) and Holme and Kim model (m = 4, p = 0.1), To fit the problem
context and prevent miscalculations, we removed all self-loops in the testing graphs and added
duplex connections between connected nodes.
Citation datasets. The number of available colors of Cora, Citeseer, and Pubmed is set to 5, 6 and
8 respectively.
The chromatic numbers of graphs in Random and Citation datasets are obtained from the CSP
Solver2.
2https://developers.google.com/optimization/cp/cp_solver
15