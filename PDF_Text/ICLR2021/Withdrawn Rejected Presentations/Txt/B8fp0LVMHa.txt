Under review as a conference paper at ICLR 2021
EMaQ: Expected-Max Q-Learning Operator
for Simple Yet Effective Offline and Online
RL
Anonymous authors
Paper under double-blind review
Ab stract
Off-policy reinforcement learning (RL) holds the promise of sample-efficient
learning of decision-making policies by leveraging past experience. However, in
the offline RL setting - where a fixed collection of interactions are provided and
no further interactions are allowed - it has been shown that standard off-policy RL
methods can significantly underperform. Recently proposed methods often aim to
address this shortcoming by constraining learned policies to remain close to the
given dataset of interactions. In this work, we closely investigate an important
simplification of BCQ (Fujimoto et al., 2018a) - a prior approach for offline RL
- which removes a heuristic design choice and naturally restrict extracted poli-
cies to remain exactly within the support of a given behavior policy. Importantly,
in contrast to their original theoretical considerations, we derive this simplified
algorithm through the introduction of a novel backup operator, Expected-Max
Q-Learning (EMaQ), which is more closely related to the resulting practical al-
gorithm. Specifically, in addition to the distribution support, EMaQ explicitly
considers the number of samples and the proposal distribution, allowing us to
derive new sub-optimality bounds which can serve as a novel measure of com-
plexity for offline RL problems. In the offline RL setting - the main focus of this
work - EMaQ matches and outperforms prior state-of-the-art in the D4RL bench-
marks (Fu et al., 2020a). In the online RL setting, we demonstrate that EMaQ is
competitive with Soft Actor Critic (SAC). The key contributions of our empirical
findings are demonstrating the importance of careful generative model design for
estimating behavior policies, and an intuitive notion of complexity for offline RL
problems. With its simple interpretation and fewer moving parts, such as no ex-
plicit function approximator representing the policy, EMaQ serves as a strong yet
easy to implement baseline for future work.
1	Introduction
Leveraging past interactions in order to improve a decision-making process is the hallmark goal
of off-policy reinforcement learning (RL) (Precup et al., 2001; Degris et al., 2012). Effectively
learning from past experiences can significantly reduce the amount of online interaction required
to learn a good policy, and is a particularly crucial ingredient in settings where interactions are
costly or safety is of great importance, such as robotics Gu et al. (2017); Kalashnikov et al. (2018a),
health Murphy et al. (2001), dialog agents (Jaques et al., 2019), and education Mandel et al. (2014).
In recent years, with neural networks taking a more central role in the RL literature, there have been
significant advances in developing off-policy RL algorithms for the function approximator setting,
where policies and value functions are represented by neural networks (Mnih et al., 2015; Lillicrap
et al., 2015; Gu et al., 2016b;a; Haarnoja et al., 2018; Fujimoto et al., 2018b). Such algorithms,
while off-policy in nature, are typically trained in an online setting where algorithm updates are
interleaved with additional online interactions. However, in purely offline RL settings, where a
dataset of interactions are provided ahead of time and no additional interactions are allowed, the
performance of these algorithms degrades drastically (Fujimoto et al., 2018a; Jaques et al., 2019).
A number of recent methods have been developed to address this shortcoming of off-policy RL
algorithms. A particular class of algorithms for offline RL that have enjoyed recent success are
1
Under review as a conference paper at ICLR 2021
those based on dynamic programming and value estimation (Fujimoto et al., 2018a; Jaques et al.,
2019; Kumar et al., 2019; Wu et al., 2019; Levine et al., 2020). Most proposed algorithms are
designed with a key intuition that it is desirable to prevent policies from deviating too much from
the provided collection of interactions. By moving far from the actions taken in the offline data,
any subsequently learned policies or value functions may not generalize well and lead to the belief
that certain actions will lead to better outcomes than they actually would. Furthermore, due to the
dynamics of the MDP, taking out-of-distribution actions may lead to states not covered in the offline
data, creating a snowball effect (Ross et al., 2011). In order to prevent learned policies from straying
from the offline data, various methods have been introduced for regularizing the policy towards a
base behavior policy (e.g. through a divergence penalty (Jaques et al., 2019; Wu et al., 2019; Kumar
et al., 2019) or clipping actions (Fujimoto et al., 2018a)).
Taking the above intuitions into consideration, in this work we investigate a simplifcation of the
BCQ algorithm (Fujimoto et al., 2018a) (a notable prior work in offline RL), which removes a
heuristic design choice and has the property that extracted policies remain exactly within the sup-
port of a given behavior policy. In contrast to the theoretical considerations in the original work, we
derive this simplified algorithm in a theoretical setup that more closely reflects the resulting algo-
rithm. We introduce the Expected-Max Q-Learning (EMaQ) operator, which interpolates between
the standard Q-function evaluation and Q-learning backup operators. The EMaQ operator makes
explicit the relation between the proposal distribution and number of samples used, and leads to
sub-optimality bounds which introduce a novel notion of complexity for offline RL problems. In
its practical implementation for the continuous control and function approximator setting, EMaQ
has only two standard components (an estimate of the base behavior policy, and Q functions) and
does not explicitly represent a policy, requiring fitting one less function approximator than prior
approaches (Fujimoto et al., 2018a; Kumar et al., 2019; Wu et al., 2019).
In online RL, EMaQ is competitive with Soft Actor Critic (SAC) (Haarnoja et al., 2018) and sur-
passes SAC in the deployment-efficient setting (Matsushima et al., 2020). In the offline RL setting -
the main focus of this work - EMaQ matches and outperforms prior state-of-the-art in the D4RL (FU
et al., 2020a) benchmark tasks. Through our explorations with EMaQ we make two intriguing find-
ings. First, due to the strong dependence of EMaQ on the quality of behavior policy used, our
results demonstrate the significant impact of careful considerations in modeling the behavior policy
that generate the offline interaction datasets. Second, relating to the introduced notion of complex-
ity, in a diverse array of benchmark settings considered in this work we observe that surprisingly
little modification to a base behavior policy is necessary to obtain a performant policy. As an exam-
ple, we observe that while a HalfCheetah random policy obtains a return of 0, a policy that at each
state uniformly samples only 5 actions and chooses the one with the best value obtains a return of
2000. The simplicity, intuitive interpretation, and strong empirical performance of EMaQ make it a
great test-bed for further examination and theoretical analyses, and an easy to implement yet strong
baseline for future work in offline RL.
2	Background
Throughout this work, we represent Markov Decision Process (MDP) as M = hS, A, r, P, γi, with
state space S, action space A, reward function r : S×A→R, transition dynamics P, and discount γ.
In offline RL, we assume access to a dataset of interactions with the MDP, which we will represent
as collection of tuples D = {(s, a, s0 , r, t)}N, where t is an indicator variable that is set to True
when s0 is a terminal state. We will use μ to represent the behavior policy used to collect D, and
depending on the context, We will overload this notation and use μ to represent an estimate of the
true behavior policy. For a given policy π, we will use the notation dπ (s), dπ (s, a) to represent the
state-visitation and state-action visitation distributions respectively.
As alluded to above, a significant challenge of offline RL methods is the problem of distribution
shift. At training-time, there is no distribution shift in states as a fixed dataset D is used for training,
and the policy and value functions are never evaluated on states outside of dμ(s). However, a very
significant challenge is the problem of distribution shift in actions. Consider the Bellman backup
for obtaining the Q-function of a given policy π,
Tn Q(s, a):= r(s, a) + Y ∙
EsOEaO〜π(a0∣s0) [Q(S , a R
(1)
2
Under review as a conference paper at ICLR 2021
The target Q-ValUes on the right hand side depend on action samples a0 〜 ∏(a0∣s0). If the sampled
actions are outside the distribution of actions observed in D, the estimated Q-values can be erroneous
leading to incorrect target valUes. The effects of action distribUtion shift are fUrther exacerbated in
actor-critic algorithms; oUt of distribUtion (OOD) actions may incorrectly be assigned high valUes,
in which case the policy will be Updated to fUrther sample OOD actions, leading to a hazardoUs loop.
An important approach - with particular recent interest - to mitigate the effects of both kinds of
distribUtional shift is to devise methods for constraining learned policies to remain close to the
behavior policy μ: dπ(s, a) ≈ dμ(s, a). Below, We set the stage by reviewing a closely related prior
work in offline RL.
Batch Constrained Q-Learning (BCQ) In BCQ (Fujimoto et al., 2018a) the aim is to constrain a
Q-Learning based algorithm such that it will be effective in the offline RL continuous control setting
with function approximators. To do so, the trained policy is parameterized as:
∏θ(a|s) = arg max Qψ (s,ai + ξθ(s,ai))	for	a% 〜μ(a∣s),i = 1,...,N	(2)
ai+ξθ(s,ai)
y(s, a, s0, r,t) = r + (1 - t) ∙ Y max Qψo (s0, ai)
a0i
for	ai 〜∏θ(a0∣s0),i = 1,…,N (3)
LQ = (y(s, a, s0, r, t) - Qψ (s, a))2	(4)
where y(s, a) are target Q-values, Qψ is learned with the objective in equation 4, μ(a∣s) is an
estimate of the base behavior policy (a generative model trained using the dataset D), and ξθ is an
action perturbation model trained to modify actions towards more optimal ones. Crucially, each
component of the output ofξθ is bounded to the range [-Φ, Φ]. The key intuition is that because ai
are sampled from an estimate of the behavior policy, they should hopefully be within the distribution
observed in D. Thus, since the perturbation model is constrained by the hyperparameter Φ, the
perturbed actions should not be too far from actions in the dataset. This should mitigate errors in
value estimates, which should in turn lead to better updates for the perturbation model.
3	Expected-Max Q-Learning
We make the observation that, in the BCQ algorithm, if we could obtain a good estimate μ(a∣s) and
sufficiently increased the number of samples N, there would be no need for the perturbation network
ξθ . This simplification would remove one additional function approximator and the associated hy-
perparameter Φ. This is the driving intuition of our work, which we frame theoretically in a manner
that encapsulates the key components: the behavior policy μ and number of samples N. Below, we
introduce the Expected-Max Q operator, illustrate its key properties for tabular MDPs, and obtain
sub-optimality bounds which can serve as a novel measure of complexity of an offline RL problem
for future theoretical work. We then provide an extension to the offline RL setting with function
approximators, and then discuss the generative model used to approximate the behavior policy.
3.1	Expected-Max Q Operator
Let μ(a∣s) be an arbitrary behavior policy, and let {ai}N 〜 μ(a∣s) denote sampling N iid actions
from μ(a∣s). Let Q : S ×A→ R be an arbitrary function. For a given choice of N, we define the
Expected-Max Q-Learning operator (EMaQ) TNQ as follows:
TμN Q(S, a) := r(S, a) + Y ∙ Es0 E{ai}N 〜μ(∙∣sθ)
max Q(S0, a0)
a0∈{a0i}N
Q-Evaluation for μ
Q-Learning
TμQ(s, a) := r(S, a) + Y ∙ Es0Ea0〜”(∙∣sθ) IQ(S0, a0)]
T*Q(s, a) := r(s, a) + Y ∙ Eso m max Q(s0, a0)]
(5)
(6)
(7)
This operator provides a natural interpolant between the on-policy backup for μ (Eq. 6) when
N = 1, and the Q-learning backup (Eq. 7) as N → ∞ (if μ(a∣s) has full support over A). We
formalize these observations more precisely below when we articulate the key properties in the
tabular MDP setting. We discuss how this relates to existing modified backup operators in the
related work.
3
Under review as a conference paper at ICLR 2021
3.2	Dynamic Programming Properties in the Tabular MDP Setting
To understand any novel backup operator it is useful to first characterize its key dynamic program-
ming properties in the tabular MDP setting. First, we establish that EMaQ retains essential contrac-
tion and fixed-point existence properties, regardless of the choice of N ∈ N. In the interest of space,
all missing proofs can be found in Appendix A.
Theorem 3.1.	In the tabular setting, for any N ∈ N, TN is a contraction operator in the L∞
norm. Hence, with repeated applications of the TN, any initial Q function converges to a unique
fixed point.
Theorem 3.2.	Let QN denote the unique fixed point achieved in Theorem 3.1, and let ∏N(a|s)
denote the policy that samples N actions from μ(a∣s), {ai}N, and chooses the action with the
maximum QN. Then QN is the Q-value function corresponding to πNN(a|s).
Proof. (Theorem 3.2) Rearranging the terms in equation 5 we have,
TNQN (S, a) = T(S, a) + Y ∙ Es0Ea0〜πN(a0∣s0) [QN(S0, a0)]
Since by definition QN is the unique fixed point of T(N, We have our result.	□
From these results we can then rigorously establish the interpolation properties of the EMaQ family.
Theorem 3.3. Let ∏μ denote the optimal policy from the class of policies whose actions are re-
stricted to lie within the support of the policy μ(a∣s). Let Qμ denote the Q-value function cor-
responding to ∏μ. Furthermore, let Qμ denote the Q-VaIUefUnCtiOn of the policy μ(a∣s). Let
μ*(s) ：= JSUPPort(∏* (a∣s)) μ(a∣s) denote the probability of optimal actions under μ(a∣s). Under
the assumption that infS μ*(s) > 0 and r(s, a) is bounded, we have that,
Qμ = Qμ	and	Iim QN = Q*μ
N→∞
That is, Theorem 3.3 shows that, given a base behavior policy μ(a∣s), the choice of N makes the
EMaQ operator interpolate between evaluating the Q-value of μ on the one hand, and learning
the optimal Q-value function on the other (optimal subject to the support constraint discussed in
Theorem 3.3). In the special case where μ(a∣s) has full support over the action space A, EMaQ
interpolates between the standard Q-Evaluation and Q-Learning operators in reinforcement learning.
Intuitively, as we increase N, the fixed-points QN should correspond to increasingly better policies
∏N(a|s). We show that this is indeed the case.
Theorem 3.4.	For all N, M ∈ N, where N > M, we have that ∀s ∈ S,∀a ∈ SUPPOrt(μ(∙∣s)),
QN(s, a) ≥ QM(s, a). Hence, ∏N(a|s) is at least as good ofa policy as ∏M(a|s).
It is also valuable to obtain a sense of how suboptimal ∏N(a|s) may be with respect to the optimal
policy supported by the policy μ(a∣s).
Theorem 3.5.	For S ∈ S let,
δ(s,n ) =	c max, ,、、Qμ(s,00 — E{ai}N 〜μ(∙∣s)Lmaχιv Qμ(s,b)]
a∈Support(μ(∙∣s))尸	b∈{ai}N
The suboptimality of QN can be upperbounded as follows,
IIQN-Qμ∣∣∞ ≤ 士maxES0[δ(s0,n)i ≤ 占maxMs,N)	(8)
The same also holds when Qμ is replaced with QN in the definition of ∆.
3.3	A Measure of C omplexity for Offline RL
The bounds in equation 8 capture the main intuitions about the interplay between μ(a∣s) and the
choice of N. If for each state, μ(a∣s) places sufficient mass over the optimal actions, ∏N will be
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Test-Time Policy πtest
Function TestEnsemble(values):
L return λ ∙ min(values) + (1 — λ) ∙ max(values)
Function πtest (s):
{ai}N 〜μ(a∣s)
return argmaX{ai}N TestEnsemble([Qi(s, a) for all i])
close to ∏μ. The two variants of ∆(s,N), based on Qμ or QN, suggest an intriguing notion of
difficulty for an offline RL problem. If we could estimate either of these Q-value functions, then
for a desired set of states (such as initial states) we could plot ∆(s, N) as decreasing function of
N. The rate at which this function decreases could serve as an intuitive notion of difficulty for a
given offline offline RL problem which consists of an MDP and a given behavior policy. While we
leave theoretical investigations of this measure for future work, our empirical results in Section 5
demonstrate that the effective value of N may be surprisingly small.
3.4	Offline RL Setting with Function Approximators
Typically, we are not provided with the policies that generated the provided trajectories. Hence, as
a first step we fit a generative model μ(a∣s) to the (s, a) pairs in the offline dataset, representing the
mixture of policies that generated this data (details below). Having obtained μ(a∣s),we move on to
the EMaQ training procedure. Similar to prior works (Fujimoto et al., 2018a; Kumar et al., 2019;
Wu et al., 2019), we train K Q functions (represented by MLPs) and make use of an ensembling
procedure to combat overestimation bias (Hasselt, 2010; Van Hasselt et al., 2016; Fujimoto et al.,
2018b). Letting D represent the offline dataset, the objective for the Q functions takes the following
form:
L(Oi) = E(s,a,s0,r,t)〜D
Qθi (s, a) — y(s, a, s0,r,t)
(9)
y(s,a, s0,r,t) = (r + (1 — t) ∙ YmaXQens(S0,a。)	for ai 〜μ(a0∣s0),i = 1,…,N (10)
where t is the indicator variable 1[s0 is terminal], and Q0ens represents the ensemble of target Q
functions. In short, we sample N actions from μ(a0∣s0) and take the value of the best action to form
the target. The algorithm box describing the full training loop can be viewed in Algorithm 2.
Notably, we do not train an explicit neural network representing the policy. At test-time, given
a state s, we sample N actions from μ(a∣s) and choose the action with the maximum value under the
ensemble ofQ functions (see Algorithm 1). While TestEnsemble can differ from the Ensemble
function used to compute target Q values1, in this work we used the same ensembling procedure with
λ = 1.0 (with the exception of experiments in Section F).
3.5	Intuition for Offline EMAQ and Modeling μ(a∣s)
The intuition for how offline EMaQ aims to address the problem of erroneous value estimates can
be understood from attending to equation 10 and the form of the test-time policy (Algorithm 1).
in equation 10 we observe that target values for the Q functions are computed by sampling actions
from the behavior policy estimate μ, and not a separately learned policy that may sample out
of distribution actions, as in BCQ. At test-time, our implicit policy is also formed by choosing
amongst actions sampled from μ. Hence, if μ accurately estimates the behavior policy well, we will
never sample actions outside the support and will not need to evaluate the value of such actions.
In the practical setting where μ may have inaccuracies, the hyperparameter N acts as an implicit
regularizer: Using very large values of N maximizes the chance of sampling actions that are out
1some examples of alternative choices are mean, max, UCB-style estimates, or simply using just one of the
trained Q functions
5
Under review as a conference paper at ICLR 2021
of distribution and have erroneous value estimates, while smaller N reduces the chance of this
happening in every update iteration and therefore smoothens the incorrect values.
With the importance of a good behavior estimates accentuated in our proposed method, we pay
closer attention to the choice of generative model used for representing μ. Past works (Fujimoto
et al., 2018a; Kumar et al., 2019; Wu et al., 2019) have typically used Variational Auto-Encoders
(VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) to represent the behavior distribution
μ(a∣s). Unfortunately, after training the aggregate posterior qagg(z) := Eχ[q(z∣x)] of a VAE does
not typically align well with its prior, making it challenging to sample from in a manner that ef-
fectively covers the distribution it was trained on2. We opt for using an autoregressive architecture
based on MADE (Germain et al., 2015) as it allows for representing more expressive distributions
and enables more accurate sampling. Inspired by recent works (Metz et al., 2017; Van de Wiele et al.,
2020), our generative model architecture also makes use of discretization in each action dimension.
Full details can be found in Appendix B.
4	Related Work
Offline RL Many recent methods for offline RL (Fujimoto et al., 2018a; Kumar et al., 2019; Wu
et al., 2019; Jaques et al., 2019), where no interactive data collection is allowed during training,
mostly rely on constraining the learned policy to stay close to the data collection distribution. Fuji-
moto et al. (2018a) clip the maximum deviation from actions sampled from a base behavior policy,
while Kumar et al. (2019); Wu et al. (2019); Jaques et al. (2019) incorporate additional distribu-
tional penalties (such as KL divergence or MMD) for regularizing learned policies to remain close
to the base policy. Our work is an instance of this family of approaches for offline RL; however,
arguably our method is simpler as it does not involve learning an additional proposal-modifying
policy Fujimoto et al. (2018a), or modifying reward functions (Kumar et al., 2019; Jaques et al.,
2019).
Finding Maximizing Actions Naively, EMaQ can also be seen as just performing approxi-
mate search for maxa Q(s, a) in standard Q-learning operator, which has been studied in various
prior works for Q-learning in large scale spaces (e.g. continuous). NAF (Gu et al., 2016b) and
ICNN (Amos et al., 2017) directly constrain the function family of Q-functions such that the opti-
mization can be closed-form or tractable. QT-OPT (Kalashnikov et al., 2018b) makes use of two
iterations of the Cross-Entropy Method (Rubinstein & Kroese, 2013), while CAQL (Ryu et al., 2019)
uses Mixed-Integer Programming to find the exact maximizing action while also introducing faster
approximate alternatives. In (Van de Wiele et al., 2020) - the most similar approach to our pro-
posed method EMaQ - throughout training a mixture of uniform and learned proposal distributions
are used to sample actions. The sampled actions are then evaluated under the learned Q functions,
and the top K maximizing actions are distilled back into the proposal distribution. In contrast to
our work, these works assume these are approximate maximization procedures and do not provide
extensive analysis for the resulting TD operators. Our theoretical analysis on the family ofTD oper-
ators described by EMaQ can therefore provide new perspectives on some of these highly successful
Q-learning algorithms (Kalashnikov et al., 2018a; Van de Wiele et al., 2020) 一 particularly on how
the proposal distribution affects convergence.
Modified Backup Operators Many prior works study modifications to standard backup opera-
tors to achieve different convergence properties for action-value functions or their induced optimal
policies. Ψ-learning (Rawlik et al., 2013) proposes a modified operator that corresponds to policy
iterations with KL-constrained updates (Kakade, 2002; Peters et al., 2010; Schulman et al., 2015)
where the action-value function converges to negative infinity for all sub-optimal actions. Similarly
but distinctly, Fox et al. (2015); Jaques et al. (2017); Haarnoja et al. (2018); Nachum et al. (2017)
study smoothed TD operators for a modified entropy- or KL-regularized RL objective. Bellemare
et al. (2016) derives a family of consistent Bellman operators and shows that they lead to increasing
action gaps (Farahmand, 2011) for more stable learning. However, most of these operators have
not been studied in offline learning. Our work adds a novel family operators to this rich literature of
2past works typically clip the range of the latent variable z and adjust the weighting of the KL term in the
evidence lower-bound to ameliorate the situation
6
Under review as a conference paper at ICLR 2021
Random
Medium
Mixed
〃(a ⑸ 5 10 25 50 100 200 400	〃(a ⑸ 5 10 25 50 100 200 400	〃向S) 5 10 25 50 100 200 400	〃向S) 5 10 25 50 100 200 400
EMaQ	EMaQ	EMaQ	EMaQ
Figure 1: Results for evaluating EMaQ on D4RL benchmark’s (Fu et al., 2020b) standard Mujoco domains,
With N ∈ {5,10, 25, 50, 100, 200, 400}. Values above μ(a|s) represent the result of evaluating the base
behavior policies. Horizontal green lines represent the reported performance of BEAR in the D4RL benchmark
(apples to apples comparisons in Figure 2). The types of offline datasets are: random: 1M transitions are
collected by a random agent, medium: 1M transitions are collected by a half-trained SAC (Haarnoja et al.,
2018) policy, mixed: the replay buffer of this half-trained policy, and medium-expert: combination of the
medium dataset and 1M additional transitions from a fully trained policy. Refer to main text (Section 5.1)
for description of color-coding. For better legibility, We have included a larger variant of these plots in the
Appendix L. Full experimental details in Appendix G.
operators for RL, and provides strong empirical validation on hoW simple modifications of operators
can translate to effective offline RL With function approximations.
5	Experiments
For all experiments We make use of the codebase of (Wu et al., 2019), Which presents the BRAC
off-policy algorithm and examines the importance of various factors in BCQ (Fujimoto et al., 2018a)
and BEAR (Kumar et al., 2019) methods. We implement EMaQ into this codebase. We make use
of the recently proposed D4RL (Fu et al., 2020b) datasets for bechmarking offline RL.
Online EMaQ Despite obtaining strong online RL results competitive with and outperforming
SAC (Haarnoja et al., 2018) (Figure 3), as the main focus of our Work is for the offline setting, We
have placed our online RL methodology and results in Appendix F. HoWever, We emphasize that
significance of obtaining strong online RL performance With effectively the same algorithm as the
offline setting should not be overlooked. Most prior offline RL Works have not considered hoW their
methods might transfer to online or batched online setting, and recent Work (Nair et al., 2020) has
demonstrated the challenges of finetuning from a policy trained offline, in the online setting.
5.1	Practical Effect OF N and THE Choice OF Generative Model μ
We begin by empirically evaluating key aspects of offline EMaQ, namely the effect of N , and choice
of generative model used for representing the behavior estimate μ. In prior approaches such as those
described in the background section of this Work, care must be taken in choosing the hyperparam-
eter that dictates the extent to Which learned policies can deviate from the base behavior policies;
too small and We cannot improve upon the base policy, too large and the value of actions cannot
be correctly estimated. in EMaQ, at least in theory, choosing higher values of N should result in
strictly better policies. Additionally, there exists a concern that N may need to be impractically
large. Thus, We empirically investigate to What extent the monotonic trend holds in practice, and
seek to understand What magnitudes of N result in good policies in practical benchmark domains.
Figure 1 presents our results With N ∈ {5, 10, 25, 50, 100, 200, 400}. in the green plots, We ob-
serve that empirical results folloW our intuitions: With increasing N the resultant policies become
7
Under review as a conference paper at ICLR 2021
Random
Medium
Mixed
Figure 2: Comparison of EMaQ, BCQ, and BEAR on D4RL (Fu et al., 2020b) benchmark domains when
using our proposed autoregressive μ(a∣s). For both BCQ and BEAR, from left to right as the value of the
hyperparameter increases, the allowed deviation from μ(a∣s) increases. Horizontal green lines represent the
reported performance of BEAR in the D4RL benchmark. Color-coding follows Figure 1. For better legibility,
we have included a larger variant of these plots in the Appendix L. Full experimental details in Appendix G.
better. In the medium-expert settings (i.e. orange plots), while for smaller values of N we observe
strong performance, there appears to be a downward trend. As discussed in Section 3.5, smaller
values of N result in an implicit regularization. Hence, the orange plots may indicate that even with
the stronger choice of generative models in EMaQ, inaccuracies in value estimates may still exist,
suggesting the need for future work that introduces better regularizers for the value functions than
ensembling (Kumar et al., 2020). Lastly, the red plots indicate settings where behavior is erratic.
Closer examination of training curves and our experiments with other off-policy methods (Figure 2)
suggests that this may be due to the intrinsic nature of these environment and data settings.
The dashed horizontal lines in Figure 2 represent the performance of BEAR - which uses a VAE for
representing μ - as reported in the D4RL (FU et al., 2020b) benchmark paper (apples to apples com-
parison in Section 5.2). Our results demonstrate that the combination of a strong generative model
and EMaQ’s simply constrained backup operator can match and in many cases noticeably exceed
results from prior algorithms and design choices. Comparing Figure 2 to Figure 6 in the Appendix,
we observe that our choice of generative model is crucial to the performance of EMaQ. With a
VAE architecture as used in prior work, EMaQ’s performance is significantly reduced, in most cases
worse than prior reported results for BEAR, and never exhibits a monotonic trend as a function of
N. This is despite the fact that when evaluating the performance of the behavior estimate μ under
the two architecture choices results in almost identical results (the first column of each sub-plot
corresponding to μ(a∣s)). We do not believe that autoregressive models are intrinsically better than
VAEs, but rather our results demonstrate the need for more careful attention on the choice of μ(a∣s).
Since EMaQ is closely tied to the choice of behavior model, it may be more effective for evalu-
ating how well μ(a∣s) represents the given offline dataset. From a practical perspective, our
results suggest that for a given domain, focusing efforts on building-in good inductive biases
in the generative models and value functions might be sufficient to obtain strong offline RL
performance in many domains.
5.2	Comparison on D4RL Offline RL B enchmark
To evaluate EMaQ with respect to prior methods, we compare to two popular and closely related
prior methods for offline RL, BCQ (Fujimoto et al., 2018a) and BEAR (Kumar et al., 2019). As
with the previous section, full experimental details can be found in Appendix G.1. Figure 2 and
Table 1 present our empirical results. Note that with our proposed autoregressive models, the re-
sults for BEAR are matched and in some cases noticeably above the values reported in the D4RL
benchmark (Fu et al., 2020b) (green horizontal lines). For easier interpretation, the plots are colored
8
Under review as a conference paper at ICLR 2021
Setting	BC	BCQ	BEAR	EMaQ	EMaQN
kitchen-complete	27.2 ± 3.2	26.5 ± 4.8	—	36.9 ± 3.7	64
kitchen-partial	46.2 ± 2.8	69.3 ± 5.2	—	74.6 ± 0.6	8
kitchen-mixed	52.5 ± 3.8	65.5 ± 1.8	—	70.8 ± 2.3	8
antmaze-umaze	59.0 ± 5.5	25.5 ± 20.0	56.3 ± 28.8	91.0 ± 4.6	100-
antmaze-umaze-diverse	58.8 ± 9.5	68.0 ± 19.0	57.5 ± 39.2	94.0 ± 2.4	50
antmaze-medium-play	0.7 ± 1.0	3.5 ± 6.1	0.2 ± 0.4	0.0 ± 0.0	—
antmaze-medium-diverse	0.4 ± 0.8	0.5 ± 0.9	0.2 ± 0.4	0.0 ± 0.0	—
antmaze-large-play	0.0 ± 0.0	0.0 ± 0.0	0.0 ± 0.0	0.0 ± 0.0	—
antmaze-large-diverse	0.0 ± 0.0	0.0 ± 0.0	0.0 ± 0.0	0.0 ± 0.0	—
door-cloned	0.0 ± 0.0	0.2 ± 0.4	0.0 ± 0.0	0.2 ± 0.3	64
hammer-cloned	1.2 ± 0.6	1.3 ± 0.5	0.3 ± 0.0	1.0 ± 0.7	64
pen-cloned	24.5 ± 10.2	43.8 ± 6.4	-3.1 ± 0.2	27.9 ± 3.7	128
relocate-cloned	-0.2 ± 0.0	-0.2 ± 0.0	0.0 ± 0.0	-0.2 ± 0.2	16
Table 1: Results on a series of other environments and data settings from the D4RL benchmark (Fu et al.,
2020a). Results are normalized to the range [0, 100], per the D4RL normalization scheme. For each method, for
each environment and data setting the results of the best hyperparameter setting are reported. The last column
indicates the best value of N in EMaQ amongst the considered hyperparameters (for the larger antmaze
domains, we do not report this value since no value of N obtains nonzero returns). All the domains below the
blue double-line are effectively unsolved by all methods. We have technical difficulties in evaluating BEAR on
the kitchen domains. This manuscript will be updated upon obtaining these results. Additional details can be
found in Appendix G.3.
the same as in Figure 1. Our key take-away is that despite its simplistic form, EMaQ is strongly
competitive with prior state-of-the-art methods, and in the case of Table 1 outperforms prior ap-
proaches. Despite this, there remain many domains in the D4RL benchmark on which none of the
considered algorithms make any progress (Table 1), indicating that much algorithmic advances are
still necessary for solving many of the considered domains.
A very eye-catching result in above figures is that in almost all settings of the standard Mujoco
environments (Figures 1 and 2),just N = 5 significantly improves upon μ(a∣s) and in most settings
matches or exceeds significantly beyond previously reported results. Concretely, this means that in
the HalfCheetah-Random setting, if at each state we sample 5 actions uniformly random and choose
the best one under the learned Q-value function, we convert a random policy with return 0 to a
policy with return 2000. In this way, EMaQ provides a quite intuitive and surprising measure
of the complexity for offline RL problems. This empirical observation also corroborates our
discussion in Section 3.3, encouraging future theoretical investigations into ∆(s, N).
6	Conclusion
In this work, we investigate a significant simplification of the BCQ (Fujimoto et al., 2018a) algorithm
by removing the heuristic perturbation network. By introducing the Expect-Max Q-Learning opera-
tor, We present a novel theoretical setup that takes into account the proposal distribution μ(a∣s) and
the number of action samples N, and hence more closely matches the resulting practical algorithm.
With feWer moving parts and one less function approximator, EMaQ matches and outperforms prior
state-of-the-art in online and offline RL. Our investigations With EMaQ demonstrate the significance
of careful considerations in the design of generative models used. Furthermore, our theoretical and
empirical findings bring into light novel notions of complexity for offline RL problems. Given the
simplicity, tractable theory, and state-of-the-art performance of EMaQ, We hope our Work can serve
as a foundation for future Works on understanding and improving offline RL.
References
Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural netWorks. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pp. 146-155. JMLR. org, 2017.
Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip S Thomas, and Remi Munos. Increasing
the action gap: NeW operators for reinforcement learning. In Thirtieth AAAI Conference on
Artificial Intelligence, 2016.
9
Under review as a conference paper at ICLR 2021
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-
ensembles. arXiv preprint arXiv:1706.01502, 2017.
Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint
arXiv:1205.4839, 2012.
Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. In Advances in
Neural Information Processing Systems,pp. 172-180, 2011.
Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft
updates. arXiv preprint arXiv:1512.08562, 2015.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning, 2020a.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. Datasets for data-driven
reinforcement learning. arXiv preprint arXiv:2004.07219, 2020b.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. arXiv preprint arXiv:1812.02900, 2018a.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018b.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. In International Conference on Machine Learning, pp. 881-889, 2015.
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-
prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247,
2016a.
Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning
with model-based acceleration. In International Conference on Machine Learning, 2016b.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning
for robotic manipulation with asynchronous off-policy updates. In International Conference on
Robotics and Automation, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Hado V Hasselt. Double q-learning. In Advances in neural information processing systems, pp.
2613-2621, 2010.
Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jose MigUel Hernandez-Lobato, Richard E
Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation mod-
els with kl-control. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1645-1654. JMLR. org, 2017.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt:
Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on
Robot Learning, 2018a.
10
Under review as a conference paper at ICLR 2021
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre
Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep
reinforcement learning for vision-based robotic manipulation. arXiv preprint arXiv:1806.10293,
2018b.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Sys-
tems, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline
reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tuto-
rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy
evaluation across representations with applications to educational games. In International Con-
ference on Autonomous Agents and Multiagent Systems, 2014.
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. arXiv preprint
arXiv:2006.03647, 2020.
Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of
continuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Susan A Murphy, Mark J van der Laan, James M Robins, and Conduct Problems Prevention Re-
search Group. Marginal mean models for dynamic regimes. Journal of the American Statistical
Association, 2001.
Ofir Nachum, Mohammad Norouzi, and Dale Schuurmans. Bridging the gap between value and
policy based reinforcement learning. In Advances in Neural Information Processing Systems, pp.
2772-2782, 2017.
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Twenty-Fourth
AAAI Conference on Artificial Intelligence, 2010.
Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning
with function approximation. In International Conference on Machine Learning, 2001.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and rein-
forcement learning by approximate inference. In Twenty-Third International Joint Conference on
Artificial Intelligence, 2013.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to no-regret online learning. In Proceedings of the fourteenth international con-
ference on artificial intelligence and statistics, pp. 627-635, 2011.
11
Under review as a conference paper at ICLR 2021
Reuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified approach to combina-
torial optimization, Monte-Carlo simulation and machine learning. Springer Science & Business
Media, 2013.
Moonkyung Ryu, Yinlam Chow, Ross Anderson, Christian Tjandraatmadja, and Craig Boutilier.
Caql: Continuous action q-learning. arXiv preprint arXiv:1909.12397, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015.
Tom Van de Wiele, David Warde-Farley, Andriy Mnih, and Volodymyr Mnih. Q-learning in enor-
mous action spaces via amortized approximate maximization. arXiv preprint arXiv:2001.08116,
2020.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
Lilian Weng.	Exploration strategies in deep reinforcement learning, Jun
2020.	URL https://lilianweng.github.io/lil-log/2020/06/07/
exploration- strategies- in- deep- reinforcement- learning.html.
Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
arXiv preprint arXiv:1911.11361, 2019.
12
Under review as a conference paper at ICLR 2021
A Proofs
All the provided proofs operate under the setting where μ(a∣s) has full support over the action space.
When this assumption is not satisfied, the provided proofs can be transferred by assuming we are
operating in a new MDP Mμ as defined below.
Given the MDP M = (S, A,r, P, γ)and μ(a∣s), let US define the new MDP Mμ =
hSμ, Aμ, r, P, γi, where Sμ denotes the set of reachable states by μ, and Aμ is A restricted to
the support of μ(a∣s) in each state in S*.
A.1 Contraction Mapping
Theorem 3.1. In the tabular setting, for any N ∈ N, TμN is a Contraction operator in the L∞
norm. Hence, with repeated applications of the TfN, any initial Q function converges to a unique
fixed point.
Proof. Let Q1 and Q2 be two arbitrary Q functions.
IlTN QI-TN Q2L =	(11)
max I (r(s, a) + Y ∙ Es，E{ai}N [max Q1(s0, a0)]) - (r(s, a) + Y ∙ Es，E{ai}N [max Q2 (s0, a')]) I
s,a	{ai}N	{ai}N
(12)
Y . max Es，E{ai}
s,a I
N max Q1 (s0, a0) - max Q2 (s0, a0)
{ai}N	{ai}N
(13)
≤
γ ∙ maxEs，EgaN max Qι(s0, a0) — max Q2(s0,a0) ≤	(14)
Y ∙ maxEs，E{ai}N kQι — Q2k∞ =	(15)
Y ∙kQ1- Q2k∞	(16)
where line 15 is due to the following: Let ^ = arg maxgaN Qi (s0,ai),
max Q1(s0,a0) — max Q2(s0, a0) = Q1(s0,^) — max Q2(s0, a0)	(17)
{ai}N 1	{ai}N 2	1	{ai}N 2
≤ Q1(s0,^) — Q2(s0,^)	(18)
≤ kQ1 — Q2k∞	(19)
□
A.2 Limiting Behavior
Theorem 3.3.	Let ∏μ denote the optimal policy from the class of policies whose actions are re-
stricted to lie within the support of the policy μ(a∣s). Let Qμ denote the Q-value function cor-
responding to ∏μ. Furthermore, let Qμ denote the Q-VaIUefUnction of the policy μ(a∣s). Let
μ*(s) := JSUPPort(∏* (a∣s)) μ(a∣s) denote the probability of optimal actions under μ(a∣s). Under
the assumption that infS μ*(s) > 0 and r(s, a), we have that,
Qμ = Qμ	and	Iim QN = Qμ
N→∞
Let μ*(s) := JSUPPort(∏*(a∣s)) μ(a∣s) denote the probability of optimal actions under μ(a∣s). To
show IimN→∞ QN = Qμ, we also require the additional assumption that infS μ*(s) > 0.
Proof. Given that,
TIQ(S, a) := r(s, a) + Y ∙ Es，E{ai}N〜“(.|s，)[Q(s0, a')]	QO)
the unique fixed-point of TI is the Q-value function of the policy μ(a∣s). Hence Q∖ = Qμ.
The second part of this theorem will be proven as a Corollary to Theorem 3.5	□
13
Under review as a conference paper at ICLR 2021
A.3 Increasingly Better Policies
Theorem 3.4.	For all N, M ∈ N, where N > M, we have that ∀s ∈ S, ∀a ∈ SUPPort(μ(∙∣s)),
QN(s, a) ≥ QM(s, a). Hence, ∏N(a|s) is at least as good ofa policy as ∏M(a|s).
Proof. It is sufficient to show that ∀s, a, QN+1(s,a) ≥ QN(s,a). We will do so by induction. Let
Qi denote the resulting function after applying TN +1, i times, starting from QN.
Base Case
By definition Q0 := QN. Let S ∈ S, a ∈ A.
Q1(S,a)= TμN +1Q0(S,a)	(21)
=r(S,a) + γ ∙ Es0E{ai}N+1 〜“(。，怙，)[,maχ, Qo(S0,a0)] {ai}N +1	(22)
≥ r(S,a) + γ ∙ Es0兀侬尸〜“(a，|s，)[max Qo(S0,a0)] {ai}N	(23)
=T(S, a) + Y ∙ Es0E{ai}N〜μ(a0∣s0) [maN QN (S', a0)] {ai}N	(24)
=QN (S, a)	(25)
= Q0(S, a)	(26)
Induction Step
Assume ∀s, a, Qi(s, a) ≥ Qi-1(s, a).
Qi+1(s, a) - Qi(s, a) = TN +1Qi(s, a) - TN+1QiT(s, a)
=Y ∙ Es0E{ai}N+ι〜”(ao∣sθ)Lmαχ, Qi(s0,a0) - , maχ, Qi-I(S0,a0
{ai}N+1	{ai}N+1
0
≥0
(27)
)]
(28)
(29)
Hence, by induction We have to ∀i,j, i > j	=⇒	∀s, a, Qi(s, a) ≥	Qj(s,	a).	Since	Q0	=	QN	and
limi→∞ Qi = QN+1, we have than ∀s, a, QN+1(s, a) ≥ QN(s, a). Thus πN +1 is a better policy
than ∏N, and by a simple induction argument, ∏N is a better policy than ∏MM when N > M.
□
A.4 Bounds
Theorem 3.5.	For S ∈ S let,
δ(S)=	c max, l ∖∖Qμ(S,a) - E{ai}N 〜μ(∙∣s)Lmaxlv Qμ (S,b)]
a∈Support(μ(∙∣s))尸	b∈{ai}N
The suboptimality of QN can be Upperbounded as follows,
IlQN-Qμ∣∣∞ ≤ I-Y max Es0 [δ(s0)] ≤ 1-γ max∆(S)	(30)
The same also holds when Qμ is replaced with QN in the definition of ∆.
Proof. The two versions where Δ(s) is defined in terms of QN and Qμ have very similar proofs.
Version with QN
14
Under review as a conference paper at ICLR 2021
Let TQL denote the backup operation in Q-Learning. Let (TQL)m = TQL ◦ TQL ◦. {^^^^	.oTQL. We
m times know the following statements to be true: QN = TNQN = r(s, a) + Y ∙ Es0E{ai}N〜μ(aθ∣sθ) [max QN(s', a0)]	(31) {ai}N TQLQN = r(s, a) + Y ∙ Es0 maX QN(s', a0)	(32) lim (TQLymQN = Q*	(33) m→∞ ∖∖(T QLynF - (t QL)m+1QN\\∞ ≤ γ ∙∖∖(t QL)m+1QN -(TQL)mQN∖∖∞	(34)	
W(TQL)m+ιQN - (tQL)mQN∖∖∞ ≤ Ym ∙ ∖∖tQLQN - qn∖∖∞ Putting these together we have that, ∞ ∖∖QN -Q*∖∖∞ ≤ X ∖∖(TQL)m+1QN - (TQL)mQN∖∖∞ m=0 ∞ ≤ X Ym ∙∖∖TqlQN -QN∖∖∞ m=0 = 1-y ∖∖τ qlQN -QN∖∖∞ = ---max (r(s, a) + Y ∙ Es『max QN(s0, a0)) 1	— Y s,a ∖	a0 μ	) -(r(s, a) + Y ∙ Es0 E{ai}N 〜μ(aθ∣sθ) [max QN (s0, a0)]) {ai}N =1-max Es0 [ma x QN (s0,a0) - E{ai}N 〜μ(a0∣s0)[maN QN 3	(35) (36) (37) (38) (39) (40) , a0)]i	(41)
≤ Tɪ max max QN (s0,a0) - E{ai}N 〜μ(aθ∣sθ)[max QN (sIa')] 1- Y s0	a0	{ai}N	(42)
Version with Qμ
Very similarly we have,
∞
∖∖QN - Q*∣∣∞ ≤ X U(TN)m+1Q* - (TN)mQ*∣∣∞	的
m=0
∞
≤ X Ym ∙∖∖TNQ*-Q*∖∖∞	(44)
m=0
1			∖∖Q*	-TN Q*∖∖∞	(45)
1	-	Y			
	1		max s,a	(r(s, a) + Y ∙ Eso maxQ*(s0, a0))	(46)
1	-	Y			
—
r(s, a) + Y ∙ Es0E{°i}N〜μ(αθ∣s0)[maχ Q*(s', a')])
{ai}N
ɪ max
1 - Y s,a
Es0 maxQ*(s0,a0) - E{°i}N〜“3^)[严a篇 Q*(s'
a	{ai}
,a0)]
≤ ɪ max
1 - Y s0
maχ Q*(s0,aO)-叫"y ~"(a0∣s0)[maN Q*(s0,aO)]
(47)
(48)
(49)
□
15
Under review as a conference paper at ICLR 2021
Corollary A.1. Let Vμ, Qμ, Aμ denote the value, Q, and advantage functions of μ respectively.
When N = 1 we have that,
kQμ - Q*k∞ ≤ YY- max ImaxQμ(s0,a0) - E。，〜“(a，|s，)[Q“(s0,a0)]∣	(50)
∞	1 -γ s0	a0
=1 γ max ∣maxQμ(s0, a0) - Vμ(s0)∣	(51)
1 -γ s，	a，
=[Y max Aμ(s0, a0)	(52)
1 - γ s，,a，
It is interesting how the sub-optimality can be upper-bounded in terms ofa policy’s own advantage
function.
Corollary A.2. (Proof for second part of Theorem 3.3)
Proof. We want to show IimN→∞ QN = Q*. More exactly, What We seek to show is the following,
N→∞ UQN -q*L = 0
or,
NC 0, ∃N, s.t. ∀M ≥ N, IlQN - Q*∣∣∞ <e
Let > 0. Recall,
δ(S) = a∈SupmX(∙∣s)) QXs a) - EWN Hs)(S，则
(53)
(54)
(55)
Let infS μ* (s) = p > 0. Let the lower and upper bounds of rewards be ' and L, and let α = ι-1γ'
and β = 1-—Y L. We have that,
E{ai}N -μ(∙∣s)[ max Qμμ(s,b)] ≥ (I- Pf ∙ α +(1-(1- Pf) ∙ max	QQ(ls.s,a') (56)
b∈{ai}N 尸	a∈Support(μ(∙∣s))尸
Hence ∀S,
∆(s) ≤ (1 — P)N ∙ max	Q*μ(s, a) — (1 — P)N ∙ α	(57)
a∈Support(μ(∙∣s))
=(1 一 P)N ∙ ( max	Q*μ(s, a) 一 α)	(58)
V a∈Support(μ(∙∣s)) N	/
≤ (1 - P)N ∙ (β - α)	(59)
Thus, for large enough N we have that,
IlQN-Qμ∣∣∞ ≤ T-Y max∆(s) <e	(60)
concluding the proof.	□
B Autoregressive Generative Model
The architecture for our autoregressive generative model is inspired by the works of (Metz et al.,
2017; Van de Wiele et al., 2020; Germain et al., 2015). Given a state-action pair from the dataset
(S, a), first an MLP produces a d-dimensional embedding for S, which we will denote by h. Below,
we use the notation ai to denote the ith index of a, and a[:i] to represent a slice from first up to
and not including the ith index, where indexing begins at 0. We use a discretization in each action
dimension. Thus, we discretize the range of each action dimension into N uniformly sized bins, and
represent a by the labels of the bins. Let `i denote the label of the ith action index.
Training We use separate MLPs per action dimension. Each MLP takes in the d-dimensional state
embedding and ground-truth actions before that index, and outputs N logits for the choice over bins.
The probability of a given index’s label is given by,
P('i∣s, a[: i]) = SOftMax(MLPi(d, a[: i]))['i]	(61)
We use standard maximum-likelihood training (i.e. cross-entropy loss).
16
Under review as a conference paper at ICLR 2021
Sampling Given a state s, to sample an action we again embed the state, and sample the action
indices one-by-one.
p('0∣s) = SOftMax(MLPi (d))[40] 'o 〜p('o|s), ao 〜Uniform(Bin corresponding to 'o) p('i∣s) = SOftMax(MLPi(d,a[: i]))['i] 'i 〜p('i∣s, a[: i]), ai 〜Uniform(Bin corresponding to 'i)	(62) (63) (64) (65)
C Algorithm Box
Algorithm 2: Full EMaQ Training Algorithm
Offline dataset D, Pretrain μ(a∣s) on D
Initialize K Q functions with parameters θi , and K target Q functions with parameters θitarget
Ensemble parameter λ, Exponential moving average parameter α
Function Ensemble(values):
L return λ ∙ min(values) + (1 — λ) ∙ max(values)
Function ytarget (s, a, s0 , r, t):
{ai}N 〜μ(a0∣s0)
Qvalues J []
for k J 1 to N do
/* Estimate the value of action a0k	*/
Qvalues.append(Ensemble([Q丁rget(s0, a/k) for all i]))
return r +(1 — t) ∙ Y max(Qvalues)
while not converged do
Sample a batch {(sm , am , sm , rm , tm)}M 〜D
for i = 1, ..., K do
L(θi) = Pm Qi(sm, am) — ytarget(sm,
θi J θi — AdamUpdate L(θi), θi
θiarget J α ∙ θiarget + (1 — α) ∙ θi
am,
D Inconclusive Experiments
D.1 Updating the Proposal Distribution
Akin to the work of (Van de Wiele et al., 2020), we considered maintaining a second proposal
distribution μ that is updated to distill argmaX{ai}N Q(s, a), and sampling from the mixture of μ
and ”. In our experiments however, We did not observe noticeabel gains. This may potentially be
due to the relative simplicity of the Mujoco benchmark domains, and may become more important
in more challenging domains with more uniformly distributed μ(a∣s).
E	Laundry List
• Autoregressive models are slow to generate samples from and EMaQ needs to take many
samples, so it was slower to train than the alternative methods. However, this may be
addressed by better generative models and engineering effort.
17
Under review as a conference paper at ICLR 2021
F	Online RL
EMaQ is also applicable to online RL setting. Combining strong offline RL methods with good
exploration policies has the potential for producing highly sample-efficient online RL algorithms.
Concretely, we refer to online RL as the setting where iteratively, a batch of M environment steps
with an exploration policy are interleaved with M RL updates (Levine et al., 2020; Matsushima
et al., 2020).
EMaQ is designed to remain within the support of the provided training distribution. This however,
is problematic for online RL which requires good exploration interleaved with RL updates. To this
end, first, We modify our autoregressive proposal distribution μ(a∣s) by dividing the logits of all
Softmaxes by T > 1. This has the effect of smoothing the μ(a∣s) distribution, and increasing the
probability of sampling actions from the loW-density regions and the boundaries of the support.
Given this online proposal distribution, a criteria is required by Which to choose amongst sampled
actions. While there exists a rich literature on hoW to design effective RL exploration policies (Weng,
2020), in this Work We used a simple UCB-style exploration criterion (Chen et al., 2017) as folloWs:
Qexplore(s,a) = mean({Qi(s,a)}κ) + β ∙ std({Qi(s, a)}κ)	(66)
Given N sampled actions from the modified proposal distribution, We take the action With highest
Qexplore.
We compare the online variant of EMaQ With entropy-constrained Soft Actor Critic (SAC) With
automatic tuning of the temperature parameter (Haarnoja et al., 2018). For EMaQ We sWept the
temperatures and used a fixed bin size of 40, 8 Q-function ensembles and N = 200. For fairness of
comparisons, We also ran SAC With similar sWeeps over different collection batch sizes and number
of Q-function ensembles. In the fully online setting (trajectory batch size 1, Figure 3a), EMaQ is
already competitive With SAC, and more excitingly, in the deployment-efficient setting3 (trajectory
batch size 50K, Figure 3b), EMaQ can outperform SAC4 5. Figures 4 and 5 present the results for
all hyperparameter settings, for SAC and EMaQ, in the batch size 1 and batch size 50K settings
respectively. In the fully online setting, EMaQ is already competitive with SAC, and more
excitingly, in the deployment-efficient setting, EMaQ can outperform SAC.
G	Offline RL Experimental Details
For each environment and data setting, we train an autoregressive model - as described above 一 on
the provided data With 2 random seeds. These generative models are then frozen, and used by the
downstream algorithms (EMaQ, BEAR, and BCQ) as the base behavior policy (μ(a∣s) in EMaQ)5.
G. 1 Comparing Offline RL Methods
Following the bechmarking efforts of (Wu et al., 2019), the range of clipping factor considered for
BCQ was Φ ∈ {0.005, 0.015, 0.05, 0.15, 0.5}, and the range of target divergence value considered
for BEAR was ∈ {0.015, 0.05, 0.15, 0.5, 1.5}. For both methods, the larger the value of the
hyperparameter is, the more the learned policy is allowed to deviate from the μ(a∣s).
The rest of the hyperparameters use can be found in Table 2. The autoregressive models have the
following architecture sizes (refer to Appendix B for description of the models used). The state
embedding MLP consists of 2 hidden layers of dimension 750 with relu activations, followed by a
linear embedding into a 750 dimensional state representation. The individual MLP for each action
dimension consist of3 hidden layers of dimension 256 with relu activations. Each action dimension
is discretized into 40 equally sized bins.
3By deployment-efficient we mean that less number of different policies need to be executed in the environ-
ment, which may have substantial benefits for safety and otherwise constrained domains (Matsushima et al.,
2020).
4It must be noted that the online variant of EMaQ has more hyperparameters to tune, and the relative
performance is dependent on these hyperparameters, while SAC with ensembles has the one extra ensemble
mixing parameter λ to tune.
5While in the original presentation of BCQ and BEAR the behvior policy is learned online, there is techni-
cally no reason for this to be the case, and in theory both methods should benefit from this pretraining
18
Under review as a conference paper at ICLR 2021
(a)	SAC vs. EMaQ, Trajectory Batch Size 1: For easier visual interpretration we plot a single hyperparameter
setting of EMaQ that tended to perform well across the 4 domains considered. The hyperparameters considered
1.0, τ ∈ {1, 5, 10, 20}. SAC performed worse when using 8 Q-functions as in
were N
200, λ
1.0, β
EMaQ. x-axis unit is 1 million environment steps.
HOPPer-V2
HalfCheetah-v2
Walker2d-v2
Ant-v2
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00	0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
(b)	SAC vs. EMaQ, Trajectory Batch Size 50K: For easier visual interpretration we plot a single hyperparameter
setting of EMaQ that tended to perform well across the 4 domains considered. The hyperparameters considered
were N = 200, λ ∈ {0.75, 1.0}, β ∈ {0.1, 1.0}, τ ∈ {1, 5, 10, 20}. x-axis unit is 1 million environment
steps.
Figure 3: Online RL results under different trajectory batch sizes.
(a) SAC batch 1 results
(b) EMaQ batch 1 results
Figure 4: All results for batch size 1
G.2 EMaQ Ablation Experiment
Hyperparameters are identical to those in Table 2, except batch size is 100 and number of updates is
500K.
G.3 DETAILS FOR TABLE ?? EXPERIMENTS
Generative Model The generative models used are almost identical to the description in Appendix
B, with a slight modification thatMLPi(d, a[: i]) is replace with MLPi(d, Lini(a[: i])) where Lini is
a linear transformation. This change was not necessary for good performance; it was as architectural
detail that we experimented with and did not revert prior generating Table ??. The model dimensions
for each domain are shown in 3 in the following format (state embedding MLP hidden size, state
embedding MLP number of layers, action MLP hidden size, action MLP number of layers, Ouput
19
Under review as a conference paper at ICLR 2021
HalfCheetah-v2
7000
6000
5000
4000
3000
2000
1000
WaIker2d-v2
——SAC (λ=1.0, 2 Q-fns)
——SAC (λ=1.0, 8 Q-fns)
5000 I — SAC (λ=0.75, 2 Q-fns)
(a) SAC batch 50K results
Walker2d-v2
——EMaQ (A= 1.0, 8 Q-fns, β=l.O, τ=l)
——EMaQ (A= 1.0, 8 Q-fns, β=l.O, τ=5)
——EMaQ (A= 1.0, 8 Q-fns, β=l.O, τ=10)
——EMaQ (A= 1.0, 8 Q-fns, β=1.0, τ=20)
…	Walker2d-v2		
		
6000	——EMaQ (A= 1.0, 8	Q-fns,β=0.1, τ=l)
	——EMaQ (A= 1.0, 8	Q-fns, β=0.1, τ=5),
5000	——EMaQ (A= 1.0, 8	Q-fns, β=0.1, τ=10) 4
4000	—EMaQ (A= 1.0, 8	Q-fns, β=0.1. τ=20)/
3000 2000	,一广			.	 		
IOOO		
0 ∣-^ ,_______,_____,_____,____,_____,_____,_____I
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Walker2d-v2
	——EMaQ (λ=0.75, 8 Q-fns, β=1.0. τ= 1) 「EMaQ (λ=0.75, 8 Q-fns, β=1.0, τ=5) ——EMaQ (λ=0.75, 8 Q-fns, 0=1.0, T=IO) —EMaQ (λ=0.75, 8 Q-fns, β=1.0, τ=20) 一一
VValker2d-v2
——IZMaQa=O.75, 8 Q-fns, 0=0.1, T=I)
「EMaQ (λ=0.75, 8 Q-fns, β=0.1, τ=5)
——EMaQ (λ=0.75, 8 Q-fns, β=0.1, τ=10).
EMaQ (Λ=0.75, 8 Q-fns, β=0.1, τ=20)卜
(b) EMaQ batch 50K results
Figure 5: All results for batch size 50K
20
Under review as a conference paper at ICLR 2021
Shared Hyperparameters	
λ Batch Size NUm Updates NUm Q Functions Q Architecture μ lr α	1.0 256 1e6 8 MLP, 3 layers, 750 hid dim, relu 5e-4 	0.995	
EMaQ Hyperparameters	
Q k		1e-4	
BEAR Hyperparameters	
π Architecture Q lr π lr	MLP, 3 layers, 750 hid dim, relu 1e-3 	3e-5	
BCQ Hyperparameters	
π Architecture Q lr π lr	MLP, 3 layers, 750 hid dim, relu 1e-4 	5e-4	
Table 2: Hyperparameters for Mujoco Experiments
size of Lini , number of bins for action discretization). Increasing the number of discretization bins
from 40 (value for standard Mujoco experiments) to 80 was the most important change. Output
dimension of state-embedding MLP is the same as the hidden size.
Hyperparameters Table 3 shows the hyperparameters used for the experiments in Table ??.
Shared Hyperparameters	
λ Batch Size Num Updates Num Q Functions Q Architecture α μ lr Kitchen μ Arch Params Antmaze μ Arch Params Adroit μ Arch Params	1.0 128 1e6 16 MLP, 4 layers, 256 hid dim, relu 0.995 5e-4 (256, 4,128,1,128, 80) (256, 4,128,1,128, 80) (256, 4,128,1,128, 80)
EMaQ Hyperparameters	
Q lr Kitchen N,s Searched Antmaze N,s Searched Adroit N,s Searched	1e-4 {4, 8,16, 32, 64} {50,100,150, 200} 	{16, 32, 64,128}	
BEAR Hyperparameters	
π Architecture Q lr π lr	MLP, 4 layers, 256 hid dim, relu 1e-4 	5e-4	
BCQ Hyperparameters	
π Architecture Q lr π lr	MLP, 4 layers, 256 hid dim, relu 1e-4 	5e-4	
Table 3: Hyperparameters for Table 1 Experiments
21
Under review as a conference paper at ICLR 2021
Random
Medium
Mixed
〃(a ⑸ 5 10 25 50 100 200 400	〃(a ⑸ 5 10 25 50 100 200 400	〃向S) 5 10 25 50 100 200 400	〃向S) 5 10 25 50 100 200 400
EMaQ	EMaQ	EMaQ	EMaQ
Figure 6: Results for evaluating EMaQ on D4RL (Fu et al., 2020b) benchmark domains when using the
described VAE implementation, with N ∈ {5,10, 25, 50,100, 200, 400}. Values above μ(a∣s) represent the
result of evaluating the base behavior policies. Horizontal green lines represent the reported performance of
BEAR in the D4RL benchmark (apples to apples comparisons in Figure 7).
H VAE Results
H.1 Implementation
We also ran experiments with VAE ParameteriZations for μ(a∣s). To be approximately matched in
parameter count with our autoregressive models, the encoder and decoder both have 3 hidden layers
of siZe 1024 with relu activations. The dimension of the latent space was twice the number of action
dimensions. The decoder outputs a vector v which, and the decoder action distribution is defined
to be N(Tanh(v), I). When sampling from the VAE, following prior work, samples from the VAE
prior (spherical normal distribution) were clipped to the range [-0.5, 0.5] and mean of the decoder
distibution was used (i.e. the decoder distribution was not sampled from). The KL divergence
loss term was weighted by 0.5. This VAE implementation was the one used in the benchmarking
codebase of (Wu et al., 2019), so we did not modify it.
H.2 Results
As can be seen in Figure 6, EMaQ has a harder time improving upon μ(a∣s) when using the VAE
architecture described above. However, as can be seen in Figure 7, BCQ and BEAR do show some
variability as well when switching to the VAEs. Since as an algorithm EMaQ is much more reliant
on μ(a∣s), our hypothesis is that if it is true that the autoregressive models better captured the action
distribution, letting EMaQ not make poor generaliZations to out-of-distribution actions. Figures 8
and 9 show autoregressive and VAE results side-by-side for easier comparison.
I EMaQ Medium-Expert Setting Results
In HalfCheetah, increasing N significantly slows down the convergence rate of the training curves;
while large N s continue to improve, we were unable to train them long enough for convergence.
In Walker, for EMaQ, BCQ, and most hyperparameter settings of BEAR, training curves have a
prototypical shape of a hump, where performance improves up to a certain high value, and then
continues to fall very low. In Hopper, for higher values of N in EMaQ we observed that increasing
batch siZe from 100 to 256 largely resolved the poor performance, but for consistency we did not
alter Figure 1 with these values.
J Comparison with Softmax Backup Operators
22
Under review as a conference paper at ICLR 2021
Random
Medium
Mixed
Figure 7: Comparison of EMaQ, BCQ, and BEAR on D4RL (Fu et al., 2020b) benchmark domains when
using When using the described VAE implementation for μ(a∣s). For both BCQ and BEAR, from left to right
the allowed deviation from μ(a∣s) increases. Horizontal green lines represent the reported performance of
BEAR in the D4RL benchmark.
We thank one of our ICLR 2021 reviewers for the motivation for this section. For clarity of writing,
we will write the forms for deterministic dynamics and remove the expectations over the next state.
An interesting connection to our proposed backup operators would be the following Softmax backup
operator with similarities to EMaQ,
TμαQ(s, a) ：= r(s, a) + Esoft(a0|s0) [Q(s0, a0)]	(67)
Soft(QIs) Y μ(a∣s) ∙ exp(α ∙ Q(s, a))	(68)
As suggested by our reviewer, the policy corresponding to sof t(a|s) is a policy that aims to max-
imize Q-values, subject to a KL-constraint between itself and the policy μ(a∣s). The looser the
constraint, the larger the effective α and the farther the policy will be from μ. One approach to
Monte Carlo estimation of the expectation on the right hand side could be to take samples using
methods from the energy-based generative modelling literature.
An alternative approach which will more closely resembles EMaQ is to use self-normalized impor-
tance sampling,
Ta Q(s,a)	：= r(s, a) + Esof t(a0 |s0) [Q(s , a )]	(69)
	=r(s, a) +	£	Wi ∙ Q(s0,a0)	(70)
	{ai }n 〜μ(a0∣s0)	
Wi	=μ(Qi |s0) ∙ exp(a ∙ Q(S0,ai))	(71)
	一	μ(Qi |s0)	
wi	Wi P Wi	(72)
	= sof tmax(α ∙ Q(s0, a0i))[i]	(73)
In this form, the soft backup is similar to EMaQ, where instead of taking ths max Q-value over the N
samples, we take an average over the N Q-values, weighted by the softmax probabilities in equation
73. For a given N, the α = 0 would be equivalent to Q-evaluation of the policy μ(a∣s), and as
α → ∞, the soft backups approach EMaQ backups.
In Figure 10 we present empirical results with the soft backup operators, under a large range α ∈
{1, 4, 8, 16, 32, 64, 128, 256, 512, 1024}, in the Halfcheetah settings. The EMaQ and soft-EMaQ
were run with the same architectures, but were smaller than the ones used for the results in the
main text. We used the same checkpoints of the generative models as for the results in the main
23
Under review as a conference paper at ICLR 2021
text. The test-time policy for both approaches is the same, sampling N actions and taking the
argmax action under the ensemble Q-value. The only difference between the EMaQ and soft-EMaQ
implementations was a one-line change to replace max with a softmax average of the Q-values.
Some interesting observations are the following: As anticipated, the soft EMaQ backups approach
EMaQ as the value ofα is increased. However, the necessary value ofα to match the performance of
EMaQ can be quite large. In the medium-expert setting, where figure 1 suggests challenges arising
from the combination of large Ns and function-approximators, we did not gain much advantage from
soft backups, and only α ∈ {8, 16, 32} seem to have provided some mitigation of the problem for
N = 25. Since the soft backup introduces an additional hyperparameter that cannot be determined
ahead of time, and does not seem to provide an advantage (at least in the limited Halfcheetah settings
considered), from a practical perspective, we would prefer to use the regular EMaQ backup.
K Qualitative Differences in Training Curves
We have sometimes observed that the curves representing agent performance throughout training can
be significantly more stable under EMaQ in comparison to BEAR and BCQ. A domain where the
differences are particularly striking are the antmaze-umaze and antmaze-umaze-diverse
domains. In figure 4 we have included plots of agent performance during training under the variety
of considered hyperparameters and random seeds. It can be seen that in these two domains, initially
the BCQ agents improves in performance close to the performance of EMaQ, and the drastically
degrades with more training. In constrast, EMaQ agents remain stable even after twice as many
training iterations as BCQ, which may indicate the downside of the heuristic perturbation model for
constraining actions.
antmaze-umaze
antmaze-umaze-diverse
Table 4: Comparison of agent returns throughout training, under the variety of hyperparameters and
and random seeds, in the small ant domains. We observe that EMaQ is significantly more stable
than BCQ in these domains, even though the values of N in EMaQ were fairly large for these plots
N ∈ {50, 100, 150, 200}.
L Larger Plots for Visibility
Due to larger size of plots, each plot is shown on a separate page below. For ablation results, see
Figure 11. For MuJoCo results, see Figure 12.
24
UnderreVieW as a Confere
4000
3000
2000
1000
VAE"(a⑸	5	10 25 50 100 200 400	5	10 25 50 100 200 400
Auto EMaQ	VAE EMaQ
3000
2000
1000
0
VAE μ(a∖s) 5	10 25 50 100 200 400	5	10 25 50 10(
Auto EMaQ	VAE EMaQ
Medium
Figure 8: Results with both autoregressive and VAE models in one plot for easier comparison.
Random
UnderreVieW as a Confere
Figure 9: Results with both autoregressive and VAE models in one plot for easier comparison.
,ə'-eM
2500
2000
1500
1000
5
qnlφ3lp±ell
random
4500
4000
3500
medium
5000
4000-
3000-
2000
1000
o-
mιxed
8000
6000
4000
2000
medium-expert
UnderreVieW as a ConferenCe PaPersICLR 2021
5	25	100
N
5	25	100
N
5	25	100
N
5	25	100
N
Figure 10:	Comparison of Soft-EMaQ with EMaQ under a large range of hypeιparameters in the Halfcheetah domain.
28
3000
2000
1000
Random
O
llfəəlojmn
μ(a∖s) 5	10	25	50	100 200 400
EMaQ
Medium
5000
5000
4500
4000
4000
3000
I
2000
μ(a∖s) 5
50	100 200 400
EMaQ
Mixed

μ(a∣s) 5	10	25	50	100 200 400
EMaQ
8000
6000
4000
ɪ
MedIUm-EXPert
X---------
I
μ(a∣s) 5	10	25	50	100 200 400
EMaQ
Oooo
Ooo
3 2 1
2 一eM
1500
3000
2000
1000
1000
μ(a∣s) 5
1500
1000
500
ɪ I
50	100 200 400
EMaQ
50	100 200 400
EMaQ
μ(a∖s) 5	10	25	50	100 200 400
EMaQ
μ(a∖s) 5	10	25	50	100 200 400
EMaQ
o∣ ；
μ(a∣s) 5
0 5 0 5 0
0 7 5 2 0
4 3 3 3 3
.IOddOH
500
4000
I
ɪ I
3000
2000
1000
I
Il
μ(a∣s) 5	10	25	50	100 200 400
EMaQ
1000
900
800
700
600
500
μ(a∣s) 5	10	25	50	100 200 400
EMaQ
3000
2000
1000
μ(a∣s) 5	10	25	50	100 200 400
EMaQ
ɪ
ɪ I
o∣ 二	^	,	I ,	二 二一
μ(a∖s) 5	10	25	50	100	200	400
EMaQ
UnderreVieW as a COnferenCe PaPereICLR 2021
Figure 11:	Results for evaluating EMaQ on D4RL (Fu et al., 2020b) benchmark domains, with N ∈ {5,10, 25,50,100, 200,400}. Values above μ(α∣s) represent
the result of evaluating the base behavior policies. Horizontal green lines represent the reported performance of BEAR in the D4RL benchmark (apples to apples
comparisons in Figure 2). Refer to main text (Section 5.1) for description of color-coding.

RandOrn

-lə* WM
0.005).0150.05 0.15 05
BCQ
ɪlɪɪl
0.003).0150.05 0.15 0.5
BCQ
0.003).0150.05 0.15 05
BCQ
H
0.6150.05 0.15 05 L5
BEAR
ɪ
0.0150.05 0.15 0.5 1.5
BEAR
'ɪɪ
0.6150.05 0.15 05 L5
BEAR
6000
0.003).6150.65 0.15 05
BCQ
Mediurn
0.0150.05 0.15 0.5 1.5
BEAR
0.005).6150.65 0.15 0.5
BCQ
0.0150.05 0.15 0.5 1.5	5 200
BEAR	EMaQ
Ii-I
0.6150.05 0.15 05 L5
BEAR
6000
5000
o ɪ ɪ I .
3000
2000
IOOO
0.003).0150.05 0.15 0.5
μ(a∣s)	BCQ
Mixed
0.0150.05 0.15 0.5 1.5	5 200
BEAR	EMaQ μ(a∣s)
1500
1250
1000
:][l[
ɪ :1 I
0.003).0150.05 0.15 0.5
μ(a∣s)	BCQ
2500
2000
1500
ɪ	1000- I ɪ ɪ
500
0.003).0150.05 0.15 0.5
μ(a∣s)	BCQ
3
0.0150.05 0.15 0.5 1.5	5 200
BEAR	EMaQ μ(a∣s)
ɪɪɪɪ
0.6150.05 0.15 05 15
BEAR
5 200
EMaQ μ(a∣s)
MediUrn-EXPert
10000
8000
6000
4000
2000
0.005).0150.05 0.15 0.5	0.0150.05 0.15 0.5 1.5	5 200
BCQ	BEAR	EMaQ
I
ɪɪɪɪl--
0.005).0150.05 0.15 0.5
BCQ
0.0150.05 0.15 0.5 1.5
BEAR
∣I
MlLE
3500
3000
2500
2000
1500
1000
500
0
0.005).6150.05 0.15 0.5	0.0150.05 0.15 0.5 1.5	5 200
BCQ	BEAR	EMaQ
Figure 12: Comparison of EMaQ, BCQ, and BEAR on D4RL (Fu et al., 2020b) benchmark domains when using our proposed autoregressive μ(a∖s). For both
BCQ and BEAR, from left to right the allowed deviation from μ(a∖s) increases. Horizontal green lines represent the reported performance of BEAR in the D4RL
benchmark. Color-coding follows Figure 1.
UnderreVieW as a COnferenCe PaPersICLR 2021