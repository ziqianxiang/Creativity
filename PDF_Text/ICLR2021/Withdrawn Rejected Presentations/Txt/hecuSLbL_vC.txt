Under review as a conference paper at ICLR 2021
Generalisation Guarantees for Continual
Learning with Orthogonal Gradient Descent
Anonymous authors
Paper under double-blind review
Ab stract
In Continual Learning settings, deep neural networks are prone to Catastrophic
Forgetting. Orthogonal Gradient Descent was proposed to tackle the challenge.
However, no theoretical guarantees have been proven yet. We present a theoretical
framework to study Continual Learning algorithms in the Neural Tangent Kernel
regime. This framework comprises closed form expression of the model through
tasks and proxies for Transfer Learning, generalisation and tasks similarity. In this
framework, we prove that OGD is robust to Catastrophic Forgetting then derive the
first generalisation bound for SGD and OGD for Continual Learning. Finally, we
study the limits of this framework in practice for OGD and highlight the importance
of the Neural Tangent Kernel variation for Continual Learning with OGD.
1	Introduction
Continual Learning is a setting in which an agent is exposed to multiples tasks sequentially (Kirk-
patrick et al., 2016). The core challenge lies in the ability of the agent to learn the new tasks while
retaining the knowledge acquired from previous tasks. Too much plasticity (Nguyen et al., 2018) will
lead to catastrophic forgetting, which means the degradation of the ability of the agent to perform the
past tasks (McCloskey & Cohen 1989, Ratcliff 1990, Goodfellow et al. 2014). On the other hand, too
much stability will hinder the agent from adapting to new tasks.
While there is a large literature on Continual Learning (Parisi et al., 2019), few works have addressed
the problem from a theoretical perspective. Recently, Jacot et al. (2018) established the connection
between overparameterized neural networks and kernel methods by introducing the Neural Tangent
Kernel (NTK). They showed that at the infinite width limit, the kernel remains constant throughout
training. Lee et al. (2019) also showed that, in the infinite width limit or Neural Tangent Kernel (NTK)
regime, a network evolves as a linear model when trained on certain losses under gradient descent.
In addition to these findings, recent works on the convergence of Stochastic Gradient Descent for
overparameterized neural networks (Arora et al., 2019) have unlocked multiple mathematical tools to
study the training dynamics of over-parameterized neural networks.
We leverage these theoretical findings in order to to propose a theoretical framework for Continual
Learning in the NTK regime then prove convergence and generalisation properties for the algorithm
Orthogonal Gradient Descent for Continual Learning (Farajtabar et al., 2019).
Our contributions are summarized as follows:
1.	We present a theoretical framework to study Continual Learning algorithms in the Neural
Tangent Kernel (NTK) regime. This framework frames Continual Learning as a recur-
sive kernel regression and comprises proxies for Transfer Learning, generalisation, tasks
similarity and Curriculum Learning. (Thm. 1, Lem. 1 and Thm. 3).
2.	In this framework, we prove that OGD is robust to forgetting with respect to an arbitrary
number of tasks under an infinite memory (Sec . 5, Thm. 2).
3.	We prove the first generalisation bound for Continual Learning with SGD and OGD. We
find that generalisation through tasks depends on a task similarity with respect to the NTK.
(Sec. 5, Theorem 3)
4.	We study the limits of this framework in practical settings, in which the Neural Tangent
Kernel may vary. We find that the variation of the Neural Tangent Kernel impacts negatively
1
Under review as a conference paper at ICLR 2021
the robustness to Catastrophic Forgetting of OGD in non overparameterized benchmarks.
(Sec. 6)
2	Related works
Continual Learning addresses the Catastrophic Forgetting problem, which refers to the tendency of
agents to ”forget” the previous tasks they were trained on over the course of training. It’s an active
area of research, several heuristics were developed in order to characterise it (Ans & Rousset 1997,
Ans & Rousset 2000, Goodfellow et al. 2014, French 1999, McCloskey & Cohen 1989, Robins
1995, Nguyen et al. 2019). Approaches to Continual Learning can be categorised into: regularization
methods, memory based methods and dynamic architectural methods. We refer the reader to the
survey by Parisi et al. (2019) for an extensive overview on the existing methods. The idea behind
memory-based methods is to store data from previous tasks in a buffer of fixed size, which can then
be reused during training on the current task (Chaudhry et al. 2019, Van de Ven & Tolias 2018).
While dynamic architectural methods rely on growing architectures which keep the past knowledge
fixed and store new knowledge in new components, such as new nodes or layers. (Lee et al. 2018,
Schwarz et al. 2018) Finally, regularization methods regularize the objective in order to preserve the
knowledge acquired from the previous tasks (Kirkpatrick et al. 2016, Aljundi et al. 2018, Farajtabar
et al. 2019, Zenke et al. 2017).
While there is a large literature on the field, there is a limited number of theoretical works on
Continual Learning. Alquier et al. (2017) define a compound regret for lifelong learning, as the regret
with respect to the oracle who would have known the best common representation for all tasks in
advance. Knoblauch et al. (2020) show that optimal Continual Learning algorithms generally solve
an NP-HARD problem and will require perfect memory not to suffer from catastrophic forgetting.
Benzing (2020) presents mathematical and empirical evidence that the two methods - Synaptic
Intelligence and Memory Aware Synapses - approximate a rescaled version of the Fisher Information.
Continual Learning is not limited to Catastrophic Forgetting, but also closely related to Transfer
Learning. A desirable property of a Continual Learning algorithm is to enable the agent to carry the
acquired knowledge through his lifetime, and transfer it to solve new tasks. A new theoretical study
of the phenomena was presented by Liu et al. (2019). They prove how the task similarity contributes
to generalisation, when training with Stochastic Gradient Descent, in a two tasks setting and for
over-parameterised two layer RELU neural networks.
The recent findings on the Neural Tangent Kernel (Jacot et al., 2018) and on the properties of
overparameterized neural networks (Du et al. 2018, Arora et al. 2019) provide powerful tools to
analyze their training dynamics. We build up on these advances to construct a theoretical framework
for Continual Learning and study the generalisation properties of Orthogonal Gradient Descent.
3	Preliminaries
Notation We use bold-faced characters for vectors and matrices. Weuse ∣∣∙k to denote the Euclidean
norm of a vector or the spectral norm of a matrix, and ∣∣ ∙ ∣f to denote the Frobenius norm of a matrix.
We use h∙, ∙i for the Euclidean dot product, and(•, ∙)h the dot product in the Hilbert space H. We
index the task ID by τ . The ≤ operator if used with matrices, corresponds to the partial ordering over
symmetric matrices. We denote N the set of natural numbers, R the space of real numbers and N? for
the set Nr {0}. We use ㊉ to refer to the direct sum over Euclidean spaces.
3.1	Continual Learning
Continual Learning considers a series of tasks {T1, T2, . . .}, where each task can be viewed as a
separate supervised learning problem. Similarly to online learning, data from each task is revealed
only once. The goal of Continual Learning is to model each task accurately with a single model. The
challenge is to achieve a good performance on the new tasks, while retaining knowledge from the
previous tasks (Nguyen et al., 2018).
We assume the data from each task Tτ, τ ∈ N?, is drawn from a distribution Dτ. Individual samples
are denoted (xτ,i, yτ,i), where i ∈ [nτ]. For a given task Tτ, the model is denoted fτ, we use the
2
Under review as a conference paper at ICLR 2021
superscript (t) to indicate the training iteration t ∈ N, while we use the superscript ? to indicate the
asymptotic convergence. For the regression case, given a ridge regularisation coefficient λ ∈ R+ , for
all t ∈ N, we write the train loss for a task Tτ as :
nτ
LT(WKt)) = X(fTt)(xτ,i) - yτ,i)2 + λ∣∣wτ⑴-wT-ι∣∣2.
i=1
3.2	OGD for Continual Learning
Let Tτ the current task, where T ∈ N?. For all i ∈ [nτ], let vτ,i = NW fT-∖(xτ-ι,i), which is the
Jacobian of task TT. We define Eτ = vec({vτ,i , i ∈ [nτ]}), the subspace induced by the Jacobian.
The idea behind OGD (Farajtabar et al., 2019) is to update the weights along the projection of the
gradient on the orthogonal space induced by the Jacobians over the previous tasks Ei ㊉...㊉ ET-ι.
The update rule at an iteration t ∈ N? for the task TT is as follows :
WT(t + 1) = WT (t) - ηΠE⊥ NWLT(WT(t)).
The intuition behind OGD is to “preserve the previously acquired knowledge by maintaining a space
consisting of the gradient directions of the neural networks predictions on previous tasks” (Farajtabar
et al., 2019). Throughout the paper, we only consider the OGD-GTL variant which stores the gradient
with respect to the ground truth logit.
3.3	Neural Tangent Kernel
In their seminal paper, Jacot et al. (2018) established the connection between deep networks and
kernel methods by introducing the Neural Tangent Kernel (NTK). They showed that at the infinite
width limit, the kernel remains constant throughout training. Lee et al. (2019) also showed that a
network evolves as a linear model in the infinite width limit when trained on certain losses under
gradient descent.
Throughout our analysis, we make the assumption that the neural network is overparameterized, and
consider the linear approximation of the neural network around its initialisation:
f(t)(x) ≈ f(0)(x) + NWf (0)(x)T (W(t) - W(0)).
4	Convergence - Continual Learning as a recursive Kernel
Regression
In this section, we derive a closed form expression for the Continual Learning models through tasks.
We find that Continual Learning models can be expressed with recursive kernel ridge regression
across tasks. We also find a that the NTK of OGD is recursive with respect to the projection of its
feature map on the tasks’ spaces. The result is presented in Theorem 1, a stepping stone towards
proving the generalisation bound for OGD in Sec. 5.
4.1	Convergence Theorem
Theorem 1 (Continual Learning as a recursive Kernel Regression)
Given T1, . . . , TT a sequence of tasks. Fix a learning rate sequence (ηT)T∈[T] and a ridge regulari-
sation coefficient λ ∈ R+. If, for all τ, the learning rate satisfies ηT <
all τ, WT (t) converges linearly to a limit solution WT? such that
∣∣Kτ (Xτ,Xτ ) + λI∣∣ > then f0r
f?(x) = fT-i(x) + Kτ(x, XT)t(KT(XT, XT) + λI)-1yτ,
3
Under review as a conference paper at ICLR 2021
where
0	0T
κτ (x, x0) = φτ (x)φτ (x0)T,
yτ = yτ - yτ-1→τ,
yτ-1→τ = fT-1(Xτ ),
e z ʌ	fVwf?(x) ∈ Rd for SGD ,
φτ(X) = [TτVwf?(x) ∈ Rd-MT for OGD.
ττ
and {Tτ ∈ Rd-Mτ,d, τ ∈ [T]} are projection matrices from Rd to ( L Ek)⊥ and Mτ = dim( L Ek)
k=1	k=1
The theorem describes how the model fτ? evolves across tasks. It is recursive because the learning
is incremental. For a given task TT, fT-ι(x) is the knowledge acquired by the agent UP to the task
Tτ-ι. At this stage, the model only fits the residual NT = y - YT-ι→τ, WhiCh complements the
knowledge acquired through Previous tasks. This residual is also a Proxy for task similarity. If the
tasks are identical, the residual is equal to zero. The knowledge increment is captured by the term:
κT (x, XT)T (κT+1(XT, XT) + λI)-1yNT. Finally, the task similarity is computed with respect to the
most recent feature map φeT , and κT is the NTK with respect to the feature map φeT .
Remark 1 The recursive relation from Theorem 1 can also be written as a linear combination of
kernel regressors as follows:
T
f？ (X) = X f?(X),
k=1
where
f?(x) = Kk(x, Xk)T(Kk(Xk, Xk) + λI)-1Yk.
4.2	Distance from initialisation through tasks
As described in Sec. 4.1, NyT is a residual. It is equal to zero if the model fT?-1 makes perfect
predictions on the next task TT . The more the next task TT is different, the further the neural
network needs to move from its previous state in order to fit it. Corollary 1 tracks the distance from
initialisation as a function of task similarity.
Corollary 1 For SGD, and for OGD under the additional assumption that {TT, τ ∈ [T]} are
orthonormal,
wT?+1 - wT?	= NyTT+1 (K(XT +1, XT+1) + λI)-1 K(XT +1, XT +1)(K(XT +1, XT+1) + λI)-1yNT +1,
Remark 2 Corollary 1 can be applied to get a similar result to Theorem 3 by Liu et al. (2019). In
this remark, we consider mostly their notations. Their theorem states that under some conditions, for
2-layer neural networks with a RELU activation function, with probability no less than 1 - δ over
random initialisation,
kW(P) - W(Q)kF ≤ JyT-qH∞-1Np→Q + e,
where, in their work:
yP→Q = HP∞Q,T HP∞-1yP,
yNP→Q = yQ - yP→Q .
Note that HP∞ is a Gram matrix, which also corresponds to the NTK of the neural network they
consider. We see an analogy with our result, where we work directly with the NTK, with no assumptions
on the neural network. One important observation is that, to our knowledge, since there are no
guarantees for the invertibility of our Gram matrix, we add a ridge regularisation to work with a
regularised matrix, which is then invertible. In our setting, by considering λ → 0, and with the
additional assumption of invertibility of HT,0,which is valid in the two-layer overparameterized RELU
neural network considered in the setting of Liu et al. (2019), we can recover a similar approximation.
4
Under review as a conference paper at ICLR 2021
5	OGD : Learning without forgetting, provably
In this section, we study the generalisation properties of OGD building-up on Thm. 1. First, we prove
that OGD is robust to catastrophic forgetting with respect to all previous tasks (Theorem 2). Then, we
present the main generalisation theorem for OGD (Thm. 3). The theorem provides several insights
on the relation between task similarity and generalisation. Finally, we present how the Rademacher
complexity relates to task similarity across a large number of tasks (Lemma 1). The lemma states
that the more dissimilar tasks are, the larger the class of functions explored by the neural network,
with high probability. This result highlights the importance of the curriculum for Continual Learning.
5.1	Memorisation property of OGD
The key to obtaining tight generalisation bounds for OGD is Theorem 2.
Theorem 2 (No-forgetting Continual Learning with OGD) Given a task Tτ, for all xk,i ∈ Dk,
a sample from the training data of a previous task Tk, given that the Jacobian of xk,i belongs to
OGD’s memory, it holds that:
fT(χk,i) = fk(χk,i).
As motivated by Farajtabar et al. (2019), the orthogonality of the gradient updates aims to preserve
the acquired knowledge, by not altering the weights along relevant dimensions when learning new
tasks. Theorem 2 implies that, given an infinite memory, the training error on all samples from the
previous tasks is unchanged, when training with OGD.
5.2	Generalisation properties of SGD and OGD
Now, we state the main generalisation theorem for OGD, which provides generalisation bounds on
the data from all the tasks, for SGD and OGD.
Theorem 3 (Generalisation of SGD and OGD for Continual Learning)
Let {T1, . . . TT} be a sequence of tasks. Let be {D1, . . . , DT} the respective distributions over
Rd × {-1, 1}. Let {(xτ,i, yτ,i), i ∈ [nt], τ ∈ [T]} be i.i.d. samples from Dτ, τ ∈ [T]. Denote
Xτ = (xτ,1, . . . , xτ,nτ), yτ = (yτ,1, . . . , yτ,nτ). Consider the kernel ridge regression solution fT?,
then, for any loss function ` : R × R → [0, c] that is c-Lipschitz in the first argument, with probability
at least 1 - δ,
[nτyτ(KT(XT, XT)+λI)-1Yτ+Rτ+3cqlogn/), for OGD, τ ∈ [1,T],
LDτ(fT) ≤ { n2 NT (Kτ (Xτ, XT)+ λI)-1yτ + RT + 3c/ l⅛), for SGD,τ = T,
[WyT(kT(Xt, XT) + λI)-1yT + 春 PT=T+1 Hk,T + RT + 3Cqo暮δ,	for SGD,τ < T.
where
NT = yτ- fT-ι(Xτ),
Rt = XX Jtr(KT(XrXT 〃 NT (KT(XT, XT) + λi)-1yτ,
T=1	T
Hk,T =yNkT(kk(Xk,Xk)+λI)-1kk(Xk,XT)kk(XT,Xk)(kk(Xk,Xk)+λI)-1Nyk.
Theorem 3 shows that the generalisation bound for OGD is tighter than SGD. This results from Thm.
2, which states that OGD is robust to Catastrophic Forgetting. The bound is looser for SGD over the
past tasks through the forgetting term that appears.
The bound of Theorem 3 comprises the following main terms :
•	n2Nt(kt(XT, XT) + λI)-1yτ : this term is due to the regularisation and describes that the
empirical loss is not equal to zero due to the regularisation term. This term tends to zero in
case there is no regularisation. For interpretation purposes, in App. D.2.7, we show that the
empirical loss tends to zero in the no-regularisation case.
5
Under review as a conference paper at ICLR 2021
•	RT captures the impact of task similarity on generalisation which we discuss in Sec. 5.3.
•	PkT=τ +1 Hk,τ is a residual term that appears for SGD only. It is due to the catastrophic
forgetting that occurs with SGD. It also depends on the tasks similarity.
These bounds share some similarities with the bounds derived by Arora et al. (2019), Liu et al. (2019)
and Hu et al. (2019), where in these works, the bounds were derived for supervised learning settings,
and in some cases for two-layer RELU neural networks. Similarly, the bounds depend on the Gram
matrix of the data, with the feature map corresponding to the NTK.
5.3	The impact of task similarity on generalisation
Now, we state Lemma 1, which tracks the Rademacher complexity through tasks.
Lemma 1 Keeping the same notations and setting as Theorem 3, the Rademacher Complexity can
be bounded as follows:
R(FT) ≤ XXO ( jtr(KT(XT, Xt))y (KT(XT, XT) + λi)-1yτ);
where FT is the function class covered by the model up to the task Tτ
The right hand side term RT in the upper bound of the generalisation theorem follows directly from
Lemma 1, and it draws a complexity measure for Continual Learning. It states that the upper
bound on the Rademacher complexity increases when the tasks are dissimilar. We define the NTK task
dissimilarity between two subsequent tasks TT-ι and TT as ST-ι→τ = yT(KT(XT, XT) + λI)-1yT.
This dissimilarity is a generalisation of the term that appears in the upper bound of Thm. 2 by Liu
et al. 2019. The knowledge from the previous tasks is encoded in the kernel KT , through the feature
map Φt. As an edge case, if two successive tasks are identical, ST-i→t = 0 and the upper bound
does not increase.
Implications for Curriculum Learning We also observe that the upper bound depends on the
task ordering, which may provide a theoretical explanation on the importance of learning with a
curriculum (Bengio et al. (2009)). In the following, we present an edge case which provided an
intuition on how the bound captures the importance of the order. Consider two dissimilar tasks T1
and T2. A sequence of tasks alternating between T1 and T2 will lead to a large upper bound, as
explained in the first paragraph, while a sequence of tasks concatenating two sequences of T1 then
T2 will lead to a lower upper bound.
6	The impact of the NTK variation on OGD
In the previous section, we demonstrated that under the NTK regime and an infinite memory, OGD is
provably robust to catastrophic forgetting. In practical settings, these two assumptions do not hold,
therefore, in this section, we study the limits of the NTK regime. We present OGD+, a variant of
OGD we use to study the importance of the NTK variation for Continual Learning in practice.
In order to decouple the NTK variation phenomena in our experiments, we propose then study the
OGD+ algorithm, which is designed to be more robust to the NTK variation, through updating its
orthonormal basis with respect to all the tasks at the end of each task, as opposed to OGD.
Algorithm 1 presents the OGD+ algorithm, we highlight the differences with OGD in red. The main
difference is that OGD+ stores the feature maps with respect to the samples from previous tasks, in
addition to the feature maps with respect to the samples from the current task, as opposed to OGD.
This small change is motivated by the variation of the NTK in practice. In order to compute the
feature maps with respect to the previous samples, OGD+ saves these samples in a dedicated memory,
we call this storage the samples memory. This memory comes in addition to the orthonormal feature
maps memory. The only role of the samples memory is to compute the updated feature maps at the
beginning of each task.
natbib
6
Under review as a conference paper at ICLR 2021
Algorithm 1: OGD+ for Continual Learning
Input : A task sequence T1, T2,..., learning rate η
1.	Initialize SJ — {} ； SD J {}; W J w0
2.	for Task ID τ = 1, 2, 3, . . . do
repeat
g J Stochastic Batch Gradient for Tτ at w;
g = g - Pv∈SJ Projv (g)；
w J w 一 ηg
until convergence;
Sample S ⊂ SD;
for (x, y) ∈ Dτ S and k ∈ [1, c] s.t. yk = 1 do
U U J Vfτ(x; W) - Pv∈SJ ProjV(Vfτ(x; W)) SJ J SJ S{u}
end
Sample D ⊂ Dτ ;
Update S D J S D S D
end
7 Experiments
We study the validity Theorem 2 which states that under the given conditions, OGD is perfectly robust
to Catastrophic Forgetting (Sec. 7.1). We find that the Catastrophic Forgetting of OGD decreases
with over-parameterization. Then, we perform an ablation study using OGD+ in order to study the
applicability and limits of the constant Jacobian assumption in practice (Sec. 7.2). We find that the
assumption holds for some benchmarks, and that updating the Jacobian (OGD+) is critical for the
robustness of OGD in the non-overparameterized benchmarks. Finally, we present a broader picture
of the performance of OGD+ against standard Continual Learning baselines (Sec. 7.3)
Benchmarks We use the standard benchmarks similarly to Goodfellow et al. (2014) and Chaudhry
et al. (2019). Permuted MNIST (Goodfellow et al., 2014) consists of a series of MNIST supervised
learning tasks, where the pixels of each task are permuted with respect to a fixed permutation. Rotated
MNIST (Farajtabar et al., 2019) consists of a series of MNIST classification tasks, where the images
are rotated with respect to a fixed angle, monotonically. We increment the rotation angle by 5 degrees
at each new task. Split CIFAR-100 (Chaudhry et al., 2019) is constructed by splitting the original
CIFAR-100 dataset (Krizhevsky, 2009) into 20 disjoint subsets, where each subset is formed by
sampling without replacement 5 classes out of 100. In order to assess the robustness to catastrophic
forgetting over long tasks sequences, we increase the length of the tasks streams from 5 to 15 for
the MNIST benchmarks, and consider all the 20 tasks for the Split CIFAR-100 benchmark. We also
use the CUB200 benchmark (Jung et al., 2020) which contains 200 classes, split into 10 tasks. This
benchmark is more overparameterized than the other benchmarks.
Evaluation Denoting aT,τ the accuracy of the model on the task Tτ after being trained on the task
TT, we track two metrics introduced by Chaudhry et al. (2019) and Lopez-Paz & Ranzato (2017) :
AVERAGE ACCURACY (AT) is the average accuracy after the model has been trained on the task
TT :
1T
AT = TEaT,τ.
τ=1
AVERAGE FORGETTING (FT) is the average forgetting after the model has been trained on the task
TT :
1	T-1
FT =	~~7 T max (at,τ — aτ,τ).
T T - 1 t∈{1,...,T -1} t,τ T,τ
τ=1
7
Under review as a conference paper at ICLR 2021
Over-parameterization We track over-parameterization of a given benchmark as the number
of trainable parameters over the average number of samples per task (Arora et al., 2019) :
O =；—.
PτT=1 nτ /T
Architectures For the MNIST benchmarks, we use the same neural network architectures as
Farajtabar et al. (2019). However, since the OGD algorithm doesn’t scale to large neural networks
due to memory limits, we considered smaller scale neural networks for the CIFAR100 and CUB200
benchmarks. For CIFAR100, we use the LeNet architecture (Lecun et al., 1998). For CUB200, we
keep the pretrained AlexNet similarly to Jung et al. (2020), then freeze the features layers and replace
the default classifier with a smaller neural network (App. F.1.1).
7.1	Ablation study : For OGD, Catastrophic Forgetting decreases with
Overparameterization (Thm. 2)
Theorem 2 states that in the NTK regime, given an infinite memory, the train error of OGD is
unchanged. We study the impact of overparameterization on Catastrophic Forgetting of OGD through
an ablation study on the number of parameters of the model.
Experiment We train a model on the full task sequence on the MNIST and CIFAR100 benchmarks,
then measure the variation of the train accuracy of the memorised samples from the first task. The
reason we consider the samples in the memory is the applicability of Thm. 2 to these samples only.
Fixing the datasets sizes, our proxy for overparameterization is the hidden size, which we vary.
Results Figures 1 and 2 show that the train error variation decreases with overparameterization for
OGD, on the MNIST and CIFAR100 benchmarks. This result concurs with Thm. 2.
Benchmark
Figure 1: The variation of the train accuracy on
the memorised samples from the first task as a
function of overparameterization (higher is bet-
ter). The forgetting decreases with overparame-
terization, as stated in Theorem 2.
Figure 2: The variation of the train accuracy
on the memorised samples from each task, after
the model was trained on all tasks in sequence
(higher is better). We vary the hidden size as a
proxy for overparameterization.
7.2	Ablation study : For non over-parameterized benchmarks, updating the
Jacobian is critical to OGD’s robustness (Thm. 2)
Our analysis relies on the over-parameterization assumption, which implies that the Jacobian is
constant through tasks. Theorem 2 follows from this property. However, in practice, this assumption
may not hold and forgetting is observed for OGD. We study the impact of the variation of the Jacobian
in practice on OGD.
In order to measure this impact, we perform an ablation study on OGD, taking into account the
Jacobian’s variation (OGD+) or not (OGD). OGD+ takes into account the Jacobian’s variation by
updating all the stored Jacobians at the end of each task.
Experiment We measure the average forgetting (Chaudhry et al., 2019) without (OGD) or with
(OGD+) accounting for the Jacobian’s variation, on the MNIST, CIFAR100 and CUB200 benchmarks.
8
Under review as a conference paper at ICLR 2021
Results Table 1 shows that for the Rotated MNIST and Permuted MNIST benchmarks, which
are the least overparameterized, OGD+ is more robust to Catastrophic Forgetting than OGD. This
improvement follows from the variation of the Jacobian in these benchmarks and that OGD+ accounts
for it. While on CIFAR100 and CUB200, the most over-parameterized benchmark, the robustness of
OGD and OGD+ is equivalent. For these benchmarks, since the variation of the Jacobian is smaller,
due to overparameterization, OGD+ is equivalent to OGD.
This result confirms our initial hypothesis that the Jacobian’s variation in non-overparameterized
settings such as MNIST is an important reason the OGD algorithm is prone to Catastrophic For-
getting. This result also highlights the importance of developing a theoretical framework for the
non-overparameterized setting, in order to capture the properties of OGD outside the NTK regime.
Dataset	Permuted MNIST	Rotated MNIST	Split CIFAR100	CUB200
Overparameterization	2	2	28	1166
OGD	-9.71 (±0.95)	-17.77 (±0.35)	-20.86 (±1.41)	-12.99 (±0.92)
OGD+	-5.98 (±0.36)	-8.57 (±0.53)	-21.59 (±1.27)	-12.89 (±0.51)
Table 1: The average forgetting of with and without accounting for the Jacobian’s variation, on the
MNIST, CIFAR and CUB200 datasets (lower is better). A higher over-parameterization coefficient
implies that the constant Jacobian assumption is more likely to hold.
7.3	Benchmarking OGD+ against other Continual Learning baselines
In order to provide a broader picture on the robustness of OGD+ to Catastrophic Forgetting, we
benchmark it against standard Continual Learning baselines.
Results Table 2 shows that, on the Permuted MNIST and Rotated MNIST benchmarks, which are
the least overparameterized, OGD+ draws an improvement over OGD and is competitive with other
Continual Learning methods. On the most overparameterized benchmarks, CIFAR100 and CUB200,
OGD+ is not competitive.
	Permuted MNIST	Rotated MNIST	Split CIFAR100	CUB200
Naive SGD	76.31 (±1.89)	71.06 (±0.41)	50.77 (±3.99)	53.93 (±0.86)
EWC	80.85 (±1.14)	80.96 (±0.42)	56.82 (±1.75)	62.15 (±0.37)
SI	86.69 (±0.4)	75.33 (±0.55)	66.66 (±2.07)	63.17 (±0.42)
MAS	85.96 (±0.72)	80.55 (±0.46)	66.33 (±1.13)	61.43 (±0.68)
Stable SGD	78.17 (±0.76)	88.92 (±0.19)	72.86 (±0.9)	58.79 (±0.36)
OGD	85.0 (±0.86)	79.17 (±0.33)	61.82 (±1.24)	57.89 (±0.89)
OGD+	88.65 (±0.38)	87.73 (±0.5)	61.11 (±1.31)	57.99 (±0.52)
Table 2: The average accuracy of several methods on the MNIST, CIFAR and CUB200 datasets.
8 Conclusion
We presented a theoretical framework for Continual Learning algorithms in the NTK regime, then
leveraged the framework to study the convergence and generalisation properties of the SGD and
OGD algorithms. We also assessed our theoretical results through experiments and highlighted
the applicability of the framework in practice. Our analysis highlights multiple connections to
neighbouring fields such as Transfer Learning and Curriculum Learning. Extending the analysis
to other Continual Learning algorithms is a promising future direction, which would provide a
better understanding of the various properties of these algorithms and how they could be improved.
Additionally, our experiments highlight the limits of the applicability of the framework to non-
overparameterized settings. In order to gain a better understanding of the properties of the OGD
algorithm in this setting, extending the framework beyond the over-parameterization assumption is
an important direction. We hope this work provides new keys to investigate these directions.
9
Under review as a conference paper at ICLR 2021
References
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning, 02
2019.
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars.
Memory aware synapses: Learning what (not) to forget. In ECCV, 2018.
Pierre Alquier, The Tien Mai, and Massimiliano Pontil. Regret Bounds for Lifelong Learning. In
Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics, volume 54 of Proceedings ofMachine Learning Research, pp. 261-269,
FortLauderdale, FL, USA, 20-22 Apr 2017. PMLR.
Bernard Ans and StePhane Rousset. Avoiding catastrophic forgetting by coupling two reverberating
neural networks. CompteS RenduS de IfAcademie des Sciences - Series III - Sciences de la Vie, 320
(12):989 - 997, 1997.
Bernard Ans and StePhane Rousset. Neural networks with a self-refreshing memory: Knowledge
transfer in sequential learning tasks without catastrophic forgetting. Connection Science, 12:1-19,
04 2000. doi: 10.1080/095400900116177.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. CoRR,
abs/1901.08584, 2019.
Natalia D 1 az Rodr ´ guez, Vincenzo Lomonaco, David Filliat, and Davide Maltoni. Don't forget,
there is more than forgetting: new metrics for continual learning. CoRR, abs/1810.13166, 2018.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, March 2003.
Shai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In Bernhard
Scholkopf and Manfred K. Warmuth (eds.), Learning Theory and Kernel Machines, pp. 567-580,
Berlin, Heidelberg, 2003. Springer Berlin Heidelberg. ISBN 978-3-540-45167-9.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pp.
41-48, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161.
doi: 10.1145/1553374.1553380.
Frederik Benzing. Understanding regularisation methods for continual learning.	ArXiv,
abs/2006.06357, 2020.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. In NeurIPS, 2019.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with a- GEM . In International Conference on Learning Representations, 2019.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees, 2018a.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around
a common mean. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 10169-10179.
Curran Associates, Inc., 2018b.
Lee H. Dicker, Dean P. Foster, and Daniel Hsu. Kernel ridge vs. principal component regression:
Minimax bounds and the qualification of regularization operators. Electron. J. Statist., 11(1):
1022-1047, 2017. doi: 10.1214/17-EJS1258.
Simon S. Du, Xiyu Zhai, Barnab a´ s P o´ czos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. CoRR, abs/1810.02054, 2018.
10
Under review as a conference paper at ICLR 2021
Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via
learning the representation, provably. ArXiv, abs/2002.09434, 2020.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. In International Conference on Learning
Representations, 2020.
Cong Fang, Hanze Dong, and Tong Zhang. Over parameterized two-level neural networks can learn
near optimal feature representations, 2019.
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual
learning. ArXiv, abs/1910.07104, 2019.
Robert French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3:
128-135,05 1999. doi:10.1016/S1364-6613(99)01294-2.
S. Goldt, M. M e´ zard, F. Krzakala, and L. Zdeborov a´ . Modelling the influence of data structure
on learning in neural networks, 2020.
Sebastian Goldt, MadhU Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborov' a .
Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 32, pp. 6979-6989. CUrran Associates, Inc.,
2019.
Ian J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C. Courville, and Yoshua Bengio. An empirical
investigation of catastrophic forgeting in gradient-based neural networks. CoRR, abs/1312.6211,
2014.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. Mean-field behaviour of neural tangent kernel
for deep neural networks, 2019.
Wei Hu, Zhiyuan Li, and Dingli Yu. Simple and effective regularization methods for training on
noisily labeled data with generalization guarantee, 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/
abs/1502.03167.
Arthur Jacot, Franck Gabriel, and Cle ment Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, NIPS’18, pp. 8580-8589, Red Hook, NY, USA, 2018. Curran
Associates Inc.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms, 2020.
Sangwon Jung, Hongjoon Ahn, Sungmin Cha, and Taesup Moon. Continual learning with node-
importance based adaptive group sparse regularization, 2020.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting
in neural networks. Proceedings of the National Academy of Sciences, 114, 12 2016. doi:
10.1073/pnas.1611835114.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526, 2017.
doi: 10.1073/pnas.1611835114.
Jeremias Knoblauch, H. Husain, and Tom Diethe. Optimal continual learning has perfect memory
and is np-hard. ArXiv, abs/2006.05188, 2020.
11
Under review as a conference paper at ICLR 2021
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems, volume 25, pp. 1097-1105. Curran As-
sociates, Inc., 2012. URL https://proceedings.neurips.cc/paper/2012/file/
c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent, 2019.
Jeongtae Lee, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. ArXiv, abs/1708.01547, 2018.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. CoRR, abs/1903.11680,
2019.
Hong Liu, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. Towards understanding the
transferability of deep representations, 2019.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continuum learning.
CoRR, abs/1706.08840, 2017. URL http://arxiv.org/abs/1706.08840.
Nicolas Masse, Gregory Grant, and David Freedman. Alleviating catastrophic forgetting using
context-dependent gating and synaptic stabilization. Proceedings of the National Academy of
Sciences, 115, 02 2018. doi: 10.1073/pnas.1803839115.
Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of Learning and Motivation, 24:109-165, 1989.
Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan Ghasemzadeh. Understand-
ing the role of training regimes in continual learning, 2020.
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learning.
In International Conference on Learning Representations, 2018.
Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano
Soatto. Toward understanding catastrophic forgetting in continual learning. ArXiv, abs/1908.01091,
2019.
Pingbo Pan, Alexander Immer, Siddharth Swaroop, Runa Eschenhagen, Richard E Turner, and
Mohammad Emtiyaz Khan. Continual deep learning by functional regularisation of memorable
past, 2020.
German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 113:54 - 71, 2019.
Roger Ratcliff. Connectionist models of recognition memory: constraints imposed by learning and
forgetting functions. Psychological review, 97 2:285-308, 1990.
Hippolyt Ritter, Aleksandar Botev, and David Barber. Online structured laplace approximations
for overcoming catastrophic forgetting. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, NIPS’18, pp. 3742-3752, Red Hook, NY, USA, 2018.
Curran Associates Inc.
A. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7:123-, 01
1995.
Nikunj Saunshi, Yi Zhang, Mikhail Khodak, and Sanjeev Arora. A sample complexity separation
between non-convex and convex meta-learning, 2020.
12
Under review as a conference paper at ICLR 2021
Jonathan Schwarz, Jelena Luketina, Wojciech Czarnecki, Agnieszka Grabska-Barwinska, Yee Teh,
Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual
learning, 05 2018.
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.
Alex J. Smola, Bernhard Scho lkopf, and Klaus-Robert Mu ller. The connection between regu-
larization operators and support vector kernels. Neural Netw., 11(4):637-649, June 1998. doi:
10.1016/S0893-6080(98)00032-X.
Gido M Van de Ven and Andreas S Tolias. Generative replay with feedback connections as a general
strategy for continual learning. arXiv preprint arXiv:1809.10635, 2018.
Gido M Van de Ven and Andreas S Tolias. Three scenarios for continual learning. arXiv preprint
arXiv:1904.07734, 2019.
Ruosong Wang, Simon S. Du, Lin F. Yang, and Sham M. Kakade. Is long horizon reinforcement
learning more difficult than short horizon reinforcement learning?, 2020.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets v.s. their induced kernel. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d’Alche Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 9709-9721. Curran Associates, Inc., 2019.
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.
Gradient surgery for multi-task learning, 2020.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
pp. 3987-3995. JMLR.org, 2017.
13
Under review as a conference paper at ICLR 2021
A Complementary discussion
A. 1 NTK variation - The importance of orthogonality for the OGD, OGD+ and
A-GEM algorithms
In the main article, we focused only on the SGD, OGD and OGD+ algorithms. In this section, we
highlight a connection of these algorithms to the A-GEM algorithm. All these algorithms perform
orthogonal projections during the weight update step, however these projections differ in the following
ways :
•	the span of the space the updates are orthogonal to.
•	the rate at which the projection space is updated.
Additionally, the A-GEM algorithm present the additional property of allowing Positive Backward
Transfer by design. Positive Backward Transfer is a desirable property in the sense that the generali-
sation on a past task increases while training on the current task.
Similarities among the algorithms Table 3 presents an overview of the connections between the
algorithm. In practice, the models are not trained in the NTK regime. Since the feature map changes
through training, the orthogonality constraint of OGD becomes less relevant and may harm the
learning. On the other hand, while A-GEM performs a projection on a smaller subspace which may
not protect all the subspaces, the subspace is updated as it is computed at each training step, the
updates are therefore expected to be more relevant. Finally OGD+ lies at the intersection of both,
while it doesn’t update the feature maps at each gradient step, the projection constraint is with respect
to a larger space.
Algorithm	Projection Space		
	Span	Update rate	Backward Transfer
SGD	None	None	No
OGD	Full	Task	Unclear
OGD+	Full	Task	Unclear
A-GEM	Sample	Gradient step	Yes
Table 3: Overview of the properties of the SGD, OGD, OGD+, A-GEM and A-GEM-NT algorithms.
On the theoretical connection between OGD and A-GEM The A-GEM algorithm (Chaudhry
et al., 2019) is a state of the art Continual Learning algorithm on the standard benchmarks. The idea
behind the algorithm is to perform a gradient descent if an estimate of the loss over the previous tasks
increases or is unchanged. Otherwise the gradient is projected orthogonally to the gradient over the
loss estimate over the previous tasks.
We find that OGD draws a upper bound in comparison to A-GEM-NT in terms of generalisation error,
as stated in Proposition 1.
Proposition 1 In the NTK regime, OGD implies A-GEM with no Positive Backward Transfer.
The proof is presented in App. E.1.
Experiments The extended experiments on the importance of the NTK variation for Continual
Learning (Sec. F.3), show that A-GEM outperforms OGD and OGD+ on most benchmarks. A
probable reason behind this difference is the rate of update of the feature map for the A-GEM
algorithm, even though it spans a smaller space than OGD and OGD+.
14
Under review as a conference paper at ICLR 2021
A.2 Expressions overview
For convenience, we present in Table 4 an overview of the notions that were mentioned in the main
article, with their respective mathematical expressions.
Name	Expression
Residual	NT = NT- yτ-ι→τ
Knowledge increment	f (x) = Kτ(x, XT )T(Kτ +l(Xτ, XT ) + λI)-1Nτ
Projection matrix	TT
Feature map	φ(x)
Effective feature map	φT (x)
Effective kernel	κT (x, x0)
Complexity measure	pT=i Jr(KT(XT ,xT)) yT (KT(XT, XT) + Xi)-%
NTK task dissimilarity	Sτ-ι→τ = yT (KT(XT, XT) + λi)-1yτ	
Table 4: Overview of the notions presented in the main manuscript with their respective notations.
15
Under review as a conference paper at ICLR 2021
B Proofs overview
B.1	Convergence :
Theorem 1 : Convergence of SGD and OGD for Continual Learning We prove Theorem 1 by
induction. We rewrite the loss function as a regression on the residual NT instead of yr. Then, We
rewrite the optimisation objective as an unconstrained strongly convex optimisation problem. Finally,
we compute the unique solution in a closed form. The full proof is presented in App. C.1.
Remark 1 : Continual Learning as a recursive Kernel Regression The remark follows directly
from the recursive form of Theorem 1.
Corollary 1 : Distance from initialisation through tasks The proof follows immediately from
the proof of Theorem 1, in which we compute a closed form of the weights variation. It is presented
in App. C.2.
B.2	Generalisation :
Theorem 2 : No-forgetting Continual Learning with OGD The proof relies on the orthogonality
property of the OGD update, which implies that the subspaces spanned by the samples in the memory
would not have any changes incurred. The proof is presented in App . D.1
Theorem 3 : Generalisation of SGD and OGD for Continual Learning The proof or Theorem
3 is presented in App. D.2. The proof follows the following structure :
•	Bounding the Rademacher complexity (App. D.2.3) : First, we state the technical Lemma 3
that upper bounds the Rademacher complexity of the function class that corresponds to the
set of linear combinations of kernel regressors. Then we apply the lemma to the function
class obtained through Theorem 1.
•	Bounding the empirical loss for SGD (App. D.2.5) : the error is the same as OGD on the last
task. While on the previous tasks, Catastrophic Forgetting can be incurred which implies
the appearance of a residual term that corresponds to the forgetting.
•	Bounding the empirical loss for OGD (App. D.2.4) : The proof techniques are very similar
to the SGD case, however we leverage Theorem 2 in order to derive tighter bounds due to
the absence of Catastrophic Forgetting.
•	Wrap-up (App. D.2.6) : Finally, we wrap-up all the lemmas to derive the bound of Theorem
3.
Lemma 1 - Implications for Curriculum Learning This results follows from the technical
Lemma 3, which upper bounds the Rademacher complexity of the function class that corresponds to
the set of linear combinations of kernel regressors.
B.3	The importance of the NTK variation for Continual Learning :
Proposition 1 - OGD implies A-GEM-NT The proof relies mainly on Theorem 2 stating the
robustness of OGD to Catastrophic Forgetting. We derive the A-GEM update then apply Theorem 2,
which leads to the implication. The full proof is presented in App. D.2.3.
16
Under review as a conference paper at ICLR 2021
C Missing proofs of section 4 - Convergence
C.1 Proof of Theorem 1
Stochastic Gradient Descent We start by proving the Stochastic Gradient Descent (SGD) case of
Theorem 1.
Proof
We prove the Theorem 1 by induction.
Our induction hypothesis Hτ is the following : Hτ : For all k ≤ τ , Theorem 1 holds.
First, we prove that H1 holds.
The proof is straightforward. For the first task, since there were no previous tasks, OGD on this task
is the same as SGD.
Therefore, it is equivalent to minimising the following objective :
argminf0(X1) + φ(Xι)T(W - w?) - yj∣2 + 左∣∣w - w?k2	(1)
w∈Rd
where Φ(x) = NW f0(x).
We replace into the objective with y applying yι = y1 一 fo(Xι):
argmin∣∣φ(Xι)T(w - w?) - yj∣2 + λ∣∣w - w?|2	(2)
w∈Rd
The objective is quadratic and the Hessian is positive definite, therefore the minimum exists and is
unique :
w? - w? = φ(X1)(φ(X1 )Tφ(Xι) + λI)-1yι	(3)
Under the NTK regime assumption :
f1?(x) = f0?(x) + Nwf0?(x)T (w1? - w0?)	(4)
Then, by replacing into w1? - w0? :
f?(X) = f0(x) + VW f?(X)T φ(X1)(φ(X1)τ φ(Xι) + λI)-1yι	(5)
f?(X) = f0(x) + κι(x, X1)(κ1(X1, Xi) + λI)-1yι	(6)
Finally :
f?(X)- f?(X) = κι(x, Xι)(κι(Xι, Xi) + λI)-1yι	(7)
Which completes the proof of H1.
Let τ ∈ N? , we assume that Hτ is true, then we show that Hτ+i is true.
On the task Tτ+i, we can write the loss Lτ+i as :
Lτ+1(W)= ∣∣φτ(Xτ+I)T (w - WT)- yτ+i∣∣2 + 2 kw - WTk2	⑻
We recall that the optimisation problem on the task Tτ+i is :
argmin∣∣φτ(Xτ +i)T(w - WT)- NT+1∣∣2 + λl∣w - w-Tr∣2	(9)
w∈Rd	2
The optimisation objective is quadratic, unconstrained, with a positive definite hessian. Therefore, an
optimum exists and is unique :
wT+1 - wT = φ(Xτ+i)(Φ(Xτ+i)TΦ(Xτ+i) + λI)-1yτ +1	(10)
17
Under review as a conference paper at ICLR 2021
We define the kernel κτ+1 : Rd × Rd → R as :
κτ +1 (x, x0) = φτ (x)T φτ (x0) for all x, x0 ∈ Rd	(11)
Finally, We recover a closed form expression for f?+i :
First, we use the induction hypothesis Hτ :
fT+ι(x) =界(x) + hVw界(x), wT+1- WTi	(12)
=界(x) + φτ (x)φ(Xτ+ι)(κτ+ι(Xτ+1, XT +1)+ λI)-1yτ+1	(13)
=f?(x) + KT+ι(x, Xτ +ι)(κτ +ι(Xτ +1, XT+ι) + λI)-1yτ +1	(14)
At this stage, We have proven Ht+1.
(15)
Orthogonal Gradient Descent NoW, We prove the Orthogonal Gradient Descent (OGD) case of
Theorem 1. The key difference in the proof is that the OGD optimisation objective is constrained.
The constraints correspond to the orthogonality to the subspace spanned by the memorised feature
maps from the previous tasks. Another key difference occurs during the regularisation, as opposed to
the SGD case Where the regularisation of the Weights spans on the Whole space, the regularisation of
OGD only applies to the “learnable” space. This property folloWs from the orthogonality constraint,
Which enforces that the space spanned by the previous tasks is unchanged.
Proof
We prove the Theorem 1 by induction. Our induction hypothesis HT is the folloWing : HT : For all
k ≤ τ, Theorem 1 holds.
First, We prove that H1 holds.
The proof is straightforWard. For the first task, since there Were no previous tasks, OGD on this task
is the same as SGD.
Therefore, it is equivalent to minimising the folloWing objective :
argmin∣∣f0(X1)φ(X1 )T(w — W?) - y1∣∣2 + λ∣∣w — w?k2	(16)
W∈Rd	2
We replace into the objective by the residual term y = y∖ 一 f0(X1)
argmin∣∣φ(X1 )T(w 一 W0) — yj∣2 + λ∣∣w - Wok2	(17)
w∈Rd
Where φ(x) = Vw0? f0?(x).
The objective is quadratic and its Hessian is positive definite, therefore the minimum exists and is
unique :
w? — w? = φ(X1)(φ(X1)τ φ(X1) + λI)-1 y1	(18)
Under the NTK regime assumption :
f1?(x) = f0?(x) + Vwf0?(x)T (w1? 一 w0?)	(19)
Then, by replacing into w1? 一 w0? :
f?(X) = f?(x) + Vw f?(X)T φ(X1)(φ(X1)τ φ(X1) + λI)-1y1	(20)
f?(X) = f?(x) + κ1(x, X1)(κ1(X1, X1) + λI)-1y1	(21)
Finally :
f?(X)- f?(X) = κ1(x, X1)(κ1(X1, X1) + λI)-1y1	(22)
18
Under review as a conference paper at ICLR 2021
Which completes the proof of H1.
Let τ ∈ N? , we assume that Hτ is true, then we show that Hτ+1 is true.
On the task Tτ+1, we can write the loss Lτ+1 as :
Lτ+1(W) = ∣∣φτ (Xτ+I)T (W - WT)- yτ+ι∣∣2+2 kw - wτk2	(23)
We recall that the optimisation problem on the task Tτ+1 is :
argmin∣∣φτ(Xτ+ι)T(W - WT)- NT +1∣∣2 + λl∣w - w?k2	(24)
W∈Rd	2
u.c. VT+1(W - WT?) = 0	(25)
T
where VT+1 is a projection matrix on ( L Ek)⊥, the Euclidean space induced by the feature maps
k=1
of the tasks {Tk, k ∈ [τ]}.
Let TT+ι ∈ Rd×(d-Kτ+1) and W ∈ Rd-Kτ+1 SUch as:
W — WT = TT +ιW	(26)
T
KT = dim(MEk)	(27)
k=1
We rewrite the objective by plugging in the variables we just defined. The two objectives are
equivalent :
argmin ∣∣Φt(Xt +ι)TTT+ιW - NT +1∣∣2 + λI∣Tt +1 W∣2	(28)
W ∈Rd-Kτ+1	2
This objective is equivalent to :
argmin ∣∣Φt(Xt +ι)TTT+ιW - NT +1∣∣2 + λ 1回『	(29)
W ∈Rd-Kτ+ι	2
For clarity, we define ZT+1Rnτ+1 ×(d-Kτ+1) as :
ZT+1 = φT(XT+1)TTT+1	(30)
By plugging in ZT+1, we rewrite the objective as :
argmin ∣∣Zt+iW - NT +1∣∣2 + λI∣Tt+iW∣∣2	(31)
W ∈Rd-Kτ+ι	2
The optimisation objective is quadratic, unconstrained, with a positive definite hessian. Therefore, an
optimum exists and is unique :
W T+1 = ZT+i(Zt+iZT+i + λI)-1NT+1	(32)
We recover the expression of the optimum in the original space :
W?+1 - WT = TT +ιZT+ι(ZT +1ZT+1 + λI)-1NT+1	(33)
We define the kernel κT+1 : Rd × Rd → R as :
κT+1(x, x0) = φT(x)TTT+1TTT+1φT(x0) for all x, x0 ∈ Rd	(34)
Now we rewrite WT?+1 - WT? :
W?+1 - WT = TT +iZT+i(kt+i(Xt+1, Xt +1) + λI)T%+1	(35)
Finally, we recover a closed form expression for fT?+1 :
First, we use the induction hypothesis HT :
f?+I(X) = f?(X) + "Wf?(x), wT+1 - wTi	(36)
=f?(x) + φτ(x)Tt +1ZT+1(κτ+1(Xt+1, Xt+1) + 入1尸%+1	(37)
=f?(X) + κτ+1(x, XT+1)(KT+1(XT+1, XT+1) + λI)-1yτ +1	(38)
At this stage, we have proven Ht+1.
We conclude the proof of Thm. 1.
(39)
19
Under review as a conference paper at ICLR 2021
C.2 Proof of the Corollary 1
Stochastic Gradient Descent The proof follows immediately from Thm. 1.
Proof
In the proof of Theorem 1 (App. C.1), for the SGD case, we showed that :
wT+1 - w? = φ(Xτ +ι)(Φ(Xτ +1)TΦ(Xτ+ι) + λI)-1yτ+1	(40)
This result implies that :
I∣wτ+1 - wT∣∣2 = yT+ι(Φ(Xτ +1)TΦ(Xτ +1)+ λI)φ(Xτ +1)Tφ(Xτ +ι)(Φ(Xτ +1)TΦ(Xτ+ι) + λI)-1%+1
(41)
=yT+1(κ(Xτ+1, XT+1)+ λI)-1 κ(Xτ+1, XT+ι)(κ(Xτ+1, Xτ+ι) + λI)-1yτ+1
(42)
(43)
Orthogonal Gradient Descent The proof is exactly the same as the proof above, the difference
lies in the kernel definition, which is implicit.
Proof
In the proof of Theorem 1 (App. C.1), for the OGD case, we showed that :
wT?+1 -wT? = TT+1ZTT+1(κT+1(XT+1, XT+1) + λI)-1yyT+1	(44)
This result implies that :
∣∣wT+1 - wT∣∣2 = yT+1(κτ +1(XT +1, XT +1) + λI)-1Zτ +1TT+1TT +1ZT+1(KT +1(XT +1, XT +1) + λl)-1yτ +1
(45)
= yyTT+1 (κT +1 (XT+1, XT+1) + λI)-1 κT +1 (XT+1, XT+1)(κT+1(XT+1, XT+1) + λI)-1 yyT +1
(46)
(47)
20
Under review as a conference paper at ICLR 2021
D Missing proofs of section 5 - Generalisation
D.1 Proof of Theorem 2
The intuition behind the proof is : since the gradient updates were performed orthogonally to the
feature maps of the training data of the source task, the parameters in this space are unchanged,
while the remaining space, which was changed, is orthogonal to these features maps, therefore, the
inference is the same and the training error remains the same as at the end of training on the source
task.
Proof
In the proof of Theorem 1, App. C.1, we showed that, for Tτ a fixed task:
fT+ι(X) = fT(x) + "wfT(x), wT+ι - wTi.	(48)
We rewrite the recursive relation into a sum:
τ
fT+ι(X)- f?(X) = XhVwf?(X), W?+1 - w?i.	(49)
k=1
We observe that, for all k ∈ [T]:
wk?+1 - w?k ∈ Ek0 .	(50)
On the other hand, for OGD+, given a sample X from Dτ, for all k0 ∈ [τ + 1, T] :
Vwfk?(X) ∈ Ek0	(51)
Therefore, for all k0 ∈ [k + 1, τ] :
hVwfk?0(X), w?k0+1 -wk?0i = 0	(52)
Therefore :
fT(x) = f?(x)	(53)
We conclude.
(54)
D.2 Proof of Theorem 3
D.2.1 Reminders
Reminder on RKHS norm
Let κ a kernel, and H the reproducing kernel Hilbert space (RKHS) corresponding to the kernel κ.
Recall that the RKHS norm of a function f(X) = αT κ(X, X) is :
kf kH = JαTκ(X, X)α	(55)
(56)
Reminder on Generalization and Rademacher Complexity Consider a loss function l : R ×
R → R. The population loss over the distribution D, and the empirical loss over n samples
D = {(Xi, yi), i ∈ [n]} from the same distribution D are defined as :
LD (f ) = E(x,y)〜D [l(f (x),y)]	(57)
1n
LS(f ) = - £l(f(xi),yi)	(58)
n i=1
Theorem 4 Suppose the loss function is bounded in [0, c] and is ρ-Lipschitz in the first argument.
Then, with probability at least - δ over sample S of size n :
Jlog(2∕δ)
V 2n
,_ , .. _ , .. , ʌ ,一
SUp {Ld(f) - LS(f)} ≤ 2ρRR(F) + 3c
f∈F
(59)
21
Under review as a conference paper at ICLR 2021
D.2.2 BOUNDING f τ H
Lemma 2 Let Hτ the Hilbert space associated to the kernel κ
τ.
We recall that :
fT(X) = Kτ(x, XT)Tατ	(60)
ατ = (KT(XT, XT)+ λI)-1yτ	(61)
Then :
恒 IIH ≤ yT (KT(XT, XT)+ λI)-1yτ	(62)
(63)
Proof
We start from the definition of the RKHS norm of f ；:
MIIH = αTKT(XT, XT)αT	(64)
=yyTT(KT(XT,XT)+λI)-1KT(XT,XT)(KT(XT,XT)+λI)-1yyT	(65)
Since (KT (XT, XT) + λI)-1 ≤ (KT (XT, XT))-1
≤yyTT(KT(XT,XT)+λI)-1KT(XT,XT)KT(XT,XT)-1yyT	(66)
≤yyTT(KT(XT,XT)+λI)-1yyT	(67)
D.2.3 Bounding the Rademacher Complexity
The goal of this section is to upper bound the Rademacher Complexity. First, we derive a general
upper bound for the Rademacher Complexity of a linear combination of kernels in Lemma 3 . Then,
we apply this bound to the linear combination of kernels obtained through Thm. 1, which describes
the model through the Continual Learning.
A general bound for the Rademacher Complexity
Lemma 3 (Rademacher Complexity of a linear combination of kernels) Let Kt : X × X →
R, t ∈ [T] kernels such that :
sup kKt(x,x)k < ∞
x∈X
To every kernel Kt, we associate a feature map φt : X → Ht, where Ht is a Hilbert space with inner
product h∙, )H, andfor all x, x0 ∈ X, κt(x, x0) = hφt(x), φt(x0)iHt
For all T ∈ N?, given a sequence {BT, τ ∈ [T]} we define FT as follows :
T
FT =	{x → Xft(x),	ft(x)	=	αtTKt (x, Xt)	∀t	∈	[T], kftkHt ≤	Bt}	(68)
t=1
Let Xi, ∙, Xn be random elements of X. Thenfor the class F, we have :
T
R(F) ≤ X 2Bt(Tr(κt(Xt, Xt)))1/2
t=1 nt
Proof
22
Under review as a conference paper at ICLR 2021
Let f ∈ F, and let x ∈ X :
T nτ
f(x)=XXαiτ κτ (x, xiτ )
τ=1 i=1
For all τ ∈ [T], we associate a feature map φτ : X → Hτ
∀x, x0 ∈ X κτ (x, x0) = hφτ (x), φτ (x0)iHτ
Therefore :
T nτ
f(x) = X X αiτ hφτ (xiτ), φτ (x)iHτ
T nτ
= XhX αiτ φτ (xiτ), φτ (x)iHτ
τ=1 i=1
On the other hand, the following holds ∀t ∈ [T] :
nτ
X αiτφτ (xiτ)
i=1
2
H
αiταjτκτ(xiτ,xjτ) ≤ Bτ2
i,j
Therefore :
T
Fτ ⊂ {x → XhWT,φτ(x)>Hτ, kwτk2 ≤ BT ∀t ∈ [T]} ：= FT
t=1
Now, we derive an upper bound of the Rademacher complexity of FT :
ʌ , ʌ , ~
R(Ft) ≤ R(Ft)
T	2 nτ
=E[ sup	EhWT, —Eqφτ(XT)iH I(XT)]
kWτk2≤Bτ,t∈[T] t=1	nT i=1	i Hτ
T	2 nτ
=∑E[ sup XT, 一	Φt (XT )iH」(Xt )]
t=1	kWτk2≤Bτ	nT i=1	Hτ
(69)
(70)
(71)
(72)
(73)
(74)
(75)
(76)
(77)
Now we apply on each function fT the upper bound from Lemma 22 by Bartlett & Mendelson 2003
T
R(FT) ≤ X -BT(Tr(KT(XT, XT)))1/2	(78)
t=1 nT
(79)
Bounding the Rademacher Complexity for Continual Learning
Lemma 4 Keeping the same notations and setting as Theorem 3, the Rademacher Complexity can
be bounded as follows:
R(Ft) ≤ X O (SyT(KT(XT, X；)+ λD-1yT J	(80)
where FT is the function class spanned by the model up to the task TT.
(81)
Proof
For all T ∈ N?, given a sequence {BT, τ ∈ [T]} we define FT as follows :
T
FT = {x → X fT (x),	fT (x)	=	αTT KT (x,	XT)	∀τ	∈	[T],	kfT kHτ	≤	BT}	(82)
T=1
23
Under review as a conference paper at ICLR 2021
where we set Bτ as :
BT =，M - yτ-ι→τ)t(κτ(Xτ, XT) + λI)-1(yτ - yτ-ι→τ)	(83)
Following Lemma 2 for all τ ∈ [T] :
fT? ∈ FT	(84)
which means that the function class Ft contains the Continual Learning model up to task TT .
We apply Lemma 3 in order to upper bound the Rademacher complexity :
T
R(FT) ≤ X -BT(Tr(KT(XT, XT)))1/2
nT
T=1
We made the assumption that for all τ ∈ [T] tr(κT (XT, XT)) = O(nT) :
(85)
(86)
(87)
(88)
(89)
D.2.4 Bounding the empirical loss for OGD
Lemma 5 Given a current task TT, the empirical losses on the data from all previous tasks (TT , τ ≤
T) can be bounded as follows :
Let T ∈ N fixed. Then, for all τ ∈ [T]
λ2
Lsτ(fT) ≤ — NT (KT(XT, XT)+ λI)-1yT	(90)
(91)
Case 1 -Task TT Proof
We start from the definition of the empirical loss :	
nT LSτ(fT)=而 X(fT(XT,i) - yτ,i)	(92)
=—kfT(XT) - yτk2 nT	(93)
We replace into fT? by its expression from Theorem 1 :	
=汨fτ (XT)+ fτ-1(χτ)- yτ∣∣2	(94)
1 nT	IIfT-l(Xτ) - yT∣∣2	(95)
1 nT	I∣(kτ(Xτ, Xt)t(KT(XT, XT) + λI)-1yτ - Nτ∣2	(96)
1 nT	∣∣(kτ(Xτ, Xt) + λI - λI)(κτ(Xτ, XT) + λI)-1yτ - Nτ∣2	(97)
1 nT	∣∣yτ - λ(KT(XT, XT) + λI) 1yτ - yτ∣∣2	(98)
λ2 nT	∣∣ (KT (XT, xt ) + λI)-1yτ∣∣2	(99)
24
Under review as a conference paper at ICLR 2021
Now, we apply Lemma 2 in order to upper bound the right hand side norm :
λ2
Lsf) ≤ -yτ(KT(XT, XT) + λI)-1yτ	(100)
nT
(101)
Case 2 - Tasks {Tτ, τ ∈ [1, T - 1]} : The proof is very similar to Case 1, we apply Theorem 2,
which is the key property of OGD in terms of no forgetting.
Proof
We start from the definition of the empirical loss :
nτ
Lsf) = — X(fT (xτ,i) - yτ,i)2	(102)
nτ
i=1
=n1 kfT(XT) - yτk2	(103)
Now, applying Theorem 2 which implies that fT(Xτ) = fT(XT):
Lsτ(fT) = — kfT(Xτ) -yτk2	(104)
nT
Then with a similar analysis as Case 1, we get :
λ2
Lsτ(fT) ≤ — NT(kτ(Xτ, XT) + λI)-1yτ	(105)
(106)
D.2.5 Bounding the empirical loss for SGD
As opposed to the analysis for OGD, forgetting can occur for OGD and Theorem 2 is no longer valid.
Similarly to the OGD empirical loss analysis, we study the case on the data from the last task and the
case on the data from all the other tasks.
Lemma 6 The empirical losses on the source and target tasks can be bounded as follows :
Let T ∈ N fixed. Then, for all τ ∈ [T]
Lsτ(fT) ≤ TT (kτ(Xτ, XT)+ λI)-1yτ	(107)
nT
For all τ ∈ [T - 1]
Lsf) ≤ -1(λ2yT(kτ(Xτ, XT))TyT	(108)
τ nT
T
+ X yykT (κk(Xk, Xk) + λI)-1κk(XT, Xk)κk(XT, Xk)T (κk(Xk, Xk) + λI)-1yyk)
k=T+1
(109)
Case 1 -Task TT For this case, the analysis is the same as for OGD, as no forgetting occurs with
respect to the data the model is being trained on.
25
Under review as a conference paper at ICLR 2021
Case 2 - Tasks {Tτ, τ ∈ [1, T - 1]} : Since SGD is not guaranteed to be robust to Catastrophic
Forgetting, this case comprises additional residual terms that correspond to forgetting.
Proof
Let τ ∈ [T - 1].
From Corr. 1, we recall that :
T
fT (X) = f?(X) + X fk (X)
k=τ +1
Then :
T	2
kfT(XT) -yτk2 = f?(XT)+ X f?(XT) -y
k=τ +1	2
T	2
≤kf?(XT)-yτk2 + X fk(Xτ)
k=T +1	2
T2
≤*(XT) - yτk: + X	f?(XT)∣∣2
, _® ~'	k=+^{----}
(B
We Can upper bound (Asimilarly to the previous paragraphs:
kf?(XT) - yTk2 ≤ λ2yT(kT(Xt, XT) + λI)-1yT
Now, We upper bound (B). Let k ∈ [τ + 1, T]:
(110)
(111)
(112)
(113)
(114)
∣∣f?(Xτ)∣∣2 = y(Kk(Xk,Xk) + λi)-1 Kk(XT,Xk)：k(XT,Xk)T (Kk(Xk,Xk) + λi)-1yfc
Similarity between the tasks T τ and Tk
(115)
We conclude by plugging back the upper bounds of (A)and (B)
kfT?(XT) -yTk22 ≤λyyTT(kT(XT,XT))-1yyT	(116)
T
+ X yykT(Kk(Xk,Xk)+λI)-1Kk(XT,Xk)Kk(XT,Xk)T(Kk(Xk,Xk)+λI)-1yyk
k=T+1
(117)
Therefore :
LSτ(fT ) ≤ -1(λ2yT (kτ (Xτ, Xt ))TyT	(118)
τ	nT	T	T
T
+ X yykT(Kk(Xk,Xk)+λI)-1Kk(XT,Xk)Kk(XT,Xk)T(Kk(Xk,Xk)+λI)-1yyk)
k=T+1
(119)
D.2.6 Proof of the Generalisation Theorem (Thm. 3)
Now, we prove the Theorem 3 by applying the lemmas we developed above.
Proof
26
Under review as a conference paper at ICLR 2021
With probability 1 - δ we have :
sup {LD(f) - LS(f)}
f∈FT
2ρRR(F T) + 3ci Pog尹
2nT
(120)
LDτ (fT?)
Lsf) + 2ρRR(F τ) + 3q 口0g”
2nT
(121)
LDτ (fT?)
LSτ (fT?) +XT O
k=1
yT (Kk (Xk, Xk))Tyk
nk
/log(2∕δ)
+ 3cV
(122)
Then, by replacing into LSτ (fT?) with Lemmas 5 and 6, we get the upper bounds of the theorem for
the various cases of SGD and OGD.
≤
≤
≤
(123)
D.2.7 Alternative proof for the empirical loss bounds - No regularisation case
This section is complementary and aims to provide a better interpretation of the bounds in Theorem 3.
We prove that in the case where there is no regularisation, the training error converges to zero. This
result illustrates better the intuition behind the λ term that appears in the upper bound of the Theorem
3, which is under the regularisation assumption.
First, we derive the differential equation of the model’s output dynamics in Lemma 7. Then, we apply
this Lemma to prove the convergence to zero in Lemma 8. Finally, in Lemma 9, we show that for the
past tasks, the training error equals to zero all the time.
The training dynamics of OGD
Lemma 7 (Differential equation of the model’s output dynamics) Let T ∈ N?, the index of the
task currently being trained on, with OGD.
For all τ < T, defining ueτ as :
Uτ(t) = W(XT) - X fk(XT) = Φτ(Xτ)T(wτ(t) - wT-ι)	(124)
k<τ
The model’s output dynamics while training on the task TT is as follows :
Uτ(t + 1) - Uτ(t) = -ηκτ(Xτ, XT)(Uτ(t) - y).	(125)
(126)
Proof
Let τ ∈ N?, we derive the dynamics while training on the task TT , while training on this task.
First, we define :
yτ = NT + Φτ(Xτ )t wT-1	(127)
We also define the projection matrix PT ∈ Rp×p
as :
PT = TTT TT	(128)
The matrix PT performs the projection from the original weight space Rp to the trainable weight
T
space during training on task TT, which corresponds to ( L Ek)⊥ .
k=1
27
Under review as a conference paper at ICLR 2021
Our starting point is the SGD update rule :
Wτ(t +1) = Wτ(t) - ηPτ+1VwLτ(wτ(t))	(129)
where :
LT(WTo ∣∣φτ(Xτ)t(Wτ(t) - wT-ι) -yτ∣∣2	(130)
=∣∣Φτ(Xτ )T Wt (t) — (yτ + Φτ(Xτ )T TwT-I )∣∣2	(131)
= ∣∣φT (XT)T WT (t) - y≈T ∣∣22	(132)
Therefore, we get the following expression for the gradient of the loss :
VWLT(WT(t)) = φT(XT)(φT(XT)TWT(t) - y≈T)	(133)
Then, the following holds :
(134)
ueT (t + 1) - ueT(t) = φT(XT)T(WT(t + 1) - WT (t))	(135)
= φT (XT)T (-ηPT VWL(WT (t)))	(136)
= φT (XT)T (-ηPT φT (XT)(φT (XT)T WT (t) - y≈T))	(137)
= (φT(XT)TT)T(-ηTTφT(XT)(φT(XT)TWT(t) - y≈T))	(138)
= -ηκT(XT, XT)(φT (XT)T WT (t) - y≈T)	(139)
=-ηκτ(Xτ, XT)(Uτ(t) - y)	(140)
=-ηκτ(Xτ, XT)(Uτ(t) - NT)	(141)
It follows that :
Uτ(t +1) - NT = (I - ηκτ(Xτ,XT))(Uτ(t) - NT)	(142)
Uτ(t) - Nt = -(I - ηKτ(Xτ, XT ))t(Uτ(0) - Nt )	(143)
(144)
The training error converges to zero
Lemma 8 For all tasks TT, τ ∈ [T], the training error LT (WT (t)) converges to 0 when t tends to
infinity .
Proof
We start with the definition of the training error on task T :
LS(fT(t))= ∣∣∣fT(t)(XT) - NT ∣∣∣2	(145)
= kueT(t) -NNTk2	(146)
= ∣∣(I - ηκT (XT, XT))t(ueT (0) - NNT)∣∣	(147)
Under the assumption that KT(XT, XT) is positive definite, for η ≤ 口(长(X1 X)计 the eigenvalues
become less than 1, therefore :
lim LS(fT(t)) =0	(148)
t→∞
The training error on the past tasks is zero
Lemma 9 For all tasks TT, τ ∈ [T], the training error LSτ (fT? ) = 0.
Proof This lemma follow immediately from Lemma 8 which states that the training error converges
to zero for OGD on all tasks and Theorem 2 which states that the training error on the past tasks is
unchanged.
28
Under review as a conference paper at ICLR 2021
E Missing proofs of App A - The importance of the NTK variation
for Continual Learning
E.1 Proof of Proposition 1 - OGD implies A-GEM-NT
We present the proof of Proposition 1, which relies mainly on Theorem 2 stating the robustness of
OGD to Catastrophic Forgetting.
Proof
Let Tτ a task fixed, we recall that in the NTK regime, the model can be expressed as :
fw(X)= fT-ι(x) + Vf?(X)(W - wT-1)	(149)
Given a task Tk and its associated memory Mk, we recall that the loss can be expressed as :
l(fw, Mk )=	X	(fT-ι(x) + Vf?(X)(W - wT-ι)- y)2	(150)
(X,y)∈Mk
Similarly to the A-GEM paper, we define the gradients vectors g and {gk, k ∈ [τ - 1]} as :
g = Vw l(fw, Dτ)	(151)
gk = Vw l(fw, Mk)	(152)
Now we extend the expressions :
gk = Vwl(fw,Dτ)	(153)
=Vw X	(fT-ι(x)+ Vf?(X)(W - wT-ι)-y)2	(154)
(X,y)∈Mk
= X	(fT-ι(x) + Vf?(X)(W - wT-ι) - y)Vf?(x)	(155)
(X,y)∈Mk
For OGD, following Thm. 2 variant for finite memory :
= X	(VfO(X)W - WT-I))Vf?(X)	(156)
(X,y)∈Mk
Finally, since the gradient updates are orthogonal to Mk :
gk = 0	(157)
Therefore, for all k ∈ [τ - 1] :
gk ∙ g = 0	(158)
We conclude, OGD implies learning with GEM and A-GEM with no Positive Backward Transfer.
(159)
29
Under review as a conference paper at ICLR 2021
F	Experiments
F.1 Reproducibility
F.1.1 Architectures
Due to the memory limitations encountered while running OGD and OGD+ on the CIFAR100 and
CUB200 dataset, we use smaller architectures which are different from Jung et al. (2020). For the
MNIST benchmark, we keep the same architecture as Farajtabar et al. (2019).
MNIST : Similarly to Farajtabar et al. (2019), the neural network is a three-layer MLP with 100
hidden units in two layers, each layer uses RELU activation function. The model has 10 logits, which
do not use any activation function.
CIFAR100 : The neural network is a multi-head LeNet Lecun et al. (1998) network with Batch
Normalisation Ioffe & Szegedy (2015) and 200 hidden units for the penultimate layer.
CUB200 : Similarly to Jung et al. (2020), our base architecture is AlexNet (Krizhevsky et al., 2012).
In order to scale for the OGD type algorithms, we changed the classifier module to a smaller 3 layer
RELU neural network, with dropout (Table 5).
Layer
Linear (4096, 256)
RELU
Dropout (0.5)
Linear (256, 128)
RELU
Linear (128, ...)
Table 5: Classifier module of the architecture used for the CUB200 benchmark.
F.1.2 Experiment setup
We run each experiment 5 times, the seeds set is the same across all experiments sets. We report the
mean and standard deviation of the measurements.
F.1.3 OGD+ Implementation Details
Memory : During the OGD+ memory update step, for each task, the new associated feature maps
replace the memory slots of the previous feature maps for the same task. The goal is to ensure a
balance of the feature maps from all tasks in the memory.
Multi-head models : For the dataset streams Split MNIST and Split CIFAR-100, we consider
multi-headed neural networks. We only store the feature maps with respect to the shared weights, the
projection step is not performed for the heads’ weights.
F.1.4 Hyperparameters
We use the the same hyperparameters as Farajtabar et al. (2019) for the algorithms SGD, OGD and
OGD+, on the MNIST benchmarks. We also keep a small learning rate, in order to preserve the
locality assumption of OGD, and in order to verify the conditions of the theorems. We report the
shared hyperparameters across the benchmarks and algorithms in Table 6.
For the other benchmarks and algorithms, we report the grid search space in Sec. F.1.5.
F.1.5 Hyperparameter search
We present our hyperparameter search ranges for each Continual Learning method. We selected the
hyperparameter sets that maximise the average accuracy. The results and scripts of the hyperparameter
search and the best hyperparameters are provided on the corresponding github repository.
30
Under review as a conference paper at ICLR 2021
Hyperparameter	MNIST	CIFAR-100	CUB-200
Epochs	5	50	=	75
Architecture	MLP	LeNet	AlexNet (variant)
Hidden dimension	100	200	—	256
Torch seeds		1to 5	
Memory size per task		100^^	
Activation		RELU	
Table 6: Hyperparameters used across experiments
MNIST Benchmarks
•	EWC, SI and MAS : we fixed the seed to 0, then performed a grid search over the regulari-
sation parameter in [0.1, 1, 10, 100, 1.000, 10.000]
•	SGD, OGD and OGD+ : we used the same hyperparameters as Farajtabar et al. (2019).
•	Stable SGD : we fixed the seed to 0 then performed a grid search over all combinations of :
-	gamma : [0.5, 0.6, 0.7, 0.8, 0.9]
-	learning rate : [0.01, 0.1, 0.25]
-	batch size : [10, 32, 64]
-	dropout [0, 0.1, 0.2, 0.3, 0.4, 0.5]
CIFAR100 and CUB200 Benchmarks
• EWC, SI and MAS : we fixed the seed to 1, then performed a grid search over :
-	regularisation coefficient : [0.1, 1, 10, 100, 1.000, 10.000]
-	learning rate: [0.00001, 0.001, 0.01, 0.1]
-	batch size : [32, 64, 256]
• SGD, OGD and OGD+ : For the MNIST benchmarks, we used the same hyperparameters
as Farajtabar et al. (2019). While for the CIFAR100 and CUB200 benchmarks, we run the
following grid search :
-	learning rate: [0.00001, 0.001, 0.01, 0.1]
-	batch size : [32, 64, 256]
-	epochs : [1, 20, 50]
• Stable SGD : we performed the following grid search :
-	gamma : [0.5, 0.6, 0.7, 0.8, 0.9]
-	learning rate : [0.001, 0.01, 0.1]
-	batch size : [10, 64]
-	dropout [0, 0.1, 0.2, 0.3, 0.4, 0.5]
-	epochs : [1, 10, 50]
31
Under review as a conference paper at ICLR 2021
F.2 Experiments : Memorisation property of OGD (Thm. 2)
F.2.1 The importance of the overparameterization assumption
Thm. 2 states that in the NTK regime, given an infinite memory, the train error of OGD is unchanged.
Experiments We track the variation of the train accuracy of the samples in the memory through
tasks after being trained on all subsequent tasks. We consider the MLP hidden layer size as a proxy
for overparameterization. We run the experiments on the Permuted MNIST and Rotated MNIST
benchmarks, with the OGD algorithm. We vary the hidden size from 100 to 400.
Results Figure 3 shows that the variation of the train accuracy of OGD+ decreases uniformly with
the hidden size. This result indicates the forgetting of OGD decreases with overparameterization.
Figure 3: The variation of the train accuracy on the memorised samples from the each task, after the
model was trained on all tasks in sequence (higher is better). We vary the hidden size as a proxy for
overparameterization.
F.2.2 The importance of the memory size assumption
Thm. 2 states that in the NTK regime, given an infinite memory, the train error of OGD is unchanged.
Experiments We track the variation of the train accuracy of the samples in the memory through
tasks after being trained on all subsequent tasks, as a function of the memory size per task.
Results Figure 4 shows that the mean train accuracy variation decreases uniformly with the memory
size.
Figure 4: The variation of the train accuracy on the memorised samples from the first task, after the
model was trained on all tasks in sequence (higher is better). We vary the memory size per task from
100 to 300.
32
Under review as a conference paper at ICLR 2021
F.3 The importance of the NTK variation for Continual Learning (Sec. 6)
In this section, we present extended results on the importance of the NTK variation for Continual
Learning. For completion, we present the variation of the test accuracy of the model at the end of
training for all tasks. Also, we run the benchmarks on the A-GEM algorithm as a supporting materials
for the discussion in App. A.
F.3.1 PERMUTED MNIST
	Task 1	Task 2	Accuracy ± Std. (%) Task 3 Task 4 Task 5			Task 6	Task 7	
A-GEM OGD+ OGD	76.0±1.6 72.8±1.6 33.1±9.2	78.4±1.7 75.2±2.9 62.8±1.6	80.3±1.4 78.0±1.8 75.5±2.8	82.0±1.6 79.8±1.4 77.1±4.4	82.1±1.5 81.4±1.4 82.2±1.4	84.6±0.9 84.3±1.8 83.5±1.9	86.2±0.9 85.4±1.9 86.5±1.3	
	Task 8	Task 9	Accuracy ± Std. (%) Task 10 Task 11 Task 12			Task 13	Task 14	Task 15
A-GEM OGD+ OGD	87.1±1.1 87.3±1.1 86.8±0.5	89.1±0.9 88.4±1.2 88.4±1.3	89.0±1.1 90.2±0.7 90.2±0.9	90.3±0.7 91.5±0.8 91.6±0.4	91.5±1.0 92.4±0.5 92.7±0.2	92.5±0.8 93.3±0.4 93.2±0.3	93.6±0.4 94.0±0.3 94.0±0.1	94.7±0.2 94.5±0.1 94.4±0.1
Table 7: Permuted MNIST : The test accuracy of models from the indicated task after being trained
on all tasks in sequence. The best Continual Learning results are highlighted in bold.
F.3.2 ROTATED MNIST
	Task 1	Task 2	Accuracy ± Std. (%) Task 3 Task 4 Task 5			Task 6	Task 7	
A-GEM OGD+ OGD	76.1±2.3 65.4±1.2 41.7±1.6	78.3±2.1 67.8±1.1 44.7±1.4	83.6±1.1 75.6±1.1 53.2±1.7	86.1±0.8 80.1±1.5 61.4±1.4	88.1±0.5 83.9±1.1 68.7±1.2	89.5±0.1 86.3±1.2 74.8±1.2	91.0±0.3 88.7±0.8 81.3±0.8	
	Task 8	Task 9	Accuracy ± Std. (%) Task 10 Task 11 Task 12			Task 13	Task 14	Task 15
A-GEM OGD+ OGD	91.6±0.2 90.3±0.7 86.0±0.7	93.0±0.2 92.3±0.4 89.7±0.5	94.0±0.2 93.9±0.2 92.5±0.2	95.1±0.2 94.9±0.1 94.4±0.1	95.9±0.0 95.9±0.1 95.7±0.1	96.4±0.1 96.1±0.1 96.1±0.2	96.3±0.1 96.1±0.1 96.2±0.2	96.0±0.1 95.6±0.1 95.8±0.2
Table 8: Rotated MNIST : The test accuracy of models from the indicated task after being trained on
all tasks in sequence. The best Continual Learning results are highlighted in bold.
33
Under review as a conference paper at ICLR 2021
(a) Task 1
(b) Task 5
(d) Task 6
(c) Task 2
(e) Task 3
(g) Task 4
(i) Task 5
(h) Task 8
(j) Task 10
Figure 5: Test accuracy on the 10 first tasks of Rotated MNIST, for SGD, OGD, OGD+ and A-GEM.
The y-axis is truncated for clarity. We report the mean and standard deviation over 5 independent
runs. The test error is measured for every 250 mini-batch interval.
34
Under review as a conference paper at ICLR 2021
F.4 Comparison with other Continual Learning baselines
In order to get a broader picture on the robustness of OGD+ to Catastrophic Forgetting, we benchmark
it against standard Continual Learning baselines.
F.4.1 Evaluation
Denoting。7,「 the accuracy of the model on the task TT after being trained on the task TT, and b the
vector of test accuracies at initialisation, we track the following metrics presented by Chaudhry et al.
(2019) and Lopez-Paz & Ranzato (2017) :
Average Accuracy : ACC
1T
TX
τ=1
aT,τ
1T
Backward Transfer : BWT =———-E aT,τ -。丁,「
τ=1
Forward Transfer : FWT
Average Forgetting : AFM
1T
T-I X aτ-i,τ- bτ
τ=1
T-1
∑max	(atτ — aτ τ)
T =ι t∈{1,...,X-1}'t,τ S
(160)
(161)
(162)
(163)
F.5 Baselines
In addition to the standard baselines, SGD, EWC, SI and MAS, we compare OGD+ against OGD and
Stable SGD, which was recently introduced by Mirzadeh et al. (2020). They show that Stable SGD
outperforms OGD on all benchmarks, in order for the evaluation to be as fair and comprehensive as
possible, we also include this method in our benchmarks.
F.5.1 Complementary Results - Continual Learning Metrics
Permuted MNIST Table 9 shows that OGD+ outperforms the baselines on AAC, while it remains
competitive on AFM and BWT.
Method	AAC	BWT	FWT	AFM
Naive SGD	76.31 (±1.89)	-19.27 (±2.0)	1.12 (±1.14)	-19.27 (±2.0)
EWC	80.85 (±1.14)	-13.69 (±1.22)	0.97 (±0.8)	-13.69 (±1.22)
SI	86.69 (±0.4)	-4.02 (±0.3)	2.0 (±1.1)	-4.02 (±0.3)
MAS	85.96 (±0.72)	-4.84 (±0.73)	2.51 (±1.36)	-4.84 (±0.73)
Stable SGD	78.17 (±0.76)	-11.63 (±1.03)	1.02 (±0.15)	-11.63 (±1.03)
OGD	85.0 (±0.86)	-9.71 (±0.95)	0.19 (±0.71)	-9.71 (±0.95)
OGD+	88.65 (±0.38)	-5.98 (±0.36)	0.92 (±1.84)	-5.98 (±0.36)
Table 9: Comparison of the average accuracy, average forgetting, forward transfer and backward
transfer of several methods on the Permuted MNIST benchmark.
35
Under review as a conference paper at ICLR 2021
Rotated MNIST Table 10 shows that OGD+ is competitive with the baselines on AAC and AFM,
while it underperforms on the other metrics. Also it draws an improvement over OGD on AAC, BWT
and AFM. One probable reason is the relatively low overparameterization of the setting as shown in
Table 1.
Method	AAC	BWT	FWT	AFM
Naive SGD	71.06 (±0.41)	-25.99 (±0.46)	85.92 (±0.61)	-26.37 (±0.45)
EWC	80.96 (±0.42)	-9.03 (±0.44)	79.01 (±0.58)	-9.58 (±0.46)
SI	75.33 (±0.55)	-18.62 (±0.6)	82.66 (±0.61)	-19.24 (±0.58)
MAS	80.55 (±0.46)	-2.78 (±0.43)	70.85 (±0.64)	-6.0 (±0.48)
Stable SGD	88.92 (±0.19)	-2.22 (±0.54)	78.73 (±1.05)	-2.8 (±0.53)
OGD	79.17 (±0.33)	-17.17(±0.35)	85.66 (±0.61)	-17.77 (±0.35)
OGD+	87.73 (±0.5)	-7.88 (±0.55)	85.45 (±0.64)	-8.57 (±0.53)
Table 10: Comparison of the average accuracy, average forgetting, forward transfer and backward
transfer of several methods on the Rotated MNIST benchmark.
Split CIFAR100 Table 11 shows that OGD+ is not competitive on Split CIFAR100 on all metrics.
Also it draws the same performance as OGD on all metrics, one probable reason is the overparameter-
ization of the setting as shown in Table 1.
Method	AAC	BWT	FWT	AFM
Naive SGD	50.77 (±3.99)	-31.63 (±4.26)	0.31 (±1.17)	-31.82 (±4.1)
EWC	56.82 (±1.75)	-20.32 (±2.5)	0.56 (±1.26)	-20.43 (±2.44)
SI	66.66 (±2.07)	-9.41 (±2.51)	0.74 (±1.26)	-10.03 (±2.38)
MAS	66.33 (±1.13)	-3.31 (±1.12)	0.56 (±0.93)	-4.64 (±0.98)
Stable SGD	72.86 (±0.9)	-11.14 (±0.99)	-0.19 (±1.69)	-11.14 (±0.99)
OGD	61.82 (±1.24)	-20.84 (±1.44)	0.33 (±1.02)	-20.86 (±1.41)
OGD+	61.11 (±1.31)	-21.56 (±1.29)	0.32 (±0.97)	-21.59 (±1.27)
Table 11: Comparison of the average accuracy, average forgetting, forward transfer and backward
transfer of several methods on the Split CIFAR100 benchmark.
CUB200 Table 12 shows that OGD+ is not competitive on the CUB200 benchmark on all metrics.
Also it draws the same performance as OGD on all metrics, one probable reason is the overparameter-
ization of the setting as shown in Table 1.
Method	AAC	BWT	FWT	AFM
Naive SGD	53.93 (±0.86)	-17.07 (±0.88)	0.15 (±0.45)	-17.07 (±0.88)
EWC	62.15 (±0.37)	-3.88 (±0.49)	-0.12 (±0.57)	-3.88 (±0.49)
SI	63.17 (±0.42)	-3.24 (±0.63)	-0.23 (±0.51)	-3.43 (±0.46)
MAS	61.43 (±0.68)	-7.76 (±0.87)	-0.17 (±0.8)	-7.85 (±0.9)
Stable SGD	58.79 (±0.36)	-6.16 (±0.23)	-0.01 (±0.8)	-6.23 (±0.17)
OGD	57.89 (±0.89)	-12.97 (±0.94)	0.02 (±0.55)	-12.99 (±0.92)
OGD+	57.99 (±0.52)	-12.88 (±0.51)	0.08 (±0.53)	-12.89 (±0.51)
Table 12: Comparison of the average accuracy, average forgetting, forward transfer and backward
transfer of several methods on the CUB200 benchmark.
36
Under review as a conference paper at ICLR 2021
F.5.2 Complementary Results - Validation accuracy through training
(b) Permuted MNIST - Task 7
(a) Permuted MNIST - Task 1
(c) Rotated MNIST - Task 1
(d) Rotated MNIST - Task 7
(e) Split CIFAR100 - Task 1
(f) Split CIFAR100 - Task 7
(g) Split CUB200 - Task 1
(h) Split CUB200 - Task 7
Figure 6: Test accuracy through tasks for different Continual Learning methods on the MNIST and
CIFAR100 and CUB200 benchmarks. The y-axis is truncated for clarity. We report the mean and
standard deviation over 5 independent runs. The test error is measured at the end of each task.
37