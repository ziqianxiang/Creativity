Under review as a conference paper at ICLR 2021
Learning Predictive Communication by Imagi-
nation in Networked System Control
Anonymous authors
Paper under double-blind review
Ab stract
Dealing with multi-agent control in networked systems is one of the biggest chal-
lenges in Reinforcement Learning (RL) and limited success has been presented
compared to recent deep reinforcement learning in single-agent domain. However,
obstacles remain in addressing the delayed global information where each agent
learns a decentralized control policy based on local observations and messages
from connected neighbors. This paper first considers delayed global information
sharing by combining the delayed global information and latent imagination of
farsighted states in differentiable communication. Our model allows an agent to
imagine its future states and communicate that with its neighbors. The predictive
message sent to the connected neighbors reduces the delay in global information.
On the tasks of networked multi-agent traffic control, experimental results show
that our model helps stabilize the training of each local agent and outperforms
existing algorithms for networked system control.
1	Introduction
Networked system control (NSC) is extensively studied and widely applied, including connected
vehicle control (Jin & Orosz, 2014), traffic signal control (Chu et al., 2020b), distributed sensing (Xu
et al., 2016), networked storage operation (Qin et al., 2015) etc. In NSC, agents are connected
via a communication network for a cooperative control objective. For example, in an adaptive
traffic signal control system, each traffic light performs decentralized control based on its local
observations and messages from connected neighbors. Although deep reinforcement learning has
been successfully applied to some complex problems, such as Go (Silver et al., 2016), and Starcraft
II (Vinyals et al., 2019), itis still not scalable in many real-world networked control problems. Multi-
agent reinforcement learning (MARL) addresses the issue of scalability by performing decentralized
control. Recent decentralized MARL performs decentralized control based on the assumptions of
global observations and local or global rewards (Zhang et al., 2018; 2019a; Qu et al., 2019; 2020b;a),
which are reasonable in multi-agent gaming but not suitable in NSC. A practical solution is to allow
each agent to perform decentralized control based on its local observations and messages from the
connected neighbors. Various communication-based methods are proposed to stabilize training and
improve observability, and communication is studied to enable agents to behave as a group, rather
than a collection of individuals (Sukhbaatar & Fergus, 2016; Chu et al., 2020a).
Despite recent advances in neural communication (Sukhbaatar & Fergus, 2016; Foerster et al., 2016;
Chu et al., 2020a), delayed global information sharing remains an open problem that widely exists
in many NSC applications. Communication protocol not only reflects the situation at hand but also
guides the policy optimization. Recent deep neural models (Sukhbaatar & Fergus, 2016; Foerster
et al., 2016; Hoshen, 2017) implement differentiable communication based on available connections.
However, in NSC, such as traffic signal control, each agent only connects to its neighbors, leading to
a delay in receiving messages from the distant agents in the system, and the non-stationarity mainly
comes from these partial observation (Chu et al., 2020a). Communication with delayed global infor-
mation limits the learnability of RL because RL agents can only use the delayed information and not
leverage potential future information. Moreover, it is not efficient in situations where an environ-
ment is sensitive when the behaviours of agents change. It is therefore of great practical relevance
to develop algorithms which can learn beyond the communication with the delayed information
sharing.
1
Under review as a conference paper at ICLR 2021
In this paper we introduce ImagComm that learns communication by imagination for multi-agent
reinforcement learning in NSC. We leverage the model of the agent’s world to provide an estimate of
farsighted information in latent space for communication. At each time step, the agent is allowed to
imagine its future states in an abstract space and convey this information to its neighbors. Therefore
unlike previous works, our communication protocol conveys not only the current sharing informa-
tion but also the imagined sharing information. It is applicable whenever communication changes
frequently, e.g. at every time step agents may receive new communication information.
We summarize our main contributions as follows: (1) We first introduce the imagination module
that can be used to learn latent dynamics for communication in networked multi-agent systems
control. (2) We predict the future state of each local agent and allow each agent to convey the latent
state to neighbors as messages, which reduce the delay of global information. (3) We demonstrate
that leveraging the predictive communication by imagination in latent space succeeds in networked
system control. We explore this model on a range of NSC tasks. Our results demonstrate that our
method consistently outperform baselines on these tasks.
2	Related Work
Networked system control (NSC) considers the problem where agents are connected via a communi-
cation network for a cooperative control objective, such as autonomous vehicle control (Jin & Orosz,
2014), adaptive traffic signal control (Chu et al., 2020b), and distributed sensing (Xu et al., 2016),
etc. Recently reinforcement learning has become popular for NSC through decentralized control
and communications by networked agents.
Communication is an important part for multi-agent RL to compensate for the information loss
in partial observations. Heuristic communication allows the agents to share some certain forms
of information, such as policy fingerprints from other agents (Foerster et al., 2017) and averaged
neighbor’s policies (Yang et al., 2018). Recently end-to-end differentiable communications have
become popular (Foerster et al., 2016; Sukhbaatar & Fergus, 2016; Chu et al., 2020a) since the
communication channel is learned to optimize the performance. Attention-based communication
(Hoshen, 2017; Das et al., 2019; Singh et al., 2019) selectively send messages to the agents chosen,
however, these are not suitable for NSC since the communication is allowed only between connected
neighbors. Our method adopts differentiable communication with end-to-end training. Compared
to existing works, we introduce a new predictive communication module through learning latent
dynamics.
Learning latent dynamics has been studied to solve single agent tasks, such as E2C (Watter et al.,
2015), RCE (Banijamali et al., 2018), PlaNet (Hafner et al., 2019), SOLAR (Zhang et al., 2019b)
and so on. Lee et al. (2019) and Gregor et al. (2019) learn belief representations to accelerate model-
free agents. World Models (Ha & Schmidhuber, 2018) learn latent dynamics in a two-stage process
to evolve linear controllers in imagination. I2A (Racaniere et al., 2017) hands imagined trajectories
to a model-free policy based on a rollout encoder. In contrast to these works, our work considers
multi-agent tasks and learns predictive communication by imagination in latent space.
3	Preliminaries
In networked system control problem, we work with a networked system, which is described by
a graph G(V, E), where i ∈ V denotes the ith agent and ij ∈ E denotes the communication link
between agents i and j . The corresponding networked (cooperative) multi-agent MDP is defined
by a tuple (G, {Si , Ai}i∈V, {Mij }ij∈E, p, {ri}i∈V). Si and Ai are the local state space and action
space of agent i. Let S := ∪i∈V Si and A := ∪i∈VAi, the MDP transitions follow a stationary
probability distribution p : S × A × S → [0, 1]. The global reward is denoted by r : S × A → R and
defined as r =步 Pi∈v r indicating that all local rewards are shared globally. The communication
is limited to neighborhoods. M denotes the message space for the communication model. That is
each agent i observes sit := s%,t ∪ mχ,i,t, where s%,t ∈ Si denotes local state space of agent i and
mNii,t := {mji,t}j∈Ni and Ni := {j ∈ V|ji ∈ E}. Message mji,t ∈ Mji denotes all the available
information at an agent’s neighbor. In NSC, the system is decentralized and the communication
is limited to neighborhoods. Each agent i follows a decentralized policy ∏ : Si XAi → [0,1]
2
Under review as a conference paper at ICLR 2021
Figure 1: Flow diagram of predictive communication. Left: An illustration of a networked system
example is an adaptive cruise control (ACC) system with vehicle-to-vehicle (V2V) communication.
For the information sharing, agent i knows information at time t including si,t ∪ si-1,t ∪ si-2,t-1 ∪
si-3,t-2, which are the delayed global observations. For the communication, based on world models
bi,t is learned from its neighbor’s future states, which compensate for delayed information. Right:
policy representation that includes messages of neighbors generated by the imagined module.
to choose its own action ai,t 〜πi(∙∣Si,t) at time t. The objective is to maximize En[R0], where
Rt =	l∞=0 γlrt+l and γ is a discount factor.
4	Methodology
Our goal is to learn predictive communication on a particular observation or environment state. We
start by introducing the networked MDP with neighborhood communications and delayed informa-
tion issue in communication. Then, we describe ImagComm that utilizes predictive communication,
which we learn the agent’s world model to provide an additional context for communication.
4.1	Delayed Communication in networked system control
Following the setting of NSC in (Chu et al., 2020a), we assume that all messages sent from agent i
are identical and we denote mij = mi , ∀j ∈ Ni . The message explicitly includes state s and policy
π and agent belief h, i.e., mi,t = si,t ∪ πi,t-1 ∪ hi,t-1 in communication. Note that πi,t-1 is the
probability distribution over discrete actions. Thus for each agent in NSC, Si,t := sVi,t ∪ πNi,t-1 ∪
hNi ,t-1 .
Note the communication phase is prior-decision, so only hi,t-1 and πi,t-1 are available. This
protocol can be easily extended for multi-pass communication. We assume that any information
that agent j knows at time t can be included in mji,t and mji,t = sj,t ∪ {mkj,t-1}k∈Nj . Then
Si,t := si,t ∪ {sj,t+1-dij }j∈V/{i}, which includes the delayed global observations. dij∙ indicates the
distance between i and j, i.e. the hops between two agents on the graph of the networked system.
We illustrate the delayed information in Figure 1. A more rigorous analysis of this conclusion can
be found in the Appendix A.
4.2	Predictive Communication
To reduce the delay of global information, we consider a forward model for predicting future states
of each agentj, then sj,t+1 can be encoded as a message for communication, and agent i can benefit
from this information. Let Si,t be the abstract state of ith agent, Wi ∈ Wi be a world model of
the transition dynamics from Si,t to the abstract state Si,t+1, and let bi,t := ∪k=1 Si,t+τ denote the
predictive message. We aim to build a policy based on delayed global observations and predictive
messages. The value of policy πi can be defined as Viπ,W (s) based on the model Wi:
Vπ,W(s,aNi) = Eai,t~∏i(•岛,t,bi,t) [Rπt 1 St= s,aNi,t = aNi].	⑴
Learning based on (1) has the benefit of reduced delay in global information compared to that with-
out bi,t; this is formally presented in Proposition 1. Proofs are provided in Appendix A.
Proposition 1. ImagComm can reduce the delay of global information by incorporating a predictive
model in the communication protocol.
3
Under review as a conference paper at ICLR 2021
—∙^~∙
We are now interested in constructing an abstract model Wi(∙;夕)to approximate Wi, which operates
—∙^~∙
on an abstract state. Let ^i,t+ι be the new abstract state sampled by ^i,t+ι 〜Wi(^i,t). We want to
π Wc
minimizek^i,t+ι - gi(si,t+ι; ψ)k, where gi(∙; ψ) is an embedding of raw states. Let Vi , be the
value function of the policy on the estimated model Wi. Towards optimizing V∏,w (s, &n), we
build a lower bound as follows and maximize it iteratively:
Vπ,W* (s,aNi) ≥ Vπ,c(s,aNi) - D(C,∏),
(2)
where D(Wc, π) ∈ R bounds the discrepancy between Viπ,W ? and Viπ,W. In practice, D(Wci, πi) is
defined as
1∙-∙∙	1∙~∙∙
Dnref(Ci,πJ= α ∙ ES0,…,St,〜∏ref [kci⑸,/ - gi(Si,t+I) k],	⑶
where α is a hyperparameter, πiref is the policy used for sampling. For each agent, we solve the
following problem:
πk+1, W k+1 = argmax Viπ,W -Dπk δ(W,π).
π∈Π,W ∈W	i ,
(4)
With the predictive imagination module, each agent utilizes the estimate of predictive state informa-
tion to learn its belief and optimize the control performance of all other agents. Follow the analysis
in (Luo et al., 2018), we can show that ImagComm can lead to monotonic improvement in policy
iteration. Proofs are defered to Appendix A.
Proposition 2. Suppose that W* ∈ Wi is the optimal model and the optimization problem in
equation (4) is solvable at each iteration. Solving (4) produces a sequence of policies πi0 , . . . , πiT
with monotonically increasing values: Viπ ,W* ≤ Viπ ,W* ≤ ∙ ∙ ∙ ≤ Viπ ,W*.
A conclusion following directly from Proposition 2 is that solving (4) will converge to a local max-
imum. ImagComm considers build a world model and predict the farsighted state by a imagination
module to eliminate the delay in global information and henceforth reduce the negative influence
of the partial observability. Because the future information after time t compensate for some of the
delayed information at time t. Next we will present the differentiable neural communication with
imagination.
4.3	Differentiable Neural Communication
In our approach, an agent performs the following operations throughout the agent’s life time: learn-
ing the latent dynamics model from the dataset of past experience to predict future states of itself,
encoding the imagined features into message and learning differentiable communication models to-
gether with the predictive information. Specifically, the predictive model lets us predict the states
ahead in the latent space without having to observe.
Different with previous works that use hi,t = gVi (hi,t-1, si,t, mNi,t), we propose to learn com-
munication with imagination (ImagComm), as shown in Figure 1, to add the imagined delayed
information to communication:
hi,t = gVi (hi,t-1, si,t, mNi,t, bNi,t)	(5)
where bNi ,t indicates the module of the predictive message, mNi ,t represents the standard com-
munication module, which is the same as previous communication work. gVi is a differentiable
function to extract informaton for the agent’s beliefs. For example, gVi can be LSTM (Hochreiter &
Schmidhuber, 1997).
Compared to (Chu et al., 2020a), ImagComm uses imagination module to provide an augmented ob-
servation of agents and reduces delays in global information. Compared to model-based approaches
(Luo et al., 2018; Janner et al., 2019), the differences are two-fold: i) instead of learning a model for
a single-agent MDP, each agent learns a decentralized predictive model locally; ii) instead of aiming
to improve the sampling efficiency, we aim to augment the message for communication, thus reduc-
ing the delay in global information from each agent’s view; this can be viewed as a combination of
model-based and model-free aspects.
4
Under review as a conference paper at ICLR 2021
4.4	Practical implementations
Simply we assume each agent is based on A2C (Advantage Actor-Critic) models. Let {πθi}i∈V and
{Vωi}i∈V be the decentralized actor-critics, and{(si,τ,mNii,τ,ai,τ,ri,τ,bNii,τ)}i∈V,τ∈Bbetheon-
policy minibatch from networked MDPs under stationary policies {πθi}i∈V. For each agent with the
belief hi,t, the actor and critic become ∏θi (hi) and Vω⅛(hi, aʌ;) for fitting the optimal policy ∏ and
value function V πi . ImagComm has three components based on (5). mNi,t represents the message
that will be passed to hi,t with mNi,t = sNi,t ∪ πNi,t-1 ∪ hNi,t-1. For the predictive message bi,t,
—∙^~∙
^i,t = fi(sVi,t, hi,t-ι; φi) and ^i,t+ι = Wi(^i,t). Let Φi = {φi, ψi,夕i} denote the parameters for
abstract state embedding, raw state embedding, and the imagination module for agent i. Then each
actor and critic are updated by losses
L (θi) = B X(TOg πθi (ai,τ I hi,τ)) Aπ,τ + βH[πθi (ai I hi,τ)],
|B| τ∈B
L (ωi) = ∣B∣ X (RnT- Vωi (hi,τ ,aNi ,τ ))2 ,
IBI τ∈B
L(Φi) = ⅛ XhkWi(fi(sVi,τ,hi,τ-1)) - gi(si,τ+1)ki ,
IBI τ∈B
(6a)
(6b)
(6c)
τ -1	0
where R∏τ = ETB=T YT T%，+ YTB TVi,τB is the target action-value, v5 =匕厂(si,τ, a,Ni,τ)
is the state-value as baseline, Aiπ,τ = Riπ,τ - vi,τ is the advantage function as critic, and β is the
hyperparameter of the entropy loss. In implementation of Eq. (6c), we adopt the root mean square
loss (RMSE). The overall algorithm is provided in Algorithm 1.
Algorithm 1 Imagination-based policy optimization
1:	Initialize policy {πi0}i∈V, predictive model {Wci0}i∈V, empty mini-batch DB.
2:	for k = 0, 1,..., T do
3:	Collect a batch of data with {πik}i∈V in real environment:
DB = {(si,τ , mNi i,τ , ai,τ , ri,τ , bNi i,τ)}i∈V ,τ∈B .
4:	Update policy {πik+1}i∈V under imagination by solving (6a).
5:	Update critic {Vik+1}i∈V under imagination by solving (6b).
6:	Update fik+1, gik+1 and Wcik+1 on dataset DB by solving (6c).
7:	end for
5 Numerical Experiments
We evaluate ImagComm on several challenging environments in networked system control and com-
pare it to current state-of-the-art algorithms for communication.
5.1	Environments
We use four existing simulation environments: ATSC Grid, ATSC Monaco, Cooperative Adaptive
Cruise Control (CACC) Catch-up and CACC Slow-down (Chu et al., 2020b). The ATSC environ-
ments are developed based on SUMO (Krajzewicz et al., 2012). In ATSC, ATSC Grid simulates
a 5 × 5 synthetic traffic grid , as shown in Figure 2a. ATSC Monaco simulates a real-world 28-
intersection traffic network from Monaco city, as shown in Figure 2b. In homogeneous scenario,
i.e. ATSC Grid, all agents have the same action space consisting of five pre-defined signal phases.
While in heterogeneous scenario, i.e. ATSC Monaco, agents have a variety of action spaces. For
both scenarios, The objective of ATSC is to adaptively control traffic lights at the intersections to
minimize traffic congestion based on real-time road-traffic measurements. Local state is defined
as st,i = {waitt[l], wavet[l]}ji∈E k工?., where l is each incoming lane of intersection i. Wait[∙]
measures the cumulative delay of the first vehicle and wave[∙] measures the total number of ap-
proaching vehicles along each incoming lane within 50m to the intersection. Rewards for each
agent are defined as r%,t = - Pji∈E ι∈L-i (queuet+∆t[l]), where queue[∙] denotes the number of
queuing vehicles on an approaching lane, which is measured by induction-loop detectors (ILD). In
5
Under review as a conference paper at ICLR 2021
(a) ATSC Grid.
(C)CACC (Catch-up and
(b) ATSC Monaco. Slow-down).
Figure 2: Environments on adaptive traffic signal control (ATSC) and cooperative adaptive cruise
control (CACC) systems.
CACC, CACC Catch-up scenario simulates a string of8 vehicles for 60s with a 0.1s control interval,
where target speedV = 15m/s and initial headway hi，。> hi，。，∀i = 1. cacc Slow-down also
simulates a string of 8 vehicles for 60s with a 0.1s control interval, where initial headway hi,。= h*
and target speed Vt linearly decreases to 15m/s during the first 30s and then stays at constant. In
both scenarios, the objective is to adaptively coordinate a platoon of vehicles to minimize the car-
following headway and speed perturbations based on real-time vehicle-to-vehicle communication.
Each vehicle observes and shares its headway h, velocity v , and acceleration a to neighbors within
one step. Models are trained to recommend appropriate hyper-parameters (α°, β°) for each OVM
controller(Bando et al., 1995), selected from four levels {(0,0),(0.5,0),(0,0.5),(0.5,0.5)}. Rewards
are designed as a cost function. Assuming the target headway and velocity profile are h* = 20m
and vt*, respectively, the cost of each agent is (hi，t - h*)2 + (vi，t - vt*)2 + 0.1ui2，t. Whenever a col-
lision happens (hi，t < 1m), a large penalty of 1000 is assigned to each agent and the state becomes
absorbing. An additional cost 5 (2hst - hi，t)2+ is provided in training for potential collisions.
5.2	Baselines and Setup
All the baselines are implemented based on the A2C agent (Mnih et al., 2016) following the methods
in Eq. (6a)(6b). The baselines include one non-communicative policy IA2C (Mnih et al., 2016), and
three communicative policies NeurComm (Chu et al., 2020a), CommNet (Sukhbaatar & Fergus,
2016), DIAL (Foerster et al., 2016). The details will be provided in Appendix B.
For ImagComm, we found that two-hop information already lead to com-
pelling performance and we use one-step imagination. We use hi，t =
LSTM(hi,t—i, Concat(I∙elu(SVi,t),reu(πN,t-i), reiu(hNi,t-i),reiu(bNi,))), ι and ”	=
tanh(concat(relu(sVi，t), hi，t-1)). Then hi，t is fed into two fully-connected neural networks
producing policy and value separately- Fote that for state encoding and gi(.)： weuse one fully-
connected layer with tanh activation. For message extracting gVi , we use a LSTM layer. All
layers are set up with 64 hidden units. For the imagination module, we stack two fully-connected
layers. Other settings include actor learning rate 5 × 10-4, critic learning rate 2.5 × 10-4, entropy
coefficient β = 0.01, batch size |B| = 120. Each method is trained over 1M steps under an
actor-critic framework. In ATSC, entropy coefficient β = 0.01, batch size |B| = 120 and in CACC,
β = 0.05, batch size |B| = 60. We use a different random seed to initialize the environment
without loss of generality. As different initial seeds may cause fluctuation, we smooth the learning
curve using moving averages with a window size of 100 episodes following NeurComm(Chu et al.,
2020a).
5.3	Training results
Figure 3 compares the learning curves of different methods on four environments.The results show
that ImagComm overall performs better than others for these environments. In both ATSC envi-
ronments, our model learns quickly with good stability and performs well in the end. In CACC,
the standard deviation of episode returns is high due to the large penalty of collisions. In ATSC
Grid, CommNet and DIAL gain little performance improvement even after 0.4M training steps,
while ImagComm and NeurComm learn with a faster speed and end with lower deviation. Imag-
6
Under review as a conference paper at ICLR 2021
3poi2d3 3s,L3>“
0.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0M
Training step
(a) ATSC Grid.
COmmNet
3poi2d3 3s,L3>“
0.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0M
Training step
-200
-400
-600
-800
-1000
3poi2d3 3s,L3>“
0.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0M
Training step
3poi2d3 3s,L3>“
0.1M 0.2M 0.3M 0.4M 0.5M 0.6M 0.7M 0.8M 0.9M 1.0M
Training step
(b) ATSC Monaco. (c) CACC Catch-up. (d) CACC Slow-down.
Figure 3: Learning curves for our method and different baselines on four environments
3po=d3 3s,L3>“
3poi2d3 3s,L3>“
3poi2d3 3s,L3>“


(a) ATSC Grid.	(b) ATSC Monaco. (C) CACC Catch-up. (d) CACC Slow-down.
Figure 4: Learning curves for ImagComm with different α.
Comm learns slower at the initial state of training owing to the learning of imagination module,
but it quickly outperforms NeurComm after 0.5M steps, showing the benefits of incorporating the
predictive communication. In ATSC Monaco, DIAL and NeurComm learn fast before 0.3M steps,
but NeurComm does not improve much after that, and ImagComm outperforms all the baselines
after 0.4M steps. In the CACC environments, ImagComm works better and more stable than others,
with a better performance in the end. It shows that adding predictive communication is useful for
these tasks. Though IA2C works well sometimes, it is very unstable, which proves the effectiveness
of communication. NeurComm and CommNet also work stable in both environments, but suffering
from delayed communication in such real-time scenarios, they perform worse than ImagComm.
一^一、
Hyper parameter study We investigate the impact of coefficient α of D(W, π) in our Imag-
Comm by comparing the learning curves among {0.01, 0.1, 1, 10, 100} on the ATSC and CACC
environments in Figure 4. The results show that different α values have different results. In ATSC
environments, α = 1, 0.1 work best for ATSC Grid and Monaco respectively. In CACC Catch-up
scenario, different α values yield similar results. But in CACC Slow-down scenario, α = 1 performs
best.
5.4 Execution performance
Additionally, we investigate the execution results of different methods. We use two more metrics,
average queue length and average intersection delay, to get a deeper understanding of controllers’
execution impact. Average intersection delay is the mean of waiting time of all vehicles, which is
another congestion metric.
Figure 5a and Figure 5b compare average queue length of different trained models in one episode.
As shown in the figures, ImagComm achieves the best performance in both scenarios. In ATSC
Grid, IA2C and CommNet both fail to reduce traffic pressure and the congestion level remains high
in the end. In contrast, ImagComm helps agents communicate with each other effectively, so the
queue length increases slower and remains low in the end.
Figure 5c and Figure 5d compare average intersection delay of different trained models in one
episode. In Figure 5c, all four communicative policies outperform IA2C dramatically, which proves
the effectiveness of communication. However, when it comes to ATSC Monaco, results turn out
differently. In Figure 5d, IA2C achieves the lowest average intersection delay while communicative
policies perform worse. This can be explained by the emphasis on the queue length as we only con-
sider queue length in reward functions. Intersection delay is not explicitly included in rewards. Thus
communicative models may tend to block vehicles in short queues. Although this inclination helps
7
Under review as a conference paper at ICLR 2021
(a) Averaged queue length (b) Averaged queue length (c) Averaged intersection (d) Averaged intersection
in ATSC Grid.	in ATSC Monaco. delay in ATSC Grid. delay in ATSC Monaco.
Figure 5: Performance on average queue length (a, b) and average intersection delay (c, d) in the
ATSC setting.
Table 1: Average reward comparison over trained models.
Scenario Name	ImagComm	NeurComm	CommNet	DIAL	IA2C
ATSC Grid	-240.91	-_-302.68~~	-339.16	-354.52	-615.85
ATSC Monaco	-168.18	-318.90	-509.26	-202.91	-385.71
CACC Catch-up	-196.60	-170.27	-364.16	-346.85	-449.27
CACC Slow-down	-1771.99	-2109.40	-1727.24	-2483.48	-1732.46
Table 2: Performance of MARL controllers in ATSC environments: synthetic traffic grid (top) and
Monaco traffic network (bottom). Best values are in bold.
Temporal Average Metrics	ImagComm	NeurComm	CommNet	DIAL	IA2C
avg queue length [veh]	088	∏7	1.47	1.64	3.84
avg vehicle speed [m/s]	3.65	3.43	3.15	2.89	0.73
trip delay [s]	194	215	225	262	950
avg queue length [veh]	098	1.75	2.65	1.18	2.10
avg vehicle speed [m/s]	0.49	0.46	0.75	0.86	1.15
trip delay [s]	594	1445	719	566	479
reduce the queue length, average intersection delay becomes longer. This problem can be alleviated
through adjusting the reward functions.
We freeze and evaluate our model for another 50 episodes using different seeds and present the
average rewards in Table 1. The results show that in ATSC scenarios, ImagComm outperforms
other models by a large margin. We further investigate queue length and vehicle speed in ATSC
enviroments and present the results in Table 2. In ATSC Grid, ImagComm achieves the lowest queue
length and highest average vehicle speed, which means vehicles flow smoothly and ImagComm
greatly reduces congestion level. In ATSC Monaco, ImagComm achieves the lowest queue length,
but CommNet attains a higher average vehicle speed. This means ImagComm makes vehicles move
steadily rather than getting blocked on the road for a long time, but with a sacrifice of moving slowly.
6 Conclusions
In this work, we study the delayed communication problem for decentralized MARL in networked
system control. We have introduced an imagination module to predict farsighted information for
predictive communication. ImagComm combines the delayed global information and predictive
state information and performs end-to-end training of the neural communication and imagination
module to optimize the control performance in NSC. Extensive empirical studies demonstrate that
by leveraging world models for learning the latent delayed information for communication, our
method achieves the compelling performance gains in the challenging traffic signal control and
adaptive cruise control tasks. We hope that our work will provide inspiration for the research in
model-based learning for (networked) multi-agent systems.
8
Under review as a conference paper at ICLR 2021
References
Masako Bando, Katsuya Hasebe, Akihiro Nakayama, Akihiro Shibata, and Yuki Sugiyama. Dy-
namical model of traffic congestion and numerical simulation. Physical Review E, 51(2):1035,
1995.
Ershad Banijamali, Rui Shu, Hung Bui, Ali Ghodsi, et al. Robust locally-linear controllable embed-
ding. In International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1751-
1759. PMLR, 2018.
Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for net-
worked system control. In International Conference on Learning Representations (ICLR), 2020a.
URL https://openreview.net/forum?id=Syx7A3NFvH.
TianshU Chu, Jie Wang, Lara Codeca, and Zhaojian Li. Multi-agent deep reinforcement learning for
large-scale traffic signal control. IEEE Transactions on Intelligent Transportation Systems, 21:
1086-1095, 2020b.
Abhishek Das, Theophile Gervet, Joshua Romoff, Dhruv Batra, Devi Parikh, Mike Rabbat, and
Joelle Pineau. Tarmac: Targeted multi-agent communication. In International Conference on
Machine Learning (ICML), pp. 1538-1546, 2019.
Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to
communicate with deep multi-agent reinforcement learning. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 2137-2145, 2016.
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In In International Conference on Machine Learning (ICML), pp. 1146-1155, 2017.
Karol Gregor, Danilo Jimenez Rezende, Frederic Besse, Yan Wu, Hamza Merzic, and Aaron van den
Oord. Shaping belief states with generative environment models for rl. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 13475-13487, 2019.
David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning (ICML), pp. 2555-2565. PMLR, 2019.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), pp. 2701-2711, 2017.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
12519-12530, 2019.
I Ge Jin and Gabor Orosz. Dynamics of connected vehicle systems with delayed acceleration feed-
back. Transportation Research Part C: Emerging Technologies, 46:46-64, 2014.
Daniel Krajzewicz, Jakob Erdmann, Michael Behrisch, and Laura Bieker. Recent development and
applications of sumo-simulation of urban mobility. International Journal on Advances in Systems
and Measurements, 5(3&4), 2012.
Alex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953,
2019.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 6379-6390, 2017.
9
Under review as a conference paper at ICLR 2021
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Interna-
tional Conference on Learning Representations (ICLR), 2018.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International Conference on Machine Learning (ICML), pp. 1928-l937, 2016.
Junjie Qin, Yinlam Chow, Jiyan Yang, and Ram Rajagopal. Distributed online modified greedy
algorithm for networked storage operation under uncertainty. IEEE Transactions on Smart Grid,
7(2):1106-1118, 2015.
Chao Qu, Shie Mannor, Huan Xu, Yuan Qi, Le Song, and Junwu Xiong. Value propagation for
decentralized networked deep multi-agent reinforcement learning. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), pp. 1182-1191, 2019.
Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning
for networked systems with average reward. arXiv preprint arXiv:2006.06626, 2020a.
Guannan Qu, Adam Wierman, and Na Li. Scalable reinforcement learning of localized policies for
multi-agent networked systems. In Learning for Dynamics and Control, pp. 256-266. PMLR,
2020b.
Sebastien Racaniere, TheoPhane Weber, David Reichert, Lars Buesing, Arthur Guez,
Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li,
et al. Imagination-augmented agents for deep reinforcement learning. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 5690-5701, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, and Marc Lanctot. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar. Learning when to communicate at scale
in multiagent cooperative and competitive tasks. In International Conference on Learning Repre-
sentations (ICLR), 2019.
Sainbayar Sukhbaatar and Rob Fergus. Learning multiagent communication with backpropagation.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 2244-2252, 2016.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Juny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control:
A locally linear latent dynamics model for control from raw images. In Advances in Neural
Information Processing Systems (NeurIPS), pp. 2746-2754, 2015.
Yong Xu, Renquan Lu, Peng Shi, Hongyi Li, and Shengli Xie. Finite-time distributed state estima-
tion over sensor networks with round-robin protocol and fading channels. IEEE Transactions on
Cybernetics, 48(1):336-345, 2016.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning (ICML), pp.
5571-5580, 2018.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning (ICML), pp. 5872-5881, 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Decentralized multi-agent reinforcement learning
with networked agents: Recent advances. arXiv preprint arXiv:1912.03821, 2019a.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew Johnson, and Sergey Levine.
Solar: Deep structured representations for model-based reinforcement learning. In International
Conference on Machine Learning (ICLR), pp. 7444-7453. PMLR, 2019b.
10
Under review as a conference paper at ICLR 2021
A Proofs
Chu et al. (2020a) has shown that the communication protocol allows the local agent to utinize the
delayed global information. We cite the proof here to stay self-contained.
Lemma 1. (Chu et al., 2020a) By communicating through hi,t = gVi (hi,t-1, si,t, mNi,t), the de-
layed global information is utilized to estimate each hidden state, that is
hi,t ⊃ si,θ:t ∪ {sj,0：t+1-dij #jQ:t-dij) j∈v∖{}
where x ⊃ y if information y is utilized to estimate x, and x0:t := {x0, x1, . . ., xt}.
Proof. Based on the definition of communication protocol, mi,t ⊃ hi,t-1, and hi,t ⊃ hi,t-1 ∪
sVi,t ∪ πNi,t-1 ∪ mNi,t. Hence,
hi,t ⊃ si,t ∪ {sj,t, πj,t-1 }j∈Ni ∪ {hj,t-1}j∈Vi
⊃ si,t ∪ {sj,t, πj,t-1}j∈Ni ∪ sj,t-1 ∪ {sk,t-1, πk,t-2}k∈Nj ∪ {hk,t-2}k∈Vj j∈V
=si,t-1：t ∪ {sj,t-1：t, nj,t-2：t-1 }j∈Ni ∪ {sj,t-1,πj,t-2}j∈{v∣dij =2} ∪ {hj,t-2}j∈{v∣dij≤2}	(7)
⊃…
⊃ si,θ:t ∪ {Sj,0∙∙t,πj,t-2∙∙t-1}j∈Ni ∪ {sj,0：t-1,nj,0：t-2}jw{v|dij = 2}
∪ ... ∪ {Sj,0:t + 1—dmax , nj，0:t -dmax }j∈{V∣dj = dmaχ},
which concludes the proof.	□
A.1 Proposition 1
Proof of Proposition 1. Based on (5), we have that
hi,t ⊃ {mj,t}j∈Ni ∪ {bj,t}j∈Ni ⊃ {hj,t-1}j∈Ni
⊃ {mj,t-1}j∈{V∣dij = 2} ∪ {bj,t-1}j∈{V∣dij =2} ⊃ ^^^	⑻
⊃ {mj,t+1-d}j∈{V∣dij=d} ∪ {bj,t+1-d}j∈{V∣dij =d} ⊃ …
Since m# = sj,t ∪ ∏j,t-ι ∪ hj,t-ι and bj,t = ∪T=∖Sj,t+τ With k the step of forward imagination,
hi,t ⊃ {sj,t+1-d}j∈{v∣dij=d} ∪ {6j,t+k+1-d}jE{v|dij=d} ∙	(9)
Hence, ^i,t=τ is included in the observation of agent j at time T + dj - k - 1, si,t=τ, ahead of
si,t=τ at time T + dj — 1 by k steps.	□
A.2 Proposition 2
Proof of Proposition 2. We follow the sketch in (Luo et al., 2018). From (2), we have that
Vπk+1,W* ≥ Vπk+1,c -Dnk (c, ∏k+ι). By solving (4), %πk+1,c -Dnk (W, ∏fc+ι) ≥ Viπk,W* -
Dnk(W*,∏k) with Dnk(W*,∏k) = 0. Thus we have Vink+1,w* ≥ V∏k,W* which completes the
proof.	□
B Experiment details
We describe the baselines used in our experiments as follows:
•	NeurComm (Chu et al., 2020a): it is one of the state-of-the-art methods for NSC. It for-
mulates the neighborhood communication under a spatiotemporal MDP and performs in-
dependent learning for actors and critics. The messages include the state of current step,
policy fingerprints and hidden state of last time step.
•	CommNet (Sukhbaatar & Fergus, 2016): it allows agents to communicate through broad-
casting a communication vector, which is the average of neighbors’ hidden states. The
messages include the current state and last step hidden state.
11
Under review as a conference paper at ICLR 2021
Figure 6: Policy representation that includes messages of neighbors generated by the imagined
module.
•	DIAL (Foerster et al., 2016): each agent encodes the received messages instead of aver-
aging them, but still sums all encoded inputs. It uses the observations of neighbors as the
message.
•	IA2C (Mnih et al., 2016): it is an advantage actor-critic method that trains decentralized
policies and critics for each agent. Each agent does not communicate with nearby agents.
It is implemented similar to MADDPG Lowe et al. (2017) as the critic takes neighboring
actions.
•	ImagComm: our method adopts an imagination module to predict the delayed message and
mitigate nonstationarity coming from partial observations in training. Fig. 6 illustrates the
diagram of policy representation.
IA2C is a non-communicative policy while the rest four approaches are communicative policies
requiring messages from the neighbors.
In terms of baseline, the communication is based on hi,t = gVi (hi,t-1, si,t, mNi,t), and the
algorithm implementation details are listed below: IA2C: hi,t = LSTM(hi,t-1 , relu(si,t)).
NeurComm:	hi,t = LSTM(hi,t-1, concat(relu(sVi,t), relu(πNi,t-1),relu(hNi,t-1))).
DIAL: hi,t = LSTM(hi,t-1, relu(sVt,t) + relu(relu((hi,t-1))) + onehot(ai,t-1).
CommNet:	hi,t = LSTM(hi,t-1, tanh(sVi,t) + linear(mean(hNi,t-1))). In
terms of ImagComm, the communication is based on (5) with hi,t	=
LSTM(hi,t-1 , concat(relu(sVi,t), relu(πNi,t-1), relu(hNi,t-1), relu(bNi,t))). For the imagi-
nation module, Si,t+1 = tanh(concat(relu(sVi,t),hi,t-ι)).
C Additional results
Figure 7 shows the convergence of the learning curve for the imagination module. Different from
the other three tasks, the ATSC Monaco is tasked with heterogeneous agents in which each agent’s
state has different dimensions. Thus in Figure 7b we plot the loss that is the summation of the square
error of N agents without average normalization by state dimensions. In Figure 7a, 7c and 7d, the
y-axis is averaged RMSE loss for N agents.
Figure 8 compares the performance of ImagComm and a modified version of NeurComm-2Hop
on CACC task. ImagComm performs one-step prediction to implicitly include the information of
two-hop neighbors. NeurComm-2Hop directly uses two-hop information. It is interesting to see
that ImagComm outperforms NeurComm-2hop, which proves the effectiveness of the imagination
module. Due to the one-step imagination before communication, the receiver obtains the information
of the neighbors that are two-hop away and the next-step information of neighbors one-hop away.
12
Under review as a conference paper at ICLR 2021
Imagination loss	Imagination loss
Figure 7: Training curve of the imagination module on ATSC (a, b) and CACC (c, d) setting.
(a) ATSC Grid.
(b) ATSC Monaco.
Imagination loss
Training step
(c) CACC Catch-up.
Imagination loss
Training step
(d) CACC Slow-down.
-200
p」eM©」φpo-d∙,ΦCT2Φ><
-1200-
-400
-600
-800
-1000
0.1M 0,2M 0,3M 0,4M 0,5M 0,6M 0,7M 0,8M 0,9M 1,0M
Training step
-5000
-1000-
-1500-
- -2000-
OJ
- -2500-
於-3000-
0)
⅛-3500-
I -4000-
-4500-
0.1M 0,2M 0,3M 0,4M 0,5M 0,6M 0,7M 0,8M 0,9M 1,0M
Training step
(a) Averaged Reward of CACC Catch-up. (b) Averaged Reward of CACC Slow-down.
Figure 8: Comparisons between ImagComm and NeurComm-2Hop. ImagComm performs one-step
prediction to implicitly include the information of two-hop neighbors. NeurComm-2Hop
directly uses two-hop information.
13