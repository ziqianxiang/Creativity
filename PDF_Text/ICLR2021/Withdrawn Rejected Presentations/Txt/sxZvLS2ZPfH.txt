Under review as a conference paper at ICLR 2021
MVP-BERT: Redesigning Vocabularies for chi-
nese BERT and multi-vocab pretraining
Anonymous authors
Paper under double-blind review
Ab stract
Despite the development of pre-trained language models (PLMs) significantly
raise the performances of various Chinese natural language processing (NLP)
tasks, the vocabulary for these Chinese PLMs remain to be the one provided by
Google Chinese Bert Devlin et al. (2018), which is based on Chinese characters.
Second, the masked language model pre-training is based on a single vocabulary,
which limits its downstream task performances. In this work, we first propose a
novel method, segJok, to form the vocabulary of Chinese BERT, with the help of
Chinese word segmentation (CWS) and subword tokenization. Then we propose
three versions of multi-vocabulary pretraining (MVP) to improve the models ex-
pressiveness. Experiments show that: (a) compared with char based vocabulary,
segJok does not only improves the performances of Chinese PLMs on sentence
level tasks, it can also improve efficiency; (b) MVP improves PLMs’ downstream
performance, especially it can improve seg-tok's performances on sequence label-
ing tasks.
1 Introduction
The pretrained language models (PLMs) including BERT Devlin et al. (2018) and its variants Yang
et al. (2019); Liu et al. (2019b) have been proven beneficial for many natural language processing
(NLP) tasks, such as text classification, question answering Rajpurkar et al. (2018) and natural
language inference (NLI) Bowman et al. (2015), on English, Chinese and many other languages.
Despite they brings amazing improvements for Chinese NLP tasks, most of the Chinese PLMs still
use the vocabulary (vocab) provided by Google Chinese Bert Devlin et al. (2018). Google Chinese
Bert is a character (char) based model, since it splits the Chinese characters with blank spaces. In
the pre-BERT era, a part of the literature on Chinese natural language processing (NLP) first do
Chinese word segmentation (CWS) to divide text into sequences of words, and use a word based
vocab in NLP models Xu et al. (2015); Zou et al. (2013). There are a lot of arguments on which
vocab a Chinese NLP model should adopt.
The advantages of char based models are clear. First, Char based vocab is smaller, thus reducing
the model size. Second, it does not rely on CWS, thus avoiding word segmentation error, which
can directly result in performance gain in span based tasks such as named entity recognition (NER).
Third, char-based models are less vulnerable to data sparsity or the presence of out-of-vocab (OOV)
words, and thus less prone to over-fitting (Li et al. (2019)). However, word based model has its
advantages. First, it will result in shorter sequences than char based counterparties, thus are faster.
Second, words are less ambiguous, thus may be helpful for models to learn the semantic meanings
of words. Third, with a word based model, exposure biases may be reduced in text generation
tasks (Zhao et al. (2013)). Another branch of literature try to strike a balance between the two by
combining word based embedding with character based embedding Yin et al. (2016); Dong et al.
(2016).
In this article, we try to strike a balance between the char based model and word based model and
provides alternative approaches for building a vocab for Chinese PLMs. In this article, there are
three approaches to build a vocab for Chinese PLMs: (1) following Devlin et al. (2018), separate the
Chinese characters with white spaces, and then learn a sub-word tokenizer (denote as char); (2) first
segment the sentences with a CWS toolkit like jieba1, and then learn a sub-word tokenizer (denoted
1https://github.com/fxsjy/jieba
1
Under review as a conference paper at ICLR 2021
as segJok); (3) do CWS and kept the high-frequency words as tokens and low-frequency words Win
be tokenized by segJok (denoted as Seg). See Figure 1 for their workflow of processing an input
sentence. Note that the first one is essentially the same with the vocab of Google Chinese BERT.
Inspired by the previous work that incorporate multiple vocabularies (vocabs) or combine multiple
vocabs in an natural wayYin et al. (2016); Dong et al. (2016), we also investigate a series of strate-
gies, which we will call Multi-Vocab Pretraining (MVP) strategies. The first version of MVP is to
incorporate a hierarchical structure to combine the char based vocab and word based vocab. From
the viewpoint of model forward pass, the embeddings of Chinese characters are aggregated to form
the vector representations of multi-gram words or tokens, which then are fed into transformer en-
coders, and then the word based vocab will be used in masked language model (MLM) training. We
will denote this version of MVP as MVPhier . Note that in MVPhier , the char based vocab is built
by splitting the Chinese words in the word based vocab into Chinese chars, and non-Chinese tokens
are kept the same. We will denote this strategy as MVPhier (V ), where V is a word based vocab.
The second version of MVP (denoted as MVPpair) is to employ a pair of vocabs in MLM. Due to
limited resources, in this article We only consider the pair between segJok and Char. MVPpair is
depicted in Figure 2(c). In MVPpair , a sentence (or a concatenation of multiple sentences in pre-
training), is processed and tokenized both in segJok and char, and the two sentences are encoded
by two parameter-sharing transformer encoders. Whole word masking Cui et al. (2019b) is applied
for pretraining. For example, the word ”篮球” (basketball) is masked. The left encoder, which is
with segJok, has to predict the single masked token is ”篮球”，and the right encoder has to predict
”篮” and ”球” for two masked tokens. MLM loss from both sides will be added with weights.
With MVPpair , parameter sharing enables the single vocab model to absorb information from the
other vocab, thus enhancing its expressiveness. Note that after pre-training, one can either keep
one of the encoder or both encoders for downstream finetuning. We will denote this strategy as
MVPpair(V1, V2, i), where V1 and V2 are two different vocabs, i = s means only the encoder with
V1 is kept for finetuning (single vocab model), and i = e means both encoders are kept (ensemble
model).
The third version of MVP (denoted as MVPobj ) is depicted in Figure 2(b). In MVPobj , the sentence
is encoded only once with a fine-grained vocab, and MLM task with that vocab is conducted. As in
the figure, he word ”喜欢” (like) is masked, and under the vocab of char, the PLM has to predict
”喜” and ”欢” for the two masked tokens. As additional training objective, we will employ a more
coarse-grained vocab like segJok and ask the model to use the starting token (“喜")‘s representation
to predict the original word under segJok. We will denote this strategy as MVPpair (V1, V2), where
V1 and V2 are a pair of vocabs and V1 is the more fine-grained one.
Figure 1: An illustration of how to process input sentence into tokens under different methods we
define.
Extensive experiments and ablation studies are conducted. We select BPE implemented by senten-
cepiece2 as the sub-word model, and Albert ? (base model) as our PLM. Pre-training is done on
Chinese Wikipedia corpus3 , which is also the corpus on which we build the different vocabs. After
2https://github.com/google/sentencepiece
3https://dumps.wikimedia.org/zhwiki/latest/
2
Under review as a conference paper at ICLR 2021
main
MLM layer
Deep transformers (Albert)
欢f
喜
□ □ □ □ □□ 0
[CLS]我[MASK] [MASK]篮球[SEP]
[CLS]我 [MASK] 篮球[SEP]
我I [MASK])篮球	]
我喜欢篮球	]
(b) MVPobj
Auxullary 1 喜欢
MLM Iayer J ↑
喜t
Deep transformers (Albert) ∣十] Deep transformers (Albert)
Parameter) Sharing
□ □ □ □ □□ □ i □ □ □ □□ □
Figure 2: The architectures for the three versions of MVP strategies.
pretraining, the three vocab building methods and three MVP strategies are compared on a series
of Chinese benchmark datasets, 4 of which are sentence classification tasks, 3 are sequence label-
ing tasks. The experimental results demonstrate the following take-aways: 1) directly use Chinese
words as VoCab (Seg) does not perform well; 2) segJok based ALBERT consistently performs better
than char and seg on sentence classification tasks, but not sequence labeling tasks; 3) MVP strate-
gies can help to improve a single vocab model on both types of tasks, especially it can help segΛok
on sequence labeling tasks; 4) MVPpair ensemble are the best model, but it comes with a higher
inference time; 5) MVPobj help to provide a better single vocab model than MVPpair .
We now summaries the following contributions in this work.
•	experiments on three ways of building new vocab for Chinese BERT, and segJok, the
combination of CWS and subword tokenization is novel.
•	We propose 3 MVP strategies for enhancing the Chinese PLMs.
2	RELATED WORK
Before and since Devlin et al. (2018), a large amount of literature on pretrained language model
appear and push the NLP community forward with a speed that has never been witnessed be-
fore. Peters et al. (2018) is one of the earliest PLMs that learns contextualized representations of
words. GPTs Radford et al. (2018; 2019) and BERT Devlin et al. (2018) take advantages of Trans-
former Vaswani et al. (2017). GPTs are uni-directional and make prediction on the input text in an
auto-regressive manner, and BERT is bi-directional and make prediction on the whole or part of the
input text. In its core, what makes BERT so powerful are the pretraing tasks, i.e., Mask language
modeling (MLM) and next sentence prediction (NSP), where the former is more important. Since
3
Under review as a conference paper at ICLR 2021
BERT, a series of improvements have been proposed. The first branch of literature improves the
model architecture of BERT. ALBERT Lan et al. (2019) makes BERT more light-weighted by em-
bedding factorization and progressive cross layer parameter sharing. Zaheer et al. (2020) improve
BERT’s performance on longer sequences by employing sparser attention.
The second branch of literature improve the training of BERT. Liu et al. (2019b) stabilize and im-
prove the training of BERT with larger corpus. More work have focused on new language pretrain-
ing tasks. ALBERT Lan et al. (2019) introduce sentence order prediction (SOP). StructBERT Wang
et al. (2019) designs two novel pre-training tasks, word structural task and sentence structural task,
for learning of better representations of tokens and sentences. ERNIE 2.0 Sun et al. (2019) pro-
poses a series of pretraining tasks and applies continual learning to incorporate these tasks. ELEC-
TRA Clark et al. (2020) has a GAN-style pretraining task for efciently utilizing all tokens in pre-
training. Our work is closely related to this branch of literature by design a series of novel pretrain-
ing objective by incorporating multiple vocabularies. Our proposed tasks focus on intra-sentence
contextual learning, and it can be easily incorporated with other sentence structural tasks like SOP.
Another branch of literature look into the role of words in pre-training. Although not mentioned in
Devlin et al. (2018), the authors propose whole word masking in their open-source repository, which
is effective for pretraining BERT. In SpanBERT Joshi et al. (2019), text spans are masked in pre-
training and the learned model can substantially enhance the performances of span selection tasks.
It is indicated that word segmentation is especially important for Chinese PLMs. Cui et al. (2019a)
and Sun et al. (2019) both show that masking tokens in the units of natural Chinese words instead
of single Chinese characters can significantly improve Chinese PLMs. In this work, compared
to literature, we propose to re-design the vocabulary of the Chinese BERT by combining word
segmentation and sub-word tokenizations.
3	Our methods
In this section, we present our methods for rebuilding the vocab for Chinese PLMs, and introduce
our series of MVP strategies.
3.1	Building the vocabs
We investigate four work-flows to process the text inputs, each corresponding to a different vocab
(or a group of Vocabs) (Figure 1). We first introduce the single Vocab models, char, segJok and Seg.
For char based vocab char, Chinese characters in the corpus are treated as words in English and are
separated with blank spaces and a sub-word tokenizer is learned. segJok requires the sentences in
corpus to be segmented and a sub-word tokenizer like BPE are learned on the segmented sentences.
Note that in segJok, some natural word will be splitted into pieces, but there are still many tokens
that haVe multiple Chinese chars. Finally, seg with size N is built with the following procedures:
a) do CWS on the corpus; b) count the words, frequency, and add the tokens from seg-tok with
frequency 0; c) for long tail Chinese words and non-Chinese tokens, tokenize them into subword
tokens with seg-tok, and the words, frequencies are added to the sub-word tokens, frequencies; d)
sort the Vocab Via frequency, and if the most frequent N words or tokens can coVer 99.95% of the
corpus, then take them as vocab. Note that some of the tokens from seg-tok will be dropped.
3.2	Multi-vocab pretraining (MVP)
In this subsection, we will introduce MVP, which is a deriVation of MLM task by DeVlin et al.
(2018). MVP has three Versions, MVPhier, MVPobj and MVPpair, all of which haVe one thing in
common, that is, they require more than one Vocab to implement pre-training.
Figure 2(a) depicts the architecture of MVPhier and Figure 2(a) depicts its procedure for processing
input sentences. Two Vocab, a fine-grained Vocab Vf, and a more coarse-grained Vocab Vc, are
combined in a hierarchical way. Sequences are first tokenized Via Vc, and then the Chinese
tokens (if containing multiple Chinese characters) are splitted into single characters. Chinese
characters and non-Chinese tokens are embedded into Vectors. Then representations of chars inside
a word is aggregated into the representation of this word, which is further fed into the transformer
encoder. During MLM task, whole word masking is applied, that is, we will mask 15% of the tokens
4
Under review as a conference paper at ICLR 2021
in the VC. For example in Figure 2(@),"_喜欢”(瓜&) is masked, thus in the char sequence, two tokens
””and”””aremasked.T henanaggregatorwillcombinetheembeddingsintothevectorsof wordtokens.AttheM LM task, a
Let x and y denote the sequences of tokens with length lx and ly, for the same sentence under Vc
and Vf, in which a part of tokens are masked. Denote xmask as the masked tokens under Vc. The
loss function for MVPhier is
lx
min - log Pθ (xmask |x, y) ≈ min - X Ii log Pθ (ximask |x, y),
θθ
i=1
in which Iix is a variable with binary values indicating whether the i-th token is masked in x.
In MVPobj , a sentence is tokenized and embedded in a fine-grained Vf (e.g., a char based vocab),
and MLM task on Vf is conducted. However, different from the vanilla MLM, another MLM task
based on a more coarse-grained vocab Vc is added. For example, encoded representations of the
chars "一喜" and "欢" inside the word "_喜欢" is aggregated to the vector representation of the
word, and an auxiliary MLM layer is tasked to predict the word based on Vc . For the aggregator
in the example, we adopt BERT-style pooler, which is to use the starting token’s representation to
represent the word’s representation.4 Denote xmask and ymask as the masked tokens under Vf and
Vc respectively. The loss function for MVPobj is as follows:
lx	ly
min - logP©(Xmask, ymask|x) ≈ min - XIx logP©(Xmask|x) - λ * X Iy log P©(ymask|x),
θ	θ	i=1	i=1
in which Iix and Iiy are variables with binary values indicating whether the i-th token is masked in
sequence x and y respectively. Here λ is the coefficient which measures the relative importance of
the auxiliary MLM task.
Now we introduce MVPpair, which is the most resource-demanding version of MVP, but it will be
proven beneficial. Our goal is to enhance the model with a single vocab, by introducing additional
encoder with another vocab and a corresponding MLM task. In this strategy, a sentence is tokenized
and embedded with two vocabs, V1 and V2, which will be fed into separate transformer encoders.
Transformer encoders will share the same parameters, but the embedding layers are separate. For
example in Figure 2(c), "_喜欢” is masked, and thus two tokens on the left sequence and(or) one
token on the right sequence are masked. The left encoder and MLM layer is tasked to recover the two
tokens ”一喜" and "欢”.And on the right, the other MLM layer needs to recover ”一喜欢”.Through
parameter sharing, self-supervised signals from one vocab is transferred to the model with the other
vocab. Formally, Thus the loss function for MVPpair is
lx	ly
min - log Pθ(XmaSk, ymask |x, y) ≈ min -E Ix log P©(Xmask |x) - λ * EIy log Pθ(ymask |y),
θ	θ i=1	i=1
If after MVPpair pretraining we decide to only keep one of the encoder, we will call this model
single vocab MVPpair . Otherwise, we can call the model as ensemble MVPpair. For single mode
MVPpair , finetuning is the same with vanilla ALBERT. Finetuning for ensemble mode MVPpair
is different. For sentence classification, the pooled vectors on both [CLS] tokens will be concate-
nated to be the feature vector of the classifier. When doing sequence labeling tasks, two ways of
ensemble can be conducted, which we will use the example from Figure 2(c) to illustrate. The first
approach is to concatenate the features from fine-grained encoder to the coarse-grained encoder.
That is, representations of "一喜" and "欢" are aggregated to the representation of ”一喜欢”,and it
will be concatenated to the representation from the coarse-grained encoder on the right. We will call
this approach fine-to-coarse ensemble. The other approach is coarse-to-fine ensemble, which is to
concatenate the representation of "_喜欢" from the coarse-grained encoder to "一喜" and "欢" from
the fine-grained encoder. Then the labels of"_喜" and "欢" are predicted.
For notational convenience, we will denote the model pretrained with MVPhier strategy and vocab
V as MVPhier(V ). MVPobj with a fine-grained vocab Vf and a coarse-grained vocab Vc are denoted
4Due to limited resources available, we leave to future work to investigate whether alternative aggregators
can bring improvements.
5
Under review as a conference paper at ICLR 2021
as MVPobj (Vf, Vc). MVPpair with two vocab V1 and V2 is denoted as MVPpair(V1, V2, i,j), where
i can be single, meaning only keep the encoder from V1 for finetuning, or ensemble, meaning
keep both encoders, and j can be ftc (short for fine-to-coarse) or ctf (coarse-to-fine), which is two
approaches for finetuning MVPpair on sequence labeling tasks. Note that if i equals single, we will
neglect the parameter j .
4	Experiments
4.1	Setup
For pre-training corpus, We use Chinese Wikipedia. The Vocab size is 21128 for char and Segdok.
50.38% of seg are Chinese tokens with length more than 1.
In this article, We adopt jieba as our CWS tool.5 To keep 70% of the vocab as natural Words from
CWS, and cover 99.95% of the corpus, seg is built With vocab size 69341.
For MVPhier, we consider MVPhier(Seg±ok). For MVPobj, We consider MVPobj(char, seg-tok)
and MVPobj (seg-tok, seg), with λ equal to 0.1, 0.5, 1.0, 2.0, 10.0. For MVPpair, we consider
MVPpair (char, seg-tok), the combination of seg-tok and char based vocab, with λ equal to 0.1,
0.5, 1.0, 2.0, 10.0. Note that to maintain the consistency for non-Chinese tokens, the char based
VocabinMVPhier (seg-tok), MVPobj (char, seg-tok) and MVPpair (char, seg-tok) is derived from
seg-tok by splitting Chinese characters of seg-tok tokens into characters, not exactly the same with
char.
For pretraining, whole word masking is adopted, and total 15% of the words (from CWS) in the
corpus are masked, which are then tokenized into different tokens under different Vocabs. For
MVPobj(char, seg-tok) and MVPpair (char, seg-tok), 1/3 of the time only tokens seg-tok are pre-
dicted, and 1/3 of the time only tokens from the deriVed char based Vocab are predicted, and for the
rest of the time, tokens from both Vocabs are predicted.
In this article, all models use the ALBERT as encoder. We make use ofa smaller parameter settings,
that is, the number of layer is 3, the embedding size is 128 and the hidden size is 256. Other ALBERT
configurations remain the same with ALBERT Lan et al. (2019). The pretraining hyper-parameters
are almost the same with ALBERT Lan et al. (2019). The maximum sequence length is 512. Here,
the sequence length is counted under the coarse-grained Vocab for MVPhier , fine-grained Vocab for
MVPobj , and the longer one of the two sequences under the two Vocabs for MVPpair . The batch
size is 1024, and all the model are trained for 12.5k steps. The pretraining optimizer is LAMB and
the learning rate is 1e-4. For finetuning, the sequence length is 256, the learning rate is 2e-5, the
optimizer is Adam Kingma & Ba (2015) and the batch size is set as the power of2 and so that each
epoch contains less than 1000 steps. Each model is run on a giVen task for 10 times and the aVerage
performance scores and standard deViations are reported for reproducibility.
4.2	benchmark tasks
For downstream tasks, we select 4 text classification tasks: (1) ChnSentiCorp (chn)6, a hotel reView
dataset; (2) Book review7 (book_review), collected from Douban8 by Liu et al. (2019a). For sentence
pair classification tasks, we include the following 4 datasets: (1) XNLI (xnli) from Conneau et al.
(2018) ;(2) LCQMC (lcqmc) Liu et al. (2018); (3) NLPCC-DBQA9 (nlpcc_dbqa), a question-answer
matching task in the open domain; (4) Law QA Liu et al. (2019a), a QA matching task in the legal
domain. We also investigate three NER tasks. MSRA NER (msra) Levow (2006) is from open
5Despite there are more sophisticated CWS toolkits avalable, and using them may lead to better perfor-
mances, jieba is efficient and good enough to prove the importance of word segmentation in the vocab design
of Chinese BERT.
6https://github.com/pengming617/bert classification
7https://embedding.github.io/evaluation/
8https://book.douban.com/
9http://tcci.ccf.org.cn/conference/2016/dldoc/evagline2.pdf
6
Under review as a conference paper at ICLR 2021
task	chn	br	lcqmc	xnli	nlpcc	msra	fin	ccks
metric	acc	acc	acc	acc	macro F1	exact F1	exact F1	exact F1
char	86.61	77.83	77.85	59.22	62.09	81.14	73.43	85.63
	± 1.12	± 0.55	± 0.78	± 0.76	± 2.57	± 0.61	± 0.61	± 0.28
seg_tok	87.17	79.08	79.79	60.19	64.02	79.81	72.12	84.79
	± 0.46	± 0.20	± 0.42	± 0.43	± 1.15	± 0.81	± 1.26	± 0.74
seg	87.06	78.53	79.27	59.71	63.32	79.07	71.24	83.96
	± 0.78	± 0.43	± 0.68	± 0.59	± 1.83	± 1.17	± 1.64	± 0.93
MVPhier (seg-tok)	87.06	78.67	79.64	59.89	64.06	80.57	72.36	84.88
	± 0.48	± 0.53	± 0.59	± 0.53	± 1.32	± 0. 75	± 1.07	± 0.98
MVPobj (char, seg.tok)	87.26	78.45	78.92	59.98	63.67	81.56	73.95	86.10
	± 0.52	± 0.64	± 0.56	± 0.48	± 1.22	± 0.56	± 0.58	± 0.35
MVPobj (Seg工ok, seg)	87.55	79.67	80.44	60.57	65.48	81.05.	73.47	85.56
	± 0.32	± 0.21	± 0.39	± 0.36	± 1.24	± 0.77	± 1.23	± 0.68
MVPpair(Char, segdok, s)	87.04	78.56	78.63	60.23	63.78	81.47	73.78	85.97
	± 0.66	± 0.68	± 0.63	± 0.47	± 1.28	± 0.43	± 0.78	± 0.45
MVPpair (seg-tok, char, s)	87.22	79.43	80.05	60.25	64.79	80.03	73.17	85.45
	± 0.41	± 0.31	± 0.44	± 0.52	± 1.03	± 0.85	± 1.18	± 0.47
MVPpair(seg-tok, char, e, ftc)	87.96	80.05	80.56	60.86	65.94	81.89	73.97	86.27
	± 0.37	± 0.18	± 0.32	± 0.29	± 0.94	± 0.65	± 0.56	± 0.36
MVPpair(char, Seg工ok, e, ctf)	-	-	-	-	-	82.28 ± 0.54	74.32 ± 0.47	87.85 ± 0.32
Table 1: Main results on the Chinese benchmark datasets. For each task and each model, experi-
ments are repeated for 10 times, and the average and standard deviation of the scores are reported.
domain, Finance NER 10 (fin) is from financial domain, and CCKS NER11 (ccks) is collected from
medical records.
4.3	Experimental results
We first compare the three single Vocab models. We find that word based Vocabs seg-tok and Seg per-
form better than char on all sentence classification tasks, and seg-tok outperforms Seg even though
having less parameters. However, Segdok is worse than or comparable to char on three NER tasks,
where these performance gaps are only partially explained by the CWS errors12. We belieVe that
its disadvantages on sequence labeling tasks is due to sparcity of tokens, i.e., each token has less
training samples, thus are less well trained for token level classification. From the above results,
we show that both CWS and subword tokenization are important to improve the expressiveness of
Chinese PLMS in sentence level tasks via vocab re-designing. In addition, Segdok is more efficient
than SegJtok. We can observe a 1.35x speed UP when changing the vocab from char to Segjtok.
The lower rows of Table 1 also demonstrates the effectiveness of MVP training. Note that in
this table, we report MVPobj with λ = 2.0, and MVPpair with λ = 1.0. First, we can see
MVPs consistently improve the models’ performances, among which the largest improvements
come from MVPobj and ensemble MVPpair. Second, for SegJok, keeping only one encoder
provides improvements on sequence labeling tasks, but the improvements are less than that pro-
vided by MVPpair (seg_tok, Seg) on sentence level tasks. Third, note that for NER tasks, ensem-
ble MVPpair performs better than single MVPpair , and coarse-to-fine ensemble is more effective
than fine-to-coarse ensemble. Fourth, MVPobj (seg-tok, Seg) achieve performances that are close to
MVPpair (seg-tok, char, e) on lcqmc, with less than half of the time in pre-training and fine-tuning.
But the model’s improvements on NER tasks are less significant. Notice that for pretraining and
finetuning, MVPpair takes almost twice the resources and time than MVPobj . Thus, for resources
limited scenarios, MVPobj could be a more suitable choice.
4.4	EFFECTS OF λ
In this section, We investigate how the relative importance coefficient λ for MVPpair(Char, segJok)
affects the ALBERT models’ downstream performances. Here for MVPpair , λ is assigned to
the loss term from coarse-grained vocab. We set λ equal to 0.1, 0.5, 1, 2, 10, and run per-
10https://embedding.github.io/evaluation/extrinsic
11https://biendata.com/competition/CCKS2017 2/
12CWS errors accounts for only less than 0.3% of the errors on CCKS.
7
Under review as a conference paper at ICLR 2021
Figure 3: How the value of coefficient λ affects the models’ downstream performances.
training under each of the setting. Finetuning results on lcqmc and msra are reported in Fig-
Ure 3. For ensemble MVPpair (char, seg-tok, e), λ = 1.0 provides the best performances. And
for MVPpair (char, seg-tok, s), increasing λ worsen the performance on NER, but not on sentence
level tasks, and vice versa for MVPpair(Seg-tok, char, s).
5	Conclusions
In this work, We first propose a novel method, seg-tok, to re-build the vocabulary of Chinese BERT,
with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose
three versions of multi-vocabulary pretraining (MVP) to improve the models’ performance. Exper-
iments show that: (a) compared with char based vocabulary, segJok does not only improves the
performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP
improves PLMs' downstream performance, especially it can improve seg-tok's performances on
sequence labeling tasks.
References
Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large an-
notated corpus for learning natural language inference. arXiv e-prints, art. arXiv:1508.05326,
August 2015.
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA:
Pre-training Text Encoders as Discriminators Rather Than Generators. arXiv e-prints, art.
arXiv:2003.10555, March 2020.
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger
Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In
Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.
2475-2485, Brussels, Belgium, October-November 2018. Association for Computational Lin-
guistics. doi: 10.18653/v1/D18-1269. URL https://www.aclweb.org/anthology/
D18-1269.
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. Pre-
Training with Whole Word Masking for Chinese BERT. arXiv e-prints, art. arXiv:1906.08101,
June 2019a.
8
Under review as a conference paper at ICLR 2021
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. Pre-
Training with Whole Word Masking for Chinese BERT. arXiv e-prints, art. arXiv:1906.08101,
June 2019b.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
C. Dong, Jiajun Zhang, C. Zong, M. Hattori, and Hui Di. Character-based lstm-crf with radical-level
features for chinese named entity recognition. In NLPCC/ICCPOL, 2016.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy.
SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint
arXiv:1907.10529, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICML, 2015.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019.
Gina-Anne Levow. The third international Chinese language processing bakeoff: Word segmenta-
tion and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese
Language Processing, pp. 108-117, Sydney, Australia, July 2006. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/W06-0115.
Xiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han, Arianna Yuan, and Jiwei Li. Is Word
Segmentation Necessary for Deep Learning of Chinese Representations? arXiv e-prints, art.
arXiv:1905.05526, May 2019.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang.
K-BERT: Enabling Language Representation with Knowledge Graph. arXiv e-prints, art.
arXiv:1909.07606, September 2019a.
Xin Liu, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Dongfang Li, and Buzhou Tang.
LCQMC:a large-scale Chinese question matching corpus. In Proceedings of the 27th Interna-
tional Conference on Computational Linguistics, pp. 1952-1962, Santa Fe, New Mexico, USA,
August 2018. Association for Computational Linguistics. URL https://www.aclweb.
org/anthology/C18-1166.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pre-
training Approach. arXiv e-prints, art. arXiv:1907.11692, July 2019b.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proc. of NAACL, 2018.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. 2018.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Don’t Know: Unanswerable Ques-
tions for SQuAD. arXiv e-prints, art. arXiv:1806.03822, June 2018.
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,
Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv
preprint arXiv:1904.09223, 2019.
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE
2.0: A Continual Pre-training Framework for Language Understanding. arXiv e-prints, art.
arXiv:1907.12412, July 2019.
9
Under review as a conference paper at ICLR 2021
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. Struct-
BERT: Incorporating Language Structures into Pre-training for Deep Language Understanding.
arXiv e-prints, art. arXiv:1908.04577, August 2019.
Ruifeng Xu, Tao Chen, Yunqing Xia, Qin Lu, Bin Liu, and Xuan Wang. Word embedding compo-
sition for data imbalances in sentiment and emotion classification. Cognitive Computation, 7, 02
2015. doi: 10.1007/s12559-015-9319-y.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.
XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv e-prints, art.
arXiv:1906.08237, June 2019.
Rongchao Yin, Quan Wang, Peng Li, Rui Li, and Bin Wang. Multi-granularity Chinese word embed-
ding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process-
ing, pp. 981-986, Austin, Texas, November 2016. Association for Computational Linguistics. doi:
10.18653/v1/D16-1100. URL https://www.aclweb.org/anthology/D16- 1100.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big Bird: Transformers
for Longer Sequences. arXiv e-prints, art. arXiv:2007.14062, July 2020.
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-Liang Lu. An empirical study on word seg-
mentation for chinese machine translation. In International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, pp. 248-263. Springer, 2013.
Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. Bilingual word embed-
dings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empir-
ical Methods in Natural Language Processing, pp. 1393-1398, Seattle, Washington, USA, Oc-
tober 2013. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/D13-1141.
10