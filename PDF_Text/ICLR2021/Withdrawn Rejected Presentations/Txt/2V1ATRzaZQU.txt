Under review as a conference paper at ICLR 2021
Characterizing Lookahead Dynamics of
Smooth Games
Anonymous authors
Paper under double-blind review
Ab stract
As multi-agent systems proliferate in machine learning research, games have at-
tracted much attention as a framework to understand optimization of multiple in-
teracting objectives. However, a key challenge in game optimization is that, in
general, there is no guarantee for usual gradient-based methods to converge to a
local solution of the game. The latest work by Chavdarova et al. (2020) report that
Lookahead optimizer (Zhang et al., 2019) significantly improves the performance
of Generative Adversarial Networks (GANs) and reduces the rotational force of
bilinear games. While promising, their observations were purely empirical, and
Lookahead optimization of smooth games still lacks theoretical understanding. In
this paper, we fill this gap by theoretically characterizing Lookahead dynamics of
smooth games. We provide an intuitive geometric explanation on how and when
Lookahead can improve game dynamics in terms of stability and convergence.
Furthermore, we present sufficient conditions under which Lookahead optimiza-
tion of bilinear games provably stabilizes or accelerates convergence to a Nash
equilibrium of the game. Finally, we show that Lookahead optimizer preserves
locally asymptotically stable equilibria of base dynamics, and can either stabilize
or accelerate the local convergence to a given equilibrium with proper assump-
tions. We verify our theoretical predictions by conducting numerical experiments
on two-player zero-sum (non-linear) games.
1	Introduction
Recently, a plethora of learning problems have been formulated as games between multiple inter-
acting agents, including Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Brock
et al., 2019; Karras et al., 2019), adversarial training (Goodfellow et al., 2015; Madry et al., 2018),
self-play (Silver et al., 2018; Bansal et al., 2018), inverse reinforcement learning (RL) (Fu et al.,
2018) and multi-agent RL (Lanctot et al., 2017; Vinyals et al., 2019). However, the optimization
of interdependent objectives is a non-trivial problem, in terms of both computational complexity
(Daskalakis et al., 2006; Chen et al., 2009) and convergence to an equilibrium (Goodfellow, 2017;
Mertikopoulos et al., 2018; Mescheder et al., 2018; Hsieh et al., 2020). In particular, gradient-based
optimization methods often fail to converge and oscillate around a (local) Nash equilibrium of the
game even in a very simple setting (Mescheder et al., 2018; Daskalakis et al., 2018; Mertikopoulos
et al., 2019; Gidel et al., 2019b;a). To tackle such non-convergent game dynamics, a huge effort
has been devoted to developing efficient optimization methods with nice convergence guarantees in
smooth games (Mescheder et al., 2017; 2018; Daskalakis et al., 2018; Balduzzi et al., 2018; Gidel
et al., 2019b;a; Schafer & Anandkumar, 2019; Yazici et al., 2019; Loizou et al., 2020).
Meanwhile, Chavdarova et al. (2020) have recently reported that the Lookahead optimizer (Zhang
et al., 2019) significantly improves the empirical performance of GANs and reduces the rotational
force of a bilinear game dynamics. Specifically, they demonstrate that class-unconditional GANs
trained by a Lookahead optimizer can outperform class-conditional BigGAN (Brock et al., 2019)
trained by Adam (Kingma & Ba, 2015) even with a model of 1/30 parameters and negligible com-
putation overheads. They also show that Lookahead optimization ofa stochastic bilinear game tends
to be more robust against large gradient variances than other popular first-order methods, and con-
verges to a Nash equilibrium of the game where other methods fail.
1
Under review as a conference paper at ICLR 2021
Despite its great promise, the study of Chavdarova et al. (2020) relied on purely empirical obser-
vations, and the dynamics of Lookahead game optimization still lacks theoretical understanding.
Specifically, many open questions, such as the convergence properties of Lookahead dynamics and
the impact of its hyperparameters on the convergence, remain unexplained. In this work, we fill this
gap by theoretically characterizing the Lookahead dynamics of smooth games. Our contributions
are summarized as follows:
•	We provide an intuitive geometric explanation on how and when Lookahead can improve
the game dynamics in terms of stability and convergence to an equilibrium.
•	We analyze the convergence of Lookahead dynamics in bilinear games and present suffi-
cient conditions under which the base dynamics can be either stabilized or accelerated.
•	We characterize the limit points of Lookahead dynamics in terms of their stability and local
convergence rates. Specifically, we show that Lookahead (i) preserves locally asymptoti-
cally stable equilibria of base dynamics and (ii) can either stabilize or accelerate the local
convergence to a given equilibrium by carefully choosing its hyperparameters.
•	Each of our theoretical predictions is verified with numerical experiments on two-player
zero-sum (non-linear) smooth games.
2	Preliminaries
We briefly review the objective of smooth game optimization, first-order game dynamics, and
Lookahead optimizer. Finally, we discuss previous work on game optimization. We summarize
the notations throughout this paper in Table A.1.
2.1	Smooth Games
Following Balduzzi et al. (2018), a smooth game between players i = 1, . . . , n can be defined as a
set of smooth scalar functions {fi}in=1 with fi : Rd → R such that d = Pin=1 di. Each fi represents
the cost of player i’s strategy xi ∈ Rdi with respect to other players’ strategies x-i. The goal of
this game optimization is finding a (local) Nash equilibrium of the game (Nash, 1951), which is a
strategy profile where no player has an unilateral incentive to change its own strategy.
Definition 1 (Nash equilibrium). Let {fi}in=1 be a smooth game with strategy spaces {Rdi}in=1 such
that d = En=I di. Then X* ∈ Rd is a local Nash equilibrium of the game if, for each i = 1,...,n,
there is a neighborhood Ui of x * such that fi(x i, x-i) ≥ fi(x *) holds for any x i ∈ Ui. Such x * is
said to be a global Nash equilibrium of the game when Ui = Rdi for each i = 1, . . . , n.
A straightforward computational approach to find a (local) Nash equilibrium of a smooth game is to
carefully design a gradient-based strategy update rule for each player. Such update rules that define
iterative plays between players are referred to as a dynamics of the game.
Definition 2 (Dynamics ofa game). A dynamics of a smooth game {fi}in=1 indicates a differentiable
operator F : Rd → Rd that describes players’ iterative strategy updates as x(t+1) = F (x(t) ).
One might expect that a simple myopic game dynamics, such as gradient descent, would suffice
to find a (local) Nash equilibrium of a game as in traditional minimization problems. However, in
general, gradient descent optimization of smooth games often fail to converge and oscillate around
an equilibrium of the game (Daskalakis et al., 2018; Gidel et al., 2019b;a; Letcher et al., 2019). Such
non-convergent behavior of game dynamics is mainly due to (non-cooperative) interaction between
multiple cost functions, and is considered as a key challenge in the game optimization (Mescheder
et al., 2017; 2018; Mazumdar et al., 2019; Hsieh et al., 2020).
2.2	First-Order Methods for Smooth Game Optimization
We introduce well-known first-order methods for smooth game optimization. To ease the notation,
We use Vχf(∙) to denote the concatenated partial derivatives (Vχι fι (.),..., Vχnfn(∙)) of a smooth
game {fi}n=ι, where VXifi(∙) is a partial derivative of a player i's cost function with respective to
its oWn strategy.
2
Under review as a conference paper at ICLR 2021
Gradient Descent (GD) minimizes the cost function of each player using the gradient descent. Its
simultaneous dynamics F GDSim of a smooth game {fi}in=1 with a learning rate η > 0 is given by
X(t+1) = FGDSim(X(t)) =f x⑴-ηVχf(x㈤).	(1)
On the other hand, its alternating dynamics FGDAlt is described by
x(t+1) = FGDAlt (x(t)) d=ef F1 ◦ . . . ◦ Fn(x(t)), where	(2)
Fi(x) d=ef (. . . ,xi-1,xi - ηVxifi(x),xi+1, . . .).	(3)
Proximal Point (PP) (Martinet, 1970) computes an update by solving a proximal problem at each
iteration. Its simultaneous dynamics F PPSim of a smooth game {fi }in=1 with a learning rate η > 0 is
x(t+1) = FPPSim (x(t)) d=ef x(t) - ηVxf(x(t+1)).	(4)
Note that this update rule is implicit in a sense that x(t+1) appears on both sides of the equation;
hence it requires solving the proximal subproblem for x(t+1) per iteration.
Extra Gradient (EG) (Korpelevich, 1976) computes an update by using an extrapolated gradient.
Its simultaneous dynamics F EGSim of a smooth game {fi}in=1 with a learning rate η > 0 is
x(t+1) = FEGSim(X(t)) = x(t) — ηVχf(x(t+ 2)), where	(5)
x(t+1) =f Xe)- ηVxf(x(t)).	(6)
2.3	Lookahead Optimizer
Lookahead (Zhang et al., 2019) is a recently proposed optimizer that wraps around a base optimizer
and takes a backward synchronization step for each k forward steps. Given a dynamics FA induced
by a base optimization method A, the Lookahead dynamics GLA-A with a synchronization period
k ∈ N and a rate α ∈ (0, 1) is
x(t+1) = GLA-A(x(t)) d=ef (1 - α)x(t) + αFAk (x(t)).	(7)
2.4	Related Work
The convergence analysis of first-order smooth game dynamics dates several decades back and have
been established in the context of saddle-point problems (Rockafellar, 1976; Korpelevich, 1976;
Tseng, 1995), which is a special case of zero-sum games. For example, Rockafellar (1976) showed
the linear convergence of PP in the bilinear and strongly-convex-strongly-concave (SCSC) saddle-
point problems. Tseng (1995) and Facchinei & Pang (2003) proved the linear convergence of EG in
the same problem, and Nemirovski (2004) did in the convex-concave problem over compact sets.
As many learning problems are formulated as games in recent years (Goodfellow et al., 2014; Madry
et al., 2018; Silver et al., 2018; Fu et al., 2018; Vinyals et al., 2019), game optimization has regained
considerable attentions from the research community. Optimistic gradient descent (OGD) (Popov,
1980), which can be seen as an efficient approximation of EG, was recently rediscovered in the
context of GAN training (Daskalakis et al., 2018). Recent work of Liang & Stokes (2019) and Gidel
et al. (2019a) proved linear convergence of OGD in bilinear and SCSC games. Mokhtari et al. (2020)
established an unifying theoretical framework for analyzing PP, EG and OGD dynamics. Zhang &
Yu (2020) presented exact and optimal conditions for PP, EG and OGD dynamics to converge in
bilinear games. While there has been a growing interest for incorporating second-order information
into game dynamics (Mescheder et al., 2017; Balduzzi et al., 2018; Mazumdar et al., 2019; Schafer
& Anandkumar, 2019; Loizou et al., 2020) to remedy non-convergent behaviors, the first-order
optimization still dominates in practice (Brock et al., 2019; Donahue & Simonyan, 2019) due to
computational and memory cost of second-order methods.
Lately, Chavdarova et al. (2020) reported that recently developed Lookahead optimizer (Zhang et al.,
2019) significantly improves the empirical performance of GANs and reduces the rotational force of
bilinear game dynamics. However, this study relied on purely empirical observation and lacked the-
oretical understanding for Lookahead optimization of smooth games. Although Wang et al. (2020)
proved that Lookahead optimizer globally converges to a stationary point in minimization problems,
its convergence in smooth games still remain as an open question.
3
Under review as a conference paper at ICLR 2021
(a) Eigenvalues of GDAlt
(b) Eigenvalues of GDSim
(c) Eigenvalues of PP/EGSi
Figure 1: The spectral contraction effect of Lookahead dynamics in Equation 8. λ± are the eigen-
values of each base dynamics, and 1 - α + αλk± are the eigenvalues of the associated Lookahead
dynamics. k forward steps of a Lookahead procedure first rotate the eigenvalues λ± of the dynam-
ics’ Jacobian matrix. Then, a synchronization backward step pulls them into a circle with a radius
smaller than their maximal modulus. This results in a reduced spectral radius of the Jacobian matrix,
which improves stability and convergence to an equilibrium.
3	Spectral Contraction Effect of Lookahead in Bilinear Games
In this section, we show that Lookahead can either stabilize or accelerate the convergence of its
base dynamics by reducing the spectral radius of its underlying Jacobian matrix. We highlight such
spectral contraction effect by analyzing the convergence of Lookahead dynamics in a simple bilinear
game (Section 3.1), and extend the results to general bilinear games (Section 3.2).
3.1	Lookahead Dynamics of a Simple Bilinear Game
We begin with a simple exemplar bilinear game that has a unique Nash equilibrium (0, 0):
min max x1 ∙ x2.	(8)
x1∈Rx2∈R
This game has been extensively studied as a representative toy example of game optimization by
Gidel et al. (2019a) due to its oscillating dynamics. The following proposition demonstrates stabi-
lization effect of Lookahead on Equation 8.
Proposition 1. Simultaneous GD dynamics F GDSim with a learning rate η > 0 diverges from the
Nash equilibrium of Equation 8. However, its Lookahead dynamics GLA-GDSim with a synchronization
period k ∈ N and a rate α ∈ (0, 1) globally converges to the Nash equilibrium if <((1 + iη)k) < 1
and α is small enough.
Proposition 1 shows that Lookahead optimizer can stabilize divergent dynamics of Equation 8. How-
ever, such stabilization effect of Lookahead raises a natural question: would there be any advantage
for using Lookahead when its base dynamics is already stable? Proposition 2 analyzes well-known
convergent PP dynamics of Equation 8 and presents an affirmative answer. Specifically, it shows
that Lookahead dynamics (i) preserves the convergence of its base dynamics, and (ii) can further
accelerate the convergence with proper hyperparameter choices.
Proposition 2. Simultaneous PP Lookahead dynamics GLA-PPSim with a learning rate η ∈ (0, 1), a
synchronization period k ∈ N and a rate α ∈ (0, 1) globally converges to the Nash equilibrium
of Equation 8. Furthermore, the rate of convergence is improved upon its base dynamics F PPSim if
<((1 + iη)k) < (1 + η2)k and α is large enough.
We provide geometric interpretation of the Lookahead procedure in Figure 1. Intuitively, Lookahead
optimizer either stabilizes or accelerates its base dynamics by pulling the eigenvalues of the dynam-
ics’ Jacobian matrix into a circle with a small radius. Specifically, k forward steps of a Lookahead
procedure first rotate the eigenvalues, and a synchronization backward step pulls them into a circle
with a radius smaller than their maximal modulus. This results in a reduction of the spectral radius
of the dynamics’ Jacobian matrix, which is known to be crucial for stability (Slotine & Li, 1991).
Such spectral contraction effect of Lookahead dynamics is captured by the following lemma.
4
Under review as a conference paper at ICLR 2021
Lemma 3 (Spectral contraction effect of Lookahead). Let k ∈ N, α ∈ (0, 1) and define a function
f : Rm×m → Rm×m by f(X) = (1 - α)I + αXk. Define θ(λ) d=ef Arg(λk - 1) and φ(λ) d=ef
arcsin SiP(X)M. Then, the following statements hold:
•	For ρ(X) = 1, ρ(f(X))
•	For ρ(X) > 1, ρ(f(X))
•	For ρ(X) < 1, ρ(f(X))
<
<
1 ifλk 6= 1,∀λ∈ λmax(X).
1 if <(λk) < 1, α < 2cosλ∏-θ(I)), ∀λ ∈ λ≥ι(X).
<
ρ(X)k if <(λk) < ρ(X)2k, α > 1 - 2ρ(X:cOS(π-φ(λi”
lλi-1|
∀λ ∈ λmax(X), ∀λi ∈ λ(X).
In short, Lemma 3 suggests that Lookahead can reduce the spectral radius of a matrix by choosing a
proper α and a k such that the entire radius-supporting eigenvalues (e.g., λ≥1(X), λmax(X)) are ro-
tated to left enough. However, such k may not exist, for example, especially when such eigenvalues
are not tightly clustered together. To help understanding when Lookahead can actually reduce the
spectral radius, we present Lemma 4 as a sufficient condition for a set of eigenvalues to admit the
existence of k that rotates them to the left half-plane.
Lemma 4 (Left-rotatable eigenvalues). LetX,J ∈ Rm×m be such that X = I - ηJfor some η > 0
and let S ⊆ λ(X). Assume that each element of S has its conjugate pair in S. Then we have
<(λk ) < 0 for each λ ∈ S if k ∈
π	3π
2θmin(S) , 2θmax(S)
and every element of S has non-zero
imaginary part. Existence Ofsuch k ∈ N is guaranteedfor a small enough η when =max(S) < 3.
=min (S)
Note that the Jacobian matrix of most well-known gradient-based dynamics can be written in the
form of I - ηJ, where η > 0 is a learning rate and J is the underlying Jacobian matrix of the game.
Intuitively, Lemma 4 suggests that for a small enough learning rate, any subset of the eigenvalues
of a dynamics with imaginary conditioning less than 3 admits the existence of k that rotates them
left enough. For such k, Lookahead can reduce the spectral radius of the dynamics by choosing a
proper α, as stated in Lemma 3. This joint usage of Lemma 3-4 plays a central role for the proofs of
our main results in Section 3.2 and Section 4. To summarize, Lemma 3-4 together highlight when
Lookahead can actually improve the game dynamics and show that the imaginary conditioning of
the radius-supporting eigenvalues is crucial for determining whether the dynamics is improvable.
3.2	Lookahead Dynamics of General Bilinear Games
In this section, we extend the analysis of Lookahead dynamics to a general bilinear game
min max x1T Ax2 - b1T x1 - b2T x2	(9)
x1 ∈Rm x2 ∈Rn	1	1	2
for some A ∈ Rm×n	and bi ∈ Rm, b2 ∈	Rn	such that	there	exists x； ∈ Rm,x2	∈ Rn	with
ATx； = b2 and Ax； = bi. The existence of x；, x allows US to rewrite the game as
min max (x1 - x1； )TU
x1 ∈Rm x2 ∈Rn	i
Σr
0
(10)
where U, Σr, V is the SVD of A with r d=ef rank(A). Therefore, we can analyze the dynamics of
Equation 9 by inspecting a rather simpler problem
min max xiTΣrx2,	(11)
x1 ∈Rr x2 ∈Rr i
as they are equivalent up to some rotations and translations. This reduction is a well-known tech-
nique and has been used by Gidel et al. (2019b;a) and Zhang & Yu (2020) for simplifying the
analysis of Equation 9.
Now we present sufficient conditions for Lookahead hyperparameters under which convergence
of each first-order base dynamics, namely GDAlt, GDSim, PPSim and EGSim , is either stabilized or
accelerated. The following first two theorems show that Lookahead can provably stabilize non-
convergent GD dynamics of general bilinear games.
5
Under review as a conference paper at ICLR 2021
Theorem 5 (Convergence of GLA-GDAlt ). Lookahead dynamics GLA-GDAlt with a learning rate η ∈
(0, σ2aχ), a synchronization period k ∈ N and a rate α ∈ (0,1) ConvergeS to a Nash equilibrium
22
of Equation 9 if k arccos(1 — n2i-) mod 2π = 0 for any σi ∈ σ(A).
Theorem 6 (Convergence of GLA-GDSim). Lookahead dynamics GLA-GDSim with a learning rate η > 0,
a synchronization period k ∈ N anda rate α ∈ (0, 1) converges to a Nash equilibrium of Equation 9
if k ∈ Qarcta； nσmin，2-：；〃。max ) and . is Small enough
Roughly, Theorem 5 suggests that almost any configurations of Lookahead can make GDAlt conver-
gent to a Nash equilibrium of the bilinear games. On the other hand, the existence of k that satisfies
the condition of Theorem 6 is guaranteed for a small enough η if σmax < 3 holds. This highlights a
limitation of the convergence guarantee for GDSim that it holds only for well-conditioned games.
The next two theorems show that Lookahead preserves the convergence of PPSim and EGSim in the
bilinear games, and can further accelerate their convergence under proper hyperparameter choices.
Theorem 7 (Acceleration of GLA-PPSim). Lookahead dynamicS GLA-PPSim with a learning rate η >
0, a Synchronization period k ∈ N and a rate α ∈ (0, 1) convergeS to a NaSh equilibrium of
Equation 9. Furthermore, the rate of convergence iS accelerated upon itS baSe dynamicS F PPSim if
k ∈ Qarc晨 n。min，2arct31nσmin ) Cmd α iS Iarge enough.
Theorem 8 (Acceleration of GLA-EGSim). Lookahead dynamicS GLA-EGSim with a learning rate η ∈
(0，σmaχ), a synchronization period k ∈ N and a rate α ∈ (0,1) converges to a Nash equilibrium
of Equation 9. Furthermore, the rate of convergence iS accelerated upon itS baSe dynamicS F EGSim if
η ∈ (0，2⅛), k ∈ Qrctanζη⅛，2arcta13>m^ ) Cmd α is Iarge enough.
1-ησmin	1-ησmin
Note that the existence of k that satisfies the acceleration conditions of Theorem 7-8 is always
guaranteed for a small enough η. This contrasts Theorem 7-8 with Theorem 6, which only applies
to well-conditioned games, and suggests that they can be applied for a wide range of bilinear games,
including the ill-conditioned ones.
4	The Limit Points of Lookahead Dynamics
In this section, we characterize the limit points of Lookahead dynamics and reveal the connections
between their stability and the hyperparameters of Lookahead. We start by defining a few stability
concepts which are standard in the dynamical system theory (Slotine & Li, 1991).
Definition 3 (Lyapunov stability). Let F be a smooth vector field on Rn. Then x ∈ Rn is Lyapunov
stable in F iffor any > 0, there exists δ > 0 such that for any y ∈ Rn, kx — yk < δ implies
kFt (x) — Ft (y)k < for all t ∈ N.
Definition 4 (Asymptotic stability). A Lyapunov stable equilibrium x* ∈ Rn of a smooth vector
field F is said to be asymptotically StabIe if there exists δ > 0 such that k x — x *k < δ implies
lim kFt(x) — x* k = 0. Such x* is said to be locally asymptotically stable ifδ < ∞.
t→∞
We show that any Lyapunov stable equilibrium (SE) ofa dynamics is a locally asymptotically stable
equilibrium (LASE) of a Lookahead dynamics. Furthermore, we show that Lookahead can either
stabilize or accelerate the local convergence to an equilibrium when the radius-supporting eigenval-
ues of the equilibrium satisfy certain assumptions on their imaginary parts.
Theorem 9 (SEA ⊆ LASELA-A). Let x* ∈ Rn be a Lyapunov stable equilibrium of a dynamics F.
Then, x* is a LASE of its Lookahead dynamics G with a synchronization period k ∈ N and a rate
α ∈ (0,1) if λk = 1 for each λi ∈ λ(VXF(X*)).
Theorem 10 (One-point local stabilization). Let x* ∈ Rn be an equilibrium ofa dynamics F with
ρ(VxF(x*)) > 1. Assume that every elementofλ≥1(VxF(x*)) has non-zero imaginary part. Then,
x* isa LASE of its Lookahead dynamics G with a synchronization period k ∈ N anda rate α ∈ (0, 1)
if k ∈ Qθmin (λ≥ 飞 X F (X *))) , 2θ max (λ≥∕ X F (x*))) ) and α is Small enough
6
Under review as a conference paper at ICLR 2021
Theorem 11 (One-point local acceleration). Let x* ∈ Rn be an equilibrium of a dynamics F with
ρ(V X F (x *)) < 1. Assume that every element of λ max (V X F (x *)) has non-zero imaginary part. Then,
the local convergence rate to x* in a Lookahead dynamics G with a synchronization period k ∈ N
and a rate α ∈ (0，1) is accelerated uPon F if k ∈ Qθmin(λιna∏NXF(X*)))，2θmax(λmf∏VXF(X*))) ) and
α is large enough.
Intuitively, Theorem 9 shows that Lookahead preserves stability of its base dynamics, and Theorem
10-11 suggest that Lookahead can either stabilize or accelerate the local convergence to an equi-
librium. Note that the stabilization and acceleration can be guaranteed when λ≥1 (VxF(x*)) and
λmax(VxF(x*)) contain no real eigenvalues and have imaginary conditioning less than 3; otherwise,
k that satisfies the conditions of Theorem 10-11 may not exist (see Appendix E.10-E.11).
An additional, but important consequence of Theorem 10 is that the inclusion relationship implied
by Theorem 9 is strict in general. In the context of Nash equilibrium (NE) computation, such
stabilization effect of Lookahead can be helpful when unstable NE are stabilized (e.g., bilinear
games). However, the stabilization effect also carries a possibility for introducing non-Nash LASE,
which is bad for the NE computation (Mazumdar et al., 2019). Hence, the overall impact of Theorem
10 on the computation ofNE depends on the global structure of the game and base dynamics.
Note that Theorem 10-11 require radius-supporting eigenvalues to have non-zero imaginary parts
and therefore does not apply to fully-cooperative (FC) games (i.e., minimization problems), which
exhibit real eigenvalues only. To give an understanding of Lookahead dynamics in FC games, we
present Proposition 12-13, together which imply that the iterates of Lookahead dynamics almost
surely avoids unstable equilibria of its base dynamics in FC games (e.g., avoids local maxima).
Proposition 12 (Avoids unstable points). Let F be a L-Lipschitz smooth dynamics for some L >
0 and let G be its Lookahead dynamics with a synchronization period k ∈ N and a rate α ∈
(0，ι+Lk). Then the random-initialized iterates of G almost surely avoids its equilibrium X* with
ρ(VXG(X*)) > 1 ifρ(VXG(X0)) 6= 1 holds for any equilibrium X0 ofG.
Proposition 13 (Preserves unstable points in FC games). Let X* ∈ Rn be an equilibrium of a
dynamics F with ρ(VXF(X*)) > 1, and assume that VXF(X*) is a symmetric matrix with positive
eigenvalues. Then, ρ(VXG(X*)) > 1 holds for a Lookahead dynamics G with a synchronization
period k ∈ N and a rate α ∈ (0, 1).
5	Experiments
—LA-GD÷t
-- LA-G。放
■ , , GDah
LA-GD÷m
LA-G∕⅛
GDSim
—LA-EG 如
—, LA-EGAim
EGsim
O 200	400	600	800 1000 1200 1400
Iterations
(a) Convergence to the NE of the bilinear game.
Figure 2: Optimization progress of multiple first-order methods with hyperparameters chosen by
(+) and against our theorems (-) in the bilinear and nonlinear games.
0 12 3
O - - -
1101010
(Oo) O-M① UUBSQ
(b) Convergence to (0, 0) in the nonlinear game.
---LA-Go卷
—, LA-G。族
,,,, GDAit
LA-GDam
LA-GDdm
GDSim
---LA-EG急
一" LA-EG 前
■ , ,, EGSim
Bilinear game. We test our theoretical predictions in Section 3.2 (Theorem 5-8) on a bilinear game
min max x1T Ax2
x1 ∈Rn x2 ∈Rn
(12)
with A =e In + e ∙ En, where each element of En ∈ MnXn is sampled from N(0,1). We report our
results using n = 10 and = 0.05, which gives a sample ofA with σmax = 1.195 and σmin = 0.852,
hence σmax = 1.401 < 3. For a fixed η = 0.1, we use Theorem 5-8 to derive a range of k
7
Under review as a conference paper at ICLR 2021
(a) Trajectories of each Lookahead dynamics with k
and α predicted by Theorem 10 and 11.
Figure 3: Visualized trajectories of each dynamics an equilibrium (0, 0) of the nonlinear game.
(b) Trajectories of each Lookahead dynamics with k
and α chosen against Theorem 10 and 11.
and an approximate scale of α that guarantee stabilization and acceleration of convergence to a
Nash equilibrium (NE) of Equation 12. We provide the derivations of theoretically recommended
values and actual configurations used for the experiment in Appendix D. Figure 2 (a) shows that the
hyperparameters predicted by our theorems, denoted by LA-GDAlt/Sim+ and LA-EGSim+, actually
stabilize and accelerates the convergence to a NE. We also test the hyperparameters that are chosen
against our theorems and denote as LA-GDAlt/Sim- and LA-EGSim-. Specifically, we choose a k
smaller than the lower bound predicted by our theorems and use large α for unstable base dynamics
and small α for stable base dynamics. The result in Figure 2 (a) suggests that Lookahead can fail to
stabilize, or even worse, slow down the convergence when hyperparameters are configured badly.
Nonlinear game. We verify our theoretical predictions in Section 4 (Theorem 10 and 11) on the
non-linear game proposed by Hsieh et al. (2020):
min max x1 ∙ x2 + eφ(x2),	(13)
x1∈Rx2∈R
where φ(x) = ∣x2 - 4x4 With e > 0. This game has an unstable critical point (0, 0) surrounded
by an attractive internally chain-transitive (ICT) set, which may contain arbitrarily long trajectories.
Hsieh et al. (2020) demonstrate that most first-order methods fail to converge in this game due to the
instability of the equilibrium and the existence of the ICT set. For a fixed = 0.01 and η = 0.05, we
use Theorem 10 and 11 to derive a range of k and an approximate scale of α that guarantee local sta-
bilization and acceleration to the equilibrium of Equation 13. We provide the detailed derivations of
the theoretically recommended values and the configurations in Appendix D. Figure 2 (b) and Fig-
ure 3 (a) shows that the hyperparameters predicted by our theorems, denoted by LA-GDAlt/Sim+ and
LA-EGSim+ , actually stabilize and accelerates the convergence to the equilibrium. In contrast, hy-
perparameters chosen against our theorems, denoted by LA-GDAlt/Sim - and LA-EGSim - in Figure 2
(b) and Figure 3 (b), neither success to stabilize nor accelerate the convergence to the equilibrium.
6	Conclusion
In this work, we derived the theoretic results for convergence guarantee and acceleration of Looka-
head dynamics in smooth games for the first time. Specifically, we derived sufficient conditions for
hyperparameters of Lookahead optimizer under which the convergence of bilinear games is either
stabilized or accelerated. Furthermore, we proved that the Lookahead optimizer preserves locally
asymptotically stable equilibria of smooth games. Finally, we showed that Lookahead can either
stabilize or accelerate the local convergence to a given equilibrium under proper assumptions.
Our results point to several future research directions. Lemma 4 suggests that the imaginary con-
ditioning of the radius-supporting eigenvalues is crucial for the performance gain in Lookahead.
Therefore, developing an optimizer that exhibits a small imaginary conditioning could improve the
convergence of its Lookahead dynamics. Another interesting application of our theoretic results
would be designing an adaptive mechanism for the Lookahead hyperparameters by applying our
theorems on local bilinear approximation (Schafer & Anandkumar, 2019) of the game for each step.
8
Under review as a conference paper at ICLR 2021
References
Waiss Azizian, Damien Scieur, Ioannis Mitliagkas, S. Lacoste-Julien, and Gauthier Gidel. Acceler-
ating smooth games by manipulating spectral shapes. In AISTATS, 2020.
D. Balduzzi, Sebastien Racaniere, J. Martens, Jakob N. Foerster, K. Tuyls, and T. GraePeL The
Mechanics of n-Player Differentiable Games. In ICML, 2018.
TraPit Bansal, Jakub W. Pachocki, S. Sidor, Ilya Sutskever, and Igor Mordatch. Emergent ComPlex-
ity via Multi-Agent ComPetition. In ICLR, 2018.
D.P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
A. Brock, J. Donahue, and K. Simonyan. Large Scale GAN Training for High Fidelity Natural
Image Synthesis. In ICLR, 2019.
Tatjana Chavdarova, Matteo Pagliardini, M. Jaggi, and Franois Fleuret. Taming GANs with Looka-
head. ArXiv, abs/2006.14567, 2020.
C. Chen. Linear System Theory and Design. Oxford University Press, 1995.
X. Chen, X. Deng, and S. Teng. Settling the ComPlexity of ComPuting Two-Player Nash Equilibria.
J. ACM, 56:14:1-14:57, 2009.
C. Daskalakis and Ioannis Panageas. The Limit Points of (OPtimistic) Gradient Descent in Min-Max
OPtimization. In NeurIPS, 2018.
C. Daskalakis, P. Goldberg, and C. PaPadimitriou. The ComPlexity of ComPuting a Nash Equilib-
rium. Electron. Colloquium Comput. Complex., 2006.
C. Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with OPtimism.
In ICLR, 2018.
J. Donahue and K. Simonyan. Large Scale Adversarial RePresentation Learning. In NeurIPS, 2019.
F. Facchinei and J. Pang. Finite-Dimensional Variational Inequalities and Complementarity Prob-
lems. SPringer, 2003.
Justin Fu, Katie Luo, and S. Levine. Learning Robust Rewards with Adversarial Inverse Reinforce-
ment Learning. In ICLR, 2018.
Gauthier Gidel, Hugo Berard, Pascal Vincent, and S. Lacoste-Julien. A Variational Inequality Per-
sPective on Generative Adversarial Nets. In ICLR, 2019a.
Gauthier Gidel, Reyhane Askari Hemmat, M. Pezeshki, Gabriel Huang, Remi Le Priol, S. Lacoste-
Julien, and Ioannis Mitliagkas. Negative Momentum for ImProved Game Dynamics. In AISTAT,
2019b.
Ian J. Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. ArXiv, abs/1701.00160,
2017.
Ian J. Goodfellow, Jean Pouget-Abadie, M. Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative Adversarial Nets. In NIPS, 2014.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. ExPlaining and Harnessing Adversarial
ExamPles. CoRR, abs/1412.6572, 2015.
Ishaan Gulrajani, F. Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved
training of wasserstein gans. ArXiv, abs/1704.00028, 2017.
Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The Limits of Min-Max Optimization
Algorithms: Convergence to Spurious Non-critical Sets. ArXiv, abs/2006.09065, 2020.
Tero Karras, S. Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adver-
sarial Networks. In CVPR, 2019.
9
Under review as a conference paper at ICLR 2021
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.
G. M. Korpelevich. The Extragradient Method for Finding Saddle Points and Other Problems.
Ekonomika i Matematicheskie Metody,12:747-756, 1976.
Marc Lanctot, V. Zambaldi, A. Gruslys, A. Lazaridou, K. Tuyls, Julien Perolat, D. Silver, and
T. Graepel. A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning. In
NeurIPS, 2017.
J. Lee, Ioannis Panageas, G. Piliouras, Max Simchowitz, Michael I. Jordan, and B. Recht. First-order
methods almost always avoid saddle points. In NeurIPS, 2019.
Alistair Letcher, D. Balduzzi, Sebastien Racaniere, J. Martens, Jakob N. Foerster, K. Tuyls, and
T. Graepel. Differentiable Game Mechanics. JMLR, 20:1-40, 2019.
Tengyuan Liang and J. Stokes. Interaction Matters: A Note on Non-asymptotic Local Convergence
of Generative Adversarial Networks. In AISTATS, 2019.
N. Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, S. Lacoste-Julien, and Ioannis
Mitliagkas. Stochastic Hamiltonian Gradient Methods for Smooth Games. In ICML, 2020.
A.	Madry, Aleksandar Makelov, L. Schmidt, D. Tsipras, and Adrian Vladu. Towards Deep Learning
Models Resistant to Adversarial Attacks. In ICLR, 2018.
B.	Martinet. Breve Communication. RegUIariSation d'inequations Variationnelles par Approxima-
tions Successives. ESAIM: Mathematical Modelling and Numerical Analysis - Modelisation
Mathematique etAnalyse Numerique, 4(R3):154-158, 1970.
Eric V. Mazumdar, Michael I. Jordan, and S. Sastry. On Finding Local Nash Equilibria (and Only
Local Nash Equilibria) in Zero-Sum Games. ArXiv, abs/1901.00838, 2019.
P. Mertikopoulos, C. Papadimitriou, and G. Piliouras. Cycles in Adversarial Regularized Learning.
In SODA, 2018.
P. Mertikopoulos, Houssam Zenati, Bruno Lecouat, Chuan-Sheng Foo, V. Chandrasekhar, and
G. Piliouras. Mirror descent in saddle-point problems: Going the extra (gradient) mile. ArXiv,
abs/1807.02629, 2019.
Lars M. Mescheder, Sebastian Nowozin, and Andreas Geiger. The Numerics of GANs. In NIPS,
2017.
Lars M. Mescheder, Andreas Geiger, and Sebastian Nowozin. Which Training Methods for GANs
do actually Converge? In ICML, 2018.
Aryan Mokhtari, A. Ozdaglar, and Sarath Pattathil. A Unified Analysis of Extra-gradient and Op-
timistic Gradient Methods for Saddle Point Problems: Proximal Point Approach. In AISTATS,
2020.
Cristinel Mortici and Hary M Srivastava. Estimates for the Arctangent Function Related to Shafer’s
Inequality. Colloq. Math, 136(2):263-270, 2014.
J. Nash. Non-Cooperative Games. Annals of Mathematics, 54:286, 1951.
A. Nemirovski. Prox-Method with Rate of Convergence O(1/t) for Variational Inequalities with
Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle Point Problems.
SIAM J. Optim., 15:229-251, 2004.
L. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical
notes of the Academy of Sciences of the USSR, 28:845-848, 1980.
R. T. Rockafellar. Monotone Operators and the Proximal Point Algorithm. Siam Journal on Control
and Optimization, 14:877-898, 1976.
Tim Salimans, Ian J. Goodfellow, W. Zaremba, Vicki Cheung, A. Radford, and Xi Chen. Improved
techniques for training gans. ArXiv, abs/1606.03498, 2016.
10
Under review as a conference paper at ICLR 2021
Florian Schafer and Anima Anandkumar. Competitive Gradient Descent. In NeurIPS, 2019.
D. Silver, T. Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, A. Guez, Marc Lanctot,
L. Sifre, D. Kumaran, T. Graepel, T. Lillicrap, K. Simonyan, and Demis Hassabis. A General Re-
inforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-play. Science,
362:1140- 1144, 2018.
J. Slotine and W. Li. Applied Nonlinear Control. Pearson, 1991.
P. Tseng. On Linear Convergence of Iterative Methods for the Variational Inequality Problem.
Journal of Computational and Applied Mathematics, 60:237-252, 1995.
Oriol Vinyals, I. Babuschkin, W. Czarnecki, Michael Mathieu, A. Dudzik, J. Chung, D. Choi,
R. Powell, Timo Ewalds, P. Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Dani-
helka, Aja Huang, L. Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, A. S. Vezhnevets, Remi
Leblond, Tobias Pohlen, Valentin Dalibard, D. Budden, Yury Sulsky, James Molloy, T. L. Paine,
Caglar Gulcehre, Ziyu Wang, T. Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wunsch,
Katrina McKinney, O. Smith, T. Schaul, T. Lillicrap, K. Kavukcuoglu, Demis Hassabis, Chris
Apps, and D. Silver. Grandmaster Level in StarCraft II using Multi-agent Reinforcement Learn-
ing. Nature, pp. 1-5, 2019.
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Lookahead Converges to Sta-
tionary Points of Smooth Non-convex Functions. In ICASSP, 2020.
Yasin Yazici, C. S. Foo, S. Winkler, Kim-Hui Yap, G. Piliouras, and V. Chandrasekhar. The Unusual
Effectiveness of Averaging in GAN Training. In ICLR, 2019.
Guojun Zhang and Y. Yu. Convergence of Gradient Methods on Bilinear Zero-Sum Games. In
ICLR, 2020.
M. Zhang, J. Lucas, Geoffrey E. Hinton, and Jimmy Ba. Lookahead Optimizer: k Steps Forward, 1
Step Back. In NeurIPS, 2019.
11
Under review as a conference paper at ICLR 2021
A Notation
Table A.1: List of mathematical notations used in the paper.
Symbol	Definition
k∙k	L2 norm
xi	i-th element of a vector x = (x1, . . . , xn)
xi	i-th vector of a concatenated vectors x = (x1, . . . , xn)
x-i	(x1, . . .,xi-1,xi+1, . . . ,xn)
Vxf(x0)	Derivative of a function f evaluated at x0
Sr	The zero-centered circle of radius r > 0 in C
<(z)	Real part of z ∈ C
=(z)	Imaginary part of z ∈ C
Arg(z)	The angle between z ∈ C and real axis of the complex plane
σ(A)	The set of singular values of A ∈ Rm×n
ρ(A)	The spectral radius of A ∈ Rm×m
λ(A)	The set of eigenvalues of A ∈ Rm×m
λ≥a(A)	The set of eigenvalues of A with modulus larger than or equal to a ∈ R
λmax (A)	The set of eigenvalues of A with the largest modulus
<(S)	{<(c)|c ∈ S} forS ⊆ C
<min(S)	min <(S)
<max(S)	max <(S)
=(S)	{=(c)|c ∈ S} forS ⊆ C
=≥0(S)	{=i ∈ =(S)|=i ≥ 0}
=min(S)	min =≥0 (S)
=max(S)	max =≥0 (S)
θ(S)	{Arg(c)|c ∈ S)} forS ⊆ C
θ≥0(S)	{θi∣θi ∈ θ(S),θi ≥ 0}
θmin(S)	min θ≥0(S)
θmax(S)	max θ≥0(S)
B	Useful Facts
B.1	Standard Results on Convergence
Lemma 14 (Bertsekas (1999)). Let F : Rm → Rm be continuously differentiable, and let x* ∈ Rm
be such that F(X*) = x*. Assume that ρ(VXF(X*)) < 1. Then, there is an open neighborhood UX*
of x * such that for any x ∈ Ux *, kF t( x) 一 x *|卜 ∈ O(ρ(VX F (x * ))t) for t → ∞.
Lemma 15 (Gidel et al. (2019b)). Let M ∈ Rm×m and u(t) be a sequence of iterates such that,
u(t+1) = Mu(t), then we have three cases of interest for the spectral radius ρ(M):
•	If ρ(M) < 1 andM is diagonalizable 1, then u(t) 2 ∈ O(ρ(M)t u(0) 2).
•	If ρ(M) > 1, then there exists u(0) such that ∣∣u⑶b ∈ Ω(ρ(M)t ∣∣u⑼b).
•	If ∣λi∣ = 1,∀λi ∈ λ(M), and M is diagonalizable, then∣∣u(t) ∣∣2 ∈ Θ(∣∣u(0) ∣∣2).
B.2	Characteristic Equations of First-Order Dynamics in Bilinear Games
Latest work of Zhang & Yu (2020) provides the exact and optimal conditions for popular first-order
methods to converge in zero-sum bilinear games, if possible. Besides from the exact conditions
and the choice of optimal hyperparameters, they also derive the characteristic equation of each first-
order dynamics in the zero-sum bilinear games. Since our proofs of theorems in Section 3.2 heavily
1Actually, M does not have to be diagonalizable; see Theorem 5.4 and Theorem 5.D4 in Chen (1995).
12
Under review as a conference paper at ICLR 2021
rely on these characteristic equations, we restate somewhat simplified version of the equations for
Equation 9 using our notations.
GDAlt	(λi — I)2 + η2σ2λi =	0.	(14)
GDSim	(λi — 1)2 + η2σ2 = 0		(15)
PPSim	(1∕λi — 1)2 + η2σ2 =	0.	(16)
EGAlt	(λi — 1)2 + (η2 + 2η)σi2 (λi — 1) + (η2σi2 + η2σi4) = 0.		(17)
EGSim	(λi — 1)2 + 2ησi2 (λi —	1) +η2σi2 + η2σi4 = 0.	(18)
We denote the singular values of matrix A in Equation 9 by σi . The eigenvalues of each dynamics’
Jacobian matrix are denoted by λi. Note that Zhang & Yu (2020) also derive characteristic equations
of memory-augmented first-order methods, such as OGD (Popov, 1980) and the momentum method,
which we do not cover in this paper.
C Omitted Results
Proposition 16. Alternating GD dynamics F GDAlt with a learning rate η ∈ (0, 2) fails to converge
and oscillates around the Nash equilibrium of the game in Equation 8. However, its Lookahead
dynamics GLA-GDAlt with a synchronization period k ∈ N and a rate α ∈ (0, 1) globally converges to
the Nash equilibrium if (1 一 η2 + ' '4-η ) =L
Proof. One can easily check from Equation 2 that the dynamics F GDAlt can be written as
def
Defining M d=ef
1	—η
η 1 一 η2
1	—η
η 1 一 η2
x(t)
x(2t)
, the Lookahead dynamics GLA-GDAlt can be written as
GLA-GDAlt (x(1t) , x(2t) ) = ((1 一 α)I + αMk)
(t)
x1
(t)
x2
(19)
(20)
It follows that the eigenvalues of Nx GLA-GDAlt Can be written as 1 一 α + aλ± with λ± = 1 — η2 ±
i"；吟 ∈ λ(M) for any η ∈ (0, 2). However, 1 — a + aλ± is an interpolation between two distinct
points on Si since ∣λ±∣ = 1 and λ± = 1 , implying |1 — a + aλ±∣ < 1. Therefore, we conclude
from Lemma 14 that the iterates of GLA-GDAlt converge to the Nash equilibrium (0, 0) of the game
with convergence rate O(|1 — α + αλk± |t/k), assuming the amortization of its computation over k
forward steps. The proof for oscillation of F GDAlt follows from Lemma 15 and can be found in Gidel
etal.(2019a).	□
Proposition 17. Simultaneous EG Lookahead dynamics GLA-EGSim with a learning rate η ∈ (0, 1),
a synchronization period k ∈ N and a rate α ∈ (0, 1) globally converges to the Nash equilibrium
of Equation 8. Furthermore, the rate of convergence is improved upon its base dynamics F EGSim if
<((1 — η2 + iη)k) < (1 — η2 + η 4)k and α is large enough.
Proof. Using simple algebra on Equation 5, the dynamics F EGSim can be written as
Defining M =f ɪ
—η
1 — η2
—η
1 — η2
(t)
1
(t)
2
(21)
1 — η2
η
, its Lookahead dynamics GLA-EGSim with a synchronization
period k ∈ N and a rate α ∈ (0, 1) can be written as
x
x
GLA-EGSim (x(1 ) , x(2 ) ) = ((1 — α)I + αMk)
(t)
x1
(t)
x2
(22)
13
Under review as a conference paper at ICLR 2021
It follows that the eigenvalues of VxGla-eGSim are 1 - a + aλ± with λ± = 1 - η2 土 iη ∈ λ(M).
However, 1 - a + αλ± is an interpolation between two distinct points on/inside Si since ∣λ±∣k < 1
for any η ∈ (0, 1). It follows that |1 - α + αλk± | < 1, from which we conclude from Lemma 14 that
the iterates of GLA-EGSim converge to the Nash equilibrium (0, 0) of the game with convergence rate
O(|1 - α + αλk± |t/k), assuming the amortization of its computation over k forward steps.
Now we show that the convergence is accelerated upon its base dynamics F EGSim if <((1 - η2 +
iη)k) < (1 - η2 + η4)2k and α is large enough. Figure 1 (c) intuitively shows that the line segment
between (1,0) and λ± contains a line segment inside S∣λ±∣k when k is such that <(λ±) < ∣λ±k|.
Therefore, for a large enough a, the interpolation 1 - a + aλ± lies inside S∣λ±∣k. This implies that
the convergence rate O(|1 - α + αλ± |t/k) of GLA-EGSim is accelerated upon the rate O(∣λ±∣t) of its
base dynamics.	□
Proposition 18 (Equilibrium of Lookahead dynamics). Let F be a dynamics and G be its associ-
ated Lookahead dynamics with a synchronization period k ∈ N. Then any equilibrium of F is an
equilibrium of G and any equilibrium of G is a periodic point of F.
Proof. Let k ∈ N and α ∈ (0, 1) be the synchronization period and synchronization rate of G,
respectively. It is trivial to see that G(x*) = ((1 - α)id + aFk)(x*) = (1 - α)x* + ax* = x* if
F(x*) = x*. Conversely, one can easily check that G(x*) = (1 — α)x* + αFk (x*) = x* implies
Fk (x*) = x*.
D Experimental Details
We report the actual hyperparameters used for the experiments of Section 5 in TableD.2 and D.3.
Furthermore, we also provide the detailed derivations of the theoretically recommended range of
synchronization period k ∈ N.
Table D.2: Hyperparameters used for the experiment on the bilinear game
Configuration	k (Theorem 5-8)	k	α (Theorem 5-8)	α
LA-GDAlt+	N	25	(0, 1)	0.1
LA-GDAlt-	N	5	(0, 1)	0.9
LA-GDSim+	(18.47, 39.62)	25	small enough	0.1
LA-GDSim-	(18.47, 39.62)	5	small enough	0.9
LA-EGSim+	(16.9, 34.93)	25	large enough	0.9
LA-EGSim-	(16.9, 34.93)	5	large enough	0.1
Table D.3: Hyperparameters used for the experiment on the nonlinear game
Configuration	k (Theorem 10,11)	k	α (Theorem 10,11)	α
LA-GDAlt+	(31.16, 93.49)	35	small enough	0.1
LA-GDAlt-	(31.16, 93.49)	5	small enough	0.9
LA-GDSim+	(31.6, 94.81)	35	small enough	0.1
LA-GDSim-	(31.6, 94.81)	5	small enough	0.9
LA-EGSim+	(121.76, 365.30)	175	large enough	0.9
LA-EGSim-	(121.76, 365.30)	5	large enough	0.1
D.1 DERIVATION OF THEORETICALLY RECOMMENDED RANGE OF k IN EQUATION 12
We plug in σ(A) = {1.195, 1.163, 1.094, 1.083, 1.018, 0.999, 0.969, 0.888, 0.879, 0.852} with
σmax = 1.195 and σmin = 0.852 to Theorem 5-8. Then we have
14
Under review as a conference paper at ICLR 2021
•	LA-GDAit： {k ∈ N : k arccos(1 — 0.[σi) mod π = 0, ∀σi},
•	LA-GDSim:(0,不—^π n 19) = (18.47, 39.62),
Sim 2 arctan 0.08 , 2 arctan 0.12	.	,	.	,
•	LA-EGSim: (5—π	, 5—3π n 1,) = (16.9, 34.93),
Sim 2 arctan 0.09 , 2 arctan 0.13	. ,	.	,
which give ranges for k as in TableD.2.
D.2 DERIVATION OF THEORETICALLY RECOMMENDED RANGE OF k IN EQUATION 13
LA-GDAlt From Equation 2, the Jacobian of dynamics F LA-GDAlt of Equation 13 can be derived as
VxFGDAlt(X1, x2) = [1 1 — η2 + —η(i — 3χ2)] [χ2],	(23)
and it is trivial to see that it has an equilibrium at (0, 0). By plugging in = 0.01 and η = 0.05, we
obtain
VxFGDAlt(0,0) = [0.05	0.9.98 ]	(24)
with eigenvalues λ± == 0.99 ± 0.05i. Note that ∣λ±∣ = 1.0003 > 1 and VxFGDAlt(0,0) has the
imaginary conditioning of 1, which implies that the origin is an unstable equilibrium of GDAlt
that can be locally stabilized by a Lookahead dynamics. By plugging in the eigenvalues and
θmin(VxFGDAlt(0,0)) = θmaχ(VxFGDAlt(0,0)) = arctan 095 = 0.0504 to Theorem 10, we obtain
the theoretically recommended range of k as (31.16, 93.49).
LA-GDSim From Equation 1, the Jacobian of dynamics F LA-GDSim of Equation 13 can be derived
as
VxFGDSim(X1,X2)=[1 1 + ne—η- 3x2)][X2] ,	(25)
and it is trivial to see that it has an equilibrium at (0, 0). By plugging in = 0.01 and η = 0.05, we
obtain
VxFGDSim (0, 0) = [0.05 —1.0.05 ]	(26)
with eigenvalues λ± =f 1.0025 ± 0.0499i. Note that ∣λ±∣ = 1.0037 > 1 and VxFGDSim(0,0)
has the imaginary conditioning of 1, which implies that the origin is an unstable equilibrium of
GDAlt that can be locally stabilized by a Lookahead dynamics. By plugging in the eigenvalues
and θmin(VxFGDSim(0,0)) = θmaχ(VxFGDSim(0,0)) = arctan ⅜0429 = 0.0497 to Theorem 10, We
obtain the theoretically recommended range of k as (31.6, 94.81).
LA-EGSim From Equation 5, the dynamics F LA-EGSim of Equation 13 can be derived as
FEGSim(X1,X2)
X1
X2
X1 — ηX2
X2 + n(Xι + e(X2 — χ2))
X1 — ηX2
X2 + η(X1 + e(X2 — X23))
where
(27)
(28)
By computing the derivatives with X1 = 0, X2 = 0 and e = 0.01, η = 0.05, we obtain
VxFEGSim(0,0)=
0.9975	—0.05
0.05	0.9005
(29)
with eigenvalues λ± = 0.949 ± 0.0122i. Note that ∣λ±∣ = 0.949 < 1 and VxFEGSim(0,0) has the
imaginary conditioning of 1, which implies that the origin is an stable equilibrium of EGSim whose
local convergence can be accelerated by a Lookahead dynamics. By plugging in the eigenvalues and
θmin(VxFEGSim(0,0)) = θmaχ(VxFEGSiιπ(0,0)) = arctan 00042 = 0.0129 to Theorem 11, we obtain
the theoretically recommended range of k as (121.76, 365.30).
15
Under review as a conference paper at ICLR 2021
E Proofs
E.1 Proof of Proposition 1
Proof. One can easily check from Equation 1 that the dynamics F GDSim can be written as
(t)
FGDSim(x(1t),x(2t))= 1	-1η	x(1t) .	(30)
η 1 x2
Defining M
def 1
=η
-1η , its Lookahead dynamics GLA-GDSim can be written as
(31)
It follows that the eigenvalues of VxGla-gdSim can be written as 1 - a + αλk± with λ± d=ef 1 ± iη ∈
λ(M). Assuming <((1 +iη)k) < 1, the line segment between (1, 0) and λk± contains a line segment
inside S1 as in Figure 1 (b). Therefore, for a small enough α, the interpolation1 - α + αλk± lies
inside Si, implying |1 - α + αλ±∣ < 1. We thus conclude from Lemma 15 that the iterates of
GLA-GDSim converge to the Nash equilibrium (0, 0) of the game. The proof for divergence of F GDSim
follows from Lemma 15 and can be found in Gidel et al.(2019a).	□
E.2 Proof of Proposition 2
Proof. Using simple algebra on Equation 4, the dynamics F PPSim can be written as
F PPSim (x(1t) , x(2t))
1 Γι	-η] M)
1 + η [η	1 ]	x2t)
(32)
Defining M = ɪ+^ 11	，, its Lookahead dynamics GLA-PPSim with a synchronization period
k ∈ N and a rate α ∈ (0, 1) can be written as
(33)
It follows that the eigenvalues of VxGLA-PPSim are 1 - a + αλ± with λ± = 1±2 ∈ λ(M). We know
that 1 - a + aλ± is an interpolation between two distinct points on/inside Si since ∣λ±∣k < 1 for
any η ∈ (0, 1). It follows that |1 - α + αλk± | < 1, from which we conclude from Lemma 14 that
the iterates of GLA-PPSim converge to the Nash equilibrium (0, 0) of the game with convergence rate
O(|1 - α + αλk± |t/k), assuming the amortization of its computation over k forward steps.
Now we show that the convergence is accelerated upon the base dynamics F PPSim if <((1 + iη)k) <
(1 + η2)k and α is large enough. Figure 1 (c) intuitively shows that the line segment between (1, 0)
and λ± contains a line segment inside S∣λ±∣k when k is such that <(λ±) < ∣λ±k|. Therefore, the
interpolation 1 - a + αλ± lies inside S∣λ±∣k for a large enough a. This implies that the convergence
rate O(∣1-ɑ+αλ± |t/k) of GLA-PPSim is accelerated upon the rate O(∣λ±∣t) ofits base dynamics. □
E.3 Proof of Lemma 3
Proof. We prove each of the cases in their order.
Case ρ(X) = 1. Assume that λmax k 6= 1 for any λmax ∈ λmax(X). Then we can immediately
conclude ρ(f (X)) < 1 since 1 - α+ αλik ∈ λ(f (X)) is an interpolation between two distinct points
(1, 0) and λik on/inside Si for any λi ∈ λ(X).
16
Under review as a conference paper at ICLR 2021
(a) ρ(X) > 1
Figure 4: Visualized eigenvalues of (1 - α)I + αXk.
(b) ρ(X) < 1
Case ρ(X) > 1. Assume that <(λk) < 1 for any λ ∈ λ≥1(X). Then for each λ ∈ λ≥1, λk can be
visualized as point B in Figure 4 (a), where the existence of point D is guaranteed by <(λ) < 1. It
is easy to see from the figure that
∣∣ACIl = α∣λk - 1| < 2cos(π - θ(λ)) = ∣∣AD∣∣	(34)
is sufficient to place 1 - α + αλk inside Si. Furthermore, for any λ ∈ λ(X) such that ∣λ∣ < 1,
1 -α+αλk lies inside S1 since 1 -α+αλk is an interpolation between two distinct points on/inside
S1 . Therefore we conclude ρ(f (X)) < 1.
Case ρ(X) < 1. Assume that <(λk) < ρ(X)2k for any λ ∈ λmax(X). Then for any λi ∈ λ(X),
λik can be visualized as point B in Figure 4 (b) since the existence of point D is guaranteed by
<(λk) < ρ(X)2k and sin(φ(λ<) = sin(θ(λi))∕ρ(X)k follows from the law of sines. Therefore We
can intuitively see from the figure that
∣∣BC∣∣ = (1 - α)∣λk - 1| < 2ρ(X)k cos(π - φ(%)) = ∣∣BD∣∣	(35)
is sufficient to place 1 — α + αλk inside Sp(x)k, concluding the proof.	□
E.4 Proof of Lemma 4
Proof. Let us denote θmin d=ef θmin (S), θmax	d=ef	θmax (S)	for	brevity	and let k	∈	N be such	that
》二，∏	3π	π	3πθmin	(∏ 3π、	πθmax 3π	1
k ∈ ^2θmin，2θmaX J- ThenWehave kθmin ∈	2 , ^mx	⊆(2，万 J	and kθmax	∈	2θmin ,9	⊆
(2, 32π), which implies <(λk) < 0 for any λi	∈	S such	that	=(λi) > 0. Since every element	of S
has its conjugate pair in S by the assumption, we conclude <(λik) < 0 for any λi ∈ S.
Now we show that the existence of k ∈ N such that k ∈
π 3π
2θmin , 2θmax
is guaranteed for a small
enough η > 0 when =max(S) < 3. Using simple algebra, we can see that θmax < f (θmin) for
f : R → R defined by f (x) = ∏+χ is equivalent to 2θπ 2θmX > 1, implying nonempty
N ∩ Qeπ. , 2θmax ). Therefore it suffices to show that θmax < f (θmin) holds for a small enough
η > 0 when =max(S) < 3.
=min(S)
Let us define a function H : R → R given by
def 2θmax +	1 + η<max(S)	1 + 2 sec θmin -
H ⑷=(1+ —) G + η<min(S)) (l + 2sec θmax+ + ^，
(36)
π
17
Under review as a conference paper at ICLR 2021
WhPrP θ - - =f arrtan η=min(S)	θ + =f ɑrctarι η=max(S) ɑnd b =f (I+2sec θmin-)tan4 θmax+
Where θmin = arctan ι+η<maχ(s), θmaχ = arctan 1+η<min(S) and b _	540
We show that the inequality
=max(S)	3
=min(S)	Hn
(37)
implies θmax < f (θmin) and conclude the proof by shoWing that there exists a small enough η > 0
SUCh that satisfies Equation 37 when =max(S) < 3.
=min(S)
Note that the inequalities θmin-	≤ θmin and θmax ≤ θmax+ direCtly follow from the definitions of
min-	and θmax+ . Furthermore, using the Shafer-type double inequalities (MortiCi & Srivastava,
2014) for arctan(∙), we obtain
min-	≥
3 tanθ min
1 + 2∖∕l + tan2 θmin-
__________3η=min__________
(1 + η<max)(1 + 2 sec θmin )
max+ ≤
3 tanθ max+	1	5	+
1 + 2p1 + tan2 θmax+ +画 tan "max
3η=max
(1 + η<min)(1 + 2 Sec θmax + )
η=maxtan θmax+
180(1 +η<min)
(38)
(39)
(40)
+
from whiCh follows that
=max(S )
=min(S)
(1 + η<max(S) A ( 1 + 2sec θmin + 5)
∖ 1 + η<min(S)Jl1 + 2 Sec θmax+	)
(41)
However, assuming inequality 37, we Can derive
=max(S) ( 1 + η<max(S) A ( 1 + 2 Sec θmin- + 5) <
=min(S) ∖ 1 + η<min(S) ) ∖ 1 + 2 sec θmax+	)
3∏	= f(θmax+)
π + 2θmax+	θ max+
(42)
Furthermore, since f 0(x) =(冗亮:产,We know that f is both concave and monotonically increasing.
HenCe it follows that
f(θmax+)
θ max +
< f(θmin)
θmin
(43)
from which we obtain θmax < f(θ min) by combining Equation 41-43.
Finally, we prove that Equation 37 holds for a small enough η > 0 when =max(S) < 3. Assume
=min(S)
=max(S)	/ Q ane IQf e ^e Q _ =max(S)、(ɔ R∖7 fha	CrYirHniiHv Cf	3	of	ʃɔ	—	(ɔ	CInrl	fha	fact that
=.(S)	< 3 and let 匕 — 3	= (S)	> o. By the	COntinUity of	H(	)	at	η	—	o	and	the	fact Ihat
H(0) =	1, there exists δ > 0	such that	|3 - Hn) | <	E holds for any η	∈ (0, δ).	Therefore	we have
=max(S)	= 3 - e < H3-τ for	any η ∈ (0, δ), concluding the proof.	□
=min(S)	H(n)
E.5 Proof of Theorem 5
Proof. From Equation 2, the dynamics F GDAlt of Equation 11 can be derived as
FGDAlt(x(1t),x(2t))= ηIΣrr Ir--ηηΣ2rΣr2 xx((1tt)) .	(44)
def
Defining M d=ef
Ir
ηΣr
-ηΣr
Ir - η2Σr2
, its Lookahead dynamics GLA-GDAlt with a synchronization pe-
riod k ∈ N and a rate α ∈ (0, 1) can be written as
GLA-GDAlt(x(1t),x(2t)) = ((1 -α)I+αMk)
x(1t)
x(2t)
(45)
18
Under review as a conference paper at ICLR 2021
Together with Equation 14, We can see that eigenvalues of ^xGla-gdaλi can be written as 1 -a+αλ±i
with λ±i = 1 - η~σ^- ± iησiy1-^2σ- ∈ λ(M) for any η ∈ 仇，σfax). In the meanwhile,
simple calculation gives Us ∣λ±∕ = 1, which implies P(M) = 1. Now assume k ∈ N is such that
22
k arccos(1 — η-^i-) mod 2π = 0 for any σ%. Then it follows that λk±i = 1 for any λ±i ∈ λ(M),
from which we obtain p(VxGla-GDAIt) < 1 from Lemma 3. It follows from Lemma 15 that the
iterates converge to the origin, and we conclude the proof by observing that the transformations
xι → U [xi； 0m-r] + x； and x2 → V [x2; 0n-r] + xf of (0,0) ∈ Rr X Rr gives (x；, xf) ∈ Rm X Rn,
which is a Nash equilibrium of Equation 9.	□
E.6 Proof of Theorem 6
Proof. From Equation 1, the dynamics F GDSim of Equation 11 can be derived as
FGDSim(x(1t),x(2t))= ηIΣrr	-ηIrΣr	xx((1tt)) .	(46)
Let us define J	d=ef	-0Σr	Σ0r	and M	d=ef	I -	ηJ.	Then its Lookahead dynamics	GLA-GDSim	with a
synchronization period k ∈ N and a rate α ∈ (0, 1) can be written as
(t)
GLA-GDSim(x(1t),x(2t))= ((1-α)I+αMk) x(1t) .	(47)
x2
Together with Equation 15, we can see that the eigenvalues of VxGLA-GDSim can be written as 1 - α +
ɑλ±i with λ±i = 1 ± in。i ∈ λ(M). In the meanwhile, one can easily see that ∣λ±∕ > 1, implying
P(M) > 1. Now assume that k ∈ (τ5——----------, τ5——-3π---). Then since tanθmin(λ(M))=
2 arctan ησmin , 2 arctan ησmax	min
nσmin and tan θmax(λ(M)) = nσmax, we have k ∈ (2θmin∏λ(M)), 2θmax∏λ(M))) ∙ It follows from
Lemma 4 that <(λk±i) < 0 for any λ±i ∈ λ(M), and the existence of k is guaranteed for a small
enough n when =max(λ(M)) = σmax < 3. Then it follows from Lemma 3 that P(VXGLA-GDSim) < 1
=min(λ(M))	σmin	.	x LA-GDSim
holds for a small enough α. Therefore, by Lemma 15, the iterates converge to the origin, and
we conclude the proof by observing that the transformations x1 7→ U [x1; 0m-r] + x1； and x2 7→
V [x2; 0n-r ] + xf of (0,0) ∈ Rr x Rr gives (x；, xf) ∈ Rm x Rn, which is a Nash equilibrium of
Equation 9.	□
E.7 Proof of Theorem 7
Proof. From Equation 4, the dynamics F PPSim of Equation 11 can be derived as
FPPSim(x(1t),x(2t))= -ηIrΣr ηIΣrr-1 "xx((1tt))# .	(48)
Let us define J d=ef -0Σr	Σ0r and M d=ef (I + ηJ)-1. Then its Lookahead dynamics GLA-GDSim with
a synchronization period k ∈ N and a rate α ∈ (0, 1) can be written as
Together with Equation 16, we can see that the eigenvalues of VxGLA-EGSim can be written as 1 -
α + aλ±i with λ±i =事算 ∈ λ(M). In the meanwhile, we can easily see that ∣λ±∕ < 1
holds for any η > 0. Therefore, 1 - α + αλk±i is an interpolation between two distinct points
(1, 0) and λk±i on/inside S1, implying P(VxGLA-PPSim) < 1. Hence it follows from Lemma 15 that
the iterates converges to the origin. However, the transformations x1 7→ U [x1; 0m-r] + x1； and
19
Under review as a conference paper at ICLR 2021
X2 → V [X2； 0n-r]+ x2 of (0,0) ∈ Rr X Rr gives (xj, X 力 ∈ Rm X Rn, which is a Nash equilibrium
of Equation 9.
Now we show that GLA-PPSim can accelerate the convergence upon its base dynamics F PPSim . Assume
k ∈ ( o——∏l-------,不——-3π-------). Note that MT shares the same eigenvalues with the Jacobian of
2 arctan ησmin 2 arctan ησmin
F GDSim. Therefore we have tan θmin(λmax(M-1)) = tan θmax(λmax(M-1)) = ησmin , which implies
k ∈ (2θmin(λιmtx(MT)), 2θmax(λ3Πx(M-i))) ∙ Then it follows from Lemma 4 that <(λ±k) < 0 for
any λ±-i1 ∈ λmax(M-1), and the existence of k ∈ N is guaranteed for a small enough η. Then
we have <(λk±i) < 0 for any λ±i ∈ λ(M) since the reciprocal of a complex number preserves the
sign of the real part. Hence it follows from Lemma 3 that p(VxGla-pPSim) < ρ(M)k holds for a
large enough a. We conclude the proof by noting that the convergence rate O(ρ(VχGLA-PPSim) k) of
GLA-PPSim provided by Lemma 15 is faster than the rate O(ρ(M)t) of F PPSim, assuming amortization
of computations over k forward steps.	□
E.8 Proof of Theorem 8
Proof. From Equation 5, the dynamics F EGSim of Equation 11 can be derived as
FEGSim(x(1t),x(2t))
Ir - ηΣr2
ηΣr
-η Σr	x(1t)
Ir - ηΣr2	x(2t)
(50)
Let us define J d=ef	Σr	Σr2 and M d=ef I - ηJ. Then its Lookahead dynamics GLA-EGSim with a
-Σr Σr
synchronization period k ∈ N and a rate α ∈ (0, 1) can be written as
GLA-EGSim (x(1t), x(2t)) = ((1 - α)I + αMk) x((1tt)) .	(51)
Together with Equation 18, We can see that the eigenvalues of VxGLA-EGSim Can be written as 1 一 α +
ɑλ±i with λ±i = 1-ησi 士iησi ∈ λ(M). In the meanwhile, we can easily see that |λ±i | < 1 forany
η ∈(0, σmax), implying P(M) < 1. Therefore, 1-α+αλ±i is an interpolation between two distinct
points (1, 0) and λk±i on/inside S1, implying ρ(Vx GLA-EGSim) < 1. Hence it follows from Lemma 15
that the iterates converges to the origin. However, the transformations xι → U [xi； 0m-r ] + x； and
X2 → V [X2; 0n-r ] + x； on (0,0) ∈ Rr X Rr gives (x；, x2) ∈ Rm X Rn, which is a Nash equilibrium
of Equation 9.
Now we show that GLA-EGSim can accelerate the convergence upon its base dynamics F EGSim . Assume
k∈
2 arctan
ησmin
1-ησmin
3π
2 arctan
ησmin
1-ησmin
and η ∈
(0,2σ⅛). NOtethat |%|2 = 2η2(σi - 口)2 + 2
π
holds for each λ±i ∈ λ(M). This implies λmax(M) = {1 一 ησmin ± iησmin} for any η ∈(0, 2σmax),
hence k ∈ (2θmin(λmax(M)), 2θmax(3max(M))) ∙ It follows from Lemma 4 that <(λ±i) < 0 holds for
any λ±i ∈ λ(M), and the existence of k is guaranteed for a small enough η. Then by Lemma 3
we have P(Vx GLA-EGSim) < P(M)k for a large enough α. We conclude the proof by noting that the
convergence rate O(P(VXGLA-EGSim)t) of GLA-EGSim provided by Lemma 15 is faster than the rate
O(P(M)t) of FEGSim, assuming amortization of computations over k forward steps.	□
E.9 Proof of Theorem 9
Proof. From Equation 7, the Jacobian of G evaluated at x； can written as
VxGM) = Vx ((1 — α)id + αFk) (x*) = (1 一 α)I + αVxFk(x*)	(52)
k
=(1 一 α)I + α Y Vx F (F i-1(x*)) = (1 一 α)I + α(VxF (x*))k,	(53)
i=1
20
Under review as a conference paper at ICLR 2021
where the chain rule is used in third equality with a slight abuse of notation F0 =e id. We use the
fact that x* is an equilibrium of dynamics F for the last equality. It is easy to see from Equation 53
that eigenvalues of VχG(x*) can be written as 1 一 α + αλk for each λ% ∈ λ(VχF(x*)).
However, λ? is either on/inside S1 since ∣λi | ≤ 1 for each i due to the Lyapunov stability of x* in F.
Therefore, 1 一 α + αλik is an interpolation between two points (1, 0) ∈ S1 and λik either on/inside
S1; hence |1 一 α + αλik | ≤ 1. By assumption that λik 6= (1, 0) for each λi ∈ λ(VxF(x*)), the
inequality is strict, i.e. |1 一 α + αλ? | < 1, implying the local asymptotic stability of x* in G by
Proposition 14.	□
E.10 Proof of Theorem 10
Proof. From Equation 7, the Jacobian of G evaluated at x* can written as
VxG(x*) = Vx ((1 — α)id + αFk) (x*) = (1 — α)I + αVχFk(x*)	(54)
k
=(1 一 α)I + α Y Vx F (F i-1(x*)) = (1 一 α)I + α(VχF (x*))k,	(55)
i=1
where the chain rule is used in third equality with a slight abuse of notation F0 d=ef id. We use the
fact that x* is an equilibrium of dynamics F for the last equality. It is easy to see from Equation 55
that the eigenvalues of VxG(x*) can be written as 1 一 α + αλik for each λi ∈ λ(VxF(x*)).
Now assume that every element of λ≥1(VxF(x*)) has non-zero imaginary part, and let k ∈
(2θmin(λ≥ι8xF(x*))), 2θmax(λ≥3(VxF(x*))) ) . Let η > 0, J ∈ Rn×n be Sueh that VXF(x* ) = I - nJ.
Then by Lemma 4, <(λik) < 0 holds for any λi ∈ λ≥1 (Vx F (x*)), and the existence of such k ∈ N
is guaranteed for a small enough η when =max(λ≥1(金£*))) < 3. Then it follows from the second
case of Theorem 3 that ρ(VxG(x*)) < 1 holds for a small enough α. By Proposition 14, this implies
local asymptotic stability of x* in G, concluding the proof.	□
E.11 Proof of Theorem 11
Proof. From Equation 7, the Jacobian of G evaluated at x* can written as
VxG(x*) = Vx ((1 一 α)id + αFk) (x*) = (1 一 α)I + αVxFk(x*)	(56)
k
=(1 一 α)I + α Y Vx F (F i-1(x*)) = (1 一 α)I + α(VxF (x*))k,	(57)
i=1
where the chain rule is used in third equality with a slight abuse of notation F0 d=ef id. We use the
fact that x* is an equilibrium of dynamics F for the last equality. It is easy to see from Equation 57
that the eigenvalues of VxG(x*) can be written as 1 一 α + αλik for each λi ∈ λ(VxF(x*)).
Now assume that every element of λmax(VxF(x*)) has non-zero imaginary part, and let k ∈
(2θmin(λmaπ(∕xF(x*)))，2θmaχ(λma35(VxF(x*))) ) . Let η > 0, J ∈ Rn×n be such that VxF(X*)=1一〃1
Then by Lemma 4, <(λik) < 0 holds for any λi ∈ λmax(VxF(x*)), and the existence of such k ∈ N
is guaranteed for a small enough η when =max山叫｝生*?)) < 3. Then it follows from the third
=min(λmax(VxF (x )))
case of Theorem 3 that ρ(VxG(x*)) < ρ(VxF(x*))k holds for a small enough α. We conclude the
proof by noting that this implies the upper bound O(P(VxG(x*) W) on the rate of local convergence
provided by Proposition 14 is faster than O(VxF(x*)t).	□
E.12 Proof of Proposition 12
Proof. We directly follow the proofs of Lemma 2.1 and Lemma 3.1 in Daskalakis & Panageas
(2018) and show that α ∈ (0, “1Lk) guarantees locally diffeomorphic Lookahead dynamics, i.e., it
is locally invertible at any given points.
21
Under review as a conference paper at ICLR 2021
Note from Equation 7 that the Jacobian of G evaluated at x can written as
VxG(X) = Vx ((1 - α)id + αFk) (x) = (1 - α)I + αVχFk(x)	(58)
k
= (1 - α)I + α Y VxF (F i-1(x)),	(59)
i=1
where we have used the chain rule in the last equality with a slight abuse of notation F0 d=ef id.
Now assume that α ∈
and consider the following inequalities
k
ρ(YVxF(Fi-1(x)))≤
i=1
k
Y VxF(Fi-1(x))
i=1
k
≤YVxF(Fi-1(x)) ≤Lk,
i=1
(60)
where the first and second inequalities hold for any operator norms and the last inequality is due to
L-Lipschitzness of F . Then it follows from the assumption that
ρ(YY VxF(Fi-1(x*))) ≤ Lk < 1-α
i=1	α
(61)
Therefore, We conclude that G locally diffeomorphic, since P(Qk=I VxF(Fi-1(x*))) < 1-α im-
plies 0 ∈/ λ(VxG(x)).
Now let Us define the set of unstable equilibria of G as U =f {x* : G(x*) = x*,ρ(VxG(x*)) > 1}.
Then it directly folloWs from the locally diffeomorphic G and the arguments of Lee et al. (2019);
Daskalakis & Panageas (2018) that the set {x(0) : lim Gt(x(0)) ∈ U} is of measure zero, which
t→∞
concludes the proof. We refer the readers to Appendix A of Daskalakis et al. (2018) for the detailed
derivation of measure-zero arguments.	□
E.13 Proof of Proposition 13
Proof. From Equation 7, the Jacobian of G evaluated at x* can written as
VxG(x*) = Vx ((1 — α)id + αFk) (x*) = (1 — α)I + αVxFk(x*)	(62)
k
= (1 -α)I+αY VxF(Fi-1(x*)) = (1 - α)I + α(VxF (x*))k,	(63)
i=1
where the chain rule is used in third equality with a slight abuse of notation F0 d=ef id. We use the fact
that x* is an equilibrium of dynamics F for the last equality. It is easy to see from Equation 63 that
the eigenvalues ofVxG(x*) can be written as 1 - α+ αλik for each λi ∈ λ(VxF (x*)). However, by
the assumption, there exists a λ ∈ λ(VxF(x*)) such that ∣λ∣ > 1. Since λ is a positive real number,
we have 11 一 α + αλk | > 1, concluding the proof.	□
F	Additional Experiments
F.1 Eigenvalues of GAN Dynamics
Theorem 10-11 assumes the radius-supporting eigenvalues, namely λ≥1 (VxF) and λmax(VxF)), to
have non-zero imaginary parts and imaginary conditioning less than 3; otherwise, the existence of
k that satisfies the sufficient conditions of Theorem 10-11 may not exist. We verify whether such
assumptions are realistic in practical settings. Specifically, we train GANs on MNIST dataset with
two different loss functions, non-saturating (Goodfellow et al., 2014) and WGAN-GP (Gulrajani
et al., 2017), and visualize the top 20 eigenvalues of Vx FGDSim for each loss function in Figure 5.
Figure 5 suggests most of the radius-supporting eigenvalues of Vx F GDSim at well-performing point
(Inception Score (IS) (Salimans et al., 2016) u 9) are distributed along the imaginary axis, and have
non-zero imaginary part with imaginary conditioning less than 3. This suggests that our assumptions
on the eigenvalues is not unrealistic and Theorem 10-11 can be applied for a practical non-linear
game like GANs.
22
Under review as a conference paper at ICLR 2021
(a) Non-saturating GAN (IS=8.69)	(b) WGAN-GP (IS=9.09)
Figure 5: Visualized top 20 eigenvalues of VxFGDSim before (blue) and after (orange) training GANs
With tWo different loss functions on MNIST.


F.2 Ill-Conditioned Bilinear Games and Momentum Methods
Iterations	Iterations
(a) Lookahead dynamics without momentum. (b) Lookahead dynamics with momentum.
Figure 6: Optimization progress of first-order methods in an ill-conditioned bilinear game. (a)
Comparison between convergence of Lookahead dynamics chosen by (+) and (-) against Theorem
5 and 8. (b) Comparison between convergence of Lookahead dynamics with positive (PM) and
negative (NM) momentums.
We test the convergence and acceleration of Lookahead dynamics in an ill-conditioned bilinear
game, and see if Lookahead can accelerate momentum-based dynamics in such game. Specifically,
we test convergence of each dynamics in the game given by Equation 12 with n = 20 and = 1,
which gives a sample ofA with σmax = 8.81 and σmin = 0.11. Note that this game has a significantly
larger conditioning number σmax = 76.4 than the bilinear game of a conditioning 1.401 We used in
σmin
Section 5.
We fix η = 0.05 throughout the experiments, and use Theorem 8 to derive theoretically recom-
mended (+) hyperparameters k = 300, α = 0.9 for LA-EGSim dynamics. We use k = 50, α = 0.1
to represent hyperparameters of LA-EGSim chosen against (-) the theorem. For LA-GDAlt, We use
k = 300, α = 0.1. We use the momentum factor β = -0.1 for negative (NM) and β = 0.1 for
positive (PM) momentum methods.
Figure 6 (a) shoWs that Theorem 5 and 8 indeed hold even for an ill conditioned game. Furthermore,
Figure 6 (b) suggests that Lookahead can significantly accelerate the convergence of momentum
methods that provably perform Well on bilinear games, including the gradient descent With negative
momentum (Gidel et al., 2019b) and extragradient With momentum (Azizian et al., 2020).
23