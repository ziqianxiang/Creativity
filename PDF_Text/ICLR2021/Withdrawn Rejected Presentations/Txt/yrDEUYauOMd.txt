Under review as a conference paper at ICLR 2021
Attainability and Optimality:
The Equalized-Odds Fairness Revisited
Anonymous authors
Paper under double-blind review
Ab stract
Fairness of machine learning algorithms has been of increasing interest. In order
to suppress or eliminate discrimination in prediction, various notions as well as
approaches to impose fairness have been proposed. However, in different scenar-
ios, whether or not the chosen notion of fairness can always be attained, even if
with unlimited amount of data, is not well addressed. In this paper, focusing on the
Equalized Odds notion of fairness, we consider the attainability of this criterion,
and furthermore, if attainable, the optimality of the prediction performance under
various settings. In particular, for classification with a deterministic prediction
function of the input, we give the condition under which Equalized Odds can hold
true; if randomized prediction is acceptable, we show that under mild assump-
tions, fair classifiers can always be derived. Moreover, we prove that compared
to enforcing fairness by post-processing, one can always benefit from exploiting
all available features during training and get better prediction performance while
remaining fair. However, for regression tasks, Equalized Odds is not always at-
tainable if certain conditions on the joint distribution of the features and the target
variable are not met. This indicates the inherent difficulty in achieving fairness in
certain cases and suggests a broader class of prediction methods might be needed
for fairness.
1	Introduction
As machine learning models become widespread in automated decision making systems, apart from
the efficiency and accuracy of the prediction, their potential social consequence also gains increasing
attention. To date, there is ample evidence that machine learning models have resulted in discrimina-
tion against certain groups of individuals under many circumstances, for instance, the discrimination
in ad delivery when searching for names that can be predictive of the race of individual (Sweeney,
2013); the gender discrimination in job-related ads push (Datta et al., 2015); stereotypes associated
with gender in word embeddings (Bolukbasi et al., 2016); the bias against certain ethnicities in the
assessment of recidivism risk (Angwin et al., 2016).
The call for accountability and fairness in machine learning has motivated various (statistical) no-
tions of fairness. The Demographic Parity criterion (Calders et al., 2009) requires the independence
between prediction (e.g., of a classifier) and the protected feature (sensitive attributes of an indi-
vidual, e.g., gender, race). Equalized Odds (Hardt et al., 2016), also known as Error-rate Balance
(Chouldechova, 2017), requires the output of a model be conditionally independent of protected
feature(s) given the ground truth of the target. Predictive Rate Parity (Zafar et al., 2017a), on the
other hand, requires the actually proportion of positives (negatives) in the original data for positive
(negative) predictions should match across groups (well-calibrated).
On the theoretical side, results have been reported regarding relationships among fairness notions.
It has been independently shown that if base rates of true positives differ among groups, then Equal-
ized Odds and Predictive Rate Parity cannot be achieved simultaneously for non-perfect predictors
(Kleinberg et al., 2016; Chouldechova, 2017). Any two out of three among Demographic Parity,
Equalized Odds, and Predictive Rate Parity are incompatible with each other (Barocas et al., 2017).
At the interface of privacy and fairness, the impossibility of achieving both Differential Privacy
(Dwork et al., 2006) and Equal Opportunity (Hardt et al., 2016) while maintaining non-trivial accu-
racy is also established (Cummings et al., 2019).
1
Under review as a conference paper at ICLR 2021
In practice, one can broadly categorize computational procedures to derive a fair predictor into three
types: pre-processing approaches (Calders et al., 2009; Dwork et al., 2012; Zemel et al., 2013;
Zhang et al., 2018; Madras et al., 2018; Creager et al., 2019; Zhao et al., 2020), in-processing ap-
proaches (Kamishima et al., 2011; Perez-SUay et al., 2017; Zafar et al., 2017a;b; Donini et al., 2018;
Song et al., 2019; Mary et al., 2019; Baharlouei et al., 2020), and post-processing approaches (Hardt
et al., 2016; Fish et al., 2016; Dwork et al., 2018). In accord with the fairness notion of interest, a
pre-processing approach first maps the training data to a transformed space to remove discrimina-
tory information between protected featUre and target, and then pass on the data to make prediction.
In direct contrast, a post-processing approach treats the off-the-shelf predictor(s) as Uninterpretable
black-box(es), and imposes fairness by oUtpUtting a fUnction of the original prediction. For in-
processing approaches, varioUs kinds of regUlarization terms are proposed so that one can optimize
the Utility fUnction while sUppressing the discrimination at the same time. Approaches based on
estimating/boUnding caUsal effect between the protected featUre and final target have also been pro-
posed (KUsner et al., 2017; RUssell et al., 2017; Zhang et al., 2017; Nabi & Shpitser, 2018; Zhang
& Bareinboim, 2018; Chiappa, 2019; WU et al., 2019).
FocUsing on the EqUalized-Odds criterion, althoUgh varioUs approaches have been proposed to im-
pose the fairness reqUirement, whether or not it is always attainable is not well addressed. The
attainability of EqUalized Odds, namely, the existence of the predictor that can score zero violation
of fairness in the large sample limit, is an asymptotic property of the fairness criterion. This char-
acterizes a completely different kind of violation of fairness compared to the empirical error boUnd
of discrimination in finite-sample cases. If Utilizing a “fair” predictor which is actUally biased, the
discrimination woUld become a snake in the grass, making it hard to detect and eliminate. ActUally,
as we illUstrate in this paper, EqUalized Odds is not always attainable for regression and even clas-
sification tasks, if we Use deterministic prediction fUnctions. This calls for alternative definitions in
the same spirit as EqUalized Odds that can always be achieved Under varioUs circUmstances. OUr
contribUtions are mainly:
•	For regression and classification tasks with deterministic prediction fUnctions, we show
that EqUalized Odds is not always attainable if certain (rather restrictive) conditions on the
joint distribUtion of the featUres and the target variable are not met.
•	Under mild assUmptions, for binary classification we show that if randomized prediction is
taken into consideration, one can always derive a non-trivial EqUalized Odds classifier.
•	Considering the optimality of performance Under fairness constraint(s), when exploiting
all available featUres, we show that the predictor derived via an in-processing approach
woUld always oUtperform the one derived via a post-processing approach (Unconstrained
optimization followed by a post-processing step).
2	Preliminaries
In this section, we first illUstrate the difference between prediction fairness and procedure fairness,
and then, we present the formal definition of EqUalized Odds (Hardt et al., 2016).
2.1	Hierarchy of Fairness
Before presenting the formUlation of fairness, it is important to see the distinction between different
levels of fairness when discUssing fair predictors. When evalUating the performance of the proposed
fair predictor, it is a common practice to compare the loss (with respect to the Utility fUnction of
choice, e.g., accUracy for binary classification) compUted on target variable and the predicted valUe.
There is an implicit assUmption lying beneath this practice: the generating process of the data, which
is jUst describing a real-world procedUre, is not biased in any sense (Danks & London, 2017). Only
when we treat the target variable (recorded in the dataset) as Unbiased can we jUstify the practice of
loss evalUation and the conditioning on target variable when imposing fairness (as we shall see in
the definition of EqUalized Odds in EqUation 1).
One may consider a mUsic school admission example. The mUsic school committee woUld decide if
they admit a stUdent to the violin performance program based on the applicant’s personal informa-
tion, edUcational backgroUnd, instrUmental performance, and so on. When evalUating whether or not
2
Under review as a conference paper at ICLR 2021
the admission is “fair”, there are actually two levels of fairness. First, based on the information at
hand, did the committee evaluate the qualification of applicants without bias (How committee eval-
uate the applicants)? And second, is committee’s procedure of evaluating applicants’ qualification
reasonable (How other people view the evaluation procedure used by the committee)?
In this paper, we consider prediction fairness, namely, assuming the data recorded is unbiased, the
prediction (made with respect to current reality) itself should not include any biased utilization of
information. The fairness with respect to the data generating procedure as well as the potential future
influence of the prediction are beyond the scope of this paper.
2.2	Equalized-Odds Fairness
Hardt et al. (2016) proposed Equalized Odds which requires conditional independence between
prediction and protected feature(s) given ground truth of the target. Let us denote the protected
feature by A, with domain of value A, additional (observable) feature(s) by X, with domain of value
X , target variable by Y , with domain Y, (not necessarily fair) predictors by Yb , and fair predictors
by Y . Equalized-Odds fairness requires
Y ⊥⊥ A | Y.	(1)
For classification tasks, one can conveniently use the probability distribution form:
∀a ∈A,t,y∈Y :P(Ye =t | A=a,Y =y) =P(Ye =t | Y =y),	(2)
or more concisely, PYe |AY (t | a,y) = PYe |Y (t | y).	(3)
For better readability, we also use the formulation in Equation 3 in cases without ambiguity. In
the context of binary classification (Y = {0, 1}), Equalized Odds requires that the True Positive
Rate (TPR) and False Positive Rate (FPR) of each certain group match population positive rates.
Throughout the paper, without loss of generality we assume there is only one protected feature for
the purpose of simplifying notation. However, considering the fact that the protected feature can be
discrete (e.g., race, gender) or continuous (e.g., the ratio of ethnic group in the population for certain
district of a city), we do not assume discreteness of the protected feature. Due to the space limit, we
will focus on the illustration and implication of our results and defer all the proofs to the appendix.
3	Fairnes s in Regression May Not be Attained
In this section we consider the attainability of Equalized Odds for regression tasks, namely, whether
or not it is possible to find a predictor that is conditionally independent from the protected fea-
ture given true value of the target. For linearly Gaussian cases, one can attain Equalized Odds by
constraining zero partial correlation between the prediction and the protected feature given target
variable (Woodworth et al., 2017). Various regularization terms have also been proposed to suppress
discrimination when predicting a continuous target (Berk et al., 2017; Mary et al., 2019). However,
whether or not one can always achieve 0-discrimination for regression, even if with an unlimited
amount of data, is not clear yet.
If “fair” predictors are deployed without carefully checking the attainability of fairness, the discrim-
ination would become a hidden hazard, making it hard to detect and eliminate. Actually as we will
show in this section, even in the simple setup of linearly correlated continuous data, Equalized Odds
is not always attainable.
3.1	Unattainability of Equalized Odds in Linear non-Gaussian Regression
As stated in Section 2.1, in this paper we consider prediction fairness, and therefore any possible bias
introduced by the data generating procedure itself is beyond the scope of the discussion. Consider
the situation where the data is generated as following (H is not measured in the dataset):
X = qA + EX,
H = bA + EH,	(4)
Y = cX + dH + EY ,
3
Under review as a conference paper at ICLR 2021
where (A, EX, EH, EY ) are mutually independent. In fact, if at most one of EX and E := EY +
dEH is Gaussian, then any linear combination of A and X with non-zero coefficients will not be
conditionally independent from A given Y , meaning that it is not possible to achieve Equalized-Odds
fairness. Let Z be a linear combination of A and X, i.e., Z = αA + βX = (α + qβ)A + βEX,
with linear coefficients α and β, where β 6= 0. In Theorem 3.1, we present the general result in
linear non-Gaussian cases, where one cannot achieve the conditional independence between Z and
A given Y .
Theorem 3.1. (Unattainability of Equalized Odds in the Linear Non-Gaussian Case)
Assume that feature X has a causal influence on Y, i.e., c 6= 0 in Equation 4, and that the protected
feature A and Y are not independent, i.e., qc + bd 6= 0. Assume pEX and pE are positive on R.
Let f1 := log pA, f2 := log pEX, and f3 := logpE. Further assume that f2 and f3 are third-order
differentiable. Then if at most one of EX and E is Gaussian, Z is always conditionally dependent
on A given Y .
From Theorem 3.1, we see that in linear non-Gaussian cases, any non-zero linear combination of
the feature (which is a deterministic function of the input) will not satisfy Equalized Odds. One may
wonder whether Equalized Odds can be achieved by nonlinear regression, instead of a linear model.
Although a proof with general nonlinear models is rather complicated, our simulation results in Sec-
tion 5.1 strongly suggest that the unattainability of Equalized Odds persists in nonlinear regression
cases.
In light of the unattainability of Equalized Odds for prediction with deterministic functions of A
and X, it is desirable to develop general, nonlinear prediction algorithms to produce a probabilistic
prediction (i.e., with a certain type of randomness in the prediction). One possible way follows the
framework of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014): we use random
standard Gaussian noise, in addition to A and X, as input, such that the output will have a specific
type of randomness. The parameters involved are learned by minimizing prediction error and en-
forcing Equalized Odds on the “randomized” output at the same time. Given that this approach is
not essential to illustrate the claims made in this paper and that theoretical properties of such non-
linear regression algorithms with randomized output are not straightforward to establish, this is left
as future work.
4	Fairnes s in Classification
In this section, we consider the attainability of Equalized Odds for binary classifiers (with a deter-
ministic or randomized prediction function), and furthermore, if attainable, the optimality of per-
formance under the fairness criterion. Admittedly, as is already pointed out by Woodworth et al.
(2017), we generally cannot have 0-discriminatory predictors with a finite number of samples; in-
stead, one should consider imposing δ-discrimination in practice (δ is the violation of Equalized
Odds). However, this does not guarantee nor rule out the possibility of attaining 0-discrimination on
population when the sample size goes to infinity.
4.1	Classification with Deterministic Prediction
We begin with considering cases when the classification is performed by a deterministic function of
the input. In particular, we derive the condition under which Equalized Odds can possibly hold true.
Theorem 4.1. Assume that the protected feature A and Y are dependent and that their joint proba-
bility P(A, Y ) (for discrete A) or joint probability density p(A, Y ) (for continuous A) is positive for
every combination of possible values of A and Y . Further assume that Y is not fully determined by
A, and that there are additional features X that are not independent of Y. Let the output of the clas-
(t)
sifier Y be a deterministic function f : A × X → Y. Let SA := {a | ∃x ∈ X s.t. f(a, x) = t}, and
SX(t)|a := {x | f(a, x) = t}. Equalized Odds holds true if and only if the following two conditions
are satisfied:
(i)	∀t ∈ Y : SA(t) = A,
(ii)	∀t ∈ Y , ∀a, a0 ∈ A (for continuous X, replace summation with integration accordingly):
Σx∈s(t) Pχ|AY(X | a,y) = ∑x∈s(t) 0 Pχ|AY(X | a0,y)∙
4
Under review as a conference paper at ICLR 2021
Let us take a look at the two conditions. Condition (i) says that within each class determined by the
classification function f , A should be able to take all possible values in A. While condition (i) is
already pretty restrictive, condition (ii) specifies an even stronger constraint on the relation between
the conditional probability PX|AY (x|a, y) (or the conditional probability density pX|AY (x|a, y) for
continuous X) and the set SX(t)|a (which is determined by the function f). By definition SX(t)|a has
following properties: (1) for any fixed value of a ∈ A, if t = t0, then sgb T sX|： = 0; (2) for any
fixed value of a ∈ A, St∈Y SX(t)|a = X. Condition (ii) says that the set SX(t)|a and the conditional
distribution PX |AY (x|a, y) are coupled in some specific way so that they happen to satisfy the
specified equality.
In special cases when X ⊥⊥ A | Y , if f is a function of only X, condition (ii) would always
hold true. In general situations, if there does not exist any subsets Ka , Ka0 ⊆ X for different
values of a, a0 ∈ A such that Σx∈Ka PX |AY (x|a, y) = Σx∈Ka0 PX |AY (x|a0, y), then condition (ii)
can never hold true (i.e., we cannot find a deterministic function f (A, X) that satisfies Equalized
Odds). Generally speaking, in order to score a better classification accuracy, one would like to make
PYe |A,X (t|a, x) as close as possible to PY |A,X (y|a, x), and if the set SX(t)|a and PX |AY (x|a, y) are
not strictly coupled, condition (ii) would be violated.
4.2	Classification with Randomized Prediction
In this section, we consider cases when randomized prediction is acceptable, namely, the classifier
would output class labels with certain probabilities. We first derive the relation between positive
rates (TPR and FPR) of binary classifiers before and after the post-processing step, i.e., Yopt (the
unconstrainedly optimized classifier) and Ypost (the fair classifier derived by post-processing Yopt),
and show that under mild assumptions, one can always derive a non-trivial Equalized-Odds (on
population level) Ypost via a post-processing step. Then, from the ROC feasible area perspective, we
prove that post-processing approaches are actually equivalent to in-processing approaches but with
additional “pseudo” constraints enforced. Therefore, using the same loss function, post-processing
approaches can perform no better than in-processing approaches.
4.2	. 1 The Post-processing Step
The post-processing step of a predictor Yb (here we drop the subscript if without ambiguity) only
utilizes the information in the joint distribution (A, Y, Y ). A fair predictor Ypost derived via a post-
processing step, for instance, the shifted decision boundary (Fish et al., 2016), the derived predictor
(Hardt et al., 2016), or the (monotonic) joint loss optimization over decoupled classifiers (Dwork
et al., 2018), is then fully specified by a (possibly randomized) function of (A, Y ). This implies
the conditional independence Ypost ⊥⊥ Y | A, Y . Since we can denote the positive rates of Y as
PYb |AY (1|a, y)* 1, positive rates of Y (here we drop the subscript for readability) as PYe |AY (1|a, y),
the relation between positive rates of binary classifiers before and after a post-processing step would
satisfy (for every a ∈ A, u, y ∈ Y):
PYe |AY (1|a, y) = Σu∈Y βa(u)PYb |AY (u|a, y), where βa(u) := P (Y = 1 | A = a, Y = u).	(5)
Notice that Equation 5 is just a factorization of probability under the conditional independence
(between Ypost and Y given A and Y ). Therefore, post-processing an existing predictor boils down
to optimizing parameters (for discrete A) or functions (for continuous A) βa(u).
4.2.2 ROC Feasible Area
On the Receiver Operator Characteristic (ROC) plane, a two-dimensional plane with horizontal
axis denoting FPR and vertical axis denoting
TPR, the performance of any binary predictor Yb (not
T _	一一_ _	,	.	、	_ , ^	,	一	、	___ _	,	.	_ , ^	.
1Recall that PYb |AY (u|a, y) = P(Y = u | A = a, Y = y). When u = 1, PYb |AY (1|a, y) = P(Y = 1 |
Λ	∖	,	C- C∙ TTrl	C 7-t	∕cl	\	7-* ∕τ^	C I A	∖ Γ	\
A = a, Y = y) represents positive rates of Y ; When u = 0, PYb |AY (0|a, y) = P(Y = 0 | A = a, Y = y)
represents positive rates of 1 - Y (the classifier that flips the prediction of Y ).
5
Under review as a conference paper at ICLR 2021

0.1
0.0
)1= Y,A|1= Y(P
0.1 0.0
)1= Y,A|1=eY(P
Ω(∣epo Ω(Ypost)
A	A = 0: Yopt
A	A =1: Yt
--
Y	Yn
--
Y	Ypost
Ω(Yn)
0 8 6 4 2
. . . . .
Ioooo
(I = '.ɪI=fcJ
0.0.00	0.25	0.50	0.75	1.00
P(Y = 1 | A,Y = 0)
0.(
0 8 6 4 2
. . . . .
Ioooo
(I = XE I= X)J
.00	0.25 0.50 0.75	1.00
P(Y = 1 | A,Y = 0)
(a) Discrete protected feature (b) Continuous protected feature
*、
(c) Feasible areas for Yn , Yplost and Ypt (d) Ω ( Y ： ) with “pseudo constraints




Figure 1: ROC feasible area illustrations. Panels (a)-(b): Attainability of Equalized Odds for bi-
nary classifiers with discrete or continuous protected feature. Panels (c)-(d): ROC feasible areas
,入、
comparison between Ω(Y⅛), Ω(Yp∩st), Ω(Y0pt), and Ω(YJ*).
•1 C∙ ∙	∖	∙ ,1	, ∙	1 C∙ .	. A 1~	. Λ	1	∙	∕C∖
necessarily a fair one) with certain value of protected feature A = a corresponds to a point γa(Y )
ZT--T-AT-. EcC、	.1	1	ɪʌ	,	1	T	T	.1	1	∕C∖
(FPR, TPR) on the plane. Denote each coordinate according to the value of Y as γay(Y ):
Ya(Y) = (YaO(Y), YaI(Y)) := (PY∣Aγ (I |a, 0), PY∣Aγ (I |a, I)).	,
(6)
^
^
Further denote the corresponding convex hull of Y on the ROC plane as Ca (Y) using vertices:
Ca(Yb) := convhull (0,0),Ya(Yb),Ya(1-Yb),(1,1) ,
(7)
and then, as already stated in Hardt et al. (2016), the (FPR, TPR) pair corresponding to a post-
processing predictor falls within (including the boundary of) Ca (Y).
Definition 4.1. (ROC feasible area) The feasible area of a predictor Ω(Y), specified by the
hypothesis space of available predictors Yb , is the set containing all attainable (FPR, TPR) pairs by
the predictor on the ROC plane satisfying Equalized Odds.
In Hardt et al. (2016) itis proposed that the post-processing fair predictor can be derived by solving a
linear programming problem on the ROC plane. However, itis not clearly stated whether or not such
problem always has a non-trivial solution. Following Hardt et al. (2016), we analyze the relation
between the (FPR, TPR) pair of predictors on the ROC plane and formally establish the existence
of the non-trivial Equalized-Odds predictor. Actually as we shall see in Theorem 4.2, under mild
assumptions, an Equalized-Odds predictor Ypost derived via post-processing Y (a predictor optimized
without fairness concern) always has non-empty ROC feasible area.
Theorem 4.2.	(Attainability of Equalized Odds)
Assume that the feature X is not independent from Y, and that Y is a function of A and X. Then for
binary classification, if Yb is a non-trivial predictor for Y, there is always at least one non-trivial
(possibly randomized) predictor Ypost derived by post-processing Y that can attain Equalized Odds:
Ω(Y‰) = 0.
Here Yepost is a possibly randomized function of only A and Yb , trading off TPR with FPR across
groups with different value of protected feature. From the panels (a) and (b) of Figure 1 we can
^


also see that Ω(Yp∩st), the ROC feasible area of Yp°st, is the intersection of Ωa(Y), indicating that
although Equalized Odds is attained, the performance of Yepost is always worse than the weakest
performance across different groups, which is obviously suboptimal.
4.2.3 Optimality of Performance among Fair Classifiers
In this subsection we discuss the optimality of performance of fair classifiers derived via different
approaches. Considering the fact that recent efforts to impose Equalized Odds in the pre-processing
manner (Madras et al., 2018; Zhao et al., 2020) approach the problem from a representation learning
perspective, where the main focus is to learn fair representations that at the same time preserve
sufficient information from the original data, we omit pre-processing approaches from the discussion
and compare the performance of post-processing and in-processing fair classifiers.
6
Under review as a conference paper at ICLR 2021
=zCH 3、--esu3≈"∙、
-1.0	-0.5	0.0	0.5	1.0
A given that Y ∈ [-0.2,0]
(a) Linear w/ Laplace distribution
=701-3、--esu3≈"∙4
-0.4	-0.2	0.0	0.2
A given that Y ∈ [-0.2,0]
(b) Linear w/ Uniform distribution
=ZCH W A ae-u3>a4
-1.0	-0.5	0.0	0.5	1.0
A given that Y ∈ [-0.2,0]
(c) Nonlinear w/ Gaussian distribution
502500255015
Oo O O O O
- - -
,1 - W AU3>4
-1.0	-0.5	0.0	0.5	1.0
A given that Y ∈ [-0.2, 0]
(d) Nonlinear w/ Gaussian distribution

Figure 2: Illustration of unattainable Equalized Odds for regression tasks. Panel (a)-(b): Linear re-
gression on the data generated with linear transformations and non-Gaussian distributed exogenous
terms (following Laplace, Uniform distribution respectively). Panel (c)-(d): Nonlinear regression
with a neural net regressor (Mary et al., 2019) on the data generated with nonlinear transforma-
tions and Gaussian exogenous terms. We can observe obvious dependencies between Y and A on a
small interval of Y . This indicates the conditional dependency between Y and A given Y , i.e., the
Equalized Odds is not achieved.
Admittedly, when only the information about joint distribution of (A, Y, Y ) is available, post-
processing is the best we can do. However, this is not the case when we have access to additional
available features during training. For any predictor specified by parameters θ ∈ Θ, the derivation
of the in-processing fair predictor Yin and the unconstrained statistical optimal predictor Yopt take
following forms respectively:
m∈in	E[l (f(A,X； θ),Y)]
s.t.	PYein|AY(t | a,y) = PYein|Y(t | y) (8)
where Yein = f(A, X; θ),
min E[l (f(A, X; θ), Y)]
θ∈Θ	(9)
where Ybopt = f(A, X; θ).

It is natural to wonder, now that one can always directly solve for Yin from Equation 8, how is it
related to Ypost, which is derived by post-processing the Yopt solved from Equation 9? Interestingly,
although Yin and Ypost are solved separately using different constrained optimization schemes, one
can draw a connection between them by utilizing Yopt as a bridge and reason about the relation
between their ROC feasible areas Ω(Yn) and Ω(%ost), as We summarize in the following theorem.
Theorem 4.3.	(Equivalence between ROC feasible areas)
Let Ω(Ypost) denote the ROC feasible area specified by the constraints enforced on Ypos. Then
Ω(Ypost) is identical to the ROC feasible area Ω(Yi*) that is specified by the following set of con-
straints:
(i)	constraints enforced on Yin ;
(ii)	additional “pseudo” constraints: ∀a ∈ A, βa(00) = βa(01), βa(10) = βa(11), where
βa(uy) = Σx∈X P (Yein = 1 | A = a, X = x)P (X = x | A = a, Y = y, Ybopt = u).
As we can see from panels (c) and (d) of Figure 1, if the additional “pseudo” constraints are intro-
duced when optimizing Y；, we have Ω(Yn) ⊇ Ω(Ypost) = Ω(Izi*). The ROC feasible area is fully
specified by the hypothesis class and the fairness constraint. Therefore, with the same objective
function and fairness constraint, the fair classifier derived from an in-processing approach always
outperforms the one derived from a post-processing approach. We can see that when we have ac-
cess to additional features and choose a post-processing approach, we lose performance (compared
to Yin) by unintentionally introducing “pseudo” constraints during optimization. These “pseudo”
constraints actually offset the benefit of utilizing additional features (in the hope to score a better
performance while remaining fair).
5	Experiments
In order to intuitively illustrate the claims, we provide numerical results for various settings. We first
present the result for (linear non-Gaussian and nonlinear) regression tasks when Equalized Odds is
not attained. We demonstrate the dependence between the prediction and the protected feature given
true value of the target variable. Then for classification tasks we compare the performance of several
7
Under review as a conference paper at ICLR 2021
0 18 Adult (Gender)
017 ”
0.16
0.15	∙	▼
0.14
0.00 0.02 0.04 0.06 0.08
EOdds Violation
0 18 Adult (Race)
0.17
0.16
0.15	∙	▼
0.14
0.00 0.04 0.08 0.12
EOdds Violation
Bank
M
0.22
0.20
* ÷ ▼
0.18
0.00 0.02 0.04 0.06 0.08 0.10
EOdds Violation
COMPAS
0.38 能
0.36
0.34	W*
0.32	▼
0.00 0.05 0.10 0.15 0.20
EOdds Violation
0 24	German Credit
0.23 .
0.22	∙ +
0.21	▼
0.20
0.00 0.05 0.10 0.15 0.20 0.25
EOdds Violation
▼ Logistic Regression ∙ BaharIoUei et al. 2020	■ Rezaei et al. 2020	∙ AgarwaIet al. 2018	+ Zafar et al. 2017a K Hardt et al. 2016
0 19 Adult (Gender)
,0.18	X
0 0.17	♦
LU
3 0.16	4
0.15	∙
0.14
0.00 0.02 0.04 0.06 0.08
EOPPo Violation
0 18 Adult (Race)
0.17
0.16	♦
0.15	∙	▼
0.14
0.00 0.04 0.08 0.12
EOppo Violation
Bank
*
0.22
.
0.20
•» ÷ ▼
0.18
0.00 0.02 0.04 0.06 0.08 0.10
EOppo Violation
COMPAS
0.38
0.36
♦ +
0.34 ∙
▼
0.32
0.00 0.05 0.10 0.15 0.20
EOppo Violation
0 24 German Credit
0.23
.
0.22	⅛ +
■ ▼
0.21
0.00 0.04 0.08 0.12 0.16
EOppo Violation
▼ Logistic Regression ∙	BaharloUeietal. 2020	■ Rezaeietal. 2020	∙ Agarwaletal. 2018	+ Zafaretal. 2017a H Hardtetal. 2016 D Doninietal. 2018
Figure 3: Results for classification with Equalized Odds/Equal Opportunity criterion.
existing methods in the literature on the Adult, Bank, COMPAS, and German Credit data sets. The
detailed description of the data sets is available in the appendix.
5.1	Regression with Linear Non-Gaussian and Nonlinear Data
In Section 3.1 we showed the unattainability of Equalized Odds for regression with linear non-
Gaussian data. Although a proof for similar results in nonlinear cases does not seem straightforward,
as strongly suggested by our numerical illustrations, the unattainability of Equalized Odds persists
in nonlinear regression cases. In Figure 2 we present scatter plots of Y versus A for Y in a small
(compared to its support) interval, for linear non-Gaussian as well as nonlinear regression cases.
For linear cases, the data is generated as stated in Equation 4, with non-Gaussian distributed exoge-
nous terms (EX, EH, and EY). We use linear regression with the Equalized Correlations constraint
(Woodworth et al., 2017), a weaker notion of Equalized Odds for linearly correlated data, as the
predictor. For nonlinear cases, the data is generated using a similar scheme but with nonlinear
transformations (e.g., combinations of sin(∙), log(∙), and polynomials) and Gaussian distributed ex-
ogenous terms. We use a neural net regressor with an Equalized Odds regularization term (Mary
et al., 2019) to perform nonlinear fair regression. As we can see in Figure 2, for nonlinear regression
tasks, Equalized Odds may not be attained even if every exogenous term is Gaussian distributed.
5.2	Fair Classification
In Figure 3, we compare the performance under Equalized Odds of multiple methods proposed in the
literature. Hardt et al. (2016) propose a post-processing approach where the prediction is random-
ized to minimize violation of fairness; Zafar et al. (2017a) use a covariance proxy measure as the
regularization term when optimizing classification accuracy; Agarwal et al. (2018) take the reduc-
tions approach and reduce fair classification into solving a sequence of cost-sensitive classification
problems; Rezaei et al. (2020) minimize the worst-case log loss using an approximated regular-
ization term; Baharlouei et al. (2020) propose to use Renyi correlation as the regularization term
to account for nonlinear dependence between variables. To measure the violation of the fairness
criterion, we use Equalized Odds (EOdds) violation, defined as maxy∈Y a,a0∈A PYe |AY (1|a, y) -
PYe |AY (1|a0, y). Following Agarwal et al. (2018), we pick 0.01 as the default violation bound that
the EOdds violation does not exceed (if practically achievable for the method) during training. For
each method we plot the testing accuracy versus the violation of Equalized Odds. Although a prob-
abilistic classification model is used across each method (here is logistic regression), ifan algorithm
output the class label where the prediction likelihood is maximized, the prediction is in essence
performed by a deterministic function of input features (e.g., Rezaei et al. (2020); Baharlouei et al.
(2020)). As we have shown in Section 4.1, for classification with a deterministic function, in general
cases the conditions specified in Theorem 4.1 are easily violated, i.e., Equalized Odds may not be
8
Under review as a conference paper at ICLR 2021
attained even if there is an unlimited amount of data. Therefore, although here we are considering
finite data cases, we can still anticipate a lower level of fairness violation with a randomized pre-
diction. This is validated by the numerical experiment: while the approach by Hardt et al. (2016)
does not score the lowest test error, the violation of Equalized Odds is the lowest compared to other
approaches. The benefit of introducing randomization can also be witnessed by the Pareto frontier
presented in Agarwal et al. (2018), where the approach can potentially achieve any desired fairness-
accuracy trade-off between that of the post-processing approach and that of the unconstrainedly
optimized classifier2. In some scenarios people tend to only care about equal TPR (e.g., the rate
of acceptance/admission) across groups, i.e., the Equal Opportunity (Hardt et al., 2016) notion of
fairness. The related numerical result on real-world data sets is also presented.
6 Conclusion and Future Work
In this paper, we focus on the Equalized-Odds criterion and consider the attainability of fairness,
and furthermore, if attainable, the optimality of the prediction performance under various settings.
We first show that, for fair regression, one can only achieve Equalized Odds when certain conditions
on the joint distribution of the features and the target variable are met. Then for classification tasks
with deterministic classifiers, we give the condition under which Equalized Odds can hold true; we
also show that under mild assumptions, one can always find a non-trivial Equalized-Odds (random-
ized) predictor, even with a continuous protected feature; in terms of the optimality of performance,
one can always (if conditions permit) benefit from exploiting all available features during training.
Future work would naturally consider nonlinear regression algorithms with randomized output and
fairness guarantees, and the attainability of more fine-grained (compared to group fairness) criteria
of fairness (e.g., individual fairness) as well as the procedure fairness in the fairness hierarchy.
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A re-
ductions approach to fair classification. In International Conference on Machine Learning, pp.
60-69, 2018.
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias: There’s software used
across the country to predict future criminals, and it’s biased against blacks. ProPublica, 2016.
Sina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn. Renyi fair inference.
In International Conference on Learning Representations, 2020.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. NIPS Tutorial,
2017.
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgen-
stern, Seth Neel, and Aaron Roth. A convex framework for fair regression. arXiv preprint
arXiv:1706.02409, 2017.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing systems, pp. 4349-4357, 2016.
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifiers with independency
constraints. In 2009 IEEE International Conference on Data Mining Workshops, pp. 13-18.
IEEE, 2009.
Silvia Chiappa. Path-specific counterfactual fairness. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pp. 7801-7808, 2019.
2In the approach proposed by Agarwal et al. (2018), the randomization can come in two folds: the first
kind of randomization comes from picking a classifier from the distribution of multiple available classifiers;
the second kind of randomization comes from the probabilistic prediction (if the hypothesis class contains
probabilistic prediction models).
9
Under review as a conference paper at ICLR 2021
Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big data, 5(2):153-163, 2017.
Elliot Creager, David Madras, Joern-Henrik Jacobsen, Marissa Weis, Kevin Swersky, Toniann
Pitassi, and Richard Zemel. Flexibly fair representation learning by disentanglement. In In-
ternational Conference on Machine Learning, pp. 1436-1445, 2019.
Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern. On the compatibil-
ity of privacy and fairness. In Adjunct Publication of the 27th Conference on User Modeling,
Adaptation and Personalization, pp. 309-315, 2019.
David Danks and Alex John London. Algorithmic bias in autonomous systems. In IJCAI, pp.
4691-4697, 2017.
Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiments on ad privacy
settings: A tale of opacity, choice, and discrimination. Proceedings on privacy enhancing tech-
nologies, 2015(1):92-112, 2015.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In Advances in Neural Information Process-
ing Systems, pp. 2791-2801, 2018.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226, 2012.
Cynthia Dwork, Nicole Immorlica, Adam Tauman Kalai, and Max Leiserson. Decoupled classifiers
for group-fair and efficient machine learning. In Conference on Fairness, Accountability and
Transparency, pp. 119-133, 2018.
Benjamin Fish, Jeremy Kun, and Adam D Lelkes. A confidence-based approach for balancing fair-
ness and accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining,
pp. 144-152. SIAM, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In
Advances in Neural Information Processing Systems, pp. 3315-3323, 2016.
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regular-
ization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp.
643-650. IEEE, 2011.
Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-offs in the fair determi-
nation of risk scores. arXiv preprint arXiv:1609.05807, 2016.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances
in Neural Information Processing Systems, pp. 4066-4076, 2017.
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. arXiv preprint arXiv:1802.06309, 2018.
Jeremie Mary, Clement Calauzenes, and Noureddine El Karoui. Fairness-aware learning for contin-
uous attributes and treatments. In International Conference on Machine Learning, pp. 4382-4391,
2019.
Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Thirty-Second AAAI Conference on
Artificial Intelligence, 2018.
10
Under review as a conference paper at ICLR 2021
Adrian Perez-Suay, Valero Laparra, Gonzalo Mateo-Garcla, Jordi Mufioz-Marl, LUis Gomez-Chova,
and Gustau Camps-Valls. Fair kernel learning. In Joint European Conference on Machine Learn-
ing and Knowledge DiscOvery in Databases, pp. 339-355. Springer, 2017.
Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian Ziebart. Fairness for robust log loss
classification. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.
Chris Russell, Matt J Kusner, Joshua Loftus, and Ricardo Silva. When worlds collide: integrating
different counterfactual assumptions in fairness. In Advances in Neural Information Processing
Systems, pp. 6414-6423, 2017.
Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 2164-2173, 2019.
Latanya Sweeney. Discrimination in online ad delivery. Queue, 11(3):10-29, 2013.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-
discriminatory predictors. In Conference on Learning Theory, pp. 1920-1953, 2017.
Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. Pc-fairness: A unified framework for
measuring causality-based fairness. In Advances in Neural Information Processing Systems, pp.
3399-3409, 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-
1180, 2017a.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp.
962-970, 2017b.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
335-340, 2018.
Junzhe Zhang and Elias Bareinboim. Fairness in decision-making—the causal explanation formula.
In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Lu Zhang, Yongkai Wu, and Xintao Wu. A causal framework for discovering and removing di-
rect and indirect discrimination. In Proceedings of the 26th International Joint Conference on
Artificial Intelligence, pp. 3929-3935, 2017.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J. Gordon. Conditional learning of fair
representations. In International Conference on Learning Representations, 2020.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Proof for Theorem 3.1
To prove the unattainability of Equalized Odds in regression, we will need the following lemma,
which provides a way to characterize conditional independence/dependence with conditional or joint
distributions.
Lemma A.1. Variables V1 and V2 are conditionally independent given variable V3 if and only if
there exist functions h(v1, v3) and g(v2, v3) such that
PV1,V2∣V3 (v1,v2 | v3) = h(v1,v3) ∙ g(v2, v3).	(IO)
Proof. First, if V1 and V2 are conditionally independent given variable V3, then Equation 10 holds:
PV1,V2∣V3 (V1,v2 | v3) = PV1∣V3 (VII v3) ∙ PV2∣V3 (V2 | v3).
∙-v
We then let h(v3) := / h(v1,v3)dv1 and g(v3) ：= / g(v2,v3)dv2. Take the integral of Equation 10
w.r.t. V1 and V2, we have:
∙-v
PV2∣V3(V2 I V3)= h(V3)∙ g(v2,V3),
PVι ∣V3 (vi I V3) = g(v3) ∙ h(vi,V3),
respectively. Bearing in mind Equation 10, one can see that the product of the two equations above
is
∙-v
PV2∣V3 (V2 I V3)∙ PV1∣V3 (V1 I V3)= h(v3)∙ g(v2,V3) ∙ g(v3) ∙ h(v2,V3)
∙-v
=h(v3) ∙ g(v3) ∙ PV1,V2∣V3 (V1,V2 I V3).
∙-v
Take the integral of the equation above w.r.t. vι and v gives h(v3) ∙ g(v3) ≡ 1. The above equation
then reduces to
PV2∣V3 (v2 i v3) ∙ PV1∣V3 (v1 i v3) = PVi ,V2∣V3 (v1,v2 i v3).
That is, Vi and V2 are conditionally independent given V3.	□
Now we are ready to prove the unattainability of Equalized Odds in linear non-Gaussian regression:
Theorem. (Unattainability of Equalized Odds in the Linear Non-Gaussian Case)
Assume that feature X has a causal influence on Y, i.e., c 6= 0 in Equation 4, and that the protected
feature A and Y are not independent, i.e., qc + bd 6= 0. Assume pEX and pE are positive on R.
Let fi := log pA, f2 := log pEX, and f3 := logpE. Further assume that f2 and f3 are third-order
differentiable. Then if at most one of EX and E is Gaussian, Z is always conditionally dependent
on A given Y .
Proof. According to Equation 4, we have
0
β
c
0
0
1
A1
Z	= α + qβ
Y qc + bd
A
EX
E
(11)
The determinant of the above linear transformation is β, which relates the probability density func-
tion of the variables on the LHS and that of the variables on the RHS of the equation. Therefore,
according to Equation 11, we can rewrite the joint probability density function by making use of
the Jacobian determinant and factor the joint density into marginal density functions (A, EX , E are
mutually independent according to the data generating process). Further let
α + qβ	cα	c
α :=---,r := bd -, and C :=—.
β	ββ
Then we have EX = β1 Z - αA, E = Y - rA - cZ, and
(12)
PA,Z,Y (a, z, y) = PA,Eχ ,E (a, ex, e)/Iei
=iβiPA(a)PEχ Iex)PE(e)
=5Pa(O)PEx (βZ - αa)pE(y - ra - Cz).
12
Under review as a conference paper at ICLR 2021
On its support, the log-density can be written as
J := log pA,Z,Y (a, z,y)
log pa (a) + log PEX (1 Z
β
—αa) + log PE (y - ra — cz) — log∣β∣
(13)
f1(a) + f2( 飞 Z
β
-αa) + f3(y - ra - cz) - log ∣β∣.
According to Lemma A.1, A ⊥⊥ Z | Y if and only if PA,Z|Y (a, Z | y) is a product of a function
of a and y and a function of Z and y. PA,Z,Y (a, Z, y) is further a product of the above function
and a function of only y. This property, under the conditions in Theorem 3.1, is equivalent to the
constraint
∂ 2J
∂A∂Z
According to Equation 13, we have
dJ = 1 ∙ f2(1Z - αa) - C ∙ f3(y - ra - CZ)
∂Z β β
∂2 J	αc	1
⇒ ∂-∂- = -β ∙ f2(βz - αa) + rc ∙ f3(y - Ta - cz).
aZ	β	β
Combining Equations 14 and 15 gives
αc	1
TC ∙ f3(y - ra - CZ) = - ∙ f2(-Z - aa).
ββ
(14)
(15)
(16)
Further taking the partial derivative of both sides of the above equation w.r.t. y yields
TC ∙ f30(y - Ta - CZ) ≡ 0.	(17)
There are three possible situations where the above equation holds:
(i)	CC = 0, which is equivalent to C = 0 and contradicts with the theorem assumption.
(ii)	T = 0. Then according to Equation 16, We have β ∙ f22( ɪ Z - aa) ≡ 0, implies either α = 0
or f2( 1Z - aa) ≡ 0. If the latter is the case, then f2 is a linear function and, accordingly,
exp(f2 ) is not integrable and does not correspond to any valid density function. If the
former is true, i.e., αC = 0, then according to Equation 12, We have α = -qβ, Which further
implies T = bd - cα = bd + qc. Therefore, in this situation, bd + qc = 0, which again
contradicts With the theorem assumption.
(iii)	f3000(y - TCa - CCZ) ≡ 0. That is, f3 is a quadratic function with a nonzero coefficient for
the quadratic term (otherwise f3 does not correspond to the logarithm of any valid density
function). Thus E follows a Gaussian distribution.
Only situation (iii) is possible, i.e., TCCC 6= 0 and E follows a Gaussian distribution. This further tells
us that the RHS of Equation 16 is a nonzero constant. Hence f2 is a quadratic function and EX also
follows a Gaussian distribution. Therefore if A ⊥⊥ Z | Y were to be true, then EX and E are both
Gaussian. Its contrapositive gives the conclusion of this theorem.	口
Corollary. Suppose that both EX and E are Gaussian, with variances σE2	and σE2 , respectively.
(The protected feature A is not necessarily Gaussian.) Then Z ⊥⊥ A | Y if and only if
α _ bdc ∙ σEX - q ∙睚
——------：----------—
β	c2 ∙ σE X + σE
(18)
Proof. Under the condition that EX and E are Gaussian, their log-density functions are third-order
differentiable. Then according to the proof of Theorem 3.1, the Equalized Odds condition A⊥⊥Z | Y
is equivalent to Equation 16, which, together with Equation 12 as well as the fact that f2 = -^1-
σEX
and f =, yields Equation 18.	口
E
13
Under review as a conference paper at ICLR 2021
A.2 Proof for Theorem 4.1
Theorem. Assume that the protected feature A and Y are dependent and that their joint probability
P(A, Y ) (for discrete A) or joint probability density p(A, Y ) (for continuous A) is positive for every
combination of possible values of A and Y. Further assume that Y is not fully determined by A, and
that there are additional features X that are not independent of Y . Let the output of the classifier
Ye be a deterministic function f : A × X → Y. Let SA(t) := {a | ∃x ∈ X s.t. f (a, x) = t}, and
SX(t)|a := {x | f(a, x) = t}. Equalized Odds holds true if and only if the following two conditions
are satisfied:
(i)	∀t ∈ Y : SA(t) = A,
(ii)	∀t ∈ Y , ∀a, a0 ∈ A, a 6= a0 :
Σ P(X = x | A = a, Y = y) = Σ	P(X = x | A = a0, Y = y).
x∈S(t)a	x∈SX)a0
Proof. We begin by considering the case when A and X are discrete (for the purpose of readability).
The Equalized Odds criterion can be written in terms of the conditional probabilities:
∀a ∈ A, t,y ∈ Y : P(Ye = t | A = a,Y = y) = P(Ye =t | Y = y).	(19)
Expand the LHS of Equation 19:
P (Ye = t | A = a, Y = y) = Σ P (Ye = t | A = a, X = x, Y = y)P (X = x | A = a, Y = y),
x∈X
and bear in mind that Y := f(A, X) is a deterministic function of (A, X), we have:
P(f (A, X)= t | A = a,X = x,Y = y) = P(f (A, X)= t | A = a,X = x) ∈{0,1}.	(20)
From Equation 20 we can see that the conditional probability P(X = x | A = a, Y = y) can
contribute to the summation only when f(a, x) = t. We can rewrite the LHS of Equation 19:
P(Ye = t | A = a, Y = y) = Σ P(X = x | A = a, Y = y) := Q(t) (a, y).
X∈SX)a
Similarly, for the RHS of Equation 19, we have:
P(Ye = t | Y = y) = Σ Σ P(Ye =t | A = a,X = x,Y = y)P (X = x,A = a | Y = y)
a∈Ax∈X
= Σ Σ P(X =x | A= a,Y=y)P(A= a | Y=y)
a∈SA)x∈SX)a
= Σ Q(t)(a, y)P (A = a | Y = y).
a∈SA(t)
Since Equalized Odds holds true if and only if Equation 19 holds true, then the LHS of the equation
does not involve a (as is the case for the RHS), i.e., Q(t) (a, y) does not change with a. Then Equation
19 becomes:
Q(t) (a, y) = a∈ΣS Q(t) (a, y)P(A = a | Y = y) = Q(t)(a, y)a∈ΣS P(A = a | Y = y),
which gives the condition (i) that SA(t) contains all possible values of A, i.e., A = SA(t) (otherwise
Σ S(t) P (A = a | Y = y) < 1). Since Q(t)(a, y) does not change with a, we have:
a∈SA
∀a, a0 ∈ A, a 6= a0 : Σ P(X = x | A = a, Y = y) = Σ	P(X = x | A = a0, Y = y),
x∈sX)a	x∈SX)a0
which gives the condition (ii). Therefore, Equalized Odds implies conditions (i) and (ii). On the
other hand, it is easy to see that when conditions (i) and (ii) are satisfied, Equation 19 holds true,
i.e., Equalized Odds holds true.
When A and X are continuous, one can replace the summation with integration accordingly. 口
14
Under review as a conference paper at ICLR 2021
A.3 Proof for Theorem 4.2
Theorem. (Attainability of Equalized Odds)
Assume that the feature X is not independent from Y, and that Y is a function of A and X. Then for
binary classification, if Yb is a non-trivial predictor for Y , there is always at least one non-trivial
predictor Ypost derived by post-processing Y that can attain Equalized Odds, i.e.,
Ω(Ypost) = 0.
Proof. Since Yb is a function of (A, X) and X ⊥6⊥ Y, Yb is
not conditionally independent from Y
given protected feature A. Furthermore, since Y is a non-trivial estimator of the binary target Y,
there exists a positive constant > 0, such that:
P (Yb = 1 | A = a,Y = 1) - P(Yb = 1 | A = a,Y = 0) ≥ , ∀a ∈ A.	(21)
Equation 21 implies that for each value of A, the corresponding true positive rate of the non-trivial
predictor is always strictly larger than its false positive rate3. As illustrated in panels (a) and (b) of
Figure 1, the (FPR, TPR) pair of the predictor Y when A = a, i.e., the point γa (Y) on ROC plane,
will never fall in the gray shaded area, and its coordinates are bounded away from the diagonal by at
1	. EI	C	,1	•	.	,♦	CIlC /-τ≥∖	11 ι	C	Ill	∙.1	.
least . Therefore, the intersection of all Ca (Y) would always form a parallelogram with non-empty
area, which corresponds to attainable non-trivial post-processing fair predictors YPost.	□
A.4 Proof for Theorem 4.3
Theorem. (Equivalence between ROC feasible areas)
Let Ω(Ypost) denote the ROC feasible area specified by the constraints enforced on Ypos. Then
Ω(Ypost) is identical to the ROC feasible area Ω(Yi*) that is specified by the following set ofcon-
straints:
(i)	constraints enforced on Yin ;
(ii)	additional “pseudo” constraints: ∀a ∈ A, βa(00) = βa(01), βa(10) = βa(11), where
βa(uy) = Σx∈X P (Yein = 1 | A = a, X = x)P (X = x | A = a, Y = y, Ybopt = u).
Proof. Since the post-processing predictor Ypost is derived by optimizing over parameters or
functions (of A) βa(u). Therefore, considering the fact that PYe |AY (1|a, y) = γay(Yepost),
PYbopt|AY (1|a, y) = γay(Yopt), we have the relation between γay(Ypost) and γay (Yopt):
γay (Ypost) = βa(0) γay (1 - Yopt) + βa(1) γay (Yopt),
βa(0) = P (Yepost = 1 | A = a, Ybopt = 0),	(22)
βa(1) = P (Yepost = 1 | A = a, Ybopt = 1).


Similarly, consider the relation between positive rates of Yin and those of Yopt, i.e., PYe |AY (1|a, y)
Λ TΛ	/Tl	∖	1	∕'	.	∙	∙	7~»	/Tl	∖	”	1 分
and PYbopt|AY (1|a, y), by factorizing PYein|AY (1|a, y) over X andYopt:
PYein|AY (1|a, y) =	PYein|AX(1|a, x)PX|AYYbopt(x|a, y, u)PYbopt|AY (u|a, y).	(23)
u∈Y x∈X
Therefore, we have the relation between γay(Yin) and γay(Yopt):
γay (Yin)	=	βa(0y) γay (1 - Yopt) + βa( y) γay (Yopt),
βa(0y)	=	Σx∈X P (Yein = 1 | A = a, X = x)P (X	=	x	|	A	=	a, Y	=	y, Ybopt	=	0),	(24)
βa(1y)	=	Σx∈X P (Yein = 1 | A = a, X = x)P (X	=	x	|	A	=	a, Y	=	y, Ybopt	=	1).
3If the TPR	of	the predictor is always smaller than its FPR, one	can	simply flip	the prediction (since	the
target is binary) and then Equation 21 holds true.
15
Under review as a conference paper at ICLR 2021
If there is more than one variable in X in Equation 24, one can expand the summation if needed; if
some variables are continuous, one may also substitute the summation with integration accordingly.
From Equation 24, βa(0y) and βa(1y) depend on the value of Y :
βa(0y) = P (Yein = 1 | A = a, Y = y, Ybopt = 1),
βa(0y) = P (Yein = 1 | A = a, Y = y, Ybopt = 0).
(25)
Apart from Equalized Odds constraints (which are shared by Yin and Ypost), when enforcing addi-
tional “pseudo” constraints βa(00) = βa(01) and βa(10) = βa(11), conditional independence Yein⊥⊥Y | A, Ybopt
is enforced, making βa(0y) and βa(0y) no longer depend on Y . This is exactly the inherent constraint
Ypost satisfies. Therefore the stated equivalence between ROC feasible areas Ω(Yp°st) (specified by
the constraints enforced on Ypost) and Ω(Yi*) (specified by the constraints enforced on Yin together
with the additional “pseudo” constraints) hold true.	口
A.5 Description of the Data Sets
(1)	Adult4 : The UCI Adult data set contains 14 features for 45,222 individuals (32,561 sam-
ples for training and 12,661 samples for testing). The census information includes gender,
marital status, education, capital gain, etc. The classification task is to predict whether a
person’s annual income exceeds 50,000 USD. We use the provided testing set for evalua-
tions and present the result with gender and race (consider white and black people only) set
as the protected feature respectively.
(2)	Bank5: The UCI Bank Marketing data set is related with marketing campaigns of a banking
institution, containing 16 features of 45,211 individuals. The assigned classification task
is to predict if a client will subscribe (yes/no) to a term deposit. The original data set is
very unbalanced with only 4,667 positives out of 45,211 samples. Therefore, we combine
“yes” points with randomly subsampled “no” points and perform experiments on the down-
sampled data set with 10,000 data points. The protected feature is the marital status of the
client.
(3)	COMPAS (Angwin et al., 2016): The COMPAS data set contains records of over 11,000
defendants from Broward County, Florida, whose risk (of recidivism) was assessed using
the COMPAS tool. Each record contains multiple features of the defendant, including de-
mographic information, prior convictions, degree of charge, and the ground truth for recidi-
vism within two years. Following Zafar et al. (2017a); Nabi & Shpitser (2018), we limit
our attention to the subset consisting of African-Americans and Caucasians defendants.
The features we use include age, gender, race, number of priors, and degree of charges.
The task is to predict the recidivism of the defendant and we choose race as the protected
feature.
(4)	German Credit6: The UCI German Credit data contains 20 features (7 numerical, 13
categorical) describing the social and economical status of 1,000 customers. The prediction
task is to classify people as good or bad credit risks. We use the provided numerical version
of the data and choose gender as the protected feature.
A.6 Additional discussion
For classification, while randomization can ensure group level of fairness, there is still some inher-
ent shortcoming of the criterion that we should pay attention to. For example, in the FICO case
study in Hardt et al. (2016), for a specific client from certain demographic group, the decision of
approve/deny the loan actually comes in two folds: if his/her credit score is above (below) the upper
(lower) threshold, the bank approve (deny) the application for sure; if the score falls in the interval
between two thresholds, the bank would flip a coin to make a decision. Then we can imagine the
following situation when a client whose credit score falls within the interval between the upper and
4http://archive.ics.uci.edu/ml/datasets/Adult
5https://archive.ics.uci.edu/ml/datasets/bank+marketing
6https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
16
Under review as a conference paper at ICLR 2021
lower thresholds goes to a bank to apply a loan. He/she can ask (if conditions permit) the bank to
run the model multiple times until the decision is approval. This would make the randomization that
was built into the system for the sake of fairness no longer effective, and the system in essence only
has one fixed threshold (i.e., the original lower threshold).
17