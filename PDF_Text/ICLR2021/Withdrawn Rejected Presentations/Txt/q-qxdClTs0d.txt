Under review as a conference paper at ICLR 2021
Out-of-distribution Prediction with Invariant
Risk Minimization: The Limitation and An Ef-
fective Fix
Anonymous authors
Paper under double-blind review
Ab stract
This work considers the out-of-distribution (OOD) prediction problem where
(1) the training data are from multiple domains and (2) the test domain is un-
seen in the training. DNNs fail in OOD prediction because they are prone to pick
up spurious correlations. Recently, Invariant Risk Minimization (IRM) is pro-
posed to address this issue. Its effectiveness has been demonstrated in the colored
MNIST experiment. Nevertheless, we find that the performance of IRM can be
dramatically degraded under strong Λ spuriousness - when the spurious correla-
tion between the spurious features and the class label is strong due to the strong
causal influence of their common cause, the domain label, on both of them (see
Fig. 1). In this work, we try to answer the questions: why does IRM fail in the
aforementioned setting? Why does IRM work for the original colored MNIST
dataset? Then, we propose a simple and effective approach to fix the problem of
IRM. We combine IRM with conditional distribution matching to avoid a specific
type of spurious correlation under strong Λ spuriousness. Empirically, we design
a series of semi synthetic datasets - the colored MNIST plus, which exposes the
problems of IRM and demonstrates the efficacy of the proposed method.
1	Introduction
Strong empirical results have demonstrated the efficacy of deep neu-
ral networks (DNNs) in a variety of areas including computer vision,
natural language processing and speech recognition. However, such
positive results overwhelmingly rely on the assumption that the train-
ing, validation and test data consist of independent identical samples
of the same underlying distribution. In contrast, in the setting of out-
of-distribution (OOD) prediction where (1) the training data is from
multiple domains and (2) the test set has different distribution from
the training, the performance of DNNs can be dramatically degraded.
This is because DNNs are prone to pick up spurious correlations which
do not hold beyond the training data distribution (Beery et al., 2018;
Arjovsky et al., 2019). For example, when most camel pictures in a
training set have a desert in the background, DNNs will pick up the
spurious correlation between a desert and the class label, leading to
failures when camel pictures come with different backgrounds in a
test set. Therefore, OOD prediction remains an extremely challeng-
ing problem for DNNs.
The invariant causal relationships across different domains turns out
to be the key to address the challenge of OOD prediction. The causal
graph in Fig. 1 describes the relationships of the variables in the OOD
prediction problem. Although spurious correlations learned in one do-
Figure 1:	The causal
graph in OOD predic-
tion: P (Y |Xc ) is invariant
across domains. The spu-
rious correlation P(Y |Xs)
may vary. A directed (bidi-
rected) edge is a causal re-
lationship (correlation).
main are unreliable in another, invariant causal relationships enable DNNs that capture causal rela-
tionships to generalize to unseen domains. In practice, it is extremely difficult to know whether an
input feature is causal or spurious. Thus, a recipe for training DNNs that capture causal relationships
is learning causal feature representations that hold invariant causal relationships with the class label.
1
Under review as a conference paper at ICLR 2021
Thus, the main challenge of OOD prediction becomes learning causal feature representations given
training data from multiple domains. Invariant Risk Minimization (IRM) (Arjovsky et al., 2019) is
proposed to learn causal feature representations for OOD prediction. IRM formulates the invariance
of causal relationships as a constraint. It requires that causal feature representations must result
in the same optimal classifier across all domains. IRM is written as the conditional independence,
Y ⊥⊥ E|F (X) in (Chang et al., 2020; Zeng et al., 2019) which can be derived from the causal graph
in Fig. 1 when F(X) is a mapping of causal features (Xc in Fig. 1) with no information loss. A
detailed discussion on IRM and Y ⊥⊥ E|F (X) is in Appendix A.
Despite IRM’s success in the colored MNIST dataset (Arjovsky et al., 2019), in this work, we find
an important issue of IRM that has not been discussed. Specifically, we consider the situation where
strong spurious correlations among the spurious features, the class label and the domain label only
hold for training data. We name this strong Λ spuriousness using the shape of the structure among
Xs, E and Y in Fig. 1. Under strong Λ spuriousness, IRM regularized empirical risk can have
low values with spurious feature representations that accurately predict the domain label. This is
because, in this setting, picking up spurious features can achieve high accuracy in predicting both
domain and class in the training set, but not in the test. However, the colored MNIST dataset cannot
expose this issue because the strong similarity between the two training domains makes it difficult
to pick up the weak spurious correlation between the domain label and the spurious features. To
illustrate this problem, We design a new dataset - the colored MNIST plus. As shown in Fig. 2, in
this dataset, under strong Λ spuriousness, the performance of IRM models is significantly degraded.
Moreover, to resolve this issue of IRM, we propose an effective solution, which combines IRM
with conditional distribution matching (CDM). The CDM constraint means that the representation
distribution of instances from the same class should be invariant across domains. Theoretically,
we show that (1) causal feature representations can satisfy CDM and IRM at the same time and
(2) CDM can prevent DNNs from learning spurious feature representations that accurately predict
the domain label. Empirically, on our newly introduced dataset, the proposed method achieves
significant performance improvement over IRM under strong Λ spuriousness.
2	Preliminaries and IRM
Notations. We use lowercase (e.g., x), uppercase (e.g., X) and calligraphic uppercase (e.g., X)
letters for values, random variables and spaces. We let X ∈ X, Y ∈ Y and E ∈ E denote raw
input, the class label and the domain label where X, Y and E are the spaces of input, class labels
and domains. A DNN model consists of a feature learning function F and a classifier G. A feature
learning function F : X → Rd maps raw input X to its d-dimensional representations F (X). A
classifier G : Rd → Y maps a feature representation to a class label. We denote their parameters by
θF and θG, respectively. Let θ = Concat(θF, θG) denote the concatenation of them.
A domain e of ne instances is denoted by De = {xie, yie}in=e1. Let Etr and Ets denote the set of
training and test domains, in OOD prediction, we have (1) |Etr | > 1 and (2) Etr ∩ Ets = 0.
Problem Statement. Given data from multiple training domains {De}e∈Etr. We aim to predict the
label yie0 of each instance with features xie0 from a test domain {xie0 , yie0 }in=e01, e0 ∈ Ets.
Invariant Risk Minimization. IRM (Arjovsky et al., 2019) is a recently proposed method to im-
pose the causal inductive bias: the causal relationships between causal features and the label should
be invariant across different domains. It not only aims to address the challenging OOD prediction
problem, but also is a pioneer work that guides causal machine learning research towards the de-
velopment of inductive bias imposing causal constraints. The effectiveness of IRM and its variants
have been demonstrated across various areas including computer vision (Ahuja et al., 2020), natural
language processing (Chang et al., 2020), CTR prediction (Zeng et al., 2019), reinforcement learn-
ing (Zhang et al., 2020a) and financial forecasting (Krueger et al., 2020). Arjovsky et al. (2019)
propose the original formulation of IRM as a two-stage optimization problem:
argmin	E(χ,y)〜De [Re(G(F(x)),y)]
θF,θG e∈Etr
s.t. θG ∈ arg min Re(G(F(x); θG0 ), y),
θG0
(1)
2
Under review as a conference paper at ICLR 2021
where Re denotes the loss function of domain e. Then, they show that the IRM constraint can be
imposed by adding a regularizer into the loss function. It turns out the IRM constraint can be written
as the conditional independence Y ⊥⊥ E|F (X) (Chang et al., 2020; Zeng et al., 2019). This can
be derived from the causal graph in Fig. 1. For brevity, we refer to Y ⊥⊥ E|F (X) as the IRM
constraint.
3	The Limitation of IRM
In this section, We find that IRM fails under strong A spuriousness - when the spurious correlations
among spurious features, the domain label and the class label are strong. To show it, we design a
new dataset - the colored MNIST plus (CMNIST+) to expose this limitation of IRM. Then, we try
to answer two crucial research questions: (1) why does IRM fail in CMNIST+? (2) Why does IRM
work for the original colored MNIST (CMNIST) dataset?
3.1	Why does IRM fail?
(d) SS①USnO-JndS <
66.32	68.96	69.89
63.44	68.38	66.40
58.42	68.25	66.00
52.60	62.40	64.12
37.45	47.01	45∙38
33.04	35.80	37.81
33.32	I 31∙67 I	36.31 I
200	400	600
Iteration IRM added (Kirm)
(a) IRM
(d) SS①USno-JndS
0.55
0.6
0.65
0.7
0.8
0.85
0.9
37.89	37.11	43.48
35.49	36.24	48.64
32.40	39.19	36.92
36.91	37.82	41.45
35.80	34.39	31.37
28.15	31.48	30.36
26.66	28.37	28.97
0.85
0.9
200	400	600
Iteration IRM added (Kirm)
(b) IRM with label balanCing
3 6 5 7 8
Q dl6d0.
U O
(d) SS①USnO-Jn
60.75
51.73
40.96
36.14
30.16
27.83
24.61
0
Iteration IRM added (Kirm)
(C) ERM
Figure 2: Test aCCuraCy of IRM, IRM with balanCed Classes in eaCh domain, and ERM on CM-
NIST+: when the Λ spuriousness is strong (ρ ≥ 0.8), the performanCe of IRM drops dramatiCally
beCause, in this situation, the spurious feature representation F (X) = E satisfies the IRM Con-
Straint. The naive solution, balancing classes in each domain cannot mitigate this problem of IRM.
Despite its previous suCCess, results in Fig. 2 show that IRM fails in our newly designed Colored
MNIST plus dataset, under strong Λ spuriousness (ρ ≥ 0.8). Here, we show that, under strong Λ
spuriousness, at least two types of feature representations, F (X) = Y and F (X) = E, satisfy the
IRM constraint. When F(X) = E, the feature representations pick up spurious relationships.
Satisfying IRM by Perfectly Predicting the Label. First, we consider a case where there exists a
feature representation that perfectly predicts the class label, i.e., F(X) = Y . In this case, we know
that the feature representation satisfies the IRM constraint because Y ⊥⊥ E |F (X) = Y always
holds. This is reasonable because the causal relationship between Y and itself must be invariant
across different domains.
Satisfying IRM by Perfectly Predicting the Domain Does Not Lead to OOD Generalizable
Feature Representations. However, there is another way to satisfy the IRM constraint. If there
exists a feature representation F(X) = E, then the IRM constraint can be satisfied by F(X). This is
due to the fact Y ⊥⊥ E|F (X) = E. However, this is not desired behavior of IRM as F(X) = E is a
type of spurious feature representation. This is because P(Y |E) can vary across domains. Using the
camel picture example, if domains are the individuals who take the picture, we may find more camel
pictures taken by person e than from person e0 (P(Y = camel|E = e) > P(Y = camel|E = e0)).
Why is F(X) = E a good solution for the original IRM optimization problem (Eq. 1)? This is
because F(X) = E also leads to low values of the loss function Re since the direct causal influence
of the domain label E on the class label Y is also strong when Λ spuriousness is strong. So, with
a proper classifier G, we can achieve low values of Re with F (X) = E. Then, we describe the
CMNIST+ dataset and show experimental and theoretical results to support our claim.
The Colored MNIST Plus (CMNIST+) Dataset. We follow CMNIST (Arjovsky et al., 2019) to
create CMNIST+ by resampling and adding colors to instances of MNIST. The digits 0 - 4 (5 - 9)
3
Under review as a conference paper at ICLR 2021
are class Y = 1 (Y = 0). We randomly flip 25% of class labels so that spuriousness can be
stronger than the causal relationship between the shape S (causal features) and the class label Y .
Table 1 describes the dataset. The variable C ∈ {G, B, R} denotes the color, which represents the
spurious feature Xs . We explain why we use three colors for CMNIST+ in Appendix C.2. The
parameter ρ ∈ (0.5, 1) controls the strength of the spurious correlations between the color (spurious
features) C and the class label Y through their common cause, the domain label E. The larger
the value of ρ, the stronger the spurious correlations. Intuitively, in the two training domains of
CMNIST+, in addition to the strong spurious correlation between the class label Y and color C,
we set the spurious correlation between the domain label E and color C to be strong, too. Thus,
the CMNIST+ dataset can expose the problem of IRM: it would not penalize the models that fit the
domain variable as the feature representation (F (X) = E). To verify this claim, we show analysis
results to support the experimental results in Fig. 2. The data generating process of CMNIST+ can
be found in Append C.2.
Table 1: Definition of CMNIST+
E	P (Y 二 1|E)	Y	P (C = G∣Y,E)	P (C = B∣Y,E)	P (C = R∣Y,E)
E = 1	0.9	Y = 1	P	(1- P)∕2	二	(1 - P)∕2	二
		Y = 0	(1 - P)∕2	—	(1- P)∕2	—		P	
E = 2	0.1	Y = 1	(1 - P)∕2	—	P	(1 - P)∕2	—
		Y = 0	(1- P)∕2	一	(1- P)∕2	一		P	
E = 3	0.5	Y = 1	0.1	0.1	0.8
		Y = 0	0.4	0.4	0.2
Figure 3: We visualize the differences between CMNIST and CMNIST+ (ρ = 0.9) in terms of
P(C|Y = 1, E). Each large triangle represents the space of P (C|Y, E). Each small triangle shows
the values of P(C = c|Y = 1, E = e), c ∈ {R, G, B}, e ∈ {1, 2, 3}.
Analysis Results. Here, we show the analysis results on the CMNIST+ to answer two questions:
what features will be learned by ERM and IRM under under different Λ spuriousness? What is the
expected test accuracy of these models? For simplicity, we assume that the classifier is deterministic,
which always predicts the majority class given the feature. If the number of instances from each
class is the same, then it would predict a random label. We analyze three types of spurious feature
representations: (1) those that fit color, (2) those that fit the domain label and (3) those that use
a combination of them for prediction. The first case mimics the behavior of ERM that picks up
the spurious correlation between color and the class label. The second one represents a model
satisfying IRM by F (X) = E. The third one stands for a model satisfying IRM by F (X) =
Concat(E, C). One may argue that IRM can be fixed if we simply balance the two classes in
each domain. Theoretical analysis shows that this is an invalid solution. We summarize results in
Table 2 where E is the domain label predicted by a deterministic classifier using color as the feature.
This is because in an unseen test domain, we cannot directly use the domain label for prediction.
The highlighted numbers show which type of features would be learned by ERM and IRM. As ρ
increases, ERM and IRM are more likely to fit spurious features. This explains results in Fig. 2.
As a DNN may still pick up some causal features (shape in CMNIST+) even when ρ = 0.9, the
test accuracy of IRM in practice would be greater than 0.35 without label balancing and 0.2 with
label balancing. Similarly, the test accuracy of ERM would be greater than 0.2. From Fig. 2, we
can see the test accuracy of IRM, IRM with label balancing and ERM are slightly better than the
aforementioned lower bounds. The derivations can be found in Appendix B.
4
Under review as a conference paper at ICLR 2021
Experimental Setup. We use a LeNet-5 (LeCun et al., 1998) instead of a three-layer MLP. To take
input with three colors, we modify the first CNN layer to have three channels. LeNet-5 has more
predictive power such that it can pick UP the three types of spurious correlations (color - domain,
color - class, and domain - class) or the causal relationship (shape - class). We randomly split the
instances from the training domains into 80% training and 20% validation. The model selection
is done by picking the one with the lowest validation loss in each run. We report the average test
accuracy of the selected models in 10 runs. It is crucial to ensure only data from training domains
are used in model selection, since the test domain should be unseen during training and validation.
During training, we begin applying the IRM penalty at iteration KIRM, set to 200, 400, or 600. By
varying KIRM, we aim to examine the following hypothesis: IRM works by pushing the spurious
features out of the representations learned by the standard ERM training before the IRM penalty is
applied (Krueger et al., 2020). More details on the setup can be found in Appendix C.1.
Experimental Results. Fig. 2 shows the performance of IRM and IRM with balanced classes in
each domain. We make the following observations: first, the test accuracy of IRM drops dramatically
under strong A spuriousness. Second, We show that a naive fix for IRM, balancing the two classes
in each domain by oversampling the minority class, does not lead to improvement in performance.
Table 2: Theoretical validation/test accuracy with the three spurious feature representations (C:
color, E: domain and E+C) and the causal feature representation (S: shape) on CMNIST+. The ones
with the best validation accuracy in each setting are highlighted, which show the type of feature
representations would be learned by ERM and IRM.
Validation/Test Accuracy								
	without label balancing				with label balancing			
P	P(Y∣C)	P(Y∣E)	P(Y∣E, C)	P(Y∣S)	P(Y∣C)	P(Y |E)	P(Y |E,C)	P(Y |S)
0.55	0.662/0.2	0.646/0.35	-0.662/0.35-	0.75/0.75	0.662/0.2	-05/05-	-0.662/0.2-	0.75/0.75
0.6	-0.7/0.2-	0.68/0.35	0.7/0.35	0.75/0.75	-0.7/0.2-	-05/05-	0.7/0.2	0.75/0.75
0.65	0.738/0.2	0.714/0.35	-0.738/0.35-	0.75/0.75	0.738/0.2	-003-	-0.737/0.2-	0.75/0.75
0.7	0.775/0.2	0.748/0.35	-0.775/0.35-	0.75/0.75	0.775/0.2	-05/05-	-0.775/0.2-	0.75/0.75
0.8	0.85/0.2	0.815/0.35	-0.815/0.35-	0.75/0.75	0.85/0.2	-05/05-	-0.85/0.2	0.75/0.75
0.85	0.888/0.2	0.849/0.35	-0.849/0.2-	0.75/0.75	0.888/0.2	-0505-	-0.888/0.2-	0.75/0.75
0.9	0.925/0.2	0.883/0.35 -	0.883/0.2 —	0.75/0.75	0.925/0.2	0.5/0.5	0.925/0.2 —	0.75/0.75
3.2 Why does IRM work for the Original Colored MNIST?
9
00
70.03	74.91	57.21
53.40	47.49	56.59
34.57	46.78	46.54
40.66	41.85	50.62
38.09	39.87	37.94
0.5
0.7
0.9
P(Y= 1∖E= l),P(Y=0∣E= 2)
(a) IRM on CMNIST test set
9
OO
61.96	64.95	57.80
52.10	47.40	48.12
39.87	47.82	42.86
34.67	45.17	43.82
37.59	44.68	38.72
0.5
0.7
0.9
P(Y= 1∣E= l),P(Y=0∣E= 2)
(b) IRM on CMNIST+ test set
Figure 4: Accuracy of IRM trained on datasets between CMNIST and CMNIST+ and tested on the
test sets of CMNIST and CMNIST+ (wplus increases): As the training set becomes more similar to
CMNIST+, the performance of IRM gradually drops, on the test sets of CMNIST and CMNIST+.
The Colored MNIST (CMNIST) dataset cannot expose the limitation of IRM under strong A spuri-
ousness. This is because its two training domains are quite similar. As shown in the large triangle on
the left in Fig. 3, the values of P(C|Y, E) are similar for E = 1 and E = 2. In addition, the values
of P(Y |E) are the same for all E. This makes it difficult to satisfy the IRM constraint by learning
feature representations F (X) ≈ E.
Then, we present experiments to show how IRM gradually goes from working on CMNIST to failing
on CMNIST+. In Fig. 3 Left and Right, we observe the differences between between CMNIST and
CMNIST+ (ρ = 0.9). In Fig. 3 Middle, we create various datasets that interpolate between these
two datasets, illustrated by the yellow line in the middle triangle. We use a parameter wplus ∈ [0, 1]
5
Under review as a conference paper at ICLR 2021
to control P (C|Y, E) of the weights of CMNIST+ (ρ = 0.9) in the interpolated dataset:
P (C|Y, E)
Pcmnist+(C|Y, E)WPlus + Pcmnist(C|Y, E)(I - WPlus)
PC(Pcmnist+(C|Y, E)WPlus + Pcmnist(C|Y, E)(I - WPlus))
(2)
where Pcmnist+(C|Y, E) and Pcmnist(C|Y, E) are the values of P(C|Y, E) in CMNIST+ and CM-
NIST. When WPlus = 0 and P(Y = 1|E = 1) = P(Y = 0|E = 2) = 0.5, the dataset is
the same with CMNIST. As WPlus and P(Y = 1|E = 1) = P(Y = 0|E = 2) increase, the
dataset becomes more similar to CMNIST+, and becomes the same with CMNIST+ (ρ = 0.9) when
WPlus = 1 and P(Y = 1|E = 1) = P(Y = 0|E = 2) = 0.9. For the training sets, we set
P(Y = 1|E = 1) = P(Y = 0|E = 2) ∈ {0.5, 0.7, 0.9}. Fig. 4 shows results of IRM on the test
sets of both CMNIST and CMNIST+. The performance of IRM gradually drops when the values of
P(C|Y, E) becomes more similar to CMNIST+ (when the Λ spuriousness becomes stronger).
4	An Effective Fix for IRM
In this section, we propose a simple but effective solution to address the limitation of the IRM
training. In a series of experiments, we show that the proposed method can improve the performance
of IRM even under strong Λ spuriousness.
Since the IRM constraint is too general, We propose an effective solution - combining the condi-
tional distribution matching (CDM) constraint and the IRM constraint. The CDM constraint (Li
et al., 2018; Long et al., 2018) requires P(F(X)|Y, E) = P(F (X)|Y), Which means the feature
representation distribution given the class label should be invariant across different domains. We
first explain Why this can be a reasonable solution. Then, We propose tWo types of models that
combine the tWo constraints.
Here, We use the tWo cases from Section 3.1 to explain Why combining IRM With CDM can be an
effective fix for IRM.
Perfectly predicting the label satisfies CDM. In this case, F (X) = Y and We already knoW the
IRM constraint is satisfied. The CDM condition also holds as P(F(X)|Y, E) = P(Y|Y, E) =
P(Y|Y)=P(F(X)|Y).
Perfectly predicting the domain label violates CDM. In this case, F(X) = E and the IRM con-
straint can be satisfied, but the CDM constraint does not hold as P(F(X)|Y, E) = P(E|Y, E) 6=
P(E|Y) = P(F(X)|Y)). This case implies that We can add CDM to IRM to exclude the undesir-
able solution of learning spurious features that accurately predict the domain label.
One Way to enforce the CDM constraint is through adversarial training: a discriminator tries to infer
the source domain from the feature representation F(X) and F(X) tries to adjust itself to fool the
discriminator. From this perspective, We can find that the CDM constraint exactly prevents F (X)
from making use of the domain labels to achieve high training accuracy (i.e., loW Re).
With the idea of combining IRM and CDM, We can formulate the optimization problem as:
argmin X E(x,y)〜d« [Re(G(F(x)),y)]
θF,θG e∈Etr	(3)
s.t. Y ⊥⊥ E|F(X),P(F(X)|Y,E = e) = P(F(X)|Y,E = e0),e 6= e0,
Where Re denotes the loss function in domain e (e.g., cross entropy loss). To impose the CDM con-
straint, We aim to minimize certain divergence betWeen feature distributions from different domains,
denotedasdiv(P(F(X)|Y,E = e)||P(F(X)|Y,E = e0)),e 6= e0.
We propose tWo choices of divergences, i.e., Maximum Mean Discrepancy (MMD) and Kullback-
Leibler (KL) divergence (through adversarial training), Which result in tWo algorithms: IRM-MMD
and IRM-ACDM (i.e., IRM-Adversarial Conditional Distribution Matching).
IRM-MMD. In IRM-MMD, We adopt Maximum Mean Discrepancy (MMD) (Long et al., 2015;
Tolstikhin et al., 2017; Shalit et al., 2017) as the distribution divergence. The MMD betWeen P and
Q, tWo d-dimensional distributions of feature representations, can be defined as:
MMDk (P, Q) = SUPIEZ〜P[f (Z)] - EZ〜Q[f (Z)]|,	(4)
f∈H
6
Under review as a conference paper at ICLR 2021
where Z ∈ Rd , f : Rd → R maps a feature representation to a real value, k : Rd × Rd → R
denotes the characteristic kernel of f and H is the RHKS of k. The MMD in Eq. 4 is not directly
computable. So, we use the unbiased estimator of MMD (Gretton et al., 2012a) with N samples
z1P, ..., zNP from P and M samples z1Q, ..., zMQ from Q:
1	1	2 NM
MMDk(P Q) = N(N - 1) Xk(zi ,zj )+ M(M - 1) X k(Zi , Zj)-MN X X k(Zi , Zj).
i6=j	i6=j	i=1 j =1
(5)
With MMD defined, we can define the loss function of IRM-MMD as:
arg min X LIeRM+βXX X MMDk(P(F(X)|y, e), P(F(X)|y, e0)),	(6)
θ	e∈Etr	y∈Y e∈Etr e0 ∈Etr \e
where LeRM = n1e Pn= 1 Re(G(F(^e)),yf) + α∣∣ 5w∣w=ι.o WRe(G(F(Xe)))||2 denotes the IRM
regularized loss (Arjovsky et al., 2019) for domain e, hyperparameters α and β control the trade-off
between the main loss, the IRM constraint and the CDM constraint.
IRM-ACDM. In IRM-ACDM, we make div(P(F(X)|Y, E = e)||P(F(X)|Y, E = e0)) be the
Kullback-Leibler (KL) divergence and use adversarial learning to estimate it (Tolstikhin et al., 2017;
Song et al., 2020). More precisely, we define the loss function of IRM-ACDM as:
arg min X LIRM + βX X YyKL(P(F(X)|Y = y,E = e)||P(F(X)|Y = y)),	⑺
θ	e∈Etr	y∈Y e∈Etr
where γey := P(E = e, Y = y). In adversarial learning, a conditional discriminator D : Rd × Y →
Etr with parameters θD is introduced to predict the domain label of an instance, given its feature
representation and class label. As proved in Li et al. (2018); Song et al. (2020), the estimation of the
KL divergence above can be reformulated as the minimax game below:
minmaχ E LIRM + βE γyE YeEF(x)〜P(F(X)|Y=y,E=e) [log De(F(x),y)],	⑻
e∈Etr	y∈Y e∈Etr
where De(F(x), y) = P(E = e|F (x), y) is the predicted probability of the instance for domain e
by the discriminator D. In practice, we solve the minimax game above efficiently by the alternative
gradient ascent/descent algorithm.
Experimental Results. We evaluate IRM-MMD and IRM-ACDM on CMNIST+ to show their ef-
ficacy under strong Λ spuriousness. We let the output of the second last layer of LeNet-5 to be the
feature representation F(X). For IRM-MMD, we use the multiple kernel MMD (MKMMD) (Gret-
ton et al., 2012b; Long et al., 2015). For IRM-ACDM, we use a two-layer MLP with |Etr| outputs as
the discriminator D. To show that CDM alone cannot solve OOD prediction under strong Λ spuri-
ousness, we set the weight of the IRM penalty α = 0 in IRM-MMD and IRM-ACDM to obtain the
models regularized by MMD and ACDM, respectively. We consider EIIL (Creager et al., 2020) that
solves a minimax game. In the max step, it learns soft domain labels for instances s.t. the IRMv1
loss is maximized. In the min step, it minimizes the IRMv1 loss. We also include ERM and oracle.
The oracle uses the original LeNet-5 architecture with a single-channel CNN as the first layer. It is
trained and tested with instances transformed into grayscale.
Table. 3 shows the performance of IRM-MMD, IRM-ACDM and the baselines. Under strong Λ
spuriousness, compared to IRM (Fig. 2), we can observe the significant and consistent performance
improvement over IRM resulting from combining CDM with IRM. IRM-MMD and IRM-ACDM
also outperform MMD and ACDM. This verifies that CDM alone cannot solve the OOD prediction
problem. EIIL reaches comparable accuracy to IRM-MMD and IRM-ACDM when ρ = 0.85, 0.9.
But EIIL has larger standard deviation ±10.32%, ±13.03%, ±13.03% with ρ = 0.8, 0.85, 0.9, com-
pared to IRM-ACDM (±4.34%, ±4.65%, ±3.09%) and IRM-MMD (±4.56%, ±2.50%, ±6.97%).
Discussion can be found in Appendix C.3.
5	Related Work
OOD Prediction. IRM (Arjovsky et al., 2019) formulates causal feature learning as a constraint
on the ERM framework (Vapnik, 1992), which imposes the causal inductive bias: causal feature
7
Under review as a conference paper at ICLR 2021
Table 3: Test accuracy on CMNIST+: by combining CDM with IRM, performance is improved
significantly and consistently under strong Λ spuriousness (P ≥ 0.8) compared to IRM.
Method	P = 0.8	ρ = 0.85	ρ = 0.9
IRM-MMD (ours) IRM-ACDM (ours)	52.91% 57.23%	40.83%- 45.47%	37.96% 42.85%
EiiL	43.40%	-43.24%~	40.93%
IRM	47.01%	-37.81%-	36.31%
MMD ACDM	23.04% 30.41%	-25.22%- 29.48%	24.22% 25.53%
ERM	30.16%	-27.83%-	24.61%
Oracle	73.10%	73.49% '	73.58%
representations lead to the existence of an optimal classifier for all domains. Then it is transformed
to a regularizer which can be minimized along with empirical risks. Chang et al. (2020) propose
a variant of IRM. They implement IRM by minimizing the difference between two classifiers’ out-
puts, which take F (X) and Concat(F (X), E) as the inputs, respectively. Ahuja et al. (2020)
reformulate the optimization problem of IRM from a game theory aspect. IRM is related to robust
optimization (Ben-Tal et al., 2009). The goal is to optimize the worst domain specific empirical risk.
Krueger et al. (2020) extend robust optimization (Ben-Tal et al., 2009) to minimize the empirical
risk of the worst domain and maximize that of other domains. They also propose to minimize the
variance of domain specific empirical risks along with the empirical risk. Jin et al. (2020) propose to
minimize the domain specific risk between two models, one trained on the same domain, the other
trained on the other domains. These methods essentially minimize the differences among domain
specific risks. Moreover, data augmentation can also improve the generalizability of DNNs from the
data perspective (Ilse et al., 2020; van der Wilk et al., 2018). However, it requires prior knowledge
on the differences between training and test domains, which are not be available in OOD prediction.
Kuang et al. (2018) and Qiao et al. (2020) aim to handle the case with only one training domain.
Zhang et al. (2020b) propose to make DNNs more robust against test data generated by unseen inter-
ventions. They model interventions in training data with a generative model and perform test-time
inference to catch unseen interventions. Compared to the existing work, this work exposes and fixes
the issue of IRM in OOD prediction under strong Λ spuriousness.
Domain Adaptation (DA) assumes that the unlabeled test set can be used during training and vali-
dation. From the methodology aspect, distribution matching methods used in DA, such as gradient
reversing (Ganin & Lempitsky, 2015) and adversarial CDM (Long et al., 2018) are useful for OOD
prediction. Peters et al. (2016) realize the invariance of causal relationships can be used for DA.
Zhang et al. (2013) propose reweighting and kernel based distribution matching methods to handle
three types of DA problems: target shift, conditional shift and generalized target shift. Gong et al.
(2016) work on extracting transferable components F(X) that ensure P(F(X)|Y ) to be invariant
across domains with location-scale transformation. Their method can identify how P(Y ) changes
across domains simultaneously. Different from DA, we strictly ensure that the test domain is unseen
during training and validation to reflect the scenario of OOD prediction in real-world applications.
6	Concluding Remarks
This work focuses on the OOD prediction problem under strong Λ spuriousness. Strong Λ spuri-
ousness means the correlations between spurious features and the class label are strong. We find an
important limitation of IRM in OOD prediction under strong Λ spuriousness: it can be satisfied by
spurious feature representations that are predictive of the domain label. To verify it, we design the
CMNIST+ dataset which has strong Λ spuriousness between color (spurious features) the class label
through their common cause - the domain variable. On CMNIST+, We observe the performance of
IRM dramatically drops when the Λ spuriousness becomes stronger. Based on this observation, we
propose a simple but an effective fix to mitigate this issue of IRM. The proposed approach com-
bines CDM and IRM because CDM can also be satisfied by causal feature representations. At the
same time, CDM can prevent DNNs from picking up the aforementioned spurious feature repre-
sentations. Experimental results on CMNIST+ shoW significant performance improvement of the
proposed method, demonstrating its effectiveness. Interesting future Work includes (1) extension of
the proposed method to OOD prediction tasks in complex data (e.g., graphs and time series) and (2)
development of general causal inductive bias that can impose various conditional independence.
8
Under review as a conference paper at ICLR 2021
References
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk min-
imization games. arXiv preprint arXiv:2002.04692, 2020.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of
the European Conference on Computer Vision (ECCV),pp. 456-473, 2018.
Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization, volume 28.
Princeton University Press, 2009.
Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S Jaakkola. Invariant rationalization. In ICML,
2020.
Elliot Creager, Jorn-Henrik Jacobsen, and Richard ZemeL Environment inference for invariant
learning. In ICML Workshop on Uncertainty and Robustness, 2020.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International conference on machine learning, pp. 1180-1189, 2015.
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Scholkopf. Domain adaptation with conditional transferable components. In International Con-
ference on machine learning, pp. 2839-2848, 2016.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012a.
Arthur Gretton, Dino Sejdinovic, Heiko Strathmann, Sivaraman Balakrishnan, Massimiliano Pontil,
Kenji Fukumizu, and Bharath K Sriperumbudur. Optimal kernel choice for large-scale two-sample
tests. In Advances in neural information processing systems, pp. 1205-1213, 2012b.
Maximilian Ilse, Jakub M Tomczak, and Patrick Forre. Designing data augmentation for simulating
interventions. arXiv preprint arXiv:2005.01856, 2020.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Domain extrapolation via regret minimization.
arXiv preprint arXiv:2006.03908, 2020.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le
Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). arXiv
preprint arXiv:2003.00688, 2020.
Kun Kuang, Peng Cui, Susan Athey, Ruoxuan Xiong, and Bo Li. Stable prediction across unknown
environments. In Proceedings ofthe 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 1617-1626, 2018.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 624-639, 2018.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97-105. PMLR,
2015.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Judea Pearl and Elias Bareinboim. External validity: From do-calculus to transportability across
populations. Statistical Science, pp. 579-595, 2014.
9
Under review as a conference paper at ICLR 2021
Jonas Peters, Peter BUhlmann, and Nicolai Meinshausen. Causal inference by using invariant Pre-
diction: identification and confidence intervals. J. R. Stat. Soc. Series B Stat. Methodol., 78(5):
947-1012, 2016.
Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, PP. 12556-
12565, 2020.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: gener-
alization bounds and algorithms. In International Conference on Machine Learning, PP. 3076-
3085. PMLR, 2017.
Yuhang Song, Wenbo Li, Lei Zhang, Jianwei Yang, Emre Kiciman, Hamid Palangi, Jianfeng Gao,
C-C Jay Kuo, and Pengchuan Zhang. Novel human-object interaction detection via adversarial
domain generalization. arXiv preprint arXiv:2005.11406, 2020.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard SchoelkoPf. Wasserstein auto-
encoders. arXiv preprint arXiv:1711.01558, 2017.
Mark van der Wilk, Matthias Bauer, ST John, and James Hensman. Learning invariances using
the marginal likelihood. In Advances in Neural Information Processing Systems, PP. 9938-9948,
2018.
Vladimir VaPnik. PrinciPles of risk minimization for learning theory. In Advances in neural infor-
mation processing systems, PP. 831-838, 1992.
Shuxi Zeng, Pengchuan Zhang, Denis Charles, Eren Manavoglu, and Emre Kiciman. Robust neural
networks for causal invariant features extraction. NeurIPS 2019 WorkshoP, “Do the right thing”:
machine learning and causal inference for imProved decision making, December 2019.
Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau,
Yarin Gal, and Doina PrecuP. Invariant causal Prediction for block mdPs. arXiv preprint
arXiv:2003.06016, 2020a.
Cheng Zhang, Kun Zhang, and Yingzhen Li. A causal view on robustness of neural networks. arXiv
preprint arXiv:2005.01095, 2020b.
Kun Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, PP. 819-827,
2013.
10
Under review as a conference paper at ICLR 2021
A IRM and the Conditional Independence
Under general conditions, the conditional independence Y ⊥⊥ E|F (X) is a necessary condition
for solutions of the original IRM optimization problem. In (Arjovsky et al., 2019), IRM is defined
as a two-stage optimization problem. Any solution F (X) to the IRM optimization problem must
satisfy Y ⊥⊥ E|F(X). However, some F(X) satisfying Y ⊥⊥ E|F (X) may not be a solution to
the original IRM problem. For example, generally, F(X) = E would not minimize the sum of the
domain-specific risk Re. However, under strong Λ spuriousness, there exist solutions to the original
IRM problem that still pick up spurious features. Consider the extreme case, in the training data, if
Y = E, then F(X) = E is a solution to the problem.
B Analysis for CMNIST+
Fitting Color for Classification. In expectation, the ERM model would learn color as the feature
representation for classification. Here, we show the theoretical results for such cases. When the
model learns color C as the feature representation F (X), we have:
P(Y|C) =	P(Y|C,E)P(E|C)
E
X P(CIY,E)p(γ∣E) P(EIC)
=⅛	P(CIE)	P(EIC)
E
=X P(C∣Y,E)P(Y∣E) PE)
(9)
The second equality is by Bayes’ rule. We know that P (C) = Y E P(CIY, E)P(YIE)P(E).
Then, given P(CIY, E), P(YIE), P(C) and Eq. 9, we can obtain P(YIC) as shown in Table 4.
ʌ___ . _. … ʌ______________ . _. _ .
These results imply that the deterministic classifier P(Y = 1∣C = G) = 1, P(Y = 1∣C = B) = 1,
P(Y = 1|C = R) = 0 for all P ∈ [0.55,0.9] as shown in Table 4. So, the accuracy of the
deterministic classifier P(YIC) on the test set is 0.2. Its accuracy on the training set is C P(Y =
YIC)P(C) where P(C) = PE PY P(CIY, E)P(YIE)P(E).
Fitting Domain Label for Classification. Since the spurious correlation between the domain label
and the class label is strong, and IRM cannot penalize models fitting the domain label, P(YIE)
can help us understand the expected behavior of the IRM model. Note that the test data is from an
unseen domain. So, we analyze the model that first predicts domain by color, then predicts class
label by domain. First, we analyze P(EIC) as below:
P(EIC) =	P(EIC, Y)P(YIC)
Y
Σ
Y
P (C∣Y,E)P (E∣Y )P (Y ∣C)
P (C∣Y)
Σ
Y
P (C∣Y,E)P (E∣Y )P (Y)
P(C
(10)
Σ
Y
P (C∣Y,E)P (Y ∣E )P (E)
P(C
where the second and forth qualities are by Bayes’ rule. With Eq. 10, we can list the values of
P(EIC) in Table 6 for ρ ∈ [0.55, 0.9]. Thus, we know the deterministic domain prediction results
ʌ , ʌ , ʌ ,
would be P(E = 1∣C = G) = 1, P(E = 1∣C = B) = 0, and P(E = 1∣C = R) = 0. Since
P(Y IE) is given in Table 1, we can obtain the deterministic classifier’s predictions as:
ʌ , ʌ , ʌ ,
P(Y = 1∣C = G) = 1, P(Y = 1∣C = B) = 0, P(Y = 1∣C = R) = 0.	(11)
So, the expected test accuracy of the model would be 0.35. Recall that the model first predicts
domain label by color and then predict class label by the predicted domain. By doing this, it would
have F (X) ≈ E. This makes it approximately satisfy the IRM constraint Y ⊥⊥ EIF(X) since
11
Under review as a conference paper at ICLR 2021
Y ⊥⊥ E |E. This implies that the model’s performance can be treated as the expected performance
of the IRM model in CMNIST+. In terms of the performance of P(Y |E) on the training set, we can
get the results using the same prediction rules as in Eq. 11.
One may argue that the IRM model can perform well if we balance the two classes in each domain.
Here, we theoretically show this is not the case. By setting P(Y |E) = 0.5, we can obtain the values
of P(E|C) in Table 7. Note that practically this can be done by oversampling the minority class
of each domain in each mini-batch. However, since P(Y |E) = 0.5, the predictions made by the
deterministic classifier P(Y |E) would be just random guess, leading to a test accuracy of 0.5.
Fitting both Domain and Color for Classification. Here, we consider the model that first predicts the
domain label by color and then predicts the class label by both the color and the predicted domain
label. The first step is the same as the model fitting the domain label. For the second step, we
analyze P (Y |C, E) as below:
P(Y|C,E)
P (C ∣Y,E )P (Y |E)
P (C |E)
(12)
With P(C|E) = Y P(C|Y, E)P(Y |E) and Eq. 12, we obtain values of P(Y |C, E) as shown in
Table 8. So, given the predicted domains, P(E = 1|C = G) = 1, P(E = 1|C = B) = 0, and
ʌ , — , _, _. . . ʌ_______________________________________________ , _. —― 、
P(E = 1|C = R) = 0, the predictions on the class label are P(Y = 1|C = G,E = 1) = 1,
ʌ___ , . _ _ 、 ʌ__________________________________ , . _ _ 、 .
P(Y = 1 |C = B, E = 2) = 1, ρ > 0.8, P(Y = 1 |C = B, E = 2) = 0, P ≤ 0.8 and
P(Y = 1|C = R,E = 2) = 0. So, the test accuracy is 0.35 when P ≤ 0.8 and 0.2 when P > 0.8.
Similarly, when we make P(Y |E) = 0.5 by oversampling the minority class in each domain, we
can obtain the predictions shown in Table 9. So, the deterministic model would make predictions as
ʌ , ʌ , ʌ ,
P(Y = 1|C = G,E = 1) = 1, P(Y = 1|C = B,E = 2) = 1, and P(Y = 1|C = R,E) = 0.
This would lead to a test accuracy of 0.2. These results reflect the reasons why the IRM model
trained with the balanced classes in each domain (P(Y |E) = 0.5) has worse performance compared
to its counterpart trained with the original CMNIST+ data.
Table 4: Analysis results: fitting color
P	P (Y = 1|C = G)	P (Y = 1|C = B)	P(Y = 1|C = R)
0.55	0.697	0.534	029
0.6	0737	0.545	025
0.65	0.775	0.56	0212
0.7	0.811	0.577	0.176
0.8	088	0.63	0111
0.85	0.912	0.67	0.081
0.9	0.942	0.73	0.053
Table 5: Analysis results: fitting color, when P(Y |E) = 0.5.
P	P (Y = 1|C = G)	P (Y = 1|C = B)	P (Y = 1|C = R)
0.55	0.633	0.633	029
0.6	0.667	0.667	025
0.65	0702	0702	0.212
0.7	0739	0739	0176
0.8	0.818	0.818	0111
0.85	0.86	0.86	0.081
0.9	0.905	0.905	0.053
C	Experimental Setup and Results
Here, we include more details and discussion on experimental setup, datasets and results.
12
Under review as a conference paper at ICLR 2021
Table 6: Theoretical analysis results: predicting domain by color.
P	P (E = 1|C = G)	P (E = 1|C = B)	P (E = 1|C = R)
0.55	0.697	0.466	0332
0.6	0737	0.455	03
0.65	0775	0.44	027
0.7	On	0423	0241
0.8	088	037	0189
0.85	0.912	033	0.165
0.9	0.942	0.27	0.142
Table 7: Theoretical analysis results: predicting domain by color, when P(Y E) = 0.5.
P	P (E = 1|C = G)	P (E = 1|C = B)	P (E = 1|C = R)
0.55	0633	0367	05
0.6	0667	0333	05
0.65	0702	0298	05
0.7	0739	0261	05
0.8	0818	0182	05
0.85	086	014	05
0.9	0.905	0.095	0.5
Table 8: Predicting by both color and domain
P	P (Y = 1|G, 1)	P(Y= 1|G, 2)	P (Y = 1|B, 1)	P (Y = 1|B, 2)	P (Y = 1|R, 1)	P(Y = 1|R, 2)
0.55	0.957	0.1	0.9	0.214	0.786	0.043
0.6	0.964	0.1	09	0.25	075	0.036
0.65	0.971	0.1	09	0.292	0708	0029
0.7	0.977	0.1	09	0.341	0.659	0.023
0.8	0.986	0.1	09	0.471	0.529	0.014
0.85	099	0.1	09	0.557	0.443	001
0.9	0.994	0.1	0.9	0.667	0.333	0.006
Table 9: Predicting by both color and domain, class balanced
P	P (Y = 1|G, 1)	P(Y = 1|G, 2)	P (Y = 1|B, 1)	P(Y = 1|B, 2)	P (Y = 1|R, 1)	P(Y = 1|R, 2)
0.55	0.71	0.5	05	0.71	0.29	0.29
0.6	075	0.5	05	075	0.25	025
0.65	0788	0.5	05	0788	0.212	0.212
0.7	0.824	0.5	05	0.824	0176	0.176
0.8	0.889	0.5	05	0.889	0H1	0m
0.85	0.919	0.5	05	0.919	0.081	0.081
0.9	0.947	0.5	0.5	0.947	0.053	0.053
C.1 Experimental Setup
Here, we provide more details on experimental setup. We perform grid search for hyperparameter
tuning. For IRM, IRM-MMD and IRM-ACDM, we search the iteration number to plug in the
IRM penalty term (KIRM) in 200, 400, 600 and IRM penalty weight α in {1, 10, ..., 108}. For
MMD, ACDM, IRM-MMD, IRM-ACDM, we search CDM penalty weight β in {1, 10, ..., 105}.
For ACDM and IRM-ACDM, we set the number of steps we train the discriminator D in each
iteration to 10. For EIIL, we do the same hyperparameter tuning on the IRM penalty weight (α), the
iteration to add IRM (KIRM) as we do for IRM, IRM-ACDM and IRM-MMD methods. We train
the soft environment weight q(E|X, Y) for {10, 100, 1000, 10000} steps.
C.2 Datasets
Here, we present more details about the datasets.
CMNIST. CMNIST is introduced by (Arjovsky et al., 2019). We can see the two training domains of
CMNIST are similar to each other in terms of both P(C|Y, E) andP(Y|E) in Table 10. This means
CMNIST does not cover the case of strong Λ spurious, since the spurious correlations, color-domain
and domain-class, are not strong.
13
Under review as a conference paper at ICLR 2021
Table 10: Description of CMNIST
E	P (Y 二 1|E)	Y	P (C = G∣Y,E)	P (C = B∣Y,E)	P(C = R∣Y,E)
E = 1	0.5	Y = 1	09	00	03
		Y = 0	0.1	00	0.9
E = 2	0.5	Y = 1	08	00	0.2
		Y = 0	02	0.0	0.8
E = 3	0.5	Y = 1	0.1	0.0	0.9
		Y = 0	0.9	0.0	0.1
Class Y=I
CMNIST+(p=0.5)
CMNIST+(p=O,5 to p=0.9)
CMNIST+(p=0.9)
Figure 5: We visualize CMNIST+ with ρ ∈ [0.5, 0.9] in terms of P(C|Y = 1, E). Each large
triangle represents the space of P (C|Y, E). Each small triangle shows the values of P(C = c|Y =
1, E = e), c ∈ {R, G, B}, e ∈ {1, 2, 3}.
CMNIST+. We visualize the CMNIST+ dataset with different values of ρ in Fig. 5. In addition, we
provide a detailed simulation recipe of CMNIST+ and compare it with that of CMNIST. This would
also show that CMNIST+ is in accordance with the causal graph in Fig. 1.
1.	We decide the true label Y * (without noise) of each instance by its original digit label (0-9).
2.	We randomly split the data into test and training.
3.	We assign the training instances to the two training domains based on the true label Y * and
P (Y|E) in Table 1. This step introduces correlations between Y and E. In each training
environment, we further randomly split the data into training and validation.
4.	In each environment, we generate noisy labels Y by randomly flipping them with 25%
probability. This means Y * → Y .
5.	Given P(C|Y, E) in Table 1, the noisy label and the domain label, we assign color to each
instance. This step introduces correlations among Xs, Y and E.
Note that there is a difference in what causal relationships mean in traditional causal inference
and in OOD prediction. In OOD prediction, the definition of causal relationships is different from a
traditional one. Traditionally, Xc → Y means the generation of Y is (partially) determined by Xc. It
does not necessarily mean P(Y|Xc) remains the same across domains (Pearl & Bareinboim, 2014).
However, in OOD prediction, we say there exists a causal relationship Xc → Y iff P (Y|Xc) is the
same across different domains. We also know in the original MNIST dataset, there exists invariant
causal relationship Xc → Y*. This implies that, from the data generating process of CMNIST+,
we confirm that (1) there exist causal relationships Xc → Y * → Y, (2) there are correlations
among Xs, Y and E. With these two conclusions, we can claim that the causal graph in Fig. 1 is in
accordance with CMNIST+ in the OOD prediction problem where the true label Y* is ignored as it
is not used in training and evaluation.
Creating Strong Λ Spuriousness with Two Colors. It is possible to setup strong Λ spuriousness
with two colors for binary classification with two training domains. Here, we use the two colors:
red (R) and green (G). To show it is possible, we use an example with the following setup: P (Y =
1|E = 1) = 0.9, P (Y = 1|E = 2) = 0.1. Let’s say for E = 1, we set P (C = G|Y = 1, E =
1) = 0.9, P (C = G|Y = 0, E = 1) = 0.1. Then, we can set P (C = G|Y = 1, E = 2) = 0.1,
P(C = G|Y = 0, E = 2) = 0.9 for E = 2. This setup makes strong correlations among the color,
the class label and the domain label. Thus, it is possible to create strong Λ spuriousness with just
two colors. Our concern with such datasets is that even if strong Λ spuriousness exists in training
14
Under review as a conference paper at ICLR 2021
domains, it is a challenge to create test domains that are diverse enough from the training ones.
Following the aforementioned setup, for the test domain E = 3, if we set P(Y = 1|E = 3) = 0.5,
P(C = G|Y = 1, E = 3) = 0.5 and P(C = G|Y = 0, E = 3) = 0.5, it would be right in
the middle of the two training domains. Unfortunately in this setting, even if IRM fails, it can be
difficult to observe it with the test accuracy. This is because a model perfectly fits the color features
can reach 0.5 test accuracy. This leads to smaller differences between models fitting causal features
and those with spurious features. Thus, it becomes more challenging to judge whether a model fails
in practice, which explains why we use three colors in CMNIST+.
C.3 More Discussion on Results of EIIL
We evaluate EIIL1 on CMNIST+. EIIL gets test accuracy 43.40± 10.32%, 43.24± 13.03%, 40.93±
13.03% on CMNIST+ with ρ = 0.8, 0.85, 0.9. Compared to IRM-ACDM 57.23 ± 4.34%, 44.83 ±
4.65%, 42.85 ± 3.09% and IRM-MMD 52.91 ± 4.56%, 40.83 ± 2.50%, 37.96 ± 6.97%, EIIL has
comparable mean accuracies when ρ = 0.85, 0.9, but is not stable. There are two reasons. (1) EIIL
relies on IRM and the soft domain weight q(E|X, Y ). When q(E|X, Y ) takes values s.t. strong Λ
spuriousness exists, it makes IRM fail. (2) It has issues in model selection. EIIL can only select
models by validation accuracy as q(E|X, Y ) is a scalar qi for instance i. It is learned for the val/test,
so validation loss is not feasible. Validation accuracy can be high when the model picks up spurious
features. While model selection with validation loss considers the regularizers (IRM and CDM),
which approximates how well models fit causal features.
1https://github.com/ecreager/eiil
15