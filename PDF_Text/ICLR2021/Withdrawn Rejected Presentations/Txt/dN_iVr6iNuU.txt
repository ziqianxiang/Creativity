Under review as a conference paper at ICLR 2021
Preventing Value Function Collapse in
Ensemble Q-Learning by Maximizing
Representation Diversity
Anonymous authors
Paper under double-blind review
Ab stract
The first deep RL algorithm, DQN, was limited by the overestimation bias of the
learned Q-function. Subsequent algorithms proposed techniques to reduce this
problem, without fully eliminating it. Recently, the Maxmin and Ensemble Q-
learning algorithms used the different estimates provided by ensembles of learners
to reduce the bias. Unfortunately, these learners can converge to the same point
in the parametric or representation space, falling back to the classic single neural
network DQN. In this paper, we describe a regularization technique to maximize
diversity in the representation space in these algorithms. We propose and com-
pare five regularization functions inspired from economics theory and consensus
optimization. We show that the resulting approach significantly outperforms the
Maxmin and Ensemble Q-learning algorithms as well as non-ensemble baselines.
1	Introduction
Q-learning (Watkins, 1989) and its deep learning based successors inaugurated by DQN (Mnih et al.,
2015) are model-free, value function based reinforcement learning algorithms. Their popularity
stems from their intuitive, easy-to-implement update rule derived from the Bellman equation. At
each time step, the agent updates its Q-value towards the expectation of the current reward plus the
value corresponding to the maximal action in the next state. This state-action value represents the
maximum sum of reward the agent believes it could obtain from the current state by taking the current
action. Unfortunately (Thrun & Schwartz, 1993; van Hasselt, 2010) have shown that this simple
rule suffers from overestimation bias: due to the maximization operator in the update rule, positive
and negative errors do not cancel each other out, but positive errors accumulate. The overestimation
bias is particularly problematic under function approximation and have contributed towards learning
sub-optimal policies (ThrUn & SchWartz,1993; Szita & Lorincz, 2008; Strehl et al., 2009).
A possible solution is to introduce underestimation bias in the estimation of the Q-value. Double
Q-learning (van Hasselt, 2010) maintains tWo independent state-action value estimators (Q-functions).
The state-action value of estimator one is calculated by adding observed reWard and maximal state-
action value from the other estimator. Double DQN (Hado van Hasselt et al., 2016) applied this
idea using neural netWorks, and Was shoWn to provide better performance than DQN. More recent
actor-critic type deep RL algorithms such as TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al.,
2018) also use tWo Q function estimators (in combination With other techniques).
Other approaches such as EnsembleDQN (Anschel et al., 2017) and MaxminDQN (Lan et al., 2020)
maintain ensembles of Q-functions to estimate an unbiased Q-function. EnsembleDQN estimates the
state-action values by adding the current observed reWard and the maximal state-action value from
the average of Q-functions from the ensemble. MaxminDQN creates a proxy Q-function by selecting
the minimum Q-value for each action from all the Q-functions and using the maximal state-action
value from the proxy Q-function to estimate an unbiased Q-function. Both EnsembleDQN and
MaxminDQN have been shoWn to perform better than Double DQN. The primary insight of this
paper is that the performance of ensemble based methods is contingent on maintaining sufficient
diversity in the representation space betWeen the Q-functions in the ensembles. If the Q-functions
in the ensembles converge to a common representation (We Will shoW that this is the case in many
scenarios), the performance of these approaches significantly degrades.
1
Under review as a conference paper at ICLR 2021
In this paper we propose to use cross-learner regularizers to prevent the collapse of the representation
space in ensemble-based Q-learning methods. Intuitively, these representations capture an inductive
bias towards more diverse representations. We have investigated five different regularizers. The
mathematical formulation of four of the regularizers correspond to inequality measures borrowed
from economics theory. While in economics, high inequality is seen as a negative, in this case we use
the metrics to encourage inequality between the representations. The fifth regularizer is inspired from
consensus optimization.
There is a separate line of reinforcement learning literature where ensembles are used to address
several different issues (Chen et al., 2017; Chua et al., 2018; Kurutach et al., 2018; Lee et al., 2020;
Osband et al., 2016) such as exploration and error propagation but we limit our solution to algorithms
addressing the overestimation bias problem only.
To summarize, our contributions are following:
1.	We show that high representation similarity between neural network based Q-functions leads
to decline in performance in ensemble based Q-learning methods.
2.	To mitigate this, we propose five regularizers based on inequality measures from eco-
nomics theory and consensus optimization that maximize representation diversity between
Q-functions in ensemble based Q-learning methods.
3.	We show that applying the proposed regularizers to the MaxminDQN and EnsembleDQN
methods can lead to significant improvement in performance over a variety of benchmarks.
2	Background
Reinforcement learning considers an agent as a Markov Decision Process (MDP) defined as a five
element tuple (S, A, P, r, γ), where S is the state space, Ais the action space, P : S × A× S → [0, 1]
are the state-action transition probabilities, r : S × A × S → R is the reward mapping and
γ → [0, 1] is the discount factor. At each time step t the agent observes the state of the environment
st ∈ S and selects an action at ∈ A. The effect of the action triggers a transition to a new state
st+1 ∈ S according to the transition probabilities P , while the agent receives a scalar reward
Rt = r (st, at, st+1). The goal of the agent is to learn a policy π that maximizes the expectation of
the discounted sum of future rewards.
One way to implicitly learn the policy π is the Q-learning algorithm that estimates the expected sum
of rewards of state st if we take the action at by solving the Bellman equation
Q* (st,at)= E Rt + maxQ*(st+ι,a0)
a0∈A
The implicit policy π can extracted by acting greedily with respect to the optimal Q-function:
arg max Q* (s, a). One possible way to estimate the optimal Q-value is by iteratively updating it for
a∈A
sampled states st and action at using
Q* (st, at)	J Q*	(st, at) + α (Yt	—	Q* (st,	at))	where	Yt	=	Rt	+ max Q*	(st+ι, a0)
a0∈A
where α is the step size and Yt is called the target value. While this algorithm had been initially
studied in the context of a tabular representation of Q for discrete states and actions, in many practical
applications the Q value is approximated by a learned function. Since the emergence of deep learning,
the preferred approximation technique is based on a deep neural network. DQN (Mnih et al., 2015),
had demonstrated super-human performance in Atari Games, but required a very large number of
training iterations. From this baseline, subsequent algorithms improved both the learning speed
and achievable performance, with one of the main means for this being techniques to reduce the
overestimation bias of the Q-function.
EnsembleDQN (Anschel et al., 2017) uses an ensemble of N neural networks to estimate state-action
values and uses their average to reduce both overestimation bias and estimation variance. Formally,
2
Under review as a conference paper at ICLR 2021
the target value for EnsembleDQN is calculated using
1N
QE S = N工 Qi (∙)
YtE = Rt + m0 ax QE (st+1, a0)	(1)
More recent, MaxminDQN (Lan et al., 2020) addresses the overestimation bias using order statistics,
using the ensemble size N as a hyperparameter to tune between underestimating and overestimating
bias. The target value for MaxminDQN is calculated using
Qm (∙, •)= min Qi(∙, ∙)
i=1,...,N
YtM = Rt + max QM (st+1, a0)	(2)
3	Related Work
Techniques to Address Overestimation Bias in RL: Addressing overestimation bias is a long
standing research topic not only in reinforcement learning but other fields of science such as economics
and statistics. It is commonly known as max-operator bias in statistics (D’Eramo et al., 2017)
and as the winner’s curse in economics (Thaler, 2012; Smith & Winkler, 2006). To address this,
(van Hasselt, 2010) proposed Double Q-learning, subsequently adapted to a neural network based
function approximators as Double DQN (Hado van Hasselt et al., 2016). Alternatively, (Zhang et al.,
2017; Lv et al., 2019) proposed weighted estimators of Double Q-learning and (Lee et al., 2013)
introduced a bias correction term. Other approaches to address the overestimation are based on
averaging and ensembling. Techniques include averaging Q-values from previous N versions of
the Q-network (Anschel et al., 2017), taking linear combinations of min and max over the pool of
Q-values (Kumar et al., 2019), or using a random mixture from the pool (Agarwal et al., 2019).
Regularization in Reinforcement Learning: Regularization in reinforcement learning has been
used to perform effective exploration and learning generalized policies. For instance, (Grau-Moya
et al., 2019) uses mutual-information regularization to optimize a prior action distribution for better
performance and exploration, (Cheng et al., 2019) regularizes the policy ∏(a∣s) using a control
prior, (Galashov et al., 2019) uses temporal difference error regularization to reduce variance in
Generalized Advantage Estimation (Schulman et al., 2016). Generalization in reinforcement learning
refers to the performance of the policy on different environment compared to the training environment.
For example, (Farebrother et al., 2018) studied the effect of L2 norm on DQN on generalization, (Tobin
et al., 2017) studied generalization between simulations vs. the real world, (Pattanaik et al., 2018)
studied parameter variations and (Zhang et al., 2018) studied the effect of different random seeds in
environment generation.
Representation Similarity: Measuring similarity between the representations learned by different
neural networks is an active area of research. For instance, (Raghu et al., 2017) used Canonical
Correlation Analysis (CCA) to measure the representation similarity. CCA find two basis matrices
such that when original matrices are projected on these bases, the correlation is maximized. (Raghu
et al., 2017; Mroueh et al., 2015) used truncated singular value decomposition on the activations to
make it robust for perturbations. Other work such as (Li et al., 2015) and (Wang et al., 2018) studied
the correlation between the neurons in the neural networks.
4	Maximizing Representation Diversity in Ensemble-Based Deep
Q-Learning
The work described in this paper is based on the conjecture that while ensemble-based deep Q-
learning approaches aim to reduce the overestimation bias, this only works to the degree that the
neural networks in the ensemble use diverse representations. If during training, these networks
collapse to closely related representations, the learning performance decreases. From this idea, we
propose to use regularization techniques to maximize representation diversity between the networks
of the ensemble.
3
Under review as a conference paper at ICLR 2021
4.1	Representation Similarity Measure
Let X ∈ Rn×p1 denote a matrix of activations of p1 neurons for n examples and Y ∈ Rn×p2
denote a matrix of activations of p2 neurons for the same n examples. Furthermore, we consider
Kij = k (xi, xj) and Lij = l (yi, yj) where k and l are two kernels.
Centered Kernel Alignment (CKA) (Kornblith et al., 2019; Cortes et al., 2012; Cristianini et al.,
2002) is a method for comparing representations of neural networks, and identifying correspondences
between layers, not only in the same network but also on different neural network architectures.
CKA is a normalized form of Hilbert-Schmidt Independence Criterion (HSIC) (Gretton et al., 2005).
Formally, CKA is defined as:
HSIC (K, L)
CKA (K, L) =
∙√HSIC (K, K) ∙ HSIC (L, L)
HSIC is a test statistic for determining whether two sets of variables are independent. The empirical
estimator of HSIC is defined as:
HSIC (K,L) = --1~6(KHLH)
(n - 1)2
1T
where H is the centering matrix Hn = In-11T.
0)
Am
23
ErQa-I a6ea><
0.95
Episode 1000
Episode 2000
0.68
0.59
Episode 500
0.25 0.31
0.68
-1.0
-1.0
0.24
0.67
0.44
0.20
0.12 0.00 0.00
0.99
0.18 0.22 0.26
0.15
0.82
0.55
0.91
0.92
0.96
0.00
-0.8
-0.8
0.79
0.85
0.87
0.38
0.26
-0.6
-0.6
0.39 0.73
0.75 0.40
0.74 0.13
0.68	0.50
0.76 0.17
0.90
0.71
0.78
0.20
1	2	3	4	5
Layer
Heatmap at point A
Episode 3000
0.19
0.57
0.05 0.00 0.00
0.97
0.94
0.93
0.97
0.00
0.55
0.92
0.96
0.97
0.00
-0.4
0.79
0.87
0.89
0.45
0.15
-0.4
0.57
0.97
0.97
0.97
0.00
0.68
0.96
0.96
0.96
0.17
-0.2
0.80
0.90
0.89
0.49
0.20
-0.2
0.70
0.97
0.94
0.93
0.09
0.97
0.68 0.58 0.55
0.29
0.97
0.80 0.79
0.39
0.15
-0.0
0.97
0.70 0.58 0.55
0.31
-ι.o
-0.8
-0.6 ⅛
-0.4ω
-0.2 U
-0.0
-1.0
-0.8
-0.6 ⅛
-0.4ω
-0.2 U
l-0.0
0)
Am
23
Φ
g
L 0.0
1	2	3	4	5
Layer
Heatmap at point B
1	2	3	4	5
Layer
Heatmap at point C
1	2	3	4	5
Layer
Heatmap at point D
Figure 1: The training graph and CKA similarity heatmaps of a MaxminDQN agent with 2 neural
networks. The letters on the plot show the time when CKA similarities were calculated. Heatmaps at
A and C have relatively low CKA similarity and have relatively higher average return as compared to
heatmaps at point B and D that have extremely high similarity across all the layers.
4.2	Correlation B etween Performance and Representation Similarity
The work in this paper starts from the conjecture that high representation similarity between neural
networks in an ensemble-based Q-learning technique correlates to poor performance. To empirically
verify our hypothesis, we trained a MaxminDQN agent with two neural networks on the Catcher
4
Under review as a conference paper at ICLR 2021
environment (Qingfeng, 2019) for about 3000 episodes (5 × 106 training steps) and calculated the
CKA similarity with a linear kernel after every 500 episodes. The training graph along with the
CKA similarity heatmaps are shown in Figure 1. Notably at episode 500 (heatmap A) and episode
2000 (heatmap C), the representation similarity between neural networks is low but the average
return is relatively high. In contrast, at episode 1000 (heatmap B) and episode 3000 (heatmap D) the
representation similarity is highest but the average return is lowest.
Additionally, in Appendix A.1, we performed a regression experiment to demonstrate that when
two neural networks trained on same data, despite having different architecture, learning rate and
batch size can learn almost identical representations. This experiment also demonstrates that random
initialization of neural networks enforces diversity is a misconception.
4.3	Regularization for Maximizing Representation Diversity
In order to maximize the representation diversity, we propose to regularize the training algorithm
with an additional criteria that favors diversity in the representation space. In the following, N is the
number of neural networks in the ensemble, `i is the L2 norm of the i-th neural network’s parameters,
' is the mean of all the L2 norms and ' is the list of all the L2 norms.
The first four metrics we consider are based on inequality measures from economic theory. While in
economics, inequality is usually considered something to be avoided, in our case we aim to increase
inequality (and thus, representation diversity).
The Atkinson Index (Atkinson et al., 1970) measures income inequality and is useful in identifying
the end of the distribution that contributes the most towards the observed inequality. Formally, it is
defined as
1 - KN X '尸)1-at, for 0 ≤ eat = 】，
Ae =	/	iN1、工	⑶
1 1 1 NT . Νν
1 - I (Nn'i)，	for eat = 1，
i=1
where eat is the inequality aversion parameter used to tune the sensitivity of the measured change.
When eat = 0, the index is more sensitive to the changes at the upper end of the distribution, while
the index becomes more sensitive towards the change at the lower end of the distribution when eat
approaches 1.
The Gini coefficient (Allison, 1978) is a statistical measure of the wealth distribution or income
inequality among a population and defined as the half of the relative mean absolute difference:
G
p3 Pj= "j i
2N 2I
(4)
The Gini coefficient is more sensitive to deviation around the middle of the distribution than at the
upper or lower part of the distribution.
The Theil index (Johnston, 1969) measures redundancy, lack of diversity, isolation, segregation
and income inequality among a population. Using the Theil index is identical to measuring the
redundancy in information theory, defined as the maximum possible entropy of the data minus the
observed entropy:
1 _ ' e 'i 1 'i
TT = —〉 — ln
T N乙'	'
i=1
(5)
The variance of logarithms (Ok & Foster, 1997) is a widely used measure of dispersion with natural
links to wage distribution models. Formally, it is defined as:
1N
Vl(') =	Rn 'i- ln g(2)]2	(6)
where g(') is the geometric mean of ' defined as (QN=ι 'i)1/N.
5
Under review as a conference paper at ICLR 2021
The final regularization method we use is inspired from consensus optimization. In a consensus
method (Boyd et al., 2011), a number of models are independently optimized with their own task-
specific parameters, and the tasks communicate via a penalty that encourages all the individual
solutions to converge around a common value. Formally, it is defined as
M = K- 4ik2	(7)
We will refer this regularizer as MeanVector throughout this paper.
4.4 Training Algorithm
Using the regularization functions defined above, we can develop diversity-regularized variants of the
the MaxminDQN and EnsembleDQN algorithms. The training technique is identical to the algorithms
described in (Lan et al., 2020) and (Anschel et al., 2017), with a regularization term added to the loss
of the Q-functions. The loss term for i-th Q-function with parameters ψi is:
L (ψi) = Es,a,r,s0 [(Qψ (s, a) - Y)2] - λI (0i, '),
where Y is the target value calculated using either Equation (1) or Equation (2) depending on the
algorithm, I is the regularizer of choice from the list above and λ is the regularization weight. Notice
that the regularization term appears with a negative sign, as the regularizers are essentially inequality
metrics that we want to maximize. For completeness, the algorithm are shown in Appendix B.
5	Experiments
5.1	Training Curves
We chose three environments from PyGames(Qingfeng, 2019) and MinAtar (Young & Tian, 2019):
Catcher, Pixelcopter and Asterix. These environments were used by the authors in the Max-
minDQN (Lan et al., 2020) paper. We reused all the hyperparameter settings from (Lan et al., 2020)
except the number of neural networks, which we limited to four and trained each solution for five fixed
seeds. For the regularization weight λ, we chose the best value from {10-5, 10-6, 10-7, 10-8}. The
baselines were also fine-tuned. The complete list of training parameters can be found in Appendix E.
(a) Catcher
(b) PixelCopter
(c) Asterix
Figure 2: Training curves and 95% confidence interval (shaded area) for the best augmented variants
for MaxminDQN and EnsembleDQN together with baseline algorithms.
Figure 2 shows the training curves for the three environments. To avoid crowding the figures, for
each environment and baseline algorithm (MaxminDQN and EnsembleDQN) we only plotted the
regularized version which performed the best. We also show as baseline the original MaxminDQN
and EnsembleDQN, as well as the DQN and DDQN algorithms. For the Catcher environment, both
Gini-MaxminDQN and VOL-EnsembleDQN were able to quickly reach the optimal performance
and stabilized after 2 × 106 training steps while the baseline MaxminDQN reached its maximum
performance after 3.5 × 106 training steps but went down afterwards. Similarly, the baseline
EnsembleDQN reached its maximum performance after 4 × 106 training steps, with the performance
fluctuating with continued training. For the PixelCopter environment, VOL-MaxminDQN and Theil-
EnsembleDQN were slower in the initial part of the learning that some of the other approaches, but
6
Under review as a conference paper at ICLR 2021
over time they achieved at least double return compared to the other approaches. Similarly, for the
Asterix environment, Atkinson-MaxminDQN and Theil-EnsembleDQN lagged in training for about
1 × 106 training steps but after that they achieved at least 50% higher return compared to the baselines.
Full results together with CKA similarity heatmaps are shown in Appendix C.
5.2	t-SNE Visualizations
To visualize the impact of the regularization, Figure 3 shows t-SNE (van der Maaten & Hinton,
2008) visualization of the activations of the last layer of the trained networks. Figure 3a show the
network trained for the Catcher environment, while Figure 3b, the network trained for the PixelCopter
environment. The upper row of the figure shows the original, unregularized models, while the
lower row a regularized version. For all combinations, we find that the activations from the original
MaxminDQN and EnsembleDQN versions do not show any obvious pattern, while the regularized
ones show distinct clusters. An additional benefit of t-SNE visualizations over CKA similarity
heatmaps is that the CKA similarity heatmaps are useful to show representation similarity between
two neural networks, but they become counter intuitive as the number of neural networks increases.
More t-SNE visualizations for four neural network experiments are shown in Appendix C.3.
Baseline MaxminDQN
Baseline EnsembIeDQN
Baseline MaxminDQN
Baseline EnsembIeDQN
Figure 3: Clustering last layer activations from Catcher and PixelCopter after processing them
with t-SNE to map them in 2D. The regularized variants have visible clusters while the baseline
MaxminDQN and EnsembleDQN activations are mixed together with no visible pattern.
5.3	Statistical Analysis
What is the impact of the regularization on the performance? Similar to the approach taken
by (Liu et al., 2020), we performed a z-score test to rigorously evaluate the improvement of regular-
ization over baseline solutions. The z-score is also known as “standard score”, the signed fractional
number of standard deviations by which the value of a data point is above the mean value. A
regularizer’s z-score roughly measures its relative performance among others. For each algorithm,
environment and neural network setting, we calculated the z-score for each regularization method
and the baseline by treating results all the results as a populations. For example, to find out which
EnsembleDQN with two neural networks is best for the Catcher environment, we took the average
reward of 10 episodes for each experiment ((5 + 1) × 5 seeds) and treated it as a population. Finally,
we averaged the z-scores to generate the final result presented in Table 1. In terms of improved
performance, all the regularizers have achieved significant improvement over the baselines for all
three environments. The z-scores for four neural network experiments are shown in Appendix C.4.
Is the improvement statistically significant? We collected the z-scores from the previous section
and performed the Welch’s t-test with the corresponding z-scores produced by the baseline. The
resulting p-values are presented in Table 2. From the results, we observed that the improvement
introduced from regularization is statistically significant (p < 0.05) in almost all the cases.
7
Under review as a conference paper at ICLR 2021
Table 1: Averaged z-scores for each regularization method.
Reg	Ensemble N=2			Maxmin N=2			Ensemble N=3			Maxmin N=3		
	Catcher	Copter	Asterix	Catcher	Copter	Asterix	Catcher	Copter	Asterix	Catcher	Copter	Asterix
Baseline	-2.125	-2.044	-1.868	-2.143	-2.031	-1.730	-1.931	-2.042	-1.634	-1.702	-2.115	-1.478
Atkinson	0.419	0.450	0.187	0.353	0.402	0.459	0.379	0.292	0.197	0.344	0.380	0.795
Gini	0.422	0.202	0.659	0.539	0.446	0.201	0.349	0.231	0.449	0.319	0.624	0.068
MeanVector	0.426	0.359	0.372	0.529	0.079	0.315	0.403	0.763	0.227	0.347	0.316	0.174
Theil	0.425	0.358	0.767	0.198	0.576	0.364	0.402	0.284	0.429	0.341	0.499	0.068
VOL	0.433	0.315	0.341	0.522	0.526	0.392	0.397	0.471	0.149	0.332	0.297	0.372
Table 2: P-values from Welch’s t-test comparing the z-scores of regularization and baseline
Reg	Ensemble N=2			Maxmin N=2			Ensemble N=3			Maxmin N=3		
	Catcher	Copter	Asterix	Catcher	Copter	Asterix	Catcher	Copter	Asterix	Catcher	Copter	Asterix
Atkinson	0.002	0.000	0.003	0.000	0.000	0.000	0.019	0.000	0.000	0.061	0.000	0.003
Gini	0.002	0.000	0.001	0.000	0.000	0.000	0.002	0.003	0.003	0.061	0.000	0.013
MeanVector	0.002	0.000	0.000	0.000	0.008	0.016	0.019	0.000	0.000	0.061	0.000	0.030
Theil	0.002	0.002	0.002	0.000	0.000	0.002	0.019	0.001	0.006	0.061	0.001	0.026
VOL	0.002	0.000	0.000	0.000	0.005	0.001	0.019	0.000	0.017	0.060	0.000	0.008
6 Identical Layers Experiment
To test the limits of the regularizers, we initialized, each layer of each neural network with the same
fixed seed. This initialization enforces maximum representation similarity and is considered the
worst case scenario for ensemble based learning methods. We performed this experiment on all three
environments and used the same seeds and hyperparameters that were used for the main experiments.
The training curves are shown in Figure 4. Notably, the results from the baseline MaxminDQN and
EnsembleDQN on both Catcher and PixelCopter environments are similar to the main results. For the
Catcher environment, both Gini-MaxminDQN and Theil-EnsembleDQN were slow in learning for
about 2 × 106 training steps but both solutions were able to achieve the optimal performance by the
end of training. Similarly for PixelCopter environment, the VOL-MaxminDQN was slow in learning
till 1.5 × 106 training steps but it was able to outperform the baseline results and achieved optimal
performance. The complete training plots for these experiments are shown in Appendix D.
60
50
40
30
20
10
D
-10
(a) Catcher
80
⊂
⅛60
Φ
(υ 40
6
∈
SJ 20
<
0
0.5	1	1.5	2
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
(b) PixelCopter
(c) Asterix
Figure 4: Training plots representing the best results from each solution for Catcher, PixelCopter and
Asterix environment when the layers of the neural networks were initialized with one fixed seed.
7 Conclusion
In this paper we showed that high representation similarity between the Q-functions in ensemble
based Q-learning algorithms such as MaxminDQN and EnsembleDQN leads to a decline in learning
performance. To mitigate this, we proposed a regularization approach using five different metrics to
maximize the diversity in the representation space of the Q-functions. Experiments have shown that
our solution outperforms baseline MaxminDQN and EnsembleDQN in standard training settings as
well as in scenarios where the parameters of the neural layers were initialized using one fixed seed.
8
Under review as a conference paper at ICLR 2021
References
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. Striving for simplicity in off-policy
deep reinforcement learning. arXiv preprint arXiv:1907.04543, 2019.
Paul D. Allison. Measures of inequality. American Sociological Review, 43(6):865-880,1978. ISSN
00031224. URL http://www.jstor.org/stable/2094626.
Oron Anschel, Nir Baram, and Nahum Shimkin. Averaged-DQN: Variance reduction and stabilization
for deep reinforcement learning. In Proceedings of the International Conference on Machine
Learning (ICML-2017), pp. 176-185, 2017.
Anthony B Atkinson et al. On the measurement of inequality. Journal of economic theory, 2(3):
244-263, 1970.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and Trends
in Machine Learning, 3(1):1-122, 2011.
Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via q-ensembles.
arXiv preprint arXiv:1706.01502, 2017.
Richard Cheng, Abhinav Verma, Gabor Orosz, Swarat Chaudhuri, Yisong Yue, and Joel Burdick. Con-
trol regularization for reduced variance reinforcement learning. arXiv preprint arXiv:1905.05380,
2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems, pp. 4754-4765, 2018.
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels based on
centered alignment. Journal of Machine Learning Research, 13(Mar):795-828, 2012.
Nello Cristianini, John Shawe-Taylor, Andre Elisseeff, and Jaz S Kandola. On kernel-target alignment.
In Proceedings of the Advances in Neural Information Processing Systems (NIPS-2002), pp. 367-
373, 2002.
Carlo D’Eramo, Alessandro Nuara, Matteo Pirotta, and Marcello Restelli. Estimating the maximum
expected value in continuous reinforcement learning problems. In Proceedings of the Conference
on Artificial Intelligence (AAAI-2017), 2017.
Jesse Farebrother, Marlos C. Machado, and Michael Bowling. Generalization and regularization in
DQN, 2018.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Alexandre Galashov, Siddhant Jayakumar, Leonard Hasenclever, Dhruva Tirumala, Jonathan Schwarz,
Guillaume Desjardins, Wojciech Czarnecki, Yee Whye Teh, Razvan Pascanu, and Nicolas Heess.
Information asymmetry in KL-regularized RL. In Proceedings of the International Conference on
Learning Representation (ICLR-2019), 2019.
Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx. Soft Q-learning with mutual-information
regularization. In Proceedings of the International Conference on Learning Representations
(ICLR-2019), 2019. URL https://openreview.net/forum?id=HyEtjoCqFX.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Scholkopf. Measuring statistical
dependence with hilbert-schmidt norms. In Algorithmic Learning Theory (ALT), pp. 63-77, 2005.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
Hado Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double
Q-learning. In Proceeding of Conference on Artificial Intelligence (AAAI-2016), 2016.
9
Under review as a conference paper at ICLR 2021
J. Johnston. H. TheiL Economics and Information Theory. The Economic Journal, 79(315):601-602,
09 1969. ISSN 0013-0133. doi: 10.2307/2230396. URL https://doi.org/10.2307/
2230396.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In Proceedings of the International Conference on Machine
Learning (ICML-2019), pp. 3519-3529, 09-15 Jun 2019.
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
Q-learning via bootstrapping error reduction. In Proceedings of the Advances in Neural Information
Processing Systems (NeurIPS-2019), pp. 11761-11771, 2019.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. In International Conference on Learning Representations, 2018.
Qingfeng Lan, Yangchen Pan, Alona Fyshe, and Martha White. Maxmin Q-learning: Controlling the
estimation bias of Q-learning. In Proceeding of the International Conference on Learning Represen-
tations (ICLR-2020), 2020. URL https://openreview.net/forum?id=Bkg0u3Etwr.
Donghun Lee, Boris Defourny, and Warren B. Powell. Bias-corrected Q-learning to Control Max-
operator Bias in Q-learning. In IEEE Symposium on Adaptive Dynamic Programming and Rein-
forcement Learning, pp. 93-99, 2013.
Kimin Lee, Laskin Michael, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified
framework for ensemble learning in deep reinforcement learning. arXiv preprint arXiv:2007.04938,
2020.
Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do
different neural networks learn the same representations? In Proceedings of the International
Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015, volume 44, pp.
196-212, 2015.
Zhuang Liu, Xuanlin Li, Bingyi Kang, and Trevor Darrell. Regularization matters in policy optimiza-
tion. arXiv preprint arXiv:1910.09191, 2020. URL https://openreview.net/forum?
id=B1lqDertwr.
P. Lv, X. Wang, Y. Cheng, and Z. Duan. Stochastic double deep q-network. IEEE Access, 7:
79446-79454, 2019. ISSN 2169-3536.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, February 2015. ISSN 00280836.
Youssef Mroueh, Etienne Marcheret, and Vaibhava Goel. Multimodal retrieval with asymmetrically
weighted truncated-svd canonical correlation analysis. CoRR, abs/1511.06267, 2015.
Efe A. Ok and James Foster. Lorenz Dominance and the Variance of Logarithms. Working Papers
97-22, C.V. Starr Center for Applied Economics, New York University, 1997. URL https:
//ideas.repec.org/p/cvs/starer/97-22.html.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. In Advances in neural information processing systems, pp. 4026-4034, 2016.
Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Ro-
bust deep reinforcement learning with adversarial attacks. In Proceedings of the International
Conference on Autonomous Agents and MultiAgent Systems (AAMAS-2018), pp. 2040-2042, 2018.
Lan Qingfeng. Gym compatible games for reinforcenment learning. https://github.com/
qlan3/gym-games, 2019.
10
Under review as a conference paper at ICLR 2021
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In Proceedings of
the Advances in Neural Information Processing Systems (NIPS-2017),pp. 6076-6085, 2017.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In Proceedings of the International
Conference on Learning Representations (ICLR-2016), 2016.
James E Smith and Robert L Winkler. The optimizer’s curse: Skepticism and postdecision surprise in
decision analysis. Management Science, 52(3):311-322, 2006.
Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement Learning in Finite MDPs:
PAC Analysis. Journal of Machine Learning Research (JMLR), 10(Nov):2413-2444, 2009.
Istvan Szita and Andras Lorincz. The Many Faces of Optimism: A Unifying Approach. In Proceed-
ings of International Conference on Machine Learning (ICML-2008), pp. 1048-1055, 2008.
Richard Thaler. The winner’s curse: Paradoxes and anomalies of economic life. Cambridge University
Press, 2012.
Sebastian Thrun and A. Schwartz. Issues in using function approximation for reinforcement learning.
In Proceedings of the Connectionist Models Summer School (CMSS-1993), January 1993.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
randomization for transferring deep neural networks from simulation to the real world. In Proceed-
ings of the International Conference on Intelligent Robots and Systems (IROS-2017), pp. 23-30,
2017.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Ma-
chine Learning Research, 9:2579-2605, 2008. URL http://www.jmlr.org/papers/v9/
vandermaaten08a.html.
Hado van Hasselt. Double Q-learning. In Proceedings of the Advances in Neural Information
Processing Systems (NIPS-2010), pp. 2613-2621, 2010.
Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft. Towards
understanding learning representations: To what extent do different neural networks learn the
same representation. In Proceedings of the Advances in Neural Information Processing Systems
(NIPS-2018), pp. 9584-9593, 2018.
Chris Watkins. Learning from Delayed Rewards. PhD thesis, King’s College, Cambridge, 1989.
Kenny Young and Tian Tian. Minatar: An atari-inspired testbed for thorough and reproducible
reinforcement learning experiments. arXiv preprint arXiv:1903.03176, 2019.
Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep
reinforcement learning. arXiv preprint arXiv:1804.06893, 2018.
Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double Q-learning. In
Proceedings of the International Joint Conferences on Artificial Intelligence (IJCAI-2017), pp.
3455-3461, 2017.
11
Under review as a conference paper at ICLR 2021
A	Supplementary Material
A. 1 Motivating Example to Demonstrate Similarity Between Neural Networks
We performed a regression experiment in which we learnt a sine wave function using two different
three layered fully connected neural networks with 64 and 32 neurons in each hidden layer with ReLU.
The neural networks were initialized using different seeds and were trained using different batch sizes
(512, 128) and learning rates (1e-4, 1e-3). The Figure 5a shows the learnt functions while Figure 5b
represents their CKA similarity heatmap before and after training. The odd numbered layers represent
pre-ReLU activations while the even numbered layers represent post-ReLU activations. It can be seen
that before training, the CKA similarity between the two neural networks from layer 4 and onward is
relatively low and the output being 0% similar while after training, the trained networks have learnt
highly similar representation while their output being 98% similar.
∕%Az
∕N√V
(a)	Regression using two different
neural networks
0.77
0.90
0.85
0.93
0.91
0.79
0.94
0.99
0.98
0.86
0.96
0.99
0.99
0.98
1.00
0.99
0.84
0.97
0.99
0.99
0.84
1.00
0.97
1
2
0.76
Before Training
0.97
0.76
0.91
0.69
0.65
0.75
0.72
0.72
0.63
4	)
Layer
0.87
0.59
0.64
0.64
0.60
0.60
0.49
0.00
0.75
0.58
0.55
0.46
0.52
0.29
0.80
0.85
0.97
0.97
1.00
After Training
.30
0.98
0.83 0.78 0.89
0.96 0.90 0.88
.67
0.89
0.87
0.94
0.93
1.00
0.99
0.97
0.96 0.89 0.86
1.00
1.00
0.97
0.84
0.81
.64
0.60
0.75 0.55 0.52
4 5 6
Layer
0.49
0.34
0.29
0.29
0.29
0.31
0.6 22
0.4
-0.2
E
in
S
U
0.8、
-ι.o
-o.o
(b)	CKA similarity heatmap between different layers of the two neural
networks used for the regression experiment.

3
5
6
7
1
2
3
7
Figure 5:	Left: Fitting a sine function using two different neural network architectures. The upper
function was approximated using 64 neurons in each hidden layer while the lower function used
32 neurons in each hidden layer. Right: Represents the CKA similarity heatmap between different
layers of both neural networks before and after training. The right diagonal (bottom left to top right)
measures representation similarity of the corresponding layers of both neural networks. The trained
networks have learnt similar representations while their output was 98% similar.
This example shows that neural networks can learn similar representation while trained on differ-
ent batches. This observation is important because in MaxminDQN and EnsembleDQN training,
each neural network is trained on a separate batch from the replay buffer but still learns similar
representation similarity (see Figure 8).
12
Under review as a conference paper at ICLR 2021
B Algorithms
For completeness, the regularizered MaxminDQN and EnsembleDQN algorithms are given below
Algorithm 1: Regularized MaxminDQN
The differences between the baseline MaxminDQN and regularized MaxminDQN are highlighted
Initialize N Q-functions {Q1, . . . , QN} parameterized by {ψ1, . . . , ψN}
Initialize empty replay buffer D
Observe initial state s
while Agent is interacting with the Environment do
Qmin(S, a) — mink∈{i,...,N} Qk(s, a), ∀a ∈ A
Choose action a by -greedy based on Qmin
Take action a, observe r, s0
Store transition (s, a, r, s0) in D
Select a subset S from {1, . . . , N} (e.g., randomly select one i to update)
for i ∈ S do
Sample random mini-batch of transitions (sD, aD, rD, s0D) from D
Get update target: YM — IrD + Y maXɑθ∈A Qmin(SD,α0)
Generate list of L2 norms : ` = kψ1 k2, . . . , kψN k2
Update Qi byminimizingEsD,aD,rD,s0D Qiψi (SD,aD) - YM2 -λI (`i,`)
end
S — S0
end
Algorithm 2: Regularized EnsembleDQN
The differences between the baseline EnsembleDQN and regularized EnsembleDQN are
highlighted
Initialize N Q-functions {Q1, . . . , QN} parameterized by {ψ1, . . . , ψN}
Initialize empty replay buffer D
Observe initial state S
while Agent is interacting with the Environment do
Qens(S, a) - NNP= Qi(s,α)
Choose action a by -greedy based on Qens
Take action a, observe r, S0
Store transition (S, a, r, S0) in D
Select a subset S from {1, . . . , N } (e.g., randomly select one i to update)
for i ∈ S do
Sample random mini-batch of transitions (SD, aD, rD, S0D) from D
Get update target: YE - rD + γ maxa0∈A Qens (S0D, a0)
Generate list of L2 norms : ` = kψ1 k2, . . . , kψN k2
Update Qi byminimizingEsD,aD,rD,s0D Qiψi (SD,aD) - YE2 -λI (`i,`)
end
S - S0
end
13
Under review as a conference paper at ICLR 2021
C All Training Plots

C.1 Training Plots for MaxminDQN
ET-? ΦOT2Φ><
0.5	1	1.5	2
Training Episodes (×106)
60
30
20
10
50
40
Maxmin N-2
Maxmin N-3
0	1	2	3	4	5
Training Episodes (×106)
Maxmin N-4
Atkinson N-2
Atkinson N-3
Atkinson N-4
0	1	2	3	4	5
Training Episodes (×106)
1	2	3	4	5
Training Episodes (×106)
80
60
40
20
O
0.5	1	1.5	2
Training Episodes (×106)
30
20
10
60
50
40
0	1	2	3	4	5
Training Episodes (×106)
1	2	3	4	5
Training Episodes (×106)
Maxmin N-4
MeanVector N≡2
MeanVector N-3
MeanVector N-4
⊂ 50
¾40
H
(υ 30
6
ns
φ20
< 10
60
D
0	1	2	3	4	5
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
Ooooooo
6 5 4 3 2 1
UJn-MaJ ΦOT2Φ><
1	2	3	4	5
Training Episodes (×106)
80
⊂
ɔ60
■M
£
φ 40
CT
ΠJ
§ 20
<
0
0.5	1	1.5	2
Training Episodes (×106)
60
E 50
n
Φ 40
α
R 30
P
Φ 20
< 10
0
0	1	2	3	4	5
Training Episodes (×106)
Ooooooo
6 5 4 3 2 1
EniIaJ Φ6ejφ><
Maxmin N≡2
Maxmin N-3
Maxmin N-4
VOL N-2
VOL N-3
VOL N-4
0	1	2	3	4	5
Training Episodes (×106)
0.5
1.5
2
Training Episodes (×106)
60
E 50
n
弦4。
⅛30
ΠJ
φ 20
⅛ɪo
O
0	1	2	3	4	5
Training Episodes (×106)
Catcher
PixelCopter
Asterix
Figure 6:	All MaxminDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance of
Logarithms
14
Under review as a conference paper at ICLR 2021
C.2 Training Plots for EnsembleDQN
u」n$」ΦOT2Φ><
5
■■ Ensemble N*2
IM Ensemble N-3
Ensemble N-4
Atkinson N-2
Mkinson N-3
Mkinson N-4
0.5
1.5
2
D
0	12	3	4
Training Episodes (×106)
Training Episodes (×106)
1	2	3	4	5
Training Episodes (×106)
QQQo
8 6 4 2
EnlIaJ Θ6ejθ><
Ensemble N-2
Ensemble N-3
Ensemble N-4
Gini N-2
Gini N-3
Gini N-4
1	2	3	4	5
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
Oooooooo
6 5 4 3 2 1 1
-
号-MaJ Φ6ejφ><
Ooooooo
6 5 4 3 2 1
号-MaJ Φ6ejφ><
Ensemble N-2
■■ Ensemble N-4
Meanvector N-2
Meanvector N-3
1	2	3	4	5
Training Episodes (×106)
Oooooooo
6 5 4 3 2 1 1
-
UJnlIaJ Φ6ejθ><
■■ Ensemble N≡2
■ Ensemble N≡3
■ Ensemble N-4
■ Tħeil N-2
■ Tħeil N-3
Theil N-4
1	2	3	4	5
Training Episodes (×106)
■■ Ensemble N*2
IM Ensemble N-3
Ensemble N-4
VOL N-2
VOL N-3
VOL N-4
0	1	2	3	4	5
Training Episodes (×106)
Catcher
Meanvector N-4
80

Φ 40
CT
ΠJ
W 20
<
0
0.5	1	1.5	2
Training Episodes (×106)
0.5
1.5
2
Training Episodes (×106)
PixelCopter
70
c60
⅛5°
(υ
40
(υ
OT 30
P
(υ 20
D
0	1	2	3	4	5
Training Episodes (×106)
60
号5。
3 40
⅛30
ΠJ
φ 20
^ɪo
D
Ensemble N≡2
Ensemble N-3
Ensemble N-4
VOL N-2
VOL N-3
VOL N-4
0	1	2	3	4	5
Training Episodes (×106)
Asterix
Figure 7: All EnsembleDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance
of Logarithms
15
Under review as a conference paper at ICLR 2021
C.3 Heatmaps and t-SNE Visualizations
Figure 8 represents the CKA similarity heatmaps using a linear kernel of all the two neural network
experiments after training averaged over all the five seeds. The point of interest is the right diagonal
(bottom left to top right) that represents the representation similarity between corresponding layers.
For the baseline experiments, the output layer has more than 96% similarity in almost all the
scenarios while for the regularized versions have around 90% similarity in the output layer. This
10% difference provides enough variance in the Q-values to prevent the ensemble based Q-learning
methods converging to the standard DQN.
Catcher	Copter	Asterix	Catcher	Copter
Asterix
<υu=<υSE8 Cowc-V-S
∙)t!∏0.89
Q.83 Q.88 Q.78
Q.96 Q.97 Q.83 Q.85I1
Q.98 Q.97 Q.81 0.82『
0.94
0 .94 √∕l<'II∙F
0.91 0.85 l<
D.92 0.93 1«
0.79 Q.93
Q.95 0.83 W
0.28 0.	26
0.66 0. 0.67 0.	67 63I
0.95 0.94 0.98 0.95	
α.78 Q.82
0 ,8Q
0.93 0.91
D.82
D.97
D.90
□ .79
D.82
0.89
0.91 0.93 0.88
<
0.88 0.95 0.85
0.81 0.79
0.78
0.95 0.92 0.87
.Q6
.62
.65
0.05
0.87 0.90©

0.96 0.97
-C-O -Jot<υ>ue<υn
国 0.90 0.79 0.85 虚 0.82 0.96 0.96 0.83 0.79 次 0.98 0.96 0.81 0.79 世		2 0.89 ^O.77FFΓ 0.94 Q.83	雇 0.90	四
		
0.89 ZBh 0.96 0.98 0.78 J≡ 0.98 0.97 Q.78 0.77		r≡⅞ ^≡0.85 0.94 0.84 B ^^ 0.94 0.96 0.86 0.98 FB	B
0.82
D.93 0.81l<
0.97 0.95 F
0.95 l∙J
0.95
il
D.89 Q.85 l∙Rc<<
0.82
□.97 □.94 l∙)W∙
0.94 Q.95
0.98 0.97
0.81 0.79
0.93
0.85 0.88
.80
0.95 0.97
0.98 0.96
0.92
0.52
0.63 0.49 0.63
0.06	0.07 0.1	1 0.08	0.92
0.62			0.07
0.54			0.13
0.68			0.08
0.70	0.69 0.4	9 0.46	I 0.07
0.97
0.82
0.07
0.08
0.05
,12	0.1
.40	0.5
,48	0.6
0.98
		
i		0.91 0.86 0.90 0.92
S	0.95	
0.93	0.84	
			0.91
i		0.90 0.87 0.87 Q.90	
	0.93		
0.92	0.83		
I ɪj
0.80
0.78
.30
.15
.39
0.16 0.13 0.15	0.43	0.75
0.23 0.17 0.2。	0.70	0.00
0.96 0.99 0.98	0.19	0.00
0.96 0.99 0.97	0.20	0.00
0.99 0.98 0.99	0.28	0.00
0.08	0.05	0.11	0.23	0.91
0.14	0.69			0.23
0.14	0.68	0.69		0.10
0.27	0.51	0.70	0.68	0.05
0.27	0.85	0.55	0.53	0.06
	0.90	叫小■叫∙F・叫”不叫小10.82
0.82 0.79 0.84 0.88 0.81	国	0.34 0.68 0.68 0.68 0.09
		0 0.8。0.82
0.95 0.96 0.78 0.80	0.341	0.94 Q.78 0.77 睡
0.98 0.95 0.78 0.80	0.321	
o
	0.91		0.10 0.07 Q.Q9 0.06 盥
0.71^^0.61 0.70	0.15		0.34 0.76 0.65 0.69 0.10
0.79 0.86 版	0.44		0.70 0.53 0.45 0.49 0.Q8
0.95 0.98	0.82	0.31		^^75 0.60 0.62 0.06
0.98 0.96	0.83	0.31		0.69 0.49 0.29 0.4。Q.Q5
0.98 0.95
0.79
.35
																	
0.04	0.92		0.28 0.24 Q.29	0.	28I	0.92		0	08	0.06	0.10 Q.Q8	0.87			0.05	0.11 0.26	0.91
0.48	面0.91 0.瑞再		0.81 0.87 国	0.	60	0.25		0		0.88	0.92 0.84	0.07		0.45	0.63	0.93 0.86	0.22
0.52	国 0.92 Q.93^K		0.85 0.84 0.8。	0.	66	0.42		0		0.89	0.89 国	0.13		0.48	0.67	0.91 0.93	0.10
																	
0.82	Q.93	IW		0.94 Q.95 0.83	0.	74	0.37		0	76	0.95	0.89 0.82	0.08		0.76	0.94	0.69 0.67	0.04
0.94	值 0.57 Q.56 0.03		0.97 0.94 0.85	0.78		0.36		0						0.93	0.86	0.57 0.54	0.05
																	
0.03	0.91		0.28 0.24 Q.26	0	26I	0.90		1	06	0.07	0.08 0.08	0.92			0.05	0.15 0.33	0.89
0.47	ffiθ.91 0.88		Q.8Q 0.86 0.80	0	52	0.21			53	0.90	0.83 0.90	0.06		0.45	0.63	0.89 0.85	0.37
0.49	■ 0.87 0.93 国		0.88 0.85 0.81	0	65	0.22			63	0.93	0.81 0.85	0.05		0.50	0.69	0.95 0.92	0.16
0.80	您 0*68 0.67 0.02		0.96 0.96 0.88	0	64	0.37			72	0.98	0.85 0.87	0.07		0.74	0.93	0.69 0.67	0.05
0.92	0.83		0.98 0.94 0.89	0	68	0.39		0.82		0.68	0.39 0.55	0.04		0.93	0.84	0.56 0.53	0.07
0











0
0




F
H


H
0
0
U
H

0
0
(a) EnsembleDQN
(b) MaxminDQN
Figure 8: Heatmaps representing the CKA similarity of 2 neural network experiments.
16
Under review as a conference paper at ICLR 2021
Figure 9 represents the t-SNE visualizations of the baseline and regularized solutions trained with
four neural networks on the PixelCopter environment. This visualization is consistent with the
visualizations shown in Section 5.2 where the baseline activations are cluttered without any pattern
while the Theil-MaxminDQN and Theil-EnsembleDQN activations have visible clusters.
Baseline MaxminDQN
Baseline EnsembIeDQN
Figure 9: Clustering last layer activations from PixelCopter after processing them witht-SNE to map
them in 2D
C.4 z-Score Table for Four Neural Network Experiments
Table 3: Averaged z-scores for each regularization method with four neural networks
Reg	Ensemble N=4			Maxmin N=4		
	CatCher	Copter	Asterix	Catcher	Copter	Asterix
Baseline	-2.096	-2.154	-1.469	-1.785	-1.803	-1.417
Atkinson	0.486	0.146	0.586	0.358	0.479	0.463
Gini	0.409	0.307	0.126	0.340	0.433	0.000
MeanVeCtor	0.452	0.685	0.019	0.362	0.206	0.351
Theil	0.341	0.669	0.596	0.363	0.842	0.384
VOL	0.409	0.374	0.141	0.363	-0.167	0.252
Table 4: P-values from Welch’s t-test comparing the z-scores of regularization and baseline
Reg	Ensemble N=4			Maxmin N=4		
	Catcher	Copter	Asterix	Catcher	Copter	Asterix
Atkinson	0.002	0.005	0.004	0.044	0.018	0.012
Gini	0.002	0.005	0.005	0.045	0.024	0.056
MeanVector	0.002	0.000	0.016	0.044	0.010	0.020
Theil	0.001	0.001	0.007	0.044	0.005	0.014
VOL	0.002	0.003	0.006	0.044	0.024	0.030
17
Under review as a conference paper at ICLR 2021
D Identical Layers Experiment (All Training Plots)
D.1 Training Plots for MaxminDQN
ET-? ΦOT2Φ><
60
50
40
30
20
10
0
E 60
ɔ
■M
φ
≥o
6
e
Φ 20
<
Oooooooo
6 5 4 3 2 1 1
-
号-MaJ Φ6ejφ><
D
Training Episodes (×106)
Ooooooo
6 5 4 3 2 1
Eα? Θ6ejθ><
2
3
40
35
30
<
5
D
2
40
30
ni N-2
ni N-3
ni N-4
Maxmin N-2
Maxmin N-3
Maxmin N-4
620
Maxmin N-2
Maxmin N-3	y.	aV×-⅛
■ Maxmin N-4	∕√ ∖ /V∕√W
■ Gini N-2	J	Vkl Γ
■ Gini N-3	Γ
■ Gini N-4 KJ
Training Episodes (×106)
BO
匚60
40
1.5
Training Episodes (×106)
Training Episodes (×106)
Φ 25
Φ 20
6
fD 15
£10
Training Episodes (×106)
Maxmin N-2
Maxmin N-3
Maxmin N-4
MeanVector N>2
MeanVector NT
Meanvector N-4
Maxmin N-2
Maxmin N-3
Maxmin N-4
Meanvector N-2
Meanvector N-3
Meanvector N-4

Maxmin N-2
Maxmin N-3
Maxmin N-4
Meanvector N-2
Meanvector N-3
Meanvector N-4
1	2	3	4	5
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
Ooooooo
6 5 4 3 2 1
号-MaJ Φ6ejφ><
Maxmin N-2
Maxmin N-3
Maxmin N-4
Theil N-2
Theil N-3
Theil N-4
70
C 60
B 50
£ 40
(υ
CT 30
E
φ 20
< 10
0
1	2	3	4	5
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
1	2	3	4	5
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
Catcher
PixelCopter
Asterix
Figure 10: All MaxminDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance
of Logarithms
18
Under review as a conference paper at ICLR 2021
D.2 Training Plots for EnsembleDQN

1	2	3	4	5
Training Episodes (×106)
80
60
40
20
0
0.5	1	1.5	2
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
Ooooooo
6 5 4 3 2 1
EniIaJ Φ6ejφ><
Ooooooo
6 5 4 3 2 1
Eα? Φ6ejφ><
1	2	3	4	5
Training Episodes (×106)
Ensemble N-2
Ensemble N-4
Gini N-2
Gini N-3
Gini N-4
80
Oooooooo
6 5 4 3 2 1 1
-
号-MaJ Φ6ejφ><
1	2	3	4	5
Training Episodes (×106)
Ooooooo
6 5 4 3 2 1
Eα? Θ6ejθ><
1	2	3	4	5
Training Episodes (×106)
Ensemble N-2
Ensemble N-3
Ensemble N-4
VOL N-2
VOL N-3
VOL N-4
1	2	3	4	5
Training Episodes (×106)
Catcher
E 60
a
≥o
6
e
Φ 20
<
0
0.5	1	1.5	2
Training Episodes (×106)
30
25
20
15
10
5
0	1	2	3	4	5
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
0.5	1	1.5	2
Training Episodes (×106)
PixelCopter
Ensemble N-2
Ensemble N-3
Ensemble N-4
Meanvector N-2
Meanvector N-3
Meanvector N-4
0	1	2	3	4	5
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
0	1	2	3	4	5
Training Episodes (×106)
Asterix
Figure 11: All EnsembleDQN Results. Top to Bottom: Atkinson, Gini, MeanVector, Theil, Variance
of Logarithms
19
Under review as a conference paper at ICLR 2021
E Implementation Details and Hyperparameters
For our implementation of MaxminDQN and EnsembleDQN we used the code provided
by the MaxminDQN authors that has implementations of different DQN based methods
(github.com/qlan3/Explorer). For the baseline experiments, we used most of the hyperparame-
ter settings provided in the configuration files by the authors except learning rates which we limited
to [1e - 3, 1e - 4, 3e - 5] and limited the number of ensembles to four. The complete list of hyper-
parameters for each environment is shown in Table 5. The values in bold represent the values used
for the reported results.
Table 5: List of hyperparameters used for the experiments.
Hyperparameter	Catcher	PiXelCopter	Asterix	|
Learning rate	[1e - 3, 1e - 4, 3e - 5]	[1e - 3, 1e - 4, 3e - 5]	[1e - 3, Ie - 4, 3e - 5]
Batch size	[32, 64]	[32, 128,1024]	[32, 64]
Buffer Size	[1e4, 1e7]	[1e4, 1e6]	[1e5, 2e5]
Exploration Steps	1e3	[1e3, 2e3]	2e5
Hidden Layer Size	[64, 64]	[64, 64]	[64, 64]
Gradient Clip	5	5	-1
Discount Factor γ	0.99	0.99	0.99
Regularization Weight	[1e - 5, 1e - 6]	[1e - 6, Ie - 7, 1e - 8]	[1e - 5, 1e - 6]
			
For the identical layer experiment, no hyperparameter tuning was performed and we reused the
hyperparameters from the main results. In terms of number of experiments, we ran 190 experiments:
(5 regularizers × 5 seeds × 3 ensemble settings × 2 algorithms) + 40 baseline experiments for each
environment totaling 570 runs for all environments after hyperparameter tuning. The same number of
experiments were performed for the identical layer experiment which sums up to 1140 runs where
each run took 11 hours of compute time on average.
F	Plotting the Gini Inequality
We measured the L2 norm inequality of the baseline MaxminDQN and EnsembleDQN along with
their regularized versions. We trained baseline MaxminDQN and EnsembleDQN with two neural
networks along with their Gini index versions with regularization weight of 10-8 on the PixelCopter
environment on a fixed seed . Figure 12 represents the L2 norm inequality of the experiments along
their average return during training. Notably, despite each neural network being trained on a different
batch, the L2 norm of the baseline MaxminDQN and EnsembleDQN are quite similar while the L2
norm of the regularized MaxminDQN and EnsembleDQN have high inequality.
Training Episodes (×106)
Figure 12: Left: Plot representing the L2 norm inequality between the two neural networks using
Gini index trained on PixelCopter environment. Right: Plot representing the average return during
training.
Training Episodes (×106)
20