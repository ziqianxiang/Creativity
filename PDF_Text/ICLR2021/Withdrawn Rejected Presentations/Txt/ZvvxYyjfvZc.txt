Under review as a conference paper at ICLR 2021
Correcting Momentum in Temporal
Difference Learning
Anonymous authors
Paper under double-blind review
Ab stract
A common optimization tool used in deep reinforcement learning is momentum,
which consists in accumulating and discounting past gradients, reapplying them at
each iteration. We argue that, unlike in supervised learning, momentum in Tem-
poral Difference (TD) learning accumulates gradients that become doubly stale:
not only does the gradient of the loss change due to parameter updates, the loss
itself changes due to bootstrapping. We first show that this phenomenon exists,
and then propose a first-order correction term to momentum. We show that this
correction term improves sample efficiency in policy evaluation by correcting tar-
get value drift. An important insight of this work is that deep RL methods are not
always best served by directly importing techniques from the supervised setting.
1	Introduction
Temporal Difference (TD) learning (Sutton, 1988) is a fundamental method of Reinforcement Learn-
ing (RL), which works by using estimates of future predictions to learn to make predictions about
the present. To scale to large problems, TD has been used in conjunction with deep neural networks
(DNNs) to achieve impressive performance (e.g. Mnih et al., 2013; Schulman et al., 2017; Hessel
et al., 2018). Unfortunately, the naive application of TD to DNNs has been shown to be brittle
(Machado et al., 2018; Farebrother et al., 2018; Packer et al., 2018; Witty et al., 2018), with exten-
sions such as the n-step TD update (Fedus et al., 2020) or the TD(λ) update (Molchanov et al., 2016)
only marginally improving performance and generalization capabilities when coupled with DNNs.
Part of the success of DNNs, including when applied to TD learning, is the use of adaptive or accel-
erated optimization methods (Hinton et al., 2012; Sutskever et al., 2013; Kingma & Ba, 2015) to find
good parameters. In this work we investigate and extend the momentum algorithm (Polyak, 1964) as
applied to TD learning in DNNs. While accelerated TD methods have received some attention in the
literature, this is typically done in the context of linear function approximators (Baxter & Bartlett,
2001; Meyer et al., 2014; Pan et al., 2016; Gupta et al., 2019; Gupta, 2020; Sun et al., 2020), and
while some studies have considered the mix of DNNs and TD (Zhang et al., 2019; Romoff et al.,
2020), many are limited to a high-level analysis of hyperparameter choices for existing optimization
methods (Sarigul & Avci, 2018; AndrychoWicz et al., 2020); or indeed the latter are simply applied
as-is to train RL agents (Mnih et al., 2013; Hessel et al., 2018). For an extended discussion of related
Work, We refer the reader to appendix A.
As a first step in going beyond the naive use of supervised learning tools in RL, We examine mo-
mentum. We argue that momentum, especially as it is used in conjunction With TD and DNNs, adds
an additional form of bias Which can be understood as the staleness of accumulated information. We
quantify this bias, and propose a corrected momentum algorithm that reduces this staleness and is
capable of improving performance.
1.1	Reinforcement Learning and Temporal Difference Learning
A Markov Decision Process (MDP) (Bellman, 1957; Sutton & Barto, 2018) M = hS, A, R, P, γi
consists of a state space S, an action space A, a reWard function R : S → R and a transition
function P(s0|s, a). RL agents usually aim to optimize the expectation of the long-term return,
G(St) = P∞=t YkTR(Sk) where Y ∈ [0,1) is called the discount factor. Policies ∏(a∣s) map
states to action distributions. Value functions V π and Qπ map states and states-action pairs to
1
Under review as a conference paper at ICLR 2021
expected returns, and can be written recursively:
Vπ (St) = En [G(St)] = En [R(St,At)+ YV (St+ι)A 〜∏(St)]
Qn (St,At)= En [R(St ,At) + γPɑ∏(a∣St+ι)Q(St+ι,a)]
We approximate Vn with Vθ . We can train Vθ via regression to observed values of G, but these
recursive equations also give rise to the Temporal Difference (TD) update rules for policy evaluation,
relying on current estimates of V to bootstrap, which for example in the tabular case is written as:
V (St) J V (St) - α(V (St)-(R(St) + YV ⑸+。)),
(1)
where α ∈ [0, 1) is the step size. Alternatively, estimates of this update can be performed by a
so-called semi-gradient (Sutton & Barto, 2018) algorithm where the “TD(0) loss” is minimized:
Θt+1 = θt- αVθt (%t (St) - (R(St) + YV(St+ι)))2 ,
(2)
with V meaning We consider V constant for the purpose of gradient computation.
1.2	Bias and staleness in momentum
The usual form of momentum (Polyak, 1964; Sutskever et al., 2013) in stochastic gradient descent
(SGD) maintains an exponential moving average with factor β of gradients w.r.t. to some objective
J , changing parameters θt ∈ Rn (t is here SGD time rather than MDP time) with learning rate α:
μt = βμt-l + (1 — β)Vθt-ι Jt(θt-i)
θt = θt-i — αμt
(3)
(4)
We assume here that the objective Jt is time-dependent, as is the case for example in minibatch
training or online learning. Note that other similar forms of this update exist, notably Nesterov’s
accelerated gradient method (Nesterov, 1983), as well as undampened methods that omit (1 - β) in
(3) or replace (1 - β) with α, found in popular deep learning packages (Paszke et al., 2019).
We make the observation that, at time t, the gradients accumulated in μ are stale. They were com-
puted using past parameters rather than θt, and in general we’d expect VθtJt(θt) 6= Vθk Jt(θk),
k < t. As such, the update in (4) is a biased update.
In supervised learning where one learns a mapping from x to y, this staleness only has one source: θ
changes but the target y stays constant. We argue that in TD learning, momentum becomes doubly
stale: not only does the value network change, but the target (the equivalent of y) itself changes1
with every parameter update. Consider the TD objective in (2), when θ changes, not only does V(s)
change, but V(s0) as well. The objective itself changes, making past gradients stale and less aligned
with recent gradients (even more so when there is gradient interference (Liu et al., 2019; Achiam
et al., 2019; Bengio et al., 2020), constructive or destructive).
Note that several sources of bias already exist in TD learning, notably the traditional parametric bias
(of the bias-variance tradeoff when selecting capacity), as well as the bootstrapping bias (of the error
in V(s0) when using it as a target; using a frozen target prevents this bias from compounding). We
argue that the staleness in momentum we describe is an additional form of bias, slowing down or
preventing convergence. This has been hinted at before, e.g. Gupta (2020) suggests that momentum
hinders learning in linear TD(0).
We wish to understand and possibly correct this staleness in momentum. In this work we propose
answers to the following questions:
•	Is this bias significant in supervised learning? No, the bias exists but has a minimal effect at
best when comparing to an unbiased oracle.
•	Is this bias significant in TD learning? Yes, we can quantify the bias, and comparisons to an
unbiased oracle reveal significant differences.
•	Can we correct μt to remove this bias? Yes, we derive an online update that approximately
corrects μt using only first order derivatives.
•	Does the correction help in TD learning? Yes, using a staleness-corrected momentum improves
sample complexity, in policy evaluation, especially in an online setting.
1Interestingly, even in most recent value-based control works (Hessel et al., 2018) a (usually frozen) copy is
used for stability, meaning that the target only changes when the copy is updated. This is considered a “trick”
which it would be compelling to get rid of, since it slows down learning, and since most recent policy-gradient
methods (which still use a value function) do not make use of such copies (Schulman et al., 2017).
2
Under review as a conference paper at ICLR 2021
2	Correcting Momentum
2.1	Identifying Bias
Here we propose an experimental protocol to quantify the bias of momentum. We assume we are
minimizing some objective in the minibatch or online setting. In such a stochastic setting, mo-
mentum is usually understood as a variance reduction method, or as approximating the large-batch
setting (Botev et al., 2017), but as discussed above, momentum also induces bias in the optimization.
We note that μt, the momentum at time t, can be rewritten as:
t
μt = (1- β) Xβt-iVθi Ji(θi),	(5)
i=1
and argue that an ideal unbiased momentum μJ= would approximate the large batch case by only
discounting past minibatches and using current parameters rather than past parameters:
t
μ= =f (i-β) X β t-iVθt Ji(θt).	(6)
i=1
Note that the only difference between (5) and (6) is the use of θi vs θt. The only way to exactly
compute μJ is to recompute the entire sum after every parameter update. We will consider this our
unbiased oracle. To compute μt empirically, since beyond a certain power βk becomes small, We
will use an effective horizon of h = 2/(1 - β) steps (i.e. start at i = t - h rather than at i = 1).
We call the difference between μt and μJ the bias, but note that in the overparameterized stochastic
gradient case, minor differences in gradients can quickly send parameters in different regions. This
makes the direct measure of ∣∣μt 一 μ=k uninformative. Instead, We measure the optimization bias
by simply comparing the loss of a model trained with μt against that of a model trained with μJ.
Finally, we note that, in RL, momentum approximating the batch case is related to (approximately)
replaying an entire buffer at once (instead of sampling transitions). This has been shown to also
have interesting forms in the linear case (van Seijen & Sutton, 2015), reminiscent of the correction
derived below. We also note that, while the mathematical expression of momentum and eligibility
traces (Sutton, 1988) look fairly similar, they estimate a very different quantity (see appendix A).
2.2	Correcting Bias
Here we propose a way to approximate μ=, and so derive an approximate correction to the bias in
momentum for supervised learning, as well as for Temporal Difference (TD) learning.
We consider here the simple regression and TD(0) cases, for the online (minibatch size 1) case. We
show the full derivations, and results for any minibatch size, TD(λ) and n-step TD, in appendix B.
In least squares regression with loss δ2 we can write the gradient gt as:
gt(θt) = (yt — fθt (Xt))V①/①(Xt) = Vθt δ2∕2.	(7)
We would like to “correct” this gradient as we move away from θt . A simple way to do so is to
compute the Taylor expansion around θt of gt:
gt(θt + ∆θ) =gt(θt)+Vθtgt(θt)>∆θ+o(∣∆θ∣22),	(8)
≈ gt(θt) + (δV2tfθt(Xt)- Vθtfθt(Xt) 0 Vθtfθt(Xt))T∆θ,	(9)
where 0 is the outer product, V2 the second derivative. We note that the term multiplying ∆θ
is commonly known as “the Hessian” of the loss. We also note that Taylor expansions around
parameters have a rich history in deep learning (LeCun et al., 1990; Molchanov et al., 2016), and
that in spite of its non-linearity, the parameter space of a deep network is filled with locally consistent
regions (Balduzzi et al., 2017) in which Taylor expansions are accurate.
The same correction computation can be made for TD(0) with TD loss δ2. Remember that we write
t as the learning time; we denote an MDP transition as (st, at, rt, s0t), making no assumption on the
distribution of transitions:
gt(θt) = (Vθt (st) - rt - γVθt (s0t))VθtVθt (st) = Vθt δt2 ∕2,	(10)
gt(θt+∆θ)≈gt(θt)+(Vθt(Vθt(st) -γVθt(s0t))0VθtVθt(st)+δVθ2tVθt(st))T∆θ.	(11)
3
Under review as a conference paper at ICLR 2021
Here, because we are using a semi-gradient method, the term multiplying ∆θ is not exactly the
Hessian: when computing Vθδ2, We hold V(s0) constant, but when computing Vθg, We need to
consider Vθ(s0) a function of θ as it affects g, and so compute its gradient.2
Without loss of generality, let us write equations like (9) and (11) using the matrix Zt ∈ Rn×n :
gt(θt + ∆θ) ≈ gt(θt) + Zt>∆θ,	(12)
where the form of Zt, the “Taylor term”, depends on the loss (e.g. the Hessian in (9)).
Recall that in (6) we define μ* as the discounted sum of gradients using θt for losses J i ≤ t. We
call those gradients corrected gradients, git := gi (θi + (θt - θi)). At timestep t we update θt with
αμt, thus we substitute ∆θ = θt 一 θi = -α P[=i μk in (12) and get:
t-1
gt =f gi(θi) - αZi Xμk ≈ gt.	(13)
k=i
We can now approximate the unbiased momentum μJ= using (13), which we denote μ:
t
μt =ef (i-β) X βt-i^t	(14)
i=1
t-1 k
=μt - α(i- β) XX βt-iz>μk.	(15)
k=1 i=1
Noting that μt can be computed as usual and that the second term of (15) has a recursive form, which
we denote η (see appendix B), we rewrite μ as follows:
μt = μt ― ηt	(16)
ηt = βηt-ι + αβZj-ιμt-ι	(correction term) (17)
t
μt = (1 - β)	βt-igi = (1 - β)gt + βμt-1	(normal momentum) (18)
i=1
t
ζt = (1 - β) X βt-iZi = (1 - β)Zt + βζt-1.	(“momentum” of Taylor terms) (19)
i=1
Algorithmically, this requires maintaining 2 vectors μ and η of size n, the number of parameters,
and one matrix ζ of size n × n. In practice, we find that only maintaining the diagonal of ζ can also
work and can avoid the quadratic growth in n.
The computation of Z also calls on computing second order derivatives V2f (e.g. in (9) and (11)),
which is impractical for large architectures. In this work, as is commonly done due to the usually
small magnitude of V2f (Bishop, 2006, section 5.4), we ignore them and only rely on the outer
product of gradients.
Ignoring the second derivatives, computing Z for TD(0) requires 3 backward passes, for g = Vδ2,
VγV(s0), and VV (s). In the online case g = δVV (s), requiring only 2 backward passes, but
in the more general minibatch of size m > 2 case, it is more efficient with modern automatic
differentiation packages to do 3 backward passes than 2m passes (see appendix B.4).
Finally, we note that this method easily extends to forward-view TD(λ) and n-step TD methods, all
that is needed is to compute Z appropriately (see appendix B.2).
3 Experimental Results
We evaluate our proposed correction as well as the oracle (the best our method could perform) on
common RL benchmarks and supervised problems. We evaluate each method with a variety of
2This computation of the gradient of Vθ (s0 ) may remind the reader of the so-called full gradient or residual
gradient (Baird, 1995), but its purpose here is very different: we care about learning using semi-gradients,
TD(0) is a principled algorithm, but we also care about how this semi-gradient evolves as parameters change,
and thus we need to compute Vθ Vθ (s0).
4
Under review as a conference paper at ICLR 2021
hyperparameters and multiple seeds for each hyperparameter setting. The full range of hyperparam-
eters used as well as architectural details can be found in appendix D.
We will use the following notation for the optimizer used to train models: μ is the usual momen-
tum, i.e. (3)&(4), and serves as a baseline. μ* is our oracle, defined in (6). μ is the proposed
correction, with updates as in (16)-(19), using the outer product approximation of Z. ∕^diag is the
proposed correction, but with a diagonal approximation of Z . Throughout our figures, shaded areas
are bootstrapped 95% confidence intervals over hyperparameter settings (if applicable) and runs.
3.1	Supervised Learning
We first test our hypothesis, that there is bias in momentum, in a simple regression task and SVHN.
For regression, we task a 4-layer MLP to regress to a 1-d function from a 1-d input. For illustration
we use a smooth but non-trivial mixture of sines of increasing frequency in the domain x ∈ [-1, 1]:
ʌ/ʌʌ ∖ ʃʌ , X	y(x) = 0.5sin(2.14(x + 2)) + 0.82sin(9x + 0.4)+
0.38 sin(12x) + 0.32 sin(38x - 0.1)	(20)
Note that this choice is purely illustrative and that our findings extend to similar simple functions.
We train the model on a fixed sample of 10k uniformly sampled points. We measure the mean
squared error (MSE) when training a small MLP with momentum SGD versus our oracle momentum
μ* and the outer product correction μ. As shown in Figure 1, we find that while there is a significant
difference between the oracle and the baseline (p ≈ 0.001 from Welsh’s t-test), the difference is
fairly small, and is no longer significant when using the corrected μ (p ≈ 0.1).
4 3 2 1
♦ ♦ ♦ ♦
Oooo
.10.1.1P①IBnbS UB①
Figure 1: Regression to (20) with varying mo-
mentums (10 seeds per setting). Note that the
difference between μ and μ is not significant,
but μ and μ* is.
O
0 5 0
♦ ♦ ♦
2 11
Pooqll①士IalOa ①AB①N
10000	15000
ID steps
00
20000
Figure 2: Classification on SVHN with varying
momentums (5 seeds per setting). Dotted lines
are test losses. The only significant difference is
between the training loss of μ* and μ.
We compare training a small convolutional neural network on SVHN (Netzer et al., 2011) with
momentum SGD versus our oracle momentum μ* and the diagonalized correction momentum ∕^diag.
The results are shown in Figure 2. We do not find significant differences except between the training
loss of μ* and μ, and find that the oracle performs worse than the normal and corrected momentum.
From these experiments we conclude that, in supervised learning, there exists a quantifiable opti-
mization bias to momentum, but that correcting it does not appear to offer any benefit. It improves
performance only marginally at best, and degrades it at worst. This is consistent with the insight that
μ* approximates the large batch gradient, and that large batches are often associated with overfitting
or poor convergence in supervised learning (Wilson & Martinez, 2003; Keskar et al., 2016).
3.2 Temporal D ifference Learning
We now test our hypotheses, that there is optimisation bias and that we can correct it, on RL prob-
lems. First, we test policy evaluation of the optimal policy on the Mountain Car problem (Singh
5
Under review as a conference paper at ICLR 2021
& Sutton, 1996) with a small MLP. We also test Acrobot and Cartpole environments and find very
similar results (see appendix C). We then test our method on Atari (Bellemare et al., 2013) with
convolutional networks.
Figure 3 shows policy evaluation on Mountain Car using a replay buffer (on-policy state transitions
are sampled i.i.d. in minibatches). We compare the loss distributions (across hyperparameters and
seeds) at step 5k, and find that all methods are significantly different (p < 0.001) from one another.
Figure 4 shows online policy evaluation, i.e. the transitions are generated and learned from once,
in-order, and one at a time (minibatch size of 1). There we see that the oracle μ* and full corrected
version μ are significantly different from the baseline μ (p < 0.001) and diagonalized correction
μdiag, as well as μ from μdiag, while μ* and μ are not significantly different (p > 0.1).
This suggests that the ζ matrix carries useful off-diagonal temporal information about parameters
which co-vary, especially when the data is not used uniformly during learning. We test another
possible explanation, which is that performance is degraded in online learning because the batch
size is 1 (rather than 16 or 32 as in Figure 3). We find that a batch size of 1 does degrade "diag’s
performance significantly, but does not fully explain its poor online performance (see appendix
Figure 10).
0	5000	10000	15000	20000
SGD steps
Figure 4: TD(0) online policy evaluation on
Mountain Car, transitions are seen in-order. The
MSE is measured against a pretrained V π (50
seeds per setting). At step 2θk, μ* and μ are not
significantly different.
Oracle: μ*
Baseline: μ
Corrected: μ
Corrected: “diag
O
4 3 2 1 0
♦ ♦ ♦ ♦ ♦
Ooooo
F τ一——卷)国二。HpəɪBnbS UBΦ
SGD steps
Figure 3: TD(0) policy evaluation on Moun-
tain Car with varying momentums on a replay
buffer. The MSE is measured against a pre-
trained V π (10 seeds per setting). At step 5k,
all methods are significantly different.
A central motivation of this work is that staleness in momentum arises from the change in gradients
and targets. In theory, this is especially problematic for function approximators which tend to have
interference, such as DNNs (Fort et al., 2019; Bengio et al., 2020), i.e. where taking a gradient step
using a single input affects the output for virtually every other input. More interference means that
as we change θ, the accumulated gradients computed from past θs become stale faster. We test this
hypothesis by (1) measuring the drift of the value function in different scenarios, and (2) computing
the cosine similarity between the corrected gradients of (13), gt, and their true value g； = PJi(θt).
In Figure 5 we compare the value drift of MLPs with that of linear models with Radial Basis Function
(RBF) features. We compute the value drift of the target on recently seen examples, i.e. we compute
the average (Vθi (Si)-^¾ (si))2 for the last h = 2nmb∕(1-β) examples, where nmb is the minibatch
size. We compute the RBF features for S ∈ R2 as exp(一∣∣s — Uij∣∣2∕σ2) for a regular grid of
uij ∈ R2 in the input domain. We find that the methods we try (even the oracle) are all virtually
identical when using RBFs (see Figure 12 in appendix). We also find, as shown in Figure 5, that
RBFs have very little value drift (due to their sparsity) compared to MLPs. This is consistent with
our hypothesis that the method We propose is only useful if there is value drift-otherwise, there is no
optimization bias incurred by using momentum. We can test this hypothesis further by artificially
increasing the width of the RBFs, σ2, such that they overlap. As predicted, we find that reducing
sparsity (increasing interference) increases value drift and increases the gap between our method
and the baseline (Figure 6). This drift correlates with performance when changing σ2 (Figure 7).
6
Under review as a conference paper at ICLR 2021
O
12 3 4
- - - -
W
汽ιq OnIBA
一 μ* MLP
--μ* RBF
μ MLP
一μ RBF /
一.μ MLP -
一μ RBF
-■ μdiag MLP
一μdiag RBF
1000 2000 3000 4000 5000
SGD steps
0.06
1
2	3	4
width of RBFs, σ2
Figure 6: MSE as a function of the width, σ2,
of RBF kernels. The larger the kernel, the more
value drift our method, μ, is able to correct (10
seeds per setting).
Figure 5: Value drift of V (s0) when training
with TD(0) on a replay buffer. We see that RBFs
being a sparse feature representation, the value
functions of recently seen data tend not to drift
(10 seeds per setting). Here σ2 = 1.
"太』——2月Hs≡5
Figure 7: MSE as a function of mean Value
drift of V (s0) for RBFs of varying kernel size.
The lines match the σ2 lines of Figure 6, and
show the relation between σ2, drift, and error.
Figure 8: Average cosine similarity of the Tay-
lor approximations gt with their true value gt for
recently seen data; Mountain Car, replay buffer
policy evaluation (40 seeds per setting).
O
O
O
In Figure 8 We measure the cosine similarity of the Taylor approximations ^t = gi + Z> (θt - θi)
with the true gradients gt = RJi(θt) for the last h = 2nm,b∕(1 - β) examples. We find that
the similarity is relatively high (close to 1) throughout training but that it gets lower as the model
converges to the true value V π . This is also consistent with our hypothesis that there is change
(staleness) in gradients, while also validating the approach of using Taylor approximations to achieve
this correction mechanism.
It is also possible to correct momentum for a subset of parameters; we try this on a per-layer basis
and find that, perhaps counter-intuitively3, it is much better to correct the bottom layers (close to x)
than the top layers (close to V ), although correcting all layers is the best (see appendix Figure 13).
We now apply our method on the Atari MsPacman game. We first do policy evaluation on an expert
agent (we use a pretrained Rainbow agent (Hessel et al., 2018)). Since the architecture required
to train an Atari agent is too large (n ≈ 4.8M) to maintain n2 values, we only use the diagonal
version of our method. We also experiment with smaller (n ≈ 12.8k) models and the full correction,
3Although one may expect that changes close to V should produce more drift (it is commonly thought that
bootstrapping happens in the last layer on top of relatively stable representations), the opposite is consistent
with μ interacting with interference in the input space, which the bottom layers have to learn to disentangle.
7
Under review as a conference paper at ICLR 2021
Figure 9: TD(0) policy evaluation on Atari
(MsPacman) With varying momentums (20 seeds)
on a replay buffer. The MSE is measured against
sampled returns Gπ .
with similar results (see D.8). To (considerably) speed up learning, since the model is large, we
additionally use per-parameter learning rates as in Adam (Kingma & Ba, 2015), where an estimate
of the second moment is used as denominator in the parameter update; We denote this combination
μdiag/√V2 + e. We see in Figure 9 that our method provides a significant (p < 0.01) advantage.
Note that our method does not use frozen tar-
gets (as is usually necessary for this environ-
ment). A simple Way to avoid momentum stal-
eness and/or target drift in TD(0) is the use of
frozen targets, i.e. to keep a separate G to com-
pute V¾(s0), updated (θ J θ) at large inter-
vals. Such a method is central to DQN (Mnih
et al., 2013), but its doWnside is that it requires
more updates to bootstrap. We find that for pol-
icy evaluation, frozen targets are much sloWer
(both in Atari and simple environments) than
our baseline (see appendix Figures 16 and 17).
We finally apply our method to control, first
in combination With a policy gradient method,
PPO (Schulman et al., 2017), in its policy eval-
uation part, and second With a 5-step Sarsa
method, but find no improvement (or marginal
at best) in either setting. As in simpler environ-
ments We measure cosine similarity and value drift. We find loW similarity (≈ 0.1) but a 2× to
3× decrease in drift using our method, suggesting that While our method corrects drift, its effect on
policy improvement is minor. We suspect that in control, even With a better value function, other
factors such as exploration or overestimation come into play Which are not addressed by our method.
4 Discussion
We propose a method Which improves momentum, When applied to DNNs doing TD learning, by
correcting gradients for their staleness via an approximate Taylor expansion. We shoW that correct-
ing this staleness is particularly useful When learning online using a TD objective, but less so for
supervised learning tasks. We shoW that the proposed method corrects for value drift in the boot-
strapping target of TD, and that the proposed approximate Taylor expansion is a useful tool that
aligns Well enough With the true gradients.
Shortcomings of the method In its most principled form, the proposed method requires comput-
ing a second-order derivative, Which is impractical in most deep learning settings. While We do find
experimentally that ignoring its contribution has a negligible effect in toy settings, We are unable
to verify this for larger neural netWorks. Compared to the usual momentum update, the proposed
method also requires performing tWo additional backWard passes, as Well as storing 2n + n2 addi-
tional scalars. This can be improved With a diagonal approximation requiring only 3n scalars, but
this approximation is not as precise, Which is currently an obstacle for large architectures. While
these extra computations improve sample complexity, i.e. We get more out of each sample, they do
not necessarily improve convergence speed in computation time (although, our method is suited to
GPU parallelism and has reasonable speed even for large n2s, including in Atari). Sample complex-
ity may be particularly important in settings Where samples are expensive to get (e.g. real World
systems), but less so When they are almost free (e.g. simulations).
Incorporating TD in optimization One meta-result stands out from this Work: something is lost
When naively applying supervised learning optimization tools to RL algorithms. In particular here,
by simply taking into account the non-stationarity of the TD objective, We successfully improve the
sample complexity of a standard tool (momentum), and demonstrate the potential value of incorpo-
rating elements of RL into supervised learning tools. The difference explored here With momentum
is only one of the many differences betWeen RL and supervised learning, and We believe there re-
main plenty of opportunities to improve deep learning methods by understanding their interaction
With the peculiarities of RL.
8
Under review as a conference paper at ICLR 2021
References
Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep q-
learning. arXiv preprint arXiv:1903.08894, 2019.
Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael
Marinier, Leonard Hussenot, MatthieU Geist, Olivier PietqUin, Marcin Michalski, Sylvain Gelly,
and Olivier Bachem. What matters in on-policy reinforcement learning? a large-scale empirical
study, 2020.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995,pp. 30-37. ElSeVier, 1995.
David Balduzzi, Brian McWilliams, and Tony Butler-Yeoman. Neural taylor approximations: Con-
vergence and exploration in rectifier networks. In International Conference on Machine Learning,
pp. 351-360, 2017.
Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artifi-
cial Intelligence Research, 15:319-350, 2001.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Richard Bellman. A markovian decision process. Journal of mathematics and mechanics, pp. 679-
684, 1957.
Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal
difference learning. In International Conference on Machine Learning, 2020.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Aleksandar Botev, Guy Lever, and David Barber. Nesterov’s accelerated gradient and momentum as
approximations to regularised update descent. In 2017 International Joint Conference on Neural
Networks (IJCNN), pp. 1899-1903. IEEE, 2017.
Felix Dangel, Frederik Kunstner, and Philipp Hennig. BackPACK: Packing more into backprop. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=BJlrF24twB.
A. P. Engelbrecht. A new pruning heuristic based on variance analysis of sensitivity information.
IEEE Transactions on Neural Networks, 12(6):1386-1399, 2001.
Andries Petrus Engelbrecht. Using the taylor expansion of multilayer feedforward neural networks.
South African Computer Journal, 2000(26):181-189, 2000.
Jesse Farebrother, Marlos C Machado, and Michael Bowling. Generalization and regularization in
dqn. arXiv preprint arXiv:1810.00123, 2018.
William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo Larochelle, Mark
Rowland, and Will Dabney. Revisiting fundamentals of experience replay, 2020.
Stanislav Fort, PaWel Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A
new perspective on generalization in neural networks, 2019.
Dhawal Gupta. Applicability of momentum in the methods of temporal learning. 2020.
Harsh Gupta, R Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate
selection for two time-scale reinforcement learning. In Advances in Neural Information Process-
ing Systems, pp. 4704-4713, 2019.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Advances in neural information processing systems, pp. 164-171, 1993.
9
Under review as a conference paper at ICLR 2021
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. CSC321, 2012.
Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, and Remi Munos. Recurrent ex-
perience replay in distributed reinforcement learning. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=r1lyTjAqYX.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima, 2016.
Diederik Kingma and Jimmy Ba. Adam: a method for stochastic optimization (2014). arXiv preprint
arXiv:1412.6980, 15, 2015.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing Systems, pp. 598-605, 1990.
Vincent Liu, Raksha Kumaraswamy, Lei Le, and Martha White. The utility of sparse representa-
tions for control in reinforcement learning. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 4384-4391, 2019.
Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.
Dominik Meyer, Remy Degenne, Ahmed Omrane, and Hao Shen. Accelerated gradient temporal
difference learning algorithms. In 2014 IEEE Symposium on Adaptive Dynamic Programming
and Reinforcement Learning (ADPRL), pp. 1-8. IEEE, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
Yu Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2)
o (1/k2). In Sov. Math. Dokl, volume 27, 1983.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krahenbuhl, Vladlen Koltun, and DaWn Song.
Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.
Yangchen Pan, Adam White, and Martha White. Accelerated gradient temporal difference learning.
arXiv preprint arXiv:1611.09328, 2016.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Boris T Polyak. Some methods of speeding up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
10
Under review as a conference paper at ICLR 2021
Joshua Romoff, Peter Henderson, David Kanaa, Emmanuel Bengio, Ahmed Touati, Pierre-Luc Ba-
con, and Joelle Pineau. Tdprop: Does jacobi preconditioning help temporal difference learning?,
2020.
M. Sarigul and M. Avci. Performance comparison of different momentum techniques on deep re-
inforcement learning. Journal of Information and Telecommunication, 2(2):205-216, 2018. doi:
10.1080/24751839.2018.1440453.
Robert E Schapire and Manfred K Warmuth. On the worst-case analysis of temporal-difference
learning algorithms. Machine Learning, 22(1-3):95-121, 1996.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Satinder P Singh and Richard S Sutton. Reinforcement learning with replacing eligibility traces.
Machine learning, 22(1-3):123-158, 1996.
Tao Sun, Han Shen, Tianyi Chen, and Dongsheng Li. Adaptive temporal difference learning with
linear function approximation. arXiv preprint arXiv:2002.08537, 2020.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9-44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, Csaba Szepesvari, and Hamid Reza Maei. A convergent o (n) algorithm for
off-policy temporal-difference learning with linear function approximation. Advances in neural
information processing systems, 21(21):1609-1616, 2008.
Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
Szepesvari, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 993-1000, 2009.
Harm van Seijen and Rich Sutton. True online td(λ). In International Conference on Machine
Learning, pp. 692-700, 2014.
Harm van Seijen and Rich Sutton. A deeper look at planning as learning from replay. In International
conference on machine learning, pp. 2314-2322, 2015.
Nino Vieillard, Bruno Scherrer, Olivier Pietquin, and Matthieu Geist. Momentum in reinforcement
learning. In International Conference on Artificial Intelligence and Statistics, pp. 2529-2538,
2020.
D Randall Wilson and Tony R Martinez. The general inefficiency of batch training for gradient
descent learning. Neural networks, 16(10):1429-1451, 2003.
Sam Witty, Jun Ki Lee, Emma Tosch, Akanksha Atrey, Michael Littman, and David Jensen.
Measuring and characterizing generalization in deep reinforcement learning. arXiv preprint
arXiv:1812.02868, 2018.
Shangtong Zhang, Wendelin Boehmer, and Shimon Whiteson. Deep residual reinforcement learn-
ing. arXiv preprint arXiv:1905.01072, 2019.
11
Under review as a conference paper at ICLR 2021
A Related work
To the best of our knowledge, no prior work attempts to derive a corrected momentum-SGD update
adapted to the Temporal Difference method. That being said, a wealth of papers are looking to
accelerate TD and related methods.
On momentum, traces, and gradient acceleration in TD
From an RL perspective, our work has some similarity to the so-called eligibility traces mechanism.
In particular, in the True Online TD(λ) method of van Seijen & Sutton (2014), the authors derive a
strict-online update (i.e. weights are updated at every MDP step, using only information from past
steps, rather than future information as in the λ-return perspective) where the main mechanism of
the derivation lies in finding an update by assuming (at least analytically) that one can “start over”
and reuse all past data iteratively at each step of training, and then from this analytical assumption
derive a recursive update (that doesn’t require iterating through all past data). The extra values that
have to be kept to compute the recursive updates are then called traces. This is akin to how we
conceptualize μ*, (6), and derive μ.
The conceptual similarities of the work of van Seijen & Sutton (2015) with our work are also in-
teresting. There, the authors analyse what “retraining from scratch” means (i.e., again, iteratively
restarting from θo ∈ Rm) but with some ideal target θ* (e.g. the current parameters) by redoing se-
quentially all the TD(0) updates using θ* for all the n transitions in a replay buffer, costing O(nm).
They derive an online update showing that one can continually learn at a cost of O(m2) rather than
paying O(nm) at each step. The proposed update is also reminiscent of our method in that it aims to
perform an approximate batch update without computing the entire batch gradient, and also main-
tains extra momentum-like vectors and matrices. We note that the derivation there only works in the
linear TD case.
In a way, such an insight can be found in the original presentation of TD(λ) of Sutton (1988),
where the TD(λ) parameter update is written as (equation (4) in the original paper, but with adapted
notation):
t
δ% = α[rt + γVθt(St+1)- Vθt (St)] X λt-k VθtVθt (Sk)
k=1
Remark the use of θt in the sum; in the linear case since Vθt Vθt (Sk) = φ(Sk), the sum does not
depend on θt and thus can be computed recursively. A posteriori, if one can find a way to cheaply
compute Vθt Vθt (Sk) ∀k, perhaps using the method we propose, it may be an interesting way to
perform TD(λ) using a non-linear function approximator.
Our analysis is also conceptually related to the work of Schapire & Warmuth (1996), where a worst-
case analysis of TD*(λ) is performed using a best-case learner as the performance upper bound.
This is similar to our momentum oracle; just as the momentum oracle is the ”optimal” approxima-
tion of the accumulation gradients coming from all past training examples, the best-case learner of
Schapire & Warmuth (1996) is the set parameters that is optimal when one is allowed to look at all
past training examples (in contrast to an online TD learner).
Before moving on from TD(λ), let us remark that eligibility traces and momentum, while simi-
lar, estimate different quantities. The usual (non-replacing) traces estimate the exponential moving
average of the gradient of Vθ, while momentum does so for the objective J (itself a function of Vθ):
tt
et = (1- λ) X λt-kVθVθ,	μt = (1- β) X βt-kVθ J(Vθ)
kk
Our method also has similarities with residual gradient methods (Baird, 1995). A recent example of
this is the work of Zhang et al. (2019), who adapt the residual gradient for deep neural networks.
Residual methods learn by taking the gradient of the TD loss with respect to both the current value
and the next state value V(S0), but this comes at the cost of requiring two independent samples of
S0 (except in deterministic environments).
Similarly, our work is related to the “Gradient TD” family of methods (Sutton et al., 2008; 2009).
These methods attempt to maintain an expectation (over states) of the TD update, which allows to
12
Under review as a conference paper at ICLR 2021
directly optimize the Bellman objective. While the exact relationship between GTD and “momen-
tum TD” is not known, they both attempt to maintain an “expected update” and adjust parameters
according to it; the first approximates the one-step linear TD solution, while the latter approxi-
mates the one-step batch TD update. Note that linear GTD methods can also be accelerated with
momentum-style updates (Meyer et al., 2014), low-rank approximations for part of the Hessian (Pan
et al., 2016), and adaptive learning rates (Gupta et al., 2019).
More directly related to this work is that of Sun et al. (2020), who show convergence properties of a
rescaled momentum for linear TD(0). While most (if not every) deep reinforcement learning method
implicitly uses some form of momentum and/or adaptive learning rate as part of the deep learning
toolkit, Sun et al. (2020) properly analyse the use of momentum in a (linear) TD context. Gupta
(2020) also analyses momentum in the context of a linear TD(0) and TD(λ), with surprising nega-
tive results suggesting naively applying momentum may hurt stability and convergence in minimal
MDPs.
Another TD-aware adaptive method is that of Romoff et al. (2020), who derive per-parameter adap-
tive learning rates, reminiscent of RMSProp (Hinton et al., 2012), by considering a (diagonal) Jacobi
preconditioning that takes into account the bootstrap term in TD.
Finally, we note that, as far as we know, recent deep RL works all use some form of adaptive
gradient method, Adam (Kingma & Ba, 2015) being an optimizer of choice, closely followed by
RMSProp (Hinton et al., 2012); notable examples of such works include those of Mnih et al. (2013),
Schulman et al. (2017), Hessel et al. (2018), and Kapturowski et al. (2019). We also note the work of
Sarigul & AVci (2018), comparing various SGD variants on the game of Othello, showing significant
differences based on the choice of optimizer.
On Taylor approximations Balduzzi et al. (2017) note that while theory suggests that Taylor
expansions around parameters should not be useful because of the ”non-convexity” of ReLU neural
networks, there nonetheless exists local regions in parameter space where the Taylor expansion is
consistent. Much earlier work by Engelbrecht (2000) also suggests that Taylor expansions of small
sigmoid neural networks are easier to optimize. Using Taylor approximations around parameters
to find how to prune neural networks also appears to be an effective approach with a long history
(LeCun et al., 1990; Hassibi & Stork, 1993; Engelbrecht, 2001; Molchanov et al., 2016).
On policy-gradient methods and others While not discussed in this paper, another class of meth-
ods used to solve RL problems are PG methods. They consist in taking gradients of the objective
wrt a directly parameterized policy (rather than inducing policies from value functions). We note in
particular the work of Baxter & Bartlett (2001), who analyse the bias of momentum-like cumulated
policy gradients (referred to as traces therein), showing that β the momentum parameter should be
chosen such that 1/(1 - β) exceeds the mixing time of the MDP.
Let us also note the method of Vieillard et al. (2020), Momentum Value Iteration, which uses the
concept of an exponential moving average objective for a decoupled (with its own parameters)
action-value function from which the greedy policy being evaluated is induced. This moving av-
erage is therein referred to as momentum; even though it is not properly speaking the optimizational
acceleration of Polyak (1964), its form is similar.
B	Complete Derivations
B.1 Derivation of momentum correction
In momentum, μt can be rewritten as:
t
μt = (1- β) X βt-iVθi Ji(θi)	(21)
i=1
We argue that an ideal unbiased momentum μJ= would approximate the large batch case by only
discounting past minibatches and using current parameters θt rather than past parameters θi :
t
μ= =f (i-β) X βt-iVθt Ji(θt)	(22)
i=1
13
Under review as a conference paper at ICLR 2021
Note that the only difference between (21) and (22) is the use of θi vs θt . The only way to compute
μt exactly is to recompute the entire sum after every parameter update. Alternatively, We could
somehow approximate this sum. Below we will define gt = Vθt Ji(θt), which we will then approx-
imate with gi. We will then show that this approximation has a recursive form which leads to an
algorithm.
We want to correct accumulated past gradients gi to their “ideal” form git, as above. We do so with
their Taylor expansion around θ, and we write the correction of the gradient gi computed at learning
time i corrected at time t as:
gt = gi(θi + ∆θ(t-	1; i))	=	gi	+	VθgT∆θ(t	- 1; i) + o(k∆θk2)	(23)
≈ gt	=	gi	+	ZT∆θ	(24)
where ∆θ(t; i) =θt -θi,gi =VθiJi,Zi = Vθigi.
Here we are agnostic of the particular form of Z, which will depend on the loss and learning algo-
rithm, and is not necessarily the so-called Hessian. To see why this is the case, and for the derivation
of Z for the squared loss, cross-entropy and TD(0), see section B.2.
Let,s now express ^ in terms of gi and updates μt. At each learning step, the current momentum μt
is multiplied with the learning rate α to update the parameters, which allows us to more precisely
write gi:
t
θt = θt-i - αμi = θo - α ɪ2 μi	(25)
i=1
t	it
∆θ(t; i) = θt - θi = θo - α E μk 一 θo + α E μk = -α E μk	(26)
t-1
gt = gi + Zi δθ(% - 1； i) = gi - αZT X μk	(27)
k=i
We can now write μt, the approximated μ↑ using gi:
t-1
μt = (1- β)gt + (1- β) X βt-i^t	(28)
i=1
t-1	t-1	t-1
=(1 - β)gt + (1 - β) X βt-igi - (1 - β) X βt-iαZT X μk	(29)
i=1	i=1	k=i
t-1 t-1
=μt - α(1 - β) XX βt-iZTμk extract μ	(30)
i=1 k=i
t-1 k
=μt 一 α(1 一 β) E E βt-iZTμk change the sum indices for convenience (31)
k=1 i=1
=μt — ηt extract η	(32)
Let’s try to find a recursive form for ηt:
14
Under review as a conference paper at ICLR 2021
t-1 k	t-2 k
ηt - ηt-1 = α(1 - β) XXβt-iZTμk - α(1 - β) XXβt-1-iZT^k
k=1 i=1	k=1 i=1
t-2 k	t-1
XX(βt-iZTμk - βt-1-iZTμk) + X βIZTμt-l
k=1 i=1	i=1
t-2 k	t-1
XX(β - 1)βt-1-iZTμk + Xβt-iZTμt-i
k=1 i=1	i=1
t-1
=(β - 1)ηt-i + αβ(1- β) X βt-1-iZTμt-i
i=1
t
Let	ζt = (1 -β)Xβt-iZi
i=1
=	(β -1)ηt-ι + αβζt-1μt-1
ηt - ηt-ι = -(I - β)ηt-ι + αβζT-1μt-1
ηt = βηt-l + αβZT-ι μt-i
We can now write the full update as:
μt = μt - ηt
ηt = βηt-l + αβZT-iμt-i
t
μt = (I- β) X βt-igi = (I- β )gt + βμt-ι
i=1
t
ζt = (1-β)Xβt-iZi= (1-β)Zt+βζt-1
i=1
with η0 = 0.
This corresponds to an algorithm where one maintains η, μ and Z.
B.2 DERIVATION OF TAYLOR EXPANSIONS AND Z
For least squares regression, the expansion around θ of g(θ) is simply the Hessian of the loss δ2:
δ2 = 21(y - fθ(x))2
g(θ) = Vθ δ2
g(θ + ∆θ) = g(θ) + HJT ∆θ + o(k∆θk22)
This Hessian has the form
HJ = Vθf 0Vθf + δ2v2f
(33)
(34)
(35)
(36)
δ2 being small when a neural network is trained and V2 being expensive to compute, a common
approximation to Hj is to only rely on the outer product. Thus we can write:
Zreg = Vθf 0 Vθf
(37)
For classification, or categorical crossentropy, Z has exactly the same form, but where f is the
log-likelihood (i.e. output of a log-softmax) of the correct class.
15
Under review as a conference paper at ICLR 2021
For TD(0), the expansion is more subtle. Since TD is a semi-gradient method, when computing
g(θ), gradient of the TD loss δ2, we ignore the bootstrap target’s derivative, i.e. we hold Vθ (s0)
constant (unlike in GTD). On the other hand, when computing the Taylor expansion around θ, we
do care about how Vθ (s0) changes, and so its gradient comes into play:
g(θ) = (Vθ(X)- γVθ(x0)- r)VθVθ(x)
g(θ + ∆θ)i = gi(θ) + (Vθ%(X)- γVθVθ(x0))VθiVθ(x) ∙ ∆θ + δVθVθiVθ(x) ∙ ∆θ
g(θ + ∆θ) = g(θ) + ((VθVθ(X)- YVθVθ(X)) 0VθVθ(X))T ∆θ + δVθ%(x)T∆θ
Similarly for TD we ignore the second order derivatives and write:
ZTD = (VθVθ(X)- γVθVθ(x0))③ VθVθ(x)
For an n-step TD objective, the correction is very similar:
ZτD(n) = (Vθ Vθ (Xt)- YnVθ Vθ (Xt+n))乳 Vθ % (Xt)
For a forward-view TD(λ) objective this is also similar, but more expensive:
ZTD(λ) = (VθVθ(Xt)-(I - λ)vΘ(YλVθ(Xt+1)+ Y2λ2Vθ(Xt+2)+ …))③ vΘVθ(Xt)
Finally, to avoid maintaining n × n values for Z, it is possible to only maintain the diagonal of Z or
some block-diagonal approximation, at some performance cost.
B.3	The Linear Case
Much of RL analysis is done in the linear case, as it is easier to make precise statements about
convergence there. We write down such a case below, but we were unfortunately unable to derive
any interesting analyses from it.
Recall that the proposed method attempts to approximate μ*:
t
μt = (i - β) X βt-iVθt Ji(Ot)
i=1
which in the linear case Vθ(X) = θ>φ(X) is simply:
t
= (1-β)Xβt-iδiφi
i=1
which depends on θi through δi the TD error. As such we can write Z as:
ZTD = (VθVθ(x) - γVθVθ(x0))与 VθVθ(x) = (φ - γφ0)φ>
which interestingly does not depend on θ. To the best of our knowledge, and linear algebra skills, this
lack of dependence on θ does not allow for a simplification of the proposed correction mechanism
(in contrast with linear eligibility traces) due to the pairwise multiplicative t, i dependencies that
emerge.
B.4	Minibatching
It is possible to only do 3 (or 2 in the supervised case) “backward” calls (e.g.
torch.autograd.grad in PyTorch) even when computing this correction for several examples.
This is in contrast to other adaptive gradient methods (Romoff et al., 2020) which require per-input
16
Under review as a conference paper at ICLR 2021
gradients (these can be computed efficiently with methods such as Dangel et al. (2020), but remain
more expensive).
Because the gradient we care to correct is the gradient of the minibatch, i.e. the sum of gradients,
then we only have to correct that sum, and not individual gradients. This allows us to compute Z
out of the gradients of the minibatch instead of the individual gradients. When the gradient is:
mm
gt = Vθ Jt(θ) = Vθ X Jt,i(θ) = X Vθ Jt,i(θ)
then the first derivative used in its Taylor expansion is, by linearity of the derivative:
∂gt
丽
m
X vθ fi ③ vθ fi + δt,iHf
i
Vθ (E fi)乳Vθ (E fi) + V2 Jt,i
(38)
(39)
(40)
Ignoring the Hessian we only have to use the gradients obtained for the sum (or the mean if J is a
mean) of losses of the minibatch, which requires only one backpropagation regardless of batch size.
The derivation is analogous for TD(0). Note that this would not work if we used the second order
derivative due to the multiplicative δt,i factors.
C Additional Figures
C.IO.I.IHP①IBnbS UB①≡5
,8	.9 .95 .975	.99 .995	,999
β
Figure 11: TD(0) policy evaluation on Moun-
tain Car with varying β on a replay buffer. The
MSE is measured after 5k SGD steps against a
pretrained V π . Shaded areas are bootstrapped
95% confidence runs (10 seeds per setting). We
use a minibatch size of 4 to reveal interesting
trends.
10-2
1
1
-
W
FTl ——注国二。HPΦIBnbs UBən
4	16	64
Minibatch size
Figure 10: TD(0) policy evaluation on Moun-
tain Car with varying minibatch size on a replay
buffer. The MSE is measured after 5k SGD
steps against a pretrained V π . Shaded areas are
bootstrapped 95% confidence runs (20 seeds per
setting).
17
Under review as a conference paper at ICLR 2021
M 0.23
12345654321
bottom used top used
Figure 13: TD(0) policy evaluation on Moun-
tain Car with an MLP. We vary the number of
layers whose parameters are used for full μ Cor-
rection (n2 params); e.g. when “bottom used” is
3, the first 3 layers, those closest to the input, are
used; when “top used” is 1, only the last layer,
that predicts V from embeddings, is used. The
parameters of other layers are either corrected
with the diagonal correction or use normal mo-
mentum. Correcting “both ends” is not better
than just the bottom (not shown here).
Figure 12: TD(0) policy evaluation on Moun-
tain Car with RBF on a variety of hyperparame-
ter settings (10 seeds per setting). Here σ2 = 1.
4.0 -
3.5 -
3.0 -
2.5 -
2.0 -
1.5 -
1.0 -
0	1000	2000	3000 4000	5000
SGD steps
Figure 15: TD(0) policy evaluation on Cartpole
with varying hyperparameters replay buffer.
The MSE is measured after 5k SGD steps
against a pretrained V π . Shaded areas are boot-
strapped 95% confidence runs (10 seeds per set-
ting).
N(卜』——最)国=O,TIHP ①-IBnbS UB ①≡5
4.0 -i
3.5 -
3.0 -
2.5 -
2.0 -
1.5 -
N(卜』——最)国=O,TIHP ①-IBnbS UB ①≡5
0	1000 2000 3000 4000 5000
SGD steps
Figure 14: TD(0) policy evaluation on Ac-
robot with varying hyperparameters on a replay
buffer. The MSE is measured after 5k SGD
steps against a pretrained V π . Shaded areas are
bootstrapped 95% confidence runs (10 seeds per
setting).
18
Under review as a conference paper at ICLR 2021
Figure 16: Replication of Figure 3 including the
frozen targets baseline.
^目——盘)国=O,TIHP ①-IBnbS UB ①≡5
30.0
27.5
25.0
22.5
20.0
17.5 -
15.0
12.5
100	200	300	400	500
SGD steps (×103)
0
0.10
g
百
二
O
J
K
P
φ
W
n
b
S
§
φ
或
0.08
0.06
0.04
0.02
O Oracle: μ*
B Baseline: μ
C	Corrected: μ
C	Corrected: μdiag
Figure 17: Replication of Figure 9 including the
frozen targets baseline. Interestingly the models
trained with frozen targets eventually become
more precise than those without, but this only
happens after a very long time. This is explained
by the stability required for bootstrapping when
TD errors become increasingly small, which is
easily addressed by keeping the target network
fixed.

0.00 H------1------1------1------1-------
0	10000	20000	30000	40000	50000
SGD steps
Figure 18:	Replication of Figure 3 with 10×
more training steps. Methods gradually con-
verge to the value function.
0.0	..........1..................1-----I
10-2	10-1
Learning rate
Figure 19:	Effect of the learning rate on Moun-
tain Car, replay buffer policy evaluation, MSE
after 5k training steps.
19
Under review as a conference paper at ICLR 2021
D Hyperparameter Settings and Comments
All experiments are implemented using PyTorch (Paszke et al., 2019). We use Leaky ReLUs
throughout. All experimental code is available in supplementary materials.
On Leaky ReLUs: we did experiment with ReLU, tanh, ELU, and SELU activation units. The latter
3 units have more stable Taylor expansions for randomly initialized neural networks, but in terms of
experimental results, Leaky ReLUs were always significantly better.
D.1 Figure 1
We use an MLP with 4 layers of width nh .
We use the cross-product of nh ∈ {8, 16, 32}, β ∈ {0.9, 0.99}, α ∈ {0.005, 0.01}, nmb ∈
{4, 16, 64}.
D.2 Figure 2
We use a convolutional model with the following sequence of layers, following PyTorch convention:
Conv2d(3, nh, 3, 2, 1), Conv2d(nh, 2nh, 3, 2, 1), Conv2d(2nh, 2nh, 3, 2, 1), Conv2d(2nh, nh, 3,
1, 1), Flatten(), Linear(16nh, 4nh ), Linear(4nh, 4nh), Linear(4nh, 10), with LeakyReLUs between
each layer.
We use the cross-product of nh ∈ {8, 16}, β ∈ {0.9, 0.99}, α ∈ {0.005, 0.01}, nmb ∈ {4, 16, 64}.
D.3 Figure 3 and 16
We use an MLP with 4 layers of width nh .
We use the cross-product of nh ∈ {8, 16, 32}, β ∈ {0.9, 0.99}, α ∈ {0.5, 0.1, 0.05}, nmb ∈
{4, 16, 64}.
D.4 Figure 4
We use an MLP with 4 layers of width nh .
We use the cross-product of nh ∈ {16}, β ∈ {0.9, 0.99}, α ∈ {0.005, 0.001, 0.0005}, nmb ∈ {1}.
D.5 Figure 5 and 12
We use a linear layer on top of a grid RBF representation with each gaussian having a variance of
σ /ngrid.
For the RBF we use ngrid = 20, α = 0.1, nmb = 16, β = 0.99. For the MLP we use nh = 16,
α = 0.1, nmb = 16,β = 0.99.
D.6 Figure 6 and 7
We use RBFs with σ2 ∈ {4, 3.5, 3, 2.5, 2, 1.5, 1.25, 1, 0.75, 0.5}, ngrid ∈ {10, 20}, α ∈ {0.1, 0.01}.
D.7 Figure 8
We use an MLP with 4 layers of width nh = 16, α = 0.1, nmb = 16, β = 0.95.
D.8 Figure 9 and 17
We use the convolutional model of Mnih et al. (2013) with the same default hyperparameters, fol-
lowing PyTorch convention: Conv2d(4, nh, 8, stride=4, padding=4), Conv2d(nh, 2nh, 4, stride=2,
padding=2), Conv2d(2nh, 2nh, 3, padding=1), Flatten(), Linear(2nh × 12 × 12, 16nh), Linear(16nh,
nacts).
20
Under review as a conference paper at ICLR 2021
We use nh = 64, nmb = 32, for Adam we use α = 5 × 10-5 and β = 0.99, for our method we use
α = 10-4 and β = 0.9 (these choices are the result of a minor hyperparameter search of which the
best values were picked, equal amounts of compute went towards our method and the baseline so as
to avoid “poor baseline cherry picking”). For the frozen target baseline we update the target every
2.5k steps.
We were able to find similar differences between Adam and our method with a much smaller model,
but using the full correction instead of the diagonal one. Although the full correction outperforms
Adam when both use this small model, using so few parameters is not as accurate as the original
model described above, and we omit these results.
This small model is: Conv2d(4, nh, 3, 2, 1), Conv2d(nh, nh, 3, 2, 1), Conv2d(nh, nh, 3, 2, 1),
Conv2d(nh, nh, 3, 2, 1), Conv2d(nh, nacts, 6). For nh = 16 (which we used for experiments),
this model has a little less than 16k parameters, making Z about 250M scalars. While this is large,
this easily fits on a modern GPU, and the extra computation time required for the full correction
mostly still comes from computing two extra backward passes, rather than from computing Z and
the correction.
This is beyond the scope of this paper, but is worth of note: Interestingly this small model still
works quite well for control (both with Adam and our method). We have not tested this extensively,
but, perhaps contrary to popular RL wisdom surrounding Deep Q-Learning, we were able to train
decent MsPacman agents with (1) no replay buffer, but rather 4 or more parallel environments (the
more the better) as in A2C (Mnih et al., 2016) (2) no frozen target network (3) a Q network with
only 16k parameters rather than the commonplace 4.8M-parameter Atari DQN model. The only
“trick” required is to use 5-step Sarsa instead of 1-step TD (as suggested by the results of Fedus
et al. (2020), although in their case a replay buffer is used).
D.9 Figure 10
We use the same configuration than Figure 3 with nmb ∈ {1, 4, 16, 64}.
D.10 FIGURE 11
We use the same configuration than Figure 3 with β ∈ {.8, .9, .95, .975, .99, .995, .999}.
D.11 Figure 13
We use an MLP with 6 layers of width nh = 16, α = 0.1, nmb = 16, β = 0.9. We vary which
layers get used in the corrected momentum; see caption.
D.12 Figure 14 and 15
We use the same settings as Figure 3, with the exception that we do 5-step TD in Figure 14, for
Acrobot, and 3-step TD for Figure 15, for Cartpole. Using n > 1 appears necessary for convergence
for both our method and the baseline.
21