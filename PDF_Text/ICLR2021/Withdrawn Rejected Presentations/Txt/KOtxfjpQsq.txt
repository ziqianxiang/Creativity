Under review as a conference paper at ICLR 2021
Meta-Model-Based Meta-Policy Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Model-based reinforcement learning (MBRL) has been applied to meta-learning
settings and has demonstrated its high sample efficiency. However, in previous
MBRL for meta-learning settings, policies are optimized via rollouts that fully
rely on a predictive model of an environment. Thus, its performance in a real
environment tends to degrade when the predictive model is inaccurate. In this pa-
per, we prove that performance degradation can be suppressed by using branched
meta-rollouts. On the basis of this theoretical analysis, we propose Meta-Model-
based Meta-Policy Optimization (M3PO), in which the branched meta-rollouts
are used for policy optimization. We demonstrate that M3PO outperforms exist-
ing meta reinforcement learning methods in continuous-control benchmarks.
1	Introduction
Reinforcement learning (RL) methods have achieved remarkable success in many decision-making
tasks, such as playing video games or controlling robots (e.g., Gu et al. (2017); Mnih et al. (2015)).
In conventional RL methods, when multiple tasks are to be solved, a policy is independently learned
for individual tasks. In general, each learning requires millions of training samples from the environ-
ment. This independent learning with a large number of samples prevents conventional RL methods
from being applied to practical multi-task problems (e.g., robotic manipulation problems involving
grasping or moving different types of objects (Yu et al., 2019)). Meta-learning methods (Schmidhu-
ber et al., 1996; Thrun & Pratt, 1998) have recently gained much attention as a promising solution to
this problem (Finn et al., 2017). They learn a structure shared in the tasks by using a large number
of samples collected across the parts of the tasks. Once learned, these methods can adapt quickly to
new (or the rest of the) tasks with a small number of samples given.
Meta-RL methods have previously been introduced into both model-free and model-based settings.
For model-free settings, there are two main types of approaches proposed so far, recurrent-based
policy adaptation (Duan et al., 2017; Mishra et al., 2018; Rakelly et al., 2019; Wang et al., 2016)
and gradient-based policy adaptation (Al-Shedivat et al., 2018; Finn & Levine, 2018; Finn et al.,
2017; Gupta et al., 2018; Rothfuss et al., 2019; Stadie et al., 2018). In these approaches, policies
adapt to a new task by leveraging the history of past trajectories. Following previous work (Clavera
et al., 2018), we refer to these adaptive policies as meta-policies in our paper. In these model-
free meta-RL methods, in addition to learning control policies, the learning of policy adaptation
is also required (Mendonca et al., 2019). Thus, these methods require more training samples than
conventional RL methods.
For model-based settings, there have been relatively few approaches proposed so far. sæmundsson
et al. (2018) and Perez et al. (2020) use a predictive model (i.e., a transition model) conditioned by
a latent variable for model predictive control. Nagabandi et al. (2019a;b) introduced both recurrent-
based and gradient-based meta-learning methods into model-based RL. In these approaches, the
predictive models adapt to a new task by leveraging the history of past trajectories. In analogy to the
meta-policy, we refer to these adaptive predictive models as meta-models in our paper. Generally,
these model-based meta-RL approaches are more sample efficient than the model-free approaches.
However, in these approaches, the meta-policy (or the course of actions) is optimized via rollouts
relying fully on the meta-model. Thus, its performance in a real environment tends to degrade when
the meta-model is inaccurate. In this paper, we address this performance degradation problem in
model-based meta-RL.
1
Under review as a conference paper at ICLR 2021
After reviewing related work (Section 2) and preliminaries (Section 3), we present our work by first
formulating model-based meta-RL (Section 4). Model-based (and model-free) meta-RL settings
have typically been formulated as special cases of solving partially observable Markov decision
processes (POMDPs) (e.g.,Duan et al. (2017); Killian et al. (2017); Perez et al. (2020)). In these
special cases, specific assumptions, such as intra-episode task invariance, are additionally intro-
duced. However, there are model-based meta-RL settings where such assumptions do not hold (e.g.,
Nagabandi et al. (2019a;b)). To include these settings into our scope, we formulate model-based
meta-RL settings as solving POMDPs without introducing such additional assumptions. Then, we
conduct theoretical analysis on its performance guarantee (Section 5). We first analyse the perfor-
mance guarantee in full meta-model-based rollouts, which most of the previous model-based meta-
RL methods hold. We then introduce the notion of branched meta-rollouts. Branched meta-rollouts
are Dyna-style rollouts (Sutton, 1991) in which we can adjust the reliance on the meta-model and
real environment data. We show that the performance degradation due to the meta-model error in the
branched meta-rollouts is smaller than that in the full meta-model-based rollouts. On the basis of this
theoretical analysis, we propose a practical model-based meta-RL method called Meta-Model-based
Meta-Policy Optimization (M3PO) where the meta-model is used in the branched rollout manner
(Section 6). Finally, we experimentally demonstrate that M3PO outperforms existing methods in
continuous-control benchmarks (Section 7).
We make the following contributions in both theoretical and empirical frontiers. Theoretical fron-
tier: 1. Our work is the first attempt to provide a theoretical relation between learning the meta-
model and the real environment performance. In the aforementioned model-based meta-RL litera-
ture, it has not been clear how learning the meta-model relates to real environment performance. Our
theoretical analysis provides relations between them (Theorems 1, 2 and 3). This result theoretically
justifies meta-training a good transition model to improve overall performance in the real environ-
ment. 2. Our analysis also reveals that the use of branched meta-rollouts can suppress performance
degradation due to meta-model errors. 3. We refine previous fundamental theories proposed by
Janner et al. (2019) to consider important premises more properly (Theorems 4 and 5). This modi-
fication is important to strictly guarantee the performance especially when the model-rollout length
is long. Empirical frontier: We propose and show the effectiveness of M3PO. Notably, we show
that M3PO achieves better sample efficiency than existing meta-RL methods in complex tasks, such
as controlling humanoids.
2	Related work
In this section, we review related work on POMDPs and theoretical analysis in model-based RL.
Partially observable Markov decision processes 1: In our paper, we formulate model-based meta-
RL as solving POMDPs, and provide its performance guarantee under the branched meta-rollout
scheme. POMDPs are a long-studied problem (e.g., (Ghavamzadeh et al., 2015; Sun, 2019; Sun
et al., 2019)), and many works have discussed a performance guarantee of RL methods to solve
POMDPs. However, the performance guarantee of the RL methods based on branched meta-rollouts
has not been discussed in the literature. On the other hand, a number of researchers (Igl et al., 2018;
Lee et al., 2019; Zintgraf et al., 2020) have proposed model-free RL methods to solve a POMDP
without prior knowledge of the accurate model. However, they do not provide theoretical analyses
of performance. In this work, by contrast, we propose a model-based meta-RL method and provide
theoretical analyses on its performance guarantee.
Theoretical analysis on the performance of model-based RL: Several theoretical analyses on the
performance of model-based RL have been provided in previous work (Feinberg et al., 2018; Henaff,
2019; Janner et al., 2019; Luo et al., 2018; Rajeswaran et al., 2020). In these theoretical analyses,
standard Markov decision processes (MDPs) are assumed, and the meta-learning (or POMDP) set-
ting is not discussed. In contrast, our work provides a theoretical analysis on the meta-learning (and
POMDP) setting, by substantially extending the work of Janner et al. (2019). Specifically, Janner
et al. (2019) analysed the performance guarantee of branched rollouts on MDPs, and introduced
branched rollouts into a model-based RL algorithm. We extend their analysis and algorithm to a
meta-learning (POMDP) case. In addition, we modify their theorems so that important premises
1We include works on Bayes-adaptive MDPs (Ghavamzadeh et al., 2015; Zintgraf et al., 2020) because they
are a special case of POMDPs.
2
Under review as a conference paper at ICLR 2021
(e.g., the effect of multiple-model rollout factors) are more properly considered. See A.1 in the
appendix for a more detailed discussion of our contribution.
3	Preliminaries
Meta reinforcement learning: We assume online adaptation situations (Nagabandi et al., 2019a;b)
where the agent can leverage a few samples to adapt to a new task. Here, a task specifies the
transition probability and the reward function. Information about task identity cannot be observed
by the agent, and the task may change at any step in an episode. A meta-RL process is composed of
meta-training and meta-testing. In meta-training, a policy and a predictive model that are prepared
for efficient adaptation are learned with a meta-training task set. In meta-testing, on the basis of the
meta-training result, the policy and the predictive model adapt to a new task. For the adaptation,
the trajectory observed from the beginning of the episode to the current time step is leveraged. As
we noted earlier, we call an adaptive policy and a predictive model of this sort a meta-policy and a
meta-model, respectively.
Partially observable Markov decision processes: We formalize our problem with a POMDP,
which is defined as a tuple hO, S, A, pob, r, γ, psti. Here, O is a set of observations, S is a set
of hidden states, A is a set of actions, pob := O × S × A → [0, 1] is the observation prob-
ability, pst := S × S × A → [0, 1] is the state transition probability, r : S × A → R
is a reward function and γ ∈ [0, 1) is a discount factor. At time step t, these functions are
used as pst(st|st-1, at-1), pob(ot|st, at-1) 2 and rt = r(st, at). The agent cannot directly
observe the hidden state, but receives the observation instead. The agent selects an action on
the basis of a policy π := p(at+1 |ht). Here, ht is a history (the past trajectories) defined as
ht := {a0, o0, ..., at, ot}. We denote the set of the histories by H. Given the definition of the
history, the history transition probability can be defined as p(ht+1 |at+1, ht) := p(ot+1 |ht). Here,
p(ot+1|ht) := Pst+1 Pst p(st|ht)p(st+1|st, at)p(ot+1|st+1, at),wherep(st|ht) is the belief about
the hidden state. The goal of RL in the POMDP is to find the optimal policy ∏* that maximizes the
expected return R := P∞=0 Ytrt (i.e., π* = arg max Ea〜∏,h〜P [R]).
π
4	Formulating model-based meta-RL
In this section, we formulate model-based meta-RL as solving a POMDP by using a parameterized
meta-policy and a meta-model. The outline of our formulation is shown in Figure 4 in the appendix.
In our formulation, the task is included in the hidden state: S := T × S0. Here T is the set
of task τ and S0 is the set of the other hidden state factors s0 . With this definition, the state
transition probability, observation probability and reward function can be defined respectively as
follows: p(st+ι∣st ,at) = p(τt+ι, st+ι∣τt, st ,at), p(0t+1∣st+1, at) = p(0t+1∣τt+1, slt+ι,at) and
r(st, at) = r(τt, s0t, at). In addition, as with Finn & Levine (2018); Finn et al. (2017); Rakelly
et al. (2019), we assume that the task set T and the initial task distribution p(τ0) do not change
in meta-training and meta-testing. Owing to this assumption, in our analysis and algorithm, meta-
training and meta-testing can be seen as an identical one. Note that the task can be changed during
an episode. Namely, the value of τt+1 is not necessarily equal to that ofτt.
We define a meta-policy and meta-model as πφ(at+1 |ht) andpθ(rt, ot+1|ht), respectively. Here, φ
and θ are learnable parameters for them. rt and ot+1 are assumed to be conditionally independent
given ht, i.e.,p(rt,ot+ι∖ht) = p(rt∣ht) ∙pθ(ot+ι∣ht). As withp(ht+1∣at+1, ht), themeta-model
for the history can be defined as pθ(ht+1 |at+1, ht) := pθ(ot+1 |ht).
We use the parameterized meta-model and meta-policy as shown in Algorithm 1. This algorithm
is composed of 1) data collection in the real environment and 2) optimization of the meta-policy
and meta-model. In 1), the data is collected from the real environment with the meta-policy
and stored into a dataset D. In 2), the meta-policy and meta-model are optimized to maximize
Ea〜∏φ,r,h〜pθ [R] 一 C(∈m(θ), En(φ)). Here, Ea〜∏φ,r,h〜p@ [R] is a meta-model return (the return of
2For simplicity, we use these probabilities by abbreviating the subscripts “st” and “ob.
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Abstract Meta Model-based Meta-Policy Optimization (Abstract M3PO)
1:	Initialize meta-policy πφ , meta-model pθ and dataset D.
2:	for N epochs do
3:	Collect trajectories from environment in accordance with πφ: D = D ∪ {(ht, ot+1 , rt)}.
4:	Optimize ∏φ and pθ: (φ, θ) - arg maxEa〜∏φ r h〜p@ [R] - C(∈m(θ), e∏(φ)). Here, D is
(φ,θ)
used to evaluate m(θ) and π(φ) and generate an initial history h0.
5:	end for
the meta-policy on the meta-model) 3. C(m (θ), π(φ)) is a discrepancy depending on the two error
quantities m and π . Their detailed definitions are introduced in the next section.
Our POMDP-based formulation covers a wide range of meta-RL settings including Bayes-adaptive
MDPs (Zintgraf et al., 2020), hidden parameter-MDPs (Killian et al., 2017; Perez et al., 2020),
parameterized MDPs (Duan et al., 2017; Finn & Levine, 2018), and the settings considered by
Nagabandi et al. (2019a;b) and Jabri et al. (2019). These settings can be primarily recovered in our
formulation by introducing both or either of the following assumptions (Asm1 and Asm2) 4. Asm1:
the task does not change during an episode. Asm2: observation o is identical to the hidden state
s0 : o = s0 . The Bayes-adaptive MDPs, hidden-parameter MDPs and parameterized MDPs can be
recovered by introducing both Asm1 and Asm2. The setting of Nagabandi et al. (2019a;b) can be
recovered by introducing Asm2, and the setting of Jabri et al. (2019) can be recovered by introducing
Asm1. As a detailed example, we recover the parameterized MDPs in Appendix A.5.
5 Performance guarantees of model-based meta-RL
In this section, we analyse the performance guarantee of model-based meta-RL with an inaccurate
meta-model. In Section 5.1, we provide the performance guarantee in a full meta-model-based roll-
out. In Section 5.2, we introduce the notion of a branched meta-rollout and analyse its performance
guarantee. We show that the meta-model error is less harmful in the branched meta-rollout than that
in the full meta-model-rollout.
5.1	Performance guarantee in a full meta-model-based rollout case
Our goal is to outline a theoretical framework in which we can provide performance guarantees for
Algorithm 1. To show the guarantees, we construct a lower bound taking the following form:
Eπφ,p [R] ≥ Eπφ,pθ[R] - C(m(θ),π(φ)).	(1)
Here, Eπφ,p [R] denotes a true return (i.e., the return of the meta-policy in the real environment).
The discrepancy between these returns, C, can be expressed as the function of two error quantities:
the generalization error of the meta-policy and the distribution shift due to the updated meta-policy.
For our analysis, we define the bounds of the generalization error m and the distribution shift π as
follows:
Definition 1. 6m(θ) := maxt Ea〜∏0,h〜P DTV (p(ht+ι∣at+ι, ht)∣∣Pθ(ht+ι∣at+ι,ht)')]. Here,
DTV is a total variation distance and πD is the data-collection policy whose actions contained
in D follow 5.
Definition 2. e∏(φ) := maxht DTV (∏d(at+ι∣ht)∣∣∏φ(at+ι∣ht)).
We also assume that the expected reward is bounded by a constant rmax .
Definition 3. rmax > maxt Pst p(st|ht)r(st, at).
Now we present our bound, which is an extension of the theorem proposed in Janner et al. (2019).
3For simplicity, we use the abbreviated style Eπφ,pθ [R].
4For simplicity, we omit implementation-level assumptions (e.g., “meta-policy or meta-model are imple-
mented on the basis of gradient-based MAML” in Finn & Levine (2018); Nagabandi et al. (2019a)).
5As with Janner et al. (2019), to simplify our analysis, we assume that the meta-model can accurately
estimate reward. We discuss the case in which the reward prediction of a meta-model is inaccurate in A.8.
4
Under review as a conference paper at ICLR 2021
Theorem 1 (The POMDP extension of Theorem 4.1. in Janner et al. (2019)).
Let Em = maxt Ea〜∏D,h〜P [Dtv (p(ht+ι∣at+ι, ht)∣∣Pθ(ht+ι∣at+ι, ht))] and En =
maxht DTV (∏d(at+ι∣ht)∣∣∏φ(at+ι∣ht))∙ Then, the true returns are bounded from below by
meta-model returns of the meta-policy and discrepancy:
E∏φ ,P [R] ≥ E∏φ,Pθ [R] - r mJ 2γ (Em +2En) + π4Enʒ 1	⑵
(1 - γ)2	(1 - γ)
'--------------{z-------------}
C(m(θ),π(φ))
This theorem implies that the discrepancy of the returns under full meta-model-based rollout scales
linearly with both Em and En. If we can reduce the discrepancy C, the two returns are closer to each
other. As a result, the performance degradation is more significantly suppressed. In the next section,
we discuss a new meta-model usage to reduce the discrepancy induced by the meta-model error Em .
5.2	Performance guarantee in the branched meta-rollout case
The analysis of Theorem 1 relies on running full rollouts through the meta-model, causing meta-
model errors to compound. This is reflected in the bound by a factor scaling quadratically with the
effective horizon, 1/(1 - γ). In such cases, we can improve the algorithm by choosing to rely less
on the meta-model and instead more on real environment data.
To allow for adjustment between meta-model-based and model-free rollouts, we introduce the notion
of a branched meta-rollout. The branched meta-rollout is a kind of Dyna-style rollout (Sutton,
1991), in which the meta-model-based rollout is run as being branched from real environment data.
More concretely, the rollout is run in the following two processes. 1) We begin a rollout from a
history under the data-collection meta-policy’s history distribution pnD (ht), and 2) we then run k
steps in accordance with πφ under the learned meta-model pθ .
Under such a scheme, the true return can be bounded from below:
Theorem 2. Under the k steps branched meta-rollouts, using the bound of a meta-model error
under ∏d, Em = maxt Ea〜∏D,h〜p,t [Dtv (p(h0∣h, a)∣∣Pθ(h0∣h,a))], the bound of the meta-poIicy
shift En = maxht DTV (∏d∣∣∏φ), and the return on the meta-model E(a,h)〜Dmodel [R] where Dmodel
is the set of samples collected through branched rollouts, the following inequation holds,
1+γ2	γ-kγk+(k- 1)γk+1
EnΦ,P [R] ≥ Eah)〜Dmodel [R] - rmax ɪ (1 - γ)2 2En +	(] - Yy	(En + Em) (3)
γk - γ	γk
+ ----- (En + Em) + 1--Y (k + I)(En + Em)) ∙
The discrepancy factors relying on Em in Theorem 2 can be smaller than those relying on Em in
Theorem 1 6. This indicates that the performance degradation due to the meta-model error can be
more suppressed than that in the full meta-model-based rollout 7 8.
6	Meta-model-based meta-policy optimization with deep RL
In the previous section, we show that the use of branched meta-rollouts can suppress performance
degradation. In this section, on the basis of this result, we modify Algorithm 1 so that the meta-
policy and meta-model are optimized with E@h)〜Dmodeι [R] - C(Em,(θ), En(φ)), instead of with
nφ,pθ [R] - C(Em(θ), En(φ)).	, model
More specifically, we propose the following modifications to Algorithm 1:
Meta-policy optimization: The meta-policy is optimized with the branched meta-rollouts re-
turn Eg,h)〜Dmodeι [R] 8. For the optimization, we adapt PEARL (RakeIly et al., 2019) be-
6See Corollary 1 in the appendix.
7In A.6, we also prove that the discrepancy can be further reduced by introducing an additional assumption.
8To stabilize the learning, we omit C(m (θ), π (φ)) from the optimization objective for the meta-policy.
The transition of π (θ) during learning with this limited optimization objective is shown in Figure 12 in the
appendix. The result indicates that π (θ) tends to decrease as the training data size (epoch) grows.
5
Under review as a conference paper at ICLR 2021
Algorithm 2 Meta-Model-based Meta-Policy Optimization with Deep RL (M3PO)
1:	Initialize meta-policy πφ, meta-model pθ, environment dataset Denv, meta-model dataset Dmodel.
2:	for N epochs do
3:	Trainmeta-modelpθ with Denv： θ J arg maxEDenv [pθ(rt,ot+ι |ht)]
θ
4:	for E steps do
5:	Take actions according to πφ ; add the trajectory to Denv
6:	for M model rollouts do
7:	Sample ht uniformly from Denv
8:	Perform k-step meta-model rollouts starting from ht using meta-policy πφ ; add fictitious
trajectories to Dmodel
9:	end for
10:	for G gradient	updates do
11：	Update policy parameters with Dmodei： Φ J Φ 一 ^Φ JDmodeI ⑹
12:	end for
13： end for
14： end for
cause it achieved a good learning performance in meta-learning settings. We use the fic-
titious trajectories generated from the branched meta-rollouts to optimize the meta-policy.
Formally, πφ is optimized by using the gradient of optimization objective JDmodel (φ) :=
E(a,h)〜Dmodel [Dkl (∏Φl∣exp (Q∏φ 一 v∏φ))]. Here, DK L is the Kullback-Leibler divergence,
Q∏φ (at+1, ht) := E(r,h)〜Dmodel [Rlat+1, ht] and V∏Φ (At) := Pat+ι Q∏Φ (at+1, ht)π φ(at+l∖ht).
As in PEARL, in πφ, the latent context is estimated by using past trajectories. The esti-
mated context is then used to augment the policy input. Formally, the meta-policy is imple-
mented as ∏φ(at+ι∣ht) = Z∏z ∏φ(at+ι∣ot, z)pφ(z|ao, oo, ...,at,ot). Z is the latent context and
Pφ(z∣ao, 00,…,at, ot) is a context encoder. ∏φ(at+ι∣θt, Z) is the conditioned policy. Similarly, in
Qπφ and Vπφ, the estimated latent context is also used to augment their input.
Meta-model optimization: The meta-model is optimized to minimize the discrepancy (i.e., min-
imize m ) 9. For the meta-model, to consider both aleatoric and epistemic uncertainties, we use a
bootstrap ensemble of B dynamics models {pθ1, ...,pθB}. Here, piθ is the i-th conditional Gaussian
distribution with diagonal covariance: p(r, 0t+1∖ht) = N (rt+ι, 0t+1∣μθ (ht), σθ (ht)). μθ and
σθi are the mean and standard deviation, respectively. In our implementation, we use the recurrent-
based architecture inspired by Duan et al. (2017); Nagabandi et al. (2019b); Rakelly et al. (2019);
at each evaluation of the model, {a1, o1, ..., at-1, ot-1} in ht is fed to a recurrent neural network
(RNN), and then its hidden unit output and {at , ot } in ht are fed to the feed-forward neural net-
work that outputs the mean and standard deviation of the Gaussian distribution. We use the gated
recurrent unit (GRU) (Cho et al., 2014) for the RNN. The recurrent layer in the GRU is composed
of five sigmoid units. In addition, the feed-forward neural network is composed of an input layer,
two hidden layers, and an output layer. The input and hidden layers are composed of 400 swish
units (Ramachandran et al., 2017). To minimize m, we learn the meta-model via maximum like-
lihood estimation with Denv. As in Lakshminarayanan et al. (2017), the ensemble {pθ1, ..., pθB} is
learned on the shuffle of the real trajectories in Denv. We apply a re-parameterization trick to evaluate
the distributions, and the meta-model parameters θ are optimized with a gradient-based optimizer.
For the gradient-based optimizer we use Adam (Kingma & Ba, 2015). To avoid overfitting, we use
weight decay and early termination (Bishop, 2006).
The resulting algorithm is shown in Algorithm 2. The modifications “Meta-model optimization”
and “Meta-policy optimization” in the above paragraph are reflected in line 3 and lines 4-13, re-
spectively. In line 4, k-step branched meta-rollouts are run. The appropriately set k contributes to
decreasing the discrepancy, and suppresses performance degradation. Thus, we treat k as a hyperpa-
rameter, and set it to different values in different environments so that the discrepancy decreases. For
the experiments described in the next section, we tune the hyperparameters for this algorithm by a
grid search. The search result for the hyperparameter values is described in Table 1 in the appendix.
9The transition of m over training epochs is shown in Figure 11 in the appendix, and it indicates that the
model error tends to decrease as the number of epochs increases.
6
Under review as a conference paper at ICLR 2021
For our experiments, we implemented our algorithm by extending the codebase of Model-based
Policy Optimization 10 11. We made two main extensions: (1) introduce a latent context to the policy
as in PEARL and (2) replace a predictive model with the meta-model based on the RNN.
7	Experiments
In this section, we report our experiments 11 aiming to answer the following questions: Q.1: Can our
method (M3PO) outperform existing meta-RL methods? Q.2: How do meta-model-rollout lengths
k affect the actual performance?
In our experiments, we compare our method (M3PO) with two baseline methods: PEARL (Rakelly
et al., 2019) and Learning to adapt (L2A) (Nagabandi et al., 2019a). More detailed information
is described in A.10 in the appendix. We conduct a comparative evaluation of the methods on a
variety of simulated robot environments using the MuJoCo physics engine (Todorov et al., 2012).
We prepare the environments proposed in the meta-RL (Finn & Levine, 2018; Nagabandi et al.,
2019a; Rakelly et al., 2019; Rothfuss et al., 2019) and robust-RL (Hiraoka et al., 2019; Rajeswaran
et al., 2017) literature: Halfcheetah-fwd-bwd, Halfcheetah-pier, Ant-fwd-bwd, Ant-crippled-
leg, Walker2D-randomparams and Humanoid-direc. In the environments, the agent is required
to adapt to a fluidly changing task that the agent cannot directly observe. Detailed information about
each environment is described in A.11 in the appendix.
Regarding Q1, our experimental results indicate that M3PO outperforms existing meta-RL methods.
In Figure 1, the learning curves of M3PO and existing meta-RL methods (L2A and PEARL) on
meta-training phases are shown. These learning curves indicate that the sample efficiency of M3PO
is better than those of L2A and PEARL 12. The performance (return) of L2A remains poor and does
not improve even when the training data increases. PEARL can improve meta-policy performance
via training in all environments. However, the degree of improvement of PEARL is smaller than
that of M3PO. In a number of the environments (e.g., Halfcheetah-pier), the relative performance
of M3PO against PEARL becomes asymptotically worse. This indicates that, as with Nagabandi
et al. (2018), dynamic switching from M3PO to PEARL or other model-free approaches needs to be
considered to further improve overall performance.
Regarding Q2, we conducted an evaluation of M3PO by varying its model-rollout length k. The
evaluation results (Figure 2) indicate that the performance tends to degrade when the model-rollout
length is long. We can see significant performance degradation especially in Ant-fwd-bwd and
Humanoid-direc. In Ant-fwd-bwd, the performance at k = 100 is significantly worse than that at
k = 10. In Humanoid-direc, the performance at k = 5 is significantly worse than that at k = 1. As
we have seen, the performance degradation in Humanoid-direc is more sensitive to the model-rollout
length than that in Ant-fwd-bwd. One reason for this is that the meta-model error in Humanoid-direc
is larger than that in Ant-fwd-bwd (Figure 11 in the appendix).
An example of meta-policies learned by M3PO with 200k samples in Humanoid-direc is
shown in Figure 3, and it indicates the learned policy successfully adapts to different tasks.
Additional examples of meta-policy learned by PEARL and M3PO are shown in the video
at the following link: https://drive.google.com/file/d/1DRA-pmIWnHGNv5G_
gFrml8YzKCtMcGnu/view?usp=sharing
8	Conclusion
In this paper, we analysed the performance guarantee (and performance degradation) of MBRL in
a meta-learning setting. We first formulated model-based reinforcement learning in a meta-learning
setting as solving a POMDP. We then conducted theoretical analyses on the performance guarantee
in both the full model-based rollout and the branched meta-rollout. We showed that the performance
degradation due to the meta-model error in the branched meta-rollout is smaller than that in the full
10https://github.com/JannerM/mbpo
11The source code to replicate the experiments will be open to the public.
12Note that, in an early stage of the training phase, there are many test episodes in which unseen tasks appear.
Therefore, the improvement of M3PO over L2A and PEARL at the early stage of learning indicates its high
adaptation capability for unseen situations.
7
Under review as a conference paper at ICLR 2021
Enlə,i能白①><
Training samples
——1_5	—PEARL
---PEARL-Iong--M3PO-long
Figure 1: Learning curves. In each figure, the vertical axis represents returns, and the horizontal axis represents
numbers of training samples (x1000). The meta-policy and meta-model are fixed and evaluated in
terms of their average return on 50 episodes at every 5000 training samples for L2A and 1000 training
samples for the other methods. In each episode, the task is initialized and changed randomly. Each
method is evaluated in six trials, and average returns on the 50 episodes is further averaged over the
trials. The averaged returns and their standard deviations are plotted in the figures. We also plot the
best performances of PEARL and M3PO trained for a longer-term as PEARL-long and M3PO-long.
Learning curves of PEARL and M3PO in the longer-term training are shown in Figure 13.
U-In电能巴①><
Figure 2: Learning curves of M3PO. In each figure, the vertical axis represents returns, and the horizontal
axis represents numbers of training samples (x1000). The meta-policy and meta-model are fixed and
evaluated their expected returns on 50 episodes per 1000 training samples. We run experiments by
varying the rollout length k of M3PO. In these experiments, the values of hyperparamters except
k are the same as those described in Table 1. Each case is evaluated in six trials, and the average
return on the 50 episodes is further averaged over the trials. The averaged expected returns and their
standard deviations are plotted in the figures. Dashed lines represent M3PO’s learning curves in
Figure 1. k = x → y means k values linearly changes from x to y during leaning.
meta-model-based rollouts. Based on the theoretical result, we introduced branched meta-rollouts
to policy optimization and proposed M3PO. Our experimental results show that it achieves better
sample efficiency than PEARL and L2A.
8
Under review as a conference paper at ICLR 2021
Figure 3: Example of a meta-policy learned by M3PO with 200k samples in Humanoid-direc.The humanoid
is highlighted in accordance with a task (red: move to left, and blue: move to right). The figures are
time-lapse snapshots, and the first figure on the left is the snapshot at the beginning time. Figures
show the policy successfully adapting to the task change.
9
Under review as a conference paper at ICLR 2021
References
Maruan Al-Shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel.
Continuous adaptation via meta-learning in nonstationary and competitive environments. In Proc.
ICLR, 2018.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In Proc. EMNLP, 2014.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. In Proc. CoRL, pp. 617-629,
2018.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl 2 : Fast
reinforcement learning via slow reinforcement learning. In Proc.ICLR, 2017.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey
Levine. Model-based value expansion for efficient model-free reinforcement learning. In Proc.
ICML, 2018.
Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient
descent can approximate any learning algorithm. In Proc.ICLR, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proc. ICML, pp. 1126-1135, 2017.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement
learning: A survey. FOUndatiOnS and Trends® in MaChine Learning, 8(5-6):359^83, 2015.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning
for robotic manipulation with asynchronous off-policy updates. In Proc. ICRA, pp. 3389-3396.
IEEE, 2017.
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-
reinforcement learning of structured exploration strategies. In Proc. NeUrIPS, pp. 5302-5311,
2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proc. ICML, pp. 1856-
1865, 2018.
Mikael Henaff. Explicit explore-exploit algorithms in continuous state spaces. In Proc. NeUrIPS,
2019.
Takuya Hiraoka, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, and Yoshimasa Tsuruoka.
Learning robust options by conditional value at risk optimization. In Proc. NeUrIPS, 2019.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for POMDPs. In Proc.ICML, pp. 2117-2126, 2018.
Allan Jabri, Kyle Hsu, Abhishek Gupta, Ben Eysenbach, Sergey Levine, and Chelsea Finn. Unsu-
pervised curricula for visual meta-reinforcement learning. In Proc. NeUrIPS, pp. 10519-10531,
2019.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-
based policy optimization. In Proc. NeUrIPS, 2019.
Taylor W Killian, Samuel Daulton, George Konidaris, and Finale Doshi-Velez. Robust and efficient
transfer learning with hidden parameter Markov decision processes. In Proc. NIPS, pp. 6250-
6261, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015.
10
Under review as a conference paper at ICLR 2021
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Proc. NIPS, pp. 6402-6413, 2017.
Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model. arXiv PrePrint arXiv:1907.00953,2019.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic
framework for model-based deep reinforcement learning with theoretical guarantees. In Proc.
ICLR, 2018.
Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn.
Guided meta-policy search. In Proc. NeUrIPS, pp. 9653-9664, 2019.
Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-
learner. In Proc. ICLR, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
A. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based
deep reinforcement learning with model-free fine-tuning. In Proc. ICRA, pp. 7559-7566, 2018.
Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and
Chelsea Finn. Learning to adapt in dynamic, real-world environments via meta-reinforcement
learning. In Proc. ICLR, 2019a.
Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning:
Continual adaptation for model-based RL. In Proc.ICLR, 2019b.
Christian F Perez, Felipe Petroski Such, and Theofanis Karaletsos. Generalized hidden parameter
MDPs transferable model-based RLina handful of trials. In Proc. AAAI, 2020.
Aravind Rajeswaran, Sarvjeet Ghotra, Sergey Levine, and Balaraman Ravindran. EPOpt: Learning
Robust Neural Network Policies Using Model Ensembles. In Proc.ICLR, 2017.
Aravind Rajeswaran, Igor Mordatch, and Vikash Kumar. A game theoretic framework for model
based reinforcement learning, 2020.
Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. In Proc. ICML, pp. 5331-5340,
2019.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv
PrePrint arXiv:1710.05941, 2017.
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. In Proc. ICLR, 2019.
Steindor sæmundsson, Katja Hofmann, and Marc Peter Deisenroth. Meta reinforcement learning
with latent variable Gaussian processes. arXiv PrePrint arXiv:1803.07551, 2018.
Juergen Schmidhuber, Jieyu Zhao, and MA Wiering. Simple principles of metalearning. TeChniCaI
report IDSIA, 69:1-23, 1996.
David Silver and Joel Veness. Monte-Carlo planning in large POMDPs. In Proc. NIPS, pp. 2164-
2172, 2010.
Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya
Sutskever. Some considerations on learning to explore via meta-reinforcement learning. arXiv
PrePrintarXiV:1803.01118, 2018.
Wen Sun. TOWardS GeneraIizatiOn and EffiCienCy in ReinfOrCement Learning. PhD thesis, Carnegie
Mellon University, 2019.
11
Under review as a conference paper at ICLR 2021
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL
in contextual decision processes: PAC bounds and exponential improvements over model-free
approaches. In Proc. COLT, pp. 2898-2933, 2019.
Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart
BUlletin, 2(4):160-163, 1991.
Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In SCienCe &
business media, pp. 3-17. Springer, 1998.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Proc.IROS, pp. 5026-5033. IEEE, 2012.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv PrePrint arXiv:1611.05763, 2016.
Grady Williams, Andrew Aldrich, and Evangelos Theodorou. Model predictive path integral control
using covariance variable importance sampling. arXiv PrePrint arXiv:1509.0i149, 2015.
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey
Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.
In Proc. CoRL, 2019.
Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. VariBAD: A very good method for Bayes-adaptive deep RL via meta-
learning. In Proc. ICLR, 2020.
12
Under review as a conference paper at ICLR 2021
A Appendices
A.1 How does our work differ from Janner et al. (2019)?
Although our work is grounded primarily on the basis of Janner et al. (2019), we provide non-trivial
contributions in both theoretical and practical frontiers: (1) We provide theorems about the relation
between true returns and returns on inaccurate predictive models (model returns) on a “meta-learning
(POMDPs)” setting (Section 5). In their work, they provide theorems about the relation between
the true returns and the model returns in the branched rollout in MDPs. In contrast, we provide
theorems about the relation between the true returns and of the model returns in the branched rollout
in POMDPs. In addition, in the derivation of theorems (Theorems 4.2 and 4.3) in their work, a
number of important premises are not properly taken into consideration (the detailed discussion on
it is described in the second paragraph in A.7). We provide new theorems, in which the premises are
more properly reflected, for both MDPs and POMDPs (A.3, A.6, and A.7). (2) We extend the model-
based policy optimization (MBPO) proposed by Janner et al. into the meta-learning (POMDPs)
setting (Section 6). MBPO is for the MDP settings and does not support POMDP settings, while our
method (M3PO) supports POMDP settings. Furthermore, we empirically demonstrate the usefulness
of the meta-model usage in the branched rollout manner in the POMDP settings (Section 7).
13
1
A.2 Outline of our model-based meta-RL formulation
Figure 4: The outline of Model-based meta-RL formulation. Here, τ is a task, s0 is hidden state factors, o is an observation, h is a history, a is an action and r is a reward.
Under review as a conference paper at ICLR 2021
Under review as a conference paper at ICLR 2021
A.3 Proofs of theorems in the main content
Before starting the derivation of the main theorems, we introduce a lemma useful for bridging
POMDPs and MDPs.
Lemma 1 (Silver & Veness (2010)). Given a POMDP hO, S, A, pob, r, γ, psti, consider the de-
rived MDP with histories as states, 〈H, A, γ, r,p加),where ∀t. P班：=p(ht+1∣at+1, ht)=
Pst∈s Pst+1∈SP(StIht)P(St+i|st, aDP(Ot+"st+ba/ and r(ht,at) ：= Pst∈sP(st|ht)r(st,at)∙
Then, the value function Vn (ht) of the derived MDP is equal to the value function Vπ (ht) of the
POMDP∙
Proof∙ The statement can be derived by backward induction on the value functions. See the proof
of Lemma 1 in Silver & Veness (2010) for details.	□
Proof of Theorem 1:
Proof∙ By Lemma 1, our problem in POMDPs can be mapped into the problem in MDPs, and then
Theorem 4.1 in Janner et al. (2019) can be applied to the problem.	□
Similarly, the proof of Theorem 2is derived by mapping our problem into that in MDPs by Lemma 1
and leveraging theoretical results on MDPs.
Proof of Theorem 2:
Proof∙ By Lemma 1, our problem in POMDPs can be mapped into the problem in MDPs, and then
Theorem 4 in A.7 can be applied to the problem.	□
A.4 The discrepancy relying on the meta-model error in Theorem 1 and that in
Theorem 2
Corollary 1. The discrepancy factors relying on m in Theorem 1, CTh1,m, are equal to or larger
than those relying on m at k = 1 in Theorem 2, CTh2,m∙
Proof∙ By Theorem 1 and 2,
2	__ 2γem
CTh1,m = rmax "Γ∖ Γ√.
,	(1 - γ)2
(4)
γ
CTh2,m = rmax —2em.	(5)
1-γ
Given that γ ∈ [0, 1), rmax > 0 and m ≥ 0,
Cτhl - CTho	= r 4γem - 2γ2 em	(6)
LyTh1,m	TyThmm = r max ∕1 ʌn	' '
(1 - γ)2
≥ 0.
□
A.5 Connection to a typical meta-RL setting
In Section 4, we formulate model-based meta-RL to solve a POMDP. However, this formulation
may make it difficult for certain readers to comprehend the connection to a typical meta-RL set-
ting. Although in the normative meta-RL setting (the parameterized MDPs (Finn et al., 2017)), the
objective to be optimized is given as the return expected with respect to a task distribution, such
an objective does not appear in the formulation in Section 4. In this section, we show that such an
objective can be derived by specializing the formulation in Section 4 under a number of assumptions
15
Under review as a conference paper at ICLR 2021
(Corollary 2). Then, we explain why we did not adopt such a specialization and maintained a more
abstract formulation in Section 4.
First, letting a task set and a task distribution be denoted by T and p(τ) where τ ∈ T , respectively,
we introduce the following assumptions:
Assumption 1. S := O × T.
Assumption 2. p(st+ι∣st,at) := p(θt+ι∣θt,τt ,at) ∙ 1 (τt+ι = Tt).
Assumption 3. For t > 0, p(st∣ht) := p(τt∣ht) ∙ 1(τ = τ°).
Assumption 4. p(τo∣ho) := p(τo).
Here, 1(∙) is the indicator function that returns one if the argument is true, and zero otherwise.
With these assumptions, the following corollary holds:
Corollary 2. Given a POMDP hO, S, A, pob, r, γ, psti and a task set T, consider the parame-
terized MDP with histories as states,〈H, A, γ, r,pobi, where ∀t. Pob ：= p(ot+ι∣ot,τo,at) and
r := r(ot, τo, at). Under Assumptions 2〜5, the expected return on the parameterized MDP
Ea〜π,h〜p,τ〜p(τ) [P∞ Ytrt] := PT∈TP(T)Ea〜∏,h〜P [P∞ Yt亍t∣τ] is equal to the expected return
on the POMDP Ea〜∏,
h〜P [R].
Proof. By applying Lemma 1, the value function in a POMDP hO, S, A, Pob, r, γ,Psti can be
mapped to the value function 吃(ht) in the derived MDP, which is(H, A,γ,r,Phii, where ∀t.phi :=
P(ht+l|at+Lht) = Pst∈s Pst+1∈SP(Stlht)P(St+1∣st,a/P(Ot+1∣st+Lat) and r(ht,at) : =
Pst∈S P(St lht)r(St, at).
By applying the assumptions, this value function can be transformed to a different representation
that explicitly contains T and its distribution:
Fort > 0,
Vrπ(ht) =	π(at+1lht)	P(Stlht)r(St, at)
at+1	(st∈S
+γ Σ Σ Σ P(Stlht)P(St+1lSt, at)P(ot+1lSt+1, at)Vrπ(ht+1)
ot+1 ∈O st∈S st+1 ∈S
=E∏(at+ι∣ht) ∖ r(θt,τo,at)+γ E P(θt+ι∣θt,τo,at) K(ht+ι)
at+1	l'{t_}	ot+ι∈O'	Pob	}
Likewise, for t = 0,
(7)
)
Vrπ(h0) =	π(a1 lh0)
a1
a0) + γ	P(T0)P(o1lo0,T0, a0)Vrπ(h1)
o1 ∈O τ0
P(T0)	π(a1lh0) r(o0, T0, a0) + γ	P(o1lo0,T0, a0)Vrπ(h1) .
τ0	a1	o1 ∈O
、	一__ 一	/
(8)
o1∈O
z^^^^^^
Vn (ho)
Therefore,
Ea 〜Π,h〜P [R] = Ep(Ao)W(ho)
h0
= XP(To) XP(ho)Vrrπ(ho)
τ0	h0
=Ea 〜π,h 〜p,τ 〜P(T)
(9)
∞
X γ t rrrt
t
16
Under review as a conference paper at ICLR 2021
vertical axis represents k ∈ [1, 50] and each horizontal axis represents m, m0 ∈ [0, 1]. In all figures,
for evaluating the discrepancy values, we set the other variables as rmax = 1, π = 1 - m for (a),
and π = 1 - m0 for (b). A key insight with the figures is following two points. First, discrepancy
values in Theorem 2 (a) tend to increase as the k value increases, whereas those in Theorem 3 (b) are
not. Thus, there could be an optimal k value that larger than one. Second, the contour colours in (b)
are paler than those in (a). This implies, given m = m0 , Theorem 3 provides a tighter bound of the
performance.
□
By Corollary 2, our formulation in Section 4 can be specialized into a problem where the
objective to be optimized is given as the return expected with respect to a task distribution.
We can derive the meta-model returns with discrepancies for bounding the true return (i.e.,
Ea〜∏φ,h〜pθ,τ〜p(τ) [P∞ γtrt] - C(em, e∏)) by using Corollary 2 instead of Lemma 1 in the proofs
of Theorem 1 and replacing pθ (o∕o0,τ0, at) with pθ (o∕ht-ι).
The main reason that we do not adopt such a specialization in Section 4 is to avoid restrictions
induced by the assumptions (Assumption 2〜5). For example, Assumption 2 states that hidden
states are composed by observations and a task. Since observations can be observed from an agent
by its definition, the information in the hidden states that cannot be observed from an agent is the
task. However, in many meta-RL settings (e.g., application domains (Jabri et al., 2019) where video
images are used as the observation), there should be other information that cannot be observed from
the agent. In addition, Assumptions 3 and 4 state that the task is invariant in each episode. However,
in many meta-RL settings (e.g., Nagabandi et al. (2019a;b)), the task can change to a different one in
each episode. To avoid such restrictions, we decided not to specialize the formulation by introducing
the assumptions.
A.6 Additional analysis of the performance degradation in the branched
meta-rollout
Figure 5a shows that the discrepancy value in Theorem 2 tends to monotonically increase as the
value of k increases, regardless of the values of γ and em . This means that the optimal value of k
is always 1. However, intuitively, we may expect that there is an optimal value for k that is higher
than 1 when the meta-model error is small. As will be shown in this section, this intuition is correct.
One of the main reasons for the mismatch of the intuition and the tendency of the discrepancy in
Theorem 2 is that, for its analysis, the meta-model error on the data collection meta-policy πD (i.e.,
em) is used instead of that on the current meta-policy πφ . This ignorance of the meta-model error
on the current meta-policy induces the pessimistic estimation of the discrepancy in the analysis (see
the evaluation of “term B” and “term C” in the proof of Theorem 4 via the proof of Theorem 2 in
the appendix for more details).
To estimate the discrepancy more tightly, as in Janner et al. (2019), we introduce the assumption that
the meta-model error on the current meta-policy can be approximated by eπ and em :
Assumption 5. An approximated meta-model error on a current policy em0: em0 (eπ) ≈ em +
e∏d⅛m0, where dmm∙ is the local change of em> with respect to e∏.
17
Under review as a conference paper at ICLR 2021
To see the tendency of this approximated meta-model error, we plot the empirical value of dm0 /dπ,
varying the size of training samples for the meta-model in Figure 10 in A.12. The figure shows that,
as the training sample size increases, the value tends to gradually approach zero. This means that
training meta-models with more samples provide better generalization on the nearby distributions.
Equipped with the approximated meta-model’s error on the distribution of the current meta-policy
πφ, we arrive at the following bound:
Theorem3. Let / ≥ maxt Ea 〜∏φ,h 〜P [D tv (p(h0∣h, a)M (h0∣h,a))],
1 + γ 2
EπΦ,P [R] ≥ E(a,h)-Dmodel [R] - rmax ɪ (ι - γ)2 2En +
γ-kγk+(k- 1)γk+
(1-γ)2
1
(m0
- π)	(10)
γk - γ	γk
+-------丁(EmO - En ) + τ-------(k + I)(EmO -
γ-	1	1-	γ
π) .
Proof. By Lemma 1, our problem in POMDPs can be mapped into the problem in MDPs, and then
Theorem 5 in A.7 can be applied to the problem.	□
Given that Em = EmO, it is obvious that the discrepancy in Theorem 3 is equal to or smaller than
that in Theorem 2. In the discrepancy in Theorem 3, all terms except the first term become negative
when EmO < En. This implies that the optimal k that minimizes the discrepancy can take on the value
higher than 1 when the meta-model error is relatively small. The empirical trend of the discrepancy
value (Figure 5b) supports it; when EmO is lower than 0.5 (i.e., EmO < En), the discrepancy values
decrease as the value of k grows regardless of the value of γ . This result motivates us to set k to the
value higher than 1 in accordance with the meta-model error for reducing the discrepancy.
A.7 THE DERIVATION OF THE RELATION OF THE RETURNS IN k-STEP BRANCHED ROLLOUTS
(k ≥ 1) IN MARKOV DECISION PROCESSES
In this section, we discuss the relation of the true returns and the model returns under the branched
rollout in an MDP, which is defined by a tuple hS, A, r, γ,psti. Here, S is the set of states, A is the
set of actions, pst := p(s0|s, a) is the state transition probability for any s0, s ∈ S and a ∈ A, r is
the reward function and γ ∈ [0, 1) is the discount factor. At time step t, the former two functions
are used as p(st|st-1, at), rt = r(st, at). The agent selects an action on the basis of a policy
π := p(at+1 |st). We denote the data collection policy by πD and the state visitation probability
under πD and p(s0 |s, a) by pnD (st). We also denote the predictive model for the next state by
Pθ(s0∣s, a). In addition, we define the upper bounds of the reward scale as rmax > maxs,a |r(s, a)|.
Note that, in this section, to discuss the MDP case, we are overriding the definition of the variables
and functions that were defined for the POMDP case in the main body. In addition, for simplicity,
We use the abbreviated style E∏,p [R] for the true return Ea〜∏,s〜P [R := P∞=0 γtrt].
Although the theoretical analysis on the relation of the returns in the MDP case is provided by
Janner et al. (2019), in their analysis, a number of important premises are not properly taken into
consideration. First, although they use the replay buffers for the branched rollout (i.e. datasets Denv
and Dmodel in Algorithm 2 in Janner et al. (2019)), they do not take the use of the replay buffers
into account in the their theoretical analysis. Furthermore, they calculate state-action visitation
probabilities based solely on a single model-based rollout factor. In the branched rollout, state-
action visitation probabilities (except for the one at t = 0) should be affected by multiple past
model-based rollouts. For example, a state-action visitation probability at t (s.t. t > k) is affected
by the model-based rollout branched from real trajectories at t - k and ones from t - k + 1 to t - 1
(in total, k model-based rollouts). However, in their analysis (the proof of Lemma B.4 in Janner
et al. (2019)), they calculate state-action visitation probabilities based solely on the model-based
rollout. For example, in their analysis, it is assumed that a state-action visitation probability at t (s.t.
t > k) is affected only by the model-based rollout branched from real trajectories at t - k. These
oversights of important premises in their analysis induce a large mismatch between those for their
theorems and those made for the actual implementation of the branched rollout (i.e., Algorithm 2 in
18
Under review as a conference paper at ICLR 2021
ɪ P版i ⑸，ɑt)	ɪ P鼠i S, αt)
i=0	i=O
Figure 6: An example of branched rollouts with k = 3. Here, the blue dots represent trajectories contained in
an environment dataset Denv and the yellow dots represent fictitious trajectories generated in accor-
dance with a current policy π under a predictive model pθ .
Janner et al. (2019)) 13. Therefore, we decided to newly derive the theorems on the branched rollout,
reflecting these premises more appropriately.
The outline of our branched rollout is shown in Figure 6. Here, we assume that the trajectories
collected from the real environment are stored in a dataset Denv. The trajectories stored in Denv
can be seen as trajectories following the true dynamics p(s0|s, a) and data collection policy (i.e., a
mixture of previous policies used for data collection) πD . At each branched rollout, the trajectories
in Denv are uniformly sampled 14, and then starting from the sampled trajectories, k-step model-
based rollouts in accordance with π under pθ is run. The fictitious trajectories generated by the
branched rollout is stored in a model dataset Dmodel 15. This process more appropriately reflects
the actual implementation of the branched rollout (i.e., lines 5-8 in Algorithm 2) in Janner et al.
(2019) 16. The performance ofπ is evaluated as the expected return under the state-action visitation
probability in Dmodel.
13The mismatch becomes larger especially when the model-rollout length k is large because state-action
visitation probabilities are affected by these rollouts more significantly with large k .
14Thus, the initial state probability for the rollout starting from the trajectories follows pπD (s)
15Here, when the trajectories are stored in Dmodel , the states in the trajectories are augmented with time step
information to deal with the state transition depending on the time step.
16Note that the extension of this process to the POMDP case is compatible with the implementation of the
branched meta-rollout in our algorithm (lines 4-13 in Algorithm 2).
19
Under review as a conference paper at ICLR 2021
Formally, We define the return in the branched rollout E@s)〜Dmodel [R] as:
Eas) ~Dmodel[R]	: =	k-1 pπD (s0, a0)r(s0, a0) +	γtptb<r k (st, at)r(st, at) s0,a0	t=1 st,at ∞ +XXγtptb≥r k(st, at)r(st, at)	(11) t=k st,at
pt<r k (st, at) :=	1 t-1 t Ept<k,i(St，at)	(12) t i=0
pt≥r k (st, at)	:=	1 k-1 k X pb≥ k,i(St, at)	(13)
pt<r k,i (st, at)	:=	E	E	PnD (Si)∏j=ipθ(Sj+ι∣Sj,aj+i)n(aj+i|Sj)	(14) si,...,st-1 ai,...,at-1
pt≥r k,i (st, at)	:=	XX st-k+i,...,st-1 at-k+i,...,at-1 pπD (St-k+i)∏j=t-k+ipθ (Sj+1 |Sj, aj+1)π(aj+1 |Sj )	(15)
Here, ptb<r k,i(st, at) and ptb≥r k,i (st, at) are the state-action visitation probabilities that the i-th yellow
trajectories (node) from the bottom at each timestep t in Figure 6 folloW. In later discussions, for
simplicity, we use the abbreviated style EDmOdeI [R] for E(C,s)〜DmOdel [R].
Before starting the derivation of theorems, We introduce a useful lemma.
Lemma	2.	Assume that the	rollout process in which the	policy and	dynamics can
be switched to other ones at time step tsw.	Letting two probabilities be p1 and
p2,	for	1	≤ t0	≤ tsw,	we assume that the dynamics distributions	are bounded
as	Cm, pre	≥	maxt，Es^pι [D TV (P1(st0∣st0-1,at0)∣∣p2(st0∣st0-1,at0))]. In	addition, for
tsw	<	t0	≤	t, we assume	that the dynamics distributions	are bounded	as m,post ≥
maxto Es 〜pi [D tv (p1(st0 ∣sto-ι,ato )∣∣p2(st0 ∣st0-1 ,ato))]. Likewise, the policy divergence is
bounded by Cπ,pre and Cπ,post. Then, the following inequation holds
|p1 (st, at) - p2 (st , at )| ≤ 2(t - tsw)(Cm,post + Cπ,post) + 2tsw (Cm,pre + Cπ,pre)	(16)
st,at
20
Under review as a conference paper at ICLR 2021
Proof. The proof is done in a similar manner to those ofLemma B.1 and B.2 in (Janner et al., 2019).
E ∖pι(st,at) — P2(st,at)∖
st ,at
=E ∖p1(0t)p1(st∖0t) -P2(0t)p2(%%)∖
st,at
= E ∖p1(0t)p1(st∖0t) -P1(at)p2(st∖ɑt) + (pι(at) 一P2(at))p2(st∖at)∖
st,at
≤ E pι(αt) ∖pι(st∖at) - p2(st∖at)∖ + E ∖pι(αt) - p2(at)∖
st,at	at
≤ E pι(αt) ∖pι(st∖at) -p2(st∖at)∖ + E ∖pι(αt, st-1) - p2(αt, st-i)∖
st,at	at,st-ι
£pi(at) ∖pι(st∖at) -p2(st∖at)∖
st,at
+ E ∖p1(st-i)p1 (αt∖st-i) -p1(st-i)p2(αt∖st-i) + (pι(st-i) -p2(st-i))p2(αt∖st-i)∖
at,st-ι
≤ £pi(at) ∖pι(st∖at) - p2(st∖αt)∖ + E pι(st-i) ∖pι(αt∖st-i) - p2(αt∖st-i)∖
st,at	at,st-ι
+ E ∖pι (St-I) - p2(st-1)∖
st-1
≤ £pi(at) ∖pι(st∖at) - p2(st∖ɑt)∖ + E pι(st-i) ∖pι(αt∖st-i) - p2(αt∖st-i)∖
st ,at	at ,st-1
+ E ∖pι(st-i, at-1) - p2(st-i, St-1)∖
st-1 ,at-ι
≤ 2em,post + 2eπ,post +):	∖p1 (st-1, at-I) - p2 (st-1, St-I)I
st-1,at-1
≤ 2(t - tsw)(Em,post + E∏,poSt) + X :	∖p1 (stsw, atsw ) - p2 (StSW, StSW ) ∖
stsw ,atsw
≤ 2(t - tsw)(Em,post + E∏,poSt) + 2tsw(fm,pre + eπ,Pre)
(17)
□
Now, we start the derivation of our bounds.
Theorem 4. Under the k-SteP branched rollouts, using the bound of a model error under ∏d,
Em = maxt Ea〜∏D,s〜p,t [Dtv (p(s0∖s, a)∖∖pθ(s0∖s,a))] and the bound of the policy shift eπ =
maxs Dtv (π∖∖∏d ), thefollowing inequation holds,
ι —	_	( 1 + 降' Y - kγk + (k - 1)γk+1 ,	、
Eπ,p [R] ≥ EDmodel [R] - rmax ɪ (1 - γ)2 2^ +	(1 - γ)2	(猷 + Em)
(18)
Yk - Y ,	、 γk ,,八，	、1
+ ---1 (En + Em) + 1-γ (k + 1) (En + Em)) ∙
21
Under review as a conference paper at ICLR 2021
Proof.
∣E∏,p [r] - EDmodel [R]|
<
Σs0,α0 {p∏(s0,a0) - p∏D (s0,a0)} r(so,ao)
+ P31 Pst,at Yt [r>π (st, at) - pb< k (St, αt)} r(st ,at)
+ PZk Σst,at Yt {P∏ (st,at) - Pb≥ k (St,at)}r(St,at)
( Ps0,a0 ∣Pπ (s0,a0) - P∏D (s0,a0)| Ir(s0,a0)|
{+ P31	Yt Pst,at	Pn (st,at)-	Pb< k(st,at)	lr(st,at)1
[+ ∑∞=k	Y t Pst,at	P∏ (St,at)-	Pb≥ k (St,at)	lr(st,at)1
<
rmax E ∣Pπ(s0, a0)- PnD (s0,a0)∣
S0 ,a0
termA
+rmax Pt=IYt〉： | Pn(St, at) - Pt<k(St,at) |
st,at
、------------------V----------------'
term B
+rmax Pt=k Yt〉： | p∏ (St,at) - p⅛>k (St, at) |
(19)

st,at
^^^^^≡
term C
For term A, we can bound the value in similar manner to the derivation of Lemma 2.
E ∣Pπ (S0,a0) - P∏d (S0,a0)∣ = E ∣Pπ(a0)P(S0) - P∏d (a0)P(S0)∣
s0,a0	s0,a0
=E ∣Pπ(a0)P(S0) - Pπ(a0)P(S0) + (Pπ (a0) - p∏d (a0)) P(S0)∣
s0 ,a0
<	E Pπ(a0) IP(SO) - P(SO)I +£ 1Pπ(a0) - PnD (a0)|
S0,a0	a0
X------------V-------------'	'---------V------------'
=0	≤2e∏
<	2e∏
(20)
For term B, we can apply Lemma 2 to bound the value, but it requires the bounded model error under
the current policy π. Thus, we need to decompose the distance into two by adding and subtracting
PnD :
「I p p	p p _ 「	Pn (St,at) - PnD (St,at)
≥Jpn (st,αt)-pto，t<k(st,αt)|	=工 +p∏d (St, at) - pb< k(St ,at)
st,at	st,at	<
< E IPn(St,at) - PnD (St,at)∣
st,at
V-----------------V---------'
≤2te∏
+	PnD (St , at ) - Pt<k (St, at)	(21)
st,at
PnD (St, at) - Ptb<r k (St, at)
st,at
(A)
<
∖ 1 | 1 v```t-1	/ ʌ 1 v```t-1 br / ʌ |
工 | t 工i=0 PnD (St,at) - t 工i=0 Pt<k,人Stiat |
st,at
1 t-1
巨£| PnD (St,at) - pb<k,i(St, at) |
i=0 st,at
1 t-1
t E {2 (t - i) • α + Em)}
i=0
t {t2(en + Em)+ t(Cn + ∈m)}	∣
(22)
<
22
Under review as a conference paper at ICLR 2021
For (A), we apply Lemma 2 with setting em,post = Em and eπ,post = e∏ for the rollout following π
andpθ(s0∣s), and em,pre = 0 and eπ,pre = 0 for the rollout following ∏d andP(SlS), respectively.
To recap term B, the following equation holds:
IPn(St, at) - Pbr,t<k (St) Ot)I	≤ 2te∏ + t(eπ + Em) + (En + Em)	(23)
st,at
For term C, we can derive the bound in a similar manner to the term B case:
E ∣Pπ(st,αt) - Pbr,t≥k(st,at)∣
st ,at
Σ
st ,at
Pπ(st, at) - p∏D (st, at)
+p∏D (st,at) - Pb≥k(St,at)
≤
E ∣Pπ(St, at) - P∏D (St,at)∣
st,at
X------------V---------------}
≤2te∏
+〉: IPnD (St,at)- p≥k (St, at) I
st,at
(24)
E lPnD (st,at) - Pb≥ k (St,at)| st,at	=X l 1 Pk=01 PnD (st,at) - 1 Pk-01 Pb≥k,i(st,at) l st,at 1 k-1 ≤ k E E I PnD (St, at) - pb≥k,i(St, at) I i=0 st ,at 1 k-1 ≤	£ ^X {2 (k - i) ∙ (En + Em)} i=0 =	[ {k2 (En + Em) + k(En + Em) }	(25) k
To recap term C, the following equation holds:
):1p∏ (St, at)	- pt≥k (St,	at) 1	≤	2tEn	+ k(En	+ Em) +	(En	+ Em)	(26)
st,at
23
Under review as a conference paper at ICLR 2021
By substituting Eqs. 20, 23, and 26, into Eq. 19, we obtain the result:
rmax2En
∣e∏,P [r] - EDmodel [R]|	≤
÷rmax Pk=IL Yt {2teπ + t(eπ + Em) + (eπ + Em)}
÷rmax Pt=k Yt {2tEn ÷ k(En ÷ Em) ÷ (En ÷ Em)}
rmax
ʃ2e∏ ÷	+FT)Yk Y (3e∏ ÷ Em)÷ 片(En ÷ Em)]
ɪ ÷ P∞=k Yt {2tEn ÷ k(En ÷ Em) ÷ (En ÷ Em)}	J
f 2E∏ ÷ iYW)Yk Y (3E∏ ÷ Em) ÷	(En ÷ Em)
rmax J	÷ P∞=1 Yt {2tEn	÷ k(En	÷ Em) ÷ (En	÷	Em)}
f	- Pt=I Yt {2tEn	÷ k(En	÷ Em) ÷ (En	÷	Em)}
f 2En ÷ iγtk(-T⅛k-I)Yk Y (3e∏ ÷ Em) ÷ Yr (En ÷ Em)
rmax J	÷(1二)2 YEn ÷ jTγ {k(En ÷ Em) ÷ (En ÷ Em)}
I - PkT Yt {2tEn ÷ k(En ÷ Em) ÷ (En ÷ Em)}
'2e∏ ÷ f(Kl)Yk Y (3e∏ ÷ Em) ÷ F (En ÷ Em)
÷ (ι-2γ)2 YEn ÷ 1;Tγ {k(En ÷ Em) ÷ (En ÷ Em)}
rmax )	ι-k7(k-1)+(k-i)7k9
(1-γ)2	2 YEn
k
、	-∖-1 {k(En ÷ Em) ÷ (En ÷ Em)}
rmax
f	(1⅛2e∏ ÷ γ-kγ⅛k-21)γk+1 (En ÷ Em)	1
[÷ ÷LYTY (En ÷ Em) ÷ (1-γγ - 2^--^) (k ÷ I)(En ÷ Em) j
rmax
]十72 2F -I- Y-kYk + (k-L)Yk+ 1 (F F ʌ
(1-y)2 4En ÷	(1-y)2	(En ÷ Em)
kk
÷ YTY (En ÷ Em) ÷ 1— (k ÷ I)(En ÷ Em)
(27)
□
Theorem 5. Let Em，≥ maxt Ea〜∏,s〜P [Dtv (p(s0∣s, a)∣∣Pθ(s0∣s,α))],
En,p [R] ≥ EDmodel
[R] - rmax
ʃ 1÷ Y2
1(1 - y)2
2En ÷
1 - kY(k-1) ÷ (k - 1)γk /	.
(f-^)2	Y(EmO- En)
(28)
Yk - Y	Yk
÷ ^γ---1 (EmO - En )÷ 1----γ (k ÷ I)(EmO - En ) ʃ ∙
Proof. The derivation of this theorem is basically the same as that in the Theorem 4 case except for
the way of evaluation of the bound of terms B and C.
For term B, we can apply Lemma 2 to bound the value:
〉:lpn(st,at) - pt< k (st,at)∣	= st ,at	XE PnD (st, at) - tPt-O pb< k,i(st,at) st,at 1 t-1
≤	!ΣΣl Pn (st,at) -pb<k,i(st,at)] i=0 st ,at
(D) ≤	1 t-1 -	]T{(t - i)2Em0 ÷ i2En } i=0 —	{2t2EmO - (t - 1)tEmO ÷ (t - 1)tEn } —	{2—2EmO — —2EmO ÷ ―EmO ÷ ―2En — —En } —	{t2(EmO ÷ En ) ÷ —(EmO - En )} 力(EmO ÷ En )÷ (EmO - En )	(29)
24
Under review as a conference paper at ICLR 2021
For (D), we apply Lemma 2 with setting em,post = em∕ and eπ,post = 0 for the rollout following π
andpθ(s0∣s), and em,pre = 0 and eπ,pre = eπ for the rollout following ∏d andP(SIS).
For term C, we can derive the bound in a similar manner to the case of term B:
X ∣Pπ(St,电)-pb≥k(st,at) I = X I 1 P= PnD (St,电)-1 ∑= pb≥fc,i(st,at) |
st,at	st,at k-1 ≤	1∑∑ 加π(st,at) — Pb≥k,i(St, at)1 i=0 st,at 1 k-1 ≤ k ^X ((k - i必m0 +(t — k + i必n} = ɪ {2k2em∕ — (k — 1)kemo + 2tke∏ — 2k%∏ + (k — 1)ke∏ } k =	7 {2kteπ + 2(∈mz — eπ)k2 — (k — l)k(€m° — eπ)} k =	{ {2kteπ + (CmO — e∏ )k2 + k(EmO — ∈π)}	(30) k
By substituting Eqs. 20, 29, and 30, into Eq. 19, we obtain the result:
I En,p [R] — EDmodel [R] I	≤	甘	Γ	2eπ + ∑k=1 Yt {t(em0 + eπ) + (EmO — eπ)1	1 —max [+ ∑∞= k Yt { k {2ktEn + (EmO-En )k2 + k(EmO-En )}} J =r	(2En + -"'-+FT/ Y(EmO + En ) + ^ 5，— E∏ )) ɪ + ∑∞=k YtI {2ktEn + (EmO-En )k2 + k(EmO-En )} J f 2En + -W-+'T)Yk Y(EmO + En ) + ^― (Em，- En )1 =rmax	+	+ ∑∞=1 Yt1	{2ktEn + (EmO — En )k2	+	k(EmO	— En )}	| [—∑k=11 YtI	{2ktEn + (EmO-En )k2	+	k(EmO-En )}	J f o	. 1-k7(fc-1)+(k-1)7k /	.	ʌ , Yk-γ(	C 2En +	(ι-γ)2	Y(EmO + En )+ γ-∕ (EmO - En ) + (Jγ)2 2YEn + 1-γ {(EmO - En )k + (EmO - En )} =rmax j	1-kY(k-1)+(k-1)γk 2	f (1-γ)2	Y n k 、	—∖-] {(Em' - En )k + (Em' - En )}	, =r	J	(1-⅝2En + γ-kγ (+-k-2I)Y + (Em- En)	1 [+ -Y--11 (EmO - En) + (1-γ - 2Y-?) (k + I)(EmO - En )J =rmax ( (!+Y2 2En + γ-kγ"(+-"I)Yk+ 1 (EmO- En) )	(31) maX 1 + yY⅛ (EmO — En ) + 占(k + 1)(Em，— En ) J
□
A.8 THE RELATION OF RETURNS IN THE CASE WHERE A REWARD PREDICTION IS
INACCURATE
In Section 5, we discuss the relation between the true returns and the returns estimated on the meta-
model under the assumption that the reward prediction error is zero. The theoretical result under
this assumption is still useful because there are many cases where the true reward function is given
and the reward prediction is not required. However, a number of readers still may want to know
what the relation of the returns is under the assumption that the reward prediction is inaccurate. In
this section, we provide the relation of the returns under inaccurate reward prediction in the MDP
case 17.
17Here, we do not discuss the theorems in the POMDP case because those in the MDP case can be easily
extended into the POMDP case by utilizing Lemma 1.
25
Under review as a conference paper at ICLR 2021
We start our discussion by defining the bound of the reward prediction error r :
Definition 4.	Cr	:=	maxt	E@,st)〜Dmodel	[∣r(st,at)	-	r (st,at)∣],	where	r (st,at):=
Ert〜pθ [rt |at, st].
We also define the return on the branched rollout with inaccurate reward prediction.
k-1
EDmodel[R]	:= EpnD (S0,aO)rθ(S0,aO)+ £ γp γtpb<k(st,at)rθ(st,at)
s0 ,a0	t=1 st ,at
∞
+XX γ	tptb≥r k(St, at)rθ(St, at)	(32)
t=k st,at
Now, we provide the relation between the returns under inaccurate reward prediction.
Theorem 6 (Extension of Theorem 4 into the case where reward prediction is inac-
curate). Under the k steps branched rollouts, using the bound of a model error un-
der ∏d, Em = maxt Ea〜∏d,s〜p,t [Dtv (p(s0∣s, a)∣∣pθ(s0∣s,a))], the bound of the policy
shift c∏	= maxs Dtv (∏∣∣∏d) and the bound of the reward prediction error Cr =
maxt E(at,st)〜Dmodel [∣r(st, at) — r (st, at)|], thefollowing inequation holds,
1 +γ	2 γ	- kγ	k + (k - 1)γ	k+1
Eπ,p [R] ≥ EDmodel[R] - rmax I (1 - γ)2 2Cn +	(1 - γ)2	(Cn + Cm)	(33)
γ	k -γ	γ k	γ
+-(Cn + Cm) +	(k + I)(Cn + Cm) - -	Cr ∙
γ - 1	1 -γ	1 -γ
Proof.
|En,p[R] - EDmodel [R] | = | E∏,P [R] - EDmodel [R]+ EDmodel [R] - EDmodel [R] |	(34)
≤ IEn,p[R] - EDmodel [R] I + IEDmodel [R] - EDmodel [R] |
|	| ||	s0,a0 pnD (SO, aO) {r(SO, aO) - rθ(SO, aO)}	||
IEDmodel [R] - EDmodel[R] 1 = + P31 Pst,at Yp<k (st, at) {r(st, at) - rθ(st, at)}
I+Pt∞=k Pst,at γ	tptb≥r k(St,at) {r(St, at) - rθ(St,at)}I
≤ pnD	(SO, aO) |r(SO, aO) - rθ(SO, aO)|
s0,a0
k-1
+ΣΣ γ	tptb<r k(St,at) |r(St, at) - rθ(St, at)|	(35)
t=1 st ,at
∞
+ γ tptb≥r k(St,at) |r(St, at) - rθ(St, at)|
t=k st,at
∞
≤ Xγ tCr
t=O
γ
=；-----Cr
1-γ
By substituting Eqs. 27 and 35 into Eq. 34, we obtain the result:
En,p[R] - EDmodel [R] | ≤ rɪ- {
(l-⅝ 2Cn +
k
γ-kγk + (k-1)γk+1
Tι-γ)2
k
(Cn + Cm)
+ YY-Y (Cn + Cm) + I-Y Ik + I)(Cn + Cm )
} + 1⅛r
(36)
□
26
Under review as a conference paper at ICLR 2021
Theorem 7 (Extension of Theorem 5 into the case where reward prediction is
inaccurate). Let em，	≥ maxt Ea〜π,s〜P [DTV (p(s0∣s, a)∣∣pθ(s0∣s, a))] and er	=
maχt E(at ,st)〜D model [|r (St ,at) - rθ (st, Qt) |],
1 + γ2 γ - kγk + (k - 1)γ k+1
Eπ,P [R] ≥ EDmodel[R] - rmax (1 - Y)2 2eπ +	(1 - Y)2	(em0 - eπ)
(37)
γk - γ
+-----7 (em0 - eπ ) +
Y-1
(k (k + I)(CmO - eπ) ∖ -
1-Y
Y
C er
1-Y
Proof. Similar to the derivation of Theorem 6, we obtain the result by substituting Eq. 31 and 35
into Eq. 34.	□
A.9 PEARL in Sections 6 and 7
The PEARL algorithm used in Sections 6 and 7 refers to “PEARL with RNN-traj” in (Rakelly et al.,
2019). The comparison result with other types of PEARL (i.e., vanilla PEARL and PEARL with
RNN-tran) in (Rakelly et al., 2019) and M3PO are shown in Figure 7. The figure indicates that
M3PO achieves better sample efficiency than them.
Halfcheetah-fwd-bwd
2500
1500
500
-500
—vPEARL	—PEARL-tran
---M3PO-vPEARL ----M3PO-PEARL-tran
一M3PO
Figure 7: The learning curve of vanilla PEARL (vPEARL), PEARL with RNN-traj (PEARL-tran), M3PO, and
its variants in which the vPEARL and PEARL-tran are used for meta-policies. In each figure, the
vertical axis represents expected returns and the horizontal axis represents the number of training
samples (x1000). The meta-policy and meta-model (and a predictive model) were fixed and their
expected returns were evaluated on 50 episodes at every 1000 training samples for the other methods.
In each episode, the task was initialized and changed randomly. Each method was evaluated in at
least three trials, and the expected return on the 50 episodes was further averaged over the trials. The
averaged expected returns and their standard deviations are plotted in the figures.
A.10 Baseline methods for our experiment
PEARL: The model-free meta-RL method proposed in Rakelly et al. (2019). This is an off-
policy method and implemented by extending Soft Actor-Critic (Haarnoja et al., 2018). By
leveraging experience replay, this method shows high sample efficiency. We reimplemented the
PEARL algorithm on TensorFlow, referring to the original implementation on PyTorch (https:
//github.com/katerakelly/oyster).
Learning to adapt (L2A): The model-based meta-RL proposed in Nagabandi et al. (2019a). In
this method, the meta-model is implemented with MAML (Finn et al., 2017) and the optimal
action is found by the model predictive path integral control (Williams et al., 2015) on the full
meta-model based rollouts. We adapt the following implementation of L2A to our experiment:
https://github.com/iclavera/learning_to_adapt
27
Under review as a conference paper at ICLR 2021
A.11 Environments for our experiment
For our experiment in Section 7, we prepare simulated robot environments using the MuJoCo
physics engine (Todorov et al., 2012) (Figure 8):
Halfcheetah-fwd-bwd: In this environment, meta-policies are used to control the half-cheetah,
which is a planar biped robot with eight rigid links, including two legs and a torso, along with six
actuated joints. Here, the half-cheetah’s moving direction is randomly selected from “forward” and
“backward” around every 15 seconds (in simulation time). If the half-cheetah moves in the correct
direction, a positive reward is fed to the half-cheetah in accordance with the magnitude of move-
ment, otherwise, a negative reward is fed.
Halfcheetah-pier: In this environment, the half-cheetah runs over a series of blocks that are floating
on water. Each block moves up and down when stepped on, and the changes in the dynamics are
rapidly changing due to each block having different damping and friction properties. These proper-
ties are randomly determined at the beginning of each episode.
Ant-fwd-bwd: Same as Halfcheetah-fwd-bwd except that the meta-policies are used for controlling
the ant, which is a quadruped robot with nine rigid links, including four legs and a torso, along with
eight actuated joints.
Ant-crippled-leg: In this environment, we randomly sample a leg on the ant to cripple. The crip-
pling of the leg causes unexpected and drastic changes to the underlying dynamics. One of the four
legs is randomly crippled every 15 seconds.
Walker2D-randomparams: In this environment, the meta-policies are used to control the walker,
which is a planar biped robot consisting of seven links, including two legs and a torso, along with
six actuated joints. The walker’s torso mass and ground friction is randomly determined every 15
seconds.
Humanoid-direc: In this environment, the meta-policies are used to control the humanoid, which
is a biped robot with 13 rigid links, including two legs, two arms and a torso, along with 17 actuated
joints. In this task, the humanoid moving direction is randomly selected from two different direc-
tions around every 15 seconds. If the humanoid moves in the correct direction, a positive reward
is fed to the humanoid in accordance with the magnitude of its movement, otherwise, a negative
reward is fed.
Left: Ant-fwd-bwd, Center: Ant-crippled-leg, Right: Humanoid-direc
Figure 8: Environments for our experiment
28
Under review as a conference paper at ICLR 2021
A.12 Complementary experimental results
AUUedə-oso
Figure 9: Discrepancy between Eπφ,p [R] and Eπφ,pθ [R] in Theorem 1. The vertical and horizontal axes
represent the discrepancy value and m ∈ [0, 1], respectively.
1 - m and rmax = 1.
We set the other variables as π
Figure 10: The local change in m0 with respect to π versus training sample size. In each figure, the vertical
axis represents the local change of the meta-model error (ddm0) and the horizontal axis represents
the training sample size (x1000). The red-dotted line is the linear interpolation of the blue dots,
which shows the trend of the local change decreasing as the training sample size grows.
29
Under review as a conference paper at ICLR 2021
Figure 11: Transition of model errors on training. In each figure, the vertical axis represents empirical values
of m and the horizontal axis represents the number of training samples (x1000). We ran five trials
with different random seeds. The result of the x-th trial is denoted by Trial-x. We used the negative
of log-likelihood of the meta-model on validation samples as the approximation of m . The figures
show that the model error tends to decrease as epochs elapse.
Training samples
----Trial-I------Trial-2-----Trial-3-----Trial-4------Trial-5
—Trial-I —Trial-2 —Trial-3 —1
Ant-fwd-bwd
Humanoid-direc
Figure 12: Transition of meta-policy divergence on training. In each figure, the vertical axis represents empir-
ical values of π and the horizontal axis represents the number of training samples (x1000). We ran
three trials with different random seeds. The result of the x-th trial is denoted by Trial-x. For π ,
we used the empirical Kullback-Leibler divergence of πθ and πD . Here, πD has the same policy
network architecture with πθ and is learned by maximizing the likelihood of actions in D (Denv for
Algorithm 2).
30
Under review as a conference paper at ICLR 2021
-------- - -- ^
1	10	19	28	37	46
Training samples (x50k)
PEARL-I----PEARL-2----PEARL-3
M3PO-1-----M3PO-2-----M3PO-3
Figure 13: Learning curve of PEARL and M3PO in a long-term training. In each figure, the vertical axis
represents expected returns and the horizontal axis represents the number of training samples
(x50000). The meta-policy and meta-model were fixed and their expected returns were evaluated
on 50 episodes at every 50,000 training samples. Each method was evaluated in three trials, and the
result of the x-th trial is denoted by method-x. Note that the scale of the horizontal axis is larger
than that in Figure 1 by 50 times (i.e., 4 in this figure is equal to 200 in Figure 1).
Walker2D-randomparams
6000 -
4000	zzχ'Γ*>'w√Iff*λΛ
2000 —/--
1	50	99	148	197
-2000 -
Ant-fwd-bwd
1500 -
】：L%⅛r⅛
0 ∣⅞rw∙≠rw'rrww
-500 L	_??_
—Trial-I —Trial-2 -M3PO
—Trial-I —Trial-2 —M3PO
Figure 14: Comparison of GHP-MDP (Algorithm 1 in Perez et al. (2020)) and M3PO. The figures show learn-
ing curves of GHP-MDP and M3PO. In each figure, the vertical axis represents expected returns
and the horizontal axis represents the number of training samples (x1000). GHP-MDP was eval-
uated in two trials, and each trial was run for three days in real-times. Due to the limitation of
computational resources, we could not run this experiment as many days and trials as other exper-
iments. The expected returns of GHP-MDP in each trial (denoted by “Trial-1” and “Trial-2”) are
plotted in the figure. The results of M3PO is referred to those in Figure 1. From the comparison
result, we can see that M3PO achieves better sample efficiency than GHP-MDP.
31
Under review as a conference paper at ICLR 2021
u」5①」①bjo巴①><
Training samples
—M2P0 —M3PO
Figure 15: Learning curve of M3PO and M2PO. In each figure, the vertical axis represents expected returns and
the horizontal axis represents the number of training samples (x1000). The meta-policy and meta-
model (and a predictive model) were fixed and their expected returns were evaluated on 50 episodes
at every 1000 training samples for the other methods. In each episode, the task was initialized and
changed randomly. Each method was evaluated in at least five trials, and the expected return on the
50 episodes was further averaged over the trials. The averaged expected returns and their standard
deviations are plotted in the figures.
,ɪotəlɑl φ∞≡ω><
Figure 16: Transition of TD-errors (Q-function error) on training. In each figure, the vertical axis represents
empirical values of m and the horizontal axis represents the number of training samples (x1000).
We ran ten trials with different random seeds and plotted the average of their results. The error bar
means one standard deviation.
32
Under review as a conference paper at ICLR 2021
A.13 Complementary analysis
In addition to Q1 and Q2 in the main content, we also conducted a complementary analysis to answer
the following question. Q.3: Does the use of a meta-model in M3PO contribute to the improvement
of the meta-policy?
In an analysis in this section, we compare M3PO with the following method. Model-based Meta-
Policy Optimization (M2PO): This method is a variant of M3PO, in which a non-adaptive predic-
tive model is used instead of the meta-model. The predictive model architecture is the same as that
in the MBPO algorithm (Janner et al., 2019) (i.e., the ensemble of Gaussian distributions based on
four-layer feed-forward neural networks).
Regarding Q3, our experimental result indicates that the use of a meta-model contributed to the
performance improvement in a number of the environments. In Figure 15 in the appendix, we can
clearly see the improvement of M3PO against M2PO in Halfcheetah-fwd-bwd. In addition, in the
Ant environments, although the M3PO’s performance is seemingly the same as that of M2PO, the
qualitative performance is quite different; the M3PO can produce a meta-policy for walking in the
correct direction, while M2PO failed to do so (M2PO produces the meta-policy “always standing”
with a very small amount of control signal). For Humanoid-direc, in contrast, M2PO tends to
achieve better sample efficiency than M3PO. We hypothesize that the primary reason for this is that
during the plateau at the early stage of training in Humanoid-direc, the predictive model used in
M2PO generates fictitious trajectories that make meta-policy optimization more stable. To verify
this hypothesis, we compare TD-errors (Q-function errors) during training, which is an indicator of
the stability of meta-policy optimization, in M3PO and M2PO. The evaluation result (Figure 16 in
the appendix) shows that during the performance plateau (10-60 epoch), the TD-error in M2PO was
actually lower than that in M3PO; this result supports our hypothesis. In this paper, we did not focus
on the study of meta-model usage to generate the trajectories that make meta-policy optimization
stable, but this experimental result indicates that such a study is important for further improving
M3PO.
A.14 Hyperparameter setting
Table 1: Hyperparameter settings for M3PO results shown in Figure 1. x → y over epohs a → b denotes a
thresholded linear function, i.e., at epoch e, f (e) = min(max(x + e-a ∙ (y — x),x),y).
N
E
M
B
G
k
epoch
environment step per epoch
meta-model rollouts per environment step
ensemble size
meta-policy update per environment step
meta-model horizon
4Z
H
200
1000
1e3 I 5e2 ∣ 1e3 ∣ 5e2
3
40	20
ŋ^^1	1 → 25^^^^1 ^^^^Γ^
over epoch
20 → 100
A.15 Notations
33
Under review as a conference paper at ICLR 2021
Table 2: List of mathematical notation in the main body. For simplicity, we do not include notation introduced
in the appendix.
Notation	Meaning
t	Time step
ot	Observation
O	Set of observations
st, s0, st	Hidden state
S, S0	Set of hidden states
a, at	Action
A	Set of actions
Pob, p(ot∣st,at-i)	Observation probability function
Pst, p(st∣st-i, αt-i)	State transition probability function
rt, r(st, at)	Reward function
Y	Discount rate
∏, Πφ, p(at+ι∣ht)	(Meta-)policy
h, ht, h0	History (past trajectories)
H	Set of history
Φ	Learnable parameters of meta-policy
θ	Learnable parameters of meta-model
P(StIht)	Belief state
R	Discounted return
Ea〜∏φ,r,h〜pθ [R], E∏φ,pθ [R]	Meta-model return under full meta-model rollout
C	Discrepancy between the meta-model return and true return
Cm	Bounds of generalization error
e∏	Bounds of distribution shift
D, Denv	Dataset (trajectories collected from the real environment)
DTV	Total variation distance
r max	Bound of the reward expected on the basis of a belief
Dmodel	Set of samples collected through branched rollouts
k	Meta-model horizon (meta-model rollout length)
∏D	Data collection policy
E∏Φ,P [R]	True return (the return of the meta-policy in the real environment)
em0	Bound of meta-model error on a current policy
Eah)~Dmodel [R]	Meta-model return in branched meta-rollouts
Z	Latent context
Pφ(z∣a0,θ0, ∙.∙,at,θt)	History enoder
DKL	Kullback-Leibler divergence
Jd~ X1 Dmodel	Optimization objective for meta-policy optimization in M3PO
Q∏Φ	Action value function
V∏φ	State value function
B	Ensemble size
i	Index
N	Gaussian distribution
μθ	Mean of Gaussian distribution
σθ	Standard deviation of Gaussian distribution
τ, Tt	Task
	T	Task set
34
Under review as a conference paper at ICLR 2021
Table 3: Ek[k = 1] settings for results shown in Figure 17. a → b denotes a thresholded linear function, i.e.,
at epoch e, f (e) = min(max(1 — b-a, 1), 0).
Ek [k = 1] J expected meta-model horizon
80 → 130
50 → 100
150 → 250
A.16 Complementary analysis 2
In Figures 1 and 13, we can see that, ina number of the environments (Halcheetah-pier, Walker2D-
randomparams and Humanoid-direc), the long-term performance of M3PO is worse than that of
PEARL. This indicates that a gradual transition from M3PO to PEARL (or other model-free ap-
proaches) needs tobe considered to further improve overall performance. In this section, we propose
to introduce such a gradual transition approach to M3PO and evaluate it on the environments where
the long-term performance of M3PO worse than that of PEARL.
For the gradual transition, we introduce the notion of the “expected” model-rollout length Ek [k]
for k ∈ {0, 1} to M3PO. In this notion, Ek [k = 1] means the probability of one-step meta-model
rollout. Namely, with the probability of Ek [k = 1], the fictitious trajectory generated by one-step
meta-model rollout is used for the policy update (in line 11 in Algorithm 2), and, with the probability
of 1 - Ek[k = 1], the real trajectory in Denv are used for the policy update. In our implementation,
we replace Dmodel in line 11 in Algorithm 2 with the mixed dataset Dmix. Here, Dmix is defined as
Dmix = Ek [k = 1] ∙ Dmodel + (1 - Ek [k = 1]) ∙ Denv.	(38)
We linearly reduce the value of Ek[k = 1] from 1 to 0 depending on training epoch. With this value
reduction, the M3PO is gradually transitioned to PEARL (i.e., the model-free approach).
We evaluate M3PO with the gradual transition (M3PO-h) in three environments (Halfcheetah-
pier, Walker2D-randomparams and Humanoid-direc) where the long-term performance of M3PO
is worse than that of PEARL in Figures 1 and 13. The hyperparameter setting (except for setting
schedule for the value of Ek [k = 1]) for the experiment is the same as that for Figures 1 and 13 (i.e.,
the one shown in Table 1). Regarding to setting schedule for the value of Ek[k = 1], we reduce the
value of Ek[k = 1] in accordance with Table 3.
Evaluation results are shown in Figure 17. Due to the limitation of our computational resource, we
could not continue the training of M3PO-h until convergence, and the resulting training epoch is
much shorter than those of PEARL. Nevertheless, we can see that some trials of M3PO-h achieve
same or better scores with the long-term performances of PEARL in all environments (e.g., M3PO-
h-1 and M3PO-h-2 achieve the same or better performance than the best scores of PEARL-2 and
PEARL-3 in Halfcheetah-pier).
35
Under review as a conference paper at ICLR 2021
Enla」①QOe-lθ><
Halfcheetah-pier
8000
6000
4000
2000
,2000 1____lŋ___19___28___17___4&
Training samples (x50)
——PEARL-I ——PEARL-2 ——PEARL-3
-M3PO-1----M3PO-2------M3PO-3
-M3P0-h-l-M3PO-h-2-M3PO-h-3
Walker2D-randomparams
Humanoid-direc
12000
10000
8000
6000
4000
2000
1 10 19 28 37 46 55 64 73 82 91
Figure 17: Learning curve of PEARL, M3PO and M3PO-h in a long-term training. In each figure, the vertical
axis represents expected returns and the horizontal axis represents the number of training samples
(x50000). The meta-policy and meta-model were fixed and their expected returns were evaluated
on 50 episodes at every 50,000 training samples. Each method was evaluated in three trials, and the
result of the x-th trial is denoted by method-x. Note that the scale of the horizontal axis is larger
than that in Figure 1 by 50 times (i.e., 4 in this figure is equal to 200 in Figure 1).
36
Under review as a conference paper at ICLR 2021
A.17 High-level summary
Figure 18: High-level summary of what we do in this paper based on previous works. Our core contribution
is proposing a model-based meta-reinforcement learning (RL) algorithm (i.e., Algorithm 2) with
performance guarantee. To achieve this, we first generalize various existing model-based meta-RL
settings as solving a partially observable Markov decision process (POMDP) (Section 4). We then
conduct theoretical analysis by extending the theorems proposed in Janner et al. (2019). We extend
their theorems (Theorems 4.1, 4.2 and 4.3) into our POMDP setting (Sections 5.1 and 5.2). As a
part of our extension, we refine Theorem 4.2 and 4.3 in order to strictly guarantee the performance
of branched rollouts (Section A.7). We also compare these theorems in order to motivate using
branched rollouts for model-based meta-RL (Sections 5.2 and A.6). We finally propose a practical
algorithm based on the result of our theoretical anyways (Section 6).
37