Under review as a conference paper at ICLR 2021
Provab le Acceleration of Wide Neural Net
Training via Polyak’s Momentum
Anonymous authors
Paper under double-blind review
Ab stract
Incorporating a so-called “momentum” dynamic in gradient descent methods is
widely used in neural net training as it has been broadly observed that, at least
empirically, it often leads to significantly faster convergence. At the same time,
there are very few theoretical guarantees in the literature to explain this apparent
acceleration effect. In this paper we show that Polyak’s momentum, in combi-
nation with over-parameterization of the model, helps achieve faster convergence
in training a one-layer ReLU network on n examples. We show specifically that
gradient descent with Polyak’s momentum decreases the initial training error at a
rate much faster than that of vanilla gradient descent. We provide a bound for a
fixed sample size n, and we show that gradient descent with Polyak’s momentum
converges at an accelerated rate to a small error that is controllable by the number
of neurons m. Prior work (Du et al., 2019b) showed that using vanilla gradient
descent, and with a similar method of over-parameterization, the error decays as
(1 - κn )t after t iterations, where κn is a problem-specific parameter. Our result
shows that with the appropriate choice of parameters one has a rate of (1 - √κn)t.
This work establishes that momentum does indeed speed up neural net training.
1	Introduction
Momentum methods are very popular for training neural networks in various applications (e.g. He
et al. (2016); Vaswani et al. (2017); Krizhevsky et al. (2012)). It has been widely observed that
the use of momentum helps faster training in deep learning (e.g. Sutskever et al. (2013); Hoffer
et al. (2017); Loshchilov & Hutter (2019); Wilson et al. (2017); Cutkosky & Orabona (2019); Liu
& Belkin (2020)). Among all the momentum methods, the most popular one seems to be Polyak’s
momentum (a.k.a. Heavy Ball momentum) (Polyak, 1964), which is the default choice of momen-
tum in PyTorch and Tensorflow. 1 The success of Polyak’s momentum in deep learning is widely
appreciated and almost all of the recently developed adaptive gradient methods like Adam (Kingma
& Ba (2015)), AMSGrad (Reddi et al. (2018)), and AdaBound (Luo et al. (2019)) adopt the use of
Polyak’s momentum, instead of Nesterov’s momentum.
However, despite its popularity, little is known in theory about why Polyak’s momentum helps to
accelerate training neural networks. Even for convex optimization, smooth twice continuously dif-
ferentiable functions like strongly convex quadratic problems seem to be one of the few cases that
Polyak’s momentum method provably achieves faster convergence than standard gradient descent
(e.g. Lessard et al. (2016); Goh (2017); Ghadimi et al. (2015); Gitman et al. (2019); Loizou &
Richtarik (2017; 2018); Can et al. (2019); Scieur & Pedregosa (2020); Flammarion & Bach (2015)).
On the other hand, the theoretical guarantees of Adam (Kingma & Ba (2015)), AMSGrad (Reddi
et al. (2018)), or AdaBound (Luo et al. (2019)) are only worse if the momentum parameter β is non-
zero and the guarantees deteriorate as the momentum parameter increases, which do not show any
advantage of the use of momentum (see also e.g. Alacaoglu et al. (2020)). Moreover, the conver-
gence rates that have been established for Polyak’s momentum in several related works (Gadat et al.,
2016; Sun et al., 2019; Yang et al., 2018; Liu et al., 2020c) do not improve upon those for vanilla
gradient descent or vanilla SGD. There are even negative cases in convex optimization showing that
1See PyTorch webpage https://pytorch.org/docs/stable/_modules/torch/optim/
sgd.html and Tensorflow webpage https://www.tensorflow.org/api_docs/python/tf/
keras/optimizers/SGD.
1
Under review as a conference paper at ICLR 2021
Algorithm 1: Gradient descent with Polyak’s momentum Polyak (1964) (Equivalent Version 1)
1:
2:
3:
4:
5:
6:
7:
Required: Step size parameter η and momentum parameter β.
Init: w0 ∈ Rd and m-1 = 0 ∈ Rd.
for t = 0 to T do
Given current iterate wt, obtain gradient VL(wt).
Update momentum mt := βmt-ι + VL(Wt).
Update iterate wt+1 := wt - ηmt .
end for
Algorithm 2: Gradient descent with Polyak’s momentum Polyak (1964) (Equivalent Version 2)
1:	Required: step size η and momentum parameter β.
2:	Init: w0 = w-1 ∈ Rd
3:	for t = 0 to T do
4:	Given current iterate wt, obtain gradient VL(wt).
5:	Update iterate wt+1 = wt - ηVL(wt) + β(wt - wt-1).
6:	end for
the use of Polyak’s momentum results in divergence (e.g. Lessard et al. (2016); Ghadimi et al.
(2015)). Furthermore, Kidambi et al. (2018) construct a problem instance for which the momentum
method under its optimal tuning is outperformed by other algorithms. A solid understanding of the
empirical success of Polyak’s momentum in deep learning has eluded researchers for some time.
In this paper, we provably show that Polyak’s momentum helps achieve faster convergence for train-
ing a one-hidden-layer ReLU network. Over the past few years there have appeared an enormous
number of works considering training a one-layer ReLU network, provably showing convergence
results for vanilla (stochastic) gradient descent (e.g. Li & Liang (2018); Ji & Telgarsky (2020);
Li & Yuan (2017); Du et al. (2019b;a); Allen-Zhu et al. (2019); Song & Yang (2019); Zou et al.
(2019); Arora et al. (2019); Jacot et al. (2018); Lee et al. (2019); Chizat et al. (2019); Brutzkus &
Globerson (2017); Tian (2017); Soltanolkotabi (2017); Bai & Lee (2020); Ghorbani et al. (2019); Li
et al. (2020); Hanin & Nica (2020); Daniely (2017); Zou & Gu (2019); Dukler et al. (2020); Daniely
(2020); Wei et al. (2019); Yehudai & Shamir (2020); Fang et al. (2019); Su & Yang (2019); Oymak
& Soltanolkotabi (2019)) as well as for other algorithms (e.g. Zhang et al. (2019); Wu et al. (2019b);
Cai et al. (2019); Wu et al. (2019a); Zhong et al. (2017); Ge et al. (2019); van den Brand et al. (2020);
Lee et al. (2020)). However, we are not aware of any theoretical works that study the momentum
method in neural net training except the work Krichene et al. (2020). Krichene et al. (2020) show
that SGD with Polyak’s momentum (a.k.a. stochastic Heavy Ball) with infinitesimal step size, i.e.
η → 0, for training a one-hidden-layer network with an infinite number of neurons, i.e. m → ∞,
converges to a stationary solution asymptotically. However, the asymptotic convergence result does
not explain the faster convergence of momentum.In this paper we consider the discrete-time setting
and consider nets with infinite neurons as well as nets with finitely many neurons. We provide a
non-asymptotic convergence rate of Polyak’s momentum, establishing a concrete improvement rel-
ative to the best-known rates for vanilla gradient descent. Our result follows the same framework as
previous results, e.g. Du et al. (2019b); Arora et al. (2019); Song & Yang (2019).
We study training a one-hidden-layer ReLU neural net of the form,
m
NW(x)：= √m X arσ(hw(r),xi),	(1)
r=1
where σ(z) := Z ∙ l{z ≥ 0} is the ReLU activation, w(1),..., w(m) ∈ Rd are the weights of m
neurons on the first layer, a1, . . . , am ∈ R are weights on the second layer, x ∈ Rd is the input, and
N(x) ∈ R is the output predicted on input x. Denote W := {w(r)}rm=1. We consider empirical loss
minimization with the squared loss,
L(W) ：= 2 Pn=ι Iryi-NW(Xi))2,	⑵
where yi ∈ R is the label of sample xi and n is the number of samples. Following previous works of
of (Du et al., 2019b; Arora et al., 2019; Song & Yang, 2019), we define a Gram matrix H ∈ Rn×n
2
Under review as a conference paper at ICLR 2021
for the weights W and its expectation H ∈ Rn×n over the random draws of w(r) 〜N(0, Id) ∈ Rd
whose (i, j) entries are defined as follows,
m
H(W)i,j := 一 ^X x>Xj l{hw(r), Xii ≥ 0 & hw(r) ,Xji ≥ 0}
m r=1	(3)
Hij :=	IE	[x>Xjl{hw(r), Xii ≥ 0 & hw(r), Xji ≥ 0}].
,	W(T)〜N (0,Id)	.	—
We note that the matrix H is also called a neural tangent kernel (NTK) matrix in the literature (e.g.
Jacot et al. (2018); Yang (2019); Huang et al. (2020); Bietti & Mairal (2019)). Assume that the
smallest eigenvalue λ := λma(H) is strictly positive and certain conditions about the step size and
the number of neurons are satisfied. Previous works of (Du et al., 2019b; Arora et al., 2019; Song
& Yang, 2019) were able to show that gradient descent decreases the empirical risk (2) at a linear
rate 1 一 等,i.e. L(Wt) =(1 — ⅞λ)L(Wt-ι). In this paper, following the same framework as (DU
et al., 2019b; Arora et al., 2019; Song & Yang, 2019), We show that gradient descent with Polyak's
momentum decreases the empirical risk at an accelerated linear rate 1 - Jη2λ to a small additive
error that is controllable by the number of neurons m. 2 This shows the combined advantage
of Polyak’s momentum and over-parameterization. As the number of neurons m and samples n
approach infinity, as considered in (Krichene et al., 2020), our analysis shows that gradient descent
with Polyak’s momentum converges to any arbitrarily small error at the accelerated rate.
2 Preliminaries
2.1	Polyak’s momentum
Algorithm 1 and Algorithm 2 show two equivalent presentations of gradient descent with Polyak’s
momentum. Given the same initialization, one can show that Algorithm 1 and Algorithm 2 produce
exact the same iterates during optimization. We note that for the ReLU activation, it is not differen-
tiable at zero. So for solving (2), we replace the notion of gradient in Algorithm 1 and Algorithm 2
with subgradient 孩(IWt) := √m Pin=1 (NWt(Xi) — yi)ar ∙ IKw(r),Xii ≥ 0]Xi and update the
neuron r as w(+)ι = w(r) 一 η dL(Wt) + β(w(r) 一 w(-)J.
t+1	∂wt	t-1
The most common example in the literature that demonstrates the advantage of Polyak’s momentum
over vanilla gradient descent is the strongly convex quadratic problem, mi□w∈Rd ɪw>Aw + b>w,
where A ∈ Rd×d 0d. Applying gradient descent with Polyak’s momentum (Algorithm 2) to the
problem, the iterate evolves according to the following dynamics,
wt+1 一 w* = (Id 一 ηA)(wt 一 w*) + β(wt 一 w*) 一 β(wt-ι 一 w*),	(4)
where Id is the identity matrix, η is the step size, and w* satisfies Aw* = b, which is the unique
minimizer of the quadratic problem. One can re-write the recursive dynamics (4) as follows,
wt+1 一 w* = Id - nA + βId 一万1d
wt 一 w*	Id	0d
wt 一 w*
wt-1 一 w*
(5)
A known result (see e.g. Lessard et al. (2016); Polyak (1987)) is that under an optimal tuning of the
momentum parameter β . The error decays at an accelerated linear rate
wt+1 一 w
wt 一 w*
t
wt 一 w*
wt-1 一 w
(6)
where λd is the smallest eigenvalues of A. On the other hand, gradient descent only has 1 一 ηλd
convergence rate (see e.g. Lessard et al. (2016)). In the next section, we will show that the dynamics
induced in the neural network training by the momentum method is similar to the quadratic function
2We borrow the term “accelerated linear rate” from the convex optimization literature (Nesterov, 2013),
because the result here has a resemblance to those results in convex optimization, even though the neural
network training is a non-convex problem.
3
Under review as a conference paper at ICLR 2021
case here (i.e. (4) and (5)), modulo some small terms whose magnitudes are controllable. The
similarity hints at why momentum helps faster neural network training.
More related works of Polyak’s momentum: There is little theory work that shows any provable
advantage of Polyak’s momentum in non-convex optimization and deep learning. Even in con-
vex optimization, related works make additional assumptions to show a provable advantage over
standard GD or SGD. Chen & Kolar (2020) study Polyak’s momentum under a growth condition.
Sebbouh et al. (2020) show that SGD with Polyak’s momentum outperforms vanilla SGD in smooth
convex optimization when the data is interpolated. On the other hand, for smooth non-convex opti-
mization, Wang et al. (2020) show that Polyak’s momentum helps to escape saddle points faster and
find a second-order stationary point faster. Yet, they also make certain assumptions regarding some
statistical properties of gradient and momentum. There are also some efforts in using continuous-
time techniques to analyze a broad family of momentum methods that includes Polyak’s momentum
(see e.g. Diakonikolas & Jordan (2019); Maddison et al. (2018)).
2.2	Assumption and prior result
As described in the introduction section, we assume that that the smallest eigenvalue of the Gram
matrix H ∈ Rn×n is strictly positive, i.e. λ := λmg(H) > 0. We will also denote the largest
eigenvalue of the Gram matrix H ∈ Rn×n as λmaχ(H). Du et al. (2019b) show that the strict
positiveness assumption is indeed mild. Specifically, they show that if no two inputs are parallel, then
the least eigenvalue is strictly positive. Panigrahi et al. (2020) were able to provide a quantitative
lower bound under certain conditions. Following the same framework of (Du et al., 2019b), we
consider that each weight vector w(r) ∈ Rd is initialized according to normal distribution, w(r)〜
N(0, Id), and each ar ∈ R is sampled from Rademacher distribution, i.e. ar = 1 with probability
0.5; and	ar	=	-1	with probability 0.5.	We also assume	kxik	≤ 1 for all samples	i.	As the
previous works (e.g. Li & Liang (2018); Ji & Telgarsky (2020); Du et al. (2019b;a); Allen-Zhu et al.
(2019); Song & Yang (2019); Zou et al. (2019); Arora et al. (2019); Zou & Gu (2019)), we consider
only training the first layer {w(r)} and the second layer {ar} is fixed throughout the iterations.
In the following, we denote ut ∈ Rn whose ith entry is the network prediction for sample i (i.e.
ut[i] = NWt(xi)) in iteration t and y ∈ Rn is the vector whose ith is the label of sample i. Now let
us state a prior result of gradient descent convergence due to (Du et al., 2019b).
Theorem 1. (Theorem 4.1 in Du et al. (2019b)) Assume that λ := λmin(H) > 0 and that w0r) 〜
N(0, Id) and a『uniformly SamPledfrom {-1,1}. Set the numberofnodes m = Ω(λ-4n6δ-3) and
the constant ^tep size η = O(6).Then, with probability at least 1 一δ over the random initialization,
vanilla gradient descent, i.e. Algorithm 1& 2 with β = 0, has
kut - yk2 ≤ (1 一 η2^) ∙ ku0 — yk2.	⑺
We note that later Song & Yang (2019) improve the network size m to m = Ω(λ-4n4 log3(n∕δ))
while obtaining the same convergence rate result of vanilla gradient descent.
3 Main results
In this section, we first state the main results and provide the intuition behind the results and detailed
analysis in the later subsections.
Theorem 2. Assume that λ := λmin(H) > 0 and that w0r) 〜N(0, Id) and a『uniformly sampled
from { — 1,1}. Fix some maximum number ofiterations T, set a constant step size η ≤ ?八 1 (H), fix
momentum parameter β =(1 —	)2, and finally set a parameter ν > 0 that controls the number
of network nodes, chosen as m = Ω(λ-4n4+2ν log3(n∕δ)). Suppose that the number of samples
n satisfies ʌʌɪɪnν = Ω(T). Then, with probability at least 1 一 δ over the random initialization,
gradient descent with Polyak’s momentum (Algorithm 1 & Algorithm 2) satisfies for any t ≤ T,
+ 占 ku0 - yk.
(8)
4
Under review as a conference paper at ICLR 2021
λmax(H). With the chosen step size, We have
- 2√κ) I『i-yJ〔 + 2√2nνku0 -yk.
Remark 1: The step size η can be chosen as large as η = 1∕(2λmaχ(H)). Denote the condition
—
number of the Gram matrix as κ :
ut - y
ut-1 -
Interestingly, the rate 1
y
- 2√K
(9)
matches that of the accelerated rate in strongly convex smooth
problems (e.g. Nesterov (2013)), where the accelerated rate has an optimal dependency on the
condition number √κ instead of κ.
Thanks to an anonymous reviewer, we note that Wu et al. (2019b) provide an improved analysis
over Du et al. (2019b), which shows that the step size η of vanilla gradient descent can be set as
η = c°λ 1 (H) for some quantity co > 0, which in turns leads to a convergence rate(1 一 击) for
some quantity c0 > 0. As we discussed above, this rate has a worse dependency on κ and hence is
not better than what Polyak’s momentum can help to achieve.
Remark 2: Note that the initialization w0 = w1 ensures that u0 = u1. The condition that
η∣~nnν = Ω(T) can be easily satisfied when the parameter ν > 0 and the number of samples
n is sufficiently large. On the other hand, for the factor(1 一 J^λ)T to be small, the number of
iterations T should satisfy T = Ω (	∙ Both conditions can be satisfied by appropriately setting
the parameter ν > 0, which in turn determines the number of neurons m.
Theorem 2 states that Polyak,s momentum helps to reduce the initial error to a number 2%” ∣∣uo 一
yk at the accelerated rate 1 一 ∙vzηλ∕2, under the optimal tuning of momentum parameter β =(1 一
,ηλ∕2)2. An interesting result of Theorem 2 is that it shows the benefit of over-parametrization.
y increasing the number of neurons m, gradient descent with Polyak’s momentum will be able
to maintain the accelerated rate until it reduces the error to a smaller error. Specifically, one can
control the error, i.e. the last term of (8), by specifying the parameter ν. If ν = 1, then by setting the
number of neurons m = Ω(λ-4n6 log3(n∕δ)), Polyak,s momentum can decrease the error at the
accelerated rate to a number O( 1 )∣uo 一 y∣ = O(√n), where we use that the initial error satisfies
Ily — uok = O(√n) (see Lemma 8 in Appendix C). Similarly, if V = 1.5, then Polyak,s momentum
can decrease the error at the accelerated rate to a number O(n⅛)∣uo 一 y∣ = O( 1) under the
condition that m = Ω(λ-4n7 log3(n∕δ)). In other words, Polyak,s momentum helps to converge at
an accelerated rate up to an O( nV ∣∣uo 一 y∣) factor. While V can be tuned to decrease this additional
factor, this is at the expense of more neurons in the hidden layer. On the other hand, vanilla GD (e.g.
Du et al. (2019b); Wu et al. (2019b)) converges to an arbitrarily error linearly and does not exhibit
such type of the neighborhood convergence.
When the number of samples n approaches infinity, and V is chosen appropriately, the last term of
(8) vanishes and we have
(10)
Compared to the related work (Krichene et al., 2020) that shows asymptotic convergence result of
Polyak,s momentum for the neural network training in the mean-field limit, our convergence rate
result clearly demonstrates the advantage of Polyak,s momentum. Our result also implies that over-
parametrization helps acceleration in optimization. To our knowledge, in the literature, there is
little theory of understanding why over-parametrization can help training a neural network faster.
The only exception that we are aware of is (Arora et al., 2018), which shows that the dynamic
of vanilla gradient descent for an over-parametrized objective function exhibits some momentum
terms, although their message is very different from ours.
Remark 3 (iteration complexity): Let us analyze the number of iterations required to have an
error. Without loss of generality, we can assume that = γ∣u0 一 y∣ for some number γ ∈ (0, 1).
According to Theorem 1, we see that for the error ∣ut 一 y ∣ to decrease to γ ∣u0 一 y ∣ for some
number γ ∈ (0, 1), vanilla gradient descent needs a number of iterations T0g := d
2 log (Y)
log (1-等)
e. On
5
Under review as a conference paper at ICLR 2021
the other hand, gradient descent with Polyak’s momentum under the optimal tuning of β takes a
number of iterations Tm :=「log T/(2√)^^
0	Ilog(I-√ηλ) * 1
for the error kut - y k to decrease to γ ku0 - y k. To
see this, let 2√∣nν ∣∣u0 - y∣∣ = Yku2-yk on the r.h.s. of (8). Then, the number iterations for the
term	uut-t1--yy	≤
to decrease to
Y ku0-yk
2
is d
log (γ42√2))
comparing T0g and T0m , we have that
log (1-√ηλ)
e. By
T0m .
log(1 - " Tg =: θ
log (1 - Vη2λ)
(11)
where in the last equality, We define θ(ηλ)
log (1一警)
log (1-√ηλ)
value of θ(α)
To analyze θ(ηλ), we plot the function
:= log(1-α) for various 0 < α < 1 on
Figure 1, which clearly shows that the θ(α) decays ex-
tremely fast as α decreases. We have that θ(α) u 0.55 at
α = 0.5, while θ(α) u 0.1 at α = 10-2 and θ(α) = 0.01
at α = 10-4. Therefore, inequality (11) suggests that T0m
is small compared to Tg. For example if ηλ = 10-4, then
Tm ≤ 0.01 ∙ Tg, which shows that Polyak,s momentum
makes fast progress.
Figure1： θ S) := ι0gg(-√)) vs. α∙
3.1 More notations
For analysis, let us define the event Air := {∃w ∈ Rd : ∣w -WOr) k ≤ R, l{x>WOr)} = l{x>W ≥
0}}, where R > 0 is a number to be determined later. The event Air means that there exists a
W ∈ Rd which is within the R-ball centered at the initial point W0(r) such that its activation pattern of
sample i is different from that of w0r). We also denote a random set Si := {r ∈ [m] : l{Air} = 0}
and its complementary set Si⊥ := [m] \Si. Furthermore, we denote Ht ∈ Rn×n whose (i,j) entry is
H(Wt)i,j = * Pr=I x>XjU{〈w(r),xii ≥ 0 & hw(r),Xji ≥ 0}. We will use the notation ξ ∈ Rn
whose ith entry is ξt[i] := ut[i] - y[i], where ut[i] is the network prediction Nt(xi) := NWt (xi) at
time t and y[i] is the true label of sample i.
1
√m
3.2 Intuition of the result
Applying gradient descent with Polyak’s momentum to solving the objective (2) leads to the follow-
ing dynamics of training errors,
ξt+1[i] = Nt+1 (xi) - yi = √m Pr=ι arσ(Wt+ι xi) - yi
m
X ar (w(r)>xi - √mar Pn=I ξt j国Wir)>χj ≥ 0]x>χi
r=1
+β(Wtr)- W(-L)>xi)叫W(+)>Xi ≥ 0] - yi, (12)
where the last equality is due to the update rule of the algorithm.
Previous works like (Du et al., 2019a; Arora et al., 2019; Song & Yang, 2019) show that under certain
conditions, the activation patterns of most of the neurons do not change, i.e. l[W(r)>Xj ≥ 0]=
1 [W0r)>Xj ≥ 0] for all t. Now to get an intuition why momentum helps, let us for a moment assume
that the patterns of all neurons do not change during training. Then, one can replace U[w(?>X ≥ 0]
6
Under review as a conference paper at ICLR 2021
and l[w(r)>x ≥ 0] with l[w0r)>X ≥ 0] for any neuron r in equation (12), which leads to
ξt+1[i] =ξt[i]+β(ξt[i]-ξt-1[i])
nm
-m XX ξt[j] ∙ 1 [w0r)>Xi ≥ o] 1 [w0r)>Xj ≥ 0] χ>Xj
j=1 r=1
=ξt[i]+ β(ξt[i] — ξt-ι W) — ηHo[i,：冷，	(13)
where in the last equality we use the definition of H0 defined in Subsection 3.1. Apparently we can
rewrite the above equation in a matrix form,
ξt+1 = (In— ηH0)ξt + β(ξt — ξt-1).	(14)
So now we see that equation (14) and (4) are in the same form, which implies that provably showing
the benefit of Polyak’s momentum for the neural network training is possible. However, one has
to deal with the situation that some neurons do change their activation patterns during training.
Lemma 1 below deals with this issue.
Lemma 1. (Dynamics of the residual error): Following the notations defined in Subsection 3.1,
suppose that for all t ∈ [T] and r ∈ [m], kwt(r) — w0(r) k ≤ R, for a number R > 0. Then, gradient
descent with Polyak’s momentum (Algorithm 1 & Algorithm 2) for (2) has
ξt+1 = (In— ηHt)ξt + β(ξt — ξt-1) + φt,	(15)
where the ith entryof φt ∈ Rn satisfies ∣φt[i]∣ ≤ 2η√mSi | (∣∣ut — y∣∣ + β P：=0 βtτ-s∣∣us — y∣∣).
The proof of Lemma 1 is available in Appendix A. The recursive dynamics of the residual vector ξt,
(15), can be rewritten as
&+l]	_	[In	— ηHt	+ βIn	-βIn∖	ξ	ξt	↑ 工	Rt'
ξt	=	In	0	ξt-1 + 0
(16)
In the later subsection, we will show that kφt k is small and controllable. Specifically, we will use
the following lemma to control kφt k.
Lemma 2. (Claim 3.12 of Song & Yang (2019)) Fix a number R1 ∈ (0, 1). Recall that Si⊥ is a
random set defined in subsection 3.1. With probability at least 1 — n ∙ exp(—mRι), we have thatfor
all i ∈ [n],
∣S⊥∣ ≤ 4mRι.
A similar lemma also appears in (Du et al., 2019b). Lemma 2 says that the number of neurons whose
activation patterns for a sample i could change during the execution is only a small faction of m if
Ri is a small number, i.e. ∣S⊥ | ≤ 4mRι《m. In the later subsection, We will set Ri = O(nλν),
which together with the upper-bound of ∣φt[i]| ≤ 2η√mSi 1 (∣∣ut — yk + βP：=0 βtτ-s∣∣us — y∣∣)
in Lemma 1 will allow us to control kφtk.
Remark 4: We note that Liu et al. (2020b;a) establish an interesting connection between solving
an over-parametrized non-linear system of equations and solving the classical linear system. They
show that for smooth and twice differentiable activation, the optimization landscape of an over-
parametrized network satisfies a notion called Polyak-Lokasiewicz (PL) condition (Polyak, 1963),
i.e. 1 ∣∣VL(w)k2 ≥ μ (L(w) — L(w*)), where w* is a global minimizer and μ > 0. However, it
is not clear if the result can be extended to ReLU, as Safran et al. (2020) show that for a one-layer
ReLU network in the student-teacher setting, the PL condition does not hold after any degree of
over-parametrization of the student network. Furthermore, to our knowledge, there is little theo-
retical result of Polyak’s momentum showing an accelerated rate when an optimization landscape
satisfies the PL condition but has more than one global minimum. On the other hand, for a problem
that satisfies PL and has a unique global minimizer, Aujol et al. (2020) show a variant of Polyak’s
momentum method having an accelerated rate in a continuous-time limit. However, it is not clear if
their result is applicable to our case.
7
Under review as a conference paper at ICLR 2021
3.3 Detailed analysis
We first upper-bound the spectral norm of the matrix
In - ηHt + βIn
In
-β0In on (16) as follows.
Lemma 3. Following the setting as Theorem 2, set m = Ω(λ-2n2 log(n∕δ)) and the momentum
parameter β = (1 一	)2. Suppose that the step η is Chosen so that η ≤
probability at least 1 — δ - n2 exp(-mR/10) ,for any set ofweight vectors W
satisfying ∣∣w(r) — w0r)k ≤ R =会 forany r ∈ [m], it holds that
1 _
2λmax(H)
. Then, with
{w(1),...,w(m)}
In - ηH(W) +βIn -βIn
In
0
2 ≤ '7*.
The proof of Lemma 3 is available in Appendix B. Now we are ready to prove Theorem 2.
Proof. (of Theorem 2) We will denote R := 64，1+v, cτ := maxt≤τ βt(1 + β* P：=0 βS), C :=
16√2cτηnR∣uo — y∣, and β* := 1 —	≥ 2. We will prove for all t ∈ [T], the following
inequalities hold
ξtξ-t1
∣wt(r) — w0(r) ∣
≤ R and	∣φt ∣ ≤ C.
(17)
(18)
The proof is by induction. For the base case t = 0, inequality (17) and the first inequality of (18)
trivially holds. It remains to bound ∣∣φo∣. With probability at least 1 — n ∙ exp( — mR):
kΦθk = √Pn=1 φθ[i]2 = JPn=I (2η√mS⊥l kuo- yk)2
≤) √Pn=ι(2η√n4mR)2(kuo - yk)2 = 8ηnR∣uo - y∣ ≤ C,
where the above inequality relies on Lemma 2, so We have that ∣S⊥∣ ≤ 4mR for all i ∈ [n]. Now
we can conclude that (17) and (18) hold for the base case 0.
Suppose that (17) and (18) hold at time s = 0, 1,2, . . . , t — 1. Then,
ξt
ξt-1
(1≤6)	In — ηHt-1 +βIn
-βIn
0
ξt-1
ξt-2
φt0-1
+
2
(a)
≤ B* ∙
ξt-1
ξt-2
+C
≤
βt 胪1」，
(19)
where (a) is by Lemma 3 and the induction that ∣φt-1 ∣ ≤ C, (b) is by the recursive expansion of
the second inequality. So (17) holds at t.
Using (19), we now show that ∣φt ∣ ≤ C. We have that
kφtk = √pn=ι φt[i]2 ≤ ∕pn=ι (2η√ms⊥l (kut - yk + β PS=O βt-j kus - yk))2
(a)
≤ 8ηnR(∣ut - yk + βPS=0 βtτ-s∣∣Us -y∣∣)
≤ 8nnR(Btt√2kuo - yk + IC； + βPS=O βt-1-s(βs√2kuo - yk + j-Ce；))
(C)	+ -l	Q
≤ 8ηnR(Btt (1 + β* Ps=o βs)√2kuO - yk + ι-β; (1 + ι-β; ))
(d)
≤ C,	(20)
8
Under review as a conference paper at ICLR 2021
where (a) We use Lemma 2 so that for all i ∈ [n], it holds that ∣S⊥∣ ≤ 4mR, with probability at
least 1 - n ∙ exp(-mR), (b) is by induction that kut - yk ≤ βtt√2∣∣uo - yk + IZC- as uo = u-ι,
1-β *
(c) uses that β = β2, and (d) is due to that βt (1 + β* P：=0 βS) ≤ CT and that
8ηnRcτ√2∣∣uo - yk ≤ C
1 _ 8ηnR (1 + β* ) ≤ ,
1 ι-β*(1 + ι-β*)
(21)
which is proved as follows. Using the definition of β* and R, we have that 1 - 8η^ (1 + τβ¾-) ≥
1-β*	1-β*
1 - 32λnR ≥ 2. So for (21) to hold, it suffices to have that 16ηnRcτ√2∣∣uo - yk ≤ C, which is true
by the definition of C. Now we are going to show that ∣∣w(r) - w0r) ∣∣ ≤ R :二 64上1也.We have that
kw(r)-w0r) k ≤) η√mn(表+
(b)	L
32nRcT1 )ky-uok ≤ η√n(或)ky-u0k
=η√mn (表)O( Jnlog(m∕δ)log2(n∕δ))
(22)
(d)
≤
λ
64n1+ν,
where (a) is due to Lemma 4 in Appendix C, (b) is because 32nRcTt = Cnt ≤ [焉λ似 ≤ W,
where we use CT ≤ √2⅛, which is shown in Lemma 9 in Appendix C, as well as the condition that
√λnν = Ω(T), (c) is due to Lemma 8 in Appendix C, which states that with probability at least
1 - δ∕3, the initial error satisfies ky - u0k2 = O(n log(m∕δ) log2(n∕δ)), and (d) is by the choice
of the number of neurons m = Ω(λ-4n4+2ν log3(n∕δ)). So we can conclude that (18) holds at t.
Furthermore, with the choice of m, we have that 3n2 exp(-mR∕10) ≤ δ. Finally, Lemma 9 in
Appendix C shows that IC^ ≤ 2√nνky - uo∣. Thus, we have completed the proof.	□
4 Conclusion
In this work, we show that Polyak’s momentum helps to accelerate training a one-layer ReLU net-
work. The insight is that the dynamic of the predictions by the neural network during training is not
very different from the accelerated dynamic in solving the strongly convex quadratic functions by
the same method, provided that the weights of the neural net do not move away from its initialization
too much so that most of the activation patterns of the neurons remain the same during training. We
note that in the literature, this is called training a neural net in the Neural Tangent Kernel (NTK)
regime (Jacot et al., 2018). Recent work of (Nakkiran et al., 2019) shows that during the early stage
of training, the functions that a neural net learns are some simple functions of data, and then it starts
learning more complicated functions after learning the simple one. Furthermore, Hu et al. (2020)
suggest that during the early stage, the network training is indeed in the NTK regime. Therefore, a
possible future work is combing our results and those of (Nakkiran et al., 2019; Hu et al., 2020) to
show that momentum helps to learn the simple functions faster. We hope that our work sheds light
on explaining why the momentum method works well in practice.
References
Ahmet Alacaoglu, Yura Malitsky, Panayotis Mertikopoulos, and Volkan Cevher. A new regret anal-
ysis for adam-type algorithms. ICML, 2020.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. ICML, 2019.
Andersen Ang. Heavy ball method on convex quadratic problem. Lecture note, 2018.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. ICML, 2018.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. NeurIPS, 2019.
9
Under review as a conference paper at ICLR 2021
Jean-Francois Aujol, Charles Dossal, and Aude Rondepierre. Convergence rates of the heavy-ball
method with lojasiewicz property. hal-02928958, 2020.
Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of
wide neural networks. ICLR, 2020.
Alberto Bietti and Julien Mairal. On the inductive bias of neural tangent kernels. NeurIPS, 2019.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. ICML, 2017.
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di He, Zhihua Zhang, and Liwei Wang.
A gram-gauss-newton method learning overparameterized deep neural networks for regression
problems. arXiv.org:1905.11675, 2019.
BUgra Can, Mert Gurbuzbalaban, and Lingjiong Zhu. Accelerated linear convergence of stochastic
momentum methods in wasserstein distances. ICML, 2019.
You-Lin Chen and Mladen Kolar. Understanding accelerated stochastic gradient descent via the
growth condition. arXiv:2006.06782, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
NeurIPS, 2019.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd.
NeurIPS, 2019.
Amit Daniely. Sgd learns the conjugate kernel class of the network. NeurIPS, 2017.
Amit Daniely. Memorizing gaussians with no over-parameterizaion via gradient decent on neural
networks. arXiv:1909.11837, 2020.
Jelena Diakonikolas and Michael I. Jordan. Generalized momentum-based methods: A hamiltonian
perspective. arXiv:1906.00436, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, , and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. ICML, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. ICLR, 2019b.
Yonatan Dukler, Quanquan Gu, and Guido Montufar. Optimization theory for relu neural networks
trained with normalization layers. ICML, 2020.
Cong Fang, Hanze Dong, and Tong Zhang. Over parameterized two-level neural networks can learn
near optimal feature representations. arXiv:1910.11508, 2019.
Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size.
COLT, 2015.
Sebastien Gadat, Fabien Panloup, and Sofiane Saadane. Stochastic heavy ball. arXiv:1609.04228,
2016.
Rong Ge, Rohith Kuditipudi, Zhize Li, and Xiang Wang. Learning two-layer neural networks with
symmetric inputs. ICLR, 2019.
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the
heavy-ball method for convex optimization. ECC, 2015.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, , and Andrea Montanari. Linearized two-
layers neural networks in high dimension. arXiv:1904.12191, 2019.
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum
in stochastic gradient methods. NeurIPS, 2019.
10
Under review as a conference paper at ICLR 2021
Gabriel Goh. Why momentum really works. Distill, 2017.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. ICLR,
2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. NIPS, 2017.
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the early-
time learning dynamics of neural networks. NeurIPS, 2020.
Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks
generalize better than deep feedforward networks? — a neural tangent kernel perspective.
arXiv:2002.06262, 2020.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. NeurIPS, 2018.
Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbi-
trarily small test error with shallow relu networks. ICLR, 2020.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of
existing momentum schemes for stochastic optimization. ICLR, 2018.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Walid Krichene, Kenneth F. Caluyay, and Abhishek Halder. Global convergence of second-order
dynamics in two-layer neural networks. arXiv:2006.07867, 2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convo-
lutional neural networks. NIPS, 2012.
Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and
Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient
descent. NeurIPS, 2019.
Jason D. Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage score
sampling for neural networks. arXiv:2009.09829, 2020.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algo-
rithms via integral quadratic constraints. SIAM Journal on Optimization, 2016.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. NeurIPS, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
NeurIPS, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Learning over-parametrized two-layer relu neural
networks beyond ntk. COLT, 2020.
Chaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learn-
ing. ICLR, 2020.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when and
why the tangent kernel is constant. arXiv:2010.01092, 2020a.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-
parameterized systems of non-linear equations: the lessons of deep learning. arXiv:2003.00307,
2020b.
11
Under review as a conference paper at ICLR 2021
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. arXiv:2007.07989, 2020c.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient,
newton, proximal point and subspace descent methods. arXiv:1712.09677, 2017.
Nicolas Loizou and Peter Richtarik. Accelerated gossip via stochastic heavy ball method. Allerton,
2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. ICLR, 2019.
Chris J. Maddison, Daniel Paulin, Yee Whye Teh, Brendan O’Donoghue, and Arnaud Doucet.
Hamiltonian descent methods. arXiv:1809.05042, 2018.
Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L. Edelman, Fred
Zhang, and Boaz Barak. Sgd on neural networks learns functions of increasing complexity.
NeurIPS, 2019.
Yurii Nesterov. Introductory lectures on convex optimization: a basic course. Springer, 2013.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global conver-
gence guarantees for training shallow neural networks. arXiv:1902.04674, 2019.
Abhishek Panigrahi, Abhishek Shetty, and Navin Goyal. Effect of activation functions on the train-
ing of overparametrized neural nets. ICLR, 2020.
Boris Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matematiki i
Matematicheskoi Fiziki, 1963.
Boris T. Polyak. Introduction to optimization. Optimization Software, 1987.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 1964.
Benjamin Recht. Lyapunov analysis and the heavy ball method. Lecture note, 2018.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. ICLR,
2018.
Itay Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the
optimization landscape of shallow relu neural networks. arXiv:2006.01005, 2020.
Michael Saunders. Notes on first-order methods for minimizing smooth functions. Lecture note,
2018.
Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum.
ICML, 2020.
Othmane Sebbouh, Robert M. Gower, and Aaron Defazio. On the convergence of the stochastic
heavy ball method. arXiv:2006.07867, 2020.
Mahdi Soltanolkotabi. Learning relus via gradient descent. NeurIPS, 2017.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.
arXiv:1906.03593, 2019.
Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approxi-
mation perspective. NeurIPS, 2019.
Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang. Non-ergodic
convergence analysis of heavy-ball algorithms. AAAI, 2019.
12
Under review as a conference paper at ICLR 2021
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. ICML, 2013.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. ICML, 2017.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networks in near-linear time. arXiv:2006.11648, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, and et al. Attention is all you need. NIPS, 2017.
Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. Escaping saddle points faster with stochastic
momentum. ICLR, 2020.
Colin Wei, Jason D. Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets v.s. their induced kernel. NeurIPS, 2019.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, , and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning. NIPS, 2017.
Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by
one-layer relu networks. NeurIPS, 2019a.
Xiaoxia Wu, Simon S Du, and Rachel Ward. Global convergence of adaptive gradient methods for
an over-parameterized neural network. arXiv:1902.07111, 2019b.
Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv:1902.04760, 2019.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. IJCAI, 2018.
Gilad Yehudai and Ohad Shamir. Learning a single neuron with gradient methods. COLT, 2020.
Guodong Zhang, James Martens, and Roger B Grosse. Fast convergence of natural gradient descent
for over-parameterized neural networks. NeurIPS, 2019.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery guarantees
for one-hidden-layer neural networks. ICML, 2017.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. NeurIPS, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
overparameterized deep relu networks. Machine Learning, Springer, 2019.
A Proof of Lemma 1
Lemma 1: (Dynamics of the residual error): Following the notations defined in Subsection 3.1,
suppose that for all t ∈ [T] and r ∈ [m], kwt(r) - w0(r) k ≤ R, for a number R > 0. Then, gradient
descent with Polyak’s momentum (Algorithm 1 & Algorithm 2) for (2) has
ξt+1 = (In - ηHt)ξt + β(ξt - ξt-1) + φt,	(23)
where the ith entry of φt ∈ Rn satisfies ∣φt[i]∣ ≤ 2η√mS⊥l (∣∣ut -yk + β PS=0 βt-1-skus - y∣∣).
Proof. For each sample i, we will divide the contribution to N (xi) into two groups.
m
N(Xi) = √m X arσ(hw(r) ,Xi))
r=1
=-J= X arσ(hw(r),Xii) + -^= X arσ(hw(r),Xii).
Vzm r∈Si	Vzm r∈S⊥
(24)
13
Under review as a conference paper at ICLR 2021
To continue, let us recall some notations; the subgradient with respect to w(r) ∈ Rd is
dL(W) := √U X(N(Xi) - yi)arXil{hw(r),xi ≥ 0}.
∂w	m i=1
and the Gram matrix Ht whose (i, j) element is
1m
Ht[i,j] := mX>Xj X l{hw(r),Xii ≥ 0 & hw(r),Xji ≥ 0}.
r=1
Let us also denote
H⊥[i,j] ：= mmχ>χj X i{hw(r),χii ≥ o & hw(r),χji ≥ 0}.
r∈Si⊥
We have that
ξt+1 [i] = Nt+1 (Xi ) - yi
(=) √m X arσ(hw(+)i,xii)+ √
r∈Si
(25)
(26)
(27)
(28)
arσ(hwt(+r)1, xii) - yi
r∈Si⊥
|
}
^^^{^^^
first term
For the first term above, we have that
√m X arσ(hw(+)i,xii) = √m X arσ(hw(r) - η
r∈Si	r∈Si
∂L(Wt)
∂w(r)
+ β(wt(r) - wt(-r)1), Xii)
'-------------------------{-------------------------}
first term
X ar hwt(r)
r∈Si
-	ηdL(Wt) + β(w(r) - w(-)ι),xii ∙ ι{hw(+)ι,χii ≥ 0}
∂wt
X ar hw(r) ,Xi i ∙ l{hw(r),Xii ≥ 0} +
r∈Si
β
√m
X ar hw(r),Xii ∙ l{hw(r),Xii ≥ 0}
r∈Si
-	√β= X ar hw(-)ι,χii ∙ l{hw(-)ι,xii ≥ 0} - η ~√= X ar h dL(Wt) ,xiil{hw(r),xii ≥ 0}
m r∈Si	m r∈Si	∂wt(r)
=Nt(xi) + β(Nt(Xi)-M-ι(Xi)) - ■√m X arhw(r),Xiil{(w(r),χii ≥ 0}
r∈Si⊥
-	√m X ar hw(r),Xiil{hw(r) ,Xi i ≥ 0} + √β= X ar hw(-)1,Xiil{hw(-)1,Xii ≥ 0})
m r∈Si⊥	m r∈Si⊥
-	η √1= X ar h dL(Wt) ,Xiil{hw(r),Xii ≥ 0}
m r∈Si	∂wt
last term
(29)
where (a) uses that for r ∈ Si, l{hw(+[,Xii ≥ 0} = l{hw(r),Xi> ≥ 0} = l{hw(-1,Xii ≥ 0} as
the neurons in Si do not change their activation patterns. We can further bound (29) as
n
=IM(Xi)+ β(Nt(Xi) -Nt-ι(Xi)) - ηX(Nt(Xj) - y)H(Wt)i,j
j=1
-	m X X>Xj (Nt(Xj )-yj) X l{hw(r),Xii ≥ 0 & hw(r),Xj i ≥ 0}
j=1	r∈Si⊥
—
+
√= X ar hW(r),Xiil{hW(r),Xii ≥ 0} - √β= X ar hwf),Xiil{hw(" ,Xi>≥ 0}
r∈Si⊥	r∈Si⊥
√= X ar hw(-)i,Xiil{hw(-)i,Xii ≥0}),
r∈Si⊥
(30)
14
Under review as a conference paper at ICLR 2021
where (b) is due to that
√m Pr∈Si ar h 啜/,xiiι{hwt ),χii ≥ 0}
、---------------{z--------------}
last term
1n
mExJxj(Nt(Xj) -yj-) E ι{hwt ,xii ≥0&hwt	,xji ≥0}
j=1
n
r∈Si
1n
mEx>xj(Nt(Xj)-yj) E ι{hw(r),χii ≥0&hw(r),χji ≥0}.
j=1
r∈Si⊥
(31)
Combining (28) and (30), we have that
n
ξt+ι[i] = ξt[i] + β(ξt[i] - ξt-ι[i]) - ηXHt\i,j\ξt[j\
j=1
- m Xχ>χj(Nt(Xj) - yj) X i{hw(r),χii ≥ 0 & hw(r),χji ≥ 0}
j=1	r∈Si⊥
+ √1= X arσ(hwt+ι,Xii) — arσ(hw(r),Xii) 一 βarσ(hw(r),Xii) + βarσ(hw(r)ι,Xii).
m r∈Si⊥
(32)
So we can write the above into a matrix form.
ξt+1 = (In - ηHt)ξt + β(ξt - ξt-1) + φt,	(33)
where the i element of φt ∈ Rn is defined as
n
φt[i] = -m Xχ>χj(Nt(Xj) - yj) X ι{hw(r),χii ≥ 0 & hw(r),χji ≥ 0}
j=1	r∈Si⊥
+ √m X {arσ(hw(+)ι,Xii) - arσ(hw(r),Xii) 一 βarσ(hw(r),Xii) + βarσ(hw(-)ι,Xi))}.
m r∈Si⊥
(34)
15
Under review as a conference paper at ICLR 2021
Now let US bound φt [i] as follows.
n
Φt[i] = -m Xx>xj(Nt(Xj) - yj) X l{hWf),xii ≥ 0 & hwtr),χji ≥ 0}
j=1	r∈S⊥
+ —1= X {arσ({wt+i,Xi}) - arb(〈w(r),g〉)- βarσ((w(r),xi)) + βarσ((w(r)1, xi))}
mm r∈S⊥
‹)η√=si | nut-yk + √= X(kw(?i-Wf)Il+βkw(r)-w(-h∣)
S r∈S⊥
⑨ η√nIS⊥l Il II , η XMX/Jt-s dL(Ws) Il , 8∣∣X Bt-I-S ”(WS)Q
=M-yk + —m Ncι入β 飞尸k+βk⅛β 飞尸k)
r∈Si
(< η√nS⅛ut-yk + + X (Xβt-skd∣≡k + βXβt-1-skd∣≡k)
m	√m	力/Jr)	∂/Λr)
V r∈S⊥ s=0	UWS	s=0	UWS
≤ η√n<⅛t-yk + η√9(Xβt-s∣us-yk + βXβt-1-s∣
mm
S=0	S=0
2η√nlS⊥l (M- yk + βXβt-1-skus - yk),
m	S=0
- yι
(35)
where (a) is because -悬 Pn=I XJXj(Nt(Xj) -y) Pr∈s⊥ l{<w(r),xi) ≥ 0 & (w(r), Xj)≥ 0} ≤
⅛⊥^ Pn=I IM(Xj) - yj I ≤ η√nS⊥l ∣∣ut - y∣, and that σ(∙) is 1-Lipschitz so that
√= X (arff(hw(+)1,Xi)) - ar σ(hwf', xA) ≤ √= X Kw(?i,Xi〉- hw(r),Xii|
V	r∈S⊥	r∈	r∈S⊥
≤ √= X kw(?I-Wf, kkXi Ii ≤ √= X ι∣w(+)ι- w(r)k,
V	r∈S⊥	V	r∈S⊥
similarly, -√= Pr∈S⊥ (ar σ(hw(r),Xi)) - ar σ( hw(-)1, Xii)) ≤ β √= Pr∈S⊥ M;" - ^-L1，⑹
is by the update rule (Algorithm 1), (c) is by Jensen,s inequality, (d) is because ∣dL(Ws ∣ =
∂ws !
I√= Pn=I (us[i] -yi)arXii{X>w(r) ≥0}∣ ≤ √nIlus -y∣∣.
□
B Proof of Lemma 3
Lemma 3: Following the setting as Theorem 2, set = = Ω(λ-2n2 log(n∕δ)) and the momentum
parameter β = (1 —	)2. Suppose that the step η is chosen so that η ≤ ?工 1 (H)∙ Then, with
probability at least 1 - δ-n2 ∙exp(-mR∕10), for any set ofweight vectors W := {w ⑴，...，w(m)}
satisfying ||w(r) — w0r)| ≤ R :=余 forany r ∈ [=], it holds that
In - ηH(W)+ βIn
In
I γ. „ JIyr I， In — ηH(W) + βIn
Proof. Denote M := ∣∣	1 y n n
In
eigenvalues of H in a decreasing order. To obtain the spectral norm M, it suffices to consider the
βIn on (16). Denote λ(1) ≥ λ(2) ≥ ∙ ∙ ∙ ≥ λ(n)
16
Under review as a conference paper at ICLR 2021
spectral norm of the sub-matrix Mk := k
1 - ηλ(k) +β
1
0β k2 ∈ R2×2 for each k ∈ [n], and
one will have M = maxk {Mk }, which is a known technique in the literature (see e.g. Saunders
(2018); Ang (2018); Recht (2018)). The eigenvalues of the 2 X 2 matrix is given by the roots of
p(k) := z2 一 (1 + β 一 ηλ(k))z + β. It can be shown that when β is at least (1 一 √ηλ(k)) , then
the magnitude of the roots of each p(k) are at most √β (see Lemma 5 in Appendix C). It remains to
bound λ(n). We have that
λ(n := λmin(H(W)) ≥ λmin(HO)-IlHO- H (W)IIF ≥ 3λ - 1 λ ≥ 2,	(36)
where in the second to last inequality, we use Lemma 6 in Appendix C, which states that with
probability at least 1 一 δ, the smallest eigenvalue satisfies λmin(Ho) ≥ ∣λ under the condition of
the number of neurons, i.e. m = Ω(λ-2n2 log(n∕δ)), and We also use Lemma 7 in Appendix C,
which shows that if ∣w(r) 一 w0"∣∣ ≤ R=会 for all r ∈ [m], then with probability at least
1 一 n2 exp(—m 10), it holds that ∣Ho — H (W )∣f ≤ 2nRR = 2n 翁=λ.
ByUSingthatkHO ― H (W )∣f ≤ λ, we also have
λ ⑴：=λmaχ(H(W)) ≤ λmaχ(Ho) + ∣∣ H° - H (W )∣F ≤ λmaχ(H0) + 4
≤ λmax(H) + IlHO - HIIF + 4 ≤ λmax(H) + 2,	(37)
where in the last inequality we use Lemma 6. Therefore, for η ≤ ?工 1 (H), we have ηλ(k) ≤ 1
2	/-f 2 (36)
for any k ∈ [n]. Consequently, we have 1 一 ηλ(n)	≤
(1 一 q~ηλ-) = β, which in turn
implies that M = maxk{Mk} ≤ √β = 1 一 Jη2λ.
□
C Some supporting lemmas
We will also need the following lemma, which shows that the iterate during the execution of the
algorithm is not far away from its initialization. Similar results appear in the previous works (e.g. Li
& Liang (2018); Ji & Telgarsky (2020); Du et al. (2019b;a); Allen-Zhu et al. (2019); Song & Yang
(2019); Zou et al. (2019); Arora et al. (2019); Zou & Gu (2019)).
Lemma 4. Following the setting as Theorem 2, if for any s ≤ t, the residual dynamics satisfies
k ξξs] k ≤ βS ∙ k ξξ: k + 1-g*, where C := 16√2ηnRcτ∣∣uo — y∣, then we have that
kw(+L - WOr)k ≤ η√nCβ*,tky - uok,
forallr ∈ H,Where Cβ*,t := (i-β*)2 +16ZRcT*)2+1) ≤ ηλ + 32nRcT(t+1)∙
17
Under review as a conference paper at ICLR 2021
Proof.
kw(r)1 - w0r)k ≤) η X km(r)k =) η X k X βs-τdL(Wτ) k
t+1	0	s	(r)
s=0	s=0 τ=0	∂wτ
(c)	_t_s_	H
≤ ηXXβs-τ√=ky-UTk
s=0 τ=0	m
ts
≤ η X X βs-τ∣
s=0 τ=0
∂L(Wτ)
∂wTr)
k
)
≤) η√mn X 1⅛ ky-U0k+「X X …)
(f) η√2n
√m (1 - β*)2
∣y -u0∣ +
η√n(l6√2ηnRcτ∣∣U0 - yk)(t + 1)
Vm(I- β*)(i- β2)
1
(g) η√2nL	H	H
=^mCβ*,tky- uok,
(38)
where (a), (b) is by the update rule of momentum, which is wt(+r)1 - wt(r) = -ηmt(r), where
mtr) := PS=O βt-sdLlWs), (C) is because k dLwWWs)k = k Pn=ι(yi-us[i])√1marXi∙l{hwSr),χi ≥
0}k ≤ √m Pn=I |yi - Us[i]∣ ≤ √√mky - Usk2, (d) is due to the assumption that k ξξ^ k ≤
βS ∙ k ξξ01 k + 1-Cβ*, (e) is because that β = β2, (f) is by P∞=1 kθk = (T-θ)2 for any θ ∈ [0,1),
and (g) We denote C0* ,t :=([*)2 + 16ηnRCT)2+1). Finally, by using that β* := (1 - q∕ηλ), We
have that C§* ,t ≤ η2λ + 32nRT(t+1). The proof is completed.
□
Lemma 5. The roots ofthe characteristics equation, z2 — (1 + β — ηλk)z + β = 0, have magnitude
|z| ≤ √β, if β ≥ (1 - √ηλk)2∙
Proof. The roots of z2 - (1 + β - ηλQz + β = 0 are Z = ι+β一ηλk±ʌ/(2+"-n") -4β.
The magnitude of the roots are the same When the roots are imaginary, Which is |z | =
q/ (1+β- ηλk )2+4β- (l+β- ηλk)2 = √β
Simple calculation shows that (1 + β - ηλk)2 - 4β ≤ 0 if β ≥ (1 - √ηλk)2.
□
Lemma 6. (Lemma 3.1 in Du et al. (2019b) and Song & Yang (2019)) Denote λ := λma(H). Set
m = Ω(λ-2n2 log(n∕δ)). Suppose that Wι,...,Wm are i.i.d. generated N(0, Id). Then, it holds
that
kH(W)- HkF ≤ 4 and λmm(H(W)) ≥ 3λ,
with probability at least 1 - δ.
Lemma 7. (Lemma 3.2 in Song & Yang (2019)) FixanUmber Ro ∈ (0,1). Suppose that Wi,..., Wm
are i.i.d. generated N(0, Id). Then, for any set of weight vectors w1, . . . , wm ∈ Rd that satisfy for
any r ∈ [m], ∣∣Wr — Wr ∣∣2 ≤ Ro, it holds that
,~ .	...
∣h(W)- H (W )∣∣f < 2nRo,
with probability at least 1 — n2 ∙ exp(-mRo/10),
18
Under review as a conference paper at ICLR 2021
Lemma 8. (Claim 3.10 in Song & Yang (2019)) Assume that w0r) 〜N(0, Id) and a『uniformly
sampled from {-1, 1}. For 0 < δ < 1, we have that
ky - uok2 = O(nlog(m∕δ) log2(n∕δ)),
with probability at least 1 - δ.
Lemma 9. Denote R := 64.+ν ,cτ ：= maxt≤τ βt (1+ β* P：=0 βS),C ：= 16√2cτ ηnR∣∣U0-y∣∣,
andβ* := 1 — ʌ∕ηλ. Then, CT ≤ 蚱 '1^ and IC ≤ 4√2β 壮 ky 一 U0k. Furthermore, if ηλ ≤ ɪ,
then β* ≥ 1; and consequently,
1
CT ≤	,
一 √2ηλ
C1
=瓦 ≤ 2√2nVky - u0k.
Proof. We have that
C
1 — β*
乎ηλcτku0 - yk
n qηF
宇 ku0-yk.
2nν
(39)
So it remains to bound CT := maxt≤τ βt(1 + β* P：=0 βS). Let Us denote X := βt. Note that
X ≤ 1. Consider maximize h(x) := x(1 + β* ∙1-X) =	—— τβ¾-x2. The derivative is Vh(x)
1—β *	1β *	1β *
-∏⅛-----52β*-x. So the maximal value is at X = ɪ and We have that h(春)
-β*	-β*	2β*	2β*
1
4β*(1-β*)
------F=-J=^. So we have that CT ≤
4(1-√ηλ)√ηλ	一
——1j=λ. Substituting it back to (39), we have that
4β*V η2T
C V 1
1 - β* — 4√2β*nν
ky - u0k.
(40)
□
19
Under review as a conference paper at ICLR 2021
D Experiment
In this section, we report a proof-of-concept ex-
periment. We sample n = 5 points from the
normal distribution, and then scale the size to
the unit norm. We generate the labels uniformly
random from {1, -1}. We let m = 1000 and
d = 10. We compare vanilla GD and gradient
descent with Polyak’s momentum. We use the
empirical Gram matrix at the initialization as
an estimate of H. Denote λmax := λmax(H0)
and λmin := λmin(H0). Then, for gradient de-

scent with Polyak’s momentum, we set the step
size η = 1/(10、max) and set the momentum
parameter β = (1 - Jηλmin)2. For gradient
descent, we set the same step size.
5	10	15	20	25	30	35	40	45	50
iteration
Figure 2: Empirical risk L(Wt) vs. iteration t.
Polyak’s momentum accelerates the optimization pro-
cess.
We also report the percentiles of pattern
changes over iterations. Specifically, we report
the quantity
Pn=I Pm=I 叫 Sign(x> Wy))=Sign(x> WOr))}
mn
as there are mn patterns. For gradient descent with Polyak’s momentum, the percentiles of pat-
tern changes is approximately 0.76%; while for vanilla gradient descent, the percentiles of pattern
changes is 0.55%.
20