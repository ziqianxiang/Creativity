Under review as a conference paper at ICLR 2021
Model information as an analysis tool in deep
LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
Information-theoretic perspectives can provide an alternative dimension of analyz-
ing the learning process and complements usual performance metrics. Recently
several works proposed methods for quantifying information content in a model
(which we refer to as “model information”). We demonstrate using model infor-
mation as a general analysis tool to gain insight into problems that arise in deep
learning. By utilizing model information in different scenarios with different con-
trol variables, we are able to adapt model information to analyze fundamental
elements of learning, i.e., task, data, model, and algorithm. We provide an ex-
ample in each domain that model information is used as a tool to provide new
solutions to problems or to gain insight into the nature of the particular learning
setting. These examples help to illustrate the versatility and potential utility of
model information as an analysis tool in deep learning.
1	Introduction
The ultimate goal of many deep learning research has been improving performance on specific
datasets, for example, aiming for superior classification accuracy on the ILSVRC challenge. We
have witnessed super-human performance on tasks in vision and language processing, but we are still
far from understanding how the learning process works and whether they resemble the learning of
human. This is partially due to the metric we use providing too little information on the dynamics of
learning. Besides, performance on the test set as a sole goal can sometimes even lead to undesirable
outcomes or misleading conclusions (Lipton & Steinhardt, 2019).
Recently, several works propose to use the description length of a model (or surrogate estimations
thereof) to understand the behavior of learning. In this paper, we refer to such measures of the
amount of information content in a model as model information. Blier & Ollivier (2018) first demon-
strated efficiently encoding a deep neural network with prequential coding technique. Zhang et al.
(2020) then proposed an approximation of model information and utilized model information to an-
alyze the information content in a task. They also used model information to explain phenomenons
in transfer learning and continual learning. They showed that model information provides a different
perspective than performance, directly characterizing the information transfer in the learning pro-
cess. Voita & Titov (2020) used model information as a probe to analyze what kind of information
is present in a text representation model. They claim that model information is more informative
and stable than performance metrics when used as a probe.
Model information can provide an informational perspective to learning. It can potentially help to
answer questions about learning dynamics, such as how much information exists in a dataset or
model, or how much information is transferred in a learning step. Furthermore, we can also re-
formulate existing problems into problems about information, for example, similarity and capacity,
as we will show in this paper. Comparing model information with model performance, model infor-
mation not only accounts for how good a model can perform but also how fast it learns to perform
well (in the sense of sample efficiency, a discussion is given by Yogatama et al. (2019)). The latter
can be interpreted as related to the quantity of information transferred in model training.
In this paper, we try to illustrate that model information can provide a framework for analyzing and
understanding phenomena in deep learning. In the next section, we provide a general definition of
model information, independent of how model information is estimated. We then unify the anal-
ysis of fundamental elements of deep learning under the framework of model information. In the
1
Under review as a conference paper at ICLR 2021
following sections, we use several common problems as examples to show how to use the model
information framework as an analysis tool.
2	An informational perspective to elements of deep learning
To measure the amount of information in a model, Voita & Titov (2020) use the codelength of a
dataset subtracting the cross-entropy of the final model:
N
L = Lθinqit (yi：N |xi：N) + X log pθn (yig.	(1)
i
Zhang et al. (2020) propose to estimate model information by subtracting codelength of k examples
(the k examples are independent and different from the training set of size N) with an initial model
and a final model, and show that it is more reliable than (1):
L = Lθinit(yi：k|xi：k) - LθrNq(yi：k|xi：k).	⑵
Both methods share an idea that model information can be derived by comparing the codelength of
encoding the model and data together, with the codelength of encoding the data alone.
This is essentially an approximation to the Kolmogorov complexity of the “generalizable knowl-
edge” in model M (in Zhang et al. (2020), this is denoted by K(M) - K(M|T)). Throughout
this paper, we use D to denote a dataset including examples sampled from task T , and use MD
to denote a model M that has been trained to converge on D. Assume labels y in task T have an
underlying generation mechanism f: yi = f(xi, i) (where i is i.i.d. noise), then the “generalizable
knowledge” in T is K(f), and generalizable knowledge in MD is at most K(f).
We denote the amount of generalizable information a model M contains about a dataset D as
L(M, D). Then L(MD, D) is the “model information”: the amount of generalizable information a
trained model MD contains about dataset D. Ideally, if learning is “perfectly efficient”, then we can
expectL(MD,D) = K(MD) - K(MD|T) = K(f).
In this paper, we investigate what can be achieved with a model information measure L. We found
that model information is a powerful tool for analyzing and understanding many phenomenons in
deep learning. This covers the fundamental elements of machine learning: task, data, model, and
algorithm. In Table 1, we list problems as examples for each domain and summarize how model
information can be used to perform analysis of the corresponding problem.
Table 1: Model information as an analysis tool in deep learning
Element	Relevant problem	Use of model information
Task	Task difficulty	L(MD,D)
Data	Domain similarity	L(MD1,D2)
Model (structure)	Model capacity	maxD L(MD , D)
(parameter)	Ablation study	L(MD ,D)
Algorithm	Knowledge distillation	L(MD+DT , D)
As we shall see, model information can enable attacking some of the above problems from a neat
and theoretically-sound perspective. Table 1 is not an exhaustive list but just examples of what
one can do with model information. In the following sections, we detail the application of model
information to perform analysis of each problem and explain how it can lead to useful insights.
For following experiments in this paper, we use (2) to estimate model information and set the en-
coding set size k=10000, unless otherwise stated.
3 Task: difficulty
The success of deep learning is marked by its superior performance at solving difficult tasks, like
large-scale visual classification and question answering. For evaluating the capability of algorithms,
2
Under review as a conference paper at ICLR 2021
Task
Figure 1: Information content, model perfor-
mance and model confidence on five tasks.
label: 3	label: 3
Image
noise
label: 3 issi label: 3
label: 3
Label
noise
massart
noise
label: 4

ju
3
Figure 2: Injecting three kinds of noise on
MNIST.
itis important to understand the difficulty of the tasks and the datasets we have. Human definitions of
difficulty are mostly subjective, for example, difficulty scoring, task performance, and time needed
to complete the task (Ionescu et al., 2016).
Several metrics have been proposed for measuring the difficulty of a task. Many focus on the com-
plexity of the data. For images, difficulty is linked with image quality, objectiveness, and clutter-ness
(Ionescu et al., 2016). For text, the relevant factors include perplexity, reading ease, and diversity
(Collins et al., 2018). For general real-valued data, one can measure the distribution of feature
value, class separability, and the geometry of class manifolds (Ho & Basu, 2002). There are also
other factors that affect the difficulty of a task, for example, class balance and class ambiguity.
As pointed out by Collins et al. (2018), these characteristics all describe some aspect of the task,
and difficulty cannot be described by any one of them alone. Complexity measures of input data
can sometimes have little impact on task difficulty. An important line of work defines the difficulty
of a classification problem by the complexity of the optimal decision boundary (Ho & Basu, 2002).
The complexity of the boundary can be characterized by its Kolmogorov complexity or its minimum
description length. This coincidences with the idea of the model information. While for simple tasks,
one can directly characterize the geometry of the decision boundary, for complex tasks that use a
neural network to model the decision boundary, the description length can only be approximated.
With description length, the difficulty of a task can be interpreted as the amount of information
needed to solve the task. This amount of information is at most K(f), which is the complexity of
the input-output relationship of the task. We measure the model information L(MD, D) of a trained
model, and use it as a measure of the information content in dataset D. In Figure 1, we use five
10-class classification tasks to illustrate the idea. The results show that the datasets vary greatly in
information content: For classifying digits, SVHN (Netzer et al., 2011) requires more information
than MNIST (LeCun et al., 1998) or EMNIST (Cohen et al., 2017), meaning classifying digits in
street view is more difficult than in standardized MNIST images. CIFAR-10 (Krizhevsky & Hinton,
2009) is even richer in information content and therefore more difficult, as natural objects are more
complex than digits.
There is also a general trend that the more difficult the task, the lower the performance and the
confidence of the model. However, performance and confidence are not only determined by task
complexity, they are also affected by noise present in the task.
Next we want to disentangle the two factors: task complexity (measured by information) and noise.
We apply three kinds of noise on MNIST to represent different forms of noise present in a dataset
(Figure 2). There are two kinds of image noise: missing pixels, where a small random block of
pixels is missing in the image, missing objects, where a large portion of the digit is missing, and one
form of label noise: massart noise, where a small ratio of labels are replaced by random labels. To
see how different kinds of noise affect model behavior, we plot model information, model accuracy,
and confidence in Figure 3.
What we observe is that information complexity and noise are two independent dimensions that
affect performance on a task. Pixels missing from the image will make the task more complex
and require more information to correctly classify the digits. This is because one must learn more
features corresponding to each digit, as any of the features could be absent at any time. However, the
3
Under review as a conference paper at ICLR 2021
Task information
Model performance
-→- Image noise: missing pixels
Image noise: missing objects
label noise： massart noise
3500-
3000-
2500-
2000-
1500-
1000-
0.00	0.05	0.10	0.15	0.20
Ratio of noisy examples
1.10-
1.05-
AiQO-
u
e
ħ 0.95-
u
y
« 0.90-
0.85-
0.80-
0.00	0.05	0.10	0.15	0.20
Ratio of noisy examples
Model confidence
—image noise： missing pixels
-∙- image noise： missing objects
label noise： massart noise
1.10-
1.05 -
8 i∙oo-
U
(υ
≡ 0.95 -
M—
⊂
S 0.90-
0.85 -
0.80-
0.00	0.05	0.10	0.15	0.20
Ratio of noisy examples
—image noise： missing pixels
image noise： missing objects
label noise： massart noise
Figure 3: Task information, model performance, and model confidence, with different kinds and
varying levels of noise.
model can still perform well after learning enough information about the task. Missing objects and
wrong labels, on the other hand, are pure noise that does not affect the information complexity of
the task. They effectively make some of the examples uninformative and simply confusing, which
makes the model less confident in its predictions.
4 Data: domain similarity
Understanding the data is a fundamental element of understanding learning. Labeled data for deep
learning comes from many sources and domains, and people seek to train models that can adapt
well to a new domain, generalize to unseen domains, or be independent to domains (Gulrajani &
Lopez-Paz, 2020). However, few attempts to understand the data before using them to train the
model. Similarity is a fundamental concept for understanding the relationship between different
data. In deep learning, we are more concerned about similarity on semantic level. Semantically
similar images can sometimes differ very much in the RGB space.
The informational similarity measure proposed by Lin (1998) measures the similarity between A
and B by the ratio of the amount of information needed to describe the commonality between A and
B and the amount information needed to describe A together with B:
I (common(A, B))
sim(A,B)= ——------：一：——/ ： 「、、
I (description(A, B ))
K f)- K f IfA)
KfAfB )
(3)
It is the Jaccard similarity between A and B measured with information. A property of the in-
formational similarity measure is universality: it does not depend on a particular representation
(or modeling) of A and B. We can use model information to estimate the information terms in
(3), and turn the similarity measure into (4): (using approximations L(MA, A) ≈ K(fA) and
L(MA, B) ≈ K(fB) - K(fB IfA)), see discussion on the symmetricity of S in Appendix A.3)
S(A, B)
______LM B)______
LM A) + L(MB ,B) - LM B)
(4)
Another benefit of information similarity is that the similarity can be measured with respect to one
side, resulting in a unidirectional similarity measure:
SUni(A B) = I(Common(A,B) = L(MA,B)
,	I (desCriPtiOn(B)	L(MB)By
(5)
Note that generally Suni(A, B) 6= Suni(B, A). The unidirectional similarity measure is useful for
depicting the relationship between A and B. For example, if Suni(A, B) < 1 and Suni(B, A) = 1,
then one can tell that A is a subset of B.
We perform experiments on two commonly-used domain adaptation datasets: Office-31 (Saenko
et al., 2010) and Office-Home (Venkateswara et al., 2017). They each have 3 and 4 different do-
mains for images of the same set of classes (Figure 4). We calculate information similarity S and
unidirectional information similarity Suni for each pair of domains in each dataset. The baseline
we use for comparison is the distance of first-order and second-order statistics in the feature space
4
Under review as a conference paper at ICLR 2021
amazon
dslr
webcam
Office-31
art
clipart
product
real-world
Office-Home
Figure 4: Image domains in Office-31 and Office-Home.
produced by ResNet-56 (He et al., 2016b) pretrained on ImageNet (Russakovsky et al., 2015) (see
Appendix for details). This is inspired by the Maximum Mean Discrepancy (MMD) (Gretton et al.,
2006) method for comparing data distributions. In domain adaptation literature, one often aims to
minimize such distances to match domains (Pan et al., 2009).
As illustrated in Figure 5, information similarity measure give a similarity score between 0 and
1, which is intuitive and comparable across datasets. S and Suni largely agree with feature
distances, but the latter is not comparable across datasets. For instance, dslr and webcam in
Office-31 are much more similar than any other pair in the two datasets, which is reflected in
S but not in feature distance. S is also more faithful within dataset for corresponding to vi-
sual similarities. Unidirectional information similarity Suni gives extra information, for instance,
Suni (amazon, dslr) > Suni (dslr, amazon) shows that amazon contains more distinct informa-
tion than dslr. This can be explained that each category in amazon has images for 90 different
objects, while each category in dslr only has images for 5 objects.
5"n∕
1.0
0.66
amazon
0.8
dslr- 0.38
0.42 0.4G
1.00
0.92
-0.6
0.98 1.00
1.0
-0.8
-0.4
O.S
clipart - 0.18
0.4
target
product - 0.20
-0.6
06
amazon
dslr 0.47
webcam - 0.38
webcam - 0.41
8」nos
real-
world
Figure 5: Pairwise domain similarity on Office-31 (top) and Office-Home (bottom). S and Suni are
information similarities, D1 and D2 are first-order and second-order feature distances.

5 Model structure: capacity
Neural networks are powerful learners that can scale to learn extremely large datasets given enough
neurons and layers. It largely remains a mystery how to qualify the true capacity of a neural network.
Capacity can be defined in different fashion, for example, Collins et al. (2017) measure capacity
by the number of bits of data the network can remember, while Baldi & Vershynin (2019) define
capacity as the number of distinct functions a network can represent. We can define the information
capacity of a network M as the maximum amount of information a model can hold for a given task
T , directly using model information to measure:
C(M) = mD aTx L(MD, D).
(6)
5
Under review as a conference paper at ICLR 2021
To illustrate the information capacity of a model, we experiment with models from small to large
(4 configurations of ResNet-11: large, standard, small, and tiny. See Appendix for details), and
datasets of varying complexity (subsets of TinyImageNet1 with 50, 100, 150 and 200 classes.). We
measure the model information of every model trained on each task, and plot results in Figure 6.
9.5-
Number of classes
(a)
le5
-1.4
Ie3
9.0-
7 85-
8.0-
7.5-
7.0
Capacity
Number of
parameters
-1.2
-1.0
0.8
-0.6
0.2
0.0
tiny small standard
Network size
-0.4 ⅛
(b)	(c)
Figure 6:	Measuring capacity using information. (a) the amount of information a model can store
is capped by its capacity. (b) increase in capacity is correlated with an increase in the number of
parameters. (c) a task has an inherent amount of information content.
The first thing We notice in Figure 6.a is that with increasing complexity of the task, model infor-
mation saturates at a certain point for each model. This displays the capacity of a model (in dotted
lines). Larger models can store more information and saturate later at larger tasks. Figure 6.b plots
the increase of information capacity w.r.t. the increase in the number of parameters. Information
capacity roughly increases with the number of parameters in the model, which agrees with the ob-
servation in Collins et al. (2017).
The same observation also apply to datasets: each dataset has a definite amount of information
content. Larger models can learn more information from the dataset, but can hardly learn more than
this amount. This is shown in Figure 6.c, and dataset information is indicated by the dotted lines.
6 Model parameter： ablation
Next we turn to the analysis of parameters within a network. A common way to understand com-
ponents in a network is to perform ablations: remove some units or structural components from
the network, and compare the performance before and after ablation (Meyes et al., 2019). A larger
performance drop signifies larger importance of the ablated component for performing on the task.
However, there are several caveats to this approach: firstly, ablations cannot reveal the true con-
tribution of a component “in vivo.” If a network layer is removed and the model re-trained, the
functionalities of the layer can get substituted by other layers. In this case, ablations fail to reveal
the contribution of the layer in the original network. Secondly, ablation cannot be performed on
components vital to model performance, for example, if residue connections are removed from a
very deep CNN, the network can fail to train. It would be unreasonable to therefore conclude that
residue connections contribute 100% to the model.
Here we propose to use model information to measure the contribution of a network component.
The information I in a network component C can be measure in the following fashion (where MC):
I(C) = L(Md,D)- L(MD,D).	(7)
To calculate the information in c, we reset all parameters within this component to their initialized
value before training. Then we measure the model information of this ablated model MD. The
difference of information in MD and MD is the information content in the parameters of component
C. In effect, we are performing information ablation: only information stored in parameters is
ablated, the model structure is kept intact.
1https://tiny-imagenet.herokuapp.com
6
Under review as a conference paper at ICLR 2021
Imaqe 一 conv 3x3 一	block0 - block1 一
Image I______16 I I_16, /2 _I I__________16
block2
32,/2
「block3-1 厂block4-1 厂block5-1 I 於	一 Li归
I____32	64 /2	64	fc	LogitS
. conv 1x1	_
(residue)
conv 1x1
(before bottleneck)
Conv 3x3
(bottleneck)
conv 1x1
(after bottleneck)
(a)
(before bn)
(bn) (after bn) (residue)
-1.4
-1.2
-1.0
-0.8
-0.6
-0.4
0.2
-0.0
N
Ablated layers
Ablated block
(b)	(c)
Figure 7:	Information ablation. (a) Structure of a variant of ResNet model in (He et al., 2016a).
(b) Information ablation by each block in the model. (c) Information ablation by different types of
convolution layers. ‘bn’ stands for the bottleneck layer.
Information ablation helps us uncover how the information in each component contributes to the
whole network, or in other words, how information in a network is stored by its components. In
Figure 7, we perform information ablation on a ResNet model as an example. The model consists
of a series of residue blocks, and each block contains several convolutional layers. Figure 7.b shows
the information in each block, as well as the information per model parameter (“density”). There
are several observations: blocks in the middle (block2 - 5) contribute the most information, blocks
near input have larger information density, and blocks that reduce spatial resolution (with ‘/2’ in
notation) contains more information than blocks that preserves resolution. The last phenomenon is
likely because convolution layers in downsampling blocks do more job of combining smaller object
parts into larger parts, thus having more knowledge about the constitution of objects.
Figure 7(c) illustrates something unique to the information ablation method: this time we measure
information contribution by four different kinds of convolution layers in each and every block. We
found out that the majority of information resides in the 3x3 bottleneck layer, which is where most
spatial feature transformations take place. Convolution layers on the residue path contain surpris-
ingly little information, despite having roughly the same number of parameters as the layers before
and after the bottleneck. This indicates that residue layers serve more of a structural functionality to
ease training, rather than doing substantial information processing.
7	Algorithm: distillation
Knowledge distillation is a method to transfer knowledge from larger teacher models to smaller
student models, which can result in better performing compact models (Hinton et al., 2015). Student
models are trained with output probabilities of a teacher model in addition to the true labels. It is
speculated that the student model can benefit from “dark knowledge” in teacher’s predictions. If
“dark knowledge” means the presence of extra information, with proper tools, we can verify the
existence of extra information provided by the teacher model and quantify how much information is
transferred in distillation.
We could measure the information transferred from teacher to student by the following formula:
I = L(MD+DT , D) - L(MD, D).	(8)
7
Under review as a conference paper at ICLR 2021
Next iteration:
Dt+1=Dt + {xt+1,yt + 1}
Dt W-----------------------
(a)
p(yt+i|xt+i)
Encode
Student
Teacher model
(b)
Coefficient a
(c)
Figure 8: (a) illustration of the training process in synced distillation.，indicates training Proce-
dures with information flow on the arrow directions. (b) information gain of students from different
teachers. (c) information gain of the student when trained with different distillation coefficient α.
DT is teacher’s predictions on dataset D. However L(MD+DT , D) cannot be directly measured
with online coding because DT already contains much information about the labels, and the student
model having access to DT can encode D with minimal effort by merely looking at DT. We propose
a method called synced distillation to calculate online codelength of dataset D with the presence of
a teacher model.
The process of online coding gradually increases the size of the dataset D and updates the model
accordingly. In synced distillation, we update the teacher and the student model in synchronization
(Figure 8.a): in each iteration, we train the teacher with a subset Dt, then using Dt and teacher’s
predictions to train the student via distillation. The student model is then used to encode yt+1. In the
end, the student will be the same as training with conventional distillation. But during the process,
the teacher never leaks any information about future examples to the student. This enables the
student to generate a valid encoding of D, which can then be used to calculate model information.
As pointed out in Zhang et al. (2020), model information can help illustrate a different aspect of
learning dynamics than model performance. For the simple 5-layer CNN model that we use as
student, distillation with different teachers and distillation coefficient α yields similar performance
(Figure 8.b and c). However, from measuring the information gain I, ResNet-56 and ResNet-101
transfer more information to the student than ResNet-20. This helps the student reach lower cross-
entropy with fewer examples, although the final performance is similar. A larger distillation coeffi-
cient also increases information transfer. This shows that as the weighting of the KL-divergence term
increase in the student’s loss function, the student learns more from the teacher. Observations such
as this can help analyze and design better algorithms or understand why algorithms like knowledge
distillation lead to performance gain (Yim et al., 2017).
8	Discussion
Currently, methods for estimating the description length of neural network models are still quite
preliminary, lacking through analysis and guarantees of their efficiency. Using prequential coding
to estimate model information or Kolmogorov complexity also introduces dependency on model
architecture and training procedure, for example, dropout, batch normalization, and SGD optimizer
will all affect the model information estimations. Because prequential codelength is always larger
8
Under review as a conference paper at ICLR 2021
than Kolmogorov complexity, we can optimize the training hyperparameters to achieve as lower
codelength as possible, which makes the estimation tighter.
When applying model information to perform analysis, as we did in this paper, it is also necessary
to vary only one variable at a time. Because L(M, D) is a function of both the model and the
dataset, when analyzing models, the dataset needs to be fixed for codelengths to be comparable.
Similarly when datasets are the center of interest the model needs to be fixed. This also introduces
dependency. For instance, theoretically we can define difficulty by K(f), but empirically when
estimating K(f) using L(MD, D), dependency on model architecture becomes inevitable. One can
choose any adequate model architecture for training on the task, and fix that to compare the difficulty
of tasks.
Nonetheless, we demonstrate that model information could be a powerful analysis tool in deep
learning. To summarize, the key observations from our experiments are:
•	Model information allows us to quantify properties such as difficulty, capacity, and simi-
larity, especially for complex data and deep models, in a consistent fashion.
•	Model information can help analyze and understand learning in deep networks by showing
how information is transferred and stored.
•	Such a tool is widely applicable to many kinds of problems because it does not depend on
specific models or learning algorithms.
We hope problems discussed in this work serve as examples of the versatility of an informational
perspective in investigating neural network learning.
References
Pierre Baldi and Roman Vershynin. The capacity of feedforward neural networks. Neural networks,
116:288-311,2019.
Leonard Blier and Yann Ollivier. The description length of deep learning models. In Samy Bengio,
Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, pp. 2220-2230, 2018.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp.
2921-2926. IEEE, 2017.
Edward Collins, Nikolai Rozanov, and Bingbing Zhang. Evolutionary data measures: Understanding
the difficulty of text classification tasks. In Anna Korhonen and Ivan Titov (eds.), Proceedings of
the 22nd Conference on Computational Natural Language Learning, CoNLL 2018, pp. 380-391.
Association for Computational Linguistics, 2018.
Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent
neural networks. In 5th International Conference on Learning Representations, ICLR, 2017.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Scholkopf, and Alexander J.
Smola. A kernel method for the two-sample-problem. In Bernhard Scholkopf, John C. Platt,
and Thomas Hofmann (eds.), Proceedings of the Twentieth Annual Conference on Neural Infor-
mation Processing Systems, 2006, pp. 513-520. MIT Press, 2006.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint
arXiv:2007.01434, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep resid-
ual networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer
Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-
14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pp. 630-
645. Springer, 2016a. doi: 10.1007∕978-3-319-46493-0∖,38. URL https://doi.org/10.
1007/978-3-319-46493-0_38.
9
Under review as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016b.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
NIPS Deep Learning and Representation Learning Workshop, 2015.
Tin Kam Ho and Mitra Basu. Complexity measures of supervised classification problems. IEEE
Trans. Pattern Anal. Mach. Intell., 24(3):289-300, 2002.
Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P. Papadopoulos,
and Vittorio Ferrari. How hard can it be? estimating the difficulty of visual search in an image.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, pp. 2157-
2166. IEEE Computer Society, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Dekang Lin. An information-theoretic definition of similarity. In Jude W. Shavlik (ed.), Proceedings
of the Fifteenth International Conference on Machine Learning (ICML 1998), Madison, Wiscon-
sin, USA, July 24-27, 1998, pp. 296-304. Morgan Kaufmann, 1998.
Zachary C. Lipton and Jacob Steinhardt. Troubling trends in machine learning scholarship. ACM
Queue, 17(1):80, 2019.
Richard Meyes, Melanie Lu, Constantin Waubert de Puiseau, and Tobias Meisen. Ablation studies
in artificial neural networks. arXiv preprint arXiv:1901.08644, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer
component analysis. In Craig Boutilier (ed.), IJCAI 2009, Proceedings of the 21st International
Joint Conference on Artificial Intelligence, Pasadena, California, USA, July 11-17, 2009, pp.
1187-1192, 2009.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
domains. In European conference on computer vision, pp. 213-226. Springer, 2010.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In (IEEE) Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.
Elena Voita and Ivan Titov. Information-theoretic probing with minimum description length. arXiv
preprint arXiv:2003.12298, 2020.
Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2017, pp. 7130-7138. IEEE Computer Society, 2017.
Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski,
Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evalu-
ating general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.
Xiao Zhang, Xingjian Li, Dejing Dou, and Ji Wu. Measuring information transfer in neural net-
works. arXiv preprint arXiv:2009.07624, 2020.
10
Under review as a conference paper at ICLR 2021
A Experiments
A.1 General experiment settings
For experiments in this paper, we use (2) to estimate model information and set the encoding set
size k=10000 (except for experiments on Office-31 and Office-Home, see below). All models are
trained using Adam optimizer and early-stopping on the validation set.
A.2 Difficulty
Statistics of noise injection experiments are given in Table 2-4. We trained ResNet-56 models on
MNIST with different kinds of injected noise. Model confidence is defined as the margin between
the probability of the top and the second class predicted by the model:
conf idence
c1
c2
Ex[pc1(x) - pc2 (x)]
arg max pk (x)
k
arg max pk (x)
k6=c1
(9)
(10)
(11)
Table 2: Model statistics on MNIST with image noise (missing pixels)
Noise level	0	0.05	0.1	0.2
L(MD,D)	1559	1718	2120	2719
Accuracy	0.996	0.992	0.988	0.981
Confidence	0.994	0.989	0.985	0.985
Table 3: Model statistics on MNIST with image noise (missing objects)
Noise level	0	0.05	0.1	0.2
L(MD,D)	1559	1489	1560	1296
Accuracy	0.996	0.953	0.909	0.865
Confidence	0.994	0.946	0.899	0.848
Table 4: Model statistics on MNIST with label noise (massart noise)
Noise level	0	0.05	0.1	0.2
L(MD,D)	1559	1833	1735	1605
Accuracy	0.996	0.949	0.908	0.867
Confidence	0.994	0.973	0.941	0.908
A.3 Domain similarity
We use ResNet-18 model pre-trained on ImageNet as the initial model, then finetune on source
dataset S and measure L(MS, T) on target dataset T. Pairwise codelength representing L(MT, T)-
L(MS , T ) are given in Table 5-6. Diagonal values are L(MT , T ).
L(MT, T) - L(MS, T) can be directly calculated using (2):
L(MT,T)-L(MS,T) =LpMreSq(y1:k|x1:k)-LpMreTq(y1:k|x1:k)	(12)
This assumes that information from ImageNet is known and is not included in calculating similari-
ties. It is possible to use random models as initialization, but as the number of examples is too small
for some domains in Office-31, we found that using pre-trained models gives more stable estimation
of codelength.
11
Under review as a conference paper at ICLR 2021
Also, because the smallest domain in Office-31 and Office-Home only have 399 and 1942 training
examples respectively, we use about half of the dataset, i.e., k=200 for Office-31 and k=1000 for
Office-Home to estimate L in (2).
Theoretically, the similarity measure in (3) and (4) is symmetric:
S(A, B)
I (common(A, B))
I (description(A, B))
K (fB )- K (fB |fA)
K(fA,fB )
K(fA)- K(fA∣fB)
K (fA,fB )
S(B, A)
which is because K(fA) + K(fB|fA) = K(fB) + K(fA|fB) = K(fA,fB). Empirically, when
using L(MA, B) as an approximation of K(fB) - K(fB |fA), usually S(A, B) 6= S(B, A) because
L(MA, B) and L(MB, A) doe not equal exactly. In this paper we report empirical results using (4).
Table 5: L(MT, T) - L(MS, T) on domains in Office-31
Source \ Target	amazon	dslr	webcam
amazon	263.0	124.7	106.3
dslr	139.4	318.4	26.6
webcam	153.9	6.6	316.6
Table 6: L(MT, T) - L(MS, T) on domains in Office-Home
Source \ Target	art	clipart	product	reaLworld
art	2776.4	1076.2	764.8	451.9
clipart	2001.2	1630.3	756.8	617.5
product	2047.6	1144.8	1480.2	466.0
real_world	1609.2	879.2	411.9	1420.2
The first-order and second-order feature distance d1 and d2 are defined as:
d1 = ||Ex[fA(x)] -Ex[fB(x)]||2	(13)
d2 = ||Covx[fA(x)] - Covx[fB(x)]||F	(14)
where fA(x) and fB (x) are the representations of x produced by model trained on domain A and
B. F stands for Frobenius norm.
A.4 Capacity
To show the capacity of a model, we need to saturate a model by information in the training set.
Therefore we choose to use ResNet-11, the smallest of ResNet configurations, and use four different
layer width in ResNet-11, as follows: Large: [16, 48, 96], Standard: [16, 32, 64], Small: [16, 24,
32], Tiny: [16, 16, 24]. For example, [16, 48, 96] means the layer width is 16, 48, and 96 in the
first, second, and the third residue block. The datasets we use are subsets of Tiny-ImageNet, each
containing 50,100,150, and 200 classes. The number of examples in each dataset is fixed at 12500.
L(MD, D) for models of different size, and datasets of different complexity, are listed in Table 7.
Table 7: L(MD, D) for model and dataset of different size
Dataset \ Model size	Large	Standard	Small	Tiny
50 class	7687.0	7140.0	7358.2	6970.1
100 class	8907.8	8496.2	8343.9	7492.2
150 class	9540.7	9322.1	8086.6	7777.0
200 class	10264.0	9410.7	8170.3	7448.8
Number of parameters	264657	124433	49905	29457
12
Under review as a conference paper at ICLR 2021
A.5 Ablation
We train a ResNet-20 model (referred to as MD) on CIFAR-10, reset the parameters of some net-
work component C (this ablated model is referred to as MD) and measure the difference of model
information by:
I(C) = L(Md,D) - L(MD,D)
=LMq (yi：k|x1：k) -LMD (yi：k |x1:k)
(15)
(16)
Ablation results for different network layers are shown in Table 8-9.
Table 8: Information I in different kinds of convolutional layers
Model component C	I (C)	# param in C	I per param
conv1x1 (before bottleneck)	1985	32000	0.062
conv3x3 (bottleneck)	4243	96768	0.044
conv1x1 (after bottleneck)	1231	43008	0.029
conv1x1 (residue)	467	41984	0.011
Table 9: Information I in different residue blocks
Model component C	I (C)	# param in C	I per param
conv3x3	379	432	0.877
block0	751	4704	0.160
block1	485	4544	0.107
block2	2088	23808	0.088
block3	1030	17792	0.058
block4	2137	94720	0.023
block5	1256	70400	0.018
fc	167	2570	0.065
all	5900	219482	0.027
A.6 Distillation
In distillation experiments, we use LeNet-5 as student and ResNet-20, ResNet-56, and ResNet-101
as teachers. The loss when training student model is given in (17).
L = αLKD + (1 - α)Lce
epk (x)/T
、∙ epj(X)/T
pk(x)
(17)
(18)
(19)
We also experiment with different coefficient α, to control the influence of teacher models. Results
are shown in Table 10-11. We observe that the student’s information gain plateaus after α = 0.2,
which kind of agrees with the conventional choose of α = 0.1 in knowledge distillation.
13
Under review as a conference paper at ICLR 2021
Table 10: Distillation experiments with different teacher model
Teacher	Teacher accuracy	Student accuracy	I
None	-	82.7	0.0
ResNet-20	87.07	84.0	452.6
ResNet-56	88.78	83.8	1246.2
ResNet-101	89.69	84.2	1277.7
Table 11: Distillation experiments with different coefficient α
α	T	Accuracy	I
0	4	82.72	0.0
0.05	4	83.93	938.1
0.1	4	83.73	1040.9
0.2	4	83.85	1178.9
0.3	4	83.77	1188.6
0.5	4	83.64	1214.4
0.7	4	83.83	1246.2
0.9	4	83.94	1177.5
14