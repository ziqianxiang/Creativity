Under review as a conference paper at ICLR 2021
Double Q-learning: New Analysis and
Sharper Finite-time B ound
Anonymous authors
Paper under double-blind review
Ab stract
Double Q-learning (Hasselt, 2010) has gained significant success in practice due
to its effectiveness in overcoming the overestimation issue of Q-learning. How-
ever, theoretical understanding of double Q-learning is rather limited and the only
existing finite-time analysis was recently established in Xiong et al. (2020) under
a polynomial learning rate. This paper analyzes the more challenging case with
a rescaled linear/constant learning rate for which the previous method does not
appear to be applicable. We develop new analytical tools that achieve an order-
level better finite-time convergence rate than the previously established result.
Specifically, we show that synchronous double Q-learning attains an -accurate
global optimum with a time complexity of Ω ((二6),and the asynchronous
algorithm attains a time complexity of Ω (笠二广三),where D is the cardinality
of the state-action space, γ is the discount factor, and L is a parameter related
to the sampling strategy for asynchronous double Q-learning. These results im-
prove the order-level dependence of the convergence rate on all major parameters
(, 1 - γ, D, L) provided in Xiong et al. (2020). The new analysis in this pa-
per presents a more direct and succinct approach for characterizing the finite-time
convergence rate of double Q-learning.
1 Introduction
Double Q-learning proposed in Hasselt (2010) is a widely used model-free reinforcement learn-
ing (RL) algorithm in practice for searching for an optimal policy (Zhang et al., 2018a;b; Hessel
et al., 2018). Compared to the vanilla Q-learning proposed in Watkins & Dayan (1992), double
Q-learning uses two Q-estimators with their roles randomly selected at each iteration, respectively
for estimating the maximum Q-function value and updating the Q-function. In this way, the over-
estimation of the action-value function in vanilla Q-learning can be effectively mitigated, especially
when the reward is random or prone to errors (Hasselt, 2010; Hasselt et al., 2016; Xiong et al.,
2020). Moreover, double Q-learning has been shown to have the desired performance in both finite
state-action setting (Hasselt, 2010) and infinite setting (Hasselt et al., 2016) where it successfully
improved the performance of deep Q-network (DQN), and thus inspired many variants (Zhang et al.,
2017; Abed-alguni & Ottom, 2018) subsequently.
In parallel to its empirical success in practice, the theoretical convergence properties of double Q-
learning has also been explored. Its asymptotic convergence was first established in Hasselt (2010).
The asymptotic mean-square error for double Q-learning was studied in Weng et al. (2020c) under
the assumption that the algorithm converges to a unique optimal policy. Furthermore, in Xiong et al.
(2020), the finite-time convergence rate has been established for double Q-learning with a polyno-
mial learning rate α = 1∕tω, ω ∈ (0,1). Under such a choice for the learning rate, they showed
that double Q-learning attains an -accurate optimal Q-function at a time complexity approaching
to but never reaching Ω(1)at the cost of an asymptotically large exponent on ɪ-Y. However, a
polynomial learning rate typically does not offer the best possible convergence rate, as having been
shown for RL algorithms that a so-called rescaled linear learning rate (with a form of ɑt = ^a^)
and a constant learning rate achieve a better convergence rate (Bhandari et al., 2018; Wainwright,
2019a;b; Chen et al., 2020; Qu & Wierman, 2020). Therefore, a natural question arises as follows:
1
Under review as a conference paper at ICLR 2021
Can a rescaled linear learning rate or a constant learning rate improve the convergence rate of
double Q-learning order-wisely? If yes, does it also improve the dependence of the convergence
rate on other important parameters of the Markov decision process (MDP) such as the discount
factor and the cardinality of the state and action spaces?
The answer to the above question does not follow immediately from Xiong et al. (2020), because
the finite-time analysis framework in Xiong et al. (2020) does not handle such learning rates to yield
a desirable result. This paper develops a novel analysis approach and provides affirmative answers
to the above question.
1.1	Our contributions
This paper establishes sharper finite-time bounds for double Q-learning with a rescaled lin-
ear/constant learning rate, which are orderwisely better than the existing bounds in Xiong et al.
(2020). We devise a different analysis approach from that in Xiong et al. (2020), which is more
capable of handling variants of double Q-learning.
For synchronous double Q-learning, where all state-action pairs are visited at each iteration,
We apply a rescaled linear learning rate at = 3+(1—7冲 and show that the algorithm can attain
an e-accurate global optimum with a time complexity of Ω Q-,where Y is the discount
factor and D = |S ||A| is the cardinality of the finite state-action space. As a comparison, for
the e dominated regime (with relatively small γ), our result attains an e-accurate optimal Q-
function with a time complexity Ω(*), whereas the result in Xiong et al. (2020) (see Table 1)
does not exactly reach Ω(1)and its approaching to such an order (η := 1 - ω → 0) is at an
additional cost of an asymptotically large exponent on ɪ-ʒ;. For 1 - Y dominated regime, our
result improves on that in Xiong et al. (2020) (which has been optimized in the dependence on
1 - γ in Table 1) by O
• For asynchronous double Q-learning, where only one state-action pair is visited at each iter-
ation, we obtain a time complexity of Ω ((一打尸声),where L is a parameter related to the
sampling strategy in Assumption 1. As illustrated in Table 1, our result improves upon that in
Xiong et al. (2020) order-wisely in terms of its dependence on e and 1 - Y as well as on L by
at least O (L5).
Our analysis takes a different approach from that in Xiong et al. (2020) in order to handle the
rescaled linear/constant learning rate. More specifically, to deal with a pair of nested stochastic
approximation (SA) recursions, we directly establish the dependence bound of the error dynamics
(of the outer SA) between the Q-estimator and the global optimum on the error propagation (of the
inner SA) between the two Q-estimators. Then we develop a bound on the inner SA, integrate it
into that on the outer SA as a noise term, and establish the final convergence bound. This is a very
different yet more direct approach than that in Xiong et al. (2020), the latter of which captures the
blockwise convergence by constructing two complicated block-wisely decreasing bounds for the
two SAs. The sharpness of the bound also requires careful selection of the rescaled learning rates
and proper usage of their properties.
1.2	Related work
Theory on double Q-learning: Double Q-learning was proposed and proved to converge asymptot-
ically in Hasselt (2010). In Weng et al. (2020c), the authors explored the properties of mean-square
errors for double Q-learning both in the tabular case and with linear function approximation, under
the assumption that a unique optimal policy exists and the algorithm can converge. The most rel-
evant work to this paper is Xiong et al. (2020), which established the first finite-time convergence
rate for tabular double Q-learning with a polynomial learning rate. This paper provides sharper
finite-time convergence bounds for double Q-learning, which requires a different analysis approach.
Tabular Q-learning and convergence under various learning rates: Proposed in Watkins &
Dayan (1992) under finite state-action space, Q-learning has aroused great interest in its theoretical
study. Its asymptotic convergence has been established in Tsitsiklis (1994); Jaakkola et al. (1994);
2
Under review as a conference paper at ICLR 2021
Table 1: Comparison of time complexity for (a)synchronous double Q-learning.
The choices ω → 1, ω = 7, and ω = 3 respectively optimize the dependence of time complexity
on , 1 - γ, and L in Xiong et al. (2020). We denote a ∨ b = max{a, b}, a ∧ b = min{a, b}.
SynCDQ	Stepsize	Time complexity			
Xiong et al. (2020)	tω, ω ∈ (3, i)	ω=1-η→1		ω = 6/7	,	..	
		ω (露 ∨ (1-γ )η)	一		ω (σ⅛7 (白∨ (lnι⅜)))	
This work	3 3+(1-γ)t	ω (±)		Ω (α ∖7 2] ∖(1-γ)7e2/		
ASynCDQ	Stepsize	Time complexity			
Xiong et al. (2020)	tω ,ω ∈- (3, i)	ω=1-η→1	ω = 6/7 ,	―		ω = 2/3
		ω (备 ∨'(ι⅛η)	C ((⅛7 (f ∨(ln 合)))		Ω ( L6Qn L)1.5 ) ω((i-γ)%3 )
This work	e2(1-γ)6∧ 1		ω （白）	ω ((1-Y)7e2)		ω ( (1-L)7e2 厂
Borkar & Meyn (2000); Melo (2001); Lee & He (2019) by requiring the learning rates to satisfy
Pt∞=0 αt = ∞ and Pt∞=0 αt2 < ∞. Another line of research focuses on the finite-time analysis of
Q-learning under different choices of the learning rates. Szepesvari (1998) captured the first con-
vergence rate of Q-learning using a linear learning rate (i.e., αt
t). Under similar learning rates,
Even-Dar & Mansour (2003) provided finite-time results for both synchronous and asynchronous
Q-learning with a convergence rate being exponentially slow as a function of -γ. Another PoP-
ular choice is the polynomial learning rate which has been studied for synchronous Q-learning in
Wainwright (2019b) and for both synchronous/asynchronous Q-learning in Even-Dar & Mansour
(2003). With this learning rate, however, the convergence rate still has a gaP with the lower bound
of O( √T) (Azar et al., 2013). To handle this, a more sophisticated rescaled linear learning rate was
introduced for synchronous Q-learning (Wainwright, 2019b; Chen et al., 2020) and asynchronous
Q-learning (Qu & Wierman, 2020), and thus yields a better convergence rate. The finite-time bounds
for Q-learning were also given with constant stepsizes (Beck & Srikant, 2012; Chen et al., 2020; Li
et al., 2020). In this paper, we focus on the rescaled linear/constant learning rate and obtain sharper
finite-time bounds for double Q-learning.
Q-learning with function approximation: When the state-action space is considerably large or
even infinite, the Q-function is usually approximated by a class of parameterized functions. In such
a case, Q-learning has been shown not to converge in general (Baird, 1995). Strong assumptions
are typically needed to establish the convergence of Q-learning with linear function approxima-
tion (Bertsekas & Tsitsiklis, 1996; Melo et al., 2008; Zou et al., 2019; Chen et al., 2019; Du et al.,
2019; Yang & Wang, 2019; Jia et al., 2019; Weng et al., 2020a;b) or neural network approxima-
tion (Cai et al., 2019; Xu & Gu, 2019). The convergence analysis of double Q-learning with function
approximation raises new technical challenges and can be an interesting topic for future study.
2 Preliminaries on double Q-learning
We consider a Markov decision process (MDP) over a finite state space S and a finite action space
A with the total cardinality given by D := |S ||A|. The transition kernel of the MDP is given by
P : S × A × S → [0,1] denoted as P(∙∣s, a). We denote the random reward function at time t
as Rt : S × A × S → [0, Rmax], with E[Rt(s, a, s0)] = R1a. A policy π := π(∙∣s) captures the
conditional probability distribution over the action space given state s ∈ S. For a policy π, we define
Q-function Qn ∈ RlSl×lAl as
Qπ(s, a) :=E
∞
X γtRt(st, at, s0t)s1 = s, a1 = a
t=1
where Y ∈ (0,1) is the discount factor, at 〜∏(∙∣st), and St 〜P(∙∣st, at).
(1)
3
Under review as a conference paper at ICLR 2021
Both vanilla Q-learning (Watkins & Dayan, 1992) and double Q-learning (Hasselt, 2010) aim to find
the optimal Q-function Q* which is the unique fixed point of the Bellman operator T (Bertsekas &
Tsitsiklis, 1996) given by
TQ(s, a) = Es0〜P(∙∣s,α) Rsa + Ym0iaχQ(s0, a0)
(2)
Note that the Bellman operator T is γ-contractive which satisfies kTQ - TQ0k ≤ γ kQ - Q0k
under the supremum norm kQk := maxs,a |Q(s, a)|.
The idea of double Q-learning is to keep two Q-tables (i.e., Q-function estimators) QA and QB,
and randomly choose one Q-table to update at each iteration based on the Bellman operator com-
puted from the other Q-table. We next describe synchronous and asynchronous double Q-learning
algorithms in more detail.
Synchronous double Q-learning: Let {βt}t≥1 be a sequence of i.i.d. Bernoulli random variables
satisfying P(βt = 0) = P(βt = 1) = 0.5. At each time t, βt = 0 indicates that QB is updated, and
otherwise QA is updated. The update at time t ≥ 1 can be written in a compact form as,
Jqa+i(S, a) = (I- αtβt)QA(s,a) + αtβt (Rt(s, a, SO) + YQB(s0,a*)),
[qb+i(S, a) = (I- αt(I- Iet)) QB(3 * s, a) + αt(I- Iet)(Rt(S, a, SO) + YQA(s0, b*)),
for all (s, a) ∈ S × A, where so is sampled independently for each (s, a) by so 〜 P(∙∣s, a),
a* = arg maxa∈AQA (SO, a), b* = arg maxa∈AQB (SO, a) and αt is the learning rate. Note that
the rewards for both updates of QtA+1 and QtB+1 are the same copy of Rt .
Asynchronous double Q-learning: Different from synchronous double Q-learning, at each itera-
tion the asynchronous version samples only one state-action pair to update the chosen Q-estimator.
That is, at time t, only the chosen Q-estimator and its value at the sampled state-action pair (St, at)
will be updated. We model this by introducing an indicator function τt(S, a) = l{(st,at)=(s,a)}.
Then the update at time t ≥ 1 of asynchronous double Q-learning can be written compactly as
QtA+1(S, a) = (1 - αtτt(S, a)et)QtA(S, a) + αtτt(S, a)et Rt + YQtB(SO, a*) ,
lQB+l(S,a) = (1 - atTt(S,a)(1 - Βt)) QB(S,a) + αtTt(S,a)(1 - βt) (Rt + YQA(S0,b*)),
(4)
for all (S, a) ∈ S × A, where Rt is evaluated as Rt(S, a, SO).
In the above update rules (3) and (4), at each iteration only one of the two Q-tables is randomly
chosen to be updated. This chosen Q-table generates a greedy optimal action, and the other Q-
table is used for estimating the corresponding Bellman operator (or evaluating the greedy action)
for updating the chosen table. Specifically, if QA is chosen to be updated, we use QA to obtain
the optimal action a* and then estimate the corresponding Bellman operator using QB to update
QA. As shown in Hasselt (2010), E[QB(SO, a*)] is likely smaller than E maxa[QA (SO, a)], where
the expectation is taken over the randomness of the reward for the same (S, a, SO) tuple. Such a
two-estimator framework adopted by double Q-learning can effectively reduce the overestimation.
Without loss of generality, we assume that QA and QB are initialized with the same value (usually
both all-zero tables in practice). For both synchronous and asynchronous double Q-learning, it has
been shown in Xiong et al. (2020) that either Q-estimator is uniformly bounded by RImax throughout
the learning process. Specifically, for either i ∈ {A, B}, We have ∣∣Q"∣ ≤ RI-Yx and IlQt - Q[∣ ≤
2 Rmax
1-Y
Vmax for all t ≥ 1. This boundedness property will be useful in our finite-time analysis.
3 Finite-time Convergence Analysis
In this section, we start with modeling the error dynamics to be nested SAs, following by a conver-
gence result for a general SA that will be applicable for both SAs. Then we provide the finite-time
results for both synchronous and asynchronous double Q-learning. Finally, we sketch the proof of
the main theorem for the synchronous algorithm to help understand the technical proofs.
4
Under review as a conference paper at ICLR 2021
3.1 Characterization of the Error Dynamics
In this subsection, we characterize the (a)synchronous double Q-learning algorithms as a pair of
nested SA recursions, where the outer SA recursion captures the error dynamics between the Q-
estimator and the global optimum Q*, and the inner SA captures the error propagation between the
two Q-estimators which enters into the outer SA as a noise term. Such a characterization enjoys
useful properties that will facilitate the finite-time analysis.
Outer SA: Denote the iteration error by r = QA - Q* and define the empirical Bellman operator
TtQ(s, a) := Rt(s, a, s0) + γmaxa0∈AQ(s0, a0). Then we can have for all t ≥ 1 (see Appendix C),
rt+ι(s, a) = (1 - αt(s, a))rt(s, a) + αt(s, a)(Gt(rt)(s, a) + εt(s, a) + γνt(s0, a*)),	(5)
where εt := TbtQ* - Q*, νt :=QtB-QtA,Gt(rt) :=TbtQtA-TbtQ* =Tbt(rt+Q*)-TbtQ*,andthe
αtβt,	for synchronous version
equivalent learning rate αt(s, a) :=	. Note that it is by
αtβtτt(s, a), for asynchronous version
design that we use the same sampled reward Rt in both TtQ* and TtQtA in the definition of Gt(rt).
These newly introduced variables have several important properties. First of all, the noise term
{εt}t is a sequence of i.i.d. random variables satisfying Eεt = E[TbtQ*] - Q* = TQ* - Q* =
0 ∈ RD. Furthermore, define the span seminorm of Q* as kQ* kspan := max(s,a)∈S×A Q*(s, a) -
min(s,a)∈S×A Q*(s, a). Then it can be shown that (see Appendix C)
kεtk ≤ 2Rmax + γ kQ*kspan:= κ.	(6)
Moreover, it is easy to show that kGt(rt)k ≤ γ krtk, which follows from the contractive property of
the empirical Bellman operator given the same next state. We shall say that Gt is quasi-contractive
in the sense that the γ-contraction inequality only holds with respect to the origin 0.
Inner SA: We further characterize the dynamics of νt = QtB - QtA as an SA recursion (see Ap-
pendix C):
νt+ι(s, a) = (1 - 0^t(s, a))νt(s, a) + αt(s, a)(Ht(νJ(s, a) + μt(s, a)),	(7)
αt,	for synchronous version
for all t ≥ 1 where α^t(s, a) :=	. It has been shown in
αtτt(s, a), for asynchronous version
Xiong et al. (2020) that Ht is quasi-contractive satisfying IlHt(Vt)k ≤ 1++γ ∣∣νtk, and {μt}t≥ι is a
martingale difference sequence with respect to the filtration Ft defined by Fi = {0, Ω} where Ω
denotes the underlying probability space and for t ≥ 2,
F = σ ({sk}, {Rk-1}, βk-1, 2 ≤ k ≤ t) , for synchronous version,	(8)
t	σ (sk, ak, Rk-1, βk-1, 2 ≤ k ≤ t) ,	for asynchronous version,
where we note that for synchronous sampling {sk} and {Rk-1} are the collections of sampled next
states and the sampled rewards for each (s, a)-pair, respectively; while for asynchronous sampling,
the pairs {(sk, ak, sk+1)}k≥2 are consecutive sample transitions from one observed trajectory.
In the sequel, we will provide the finite-time convergence guarantee for (a)synchronous double Q-
learning using the SA recursions described by (5) and (7).
3.2	Finite-time Bound for a General SA
In this subsection, we develop a convergence result for a general SA that will be applicable for both
inner and outer SAs described in Section 3.1.
Consider the following general SA algorithm with the unique fixed point θ* = 0:
θt+1 = (1 - αt)θt + αt (Gt(θt) + εt + γνt) ,	(9)
for all t ≥ 1, where θt ∈ Rn and we abuse the notation of a general learning rate αt ∈ [0, 1). Then
we bound θt in the following proposition, the proof of which is provided in Appendix D.
5
Under review as a conference paper at ICLR 2021
Proposition 1. Consider an SA given in (9). Suppose Gt is quasi-contractive with a constant pa-
rameter γ, that is, kGt (θt)k ≤ γ kθtk where γ ∈ (0, 1). Then for any learning rate αt ∈ [0, 1), the
iterates {θt} satisfy
kθtk ≤tY-1(1-(1-γ)αk)kθ1k+γαt-1(kWt-1k+kνt-1k)
k=1
+γ
t-2	t-1
X Y (1-(1-γ)αl)
k=1 l=k+1
αk(kWkk+kνkk)+kWtk,
(10)
where the sequence {Wt} is given by Wt+1 = (1 - αt)Wt + αtεt with W1 = 0.
We note that an SA with a similar form to that in (9) has been analyzed in Wainwright (2019b),
which additionally requires a monotonicity assumption. In contrast, our analysis does not require
this assumption. Moreover, distinct from Wainwright (2019b), we treat the noise terms εt and νt
separately rather than bounding them together. This is because for double Q-learning, the noise term
νt has its own dynamics which is significantly more complex than the i.i.d. noise εt. Bounding them
as one noise term will yield more conservative results.
Note that the SA recursion (7) is a special case of (9) by setting νt = 0. Therefore, Proposition 1 is
readily applicable to both (5) and (7).
3.3	Finite-time Analysis of Synchronous Double Q-learning
We apply the above bound for SA to synchronous double Q-learning and bound the error krt k =
∣∣QA - Q*∣ J. The first result is stated in the following theorem.
Theorem 1. Fix γ ∈ (0, 1). Consider synchronous double Q-learning in (3) with a rescaled linear
learning rate ɑt = 3+(3-γ)t, ∀t ≥ 0. Then the learning error r = QA — Q* satisfies
E krt+1k ≤
3 krιk	3√3κCC	1	36√3%aχD 1
(T-Yt+ (1 - γ)3∕2 √t+ (1 - γ)5∕2 √t,
(11)
where G := 6√ln2D + 3√π, D := 2√ln2D + √π. and K is defined in (6) which is the uniform
bound of ∣εt∣.
Theorem 1 provides the finite-time error bound for synchronous double Q-learning. To understand
Theorem 1, the first term on the RHS (right hand side) of (11) shows that the initial error decays
sub-linearly with respect to the number of iterations. The second term arises due to the fluctuation
of the noise term εt , which involves the problem specific quantity κ. The last item arises due to the
fluctuation of the noise term μt in the %-recursion(7), i.e., the difference between two Q-estimators.
Corollary 1. The time complexity (i.e., the total number of iterations) to achieve an -accurate
optimal Q-function (i.e., EkrT∣∣ ≤ e) is g^ven by T(e, γ, D) = Ω Q-^2).
Proof. The proof follows directly from Theorem 1 by noting that the middle term on the RHS of
5
(11) scales as (±)2 since K = 2Rmaχ + Y ∣∣Q*kspan ≤ 2Rmax = %aχ.	□
We next compare Corollary 1 with the time complexity of synchronous double Q-learning provided
in Xiong et al. (2020), which is given by
T = Ω ((	1 ln D 7 2 )ω + (ɪln,	1 .2 广) ,	(12)
(1 - γ)6e2	(1 - γ)7e2	1 - γ	(1 - γ)2e
where ω ∈ (3, 1). For the e dominated regime (with relatively small Y), the result in (12) clearly
cannot achieve the order of 表 and ln D as our result does. Further, its approaching to such an order
(η → 0 in Table 1) is also at an additional cost of an asymptotically large exponent on y-γ. For 1 - Y
dominated regime, the dependence on 1 - Y can be optimized by taking ω = 7 in (12), compared
to which our result achieves an improvement by a factor of O ι (ln ɪ-^ ) ι (see Table 1).
6
Under review as a conference paper at ICLR 2021
3.4 Finite-time Analysis of Asynchronous Double Q-learning
In this subsection, we provide the finite-time result for asynchronous double Q-learning. Differently
from the synchronous version, at each iteration asynchronous double Q-learning only update one
state-action pair of a randomly chosen Q-estimator. Thus the sampling strategy is important for the
convergence analysis, for which we first make the following assumption.
Assumption 1. The Markov chain induced by the stationary behavior policy π is uniformly ergodic.
This is a standard assumption under which Markov chain is most widely studied (Paulin et al., 2015).
It was also assumed in (Qu & Wierman, 2020; Li et al., 2020) for the asynchronous samples in Q-
learning. We further introduce the following standard notations (see for example Qu & Wierman
(2020); Li et al. (2020)) that will be useful in the analysis.
First, We denote μ∏ as the stationary distribution of the behavior policy over the state-action space
S×A and denote μmin := min(s,a)∈s×A μ∏ (s, a). It is easy to see that the smaller μmin is, the more
iterations We need to visit all state-action pairs. Formally, We capture this probabilistic coverage by
defining the folloWing covering number:
L = min It : min	P(Bt∣(sι, aι)) ≥ 11 ,	(13)
(s1,a1)∈S×A	2
Where Bt denotes the event that all state-action pairs have been visited at least once in t iterations.
In addition, the ergodicity assumption indicates that the distribution of samples Will approach to the
stationary distribution μ∏ in a so-called mixing rate. We define the corresponding mixing time as
tmix = min It ： max	"tv (Pt(∙∣(sι, aι)),μ∏) ≤ 1 ∖ ,	(14)
(s1,a1)∈S ×A	4
where Pt(∙∣(sι, aι)) is the distribution of (st, a. given the initial pair (si, a。，and dτv(μ, V) is the
variation distance between two distributions μ, V.
Next, we provide the first result for asynchronous double Q-learning in the following theorem whose
proof is seen in Appendix H.
Theorem 2. Fix Y ∈ (0,1), δ ∈ (0,1), e ∈ (0, ɪ--Y) and suppose that Assumption 1
holds. Consider asynchronous double Q-learning with a constant learning rate αt = α =
Cl
inDT
δ
min {(1 — γ)6e2, t1-} with some ConStant ci. Then asynchronous double Q-learning learns
an e-accurate optimum, i.e., ∣∣QA — Q[∣ ≤ e, with probability at least 1 — δ g^ven the time com-
plexity of
t1
μmin(1-Y)) e(1 — γ)2
1
~
T = Ω
μmine2(1 - γ)7 +
where tmix is defined in (14).
〜
T = Ω
The complexity in Theorem 2 is given in terms of the mixing time. To facilitate comparisons, we
provide the following result in terms of the covering number.
Theorem 3. Under the same conditions of Theorem 2, consider a constant learning rate αt =
α =	CDT min {(1 — γ)6e2,1} with some constant c2. Then asynchronous double Q-learning can
ιn δ
learn an e-accurate optimum, i.e., ∣QA — Q*∣∣ ≤ e, with probability at least 1 — δ given the time
complexity of
L I 1
e2(1 — Y)7 n e(1 — γ)2
where L is defined in (13).
We next compare Theorem 3 with the result obtained in Xiong et al. (2020). In Xiong et al. (2020),
the authors provided the time complexity for asynchronous double Q-learning as
T = Ω
((L4	1	DL4	∖1 L L2 1	1 λτ-1ω
(1(I-Y )6e2	(1-Y )7e2 )	+(1	(1-γ)2e))
(15)
7
Under review as a conference paper at ICLR 2021
where ω ∈ (3, 1). It can be observed that our result improves that in (15) with respect to the order
of all key parameters , D, 1 - γ, L (see Table 1). Specifically, the dependence on L in (15) can be
optimized by choosing ω = 2, upon which Theorem 3 improves by a factor of at least L4 5 * *.
3.5 Proof S ketch of Theorem 1
In order to provide the convergence bound for double Q-learning under the rescaled linear learning
rate, we develop a different analysis approach from that in Xiong et al. (2020), the latter of which
does not handle the rescaled linear learning rate. More specifically, in order to analyze a pair of
nested SA recursions, we directly bound both the error dynamics of the outer SA between the Q-
estimator and the global optimum and the error propagation between the two Q-estimators captured
by the inner SA. Then we integrate the bound on the inner SA into that on the outer SA as a noise
term, and establish the final convergence bound. This is a very different yet more direct approach
than the techniques in Xiong et al. (2020) which constructs two complicated block-wisely decreasing
bounds for the two SAs to characterize a block-wise convergence.
Our finite-time analysis for synchronous double Q-learning (i.e., Theorem 1) includes four steps.
Step I: Bounding outer SA dynamics E krtk by inner SA dynamics E kνtk. Here, rt := QA - Q*
captures the error dynamics between the Q-estimator and the global optimum, and νt := QtA - QtB
captures the error propagation between the two Q-estimators. We apply Proposition 1 to the error
dynamics (5) of rt , take the expectation, and apply the learning rate inequality (24) to obtain
t-1
Ekrtk ≤ αt-i krιk + 2a— X (EkWkk + EkVkk) + EkWtk,	(16)
k=1
where Wt+ι = (1 一 at)Wt + a=εt, with initialization Wi = 0.
Step II: Bounding E kWt k. We first construct a Ft-martingale sequence {Wi}1≤i≤t+1 with
Wt+i = Wt+i and Wi = 0. Next, We bound the squared difference sequence (Wi+i 一 Wi)2 by
4Vm2axatN /aiN-2, for 1 ≤ i ≤ t, where N is defined in (26). Then we apply the Azuma-Hoeffding
inequality (see Lemma 5) to {Wi }i≤i≤t+i and further use Lemma 6 to obtain the bound on E kWtk
in Proposition 2 which is given by
一 ..__ .. ≈. ,------
EkWt+1k ≤ κC^√at,	(17)
where C = 6√ln2D + 3√π and K is defined in (6).
Step III: Bounding inner SA dynamics E kνtk. Similarly to Step I, we apply Proposition 1 to the
νt-recursion (7), take the expectation, and apply the learning rate inequality (24) to obtain
1 + γ	t-i
EIlVtk ≤ at-ι IlVIk +—a~-it-ι	EkMkk +EkMtk,	(18)
k=2
where Mt+i = (1 一 at)Mt + atμt, with initialization Mi = 0. Using a similar idea to Step II, we
obtain the bound on E kMtk in Proposition 3. Finally, we substitute the bound of E kMtk back in
(18) and use the fact kVi k = 0 to obtain
Q	_____
EkVtk ≤ 6, max √an,	with D = 2√ln2D + √∏.	(19)
1 一 Y
Step IV: Deriving finite-time bound. Substituting (17) and (19) into (16) yields (11).
4 Conclusion
In this paper, we derived sharper finite-time bounds for double Q-learning with both synchronous
sampling and Makovian asynchronous sampling. To achieve this, we developed a different approach
to bound two nested stochastic approximation recursions. An important yet challenging future topic
is the convergence guarantee for double Q-learning with function approximation. In addition to the
lack of the contraction property of the Bellman operator in the function approximation setting, it is
likely that neither of the two Q-estimators converges, or they do not converge to the same point even
if they both converge. Characterizing the conditions under which double Q-learning with function
approximation converges is still an open problem.
8
Under review as a conference paper at ICLR 2021
References
Bilal H Abed-alguni and Mohammad Ashraf Ottom. Double delayed Q-learning. International
Journal of Artificial Intelligence ,16(2):41-59, 2018.
Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on Learning Theory(COLT), pp. 67-83, 2020.
Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325-349, 2013.
Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
Machine Learning Proceedings 1995, pp. 30-37. Elsevier, 1995.
Carolyn L Beck and R Srikant. Error bounds for constant step-size Q-learning. Systems & Control
Letters, 61(12):1203-1208, 2012.
Dimitri P. Bertsekas and John N Tsitsiklis. Neuro-Dynamic Programming, volume 5. Athena Sci-
entific, 1996.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference on Learning Theory (COLT), 2018.
Vivek S Borkar and Sean P Meyn. The ODE method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning con-
verges to global optima. In Advances in Neural Information Processing Systems (NeurIPS), pp.
11312-11322, 2019.
Zaiwei Chen, Sheng Zhang, Thinh T. Doan, Siva Theja Maguluri, and John-Paul Clarke. Finite-
time analysis of Q-learning with linear function approximation. arXiv preprint arXiv:1905.11425,
2019.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-
sample analysis of stochastic approximation using smooth convex envelopes. arXiv preprint
arXiv:2002.00874, 2020.
Gal Dalal, Balazs Szorenyi, and Gugan Thoppe. A tale of two-timescale reinforcement learning with
the tightest finite-time bound. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 3701-3708, 2020.
Thinh T Doan. Finite-time analysis and restarting scheme for linear two-time-scale stochastic ap-
proximation. arXiv preprint arXiv:1912.10583, 2019.
Kefan Dong, Yuanhao Wang, Xiaoyu Chen, and Liwei Wang. Q-learning with UCB exploration is
sample efficient for infinite-horizon MDP. arXiv preprint arXiv:1901.09311, 2019.
Simon S Du, Yuping Luo, Ruosong Wang, and Hanrui Zhang. Provably efficient Q-learning with
function approximation via distribution shift error checking oracle. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), pp. 8058-8068, 2019.
Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of Machine Learning
Research, 5(Dec):1-25, 2003.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep Q-
learning. arXiv preprint arXiv:1901.00137, 2019.
Hado V Hasselt. Double Q-learning. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 2613-2621, 2010.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Proc. AAAI Conference on Artificial Intelligence (AAAI), 2016.
9
Under review as a conference paper at ICLR 2021
Jiafan He, Dongruo Zhou, and Quanquan Gu. Minimax optimal reinforcement learning for dis-
counted MDPs. arXiv preprint arXiv:2010.00587, 2020.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Proc. AAAI Conference on Artificial Intelligence (AAAI), 2018.
Tommi Jaakkola, Michael I Jordan, and Satinder P Singh. Convergence of stochastic iterative
dynamic programming algorithms. In Advances in Neural Information Processing Systems
(NeurIPS),pp. 703-710,1994.
Zeyu Jia, Lin F. Yang, and Mengdi Wang. Feature-based q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems (NeurIPS), pp. 4863-4873, 2018.
Yujia Jin and Aaron Sidford. Efficiently solving MDPs with stochastic mirror descent. arXiv preprint
arXiv:2008.12776, 2020.
Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time
analysis of linear two-timescale stochastic approximation with markovian noise. arXiv preprint
arXiv:2002.01268, 2020.
Donghwan Lee and Niao He. A unified switching system perspective and ODE analysis of Q-
learning algorithms. arXiv preprint arXiv:1912.02270, 2019.
Donghwan Lee and Niao He. Periodic Q-learning. arXiv preprint arXiv:2002.09795, 2020.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous
Q-learning: Sharper analysis and variance reduction. arXiv preprint arXiv:2006.03041, 2020.
Francisco S Melo. Convergence of Q-learning: A simple proof. Institute of Systems and Robotics,
Tech. Rep, pp. 1-4, 2001.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning
with function approximation. In Proceedings of the 25th international conference on Machine
learning, pp. 664-671, 2008.
Daniel Paulin et al. Concentration inequalities for markov chains by marton couplings and spectral
methods. Electronic Journal of Probability, 20, 2015.
Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation
and Q-learning. arXiv preprint arXiv:2002.00260, 2020.
Devavrat Shah and Qiaomin Xie. Q-learning with nearest neighbors. In Advances in Neural Infor-
mation Processing Systems (NeurIPS), pp. 3111-3121, 2018.
Aaron Sidford, Mengdi Wang, Xian Wu, Lin F Yang, and Yinyu Ye. Near-optimal time and sample
complexities for solving discounted markov decision process with a generative model. arXiv
preprint arXiv:1806.01492, 2018a.
Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye. Variance reduced value iteration and faster
algorithms for solving markov decision processes. In Proceedings of the Twenty-Ninth Annual
ACM-SIAM Symposium on Discrete Algorithms, pp. 770-787. SIAM, 2018b.
Sean R Sinclair, Siddhartha Banerjee, and Christina Lee Yu. Adaptive discretization for episodic
reinforcement learning in metric spaces. Proceedings of the ACM on Measurement and Analysis
of Computing Systems, 3(3):1-44, 2019.
Csaba Szepesvari. The asymptotic convergence-rate of Q-learning. In Advances in Neural Informa-
tion Processing Systems (NeurIPS), pp. 1064-1070, 1998.
John N Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine Learning, 16
(3):185-202, 1994.
10
Under review as a conference paper at ICLR 2021
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019a. doi: 10.1017/
9781108627771.
Martin J Wainwright. Stochastic approximation with cone-contractive operators: Sharp '∞-bounds
for Q-learning. arXiv preprint arXiv:1905.06265, 2019b.
Mengdi Wang. Primal-dual π learning: Sample complexity and sublinear run time for ergodic
markov decision problems. arXiv preprint arXiv:1710.06100, 2017.
Christopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279-292,1992.
Bowen Weng, Huaqing Xiong, Yingbin Liang, and Wei Zhang. Analysis of Q-learning with adapta-
tion and momentum restart for gradient descent. In Proceedings of the Twenty-Ninth International
Joint Conference on Artificial Intelligence (IJCAI-20), pp. 3051-3057, 2020a.
Bowen Weng, Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Momentum Q-learning
with finite-sample convergence guarantee. arXiv preprint arXiv:2007.15418, 2020b.
Wentao Weng, Harsh Gupta, Niao He, Lei Ying, and Srikant R. Provably-efficient double Q-
learning. arXiv preprint arXiv:arXiv:2007.05034, 2020c.
Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Finite-time analysis for double Q-
learning. arXiv preprint arXiv:2009.14257, 2020.
Pan Xu and Quanquan Gu. A finite-time analysis of Q-learning with neural network function ap-
proximation. arXiv preprint arXiv:1912.04511, 2019.
Tengyu Xu, Shaofeng Zou, and Yingbin Liang. Two time-scale off-policy td learning: Non-
asymptotic analysis over markovian samples. arXiv preprint arXiv:1909.11907, 2019.
Kunhe Yang, Lin F. Yang, and Simon S. Du. Q-learning with logarithmic regret. arXiv preprint
arXiv:arXiv:2006.09118, 2020.
Lin Yang and Mengdi Wang. Sample-optimal parametric Q-learning using linearly additive features.
In International Conference on Machine Learning (ICML), pp. 6995-7004, 2019.
Qingchen Zhang, Man Lin, Laurence T Yang, Zhikui Chen, Samee U Khan, and Peng Li. A dou-
ble deep Q-learning model for energy-efficient edge scheduling. IEEE Transactions on Services
Computing, 12(5):739-749, 2018a.
Yi Zhang, Ping Sun, Yuhan Yin, Lin Lin, and Xuesong Wang. Human-like autonomous vehicle
speed control by deep reinforcement learning with double Q-learning. In Proc. IEEE Intelligent
Vehicles Symposium (IV), pp. 1251-1256, 2018b.
Zongzhang Zhang, Zhiyuan Pan, and Mykel J Kochenderfer. Weighted double Q-learning. In
International Joint Conferences on Artificial Intelligence (IJCAI-17), pp. 3455-3461, 2017.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for SARSA with linear
function approximation. In Advances in Neural Information Processing Systems (NeurIPS), pp.
8665-8675, 2019.
11