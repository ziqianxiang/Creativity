Under review as a conference paper at ICLR 2021
Higher-order Structure Prediction in Evolv-
ing Graph Simplicial Complexes
Anonymous authors
Paper under double-blind review
Ab stract
Dynamic graphs are rife with higher-order interactions, such as co-authorship
relationships and protein-protein interactions in biological networks, that natu-
rally arise between more than two nodes at once. In spite of the ubiquitous
presence of such higher-order interactions, limited attention has been paid to the
higher-order counterpart of the popular pairwise link prediction problem. Existing
higher-order structure prediction methods are mostly based on heuristic feature
extraction procedures, which work well in practice but lack theoretical guaran-
tees. Such heuristics are primarily focused on predicting links in a static snapshot
of the graph. Moreover, these heuristic-based methods fail to effectively utilize
and benefit from the knowledge of latent substructures already present within the
higher-order structures. In this paper, we overcome these obstacles by captur-
ing higher-order interactions succinctly as simplices, model their neighborhood
by face-vectors, and develop a nonparametric kernel estimator for simplices that
views the evolving graph from the perspective of a time process (i.e., a sequence of
graph snapshots). Our method substantially outperforms several baseline higher-
order prediction methods. As a theoretical achievement, we prove the consistency
and asymptotic normality in terms of Wasserstein distance of our estimator using
Stein’s method.
1	Introduction
Numerous types of networks like social (Liben-Nowell & Kleinberg, 2007a), biological (Airoldi
et al., 2006), and chemical reaction networks (Wegscheider, 1911) are highly dynamic, as they
evolve and grow rapidly via the appearance of new interactions, represented as the introduction of
new links / edges between the nodes of a network. Identifying the underlying mechanisms by which
such networks evolve over time is a fundamental question that is not yet fully understood. Typically,
insight into the temporal evolution of networks has been obtained via a classical inferential problem
called link prediction, where given a snapshot of the network at time t along with its linkage pattern,
the task is to assess whether a pair of nodes will be linked at a later time t0 > t.
While inferring pairwise links is an important problem, it is oftentimes observed that most of the
real-world graphs exhibit higher-order group-wise interactions that involve more than two nodes
at once. Examples illustrating human group behavior involve a co-author relationship on a single
paper and a network of e-mails to multiple recipients. In nature too, one can observe several proteins
interacting together in a biological network simultaneously.
In spite of their significance, in comparison to single edge inference, relatively fewer works have
studied the problem of predicting higher-order group-wise interactions. Benson et al. (2018) orig-
inally introduced a simplex to model group-wise interactions between nodes in a graph. They pro-
posed predicting a simplicial closure event, whereby an open simplex (with just pairwise interactions
between member vertices) transitions to a closed simplex (where all member vertices participate in
the higher-order relationship simultaneously), in the near future. Figure 1 (Middle) shows an ex-
ample of such a transition from an open triangle to a closed one. Recently, several works have
proposed modeling higher-order interactions as hyperedges in a hypergraph (Xu et al., 2013; Zhang
et al., 2018; Yoon et al., 2020; Patil et al., 2020). Given a hyperedge ht at time t, the inference task
is to predict the future arrival of a new hyperedge ht0, which covers a larger set of vertices than ht
and contains all the vertices in ht . Figure 1 (Right) illustrates this hyperedge prediction task.
1
Under review as a conference paper at ICLR 2021
Figure 1: [Left] Given a 4-node graph, at time t, the 2-simplex [A, B, C] also contains 1-simplices
[A, B], [B, C] and [A, C]. At time t0 > t, the 2-simplex evolves (by connecting with D) to a 3-
simplex [A, B, C, D] which additionally contains 1-simplices [A, D], [B, D] and [C, D], along with
2-simplices [A, B, D], [A, C, D] and [B, C, D]. [Middle] Simplex setting with Benson et al. (2018).
The method predicts [A, B, C] (closed triangle) at time t0 > t from an open triangle with links/edges
[A, B], [B, C] and [A, C] at time t. [Right] Hypergraph represents a triple [A, B, C] as a hyperedge,
without any of its subsets. It cannot distinguish between [[A, B], [B, C], [A, C]] and [A, B, C].
Although prediction models based on either simplicial closure event prediction or hyperedge arrival,
deal with higher-order structures, they both fail to capture the highly complex and non-linear evo-
lution of higher-order structures over time. Both these kinds of models have two major limitations.
First, they predict structures from a single static snapshot of the graph, thus not viewing the evolu-
tion process of adding new edges as a time process. Second, their feature extraction is mostly based
on popular heuristics (Adamic & Adar, 2003; Brin & Page, 2012; Jeh & Widom, 2002; Zhou et al.,
2009a; Barabasi & Albert, 1999; Bhatia et al., 2019) that work well in practice but are not accompa-
nied by strong theoretical guarantees. In addition to the aforementioned shortcomings, hypergraph
based methods model higher-order structures as hyperedges, which omit lower-dimensional sub-
structures present within a single hyperedge. As a consequence, they cannot distinguish between
various substructure relationships. For example, hyperedge [A, B, C] in Figure 1 (Right) cannot
distinguish between group relationships like [[A, B], [B, C], [A, C]] (a set of pairwise interactions)
versus [A, B, C] (all A, B and C simultaneously in a relationship). This problem is remedied by
the use of simplices because they naturally model these substructures as a collection of subsets (i.e.,
faces) of the simplex.
We provide real-world examples of where our simplicial complex based approach can play a signif-
icant role. (i) Organic Chemistry: It is quite common to have the same set of elements interacting
with each other in different configurations, which result in very different functioning compounds
(Ma et al., 2011). Specifically, R-thalidomide and S-thalidomide are two different configurations of
thalidomide, where the R-form was meant to help sedate pregnant women, while the S-form unfor-
tunately resulted in birth defects. This is a famous example in stereo chemistry to show the conse-
quences of mistaking two extremely close configurations (differing by a single bond) as being the
same. Structure prediction to avoid such phenomenon in drug synthesis allows chemists to achieve a
much higher yield and avoid wastage of expensive resources. (ii) Gene expression networks: Gene
networks have nodes that represent genes and edges connect genes which have similar expression
patterns (Zhang & Horvath, 2005). Subgraphs called modules are tightly connected genes in such a
gene expression network. Genomics research provides evidence that higher-order gene expression
relationships (like second and third-order) and their measurements can have very important implica-
tions for cancer prognosis. When making structural predictions in these aforementioned examples,
our simplicial complex based approach provides much more fine-grained control over competing
methods by capturing subtler differences in configurations.
To combat these challenges, our approach views the evolving graph1 as a time process under the
framework of nonparametric time series prediction, which models the evolution of higher-order
structures (as simplices) and their local neighborhoods (spatial dimension) over a moving time win-
dow (temporal dimension). Our inference problem is then modeled as predicting the evolution to
a higher-dimensional simplex at time t0 > t, given a simplex at time t. It is important to note that
this task is more general and greatly diverges from the task proposed by Benson et al. (2018). Our
1We handle the incremental model (edge insertions only) as opposed to the harder fully dynamic model (edge
insertions and deletions allowed) for which most previous methods too cannot provide theoretical guarantees.
2
Under review as a conference paper at ICLR 2021
task requires just a single simplex σ in order to predict a higher-dimensional simplex τ , whose face
/ subset is σ, whereas Benson et al. (2018) requires the presence of all constituent σ faces in order to
predict τ. For example, in Benson et al. (2018) (also shown in Figure 1 (Middle)) all faces [A, B],
[B, C] and [A, C] (open triangle) need to be present in order to predict a closed triangle [A, B, C].
Contrastingly, in our approach, just a single face/edge like [A, B] or [B, C] or [A, C], suffices to
predict its evolution to [A, B, C]. Figure 1 (Left) illustrates an additional example of our proposal
to predict a 3-simplex [A, B, C, D] given only one of its faces [A, B, C].
To this effect, we succinctly capture the features characterizing the local neighborhood of a sim-
Plex as a combination of a face-vector (Bjomer & Kalai, 2006), which is a well-established vector
signature in combinatorial topology literature and a novel scoring function which infers the affinity
of sub-simPlices based on the strength of their Past interactions. Based on these features, we de-
sign a kernel estimator to infer future evolution to higher-dimensional simPlices and Prove both the
consistency and asymPtotic normality of our estimator.
Our contributions: (a) We ProPose a kernel estimator that Predicts higher-order interactions in an
evolving network. (b) We Prove the consistency and asymPtotic normality of our kernel estimator.
(c) We evaluate our method on real-world dynamic networks by ProPosing higher-order link Predic-
tion baselines and observe significant gains in Prediction accuracy in comParison to the baselines.
1.1	Related Studies
Single link prediction: Most literature that Predicts a single edge/link can be broadly classified
as based on: (i) heuristics, (ii) random-walks, or (iii) graPh neural networks (GNNs). (i) Heuristic
methods comPrise of Common neighbors, Adamic-adar (Adamic & Adar, 2003), PageRank (Brin &
Page, 2012), SimRank (Jeh & Widom, 2002), resource allocation (Zhou et al., 2009a), Preferential
attachment (BarabaSi & Albert, 1999), persistence homology based ranking (Bhatia et al., 2019),
and similarity-based methods (Liben-Nowell & Kleinberg, 2007b; Lu & Zhou, 2011). (ii) Random
walk based methods consist of DeepWalk (Perozzi et al., 2014) , Node2Vec (Grover & Leskovec,
2016b) and SpectralWalk (Sharma et al., 2020). (iii) Finally, for both link prediction and node
classification tasks, recent works are mainly GNN-based methods such as VGAE (Kipf & Welling,
2016), WYS (Abu-El-Haija et al., 2018), and SEAL (Zhang & Chen, 2018b).
Higher-order link prediction: Benson et al. (2018) are the first to introduce a higher-order link
prediction problem where they study the likelihoods of future higher-order group interactions as sim-
plicial closure events (explained earlier). Furthermore, there are studies using hypergraphs which
also help naturally represent group relations (Xu et al., 2013; Zhang et al., 2018; Yoon et al., 2020;
Patil et al., 2020). Especially, to represent higher-order relationships, Yoon et al. (2020) proposed
n-projected graphs. For larger n, i.e., higher-order groups, the enumeration of subsets, and keep-
ing track of node co-occurrences quickly becomes infeasible. In comparison to a hypergraph, our
graph simplicial complex is closed under taking subsets, which enables us to better encode more
information for improved inference.
2	Preliminary: Graph Simplicial Complex (GSC)
We start with a general notion of an abstract sim-
plicial complex (ASC), then define a simplex us-
ing ASCs. We specialize this definition to graphs
and define a graph simplicial complex (GSC).
Definition 1 (Abstract simplicial complex and
simplex). An abstract simplicial complex (ASC)
is a collection A of finite non-empty sets, such
that if σ is an element of A, then so is every non-
empty subset of σ . The element σ of A is called
a simplex of A; its dimension is one less than the
number of its elements.
Now, we analyze graphs using the definition of
ASCs. Let G = (V, E) be a finite graph with
Figure 2: Example of evolution of GSC G . The
yellow triangles are 2-simplices. In the k-ball
around 1-simplex [9, 10] (in red), a 1-simplex
[10,7] is added at time t.
Qt
3
Under review as a conference paper at ICLR 2021
vertex set V and edge set E. A graph simpicial complex (GSC) G on G is an ASC consisting
of subsets of V . In particular, G is a collection of subgraphs of G. With graphs, we denote a d-
dimensional simplex (or d-simplex) of a GSC by σ(d) = [v0, v1, . . . , vd]. Each non-empty subset of
σ(d) is called a face of σ(d).
We define several notions related to GSCs, that are useful for describing the evolution of graphs.
Definition 2 (Filtered GSC). For I ⊂ N, a filtered GSC indexed over I is a family (Gt)t∈I of GSCs
such that for every t ≤ t0 in I, Gt ⊂ Gt0 holds.
Obviously, Gt0 ⊂ Gt1 ⊂ . . . Gtn is a discrete filtration induced by the arrival times of the simplices:
Gti \ Gti-1 = σti. This depicts a higher-order analogue of an evolving graph (incremental model),
which allows attaching new simplices at each time-step to an existing GSC to build anew GSC. A fil-
tered GSC Gt,p for the last p discrete time steps is defined as Gt,p := (Gt0)tt0 =t-p = (Gt-p, . . . , Gt),
where Gt0 ⊃ Gt0-1.
We define a notion for dealing with the neighborhood around a given simplex σ(d) ∈ G. We intro-
duce a set of all simplices of dimension d0 or less from G, i.e., G-(d ) := {σ(d) ∈ G | d ≤ d0}. G-(0) is
a vertex set and G-1) is the set of edges and vertices. We also write i 〜j whenever vertices i and j
are adjacent in Ge), and write i 〜k j to indicate that vertex j is k-reachable from i, i.e., there exists
a path of length at most k, connecting i andj in G-(1). Then, we define a ball around σ(d).
Definition 3 (k-ball centered at vertex and simplex). At time t, we define a k-ball centered at vertex
i by Bt,k (i) := {j : i 〜k j and i,j ∈ G(0-)}. We also define a k-ball centered at a simplex σ(d) as
Bt,k(σ(d)) := Ui"rt(σ(d)) Bt,k(i), where Vert(σ(d)) denotes all vertexes in σ(d).
Now, we define a sub-complex Gt0 (σ(d)) ⊆ Gt as the GSC that contains all the simplices in Gt
spanned by the vertices in the k-ball Bt,k (σ(d)).
3	Predicting the Arrival of Higher-order Simplices
We consider the prediction of a simplex’s arrival in the setting described below. Consider a filtered
GSC Gt,p. At time t, given a d-dimensional simplex σ(d) = [v0, ∙∙∙ , vd], We predict the formation
of a (d + 1)-simplex T(d+1) = [v0,…，vd,e] with a new vertex e ∈ Bt,k(σ(d)). Here, We restrict
ve to be k-reachable from σ(d). In order to find out which simplices and vertices are most likely to
appear in τ (d+1) at time t + 1, we need to design features for σ(d) and ve.
3.1	Feature Design for Simplex
We develop a feature design of a simplex associated with the notion of k-balls. The design is
organized into two main elements: (i) a face-vector with a sub-complex, and (ii) a scoring function.
(i)	Face vector of sub-complex: We first define a face-vector of a fixed GSC. The face-vector is an
important topological invariant2 of the GSC.
Definition 4 (face-vector). A combinatorial statistic of G is the face-vector (or f -vector) as f (G) =
(f-1, f0, . . . , fd-1), where fk = fk(G) records the number of k-dimensional faces σ(k) ∈ G.
We then define the feature of a simplex σ(d) at time t, denoted by Nt(σ(d)) = f (Gt0(σ(d))). In
words, the feature is compactly represented as the face-vector of sub-complex Gt0(σ(d)). Our face-
vector representation of a node’s neighborhood can be considered as a higher-order analogue of
the Weisfeiler-Lehman (WL) kernel Shervashidze et al. (2011) on unlabeled graphs, which for each
vertex, iteratively aggregates the vertex degrees of its immediate neighbors to compute a unique
vector of the target vertex that captures the structure of its extended neighborhood.
(ii)	Scoring function: The purpose of this function is to extract the features of ve using its proximity
to σ(d). To this end, we begin by describing affinity between two vertices. Given two vertices
2A topological invariant is a property that is preserved by homeomorphisms.
4
Under review as a conference paper at ICLR 2021
Table 1: Notation table
Basic	
G=(V,E) G σ(d) = [v0, ..., vd] τ (d+1) = [v0, ...,vd,ve] Gt,p = {Gt-p, ..., Gt} G-(d0) = {σ(d) ∈ G | d≤ d0}	graph with vertex set V and edge set E graph simplicial complex (collection of subgraphs of G) d-dimensional simplex (d-simplex) d + 1-simplex for prediction GSCs from the previous p time steps set of simplices of dimension d0 or less
Local simplex	
Bt,k(i) = {j : i ~k j, i, j ∈ Gt(-0)} Bt,k (°⑷)=Svert(σ(d)) Bt,k (i) Gt0(σ(d))	k-ball centered at vertex i k-ball centered at simplex σ(d) all simplices from Gt spanned by vertices in Bt,k (σ(d))
Feature of simplex	
fk = fk(G) f(G) = (f-1,f0,...,fd-1) Nt (σ(d)) = f(Gt0(σ(d))) s(v, v0) ht(σ(d), v) = Pid=0s(vi,ve) Ft(σ(d),ve) =(Nt(σ(d)),ht(σ(d),ve)) Pt(σ(d), F)	total number of k-simplices σ(k) ∈ G face-vector of G feature of σ(d) weighted sum of past co-occurrences ofv, v0 ∈ σ(d) scoring function feature vector set of σ(d), ve with corresponding feature F
v, v0 ∈ G-(0), we denote by s(v, v0) the weighted sum of all past co-occurrences of vertices v and v0 in σ(d), where the weight is d from σ(d). We then devise a scoring function h(∙, ∙) that assigns an	
integral score to the possible introduction of a vertex ve to a d-simplex σ(d) = [v0
d
ht(σ(d), ve) = Xs(vi,ve).
i=0
(1)
Intuitively, the scoring function describes higher co-occurrence between σ(d) and ve at time t, indi-
cates a higher likelihood of forming a (d + 1)-simplex τ (d+1) = [σ(d), ve] together at a future time
t+ 1. We give a higher score to past co-occurrences of vertex pairs in higher dimensional simplices.
Feature vector: Finally, for a given d-simplex σ(d) at time t, and a possible introduction of a new
vertex ve ∈ Bt,k (σ(d) ), we assign a feature vector
Ft(σ(d),ve) = (Nt(σ(d)), ht(σ(d), ve)).	(2)
We denote the set of all such possible (σ(d), ve) pairs with their corresponding feature vectors equal
to F as Pt(σ(d), F). Furthermore, among the pairs in Pt(σ(d), F), we denote by Ptτ (σ(d), F) those
set of pairs with feature vectors equal to F that actually form τ (d+1) = [σ(d) , ve] at time t, i.e.,
σ(d) appears as a face in τ (d+1) at time t. Note the distinction that not all d-simplices counted
in Pt(σ(d) , F) end up being promoted to higher (d + 1)-simplices in the next time step. Table 1
provides a list of notations.
3.2	Prediction Model and Kernel Estimator
For the prediction, we define an indicator variable that displays the appearance of a new simplex.
Given a d-simplex σ(d) = [vo,…，vd] ∈ Gt, the arrival at time t + 1 ofa (d + 1)-simplex T(d+1) =
[v0, ∙∙∙ ,vd,e] with a new vertex V ∈ Bt,k (σ(d)) is captured by the following variable
Yt+1(τ(d+1)) := 10
if σ(d) is a face ofτ(d+1) ,
otherwise.
(3)
Prediction model: Our approach for the prediction is to model the indicator variable. Namely, we
assume that the indicator variable follows the following distribution:
匕+ι(τ(d+1)) | Gt,p ~ BemoUlli(g(Ft(σ(Re)))	(4)
5
Under review as a conference paper at ICLR 2021
where 0 ≤ g(∙) ≤ 1 is a function of the feature vector Ft(σ(d),e). In words, the indicator vari-
able Yt+1(τ(d+1)) is Bernoulli distributed with success probability given by function g(Ft (σ* * (d), ve)),
conditioned on having seen the last p states of GSC Gt . This model describes that the appearance
probabilities for two simplices σi and σj are likely tobe similar, if their feature vector is also similar.
Estimator with Kernels: We utilize a kernel method to estimate the success probability of the
model (Equation 4) based on observed simplices at time t. Let Gt(d) be a set of d-dimensional
simplices at time t from Gt,p. Also, for brevity, let F represent a feature Ft(σ(d), vem) with some
t, σ(d) and vem subject to vem ∈ Bt,k(σi(d)). Let kF - F0k1 denote the L1-distance between two
feature vectors, and also define a L1-ball Γ(F, δ) := {F0 : kF - F0k1 ≤ δ}.
We define our kernel function K(∙, ∙) as follows. With two features F and F0, we define it as
I{F = F0} + βI{kF — F0k1 ≤ δ}
1 + β∣Γ(F,δ)∣
(5)
where β > 0 is the bandwidth parameter, and I is the indicator function. Given the feature F with
only integer components, we are interested in only those close by feature vectors that are either
exactly the same as f or lie within an L1-ball of radius δ centered on f. This explains the choice of
our kernel function with discrete indicator variables.
Now, we define our estimator. At time T, we fix σ(d) and ve and set F = FT (σ(d), ve), our estimator
of g(F) in (Equation 4) is written as follows:
geT (F) :
PT=T-P Pσ(d)∈Gt0(d) Pvn∈Bt0,k")K(F,Ft,(σjdd,0n)∙ Yt0 + l([σjd),en])
PT=T-p Pf (d) Pn∈Bt0,k (j)) K(F,F (σ%n)
(6)
(τ)
Time-complexity of our estimator: As both the face-vectors and counts Pt+1 / Pt+1 (updated in
a data cube) are computed simultaneously, they incur the same time overhead. At time t, for a d-
simplex σ(d) it takes O(|Vk| + |Ek|) time to compute a k-ball around σ(d), where Vk and Ek are
the set of vertices and edges in the k-hop subgraph of a vertex. We must check this k-ball of σ (d)
against p|Gt(-d) | number of simplices (with dimension at most d) from the previous p time steps, by
intersecting against them to get counts for the face vector and data cube. Each intersection test takes
O(|Vk|) time. Recall that fd(Gt) denoted the total number of d-simplices in Gt. Vk and Ek are the
set of vertices and edges in the k-hop subgraph ofa vertex. We must check this k-ball of σ(d) against
p|Gt(-d) | number of simplices (with dimension at most d) from the previous p time steps. So, the entire
computation across T time chunks has a time complexity of O(Tpfd(Gt)(|Vk| + |Ek|))|Gt(-d) |).
Storage-complexity of our estimator: For a single simplex, our estimator requires storing: (i) a
pair of integer counts, namely (Pt(∙, ∙), Pf (∙, ∙)), in a datacube, which costs O(1) and (ii) a d + 1-
dimensional face vector which takes storage O(d + 1). As the total number of simplices is denoted
by fd(Gt), we arrive at a total storage cost of O(dfd(Gt)).
4	Theoretical Property of the Estimator
We show that our estimator has theoretical validity: (i) consistency
and (ii) asymptotic normality. (i) The consistency guarantees geT
achieves zero error as T increases by converging to g. (ii) The
asymptotic normality implies that the error geT - g converges to a
normal distribution, which is useful to evaluate the size of the error
and can be applied to statistical tests and confidence analysis. Both
properties are very important in statistics (Van der Vaart, 2000).
Asymptotic
Normality
gτ g
Consistency
τ → ∞
4.1 Consistency
We study the consistency of our estimator. To discuss the property of the estimators with GSCs, it
is necessary to organize the Markov property, that the GSC evolution process clearly exhibits. It is
6
Under review as a conference paper at ICLR 2021
well-known that there exists a set of irreducible closed communication classes C in the state space
S. We denote the time of entering class C by TC and the event as E (TC). Let SC denote the event
St ∈ C, where St is the state of the Markov chain at time t. Then, E(TC) ∩ SC is the event that the
chain enters class C at time TC and remains in that communication class indefinitely.
With the event SC, we provide the bias-variance decomposition of geT - g, which is common
for theoretical analysis of estimators. The bias represents an error due to the expressive power
of the model and the variance represents the over-fitting error due to algorithm uncertainty. By
analyzing these terms separately, we can analyze the overall prediction error. We define two
functions as bτ(F) = (T - P)T PTp j(d)1 ∣Gt(d)l-1∣Pt+ι(σ(d),F)| and AT(F) = (T -
P)T PT-1 Pj=(d)1 ∣Gt(d)∣-1∣Pt+ι(σ(d),F )|. For the sake of brevity, We fix F and denote gτ(F)
by geT. Similarly, g, hT and dT are used. Also, We define a term BT (F, C) = E[hT | SC]/E[dT |
SC] - g. Then, by Proposition 2 in the supplementary material, We decompose geT - gas
[hT - gdT] - E[hT - gdT | SC]
~	L ɪ -=, ɪ J -L ɪ -=, ɪ I ~~,
gT - g = ----------------------K----------------------+
dT
BT (F, C)E[dT | SC]
dτ
+o(1).
(7)
|
|
z
=:VT (variance)
z
=:BT (bias)
To bound the variance term VT , We make an assumption that our Markov chain Xt exhibits a α-
mixing property Which describes a dependent property of the dynamic process. It is one of the most
common and Well-used assumptions describing time-dependent processes including dynamic graphs
(Sarkar et al., 2014b). Precisely, We present the definition of α-mixing:
Definition 5 (α-mixing). A stochastic process Xtis α-mixing, if a coefficient α(r), defined as
α(r)=	SUp {∣Pr(A ∩ B)- Pr(A)Pr(B)| : A ∈ Σ(X-),B ∈ Σ(X+)},
∣t1-t2∣≥r
satisfies α(r) → 0 as r → ∞. Here, Σ(Xt-) and Σ(Xt+) are the sigma-algebras of past and future
events of the stochastic process up to and including t1.
This definition implies that time-dependent processes get close to independent as time passes. That
is, events at time t and t + 10000 are close to independent, While events at time t and t + 1 can be
correlated. The simplest example is the evolution of stock prices in financial markets: the movement
of stock prices today does not correlate With the movement of stock prices 10 years ago.
To bound the bias term BT , We impose a smoothness condition on g. A similar assumption is often
used in the problem of predicting links (e.g. Assumption 1 in Sarkar et al. (2014a)). Our assumption
is a general and Weaker version of the common assumption.
Assumption 1 (Smoothness on g). There exists a function κ : R → R in the Schwartz space (i.e.
it is infinitely differentiable and converging to zero faster than any polynomial as x → ±∞) with a
parameter b > 0 such that |g(F) - g(F 0)| = O(κ(-kF - F0k1/b)), as b → 0, ∀F, F0.
We utilize all the assumptions, then prove the consistency ofgeT.
Theorem 1 (Consistency). Suppose that the GSC filtration process is α-mixing, β = o(1), and
Assumption 1 holds. Then, for any F and conditional on SC, our estimator geT(F) is well-defined
with probability tending to 1, and |geT (F) - g(F)| →p 0 holds as T → ∞.
4.2 Asymptotic Normality
We shoW the asymptotic normality of the proposed estimator. That is, We prove that the error of
the estimator converges Weakly to a normal distribution. This property alloWs for more detailed
investigations, such as correcting for errors in estimators or performing statistical tests.
Technically speaking, We develop a distribution approximation result With Wasserstein distance (Vil-
lani, 2008) and Stein’s method (Stein et al., 1986) to handle the dependency property of GSCs. We
are interested in approximating a random variable Zn by a Gaussian random variable, Where Zn is
a sum of n mean-centered random variables {Ai}in=0, Where Ai corresponds to a random variable
Which depends on the i-th d-simplex σi(d) in our GSC, Which is dependent on other d-simplices
7
Under review as a conference paper at ICLR 2021
whose neighborhoods largely overlap with that of σi(d). Here, let dw be the Wasserstein distance be-
tween the underlying distributions of the random variables, and N is a standard Gaussian variable.
Then, we develop the general results for Zn . We provide the following theoretical result. Its formal
statement is Theorem 3, which is deferred to the supplementary material due to its complexity.
Proposition 1 (Gaussian approximation for dependent variables; Simple version of Theorem
3). Suppose Zn = in=0 Ai, where {Ai }in=0 is generated by zero-mean random variables
Xo, Xi, ∙∙∙ , Xn satisfying the α-mixing condition, such as Ai = Xi/Bn with Bn = E∖∑Zo X2]∙
Also, suppose that Pr {∣Xi∣ ≤ L} = 1 holdsfor i = 1,…,n with some constant L > 0. Then, with
an existing finite constant C > 0, we have dw (Zn, N) ≤ C (Pn=I E|Ai|3 + TL Pn-II nα(n)).
This result extends Sunklodas (2007) in the sense of Markov chains on GSCs that satisfy the α-
mixing condition. This extension makes it possible to study the contributing effect of neighboring
d-simplices (represented as a set of weakly dependent r.v.’s) on a central d-simplex.
We provide the asymptotic normality of our estimator. It shows that our estimator converges to
a normal distribution in terms of the Wasserstein distance, which leads to weak convergence. To
achieve the result, we utilize the decomposed terms VT and BT from (7). Then, we regard VT as
a sum of dependent random variables and apply the result developed in Proposition 1. Let σc2 be
a limit of variance of the numerator in T-1/2VT as T → ∞. We recall that SC denotes the event
St ∈ C, where St is the state of the Markov chain at time t.
Theorem 2 (Asymptotic Normality). Suppose that Assumption 1 holds, the GSC filtration process
is α-mixing, and σc > 0. Ifβ = o(T-1/2) and b = o(T-1/2), then, for any F and conditioned on
Sc,thefollowingholds: √T(gτ(F) 一 g(F)) → N(0,σ2∕R(C)2), as T → ∞.
By using this property, we can make detailed inferences based on the distribution of the estimation
error. For example, it is possible to create confidence intervals for predictions and perform statistical
tests to rigorously test hypotheses about simplex arrivals.
5	Real-World Data Experiments
We empirically evaluate the performance of our proposed estimator on real-world dynamic graphs
compared to baselines. The basic premise in our experiments is to capture local and higher-order
properties surrounding a d-simplex up to time t to predict the appearance of a new (d + 1)-simplex
at time t0 > t, which contains σ(d) as its face. Note that we compare our method to other closely
related methods that were designed to solve different structure prediction tasks.
Datasets: We report results on real-world dynamic graph datasets sourced from Benson et al. (2018).
Each dataset contains n nodes, m formed edges, and x timestamped simplices (represented as a set
of nodes). There are four datasets named Enron (n = 143, m = 1.8K, x = 5K), EU (n = 998,
m = 29.3K, x = 8K), Contact (n = 327, m = 5.8K, x = 10K), and NDC (National Drug Code)
(n = 1.1K, m = 6.2K, x = 12K).
Experimental setup: We first ordered by arrival times and grouped the timestamped simplices into
T time slices. For most of our experiments, T was set to 20, except for d = 2, where T was set to
6 and 12 for EU and NDC, respectively. Then, we randomly sampled a set of d-simplices from the
time slices in the range [1, T 一 1]. Those d-simplices paired with a vertex that successfully formed
a face in a (d + 1)-simplex in the T-th time slice were classified as positive samples, while the rest
were deemed as negative samples. We picked an equal number of positive and negative samples for
evaluation. For K-fold cross-validation for β, we swapped the T-th time slice with one of the K
slices preceding the T-th time slice for each fold. K was set to 3. All experiments where repeated
10 times and average AUC scores and runtimes are reported.
Compared methods: As naive baselines, we averaged the results of single-edge prediction methods,
where a new edge would form between each node in the d-simplex and the vertex to be paired with.
Specifically, we compare our estimator with: (i) heuristic (Adamic-Adar (AA) (Adamic & Adar,
2001), Jaccard Coefficient (JC) (Salton & McGill, 1986), and Preferential attachment (PA) (Mitzen-
macher, 2004), (ii) deep-learning based (Node2vec (NV) (Grover & Leskovec, 2016a), and SEAL
(SL) (Zhang & Chen, 2018a)), and (iii) temporal graph network based (TGAT (TT) da Xu et al.
8
Under review as a conference paper at ICLR 2021
Table 2: AUC ScoreS and runtimes for baselines VerSUS our method's estimator for d = 1 and d = 2.
Enron Contact NDC EU
Enron Contact NDC EU
(d = 1) AUC / runtime (sec)
AA 0.54/0.18 0.57/0.18 0.30/0.38 0.61 /0.15
JC	0.42 / 0.20	0.63 / 0.2	0.16 / 0.37	0.65 / 0.20
PA	0.55 / 0.15	0.60 / 0.28	0.55 / 0.11	0.38 / 0.09
NV	0.45 / 74	0.25 / 155	0.30 / 406	0.67 / 280
SL	0.54/ 152	0.91/241	0.33 /	260	0.57/431
TT	0.62 / 180	0.80 / 202	0.42 /	285	0.61 /420
TN 0.67/212 0.84/241 0.48 / 280 0.55/512
HP 0.26 / 22 0.64/ 142 0.57/54 0.45 / 17
(d = 2) AUC / runtime (sec)
0.31 / 0.16
0.41 / 0.16
0.52 / 0.15
0.49 / 71
0.48 / 152
0.70/180
0.62/212
0.26 / 22
0.33 / 0.20
0.44 / 0.21
0.63 / 0.20
0.45 / 155
0.54 / 241
0.78/205
0.70/241
0.76 / 144
0.47 / 0.25
0.23 / 0.24
0.74/0.24
0.49/3989
0.40 / 260
0.45 / 285
0.46 / 281
0.45 /58
0.25 / 0.28
0.32 / 0.27
0.34 / 0.18
0.40 / 374
0.29 / 431
0.67/422
0.64/512
0.41 / 18
Ours 0.88 / 1.08	0.87/7.56	0.78 /5.84	0.83 / 1.48	0.94 /2.25	0.83 /2.76	0.96 /0.76	0.80 /0.642
(β=1)	(β=0.1)	(β=0.1)	(β=10)	(β=0.01)	(β=0.01)	(β=0.01)	(β=10)
(2020) and TGN (TN) Rossi et al. (2020)) link prediction methods. We note that (Benson et al.,
2018) for predicting a “simplicial closure” has the closest motiVation to our method, yet has diVer-
gent objectiVes, therefore we omit comparison to their work. For hyper-edge prediction (HP), we
picked the recent most representatiVe work by Yoon et al. (2020) to compare against, although this
work only works for static non-eVolVing hypergraphs.
5.1	Results and Discussion
We aVeraged the classification accuracy and runtimes of our estimator and the baselines. We per-
formed two sets of experiments on the arriVal of a (d + 1)-simplex and summarize it in Table 2 for
d = {1, 2}. We also report the bandwidth β for our estimator selected by cross-Validation.
Predicting 2-simplex (d = 1): We obserVe that our method is nearly two orders of magnitude faster
than the deep learning based methods (NV and SL) and nearly an order of magnitude faster than the
hypergraph prediction method (HP). While the single edge heuristic methods are relatiVely faster,
their AUC scores are not comparable to our method’s AUC scores. Also, we achieVe nearly 30%
improVement (in Enron) oVer the next best performing prediction method.
Predicting 3-simplex (d = 2): The gap in AUC scores
between our method and the baselines are far more pro-
nounced. Our runtimes also improVe due to the far fewer
number of simplices with dimensions exceeding 3. As
obserVed in Yoon et al. (2020) about slight drops in ac-
curacy for higher-dimensional hyper-edges, we also note
that in HP, the AUC score remains the same or drops
slightly compared to prediction at d = 1.
Advantage of higher dimensional simplices: We per-
form additional experiments by increasing d from 1 to 8
and show that handling high-dimensional simplices ex-
hibits high prediction accuracy. In Figure 3, the predic-
tion is basically improVes as d increases.
Predicted Simplex Dimension
Figure 3: AUC score predicted by our
estimator for future formation of a d-
simplex, giVen a d - 1-simplex.
Empirical summary: Traditional estimators fail to accurately capture the rich latent information
present in higher-order structures (and their sub-structures) that eVolVe oVer time. Our estimator
succinctly captures this information Via the f -Vector and weighted scoring of (σ(d), v) pair formation
depending on the dimension of the simplex in which the pair co-occur in the past.
6 Conclusion
We modeled the higher-order interaction as a simplex and demonstrated a noVel kernel estimator
to solVe the higher-order structure prediction problem. From a theoretical standpoint, we proVed
the consistency and asymptotic normality of our estimator. We empirically argue that our estimator
outperforms hypergraph based and higher-order link prediction baselines from both heuristic and
deep-learning based pairwise link prediction methods.
9
Under review as a conference paper at ICLR 2021
References
Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou, and Alexander A Alemi. Watch your step:
Learning node embeddings via graph attention. In Advances in Neural Information Processing
Systems,pp. 9180-9190, 2018.
Lada A. Adamic and Eytan Adar. Friends and neighbors on the web. SOCIAL NETWORKS, 25:
211-230, 2001.
Lada A Adamic and Eytan Adar. Friends and neighbors on the web. Social networks, 25(3):211-230,
2003.
Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing, and Tommi Jaakkola. Mixed
membership stochastic block models for relational data with application to protein-protein inter-
actions. In In Proceedings of the International Biometrics Society Annual Meeting, 2006.
Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. science, 286
(5439):509-512, 1999.
Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial
closure and higher-order link prediction. Proceedings of the National Academy of Sciences, 115
(48):E11221-E11230, 2018. doi: 10.1073/pnas.1800683115.
Sumit Bhatia, Bapi Chatterjee, Deepak Nathani, and Manohar Kaul. A persistent homology per-
spective to the link prediction problem. volume 881 of Studies in Computational Intelligence, pp.
27-39. Springer, 2019.
Anders Bjorner and Gil Kalai. On f-vectors and homology. Annals of the New York Academy of
Sciences, 555:63 - 80, 12 2006.
Sergey Brin and Lawrence Page. Reprint of: The anatomy ofa large-scale hypertextual web search
engine. Computer networks, 56(18):3825-3833, 2012.
L.H.Y. Chen, L. Goldstein, and Q.M. Shao. Normal Approximation by Stein’s Method. Springer
Verlag, 2010.
da Xu, chuanwei ruan, evren korpeoglu, sushant kumar, and kannan achan. Inductive representation
learning on temporal graphs. In International Conference on Learning Representations (ICLR),
2020.
Aditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ’16, pp. 855-864, 2016a.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864. ACM, 2016b.
Glen Jeh and Jennifer Widom. Simrank: a measure of structural-context similarity. In Proceedings
of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 538-543. ACM, 2002.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. NIPS Workshop on Bayesian
Deep Learning, 2016.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. J.
Am. Soc. Inf. Sci. Technol., 58(7):1019-1031, May 2007a. URL http://dx.doi.org/10.
1002/asi.v58:7.
David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. Journal
of the American society for information science and technology, 58(7):1019-1031, 2007b.
Linyuan Lu and Tao Zhou. Link prediction in complex networks: A survey. Physica A: statistical
mechanics and its applications, 390(6):1150-1170, 2011.
10
Under review as a conference paper at ICLR 2021
Shuangge Ma, Michael R Kosorok, Jian Huang, and Ying Dai. Incorporating higher-order repre-
sentative features improves prediction in network-based cancer prognosis analysis. BMC medical
genomics, 4(1):5, 2011.
M. Mitzenmacher. A brief history of generative models for power law and lognormal distributions.
Internet Mathematics, 1(2):226-251, 2004.
Prasanna Patil, Govind Sharma, and M. Narasimha Murty. Negative sampling for hyperlink predic-
tion in networks. volume 12085 of Lecture Notes in Computer Science, pp. 607-619. Springer,
2020.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710. ACM, 2014.
Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael
Bronstein. Temporal graph networks for deep learning on dynamic graphs. In ICML 2020 Work-
shop on Graph Representation Learning, 2020.
Gerard Salton and Michael J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill,
Inc., USA, 1986. ISBN 0070544840.
Purnamrita Sarkar, Deepayan Chakrabarti, and Michael Jordan. Nonparametric link prediction
in large scale dynamic networks. Electron. J. Statist., 8(2):2022-2065, 2014a. doi: 10.1214/
14-EJS943.
Purnamrita Sarkar, Deepayan Chakrabarti, Michael Jordan, et al. Nonparametric link prediction in
large scale dynamic networks. Electronic Journal of Statistics, 8(2):2022-2065, 2014b.
Charu Sharma, Jatin Chauhan, and Manohar Kaul. Learning representations using spectral-biased
random walks on graphs, 2020.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M.
Borgwardt. Weisfeiler-lehman graph kernels. J. Mach. Learn. Res., 12:2539-2561, 2011.
C. Stein, JSTOR (Organization), and Institute of Mathematical Statistics. Approximate Computation
of Expectations. IMS Lecture Notes. 1986.
J. Sunklodas. On normal approximation for strongly mixing random variables. Acta Applicandae
Mathematicae, 97:251-260, 06 2007.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Rud Wegscheider. Uber simultane gleichgewichte und die beziehungen zwischen thermodynamik
Und reactionskinetik homogener Systeme. Monatshefte fur Chemie Und verwandte Teile anderer
Wissenschaften, 32(8):849-906, 1911.
Ye Xu, Dan Rockmore, and Adam M. Kleinbaum. Hyperlink prediction in hypernetworks using
latent social features. In Johannes FUrnkranz, Eyke HUllermeier, and Tomoyuki HiguChi (eds.),
Discovery Science, pp. 324-339, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg.
Se-eun Yoon, H. Song, Kijung Shin, and Y. Yi. How much and when do we need higher-order
informationin hypergraphs? a case study on hyperedge prediction. Proceedings of The Web
Conference 2020, 2020.
Bin Zhang and Steve Horvath. A general framework for weighted gene co-expression network
analysis. Statistical applications in genetics and molecular biology, 4(1), 2005.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. arXiv preprint
arXiv:1802.09691, 2018a.
11
Under review as a conference paper at ICLR 2021
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems,pp. 5165-5175, 2018b.
Muhan Zhang, Zhicheng Cui, Shali Jiang, and Yixin Chen. Beyond link prediction: Predicting
hyperlinks in adjacency space, 2018.
Tao Zhou, LinyUan Lu, and Yi-Cheng Zhang. Predicting missing links via local information. The
European Physical Journal B, 71(4):623-630, 2009a.
Tao Zhou, Linyuan LU, and Yi-Cheng Zhang. Predicting missing links via local information. The
European Physical Journal B: Condensed Matter and Complex Systems, 71(4):623-630, 2009b.
12
Under review as a conference paper at ICLR 2021
Supplementary Materials of Higher-order Link Prediction in
Dynamic Graph Simplicial Complexes
A	Example of S implex and Related Notions
Example 1. We begin by computing the k-balls centered at 1-simplex [9, 10] in Gt and Gt-1, re-
spectively.
The k-ball at time t for k = 1 (i.e., 1-hop vertices only) centered at [9, 10] is:
Bt,1([9, 10]) = Bt,1([9]) ∪ Bt,1([10]). This is the union of k-balls at underlying vertices 9 and 10
according to Definition 3.
Bt,1([9, 10]) = Bt,1([9]) ∪ Bt,1([10])
= {9,13,8,5,6,10}∪{10,14,13,9,6,7,11,15}
= {9,13,8,5,6,10,14,7,11,15}
Similarly, The k-ball at previous time step t - 1 for k = 1 (i.e., 1-hop vertices only) centered at
[9, 10] is:
Bt-1,1([9, 10]) = Bt-1,1([9]) ∪ Bt-1,1([10])
= {9,13,8,5,6,10}∪{10,14,13,9,6,11,15}
= {9,13,8,5,6,10,14,11,15}
Notice that there is only a difference of vertex 7 missing from set Bt-1,1([9, 10]) as compared to set
Bt,1([9, 10]) at time t.
Now, we calculate the subcomplex spanned by Bt,1 ([9, 10]) Then3,
Gt0([9,10]) ={[9], [13], [8], [5], [6], [10], [14], [11], [15], [7],	(8)
[9, 10], [9, 6], [9, 5], [9, 8], [9, 13], [10, 6], [10, 7], [10, 11], [10, 15], [10, 14], [10, 13],
[9, 6, 10], [10, 14, 15]}
Now, we compute the compressed f -vector notation of Gt0([9, 10]) to get
f(Gt0([9,10]))=(1,10,11,2)
Finally, the neighborhood Nt ([9, 10]) = (1, 10, 11, 2).
B Further details of the experiment
B.1	Datasets
We report results on real-world dynamic graph datasets sourced from Benson et. al. Benson et al.
(2018). Each dataset is a set of timestamped simplices (represented as a set of nodes). In each
dataset, let n, m, and x denote the number of nodes, edges formed, and timestamped simplices,
respectively. Enron (n = 143, m = 1.8K, x = 5K) and EU (n = 998, m = 29.3K, x = 8K) model
email networks where nodes are email addresses and all recipients of an email form a simplex in
the network. Contact (n = 327, m = 5.8K, x = 10K) is a proximity graph where nodes represent
persons and a simplex is a set of persons in close proximity to each other. NDC (n = 1.1K,
m = 6.2K, x = 12K) is a drug network from the National Drug Code directory, where nodes are
class labels and a simplex is formed when a set of class labels appear together on a single drug.
B.2	Compared methods
Adamic-Adar Adamic & Adar (2001) and the Jaccard Coefficient Salton & McGill (1986) measure
link probability between two nodes based on the closeness of their respective feature vectors. Prefer-
ential attachment Mitzenmacher (2004) has received considerable attention as a model of growth of
3all simplices are placed on a line each in increasing order of their dimension
13
Under review as a conference paper at ICLR 2021
networks as they model future link probability as the product of the current number of neighbors of
the two nodes. Motivated by resource allocation in transportation networks (much alike the Optimal
Transport (OT) problem), Resource allocation index Zhou et al. (2009b) proposes a node x tries to
transmit a unit resource to node y via common neighbors that play the role of transmitters and sim-
ilarity is measured by the amount of the resource y received from x. Node2vec Grover & Leskovec
(2016a) and SEAL Zhang & Chen (2018a) are deep-learning based graph embedding methods that
are used in link prediction.
Remark 1 (Difference between our setting and simplical closure). The closest work Benson et al.
(2018) proposed predicting a “simplicial closure” event where at time t there exists a set of nodes
which are pairwise edge connected and the task is to predict whether at time t + 1 there will arrive
a simplex which covers all these nodes. This phenomenon was termed as simplicial closure. For
example, authors A, B and C have all co-authored in pairs (i.e., {A, B}, {A, C} and {B, C}) and
a simplicial closure event would take place at t + 1, if a simplex {A, B, C} arrives, implying that
all three authors co-author on a single paper. Our prediction task significantly diverges and aims to
solve a different problem. Considering our previous example, we are given a single co-authorship
relationship between say A and B at time t, we predict whether authors A and B will co-author with
a third author C (ternary co-authorship relationship) on a single paper at time t + 1 in the future.
C Proof for Consistency
For preparation, we rewrite the estimator gbT . Plugging in the definition of our kernel (Equation 5)
into the equation of our estimator (Equation 6) along with the definitions of Pt(∙, ∙) and PT(∙, ∙) to
replace the indicator variables with actual counts, we obtain the following simplification of Equa-
tion 6. Then, it is reformulated as
geT (F)
PT=-P Pσjd)∈Gt0(d) (∣PT+1 Ed),F )1 + β Ps∈Γ(F,δ)IPT+1 Ed),s)l)
PT=-P P,**) (IP，，+I(常),F )1 + β Ps∈Γ(F,δ)IPto+1 (σ*s)l)
(9)
When we set β = 0, we look for other pairs whose feature corresponds to F and we calculate a
fraction of how many such close by pairs actually form a (d + 1)-simplex at time t0 + 1. This
fraction is summed across various d-simplices σj(d) by varying j and also across various discrete
time steps by varying t0 . Setting β > 0 allows our estimator to smooth over close by features.
It turns out simpler to study a proxy estimator gbT, which omits the smoothing of the original one. We
will show that it is asymptotically equivalent to ge. Let |Gt(d)| denote the total number of d-simplices
in GSC Gt at time t. Then, for a feature F, we define the proxy estimator as
gb(F):
^
bτ (F)
^^~~1 ~
db (F)
(10)
where the terms are defined as
^
bh(F)
T-1 |Gt(d)|
XX
t=P j=1
∣Pi+ι(σj∖F )|
-GHI-
, and d (F )
T-1 |Gt(d)|
XX
t=P j=1
∣Pt+ι(σ"F )1
IGW
1
T — P
1
T — P
Observe that P|iG=t1(d)| ranges over the total number of d-simplices in Gt4 and note that IGt(d)I
changes with time step t. Recall that the terms P+ι(∙, ∙) and Pt+ι(∙, ∙) count all actual and possible
formation of [σi(d), vem] ((d + 1)-simplex) that have the same feature vector F. Lemma 1 in the
supplementary material proves IgeT (F) - gbT (F)I → 0 as β → 0. First of all, we show the validity
of the proxy estimator gbT .
Lemma 1 (Approximation by proxy). We obtain
IgeT(F)-gbT(F)I = O(β), ∀F.
4In practice, the total number of d-simplices is much less than the maximum possible cliques with n vertices.
14
Under review as a conference paper at ICLR 2021
Proof of Lemma 1. Recall that Γ(F, δ) denotes the set of features at L1-distance at most δ from F.
We denote by ∣Γ(F,δ) | the cardinality of this set.
^
〜⑶	bτ(F) + CI
gτ (F) = ɪ———
dτ(F) + C2
Where Ci = β P〃 Ps∈r(F,δ)∣Pt+ι(σjd),s)∣ and C = β P〃 Ps∈r(F,δ)∣Pt+ι(σjd),s)∣. Dueto
the finiteness of features in Pt+ι(∙, ∙) and Pt+ι(∙, ∙), we have that Ci = C2 = O(β). Both Ci and
C2 are non-negative integers. So,
leτ(F) -GT(F)|
bτ (F)+ Ci	hτ(F) I )
dτ (F) + C	dτ (F )∣
In the last step, the second fraction is a positive constant and can thus be ignored from our asymptotic
analysis because both hτ and dτ are bounded.
□
Next, we prove the convergence of the proxy estimator gbτ (F). As the first step, we describe the
detail of its decomposition in (7). To simplify the notation, we will drop F from all estimator
notations.
Proposition 2. As written in Equation 7, we obtain
gbτ - g = Vτ + Bτ .
Furthermore, with the event SC, there exist a stochastic terms qt for t such as
”	(T - p)-i PT-Iqt
Vτ =.
dτ
Proof of Proposition 2. With the definition of Bτ (F, C), we have
ʌ	hτ
gτ - g =〒- g
dτ
G S'
hτ - gdτ
(11)
dτ
^ ^ . ^ ^
^ ^
[hτ - g dτ] - E[hτ - gdτ | SC] + E[hτ - gdτ | SC]
dτ
^ ^ . ^ ^ .
[hτ — gdτ] - E[hτ — gdτ | SC] + E[hτ | SC] — gE[dτ | SC]
dτ
^ ^
dτ
^
[hτ - gdτ] — E[hτ - gdτ | SC] + Bτ(F, C)E[dτ | SC]
dτ
dτ
^ ^
^
^
Vτ + Bτ .
(12)
We are interested in the asymptotic behavior of the Markov chain at time T → ∞. Let F denote
Fτ (σ(d),em). Recall that the terms ∣pτ +i(∙, ∙)∣ and ∣Pτ +ι(∙, ∙)∣ count all actual and possible forma-
tion of [σi(d), vem] ((d+1)-simplex) that result in the same feature vector F at time T+1. Our Markov
chain has a finite state space and hence belongs to a closed communication class with probability
approaching to 1. We provide a statistical consistency conditional on SC for any communication
class C.
For a given time step t, we define
1	|Gt(d)|
bτ(t) := ∣G(d∣ X ∣Ptτ+i(σjd),F)1	(13)
t j=i
15
Under review as a conference paper at ICLR 2021
1	|Gt(d)|
dT(t) := ∣Gt(d)∣ X lPt+1(σjrf),F)l
Note that hT = T-P Pt=P hT ⑴ and dT = T-P Pt=P dT⑴.
Let us set
qt := [bhT (t) - gdbT(t)] - E[bhT (t) - gdbT(t) l SC]	(14)
Note that qt is the numerator of the stochastic term in Equation 7 and a bounded deterministic
function of SC at a given time step t.	□
For the stochastic term dbT which appears in the denominator, we show its convergence. The follow-
ing two lemmas provide the result.
Lemma 2. If the GSC process is α-mixing, then, as T → ∞, we obtain
Var(bhT (F)lSC) → 0, and Var(dbT (F)lSC) → 0,
for any F.
ProofofLemma 2. We show that variance divided by T converges to a non-negative constant. Let
UT ：= Pt qt/VT, where qt (as shown in Equation 14) is a bounded deterministic function of
the state of Xt at time t. As demonstrated in Sarkar et. al. Sarkar et al. (2014a), we too break
our weighted sum UT across three time intervals: (i) [1, TC - 1], (ii) [TC, TC + M - 1], and (iii)
[TC + M, T], where M is a constant. Now, from Sarkar et al. (2014a), we simply apply Lemma 5.7
to get that E[Var(UT l E(TC), SC) l SC] → σc, for some σc ≥ 0 and from Lemma 5.8, we have
that Var(E[UT l E(TC),SC] l SC) = o(1).
Now, since the law of total variance provides
Var(UT l SC) = E[Var(UT l E(TC),SC) l SC] + Var(E[UT l E(TC),SC] l SC)
we use the previous results from Lemmas 5.7 and 5.8 in Sarkar et al. (2014a) to get
Var(UT l SC) → σc as T → 0, for some constant σc ≥ 0
Plugging in the definition of qt into UT and calculating Var(UT l SC), it follows trivially that
Var(hT l SC) → 0 and Var(dT l SC) → 0 as T → ∞. We refer readers to Remark 5.10 in Sarkar
et al. (2014a) to see how these results also hold in the case when C is aperiodic.	□
Lemma 3. If the GSC process is α-mixing, then there exist a function R(C) with a deterministic
function of class C denote, such as
^
^
lim E[dT (F) l E(TC),SC] = R(C), and lim E[dT (F) l SC] = R(C).
T→∞	T→∞
Proof of Lemma 3. We know by definition that
1	T-1 |Gt(d)|
E[dT(F) l E(TC),Sc] = Tl XX E
p t=P j=1
This is an average of terms E
'∣Pt+ι(σjd),F )|
-∣Gt(d)l-
∣Pt+ι(σjd),F )∣
Mdl
E(TC),SC
(15)
E(TC), SC spanning across d-simplices with indices
j ∈ {1,…，∣Gt(d)∣} and discrete time steps t ∈ {p,…，T - 1}.
For ease of notation, let
X = ∣Pt+ι(σjd),F )∣
XL -Mdl-
Xj denotes the total number of possible (d + 1)-simplices with a d-face as σj(d) divided by the total
number of d-simplices in Gt .
16
Under review as a conference paper at ICLR 2021
In the R.H.S. of Equation 15, the term inside the summation is simplified as
E[Xj | E(TC),SC] = X xPr[Xj =x | E(TC),SC]
x
We know that both Pt+1(σj(d), F) and Gt(d) are fully determined given the current state St of the
Markov chain. Let IS (Y ) denote an indicator variable of whether “Y is in state S” or not.
We have,
Pr[Xj =x | E(TC),SC] = XIS(Xj = x)Pr[St =S | E(TC),SC]
S
As a result, the R.H.S. of Equation 15 becomes
T XX(XxIS(Xj = x))Pr[St = S | E(TC),SC]	(16)
t S j,x
Let λ(S) =	j,x xIS(Xj = x) as this term is fully determined by state S. Then, Equation 16 can
be rewritten as
Et Pr[St = S |E(TC),Sc]
T
λ(S)
S
(17)
Due to stationarity, the average t Pr[St = S | E(TC), SC]/T will converge to a constant function
of state S, denoted by R(S). Given that λ(S) is bounded and the average term converges to a
constant R(S), we say that Equation 17 converges to some constant R(C) > 0, where R(C) is a
deterministic function of communication class C. This proves the first part.
A simple application of the tower property of expectation followed by the dominated convergence
theorem shows that limT→∞ E[d(F) | SC] = RC. This completes the proof for the result. 口
Then, we are ready to prove the convergence of the variance term.
Proposition 3 (Variance). If the GSC filtration process is α-mixing, then, conditional on SC, we
obtain VT →p 0 as T → ∞.
Proofof Proposition 3. By Proposition 2, the term VT is written as (T-p, Pt qt.
For the denominator, Lemma 3 shows that E[dT | SC] → R(C), where R(C) is a positive deter-
ministic function of class C. Also, Lemma 2 states that V ar(dT | SC) → 0 holds as T → ∞. Thus,
dbT →p R(C) > 0 holds. Also, VT is asymptotically well defined for class C.
For the nominator, Lemma 2 also shows that limT →∞ V ar(Pt qt/T | SC) = 0 as T → ∞ and
E[qt | SC] = 0, therefore we have limT →∞ 1/T Pt qt q→m 0 conditioned on SC.
By the continuous mapping theorem, we obtain the statement.
□
Next, we discuss the bias term BT. To this aim, we rewrite the term BT (F, C) as follows:
BT(F,C)=
. ^ , , . - - ∙^∙. ..	-.
(E[hτ(F) | SC] - g(F)E[bτ(F) | SC])
Ξ^~1 ：~：	Z
E[dbT (F) | SC]
Lemma 4. Given that Assumption 1 holds and that when T → ∞, the bandwidth parameter b → 0.
Then, we have that BT(F, C) = O(b) = o(1) as T → ∞.
Proof of Lemma 4. For t ∈ [p, T - 1], j ∈ [1, |Gt(d)|] and a fixed feature vector F, the numerator
of BT(F, C) can be expressed as an average of terms of the form
At := E
IPτ+ι(σjd),F )1
~GW-
∣Pt+ι(σjd),F )|
~GW-
SC g(F)
(18)
17
Under review as a conference paper at ICLR 2021
The first term in Equation 18 can be rewritten using the tower property as
E "E " Pτ+ι(σjd),F )1 E (T)S # ∣√
E[E[	∣Gt(d)l	E(TC),SCJ ∣ SC
1	c/E 、 .	1	∣Ptτ+ι(σ(d),F )|	. 1	1	CC .c	E
When We condition on E (TC), it makes -~+Gj~~- conditionally independent of SC, if t > TC.
Also, for t ≥ TC, we have
E
IP+ι(σjd),F )|
Mdl
E(TC),SC
lPt+1(σj(d),F)l
lGt(d)l
g(Ft(σj(d)
ven))
(19)
Where ven ∈ Bt-1,k(σj(d)).
Given the result in Equation 19 and the fact that the term
∣Pt+ι(σjd),F )∣
-∣Gt(d)∣-
is bounded results in
lPτ+ι(σjd),F )l
Mdl
E(TC),SC
(20)
E
≤ |Pt+|G；d)jF )l g(Ft(σjd),en))I[TC ≤ t]+ cI[TC > t]
≤ 'Gid： F)l g(Ft(σjd百n) + cI[TC > t],
Where c > 0 an existing constant.
NoW, the numerator of BT (F, C) can be upper bounded as
X At/T ≤ X T E lPt+lGσd))jF,l (g(Ft(σjd),en))-g(F)) SC	(21)
+c0XPr[TC >t]/T.
t
The second term in Equation 21 vanishes as T → ∞ because it is of order O(E[TC]/T). Thus, the
numerator of BT (F, C ) is an average of terms of the form
E lPt+lGσd))j F,l (g(Ft(σjd),en))-g(F)) SC .	(22)
Our feature vector counts and simplex neighborhoods are finite because lGt(d)l is bounded. The
expectation in Equation 22 is just a summation of finite terms. We set F0 = Ft(σj(d), ven), and make
use of our smoothness assumption 1, so that
lg(F0) - g(F)l = O(κ(-kF - F0k1/b)).
We also use Lemma 3 to say that the denominator of our bias term converges to a constant R(C).
So,
BT(F,C) = O(κ(-kF - F0k1/b)) = O(b).
The last equality folloWs the property of κ in the SchWartz space. NoW, since BT (F, C) = O(b)
and b → 0 as T → ∞, then we have that BT(F, C) = o(1). This completes the proof.	口
Then, We prove the convergence of the bias. Then, We have the result:
Proposition 4 (Bias). If Assumption 1 holds, then, conditional on SC, BT →p 0 holds as T → ∞.
Proof of Proposition 4. Proposition 2 shows that BT(F) = BT (F, C)/dT(F).
By Lemma 2 and 3, we obtain dbT → R(C) > 0 as T → ∞, as similarly shown in the proof of
Proposition 3. Further, Lemma 4 states that BT (F, C) = o(b). Combining the results, we obtain the
statement.	口
18
Under review as a conference paper at ICLR 2021
Now, we can prove the consistency (Theorem 1).
Proof of Theorem 1. For the result of gbT, we apply the results of Proposition 3 and 4 to the decom-
position in the equation 7, then obtain the statement.
For the result of BT, We additionally combine the result of Lemma 1, then obtain the statement. □
D Proof for Asymptotic Normality
D. 1 Introduction to Wasserstein distance and approximation technique
We denote by BL(R) the space of such bounded functions h that are 1-Lipschitz. More formally,
khk∞
sup|h(x)| < ∞ and Lip(h)
x∈R
1, where Lip(h) = sup Ih(X——h(y)1
x6=y	|x- y|
So, h ∈ BL(R).
We now make use of the Wasserstein metric to measure the distance between distributions. There-
fore, our estimator represented as W can be shown to converge to Z, when the Wasserstein distance
between W and Z’s underlying distributions converges to zero. We have that
dw(W, Z) =	sup |Eh(W) - Eh(Z)|	(23)
h∈BL(R)
D.1.1 Introduction to Stein’s Method for normal approximation
Stein Stein et al. (1986) introduced a powerful technique to estimate the rate of convergence of
sums of weakly dependent r.v.s to the standard normal distribution. A remarkable feature of Stein’s
method is that it can be applied in many circumstances where dependence plays a role, therefore we
propose an adaptation of Stein’s method to our setting of dynamic GSCs.
Given a standard normal r.v. Z, Stein’s lemma (stated below) provides a characterization of Z’s
distribution.
Lemma 5 (Stein’s Lemma Chen et al. (2010)). If W has a standard normal distribution, then
Ef0(W) = E[Wf(W)],	(24)
for all absolutely continuous functions f : R → R with E|f0(Z)| < ∞. Conversely, if Equa-
tion 24 holds for all bounded, continuous, and piecewise continuously differentiable functions f
with E|f0(Z)| < ∞, then W has a standard normal distribution.
In order to show that a r.v. W has a distribution close to that of a target distribution of Z, one must
compare the values of expectations of the two distributions on some collection of bounded functions
h : R → R. Here, Stein’s lemma (Lemma 5) shows that W =d Z, if
Ef0(W) - E[Wf(W)] = 0	(25)
holds. Observe that if the distribution of W is close to that of Z’s distribution, then evaluating
the L.H.S of Equation 25 when W is replaced by Z would result in a small value. Putting these
difference equations together, the following linear differential equation known as Stein’s equation is
arrived at
f0(W) - Wf(W) = h(W) - Eh(Z)	(26)
The f that satisfy Equation 26 with h ∈ BL(R) must satisfy the following conditions for all y, z ∈
R
kfk≤ 2 ,kf0k≤ 2 ,kf00k≤ p2∕∏	(27)
|f0(y+z)-f0(y)| ≤ D|z|
where ho(y) = h(y)-Eh(Z), ci = supχ≥0 ξ(x), c2 = supχ≥0 x(1-xξ(x)), and ξ(x) = (1—Φ)∕φ
(where Φ(x) is the distribution function and φ(x) = Φ0(x)). Then, D = (c1 + c2)kh0k∞ + 2 is
19
Under review as a conference paper at ICLR 2021
a constant. Additionally, we have a bound on the covariance, given the dependent r.v.’s are also
bounded.
IfPr{|X| ≤ C1} = Pr{|Y | ≤ C2} = 1, then
| Cov(X, Y )| ≤ 4C1C2α(r)
(28)
We take a similar approach to Sarkar et. al. Sarkar et al. (2014a) in terms of using the Wasserstein
distance to bound the normal approximation. We first define the dependency in our GSCs and then
propose a notion of α mixing in our context of GSCs. We obtain a tighter bound than the bound pro-
posed in Sarkar et al. (2014a), by instead following an approach proposed by Sunklodas Sunklodas
(2007).
D.2 Gaussian approximation for dependent variables with GSC
In our model, we assume the r.v. Ai to represent a d-simplex σi(d) in a GSC G. In order to have a
notion of α-mixing in our setting, we must first define a distance between two d-simplices σi and
σj . We drop the (d) superscript for brevity and ease of notation. We define this distance as the
Hausdorff distance between simplices as
dH(σi, σj)
max
sup inf dg(v, v0), sup inf dg (v, v0)
v∈σi v0∈σj	v0∈σj v∈σi
(29)
where dg(v, v0) counts the number of edges in the geodesic connecting vertices v and v0 in G-(1).
In Stein’s method, the sum of dependent r.v.’s is studied by breaking the sum Zn into two sets based
on the r in mixing coefficient α(r). In our setting, given a fixed d-simplex σi, we study two partial
sums pertaining to: 1) all d-simplices that are at most r-apart from Ai and 2) the remaining partial
sum after removing the variables pertaining to 1) from Zn.
With this notion of distance between sets of r.v.’s, we modify with slight deviations from proposition
4 in Sunklodas et. al. Sunklodas (2007) to accommodate our α-mixing in Markov chains based on
GSCs. For a sequence of r.v.’s Xi, X2, ∙…satisfying the α-mixing condition, We write
n
Zn = XAi,	Ai
i=1
Xi
Bn
n
Bn2 = E(XXi)2
i=0
We assume Bn > 0.
Ti(m) denotes the contribution of d-simplices that are further than m away from σi and x(σi , r)
denotes the partial sum of r.v.’s representing simplices that are exactly r away from Ai . Therefore,
Prm=0 x(σi , r) gets us all those d-simplices that are greater than or equal to m away from σi . We
are interested in the contribution of simplices r away from σi as we vary r from 0 to m. With
Proposition 5, we proceed to derive an upper bound on dw (Zn, N) (i.e., the Wasserstein distance
between Zn and N).
Proposition 5. Let S(σi, r) denote the set of d-simplices whose Hausdorff distance equals r. More
formally,
S(σi, r) = {σj : dH(σi, σj) = r}
Additionally, let Xb denote a mean-centered version of r.v. X. Then,
x(σi, r) =	Ap	(x(σi, 0) = Ai)
p∈S(σi,r)
m
Tim = Zn - Xx(σi,r), m = 0, 1,…,(T(T)= Zn)
r=0
Suppose that EZn = 0, EZn = 1, and EA2 < ∞ for all i = 1,…，n. Let E be a r.v. uniformly
distributed in [0, 1] and independent of other r.v.’s. Let f : R → R be a differentiable function such
that supx∈R |f 0 (x)| < ∞. Then we have
Ef 0(Zn) — EZnf(Zn) = Ei +--------+ E7
20
Under review as a conference paper at ICLR 2021
where
n
E1 =-XXEAix(σi,r) f0(Ti(r) + x(σi, r)) - f0(Ti(r))
n
E2 =-XEAi2 f0(Ti(0) + Ai) - f0(Ti(0))
i=1
n	2r	n
E3 = - XXX EA∖r)δ"E4 = - XX X EAi∖r)δ(q)
i=1 r≥1 q=r+1	i=1 r≥1 q≥2r+1
n	rn	n
E5 =XXEAix(σi,r)XEδi(q),E6= -XXEAci2δi(q),E7=XEAi2Eδi(q),
i=1 r≥1	q=0	i=1 q≥1	i=0
and
δi(q) = f0(Ti(q-1)) - f0(Ti(q)).
Theorem 3. Consider a sequence ofrv.,s X1,X2 ,∙…that satisfy α-mixing condition (Definition 5).
Let EXi = 0, Pr{|Xi| ≤ L} = 1, for i = 1,…，n, for some constant L > 0. Then, for ^very
h ∈ BL(R),
XnL3 n-1
EMI3 + n⅛ X Er)
n r=1
dw(Zn,N) ≤ C(S,D)
where C (S, D) is a finite constant which depends on D and maxi ∣S(σi, r)∣.
Proof of Theorem 3. Given a d-simplex σi and its corresponding r.v. Ai, recall that S(σi, r) denotes
the set of d-simplices that are at Hausdorff distance r away from σi . We additionally define Sm to
denote the maximum cardinality of S(σi, r) for all i and a fixed r, i.e.,
Sm = max ∣S(σi,r)∣
i
In order to upper bound the Wasserstein distance between Zn and N, we estimate the difference
Eh(Zn) - Eh(N) using Proposition 5. It was shown in Proposition 5 that this difference is a sum
of terms Ei,…，E7. We will proceed by individually bounding each term.
Bounding E1 :
|E1|
n
XX EAix(σi, r) f0(Ti(r)+x(σi,r))-f0(Ti(r))
i=1 r≥1	S-------------{z---------------}
(i)
We can upper bound term (i) in Equation 30 using Equation 27 by
D∣d∣x(σi,r)∣ ≤ D∣x(σi,r)∣	(since |e| ≤ 1)
Then,
x(σi, r) hf 0(Ti(r) + x(σi, r)) - f 0(Ti(r))i ≤ D(x(σi, r))2
Now, we upper bound x(σi , r) as
x(σi, r) ≤ Sm
(30)
(31)
(32)
since each normalized r.v. in Zn is upper bounded by L/Bn. The L.H.S. of Equation 31 is upper
bounded by DSm ( BL2 )∙
We know that
Cov
x(σi, r) f 0(Ti(r) + x(σi, r)) - f 0(Ti(r)
X-------------------------------------------------
(33)
z
21
Under review as a conference paper at ICLR 2021
E Aix(σi, r) f0(Ti(r) + x(σi, r)) - f0(Ti(r))
X--------------------------------------------------------}
-z
(ii)
+ EAi Ex(σi,r)[广w⑺ + ex3,r)) - f 0(T(r))]
=0
Notice that term (ii) is nothing but the summand in Equation 30. We apply the covariance bounds
(Equation 28), to obtain
(ii) ≤ 4
L2	L3
DSm Bα(r) ≤4DSm B3 Mr)
Bn	Bn
(34)
Then, we have that
∣Eι∣≤ XX(4DSmBα(r)) ≤ 4DS*B X α(r).
i=1 r≥1	Bn	Bn r=1
(35)
Here, the summation Prn=-11 appears, because there are only n variables that should measure the
dependence between each other.
Bounding E2:
|E2| = ∣∣∣X EAi2 f0(Ti(0)+Ai)-f0(Ti(0))
i=1	|
^{^―
(iii)
Using Equation 27, we have that term (iii) ≤ D|Ai|. Then, |E2 | can simply be bounded as
n
∣E2∣ ≤ D X E|Ai |3
i=1
Bounding E3:
|E3| =
2r
EAiX∖r)δ*
i=1 r≥1 q=r+1
2r
2r
(36)
(37)
(38)
≤XX X
EAix3, r)s(1 + XX X lEAiχ(σi,r)1 E ∣δ(
i=1 r≥1 q=r + 1、--	/ i=1 r≥1q=r+1 Y
(iv)
Let us focus on bounding terms (iv) and (v), separately.
For term (iv): We can further split it as
{z^
(v)
n
}
n
n
n
}
-- /
∙∖z
b∣
(39)
We have previously worked out the bounds for terms Ai and x(σi , r). Therefore, we now focus our
attention on bounding δi(q) .
δi(q) = f 0(Ti(q-1)) -f0(Ti(q))
= f0(Ti(q) + x(σi, r)) - f0(Ti(q))
≤ D∣x(σi, r)|	(using Equation 27)
(40)
≤D
22
Under review as a conference paper at ICLR 2021
Therefore, term b (in term (iv)) is upper bounded by DSm BL2.
Applying the covariance bound (Equation 28), we have that
(iv) ≤ 4
L2	L3
DSm B2 J a(r) ≤ 4DSm B3 a(r)
(41)
For term (v): We have calculated some bounds previously, so we can again split (v) as
(42)
Now, combining the inequalities for terms (iv) and (v), we have that
n	2r	L3	L3 n-1
|E31≤∑∑ E 8DSmι B a(r) ≤ BDSm 需 1>。6
i=1 r≥1 q=r+1	n	n r=1
(43)
Bounding E4 : We have that
n
—,—
∣E4∣ =EE E EAix(σi
,r
(44)
i=1 r≥1 q≥2r+1
≤ 4DSm"B3^ X X a(q-r)
n r≥1 q≥2r+1
L3 n-1
≤ 4DSm F X ra(r)
Bn r=1
because Pr≥1 Pq≥2r+1 α(q - r) < Prn=-11 rα(r).
Now, using our previous bounds, the remaining terms, i.e., |E5 |, |E6|, and |E7| are bounded as
follows:
nL3 n-1
∣E5∣≤ 4DSm前 ∑rα(r)
Bn3 r=1
nL3 n-1
∣E6∣≤ 4DSm前 ∑α(r)
Bn3 r=1
n
|E7| ≤ DXE|Ai|3
i=0
Finally, we combine these bounds to arrive at our final upper bound as
(45)
XnnL3n-1
E|Ai|3 + -B3 Era(r)
Bn r=1
(46)
where C(Sm, D) is a constant term depending on constants Km and D. This completes our proof.
□
D.3 Deferred Proof
In this section, we establish our estimator’s result on the asymptotic normality.
23
Under review as a conference paper at ICLR 2021
Letn = |Gt (d)| denote the total number of d-simplices in GSC Gt at time t. Recall from Equation 13
that
1n
bτ(t) := - X∣Pt+ι(σjd),F)1
j=1
-n
dτ(t) ：= - ∑∣Pt+ι(σjd),F)|
n j=1
Also, from Equation 14, we have
qt := [bhT (t) - gdbT (t)] - E[bhT (t) - gdbT(t) | SC]
Additionally, let us define a variable pt for convenience as follows
pt := [bhT (t) - gdbT(t)] - E[bhT (t) - gdbT (t) | E(TC), SC]
Note that pt is conditioned on both E(TC) and SC, while qt is just conditioned on SC. Keeping
these expressions in mind, we begin by showing the weak convergence with pt .
Lemma 6. Under Assumption 1, given pt for any finite TC and σc > 0
^X Pt∕√T → N(0, σ2)	conditioned on E(TC) ∩ SC as T → ∞
t≥TC +M
Proof of Lemma 6. Given a normalized r.v. WT which is a sum of weakly dependent r.v.’s as
WT :=	Pt≥Tc+MPt	—
Var P
t≥Tc + M pt | E(TC),SC
where pt is already bounded and mean-centered. We have to show that our upper bound in Theo-
rem 3 converges to zero, so that according to Stein’s lemma 5, we have
WT →d N(0, -)	conditioned on E(TC) ∩ SC
Recall that by Lemma 5.7 in Sarkar et al. (2014a), we have
Var X pt | E(TC),SC /T →σc2
t≥TC+M
Now, we show that conditioned on event E(TC) ∩ SC, our bound in Theorem 3 has a convergence
rate of O(T -1/2).
Note that our pt corresponds to Ai andT ton in Theorem 3. Aspt is a function ofSt, it involves p+-
GSCs (Gt-ρ+ι,…，Gt+ι) and the distance between the i and j-th GSC is defined as dist(i, j)=
max(|i - j| - (p + -), 0). Therefore, you will observe that we have for x(σi, r) only 2 states that
are at distance r apart from σi, i.e., Ai - r and Ai + r. Therefore, Sm = O(-).
Given that Pr{∣pt | ≤ L} = 1, for t = 1,…，T and for some constant bound L > 0, we have
T	L3 T-1
E E|pt|3 + T/	rα(r)	(ignoring some constant terms)	(47)
t=1	T r=1
≤ TB (1 + χιrα(r)!
We additionally impose that Pr=I rα(r) < ∞ because we use a decaying function for α(∙) and
BT2 ≥ c0T with a positive constant c0 . We choose
α(r) ≤ CI r2(iog r)p ,
24
Under review as a conference paper at ICLR 2021
for r ≥ 2 and fixed p > 1, where 0 < C1 < ∞ is a constant. Then,
T-L3 fl + X rα(r)] ≤ 与(l + α(1) + Ci X 1	] ɪ
BTI	r=i	C ~ c0/2 ∖	1 r=2 小寒*P) √t
'-------------{------------}
=K(L,C1 ,c0)<∞
(48)
where K(L, C1,c0) is a positive and finite constant only depending on the quantities within the
parenthesis. The inequality follows the fact BT2 ≥c0T. Thus, we achieve an asymptotic bound of
O(TT/2). This completes our proof.	口
The weak convergence with pt implies the weak convergence with qt , by a simple application of
Lemma 7.2 in Sarkar et al. (2014a).
Lemma 7 (Lemma 7.2 in Sarkar et al. (2014a)). Suppose Lemma 6 holds. Then, under Assumption 1
and assuming σc > 0,
Conditioned on Sc,	^X qt/√T → N(0, σ2)	as T → ∞
t
We now prove the weak convergence of our estimator.
Proof of Theorem 2. By the definition of gbT - g in Theorem 1, we achieve
√t(eτ - g) = √T(gτ - bτ) + √T(ST - g)
=O(√Tβ) + T-1/2Pt qt + T1/2BTE向SC].
dT	dT
We are able to assess the convergence of each item. Since Lemma 3 shows E[dT |SC] → R(C) and
Lemma 2 shows Var(dbT|SC) → 0, we obtain dbT →p R(C). By Lemma 7, T -1/2 Pt qt converges
to N(0, σc2 ). Further, Lemma 4 proves BT = O(b). Combining these results with the Slutsky’s
lemma, we get the following results:
√T(gτ - g) = O(√Tβ) + Wt + OP(√Tb),
where WT is a random variable such as
Wt → N(0, σ2∕R(C)2).
With the settings β = o(T-1/2) and b = o(T-1/2), We obtain the statement.	口
25