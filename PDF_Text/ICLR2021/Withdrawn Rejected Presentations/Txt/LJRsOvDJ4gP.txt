Under review as a conference paper at ICLR 2021
Asymptotic Optimality of Self-Representative
Low-Rank Approximation and Its Applications
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel technique for finding representatives from a large, unsupervised
dataset. The approach is based on the concept of self-rank, defined as the minimum
number of samples needed to reconstruct all samples with an accuracy proportional
to the rank-K approximation. As the exact computation of self-rank requires a com-
putationally expensive combinatorial search, we propose an efficient algorithm that
jointly estimates self-rank and selects the optimal samples. The ratio of obtained
projection error using selected samples to the error of best rank-K approximation
is called approximation ratio (AR). In this paper, a new upper bound for AR is
derived which is tight for two asymptotic cases. The best AR for self-representative
low-rank approximation was presented in ICML 2017 Chierichetti et al. (2017),
which was further improved by the bound √1 + K reported in NeUrIPS 2019 Dan
et al. (2019). Both of these bounds are obtained by brute force search which is
not practical and these bounds depend solely on K, the rank which is targeted. In
this paper, for the first time, we present an adaptive AR depending on spectral
properties of the original dataset, A ∈ RN ×M . In particular, our performance bound
is proportional to the condition number κ(A) which is a well-known spectral prop-
erty. Our derived AR is expressed as 1 + (κ(A)2 - 1)/(N - K) which approaches
1 in certain asymptotic cases. Our proposed algorithm enjoys linear complexity
w.r.t. the size of original dataset which results in filling a historical gap between
practical and theoretical methods in finding representatives. In addition to evaluat-
ing the proposed algorithm on a synthetic dataset, we show that it can be utilized
in real-world applications such as graph node sampling for optimizing the shortest
path criterion, and learning a classifier with representative data.
Introduction
Low-rank approximation is one of the fundamental workhorses of machine learning and optimization.
It has applications for dimension reduction and clustering in recommendation systems, text mining,
and computer vision. It is a building block for data compression algorithms such as PCA, where the
data are transferred into an eigenspace representation to learn the representative samples which are
suitable for the whole data. The representations are either in a lower-dimensional space or expressed
in terms of pseudo-data not part of the original dataset.1 Thus, these methods are not applicable for
data sampling. However, it was shown recently that spectral methods are useful tools to devise a
sampling framework Zaeemzadeh et al. (2019). In spectral-based sampling, the goal is to keep the
spectrum of representatives as close as possible to the spectrum of the original data. Shannon sampling
Unser (2000), the most theoretically strong sampling scheme also follows the strategy of keeping the
spectrum of the original and sampled function equal to each other, and a reconstruction guarantee is
established for signal sampling in an infinite-dimensional space based on spectral properties.
Datasets encountered in practice are usually composed of a large number of vectors in a finite-
dimensional space. Assume M samples in an N-dimensional space organized in matrix A ∈ RN ×M .
Let S be a set of integers between 1 and M, which, when used to index the columns of A, define a
collection of samples. We define a matrix of the form V = [vmk] which projects back from the sampled
to the original data. We consider the following optimization problem for finding the optimal sampling
1For example, when studying a face dataset, eigenfaces are spectral components found by PCA and are not
part of the dataset. This is similar to K-means clustering, where centroids are not members of the original dataset.
1
Under review as a conference paper at ICLR 2021
and back-projection:
M
{S*, V} = argmin ∑ ∣∣am - ∑ akVmkk2 + λ|S|,	(1)
S,V m=1	k∈S
where λ is a parameter that regularizes the rate of sampling. We refer the cardinality of the set S* as
the self-rank of matrix A in this paper. Substituting ak in the inner summation with an arbitrary vector
uk ∈RN simplifies this problem to the truncated singular value decomposition (SVD). In this case, |S|
is simplified to the conventional definition of rank for matrix A. However, the conventional spectral
decomposition using SVD is not suitable for data sampling since singular vectors are arbitrary vectors
not actual samples. Enforcing the principal bases to be from the dataset itself in equation 1 results in
a self-representative low-rank approximation. This problem is related to the column subset selection
problem (CSSP) Boutsidis et al. (2009), a problem that is known to be NP-hard Shitov (2017); Civril
(2014) and subject of ongoing research Dan et al. (2019); Song et al. (2019b;a).
In this paper, we propose a versatile sampling framework to determine the self-rank and to identify the
corresponding samples. Our approach is based on spectral decomposition; we compute the minimum
number of samples that cover the K most significant spectral components of the dataset. The main
motivation of the present work and the role of proposed method among the vast literature of CSSP
are explained in the next section. The main contributions of this paper can be summarized as follows:
•	We introduce Spectrum Pursuit with Residual Descent (SP-RD), a constructive algorithm that jointly
estimates the self rank and samples corresponding to informative columns.
•	We prove an upper bound for the SP-RD projection error, and show that in two special cases, the
bound is asymptotically the tightest one.
•	We show that the SP-RD algorithm leads to improved performance in a number of applications,
including graph node sampling with shortest path criterion and selection of training data for a classifier.
Related Work and Motivation
There is a large line of work on data selection in machine learning which are mainly studied under
context of CSSP and volume sampling (VS). See Bardenet & Maillard (2015); Dieng et al. (2017);
Campbell & Broderick (2018); Langberg & Schulman (2010); Dasgupta (2011; 2006); Huggins
et al. (2016); Clarkson & Woodruff (2013); J. W. Demmel & Xiang (2015); Gu (2015); Duersch
& Gu (2015) for a non-exhaustive list. Motivated by the fact that datasets continue to grow larger
and larger over time, a popular approach relies on the modification of the dataset itself, such that its
size is shrunken while preserving its original statistical properties. On this observation, (Bardenet &
Maillard, 2015) studied the reduction in size of a large dataset using random linear projection. On a
similar note, (Huggins et al., 2016; Dieng et al., 2017; Campbell & Broderick, 2018) constructed a
weighted subset of a large dataset, the Bayesian coreset, for a wider class of Bayesian models.
Many other efforts have been devoted to data selection and coreset construction algorithms. Examples
include pivoted QR factorization and matrix subset selection, which are under the umbrella of
column subset selection problem Paul et al. (2015); Dan et al. (2019); Song et al. (2019a;b), that
can be considered as a form of unsupervised feature selection or prototype selection. We briefly
discuss the key models relevant to our work. One line of work focuses on the randomized matrix
algorithms which have been developed in fast least squares Clarkson & Woodruff (2013), sketching
algorithms J. W. Demmel & Xiang (2015) and low-rank approximation problems Gu (2015). Authors
in Duersch & Gu (2015) develop a randomized QR with column pivoting (RQRCP), where random
sampling (RNDS) is used for column selection. A comprehensive summary of these algorithms and
their theoretical guarantee is available in Table 1 in Christos Boutsidis & Drinea (2009). Most of these
algorithms can be roughly categorized into two branches. One branch of algorithms are based on rank-
revealing QR (RRQR) decomposition Gu & Eisenstat (1996). It has been proved in Christos Boutsidis
& Drinea (2009) that RRQR is nearly optimal in terms of residue norm. On the other hand, sampling
based methods Petros Drineas & Muthukrishnan (2008) try to select columns by sampling from
certain distributions over all columns of an input matrix. Extension of sampling based methods
to general low-rank matrix approximation problems was also investigated in Srinadh Bhojanapalli
(2016). Let us denote the best rank-K approximation of matrix A by AK . The data matrix A is
approximated in polynomial-time in Deshpande & Rademacher (2010b) and a lower bound on the
required number of samples as O (K log K + K/ε), where ε is the coefficient of the relative error,
2
Under review as a conference paper at ICLR 2021
Table 1: The comparison of upper bound on the error of some well-known low-rank approximation algorithms
for the CSSP. In all methods, K is considered to be the target rank and approaching AK is the goal of sampling.
Our achieved AR is compared with Brute-force Dan et al. (2019), Improved CSSP Boutsidis et al. (2009),
Volume sampling Deshpande & Rademacher (2010a), CUR Decomposition Drineas et al. (2008), and Near-
optimal Boutsidis et al. (2014).
Algorithm	selected samples	Upper bound	Complexity
Brute-force Improved CSSP Volume sampling CUR Decomposition Near-optimal	K K K O( KlOg (K )∕ε2) 2 K/ε	√1 + K Il a - AKll F O (K√IogK)k A - Ak k F (K+ 1) ∣A -AK∣F (1 +ε)∣A -AK∣F (I + ε)kA - AKkF	brute force (M) O (min { MN2, M2N }) O (KNMw log M) O (MK2 log (K)) O (NMK2ε-2)
Ours	K	(I + ε)kA - akIlf	O (NMK)
has been obtained in Deshpande & Rademacher (2010b); P. Drineas & Muthukrishnan (2006). In
other words, a subset of data is selected such that the projection error of all samples on it is less than
(1 + ε)kA - AKkF. For a smaller value of ε, we need more selected samples to achieve the desired
projection error. A tighter bound for the number of required samples is introduced in Boutsidis et al.
(2014). They show that O (K/ε) columns contain a subspace which approximates A with coefficient
error of (1 + ε). Here, we introduced our approach based on recent advances on data sampling. The
reader is invited to see the supplementary document for more elaboration. Our proposed bound is
the first work in the literature that targets rank-K approximation using only K samples while ε can
approach to 0 asymptotically.
The AR of existing low-rank approximation methods are only a function of K. We refer the reader
to Table 1 for a brief survey of upper bounds on various low-rank models for sampling. Methods
in Table 1 are either impractical or shown to be working in very limited practical settings in their
experiments. This motivates two main questions in our work: How many samples suffice to represent
a dataset? and How can we select such data representatives considering the intrinsic structure of
the dataset?. Our proposed method is based on spectrum pursuit (SP) algorithm which is shown to
be working efficiently in a wide range of applications Joneidi et al. (2020). In the present paper we
will show theoretically that why SP outperforms state-of-the-art methods in sampling. Moreover, an
extension of SP algorithm will be presented which is more accurate and provably convergent. As
reflected in Table 1, there is no sampling algorithm whose AR is controlled by structural or spectral
properties (such as condition number κ (A)) of the matrix A. In this paper, a theoretical upper bound
is established which depends on spectral properties of A in addition to data dimensions and the
target rank K. Sampling literature includes a vast set of practical methods which function in limited
occasions appropriately, however, come short in theoretical guarantee. On the other hand, one can find
sampling methods in the literature supported by established theoretical guarantees with no practical
applications. In the present work, for the first time we fill a historical gap between practical sampling
and theoretical sampling.
Joint Self-rank Estimation and Sampling
We introduce an equivalent problem to equation 1 that is solved efficiently.
M
S*= argmin |S| s.t. ∑ ∣∣a m - ∑ a kvιnk ∣∣2 ≤ EK ∣∣ A kF.	⑵
S	m=1	k∈S
where EK ∈ [0, 1] is the projection error corresponding to the best rank-K approximation of data
normalized to ∣A∣2F . Parameter K is a user specified target rank. As K increases, more samples are
needed in set S in order to reach the desired error. The cardinally of set S* for a specified K is denoted
by SK (A) and it is called the self-rank of dataset A with parameter K. For example, setting EK = 0.25
results in a set of samples such that their combination is able to reconstruct all data with 25% error. It
is straightforward to find a target rank to provide the desired error by using truncated SVD. Fig. 1
Left shows the low-rank approximation error of SVD versus an assumed target rank for a subset of
Multi-PIE face dataset with 52, 000 images. For instance, 9 spectral components are needed to span
all data with a normalized error of less than 0.25. Since spectral components in SVD are not among
samples of data, we need to solve Problem (2) in order to find the minimum number of actual samples,
such that their span provides a span as accurate as that of the first 9 spectral components. Fig. 1 Right
3
Under review as a conference paper at ICLR 2021
—Self-rank
=Lower-bound
1 2 3 4 5 6 7 8 9 10
K
Figure 1: The relation between an assumed target rank and the corresponding self-rank. (a) Reconstruction
error of the truncated SVD for 52,000 images from the CMU Multi-PIE face dataset. In order to reach below
Figure 2: (a) The set of first 9 spectral components a.k.a. Eigen (Fisher) faces for 52,000 images from CMU
Multi-PIE dataset. Linear combination of these components is able to reconstruct all 52,000 images with an
average error less than 25%. (b) A subset of dataset with 15 images is found such that their span is as accurate as
the span of the first 9 spectral components.
shows the computed self-rank versus the target rank. As an example, there is a subset of the dataset
with 15 samples such that their span is as accurate as the span of first 9 spectral components as shown
in Fig 2. In other words, there are 15 samples in the dataset such that they are able to reconstruct all
the dataset with an error less than 25%. The normalized error, EK, is a user-specified parameter and
it can be set to any value between 0 and 1. The target rank of matrix A corresponding to a desired
projection error, EK, is defined as the minimum integer K that satisfies (∑N= K+1 σ2)/k A∣F ≤ EK.
The nth singular value of matrix A is denoted by σn. The self-rank of matrix A with parameter K is
equal to the minimum number of columns in A such that their span approximates A by an error less
than the error of best rank-K approximation, i.e., the smallest size for set S that holds
kA - ASASAkF ≤
∑nN=K+1σn2
kAkF
Matrix AS refers to sampled columns of A, and AS is the Moore-PenroSe inverse of As.
Practical Algorithm for Sampling
Solving Problem equation 2 implies a combinatorial search that is not feasible for a massive dataset
and a large target rank. However, computing the target rank is tractable via SVD. Since self-rank is
lower-bounded by target rank, a practical algorithm should start searching for self-rank values greater
than the target rank. Thus, we start with solving the following problem with a fixed |S| = K, and
check if the constraint in Problem equation 2 is satisfied for a desired error threshold.
M
S*	= argmin ∑ ∣∣a m - ∑ a Cmk ∣∣2 s.t. |S| ≤ K.	(3)
S	m=1	k∈S
The solution for S does not guarantee that the error in the constraint of Problem equation 2 is less
than the desired EK . The minimum integer, K, that satisfies the constraint in (2) is considered as the
self-rank of dataset A with parameter K. If the error is higher than the desired threshold indicated by
EK, the cardinality of set S is increased by 1, and Problem (3) is solved with a new value for K.
Spectrum Pursuit (SP) Algorithm is proposed in Joneidi et al. (2020) for solving equation 3. Inspired
by SP, a more accurate solver for equation 3 is proposed in this paper which is called Spectrum Pursuit
with Residual Descend (SP-RD). Deriving SP-RD Alg from equation 3 and its convergence analysis
are studied in the supplementary document. Alg. 1 shows the steps of SP-RD algorithm. In this
algorithm P is a parameter that controls the computational burden of the algorithm. The joint problem
of self-rank estimation and data sampling is summarized in Alg. 2. The heart of this algorithm is the
mentioned SP-RD algorithm. In the experiments we refer Alg. 2 as adaptive SP-RD. In Line 1 of
the algorithm K can be initialized using truncated SVD easily. Moreover, ΠS(A) is the projection
4
Under review as a conference paper at ICLR 2021
operator on the subspace spanned by columns of A indexed by set S. Mathematically, it is equal to
AS(ASTAS)-1ASTA. It is noteworthy that the computational complexity of the SP-RD algorithm is
O(MN +K* 1 2 3N + MNP) per iteration, which depends on the computational burden parameter P. Note
that we only need the first singular vector and there are fast methods to get it. For the most relevant
cases, large dataset with K < N < M, the complexity is dominated by the O(MNP) per iteration and
we need to perform O(K) number of iterations. This is a linear complexity w.r.t M.
Algorithm 1: Spectrum Pursuit with Residual
Descent (SP-RD)
Require: A, P and K
Output: S
1:	Initialization:
S JA random subset of {1,..., M} with |S| = K
{Sk}K=ι J Partition S into K singletons.
iter= 0
while the stopping criterion is not met
2:	k =mod(iter,K)+1
3:	Uk = normalize column(As∖sk)
4:	Vk = A T Uk(UT Uk )-1
5:	Ek = A - Uk Vk T
6:	Uk = first left singular-vector of Ek
7:	Ω J P most correlated Col. of Ek with Uk
8:	Ω = Ω U Sk
9:	Sk J argmink∈∈ΩkEk- akakEkkF
10:	S J- UkK0=1 Sk0
11:	iter=iter+1
end while
Algorithm 2: Joint Self-rank Estimation and Data
Sampling via Adaptive SP-RD
Require: dataset A, P, and the desired sampling er-
ror e ∈ [0, 1]
Output: set S.
1: K J smallest K such that kA - AK k2F ≤ ekAk2F
EK = e
α=K
SJ SP-RD (A,P,α)
WhilekA-ΠS(A)k2F>EKkAk2F
2:	α J α + 1
3:	S J SP-RD (A, P, α)
The state-of-the-art upper bound for the minimum required number of samples is introduced as O (K)
Boutsidis et al. (2014), in order to guarantee that the projection error of sampling is not worse than the
projection error of rank-K approximation. In general, the purpose in CSSP problem is to find an upper
bound on the number of sampled data in set S to ensure that kA - ΠS(A)kF ≤ (1 +ε)kA - AKkF. In
the following theorem, we prove there exists an upper bound for ε when the algorithm SP-RD is used
to select one sample. Please note that when K = 1, in Line 3, Uk is empty and in Line 5, Ek = A.
Theorem 1.	Let E1 denote the error achieved by the best first rank-1 approximation of full-rank
data matrix A, i. e., kA - A1 k2F. Further, let ρ(A) denote rank-oneness measure of A defined in
Zaeemzadeh et al. (2019), and κ(A) denote the condition number of matrix A. Assume that N is less
than M for the given large dataset A. Also, let {s1} denote the singleton containing the best sample
selected by SP-RD algorithm. Then,
kA - πs1 (A)k2F ≤ (1+ε)E1,	(4)
where ε
(K(A)2-1)(1-P(A)2)
N — 1
Therefore, the smallest possible ε for the upper bound to be held can be set to the value found in the
above equation. Before proceeding to establish an upper bound for K selected samples, we briefly
discuss the established bound for one sample in the following remarks clarifying certain properties of
the obtained bound in two specific instances,
Remark 1- When κ(A) = 1, the data matrix is isometric, meaning that all samples are of equal importance.
In this case, the projection error on the span of any sample is equal to the projection error on any spectral
component. Thus, kA - πs1 (A)kF = kA - A1 kF. In other words, the upper bound holds with correction factor
ε = 0.
Remark 2- ρ (A) = 1 means that all the energy of the samples selected is accumulated in the direction of the
first eigenvector. In other words, the first sample is oriented towards the best rank-1 approximation of the data
matrix, i.e., u1. Therefore, ε = 0, and the upper bound holds tightly. This is trivial as the first sample turns out to
be the best rank-1 approximation itself.
5
Under review as a conference paper at ICLR 2021
Theorem 2.	Let kA - AK k2F denote the minimum error achieved by the best first rank-K approximation of data
matrix A. Further, let SK = {s1 , ...sK} be the set containing K samples selected by Alg 1. Then,
kA - πs x (A)kF ≤ (I + ε )kA - A K kF, for any ε ≥ K(A) K 1
N-K
All proofs can be found in the supplementary material. The proposed SP-RD is the extended version of SP
Joneidi et al. (2020) and iterative projection and matching (IPM) algorithm Zaeemzadeh et al. (2019). Theorem
2 shows us the theoretical reason behind the exhibited success of SP family algorithms for a very wide range of
applications due to their simplicity and accuracy. SP and IPM algorithms has no parameter for fine tuning and
parameter P in SP-RD does not need fine tuning and it can be set according to our accessible computational power.
These favorable properties make the class of SP algorithms problem-independent and dataset-independent.
Experimental Results
In this section, we apply our above theoretical results to perform learning tasks employing the selected samples
of datasets based on their complexity, which is reflected in their self-rank. Our aim is to reduce training size of
massive data such that accuracy of learning tasks is maintained. Applications of the class of SP algorithms are
studied recently Joneidi et al. (2020); Zaeemzadeh et al. (2019). In this section, first a synthetic experiment is
designed in order to estimate an oracle self-rank. Then, two new real-world applications are studied.
Self-rank Estimation on Synthetic Data
Since the computation of self-rank is a combinatorial problem, we need to synthesize a dataset with a
known self-rank in order to evaluate sampling algorithms in Alg. 1 and Alg. 2. The known self-rank
is assumed as a lower bound on the estimated self-rank using Alg. 2.
A synthetic full-rank dataset is generated with M
samples in 200 dimensional space. First, we gener-
ate 20 linearly-independent samples whose target
rank is equal to 15 with parameter EK = 0.12 We
call these 20 samples base samples. Then, M-20
samples are generated by linear combination of
base samples. Finally, all M generated samples
are contaminated with noise in the null-space of
base samples to result in a full rank dataset. Since
base samples can be approximated by a rank-15
subspace, the whole dataset can be approximated
by a rank-15 decomposition. However, those 20
base samples correspond to the self-rank of dataset.
We test this dataset using different selection al-
gorithms. An efficient algorithm summarizes the
Figure 3: A synthetic dataset is generated With Self-
rank equal to 20 with EK = 0.1. There are 20 base
samples in the dataset Which are unknoWn. This figure
shoWs the probability of capturing the base samples
in the sampled subset.
dataset to the base samples. Fig. 3 shoWs the probability of selection from the underlying base samples
using different algorithms. We let all selection algorithms to sample 20 data. As can be seen, SP-RD
algorithm successfully finds all base samples for up to 1000 generated samples. Increasing P (number
of correlated samples) in the SP-RD algorithm results in a more accurate solution to Problem 3. We
perform SP-RD With 200 iterations. Note that SP algorithm is equivalent to the SP-RD algorithm
With P = 1. Table 2 shoWs the estimated value for the self-rank of the generated synthetic dataset.
Setting P ≥ 8 results in estimating the oracle value for the underlying self-rank. The desired EK is set
to 0.1. It is WorthWhile to mention that random selection on average needs 67 samples to have the
same accuracy as the projection error of the 20 underlying base samples. While SP-RD With P = 8
only needs 20 samples, i.e., base samples are selected intelligently.
Representative Selection from CMU Multi-Pie Dataset
Here, CMU Multi-PIE Face Database is considered for representative selection Gross et al. (2010). A
cropped version of this dataset is available online at 3, Which contains 249 subjects With 13 poses,
20 illuminations, and 2 expressions. Thus, there are 520 images for each subject. Fig. 4 compares
2Please refer to Fig. 1 and Fig. 2 for definition of target rank and its relation to self-rank.
3https://drive.google.com/open?id= 1QxNCh6vfNSZkod1Rg_zHLI1FM8WyXix4
6
Under review as a conference paper at ICLR 2021
Table 2: Estimated self rank for the described synthetic data using SP-RD algorithm with different values for
parameter P. The expected estimated self-rank using random selection is computed as 67. It means on average
67 samples are needed to span the dataset as accurate as the span of 20 samples found by SP-RD algorithm.
P	P= 1 (SP) P=3 P=5 P=7 P=8 P= 10
Estimated self-rank 54	43	34	26	20	20
(57.22 sec)
(1.849 sec)
(0.431 sec)
(737.1 sec)
(0.145 sec)
(1047 sec)
(0.937 sec)
Figure 4: The averaged cost function of CSSP for selecting K samples from each class of Multi-pie face dataset
with 130,000 images. The projection error is normalized by the projection error of the best rank-K approximation.
SP-RD algorithm is compared with the state-of-the art methods including with SP Joneidi et al. (2020), DS3
Elhamifar & Vidal (2013), FFS You et al. (2018), SMRS Elhamifar et al. (2012), S5C Matsushima & Brbic
(2019), K-medoids Vijaya et al. (2004) and volume sampling Deshpande & Rademacher (2010b).
^-K-MED
—S5C
-SMRS
-FFS
^DS3
-SP
the performance of different state-of-the-art selection algorithms in terms of CSSP normalized
projection error, which is defined as the CSSP cost function in equation 3 for a given selection method
normalized by kA - AK k. The normalized error is averaged for all 249 subjects. Parameter P is set
as 10 for SP-RD algorithm and in order to select K samples, 5K iterations are performed. As it can
be seen, the obtained approximation ratios are much better than the tightest theoretical bound in the
literature which is √1 + K .In practice, for multi-pie face dataset, the achieved AR using different
algorithms is better than 1.42 for K ≤ 15. Moreover, the proposed SP-RD reaches an AR around 1.15
which is the closest to the best possible AR which is 1.
Adaptive Sampling from MNIST Dataset for Classification
To evaluate the effectiveness of our algorithm, we apply our algorithm on MNIST dataset classification
task. In order to apply adaptive sampling, first MNIST data points are transferred to a feature space. A
simple back-bone architecture is utilized for both feature selection and classification tasks to prevent
architecture-specific bias. We train our feature net for classification task on Omniglot dataset Lake
et al. (2011) and pick the last convolution layer as the features. We transform every image in MNIST
training set into a vector of length 256 with the trained feature net. Our classifier also has the same
architecture as our feature net and the only difference is that the last layer consists of only 10 neurons
which is the number of classes in MNIST.
Figure 5 shows the result of these experiments. In adaptive sampling scenario using SP-RD algorithm,
we consider the complexity of each class in MNIST training data determined by the introduced
self-rank for each class. We compute the self-rank of each class using Alg. 2 alongside with optimal
samples. Therefore, the number of selected samples are class dependent. This forms a non-uniform
sampling which samples data for learning from each class in an adaptive fashion. We compare
this to random selection, and the selection carried out by SP-RD with a fixed number of selected
samples from each class. Moreover, K-mediods, SMRS, and dual volume sampling (DVS) Li et al.
(2017) algorithms are compared. Splitting the sampling budget adaptively according to the introduced
self-rank results in improvement of classification performance. This experiment is designed based on
a generic architecture with a typical dataset. However, other selection algorithms do not provide a
gain over random selection.
Adaptive Graph Summarization
A graph network is characterized by structural features such as degree distribution, average shortest-
path length (ASPL) and the clustering coefficient. Calculating the APSL of a large graph is memory
space and computation intensive. Hence, we propose an alternative efficient method for computing the
ASPL with a reasonable error. Instead of the shortest path between all pairs of vertices we compute
7
Under review as a conference paper at ICLR 2021

Average Sample Size per Class
Figure 5: Comparison between the accuracy of
MNIST classification task when a portion of each class
is sampled. In this scenario, K-mediods performs even
worse than random selection and the performance of
SMRS and dual volume sampling are close to that
of random selection. The vertical lines show a 95%
confidence interval.
Table 3: Number of samples needed to achieve the threshold of 0.15 on the error of APSL on the Cora dataset.
Algorithms	Number of samples
SP-RD (P=10)	105
FFS You et al. (2018)	138
DS3 Elhamifar et al. (2016)	145
VSDeshpande & Rademacher (2010b)	147
IS Olivier Bachem (2017)	250
SP Joneidi et al. (2020)	140
FWCampbell & Broderick (2018)	384
MP Bo et al. (2011)	475
GIGA Campbell & Broderick (2018)	490
the ASPL between all the vertices and some selected vertices. Given a graph G we first select a
subset of the vertices and then we exploit the measure of ASPL to evaluate the accuracy. Let G be
the graph of the real world citation graph datasets, Cora with the set of vertices V 4. Further, let
dist(v1, v2) denote the shortest distance between v1 and v2 (v1,v2 ∈ V). Then, the error is defined as
1
err=
∑ dist(vi,Vj) — ∣⅛ ∑ dist(vi,Vj) where |S| is the selected samples by the algorithms
(i, j)∈V	|S| i∈S,j∈V
specified in the table. The more representative these selected vertices are the less error we get. More
importantly, the less selected samples are adopted by each algorithm the faster the ASPL will be
obtained. For a fair comparison between our algorithm and the state-of-the-art we consider a pre-
defined threshold (errth = 0.15) on the error of the approximation based on the topology of the graph
and we observe each algorithm to see how many data each algorithm needs to satisfy that precision
of error on the shortest path. The results of these experiments are shown in Table 3.
Conclusion
In this paper, we study the problem of subset selection, which has many applications in machine
learning. The general goal is to select a subset from a large set of data such that their linear combination
is able to target rank-K approximation. We have propounded a novel adaptive sampling technique
from a given dataset for machine learning tasks. This sampling method is based on considering
the spectral properties of the given data matrix. Our algorithm delivers a tight theoretical bound
on the approximation ratio. The proposed algorithm enjoys a linear computational complexity and
at the same time it provides an outstanding practical performance and a theoretical approximation
guarantee. These favorable properties make our proposed algorithm a new paradigm in the literature
of data sampling. The elongated proof is included in our supplementary material. We also present
experiments on synthetic and real world datasets to demonstrate significant performance superiority
to other sampling methods in different learning tasks.
4The dataset contain sparse bag-of-words feature vectors for each document and a list of citation links
between documents. We treat the citation links as (undirected) edges and construct a binary, symmetric adjacency
matrix A with 2,708 nodes and 5,429 edges.
8
Under review as a conference paper at ICLR 2021
References
Remi Bardenet and Odalric-Ambrym Maillard. A note on replacing uniform subsampling by random
projections in mcmc for linear regression of tall datasets. In 2015. ffhal-01248841f, 2015.
Liefeng Bo, Xiaofeng Ren, and Dieter Fox. Hierarchical matching pursuit for image classification:
Architecture and fast algorithms. In Advances in Neural Information Processing Systems 24: 25th
Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting
held 12-14 December 2011, Granada, Spain., pp. 2115-2123, 2011.
Christos Boutsidis, Michael W Mahoney, and Petros Drineas. An improved approximation algorithm
for the column subset selection problem. In Proceedings of the twentieth annual ACM-SIAM
symposium on Discrete algorithms, pp. 968-977. SIAM, 2009.
Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based matrix
reconstruction. SIAM Journal on Computing, 43(2):687-717, 2014.
Trevor Campbell and Tamara Broderick. Bayesian coreset construction via greedy iterative geodesic
ascent. In International Conference on Machine Learning, 2018.
Flavio Chierichetti, Sreenivas Gollapudi, Ravi Kumar, Silvio Lattanzi, Rina Panigrahy, and David P.
Woodruff. Algorithms for p low-rank approximation. In Proceedings of the 34th International
Conference on Machine Learning - Volume 70, pp. 806-814. JMLR.org, 2017.
Michael Mahoney Christos Boutsidis and Petros Drinea. An improved approximation algorithm for
the column subset selection problem. In In Proceedings of ACM-SIAM Symposium on Discrete
Algorithms (SODA, 2009.
Ali CivriL Column subset selection problem is ug-hard. Journal of Computer and System Sciences,
80(4):849-859, 2014.
K. L. Clarkson and D. P. Woodruff. Low rank approximation and regression in input sparsity time. In
in Proceedings of the forty-fifth annual ACM symposium on Theory of computing. ACM, pp. 81-90,
2013.
Chen Dan, Hong Wang, Hongyang Zhang, Yuchen Zhou, and Pradeep K Ravikumar. Optimal
analysis of subset-selection based l_p low-rank approximation. In Advances in Neural Information
Processing Systems, pp. 2537-2548, 2019.
Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In Advances in neural
information processing systems, pp. 235-242, 2006.
Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767-1781,
2011.
Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection.
In 51th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2010, October
23-26, 2010, Las Vegas, Nevada, USA, pp. 329-338, 2010a.
Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection.
In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pp. 329-338. IEEE,
2010b.
Adji Bousso Dieng, Dustin Tran, Rajesh Ranganath, John W. Paisley, and David M. Blei. Variational
inference via \chi upper bound minimization. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pp. 2729-2738, 2017.
Petros Drineas, Michael W Mahoney, and Shan Muthukrishnan. Relative-error cur matrix decompo-
sitions. SIAM Journal on Matrix Analysis and Applications, 30(2):844-881, 2008.
J. A. Duersch and M. Gu. True blas-3 performance qrcp using random samplin. In arXiv preprint
arXiv:1509.06820, 2015.
9
Under review as a conference paper at ICLR 2021
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
IEEE transactions on pattern analysis and machine intelligence, 35(11):2765-2781, 2013.
Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking at a few: Sparse modeling
for finding representative objects. In 2012 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1600-1607. IEEE, 2012. ISBN 9781467312264. doi: 10.1109/CVPR.2012.
6247852.
Ehsan Elhamifar, Guillermo Sapiro, and S. Shankar Sastry. Dissimilarity based sparse subset selection.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(11):2182-2197, 2016. ISSN
01628828. doi: 10.1109/TPAMI.2015.2511748.
Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, and Simon Baker. Multi-PIE. Image and
Vision Computing, 28(5):807-813, 5 2010. ISSN 0262-8856. doi: 10.1016/J.IMAVIS.2009.08.002.
URL https://www.sciencedirect.com/science/article/pii/S0262885609001711.
M. Gu. Subspace iteration randomization and singular value problems. In SIAM Journal on Matrix
Analysis and Applications, volume 37, pp. A1139-A117, 2015.
Ming Gu and Stanley C Eisenstat. Efficient algorithms for computing a strong rank revealing qr
factorization. In SIAM Journal on Scientific Computing, 1996.
Jonathan H. Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian
logistic regression. In Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp.
4080-4088, 2016.
M. Gu J. W. Demmel, L. Grigori and H. Xiang. Communication avoiding rank revealing qr factoriza-
tion with column pivoting. In SIAM Journal on Matrix Analysis and Applications, volume 36, pp.
55-89, 2015.
Mohsen Joneidi, Saeed Vahidian, Ashkan Esmaeili, Weijia Wang, Nazanin Rahnavard, Bill Lin, and
Mubarak Shah. Select to better learn: Fast and accurate deep learning using data selection from
nonlinear manifolds. Computer Vision and Pattern Recognition, CVPR 2020. IEEE Conference on,
2020.
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of
simple visual concepts. In Proc. of the Annual Meeting of the Cognitive Science Society, volume 33,
2011.
Michael Langberg and Leonard J. Schulman. Universal epsilon-approximators for integrals. In
Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA
2010, Austin, Texas, USA, January 17-19, 2010, pp. 598-607, 2010.
Chengtao Li, Stefanie Jegelka, and Suvrit Sra. Polynomial time algorithms for dual volume sampling.
In Advances in Neural Information Processing Systems, pp. 5038-5047, 2017.
Shin Matsushima and Maria Brbic. Selective sampling-based scalable sparse subspace clustering. In
Advances in Neural Information Processing Systems, 2019.
Andreas Krause Olivier Bachem, Mario Lucic. Practical coreset constructions for machine learning.
Thesis at Department of Computer Science, ETH Zurich, 2017.
M.W. Mahoney P. Drineas and S. Muthukrishnan. Polynomial time algorithm for column-row based
relative-error low-rank matrix approximation. In Report, DIMACS, 2006.
Saurabh Paul, Malik Magdon-Ismail, and Petros Drineas. Column selection via adaptive sampling.
In Advances in neural information processing systems, pp. 406-414, 2015.
Michael W Mahoney Petros Drineas and S Muthukrishnan. Relative-error cur matrix. In SIAM
Journal on Matrix Analysis and Applications, volume 3, pp. 844-881, 2008.
Yaroslav Shitov. Column subset selection is np-complete. arXiv preprint arXiv:1701.02764, 2017.
10
Under review as a conference paper at ICLR 2021
Zhao Song, David Woodruff, and Peilin Zhong. Average case column subset selection for entrywise
'ι-norm loss. In Advances in Neural Information Processing Systems, pp. 10111-10121, 2019a.
Zhao Song, David Woodruff, and Peilin Zhong. Towards a zero-one law for column subset selection.
In Advances in Neural Information Processing Systems, pp. 6120-6131, 2019b.
and Nathan Srebro Srinadh Bhojanapalli, Behnam Neyshabur. Global optimality of local search for
low rank matrix recovery. In Advances in Neural Information Processing Systems, volume 43,
2016.
Michael Unser. Sampling-50 years after shannon. Proceedings of the IEEE, 88(4):569-587, 2000.
P A Vijaya, M Narasimha Murty, and D K Subramanian. Leaders-Subleaders: An efficient hierarchical
clustering algorithm for large data sets. Pattern Recognition Letters, 25(4):505-513, 2004.
Chong You, Chi Li, Daniel P Robinson, and Rene Vidal. Scalable exemplar-based subspace clustering
on class-imbalanced data. In Proceedings of the European Conference on Computer Vision (ECCV),
pp. 67-83, 2018.
Alireza Zaeemzadeh, Mohsen Joneidi, Nazanin Rahnavard, and Mubarak Shah. Iterative
Projection and Matching: Finding Structure-Preserving Representatives and Its Appli-
cation to Computer Vision. In The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5414-5423, 2019. URL http://openaccess.thecvf.com/content_
CVPR_2019/html/Zaeemzadeh_Iterative_Projection_and_Matching_Finding_Structure-
Preserving_Representatives_and_Its_Application_CVPR_2019_paper.html.
11