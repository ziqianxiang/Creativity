Under review as a conference paper at ICLR 2021
PERIL: Probabilistic Embeddings for hybrid
Meta-Reinforcement and Imitation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Imitation learning is a natural way for a human to describe a task to an agent,
and it can be combined with reinforcement learning to enable the agent to solve
that task through exploration. However, traditional methods which combine imi-
tation learning and reinforcement learning require a very large amount of interac-
tion data to learn each new task, even when bootstrapping from a demonstration.
One solution to this is to use meta reinforcement learning (meta-RL) to enable
an agent to quickly adapt to new tasks at test time. In this work, we introduce
a new method to combine imitation learning with meta reinforcement learning,
Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning
(PERIL). Dual inference strategies allow PERIL to precondition exploration poli-
cies on demonstrations, which greatly improves adaptation rates in unseen tasks.
In contrast to pure imitation learning, our approach is capable of exploring be-
yond the demonstration, making it robust to task alterations and uncertainties. By
exploiting the flexibility of meta-RL, we show how PERIL is capable of interpo-
lating from within previously learnt dynamics to adapt to unseen tasks, as well as
unseen task families, within a set of meta-RL benchmarks under sparse rewards.
1	Introduction
Reinforcement Learning (RL) and Imitation Learning (IL) are two popular approaches for teaching
an agent, such as a robot, a new task. However, in their standard form, both require a very large
amount of data to learn: exploration in the case of RL, and demonstrations in the case of IL. In
recent years, meta-RL and meta-IL have emerged as promising solutions to this, by leveraging a
meta-training dataset of tasks to learn representations which can quickly adapt to this new data.
However, both these methods have their own limitations. Meta-RL typically requires hand-crafted,
shaped reward functions to describe each new task, which is tedious and not practical for non-
experts. A more natural way to describe a task is to provide demonstrations, as with meta-IL. But
after adaptation, these methods cannot continue to improve the policy in the way that RL methods
can, and are restricted by the similarity between the new task and the meta-training dataset. A third
limitation, which both methods can suffer from, is that defining a low-dimensional representation
of the environment for efficient learning (as opposed to learning directly from high-dimensional
images), requires hand-crafting this representation. As with rewards, this is not practical for non-
experts, but more importantly, it does not allow generalisation across different task families, since
each task family would require its own unique representation and dimensionality.
In this work, we propose a new method, PERIL, which addresses all three of these limitations, in
a hybrid framework that combines the merits of both RL and IL. Our method allows for tasks to
be defined using demonstrations only as with IL, but upon adaptation to the demonstrations, it also
allows for continual policy improvement through further exploration of the task. Furthermore, we
define the state representation using only the agent’s internal sensors, such as position encoders of a
robot arm, and through interaction we implicitly recover the state of external environment, such as
the poses of objects which the robot is interacting with. Overall, this framework allows for learning
of new tasks without requiring any expert knowledge in the human teacher.
PERIL operates by implicitly representing a task by a latent vector, which is predicted during learn-
ing of a new task, through two means. First, during meta-training we encourage high mutual infor-
mation between the demonstration data and the latent space, which during testing forms a prior for
1
Under review as a conference paper at ICLR 2021
Figure 1: Overview of our proposed method. We obtain a set of demonstrations of an unseen task
and we adapt to it through efficient demonstration-conditioned exploration.
the latent space and represents the agent’s task belief from demonstrations alone. Second, we allow
further exploration to continually update the latent space, by conditioning on the robot’s states and
actions during exploration of this new task. We model the latent space via an encoder, from which
posterior sampling can be done to encode the agent’s current task belief. In essence, the encoder
aims to learn an embedding which can simultaneously (i) infer the task intent, and (ii) output a policy
which can solve the inferred task. During meta-training, PERIL is optimised end-to-end, by simul-
taneously learning both a policy and the embedding function upon which the policy is conditioned.
In our experiments we find PERIL achieves exceptional adaptation rates and is capable of exploiting
demonstrations to efficiently explore unseen tasks. Through structured exploration, PERIL outper-
forms other Meta-RL and Meta-IL baselines and is capable of zero-shot learning. We show how
our method is capable of multi-family meta-learning as well as out-of-family meta-learning by clus-
tering distinct meta-trained latent space representations. As an extension, we also show how to
use privileged information during training to create an auxiliary loss for training the embedding
function, which helps to form a stronger relationship between the latent space and the true under-
lying state which defines the task. Supplementary videos are available at our anonymous webpage
https://sites.google.com/view/peril-iclr-2021
2	Related Work
Meta-RL was conceptualised as an RNN-based task. Developed by Wang (2016) and Duan et al., the
authors use RNNs to feed a history of transitions into the model such that the policy can internalise
the dynamics. On another line, Finn et al. (2017) developed a learning-to-learn strategy, model
agnostic meta learning (MAML), which meta-learns an initialisation that adapts the parameters of
the policy network and fine tunes it during meta-testing. Although promising results in simple goal-
finding tasks, MAML-based methods fail to produce efficient stochastic exploration policies and
adapt to complex tasks (Gupta et al., 2018). Meta-learning robust exploration strategies is key in
order to improve sample efficiency and allow fast adaptation at test time. In light of this, context-
based RL was developed with the aim of reducing the uncertainty of newly explored tasks. These
map transitions T collected from an unseen task into a latent space Z via an encoder qφ(z∣τ), such
that the conditioned policy πθ(T|z) can efficiently solve said task (Rakelly et al.; Wang & Zhou,
2020). An underlining benefit of decoupling task encoding from the policy is that it disentangles
task inference from reward maximisation, whilst gradient-based and RNN-based meta-RL policies
do this internally. An important remark regarding training conditions of the discussed meta-RL
methods is that they are typically meta-trained using dense reward functions (Rakelly et al.; Wang,
2016; Wang & Zhou, 2020). These dense reward functions provide information-rich contexts of
the unseen task. Considering that the ultimate goal is to allow agents to solve new tasks in the real
world, adaptation during test time must be robust to sparse reward feedback (Schoettler et al., 2020).
In the context of RL, incorporating demonstrations has proven successful in aiding exploration
strategies, stabilising learning and increasing sample efficiency (Vecerik et al., 2017). Learning
expressive policies from a set of demonstrations requires a vast amount of expert trajectories (Ross
et al.), particularly in high dimensional state-action spaces (Rajeswaran et al.). In contrast, Meta-IL
can be implemented in meta-RL by conditioning the agent with expert trajectories. On the an-
other hand, Zhou et al. propose a MAML-based meta-IL approach which averages objective across
demonstrations. The latter learns to adapt at test-time by receiving binary rewards. A similar strat-
egy was also developed by and Mendonca et al.. The caveats of these approaches remain that of
traditional IL: (i) Imitating expert trajectories hinders the policy from doing better than the demon-
strations; (ii) Cloning behaviours reduces flexibility and generalisation capacity.
2
Under review as a conference paper at ICLR 2021
3	Hybrid Meta-Reinforcement and Imitation Learning
3.1	Problem Statement
We assume access to a set of tasks T ∈ p(T), with each task represented as a partially observable
Markov decision process (POMDP). Observations o are incomplete and only include measurements
of the agent’s internal state. For example, a peg-in-hole task would include the robot’s (and therefore
the peg’s) pose in the observation, but not the hole’s pose. The full state s contains both the robot’s
internal state, and the state of the external environment. Our method involves an agent inferring z,
a descriptor of a task which provides the information required to solve that task, such as a represen-
tation of s. Each task is then defined as T = {p(o0 |z), p(o0, z0|o, a, z), r(o, a|z)}, with an unknown
initial observation distribution p(o0|z), transition distribution p(o0, z0|o, a, z), and reward function
r(o, a|z), where a is the action taken by the agent. By leveraging task beliefs, PERIL exploits en-
riched observational spaces (S 〜{o ∩ z}) with the objective of closing said POMDP into a stable
MDP form. With the aim of supporting continuous task inference through interaction, we construct
z as a probabilistic embedding conditioned on a set of recently collected transitions, referred to as
the context c. We define a contexton ctT = (ot, at, rt, ot+1) as a transition collected on task T at
time-step t such that the context c0T:t (what we call c) denotes the set of accumulated contextons.
During meta-testing, the agent then attempts to find the true posterior p(z|c) over the task belief
Z 〜p(z∣c), by conditioning on c.
Demonstrations significantly reduce the search space in exploration whilst providing a natural means
of communicating the task. Thus, access to expert trajectories provides information-rich context
which can be exploited to pre-condition the policy via a prior over z. Subsequently, online adaptation
through RL can further disambiguate z . In our approach, we leverage dual meta-learning objectives
to perform hybrid adaptation to new tasks, with conditioning on both demonstration contexts and
exploratory contexts, where we define a demonstration as a trajectory ofT observations and actions
d = {(ot, at)}tT=1, and an adaptation episode τe as a trajectory ofN steps τe = {(ot, at, rt, zt)}tN=1:
Primal Inference: The agent observes a set of k = 1, ..., K demonstrations from an expert set
Ddemo := {dk} to form a context prior cdemo = {dk}kK.
Exploratory Adaptation: The agent explores with initialised context C J Cdemo and adapts with
sampled trajectories T such that at time-step t, a hybrid context is formed C J Cdemo ∩ 丁。疗.
Primal task inference aims to provide long-term inference on a task belief: meta-trained inference
from previously solved tasks. Using posterior sampling, exploratory adaptation provides a proxy
for short-term memory: as the context is updated with recent transitions, structured exploration can
adjust primal inference by reasoning about the uncertainty ofz, where C := {Cdemo ∩(τeT, ..., τe∞T)}.
3.2	Probabilistic Embeddings for Meta-RL
As we do not have access to the posterior p(z|C), we use variational inference methods to produce
an approximation qφ(z∣c). Through generative processes, We sample Z 〜 qφ(z∖c) and optimise
φ by maximising a meta-objective conditioned on z . Variational Encoders (VEs) are capable of
producing latent distributions which optimise an arbitrary task-dependent objective G(T∖∙). We
average the expectancy over a set of tasks from p(T) to formulate a meta-objective which rewards
fast adaptation to unseen tasks (Eq. 1), where β controls the constraint for mutual information
between the inferred variable Z and context C. This information bottleneck filters down redundant
information and compresses the Z into a generalisable form. We set the prior p(Z) to a unit Gaussian.
φ* = arg max Eτ∈p(τ)此〜qφ(z∣°τ )[G(T∖z) + βDκL [qφ(z∖cT )∖∖p(z)]]	⑴
Traditional meta-RL methods leverage RNN-based inference systems to produce latent features from
a history of recently collected transitions. The problem with this approach is that learning from en-
tire trajectories leads to massive variances which hinder the learning process (Duan et al.; Humplik
et al., 2019). In theory recurrence is not strictly required, since any MDP is defined by a distribu-
tion of sequentially invariant transitions. In light of this, we exploit task encoders as suggested by
Rakelly et al., where q(Z∖C) is modelled as a product of Gaussian Factors. Each factor ψφ(Z∖Cn)
parameterised by φ, is independently computed: ψφ(z∖cn) = N(fψ(c∙n),fψ(Cn)).
3
Under review as a conference paper at ICLR 2021
We build on top of probabilistic meta-RL methods derived by Rakelly et al.. Decoupling task in-
ference from task completion allows efficient off-policy objectives to be optimised. Moreover, con-
ditioning a policy with diverse task descriptors aims to generalise behaviours amongst different
dynamics and rewards. This is generally not the case when employing MAML-based or RNN-based
meta-training kernels to ”learn how to learn” new tasks, since policy gradients modify the policy pa-
rameters online which, in turn, can result in highly unstable and inefficient adaptation to new tasks,
requiring up to hundreds of sample trajectories in unseen environments.
To that end, We employ meta-adapted maximum entropy RL, where a proxy for the policy ∏(a∣s) is
defined by including the task belief Z as part of the observational space ∏θ (a|o, z), resulting in task-
conditioned critic LTritic and actor LTCtor losses for the the Q-function Qθ(∙) and the policy ∏(∙)
respectively (see A.1 for more details). As VES are a form of generative processes, G(T∣∙) can be
defined with the aim of recovering transition dynamics or reconstructing state spaces (Humplik et al.,
2019). Instead, we optimise qφ(z∖c) in a model-free manner as proposed in Rakelly et al.. Here,
parameters φ can be optimised to maximise expected discounted rewards under a policy ∏θ(∙∖z)
through reconstruction of Qθ (∙,z). We fold LTritiC as a proxy for the task-dependent goal G(T∖z):
G(T∖Z) 一 LritiC ⑶	⑵
3.3	Conditioning on Demonstrations
By leveraging meta-learning, we suggest that the agent can learn to exploit the overlapping statistical
information from within heterogeneous demonstrations belonging to different tasks, with the aim of
inferring task embeddings z which can help improve exploration during test-time adaptation. In
order to link demonstrations into the probabilistic meta-RL framework, we formulate an objective
based on mutual information I(z; τ) between the demonstration trajectories and the latent space
distribution p(z) (Eq. 3). This objective is similar to that used in Yu et al. with the exception that
we adapt this to meta-imitation learning and off-policy RL instead of recovering reward functions.
I(Z； τ ) = Ez 〜p(z),τ 〜Pθ(τ |z) [lθg P(ZIT ) - log P(Z)]	(3)
As direct access to the posterior p(z∖τ) is not available, I(z; τ) is intractable. However, in order
to adjust for this, we leverage a variational approximation to P(Z∖τ) using our task belief encoder
qφ(Z∖τ). We assume access to a distribution of expert demonstrations PπE (τ∖Z). Additionally, we
include further desiderata over the mutual information objective based on distributional matching:
Learning from demonstrations. Matching generated trajectories to those sampled from expert
trajectories: minθ Ep(z) [DKL(PπE (τ ∖Z)∖∖Pθ (τ ∖Z))]. This secondary objective can be considered as
trying to match the distribution of trajectories generated by the agent’s policy conditioned on Z, to
those from the expert policy (similar to BC). Since they share the same marginal distribution P(Z),
matching these distributions also encourages matching of the conditionals PπE (Z∖τ) andP(Z∖τ).
Linking variational posterior. minθ Epθ(τ) [DKL(P(Z∖τ)∖∖q(Z∖τ))]. Encourage qφ(Z∖τ) to approx-
imate the true posterior P(Z∖τ) such that, given a new demonstration, the encoder properly infers
the task. This objective acts as a regulator as it reduces over-fitting by penalising variance: optimal
distributional mismatch is reached when the encoder produces constant information-less represen-
tations of Z . The distributional matching objective can therefore be defined as in (4).
min -I(Z; τ) + Ep(z)[DKL(PπE (τ ∖Z)∖∖P(τ ∖Z))] + Ep(τ)[DKL(P(Z∖τ)∖∖qφ(Z∖τ))]	(4)
After several manipulation steps (see section A.1 for more details), we deconstruct this objective
into a differentiable form and define two separate loss components LbTC (Eq. 5) and LiTnfo (Eq. 6),
where πb is the behavioural policy which collected the transitions (truncated importance sampling).
LbC = -Eτ~p∏B (T ),z~qφ(z∣τ) [log πθ (T ∖z)]	(5)
Linfo = Eτ~PTE ,z^qφ(z∖τ ),Tb~nb(T Iz) min (log & (Tb ]) , 1) log qφ(ZITb)	(6)
As LiTnfo conditions the encoder, we aggregate it to the task-dependent goal G(T∖Z) (7).
G(T∖Z) <- LTritic(Z) + LTnfo(Z)	⑺
4
Under review as a conference paper at ICLR 2021
3.4	Auxiliary Systems
An inherent problem is that the agent generates predictions Qθ (o, a, z) based on unsupervised esti-
mates Z 〜qφ(z∖cτ). Whilst this is theoretically generalisable to any MDP, it brings large variances
and instabilities during training because the agent must simultaneously (i) distinguish one task from
another, and (ii) use the task beliefs to solve that task. A plausible solution to mitigate these instabil-
ities is to exploit privileged information (a brief task descriptor such as the position and orientation
of a door) during training, allowing the encoder qφ(z∖cT) to produce task beliefs z which provide
succinct descriptors of T. Although this information is not available during testing, it could be made
available, for example, if training is performed in a full-state simulation, orin the real-world but with
extra environment instrumentation. In theory, a perfect task descriptor is that which provides suffi-
♦	.	∙ i'	. ∙	.	1	.1 l ʌ z ʌ » « l ʌ l ʌ I-V T	1	1	1	.	.1.11	∙	T i' ZT-	.
cient information to close the POMDP. We model ground truth task descriptor b for T as a vector
b ∈ Rv where v is the dimension of the task descriptor. During training we wish to condition the
encoder to produce latent spaces Z 〜qφ(z∖cτ) which, when mapped into the dimensions of b, can
produce approximations of b. We use a VAE dλ(b∖Z) parameterised by λ as our auxiliary module,
which produces vectors bμ, bσ ∈ Rv and optimise it via a log-likelihood maximisation objective (8).
LaUx = -E(b,c)〜T,z〜qφ(z∣c) [lθg dλ(b = b∖z)]	(8)
Inference b 〜dλ(b∖z) from the trajectories in CT is decoupled from RL as gradients do not pass
through the actor or the critic. This stabilises the encoder by having a fixed supervised target, and
allows off-policy training. Leveraging auxiliary objectives for meta-RL was coined by Humplik
et al. (2019), where the authors condition task inference solely on LaTux to train RNN-based poli-
cies. However, in PERIL we use simple descriptors such that these solely condition the encoder to
converge towards a suggested direction. Thus, we extend G(T ∖Z) with the auxiliary loss to produce
the ultimate meta-objective (9). Details of the final computational graph is presented in Figure 8. By
keeping the critic loss LcTritic in G(T ∖Z), we give freedom for the encoder to perform unsupervised
learning so that it can structure representations ofZ which represent different dynamics and rewards:
ifwe solely base task inference on LaTux, a task such as screwing a bolt would output the same task
belief as a task of inserting a peg, even though the tasks are fundamentally different.
G(TIZ) - LTitic(Z)+ LTfo(Z) + LTux(Z)	⑼
3.5	Implementation
Meta-training requires sampling from a set of transitions (ot, at, rt, ot+1)tT=0 corresponding to a dis-
tribution of tasks T ∈ p(T) and optimising the meta-objective. We use task-specific replay buffers
BT to store collected transitions from which we can randomly sample during training. Following
primal inference objective outlined in section 3.1, task inference is initially conditioned via con-
textons retrieved from expert demonstrations. Consequently, we store a set of k demonstrations
generated by an expert policy on task-conditioned demonstration buffers DT . This expert policy
may consist of a human user or, in our case, a pre-trained SAC agent with access to the full state
of the environment. Meta-training algorithms are presented in Algorithms 1 and its process is illus-
trated in Figure 2. Further details of the meta-training and meta-testing processes in section A.2. We
augment DT with imperfect trajectories which lead to demo-like (and increased) sum of rewards.
This allows PERIL to: (i) be robust to imperfect demonstrations and surpass performance from the
expert; (ii) combat the overfitting nature of BC from a narrow set of k demonstrations. The agent
must also exploit posterior sampling to update its task belief in order to accomplish continuous adap-
tation objectives. To this end, we use exploratory data to update the task belief during adaptation and
set additional task-dependent encoder buffers XT which only stores recently collected contextons.
This mitigates distributional shift between off-policy training and on-policy adaptation.
4	Experiments
We leverage probabilistic meta-RL methods to find values for Z which reconstruct unobserved rep-
resentations of a new task. To that end, we focus on validating PERIL in environments where
the agent must explore in order to understand the dynamics, whilst also utilising sparse rewards to
identify the task intent. Furthermore, neural networks, are particularly susceptible to abrupt discon-
tinuities. Thus, it is of particular interest to leverage meta-training in situations as such. We devise
5
Under review as a conference paper at ICLR 2021
Algorithm 1 PERIL: Meta-training
Require: Batch of T training tasks {Ti}i=1,..,T, Ti ∈ p(T)
1:	Initialise πθ, Qθ, qφ, bλ and replay BiT , encoder XiT and demo buffers DiT for T ∈ {Ti}i=1,..,T
2:	while not done do
3:	for each Ti, i ∈ {1, ..., T} do	. Collect data
4:	Initialise context from demonstration ci = {τE }, TE 〜Di and clear encoder buffer
5:	for k = 1,...,K do
6:	Sample from context Z 〜qφ (z|ci)
7:	Roll out policy T 〜πθ(T|z). Add to Bi and	Xi.	Add to Di	if ER[τ]	> ER[τE]
8:	Update context ci - {ci ∪ Xi}, xi ∈ Xi
9:	for n = 1, ..., Nt do	. Training for Nt steps
10:	for each Ti, i ∈ {1, ..., T } do
11:	Sample buffers xi 〜Xi, te 〜Di, bi 〜Bi to form hybrid context ci = {te ∪ xi}
12:	SamPle Z 〜qφ(Zlci) and COmPUte losses Lmi, Lbc, Lactor Lcritic, LDKL , Laux
13:	Update parameters θπ , θQ , φ, λ via gradient descent
Figure 2: Meta-training framework. Environment contains a set of tasks T 〜P(T), each with its
own ground truth task descriptor b. Samples generated by the policy overwrite the encoder buffer
XT for the current task T. All samples are added to the replay buffer BT . concatenates context
from the demonstration and XT to produce a hybrid context cE . Red dotted line copies collected
transitions into the demonstration buffer if these result in higher rewards than the demonstrations.
tasks which involve contact-rich interactions and boundaries, factors which are considered to require
a higher level of perception of the environment (Zhu et al., 2020). More details on A.3.
Figure 3: Snapshots of the 5 different task families used. We present two (top and bottom) distinct
tasks for each task family: Reach2D, Stick2D, Peg2D, Key2D, Reach3D (left to right). A robotic
agent must navigate in order to find the unseen goal. In Stick2D and Peg2D the agent must insert
the effector. In Key2D the agent must also rotate the interactive (blue) handle to an arbitrary angle.
The different task families tested in this study are illustrated in Figure 3. Within a task family, one
single task is defined by the configuration of the environment, such as the pose of the object or
goal. When leveraging privileged information, this pose is used as the task descriptor. For each
task, demonstrations are provided from different robot initial poses, and the agent is then required
to perform that task, from any new robot initial pose. We compare our proposed methods to other
meta-RL and IL baselines. To ensure fairness in all tests, we used the same number of parame-
ters during training. Specifically, MetaIL (Yu et al.), extends PEARL by conditioning exploration
on demonstrations. We also use Noisy-BC as a baseline where the agent clones a demonstration
with additional noise (20% of p(o01∙)) . We also consider the case where privileged information is
unavailable, thus compare these baselines to both PERIL and PERIL-A. In PERIL we omit Laux .
6
Under review as a conference paper at ICLR 2021
4.1 Performance
We evaluate the performance of each method over 5 task families (Figure 4) using k = 3 demon-
strations per task. Since meta-learning requires trajectory-based adaptation, we record the averaged
trajectory return after 3 policy roll-outs. The results indicate that, in contrast to other baselines,
meta-learning without demonstrations through sparse reward feedback is particularly ineffective
(PEARL). On the other hand, PERIL-based methods significantly outperform MetaIL and Noisy-
BC, especially in tasks from Peg2D or Reach2D/3D where the agent must explore efficiently. It is
in these cases where the auxiliary module contributes greatest. On the other hand, in task families
such as Stick2D and Key2D, a few contact-rich interactions with the environment can quickly help
decipher the task. In this case, goal-oriented auxiliary targets are not as helpful.
EiUaM,19LaEaAV
0.0	2.5	5.0	7.5	10.0 12.5 15.0 175 20.0
Number βf eπvlraπmeπt steps (xlC4)
S∏Ck2□
0.0	2.5	5.0	7.5 IOA 12.5	15.0	17.5	20.0
Numbercfenvlronment steps (xlθ4)
Reach 3D	!⅛y2D
-14-
0	5	10	15 2β 25	30
Numbcrofenvlranment steps txlθ4)
ReaCh2D
-10
Easκ,aEaΛV
0.0	25	5.0	7.5	10.0	12.5	15.0	17.5 2CX)
Number of eπvlraπmeπt steps (XX0")
EBaM-S⅞L aE∂Λy
0.0	2.5	5»	7.5	10.0	12.5	15.0	17.5	20.0
Number of eπvlraπment steps (xi04)
Figure 4: Test-task performance vs. number of collected transitions during meta-training. By ex-
ploiting hybrid inference, PERIL consistently achieves better performance with respect to other
baselines. Auxiliary systems are particularly useful in complex tasks which require efficient explo-
ration. All results are 3-point averaged.
4.2	Increasing Generalisation
Most meta-RL research involves adaptation to tasks within a single task family (Rakelly et al.;
Duan et al.; Yu et al.). But in practice, we seek agents capable of interpolating from a diverse set of
dynamics. To test the representational power behind PERIL we evaluate training and adaptation per-
formance in multi-task family settings (Figure 5). Here, we train a single agent along each task from
from within the 2D task families. TSNE plots of the latent space distribution created during meta-
testing reveals PERIL-based methods produce structured task beliefs during hybrid inference of new
tasks. Moreover, this verifies PERIL’s long-term memory contribution. Last, it showcases PERIL
performs structured exploration at the multi-task level, allowing the agent to efficiently switch from
one macro policy to another and robustly inherit a diverse set of behaviours. Despite the effec-
tiveness of privileged information in complex exploration tasks in single task family adaptation,
auxiliary systems do not generally contribute as much during meta-multi-tasking. We believe that
task inference is substantially more convoluted in the multi-task family case as discerning dynam-
ics and intent requires more complex embeddings in the latent space than the position of the goal.
We also test generalisation by adapting PERIL in out-of-task distributions (Figure 6), where PERIL
interpolates from within meta-trained latent space representations to adapt to unseen dynamics.
Metrics during adaptation of unseen tasks reveal PERIL is substantially more efficient in exploration
with respect to other baselines, and is capable of zero-shot learning (Table 1), where the context from
the demonstration is sufficient to condition the agent to adapt to an unseen task. Note we record the
K-shot as the average kth adaptation trajectory until successful completion of the new task.
7
Under review as a conference paper at ICLR 2021
-5.0
-7.5
-10.0
-12.5
-15.0
-17.5
-20.0
-22.5
-25.0
u」n痘 ωra2ω><
10	20	30	40	50	60	70
Number of environment steps (xlθ4)
-35
u」n痘 ωra2ω><
10	20	30	40	50	60	70
Number of environment steps (xlθ4)
——PERIL
——PERIL-A
——PEARL
'——MetaIL
Expert
Noisy BC
Reach2D
■ ■
u-rual ωra2ω><
Table 1: Mean adaptation rate (K-shot) along
all task families. PERIL baselines demon-
strate superior adaptation to unseen tasks
both in adaptation speed and performance
and can endure zero-shot learning.
Model I	K-Min	I K-Mean ∣	K-Max
PEARL	7.2	∞	∞
MetaIL	1.6	5.1	10.2
PERIL	0	1.0	2.7
PERIL-A	0	0.9	3.1
NoiSy-BC	1.8	8.4	12.3
Figure 5: Test-task performance vs. number of collected transitions during meta-training (left and
middle). PERIL-based methods are capable of generalising within task families. Latent task dis-
tributions p(z) during adaptation (right) demonstrate MetaIL (top) is incapable of clustering tasks
within different task families. In contrast, PERIL (bottom) can discriminate them.
tsne-2d-one
Figure 6: TSNE plot of collected z on adaptation.
Stick2D is held out of meta-training and PERIL
interpolates dynamics from Key2D and Stick2D.
4.3	Alterations on K
We study the inherent meta-IL dependency over the number of
per-task expert demonstrations k. We find that increasing the
number of demonstrations per task slightly improves sample
efficiency but does not alter the asymptotic capacity of PERIL
(Figure 7). Improvement is small since, by augmenting the
demonstrations online, PERIL is capable of self-inducing ad-
ditional demos which improves sample efficiency.
Figure 7: Effect on k .
5 Conclusion
We have introduced PERIL, a new method for meta imitation and meta reinforcement learning which
builds a representation of a new task by conditioning on both demonstrations, and further explo-
ration. This provides a framework where new tasks can be defined naturally by a non-expert, without
requiring reward shaping or state-space engineering. Experiments across a range of tasks show our
method is able to adapt not only to novel instances within a task family, but also to entirely novel
task families, whilst doing so with superior data efficiency to a range of baselines. This provides
the foundation for further theoretical extensions of this framework, such as learning from imperfect
demonstrations, as well as further applications, such as real-world contact-rich robot manipulation.
8
Under review as a conference paper at ICLR 2021
References
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Under
review as a conference paper at ICLR 2017 RL 2 : FAST REINFORCEMENT LEARNING VIA
SLOW REINFORCEMENT LEARNING. Technical report.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adapta-
tion of Deep Networks. Technical report, 2017.
Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Meta-
Reinforcement Learning of Structured Exploration Strategies. Advances in Neural Information
Processing Systems, 2018-December:5302-5311, 2 2018. URL http://arxiv.org/abs/
1802.07245.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy
Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. Technical report.
Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, and
Nicolas Heess. Meta reinforcement learning as task inference. 5 2019. URL http://arxiv.
org/abs/1905.06424.
Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea Finn.
Guided Meta-Policy Search. Technical report.
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning Complex Dexterous Manipulation with Deep Reinforce-
ment Learning and Demonstrations. Technical report. URL http://sites.google.com/
view/deeprl-dexterous-manipulation.
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient Off-
Policy Meta-Reinforcement Learning via Probabilistic Context Variables. Technical report. URL
https://github.com/katerakelly/oyster.
StePhane Ross, Geoffrey J Gordon, J Andrew Bagnell, and Machine Learning. A Reduction of
Imitation Learning and Structured Prediction to No-Regret Online Learning. Technical report.
Gerrit Schoettler, Ashvin Nair, Juan AParicio Ojea, Sergey Levine, and Eugen Solowjow. Meta-
Reinforcement Learning for Robotic Industrial Insertion Tasks. 4 2020. URL http://arxiv.
org/abs/2004.14404.
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas
Heess, Thomas Rothorl, Thomas Lampe, and Martin Riedmiller. Leveraging Demonstrations
for DeeP Reinforcement Learning on Robotics Problems with SParse Rewards. 7 2017. URL
http://arxiv.org/abs/1707.08817.
Hanlei Wang. Adaptive Control of Robot Manipulators With Uncertain Kinematics and Dynamics.
Technical report, 2016.
Haozhe Wang and Jiale Zhou. Learning Context-aware Task Reasoning for Efficient Meta-
reinforcement Learning KEYWORDS Multitask Learning; Deep Reinforcement Learning. Tech-
nical report, 2020. URL www.ifaamas.org.
Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-Inverse Reinforcement Learning
with Probabilistic Context Variables. Technical report.
Allan Zhou, Eric Jang Google Brain, Daniel Kappler, Alex X Herzog, Mohi Khansari, Paul Wohlart,
Yunfei Bai, Mrinal X Kalakrishnan, Sergey Levine, and Chelsea Finn Google Brain. WATCH,
TRY, LEARN: META-LEARNING FROM DEMONSTRATIONS AND REWARDS. Technical
report. URL https://sites.google.com/view/watch-try-learn-project.
Henry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah, Kristian Hartikainen, Avi Singh, Vikash Ku-
mar, and Sergey Levine. The Ingredients of Real-World Robotic Reinforcement Learning. 4
2020. URL http://arxiv.org/abs/2004.12570.
9
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Mathematical Extensions
Probabilistic Kernel
We adopt a MaxEnt RL formulation to define a meta-objective for task T ∈ p(T) (10), where
H(∙∣o) represents the observational entropy, and the temperature α controls the weighting term for
the entropy-based reward.
π* = arg max E Ec〜T EEz〜p(z|c),(ot,at)〜∏Hr(Ot,at, Z) + aH(n(∙|ot, Z)]	(IO)
T∈p(T)	t∈T
Optimising the objective defined in (10) is possible by using the SAC soft policy iteration approach.
The difference to standard SAC is that in our case, the observation is augmented by Z resulting in
task-conditioned critic (11) and actor (12) losses for the the Q-function Qθ(∙) and the policy ∏θ(∙)
respectively. V corresponds to the frozen target value function (no gradients stored in the forward
pass).
Notice we use the term BT to represent the distribution of transitions in task T, which is modelled
by a replay buffer. Moreover, the over-line operator in Z denotes that gradients are detached. You
may seek further details on the derivation of actor and critic losses in Appendix B of (Haarnoja
et al.).
Lcritic = Ec~T ,z~qφ(z∣c),(o,a,r,O0)〜BT [Qθ(o, a, Z)-(T + V(o0,z))]	(II)
Lactor = Ec〜T,z〜qφ(z∣c),o〜BT,a〜∏θ(a∣o,z) DKL I πθ(a|o, Z)
exp(Qθ(o, a,Z))
Zθ(o)
))
(12)
Where Zθ(o) is the normalising partition function, which is intractable yet has no effect on the com-
putation of the gradients. In the computation of LTctor, the gradients for Z are detached, allowing
the policy loss to exploit the representation of the MDP passed on by the critic without conditioning
the inference network qφ(z∣c).
Observe in 11 we allow gradients from the inference process q : C -→φ Z to pass onto the
computation of Qθ(o, a, Z) such that meta-optimisation of the latent variable Z is conditioned on
reconstructing the Q-function.
Conditioning on Demonstrations
We minimise the conditioned mutual information term by enduring the following deconstruction
process.
min —I(z； T) + Ep(z) [Dkl(p∏e (τ|z)∣∣p(τ|z))] + Ep(T) [Dkl(p(z∣t) ∣∣qφ(z∣τ))]
φ
min
φ
Ep(z)[DκL(p∏E (τ ∣z)∣∣p(τ |z))] + Ep(z,τ) log
P(Z) l 1 P(Z|T)
P(ZIT)	ɑg qφ(Hτ)
(13)
min
φ
Ep(Z)[DKL(PnE (TIZ)IIP(T|z))] + Ez〜p(z),τ〜p(τ|z)
logqφ(ZIT)
In order to optimise the two terms in (13), we define Lbc and Linfo as the leftmost and rightmost
distribution expectancies respectively. In this definition, the expectancies are not differentiable and
requires parametric manipulation. The first step is to adapted this objective into a meta-RL form, by
approximating the conditionalP(TIZ) with the policyPθ(TIZ).
We define the first term Lbc(θ) as the conditional behavioural cloning loss.
Lbc = Ep(z)[DKL(PπE(TIZ)IIPθ(TIZ))]
(14)
10
Under review as a conference paper at ICLR 2021
By manipulation of the Kullback-Liebler divergence we obtain the following expansion.
min Ep(z)[Dkl(p∏e (T ∣z)∣∣Pθ (T |z)] = ^n Ez 〜「。但,)
θθ
logP∏E (T|z)
log pθ (τ |z)
min Ez 〜Pθ(z∣T ) logp^：Zz) =min Ez 〜Pθ(z∣T) h log PnE(T ⑶i - min Ez~Pθ (z∣τ) h log Pθ (τ |z)i
We then substitute policy ∏θ as the conditional distribution pθ(τ|z) and our variatonal inference
approximation for the posterior qφ(z∖τ):
minEz〜Pθ(z∣τ) [logP∏E (τ∖z)] - minEz~Pθ(z∣τ) log pθ (T ∖z)
=min H(PnE (τ ∖z)) - Ez 〜qφ(z∣τ) [log ∏θ (T ∖z)]
Because the entropy term is not dependent on the parameters θ, we derive a lower bound objective
which is identical to the maximum likelihood estimator for trajectory distributional matching in BC:
LT = -EipTE (T),z~qφ(z∣τ)[log πθ (T Iz)]
(15)
Note that in the final representation (15), expert trajectory distributions PnTE (T) are conditioned by
task T as would be the case during meta-training. To obtain a tractable form of Linfo we leverage
generative processes to switch sampling distributions as we do not have direct access to the prior
p(z) to sample z. To that end We sample from both expert distributions T 〜PnE (T) and the posterior
approximation (encoder) Z 〜qφ(z∣T) to arrive to (16).
-ET〜pTE ,z~qφ(z∣τ),Tθ~∏θ(T|z)
log qφ(z∖Tθ)
(16)
Notice hoW in LiTnfo We generate a set of trajectories Tθ from a task belief sampled from expert
demonstration trajectories. From this We aim to maximise the likelihood of matching the posteriors
on z given Tθ. In essence, this can be vieWed as a tWo-player game: We Want to match the z generated
by demonstrations to the z generated by a policy Which is also conditioned on the demonstrations.
In this form, LiTnfo is incompatible With off-policy RL as Tθ is sampled from the current policy πθ .
We propose truncated importance sampling by sampling from the distribution Which collected Tθ
(17). We use a behavioural policy πb(T∖z) Which is updated every episode as a frozen copy ofπθ.
Linfo = -ET~pTE ,z~qφ(zlτ),Tb~nb(TIz) min ( log∏(T⅛∣Z) , 1 ) log qφ(ZITb)	(17)
A.2 Algorithm Details
Computational Graph
Figure 8 illustrates the computational graph and training procedure of PERIL.
Meta Training
Algorithm 2 is a detailed extension of the meta-training algorithm presented in the main text.
11
Under review as a conference paper at ICLR 2021
Figure 8: Computational graph of PERIL. Loss gradients (defined by blue circles) propagate to
their corresponding models. These include the task encoder qφ, actor πθπ, critic QθQ, and auxiliary
module dλ. Black circles denote placeholders where latent task beliefs z can be updated (arrow head
pointing inwards) or evaluated (arrow head pointing outwards). The operator denotes summation
of the loss gradients. The colour of the arrows pointing outwards from the losses indicates the model
dependency for that loss. Notice critic and auxiliary losses are shared within two models. Last, the
black outlines denote the loss gradients which contribute to the meta-objective G(T |z).
Algorithm 2 PERIL: Meta-training
	Require: Batch of T training tasks {Ti}i=1,..,T , Ti ∈ p(T)
1: 2:	Initialise πθ, Qθ, qφ, bλ and απ, αQ, αφ, αb. Initialise replay BiT , encoder XiT and demo buffers DiT for T ∈ {Ti }i=1,..,T
3:	while not done do
4:	for each Ti, i ∈ {1, ..., T } do
5:	Initialise context by sampling demonstrations ci = {τE}, TE 〜Di
6: 7:	Xi — 0	. Clear task encoder to reduce on-policy mismatch for k = 1,...,Kprior do	. Demonstration-conditioned trajectories
8: 9:	Sample Z 〜qψ (z |ci) Collect trajectories T 〜πθ(τ|z) and add to Bi and Xi
10:	for k = 1,...,Kpost do	. Exploration-conditioned trajectories via posterior sampling
11:	Sample Z 〜qφ (z |ci)
12:	T 〜∏θ(τ|z), Bi — Bi ∪ T
13:	if ER [τ] > ER [τE] then
14:	Di —- {Di ∪ T }	. Demo augmentation
15:	ci —- {ci ∪ xi}, xi ∈	Xi
16:	for i = 1, ..., Nt do
17:	Sample R random training tasks from p(T)
18:	Clone behavioural policy πb —- πθ	. To apply TRIS
19:	for each Ti , i ∈ R do
20:	Sample recent trajectories Xi 〜Xi, te 〜Di, bi 〜Bi
21:	Form hybrid context ci = {TE ∪ xi }
22:	Sample latent task descriptor Z 〜qφ (z |ci)
23:	ComputelossesLimi,Libc,LiactorLicritic,LiDKL,Liaux
24:	φ ― φ - αΦ Vφ pi∈R (Lcritic+ LDKL + Lmi+ Laux)
25:	θQ — θ∏ - α∏VF Pi∈R Lcritic
26:	θπ —- θQ - αQVQ Pi∈R (Liactor + Libc)
27:	λ —- λ - αλVλ Pi∈R Liaux
12
Under review as a conference paper at ICLR 2021
Meta Testing
Algorithm 3 presentes the process behind meta-testing.
Algorithm 3 Meta-Test adapation
Require: Networks ∏θ, qφ, test task T 〜p(T), demonstration buffer DT,
1:	Initialise context by sampling k demonstrations C = {te}f 〜DT
2:	for r = 1, ..., R do
3:	Sample Z 〜qφ(z∖c)
4:	Roll out policy ∏θ(a∖o,z) for N steps and collect trajectory T = {(ot, at, rt, ot+ι)N=ι}
5:	Update context C — {c ∩ T}
A.3 Environments
2D task families are set up with Pymunk and the 3D reach task families are set up from MuJoCo’s
ReachEnv. Each task T ∈ p(T) also contains an initial observational distribution p0 (o)T. This
distribution defines how the robot is initialised in the environment before collecting data. Since in
this project we want to focus on meta-learning the dynamics of contact-rich tasks, we provide an
initial distribution p0(o)T which allows the agent to start relatively close to the interaction. We
summarise p0 (o)T in Table 2, where P0 and θ0 represent the initial effector position in Pymunk
units (PyU) (or meters if Reach3D) and angle (radians). Note that via inverse kinematics, the other
links of the robot are automatically set.
Table 2: Initial observational distribution for all task families, where E denotes the point of inter-
action and is specific to each task family. In Peg2D and Key2D, E is defined as the clearence hole
entry point, where in Stick2D, E is defined by the extrusion’s tip.
Task Family	Distribution
Peg2D, StiCk2D, Key2D Reach2D Reach3D	100 < ∖Po — E∖ < 250, ∖θo∖ <π∕4 250< ∖P0 -E∖ < 500, ∖θ0∖ < π 0.05 < ∖P0 - E∖ < 0.1
A.3.1 S paces
Each task has an accessible ground truth task descriptor b ∈ R2 which describes the exact position
in space of its goal G. In all 2D tasks, the base (origin) of the robotic manipulator is kept constant.
The first component of the observational space O is defined by the normalised relative position from
the tip of the end effector (P ∈ R2) to the origin Ω of the robot (18), where ω is the vector which
defines the width and height in PyU of the visualisation domain (1280, 720). The second component
of the observational space corresponds to the sine and cosines of the angle of the end effector θ. The
action space A gives complete freedom to the effector and it includes its cartesian velocity (vx, vy)
(PyU/s) and angular velocity ωθ (rad/s). Through inverse kinematics, the latter are translated back
to the other links. Observation and action spaces are summarised in Table 3.
orel = O(P)
P — Ω
ω
(18)
Table 3:	State-action spaces for all task families.
Feature
O
A
Description
{θXel,Oyel, COS θ, Sin θ} ∈ R4
{vx, vy, ωθ} ∈ R3
By definition of the observational space, the agent has no information about the position or condi-
tion of the goal. Hence, during adaptation the robot must seek to find the goal and identify the task
13
Under review as a conference paper at ICLR 2021
dynamics using its own interactions as well as the information given by the demonstration. Note that
in real world conditions, observation and action spaces could be augemented with kinematic compo-
nents such as forces and torques at the end effector. However, in this simulated environment, we will
show that deciphering the dynamics can be performed by using the aforementioned observational
and action spaces.
A.3.2 Reward Functions
Naive dense reward functions are provided during meta-training of the critic. These reward functions
are unshaped and give no information about the dynamics. In essence the reward function r(∙) is
defined as the absolute distance from the effector tip P to the goal G. The only exception is the
Key2D task family, where reward functions are augmented with additional incentives to twist the
handle to a desired angle α. Table 4 summarises the reward functions used in each task family.
Table 4:	Dense reward function used for critic meta-training for all task families in MetaSim.
Task Family
Peg2D, Reach2D, Reach3D, Stick2D
Key2D
Function
|G-P|
|G - P| + 0.5∣θ - α∣
On the other hand, We only provide binary sparse rewards r(∙) ∈ [0,1] to the task encoder. These
are 0 in all cases except when the effector has reached its goal, where we provide a reward of 1.
A.4 Hyper-parameters
Unless otherwise noted, the default hyperparameters for PERIL are as described below.
Training Loop
•	Action space |a|: 3
•	Observational space |o|: 4
•	Latent space |z |: 8
•	Maximum path length H : 60
•	Prior collection steps Kprior : 240
•	Posterior collection steps Kposterior : 240
•	RL batch size: 256
•	Encoder batch size: 64
•	Number of expert demonstrations K : 3
•	RL sparse rewards: False
•	Task belief sparse rewards: True
•	Maximum context size: 256
•	Number of pre-training steps: 1000
•	Number of training steps: 1000
•	Number of tasks to average gradients in meta-training R: 16
•	Number of tasks to collect data from each epoch: 32
•	Number of Epochs: 100
•	Discount factor γ: 0.98
•	Entropy temperature: 0.1
•	Soft target value = 0.005
•	Kullback-Liebler divergence weighting wKL : 0.1
•	Behavioural cloning weighting wbc : 0.5
14
Under review as a conference paper at ICLR 2021
•	Mutual information weighting wmi : 0.2
•	Auxilliary loss weighting waux : 4.0
Actor
•	Hidden layer size h: 256
•	Architecture: [|o, a, z|, h, h, 1]
•	Learning rate απ: 3 × 10-4
•	Output: Bounded action [-1,1] via tanh
Critic
•	Hidden layer size h: 256
•	Architecture: [|a, z|, h, h, 1]
•	Learning rate: αQ: 3 × 10-4
Encoder
•	Latent size: d
•	Input affinity architecture: [|o, a, r, o|, 256, 256, d]
•	Output affinity architecture: [d, 64, 64, |z |]
•	Learning rate: αq: 3 × 10-4
Belief Module
•	Task descriptor dimension |b|: 2
•	Architecture: [|z|, 32, 16, |b|]
•	Learning rate: αb: 3 × 10-4
15