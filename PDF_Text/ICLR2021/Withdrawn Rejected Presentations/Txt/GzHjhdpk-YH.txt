Under review as a conference paper at ICLR 2021
The Unbalanced Gromov Wasserstein Dis-
tance: Conic Formulation and Relaxation
Anonymous authors
Paper under double-blind review
Ab stract
Comparing metric measure spaces (i.e. a metric space endowed with a probability
distribution) is at the heart of many machine learning problems. This includes
for instance predicting properties of molecules in quantum chemistry or generat-
ing graphs with varying connectivity. The most popular distance between such
metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the
solution of a quadratic assignment problem. This distance has been successfully
applied to supervised learning and generative modeling, for applications as di-
verse as quantum chemistry or natural language processing. The GW distance
is however limited to the comparison of metric measure spaces endowed with a
probability distribution. This strong limitation is problematic for many applica-
tions in ML where there is no a priori natural normalization on the total mass of
the data. Furthermore, imposing an exact conservation of mass across spaces is
not robust to outliers and often leads to irregular matching. To alleviate these is-
sues, we introduce two Unbalanced Gromov-Wasserstein formulations: a distance
and a more tractable upper-bounding relaxation. They both allow the comparison
of metric spaces equipped with arbitrary positive measures up to isometries. The
first formulation is a positive and definite divergence based on a relaxation of the
mass conservation constraint using a novel type of quadratically-homogeneous
divergence. This divergence works hand in hand with the entropic regularization
approach which is popular to solve large scale optimal transport problems. We
show that the underlying non-convex optimization problem can be efficiently tack-
led using a highly parallelizable and GPU-friendly iterative scheme. The second
formulation is a distance between mm-spaces up to isometries based on a conic
lifting. Lastly, we provide numerical simulations to highlight the salient features
of the unbalanced divergence and its potential applications in ML.
1 Introduction
Comparing data distributions on different metric spaces is a basic problem in machine learning.
This class of problems is for instance at the heart of surfaces (Bronstein et al., 2006) or graph
matching (Xu et al., 2019) (equipping the surface or graph with its associated geodesic distance),
regression problems in quantum chemistry (Gilmer et al., 2017) (viewing the molecules as dis-
tributions of points in R3) and natural language processing (Grave et al., 2019; Alvarez-Melis &
Jaakkola, 2018) (where texts in different languages are embedded as points distributions in different
vector spaces).
Metric measure spaces. The mathematical way to formalize these problems is to model the data
as metric measure spaces (mm-spaces). A mm-sPace is denoted as X = (X, d, μ) where X is a
complete separable set endowed with a distance d and a positive Borel measure μ ∈ M+(X). For
instance, if X = (xi)i is a finite set of points, then μ = Pi miδχi (here δχi is the Dirac mass at Xi)
is simply a set of positive weights mi = μ({xi}) ≥ 0 associated to each point Xi, which accounts
for its mass or importance. For instance, setting some mi to 0 is equivalent to removing the point
Xi. We refer to Sturm (2012) for a mathematical account on the theory of mm-spaces.
In all the applications highlighted above, it makes sense to perform the comparisons up to iso-
metric transformations of the data. Two mm-spaces X = (X,dχ ,μ) and Y = (Y,dγ, ν) are
1
Under review as a conference paper at ICLR 2021
considered to be equal (denoted X ~ Y) if they are isometric, meaning that there is a bijection
ψ : spt(μ) → SPt(V) (where spt(μ) is the support of μ) such that dχ(x,y) = dγ(ψ(x),ψ(y))
and ψ]μ = V. Here ψ] is the push-forward operator, so that ψ*μ = V is equivalent to imposing
V(A) = μ(ψ-1(A)) for any set A ⊂ Y. For discrete spaces where μ = Pi miδχi, then one should
have v = ψ]μ = Pi m%δψ(χi). As highlighted by Memoli (2011), considering mm-spaces up to
isometry is a powerful way to formalize and analyze a wide variety of problems such as matching,
regression and classification of distributions of points belonging to different spaces. The key to
unlock all these problems is the computation of a distance between mm-spaces up to isometry. So
far, existing distances (reviewed below) assume that μ is a probability distribution, i.e. μ(X) = 1.
This constraint is not natural and sometimes problematic for most of the practical applications to
machine learning. The goal of this paper is to alleviate this restriction. We define for the first time a
class of distances between unbalanced metric measure spaces, these distances being upper-bounded
by divergences which can be approximated by an efficient numerical scheme.
Csiszar divergences The simplest case is when X = Y and one simply ignores the underly-
ing metric. One can then use CSiSzar divergences (or 夕-divergences), which perform a point-
wise comparison (in contrast with optimal transport distances, which perform a displacement
comparison). It is defined using an entropy function 夕：R+ → [0, +∞], which is a con-
vex, lower semi-continuous, positive function with 夕(1) = 0. The CSiSzar 夕-divergence reads
Dφ(μ∣v) =. Rχ 夕(ddV)dv + 夕∞ RX dμ⊥, where μ = 黑V + μ⊥ is the Lebesgue decomposition
of μ with respect to V and 夕∞ = limr→∞ 夕(r)/r ∈ R ∪ {+∞} is called the recession con-
stant. This divergence DW is convex, positive, 1-homogeneous and weak* lower-semicontinuous,
see Liero et al. (2015) for details. Particular instances of 夕-divergences are Kullback-Leibler (KL)
for 夕(r) = r log(r) - r + 1 (note that 夕∞ = ∞) and Total Variation (TV) for 夕(r) = |r - 11.
Balanced and unbalanced optimal transport. If the common embedding space X is equipped
with a distance d(x, y), one can use more elaborated methods such as optimal transport (OT) dis-
tances, which are computed by solving convex optimization problems. This type of methods has
proven useful for ML problems as diverse as domain adaptation (Courty et al., 2014), supervised
learning over histograms (Frogner et al., 2015) and unsupervised learning of generative models (Ar-
jovsky et al., 2017). In this case, the extension from probability distributions to arbitrary positive
measures (μ, V) ∈ M+(X)2 is now well understood and corresponds to the theory of unbalanced
OT. Following Liero et al. (2015); Chizat et al. (2018c), a family of unbalanced Wasserstein dis-
tances is defined by solving
UW(μ, V)q
def.
= inf
π∈M(X×X)
/ λ(d(x,y))dπ(x,y) + Dφ(∏ι ∣μ) + D.(∏2∣μ).
(1)
Here (π1 , π2 ) are the two marginals of the joint distribution π, defined by π1 (A) = π(A × Y) for
A ⊂ X. The mapping λ : R+ → R and exponent q ≥ 1 should be chosen wisely to ensure for
instance that UW defines a distance (see Section 2.2.1). It is frequent to take ρDW instead ofDW (i.e.
take ψ = P夕)to adjust the strength of the marginals, penalization. Balanced OT is retrieved with
the convex indicator 夕=∣{i} or by taking the limit P → +∞, which enforces ∏ι = μ and ∏2 = V.
When 0 < P < +∞, unbalanced OT operates a trade-off between transportation and creation of
mass, which is crucial to be robust to outliers in the data and to cope with mass variations in the
modes of the distributions. For supervised tasks, the value of P should be cross-validated to obtain
the best performances.
Its use is gaining popularity in applications, such as medical imaging registration (Feydy et al.,
2019a), videos (Lee et al., 2019), generative learning (Balaji et al., 2020) and gradient flow to
train neural networks (Chizat & Bach, 2018; Rotskoff et al., 2019). Furthermore, existing efficient
algorithms for balanced OT extend to this unbalanced problem. In particular Sinkhorn’s iterations,
introduced in ML for balanced OT by Cuturi (2013), extend to unbalanced OT (Chizat et al., 2018a;
SejOUme et al., 2019), as detailed in Section 3.
The Gromov-Wasserstein distance and its applications. The Gromov-Wasserstein (GW) dis-
tance (Memoli, 2011; Sturm, 2012) generalizes the notion of OT to the setting of mm-spaces up to
isometries. It corresponds to replacing the linear cost λ(d)dπ of OT by a quadratic function
GW(X,Y)q d=ef.	min
π∈M+(X×Y)
λ(ldχ(χ,χ0) - dγ(y,y0)l)dπ(χ,y)dπ(χ0,y0) ： ；； = μ ∖,⑵
2
Under review as a conference paper at ICLR 2021
It is proved in Memoli (2011); Sturm (2012) that GW defines with λ(t) = tq a distance UP to
isometries on balanced mm-spaces (i.e. the measures are probability distributions). In this paper, we
extend this construction to arbitrary positive measures, and provide explicit settings in Section 2.2.1.
This distance is applied successfully in natural language processing for unsupervised translation
learning (Grave et al., 2019; Alvarez-Melis & Jaakkola, 2018), in generative learning for objects
lying in spaces of different dimensions (Bunne et al., 2019) and to build VAE for graphs (Xu et al.,
2020). It has been adapted for domain adaptation over different spaces (Redko et al., 2020). It is also
a relevant distance to compute barycenters between graphs or shapes (Vayer et al., 2018; Chowdhury
& Needham, 2020). When (X , Y ) are Euclidean spaces, this distance compares distributions up to
rigid isometry, and is closely related (but not equal) to metrics defined by procrustes analysis (Grave
et al., 2019; Alvarez-Melis et al., 2019).
The problem (2) is non convex because the quadratic form J λ(∣dχ - dγ∣)dπ 0 π is not positive in
general. It is in fact closely related to quadratic assignment problems (Burkard et al., 1998), which
are used for graph matching problems, and are known to be NP-hard in general. Nevertheless, non-
convex optimization methods have been shown to be successful in practice to use GW distances
for ML problems. This includes for instance alternating minimization (Memoli, 2011; Redko et al.,
2020) and entropic regularization (Peyre et al., 2016; Gold & Rangarajan, 1996).
Related works and contributions. The concomitant work of De Ponti & Mondino (2020) extends
the Lp transportation distance defined in Sturm et al. (2006) to unbalanced mm-spaces and studies
its geometric properties. This distortion distance is not equivalent to the GW distance, and is more
difficult to estimate numerically because it explicitly imposes a triangle inequality constraint in the
optimization problem. The work of Chapel et al. (2020) relaxes the GW distance to the unbalanced
setting by hybridizing GW with partial OT (Figalli, 2010) for unsupervised labeling. It ressembles
one particular setting of our formulation, but with some important differences, detailed in Section 2.
Our construction is also connected to partial matching methods, which find numerous applications
in graphics and vision (Cosmo et al., 2016). In particular, Rodola et al. (2012) introduces a mass
conservation relaxation of the GW problem.
The two main contributions of this paper are the definition of two formulations relaxing the GW
distance. The first one is called the Unbalanced Gromov-Wasserstein (UGW) divergence and can
be computed efficiently on GPUs. The second one is called the Conic Gromov-Wasserstein distance
(CGW). It is proved to be a distance between mm-spaces endowed with positive measures up to
isometries, as stated in Theorem 1 which is the main theoretical result of this paper. We also prove
in Theorem 1 that UGW can be used as a surrogate upper-bounding CGW. We present those concepts
and their properties in Section 2. We also detail in Section 3 an efficient computational scheme for
a particular setting of UGW. This method computes an approximate stationary point of the non-
convex energy. It leverages the strength of entropic regularization and the Sinkhorn algorithm,
namely that it is GPU-friendly and defines smooth loss functions amenable to back-propagation
for ML applications. Section 4 provides some numerical experiments to highlight the qualitative
behavior of this algorithm, which shed some lights on the favorable properties of UGW to cope with
outliers and mass variations in the modes of the distributions.
2 Unbalanced Gromov-Wasserstein formulations
We present in this section our two new formulations and their properties. The first one, called UGW,
is exploited in Sections 3 and 4 to derive an efficient algorithm used in numerical experiments. The
second one, called CGW, defines a distance between mm-spaces up to isometries. Those results
build upon the results of Liero et al. (2015), anda summary of the construction of UOT is detailed in
Appendix A In all what follows, we consider complete separable mm-spaces endowed with a metric
and a positive measure.
2.1	The unbalanced Gromov-Wasserstein divergence
This new formulation makes use of quadratic 夕-divergences, defined as Dj (ρ∣ν) =. Dφ(ρ0ρ∣ν 0ν),
where ρ 0 ρ ∈ M+(X2) is the tensor product measure defined by d(ρ 0 ρ)(x, y) = dρ(x)dρ(y).
Note that Dj is not a convex function in general.
3
Under review as a conference paper at ICLR 2021
Definition 1 (Unbalanced GW). The Unbalanced Gromov-Wasserstein divergence is defined as
UGW(X,Y) = infπ∈M+(X×Y) L(π) where
L(π) d=ef.
X2×Y2
λ(∣dχ(x,x0) - dγ(y,y0)∣)dπ(x,y)dπ(x0,y0)+ D^(∏ι∣μ) + DI(∏2∣ν).
(3)
This definition can be understood as an hybridation between (1) and (2) but with a twist: one needs
to use the quadratic divergence D? in place of Dφ. To the best of our knowledge, it is the first time
such quadratic divergences are being used and studied. In the TV case, this is the most important
distinction between UGW and partial GW (Chapel et al., 2020). Note also that the balanced GW
distance (2) is recovered as a particular case when using 夕=∣= or by letting P → +∞ for an
entropy ψ = Pp
Using quadratic divergences results in UGW being 2-homogeneous: for θ ≥ 0, writing (Xθ, Yθ)
equiped with (θμ,θν), one has Θ-2UGW(Xθ, Yθ) = UGW(X, Y). When using non tensorized
夕-divergences, the resulting unbalanced Gromov-Wassertein functional between Xθ and Yθ have
very different and inconsistent behaviors when θ → 0 and θ → +∞. Indeed, once normalized by
θ-2 and θ-1, one obtains respectively balanced GW anda Hellinger-type distance. Using tensorized
divergences ensure that the behavior does not depends on θ.
We first prove the existence of optimal plans π solution to (3), which holds for the three key settings
of Section 2.2.1, namely for KL, TV, and for compact metric spaces (such as finite pointclouds and
graphs). All proofs are deferred in Appendix B.
Proposition 1 (Existence of minimizers). We assume that (X, Y) are compact and that either (i)夕
Superlinear i.e 夕∞ = ∞, or (ii) λ has compact SubleVel sets in R+ and 2夕∞ + inf λ > 0. Then
there exists π ∈ M+(X × Y) such that UGW(X, Y) = L(π).
The following proposition ensures that the functional UGW can be used to compare mm-spaces.
Proposition 2 (Definiteness OfUGW). Assume that 夕-1({0}) = {1} and λ-1({0}) = {0}. Then
UGW(X, Y) ≥ 0 and is 0 ifand only if X 〜Y.
We end this section with a reformulation of UGW (3) which is important to make the connection with
the second formulation of the following section. Its proof is deferred to Appendix B. It splits UGW
into two parts: the term 夕(0)(∣(μ 0 μ)⊥∣ + ∣(ν 0 V )⊥∣) accounts for the pure creation/destruction of
mass and a new transport cost Lc accounts for the remaining part (partial/pure transport and partial
creation/destruction of mass).
Lemma 1. Defining Lc(a,b) =. C + a夕(1/a) + b夕(1/b), and writing (f =. dd∏μ, g =. 匿)the
Lebesgue densities of (μ, V) w,r.t. (∏1,∏2) such that μ = f∏ι + μ⊥ and V = g∏2 + ν⊥, one has
L(π) =
X2 ×Y2
Lλ(∣dχ-dγ∣)(f 0 f,g 0 g)d∏d∏ + 3(0)(l(μ 0 μ)⊥l + l(ν 0 V)⊥∣).
(4)
2.2	The conic Gromov-Wasserstein distance
We introduce a second “conic” formulation of unbalanced GW, which is connected to UGW, and
whose construction is inspired by the conic formulation of UOT (see Appendix A for an overview).
2.2	. 1 Background on cone sets and distances
The conic formulation lifts a point x ∈ X to a couple (x, r) ∈ X × R+ where r encodes some
(power ofa) mass. Then we seek optimal transport plans defined over C[X] d=ef. X × R+/(X × {0}),
where coordinates (x, r = 0) with no mass are merged into a single point 0X called the apex of the
cone. In the sequel, points ofX × R+ are noted (x, r), while [x, r] are quotiented points of C[X].
While transport plans depend on variables ([x, r], [y, s]) and ([x0, r0], [y0, s0]) in C[X] × C[Y],
the transportation cost involved in our conic formulation only makes use of the 2-D cone
C[R+] over R+ endowed with the distance |u - v| (note that any other distance on R could
be used as well). More specifically, we consider coordinates of the form ([u, a], [v, b]) =
4
Under review as a conference paper at ICLR 2021
([dX (x, x0), rr0], [dY (y, y0), ss0]) ∈ C[R+] × C[R+]. Thus we now describe conic discrepancies
D on C[R+], which are defined for (p, q) ≥ 0 as
D([u,α], [v,b])q def. Hλ(∣u-v∣)(ap,bp) where Hc(ap,bp) = 财θLC(ap,豹
θ≥0
is the perspective transform of Lc introduced in Lemma 1. The intuition underpinning the definition
of this cost is that the perspective transform accounts for the possibility to rescale a transport plan
π by a scalar θ but the scaling is performed pointwise instead of globally. In general D is not a
distance, but it is always definite as stated by this result proved in Appendix A.
Proposition 3. Assume λ-1({0}) = {0},夕-1({0}) = {1} and 夕 is coercive. Then D is definite on
C[R+], i.e. D([u, a], [v, b]) = 0 if and only if (a = b = 0) or (a = b and u = v).
Of particular interest are those φ where D is a distance, which necessitates a careful choice of λ,p
and q. We now detail three examples where this is the case.
Gaussian Hellinger distance (GH). When DW = KL, λ(t) = t2 and q = P = 2, then one has
D([u, a], [v, b])2 = a2 +b2 -2abe-|u-v|/2. This cone distance (Burago et al., 2001) is further gener-
alized by De Ponti (2019) who shows that D is a distance for power entropies 夕(S)
if p ≥ 1 (the case p = 1 corresponding to DW = KL).
SP-P(S-1) — 1
P(P-I)
Hellinger-Kantorovich (HK) / Wasserstein-Fisher-Rao distance (WFR). When DW = KL,
λ(t) = — log cos2(t∧ 2) and q = P = 2, then one has D([u, a], [v,b])2 = a2 + b2 — 2ab cos(π ∧ ∣u —
v|). This construction, which might seem peculiar, corresponds to the one used to make unbalanced
OT a geodesic distance, as detailed in (Liero et al., 2015; Chizat et al., 2018c).
Partial optimal transport distance (PT). When DW = TV, λ(t) = tq, q ≥ 1 and P = 1, then
D([u, a], [v, b])q = a + b — (a ∧ b)(2 — |u — v|q)+ defines a cone distance (Chizat et al., 2018c).
2.2.2 Definitions and properties
The conic formulation consists in solving a GW problem on the cone, with the addition of two linear
constraints. Informally speaking, Lc from Lemma 1 becomes D, the term (∣(μ0μ)⊥ |+ |(V 0 ν)⊥D
is taken into account by the constraints (5) below, and the variables (f, g) are replaced by (rP, sP).
It reads CGW(X, Y) = infα∈up(μ,ν) H(α) where
H(α) d=ef.
Up(μ,ν) def.
D([dX (x, x0), rr0], [dY(y,	y0),	ss0])qdα([x,	r], [y, s])dα([x0, r0], [y0,	s0]),
(α ∈ M+(C[X ] X C [Y ])	:	Z	rpdαι (∙, r)	= μ, Z SP dα2(∙, S) =	V).
R+	R+
It is similar to the conic formulation of UW, see Appendix A. Note that similarly to the GW formu-
lation (2) 一 and in sharp contrast with the conic formulation of UW - here the transport plans are
defined on the cone C[X] × C[Y] but the cost D is a distance on C[R+].
We present now the main contributions of this paper, proved in Appendix C. We state that CGW
defines a distance under conditions that hold for the settings of Section 2.2.1, and that it is upper-
bounded by UGW. While the distance CGW1/q cannot be casted as a finite dimensional program
even in discrete settings (because it is defined on a lifted space), UGW can be approximated with
efficient numerical schemes as detailed in Section 3. The tightness of the bound between UGW and
CGW and the computation of CGW are open questions left for future works.
Theorem 1. (i) The divergence CGW is symmetric, positive and definite up to isometries. (ii) If D
is a distance on C[R+], then CGW1/q is a distance on the set of mm-spaces up to isometries. (iii)
For any (DW, λ, P, q) with associated cost D on the cone, one has UGW ≥ CGW.
Sketch of proof (i) CGW is positive and symmetric. Definiteness holds thanks to Proposition 3.
(ii) The triangle inequality is similar to balanced OT, and applies the gluing lemma (Villani, 2003,
Lemma 7.6). The non-trivial part is showing that the latter lemma holds, because it glues two plans
provided they have a common marginal. Since CGW is invariant under radial rescalings (called
dilations in Appendix C), it is possible to dilate two plans such that they have a common marginal
5
Under review as a conference paper at ICLR 2021
and remain optimal. (iii) Take an optimal plan π for UGW(X, Y). From this π one can build a plan
α such that L(π) ≥ H(α) because Lc ≥ Hc. Furthermore α ∈ Up, and is thus admissible and
suboptimal, which yields UGW(X, Y) = L(π) ≥ H(α) ≥ CGW(X, Y).
3 Algorithms
The computation of the distance CGW is in practice out-of-reach because it requires an optimiza-
tion over a lifted conic space, which would need to be discretized. We focus in this section on the
numerical computation of the upper bound UGW, using an alternate minimization coupled with
entropic regularization. The algorithm is presented on arbitrary measures, the special case of dis-
crete measures being a particular case. The discretized formulas and algorithms are detailed in
Appendix D, see also Chizat et al.(2018a); Peyre et al. (2016). All implementations are available at
https://github.com/anonymous-conference-submission.
In order to derive a simple numerical approximation scheme, following Memoli (2011), We intro-
duce a lower bound obtained by introducing two transportation plans. To further accelerate the
method and enable GPU-friendly iterations, similarly to Gold et al. (1996); Solomon et al. (2016),
we consider an entropic regularization. It reads, for any ε ≥ 0,
UGWε(X, Y) =. inf L(∏) + εKL0(π∣μ 0 V) ≥ inf F(π, Y) + εKL(π 0 γ∣(μ 0 V尸2),	(6)
and
F(π,γ)d=ef.
X2
λ(∣dχ — dγ∣)dπ 0 γ + Dφ(∏ι 0 γι∣μ 0 μ) + Dφ(∏2 0 γ2∣ν 0 V),
where (γ1, γ2) denote the marginals of the plan γ. Note that in contrast to the entropic regularization
of GW Peyre et al. (2016), here we use a tensorized entropy to maintain the overall homogeneity of
the energy. A simple method to approximate this lower bound is to perform an alternate minimiza-
tion on π and γ, which is known to converge for smooth 夕 to a stationary point since the coupling
term in the functional is smooth (Tseng, 2001).
Note that if π 0 Y is optimal then so is (sπ) 0 (ɪ Y) with S ≥ 0. Thus without loss of generality we
optimize under the constraint m(π) = m(γ) by setting S = Vzm(Y)/m(∏) ∙ In general, this bound
is not expected to be tight, but empirically, alternate minimization often converges to a solution with
π = Y (as already observed for instance in Rangarajan et al. (1999); Solomon et al. (2016)), so
that the algorithm also finds a local minimizer of the UGWε problem. In the Balanced-GW case
in Euclidean spaces, the optimum is known to satisfy π = Y (Konno, 1976) and alternate descent
is equivalent to a mirror descent algorithm (Solomon et al., 2016). Minimizing the lower bound
of (6) with respect to either ∏ or Y is non-trivial for an arbitrary 夕.We restrict our attention to the
Kullback-Leibler case DW = PKL with P > 0, which can be addressed by solving a regularized and
convex unbalanced problem as studied in Chizat et al. (2018a); SejOUme et al. (2019). It is explained
in the following proposition.
Proposition 4. For a fixed Y, the optimal π ∈ arg min F (∏,y) + εKL(π 0 Y ∣(μ 0 v产2) is the
π
solution of min / cγ(x, y)dπ(x, y) + Pm(Y)KL(∏ι∣μ) + Pm(Y)KL(∏2∣v) + εm(τ)KL(π∣μ 0 v),
πγ
def.
where m(Y) = Y(X × Y ) is the mass of Y, and where we define the cost associated to Y as
CY (χ,y) = R λ(IdX(X, ∙) — dγ (y, ∙)l)dY+PR log( ddμ1 )dYi+PR log( ddV2 )dY2+ε R log( 瑞V )dY∙
Computing the cost cεγ for spaces X and Y of n points has in general a cost O(n4) in time and
memory. However, as explained for instance in Peyre et al. (2016), for the special case λ(t) = t2,
this cost is reduced to O(n3) in time and O(n2) in memory. This is the setting we consider in the
numerical simulations. This makes the method applicable for scales of the order of 104 points. For
larger datasets one should use approximation schemes such as hierarchical approaches (Xu et al.,
2019) or Nystrom compression of the kernel (Altschuler et al., 2018).
The resulting alternate minimization method is detailed in Algorithm 1, see Appendix D for a dis-
cretized version. It uses the unbalanced Sinkhorn algorithm of Chizat et al. (2018a); SejOUrne et al.
(2019) as sub-iterations and is initialized using π = μ 0 v//m(μ)m(v). This Sinkhorn algorithm
operates over a pair of continuous functions (so-called Kantorovitch potentials) f(x) and g(y). For
discrete spaces X and Y of size n, these functions are stored in vectors of size n, and that integral
6
Under review as a conference paper at ICLR 2021
Algorithm 1 - UGW(X, Y, ρ, ε)
Input: mm-spaces (X, Y), relaxation ρ, regularization ε
Output: approximation (π, γ) minimizing 6
1:	Initialize π = Y = μ 0 ν∕/m(μ)m(ν), g = 0.
2:	while (π, γ)has not converged do
3:	Update π J Y, then C J c∏, P J m(π)ρ, ε J m(π)ε
4:	while (f, g) has not converged do
5:	∀x,	f(x) J ∣+i log (R e(g(y)-c(χ,y)"εdν(y))
6:	∀y,	g(y) J——εε+ρ log (R e(f(x)-c(x,y)”ε⅛μ(x))
7：	Update	γ(x, y) J exp [(f (x) + g(y) — c(x, y))∕ε] μ(x)ν(y)
8:	Rescale Y J ʌ/m(n)/m(Y)Y
9:	Return (π, Y ).
involved in the updates becomes a sum. Each iteration of Sinkhorn thus has a cost n2, and all the in-
volved operation can be efficiently mapped to parallelizable GPU routines as detailed in Chizat et al.
(2018a); SejoUme et al. (2019). Another advantage of using an unbalanced Sinkhorn algorithm is its
complexity O(n2∕ε) to compute an ε-approximation, as stated in Pham et al. (2020), which should
be compared to O(n2∕ε2) operations for balanced Sinkhorn.
Note also that balanced GW is recovered as a special case when setting P → +∞, so that P∕(M +
P) → 1 should be used in the iterations. In order to speed up Sinkhorn inner-loops, especially for
small values of ε, one can use linear extrapolation (Thibault et al., 2017) or non-linear Anderson
acceleration (Scieur et al., 2016).
There is an extra scaling step after computing Y involving the mass m(π). It corresponds to the
scaling s ofπ0Y such that m(π) = m(Y), and we observe that this scaling is key not only to impose
this mass equality but also to stabilize the algorithm. Otherwise we observed that m(Y) < 1 < m(π)
and underflows whenever m(Y) → 0 and m(π) → ∞.
4	Numerical experiments
This section presents numerical simulations on synthetic examples, to highlight the qualitative be-
havior of UGW with respect to mass variation and outliers. In all these experiments, μ and V are
probability distributions, which allows us to compare GW with UGW.
Robustness to imbalanced classes. In this first example, we take X = Y = R2 and consider
E , C and S to be uniform distributions on an ellipse, a disk and a square. Figure 1 contrasts the
transportation plan obtained by GW and UGW for a fixed μ = 0.5E + 0.5C and V obtained using
two different mixtures ofE and S. The black segments show the largest entries of the transportation
matrix π, for a sub-sampled set of points (to ease visibility), thus effectively displaying the match-
ing induced by the plan. Furthermore, the width of the dots are scaled according to the mass of the
marginals ∏ι ≈ μ and ∏ ≈ ν, i.e. the smaller the point, the smaller is the amount of transported
mass. This figure shows that the exact conservation of mass imposed by GW leads to a poor geomet-
rical matching of the shapes which have different global mass. As this should be expected, UGW
recovers coherent matchings. We suspect the alternate minimization algorithm was able to find the
global minimum in these cases.
Influence of ε and debiasing. This figure (and the following ones) does not show the influence
of ε. This parameter is set of a low value ε = 10-2 on a domain [0, 1]2 so as to approximate the
optimal plan of the unregularized UGW problem . The impact of ε is similar to those of classical
OT, namely that it introduces an extra diffusion bias.
Robustness to outlier Figure 2 shows another experiment on a2-D dataset, using the same display
convention as Figure 1. It corresponds to the two moons dataset with additional outliers (displayed
7
Under review as a conference paper at ICLR 2021
GW	UGW	GW	UGW
Figure 1: GW vs. UGW transportation plan, using ν = 0.3E+0.7S on the left, and ν = 0.7E+0.3S
on the right.
GW	UGW, ρ = 100	UGW,ρ= 10-1	UW,ρ = 10-2
Figure 2: GW and UGW applied to two moons with outliers.
in cyan). Decreasing the value of ρ (thus allowing for more mass creation/destruction in place of
transportation) is able to reduce and even remove the influence of the outliers, as expected. Further-
more, using small values of ρ tends to favor “local structures”, which is a behavior quite different
from UW (1). Indeed, for UW, ρ → 0 sets to zero all the mass of π outside of the diagonal (points
are not transported), while for UGW, it is rather pairs of points with dissimilar pairwise distances
which cannot be transported together.
Graph matching and comparison with Partial-GW. We now consider two graphs (X, Y )
equipped with their respective geodesic distances. These graphs correspond to points embedded
in R2, and the length of the edges corresponds to their Euclidean length. These two synthetic graphs
are close to be isometric, but differ by addition or modification of small sub-structures. The colors
c(x) are defined on the “source” graph X and are mapped by an optimal plan π on y ∈ Y to a
color n(y)JX c(x)dπ(x, y). This allows to visualize the matching induced by GW and UGW for a
varying ρ, as displayed in Figure 3. The graphs for GW should be taken as reference since there is
no mass creation. The POT library (Flamary & Courty, 2017) is used to compute GW.
For large values of ρ, UGW behaves similarly to GW, thus producing irregular matchings which
do not preserve the overall geometry of the shapes. In sharp contrast, for smaller values of ρ (e.g.
ρ = 10-1), some fine scale structures (such as the target’s small circle) are discarded, and UGW is
able to produce a meaningful partial matching of the graphs. For intermediate values (ρ = 100), we
observe that the two branches and the blue cluster of the source are correctly matched to the target,
while for GW the blue points are scattered because of the marginal constraint.
Figure 4 shows a comparison with Partial-GW (Chapel et al., 2020), computed using the POT library.
It is close to UGW with a TV0 penalty, since partial OT is equivalent to the USe of a TV relaxation
of the marginal. UGW with a KL0 penalty is first computed for a given ρ, then the total mass m
of the optimal plan is computed, and is used as a parameter for PGW which imposes this total mass
as a constraint. Figure 3 and 4 display the transportation strategy associated to both methods. KL-
UGW operates smooth transitions between transportation and creation of mass, while PGW either
performs pure transportation or pure destruction/creation of mass. This can be observed in Figure 4
where nodes of the graphs are removed and not taken into account by the matching. Note also that
since PGW is equivalent to solving GW on sub-graphs, the color distribution of GW and PGW are
the same.
8
Under review as a conference paper at ICLR 2021
X 8⅛0∞ Xsmjbi
ρ=0.1
ρ=1
ρ= 10	GW(ρ= ∞)
Figure 3: Comparison of UGW and GW for graph matching.
X乡nos X 一缺回
Figure 4: Comparison of Partial-GW for graph matching. Here m is the budget of transported mass.
5	Conclusion and perspectives
This paper defines two Unbalanced Gromov-Wasserstein formulations. We prove that they are both
positive and definite. We provide a scalable, GPU-friendly algorithm to compute one of them, and
show that the other is a distance between mm-spaces up to isometry. These divergences and distances
allow for the first time to blend in a seamless way the transportation geometry of GW with creation
and destruction of mass. This hybridization is the key to unlock both theoretical and practical issues.
This work opens new questions for futures works. On the theoretical side, the geodesic structures
induced by unbalanced GW distances and divergences is an important subject of study. On the
practical side, removing the bias introduced by the use of entropic regularization is important for
applications to ML. Note that such a debiasing was successfully applied for Balanced-GW in Bunne
et al. (2019) and is shown to lead to a valid divergence for balanced OT in Feydy et al. (2019b) and
UW in Sejourne et al. (2019). The design of efficient numerical solvers for the the conic formulation
is also an interesting avenue for future works.
9
Under review as a conference paper at ICLR 2021
References
Jason Altschuler, Francis Bach, Alessandro Rudi, and Jonathan Weed. Massively scalable Sinkhorn
distances via the nystr\” om method. arXiv preprint arXiv:1812.05189, 2018.
David Alvarez-Melis and Tommi S Jaakkola. Gromov-wasserstein alignment of word embedding
spaces. arXiv preprint arXiv:1809.00013, 2018.
David Alvarez-Melis, Stefanie Jegelka, and Tommi S Jaakkola. Towards optimal transport with
global invariances. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp.1870-1879. PMLR, 2019.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein GAN. arXiv preprint
arXiv:1701.07875, 2017.
Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications in
generative modeling and domain adaptation. Advances in Neural Information Processing Systems,
33, 2020.
Alexander M Bronstein, Michael M Bronstein, and Ron Kimmel. Generalized multidimensional
scaling: a framework for isometry-invariant partial surface matching. Proceedings of the National
Academy of Sciences, 103(5):1168-1172, 2006.
Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative
models across incomparable spaces. arXiv preprint arXiv:1905.05461, 2019.
Dmitri Burago, Iu D Burago, Yuri Burago, Sergei A Ivanov, and Sergei Ivanov. A course in metric
geometry, volume 33. American Mathematical Soc., 2001.
Rainer E Burkard, Eranda Cela, Panos M Pardalos, and Leonidas S Pitsoulis. The quadratic assign-
ment problem. In Handbook of combinatorial optimization, pp. 1713-1809. Springer, 1998.
Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial gromov-wasserstein with applications
on positive-unlabeled learning. arXiv preprint arXiv:2002.08276, 2020.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Scaling algorithms
for unbalanced transport problems. to appear in Mathematics of Computation, 2018a.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. An interpolating
distance between optimal transport and fisher-rao metrics. Foundations of Computational Math-
ematics, 18(1):1-44, 2018b.
LenaIC Chizat, Gabriel Peyre, Bernhard Schmitzer, and FranCois-Xavier Vialard. Unbalanced opti-
mal transport: Dynamic and kantorovich formulations. Journal of Functional Analysis, 274(11):
3090-3123, 2018c.
Samir Chowdhury and Tom Needham. Gromov-wasserstein averaging in a riemannian framework.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Work-
shops, pp. 842-843, 2020.
Luca Cosmo, Emanuele Rodola, Michael M Bronstein, Andrea Torsello, Daniel Cremers, and
Y Sahillioglu. Shrec’16: Partial matching of deformable shapes. Proc. 3DOR, 2(9):12, 2016.
Nicolas Courty, Remi Flamary, and Devis Tuia. Domain adaptation with regularized optimal
transport. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pp. 274-289. Springer, 2014.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Adv. in Neural
Information Processing Systems, pp. 2292-2300, 2013.
10
Under review as a conference paper at ICLR 2021
Nicolo De Ponti. Metric properties of homogeneous and spatially inhomogeneous f -divergences.
IEEE Transactions on Information Theory, 66(5):2872-2890, 2019.
Nicolo De Ponti and Andrea Mondino. Entropy-transport distances between unbalanced metric
measure spaces. arXiv preprint arXiv:2009.10636, 2020.
Jean Feydy, Pierre Roussillon, Alain Trouve, and Pietro Gori. Fast and scalable optimal transport
for brain tractograms. In International Conference on Medical Image Computing and Computer-
Assisted Intervention, pp. 636-644. Springer, 2019a.
Jean Feydy, Thibault Sejourne, FranCoiS-Xavier Vialard, Shun-ichi Amari, Alain Trouve, and
Gabriel Peyre. Interpolating between optimal transport and mmd using sinkhorn divergences.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2681-2690,
2019b.
Alessio Figalli. The optimal partial transport problem. Archive for rational mechanics and analysis,
195(2):533-560, 2010.
Remi Flamary and Nicolas Courty. Pot python optimal transport library. GitHub: https://github.
com/rflamary/POT, 2017.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A Poggio. Learn-
ing with a Wasserstein loss. In Advances in Neural Information Processing Systems, pp. 2053-
2061, 2015.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Steven Gold and Anand Rangarajan. A graduated assignment algorithm for graph matching. IEEE
Transactions on pattern analysis and machine intelligence, 18(4):377-388, 1996.
Steven Gold, Anand Rangarajan, et al. Softmax to softassign: Neural network algorithms for com-
binatorial optimization. Journal of Artificial Neural Networks, 2(4):381-399, 1996.
Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised alignment of embeddings with
wasserstein procrustes. In The 22nd International Conference on Artificial Intelligence and Statis-
tics, pp. 1880-1890, 2019.
Hiroshi Konno. Maximization of a convex quadratic function under linear constraints. Mathematical
programming, 11(1):117-127, 1976.
John Lee, Nicholas P Bertrand, and Christopher J Rozell. Parallel unbalanced optimal transport
regularization for large scale imaging problems. arXiv preprint arXiv:1909.00149, 2019.
Matthias Liero, Alexander Mielke, and Giuseppe Savare. Optimal entropy-transport problems and
a new hellinger-kantorovich distance between positive measures. Inventiones mathematicae, pp.
1-149, 2015.
Matthias Liero, Alexander Mielke, and Giuseppe Savare. Optimal transport in competition with re-
action: The hellinger-kantorovich distance and geodesic curves. SIAM Journal on Mathematical
Analysis, 48(4):2869-2911, 2016.
Facundo Memoli. Gromov-wasserstein distances and the metric approach to object matching. Foun-
dations of computational mathematics, 11(4):417-487, 2011.
Gabriel Peyre, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and
distance matrices. In International Conference on Machine Learning, pp. 2664-2672, 2016.
Khiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport:
An analysis of sinkhorn algorithm. arXiv preprint arXiv:2002.03293, 2020.
Anand Rangarajan, Alan Yuille, and Eric Mjolsness. Convergence properties of the softassign
quadratic assignment algorithm. Neural Computation, 11(6):1455-1474, 1999.
11
Under review as a conference paper at ICLR 2021
Ivegen Redko, TitoUan Vayer, Remi Flamary, and Nicolas Courty. Co-optimal transport. arXiv
preprint arXiv:2002.03731, 2020.
Emanuele Rodola, Alex M Bronstein, Andrea Albarelli, Filippo Bergamasco, and Andrea Torsello.
A game-theoretic approach to deformable shape matching. In 2012 IEEE Conference on Com-
Puter Vision and Pattern Recognition, pp. 182-189. IEEE, 2012.
Grant Rotskoff, Samy Jelassi, Joan Bruna, and Eric Vanden-Eijnden. Global convergence of neuron
birth-death dynamics. arXiv preprint arXiv:1902.01843, 2019.
Damien Scieur, Alexandre d’Aspremont, and Francis Bach. Regularized nonlinear acceleration. In
Advances In Neural Information Processing Systems, pp. 712-720, 2016.
Thibault Sejourne, Jean Feydy, Francois-Xavier Vialard, Alain Trouve, and Gabriel Peyre. Sinkhorn
divergences for unbalanced optimal transport. arXiv preprint arXiv:1910.12958, 2019.
J. Solomon, G. Peyre, V. Kim, and S. Sra. Entropic metric alignment for correspondence problems.
ACM Transactions on Graphics (TOG), 35(4), 2016.
Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient flows on the space of
metric measure spaces. arXiv preprint arXiv:1208.0434, 2012.
Karl-Theodor Sturm et al. On the geometry of metric measure spaces. Acta mathematica, 196(1):
65-131, 2006.
Alexis Thibault, LenaIc Chizat, Charles Dossal, and Nicolas Papadakis. Overrelaxed Sinkhorn-
Knopp algorithm for regularized optimal transport. arXiv preprint arXiv:1711.01851, 2017.
Paul Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization.
Journal of optimization theory and applications, 109(3):475-494, 2001.
Titouan Vayer, Laetita Chapel, Remi Flamary, Romain Tavenard, and Nicolas Courty. Fused
gromov-wasserstein distance for structured objects: theoretical foundations and mathematical
properties. arXiv preprint arXiv:1811.02834, 2018.
Cedric Villani. Topics in C. Transportation. Graduate studies in Math. AMS, 2003.
Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph
partitioning and matching. In Advances in neural information processing systems, pp. 3046-
3056, 2019.
Hongteng Xu, Dixin Luo, Ricardo Henao, Svati Shah, and Lawrence Carin. Learning autoencoders
with relational regularization. arXiv preprint arXiv:2002.02913, 2020.
12
Under review as a conference paper at ICLR 2021
A Background on unbalanced optimal transport
Following Liero et al. (2015), this section reviews and generalizes the homogeneous and conic
formulations of unbalanced optimal transport. These three formulations are equal in the convex
setting of UOT. Our relaxed divergence UGW and conic distance CGW defined in Section 2 build
upon those constructions but are not anymore equal due to the non-convexity of GW problems.
A. 1 Homogeneous formulation
To ease the description of the homogeneous formulation, We develop and refactor the Csiszar diver-
gence terms of (1) in a form analog to Lemma 1. It reads
UW(μ,ν)q =	inf 2, L Lλ(d(χ,y))(f (χ),g(y))dπ(x,y) + ψ∞(Iμ⊥l + IV⊥l),	(J)
π∈M(X2)
where Lc(T, S) =. C + rφ(1∕r) + s^(ɪ/s), ∣μ⊥∣ =. μ⊥(X) and (f =. 黑,g =. 黑)are the
densities of the Lebesgue decomposition of (μ, V) with respect to (∏ι, ∏2) and
μ = f∏ι + μ⊥ and V = g∏2 + ν⊥.	(8)
Such form is helpful to explicit the terms of pure mass creation/destruction (∣μ⊥ ∣ + ∣ν ⊥∣) and rein-
terpret the integral under π as a transport term with a new cost Lλ(d).
Then the authors of Liero et al. (2015) define the homogeneous formulations HUW as
HUW(μ,ν )q =.	inf	H Ηλ(d(χ,y))(f (x),g(y))dπ(χ,y) + ψ∞ (|〃」+ IV ⊥}
π∈M(X2)
where the 1-homogeneous function Hc is the perspective transform of Lc
Hc(r, S) =. inf θ(c + Ψ(θ) + Ψ(S)) = inf OLc(W, S).
θ≥0	θ≥0
By definition one has Lc ≥Hc, though after optimization one has UW = HUW.
(9)
(10)
A.2 Cone sets, cone distances and explicit settings
The conic formulation detailed in Section A.3 is obtained by performing the optimal transport on
the cone set C[X] d=ef. X × R+/(X × {0}), where the extra coordinate accounts for the mass of the
particle. Coordinates of the form (x, 0) are merged into a single point called the apex of the cone,
noted 0X. In the sequel, points of X × R+ are noted (x, r) and those of C[X] are noted [x, r] to
emphasize the quotient operation at the apex.
For a pair (p, q) ∈ R+, we define for any [x, r], [y, S] ∈ C[X]2
DC[X]([x,r],[y,S])qd=ef.Hλ(d(x,y))(rp,Sp).	(11)
In general DC[X] is not a distance, but it is always definite as proved by the following result.
Proposition 5. Assume that d is definite, λ-1({0}) = {0} and 夕-1({0}) = {1}. Assume also that
for any (r, s) ,there always exists θ* such that Hc(r, s) = θ*Lc( θr,言).Then Dc[x] is definite on
C[X], i.e. DC[X] ([x, r], [y, S]) = 0 if and only if(r = S = 0) or (r = S and x = y).
Proof. Assume Dcχ] ([x, r], [y, s]) = 0, and write θ* such that
Dc[x]([x, r], [y, s])q = θ*Lc(θ, Sp) = θ*λ(d(x, y)) + rpφ(烁)+ S以S),
where the last line is given by the definition of reverse entropy. There are two cases. If θ* > 0,
since all terms are positive, there are all equal to 0. By definiteness of d it yields x = y and
because 夕-1({0}) = {1} we have rp = Sp = θ* and r = s. If θ* = 0 then Dc[χ] ([x,r], [y, s])q =
夕(0)(rp + sp). The assumption 夕-1({0}) = {1} implies 夕(0) > 0, thus necessarily r = S = 0. 口
The function Hc can be computed in closed form for a certain number of common entropies 夕,
and we refer to Liero et al. (2015, Section 5) for an overview. Of particular interest are those 夕
where DC[X] is a distance, which necessitates a careful choice of λ, p and q. We now detail three
particular settings where this is the case. In each setting we provide (Dφ, λ,p, q) and its associated
cone distance DC[X] .
13
Under review as a conference paper at ICLR 2021
Gaussian Hellinger distance It corresponds to
DW = KL,	λ(t) = t2 and q = P = 2,
DC[X] ([x, r], [y, s])2 = r2 + s2 - 2rse-d(x,y)/2,
in which case it is proved in Liero et al. (2015) that DC[X] is a cone distance.
Hellinger-Kantorovich / Wasserstein-Fisher-Rao distance It reads
DW = KL,	λ(t) = — logcos2 (t ∧ ∏2) and q = P = 2,
Dc[x] ([x, r], [y, s])2 = Tr + s2 - 2rs cos(∏2 ∧ d(x, y)),
in which case it is proved in Burago et al. (2001) that DC[X] is a cone distance.
The weight λ(t) = - log cos2 (t ∧ ∏), which might Seem more peculiar, is in fact the penalty that
makes unbalanced OT a length space induced by the Gaussian-Hellinger distance (if the ground
metric d is itself geodesic), as proved in Liero et al. (2016); Chizat et al. (2018b). This weight
introduces a cut-off, because λ(d(x, y)) = +∞ if d(x, y) > π∕2. There is no transport between
points too far from each other. The choice of n/2 is arbitrary, and can be modified by scaling
λ → λ(∙∕s) for some cutoff s.
Partial optimal transport It corresponds to
DW = TV,	λ(t) = tq and q ≥ 1 and P = 1,
DC[X]([x, r], [y,s])q = r+ s - (r∧ s)(2 - d(x, y)q)+,
in which case it is proved in Chizat et al. (2018c) thatDC[X] is a cone distance. The case DW = TV is
equivalent to partial unbalanced OT, which produces discontinuities (because of the non-smoothness
of the divergence) between regions of the supports which are being transported and regions where
mass is being destroyed/created. Note that Liero et al. (2015) do not mention that this DC[X] defines
a distance, so this result is new to the best of our knowledge, although it can be proved without a
conic lifting that partial OT defines a distance as explained in Chizat et al. (2018c).
A.3 Conic formulation of UW
The last formulation reinterprets UW as an OT problem on the cone, with the addition of two linear
constraints. Informally speaking, Hc becomes Dc[x], the term (∣μ⊥| + ∣ν⊥∣) is taken into account
by the constraints (13) below, and the variables (f, g) are replaced by (rp, sp). It reads
CUW(μ, V)q =. a∈Unfμ,ν) / DC[X]([x, r], [y, S]))qdα([x, r], [y, s]),
where the constraint setUp(μ, V) is defined as
Up(μ,ν) =. < α ∈M+(C[X]2) : ( rpdαι(∙,r) = μ, [ spdα2(∙,s) = ν > .
R+	R+
(12)
(13)
Thus CUW consists in minimizing the Wasserstein distance WDC[X] (α1, α2) on the cone
(C[X], DC[X]). The additional constraints on (α1, α2) mean that the lift of the mass on the cone
must be consistent with the total mass of (μ, V). When Dcχ] is a distance, CUW inherits the metric
properties of WDC[X] . Our theoretical results rely on an analog construction for GW.
The following proposition states the equality of the three formulations and recapitulates its main
properties. The proofs are detailed in Liero et al. (2015).
Proposition 6 (From Liero et al. (2015)). One has UW = HUW = CUW, which are symmet-
ric, positive and definite. Furthermore, if (X, dX ) and (C[X], DC[X] ) are metric spaces with X
separable, then M+(X) endowed with CUW is a metric space.
Proof. The equality UW = HUW is given by Liero et al. (2015, Theorem 5.8), while the equality
HUW = CUW holds thanks to Liero et al. (2015, Theorem 6.7 and Remark 7.5), where the latter
14
Under review as a conference paper at ICLR 2021
theorem can be straightforwardly generalized to any cone distance built as in Section 2.2.1. Since
DC[X] is symmetric, positive and definite (see Proposition 3), then so is CUW. Furthermore, if
DC[X] satisfies the triangle inequality, separability of X allows to apply the gluing lemma (Liero
et al., 2015, Corollary 7.14) which generalizes to any exponent P defining Up (μ, V) and any cone
distance Dc[x].	□
B UGW formulation and definitenes s
We present in this section the proofs of the properties of our divergence UGW. We refer to Sec-
tion 2 for the definition of the UGW formulation and its related concepts. For conciseness we write
Γ(x, x0, y, y0) = |dX(x,x0) - dY (y, y0)|.
We first start with the existence of minimizers stated in Proposition 1. It illustrates in some sense
that our divergence is well-defined.
Proposition 7 (Existence of minimizers). Assume(X, Y) to be compact mm-spaces and that we
either have
1	.夕 Superlinear i.e 夕∞ = ∞
2	. λ has compact SubleVel sets in R+ and 2夕∞ + inf λ > 0
Then there exists π ∈ M+(X × Y ) such that UGW(X, Y) = L(π).
Proof. We adapt here from Liero et al. (2015, Theorem 3.3). The functional is lower semi-
continuous as a sum of l.s.c terms. Thus it suffices to have relative compactness of the set of min-
imizers. Under either one of the assumptions, coercivity of the functional holds thanks to Jensen’s
inequality
L(π) ≥ m(π)2 inf λT) + m(μ)2ψ(m(π)2) + m(V)2以m(π)2)
m(μ)2	m(ν)2
2	/ ∖2∣^∙ r` e∖ m(μ)2 ,m(π)2.	m(ν )2 .m(π)2 .ι
≥ m(n) inf λ(F) +	Γʒ2 2(-Γ)+ ) +	Γʒ2 2(-r)∖2 ) ∙
L	m(π)2	m(μ)2	m(π)2	m(ν)2」
As m(π) → +∞ the right hand side converges to 2夕∞ + inf λ > 0, which under either one of the
assumptions yields L(π) → +∞, hence the coercivity. Thus we can assume there exists some M
such that m(π) < M. Since the spaces are assumed to be compact, the Banach-Alaoglu theorem
holds and gives relative compactness in M+(X × Y ).
Take any sequence of plans πn that approaches UGW(X, Y) = inf L(π). Compactness gives that
a subsequence ∏^ weak* converges to some ∏*. Because L is l.s.c, We have L(∏*) ≤ inf L(∏),
thus L(∏*) = inf L(∏). The existence of such limit reaching the infimum gives the existence of a
minimizer.	□
Note that this formulation is nonegative and symmetric because the functional L is also nonega-
tive and symmetric in its inputs (X , Y). This formulation allows straightforwardly to prove the
definiteness of UGW.
Proposition 8 (Definiteness OfUGW). Assume that 夕-1({0}) = {1} and λ-1({0}) = {0}. The
following assertions are equivalent:
1.	UGW(X, Y) = 0
2.	∃π ∈ M+ (X X Y) whose marginals are (μ, V) such that dχ (x, x0) = dγ (y, y0) for π 0 π-
a.e. (x, x0, y, y0) ∈ (X × Y)2.
3.	There exists a mm-space (Z, dZ, η) with full support and Borel maps ψX : Z → X and
ψγ : Z → Y. such that (ψχ)]η = μ, (ψγ)]η = V and dz = (ψχ)]dχ = (ψY)]dY
4.	There exists a Borel measurable bijection between the measures' supports ψ : sρt(μ) →
Spt(V) with Borel measurable inverse such that ψ]μ = V and dγ = ψ]dχ.
15
Under review as a conference paper at ICLR 2021
Proof. Recall that (2) ⇔ (3) ⇔ (4) from Sturm (2012, Lemma 1.10). thus it remains to prove
(1) ⇔ (2).
If there is such coupling plan ∏ between (μ, V) then one has ∏ 0 n-a.e. that Γ = 0, and all 夕-
divergences are zero as well, yielding a distance of zero a.e.
Assume now that UGW(X, Y) = 0, and write π an optimal plan. All terms of L are positive, thus
Under our assumptions we have Γ = 0, ∏ι 0 ∏ι = μ 0 μ and ∏ 0 ∏ = V 0 V. Thus we get that π
has marginals (μ, V) and that dχ(x, x0) = dγ(y, y0) π 0 π-a.e.	□
We end with a result on the reformulation of UGW which is the first step to connnect it with the
conic formulation CGW.
Lemma 2. Defining Lc(r, S) =. C + r夕(1/r) + S夕(1/s) ,and writing (f =. 条,g = 黑)the
Lebesgue densities of (μ, V) w.r.t. (∏1,∏2) such that μ = f∏ι + μ⊥ and V = g∏2 + ν⊥, one has
L(π) =
X2 ×γ2
Lλ(r)(f 0 f,g 0 g)dπdπ + ^(0)(∣(μ 0 μ)⊥∣ + |(v 0 V)⊥∣).
Proof. Using Equation (23), one has
L(π)
/
X2×γ2
Z
X2×γ2
Z
X2×γ2
λ(Γ)dπdπ + D1(∏ι ∣μ) + D^(∏2∣v )
λ(Γ)dπdπ + D 氯 μ∣∏ι)+ D 飘 V∣∏2)
λ(Γ)dπdπ +	ψ(f 0 f)dπ1 dπ1 +	ψ(g 0 g)dπ2dπ2
X2	γ2
+ (Xo)(I(〃 0 μ)⊥l + I(V 0 v)⊥D
L Lλ(r)(f 0 f,g 0 g)d∏dπ + ((0)(∣(μ 0 μ)⊥∣ + ∣(v 0 V)⊥∣).
X2×γ2
□
C Conic formulation and metric properties
We present in this section the proofs of the properties mentioned in Section 2. We refer to Section 2
and Appendix A for the definition of the conic formulation and its related concepts.
In this section we frequently use the notion of marginal for neasures. For any sets E, F, we write
P(E) : E × F → E the canonical projection such that for any (x, y) ∈ E × F, P(E) (x, y) = x.
Consider two complete separable mm-spaces X = (X, dχ, μ) and Y = (Y,dγ, v). Write ∏ ∈
M+(X × Y ) a coupling plan, and define its marginals by π1 = P(]X)π and π2 = P(]γ)π. The
definition of the marginals can also be seen by the use of test functions. In the case ofπ1 it reads for
any test function ξ
/
ξ(x)dπ1(x)
ξ(x)dπ(x, y).
C.1 Preliminary results
We present in this section concepts and properties which are necessary for the proof of Theorem 1.
We introduce a dilation operator whose role is to rescale the radial coordinate of a measure with a
given scaling.
Definition 2 (dilations). Consider v([x, r], [y, S]) a Borel measurable scaling function depending
on [x, r], [y, S] ∈ C[X] × C[Y ]. Take a plan α ∈ M+ (C[X] × C[Y ]). We define the dilation
Dilv : α 7→ (hv)](vpα) where
hv ([x, r], [y, S]) d=ef. ([x, r/w], [y, S/w]),
16
Under review as a conference paper at ICLR 2021
where w = v([x, r], [y, s]). It reads for any test function ξ
ξ([x, r], [y, s])dDilv(α) =
ξ([x, r/w], [y, s∕w])wpdɑ.
The importance of dilations is given by the following lemma.
Lemma 3 (Invariance to dilation). The problem CGW is invariant to dilations, i.e. for any α ∈
Up(μ, V), we have Dilv (a) ∈ Up(μ, ν) and H(α) = H(Dilv (α)).
Proof. First We prove the stability of Up(μ, V) under dilations. Take α ∈ Up(μ,ν). For any test
function ξ defined on X we have
ξ(x)(r)p.vpd(α) = J ξ(x)rpda = J ξ(x)dμ(x).
ξ(x)rpdDilv(α) =
Similarly we get P(]Y) (sqDilv (α)) =
ν, thus Dilv(a) ∈ Up(μ, V).
It remains to prove the invariance of the functional. Recall that Dq is p-homogeneous. It yields
H(Dilv (α)) =
=Z
D([dX (x, x0), rr0], [dY (y, y0), ss0]))qdDilv(α)dDilv(α)
D([dχ(x,x0), r ∙ — ], [dγ(y,y0), s ∙ - ]))qvp ∙ vpdadα
vv
vv
D([dX (x, x0), rr0], [dY(y, y0), ss0]))q v 2p dαdα
=	D([dX (x, x0), rr0], [dY (y, y0), --0]))qdαdα
= H(α)
Both the functional and the constraint set are invariant, thus the Whole CGW problem is invariant to
dilations.
□
The above lemma alloWs to normalize the plan such that one of its marginal is fixed to some value.
Fixing a marginal alloWs to generalize the gluing lemma Which is a key ingredient of the triangle
inequality in optimal transport.
Lemma 4 (Normalization lemma). Assume there exists α ∈ Up(μ,ν) such that CGW(X, Y)=
H(α). Then there exists a such that α ∈ Up(μ, ν) and CGW(X, Y) = H(4) and whose marginal
on C[Y] is νc[γ] = P(CY])]α = δ°γ + p](ν 0 δι), where P is the canonical injectionfrom Y X R+
to C[Y].
Proof. The proof is exactly the same as Liero et al. (2015, Lemma 7.10) and is included for com-
pleteness. Take an optimal plan α. Because the functional and the constraints are homogeneous in
(r, s), the plan α = α + δ0χ 0 δ°γ verifies α ∈ Up(μ, V) and H(α) = H(α). Indeed, because of
this homogeneity the contribution δ0X 0 δ0Y has (r, -) = (0, 0) Which has thus no impact.
Considering α instead of α allows to assume without loss of generality that the transport plan charges
the apex, i.e. setting
S = {[x, r], [y,]∈ C[X] × C[Y], [y,]= 0Y},	(14)
one has ωγ =. α(S) ≥ 1. Then we can define the following scaling
v([x,r],[y,-])= --if1-/q> 0
ωY	otherwise.
(15)
We prove now that Dilv (α) has the desired marginal on C(Y) by considering test functions ξ([y,s]).
We separate the integral into two parts with the set S, and write α = α∣s + α卜0 their restrictions
17
Under review as a conference paper at ICLR 2021
to S and Sc respectively. It reads
/ ξ([y,s])dDilv (α) = /
=Z
ξ([y,s∕v])vpdα
g([y,s/VDvpd a|s + / ξ([y, s/VDvpd a|sc
/ξ(0γ)ω-1d α∣s + /ξ([y,s∕SDSpd 6|$。
ξ(θγ) ∙ ωγ ∙ ωγ1 + / ξ([y, 1])spdα
ξ(0γ) + / ξ(p(y, s))d(ν(y) X δι(s))
/ ξ([y, s])d(δ°γ + p](ν X δι)),
which is the formula of the desired marginal on C[Y]. Since α ∈ Up(μ, V), its dilation is also in
Up(μ,ν),and HO H(α) = H(Dilv(α)).	□
C.1.1 Proof of Theorem 1
Non-negativity and symmetry hold since H is a sum of non-negative symmetric terms. To prove
Definiteness, assume CGW(X , Y) = 0, and write α an optimal plan. We have α X α-a.e. that
dX (x, x0) = dY (y, y0) and rr0 = SS0 because D is definite (see Proposition 3). Thanks to the
completeness of (X, Y) and a result from Sturm (2012, Lemma 1.10), such property implies the
existence of a Borel isometric bijection with Borel inverse between the supports of the measures
ψ : Suρρ(μ) → SuPP(V), where SuPP denotes the support. The bijection ψ verifies dχ(x, x0)=
dγ(ψ(x), ψ(x0)). To prove X 〜 Y it remains to prove ψ*μ = ν. Due to the density of continuous
functions of the form ξ(x)ξ(x0), the constraints of Up(μ, V) are equivalent to
J (rr0)pdaι(∙,r)dαι(∙,r0) = μ X μ, J (ss0)pdα2(∙,s)dɑ2(∙,s0) = V X ν.
Take a continuous test function ξ defined on SuPP(V)2. Writing y = ψ(x) and y0 = ψ(x0), one has
ξ(y, y0)dVdV =	ξ(y, y0)(SS0)pdαdα ξ(ψ(x), ψ(x0))(SS0)pdαdα ξ(ψ(x), ψ(x0))(rr0)pdαdα 二 /ξ(ψ(x),ψ(x0))dμdμ J ξ(x,x0)dψ]μdψ]μ.
Since ψ is a bijection, there is a bijection between continuous functions ξ ofSuPP(V)2 and functions
ξ of Supp(μ)2. Thus We obtain V = ψ*μ and We have X 〜Y.
It remains to prove the triangle inequality. Assume now that D satisfies it. Given three mm-spaces
(X, Y, Z) respectively equipped with measures (μ, v, η), consider α, β which are optimal plans for
CGW(X, Y) and CGW(Y, Z). Using Lemma 4 to both a and β, we can consider measures (α, β)
which are also optimal and have a common marginal V on C [Y ]. Thanks to this common marginal
and the separability of (X, Y, Z), the standard gluing lemma (Villani, 2003, Lemma 7.6) applies and
yields a glued plan γ ∈ M+(C[X] × C[Y] × C[Z]) whose respective marginals on C[X] × C[Y] and
C[Y] X C[Z] are (α, β). Furthermore, the marginal V of Y on C[X] X C[Z] is in Up(μ, η). Indeed,
(γV, αV) have the same marginal on C[X] and same for (γV, βV) on C[Z], hence this property. Write
18
Under review as a conference paper at ICLR 2021
dX = dX (x, x0) for sake of conciseness (and similarly for Y, Z). The calculation reads
1
CGW(X, Z)q	(16)
1
≤ (/ D([dχ,rr0], [dz,tt0])qd彳([x,r], [z,t])dγ([x0,r0], [z0,t0])) q	(17)
1
≤ (/DadX,rr'], [dz,tt0])qdγ([χ, r], [y, s], [z, t])dγ([χ0, r0], [y0, s0], [z0,t0]))q (18)
1
≤ ( J (D([dχ,rr0],[dγ, ss0]) + D([dγ, ss0], [dz,tt0]))qdγdγ) q	(19)
≤ (Z D([dχ,rr0], [dγ,ss0])qdγdγ) q + (Z D([dγ,ss0], [dz,tt0])qdγdγ) q	(20)
1
≤ (/DadX,rr0],[dY, ss0])qdα([χ,r],[y, sDdα([χ0,r0],[y0,s0D)q
1
+ U D([dγ,ss0],[dz,tt0])qdβ([y,s],[z,t])dβ([y0,s0],[z0,t0]))q	(21)
1	1
≤ CGW(X,Y)q + CGW(Y, Z)q.	(22)
Since Y ∈ Up(μ, η), it is thus SUboPtimal, which yields Equation (17). Because Y is the marginal
of γ we get Equation (18). Equations (19) and (20) are respectively obtained by the triangle and
Minkowski inequalities, which hold because D which is a distance. Equation (21) is the marginal-
ization of Y, and Equation (22) is given by the optimality of (α, β), which ends the proof of the
triangle inequality.
C.1.2 Proof of the inequality between UGW and CGW
The proof consists in considering an optimal plan π for UGW, building a lift α of this plan into the
cone such that L(π) ≥ H(α), and prove that α is admissible for the program CGW, thus suboptimal.
Using Equation (8), we have
μ 0 μ =(f 0 f )∏1 0 ∏1 + (μ 0 μ)⊥,
(μ 0 μ)⊥ = μ⊥ 0 (f∏1) + (f∏1) 0 μ⊥ + μ⊥ 0 μ⊥,
ν 0 ν = (g 0 g)π2 0 π2 + (ν 0 ν)⊥,
(ν 0 ν)⊥ = ν⊥ 0 (gπ2) + (gπ2) 0 ν⊥ + ν⊥ 0 ν⊥ .
(23)
Recall that the canonic injection p reads p(x, r) = [x, r]. Based on the above Lebesgue decomposi-
tion, we define the conic plan
α = (p(x, f (x)P), p(y,	g(y) 1))]π(x, y)	+ δ°χ	0 p] [ν⊥	0 δ1] + p] [μ⊥	0	δ1]	0 δ°γ.	(24)
We have that α ∈ Up(μ, ν). Indeed for the first marginal (and similarly for the second) We have for
any test function ξ(x)
ξ(x)f(x)dπ1 (x) +0+	ξ(x)(1)pdμ⊥(x)
ξ(x)d(f(x)π1 + μ⊥)
ξ(x)(r)pdα =
=Z
=Z
ξ(x)dμ(x).
We define θ* = θ*(r, S) the parameter which verifies Hc(r, S) = θ*Lc(r∕θ*, s/θ*). We restrict
α 0 a to the set S = {01(「)((rr0)p, (ss0)p) > 0}. By construction, θC(r, s) is 1-homogeneous in
(r, S). Thus on S we necessarily have r, r0, S, S0 > 0. It yields
α 0 a|s = (P(X,f (X) 1), P(y,g(y)1 ),P(X0,f (XO) 1), P(y0,g(y0)1 ))](π 0 π).
19
Under review as a conference paper at ICLR 2021
Concerning the orthogonal part of the decomposition, note that whenever θ* = 0, due to the defini-
tion of H the cone distance reads
D([x,r], [y,s])q =以0)(rp + SP)
(25)
It geometrically means that the shortest path between [x, r] and [y, s] must pass via the apex, which
corresponds to a pure mass creation/destruction regime.
Furthermore we have that
l(μ乳
∣(ν 0
μ)⊥l = /
V )⊥l=Z
(r ∙ r0)pd (α 乳 α)∣Sc ,
(S ∙ s0)pd (α 0 α)∣sc .
Indeed, thanks to Equation (24) we have for the first marginal that
∣(μ 0μ)⊥∣ = (μ⊥ 0 (f∏ι) + (f∏ι) 0μ⊥ + μ⊥ 0 μ⊥)(X2)
=/(rr0)pdp][μ⊥ 0 δι]dp(x0,f (x0) 1 )]∏ι(x0)
+
+
/(rr0)pdp(x,f(x) 1 )]∏ι(x)dp][μ⊥ 0 δι]
/(rr0)pdp][μ⊥ 0 δι]dp] [μ⊥ 0 δι]
=/(rr0)pd (α 0 @)孱.
Note that the last equality holds because each term of α 0 α involving a measure δ0X cancels out
when integrated against (rr0)p.
Eventually the computation gives (thanks to Lemma 1)
L(π)
L Lλ(Γ)(f 0 f,g 0 g)d∏dπ + 夕(0)(∣(μ 0 μ)⊥∣ + ∣(ν 0 V)⊥∣)
X2 ×Y2
≥ / Hλ(Γ)(f 0 f,g 0 g)d∏dπ + 夕(0)(∣(μ 0 μ)⊥∣ + ∣(ν 0 V)⊥∣)
≥ /DadX(X,χO), (f 0 f) 1 ], [dγ(y,y0), (g0 g) 1 Dqdπdπ
+ /2(0)(rr0)pd (a 0 @)院 + /2(0)(ss0)pd (a 0 @)屋
≥ /D([dχ(x,x0),rr0], [dγ(y,y0),ss0])qd (α 0 α)∣s
+ / 夕(0)((rr0)p + (ss0)p)d (α 0 a)院
≥	D([dX (x, x0), rr0], [dY(y, y0), SS0])qdαdα
≥ H(α).
ThuswehaveUGW(X,Y) = L(π) ≥ H(α) ≥ CGW(X,Y).
D Algorithmic details and formulas
D. 1 Decomposition of KL quadratic divergence
We present in this section an additional property on the quadratic-KL divergence which allows
to reduce the computational burden to evaluate it by involving the computation of a standard KL
divergence.
20
Under review as a conference paper at ICLR 2021
Proposition 9. For any measures (μ, V) ∈ M+(X), one has
KL(μ 0 v∣ɑ 0 β) = m(ν)KL(μ∣α) + m(μ)KL(ν∣β)
+ (m(μ) — m(α))(m(v) — m(β)).
In particular,
KL(μ 0 μ∣v 0 V) = 2m(μ)KL(μ∣v) + (m(μ) — m(v))2.
(26)
(27)
Proof. Assuming KL(μ 0 ν∣ɑ 0 β) to be finite, one has μ = fα and V = gβ. It reads
KL(μ 0 v∣ɑ 0 β)
log(f 0 g)dμdV — m(μ)m(V) + m(α)m(β)
m(V)
log(f)dμ + m(μ)
log(g)dV
— m(μ)m(V) + m(α)m(β)
m(v) [KL(μ∣α) + m(μ) — m(α)]
+ m(μ) [KL(v∣β) + m(v) — m(β)]
— m(μ)m(V) + m(α)m(β)
m(v )KL(μ∣α) + m(μ)KL(v ∣β)
+ m(μ)m(V) — m(V)m(α) — m(μ)m(β) + m(α)m(β)
m(v )KL(μ∣α) + m(μ)KL(v ∣β)
+ (m(μ) — m(α))(m(V) — m(β)).
□
We now prove Proposition 4 which applies the above result.
Proposition 10. For a fixed Y, the optimal π ∈ arg min F (π, Y) + εKL(π 0 γ∣(μ 0 V 产2) is the
π
solution of min J Cγ (x, y)dπ(x, y) + Pm(Y)KL(∏ι∣μ) + Pm(Y)KL(∏2∣v) + εm(γ)KL(π∣μ 0 v),
where m(Y) d=ef. Y(X × Y ) is the total mass ofY, and where we define the cost and weight associated
to Y as
cY (x,y) =./ λ(r(x, ∙,y, ∙))dγ + ρ /
)dγ1+PhgddY )dγ2+Jb虱 a )dγ.
Proof. First note that F(Y, π) = F(π, Y) so that minimizing with the first or the second argument
gives the same solution. The rest follows from the relation
KL(∏1 0 γι∣μ 0 μ) = m(γ)KL(∏ι∣μ) + m(π)KL(γι∣μ) + (m(γ) — m(μ))(m(π) — m(μ)),
and also from KL(∏ι∣μ) = ʃlog(dγ1 )dγι — (m(γ) — m(μ)). Similar formulas hold for (∏2, γ2)
and (∏,γ).	□
D.2 Discrete setting and formulas
In order to implement those algorithms, one consider discrete mm-spaces X = (xi)in=1 and Y =
(yj)jm=1, endowed with discrete measures μ = Pi μiδxi and V = Pj Vjδyj, where μi, Vj ≥ 0. The
distance matrices are DiX,i0 d=ef. dX(xi, xi0) and DjX,j0 d=ef. dX (yj, yj0). Transport plan are thus also
discrete π = Pi,j πi,jδ(xi,yj).
The functional L now reads in this discrete setting
(dχ(x,x0) — dγ(y,y0))2d∏(x,y)d∏(x0,y0)= X (DXj- Dh)2∏i,k∏j,',
i,j,k,`
21
Under review as a conference paper at ICLR 2021
and KL(∏ι	0 ∏ι∣μ	0 μ)=	Σlog	(:…1 )π1,iπ1,j	Σ π1,iπ1,j	+ Σ μiμj
. ∙	'	μiμj /	--	--
i,j	i,j	i,j
=2m(π) Xlog (π1* 1)∏ι,i — m(∏)2 + m(μ)2,
where We define the marginals ∏ι,k =. Pj ∏k,j, ∏2,' =. Pi ∏i,' and m(π) = Pij ∏i,j.
When one runs the stabilized implementation of Sinkhorn’s iterations with a ground cost Ci,j =
C(xi, yj) between the points, it is necessary to use a Log-Sum-Exp reduction which reads
fi4----ε + ρLSEj [(gj - Cij "ε + log(μj)]	(28)
where LSEj is a reduction performed on the index j . It reads
LSEj (Ci,j) d=ef. log X exp(Ci,j - maxCi,k) + maxCi,k,	(29)
where the logarithm and exponential are pointwise operations.
Algorithm 2 - UGW(X, Y, ρ, ε) in discrete form
Input: mm-spaces X = (Dχj, (μi)i) and Y = (Dγj, (Vj)j), relaxation ρ, regularization ε
Output: approximation (π, γ ) minimizing 6
1:	Initialize matrix πi,j = γi,j = μiνj/ J(Pi μi)(Pj νj), vector gjs 6=0) = 0.
2:	while π has not converged do
3:	Update ∏ J Y
4:	Define m(π) — Ei j πi,j, P — m(π)ρ, M — m(π)ε
5:	Define c J ComputeCost(X, Y, π, ρ, ε)
6:	while (f, g) has not converged do
7:	f j -ε+¾ log Pj exp ((gj — ci,j"巨+log Vj)]
8:	g J——εq⅞ log PieXp ((fi — ci,j)/巨 + log μi)
9:	Update γi,j J exp [(fi + gj — ci,j)/巨]μiνj
10:	Rescale Y J ,m(π)∕m(γ)γ
11:	Return (π, γ ).
We also provide an algorithm that computes the cost cεπ defined in Proposition (10). We focus on
the case DW = PKL and λ(t) = t2 which is computable with complexity O(n3 4) as shown in Peyre
et al. (2016). Indeed, note that one has
(dX(x,x0) — dY(y, y0))2dπ(x0, y0) =	dX(x,x0)2dπ1(x0) +	dY (y, y0)2dπ2(y0)
— 2	dX (x, x0)dY (y, y0)dπ(x0, y0).
Algorithm 3 - ComputeCost(X, Y, π, P, ε) in discrete form
Input: mm-spaces X = (Dχj, (μi)i) and Y = (D^`, (νj)j), transport matrix (∏j,k)j,k, relaxation
P, regularization ε
Output: cost cεπ defined in Proposition 10
1: Compute π1,j J Pk πj,k and π2,k J Pj πj,k	. π1 = π1 and π2 = π> 1
2: Compute Ai J Pj(DX-)2∏ι,j	.A = (DXyt2∏∖
3: Compute b` J Pk(D晨)2∏2,k	.B = (DY)^2∏2
4: COmPUte Ci,' J Pj DXj ( Pk D晨πj,k)	.C = DXπDY
5: COmPUte E J PPjlog (∏1j)πι,j + PPk log (π2kk)π2,k + ε Pj,k log (j)πj,k
6: Return c∏,i,' J Ai + b` — 2Ci,' + E
22