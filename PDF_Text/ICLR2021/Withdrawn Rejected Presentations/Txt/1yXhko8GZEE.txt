Under review as a conference paper at ICLR 2021
Precondition Layer and Its Use for GANs
Anonymous authors
Paper under double-blind review
Abstract
One of the major challenges when training generative adversarial nets (GANs) is
instability. To address this instability spectral normalization (SN) is remarkably
successful. However, SN-GAN still suffers from training instabilities, especially
when working with higher-dimensional data. We find that those instabilities are
accompanied by large condition numbers of the discriminator weight matrices. To
improve training stability we study common linear-algebra practice and employ
preconditioning. Specifically, we introduce a preconditioning layer (PC-layer)
that performs a low-degree polynomial preconditioning. We use this PC-layer
in two ways: 1) fixed preconditioning (FPC) adds a fixed PC-layer to all lay-
ers; and 2) adaptive preconditioning (APC) adaptively controls the strength of
preconditioning. Empirically, we show that FPC and APC stabilize training of un-
conditional GANs using classical architectures. On LSUN 256 × 256 data, APC
improves FID scores by around 5 points over baselines.
1 Introduction
Generative Adversarial Nets (GANs) (Goodfellow et al., 2014) successfully transform samples from
one distribution to another. Nevertheless, training GANs is known to be challenging, and its perfor-
mance is often sensitive to hyper-parameters and datasets. Understanding the training difficulties of
GAN is thus an important problem.
Recent studies in neural network theory (Pennington et al., 2017; Xiao et al., 2018; 2020) suggest
that the spectrum of the input-output Jacobian or neural tangent kernel (NTK) is an important metric
for understanding training performance. While directly manipulating the spectrum of the Jacobian
or NTK is not easy, a practical approach is to manipulate the spectrum of weight matrices, such
as orthogonal initialization (Xiao et al., 2018). For a special neural net, Hu et al. (2020) showed
that orthogonal initialization leads to better convergence result than Gaussian initialization, which
provides early theoretical evidence for the importance of manipulating the weight matrix spectrum.
Motivated by these studies, we suspect that an ‘adequate’ weight matrix spectrum is also important
for GAN training. Indeed, one of the most popular techniques for GAN training, spectral normal-
ization (SN) (Miyato et al., 2018), manipulates the spectrum by scaling all singular values by a
constant. This ensures the spectral norm is upper bounded. However, we find that for some hyper-
parameters and for high-resolution datasets, SN-GAN fails to generate good images. In a study we
find the condition numbers of weight matrices to become very large and the majority of the singular
values are close to 0 during training. See Fig. 1(a) and Fig. 2(a). This can happen as SN does not
promote a small condition number.
This finding motivates to reduce the condition number of weights during GAN training. Recall
that controlling the condition number is also a central problem in numerical linear algebra, known
as preconditioning (see Chen (2005)). We hence seek to develop a “plug-in” preconditioner for
weights. This requires the preconditioner to be differentiable. Out of various preconditioners, we
find the polynomial preconditioner to be a suitable choice due to the simple differentiation and
strong theoretical support from approximation theory. Further, we suggest to adaptively adjust the
strength of the preconditioner during training so as to not overly restrict the expressivity. We show
the efficacy of preconditioning on CIFAR10 (32 × 32), STL (48 × 48) and LSUN bedroom, tower
and living room (256 × 256).
Summary of contributions. For a deep linear network studied in (Hu et al., 2020), we prove
that if all weight matrices have bounded spectrum, then gradient descent converges to global min-
1
Under review as a conference paper at ICLR 2021
(a)
(b)
JlυqEnN uospuou
Figure 1: Evolution of the 5 smallest singular values of (a) SN-GAN, FID 147.9 and (b) APC-GAN, FID 34.08
when generating STL-10 images with a ResNet trained with Dit =1for 200k iterations. The max singular
value is around 1 due to SN, thus not shown here.
2 4 6 8 10 12 14 16 18 20
xl0000 iteration
(a)	(b)
Figure 2: Evolution of condition number of (a) SN-GAN, FID 147.9 and (b) APC-GAN, FID 34.08 when
generating STL-10 images with a ResNet trained with Dit =1for 200k iterations.
imum at a geometric rate. We then introduce a PC-layer (preconditioning layer) that consists of
a low-degree polynomial preconditioner. We further study adaptive preconditioning (APC) which
adaptively controls the strength of PC on different layers in different iterations. Applying PC and
APC to unconditional GAN training on LSUN data (256 × 256), permits to generate high-quality
images when SN-GAN fails. We also show that APC achieves better FID scores on CIFAR10, STL,
and LSUN than a recently proposed method of Jiang et al. (2019).
1.1	Related Work
Related to the proposed method is work by Jiang et al. (2019), which also controls the spectrum in
GAN training. They re-parameterize a weight matrix W via W = USVT , add orthogonal regu-
larization of U, V and certain regularizer of entries of the diagonal matrix S . This approach differs
from ours in a few aspects. First, Jiang et al. (2019) essentially solves a constrained optimization
problem with constraints UTU = I,VTV = I using a penalty method (Bertsekas, 1997). In con-
trast, our approach solves an unconstrained problem since we add one layer into the neural net,
similar to batch normalization (BN) (Ioffe & Szegedy, 2015) and SN (Miyato et al., 2018). Second,
our PC layer is a direct generalization of SN as it includes SN-layer as a special case. In contrast, the
method of Jiang et al. (2019) differs from SN-layer in any case. Our proposed method thus offers a
smoother transition for existing users of SN.
In a broader context, a number of approaches have been proposed to stabilize and improve GAN
training, such as modifying the loss function (Arjovsky et al., 2017; Arjovsky & Bottou, 2017; Mao
eettaall..,,2017; Li et al., 2017b; Deshpande et al., 2018), normalization and regularization (Gulrajani
2017; Miyato et al., 2018), progressive growing techniques (Karras et al., 2018; Huang et al.,
, changing architecture (Zhang et al., 2019; Karnewar & Wang, 2019), utilizing side informa-
tion like class labels (Mirza & Osindero, 2014; Odena et al., 2017; Miyato & Koyama, 2018). Using
this taxonomy, our approach fits the “normalization and regularization” category (even though our
method is not exactly normalization, the essence of “embedded control” is similar). Note that these
directions are relatively orthogonal, and our approach can be potentially combined with other tech-
niques such as progressive growing. However, due to limited computational resources, we focus on
unconditional GANs using classical architectures, the setting studied by Miyato et al. (2018).
1.2	Notation and Definition
We use eig(A) to denote the multiset (i.e., allow repetition) of all eigenvalues ofA. If all eigenvalues
of A are non-negative real numbers, we say A is a positive semidefinite (PSD) matrix. The singular
values of a matrix A ∈ Rn×m are the square root of the eigenvalues of AT A ∈ Rm×m. Let
σmax(A) and σmin(A) denote the maximum and minimum singular values of A. Let kAk2 denote
2
Under review as a conference paper at ICLR 2021
the spectral norm of A, i.e., the largest singular value. The condition number of a square matrix A is
traditionally defined as K(A) = ∣∣Ak2kA-1k2 = ：max(A) .Weextendthis definition to a rectangular
matrix A ∈ Rn×m where n ≥ m via K(A) = ：max(A). Letdeg(P) denote the degree of a polynomial
p and let Pk = {p | deg(p) ≤ k} be the set of polynomials with degree no more than k.
2Why Controlling the Spectrum?
To understand why controlling the spectrum is helpful we leverage recent tools in neural network
theory to prove the following result: if weight matrices have small condition numbers, then gradient
descent for deep pyramid linear networks converges to the global-min fast. This is inspired by Hu
et al. (2020) analyzing a deep linear network to justify orthogonal initialization.
Similar to Hu et al. (2020), we consider a linear network that takes an input x ∈ Rdx ×1 and outputs
F (θ; x) = WLWL-1 ...W1x ∈ Rdy×1,	(1)
where θ =(W1 ,. .., WL ) represents the collection of all parameters and Wj is a matrix of di-
mension dj × dj-1, j = 1,...,L. Here we define d0 = dx and dL = dy. Assume there exists
r ∈ {1,..., L}, such that dy = d，L ≤ dL-ι ≤ •…≤ d, and n ≤ d° ≤ di ≤ •…≤ d『.This means
the network is a pyramid network, which generalizes the equal-width network of Hu et al. (2020).
Suppose y = (y1; ...; yn) ∈ Rndy×1 are the labels, and the predictions are F(θ; X)=
(F(θ; xi);..., F(θ; Xn)) ∈ Rndy ×1. We consider a quadratic loss L(θ) = ɪ ∣∣y 一 F(θ; X)∣2.
Starting from θ(0), we generate θ(k)=(W1(k),...,WL(k)),k=1, 2,... via gradient descent:
θ(k + 1) = θ(k) — ηVL(θ(k)).
Denote the residual e(k) = F (θ(k); X) 一 y. For given Tl ≥ 1,μι ≥ 0,l = 1,...,L, define
R , {θ =(Wl,..., WL) | Tl ≥ σmaχ(Wl) ≥ OmE (Wl) ≥ μl, ∀l}.
β , LIlXIl2Tl	... Ti	(∣∣e(0)k +	∣∣X∣∣FTL	...	Ti),	μ，(μι	... μL)2σmin(X)2.
The following result states that if θ(k) stay within region R (i.e., weight matrices have bounded
spectrum) for k =0, 1,...,K, then the loss decreases at a geometric rate until iteration K. The rate
(1 一 β) depends on (IL …；；)2, which is related to the condition numbers of all weights.
Theorem 1 Suppose η = ɪ. Assume θ(k) ∈ R, k = 0,1,...,K. Then we have
ke(k + 1)k2 ≤ (1 — μ)ke(k)k2,	k = 0,1,...,K.	⑵
β
See Appendix D.3.1 for the proof and detailed discussions.
For proper initial point θ(0) where Wl(0)’s are full-rank, we can always pick Tl,σl so that θ(0) ∈ R.
The trajectory {θ(k)} either stays in R forever (in which case K = ∞), or leaves R at some finite
iteration K. In the former case, the loss converges to zero at a geometrical rate; in the latter case,
the loss decreases to below (1 一 μ∕β)k∣∣e(0)k2. However, our theorem does not specify how large
K is for a given situation. Previous works on convergence (e.g., Hu et al., 2020; Du et al., 2018;
Allen-Zhu et al., 2019; Zou et al., 2018) bound the movement of the weights with extra assumptions,
so that the trajectory stays in a certain nice regime (related to R). We do not attempt to prove the
trajectory stays in R. Instead, we use this as a motivation for algorithm design: if we can improve
the condition numbers of weights during training, then the trajectory may stay in Rfora longer time,
and thus lead to smaller loss values. Next, we present the preconditioning layer as such a method.
3	Preconditioning Layer
In the following, we first introduce classical polynomial preconditioners in Sec. 3.1. We then present
the preconditioning layer for deep nets in Sec. 3.2. We explain how to compute a preconditioning
polynomial afterwards in Sec. 3.3, and finally present an adaptive preconditioning in Sec. 3.4.
3
Under review as a conference paper at ICLR 2021
3.1	Preliminary: Polynomial Preconditioner
Preconditioning considers the following classical question: for a symmetric matrix Q, how to find
an operator g such that κ(g(Q)) is small? Due to the importance of this question and the wide
applicability there is a huge literature on preconditioning. See, e.g., Chen (2005) for an overview,
and Appendix B for a short introduction. In this work, we focus on polynomial preconditioners
(Johnson et al., 1983). The goal is to find a polynomial P such that P(Q)Q has a small condition
number. The matrix P(Q) is often called preconditioner, and g(Q)，p^(Q)Q is the PreCOndtiOned
matrix. We call g the preconditioning polynomial. Polynomial preconditioning has a special merit:
the difficult problem of manipulating eigenvalues can be transformed to manipulating a 1-d function,
based on the following fact (proof in Appendix E.2.1).
Claim 3.1 Suppose g is any polynomial, and Q ∈ Rm×m is a real symmetric matrix with eigen-
values λι ≤ •… ≤ λm. Then the eigenvalues of the matrix ^(Q) are g(λi), i = 1,...,m. As a
corollary, if^([λι, λm]) ⊆ [1 - e, 1], then eig(^(Q)) ⊆ [1 - e, 1].
To find a matrix g(Q) = p^(Q)Q that is well-conditioned, we need to find a polynomial P such that
g(χ) = p(χ)χ maps [λι, λm] to [1 - e, 1]. This can be formulated as a function approximation
problem: find a polynomial g(χ) of the form χp(χ) that approximates a function f (λ) in λ ∈
[λ1, λm]. Under some criterion, the optimal polynomial is a variant of the Chebychev polynomial,
and the solutions to more general criteria are also well understood. See Appendix B.1 for more.
A scaling trick is commonly used in practice. It reduces the problem of preconditioning Q to the
problem of preconditioning a scaled matrix Qsca = Q∕λm. Scaling employs two steps: first, We find
a polynomial g that approximates f (x) = 1 in X ∈ [λι∕λm, 1]; second, set g(λ) = g(λ∕λm) and use
g(Q) = g(Q∕λm) = g(Qsca) as the final preconditioned matrix. It is easy to verify g approximates
f(λ)=1in [λ1,λm]. Thus this approach is essentially identical to solving the approximation
problem in [λ1,λm]. Johnson et al. (1983) use this trick mainly to simplify notation since they can
assume λm = 1 without loss of generality. We will use this scaling trick for a different purpose (see
Section 3.3).
3.2	Preconditioning Layer in Deep Nets
Suppose D(W1,...,WL) is a deep net parameterized by weights W1,...,WL for layers l ∈
{1,...,L}. To control the spectrum of a weight Wι, We want to embed a preconditioner g into the
neural net. Among various preconditioners, polynomial ones are appealing since their gradient is
simple and permits natural integration with backpropagation. For this we present a preconditioning
layer (PC-layer) as follows: a PC-layer g(W) = g(SN(W)) is the concatenation of a precondition-
ing polynomial g and an SN operation of (Miyato et al., 2018) (see Appendix app sub: details of FPC
and APC for details of SN(W)). The SN operator is used as a scaling operator (reason explained
later). We describe an efficient implementation of PC-layer in Appendix C.3.
In our case, we use A = SN(W) to indicate the scaled matrix. Prior work on polynomial pre-
conditioners (Johnson et al., 1983; Chen, 2005) often study square matrices. To handle rectangular
matrices, some modifications are needed.
A naive solution is to apply a preconditioner to the symmetrized matrix AT A, leading to a matrix
g(A) = P(ATA)ATA. This solution works for linear models (see Appendix B.2 for details), but
it is not appropriate for deep nets since the shape ofP(ATA)ATA ∈ Rm×m differs from A.To
maintain the shape of size n × m, we propose to transform A to g(A) = P(AAT)A ∈ Rn×m. This
transformation works for general parameterized models including linear models and neural nets. For
a detailed comparison of these two approaches, see Appendix B.2. The following claim relates the
spectrum of A and P(AAT)A; see the proof in Appendix E.2.2.
Claim 3.2 Suppose A ∈ Rn×m has singular values σι ≤ … ≤ σm. Suppose g(x) = p(x2)x
where P is a polynomial. Then the singular values of g(A) = P(AAT )A are ∣g(σι)∣,..., ∣g(σm)∣.
To find a polynomial P such that g(A) = P(AAT)A is well-conditioned, we need to find a poly-
nomial P such that g(x) = P(x2)x maps [σ1,σm] to [1 - e, 1] for some e. This can be formulated
as a function approximation problem: find a polynomial g(x) in Gk that approximates a function
4
Under review as a conference paper at ICLR 2021
f(σ)=1in σ ∈ [σ1, σm] where Gk = {g(x) = xp(x2) | p ∈ Pk}. We will describe the algorithm
for finding the preconditioning polynomial g in Sec. 3.3.
In principle, the PC-layer can be added to any deep net including supervised learning and GANs.
Here, we focus on GANs for the following reason. Current algorithms for supervised learning
already work quite well, diminishing the effect of preconditioning. In contrast, for GANs, there is
a lot of room to improve training. Following SN-GAN which applies SN to the discriminator of
GANs, in the experiments we apply PC to the discriminator.
3.3	Finding Preconditioning Polynomials
In this subsection, we discuss how to generate preconditioning polynomials. This generation is done
off-line and independent of training. We will present the optimization formulation and discuss the
choice of a few hyperparameters such as the desirable range and the target function f.
Optimization formulation. Suppose we are given a range [γL ,γU], a target function f, and an
integer k; the specific choices are discussed later. Suppose we want to solve the following approx-
imation problem: find the best polynomial of the form g(x) = x(a0 + aιx2 + … + a*x2k) that
approximates f(x) in domain [γL,γU], i.e., solve
min d[γL ,γU] (g(x), f (x)),	(3)
g∈Gk
where Gk = {g(x) = xp(x2) | p ∈ Pk}, d[γ ,γ ] is a distance metric on the function space
C[yl, γu], such as the '∞ distance d[γlj,γu] (f, g) = maxt∈[γb,γu] |f (t) - g(t)|. We consider a
weighted least-square problem suggested by Johnson et al. (1983):
γU
min	|g(x) - f (x)|2w(x)dx,
g∈Gk γ
(4)
where w(x) = xα is a weight function used in (Johnson et al., 1983). We discretize the objective
and solve the finite-sample version of Eq. (4) as follows:
min
c=(c0,c1 ,...,ck)∈Rk+1
nk	2
E Xi Ectx2t - f(xi)w(x。,
i=1	t=0
(5)
where xi ∈ [γL,γU], ∀i (e.g., drawn from uniform distribution on [γL,γU]). This is a weighted least
squares problem (thus convex) that can be easily solved by a standard solver.
Choice of desirable range [γL ,γU]. The range [γL ,γU] within which we want to approximate
the target function is often chosen to be the convex hull of the singular values of the matrix to be
preconditioned. For the original matrix W, the desirable range [γL,γU]=[σmin(W),σmax(W)].
However, this range varies across different layers and different iterations. For this reason we scale
each W by 1/kW k2 to obtain A so that its singular values lie in a fixed range [0, 1]. Note that a more
precise range is [σmin(A)∕σmaχ(A), 1], but We can relax it to [0,1]. We follow Miyato et al. (2018)
to use one power iteration to estimate the spectral norm W ≈ ∣∣W∣∣2, and denote SN(W) = W/W.
Since W is not exactly ∣∣ W∣∣2, the range of singular values of A = SN(W) may not be exactly in
[0, 1]. We have checked the empirical estimation and found that the estimated spectral norm during
the training of SN-GAN is often less than 1.1 times the true spectral norm (see Fig. 5 in Appendix
E.1), thus we pick [γL, γU] = [0, 1.1] in our implementation.
Choice of target function f (λ). Previously, we discuss the ideal situation that [γL,γU]=[λ1,λm],
thus the target function is 1. In the previous paragraph, we have relaxed the desirable range to
[0, γU], then we cannot set f(x)=1in [0,γU], because any polynomial g(λ) ∈ Gk must satisfy
g(0) = 0, causing large approximation error at λ =0. We shall set f(0) = 0. A candidate target
x/b,
1,
function is PLb(x), where PLb(x)
x<b
is defined as a piece-wise linear function with
x≥b
cutoff point b. If the cutoff point b < λmin(A), then PLb(λ) maps all singular values ofA to 1.
While setting all singular values to 1 is ideal for fast training, this may reduce the expressiveness
for deep nets. More specifically, the set of functions {D(W1,...,WL) | eig(WTWl) ⊆ {1}} is
smaller than {D(W1,...,WL) | eig(WTWl) ⊆ [λ0, 1]}, thus forcing all singular values to be 1
5
Under review as a conference paper at ICLR 2021
Figure 3: Left: Different piecewise linear functions; Right: the corresponding fitted preconditioning polyno-
mials. The smaller the cutoff, the more aggressive the preconditioner. For instance, when the cutoff is 0.3, the
preconditioner pushes all singular values above 0.3 to 1. We show the fitting polynomials of degree 3, 5, 7, 9
for PL0.8, PL0.6, PL0.4, PL0.3 respectively. More details are in Sec. C.2
may hurt the representation power. Therefore, we do not want the target function to have value 1 in
[λmin(A), γU]. In practice, the value of λmin(A) varies for different problems, therefore we permit
a flexible target function f, to be chosen by a user.
In our implementation, we restrict target functions to a family of piece-wise linear functions. We
use PLb(x) with a relatively large cutoff point b, such as 0.8 and 0.3. We plot our candidate target
functions PL0.3 , PL0.4, PL0.6 and PL0.8 in Figure 3. As the cutoff point b changes from 1 to 0,
the function PLb becomes more aggressive as it pushes more singular values to 1. As a result, the
optimization will likely become easier, while the representation power becomes weaker. The exact
choice of the target function is likely problem-dependent, and we discuss two strategies to select
them in Section 3.4.
Search space of preconditioning polynomial. As mentioned earlier, the default search space is
Gk = {g(x) = xp(x2) | p ∈ Pk} for a pre-fixed k. The degree of g(λ) is an important hyperpa-
rameter. On the one hand, the higher the degree, the better the polynomial can fit the target function
f. On the other hand, higher degree leads to more computation. In our implementation, we consider
k =1, 2, 3, 4, i.e., polynomials of degree 3, 5, 7 and 9. The extra time is relatively small; see Section
C.4 for details.
3.4	Fixed Preconditioning and Adaptive preconditioning
The preconditioning polynomial can be determined by the target function and the degree k . Which
polynomial shall we use during training?
Candidate preconditioners. At first sight, there are two hyper-parameters b and k. Nevertheless,
if b is small (steep slope), then it is hard to approximate PLb by low-order polynomials. For each
degree k ∈ {3, 5, 7, 9}, there is a certain bk such that b<bk leads to large approximation error. We
find that b3 ≈ 0.8, b5 ≈ 0.6, b7 ≈ 0.4, b9 ≈ 0.3. After fitting PL0.3, PL0.4, PL0.6 and PL0.8, the
resulting polynomials are shown in Figure 3.
A natural approach is to add the PC-layer to all layers of the neural net, resulting in a preconditioned
net DPC(θ)=D(g(SN(W1)),...,g(SN(WL)). We call this method fixed preconditioning (FPC).
Just like other hyperparameters, in practice, we can try various preconditioners and pick the one
with the best performance. Not surprisingly, the best preconditioner varies for different datasets.
Adaptive preconditioning (APC). Motivated by adaptive learning rate schemes like Adam
(Kingma & Ba, 2014) and LARS (You et al., 2017), we propose an adaptive preconditioning scheme.
In APC, we apply the preconditioner in a epoch-adaptive and layer-adaptive manner: at each epoch
and for each layer the algorithm will automatically pick a proper preconditioner based on the current
condition number.
The standard condition number K(A) = σmax(A) is not necessarily a good indicator for the opti-
σmin (A)
mization performance. In APC, We use a modified condition number K(A)
σmax (A)
(PmI	σi(A))∕m0 .
where A has m columns and m° = d m ]. We prepare r preconditioning polynomials gι,...,gr
With different strength (e.g., the four polynomials g1,g2,g3,g4 shoWn in Figure 3). We set a number
of ranges [0,τ1], [τ1,τ2],...,[τr, ∞] and let τ0 =0,τr+1 = ∞. If the modified condition number
of A falls into the range [τi,τi+1] for i ∈{0, 1,...,r}, we will use gi in the PC-layer. In our im-
6
Under review as a conference paper at ICLR 2021
plementation, we set r =4. To save computation, we only compute the modified condition number
and update PC strength at a fixed interval (e.g., every 1000 iterations). The summary of APC is
presented in Table 3 in Appendix C.2.
Computation time. We use a few implementation tricks; see Appendix C.3. In our implementation
of FPC with a degree 3, 5, 7 or 9 polynomial, the actual added time is around 20 - 30% (Fig. 4 (a))
of the original training time of SN-GAN. Fig. 4 (b) shows that the extra time of APC over SN is
often less than 10%. See Appendix C.4 for more on the computation time.
4	Experimental Results
We will demonstrate the following two findings. First, SN-GAN still suffers from training instabili-
ties, and the failure case is accompanied by large condition numbers. Second, PC layers can reduce
the condition number, and improve the final performance, especially for high resolution data (LSUN
256 × 256).
We conduct a set of experiments for unconditional image generation on CIFAR-10 (32 × 32), STL-
10 (48 × 48), LSUN-bedroom (128 × 128 and 256 × 256), LSUN-tower (256 × 256) and LSUN-
living-room (256 × 256). We also compare the condition numbers of the discriminator layers for
different normalization methods to demonstrate the connection between the condition number and
the performance. The following methods are used in our experiments: standard SN; SVD with D-
Optimal Reg. (Jiang et al., 2019); FPC with degree 3 or 7 preconditioners; APC. Following Miyato
et al. (2018), we use the log loss GAN on the CNN structure and the hinge loss GAN on the ResNet
structure.
CIFAR and STL: Training failure of (1,1)-update. Tuning a GAN is notoriously difficult and
sensitivity to hyper-parameters. Even for low-resolution images, without prior knowledge of good
hyper-parameters such as Dit,Git, training a GAN is often not trivial. On CIFAR10, SN-GAN uses
Dit =5,Git =1for ResNet; for simplicity, we call it a (5, 1)-update. However, using a (1, 1)-
update, i.e., changing Dit =5to Dit = 1 while keeping Git =1, will lead to an SN-GAN training
failure: a dramatic decrease of final performance and an FID score above 77. SN-GAN with (1, 1)-
update also fails on STL data, yielding an FID score above 147. We are interested in stabilizing
the (1, 1)-update for two reasons: first, trainability for both (1, 1)-update and (5, 1)-update means
improved training stability; second, the (1, 1)-update requires only about 1/3 of the time compared
to the (5, 1)-update. Therefore, in the first experiment, we explore GAN-training with (1, 1)-update
on CIFAR-10 and STL-10.
Failure mode: large condition numbers. Understanding the failure mode of training is often very
useful for designing algorithms (e.g., Glorot & Bengio, 2010). We suspect that a large condition
number is a failure mode for GAN training. As Table 1 shows, the high FID scores (bad case) of
SN-GAN are accompanied by large condition numbers.
PC reduces condition numbers and rectifies failures. Table 1 shows that FPC and APC can both
greatly improve the training performance: they reduce FID from 77 to less than 20 for CIFAR-10,
and reduce FID from 147 to less than 34 for STL in 200k iterations. The evolution of the 5 smallest
singular values of the adaptive preconditioned matrices and the condition numbers are showed in
Fig. 1(b) and Fig. 2(b) for STL-10 training on ResNet with Dit =1. This shows that PC-GAN
successfully improves the spectrum of weight matrices in this setting.
Experiments on “good” case of SN-GAN. We report the results for the (5, 1)-update on CIFAR-10
and STL-10 with ResNet in the Appendix. For those FPC and APC achieve similar or slightly better
FID scores. We also report IS scores there. We also list the results of PC and multiple baselines on
the CNN structure in the Appendix.
High resolution images LSUN. Using high-resolution data is more challenging. We present numer-
ical results on LSUN bedroom (128 × 128, and 256 × 256) , LSUN tower (256 × 256) and LSUN
living room (256 × 256) data in Table 2. The training time for one instance is 30 hours on a single
RTX 2080 Ti (200k iterations).
Note, SN-GAN is unstable and results in FID > 80 for LSUN-bedroom 256 × 256. The SVD
method, our FPC and APC generate reasonable FID scores on all three data sets. Importantly, our
FPC is comparable or better than SVD, and our APC consistently outperforms the SVD method by
7
Under review as a conference paper at ICLR 2021
Setting	Method	FID score	maxL=ι 用(Wι)	2ndmaxL=ι K(Wι)	Avg ∕⅞(Wι)
CIFAR-10	SN-GAN	77.82	113.55	19.46	17.77
ResNet	FPC, deg-3	20.09	22.65	1.88	3.16
Dit =1	FPC, deg-7	19.31	9.88	1.32	1.85
	APC	19.53	12.06	2.81	3.19
STL-10	SN-GAN	-147.90	217.06	167.97	53.37
ResNet	FPC, deg-3	33.99	23.75	22.35	4.36
Dit =1	FPC, deg-7	34.28	2.55	2.72	1.53
	APC	34.08	6.79	6.47	3.90
Table 1: Comparison of SN-GAN and PC-GAN, using ResNet With Dit = 1∙ Here Wl is the preconditioned
weighted matrix (i.e., after applying preconditioning). “2nd maxL=ι κ(∖Wl)" indicates the second largest con-
dition number of all layers. “Avg κ(∖Wl)” indicates the average of all layer condition numbers.
Setting	Method	FID score	maxL=ι 方(Wι)	2ndmaxL=ι 用(Wι)	Avg K(Wι)
	SN	53.48	5.00	2.20	2.14
LSUN	GaTech	50.92	1.75	1.55	1.46
Bedroom 128	PC, deg-3	51.05	1.23	1.15	1.09
	APC	45.32	2.75	2.40	1.82
	SN	81.09	26.19 =	6.02	6.45
LSUN	GaTech	36.89	2.76	2.18	1.82
Bedroom 256	PC, deg-3	35.61	3.04	2.03	1.64
	APC	31.17	4.35	2.80	2.18
	SN	193.75	66.87 =	9.86	13.54
LSUN	GaTech	32.79	2.00	1.95	1.60
Living Room 256	PC, deg-3	28.20	2.95	2.91	1.64
	APC	28.29	3.99	3.63	2.23
	SN	33.10	12.75 =	3.48	3.50
LSUN	GaTech	35.05	2.83	2.36	1.85
Tower 256	PC, deg-3	30.81	2.43	1.45	1.33
	APC	29.58	3.38	2.53	1.91
Table 2: Results on LSUN data.					
4-6 FID scores in most cases. Also note, the condition numbers of the failure case of SN-GAN are
much higher than the two normal cases of SN-GAN. In all cases, FPC and APC achieve significantly
lower condition numbers than SN-GAN. APC achieves higher condition numbers than FPC, and
also better FID scores. We suspect that FPC over-controls the condition numbers which leads to
lower representation power. In contrast, APC strikes a better balance between representation and
optimization than FPC. The generated image samples are presented in Appendix F.5.
5	Conclusion
We prove that for a deep pyramid linear networks, if all weight matrices have bounded singular
values throughout training, then the algorithm converges to a global minimal value at a geometric
rate. This result indicates that small weight matrix condition numbers are helpful for training. We
propose a preconditioning (PC) layer to improve weight matrix condition numbers during training,
by leveraging tools from polynomial preconditioning literature. It is differentiable, and thus can
be plugged into any neural net. We propose two methods to utilize the PC-layer: in FPC (fixed
preconditioning), we add a fixed PC-layer to all layers; in APC (adaptive preconditioning), we add
PC-layers with different preconditioning power depending on the condition number. Empirically,
we show that applying FPC and APC to GAN training, we can generate good images in a few cases
that SN-GAN perform badly, such as LSUN-bedroom 256×256 image generation.
8
Under review as a conference paper at ICLR 2021
References
Jonas Adler and Sebastian Lunz. Banach wasserstein gan. In NeurIPS, 2018.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In ICML, 2019.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In ICLR, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. In ICML, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In ICML, 2017.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Grae-
pel. The mechanics of n-player differentiable games. arXiv preprint arXiv:1802.05642, 2018.
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007.
Hugo Berard, Gauthier Gidel, Amjad Almahairi, Pascal Vincent, and Simon Lacoste-Julien. A
closer look at the optimization landscapes of generative adversarial networks. arXiv preprint
arXiv:1906.04848, 2019.
David Berthelot, Tom Schumm, and Luke Metz. Began: Boundary equilibrium generative adversar-
ial networks. arXiv preprint arXiv:1703.10717, 2017.
Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):
334-334,1997.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
Ke Chen. Matrix preconditioning techniques and applications, volume 19. Cambridge University
Press, 2005.
Ruohan Wang Antoine Cully, Hyung Jin Chang, and Yiannis Demiris. Magan: Margin adaptation
for generative adversarial networks. arXiv preprint arXiv:1704.03817, 2017.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In ICLR, 2018.
Ishan Deshpande, Ziyu Zhang, and Alexander Schwing. Generative modeling using the sliced
wasserstein distance. In CVPR, 2018.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien.
A variational inequality perspective on generative adversarial networks. arXiv preprint
arXiv:1802.10551, 2018.
Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Remi Lepriol, Gabriel Huang, Si-
mon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In AISTATS, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
9
Under review as a conference paper at ICLR 2021
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein gans. In NeurIPS, 2017.
Roger A Horn, Roger A Horn, and Charles R Johnson. Topics in matrix analysis. Cambridge
university press, 1994.
Wei Hu, Lechao Xiao, and Jeffrey Pennington. Provable benefit of orthogonal initialization in opti-
mizing deep linear networks. arXiv preprint arXiv:2001.05992, 2020.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. In CVPR, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, 2018.
Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On com-
putation and generalization of generative adversarial networks under spectrum control. In Inter-
national Conference on Learning Representations, 2019.
Olin G Johnson, Charles A Micchelli, and George Paul. Polynomial preconditioners for conjugate
gradient calculations. SIAM Journal on Numerical Analysis, 20(2):362-376,1983.
Animesh Karnewar and Oliver Wang. Msg-gan: multi-scale gradient gan for stable image synthesis.
arXiv preprint arXiv:1903.06048, 2019.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In ICLR, 2018.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Soheil Kolouri, Charles E Martin, and Gustavo K Rohde. Sliced-wasserstein autoencoder: An
embarrassingly simple generative model. arXiv preprint arXiv:1804.01947, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8572-8583,
2019.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan:
Towards deeper understanding of moment matching network. In NeurIPS, 2017a.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic
gradient langevin dynamics for deep neural networks. arXiv preprint arXiv:1512.07666, 2015.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dy-
namics of generative adversarial networks. In ICML, 2018.
Y. Li, A. G. Schwing, K.-C. Wang, and R. Zemel. Dualing GANs. In NeurIPS, 2017b.
Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. Pacgan: The power of two samples in
generative adversarial networks. In NeurIPS, 2018.
Zinan Lin, Vyas Sekar, and Giulia Fanti. Why Spectral Normalization Stabilizes GANs: Analysis
and Improvements. In arXiv e-prints, 2020.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In ICCV, 2017.
10
Under review as a conference paper at ICLR 2021
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In ICML, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. In ICLR, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint
arXiv:1802.05637, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In ICLR, 2018.
Youssef Mroueh and Tom Sercu. Fisher gan. In NeurIPS, 2017.
Youssef Mroueh, Tom Sercu, and Vaibhava Goel. Mcgan: Mean and covariance feature matching
gan. arXiv preprint arXiv:1702.08398, 2017.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Pro-
Ceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2603-2612.
JMLR. org, 2017.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In NeurIPS, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. In International conference on machine learning, pp. 2642-2651, 2017.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B. Brown, Christopher Olah, Colin Raf-
fel, and Ian Goodfellow. Is Generator Conditioning Causally Related to GAN Performance? In
ICML, 2018.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Advances in neural information
processing systems, pp. 4785-4795, 2017.
Ben Poole, Alexander A Alemi, Jascha Sohl-Dickstein, and Anelia Angelova. Improved generator
objectives for gans. arXiv preprint arXiv:1612.02780, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in neural information processing systems, pp. 901-
909, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
Xi Chen, and Xi Chen. Improved techniques for training gans. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), NeurIPS, pp. 2234-
2242. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6125-improved-techniques-for-training-gans.pdf.
Jiqing Wu, Zhiwu Huang, Wen Li, Janine Thoma, and Luc Van Gool. Sliced wasserstein generative
models. In CVPR, 2019.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla
convolutional neural networks. arXiv preprint arXiv:1806.05393, 2018.
Lechao Xiao, Jeffrey Pennington, and Samuel S Schoenholz. Disentangling trainability and gener-
alization in deep neural networks. 2020.
11
Under review as a conference paper at ICLR 2021
Yasin Yazici, ChUan-Sheng Foo, Stefan Winkler, Kim-HUi Yap, Georgios Piliouras, and Vijay Chan-
drasekhar. The unusual effectiveness of averaging in gan training. In ICLR, 2019.
Yang YoU, Igor Gitman, and Boris GinsbUrg. Scaling sgd batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 6, 2017.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and AUgUstUs Odena. Self-attention generative
adversarial networks. In International Conference on Machine Learning, pp. 7354-7363. PMLR,
2019.
Difan ZoU, YUan Cao, DongrUo ZhoU, and QUanqUan GU. Stochastic gradient descent optimizes
over-parameterized deep relU networks. arXiv preprint arXiv:1811.08888, 2018.
12