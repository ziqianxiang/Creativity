Under review as a conference paper at ICLR 2021
DBT: A Detection Booster Training method
FOR IMPROVING THE ACCURACY OF CLASSIFIERS
Anonymous authors
Paper under double-blind review
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
Ab stract
Deep learning models owe their success at large, to the availability of a large
amount of annotated data. They try to extract features from the data that contain
useful information needed to improve their performance on target applications.
Most works focus on directly optimizing the target loss functions to improve the
accuracy by allowing the model to implicitly learn representations from the data.
There has not been much work on using background/noise data to estimate the
statistics of in-domain data to improve the feature representation of deep neural
networks. In this paper, we probe this direction by deriving a relationship between
the estimation of unknown parameters of the probability density function (pdf)
of input data and classification accuracy. Using this relationship, we show that
having a better estimate of the unknown parameters using background and in-
domain data provides better features which leads to better accuracy. Based on
this result, we introduce a simple but effective detection booster training (DBT)
method that applies a detection loss function on the early layers of a neural network
to discriminate in-domain data points from noise/background data, to improve
the classifier accuracy. The background/noise data comes from the same family
of pdfs of input data but with different parameter sets (e.g., mean, variance). In
addition, we also show that our proposed DBT method improves the accuracy even
with limited labeled in-domain training samples as compared to normal training.
We conduct experiments on face recognition, image classification, and speaker
classification problems and show that our method achieves superior performance
over strong baselines across various datasets and model architectures.
1	Introduction
Modern pattern recognition systems achieve outstanding accuracies on a vast domain of challenging
computer vision, natural language, and speech recognition benchmarks (Russakovsky et al. (2015);
Lin et al. (2014); Everingham et al. (2015); Panayotov et al. (2015)). The success of deep learning
approaches relies on the availability of a large amount of annotated data and on extracting useful
features from them for different applications. Learning rich feature representations from the available
data is a challenging problem in deep learning. A related line of work includes learning deep latent
space embedding through deep generative models (Kingma & Welling (2014); Goodfellow et al.
(2014); Berthelot et al. (2019) or using self-supervised learning methods (Noroozi & Favaro (2016);
Gidaris et al. (2018); Zhang et al. (2016b)) or through transfer learning approaches (Yosinski et al.
(2014); Oquab et al. (2014); Razavian et al. (2014)).
In this paper, we propose to use a different approach to improve the feature representations of deep
neural nets and eventually improve their accuracy by estimating the unknown parameters of the
probability density function (pdf) of input data. Parameter estimation or Point estimation methods
are well studied in the field of statistical inference (Lehmann & Casella (1998)). The insights from
the theory of point estimation can help us to develop better deep model architectures for improving
the model’s performance. We make use of this theory to derive a correlation between the estimation
of unknown parameters of pdf and classifier outputs. However, directly estimating the unknown
pdf parameters for practical problems such as image classification is not feasible since it can sum
up to millions of parameters. In order to overcome this bottleneck, we assume that the input data
points are sampled from a family of pdfs instead of a single pdf and propose to use a detection
based training approach to better estimate the unknowns using in-domain and background/noise data.
One alternative is that we can use generative models for this task, however, they mimic the general
1
Under review as a conference paper at ICLR 2021
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
distribution of training data conditioned on random latent vectors and hence cannot be directly applied
for estimating the unknown parameters of a family of pdfs. Our proposed detection method involves
a binary class discriminator that separates the target data points from noise or background data. The
noise or background data is assumed to come from the same family of distribution of in-domain
data but with different moments (Please refer to the appendix for more details about the family of
distributions and its extension to a general structure). In image classification, this typically represents
the background patches from input data that fall under the same distribution family. In speech domain,
it can be random noise or the silence intervals in speech data. Collecting such background data to
improve the feature representations is much simpler as compared to using labeled training data since
it is time-consuming and expensive to collect labeled data. Since the background patches in images
or noise in speech signals are used for binary classification in our method, we refer to such data
as the noise of an auxiliary binary classification problem denoted by auxiliary binary classification
(ABC)-noise dataset. An advantage of using ABC-noise data during training is that it can implicitly
add robustness to deep neural networks against the background or noisy data.
Since ABC-noise data can be collected in large quantities for free and using that data in our approach
improves the classification benchmarks, we investigate whether this data can act as a substitute for
labeled data. We conduct empirical analysis and show that using only a fraction of labeled training
data together with ABC-noise data in our DBT method, indeed improves the accuracy as compared
to normal training.
To summarize, our contributions are threefold. First, we present a detailed theoretical analysis on
the relation between the estimation of unknown parameters of pdf of data and classification outputs.
Second, based on the theoretical analysis, we present a simple booster training method to improve
classification accuracy which also doubles up as an augmented training method when only limited
labeled data is available. Third, we consistently achieve improved performances over strong baselines
on face recognition, image classification, and speaker recognition problems using our proposed
method, showing its generalization across different domains and model architectures.
2	Related work
Notations and Preliminary: In this paper, vectors, matrices, functions, and sets are denoted by bold
lower case, bold uppercase, lower case, and calligraphic characters, respectively. Consider a datapoint
denoted by x. We assume that x belongs to a family of probability density functions (pdf’s) defined
as P = {p(x, θ), θ ∈ Θ}, where Θ is the possible set of parameters of the pdf. In general, θ is a real
vector in higher dimensions. For example, in a mixture of Gaussians, θ is a vector containing the
component weights, the component means, and the component covariance matrices. In this paper, we
assume that θ is an unknown deterministic function (There are other approaches such as bayesian
that consider θ as a random vector). In general, although the structure of the family of pdfs is itself
unknown, defining a family of pdfs such as P can help us to develop theorems and use those results
to derive a new method. For the family of distribution P, we can define the following classification
problem
{ Cl ： θ ∈ Θ1,C2 ： θ ∈ Θ2, ∙∙∙,Cn ： θ ∈ Θn }	(1)
where set of Θis is a partition of Θ. The notation of (1) means that, class Ci deals with a set of
data points whose pdf is p(x, θi ) where θi ∈ Θi . A wide range of classification problems can be
defined using (1) e.g., ((Lehmann & Casella, 2006, Chapter 3)) and ((Duda et al., 2012, Chapter 4)).
The problem of estimating θ comes under the category of parametric estimation or point estimation
(Lehmann & Casella (1998)). Estimating the unknown parameters of a given pdf p(x, θ), have been
extensively studied in the field of point estimation methods (Lindgren (2017); Lee et al. (2018);
Lehmann & Casella (2006)). An important estimator in this field is the minimum variance unbiased
estimator and it is governed by the Cramer Rao bound. The Cramer Rao bound provides the lower
bound of the variance of an unbiased estimator (Bobrovsky et al. (1987)). Let the estimation of
θ be denoted by θ, and assume that θ is an unbiased estimator, i.e., E(θ) = θ. Its covariance
matrix denoted by Σθb satisfies Σθb - I-1(θ) 0, where A 0 implies that A is a non-negative
definite matrix ((Lehmann & Casella, 1998, chapter 5)) and I(θ) := -E(∂2 log(p(x, θ))∕∂θ2)
is called the Fisher information matrix. For an arbitrary differentiable function g(∙), an efficient
estimator of g(θ) is an unbiased estimator when its covariance matrix equals to Ig-1(θ), where Ig-1(θ)
is the fisher information matrix of g(θ), i.e., the efficient estimator achieves the lowest possible
2
Under review as a conference paper at ICLR 2021
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
variance among all unbiased estimators. The efficient estimator can be achieved using factorization of
∂ log(p(x, θ))∕∂g(θ) = Ig(θ)(b(x) - g(θ)), if it exists (Rao (1992); Lehmann & Casella (1998)).
Based on these results, we derive a relationship between the efficient estimation of unknowns and
maximum likelihood classifier of (1) and use auxiliary binary classifiers to apply that result in our
proposed DBT method.
Parameter Estimations: Independent component analysis (Hyvarinen (1999)) decomposes a multi-
variate signal into independent non-Gaussian signals. ICA can extract non-Gaussian features from
Gaussian noise. Additionally, there is a class of classifiers called generalized likelihood ratio functions
that replaces the estimation of unknown parameters into the likelihood functions. This approach
provides a huge improvement in the field of parametric classifiers, where the family of pdf of data
is given (Zeitouni et al. (1992), Conte et al. (2001), Lehmann & Casella (2006)). Noise-contrastive
estimation (NCE) (Gutmann & Hyvarinen (2010)) involves training a generative model that allows
a model to discriminate data from a fixed noise distribution. Then, this trained model can be used
for training a sequence of models of increasing quality. This can be seen as an informal competition
mechanism similar in spirit to the formal competition used in the adversarial networks game. In
Bachman et al. (2019), a feature selection is proposed by maximizing the mutual information of the
difference between features extracted from multiple views of a shared context. In that work, it is
shown that the best results is given by using a mutual information bound based on NCE. The key
difference between our method and NCE is that, we do not construct a generative model for noise.
Instead of estimating the pdf of noise in NCE, we estimate the parameters of pdf of in-domain dataset
using an auxiliary class that has many common parameters in its pdf. Moreover, we show that the
estimation of that parameters are sufficient statistic for a classifier. We assume that the noise dataset is
not pure and it has some similarity with the in-domain dataset, where it can help the feature selection
layers to select relevant (in-domain) features, e.g., see Fig. 3. Further, in our approach, we do not
construct the pdf of noise or in-domain data, instead we estimate its parameters directly, which is
more efficient in terms of training, computation and also dimensionality reduction.
Auxiliary classifiers were introduced in inception networks (Szegedy et al. (2015)) and used in (Lee
et al. (2015); S. et al. (2016)) for training very deep networks to prevent vanishing gradient problems.
Further, auxiliary classifiers were also proposed for early exit schemes (Teerapittayanon et al. (2016))
and self-distillation methods (Zhang et al. (2019a;b)). Such auxiliary classifiers tackle different
problems by predicting the same target as the final classification layer. In contrast, our proposed DBT
method involves auxiliary binary classifiers that detect noise, interference, and/or background data
from in-domain data points for improving the target classification accuracy.
3 Estimation of Parameters of pdf and Classification
For (1), We define a deterministic discriminative function of Θi, denoted by ti(∙) such that the
following conditions are satisfied:
•	ti(∙) maps Θ to real numbers such that ti(θ) > 0, if θ ∈ Θi and ti(θ) ≤ 0 for θ ∈ Θi.
•	ti(∙) is a differentiable function almost everywhere and fθ ∣ti(θ)∣dμι(θ) < ∞, where μι denotes
the Lebesgue measure.
The following theorem shows the relationship of ti(∙) and the log-likelihood ratio of class Ci versus
other classes. The proofs of Theorems 1, 2 and 3 are provided in the appendix.
Theorem 1	Assume that the pdf p(x, θ) is differentiable with respect to θ almost everywhere. If the
efficient minimum variance and unbiased estimation of a deterministic discriminative function of Θi
exists, then the log likelihood ratio of class i against the rest of classes is an increasing function of
the minimum variance and unbiased estimation of Θi.
Directly from this theorem, it follows that the optimal classifier using the maximum likelihood for (1)
is given as follows d(x) = arg maxi∈{i,…,n} ki(b(x)), where ki's are some increasing functions and
ti(∙)'s are the deterministic discriminative function of Θ∕s such that the efficient minimum variance
and unbiased estimation for them exists. Based on this result, a set of minimum variance and unbiased
estimation of deterministic discriminative functions of Θi ’s leads us to the maximum likelihood
classifier. One approach is to directly estimate the deterministic discriminative functions, instead of
maximizing the likelihood function. However, finding deterministic discriminative functions that
have efficient minimum variance and unbiased estimation may not be feasible in practical problems,
3
Under review as a conference paper at ICLR 2021
Theorem 1
Error rate that is achieved by the true likelihood function
assuming that the unknown parameters are known
Figure 1: Visualizing Theorems 1,2 and 3 Figure 2: A general schema of our proposed
DBT method with PEF, DDF and ABC blocks
MUIti—class OUtPUtS
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
especially when the dimension of θ increases. Theorems 2 and 3 study the same relationship between
the estimation of unknown parameters and the accuracy of classifiers for sub-optimal estimators and
classifiers.
Theorem 2 Consider the output of two classifiers for the ith class as follows: rj (x) = i if hj (x) > τ
and rj (x) = other classes if hj (x) < τ, where j ∈ {1, 2}. where hj (x) is the estimation of a
deterministic discriminative function and τ is a classification threshold. Assume that the cumulative
distribution function of hj (x)’s have bounded inflection points, and also, the probability of true
positive of rj (x) is an increasing function of d(θ), which is the deterministic discriminative function
of class i, for all i. Further assume that for each τ the probability of false positive of r1 (x) is less
than the probability of false positive of r2(x) and the probability of true positive of r1(x) is greater
than the probability of true positive of r2(x). Then, there exists a hmin such that for all d(θ) > hmin
and all θ we have Pr(∣h1(x) - d(θ) | < e) > Pr(∣h2(x) - d(θ) | < e).
Theorem 2	shows that a better classifier leads to a better estimation of d(θ). In the next theorem, we
show the dual property of this result.
Theorem 3	Let Θm be a Borel set with positive Lebesgue measure in (1) for all m ∈ {1, ∙ ∙ ∙ , n}.
ʌ
ʌ
Assume that r1 (∙) and r2 (∙) are given as follows r1 (x) = m, if θ1 ∈ Θm and r2 (x) = m, if θ2 ∈ Θm.
Also, assume that Pr(kθ1 - θk ≤ ) ≥ Pr(kθ2 - θk ≤ ), for all θ ∈ Θ = ∪nm=1Θm and > 0,
then the probability of classification error r1 (∙) IS less than r2 (∙) where θ1 and θ2 are two different
estimators of θ ∈ Θ = ∪mM=-01Θm.
Theorem 3 proves that a more accurate estimator leads to a classifier that has a lower probability
of classification error. From Theorem 1, we can infer that a sufficient statistic for developing the
maximum likelihood classification is ti(x), which is the efficient minimum variance and unbiased
estimation of the deterministic discriminative functions of Θi's denoted by ti (θ). In other words, the
maximum likelihood classifier is a function of x only via the efficient minimum variance and unbiased
estimation ti(θ). We can estimate ti(θ) by replacing the estimation θ in ti(∙), ι.e., ti(θ) ≈ ti(θ),
where θ is a function of x. From the above theorems, we conclude that improving the estimation
of unknown parameters of pdf of data can improve the accuracy of the classifier. On the other side,
having a good classifier means having a good estimator of unknowns of the pdf of input data. In
many practical problems, the optimal maximum likelihood classifier may not be achievable, but the
likelihood function of the classifier provides an optimal bound of the probability of error. In such
cases, we can improve the accuracy of sub-optimal classifiers and that is the main focus of this paper.
Fig. 1 illustrates the proposed theorems visually.
4 Proposed Method: Detection Booster Training (DBT)
In this section, we propose the detection booster training (DBT) method based on the achieved
theorems in the previous section to improve the accuracy of deep networks. Specifically, we divide
a deep model into two parts - early and later layers. We apply a detector (detection here means
detecting a target pattern from noise/background) on the early layers of the neural network in order
4
Under review as a conference paper at ICLR 2021
Loss	Ver. Acc. (%)	Loss	Acc.	Acc. on H-set
ReSNet-50-DBT (CE) ReSNet-50-DBT	98.96 99.12	ResNet-100-AF ResNet-100-DBT	78.85 81.11	00.04 21.00
Table 1: Verification	accuracy on LFW	Table 2: Comparison of Rank-1 identification		
dataset for two different LABC trained using		accuracy on the IJB-B, with animal distrac-		
CASIA Yi et al. (2014) dataset.		tors.		
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
to improve the estimation of unknown parameters of the family of pdf (based on Theorem 2). A
better estimation of unknown parameters corresponds to better feature representations in the early
layers and these features are input to the rest of the layers to construct the deterministic discriminative
functions (DDF) useful for the in-domain data classification (based on Theorem 3).
A general schema for dividing a deep model into two sub-models namely PEF (parameter estimator
functions) and DDF is depicted in Figure 2. The early layers of the model estimate the unknown
parameters of pdf of data while the later layers construct the discriminative functions essential for
classification. Based on this scheme, we formally define the three main components of DBT as
follows:
•	parameter estimator functions (PEF): The sub-network from input layer to the kth layer, where k is
a hyperparameter in the DBT approach.
•	auxiliary binary classification (ABC): Some additional layers are attached to the end of PEF,
mapping the output of the kth layer to a one-dimensional vector.
•	deterministic discriminative functions (DDF): The sub-network from kth layer to the output of the
model. The output of model is a vector equal to the length of the number of classes n.
From Theorem 2, we showed that unknown parameter estimation can be improved using a detection
approach. During training, we apply a binary classification on the early layers (PEF) of the model to
improve the estimation of unknown parameters of pdf and subsequently provide rich feature vectors
for DDF. We define the auxiliary binary classification problem (ABC problem) as follows:
•	Class 1 (alternative hypothesis) of ABC problem denoted by H1 is set of all data points of classes
of C1 to Cn, i.e. θ ∈ ∪in=1Θi.
•	Class 0 (null hypothesis) of ABC problem denoted by H0 is a dataset of data points from same
distribution p(x, θ) but θ ∈/ ∪in=1Θi. We also define the dataset of Class 0 of ABC as ABC-noise
dataset, i.e., the ABC is given by the following hypothesis testing problem: H1 : θ ∈ ∪in=1Θi versus
H0 : θ ∈/ ∪in=1Θi. In many practical problems, the noise, background or interference data related to
the in-domain dataset have same type of probability distribution but different pdf parameters. Hence,
using that dataset is a cheap and adept choice for the null hypothesis of ABC.
The Auxiliary Binary Classification problem influences only the PEF and ABC units while the main
classification problem with n classes updates the parameters of both PEF and DDF using in-domain
data. Since the auxiliary classifier is only used during training, the inference model (IM) consists of
only PEF and DDF and hence, there is no additional computation cost during inference. We formulate
the aforementioned method in the following notations and loss functions. Assume that x is a data
point that belongs to Class Ci, i ∈ {1, ∙∙∙ , n} or Class Ho of ABC. Here, We define two type of
labels denoted by lABC and lMC, where the subscription "MC" stands for multi-classes. So, if x
belongs to class Ci, then lABC = 1 and lMC = i - 1, else if x is a ABC-noise data point, lABC = 0
and lMC is None. Therefore, the loss function is defined as:
Ltot = LABC(QABC(QPEF(x)), lABC) + λlABCLMC(QDDF(QPEF(x)), lMC),	(2)
where QPEF , QABC and QDDF are the functions of PEF, ABC and DDF blocks, respectively. We
set the hyperparameter λ = 1 to balance the two loss terms. It is seen that, the second term of the
total loss is zero if lABC = 0. LABC and LMC are selected based on the problem definition and
datasets. For classification, a simple selection for them can be binary cross-entropy and cross-entropy,
respectively. For a given task and deep neural network, the choice of k and LABC influences the
feature representation of early layers differently and consequently the accuracy of the model. We
provide empirical studies in the next section to verify the same.
5
Under review as a conference paper at ICLR 2021
Figure 3: Maximally activated receptive fields
of layer 15 of Inception-ResNet-v1 with (top
row) and without (bottom row) DBT.
Figure 4: Examples of mis-identified faces
along with their corresponding animal distrac-
tors on the IJB-B for ArcFace.
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
5	Experimental study of DBT
Face Recognition
We conduct experiments on face recognition benchmarks and show that the DBT method learns rich
features essential for face recognition. We also discover an important observation that current state-
of-the-art (SOTA) face recognition models are very sensitive to non-face data, in particular, animal
faces. Fig. 4 shows a few examples of misidentified faces and their corresponding animal distractors
from the IJB-B dataset using the ArcFace (Deng et al. (2019)) model. We show that our DBT method
not only improves the verification accuracy but also implicitly tackles this robustness issue of current
models against non-face data. Implementation details are provided in the appendix.
We consider the PEF discussed in Section 4 to be the first three layers of the model and DDF to be
the rest of layers. Ablation studies on the choice of PEF and DDF are provided in the supplementary
material. We define LMC in (2) as the SOTA ArcFace loss function proposed in (Deng et al. (2019)).
The ABC-noise is a non-face dataset containing 500K images that we collected from background
patches of MS1MV2 (Guo et al. (2016)) (More details in Appendix). We experimented with two
different loss functions for LABC. For the first one, since popular face recognition models (Deng et al.
(2019); Wang et al. (2018)) use normalized output features and compute the losses on a hypersphere,
we select LABC as follows. Let pf ∈ Rd and pnf ∈ Rd denote the prototypes for faces and non-
faces, respectively. Following (Mettes et al. (2019)), we constrain the face/non-face prototypes on
diametrically opposite directions i.e cos(θpf pnf) = -1 and normalize the output feature vectors for
faces and non-faces such that kpfi k = kpnf i k = 1. We then define the LABC as,
1 N	es(cos(m1 θyi +m2)-m3)	1 N
LABC = - N Elog Is(cos(m1θyi+m2)-m3) + escosθ2)+ N Σ√( ɪ	1pfi.pnfiD ,	⑶
where θyi and θ2 correspond to the angles between the weights and the features for face and non-face
labels, respectively; m1, m2, m3 are the angular margins; s denotes the radius of the hypersphere. For
the second choice, we use simple binary cross entropy for LABC . Table 1 shows that the verification
accuracy on LFW (Huang et al. (2007)) using (3) is 0.16% higher than simple cross entropy loss. This
also shows that choosing a task-specific LABC is essential in obtaining more accurate results. We use
Eqn.1 as the default for LABC in all our face recognition experiments, unless otherwise stated.
Table 3 compares the verification accuracy of our method versus the current SOTA method ArcFace
on five different test sets, LFW, CPLFW (Zheng & Deng (2018)), CALFW (Zheng et al. (2017)),
CFP-FP (Sengupta et al. (2016)) and AgeDb-30 (Moschoglou et al. (2017)). For the LFW test set,
we follow the unrestricted with labeled outside data protocol to report the performance. We trained
ResNet-50 and ResNet-100 using ArcFace and DBT approaches on CASIA (small) and MS1MV2
(large) datasets, respectively. The results show that DBT method outperforms ArcFace on all datasets.
Table 7 shows the angle statistics of the trained ArcFace and DBT models on the LFW dataset. Min.
Inter and Inter refer to the mean of minimum angles and mean of all angles between the template
embedding features of different classes (mean of the embedding features of all images for each class),
respectively. Intra refers to the mean of angles between xi and template embedding feature for each
class. From Table 7, we infer that DBT extracts better face features and hence reduces the intra-class
variations. Directly from Tables 3 and 7, we infer that first, DBT consistently improves the accuracy
6
Under review as a conference paper at ICLR 2021
Method	LFW	CALFW	CPLFW	CFPFP	AgeDb-30
ResNet-50-AF (ArcFace)	98.46	89.48	80.88	86.74	-88.98-
ResNet-50-DBT	99.12	91.38	87.10	94.95	91.23
ResNet-100-AF (ArcFace)	99.61	94.50	89.35	96.14	-95.33-
ResNet-100-DBT	99.75	95.13	90.70	96.90	96.16
Table 3: ArcFace vs. DBT-ArcFace: verification(%) accuracy on LFW, CALFW, CPLFW, CFP-FP
and AgeDb-30 of models ResNet-100 and ResNet-50.
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
on all test sets. Second, learning better features in the early layers is crucial to obtain rich face feature
embeddings. Third, the achieved gain using DBT is more pronounced on models trained using a
smaller (CASIA) dataset (it has fewer identities and images). This shows that DBT can address the
issue of the lack of in-domain data using cheap ABC-noise data.
We also provide the results of training Inception-ResNet-V1 and ResNet-64 models using DBT on
MS1MV2 to show the generalization capacity of the DBT method. For the Inception-ResNet-V1 and
ResNet-64, the PEF is set to be the first six layers and the DDF is the rest of the model. We use large
margin cosine loss (LMCL) Wang et al. (2018) for LMC and Cross entropy (CE) for LABC . Table 4
shows the verification accuracy on LFW for Inception-ResNet-V1 and ResNet-64 models trained
on MS1MV2 with and without DBT. The results show that DBT method is independent of model
depth or architectures or loss functions and thereby consistently improves the accuracy compared
to baseline results. Table 4 also compares the DBT method with state-of-the-art methods on LFW
and YTF datasets. DBT method notably improves the baselines that are comparable to ArcFace and
superior to all the other methods. We were not able to reproduce the results of the ArcFace paper
using our Tensorflow implementation and dataset. We believe that using the original implementation
and dataset from ArcFace will achieve superior results over the baselines on the benchmark datasets
as evident from the results of our implementation. Finally, we compare the result ArcFace and DBT
on IJB-B and IJB-C, in Table 5. It is seen that DBT provides a notable boost on both IJB-B and
IJB-C by a considerable margin. DBT improves the verification accuracy as high as 1.94 % on IJB-B
and 2.57 % on IJB-C dataset at 10-4 false alarm rate (FAR). We plot the receptive fields of the top
ten maximally activated neurons of an intermediate layer of the face recognition model to visualize
the features learned using the DBT method. Fig. 3 shows that the receptive fields of layer 15 of
the inception-resnet-v1 model trained using DBT attends to the regions of eyes, nose and mouth as
compared to insignificant regions in the normal training method. This shows that DBT learns more
discriminative features essential to face recognition, corroborating our theoretical claims.
To show that current SOTA models are not robust to animal faces, we performed a 1:N identification
experiment with approximately 3000 animal distractors on the IJB-B (Whitelam et al. (2017)) dataset.
We trained the face recognition model with about 500K non-face data which contains 200 animal
faces. This is disjoint from the 3000 distractors used in the identification experiment. We collected the
animal faces from web images using MTCNN (Zhang et al. (2016a)) face detector which are the false
positives from the face detector. Table 2 shows the Rank-1 identification accuracy of ResNet-100
on IJB-B dataset, trained on MS1MV2 using the ArcFace loss (ResNet-100-AF) versus our DBT
approach (ResNet-100-DBT). The third column of Table 2 denotes the accuracy on a hard subset
of images (false positives from ArcFace model) on the IJB-B dataset denoted by H-set. Results
of Table 2 show that current face recognition models are unable to discriminate out-of-distribution
(non-face) images from face images. Our ResNet-100-DBT significantly (as high as 21%) reduces the
misidentification rate as compared to the ArcFace model which shows that DBT method inherently
overcomes this issue while also improving face recognition accuracy.
Image Classification
In this section, we evaluate ResNet-110 and ResNext-101 models trained with and without DBT on
image classification problem using CIFAR-10, CIFAR-100, and ImageNet. We also show the power
of DBT to compensate for the smaller in-domain training set. For all implementations, PEF is defined
to be the first three layers and DDF is the rest of the model. LABC and LMC are set to cross-entropy
loss. ABC-noise is the same data used in face recognition experiments. We follow the same training
configurations from (He et al. (2016); Xie et al. (2017)).
To study the efficacy of the DBT method in augmenting smaller in-domain training datasets, we
also trained ResNet-100 and ResNext-101 using partial training data on CIFAR-10 and CIFAR-100.
7
Under review as a conference paper at ICLR 2021
Model	Loss	LFW	Method	LFW	YTF
Inception Resnet	-CE-	^9945	Center Loss	99.28	94.9
Inception Resnet-DBT	CE	99.50	Range Loss	99.52	93.7
Inception Resnet	LMCL	^9955	SphereFace	99.42	95.0
Inception Resnet-DBT	LMCL	99.60	SphereFace+	99.47	-
Resnet 64	-CE-	^9955	CosFace	99.73	97.6
Resnet 64-DBT	CE	99.63	ArcFace	99.82	98.02
Resnet 64	LMCL	-99.65	ArcFace**	99.61	97.31
Resnet 64-DBT	LMCL	99.68	ReSNet-100-DBT	99.75	97.67
Table 4: Comparison of DBT models with SOTA methods on LFW and YTE ArcFace ** refers to
our arcface implementation.
Method		10-5	IJB-B 10-4	10-3	10-2	10-1	10-6	IJB-C			10-2	10-1
	10-6							10-5	10-4	10-3		
ArcFace 38.47		65.60	82.97	91.11	96.01	98.91	61.96	73.22	83.84	91.85	96.51	99.08
DBT	47.01	72.70	84.91	91.92	96.37	99.03	67.42	77.33	86.41	92.75	96.66	99.06
Table 5: 1:1 verification: ResNet-100: DBT vs. ArcFace on the IJB-B and IJB-C datasets
Method	Top-1	Top-5	Method	Min. Inter	Intra	Inter
ResNet	22.10	6.15	ArcFace	-53.23-	7.2	88.73
ResNet-DBT	21.82	6.02	ResNet-DBT	52.96	7.16	88.52
Table 6: Top-1 and Top-5 error rates (%) on ILSVRC15 benchmark for ResNet w/o DBT.	Table 7: Comparison of inter and intra angles (degrees) for different methods on LFW.
ResNet Models	CIFAR-10	CIFAR-100	ResNext Models	CIFAR-10	CIFAR-100
He et al.(2016)*	584	-22.T5	Xie etal. (2017)*	503	21:24
DBT (5/5)	5.25	21.53	DBT (5/5)	4.68	19.79
ReSNet (4/5)	589	24:23	-ReSNext (4/5)-	-493	-23.52
DBT (4/5)	5.36	23.98	DBT (4/5)	4.76	22.56
ReSNet (3/5)	-6.61	-27.99	-ResNext (3/5)-	538	-27.25
DBT (3/5)	5.44	26.81	DBT (3/5)	4.77	26.04
ReSNet (2/5)	-706	33:81	-ResNext (2/5)-	585	33:62
DBT (2/5)	5.94	31.95	DBT (2/5)	5.05	30.73
ReSNet (1/5)	8.20	-47.43	-ResNext (1/5)-	-724	-48.05
DBT (1/5)	6.86	43.65	DBT (1/5)	6.05	42.56
Table 8: Comparison of Top-1 error rates (%) for CIFAR-10 and CIFAR-100 datasets w/o DBT.*
denotes our implementation. (x/5) denotes the fraction of training data used for training that model.
Method	VoxC (top 1)	VoxC (top 5)	Librispeech	VCTK	ELSDSR
VGG-M CNN	805	921	93712	82.52	79.98
VGG-M CNN-DBT	82.3	95.8	95.62	88.14	81.56
Table 9: Accuracy of speaker identification (%) for different datasets.
Method	CIFAR-10	CIFAR-100
ResNet-Back	565	2184
ResNet-DBT	5.25	21.53
ResNext-Back	-4797	2165
ResNext-DBT	4.68	19.79
Table 10: Comparison of top-1 error rates on CIFAR-10 and CIFAR-100 using an additional back-
ground class vs DBT.
8
Under review as a conference paper at ICLR 2021
Method	LFW	CALFW	CPLFW	CFP-FP	AgeDb-30
ResNet+mod	99.16	91.46	86.11	93.81	-9271-
ResNet-DBT+mod	99.65	95.05	90.08	96.20	95.87
Table 11: Ablation study on the verification performance of adding background class to the model on
MS1MV2 dataset.
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
We randomly selected a fraction of the training data to be our training set, e.g., k/5 of dataset
means that we only used k fifth of total samples for training. From first row of Table 8, we find that
models trained with DBT show 0.59% and 0.35% improvement on CIFAR-10, 0.62% and 1.45%
improvement on CIFAR-100 over baseline models for ResNet-110 and ResNext-101 architectures,
respectively. Furthermore, using partial training data with our DBT method achieves superior results
(as high as 5.49 % on ResNext (1/5) CIFAR-100) as compared to normal training. Table 6 shows
the results on Imagenet. We see that DBT improves the accuracy by 0.28% on Top-1 accuracy. This
shows that the DBT method consistently improves the results on both small and large datasets.
Speaker Identification
We consider the problem of speaker identification using the VGG-M (Chatfield et al. (2014)) model.
We set PEF as the first two CNN layers and DDF as the remaining CNN layers. LABC and LMC
are defined to be the cross-entropy loss. The ABC-noise is generated from the silence intervals of
VoxCeleb (Nagrani et al. (2017)) augmented with Gaussian noise with variance one. The input to the
model is the short-time Fourier transformation of speech signals with a hamming sliding window
of width 25 ms and step 10 ms. Table 9 provides the accuracies of VGG-M model trained with and
without DBT on VoxCeleb, Librispeech (Panayotov et al. (2015)), VCTK (Veaux et al. (2016)) and
ELSDR (L. (2004)) datasets. Table 9 shows that the trained models using DBT significantly improves
the accuracy (as high as 5.62%) for all datasets. Implementation details are provided in the appendix.
Miscellaneous Experiments
In this section, we experiment with the naive way of using background data by considering non-faces
as a separate class in the final classification layer. For face recognition, Table 11 shows the results
of training with an additional background class on MS1MV2 dataset with and without using DBT.
ResNet+mod refers to a model trained with ArcFace loss and n + 1 classes where the additional class
corresponds to the non-faces. ResNet-DBT+mod refers to a model trained with both DBT and the
additional non-face class. We find that adding the additional non-face class hurts the performance
of the model whereas ResNet-DBT+mod improves the results significantly relative to ResNet+mod
model. Since the non-face dataset is sampled from a wide range of a family of distributions compared
with faces, it has a larger range of unknown parameters, then the sufficient statistic of them should be
larger than the sufficient statistics of face data. Thus, when we restrict faces and non-faces on the
surface of a hypersphere, the non-face data is more spread on the surface compared with each of the
other face classes. We demonstrate this effect with the help of a toy example in Fig. 6 in the appendix.
We also conduct this experiment on CIFAR-10/CIFAR-100 and report it in Table 10. We see that
naively incorporating the background class is inferior to DBT showing that DBT is an effective
technique to utilize background data to boost the performance of classification models.
6 Conclusion
In this paper, we presented a detailed theoretical analysis of the dual relationship between estimating
the unknown pdf parameters and classification accuracy. Based on the theoretical study, we presented
a new method called DBT using ABC-noise data for improving in-distribution classification accuracy.
We showed that using ABC-noise data helps in better estimation of unknown parameters of pdf of
input data and thereby improves the feature representations and consequently the accuracy in image
classification, speaker classification, and face recognition benchmarks. It also augments the training
data when only limited labeled data is available by improving accuracy. We showed that the concept
of DBT is generic and generalizes well across domains through extensive experiments using different
model architectures and datasets. Our framework is complementary to existing training methods and
hence, it can be easily integrated with current and possibly future classification methods to enhance
accuracy. In summary, the proposed DBT method is a powerful technique that can augment limited
training data and improve classification accuracy in deep neural networks.
9
Under review as a conference paper at ICLR 2021
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, and S. Ghemawat and. TensorFlow: Large-scale machine learning on heterogeneous
systems, 2015.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In Advances in Neural Information Processing Systems, pp.
15535-15545, 2019.
T. L. Berg, A. C. Berg, J. Edwards, and D. A. Forsyth. Who’s in the picture. NeurIPS, 2004.
David Berthelot, Colin Raffel, Aurko Roy, and Ian Goodfellow. Understanding and improving
interpolation in autoencoders via an adversarial regularizer. ICLR, 2019.
Ben-Zion Bobrovsky, E Mayer-Wolf, and M Zakai. Some classes of global Cramer-rao bounds. The
Annals of Statistics, pp. 1421-1438, 1987.
Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the
details: Delving deep into convolutional nets. BMVC, 2014.
Ernesto Conte, Antonio De Maio, and Giuseppe Ricci. Glrt-based adaptive detection algorithms for
range-spread targets. IEEE transactions on signal processing, 49(7):1336-1348, 2001.
J.	Deng, J. Guo, X. Niannan, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face
recognition. In Computer Vision and Pattern Recognition (CVPR), 2019.
Richard O Duda, Peter E Hart, and David G Stork. Pattern classification. John Wiley & Sons, 2012.
Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
Zisserman. The pascal visual object classes challenge: A retrospective. International journal of
computer vision, 111(1):98-136, 2015.
F.	F. Li, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An
incremental bayesian approach tested on 101 object categories. In CVPR Workshop, pp. 178-178,
2004.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. ICLR, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, pp. 2672-2680, 2014.
Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale
face recognition. ECCV, 9907:87-102, 2016.
Michael Gutmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, pp. 297-304, 2010.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. Computer Vision
and Pattern Recognition, pp. 770-778, 2016.
G.	B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for
studying face recognition in unconstrained environments. In Technical Report, 2007.
Aapo Hyvarinen. Survey on independent component analysis. 1999.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. International Conference on Machine Learning, 37:448-456, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. ICLR, 2014.
Feng L. Speaker recognition, informatics and mathematical modelling. Technical University of
Denmark, DTU, 2004.
10
Under review as a conference paper at ICLR 2021
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
C-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-Supervised Nets. Proceedings of
Machine Learning Research (PMLR), 38:562-570, 2015.
Youngjo Lee, John A Nelder, and Yudi Pawitan. Generalized linear models with random effects:
unified analysis via H-likelihood, volume 153. CRC Press, 2018.
E. L. Lehmann and G. Casella. Theory of point estimation, 1998. 2ndn ed.
Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolldr, and C LaWrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Bernard Lindgren. Statistical theory. Routledge, 2017.
B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K. Jain, W. T. Niggel, J. Anderson,
J. Cheney, and P. Grother. Iarpa janus benchmark - c: Face dataset and protocol. International
Conference on Biometrics, pp. 158-165, 2018.
P. Mettes, E. van der Pol, and C. Snoek. Hyperspherical prototype netWorks. NeuRIPS, 01 2019.
S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia, and S. Zafeiriou. Agedb: the first
manually collected, in-the-Wild age database. CVPR Workshop, 2(3):5, 2017.
Arsha Nagrani, Joon Son Chung, and AndreW Zisserman. Voxceleb: a large-scale speaker identifica-
tion dataset. arXiv preprint arXiv:1706.08612, 2017.
M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaW puzzles.
ECCV, 2016.
M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representa-
tions using convolutional neural netWorks. CVPR, pp. 1717-1724, 2014.
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public
domain audio books. International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 5206-5210, 2015.
BLS Prakasa Rao. Cramer-rao type integral inequalities for estimators of functions of multidimen-
sional parameter. Sankhya： The Indian Journal OfStatistics, SeriesA, pp. 53-73, 1992.
Ali Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features off-the-shelf:
an astounding baseline for recognition. CVPR Workshops, 2014.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and F.F Li. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV), 115(3):211-252, 2015.
Christian S., Vincent V., Sergey I., Jon S., and ZB W. Rethinking the inception architecture for
computer vision. CVPR, 2016.
S. Sengupta, J. Chen, C. Castillo, V. M. Patel, R. Chellappa, and D. W. Jacobs. Frontal to profile face
verification in the Wild. In Winter Conference on Applications of Computer Vision (WACV), pp.
1-9, 2016.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple Way
to prevent neural netWorks from overfitting. J. Mach. Learn. Res., 15(1):1929-1958, 2014.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper With convolutions. CVPR, pp. 1-9, 2015.
S. Teerapittayanon, B. McDanel, and H. T. Kung. Branchynet: Fast inference via early exiting from
deep neural netWorks. ICPR, 2016.
11
Under review as a conference paper at ICLR 2021
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Superseded-cstr vctk corpus:
English multi-speaker corpus for cstr voice cloning toolkit. University of Edinburgh. The Centre
for Speech Technology Research (CSTR), 2016.
H. Wang, Y. Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and W. Liu. Cosface: Large margin
cosine loss for deep face recognition. CVPR, pp. 5265-5274, 2018.
C.	Whitelam, E. Taborsky, A. Blanton, B. Maze, J. Adams, T. Miller, N. Kalka, A. K. Jain, J. A.
Duncan, K. Allen, J. Cheney, and P. Grother. Iarpa janus benchmark-b face dataset. CVPR
Workshops, pp. 592-600, 2017.
L. Wolf, T. Hassner, and I. Maoz. Face recognition in unconstrained videos with matched background
similarity. CVPR, pp. 529-534, 2011.
S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He. Aggregated residual transformations for deep neural
networks. CVPR, pp. 5987-5995, 2017.
D.	Yi, Z. Lei, S. Liao, and S. Z. Li. Learning face representation from scratch. arXiv, abs/1411.7923,
2014.
J.	Yosinski, J. Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural
networks? NIPS, 2014.
Ofer Zeitouni, Jacob Ziv, and Neri Merhav. When is the generalized likelihood ratio test optimal?
IEEE Transactions on Information Theory, 38(5):1597-1602, 1992.
K.	Zhang, Z. Zhang, Z. Li, and Y. Qiao. Joint face detection and alignment using multi-task cascaded
convolutional networks. Signal Processing Letters, 23(10):1499-1503, 2016a.
L.	Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma. Be your own teacher: Improve the
performance of convolutional neural networks via self distillation. ICCV, 2019a.
Linfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Scan:
A scalable neural networks framework towards compact and efficient models. NeurIPS, 2019b.
Richard Zhang, Phillip Isola, and Alexei Efros. Colorful image colorization. ECCV, 2016b.
T. Zheng and W. Deng. Cross-pose lfw: A database for studying cross-pose face recognition in un-
constrained environments. Technical Report, Beijing University of Posts and Telecommunications,
2018.
T. Zheng, W. Deng, and J. Hu. Cross-age lfw: A database for studying cross-age face recognition in
unconstrained environments. arXiv, abs/1708.08197, 2017.
Appendix
In-domain family of pdfs and the extended family of distributions
In this section, we discuss about background/noise and in-domain data points and their corresponding
distributions to clarify the definition of those concepts in this paper. Consider a random vector denoted
by s. Assume that the corresponding distribution is Gaussian with mean and variance given by α 6= 0
and σ = 1, respectively. Now, assume that we observed x = s + n, where the pdf of n is assumed to
be Guassian with zero mean and variance σn2, hence the pdfofx is Gaussian with mean α and variance
1 + σn2. Here, n is the background or noise data and the vector of unknowns is given by, θ = [α, σn2].
The in-domain family of pdfs for X is then given by Px = {N (α, 1 + σn )∣α = 0,σn > 0}. If We
include the family of pdf of n to Px, then we can extend Px as P = {N(α, 1 + σ∖) ∣α ∈ R, σ∖ > 0}.
So P is the union of family of pdfs of in-domain data points and noise/background data. From
estimation theory, we know that the sufficient statistics and the unknown parameters of P can also
represent the sufficient statistics and the unknown parameters of Px . In other words, an estimation of
α can help us detect if the observed data point is from s + n or n by comparing it with a threshold.
Thus, estimating the unknown parameters of the family of pdfs using P can provide more information
about the observed data useful for tasks such as classification.
In general, we can assume that a generalized family of pdfs is given by the family of pdf of noise or
background along with the family of pdfs of in-domain data. Hence, estimating from the extended
12
Under review as a conference paper at ICLR 2021
Figure 5: In-domain data point versus background data point. The background is cropped from the
in-domain image and provides complementary information to the main data, thereby we can provide
a better estimation of the pdf parameters of in-domain data.
499
500
501
502
503
504
505
506
507
508
509
510
511
512
family of distribution can provide more information about the in-domain distribution. Let us consider
that the pdf of in-domain data points is given by px(x, [θs, θn]) and the pdf of noise/background is
given by pn(x, θn), so the extended pdf can be represented by
h(pn (x, θn), px (x, [θs , θn])),
where h is a function that combines two pdfs in a general structure. So a general family of distribution
can be denoted as follows:
P = {h(Pn(x, θn ),Px(x, [θs, θn]))∣θ ：= [θs, θn] ∈。54},
where θ is defined as a new set of parameters in a higher dimension and Θs,n are set of all possible
[θs , θn] that belongs to pn and px. The extended family of pdf provides more information about
the nuisance parameters of pdf of in-domain datapoints. Inspired by this observation, we develop
our detection booster training method using background/noise data. Figure 5 shows an example of
background and in-domain data point.
Proof of Theorem 1
Let ti(∙) denote deterministic discriminative function of Θi. Since the efficient minimum variance
and unbiased estimation of ti(θ) exists, we have
d!n∂t⅛≡ = Iti ⑹(b(X)- M，
(4)
where tbi(x) is the minimum variance and unbiased estimation of ti(θ) using the data point x and
Iti (x) is the Fisher information function of ti(θ), which is given by
Iti(θ) =端TI(θ)端 ≥0,
where T denotes the transpose and I(θ) is the Fisher information matrix of θ. Now we show that
the log-likelihood ratio is an increasing function in tbi(x). Note that Iti (θ) ≥ 0 (Lehmann & Casella
(2006)).
On the other hand, We have dln(p(x, θ)) = P d吗，"""dθj, therefore,
ln(p(x, θ)) + k(x) = X Z ^n⅛x^dθj = X Z ⅛≡ 小j =
j	∂θj	j	∂ti (θ)	∂θj
Z dln(p(x,θ) X ⅛θ)dθj = Z (It, (θ)(b(x) - ti(θ))) X 驾黑dθj = α(θ)b(x) - β(θ) (5)
∂ti(θ)	彳 ∂θj	j	ti	iy -	∂θj ∂θj	j	i '八
where the third equality is archived based on the third property of ti(∙) in its definition and the forth
equality is given by replacing (4; k(x) is the constant of integration. Finally, the last equality is given
by defining the following terms
α(θ) := / Iti (θ) X d∂θθ)dθj,	β(θ) := / Iti (θ)ti(θ) X d∂θθ)dθj,
jj	jj
(6)
thus di(θ)=Iti (θ) ≥ 0, i.e., α(θ) is increasing in ti (θ). Since, ti is a deterministic discriminative
function of Θi, so for each j 6= i and θi ∈ Θi and θj ∈ Θj, we have ti (θi) > ti (θj ), therefore
13
Under review as a conference paper at ICLR 2021
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
α(θi) ≥ α(θj). The later inequality is achieved based on the increasing property of α(θ) with
respect to ti(θ).
Using (5), the log likelihood ratio of class i against the rest of classes is given by LLR :=
ln(p(x, θi)) - ln(p(x, θj)), so We have LLR = (α(θi) - α(θi))tb(x) - (β(θi) - β(θj)). LLR
depends on x only via tbi(x) and since for each j 6= i and θi ∈ Θi and θj ∈/ Θi, α(θi) - α(θi) > 0,
then LLR is increasing in tbi (x).
Proof of Theorem 2
The probability of true positive of class i of rj is given by
Ptp,i,j = Prθ(hj(x) > τ) = 1 - Fjθ (τ),
where 尤8 (∙) denotes the Cumulative distribution function (CDF) of hj. Since the probability of true
positive of class i of r1 is greater than r2 for all τ, F1θ (τ) < F2θ (τ), for all τ. NoW We define a
function as follows
u(τ, θ) := F2θ (τ) - F1θ (τ).
Since the CDFs are increasing in τ and tend to 1 and the number of inflection points of these CDFs
are bounded, there is an hmin such that, for τ > hmin, such that u(τ, θ) is a monotonically decreasing
function in τ. Thus for any θ that satisfies d(θ) > hmin we have
u(d(θ) +,θ) < u(d(θ) -,θ).
Replacing u(h, θ) = F2θ (h) - F1θ (h) in the last inequality, we have
F2θ(d(θ)+)-F1θ(d(θ)+) <F2θ(d(θ)-)-F1θ(d(θ)-) ⇒	(7)
F2θ (d(θ) + ) - F2θ (d(θ) - ) < F1θ (d(θ) + ) - F1θ (d(θ) - ).	(8)
Based on the definition of CDF, we have
Prθ (∣h2(x) - d(θ)∣ < e) = Prθ (d(θ) - e < h2(x) < d(θ) + E) <
Prθ (d(θ) - E < hι(x)) < d(θ) + E) = Prθ (∣hι(x) - d(θ)∣ < e) .	(9)
Proof of Theorem 3
First, we prove the following claim,
Claim: For any open set, there exists a set of disjoint countable open balls such that their union equals
the origin open set.
Proof of claim: Consider an open set O, and also consider x0 ∈ O, such that B(x0, r0) ⊆ O
and r0 is the greatest possible radius between all possible open balls in O, where B(x0, r0) is the
open ball with radius ro at point x°. Now, we define xι ∈ O - B(x0,r0), where B(x0,r0) is
the closure of B(x0,r0), as the point with greatest radius in O - B(xo, ro) and similarly Xi ∈
O - ∪k=0B(xk, rk) such that B(xi, ri) provides the greatest radius in O - ∪k=0B(xk ,rk). So
we have O = ∪k∞=0B(xk , rk). This is because, if the latest equality is not valid, then there exists
an open ball in O - ∪∞=0B (Xk ,rk) hence another open ball with greatest radius will be added to
∪k∞=0B(xk, rk ), which has a contradiction with the definition of ∪k∞=0B(xk, rk). The claim is proven
at this point.
Now, we show the true positive probability of r1 is greater than r2 . Let Θ0m be the set of interior
points of Θm, then, there exists a union of disjoint open balls such that Θ0m = ∪k∞=0B(Xk, rk). From
assumptions in the theorem, we have Pr(kθ1 - θk ≤ E) ≥ Pr(kθ2 - θk ≤ E), then
Prθ(θb1 ∈ B(Xk, rk)) ≥ Prθ(θb2 ∈ B(Xk, rk)),
where θ ∈ Θm . Based on the claim we have
Prθ(θb1 ∈ Θ0m) ≥ Prθ(θb2 ∈ Θ0m).	(10)
Moreover, based on definition of ri , the true positive probability of class m is given by
ptp,i = Prθ (θbi ∈ Θm) = Prθ(θbi ∈ Θm) + Prθ (θbi ∈ Θm - Θm),
14
Under review as a conference paper at ICLR 2021
Theorem 3 guarantees that the multi class classifier has a better performance, since it
is constructed from a better estimation of unknown parameters
In domain
PEF
DDF
Output
multi CIaSS
ABC-noise
inputs
In domain
ABC-noise
Output
I ：
____________ IABCJ a∣____________________________
Inference mode: Theorem 3 guarantees t⅛e accuracy of a multi^class
Classifierbdilt Usingestifnatesthe :uhkhŋwh parameters:
Figure 6: Relationship between the theorems in Section 3 and the proposed method in Section 4.
Figure 7: Feature distance between different classes with and without additional background class for
a toy example. Left: Contains 8 classes and the feature separation is visibly larger; Right: Contains
an additional noise class that decreases the feature distance for all the other classes.
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
for i = 1,2. Additionally, from the CauChy-SChWarz inequality, We have
Prθ (θi ∈ Θm - Θ0m) ≤ μi (Θm - θ") = 0,
So, ptp,i = Prθ(θi ∈ Θ0m) and from (10) the true positive probability of Class i of r1 is greater than
r1.
The error probability of rj is given by per,j = 1 - Pin=1 Pi Ptp,i,j, Where Pi is the prior probability
of Class i. Therefor, per,1 ≤ per,2 .
Connecting the theorems with the proposed method
Fig. 6 shoWs the ConneCtion betWeen the proposed theorems and the approaCh. In part 1, Theorem
2 ConneCts the estimation of unknoWn parameters to the auxiliary Classifier. In part 2, the learned
features are passed to a deCision making netWork (result of Theorem 2). In part 3, Theorem 3
guarantees that the multi-Class Classifier outperforms other Classifiers, beCause it is using the features
from a better estimation of unknoWn parameters of pdf.
Toy example:
We demonstrate the effeCt of adding baCkground Class to the original Classifier With a toy example
and visualize it in Fig. 7. In this example, the input is a sequenCe of binary bits (+1 and -1) With
length 3 in White Gaussian noise. the Classifier is ConstruCted using tWo fully ConneCted layers With
sigmoid and the last layer is normalized on unit CirCle. As seen from Fig. 7, adding an additional
noise Class visibly reduCes the feature separation betWeen all the other Classes.
Implementation Details
Face Recognition
We use TensorfloW (Abadi et al. (2015)) to ConduCt all our experiments. We train With a batCh
size of 256 on tWo NVIDIA TeslaV100 (32G) GPUs. We train our models folloWing small (less
than 1M training images) and large (more than 1M training images) protoCol Conventions. We use
CASIA-WebfaCe (Yi et al. (2014)) dataset for small protoCol and MS1MV2 dataset for the large
protoCol. We use ResNet-50 (He et al. (2016)) and ResNet-100 models for small and large protoCols,
respeCtively. The PEF is seleCted as the first three layers. FolloWing (Deng et al. (2019)), We apply
15
Under review as a conference paper at ICLR 2021
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
BN (Ioffe & Szegedy (2015)), dropout (Srivastava et al. (2014)) to the last feature map layer followed
by a fully connected layer and batch normalization to obtain the 512-D embedding vector. We set
the feature scale s parameter to 64 following (Wang et al. (2018); Deng et al. (2019)) and set the
margin parameters (m1, m2, m3) to (1, 0.5, 0), respectively. For small scale protocol, we start the
learning rate at 0.01 and divide the learning rate by 10 at 40K, 80K, and 100K iterations. We train for
120K iterations. For large scale protocol, we start the learning rate at 0.01 and divide the learning
rate by 10 at 80K, 100K, and 200K iterations. We train for 240K iterations. We use Momentum
optimizer and set the momentum to 0.9 and weight decay to 5e-4. We use the feature centre of all
images from a template or all frames from a video in order to report the results on IJB-B, IJB-C and
YTF datasets. For ABC-noise data, we cropped background images patches from MS1MV2 (Guo
et al. (2016)) dataset and cropped hard examples from the Caltech-101 (F. F. Li et al. (2004)) dataset
plus a few open sourced images (animal faces) using MTCNN (Zhang et al. (2016a)) face detector.
We generated roughly 500K non-face images for training the ABCs.
Speaker Identification
L2 loss and dropout with a rate of 0.2 are applied during training for generalization. The ABC-noise
is collected form silence intervals of the VoxCeleb dataset, where an energy-based voice activity
detection (VAD) is applied to detect the silence intervals. To augment the ABC-noise, Gaussian
noise is added to the silence intervals. Each batch size is set to 64 and the optimizer is ADAM with
a learning rate of 0.001. The VoxCeleb dataset is trained for 11 epochs and the other datasets are
trained for 6 epochs.
LFW and YTF datasets
LFW database contains the annotations for 5171 faces in a set of 2845 images taken from the Faces
in the Wild data set (Berg et al. (2004)). YouTubeFaces (Wolf et al. (2011)) contains 3,425 videos of
1,595 people. Following the standard convention, we report the results on 5000 video pairs using
unrestricted with labeled outside data protocol.
IJB-B and IJB-C datasets
The IJB-B contains 1,845 subjects with 21.8K still images and 55K frames from 7,011 videos. In
total, there are 12,115 templates with 10,270 genuine matches and 8M impostor matches. The IJB-C
dataset (Maze et al. (2018)) is a further extension of IJB-B, having 3,531 subjects with 31.3K still
images and 117.5K frames from 11,779 videos. In total, there are 23, 124 templates with 19,557
genuine matches and 15, 639K impostor matches.
16