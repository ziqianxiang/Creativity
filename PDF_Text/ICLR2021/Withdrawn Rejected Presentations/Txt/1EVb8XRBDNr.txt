Under review as a conference paper at ICLR 2021
RMIX: Risk-Sensitive Multi-Agent
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Centralized training with decentralized execution (CTDE) has become an impor-
tant paradigm in multi-agent reinforcement learning (MARL). Current CTDE-
based methods rely on restrictive decompositions of the centralized value func-
tion across agents, which decomposes the global Q-value into individual Q values
to guide individuals’ behaviours. However, such expected, i.e., risk-neutral, Q
value decomposition is not sufficient even with CTDE due to the randomness of
rewards and the uncertainty in environments, which causes the failure of these
methods to train coordinating agents in complex environments. To address these
issues, we propose RMIX, a novel cooperative MARL method with the Condi-
tional Value at Risk (CVaR) measure over the learned distributions of individuals’
Q values. Our main contributions are in three folds: (i) We first learn the re-
turn distributions of individuals to analytically calculate CVaR for decentralized
execution; (ii) We then propose a dynamic risk level predictor for CVaR calcula-
tion to handle the temporal nature of the stochastic outcomes during executions;
(iii) We finally propose risk-sensitive Bellman equation along with Individual-
Global-MAX (IGM) for MARL training. Empirically, we show that our method
significantly outperforms state-of-the-art methods on many challenging StarCraft
II tasks, demonstrating significantly enhanced coordination and high sample ef-
ficiency. Demonstrative videos and results are available in this anonymous link:
https://sites.google.com/view/rmix.
1	Introduction
Reinforcement learning (RL) has made remarkable advances in many domains, including arcade
video games (Mnih et al., 2015), complex continuous robot control (Lillicrap et al., 2016) and the
game of Go (Silver et al., 2017). Recently, many researchers put their efforts to extend the RL
methods into multi-agent systems (MASs), such as urban systems (Singh et al., 2020), coordination
of robot swarms (HUttenraUch et al., 2017) and real-time strategy (RTS) video games (Vinyals et al.,
2019). Centralized training with decentralized execution (CTDE) (Oliehoek et al., 2008; Kraemer
& Banerjee, 2016) has drawn enormoUs attention via training policies of each agent with access to
global trajectories in a centralized way and execUting actions given only the local observations of
each agent in a decentralized way. Empowered by CTDE, several MARL methods, inclUding valUe-
based and policy gradient-based, are proposed (Foerster et al., 2017a; SUnehag et al., 2017; Rashid
et al., 2018; Son et al., 2019). These MARL methods propose decomposition techniqUes to factorize
the global Q valUe either by strUctUral constraints orby estimating state-valUes or inter-agent weights
to condUct the global Q valUe estimation. Among these methods, VDN (SUnehag et al., 2017) and
QMIX (Rashid et al., 2018) are representative methods that Use additivity and monotonicity strUctUre
constraints, respecitively. With relaxed strUctUral constraints, QTRAN (Son et al., 2019) gUarantees
a more general factorization than VDN and QMIX. Some other methods inclUde incorporating an
estimation of advantage valUes (Wang et al., 2020a) and proposing a mUlti-head attention method to
represent the global valUes (Yang et al., 2020).
Despite the merits, most of these works focUs on decomposing the global Q valUe into individUal Q
valUes with different constraints and network architectUres, bUt ignore the fact that the expected, i.e.,
risk-neUtral, valUe decomposition is not sUfficient even with CTDE dUe to the randomness of rewards
and the Uncertainty in environments, which caUses the failUre of these methods to train coordinating
agents in complex environments. Specifically, these methods only learn the expected valUes over
1
Under review as a conference paper at ICLR 2021
returns (Rashid et al., 2018) and do not handle the high variance caused by events with extremely
high/low rewards to agents but small probabilities, which cause the inaccurate/insufficient estima-
tions of the future returns. Therefore, instead of expected values, learning distributions of future
returns, i.e., Q values, is more useful for agents to make decisions. Recently, QR-MIX (Hu et al.,
2020) decomposes the estimated joint return distribution (Bellemare et al., 2017; Dabney et al.,
2018a) into individual Q values. However, the policies in QR-MIX are still based expected individ-
ual Q values. Even further, given that the environment is nonstationary from the perspective of each
agent, each agent needs a more dynamic way to choose actions based on the return distributions,
rather than simply taking the expected values. However, current MARL methods do not extensively
investigate these aspects.
Motivated by the previous reasons, we intend to extend the risk-sensitive1 2 RL (Chow &
Ghavamzadeh, 2014; Keramati et al., 2020; Zhang et al., 2020) to MARL settings, where risk-
sensitive RL optimizes policies with a risk measure, such as variance, power formula measure value
at risk (VaR) and conditional value at risk (CVaR). Among these risk measures, CVaR has been gain-
ing popularity due to both theoretical and computational advantages (Rockafellar & Uryasev, 2002;
RUszczynski, 2010). However, there are two main obstacles: i) most of the previous works focus on
risk-neutral or static risk level in single-agent settings, ignoring the randomness of reward and the
temporal structure of agents’ trajectories (Dabney et al., 2018a; Tang et al., 2019; Ma et al., 2020;
Keramati et al., 2020); ii) many methods use risk measures over Q values for policy execution with-
out getting the risk measure values used in policy optimization in temporal difference (TD) learning,
which causes the global value factorization on expected individual values to sub-optimal behaviours
in MARL. We provide a detailed review of related works in Appendix A due to the limited space.
In this paper, we propose RMIX, a novel cooperative MARL method with CVaR over the learned
distributions of individuals’ Q values. Specifically, our contributions are in three folds: (i) We first
learn the return distributions of individuals by using Dirac Delta functions in order to analytically
calculate CVaR for decentralized execution. The resulting CVaR values at each time step are used
as policies for each agent via arg max operation; (ii) We then propose a dynamic risk level predictor
for CVaR calculation to handle the temporal nature of stochastic outcomes during executions. The
dynamic risk level predictor measures the discrepancy between the embedding of current individual
return distributions and the embedding of historical return distributions. The dynamic risk levels are
agent-specific and observation-wise; (iii) We finally propose risk-sensitive Bellman equation along
with IGM for centralized training. The risk sensitive Bellman equation enables CVaR value update
in a recursive form and can be trained with TD learning via a neural network. These also allow our
method to achieve temporally extended exploration and enhanced temporal coordination, which are
key to solving complex multi-agent tasks. Empirically, we show that RMIX significantly outper-
forms state-of-the-art methods on many challenging StarCraft IITM2 tasks, demonstrating enhanced
coordination in many symmetric & asymmetric and homogeneous & heterogeneous scenarios and
revealing high sample efficiency. To the best of our knowledge, our work is the first attempt to
investigate cooperative MARL with risk-sensitive policies under the Dec-POMDP framework.
2	Preliminaries
In this section, we provide the notation and the basic notions we will use in the following. We
consider the probability space (Ω, F, Pr), where Ω is the set of outcomes (sample space), F is a
σ-algebra over Ω representing the set of events, and Pr is the set of probability distributions. Given
a set X, we denote with P(X) the set of all probability measures over X.
DEC-POMDP A fully MARL problem can be described as a decentralised partially observable
Markov decision process (Dec-POMDP) (Oliehoek et al., 2016) which can be formulated as a tuple
M = hS, U, P, R, Υ, O, N, γi, where s ∈ S denotes the true state of the environment. Each agent
i ∈ N := {1, ..., N} chooses an action ui ∈ U at each time step, giving rise to a joint action vector,
u := [ui]iN=1 ∈ UN. P(s0|s, u) : S × UN × S 7→ P(S) is a Markovian transition function and
governs all state transition dynamics. Every agent shares the same joint reward function R(s, u) :
S × UN 7→ R, and γ ∈ [0, 1) is the discount factor. Due to partial observability, each agent has
individual partial observation υ ∈ Υ, according to some observation function O(s, i) : S ×N 7→ Υ.
1“Risk” refers to the uncertainty of future outcomes (Dabney et al., 2018a).
2StarCraft II is a trademark of Blizzard Entertainment, Inc.
2
Under review as a conference paper at ICLR 2021
Each agent also has an action-observation history Ti ∈ T := (Y XU )*, on which it conditions its
stochastic policy ∏i(u∕τi) : T ×U → [0,1].
CVaR CVaR is a coherent risk measure and enjoys computational properties (Rockafellar & Urya-
sev, 2002) that are derived for loss distributions in discreet decision-making in finance. It gains
popularity in various engineering and finance applications. CVaR (as illustrated in Figure 1) is the
expectation of values that are less equal than the α-percentile value of the distribution over returns.
Formally, let X ∈ X be a bounded random variable with
cumulative distribution function F (x) = P [X ≤ x] and
the inverse CDF is F-1 (u) = inf {x : F(x) ≥ u}. The
conditional value at risk (CVaR) at level α ∈ (0, 1] of
a random variable X is then defined as CVaRα (X) :=
SuPV {ν — 1 E[(ν - X)+]} (Rockafellar et al., 2000)
when X is a discrete random variable. Correspond-
ingly, CVaRa(X) = EX〜F [X|X ≤ F-1(α)] (Acerbi
& Tasche, 2002) when X has a continuous distribution.
Figure 1: CVaR
The α-percentile value is value at risk (VaR). For ease
of notation, we write CVaR as a function of the CDF F,
CVaRα(F).
Risk-sensitive RL Risk-sensitive RL uses risk criteria over policy/value, which is a sub-field of the
Safety RL (GarCia et al., 2015). Von Neumann & Morgenstern (1947) proposed the expected utility
theory where a decision policy behaves as though itis maximizing the expected value of some utility
functions. The theory is satisfied when the decision policy is a consistent and has a particular set
of four axioms. This is the most pervasive notion of risk-sensitivity. A policy maximizing a linear
utility function is called risk-neutral, whereas concave or convex utility functions give rise to risk-
averse or risk-seeking policies, respectively. Many measures are used in RL such as CVaR (Chow
et al., 2015; Dabney et al., 2018a) and power formula (Dabney et al., 2018a). However, few works
have been done in MARL and it cannot be easily extended. Our work fills this gap.
CTDE CTDE has recently attracted attention from deep MARL to deal with nonstationarity while
learning decentralized policies. One of the promising ways to exploit the CTDE paradigm is value
function decomposition (Sunehag et al., 2017; Rashid et al., 2018; Son et al., 2019) which learns a
decentralized utility function for each agent and uses a mixing network to combine these local Q
values into a global action-value. It follows the IGM principle where the optimal joint actions across
agents are equivalent to the collection of individual optimal actions of each agent (Son et al., 2019).
To achieve learning scalability, existing CTDE methods typically learn a shared local value or policy
network for agents.
3	Methodology
In this section, we present our framework RMIX, as displayed in Figure 2, where the agent network
learns the return distribution of each agent, a risk operator network determines the risk level of each
agent and the mixing network mixes the outputs of risk operators of agents to produce the global
value. In the rest of this section, we first introduce the CVaR operator to analytically calculate the
CVaR value with the modeled individual distribution of each agent in Section 3.1 and propose the
dynamic risk level predictor to alleviate time-consistency issue in Section 3.2. Then, we introduce
the risk-sensitive Bellman equation for both centralized training and decentralized execution in Sec-
tion 3.3. Finally, we provide the details of centralized training of RMIX in Section 3.4. All proofs
are provided in Appendix B.
3.1	CVaR of Return Distribution
In this section, we describe how we estimate the CVaR value. The value of CVaR can be either
estimated through sampling or computed from the paremetrized return distribution (Rockafellar
& Uryasev, 2002). However, the sampling method is usually computationally expensive (Tang
et al., 2019). Therefore, we let each agent learn a return distribution parameterized by a mixture
3
Under review as a conference paper at ICLR 2021
r
T PrediCtor 叫
J)
C(,∙,∙)
■
Ctot(τ, u, d) — TD Loss
Mixing Network
t
Ci(∙,∙,∙)	…	(
MLP
RiSk Operator n
RiSk Operator 1
Zl(G,∙)
Z.(T")
Agent n
Agent 1
LMLP
(其,"I1)
(。:,呜T)
(b)
{Q (∙,∙,∙)}，才
(Ot,红])
(a)
Zi(T)玷:t—i
Γ7J∖
Ctot(τ, u, a)
——St
(C)

W2 --■

■
O


√
Figure 2: The framework of RMIX (dotted arrow indicates that gradients are blocked during train-
ing). (a) Agent network structure (bottom) and risk operator (top). (b) The overall architecture. (c)
Mixing network structure. Each agent i applies an individual risk operator Παi on its return distri-
bution Zi(∙, ∙) to calculate Ci(∙, ∙, ∙) for execution given risk level ai predicted by the dynamic risk
level predictor ψi. {Ci(∙, ∙, ∙)}n=-1 are fed into the mixing network for centralized training.
of Dirac Delta (δ) functions 3, which is demonstrated to be highly expressive and computationally
efficient (Bellemare et al., 2017). For convenience, we provide the definition of the Generalized
Return Probability Density Function (PDF).
Definition 1. (Generalized Return PDF). For a discrete random variable R ∈ [-Rmax, Rmax]
and probability mass function (PMF) P(R = rk), where rk ∈ [-Rmax, Rmax], we define the
generalized return PDF as: fR(r) = r ∈R PR(rk)δ(r - rk). Note that for any rk ∈ R, the
probability of R = rk is given by the coefficient of the corresponding δ function, δ(r - rk).
We define the return distribution of each agent i at time step t as:
Zit(τi,uit-1) =Xm Pj(τi,uit-1)δj(τi,uit-1)	(1)
j=1
where m is the number of Dirac Delta functions. δj(τi, uti-1) is the j-th Dirac Delta function and in-
dicates the estimated value which can be parameterized by neural networks in practice. Pj(τi, uit-1)
is the corresponding probability of the estimated value given local observations and actions. τi and
uit-1 are trajectories (up to that timestep) and actions of agent i, respectively. With the individual
return distribution Zit(τi, uit-1) ∈ Z and cumulative distribution function (CDF) FZ (τ ,ut-1), we
define the CVaR operator Παi, at a risk level αi (αi ∈ (0, 1] and i ∈ A) over return as4
Cit(τi,uit-1,αi) =ΠαtiZit(τi,uit-1) = CVaRαit(FZt(τi,ut-1)),	(2)
where C ∈ C. As we use CVaR on return distributions, it corresponds to risk-neutrality (expectation,
αi = 1) and indicates the improving degree of risk-aversion (αi → 0). CVaRαi can be estimated
in a nonparametric way given ordering of Dirac Delta functions {δj}jm=1 (Kolla et al., 2019) by
leveraging the individual distribution:
m
CVaRai= Ej=I Pj j {δj ≤ Vm,αJ,	⑶
where 1{∙} is the indicator function and Gmai is estimated value at risk from Vm,ai = ∖βm(i-σi)C
with [∙C being floor function. This is a closed-form formulation and can be easily implemented in
practice. The optimal action of agent i can be calculated via arg maxui Ci(τi, uit-1, αi). We will
introduce the decentralized execution in detail in Section 3.3.
3The Dirac Delta is a Generalized function in the theory of distributions and not a function given the prop-
erties of it, we use the name Dirac Delta function by convention.
4We will omit t in the rest of the paper for notation brevity.
4
Under review as a conference paper at ICLR 2021
3.2	Dynamic Risk Level Prediction
The values of risk levels, i.e., αi, i ∈ A, are important for the agents to make decisions. Most
of the previous works take a fixed value of risk level and do not take into account any temporal
structure of agents’ trajectories, which can impede centralized training in the evolving multi-agent
environments. Therefore, we propose the dynamic risk level prediction, which determines the risk
levels of agents by explicitly taking into account the temporal nature of the stochastic outcomes, to
alleviate time-consistency issue (Ruszczynski, 2010; Iancu et al., 2015) and stabilize the centralized
training. Specifically, we represent the risk operator Πα by a deep neural network, which calculates
the CVaR value with predicted dynamic risk level α over the return distribution.
Figure 3: Agent architecture.
K Dirac functions
■ llllll > ∕embi
7⅛'distrV'ons *!■■■■■!
Zi “Jl・l
Φi
The dimension is K
SoftmaX
=>巴玲
0
0
0
1
1
1
1
X Zi
Figure 4: Risk level predictor ψi .
×
We show the architecture of agent i in Figure 3 and illustrate how ψi works with agent i for CVaR
calculation in practice in Figure 4. As depicted in Figure 4, at time step t, the agent’s return dis-
tribution is Zi and its historical return distribution is Zi. Then We conducts the inner product to
measure the discrepancy between the embedding of individual return distribution femb (Zi) and the
embedding of past trajectory φi (τi0't-1 ∕ut-1) modeled by GRU (Chung et al., 2014). We discretize
the risk level range into K even ranges for the purpose of computing. The k-th dynamic risk level
αik is output from ψi and the probability of αik is defined as:
次(αk ) =	exp (Uemb(Zi)k, φk〉)
—EK-0 eXP (femb (Zi)k0 ,Ok'〉)
(4)
Then We get the k ∈ [1, . . . , K] With the maximal probability by arg maX and normalize it into
(0, 1], thus αi= k/K. The prediction risk level αi is a scalar value and it is converted into to a
K-dimensional mask vector Where the first bαi × Kc items are one and the rest items are zero. This
mask vector is used to calculate the CVaR value (Eqn. 2 and 3) of each action-return distribution
that contains K Dirac functions. Finally, We obtain Ci and the policy πi as illustrated in Figure 3.
During training, fembi updates its Weights and the gradients of fembi are blocked (the dotted arroW
in Figure 3) in order to prevent changing the Weights of the netWork of agent i.
We note that the predictor differs from the attention netWork used in previous Works (Iqbal & Sha,
2019; Yang et al., 2020) because the agent’s current return distribution and its return distribution
of previous time step are separate inputs of their embeddings and there is no key, query and value
Weight matrices. The dynamic risk level predictors alloW agents to determine the risk level dynami-
cally based on historical return distributions.
3.3	Risk-sensitive Bellman Equation
Motivated by the success of optimizing the CVaR value in single-agent RL (ChoW & Ghavamzadeh,
2014), RMIX aims to maximize the CVaR value of the joint return distribution, rather than the ex-
pectation (Rashid et al., 2018; Son et al., 2019). As proved in Theorem 1, the maximizing operation
of CVaR values satisfies the IGM principle, Which implies that maximizing the CVaR value of joint
return distribution is equivalent to maximizing the CVaR value of each agent.
Theorem 1. In decentralized execution, given α=	{αi}in=-01, we define the global arg maX per-
formed on global CVaR Ctot(τ, u, α) as:
arg max Ctot(τ, u, α) = I arg max C1(τ1,u1,a1),… ,arg max Cn(Tn, un, ɑn) I (5)
u	u1	un
5
Under review as a conference paper at ICLR 2021
where τ and u are trajectories (up to that timestep) and actions of all agents, respectively. The in-
dividuals’ maximization operation over return distributions defined above satisfies IGM and allows
each agent to participate in a decentralised execution solely by choosing greedy actions with respect
to its Ci (τi , ui , αi).
To maximize the CVaR value of each agent, we define the risk-sensitive Bellman operator T:
TCtot(s,u,α)
E hR(s, u) + γmaxCtot(s0, u0, α0)i
(6)
where α and α0 are agents’ static risk levels or dynamic risk levels output from the dynamic risk
level predictor ψ at each time step. The risk-sensitive Bellman operator T operates on the CVaR
value of the agent and the reward, which can be proved to be a contracting operation, as showed in
Proposition 1. Therefore, we can leverage the TD learning (Sutton & Barto, 2018) to compute the
maximal CVaR value of each agent, thus leading to the maximal global CVaR value.
Proposition 1. T : C 7→ C is a γ-contraction.
3.4	Centralized Training
We introduce the centralized training of RMIX. We utilize the monotonic mixing network of QMIX,
which is a value decomposition network via hypernetwork (Ha et al., 2017) to maintain the mono-
tonicity and has shown success in cooperative MARL. Based on IGM (Son et al., 2019) principle,
we define monotonic IGM between Ctot and Ci for RMIX:
∂Ctot
~7^≥ 0, ∀i ∈{1, 2,...,N},	(7)
∂Ci
where Ctot is the total CVaR value and Ci (τi , ui) is the individual CVaR value of agent i, which
can be considered as a latent combination of agents’ implicit CVaR values to the global CVaR value.
Following the CTDE paradigm, we define the TD loss of RMIX as:
L∏(θ) ：= Ed，” [(ytot — Ctot (st, ut, αt))2]	(8)
where ytot = (r + Ymax/COt (st+ι, u0, α0)), and (ytot - C]ot (st,ut, a。) is our CVaR TD
error for updating CVaR values. θ is the parameters of Ctot which can be modeled by a deep neural
network and θ indicates the parameters of the target network which is periodically copied from θ
for stabilizing training (Mnih et al., 2015). While training, gradients from Zi are blocked to avoid
changing the weights of the agents’ network from the dynamic risk level predictor. We train RMIX
in an end-to-end manner. ψi is trained together the agent network via the loss defined in Eq. 8.
During training, fembi updates its weights while gradients of fembi are blocked in order to prevent
changing the weights of the return distribution in agent i. The pseudo code of RMIX is shown in
Algorithm 1 in Appendix D. We present our framework as shown in Figure 2. Our framework is
flexible and can be easily used in many cooperative MARL methods.
4 Experiments
We empirically evaluate our methods on various StarCraft II scenarios. Especially, we are inter-
ested in the robust cooperation in complex asymmetric and homogeneous/heterogeneous scenarios.
Additional introduction of baselines, scenarios and results are in Appendix C, E, F and G.
4.1 Experiment Setup
StarCraft II We consider SMAC (Samvelyan et al., 2019) benchmark4 5 (screenshots of some sce-
narios are in Figure 5), a challenging set of cooperative StarCraft II maps for micromanagement,
as our evaluation environments. We evaluate our methods for every 10,000 training steps during
training by running 32 episodes in which agents trained with our methods battle with built-in game
bots. We report the mean test won rate (test_battle_won_mean, percentage of episodes won
of MARL agents) along with one standard deviation of won rate (shaded in figures). Due to limited
5https://github.com/oxwhirl/smac
6
Under review as a conference paper at ICLR 2021
Figure 5: SMAC scenarios: 27m_vs_30m, 5m_vs_6m, 6h_vs_8z, corridor and MMM2.
(a) 27m_vs_30m	(b) 5m_vs_6m (c) 6h_vs_8z
(d) corridor	(e) MMM2
Ue ① UJlUoMl ①-七eq Js ① 4
1.00-
RMIX
0.75
0.50
0.25
0.00
：Il「T……I
I	I	I	I	I	I	I	I	I	I	I	I	I	I	I	I	I
Figure 6:	test_battle_won_mean summary of RMIX and QMIX on 17 SMAC scenarios.
page space, we present the results of our methods and baselines on 8 scenarios (we train our methods
and baselines on 17 SMAC scenarios): corridor, 3s5z_vs_3s6z, 6h_vs_8z, 5m_vs_6m, 8m_vs_9m,
10m_vs_11m, 27m_vs_30m and MMM2. Table 1 shows detailed information on these scenarios.
Baselines and training The baselines are IQL (Tampuu et al., 2017), VDN (Sunehag et al.,
2017), COMA (Foerster et al., 2017a), QMIX (Rashid et al., 2018), QTRAN (Son et al., 2019),
MAVEN (Mahajan et al., 2019) and Qatten (Yang et al., 2020). We implement our methods on
PyMARL6 and use 5 random seeds for training each method on 17 SMAC scenarios. We carry out
experiments on NVIDIA Tesla V100 GPU 16G.
4.2 Experiment Results
RMIX demonstrates substantial superiority
over baselines in asymmetric and homogeneous
scenarios as depicted in Figure 7. RMIX
outperforms baselines in asymmetric homo-
geneous scenarios: 5m_vs_6m, 8m_vs_9m,
10m_vs_11m and 27m_vs_30m (hard game).
In 3s5z_vs_3s6z (asymmetric heterogeneous,
very hard game) and MMM2 (symmetric het-
erogeneous, hard game), RMIX also shows
leading performance over baselines. RMIX
learns micro-trick (wall off) and micro-trick
(focus fire) faster and better in very hard cor-
ridor and 6h_vs_8z. RMIX improves coordina-
tion in a sample efficient way via risk-sensitive
policies. We summarize the performance of
1.0
0.8
0.6
0.4
0.2
0.0
0.0	1.0M	2.0M	3.0M	4.0M	5.0M	6.0M	7.0M	8.0M
SlRMlX
MAVEN
"-Qatten 一
—QMIX
Figure 8: test_battle_won_mean of
RMIX, QMIX, Qatten and MAVEN on very
hard corridor scenario.
RMIX and QMIX on 17 SMAC scenarios in Figure 6. Readers can refer to Figure 19 and 20
for more results. We present results of RMIX in Figure 13 and 14 on 3s5z_vs_3s6z and 6h_vs_8z
in 8 million training steps. Although training 27m_vs_30m is memory-consuming, we also present
results of 2.5 million training steps, as depicted in Figure 15.
Interestingly, as illustrated in Figure 8, RMIX also demonstrates leading exploration performance
over baselines on very hard corridor (in Figure 5(d)) scenario, where there is a narrow corridor con-
6https://github.com/oxwhirl/pymarl
7
Under review as a conference paper at ICLR 2021
(a) corridor
(b) 3s5z_vs_3s6z
(c) MMM2
LO H——
(e) 5m_vs_6m
0	IM	2M
1.0
0.8
0.6
0.4
0.2
0.0
(f) 8m_vs_9m	(g) 10m_vs_11m	(h) 27m_vs_30m
Figure 7:	test_battle_won_mean of RMIX and baselines on 8 scenarios, the x-axis denotes
the training steps and the y-axis is the test battle won rate, ranging from 0 to 1. It applies for result
figures in the rest of the paper, including figures in appendix.
necting two separate rooms, and agents should learn to cooperatively combat the opponents to avoid
being beaten by opponents with the divide-and-conquer strategy. RMIX outperforms MAVEN (Ma-
hajan et al., 2019), which was originally proposed for tackling multi-agent exploration problems,
both in sample efficiency and final results. After 4 million training steps, RMIX starts to converge
while MAVEN starts to converge after over 7 million training steps.
(a) 5m_vs_6m
(b) corridor
(d) 3s5z_vs_3s6z
Figure 9:	test_battle_won_mean of RMIX vs QR-MIX and QMIX.
In addition, we compare RMIX with QR-MIX (Hu et al., 2020). We implement QR-MIX with
PyMARL and train it on 5m_vs_6m (hard), corridor (very hard), 6h_vs_8z (very hard) and
3s5z_vs_3s6z (very hard) with 3 random seeds for each scenario. Hyper-parameters used during
training are from QR-MIX paper. As shown in Figure 9, RMIX shows leading performance and
superior sample efficiency over QR-MIX on 5m_vs_6m , 27m_vs_30m and 6h_vs_8z. With dis-
tributional RL, QR-MIX presents slightly better performance on 3s5z_vs_3s6z. We present more
results of RMIX vs QR-MIX in Appendix G.3.
(a) 27m_vs_30m
(b) 3s5z_vs_3s6z
(c) 6h_vs_8z
(d) 8m_vs_9m
Figure 10:	test_battle_won_mean of RMIX and baselines on 4 scenarios.
We conduct an ablation study by fixing the risk level with the value of 1, thus we get the risk-neutral
method, which we name as RMIX (α = 1). We present results of RMIX, RMIX (α = 1) and QMIX
8
Under review as a conference paper at ICLR 2021
on 4 scenarios in Figure 10. RMIX outperforms RMIX (α = 1) in many heterogeneous and asym-
metric scenarios, demonstrating the benefits of learning risk-sensitive MARL policies in complex
scenarios where the potential of loss should be taken into consideration in coordination. Intuitively,
for asymmetric scenarios, agents can be easily defeated by the opponents. As a consequence, co-
ordination between agents is cautious in order to win the game, and the cooperative strategies in
these scenarios should avoid massive casualties in the starting stage of the game. Apparently, our
risk-sensitive policy representation works better than vanilla expected Q values in evaluation. In
heterogeneous scenarios, action space and observation space are different among different types of
agents, and methods with vanilla expected action value are inferior to RMIX.
To show that our proposed method is flex-
ible in other mixing network for the ab-
lation study, we apply additivity of in-
dividual CVaR values to represent the
global CVaR value as Ctot (τ , u, α) =
Cl(τι,Uι,αι) + …+ Cn(Tn,Un,αn).
Following the training of RMIX, we name
this method Risk Decomposition Network
(RDN) for ablation study. we use ex-
periment setup of VDN and train RDN
on 5 SMAC scenarios. With CVaR val-
ues, RDN outperforms VDN and QMIX
in 1c3s5z, 5m_vs_6m, 8m_vs_9m and
MMM2 with convincing improvements,
as depicted in Figure 11. In some scenar-
—RMIX
RDN
—QMIX
VDN
5m_vs_6m
Figure 11: test_battle_won_mean of RMIX,
RDN, QMIX and VDN on 5 SMAC scenarios.
ios, for example 1c3z5z and 8m_vs_9m, the converged performance is even equal to that of RMIX,
which demonstrate that RMIX is flexible in additivity mixing networks. Overall, with the new pol-
icy representation and additivity decomposition network, we can gain convincing improvements of
RDN over VDN.
We present how the risk level α of each agent changes during the episode and emergent cooperative
strategies between agents in the results analysis of RMIX in Appendix G.2 due to limited space.
5 Conclusion
In this paper, we propose RMIX, a novel risk-sensitive MARL method with CVaR over the learned
distributions of individuals’ Q values. Our main contributions are in three folds: (i) We first learn the
return distributions of individuals to analytically calculate CVaR for decentralized execution; (ii) We
then propose a dynamic risk level predictor for CVaR calculation to handle the temporal nature of
the stochastic outcomes during executions; (iii) We finally propose risk-sensitive Bellman equation
along with Individual-Global-MAX (IGM) for MARL training. Empirically, we show that RMIX
significantly outperforms state-of-the-art methods on many challenging StarCraft II tasks, demon-
strating enhanced coordination in many complex scenarios and revealing high sample efficiency.
To the best of our knowledge, our work is the first attempt to investigate cooperative MARL with
risk-sensitive policies under the Dec-POMDP framework.
9
Under review as a conference paper at ICLR 2021
References
Carlo Acerbi and Dirk Tasche. On the coherence of expected shortfall. Journal of Banking &
Finance, 26(7):1487-1503, 2002.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449-458, 2017.
Yinlam Chow and Mohammad Ghavamzadeh. Algorithms for cvar optimization in MDPs. In Ad-
vances in Neural Information Processing Systems, pp. 3509-3517, 2014.
Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision-
making: a cvar optimization approach. In Advances in Neural Information Processing Systems,
pp. 1522-1530, 2015.
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. In Advances in Neural Information
Processing Systems 2014 Workshop on Deep Learning, 2014.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In International Conference on Machine Learning, pp.
1096-1105, 2018a.
Will Dabney, Mark Rowland, Marc G Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelli-
gence, pp. 2892-2901, 2018b.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. arXiv preprint arXiv:1705.08926, 2017a.
Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip HS Torr, Pushmeet
Kohli, and Shimon Whiteson. Stabilising experience replay for deep multi-agent reinforcement
learning. In International Conference on Machine Learning, pp. 1146-1155, 2017b.
Javier Garcia et al. A comprehensive survey on safe reinforcement learning. Journal of Machine
Learning Research, 16(42):1437-1480, 2015.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. In International Conference on Learning
Representations. OpenReview.net, 2017.
He He, Jordan Boyd-Graber, Kevin Kwok, and Hal Daume III. Opponent modeling in deep rein-
forcement learning. In International Conference on Machine Learning, pp. 1804-1813, 2016.
Takuya Hiraoka, Takahisa Imagawa, Tatsuya Mori, Takashi Onishi, and Yoshimasa Tsuruoka.
Learning robust options by conditional value at risk optimization. In Advances in Neural In-
formation Processing Systems, pp. 2619-2629, 2019.
Jian Hu, Seth Austin Harding, Haibin Wu, and Shih-wei Liao. Qr-mix: Distributional value
function factorisation for cooperative multi-agent reinforcement learning. arXiv preprint
arXiv:2009.04197, 2020.
Maximilian Huttenrauch, Adrian Sosic, and Gerhard Neumann. Guided deep reinforcement learn-
ing for swarm systems. In AAMAS 2017 Autonomous Robots and Multirobot Systems (ARMS)
Workshop, 2017.
Dan A Iancu, Marek Petrik, and Dharmashankar Subramanian. Tight approximations of dynamic
risk measures. Mathematics of Operations Research, 40(3):655-682, 2015.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 2961-2970, 2019.
Jiechuan Jiang and Zongqing Lu. Learning attentional communication for multi-agent cooperation.
In Advances in Neural Information Processing Systems, pp. 7254-7264, 2018.
10
Under review as a conference paper at ICLR 2021
Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be
conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on Artificial
Intelligence,pp. 4436-4443, 2020.
Ravi Kumar Kolla, LA Prashanth, Sanjay P Bhat, and Krishna Jagannathan. Concentration bounds
for empirical conditional value-at-risk: The unbounded case. Operations Research Letters, 47(1):
16-20, 2019.
Landon Kraemer and Bikramjit Banerjee. Multi-agent reinforcement learning as a rehearsal for
decentralized planning. Neurocomputing, 190:82-94, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations, 2016.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
Xueguang Lyu and Christopher Amato. Likelihood quantile networks for coordinating multi-agent
reinforcement learning. In Proceedings of the 19th International Conference on Autonomous
Agents and MultiAgent Systems, pp. 798-806, 2020.
Xiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Distribu-
tional soft actor critic for risk sensitive learning. arXiv preprint arXiv:2004.14547, 2020.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. MAVEN: Multi-agent
variational exploration. In Advances in Neural Information Processing Systems, pp. 7613-7624,
2019.
Anirudha Majumdar and Marco Pavone. How should a robot assess risk? Towards an axiomatic
theory of risk in robotics. In Robotics Research, pp. 75-84. Springer, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Frans A Oliehoek, Matthijs TJ Spaan, and Nikos Vlassis. Optimal and approximate q-value func-
tions for decentralized POMDPs. Journal of Artificial Intelligence Research, 32:289-353, 2008.
Frans A Oliehoek, Christopher Amato, et al. A Concise Introduction to Decentralized POMDPs,
volume 1. Springer, 2016.
Nicolas Privault. Notes on Financial Risk and Analytics. Course notes, 268 pages, 2020. Accessed:
2020-09-27.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. QMIX: Monotonic value function factorisation for deep multi-agent
reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
D Sai Koti Reddy, Amrita Saha, Srikanth G Tamilselvam, Priyanka Agrawal, and Pankaj Dayama.
Risk averse reinforcement learning for mixed multi-agent environments. In Proceedings of the
18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2171-2173,
2019.
R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions.
Journal of Banking & Finance, 26(7):1443-1471, 2002.
R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal
of Risk, 2:21-42, 2000.
Andrzej RUszczynski. Risk-averse dynamic programming for markov decision processes. Mathe-
matical Programming, 125(2):235-261, 2010.
11
Under review as a conference paper at ICLR 2021
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354-359, 2017.
Arambam James Singh, Akshat Kumar, and Hoong Chuin Lau. Hierarchical multiagent reinforce-
ment learning for maritime traffic management. In Proceedings of the 19th International Confer-
ence on Autonomous Agents and MultiAgent Systems, pp. 1278-1286, 2020.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 5887-5896, 2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning. arXiv preprint arXiv:1706.05296, 2017.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.
Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for co-
herent risk measures. In Advances in Neural Information Processing Systems, pp. 1468-1476,
2015.
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
PLoS ONE, 12(4), 2017.
Yichuan Charlie Tang, Jian Zhang, and Ruslan Salakhutdinov. Worst cases policy gradients. CoRL,
2019.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,
Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, et al.
Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
John Von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior, 2nd rev.
Princeton university press, 1947.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. arXiv preprint arXiv:2008.01062, 2020a.
Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi Rabinovich. Learning efficient
multi-agent communication: An information bottleneck approach. International Conference on
Machine Learning, 2020b.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao
Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv
preprint arXiv:2002.03939, 2020.
Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. Cautious reinforcement learning
via distributional risk in the dual domain. arXiv preprint arXiv:2002.12475, 2020.
12
Under review as a conference paper at ICLR 2021
A Related Works
As deep reinforcement learning (DRL) becomes prevailing (Mnih et al., 2015; Schulman et al.,
2017), recent years have witnessed a renaissance in cooperative MARL with deep learning. How-
ever, there are several inevitable issues, including the nonstationarity of the environment from the
view of individual agents (Foerster et al., 2017b), the credit assignment in cooperative scenarios
with shared global rewards (Sunehag et al., 2017; Rashid et al., 2018), the lack of coordination and
communication in cooperative scenarios (Jiang & Lu, 2018; Wang et al., 2020b) and the failure to
consider opponents’ strategies when learning agent policies (He et al., 2016). Aiming to address
these issues, centralized training with decentralized execution (CTDE) (Oliehoek et al., 2008; Krae-
mer & Banerjee, 2016) has drawn enormous attention via training policies of each agent with access
to global trajectories in a centralized way and executing actions given only the local observations of
each agent in a decentralized way. Several MARL methods are proposed (Lowe et al., 2017; Foer-
ster et al., 2017a; Sunehag et al., 2017; Rashid et al., 2018; Son et al., 2019; Yang et al., 2020; Wang
et al., 2020a), including value-based and policy gradient. Among these methods, VDN (Sunehag
et al., 2017) and QMIX (Rashid et al., 2018) are representative methods that use value decomposi-
tion of the joint action-value function by adopting additivity and monotonicity structural constraints.
Free from such structural constraints, QTRAN (Son et al., 2019) guarantees more general factoriza-
tion than VDN and QMIX, however, such linear affine transformation fails to scale up in complex
multi-agent scenarios, for example, StarCraft II environments.
However, most of works focus on decomposing the global Q value into individual Q values with dif-
ferent formulas and network architectures, either with structural constraints, for example additivity
and monotonicity (Sunehag et al., 2017; Rashid et al., 2018), or with estimating values that forms a
new global Q value estimation, for example representing Q values via summation of state-value and
advantage or multi-head attention networks (Wang et al., 2020a; Yang et al., 2020). Current MARL
methods neglect the limited representation of agent values, which fails to consider the agent-level
impact of individuals to the whole system when transforming individual utilities to global values,
leading to hostile training in complex scenarios. Typically, the problem of random cost underly-
ing the nonstationarity of the environment, a.k.a risk-sensitive learning, which is very important
for many real-world applications, has rarely been investigated. Current work are either confined in
simple settings (Reddy et al., 2019) or have no general framework and convincing results in com-
plex domains (Lyu & Amato, 2020). Further research should be done in risk-sensitive cooperative
MARL. We propose RMIX and fill the gap in this field.
Recent advances in distributional RL (Bellemare et al., 2017; Dabney et al., 2018a;b) focuses on
learning distribution over returns. However, with return distributions, these works still focus on
either risk-neutral settings or with static risk level in single-agent setting, which neglects the ubiq-
uitous significant risk-sensitive problems in many multi-agent real-world applications, including
pipeline robots cooperation in factories, warehouse robots coordination, etc. This is very common
in real-world applications, especially in highly dynamic tasks for example military action, resource
allocation, finance portfolio management and Internet of Things (IoT), etc.
Chow & Ghavamzadeh (2014) proposed considered the mean-CVaR optimization problem in MDPs
and proposed policy gradient with CVaR, and Garcia et al. (2015) presented a survey on safe RL,
which have ignited the research on borrowing risk measures in RL (Garcia et al., 2015; Tamar et al.,
2015; Tang et al., 2019; Hiraoka et al., 2019; Majumdar & Pavone, 2020; Keramati et al., 2020; Ma
et al., 2020). However, these works focus on single-agent settings, where there is one agent inter-
acting with the environment compared with dynamic and non-stationary multi-agent environments.
The merit of CVaR in optimization of MARL has yet to be discovered. We explore the CVaR risk
measure in our methods and demonstrate the leading performance of our MARL methods.
B Proofs
We present proofs on our propositions introduced in previous sections. The proposition and equa-
tions numbers are reused in restated propositions.
Assumption 1. The mean rewards are bounded in a known interval, i.e., r ∈ [-Rmax, Rmax].
13
Under review as a conference paper at ICLR 2021
This assumption means we can bound the absolute value of the Q-values as |Qsa | ≤ Qmax
HRmax , where H is the maximum time horizon length in episodic tasks.
Proposition 1. T : C 7→ C is a γ-contraction.
Proof. We consider the sup-norm contraction,
TC(1)(s,u,α(1)) - TC(2)(s,u,α(2)) ≤ γ C(1)(s,u,α(1)) - C(2)(s,u,α(2))∞	(9)
∀s ∈ S, u ∈ U , α(i),i∈{1,2} ∈ A.
The sup-norm is defined as kCk∞ = sups∈S,u∈U,α∈A |C(s,u)| andC ∈ R.
In {Ci}in=-01, the risk level is fixed and can be considered implicit input. Given two risk level set
α(1) and α(2), and two different return distributions Z(1) and Z(2), we prove:
T C(1) - T C(2)
≤ max T C(1) (s, u, α(1)) - TC(2) (s, u, α(1))
s,u
= max γ X P (s0 |s, u) max C(1) (s0, u0, α0) - max C(2) (s0, u0, α0)
≤ γmax max C(1) (s0, u0, α0) - max C(2) (s0, u0, α0)
s0	u0	u0
≤ γ max C(1) (s0, u0, α0) - C(2) (s0, u0, α0)
s ,u
= γ C(1) - C(2)∞
This further implies that
T C(1) - T C(2) ≤ γ C(1) - C(2)∞	∀s ∈S, u ∈ U, α(i),i∈{1,2} ∈ A.
(10)
(11)
□
With proposition 1, we can leverage the TD learning (Sutton & Barto, 2018) to compute the maximal
CVaR value of each agent, thus leading to the maximal global CVaR value. In some scenarios, where
risk is not the primal concern for policy optimization, for example corridor scenario, where agents
should learn to explore to win the game. RMIX will show less sample efficient to learn the optimal
policy compared with its risk neutral variant, RMIX (α = 1). As we can see in Figure 16(b), section
G.1.1, RMIX learns slower than RMIX (α = 1) because it relies on the dynamic risk predictor while
the predictor is trained together with the agent, it will take more samples in these environments.
Interestingly, RMIX shows very good performance over other baselines.
Theorem 1. In decentralized execution, given α = {αi}in=-01, we define the global arg max per-
formed on global CVaR Ctot (τ, u, α) as:
arg max Ctot(τ, u, α) = I arg max Ci (τ1,u1,a1),… ,arg max Cn(Tn,Un,ɑn) [	(5)
u	u1	un
where τ and u are trajectories (up to that timestep) and actions of all agents, respectively. The in-
dividuals’ maximization operation over return distributions defined above satisfies IGM and allows
each agent to participate in a decentralised execution solely by choosing greedy actions with respect
to its Ci (τi, ui, αi).
Proof. With monotonicity network fm , in RMIX, we have
Ctot(τ,u,α) = fm(C1(τ1,u1, α1), . . . ,Cn(τn,un, αn))	(12)
Consequently, we have
Ctot (τ, {argmaxCi(τi,u0, αi)}in=-01) = fm({maxCi(τi, u0, αi)}in=-01)	(13)
u0	u0
By the monotonocity property of fm, We can easily derive that if j ∈ {0,1,...,n - 1}, Uj =
argmaxuo Cj(τj, U, αj), αj ∈ (0,1] is the optimal risk level given the current return distributions
and historical return distributions, and actions of other agents are not the best action, then We have
fm ({Cj (τj, uj, αj)}in=-0 ) ≤ fm ({Cj (τj, uj, αj)}in=-0,i6=j, Cj(τj, ujj, αj)).	(14)
14
Under review as a conference paper at ICLR 2021
So, for all agents, ∀j ∈ {0,1,...,n 一 1}, Uj = arg maxuo Cj (Tj,u0, αj), We have
fm({Cj (Ti, ui, αi )}n=01 ) ≤ fm({Cj (Tj , uj, αj )}n=0^,i=j, Cj (Tj, Uj))
≤ fm({Ci(τi,uij,αi)}in=-01)	(15)
= max fm({Ci(Ti,Ui,αi)}in=-01).
{ui }in=-01
Therefore, We can get
max	fm({Ci(Ti, Ui, αi)}in=-01) = maxCtot(τ,u, α),	(16)
{ui,αi}in=-01	i=0	u,α
Which implies
max Ctot (τ, u, α) = Ctot (τ, {argmaxCi(Ti, U0, αi)}in=-01).	(17)
u	u0,α0
□
Proposition 2. For any agent i, i ∈ {0, . . . , n 一 1}, ∃λ(Ti, Ui) ∈ (0, 1], such that Ci(Ti, Ui) =
λ(Ti, Ui)E [Zi(Ti, Ui)].
Proof. We first provide that given a return distribution Z, return random variable Z and risk level
α ∈ A, ∀z, ΠαZ can be reWritten as E [Z |Z < z] < E [Z ]. This can be easily proved by folloWing
Privault (2020)’s proof. Thus We can get ΠαZ < E [Z], and there exists λ(τi,ui) ∈ (0, 1], Which is a
value of agent,s trajectories, such that ∏aZi(τ ,u)= 1(偌,如)旧[Zi(τi,uj].	□
Proposition 2 implies that We can vieW the CVaR value as truncated values ofQ values that are in the
loWer region of return distribution Zi (Ti, Ui). CVaR can be decomposed into tWo factors: λ(τi,ui)
and E[Zi (Ti, Ui)].
C Additional Background
We introduce additional background on cooperative MARL algorithms, including QMIX, MAVEN
and Qatten, for the convenience of readers Who Want to knoW more on these algorithms.
Q-based cooperative MARL is concerned With estimating an accurate action-value function to select
the actions With maximum expected returns. The optimal Q-function is defined as (Rashid et al.,
2018):
∞
Qθot(s, u) := E X Ytr (st, ut,θ) | st+1 ~ P « | Stout,θ),、
F , , )	ʌj t t t, t, " ut+ι = arg max Q%ot (st+ι, ∙)
=r(s, u,θ) + YE [max Qθot (s0, ∙) | s0 〜P (∙ | s, u,θ)],
Where θ is the parameters of Qtot Which can be model by deep neural netWorks Working as the
Q-function approximator. This Q-netWork can be trained by minimizing the loss function in a
supervised-learning fashion as defined beloW:
L(θ) ：= Edo〜D (rt + YQtot (st+1, arg max Qθot (st+ι, ∙)) -Qθot (st,ut))2
、--------------------------------------------V------------------------}
yttot
where D0 = (st, ut, rt, st+ι) and D is the replay buffer. θ indicates the parameters of the target
netWork Which is periodically copied from θ for stabilizing training (Mnih et al., 2015). The netWork
is trained in a centralized way with all partial observations accessible to all agents.
15
Under review as a conference paper at ICLR 2021
QtOt(T, u)
Mixing Network
Figure 12: The overall setup of QMIX (best viewed in colour), reproduced from the original
paper (Rashid et al., 2018) (a) Mixing network structure. In red are the hypernetworks that produce
the weights and biases for mixing network layers shown in blue. (b) The overall QMIX architecture.
(c) Agent network structure.
QMIX QMIX (Rashid et al., 2018) is a well-known multi-agent Q-learning algorithm in the cen-
tralised training and decentralised execution paradigm, which restricts the joint action Q-values it
can represent to be a monotonic mixing of each agent’s utilities in order to enable decentralisation
and value decomposition:
QmiX := QQtot | Qtot(T, U) = fm (Q1 (τ1
U1) ,...Qn (Tn, Un)) , ∂fm ≥ 0, Qa (T, U)∈ R
∂Qa
and the arg max operator is used to get the Qtot for centralized training via TD loss similar to
DQN (Mnih et al., 2015)
arg maxu1 Q1(τ1,U1)
arg max Qtot (T, U)=	:
.
arg maxun Qn(τn, Un)
The architecture is shown in Figure 12: The monotonic mixing network fm is parametrised as a
feedforward network, of which non-negative weights are generated by hypernetworks (Ha et al:,
2017) with the state as input:
MAVEN MAVEN (Mahajan et al:, 2019) (multi-agent variational exploration) overcomes the
detrimental effects of QMIX’s monotonicity constraint on exploration via learning a diverse ensem-
ble of monotonic approximations with the help of a latent space: It consists of value-based agents
that condition their behaviour on the shared latent variable z controlled by a hierarchical policy that
offloads -greedy with committed exploration: Thus, fixing z, each joint action-value function is a
monotonic approximation to the optimal action-value function that is learnt with Q-learning:
Qatten Qatten (Yang et al:, 2020) explicitly considers the agent-level impact of individuals to the
whole system when transforming individual Qi s into Qtot : It theoretically derives a general formula
of Qtot in terms of Qi , based on a multi-head attention formation to approximate Qtot , resulting
in not only a refined representation of Qtot with an agent-level attention mechanism, but also a
tractable maximization algorithm of decentralized policies:
16
Under review as a conference paper at ICLR 2021
D Pseudo Code of RMIX
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
Algorithm 1: RMIX
input: K, Y;
Initialize parameters θ of the network of agent, risk operator and monotonic mixing network;
Initialize parameters θ of the target network of agent, risk operator and monotonic mixing
network;
Initialize replay buffer D;
for e — 0 to MAX_EPISODE do
Start a new episode;
while EPISODE_IS_NOT_TEMINATED do
Get the global state st ;
for agent i J 0 to n 一 1 do
Get observation oit from the environment;
Get action of last step uti-1 from the environment;
Estimate the local return distribution Zit(oit, uit-1);
Predict the risk level αi :
argmaχk {PK-Op(Pf ZZOi蠢 i) }k /K,k ∈ {1,....,K};
Calculate CVaR values Ci (o：, u：-1, αi) = ∏αtZ： ((ott, UiT);
Get the action uti = arg maχut Cit oit, uti-1, αi ;
Concatenate uit, i ∈ [0, .., n 一 1] into ut;
Execute uit into environment;
Receive global reward rt and observe a new state s0 ;
Store (st, {oit}i∈[0,...,n-1], ut, rt, s0) in replay buffer D;
if UPDATE then
Sample a min-batch D0 from replay buffer D;
For each sample in D0, calculate CVaR value Ci by following steps in line 9-13;
Concatenate CVaR values {[C00, . . . , Cn0-1]0, . . . , [C0|D0|-1, . . . , Cn|D-01|-1]|D0|-1};
For each [C0j, . . . , Cnj -1]0,j∈[0,...,|D0|-1], calculate Cjtot in the mixing network;
Calculate the target value ytot = (r： + Y max” Crot);
Calculate the TD loss L∏(θ) := Ed，〜D [(ytot 一 Ctot)2];
Update θ by minimizing the TD loss;
Update θ: θ J θ;
return θ;
17
Under review as a conference paper at ICLR 2021
E	Additional Environment Introduction
SMAC benchmark is a challenging set of cooperative StarCraft II maps for micromanagement devel-
oped by Samvelyan et al. (2019) built on DeepMind’s PySC2 (Vinyals et al., 2017). We introduce
states and observations, action space and rewards of SMAC, and environmental settings of
RMIX below.
States and Observations At each time step, agents receive local observations within their field of
view. This encompasses information about the map within a circular area around each unit with a
radius equal to the sight range. The sight range makes the environment partially observable for each
agent. An agent can only observe other agents if they are both alive and located within its sight range.
Hence, there is no way for agents to distinguish whether their teammates are far away or dead. The
feature vector observed by each agent contains the following attributes for both allied and enemy
units within the sight range: distance, relative x, relative y, health, shield, and unit type. All Protos
units have shields, which serve as a source of protection to offset damage and can regenerate if no
new damage is received. The global state is composed of the joint observations but removing the
restriction of sight range, which could be obtained during training in the simulations. All features,
both in the global state and in individual observations of agents, are normalized by their maximum
values.
Action Space The discrete set of actions which agents are allowed to take consists of
move[direction], attack[enemy id], stop and no-op. Dead agents can only take no-op action while
live agents cannot. Agents can only move with a fixed movement amount 2 in four directions:
north, south, east, or west. To ensure decentralization of the task, agents are restricted to use the
attack[enemy id] action only towards enemies in their shooting range. This additionally constrains
the ability of the units to use the built-in attack-move macro-actions on the enemies that are far
away. The shooting range is set to be6 for all agents. Having a larger sight range than a shooting
range allows agents to make use of the move commands before starting to fire. The unit behavior of
automatically responding to enemy fire without being explicitly ordered is also disabled.
Rewards At each time step, the agents receive a joint reward equal to the total damage dealt on the
enemy units. In addition, agents receive a bonus of 10 points after killing each opponent, and 200
points after killing all opponents for winning the battle. The rewards are scaled so that the maximum
cumulative reward achievable in each scenario is around 20.
Environmental Settings of RMIX The difficulty level of the built-in game AI we use in our experi-
ments is level 7 (very difficult) by default as many previous works did (Rashid et al., 2018; Mahajan
et al., 2019; Yang et al., 2020). The scenarios used in Section 4 are shown in Table 1. We present
the table of all scenarios in SMAC in Table 1 and the corresponding memory usage for training
each scenario in Table 2. The Ally Units are agents trained by MARL methods and Enemy Units are
built-in game bots. For example, 5m_vs_6m indicates that the number of MARL agent is 5 while
the number of the opponent is 6. The agent (unit) type is marine7. This asymmetric setting is hard
for MARL methods.
7A type of unit (agent) in StarCraft II. Readers can refer to https://liquipedia.net/
starcraft2/Marine_(Legacy_of_the_Void) for more information
18
Under review as a conference paper at ICLR 2021
Table 1: SMAC Environments
Name	Ally Units	Enemy Units	Type
3m	3 Marines	3 Marines	homogeneous & symmetric
8m	8 Marines	8 Marines	homogeneous & symmetric
25m	25 Marines	25 MarineS	homogeneous & symmetric
3s5z	3 Stalkers & 5 Zealots	3 Stalkers & 5 Zealots	heterogeneous & symmetric
	1 Medivac,	1 Medivac,	
MMM	2 Marauders &	2 Marauders &	heterogeneous & symmetric
	7 Marines	7 Marines	
5m_vs_6m	5 Marines	6 Marines	homogeneous & asymmetric
8m_vs_9m	8 Marines	9 Marines	homogeneous & asymmetric
10m_vs 11m	10 Marines	11 Marines	homogeneous & asymmetric
27m_vs_30m	27 Marines	30 Marines	homogeneous & asymmetric
3s5z_vs_3s6z	3 Stalkers & 5 Zealots	3 Stalkers & 6 Zealots	heterogeneous & asymmetric
	1 Medivac,	1 Medivac,	
MMM2	2 Marauders &	3 Marauders &	heterogeneous & asymmetric
	7 Marines	8 Marines	
	1 Colossi &	1 Colossi &	
1c3s5z	3 Stalkers &	3 Stalkers &	heterogeneous & symmetric
	5 Zealots	5 Zealots	
2m_vs_1z	2 Marines	1 Zealot	micro-trick: alternating fire
3s_vs_5z	3 Stalkers	5 Zealots	micro-trick: kiting
6h_vs_8z	6 Hydralisks	8 Zealots	micro-trick: focus fire
corridor	6 Zealots	24 Zerglings	micro-trick: wall off
bane_vs_bane	20 Zerglings & 4 Banelings	20 Zerglings & 4 Banelings	micro-trick: positioning
F Additional Training Details
The baselines are list in table 3 as depicted below. To make a fair comparison, we use episode
(single-process environment for training, compared with parallel) runner defined in PyMARL
to run all methods. The evaluation interval is 10, 000 for all methods. We use uniform probability to
estimate Zi(∙, ∙) for each agent. We use the other hyper parameters used for training in the original
papers of all baselines. The metrics are calculated with a moving window size of 15. Experiments
are carried out on NVIDIA Tesla V100 GPU 16G. We also provide memory usage of baselines
(given the current size of the replay buffer) for training each scenario of SCII domain in SMAC.
We use the same neural network architecture of agent used by QMIX (Rashid et al., 2018). The
trajectory embedding network φi is similar to the network of the agent.
19
Under review as a conference paper at ICLR 2021
Table 2: Memory usage (given the current size of the replay buffer) for the training of each method
(exclude COMA, which is an on-policy method without using replay buffer) on scenarios of SCII
domain in SMAC.		
	Scenario Name	Memory Usage (GB)
	3m	27
	2m_vs_1z	2.8
	5m_vs_6m	3
	3s_vs_5z	4
	6h_vs_8z	4.6
	8m	4.8
	8m_vs_9m	4.9
	3s5z	6.4
	10m_vs_11m	7.1
	3s5z_vs_3s6z	7.5
	1c3s5z	8.6
	MMM	8.7
	MMM2	10.8
	corridor	14.4
	25m	27
	27m_vs_30m	39.5
	bane_vs_bane	41
Algorithms
Table 3: Baseline algorithms
Brief Description
Independent Q-learning
Value decomposition network
Counterfactual Actor-critic
Monotonicity Value decomposition
Value decomposition with linear affine transform
MARL with variational method for exploration
Multi-head attention for decomposing the global Q values
IQL (Tampuu et al., 2017)
VDN (Sunehag et al., 2017)
COMA (Foerster et al., 2017a)
QMIX (Rashid et al., 2018)
QTRAN (Son et al., 2019)
MAVEN (Mahajan et al., 2019)
Qatten (Yang et al., 2020)
G Additional Experiments on SMAC
G. 1 Ablations
We use the same hyper-parameters in ablation study unless otherwise specified.
G.1.1 Static Risk Level
We present more results of RMIX and RMIX (α = 1) in Figure 13, 14 and 15. In vary hard
3s5z_vs_3s6z, 6h_vs_8z games, RMIX and RMIX (α = 1) outperforms baselines. Surprisingly,
RMIX is even slightly better in 6h_vs_8z where micro-trick (focus fire) is learned. In conclusion,
RMIX is also capable of learning hard micro-trick tasks in StarCraft II. In asymmetric scenario
27m_vs_30m, RMIX shows leading performance as well.
We also conduct an ablation study with static risk level of α = 0.1 and α = 0.3 in RMIX-static.
Obviously, as shown in Figure 16, with static risk level, RMIX-static shows steadily progress over
time, but its performance is lower than RMIX in 6h_vs_8z and 5m_vs_6m. On asymmetric scenario
5m_vs_6m, the converged performance of RMIX-static is 0.6, which is lower than RMIX’s (0.8).
In some scenarios, where risk is not the primal concern for policy optimization, for example corridor
scenario, where agents should learn to explore to win the game. RMIX shows less sample efficient
to learn the optimal policy compared with its risk neutral variant, RMIX (α = 1) in corridor as
shown in Figure 16(b). RMIX learns slower than RMIX (α = 1) because it relies on the dynamic
risk predictor while the predictor is trained together with the agent, it will take more samples in
these environments. Interestingly, RMIX shows very good performance over other baselines.
20
Under review as a conference paper at ICLR 2021
training steps
Figure 13: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 3s5z_vs_3s6z
(heterogeneous and asymmetric scenario, very hard game).
Figure 14: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 6h_vs_8z (homo-
geneous and asymmetric scenario, very hard game)
training steps
Figure 15: test_battle_won_mean of RMIX, RMIX (α = 1) and QMIX on 27m_vs_30m
(homogeneous and asymmetric scenario, very hard game).
21
Under review as a conference paper at ICLR 2021
(a) 6h_vs_8z
(b) corridor
(c) 3s5z_vs_3s6z
(d) 5m_vs_6m
Figure 16: test_battle_won_mean of RMIX-static.
22
Under review as a conference paper at ICLR 2021
Table 4: Hyper-parameters
hyper-parameter	Value
Batch-size	32
Replay memory size	5,000
Optimizer	Adam
Learning rate (lr)	5e-4
Critic lr	5e-4
RMSProp alpha	0.99
RMSProp epsilon	0.00001
Gradient norm clip	10
Action-selector	-greedy
-start	1.0
-finish	0.05
-anneal-time	50,000 steps
Target-update-interval	200
Evaluation interval	10,000
Number of atoms (m)	55
K	10
Runner	episode
Training steps	1, 2, 3, 1.5, 2.5 and 8 millions
Discount factor (γ)	0.99
RNN hidden dim	64	
G.2 Additional Results Analysis
We provide additional results analysis of RMIX on corridor in Figure 17. There are 6 RMIX agents
in corridor. For brevity of visualization, we use the data of 3 agents (agent 0, 1 and 3) to analyse the
results and to demonstrate RMIX agents have learned to address time-consistency issue.
23

cr_agent_0
or_agent_l
a agent 3
Time-consistency a values
30
70
Game starts
RMIX agents are going to
win. Agent 1 (low health
value) walks outside the
observation of enemies in
order to survive
ιsode Steps
Agent 0 (low health value)
goes across the corridor to
draw enemies’ attention, a
values are low (risk-averse)
Agent 3 combats
with most of
enemies and it
quickly dies (a = 0)
Agent 0 and 1 show similar a
values as they walking around
and fighting with enemies. The
health values are high
Agent 0 dies (step
95) in the other side
of the corridor. The
risk level is zero
24 ZergIings∕⅛
.enemies
∣HSfYZealots：
vf⅛ 声，RMIX agents
Under review as a conference paper at ICLR 2021
One episode of game
starts, 6 Zealots
consist of RMIX
agents and 24
Zerglings compose
enemies
In order to win the game,
agent 3 draws the
attention of enemies
and go the other side of
the battlefield. The rest
agents combat with
fewer number enemies
As being
outnumbered, agent
3 quickly dies as
shown in green line
after agents die the
a = 0
Agent 0 is followed by
agent 1 (low health value)
in order to draw enemies
to come over. To avoid
being killed, a values are
low which means the
policy is risk-averse
In the other side, as
there are many
agents around and
the number of
enemies is small, the
a values are over 0.8
(nearly risk-neutral)
RMIX agents are
going to win, agent 1
walks outside the
range of the a
values of agent 1 are
over 0.8 near risk-
neutral
Figure 17: RMIX results analysis on corridor. We use trained model of RMIX and run the model to collect one episode data including game replay, states, actions,
rewards and α values (risk level). We show rewards of one episode and the corresponding α value each agent predicts per time step in row one and row two. We
provide description and analyses on how agents learn time-consistency α values for the rest rows. Pictures are screenshots from the game replay. Readers can watch
the game replay via this anonymous link: https://youtu.be/J-PG0loCDGk. Interestingly, it also shows emergent cooperation strategies between agents at
different time step during the episode, which demonstrate the superiority of RMIX.
Under review as a conference paper at ICLR 2021
G.3 Additional Results
We conduct experiments of RMIX, QMIX, MAVEN, Qatten, VDN, COMA and IQL on 17 SMAC
scenarios. We show results of test_battle_won_mean and test_return_mean of afore-
mentioned methods in Figure 19 and 20, respectively. RMIX shows leading performance on most
of scenarios, ranging from symmetric homogeneous scenarios to asymmetric heterogeneous scenar-
ios. Surprisingly, RMIX also shows superior performance on scenarios where micro-trick should be
learned to win the game.
In addition, we compare RMIX with QR-MIX (Hu et al., 2020). Unlike QMIX, QR-MIX decom-
poses the estimated joint return distribution into individual Q values. We implement QR-MIX with
PyMARL by using hyper-parameters in QR-MIX paper and train it on 3m (easy), 1c3s5z (easy),
5m_vs_6m (hard), 8m_vs_9m (hard), 10_vs_11m (hard), 27m_vs_30m (very hard), MMM2 (very
hard), 3s5z_vs_3s6z (very hard), corridor (very hard) and 6h_vs_8z (very hard) with 3 random seeds
for each scenario. Results are shown in Figure 18.
(b) 1c3s5z
(c) 5m_vs_6m
(d) 8m_vs_9m
(e) 10m_vs_11m
(f) 27m_vs_30m
(g) corridor
(h) 3s5z_vs_3s6z
(i) MMM2	(j) 6h_vs_8z
Figure 18: test_battle_won_mean of RMIX vs QR-MIX and QMIX.
25
Under review as a conference paper at ICLR 2021
3m
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.5M
Qatten
MAVEN
QMIX
IQL
QTRAN
VDN
COMA
Figure 19:	test_battle_won_mean of RMIX, MAVEN, QMIX, Qatten, IQL, QTRAN, VDN
and COMA on 17 SMAC StarCraft II scenarios.
26
Under review as a conference paper at ICLR 2021
6h vs 8z
5m_vs_6m	8m_vs_9m
0.0	1.0M 2.0M 3.OM
0.0	1.0M	2,OM 0.0	0.5M	1,0M
RMIX
Qatten
MAVEN
QMIX
IQL
QTRAN
VDN
COMA
Figure 20:	test_return_mean of RMIX, MAVEN, QMIX, Qatten, IQL, QTRAN, VDN,
COMA on 17 SMAC StarCraft II scenarios.
27