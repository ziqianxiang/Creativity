Under review as a conference paper at ICLR 2021
Augmented Sliced Wasserstein Distances
Anonymous authors
Paper under double-blind review
Abstract
While theoretically appealing, the application of the Wasserstein distance to large-scale
machine learning problems has been hampered by its prohibitive computational cost.
The sliced Wasserstein distance and its variants improve the computational efficiency
through random projection, yet they suffer from low projection efficiency because
the majority of projections result in trivially small values. In this work, we propose
a new family of distance metrics, called augmented sliced Wasserstein distances
(ASWDs), constructed by first mapping samples to higher-dimensional hypersurfaces
parameterized by neural networks. It is derived from a key observation that (random)
linear projections of samples residing on these hypersurfaces would translate to much
more flexible nonlinear projections in the original sample space, so they can capture
complex structures of the data distribution. We show that the hypersurfaces can be
optimized by gradient ascent efficiently. We provide the condition under which the
ASWD is a valid metric and show that this can be obtained by an injective neural
network architecture. Numerical results demonstrate that the ASWD significantly
outperforms other Wasserstein variants for both synthetic and real-world problems.
1	Introduction
Comparing samples from two probability distributions is a fundamental problem in statistics and machine
learning. The optimal transport (OT) theory (Villani, 2008) provides a powerful and flexible theoretical
tool to compare degenerative distributions by accounting for the metric in the underlying spaces. The
Wasserstein distance, which arises from the optimal transport theory, has become an increasingly popular
choice in various machine learning domains ranging from generative models to transfer learning (Gulrajani
et al., 2017; Arjovsky et al., 2017; Kolouri et al., 2019b; Lee et al., 2019; Cuturi & Doucet, 2014; Claici
et al., 2018; Courty et al., 2016; Shen et al., 2018; Patrini et al., 2018).
Despite its favorable properties, such as robustness to disjoint supports and numerical stability (Arjovsky
et al., 2017), the Wasserstein distance suffers from high computational complexity especially when the
sample size is large. Besides, the Wasserstein distance itself is the result of an optimization problem
— it is non-trivial to be integrated into an end-to-end training pipeline of deep neural networks, unless
one can make the solver for the optimization problem differentiable. Recent advances in computational
optimal transport methods focus on alternative OT-based metrics that are computationally efficient and
出fferentiably solvable (Peyre & Cuturi, 2019). Entropy regularization is introduced in the Sinkhorn
distance (Cuturi, 2013) and its variants (Altschuler et al., 2017; Dessein et al., 2018; Lin et al., 2019) to
smooth the optimal transport problem; as a result, iterative matrix scaling algorithms can be applied to
provide significantly faster solutions with improved sample complexity (Genevay et al., 2019).
An alternative approach is to approximate the Wasserstein distance through slicing, i.e. linearly
projecting, the distributions to be compared. The sliced Wasserstein distance (SWD) (Bonneel et al.,
2015) is defined as the expected value of Wasserstein distances between one-dimensional random
projections of high-dimensional distributions. The SWD shares similar theoretical properties with the
Wasserstein distance (Bonnotte, 2013) and is computationally efficient since the Wasserstein distance
in one-dimensional space has a closed form solution based on sorting. Deshpande et al. (2019) extends
the sliced Wasserstein distance to the max-sliced Wasserstein distance (max-SWD), by finding a single
projection direction with the maximal distance between projected samples. In Nguyen et al. (2020), the
distributional sliced Wasserstein distance (DSWD) finds a distribution of projections that maximizes the
expected distances over these projections. The subspace robust Wasserstein distance extends the idea of
slicing to projecting distributions on linear subspaces (Paty & Cuturi, 2019). However, the linear nature
1
Under review as a conference paper at ICLR 2021
▽ Projected sample from μ	Q Sample from μ
▼ Projected sample from ν	∙ Sample from ν
(a)
(b)
Distance between 1-d projections Distance between 1-d projections
(c)	(d)
Figure 1: (a) and (b) are visualizations of projections for the ASWD and the SWD between two
2-dimensional Gaussians. (c) and (d) are distance histograms for the ASWD and the SWD between two
100-dimensional Gaussians. Figure 1(a) shows that the injective neural network embedded in the ASWD
learns data patterns (in the X-Y plane) and produces well-separate projected values (Z-axis) between
distributions in a random projection direction. The high projection efficiency of the ASWD is evident
in Figure 1(c), as almost all random projection directions in a 100-dimensional space lead to significant
distances between 1-dimensional projections. In contrast, random linear mappings in the SWD often
produce closer 1-d projections (Z-axis) (Figure 1(b)); as a result, a large percentage of random projection
directions in the 100-d space result in trivially small distances (Figure 1(d)), leading to a low projection
efficiency in high-dimensional spaces.
of these projections usually leads to low projection efficiency of the resulted metrics in high-dimensional
spaces (Deshpande et al., 2019; Liutkus et al., 2019; Kolouri et al., 2019a).
More recently, there are growing interests and evidences that slice-based Wasserstein distances with
nonlinear projections can improve the projection efficiency, leading to a reduced number of projections
needed to capture the structure of the data distribution (Kolouri et al., 2019a; Nguyen et al., 2020).
(Kolouri et al., 2019a) extends the connection between the sliced Wasserstein distance and the Radon
transform (Abraham et al., 2017) to define generalized sliced Wasserstein distances (GSWDs) by utilizing
generalized Radon transforms (GRTs). It is shown in (Kolouri et al., 2019a) that the GSWD is indeed
a metric if and only if the adopted GRT is injective. Injective GRTs are also used to extend the DSWD
to the distributional generalized sliced Wasserstein distance (DGSWD) (Nguyen et al., 2020). However,
both the GSWD and the DGSWD are restricted by the limited class of injective GRTs, which utilize the
circular functions and a finite number of harmonic polynomial functions with odd degrees as their defining
function (Kuchment, 2006; Ehrenpreis, 2003). The results reported in (Kolouri et al., 2019a; Nguyen et al.,
2020) show impressive performance from the GSWD and the DGSWD, yet they require one to specify
a particular form of defining function from the aforementioned limited class of candidates. However, the
selection of defining function is usually task-dependent and needs domain knowledge. In addition, the
impact on performance from different defining functions is still unclear.
One variant of the GSWD (Kolouri et al., 2019a) is the GSWD-NN, which generates projections directly
with neural network outputs to remove the limitations of slicing distributions with predefined GRTs. In
the GSWD-NN, the number of projections, which equals the number of nodes in the neural network’s
output layer, is fixed. Hence different neural networks are needed if one wants to change the number of
projections. There is also no random projections involved in the resulted GSWD-NN, since the projection
results are determined by the neural network’s weights. Besides, the GSWD-NN is a pseudo-metric since
it uses a vanilla neural network, rather than the Radon transform or GRTs, as its push-forward operator.
Therefore, the GSWD-NN does not fit into the theoretical framework of the GSWD and does not inherit
its geometric properties.
In this paper, we present the augmented sliced Wasserstein distance (ASWD), a distance metric constructed
by first mapping samples to hypersurfaces in an augmented space, which enables flexible nonlinear slicing
of data distributions for improved projection efficiency (See Figure 1). Our main contributions include: (i)
We exploit the capacity of nonlinear projections employed in the ASWD by constructing injective mapping
with arbitrary neural networks; (ii) We prove that the ASWD is a valid distance metric; (iii) We provide
a mechanism in which the hypersurface where high-dimensional distributions are projected onto can be
2
Under review as a conference paper at ICLR 2021
optimized and show that the optimization of hypersurfaces can help slice-based Wasserstein distances
improve their projection efficiency. Hence, the ASWD is data-adaptive, i.e. the hypersurfaces can be
learned from data. This implies one does not need to specify a defining function from a limited class of
candidates; (iv) We demonstrate superior performance of the ASWD in numerical experiments for both
synthetic and real-world datasets.
The remainder of the paper is organized as follows. Section 2 reviews the necessary background. We
present the proposed method and its numerical implementation in Section 3. Numerical experiment results
are presented and discussed in Section 4. We conclude the paper in Section 5.
2	Background
In this section, we provide a brief review of concepts related to the proposed work, including the
Wasserstein distance, (generalized) Radon transform and (generalized) sliced Wasserstein distances.
Wasserstein distance: Let Pk (Ω) bea set ofBorel probability measures with finite k-th moment onaPolish
metric space (Ω,d) (Villani, 2008). Given two probability measures μ, V ∈ Pk (Ω), whose probability density
functions (PDFs) are pμ and PV, the Wasserstein distance of order k ∈ [1,+∞) between μ and V is defined as:
Wk(μ,ν) =
(∙γ∈inf J d(χ,y)kdγ(χ,y)),
(1)
where d(∙, ∙)k is the cost function, Γ(μ,ν) represents the set of all transportation plans γ, i.e. joint
distributions whose marginals are pμ and PV, respectively. With a slight abuse of notation, We
interchangeably use Wk(μ,ν) and Wk(pμ,Pν).
While the Wasserstein distance is generally intractable for high-dimensional distributions, there are several
favorable cases where the optimal transport problem can be efficiently solved. In particular, if μ and V
are continuous one-dimensional measures, the Wasserstein distance between μ and V has a closed form
solution (Bonneel et al., 2015):
Wk(μ,V)= (/ d(F-I(Z),FL(Zy)kdz
1
k
(2)
where F-I and F-I are inverse cumulative distribution functions (CDFs) of μ and v, respectively.
Radon transform and generalized Radon transform: The Radon transform (Radon, 1917) maps a
function f (∙) ∈ L1 (Rd) to the space of functions defined over spaces of lines in Rd. The Radon transform
of f (∙) is defined by line integrals of f (∙) along all possible hyperplanes in Rd:
Rf(t,θ)=	f (x)δ(t-hx,θi)dx,
Rd
(3)
where t ∈R and θ ∈ Sd-1 represent the parameters ofhyperplanes in Rd, δ(∙) is the Dirac delta function,
and h,∙ refers to the Euclidean inner product.
By replacing the inner product hx,θi in Equation (3) with β(x,θ), a specific family of functions named
as defining function in Kolouri et al. (2019a), the generalized Radon transform (GRT) (Beylkin, 1984)
is defined as:
Gf(t,θ)=
Rd
f (x)δ(t-β(x,θ))dx,
(4)
where t ∈ R, θ ∈ Ωθ while Ωθ is a compact set of all feasible θ, e.g. Ωθ = SdT for β(x,θ) = hx,θ) (Kolouri
et al., 2019a).
In practice, we can empirically approximate the Radon transform and the GRT of a probability density
function p* via:
1N
RPμ(t,θ) ≈ N^δ(t-hXn,θi),	⑸
n=1
1N
GPμ(t,θ) ≈ Nfδ(t-β(xn,θ)),	(6)
n=1
3
Under review as a conference paper at ICLR 2021
where Xn 〜pμ and N is the number of samples. Notably, the Radon transform is a linear bijection (Helga-
son, 1980), and the GRT is a bijection if the defining function β satisfies certain conditions (Beylkin, 1984).
Sliced Wasserstein distance and generalized sliced Wasserstein distance: By applying the Radon
transform to pμ and PV to obtain multiple projections, the sliced WaSSerStein distance (SWD) decomposes
the high-dimensional Wasserstein distance into multiple one-dimensional Wasserstein distances which
can be efficiently evaluated (Bonneel et al., 2015). The k-SWD between μ and V is defined by:
1
SWDk(μ,ν)= (/a IWk(Rpμ(∙,θ),Rpν(M)dθ)”
(7)
where the Radon transform R defined by Equation (3) is adopted as the measure push-forward operator.
The GSWD generalizes the idea of SWD by projecting distributions onto hypersurfaces rather than
hyperplanes (Kolouri et al., 2019a). The GSWD is defined as:
1
GSWDk(μ,ν)= (W Wk(Gpμ(∙,θ),Gpν(M)Q”
(8)
where the GRT G is used as the measure push-forward operator. The Wasserstein distances between
one-dimensional distributions can be obtained by sorting projected samples and calculating the distance
between sorted samples (Kolouri et al., 2019b): with L random projections, the SWD and GSWD between
μ and V can be approximated by:
(1 LN	、1
SWDk(μ,ν) ≈ ZE EKxIx [n] ,θιi-hyιy 网仇器,
l=1 n=1
/1 L N	、1
GSWDk (μ,ν) ≈ GA N∣β(xIx [n],θl)-β(yIy [n],θl)∣k),
(9)
(10)
where Ixl and Iyl are sequences consist of the indices of sorted samples which satisfy
hxIxl [n],θli ≤ hxIxl [n+1],θli, hyIyl [n],θli ≤ hyIyl [n+1],θli in the SWD, and β(xIxl [n],θl) ≤ β(xIxl [n+1] ,θl),
β(yIl [n],θl) ≤ β(yIl [n+1],θl) in the GSWD. It is proved in Bonnotte (2013) that the SWD is a valid
distance metric. The GSWD is a valid metric except for its neural network variant (Kolouri et al., 2019a).
3	Augmented sliced Wasserstein distances
In this section, we propose a new distance metric called the augmented sliced Wasserstein distance (ASWD),
which embeds flexible nonlinear projections in its construction. We also provide an implementation recipe
for the ASWD.
3.1	Spatial Radon transform and augmented sliced Wasserstein distance
In the definitions of the SWD and GSWD, the Radon transform (Radon, 1917) and the generalized Radon
transform (GRT) (Beylkin, 1984) are used as the push-forward operator for projecting distributions to
a one-dimensional space. However, it is not straightforward to design defining functions β(x,θ) (Kolouri
et al., 2019a) for the GRT due to certain non-trivial requirements for the function (Beylkin, 1984). In
practice, the assumption of the transform can be relaxed, as Theorem 1 shows that as long as the transform
is injective, the corresponding ASWD metric is a valid distance metric.
To help us define the augmented sliced Wasserstein distance, we first introduce the spatial Radon transform
which includes the vanilla Radon transform and the polynomial GRT as special cases (See Remark 2).
Definition 1. Given an injective mapping g(∙): Rd → Rdθ and a probability measure μ ∈ P(Rd) which
probability density function (PDF) is pμ, the spatial Radon transform of pμ is defined as
Hpμ(tθ∖g)= / pμ(x)δ(t-hg(x),θ))dx,
Rd
(11)
where t∈R andθ∈Sdθ-1 are the parameters of hypersurfaces in Rd.
4
Under review as a conference paper at ICLR 2021
Remark 1. Note that the spatial Radon transform can be interpreted as applying the vanilla Radon
transform to the PDF of ^ = g(x), where X 〜pμ. Denote the PDF of X by p^g, the spatial Radon transform
defined by Equation (11) can be rewritten as:
Hp“(t,e；g)= Eχ~P”[δ(t-hg(x),θi)],
=E^~p^g[δ(t-h^,θi)]
=/p^g (X')δ(t-hx,θ'i')dx
=RPμg (t,θ).
(12)
Hence the spatial Radon transform inherits the theoretical properties of the Radon transform subject to
certain conditions of g(∙) and incorporates nonlinear projections through g(∙).
In What follows, We use f ≡ f2 to denote functions fι(∙) : X → R and f2(∙) : X → R that satisfy
f1(X) =f2(X) for almost∀X∈X.
Lemma 1. Given an injective mapping g(∙) : Rd → Rdθ and two probability measures μ,ν ∈ P(Rd)
whose probability density functions are pμ and PV, respectively, for all t ∈ R and θ ∈ Sdθ-1,
Hpμ(t,θ[g) ≡Hpν (t£；g) ifand only if pμ ≡PV, i.e. the spatial Radon transform is injective. Moreover,
the spatial Radon transform is injective ifand only ifthe mapping g(∙) is an injection.
See Appendix A for the proof of Lemma 1.
Remark 2. The spatial Radon transform degenerates to the vanilla Radon transform when the mapping
g(∙) is an identity mapping. When g(∙) is a homogeneous polynomialfunction with odd degrees, the spatial
Radon transform is equivalent to the polynomial GRT (Ehrenpreis, 2003).
Appendix B provides the proof of Remark 2.
We noW introduce the augmented sliced Wasserstein distance, by utilizing the spatial Radon transform
as the measure push-forWard operator:
Definition 2. Given two probability measures μ,ν ∈ Pk (Rd), whose probability densityfunctions are p*
andPV, respectively, and an injective mapping g(∙): Rd →Rdθ, the augmented sliced Wasserstein distance
(ASWD) of order k ∈ [1,+∞) is defined as:
ASWDk(〃,v;g)=(/	Wk 出pμ(Rg),Hpν (*g))dθ)k,	(13)
Sdθ-1
where θ ∈ Sdθ-1, Wk is the k-Wasserstein distance defined by Equation (1), andH refers to the spatial
Radon transform defined by Equation (11).
Remark 3. Following the connection between the spatial Radon transform and the vanilla Radon
transform as shown in Equation (12), the ASWD can be rewritten as:
ASWDk(〃,v;g)= ( Z	Wk(RPμg(∙,θ),Rp^g(∙,θ))dθ) k
Sdθ-1
=SWDk (μg ,Vg),	(14)
where μg and Vg are probability measures on Rdθ which satisfy g(x)〜μg for X 〜μ and g(y)〜Vg for
y〜v.
Theorem 1. The augmented sliced Wasserstein distance (ASWD) of order k ∈ [1, +∞) defined by
Equation (13) with a mapping g(∙): Rd → Rdθ is a metric on Pk(Rd) ifand only if g(∙) is injective.
Proof of Theorem 1 is provided in Appendix C.
3.2 Numerical implementation
We discuss in this section how to realize injective mapping g(∙) with neural networks due to their
expressiveness and optimize it With gradient based methods.
Injective neural networks: As stated in Lemma 1 and Theorem 1, the injectivity of g(∙) is the sufficient
and necessary condition for the ASWD being a valid metric. Thus we need specific architecture designs on
5
Under review as a conference paper at ICLR 2021
implementing g(∙) by neural networks. One option is the family of invertible neural networks (Behrmann
et al., 2019; Karami et al., 2019; Song et al., 2019), which are both injective and surjective. However,
the running cost of those models is usually much higher than that of vanilla neural networks. We propose
an alternative approach by concatenating the input x of an arbitrary neural network to its output φω (x):
gω (x) = [x,φω (x)].	(15)
It is trivial to show that gω(x) is injective, since different inputs will lead to different outputs. Although
embarrassingly simple, this idea of concatenating the input and output of neural networks has found
success in preserving information with dense blocks in the DenseNet (Huang et al., 2017), where the input
of each layer is injective to the output of all preceding layers.
Optimization objective: We aim to project samples to maximally discriminating hypersurfaces between
two distributions, so that the projected samples between distributions are most dissimilar subject to certain
constraints on the hypersurface, as shown in Figure 1. Similar ideas have been employed to identify
important projection directions (Deshpande et al., 2019; Kolouri et al., 2019a; Paty & Cuturi, 2019) or
a discriminative ground metric (Salimans et al., 2018) in optimal transport metrics. For the ASWD, the
parameterized injective neural network gω(∙) is optimized by maximizing the following objective:
1
L(μ”,gω,λ)=( [	wk(HPμ(∙,θ'gω),HPν 3θ,gω ))dθ} -Lλ,	(16)
Sdθ-1
where λ>0 and the regularization term Lλ = λEχ,y〜*〃 [(∣∣gω(x)∣∣2 + ∣∣gω(y)∣∣2)] is used to control the
norm of the output of gω (∙), otherwise the projections may be arbitrarily large.
Remark 4. The regularization coefficient λ adjusts the introduced non-linearity in the evaluation of the
ASWD by controlling the norm of φω(∙) in Equation (15). In particular, when λ →∞, the nonlinear term
φω(∙) shrinks to 0. The rank ofthe augmented space is hence explicitly controlled by theflexible choice
of φω(∙) and implicitly regularized by Lλ.
By plugging the optimized g： λ(∙) = argmax(L(μ,ν[gω,λ)) into Equation (13), We obtain the empirical
gω
version of the ASWD. Notably, the regularization term Lλ is only used when maximizing the objective in
Equation (16), once the optimization is completed, Lλ is not used in the calculation of the ASWD defined
by Equation (13). Pseudocode is provided in Appendix D.
4	Experiments
In this section, we describe the experiments that we have conducted to evaluate performance of the
proposed distance metric. The GSWD leads to the best performance in a sliced Wasserstein flow problem
reported in Kolouri et al. (2019a) and the DSWD outperforms the compared methods in the generative
modeling task examined in Nguyen et al. (2020). Hence we compare performance of the ASWD with
the state-of-the-art distance metrics in the same examples and report results as below1.
To examine the robustness of the ASWD, throughout the experiments, we adopt the injective network
architecture given in Equation (15) and set φω to be a one layer fully-connected neural network whose
outputs’ dimension equals its inputs’ dimension, with a ReLU layer as its activation function.
4.1	Sliced Wasserstein flows
We first consider the problem of evolving a source distribution μ toa target distribution V by minimizing
Wasserstein distances between μ and V in the sliced Wasserstein flow task reported in Kolouri et al. (2019a).
∂tμt = -VSWD(μt,ν),	(17)
where μt refers to the updated source distribution at each iteration t. The SWD in Equation (17) can
be replaced by other sliced-Wasserstein distances to be evaluated. As in Kolouri et al. (2019a), the
2-Wasserstein distance was used as the metric for evaluating performance of different distance metrics
in this task. The set of hyperparameter values used in this experiment can be found in Appendix E.1.
1Code to reproduce experiment results is available at : https://bit.ly/2Y23wOz.
6
Under review as a conference paper at ICLR 2021
ASWD
SWD
DSWD
GSWD-NN 1
GSWD-PoIy 3
GSWD-CircuIar
8 GauSSian
Knot
Swiss Roll
Moon
Figure 2: The first and third columns are target distributions. The second and fourth columns are log
2-Wasserstein distances between the target distribution and the source distribution. The horizontal axis
show the number of training iterations. Solid lines and shaded areas represent the average values and 95%
confidence intervals of log 2-Wasserstein distances over 50 runs. A more extensive set of experimental
results can be found in Appendix F.1.
Without loss of generality, We initialize μo to be the standard normal distribution N(0,I). We repeat each
experiment 50 times and record the 2-Wasserstein distance between μ and V at every iteration. In Figure
2, We plot the 2-Wasserstein distances betWeen the source and target distributions as a function of the
training epochs and the 8-Gaussian, the Knot, the Moon, and the Swiss roll distributions are respective
target distributions. For clarity, Figure 2 displays the experiment results from the 6 best performing
distance metrics, including the ASWD, the DSWD, the SWD, the GSWD-NN 1, which directly generates
projections through a one layer MLP, as well as the GSWD with the polynomial of degree 3, circular
defining functions, out of the 12 distance metrics we compared.
We observe from Figure 2 that the ASWD not only leads to lower 2-Wasserstein distances, but also
converges faster by achieving better results with fewer iterations than the other methods in these four
target distributions. A complete set of experimental results with 12 compared distance metrics and 8
target distributions are included in Appendix F.1. The ASWD outperforms the compared state-of-the-art
sliced-based Wasserstein distance metrics with 7 out of the 8 target distributions except for the 25-Gaussian.
This is achieved through the simple injective network architecture given in Equation (15) and a one layer
fully-connected neural network with equal input and output dimensions throughout the experiments. In
addition, ablation study is conducted to study the effect of injective mappings, the regularization coefficient
λ, and the optimization of hypersurfaces in the ASWD. Details can be found in Appendix F.2.
4.2	Generative modeling
In this experiment, we use the sliced-based Wasserstein distances for a generative modeling task described in
Nguyen et al. (2020). The task is to generate images using generative adversarial networks (GANs) (Good-
fellow et al., 2014) trained on either the CIFAR10 dataset (64×64 resolution) (Krizhevsky, 2009) or the
CELEBA dataset (64×64 resolution) (Liu et al., 2015). Denote the hidden layer and the output layer of the
discriminator by hψ and DΨ, and the generator by GΦ, we train GAN models with the following objectives:
min SWD(hψ(pr),hψ (GΦ(pz))),
max Ex〜pr[log(Dψ(hψ(x)))]+Ez〜pz[log(1-Dψ(hψ(Gφ(z))))],
Ψ,ψ	r	z
(18)
(19)
7
Under review as a conference paper at ICLR 2021
Table 1: FID scores of generative models trained with different distance metrics. Lower scores indicate better
image qualities. L is the number of projections, we run each experiment 10 times and report the average
values and standard errors of FID scores for CIFAR10 dataset and CELEBA dataset. The running time per
training iteration for one batch containing 512 samples is computed based on a computer with an Intel (R)
Xeon (R) Gold 5218 CPU 2.3 GHz and 16GB of RAM, and a RTX 6000 graphic card with 22GB memories.
L	SWD	GSWD	DSWD	ASWD
	FID I t (s/it)	FJID	I t (s/it)	FID I t (s/it)	FJID	I t (s/it)
OFAR10
10	192.6±5.7	0.32	189.5±6.0	0.35	79.0±4.2	0.48	732±3.1	0.55
100	155.0±2.9	0.32	155.9±3.2	0.70	72.2±8.2	0.51	66.7±32	0.57
1000	126.0±2.9	0.34	134.5±2.7	2.10	74.3±4.3	1.22	65.5±3.9	1.32
CELEBA
10	118.3±3.1	0.32	143.2±5.5	0.35	105.3±3.4	0.49	99.2±4.3	0.53
100	116.0±2.8	0.33	120.8±1.8	0.69	103.1±3.8	0.51	94.3±2.2	0.56
1000	104.4±2.8	0.34	101.8±1.8	2.14	97.4±2.1	1.21	90.5±3.0	1.31
Epoch
(a) FID scores on CIFAR10 (L = 1000)
(b) FID scores on CELEBA (L= 1000)
Figure 3: FID scores of generative models trained with different metrics on CIFAR10 and CELEBA
datasets with L= 1000 projections. The error bar represents the standard deviation of the FID scores at
the specified training epoch among 10 simulation runs.
where pz is the prior of latent variable z and pr is the distribution of real data. The SWD in Equation (18)
is replaced by the ASWD and other variants of the SWD to compare their performance. The GSWD with
the polynomial defining function and the DGSWD is not included in this experiment due to its excessively
high computational cost in high-dimensional space.The Frechet Inception Distance (FID score) (HeUseI
et al., 2017) is used to assess the quality of generated images. More details on the network structures and
the parameter setup used in this experiment are available in Appendix E.2.
We run 200 and 100 training epochs to train the GAN models on the CIFAR10 and the CELEBA dataset,
respectively. Each experiment is repeated for 10 times. We report experimental results in Table 1. With
the same number of projections and a similar computation cost, the ASWD leads to significantly improved
FID scores among all evaluated distances metrics on both datasets, which implies that images generated
with the ASWD are of higher qualities. Figure 3 plots the FID scores recorded during the training process.
The GAN model trained with the ASWD exhibits a faster convergence as it reaches lower FID scores
with fewer epochs. Randomly selected samples of generated images are presented in Appendix G.
5	Conclusion
We proposed a novel variant of the sliced Wasserstein distance, namely the augmented sliced Wasserstein
distance (ASWD), which is flexible, has high projection efficiency, and generalizes well. The ASWD
adaptively updates the hypersurface where the samples are projected onto by learning from data. We
proved that the ASWD is a valid distance metric and presented its numerical implementation. We
8
Under review as a conference paper at ICLR 2021
reported empirical performance of the ASWD over state-of-the-art sliced Wasserstein metrics in numerical
experiments. The ASWD leads to the smallest distance errors over the majority of datasets in a sliced
Wasserstein flow task and superior performance in a generative modeling task.
The ASWD can be extended in several directions. One future research topic is to incorporate the framework
of the ASWD into the sliced Gromov-Wasserstein distance, such that data-adaptive, nonlinear projections
can be learned to compare distributions whose supports do not necessary lie in the same metric space.
It would be also interesting to explore the identification of the barycenter of objects via the ASWD, e.g.
through an optimization scheme alternating between optimization of the nonlinear maps in the ASWD
framework and the barycenter. Another interesting area is the incorporation of the ASWD with state-of-the-
art generative models to further exploit projection efficiency. We anticipate that they will provide promising
directions for enabling the generalization for sliced Wasserstein distance to be applied in wider domains.
9
Under review as a conference paper at ICLR 2021
References
I.	Abraham, R. Abraham, M. Bergounioux, and G. Carlier. Tomographic reconstruction from a few views:
a multi-marginal optimal transport approach. Applied Mathematics & Optimization, 75(1):55-73,2017.
J.	Altschuler, J. Niles-Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal
transport via Sinkhorn iteration. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
pp. 1964-1974, Long Beach, California, USA, 2017.
M.	Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Proc.
International Conference on Machine Learning (ICML), pp. 214-223, Sydney, Australia, 2017.
J. Behrmann, W. Grathwohl, R. T. Q. Chen, D. Duvenaud, and J. Jacobsen. Invertible residual networks.
In Proc. International Conference on Machine Learning (ICML), pp. 573-582, Long Beach, California,
USA, 2019.
G. Beylkin. The inversion problem and applications of the generalized Radon transform. Communications
on Pure and Applied Mathematics, 37(5):579-599, 1984.
N. Bonneel, J. Rabin, G. Peyre, and H. Pfister. Sliced and Radon Wasserstein barycenters of measures.
Journal of Mathematical Imaging and Vision, 51(1):22-45, 2015.
N.	Bonnotte. Unidimensional and evolution methods for optimal transportation. PhD thesis, Paris 11, 2013.
S. Claici, E. Chien, and J. Solomon. Stochastic Wasserstein barycenters. In Proc. International Conference
on Machine Learning (ICML), pp. 999-1008, Stockholm, Sweden, 2018.
N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE
Transactions on Pattern Analysis and Machine Intelligence (IPAMI), 39(9):1853-1865, 2016.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 2292-2300, Lake Tahoe, Nevada, USA, 2013.
M. Cuturi and A. Doucet. Fast computation of Wasserstein barycenters. In Proc. International Conference
on Machine Learning (ICML), pp. 685-693, Beijing, China, 2014.
I. Deshpande et al. Max-sliced Wasserstein distance and its use for GANs. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 10648-10656, Long Beach, California, USA,
2019.
A. Dessein, N. Papadakis, and J. Rouas. Regularized optimal transport and the rot mover’s distance. The
Journal of Machine Learning Research (JMLR), 19(1):590-642, 2018.
L. Ehrenpreis. The universality of the Radon transform, chapter 5, pp. 299-363. Oxford University Press,
Oxford, UK, 2003.
A. Genevay, L. Chizat, F. Bach, M. Cuturi, and G. Peyre. Sample complexity of Sinkhorn divergences. In
International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1574-1583, Okinawa,
Japan, 2019.
I. Goodfellow et al. Generative adversarial nets. In Proc. Advances in Neural Information Processing
SyStemS (NeurIPS),期.2672-2680, Montreal, Canada, 2014.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein
GANs. In Proc. AdvanceS in neural information proceSSing SyStemS (NeurIPS), pp. 5767-5777, Long
Beach, California, USA, 2017.
S. Helgason. The Radon tranSform, volume 2. Basel, Switzerland: Springer, 1980.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two time-scale
update rule converge to a local Nash equilibrium. In Proc. AdvanceS in neural information proceSSing
SyStemS (NeurIPS), pp. 6626-6637, Long Beach, California, USA, 2017.
G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional networks.
In Proc. IEEE Conference on Computer ViSion and Pattern Recognition (CVPR), pp. 4700-4708,
Hawaii, USA, 2017.
10
Under review as a conference paper at ICLR 2021
M. Karami, D. Schuurmans, J. Sohl-Dickstein, L. Dinh, and D. Duckworth. Invertible convolutional flow.
In Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 5636-5646, Vancouver,
Canada, 2019.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. International Conference
on Learning Representations (ICLR), San Diego, California, USA, 2015.
S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde. Generalized sliced Wasserstein distances.
In Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 261-272, Vancouver,
Canada, 2019a.
S. Kolouri, P. E Pope, C. E. Martin, and G. K. Rohde. Sliced-Wasserstein autoencoders. In Proc.
International Conference on Learning Representations (ICLR), New Orleans, Louisiana, USA, 2019b.
A. Krizhevsky. Learning multiple layers of features from tiny images. Tech Report, 2009.
P. Kuchment. Generalized transforms of Radon type and their applications. In Proc. Symposia in Applied
Mathematics, volume 63, pp. 67, San Antonio, Texas, USA, 2006.
C. Lee, T. Batra, M. B. Baig, and D. Ulbricht. Sliced Wasserstein discrepancy for unsupervised domain
adaptation. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
10285-10295, Long Beach, California, USA, 2019.
T. Lin, N. Ho, and M. I. Jordan. On efficient optimal transport: an analysis of greedy and accelerated
mirror descent algorithms. University of California, Berkeley, 2019.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proc. IEEE International
Conference on Computer Vision (ICCV), pp. 3730-3738, Las Condes, Chile, 2015.
A. Liutkus, U. Simyekli, S. Majewski, Szymon, Alain D., and E Stoter. Sliced-WaSserstein flows:
Nonparametric generative modeling via optimal transport and diffusions. In Proc. International
Conference on Machine Learning (ICML), pp. 4104-4113, 2019.
K. Nguyen, N. Ho, T. Pham, and H. Bui. Distributional sliced-Wasserstein and applications to generative
modeling. arXiv:2002.07367, 2020.
G. Patrini et al. Sinkhorn autoencoders. In Proc. Conference on Uncertainty in Artificial Intelligence
(UAI), Tel Aviv, Israel, 2018.
F.	Paty and M. Cuturi. Subspace robust Wasserstein distances. In Proc. International Conference on
Machine Learning (ICML), pp. 5072-5081, Long Beach, California, USA, 2019.
G.	Peyre and M. Cuturi. Computational optimal transport. Foundations and Trends® in Machine
Learning, 11(5-6):355-607, 2019.
J. Radon. Uber die bestimmung von funktionen durch ihre integralwerte laengs gewisser mannigfaltigkeiten.
Ber. Verh. Saechs. Akad. Wiss. Leipzig Math. Phys. Kl., 69:262, 1917.
D. Rezende and S. Mohamed. Variational inference with normalizing flows. In International Conference
on Machine Learning (ICML), pp. 1530-1538, Lille, France, 2015.
T. Salimans, H. Zhang, A. Radford, and D. Metaxas. Improving GANs using optimal transport. In Proc.
International Conference on Learning Representations (ICLR), Vancouver, Canada, 2018.
J. Shen, Y Qu, W Zhang, and Y Yu. Wasserstein distance guided representation learning for domain
adaptation. In Proc. AAAI Conference on Artificial Intelligence (AAAI), pp. 4058-4065, New Orleans,
Louisiana, USA, 2018.
Y. Song, C. Meng, and S. Ermon. Mintnet: Building invertible neural networks with masked convolutions. In
Proc. Advances in Neural Information Processing Systems, pp. 11002-11012, Vancouver, Canada, 2019.
C. Villani. Optimal Transport: old and new, volume 338. Berlin, Germany: Springer Science & Business
Media, 2008.
11