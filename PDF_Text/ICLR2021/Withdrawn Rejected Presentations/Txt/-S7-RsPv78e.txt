Under review as a conference paper at ICLR 2021
Optimizing Quantized Neural Networks in a
Weak Curvature Manifold
Anonymous authors
Paper under double-blind review
Ab stract
Quantized Neural Networks (QNNs) have achieved an enormous step in improv-
ing computational efficiency, making it possible to deploy large models to mobile
and miniaturized devices. In order to narrow the performance gap between low-
precision and full-precision models, we introduce the natural gradient to train a
low-precision model by viewing the parameter space as a Riemannian manifold.
Specifically, we propose a novel Optimized Natural Gradient Descent (ONGD)
method defined by the Hyperbolic divergence, which provides a perspective to
calculate the optimized natural gradient in weak curvature and updates the param-
eters with an amount of computation comparable to Stochastic Gradient Descent
(SGD). We conduct an ablation study and results show that the 4-bit quantized
ResNet-32 trained with ONGD has a better result than SGD, i.e. 2.05% higher in
Top-1 accuracy on CIFAR100 dataset. Further comparison experiments illustrate
that our method achieves state-of-the-art results in CIFAR and ImageNet datasets,
where the 8-bit version of MobileNet achieves 0.25%/0.13% higher in Top-1/Top-
5 accuracies than the full-precision version on ImageNet dataset.
1 Introduction
Neural networks can handle many complex tasks due to their large number of trainable parameters
and strong nonlinear capabilities. However, the massive amount of models and calculations hinders
the application of neural networks on mobile and miniaturized devices, which naturally comes with
constraints on computing power and resources. Neural network quantization is considered to be
an efficient solution in the inference that alleviates the number of parameters and optimizes the
computation by reducing the bit width of weights and activations.
Neural network quantization can be roughly divided into two categories: forced quantization and
relaxed quantization. Most of the quantization methods adopted by the QNNs belong to the former,
i.e. enforcing discretization during training. For instance, XNOR-Net (Rastegari et al., 2016) quan-
tized the filters and the inputs of neural network to binary, which can use efficient binary operations.
DeepShift (Elhoushi et al., 2019) replaced all multiplications with bitwise shift and sign flipping
by representing the weights as 6-bit. Besides, INT8 (Zhu et al., 2020) had achieved the quantized
gradients according to the distinctive characteristics of gradients. Moreover, MeliusNet (Bethge
et al., 2020) proposed a mixed-precision method between 32-bit and 1-bit by adding dense block
and improvement block into the network structure. Other relaxed quantizations maintain partial
discretization during training, e.g. INQ (Zhou et al., 2017) divided the weights into two groups
until all parameters are quantized, where the first group is directly quantized and fixed; the second
group needs to be retrained to make up for the decrease of accuracy caused by quantization of the
first group. RQ (Louizos et al., 2019) introduced a differentiable quantizer that can transform the
continuous distributions of weights and activations to categorical distributions with gradient-based
optimization. Recently, AQE (Chen et al., 2020) proposed a novel asymptotic-quantized estimator
to discretize the weights and activations gradually. However, these methods all use general gradi-
ents for training, which lacks the model-level information. In this paper, we introduce the natural
gradient to train a low-precision model that considers the model’s curvature information, as shown
the red line in Figure 1.
For neural networks with a scale of one million or more parameters, the time complexity of inverting
Fisher Information Matrix (FIM), a component of natural gradients, is O(n3) (Povey et al., 2014).
1
Under review as a conference paper at ICLR 2021
Figure 1: The general gradient can be intuitively expressed by the black line in Euclidean space.
Suppose the parameter space is changed to a Riemannian manifold. In that case, the natural gradient
can be described by the red line (Amari, 2016) in arbitrary curvature. In weak curvature, we can
visualize the optimized natural gradient with the blue line.
Prior to this, there were some works to calculate the natural gradient efficiently, e.g. (Roux et al.,
2008) decomposed the FIM into multiple diagonal blocks, where each diagonal block is approxi-
mated by a low-rank matrix. (Bastian et al., 2011) also used the idea of diagonal blocks by construct-
ing a diagonal block that corresponds to a weight matrix. (Martens & Grosse, 2015) proposed to
approximate FIM through the Kronecker product of two smaller matrices to improve computational
efficiency. Even so, the complex decomposition methods are still not suitable for large-scale tasks,
and the amount of computation is large compared to general gradient. We propose a novel ONGD
method in weak curvature with constraint Hyperbolic divergence, i.e. the blue line in Figure 1,
whose calculated amount is close to general gradient.
In this paper, our purpose is to introduce a novel method of training QNNs that bypasses SGD to
achieve better performance. The main contributions of this work are three-fold: First, we show
how to calculate the natural gradient by constructing a Riemannian metric or FIM in a low-precision
model. Second, we define a novel Hyperbolic divergence by a convex function with geometric
structure. With constraint Hyperbolic divergence, we introduce an efficient method ONGD for com-
puting optimized natural gradient in weak curvature. Naturally, forced quantization is transformed
into smooth quantization by establishing a geometric constraint in low-precision parameter space.
Third, we reveal that the essence of two-way distillation between low-precision and full-precision
models is a synchronization phenomenon, and fine-tune the low-precision model on this basis.
2 Related work
2.1 General gradient
For a convolutional neural network, the full-precision weight tensor for all layers is just marked as
w. Let M = {w ∈ Rn } be a parameter space on which a loss function L associated with weight is
well-defined. It is relatively easy to express the general gradient in training:
Vw L =
∂L
∂ W
(1)
which is the steepest descent method when M is considered Euclidean space with an orthonormal
coordinate. Note that the negative gradient represents the direction of the steepest descent inherently.
When Euclidean space is considered, Euclidean divergence between two sufficiently closed points
w and w 0 is actually defined by default:
DE[w : w0] = 2 X(W — w0)2,	(2)
which is a half of the square of the Euclidean distance identified by Euclidean metric δij , so that
|dw |2 = 2DE [w : w + dw] = X(dwi)2 = X δijdwidwj .	(3)
i,j
2
Under review as a conference paper at ICLR 2021
2.2	Natural gradient
The general gradient descent method only considers the parameter update along the gradient di-
rection that does not involve the model-level information, which may cause uneven neural network
update. By treating the parameter space M as a Riemannian manifold (Amari, 1998), rough opti-
mization is alleviated through the local curvature information distributed in the parameter space.
In that case, the steepest descent direction also depends on the quadratic form introduced by a small
incremental row vector dw that connects w and w + dw, whose form is given by
|dw|2 =	Gij (w)dwidwj,
i,j
(4)
where Gij (w) is a Riemannian metric, and dwi is the component of dw. Under the constraint
|dw|2, the steepest descent direction is toward the optimization goal of L(w + dw). Intuitively,
it measures the Kullback-Leibler (KL) divergence between two distributions ψ(w), ψ(w + dw) of
network model, which is equivalent to the interval of two adjacent points on a Riemannian manifold.
The KL divergence under the Riemannian metric is well approximated through the FIM with the
second-order Taylor expansion of the KL divergence (see Appendix 6.4) (Ba et al., 2016):
|dw|2 = 2Dkl[Ψ(w) : ψ(w + dw)] ≈ —dwEψw [V2 logψ(w)]dw> = dwFj(w)dw>.	(5)
Amari deduced that the Riemannian metric is given by the Hessian of ψ (w): Gij (W) = V2 log ψ (W)
(Amari, 1998). By the Lagrangian form, we have
VWL = FT(W)焉=-Eψw[G(w)]-11W，whereF(W) = Eψw[Vlogψ(w)Vlogψ(w)], (6)
where VWL is the natural gradient that is the steepest descent method in a Riemannian space, which
is why the natural gradient can also be called the Riemannian gradient. Note that VWL will return
to VWL when Gij (W) is equal to the Euclidean metric δij.
2.3	Network Quantization
Considering a quantized model, We notate the low-precision weight tensor using W for differentiating
full-precision weight w. During training a QNN, the forced quantizer Q(∙) is amany-to-one mapping
Q : w ∈ Rn → W ∈ Rn that can be expressed as
Wi = Q(wi),	(7)
where the weight vector Wi or Wi straightens the i-th layer of the weight tensor W or W. Quantized
models bring new challenges that have caught the disappearance or explosion of the gradients arisen
with the staircase character of quantizers during back-propagation. Fortunately, this issue can always
be solved by Straight-Through Estimator (STE) (Hinton, 2012; Courbariaux et al., 2016):
VwiL = VwiL ◦ I∣Wi∣≤1,	⑻
where ◦ is the Hadamard product, and I is the indicator function.
As the parameter space M has been assumed to be a flat and orthogonal Euclidean space, the
parameters are updated along the general gradient described as the black line in Figure 1. This
simply discretization of the neural network applied to the rounding quantizer leads us to add STE
into SGD. Except for the application of STE and quantizers, there is no difference between low-
precision and full-precision models in training.
3	The geometry of quantized models
In this section, we will describe in detail how to optimize the training of low-precision models using
geometric methods. Then we elaborate the principle and process in the subsequent.
3
Under review as a conference paper at ICLR 2021
3.1	Network quantization with natural gradient
By involving the practice to train a neural network with low bit-width, the process of the quantization
needs to be further designed that plays a vital role in the final performance, as for the other parts,
e.g. via mapping from an input pattern ^i-ι to output a can still be imitated in the same way as
full-precision neural networks:
Si = Wi^i-ι,^i = Q(fi ◦ Si),	(9)
where fi is non-linear function acted on element-wise. To distinguish the full-precision model, we
mark the notation “ ^ ” to represent the operation through a quantizer in a low-precision model,
whether for weight w or activation a.
The concept of the natural gradient is closely related to FIM and KL divergence (see Appendix 6.4).
Since KL divergence is intrinsic, the natural gradient is also intrinsic that remains unchanged under
the parameter transformation. By viewing FIM as an l by l block matrix where l denotes the number
of layers in neural network, the natural gradient formula can be introduced when training a QNN
and updating its parameters (Martens & Grosse, 2015):
qWL S=E F-I(W)∂W ◦ I∣w∣≤ι
E J∂logψ(W) ∂logψ(W)
∂Wi	∂Wj
vec( ∂L )vec
∂Wi
E [vec( ∂L )vec( ιw) )>]
.
.
.
E hvec( ∂^)vec(繇 )> i
-1
∂L
_ ʌ
∂ W
◦ I∣W∣≤1
∂L
_ ʌ
∂W
I∣W∣≤1
-1
(10)
◦ I∣w∣≤ι,
E
where vec( ∂∂L-) Can be represented by the gradient error ∂∂L 1. Considering that the gradient prop-
agation needs to span over the quantized neurons and graphs, we still need to use STE (see Ap-
pendix 6.1) to update the gradient of QNNs in the learning procedure:
∂L >
vec(西 )= %τ 0
◦ f0(Si) s=e^3 ㊈
◦ I∣ai∣≤1 ◦ fi(Si) ,	(11)
where 0 denotes the Kronecker product1 2. Relying on the KL divergence, We indicate the parameter
space as Riemannian manifold rather than Euclidean space in the quantization procedure.
3.2	Weak curvature condition
The reason why we can use Euclidean coordinates to calculate the natural gradient on the manifold
is the local homeomorphism of a manifold based on Definition 1 (Wald, 2010). Note that the
homeomorphic mapping φU satisfies U ∈ M 7→ φU(U) ∈ Rn. For any point x ∈ U, we can define
φU (x) as the Euclidean coordinate absolutely. The natural gradient calculated here is obviously the
result of the homeomorphic mapping.
Definition 1 Let M be an n-dimensional manifold, for any point x ∈ M, and then it satisfies that
there exists a neighborhood U ofx in M which is homeomorphic to an open set in an n-dimensional
Euclidean space Rn.
The natural gradient poses a significant challenge for computing. Intuitively, the computing Fi-1 (W)
attached to the natural gradient will take on massive computation, which can only be numerically es-
timated. Mostly, this inversion is entirely unrealistic, when deep neural networks are very redundant,
with tens of thousands of the neural connections.
1Note that We use W or W to denote a large matrix of l × n that is composed by each layer of weight tensor
w or W as a row vector, where n is the largest size of rows and other insufficient rows are filled with zero. The
weight vector Wi or Wi is obtained by straightening i-th layer of the weight tensor W or W into a row vector.
The weight vector W or W is obtained by straightening the weight tensor W or W into a row vector. The operator
vec(∙) means to straighten a matrix or tensor into a row vector.
2Since the input ^>-ι and output ∂∂L sizes of each layer may be different, Kronecker product is necessary.
4
Under review as a conference paper at ICLR 2021
In this paper, we consider the optimized natural gradient under the condition of weak curvature
in low-precision models. This means that the Riemannian manifold is nearly flat, where the Rie-
mannian metric is an approximation of the Euclidean metric. In practice, we develop a linearized
Riemannian metric from the Euclidean metric δij , which is systematically defined in general rela-
tivity (Wald, 2010):
Gij = δij + ij ,	(12)
where the metric ij is smaller than 1 in global Euclidean coordinate system of δij . It is an adequate
definition of “weakness” in this context and ensures to be a positive-definite metric. Combining
eq. 6, FIM can be linearly decomposed in weak curvature 3:
Fij(W) = -Eψw[δij + €ij] = -δij - Eψwkij(diag(vec(W)))].	(13)
Since the complexity of FIM is determined by KL divergence in arbitrary curvature, we urgently
need to develop a divergence defined in weak curvature to help simplify the calculation.
3.3	Optimized natural gradient in Hyperbolic divergence
In practice, We can determine a unique geodesic through exponential map expw (W — W) that maps
W - W back to M, by defining W - W ∈ TwM as the tangent vector. Note that the definition of
exponential map is developed by (Wald, 2010; Helgason, 2001).
Definition 2 Let M be a Riemannian manifold, for the tangent vector v ∈ TxM in a point x ∈ M
where TxM is the tangent space, there is a unique geodesic γv (t) locally that satisfies γv (0) = x
and γv0 (0) = v. The exponential map expx : TxM 7→ M corresponding to γv (t) is defined as
expx (v) = γv (1). When constraining expx to a neighbourhood U, this mapping is one-to-one.
The exponential map expw(W - W) is to map a tangent vector (W - W) in the tangent bundle to the
point where the arc length from point W is equal to |w - W| on geodesic with the initial condition
(w, w — W). In order to make exp(W — W) and exp(W — w) have the same effect in the training
process, we symmetrize it and derive a convex function
ψ(w - W)= log ]exp(w - W) + exP(W -W) 1 = log(cosh(w - W)).	(14)
Geometrically, the convex function with geometric structure is introduced into Bregman divergence
(see Appendix 6.3) (Bregman, 1967), and we obtain a novel Hyperbolic divergence that satisfies the
criteria of divergence (see Appendix 6.2).
Definition 3 For a convex function ψ defined by eq. 14 in a weak curvature space, the Hyperbolic
divergence between w0 - W and W - W is
DH[w0 — W : w — W] = log c。Sh了-----— (w0 — w) tanh(w — W).	(15)
cosh(w — W)
Let dW → 0. The Taylor expansion of Hyperbolic divergence is (see Appendix 6.5)
|dw|2 = 2D H [w — W : w + dw — W] ≈ dw diag (vec(1 — tanh2(w - W))) dw>.	(16)
Comparing eq. 4 and eq. 12, we can deduce the metric Eij = - diag (vec(tanh2(w - W)))j. Now
we can deduce the steepest descent direction while taking into account the weak curvature in Rie-
mannian manifold defined by Hyperbolic divergence. With constraint Hyperbolic divergence in a
constant c, we do the minimization of the loss function L(W) in Lagrangian form:
dw* =	arg min	L(w + dw)
s.t.DH [w—w:w+dw—w] = c
=arg min L(w + dw) — ɪ (DH [w — W : W + dw — W] — c)
dw	λ
≈ arg minL(w) + Vec(^^)dw>---d-dwdiag (vec(1 — tanh2(w — W))) dw> + c.
dw	∂W	2λ	λ
(17)
3The operator diag(∙) means to convert a row vector to a diagonal matrix.
5
Under review as a conference paper at ICLR 2021
To solve the above minimization, we set its derivative with respect to dw to zero:
0 = vec(lτ^^)> - TH- diag (vec(1 - tanh2(W - W))) dw>
∂w	2λ
1	∂L
—diag (vec(1 - tanh2(w - W))) dw> = vec(-)>	(18)
∂L
dw> ≈ λdiag (vec(1 + tanh2(w — W))) Vec(^-)>.
∂W
Where a constant factor λ can be absorbed into learning rate. This is the optimized natural gra-
dient defined by Hyperbolic divergence, which is only suitable for weak curvature manifolds. By
constructing a geometric structure with weak curvature, this natural gradient is consistent with the
derived eq. 13. Although it has certain limitations compared with the natural gradient defined by KL
divergence, it is quite computationally friendly and maks full use of the inherent geometric structure
of the low-precision model.
3.4	Smooth quantization along the nearly flat manifold
For the process of the quantization, we design the smooth quantization Q based on the proposed
Hyperbolic divergence in weak curvature:
s
Q(Wi, Wi) = Wi - 2 tanh(wi - Wi),
(19)
where a constant factor s is used to switch between training and test states (s = 1 corresponds to
the training process, and s = 0 corresponds to the test process). Note that keeping s at zero during
training will degrade ONGD to SGD. By limiting the value range of weights to [-1, 1] and applying
STE, we can obtain the optimized natural gradient through back-propagation:
__ ≈, ..
r∖>	/ dL	aQ(wi,wi)∖>
vec( Vw. L) = vec( —---------◦---------)
‘wi'	'∂Q(wi,Wi)	∂wi	,
S=E Vec( CJL ..、)> ◦ I∣Wi∣≤1 - Idiag (VeC(I - tanh2(wi - Wi))) vec( CJL ..、)>
∂Q(wi,Wi)	2	∂Q(Wi,Wi)
=2 diag (VeC(I+ tanh2(Wi - Wi))) VeC(VQ(wi,Wi)L)>.
(20)
Intuitively, we utilize Vwi L to calculate the optimized natural gradient of the loss function with
respect to Q(w%, Wi), by transforming the parameter space from Wi to Q(w%, w£.
3.5	Synchronization in progressive training
Given the smooth quantization, it is necessary to state that the form of quantization function is un-
constrained. For weights quantized to k-bit, we can design the uniform quantization, non-uniform
power quantization or any other quantizations (see Appendix 6.6). Note that the uniform quanti-
zation keeps convolutional calculation into low-precision fixed-point. And the quantization con-
strained to the power of 2 replaces multiplication with shift operation, which can make computation
extremely efficient. Of course, we take the same measure to quantize activations, whether it is the
quantization function or the smooth method.
According to the model of input-output relations of neural networks, the training of a low-precision
model is regarded as the process of approximating the output probability distribution of the full-
precision model. To link these two models between full-precision and low-precision, we add the KL
divergence into the loss function. The complete training with the distillation can be carried out in
two stages 4. In the first stage, we train the low-precision model based on ONGD-D with the loss
function:
L(Q(Wi, Wi)) = αCE(yQ(Wi,Wi),ytrue) + (1-a)DKL[yQ(Wi,Wi). yw0],	QI)
4 Note ONGD-D represents that the optimized natural gradient and distillation are used in the training, and
ONGD represents that the optimized natural gradient is only used in the training, i.e. without the information
of the full-precision model.
6
Under review as a conference paper at ICLR 2021
Table 1: The classification accuracy results on CIFAR10 and CIFAR100 with ResNet-20, ResNet-32
and VGG13-small. Note the accuracy of full-precision baseline is reported by (Chen et al., 2020).
Method	W	A	CIFAR10		CIFAR100	
					Top-1	Gap
			Top-1	Gap		
ResNet-20 (Original)	32	32	92.25%	-	68.14%	-
ONGD (ours)	4	4	89.41%	-2.84%	63.30%	-4.84%
SGD	4	4	89.27%	-2.98%	62.38%	-5.76%
ONGD-D (ours)	1	8	91.16%	-1.09%	65.65%	-2.49%
PQ-B (Bai et al., 2018)	1	32	90.65%	-1.60%	-	-
ResNet-32 (Original)	32	32	93.29%	-	69.74%	-
ONGD (ours)	4	4	91.70%	-1.59%	66.18%	-3.56%
SGD	4	4	91.23%	-2.06%	64.13%	-5.61%
ONGD-D (ours)	1	8	92.36%	-0.93%	67.07%	-2.67%
PQ-B (Bai et al., 2018)	1	32	91.47%	-1.82%	-	-
VGG13-small (Original)	32	32	93.17%	-	72.06%	-
ONGD (ours)	4	4	92.69%	-0.47%	71.19%	-0.87%
SGD	4	4	92.43%	-0.74%	70.44%	-1.62%
where α is a balancing parameter that also serves as a smooth label. Specifically, CE(∙) is the
cross-entropy between the softmax output of low-precision models Q(wi, Wi) and true hard label,
and DKL H is the KL divergence of the softmax output between low-precision Q(Wi, Wi) and full-
precision wi0 models. In the next second stage, following with the works (Hinton et al., 2015; Zhuang
et al., 2018), we consider two-way knowledge distillation for low-precision and full-precision mod-
els in progressive training. As such, we continue to fine-tune the low-precision model with eq.21,
and update the full-precision model with eq.22, alternating between the two pieces of training,
L(Wi) = α CE(ywi ,ytrue) + (I- α)DκL[yw0 ,yQ(wi:Wi)].	(22)
Since we have built a bridge between the two models, this measure to improve the accuracy of the
low-precision model can be attributed to a physical phenomenon called synchronization. The two
pendulum clocks placed on the same wooden board will have the same frequency θ1 and phase θ2
after a while, whether their previous states θ = (θ1, θ2) are consistent or not. Here, the essence of
two-way distillation is also a synchronization phenomenon, which will synchronize the states θ0 =
(θ1 ,θ2,... ,O*) ofthe two models (namely the classification probability of the network output). The
KL divergence between the two models acts as the wooden board that transmits the kinetic energy
of two pendulum clocks to each other. As a result, we have obtained a low-precision model with
improved performance.
4	Experiment
In this section, we implement experiments to demonstrate the effectiveness of our proposed meth-
ods on benchmark datasets that are CIFAR and ImageNet mainly here. Intuitively, experiments on
CIFAR and ImageNet are the ablation study to validate the advantages of ONGD compared to SGD.
Comparisons with other quantizers on quantitative indicates will be carried out on ImageNet. All
experiments are conducted with TensorFlow (Abadi et al., 2016) and Keras (Chollet et al., 2015).
4.1	Experiments on CIFAR
In order to illustrate the superiority of the optimized natural gradient, we train QNNs with our
method ONGD on the CIFAR dataset from scratch. We choose 1-bit, 4-bit or 8-bit quantization
with eq.19 and eq.38 to test the ResNet-20 (He et al., 2016) model and VGG13-small (Simonyan &
Zisserman, 2014) model with standard data augmentation and pro-precessing. Note thats VGG13-
small is the structure of VGG13 that does not contain all fully-connected layers. We use a weight
7
Under review as a conference paper at ICLR 2021
(a) ResNet-20 on CIFAR10
(b) ResNet-20 on CIFAR100
Figure 2: Training and test curves of ResNet-20 compared between ONGD and SGD.
decay of 1e-4, a batch size of 128 and nesterov momentum of 0.9. The learning strategy is lowered
by 10 times at epoch 80, 140 and 200, with the initial 0.1. The accuracy results are shown in Table 1,
where the most significant results show that the 4-bit quantized ResNet-32 trained using ONGD has
better results than using SGD, i.e. 2.05% higher on CIFAR100 dataset.
By observing the curves in Figure 2, we see that our method ONGD can make the test error lower
when its training loss is much larger than SGD and ADAM (ADAM with a learning rate of 1e-4).
Under the same condition, including quantization function, quantized layers, etc., we evaluate the
training error of ResNet-20 model using ONGD that increases much more than using SGD (i.e.
1.49% in CIFAR10 and 4.86% in CIFAR100). However, the test error of ResNet-20 model with
ONGD can be lower compared to SGD (i.e. 0.14% in CIFAR10 and 0.92% in CIFAR100).
(a) VGG13-small on CIFAR10
(b) VGG13-small on CIFAR100
Figure 3: Training and test curves of VGG13-small compared between ONGD and SGD.
Similar to the above experiment, we test our method in Figure 3 using VGG13-small model that
is more redundant than ResNet model (The learning strategy of ADAM is lowered by 10 times at
epoch 80 and 180, with the initial 1e-4). The graph shows that the performance of ONGD in training
continues to be worse, but its test error is lower than SGD (i.e. 0.26% in CIFAR10 and 0.75% in
CIFAR100).
4.2	Experiments on ImageNet
We next conduct experiments on the more challenging large ImageNet dataset (Krizhevsky et al.,
2012). For training, we use standard data augmentation, i.e., resize the images to 256x256 and then
randomly crop them to 224x224, and apply random horizontal flips. We use the 224x224 crop of
the images on evaluation. In weights or activations, the quantizer applies eq.19 and eq.38 for 1-bit
or 8-bit, and we use eq.19 and eq.39 for 4-bit. We show the accuracy results in Table 2.
For ImageNet, the quantized models are trained from the guidance of a pre-trained full-precision
model with a weight decay of 1e-4 and nesterov momentum of 0.9 followed with (Elhoushi et al.,
2019). The learning strategy is lowered by 10 times every 15 epochs, with the initial 0.1. As a re-
sult, the best accuracy for ResNet-50 with 8-bit quantization is less than the full-precision model by
8
Under review as a conference paper at ICLR 2021
Table 2: The classification accuracy results on ImageNet and comparison with other quantizers for
fair, with AlexNet, ResNet-18, ResNet-50 and MobileNet. Note that the accuracy of full-precision
baseline is reported by (Elhoushi et al., 2019).
Method	W	A	Top-1		Top-5	
			Accuracy	Gap	Accuracy	Gap
AlexNet (Original)	32	32	56.52%	-	79.07%	-
ONGD-D-small (ours)	4	4	55.89%	-0.63%	78.58%	-0.49%
DeepShift-Q (Elhoushi et al., 2019)	6	32	54.97%	-1.55%	78.26%	-0.81%
ResNet-18 (Original)	32	32	69.76%	-	89.08%	-
Bi-Real (Liu et al., 2018)	1	1	56.40%	-13.36%	79.50%	-9.58%
ONGD-D-small (ours)	1	1	58.63%	-11.13%	81.44%	-7.64%
QN (Yang et al., 2019)	1	2	63.40%	-6.36%	84.90%	-4.18%
ONGD-D-small (ours)	1	8	64.05%	-5.71%	85.09%	-3.99%
MetaQuant (Chen et al., 2019)	1	32	60.32%	-9.44%	83.02%	-6.06%
ONGD-D-small (ours)	4	4	67.55%	-2.21%	87.32%	-1.76%
RQ ST (Louizos et al., 2019)	4	4	62.46%	-7.30%	84.78%	-4.30%
ResNet-50 (Original)	32	32	76.13%	-	92.86%	-
ONGD-D-small (ours)	4	4	73.51%	-2.62%	90.76%	-2.10%
LQ-Nets (Zhang et al., 2018)	4	4	75.10%	-1.03%	92.40%	-0.46%
ABC-Net (Lin et al., 2017)	5	5	70.10%	-6.03%	89.70%	-3.16%
ONGD-D-large (ours)	8	8	76.10%	-0.03%	92.88%	+0.02%
INT8 (Zhu et al., 2020)	8	8	75.87%	-0.26%	-	-
MobileNet (Original)	32	32	70.61%	-	89.47%	-
ONGD-D-small (ours)	4	4	61.32%	-9.31%	82.35%	-7.12%
RQ (Louizos et al., 2019)	5	5	61.38%	-9.23%	83.73%	-5.74%
SR+DR (Gysel et al., 2018)	5	5	59.39%	-11.22%	82.35%	-7.12%
RQ ST (Louizos et al., 2019)	5	5	56.85%	-13.76%	80.35%	-9.12%
ONGD-D-large (ours)	8	8	70.86%	+0.25%	89.60%	+0.13%
RQ (Louizos et al., 2019)	8	8	70.43%	-0.18%	89.42%	-0.05%
0.03% in Top-1 accuracy. For MobileNet (Howard et al., 2017), the accuracy of applying ONGD-D
is even slightly higher than the original model about 0.25% in Top-1 accuracy. As for 4-bit quanti-
zation, our performance is also better than some other quantizers under the same conditions. Note
that more ablation studies about the optimized natural gradient and distillation are in Appendix 6.7.
5	Conclusion
By defining a convex function with a geometric structure, we get a novel Hyperbolic divergence that
helps us calculate the optimized natural gradient in weak curvature, i.e. differenting from the natural
gradient in arbitrary curvature under the definition of KL divergence. Coupled with the two-way
distillation between low-precision and full-precision models, we demonstrate that the QNN trained
with ONGD can achieve state-of-the-art classification results on ImageNet.
The development of QNNs began with the study of ultra low-precision, e.g. BNN (Courbariaux
et al., 2016), BC (Courbariaux et al., 2015), TWN (Li et al., 2016) and TTQ (Zhu et al., 2016),
which replace multiplication with XNOR operation. It was later developed into DeepShift (Elhoushi
et al., 2019) that replaces multiplication with SHIFT operation. Recently, there are works to modify
the network structure (Bethge et al., 2020) and activation function (Choi et al., 2018b;a), which can
be said to include all aspects of neural networks. Future work may need to pay more attention to the
characteristics of the quantization itself and use mathematical tools to study it.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorflow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251—
276, 1998.
Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal
operators. arXiv preprint arXiv:1810.00861, 2018.
Michael R Bastian, Jacob H Gunther, and Todd K Moon. A simplified natural gradient learning
algorithm. Advances in Artificial Neural Systems, 2011, 2011.
Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, and Christoph Meinel. Meliusnet: Can
binary neural networks achieve mobilenet-level accuracy? arXiv preprint arXiv:2001.05936,
2020.
Lev M Bregman. The relaxation method of finding the common point of convex sets and its applica-
tion to the solution of problems in convex programming. USSR computational mathematics and
mathematical physics, 7(3):200-217, 1967.
Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. Addernet:
Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 1468-1477, 2020.
J. Chen, Y. Liu, H. Zhang, S. Hou, and J. Yang. Propagating asymptotic-estimated gradients for low
bitwidth quantized neural networks. IEEE Journal of Selected Topics in Signal Processing, 14(4):
848-859, 2020.
Shangyu Chen, Wenya Wang, and Sinno Jialin Pan. Metaquant: Learning to quantize by learning to
penetrate non-differentiable quantization. In Advances in Neural Information Processing Systems,
volume 32, pp. 3916-3926. Curran Associates, Inc., 2019.
Jungwook Choi, Pierce I-Jen Chuang, Zhuo Wang, Swagath Venkataramani, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Bridging the accuracy gap for 2-bit quantized neural net-
works (qnn). arXiv preprint arXiv:1807.06964, 2018a.
Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srini-
vasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085, 2018b.
Francois Chollet et al. keras, 2015.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
systems, pp. 3123-3131, 2015.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized
neural networks: Training deep neural networks with weights and activations constrained to+ 1
or-1. arXiv preprint arXiv:1602.02830, 2016.
Mostafa Elhoushi, Zihao Chen, Farhan Shafiq, Ye Henry Tian, and Joey Yiwei Li. Deepshift: To-
wards multiplication-less neural networks. arXiv preprint arXiv:1905.13298, 2019.
Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for
empirical study of resource-efficient inference in convolutional neural networks. IEEE transac-
tions on neural networks and learning systems, 29(11):5784-5789, 2018.
10
Under review as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sigurdur Helgason. Differential geometry and symmetric spaces, volume 341. American Mathe-
matical Soc., 2001.
G Hinton. Neural networks for machine learning. coursera,[video lectures], 2012.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. arXiv preprint arXiv:1605.04711,
2016.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In
Advances in Neural Information Processing Systems, pp. 345-353, 2017.
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net:
Enhancing the performance of 1-bit cnns with improved representational capability and advanced
training algorithm. In Proceedings of the European Conference on Computer Vision (ECCV),
September 2018.
Christos Louizos, Matthias Reisser, Tijmen Blankevoort, Efstratios Gavves, and Max Welling. Re-
laxed quantization for discretized neural networks. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=HkxjYoCqKX.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of dnns with natural gradi-
ent and parameter averaging. arXiv preprint arXiv:1410.7455, 2014.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet
classification using binary convolutional neural networks. In European conference on computer
vision, pp. 525-542. Springer, 2016.
Nicolas L Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gra-
dient algorithm. In Advances in neural information processing systems, pp. 849-856, 2008.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Robert M Wald. General relativity. University of Chicago press, 2010.
Jiwei Yang, Xu Shen, Jun Xing, Xinmei Tian, Houqiang Li, Bing Deng, Jianqiang Huang, and Xian-
sheng Hua. Quantization networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2019.
Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and Gang Hua. Lq-nets: Learned quantization for
highly accurate and compact deep neural networks. In Proceedings of the European conference
on computer vision (ECCV), pp. 365-382, 2018.
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantiza-
tion: Towards lossless cnns with low-precision weights. arXiv preprint arXiv:1702.03044, 2017.
11
Under review as a conference paper at ICLR 2021
Chenzhuo Zhu, Song Han, Huizi Mao, and William J Dally. Trained ternary quantization. arXiv
preprint arXiv:1612.01064, 2016.
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and
Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1969-1979, 2020.
Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, and Ian Reid. Towards effective low-
bitwidth convolutional neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 7920-7928, 2018.
6 Appendix
6.1	Straight through estimator in n-bit models
Let Us define L = L(ai, e£ where ^i follows eq.9 that is chosen from (±2-0, ±2-1,…,0), then
we get a new expression as
Eei p— = c~^~, if |si| ≤ 1.	(23)
∂si	∂ai
Where e% is the noise source that influences si, EeiH means the expectation over s” and C is a
constant. We have
Ee
ei
.∂bL
∂
—Ee∙[L]
∂si	eiL」
∂∣7[XL(^i = +2j)P(Si > ei∣Si) + XL(^i = -2j)(1 - P(Si > e∕si))]
si i	j
(24)
竺生力XL(^i = +2j) - XL(^i = -2j)].
∂si
For L(ai = ±2j), We can approximate it using the Taylor expansion:
L(^i =	+2j))	=L(ai = 0) + ∂ a，i	i	∂2L ai=o2 + 碇	22j + O ^i=0	∂3LL /a3	23j ai = 0
L(^i =	-2j))	=L(ai = 0)-万丁 ∂ a，i	2j + ∂2L ai = 0	此	22j + O ai=0	∂ ∂3L	23j ^i = 0
(25)
For d(Sd>ei|Si), we split it into two parts:
∂P(Si > ei∣Si) _ ∂P(Si > ei∣Si) + ∂P(si > e/sj
∂si
∂si
|Si|>1
∂si
∣si∣≤ι
d R-ι 1 dej + d R-Si 1 dei
∂Si
(26)
∂si
1∣Si∣≤ι.
Combining eq.25 and eq.26, the eq.24 can be derived as
Eei
I∣Si∣≤ι ◦
2 X 22j ∂L
∂ai	∂&i
(27)
^i=0
Let 2 Pj 22j = c, then
Ee
ei
∂L
∂Si
∂L
C西◦ Ilsil≤1.
(28)
12
Under review as a conference paper at ICLR 2021
6.2	Definition of divergence in a manifold
D [P : Q] is called a divergence when it satisfies the following criteria:
1)	D[P : Q] ≥0.
2)	D[P : Q] = 0 when and only when P = Q.
3)	When P and Q are sufficiently close, by denoting their coordinates by ψP and ψQ = ψP + dψ,
the Taylor expansion of D is written as
D[ψp ： Ψp + dψ] = 2 X Gij(Ψp)dψidψj + O(∣dψ∣3),	(29)
i,j
and Riemannian metric Gij is positive-definite, depending on ψP.
6.3	Bregman divergence
Bregman divergence DB [w : w0] is defined as the difference between a convex function ψ(w) and
its tangent hyperplane Z = ψ(w0) + (w - w0)Vψ(w0), depending on the Taylor expansion at the
point w0 :
DB[w : w0] = ψ(w) - ψ(w0) - (w - w0)Vψ(w0).	(30)
6.4	KL divergence and Fisher Information Matrix
KL divergence can be defined between ψ(χ∣w) and ψ(x∣w0):
DKL[ψ(XIw): ψ(xlw0)]=/ψ(XIw) log ψ(xlw)dx - /ψ(XIw) logψ(xlw0)dx.	(31)
The first derivative is:
Vw0DKL[ψ(XIw): ψ(XIw0)] =	ψ(XIw)Vw0 log ψ(XIw)dX -	ψ(XIw)Vw0 log ψ(XIw0)dX
= -Zψ(XIw)Vw0logψ(XIw0)dX.
(32)
The second derivative is:
VWoDkl[Ψ(x∣w) : ψ(x∣w0)] = / ψ(x∣w)VWo log ψ(x∣w)dx - / ψ(x∣w)VW, logψ(x∣w0)dx
= -∕ψ3 W)VwOlog ψ(χlw0)dχ.
(33)
We deduce the Taylor expansion of KL divergence at w = w0 :
DKL[ψ(XIw) : ψ(XIw0)] ≈ DKL [ψ (XIw) : ψ(XIw)]-
(∕ψ(XIw)Vw0log ψ(χlw)lw=w0 Qdw-1 dw Uag ψ(χlw0)lw=wo dx)bw>
ψ(XIw)∑ψ⅛) dx) dw> — ɪdw (/ ψ(x∣w)V
Vψ (XIw)
ψ(XIw)
dx dw>
-(V ∕ψ(xlw)dχ bw>- 2 dw (∕ψ3w)
V2ψ(x∣w)ψ(x∣w) — Vψ(x∣w)Vψ(x∣w)
-(V1)dw> — ɪdw
ψ(XIw)dX -	ψ(XIw)
ψ(x∣w)ψ(x∣w)
Vψ(x∣w)] ΓVψ(x∣w)
dx dw>
Ψ(χ∣w) _l L ψ(χ∣w)
dw>
gdwEψw [V log ψ(w)V log ψ(w)]dw> = ^dwFij dw>.
(34)
The Taylor expansion of KL divergence at w = w0 is related to Fisher Information Matrix.
13
Under review as a conference paper at ICLR 2021
Table 3: The ablation studies on ImageNet, with AlexNet, ResNet-18 and ResNet-50. Note that the
accuracy of full-precision baseline is reported by (Elhoushi et al., 2019).
Method	W	A	Top-1		Top-5	
			Accuracy	Gap	Accuracy	Gap
AlexNet (Original)	32	32	56.52%	-	79.07%	-
ONGD-D (ours)	4	4	55.89%	-0.63%	78.58%	-0.49%
ONGD (ours)	4	4	54.95%	-1.57%	78.38%	-0.69%
SGD	4	4	53.84%	-2.68%	77.57%	-1.5%
ResNet-18 (Original)	32	32	69.76%	-	89.08%	-
ONGD-D (ours)	4	4	67.55%	-2.21%	87.32%	-1.76%
ONGD (ours)	4	4	66.42%	-3.34%	86.20%	-2.88%
SGD	4	4	64.64%	-5.12%	85.10%	-3.98%
ResNet-50 (Original)	32	32	76.13%	-	92.86%	-
ONGD-D (ours)	4	4	73.51%	-2.62%	90.76%	-2.10%
ONGD (ours)	4	4	71.80%	-4.33%	90.45%	-2.41%
SGD	4	4	68.99%	-7.14%	89.39%	-3.47%
6.5	Hyperbolic divergence
The first derivative of Hyperbolic divergence is:
▽w0 DH [w0 — W ： W — W] = VwO logcosh(w0 — W) — 0 — Vwo (w0 — W) tanh(w — W)
=tanh(W0 — W) — tanh(W — W).
The second derivative of Hyperbolic divergence is:
VWo DH [w0 — W : W — W] = VWo logcosh(W0 — W) — 0 — SVwO (w0 — W) tanh(W — W)
=1 — tanh2(W0 — W).
We deduce the Taylor expansion of Hyperbolic divergence at W = W0 :
DH [w0 — W : W — W] ≈ DH [w — W : W — W] +
Vec (tanh(w0 — W)|w=w『一tanh(w — W)) dw> +
ɪdwdiag (vec(1 — tanh2(w0 — W))) |w=w『dw>
= dww diag (vec(1 — tanh2(w — W))) dw>.
6.6	Quantization function
The uniform quantization:
Wi = max(∣Wi∣) -kτ1--round Γ(2k-1 — 1)---------w-).
2k-1 — 1	max(|Wi|)
The non-uniform power quantization:
Wi = max(|Wi|) Sign(Wi)2^ round ∣ clip(log2(——lwil	+ e), —2k-1 + 1,0)
2 max(|Wi|)
(35)
(36)
(37)
(38)
(39)
where is a small constant to prevent the log function from exploding. In a convolutional layer, the
full-precision convolutional operation can be converted into the low-precision fixed-point convolu-
tional operator and multiplied by a factor max(|Wi|) max(|ai|) 5:
5The operator Conv(∙, ∙) means the 2D convolutional operation.
14
Under review as a conference paper at ICLR 2021
Conv(wi, ai) ≈ Conv
max(IwiI)maw⅛，max(IaiI)maa⅛
max(IwiI) max(IaiI) Conv
wi	&i
max(IwiI) , max(IaiI)
(40)
6.7 The ablation study on ImageNet
The ablation studies on ImageNet are shown in Table 3. We train QNNs with our method on the
ImageNet dataset from scratch, with a weight decay of 1e-4 and nesterov momentum of 0.9. The
learning strategy is lowered by 10 times every 15 epochs, with the initial 0.1 and a batch size of 128.
During training and inference, we strictly ensure that quantization function and other conditions are
the same. We use the initialization of Xavier, and quantize the weights and activations of all layers
except the first layer where the low-precision model is trained from scratch.
15