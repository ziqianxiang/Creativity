Under review as a conference paper at ICLR 2021
Efficient Estimators for
Heavy-Tailed Machine Learning
Anonymous authors
Paper under double-blind review
Ab stract
A dramatic improvement in data collection technologies has aided in procuring
massive amounts of unstructured and heterogeneous datasets. This has conse-
quently led to a prevalence of heavy-tailed distributions across a broad range of
tasks in machine learning. In this work, we perform thorough empirical stud-
ies to show that modern machine learning models such as generative adversarial
networks and invertibleflow models are plagued with such ill-behaved distribu-
tions during the phase of training them. To alleviate this problem, we develop
a computationally-efficient estimator for mean estimation with provable guaran-
tees which can handle such ill-behaved distributions. We provide specific conse-
quences of our theory for supervised learning tasks such as linear regression and
generalized linear models. Furthermore, we study the performance of our algo-
rithm on synthetic tasks and real-world experiments and show that our methods
convincingly outperform a variety of practical baselines.
1 Introduction
Existing estimators in machine learning are largely designed for “thin-tailed” data, such as those
coming from a Gaussian distribution. Past work in statistical estimation has given sufficient evidence
that in the absence of these “thin-tails”, classical estimators based on minimizing the empirical error
perform poorly (Catoni, 2012; Lugosi et al., 2019). Theoretical guarantees for methods commonly
used in machine learning usually place assumptions on the tails of the underlying distributions that
are analyzed. For instance, rates of convergences proven for a variety of stochastic optimization
procedures assume that the distribution of gradients have bounded variance (for e.g., Zou et al.
(2018)) or in some cases are sub-Gaussian (for e.g., Li & Orabona (2019)). Thus, these guarantees
are no longer applicable for heavy-tailed gradient distributions.
From a practical point of view however, this is a less than desirable state of affairs: heavy-tailed
distributions are ubiquitous in a variety offields including large scale biological datasets andfinan-
cial datasets among others (Fan et al., 2016; Zhou et al., 2017; Fan et al., 2017). While this may
be argued as just artifacts of the domain, recent work has found interesting evidence of heavy-tailed
distributions in the intermediate outputs of machine learning algorithms. Specifically, recent work
by Simsekli et al. (2019) and Zhang et al. (2019) have provided empirical evidence about the ex-
istence of such heavy-tailed distributions, especially during neural network training for supervised
learning tasks.
Following these empirical analyses of Simsekli et al. (2019) and Zhang et al. (2019), we look for
sources of heavy-tailed gradients arising during the training of modern generative model based unsu-
pervised learning tasks as well. In our preliminary investigation, we noticed that the distribution of
gradient norms i.e.,g t2 are indeed heavy-tailed. These are showcased in Figure 1; Figures 1a and
1b show the distribution of gradient norms obtained while training the generator of aDCGAN(Rad-
ford et al., 2015) andReal-NVP(Dinh et al., 2016) on the CIFAR-10 dataset, respectively. These
distributions are noticeably heavy-tailed, especially when juxtaposed with those obtained from a
Gaussian distribution (Figure 1c). We discuss more about the empirical setup in Section 5.2.
Interestingly, in all the supervised and unsupervised machine learning problems discussed above, we
merely need to compute expectations of these varied random heavy-tailed quantities. For instance,
mini-batch gradient descent involves aggregating a batch of gradients pertaining to each sample in
1
Under review as a conference paper at ICLR 2021
(a)DCGAN	(b)Real-NVP	(c) Gaussian	(d)α-stable
Figure 1: Distribution of sampled gradient norms while trainingDCGAN(a) andReal-NVP(b) on
the CIFAR-10 dataset. (c) Distribution of norms of Gaussian random vectors and (d) Distribution of
norms ofα-stable random vectors withα= 1.95. X-axis: norm, Y-axis: Density.
the mini-batch. Typically, this aggregation is performed by considering the sample mean, and this is
a reasonable choice due to its simplicity as an estimate of the expectation of the random gradient.
For computing the mean of such heavy-tailed gradient distributions, the sample mean however is
highly sub-optimal. This is because sample mean estimates are greatly skewed by samples on the
tail. Thus gradient estimates using these sub-optimal sample means of gradients do not necessarily
point in the right direction leading to bad solutions, prolonged training time, or a mixture of both.
Thus, a critical requirement for training of modern machine learning models is a scalable estimation
of the mean of a heavy-tailed random vector. Note that such computations of mean of sample
gradients are done in each iteration of (stochastic) gradient descent, so that we require that the
heavy-tailed mean estimation be extremely scalable, yet with strong guarantees. Note that once we
have such a scalable heavy-tailed mean estimator, we could simply use it to compute robust gradient
estimates Prasad et al. (2020) , and learn generic statistical models.
We summarize our contributions as follows:
•	We extend recent analyses of heavy-tailed behavior in machine learning, and provide novel em-
pirical evidence of heavy-tailed gradients while training modern generative models such as gen-
erative adversarial networks (GANs) and invertibleflow models.
•	To combat the issue of aggregating gradient samples from a heavy-tailed distribution, we propose
a practical and easy-to-implement algorithm for heavy-tailed mean estimation with provable
guarantees on the error of the estimate.
•	We use the proposed mean estimator to compute robust gradient estimates, which allows us
to learn generalized linear models in the heavy-tailed setting, with strong guarantees on the
estimation errors.
•	Finally, we propose a heuristic approximation of the mean estimation algorithm, which scales to
random vectors with millions of variables. Accordingly, we use this heuristic to compute robust
gradients of large-scale deep learning models with millions of parameters. We show that training
with this heuristic outperforms a variety of practical baselines.
Notation and other	definitions.	LeberbenrOmdom	VeCtOtWithmean	∕e. We	Say	thaX	hhe x has
bounded 2k--moments iffor all ∈ S SpT(Unitball), E[((oT(x — /)))2k] ≤ O2k (E[(vT(e — )))2])k.
ThrVughVut the paper, we ueec, c 1, c2 , . . . , C, C1, C2 , . . .tV denVte pVeitioe unioereal cVnetante.
2	Efficient and Practical Mean Estimation
We begin by iorializing the notion oi heaoy-tailed dietributione.
Definition 1(Heaoy-Tailed ietribution (Reenick, 2007)).A non-negative random variableXis
called heavy-tailed if the tail probability P(X > )) isαSymPtOcaCaUPPrOPoronneIt to t-α , where
α* is a positive constant called the tail index Pf X.
Intuitioely, thie definition etatee that ii the tail oi the dietributionP(X > t)decreaeee at a rate
elower thate -t, then the dietribution ie heaoy-tailed. An intereeting coneequence oi thie definition
ie the non-exietence oi higher order ioiente. Specifically, one can ehow that the quantityE[X α ]
is finnite OOr any i ifandonIy if α < q* and XiShheaVyaiaiIed random VaaiaeIwWithiaiIindeX
α*. In recent statistical estimation literature (for e.g., Mineker (2015); HOPkine (2018); LugOei
& MendelsOn (2019)), heaoy-tailed distributiOns are defined by the absence Oifinite higher Order
iOients.
2
Under review as a conference paper at ICLR 2021
In the heavy-tailed mean estimation task, we observensamplesx 1, . . . , xn drawn independently
from a distributionPwherex i ∈R p, which is only assumed to havefinite low-order moments,
therefore heavy-tailed. The goal of past work (Catoni, 2012; Minsker, 2015; Lugosi et al., 2019;
Catoni & Giulini, 2017) has been to design an estimator θn of the true mean MOfP Chiha has a
small 2 -error with high-probability.
As a benChmark for estimators in the heavy-tailed model, we observe that whenfis the multivariate
normal (or equivalently a SUb-GaUssian) distribution with mean MandCOVaaianæ ∑,the^pnple
mean Rn = 1/n Ei Xi satisfies, with probability at least I- 1 1:
出-H2 < Jtraced + J 惶"2 log'10 =f OPTn,Σ,δ	(1)
nn
Seminal work by Catoni (2012) showed that the sample mean is extremely sub-optimal, while
more reCent work by Lugosi et al. (2019) showed that the sub-Gaussian error bound is aChiev-
able whileonly assuming thatfhasfinite variancei.e.,2	nd moment. In the multivariate setting,
Minsker (2015) showed that the extremely praCtiCal geometriC-median-of-means estimator (GMOM)
aChieves a sub-optimal error bound by showing that with probability at least1-δ:
质OM,δi∣∣2 < Jtrace ⑶ log'10 .	(2)
n
Computationally intraCtable estimators that truly aChieve the sub-Gaussian error bound were pro-
posed by Lugosi et al. (2019); Catoni & Giulini (2017). Hopkins (2018) and later Cherapanamjeri
et al. (2019) developed a sum-of-squares based relaxation of the estimator in Lugosi et al. (2019),
thereby giving a polynomial time algorithm whiCh aChieves optimal rates. More reCent work has
studied the problem of mean estimation, foCusing on ConstruCting theoretiCally fast polynomial time
estimators (Dong et al., 2019; Lugosi & Mendelson, 2019; Diakonikolas & Kane, 2019; Lei et al.,
2020; LeCUe & Depersin, 2019). However, these estimators have several hyperparameters, which
require to be tuned for praCtiCe, making them impraCtiCal.
Now, we present our algo-
rithmFilterpdfor heavy-tailed
mean estimation, and is for-
mally stated as Algorithm 1. It
proCeeds in an iterative fash-
ion, by (1) Computing the lead-
ing eigenveCtor of the empir-
iCal CovarianCe matrix (Step
(3)), (2) projeCting points along
this leading eigenveCtor (Step
(4)), and (3) randomly sampling
points based on their projeCtion
sCores (Step (5) and (6)). This
proCedure is repeated for afixed
number of steps.
Algorithm 1Filterpd- Heavy Tailed Mean Estimator
Require: Smnplss S = {z}n==,, Iterations T*
1: for 力=ItT T* do
2:
3:
4:
5:
6:
7:
1 |S|	1 |S|
Compute %=JSEzi and ΣS = JS £(Zi-%产
|S| i=1	|S| i=1
Letvbe the leading eigenveCtor of Σ S
For eaChzi , let τi d=ef vT (zi - θS) 2 be its sCore.
Sample a point N 〜SaCCOndingtr Pr(Ni) a Ti
Remove samplezfromSi.e.S=S\ {z}
end for
8: return
Our proposed algorithm is primarily based on the SVD-basedfiltering algorithm, whiCh has ap-
peared in different forms (Klivans et al., 2009; Awasthi et al., 2014) and was reCently reused in
Diakonikolas et al. (2016; 2017) for adversarial mean estimation. For instanCe, the algorithm in
Diakonikolas et al. (2017) follows a similar proCedure, but remove a subset of points at a step de-
pending on magnitude of the projeCtion sCore.
Ourfirst main result is presented as follows:
Theorem 1. UpPPsee {ii}==1 〜PWhhZ Ni ∈ RP for all,, WherePhabbodnded thh moment and n
satisfies
n≥Cr 2'Σ)
log2(p∕δ)
log(1∕δ)
def trace 'Σ)
Q = ^Π∑iΓΓ
(3)
1Here and throughout our paper we use the notation<to denote an inequality with universal Constants
dropped for ConCiseness.
3
Under review as a conference paper at ICLR 2021
Then, FiIterPdhen孔加SanZiaaeddorT T= = 「Clog(1∕δ)^∣ StePS returns nn eStimiate θδ which satisfies
with probability at least1-4δ,δ∈(0,0.25):
隰一H 2 SOPTn,∑,δ
Remarks:Theorem 1 shows that whennis sufficiently large,Fiiterpdreturns a mean estimate that
achieves theoptimal sub-Gaussian deviation bound. This algorithm is also extremely practical as
compared to existing algorithms (for e.g., Lei et al. (2020)) and engenders development of scalable
variants, which we later describe in Section 4. We defer the proofs of Theorem1to the appendix.
2.1 Provable Algorithms for Generalized Linear Models
At this stage, with an optimal mean estimator in hand, we explore its consequences for general
supervised learning tasks, namely linear regression and generalized linear models. The goal is to
design efficient estimators which work well in the presence of heavy-tailed data. To this end, we
borrow therobust gradientframework in Prasad et al. (2020) and present it in Algorithm 2. In par-
ticular,RGD-Fiiterpdproceeds by passing the gradients at the current iterateθ t throughFiiterpd
in Step (5).
Note that Prasad et al. (2020) used a similar algorithm in their work, but usedGMOM(Minsker,
2015) as their mean estimator, which led to weaker results in the heavy-tailed setting. We show
that usingFiiterpdas the mean estimator automatically results in better bounds. The proofs for the
technical results appearing henceforth can be found in the appendix.
Algorithm 2RGD-Fiiterpd
- Robust Gradient Descent (Prasad et al., 2020)
Require: Daaa {&}之「Loss Function L.
Require:Step sizeη, Number of IterationsT, Confidenceδ.
Require: Initiiilizaoon O0 and constraint set Θ).
1:	Split samples intoTsubsets{Z t}tT=1 of sizen= n∕T
2:	SetT = =Clog( T∕δ)
3:	fort= 1toTdo
4:	Obtain St = {▽ C(nt-1; &) : Zi ∈ Zt}
5:	Letg t =Fiiterpd(S t,T =)
6:	Updaten t = argmin n-(n t-1 -ηg t)22
θ∈Θ
7:	end for
8:	return{n t}tT=1
In this setting, we observe pairs
of samples{(x 1,y1), . . .(xn,yn)},
where each(x i, yi)∈R p ×R. We
assume that the(x, y)pairs sam-
pled from the true distributionPare
linked via a linear model:
y=x Tn= +w,(4)
wherewis drawn from a iero-mean
distribution with bounded4 th mo-
ment with varianceσ 2 . We suppose
that underPthe covariatesx∈R	p,
have mean0, covarianceΣ x sat-
isfyingτ Ip	Σ x τ uIp and
bounded8 th moment.
Theorem 2.Consider the statistical model in(4). Suppose we observenpairs of samples, wheren
satisfies
n≥C 1 p2
Toog 2(PT∕δ)
log(T∕δ)
(5)
RGD-Fiiterpdwhen initialized atn 0 with step sizeη=	2∕(τu +τ) and confidence parameterδ
returns iterates{ nt}tT=1 which with probability at least1-δsatisfy:
% ≤ K'W -0"2 + ι⅛ (∕raCp) + ιC2σ (j≡⅞≡
for some contraction parameterκ<1.
(6)
Remarks:For an appropriately chosenT, we achieve the best known result for heavy-tailed lin-
ear regression, improving on the previously best known rate by Hsu & Sabato (2016). A detailed
exposition with the form ofTand detailed comparisons to other work is presented in the appendix
due to space constraints.
We also note thatRGD-Fiiterpdis also effective for generaliied linear models (GLMs) such as
logistic regression in the heavy-tailed setting. We extend guarantees for these statistical models in
the appendix (Sections B.3 and B.4) due to space constraints.
4
Under review as a conference paper at ICLR 2021
(a) TrainingDCGANon CIFAR-10	(b) TrainingDCGANon MNIST	(c) TrainingReal-NVPon
CIFAR-10
Figure 2: Variation of gradient distributions across different iterations for different models and
datasets. (a) and (b) show the variation of generator gradient norms over iterations forDCGAN
and (c) shows the variation of the complete gradient norms over iterations forReal-NVP. X-axis:
Gradient norm, Y-axis: Density
3 Existence of heavy-tails in deep generative models
In this section, we elaborate on the preliminary study discussed in Section 1. The evidence of
heavy-tailed gradient distributions observed inDCGANandReal-NVPsuggests that the issue of
heavy-tailedness is more pervasive than thought to be. While recent analyses have only considered
supervised learning models focusing on image classification (Simsekli et al., 2019) and on attention
models (Zhang et al., 2019), they miss out a large family of models that benefit from gradient
estimates, namely probabilistic models (Mohamed et al., 2019). Examples of probabilistic models
are generative adversarial networks (GANs) (Goodfellow et al., 2014) and invertibleflow models
(Dinh et al., 2014), and these are the models we have considered in our study.
Extending on the study from earlier, we investigate the variation in the norms of the gradients across
iterations while training. Zhang et al. (2019) had previously identified the effect of different datasets
for their supervised learning setup. Therefore, we also consider an additional dataset in our study
with GANs to observe any effects that the data distribution could have on this distribution. The plots
are showcased in Figure 2.
To quantitatively assess the heavy-tailedness of these gradient norm distributions, we use an esti-
mator to measure the heavy-tailedness, which assumes that the underlying distribution is a strictly
α-stable distribution. We use theα-index estimator proposed by Mohammadi et al. (2015) and used
in Simsekli et al. (2019). While there are some drawbacks of using this estimator, such as the require-
ment of symmetric distributions and the assumption that the distribution isα-stable, this estimator
provides a rough idea about how heavy the tail of a distribution is given samples from that distribu-
tion. Formally, given nsamppees XXi}n=ι and natural numbers K 1,K2 such that K1 ∙ K2 = n, the
α-index estimateαis computed as follows
ɑ α= 1。Og(K) (J E bg(IXiD- K E bg(IYiI))	Yi = K E Xj+(i-i)κ2
n i=1	K2 i=1	K1 j=1
Anα-index close to2indicates that the samples are closer to being Gaussian, and a lowerα-index
is indicative of heavy-tailedness. In the legend of each plot in Figure 2, we specify the estimated
α-index for the gradient norm distributions. An iteration-wise variation is presented in the appendix
(Section C), along with an alternate method to quantify heavy-tailedness.
With the presented evidence about the existence of heavy-tailed distributions in certain deep gen-
erative models, the use of a heavy-tailed mean estimator could be beneficial. However,Filterpd
requires to be scaled to work for models with millions of parameters, and for sample sizes that are
much smaller than the number of parameters.
4Streaming-Filterpd:	a heuristic for large-scale models
Modern large-scale machine learning models have millions of parameters, and hence, are generally
trained using stochastic (or mini-batch) gradient descent (Robbins & Monro, 1951) in practice. In
such a setting, we cannot directly useFilterpdto aggregate gradients. This is because when the
5
Under review as a conference paper at ICLR 2021
Algorithm 3Streaming-Filterpd
Require:History parameterα, IterationsT, Discard parameterd, Initializationθ	0, OptimizerALG
/ 02 def t∖
f vw2 = W )
fort= 0toT-1do
Obtain gradient samples{g t(i)}in=1 of the objective atθ t.
ift= 0then
1n	1n
Compute mt = - £ g(i and Ct = - £(g(i) - mt)02.
ni=1 ni=1
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
else
nn
Update mt = (1 - α)rnιt-ι + - £ g(i, Ct = (1 - a)%―1 v02ι +——£(g「- mt)02
ni=1	n-1 i=1
end if
Construct Bt whose columns are {，(1 - α)X —vt-ι} ∪ {/α‰-1 (Wi) - mt)}n=1
Cache λt and Vt - the leading eigenpair of Ct = BtB
Calculate scoresτ i= (vtT (gt(i) -m	t))2 and discard the top-dgradients ordered by scores
1	n-d
Compute fiinal estimate G(θt) =------ £ g(i, obtain update θt+ι = ALG(Gθt), θt)
n-d i=1
end for
13:	returnθ T
mini-batch sizenis much smaller than the dimensionp, the sample covariance matrix does not
concentrate well around the true covariance matrix. Due to this, the leading eigenvector of the
sample covariance matrix could be significantly different from that of the true covariance matrix.
Moreover, it is not viable to run T* leading eigenvector computations for models with millions of
parameters, since the additional time complexity of this operation is proportional topand is not as
efficient as computing the sample mean.
We now look at approaches to mitigate the above issues and present the spirit of the approaches. The
first issue is with regard to the concentration of sample covariance. For this, we intend on reusing
previous gradient samples to improve concentration. However, the gradient samples from earlier
iterations cannot be weighted equally as neither the distributions are the same nor are the samples
independent. Thus, given gradient samples at the current iteration and the mean and covariance from
the previous iteration, a potential solution could be to compute the current estimates of the samples
statistics by an exponential average like so.
mt = (1 — 0)m t— + QRt	Ct = (1 — 0)Ct— + αΣ t
where Ot and Σt are the mean and covariance of the gradient samples at iteration 力 add Tnt-I and
Ct—1 are the mean and covariance computed at the previous iteration. This solution, while promis-
ing, is not tractable, as it requires storing the complete covariance matrix. To circumvent this, we
approximateC t-1 by a rank-one matrix, which is given by the eigenpair(λ t-1, vt-1)ofthe covari-
ance matrixC t-1.
Recall that inFilterpd, we require to computeT * leading eigenpairs. To reduce computational costs,
we propose running one leading eigenpair computation at every iteration. This leading eigenpair is
used in two ways: 1) to compute “outlier scores”, and 2) to compute the exponential weighted
covariance in the next iteration as discussed above.
The heuristic is formally presented in Algorithm 3. The aforementioned exponential averaging is
performed in Step (6). Step (8) is a computational trick used to avoid computing the complete covari-
ance matrixC t. The leading eigenpair and the scores of the gradient samples are computed in Steps
(9) and (10) respectively. We discard points based on scores obtained using the computed eigenvec-
tor (the analogous steps inFilterpdare Steps (5) and (6)), andfinally pass our robust aggregate to
the optimization algorithm of choice in Step (11) - this allows us to leverage the benefits of certain
optimization algorithms. We repeat this forTiterations as done in standard training procedures.
The algorithm is memory efficient due to Steps (8) and (9). This is due to the construction of the
matrixB t which only requiresO(np)memory. From a computation standpoint, we only require the
leading left singular vector ofB t, which takesO(np)time to run. In contrast, a naive implementa-
tion of this algorithm is not memory efficient since it would require computing ap×pmatrixC	t
6
Under review as a conference paper at ICLR 2021
forp= 20andn= 500
Figure 3: Results for Heavy-Tailed Linear Regression. Smaller values for Qδ (θn,δ) are better.
Legend: (red, cross) - RGD — Ftlterpd, (blue, SqUare)-RDD — GMOM,(bkack, CirC-O)LoLS
(c)Q δ (θn,δ)vs.p
forn= 500and
δ= 0.1
(d)Q δ (θn,δ)vs.σ
forn= 500, p= 20
and δ = 0.1
and computing the leading eigenvector which can takeO(p 2)time. In our real-world experiments
described in Section 5.2, we noted thatStreamingDFilterpdprovides speedups ofat least4×over
Filterpdwith a smaller memory footprint, in addition to better performance on our metrics due to
the streaming approximation.
5	Experiments
5.1	Synthetic Experiments - Linear Regression
To corroborate our theoretical results, we conduct heavy-tailed linear regression to study the perfor-
mance of our proposed algorithms.
SetupWe generate covariatesx∈R	p from an standard Gaussian distribution. The true regression
parameter is set as	θ*	= [1,1,..., 1∈ R	Rp.	The response	yi	isgenatatedby g =	XTθ*	+ σw
wherewis drawn from a standardized Pareto distribution with tail-parameterβ= 3. In this setup,
we experiment with differentn, pandδ. For each setting of(n, p,δ), and cumulative metrics are
reported over100trials. We varynfrom100to500,pfrom20to100,δfrom0.01to0.1andσ
from0.01to5on a logarithmic scale.
MethodsWe compareRGDDFilterpdwith two baselines: Ordinary Least Squares (GLS) and
robust gradient descent which uses Algorithm 2 withGMGMas used in Prasad et al. (2020). Note
that Prasad et al. (2020) had previously shown thatRGDDGMGMoutperformed several other
estimators such as Hsu & Sabato (2016) and ridge regression, hence we skip them in our comparison.
Metric and Hyperparameter TuningFor any estimator θn,δ, we use( θn,δ) = θn,δ Dθ * 2 as
our primary metric. We also measure the quantile error of the estimator,i.e.Q δ(θn,δ) = inf{α:
Pr((θn,δ)>α)≤δ}. This can also be thought of as the length of confidence interval for a confi-
dence level of 1 — RThenmnbeoofblcckk ⅛and tnemιmber OfiteronOTS T* is set to ∣^3.5 IOg(I/δ)]
inRGDDGMGMandRGDDFilterpdrespectively.
ResultsFigure 3 shows that ourRGDDFilterpdclearly outperforms noth naselines. Figures 3a, 3n
and 3c are generated withσ= 0.1. Figure 3a indicates that for any confidence level1Dδ, the length
of the oracle confidence interval (Qδ (θn,δ)) for our estimator is netter than all naselines. We also see
netter sample complexity in Figure 3n, and netter dimension dependence in Figure 3c. We also on-
serve that whenσis increased, which corresponds to a lower signal-to-noise ratio,RGDDFilterpd
performs netter in comparison to the other naselines.
We have also conducted synthetic experiments on heavy-tailed mean estimation, and defer the results
to the appendix (Section D) due to space constraints. In summary,Filterpdoutperforms nothGMGM
and the sample mean with respect to the metric considered.
5.2	Real World Experiments
With the nacking of empirical evidence that the distrinutions of gradients during training certain
generative models are heavy-tailed as shown in Section 3, we seek to apply a heavy-tailed mean
estimator over the gradient samples to ontain ronust estimates of the mini-natch gradient. Due to the
size of the models, it is infeasinle to runFilterpd, and hence we useStreamingDFilterpdinstead.
7
Under review as a conference paper at ICLR 2021
5.2.1	Generative Adversarial Networks
SetupWe consider a DCGAN architecture for our experiments, same as the one considered in our
investigations presented in Section 3. The model architecture is detailed in the code appendix.
MethodsWe train a DCGAN usingStreaming-Filterpd(abbrev. StrFpd) with the optimizer
asADAMfor 50000 iterations on the MNIST and CIFAR10 datasets. For comparison, we train
other DCGANs with the same initialization, usingADAM; however, the mini-batch gradients for
the generator are aggregated via the following methods: sample mean (abbrev. Mean), gradient
clipping (abbrev. Clip), removal of gradient samples with largest norm (abbrev. NrmRmv). All
other relevant training hyperparameters are detailed in the appendix.
MetricsWe consider two key metrics: the Inception Score (Salimans et al., 2016) for CIFAR10 and
Mode Score (Che et al., 2017) for MNIST, and the Parzen window based log-likelihood estimates
as described in Goodfellow et al. (2014).
ResultsWe present our results in Table
1. We observe that training with the
Streaming-Filterpdprovides benefits in
terms of high metrics. For CIFAR10, we see
a significant improvement in the Inception
score. For MNIST, the MODE scores are
only slightly higher, however there is a dis-
cernible increase in the log-likelihood when
trained usingStreaming-Filterpdas com-
pared to another baselines.
	IS (CIFAR10)	MODE (MNIST)	Parzen LL (MNIST)
StrFPd	-5:20-	-8:94-	218∙95
Mean	-5:07-	-8:91-	-20761-
Clip	-509-	-8:92-	-21429-
NrmRmv	-4:94-	-8:87-	-21425-
GMOM	4.97	8∙90	205∙66 一
Table 1: Table of comparison of metrics - incep-
tion score (IS), mode score (MODE) and Parzen
log-likelihood (Parzen LL)
5.2.2	Invertible Flow Models
Setup ASdneennecConn 3, WeOnSSddeRRalNNPP - as3e-of-ehe-ammedeL
MethodsWe train aRealNVPmodel usingStreaming-Filterpd(abbrev. StrFpd) with the
optimizer asADAMfor 5000 iterations on the CIFAR10 dataset. For comparison, we train
twoRRalNNPmodels, both using sample mean for aggregation of gradient samples. How-
ever, one of the models does not usenorm clippingover the estimated gradient, and neither does
Streaming-Filterpd. All other relevant training hyperparameters are detailed in the appendix.
MetricsWe compute the negative log-likelihood on the test setNLL test every250iterations. We
report the test bits-per-dimension (BPD) given by BPD = PNLL(2) where P isthedimensinnaIity
of the samples. Additionally, to show stability, we report the average test BPD over the last 2000
iterations.
ResultsWe present our results in Figure 4 and Table 2. Figure 4 shows the erratic variation inBPD
whenStreaming-Filterpdis not used and further validates our hypothesis regarding the heavy-
tailedness of gradients. Additionally, note that training withStreaming-Filterpdis the most stable
among the3. We also see thatStreaming-Filterpdachieves the best average testBPDover the
last 2000 iterations.
Figure 4: Variation of TestBPDwith itera-
tions
Method	Average TestBPD over last 2000 iterations
StrFPd	12:97
Mean (no-clipping)	64166
Mean (with-clipping)	459∙16
Table 2: Table of Average Test BPD over the last
2000 iterations
8
Under review as a conference paper at ICLR 2021
6	Discussion
In this work, we study methods for mean estimation, which are applicable for aggregating gradients.
This is especially useful when the underlying distribution of these gradients is heavy-tailed, where
the sample-mean can be highly sub-optimal. We motivate the need for a robust, heavy-tailed mean
estimator by studying distribution of gradients of certain deep generative models. We also develop a
heuristic for our mean estimation algorithm that scales to models with millions of parameters. One
potential avenue of improvement is to make the heuristic applicable to extremely large models, such
as to GANs that generate high quality media or certain language models. Nonetheless, we hope that
this work encourages the development of principles approaches to tackling heavy-tailed distributions
arising in machine learning.
References
Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently
learning linear separators with noise. InProceedings of the forty-sixth annual ACM symposium
on Theory ofcomputing, pp. 449-458. ACM, 2014.
Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. In
Annales de IfInstitut Henri Poincare, Probabilites et Statistiques, volume 48, pp. 1148-1185.
Institut Henri Poincare, 2012.
Olivier Catoni and Ilaria Giulini. Dimension-free pac-bayesian bounds for matrices, vectors, and
linear least squares regression.arXiv preprint arXiv:1712.02747, 2017.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. In5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.
Yeshwanth Cherapanamjeri, Nicolas Flammarion, and Peter L. Bartlett. Fast mean estimation with
sub-gaussian rates. volume 99 ofProceedings of Machine Learning Research, pp. 786-806,
Phoenix, USA, 25-28 Jun 2019. PMLR.
Yeshwanth Cherapanamjeri, Samuel B. Hopkins, Tarun Kathuria, Prasad Raghavendra, and Nilesh
Tripuraneni. Algorithms for heavy-tailed statistics: Regression, covariance estimation, and be-
yond. InProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2020, pp. 601-609. Association for Computing Machinery, 2020. ISBN 9781450369794.
doi: 10.1145/3357713.3384329.
Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust
statistics.arXiv preprint arXiv:1911.05911, 2019.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high dimensions without the computational intractability. InFoundations of
Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 655-664. IEEE, 2016.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Being robust (in high dimensions) can be practical. InInternational Conference on Machine
Learning, pp. 999-1008, 2017.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components esti-
mation.arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp.arXiv
preprint arXiv:1605.08803, 2016.
Yihe Dong, Samuel Hopkins, and Jerry Li. Quantum entropy scoring for fast robust mean estimation
and improved outlier detection. InAdvances in Neural Information Processing Systems, pp. 6067-
6077, 2019.
Jianqing Fan, Weichen Wang, and Ziwei Zhu. A shrinkage principle for heavy-tailed data: High-
dimensional robust low-rank matrix recovery, 2016.
9
Under review as a conference paper at ICLR 2021
Jianqing Fan, Quefeng Li, and Yuyan Wang. Estimation of high dimensional mean regression in the
absence of symmetry and light tail assumptions.Journal of the Royal Statistical Society: Series
B (StatisticaIMethodoIogy),79(1):247-265, 2017.
Simon Foucart and Holger Rauhut.A mathematical introduction to compressive sensing, volume 1.
Birkhauser Basel, 2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. InAdvances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Samuel B Hopkins. Sub-gaussian mean estimation in polynomial time.arXiv preprint
arXiv:1809.07425, 2018.
Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails.Journal
of Machine Learning Research, 17(18):1-40, 2016.
Adam R Klivans, Philip M Long, and Rocco A Servedio. Learning halfspaces with malicious noise.
Journal of Machine Learning Research, 10(Dec):2715-2740, 2009.
Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved
clustering via sum of squares. InProceedings of the 50th Annual ACM SIGACT Symposium on
Theory of Computing, pp. 1035-1046. ACM, 2018.
Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In
Foundations of Computer Science (FOCS), 2016 IEEE 57th Annual Symposium on, pp. 665-674.
IEEE, 2016.
GuillaUme Lecue and Jules DePersin. Robust SUbgaUssian estimation of a mean vector in nearly
linear time.arXiv preprint arXiv:1906.03058, 2019.
Zhixian Lei, Kyle Luh, Prayaag Venkat, and Fred Zhang. A fast sPectral algorithm for mean esti-
mation with sub-gaussian rates. InConference on Learning Theory, PP. 2598-2612, 2020.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaPtive
stePsizes. InThe 22nd International Conference on Artificial Intelligence and Statistics, PP. 983-
992, 2019.
Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. High dimensional robust sParse
regression. InProceedings of the Twenty Third International Conference on Artificial Intelligence
and Statistics, volume 108 ofProceedings of Machine Learning Research, PP. 411-421. PMLR,
2020.
Gabor Lugosi and Shahar Mendelson. Robust multivariate mean estimation: the oPtimality of
trimmed mean.arXiv preprint arXiv:1907.11391, 2019.
Gabor Lugosi, Shahar Mendelson, et al. Sub-gaussian estimators of the mean of a random vector.
The annals of statistics, 47(2):783-794, 2019.
Stanislav Minsker. Geometric median and robust estimation in banach sPaces.Bernoulli, 21(4):
2308-2335, 2015.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient esti-
mation in machine learning.arXiv preprint arXiv:1906.10652, 2019.
Mohammad Mohammadi, Adel MohammadPour, and Hiroaki Ogata. On estimating the tail index
and the sPectral measure of multivariateα-stable distributions.Metrika: International Journal for
Theoretical and Applied Statistics, 78(5):549-561, July 2015. doi: 10.1007/s00184-014-0515-7.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, PradeeP Ravikumar, et al. Robust es-
timation via robust gradient estimation.Journal of the Royal Statistical Society Series B, 82(3):
601-627, 2020.
Alec Radford, Luke Metz, and Soumith Chintala. UnsuPervised rePresentation learning with deeP
convolutional generative adversarial networks.arXiv preprint arXiv:1511.06434, 2015.
10
Under review as a conference paper at ICLR 2021
Sidney I Resnick.Heavy-tail phenomena: probabilistic and statistical modeling. Springer Science
& Business Media, 2007.
Herbert Robbins and Sutton Monro. A stochastic approximation method.The annals of mathemati-
Cal statistics, pp. 400-407, 1951.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. InAdvances in neural information processing systems,
pp. 2234-2242, 2016.
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic gra-
dient noise in deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 ofProceed-
ings of Machine Learning Research, pp. 5827-5837, Long Beach, California, USA, 09-15 Jun
2019. PMLR.
Jacob Steinhardt.Robust Learning: Information Theory and Algorithms. PhD thesis, Stanford
University, 2018.
Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive huber regression.Journal of the American
Statistical Association, pp. 1-24, 2019.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices.arXiv preprint
arXiv:1011.3027, 2010.
Huan Xu, Constantine Caramanis, and Shie Mannor. Outlier-robust pca: the high-dimensional case.
IEEE transactions on information theory, 59(1):546-572, 2013.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J Reddi,
Sanjiv Kumar, and Suvrit Sra. Why adam beats sgd for attention models.arXiv preprint
arXiv:1912.03194, 2019.
Wen-Xin Zhou, Koushiki Bose, Jianqing Fan, and Han Liu. A new perspective on robustm-
estimation: Finite sample theory and applications to dependence-adjusted multiple testing.arXiv
preprint arXiv:1711.05381, 2017.
Fangyu Zou, Li Shen, Zequn Jie, Ju Sun, and Wei Liu. Weighted adagrad with unified momentum.
arXiv preprint arXiv:1808.03408, 2018.
11