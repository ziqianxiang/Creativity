Under review as a conference paper at ICLR 2021
Regret Bounds and Reinforcement Learning
Exploration of EXP-based Algorithms
Anonymous authors
Paper under double-blind review
Ab stract
EXP-based algorithms are often used for exploration in multi-armed bandit. We
revisit the EXP3.P algorithm and establish both the lower and upper bounds of
regret in the Gaussian multi-armed bandit setting, as well as a more general
distribution option. The analyses do not require bounded rewards compared to
classical regret assumptions. We also extend EXP4 from multi-armed bandit to
reinforcement learning to incentivize exploration by multiple agents. The resulting
algorithm has been tested on hard-to-explore games and it shows an improvement
on exploration compared to state-of-the-art.
1	Introduction
Multi-armed bandit (MAB) is to maximize cumulative reward of a player throughout a bandit game
by choosing different arms at each time step. It is also equivalent to minimizing the regret defined
as the difference between the best rewards that can be achieved and the actual reward gained by the
player. Formally, given time horizon T, in time step t ≤ T the player choose one arm at among K
arms, receives rat among rewards rt = (r1t , r2t , . . . , rKt ), and maximizes the total reward PtT=1 rat
or minimizes the regret. Computationally efficient and with abundant theoretical analyses are the
EXP-type MAB algorithms. In EXP3.P, each arm has a trust coefficient (weight). The player samples
each arm with probability being the sum of its normalized weights and a bias term, receives reward of
the sampled arm and exponentially updates the weights based on the corresponding reward estimates.
It achieves the regret of the order O(√T) in a high probability sense. In EXP4, there are any number
of experts. Each has a sample rule over actions and a weight. The player samples according to the
weighted average of experts’ sample rules and updates the weights respectively.
Contextual bandit is a variant of MAB by adding context or state space S. At time step t, the player
has context St ∈ S with si：T = (si, s2,..., ST) being independent. Rewards rt follow F(μ(st))
where F is any distribution and μ(st) is the mean vector that depends on state st. Reinforcement
Learning (RL) generalizes contextual bandit, where state and reward transitions follow a Markov
Decision Process (MDP) represented by transition kernel P(st+i, rt|at, st). A key challenge in RL
is the trade-off between exploration and exploitation. Exploration is to encourage the player to try
new arms in MAB or new actions in RL to understand the game better. It helps to plan for the future,
but with the sacrifice of potentially lowering the current reward. Exploitation aims to exploit currently
known states and arms to maximize the current reward, but it potentially prevents the player to gain
more information to increase local reward. To maximize the cumulative reward, the player needs to
know the game by exploration, while guaranteeing current reward by exploitation.
How to incentivize exploration in RL has been a main focus in RL. Since RL is built on MAB, it
is natural to extend MAB techniques to RL and UCB is such a success. UCB (Auer et al. (2002a))
motivates count-based exploration (Strehl and Littman, 2008) in RL and the subsequent Pseudo-
Count exploration (Bellemare et al., 2016). New deep RL exploration algorithms have been recently
proposed. Using deep neural networks to keep track of the Q-values by means of Q-networks in
RL is called DQN (Mnih et al. (2013)). This combination of deep learning and RL has shown
great success. -greedy in Mnih et al. (2015) is a simple exploration technique using DQN. Besides
-greedy, intrinsic model exploration computes intrinsic rewards by focusing on experiences. Intrinsic
rewards directly measure and incentivize exploration if added to extrinsic (actual) rewards of RL, e.g.
DORA (Fox et al., 2018) and (Stadie et al., 2015). Random Network Distillation (RND) (Burda et al.,
2018) is a more recent suggestion relying on a fixed target network. A drawback of RND is its local
focus without global exploration.
1
Under review as a conference paper at ICLR 2021
In order to address weak points of these various exploration algorithms in the RL context, the
notion of experts is natural and thus EXP-type MAB algorithms are appropriate. The allowance of
arbitrary experts provides exploration for harder contextual bandits and hence providing exploration
possibilities for RL. We develop an EXP4 exploration algorithm for RL that relies on several general
experts. This is the first RL algorithm using several exploration experts enabling global exploration.
Focusing on DQN, in the computational study we focus on two agents consisting of RND and
-greedy DQN.
We implement the RL EXP4 algorithm on the hard-to-explore RL game Montezuma’s Revenge and
compare it with the benchmark algorithm RND (Burda et al. (2018)). The numerical results show
that the algorithm gains more exploration than RND and it gains the ability of global exploration by
not getting stuck in local maximums of RND. Its total reward also increases with training. Overall,
our algorithm improves exploration and exploitation on the benchmark game and demonstrates a
learning process in RL.
Reward in RL in many cases is unbounded which relates to unbounded MAB rewards. There are three
major versions of MAB: Adversarial, Stochastic, and herein introduced Gaussian. For adversarial
MAB, rewards of the K arms rt can be chosen arbitrarily by adversaries at step t. For stochastic MAB,
the rewards at different steps are assumed to be i.i.d. and the rewards across arms are independent.
It is assumed that 0 ≤ rit ≤ 1 for any arm i and step t. For Gaussian MAB, rewards rt follow
multi-variate normal N(μ, Σ) with μ being the mean vector and Σ the covariance matrix of the K
arms. Here the rewards are neither bounded, nor independent among the arms. For this reason the
introduced Gaussian MAB reflects the RL setting and is the subject of our MAB analyses of EXP3.P.
EXP-type algorithms (Auer et al. (2002b)) are optimal in the two classical MABs. Auer et al. (2002b)
show lower and upper bounds on regret of the order O(√T) for adversarial MAB and of the order
O(log(T)) for stochastic MAB. All of the proofs of these regret bounds by EXP-type algorithms are
based on the bounded reward assumption, which does not hold for Gaussian MAB. Therefore, the
regret bounds for Gaussian MAB with unbounded rewards studied herein are significantly different
from prior works.
We show both lower and upper bounds on regret of Gaussian MAB under certain assumptions. Some
analyses even hold for more generally distributed MAB. Upper bounds borrow some ideas from
the analysis of the EXP3.P algorithm in Auer et al. (2002b) for bounded MAB to our unbounded
MAB, while lower bounds are by our brand new construction of instances. Precisely, We derive
lower bounds of order Ω(T) for certain fixed T and upper bounds of order O*(√T) for T being large
enough. The question of bounds for any value ofT remains open.
The main contributions of this work are as follows. On the analytical side we introduce Gaussian
MAB with the unique aspect and challenge of unbounded rewards. We provide the very first regret
lower bound in such a case by constructing a novel family of Gaussian bandits and we are able to
analyze the EXP3.P algorithm for Gaussian MAB. Unbounded reward poses a non-trivial challenge
in the analyses. We also provide the very first extension of EXP4 to RL exploration. We show its
superior performance on two hard-to-explore RL games.
A literature review is provided in Section 2. Then in Section 3 we exhibit upper bounds for unbounded
MAB of the EXP3.P algorithm and lower bounds, respectively. Section 4 discusses the EXP4
algorithm for RL exploration. Finally, in Section 5, we present numerical results related to the
proposed algorithm.
2	Literature review
The importance of exploration in RL is well understood. Count-based exploration in RL relies
on UCB. Strehl and Littman (2008) develop Bellman value iteration V (s) = maxa R(s, a) +
YE[V(s0)] + βN(s, a)-2, where N(s, a) is the number of visits to (s, a) for state S and action a.
Value N(s, a)-2 is positively correlated with curiosity of (s, a) and encourages exploration. This
method is limited to tableau model-based MDP for small state spaces, while Bellemare et al. (2016)
introduce Pseudo-Count exploration for non-tableau MDP with density models.
In conjunction with DQN, -greedy in Mnih et al. (2015) is a simple exploration technique using
DQN. Besides -greedy, intrinsic model exploration computes intrinsic rewards by the accuracy of
a model trained on experiences. Intrinsic rewards directly measure and incentivize exploration if
2
Under review as a conference paper at ICLR 2021
added to extrinsic (actual) rewards of RL, e.g. DORA in Fox et al. (2018) and Stadie et al. (2015).
Intrinsic rewards in Stadie et al. (2015) are defined as e(s, a) = ∣∣σ(s0) - Mφ(σ(s), a)∣∣2 where Mφ
is a parametric model, s0 is the next state and σ is input extraction. Intrinsic reward e(s, a) relies on
stochastic transition from s to s0 and brings noise to exploration. Random Network Distillation(RND)
in Burda et al. (2018) addresses this by defining e(s, a) = ||f(s0) - f(s0)||22 where f is a parametric
model and f is a randomly initialized but fixed model. Here e(s, a), independent of the transition,
only depends on state s0 and drives RND to outperform other algorithms on Montezuma’s Revenge.
None of these algorithms use several experts which is a significant departure from our work.
In terms of MAB regret analyses focusing on EXP-type algorithms, Auer et al. (2002b) first introduce
EXP3.P for bounded adversarial MAB and EXP4 for contextual bandits. Under the EXP3.P algorithm,
an upper bound on regret of the order O(√T) is achieved, which has no gap with the lower bound
and hence it establishes that EXP3.P is optimal. However these regret bounds are not applicable to
Gaussian MAB since rewards can be infinite. Meanwhile for unbounded MAB, Srinivas et al. (2010)
demonstrate a regret bound of order O(√T ∙ YT) for noisy Gaussian process bandits where a reward
observation contains noise. The information gain γT is not well-defined in a noiseless Gaussian
setting. For noiseless Gaussian bandits, Grunewalder et al. (2010) show both the optimal lower and
upper bounds on regret, but the regret definition is not consistent with the one used in Auer et al.
(2002b). We establish a lower bound of the order Ω(T) for certain T and an upper bound of the
order O*(√T) asymptotically on regret of unbounded noiseless Gaussian MAB following standard
definitions of regret.
3	Regret bounds for Gaussian MAB
For Gaussian MAB with time horizon T, at step 0 < t ≤ T rewards rt follow multi-variate
normal N(μ, Σ) where μ = (μι, μ2,..., μκ) is the mean vector and Σ = (aj)i,j∈{i,...,κ} is the
covariance matrix of the K arms. The player receives reward yt = ratt by pulling arm at. We use
RT = T ∙ maxk μk - Pt E[yt] to denote pseudo regret called simply regret. (Note that the alternative
definition of regret RT = maxi PtT=1 rit - PtT=1 yt depends on realizations of rewards.)
3.1	Lower bounds on regret
In this section we derive a lower bound for Gaussian and general MAB under an assumption. General
MAB replaces Gaussian with a general distribution. The main technique is to construct instances or
sub-classes that have certain regret, no matter what strategies are deployed. We need the following
assumption or setting.
Assumption 1 There are two types of arms with general K with one type being superior (S is
the set of superior arms) and the other being inferior (I is the set of inferior arms). Let 1 - q, q be
the proportions of the superior and inferior arms, respectively which is known to the adversary and
clearly 0 ≤ q ≤ 1. The arms in S are indistinguishable and so are those in I . The first pull of the
player has two steps. In the first step the player selects an inferior or superior set of arms based on
P(S) = 1 - q and P(I) = q and once a set is selected, the corresponding reward of an arm from the
selected set is received.
An interesting special case of Assumption 1 is the case of two arms and q = 1/2. In this case, the
player has no prior knowledge and in the first pull chooses an arm uniformly at random.
The lower bound is defined as RL (T) = inf sup RT0 , where, first, inf is taken among all the strategies
and then sup is among all Gaussian MAB. All proofs are in the Appendix.
The following is the main result with respect to lower bounds and it is based on inferior arms being
distributed as N(0, 1) and superior as N(μ, 1) with μ > 0.
Theorem 1.	InGaussianMABunderAssumption 1,forany q ≥ 1/3 we have RL(T) ≥ (q — e) ∙μ∙T
where μ has to satisfy G(q, μ) < q with and T determined by
G(q,μ) <e<q,	T ≤ ---------，- GP ∣ +2
(1-q) ∙ Rie-x2- -e- -|
and G(q, μ) = max R iiqe
x2
ɪ - (1 - q)e-
(X -μ I dx, R 1(1 — q)e-号
(x-μ)2 I 7	]
qe 2 dx,.

3
Under review as a conference paper at ICLR 2021
To prove Theorem 1, we construct a special subset of Gaussian MAB with equal variances and zero
covariances. On these instances we find a unique way to explicitly represent any policy. This builds a
connection between abstract policies and this concrete mathematical representation. Then we show
that pseudo regret R0T must be greater than certain values no matter what policies are deployed, which
indicates a regret lower bound on these subset of instances.
The feasibility of the aforementioned conditions is established in the following theorem.
Theorem 2.	In Gaussian MAB under Assumption 1, for any q ≥ 1/3, there exist μ and e,e<μ such
that RL(T) ≥ (q - C) ∙ μ ∙ T.
The following result with two arms and equal probability in the first pull deals /with general probabil-
ities. Even in the case of Gaussian MAB it is not a special case of Theorem 2 since it is stronger.
Theorem 3. ForgeneralMAB under Assumption 1 with K = 2, q = 1/2, we have that RL(T) ≥ Tμ
holds for any distributions f0 for the arms in I and f1 for the arms in S with |f1 - f0 | > 0 (possibly
with unbounded support), for any μ > 0 and T satisfying T ≤ ?R,：-% ∣ + 1.
The theorem establishes that for any fixed μ > 0 there is a finite set of horizons T and instances of
Gaussian MAB so that no algorithm can achieve regret smaller than linear in T. Table 1 provides the
values of the relationship between μ and largest T in the Gaussian case where the inferior arms are
distributed based on the standard normal and the superior arms have mean μ > 0 and variance 1. For
example, there is no way to attain regret lower than T ∙ 10-4/4 for any 1 ≤ T ≤ 2501. The function
decreases very quickly.
Table 1: Upper bounds for T as a function of μ
μ	10-5	10-4	10-3	10-2	10-* 1
Upper bound for T	25001	2501	251	26	3.5
The established lower bound result RL(T) ≥ Ω(T) is larger than known results of classical MAB.
This is not surprising since the rewards in classical MAB are assumed to be bounded, while rewards
in our setting follow an unbounded Gaussian distribution, which apparently increases regret.
Besides the known result Ω(√T) of adversarial MAB and Ω(log T) of stochastic MAB, for noisy
Gaussian Process bandits, Srinivas et al. (2010) show RL(T) ≤ Ω(√T ∙ YT). Our lower bound for
Gaussian MAB is different from this lower bound. The information gain term γT in noisy Gaussian
bandits is not well-defined in Gaussian MAB and thus the two bounds are not comparable.
3.2 Upper bounds on regret
In this section, we establish upper bounds for regret of Gaussian MAB by means of the EXP3.P
algorithm (see Algorithm 1) from Auer et al. (2002b). We stress that rewards can be infinite, without
the bounded assumption present in stochastic and adversarial MAB. We only consider non-degenerate
Gaussian MAB where variance of each arm is strictly positive, i.e. mini aii > 0.
Algorithm 1: EXP3.P
Initialization: Weights Wi(1) = exp (α3δ∖∣~K), i ∈ {1,2,..., K} for a > 0 and δ ∈ (0,1);
for t = 1, 2, . . . , T do
for i = 1, 2, . . . , K do
I Pi⑴= (I- δ)pK=iW(t)+ 会
end
Choose it randomly according to the distribution p1(t), . . . , pK (t);
Receive reward rit (t);
for j = 1, . . . , K do
I Xj (t) = pj(t) ∙ 1j=it, Wj (t +1) = Wj (t) exp 3K (Xj (t) + Pj (t)√KT )
end
end
4
Under review as a conference paper at ICLR 2021
Formally, we provide analyses for upper bounds on RT with high probability, on E[RT] and on R0T .
In Auer et al. (2002b) EXP3.P is studied to yield a bound on regret RT with high probability in the
bounded MAB setting. As part of our contributions, We show that EXP3.P regret is of the order
O*(√T) in the unbounded Gaussian MAB in the case of RT with high probability, E[Rt] and RT.
The results are summarized as follows. The density of N(μ, Σ) is denoted by f.
Theorem 4.	For Gaussian MAB, any time horizonT, for any 0 < η < 1, EXP3.P has regret
RT ≤ 4∆(η) ∙ (JkT log(KT) +4J 3 KT log K + 8 log( KT)) With probability (1 一 δ) ∙ (1 一 η)τ
where ∆(η) is determined by R-∆∆ . . . R-∆∆ f (x1, . . . , xK) dx1 . . . dxK = 1 - η.
In the proof of Theorem 4, we first perform truncation of the rewards of Gaussian MAB by dividing
the rewards to a bounded part and unbounded tail throughout the game. For the bounded part, we
directly borrow the regret upper bound of EXP3.P in Auer et al. (2002b) and conclude with the regret
upper bound of order O(∆(η)√T). Since a Gaussian distribution is a light-tailed distribution we can
control the probability of tail shrinking which leads to the overall result.
The dependence of the bound on ∆ can be removed by considering large enough T as stated next.
Theorem 5.	For Gaussian MAB, and any a > 2, 0 < δ < 1, EXP3.P has regret
RT ≤ log(1∕δ)O*(√T) with probability (1 — δ) ∙ (1 — Ta)τ.
The constant behind O* depends on K, a, μ and Σ.
The above theorems deal with RT but the aforementioned lower bounds are with respect to pseudo
regret. To complete the analysis of Gaussian MAB, it is desirable to have an upper bound on pseudo
regret which is established next. It is easy to verify by the Jensen’s inequality that R0T ≤ E [RT] and
thus it suffices to obtain an upper bound on E [RT].
For adversarial and stochastic MAB, the upper bound for E[RT] is of the same order as RT which
follows by a simple argument. For Gaussian MAB, establishing an upper bound on E [RT] or R0T
based on RT requires more work. We show an upper bound on E [RT] by using select inequalities,
limit theories, and Randemacher complexity. To this end, the main result reads as follows.
Theorem 6.	The regret of EXP3.P in Gaussian MAB satisfies
RT ≤ E[Rτ] ≤ O*(√T).
All these three theorems also hold for sub-Gaussian MAB, which is defined by replacing Gaussian
with sub-Gaussian. This generalization is straightforward and it is directly shown in the proof of
Gaussian MAB in Appendix. Optimal upper bounds for adversarial MAB and noisy Gaussian Process
bandits are of the same order as our upper bound. Auer et al. (2002b) derive an upper bound of the
same order O(√T) as the lower bound for adversarial MAB. For noisy Gaussian Process bandits,
there is also no gap between its upper and lower bounds.
Our upper bound of the order O*( √T) is of the same order as the one for bounded MAB. In our case
the upper bound result O*(√T) holds for large enough T which is hidden behind O* while the linear
lower bounds is valid only for small values of T. This illustrates the rationality of the lower bound of
O(T) and the upper bound of order O*(√T).
4	EXP4 algorithm for RL
EXP4 has shown great success in contextual bandits. Therefore, in this section, we extend EXP4 to
RL and develop EXP4-RL illustrated in Algorithm 2.
The player has experts that are represented by deep Q-networks trained by RL algorithms (there
is a one to one correspondence between the experts and Q-networks). Each expert also has a trust
coefficient. Trust coefficients are also updated exponentially based on the reward estimates as in
EXP4. At each step of one episode, the player samples an expert (Q-network) with probability that is
proportional to the weighted average of expert’s trust coefficients. Then -greedy DQN is applied on
the chosen Q-network. Here different from EXP4, the player needs to store all the interaction tuples
5
Under review as a conference paper at ICLR 2021
in experience buffer since RL is a MDP. After one episode, the player trains all Q-networks with the
experience buffer and uses the trained networks as experts for the next episode.
Algorithm 2: EXP4-RL
Initialization: Trust coefficients wk = 1 for any k ∈ {1, . . . , E}, E = number of experts
(Q-networks), K = number of actions, ∆, , η > 0 and temperature z, τ > 0, nr = -∞ (an
upper bound on reward);
while True do
Initialize episode by setting s0 ;
for i = 1, 2, . . . , T (length of episode) do
Observe state si ;
Let probability of Qk-networkbe Pk = (1 - η) Pw W + En;
Sample network k according to {ρk}k;
For Qk-network, use e-greedy to sample an action
a* = argmaXaQk (si,a),
πj = (1-e)∙1j=a* + K - J ∙1j=a*
j∈{1,2,...,K}
Sample action ai based on π;
Interact with the environment to receive reward ri and next state si+1;
nr = max{ri, nr};
Update the trust coefficient wk of each Qk-network as follows:
+a∆(nr -ri),j ∈ 1, 2,...,K,yk
Store (si, ai, ri, si+1) in experience replay buffer B;
end
Update each expert’s Qk-network from buffer B;
1j
Pk = e-greedy(Qk ),^kj = I-L
Pkj
E[Xkj ],wk = Wk ∙e F
end
The basic idea is the same as EXP4 by using the experts that give advice vectors with deep Q-networks.
It is a combination of deep neural networks with EXP4 updates. From a different perspective, we can
also view it as an ensemble in classification (Xia et al. (2011)), by treating Q-networks as ensembles
in RL, instead of classification algorithms. While Q-networks do not necessarily have to be experts,
i.e., other experts can be used, these are natural in a DQN framework.
In our implementation and experiments we use two experts, thus E = 2 with two Q-networks. The
first one is based on RND (Burda et al. (2018)) while the second one is a simple DQN. To this end, in
the algorithm before storing to the buffer, we also record Cr = ||f(si) - f (si)∣∣2, the RND intrinsic
reward as in Burda et al. (2018). This value is then added to the 4-tuple pushed to B. When updating
Q1 corresponding to RND at the end of an iteration in the algorithm, by using rj + cjr we modify
the Q1-network and by using cjr an update to f is executed. Network Q2 pertaining to e-greedy is
updated directly by using rj .
Intuitively, Algorithm 2 circumvents this drawback with the total exploration guided by two experts
with EXP4 updated trust coefficients. When the RND expert drives high exploration, its trust
coefficient leads to a high total exploration. When it has low exploration, the second expert DQN
should have a high one and it incentivizes the total exploration accordingly. Trust coefficients are
updated by reward estimates iteratively as in EXP4, so they keep track of the long-term performance
of experts and then guide the total exploration globally. These dynamics of EXP4 combined with
intrinsic rewards guarantees global exploration. The experimental results exhibited in the next section
verify this intuition regarding exploration behind Algorithm 2.
We point out that potentially more general RL algorithms based on Q-factors can be used, e.g.,
boostrapped DQN (Osband et al. (2016)), random prioritized DQN (Osband et al. (2018)) or adaptive
e-greedy VDBE (Tokic (2010)) are a possibility. Furthermore, experts in EXP4 can even be policy
networks trained by PPO (Schulman et al. (2017)) instead of DQN for exploration. These possibilities
demonstrate the flexibility of the EXP4-RL algorithm.
6
Under review as a conference paper at ICLR 2021
5	Computational study
As a numerical demonstration of the superior performance and exploration incentive of Algorithm 2,
we show the improvements on baselines on two hard-to-explore RL games, Mountain Car and
Montezuma’s Revenge. More precisely, we present that the real reward on Mountain Car improves
significantly by Algorithm 2 in Section 5.1. Then we implement Algorithm 2 on Montezuma’s
Revenge and show the growing and remarkable improvement of exploration in Section 5.2.
Intrinsic reward cir = ||f (si) - f (si)||2 given by intrinsic model f represents the exploration of RND
in Burda et al. (2018) as introduced in Sections 2 and 4. We use the same criterion for evaluating
exploration performance of our algorithm and RND herein. RND incentivizes local exploration with
the single step intrinsic reward but with the absence of global exploration.
5.1	Mountain Car
In this part, we summarize the experimental results of Algorithm 2 on Mountain Car, a classical
control RL game. This game has very sparse positive rewards, which brings the necessity and
hardness of exploration. Blog post (Rivlin (2019)) shows that RND based on DQN improves the
performance of traditional DQN, since RND has intrinsic reward to incentivize exploration. We
use RND on DQN from Rivlin (2019) as the baseline and show the real reward improvement of
Algorithm 2, which supports the intuition and superiority of the algorithm.
The comparison between Algorithm 2 and RND is presented in Figure 1. Here the x-axis is the
epoch number and the y-axis is the cumulative reward of that epoch. Figure 1a shows the raw
data comparison between EXP4-RL and RND. We observe that though at first RND has several
spikes exceeding those of EXP4-RL, EXP4-RL has much higher rewards than RND after 300 epochs.
Overall, the relative difference of areas under the curve (AUC) is 4.9% for EXP4-RL over RND,
which indicates the significant improvement of our algorithm. This improvement is better illustrated
in Figure 1b with the smoothed reward values. Here there is a notable difference between EXP4-RL
and RND. Note that the maximum reward hit by EXP4-RL is -86 and the one by RND is -118,
which additionally demonstrates our improvement on RND.
(a) original	(b) smooth
Figure 1: The performance of Algorithm 2 and RND measured by the epoch-wise reward on Mountain
Car, with the left one being the original data and the right being the smoothed reward values.
We conclude that Algorithm 2 performs better than the RND baseline and that the improvement
increases at the later training stage. Exploration brought by Algorithm 2 gains real reward on this
hard-to-explore Mountain Car, compared to the RND counterpart (without the DQN expert). The
power of our algorithm can be enhanced by adopting more complex experts, not limited to only DQN.
5.2	Montezuma’ s Revenge and Pure exploration setting
In this section, we show the experimental details of Algorithm 2 on Montezuma’s Revenge, another
notoriously hard-to-explore RL game. The benchmark on Montezuma’s Revenge is RND based on
DQN which achieves a reward of zero in our environment (the PPO algorithm reported in Burda et al.
(2018) has reward 8,000 with many more computing resources; we ran the PPO-based RND with 10
parallel environments and 800 epochs to observe that the reward is also 0), which indicates that DQN
has room for improvement regarding exploration.
To this end, we first implement the DQN-version RND (called simply RND hereafter) on Montezuma’s
Revenge as our benchmark by replacing the PPO with DQN. Then we implement Algorithm 2 with
two experts as aforementioned. Our computing environment allows at most 10 parallel environments.
In subsequent figures the x-axis always corresponds to the number of epochs. RND update probability
is the proportion of experience that are used for training the intrinsic model f (Burda et al., 2018).
7
Under review as a conference paper at ICLR 2021
A comparison between Algorithm 2 (EXP4-RL) and RND without parallel environments (the update
probability is 100% since it is a single environment) is shown in Figure 2 with the emphasis on
exploration by means of the intrinsic reward. We use 3 different numbers of burn-in periods (58,
68, 167 burn-in epochs) to remove the initial training steps, which is common in Gibbs sampling.
Overall EXP4-RL outperforms RND with many significant spikes in the intrinsic rewards. The larger
the number of burn-in periods is, the more significant is the dominance of EXP4-RL over RND.
EXP4-RL has much higher exploration than RND at some epochs and stays close to RND at other
epochs. At some epochs, EXP4-RL even has 6 times higher exploration. The relative difference in
the areas under the curves are 6.9%, 17.0%, 146.0%, respectively, which quantifies the much better
performance of EXP4-RL.
(a) small
(b) medium
(c) large
Figure 2: The performance of Algorithm 2 and RND measured by intrinsic reward without parallel
environments with three different burn-in periods
0.25 update
smoothing with 0.25 update smoothing with 0.125 update
Figure 3: The performance of Algorithm 2 and RND with 10 parallel environments and with RND
update probability 0.25 and 0.125, measured by loss and intrinsic reward.
(a) Q-network losses with (b) Intrinsic reward after (c) Intrinsic reward after
We next compare EXP4-RL and RND with 10 parallel environments and different RND update
probabilities in Figure 3. The experiences are generated by the 10 parallel environments.
Figure 3a shows that both experts in EXP4-RL are learning with decreasing losses of their Q-networks.
The drop is steeper for the RND expert but it starts with a higher loss. With RND update probability
0.25 in Figure 3b we observe that EXP4-RL and RND are very close when RND exhibits high
exploration. When RND is at its local minima, EXP4-RL outperforms it. Usually these local minima
are driven by sticking to local maxima and then training the model intensively at local maxima,
typical of the RND local exploration behavior. EXP4-RL improves on RND as training progresses,
e.g. the improvement after 550 epochs is higher than the one between epochs 250 and 550. In terms
for AUC, this is expressed by 1.6% and 3.5%, respectively. Overall, EXP4-RL improves RND local
minima of exploration, keeps high exploration of RND and induces a smoother global exploration.
With the update probability of 0.125 in Figure 3c, EXP4-RL almost always outperforms RND with a
notable difference. The improvement also increases with epochs and is dramatically larger at RND’s
local minima. These local minima appear more frequently in training of RND, so our improvement
is more significant as well as crucial. The relative AUC improvement is 49.4%. The excellent
performance in Figure 3c additionally shows that EXP4-RL improves RND with global exploration
by improving local minima of RND or not staying at local maxima.
Overall, with either 0.25 or 0.125, EXP4-RL incentivizes global exploration on RND by not getting
stuck in local exploration maxima and outperforms RND exploration aggressively. With 0.125 the
improvement with respect to RND is more significant and steady. These experimental evidence
verifies our intuition behind EXP4-RL and provides excellent support for it. With experts being more
advanced RL exploration algorithms, e.g. DORA, EXP4-RL can bring additional possibilities.
8
Under review as a conference paper at ICLR 2021
References
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235-256, 2002a.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM Journal on Computing, 32(1):48-77, 2002b.
M. F. Balcan. 8803 machine learning theory. http://cs.cmu.edu/~ninamf/ML11/
lect1117.pdf, 2011.
M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-based
exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, pages
1471-1479, 2016.
Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. In
International Conference on Learning Representations, 2018.
S. Chatterjee. Superconcentration and related topics, volume 15. Cham: Springer, 2014.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: a large-scale hierarchical
image database. In 2009 IEEE conference on Computer Vision and Pattern Recognition, pages
248-255. IEEE, 2009.
L. Devroye, A. Mehrabian, and T. Reddad. The total variation distance between high-dimensional
gaussians. arXiv preprint arXiv:1810.08693, 2018.
J. Duchi. Probability bounds. http://ai.stanford.edu/~jduchi/projects/
probability_bounds.pdf, 2009.
L. Fox, L. Choshen, and Y. Loewenstein. Dora the explorer: directed outreaching reinforcement
action-selection. In International Conference on Learning Representations, 2018.
S. Grunewalder, J. Y. Audibert, M. Opper, and J. ShaWe-Taylor. Regret bounds for gaussian
process bandit problems. In Proceedings of the Thirteenth International Conference on Artificial
Intelligence and Statistics, pages 273-280, 2010.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems, pages 1097-1105, 2012.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller.
Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, and S. Petersen. Human-level control through deep rein-
forcement learning. Nature, 518(7540):529-533, 2015.
I.	Osband, C. Blundell, A. Pritzel, and B. Van Roy. Deep exploration via bootstrapped dqn. In
Advances in Neural Information Processing Systems, pages 4026T034, 2016.
I.	Osband, J. Aslanides, and A. Cassirer. Randomized prior functions for deep reinforcement learning.
In Advances in Neural Information Processing Systems, pages 8617-8629, 2018.
O. Rivlin. Mountaincar_dqn_rnd. https://github.com/orrivlin/MountainCar_DQN_
RND, 2019.
J.	Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
N.	Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit
setting: no regret and experimental design. In Proceedings of the 27th International Conference on
Machine Learning, 2010.
B. C. Stadie, S. Levine, and P. Abbeel. Incentivizing exploration in reinforcement learning with deep
predictive models. arXiv preprint arXiv:1507.00814, 2015.
9
Under review as a conference paper at ICLR 2021
A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision
processes. Journal ofComputer and System Sciences, 74(8):1309-1331, 2008.
M. Tokic. Adaptive ε-greedy exploration in reinforcement learning based on value differences. In
Annual Conference on Artificial Intelligence, pages 203-210. Springer, 2010.
R. Xia, C. Zong, and S. Li. Ensemble of feature sets and classification algorithms for sentiment
classification. Information Sciences, 181(6):1138-1152, 2011.
10
Under review as a conference paper at ICLR 2021
A	Details about numerical experiments
A. 1 Mountain Car
For the Mountain Car experiment, We use the Adam optimizer with the 2 ∙ 10-4 learning rate. The
batch size for updating models is 64 with the replay buffer size of 10,000. The remaining parameters
are as follows: the discount factor for the Q-networks is 0.95, the temperature parameter τ is 0.1,
η is 0.05, and is decaying exponentially with respect to the number of steps with maximum 0.9
and minimum 0.05. The length of one epoch is 200 steps. The target networks load the weights and
biases of the trained networks every 400 steps. Since a reward upper bound is known in advance, we
use nr = 1.
We next introduce the structure of neural networks that are used in the experiment. The neural
networks of both experts are linear. For the RND expert, it has the input layer with 2 input neurons,
followed by a hidden layer with 64 neurons, and then a two-headed output layer. The first output layer
represents the Q values with 64 hidden neurons as input and the number of actions output neurons,
while the second output layer corresponds to the intrinsic values, with 1 output neuron. For the DQN
expert, the only difference lies in the absence of the second output layer.
A.2 Montezuma’ s Revenge
For the Montezuma’s Revenge experiment, we use the Adam optimizer with the 10-5 learning rate.
The other parameters read: the mini batch size is 4, replay buffer size is 1,000, the discount factor
for the Q-networks is 0.999 and the same valus is used for the intrinsic value head, the temperature
parameter τ is 0.1, η is 0.05, and is increasing exponentially with minimum 0.05 and maximum 0.9.
The length of one epoch is 100 steps. Target networks are updated every 300 steps. Pre-normalization
is 50 epochs and the weights for intrinsic and extrinsic values in the first network are 1 and 2,
respectively. The upper bound on reward is set to be constant nr = 1.
For the structure of nerual networks, we use CNN architectures since we are dealing with videos.
More precisely, for the Q-network of the DQN expert in EXP4-RL and the predictor network f for
computing the intrinsic rewards, we use Alexnet (Krizhevsky et al. (2012)) pretrained on ImageNet
(Deng et al. (2009)). The number of output neurons of the final layer is 18, the number of actions in
Montezuma. For the RND baseline and RND expert in EXP4-RL, we customize the Q-network with
different linear layers while keeping all the layers except the final layer of pretrained Alexnet. Here
we have two final linear layers representing two value heads, the extrinsic value head and the intrinsic
value head. The number of output neurons in the first value head is again 18, while the second value
head is with 1 output neuron.
More details about the setup of the experiment on Montezuma’s Revenge are elaborated as follows.
The experiment of RND with PPO in Burda et al. (2018) uses many more resources, such as 1024
parallel environments and runs 30,000 epochs for each environment. Parallel environments generate
experiences simultaneously and store them in the replay buffer. Our computing environment allows
at most 10 parallel environments. For the DQN-version of RND (called simply RND hereafter), we
use the same settings as Burda et al. (2018), such as observation normalization, intrinsic reward
normalization and random initialization. RND update probability is the proportion of experience in
the replay buffer that are used for training the intrinsic model f in RND (Burda et al., 2018). Here in
our experiment, we compare the performance under 0.125 and 0.25 RND update probability.
B Proof of results in Section 3.1
For brevity, we define n = T - 1.
We start by showing the following proposition that is used in the proofs.
Proposition 1. Let G(q, μ), q, and μ be defined as in Theorem 1. Thenfor any q ≥ 1/3, there exists
a μ that satisfies the constraint G(q, μ) < q.
11
Under review as a conference paper at ICLR 2021
Proof. Let us denote G1 =	|qf0 (x) - (1 - q)f1 (x)| dx, G2 =	|(1 - q)f0(x) - qf1(x)| dx.
Then we have
Gι(q,μ) = / ∣qfo(χ)-(i- q)fι(χ)l dχ
=	(qf0(x) - (1 - q)f1(x)) 1qf0(x)>(1-q)f1(x)dx
+	(-qf0(x) + (1 - q)f1(x)) 1qf0(x)<(1-q)f1(x)dx
=J (qfo(x) - (1 - q)fi(x)) 1χ<g(μ)dx + / (-qfo(x) + (1- q)fι(x)) 1χ>g(μ)dx
∖	∞ (	_X2	(x-μ)2 ∖
I dx + I —qe-^2 + (1 — q)e	2- 1 dx
g	Jg(μ) ∖	)
_ χ2
e-^W
where g(μ) = ɪ ∙ μ - log[q ). Similarly We get
G2(q,μ)
∕*g(μ) x2
(1-q)/ L
J-g(μ)
rg(μ)-μ
-q	e
J-g(μ)+μ
x2
2

It is easy to establish continuity of Gι(q, μ) and G2(q, μ) on [0, ∞), as well as the continuity of
G(q, μ). Indeed, we have
G""qq1-q)
μ = 0
μ - → oc .
Since q ≥ 1, then |1 - 2q| < q. From continuity of G(q, μ), there exists μo > 0 such that
G(q, μ) < q for any μ ≤ μ0.	□
Proof of Theorem 1. As in Assumption 1, let the inferior arm set be I and the superior one be S,
respectively, P(I) = q and P(S) = 1 - q. Arms in I follow f0(x) = N(0, 1) and arms in S follow
fι(χ) = N(μ, 1) where μ > 0. According to Assumption 1, at the first step the player pulls an arm
from either I or S and receives reward y1. At time step i > 1, the reward is yi and let bi represent a
policy of the player. We can always define bi as
b = 1 if the chosen arm at step i is not in the same arm set as the initial arm,
i 0 otherwise.
Let ai ∈ {0, 1} be the actual arm played at step i. It suffices to only specify ai is in arm set I (ai = 0)
or S (ai = 1) since the arms in I and S are identical. The connection between ai and bi is explicitly
given by bi = |ai - a1|. By Assumption 1, it is easy to argue that bi = Si0(y1, y2, ..., yi-1) for a set
of functions S20 , S30 , . . . , Sn0 , Sn0 +1. We proceed with the following lemma.
Lemma 1. Let the rewards of the arms in set I follow any L1 distribution f0(x) and in set S follow
any Li distribution fι(x) where the means satisfy μ(fι) > μ(fo). Let B be the number of arms
played in the game in set S. Let us assume the player meets Assumption 1. Then no matter what
strategy the player takes, we have
I E[B]-(1-q)∙(n+1)∣ ≤ E
where , T, f0 , f1 satisfy
G(q, f0, f1) + (1 - q)(n - 1)	|f0(x) - f1(x)| ≤ E,
12
Under review as a conference paper at ICLR 2021
G(q, fo, fl) = max{∕ ∖qf0(x) - (1 - q)fι(x)∣ dx, J |(1 - q)f0(x) - qfι(x)∣ dx}.
Proof. We have
E[B] = /
(αι + a +--+ an+i) fαι (yι) fa2 (y2)…fan (yn) dyidy2 . ∙∙dyn.
If ai = 0, then ai = bi and
E [B|ai = 0]=
/ (0 + b (yi：i) + ... + bn+i (yi：n)) fo (yi) fb2 (y2) ...fbn (Vn) dy1dy2 ...dyn.
If ai = 1, then 1 - ai = bi and
E [B∣aι
1]
J (1 + 1 - b2 (y1:I) + •…+ 1 - bn+1 (y1:n)) f1 (yI) ∙∙∙ f1-bn (yn) dy1dy2 ∙∙∙dyn∙
This gives US
E[B] = q ∙ E [B|ai = 0] + (1 — q) ∙ E [B|ai = 1]
=(1 - q)(n + 1)
+∕(b2
+ …+ bn+1) ∙ (q ∙ f0 (y1) ...fbn (yn)-
(1 - q) ∙ fi (yi)... fi-bn (yn)) dy1dy2. ..dyn.
By defining bi = 0, we have
E [B] = (1 - q) ∙ (n + 1)+
/(b2 +---+ bn+i)(q∙fbι (yi)...fbn (yn) - (1 - q) ∙ f1-b1 (yi)...fi-bn (yn))dy1dy2...ʤ
For any 1 ≤ m ≤ n we also derive
m
m
ɪɪ fbi (yi) - ɪɪ fi-bi (yi) dyidy2... dym
i=i
m-i
≤ Y
J i=i
r I m-1
fbi (yi) ∖fbm (ym) - fi-bm (yn)∖ dyidy2... dym+
m-1
∕∣∏fbi (yi) - ∏fi-bi (yi) ∣fi-bm
m	m Im-1
/ ∣f0(X)- fi(X)Idx + J I Y fbi (yi)
(ym) dyidy2 . . .dym
m-i
/ ∣f0(x) - fi(x)∣ dx + J I Y fbi (yi)
m-i
-ɪɪ fi-bi (yi) fi-bm (ym) dyidy2... dy
i=i
m-i
-ɪɪ fi-bi (yi) dyidy2 . ..dym-i
i=i
m-2
(1)
≤
/
/
m
Im
≤
2 ∙
∣f0(x) - fi(x)∣ dx + I I ɪɪ fbi (yi) - ɪɪ fi-bi (yi) dyidy2 . .. dym-2
i=i
≤
m
|f0(x) - fi(x)| .
13
Under review as a conference paper at ICLR 2021
This provides
E [B] -(1 — q) ∙(n +1)
n+1
n
n
≤ q ∙ Y
n
i=1
fbi(y^-(i- q) ∙ ɪɪ fι-bi (yi) dyιdy ∙ ∙ ∙ dyn
i=1
Zn-1
ɪɪ fbi(yi) |q ∙ fbn (yn) - (1 - q) ∙ fι-bn (yn)| dyιdy2 …dyn+
i=1
n-1
i=1
n-1
q) ∙ ɪɪ fbi(yi)-(1-q) ∙ ɪɪ fι-% (y，)fn (yn) dyιdy …dy
i=1
≤ max ∖ I |q ∙ fo(x)
-(1 - q) ∙ fι(x)∣ dx,/ 1(1 - q) ∙ fo(x) - q ∙ fι(x)∣ dx} +
(1 - q) /Y1 fbi
n-1
(yi) -	f1-bi (yi) dy1dy2 . . . dyn-1
i=1
≤ max S I |q ∙ fo(x)
-(1 - q) ∙ fι(x)∣ dx,/ 1(1 - q) ∙ fo(x) - q ∙ fι(x)∣ dx} +
(1 - q) ∙(n - 1) •/ |fo(X)- fι(X)I,
where the last inequality follows from (1).The statement of the lemma now follows.
According to Proposition 1, there is such μ satisfying the constraint G(q, μ) < q. Note that
G(q, μ) = G(q, fo, fι). Then We can choose E to be any quantity such that G(q, μ) < e < q. Finally,
there is T SatiSfying T ≤ (I-G∙R-G(X,μ)f1(X)I + 2 that gives US
G(q,μ) + (1 - q)(T - 2) R Ifo(x) - fl(x)I ≤ e∙
By choosing e, T, μ as above, by Lemma 1 we have
E[B] -(I- q) ∙ T
T	,
which is equivalent to E[B] < (1 - q + E) ∙ T. Therefore, regret RT satisfies, with A being the
number of arm pulls from I, inequality
Rr = X maχ(μk) - X E[yt] = Tμ - X E[yt] = Tμ - (E[B] ∙ μ + E[A] ∙ 0)
≥Tμ — (1 — q + E)μT = (q — e)μT.
This yields RT = inf sup RT ≥ (q — E) ∙ μT.	□
Theorem 2 follows from Theorem 1 and Proposition 1.
Proof of Theorem 3. The assumption here is the special case of Assumption 1 where there are two
arms and q = 1/2. Set I follows fo and S follows fι where μ(fo) < μ(fι).
In the same was as in the proof of Theorem 1 we obtain
RL(T) ≥ (1 -E) ∙ T ∙ μ
under the constraint that n/2 ∙ / |fo - fι∣ = n/2 ∙ TV(fo, fι) < E where TV stands for total variation.
Here we use G(1∕2, μ) = 1/2 ∙ TV(fo, fι). Setting E = 1/4 yields the statement.	□
n
□
14
Under review as a conference paper at ICLR 2021
In the Gaussian case it turns out that = 1/4 yields the highest bound. For total variation of Gaussian
variables N(μι,σ2 and N(μ2, σ2), Devroye et al. (2018) show that
TV (N (μι,σ2),N (〃2, σ2)) ≤ 3lσ2-σ2∣ + ⅛2l,
which in our case yields TV ≤ μ. From this we obtain μ ∙ T ≥ E and in turn RT ≥ e ∙ (2 - e). The
maximum of the right-hand side is obtained at E = ɪ. This justifies the choice of E in the proof of 3.
C Proof of results in Section 3.2
C.1 Proof for Theorem 4
Proof. Since the rewards can be unbounded in our setting, we consider truncating the reward with
any ∆ > 0 for any arm i by r； = 洛 + ^ where
rt = rt ∙ 1(-∆≤rt≤∆), rt = rt，1(∣rt∣>∆).
Then for any parameter 0 < η < 1, we choose such ∆ that satisfies
P(rt =尸t, i ≤ K)= P(-△ ≤ rt ≤ △,•..,-△ ≤ rK ≤ △)
=	. . . f(x1, . . . , xK)dx1 . . . dxK ≥ 1 - η .
-∆ -∆	-∆
(2)
The existence of such △ = ∆(η) follows from elementary calculus.
Let A = {|rit| ≤ △ for every i ≤ K, t ≤ T}. Then the probability of this event is
P(A) = P(r； = ft,i ≤ K,t ≤ T) ≥ (1 - η)T.
With probability (1 - η)T, the rewards of the player are bounded in [-△, △] throughout the game.
Then RT = PT=1(maxi ft - rt) ≤ T ∙ △ - PT=I rt is the regret under event A, i.e. RT = RT
with probability (1 - η)T. For the EXP3.P algorithm and RTB, for every δ > 0, according to Auer
et al. (2002b) we have
RT ≤ 4^
ZKT log( KT) + 4ʌ/5KT log K + 8log(
with probability 1 - δ.
Then we have
RT ≤ 4^(η) (JKTlog(KT) + 4 J3KTlogK + 8log(KT)) with probability (1 -δ)∙(1 -η)T.
□
C.2 Proof for Theorem 5
Lemma 2. For any non-decreasing differentiable function △ = △(T) > 0 satisfying
IimT→∞ /TT) = ∞,	IimT→∞ ^0(T) ≤ Co < ∞,
and any 0 < δ< 1, a > 2 we have
P (RT ≤ △(T) ∙ log(1∕δ) ∙ O*(√T))
≥ (1 - δ) (1 - Ta)
for any T large enough.
15
Under review as a conference paper at ICLR 2021
Proof. Let a > 2 and let US denote
F(y) = / f (x1,x2,..., xK)dx1dx2 . .. dxK,
-y
ζ (T )= F (∆(T) ∙ 1) - (1- T
for y ∈ RK and 1 = (1)..., 1) ∈ RK. Let also y-i = (y1,...,yi-1,yi+1,...,yκ) and x|g=y
(xι,...,xi-ι,y,xi+ι,...,xκ). Wehavelimτ→∞ ζ(T) = 0.
The gradient of F can be estimated as
VF
y-ι	「y-K
f (x∣xι=yι) dx2 ... dxK,..., /	f (x∣χκ=yκ) dxi ...dxK-1
y-1	-y-K
According to the chain rule and since ∆0(T) ≥ 0, we have
dF(∆(T) ∙ D ≤ 广(T)1-1 f (x∣x1=∆(T))dx2...dxK ∙∆0(T)+
dT	√-∆(T )-1-1
∕∆(T )-1-κ
...+	f (XIXK=∆(T)) dxι ...dxK-1 ∙ ∆0(T).
/-∆(T )-1-κ
Next we consider
∕∆(T )1-i
I	f (x∣χi=∆(T)) dxι ... dxi-ιdxi+ι .. . dxK
J-∆(T )1-i	2
C	r∆(T )1 -
e-2%(△(T)) +&△(T) ∙	eg(X-)dxι ...dxi-ιdxi+ι ...dxK.
J-∆(τ )1-i
Here eg(x-i) is the conditional density function given Xi
J^^TT)i，eg(x-i')dxι... dxi-ιdxi+ι... dxκ ≤ 1. We have
∆(T) and thus
/△(T )1-i
/	f (x∣Xi=∆(T)) dxι . .. dxi-idxi+ι ... dxK
/-∆(T )1-i
≤ e-2aii(∆(T))2+μz∆(T)
≤ e-1 minj ajj。(T ))2+maXj μj ∆(τ).
Then for T ≥ To we have ∆τ ≤ C0 + 1 and in turn
ζ0(T) ≤ (Co + 1) ∙ K ∙ e-1 minjajj°(T))2 +maχj *Q(T) _ a ∙ T-α-1
Since we only consider non-degenerate Gaussian bandits with min an > 0, μi are constants and
∆(T) → ∞ as T → ∞ according to the assumptions in Lemma 2, there exits Ci > 0 and Ti such
that
e-1 minjajj。(T))2+maχj *jA(T) ≤ g-Ci4(T)2 for every T > Ti.
~	1.	∆(T )2	.
Since IimT→∞ Iog(TJ = ∞, we have
∆(T )2 > 2¾+1) ∙ log(T) for T> T2.
16
Under review as a conference paper at ICLR 2021
These give us that
ζ(T)0 ≤ (C0+ 1)K e-2(a+1) log T - aT -a-1
= (C0+ 1)K e-2(a+1) log T - ae-(a+1) log T
< 0 for T ≥ T3 ≥ max(T0, T1 , T2).
This concludes that ζ0 (T ) < 0 for T ≥ T3. We also have limT →∞ ζ(T ) = 0 according to the
assumptions. Therefore, we finally arrive at ζ(T) > 0 for T ≥ T3. This is equivalent to
/△(T )∙1
J-∆(T )∙1
f (x1, . . . , xK) dx1 .
..dxK ≥ 1 - Ta,
i.e. the rewards are bounded by ∆(T) with probability 1 - T1a. Then by the same argument for T
large enough as in the proof of Theorem 4, we have
P (RT ≤ ∆(T) ∙ iog(1∕δ) ∙ O*(√T)) ≥ (1 - δ)(1 - T)t.
□
Proof of Theorem 5. In Lemma 2, we choose ∆(T) = log(T), which meets all of the assumptions.
The result now follows from log T ∙ O*(√T) = O*(√T), Lemma 2 and Theorem 4.	口
C.3 Proof for Theorem 6
We first list 3 known lemmas. The following lemma by Duchi (2009) provides a way to bound
deviations.
Lemma 3. For any function class F, and i.i.d. random variable {x1 , x2, . . . , xT}, the result
ExhsuPf∈F∣Eχf - T PT=If(Xt)∣i ≤ 2RT(F)
holds where RcT(F) = Ex,σ supf ∣1 PL1 σtf (xt)∣∣ and σt is a {-1, 1} random walk of t steps.
The following result holds according to Balcan (2011).
Lemma 4. For any subclass A ⊂ F, we have RT ≤ R(A, T) ∙ "Tg |A|, where R(A,T)=
suPf ∈A (PT=I f(xt))2 and RT = suPf ∣T PT=I σtf (xt)∣.
A random variable X is σ2-sub-Gaussian if for any t > 0, the tail probability satisfies
P(|X| > t) ≤ Be-σ2t2,
where B is a positive constant. The following lemma is listed in the Appendix A of Chatterjee (2014).
Lemma 5. For i.i.d. σ 2-sub-Gaussian random variables {Y1 , Y2, . . . , YT}, we have
E [maχ1≤t≤T |Yt|] ≤ σ√2 log T + √24σgT .
Proof for Theorem 6. Let us define F = {fj : x → xj |j = 1, 2, . . . , K}. Let xt = (r1t , r2t , . . . , rKt )
where rit is the reward of arm i at step t and let at be the arm selected at time t by EXP3.P. Then for
any fj ∈ F, fj(xt) = rjt. In Gaussian-MAB, {x1 , x2, . . . , xT} are i.i.d. random variables since the
Gaussian distribution N(μ, Σ) is invariant to time and independent of time. Then by Lemma 3, we
have
E hmaxi ∣μi - 1 Pt=1 rt∣i ≤ 2RT (F).
17
Under review as a conference paper at ICLR 2021
We consider
E [|R0T - RT |]
max μi
i
T
Dat
t=1
T
max	rit
i t=1
max μi
i
T
max	rit -
i t=1
T
T
X
t=1
T
X
t=1
ratt
ratt
max μi
i
iax	rit +E
max
t=1
T
t=1
T
μat
T
- X rat t
t=1
(3)
max
i
T ∙ μi
t=1
rit	+E
μat
t=1
T
- X ratt
t=1
E
E
≤
≤
≤
E
E
T ∙
T •
T •
—
—
—
—
—
—
—
i
2TRT (F) + 2TιRT1 (F) + …+ 2Tκ RTK (F)
where Ti is the number of pulls of arm i. Clearly T1 + T2 + . . . + TK = T. By Lemma 4 with
A = F we get
RT(F) = E [RT(F)] ≤ E[R(F,T)]
RT"(F) ≤ E [R (F,Ti)]∙，2Tg K
√2log K
-T-
i={1,2,,...,K}.
Since R(F, T) is increasing in T and Ti ≤ T, we have RTi(F) ≤ E [R (F,T)] ∙ V/2TgK
We next bound the expected deviation E [|R0T - RT|] based on (3) as follows
E [|RT - Rt∣] ≤ 2TE[R(F,T)] √2Tg K + X
≤ 2(K +1)P2logKE[R(F,T)].
Regarding E[R(F, T)], we have
2TiE[R(F,T )]72 Tog K
(4)
E[R(F, T)]
1~\
2
(5)
We next use Lemma 5 for any arm i. To this end let Yt = rit . Since xt are Gaussian, the marginals
Yt are also Gaussian with mean μ% and standard deviation of a% Combining this with the fact
that a Gaussian random variable is also σ2-sub-Gaussian justifies the use of the lemma. Thus
E hmaχι≤j≤τ|rj |i ≤ ai,i ∙ √2log T + √4aogT.
Continuing with equation 5 we further obtain
E[R(F,T)] ≤√T ∙ K ∙ max (ai,i,2log T + √2==
=(Kp2T log T + √4⅛) ∙ max a'-
(6)
18
Under review as a conference paper at ICLR 2021
By combining equation 4 and equation 6 we conclude
E [∖R°γ - Rt|] ≤ 2(K +1)√2log K ∙ max ai,i ∙ (k√2TlogT + √√T)	⑺
=O*(√T).
We now turn our attention to the expectation of regret E[Rt]. It can be written as
E [Rt ] = E
Rt≤O*(√T)] + E [RT 1Rτ>O*(√T)]
≤ O(√T )P (RT ≤ O(√T)) + e[Rt 1rt >o*(√t )] ≤ O (√T)+ e[Rt 1rt>o*(√T )]
=0*(VT) + E [RT 1O*(√T)<Rt<O*(√T)+E[Rt]] + E [RT 1Rt≥O*(√T)+E[Rt]] .	⑻
We consider δ = 1/√T and η = T-α for a > 2. We have
TimO(I- δ)(1 - n)T=TimO(I - δ)(1 - T)T
lim (1 - δ)(1-- )(Ta)'T = lim eT
T →∞ τ	T &	T →∞
and
lim (1 — (1 — δ)(1 — η)τ) ∙ logT ∙ T = lim (1 — eTTr) ∙ log(T) ∙ T
T→∞	T→∞
≤ lim log(T) ∙ T ∙ T1-α = lim T2-a ∙ log(T) = 0.
T →∞	T →∞
(9)
Let P1 = P (Rt ≤ log(1∕δ)O*(√T)) which equals to P (RT ≤ O*(√T)) since log(1∕δ)
log(√T) = O*(√T). By Theorem 5 we have P1 = (1 - δ) ∙ (1 - η)τ.
Note that E [Rt] ≤ C0 log(T) ∙ T as shown by
T
T
E [Rt ] = E
maxΣS rt- Era t
≤ 2E
_	t=1	t=1 _
K
≤ 2T ∙ ^X E [max 卜外]≤ 2T ∙
i=1
max ^X ∖rt∖ ≤ 2T ∙ E [max max ∖rt∖^l
. i t=1	」	i t
K (
E ai,i √2logT
i=1、
+	4αi,i
+ √l≡gT
_ A	/ L一—ɪ	4	∖
≤2τ ∙ ∑ max 叫(HgT+√og于)
≤。0 ∙ τ ∙ log(T)
for a constant C0.
The asymptotic behavior of the second term in equation 8 reads
E [RT 1O*(√T)<Rt<O*(√T) + E[Rt]] = E [RT 1Rt-O*(√T)∈(0,E[Rt])]
=E [(rt - o*(√t)) 1Rt-O*(√T)∈(0,E[Rt])] + o*(√t)
≤ E [Rt] P (RT
≤ E [Rt] P (RT
-o*(Vt) ∈ (0,e[Rt])) + o*(Vt)
-o*(Vt) > 0) + o*(Vt)
≤ C0 log(τ) ∙ τ ∙ (1 - P1) + o*( Vt ) = o*( √T)
where at the end we use equation 9.
19
Under review as a conference paper at ICLR 2021
Regarding the third term in equation 8, we note that R0T ≤ E[RT] by the Jensen’s inequality. By
using equation 7 and again equation 9 we obtain
E RTT 1Rt≥O* (√T)+E[Rt J
=Eh(RT- RT) 1(Rt-E[Rt])≥O*(√T)i + E
(RT-E[Rt ])≥O*(√T)]
≤ E [|RT - RT|] + RT ∙ P(RT ≥ E [RT] + O*(√T))
≤ E [|RT - RT|] + E [RT] ∙ P (RT ≥ E [RT] + O*(√T))
≤ O*(√T) + Co ∙ log(T) ∙ T ∙ P (RT ≥ O*(√T))
o*(Vt) + Co ∙ iog(τ) ∙ τ (i - Pi) = o*(√T).
Combining all these together We obtain E[RT] = O* (√T) which concludes the proof.	口
20