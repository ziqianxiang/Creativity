Under review as a conference paper at ICLR 2021
Latent Programmer: Discrete Latent Codes
for Program Synthesis
Anonymous authors
Paper under double-blind review
Ab stract
In many sequence learning tasks, such as program synthesis and document sum-
marization, a key problem is searching over a large space of possible output se-
quences. We propose to learn representations of the outputs that are specifically
meant for search: rich enough to specify the desired output but compact enough to
make search more efficient. Discrete latent codes are appealing for this purpose,
as they naturally allow sophisticated combinatorial search strategies. The latent
codes are learned using a self-supervised learning principle, in which first a dis-
crete autoencoder is trained on the output sequences, and then the resulting latent
codes are used as intermediate targets for the end-to-end sequence prediction task.
Based on these insights, we introduce the Latent Programmer, a program synthe-
sis method that first predicts a discrete latent code from input/output examples,
and then generates the program in the target language. We evaluate the Latent
Programmer on two domains: synthesis of string transformation programs, and
generation of programs from natural language descriptions. We demonstrate that
the discrete latent representation significantly improves synthesis accuracy.
1 Introduction
Our focus in this paper is program synthesis, one of the longstanding grand challenges of artificial
intelligence research (Manna & Waldinger, 1971; Summers, 1977). The objective of program syn-
thesis is to automatically write a program given a specification of its intended behavior, such as a
natural language description or a small set of input-output examples. Search is an especially diffi-
cult challenge within program synthesis (Alur et al., 2013; Gulwani et al., 2017), and many different
methods have been explored, including top-down search (Lee et al., 2018), bottom up search (Udupa
et al., 2013), beam search (Devlin et al., 2017), and many others (see Section 2).
We take a different philosophy: Can we learn a representation of programs specifically to help
search? A natural way of representing a program is as a sequence of source code tokens, but the
synthesis task requires searching over this representation, which can be difficult for longer, more
complex programs. A programmer often starts by specifying high-level components of a program
as a plan, then fills in the details of each component i.e. in string editing, a plan could be to extract
the first name, then the last initial. We propose to use a sequence of latent variable tokens, called
discrete latent codes, to represent such plans. Instead of having a fixed dictionary of codes, we let a
model discover and learn what latent codes are useful and how to infer them from specification.
Our hypothesis is that a discrete latent code - a sequence of discrete latent variables - can be a
useful representation for search (van den Oord et al., 2017; Roy et al., 2018; Kaiser et al., 2018).
This is because we can employ standard methods from discrete search, such as beam search, over
a compact space of high-level plans and then over programs conditioned on the plan, in a two-level
procedure. We posit that the high-level search can help to organize the search over programs. In
the string editing example earlier, a model could be confident that it needs to extract the last initial,
but is less sure about whether it needs to extract a first name. By changing one token in the latent
code, two-level search can explore alternative programs that do different things in the beginning.
Whereas in traditional single-level search, the model would need to change multi-token prefixes of
the alternatives, which is difficult to achieve in limited budget search.
We propose the Latent Programmer, a program synthesis method that uses learned discrete repre-
sentations to guide search via a two-level synthesis. The Latent Programmer is trained by a self-
1
Under review as a conference paper at ICLR 2021
Inputs	Outputs	Program
“Mason Smith"	“Smith M"	
“Henry Myers"	“Myers H"	GetToken_PROP_CASE_2 | Const(“ ") |
“Barry Underwood"	“Underwood B"	GetToken_ALL_CAPS_1
“Sandy Jones"	“Jones S"	
Figure 1: A string transformation task with 4 input-output examples a possible program in the string
transformation DSL that is consistent with the examples.
supervised learning principle. First a discrete autoencoder is trained on a set of programs to learn
discrete latent codes, and then an encoder is trained to map the specification of the synthesis task to
these latent codes. Finally, at inference time, Latent Programmer uses a two-level search. Given the
specification, the model first produces a L-best list of latent codes from the latent predictor, and uses
them to synthesize potential programs. On two different program synthesis domains, we find empir-
ically that the Latent Programmer improves synthesis accuracy by over 10% compared to standard
sequence-to-sequence baselines as RobustFill (Devlin et al., 2017). We also find that our method
improves diversity of predictions, as well as accuracy on long programs.
2	Background
Problem Setup The goal in program synthesis is to find a program in a given language that is
consistent with a specification. Formally, we are given a domain specific language (DSL) which
defines a space Y of programs. The task is described by a specification X ∈ X and is solved
by some, possibly multiple, unknown program(s) Y ∈ Y . For example, each specification can be
a set of input/output (I/O) examples denoted X = {(I1, O1), . . . (IN, ON)}. Then, we say that
we have solved specification X if we found a program Y which correctly solves all the examples:
Y (Ii) = Oi, ∀i = 1, . . . , N. As another example, each specification can be a natural language
description of a task, and the corresponding program implements said task. An example string
transformation synthesis task with four I/O examples together with a potential correct program in
the string transformation DSL is shown in Figure 1.
Vector Quantization Traditionally, neural program synthesis techniques process the input specifi-
cation as a set of sequences and predicts the output program token-by-token (Devlin et al., 2017). In
this work, we present a new approach for synthesis that performs structured planning in latent space
using a discrete code. We conjecture that programs have an underlying discrete structure; specif-
ically, programs are compositional and modular with components that get reused across different
problems. Our approach leverages this structure to guide the search over large program spaces. Fol-
lowing works in computer vision (van den Oord et al., 2017; Roy et al., 2018), we discover such
discrete structure by using a Vector Quantized Variational Autoencoder (VQ-VAE). VQ-VAEs work
by feeding the intermediate representation of an autoencoder through a discretization bottleneck
(van den Oord et al., 2017). For completeness, we provide background on VQ-VAEs below.
In a VQ-VAE, latent codes are drawn from a discrete set of learned vectors c ∈ RK×D, or codebook.
Each element in the codebook can be viewed as either a token with id k ∈ [K] or as an embedding
ck ∈ RD . To generate the discrete codes, the continuous autoencoder output e is quantized via
nearest-neighbor lookup into the codebook. Formally, the token id qk(e) and quantized embedding
qc(e) are defined as
qc(e)
cqk(e) where qk(e)
arg km∈[iKn] ||e - ck||2.
(1)
For input x, the training loss for a VQ-VAE consists of: a reconstruction loss for the encoder-decoder
weights, a codebook loss that encourages codebook embeddings to be close to the continuous vectors
which are quantized to them, and a commitment loss that encourages the encoded input ec(x) to
"commit" to codes i.e. not switch which discrete code it is quantized to. The loss is given by,
L(c, θ, Φ) = logPθ (X | qc(ecφ(x))) + ∣∣sg(ecφ(x)) - c)||2 + β∣∣sg(c) - ecφ(x)∣∣2,	(2)
where θ,φ are the parameters of the decoder and encoder, respectively, sg(∙) is the stop gradient
operator that fixes the operand from being updated by gradients, and β controls the strength of the
commitment loss. To stabilize training, van den Oord et al. (2017) also proposed removing the
codebook loss and set the codebook to an exponential moving average (EMA) of encoded inputs.
2
Under review as a conference paper at ICLR 2021
Figure 2: High-level architecture for the Latent Programmer system. The latent predictor generates
probabilities over latent sequences, which can be decoded into a predicted latent sequence Z0 . Z0 is
fitted to a ground-truth latent sequence Z generated by a program encoder, and used during decoding
to by the latent program decoder to generate programs.
3	Synthesis with Discrete Latent Variables
We propose a two-level hierarchical approach to program synthesis that first performs high-level
planning over an intermediate sequence, which is then used for fine-grained generation of the pro-
gram. In our approach, a top-level module first infers a latent code, which gets used by a low-level
module to generate the final program.
3.1	Hierarchy of Two Transformers
Our proposed Latent Programmer (LP) architecture consists of two Transformers in a two-level
structure. The architecture comprises of two modules: a latent predictor which produces a latent
code, which can be interpreted as a course sketch of the program, and a latent program decoder,
which generates a program conditioned on the code. The latent code consists of discrete latent
variables as tokens, which we arbitrarily denote TOK_1,..., TOK_K, whose meanings are as-
signed during training. Both components use a Transformer architecture due to their impressive
performance on natural language tasks (Vaswani et al., 2017).
To help the model assign useful meanings to the latents, we also leverage a program encoder,
which is only used during training. The program encoder ec(Y ) encodes the true program Y =
[y1, y2, . . . , yT] into a shorter sequence of discrete latent variables Z = [z1, z2, . . . , zS], represented
as codebook entries; that is, each zi ∈ RD is one of K entries in a codebook c. The latent sequence
serves as the ground-truth high-level plan for the task. The function ec(Y ) is a Transformer encoder,
followed by a stack of convolutions of stride 2, each halving the size of the sequence. We apply the
convolution ' times, which reduces a T-length program to a latent sequence of length「T/2'J. This
provides temporal abstraction, since the high-level planning actions are made only every 2' steps.
In summary, the program encoder is given by
ec(Y) — h`;	hm — Conv(hm-ι) for m ∈ 1...'; ho — TransformerEncoder(Y).	(3)
Here TransformerEncoder(∙) applies a stack of self-attention and feed-forward units on input em-
beddings via a residual path, described in detail by Vaswani et al. (2017). This will be used, along
with the latent program decoder, as an autoencoder during training (see Section 3.2).
The latent predictor lp(X) autoregressively predicts a coarse latent code lp(X) ∈ RS×K, con-
ditioned on the program specification X . The latent predictor outputs a sequence of probabilities,
which can be decoded using search algorithms such as beam search to generate a predicted latent
code Z0 . This is different than the program encoder, which outputs a single sequence Z, because
we use the latent predictor to organize search over latent codes; at test time, we will obtain a L-best
list of latent token sequences from lp(X). The latent predictor is given by a stack of Transformer
blocks with the specification X as inputs.
Similarly, the latent program decoder d(Z, X) defines an autoregressive distribution over program
tokens given the specification X and the coarse plan Z ∈ RS ×K , represented as codebook entries.
The decoder is a Transformer that jointly attends to the latent sequence and program specification.
This is performed via two separate attention modules, whose outputs are concatenated into the hid-
den unit. Formally, given a partially generated program Y0 = [y10 , y20 , . . . , yt0-1], and the encoded
3
Under review as a conference paper at ICLR 2021
specification E = TransformerEncoder(X), the latent program decoder performs
ht = Concat (TransformerDecoder(Y 0, E)t-1, TransformerDecoder(Y 0, Z)t-1) ,	(4)
where TransformerDecoder(x, y) denotes a Transformer decoder applied to outputs y while at-
tending to inputs encoding x, and the subscript indexes an entry in the resulting output sequence.
Finally, the distribution over output token k is given by dt(Z, X) = Softmax (W(ht)) , where W is
a learned parameter matrix. Finally, the latent program decoder defines a distribution over programs
autoregressively as p(Y |Z,X) = Qtp(yt|y<t, Z,X), where p(yt|y<t, Z,X) = dt(Z, X). When
X is multiple I/O examples, each example is encoded as Ei = TransformerDecoder(Ii, Oi). Then,
a separate hidden state per I/O is computed following equation 4, followed by a late max-pool to
get the final hidden state. Note that the program encoder and latent program decoder make up a
VQ-VAE model of programs, with additional conditioning on the specification.
The complete LP architecture is summarized in Figure 2, and an end-to-end example run of our
architecture is shown in Figure 4.
3.2	Training
Our LP performs program synthesis using a two-level search, first over latent sequences then over
programs. Given program specification, we want to train our latent predictor to produce an informa-
tive latent sequence from which our latent program decoder can accurately predict the true program.
Our training loss for the LP model consists of three supervised objectives.
The autoencoder loss ensures that the latent codes contain information about the program. It is a
summation of the reconstruction loss between the autoencoder output d(qc(Y ), X) and true program
Y , as well as a commitment loss to train the encoder output ec(Y ) to be close to codebook c. Like
in Roy et al. (2018), codebook is not trained but set to the EMA of encoder outputs. This loss is
similar to the loss function of a VQ-VAE as in equation 2, but also depends on specification X . This
objective trains the latent tokens in the codebook so that they correspond to informative high-level
actions, as well as make sure our latent program decoder can accurately recover true program given
the specification and a plan comprising of such actions.
The latent prediction loss ensures that latent codes can be predicted from specifications. It is a re-
construction loss between the distribution over latents predicted from the specification lp(X) and the
autoencoded latents qk(ec(Y )) from the ground-truth program. This is a self-supervised approach
that treats the autoencoded latent sequence as the ground-truth high-level plan, and trains the latent
predictor to generate the plan using just the program specification X . Note that the program encoder
is only used in training, as at test time ec(Y ) is unknown, so the LP model uses lp(X) instead.
Finally, the end-to-end loss ensures that programs can be predicted from specifications. This is
especially important because in the reconstruction loss, the latent program decoder receives as input
latent codes from the autoencoded latent sequences ec(Y ), whereas at test time, the decoder receives
a latent code from the latent predictor lp(X). This can result in mistakes in the generated program
since the decoder has never been exposed to noisy results from the latent predictor. The end-to-end
loss alleviates this issue. The end-to-end loss is probability of the correct program Y when predicted
from a soft-quantized latent code, given by lp(X)Tc. This has the added benefit of allowing gradient
to flow through the latent predictor, training it in an end-to-end way.
In summary, the full loss for a training instance is
L(c, θ, Φ,Ψ) = logPθ (Y | qc(eCφ(Y)),X)+ β∣∣sg(c) - eCφ(Y)||2
'-------------------------V------------------------}
autoencoder
+ logP (qk(ecφ(Y)) | lpψ(X)) + logPθ (Y | lpψ(X)Tc,X)	(5)
、--------------V-------------} 、-------------{-------------}
latent prediction	end-to-end
where we explicitly list out θ, φ, and ψ representing the parameters of the latent program decoder,
program encoder, and latent decoder respectively.
Furthermore, for the first 10K steps of training, we give embeddings of the ground-truth program Y ,
averaged over every 2' tokens, as the latent sequence instead of ec(Y). This pre-training ensures that
initially, the latent code carries some information about the program so that the attention to the code
4
Under review as a conference paper at ICLR 2021
has reasonable gradients that can then to propagated to the program encoder afterward pre-training.
Doing this was empirically shown to prevent the bypassing phenomenon where the latent code is
ignored during decoding (Bahuleyan et al., 2017).
3.3	Inference
During inference, we use a multi-level variant of beam search to decode the output probabilities of
our LP model. Standard beam search with beam B will generate the top-B most likely programs ac-
cording to the model, and find the first one (if any) that is consistent with the specification (Parisotto
et al., 2017; Devlin et al., 2017). In our case, we first perform beam search for L latent beams,
then for bB/Lc programs per latent sequence. Note that during inference, the latent predictor will
continue to generate latent tokens until an end-of-sequence token is produced. This means that
the generated latent sequence does not necessarily satisfy having lengthdT/2'e as during training;
however, we found the latent sequence lengths during training and evaluation to be close in practice.
Setting L = B allows for the maximum exploration of the latent space, while setting L = 1 reduces
our method to standard beam search, or exploitation of the most likely latent decoding. We choose
L = √B in our experiments, but explore the effect of various choices of L in Section 5.2.
4	Related Work
Program Synthesis Our work deals with program synthesis, which involves combinatorial search
for programs that match a specification. Many different search methods have been explored within
program synthesis, including search within a version-space algebra (Gulwani, 2011), bottom-up
enumerative search (Udupa et al., 2013), stochastic search (Schkufza et al., 2013), genetic program-
ming (Koza, 1994), or reducing the synthesis problem to logical satisfiability (Solar-Lezama et al.,
2006). Neural program synthesis involves learning neural networks to predict function distributions
to guide a synthesizer (Balog et al., 2017), or the program autoregressively in an end-to-end fashion
(Parisotto et al., 2017; Devlin et al., 2017). SketchAdapt (Nye et al., 2019) combined these ap-
proaches by first generating a program sketch with holes, and then filling holes using a conventional
synthesizer. Related to our work, DreamCoder (Ellis et al., 2020) iteratively builds a sketches us-
ing progressively more complicated primitives though a wake-sleep algorithm. Our work is closely
related in spirit but fundamentally differs in two ways: (1) our sketches are comprised of a general
latent vocabulary that is learned in a simple, self-supervised fashion, and (2) our method avoids
enumerative search, which is prohibitively expensive for large program spaces. There is also a line
of work that deals with learning to process partial programs in addition to the specification. In
execution-guided program synthesis, the model guides iterative extensions of the partial programs
until a matching one is found (Zohar & Wolf, 2018; Chen et al., 2019; Ellis et al., 2019). Balog et al.
(2020) of late proposed a differentiable fixer that is trained to iteratively edit incorrect programs. We
treat these works as complementary, and can be combined with ours to refine predictions.
Discrete Latent Bottlenecks Variational autoencoders (VAE) were first introduced using contin-
uous latent representations (Kingma & Welling, 2014; Rezende et al., 2014). Several promising
approaches were proposed to use discrete bottlenecks instead, such as continuous relaxations of cat-
egorical distributions i.e. the Gumbel-Softmax reparametrization trick (Jang et al., 2017; Maddison
et al., 2017). Recently, VQ-VAEs using nearest-neighbor search on a learned codebook (see Sec-
tion 2 for more details) achieved impressive results almost matching continuous VAEs (van den
Oord et al., 2017; Roy et al., 2018). Discrete bottlenecks have also been used for sentence com-
pression (Miao & Blunsom, 2016) and text generation (Puduppully et al., 2019), but these works
does not learn the semantics of the latent codes, like ours does. Within the domain of synthesis
of chemical molecules, Gdmez-Bombarelli et al. (2018) have applied Bayesian optimization within
a continuous latent space to guide this structured prediction problem. Learning to search has also
been considered in the structured prediction literature (DaUm6 et al., 2009; Chang et al., 2015; Ross
et al., 2011), but to our knowledge, these works do not consider the problem of learning a discrete
representation for search. Notably, VQ-VAE methods have been successfully used to encode natural
language into discrete codes for faster decoding in machine translation (Kaiser et al., 2018). Our
work similarly uses a VQ-VAE to learn a discrete code, but we use the learned code in a two-level
search that improves accuracy. To do so, we propose a model that is autoregressive on both the la-
tent and program space, and perform two-level beam search on latent codes and programs. The key
novelty behind our work is that first searching over a learned discrete latent space can assist search
over the complex program space; using a VQ-VAE as Kaiser et al. (2018) did enables us to do so.
5
Under review as a conference paper at ICLR 2021
5	Experiments
We now present the results of evaluating our Latent Programmer model in two test domains: syn-
thesis of string transformation programs from examples and code generation from natural language
descriptions. We compare our LP model against several strong baselines.
RobustFill [LSTM] is a seq-to-seq LSTM with attention on the input specification, and trained to
autoregressively predict the true program. The architecture is comparable to the RobustFill model
designed originally for the string transformation tasks in our first domain (Devlin et al., 2017), but
easily generalizes to all program synthesis domains. We detail the architecture in Appendix A.
RobustFill [Transformer] alternatively uses a Transformer architecture, equivalent in architecture
to the latent planner in our LP model, also trained to autoregressively predict the program. Trans-
formers were found to perform much better than LSTMs in language tasks because they process the
entire input as a whole, and have no risk of forgetting past dependencies (Vaswani et al., 2017). This
baseline can be also be considered of an ablation of our LP model without any latent codes.
The central novelty of our work is in realizing that by learning a discrete representation, we can
perform structured search on two levels. We introduce two ablative baselines, which replace the
VQ-VAE with either a generic autoencoder or a VAE. In both cases the latent space is continuous,
and well-known combinatorial search algorithms such as beam search cannot search over the space.
Latent RobustFill [AE] replaces the VQ-VAE component of our LP model with a generic autoen-
coder. This makes the latent code a sequence of continuous embeddings. The latent prediction loss
in equation 5 is simply replaced by a squared error between the output of the autoencoder and the
latent predictor. Performing beam search over the continuous latent space is intractable, so during
inference we generate only one latent sequence per task; this is equivalent to two-level beam search
described earlier with L = 1. In addition, because we cannot define an end-of-sequence token
in the latent space, this baseline must be given knowledge of the true program length even during
inference, and always generates a latent sequence of lengthdT/2'].
Latent RobustFill [VAE] substitutes the VQ-VAE component with a VAE (Kingma & Welling,
2014). This again produces a continuous latent space, but regularized to be distributed approximately
as a standard Gaussian. Performing beam search is still intractable, but we can sample L latent
sequences from the Gaussians determined by the VAE, and perform beam search on the programs
afterwards. Again, we assume that the true program length is known during inference.
5.1	String Transformation
The first test domain is a string transformation DSL frequently studied in the program synthesis
literature (Parisotto et al., 2017; Devlin et al., 2017; Balog et al., 2020). Tasks in this domain involve
finding a program which maps a set of input strings to a corresponding set of outputs. Programs in
the DSL are a concatenation of expressions that perform regex-based string transformations (see
Appendix A for the full DSL).
We perform experiments on a syn-
thetic dataset generated by sampling
programs from the DSL, then the cor-
responding I/O examples using an
heuristic similar to the one used in
NSPS (Parisotto et al., 2017) and Ro-
bustFill (Devlin et al., 2017) to en-
sure nonempty output for each input.
We consider programs comprising of
a concatenation of up to 10 expres-
sions and limit the lengths of strings
Method		Accuracy	
	B=1	10	100
RobustFill [LSTM]	45%	49%	61%
RobustFill [Transformer]	47%	51%	61%
Latent RobustFill [AE]	47%	50%	60%
Latent RobustFill [VAE]	46%	51%	62%
Latent Programmer	51%	57%	68%
Table 1: Accuracy on string transformation domain.
in the I/O to be at most 100 characters. All models have an embedding size of 128 and hidden size
of 512, and the attention layers consist of 3 stacked layers with 4 heads each. For the LP model,
we used a latent compression factor ` = 2 and vocabulary size K = 40. The models are trained on
roughly 25M tasks, and evaluated on 1K held-out ones.
In Table 1, We report the accuracy-the number of time a program was found conforming to the UO
examples-of our method against the baselines. Across all beam sizes, our LP model performed 5-7
6
Under review as a conference paper at ICLR 2021
Aue.lnuuq UOREP=EA OrtiESm
4e6	le7 2e7
Trainable Parameters
—∙- RobustFill [Transformer] →⅛- Latent Programmer
Length RobustFill Acc. LP Acc.
1	94.5%	94.0%
2	83.9%	84.6%
3	72.8%	72.2%
4	63.1%	66.1%
5	47.1%	49.8%
6	40.6%	43.0%
7	30.2%	34.6%
8	22.7%	28.4%
9	18.6%	27.0%
10	14.4%	25.6%
(b)
2e6
-Λr- RobustFiII [LSTM]
(a)
Figure 3: (a): Influence of hidden size on beam-10 accuracy. (b): Beam-10 accuracy of baseline
transformer and LP by ground truth program length.
percentage points better (over 10% of baseline accuracy) than the next best model. From our ablative
study, we see that having two-level using discrete latent codes was important, as the baselines over
continuous latent spaces performed comparably to the traditional RobustFill model.
Recently, SketchAdapt also proposed
two-level search (Nye et al., 2019),
but in the top-level, it performs
beam search over program space aug-
mented with a HOLE token. In con-
strast, our method searches over a
learned, general latent space. During
low-level search, SketchAdapt enu-
merates partial programs to co-opt
the HOLE tokens using a learned syn-
Method	Accuracy	
	B=50	100
SketchAdaPt (Nye et al., 2019)	63%	64%
Latent Programmer	64%	67%
Table 2: Accuracy on string transformation domain of Nye
et al. (2019). SketchAdapt results are from Nye et al. (2019)
with 3, 000 synthesized programs (similar wall clock time).
thesizer similar to DeepCoder (Balog et al., 2017), whereas we again perform beam search. To
compare the two, we evaluate our LP model on samples generated according to Nye et al. (2019),
which slightly modifies the DSL to increase the performance of synthesizers, and report results
in Table 2. Since enumeration can be done more quickly than beam search, we let SketchAdapt
synthesize 3, 000 programs using B top-level beams, whereas our LP model can only generate B
programs. Our LP model is able to outperform SketchAdapt even in the modified DSL.
5.2	Analysis
We conduct extensive analysis to better understand our LP model in terms of learning, the ability to
generate long programs, and diversity in the beams. All results are reported with beam size B = 10.
Model Size Our LP model uses an additional latent code for decoding, which introduces additional
parameters into the model than the baseline RobustFill model. To make a fair comparison, we vary
the embedding and hidden dimension of all of our evaluated methods, and compare the effect of the
number of trainable parameters on the accuracy. Figure 3(a) shows that all methods respond well to
an increase in model size. Nevertheless, we see that even when normalized for size, our LP model
outperforms baselines by a significant margin.
Program Length Prior work has shown that program length is a reasonable proxy measure of
problem difficulty. We hypothesize that using latent codes is most beneficial when generating long
programs. Figure 3(b) shows how ground-truth program length affects the accuracy of our LP model
compared to RobustFill, which lacks latent codes. As expected, accuracy decreases with problem
complexity. Perhaps surprisingly, though, we see a large improvement in our LP model’s ability to
handle more complex problems. In Figure 4, we also show an illustrative example in the domain
where our LP model found a valid program whereas the RobustFill model did not. In this example,
the ground-truth program was long but had a repetitive underlying structure. Our LP model correctly
detected this structure, as evidenced by the predicted latent sequence. We show additional examples
7
Under review as a conference paper at ICLR 2021
Inputs	Outputs		Program			
“Jacob,Ethan,James 11"		“11:J.E.J."	GetToken	_NUMBER_1		| Const(:) |
“Elijah,Daniel,Aiden 3162"		“3162:E.D.A"	GetToken	_ALL	_CAPS_1	| Const(.) |
“Rick,Oliver,Mia 26"		“26:R.O.M."	GetToken	_ALL	_CAPS_2	| Const(.) |
“Mark,Ben,Sam 510"		“510:M.B.S."	GetToken	_ALL	_CAPS_3	| Const(.)
RobustFill	GetAll_NUMBER	| Const(:)|	GetToken_	ALL_	CAPS_2	| Const(.)
LP	GetAll_NUMBER	| Const(:)	| GetToken_ALL_CAPS_			1 | Const(.) |
	GetToken_ALL_	CAPS_2 | Const(.) | GetToken_ALL				_CAPS_-1 | Const(
LP Latent	TOK_14 | TOK_	36 | TOK_36	| TOK_36			
Figure 4: Illustrative string transformation problem where the ground-truth program was long but
had repetitive structure. The baseline Transformer was unable to generate the program but our LP
model, which first predicts a coarse latent sequence, was able to.
	2'	Accuracy
		
Distinct n-Grams Latent Beam SiZe Accuracy 		2	^^52%
n=1	2	3	4	4	55%
L=1	52%	0.13	0.23	0.26 0.28	8	49%
2	55%	0.13	0.24 0.26 0.28		—
3	57%	0.14	0.25 0.28 0.31	K	Accuracy
5	57%	0.14	0.26 0.29 0.32	10	~^48%
10	56%	0.14	0.26 0.30 0.33	40	55%
	100	51%
(a)		—
(b)
Figure 5:	(a): Effect of latent beam size on beam-10 accuracy and number of distinct n-grams
(normalized by total number of tokens). (b): Effect of latent length compression ` and vocabulary
size K on beam-10 accuracy.
in Figure 9 of Appendix B. It is important to note that our method allows tokens in the discrete
latent code to have arbitrary meaning, yielding rich and expressive latent representations. However,
the trade-off is that because the latent codes were not grounded, it is difficult to objectively interpret
the latent codes. Grounding the latent space to induce interpretability is an avenue for future work.
Latent Beam Size In multi-level beam search of beam size B, first L latent beams are decoded,
then bB/Lc programs per latent sequence. The latent beam size L controls how much search is
performed over latent space. We theorize that higher L will produce more diverse beams; however,
too high L can be harmful in missing programs with high joint log-probability. We show the effect
of latent beam size on both the beam-10 accuracy and a proxy measure for diversity. Following prior
work, we measure diversity by counting the number of distinct n-grams in the beams, normalized
by the total number of tokens to bias against long programs (Vijayakumar et al., 2018). We report
the results varying L for B = 10 in Figure 5(a). As expected, increasing the latent beam size
L improves diversity of output programs, but excessively large L harms the final accuracy. An
important observation is that the L = 1 case, where one latent code is used to decode all programs,
performs similarly to baseline RobustFill. In this extreme, no search is performed over the latent
space, and our proposed two-level search reduces to only searching over programs; this is further
evidence that explicitly having two-level search is critical to the LP model’s improved performance.
Latent Length and Vocabulary Size Since the discretization bottleneck is a critical component
in generating latent codes in our LP model, we also investigate its performance in conjunction with
different settings of hyperparameters. Two important variables for the VQ are the latent length
compression factor c, and size of latent vocabulary K. If c is too small, the latent space becomes too
large to search; on the other hard, too large c can mean individual latent tokens cannot encoded the
information needed to reconstruct the program. Similarly, we expect that too small of a vocabulary
K can limit the expressiveness of the latent space, but too large K can make predicting the correct
latent code too difficult. We confirm this in our evaluations in Figure 5(b) and Figure 5(c).
8
Under review as a conference paper at ICLR 2021
Docstring	Program
get an environment variable	def getenv(key, default=None): return environ.get(key, default)
return a list of the words	def split(s, sep=None, maxsplit=-1):
in the string s	return s.split(sep, maxsplit)
Figure 6:	Example problems from the Python code generation dataset.
5.3 Python Code Generation
Our next test domain is a Python code generation (CG) task, which involves generating code for a
function that implements a natural-language specification. The dataset used consists of 111K python
examples, which consist of a docstring and corresponding code snippet, collected from Github (Wan
et al., 2018). An example docstring and program from the dataset is shown in Figure 6.
We used a language-independent tokenizer jointly on data (Kudo & Richardson, 2018), and pro-
cessed the dataset into a vocabulary of 35K sub-word tokens. Furthermore, following Wei et al.
(2019), we set the maximum length of the programs to be 150 tokens resulting in 85K examples.
Across all models, we set the embedding size to be 256 and hidden size to be 512, and the atten-
tion layers consist of 6 stacked layers with 16 heads each, similar to in neural machine translation
(Vaswani et al., 2017). For the LP model, we used a latent compression factor c = 2 and vocabulary
size K = 400 after a hyperparameter search. The models are evaluated on 1K held-out examples.
We initially found that it was difficult for the program encoder to detect latent sequence structure
in the ground-truth programs as is due to the noise in variable names. To remedy this, we used an
abstract syntax tree (AST) parser on the ground-truth programs to replace the i-th function argument
and variable appearing the program with the token ARG_i and VAR_i, respectively. This was only
used in training the program encoder and did not impact evaluation.
Method	BLEU			We evaluate performance by comput- ing the best BLEU score among the output beams (Papineni et al., 2002). We computed BLEU as the geometric mean of n-gram matching precision scores up to n = 4. Table 3 shows
	B=1	10	100	
Base (Wei etal., 2019) Dual (Wei etal., 2019)	10.4 12.1	- -	- -	
RobustFill [LSTM] RobustFill [Transformer] Latent Programmer	11.4 12.1 14.0	14.8 15.5 18.6	16.0 17.2 21.3	that our LP model outperforms the baselines. From the results, it can be seen that this is a difficult task, which may be due to the ambiguity in speci- fying code from a short docstring de- scription. As evidence, we addition-
Table 3: BLEU score on code generation task.				
ally include results from a recent work that proposed seq-to-seq CG models on the same data that
performed similar to our baselines (Wei et al., 2019). These results show that improvements due to
the LP model exist even in difficult CG domains. For example docstrings and code generated by the
LP Model, refer to Figure 9 in Appendix B.
6 Conclusion
In this work we proposed the Latent Programmer (LP), a novel neural program synthesis technique
that leverages a structured latent sequences to guide search. The LP model consists of a latent predic-
tor, which maps the input specification to a sequence of discrete latent variables, and a latent program
decoder that generates a program token-by-token while attending to the latent sequence. The latent
predictor was trained via a self-supervised method in which a discrete autoencoder of programs was
learned using a discrete bottleneck, specifically a VQ-VAE (van den Oord et al., 2017), and the
latent predictor tries to predict the autoencoded sequence as if it were the ground-truth. During in-
ference, the LP model first searches in latent space for discrete codes, then conditions on those codes
to search over programs. Empirically, we showed that the Latent Programmer outperforms state-of-
the-art baselines as Robustfill (Devlin et al., 2017), which ignore latent structure. Exciting future
avenues of investigation include achieving better performance by grounding the latent vocabulary
and generalizing our method to other tasks in natural language and structured prediction.
9
Under review as a conference paper at ICLR 2021
References
Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo M. K. Martin, Mukund Raghothaman, Sanjit A.
Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-
guided synthesis. In Formal Methods in Computer-Aided Design, FMCAD 2013, Portland, OR,
USA, October20-23, 2013, pp. 1-8. IEEE, 2013.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations (ICLR),
2016.
Hareesh Bahuleyan, Lili Mou, Olga Vechtomova, and Pascal Poupart. Variational attention for
sequence-to-sequence models. CoRR, abs/1712.08207, 2017.
Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow.
Deepcoder: Learning to write programs. In International Conference on Learning Representa-
tions (ICLR), 2017.
Matej Balog, Rishabh Singh, Petros Maniatis, and Charles Sutton. Neural program synthesis with a
differentiable fixer. CoRR, abs/2006.10924, 2020. URL https://arxiv.org/abs/2006.
10924.
Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Daume III, and John Langford. Learning
to search better than your teacher. In International Conference on Machine Learning (ICML),
2015.
Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In Interna-
tional Conference on Learning Representations (ICLR), 2019.
Hal Daum6, III, John Langford, and Daniel Marcu. Search-based structured prediction. Machine
Learning Journal, 2009.
Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and
Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. CoRR, abs/1703.07469,
2017. URL http://arxiv.org/abs/1703.07469.
Kevin Ellis, Maxwell I. Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama.
Write, execute, assess: Program synthesis with a REPL. In Neural Information Processing Sys-
tems (NeurIPS), 2019.
Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lucas Morales, Luke
Hewitt, Armando Solar-Lezama, and Joshua B. Tenenbaum. Dreamcoder: Growing generalizable,
interpretable knowledge with wake-sleep bayesian program learning. CoRR, abs/2006.08381,
2020. URL https://arxiv.org/abs/2006.08381.
Rafael Gdmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Herndndez-Lobato,
Benjamin SdnChez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Aldn Aspuru-Guzik. Automatic chemical design using a Data-Driven con-
tinuous representation of molecules. ACS Cent Sci, 4(2):268-276, February 2018.
Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. In
PoPL’11, January 26-28, 2011, Austin, Texas, USA, 2011.
Sumit Gulwani, Oleksandr Polozov, and Rishabh Singh. Program synthesis. Foundations and Trends
in Programming Languages, 4(1-2):1-119, 2017. doi: 10.1561/2500000010. URL https:
//doi.org/10.1561/2500000010.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
International Conference on Learning Representations (ICLR), 2017.
Eukasz Kaiser, Aurko Roy, Ashish Vaswani, Niki Parmar, Samy Bengio, Jakob Uszkoreit, and Noam
Shazeer. Fast decoding in sequence models using discrete latent variables. In International Con-
ference on Machine Learning (ICML), 2018.
10
Under review as a conference paper at ICLR 2021
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR), 2014.
John R Koza. Genetic programming as a means for programming computers by natural selection.
Statistics and computing, 4(2):87-112,1994.
Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, November
2018.
Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. Accelerating search-based program syn-
thesis using learned probabilistic models. In Conference on Programming Language Design and
Implementation (PLDI), pp. 436-449, June 2018.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. In International Conference on Learning Representations
(ICLR), 2017.
Zohar Manna and Richard J. Waldinger. Toward automatic program synthesis. Commun. ACM, 14
(3):151-165, 1971.
Yishu Miao and Phil Blunsom. Language as a latent variable: Discrete generative models for sen-
tence compression. CoRR, abs/1609.07317, 2016. URL http://arxiv.org/abs/1609.
07317.
Maxwell I. Nye, Luke B. Hewitt, Joshua B. Tenenbaum, and Armando Solar-Lezama. Learning to
infer program sketches. In International Conference on Machine Learning (ICML), 2019.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic
evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for
Computational Linguistics, pp. 311-318. Association for Computational Linguistics, 2002.
Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Push-
meet Kohli. Neuro-symbolic program synthesis. In International Conference on Learning Rep-
resentations (ICLR), 2017.
Ratish Puduppully, Li Dong, and Mirella Lapata. Data-to-text generation with content selection and
planning. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-
First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI
Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii,
USA, January 27 - February 1, 2019, pp. 6908-6915. AAAI Press, 2019. doi: 10.1609/aaai.
v33i01.33016908. URL https://doi.org/10.1609/aaai.v33i01.33016908.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. CoRR, abs/1401.4082, 2014. URL https:
//arxiv.org/abs/1401.4082.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and struc-
tured prediction to No-Regret online learning. In Geoffrey Gordon, David Dunson, and Miroslav
DUdik (eds.), Conference on Artificial Intelligence and Statistics (AISTATS), volume 15 of Pro-
ceedings of Machine Learning Research, pp. 627-635, Fort Lauderdale, FL, USA, 2011. PMLR.
Aurko Roy, Ashish Vaswani, Arvind Neelakantan, and Niki Parmar. Theory and experiments on
vector quantized autoencoders. arXiv, May 2018.
Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In Proceedings of
the Eighteenth International Conference on Architectural Support for Programming Languages
and Operating Systems, ASPLOS ’13, pp. 305-316, New York, NY, USA, 2013. Association for
Computing Machinery. ISBN 9781450318709. doi: 10.1145/2451116.2451150. URL https:
//doi.org/10.1145/2451116.2451150.
11
Under review as a conference paper at ICLR 2021
Armando Solar-Lezama, LiviU Tancau, Rastislav Bodk Sanjit A. Seshia, and Vijay A. Saraswat.
Combinatorial sketching for finite programs. In Conference on Architectural Support for Pro-
gramming Languages and Operating Systems, ASPLOS 2006, San Jose, CA, USA, October 21-25,
2006, pp. 404-415. ACM, 2006.
Phillip D Summers. A methodology for lisp program construction from examples. Journal of the
ACM (JACM), 24(1):161-175, 1977.
Abhishek Udupa, Arun Raghavan, Jyotirmoy V Deshmukh, Sela Mador-Haim, Milo M K Mar-
tin, and Rajeev Alur. TRANSIT: Specifying protocols with concolic snippets. In Conference
on Programming Language Design and Implementation (PLDI), pp. 287-296. Association for
Computing Machinery, 2013.
Aaron van den Oord, Oriol Vinyals, and Koray KavUkcUoglu. Neural discrete representation learn-
ing. In Neural Information Processing Systems (NeurIPS), 2017.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
Systems (NeurIPS), 2017.
Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun, Stefan Lee,
David J. Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from
neural sequence models. In AAAI, 2018.
Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu. Improving automatic source
code summarization via deep reinforcement learning. In 2018 33rd IEEE/ACM International
Conference on Automated Software Engineering (ASE), pp. 397-407, 2018.
Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summa-
rization. In Neural Information Processing Systems (NeurIPS), 2019.
Amit Zohar and Lior Wolf. Automatic program synthesis of long programs with a learned garbage
collector. In Neural Information Processing Systems (NeurIPS), 2018.
12
Under review as a conference paper at ICLR 2021
Program Y := Concat(e1 , e2, . . .)
Expression e := f | n | n1(n2) | n(f) | ConstStr(c)
Substring f	:=	SubStr(k1 , k2) | GetSpan(r1 , i1 , b1 , r2, i2, b2)
Nesting n	:=	GetToken(t, i) | ToCase(s) | Replace(δ1 , δ2)	| Trim()	| GetUpto(r)	| GetFrom(r)
| GetFirst(t, i) | GetAll(t)
Regex r := t1 | . . . | tn | δ1 | . . . | δm
Type t := NUMBER | WORD | ALPHANUM | ALL_CAPS | PROP_CASE | LOWER | DIGIT | CHAR
Case s := PROPER | ALL_CAPS | LOWER
Position k := - 100 | - 99 | . . . | 1 | 2 | . . . | 100
Index i := - 5 | - 4 | . . . | 1 | 2 | . . . | 5
Boundary b := START | END
Delimiter δ := &,.?@()[]%{}/ :; $#”0
Character c := A - Z | a - z | 0 - 9 | &, .?@ . . .
Figure 7:	The DSL for string transformation tasks (Devlin et al., 2017)
A	Extended Description of DSL and RobustFill Model
The DSL for string transformations we use is the same as used in RobustFill (Devlin et al., 2017),
and is shown in Figure 7. The top-level operator for programs in the DSL is a Concat operator
that concatenates a random number (up to 10) of expressions ei . Each expression e can either be a
substring expression f, a nesting expression n, or a constant string c. A substring expression can
either return the substring between left k1 and right k2 indices, or between the i1-th occurence of
regex r1 and i2-th occurence of regex r2. The nesting expressions also return substrings of the input,
such as extracting the i-th occurrence of a regex, but can also be composed with existing substring
or nesting expressions for more complex string transformations.
RobustFill Model RobustFill (Devlin et al., 2017) is a seq-to-seq neural network that uses a
encoder-decoder architecture where the encoder computes a representation of the input e(X), and
the decoder autoregressively generates the output given the source representation, i.e. conditional
likelihood of Y = [y1, . . . , yT] decomposes as p(Y |X) = QtT=1 p(yt|y<t, X).
In RobustFill, the probability of decoding each token yt is given by p(yt|y<t, X) =
Softmax (W(ht)) with W being the projection onto logits, or unnormalized log probabilities. The
hidden representation ht is an LSTM hidden unit given by,
Et = Attention (ht-1, e(X)) ,
ht = LSTM(ht-1,Et) .
Here e(X) is the sequence of hidden states after processing the specifications with an LSTM en-
coder, and Attention (Q, V ) denotes the scaled dot-product attention with query Q and key-value
sequence V (Bahdanau et al., 2016). In the case of X being multiple I/O examples, the RobustFill
model of Devlin et al. (2017) uses double attention
stI,i = Attention (ht-1, e(Ii))
stO,i = Attention Concat ht-1, stI,i , e(Oi)
ht,i = LSTM (ht-ι, Concat (sI,i, SOi)) ∀1 ≤ i ≤ N,
and hidden states are pooled across examples before being fed into the final softmax layer, or ht =
maxpool1≤i≤N tanh(V (ht,i)) , where V is another projection.
13
Under review as a conference paper at ICLR 2021
B Examples of Generated Programs and Latent Codes
Inputs	Outputs	LP Outputs
“Mason Smith"	“Smith M"	“Smith M"
“Henry Myers"	“Myers H"	“Myers H"
“Barry Underwood"	“Underwood B"	“Underwood B"
“Sandy Jones"	“Jones S"	“Jones S"
LP	GetTOken_PROP_CASE_2 | COnstStrr ") I GetTOken_CHAR_1(GetTOken_PROP_CASE_1)
LP Latent TOK_3 0 I TOK_13 I TOK_3 9 I TOK_3 0
Inputs	Outputs	LP Outputs
“January 15"	“jan 15"	“jan 15"
“febuary 28"	“feb 28"	“feb 28"
“march 1"	“mar 1"	“mar 1"
“October 31"	“oct31"	“oct 31"
LP	ToCase_LOWER(SubStr(1, 3)) ∣ ConstStrr ") I GetToken_NUMBER_1
LP Latent tok_ii | TOK_26 | tok_17
Inputs	Outputs	LP Outputs
“(321) 704 3331" “(499) 123 3574" “(555) 580 8390" “(288)225 6116"	“321.704.3331”	“321.704.3331" “499.123.3574"	“499.123.3574" “555.580.8390"	“555.580.8390" “288.225.6116"	“288.225.6116"
LP	GetToken_NUMBER_1 | ConstStr(.) | Replace_“ "_.(SubStr(-8, -1))
LP Latent TOK_17 I TOK_27 I TOK_24 I TOK_16
Inputs	Outputs	LP Outputs
“Milk 4, Yoghurt 12, Juice 2, Egg 5"	“M.E.”	“M.E."
“US:38 China:35 Russia:27 India:1"	“U.I."	“U.I."
“10 Apple 2 Oranges 13 Bananas 40 Pears"	“A.P."	“A.P."
“parul 7 rico 12 wolfram 15 rick 19"	“P.R."	“.."
LP	GetToken_CHAR_1(GetToken_PROP_CASE_1) | Const(.) |
GetToken_CHAR_-1(GetAll_ALL_CAPS) | Const(.)
LP Latent TOK_39 I TOK_30 I TOK_6 I TOK_38 I TOK_30
Figure 8:	Latent codes and programs found by Latent Programmer in string transformation tasks.
Red denotes I/O where the predicted program mapped input to an incorrect output.
14
Under review as a conference paper at ICLR 2021
Docstring	Program
get an environment variable	def set_key(key, val, key_prefix=None): return environ.get(key, key_prefix)
return a list of the words in the string s	def split(s, sep=None, maxsplit=-1): return s.split(sep, maxsplit)
mean squared error function	def mean_squared_error(y_true, y_pred): return tf.reduce_mean(tf.square((y_true - y_pred)))
read a python file	def read_file(fname): f = open(fname) with open(fname, ‘r’) as f: f.seek(0) return f.read()
pickle dump	def pickle_save(filename, data): with open(filename, ‘r’) as f: pickle.dump(data, f)
takes a timedelta and returns the total number of seconds	def total_seconds(delta): return ((delta.microseconds + ((delta.days * 24) * 3600) * (10**6))/(10**6))
Figure 9:	Programs found by Latent Programmer in Python code generation dataset. Red denotes
ares where the predicted program deviates from human code.
15