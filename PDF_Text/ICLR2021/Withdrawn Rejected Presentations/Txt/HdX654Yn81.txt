Under review as a conference paper at ICLR 2021
Improving the Unsupervised Disentangled
Representation Learning with VAE Ensemble
Anonymous authors
Paper under double-blind review
Ab stract
Variational Autoencoder (VAE) based frameworks have achieved the state-of-the-	1
art performance on the unsupervised disentangled representation learning. A re-	2
cent theoretical analysis shows that such success is mainly due to the VAE im-	3
plementation choices that encourage a PCA-like behavior locally on data sam-	4
ples. Despite this implied model identifiability, the VAE based disentanglement	5
frameworks still face the trade-off between the local orthogonality and data re-	6
construction. As a result, models with the same architecture and hyperparameter	7
setting can sometimes learn entangled representations. To address this challenge,	8
we propose a simple yet effective VAE ensemble framework consisting of multi-	9
ple VAEs. It is based on the assumption that entangled representations are unique	10
in their own ways, and the disentangled representations are “alike” (similar up to a	11
signed permutation transformation). In the proposed VAE ensemble, each model	12
not only maintains its original objective, but also encodes to and decodes from	13
other models through pair-wise linear transformations between the latent repre-	14
sentations. We show both theoretically and experimentally, the VAE ensemble	15
objective encourages the linear transformations connecting the VAEs to be triv-	16
ial transformations, aligning the latent representations of different models to be	17
“alike”. We compare our approach with the state-of-the-art unsupervised disen-	18
tangled representation learning approaches and show	the improved performance.	19
1	Introduction	20
Disentangled representation learning aims to capture the semantically meaningful compositional	21
representation of data (Higgins et al., 2018; Mathieu et al., 2018), and is shown to improve the	22
efficiency and generalization of supervised learning (Locatello et al., 2019), reinforcement learning	23
(Watters et al., 2019), and reasoning tasks (van Steenkiste et al., 2019). The current state-of-the-	24
art unsupervised disentangled representation learning deploy the Variational Autoencoder (VAE)	25
(Kingma & Welling, 2013; Rezende et al., 2014). The main challenge is to reduce the trade-off	26
between learning a disentangled representation and reconstructing input data. Most of the recent	27
works extend the original VAE objective with carefully designed augmented objective to address	28
this trade-off (Higgins et al., 2017; Burgess et al., 2017; Kim & Mnih, 2018; Chen et al., 2018;	29
Kumar et al., 2017). A recent study in (Locatello et al., 2018) compared these methods and showed	30
that their performance is sensitive to initialization and hyperparameter setting of the augmented	31
objective function.	32
Recently, Duan et al. (Duan et al., 2019) developed an unsupervised model selection method named 33
Unsupervised Disentanglement Ranking (UDR) to address the challenge of hyperparameter search	34
and model selection. UDR leverages the finding in (Rolinek et al., 2019) that the implementation	35
choices of VAE encourage a local PCA-like behavior locally on data samples. As a result, disen-	36
tangled representations by VAEs are “alike” as they are similar up to signed permutation transfor-	37
mations. On the contrary, the entangled representations by VAEs are “unique” as they are similar	38
at least up to non-degenerate rotation matrices. UDR uses multiple models trained with different	39
initializations and hyperparameter settings, and builds a similarity matrix measuring the pair-wise	40
similarity between the latent variables from different models. A higher score is given to the model	41
that can match its representations to many others models. The results show close match between	42
UDR and commonly used supervised metrics, as well as the performance of downstream tasks using	43
the latent representations.
44
1
Under review as a conference paper at ICLR 2021
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
Inspired by the findings from these studies, we propose a simple yet effective VAE ensemble frame-
work to improve the disentangled representation by VAE. The proposed VAE ensemble consists of
multiple VAEs. The latent variables in every pair of these models are connected through linear lay-
ers to force the latent representations in the ensemble to be similar up to a linear transformation.
We show that the VAE ensemble objective encourages these pair-wise linear transformations to con-
verge to trivial transformations, making latent representations of different VAEs in the ensemble to
be “alike”, thus disentangled. In this paper, we make the following contributions: (1) We introduce
a simple yet effective VAE ensemble framework to improve the disentangled representation learning
using the original VAE. (2) We show in theoretical analysis that the linear transformations connect-
ing the latent representations of the individual models in the ensemble tend to converge to trivial
transformations thus encourage disentangled representation, and verify this result with experiments.
(3) We evaluate our approach using the original VAE model, and show the improved state-of-the-art
performance across different datasets.
2	Related Work
Variatioanl Autoencoder is a deep directed probabilistic graphical model consisting of an encoder
and a decoder (Kingma & Welling, 2013; Rezende et al., 2014). The encoder qφ(z∣χ) maps the
input data x ∈ Rn to a probabilistic distribution as the latent representation z ∈ Rd , and the decoder
qθ(x|z) maps the latent representation to the data space noted as qθ(x|z), where φ and θ represent
model parameters. The VAE objective is to maximize the marginalized log-likelihood of data. Direct
optimization of this objective is not tractable and it is approximated by the evidence lower bound
(ELBO) as:
LVAE = Eqφ(z∣x) [log qθ(XIz)] — KLSφ(ZIx) k P(Z)),	⑴
In practice, the first term is estimated by reconstruction error. The second term is the Kullback-
Leibler divergence between the posterior qφ(ZIx) and the prior p(Z) commonly chosen as an
isotropic unit GaussianP(Z)〜N(0, I).
Disentangled representation by VAE has achieved the state-of-the-art performance (Higgins et al.,
2017; Burgess et al., 2017; Kim & Mnih, 2018; Chen et al., 2018; Kumar et al., 2017), despite the
fact that the VAE objective only models the marginal distribution of the data instead of the desired
joint distribution over data and latent variables. The reason for this success is the implementation
choices of the VAE framework (Rolinek et al., 2019). In practice, the latent variables in VAE often
work in “polarized” modes. The “passive” mode is defined by μj(x)《1 and σ2(x) ≈ 1, while the
“active” mode is defined by σj2(x) 1. The “passive” latent variables closely approximate the prior
and have little effect on the decoder. The “active” latent variables, on the other hand, are closely
related to both the per sample KL loss and the decoder output. The “polarized regime” enables a
reformulated VAE objective showing that VAEs optimize a trade-off between data reconstruction
and orthogonality of the linear approximation of decoder Jacobian locally around a data sample.
This PCA-like behavior near data points encourages an identifiable disentangled latent space by
VAE. Furthermore, it was suggested that finding an appropriate “polarized regime” is dependent
on the initialization and the hyperparameter tuning of the state-of-the-art approaches. In this study,
we show that the proposed VAE ensemble aligns the “polarized regime” of individual VAE models
towards the disentangled representation.
Model selection In practice, we often observe neural networks achieve similar performance with
different internal representations when trained with the same hyperparameters (Raghu et al., 2017;
Wang et al., 2018; Morcos et al., 2018). For the unsupervised disentanlged representation, as dis-
cussed in (Locatello et al., 2018; Duan et al., 2019), we often observe high variance in the perfor-
mance from the model trained with the same architecture and hyperparameter setting. This poses
a challenge for choosing the model in practice. Duan et al. (2019) proposed Unsupervised Disen-
tanglement Ranking (UDR) to address this challenge. The extensive empirical evaluations on UDR
using both the supervised metric measurement and the performance of downstream tasks validates
its effectiveness. They also confirm that disentangled representations are “alike” and entangled rep-
resentations are unique in their own ways. The proposed VAE ensemble leverages this finding.
Identifiable VAE Built on the recent breakthroughs in nonlinear Independent Component Analy-
sis (ICA) literature (Hyvarinen & Morioka, 2016; 2017; Hyvarinen et al., 2019), Khemakhem et
al. show that the identification of the true joint distribution over observed and latent variables is
2
Under review as a conference paper at ICLR 2021
Disentangled
representation χiχ
Entangled
representation
Unlabeled
training data
VAE	BetaVAE FactorVAE	VAE Ensemble
Figure 1: The proposed VAE ensemble consists of multiple original VAE models. The encoders
of the VAEs in the ensemble generate input encoding that can be linearly transformed among each
other. The decoders of the VAEs in the ensemble reconstruct the input data from both their corre-
sponding encoder and the linearly transformed encodings from other encoders. The x and y axis
of the circles on the left hand side of the plot represent two generative factors as an example. The
aligned arrows with x and y axis show a model with disentangled representation and unaligned ones
show a model with entangled representation.
possible up to very simple transformations (Khemakhem et al., 2019). They proposed identifiable 98
VAE (iVAE) that requires a factorized prior distribution over the latent variables conditioned on an 99
additionally observed variable, such as a class label or almost any other observation. We believe the 100
proposed VAE ensemble is related to such framework where the latent representation from one VAE 101
model can be regarded as the auxiliary observations for another.	102
Ensemble learning The idea of ensemble learning is to combine multiple learning models (poten- 103
tially weak learners) to improve the task performance or robustness over a single model (Schapire, 104
1990). It achieves the improved performance by averaging the bias, reducing the variance thus pre- 105
venting the over-fitting (Drucker et al., 1994; Breiman, 1996). Early works in neural networks have 106
used the ensemble learning to achieve top performance in the related competition (Krizhevsky et al., 107
2012; Simonyan & Zisserman, 2014). In this work, we apply the ensemble learning to enforce the 108
alignment among the latent representations of different models. This results in latent representations 109
that are similar among each other in the ensemble up to a trivial transformation.	110
3	The VAE Ensemble Framework
111
As illustrated in Figure 1, the proposed VAE ensemble consists of n original VAE models with the 112
same architecture but different initializations. It also consists ofn × (n - 1) linear layers connecting 113
the latent representations of every two VAE models. Each model in the ensemble maintains its 114
original VAE objective as Eq. 1. In addition, l2 loss is used to force mapping between latent 115
representations via pair-wise linear layers (cross-model linear transformation). The decoder of each 116
VAE model generates the input reconstruction from not only their corresponding encoder (within- 117
model reconstruction), but also the linearly transformed encodings from other encoders (cross-model 118
reconstruction). Overall, the VAE ensemble is trained with the following objective:	119
nn	n
L(θ, φ) =E £%,j (z,j∣χ) [log qθj (XIzij)]- EKLgφii (Zii Ix) k P(Zii))
i=1 j=1	i=1
nn
-γXXEqφij(zij∣x)kZjj-Zijk2,
(2)
3
Under review as a conference paper at ICLR 2021
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
where n is the number of models in the ensemble; φ := (φij ) is the encoders parameters where φij
represents the encoder of VAEi and its associated linear layer mapping the latent representation from
VAEi to VAEj (notice that φii represents the encoder parameters of VAEi only and its associated
linear transformation can be be regarded as an identity transformation); θ := (θi) represents the de-
coder parameters of VAEi ; and zjj represents the latent representation of VAEj while zij represents
the linearly transformed latent representation from VAEi to VAEj . γ is a hyperparameter to balance
the effect of the estimation error between the latent representaitons. P(Zii)〜N(0, I) is assume to
be the prior as defined in the original VAE objective.
Comparing to the original VAE objective in Eq. 1, the objective of each individual VAE model in
the ensemble, L0VAE, can be written as:
n-1
L0 VAE (θ,φ) = LVAE (θ,φ) + X {Eqφj (Zj ∣x)[log 9θ (XIzj )] - YEqφj(Zj |x) llzj - zk2 },
j=1
、--------------------------V---------------------------}
Ensemble Regularization
(3)
where n is the number of models in the ensemble, φj stands for the parameters of the encoder and
its linear transformation layers from other VAEs, and zj represents the linear transformed latent
representation of the encoding from other encoders.
In this form, the VAE ensemble regularizes each VAE model with additional terms on the encoder
as γEqφj(Zj∣x) Ilzj — zk2, and on the decoder as Pn=I Eqφ,(Zj∣χ)[logqθ(x|zj)]. These regulariza-
tions directly constrain the latent representations among different VAE models in the ensemble to
be similar. In particular, for a given input data, lzj - z l2 encourages the encoders to generate
similar encodings UP to the linear transformations; and P；=i Eqφj (zj ∣x)[log qθ(x|zj)] emphasizes
the similar effect on the data reconstruction from the latent variables such that the decoders can
reconstruct the input data with both the original encoding z and the linearly transformed encoding
zj . As we shall discuss in the next section, together these regularizations encourage similar latent
representation up a trivial transformation by different models in the ensemble.
We also introduce the hyperparameter γ to balance the trade-off between these two regularizations:
higher value forces closer mapping between the encoders and reduce the cross-model reconstruction
error of the decoders; lower value relaxes the mapping between the encoders and increases the cross-
model reconstruction error of the decoders. As we show in Section 5, both components are important
and balancing the trade-off between them is important as the ensemble size increases.
Computational complexity It is a common practice to train a number of seeds per hyperparameter
setting for the current state-of-the-art unsupervised disentanglement VAE models (Locatello et al.,
2018; Duan et al., 2019). Comparing to training n original VAEs, the proposed VAE ensemble
requires additional n × (n - 1) linear layers. While this addition does not increase the size of the
model much, the estimation of the linear transformations loss and the cross-model reconstruction
losses grow with n × (n - 1), which may be computationally expensive especially when n is large.
That being said, the results in Section 5 show that the VAE ensemble achieves more stable results
comparing to the current state-of-the-art models. Also, its computation is highly parallelisable.
4	Theoretical Justification
In this section, we present the theoretical analysis on why the proposed VAE ensemble can improve
the disentangled representation. We start with analysing the l2 objective in Eq. 2 of the pair-wise
liner transformations in the VAE ensemble, and show that: (1) the pair-wise linear transformations
encourage similar “polarized” regime (see Sec. 2) among the VAEs in the ensemble; (2) the linear
transformations are close to the orthogonal transformations. Based on these two properties, we then
discuss how the cross-model reconstructions by the VAE ensemble encourage learning a disentan-
gled representation over its entangled counterpart.
4.1	The Effect of Linear Transformation between Latent Representations
Let VAEi and VAEj be two different VAE models in the ensemble, and Mji be the linear trans-
formation that maps the latent representation of a given input x by VAEj to the one by VAEi , as
4
Under review as a conference paper at ICLR 2021
Zj(x)〜 N(μj(x), diag(bj(x)2)) to Zi(x)〜 N(μi(x), diag(σi(x)2)). In the following We re-
move the input notation from the VAE latent representations for simplicity (i.e. zj (x) is simplified
as zj ), while keeping in mind that the analysis is based on the local latent representation of a given
input x.
For VAEj , the l2 term of the VAE ensemble loss in Eq. 2 aims to find Mji and zj that minimize
E kzi - Mjizj k2, where the expectation is over the stochasticity of VAEj. We can write zi and zj
as Zi = μi + €i and Zj = μj + €j, where 且 〜N(0, diag(σ2)) and e7∙〜N(0, diag(σj')). Hence
using bias-variance decomposition, the l2 term can be written as:
min	E kZi - MjiZj k2
Mji ,zj~N (μj ,diag(σj))
min	I kμi - Mjiμj∣∣2 + E kMjiMj - MjiZjk2 + E kμi - zi『
Mji,Zj〜N(μj ,diag(σj2))	zj	zi
min	[∣∣μi - Mjiμj∣∣2 + E kMjiMj - MjiZjk 2 + + CI
Mji,Zj〜N(μj ,diag(σj2))	zj
min	k kμi - Mjiμj k2 + E kMjiej+ CI
Mji,zj〜N (μj,diag(σ2))	Wj
(4)
166
167
168
169
170
171
172
173
where the constant Ci arises from the fact E ∣∣μi - Zik2 does not depend on Mji and Zj. Eq. 4 Con-
zi
sists of a deterministic component of ∣∣μi - Mjiμj ∣∣2 and a stochastic component of E ∣Mjiej∣2.
j
The deterministic component can be minimized by adjusting the parameters in VAEj such that its
mean encoding μj∙ is optimized for any given Mji. This simplifies our analysis to focus on the
stochastic component. Between Mji and j in this stochastic component, we separately optimize
one while having the other fixed.
We start with fixed Mji and optimizing for Ej 〜N(0, diag(σjy). Notice that σj is associated with
VAEj objective. In (Rolinek et al., 2019), the VAE objective is reformulated into the deterministic
reconstruction, the stochastic reconstruction and the KL loss. The last two components define the
stochastic loss of VAE. It is formulated as:
174
175
176
177
178
179
180
181
182
183
min	E ∣DEj∣2 s.t.	LKL = C1,
V,σ2	彳弓〜N (0,diag(σ2))	j"	弋
XX
(5)
where X represents the dataset, D represents the local linear approximation of the Jacobian of the
decoder with singular value decomposition as D = UΣV T. Furthermore, the KL loss LKL =
1 PL1(μjk + σ2k - log σ2k - I) can be SimPlified as L≈KL = 1 Pk∈ “active (μjk - log σ2k - I)
based on the “polarized” regime of VAE. (Rolinek et al., 2019) shows that σj2 act as the precision
control allowed for each latent variable where more influential ones receive more precision. Com-
bining the stochastic loss of the linear transformation in Eq. 4 and the stochastic loss of the original
VAE in Eq. 5, the overall stochastic loss on σj2 can be formulated as:
184
185
186
187
188
189
190
mi2n	E 2	[∣MjiEj∣2 + ∣DEj∣2] s.t.	- log σj2k	=	C2,	(6)
σj Wj〜N(0,diag(σ2))	' /
where σj2k is the kth element of σj2 . Here we further simplify LKL with the L≈KL up
to additive constants C2 when μj∙ is fixed. In addition to the precision control of σj on
VAEj , this objective also aims to find an optimal distribution of σj2 that aligns the “polarized
regime” among different VAEs. To see why, let ck be the kth column of Mji , we then have
EkMjiejk2 = Pk Ilck∣∣2 σjk. The Arithmetic-Mean-Geometric-Mean (AM/GM) inequality sug-
gests that Pk ∣ck∣2σj2k ≥ n Qk ∣ck∣2σj2k
1/n
n Qk ∣ck ∣2	exp(-C), where the equal-
ity is achieved when ∣cm ∣2 σj2m = ∣cn ∣2 σj2n for any m 6= n. This suggests that latent variables
with high ∣ck∣2 mapping from Zj to Zi will have smaller variance. Hence, these latent variables in Zj
are encouraged to stay in the “active” mode. On contrary, the latent variables that do not share sim-
ilar generative factors between Zj and Zi will be assigned larger variance, and being pushed towards
the “passive” mode.
191
192
193
194
195
196
197
198
199
200
201
5
Under review as a conference paper at ICLR 2021
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
Now We fix the optimal distribution of σj, and optimize for M% Since Wj 〜N(0, diag(σj∖), this
objective can be understood as optimally rotating the latent space of VAEj such that the stochastic
component in Eq. 4 is minimized. Specifically, we have the following objective:
min
Mji
kMjiWj k2 = min MjiRT Wj2
R
(7)
where R is an orthogonal transformation. Let c0k be the kth column of MjiR. Similar as before, the
AM/GM inequality suggests MjiRT Wj2 = Pk kc0kk2 σj2k ≥ Qk kc0kk2 exp(-C3). Hadamard’s
inequality suggests that Qk kc0kk2 ≥ |det(MjiR)|, and the equality is satisfied when c0ks are pair-
wise orthogonal. This can be understood from the geometric perspective where Qk kc0kk2 gives an
upper bound on Volume( MjiRTx : x ∈ [0, 1]d ). As a result, the optimization of Mji will lead to
an orthogonal transformation. Together the optimization of Eq. 6 and Eq. 7 encourages the align-
ment of the “polarized regime” among different models under orthogonal linear transformations.
They force different models in the ensemble to capture the same mixture of the generative factors.
In the next section, we discuss the effect of the cross-model reconstruction in the VAE ensemble that
encourages the disentangled representation over the entangled ones.
4.2	The Effect of cross-model Reconstruction
In an entangled representation, each latent variable captures a mixture of generative factors in its
unique way. Since different generative factors typically have different effects on data variations
(Duan et al., 2019), the orthogonal transformation from one entangled representation zj to another
one zi introduces different encoding variance. Some of the transformed latent variables in Mjizj
carry larger variance comparing to the corresponding ones in zi . This discrepancy leads to larger
cross-model reconstruction of VAEi than the within model reconstruction. This error forces both
VAEi and VAEj to adjust their representations until the effect on the data reconstruction by indi-
vidual latent variables matches between Mji zj and zi . The process applies to all models in the
ensemble and eventually leads to a one-to-one mapping of latent variables between different mod-
els, where Mji becomes a trivial transformation (signed permutation matrix). In particular, if one of
the models in the VAE ensemble learns a disentangled representation, other models in the ensemble
will converge to it. This is because the orthogonal transformation from an entangled representation
to a disentangled representation introduces larger cross-model encoding variance due to the mixture
of different generative factors in the former, thus a larger cross-model reconstruction by the disen-
tangled model. On contrary, the orthogonal transformation from a disentangled representation to an
entangled representation would not introduce larger cross-model encoding variance than the within
model encoding, thus similar cross-model reconstruction as within model reconstruction by the en-
tangled model. Such a gap encourages the entangled representations to align with the disentangled
representation. We illustrate the geometric interpretation of such a case in Appendix C.
From these discussions, we conclude that the VAE ensemble encourages different individual mod-
els to capture similar generative factors, thus learn representations that are “alike” up to a trivial
transformation. In the next section, we verify these analytic results with experiments.
5	Experiments
Our experiments are designed to confirm the discussions in the previous sections. Particularly we
ask the following questions: Q1: Do the linear transformations in the ensemble converge to trivial
transformation? Q2: Do the VAEs in the ensemble work in similar “polarized” regime? Q3: Does
VAE ensemble improve the unsupervised disentangled representation learning, and what is the ef-
fect of ensemble size? Q4: What are the effects of the cross-model reconstruction loss, the linear
transformation loss and the hyperparameter γ in the VAE ensemble objective?
We analyze the inner working of the proposed VAE ensemble using the benchmark dSprite dataset
(Matthey et al., 2017) with fully known generative processes, and the real-world CelebA dataset (Liu
et al., 2015) with unknown generative process. Furthermore, for dSprites dataset, we compare our
results with the original VAE model and the state-of-the-art disentanglement VAE models including
β-VAE (Higgins et al., 2017), FactorVAE (Kim & Mnih, 2018), TC-VAE (Chen et al., 2018) and
DIP-VAE (Kumar et al., 2017). We use two widely used supervised metrics including FactorVAE
metric (Kim & Mnih, 2018) and DCI Disentanglement scores (Eastwood & Williams, 2018) as the
6
Under review as a conference paper at ICLR 2021
Figure 2: Comparing the DtO of linear transformations in the VAE ensemble (γ=10) with the one
between well-trained individual VAEs, as well as the well-trained individual state-of-the-art VAE
models. The latent dimension for all models is set to 10 and evaluated on the dSprite dataset.
quantitative measurements. They are shown to correlate with other common supervised metrics
(Locatello et al., 2018). For example, FactorVAE metric and β-VAE metric (Higgins et al., 2017)
capture similar notions, while DCI Disentanglement and Mutual Information Gap (MIG) (Chen
et al., 2018) capture similar notions. In addition, DCI Disentanglement is closely related to the
unsupervised model selection method UDR (Duan et al., 2019). For CelebA dataset, we show the
latent traversal visulization as a qualitative measurement in Appendix E. We provide the details of
the experiments in Appendix D.
Q1: We use the Distance to Orthogonality (DtO) (Rolinek et al., 2019) to check if the linear trans-
formations in the ensemble converge to a signed permutation matrix during training. DtO is the
Frobenius norm of the difference between a matrix M and its closest signed permutation matrix
P (M ). It is solved with mixed-integer linear programming (MILP) formulation. The details on
DtO can be found in Appendix B. In Figure 2, we show the DtO estimation of the linear transfor-
mations in the VAE ensemble of different ensemble size for the dSprite dataset. We show the mean
and standard deviation of DtO across all linear transformations over 10 different runs. Furthermore,
we compare these results with a baseline measurement where DtO is estimated for the linear trans-
formations between the mean latent representations of well-trained individual models. Specifically,
we use ten well-trained individual models and report the mean and standard deviation of the DtO
estimations. As seen in the figure, the VAE ensemble models with different ensemble size all ap-
proach to trivial transformations between the individual models, while other VAE models do not
have such property. In Fig. 6, we show that during training, the VAE ensemble remains maintains
low DtO while the original VAEs do not have such property. A similar result for models trained on
the CelebA dataset with different latent dimensions is shown in Fig. 7. Further discussion on these
results are provided in Appendix E.
Q2: To check if the models in the VAE ensemble work in similar “polarized” regime, we estimate
the relative error between LKL and L≈KL as ∆ = LKL-LjKL for each latent variable. Smaller ∆
indicates closer matching between LKL and L≈KL of a latent variable, thus more “active”. Figure
3(a) and 3(b) show log(∆) of the 10 latent variables of individual models in VAE_E2 and VAE_E3
with different γ settings trained on the dSprite dataset. The results show that individual models
in the ensemble do work in similar ”polarized regime’. In Figure 3(a), we also compare the VAE
ensemble with the β-VAE where β = 4. This setting was found previously to be the optimal setting
for the dSprite data for β-VAE (Higgins et al., 2017). We see that the VAE ensemble encourages
more “active” latent variables than β-VAE. When we compare Fig. 3(a) and 3(b), we see that as
the ensemble size increases, individual models are forced to have more “active” latent variables by
decomposing the generative factors. This can be observed in the latent traversals shown in Appendix
E. The dSprites dataset contains five ground truth generative factors. The VAE_E2 models can
have up to eight “active” latent variables depending on input, and these representations capture a
decomposition of the ground truth generative factors.
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
7
Under review as a conference paper at ICLR 2021
(a)
Figure 3: The “polarized regime” comparison between models in the VAE ensemble. The latent
dimension is set to 10 and the results are over 10 runs of training on the dSprite dataset. (a) The
“polarized regime” comparison by log( LKL-LLjKL ) of each latent variable of the two models in
VAE-E2 as well as β-VAE model. (b) Similar to (a) but for the three models in VAE-E?.
Zl Zi Z3 Za Z5 z⅛ Zi Za Zg Zw
Latent Dimensions
(b)
FactorVAE Metric
Individual Model			VAE EnSembIe (VAE-E)				
VAE	0.635±0.083		γ=1	γ=5	γ=10
β-VAE (β=4)	0.665±0.089	VAE-E2	0.711±0.106	0.736±0.085	0.741±0.086
FactorVAE (γ=40)	0.764±0.075	VAE-E3	0.794±0.030	0.792±0.075	0.821±0.066
DIP-VAE-I (λod=5)	0.638±0.108	VAE-E4	0.833±0.037	0.790±0.038	0.800±0.078
DIP-VAE-II(λod=5)	0.676±0.122	VAE-E5	0.828±0.016	0.786±0.051	0.739±0.085
TC-VAE (β=4)	0.808±0.079~				
DCI-Disentanglement Metric
IndiVidUal Model				VAE EnSembIe (VAE-E)				
VAE	0.143±0.033		γ=1	γ=5	γ=10
β-VAE (β=4)	0.198±0.076	VAE-E2	0.176±0.043	0.243±0.029	0.201±0.037
FactorVAE (γ=40)	0.253±0.072	VAE-E3	0.214±0.064	0.236±0.051	0.311±0.060
DIP-VAE-I (λod=5)	0.049±0.017	VAE-E4	0.240±0.059	0.223±0.045	0.251±0.038
DIP-VAE-II(λod=5)	0.106±0.032	VAE-E5	0.242±0.032	0.244±0.039	0.196±0.050
TC-VAE (β=4)	0.303±0.05方				
Table 1: Comparison between the proposed VAE ensemble, the original VAE, and the current state-
of-the-art disentangled VAE models. We report the mean and standard deviation of the FactorVAE
metric and and DCI Disentanglement scores over 10 runs trained on the dSprite data.
289
290
291
292
293
294
295
296
297
298
299
300
301
302
Q3: In Table 1, we compare the disentangled representation performance between the proposed
VAE ensemble and the state-of-the-art models. For the VAE ensemble, we report the performance
of the first model in the ensemble. We also report the results for the VAE ensemble with different
ensemble size and γ values. As shown in the table, the VAE ensemble significantly improves the
performance over the original VAE model. In many settings, the VAE ensemble achieves similar
or better performance over the state-of-the-art models. In Table 2, we evaluate the consistency
among the models in the ensemble by reporting the standard deviation of the evaluation metrics
using different models in the same ensemble. The small values confirm that different models in
the ensemble learn similar latent representations. Furthermore, Table 1 shows the joint effect of
ensemble size and γ setting. When γ = 1, the performance of VAE ensemble increases as the
ensemble size increases, indicated by the higher mean and smaller variance of both the FactorVAE
and DCI Disentanglement metrics. This behavior is consistent with the characteristic of ensemble
learning where the increase in performance becomes smaller as the size of ensemble increases.
However, as γ increases, having larger ensemble size can reduce the performance. We believe this
8
Under review as a conference paper at ICLR 2021
	工		VAE_E2-	VAE卫3-	VAE卫4-	VAE-E5-
FactorVAE Metric	(Y =I)	0.0019	0.0090	0.0048	0.0081
	(Y=5)	0.0058	0.0064	0.0089	0.0163
	(γ=10)	0.0060	0.0046	0.0147	0.0139
DCI-Disent Metric	(γ=1)	0.0024	0.0026	0.0028	0.0036
	(Y=5)	0.0037	0.0054	0.0049	0.0040
	(Y =10)	0.0013	0.0024	0.0041	0.0042
Table 2: The comparison between individual models in the same ensemble. We report the average
of the standard deviation of the metrics by individual models in the ensemble across 10 runs.
Figure 4: Ablation study to understand the effect of cross-model reconstruction and linear transfor-
mation in the VAE ensemble objective using the FactorVAE metric. (w.o. CR - without cross-model
reconstruction loss; w.o. LT - without linear transformation loss; org - original VAE ensemble loss)
is due to the increased difficulty of balancing between the cross-model and within model objectives
of VAE ensemble for larger ensembles. The reduced alignment of the latent representations among
different models can also be seen in Table 2 where difference in the performance among individual
models in the ensemble increases as ensemble size increases.
Q4: We conduct the ablation study to further understand the effect of the linear transformation loss
and the cross-model reconstruction loss in the VAE ensemble objective. As shown in Fig. 4, remov-
ing either component leads to a lower FactorVAE metric for the VAE ensemble. Without the linear
transformation loss, the performance of VAE ensemble decreases significantly across different en-
semble sizes. Without the cross-model reconstruction loss, the performance of VAE ensemble also
decreases but the gap becomes smaller as γ increases. This matches the discussion in Section 3 that
higher γ forces closer mapping between the encoders and reduce the cross-model reconstruction er-
ror of the decoders. However, this also reduces the effect of cross-model reconstruction as discussed
in Section 4.2. A similar result is also found for the DCI Disentanglement metric as shown in Fig.
8 in Appendix E. Overall, the results from the ablation study confirms the importance of both the
linear transformation loss and the cross-model reconstruction loss in the VAE ensemble objective.
6	Conclusion
In this study, we propose a simple yet effective VAE ensemble framework consisting of multiple
original VAEs to learn disentangled representation. The individual models in the ensemble are
connected through linear layers that regularize both encoders and decoders to align the latent repre-
sentations to be similar up to a linear transformation. We show in theory that the regularization by
the VAE ensemble forces the linear transformations to be trivial transformations and show improved
performance on the unsupervised disentangled representation learning. The theoretical discussion
in Section 4 is based on the original VAE objective, and our experiments also focus on the ensemble
with original VAE. We believe such framework can be extended to other disentangled VAE models,
or even a mixture of different VAE models, as long as the regularization by the ensemble does not
conflict with the augmented objective of these models.
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
9
Under review as a conference paper at ICLR 2021
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
References
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β -VAE. (NIPS), 2017.
Tian Qi Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating Sources of Disentangle-
ment in Variational Autoencoders. (ICLR):1-13, 2018.
Harris Drucker, Corinna Cortes, Lawrence D Jackel, Yann LeCun, and Vladimir Vapnik. Boosting
and other ensemble methods. Neural Computation, 6(6):1289-1301, 1994.
Sunny Duan, Loic Matthey, Andre Saraiva, Nick Watters, Chris Burgess, Alexander Lerchner, and
Irina Higgins. Unsupervised model selection for variational disentangled representation learning.
In International Conference on Learning Representations, 2019.
Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. ICLR, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-Vae: Learning Basic Visual Concepts With a Con-
strained Variational Framework. ICLR 2017, (July):1-13, 2017.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a Definition of Disentangled Representations. arXiv, pp. 1-29,
2018.
Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ica. In Advances in Neural Information Processing Systems, pp. 3765-3773, 2016.
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and
generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 859-868, 2019.
AJ Hyvarinen and Hiroshi Morioka. Nonlinear ica of temporally dependent stationary sources.
Proceedings of Machine Learning Research, 2017.
Ilyes Khemakhem, Diederik P Kingma, and AaPo Hyvarinen. Variational autoencoders and nonlin-
ear ica: A unifying framework. arXiv preprint arXiv:1907.04809, 2019.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. Proceedings of Machine Learning
Research, 80:2649-2658, 10-15 Jul 2018.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114,
2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentan-
gled latent concepts from unlabeled observations. CoRR, abs/1711.00848, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Scholkopf, and L G Dec.
Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representa-
tions. arXiv, pp. 1-33, 2018.
Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Scholkopf, and
Olivier Bachem. On the fairness of disentangled representations. In Advances in Neural Informa-
tion Processing Systems, pp. 14584-14597, 2019.
10
Under review as a conference paper at ICLR 2021
Emile Mathieu, Tom Rainforth, N Siddharth, and Yee Whye Teh. Disentangling disentanglement in 374
variational autoencoders. arXiv preprint arXiv:1812.02833, 2018.	375
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement 376
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.	377
Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural 378
networks with canonical correlation. In Advances in Neural Information Processing Systems, pp. 379
5727-5736,2018.	380
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector 381
canonical correlation analysis for deep learning dynamics and interpretability. In Advances in 382
Neural Information Processing Systems, pp. 6076-6085, 2017.	383
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and 384
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.	385
Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca direc- 386
tions (by accident). In Proceedings of the IEEE Conference on Computer Vision and Pattern 387
Recognition, pp. 12406-12415, 2019.	388
Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197-227, 1990.	389
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image 390
recognition. arXiv preprint arXiv:1409.1556, 2014.	391
Sjoerd van Steenkiste, Francesco Locatello, Jurgen Schmidhuber, and Olivier Bachem. Are disen- 392
tangled representations helpful for abstract visual reasoning? In Advances in Neural Information 393
Processing Systems, pp. 14222-14235, 2019.	394
Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft. Towards 395
understanding learning representations: To what extent do different neural networks learn the 396
same representation. In Advances in Neural Information Processing Systems, pp. 9584-9593, 397
2018.	398
Nicholas Watters, Loic Matthey, Matko Bosnjak, Christopher P Burgess, and Alexander Lerchner. 399
Cobra: Data-efficient model-based rl through unsupervised object discovery and curiosity-driven 400
exploration. arXiv preprint arXiv:1905.09275, 2019.	401
11
Under review as a conference paper at ICLR 2021
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
A	Technical Lemmas
In this section, we give the lemmas used in the theoretical discussion in Section 4.
Lemma 1. (Jensen’s inequality) If g(x) is a convex transformation on x, then this convex transfor-
mation ofa mean g[E(x)] is less than or equal to the mean of the convex transformed value E[g(x)];
it is a simple corollary that the opposite is true of concave transformations.
Lemma 2. (AM-GM inequality) As an extension of Jensen’s inequality, given a list of non-negative
real numbers x1,x2,..., Xn, the arithmetic mean of this list ɪ En=I Xi is greater than or equal
to the geometric mean of the same list (Qn=I Xi) n1 ； and further the equality holds if and only if
χ1 = χ2 = ∙ ∙ ∙ = Xn.
Lemma 3. (Hadamard’s inequality). if M is the matrix having columns ci, then |det(M)| ≤
in=1 kci k; and the equality in Hadamard’s inequality is achieved if and only if the vectors are
orthogonal.
B	Distance to Orthogonality (DtO)
In this section, we introduce the detail of Distance to Orthogonality (DtO) that is used in our exper-
iment to check if the linear transformations in the VAE ensemble approach trivial transformations.
This measurement is also used in (Rolinek et al., 2019) for a similar purpose. DtO is the Frobe-
nius norm of the difference between a square matrix M and its closest signed permutation matrix
P(M). Finding P(M) can be formulated as a mixed-integer linear programming (MILP) problem
as following:
mPin	|Mi,j -P(M)i,j|
i,j
s.t. P (M)i,j ∈ {-1,0,1},	∀(i, j)
X|Pi,j| = 1,	∀j	(8)
i
X|Pi,j| = 1,	∀i
j
By introducing new variables Pi+,j, Pi-,j ∈ {0, 1} and Di,j = |Mi,j - P (M)i,j |, we can reformulate
the above optimization problem as:
mPin	Di,j
i,j
s.t. (Pi+,j-Pi-,j)-Mi,j ≤Di,j,	∀(i,j)
Mi,j-(Pi+,j-Pi-,j)≤Di,j,	∀(i,j)
X(Pi+,j+Pi-,j)=1,	∀j
i
X(Pi+,j+Pi-,j)=1,	∀i
j
Using this optimization formulation, DoT of a given matrix M ∈ Rn×n is defined as:
DOT = n12 X ∣Mi,j - P(M)i,j I
i,j
(9)
(10)
12
Under review as a conference paper at ICLR 2021
Figure 5: Geometric interpretation of the cross-model reconstruction between a disentangled repre-
sentation space and an entangled representation space.
C Geometric Interpretation of the Effect of Cross-model	424
Reconstruction	425
Given a disentangled and entangled latent representation space, Fig. 5 illustrates the effect of the 426
cross-model reconstruction by VAE ensemble. The left part shows the orthogonal transformation 427
from a disentangled representation to an entangled space, and the right part shows the transfor- 428
mation in the opposite direction. As shown in the figure, the orthogonal transformation from the 429
disentangled representation to the entangled space does not introduce larger variance than the en- 430
tangled representation. Hence, we can expect similar cross-model reconstruction and within model 431
reconstruction. However, the transformation from the entangled representation to the disentangled 432
space introduces larger variance (yellow shaded area over blue area on the right) than the disentan- 433
gled representation. This leads to larger cross-model reconstruction by the disentangled model. 434
D Model Architecture and Training Details	435
We conducted our experiments, including training and evaluating the current state-of-the- 436
art disentanglement models as well as evaluating the proposed VAE ensembles, using the 437
disentanglement_lib 1 open-source library (Locatello et al., 2018).	438
Table 3	shows the encoder and the decoder architecture of the VAE model used in our experiments. 439
This architecture is the same as the one used in the original β-VAE Higgins et al. (2017).	440
Encoder	Decoder
Input 64×64 binary/RGB image	Input Rd
4×4 conv, 32 ReLu, stride 2, pad 1	FC d×256, ReLu
4×4 conv, 32 ReLu, stride 2, pad 1	4×4 upconv, 64 ReLu, stride 1
4×4 conv, 64 ReLu, stride 2, pad 1	4×4 conv, 64 ReLu, stride 2, pad 1
4×4 conv, 64 ReLu, stride 2, pad 1	4×4 conv, 32 ReLu, stride 2, pad 1
4×4 conv, 256 ReLu, stride 1	4×4 conv, 32 ReLu, stride 2, pad 1
FC 256 × (2×d)	4×4 conv, nc , stride 2, pad 1
Table 3: Encoder and Decoder architecture, d: dimension of the latent representation; nc: number
of input image channel (For dSprites dataset nc = 1, for CelebA dataset nc = 3).
Table 4	shows the hyperparameters setting used throughout the experiments. These parameters are 441
fixed for all the experiments.	442
E	Additional Experimental Results	443
In this section, we present the additional results including the DtO and “polarized regime” analysis 444
on the models trained on the CelebA dataset similar to the ones conducted on dSprite dataset in 445
Section 5; the ablation results with DCI-Disentanglement metric and the DtO estimation; and the 446
1https://github.com/google- research/disentanglement_lib
13
Under review as a conference paper at ICLR 2021
Parameter value
Batch size	64
Latent dimension	10
Optimizer	Adam
Adam: beta1	0.9
Adam: beta2	0.999
Learning rate	1e-4
Table 4: Hyperparameters setting.
108 6 4 2
(IOQ) A4=eu060q 七 o2ΦUUJSSQ
(J.OCl)H
Figure 6: Characteristics of the linear transform between latent representations. The latent dimen-
sion is set to 10 and the results are over 10 runs of training on the dSprite dataset. (a) Comparing
the DtO of linear transformations in the VAE ensemble (γ=10) and the one between original VAEs.
(b) VAE ensemble (γ=10) with different ensemble size all achieve small DtO of the linear transfor-
mations between the models.
(a)
(b)
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
latent traversal of the trained model on both dSprite and CelebA dataset along with further discuss
the effect of VAE ensemble on the latent representation.
E.1 Characterization of the Linear Transformation in VAE Ensemble
In Figure 6, we show the DtO estimation of the linear transformations in the ensemble during train-
ing for the dSprite dataset. We report the mean and standard deviation of DtO across all linear
transformations over 10 different runs. Furthermore, we compare these results with a VAE baseline
where DtO is estimated for the linear transformations between original VAEs. Specifically, we train
ten different VAEs separately and estimate the DtO of the pairwise linear transformation among
these models during training. Similarly we report the mean and standard deviation of these DtO es-
timations. As seen in the figure, the VAE ensemble models with different ensemble size all approach
to trivial transformations between the individual models, while the original VAEs do not have such
property. A similar result is also found in models trained for CelebA dataset. Similar to the results
in Figure 6, we observe decreased DtO of the linear transformations in the VAE ensemble during
training.
We also compare models trained with different latent dimension size. We observe decreased DtO
as the latent dimension of the model increases in Figure 7. This is because, as discussed in the
main paper, the VAE ensemble encourages more “active” latent variables. Models with higher latent
dimension likely to learn a decomposition of generative factors. As a result, the alignment of the
latent variable between different models are easier thus the linear transformations between the latent
representations is closer to the trivial transformation. On the contrary when there are less latent
variables in the model than the generative factors, some of the latent variables will capture more
than a single generative factor. As a result, the one-to-one mapping between the latent variables of
different models will not lead to a trivial transformation.
14
Under review as a conference paper at ICLR 2021
(J-O92-BUO6θl∣ 七 OSUUBtnQ
Figure 7: Distance to Orthogonality (DtO) measurement of the linear transforms between latent
representations in VAE-≡2 during training on the CelebA dataset. We also compare models with
different latent dimensions of 10, 16 and 32 and the results are averaged over 5 runs. In the figure
legend, We use “VAE_Ei nd Y = g” to represent VAE Ensemble (VAE_E) model with i individual
VAE models, n latent dimensions and γ value equal to g.
In Figure 9 and Figure 10, we show the “polarized regime” estimation for models in VAE-E2 and 470
VAE-≡3 trained for CelebA datasets, respectively. Similar to the results in Figure 3, individual 471
models in the VAE ensemble tend to have similar ‘polarized regime”, and higher γ enforces the “po- 472
larized regime” by separating “passive” latent variables from the “active” ones. When we compare 473
between VAE卫2 and VAE-≡3, we observe increased “active” latent variables similar to the result 474
on dSprite dataset in Section 5. More importantly, as discussed earlier, latent variables in a model 475
with limited latent dimensions need to capture more than a single generative factor, especially for 476
a complicated real-world dataset such as CelebA. This makes the linear transformation between the 477
latent representations less trivial. As the latent dimension size grows, such constraint is relaxed and 478
the linear transformations are closer to trivial.	479
These additional results confirm the conclusion in Section 5: (1) as the ensemble size increases, 480
DtO increases due to the difficulty of aligning the latent representations among different models; 481
(2) as the model latent dimension increases, DtO decreases due to the increased model capacity, and 482
encourages the one-to-one mapping between latent variables in different models; (3) hyperparameter 483
γ does not affect DtO significantly, but plays an important role on separating “active” and “passive” 484
latent variables, especially when the latent dimension is large enough.	485
Furthermore, we believe the DtO measurement of the linear transformation in VAE ensemble could 486
be a useful indicator for latent dimension size. As shown in Figure 7 and Figure 9, when the latent 487
dimension is sufficient for a given dataset, the DtO of the linear transformation is small and some 488
latent variables are pushed to “passive” mode.	489
E.2 Ablation Study
490
Similar to the ablation result shown in Section 5, here we show the same ablation study using the DCI 491
Disentanglement metric in Fig. 8(a) as well as the DtO measurement 8(b). Similar as the results of 492
the FactorVAE metric in Fig. 4, removing either component leads to a lower DCI Disentanglement 493
metric for the VAE ensemble. Without the linear transformation loss, the performance of VAE 494
ensemble decreases significantly across different ensemble sizes. Fig. 8(b) shows that for VAE 495
15
Under review as a conference paper at ICLR 2021
org. γ=l
org. y = 5
iM org. y = 10
w.o. CR y = 1
w.o. CR y = 5
w.o. CR y = 10
w.o. LT
(a) Comparison on the DCI-Disentanglement score
org. y = 1
org. y = 5
org. y = 10
w.o. CRy=I
w.o. CR y, = 5
w.o. CR y, = 10
w.o. LT
vae_e2 vae_e3 vae_e4 vae_e5
(b) Comparison on the Distance to Orthogonality (DtO)
Figure 8: Ablation study to understand the effect of cross-model reconstruction and linear transfor-
mation in the VAE ensemble objective using the DCI Disentanglement metric and DtO. (w.o. CR -
without cross-model reconstruction loss; w.o. LT - without linear transformation loss; org - original
VAE ensemble loss)
496
497
498
499
500
501
502
503
504
505
506
507
508
509
ensemble without the cross-model reconstruction, the linear transformations among models are close
to a trivial transformation (signed permutation). This implies the orthonormal transformation of the
linear transformations. This result further supports our intuitive justification in Appendix C that the
cross-model objective encourages entangled models to align to disentangled models. Indeed, we see
that adding the cross-model reconstruction can further reduce the DtO of the linear transformations
among the models in the ensemble.
E.3 Latent Traversal
In this section we show the latent traversal of models trained on both dSprites and CelebA datasets.
For a fixed input image, to extract the latent traversal we change the value of a single latent variable
zi in the corresponding encoding, and observe the generated output image to understand the effect
of zi . The range of the value are usually chosen to be from -3 to 3 due to the standard Gaussian
prior.
In Figure 11, we show the latent traversal for both VAE-E2 and a single VAE model with 10 latent
dimensions trained on dSprites dataset. Three images as shown in the last column of each block
16
Under review as a conference paper at ICLR 2021
are used as input. Both models are able to capture certain generative factors of the data including 510
position, shape, rotation and scale. In Section 5, we argue that the representation by VAE ensemble 511
encourages more “active” latent variables, thus can capture a decomposition of the ground truth gen- 512
erative factors. Especially from the “polarized regime” estimation in Figure 3, we observe that some 513
latent variables in the VAE ensemble are in-between “active” and “passive” modes. This suggests 514
that the VAE ensemble model generates input-dependent factors based on the input complexity. In 515
Figure 11, we observe this behavior highlighted with color boxes. The traversal on the second la- 516
tent variable z2 shows that an ellipse shape does not lead to an “active” latent variable. However, 517
both heart and square shape lead to an “active” latent variable that changes the output. In contrast, 518
the single VAE model does not have such behavior where the “active” modes are consistent across 519
different input data.	520
In Figure 12 and Figure 13, We show the latent traversal for both VAE-≡2 and a single VAE model 521
with 16 latent dimensions trained on CelebA dataset, respectively. In this real-world dataset, the 522
generative factors are unknown. We observe different factors including background, azimuth, gen- 523
der, hair style being captured by both models. Similar as before, the single VAE model maintains 524
similar “active” mode for all latent variables where similar traversal patterns are observed for both 525
input images. However, VAE-E2 shows semantically consistent but input-dependent “active" mode. 526
This is translated into different traversal effects and more realistic and sharper images by VAE-E2, 527
especially for the first input image that is less common in the dataset. We believe this is important 528
towards a meaningful compositional latent representation learning.	529
Overall, the latent traversal results in this section confirm the findings on the inner working of the 530
VAE ensemble shown in the previous section as well as the discussion in Section 5.	531
17
Under review as a conference paper at ICLR 2021
Latent Dimensions
(a) VAE-E2 With latent dimension of 32, CelebA data.
(b) VAEH with latent dimension of 16, CelebA data.
(c) VAE_E2 with latent dimension of 10, CelebA data.
Figure 9: The “polarized regime” comparison between models in VAE-E2. The results are over 5
runs of training on the CelebA dataset.
18
Under review as a conference paper at ICLR 2021
(b) VAEE With latent dimension of 16, CelebA data.
(C) VAE_E3 with latent dimension of 10, CelebA data.
Figure 10: The “polarized regime” comparison between models in the VAE_E3 on the CelebA
dataset.
19
Under review as a conference paper at ICLR 2021
(b) Single VAE
-esJ8>e.IL
Zl Z2 Z3 Z4 Z5 Z6 Z7 Z8 Zθ ZlO Zl Zl Z3 Z4 Z5 Zβ Z7 Z8 Z9 ZlO
Zl Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 ZlO
(c) β- VAE
ωSJΦ> 巴一
(d) FactorVAE
Figure 11: Latent traversal on three different input images using VAE_E?, a single VAE and the
state-of-the-art VAE models with 10 dimensional latent representation. The three input images are
ellipse, heart and square shapes as shown in the last column.
20
Under review as a conference paper at ICLR 2021

(e) DIP-VAE-I
ωSJΦ>ω.ll
(f) DIP-VAE-II
-BSJ φ> 巴一
(g) TC-VAE
Figure 11: (cont.) Latent traversal on three different input images using VAE_E2, a single VAE and
the state-of-the-art VAE models with 10 dimensional latent representation. The three input images
are ellipse, heart and square shapes as shown in the last column.
21
Under review as a conference paper at ICLR 2021
Sna
mis

C《豆豆 CCN¾¾⅞
Ims
^ɪsɪnɪɪ
豆豆々女直立IMe££
t i £ U 宜宜>■¥-0⅝lB⅞l3Rl3l∙
-?9tis
OOθ⅛⅜⅜*⅞⅞
⅞I^II^^I^NI^PI⅞⅞⅝rt力
•••★■r+lw
⅞⅞⅞⅞l^l^^^4^^∙zl
Zlo∙
二二 amxz6!999? f
i⅛⅛uE EC4G4A<⅞hτ股⅞⅞∙>1>∙
直直立 £2∙aτ着7!z7QQe® ⑥ 0 够。⅛
ClAaaaR &£££北A■・立■■•飞飞击
交交 H M £ ⅛laZ4Sl9999i


黔电也a c CZ3
，理 e^ ⅜nx 力 £ e- Sm
■■xiEf宜总交产CrCCCFCaOW
-ESJaAeJl
-ESJBAeJl
Figure 12: Latent traversal on two different input images of CelebA dataset using VAE旦 with
latent dimension of 16.
22
Under review as a conference paper at ICLR 2021
Xxsmz
lɪɪɪɪ
SUB
¾¾¾⅛r99c》ŋ电
■FA C W〔9 KFGTPTI
¾⅛⅝⅛CCCocrQ
CCe CCc GGeWP
jCJC C GlG .OMT -
一9CC
CC CC CGFZg
4 re CQo, C"tZs
⅞⅞^^,tk^^⅛⅛Z7
¾00 电e⅞0 电©。左
FFfFe>■CeDwZ5
Uc: 口 iz4
C π C'90⅞qZ3
■■S9SXXX££ 力¾0r9C 窗Geoe⅛ *
xx⅞^ul^τrp??CCGooɔ勾
-Bs」a>B」l
-PS」①'B」1
Figure 13: Latent traversal on two different input images of CelebA dataset using a single VAE with
latent dimension of 16.
23