Under review as a conference paper at ICLR 2021
A Communication Efficient Federated Kernel
k-MEANS
Anonymous authors
Paper under double-blind review
Ab stract
A federated kernel k-means algorithm is developed in this paper. This algorithm
resolves two challenging issues: 1) how to distributedly solve the optimization
problem of kernel k-means under federated settings; 2) how to maintain commu-
nication efficiency in the algorithm. To tackle the first challenge, a distributed
stochastic proximal gradient descent (DSPGD) algorithm is developed to deter-
mine an approximate solution to the optimization problem of kernel k-means. To
tackle the second challenge, a communication efficient mechanism (CEM) is de-
signed to reduce the communication cost. Besides, the federated kernel k-means
provides two levels of privacy preservation: 1) users’ local data are not exposed
to the cloud server; 2) the cloud server cannot recover users’ local data from the
local computational results via matrix operations. Theoretical analysis shows: 1)
DSPGD with CEM converges with an O(1/T) rate, where T is the number of
iterations; 2) the communication cost of DSPGD with CEM is unrelated to the
number of data samples; 3) the clustering quality of the federated kernel k-means
approaches that of the standard kernel k-means, With a (1 + e) approximate ra-
tio. The experimental results show that the federated kernel k-means achieves the
highest clustering quality With the communication cost reduced by more than 60%
in most cases.
1	Introduction
Conventionally, kernel k-means (Dhillon et al., 2004) is conducted in a centralized manner Where
training data are stored in one place, such as a cloud server. HoWever, as a rapidly groWing num-
ber of devices are connected to the Internet, the volume of generated data increase exponentially
(Chiang & Zhang, 2016). Uploading all these data to the cloud server can lead to large cost of
communication bandWidth. For example, a smartphone manufacturer usually needs to analyze us-
age patterns of its smartphones, purposing to optimize energy consumption performance of these
smartphones. The usage patterns can be obtained by clustering users’ energy consumption data via
kernel k-means. HoWever, if the number of users reaches the order of millions, it may not be a
cost-effective scheme to upload all the users’ energy consumption data to the cloud server. Besides,
uploading users’ raW data to the cloud server can lead to data privacy issues. To resolve these issues,
a promising approach is to develop a distributed kernel k-means algorithm that can be executed
under federated settings (McMahan et al., 2017; Yang et al., 2019) Where raW data are maintained
by users and the cloud has no access to the raW data. In this algorithm, a local training process
is conducted at each user’s device, based on the local data only. The local computational results,
rather than the local data, are then uploaded to the cloud server to accomplish the kernel k-means
clustering. During this procedure, users’ local data are no longer exposed to the cloud server, Which
provides a basic level of privacy. Besides, it is usually more communication efficient to upload the
local computational results than to upload the local data to the cloud server.
HoWever, it is nontrivial to design a federated learning algorithm for kernel k-means due to three
challenging issues: 1) hoW to solve the optimization problem of kernel k-means in a distributed
manner Without sending users’ data to a central place; 2) hoW to maintain communication efficiency
in the algorithm; 3) hoW to protect users’ data privacy in the algorithm. Considering the first issue
under federated settings, the key problem is to obtain the top eigenpairs of the kernel matrix K (as
required by kernel k-means) in a distributed manner. To solve this problem, a distributed stochastic
proximal gradient descent (DSPGD) algorithm is developed as folloWs. Since K is not available
1
Under review as a conference paper at ICLR 2021
under federated settings, an estimate of K, denoted as ξ, is first constructed distributively at users’
devices based on random features (Rahimi & Recht, 2008) of local data samples. Since the estimate
is distributed among different devices, it is processed by the distributed Lanczos algorithm (DLA)
(Penna & Staiiczak, 2014) to obtain an estimate of K (denoted as Z) at the cloud server. Afterwards,
an approximate version of the top eigenpairs of K can be obtained from Z through singular value
decomposition (SVD). To improve the accuracy of approximation, the former steps are conducted
in an iterative way. More specifically, in the t-th iteration, an estimate ξt is constructed at users’
devices, and then the estimate Zt at the cloud server is updated to Zt+1 via stochastic proximal
gradient descent (SPGD) (Zhang et al., 2016). It is proved that, after sufficient iterations, Zt can
converge to a low rank matrix whose top eigenpairs are the same as those of K1 As a result, top
eigenpairs ofK are finally obtained at the cloud server.
To resolve the second issue, DLA operations in DSPGD need to be enhanced to reduce commu-
nication cost. When DLA is executed in DSPGD, the process of obtaining an updated Zt at the
cloud server results in high communication cost between users’ devices and the cloud server, be-
cause the operation is conducted upon matrices (e.g., ξt) with the number of rows/columns equal
to the number of data samples. To prevent the communication cost from growing with the number
of data samples, a communication efficient mechanism (CEM) is designed so that DLA is operated
upon a different type of matrices whose dimensions are reduced and independent of the number of
data samples. More specifically, a new matrix Wt is designed such that: 1) WtWJ has the same
eigenvectors as those of Zt+1, but its eigenvalues are smaller by a constant; 2) Wt and Zt can be
constructed distributively at users’ devices based on local values of ξt. Furthermore, DLA is applied
to WtJWt (instead of WtWtJ), so its operations are performed upon matrices with a highly re-
duced dimension. Via DLA operations between users’ devices and the cloud server, Wt and Zt are
updated iteratively, and then the top eigenpairs of WtJWt are obtained at users’ devices. Once Zt
converges, users’ devices transform the top eigenpairs of WtJWt to those of WtWtJ and further
obtain the eigenpairs of Zt . Instead of sending these eigenpairs to the cloud server, a distributed
linear k-means algorithm (Balcan et al., 2013) is incorporated into CEM so that the cloud server can
perform clustering directly on the eigenpairs of the converged Zt. As shown in the process of CEM,
the communication efficiency of DSPGD is significantly improved.
For the third issue, FK k-means based on DSPGD and CEM provides two levels of privacy preser-
vation: 1) users’ local data are not exposed to the cloud server; 2) the cloud server cannot recover
users’ local data from the local computational results via matrix operations. To provide stronger pri-
vacy, a differential privacy mechanism (Dwork et al., 2006) needs tobe integrated with FK k-means,
which is subject to future study.
The theoretical analysis shows that DSPGD with CEM converges to Z* at an O(1∕T) rate, where
T is the iteration number. The communication cost of DSPGD with CEM is linear to the dimension
of the right singular vector times the number of users, which can be much smaller than the number
of data samples. The clustering quality of the federated kernel k-means approaches that of kernel k-
means, witha (1+e) approximate ratio. The experimental results show that, compared with the state-
of-the-art schemes, FK k-means achieves the highest clustering quality with the communication cost
reduced by more than 60% in most cases.
2	Related Work
2.1	DISTRIBUTED KERNEL k-MEANS
Many algorithms have been developed to conduct the kernel k-means clustering in a distributed
way. The kernel approximation method is a popular approach employed in these algorithms, such as
the NyStrOm method (Chitta et al., 2011; 20l4; Wang et al., 2019) and the random feature method
(Chitta et al., 2012). A trimmed kernel k-means algorithm (Tsapanos et al., 2015) decreases the
computational cost and the space complexity by significantly reducing the non-zero entries in K
via a kernel matrix trimming algorithm. In (Elgohary et al., 2014) an approximate nearest centroid
(APNC) embedding is developed to embed the data samples so that the clustering assignment step
1More specifically, the top eigenvectors of the low rank matrix are the same as those of K and its nonzero
eigenvalues are smaller than those of K by a constant.
2
Under review as a conference paper at ICLR 2021
of kernel k-means can be parallel executed. A communication efficient kernel principle component
analysis (PCA) algorithm (Balcan et al., 2016) along with distributed liner k-means can approx-
imately solve the optimization problem of kernel k-means while maintaining the communication
efficiency. However, these algorithms are designed with an assumption that they are executed at the
cloud server where users’ raw data are collected. Besides, many of these algorithms (Chitta et al.,
2011; 2012; 2014; Wang et al., 2019) are one-shot algorithms, i.e., they only determine an approx-
imate kernel matrix K once. Thus, their clustering quality is limited by the accuracy of K. In
contrast to these algorithms, FK k-means is the first distributed kernel k-means scheme designed
under federated settings. In addition, FK k-means is an iterative algorithm that can approach the top
eigenpairs of K more accurately by employing more iterations.
2.2	Federated Learning
Federated learning (McMahan et al., 2017) is a new machine learning framework aiming to protect
users’ data privacy and save the communication cost during the learning process. In the framework,
a local model is updated at each user’s device, and these local models instead of users’ local data are
then aggregated at the cloud server to generate a global model. The distributed optimization method
in the framework is applicable to the models whose optimization problem can be decomposed into
several independent subproblems, such as neural networks. and many algorithms (Konecny et al.,
2016; Yang et al., 2018; Yurochkin et al., 2019) are developed. However, it is non-trivial to decom-
pose the optimization problem of kernel k-means under the federated learning framework. Some
algorithms (Liu et al., 2017; Caldas et al., 2018) improve federated multi-task learning (Smith et al.,
2017) with kernel. However, these algorithms either employ explicit feature mapping (Liu et al.,
2017) that can lead to impractical computational cost, or require to send the support vectors of users’
local data (i.e., some local data samples) to the cloud server (Caldas et al., 2018), which can leak
users’ privacy information. Due to these limitations, these algorithms are not applicable to kernel
k-means under the federated learning framework. Recently, a concept of clustered federated learn-
ing (Ghosh et al., 2020; Sattler et al., 2020; Mansour et al., 2020) is proposed, where the clients are
clustered according to their gradient updates or their local models. However, their clustering prob-
lems are different from the optimization problem of kernel k-means, and thus are not feasible for
the optimization problem of kernel k-means in the federated setting.
2.3	Stochastic Kernel PCA
In (Zhang et al., 2016), the stochastic kernel PCA is accomplished a stochastic proximal gradient
descent (SPGD) algorithm. As a result, SPGD is a centralized counterpart of the distributed proxi-
mal gradient descent (DSPGD) algorithm in FK k-means. However, DSPGD is distinct from SPGD
in three features. First, DSPGD is conducted under federated settings while SPGD is conducted in
a centralized manner where users’ raw data are collected at the cloud server. Second, the communi-
cation cost is considered in the design of DSPGD, which results in CEM, while the communication
cost is not considered in SPGD. Third, although both DSPGD and the SPGD aim at approaching the
top eigenpairs of K, in the t-th iteration, DSPGD only needs to obtain an approximate solution Zt+1
to the problem of updating Zt instead of the exact solution Z∖ι to the same problem like SPGD,
which leads to less communication cost under federated settings.
3	Preliminary
Let {xi}N=ι ⊆ X be a set of N data samples. Given a feature mapping φ(∙) : X → H and the
number of clusters k, the problem of kernel k-means whose objective is to find an optimal indicator
matrix Y* can be written as
min Tr(K) - Tr(L1 YτKYL2) s.t, Y1k = 1n,	(1)
Y∈{0,1}N×k
where K is the kernel matrix with each entry Kij = φ(xjτφ(xj), L1 = Diag([√=ι,..., √N= ]) is
a diagonal matrix, Ni is the number of samples in the i-th cluster, and 1k is a column vector with all
the k items equal to 1. However, the problem in equation (1) is an NP-hard problem (Garey et al.,
1982; Wang et al., 2019). To this end, an approximate solution Y is required. An efficient approach
3
Under review as a conference paper at ICLR 2021
to obtaining the approximate solution is as follows. K is decomposed as K = UΛUτ via eigen-
value decomposition (EVD), and then linear k-means is applied to the matrix H = UΛ2 to obtain
二，_. _________________________________ _ __________ 一 ・ • . ・ ■
Y (Ding et al., 2005; Chitta et al., 2012; Wang et al., 2019). To reduce the computational complex-
ity, only the first s column vectors of H are selected as the input of linear k-means (Cohen et al.,
2015).
4	FEDERATED KERNEL k-MEANS
InFK k-means, the approximate solution to the problem in equation (1) is also determined based on
the top-s eigenpairs of the kernel matrix K. To obtain these eigenpairs under federated settings, a
distributed stochastic proximal gradient descent (DSPGD) algorithm is developed in Section 4.1. A
communication efficient mechanism is then designed to reduce the communication cost of DSPGD
in Section 4.2.
4.1	Distributed Stochastic Proximal Gradient Descent
The key problem for designing FK k-means is to obtain the the top-s eigenpairs of K in a dis-
tributed manner. To solve this problem, a distributed stochastic proximal gradient descent algorithm
is developed as follows. Under the federated settings, the main challenge on determining the top-s
eigenpairs of K is that K is not available since users’ local data cannot be exposed to the cloud
server or other users. To this end, an estimate of K, denoted as ξ, is constructed distributively at
users’ devices based on the random features (Rahimi & Recht, 2008; Kar & Karnick, 2012) of their
local data samples. More specifically, ξ = D AAτ and E[ξ] = K, where D is the number of ran-
dom features of each data samples, and A = [A[1]τ , . . . , A[M]τ]τ is the random feature matrix
distributed over M users’ devices (the details of the random feature method are included in Ap-
pendix A). Since ξ is distributed over users’ devices, it is then processed by the distributed Lanczos
algorithm (DLA) (Penna & Stanczak, 2014) to obtain an estimate of K, i.e., Z at the cloud server.
Afterwards, an approximate version of the top eigenpairs ofK can be obtained from Z through SVD.
To improve the accuracy of approximation, one method is to increase the value ofD. However, this
method is only feasible when each user’s device has enough memory space. To adapt DSPGD to de-
vices with different memory space, DSPGD improves the accuracy of approximation via an iterative
method. More specifically, in the t-th iteration, an estimate ξt is constructed at users’ devices, and
then the estimate Zt at the cloud server is updated to Zt+1 via stochastic proximal gradient descent
(Zhang et al., 2016):
Zt+ι = argmin 5HZ - Zt||F + ηt(Z - Zt, Zt - ξt) + ηtλllZl1*,
Z∈Rn×n 2
where ηt is a learning rate. Zt+ι has an explicit expression Zt+ι = Eija t>mλ (λi,t - ηtλ)u"Kt,
where (Ui,t, λ*t) is the i-th eigenpair of a matrix Rt = (1 - ηt)Zt + ηtξt∙ Since ξ is distributed
over users’ devices, Zt+1 is determined via DLA at the cloud server. It is proved that, after sufficient
iterations, Zt can converge to a low rank matrix K = Eo (λi - λ)uiuτ where (ui, λi) is the
i-th eigenpair of K. As a result, the top eigenpairs ofK are finally obtained at the cloud server.
The t-th iteration of DSPGD is executed as follows. The main task is to approach the top eigenpairs
of Rt via DLA. The cloud server first initializes a random vector c1 ∈ RN . In the q-th iteration of
DLA, the cloud server determines a vector gq = Rtcq = (1 - ηt)ZtCq + ηtAt ATCq/D, where
Ztcq is computed at the cloud server, and AtAtτcq is computed in a distributed manner. The
computation of AtAtτcq is accomplished by five steps: 1) the cloud server partitions the vector
cq = [cq[1]τ, . . . , cq[M]τ]τinto M parts and sends the m-th part cq[m] to the user m; 2) the user
m computes a local vector At [m]τcq [m] and uploads the vector to the cloud server; 3) the cloud
server sums up these vectors to obtain the vector Atτcq and then broadcasts this vector to all the
users; 4) the m-th user computes a new local vector At [m]Atτcq and then uploads this vectors to
the cloud server; 5) the cloud server finally concatenates these vectors from users to form AtAtτcq;
When gq is determined, the cloud server then applies the Lanczos algorithm to the collected vectors
{g1, ..., gq} to approximate the top eigenpairs of Rt (the details about the Lanczos algorithm and
4
Under review as a conference paper at ICLR 2021
the complete procedure of DLA are provided in Appendix B). After sufficient iterations of DLA, the
top eigenpairs of Rt are obtained at the cloud server, and then Zt+1 is determined accordingly.
4.2	Communication Efficient Mechanism
When DLA is executed in DSPGD, the process of obtaining an updated Zt at the cloud server results
in high communication cost between the cloud server and users’ devices, because its operation is
upon matrices (e.g., ξt) with the number of rows/columns equal to the number of data samples. To
prevent the communication cost from growing with the number of data samples, a communication
efficient mechanism (CEM) is designed so that DLA is operated upon a different type of matrices
whose dimensions are independent from the number of data samples.
A new matrix Wt that satisfies WtWJ equals Rt is designed as follows. Let Zt = UJtΛtUJ: be
2
the eigendecomposition of Zt, and Bt equal UtΛj22. Based on Bt and the random feature matrix
At, Wt is constructed as Wt = [ʌ/ɪAt, √1 - ηBt]. Assume that Bt is divided like At, i.e.,
Bt = [Bt [1]J, . . . , Bt [M]J]J, and Bt[m] is maintained at the m-th user’s device. As a result, a
submatrix of Wt can be constructed at the m-th device by Wt [m] = [ʌ/ɪAt [m], √1 - ηt Bt [m]]∙
Furthermore, DLA is applied to WtJWt instead of WtWtJ . The number of rows/columns of
WtJWt equals rt+D where rt is the rank of Zt andD is the number of random features. Compared
with the number of data samples N, rt+D is usually much smaller than N, so the operation of DLA
is upon matrices with a highly reduced dimension. In the q -th iteration of DLA, the computation
of gq = WtJWtcq is accomplished by three steps: 1) the cloud server first broadcasts a vector
cq; 2) each user m computes a local vector Wt[m]JWt[m]cq and uploads the vector to the cloud
server; 3) the cloud server sums up these vectors to obtain gq. The cloud server then transforms gq
into cq+1 following the Lanzcos iteration. After sufficient iterations of DLA, the top-st eigenpairs
J
{(λi,t, V i,t), i = 1,..., st} of WJ Wt converge atthe cloud server. The cloud server then broadcasts
the obtained top-st eigenpairs to all the users’ devices. Since the user m keeps Wt [m], it then
determines St vectors Ui,t[m] = ʃɪ——Wt[m]Vi,t, i = 1,…，st, where Ui,t[m] is one part of the
λi,t
eigenvector Ui,t, i.e., U i,t = [U i,t[1]j,…，U i,t[M ]τ]τ. Based on the vectors {U i,t[m],i = 1,…，st}
and the eigenvalues {λi,t, i = 1, ..., st}, the user m also determines a submatrix Bt+1 [m] of Bt+1
via Bt+ι[m] = [ʌ/ʌi,t - ηtλUι,t[m],…，ʌ/--i,t - ηtλUi-ι,t[m]], which enables the (t + 1)-th
iteration of DSPGD with CEM.
As DSPGD with CEM converges after T iterations, a matrix H[m] is constructed at the m-
th user’s device by H[m]
[AT
+ (1 - ητ)λUι,T[m],..
ΛT
+ (1 - ητ)λUs,τ[m]].
distributed linear k-means algorithm (Balcan et al., 2013) is then applied to the rows of H =
[H[1]τ, ..., H[M]τ]τ to obtain the clustering result. DSPGD with CEM and the distributed linear
k-means algorithm constitute FK k-means. The pseudo code of FK k-means is shown in Algo-
rithm 1.
A
5	Theoretical Analysis
The convergence of DSPGD with CEM is analyzed in Section 5.1. The communication cost of
CEM is analyzed in Section 5.2, which shows CEM is important for FK k-means to maintain the
communication efficiency. It is then proved that the clustering quality of FK k-means can approach
that of the standard kernel k-means in Section 5.3. Besides, the privacy preservation provided by
FK k-means is analyzed in Section 5.4.
5.1	Convergence Analysis for DSPDG
The convergence rate of DSPGD with CEM is derived in Theorem 1.
Theorem 1. Define Y = maxt∈[τ] l∣Zt∣∣* andC2 = maxt∈[τ] ||Zt — ξtllF∙ Assume ∣∣ξt - K||f ≤
G, and l∣Zt - z*llF ≤ H, ∀t > 2. By setting ηt = 2/t, the following upper bound of
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Federated Kernel k-Means Algorithm
1: Input: The threshold parameter λ, the number of eigenvectors S to be approached, the number
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
of random features D, the maximal number of iterations T, local datasets Lm, m = 1, ..., M,
the initial local matrix B1 [m] = 0, m = 1, ..., M
Output: the clustering assignment for each data sample
Server executes:
for t = 1, 2, . . . , T do
Initialize ηt = 1/t, q = 0
for each client m, m = 1, 2, ..., M in parallel do
Compute At [m] by applying a random feature method to Lm
Construct Wt[m] = [^DAt[m], √1 - ηBt[m]]
end for
∖	T∙7∙Tτ∙7∙
CallDLAto determine the eιgenpairs {(λi,t, V ι,t),i = 1,…，st} of Wt Wt
for each client m, m = 1, 2, ..., M in parallel do
Compute Ui,t[m] = -4= Wt[m]Vi,t fori = 1,…，st
V λi,t_________ ________________________
Compute Bt+ι[m] = [CLt - ηtλUι,t[m],..., JxSt,t - ηtλUst,t[m]]
end for
end for
for each client m, m = 1, 2, ..., M in parallel do
Construct H[m] = [ λx1,T + (1 - ηT)λux 1,T [m], ...,	λxs,T + (1 - ηT)λux s,T [m]]
end for
Apply a distributed linear k-means algorithm over the rows of H = [H[1]τ,..., H[M]τ]τ
Return clustering assignment for each data sample
l∣Zτ+1 - Z」F holds with a probability at least 1 - δ
||ZT +1 - z* ||F
≤ T (C2 + λγ + 2G2τ + 3GHT + GH)
O(1/T),
(2)
where T = log「2 log2 T〕.
The result in Theorem 1 indicates that DSPGD with CEM converges to Z* at an O(1∕T) rate. The
proof of Theorem 1 is provided in Appendix C.
5.2	Communication Cost Analysis for CEM
Define the communication cost as the number of floating-point numbers uploaded from users’ de-
vices to the cloud server. In Theorem 2, the communication cost of DSPGD with CEM and the
communication cost of DSPGD without CEM are both analyzed.
Theorem 2. For DSPGD with CEM, in the t-th iteration, its communication cost is linear to rt + D
where rt is the rank of Zt and D is the number of random features. Define the communication ratio
as the ratio of the communication cost of DSPGD without CEM to that of DSPGD with CEM. In the
t-th iteration, the communication ratio equals MN+M+D)Q0 where N is the number of data samples;
M is the number of users; Q0 and Q1 are the number of Lanczos iterations for DSPGD without
CEM and DSPGD with CEM, respectively.
By Theorem 2, the communication cost of DSPGD with CEM is unrelated to the number of data
samples N, and the communication cost reduced by CEM can be revealed by the ratio. The values
of Q0 and Q1 are affected by the selection of initial vector c1. However, empirically the values of
Q0 and Q1 are at the same order no matter which initial vectors are chosen. The dominant factor
of the ratio is still M+M+D). Since Zt is used to approach the top-s eigenpairs of K, empirically
its rank rt has an upper bound. In our experiments, the value of rt is at the same order of s, i.e.
the number of eigenvectors ofK to be determined by DSPGD. Usually, the number of data samples
at a user’s device is much larger than s so that it is easy to satisfied that N > Mrt, and CEM can
definitely reduce the communication cost for DSPGD in these cases. The proof of Theorem 2 and
the empirical results for rt are given in Appendix D.
6
Under review as a conference paper at ICLR 2021
5.3	APPROXIMATE RATIO ANALYSIS FOR FEDERATED KERNEL k-MEANS
Before the analysis, a γ-approximate algorithm is first defined as follows.
Definition 1. A linear k-means algorithm is applied to a matrix H with n row, where an indicator
matrix Y is obtained. This algorithm is called a γ-approximate algorithm if, for any matrix H,
f(Y; H) ≤ γ minY f(Y; H) where f is the objective function of linear k-means.
It has been proved that the standard kernel k-means algorithm (Dhillon et al., 2004) is a γ-
approximate algorithm (Wang et al., 2019). The approximate ratio is then derived in Theorem 3
for FK k-means.
Theorem 3. The objective function of kernel k-means in (1) is denoted as fK. HT is the output
of DSPGD with CEM after T iterations, and a γ -approximate algorithm is applied to the first s
columns of HT to obtain YT . Assume the assumptions in Theorem 1 hold. For YT, the following
k
inequality holds With a probability at least 1 — δ(T), i.e., fκ (YT) ≤ γ(1 + ε + S) mmγ fκ (Y),
where ε = O(yJ~T).
Note that as T increases, δ(T) decreases. If S = O(k∕ε), T = O(k∕ε3), then the clustering quality
(in terms of the loss fK(Y)) ofFK k-means approaches that of the standard kernel k-means with a
(1 + ε)-approximate ratio. The proof of Theorem 3 is given in Appendix E.
5.4	PRIVACY ANALYSIS FOR FEDERATED KERNEL k-MEANS
FK k-means can provide two levels of privacy preservation: 1) one user’s local data are not exposed
to the cloud server and other users; 2) the cloud server cannot recover users’ local data from the
collected local computational results via matrix operation. The first level can be easily verified from
the procedure ofFK k-means. The second level is proved by Theorem 4.
Theorem 4. Based on the collected local computational results, the cloud server can at most recover
the matrices {Wt[m]τWt[m], m = 1,…,M} via matrix operations. Moreover, recovering the
random feature matrix At from such matrices is an ill-posed problem with infinite solutions.
By Theorem 4, the cloud server cannot recover the random feature matrices from the local com-
putational results. Without such random feature matrices, it is infeasible for the cloud server to
recover users’ local data via matrix operations. More explanation and the proof of Theorem 4 are
provided in Appendix F. Moreover, FK k-means can incorporate the differential privacy mechanism
(Dwork et al., 2006; Su et al., 2016) or random perturbation (Lin, 2016) to provide higher level of
privacy preservation, which is subject to future work.
6	Experiments
6.1	Experimental Setting
Four types of existing schemes are considered in the experiments: centralized kernel k-means
(Zha et al., 2001) (denoted as CK k-means), scalable kernel k-means (Wang et al., 2019) (denoted
as SK k-means), distributed kernel k-means with random feature (Chitta et al., 2012) (denoted as
RFK k-means), and communication efficient distributed kernel PCA (Balcan et al., 2016) (denoted
as CE PCA). CK k-means and SK k-means are executed at the cloud server (denoted as cloud-
based algorithms), and the rest methods are executed in a distributed manner where users’ raw data
cannot be uploaded to the cloud server (denoted as client-based algorithms). Besides, Gaussian ker-
nel is used in each algorithm. Four datasets are selected for performance evaluation: Three public
datasets (Mushrooms, MNIST, and Covtype) from the LIBSVM dataset repository and one dataset
(Smartphone) provided by a company. In addition, 20, 000 data samples are randomly selected from
the dataset MNIST to construct a dataset MNIST-small that is used to validate the convergence of
DSPGD. The statistical information of these datasets is given in Table 1. The description of the
Smartphone dataset and the existing methods are included in Appendix G.
The hyperparameters of FK k-means are determined as follows. The kernel parameter γ is com-
puted based on the average interpoint distance in the given dataset (Wang et al., 2019): γ =
7
Under review as a conference paper at ICLR 2021
Table 1: Datasets statistics and hyperparameter settings for FK k-means
Dataset	#samples (N)	#features (d)	#clusters (k)	#random features (D)	#eigenvectors (s)
Mushrooms	8, 124	112	2	15	2
MNIST-small	20, 000	780	10	200	10
MNIST	60, 000	780	10	200	12
Covtype	581, 012	54	7	30	10
Smartphone	177, 029	12	4	20	6
×10-2	N XIO-2
」0t山-l(υ>ouφa p<υz=BE-ION
(a) Mushrooms
3 2 1
」£山,Ia>ou3a PaZ--EUU-ION
O IO 20 30 40 50 O 10
Iteration
(b) MNIST-Small
20 30 40 50
Iteration

Figure 1: The convergence curves of the two versions of DSPGD and standard deviation curves of
the normalized recover error on the Mushrooms dataset and the MNIST-small dataset.
N「N2 ~-~. The threshold parameter λ of DSPGD is set to the (k + 2)-th eigenvalue
2 Ti=I Tj = I Ilxi-xj ∣∣2
obtained in the first iteration of DSPGD. The configuration of the number of random features D and
the parameter s in the top-s eigenvectors for each dataset is provided in Table 1 (the hyperparameter
configuration of the existing methods and a discussion on the configuration ofD are provided in Ap-
pendix G). In the experiments, M = 5 worker processes and one coordinator process are generated
to simulate users’ devices and the cloud server, respectively. The worker processes communicate
with the coordinator process via the message passing interface (MPI) in a synchronized manner. All
the experiments are executed in a server with one i7-6850k CPU and 32 GB RAM.
6.2	Experimental Results
The experimental results are presented from three aspects. First, the convergence results of DSPGD
is shown in Figure 1 to verify its convergence rate. Second, the average communication cost per
iteration of the two versions of DSPGD is provided in Figure 2 to show that CEM highly reduces
the communication cost of DSPGD. Third, in Figure 3, FK k-means is compared with the cloud-
based kernel k-means schemes in terms of clustering quality to show FK k-means can achieve the
comparable clustering results as that of the cloud-based schemes; FK k-means is also compared
with the existing distributed kernel k-means schemes under the federated settings to show the higher
communication efficiency ofFK k-means.
The convergence of DSPGD is validated over two datasets, Mushrooms and MNIST-small, whose
low rank matrices K = Ms=I λiuuj canbe computedby performing SVD on their kernel matrices
IiKt-K ∣∣2
K. A normalized recover error " N2 "F (Zhang et al., 2016) is recorded for each iteration t of
DSPGD, where Kt = Ms=ι (ʌi,t + (1 - ηt)λ)ui,tU[t is the estimation of K at iteration t. In the
left subfigure of Figure 1(a) and that of Figure 1(b), the convergence curves of DSPGD are lower
than the curve of 0.4/t and the curve of 0.03/t, respectively, which verifies DSPGD converges at an
O(1/t) rate. In Figure 1(a) and 1(b), the curves of two versions of DSPGD nearly overlap, indicating
that CEM has little impact on the convergence of DSPGD.
The average communication cost per iteration of DSPGD with CEM and that of DSPGD without
CEM are compared in Figure 2 to evaluate the effectiveness of CEM. The log-scale is used for the
y-axis of each subfigure, and the unit of the y-axis is the number of the floating-point numbers. As
shown in the four subfigures, CEM can reduce communication cost of DSPGD by more than 98%,
which indicates that CEM is important for FK k-means to maintain communication efficiency.
In order to evaluate the clustering quality and the communication cost of FK k-means, curves of
average normalized mutual information (NMI) (Strehl & Ghosh, 2002) versus average communi-
8
Under review as a conference paper at ICLR 2021
⅛0u co-⅛yc3EEOQ ⅛CTSΦ><
(a) Mushrooms
⅛0u CO-⅛V-C3EEOU ωCT≡ω><
(b) MNIST
ttou CO-⅛V-C3EEOU φcteq>
IO7
IO6
IO5
IO4
w/o CEM w/ CEM
O oo4
111
⅛0uCOSaU-CSEEOU ΦCTSΨ><
(C) CovtyPe	(d) SmartPhone
Communication Cost*IO5
(a) Mushrooms
Figure 2: The average communication cost per iteration of the two versions of DSPGD.
(b) MNIST	(c) Covtype	(d) Smartphone
Figure 3: The NMI score versus the average communication cost of FK k-means and the existing
methods on the four datasets.
cation cost are plotted in Figure 3 for FK k-means and the existing schemes. The error bars in
Figure 3 are the 95% confidence interval of the average NMI scores. For the three public datasets,
FK k-means can achieve comparable average NMI scores to that of the cloud-based algorithms (CK
k-means for Mushroom dataset and SK k-means for MNIST dataset and covtype dataset). For a
cloud-based algorithm, its communication cost equals to the volume of a dataset, which is too large
to be shown. Thus, in Figure 3(a), 3(b), and 3(c) the dash line only represents the average NMI
scores rather than the relationship of average NMI scores versus the corresponding communication
cost. InFigure 3(a) and 3(b), given a fixed communication cost, FK k-means can achieve the highest
average NMI scores among the three client-based algorithms. Besides, in 3(b), FK k-means nearly
achieves the upper bound of the average NMI score with a low communication cost. To reach such
a NMI score, FK k-means reduces the communication cost by more than 60% compared with RFK
k-means. In Figure 3(c), FK k-means has a similar performance to that ofRFK k-means. Compared
with CE PCA, FK k-means reduces the communication cost by more than 60% when the highest
average NMI score is considered. Forthe Smartphone dataset, since it has no labels, the cluster qual-
ity of a given clustering algorithm is evaluated by measuring the similarity between the clustering
results of the algorithm and that of SK k-means. To this end, the clustering results of SK k-means
are used as the labels to compute NMI scores. It is shown in Figure 3(d) that FK k-means has a
much higher upper bound for the average NMI score (close to 0.9) than that of RFK k-means and
CEPCA.
7 Conclusion
In this paper, FK k-means was developed. In the algorithm, a distributed stochastic proximal gra-
dient descent approach was first designed to determine the eigenpairs of the kernel matrix in a
distributed manner. A communication efficient mechanism was then designed to reduce the com-
munication cost. In theoretical analysis, DSPGD with CEM was proved to converge at an O(1∕T)
rate. The communication cost of DSPGD with CEM is unrelated to the number of data samples.
The clustering loss of FK k-means can approach that of the centralized kernel k-means. It was
also analyzed that FK k-means provided two levels of privacy preservation. The effectiveness of
the FK k-means was validated by experiments on several real-world datasets. FK k-means can still
be improved in terms of the asynchronous execution, the robustness to dropout users, and stronger
privacy, which can be interesting topics in our future work.
9
Under review as a conference paper at ICLR 2021
References
Maria Florina Balcan, Yingyu Liang, Le Song, David Woodruff, and Bo Xie. Communication effi-
cient distributed kernel principal component analysis. In Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 725-734, 2016.
Maria-Florina F Balcan, Steven Ehrlich, and Yingyu Liang. Distributed k-means and k-median
clustering on general topologies. In Advances in Neural Information Processing Systems 26, pp.
1995-2003, 2013.
Sebastian Caldas, Virginia Smith, and Ameet Talwalkar. Federated kernelized multi-task learning.
In SysML Conference 2018, 2018.
Mung Chiang and Tao Zhang. Fog and iot: An overview of research opportunities. IEEE Internet of
Things Journal, 3(6):854-864, 2016.
Radha Chitta, Rong Jin, Timothy C Havens, and Anil K Jain. Approximate kernel k-means: So-
lution to large scale kernel clustering. In Proceedings of the 17th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 895-903, 2011.
Radha Chitta, Rong Jin, and Anil K Jain. Efficient kernel clustering using random fourier features.
In 2012 IEEE 12th International Conference on Data Mining, pp. 161-170, 2012.
Radha Chitta, Rong Jin, Timothy C Havens, and Anil K Jain. Scalable kernel clustering: Approxi-
mate kernel k-means. arXiv preprint arXiv:1402.3849, 2014.
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimen-
sionality reduction for k-means clustering and low rank approximation. In Proceedings of the
47th Annual ACM Symposium on Theory of Computing, pp. 163-172, 2015.
James W Demmel. Applied numerical linear algebra. SIAM, 1997.
Inderjit S Dhillon, Yuqiang Guan, and Brian Kulis. Kernel k-means: spectral clustering and nor-
malized cuts. In Proceedings ofthe 10th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 551-556, 2004.
Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative matrix factoriza-
tion and spectral clustering. In Proceedings of the 2005 SIAM International Conference on Data
Mining, pp. 606-610, 2005.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity
in private data analysis. In Theory of cryptography conference, pp. 265-284, 2006.
Ahmed Elgohary, Ahmed K Farahat, Mohamed S Kamel, and Fakhri Karray. Embed and conquer:
Scalable embeddings for kernel k-means on mapreduce. In Proceedings of the 2014 SIAM Inter-
national Conference on Data Mining, pp. 425-433, 2014.
MR Garey, David Johnson, and Hans Witsenhausen. The complexity of the generalized lloyd-max
problem (corresp.). IEEE Transactions on Information Theory, 28(2):255-256, 1982.
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for
clustered federated learning. arXiv preprint arXiv:2006.04088, 2020.
Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In Proceedings
of the 15th International Conference on Artificial Intelligence and Statistics, pp. 583-591, 2012.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtdrik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear differ-
ential and integral operators. United States Governm. Press Office Los Angeles, CA, 1950.
Keng-Pei Lin. Privacy-preserving kernel k-means clustering outsourcing with random transforma-
tion. Knowledge and Information Systems, 49(3):885-908, 2016.
10
Under review as a conference paper at ICLR 2021
Sulin Liu, Sinno Jialin Pan, and Qirong Ho. Distributed multi-task relationship learning. In Pro-
ceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining,pp. 937-946,2017.
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of
the 20th International Conference on Artificial Intelligence and Statistics, pp. 1273-1282, 2017.
Federico Penna and Slawomir Staiiczak. Decentralized eigenvalue algorithms for distributed signal
detection in wireless networks. IEEE Transactions on Signal Processing, 63(2):427-440, 2014.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
Neural Information Processing Systems 20, pp. 1177-1184, 2008.
Felix Sattler, Klaus-Robert Miiller, and Wojciech Samek. Clustered federated learning: Model-
agnostic distributed multitask optimization under privacy constraints. IEEE Transactions on Neu-
ral Networks and Learning Systems, 2020.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task
learning. In Advances in Neural Information Processing Systems 30, pp. 4424-4434, 2017.
Alexander Strehl and Joydeep Ghosh. Cluster ensembles—a knowledge reuse framework for com-
bining multiple partitions. Journal of machine learning research, 3(Dec):583-617, 2002.
Dong Su, Jianneng Cao, Ninghui Li, Elisa Bertino, and Hongxia Jin. Differentially private k-means
clustering. In Proceedings of the 6th ACM Conference on Data and Application Security and
Privacy, pp. 26-37, 2016.
Nikolaos Tsapanos, Anastasios Tefas, Nikolaos Nikolaidis, and Ioannis Pitas. A distributed frame-
work for trimmed kernel k-means clustering. Pattern recognition, 48(8):2685-2698, 2015.
Shusen Wang, Alex Gittens, and Michael W Mahoney. Scalable kernel k-means clustering with
Nystrom approximation: relative-error bounds. The Journal of Machine Learning Research, 20
(1):431-479, 2019.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology, 10(2):1-19, 2019.
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ra-
mage, and FranCOiSe Beaufays. Applied federated learning: Improving google keyboard query
suggestions. arXiv preprint arXiv:1812.02903, 2018.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Proceed-
ings ofthe 36th International Conference on Machine Learning, pp. 7252-7261, 2019.
Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, and Horst D Simon. Spectral relaxation for
k-means clustering. In Advances in Neural Information Processing Systems 14, pp. 1057-1064,
2001.
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Stochastic optimization for
kernel PCA. In Proceedings of the 30th AAAI Conference on Artificial Intelligence, pp. 2316-
2322, 2016.
11
Under review as a conference paper at ICLR 2021
Algorithm 2 Lanczos Algorithm
1:	Input: An symmetric matrix R, an initial vector ci
2:	Output: An approximation PQ to the eigenvectors of R, and an approximation σ
[σ1, ..., σQ] to the eigenvalues of R
3:	Initialize β0 = 0 and c0 = 0
4:	for q = 1, 2, . . . , Q do
5:	g = Rcq
6：	αq = c；g
7:	g = g - αqcq - βq-1cq-1
8:	βq = ||g||2
9:	if βq = 0 then
10:	break
11:	end if
12：	cq+1 = g/Bq
13:	Construct a symmetric tridiagonal matrix TQ
14： Perform EVD on TQ to obtain its eigenvectors PQ and its eigenvalues σ = [σ1, ..., σQ]
15：	Compute CQPQ
16： end for
A Details of Random Feature Method
Fora kernel matrix K, a random feature method (Rahimi & Recht, 2008; Kar & Karnick, 2012) can
generate an unbiased estimate of K, denoted as ξ, with the following expression：
ξ=D aat,
where the i-th row of A is the random feature vectors a(xi) for the data sample xi. The matrix ξ
satisfies E[ξ] = K.
We then use the example of shift-invariant kernels to show how a random feature vector is con-
structed. For popular shift-invariant kernels κ(xi, xj) with Fourier representation
κ(xi, xj) =	p(w)exp(jwt (xi - xj))dw
where p(w) is a probability density function, they can be estimated by the random Fourier features
(Rahimi & Recht, 2008) as follows. By randomly drawing D independent samples {w1, . . . , wD}
from p(w), a random feature vector a(xi) for a data sample xi can be written as a(xi) =
[√2cos(WTXi + bi),..., √2cos(wDXi + bp)]t where {bi, ...,bp} are independent random vari-
ables drawn from [0, 2π) uniformly. As a result, an unbiased estimation of K can be written as
ξ = DDAAT where A = [a(xι),…，a(xn)]τ.
B	Details ab out distributed Lanczos algorithm
To find the eigenpairs of a symmetric matrix R, the Lanczos algorithm (LA) (Lanczos, 1950) first
build a Krylov subspace Kq(R, ci) = span[ci, Rci, ..., Rq-ici] where ci is an initial vector, and
then it employs the Rayliegh-Ritz procedure to construct the best approximate eigenpairs for R in
the Krylov subspace. In the first step, LA constructs an orthogonal basis of the Krylov subspace
following the procedure of line 5 to line 12 in Algorithm 2. Meanwhile, a symmetric tridiagonal
matrix TQ = CQTRCQ can be explicitly constructed with αQ and βQ via
5 Bi	-
Tq = βi ...	...	.
..	..βQ-i
BQ-i	αQ
Based on TQ, the Rayliegh-Ritz procedure can be utilized to approximate the eigenpairs of R. Let
TQ = PQΣQPQT be the eigendecomposition of TQ. It has been proved that the columns of CQPQ
12
Under review as a conference paper at ICLR 2021
and the diagonal entries of ΣQ are the optimal approximation to the eigenvectors and eigenvalues of
R, respectively (Demmel, 1997). Thus, in the Rayliegh-Ritz procedure, PQ and ΣQ are determined
by performing EVD on TQ, and then CQPQ are computed as the approximation to the eigenvectors
of R. As the number of iteration Q increases, the columns of CQPQ and the diagonal entries of
ΣQ can converge to the eigenvectors and eigenvalues of R, respectively (Demmel, 1997).
As for the distributed Lanczos algorithm (DLA), only the step of line 5 in Algorithm 2 is conducted
in a distributed manner, and other steps are conducted at the cloud server. In our problem, if
R = (I- Irnt)Zt + ηtξt = (I- ηt)Zt + ηt AtAj,
where Zt and Cq are known at the cloud server, and At = [At[1]τ,..., A/M]τ]τ are distributed
over M users' devices, then (1 - ηt)ZtCq is computed at the cloud server and 々AtA]Cq/D is
computed in a distributed manner as follows. The vector cq = [cq[1]τ, . . . , cq[M]τ]τ is first
partitioned into M parts at the cloud server, and the m-th part Cq[m] is sent to the m-th user’s device.
A local vector At[m]τCq[m] is then computed at the m-th user’s device. These local vectors from
M users’ devices are summed up at the cloud to obtain a vector AtτCq. AtτCq is then broadcast
to M users’ devices, and a vector At [m]AtτCq is computed at the m-th user’s device. These M
vectors are sent back to the cloud server where they are concatenated to form AtAtτCq. If
M
R = WTWt = E Wt[m]τWt[m],
m=1
where Wt [m]τWt [m] can be computed at the m-th user’s device, then each user’ devices first
determines Wt [m]τWt [m]Cq locally, and these M vectors are then uploaded to the cloud server
where they are summed up to form WtτWtCq.
In the t-th iteration of DSPGD, DLA is used to compute the eigenvalues larger than ηtλ of the Rt .
Thus, in practice, the convergence criterion of DLA is that all the approximated eigenvalues larger
than ηtλ converge, rather than that the number of iteration reaches its maximal value Q.
One issue ofLA and DLA in practice is that they can only be conducted in floating point arithmetic,
which can destroy the orthogonality of the columns in Cq, and further affect the convergence of
DSPGD. To this end, a full reorthogonalization method (Demmel, 1997) is utilized to guarantee that
Cq is an orthogonal matrix with a high probability. The key idea of this method to generate a new
vector Cq from a subspace that is orthogonal to all the previous vectors {C1, ..., Cq-1}, which can be
accomplished by replacing line 7 in Algorithm 2 with
q-1
g = g -	gτCiCi.	(3)
i=1
The operation in (3) can be called multiple times in one iteration of LA to increase the probability
that Cq is an orthogonal matrix. In the implementation of federated kernel k-means, such operation
is called twice in each iteration of DLA. Note that the full reorthogonalization only requires more
flops at the cloud server, which does not affect the algorithm complexity at users’ devices.
C Proof of Theorem 1
This proof partially follows the proof of Theorem 1 in Zhang et al. (2016). The difference is that
the t-th iteration of Z* obtained by DSPGD, i.e., Zt, may not equal Z；=。边工[(1 - ηt)Zt +
ηtξt]. The gap between Zt and Z； is caused by that the distributed Lanczos algorithm (DLA) only
approximates the eigenpairs ofa target matrix. Thus, in this proof, it is assumed that
||Zt - Z；||F ≤ e, ∀t,	(4)
when DLA reaches its convergence criterion, where E ≪ 1.
Before the proof, we first define
F (Z) =2 E[||Z -ξ∣lF ],
ft(Z)=1 ||Z-ξt∣∣F.
13
Under review as a conference paper at ICLR 2021
Fora μ-strongly convex function l(Z), if /(Z1) ≥ l(Z2),then
1(Z1) -/(Z2) ≥ 2||Z1 - Z2∣∣F.	(5)
Inthe t+ 1-th iteration OfDSPGD, the goal is to determine the optimal solution Zj+ι to the following
optimization problem
min 1 ∣∣Z - Zt∣∣F + 加Z - Zt, Vft(Z》+ ηλ∣∣Z∣∣*.	(6)
Z∈Rn×n 2
By DLA, an approximate solution Zt+ι that satisfies (4) can be obtained. The following lemma is a
key step in this proof.
Lemma 1. Before the convergence of DSPGD, the following inequality holds, i.e.,
5∣∣Zt+ι - Zt∣∣F + 2〈Zt+i - Zt, Vft(Zt' + ηλ∣Z+ι∣∣*
1	⑺
≤2∣∣Z*- Zt∣∣F + 加Z*- Zt, Nft(Zt)) + ηλ∣∣Z*∣∣*.
Proof. The objective function in (6) can be rewritten as
2∣∣z - Zt∣∣F + 2〈Z - Zt, Vft(Zt)) + ηλ∣∣z∣∣*
=1 ∣∣Z - Zt∣∣F + 加 Z - Zt, Vft (Zt)) + η2 ∣∣vft(Zt)∣∣F - η2 ∣∣vft %)∣∣F + ηλ∣∣z∣∣*
1	Cn2	c
=2∣∣z - [(1 -ηt)Zt + ntξ]∣∣F + nλ∣∣z∣∣* - g∣∣Vft(Zt)∣∣F.
2
Since ηt ∣∣ Vft(Zt) ∣∣F is a constant, we can only consider
I(Z) = 2∣∣z - [(I- ηt)zt+ ηtξ]∣∣F + ηλ∣∣z∣∣*
in the following part of the proof. Now we first assume that /(Z*) ≤ /(Zt+ι), then we have
/(Zt+1) - /(Zt+1) ≥ /(Z*) - /(Zt+1) ≥ 2∣∣Zt+ι - Z*∣∣F.	(8)
Moreover, /(Zt+1) - /(Zt+1) canbe expanded as
/(Zt+1) - /(Zt+1)
=2∣∣Zt+1 - Rt∣∣F + ηλ∣∣Zt+1∣∣*- 1 ∣∣Zt+1- Rt∣∣F + ηλ∣∣Zt+1∣∣*
=1(∣∣Zt+1 - Rt∣∣F -∣∣Zt+1 - Rt∣∣F)(∣∣Zt+1 - Rt∣∣F + ∣∣Zt+1 - R∣∣f)	(9)
+ ηtλ(∣∣Zt+1∣∣*-∣∣Zt+1∣∣*)
≤ 2 ∣∣zt+1 - zt+1∣∣F (∣∣zt+1 - zt+1∣∣F + 2∣∣zt+1 - r∣∣f )+ ηtλ∣∣zt+1 - zt+1∣∣*
It is well known that given a matrix M the following inequality holds for its nuclear norm and its
Frobenius norm, i.e., ∣∣M∣∣2 ≤ rank(M)∣∣M∣∣F. By this inequality, we have
∣∣zt+1 - zt+1∣∣* ≤ √r∣∣zt+1 - zt+1∣∣F ≤ √r^	(10)
where r is the rank of (Zt+1 - Zt+1). Substitute (4) and (10) into (9), we have
l(zt+1) - l(zt+1) ≤ 2E2 + e∣∣zt+1 - RtIIF + ηtλ√re∙
Since ∣∣Zt+1 - Rt∣∣F is a constant, this upper bound of /(Zt+1) - /(Zt+1) can become arbitrarily
small if E is arbitrarily small. Hence, according to (8), ∣∣Zt+1 - Z* ∣∣F can also be arbitrarily small.
However, this contradicts that ∣∣Zt+1 - Z* ∣∣F cannot become arbitrarily small before the conver-
gence ofDSPGD. Therefore, before the convergence ofDSPGD, /(Z*) ≥ /(Zt+1) is satisfied. □
14
Under review as a conference paper at ICLR 2021
The rest part then follows the proof of Theorem 1 in Zhang et al. (2016). Based on Lemma 1 and
the property of strongly convex function in (5), the update rule of SPDG implies
5∣∣Zt+1 - Zt IIF + η⅛(Zt+ι — Zt, V/t (Zt ))+ ηλ∣∣zt+ι∣∣*
1	1	(11)
≤ 2 ∣∣z* — zt∣∣F + ηt(z* — Zt, Vft(Zt))+ ηtλ∣∣z*∣∣* — ^ ∣∣z* — zt+ι∣∣F ∙
Since F(Z) is 1-strongly convex, it canbe shown that
1 ∣∣Zt - z*∣∣F
≤F (Zt) + λ∣∣Zt∣∣* — F (Z*) — λ∣∣Z*∣∣*
≤(Zt — z*, VF(Zt))- 2∣∣Zt — Z*∣∣F + λ∣∣Zt∣∣L λ∣∣Z*∣∣*
=(Zt- Z*, Vft(Zt)) — λ∣∣z*∣∣* — ɪ∣∣Zt — z*∣∣F
2ηt
+ λ∣∣Zt∣∣* — 1 ∣∣Zt — z*∣除 + ɪ ∣∣Zt — z*∣除 + (VF (Zt) — Vft(Zt), Zt — Z*)
2	2ηt
(11)	ι	ι
≤ (Zt-zt+ι,Vft(Zt))- λ∣∣zt+ι∣∣* — ʒ-∣∣zt+ι — zt∣∣F — ʒ-∣∣z* — zt+ι∣∣F
2ηt	2ηt
+λ∣∣zt∣∣*+ 2 (η—ι) ∣∣Zt- z*∣∣F+ (VF(Zt)-Vft(Zt), zt- z")
≤max ((W) Vft(Zt))一厂∣∣w∣∣f) -	∣∣zt+ι- z*∣∣F
W	2ηt	2	2ηt
+ λ∣∣zt∣∣* - λ∣∣zt+ι∣∣* + - (---1) ∣∣zt - z*∣∣⅛ + (VF(Zt)- Vft(Zt), zt - z")
2	ηt	F
=η ∣∣Vft(Zt)K — ɪ ∣∣Zt+1 — z*K
2	2ηt
+ λ∣∣zt∣∣* - λ∣∣zt+ι∣∣* + 5 (---1) ∣∣zt - z*∣∣⅛ + (VF(Zt)- Vft(Zt), zt - z*),
2	ηt	F
(12)
where the third inquality holds based on the inequality in (11).
By substituting δt = & - K, Zt — Z*) and C2 = maxt∈[τ] ∣∣Zt — ξ∕∣* into (12),
∣∣Zt+ι- Z*∣怡 ≤ η2C2 + 2ηtδt + 2ληt (∣∣Zt∣∣* — ∣∣Zt+ι∣∣*) + (1 — 2ηt) ∣∣Zt — Z*∣怡.(13)
The inequality in (13) is the same as the result of Lemma 1 in Zhang et al. (2016). Thus, the fol-
lowing lemmas 2 from Zhang et al. (2016) can be directly utilized to derive a probability bound for
∣∣Zt+ι - z*K.
Lemma 2 (Lemma 2 in Zhang et al. (2016)). Define Y = maxt∈[τ] ∣∣Zt∣∣*. By setting ηt = ∣, an
upper bound of ∣∣Zt+ι — Z*∣∣2^ can be written as
∣∣zt +ι - z*∣∣f ≤
4(C2 + λγ) ι 2
T 十 T(T — 1)
■ T	T	-
2 E (t -	- E (t — 1)∣∣zt — z*∣∣⅛
The upper bound of EL2 (t - 1)δt in Lemma 2 is then provided in Lemma 3.
Lemma 3 (Lemma 3 in Zhanget al. (2016)). Assume ∣∣ξt — K∣∣f ≤ G and ∣∣Zt — Z*∣∣f ≤ H,
Vt > 2. With a probability at least 1 — δ, ET2 (t — 1)δt is upper bounded by
T	1 T	2
E(t - 1)δt ≤ -]T(t — 1)∣∣Zt — Z*∣∣F + 2G2τ(T — 1) + -GH(T — 1)τ + GH(T — 1),
t=2	t=2
2These lemmas can be found in the supplementary material of Zhang et al. (2016) that can be downloaded
from https：//cs.nju.edu.cn/zlj/pdf/AAAI-2016-Zhang- S .Pdf
15
Under review as a conference paper at ICLR 2021
Iteration
(a) Mushrooms
JO .JBqEnU Φσ2φ><
Iteration
(b) MNIST
Iteration
(C) CovtyPe
Iteration
(d) SmartPhone
Figure 4: The values of Q0 and Q1 versus the number of iteration t for the four real-world datasets
where T = log「2 log2 T〕.
Based on Lemma 2 and Lemma 3, the following upper bound of ||Zt +i - Z*∣∣F holds With a
Probability at least 1 - δ
||Zt +1 - Z*∣∣F ≤ T (C2 + λγ + 2G2T +2 GHT + GH) = O(1∕T).
(14)
D Proof of Theorem 2 and Empirical Results
For DSPGD with CEM, in the t-the iteration, the m-th user's device only needs to upload one vector
in each iteration of DLA, i.e., Wt[m]τWt[m]cq. Since Wt[m] = [ʌ/ɪAt[m], √1 - ηtBt[m]],
the dimension of W∕m]τW∕m]cq equals D + rt, where r is the rank of Zt and D is the number
of random feature. Moreover, DLA requires several Lanczos iterations to approach the eigenpairs
of WtτWt. Thus, the communication cost of DSPGD with CEM is linear to D + rt.
To compute the ratio, we first derive the communication cost for DSPGD without CEM. in the t-the
iteration, the m-th user’s device needs to upload two vectors in each iteration of DLA: At[m]τcq[m]
and At[m]Atτcq, where the dimension of Atτcq equals D. For the concatenation of all M vectors
{At[m]Atτcq, m = 1, ..., M}, its dimension equals the number of data samples N. Thus, the
communication cost of DSPGD without CEM is linear to N + MD.
Given the number of Lanczos iterations Q0 for DSPGD without CEM and the number of Lanczos
iterations Qi forDSPGD with CEM, the ratio canbe determined by M：MD)Q；.
According to Figure 4, it can be seen that the average value of Q0 is close to that of Q1 in each
iteration t, which indicates 窑 ≈ 1. As a result, the dominant factor of the ratio is still M+MD)
Empirically, the figures of the average value of rt versus the number of iterations t for the four
real-world datasets are shown in Figure 5. The results show that the rank rt tends to converge as
16
Under review as a conference paper at ICLR 2021
108 6 4
'UeH ΦCT2Φ><
0	20	40
Iteration
(a) Mushrooms
0 8 6 4 2
2 1111
>lu(Dtt:φcteφ><
0	20	40
Iteration
(b) MNIST
5 2 0 8 5 2 0
2 2 2 1 1 1 1
'UeH φ^2φ><
20	40
Iteration
(C) CovtyPe
8O
109 8 7 6
>luea ΦCT2Φ><
0	20	40
Iteration
(d) SmartPhone
Figure 5: The rank of Zt versus the number of iteration t for the four real-world datasets
the value of t inCreases. Besides, the uPPer bound of rt is a Constant faCtor larger than the number
of eigenveCtors s in Table 1, and suCh uPPer bound is muCh smaller than the number of users’ loCal
data samPles, whiCh Can exPlain the dramatiCal reduCtion on the CommuniCation Cost in Figure 2.
E Proof of Theorem 3
Define K = UΛUτ, and P = UΛ2. The low-rank approximation of K with rank S is denoted as
T	1
Ks = UΛsUτ, and Ps = UΛs2 where the diagonal of Λs Contains the s largest eigenvalues of K
while its rest diagonal entries are all zero. The output of DSPGD at iteration t is an estimation of
K, denoted as Kt, and Kt = Ptlɔt .
The following two lemmas will be used in the proof of Theorem 3.
Lemma 4. Given K t, the following inequality holds with a probability at least 1 一 δ for any rank k
projection matrix Π ∈ Rn×n,
Tr(∏(Ks - Kt)) ≤ O(
Proof. SinCe Π is a rank-k projeCtion matrix, it is obvious that
Tr(∏(Ks- Kt)) ≤ ||Ks - Kt||*.
For a rank-s matrix A, the following inequality holds for its NuClear norm and its Frobenius norm
IlAM ≤ s∣∣A∣∣F.
HenCe,


IIKs - Ktll* ≤ √S∣∣K3 - KtIlF.
17
Under review as a conference paper at ICLR 2021
By Theorem 1, Zt converges to Z* at an O(1∕t) rate. Note Z* has the same eigenvectors as Ks.
Thus, Kt constructed based on Zt also converges to KS atan O(1∕t) rate with a probability at least
1 — δ, i.e., ||Ks - Kt∣∣F has an upper bound as
∣∣Ks - Kt∣∣F ≤ O(i∕t).
Hence, the following inequality holds with a probability at least 1 - δ
Tr(Π(Ks - Kt)) ≤ √s∣∣K3 - Kt∣∣F
≤ °(√ t).
□
Lemma 5. Fix an error parameter ε ∈ (0,1). For any rank k projection matrix Π ∈ Rn×n,
Tr (∏(K - Kt)∏) ≤ (ε + :)∣∣P - ΠP∣∣F.
Proof. It holds that
Tr ((In - Π)(K - Kt)) = Tr(K - Kt) - Tr (∏(K - Kt)∏)
=∣∣K - Ks∣∣* + Tr(KS- Kt) - Tr (π(K - K∕π).
Thus, Tr (∏(K - Kt)∏) can be rewritten as
Tr (∏(K - Kt)∏) = ∣∣K - Ks∣∣* + Tr(K3 - Kt) - Tr ((In - Π)(K - Kt)(I„ - ∏)).
It follows that
Tr ((In - ∏)(K - Kt)(In - ∏))
=Tr ((In - ∏)(K - K3)(In - ∏)) + Tr ((In - ∏)(K3 - Kt)(In - ∏))
=Tr ((In - ∏)(K - Ks)(In - ∏)) + Tr(Ks - Kt)- Tr(Π(K3 - Kt))
≥∣∣p - ps+k ∣∣F + Tr(KS- Kt)-。(	),
where the last inequality comes from Lemma 4. Thus,
Tr (∏(K - Kt)∏) ≤ ∣∣K - Ks∣∣* -∣∣p - Ps+k∣∣F +。(，∣)
=∣∣P - Ps∣∣F -∣∣P - Ps+k∣∣F + o( ʌ/f)
n	n	I-
=E σ2(P)- E σ2(P)+ O(Jt)
i=s+1	i=s+k+1
s + k	i—
=∑σf(P) + O(J t)
i=s + 1	Y
7 s+k	i-
≤ S ∑ 蟾(P)+O().
i=k+1	Y
Since O(^j) can be arbitrarily small, it can be rewritten as O(^j) = ε∣∣P - Pk∣∣F. Besides,
ESik+1 σ2(P) ≤ ENk+1 σ2(P) = ∣∣p - pk∣∣F. Hence, it c∏nbe obtainedthat
Tr (∏(K - Kt)∏) ≤ (ε + S)∣∣P -Pk∣∣F.
□
18
Under review as a conference paper at ICLR 2021
It can be obtained that
||(In - ∏)P∣∣F - ||(In - ∏)Pt||F = Tr((In - ∏)PPτ) - Tr((In- ∏)PtP；)
=Tr(ppτ - PtPT) - Tr(∏(ppτ - PtPT)n).
Let α = Tr(PPτ — PtPt ), and then the above equation can be rewritten as
||(In - ∏)PHF + t⅛(∏(ppt - PtPτ)∏) = α +1|(In - ∏)Pt||F.
After sufficient iterations, both α and Tr(∏(PPτ - PtPT)∏) are non-negative with a high prob-
ability. Thus, by Lemma 5 it holds that
||(In - ∏)P"F ≤ α + ||(In - ∏)Pt||F
=||(In - ∏)P"F + Tr(∏(PPτ - PtPτ)∏)	(15)
k	C
≤ (1 + ε + 一)||(In-∏)PHF.
S
τ
Based on (15), Theorem 3 can be proved as follows. Let ∏ = YtLtYt , where Yt is the indicator
matrix obtained by applying a γ-approximate algorithm to Pt, then
ττ
||(In - YtL tY t)p||F ≤ α + ||(In - YtL tY t)Pt||F
* * *T
≤ α + Y||(In- YtLtYt )Pt||F,
where Yt is the optimal indicator matrix for the linear k-means problem on Pt. Since γ > 1, it
follows that
* * *T
α + Y||(In - YtLtYt )Pt||F ≤ α + Y||(In - Y*L*Y*T)Pt||F
k	. ɪ
≤ γ(1+ ε + 一)||(In - Y*L*Y*T)P||F.
S
Thus,
τ
||(In - YtLtYt )P||F ≤ Y(1 + ε + 一)||(In- Y*L*Y*T)P||F,
S
which is equivalent to fκ(Yt) ≤ γ(1 + ε + S) minγ fκ(Y).
F Privacy Preservation Property of Federated Kernel k-Means
F.1 Recover Users, Data from Random Feature Matrices
A random feature for a data sample Xi has the form cos(ωτXi + b) where the ω and b are determined
by the cloud server. Since the value of ωτ Xi+b cannot be arbitrarily large, the number of its possible
values is limited. If enough such random features are collected, the cloud server can determine the
value of ωτXi + b for each random feature, and then recover Xi by solving a system of linear
equations.
F.2 Proof of Theorem 4
We then prove that the cloud server can at most recover the matrices (Wt[m]τWt[m], m =
1,..., M} (only the multiplication Wt[m]τWt[m] not the matrix Wt[m]) from the local com-
putational results (e.g., Wt[m]τWt[m]cq). The eigenpairs Wt[m]τWt[m] are determined via the
distributed Lanczos algorithm. since
[Ts「] J	DAtHTAtH	,ηt⅛η)AtHTBtH
Wt[m] 1 Wt[m] =	>_d-	V D
[,ηt⅛ηt2Bt[m]τAtH	(1 - ηt)Bt[m]TBtH
At [m]τ At [m] can be recovered from Wt[m]τWt[m].
19
Under review as a conference paper at ICLR 2021
For a matrix At [m] ∈ Rnm × D (nm, < D), a matrix A ∈ Rnm × D can be constructed via
A = UoAtm],
where Uo ∈ Rnm×nm is an arbitrary orthogonal matrix with U]Uo = In. By this construction, it
can be derived that
ArTA = At[m]τ U]UoAt[m] = At[m]τInAt[m] = At[m]τAt[m].
Since there exist infinite matrices Uo satisfying UTUo = In, the problem A∕m]τA∕m] = A'τA'
has infinite solutions. Hence, recovering the random feature matrix At [m] from At [m]τAt [m] is
an ill-posed problem with infinite solutions.
Since by employing CEM, the cloud cannot recover the random feature matrices via matrix opera-
tions, according to Section F.1, it is infeasible for the cloud server to recover users’ data by solving
a system of linear equations.
G Additional Experimental Settings
The three public datasets (Mushrooms, MNIST, Covtype)3 are selected from the LIBSVM dataset
repository. The Smartphone dataset is provided by a company. The smartphone dataset contains the
power consumption data of one app on users’ smartphones. Its twelve features represent the power
consumption on twelve hardware components. The clustering task for this dataset is to find the
distinct usage patterns of the app based on the power consumption data. For the concern of privacy,
the Smartphone dataset will not be disclosed.
The description of the four existing methods used in the experiments are listed as follows.
1.	Centralized kernel k-means Zha et al. (2001) (denoted as CK k-means): directly perform
truncated SVD on the kernel 1matrix K = UΛUτ to obtain a matrix that consists of the
first S column vectors of UΛ 2, and then apply linear k-means to this matrix;
2.	Scalable kernel k-means Wang et al. (2019) (denoted as SK k-means): utilize the NyStrOm
method to approximate the kernel matrix K, and conduct kernel k-means over the approx-
imated kernel matrix;
3.	Distributed kernel k-means with random feature (Chitta et al., 2012) (denoted as RFK k-
means): first transform the raw data samples to the corresponding random vector via the
random Fourier feature method (Rahimi & Recht, 2008) and then utilize a distributed linear
k-means to find the clusters in space of these random features;
4.	Communication efficient distributed kernel PCA Balcan et al. (2016) (denoted as CE PCA):
first conduct dimension reduction on the raw data samples through the communication
efficient kernel PCA that integrates subspace embedding and adaptive sampling techniques
to perform approximated kernel PCA in a distributed manner, and then apply a distributed
linear k-means algorithm to the data samples after the dimension reduction.
For three distributed algorithms (FK k-means, RFK k-means, and CE PCA), the distributed linear k-
means algorithm developed in (Balcan et al., 2013) is utilized to obtain the clustering results. Thus,
the number of data samples C in the coreset should be assigned.
For FK k-means, the maximal iteration number T is selected from [10, 20, 30, 40, 50]. In the exper-
iments of the Mushrooms dataset, C is set to 1000. In the experiment of the MNIST dataset, C is
set to 1000. In the experiment of the Covtype dataset, C is set to 4000. In the experiment of the
Smartphone dataset, C is set to 4000.
RFK k-means has two hyperparameters: the kernel parameter γ, and the number of random features
D . The hyperparameter configuration for RFK k-means is set as follows. The value of γ is the
same as that of FK k-means. In the experiments of the Mushrooms dataset, D = 200, and C is
selected from [100, 300, 500, 700]. In the experiment of the MNIST dataset, D = 800, and C is
3These datasets can be downloaded from
https ://www.csie.ntu.edu.tw∕~cjlin/libsvmtoolS/datasets/.
20
Under review as a conference paper at ICLR 2021
selected from [100, 300, 500, 700, 900]. In the experiment of the Covtype dataset, D = 100, and
C is selected from [1000, 2000, 3000, 4000, 5000]. In the experiment of the Smartphone dataset,
D = 50, and C is selected from [500, 1000, 2000, 3000].
CE PCA has six hyperparameters: the kernel parameter γ, the number of principle components d
after PCA, the number of random features D, the subspace embedding dimension for the feature
expansion ds , The subspace embedding dimension for the data points dp, and the number of repre-
sentative points p. The hyperparameter configuration for CE PCA is set as follows. The value of γ
is the same as that of FK k-means In the experiments over different datasets, some hyperparameters
of these two methods are not changed. For CE PCA, ds = 50, dp = 250, and p = 500. Besides, the
number of principle components d in CE PCA is the same as the number of eigenvectors s in FK k-
means. To obtain different communication cost and normalized mutual information scores, the value
of D is set to different values for both methods. In the experiment over the Mushrooms dataset, D is
selected from [20, 50, 100, 200], and C is set to 1000. In the experiment over the MNIST dataset, D
is selected from [100, 200, 400, 800], C is set to 1000. In the experiment over the Covtype dataset,
D are selected from [20, 50, 100, 200], and C is set to 4000. In the experiment over the Smartphone
dataset, D is selected from [20, 50, 100], C is set to 4000.
The discussion of the configuration of D is as follows. For RFK k-means and CE PCA, D is usually
set to large values (more than 100). While for federated kernel k-means, D can be set to relatively
small values (less than 50). The reason is as follows. RFK k-means (and CE PCA) only employs
random feature once to estimate the kernel matrix. Thus, it requires a large number of random
features to obtain an estimation of the kernel matrix with low approximation error, and furthermore
a high NMI score. In contrast, federated kernel k-means is an iterative algorithm where random
features are employed in each iteration to reduce the gap between the estimation and the kernel
matrix. Hence, the number of random feature is not necessary to be set to a large value in each
iteration.
The communication cost of the three algorithms are determined as follows. For FK k-means, the
communication cost is the communication cost of DSPGD with CEM plus the that of the distributed
linear k-means. For RFK k-means, its communication cost equals the communication cost of the
distributed linear k-means. For CE PCA, its communication cost is the communication cost of
performing distributed PCA plus the communication cost of the distributed linear k-means. The
communication cost the distributed linear k-means equals the number of data samples in the coreset
times the dimension of a data sample. In both FK k-means and CE PCA, the dimension of a data
sample equals the number of eigenvectors s. In RFK k-means, the dimension of a data sample equals
the number of random features D since the raw data cannot be exposed to the cloud server, and only
the random feature vectors can be uploaded to the cloud server.
21