Under review as a conference paper at ICLR 2021
Probabilistic Mixture-of-Experts for
Efficient Deep Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Deep reinforcement learning (DRL) has successfully solved various problems
recently, typically with a unimodal policy representation. However, grasping the
decomposable and hierarchical structures within a complex task can be essential
for further improving its learning efficiency and performance, which may lead to a
multimodal policy or a mixture-of-experts (MOE). To our best knowledge, present
DRL algorithms for general utility do not deploy MOE methods as policy function
approximators due to the lack of differentiability, or without explicit probabilistic
representation. In this work, we propose a differentiable probabilistic mixture-
of-experts (PMOE) embedded in the end-to-end training scheme for generic off-
policy and on-policy algorithms using stochastic policies, e.g., Soft Actor-Critic
(SAC) and Proximal Policy Optimisation (PPO). Experimental results testify the
advantageous performance of our method over unimodal polices and three different
MOE methods, as well as a method of option frameworks, based on two types of
DRL algorithms. We also demonstrate the distinguishable primitives learned with
PMOE in different environments.
1	Introduction
The mixture-of-experts method (MOE) (Jacobs et al., 1991a) is testified to be capable of improving the
generalisation ability of reinforcement learning (RL) agents (Hausknecht & Stone, 2016a; Peng et al.,
2016; Neumann et al.). Among these methods, the Gaussian Mixture Models (GMM) are promising
to model multimodal policy in RL (Peng et al., 2019; Akrour et al., 2020), in which distinguishable
experts or so-called primitives are learned. The distinguishable experts can propose several solutions
for a task and have a larger range of exploration space, which can potentially lead to better task
performance and sample efficiency compared to its unimodal counterpart (Bishop, 2007). The
multimodal policy can be learned by various methods, such as a two-stage training approach (Peng
et al., 2019), specific clustering method (Akrour et al., 2020), or especially parameterised actions
design (Hausknecht & Stone, 2016b). However, these methods are limited, neither applicable to
complicated scenarios such as high-dimensional continuous control tasks nor the training algorithms
are too complex to deal with general utility. To the best of our knowledge, the present DRL algorithms
for general utility do not deploy MOE to model the multimodal policy mainly due to the lack of
differentiability, or without explicit probabilistic representation. Therefore, in the policy gradient-
based algorithms (Sutton et al., 1999a), the gradient of the performance concerning the policy
parameters is undifferentiated. The undifferentiability problem also remains to learn a deep neural
network policy thus making the combinations of MOE and DRL not trivial.
In this paper, we propose a probabilistic framework to tackle the undifferentiated problem by holding
the mixture distribution assumption. We will still use the GMM to model the multimodal policies.
Once the undifferentiated problem is solved, our training methods can be combined with the policy
gradient algorithms by simply setting the number of experts (mixtures) greater than one. Hereafter,
the contribution can be summarised as follows:
•	We analyse the undifferentiability problem of approximating policy as the GMM in DRL
and its associated drawbacks.
•	We propose an end-to-end training method to obtain the primitives with probability in a
frequentist manner to solve the undifferentiability problem.
1
Under review as a conference paper at ICLR 2021
•	Our experiments show the proposed method can achieve better task performance and sample
efficiency by exploring larger behaviours space, especially in complicated continuous control
tasks, compared with unimodal RL algorithms and three different MOE methods or option
frameworks.
2	Related Work
Hierarchical Policies There are two main related hierarchical policy structures. The feudal schema
(Dayan & Hinton, 1992) has two types of agents: managers and workers. The managers first make
high-level decisions, then the workers make low-level actions according to these high-level decisions.
The options framework (Sutton et al., 1999b) has an upper-level agent (policy-over-options), which
decides whether the lower level agent (sub-policy) should start or terminate. In the early years, it’s
the subject of research to discover temporal abstractions autonomously often in discrete actions and
the state space (McGovern & Barto, 2001; Menache et al., 2002; Simsek & Barto, 2008; Silver &
Ciosek, 2012). Recently, (Mankowitz et al., 2016) proposes a method that assumes the initiation sets
and termination functions have particular structures. (Kulkarni et al., 2016) uses internal and extrinsic
rewards to learn sub-policies and policy-over-options. (Bacon et al., 2017) trains sub-policies and
policy-over-options in end-to-end fusion with a deep termination function. (Vezhnevets et al., 2017)
generalises the feudal schema into continuous action space and uses an embedding operation to
solve the indifferentiable problem. (Peng et al., 2016) introduces a mixture of actor-critic experts
approaches to learn terrain-adaptive dynamic locomotion skills. (Peng et al., 2019) changes the
mixture-of-experts distribution addition expression into the multiplication expression.
Mixture-of-Experts and Ensemble Methods To speed up the learning and improve the generali-
sation ability on different scenarios, Jacobs et al. (1991a) proposed to use several different expert
networks instead of a single one. To partition the data space and assign different kernels for different
spaces, Lima et al. (2007); Yao et al. (2009) combines MOE with SVM. To break the dependency
among training outputs and speed up the convergence, Gaussian process (GP) is generalised similarly
to MOE (Tresp, 2000; Yuan & Neubauer, 2008; Luo & Sun, 2017). MOE can be also combined with
RL (Doya et al., 2002; Neumann et al.; Peng et al., 2016; Hausknecht & Stone, 2016a; Peng et al.,
2019), in which the policies are modelled as probabilistic mixture models and each expert aim to
learn distinguishable policies.
Policy-based RL Policy-based RL aims to find the optimal policy to maximise the expected return
through gradient updates. Among various algorithms, Actor-critic is often employed (Barto et al.,
1983; Sutton & Barto, 1998). Off-policy algorithms (O’Donoghue et al., 2016; Lillicrap et al., 2016;
Gu et al., 2017; Tuomas et al., 2018) are more sample efficient than on-policy ones (Peters & Schaal,
2008; Schulman et al., 2017; Mnih et al., 2016; Gruslys et al., 2017). However, the learned policies
are still unimodal.
3	Method
3.1	Notation
The model-free RL problem can be formulated by Markov Decision Process (MDP), denoted as a
tuple (S, A, P, r), where S and A are continuous state and action space, respectively. The agent
observes state st ∈ S and takes an action at ∈ A at time step t. The environment emits a reward
r : S × A → [rmin, rmax] and transitions to a new state st+1 according to the transition probabilities
P : S × S × A → [0, ∞). In deep reinforcement learning algorithms, we always use the Q-value
function Q(st, at) to describe the expected return after taking an action at in the state st. The Q-value
can be iteratively computed by applying the Bellman backup given by:
Q(st, at) , Est+ι〜P [r(St,a∕ + YEat+ι〜π [Q(St+1, at+1)]] .	(I)
Our goal is to maximise the expected return:
∏θ*(at∣st) = arg max Eat 〜∏θ(at∣st) [Q(st ,at)],	(2)
πΘ (at |st)
2
Under review as a conference paper at ICLR 2021
where Θ denotes the parameters of the policy network π. With Q-value network (critic) Qφ param-
eterised by φ, Stochastic gradient descent (SGD) based approaches are usually used to update the
policy network:
Θ = Θ + VθEa 〜∏θ(at∣st)[Qφ(st,αt)].
(3)
3.2	Probabilistic Mixture-of-Experts (PMOE)
The proposed PMOE method decomposes a stochastic policy π as a mixture of low-level policies
while retaining the probabilistic properties of the stochastic policy as a probability distribution, with
the following formula:
K
K
∏{θ,ψ}(at∣st) = J^wθi (st)∏ψi (at∣st), s±. £w6i = 1, wθi > 0,
(4)
i=1
i=1
where each πψi denotes the action distribution within each low-level policy, i.e. a primitive, and K
denotes the number of primitives. wψi is the weight that specifies the probability of the activating
primitive πψi , which is called the routing function. θi and ψi are parameters of wθi and πψi ,
respectively. After the policy decomposition with PMOE method, we can rewrite the update rule in
Eq. 3 as:
θ = θ + VθEat〜∏{θ,ψ)(at∣St)[Qφ(st, at儿
ψ = ψ + VψEat〜∏{θ,ψ)(at∣St)[Qφ(st, at)].
(5)
In practice, we usually apply a Gaussian distribution for either a unimodal policy or the low-level
policies here in PMOE, making the overall stochastic policy with PMOE to be a GMM. However,
sampling from the distributions of primitives usually embeds a sampling process from a categorical
distribution w, which makes the differential calculation of policy gradients commonly applied in
DRL hard to achieve. We provide a theoretically guaranteed solution for approximating the gradients
in the sampling process of PMOE and successfully optimising the PMOE policy model within DRL,
which will be described in details.
3.3	Learning the Routing
To optimise the routing function w, which involves a sampling process from a categorical distribution,
we propose a frequency loss function to approximate the gradients, which is theoretically proved
to approximate w as the probability of the corresponding primitive being the optimal one w.r.t. the
Q-value function.
Specifically, given a state st, we sample one action ait from each primitive πψi, to get a total of K
actions {ai; i = 1,2,…,K}, and compute K Q-ValUe estimations {Qφ(st, ai); i = 1,2,…,K}
for each of the actions. Then we select an “optimal” primitive index as j = arg maxi Qφ(st, ati).
Then We encode a one-hot code vector V = [v1,v2,…，VK] with:
1, ifj = argmaxQφ(st, ait);
Vj =	i	(6)
0, otherwise.
Here we define a frequency loss function as:
Lfreq = (V - W)(V - W)T ,W = [wθι, Wθ2 ,…，Wθκ ].	(7)
We use the proposed frequency loss Lfreq as a smooth and differentiable function to update the
routing function parameters θ, which is guaranteed to approximate Wθi as the probability of the i-th
primitive being the optimal primitive for current state. Detailed proof is provided in Appendix B.
3.4	Learning the Primitive
To update the ψi within each primitive, we provide two approaches of optimising the primitives:
back-propagation-all and back-propagation-max manners.
For the back-propagation-all approach, we update all the primitive:
K
Lppa = - X Qφ(st,at), at 〜πψi (at|st).	⑻
i
3
Under review as a conference paper at ICLR 2021
For the back-propagation-max approach, we use the highest Q-value estimation as the primitive loss:
Lppm = — max{Qφ(st,ai; i = 1, 2,…，K)}, at 〜∏ψt (at∣st).	(9)
With either approach, we have the stochastic policy gradients as following:
VψiLpri = -VψjE∏ψi [Qφ(st,at)]
=E∏Ψi[-Qφ(st, at)vψj log πψj (at |St)]
(10)
Ideally, both approaches are feasible for learning a PMOE model. However, in practice, we find that
the back-propagation-all approach will tend to learn primitives that are close to each other, while the
back-propagation-max approach is capable of keeping primitives distinguishable. The phenomenon is
demonstrated in our experimental analysis. Therefore, we adopt the back-propagation-max approach
as the default setting of PMOE model without additional clarification.
3.5	Learning the Critic
Similar to standard off-policy RL algorithms, our Q-value network is also trained to minimise the
Bellman residual:
Lcritic = E(st,at)〜D k Qφ (st,at) — 旌 + Y max Qφ(st+1,at+1)]k2,	(11)
at+1
where φ is the parameters of the target network.
The learning component can be easily embedded into the popular actor-critic algorithms, such as soft
actor-critic (SAC) (Tuomas et al., 2018), one of the state-of-the-art off-policy RL algorithms. In SAC,
Qψ(st, atj) is substituted with Qψ(st, atj) + αHj, where α is temperature and Hj — log πψj (at|st)
is the entropy which are the same as in SAC. The algorithm is summarised in Algorithm 1. When
K = 1, our algorithm simply reduced to the standard SAC.
4 Experiments
In our experiments, we will answer the following questions concerning the advantages of PMOE
method and the training settings for achieving good performance:
Q1: Is our proposed method more efficient and stable than other model-free RL algorithms?
Q2: What is the benefit of using GMM-based policy approximation over unimodal Gaussian
policy?
Q3: How does the number of primitives affect the performance?
To answer Q1, we conduct a thorough comparison on a set of challenging continuous control tasks in
OpenAI Gym MuJoCo environments (Brockman et al., 2016) with other baselines, including a MOE
method with gating operation (Jacobs et al., 1991b), Double Actor-Critic (DAC) (Zhang & Whiteson,
2019) option framework, and the Multiplicative Compositional Policies (MCP) (Peng et al., 2019).
Well-known sample efficient algorithms involving Soft Actor-Critic (SAC) (Tuomas et al., 2018) and
Proximal Policy Optimisation (PPO) (Schulman et al., 2017) are adopted as basic DRL algorithms in
our experiments, where different multimodal policy approximation methods are built on top. This
verifies the generality of PMOE for different DRL algorithms. To answer Q2, a deeper investigation
of PMOE is conducted to find out the additional effects caused by deploying mixture models rather
than single model in policy learning. We start with a simple self-defined target-reaching task to show
our method can indeed learn various optimal solutions with distinguishable primitives, which are
further demonstrated on complicated tasks in MuJoCo. Additionally, the exploration behaviors are
also compared between PMOE and other baselines to explain the advantageous learning efficiency
of PMOE. To know how the number of primitives affects the performance (Q3), we test PMOE
with different different values of K on the HumanoidStandup-v2 environment. We provide intuitive
experiences for determining the number of primitives when applying PMOE. We also demonstrate
the advantages of proposed PMOE in its exploration behaviours and the distinctiveness of different
primitives learned with PMOE against the baselines.
4
Under review as a conference paper at ICLR 2021
Algorithm 1: POME Training Algorithm.
Input: Policy network π{θ,ψ} with parameters {θ, ψ}. Critic network Qφ with parameters φ.
Initialise target policy network and critic network parameters: θ J θ,ψ J ψ,φ J φ.
Initialise an empty replay buffer: DJ Φ.
while not converge do
for each environment step do
Sample action from policy: at 〜 ∏{θ,ψ}(at∣st).
Interact with the environment: st+1 = p(st+1|at,st).
_ Store data in replay buffer: D = D ∪ {st,at,st+ι,r(st,at)}.
for each update step do
Sample from replay buffer: {st,at,st+ι,rt}〜 D.
for each policy update step do
Sample K actions from primitives: {a；〜∏ψi (at∣st); i = 1, 2, ∙∙∙ , K}.
Compute K target Q-value estimations Q = {Qφ(st, a；)； i = 1,2,…,K},
compute primitive loss Lpri according to 9.
Compute mixing coefficients w(st) and one-hot code vector v according to Eq. 6,
then compute freq-loss Lfreq according to Eq. 7.
_	Update policy network: θ = θ 一 VθLfreq, ψ = ψ 一 VψLpri.
for each critic update step do
Sample action from mixture policy: a；+i 〜 ∏{θ,ψ}(at+ι∣st+ι).
Compute next time step target Q-value estimation: Qφ(st+1,αt+1).
Compute Q-value estimation Qφ(st, at) and compute critic loss Lcritic according to
Eq. 11.
_	Update critic network: φ = φ 一 VφLcritic.
_	Update target network: θ= τθ + (1 一 τ)θ, ψ = τψ + (1 — τ)ψ, θ = τθ + (1 一 τ)θ.
Output: θ, ψ, φ.
4.1	Comparative Evaluation
The evaluation on the average returns with SAC-based and PPO-based algorithms are shown in Fig. 1
and Fig. 2, respectively. In addition, we compute the AUC (the area under the curves) values for those
comparisons in Appendix G. Specifically, in Fig. 1, SAC-based algorithms with either unimodal policy
or our PMOE for policy approximation are compared against the MCP (Peng et al., 2019) and gating
operation methods (Jacobs et al., 1991b) in terms of the average returns across six typical MuJoCo
tasks. In Fig. 2, a comparison with similar settings while basing on a different DRL algorithm is
conducted to show the generality of PMOE method. Specifically, we compare the proposed PMOE
for policy approximation on PPO with DAC and PPO-based MCP methods. From the comparisons,
we found that in relatively simple environments such as Ant-v2 and Walker2d-v2, large K might
not be necessary. While complicated environments like Humanoid-v2 and HumanoidStandup-v2,
larger K is preferred. This is possible because the optimal solutions might be unique in simple
environments thus making multimodal representation unnecessary. Nevertheless, our method is still
more sample efficient, showing the capability of degradation of models in our method, where the
redundant primitives automatically re-fit onto other necessary primitives and diminish the effective
number of K. PMOE is testified to provide considerable improvement for general DRL algorithms
with stochastic policies on a variety of tasks. Training details are provided in Appendix D.
4.2	Experiment Analysis
We provide in-depth analysis of the proposed PMOE to analysis the differences between PMOE
method and other methods in RL process (Q2), and estimate the effects of the number of primitives
(Q3). We use a self-defined target-reaching environment to analyse our method. In this environment,
the agent starts from a fixed initial position, then acts to reach a random target position within a
certain range and avoids the obstacles on the path. Details about this environment is provided in
Appendix C.
5
Under review as a conference paper at ICLR 2021
u-lmα,°;ωσsω><
(a) Ant-v2
(b) Hopper-v2
u-ln"0=ωσsω><
(c) Walker2d-v2
u-lmα,°=
u-lmα,°=
(e) HumanoidStandup-v2
(f) HalfCheetah-v2
Figure 1:	Training Curves on MuJoCo benChmarks with SAC-based algorithms. We set PMOE with
K = 4 in all the experiments exCept HalfCheetah-v2 with K = 2 and HumanoidStandup-v2 with
K = 10.
4000
3000
2000
1000
En*jωccφσsφ><
Million Steps
En*jωccφσsφ><
8.0 0.2
0.4	0.6	0.8
Million Steps
En*jωccφσsφ><
4000
3000
2000
1000
12	3
Million Steps
(C) Walker2d-v2
1.0
(a) Ant-v2
(d) Humanoid-v2
(b) Hopper-v2
EnsccφσEΦ><
(e) HumanoidStandup-v2
En*jωccφσsφ><
(f) HalfCheetah-v2
Figure 2:	Training Curves on MuJoCo benChmark with PPO-based algorithms. We set a larger
number, K = 8, for the high-dimensional environments Humanoid-v2 and HumanoidStandup-v2,
and K = 4 for other environments . The results show that our PMOE has Comparable performanCe
with the baseline on low-dimension environments, but signifiCantly better than the baseline on
high-dimensional environments.
Distinguishable Primitives. Fig. 3 demonstrates the distinguishable primitives (Peng et al., 2019)
learned with PMOE on the target-reaChing environment, for providing a simple and intuitive under-
standing. After the training stage, we sample 10 trajeCtories for eaCh method and visualise them in
6
Under review as a conference paper at ICLR 2021
(a) Gating operation.
Figure 3: Trajectories of the agents with our method and the baselines in the target-reaching en-
vironment. We fix the reset locations of target, obstacles and agent. (a), (b), (c) and (d) visualize
the 10 trajectories collected with methods involving: original SAC, gating operation with SAC,
back-propagation-all PMOE (discussed in Sec. 3.4) and back-propagation-max PMOE, respectively.
(e) shows the trajectories collected with two individual primitives with our approach.
(b) States clusters.
(c) Our approach.
Figure 4: Visualisation of distinguishable primitives learned with PMOE using t-SNE plot on Hopper-
v2 environment. The states are first clustered as in (b). Then actions within the same state cluster
are plotted with t-SNE as in (a) and (c) for the gating method and our approach, respectively. Our
method clearly demonstrates more distinguishable primitives for the policy.
Fig. 3. As we can see in Fig. 3(e), PMOE trained in back-propagation-max manner generates two
distinguishable trajectories for different primitives.
In Fig. 4, we further demonstrate that PMOE can learn distinguishable primitives on more complex
environments with the t-SNE (van der Maaten & Hinton, 2008) method. We sample 10K states
{st； t = 1, 2,…，10K} from 10 trajectories and use t-SNE to perform dimensionality reduction on
states and visualise the results in Fig. 4(b). We randomly choose one state cluster and sample actions
corresponding to the states in that cluster. Then we use t-SNE to perform dimensionality reduction on
those actions. The reason for taking a state clustering process before the action clustering is to reduce
the variances of data in state-action spaces, so that we can better visualise the action primitives for
a specific cluster of states. The visualisation of action clustering with our approach and the gating
operation are displayed in Fig. 4(a) and Fig. 4(c). More t-SNE visualisations for other MuJoCo
environments can be found in Appendix F. Our proposed PMOE method is testified to have stronger
capability in distinguishing different primitives during policy learning process. The t-SNE parameters
are the same in all the experiments: number of components=2, perplexity=300, learning rate=150,
number of iterations=7000.
Exploration Behaviours. Fig. 5 demonstrates the exploration trajectories in the target-reaching
environment, although all trajectories start from the same initial positions, our methods demonstrate
larger exploration ranges compared against other baseline methods, which also yields a higher visiting
frequency to the target region (in green) and therefore accelerates the learning process. To some
extent, this comparison can explain the improvement of using PMOE as the policy representation
over a general unimodal policy. We find that by leveraging the mixture models, the agents gain
7
Under review as a conference paper at ICLR 2021
effective information more quickly via different exploration behaviours, which cover a larger range of
exploration space at the initial stages of learning and therefore ensure a higher ratio of target reaching.
(a) Ours
(b) Success Rate in Learning
(c) SAC
Figure 5: Visualisation of exploration trajectories in the initial training stage for the target-reaching
environment. The initial 10K steps (the grey region on the learning curves in (b)) of exploration
trajectories are plotted in (a) and (c) for our PMOE method (red) and SAC (blue), respectively. The
green rectangle is the target region.
Number of Primitives. We investigate the
effects caused by different numbers of prim-
itives in GMM, as shown in Fig. 6. This
experiment is conducted on a relatively com-
plex environment — HumanoidStandup-v2
that has observations with dimension of 376
and actions with dimension of 17, therefore
various skills could possibly lead to the goal
of the task. The number of primitives K is se-
lected from {2, 4, 8, 10, 14, 16}. The results
show that K = 10 seems to perform the best,
and K = 2 performs the worst among all
settings, showing that increasing the num-
ber of primitives can improve the learning
efficiency in some situations. Our intuitive
choice is that the number of primitives needs
to be increased when the dimension of action
is large.
5 Conclusion
Million Steps
Figure 6:	Comparison of different numbers
of primitives K in terms of average returns on
HumanoidStandup-v2 environment. For each case,
we conduct 5 runs and take the means. The perfor-
mance increases when K increase from 2 to 10, but
decreases if K keep increasing from 10 to 16.
To cope with the problems of low learning
efficiency and multimodal solutions in high-
dimensional continuous control tasks when
applying DRL, this paper proposes the differentiable PMOE method that enables an end-to-end train-
ing scheme for generic RL algorithms with stochastic policies. Our proposed method is compatible
with policy-gradient-based algorithms, like SAC and PPO. Experiments show performance improve-
ment across various tasks is achieved by applying our PMOE method for policy approximation, as
well as displaying distinguishable primitives for multiple solutions.
References
Riad Akrour, Davide Tateo, and Jan Peters. Reinforcement learning from a mixture of interpretable
experts. CoRR, abs/2006.05911, 2020.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In AAAI, pp.
1726-1734, 2017.
8
Under review as a conference paper at ICLR 2021
Andrew G. Barto, Richard S. Sutton, and Charles W. Anderson. Neuronlike adaptive elements that
can solve difficult learning control problems. IEEE Trans. Syst. Man Cybern., 13(5):834-846,
1983.
Christopher M. Bishop. Pattern recognition and machine learning, 5th Edition. Information science
and statistics. Springer, 2007.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
Peter Dayan and Geoffrey E. Hinton. Feudal reinforcement learning. In NeurIPS, pp. 271-278, 1992.
Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato. Multiple model-based
reinforcement learning. Neural Comput., 14(6):1347-1369, 2002.
Audrunas Gruslys, Mohammad Gheshlaghi Azar, Marc G. Bellemare, and Remi Munos. The reactor:
A sample-efficient actor-critic architecture. CoRR, abs/1704.04651, 2017.
Shixiang Gu, Timothy P. Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine.
Q-prop: Sample-efficient policy gradient with an off-policy critic. In ICLR, 2017.
Matthew J. Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space.
In ICLR, 2016a.
Matthew J. Hausknecht and Peter Stone. Deep reinforcement learning in parameterized action space.
In ICLR, 2016b.
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures
of local experts. Neural Comput., 3(1):79-87, 1991a.
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures
of local experts. Neural Comput., 3(1):79-87, 1991b.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Tejas D. Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep
reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In NeurIPS, pp.
3675-3683, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016.
Clodoaldo Ap. M. Lima, Andre L. V. Coelho, and Fernando J. Von Zuben. Hybridizing mixtures of
experts with support vector machines: Investigation into nonlinear dynamic systems identification.
Inf. Sci., 177(10):2049-2074, 2007.
Chen Luo and Shiliang Sun. Variational mixtures of gaussian processes for classification. In IJCAI,
pp. 4603-4609, 2017.
Daniel J. Mankowitz, Timothy A. Mann, and Shie Mannor. Adaptive skills adaptive partitions
(ASAP). In NeurIPS, pp. 1588-1596, 2016.
Amy McGovern and Andrew G. Barto. Automatic discovery of subgoals in reinforcement learning
using diverse density. In ICML, pp. 361-368, 2001.
Ishai Menache, Shie Mannor, and Nahum Shimkin. Q-cut - dynamic discovery of sub-goals in
reinforcement learning. In ECML, volume 2430, pp. 295-306, 2002.
Volodymyr Mnih, Adria PUigdOmeneCh Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In ICML, volume 48, pp. 1928-1937, 2016.
Gerhard Neumann, Wolfgang Maass, and Jan Peters. Learning complex motions by sequencing
simpler motion templates. In ICML, volume 382, pp. 753-760.
9
Under review as a conference paper at ICLR 2021
Brendan O'Donoghue, Remi Munos, Koray KavUkcUoglu, and Volodymyr Mnih. PGQ: combining
policy gradient and q-learning. CoRR, abs/1611.01626, 2016.
Xue Bin Peng, Glen Berseth, and Michiel van de Panne. Terrain-adaptive locomotion skills using
deep reinforcement learning. ACM Trans. Graph., 35(4):81:1-81:12, 2016.
Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. MCP: learning
composable hierarchical control with multiplicative compositional policies. In NeurIPS, pp.
3681-3692, 2019.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
Networks, 21(4):682-697, 2008.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
David Silver and Kamil Ciosek. Compositional planning using optimal option models. In ICML,
2012.
Ozgur Simsek and Andrew G. Barto. Skill characterization based on betweenness. In NeurIPS, pp.
1497-1504, 2008.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive
computation and machine learning. MIT Press, 1998.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In NeurIPS, pp. 1057-1063,
1999a.
Richard S. Sutton, Doina Precup, and Satinder P. Singh. Between mdps and semi-mdps: A framework
for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181-211, 1999b.
Volker Tresp. Mixtures of gaussian processes. In NeurIPS, pp. 654-660, 2000.
Haarnoja Tuomas, Zhou Aurick, Abbeel Pieter, and Levine Sergey. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. volume 80, pp. 1861-1870.
PMLR, 10-15 Jul 2018.
Laurens van der Maaten and Geoffrey Hinton. Viualizing data using t-sne. JMLR, 9:2579-2605, 11
2008.
Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In ICML,
volume 70, pp. 3540-3549, 2017.
Bangpeng Yao, Dirk B. Walther, Diane M. Beck, and Fei-Fei Li. Hierarchical mixture of classification
experts uncovers interactions between brain regions. In NeurIPS, pp. 2178-2186, 2009.
Chao Yuan and Claus Neubauer. Variational mixture of gaussian process experts. In NeurIPS, pp.
1897-1904, 2008.
Shangtong Zhang and Shimon Whiteson. DAC: the double actor-critic architecture for learning
options. In NeurIPS, pp. 2010-2020, 2019.
10
Under review as a conference paper at ICLR 2021
A Probabilistic formulation of PMOE and Gating Operation
In this section, we show a detailed comparison of probabilistic formulation for GMM (as Eq. (12)
and (13), used in our PMOE method) and the gating operation method (Eq. (14) to (16)), in term
of their PDFs. The gating operation degenerates the multimodal action to a unimodal distribution,
which is different from our PMOE method.
For GMM, suppose a primitive ∏i(s) is a Gaussian distribution N(a∣μ(s), σ2(s)), drawing a sample
from the mixture model can be seen as the following operation:
KK
a 〜∏(a∣s) = Ewi(S)∏i(a∣s) = Ewi(S)N(a∣μi(s),σ2(s)),	(12)
i=1	i=1
where the PDF is:
/、 L Wi(S)	( (a - μi(S))2]
p(a)=工 k / 、exp{——ʒ-ɪʒ一}.	(13)
i=1 √2∏σi(s)	2σ2(S)
For gating operation, the outputs of the weight operation are the weights of each action from
different primitives. With those weights, the gating operation uses the weighted action as the final
output action according to Peng et al. (2019):
K
a =wi(S)ai, S.t.ai 〜 ∏i(a∣S) = N (a∣μi(S),σ2(S)).	(14)
i=1
As a primitive is a Gaussian distribution, Eq. 14 becomes:
KK
a 〜N(N Ewi(S)μi(S), £wi(S)σ2(S)),	(15)
where the PDF is:
/ \	1	r (a - PK=1 Wi(S)μi(S))2 ]
p(a) = / K	exp{--.-K 1 ( 、 2( 、	},	(16)
2π PiK=1wi(S)σi2(S)	2 i=1wi(S)σi2(S)
The above PDF shows that the gating operation could degenerate the Gaussian mixture model into the
univariate Gaussian distribution. Other methods (Jacobs et al., 1991a; Peng et al., 2016; Vezhnevets
et al., 2017) also have the similar formulation.
B Proof of Frequency Loss
In this section, we prove that the proposed frequency loss in our PMOE method produces a consistent
estimation of the gradients for optimizing GMMs. The mixing coefficient, also called the weight w,
stands for the probability of choosing the optimal primitive.
Given the state St, the frequency of arg maxi Qφ(St, ati) = j being sampled is defined as fj. When
we randomly choose N (close to infinity) samples with state St from the replay buffer, the number
of samples satisfying arg maxi Qφ(St, ait) = j should be Nfj, while the number of those with
arg maxi Qφ(St, ait) 6= j is N(1 - fj). Suppose that we have a loss function for wθj (St) as:
Lwθj(st) =Nfj[1-wθj(St)]2+N(1-fj)[0-wθj(St)]2
=N[fj(1-wθj(St))2+(1-fj)(wθ2j(St)]
= N[fj - 2fjwθj(St) + fjwθ2j(St) +wθ2j(St) - fjwθ2j(St)]
= N[wθ2j(St) - 2fjwθj(St) + fj].
Then the gradient for θj is:
NejLwθj (St) = NejN[w2j (St)- 2fjwθj (St) + fj]
=N[Nejw2j (St)- 2fjNejwej (St)]
= 2N (wej (St) - fj)Nejwej (St),
(17)
(18)
Therefore, optimising Eq. 17 is the same as minimising the distance between wθj (St) and fj, with
the optimal situation as wθj (St) = fj when letting the last formula of Eq. 18 be zero.
11
Under review as a conference paper at ICLR 2021
C Details of Target-Reaching Environment
The visualisation of the target-reaching environment is shown in Fig.7, the blue circle is the agent,
the gray circles are obstacles and the circle in red is the target. The agent state is represented by an
action vector a = [ax, ay] and a velocity vector v = [vx, vy]. The playground of the environment is
continuous and limited in [-5, 5] in both x-axis and y-axis. The agent speed is limited into [-2, 2],
the blue coloured agent is placed at position [xag, yag] = [-4.5, -4.5] and the red coloured target is
randomly placed at position [xtg, ytg], where Xtg ,ytg 〜U(0,3) and U denotes uniform distribution.
There are M gray coloured obstacles with each position [xOb§,y^bs∖, where χ0bs,yiobs 〜N(0,32)
and N denotes Gaussian distribution. The observation is composed of [[xtg -xag, ytg -yag], {[xiobs -
Xag, yθbs - yθg]； i = 1, 2,…，M}, a, v]. The input action is the continuous acceleration a which is
in the range of [-2, 2].
Figure 7: Visualisation of the target-reaching environment
The immediate reward function for each time step is defined as:
(100, if the agent reaches the target;
- 10, if the agent collides with edges or obstacles;	(19)
||v||2 , otherwise.
D	Training Details
For PMOE-SAC policy network, we use a two-layer fully-connected (FC) network with 256 hidden
units and ReLU activation in each layer. For primitive network πψ , we use a two single-layer FC
network, which outputs μ and σ for the Gaussian distribution. Both the output layers for μ and σ have
the same number of units, which is K * dim(A), with K as the number of primitives and dim(A) as
the dimension of action space, e.g., 17 for Humanoid-v2. For the routing function network wθ, we
use a single FC layer with K hidden units and the softmax activation function.. In critic network we
use a three-layer FC network with 256, 256 and 1 hidden units in each layer and ReLU activation for
the first two layers. Other hyperparameters for training are showed in Tab. 1(a). For PMOE-PPO, we
use a two-layer FC network to extract the features of state observations. The FC layers have 64 and
64 hidden units with ReLU activation. The policy network has a single layer with the Tanh activation
function. The routing function network has a single FC layer with K units and the softmax activation
function. The critic contains one layer only. Other training hyperparameters are showed in Tab. 1(b).
We use the same hyperparameters in all the experiments without any fine-tuning. For MCP-SAC, we
use the same network structure as MCP-PPO, other training hyperparemeters are the same as shown
in Tab. 1(a). For other baselines, we use original hyperparameters mentioned in their paper. The full
algorithm is summarised in Algorithm 1.
12
Under review as a conference paper at ICLR 2021
(a) Hyperparameters for PMOE-SAC
Parameter	Value
optimiser	Adam (Kingma & Ba, 2015)
learning rate	10-3
discount (γ)	0.99
replay buffer size	106
alpha	0.2
batch size	100
polyak (τ)	0.995
episode length	103
target update interval	1	
(b) Hyperparameters for PMOE-PPO
Parameter	Value
optimiser learning rate discount (γ) alpha batch size Polyak (τ) episode length gradient clip optimisation epochs	Adam (Kingma & Ba, 2015) 3 * 10-4 0.99 0.2 64 0.95 2*103 0.2 20	
Table 1: Hyperparameters
E Probab ility Visualisation
We visualise the probabilities of each primitive over the time steps in the MuJoCo HalfCheetah-v2
environment. As shown in Fig. 8, we found that the probabilities are changed periodically. We also
visualise the actions at the selected 5 time steps in one period. As shown in Fig. 9, the primitives are
distinguishable enough to develop distinct specialisations.
Figure 8: Visualisation of the probabilities of each primitive over the time steps in the MuJoCo
HalfCheetah-v2 environment. The y-axis shows the probabilities of different primitives.
13
Under review as a conference paper at ICLR 2021
(a) time step 1
(b) time step 3
(c) time step 5
(d) time step 6
(e) time step 7
Figure 9: Visualisation of the actions at the selected 5 time steps in one period. The y-axis shows
the probabilities of different primitives. This result shows that the primitives develop distinct
specialisations, with the primitive 0 becomes the most active when the front leg touches the ground,
while the primitive 1 becomes the most active when the leg leaves the ground.
14
Under review as a conference paper at ICLR 2021
F T-SNE Visualisation
N>,puα,-eM
(a) Gating Operation
Figure 10: We plot the t-SNE
(b) State Observations
(c) Ours
visualisation for other 5 MuJoCo environments: Ant-v2,
HumanoidStandup-v2, Humanoid-v2, HalfCheetah-v2 and Walker2d-v2. Parameters and other details
are the same as the setting mentioned in Sec 4.2.
G Comparison of AUC
We compute the AUC (the area under the learning curve) to make the figure more readable.
15
Under review as a conference paper at ICLR 2021
For SAC-based experiments, we assume the AUC of SAC is 1, and AUC values for all methods are
shown in Table 2.
Env	Walker	HalfCheetah	Humanoid	HumanoidStandup	Ant	Hopper
SAC	100%	100%	-100%-	100%	100%	100%
MCP-SAC	104.8%	103.2%	91.2%	98.2%	96.4%	100.5%
Gating	97.6%	95.1%	84.4%	92.3%	89.3%	109.9%
PMOE-SAC (ours)	105.5%	99.6%	94.5%	113.3%	113.4%	115.4%
Table 2: Comperation of the AUC for the SAC-based methods.
For PPO-based experiments, we assume the AUC of DAC is 1, and AUC values for all methods are
shown in Table 3.
Env	Walker	HalfCheetah	Humanoid	HumanoidStandup	Ant	Hopper
DAC	100%	100%	-100%-	100%	100%	100%
MCP-PPO	75.8%	158.6%	159.5%	116.2%	83.8%	93.5%
PMOE-SAC(ours)	101.1%	171.3%	162.2%	154.3%	93.3%	123.0%
Table 3: Comparison of the AUC for the PPO-based methods.
For the number of K experiments, we assume the AUC for PMOE-SAC with K = 2 is 1, so all the
methods AUC values are shown in Table 4.
Number of K	HumanoidStandup
2	100%
4	107.1%
8	106.5%
10	111.7%
14	108.0%
16	107.0%
Table 4: Comparison of the AUC as a function of K for PMOE-SAC algorithm on HumanoidStandup-
v2 environment.
H Comparison with other optimization methods
As shown in Fig. 11, we compare our methods with the Gumbel-softmax and REINFORCE in one low
dimensional MuJoCo task Hopper-v2 and one high dimensional MuJoCo task HumanoidStandup-v2.
For all the methods, we randomly choose 5 seeds to plot the learning curves.
I	Comparison with PPO
As shown in Fig. 12, we compared with the PPO in the MuJoCo task HalfCheetah-v2. For all the
methods, we randomly choose 5 seeds to plot the learning curves.
J	Robustness Evaluation
To evaluate the robustness of our approach, we develop an experiment on the Hopper-v2 environment.
We add a random noise e 〜N(0, σ2) to the state observation s, and use the noised state observation
S = S + e as the input of the policy. Our approach has a better performance in the noised input
observation situation, which is shown in Table 5.
16
Under review as a conference paper at ICLR 2021

(a) Hopper-v2
(b) HumanoidStandup-v2
Figure 11: Comparison with Gumbel-softmax and REINFORCE in the MuJoCo tasks Hopper-v2 and
HumanoidStandup-v2. We found our method PMOE-SAC has a better performance.
E5①α φσro^φ><

Figure 12: Comparison with PPO in the MuJoCo task HalfCheetah-v2. We found our method
PMOE-PPO has a better performance.
K	Comparison with different primitive numbers and different
ENTROPY REGULARISATION
To analyse the relationship of different K and different entropy regularisation(alpha), we compared 7
settings in the MuJoCo task HumanoidStandup-v2, where K is the number of primitives and alpha is
the entropy regularisation. For each setting, we randomly choose 5 seeds to plot the learning curves
in Fig. 13.
Method	0	0.05	0.1
SAC	3387.4 ± 2.0^^	1994.9 ± 718.6	1006.2 ± 389.6
gating	3444.8 ± 3.1	2606.9 ± 864.4	1626.0 ± 771.9
MCP	3524.8 ± 114.6	1610.2 ± 357.0	1008.4 ± 333.4
Ours	3632.2 ± 4.0^^	3460.4 ± 456.7	1730.0 ± 703.1
Table 5: We test our approach in the Hopper-v2 environment, each column stands for the average
return with different variance of the noise distribution, the average return of each methods is averaged
over 100 rounds.
17
Under review as a conference paper at ICLR 2021
UJmea ΦCTSΦ><
180000
160000-
140000-
120000-
100000
800Oor
0.0
---- K=4,alpha=0.2
---- K=10,alpha=0.2
----K=4,3∣ph8 = l
K=4,alpha=10
----SAC,alpha=0.2
----SAC,alpha=l
SAC,alpha=10
0.2
0.4
0.6
0.8
1.0
Million Steps
Figure 13: Comparison with different K and different amounts of entropy regularisation. Our
approach can be considered as a kind of entropy regularisation method and the number of primitives
is positively correlated with the entropy of the policy. The larger number of primitives with smaller
entropy has a similar performance to the smaller number of primitives with larger entropy.
18