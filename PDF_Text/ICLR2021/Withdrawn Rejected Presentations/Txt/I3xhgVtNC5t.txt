Under review as a conference paper at ICLR 2021
Wasserstein Distributionally Robust Opti-
mization: A Three-Player Game Framework
Anonymous authors
Paper under double-blind review
Ab stract
Wasserstein distributionally robust optimization (DRO) has recently received sig-
nificant attention in machine learning due to its connection to generalization, ro-
bustness and regularization. Existing methods only consider a limited class of loss
functions or apply to small values of robustness. In this paper, we present a three-
player game framework for solving Wasserstein DRO problem with arbitrary level
of robustness, which can handle general loss functions. Specifically, we formulate
a min-max game between three players who optimize over probability measures,
model parameters and Lagrange multipliers. We also propose new algorithms for
finding an equilibrium of the game in convex and non-convex settings which both
enjoy provable convergence guarantees. Furthermore, we prove an excess risk
bound for the proposed algorithms which shows that the solution returned by the
algorithms closely achieves the optimal minimax risk.
1	Introduction
Distributionally robust optimization (DRO) has become popular in recent years in machine learning
due to its ability to improve robustness of learning models. Instead of choosing a model f to mini-
mize an expected loss on instances generated according to P , DRO considers a perturbation to the
underlying data distribution within an ambiguity set, whether they are from covariate shifts, changes
in the underlying domain or adversarial attacks, and seeks to solve the following saddle-point prob-
lem
min sup RQ(f),	(1)
f∈F Q∈A(P)
where A(P) is an ambiguity set containing P. The choice of ambiguity set A(P) influences the
richness of the uncertainty set that we wish to consider as well as computability and can be con-
structed in a variety of ways such as f -divergence ball (Ben-Tal et al., 2013; Namkoong & Duchi,
2017; 2016; Blanchet et al., 2018) or Wasserstein ball (Blanchet et al., 2019b;a; Abadeh et al., 2015;
Gao et al., 2017; Volpi et al., 2018).
Despite of the induced robustness, the formulation of DRO problem (1) is intractable for general
cases. Prior work on DRO has focused on tractable classes of ambiguity set and loss functions. For
example, it has been shown that the DRO problem with Wasserstein distance can be reformulated as
a convex optimization problem by proving a duality result (Blanchet & Murthy, 2019; Gao & Kley-
wegt, 2016; Esfahani & Kuhn, 2018; Zhao & Guan, 2018). But this reformulation is only possible
for limited classes of loss functions. To deal with a larger class of losses, Sinha et al. (2018) consider
a relaxation of the saddle-point problem (1) and provides a stochastic gradient-type adversarial train-
ing procedure to solve it. However, there is no guarantee to find the global optimum in non-convex
case even for this relaxed problem, and their method requires additional smoothness condition on
loss functions and can only achieve a small amount of robustness. Moreover, the Wasserstein DRO
reformulations typically possess complicated structures and can not be solved efficiently by off-the-
shelf solvers in large-scale problems. To tackle this difficulty, Li et al. (2019) propose a linearized
proximal ADMM algorithm, which can achieve the same accuracy up to hundreds times faster than
the standard off-the-shelf solver. However their algorithm can only solve Wasserstein distribution-
ally robust logistic regression problems. There are also some work on DRO with f -divergence ball
(Ben-Tal et al., 2013; Namkoong & Duchi, 2017; 2016). But any f -divergence ambiguity set around
P can only contain distributions with the same support as P and thus does not allow robustness to
unseen data.
1
Under review as a conference paper at ICLR 2021
In this paper, we present a three-player framework for Wasserstein DRO problem, which applies to
general loss functions and arbitrary level of robustness. Instead of reducing the original intractable
saddle-point problem (two-player game) to a complicated convex optimization as in the literature,
we introduce auxiliary Lagrange multiplier for the Wasserstein constraints in the inner supremum
problem and formulate a min-max game between three-players who optimizes over probability dis-
tributions, model parameters and Lagrange multiplier. Each of the three players can use different
optimization algorithms customized for their problem, as long as the player strategies lead to an
equilibrium of the game.
Finding an equilibrium of a game between players remains an active topic of research in machine
learning. Much of the focus has been on a two-player game (Agarwal et al., 2018; Donini et al.,
2018; Cotter et al., 2019a;b; Kearns et al., 2018). In particular, Agarwal et al. (2018) propose an
algorithm for fair classification by computing an equilibrium of a two-player game where one player
chooses a mixture of objective functions, and the other player minimizes the loss of the mixture.
Recently, Narasimhan et al. (2019) also present a three-player approach for optimizing generalized
rate metrics. But their game formulation as well as the algorithms for each player is different from
ours. We summarize our contribution as follows:
•	We present a three-player game framework for solving Wasserstein DRO problem. Our
framework applies to arbitrary level of robustness and general loss functions, whereas prior
work focuses on either a limited class of loss functions or small values of robustness.
•	We propose new algorithms for solving an equilibrium of the game in convex and non-
convex settings which both enjoy provable convergence guarantees. In particular, the player
who optimizes over model parameters uses gradient descent algorithm in the convex case
and MCMC sampling when the loss function is non-convex.
•	We prove an excess risk bound for the proposed algorithms which shows that with high
probability the learned model achieves the optimal minimax risk up to the optimization
error, which grows linearly with a predefined accuracy parameter μ, and the generalization
error, which grows as 1 /√n.
In the next section, we formalize the problem. In Section 3, we present the three-player game
framework including the game formulation, proposed algorithms and theoretical guarantees. The
experimental results are provided in Section 4. Finally we conclude and discuss future directions.
All proofs are deferred to the appendix.
2	Problem setup
We consider Wasserstein distributionally robust optimization (DRO) problem. Let (Z, dZ) be a
compact Polish space with metric dZ and diam(Z) := supz,z0∈Z dZ (z, z0). Denote with P(Z)
the space of all Borel probability measures on Z and P1(Z) the space of all P ∈ P(Z) with finite
moment of order 1, i.e., P1(Z) := {P ∈ P(Z) : EP [dZ (z, z0)] < ∞ for z0 ∈ Z}. Then, for any
two probability measures P, Q ∈ P1(Z), the Wasserstein distance between P and Q is defined as
W(P,Q):
inf ∖(E(z∕0)〜M [dz (z,z0)]),
M∈Γ(P,Q)	( , )
where Γ(P, Q) denotes the collection of all measures on Z × Z with marginals P and Q on the first
and second factors, respectively.
Given a model family Ω and a loss function l : Ω × Z → R+, the performance of model ω on in-
stances generated according to P is measured by expected risk denoted by RP(ω) := Ez〜Pl(ω, z).
In this work, we do not assume the loss function to be convex in ω. The Wasserstein DRO problem
formulation takes the form
min sup	RQ (ω),	(2)
ω∈Q Q:W(Q,P)≤e
where the parameter e > 0 is the radius of Wasserstein ball and represents the level of robustness.
To deal with general loss function, whether convex or non-convex, we enlarge the space of possible
solutions from deterministic models Ω to stochastic models characterized by a distribution over Ω.
A stochastic model π first samples a model ω from the distribution π and then use ω to compute
2
Under review as a conference paper at ICLR 2021
the loss. The resulting expected risk for π is RP (π) := RP (ω)π(dω). By replacing RQ (ω) with
RQ(π), the Wasserstein DRO problem (2) is re-formulated as
min sup	RQ(π),	(3)
π∈δ Q:W(Q,P)≤e
where ∆ is the set of all distributions over Ω. Note that Problem (3) and (2) are equivalent for convex
loss function. In practice, we do not know the true distribution P and only have access to a data
set of training samples {zi }in=1 drawn i.i.d. from P . We therefore replace the true distribution P in
problem (3) with empirical distribution Pn = ɪ P2ι δzi and seek to solve the following problem:
min sup RQ(π).	(4)
π∈δ QW(Q,Pn)≤e
3	Three-player game framework
In this section, we first show how the Wasserstein DRO problem (4) can be formulated as a three-
player game. Then we propose algorithms for finding an equilibrium of the expanded game in
both convex and non-convex setting. We finally establish excess risk bounds which compare the
performance of the proposed algorithms to the optimal solution of (3).
3.1	Game formulation
To derive the three-player game, we first observe that problem (4) is equivalent to
max min	[-RQ (π)].
∏∈∆ Q:W(Q,Pn)≤e	"
(5)
Then we consider its dual problem (6) obtained by switching max and min and show the equivalence
of primal and dual problem in the following lemma.
Lemma 1. The strong duality holds, i.e.,
max min	[-RQ (π)] = min	max[-RQ (π)].
∏∈∆ Q：W(Q,Pn)≤e	"	Q:W(Q,Pn)≤e ∏∈∆	"
(6)
From our Lemma 1 and Lemma 36.2 of Rockafellar (1970), the maximum value of problem (5)
and the minimum value of (6) are equal and coincide with -RQ§ (n§) where n§ and Q§ denote the
solutions of the two problems respectively, i.e., (Q§, n§) is the saddle point. Therefore, by finding
the saddle point of problem (6), we can obtain the solution n§ of problem (5). However, because of
the Wasserstein ball constraints, itis very hard to directly work on problem (6). A standard approach
is to introduce a Lagrange multiplier λ ≥ 0 for the Wasserstein constraint and write the Lagrangian
for the problem
~ . , , , .
L(Q, π, λ) = -Rq(π) + λ(W(Q, Pn)- e).
Then one minimize the Lagrangian over probability measures Q, and maximize it over π ∈ ∆ and
λ∈R+,
min max L(Q, π, λ).	(7)
We pose this min-max problem as a zero-sum game between three players: a Q-player who min-
imizes L over Q, a π-player who maximizes L over π and a λ-player who maximizes L over λ.
Since L is convex in Q and linear in π and λ, one can solve an equilibrium of this game and obtain
a solution for the problem (5). For technical reasons, we impose an additional constraint on λ and
aim to solve the constrained version of problem (7)
min max	L(Q, π, λ),
Q π∈∆,B≥λ≥0
(8)
where B is a predefined parameter and L(Q, π, λ) = -RQ (π) + λ(W (Q, Pn ) - ). Notice that
because of the introduction of B we also substitute e with E and try to optimize L rather than L .We
will show how to choose B and later in our theoretical analysis in Section 3.3. Now all we need
to do is to choose the strategy that each player uses to optimize their objective, so that the players
converge to an (approximate) equilibrium of this game. We consider two cases: the loss function
is (1) convex in ω, and (2) non-convex in ω . For each case, we propose an algorithm for finding
a μ-approximate Nash equilibrium of the game, which is a triple (Q, ∏, λ), where L(Q, ∏, λ) ≤
L(Q, π, λ) + μ for all Q and L(Q, π, λ) ≥ L(Q, π, λ) - μ for all π ∈ ∆ and λ ∈ [0, B].
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Three-player game for convex
loss
Algorithm 2 Three-player game for non-convex
loss
Input: training sample {zi }in=1, Wasserstein
ball radius e, bound B, accuracy μo and μι,
learning rate α, β
Initialize θ1 = 0, ω1 = 0
for t = 1,2,…do
λ J B exP(θt)
t 1 + exp(θt)
Qt J BESTQ,μι (ωt,λt)
θt+1 J θt + α(W (Qt, Pn ) - )
ωt+1 J πn(ωt + Bdω (-RQt (ωt∕
end for
return：	Qt	=	1 p∖=ι Qt', ^⅛ =
t P∖ = 1 ωt0，λt = 1 Pt'=1 λt0
Input: same as in Algorithm 1, learning rate α
and η, a prior π1
Initialize θ1 = 0
for t = 1,2,…do
λ J B eχp(θt)
t 1 + exp(θt)
Sample ωt 〜∏t
Qt J BESTQ网 (πt, λt)
θt+1 J θt + α(W (Qt, Pn) - )
∏t+ι(dω) = :xP(-ηRQ 3F吁、
R eχp(-ηRQt (Y))πt(dγ)
end for
return: Qt =1 Pto=ι QtO, πt = 1 Pt0=1 ωt0,
t Pt0=1 λt0
3.2	Algorithms
In this subsection, we propose algorithms for solving problem (8) in both convex and non-convex
cases. Then in the following subsection, we show how the solution returned by our algorithms can
approximately solved the original problem (4) (or (3)) by choosing appropriate parameters B and .
3.2.1	CASE OF CONVEX l
We start with the case where l(ω, z) is convex in ω for any z. We find the approximate equilibrium
by using the standard scheme of Freund & Schapire (1996). We proceed iteratively with the Q-
player playing best response to the opponents strategies, while running the exponentiated gradient
algorithm (Kivinen & Warmuth, 1997) and online gradient descent for λ-player and π-player respec-
tively and terminate as soon as the sub-optimality of the average play falls below the pre-specified
accuracy μ := μo + μ1. The resulting algorithm is outlined in Algorithm 1. Here, ∏ω denotes the
l2 projection onto Ω, and BESTQ,*】represents the μ1-approximate best response of Q-PIayer, i.e.,
L(BESTQ,μι (ω, λ), ω, λ) ≤ minQ L(Q, ω, λ) + μ1, which is to solve a minimization problem of a
probability functional and will be discussed in Section 3.2.3. After the loop terminates, it results in
a deterministic model ω. Algorithm 1 is guaranteed to find an approximate equilibrium of the game
in (8) by the following theorem.
Theorem 1. Suppose that l(ω, Z) is convex in ω and K -Lipschitz in ω with respect to ∣∣∙∣∣2 for any
Z. Let P := diam(Z) + E and Bq ≥ maXω∈Ω ∣∣ω∣∣2.
Then setting α =	μ0 and β = -μ07,
4ρ2 B	4K2
Algorithm 1 will return a μ-approximate equilibrium in at most
4K 2BΩ + 8B2ρ2 log(2)
μ0
iterations.
4K2B2 +8B2ρ2 log(2)
Theorem 1 shows that We may need up to-------ω-μ^—以2 iterations to achieve a sub-optimality
μ. Note that B is the restriction that we place on the Lagrange multiplier λ. In general, large value
of B will bring the problem (8) closer to (7) and thus to the primal problem (4), but at the cost of
needing more iterations to reach any given suboptimality μ.
3.2.2	CASE OF NON-CONVEX l
In modern machine learning models, like deep neural networks, the loss function is usually non-
convex. We now provide a different algorithm for non-convex loss functions where the π-player
updates a probability distribution on the set of model parameters Ω using exponentially weighted
aggregation procedure (Cesa-Bianchi & Lugosi, 2006; Alquier et al., 2017) instead of online gra-
dient descent. The Q-player and λ-player, however, continue to operate in the same way as in
Algorithm 1.
4
Under review as a conference paper at ICLR 2021
Subroutine 3 Sampling ωt 〜∏
Start: Draw ωι 〜∏ι
for t = 1,2,…do
Set ω := ωt
Metropolis-Hastings algorithm: Repeat N times
for j = 0,1,…，N — 1 do
Sample U 〜U[o,i]
Sample ω0 〜N(ω, σ2I)
if u < min{1, exp[η(Ptt0=1(RQt0(ω) — RQt0(ω0)))]} then
ω J ω0
end if
end for
Set ωt+1 := ω
end for
In the proposed algorithm, outlined in Algorithm 2, the π-player maintains a prior distribution on
the set of model parameters, which is updated after the encounter of each new task Qt using the
performance of the models. Typically, if the expected risk for model ω is small, then we will assign
a large weight to ω. This results in weighting more those models whose accumulative loss is small.
We first give a bound for the cumulative regret of the strategy with respect to any model.
Lemma 2. Assume that l(ω, z) is K-Lipschitz in ω with respect to || ∙ ∣∣2 for any Z and uniformly
boundedfor any ω ∈ Ω and Z ∈ Z, i.e., 0 ≤ l(ω, z) ≤ M. Let Ω = {ω : ∣∣ω∣∣2 ≤ Bω, ω ∈ Rd} and
∏ι the uniform distribution on Ω. Let Qi, Q2, •…，QT, ∏ι, ∏2, •…，∏t be the iterates generated by
Algorithm 2. Thenfor any ω ∈ Ω,
TT
XRQt(πt) ≤XRQt(ω)+τT,
t=1	t=1
where TT :=当T + 字 + d log*).
8	4 η	μo
Now we show that the algorithm provably converges to an approximate equilibrium of the game (8)
with high probability.
Theorem 2. Let P := diam(Z) + e. Then, under the assumptions ofLemma 2, by setting α = μ0
4ρ2 B
and η = ^μ0, with probability at least 1 — 2δ, Algorithm 2 will return a μ-approximate equilibrium
M2
8M2 log(1∕δ) + 32B2ρ2 log⑵ + 4dM2 log(8Kj⅛Ω∕μo)
in at most
iterations.
Remember that in convex setting the algorithm returns a deterministic model ω^τ. However when
the loss function is non-convex, it yields a stochastic model ∏t with a probability mass of TT on each
ωt. For the case that T is very large, this stochastic model may be undesirable in practice. To select
a deterministic model, a heuristic approach is to compute Q using Algorithm 2 and then have the
∏-player play the best response to Q which is the maximizer of —Rq (∏) and can always be chosen
to put all of the mass on one of the candidate ω.
Remark 1. As will be shown in Section 3.2.3, the step of calculating Qt is equivalent to a stochastic
optimization problem where the objective depends on the random variable ω 〜∏t. Stochastic AP-
proximation (SA) methods (Kushner & Yin, 2003) can be applied to this problem, at each iteration
using a random sample of ω. Now, the algorithm can be implemented without calculating πt exactly
as in the last step since it only requires being able to sample a model ω from the distribution πt . This
can be achieved by using Markov chain Monte Carlo (MCMC) methods which allow us to sample
the distribution without calculating the integral in the denominator (see Subroutine 3).
Remark 2. In Subroutine 3, we use N -steps of Metropolis-Hastings algorithm with a Gaussian
proposal distribution (Robert & Casella, 2013) to sample ωt+1 from πt+1 . To ensure a short burn-in
5
Under review as a conference paper at ICLR 2021
period, we use previous drawing ωt as a starting point. Note that we have to compare ω and ω0 on
the whole distributions that Q-player has yielded so far for computing the acceptance ratio, which
might make our algorithm become slow when t grows. In practice, to improve the speed we may
truncate past history and use only a fixed number of timesteps.
3.2.3	BEST: THE Q-PLAYER’ S BEST RESPONSE
In both algorithms, We let the Q-Player Play the μι-approximate best response to the opponents
strategies. Recall that BESTq^i (ω, λ) for a given ω and λ is any μι-approximate minimizer of
a functional L(Q, ω, λ) over all probability measures on the metric space (Z, dZ). Because the
optimization noW takes place in the infinite dimensional space of probability measures, standard
finite dimensional algorithms like gradient descent are initially unavailable; even the proper notion
for the derivate of the probability functional is unclear. Luckily, it can be shoWn that this probability
functional minimization problem is equivalent to solving a transportation map. We first give the
definition of the transportation map Tω,λ(z) : Z → Z:
Tω,λ(z) = argmax{l(ω,z0) - λ ∙ dz(z0,z)}.	(9)
Then We can prove that BESTQ can be derived exactly by using the transportation map Tω,λ in the
folloWing lemma.
Lemma 3. Let Qω,λ ：= 几,入#Pn be the pushforward of the empirical distribution Pn under the
transportation map Tω,λ, i.e., Qω,λ = ɪ ∑2n=ι δτω λ⑵).Then the best response of Q-player for a
given ω and λ is attained by
nn
BESTQ(ω,λ = -X δτω,λ(zi) and W(BESTQ(ω,λj, Pn) = -X dz(‰,λ(zi),zi).
n i=1	n i=1
The above lemma shoWs that BESTQ can be obtained directly by solving the transportation map
Tω,λ(zi). Similarly, in the case of non-convex loss functions, computing BESTq小、(π, λ) is equiv-
alent to solving the stochastic transportation map, i.e., T∏,χ(z) = argmaxz，{Eω〜∏[l(ω, z0) - λ ∙
dz(z0, z)]}, Which is a stochastic optimization problem and can be tackled With Stochastic Approxi-
mation methods by using a random sample ω at each iteration. Moreover, computing the (stochastic)
transportation map is a strongly-concave optimization problem for large λ When the loss function is
smooth and dz(∙, Zi) is strongly convex (Sinha et al., 2018). Thus (stochastic) gradient method can
solve T(π)ω,λ (zi) efficiently With provable convergence guarantees. In practice, We use (stochastic)
gradient ascent steps to solve the map. See Appendix D for the pseudo-code. It should be mentioned
that this concave property does not necessarily hold When λ is small and hence gradient ascent in that
case can only guarantee to find a local maximum (Reddi et al., 2016; Allen-Zhu & Hazan, 2016).
We leave this problem to future Work. And in our experimental section, We empirically shoW that it
behaves Well on real-World datasets.
Remark 3. We discover that the the algorithm proposed in Sinha et al. (2018) for solving a relaxed
DRO problem can be recovered from our frameWork by having the Q-player play best response, π-
player play gradient descent and λ-player play a constant strategy, meaning that he alWays chooses
a fixed λ no matter What the opponent strategies are. Therefore our frameWork might be vieWed as
a generalization of their method despite that our motivation and techniques are different.
3.3	Risk bounds
From results in the previous section, Algorithm 1&2 are guaranteed to converge to an approximate
equilibrium of problem (8) (or With high probability). NoW We shoW that hoW such algorithms solve
the original problem (3), i.e., generalizing robustness to the perturbations on the true distribution
P. Let (Q,π,λ) be the μ-approximate equilibrium returned by our algorithms. Denote π* =
argmin∏∈∆ sup@：W(Q,p)≤g Rq(∏) be the solution of the problem (3). The performance of our
algorithms is measured in terms of excess risk defined as
sup	Rq(Π) — sup	Rq(∏*).
Q:W (Q,P )≤e	Q: W (Q,P )≤e
In this section, We Will provide an upper bound for the excess risk of our algorithms. Notice that
in the process of deriving Algorithms 1&2 We introduce three different sources of error. First, We
6
Under review as a conference paper at ICLR 2021
replace the true distribution P with the empirical distribution Pn . Second, we introduce a bound B
on the λ and substitute e with U Finally, We only run the algorithms for a fixed iterations until it
reaches SUb-OPtimality level μ. The first source of error is unavoidable and also called generalization
error in statistical learning theory. We can upper bound it by using existing uniform convergence
bounds for local worst-case risk (Lee & Raginsky, 2018). The other two sources of error are caused
by the oPtimization algorithm and can be driven arbitrary small via a careful selection of B and U.
We first establish that the equilibrium (Q, ∏, λ) satisfies the following property and then show how
to use it to choose B and U.
Lemma 4. Suppose that the loss function l(ω, z) is uniformly bounded for any ω ∈ Ω and Z ∈ Z,
i.e., 0 ≤ l(ω, z) ≤ M. Then we have
2 2μ + M	2 2	2μ + M
入 ≤----and W(Q, Pn) — e ≤   .
UB
Recall that Uand B are predefined parameters which are introduced in problem (8). We can always
choose a large enough B such that 2μ + ʌʃ < U and let E = e - 2μ + M. Then by Lemma 4, we
BB
have W(Q, Pn) ≤ EU. We now combine the three different sources of error and yield the following
excess risk bound.
Theorem 3. Assume that the loss function l(ω, z) is Lipschitz with respect to z and uniformly
bounded for any ω ∈ Ω and Z ∈ Z, i.e., 0 ≤ l(ω, z) ≤ M. Let (Q, π, λ) be the μ-approximate
.…..	--∙,,	, C ∙,…	(2μ + M)2 , 2μ + M ,	2μ + M~
equilibrium returned by Algorithm 1 or 2 with B = -----Z------1-----Z---and e =  -------e.
μe	E	3μ + M
Then, with probability at least 1 一 δ, π satisfies
SuP	Rq(∏) — SuP	Rq(∏*) ≤ 3μ + O^(1∕√n),
Q: W (Q,P )≤e	Q:W (Q,P )≤e
where O suppresses polynomial dependence on log(1∕δ).
This theorem shows that the solution returned by our algorithms achieves the optimal worst-case
loss on the true distribution up to the optimization error, which grows linearly with μ, and the
generalization error, which grows as 1∕√n. For Algorithm 1, we can set μ 8 1∕√n to guarantee
that the optimization error does not dominate the generalization error. Then, by Theorem 3 and
Theorem 1, we know that Algorithm 1 with B H √n, E ≈ u, α H ρ-2n-1 and β H 1/√n will
terminate in at most O(n2ρ2) iterations and return ω, which with probability at least 1 — δ satisfies
suPQ：W(Q,p)≤ Rq(c^) — supQ：W(Q,p)≤ Rq(π*) ≤ O(1∕√n). A similar analysis also applies to
Algorithm 2.
4 Experiments
We evaluate our algorithms on real-world datasets in convex and non-convex settings with the sim-
ple binary Logistic Regression (LR) and the widely-used convolutional neural networks (CNNs)
respectively. We train on MNIST (LeCun et al., 1998) and test on MNIST-M (Ganin & Lempitsky,
2015), SVHN (Netzer et al., 2011), SYN (Ganin & Lempitsky, 2015) and USPS (Denker et al.,
1989) datasets, using test accuracy as a metric for evaluating distributional robustness.
For convex setting, we consider the number-pairwise classification, and take the raw pixel values in
[0, 1] as the features accommodated with a LR classifier. For each class pair, we randomly sample
60% images for training and determine parameters by cross-validation with grid search (e.g., E = 0.5
and B = 3000). Besides the classic LR, we also compare our method with a distributionally robust
variant DRLR (Li et al., 2019). For non-convex setting, we use a ConvNet (Volpi et al., 2018) for
multi-class classification, which is composed of two convolutional layers with two fully connected
layers, and set the hyperparameter N = 50 for MCMC sampling (results with LeNet (LeCun et al.,
1998) in Appendix E). We use 10000 digit images for training. The results are compared to that of
Empirical Risk Minimization (ERM), the iterative method (ITP) (Volpi et al., 2018) and the stochas-
tic gradient descent procedure (WRM) (Sinha et al., 2018), the latter two of which are aimed to solve
a relaxed Wasserstein DRO problem. We use TQ = 15 iterations for solving the transportation map.
7
Under review as a conference paper at ICLR 2021
Table 1: Average classification accuracy on test datasets with CNNs.
DATASETS	ERM	ITP	WRM	TPG-DRO
USPS	78.9%±1.7%	78.4%±1.3%	80.0%±1.3%	78.9%±1.9%
SVHN	28.3%±3.2%	34.6%±3.3%	35.4%±1.4%	35.6%±3.1%
MNIST-M	54.8%±2.1%	59.8%±1.8%	58.6%±1.3%	58.7%±2.0%
SYN	40.6%±2.2%	45.0%±1.4%	42.6%±0.8%	45.1%±1.4%
For the transportation cost of the underlying space, we use the Euclidean norm cost for the feature
vectors and define the overall as dz((x, y), (x0, y0)) = ||x — x01|2 + ∞ly=yo.
The results are averaged over 10 independent sampling, and reported in Figure 1 and Table 1. We
observe that our method TPG-DRO (LR) achieves higher or at least the same accuracy on almost
all test datasets compared to the models trained with LR and DRLR. In non-convex setting, our
method TPG-DRO outperforms ERM on all datasets and improves the performances of ITP on
USPS, SVHN and SYN datasets. On MNIST-M, the performance of our method is slightly inferior
to ITP. Compared to WRM, TPG-DRO also achieves higher accuracy on all datasets except for
USPS. The running time of our algorithms as well as the baselines is provided in Appendix E.
Figure 1: Average classification accuracy on test datasets with LR.
5 Conclusion
We introduce a three-player game framework for solving Wasserstein DRO problem with arbitrary
level of robustness, which applies to convex and non-convex loss functions. One advantage of the
framework is that it makes new algorithms possible by designing different strategies for each player.
There remain many avenues for future investigation. Our algorithm for non-convex loss functions
uses MCMC sampling method instead of gradient-based algorithms. A major benefit of adapting
this strategy is that MCMC is (in theory) able to fully explore the parameter space. Thus it may find
a better optima for a non-convex function. As shown in Theorem 2, it returns an approximate equi-
librium with high probability. However compared to gradient-based algorithms MCMC sampling
8
Under review as a conference paper at ICLR 2021
is computationally expensive and requires more tuning such as choosing a proper proposal distribu-
tion. In the future, one interesting problem is to develop more efficient and theoretically grounded
(gradient-based) algorithm for the π-player in non-convex setting.
References
Soroosh Shafieezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distri-
butionally robust logistic regression. In Advances in Neural Information Processing Systems, pp.
1576-1584, 2015.
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A re-
ductions approach to fair classification. In International Conference on Machine Learning, pp.
60-69, 2018.
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning, pp. 699-707, 2016.
P Alquier, TT Mai, and M Pontil. Regret bounds for lifelong learning. In AISTATS, volume 54, pp.
261-269. PMLR (Proceedings of Machine Learning Research), 2017.
Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management Sci-
ence, 59(2):341-357, 2013.
Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research, 44(2):565-600, 2019.
Jose Blanchet, Karthyek Murthy, and Fan Zhang. Optimal transport based distributionally robust
optimization: Structural properties and iterative schemes. arXiv preprint arXiv:1810.02403, 2018.
Jose Blanchet, Peter W Glynn, Jun Yan, and Zhengqing Zhou. Multivariate distributionally ro-
bust convex regression under absolute error loss. In Advances in Neural Information Processing
Systems, pp. 11794-11803. 2019a.
Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applica-
tions to machine learning. Journal of Applied Probability, 56(3):830-857, 2019b.
Olivier Catoni. A pac-bayesian approach to adaptive classification. preprint, 840, 2003.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Andrew Cotter, Heinrich Jiang, and Karthik Sridharan. Two-player games for efficient non-convex
constrained optimization. In Algorithmic Learning Theory, pp. 300-332, 2019a.
Andrew Cotter, Heinrich Jiang, Serena Wang, Taman Narayan, Seungil You, Karthik Sridharan, and
Maya R Gupta. Optimization with non-differentiable constraints with applications to fairness,
recall, churn, and other goals. Journal of Machine Learning Research, 20(172):1-59, 2019b.
JS Denker, WR Gardner, HP Graf, D Henderson, RE Howard, W Hubbard, LD Jackel, HS Baird,
and I Guyon. Advances in neural information processing systems 1. chapter neural network
recognizer for hand-written zip code digits. 1989.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In Advances in Neural Information Process-
ing Systems, pp. 2791-2801, 2018.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115-166, 2018.
Yoav Freund and Robert E Schapire. Game theory, on-line prediction and boosting. In COLT,
volume 96, pp. 325-332. Citeseer, 1996.
9
Under review as a conference paper at ICLR 2021
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International Conference on Machine Learning, pp.1180-1189, 2015.
Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016.
Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributional robustness and regularization
in statistical learning. arXiv preprint arXiv:1712.06050, 2017.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the
American Statistical Association, pp. 13-30, 1963.
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerryman-
dering: Auditing and learning for subgroup fairness. In International Conference on Machine
Learning, pp. 2569-2577, 2018.
Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear
predictors. information and computation, 132(1):1-63, 1997.
Harold J. Kushner and G. George Yin. Stochastic Approximation and Recursive Algorithms and
Applications. Springer, 2003.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In Ad-
vances in Neural Information Processing Systems, pp. 2687-2696, 2018.
Jiajin Li, Sen Huang, and Anthony Man-Cho So. A first-order algorithmic framework for distribu-
tionally robust logistic regression. In Advances in Neural Information Processing Systems, pp.
3939-3949. 2019.
Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Advances in Neural Information Processing Systems, pp.
2208-2216, 2016.
Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In
Advances in Neural Information Processing Systems, pp. 2971-2980, 2017.
Harikrishna Narasimhan, Andrew Cotter, and Maya Gupta. Optimizing generalized rate metrics
with three players. In Advances in Neural Information Processing Systems, pp. 10746-10757,
2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011.
J v Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295-320, 1928.
Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International Conference on Machine Learning, pp.
314-323, 2016.
Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business
Media, 2013.
R Tyrrell Rockafellar. Convex analysis, volume 28. Princeton university press, 1970.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and
Trends® in Machine Learning, 4(2):107-194, 2012.
Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=Hk6kPgZA-.
10
Under review as a conference paper at ICLR 2021
Maurice Sion et al. On general minimax theorems. Pacific Journal of mathematics, 8(1):171-176,
1958.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio
Savarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in
Neural Information Processing Systems, pp. 5334-5344, 2018.
Chaoyue Zhao and Yongpei Guan. Data-driven risk-averse stochastic optimization with wasserstein
metric. Operations Research Letters, 46(2):262-267, 2018.
11
Under review as a conference paper at ICLR 2021
A Proofs in Section 3.1
A.1 Proof of Lemma 1
Proof. We first prove that the Wasserstein ball is compact and convex. Since Z is compact by
our assumption, by Theorem 6.18 of Villani (2008), the Wasserstein space P1(Z) is also compact.
Notice that the Wasserstein ball is a closed subset of P1 (Z) since Wasserstein distance defines a
distance on P1(Z), and a closed subset of a compact space is compact. So the Wasserstein ball is
compact. The convexity of the Wasserstein ball is straightforward since the Wasserstein distance is
convex. Then by the minimax theorem (Neumann, 1928; Sion et al., 1958), the strong duality holds
because -RQ (∏) is linear in ∏ and Q and the domains of Q and ∏ are compact and convex. □
B Proofs in Section 3.2
B.1 Proof of Theorem 1
Proof. We first derive the regret bounds for the updates of λ-player and π-player using standard
convergence analysis for online convex optimization. Let Λ := {λ : 0 ≤ λ ≤ B} and Λ0 := {λ0 ∈
Rj ： ∣∣λ0∣∣ι = B}. We associate every λ ∈ Λ with the λ0 ∈ Λ which is equal to λ on the first
dimension and puts the remaining mass on the second dimension. Consider a run of Algorithm 1.
For each λt, let λ0t ∈ Λ0 be the associated element ofΛ0. Let rt := W(Qt, Pn) - and let rt0 ∈ R2
be equal to rt on the first coordinate and 0 on the second coordinate. By definition of Wasserstein
distance, it is easy to see that ||r01∣∞ =卜11 ≤ W(Qt, Pn) + E ≤ diam(Z) + E = ρ. Moreover, for
any λ and the associated λ0, we have for all t
λrt = (λ0)Trt0 ,
and in particular,
Ifwe interpret rt0 as the reward vector for the λ-player, then the choices of λ0t correspond to those of
the exponentiated gradient algorithm. By Corollary 2.14 and 2.16 of Shalev-Shwartz et al. (2012),
for any λ0 ∈ Λ0, we have
TT
X(λ0)Trt0 ≤X(λ0t)Trt0+ξT,
t=1	t=1
where ξT :
Blog(2)
α
+ αρ2BT. Therefore, we also have for any λ ∈ Λ,
TT
Xλrt≤Xλtrt+ξT.
Similarly, for the ∏-player, the choices of ωt ∈ Ω correspond to those of the online gradient descent
with a sequence of loss Rq、(w), Rq2 (w),…,RQT (W). Since l(ω, Z) is convex in ω for any Z
by our assumption, RQt (ω) is also convex in ω for any t. Then, by Corollary 2.17 and 2.13 of
Shalev-Shwartz et al. (2012), for any ω ∈ Ω, we have
TT
X -RQt (ω) ≤ X -RQt(ωt) + ζT,
12
Under review as a conference paper at ICLR 2021
B2
where ZT = —Ω + βTK2. Now We use these regret bounds to bound the suboptimality of
2β
T / A ʌ ʌ ∖	τ-,∙	. r∙	/ ʌ ∖
L(QT, ^^t, λτ). First, for any (π, λ),
,ʌ , , ʌ
L(Qτ,π, λ) = -Rqt(π) + λ(W(Qτ,Pn)-e).
≤ T PT=I(-RQt ⑴ + λ(W (Qt,Pn)-E))
≤ T PT=1(-RQt (ωt) + λt (W(Qt, Pn) - E)) + IT + (T
=T PL L(Qt,ωt,λt) + ξT + ZT	,
≤ T PT=IL(Qτ, ωt, λt) + μι + ~T + ST
≤ L(QT, ωτ,又T) + μι + IT + *
where the first inequality uses the linearity of -RQ(π) in Q and the convexity of mapping u →
W(u, v), the second inequality follows from the regret bounds, the third inequality is by the choice
of Qt, and the last inequality uses convexity of RQT (ω) in ω.
Also, for any Q,
L(Q, ωT, λT) ≥ T PT=I L(Q, ωt, λt))
1T T
≥ T ∑t=1 L(Qt, ωt, λt) - μ1
=T PT=1(-RQt (ωt) + λt(W(Qt,pn) - Ey)- μ1
≥ T P；=1(-RQt (ωT) + WT(W(Qt,pn) - E)) - ξT - ZT - μi
≥ -RQT (&T) + λT (W (Q T ,Pn) - E) - IT - IT - μi
=L(QT, ωT, At) - IT - * - μι
where the first inequality uses the convexity of RQ (ω), the second inequality follows from the
definition of Qt, the third inequality uses the regret bounds, and the last inequality is due to convexity
of Wasserstein distance.
Combining the above results immediately implies that for any T ≥ 1, the suboptimality μτ of
r / K ʌ ʌ ∖	. ∙ r∙
L(Qτ, ^^τ, λτ) satisfies
μτ ≤ ξT + Zτ + …B⅛≡+αρ2B + * + βK2 + 〃「
By PIUggingα = 4p⅞and β =鼻,if t ≥
4K2BΩ + 8Β2ρ2 log(2)	,
-------ω-----2----------, we can verify that
μ0
μτ ≤
4ρ2Β2 log(2)	+	2K2BΩ	+ 竺 +	=
4K2BΩ+8B2ρ2 log(2) +	4K2BΩ +8B2ρ2 log(2) + 2 + "1 =内
μo —r—	μo —r—
B.2 Proof of Lemma 2
Proof. First, note that
πt(dω)
exp(-η Ptt0-=11 RQt0 (ω))π1(dω)
R exp(-η Pt-11 RQtO(Y))π1(dγ)
Denote Ht = R exp(-η Ptt0-=11 RQt0 (γ))π1(dγ). Since RQt(ω) ∈ [0, M] by assumption, using
Hoeffdings inequality (Hoeffding, 1963), for any t, we have
log Eπt (exp(-ηRQt (ω))) ≤ -ηRQt (πt) +
η2M2
□
8
13
Under review as a conference paper at ICLR 2021
which can be rewritten as
η2M2
Ent(eχp(-ηRQt(ω)))eχp(----丁) ≤ eχp(-ηRQt(πt)).
8
For Eπt (eχp(-ηRQt (ω))), it can be calculated as
Eπt (eχp(-ηRQt (ω))) = R eχp(-ηRQt (ω))πt(dω)
e -	rt i ^eχp(-ηPt-II RQto(ω))πι(dω)
=I eχp(-ηRQt(ω))--------------H-t------------
=R exP(-n P" RQto 3)π1M)	.
=~	H
_ Ht+1
=-Hr
Multiplying over t = 1,2,…，T, we get
η2M2T	T
eχp(----8一)HT +1 ≤ exp(-η ERQt (πt)).
t=1
Taking log on both sides and rearranging the terms, we obtain
PTR η ηM ηM2T	log(Hτ +1)
∑t=ι RQt (πt) ≤ —8-η—
ηM 2T	IOg(R eχp(-η PT=I RQt (γ))πι(dγ))
=---------------------------
8η
ηM 2T	log(E∏ι [eχp(-η PT=I RQt (Y))D
=-------------------------,
8η
=ηMT + inf∏ {E∏ PT=ι RQt (γ) + KLnInI}
8η
ηM 2T	T	KL(n||n1)
=∙L^- + mf∏ { Pt=ι RQt (∏) + —手」}
where the third equality follows from the duality formula for Kullback-Leibler divergence (Catoni,
2003). Denote ω* = arg min PL RQt (ω). Consider a parametric distribution family Pc(dω) 8
l{∣∣ω - ω*∣∣2 ≤ c}∏ι (dω) where 1{∙} is the indicator function. Note that when C is small, Pc highly
concentrates on ω*. Now We have
KL(pc∣∣∏ι) = -log(∏ι(∣∣ω - ω*∣∣2 ≤ c)),
and
川 *∣∣ V、> π"2(c∕2)d / πdjBΩ)d _( c 、d
π1(llω - ω 1|2 ≤ c) ≥ Γ(d∕2 + 1)∕r(d∕2+1) = (2<) ,
where the inequality is due to the fact that since ∏ι is uniformly distributed on the Bω d-ball, the
probability to be calculated is greater than the ratio between the volume of d-ball with radius of c∕2
and the volume of d-ball with radius Bq. So We get
O 口…
KL(pc∣∣∏ι) ≤ dlog(2^).
c
Furthermore, by Lipschitz assumption,
T
T
ERQt (Pc) ≤ ERQt (ω*) + TKc.
t=1
t=1
Therefore,
PT=1 RQt (∏t) ≤ ηM2T + inf∏ { PT=ι RQt (∏) +
8
KL(π∣∣∏ι)
n
ZnM2T	PTτ K) / -KL(Pc||ni)
≤	+inf c {Et=I RQt (Pc)+—n—
≤ PT=1 RQt (ω*) + nM2T + infc {TKc + d log(2^ }
8	ηc
VPTR ( *jnM2T	μoT	d	8KBω
≤ £'=1 RQt (ω)+-+—+n log( 丁)
14
Under review as a conference paper at ICLR 2021
where the last equality follows by choosing c
-μ0. We complete the proof.
□
B.3 Proof of Theorem 2
Proof. The proof follows the same logic as that of Theorem 1. First, since the strategy for λ-player
remains the same, the regret bound obtained in Theorem 1 still holds, that is, for any λ ∈ Λ,
TT
Xλrt≤Xλtrt+ξT.
Then the suboptimality of L(QT, ∏t, λτ) is bounded as follows. For any (∏, λ),
,ʌ , , ʌ
L(Q T ,π,λ) = -Rqt (π)+ λ(W (QT ,Pn)i).
≤ T PT=I (-RQt (n) + λ(W(Qt,pn) 一明
≤ T PT=1(-RQt (πt) + λt(W(Qt, Pn) - E)) + TT + TT
=T PL L(Qt,∏t,λt) + ξτ + TT
≤ T PT=IL(QT,	πt, λt)	+ μι +	-T + TT
≤ T PT=I	L(QT,	ωt, λt)	+ μι +	ξT + TT	+ M V^og(T(with probability	at least 1 - δ)
=L(Q T ,∏T ,λT) + μι + 彳 + τT + M 死兽
where the first inequality uses the linearity of -RQ(π) in Q and the convexity of mapping u →
W(u, v), the second inequality follows from the regret bound for λ-player and Lemma 2, the third
inequality is by the choice of Qt, and the last inequality uses Chernoff bound.
Also, for any Q,
L(Q, πT, *T) = T PT=I L(Q, ωt, λt))
1T
≥ T Pt=ιL(Q, πt, λt))-
≥ T PT=IL(Qt, πt, λt)-
M^ɪɑg^ɪ^ɪ (with probability at least 1 一 δ)
l log(1∕δ)
μ∖- MV
=T PT=I(-RQt (nt) + λt(W(Qt, PnaI- Myl0g2Tδ)
≥ T PT=ι(-RQt (∏T) + At (W (Qt, Pn) - e)) - -T - TT - μι - M J l0g(Tδ)
≥ -RQT (∏T) + λτ(W(QT, Pni- -T - Tt - μi - M Jl0g2Tδ)
=L(QT, ∏T, λτ)-乎-tT - μι - Mrl0g2≡
where the first inequality uses Chernoff bound, the second inequality follows from the definition
of Qt, the third inequality uses the regret bound and Lemma 2, and the last inequality is due to
convexity of Wasserstein distance.
The above results immediately imply that with probability at least 1 - 2δ,
ViξT	τT	m ∕log(1∕δ)
μτ ≤ -T + -T + μι + MV —2T—
B log(2) ,	2n , ηM2 , μo , d 1	∕8KBω、「，∕log(1∕δ) ι
=~ɑ^r+αρB + —+ τ + ηTlog( 丁) + MV ~2T~+μ1
By	plugging α =	4ρμμ0B	and η =	2M0,	if T
8M2 log(1∕δ) + 32B2ρ2 log(2) + 4dM2 log(8KIBΩ∕μ0)	,r κ /
---------------------22-------------------,We can verify that μτ ≤ μ.
μ20
≥
□
15
Under review as a conference paper at ICLR 2021
B.4 Proof of Lemma 3
The proof of Lemma 3 uses the following strong duality result by Blanchet & Murthy (2019):
Proposition 1. Let f : Z → R be upper Semicontinuous. Denote Xf泌(Z) := suPzθ∈z{f (z0) — H ∙
dZ(z, z0)}. Then for any P ∈ P1(Z),
sup	Rq (f) = min{He + EP [XfAz)}},
Q:W (Q,P )≤e	少 ≥0
and for any H ≥ 0,
sup{RQ(f) — HW (Q,P)} = Ep[χf泌(z)]∙
Q
Proof. First by definition, BESTq(3, λ) = arg minQ L(Q,ω, λ) = argminQ [—Rq(ω) +
λ(W (Q, Pn) — )] = arg maxQ[RQ(ω) — λW (Q, Pn)], i.e., BESTQ is the maximizer of
RQ(ω) — λW (Q, Pn). Then for Qω,λ, we have
RQω,λ (ω) — λW(Qω,λ, Pn) ≤ SUPq{Rq (ω) — λW(Q, 2)} = EPn [Xω,λ(z)]
= EPn [l(ω, Tπ,λ(z))] — λEPn [dZ (Tω,λ(z), z)]	,
=RQω,λ (ω) — λEpn [dz(Tω,λ(z),z)] ≤ RQω,λ 3) — λW(Qω,λ, Pn)
where the first equality follows from Proposition 1 and the last inequality uses the definition
of Wasserstein distance. Therefore we obtain RQω,λ (ω) — λW (Qω,λ, Pn) = supQ{RQ(ω) —
λW (Q, Pn)} and W (Qω,λ, Pn) = EPn [dZ (Tω,λ(z), z)], which means that the best response
BESTQ can be attained by Qω,λ. We complete the proof.	□
C Proofs in Section 3.3
C.1 Proof of Lemma 4
Proof. Define λ = 0 if W(Q, Pn) ≤ E otherwise λ = B. Then we have
,. ʌ ,
μ — RPn (∏) + λ(W(Pn,Pn) — e)
=μ + L(Pn, ∏, ʌ) ≥ Lg, ∏, ʌ)
_, ʌ .-、
≥ L(Q,π, λ) — μ	,
,. , ʌ
=-RQ (π) + λ(W(Q, Pn)-E)- μ
,. ,,ʌ . .
=—Rq (∏) + B (W (Q,Pn)—e)+ — μ
where the fist and second inequalities follows from the definition of μ-approximate equilibrium and
the last equality uses the definition ofλ and notation x+ = max{x, 0}. Arranging the terms on both
sides and using RQ (∏) — RPn (∏) ≤ M by assumption gives
2μ + M — ^e ≥ B(W(Q,Pn) — e)+ ≥ 0.
1	1 C 2μ + M	1	…	1	f	…
Thus, We obtain λ ≤ ----, proving the first part of the lemma. The second part of the lemma
E
follows immediately by (W(Q, Pn) — e)+ ≥ W(Q, Pn) — E and 2μ + M ≥ 2μ + M — λe.	□
C.2 Proof of Theorem 3
We begin by establishing an auxiliary lemma:
Lemma C.1. Under the assumptions of Theorem 3, denote F = {l(ω,z) : ω ∈ Ω} and FT =
{ T PT=1 l(ωi, z) : ωi ∈ Ω} for any given T, then with probability at least 1 — δ ,for all g ∈ FT,
一，、	一，、48√T C(F)…
sup	Rq (g) — sup	Rq (g) ≤ ---+-L + M
Q：W (Q,P )≤e	Q:W (Q,Pn)≤e	√n
where C(F) = f∞ ,IogN (F, ∣∣∙ ∣∣∞,u∕2)du and N (F, ∣∣∙∣∣∞, u/2) denotes the covering number
ofF.
v⅛)+√nC∙ diam(Z),
16
Under review as a conference paper at ICLR 2021
Proof. First, by Theorem 2 of Lee & Raginsky (2018), we have with probability at least 1 - δ, for
all g ∈ FT ,
sup	RQ (g) - sup	RQ(g) ≤
Q:W (Q,P )≤e	Q: W (Q,Pn)≤e
48W)
+Mʌ/ɪog(ɪɪ +孚 C• diam(Z)
n	2n n
Then, for any u, suppose F0 is a u/2-covering set of F. Define FT = {1 PT=I f : f ∈ F0}. It is
easy to verify that FT is also a u/2-covering set of FT. Therefore We haveN(FT, ∣∣∙∣∣∞, u/2) ≤
N(F, || • ∣∣∞,u∕2)τ. The lemma now follows by C(FT) ≤ √TC(F).	□
NoW We are ready to prove Theorem 3.
Proof. We start by decomposing the excess risk.
sup	Rq(∏) — sup	Rq(∏*)
Q:W (Q,P )≤e	Q:W (Q,P )≤e
= sup	Rq(∏) —	sup	Rq(∏) + sup	Rq(∏) — sup	Rq(∏*)+
Q:W (Q,P )≤e	Q:W (Q,Pn)≤e	Q:W (Q,Pn )≤e	Q:W (Q,Pn)≤e
sup	RQ (π*) — sup	Rq(π*)
Q:W (Q,Pn)≤e	Q:W (Q,P )≤e
For the first difference term, by Lemma C.1, we have with probability at least 1 — δ∕2,
sup	Rq(Π)—
Q:W (Q,P )≤∈
sup
Q:W (Q,Pn)≤e
Rq(∏) ≤o(i∕√n) + M
1∕log≡
V 2n
We next bound the second difference term. Denote Q = argminQ:W(Q,pn)≤∈ —Rq(∏). Since
~	I
E=E+
〜
2μ + M	1	1 TT…-、	1
------in our setting, by Lemma 4, we have W(Q, Pn) ≤ e. Then,
B
sup	Rq(∏)—
Q:W (Q,Pn)≤e
= min
Q:W (Q,Pn)≤e
≤ -RQ(π*)—
sup	Rq(∏*)
Q:W (Q,Pn )≤e
—Rq(π*) —	min	—Rq(Π)
Q	Q:W (Q,Pn)≤e	Q
min	—Rq(∏)
Q:W (Q,Pn)≤e
=L(Q ,∏*, 0) —	min	—Rq(Π)
'	'Q:W (Q,Pn)≤e	Q '
一 , 2	. C、	一 ,.、
≤ L(Q,π,λ)— c Wmin w--RQ(π)+ μ
Q:W (Q,Pn)≤e
≤ L(Q, ∏,入)+ RQ(∏) + 2μ
ʌ _______________
.. " — . . ..
—Rq(∏) + ^(W(Q, Pn) — e) + Rq(∏) + 2μ
ʌ ____
" .— . .
^(W (Q,Pn) — e) + 2μ
≤ X(E — e) + 2μ ≤
(2μ + M )2
+ 2μ = 3μ




B
where the first inequality follows from W(Q,Pn) ≤ e, the second and third inequalities use the
definition of μ-approximate equilibrium, the fourth inequality is due to the fact that W(Q, Pn) ≤ E
by the definition of Q and the last inequality uses Lemma 4.
Finally, by Proposition 1, the last difference term can be rewritten as
sup	Rq(∏*) — sup	Rq(∏*)
Q:W (Q,Pn)≤e	Q:W (Q,P )≤e
=m≥n{洗+ L Xθ,l∏* (Z)Pn(dz)} — m≥n{注+ L Xθ,'∏* (Z)P(dz)}
Denote 历=arg min^≥o{^<≡ + JiZ Xqιπ* (Z)P(dz)}. Then, we have
sup	RQ(π*) —	sup	RQ(π*) ≤ χ X初,lπ* (Z)(Pn ― P)(dz) .
Q:W (Q,Pn )≤e	Q:W (Q,P )≤e	JZ
17
Under review as a conference paper at ICLR 2021
Algorithm 4 Pseudo-code of the three players
Sample {zi}rm=ι 〜Pn
Q-player update:
for i = 1, 2,…，m do
Zk J Zi
Update zik by ascending its stochastic descents
if non-convex then
Sample ω from π
end if
Zik J Zik + κ∂z [l(ω, Zik ) - λdZ (Zik , Zi)]
end for
λ-player update:
Update θ by
θ — θ+α( m Pm=I dz (zk, Zi) - E)
π-player update:
Case 1:	Convex loss
Update ω by
ω J πΩ(W - β( m Pm=I dω l(ω, Zk )))
Case 2:	Non-convex loss
Metropolis-Hastings algorithm: Repeat N times
fθr j = 0,1,…，N 一 1 do
Sample U 〜U[o,i]
Sample ω0 〜N(ω, σ2I)
if u < min{1,eχp[η(p∖=ι m1 Pm=ι(l(ω,zkto) - l(ω0,zkt0)))]}then
ω J ω0
end if
end for
Since l(ω, z) takes values in [0, M] by assumption, the same holds for l∏* and X初,1*. From Hoeffd-
ings inequality, it follows that
SuP	Rq(∏*) —
QW (Q,Pn)≤e
suP
Q:W (Q,P )≤e
Rq(∏*) ≤ M
holds with probability at least 1 一 δ∕2.
Collecting all these terms and applying the union bound, we have
SupQ:W(Q,P)≤e RQ(π) - SupQ:W(Q,P)≤e RQ(π*) ≤ O(I∕√n) + 2M
with probability at least 1 - δ.
□
D Pseudo-code
The pseudo-code of each player is summarized in Algorithm 4.
E Additional Experimental Results
Here, we provide the time consumption of our algorithms and the baselines used in the experimental
section. All the experiments are conducted on a single NVIDIA TITAN Xp GPU. We compute the
overall training time on MNIST for 20 epochs. Table 2 summarizes the running time for different
algorithms. We observe that in convex case the time cost of our algorithm is close to LR and DRLR,
18
Under review as a conference paper at ICLR 2021
Table 2: The running time for different algorithms on MNIST.
Methods	Training Duration
Non-convex (hours)	ERM	0.1 ITP	0.1 WRM	0.6 TPG-DRO	1
Convex (mins)	TR	3 DRLR	3 TPG-DRO (LR)	6
Table 3: Average classification accuracy on test datasets with LeNet.
DATASETS	ERM	ITP	WRM	TPG-DRO
USPS	76.2%±2.1%	76.0%±1.4%	81.0%±1.0%	81.6%±2.3%
SVHN	24.1%±1.2%	24.4%±1.5%	32.2%±2.1%	33.4%±2.5%
MNIST-M	48.0%±1.9%	51.4%±0.2%	58.6%±0.7%	57.2%±1.0%
SYN	34.7%±1.2%	37.0%±0.6%	44.7%±0.8%	45.1% ±1.0%
but in non-convex case our algorithm is a little bit slower than other baselines mainly because of
the sampling step. We believe that this time gap in non-convex setting can be greatly reduced by
using more advanced MCMC sampling method. Table 3 reports the results using LeNet. As in the
body, the model trained with our algorithm achieves higher or at least comparable accuracy on all
test datasets compared to the models trained with ERM, ITP and WRM.
19