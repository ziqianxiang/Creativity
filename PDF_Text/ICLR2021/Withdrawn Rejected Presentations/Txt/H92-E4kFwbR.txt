Under review as a conference paper at ICLR 2021
Composite Adversarial Training for Multiple
Adversarial Perturbations and Beyond
Anonymous authors
Paper under double-blind review
Ab stract
One intriguing property of deep neural networks (DNNs) is their vulnerability to
adversarial perturbations. Despite the plethora of work on defending against indi-
vidual perturbation models, improving DNN robustness against the combinations
of multiple perturbations is still fairly under-studied. In this paper, we propose
composite adversarial training (Cat), a novel training method that flexibly inte-
grates and optimizes multiple adversarial losses, leading to significant robustness
improvement with respect to individual perturbations as well as their “compo-
sitions”. Through empirical evaluation on benchmark datasets and models, we
show that Cat outperforms existing adversarial training methods by large margins
in defending against the compositions of pixel perturbations and spatial transfor-
mations, two major classes of adversarial perturbation models, while incurring
limited impact on clean inputs.
1	Introduction
Despite their state-of-the-art performance in tasks ranging from computer vision (Szegedy et al.,
2016) to natural language processing (Seo et al., 2017), deep neural networks (DNNs) are inherently
susceptible to adversarial examples (Szegedy et al., 2014), which are maliciously crafted samples
to deceive target DNNs. A flurry of adversarial attacks have been proposed, which craft adversarial
examples via either pixel perturbation (Goodfellow et al., 2015b; Moosavi-Dezfooli et al., 2016;
Carlini & Wagner, 2017a) or spatial transformation (Engstrom et al., 2017; Xiao et al., 2018; Alaifari
et al., 2019). To defend against such attacks, a line of work attempts to improve DNN robustness
by developing new training and inference strategies (Kurakin et al., 2017; Guo et al., 2018; Liao
et al., 2018; Tramer et al., 2018). Yet, the existing defenses are often circumvented or penetrated by
adaptive attacks (Athalye et al., 2018), while adversarial training (Madry et al., 2018; Shafahi et al.,
2019) proves to be one state-of-the-art defense that still stands against adaptive attacks.
While most adversarial training methods are primarily designed for individual attacks which are
either fixed (Madry et al., 2018) or selected from a pre-defined pool (Tramer & Boneh, 2019; Maini
et al., 2020), in realistic settings, the adversary is not constrained to individual perturbation models
but free to “compose” multiple perturbation models to construct more powerful attacks. Despite their
robustness against individual attacks, the DNNs trained using existing methods often fail to defend
against such composite attacks (details in § 2). Moreover, the existing adversarial training methods
focus on pixel perturbation-based attacks (e.g., bounded by `p-norm balls), while the research on
training DNNs robust against spatial transformation-based attacks is still limited.
To bridge this striking gap, in this paper, we present Cat, a novel adversarial training method able to
flexibly integrate and optimize multiple adversarial robustness losses, which leads to DNNs robust
with respect to multiple individual perturbation models as well as their “compositions”. Specifically,
Cat assumes an attack model that composes multiple perturbations and, while bounded by the over-
all perturbation budget, optimally allocates the budget to each iteration. To solve the computational
challenges of this formulation, we extend the recent advances on fast projection to `p,1 mixed-norm
ball (LiU & Ye, 2010; Sra, 2012; Bejar et al., 2019) to our setting and significantly improve the
optimization efficiency. We validate the efficacy of Cat on benchmark datasets and models. For
instance, on MNIST, CAT outperforms alternative adversarial training methods (Tramer & Boneh,
2019) by over 44% in terms of adversarial accuracy against attacks that combine pixel perturbation
and spatial transformation (details in § 4), with comparable clean accuracy and training efficiency.
1
Under review as a conference paper at ICLR 2021
Our contributions can be summarized as follows. First, we demonstrate that a new class of adver-
sarial attacks, which “compose” multiple perturbations, render most existing adversarial training
methods ineffective; then, we propose Cat, the first adversarial training method designed for mul-
tiple perturbation models as well as their compositions; further, we validate the efficacy of Cat by
comparing it against alternative methods on benchmark datasets and DNNs; finally, we explore the
optimization space of composite perturbations, leading to several promising research directions.
2	Fundamentals
2.1	Adversarial Training
Adversarial training is a class of techniques to train robust DNNs by minimizing the worst-case loss
with respect to a given adversarial perturbation model. Formally, let fθ be a DNN parameterized by
θ, ` the loss function, and Dtrain = {xi , yi}in=1 the training set. Then the adversarial training with
respect to an `p adversary with perturbation magnitude is defined as:
θ* = arg min ^X maX'(fa (xi + δ),yi),	(1)
θ i δ∈Bp()
where Bp() = {δ : kδkp ≤ } is the `p-norm ball of radius . Here, the inner maximization
problem essentially defines the target adversarial attack. For instance, instantiating it as the '∞
projected gradient descent (PGD) attack leads to the well-known PGD adversarial training.
Despite their effectiveness against considered perturbation models (e.g., '∞ perturbation), the exist-
ing adversarial training methods often fail to defend against perturbations that they are not designed
for (Tramer & Boneh, 2019). Motivated by this, some recent work explores conducting adversarial
training with respect to multiple perturbation models simultaneously.
AVG and MAX - Tramer & Boneh (2019) propose two methods, AVG and Max, to aggregate
multiple perturbations. Specifically, Avg formulates the robustness optimization as:
θ* = arg min	δ max '(fθ (Xi + δp),yi)	(2)
θ i p∈Aδp∈Bp()
where A = {1, 2, ∞}. Compared with Eq. 1, Eq. 2 aggregates multiple adversarial perturbations in
the inner loop. Similarly, instead of averaging multiple perturbations, Max selects the perturbation
resulting in the largest loss:
θ* = arg min	max	`(fθ (xi + δ), yi)	(3)
θ	片{δ∈Bp(e)∣p∈A}
If A contains only one adversarial perturbation, Eq. 3, Eq. 2, and Eq. 1 are all equivalent.
MSD - While AVG and MAx achieve varying degrees of robustness to the considered perturbations,
it is practically difficult to minimize the worst-case loss with respect to the union of perturbations.
To this end, Maini et al. (2020) propose multiple steepest descent (Msd) which improves Max (and
Avg) along two aspects. First, it selects the largest (or average) perturbation at each inner itera-
tion; Second, it applies the steepest descent instead of the projected gradient descent in generating
adversarial inputs. Formally, MSD formulates the optimization at the t-th iteration as:
δp(t+1) = ProjBp() δ(t) + vp(δ(t))	forp ∈ A	(4)
δ(t+1) = arg max '(fθ(x + δPt+1)),y)	(5)
δp(t+1)
where ProjC(∙) is the projection operator onto the convex set C, and Vp(δ(t)) is the steepest descent
direction for 'p perturbation, vp(δ) = argmaXkvkP≤λ VTV'(fθ(x + δ), y), where λ is the step size.
2.2 Composite Adversarial Attack
While the existing adversarial training methods seem effective against individual perturbation mod-
els which are either fixed or selected from a pre-defined pool (i.e., the union of perturbations), in
2
Under review as a conference paper at ICLR 2021
truth airplane car bird cat deer dog frog horse
label
natural
image
ship
l⅛12-*, 1
truck
_■
δpixel
H
perturbed
image
preladbicetled frog truck deer dog horse bird dog dog airplane
car
Figure 1:	Samples produced by composite attacks on CIFAR10 (p = 0.015, f = 0.175, untargeted).
a realistic setting, the adversary is able to combine multiple perturbation models to construct more
destructive attacks, which we exemplify with a new class of composite adversarial attacks.
Intuitively, the attack constructs an adversarial example by applying a sequence of perturbation
models {Ai}im=1, each Ai bounded by an independent perturbation budget i. Here, we treat Ai as
an abstract operator Ai(∙, ej (e.g., pixel perturbation or spatial transformation), which applies the
corresponding perturbation (bounded by i) over the output of its previous perturbation:
x(i) = x(i-1) + δi-1,	δi = Ai(x(i) , ei)	for i = 1, . . . , m	(6)
We can further generalize the attack to a more flexible setting in which the adversary, while bounded
by the overall perturbation budget, is able to optimally allocate the budget to each optimization
iteration. The details of this generalization are discussed in Appendix B. Figure 1 illustrates samples
generated by the composite adversarial attack which combines one pixel perturbation (with budget
ep) and one spatial transformation (with budget ef).
Given attacks {Ai }im=1 with Ai bounded by ei, in the composite attack, we re-scale the perturbation
budget by a factor of 1/m (i.e., ei/m for Ai), to make the composition of {Ai}im=1 comparable with
the union attack. Intuitively, with proper setting of {ei}im=1, the composition of {Ai}im=1 is strictly
stronger than each individual attack as well as their union (detailed proofs in Appendix A). Figure 2
compares the robust accuracy of Avg, Max, and Msd on the unions and compositions of multiple
perturbations. Observe that while effective on the union attacks, the existing methods fail to defend
against the composite attacks, with robust accuracy drop as large as 40%.
0.6
0.4
0.2
0.0
0.0
AVG	MAX	MSD	AVG	MAX	MSD
□ Union □ Composition
Figure 2:	Adversarial accuracy of Avg, Max, and MSD w.r.t. unions and compositions of A = {'ι, '2, '∞}.
3 CAT: Composite Adversarial Training
We now present Cat, a new adversarial training method to defend against multiple perturbations as
well as their compositions.
3.1	Formulation
At a high level, Cat adopts the composite adversarial attack as the inner loop of Eq. 1, which
generates adversarial examples through a sequence of perturbations:
m
x* = {max '(fθ(X + 工δi),y)
s.t.	kδi k ≤ ei for i = 1, . . . , m
(7)
3
Under review as a conference paper at ICLR 2021
As concrete instances, in the case of composing pixel perturbations (i.e., 'ι, '2, '∞ perturbations)
with budget e], €2, and e∞, respectively, the straightforward composite adversarial exa^∩ples is
produced by x* = X + δ] + δ2 + δ∞, where We omit the clipping operator.
Next We consider composing pixel perturbation and spatial transformation (Xiao et al., 2018; Alai-
fari et al., 2019) with budget ep and ef respectively. We first give a brief introduction of spatial
transformation. Instead of directly perturbing the values of pixels, spatial transformation displaces
their coordinates. Formally, the input X is represented as a set of tuples X = {(ui, vi, b)}n=ι, where
(ui, Vi) are the coordinates of x,s i-th pixel and b is its value. We set f = {(△%, ∆vi)}n=ι (with
abuse of notations here), where (△%, ∆vi) are the displacement of x,s i-th pixel. When we
apply f to X to generate the adversarial input χ0, (ui,Vi) = (Ui + ∆ui, Vi + ∆vj. Typi-
cally, bi-linear interpolation is applied to handle fractional pixel positions (Jaderberg et al., 2015):
bi = Pq∈N(ui,vi) bq(1 - ∣Ui - Uq|)(1 - E - Vq|), where N(ui,Vi) is the neighboring points of
(ui, Vi). Following Alaifari et al. (2019), we measure the perturbation magnitude as f's '∞-norm:
kfk∞ = max{maxi ∣∆ui|, maxi ∣∆vi∣}.
MAX	AVG
Attack A Attack B	Attack A Attack B
JL1 ɪ	A, r.L
MSD
Attack A Attack B
TnHTnU
uoEJ9I £，
TnHTnU
TnHTnU
uoEJ9I I.p，3
TΠHTΠU.
uoncEJiiI 中 3
CAT
Attack A Attack B
uo-cEJaI I-P-A
Max	Avg	Max
Figure 3: Comparison of different adversarial training frameworks.
3.2 DISCUSSION
Figure 3 compares different adversarial training methods (Avg, Max, Msd, and Cat). The design
of Cat enjoys two major benefits. First, by definition, the composite adversarial attack naturally
covers the strongest individual attack and the union of these attacks, as demonstrated in the two
instantiations above. Second, Cat generalizes adversarial robustness from individual attacks which
are either fixed or selected from a fixed pool to their compositions.
As composite adversarial attacks are by nature stronger than individual attacks, setting the pertur-
bation budget overly large in Cat may cause accuracy degradation with respect to clean inputs. We
propose a variant α-CAT to mitigate this issue. With 0 < 1 ≤ α as a hyper-parameter, we re-scale
the perturbation budget of each component attack Ai to αei during the adversarial training.
CAT-r
One issue with the above α-CAT formulation is the trade-off between attack strength of individual
component attack and clean accuracy. With smaller a, we expect a high clean accuracy. However,
the component attack might not strong enough to cover the original perturbation size. For instance,
if we take α = 3, the trained model may have low robust accuracy for each individual attack and
hence their union. On the contrary, a very large a (close to 1) causes significant drop in the clean
accuracy, make the robust model is not useful. Our work-round solution for this is to use smaller ɑ
to ensure clean accuracy, while we sample component attacks with replacement during adversarial
training to enhance the robustness against individual attacks. In fact, under this implementation, we
may sample a multiple stack of the same type attack, which corresponds to that attacker with larger
perturbation size.
4	Empirical Evaluation
We empirically evaluate the efficacy of Cat in various settings. All the experiments are performed
on MNIST and CIFAR10 dataset. Specifically, our results convey two key messages. First, Cat
trains DNNs robust against both composition perturbations and union perturbations in the 'p space
('1, '2 and '∞), suggesting that composite adversarial robustness is a generalization of the adversar-
4
Under review as a conference paper at ICLR 2021
ial robustness with respect to the union of multiple perturbations. Second, Cat is able to train DNNs
robust against both pixel perturbation (e.g., '∞ perturbation) and free-form spatial transformation.
Models and Hyper-parameters - On MNIST, We use the network architecture used by both Maini
et al. (2020) and Madry et al. (2018), which is a DNN consisting of4 convolutional layers, followed
by 2 fully connected layers. On CIFAR10, we use a pre-activation version of ResNet32 (He et al.,
2016), which is build up with 16 residual blocks, followed by 1 global averaging pool layer, and 1
fully connected layer.
Setting of α - As discussed in § 3.2, to minimize the performance degradation on clean inputs, we
re-scale the allowed perturbation magnitude uniformly by α. To set α properly, we conduct a grid
search within {1.0, 0.9, 0.8, 0.7, 0.6, 0.5} and select the optimal α value by optimizing the trained
model’s robust accuracy (under the union threat model) and clean accuracy (with less than 10.0%
drop from all baselines under the union threat model).
We implement all the algorithms with PyTorch and run all the experiments on a single Nvidia RTX
6000. The detailed setting of models and (hyper-)parameters is summarized in Appendix B.
4.1	ROBUSTNESS IN `p SPACE
In the first part, we evaluate Cat and baselines on the regular pixel perturbation-based attacks.
Baselines - We compare CAT with MAX- worst-case perturbation, AVG- average-case perturba-
tion (Tramer & Boneh, 2019), and MSD (Maini et al., 2020). Besides, we also include robust DNNs
trained with PGD attacks with respect to individual perturbations.
Component Attacks for CAT- We consider 3 commonly used 'p perturbations, namely '∞, '2, and
'1 in Cat. For '∞ attack, we use '∞ PGD attack (Madry et al., 2018). For '2 attack, we implement
an `2 PGD adversary. For `1 attack, we use the enhanced `1 attack proposed in Maini et al. (2020).
Attacks Used for Evaluation - To evaluate the robustness of DNNs trained by CAT and baselines,
we consider a collection of representative adversarial attacks.
Individual perturbations - For '∞ attack, we consider both '∞ PGD attack and Fast Gradient Sign
attack (Goodfellow et al., 2015a); for '2 attack, we consider '2 PGD, DeepFool (Moosavi-Dezfooli
et al., 2016), C&W attack (Carlini & Wagner, 2017b), and Salt&Pepper attack (Rauber et al., 2017);
for '1 attack, we use '1 PGD attack.
Combined perturbations - Besides, we consider both unions and compositions of multiple perturba-
tions. Following Maini et al. (2020), in the union threat model, the adversary applies all the attacks
on the given input and is considered successful if one of the attacks succeeds. In the composite
threat model, we consider a set of composite attacks with A1 = '∞-PGD, A2 = '2-PGD, and
A3 = '1 -PGD. For the composite attacks, we measure the robust accuracy under different settings
of re-scale factor α, which is defined similarly in CAT-α.
Results - Table 1 and Table 2 summarize the results with respect to pixel perturbation-based attacks
on CIFAR10 and MNIST. The perturbation budget for each type of'p attack is shown in the tables.
To measure models’ performance on clean inputs, we use their accuracy on the test set. To evaluate
the robustness of each model, we measure the robust accuracy of all the models on a random sample
of 1,000 test inputs. The robust accuracy is defined as the fraction of test inputs that are misclassified
initially or are predicted to wrong classes after an attack. In reporting robust accuracy, we aggregate
results of all the attacks from each target norm. In other word, an input is correctly predicted after
attacks for 'p norm if and only if all the attacks of 'p norms from above fail for the input. Similarly,
for the union setting, all the attacks except combined perturbations are considered.
On CIFAR10, with a slight degradation of clean accuracy, Cat achieves the same robustness accu-
racy as MSD under the union of three 'p perturbations and outperforms all other baselines. Further-
more, CAT is much more robust against composite adversarial attacks when α = 0.5, outperforming
Msd and other baselines by over 10%.
On MNIST, PGD-'∞ (P∞) achieves the best performance across all the settings, which however
is attributed to the holding gradient masking effects (Tramer & Boneh, 2019). In Appendix C, we
show the robust accuracy of models in Table 9 on two decision-based attacks, and we confirm the
higher attack success rate with black-box models of P∞ . Thus, we exclude P∞ in the following
5
Under review as a conference paper at ICLR 2021
	P∞	P2	Pi	MAX	AVG	MSD	Cat	CAT-r
clean accuracy	83.3%	90.2%	73.3%	81.0%	84.6%	81.1%	72.6%	81.6%
'∞ attacks (e = 0.03)	49.7%	21.4%	0.0%	45.9%	42.8%	47.5%	46.9%	43.6%
`2 attacks ( = 0.5)	59.0%	65.5%	0.0%	62.7%	67.6%	65.8%	60.3%	66.7%
`1 attacks ( = 12)	16.6%	25.8%	10.2%	39.3%	57.3%	54.8%	59.6%	63.1%
Union	16.6%	18.1%	0.0%	34.5%	42.1%	46.7%	46.7%	43.5%
Composite (0.5)	11.2%	11.6%	0.3%	31.0%	33.5%	37.7%	47.0%	43.4%
Table 1. Performances of CAT and baselines on CIFAR10 with `p perturbations (p = 1, 2, ∞). Rows represent
attacks, and columns denote robust trained models. P∞, P2, and P1 are models adversarially trained with PGD
attacks (with corresponding norms).
	P∞	P2	Pi	MAX	AVG	MSD	Cat	CAT-r
clean accuracy	99.1%	99.2%	99.3%	98.6%	99.1%	98.3%	91.7%	98.7%
'∞ attacks (e = 0.3)	92.1%	1.5%	0.0%	57.5%	71.0%	68.1%	42.5%	67.1%
'2 attacks (e = 2)	63.0%	75.9%	45.8%	68.1%	71.0%	74.7%	67.5%	77.6%
'1 attacks (e = 10)	71.5%	69.3%	77.2%	57.1%	65.1%	70.9%	74.3%	75.2%
Union	59.6%	1.5%	0.0%	47.7%	55.8%	63.4%	42.4%	63.9%
Composite (0.5)	49.0%	3.3%	0.0%	7.8%	12.0%	18.5%	38.8%	49.4%
Table 2. Performances of CAT and baseline methods on MNIST with `p perturbations (p = 1, 2, ∞). Rows
represent attacks, and columns denote robust trained models. P∞, P2, and P1 are models adversarially trained
with PGD attacks (with corresponding norms).
discussion. Across all the other methods, Cat only performs slightly worse than Max, Avg, and
MSD under the union threat model. Under the composite threat model (α = 0.5), CAT outperforms
baselines in terms of robust accuracy by margins over 20%.
The above results indicate that Cat assumes a strong adversarial attack model during the adversarial
training process and produces DNNs with robustness not only against individual perturbations (and
their unions) but also against their compositions.
4.2	Robustness against Pixel Perturbations and S patial Transformations
Next we evaluate Cat and baselines against compositions of pixel perturbations and spatial trans-
formations.
Component Attacks - For pixel perturbation, We consider '∞ PGD attack. For spatial transforma-
tion, we consider a projected gradient descent approach. The key differences between this formu-
lation and ADef (Alaifari et al., 2019) are that 1) PGD is much faster than the DeepFool procedure
proposed in ADef and 2) it uses free-form floWs instead of the smoothed floWs.
Attacks Used for Evaluation - We evaluate CAT and baselines against the above individual pertur-
bations as Well as their unions and compositions.
Baselines - Since MSD is only applicable to pixel perturbation-based attacks, We consider AVG and
Max as the baselines. Plus, We use tWo DNNs that are adversarially trained using the above piexl
perturbation and spatial transformation respectively, Which We refer to as Ppixel and Pflow.
Results - Table 3 and 4 summarize the results. We have the folloWing observations. First, CAT
achieves similar (on MNIST) or even better (on CIFAR10) robust accuracy than the baselines under
the union threat model. Second, Cat outperforms all the baselines under the composite threat model
by large margins. For instance, on MNIST, the robust accuracy of all the baselines drops to close to
0 even With α = 0.5 (i.e., half of the specified perturbation budget); in contrast, CAT attains 46%
robust accuracy.
	PpiXel	Pflow	MAX	AVG	Cat	Cat-r
clean accuracy	99.1%	98.8%	98.0%	98.2%	95.6%	96.7%
pixel attack (e = 0.3)	92.1%	0.0%	77.3%	85.2%	91.8%	88.9%
floW attack (e = 0.75)	3.0%	52.4%	44.8%	43.3%	41.3%	53.5%
Union	3.0%	0.0%	42.5%	42.4%	41.2%	53.5%
Composite (0.5)	12.5%	0.0%	1.0%	2.0%	46.0%	72.4%
Table 3. Performance of Cat and baselines on MNIST With respect to pixel and spatial perturbations. RoWs
represent attacks, and columns denote robust trained models.
6
Under review as a conference paper at ICLR 2021
	PpiXel	Pflow	MAX	AVG	Cat	CAT-r
clean accuracy	83.3%	82.5%	77.2%	79.8%	71.3%	74.7%
pixel attack ( = 0.03)	50.0%	0.0%	45.5%	45.3%	47.4%	39.8%
floW attack ( = 0.35)	21.3%	48.4%	40.8%	40.6%	44.1%	39.0%
Union	18.7%	0.0%	35.4%	32.8%	38.6%	32.3%
Composite (0.5)	26.5%	0.2%	38.2%	36.7%	43.0%	38.6%
Table 4. Performance of Cat and baselines on CIFAR10 With respect to pixel and spatial perturbations. RoWs
represent attacks, and columns denote robust trained models.
Composite Type I I Composite (0.5) . I Composite (0.8) . ∣ Composite (1.0)
Composite Type I I Composite (0.5) . I Composite (0.8) . ∣ Composite (1.0)
(a) CIFAR10: `p space	(b) CIFAR10: pixel and spatial perturbations
Figure 4: Robust accuracy of Cat and baselines on CIFAR10 dataset under composite adversarial attacks with
re-scaling factor α=0.5, 0.8, and 1.0.
4.3 ROBUSTNESS AGAINST COMPOSITE ATTACKS WITH VARYING α
Thus far, we have assumed the defender and attacker use the same setting of re-scaling factor
(α = 0.5) in attacking the DNNs. Next, we evaluate the impact of varying α by the attacker on
the robust accuracy. Figure 4 summarizes the results on CIFAR10 under α = 0.5, 0.8, and 1.0
(which correspond to stronger attacks). Observe that as expected, both Cat and baselines expe-
rience performance degradation under large α and yet, CAT still consistently outperforms all the
baselines by large margins across all the settings.
5	Exploring the Space of Composite Perturbations
In this section, we empirically study the critical properties of composite adversarial attacks and Cat.
One particular aspect is the impact of the ordering of component attacks on the adversarial training.
Besides, we consider an even stronger composite attack which, bounded by overall perturbation
budget, is able to optimally allocate the budget to each round.
5.1	Ordering of Component Attacks
We evaluate the impact of the ordering of component attacks under the setting of `p perturbations
only as well as compositions of pixel perturbations and spatial transformations.
'p Perturbations - In this set of experiments, We consider the compositions of '∞-PGD and '?-
PGD attacks. Table 5 and 6 summarize the performance of CAT under the compositions of '∞ and
`2 perturbations. We have the folloWing observations. First, flipping the tWo attacks in CAT has little
impact on both clean accuracy and robust accuracy of Cat. Second, observable from the last tWo
columns of each table, the effectiveness of composite adversarial attacks seems also independent of
the ordering of component attacks.
			Clean Accuracy	'∞	'2		Union	'∞ - '2 (0.5)	'2 - '∞ (0.5)
'∞	= 0.3 - `2	2.0	991%	92.5%	77.7%	77.4%	842%	84.2%
`2	= 2.0 - '∞ 二	0.3	99.1%	92.8%	79.5%	79.1%	85.7%	84.7%
'∞	= 0.15 - `2	= 2.0	994%	96.9%	58.2%	58.2%	86.0%	85.7%
`2 =	2.0 -'∞ =	0.15	99.4%	96.8%	57.7%	57.7%	85.3%	85.3%
'∞	= 0.3 - `2	1.0	991%	92.4%	95.9%	92.1%	93.3%	93.4%
`2	=1.0 - '∞ 二	0.3	99.0%	92.2%	95.8%	92.0%	93.4%	93.8%
Table 5. Impact of the ordering of component attacks in Cat on MNIST under compositions of '∞ and '2
perturbations (A1 - A2 is the ordering used in CAT and composite attacks).
7
Under review as a conference paper at ICLR 2021
			Clean Accuracy	'∞	'2		Union	'∞ - '2 (0.5)	'2 - '∞ (0.5)
'∞	= 0.03 -`2	= 0.5	827%	48.8%	64.4%	48.8%	57.0%	57.0%
`2	=0.5 - '∞ =	0.03	82.8%	50.3%	64.9%	50.3%	58.5%	58.6%
-'∞	= 0.015 - `2	= 0.5	888%	69.1%	68.2%	67.1%	68.3%	68.4%
`2 =	= 0.5 - '∞ =	0.015	89.1%	69.8%	67.2%	66.7%	68.7%	68.7%
-'∞	= 0.03 - `2	0.25	833%	50.0%	74.5%	50.0%	627%	62.5%
`2 =	二 0.25 - '∞ 二	0.03	82.7%	51.0%	73.4%	51.0%	62.7%	62.7%
Table 6. Impact of the ordering of component attacks in Cat on CIFAR10 under compositions of '∞ and '2
perturbations (Ai - A2 is the ordering used in Cat and composite attacks).
		Clean Accuracy	Pixel	Flow	Union	p∞ - f∞ (0.5)	f∞ - p∞ (0.5)
MNIST	p∞ - f∞	95.6%	91.8%	41.3%	41.2%	46.0%	52.6%
	f∞ - p∞	96.7%	92.8%	34.4%	34.4%	40.6%	42.7%
CIFAR10	p∞ - f∞	713%	47.4%	44.1%	38.3%	43.0%	42.3%
	f∞ - p∞	70.3%	48.0%	43.2%	38.8%	42.8%	42.6%
Table 7. Impact of the ordering of component attacks (pixel and spatial perturbations) in Cat on MNIST and
CIFAR10 (Ai - A2 denotes the ordering of component attacks used in Cat and composite attacks, while the
perturbation budget is the same as Table 3 and 4).
Pixel and Spatial Perturbations - Similarly, we empirically evaluate the impact of the ordering of
pixel and spatial perturbations. Here we follow the same setup as § 4 (the same attacks and α for
each dataset). The results are summarized in Table 7. We observe that on CIFAR10, the ordering
has fairly limited impact as in the `p case; however, we find that first applying pixel perturbation and
then spatial transformation results in more robust model on MNIST. We consider the study of the
root cause of this interesting phenomenon as our ongoing work.
5.2	Robustness Against Fine-grained Composite Adversaries
Finally, we consider an extension of the basic composite adversarial attack (each perturbation is
applied only once) to a multiple round setting. Under this setting, the adversary, while bounded by
the overall perturbation budget, is able to optimally allocate the budget to each iteration, leading to
even stronger attacks.
Formulation and Solution - We sketch the differences between CAT and K-round CAT here, with
full details deferred to the appendix B.
Perturbation Accounting - For each component perturbation Ai, instead of seeking a single δi, We
divide it into K parts δi,k for k = 1, . . . , K. We measure the overall perturbation cost as the sum of
{kδi,kkp}kK=1, where p is `p norm. In other word, the new constraint is PkK=1 kδi,k kp ≤ i .
Optimization Solution - This new constraints essentially specifies a 'ι,p mixed-norm ball constraint.
Therefore, we can still solve this new optimization problem with projected gradient descent. We
extend the recent advances (LiU & Ye, 2010; Bejar et al., 2019) to solve this new problem for the
cases of P = 2 and P = ∞ respectively (details in the appendix B).
0.46-
4 2 0
φ 4 4
0.0.0.
^02⊃00< ISnqoH
8
3
6
0.36-
1	2	3	4	5
K Rounds
MNIST-Ip . ClFAR10-lp ♦ MNlST-Pixel+印atial ♦ CIFAR10-pixel+spatial
Figure 5: Robust accuracy versus the number of rounds K on MNIST and CIFAR10 under composite adver-
sarial attacks.
Results - Figure 5 displays how K -round composite attack impacts the robust accuracy of CAT
under the settings of`p perturbations only as well as pixel plus spatial perturbations. As K increases,
the robust accuracy decreases by 4% and 7% respectively on MNIST and CIFAR10 under the setting
8
Under review as a conference paper at ICLR 2021
of pixel plus spatial perturbations, while the decrease is much less evident under the setting of `p
perturbations only, indicating that CAT is fairly robust to K-round composite attacks. Moreover, by
incorporating K-round composite attacks in the adversarial training of CAT, we expect to see further
robustness improvement.
6	Conclusion
While effective against individual perturbation models, existing adversarial defenses often fail to
defend against combinations of multiple perturbations. In this paper, we first present a new class
of composite attacks that combine multiple perturbations and penetrate the state-of-the-art defenses.
We then propose composite adversarial training (Cat), a novel training method that improves DNNs
robustness not only against individual perturbations but also against their compositions. Empirical
evaluation on benchmark datasets and models shows its promising performance.
References
Rima Alaifari, Giovanni S. Alberti, and Tandri Gauksson. ADef: an iterative algorithm to construct adversarial
deformations. In Proceedings of International Conference on Learning Representations (ICLR), 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated Gradients Give a False Sense of Security:
Circumventing Defenses to Adversarial Examples. In Proceedings of IEEE Conference on Machine Learning
(ICML), 2018.
Benjamin Bejar, Ivan Dokmanic, and Rene Vidal. The fastest 'ι,∞ Prox in the west, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017a.
Nicholas Carlini and David A. Wagner. Towards Evaluating the Robustness of Neural Networks. In Proceedings
of IEEE Symposium on Security and Privacy (S&P), 2017b.
Gustavo. Chau, Brendt. Wohlberg, and Paul. Rodriguez. Efficient projection onto the '∞,ι mixed-norm ball
using a newton root search method. SIAM Journal on Imaging Sciences, 12(1):604-623, 2019.
L. Engstrom, B. Tran, D. Tsipras, L. Schmidt, and A. Madry. A Rotation and a Translation Suffice: Fooling
CNNs with Simple Transformations. ArXiv e-prints, 2017.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and Harnessing Adversarial Examples.
In Proceedings of International Conference on Learning Representations (ICLR), 2015a.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In
International Conference on Learning Representations, 2015b. URL http://arxiv.org/abs/1412.
6572.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering Adversarial Images
Using Input Transformations. In Proceedings of International Conference on Learning Representations
(ICLR), 2018.
C. Gustavo, B. Wohlberg, and P. Rodriguez. Fast projection onto the '∞,ι-mixed norm ball using steffensen
root search. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 4694-4698, April 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. Spatial transformer networks. In
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information
Processing Systems, pp. 2017-2025. Curran Associates, Inc., 2015. URL http://papers.nips.cc/
paper/5854-spatial-transformer-networks.pdf.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of Interna-
tional Conference on Learning Representations (ICLR), 2015.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial Machine Learning at Scale. In Proceedings
of International Conference on Learning Representations (ICLR), 2017.
9
Under review as a conference paper at ICLR 2021
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against Adver-
sarial Attacks Using High-Level Representation Guided Denoiser. In Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2018.
Jun Liu and Jieping Ye. Efficient l1/lq norm regularization. ArXiv e-prints, abs/1009.4766, 2010.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards Deep
Learning Models Resistant to Adversarial Attacks. In Proceedings of International Conference on Learning
Representations (ICLR), 2018.
Pratyush Maini, Eric Wong, and J Zico Kolter. Adversarial robustness against the union of multiple perturbation
models. In Proceedings of IEEE Conference on Machine Learning (ICML), 2020.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate
method to fool deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition,
2016.
Ariadna Quattoni, Xavier Carreras, Michael Collins, and Trevor Darrell. An efficient projection for 'ι,∞
regularization. In Proceedings of the International Conference on Machine Learning, pp. 857-864. ACM,
2009.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the robust-
ness of machine learning models. In Reliable Machine Learning in the Wild Workshop, 34th International
Conference on Machine Learning, 2017. URL http://arxiv.org/abs/1707.04131.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust
neural network model on MNIST. In Proceedings of International Conference on Learning Representations
(ICLR), 2019.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for
machine comprehension. In International Conference on Learning Representations, 2017.
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Advances in Neural Information
Processing Systems, pp. 3353-3364, 2019.
Suvrit Sra. Fast projections onto mixed-norm balls with applications. Data Mining and Knowledge Discovery,
25(2):358-377, 2012.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations,
2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the incep-
tion architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition, pp.
2818-2826, 2016.
F. Tramer, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel. Ensemble Adversarial Train-
ing: Attacks and Defenses. In Proceedings of International Conference on Learning Representations (ICLR),
2018.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In Proceedings
of Advances in Neural Information Processing Systems (NeurIPS), 2019.
C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially Transformed Adversarial Examples. In
Proceedings of International Conference on Learning Representations (ICLR), 2018.
A Proofs
We assume a set of component models {Ai}im=1, each Ai bounded by budget i.
Lemma 1. The union of {Ai}im=1 is stronger than each individual attack.
10
Under review as a conference paper at ICLR 2021
Proof. (Lemma 1) The adversarial loss of a given input (χ, y) with respect to Ai is defined as:
Li(X) = slTX )'(f(χ + δ),y)	(8)
where Bi (e) is the feasible set for Ai with bound ei. Meanwhile, the adversarial loss with respect to
the union of {Ai}m=ι is defined as:
Lu(X)= max max '(f(x + δ),y) ≥ Li(x)	(9)
i δ∈Bi(ei)
Given that Ai is arbitrarily chosen, the union attack is stronger than each component attack. □
Therefore, we only need to show that the composite attack is stronger than the union attack. We
prove this by showing that the feasible set for the composite attack is larger than the union attack. To
simplify the discussion, we consider two 'p attacks Ai and A∞ with budget eι and e∞ respectively,
and d is the data dimensionality.
Lemma 2. If d(ei + e∞)d∕2d 一 2(ei — e∞)d > ed∕2d + de∞, the composition of Ai and A∞ is
strictly stronger than their union.
Figure 6: Comparison of union and composition for d = 2.
Proof. (Lemma 2) First, it can be verified that with the constraint, e∞ < ei < de∞, thus the d-
dimensional hypercubes represented by ei and e∞ intersect but none of them completely contains
the other. Figure 6 shows the case for d = 2.
The feasible set Bu for the union attack is given by:
Bu = Bi(eι) ∪B∞(e∞)	(10)
where Bp(ep) denotes the feasible set for Ap (P = 1, ∞). The volume of Bu, Vol(Bu) is given by:
2d+i
Vol(Bu) = 2de∞ +-----ʒ-(ei - e∞)d	(II)
d
In comparison, the volume of the feasible set Bc for the composite attack (with budget e∖∕2 and
e∞∕2 for Ai and A∞) is given by:
Vol(Bc) = (ei + e∞)d 一 ed∕d	(12)
Given the constraint, it is trivial to see Vol(Bc) > Vol(Bu), indicating that the composite attack
entails a larger perturbation space than the union attack.	□
This result can be generalized to the cases of other 'p attacks and more than two component attacks.
B Experiment Settings
We present the detailed setting of Cat, baseline methods, as well as the attacks used in § 4.
11
Under review as a conference paper at ICLR 2021
Case MNIST-'p CIFAR10-'p MNIXT-PixelandsPatial CIFAR10-PixelandsPatial
-α^^	0.8	0.8^^^0	0:7
Table 8. The rescale factor α for CAT models Presented in the PaPer.
B.1	Model Training
We reuse a few Pre-trained models from rePository Provided by Previous work. For the rest of
models, their training use the following setuPs for each dataset.
•	MNIST. The models are oPtimized with an Adam oPtimizer (Kingma & Ba, 2015). The
learning rate linearly increases from 0 to 0.001 in the first 6 ePochs, and then linearly
decreases to 0 in the last 9 ePochs.
•	CIFAR10. The models are oPtimized by a SGD oPtimizer with momentum of 0.9. The
learning rate linearly increases from 0 to 0.1 in the first 20 ePochs, then linearly decrease
to 0.005 in the next 20 ePochs, and it linearly decay to 0 in the last 10 ePochs. Besides, we
regularize models with a `2 weight decay of 5 × 10-4.
B.2	CAT
•	`p cases. We run the comPosite attack during CAT model training with steP sizes of 0.1 × p
for each p ∈ {1, 2, ∞}. The number of iterations for attacks is 50.
•	pixel and spatial perturbations. We run the comPosite attack during CAT model training
with steP sizes of 0.1 × p and 0.1 × f for Pixel and sPatial Perturbation. The number of
iterations for attacks is 40.
We also Present the re-scaling factor α for the models Presented in the main text in Table 8.
B.3	Adversarial Attacks
In evaluation, we run all the attacks excePt C&W with 5 random restarts. Below we summarize all
the attacks we used in the evaluation and their hyPer-Parameters.
•	Attacks from Foolbox. We Use the following attacks from Foolbox 3.1.11: '∞ PGD, '2
PGD, `1 PGD, Fast Gradient Sign Method, DeePFool, C&W, Salt&PePPer. We take their
defaUlt settings from Foolbox in the evalUation.
•	Our Implementation of `p PGD Attacks. PlUs, we also imPlement PGD attacks for three
`p norms oUrselves, which achieves higher attack sUccces rate than the version from the
Foolbox. For '∞ PGD, we run 200 iterations with a step size of 0.1 X e. For '2 PGD, we
rUn 500 iterations with a steP size of 0.05 × . OUr imPlementation of `1 PGD attack is
based uPon Section A.1 of (Maini et al., 2020), where we set the the range of number of
Pixels to modify in each iteration [k1, k2] as: k1 = 5 and k2 = 20. The number of iterations
of this attack is 200, and the steP size is set to 0.05 × .
•	The PGD Spatial Perturbation Attack. We run this attack with 200 iterations and steP
size of 0.1 × f.
•	Composite Adversarial Attacks. We run this attack with 200 iterations and steP sizes of
0.1 × for all the comPonent attacks.
B.4	Baselines
•	'p Perturbations. We use Pre-trained robust models Provided by MSD (Maini et al., 2020)2
for all the baseline methods, including P∞, P2, P1, AVG, MAX, and MSD.
•	Pixel and Spatial Perturbations. The adversarial trained models with '∞ PGD is the
same as in the 'p Perturbations case. For Pflow, the number of iterations for sPatial attack
is 50, and the steP size is set to 0.1 × f. The same rule aPPlies to AVG and MAX on the
both two attacks.
1https://github.com/bethgelab/foolbox
2https://github.com/locuslab/robust_union
12
Under review as a conference paper at ICLR 2021
C K-ROUND COMPOSITE ADVERSARIAL ATTACK
We describe the detailed formulation for the multiple round composite adversarial attack and tech-
nical tools to solve this new attack.
C.1 Formulation
We denote m component attacks as A1, . . . , Am, and whose perturbation sizes are 1, . . . , m re-
spectively. Unlike the definition in § 2, here we represent each attack Ai as Ai(x, δi), which denotes
applying perturbation δi onto x with the mechanism ofAi. The K-round attack generalizes compos-
ite adversarial attack in the following sense, the attack runs in K-round. At k-th round, it performs
a composite attack with perturbations δk,1, . . . , δk,m. The constraint for adversary is the overall
magnitude of perturbations he spent for each attack. Formally,
(δ;,..., δm)argmin '(xκ, y)
δi
{X0 = X
xk = Am (. . . A2 (A1 (xk-1, δk,1), δk,2) . . . , δk,m)	k = 1, . . . , K	(13)
PkK=1 kδi,kk ≤ i i = 1, . . . ,m
where xk is the perturbed sample after the k-th round and δi = (δi,1, . . . , δi,K) is the concatenation
of the perturbation at each iteration. Observe that by specifying K, Eqn 13 instantiates a spectrum
of attacks. As K approaches infinity, Eqn 13 essentially considers all finite combinations of ways of
allocating the total budgets {i}im=1 using the m perturbation mechanisms.
C.2 Derivation
Now we present an iterative projected gradient descent algorithm to find a solution to Eqn (13). We
use the superscript (t) to denote the value of related variables at t-th iteration. We randomly initialize
δi(0). At the t-th iteration, the update rule is defined as:
〃⑴ ʃ d'(xK),y)
i	∂δ(t)
δ(t+1) J 口但：PK=Ikδi,kk≤ei}(δ(" - ag(t))
where α is the learning rate and ∏s(∙) is the projection operator of a convex set S. The computation
of Eqn (14) is straightforward. We focus our discussion on the projection operator in Eqn (15).
Without loss of generality, we omit the subscript i for simplicity. Let V be a matrix with its k-th
row as δk (for k = 1, . . . , K). Then the summation PkK=1 kδk k ≤ can be rewritten as PkK=1 kvkk.
Additional, We suppose the perturbation of i-th component attack is measured with `p norm for
some p ∈ {1, 2, ∞}, which hold for all the experiments in the paper. Thus, we reach to the `p,1
mixed-norm of V, denoted as ∣∣ V∣∣p,ι, which is a special case of 'p,q mixed-norm ball: ∣∣ V∣∣p,q =
(Pim=1 kvikqp)1/q. Hence, we cast Eqn (15) as the projection operator onto an `p,1 mixed-norm ball.
For p = 1, the calculation is straightforward since it reduces to the `1 projection operator for the
concatenated vectors. We work with p = 2 and p = ∞ in the next.
(14)
(15)
C.3 IMPLEMENTATION FOR p = 2
We leverage Algorithm 1 in (Sra, 2012) to compute the '2,∞ mixed-norm of an input x. Interested
readers could find more details in (Sra, 2012).
C.4 IMPLEMENTATION FOR p = ∞
We leverage the method proposed in (Bejar et al., 2019), which solves the proximal operator of 'ι,∞
mixed-norm using an active set approach and attains better efficiency than previous methods (Quat-
toni et al., 2009; Gustavo et al., 2018; Chau et al., 2019). However, as this is a primitive procedure
13
Under review as a conference paper at ICLR 2021
in Cat, which is executed for hundreds of iterations, the basic implementation in (Bejar et al., 2019)
is not scalable for our setting.
We now improve the scalability of (Bejar et al., 2019) for Cat, with their original algorithm sketched
in Algorithm 1. The method is based on computing the proximal operator of the dual form of '∞,ι
mixed-norm ball, '∞,ι mixed-norm. We define U = abs(V). Given an initial radius of 'ι ball for its
row ui, denoted by r, it iteratively improves r by first projecting each row with '「norm exceeding
r to the `1 ball of radius r (line 4 to 8) and then computing a larger r based on the nonzero elements
of the projected row (line 9). In particular, Algorithm 1 uses sorting to solve the `1 -norm projection
problem, which is detailed in Algorithm 2. We state our more efficient implementation based on two
key observations here.
Observation 1 - The projection radius r (line 6) in Algorithm 1 for related rows increases at every
iteration. Thus, for each related row v, we face a set of queries with increasing radii ri ≤ ∙∙∙ ≤ rτ,
where T is the number of queries. Following the notations of Algorithm 2, let u denote the sorted
v in non-decreasing order, and we define hk = Pjk=1 uj - kuk . Note that h is monotonically
increasing:
hk+1 - hk = uk+1 - (k + 1)uk+1 + kuk = k(uk - uk+1) ≥ 0
Leveraging this observation, we optimize Algorithm 2 as follows. At line 2, K is the largest item
among 1, . . . , N such that r > hK; we only need to scan hk one pass to find the optimal Kt for all
at due to that both h and r are increasing. At line 3, we only need to pre-process the partial sums
once for each row before the main loop.
Observation 2 - To update the radius r of the `1 -norm ball in Algorithm 1, we need to access
the statistics of line 7 〜9. A critical observation is that it is not necessary to explicitly compute
the projected vector at every iteration to compute the statistics. Specifically, line 7 computes the
number of non-zero elements Ji for the projected i-th row; and line 9 updates r using the sums of
the projected rows Pjn=1 xi,j and Ji. Combining the previous observation, if Ki,t is the optimal K
(line 2 of Algorithm 2) for the t-th radius rt and the i-th row, it holds that
|Ji | = Kt,i and	ui,j =	(xi,j + τt,i) = rt + Ki,tτi,t
j∈Ji	{j:xi,j >0}
1
2
3
4
5
6
7
8
9
10
11
where τi,t is calculated based on line 3 of Algorithm 2 for i-th row with radius rt . Thus, we can
avoid computing the intermediate `1 projections.
Algorithm 1: Proximal operator of mixed 'ι,∞ norm: proxλ∣∣∙kι (∙) Bejar et al. (2019)
Input: m × n matrix V
Output: X
U - abs (V);
r — initial radius computed via Lemma 2 and Lemma 3 in Bejar et al. (2019);
do
M -{i∣r< Iluikι};
foreach i in M do
I Xi -- π∣H∣l≤r (Ui) ;
LJi- {jιxi,j >o};
Pi∈M j Pj∈j ui,j-λ.
/ L P -ɪ-	;
乙i∈M JT
while M or {Ji }im=1 change;
for i - 1 ,...,m do μi — max ( "jJ；i,j r, 0);
X — Sgn(V) Θ max (U — λμ1τ, 0);
D Additional Results
D.1 Decision-based Attacks on MNIST
Table 9 summarizes the results of all the models in Table 2 on two decision-based attacks: `2 -
Pointwise attack and 'i-Pointwise attack (Schott et al., 2019). One may notice that the inferior
performance of P∞ model on these black-box attacks compared to its performance in Table 2 for
white-box attacks. In summary, we find strong gradient masking effects within this model.
14
Under review as a conference paper at ICLR 2021
Algorithm 2: Algorithm for projection v ∈ RN onto simplex PnN=1 xn = r, and xn ≥ 0 for
n = 1,…,N.
Input: y ∈ RN
Output: x
ι U — sort V in non-increasing order: uι ≥ ∙∙∙ ≥ UN ;
2	K - maxι≤k≤N{k∣(Pk=ι Uj- r)/k < Uk}；
3	T 一(PK=I Uk-r)/K ；
4	for n — 1,…，N do
5	Lxn J max(vn - τ, 0);
	P ∞ ∞	P2	Pi	MAX	AVG	MSD	CAT
Pointwise ('2)	68.9%	97.3%	98.4%	94.0%	92.9%	95.3%	87.5%
Pointwise (`1 )	22.5%	92.7%	93.9%	83.4%	73.0%	86.6%	82.0%
Table 9. Robust Accuracy to decision-based attacks for models trained with Cat and baseline methods on
MNIST dataset.
D.2 Visualization of Composite Adversarial Examples on MNIST
Figure 7 complements Figure 1 with adversarial examples against a natural model for MNIST
dataset.
D.3 Preliminary Results for CIFAR 100 Datasets
Table 10 and Table 11 present performances of baseline models and Cat for CIFAR100 dataset.
Similar to previous results, with slightly reduction in clean accuracy, Cat outperforms MAX and
AVG as well as simple PGD adversarial trained models. The MSD is excluded here temporally due
to it takes extreme long time to train. We will fill MSD’s result when it is available.
D.4 Computational Efficiency of Cat
We describe the computational cost of MAX, AVG, and Cat here. MSD is excluded here due to
it requires way more time, as the official implementation. We consider filling its result with our
implementation later. All the experiments are with CIFAR10 dataset and of `p settings. The batch
size of all the experiments is 50, and the total number of epochs is 50. The result is in Table 12.
15
Under review as a conference paper at ICLR 2021
		P∞	P2	P1	MAX	AVG	Cat
clean accuracy		49.0%	70.7%	64.5%	56.7%	62.8%	53.3%
'∞ attacks (e =	0.03)	25.9%	2.8%	0.0%	23.6%	17.8%	22.6%
`2 attacks ( =	0.5)	34.5%	25.6%	0.0%	36.8%	38.7%	38.6%
`1 attacks (	12)	9.6%	5.0%	0.0%	29.6%	36.7%	37.6%
Union		9.2%	2.4%	0.0%	21.8%	17.7%	22.3%
Composite (0.5)		8.3%	1.6%	0.0%	19.3%	15.8%	22.7%
Table 10. Performances of CAT and baselines on CIFAR100 with `p perturbations (p = 1, 2, ∞). Rows
represent attacks, and columns denote robust trained models. P∞, P2, and P1 are models adversarially trained
with PGD attacks (with corresponding norms).
	PpiXel	Pflow	MAX	AVG	Cat
clean accuracy	57.7%	61.4%	55.1%	57.0%	54.0%
pixel attack ( = 0.03)	26.1%	5.0%	23.6%	22.0%	23.7%
flow attack ( = 0.35)	23.2%	34.8%	29.1%	29.9%	32.0%
Union	19.5%	5.0%	22.3%	20.9%	22.9%
Composite (0.5)	24.1%	3.4%	25.2%	25.1%	27.2%
Table 11. Performance of Cat and baselines on CIFAR100 with respect to pixel and spatial perturbations.
Rows represent attacks, and columns denote robust trained models. Pflow and Ppixel are models trained with
PGD attacks (with corresponding perturbation spaces).
Method	Time per epoch (minute)
MAX	63.2
AVG	63.5
MSD	33.3
Cat	247
Table 12. Computational Cost of Cat and baseline methods.
16