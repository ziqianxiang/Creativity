Under review as a conference paper at ICLR 2021
Connection- and Node-Sparse Deep Learning:
Statistical Guarantees
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks are becoming increasingly popular in applications, but a com-
prehensive mathematical understanding of their potentials and limitations is still
missing. In this paper, we study the prediction accuracies of neural networks from
a statistical point of view. In particular, we establish statistical guarantees for
deep learning with different types of sparsity-inducing regularization. Our bounds
feature a mild dependence on network widths and depths, and, therefore, support
the current trend toward wide and deep networks. The tools that we use in our
derivations are uncommon in deep learning and, hence, might be of additional
interest.
1	Introduction
Sparsity reduces network complexities and, consequently, lowers the demands on memory and
computation, reduces overfitting, and improves interpretability (Changpinyo et al., 2017; Han et al.,
2016; Kim et al., 2016; Liu et al., 2015; Wen et al., 2016). Three common notions of sparsity are
connection sparsity, which means that there is only a small number of nonzero connections between
nodes, node sparsity, which means that there is only a small number of active nodes (Alvarez &
Salzmann, 2016; Changpinyo et al., 2017; Feng & Simon, 2017; Kim et al., 2016; Lee et al., 2008;
Liu et al., 2015; Nie et al., 2015; Scardapane et al., 2017; Wen et al., 2016), and layer sparsity, which
means that there is only a small number of active layers (Hebiri & Lederer, 2020). Approaches
to achieving sparsity include augmenting small networks (Ash, 1989; Bello, 1992), pruning large
networks (Simonyan & Zisserman, 2015; Han et al., 2016), constraint estimation (Ledent et al., 2019;
Neyshabur et al., 2015; Schmidt-Hieber, 2020), and statistical regularization (Taheri et al., 2020).
The many empirical observations of the benefits of sparsity have sparked interest in mathematical
support in the form of statistical theories. But such theories are still scarce and, in any case, have
severe limitations. For example, statistical guarantees for deep learning with connection-sparse
regularization have been established in Taheri et al. (2020), but they do not cover node sparsity, which,
in view of the removal of entire nodes, has become especially popular. Moreover, their estimator
involves an additional parameter, their theory is limited to a single output node, and their results
have a suboptimal dependence on the input vectors. Statistical guarantees for constraint estimation
over connection- and node-sparse networks follow from combining results in Neyshabur et al.
(2015) and Bartlett & Mendelson (2002). But for computational and practical reasons, regularized
estimation is typically preferred over constraint estimation in deep learning as well as in machine
learning at large (Hastie et al., 2015). Moreover, their theory is limited to a single output node and
ReLU activation, scales exponentially in the number of layers, and requires bounded loss functions.
Statistical prediction guarantees for constraint estimation over connection-sparse networks have been
derived in Schmidt-Hieber (2020), but their theory is limited to a single output node and ReLU
activation and assumes bounded weights. In short, the existing statistical theory for deep learning
with connection and node sparsity is still deficient.
The goal of this paper is to provide an improved theory for sparse deep learning. We focus on
regression-type settings with layered, feedforward neural networks. The estimators under considera-
tion consist of a standard least-squares estimator with additional regularizers that induce connection
or node sparsity. We then derive our guarantees by using techniques from high-dimensional statis-
tics (Dalalyan et al., 2017) and empirical process theory (van de Geer, 2000). In the case of
1
Under review as a conference paper at ICLR 2021
jl(lθg[mnpD3	and j
subgaussian noise, we find the rates
mlp(log[mnp])3
n
for the connection-sparse and node-sparse estimators, respectively, where l is the number of hidden
layers, m the number of output nodes, n the number of samples, P the total number of parameters,
and p the maximal width of the network. The rates suggest that sparsity-inducing approaches can
provide accurate prediction even in very wide (with connection sparsity) and very deep (with either
type of sparsity) networks while, at the same time, ensuring low network complexities. These findings
underpin the current trend toward sparse but wide and especially deep networks from a statistical
perspective.
Outline of the paper Section 2 recapitulates the notions of connection and node sparsity and
introduces the corresponding deep learning framework and estimators. Section 3 confirms the
empirically-observed accuracies of connection- and node-sparse estimation in theory. Section 4
summarizes the key features and limitations of our work. The Appendix contains all proofs.
2	Connection- and Node-Sparse Deep Learning
We consider data (y1, x1), . . . , (yn, xn) ∈ Rm × Rd that are related via
yi = g*[xi ] + Ui	for i ∈ {1,...,n}	(1)
for an unknown data-generating function g* : Rd → Rm and unknown, random noise uι,..., Un ∈
Rm. We allow all aspects, namely y%, g*, Xi, and Ui, to be unbounded. Our goal is to model the
data-generating function with a feedforward neural network of the form
gθ[x] ∙=Θlfl [Θlτ.∙∙ f1 [Θ0x]]	for x ∈ Rd	(2)
indexed by the parameter space M ∙= {Θ = (Θl,..., Θ0) : Θj ∈ Rpj+1×pj}. The functions
fj : Rpj → Rpj are called the activation functions, and p0 ∙= d and pl+1 ∙= m are called the
input and output dimensions, respectively. The depth of the network is l, the maximal width is
p ∙= maxj∈{o . ι-i} pj+1, and the total number of parameters is p ∙= Pj=0 pj+1pj.
In practice, the total number of parameters often rivals or exceeds the number of samples: P ≈ n
or p》n. We then speak of high dimensionality. A common technique for avoiding overfitting
in high-dimensional settings is regularization that induces additional structures, such as sparsity.
Sparsity has the interesting side-effect of reducing the networks’ complexities, which can facilitate
interpretations and reduce demands on energy and memory. Our first sparse estimator is
Θb con
∈ arg min
Θ∈M1
∣∣yi - gθ[χi]∣∣2 + rcon∣∣∣θl∣∣∣ι
(3)
)
for a tuning parameter rcon ∈ [0, ∞), a nonempty set of parameters
M1 ⊂ nΘ ∈ M :	max	∣∣∣Θj∣∣∣1 ≤ 1o ,
j∈{0,...,l-1}
and the `1 -norm
pj+1 pj
ll∣Θj∣∣∣ι ∙= XX∣(Θj)ik∣	for j ∈{0,...,l}, Θj ∈ Rpj+1×pj.
i=1 k=1
This estimator is an analog of the lasso estimator in linear regression (Tibshirani, 1996). It induces
sparsity on the level of connections: the larger the tuning parameter rcon, the fewer connections
among the nodes.
Deep learning with 'ι-regularization has become common in theory and practice (Kim et al., 2016;
Taheri et al., 2020). Our estimator (3) specifies one way to formulate this type of regularization. The
estimator is indeed a regularized estimator (rather than a constraint estimator), because the complexity
2
Under review as a conference paper at ICLR 2021
Estimator (3)
input x
Estimator (6)
input x
output y
Figure 1: exemplary networks produced by the connection-sparse estimator (3) and the node-sparse
estimator (6)
is regulated entirely through the tuning parameter rcon in the objective function (rather than through
a tuning parameter in the set over which the objective function is optimized). But '1-regularization
could also be formulated slightly differently. For example, one could consider the estimators
Θcon ∈ arg min
Θ∈M
yi -gθ[χi]∣∣2 +『con Y l∣θj∣∣ι]
j=0
(4)
or
〜
Θ
con
∈ arg min
Θ∈M
yi - gΘ[xi]22 + rcon X ∣∣∣Θj∣∣∣1	.
j=0
(5)
The differences among the estimators (3)-(5) are small: for example, our theory can be adjusted
for (4) with almost no changes of the derivations. The differences among the estimators mainly
concern the normalizations of the parameters; we illustrate this in the following proposition.
Proposition 1 (Scaling of Norms). Assume that the all-zeros parameter (0pl+1 ×pl , . . . , 0p1 ×p0) ∈
M1 is neither a solution of (3) nor of (5), that rcon > 0, and that the activation functions are
nonnegative homogenous: fj [ab] = afj [b] for all j ∈ {1, . . . , l}, a ∈ [0, ∞), and b ∈ Rpj.
Then, ∣∣∣(Θcon)0∣∣∣1, . . . , ∣∣∣(Θcon)l-1∣∣∣1 = 1 (concerns the inner layers) for all solutions of (3), while
∣∣(Θcon)0∣∣1 =…=∣∣(Θcon)l∣∣ι (concerns all layers)for at least one solution of (5).
Another way to formulate `1 -regularization was proposed in Taheri et al. (2020): they reparametrize
the networks through a scale parameter and a constraint version of M and then to focus the regular-
ization on the scale parameter only. Our above-stated estimator (3) is more elegant in that it avoids
the reparametrization and the additional parameter.
The factor ∣∣∣Θl ∣∣∣1 in the regularization term of (3) measures the complexity of the network over the
set M1 , and the factor rcon regulates the complexity of the resulting estimator. This provides a
convenient lever for data-adaptive complexity regularization through well-established calibration
schemes for the tuning parameter, such as cross-validation. This practical aspect is an advantage of
regularized formulations like ours as compared to constraint estimation over sets with a predefined
complexity.
The constraints in the set M1 of the estimator (3) can also retain the expressiveness of the full param-
eterization that corresponds to the set M: for example, assuming again nonnegative-homogeneous ac-
tivation, one can check that for every Γ ∈ M, there is a Γ0 ∈ {Θ ∈ M : maxj∈{0,...,l-1} ∣∣∣Θj ∣∣∣1 ≤
1} such that gΓ = gΓ0 —cf. Taheri et al. (2020, Proposition 1). In contrast, existing theories on neural
networks often require the parameter space to be bounded, which limits the expressiveness of the
networks.
Our regularization approach is, therefore, closer to practical setups than constraint approaches. The
price is that to develop prediction theories, we have to use different tools than those typically used
in theoretical deep learning. For example, we cannot use established risk bounds such as Bartlett
& Mendelson (2002, Theorem 8) (because Rademacher complexities over classes of unbounded
functions are unbounded) or Lederer (2020a, Theorem 1) (because our loss function is not Lipschitz
continuous) or established concentration bounds such as McDiarmid’s inequality in McDiarmid
(1989, Lemma (3.3)) (because that would require a bounded loss). We instead invoke ideas from
high-dimensional statistics, prove Lipschitz properties for neural networks, and use empirical process
theory that is based on chaining (see the Appendix).
3
Under review as a conference paper at ICLR 2021
Our second estimator is
Θb node ∈ arg min
Θ∈M2,1
yi - gΘ [xi] Il 2 + rnode Nθ' ||2,1 }
(6)
for a tuning parameter rnode ∈ [0, ∞), a nonempty set of parameters
M2,1 ⊂ ]θ ∈ M :	max	∣∣∣Θjg,ι ≤ ” ,
j∈{0,...,l-1}
and the '2∕'1-norm
pj	u
忡 MI = Xt
k=1
pj+1
X ∣(Θj )ik I2	for j ∈{0,...,l - 1}, Θj ∈ Rpj+1 ×pj .
i=1
This estimator is an analog of the group-lasso estimator in linear regression (Bakin, 1999). Again,
to avoid ambiguities in the regularization, our formulation is slightly different from the standard
formulations in the literature, but the fact that group-lasso regularizers leads to node-sparse networks
has been discussed extensively before (Alvarez & Salzmann, 2016; Liu et al., 2015; Scardapane et al.,
2017): the larger the tuning parameter rnode , the fewer active nodes in the network.
The above-stated comments about the specific form of the connection-sparse estimator also apply to
the node-sparse estimator.
An illustration of connection and node sparsity is given in Figure 1. Connection-sparse networks
have only a small number of active connections between nodes (left panel of Figure 1); node-sparse
networks have inactive nodes, that is, completely unconnected nodes (right panel of Figure 1). The
two notions of sparsity are connected: for example, connection sparsity can render entire nodes
inactive “by accident” (see the layer that follows the input layer in the left panel of the figure). In
general, node sparsity is the weaker assumption, because it allows for highly connected nodes; this
observation is reflected in the theoretical guarantees in the following section.
The optimal network architecture for given data (such as the optimal width) is hardly known before-
hand in a data analysis. A main feature of sparsity-inducing regularization is, therefore, that it adjusts
parts of the network architecture to the data. In other words, sparsity-inducing regularization is a
data-driven approach to adapting the complexity of the network.
While versions of the estimators (3) and (6) are popular in deep learning, statistical analyses, especially
of node-sparse deep learning, are scarce. Such a statistical analysis is, therefore, the goal of the
following section.
3 Statistical Prediction Guarantees
We now develop statistical guarantees for the sparse estimators described above. The guarantees are
formulated in terms of the squared average (in-sample) prediction error
1n
err[Θ] ∙=/£ ∣∣g*[xi] - gθ[xi]∣∣2	for Θ ∈ M ,
i=1
which is a measure for how well the network gθ fits the unknown function g* (which does not need
to be a neural network) on the data at hand, and in terms of the prediction risk (or generalization
error) for a new sample (y, x) that has the same distribution as the original data
risk[Θ] ∙= E∣y - gθ[x]∣∣2	for Θ ∈ M ,
which measures how well the network gΘ can predict a new sample. We first study the prediction
error, because it is agnostic to the distribution of the input data; in the end, we then translate the
bounds for the prediction error into bounds for the generalization error.
We first observe that the networks in (2) can be somewhat “linearized:” For every parameter Θ ∈ M1,
there is a parameter
Θ ∈Mι ∙= ∣Θ = (Θl-1,..., Θ0) : Θj ∈ Rpj+1×pj, . max PiI ≤ l}
j∈{0,...,l-1}
4
Under review as a conference paper at ICLR 2021
such that for every x ∈ Rd
gθ[x] = Θlgθ[x] with 9& [x] ∙= f l [θl-1 ∙ ∙ ∙ f 1[Θ0x]] ∈ Rpl .	(7)
This additional notation allows us to disentangle the outermost layer (which is regularized directly)
from the other layers (which are regularized indirectly). More generally speaking, the additional
notation makes a connection to linear regression, where the above holds trivially with g0[x] = x.
We also define
M2 1	∙=	(θ =(θlτ,..∙, θ0)	:	θj	∈	Rpj+1×pj,	max	∣∣θj∣∣2 1 ≤ ]}
,	j∈{0,...,l-1}	,
accordingly.
In high-dimensional linear regression, the quantity central to prediction guarantees is the effective
noise (Lederer & Vogt, 2020). The effective noise is in our notation (with l = 0 and m = 1 to
describe linear regression) 2∣∣ Pin=1 uixi∣∣∞. The above linearization allows us to generalize the
effective noise to our general deep-learning framework:
Won ：= 2_sup
Ψ∈M1
n
Eui (gψ[χi])
i=1	∞
rnode ∙= 2Vm_sup
Ψ∈M2,1
n
Eui (gψ[χi])
i=1
(8)
∞
where ∣∣∣A∣∣∣∞ ∙= max(i j)∈{i m}×{i pl}∣Aj ∣ for A ∈ Rm×pl. The effective noises, as we will
see below, are the optimal tuning parameters in our theories; at the same time, the effective noises
depend on the noise random variables u1 , . . . , un, which are unknown in practice. Accordingly, we
call the quantities rcnon and rnnode the oracle tuning parameters.
We take a moment to compare the effective noises in (8) to Rademacher complexities (Koltchinskii,
2001; Koltchinskii & Panchenko, 2002). Rademacher complexities are the basis of a line of other
statistical theories for deep learning (Bartlett & Mendelson, 2002; Golowich et al., 2017; Lederer,
2020a; Neyshabur et al., 2015). In our framework, the Rademacher complexities in the case m = 1
are (Lederer, 2020a, Definition 1)
Ex1,...,xn,k1,...,kn
1 n
sup I- Tkigθ[xi]∣
Θ∈M1 n i=1
and	Ex1,...,xn,k1,...,kn
I1 n	I
sup I- Tkigθ[xi]l
Θ∈M2,1 n i=1
for i.i.d. Rademacher random variables k1, . . . , kn. The effective noises might look like (rescaled)
empirical versions of these quantities at first sight, but this is not the case. Two immediate differences
are that (8) apply to general m and circumvent the outermost layers of the networks. But more
importantly, Rademacher complexities involve external i.i.d. Rademacher random variables that
are not connected with the statistical model at hand, while the effective noises involve the noise
variables, which are completely specified by the model and, therefore, can have any distribution
(see our sub-Gaussian example further below). Hence, there are no general techniques to relate
Rademacher complexities and effective noises.
Not only are the two concepts distinct, but also they are used in very different ways. For example,
existing theories use Rademacher complexities to measure the size of the function class at hand,
while we use effective noises to measure the maximal impact of the stochastic noise on the estimators.
(Our proofs also require a measure of the size of the function class, but this measure is entropy—
cf. Lemma 1.) In general, our proof techniques are very different from those in the context of
Rademacher complexities.
We can now state a general prediction guarantee.
Theorem 1 (General Prediction Guarantees). If rcon ≥ rcnon, it holds that
err[Θcon] ≤ inf (err[Θ] + 2rcon∣∣Θl∣∣ιO .
Θ∈M1	n
Similarly, if rnode ≥ rnnode, it holds that
err[Θnode] ≤	inf (err[Θ] + 2rnode ∣∣θl ∣∣2,1}.
Θ∈M2,1	n
5
Under review as a conference paper at ICLR 2021
Each bound contains an approximation error err[Θ] that captures how well the class of networks
can approximate the true data-generating function g* and a statistical error proportional to rcon/n
and rnode/n, respectively, that captures how well the estimator can select within the class of networks
at hand. In other words, Theorem 1 ensures that the estimators (3) and (6) predict—up to the statistical
error described by rcon/n and rnode/n, respectively—as well as the best connection- and node-sparse
network. This observation can be illustrated further:
Corollary 1 (Parametric Setting). Ifadditionally g* = gθ* for a Θ* ∈ Mι,it holds that
err[Θ赳]≤ 2rcon∣∣∣(Θ*)l∣∣∣ι.
n
Ifinstead g* = gθ* for a Θ* ∈ M2,1, it holds that
err[Θnode] ≤ 2r*(Θ*)lMj .
Hence, if the underlying data-generating function is a sparse network itself, the prediction errors of
the estimators are essentially bounded by the statistical errors rcon/n and rnode/n.
The above-stated results also identify the oracle tuning parameters r^n and r:°de as optimal tuning
parameters: they give the best prediction guarantees in Theorem 1. But since the oracle tuning
parameters are unknown in practice, the guarantees implicitly presume a calibration scheme that
satisfies rTOn ≈ 匕口 in practice. A natural candidate is cross-validation, but there are no guarantees
that cross-validation provides such tuning parameters. This is a limitation that our theories share with
all other theories in the field.
Rather than dealing with the practical calibration of the tuning parameters, we exemplify the oracle
tuning parameters in a specific setting. This analysis will illustrate the rates of convergences that
we can expect from Theorem 1, and it will allow us to compare our theories with other theories
in the literature. Assume that the activation functions satisfy fj [0pj] = 0pj and are 1-Lipschitz
continuous with respect to the Euclidean norms on the functions’ input and output spaces Rpj . A
popular example is ReLU activation (Nair & Hinton, 2010), but the conditions are met by many other
functions as well. Also, assume that the noise vectors u1, . . . , un are independent and centered and
have uniformly subgaussian entries (van de Geer, 2000, Display (8.2) on Page 126). Keep the input
vectors fixed and capture their normalizations by
v∞ ∙= t
-n
-∑S∣∣xi∣∣∞	and v2 ∙=
n i=1
1n
-∑∣ki∣l2.
n i=1
∖
Then, we obtain the following bounds for the effective noises.
Proposition 2 (Subgaussian Noise). There is a constant c ∈ (0, ∞) that depends only on the
subgaussian parameters of the noise such that
and
巴 rCon ≤ cv∞
qnl(log[2mnp])3} ≥ ]— —
P rC
rnode
≤ cV2 {mnlp (log [2mnp])3 } ≥ 1 — —.
Broadly speaking, this result combined with Theorem 1 illustrates that accurate prediction with
connection- and node-sparse estimators is possible even when using very wide and deep networks.
Let us analyze the factors one by one and compare them to the factors in the bounds of Taheri et al.
(2020) and Neyshabur et al. (2015), which are the two most related papers. The connection-sparse
case compares to the results in Taheri et al. (2020), and it compares to the results in Neyshabur et al.
(2015) when setting the parameters in that paper to p = q = - (which gives a setting that is slightly
more restrictive than ours) or p = -; q = ∞ (which gives a setting that is slightly less restrictive than
ours). The node-sparse case compares to Neyshabur et al. (2015) with p = 2; q = ∞ (which gives
a setting that is more restrictive than ours, though). Our setup is also more general than the one in
Neyshabur et al. (2015) in the sense that it allows for activation other than ReLU.
6
Under review as a conference paper at ICLR 2021
The dependence on n is, as usual, 1 /√n UP to logarithmic factors.
In the connection-sparse case, our bounds involve v∞ = VZPn=I ∣x∕l∞∕n rather than the factor
v∞ ∙= maxi∈{i,…,n} ∣Xi∣∞ of Neyshabur et al. (2015) or the factor V = vzPn=ι ∣Xi∣2∕n of
Taheri et al. (2020). In principle, the improvements of v∞ over v∞ and V can be up to a factor √n
and up to a factor √d, respectively; in practice, the improvements depend on the specifics on the
data. For example, on the training data of MNIST (LeCUn et al., 1998) and Fashion-MNIST (Xiao
et al., 2017) (√n ≈ 250; √d = 28 in both data sets), it holds that v∞ ≈ v∞ ≈ V2∕9 and v∞ ≈
v∞ ≈ V2∕i2, respectively. In the node-sparse case, our bounds involve V2, which is again somewhat
smaller than the factor V ∙= maxi∈{i,…,n} Ilxi∣2 in Neyshabur et al. (2015).
The main difference between the bounds for the connection-sparse and node-sparse estimators
are their dependencies on the networks, maximal width p. The bound for the connection-sparse
estimator (3) depends on the width P only logarithmically (through p), while the bound for the
node-sparse estimator (6) depends on P sublinearly. The dependence in the connection-sparse case is
the same as in Taheri et al. (2020), while Neyshabur et al. (2015) can avoid even that logarithmic
dependence (and, therefore, allow for networks with infinite widths). The node-sparse case in
Neyshabur et al. (2015) does not involve our linear dependence on the width, but this difference stems
from the fact that they use a more restrictive version of the grouping—we take the maximum over
each layer, while they take the maximum over each node— and our results can be readily adjusted to
their notion of group sparsity. These observations indicate that node sparsity as formulated above is
suitable for slim networks (p《n) but should be strengthened or complemented with other notions of
sparsity otherwise. To give a numeric example, the training data in MNIST (LeCun et al., 1998) and
Fashion-MNIST (Xiao et al., 2017) comprise n = 60 000 samples, which means that the width
should be considerably smaller than 60 000 when using node sparsity alone. (Note that the input
layer does not take part in p, which means that d could be larger.)
For unconstraint estimation, one can expect a linear dependence of the error on the total number
of parameters (Anthony & Bartlett, 1999). Our bounds for the sparse estimators, in contrast, only
have a log[p] dependence on the total number of parameters. This difference illustrates the virtue of
regularization in general, and the virtue of sparsity in particular.
Both of our bounds have a mild √7 dependence on the depth. These dependencies considerably
improve on the exponentially-increasing dependencies on the depth in Neyshabur et al. (2015) and,
therefore, are particularly suited to describe deep network architectures. Replacing the conditions
maxj IIIΘj III1 ≤ 1 and maxj IIIΘj III2,1 ≤ 1 in the definitions of the connection-sparse and node-sparse
estimators by the stricter conditions Pj IIIΘj III1 ≤ 1 and Pj IIIΘj III2,1 ≤ 1, respectively (cf. Taheri
et al. (2020) and our discussion in Section 2), the dependence on the depth can be improved further
from √7 to (2∕l)l √7 (this only requires a simple adjustment of the last display in the proof of
Proposition 4), which is exponentially decreasing in the depth.
Our connection-sparse bounds have a mild log[m] dependence on the number of output nodes; the
node-sparse bound involve an additional factor √m. The case of multiple outputs has not been
considered in statistical prediction bounds before.
Proposition 2 also highlights another advantage of our regularization approach over theories such
as Neyshabur et al. (2015) that apply to constraint estimators. The theories for constraint estimators
require bounding the sparsity levels directly, but in practice, suitable values for these bounds are rarely
known. In our framework, in contrast, the sparsity is controlled via tuning parameters indirectly, and
Proposition 2—although not providing a complete practical calibration scheme—gives insights into
how these tuning parameters should scale with n, d, l, and so forth.
We also note that the bounds in Theorem 1 can be generalized readily to every estimator of the form
Θb
gen
∈ arg min
Θ∈Mgen
yi -gΘ[xi]22 +rgenIIIΘlIII
where rgen ∈ [0, ∞) is a tuning parameter, Mgen any nonempty subset of M, and ∣∣ ∙ || any norm.
The bound for such an estimator is then
err[Θgen] ≤
Θ∈iMgen{errθ] + ⅛n 2 ll0
7
Under review as a conference paper at ICLR 2021
Approach	Basis	References
FatShat	Fat-shattering dimension	Bartlett (1998)
Rad1	Rademacher complexity	Bartlett & Mendelson (2002); Lederer (2020a); Neyshabur et al. (2015)
Rad2	Rademacher complexity	Bartlett & Mendelson (2002); Golowich et al. (2017); Lederer (2020a)
NonPar	Non-parametric statistics	Schmidt-Hieber (2020)
HighDim	High-dimensional statistics	Taheri et al. (2020)
FatShat X
Radl	X	X
Rad2	X	X
NonPar	X	X
HighDim - ^X	X	X	X
Table 1: presence (X) or absence (X) of certain features in previous statistical theories for sparse
deep learning
for rgen ≥ rgen, Where rgen is as 优0口 but based on the dual norm of ∣∣∣ ∙ ∣∣∣ instead of the dual norm
of HI ∙ ∣∣∣ι. For example, one could impose connection sparsity on some layers and node sparsity
on others, or one could impose different regularizations altogether. We omit the details to avoid
digression.
We finally illustrate that the bounds for the prediction errors also entail bounds for the generalization
errors. For simplicity, We consider a parametric setting and subgaussian noise again.
Proposition 3 (Generalization Guarantees). Assume that the inputs x, x1, . . . , xn are i.i.d. random
vectors, that the noise vectors u1 , . . . , un are independent and centered and have uniformly sub-
gaussian entries, and that r↑on, r*οde → 0 as n → ∞. Consider an arbitrary positive Constant
b ∈ (0, ∞). If g* = gθ* for a Θ* ∈ Mi that is independent of the sample size n, it holds with
probability at least 1 - 1/n that

_	. O	r	,	_、	_	- _	，r
门sk[Θcon] ≤ (1 + b) risk[Θ ] + cv∞
l(log[2mnp])3
n
IH(θ*)lH∣1
for a constant c ∈ (0, ∞) that depends only on b and the subgaussian parameters of the noise.
Similarly, if g* = gθ* for a Θ* ∈ M2,1 that is independent of the sample size n, it holds with
probability at least 1 - 1/n that
m	∕mlp(log[2mnp])3
risk[Θcon] ≤ (1 + b) risk[Θ*] + cv2 ∖ —=-----------
n
∣II(Θ*)1∣∣∣2,1
for a constant c ∈ (0, ∞) that depends only on b and the subgaussian parameters of the noise.
Hence, the generalization errors are bounded by the same terms as the prediction errors.
4 Discussion
Our statistical theory for sparse deep learning incorporates node sparsity as Well as connection
sparsity, scales favorably in the number of layers, provides insights into hoW the tuning parameters
should scale With the dimensions of the problem, and applies to unbounded loss functions. It is
the first statistical theory that has all of these features—cf. Table 1. Additionally We avoid the
introduction of an additional scaling parameter and improve the dependence of the rates on the input
data. Finally, our novel proof approach based on high-dimensional statistics and empirical-process
theory is of independent interest.
8
Under review as a conference paper at ICLR 2021
Evidence for the benefits of deep networks has been established in practice (LeCun et al., 2015;
Schmidhuber, 2015), approximation theory (Liang & Srikant, 2016; Telgarsky, 2016; Yarotsky,
2017), and statistics (Golowich et al., 2017; Taheri et al., 2020). Since our guarantees scale at most
sublinearly in the number of layers (or even improve with increasing depth—see our comment on
Page 7), our paper complements these lines of research and shows that sparsity-inducing regularization
is an effective approach to coping with the complexity of deep and very deep networks.
Connection sparsity limits the number of nonzero entries in each parameter matrix, while layer
sparsity only limits the total number of nonzero rows. Hence, the number of columns in a parameter
matrix, that is, the width of the preceding layer, is regularized only in the case of connection sparsity.
Our theoretical results reflect this insight in that the bounds for the connection- and node-sparse
estimators depend on the networks’ width logarithmically and sublinearly, respectively. Practically
speaking, our results indicate that connection sparsity is suitable to handle wide networks, but node
sparsity is suitable only when complemented by connection sparsity or other strategies.
The mild logarithmic dependence of our connection-sparse bounds on the number of output nodes
illustrates that networks with many outputs can be learned in practice. Our prediction theory is the
first one that considers multiple output nodes; a classification theory with a logarithmic dependence
on the output nodes has been established very recently in Ledent et al. (2019).
The mathematical underpinnings of our theory are very different from those of most other papers
in theoretical deep learning. The proof of the main theorem shares similarities with proofs in high-
dimensional statistics; to formulate and control the relevant empirical processes, we use the concept
of effective noise, chaining, and Lipschitz properties of neural networks. These tools are not standard
in deep learning theory and, therefore, might be of more general interest (see Appendix A.7 for
further details).
Our theory shares some limitations with all other current theories in deep learning: the network
architectures are simpler than the ones typically used in practice (cf. Lederer (2020b), though);
the bounds concern global optima rather than the local optima or saddle points provided by many
practical algorithms; and the theory does not entail a practical scheme for the calibration of the tuning
parameters. Nevertheless, our theory, and mathematical theory in general, provides insights about
what accuracies to expect in practice and about what network types and estimators might be suitable
for a given problem.
In summary, our paper highlights the benefits of sparsity in deep learning and, more generally,
showcases the usefulness of statistical analyses for understanding neural networks.
References
J. Alvarez and M. Salzmann. Learning the number of neurons in deep networks. In Adv. Neural Inf.
Process Syst.,pp. 2270-2278, 2016.
M. Anthony and P. Bartlett. Neural network learning: theoretical foundations. Cambridge University
Press, 1999.
T. Ash. Dynamic node creation in backpropagation networks. Connect. Sci., 1(4):365-375, 1989.
S. Bakin. Adaptive regression and model selection in data mining problems. PhD thesis, The
Australian National University, 1999.
P. Bartlett. The sample complexity of pattern classification with neural networks: the size of the
weights is more important than the size of the network. IEEE Trans. Inform. Theory, 44(2):
525-536, 1998.
P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural
results. J. Mach. Learn. Res., 3:463-482, 2002.
M. Bello. Enhanced training algorithms, and integrated training/architecture selection for multilayer
perceptron networks. IEEE Trans. Neural Netw., 3(6):864-875, 1992.
S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of
independence. Oxford University Press, 2013.
9
Under review as a conference paper at ICLR 2021
B. Carl. Inequalities of Bernstein-Jackson-type and the degree of compactness of operators in Banach
spaces. In Ann. Inst. Fourier, volume 35, pp. 79-118, 1985.
S. Changpinyo, M. Sandler, and A. Zhmoginov. The power of sparsity in convolutional neural
networks. arXiv:1702.06257, 2017.
A. Dalalyan, M. Hebiri, and J. Lederer. On the prediction performance of the lasso. Bernoulli, 23(1):
552-581, 2017.
J. Feng and N. Simon. Sparse-input neural networks for high-dimensional nonparametric regression
and classification. arXiv:1711.07592, 2017.
N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
arXiv:1712.06541, 2017.
S.	Han, H. Mao, and W. Dally. Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding. In Proc. ICML, 2016.
T.	Hastie, R. Tibshirani, and M. Wainwright. Statistical learning with sparsity: the lasso and
generalizations. CRC press, 2015.
M. Hebiri and J. Lederer. Layer sparsity in neural networks. arXiv2006.15604, 2020.
J. Kim, V. Calhoun, E. Shim, and J.-H. Lee. Deep neural network with weight sparsity control and
pre-training extracts hierarchical features and enhances classification performance: evidence from
whole-brain resting-state functional connectivity patterns of schizophrenia. Neuroimage, 124:
127-146, 2016.
V. Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Trans. Inform. Theory,
47(5):1902-1914, 2001.
V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization
error of combined classifiers. Ann. Statist., 30(1):1-50, 2002.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436-444, 2015.
A. Ledent, Y. Lei, and M. Kloft. Norm-based generalisation bounds for multi-class convolutional
neural networks. arXiv:1905.12430, 2019.
J. Lederer. Bounds for Rademacher processes via chaining. arXiv:1010.5626, 2010.
J. Lederer. Risk bounds for robust deep learning. arXiv:2009.06202, 2020a.
J. Lederer. No spurious local minima: on the optimization landscapes of wide and deep neural
networks. arXiv:2010.00885, 2020b.
J. Lederer and S. van de Geer. New concentration inequalities for suprema of empirical processes.
Bernoulli, 20(4), 2014.
J. Lederer and M. Vogt. Estimating the lasso’s effective noise. arXiv:2004.11554, 2020.
H. Lee, C. Ekanadham, and A. Ng. Sparse deep belief net model for visual area V2. In Adv. Neural
Inf. Process. Syst., pp. 873-880, 2008.
W. Li and J. Lederer. Tuning parameter calibration for 'ι-regularized logistic regression. J. Statist.
Plann. Inference, 202:80-98, 2019.
S. Liang and R. Srikant. Why deep neural networks for function approximation? arXiv:1610.04161,
2016.
B.	Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky. Sparse convolutional neural networks. In
IEEE Int. Conf. Comput. Vis. Pattern Recognit., pp. 806-814, 2015.
10
Under review as a conference paper at ICLR 2021
C.	McDiarmid. On the method of bounded differences. Surv. Comb.,141(1):148-188,1989.
V. Nair and G. Hinton. Rectified linear units improve restricted Boltzmann machines. In Int. Conf.
Mach. Learn., pp. 807-814, 2010.
B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In Conf.
Learn. Theory, pp. 1376-1401, 2015.
L. Nie, M. Wang, L. Zhang, S. Yan, B. Zhang, and T.-S. Chua. Disease inference from health-related
questions via sparse deep learning. IEEE Trans. Knowl. Data Eng., 27(8):2107-2119, 2015.
S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini. Group sparse regularization for deep
neural networks. Neurocomputing, 241:81-89, 2017.
J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85-117, 2015.
J.	Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. Ann. Statist., 48(4):1875-1897, 2020.
K.	Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
In Int. Conf. Learn. Representations, 2015.
M. Taheri, F. Xie, and J. Lederer. Statistical guarantees for regularized neural networks.
arXiv:2006.00294, 2020.
M.	Telgarsky. Benefits of depth in neural networks. In Proc. Mach. Learn. Res., volume 49, pp.
1517-1539, 2016.
R.	Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B. Stat. Methodol.,
58(1):267-288, 1996.
S.	van de Geer. Empirical processes in M-estimation. Cambridge Univ. Press, 2000.
A. van der Vaart and J. Wellner. Weak convergence and empirical processes. Springer, 1996.
W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li. Learning structured sparsity in deep neural networks.
In Adv. Neural Inf. Process Syst., pp. 2082-2090, 2016.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking
machine learning algorithms. arXiv:1708.07747, 2017.
D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:
103-114, 2017.
R. Zhuang and J. Lederer. Maximum regularized likelihood estimators: A general prediction theory
and applications. Stat, 7(1):e186, 2018.
11
Under review as a conference paper at ICLR 2021
A Appendix
The Appendix consists of two auxiliary results and the proofs of Theorem 1 and Propositions 1 and 2.
Our approach combines techniques from high-dimensional statistics and empirical-process theory
that are very different from the techniques used in most other approaches in the literature.
A. 1 Lipschitz Property
In this section, we prove a Lipschitz property that we use in the proof of Proposition 2.
Proposition 4 (Lipschitz Property). In theframework ofSections 2 and3, it holdsfor all Θ, Γ ∈ Mι
that	_ _
∣∣gθ[x]- gr[x]IL ≤ W同 ∞Nθ - γ∣If
and for all Θ, Γ ∈ M2,1 that
∣∣gθ[x] -gr[χ]∣∣2 ≤√l同2∣∣Θ-γ∣∣f.
The Frobenius norm is defined as
l-1 pj+1 pj
XXXI® )ik ∣2
j=0 i=1 k=1
for Θ ∈M21 = Mi ∪M21.
Proposition 4 generalizes (Taheri et al., 2020, Proposition 2) to vector-valued network outputs and to
node sparsity, and it replaces their ∣∣x∣∣2 with the smaller ∣∣x∣∣∞ in the connection-sparse case.
Proof of Proposition 4. This proof generalizes and sharpens the proof of Taheri et al. (2020), and it
simplifies some arguments of that proof. We define the “inner subnetworks” of a network ge with
Θ ∈M2,1 as the vector-valued functions
S0gθ : Rd → Rp1
X → S0gθ[x] ∙= Θ0x
and
Sjgθ : Rd → Rpj+1
X → Sjgθ[x] ∙= Θjfj [∙∙∙ f 1[Θ0x]]
for j ∈ {1,...,l - 1}. Similarly, we define the “outer subnetworks” of ge as the real-valued
functions
Sjgθ : Rpj → Rpl
Z → Sjgθ[z] ∙= f l[Θl-1 …fj[z]]
for j ∈ {1, . . . , l - 1} and
Slgθ : Rpl → Rpl
z → Slgθ[z] ∙= fl[z].
The initial network can be split into an inner and an outer network along every layer j ∈ {1, . . . , l}:
gθ[x] = Sjgθ[Sj-ιgθ[x]]	for x ∈ Rd .
We call this our splitting argument.
To exploit the splitting argument, we derive a contraction result for the inner subnetworks and a
Lipschitz result for the outer subnetworks. We denote the '2-operator norm of a matrix A, that is, the
largest singular value of A, by ∣∣∣A∣∣∣op . Using then the assumptions that the activation functions are
1-Lipschitz and f j[0pj] = 0pj, we get for every Θ = (Θl-1,..., Θ0) ∈ M2,1 and X ∈ Rd that
∣∣Sj-2gΘ[x]∣∣2 = ∣∣θj-2f j-2[Sj-3gΘ[x]]∣∣2
12
Under review as a conference paper at ICLR 2021
≤ I I Θj-2 I I op∣∣∕j-2[sj-3ffθ[χ]] Il 2
≤ Il∣Θj-2 Iop I 岛-3gθ[x] Il 2
≤∙∙∙
for all j ∈ {2,...,l}. Now, since ∣∣ Θk ∣∣ op ≤ ∣ ∣ Θk 11 ∖ f ≤ ∣∣ Θk ∣∣ 2,1 and Θ ∈ M2,1, we can deduce
from the display that
Il Sj-2gθ[χ]∣∣ 2 ≤ (Y∣∣Θk ∣∣2,1)∣χ ∣ 2.
This inequality is our contraction property.
By similar arguments, we get for every zι, z2 ∈ Rpj that
l I Sjg®[z1] - Sjg®[z2] l I 2
=I I f 但T …fj [Z1]] - f l[Θ'T …fj [Z2]]I I 2
≤ IIΘl-1[fl-1 ∙∙∙fj[Z1]] -Θl-1[fl-1 ∙∙∙fj[Z2]]I I 2
≤∣∣Θj ∣∣opII f l-1[∙∙∙ fj [Z1]] - f l-1[∙∙∙ fj [Z2]]I I 2
≤ ∙ ∙ ∙
for j ∈ {i,..., l}, where Qk-I ∣θk∣∣op ∙= ι∙Hence, similarly as above,
IISjgθ[Z1] - Sjgθ[Z2] II 2 ≤ (Y ∣∣Θk ∣∣2,1 )∣ I Z1 - Z2 12 .
This inequality is our Lipschitz property.
We now use the contraction and Lipschitz properties of the subnetworks to derive a Lipschitz result for
the entire network. We consider two networks g@ and gγ with parameters Θ = (Θl-1,..., Θ0) ∈
M2,1 and Γ = (Γl-1,..., Γ0) ∈ M2,1, respectively. Our above-derived splitting argument applied
with j = 1 and j = l, respectively, yields
i I gΘ [x] - gr[x] i i 2 = I S1g®[S0g®[x]] - S lgr[Sl-1gr[x]] I i 2 .
Elementary algebra and the fact that SjTg@[Sj-2gr[x]] = Sjg@ [Θj-1f j-1[Sj-2gr[x]] for
j ∈ {2,...,l} then allow us to derive
I l g@[x] - gr[x] I I 2
l
=I I S 1gθ[S0g@[x]] - X(Sjgθ[Sj-1gr[x]] - Sjg@[Sj-1gr[x]]) - Slgr[Sl-1gr[χ]]∣∣
j=1
=I I S 1gΘ [S0g@[x]] - S1gθ[S0gΓ [x]]
l
-X(Sjg@[Sj-1ffr[χ]] - Sj-1 gθ[Sj-2gr[χ]])
j=2
+ Slgθ[Sl-1gr[x]] - Slgr[Sl-1gr[x]] IL
13
Under review as a conference paper at ICLR 2021
=IIS 1gΘ [S0g®[x]] — S1gθ[S0gΓ [x]]
l
-X (S jgθ[S j-1gr[x]] - S jgθ[ΘjTfjT[Sj-2gr[x]]])
j=2
+ Slg® [Sι-ιgr[x]] - Slgr[Sι-ιgr[x]] ∣[
≤ ∣ ∣ S1g® [S0g®[x]] - S1gθ[Sogr[x]] I I 2
l
+ XI I Sjg®[Sj-igr[x]] - Sjgθ[ΘjTfT [Sj-2gr[x]]]∣ I 2
j=2
+ I i Slg®[Sι-ιgr[x]] -SlIgr[Sι-ιgr[x]] I 2∙
We bound this further by using the above-derived Lipschitz property of the outer networks and the
observation that Slg®[Sl-igr[x]] = Sl3r[Sl-ιgr[x]]:
ii g®[x] - gr[x] I I 2 ≤ (Y 画kM,ι)∣ S0g®[x] - S0gr[x] I I 2
l ∕l-1 _ ∖ _
+ Σ ∏∣ l l Θk HI 2,1 I I Sj-ιgr[x] - Θj-1fj-1[Sj-2gr[x]] II 2,
j = 2 'k=j	'
which is by the definition of the inner networks equivalent to
I I g®[x] -gr[x] II 2 ≤ (Y IlΘfc Ill 2,1)lΘ0x-Γ0x ∣∣ 2
+ X (Y llΘk HI 2,1) I I Γj-1fj-1[Sj-2gr[x]] - Θj-1fj-1[Sj-2gr[x]] II 2.
j=2 'k=j	'
Using the properties of the operator norm, we can deduce from this inequality that
I I g®[x] - gr[x] I I 2 ≤ (Y llΘfc lll 2,1) HI θ0 - Γ0 HlOPHx H 2
+ X ( YHB HI 2,1) lHrjT- Θj-ιl∣∣op ∣∣ f j-1 [Sj-2gr[x]] ∣∣ 2.
j=2 'k=j	'
Invoking the mentioned conditions on the activation functions and the contraction property for the
inner subnetworks then yields
Y	max{H∣Θk IH 2,1,IHrk IH 2,1}) (XHlrj-Θj IH op)I I x I 2
k∈{0,...,l-1}	) ∖j=0	)
k=v
≤ √lHx H 2 II∣Θ - γIHf .
The proof for the connection-sparse case is almost the same. The main difference is that one needs to
use the H ∙ ∣∣ ∞- and H ∙ ∣ ∣ ∣ 1-norms (rather than the I ∙ I 2- and H∣ ∙ HIOP-norms) and the inequality H AbI ∣ ∞ ≤
HI A∣ I I 1 H bH ∞ (rather than the inequality ∣∣ Ab∣∣ 2 ≤ ∣∣∣ A∣op IlbH 2) to establish suitable contraction and
Lipschitz properties.	□
11 g®[x] - gr[x] 11 2 ≤ “Jmax-i
∖ V ∈{ 0 ,...,l 1}
A.2 ENTROPY BOUND
In this section, we establish bounds for the entropies of M1 and M2,1. The distance between
two networks g® and gr is defined as dist[g®, gp]：= √∑n=1 Ii g®[xi] - gr[xi] Ii ∞∕n. GiVen
this distance function and a radius t ∈ (0, ∞), the metric entropy of a nonempty set A ⊂ {Θ =
(Θl-1,..., Θ0) : Θj ∈ Rpj+1×pj} is denoted by H[t, A]. We then get the following entropy bounds.
14
Under review as a conference paper at ICLR 2021
Lemma 1 (Entropy Bounds). In the framework of Sections 2 and 3, it holds for a constant cH ∈
(0, ∞) and every t ∈ (0, ∞) that
H[t,Mi] ≤ CH
叫(v∞)2l+ 2
and
H [t, M2,1] ≤ CH (⅛图 log] (V∞)2l + 2
Proof of Lemma 1. The first bound can be derived by combining established deterministic and
randomization arguments (Carl, 1985);(Lederer, 2010, Proof of Theorem 1.1);(Taheri et al., 2020,
Proposition 3).
For the second bound, observe that
l∣θjl∣ι
pj+1 Pj	Pj
XX∣(θj)ik∣≤ PPj+1 X
pj+1
X ∣(θj )ik ∣2 = pp+η∣θj∣∣2,ι
i=1
pp∣∣θj∣∣2,1
for all j ∈ {0,...,l - 1} and Θj ∈ Rpj+1 ×pj. We used in turn 1. the definition of the ∣∣ ∙ ∣∣ι-norm on
Page 2, 2. the linearity and interchangeability of finite sums and the inequality ∣∣a∣ι ≤ √b∣a∣2 for all
a ∈ Rb, 3. the definition of the ∣∣ ∙ ∣∣ 2,1-norm on Page 4, and 4. the definition of the width p on Page 2.
Hence, M2,1 ⊂ √pMι. A bound for the entropies of M2,1 can, therefore, be derived from the first
bound by replacing the radii t on the right-hand side by t/√p.	□
A.3 Proof of Theorem 1
In this section, we state a proof for Theorem 1. The proof is inspired by derivations in high-
dimensional statistics—see, for example, (Zhuang & Lederer, 2018) and references therein.
Proof of Theorem 1. The main idea of the proof is to contrast the estimators’ objective functions
evaluated at their minima with the estimators’ objective functions at other points. Our first step
is to derive what we call a basic inequality. By the definition of the estimator in (6), it holds for
every Θ ∈ M2,1 that
X yi - gΘb [xi]22 + rnode∣∣∣Θb l∣∣∣2,1 ≤ X yi -gΘ[xi]22 + rnode∣∣∣Θl∣∣∣2,1 ,
i=1	i=1
where we use
the shorthand Θb
∙= Θnode. We then invoke the model in (1) to rewrite this inequality
as
nn
E ∣∣g*[Xi] + Ui- gθ [Xi]∣∣2 + rnode∣∣Θ ∣∣2,1 ≤ E ∣∣g*[Xi] + Ui- gθ[Xi]∣∣2 + Fde ∣∣Θl ∣∣2,1 .
i=1	i=1
Expanding the squared terms and rearranging the inequality then yields
X∣∣g*[xi]-gθ [xi]∣∣2 ≤ X∣∣g*[xi]- gθ[xi ]∣∣2
i=1	i=1
nn
+ 2 X(gΘ [xi]) Ui - 2 X(gθ[xi]) ui + rnode∣∣∣θ ∣∣∣2,1 - rnode∣∣∣θ ∣∣2,1 .
i=1	i=1
This is our basic inequality.
In the remainder of the proof, we need to bound the first two terms in the last line of the basic
inequality. We call these terms the empirical process terms. Using the reformulation of the networks
in (7), we can write the empirical process term of a general parameter Γ ∈ M2,1 according to
nn
2 X(gr[xi])>Ui = 2 X(Γl gr[xi])>Ui
i=1	i=1
15
Under review as a conference paper at ICLR 2021
with Γ ∈ M2,1. Using the 1. the properties of transpositions, 2. the definition of the trace function,
3. the cyclic property of the trace function, and 4. the linearity of the trace function yields further
n	n
2 X(gr[xi])> Ui = 2 X(ffτ[xi])τ(Γl)τui
i=1	i=1
n
=2 X trace [(gr[xi])τ(Γl)τuJ
i=1
n
=2 X trace[ui(%[xi])τ(Γl)τ]
i=1
=2 trace [(X%(齐忠])>) (Γl)τ .
Now, 1. denoting the column-vector that corresponds to the kth column of a matrix A by A∙k, 2. using
Holder,s inequality, 3. using Holder,s inequality again, and 4. again Holder,s inequality and our
definitions of the elementwise '∞-and '「norms, we find
n	Pl	/ / n	、
2X(gr[xi])TUi = 2X((Xui(gr[xi])T)	, T')∙k
i=1	k=1 ' ×i=1	• ∙k
l
≤ 2Xl(XUig[Xi])T)	Il (Γl).k Il 2
k=1 " 'i=1	• ∙k 2
≤2
max
k∈{1,...,p
、	Pl
ui (gΓ [xi])T)	ɪs l I (P)∙k 11 2
•∙k 2k=1
n
≤ 2√m XUi(gr[xi])τ III γ1 III 2,1,
i=1	∞
which implies in view of the definition of the effective noise in (8)
n
2X(gr[Xi])TUi ≤ r：ode II Γl Ill2,1.
i=1
This inequality is our bound on the empirical process terms.
We can combine the bound on the empiricial process term and the basic inequality to find
XI l g*[xi] — g&[χi]∣l 2 ≤ Xl I g*[xi] — g®[xi] I 2
i=1	i=1
+ rnode i I θl I I I 2,1 + rXodeg' N 2,1 + rnode 忖 ∣∣∣ 2,1 - rnode ∣∣∣ θ' N 2,1 ∙
Using then the assumption rnode ≥ rXode yields
nn
):l l gn [xi] - gΘ [xi] l l 2 ≤ ' : l l gn [xi] - 9® [xi] ] [ 2 + 2rnode l ∣ θ1 ∣∣ 2,1 ∙
i=1	i=1
Multiplying both sides by 1/n and taking the infimum over Θ ∈ M2,1 on the right-hand side then
gives
n Xl l 9*[xi] -9®[xi] i 2 ≤ θ∈■虬 ι∣1 Xl l gm -9®[xi] i l 2+2rnde 1 1 θ' i∣∣ 2,1}.
i=1	, '	i=1	'
Invoking the definition of the prediction error on Page 4 gives the desired result.
The proof for the connection-sparse estimator is virtually the same.	□
16
Under review as a conference paper at ICLR 2021
A.4 Proof of Proposition 1
In this section, we give a short proof of Proposition 1.
Proof of Proposition 1. Verify the fact that if the all-zeros parameter is neither a solution of (3) nor
jj
of (5), all solutions Θcon and Θcon of (3) and (5), respectively, satisfy (Θcon)j, (Θcon)j 6= 0pj+1 ×pj
for all j ∈ {0, . . . , l}.
It then follows from the assumed nonnegative homogeneity, rcon > 0, and the definition of the
estimator in (3) that ∣∣(Θcon)0∣∣∣ι,..., ∣∣∣(Θcon)l-1∣∣∣ι = 1 for all solutions Θ∞n.
Given a solution Θ of (5), define a ：= ∣∣∣(Θ con)0∣∣∣ι∕(l + 1)+---------+ ∣∣(Θ con)l∣∣ι∕(l + 1) and verify
the fact that Γ ∈ M With Γ0 ∙= a(Θ con)0∕∣∣∣(Θ TOn)0∣∣ι, Γ1 ∙= a(Θ con)1∕∣∣∣(Θ TOn)1∣∣ι,... has the
same value in the objective function as Θcon .
□
A.5 Proof of Proposition 2
In this section, We establish a proof of Proposition 2. The key tools are the Lipschitz property of
Proposition 4 and the entropy bounds of Lemma 1.
Proof of Proposition 2. The main idea is to reWrite the event under consideration in a form that
is amenable to knoWn tail bounds for suprema of empirical processes With subgaussian random
variables.
The connection-sparse bound folloWs from
p|r：on ≥ cv∞qnl(log[2mnp])3}
P〈 2 sup
I Ψ∈Mι
n
Eui (gψ[xi])
i=1
≥ cV∞ Jnl(log[2mnp])31
∞
≤ mp max P42 sup I ( V^ui(gψ[xi]))
j∈{1,...,m}	I Ψ∈Mι I ∖~1	Ijk
k∈{1,...,pl}	=
≥ cv∞ Jnl(log[2mnp])3 }
≤ mp ∙-=
— mnp
≤1,
n
where we use in turn 1. the definition of 噎口 in (8), 2. the union bound, 3. van de Geer (2000,
Corollary 8.3) and our Proposition 4 and Lemma 1, and 4. the inequality pl ≤ P = Pj=o pj+1pj and
consolidating the factors. The key concept underlying van de Geer (2000, Corollary 8.3 on Page 128)
is chaining (van der Vaart & Wellner, 1996, Page 90).
The same considerations also apply to the node-sparse case, but we get an additional factor √m from
the definition of the effective noise in (8) and a factor √p from the entropy bound in Lemma 1. The
differences between the bounds for the connection- and node-sparse cases in terms of v∞ vs. V stem
from the different Lipschitz constants in Proposition 4.	□
A.6 Proof of Proposition 3
Proof of Proposition 3. The proof is based on standard empirical-process theory, including contrac-
tion and symmetrization arguments.
17
Under review as a conference paper at ICLR 2021
Using basic algebra and measure theory, it is easy to show that
.,^	r	,	.	^	r
门sk[Θcon] ≤ (1 + b) risk[Θ ] + Cb err[Θcon]
1 n
+ Cb n X(Ug*[xi] - gθ con[xi]U2 - Ellg*[xi] - gθcon[xi]U2
i=1
for a constant Cb ∈ (0, ∞) that depends only on b. The first term in this bound is the minimal risk
as stated in the proposition, and the second term can be bounded by Corollary 1 and Proposition 2.
Hence, it remains to bound the third term.
In view of the law of large numbers, it is reasonable to hope for the third term to be small. But to
make this precise, we have to keep in mind that the estimator itself depends on the input vectors. We,
therefore, need to prepare the third term for the application of a uniform version of the law of large
numbers. Using standard contraction arguments—see (Boucheron et al., 2013, Chapter 11.3), for
example-and Holder,s inequality, We can bound the third term by bounding
max{∣∣∣(Θ*)l∣∣∣ι,∣∣KΘ 仙卉岛 /up
Θ∈Mι
X(gΘ* [xi] - gΘ[xi] - E [gΘ* [xi] - gΘ[xi]]) III
i=1	∞
Which removes the dependence on the estimator Θcon up to the leading factor. To see that We can
also neglect that factor, verify (see Proposition 2 and the proof of Theorem 1) that ∣∣∣(Θcon)l ∣∣∣1 ≤
2∣∣∣(Θ*)l∣∣∣ι with high probability as long as r*on ≥ cv∞,nl(log[2mnp])3 with C large enough.
Consequently, We just need to consider the quantity
sup
Θ∈Mι
in the following.
n
X (gΘ* [xi] - gΘ[xi] - E[gΘ* [xi] - gθ[xi]])
i=1
2
∞
The last step is to bring this term in a form that is amenable to our earlier proofs. Using standard
symmetrization arguments—see van der Vaart & Wellner (1996, Chapter 2.3), for example)—we can
bound this quantity by bounding
sup
Θ∈Mι
n	III2
ɪ3 ki (gΘ* [xi] - gΘ[xi])
i=1	∞
where k1, . . . ,kn are i.i.d. Rademacher random variables. But even though k1, . . . ,kn are
i.i.d. Rademacher random variables, we do not resort to Rademacher complexities; instead, we
use that Rademacher random variables are subgaussian, so that we can then proceed similarly as in
the proof of Proposition 2.
The node-sparse case can be treated along the same lines.	□
A.7 Extensions
Our proof approach disentangles the specifics of the objective function (proof of Theorem 1), of
the network structure (proof of Proposition 4), and of the stochastic terms (proofs of Lemma 1 and
Proposition 2). This feature allows one to generalize and extend the results of this paper in straight-
forward ways. For example, extensions to different noise distributions only need a corresponding
version of Proposition 2—with everything else unchanged. One could envision, for example, using
concentration inequalities for heavy-tailed distributions such as in Lederer & van de Geer (2014).
Extensions to different loss functions, to give another example, can be established by adjusting
Theorem 1 accordingly. This can be done, for example, by invoking ideas from specialized literature
on high-dimensional logistic regression such as Li & Lederer (2019). We avoid going into further
details to avoid digression; the key message is that the flexibility of the proofs is yet another advantage
of our approach.
18