Under review as a conference paper at ICLR 2021
Learning to Actively Learn: A Robust Ap-
PROACH
Anonymous authors
Paper under double-blind review
Ab stract
This work proposes a procedure for designing algorithms for specific adaptive
data collection tasks like active learning and pure-exploration multi-armed ban-
dits. Unlike the design of traditional adaptive algorithms that rely on concentration
of measure and careful analysis to justify the correctness and sample complexity
of the procedure, our adaptive algorithm is learned via adversarial training over
equivalence classes of problems derived from information theoretic lower bounds.
In particular, a single adaptive learning algorithm is learned that competes with the
best adaptive algorithm learned for each equivalence class. Our procedure takes
as input just the available queries, set of hypotheses, loss function, and total query
budget. This is in contrast to existing meta-learning work that learns an adap-
tive algorithm relative to an explicit, user-defined subset or prior distribution over
problems which can be challenging to define and be mismatched to the instance
encountered at test time. This work is particularly focused on the regime when the
total query budget is very small, such as a few dozen, which is much smaller than
those budgets typically considered by theoretically derived algorithms. We per-
form synthetic experiments to justify the stability and effectiveness of the training
procedure, and then evaluate the method on tasks derived from real data including
a noisy 20 Questions game and a joke recommendation task.
1	Introduction
Closed-loop learning algorithms use previous observations to inform what measurements to take
next in a closed-loop in order to accomplish inference tasks far faster than any fixed measurement
plan set in advance. For example, active learning algorithms for binary classification have been
proposed that under favorable conditions require exponentially fewer labels than passive, random
sampling to identify the optimal classifier (Hanneke et al., 2014). And in the multi-armed bandits
literature, adaptive sampling techniques have demonstrated the ability to identify the “best arm” that
optimizes some metric with far fewer experiments than a fixed design (Garivier & Kaufmann, 2016;
Fiez et al., 2019). Unfortunately, such guarantees often either require simplifying assumptions that
limit robustness and applicability, or appeal to concentration inequalities that are very loose unless
the number of samples is very large (e.g., web-scale).
The aim of this work is a framework that achieves the best of both worlds: algorithms that learn
through simulated experience to be as effective as possible with a tiny measurement budget (e.g.,
20 queries), while remaining robust due to adversarial training. Our work fits into a recent trend
sometimes referred to as learning to actively learn (Konyushkova et al., 2017; Bachman et al., 2017;
Fang et al., 2017; Boutilier et al., 2020; Kveton et al., 2020) which tunes existing algorithms or
learns entirely new active learning algorithms by policy optimization. Previous works in this area
learn a policy by optimizing with respect to data observed through prior experience (e.g., meta-
learning or transfer learning) or an assumed explicit prior distribution of problem parameters (e.g.
the true weight vector for linear regression). In contrast, our approach makes no assumptions about
what parameters are likely to be encountered at test time, and therefore produces algorithms that do
not suffer from a potential mismatch of priors. Instead, our method learns a policy that attempts to
mirror the guarantees of frequentist algorithms with instance dependent sample complexities: if the
problem is hard you will suffer a large loss, if it is easy you will suffer little.
1
Under review as a conference paper at ICLR 2021
The learning framework is general enough to be applied to many active learning settings of interest
and is intended to be used to produce novel and robust high performing algorithms. The difference is
that instead of hand-crafting hard instances that witness the difficulty of the problem, we use adver-
sarial training inspired by the robust reinforcement learning literature to automatically train minimax
policies. Embracing the use of a simulator allows our learned policies to be very aggressive while
maintaining robustness. Indeed, this work is particularly useful in the setting where relatively few
rounds of querying can be made, where concentration inequalities of existing algorithms are vacu-
ous. To demonstrate the efficacy of our approach we implement the framework for the (transductive)
linear bandit problem. This paradigm includes pure-exploration combinatorial bandits (e.g., shortest
path, matchings) as a special case which itself reduces to active binary classification. We empirically
validate our framework on a simple synthetic experiment before turning our attention to datasets de-
rived from real data including a noisy 20 questions game and a joke recommendation task.
2	Proposed Framework for Robust Learning to Actively Learn
Whether learned or defined by an expert, any algorithm for active learning can be thought of as
a policy from the perspective of reinforcement learning. At time t, based on an internal state st ,
the policy takes action xt and receives observation yt , which then updates the state and the process
repeats. In our work, at time t the state st ∈ S is a function of the history {(xi, yi)}it=-11 such
as its sufficient statistics. Without loss of generality, a policy π takes a state as input and defines
a probability distribution over X so that at time t We have Xt 〜 ∏(st). FiX a horizon T. For
t = 1, 2, . . . , T
•	state st ∈ S is a function of the history, {(xi, yi)}it=-11,
•	action xt ∈ X is drawn at random from the distribution π(st) defined over X, and
•	next state st+ι ∈ S is constructed by taking action Xt in state St and observing yt 〜f (∙∣0*, st, Xt)
until the game terminates at time t = T and the policy receives loss LT. Note that LT is a random
variable that depends on the tuple (π, {(xi, yi)}T=ι,θ*). We assume that f is a distribution ofknown
parameteric form to the policy (e.g., f (∙∣θ, s, x) ≡ N(hx, θ), 1)) but the parameter θ is unknown to
the policy. Let Pπ,θ, Eπ,θ denote the probability and expectation under the probability law induced
by executing policy π in the game with θ* = θ to completion. Note that P∏,θ includes any internal
randomness of the policy π and the random observations yt 〜f (∙∣θ, st,xt). Thus, P∏,θ assigns a
probability to any trajectory {(xi,yi)}T=ι∙ For a given policy π and θ* = θ, the metric of interest
we wish to minimize is the expected loss '(∏,θ) := E∏,θ [Lt] where LT as defined above is the
loss observed at the end of the episode. For a fixed policy π, '(∏,θ) defines a loss surface over all
possible values of θ. This loss surface captures the fact that some values of θ are just intrinsically
harder than others, but also that a policy may be better suited for some values of θ versus others.
Example: In active binary classification, T is a label budget, X could be a set of images such
that we can query the label of example image xt ∈ X, yt ∈ {-1, 1} is the requested binary
label, and the loss LT is the classification error of a trained classifier on these collected labels.
Finally, θx = p(y = 1|x) for all x ∈ X . More examples
can be found in Appendix A.
2.1 Instance dependent performance metric
We now define the sense in which we wish to evaluate
a particular policy. For any fixed value of θ one could
clearly design an algorithm that would maximize perfor-
mance on θ, but then it might have very poor performance
on some other value θ0 6= θ. Thus, we would ideally like
π to perform uniformly well over a set of θ's that are all
equivalent in a certain sense. Define a positive function
C : Θ → (0, ∞) that assigns a score to each θ ∈ Θ that
intuitively captures the “difficulty” of a particular θ, and
can be used as a partial ordering of Θ. Ideally, C(θ) is a
monotonic transformation of '(e, θ) for some “best” pol-
icy πe that we will define shortly. We give the explicit C(θ)
」0」」山①DUeE.JOtBd
r (Difficulty)
Figure 1: The r-dependent baseline defines
a different policy for each value of r, thus,
the blue curve may be unachievable with
just a single policy. π* is the single policy
that minimizes the maximum gap to this r-
dependent baseline policy.
2
Under review as a conference paper at ICLR 2021
for the active binary classification example in Section 3, further description of C in Section 2.2, and
more examples in Appendix A. For any set of problem instances Θ define
'(π, Θ) := sup '(π, θ).
θ∈Θ
And for any r ≥ 0, define
Θ(r) = {θ : C(θ) ≤ r}.
The quantity '(π, Θ(r)) 一 inf∏ '(π0, Θ(r)) is then a function of r that describes the sub-optimality
gap of a given policy π relative to an r-dependent baseline policy trained specifically for each r. For
a fixed rk > 0, a policy π that aims to minimizejust '(π, Θ(r)) might focus just on the hard instances
(i.e., those with C(θ) close to r) and there may exist a different policy π0 that performs far better
than π on easier instances (i.e., those with C(θ)《r). To avoid this, assuming SuPr '(π, Θ(r)) 一
inf∏o '(∏0, Θ(r)) < ∞, We define
π* := arginfsup ('(∏, Θ(r)) — inf '(π0, Θ(r)))	(1)
π r>0	π0
as the policy that minimizes the worst case sub-optimality gap over all r > 0. Figure 1 illustrates
these definitions. Instead of computing inf∏ '(∏0, Θ(r)) for all r, in practice we define a grid with
an increasing sequence {rk}K=ι, to find an approximation to ∏*. We are now ready to state the goal
of this work:
Objective: Given an increasing sequence ri < ∙ ∙ ∙ < rκ that indexes nested sets of problem
instances of increasing difficulty, Θ(r1) ⊂ Θ(r2) ⊂ •…⊂ Θ(rK), we wish to identify a policy b
that minimizes the maximum sub-optimality gap with respect to this sequence. Explicitly, we seek
to learn
b := arginf max 卜(∏, Θ(rk)) — inf '(π0, Θ(rk)))	(2)
π	k≤K	π0
where '(∏, Θ) := SuP '(∏, θ) and '(∏, θ) is the expected loss incurred by policy ∏ on instance θ.
θ∈Θ
Note that as K → ∞ and SuPk rk+1 → 1, (1) and (2) are essentially equivalent under benign
rk
smoothness conditions on C(θ), in which case b → ∏*. In practice, we choose a finite K where
ΘrK contains all problems that can be solved within the budget T relatively accurately, and a small
e > 0, where maxk rk+1 = 1 + e. Furthermore, the objective in (2) is equivalent with
rk
b = arginf max ('(∏, Θ(rk)) — '(∏k, Θ(rk)))
where ∏k ∈ arg inf SuP '(π, θ).
π θ=C(θ)≤rk
We can efficiently solve this objective by first computing ∏k for all k ∈ [K] to obtain '(∏k, Θ(rk))
as benchmarks, and then use these benchmarks to train πb.
2.2	PICKING THE COMPLEXITY FUNCTION C(θ)
We have defined an optimal policy in terms of a function C (θ) that determines a partial ordering
over instances θ. This function can come from a heuristic that intuitively captures the difficulty of
an instance. Or it can be defined and motivated from information theoretic lower bounds that often
describe a general ordering, but are typically very loose relative to empirical performance. For ex-
ample, consider the standard multi-armed bandit game where an agent has access to K distributions
and in each round t ∈ [T] she chooses a distribution It ∈ [K] and observes a random variable in
[0, 1] with mean θIt. If her strategy is described by a policy π, once t reaches T she receives loss
LT = maxi∈[κ] PT=I θi -θit with expectation '(∏, θ) = E[Lt] where the expectation is taken with
respect to the randomness in the observations, and potentially any randomness of the policy. Under
benign conditions, it is known that any policy must suffer '(π, θ) & min{√KT, Pi=* (θ* — θi)-1}
where θ* = maxi∈[k] θ% (Lattimore & Szepesvari, 2018). Such a lower bound is an ideal candi-
date for C(θ). We define a different C(θ) for our particular experiments of interest, and others are
described in Appendix A. The bottom line is that any function C(θ) works, but ifit happens to cor-
respond to an information theoretic lower bound, the resulting policy will match the lower bound if
it is achievable.
3
Under review as a conference paper at ICLR 2021
2.3	Differentiable policy optimization
The first step in learning the policy πb defined in Equation 2 is to learn each πk :=
inf∏ supθqθ)≤rk '(π, θ) for all k = 1,..., K. Once all ∏k are defined, b of (2) is an optimiza-
tion of the same form after shifting the loss by the scalar '(∏k, Θ(rk)). Consequently, to learn b
it suffices to develop a training procedure to solve inf∏ supθ∈ω '0(∏, θ) for an arbitrary set Ω and
generic loss function '0(π, θ).
To make the optimization problem inf∏ supθ∈ω '0(∏, θ) tractable, We parameterize it as follows.
N
First, to compute the suprema over Θ, we consider a finite set Θ := {θi}N=ι ⊂ Ω, weighted by
SOFTMAX(w) where w ∈ RN . In addition, instead of optimizing over all possible policies, we
restrict the policy as the class of neural networks that take state representation as input and output a
probability distribution over actions, parameterized by weights ψ. Mathematically, it could be stated
as the following:
inf sup '(π, θ) = inf	sup max '(π,θi)		(3)
π θ∈Ω	π	θlN ⊂Ω i∈[N]		
= inf π	sUp	Ei 〜SOFTMAX(W) w∈Rn ,θiN ⊂Ω	h'(∏,θi)i	(4)
≈ inf ψ	sUp	Ei 〜SOFTMAX(W)	h'(∏ψ ,0i)].	(5)
	w∈RN,ei：N ⊂Ω		
Algorithm 1: Gradient Based Optimization of (5)
ι Input: partition Ω, number of iterations Nit, number of problem samples M, number of
rollouts per problem L, and loss variable LT at horizon T (see beginning of Section 2).
2	Goal: Compute the optimal policy arginf∏ suPθ∈ω '(∏, θ) = arginf∏ suPθ∈ω E∏,θ [Lt].
3	Initialization: w, finite set Θ and ψ
4	for t = 1, ..., Nit do
5	Collect rollouts of play:
.-- 一一 ^一.	_	—勿54__	,
6	Sample M problem indices Ii,...,Im 〜SOFTMAX(W)
7	for m = 1, ..., M do
8	Collect L independent rollout trajectories, denoted as 丁①，上工,by the policy ∏ψ
ψ
for problem instant θIm and observe losses ∀1 ≤ l ≤ L, LT(πψ, τm,l, θIm ).
9	end
io	Optimize worst cases in Ω:
11	Update the generating distribution by taking ascending steps on gradient estimates:
1M	L
W 一W + ML X Vw log(SOFTMAX(w)im) ∙ (XLτ(πψw,@"))(6)
m=1	l=1
ML
Θ -Θ + — XX (Vθ Lbarrier(elm , Ω)+ V&LT (∏ψ ,Trn,l ,乐皿)
m=1 l=1
+ LT (∏ψ ,Tm,l, elm ) ∙ Vθ lθg(P∏ψ ,eIm (Tm,ι)))	⑺
where Lbarrier is a differentiable barrier loss that heavily penalizes the θιm S outside Ω.
12	Optimize policy:
13	Update the policy by taking descending step on gradient estimate:
1 ML
ψ J ψ - ML X X LT (πψ, τm,l,θIm ) ∙ vψ log(P∏ψ ,eIm (Tm,l))	⑻
m=1 l=1
14 end
Note that the objectives in (3) and (4) are indeed equivalent as 9上N are free parameters we optimize
over rather than taking fixed values. Now, to motivate (5), starting from the left hand side of (3),
4
Under review as a conference paper at ICLR 2021
observe that a small change in ∏ may result in a large change in argsupθ∈Ω '(∏, θ). Therefore,
with the goal of covering the entire Ω, We optimize the N points so that when ∏ changes a bit,
there is at least one θi close to the optimal argsup. In addition, to covering the entire space of Ω,
N is expected to be very large in practice. However, to optimize the objective effectively, we can
only evaluate on M of θ s (M《N) in each iteration. Therefore, instead of naively sampling M
points uniformly at random from the N points, in (4), we optimize an extra multinomial distribution,
SOFTMAX(w), over the N points so that the points around the argsup are sampled more often. The
final approximation in (5) comes from parameterizing the policy by a neural network.
To solve the saddle point optimization problem in (5), we use an instance of the Gradient Descent
Ascent (GDA) algorithm as shown in Algorithm 1. The gradient estimates are unbiased estimates of
the true gradients with respect to ψ , w and Θ (shown in Appendix B). We choose N large enough to
avoid mode collapse, and M, L as large as possible to reduce variance in gradient estimates while
fitting the memory constraint. We use Adam optimization (Kingma & Ba, 2014) in taking gradient
updates and regularize some of the parameters (an example will be presented in the next section).
Note the decomposition for log(Pπψ,θ0 (τ)) in (7) and (8), where rollout τ = {(xt, yt)}tT=1, and
iog(P∏ψa({(χt,yt)}T=ι)) = log (∏ψ(χι) ∙ f(yι∣θ0, si) ∙ QT=2 ∏ψ(st,χt) ∙ f(ytW,st,χt,.
ψ
Here πψ and f are only dependent on ψ and Θ respectively. During evaluation of a fixed policy π ,
we are interested in solving supθ∈ω '(∏, θ) by gradient ascent updates like (7). The decoupling of
πψ and f thus enables us to optimize the objective without differentiating through a policy π, which
could be non-differentiable policies like deterministic algorithms.
Finally, we make a few remarks on the parameterization of (5). As given in (5), we represent the gen-
erating distribution P as a simple finite number of weighted particles, analogous to a particle filter.
Our policy parameterization πψ could be modelled by multi-layer perceptrons, recurrent neural net-
works, etc. We note that when using alternative generator parameterization like GANs (Goodfellow
et al., 2014), an unbiased gradient can also be derived similarly.
3 Implementation for Linear Bandits and Classification
We now apply the general framework of the previous section to a specific problem: transductive
linear bandits. As described in Sections 5 and Appendix A this setting generalizes standard multi-
armed bandits, linear bandits, and all of binary classification through a simple reduction to combi-
natorial bandits. We are particularly motivated to look at classification because the existing agnostic
active learning algorithms are very inefficient (see Section 5). Indeed when applied to our setting of
T = 20 they never get past their first stage of uniform random sampling. Consider the game:
Input: Policy π, X ⊂ Rd, Z ⊂ Rd, time horizon T ∈ T
Initialization: Nature chooses θ* ∈ Rd (hidden from policy)
for t = 1, 2, . . . , T
•	Policy π selects xt ∈ X using history {(xs, ys)}ts-=11
•	Nature reveals y 〜f (∙∣θ*,χt) with E[yt∣θ≠,χt] = <χt,θ*i
Output: Policy π recommends b ∈ Z as an estimate for z?(θ*) := argmaxz∈z(z, θj and
suffersloss LT=东量-R
U{z*(θ*) = z}
if Simple Regret
if Best Identification
The observation distribution f (∙∣θ, x) is domain specific but typically taken to be either a Bernoulli
distribution for binary data, or Gaussian for real-valued data. We are generally interested in two
objectives: BEST IDENTIFICATION which attempts to exactly identify the vector z? ∈ Z that is
most aligned with θ*, and Simple Regret which settles for an approximate maximizer.
Defining C(θ) Recalling the discussion of Section 2.1, C(θ) should ideally be monotonically in-
creasing in the intrinsic difficulty of minimizing the loss with respect to a particular θ. For arbitrary
X ⊂ Rd and Z ⊂ Rd, it is shown in Fiez et al. (2019) that the sample complexity of identify-
ing z？(θ) = argmaxz∈z(z, θ) with high probability is proportional to a quantity ρ*(θ), the value
5
Under review as a conference paper at ICLR 2021
obtained by an optimization program. Another complexity term that appears in the combinatorial
bandits literature (Cao & Krishnamurthy, 2017) where X = {ei : i ∈ [d]} and Z ⊂ {0, 1}d is
d
ρe(θ) =* X
i=1
max	Jz -z?(%
z∙.Zi = zi(θ∖ hz — z*(θ),θ)
(9)
One can show ρ?(θ) ≤ e(θ) and in many cases track each other. Because e(θ) can be computed
much more efficiently compared to ρ*(θ), We use C(θ) = p(θ) in our experiments.
Algorithm 2: Training Workflow
1	Input: sequence {rk}kK=1, complexity function C, and obj ∈ {SIMPLE REGRET, BEST
IDENTIFICATION}.
2	Define k(θ) ∈ [K] such that rk(θ)-1 < C(θ) ≤ rk(θ) for all θ with C(θ) ≤ rK
3	For each k ∈ [K], obtain policy e by Algorithm 1 with Ω = Θ(rk) and Simple Regret
loss
4	if obj is SIMPLE REGRET then
5	For each k ∈ [K ], compute '(∏k ,rk)	// In this case, ∏k = Tk.
6	Warm start b = e∖κ∕2∖; optimize b by Algorithm 1 with Ω = Θ(rK) and objective in (2),
i.e., LT = hz?(θ) - b,θi- '(∏k(θ), Θ(rk(θ)))
7	else if obj is BEST IDENTIFICATION then
8	For each k ∈ [K], warm start ∏k = Tk; optimize ∏k by Algorithm 1 with Ω = Θ(rk) and
Best Identification loss; compute '(∏k, Θ(rk))
9	Warm start e = ∏∖κ∕2j; optimize e by Algorithm 1 with Ω = Θ(rK) and objective in (2),
i.e., LT = 1{z*(θ) = b} - '(∏k(θ), Θ(rk(θ)))
10	end
11	Output: πb (an approximate solution to (2))
Training. When training our policies, we follow the following procedure in Algorithm 2. Note
that even when we are training for Best Identification, we still warm start the training with
optimizing Simple Regret. This is because a random initialized policy performs so poorly that
Best Identification is nearly always 1, making it difficult to improve the policy. in addition,
our generating distribution parameterizations exactly follows from Section 2.3, while detailed state
representations, policy parametrization and hyperparamters can be found in Appendix C, D.
Loss functions. instead of optimizing the approximated quantity from (5) directly, we add regu-
larizers to the losses for both the policy and generator. First, we choose the Lbarrier in (7) to be
λba∏∙ier ∙ max{0, log(C(X, Z, θ)) - log(rk)}, for some large constant λbarrier. To discourage the pol-
icy from over committing to a certain action and/or the generating distribution from covering only a
small subset of particles (i.e., mode collapse), we also add negative entropy penalties to both policy’s
output distributions and SOFTMAX(w) with scaling factors λPol-reg and λGen-reg.
4	Experiments
We now evaluate the approach described in the previous section for combinatorial bandits with
X = {ei : i ∈ [d]} and Z ⊂ {0, 1}d. We stress that the framework implemented here can be
applied to any X, Z ⊂ Rd and any appropriate f-just plug and play to learn a new policy. In our
experiments we take particular instances of combinatorial bandits with Bernoulli observations. We
evaluated based on two criterion: instance-dependent worst-case and average-case. For instance-
dependent worst-case, we measure, for each rk and policy π, '(∏, Θ(rk)) := max '(∏, θ) and
θ∈Θ(rk)
plot this value as a function of rk . We note that our algorithm is designed to optimize for such
metric. For the secondary average-case metric, we instead measure, for policy π and some collected
set Θ, 舌 Pθ∈θ '(π, θ). Performances of instance-dependent worst-case metric are reported in
Figures 2, 3, 4, 6, and 7 below while the average case performances are reported in the tables and
Figure 5. Full scale of the figures can also be found in Appendix F.
6
Under review as a conference paper at ICLR 2021
Algorithms. We compare against a number of baseline active learning algorithms (see Section 5
for a review). UNCERTAINTY SAMPLING at time t computes the empirical maximizer of hz, θi and
the runner-up, and samples an index uniformly from their symmetric difference; if either are not
unique, an index is sampled from the region of disagreement of the winners (see Appendix G for
details). The greedy methods are represented by soft generalized binary search (SGBS) (Nowak,
2011) which maintains a posterior distribution over Z and samples to maximize information gain.
A hyperparameter β ∈ (0, 1/2) of SGBS determines the strength of the likelihood update. We
plot or report a range of performance over β ∈ {.01, .03, .1, .2, .3, .4}. The agnostic algorithms for
classification (Dasgupta, 2006; Hanneke, 2007b;a; Dasgupta et al., 2008; Huang et al., 2015; Jain &
Jamieson, 2019) or combinatorial bandits (Chen et al., 2014; Gabillon et al., 2016; Chen et al., 2017;
Cao & Krishnamurthy, 2017; Fiez et al., 2019; Jain & Jamieson, 2019) are so conservative that given
just T = 20 samples, they are all exactly equivalent to uniform sampling and hence represented by
Uniform. To represent a policy based on learning to actively learn (LAL), we employ the method
of Kveton et al. (2020) with a fixed prior P constructed by drawing a z uniformly at random from
Z and defining θ = 2z - 1 ∈ [-1, 1]d (details in Appendix H). When evaluating each policy, we
use the successive halving algorithm (Li et al., 2017; 2018) for optimizing our non-convex objective
with randomly initialized gradient descent and restarts (details in Appendix E).
Thresholds. We begin with a
very simple instance to demonstrate
the instance-dependent performance
achieved by our learned policy. For
d = 25letX = {ei : i ∈ [d]}, Z =
{0 + Pik=1 ei : k = 0, 1, . . . , d},
and f (∙∣θ,x) is a Bernoulli distribu-
tion over {-1, 1} with mean hx, θi ∈
[-1, 1]. Note this is a binary classi-
fication task in one-dimension where
the set of classifiers are thresholds on
a line. We trained baseline policies
{πk }9k=1 for the BEST IDENTIFICA-
TION metric with C(θ) = ρe(X , Z, θ)
and rk = 23+i/2 for i ∈ {0, . . . , 8}.
First we compare the base poli-
cies πk to πb. Figure 2 presents
'(π, θ(r)) = suPθ0(θ)≤r=πM =
suPθ∕(θ)≤r Pπ,θ (z = z?(J)) as a
function of r for our base policies
{πk}k and the global policy π*, each
as an individual curve. Figure 3 plots
Figure 2: Learned policies,
lower is better
Figure 4: Max {θ : e(θ) ≤ Figure 5: Average Eθ〜Ph [∙],
r}, lower is better	lower is better
Figure 3: Sub-optimality of
individual policies, lower is
better
the same information in terms of gap: '(π, Θ(r)) - min	'(πk, Θ(rk)). We observe that each
k:r`k-i <r≤rk
πk performs best in a particular region and π* performs almost as well as the r-dependent baseline
policies over the range of r . This plot confirms that our optimization objective of (2) was successful.
Under the same conditions as Figure 2, Figure 4 compares the performance of π* to the algorithm
benchmarks. Since SGBS and LAL are deterministic, the adversarial training finds a θ that tricks
them into catastrophic failure. Figure 5 trades adversarial evaluation for evaluating with respect to a
parameterized prior: For each h ∈ {0.5,0.6,..., 1}, θ 〜Ph is defined by drawing a Z uniformly at
random from Z and then setting θi = (2zi — 1)(2αi — 1) where ai 〜Bernoulli(h). Thus, each sign
of 2z—1 is flipped with probability h. We then compute Eθ〜Ph [Pπ,θ (b = z?(θ))] = Eθ〜Ph ['(π,θ)].
While SGBS now performs much better than uniform and uncertainty sampling, our policy π* is still
superior to these policies. However, LAL is best overall which is expected since the support of Ph
is basically a rescaled version of the prior used in LAL.
20 Questions. We now address an instance constructed from the real data of Hu et al. (2018).
Summarizing how we used the data from Hu et al. (2018) (see Appendix I for details), 100 yes/no
questions were considered for 1000 celebrities. Each question i ∈ [100] for each person j ∈ [1000]
was answered by several annotators to construct an empirical probability Ej ∈ [0,1] denoting the
7
Under review as a conference paper at ICLR 2021
proportion of annotators that answered “yes.” To construct our instance, we take X = {ei : i ∈
[100]} and Z = {z(j) : [z(j)] = 1{p(j) > 1/2}} ⊂ {0,1}1000. Just as before, We trained {∏k}k=ι
for the BEST IDENTIFICATION metric with C(θ) = ρe(X, Z, θ) and ri = 23+i/2 for i ∈ {1, . . . , 4}.
Figure 6 is analogous to Fig-
ure 4 but for this 20 questions
instance. Uncertainty sampling
performs remarkably Well on this
instance. A potential explana-
tion is that on a noiseless instance
(e.g., θ = 2z - 1 for some z ∈
Z), our implementation of uncer-
tainty sampling is equivalent to
CAL (Cohn et al., 1994) and is
Figure 6: Max {θ : ρe(θ) ≤ r}
Table 1: Average E。〜P H
Method	Accuracy (%)
π* (Ours)	-T7：9
SGBS	{26.5, 26.2,
	27.2, 26.5,
	21.4, 12.8}
Uncertainty	14.3
LAL	4.1
Uniform	6.9	
knoWn to have near-optimal sample complexity (Hanneke et al., 2014). Uncertainty sampling even
outperforms our r-dependent baseline by a bit which in theory should not occur-we conjecture this
is due to insufficient convergence of our policies or local minima. Our second experiment constructs
a distribution P based on the dataset: to draw a θ 〜P we uniformly at random select a j ∈ [1000]
and sets θ, = 2p(j) - 1 for all i ∈ [d]. As shown in Table 1, SGBS and π* are the winners. LAL
performs much worse in this case, potentially because of the distribution shift from P (prior we
train on) to P (prior at test time). The strong performance of SGBS may be due to the fact that
sign(θi) = 2z*(θ)i — 1 for all i and θ 〜P, a realizability condition under which SGBS has strong
guarantees (Nowak, 2011).
Jester Joke Recommendation
We now turn our attention away
from best identification of the
last two experiments '(π, θ) =
Pπ,θ (zb 6= z? (θ)), to simple re-
gret '(π,θ) = E∏,θ[hz?(θ)-
zb, θi]. We consider the Jester
jokes dataset of Goldberg et al.
(2001) that contains jokes rang-
ing innocent puns to grossly of-
Figure 7: Max {θ : ρe(θ) ≤ r}
Table 2: Average E。〜P H
Method	Average Regret
π* (OUrS)^^	3.209
SGBS	{3.180, 3.224,
	3.278, 3.263,
	3.153, 3.090}
Uncertainty	3.027
LAL	3.610
Uniform	3.877	
fensive jokes. We filter the dataset to only contain users that rated all 100 jokes, resulting in 14116
users. A rating of each joke was provided on a [-10, 10] scale which was rescaled to [-1, 1] and
observations were simulated as Bernoulli’s like above. We then clustered the ratings of these users
(see Appendix J for details) to 10 groups to obtain Z = {z(k) : k ∈ [10], z(k) ∈ {0, 1}100} where
zi(k) = 1 corresponds to recommending the ith joke in user cluster z(k) ∈ Z. Figure 7 shows the
same style plot as Figures 4,6 but for this jokes dataset, with our policy alone nearly achieving the
r-dependent baseline for all r. Mirroring the construction of the 20Q prior, we construct P by uni-
formly sampling a user and employing their θ to answer queries. Table 2 shows that despite our
policy not being trained for this setting, its performance is still among the top.
5	Related work
Learning to actively learn. Previous works vary in how the parameterize the policy, ranging from
parameterized mixtures of existing expertly designed active learning algorithms (Baram et al., 2004;
Hsu & Lin, 2015; Agarwal et al., 2016), parameterizing hyperparameters (e.g., learning rate, rate of
forced exploration, etc.) in an existing popular algorithm (e.g, EXP3) (Konyushkova et al., 2017;
Bachman et al., 2017; Cella et al., 2020), and the most ambitious, policies parameterized end-to-end
like in this work (Boutilier et al., 2020; Kveton et al., 2020; Sharaf & DaUme III, 2019; Fang et al.,
2017; Woodward & Finn, 2017). These works take an approach of defining a prior distribution
either through past experience (meta-learning) or expert created (e.g., θ 〜N(0, Σ)), and then
evaluate their policy with respect to this prior distribution. Defining this prior can be difficult, and
moreover, if the θ encountered at test time did not follow this prior distribution, performance could
suffer significantly. Our approach, on the other hand, takes an adversarial training approach and can
8
Under review as a conference paper at ICLR 2021
be interpreted as learning a parameterized least favorable prior (Wasserman, 2013), thus gaining a
much more robust policy as an end result.
Robust and Safe Reinforcement Learning: Our work is also highly related to the field of robust
and safe reinforcement learning, where our objective can be considered as an instance of minimax
criterion under parameter uncertainty (Garcia & Fernandez, 2015). Widely applied in applications
such as robotics (Mordatch et al., 2015; Rajeswaran et al., 2016), these methods train a policy in a
simulator like Mujoco (Todorov et al., 2012) to minimize a defined loss objective while remaining
robust to uncertainties and perturbations to the environment (Mordatch et al., 2015; Rajeswaran
et al., 2016). Ranges of these uncertainty parameters are chosen based on potential values that could
be encountered when deploying the robot in the real world. In our setting, however, defining the set
of environments is far less straightforward and is overcome by the adoption of the C(θ) function.
Active Binary Classification Algorithms. The literature on active learning algorithms can be
partitioned into model-based heuristics like uncertainty sampling, query by committee, or model-
change sampling (Settles, 2009), greedy binary-search like algorithms that typically rely on a form
of bounded noise for correctness (Dasgupta, 2005; Kaariainen, 2006; Golovin & Krause, 2011;
Nowak, 2011), and agnostic algorithms that make no assumptions on the probabilistic model (Das-
gupta, 2006; Hanneke, 2007b;a; Dasgupta et al., 2008; Huang et al., 2015; Jain & Jamieson, 2019).
Though the heuristics and greedy methods can perform very well for some problems, it is typically
easy to construct counter-examples (e.g., outside the assumptions) in which they catastrophically
fail (as demonstrated in our experiments). The agnostic algorithms have strong robustness guaran-
tees but rely on concentration inequalities, and consequently require at least hundreds of labels to
observe any deviation from random sampling (see Huang et al. (2015) for comparison). Therefore,
they were not included in our experiments explicitly but were represented by uniform.
Pure-exploration Multi-armed Bandit Algorithms. In the linear structure setting, for sets
X, Z ⊂ Rd known to the player, pulling an “arm” X ∈ X results in an observation(x, θ*>+ zero-
mean noise, and the objective is to identify arg maxz∈z (z, θ*) for a vector θ* unknown to the player
(Soare et al., 2014; Karnin, 2016; Tao et al., 2018; Xu et al., 2017; Fiez et al., 2019). A special case
of linear bandits is combinatorial bandits where X = {ei : i ∈ [d]} and Z ⊂ {0, 1}d (Chen et al.,
2014; Gabillon et al., 2016; Chen et al., 2017; Cao & Krishnamurthy, 2017; Fiez et al., 2019; Jain
& Jamieson, 2019). Active binary classification is a special case of combinatorial pure-exploration
multi-armed bandits (Jain & Jamieson, 2019), which we exploit in the threshold experiments. While
the above works have made great theoretical advances in deriving algorithms and information theo-
retic lower bounds that match up to constants, the constants are so large that these algorithms only
behave well when the number of measurements is very large. When applied to the instances of our
paper (only 20 queries are made), these algorithms behave no differently than random sampling.
6	Discussion and Future Directions
We see this work as an exciting but preliminary step towards realizing the full potential of this
general approach. From a practical perspective, training a ∏ can take many hours of computational
resources for even these small instances. Scaling these methods to larger instances is an important
next step. While training time scales linearly with the horizon length T , we note that one can take
multiple samples per time step with minimal computational overhead enabling problems that require
larger sample complexities. In our implementation we hard-coded the decision rule for zb given sT,
while it could also be learned as in (Luedtke et al., 2020). Likewise, the parameterization of the
policy and generator worked well for our purposes but was chosen somewhat arbitrarily-are there
more natural choices? Finally, while we focused on stochastic settings, this work naturally extends
to constrained fully adaptive adversarial sequences which is an interesting direction of future work.
9
Under review as a conference paper at ICLR 2021
Funding disclosure
Removed for anonymization purposes.
Acknowledgement
Removed for anonymization purposes.
References
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algo-
rithms. arXiv preprint arXiv:1612.06246, 2016.
VM Aleksandrov, VI Sysoyev, and SHEMENEV. VV. Stochastic optimization. Engineering Cybernetics, (5):
11-+,1968.
Philip Bachman, Alessandro Sordoni, and Adam Trischler. Learning algorithms for active learning. In Pro-
ceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 301-310. JMLR. org,
2017.
Yoram Baram, Ran El Yaniv, and Kobi Luz. Online choice of active learning algorithms. Journal of Machine
Learning Research, 5(Mar):255-291, 2004.
Craig Boutilier, Chih-Wei Hsu, Branislav Kveton, Martin Mladenov, Csaba Szepesvari, and Manzil Zaheer.
Differentiable bandit exploration. arXiv preprint arXiv:2002.06772, 2020.
Tongyi Cao and Akshay Krishnamurthy. Disagreement-based combinatorial pure exploration: Efficient algo-
rithms and an analysis with localization. stat, 2017.
Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification
bandit problem. In Conference on Learning Theory, pp. 590-604, 2016.
Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits.
arXiv preprint arXiv:2005.08531, 2020.
Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, and Ruosong Wang. Nearly optimal sampling algorithms
for combinatorial pure exploration. In Conference on Learning Theory, pp. 482-534, 2017.
Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of
multi-armed bandits. In Advances in Neural Information Processing Systems, pp. 379-387, 2014.
David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine learning,
15(2):201-221, 1994.
Sanjoy Dasgupta. Analysis of a greedy active learning strategy. In Advances in neural information processing
systems, pp. 337-344, 2005.
Sanjoy Dasgupta. Coarse sample complexity bounds for active learning. In Advances in neural information
processing systems, pp. 235-242, 2006.
Sanjoy Dasgupta, Daniel J Hsu, and Claire Monteleoni. A general agnostic active learning algorithm. In
Advances in neural information processing systems, pp. 353-360, 2008.
Meng Fang, Yuan Li, and Trevor Cohn. Learning how to active learn: A deep reinforcement learning approach.
arXiv preprint arXiv:1708.02383, 2017.
Tanner Fiez, Lalit Jain, Kevin G Jamieson, and Lillian Ratliff. Sequential experimental design for transductive
linear bandits. In Advances in Neural Information Processing Systems, pp. 10666-10676, 2019.
Victor Gabillon, Alessandro Lazaric, Mohammad Ghavamzadeh, Ronald Ortner, and Peter Bartlett. Improved
learning complexity in combinatorial pure exploration bandits. In Artificial Intelligence and Statistics, pp.
1004-1012, 2016.
Javier GarCIa and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. Journal of
Machine Learning Research, 16(1):1437-1480, 2015.
10
Under review as a conference paper at ICLR 2021
Aurelien Garivier and Emilie Kaufmann. Optimal best arm identification With fixed confidence. In Conference
on Learning Theory, pp. 998-1027, 2016.
Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Eigentaste: A constant time collaborative
filtering algorithm. information retrieval, 4(2):133-151, 2001.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning and
stochastic optimization. Journal of Artificial Intelligence Research, 42:427-486, 2011.
Ian GoodfelloW, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information process-
ing systems, pp. 2672-2680, 2014.
Steve Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th
international conference on Machine learning, pp. 353-360, 2007a.
Steve Hanneke. Teaching dimension and the complexity of active learning. In International Conference on
Computational Learning Theory, pp. 66-81. Springer, 2007b.
Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and TrendsR in Machine
Learning, 7(2-3):131-309, 2014.
Botao Hao, Tor Lattimore, and Csaba Szepesvari. Adaptive exploration in linear contextual bandit. arXiv
preprint arXiv:1910.06996, 2019.
Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In Twenty-Ninth AAAI conference on artificial
intelligence, 2015.
Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu, and Zhan Chen. Playing 20
question game With policy-based reinforcement learning. arXiv preprint arXiv:1808.07645, 2018.
Tzu-Kuo Huang, Alekh AgarWal, Daniel J Hsu, John Langford, and Robert E Schapire. Efficient and parsi-
monious agnostic active learning. In Advances in Neural Information Processing Systems, pp. 2755-2763,
2015.
Lalit Jain and Kevin G Jamieson. A neW perspective on pool-based active classification and false-discovery
control. In Advances in Neural Information Processing Systems, pp. 13992-14003, 2019.
Matti Kaariainen. Active learning in the non-realizable case. In International COnference on Algorithmic
Learning Theory, pp. 63-77. Springer, 2006.
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits. In Inter-
national Conference on Machine Learning, pp. 1238-1246, 2013.
Zohar S Karnin. Verification based solution for structured mab problems. In Advances in Neural Information
Processing Systems, pp. 145-153, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In Advances in
Neural Information Processing Systems, pp. 4225-4235, 2017.
Branislav Kveton, Martin Mladenov, Chih-Wei Hsu, Manzil Zaheer, Csaba Szepesvari, and Craig Boutilier.
Differentiable meta-learning in contextual bandits. arXiv preprint arXiv:2006.05094, 2020.
Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear
bandits. arXiv preprint arXiv:1610.04491, 2016.
Tor Lattimore and Csaba Szepesvari. Bandit algorithms. preprint, pp. 28, 2018.
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin Recht, and Ameet
TalWalkar. Massively parallel hyperparameter tuning. arXiv preprint arXiv:1810.05934, 2018.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet TalWalkar. Hyperband: A novel
bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):
6765-6816, 2017.
11
Under review as a conference paper at ICLR 2021
Alex Luedtke, Marco Carone, Noah Simon, and Oleg Sofrygin. Learning to learn from data: Using deep
adversarial learning to construct optimal statistical procedures. Science Advances, 6(9), 2020. doi: 10.1126/
sciadv.aaw2140. URL https://advances.sciencemag.org/content/6/9/eaaw2140.
Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem.
Journal of Machine Learning Research, 5(Jun):623-648, 2004.
Igor Mordatch, Kendall Lowrey, and Emanuel Todorov. Ensemble-cio: Full-body dynamic motion planning
that transfers to physical humanoids. In 2015 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS), pp. 5307-5314. IEEE, 2015.
Robert D Nowak. The geometry of generalized binary search. IEEE Transactions on Information Theory, 57
(12):7893-7906, 2011.
Jungseul Ok, Alexandre Proutiere, and Damianos Tranos. Exploration in structured reinforcement learning. In
Advances in Neural Information Processing Systems, pp. 8874-8882, 2018.
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine. Epopt: Learning robust neural
network policies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison Department
of Computer Sciences, 2009.
Amr Sharaf and Hal Daume III. Meta-Iearning for contextual bandit exploration. arXiv preprint
arXiv:1901.08159, 2019.
Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. In
Advances in Neural Information Processing Systems, pp. 1151-1160, 2019.
Max Simchowitz, Kevin Jamieson, and Benjamin Recht. The simulator: Understanding adaptive sampling in
the moderate-confidence regime. arXiv preprint arXiv:1702.05186, 2017.
Marta Soare, Alessandro Lazaric, and Remi Munos. Best-arm identification in linear bandits. In Advances in
Neural Information Processing Systems, pp. 828-836, 2014.
Chao Tao, SaUl Blanco, and Yuan Zhou. Best arm identification in linear bandits with linear dimension depen-
dency. In International Conference on Machine Learning, pp. 4877-4886, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033. IEEE, 2012.
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business Media, 2008.
Bart Van Parys and Negin Golrezaei. Optimal learning for structured bandits. 2020.
Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science & Business
Media, 2013.
Mark Woodward and Chelsea Finn. Active one-shot learning. arXiv preprint arXiv:1702.06559, 2017.
Liyuan Xu, Junya Honda, and Masashi Sugiyama. Fully adaptive algorithm for pure exploration in linear
bandits. arXiv preprint arXiv:1710.05552, 2017.
12
Under review as a conference paper at ICLR 2021
A Instance dependent sample complexity
Identifying forms of C(θ) is not as difficult a task as one might think due to the proliferation of tools
for proving lower bounds for active learning (Mannor & Tsitsiklis, 2004; Tsybakov, 2008; Garivier
& Kaufmann, 2016; Carpentier & Locatelli, 2016; Simchowitz et al., 2017; Chen et al., 2014). One
can directly extract values of C(θ) from the literature for regret minimization of linear or other
structured bandits (Lattimore & Szepesvari, 2016; Van Parys & Golrezaei, 2020), contextual bandits
(Hao et al., 2019), and tabular as well as structured MDPs (Simchowitz & Jamieson, 2019; Ok et al.,
2018). Moreover, we believe that even reasonable surrogates of C(θ) should result in a high quality
policy ∏*.
We review some canonical examples:
•	Multi-armed bandits. In the best-arm identification problem, there are d ∈ N Gaussian distri-
butions where the ith distribution has mean θi ∈ R for i = 1, . . . , d. In the above formulation,
this problem is encoded as action Xt = it results in observation yt 〜 BernoUlli(θij and the loss
'(π, θ) := E∏,θ[1{i = i*(θ)}] where i is n's recommended index and i*(θ) = argmaxi θi. It's
been shown that there exists a constant c0 > 0 such that for any sufficiently large ν > 0 we have
inf sup	'(∏,θ) ≥ exp(-c°T∕ν)
π θ:CMAB (θ)≤ν
where CMAB(θ) ：= E G*(θ) - θi)-2
i=i*(θ)
Moreover, for any θ ∈ Rd there exists a policy e that achieves '(π, θ) ≤ ci exp(-c2T/CMAB (θ))
where c1, c2 capture constant and low-order terms (Carpentier & Locatelli, 2016; Karnin et al.,
2013; Simchowitz et al., 2017; Garivier & Kaufmann, 2016).
The above correspondence between the lower bound and the upper bound suggests that CMAB (θ)
plays a critical role in determining the difficult of identifying i*(θ) for any θ. This exercise extends
to more structured settings as well:
•	Content recommendation / active search. Consider n items (e.g., movies, proteins) where the
ith item is represented by a feature vector xi ∈ X ⊂ Rd and a measurement xt = xi (e.g.,
preference rating, binding affinity to a target) is modeled as a linear response model such that
yt 〜N(hXi, θi, 1) for some unknown θ ∈ Rd. If '(π, θ) := E∏,θ[1{b = i*(θ)}] as above then
nearly identical results to that of above hold for an analogous function of CMAB (θ) (Soare et al.,
2014; Karnin, 2016; Fiez et al., 2019).
•	Active binary classification. For i = 1, . . . , d let φi ∈ Rp be a feature vector of an unlabeled
item (e.g., image) that can be queried for its binary label yi ∈ {-1,1} where yi 〜BernOUlli(θi)
for some θ ∈ Rd. Let H be an arbitrary set of classifiers (e.g., neural nets, random forest, etc.)
such that each h ∈ H assigns a label {-1, 1} to each of the items {φi}id=1 in the pool. If items
are chosen sequentially to observe their labels, the objective is to identify the true risk minimizer
h*(θ) = argminh∈H Pd=I Eθ[1{h(φi) = yi}] using as few requested labels as possible and
'(π,θ) := E∏,θ[1{h = h*(θ)}] where h ∈ H is π s recommended classifier. Many candi-
dates for C (θ ) have been proposed from the agnostic active learning literature (Dasgupta, 2006;
Hanneke, 2007b;a; Dasgupta et al., 2008; Huang et al., 2015; Jain & Jamieson, 2019) but we
believe the most granular candidates come from the combinatorial bandit literature (Chen et al.,
2017; Fiez et al., 2019; Cao & Krishnamurthy, 2017; Jain & Jamieson, 2019). To make the re-
duction, for each h ∈ H assign a z(h) ∈ {0, 1}d such that [z(h)]i := 1{h(φi) = 1} for all
i = 1,...,d and set Z = {z(h) : h ∈ H}. It is easy to check that z*(θ) := argmaxz∈z(z,θ)
satisfies z*(θ) = z(h*(θ)). Thus, requesting the label of example i is equivalent to sam-
pling from Bernoulli(hei, θi) ∈ {-1, 1}, completing the reduction to combinatorial bandits:
X = {ei : i ∈ [d]}, Z ⊂ {0, 1}d. We then apply the exact same C(θ) as above for linear
bandits.
B Gradient Estimate Derivation
Here we derive the unbiased gradient estimates (6), (7) and (8) in Algorithm 1. Since each the
gradient estimates in the above averages over M ∙ L identically distributed trajectories, it is therefore
sufficient to show that our gradient estimate is unbiased for a single problem θei and its rollout
trajectory {(Xt,yt)}tT=1.
13
Under review as a conference paper at ICLR 2021
For a feasible w, using the score-function identity (Aleksandrov et al., 1968)
Vw Ei 〜SOFTMAX(w) ['5ψ , ei)i = Ei 〜SOFTMAX(w) ['5ψ, ei) Ww Iog(SOFTMAX(W)ij .
Observe that if i 〜SOFTMAX(W) and {(xt, yt)}T=ι is the result of rolling out a policy πψ on ei
then
gw ：= Lt(∏ψ, {(xt, yt)}T=ι,ei) ∙Vw log(SOFTMAX(w)i)
is an unbiased estimate of VwEi〜SOFTMAx(w) ['(∏ψ,θi)].
「	C ♦一 . x .	.	...	… 八、
For a feasible set Θ, by definition of '(π, θ),
vΘEi〜SOFTMAX(w) ['(πψ, eiU =Ei〜SOFTMAX(w) [vΘE1∏,ei LT (π, {(xt,yt)}tT=1,θei)
=Ei〜SOFTMAX(w) [E∏,ei [vθlWπ,{(Xt,y∕}T=ι,θ∕
(10)
+ Lτ(∏,{(χt,yt)}T=ι,ei) ∙Vθiog(P∏ψθ({(χt,yt)}T=ι))]]
where the last equality follows from chain rule and the score-function identity (Aleksan-
drov et al., 1968). The quantity inside the expectations, call it gΘ, is then an unbiased
estimator of V&Ei〜SOFTMAX(w) ['(∏ψ,ei)] given i and {(χt, yt)}t=ι are rolled out accord-
ingly. Note that if Lbarrier = 0, VθLbarrier(ei, Ω) is clearly an unbiased gradient estimator of
EiYOFTMAX(W) [Eπ,ei [Lbarrier(ei, Ω)]] given i and rollout are sampled accordingly.
Likewise, for policy,
gψ ：= Lτ(∏ψ, {(xt, yt)}T=ι, ei) ∙ Vψ log(P∏ψ,ei({(χt, yt)}T=ι))
is an unbiased estimate of VψEi〜SOFTMAX(w) ['(∏ψ, ei)].
C Linear Bandit Parameterization
C.1 State Representation
We parameterize our state space S as a flattened |X | × 3 matrix where each row represents a distinct
x ∈ X . Specifically, at time t the row of st corresponding to some x ∈ X records the number of
times that action x has been taken Pts-=11 1{xs = x}, its inverse (Pts-=11 1{xs = x})-1, and the sum
of the observations Pts-=11 1{xs = x}ys.
C.2 Policy MLP Architecture
Our policy πψ is a multi-layer perceptron with weights ψ. The policy take a 3|X | sized state as
input and outputs a vector of size |X | which is then pushed through a soft-max to create a prob-
ability distribution over X . At the end of the game, regardless of the policy’s weights, we set
zb = argmaxz∈Zhz, ebi where ebis the minimum `2 norm solution to argminθ PsT=1(ys - hxs, ei)2.
Our policy network is a simple 6-layer MLP, with layer sizes {3|X |, 256, 256, 256, 256, |X |} where
3|X | corresponds to the input layer and |X | is the size of the output layer, which is then pushed
through a Softmax function to create a probability over arms. In addition, all intermediate layers
are activated with the leaky ReLU activation units with negative slopes of .01. For the experiments
for 1D thresholds and 20 Questions, they share the same network structure as mentioned above with
|X | = 25 and |X | = 100 respectively.
D Hyper-parameters
In this section, we list our hyperparameters. First we define λbinary to be a coefficient that gets
multiplied to binary loses, so instead of 1{z*(e*) = b}, we receive loss λbi∩ary ∙ 1{z?(e*) = b}. We
14
Under review as a conference paper at ICLR 2021
choose λbinary so that the recieved rewards are approximately at the same scale as SIMPLE REGRET.
During our experiments, all of the optimizers are Adam. All budget sizes are T = 20. For fairness
of evaluation, during each experiment (1D thresholds or 20 Questions), all parameters below are
shared for evaluating all of the policies. To elaborate on training strategy proposed in Algorithm 2
more, we divide our training into four procedures, as indicated in Table 3:
•	Init. The initialization procedure takes up a rather small portion of iterations primarily for the
purpose of optimizing for Lbarrier so that the particles converge into the constrained difficulty
sets. In addition, during the initialization process we initialize and freeze w = ~0, thus putting an
uniform distribution over the particles. This allows us to utilize the entire set of particles without
w converge to only a few particles early on. To initialize Θ, we sample 2/3 of the N particles
uniformly from [-1,1]|X| and the rest 1/3 of the particles by sampling, for each i ∈ [|Z|], 3^^
particles uniformly from {θ : argmaxj hθ, zj i = i}. We initialize our policy weights by Xavier
initialization with weights sampled from normal distribution and scaled by .01.
•	Regret Training, πei Training with SIMPLE REGRET objective usually takes the longest among
the Procedures. The primary purpose for this process is to let the policy converge to a reasonable
warm start that already captures some essence of the task.
•	Fine-tune πi. Training with BEST IDENTIFICATION objective run multiple times for each πi with
their corresponding complexity set Θi. During each run, we start with a warm started policy, and
reinitialize the rest of the models by running the initialization procedure followed by optimizing
the B est Identification objective.
•	Fine-tune b This procedure optimizes (2), with baselines mink '(∏k, Θ(rk)) evaluated based on
each πi learned from the previous procedure. Similar to fine-tuning each individual πi, we warm
start a policy π∖κ∕2c and reinitialize W and Θ by running the initialization procedure again.
		Experiment		
Procedure	Hyper-parameter	1D Threshold |X|=25	20 Questions |X| = 100	Jester Joke |X| = 100
Init	Nit ψ learning rate Θ learning rate w learning rate		20000 (all) 10-4 (all) 10-3 (all) 0 (all)	
Regret Training	Nit ψ learning rate Θ learning rate w learning rate		480000 (all) 10-4 (all) 10-3 (all) 10-3 (all)	
Fine-tune πi	Nit ψ learning rate Θ learning rate w learning rate		200000 (all) 10-4 (all) 10-3 (all) 10-3 (all)	
Fine-tune π*	Nit ψ learning rate Θ learning rate w learning rate	500000	300000 10-4 (all) 10-3 (all) 10-3 (all)	500000
Adam Optimizer	β1 β2		.9 (all) .999 (all)	
Table 3: Number of Iterations and Learning Rates
15
Under review as a conference paper at ICLR 2021
Procedure	Hyper-parameter	Experiment		
		1D Threshold |X| = 25	20 Questions |X| = 100	Jester Joke |X| = 100
	N	1000 X |Z|	300 × |Z|	2000 × |Z|
	M	1000	500	500
	L	10	30	30
Init + Train +	λbinary	7.5	30	30
Fine-tune	λPol-reg(regret)	.2	.8	.8
	λPol-reg(fine-tune)	.3	.8	.8
	λGen-reg	.05	.1	.05
	λbarrier		103 (all)	
Table 4: Parallel Sizes and Regularization coefficients
To provide a general strategy of choosing hyper-parameters, we note that L, firstly, λbinary, λPol-reg
are primarily parameters tuned for |X | as the noisiness and scale of the gradients, and entropy over
the arms X grows with the size |X |. Secondly, λGen-reg is primarily tuned for |Z| as it penalizes
the entropy over the N arms, which is a multiple of |Z |. Thirdly, learning rate of θ is primarily
tuned for the convergence of constraint ρ* into the restricted class, thus Lbarrier becoming 0 after
the specified number of iterations during initialization is a good indicator. Finally, we choose N
and M by memory constraint of our GPU. The hyper-parameters for each experiment was tuned
with less than 20 hyper-parameter assignments, some metrics to look at while tuning these hyper-
parameters includes but are not limited to: gradient magnitudes of each component, convergence of
each loss and entropy losses for each regularization term (how close it is to the entropy ofa uniform
probability), etc.
E Policy Evaluation
When evaluating a policy, we are essentially solving the following objective for a fixed policy π :
max '(∏,θ)
where Ω is a set of problems. However, due to non-concavity of this loss function, gradient descent
initialized randomly may converge to a local maxima. To reduce this possibility, we randomly
initialize many initial iterates and take gradient steps round-robin, eliminating poorly performing
trajectories. To do this with a fixed amount of computational resource, we apply the successive
halving algorithm from Li et al. (2018). Specifically, we choose hyperparamters: η = 4, r = 100,
R = 1600 and s = 0. This translates to:
•	Initialize ∣Θ∣ = 1600, optimize for 100 iterations for each θi ∈ Θ
•	Take the top 400 of them and optimize for another 400 iterations
•	Take the top 100 of the remaining 400 and optimize for an additional 1600 iterations
We take gradient steps with the Adam optimizer (Kingma & Ba, 2014) with learning rate of 10-3
β1 = .9 and β2 = .999.
16
Under review as a conference paper at ICLR 2021
F Figures at Full Scale
(e)*z≠g)6 1 dns (％)」。」」山
Oooo
8 6 4 2
40
60
Figure 9: Full scale of Figure 3
Figure 8: Full scale of Figure 2
17
Under review as a conference paper at ICLR 2021
20	40	60	80	100	120
r
Figure 10: Full scale of Figure 4
----π (Ours)
----SGBS, various β
----Uncertainty
----Uniform
----r-dependent baseline
----LAL
Oooooooo
7 6 5 4 3 2 1
≡6) ∙z HgIQrd-S (四」0」」山
0.5	0.6	0.7	0.8	0.9	1.0
h
Figure 11: Full scale of Figure 5
18
Under review as a conference paper at ICLR 2021
----π (Ours)
----SGBS, various β
----Uncertainty
----Uniform
r-dependent baseline
LAL
15	20	25	30
r
Figure 12: Full scale of Figure 6
JMSS忌
E<nie)∙z) dns 旬」6 3工
Jl
7
6
----π (Ours)
----SGBS, various β
----Uncertainty
----Uniform
----r-dependent baseline
----LAL
20	40	60	80	100	120
Figure 13: Full scale of Figure 7
19
Under review as a conference paper at ICLR 2021
G	Uncertainty Sampling
We define the symmetric difference of a set of binary vectors, SymDiff({z1, ..., zn}) = {i : ∃j, k ∈
[n] s.t., zj(i) = 1 ∧ zk(i) = 0}, as the dimensions where inconsistencies exist.
Algorithm 3: Uncertainty sampling in very small budget setting
1	Input: X , Z
2	for t = 1, ..., T do
3	θbt-1 = argminθ PsT=1(ys - hxs,θi)2
4	Z = {z ∈ Z : maxz0∈Z hz0, θt-1i = hz, θt-1i}
5	if |Zb| = 1 then
6	Zt = ZU{z ∈ Z ： maxz0∈(z∖Zb)hz , θt-ii = hz, θt-ιi}
7	else
8	Zbt = Zb
9	end
10	Uniformly sample	It	from SymDiff(Zbt)
11	Pull	xIt and observe	yt
12	end
H Learning to Actively Learn Algorithm
To train a policy under the learning to actively learn setting, we aim to solve for the objective
min EA 看['(πψ,θ)]
ψ	θ〜PL
where our policy and states are parameterized the same way as Appendix C for a fair comparison.
To optimize for the parameter, we take gradient steps like (8) but with the new sampling and rollout
where θi 〜P. This gradient step follows from both the classical policy gradient algorithm in
reinforcement learning as well as from recent LAL work by Kveton et al. (2020).
Moreover, note that the optimal policy for the objective must be deterministic as justified by deter-
ministic policies being optimal for MDPs. Therefore, it is clear that, under our experiment setting,
the deterministic LAL policy will perform poorly in the adversarial setting (for the same reason why
SGBS performs poorly).
I 20 Questions Setup
Hu et al. (2018) collected a dataset of 1000 celebrities and 500 possible questions to ask about each
celebrity. We chose 100 questions out of the 500 by first constructing 夕0, X0 and Z0 for the 500
dimensions data, and sampling without replacement 100 of the 500 dimensions from a distribution
derived from a static allocation. We down-sampled the number of questions so our training can run
with sufficient M and L to de-noise the gradients while being prototyped with a single GPU.
Specifically, the dataset from Hu et al. (2018) consists of probabilities of people answering Yes /
No / Unknown to each celebrity-question pair collected from some population. To better fit the
linear bandit scenario, we re-normalize the probability of getting Yes /No, conditioning on the event
that these people did not answer Unknown. The probability of answering Yes to all 500 questions for
each celebrity then constitutes vectors p0(* 1),…,p0(1000) ∈ R500, where each dimension of a give p?j
represents the probability of yes to the ith question about the jth person. The action set X0 is then
constructed as X0 = {ei : i ∈ [500]}, while Z0 = {z(j) : [z(j)] = 1{p(j) > 1/2}} ⊂ {0,1}1000 are
binary vectors taking the majority votes.
To sub-sample 100 questions from the 500, we could have uniformly at random selected the ques-
tions, but many of these questions are not very discriminative. Thus, we chose a “good” set of
queries based on the design recommended by ρ? of Fiez et al. (2019). If questions were being an-
swered noiselessly in response to a particular z ∈ Z0, then equivalently we have that for this setting
20
Under review as a conference paper at ICLR 2021
θ = 2z - 1. Since ρ? optimizes allocations λ over X0 that would reduce the number of required
queries as much as possible (according to the information theoretic bound of (Fiez et al., 2019)) if
we want to find a single allocation for all z0 ∈ Z simultaneously, we can perform the optimization
problem
2
(Pi λiXiXT )-1
)T(2z0 - 1))2 .
kz0-zk
min max max
λ∈∆(∣x∣-ι) z0∈Z0 z=z0 ((z0 - Z
We then sample elements from X0 according to this optimal λ without replacement and add them to
X until |X | = 100.
J	Jester Joke Recommendation Setup
We consider the Jester jokes dataset of Goldberg et al. (2001) that contains jokes ranging from pun-
based jokes to grossly offensive. We filter the dataset to only contain users that rated all 100 jokes,
resulting in 14116 users. A rating of each joke was provided on a [-10, 10] scale which was shrunk
to [-1,1]. Denote this set of ratings as ΘΘ = {θi : i ∈ [14116], θ% ∈ [-1,1]100}, where θ% encodes
the ratings of all 100 jokes by user i. To construct the set of arms Z, we then clustered the ratings of
these users to 10 groups to obtain Z = {zi : i ∈ [10], zi ∈ {0, 1}100} by minimizing the following
metric:
14116
min > max	hz*,θii- maxhz,θii.
Z：|Z| = 10 J z* ∈{0,1}100'	z∈z'
i=1
To solve for Z, we adapt the k - means algorithm, with the metric above instead of the L -2 metric
used traditionally.
21