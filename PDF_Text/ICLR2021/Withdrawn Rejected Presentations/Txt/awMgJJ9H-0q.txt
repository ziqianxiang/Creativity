Under review as a conference paper at ICLR 2021
Generative Learning With Euler Particle
Transport
Anonymous authors
Paper under double-blind review
Ab stract
We propose an Euler particle transport (EPT) approach for generative learning. The
proposed approach is motivated by the problem of finding the optimal transport
map from a reference distribution to a target distribution characterized by the
Monge-Ampere equation. Interpreting the infinitesimal linearization of the Monge-
Ampere equation from the perspective of gradient flows in measure spaces leads
to a stochastic McKean-Vlasov equation. We use the forward Euler method to
solve this equation. The resulting forward Euler map pushes forward a reference
distribution to the target. This map is the composition of a sequence of simple
residual maps, which are computationally stable and easy to train. The key task
in training is the estimation of the density ratios or differences that determine the
residual maps. We estimate the density ratios (differences) based on the Bregman
divergence with a gradient penalty using deep density-ratio (difference) fitting. We
show that the proposed density-ratio (difference) estimators do not suffer from the
“curse of dimensionality” if data is supported on a lower-dimensional manifold.
Numerical experiments with multi-mode synthetic datasets and comparisons with
the existing methods on real benchmark datasets support our theoretical results and
demonstrate the effectiveness of the proposed method.
1	Introduction
The ability to efficiently sample from complex distributions plays a key role in a variety of prediction
and inference tasks in machine learning and statistics (Salakhutdinov, 2015). The long-standing
methodology for learning an underlying distribution relies on an explicit statistical data model,
which can be difficult to specify in many applications such as image analysis, computer vision and
natural language processing. In contrast, implicit generative models do not assume a specific form
of the data distribution, but rather learn a nonlinear map to transform a reference distribution to the
target distribution. This modeling approach has been shown to achieve impressive performance in
many machine learning tasks (Reed et al., 2016; Zhu et al., 2017). Generative adversarial networks
(GAN) (Goodfellow et al., 2014), variational auto-encoders (VAE) (Kingma & Welling, 2014) and
flow-based methods (Rezende & Mohamed, 2015) are important representatives of implicit generative
models.
In this paper, we propose an Euler particle transport (EPT) approach for learning a generative model
by integrating ideas from optimal transport, numerical ODE, density-ratio estimation and deep neural
networks. We formulate the problem of generative learning as that of finding a nonlinear transform
that pushes forward a reference to the target based on the quadratic Wasserstein distance. Since it
is challenging to solve the resulting Monge-Ampere equation, We consider the continuity equation
derived from the linearization of the MOnge-AmPere equation, which is a gradient flows converging to
the target distribution. We solve the Mckean-Vlasov equation associated With the gradient floW using
the forward Euler method. The resulting EPT that pushes forward the reference distribution to the
target distribution is a composition of a sequence of simple residual maps, which are computationally
stable and easy to train. The residual maps are completely determined by the density ratios between
the distributions at the current iterations and the target distribution. We estimate density ratios based
on the Bregman divergence with a gradient regularizer using deep density-ratio fitting.
We establish bounds on the approximation errors due to linearization of the MOnge-AmPere equation,
Euler discretization of the Mckean-Vlasov equation, and deep density-ratio estimation. Our result on
1
Under review as a conference paper at ICLR 2021
the error rate for the proposed density-ratio estimators improves the minimax rate of nonparametric
estimation via exploring the low-dimensional structure of the data and circumvents the “curse of
dimensionality”. Experimental results on multi-mode synthetic data and comparisons with state-
of-the-art GANs on benchmark data support our theoretical findings and demonstrate that EPT is
computationally more stable and easier to train than GANs. Using simple ReLU ResNets without
batch normalization and spectral normalization, we obtained results that are better than or comparable
with those using GANs trained with such tricks.
2	Euler particle transport
Let X ∈ Rm be a random vector with distribution ν, and let Z be a random vector with distribution
μ. We assume that μ has a known and simple form. Our goal is to construct a transformation T such
that 7⅛μ = V, where 7⅛μ denotes the push-forward distribution of μ by T, that is, the distribution
of T(Z). Then we can sample from V by first generating a Z 〜μ and calculate T(Z). In practice, V
is unknown and only a random sample {Xi}in=1 i.i.d. ν is available. We must construct T based on
the sample.
There may exist multiple transports T with 7^μ = V. The optimal transport is the one that minimizes
the quadratic Wasserstein distance between μ and V defined by
W2(μ,ν) = { inf E(Z,X)〜Y[kZ - Xk2]}2,	(1)
Y∈Γ(μ,ν)
where Γ(μ, V) denotes the set of couplings of (μ, V) (Villani, 2008; Ambrosio et al., 2008). Suppose
that μ and V have densities q and P with respect to the Lesbeque measure, respectively. Then
the optimal transport map T such that 7⅛μ = V is characterized by the MOnge-AmPere equation
(Brenier, 1991; McCann, 1995; Santambrogio, 2015). Specifically, the minimization problem in
(1) admits a unique solution Y = (1, T)#μ with T = VΨ, μ-a.e., where 1 is the identity map and
VΨ is the gradient of the potential function Ψ : Rm → R. This function is convex and satisfies the
MOnge-AmPere equation
det(v2ψ(z)) = P(VΨ⅛, Z ∈ Rm	⑵
Therefore, to find the optimal transport T, it suffices to solve (2) for Ψ. However, it is challenging to
solve this degenerate elliptic equation due to its highly nonlinear nature.
Below we describe the proposed EPT method for obtaining an approximate solution of the Monge-
Ampere equation (2). It consists of the following steps: (a) linearizing (2) via residual maps, (b)
determining the velocity fields governing the stochastic McKean-Vlasov equation resulting from
the linearization, (c) calculating the forward Euler particle transport map and, (d) training the EPT
map by estimating the velocity fields from data. Since velocity fields are completely determined by
density ratios, this step amounts to nonparametric density ratio estimation. We also provide bounds
on the errors due to linearization, discretization and estimation. Mathematical details and proofs are
given in the appendix.
Linearization via residual map A basic approach to addressing the difficulty due to nonlinearity is
linearization. We use a linearization method based on the residual map
Tt,Φt =VΨ=1+tVΦt,t≥ 0,	(3)
where Φt : Rm → R1 is a function to be chosen such that the law of Tt,Φt (Z) approaches V as t
increases (Villani, 2008). We give the specific form of Φt below, see Theorem B.1 in the appendix
for details.
This linearization scheme leads to the stochastic process Xt : Rm → Rm satisfying the McKean-
Vlasov equation
-^-Xt(x) = Vt(Xt(X)), t ≥ 0, with Xo 〜μ, μ- a.e. X ∈ Rm,	(4)
where vt is the velocity vector field of Xt . In addition, we have vt = VΦt . Thus vt also determines
the residual map (3). The details of the derivation are given in Theorems B.2 and B.1. in the appendix.
Therefore, estimating the residual map (3) is equivalent to estimating vt .
2
Under review as a conference paper at ICLR 2021
The movement of Xt along t is completely governed by vt , given the initial value. We choose a vt
to decrease the discrepancy between the distribution of Xt, say μt, at time t and the target V with
respect to a properly chosen measure. An equivalent formulation of (4) is through the gradient flow
{μt}t≥o with {vt}t≥o as its velocity fields, see Proposition B.1 in the appendix. Computationally it
is more convenient to work with (4).
Determining velocity field The basic intuition is that we should move in the direction that decreases
the differences between μt and the target V. We use an energy functional L[μt] to measure such
differences. An important energy functional L[μt] is the f-divergence (Ali & Silvey, 1966),
L[μt]
Df (μtkν)
(5)
where qt is the density of μt, P is the density of V and f : R+ → R is assumed to be a twice-
differentiable convex function with f (1) = 0. We choose Φt such that L[μt] is minimized. We show
in Theorem B.1 in the appendix that Φt(x) = -f(rt(X)) and vt(x) = vΦt(x). Therefore,
vt(x) = -f 0(rt(X))Vrt(x),
where rt(x) = q(x, X ∈ Rm
p(X)
For example, ifwe use the χ2-divergence with f(c) = (c- 1)2/2, then vt(X) = vrt(X) is simply the
gradient of the density ratio. Other types of velocity fields can be obtained by using different energy
functionals such as the Lebesgue norm of the density difference, i.e., L[μt] = JRm Iqt(X) — p(x)∣2dx,
see Section B.2 for details.
The forward Euler method Numerically, we need to discretize the McKean-Vlasov equation (4).
Let s > 0 be a small step size. We use the forward Euler method defined iteratively by:
Tk = 1 + svk ,	(6)
Xk+1 = Tk(Xk),	(7)
μk + 1 = (Tk)#Mk,	⑻
where Xo 〜μ, μo = μ, Vk is the velocity field at the kth step, k = 0,1,...,K for some large K.
The particle process {Xk}k≥0 is a discretized version of the continuous process {Xt}t≥0 in (4). The
final transport map is the composition of a sequence of simple residual maps T0 , T1 . . . , TK, i.e.,
T = Tk ◦ Tκ-ι •••◦To. This updating scheme is based on the forward Euler method for solving
equation (4). This is the reason we refer to the proposed method as Euler particle transport (EPT).
Training EPT When the target V is unknown and only a random sample is available, it is natural to
learn V by first estimating the discrete velocity fields vk at the sample level and then plugging the
estimator of vk in (6). For example, if we use the f -divergence as the energy functional, estimating
vk(X) = -f00(rk (X))vrk(X) boils down to estimating the density ratios rk (X) = qk(X)/p(X)
dynamically at each iteration k. Nonparametric density-ratio estimation using Bregman divergences
and gradient regularizer are discussed in Section 4 below. Let Vk be the estimated velocity fields at
the kth iteration. The kth estimated residual map is Tk = 1 + SVk. Finally, the trained map is
^ ^ ^ ^	—
Tb = Tbκ ◦ Tbκ-1 ◦ • • • ◦ Tbo .	(9)
Theoretical guarantees We establish the following bound on the approximation error due to the
linearization of the MOnge-AmPere equation under appropriate conditions:
W2(μt,ν )= O(e-λt),	(10)
for some λ > 0, see Proposition B.1 in the appendix. Therefore, μt converges to V exponentially fast
as t → ∞. For an integer K ≥ 1 and a small s > 0, let {μS : t ∈ [ks, (k + 1)s), k = 0,..., K} be
a piecewise constant interpolation between μks and μ(k+i)s, k = 0,1,...,K. Under the assumption
that the velocity fields Vt are Lipschitz continuous with respect to (x, μt), the discretization error of
μS can be bounded in a finite time interval [0, T) as follows:
sup W2(μt,μs) = O(s).	(11)
t∈[o,T)
The proof of (11) is given in Proposition B.2 in the appendix. The error bounds (10) and (11) imply
that the distributions of the particles Xk generated by the EPT map defined in (7) with a small s and
a sufficiently large k converges to the target V at the rate of discretization size s.
3
Under review as a conference paper at ICLR 2021
When training the EPT map, we use the deep neural networks to estimate the density ratios (density
differences) with samples. In Theorem 4.1, we provide an estimation error bound that improves
the minimax rate of deep nonparametric estimation via exploring the low-dimensional structure of
data and circumvents the “curse of dimensionality.” Thus this result is of independent interest in
nonparametric estimation using deep neural networks.
3 Implementation
We now described how to implement EPT and train the optimal transport T with an i.i.d. sample
{Xi }in=1 ⊂ Rm from an unknown target distribution ν. The EPT map is trained via the forward
Euler iteration (6)-(8) with a small step size s > 0. The resulting map is a composition of a
sequence of residual maps, i.e., TK ◦ TK-1 ◦ ... ◦ T0 for a large K. As implied by Theorem 4.1
in Section 4, each Tk, k = 0,…,K can be estimated With high accuracy by Tk = 1 + Svk, where
vk(x) = -f 00(Rφ(x))VRφ(x). Here Rφ is the density-ratio estimator defined in (14) below based
on {Yi}n=ι 〜qk and the data {Xi}n=ι 〜p. Therefore, according to the EPT map (9), the particles
^ ,ʌ. ^ ^ ^ ,ʌ.
T (Yi) ≡ TbK ◦亍K-1 ◦ ... QT0(Yi),i = 1,...,n
serve as samples drawn from the target distribution ν, where particles {Yi}in=1 ⊂ Rm are sampled
from a simple reference distribution μ.
In many applications, high-dimensional complex data such as images, texts and natural languages,
tend to have low-dimensional latent features. To learn generative models with latent low-dimensional
structures, itis beneficial to have the option of first sampling particles {Zi}in=1 from a low-dimensional
reference distribution μ ∈ P2(R') with '《d. Then we apply Tb to particles Yi = Gθ(Zi),i =
1,…，n, where we introduce another deep neural network Gθ : R' → Rm with parameter θ. We can
estimate Gθ via fitting the pairs {(Zi, Yi)}in=1. We describe the EPT algorithm below.
• Outer loop for modeling low dimensional latent structure (optional)
-Sample {Zi}n=ι ⊂ R' from a low-dimensional reference distribution μ and let Yi =
Gθ(Zi), i = 1, 2, . . . , n.
- Inner loop for finding the push-forward map
τ i' .1	.	1	1 Wr	■	1
* If there are no outer loops, sample 匕 〜μ,i = 1,...,n.
.,	...,^ _____^ , , . ___________________________
*	Get v(x) = -f (Rφ(x))VRφ(x) Via solving (14) below with Yi = Yi. Set
T = 1 + Sv with a small step size s.
*	Update the particles Yi = T (Yi), i = 1, . . . , n.
- End inner loop
-If there are outer loops, update the parameter θ of Gθ(∙) via solving
mine Pn=ι kGθ(Zi)- Yik2/n.
• End outer loop
4	Deep density-ratio and density-difference fitting
The evaluation of velocity fields depends on the dynamic estimation of a discrepancy between the
push-forward distribution qt and the target distribution p. Density-ratio and density-difference fitting
with the Bregman score provides a unified framework for such discrepancy estimation without
estimating each density separately (Gneiting & Raftery, 2007; Dawid, 2007; Sugiyama et al., 2012a;b;
Kanamori & Sugiyama, 2014).
Let r(x) = q(x)/p(x) be the density ratio between a given density q(x) and the target p(x). Let
g : R → R be a differentiable and strictly convex function. The separable Bregman score with
the base probability density p for measuring the discrepancy between r and a measurable function
R : Rm → R1 is
B(r, R) = EX〜p[g0(R(X))R(X) - g(R(X))] - Exf[g0(R(X))].
4
Under review as a conference paper at ICLR 2021
Here we focus on the widely used least-squares density-ratio (LSDR) fitting with g(c) = (c - 1)2 as
a working example, i.e.,
BLSDR(r, R) = EX〜p[R(X)2] - 2Eχ〜q [R(X)] + L	(12)
For other choice of g, such as g(c) = clog c - (c + 1) log(c + 1) corresponding to estimating r via
the logistic regression (LR), and the scenario of density difference fitting will be presented in detail
in Section B.3.1.
Gradient regularizer The distributions of real data may have a low-dimensional structure with
their support concentrated on a low-dimensional manifold, which may cause the f -divergence to be
ill-posed due to non-overlapping supports. To exploit such underlying low-dimensional structures and
avoid ill-posedness, We derive a simple weighted gradient regularize1 1 Ep[g00(R)kVRk2], motivated
by recent works on smoothing via noise injection (S0nderby et al., 2017; ArjOVSky & Bottou, 2017).
This serves as a regularizer for deep density-ratio fitting. For example, with g(c) = (c - 1)2, the
resulting gradient regularizer is
Ep[kVRk22],	(13)
which recovers the well-known squared Sobolev semi-norm in nonparametric statistics. Gradient
regularization stabilizes and improves the long time performance of EPT. The detailed derivation is
presented in Section B.3.2.
LSDR estimation with gradient regularizer Let {Xi}in=1 and {Yi}in=1 be two collections of i.i.d
data from densities p(x) and q(x), respectively. Let H ≡ HD,W,S,B be the set of ReLU neural
networks Rφ with parameter φ, depth D, width W, size S, and kRφ k∞ ≤ B. We combine the least
squares loss (12) with the gradient regularizer (13) as our objective function. The resulting gradient
regularized LSDR estimator of r = p/q is given by
nn
Rφ ∈ arg min — X[Rφ(Xi)2 - 2Rφ(Yi)] + α- X ∣∣VRφ(Xi)∣∣2,	(14)
Rφ∈Hn	n
i=1	i=1
where α ≥ 0 is a regularization parameter.
Estimation error bound We first show that the density ratio r is identifiable through the objective
function by proving that, at the population level, we can recover the density ratio r via minimizing
BLαSDR(R) = BLSDR(r, R) +αEp[kVRk22]+C,
where BLSDR is defined in (12) and C = EX〜q [r2(X)] 一 1.
Lemma 4.1. For any α ≥ 0, we have r ∈ arg minR BLαSDR(R). In addition, BLαSDR(R) ≥ 0 for
any R with EX〜pR2(X) < ∞, and BaSDR(R) = 0 iff R(X) = r(x) = 1 (q,p)-a.e. X ∈ Rm.
This identifiabiity result shows that the target density ratio is the unique minimizer of the population
version of the empirical criterion in (14). This provides a the basis for establishing the convergence
result of deep nonparametric density-ratio estimation.
Next we bound the nonparametric estimation error kRφ - rkL2 (ν) under the assumptions that the
support of ν is concentrated on a compact low-dimensional manifold and r is Lipsichiz continuous.
Let M ⊆ [-c, c]m be a Riemannian manifold (Lee, 2010) with dimension m, condition number
1∕τ, volume V, geodesic covering regularity R, and m《M = O (mln(mVR∕τ))《m. Denote
M = {X ∈ [-c, c]m : inf{kX - yk2 : y ∈ M} ≤ } , ∈ (0, 1).
Theorem 4.1. Assume supp(r) = M and r(X) satisfies |r(X)| ≤ B for a finite constant B > 0 and
is Lipschitz continuous with Lipschitz constant L. Suppose the topological parameter of HD,W,S,B
in (14) with α = 0 satisfies D = O (log n), W = O(n2(2MM) / log n), S = O(n M+2 / log4 n), and
B= 2B. Then,
E{Xi,X}n=ι [kRφ - rkL2(ν)] ≤ C (B2 + cLmM)n-2/(2+M),
where C is a universal constant.
The error bound established in Theorem 4.1 for the nonparametric deep density-ratio fitting is
new. This result is of independent interest for nonparametric estimation with deep neural networks.
5
Under review as a conference paper at ICLR 2021
_____2
The above derived rate O(n- 2+Mlnm) is faster than the optimal rate of convergence for nonpara-
2
metric estimation of a LiPschitz target in Rm, where the optimal rate is O(n-2+m) (Stone, 1982;
Schmidt-Hieber, 2020) as long as the intrinsic dimension M of the data is much smaller than the
ambient dimension m. Therefore, the proposed density-ratio estimators circumvent the “curse of
dimensionality” if data is supported on a lower-dimensional manifold.
5	Related Work
We discuss connections between EPT and the existing related works. The existing generative models,
such as VAEs, GANs and flow-based methods, parameterize a transform map with a neural network,
say G, that solves
min D(G#m,v),	(15)
where D(∙, ∙) is an integral probability discrepancy. The original GAN (Goodfellow et al., 2014),
f -GAN (Nowozin et al., 2016) and WGAN (Arjovsky et al., 2017) solve the dual form of (15) by
parameterizing the dual variable using another neural network with D as the JS-divergence, the
f -divergence and the 1-Wasserstein distance, respectively. Based on the fact that the 1-Wasserstein
distance can be evaluated from samples via linear programming (Sriperumbudur et al., 2012), Liu
et al. (2018) and Genevay et al. (2018) proposed training the primal form of WGAN via a two-stage
method that solves the linear programm. SWGAN (Deshpande et al., 2018) and MMDGAN (Li et al.,
2017; Binkowski et al., 2018) use the sliced quadratic Wasserstein distance and the maximum mean
discrepancy (MMD) as D, respectively.
Vanilla VAE (Kingma & Welling, 2014) approximately solves the primal form of (15) with the
KL-divergence loss under the framework of variational inference. Several authors have proposed
methods that use optimal transport losses, such as various forms of Wasserstein distances between
the distribution of learned latent codes and the prior distribution as the regularizer in VAE to improve
performance. These methods include WAE (Tolstikhin et al., 2018), Sliced WAE (Kolouri et al.,
2019) and Sinkhorn AE (Patrini et al., 2019).
Discrete time flow-based methods minimize (15) with the KL divergence loss (Rezende & Mohamed,
2015; Dinh et al., 2015; 2017; Kingma et al., 2016; Papamakarios et al., 2017; Kingma & Dhariwal,
2018). Grathwohl et al. (2019) proposed an ODE flow approach for fast training in such methods using
the adjoint equation (Chen et al., 2018b). By introducing the optimal transport tools into maximum
likelihood training, Chen et al. (2018a) and Zhang et al. (2018) considered continuous time flow. Chen
et al. (2018a) proposed a gradient flow in measure spaces in the framework of variational inference
and then discretized it with the implicit movement minimizing scheme (De Giorgi, 1993; Jordan
et al., 1998). Zhang et al. (2018) considered gradient flows in measure spaces with time invariant
velocity fields. CFGGAN (Johnson & Zhang, 2018) derived from the perspective of optimization
in the functional space is a special form of EPT with L[∙] taken as the KL divergence. SW flow
(Liutkus et al., 2019) and MMD flow (Arbel et al., 2019) are gradient flows in measure spaces. MMD
flow can be recovered from EPT by first choosing L[∙] as the Lebesgue norm and then projecting the
corresponding velocity vector fields onto reproducing kernel Hilbert spaces, please see Appendix B.4
for a proof. However, neither SW flow nor MMD flow can model hidden low-dimensional structure
with the particle sampling procedure.
SVGD in (Liu, 2017) and the proposed EPT are both particle methods based on gradient flow in
measure spaces. However, the SVGD samples from an un-normalized density, while EPT focuses on
generative leaning, i.e., learning the distribution from samples. At the population level, projecting
the velocity fields of EPT with KL divergence onto reproducing kernel Hilbert Spaces will recover
the velocity fields of SVGD. The proof is given in Appendix B.5. Score-based methods in (Song &
Ermon, 2019; 2020; Ho et al., 2020) are also particle methods based on unadjusted Langevin flow and
deep score estimators. At the population level, the velocity fields of these score-based methods are
random since they have a Brownian motion term, while the velocity fields of EPT are deterministic.
At the sample level, these score-based methods need to learn a vector-valued deep score function.
while in EPT we need to estimate the density ratios which are scalar functions.
6
Under review as a conference paper at ICLR 2021
6	Experiments
The implementation details on numerical settings, network structures, SGD optimizers and hyper-
parameters are given in the appendix. All experiments are performed using NVIDIA Tesla K80 GPUs.
The PyTorch code of EPT is available at https://github.com/anonymous/EPT.
2D Examples. We use EPT to learn 2D distributions adapted from Grathwohl et al. (2019) with
multiple modes and density ridges. The first row in Figure 1 shows kernel density estimation (KDE)
plots of 50k samples from target distributions including (from left to right) 8Gaussians, pinwheel,
moons, checkerboard, 2spirals, and circles.
The second and third rows show the KDE plots of learned samples via EPT with f -divergence/
Lebesgue norm (left six of the second/ third row), and surface plots of estimated density ratio/
difference after 20k iterations of EPT with f -divergence/ Lebesgue norm (right six of the second/
third row), respectively. Clearly, the generated samples via EPT are nearly indistinguishable from
those of the target samples and the estimated density-ratio/ difference functions are approximately
equal to 1/0, indicating the learnt distribution matches the target well.
We further visualize the transport maps learned with 5squares and large4gaussians from 4squares
and small4gaussians, respectively. We use 200 particles connected with grey lines to manifest the
learned transport maps. As shown in the left two figures in Figures 2, the central squares of 5squares
were learned better with the gradient penalty, which is consistent with the result of the estimated
density-ratio right two figures in Figure 2. For large4gaussians, the learned transport map exhibited
some optimality under quadratic Wasserstein distance due to the obvious correspondence between
the samples in left two figures in Figure 2.
图国
Figure 1: KDE plots of the target samples (the first row), KDE plots of the learned samples via EPT
with f -divergence/ Lebesgue norm (left six of the second/third row)) and surface plots of estimated
density ratio/ difference after 20k iterations ofEPT with f -divergence/ Lebesgue norm (right six of
the second/ third row).
9BS¾
O
Figure 2: Learned transport maps (left two) and estimated density ratio (right two) in learning
5squares from 4squares and learning large4gaussians from small4gaussians.
Results on Benchmark Image Data. We show the performance of applying EPT to benchmark data
MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky & Hinton, 2009) and CelebA (Liu et al., 2015)
using ReLU ResNets without batch normalization and spectral normalization. The particle evolutions
on MNIST and CIFAR10 without using outer loop are shown in Figure 3. Clearly, EPT can transport
samples from a multivariate normal distribution into a target distribution.
We further compare EPT using the outer loop with the generative models including WGAN, SNGAN
and MMDGAN. We considered different f -divergences, including Pearson’s χ2 , KL, JS and logD
(Gao et al., 2019)) and different deep density-ratio fitting methods (LSDR and LR). Table 1 shows
FID (Heusel et al., 2017) evaluated with five bootstrap sampling of EPT with four divergences on
CIFAR10. We can see that EPT using ReLU ResNets without batch normalization and spectral
normalization attains (usually better) comparable FID scores with the state-of-the-art generative
models. Comparisons of the real samples and learned samples on MNIST, CIFAR10 and CelebA are
shown in Figure 4, where high-fidelity learned samples are comparable to real samples visually.
7
Under review as a conference paper at ICLR 2021
Figure 3: Particle evolution of EPT on MNIST and CIFAR10.
Table 1: Mean (standard deviation) of FID scores on CIFAR10. The FID score of NSCN is reported
in Song & Ermon (2019) and results in the right table are adapted from Arbel et al. (2018).
Models	CIFAR10 (50k)	Models	CIFAR10 (50k)
EPT-LSDR-χ2 EPT-LR-KL	24.9 (0.1) 25.9 (0.1)		
		WGAN-GP	31.1 (0.2)
EPT-LR-JS	25.3 (0.1)	MMDGAN-GP-L2	31.4 (0.3)
EPT-LR-logD NCSN	24.6 (0.1) 25.3	SN-GAN SN-SMMDGAN	26.7 (0.2) 25.0 (0.3)
Figure 4: Visual comparisons between real images (left 3 panels) and generated images (right 3
panels) by EPT-LSDR-χ2 on MNIST, CIFAR10 and CelebA.
7	Conclusion
EPT is a new approach for generative learning via training a transport map that pushes forward a
reference to the target. This approach uses the forward Euler method for solving the McKean-Vlasov
equation, which results from linearizing the Monge-AmPere equation that characterizes the optimal
transport map. The EPT map is a composition of a sequence of simple residual maps. The key task in
training is the estimation of density ratios that completely determine the residual maps. We estimate
density ratios based on the Bregman divergence with gradient penalty using deep density-ratio fitting.
We establish bounds on the approximation errors due to linearization, discretization, and density-ratio
estimation. Our results provide strong theoretical guarantees for the proposed method and ensure that
the EPT map converges fast to the target. We also show that the proposed density-ratio (difference)
estimators do not suffer from the “curse of dimensionality” if data is supported on a lower-dimensional
manifold. This is an interesting result in itself since density-ratio estimation is an important basic
problem in machine learning and statistics. Because EPT is easy to train, computationally stable, and
8
Under review as a conference paper at ICLR 2021
enjoys strong theoretical guarantees, we expect it to be a useful addition to the methods for generating
learning.
The proposed EPT method is motivated from the Monge-Ampere equation that characterizes the
optimal transport map. However, while the EPT map pushes forward a reference distribution to the
target, it is not an estimate of the optimal transport map itself. How to consistently estimate the
Monge-AmPere optimal map is a challenging and open problem.
9
Under review as a conference paper at ICLR 2021
References
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one
distribution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28
(1):131-142,1966.
LUigi Ambrosio, Nicola Gigli, and GiUsePPe Savare. Gradient flows: in metric spaces and in the
space of probability measures. Springer Science & Business Media, 2008.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
University Press, 2009.
Michael Arbel, DoUgal SUtherland, Mikolaj Binkowski, and ArthUr Gretton. On gradient regUlarizers
for MMD GANs. In NIPS, 2018.
Michael Arbel, Anna Korba, Adil Salim, and ArthUr Gretton. MaximUm mean discrePancy gradient
flow. In NeurIPS, 2019.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In ICLR, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In ICML, 2017.
Vladimir Igorevich Arnold. Geometrical methods in the theory of ordinary differential equations,
volume 250. Springer Science & Business Media, 2012.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3:463-482, 2002.
Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
Research, 20:1-17, 2019.
Mikolaj Binkowski, Dougal J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD
GANs. In ICLR, 2018.
Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Commu-
nications on Pure and Applied Mathematics, 44(4):375-417, 1991.
Changyou Chen, Chunyuan Li, Liqun Chen, Wenlin Wang, Yunchen Pu, and Lawrence Carin Duke.
Continuous-time flows for efficient inference and density estimation. In ICML, 2018a.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differen-
tial equations. In NIPS, 2018b.
Frank H Clarke. Optimization and nonsmooth analysis, volume 5. Siam, 1990.
A Philip Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical
Mathematics, 59(1):77-93, 2007.
E De Giorgi. New problems on minimizing movements, boundary value problems for partial
differential equations. Results in Applied Mathematics, 29:81-98, 1993.
Ishan Deshpande, Ziyu Zhang, and Alexander G Schwing. Generative modeling using the sliced
wasserstein distance. In CVPR, 2018.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. In ICLR, 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. In
ICLR, 2017.
Yuan Gao, Yuling Jiao, Yang Wang, Yao Wang, Can Yang, and Shunkang Zhang. Deep generative
learning via variational gradient flow. In ICML, 2019.
10
Under review as a conference paper at ICLR 2021
Izrail Moiseevitch Gelfand and Sergei Vasilevich Fomin. Calculus of variations. Dover Publications,
2000.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn diver-
gences. In ICML, 2018.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of theAmerican statistical Association, 102(477):359-378, 2007.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD:
Free-form continuous dynamics for scalable reversible generative models. In ICLR, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in
Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.
Richard Holley and Daniel Stroock. Logarithmic sobolev inequalities and stochastic ising models.
Journal of Statistical Physics, 46:1159-1194, 1987.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, 2015.
Rie Johnson and Tong Zhang. Composite functional gradient learning of generative adversarial
models. In ICML, 2018.
Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the Fokker-Planck
equation. SIAM Journal on Mathematical Analysis, 29(1):1-17, 1998.
Takafumi Kanamori and Masashi Sugiyama. Statistical analysis of distance estimators with density
differences and density ratios. Entropy, 16(2):921-942, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
NIPS, 2018.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improved variational inference with inverse autoregressive flow. In NIPS, 2016.
Soheil Kolouri, Phillip E Pope, Charles E Martin, and Gustavo K Rohde. Sliced-Wasserstein
autoencoder: An embarrassingly simple generative model. In ICLR, 2019.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
John Lee. Introduction to Riemannian Manifolds. Springer, 2010.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and BarnabaS Poczos. MMD GAN:
Towards deeper understanding of moment matching network. In NIPS, 2017.
Huidong Liu, GU Xianfeng, and Dimitris Samaras. A two-step computation of the exact gan
Wasserstein distance. In ICML, 2018.
Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information
Processing Systems, volume 30, pp. 3115-3123. Curran Associates, Inc., 2017.
11
Under review as a conference paper at ICLR 2021
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
ICCV, 2015.
Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, Fabian-Robert Stoter, KamaIika
Chaudhuri, and Ruslan Salakhutdinov. Sliced-Wasserstein flows: Nonparametric generative
modeling via optimal transport and diffusions. In ICML, 2019.
Robert J. McCann. Existence and uniqueness of monotone measure-preserving maps. Duke Mathe-
matiCalJournal, 80(2):309-324,1995.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In ICLR, 2018.
Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv
preprint arXiv:1610.03483, 2016.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f -GAN: Training generative neural samplers
using variational divergence minimization. In NIPS, 2016.
F. Otto and C. Villani. Generalization of an inequality by talagrand and links with the logarithmic
sobolev inequality. Journal of Functional Analysis, 173:261-400, 2000.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In NIPS, 2017.
Giorgio Patrini, Samarth Bhargav, Rianne van den Berg, Max Welling, Patrick Forre, Tim Genewein,
Marcello Carioni, KFU Graz, Frank Nielsen, and CSL Sony. Sinkhorn autoencoders. In UAI, 2019.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In ICML, 2016.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
ICML, 2015.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In NIPS, 2017.
Ruslan Salakhutdinov. Learning deep generative models. Annual Review of Statistics and Its
Application, 2:361-385, 2015.
Filippo Santambrogio. Optimal transport for applied mathematicians. Springer, 2015.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. The Annals of Statistics, in press, 2020.
Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation characterized by
number of neurons. arXiv preprint arXiv:1906.05497, 2019.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised
map inference for image super-resolution. In ICLR, 2017.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
Advances in Neural Information Processing Systems, volume 33. Curran Associates, Inc., 2020.
Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schlkopf, and Gert R. G
Lanckriet. On the empirical estimation of integral probability metrics. Electronic Journal of
Statistics, 6:1550-1599, 2012.
Charles J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals of
Statistics, 10(4):1040-1053, 1982.
12
Under review as a conference paper at ICLR 2021
Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus D Plessis, Song Liu, and Ichiro
Takeuchi. Density-difference estimation. In NIPS, 2012a.
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine
learning. Cambridge University Press, 2012b.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR, 2016.
I Tolstikhin, O Bousquet, S Gelly, and B SchOlkopf. Wasserstein auto-encoders. In ICLR, 2018.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Linfeng Zhang, Weinan E, and Lei Wang. Monge-AmPere flow for generative modeling. arXiv
preprint arXiv:1809.10188, 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In ICCV, 2017.
13
Under review as a conference paper at ICLR 2021
APPENDIX
In the appendix, we provide the implementation details on numerical settings, network structures,
SGD optimizers, and hyper-parameters in the paper. We show the numerical convergence of EPT with
simulated datasets and compare the learning and inference of EPT with other generative models. We
give detailed theoretical background and proofs of the results mentioned in the paper. We also provide
proofs MMD flow and SVGD can be derived from EPT by choosing appropriate f -divergences.
A	Appendix: Numerical experiments
A. 1 Implementation details, network structures, hyper-parameters
We provide the details of two versions of the EPT algorithm, EPTv1 in Algorithm 1 and EPTv2 in
Algorithm 2 below. In Algorithm 1, we describe the algorithm without outer loops. In Algorithm 2,
we describe the algorithm with a latent structure and outer loops.
A.1.1 2D examples
Experiments on 2D examples in our work were performed with deep LSDR fitting and the Pearson χ2
divergence. We use the EPTv1 (Algorithm 1) without outer loops. In inner loops, only a multilayer
perceptron (MLP) was utilized for dynamic estimation of the density ratio between the model
distribution qk and the target distribution p. The network structure and hyper-parameters in EPT
and deep LSDR fitting were shared in all 2D experiments. We adopt EPT to push particles from a
predrawn pool consisting of 50k i.i.d. Gaussian particles to evolve in 20k steps. We used RMSProp
with the learning rate 0.0005 and the batch size 1k as the SGD optimizer. The details are given in
Table A1 and Table A2. We note that s is the step size, n is the number of particles, α is the penalty
coefficient, and T is the mini-batch gradient descent times of deep LSDR fitting or deep logistic
regression in each inner loop hereinafter.
Table A1: MLP for deep LSDR fitting.
Layer	Details	Output size
1	Linear, ReLU	64
2	Linear, ReLU	64
3	Linear, ReLU	64
4	Linear	1
Table A2: Hyper-parameters in EPT on 2D examples.
Parameter s	n α	T
Value	0.005	50k	0 or 0.5	5
A.1.2 Real image data
Datasets. We evaluated EPT on three benchmark datasets including two small datasets MNIST,
CIFAR10 and one large dataset CelebA from GAN literature. MNIST contains a training set of 60k
examples and a test set of 10k examples as 28 × 28 bilevel images which were resized to 32 × 32
resolution. There are a training set of 50k examples and a test set of 10k examples as 32 × 32 color
images in CIFAR10. We randomly divided the 200k celebrity images in CelebA into two sets for
training and test according to the ratio 9:1. We also pre-processed CelebA images by first taking a
160 × 160 central crop and then resizing to the 64 × 64 resolution. Only the training sets are used to
train our models.
14
Under review as a conference paper at ICLR 2021
Algorithm 1: EPTv1: Euler particle transport			
Input: K ∈ N*, s > 0, α > 0 regularization coeficient		// maximum loop count,	step size,
X k w	(i 〜ν, Yi0 〜μ, i = 1, 2, •一,n	// real samples, initial J 0 hile k < K do Rφ ∈ argminRφ 1 P"=1[Rφ(Xi)2 + α∣VRφ(Xi)k2 - 2Rφ(Yik)] viaSGD // determine the density ratio		particles
	vk (χ) = -f00(Rφ(χ))vRφ(χ)	// approximate the velocity field	
	Tbk = 1 + sVk	// define the forward	Euler map
e	Yik+1 = Tbk(Yk), i = 1, 2,…，n kJk+1 nd	// update	particles
Output: Yik 〜μk, i = 1, 2,…，n		// transported	particles
Evaluation metrics. FrechetInception Distance (FID) (Heusel et al., 2017) computes the Wasserstein
distance W2 with summary statistics (mean μ and variance Σ) of real samples Xs and generated
samples gs in the feature space of the Inception-v3 model (Szegedy et al., 2016), i.e., FID =
∣∣μx - μg k2 + Tr(Σχ + Σg - 2(ΣχΣg) 1). Here, FID is reported with the TensorFlow implementation
and lower FID is better.
Network architectures and hyper-parameter settings. We employed the ResNet architectures
used by Gao et al. (2019) in our EPT algorithm. Especially, the batch normalization (Ioffe & Szegedy,
2015) and the spectral normalization (Miyato et al., 2018) of networks were omitted for EPT-LSDR-
χ2 . To train neural networks, we set SGD optimizers as RMSProp with the learning rate 0.0001
and the batch size 100. Inputs {Zi}in=1 in EPTv2 (Algorithm 2) were vectors generated from a
128-dimensional standard normal distribution on all three datasets. Hyper-parameters are listed in
Table A3 where IL expresses the number of inner loops in each outer loop. Even without outer loops,
EPTv1 (Algorithm 1) can generate images on MNIST and CIFAR10 as well by making use of a large
set of particles. Table A4 shows the hyper-parameters.
Table A3: Hyper-parameters in EPT with outer loops on real image datasets.
Parameter	`	s	n	α	T	I L
Value	128	0.5	1k	0	1	20
Table A4: Hyper-parameters in EPT without outer loops on real image datasets.
Parameter	s	n	α	T
Value	0.5	4k	0	5
A.2 Numerical convergence
We illustrate the convergence property of the learning dynamics of EPTv1 on synthetic datasets
pinwheel, checkerboard and 2spirals. As shown in Figure 5, on the three test datasets, the dynamics
of both the estimated LSDR fitting losses in (14) with α = 0 and the estimated value of the gradient
norms Eχ^qk [∣VRφ(X)∣∣2] demonstrate the estimated LSDR loss converges to the theoretical value
-1.
15
Under review as a conference paper at ICLR 2021
Algorithm 2: EPTv2: Euler particle transport with latent structure
Input: IL, OL ∈ N*, s > 0, α > 0
// maximum inner loop count, maximum
outer loop count, step size, regularization coeficient
Xi 〜V, i = 1, 2,…，n
b0	init
Gθ - Gθ
j 一 0
/* outer loop */
while j < OL do
Zj 〜μ, i = 1, 2,...,n
Yi0 = Gbθ(Zj), i=1, 2,…，n
k — 0
// real samples
// initialize the transport map
// latent particles
// intermediate particles
/* inner loop */
while k < IL do
Rφ ∈ argminRφ 1 £乙及(%)2 + α∣∣VRφ(Xi)∣∣2 - 2Rφ(Yk)] ViaSGD
// determine the density ratio
vk(x) = —f00(Rφ(X))VRφ(x)	// approximate the velocity field
Tbk = 1 + Svk	/ / define the forward Euler map
Yik+1 = Tk(Yik), i = 1, 2, ∙∙∙ ,n	// update particles
k - k + 1
end
Gbθ+1 ∈ argminGθ 1 Pn=I l∣Gθ(Zj) — YiILk2 ViaSGD // fit the transport
map
j - j + 1
end
Output： GOL : R' → Rd	// transport map with latent structure
-HHaS-I
50 IOO 150	200
---pmwheeɪ
Checterboard
—2spirals
12000	14000	16000	180∞	20000
k
pinwheel
Cheeterboarti
2sρirals
Figure 5: The numerical convergence phenomenon of EPTv1 on simulated datasets. First row: LSDR
fitting loss (14) with α = 0 v.s. iterations on pinwheel, checkerboard and 2spirals. Second row:
Estimation of the gradient norm EX 〜qk [∣∣VRφ(X) ∣∣2] v.s. iterations on pinwheel, checkerboard and
2spirals.
O IOOO 2000	3000	40∞	5000
k
A.3 Learning and inference
The learning process of EPT performs particle evolution via solving the McKean-Vlasov equation
using forward Euler iterations. The iterations rely on the estimation of the density ratios (difference)
between the pushforward distributions and the target distribution. To make the inference of EPTv1
more amendable, we propose EPTv2 based on EPTv1. EPTv2 takes advantage of a neural network to
fit the pushforward map. The inference of EPTv2 is fast since the pushforward map is parameterized
as a neural network and only forward propagation is involved. These aspects distinguish EPTv2 from
score-based generative models Song & Ermon (2019; 2020) which simulate Langevin dynamics to
generate samples.
16
Under review as a conference paper at ICLR 2021
B Appendix: Theoretical Backgrounds and Proofs
B.1 Theoretical backgrounds for Section 2
For convenience, we first give the following notation to be used in this section. Let P2 (Rm)
denote the space of Borel probability measures on Rm with finite second moments, and let
P2a(Rm) denote the subset of P2 (Rm) in which measures are absolutely continuous with respec-
t to the Lebesgue measure (all distributions are assumed to satisfy this assumption hereinafter).
TanμP2(Rm) denotes the tangent space to P2(Rm) at μ. Let ACloc(R+,P2(Rm)) := {μt : I →
P2(Rm) is absolutely continuous, ∣μt∣ ∈ L2 (I),I ⊂ R+}. LiPloc(Rm) denotes the set of func-
tions that are Lipschitz continuous on any compact set of Rm. For any ' ∈ [1, ∞], We use L'(μ, Rm)
(Ll'oc(μ,Rm)) to denote the L' space of μ-measurable functions on Rm (on any compact set of
Rm). With 1, det and tr, we refer to the identity map, the determinant and the trace. We use V, V2
and ∆ to denote the gradient or Jacobian operator, the Hessian operator and the Laplace operator,
respectively.
We are now ready to describe the proposed method in a mathematically rigorous fashion and provide
theoretical guarantees. Let X 〜q, X = Tt,φ(X), and denote the distribution of X as q. With a
small t, the map Tt,φ is invertible according to the implicit function theorem, and we have the change
of variables formula
det(V2 Ψ)(x) = ∣det(VTt,φ)(x)∣ = q(x),	(B-1)
q(x)
where
X = Tt,φ(x).	(B-2)
Using the fact 今：=。det(A + tB) = det(A)tr (A-1B) ∀A, B ∈ Rm×m with A invertible, and
applying the first order Taylor expansion to (B-1), we have
log e(X) - log q(x) = -t∆Φ(x) + o(t).	(B-3)
Let t → 0 in (B-2) and (B-3), we obtain a random process {xt} and its law qt satisfying
dxt = VΦ(xt), with xo 〜q,	(B-4)
dlndt(Xt) = -∆Φ(xt), with qo = q.	(B-5)
Equations (B-4) and (B-5) resulting from linearization of the MOnge-AmPere equation (2) can be
interpreted as gradient flows in measure spaces (Ambrosio et al., 2008). And thanks to this connection,
we can resort to solving a continuity equation characterized by a type of McKean-Vlasov equation,
an ODE system that is easier to handle.
B.2 GRADIENT FLOWS IN P2a(Rm)
For μ ∈ Pa (Rm) with density q, let
L[μ]
F (q(X))dX
Rm
: P2a(Rm) → R+ ∪ {0}
(B-6)
be an energy functional satisfying V ∈ arg min L[∙], where F(∙) : R+ → R1 is a twice-differentiable
convex function. Among the widely used metrics on P2a (Rm) in implicit generative learning, the
following two are important examples of L[∙] : (1) f -divergence given in (5) (Ali & Silvey, 1966); (2)
Lebesgue norm of density difference:
kμ -
2
νkL2(Rm) =
Rm
|q(X) - p(X)|2dX.
(B-7)
Definition. We call {μt}t∈R+ ⊂ ACloc(R+, P2(Rm)) a gradient flow of the functional L[∙], if
{μt}t∈R+ ⊂ Pa(Rm) a.e., t ∈ R+ and the velocity vector field Vt ∈ TanμtP2(Rm) satisfies
Vt ∈ -∂L[μt] a.e. t ∈ R+, where ∂L[∙] is the subdifferential of L[∙].
The gradient flow {μt}t∈R+ of LH enjoys the following nice properties.
17
Under review as a conference paper at ICLR 2021
Proposition B.1.	(i) The following continuity equation holds in the sense of distributions.
∂
∂tμt = -▽ ∙ (μtvt) in R+ X Rm With μo = μ,	(b-8)
(ii)	Energy decay along the gradient flow:奈 L[μt] = -∣WtkL2(μ亡 Rm)	a.e. t ∈ R+. In addition,
W2(μt, V) = O(exp-λt), if L[μ] is λ-geodetically convex with λ > 0 1.
(iii)	Conversely, if {μt}t is the solution ofcontinuity equation (B-8) in (i) with vt(x) specified by
(B-9) in (ii), then {μt}t is a gradient flow of L[∙].
Remark B.1. In part (ii) of Proposition B.1, for general f -divergences, we assume the functional L
to be λ-geodesically convex for the convergence of μt to the target V in the quadratic Wasserstein
distance. However, for the KL divergence, the convergence can be guaranteed if ν satisfies the log-
Sobolev inequality(Otto & Villani, 2000). In addition, the distributions that are strongly log-concave
outside a bounded region, but not necessarily log-concave inside the region satisfy the log-Sobolev
inequality, see, for example, Holley & Stroock (1987). Here the functional L can even be nonconvex,
an example includes the densities with double-well potential.
Remark B.2. Equation (8.48) in Proposition 8.4.6 of and Ambrosio et al. (2008) shows the connec-
tion (locally) of the velocity Vt of the gradient flow μt and the optimal transport along μt, i.e., let
Tμμt+h be the optimal transport from μt to μt+h, then Tμμt+h = I + hvt + o(h) in LP. So locally,
I + hvt approximates the optimal transport mapfrom μt to μt+h on [t, t + h] for a small h.
Proof. (i) The continuity equation (B-8) follows from the definition of the gradient flow directly, see,
page 281 in (Ambrosio et al., 2008). (ii) The first equality follows from the chain rule and integration
by part, see, Theorem 24.2 of Villani (2008). The second one on linear convergence follows from
Theorem 24.7 of Villani (2008), where the assumption on λ in equation (24.6) is equivalent to
the λ-geodetically convex assumption here. (iii) Similar to (i) see, page 281 in Ambrosio et al.
(2008).	□
Theorem B.1. (i) Representation ofthe velocity fields: ifthe density qt of μt is differentiable, then
vt(x) = -VF0(qt(x)) μt-a.e. X ∈ Rm.	(B-9)
(ii) Ifwe let Φ be time-dependent in (B-4)-(B-5),i.e., Φt, then the linearizedMonge-Ampere equations
(B-4)-(B-5) are the same as the continuity equation (B-8) by taking Φt(x) = -F0(qt(x)).
Proof. (i) Recall L[μ] is a functional on Pa(Rm). By the classical results in calculus of variation
(Gelfand & Fomin, 2000),
∂qq(X) = dtL[q + tg] ∣t=0= F 0(q(X)),
where dL[[q] denotes the first order of variation of L[∙] at q, and q, g are the densities of μ and an
arbitrary ξ ∈ P2a (Rm), respectively. Let
LF (z) = zF0(z) - F(z) : R1 → R1.
Some algebra shows,
VLF(q(X)) = q(X)VF 0(q(X)).
Then, it follows from Theorem 10.4.6 in (Ambrosio et al., 2008) that
VF 0(q(x)) = ∂oL(μ),
1 We say that L is λ-geodetically convex if there exists a constant λ > 0 such that for every μι, μ2 ∈
Pa(Rm), there exists a constant speed geodestic Y : [0,1] → Pa(Rm) such that γo = μι ,γι = μ2 and
L(Ys) ≤ (1 - s)L(μι) + sL(μ2) - 2s(1 - s)d(μ1,μ2), ∀s ∈ [0,1],
where d is a metric defined on P2a (Rm) such as the quadratic Wasserstein distance.
18
Under review as a conference paper at ICLR 2021
where, ∂oL(μ) denotes the one in ∂L(μ) with minimum length. The above display and the definition
of gradient flow implies the representation of the velocity fields vt .
(ii) The time dependent form of (B-4)-(B-5) reads
dxt
K = vΦt(xt), With X0 〜q,
dlndt(xt) = -∆Φt(χt), with qo = q.
By chain rule and substituting the first equation into the second one, we have
-1(dqt + 普罂)=1(dqt + Vq"Φt(xt))
qt dt	dxt dt	qt dt
= -∆Φt(xt),
which implies,
dqt = -qt∆Φt(xt) - VqtVΦt(xt) = -V∙ (qtVΦt).
By (B-9), the above display coincides with the continuity equation (B-8) with vt = VΦt =
-VF 0(qt(x)).	□
Theorem B.1 and Proposition B.1 imply that {μt}t, the solution of the continuity equation (B-8) with
vt(x) = -VF0(qt(x)), converges rapidly to the target distribution ν. Furthermore, the continuity
equation has the following representation under mild regularity conditions on the velocity fields.
Theorem B.2. Assume ∣∣vt∣∣Li(μt,Rm) ∈ Lk(R+) and vt(∙) ∈ LiPloc(Rm) with upper bound Bt
and Lipschitz constant Lt such that (Bt + Lt) ∈ Ll1oc(R+). Then the solution of the continuity
equation (B-8) can be represented as μt = (Xt)#M, where Xt(x) : R+ X Rm → Rm satisfies the
McKean-Vlasov equation (4).
Proof. The Lipschitz assumption of vt implies the existence and uniqueness of the McKean-Vlasov
equation (4) according to the classical results in ODE (Arnold, 2012). By the uniqueness of the
continuity equation, see Proposition 8.1.7 in Ambrosio et al. (2008), it is sufficient to show that
μt = (Xt)#M satisfies the continuity equation (B-8) in a weak sense. This can be done by the
standard test function and smoothing approximation arguments, see, Theorem 4.4 in Santambrogio
(2015) for details.	□
As shown in Lemma B.1 below, the velocity fields associated with the f -divergence (5) and the
Lebesgue norm (B-7) are determined by density ratio and density difference respectively.
Lemma B.1. The velocity fields vt satisfy
-f0 Wt(Xy)VYt(X) L[μ] = Df (μkν), where rt(x) = qt^,
—2Vdt(x), L[μ] = kμ —沙信邯小)，where dt(x) = qt(x) -p(x).
(
vt(X)
Proof. By definition,
(
F(qt(X))
p(x)f(需),L[μ]= Df(μkν),
(qt(x) -P(X))2, L[μ] = kμ -必任俚皿”
Direct calculation shows
F0(qt(X)) =
f(需),L[μ] = Df(μkν),
2(qt(x) - P(X)) , L[μ] = kμ - V kL2(Rm).
Then, the desired result follows from the above display and (B-9).
□
Several methods have been developed to estimate density ratio and density difference in the literature.
Examples include probabilistic classification approaches, moment matching and direct density-ratio
(difference) fitting, see Sugiyama et al. (2012a;b); Kanamori & Sugiyama (2014); Mohamed &
Lakshminarayanan (2016) and the references therein.
19
Under review as a conference paper at ICLR 2021
Proposition B.2. Suppose that the velocity fields Vt are Lipschitz continuous with respect to (x, μt),
that is, there exists a finite constant Lv > 0 such that
∣∣vt(x) - V*(χ)k ≤ Lv[∣∣x - xk + W2(μt, μ∕],t,力 > 0 and x, X ∈ R .	(B-10)
Then for any finite T > 0, the bound (11) on the discretization error holds:
sup W2(μt,μs) = O(s).
t∈[0,T]
Remark B.3. Ifwe take f(x) = (X — 1)2/2 in Lemma B.1, then the velocity fields Vt(X) = Vrt(x),
where rt(x) = qt(x)/p(x). In the proof of Theorem B.1, part (ii), it is shown that qt satisfies
dqt/dt = —V ∙ (qtVΦt). Thusfor this simple f -divergencefunction, the verification of the Lipschitz
condition (B-10) amounts to verifying that Vrt(x) is Lipschitz in the sense of (B-10).
Proof. Without loss of generality let K =f > 1 be an integer. Recall {μt t ∈ [ks, (k + 1)s) is the
piecewise constant interpolation between μk and μk+ι defined as
μS = (Ttk,s)#〃k,
where,
Ttk,s = 1 + (t - ks)Vk,
μk is defined in (16)-(18) with Vk = Vks, i.e., the continuous velocity in (B-9) at time ks, k =
0,.., K — 1, μo = μ. Under assumption (B-10) we can first show in a way similar to the proof of
Lemma 10 in Arbel et al. (2019) that
W2 (μks, μk ) =O(S).
(B-11)
Let Γ be the optimal coupling between μk and μks, and (X, Y)〜 Γ. Let Xt = Ttk,s(X) and Yt be
the solution of (4) with X0 = Y and t ∈ [ks, (k + 1)s). Then
Xt 〜μs, Yt 〜μt
and
Yt = Y + Zkt Vt(Yt)dt.
It follows that
W2(μt,μks)	≤ E[k匕-Yk2]
=E[k /t W(Yτ)d训2]
ks
≤ E[((kw(YDk2dty]
≤ O(s2).
(B-12)
where, the first inequality follows from the definition of W2 , and the last equality follows from the
the uniform bounded assumption of Vt . Similarly,
W2(μk,μS)	≤ E[∣X - Xtk2]
= E[k(t - ks)Vk(X)k22]
≤ O(s2).	(B-13)
Then,
W2(μt, μs) ≤ W2(μt, μks) + W2(μks,μk) + W2(μk,μs)
≤ O(s),
where the first inequality follows from the triangle inequality, see for example Lemma 5.3 in
Santambrogio (2015), and the second one follows from (B-11)-(B-13).	□
20
Under review as a conference paper at ICLR 2021
B.3 Derivation and Proofs of the results in Section 4.
B.3. 1 B regman score for Density ratio/Difference
The separable Bregman score with the base probability measure p to measure the discrepancy between
a measurable function R : Rm → R1 and the density ratio r is
Bratio(r,R) = EX〜p[g0(R(X))(R(X) - r(X)) - g(R(X))]
=EX〜p[g0(R(X))R(X) - g(R(X))] - Exf[g0(R(X))].
It can be verified that Bratio(r, R) ≥ Bratio(r, r), where the equality holds iff R = r.
For deep density-difference fitting, a neural network D : Rm → R1 is utilized to estimate the
density-difference d(x) = q(x) - p(x) between a given density q and the target p. The separable
Bregman score with the base probability measure w to measure the discrepancy between D and d
can be derived similarly,
Bdiff(d,D) = EX〜p[w(XW(D(X))] - Eχ-q[w(XH(D(X))]
+ EX〜w[g0(D(X))D(X) - g(D(X))].
Here, we focus on the widely used least-squares density-ratio (LSDR) fitting with g(c) = (c - 1)2 as
a working example for estimating the density ratio r. The LSDR loss function is
BLSDR(r, R)= EX〜p[R(X)2] - 2Ex〜q [R(X)] + 1.
B.3.2 Gradient Penalty
We consider a noise convolution form of Bratio (r, R) With Gaussian noise E 〜N(0, αI),
Baatio(r, R) = EX〜pEJg0(R(X + E))R(X + E)- g(R(X + E))] - EX〜qEJg0(R(X + €))].
Taylor expansion applied to R gives
Ee[R(x + e)] = R(x) + α ∆R(x) + O(a2).
Using equations (13)-(17) in Roth et al. (2017), We get
BOItio(r,R) ≈ Bratio(r,R) + 2Ep[g00(R)kVRk2],
i.e., 1 Ep[g00(R)kVRk2] serves as a regularizer for deep density-ratio fitting when g is twice differen-
tiable.
B.3.3 Proofs in Section 4
Below we prove Theorem 4.1 in Section 4.
LemmaB.2. For given densities p(x) and q(x), let r(x) = q(x)∕p(x) with C = Ex^ [r(X)] -1 <
∞. For any α ≥ 0, define a nonnegative functional
BLαSDR(R) = BLSDR(r, R) + αEp[kVRk22] + C, R is measurable.
Then, r ∈ arg minR B0LSDR(R). Moreover, Bα (R) = 0 iff R(x) = r(x) = 1, (q, p)-a.e. x ∈ Rm.
Proof. By definition, it is easy to check
B0LSDR(R) = Bratio (r, R) - Bratio (r, r),
where Bratio(r, R) is the Bregman score with the base probability measure p between R and r. Then
r ∈ arg minmeasureable R B0LSDR(R) follow from the fact Bratio(r, R) ≥ Bratio(r, r) and the equality
holds iff R = r. Since
Bα(R) = B0L
SDR(R) + αEp[kVRk22] ≥0,
Then,
Bα (R) = 0
iff
B0LSDR(R) = 0 and Ep [kVRk22] = 0,
which is further equivalent to
R = r = constant (q, p)-a.e. ,
and the constant = 1 since r is a density ratio.
□
21
Under review as a conference paper at ICLR 2021
Theorem B.3. Assume supp(r) = M and r(x) is Lipschitz continuous with the bound B and the
Lipschitz constant L. Suppose the topological parameter of HD,W,S,B in (20) with α = 0 satisfies
D = O (log n), W = O(n 2(2+M) / log n), S = O(n M+2 / log4 n), and B = 2B. Then,
E{Xi,Yi}n[kRφ - rkL2(ν)] ≤ C(B2 + cLmM)n-2/(2+M),
where C is a universal constant.
Proof. We use B(R) to denote B0LSDR - C for simplicity, i.e.,
B(R) = EX 〜p[R(X )2] — 2Eχ 〜q [R(X)].	(B-14)
Rewrite (20) with α = 0 as
n1
Rbφ ∈ arg min	B(Rφ) = V-(Rφ(Xi)2 - 2Rφ(Yi)).
Rφ∈HD,W,S,B	n
i=1
(B-15)
By Lemma B.2 and Fermat’s rule (Clarke, 1990), we know 0 ∈ ∂B(r). Then, ∀R direct calculation
yields,
kR - rk2L2(ν) = B(R) - B(r) - h∂B(r),R-ri = B(R) - B(r).	(B-16)
∀Rφ ∈ Hdw,s,B We have,
kRbφ - rk2L2(ν) = B(Rbφ) - B(r)	(B-17)
= B(Rφ)- B(Rφ) + B(Rφ)- B(Rφ)
+ B(Rφ)- B(Rφ) + B(Rφ)- B(r)
≤ 2 sup	B(R)- B (R)| + ∣∣Rφ-r∣∣L2(ν),
R∈HD,W,S,B
Where the inequality uses the definition of Rφ, Rφ and (B-16). We prove the theorem by upper
bounding the expected value of the right hand side term in (B-17). To this end, We need the folloWing
auxiliary results (B-18)-(B-20).
E{Zi}in[sup|B(R) - Bb (R)|] ≤ 4C1(2B + -)G(H),	(B-18)
R
Where
-n
G(H)= E{Zi"i}n	sup | — ^>R(Zi)∣
i R∈HD,W,S,B n i=1
is the Gaussian complexity of HD,W,S,B (Bartlett & Mendelson, 2002).
Proof of (B-18). Letg(c) = c2 - c,z = (x, y) ∈ Rm × Rm,
Re(z) = (g ◦ R)(z) = R2(x) - R(y).
_ ________ _ ，一 ________ .一 一一 ..一 ____________________________ :一
Denote Z = (X, Y), Zi = (Xi, Yi), i = 1,..., n with X, Xi i.i.d.〜p, Y, Yi i.i.d.〜q. Let Zi be
an i.i.d. copy of Zi, and σi(i) be i.i.d. Rademacher random (standard normal) variables that are
independent of Zi and Zi . Then,
-
B(R)= EZ [R(Z)] = n Ezi[R(Zi)],
and
-n
B (R) = — ER(Zi).
n i=1
Denote
-n
R(H) = - E{Zi,σi}n [ sup	|£ σiR(Zi)∣]
n	R∈HD,W,S,B i=1
22
Under review as a conference paper at ICLR 2021
as the Rademacher complexity of HD,W,S,B (Bartlett & Mendelson, 2002). Then,
E{Zi}in [Sup |B(R) - Bb (R)|]	=	-n -E{Zi}n [sup I E(EZi [R(Zi)] - R(Zi))|] n	R i=1
≤	- nE{Zi,ei}n Isup IR(Zi)- R(Zi) |] -n -E{ZiZ,σi}n [sup 1 y^σi (R(Zi)- R(Zi)) |] n	i R i=1
≤	-	n-	n nE{Zi,σi}n [sup | X σiR(Zi) |] + nE{Zi,σi}n [sup | X σiR(Zi) |] 2R(g ◦ H)	=	=
≤ ≤	4(2B + -)R(H) 4C1(2B + )G(H),
where, the first inequality follows from the Jensen’s inequality, and the second equality holds since
the distribution of σi(R(Zi) - R(Zi)) and R(Zi) - R(Zi) are the same, and the last equality holds
since the distribution of the two terms are the same, and last two inequality follows from the Lipschitz
contraction property where the Lipschitz constant of g on HD,W,S,B is bounded by 2B + 1 and the
relationship between the Gaussian complexity and the Rademacher complexity, see for Theorem 12
and Lemma 4 in Bartlett & Mendelson (2002), respectively.
G(H) ≤ C2B1 /ɪɪɪlog cyn & exp(-log2	).	(B-19)
DSlog S	DS log S	DS log S
Proof of (B-19).
Since H is negation closed,
1n
G(H) = E{Zi"i}n [	SUp	- ∑>R(Zi)]
R∈HD,W,S,B n i=1
-n
=EZi[%[	Sup	— ^βiR(Zi)]∣{Zi}n=ι].
R∈HD,W,S,B n i=1
Conditioning on {Zi}in=1, ∀R, R ∈ HD,W,S,B it easy to check
Ve」-XX MR(Zi) - R(Zi))] = dH√,R),
where, dH(R, R) = √ JP3(R(Zi)- R(Zi))2. Observing the diameter of Hd,w,s,b
is at most B, we have
under d2H
G(H) ≤
H, d2H, δ)dδ]
√= E{Zi}n=ι [/B q0gN
Jlog N(H,d∞,δ)dδ]
≤√3 Z0 JVCH iog 骷dδ,
≤ C4B(^n-)1/2 log( Tn- )exp(- log2(^n-))
VCH	VCH	VCH
rnn	n
DSIogSlogDS1og5exp(TOg DSIogS)
where, the first inequality follows from the chaining Theorem 8.1.3 in Vershynin (2018), and the
second inequality holds due to d2H ≤ dH∞, and in the third inequality we used the relationship between
23
Under review as a conference paper at ICLR 2021
the matric entropy and the VC-dimension of the ReLU networks HD,W,S,B (Anthony & Bartlett,
2009), i.e.,
6Bn
log N (H, d∞,δ) ≤ VCH log AVC ，
δVCH
and the fourth inequality follows by some calculation, and the last inequality holds due to the upper
bound of VC-dimension for the ReLU network HD,W,S,B satisfying
VCH ≤ C5DS log S,
see Bartlett et al. (2019).
For any two integer M, N, there exists a Rφ ∈ Hd,w,s,b with width W = max{8MN1/M +
4M, 12N + 14} and depth D = 9M + 12, and B = 2B, such that
kr — RφkL2(ν) ≤ C6cLmM(NM)-4/M	(B-20)
Proof of (B-20).
We use Lemma 4.1, Theorem 4.3, 4.4 and following the proof of Theorem 1.3 in Shen et al. (2019).
Let A be the random orthoprojector in Theorem 4.4, then it is to check A(Me) ⊂ A([-c, c]m) ⊂
[-c√m, √mc]M. Let r be an extension of the restriction of r on Me, which is defined similarly as
g on page 30 in Shen et al. (2019). Since we assume the target r is Lipschitz continuous with the
bound B and the Lipschitz constant L, let small enough, then by Theorem 4.3, there exist a ReLU
network Rφ ∈ Hd,w,s,b with width
W = max{8MN1/M +4M,12N+ 14},
and depth
and B = 2B , such that
D=9M+ 12,
kr - RφkL∞(M,∖N) ≤ 80cL√mM(NM)-2∕m,
and
kRφkL∞(Mv) ≤ B + 3Lc√mM,
where, N is a ν- negligible set with ν(N) can be arbitrary small. Define Rφ = Rφ ◦ A. Then,
following the proof after equation (4.8) in Theorem 1.3 of Shen et al. (2019), we get our (B-20) and
kRφ∣∣L∞(Me∖N) ≤ 2B, ∣∣Rφ∣∣L∞(N) ≤ 2B + 3cL√mM.
Let DS logS < n, combing the results (B-17) - (B-20), we have
E{Xi,Yi}1n [kRbφ - rk2L2(ν)]
≤ 8C1(2B + 1)G(H) + C6cLmM(NM)-4/M
≤ 8Cι(2B + 1)02bJD⅛Slog	：
n DS log S
+ C6cLmM(NM)-4/M
≤ C(B2 + cLmM)n-2/(2+M),
M	M-2	4
where, last inequality holds since we choose M = log n, N = n 2(2+M) / log n, S = n M+2 / log n,
M ,
i.e., D = 9 log n +12, W = 12n 2(2+M) / log n + 14.	□
B.4	The relationship between EPT and MMD flow
Here we show that MMD flow can be considered a special case of EPT.
Proof. Let H be a reproducing kernel Hilbert space with characteristic kernel K(x, z). Recall in
MMD flow,
L[μ] = 2 ι∣μ—V ι∣mmd,
24
Under review as a conference paper at ICLR 2021
and
dLμ](x) = Z K(x, z)d4(z) - Z K(x, z)dν(z),
and the vector fields
c,mmd — γ7dL[μ]
Vt	= 一▽西T
/ VχK(x, z)dν(z)-/
VχK(x, z)dμt(z)
VxK(x, z)p(z)dz -
VxK(x, z)qt (z)dz
By Lemma B.1,the vector fields corresponding the Lebesgue norm 1 ∣∣μ一沙|任邯小)=2 JRm |q(x) 一
p(x)|2dx are defined as
vt = Vp(x) 一 Vqt(x).
Next, we will show the vector fields vtmmd is exactly by projecting the vector fields vt on to the
reproducing kernel Hilbert space Hm = H0m. By the definition of reproducing kernel We have,
p(x) = hp(∙), K(x, ∙)>H = / K(x, z)p(z)dz,
and
qt(x) = hqt(∙),K(x, ∙)>H = / K(x, z)qt(z)dz.
Hence,
vt(x) = Vp(x) 一 Vqt(x)
VxK(x, z)(p(z) 一 qt (z))dz
vtmmd(x).
This completes the proof.
□
B.5	Proof of the relation between EPT and SVGD
Proof. Let f (u) = ulogU in (5). With this f the velocity fields Vt = -f00(rt)Vrt = 一 ：；(Xx) Let
g in a Stein class associated With qt.
hVt, giH(qt)
=一 Z g(X)TVrt(X)qt(X)dχ
rt(x)
= 一	g(X)T V log rt(X)qt(X)dX
=―EX〜qt(x)[g(x)TV log qt(X)+ g(x)TV logp(X)]
=一 EX〜qt(χ) [g(x)TV log qt(X) + V ∙ g(x)]
+ EX〜qt(χ) [g(x)TV logp(X) + V ∙ g(x)]
=一 EX〜qt(x)Wtg] + EX〜qt(x)[Tpg]
=EX〜qt(x) [TPg],
Where the last equality is obtained by restricting g in a Stein class associated With qt, i.e.,
EX〜qt(χ)%tg = 0. This is exactly the velocity fields of SVGD (Liu, 2017).	□
25