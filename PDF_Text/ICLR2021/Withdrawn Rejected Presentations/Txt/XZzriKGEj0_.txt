Under review as a conference paper at ICLR 2021
Learning What Not to Model: Gaussian Pro-
cess Regression with Negative Constraints
Anonymous authors
Paper under double-blind review
Ab stract
We empirically demonstrate that our GP-NC framework performs better than the
traditional GP learning and that our framework does not affect the scalability
of Gaussian Process regression and helps the model converge faster as the size
of the data increases. Gaussian Process (GP) regression fits a curve on a set of
datapairs, with each pair consisting of an input point ‘x’ and its corresponding
target regression value ‘y(x)’ (a positive datapair). But, what if for an input point
‘X'，We want to constrain the GP to avoid a target regression value 'y(x)' (a
negative datapair)? This requirement can often appear in real-world navigation
tasks, where an agent would want to avoid obstacles, like furniture items in a room
when planning a trajectory to navigate. In this work, we propose to incorporate
such negative constraints in a GP regression framework. Our approach, ‘GP-NC’
or Gaussian Process with Negative Constraints, fits over the positive datapairs
while avoiding the negative datapairs. Specifically, our key idea is to model the
negative datapairs using small blobs of Gaussian distribution and maximize its KL
divergence from the GP. We jointly optimize the GP-NC for both the positive
and negative datapairs. We empirically demonstrate that our GP-NC framework
performs better than the traditional GP learning and that our framework does not
affect the scalability of Gaussian Process regression and helps the model converge
faster as the size of the data increases.
1	Introduction
Gaussian process are one of the most studied model class for data-driven learning as these are
nonparametric, flexible function class that requires little prior knowledge of the process. Traditionally,
GPs have found their applications in various fields of research, including Navigation systems (e.g.,
in Wiener and Kalman filters) (Jazwinski, 2007), Geostatistics, Meteorology (Kriging (Handcock
& Stein, 1993)) and Machine learning (Rasmussen, 2006). The wide range of applications can be
attributed to the property of GPs to model the target uncertainty by providing the predictive variance
over the target variable.
Gaussian process regression in its current construct fits only on a set of positive datapairs, with each
pair consisting of an input point and its desired target regression value, to learn the distribution on a
functional space. However, in some cases, more information is available in the form of datapairs,
where at a particular input point, we want to avoid a range of regression values during the curve
fitting of GP. We designate such data as negative datapairs.
An illustration where modeling such negative datapairs would be extremely beneficial is given in
Fig 1. In Fig 1(b), an agent wants to model a trajectory such that it covers all the positive datapairs
marked by ‘x’. However, it is essential to note that the agent would run into an obstacle if it models
its trajectory based only on the positive datapairs. We can handle this problem of navigating in the
presence of obstacles in two ways, one way is to get a high density of positive datapairs near the
obstacle, and the other more straightforward approach is to just mark the obstacle as a negative
datapair. The former approach would unnecessarily increase the number of positive datapairs for GP
to regress. Hence, it may run into scalability issues. However, in the latter approach, if the point is
denoted as a negative datapair with a sphere of negative influence around it as illustrated by Fig 1.c,
the new trajectory can be modeled with less number of datapairs that accounts for all obstacles on the
1
Under review as a conference paper at ICLR 2021
(a)	(b)	(C)
Figure 1: An illustration of our problem setup. (a) top view of the room where the agent wants to travel to a
particular location while avoiding obstacles; (b) the agent has been given the location of the positive datapairs
that are needed to be covered in its trajectory. Since the number of these observed points is low, the agent is not
able to avoid the obstruction (coffee table) while forecasting its course; (c) the agent is given both the positive
datapairs which it needs to reach along with negative datapairs (area of influence is given by shaded red region)
that should be avoided during the modeling of future trajectory.
way. Various GP methods in their current framework lack the ability to incorporate these negative
datapairs for the regression paradigm.
Contributions:In this paper, we explore the concept of negative datapairs. We provide a simple yet
effective GP regression framework, called GP-NC which can fit on the positive datapairs while
avoiding the negative datapairs. Specifically, our key idea is to model the negative datapairs using
a small Gaussian blob and maximize its KL divergence from the GP . Our framework can be
easily incorporated for various types of GP models (e.g., exact, SVGP (Hensman et al., 2013),
PPGPR (Jankowiak et al., 2019)) and works well in the scalable settings too. We empirically show in
§5 that the inclusion of negative datapairs in training helps with both the increase in accuracy and the
convergence rate of the algorithm.
2	Review of Gaussian Process Regression
We briefly review the basics of Gaussian Process regression, following the notations in (Wilson et al.,
2015). For more comprehensive discussion of GPs, refer to (Rasmussen, 2006).
A Gaussian process is a collection of random variables, any finite number of which have a joint
Gaussian distribution (Rasmussen, 2006). We consider a dataset D with n D-dimensional input
vectors, X = {x1, ∙ ∙ ∙ , Xn} and corresponding n X 1 vector of targets y = (y(x1), ∙ ∙ ∙ , y(xn))T.
The goal of GP regression is to learn a function f that maps elements from input space to a target
space, i.e., y(x) = f (x) + e where e is i.i.d. noise. If f (x)〜GP(μ, kθ), then any collection of
function values f has a joint multivariate normal distribution given by,
f = f (X) = [f (xι),…，f (Xn)]T 〜N (μx,Kx,x)	(1)
with the mean vector and covariance matrix defined by the functions of the Gaussian Process, as
(μx)i = μ(xi) and (KXX)ij = kθ(xi, Xj). The kernel function kθ of the GP is parameterized by θ.
Assuming additive Gaussian noise, y(x)∣f (x)〜N(y(x); f (x), σ2), then the predictive distribution
of the GP evaluated at the n* test points indexed by X*, is given by
f* |X*, X, y,θ,σ2 〜N (E[f*], cov(f*)),
E[f*] = μχ* + Kχ*,χ [Kχ,χ + σ2I] 1 y,	⑵
cov(f*) = Kχ* ,X* - Kχ* ,X [Kχ,χ + σ2I] 1 Kχ,χ*
KX* ,X represents the n* X n covariance matrix between the GP evaluated at X* and X. Other
covariance matrices follow similar conventions. μx* is the mean vector of size n* X 1 for the
test points and KX,X is the n X n covariance matrix calculated using the training inputs X. The
underlying hyperparameter θ implicitly affects all the covariance matrices under consideration.
2
Under review as a conference paper at ICLR 2021
2.1	GPS: LEARNING AND MODEL SELECTION
We can view the GP in terms of fitting a joint probability distribution as,
P (y, HX) = p(y∣f ,σ2) P (f∣x)	(3)
and we can derive the marginal likelihood of the targets y as a function of kernel parameters alone
for the GP by integrating out the functions f in the joint distribution of Eq. (3). A nice property of
the GP is that this marginal likelihood has an analytical form given by,
L(θ) = logp(y∣θ,X) = -2 (yT (Kθ + σ2I)-1 y + log (∣Kθ + σ2I∣) + Nlog(2π))	(4)
where we have used Kθ as a shorthand for KX,X given θ. The process of kernel learning is that of
optimizing Eq. (4) w.r.t. θ.
The first term on the right hand side in Eq. (4) is used for model fitting, while the second term
is a complexity penalty term that maintains the Occam’s razor for realizable functions as shown
by (Rasmussen & Ghahramani, 2001). The marginal likelihood involves matrix inversion and
evaluating a determinant for n × n matrix, which the naive implementation would require a cubic
order of computations O(n3) and O(n2) of storage. Approaches like Scalable Variational GP
(SVGP) (Hensman et al., 2013) and parametric GPR (PPGPR) (Jankowiak et al., 2019) have proposed
approximations that lead to much better scalability. Please refer to Appexdix A for details.
3	GP REGRESSION WITH NEGATIVE DATAPAIRS
As shown in Fig. 1, we want the model to avoid certain negative datapairs in its trajectory. In other
words, we want the trajectory of the Gaussian Process to have a very low probability of passing
through these negative datapairs. In this section, we will first formalize the functional form of the
negative datapairs and then subsequently describe our framework called GP-NC regression.
3.1	Definition of positive & negative datapairs
Positive datapairs: The set of datapairs through which the GP should pass are defined as pos-
itive datapairs. We assume a set of n datapairs (input, positive target) with D-dimensional in-
PUt vectors, X = {xι,…，Xn} and corresponding n X 1 vector of target regression values
y = {y(χι),…，y(χn)}.
Negative datapairs: The set of datapairs which the GP should avoid (obstacles) are defined as
negative datapairs. We assume a set of m datapairs (input, negative target) with D-dimensional input
vectors X = {Xι,…,Xm} and corresponding set of negative targets y = {y(Xι),…,y(Xm)}.
The sample value of GP at input Xi, given by f (Xi), should be far from the negative target regression
value y(Xi).
Note that it is possible that a particular input X can be in both the positive and negative data pair
set. This will happen, when at a particular input we want the GP regression value to be close to its
positive target regression value y(X) and far from its negative target regression value y(X).
3.2	Functional representation of negative datapairs
For our framework, we first get a functional representation of the negative datapairs. We define a
Gaussian distribution around each of the negative datapair, q(y∣X)〜N (y(X), σ2eg), with mean equal
to the negative target value y(X) and σ2eg is the variance which is a hyperparameter. The Gaussian
blob can also be thought of as the area of influence for the negative datapair with the variance σneg
indicating the spread of its influence.
3.3	GP-NC REGRESSION FRAMEWORK
The aim of our GP-NC regression framework is to simultaneously fit the GP regression on the
positive datapairs (X) and avoid the negative datapairs (X) (i.e., using them as negative constraints
(NC)). The former is achieved by maximizing the marginal likelihood given in the Eq. (4). To avoid
3
Under review as a conference paper at ICLR 2021
	Algorithm 1: Training of GP-NC	
Algorithm describing the GP-NC regression. We alternatively up- date between the negative log- likelihood and KL divergence term with respect to the kernel parame- ters θ. For different GP methods we can appropriately plug-in the log-likelihood term (NLL).	I P H w e	nput	: Datapairs {X, y}+, {X, y}- arameters	: GP Kernel Parameters 'θ' yperparameters : σneg, λ hile until convergence do NLL= - p(y∖θ, X); θ —minimize (NLL); KLdiv = λ ∙ log DKL (P (y∖θ,X) ∖∖N (y,σ2eg)); θ — maximize (KLdiv ) nd
the negative datapairs, we want our GP model to adjust its distribution curve so that while drawing
samples from the predictive GP distribution, its values do not lie in the influence region of the
negative datapairs. To this end, we propose to fit the GP regression model on the positive datapairs
along with maximizing the Kullback-Leibler (KL) divergence between the distributions of the GP
regression model and the Gaussian distributions defined over the negative datapairs.
Thus, mathematically, we want to maximize the following KL divergence given by
∆ = argmax DκL(p(y∣θ,X)kq(y∣X))	(5)
θ
We chose to maximize the DKL term in the p → q direction, as this fixes the negative datapairs
distribution q(^y∖X) as the reference probability distribution. Now, since the KL divergence is an
unbounded distance metric, the following section describes a practical workaround to maximize it.
3.3.1	Maximizing KL divergence using the logarithm trick
Eq. (5) is increasing the distance between the GP distribution and the negative datapairs distribution
by maximizing the KL divergence. However, KL divergence is an unbounded function, i.e., DKL ∈
[0, ∞). Implementing the DKL divergence directly in the form of Eq. (5) can create problems for the
gradient updates and convergence.
We also want to maximize the marginal log-likelihood Eq. (4) and the ∆ terms simultaneously. This
raises a problem of mismatch in the magnitude of scale for the marginal log-likelihood term and DKL
divergence term as the values of DKL divergence will be significantly higher. Thus, the gradient
update would be dominated by ∆ term. In essence, the model would fixate more on avoiding the
negative datapairs than fitting the curve on the positive datapairs. We also observed this empirically.
Hence, to suppress the gradient update from the ∆ term, we encapsulate Eq. (5) in a logarithmic
function. This turns out to be beneficial in multiple ways. Firstly, maximizing the DKL term is
equivalent to maximizing the log(DKL), as log is an monotonically increasing function. Secondly,
and more importantly, the scale of magnitude of ∆ term becomes equivalent to the scale of magnitude
for the marginal log-likelihood term which makes the convergence stable.
3.3.2	GP-NC: LEARNING AND MODEL SELECTION
We apply a log function to the DKL given in Eq. (5) and write the combined objective function for
our GP-NC regression,
L(θ) = arg min [-log p(y∖θ,X) - λ log DκL(p(y∖θ,X) kq(y∖X))]	⑹
θ
where p(y∖θ, X ) is the marginal log-likelihood term that represents the model to be fitted on the
observed datapoints. The parameter λ is the tradeoff hyperparameter between curve fitting and
avoidance of the negative datapoints, or how relaxed can the negative constraints be.
We already know the analytical form of the log-likelihood term from Eq. (4). We now focus on the
log(DκL) term. Since, both the likelihoodp(y∖θ, X) and negative datapair distributions q(y∖X) are
modeled using Gaussians, we can simply use the analytical form of KL divergence between any two
Gaussian distributions given by,
DKL(p, q) = log σ2 + σ1 + (μ12 μ - 1	(7)
σ1	2σ2	2
4
Under review as a conference paper at ICLR 2021
herep, q are Gaussian distributions defined as N(μι,σι) andN(μ2, σ2) respectively. The DKL term
is adjusting the mean and variance of the likelihood P (Y∣θ, X) with respect to the fixed blobs of
Gaussian distributions around the negative datapairs. Specifically, we can consider the distribution
'p ≡ N(μ1,σ1) ≡ p(y∣θ,X)' and 'q ≡ N(μ2,σ2) ≡ q(y|X)' in Eq. 7. Now, if We refer Eq. 2,
μι = E[fJ and σι = cov(f*) which contain the parameters θ of the GP that are optimized. μ2,σ2
correspond to the hyperparameters of the Gaussian distribution representing the negative datapairs
and are constant. Algorithm (1) gives an overview of the training of GP-NC regression.
A note on the difference between the GP-NC regression and general classification settings: In the
case of GP-NC regression, the boundary for every negative datapair is optimized independent of each
other. In classification settings, all the negative points belong to a class and they jointly affect the
decision boundary of the GP for their class.
3.4	Sparse Gaussian Processes with negative datapoints
In Appendix A, we show that it is straightforward to modify the class of scalable and Sparse GP
regression models to account for the negative datapairs in their formulation. In particular we review
the SVGP model by (Hensman et al., 2013), which is a popular scalable implementation of GPs. We
also investigate a recent parametric Gaussian Process regressors (PPGPR) method by (Jankowiak
et al., 2019). We evaluate the performance of these methods with our GP-NC framework in the
experiments section.
4	Related Works
Classical GP: To the best of our knowledge, the classical GP regression introduced in (Rasmussen,
2006) and many subsequent works primarily focus on positive datapairs for curve fitting. Even with
the absence of the concept of negative datapairs, GP regression methods have been widely used
for obstacle-aware navigation task which is one of the relevant applications to evaluate our GP-NC
framework.
GPs for navigation: GPs are extensively used in the field of navigation and often are a component
of path planning algorithms. (Ellis et al., 2009) used GP regression in modeling the pedestrian
trajectories by using positive datapairs. (Aoude et al., 2013) used heuristic based approach over GP
regression to incorporate dynamic changes and environmental constraint in the surroundings. Their
solution, named RR-GP, builds a learned motion pattern model by combining the flexibility of GP
with the efficiency of RRTReach, a sampling-based reachability computation. Obstacle trajectory GP
predictions are conditioned on dynamically feasible paths identified from the reachability analysis,
yielding more accurate predictions of future behavior. (Goli et al., 2018) introduced the use of
GP regression for long-term location prediction for collision avoidance in Connected Vehicle (CV)
environment. The GPs are used to model the trajectory of the vehicles using the historical data.
The collected data from vehicles together with GPR models received from infrastructure are then
used to predict the future trajectories of vehicles in the scene. (Meera et al., 2019) designed an
Obstacle-aware Adaptive Informative Path Planning (OA-IPP) algorithm for target search in cluttered
environments using UAVs. This method uses GP to detect the obstacles/target, which the UAV gets
by marking dense number of points (positive datapairs) around the obstacles. (Hewing et al., 2020;
Yuan & Kitani, 2019) are some of the works using sampling based techniques for trajectory prediction.
(Choi et al., 2015) is one of the work which tries to incorporate the concept of negative datapairs
in classical GP construct by introducing a leveraged parameter in kernel function. The authors
demonstrate that having the ability to incorporate negative targets increases the efficiency of trajectory
predictions. However, this approach fundamentally differs from ours in terms of incorporation of
negative datapairs ours try to maximize the KL-Divergence between y(x) and y(x) while theirs
utilizes additional leveraged parameter in the kernel function. Besides, our approach is more scalable
as the size of the covariance matrix doesn’t increase to incorporate the negative datapairs.
Scalable GP: Naive implementation of GP regression is not scalable for large datasets as the model
selection and inference of GP requires a cubic order of computations O(n3) and O(n2) of storage.
Since the GP-NC framework is quite generic and can work for various scale GP methods, we want to
highlight few of these methods. (Hensman et al., 2013; Dai et al., 2014; Gal et al., 2014) are some of
the well known scalable methods suitable for our framework as they use stochastic gradient descent
5
Under review as a conference paper at ICLR 2021
PO Positive Targets ∙ Negative Targets ∙ Inducing Points --- Curve Fitted ∣	∣ Confidence
Figure 2: Visualizing GP-NC regression framework: The figures compare how the SVGP regression fits using
the classical GP framework (left) VS the GP-NC framework (right). The aim is to fit the regression targets
marked in 'black' (positive datapairs) and avoid the targets marked in 'red’ (negative datapairs). The classical
GP framework only uses the positive datapairs whereas our proposed GP-NC framework uses both the positive
and negative datapairs for fitting the regression curve. The points in 'yellow' are the inducing points used to fit
the GP. We used two inducing points setting. Top figures: locations of inducing points were taken at the start of
curve. Bottom figures: we randomly sampled the inducing points from the whole range of training inputs. For
GP-NC framework (right), hyper-parameters were selected as λ = 0.1 and σ∏eg = 1.2
for optimization. Furthermore, recent works (Wilson & Nickisch, 2015; Wilson et al., 2015; 2016)
have improved scalability by reducing the learning to O(n) and test prediction to O(1) under some
assumptions.
Negative datapairs in other domains: The concept of negative datapairs has been extensively utilized
in the self-supervised learning. Applications include learning word embeddings (Mikolov et al., 2013;
Mnih & Kavukcuoglu, 2013), image representations (He et al., 2020; Misra & Maaten, 2020; Feng
et al., 2019), video representations (Sermanet et al., 2018; Fernando et al., 2017; Misra et al., 2016;
Harley et al., 2020), etc. In these works negative and positive samples are created as pseudo labels to
train a neural network to learn the deep representations of the inputs.
5	Experiments
We compared various GP regression models in their classical form (using only positive datapairs) with
their corresponding GP-NC regression models where we used our negative constraints framework.
We used Negative Log-likelihood (NLL) and Root Mean Squared Error (RMSE) as our evaluation
metrics. We compared our framework on a synthetic dataset and six real world datasets. Throughout
our experiments, we found that for every GP model, the GP-NC regression framework outperforms
its corresponding classical GP regression setting. We used GPytorch (Gardner et al., 2018) to
implement all the GP (ours + baselines) models. We use zero mean value and the RBF kernel for GP
prior for all of the models unless mentioned otherwise.
5.1	SYNTHETIC DATASET: VISUALIZING THE GP-NC REGRESSION FRAMEWORK
We aim to visualize the GP-NC regression framework using a toy dataset. We sampled 400 positve
datapairs from a sinusoidal function and randomly sampled 15 negative datapairs as represented
by Fig. 2. We trained a sparse SVGP model to regress a curve on the positive datapair using the
classical GP framework and the one with negative constraints GP-NC . For the top figures of Fig. 2,
we trained SVGP with 80 inducing points, all at the starting location of training input range. For
the bottom figures of Fig. 2, we randomly sampled 10 inducing points from the range of training
inputs. SVGP with a constant mean and a RBF kernel for the GP prior was used. After training the
SVGP model in both settings for 100 epochs we obtain the curves as depicted by the figures on the
6
Under review as a conference paper at ICLR 2021
Wine Quality	Elevators	Protein	Prud. R. A. 3DRoad
PPGPR
PPGPR-NC
SVGP
SVGP-NC
Exact
EXaCt-NC
Figure 3: Comparison on real world data: We plot test negative log-likelihoods (NLL) for 6 univariate regression
datasets (lower is better). Results are averaged over 10 random train/test/valid splits.
Table 1: Runtime comparison of the classical GP and GP-NC frameworks which includes negative datapairs
on different datasets. ∆t is the runtime difference of the GP model in GP-NC framework vs the classical GP
framework. We used GPU accelerated GP implementation of GPyTorch library.
Datasets	Size of data	Type of Target Variable	∆t Exact GP	∆t sparse SVGP
Wine quality - red	1599	Discrete	21ms	3.1s
Wine quality - white	4898	Discrete	40ms	3.7s
Elevators	16599	Continuous	5s	34s
Protein	45730	Continuous	50s	37s
Prudential	59381	Discrete	1.2s	34s
3DRoad	434874	Continuous	2208s	292s
left-side in Fig. 2. The inability to incorporate the information provided by the negative datapairs in
the classical GP construct hinders its ability to fit the data well as patently visible in the left figure.
Mean and predictive variance are not only losing out on some positive datapairs but are also engulfing
the negative datapairs in the confidence region which is undesirable.
Our GP-NC framework re-calibrates the curve by integrating the information provided by the negative
datapairs as seen in the right-hand side figures of Fig 2. As evident from the figure, the additional
information from a few negative datapairs helps the model to fit better to the positive datapairs in
addition to avoiding most of the negative datapairs. We can tune the values of λ given in Eq. (6) to
balance between the weightage given by the GP to positive and negative datapairs. Decreasing the
value of λ results in reduction of influence of the negative datapairs. Notice that curved learned by
our approach even with sub-optimal inducing points.
5.2	TRAJECTORY PREDICTION USING GP-NC REGRESSION FRAMEWORK
We want to model an agent’s trajectory using a GP regression model such that it takes the agent’s
present location (x, y) as input and predicts agents next location (X, y). For this set of experiments,
we synthesize a 2d-virtual traffic scene given by Fig. 4. Furthermore, the road contain pitfalls,
roadblocks, accidents, etc. that need to be avoided are represented as red diamonds in the figure.
We designate these targets as negative targets that are to be avoided for ensuring the safety of traffic.
There are a total number of 10 negative datapairs present in the scene. Next, We sample 250 observed
co-ordinates on the 2d virtual path for modeling the future trajectory of the agent.
We trained a classical SVGP model and ours SVGP-NC model to predict the trajectory of agent. For
both the models we utilize a constant mean and an RBF kernel for the GP prior. Both the models
were trained for 100 epochs. It can be observed from the Fig. 4.a that the classical GP framework
lacks the ability to incorporate negative datapairs, which results in a loosely fitted GP model. On
the other hand, when trained with an additional constraints given by negative datapairs GP-NC fits a
tighter curve on the observed datapairs and avoiding all the negative datapairs as shown in Fig 4.b.
Moreover, in these set of experiments it is easy to demonstrate the impact of the λ values from Eq. (6)
on the GP regression. Decreasing the value of λ results in reduction of influence of the negative
7
Under review as a conference paper at ICLR 2021
X Positive Targets ¼ NegativeTargets Curve Fitted Confidence
Figure 4: Trajectory prediction with GP -NC regression framework: The figures compare trajectory prediction
in a 2D-virtual environment using the classical GP framework (a) vs. the GP-NC framework (b,c). The car
is navigating through the forest and our aim is to avoid the roadblocks marked in ‘red while maintaining the
car’s proximity to the ‘black’ trajectory markers. The classical GP framework only uses the positive datapairs
whereas our proposed GP-NC framework uses both the positive & negative datapairs for prediction of agent’s
trajectory. (b) depicts the GP-NC framework with the hyper parameter λ = 1 (c) depicts the GP-NC framework
with the hyper parameter λ = 0.1
datapairs as can be observed from Fig 4.b vs. Fig 4.c. However, it can be observed by juxtaposing
both the figures (Fig 4.a and Fig 4.c) that predicted mean trajectory by classical GP and GP-NC with
λ = 0.1, the latter fits better to the black trajectory markers. Hence, for saftey critical applications
like navigation GP -NC is superior than the classical GP in incorporating negative constraints.
5.3	Realworld datasets
We evaluated our GP -NC framework on six real world datasets with the number of datapoints ranging
from N 〜1500, 5000,15000, 50000, 450000 and the number of input dimension d ∈ [3,127].
Among the six datasets five of them are from UCI repository (Dua & Graff, 2017) (Wine quality - red,
white, Elevators, Protein, and 3DRoad), while the sixth one is from Kaggle Competition (Prudential
life insurance risk assessment). These datasets consists of two different kind of prediction/regression
variables namely discrete variable and continuous variable. For discreet variable the value of elements
lies between certain range, i.e., integer values lie between [0, 10]. For continuous variable, the value
of target regression can be any real number. Datasets (Prudential risk assessment, Wine quality - red,
white) are all discrete target variable datasets while datasets (Elevators, Protein and 3DRoad) have
continuous target variables.
Random shuffling technique for creating negative datapairs for GP -NC: As we are only given
positive regression target values in these datasets, we create pseudo-negative regression targets by
randomly shuffling the labels and pairing them with the inputs to create negative datapairs. This
generates a set of valid datapairs as given the input x only y(x) can be associated as true regression
value/label, we can assume whatever label we get by random shuffling as a negative target.
We compared our model against the standard baselines of Exact GP (Gardner et al., 2019; Wang
et al., 2019), and Sparse GP methods like SVGP (Hensman et al., 2013) and PPGPR (Jankowiak
et al., 2019). For training the sparse methods, we used 1000 inducing points. We trained the models
using Adam optimizer with a learning rate of 0.1 for 400 epochs on each dataset. For GP-NC
framework, we used 200 negative datapairs. We maintained consistency in all the models in all terms
of maintaining a constant mean and RBF kernel for a GP prior.
Fig. 3 compares negative log likelihood values of various GP regression methods, in both classical
and GP-NC frameworks, on all six real datasets. The orange dots represents our methods while the
blue dots in the plots depict the baselines. It can be observed that GP-NC framework outperforms
the classical GP framework. Methods like SVGP - NC and Exact GP - NC performs on an average
0.2 nats better than baseline SVGP and Exact GP respectively. It is interesting to note that including
negative datapairs by the ‘random shuffling technique’ is quite effective and we can observe gains in
terms of model performance.
Table 1 compares runtime difference between the classical GP and the GP -NC to see how the
additional KL divergence term that accounts for negative datapairs affects the runtime of GP-NC
framework. We train all the GP models for 50 epochs and measure the average excess time (∆t)
8
Under review as a conference paper at ICLR 2021
0	20	40	60	80	100	0	20	40	60	80	100	0	20	40	60	80	100
# epochs	# epochs	# epochs
Figure 5: RMSE plots on real world data (top - Exact GP ; middle - SVGP; bottom - PPGPR): Plots show the
test RMSE for six univariate regression datasets (lower is better). Models are fitted by using cross validation on
training data. Convergence of GP-NC framework is consistently faster than its classical GP counterpart for all
the models.
# epochs
# epochs
0	20	40	60	0	20	40	60
# epochs	# epochs
value over 10 runs. It can be observed from Table 1 that the ∆t depends on the size of the dataset and
also on the type of target regression variable. For the Exact GP and discrete target variables setting
the ∆t does not increase much with size of the dataset, however for continuous target variable there
is a considerable amount of increase. For SVGP model the increase in ∆t can be attributed to the size
of the dataset. Overall, the average increase in training is not very significant. Thus, our experiments
indicate that the added penalty term to likelihood term in Eq. (6) does not significantly affect the
scalability of current scalable GP architectures.
Figure 5 shows the RMSE plots for Exact GP, SVGP, and PPGPR models with the classical GP
juxtaposed on to the models with the negative constraint GP-NC framework. It can be observed
from the plots that our GP-NC models converges faster, and better, than the classical GP models for
the six univariate real world datasets. In addition, it is interesting to note that as the size of the data
increases, the convergence curve of the GP-NC model becomes steeper.
6	Conclusion
We presented a novel and generic Gaussian Process regression framework GP-NC that incorporates
negative constraints along with fitting the curve for the positive datapairs. Our key idea was to
assume small blobs of Gaussian distribution on the negative datapairs. Then, while fitting the GP
regression on the positive datapairs, our GP-NC framework simultaneously maximizes the KL
divergence from the negative datapairs. Our work highlights the benefits of modeling the negative
datapairs for GP s and our experiments support the effectiveness of our approach. We hope that this
successful realization of the concept of negative datapairs for GP regression will be useful in variety
of applications.
References
Georges S Aoude, Brandon D Luders, Joshua M Joseph, Nicholas Roy, and Jonathan P How.
Probabilistically safe motion planning to avoid dynamic obstacles with uncertain motion patterns.
Autonomous Robots, 35(1):51-76, 2013.
9
Under review as a conference paper at ICLR 2021
S. Choi, E. Kim, K. Lee, and S. Oh. Leveraged non-stationary gaussian process regression for
autonomous robot navigation. In 2015 IEEE International Conference on Robotics and Automation
(ICRA),pp.473-478,2015. doi:10.1109/ICRA.2015.7139222.
Zhenwen Dai, Andreas Damianou, James Hensman, and Neil Lawrence. Gaussian process models
with parallelization and gpu acceleration. In arXiv, 2014.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
David Ellis, Eric Sommerlade, and Ian Reid. Modelling pedestrian trajectory patterns with gaussian
processes. In 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV
Workshops, pp. 1229-1234. IEEE, 2009.
Zeyu Feng, Chang Xu, and Dacheng Tao. Self-supervised representation learning by rotation
feature decoupling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2019.
Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video
representation learning with odd-one-out networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), July 2017.
Yarin Gal, Mark van der Wilk, and Carl E. Rasmussen. Distributed variational inference in sparse
gaussian process regression and latent variable models. In arXiv, 2014.
Jacob R Gardner, Geoff Pleiss, David Bindel, Kilian Q Weinberger, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances
in Neural Information Processing Systems, 2018.
Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In arXiv,
2019.
S. A. Goli, B. H. Far, and A. O. Fapojuwo. Vehicle trajectory prediction with gaussian process
regression in connected vehicle environment?. In 2018 IEEE Intelligent Vehicles Symposium (IV),
pp. 550-555, 2018.
Mark S Handcock and Michael L Stein. A bayesian analysis of kriging. Technometrics, 35(4):
403-410, 1993.
Adam W. Harley, Shrinidhi K. Lakshmikanth, Fangyu Li, Xian Zhou, Hsiao-Yu Fish Tung, and
Katerina Fragkiadaki. Learning from unlabelled videos using contrastive predictive neural 3d
mapping. In arXiv, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In arXiv, 2020.
James Hensman, Nicolo Fusi, and Neil D. Lawrence. Gaussian processes for big data. In arXiv, 2013.
Lukas Hewing, Elena Arcari, Lukas P. Frhlich, and Melanie N. Zeilinger. On simulation and trajectory
prediction with gaussian process dynamics. 2020.
Martin Jankowiak, Geoff Pleiss, and Jacob R Gardner. Parametric gaussian process regressors. arXiv,
pp. arXiv-1910, 2019.
Andrew H Jazwinski. Stochastic processes and filtering theory. Courier Corporation, 2007.
Ajith Anil Meera, Marija Popovic, Alexander Millane, and Roland Siegwart. Obstacle-aware adaptive
informative path planning for uav-based target search. In arXiv.cs.RO, 2019.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems
26, pp. 3111-3119. Curran Associates, Inc., 2013.
10
Under review as a conference paper at ICLR 2021
Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
6707-6717, 2020.
Ishan Misra, C Lawrence Zitnick, and Martial Hebert. Shuffle and learn: unsupervised learning using
temporal order verification. In European Conference on Computer Vision, pp. 527-544. Springer,
2016.
Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive
estimation. In Advances in neural information processing systems, pp. 2265-2273, 2013.
Carl Edward Rasmussen. Gaussian processes for machine learning. MIT Press, 2006.
Carl Edward Rasmussen and Zoubin Ghahramani. Occam’s razor. In In Advances in Neural
Information Processing Systems 13, pp. 294-300. MIT Press, 2001.
P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine, and G. Brain. Time-
contrastive networks: Self-supervised learning from video. In 2018 IEEE International Conference
on Robotics and Automation (ICRA), pp. 1134-1141, 2018.
Ke Alexander Wang, Geoff Pleiss, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger, and
Andrew Gordon Wilson. Exact gaussian processes on a million data points. In arXiv, 2019.
Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured gaussian
processes (kiss-gp). In arXiv, 2015.
Andrew Gordon Wilson, Christoph Dann, and Hannes Nickisch. Thoughts on massively scalable
gaussian processes. In arXiv, 2015.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Stochastic variational
deep kernel learning. In arXiv, 2016.
Ye Yuan and Kris Kitani. Diverse trajectory forecasting with determinantal point processes. In arXiv,
2019.
11
Under review as a conference paper at ICLR 2021
A GP -NC FOR S CALAB LE GP METHODS
We can replace the NLL term in Algorithm (1) by the log likelihood of the different scalable GP
methods. We have a scalable implementation of the DKL update, so the entire Algorithm scales
well with the input data size. It is straightforward to plug-in the class of scalable and Sparse GP
regression models in the likelihood term of Algorithm (1) to account for the negative datapairs in
their formulation. In particular we review the SVGP model by (Hensman et al., 2013), which is a
popular scalable implementation of GPs. We also investigate a recent parametric Gaussian Process
regressors (PPGPR) method by (Jankowiak et al., 2019). In this section, we follow the notations
given in their respective research works and give their derivations of the log likelihood function here
for the sake of completeness.
A.1 SVGP regression model
(Hensman et al., 2013) proposed the Scalable Variational GP (SVGP) method. The key technical
innovation was the development of inducing point methods which we now review. By introducing
inducing variables u that depend on variational parameters {zm}mM=1, where M = dim(u) N
and with each zm ∈ Rd, we augment the GP prior as follows:
p(f|X) → p(f |u, X, Z)p(u|Z)
We then appeal to Jensen’s inequality and lower bound the log joint density over the targets and
inducing variables:
/
logp(y, u|X, Z) = log
df p(y|f)p(f |u)p(u)
≥ Ep(f|u) [log p(y|f) + log p(u)]
N
XlogMyiIkTKMMu,σ0bS)- 2σ2-TrKtNN + logP(U)
i=1	os
where ki = k(xi, Z), KMM = k(Z, Z) and KtNN is given by
KtNN = KNN - KNMKM-1MKMN
(8)
(9)
with KNM = KMT N = k(X, Z). The essential characteristics of Eqn. 8 are that: i) it replaces
expensive computations involving KNN with cheaper computations like KM-1M that scale as O(M 3);
and ii) it is amenable to data subsampling, since the log likelihood and trace terms factorize as sums
over datapoints (yi, xi).
A.1.1 SVGP likelihood function
SVGP proceeds by introducing a multivariate Normal variational distribution q(U) = N(m, S). The
parameters m and S are optimized using the ELBO (evidence lower bound), which is the expectation
of Eqn. 8 w.r.t. q(U) plus an entropy term term H [q(U)]:
Lsvgp = Eq(u) [log p(y, U|X, Z)] + H[q(U)]
=X [logN3向(Xi),σ0bS) 一 σ(Xi ] - DKLg(U)Ip(U))
i=1	2σobs
(10)
where KL denotes the Kullback-Leibler divergence, μf (xi) is the predictive mean function given by
μf(xi) = kτKMMm and σf(x∕2 ≡ Var[fi∣Xi] = Ktii + kτKMMSKMMki denotes the latent
function variance.
LSvgp , which depends on m, S, Z, σobS and the various kernel hyperparameters θ, can then be
maximized with gradient methods. We refer to the resulting GP regression method as SVGP.
A.2 PPGPR-NC regression model: Likelihood function
Jankowiak et al. (2019) recently proposed a parametric Gaussian Process regressors (PPGPR) method.
We defer the reader to Section (3.2) of their paper for details about their likelihood function.
12