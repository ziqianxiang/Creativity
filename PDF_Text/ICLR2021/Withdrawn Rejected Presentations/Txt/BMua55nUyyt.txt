Under review as a conference paper at ICLR 2021
Median DC for Sign Recovery: Privacy can be
Achieved by Deterministic Algorithms
Anonymous authors
Paper under double-blind review
Ab stract
Privacy-preserving data analysis becomes prevailing in recent years. It is a com-
mon sense in privacy literature that strict differential privacy can only be obtained
by imposing additional randomness in the algorithm. In this paper, we study the
problem of private sign recovery for sparse mean estimation and sparse linear re-
gression in a distributed setup. By taking a coordinate-wise median among the re-
ported local sign-vectors, which can be referred to as a median divide-and-conquer
(Med-DC) approach, we can recover the signs of the true parameter with a prov-
able consistency guarantee. Moreover, without adding any extra randomness to
the algorithm, our Med-DC method can protect data privacy with high probability.
Simulation studies are conducted to demonstrate the effectiveness of our proposed
method.
1	Introduction
With the development of technology for data acquisition and storage, the modern dataset has a larger
scale, more complex structure, and more practical considerations, which addresses new challenges
for data analysis. In recent years, large quantities of sensitive data are collected by individuals and
companies. While one wants to extract more accurate statistical information from the distributed
dataset, we must also beware of the leakage of this sensitive personal information during the training
process. This calls for the study of distributed learning under privacy constraints (Pathak et al., 2010;
Hamm et al., 2016; Jayaraman et al., 2018).
In privacy literature, differential privacy, which was firstly proposed in Dwork et al. (2006), has been
the most widely adopted definition of privacy tailored to statistical data analysis. It has achieved
tremendous success in real-world applications. Denote the data universe to be X, for the dataset
X = {Xi}in=1 ∈ Xn where Xi’s are the data observations. The (, δ)-differentially privacy can be
defined as follows.
Definition 1. (Differential Privacy in Dwork et al. (2006)) A randomized algorithm A : Xn → Θ
gives (, δ)-differentially private if for any pair of adjacent datasets X ∈ Xn and X0 ∈ Xn, there
always holds
P{A(Xrn) ∈ U} ≤ ee ∙ P{A(X]n) ∈ U} + δ,	(1)
for every subset U ⊆ Θ.
Here two datasets X and X0 are adjacent if and only if the Hamming distance (Lei, 2011) of these
two datasets of same size H(X, X0) = 1. As we can see, the quantities and δ measure the level
of privacy loss. There are also several relaxations of differential privacy (see, e.g., Bun & Steinke
(2016); Dwork & Rothblum (2016); Mironov (2017); Dong et al. (2019)) designed for the ease
of analysis. However, in these definitions, the dataset X is always assumed to be fixed, and the
probability in (1) only takes over the randomness of the algorithm A. Therefore, it is impossible
to achieve strict differential privacy without adding auxiliary perturbations in the algorithm. On
the other hand, the statistical performance of the output is inevitably deteriorated by the additional
randomness. This lead to a large body of works discussing the tradeoff between accuracy and privacy
(Wasserman & Zhou, 2010; Bassily et al., 2014; Bun et al., 2018; Duchi et al., 2018; Cai et al., 2019).
In this paper, we consider the private sign recovery problem in the distributed system. To be more
precise, assume the parameter of interest is a sparse vector, which has many zeros in its entries.
1
Under review as a conference paper at ICLR 2021
The task is to identify the signs of the parameter from the observations stored in multiple machines
while protecting each individual’s privacy. The sign recovery problem, as an extension of sparsity
pattern recovery, has found its significance in a broad variety of contexts, including variable selection
(Tibshirani, 1996; Miller, 2002), graphical models (MeinshaUsen & Buhlmann, 2006; Cai et al.,
2011), compressed sensing (Candes & Tao, 2005; Donoho, 2006), and signal denoising (Chen et al.,
2001). However, this problem is rarely considered in the privacy community.
To address the sign recovery problem, we propose the Median Divide-and-Conquer (Med-DC)
method, a simple two-step procedure. Firstly, each local machine estimates the sparse parameter
and sends the sign-vectors back to the server; Secondly, the server aggregates these sign-vectors
using coordinate-wise median and output the final sign estimator. While mean based divide-and-
conquer (also referred to as Mean-DC) approaches have been widely analyzed in distributed learn-
ing literature (Mcdonald et al., 2009; Zhang et al., 2013; Lee et al., 2017; Battey et al., 2018), the
median-based counterpart has not yet been well explored. It is well-known that naively averaging the
local estimators behaves badly for nonlinear and penalized optimization problems. This is because
averaging cannot reduce the bias in the local sub-problems. In particular, for the distributed Lasso
problem, as mentioned in Lee et al. (2017), the estimation error of averaged local Lasso estimator
is of the same order as that of local estimators. However, when only considering the sign recov-
ery problem, we found that the Med-DC method perfectly fits the nature of the distributed private
setup. (See Section 2.2 for more detailed discussions) For the sake of clarity, we only consider the
sign recovery problem for sparse mean estimation and sparse linear regression, the two fundamental
models in statistics.
The proposed Med-DC method has the following advantages:
•	Consistent recovery. For both sparse mean estimation and sparse linear regression, the
Med-DC method consistently recovers the signs of the true parameter with theoretical guarantees.
Under some constraints, We can prove that our approach can identify signals larger than C ,log n/N
for some constant C > 0 (where N is the full sample size and n is the local sample size), which co-
incides with the minimal signal level in the single machine setting (all data stored in one machine).
•	Efficient communication. To recover the signs of the parameter of interest in the distributed
setup, a naive approach is to estimate the parameter using existing private distributed estimation
methods and take the signs of the estimators. However, these methods usually involve multi-round
aggregation of gradient information or local estimators, which seems costly for the simple sign re-
covery problem. Instead, our approach only aggregates the vectors of signs (bits information) in one
shot, which is much more communicationally efficient.
•	Weak privacy. By relaxing the differential privacy to high-probability sense, our determin-
istic Med-DC method can be proved to be weakly ‘private’. We also extend this concept to group
privacy. To the best of our knowledge, this is the first deterministic algorithm that has a provable
high-probability privacy guarantee. Moreover, as each machine only needs to transmit the vectors
of signs, instead of the local estimators or gradient vectors, our proposed method also protects the
privacy of each local machine, since gradient sharing can also result in privacy leakage (Zhu et al.,
2019).
•	Wide applicability. We believe the Med-DC approach deserves more attention due to its ex-
cellent practical performance and ease of implementation. For example, it is promising to apply the
Med-DC method to wider classes of models, (e.g. , generalized linear model, M -estimation, etc.) or
hybridize this method with many sophisticated distributed algorithms like averaged de-biased esti-
mator in Lee et al. (2017) and Communication-Efficient Accurate Statistical Estimator (CEASE) in
Fan et al. (2019).
Notations. For every vector V = (vι,…,vp)τ, denote ∣v∣2 = VZPp=I *,|v|i = PP=ι |vi|, and
∣v∣∞ = maxι≤ι≤p |v/|. Moreover, we use SuPP(V) = {1 ≤ l ≤ P | vι = 0} as the support of the
vector v, and v-l = (v1, . . . , vl-1, vl+1, . . . , vp)T. For every matrix A ∈ Rp1 ×p2, define kAk =
suP∣v∣2 = 1 |Av|2, kAk∞ = max1≤lι≤pι,1≤l2≤P2 |Ali,l2 |, kAkL∞ = suP∣v∣∞ = 1 lAvl∞ as various
matrix norms, Λmax(A) and Λmin(A) as the largest and smallest eigenvalues ofA respectively. We
will use I(∙) as the indicator function and sgn(∙) as the sign function. For two sequences an, bn,
we say an bn when an = O(bn) and bn = O(an) hold at the same time. For simplicity, we
denote Sp-1 and Bp as the unit sphere and unit ball in Rp centered at 0. For a sequence of vectors
{vi}n=ι ⊆ Rp, we denote med(∙) as the coordinate-wise median. Lastly, the generic constants are
assumed to be independent ofm, n, and p.
2
Under review as a conference paper at ICLR 2021
2	Private Sign Recovery of Mean Vector
2.1	Median based Divide-and-Conquer
Let μ* =(Μ；，...，μp)T be the true parameter of interest. We assume the vector is sparse in the
sense that many entries μ↑ are zero. There are N i.i.d. observations Xi’s satisfying E[Xi]=
μ*, and they are evenly stored in m different machines Hj (where 1 ≤ j ≤ m). Denote X =
{X1, . . . , XN} as the full dataset. For simplicity we assume N = mn so that each machine has
equally n samples. Our task is to identify the signs of μ* on all coordinates (denoted as sgn(μ*)) in
this distributed setup while protecting the privacy of every element Xi on each machine Hj.
To recover the signs of the true mean vector μ* privately, the most direct way is to estimate the mean
by some existing differentially private algorithms and take the signs of the estimator coordinate-
wisely. By post-processing property of differential privacy (Proposition 2.1 in Dwork & Roth
(2014)), we know this sign recovery method is also differentially private. Private mean estima-
tion is a fundamental problem in private statistical analysis and has been studied intensively (Dwork
et al., 2006; Lei, 2011; Bassily et al., 2014; Cai et al., 2019). The standard approach is to project the
data onto a known bounded domain, and then apply the noises according to the diameter of the fea-
sible domain and the privacy level. However, this method requires input data or the true parameter
lies in a known bounded domain, which seems unsatisfactory in practice. Moreover, since we only
want to estimate the signs, which take value in the discrete set {-1, 0, 1}, it seems unnecessary to
perturb the mean directly.
Indeed, to recover the signs of the true parameter, there is no need to obtain an accurate mean with
all data. Instead, we propose a Median based Divide-and-Conquer (Med-DC) approach. To be more
precise, we can estimate the signs on each local machine Hj , and aggregate these vectors of signs
by taking median to produce a more accurate sign estimator.
To present our method more clearly, We define the following quantization function Qλ(∙),
sgn(x) if |x| > λ,
Qλ(X) = sgn{ Sgn(X) TxI- λ)+,} = ∣0	if |x| ≤ λ	⑵
shrinkage operator
Here λ is a thresholding parameter. When X is a vector, Qλ(X) performs the above operation
coordinate-wisely. In particular, when λ = 0, the function Qo(∙) acts the same as the sign func-
tion sgn(∙). Then we present our method in Algorithm 1.
Algorithm 1 Median divide-and-conquer (Med-DC) for sparse mean estimator.
Input: Dataset X = {X1, . . . , XN} evenly divided into m local machines Hj (where j =
1, . . . , m), the universal thresholding parameter λN .
1:	for j = 1, . . . , m do
2:	The j-th machine Hj computes the local sample mean Xj = n-1 Pi∈Hj Xi. Then Hj
sends Qj = QλN (Xj) to the server.
3:	end for
4:	The server takes coordinate-wise median
Q(X) = med(QλN(Xj) ∣ 1 ≤ j ≤ m),	⑶
Output: The vector of signs Q(X).
The choice of thresholding parameter will be discussed after Theorem 1 in Section 2.3. Especially
mention that there are some cases when the median is not uniquely determined. For example, it
is possible that there are the same numbers of 0’s and 1’s, then the median can be arbitrary value
in [0, 1]. To avoid ambiguity, we simply take Ql(X) = 0 (where l denotes the coordinate index)
whenever the median Ql(X) is not unique.
Another important remark is that, the proposed sign recovery algorithm Q(∙) is deterministic. More
precisely, as there is no additional random perturbation in Algorithm 1, the output Q(X) is com-
pletely determined by the input dataset X. It is able to protect data privacy in a weaker sense.
3
Under review as a conference paper at ICLR 2021
Med-DC
Figure 1: This figure visualizes the mechanism of the median divide-and-conquer (Med-DC) ap-
proach. Denote S+, S- and Sc as the sets of positives, negatives, and zeros of the true parameter
μ* respectively. The black dots and white dots on each column represent the estimated positive and
negative locations on each local machine.
2.2	Intuition Behind Med-DC
Before presenting the theoretical results of our Med-DC method, we briefly illustrate the intuition
behind it. The median mechanism among the collection of discrete values in {-1, 0, 1} can be
equivalently regarded as a voting game. At each coordinate, we take the element which gets more
than half of the votes, and we take it as 0 when there is no such element. The mechanism of the
Med-DC method can be visualized in Figure 1.
Recovery Consistency. The key insight of the Med-DC method is that, according to Berry-Essen
theorem, the distribution of sample means on local machines is close to normal distribution centered
at the true parameter μ*. Therefore, on each coordinate l, the local sample means {X1,1,..., Xm,ι}
are approximately symmetrically distributed around μ∣. Based on this observation, the sign recovery
consistency of the median mechanism becomes clear: For μ∣ > Xn (μj < -Xn), it is likely to have
at least m/2 elements larger than λN (smaller than -λN), which makes the median of local signs
more inclined to be 1 (-1). For μj = 0, by the approximate symmetry of local sample means, the
numbers of 1’s and -1’s are likely to be equal. Thus the median tends to be 0.
Weak Privacy. Given the dataset X and the adjacent datasets X0 , when applying Algorithm 1 to X0 ,
there would be at most one element among the set of signs {QλN (Xj)}m=ι change. With high prob-
ability, the change of one element would not affect the median value of Q(X). This interpretation is
conceptually coincident with the idea of differential privacy in Definition 1. However, this ‘privacy’
guarantee does not hold for all data. For instance, in one-dimensional case, it is possible that X
produces (m/2) positives and m/2 negatives (Q(X) = 0), and the adjacent dataset X0 produces
m/2 + 1 positives and m/2 - 1 negatives (Q(X0) = 1), which contradict with standard definition
of differential privacy. However, by Proposition 1, we can show that the Med-DC method guaran-
tees privacy in a weaker sense with probability tending to 1, which implies that the aforementioned
unpleasant case only appears with a small probability.
Connection with Robust Statistics. Our Med-DC method has an intimate connection with the
median-of-means (MOM) estimator, a robust mean estimator which has attracted considerable recent
interests in statistics and machine learning communities (Nemirovsky & Yudin, 1983; Yin et al.,
2018; Minsker, 2019; LeCUe & Lerasle, 2020). To estimate the mean, the MOM estimator takes the
median among the local sample means. Both the MOM estimator and our Med-DC method use the
4
Under review as a conference paper at ICLR 2021
symmetrization effect of the local average. As our task is to find the signs of μ*, We only aggregate
the signs of each local estimator.
Robust statistics studies the estimator that is not much influenced by a small portion of data, Which is
conceptually similar to differential privacy. Indeed, the connection betWeen robustness and privacy
has been pointed out in DWork & Lei (2009); Smith (2011); Avella-Medina (2019); Brunel & Avella-
Medina (2020). In particular, Brunel & Avella-Medina (2020) leverages the MOM estimator and
“Propose-Test-Release” (PTR) frameWork in DWork & Lei (2009) to develop private mean estimator
Without any boundedness assumptions on the data and parameter. It is WorthWhile noting that, We
can also combine the PTR-frameWork With our Med-DC approach to develop a strictly differentially
private sign estimator.
2.3	Theory of Mean Vector Sign Recovery
To discuss the theoretical properties of our method, We introduce the distribution space P ofX
P(μ*,C) = (p∣Ep[X] = μ, max Ep[∣Xi - 〃； |3] ≤ ",	(4)
1≤l≤p
Where C > 0 is some constant. This is a rather Weak condition on the distribution of X . At each
coordinate, We assume Xl has a finite third-order moment, Which is common in median-of-mean
ʌ
literature (Minsker, 2019). Then We have the sign consistency of the proposed estimator Q(X).
Theorem 1. (Sign consistency of Med-DC) Let N = mn i.i.d. random vectors {X1, . . . , XN}
SamPledfrom P(μ*,C) be evenly distributed in m subsets Hi,..., Hm. Moreover, there are suffi-
ciently large constants C1,C2,γ0 > 0, such that
(a)	The dimension P satisfies P = O(nγ0), and take Xn = Ci(dlog n/N + 1/n);
(b)	Denote S = supp(μ*), then there is minι∈s ∣μj∣ ≥ C2λN.
r∏ι z∖∕'^7∙∖ 1 /` 1 ■ λ 1	∙. 1	1	. ■ rι .1 . r	ι	ι	ι	C C	. ι	■
Then Q(X) defined in Algorithm 1 satisfies that, for some large γ1 dePends on C1, C2, γ0, there is
P(Q(X) = sgn(μ*)) ≥ 1 - n-γ1.
As We can see from assumptions (a) and (b), When m = O(n), the thresholding parameter λN can
be chosen to be Ci，log n/N, which means our algorithm can identify the signal above the order of
O(dlog n/N). This is coincident with the optimal signal-to-noise ratio in a single machine setting
(all N data are stored in one machine). Moreover, we note that the assumptions in Theorem 1 does
not really require the true parameter to be ‘sparse, in the sense that S《p. Instead, we only assume
there is a fixed gap (at the level of O(dlog n/N)) between zeros and those nonzero elements. In the
following proposition, we show that our algorithm can protect data privacy with high probability.
Proposition 1. (Privacy of Med-DC) Under the same assumPtions as Theorem 1, denote D(X) as
the collection of datasets X0 adjacent to X, then for some large γ2 > 0, there is
P(Q(X) = Q(X0), for all X0 ∈ D(X)) ≥ 1 — n-γ2.	(5)
It is worth noting that, the privacy guarantee formulated in equation (5) differs with the standard
definition of differential privacy (see Definition 1) in several aspects. Firstly, as the algorithm Q(∙)
is deterministic, the randomness in (5) comes from the data generating mechanism. It implies that
this algorithm preserves data privacy with probability tending to 1, and precludes some extreme
cases, which may happen with a small probability. This is essentially different from the standard
definition of differential privacy and its variants, which always assume the dataset is fixed, and the
randomness comes from the algorithm itself. Secondly, combining Theorem 1 and Proposition 1, we
know that with probability tending to 1, this algorithm recovers the true signs, and the modification
ofa single entry ofX does not affect the output at all (0 privacy loss). Therefore, this method can be
roughly regarded as a (0, 0)-differentially private algorithm.
5
Under review as a conference paper at ICLR 2021
3	Private Sign Recovery of Linear Regression
3.1	Med-DC of Regression Parameter
Let (Xi, Yi) (where i = 1, . . . , N) be i.i.d. observations from the model
Y = X Tθ* + z,	(6)
where θ* = (θɪ,..., θp)T is the true sparse regression parameter, and Z is the noise independent
with the covariate X. Denote the full dataset as X = {(X1, Y1), . . . , (XN, YN)}, and X is evenly
divided into m local machines Hj (where 1 ≤ j ≤ m). Similarly, we attempt to recovery the vector
of signs sgn(θ*) = (sgn(θ 打，...，sgn(θp))T.
Sparse linear regression is an important topic in the statistical literature. The least absolute shrinkage
and selection operator (Lasso), which was firstly introduced in Tibshirani (1996), has been one of
the most popular approaches because of its benign theoretical guarantee and excellent empirical
performance. Recently, a private iteratively hard thresholding pursuit algorithm was developed in
Cai et al. (2019) to solve Lasso problem in the differential privacy framework. To deal with the
private sign recovery problem in the distributed setup, a naive approach is to solve the private Lasso
problem on each local machine and take the signs of the average of all local estimators. However,
this method also requires the boundedness assumption on the covariates, and it is too complicated
for sign recovery. More importantly, the average of the local Lasso estimators is likely to cause
more non-zero elements because the coordinate will become non-zero as long as one of these local
estimators is non-zero at this coordinate.
By leveraging the idea of Med-DC, we present Algorithm 2 for sign recovery of sparse regression.
Algorithm 2 Median divide-and-conquer for sparse linear regression (Med-DC Lasso)
Input: Data on local machines {(Xi, Yi) | i ∈ Hj} for j = 1, . . . , m, the universal regularization
parameter λN .
1:	for j = 1, . . . , m do
2:	The j-th part Hj computes the local sample mean
θj = argmin ɪ X (Yi - XTθ)2 +》n∣Θ∣i.	(7)
θ∈Rp 2n i∈Hj
Then the j-th local machine sends Q0 (θj) to the server.
3:	end for
4:	The server takes coordinate-wise median Q(X) = med(Q0(θj) |1 ≤ j ≤ m).
Output: The vector of signs Q(X).
Similarly as Algorithm 1, every step of the Med-DC Lasso method is deterministic. Therefore, we
can solve each local subproblem (7) efficiently by many well-developed algorithms like FISTA in
Beck & Teboulle (2009), ADMM in Boyd et al. (2011), etc.
3.2	Theory of Regression Parameter Sign Recovery
For linear regression, we consider the following distribution space
Px,y(θ*,η1,C1,η2,C2) = (p∣ SUp Ep{exp(ηι∣vTX|2)} ≤ Cl,
|v|2=1	(8)
Z = Y - XTθ*,z ⊥ X, Ep{ exp(η2∣z∣2)} ≤ C2},
where C1 , C2 , η1 , η2 are some positive constants. This implies that both the covariate vector X and
the noise Z are sub-Gaussian. Then we have the following result of sign recovery consistency.
Theorem 2. (Sign consistency of Med-DC Lasso) Let N = mn samples X =
{(X ι,Yι),...,(X N ,Yn )} from Pχ,γ (θ*,η1,C1,η2, C2) be evenly distributed in m subsets
6
Under review as a conference paper at ICLR 2021
Hi,..., Hm. Moreover, there are some sufficient large constants C3, C4, ∆0 > 0 such that
(a)	The dimension P is fixed, and take Xn = C3(y∕∖0g n/N + log n/n);
(b)	Denote S = supp(μ*), the minimal signal satisfies minι∈s ∣μj∣ ≥ C4λN;
(c)	The covariance matrix Σ = EXXT is positive definite. Let Σ-1 = (ω1, . . . , ωp), there is
|3-l|1 / 1 八
max-------- ≤ 1 一 ∆0.
l∈Sc ωl,l
r∏ι	ʌ ∕∙^r∖ J	J- Al ∙.ι	.	.1 . r	ι	ι	ι	CC A,，	■
Then Q(X) defined in Algorithm 2 satisfies that, for some large γ1 depends on C3, C4, ∆0, there is
P(Q(X) = sgn(θ*)) ≥ 1 - n-γ1.
From the assumption (b), when m log n = O(n), the minimal signal has the order of O (ʌ/log n/N),
which meets the “beta-min” condition (Wainwright, 2009) of standard Lasso problem for full sample
case (all N samples stored in a single machine). Note that the regularization parameter λN is
a universal constant among all local machines. By assumption (a), the regularization parameter
satisfies Xn N，log n/N, which is smaller than the standard setting O(dlog n/n) (Lee et al.,
2017). Therefore, the local estimators θj is not very sparse because the regularization parameter XN
is unable to annihilate the noises brought by the local data. However, by taking the median among
the local signs, the noises are canceled out and the signals become detectable. Owing to the smaller
scale, it helps to identify the smaller magnitude of signals. The assumption (c) implies that, for
l ∈ Sc, the l-th row of the precision machine Σ-1 is dominated by the diagonal entry ωl,l. It can be
regarded as a more strict irrepresentability condition (Zhao & Yu, 2006; Wainwright, 2009).
Proposition 2. (Privacy of Med-DC Lasso) Under the same assumptions as Theorem 2, denote
D(X) as the collection of datasets X0 adjacent to X, then for some large γ2 > 0, there is
P(Q(X) = Q(X0), for all X0 ∈ D(X)) ≥ 1 — n-γ2.	(9)
Similar as (5), equation (9) is a weakened privacy guarantee, which can protect data privacy with
high probability. In addition, follow the proofs of Proposition 1 and 2, our proposed methods also
guarantee group privacy with high probability (Section 10.1 in Dwork & Roth (2014)).
Corollary 1. (Group Privacy) Under the same assumptions as Theorem 2, denote Dk (X) as the
collection of datasets X0 have at most k elements differing with X, then for some large γ3 > 0, then
P(Q(X) = Q(X0), for all X0 ∈ Dk (X)) ≥ 1 - n-γ3.
4 Simulation Study
4.1	Results for Sparse Mean Estimation
In the first experiments, we consider the sparse mean estimation problem, observations
{X1, . . . , XN} are sampled from the model
X i = μ* + zi,
where the noises zi’s are drawn from the multivariate normal distribution N (0, Ip). We fix the
dimension p as 200. The parameter of interest is defined as
μ = (1,0.8,…，0.2,0,-0.2,…，一0.8, -1, 0T-ii )t,	(10)
which means the sparsity level s is fixed as 10. The data is divided into 100 local machines
H1, . . . H100, each local sample size is n = 200. Therefore, the entire sample size is N = 200×100.
For the choice of regularization parameter XN in each local machine, we first choose X0N based on
the dataset in the first local machine Hi by five-fold cross-validation. Moreover, motivated by the
theoretical scale difference in Theorem 1, We further divide λn by √m, namely, Xn = λn/√m. We
compare with the following three methods:
(a)	Mean-DC: Replace the aggregator median in Med-DC by taking average;
(b)	CWZ: Perform the method proposed in Cai et al. (2019) on all samples in a single machine.
7
Under review as a conference paper at ICLR 2021
Same as Cai et al. (2019), We adopt the oracle T = 2√log N, S = 10, and (e, δ) = (0.5,10/N1.1);
(C) Pooled-Mean: Take average among all samples and take quantization function Q，n (X).
Note that Mean-DC and CWZ need to transmit the local estimators to the server, Which is more
communication costly. The performance of sign recovery is measured by the folloWing four criteria:
•	Positive and Negative False Discovery Rate.
Pfrf „+ I{Qι(X) = 1}
PFDR =—乙l∈S+ i"'——-—,NFDR
max[Pf=ι I{Q^ι (X) = 1},1],
Pl∈S- I{QI(X) = -1}
max[Pl=ι I{Qι(X) = -1}, 1]
•	Total False Discovery Rate and Power.
FDR=W，Power=m⅛≡bl
Table 1: The PFDR, NFDR, FDR, poWer, and their standard errors (in parentheses) of different
methods under sample size N = 200 X 100, local sample size n = 200.
	Med-DC	Mean-DC	CWZ	Pooled-Mean
PFDR	0.0684 (0.1251)	0.9472 (0.0038)	0.6981 (0.2122)	0.0000 (0.0000)
NFDR	0.0729 (0.1283)	0.9473 (0.0036)	0.6834 (0.2095)	0.0000 (0.0000)
FDR	0.0751 (0.1170)	0.9475 (0.0008)	0.6800 (0.1807)	0.0000 (0.0000)
Power	1.0000 (0.0000)	1.0000 (0.0000)	0.3200 (0.1807)	0.7840 (0.0846)
From Table 1, our Med-DC approach clearly outperforms the non-private Mean-DC and private
CWZ method. Comparing With the pooled-mean estimator, our method has higher poWer.
4.2 Results for Sparse Linear Regression
In the second experiments, We consider the linear model defined in (6). Let the noises are i.i.d.
from N(0, 1) and assume the i.i.d. covariate vectors XiT = (Xi,1, . . . , Xi,p) (i = 1, . . . , N) are
draWn from a multivariate normal distribution N (0, Σ). The covariance matrix Σ is ap × p Toeplitz
matrix With its (i, j)-th entry Σij = 0.5|i-j|, Where 1 ≤ i, j ≤ p. We fix the dimension p = 200.
Moreover, we set the true coefficient θ* the same as μ* in (10). Similarly, we set m = 100,
n = 200. For the choice of regularization parameter λN in each local machine, We first choose
λ0N based on the dataset in the first local machine H1 by five-fold cross-validation. As suggested in
classical literatures (Wainwright, 2009), motivated by the theoretical scale difference in Theorem 2,
and further divide it by √m. In addition to Mean-DC, we mainly compare with other two methods:
(d)	CSL: Use the Communication-efficient Surrogate Likelihood (CSL) framework in Jordan et al.
(2019) to obtain an estimator Θcsl for the true parameter and take the signs of it.
(e)	Pooled-Lasso: Solve the Lasso problem on all data in a single machine and take the signs.
Note that all the above-mentioned methods are not private. Both the Mean-DC method and the CSL
method require to transmit information of local parameters or multi-round gradients, which is more
communication-costly than ours. In particular, we note that the CSL method includes an iterative
refinement of the estimator. In our simulation study, we present the results of the five-step CSL
method. For each experiment, we repeat 500 independent simulations and report the number of
PFDR, NFDR, FDR, and power.
Table 2: The PFDR, NFDR, FDR, power and their standard errors (in parentheses) of different
methods under sample size N = 200 × 100, local sample size n = 200.
Med-DC	Mean-DC	CSL	Pooled-Lasso
PFDR
NFDR
FDR
Power
0.0639 (0.1329)
0.0582 (0.1199)
0.0645 (0.1184)
1.0000 (0.0000)
0.9454 (0.0425)
0.9456 (0.0425)
0.9457 (0.0424)
1.0000 (0.0000)
0.4453 (0.3447)
0.4325 (0.3480)
0.4466 (0.3397)
0.9998 (0.0045)
0.5703 (0.1758)
0.5808 (0.1717)
0.5896 (0.1464)
1.0000 (0.0000)
It can be observed from Table 2, while all these methods can select the true support set, the Med-DC
Lasso method has apparently smaller false discovery rate than others.
8
Under review as a conference paper at ICLR 2021
References
Marco Avella-Medina. Privacy-preserving parametric inference: A case for robust statistics. Journal
OftheAmerican Statistical Association, 0(ja):1-45, 2019.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In Proceedings of the 2014 IEEE 55th Annual Symposium on
Foundations OfComputer Science, FOCS '14,pp. 464T73. IEEE Computer Society, 2014. ISBN
9781479965175.
Heather Battey, Jianqing Fan, Han Liu, Junwei Lu, and Ziwei Zhu. Distributed estimation and
inference with statistical guarantees. AnnalS OfStatiStiCS, 46(3):1352-1382, 2018.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J. Imaging Sci., 2(1):183-202, 2009.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers. Foundations and
Trends in Machine Learning, 3(1):1-122, 2011. ISSN 1935-8237.
Victor-Emmanuel Brunel and Marco Avella-Medina. Propose, Test, Release: Differentially private
estimation with high probability. arXiv e-prints, art. arXiv:2002.08774, February 2020.
Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions, and
Iowerbounds. In TCC(BI), pp. 635-658. Springer, 2016. doi: 10.1007/978-3-662-53641-4.24.
Mark Bun, Jonathan Ullman, and Salil Vadhan. Fingerprinting codes and the price of approximate
differential privacy. SIAM Journal on Computing, 47(5):1888-1938, 2018.
T. Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for
parameter estimation with differential privacy. arXiv e-prints, art. arXiv:1902.04495, February
2019.
Tony Cai and Weidong Liu. Adaptive thresholding for sparse covariance matrix estimation. Journal
of the American Statistical Association, 106(494):672-684, 2011. ISSN 01621459.
Tony Cai, Weidong Liu, and Xi Luo. A constrained `1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical Association, 106(494):594-607, 2011.
E. J. Candes and T. Tao. Decoding by linear programming. IEEE Transactions on Information
Theory, 51(12):4203-4215, 2005.
Scott Shaobing Chen, David L. Donoho, and Michael A. Saunders. Atomic decomposition by basis
pursuit. SIAM Review, 43(1):129-159, 2001.
Y.S. Chow and H. Teicher. Probability Theory: Independence, Interchangeability, Martingales.
Springer Texts in Statistics. Springer New York, 2012. ISBN 9781461219507.
Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian differential privacy. arXiv e-prints, art.
arXiv:1905.02383, May 2019.
D. L. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-1306,
2006.
John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Minimax optimal procedures for
locally private estimation. Journal of the American Statistical Association, 113(521):182-201,
2018.
Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In Proceedings of the Forty-
First Annual ACM Symposium on Theory of Computing, STOC ’09, pp. 371-380. Association for
Computing Machinery, 2009. ISBN 9781605585062.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, August 2014. ISSN 1551-305X.
9
Under review as a conference paper at ICLR 2021
Cynthia Dwork and Guy N. Rothblum. Concentrated differential privacy. arXiv e-prints, art.
arXiv:1603.01887, March 2016.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, pp. 265-284.
Springer Berlin Heidelberg, 2006. ISBN 978-3-540-32732-5.
Jianqing Fan, Yongyi Guo, and Kaizheng Wang. Communication-efficient accurate statistical esti-
mation. arXiv e-prints, Jun 2019.
Jihun Hamm, Yingjun Cao, and Mikhail Belkin. Learning privately from multiparty data. In
Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of Machine Learning Re-
search, volume 48, pp. 555-563. PMLR, 20-22 Jun 2016.
Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Distributed learning without
distress: Privacy-preserving empirical risk minimization. In Advances in Neural Information
Processing Systems 31, pp. 6343-6354. Curran Associates, Inc., 2018.
Michael I. Jordan, Jason D. Lee, and Yun Yang. Communication-efficient distributed statistical
inference. Journal of the American Statistical Association, 114(526):668-681, 2019.
Guillaume LecUe and MatthieU Lerasle. Robust machine learning by median-of-means: Theory and
practice. Annals of Statistics, 48(2):906-931, 2020.
Jason D. Lee, Qiang Liu, Yuekai Sun, and Jonathan E. Taylor. Communication-efficient sparse
regression. Journal of Machine Learning Research, 18(1):115-144, January 2017. ISSN 1532-
4435.
Jing Lei. Differentially private m-estimators. In Advances in Neural Information Processing Systems
24, pp. 361-369. Curran Associates, Inc., 2011.
Ryan Mcdonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S. Mann. Efficient
large-scale distributed training of conditional maximum entropy models. In Advances in Neural
Information Processing Systems 22, pp. 1231-1239. Curran Associates, Inc., 2009.
Nicolai Meinshausen and Peter BUhlmann. High-dimensional graphs and variable selection with the
lasso. Annals of Statistics, 34(3):1436-1462, 06 2006.
A. Miller. Subset Selection in Regression. New York: Chapman and Hall/CRC, 2002.
Stanislav Minsker. Distributed statistical estimation and rates of convergence in normal approxima-
tion. Electronic Journal of Statistics, 13(2):5213-5252, 2019.
I.	Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Sympo-
sium (CSF), pp. 263-275, 2017.
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem Complexity and Method
Efficiency in Optimization. Wiley, 1983.
Manas Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggregation of
locally trained classifiers. In Advances in Neural Information Processing Systems 23, pp. 1876-
1884. Curran Associates, Inc., 2010.
Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In Pro-
ceedings of the Forty-Third Annual ACM Symposium on Theory of Computing, STOC ’11, pp.
813-822. Association for Computing Machinery, 2011. ISBN 9781450306911.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 58(1):267-288, 1996.
Martin J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-
constrained quadratic programming (lasso). IEEE Transactions on Information Theory, 55(5):
2183-2202, 2009.
10
Under review as a conference paper at ICLR 2021
Larry Wasserman and Shuheng Zhou. A statistical framework for differential privacy. Journal of
the American Statistical Association,105(489):375-389, 2010.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed
learning: Towards optimal statistical rates. In Proceedings of the 35th International Conference
on Machine Learning, volume 80, pp. 5650-5659, 10-15 Jul 2018.
Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Communication-efficient algorithms for
statistical optimization. Journal of Machine Learning Research, 14:3321-3363, 2013.
Peng Zhao and Bin Yu. On model selection consistency of lasso. Journal of Machine Learning
Research, 7(Nov):2541-2563, 2006.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems 32, pp. 14774-14784. Curran Associates, Inc., 2019.
11
Under review as a conference paper at ICLR 2021
A Technical Lemmas
Lemma 1. (Berry-Esseen Theorem, Theorem 9.1.3 in Chow & Teicher (2012)) If {Xi, i ≥ 1} are
i.i.d. mean-zero random variables with E[X，] = σ2, E|Xi|3 < ∞. Then there exists a constant
CB > 0 such that
sup P
一∞<x<∞
(^X Xi < √nσxj - Φ(x) ≤
CB
Lemma 2. (Exponential Inequality, Lemma 1 in Cai & Liu (2011)) Let Xi,…,Xn be i.i.d. random
variables with zero mean. Suppose that there exist some η > 0 and C > 0 such that E[X2eη∣Xi∣] ≤
C. Then uniformlyfor 0 < x ≤ C and n ≥ 1, there is
P nn XXi ≥ (η + η-1)χ} ≤ exp (-nC-).
Lemma 3. Let N(= mn) i.i.d. random variables X∖,...,Xn evenly distributed in m subsets
Hi,..., Hm. Suppose E [Xi] = 0, E[X2] = σ2, E∣Xi∣3 < ∞. Denote Xj = Pi∈% Xiln as
the local sample mean on Hj. For ^very γ > 1, denote
CY=C( ɪ+ɪ+√⅛),
∖ n	mʌ/n	V mn I
where C > 0 is sufficiently large enough. Thenfor every fixed non-negative COnStant k, there is
{m	}	( m	}
XI (Xj < Cγ) ≤ m + k+ P JXI (Xj > -CY) ≤ m + k= O(L).	(11)
Proof. For every x > 0, there is
p∣χχ i (Xj<χ) ≤ m+k
(1	二一	、 一 、1	k 一	、
m	∑i	(Xj	< x)	- P (X1	<χ)	≤	2	+	m - P (Xi	<χ)
≤p1 m∙	X i	(Xj	<χ	- P X	<x)	≤	2+m+√n-	φ	F
=PJ m	X I	(3	< χ)	-P X	<χ)	≤	- rCγmgn),
where the last line uses Berry-Esseen Theorem (Lemma 1), and X is given by
X=J φ-i(1+&+CB+rCγ iog n).
√n	y 2 m	√n	Vml
Applying Lemma 2 to the i.i.d. sequence I (Xj < χ) - P (Xi < χ), we have
P [ m X1 (Xj < χ) - P (Xi <χ) ≤ - JCYmgn ) = O(n-γ),
for some C large enough. Moreover, we have the following elementary facts
悭T(XO)I = ∣φT(χo) - φ-i (1∕2)∣≤∣χ0 - 1∕2∣ (φT)' (χo) ≤ ψ{φ二(；£)}
12
Under review as a conference paper at ICLR 2021
holds for any 1/4 ≤ xo < 3/4. On the other hand, We know that 1/4 ≤ Φ(√nx∕σ) < 3/4 holds
for m, n sufficiently large. Denote Cδ = 1/ψ{Φ-1(3/4)}, then there is
p {X I "√σ (√n+mk+「)) ≤ 三
≤P∣ m XI(3 < X)-P X < X)≤ -rCγmgn} ≤『「
Therefore, if we choose C ≥ max{CδCBσ, Cδ√Cσ}, the bound of the first term in the left hand
side of (11) is proved. By repeating the same procedure, we can prove the bound of the second term,
which yields the desired result.	□
Lemma 4. Let X1, . . . , Xn be i.i.d. random vectors sampled from the distribution in (8). Denote
its covariance matrix as Σ, and the sample covariance matrix as Σ. Thenfor every γ > 1, there
exists a constant C > 0 such that
P(HIς - Ik ≥ C
O(n-γ).
Proof. Recall that the inverse covariance matrix is denoted as Σ-1 = (ω1, . . . , ωp), and el as the
l-th coordinate vector. Then the (lι, l2)-entry of the matrix Σ-1Σ — I is
1n
(£ 'ς - I )lι,l2 = n Eωf X i ∙ el2 X i - δl1,l2 .
n i=1
Since the dimension p is assumed to be bounded, and the covariance matrix is positive definite, there
exist a constant ρ ∈ (0, 1) such that
ρ ≤ Λmin(Σ) ≤ Λmax(Σ) ≤ ρ-1.
Then we have
max M |2 ≤ ∣∣ς-1∣∣ ≤ ρ-1.
1≤l≤p
Since X is sub-Gaussain in (8), we obtain
max .E{ expηιP∣ωTX「eTXi - δl 1 ,l 2 I}
1≤l1,l2 ≤p
≤eη1ρ ∙ SUp E{η1∣vTX|2} ≤ eη1ρC1.
∣V∣2≤1
Therefore, we can apply Lemma 2 to each coordinate and yield
p(∣∣∑-iς - Ill∞: +f)
≤p2 1≤maX≤pP(l1 X ωT1 Xi ∙ eT Xi- 必』≥ CrlF
i=1
=O(p2n-γ-2 ) = O(n-γ),
1'	入 i'{'	♦	.1	1	EI	Γ∙	.1	1	♦	1
for some C sufficiently large. Therefore, the lemma is proved.
□
B Proof of Main Results
Proof of Theorem 1. Ifwe can show that
max P(QI(X) = sgn(〃；)) = O(n-γ),	(12)
1≤l≤p
13
Under review as a conference paper at ICLR 2021
where γ > 0 is large enough. Then this theorem is proved as follows
1 - P (Q(X) = sgn(μ*))
≤p max P(Qι(X) = sgn(μi))=O(Pn-Y) = O(n-γ+γ0),
1≤l≤p
provided that γ > γo. For the l-th coordinate, We firstly suppose that μj = 0. Since Xis are
sampled from the distribution (4), by Lemma 3 with k = 0, if
We have
P(Q ι (X) = Sgn(〃；))
mm
≤p{ XIX^j-^1	V 入N)	≤ y}	+ p{	XIX^jj,ι	> —》n)	≤ y}	= O(n-γ).
j=1	2	j=1	2
Next we assume μj > 0. We use Lemma 3 again, if
记 ≥C1 (> +1 )≥cγ + λN,
mn n
then there is
m
P (Ql(X) = sgn(〃；)) ≤ p{ X IXjjj,ι > λ" ≤ 蓝}
j=1
m
≤p{ XI(Xj,ι -μ; > -cγ) ≤ m}=O(n-γ).
j=1
Lastly, when μj < 0, the proof is the same as above. Therefore (12) is proved.
□
Proof of Proposition 1. For X0 ∈ D(X), denote the elements in X0 as X0i (where 1 ≤ i ≤ N),
where
N
X I(X0i 6= Xi) = 1.
i=1
Moreover, since these data are stored in m different machines H1, . . . , Hm, we have
m
0≤XI(Xj0j 6=Xjj) ≤ 1.	(13)
j=1
Noticing that
Q Q Q
(((
PPP
≤≤
for all X0 ∈ D
=sgn(μ*), for all X0 ∈ D(X); Q(X) = sgn(μ*)) + P(Q(X)= sgn(μ*))
=sgn(μ*), for all X0 ∈ D(X)) + O(n-γι).
Therefore, we only need to show that
P(Q(XO) = sgn(μ*), forallX0 ∈ D(X))=O(n-γ2).	(14)
14
Under review as a conference paper at ICLR 2021
For the l-th coordinate, We firstly suppose that μ↑ = 0. Then there is
P(Qι(X0) = 0, for all X0 ∈ D(X))
m
≤p{ X I(Xj,1 < Xn) ≤ 彳,for all X0 ∈ D(X)}
j=1	2
m
+ P{ X I(Xj,ι > -Xn) ≤ mm, for all X0 ∈ D(X)}
j=1	2
mm
≤p{ XI(xj,ι < λN) ≤	+ l} + p{ XI(Xj,ι > -λN) ≤ m + l},
Where the last inequality uses (13). Using Lemma 3 With k = 1, With XN properly chosen, there is
mm
p{ XI(Xj,ι < xn) ≤ "2 + 1} + p{ XI(Xj,ι > -Xn) ≤ ^2 + 1} = O(n-γ),
j=1	j=1
for some γ > 0. Similarly, if μ∣ > 0, there is
P(Qι(X0) = 1, for all X0 ∈ D(X))
m
≤p{ X I(Xj,1 > Xn) ≤ mm, for all X0 ∈ D(X)}
j=1	2
m
≤p{XI(Xj,ι >λN) ≤ m + ι}
j=1
m
≤P{ XI(^jil - μ >λN - μj ≤ _2 + 1} = O(n-γ),
j=1
Where the penultimate line uses (13) and the last line uses Lemma 3 With k = 1. The proof When
μj < 0 is similar, therefore
P(Q(X0) =sgn(μ*), for all X0 ∈ D(X))
≤pP(Qι(X0) = sgn(μ*), for all X0 ∈ D(X)) = O(Pn-Y) = O(n-γ2),
which proves (14). Therefore the proposition is proved.	□
Proof of Theorem 2. For each j ∈ {1, . . . , m}, taking sub-gradient of (7) at θj, we have that
1 X (Yi - XTθj)Xi + XnZj = 0,
n i∈Hj
where Z j is the sub-gradient satisfying |Z j∣∞ ≤ 1. Rearranging the terms and multiplying Σ-1 on
the both sides, we have
θj - θ* = (I - Σ-11 X XiXT)(θj - θ*) + 1 X Σ-1Xizi + XnΣ-1Zj.	(15)
n i∈Hj	n i∈Hj
Taking η = min{ηιρ, η2}, since the noise and covariates are assumed to be sub-Gaussian in (8), for
each coordinate l ∈ {1, . . . , p}, we have
max
1≤1≤p
E{ expη∣ωιX ∙ z∣}
≤ max En exp (1 npMX|2 +1 η2
1≤ι≤p	2	2
|z|2
≤ max
1≤ι≤p
1/2
[e{ expηρ∣ωιX∣2} ∙ E{ expη2∣z∣2}]
≤ √ClC2.
15
Under review as a conference paper at ICLR 2021
Therefore by Lemma 2, We know that there exists a constant CCi > 0 such that
max
1≤j≤m
I1 X ∑-1χ izi∣
n i∈Hj	∞
≤ CIa
n
(16)
with probability larger than 1 - O(n-γ). On the other hand, by the fact that |Zj∣∞ ≤ 1, we have
∣λN∑-1zj∣∞ ≤ λN∣∣∑-1∣∣Lo.
Moreover, by Lemma 4, we know
max ∣I -
1≤j≤m
ς-1 1X XiXT∣∣L∞
i∈Hj
(17)
(18)
Substitute (16) (17) (18) into (15), we have
∣θj - θ*l∞ ≤ 2λN∣∣∑-1∣∣L
+ 2C71A ∣0gn
∞n
From (15), the l-th coordinate can be written in the following form
O
θj,ι
-θl = λNωl,lZj,l + λNωl,-lZj,-l + n ,X ωTXizi + oP
(19)
It left to rehash the argument in the proof of Theorem 1. From Lemma 5 below we have that
1 - P(Q(X) = sgn(θ*)) ≤ P ∙ maχ P(Qι(X) = sgn(θ:)) = O(n-γ+1).
Therefore we proved Theorem 2.
□
Lemma 5. Assume the same assumptions in Theorem 2. For every 1 ≤ l ≤ p, we have
P(QI(X) = sgn* = O(n-γ),
for arbitrarily fixed γ > 1.
Proof. When θf = 0, we know that
mm
P(QI(X) = SgnG*)) ≤ p{Xι(θj,ι ≤ 0)≤ m2}+P{Xι(θj,ι ≥o) ≤ m2}∙
j=1	2	j=1	2
By symmetry of the formulation, it is enough to prove
m
p{ X l(θj,ι > 0)≥ m2}= O(n-γ).	(20)
j=1
When θj,ι > 0, we know that Zj,ι = 1 (see (19)). Moreover, by assumption (c), we have
λNωι,ιZj,ι + λNωι,-iZj,-i ≥ λNωι,ι — Xn∣ωι,-i|i ≥ Δ0Xnωι,ι.
Therefore from (19) we have that
P{XI(θj,ι > 0)≥ m20 ≤ P{XI(n XωTXizi ≤ 2δ0λNωΙ,ι) ≤ "2- 0 .
j=1	j=1 n i∈Hj
Applying Lemma 3 with k = 0 to the i.i.d. random variables ωιTXizi we can prove (20) by taking
(21)
with C sufficiently large. Repeat the argument for the other half, we can prove the case for θj = 0.
16
Under review as a conference paper at ICLR 2021
When θl > 0, again from equation (19), We have
θj,l ≥ θι + n X ωTXizi - IIς-1IIl∞λN + oP(Ionn)∙
i∈Hj
Therefore
m
p(Ql(X) = sgnG*)) ≤ p{ XMM ≤ 0) ≥ z2}
j=1	2
m
≤Pn XI(n X 3] Xizi ≤ θι -2 ∣1ς- ∣il∞ λN) ≥ ~2}.
j=1	i∈Hj
Applying Lemma 3 We can shoW that
m
Pn XI(- X 3TXizi ≤ 优- 2 ∣∣ς-1∣∣l∞ λ" ≥ 彳} = O(n-γ),
j=1	i∈Hj
provided that
θθ - 2∣ς-1∣l∞ λN ≥ C2(∏in+⅛2 )，
With C2 sufficiently large. Combining With (21) We knoW
_ , ≈..
θl ≥ c3λN,
for some C3 > 0. When θj < 0, the prove is essentially the same as above, hence we omit it for
brevity. Thus the lemma is proved.	□
Proof of Proposition 2. The proof is similar as that of Proposition 1. For X0 ∈ D(X), denote the
elements in X0 as (X0i, Yi0) (where - ≤ i ≤ N), where
N
XI (X0i,Yi) 6=(Xi,Yi) =-.
i=1
0
Since these data are stored in m machines, we denote θj as the local estimator given by data
{(X0i, Yi0) | i ∈ Hj}, then there is
m
0 ≤ X I(0j= θj) ≤ L	(22)
j=1
Noticing that
P(Q(X) = <Q(X0), for all X0 ∈ D(X))
≤P(Q(X0) =sgn(θ*), forallX0 ∈ D(X); Q(X)=Sgn(θ*)) + P(Q(X)= sgn(θ*))
≤P(Q(X0) =sgn(θ*), forallX0 ∈ D(X)) + O(n-γ1).
Therefore we only need to prove
P(Q(X0) = sgn(θ*), for all X0 ∈ D(X)) = O(n-γ).	(23)
When θj = 0, we have
P(Qι(X0) = 0, forallX0 ∈ D(X))
m
≤p{ Xl(θj,ι ≤ 0)≤ m，forallX0 ∈ D(X)}
j=1	2
m
+ P{ Xl(θj,ι ≥ 0)≤ mm, forallX0 ∈ D(X)}
j=1	2
mm
≤P{X I&，≤ 0) ≤ m+ι}+P{X I即 ≥ 0) ≤ m+ι},
17
Under review as a conference paper at ICLR 2021
where the last line uses (22). Using the expansion (19) and rehash the proof in Lemma 5, we can
show that
mm
p{ Xl(θj,ι ≤ 0)≤ 22 + 1} + p{ Xl(θj,ι ≥ 0)≤	+ 1} = O(n-γ).
j=1	j=1
When θj > 0, similarly We have that
P(Qι(X0) = 1, for all X0 ∈ D(X))
m
≤p{ X I(⅛1 ≤ °) ≥ m，for all X0 ∈ D(X)}
j=1	2
m
≤P{ XI®, ≤ °)≥ ： - l}=O(n-γ).
j=1	2
Similar argument can be applied for the case when θf < 0, which concludes the proof of (23).
Therefore, Proposition 2 is proved.	□
18