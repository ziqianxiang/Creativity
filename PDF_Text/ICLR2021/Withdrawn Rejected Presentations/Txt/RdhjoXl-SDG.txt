Under review as a conference paper at ICLR 2021
Multiscale Invertible Generative Networks
for High-Dimensional Bayesian Inference
Anonymous authors
Paper under double-blind review
Abstract
High-dimensional Bayesian inference problems cast a long-standing challenge
in generating samples, especially when the posterior has multiple modes. For a
wide class of Bayesian inference problems equipped with the multiscale structure
that low-dimensional (coarse-scale) surrogate can approximate the original high-
dimensional (fine-scale) problem well, we propose to train a Multiscale Invertible
Generative Network (MsIGN) for sample generation. A novel prior conditioning
layer is designed to bridge networks at different resolutions, enabling coarse-to-
fine multi-stage training. Jeffreys divergence is adopted as the training objec-
tive to avoid mode dropping. On two high-dimensional Bayesian inverse prob-
lems, MsIGN approximates the posterior accurately and clearly captures multiple
modes, showing superior performance compared with previous deep generative
network approaches. On the natural image synthesis task, MsIGN achieves the
superior performance in bits-per-dimension compared with our baseline models
and yields great interpret-ability of its neurons in intermediate layers.
1 Introduction
Bayesian inference provides a powerful framework to blend prior knowledge, data generation pro-
cess and (possibly small) data for statistical inference. With some prior knowledge ρ (distribution)
for the quantity of interest x ∈ Rd, and some (noisy) measurement y ∈ Rdy, it casts on x a posterior
q(x∣y) X ρ(x)L(y∣x), where L(y∣x) = N(y - F(x); 0, Γε).	(1)
where L(y|x) is the likelihood that compares the data y with system prediction F(x) from the
candidate x, here F denotes the forward process. We can use different distributions to model the
mismatch ε = y -F(x), and for illustration simplicity, we assume Gaussian in Equation 1. For
example, Bayesian deep learning generates model predicted logits F(x) from model parameters x,
and compares it with discrete labels y through binomial or multinomial distribution.
Sampling or inferring from q is a long-standing challenge, especially for high-dimensional (high-d)
cases. An arbitrary high-d posterior can have its importance regions (also called “modes”) anywhere
in the high-d space, and finding these modes requires computational cost that grows exponentially
with the dimension d. This intrinsic difficulty is the consequence of “the curse of dimensionality”,
which all existing Bayesian inference methods suffer from, e.g., MCMC-based methods (Neal et al.,
2011; Welling & Teh, 2011; Cui et al., 2016), SVGD-type methods (Liu & Wang, 2016; Chen et al.,
2018; 2019a), and generative modeling (Morzfeld et al., 2012; Parno et al., 2016; Hou et al., 2019).
In this paper, we focus on Bayesian inference problems with multiscale structure and exploit this
structure to sample from a high-d posterior. While the original problem has a high spatial resolution
(fine-scale), its low resolution (coarse-scale) analogy is computationally attractive because it lies in
a low-dimension (low-d) space. A problem has the multiscale structure if such coarse-scale low-d
surrogate exists and gives good approximation to the fine-scale high-d problem, see Section 2.1.
Such multiscale property is very common in high-d Bayesian inference problems. For example,
inferring 3-D permeability field of subsurface at the scale of meters is a reasonable approximation
of itself at the scale of centimeters, while the problem dimension is 106-times fewer.
We propose a Multiscale Invertible Generative Network (MsIGN) to sample from high-d Bayesian
inference problems with multiscale structure. MsIGN is a flow-based generative network that can
both generate samples and give density evaluation. It consists of multiple scales that recursively
1
Under review as a conference paper at ICLR 2021
lifts up samples to a finer-scale (higher-resolution), except that the coarsest scale directly samples
from a low-d (low resolution) distribution. At each scale, a fixed prior conditioning layer combines
coarse-scale samples with some random noise according to the prior to enhance the resolution, and
then an invertible flow modifies the samples for better accuracy, see Figure 1. The architecture of
MsIGN makes it fully invertible between the final sample and random noise at all scales.
Figure 1: MsIGN generates samples from coarse to fine scale. Each scale, as separated by vertical
dash lines, takes in feature xl-1 from the coarser scale and Gaussian noise zl, and outputs a sample
xl of finer scale. The prior conditioning layer PCl lifts up the coarser-scale sample xl-1 to a finer
scale Xl, which is the best guess of Xl given its Coarse-SCale value xl-1 and the prior. An invertible
flow Fl further modifies Xl to better approximate xl. See Section 2.1 for detailed explanation.
MsIGN undergoes a multi-stage training that learns a hierarchy of distributions with dimensions
growing from the lowest to the highest (the target posterior). Each stage gives a good initialization
to the next stage thanks to the multiscale property. To capture multiple modes, we choose Jeffreys
divergence DJ (pkq) as the training objective at each stage, which is defined as
Dj(Pkq) = DKL(Pkq) + DKL(qIlp) = Ex〜P [log (p(x)/q(X))] + Ex〜q [log (q(X)/P(X))] .⑵
Jeffreys divergence removes bad local minima of single-sided Kullback-Leibler (KL) divergence to
avoid mode missing. We build an unbiased estimation of it by leveraging prior conditioning layer
in importance sampling. Proper loss function and good initialization from multi-stage training solve
the non-convex optimization stably and capture multi-modes of the high-d distribution.
In summary, we claim four contributions in this work. First, we propose a Multiscale Invertible
deep Generative Network (MsIGN) with a novel prior conditioning layer, which can be trained
in a coarse-to-fine scale manner. Second, Jeffreys divergence is used as the objective function to
avoid mode collapse, and is estimated by importance sampling based on the prior conditioning
layer. Third, when applied to two Bayesian inverse problems, MsIGN clearly captures multiple
modes in the high-d posterior and approximates the posterior accurately, demonstrating its superior
performance compared with previous methods via the generative modeling approach. Fourth, we
also apply MsIGN to image synthesis tasks, where it achieves superior performance in bits-per-
dimension among our baseline models, like Glow (Kingma & Dhariwal, 2018), FFJORD (Grathwohl
et al., 2018), Flow++ (Ho et al., 2019), i-ResNet (Behrmann et al., 2019), and Residual Flow (Chen
et al., 2019b). MsIGN also yields great interpret-ability of its neurons in intermediate layers.
2 Methology
We will abbreviate q(X|y) in Equation 1 as q(X) for simplicity in the following context, because y
only plays the role of defining the target distribution q(X) in MsIGN. In Section 2.1, we discuss the
multiscale structure in detail of the posterior q(X) and derive a scale decoupling that can be utilized
to divide and conquer the high-d challenge of Bayesian inference.
As a flow-based generative model like in Dinh et al. (2016), MsIGN models a bijective that maps
Gaussian noise z to a sample X whose distribution is denoted as Pθ (X), where θ is the network pa-
rameters. MsIGN allows fast generation of samples X and density evaluation Pθ (X), so we train our
working distribution Pθ (X) to approximate the target distribution q(X). We present the architecture
of MsIGN in Section 2.2 and the training algorithm in Section 2.3.
2
Under review as a conference paper at ICLR 2021
2.1	Multiscale Structure and Scale Decoupling
We say a Bayesian inference problem has multiscale structure if the associated coarse-scale likeli-
hood Lc approximates the original likelihood L well:
L(y∣x) ≈ Lc(y∣Xc),	where Lc(y∣Xc) := N(y - Fc(Xc); 0, Γε).	(3)
Here xc ∈ Rdc is a coarse-scale version of the fine-scale quantity x ∈ Rd (dc <d), given by a
deterministic pooling operator A : Xc = A(X). The map Fc : Rdc → Rdy is a forward process that
gives system prediction based on the coarse-scale information Xc. A popular case of the multiscale
structure is when A is the average pooling operator, and F(X) ≈ Fc(Xc), meaning that the system
prediction mainly depends on the lower-resolution information Xc . Equation 3 motivates us to define
a surrogate distribution q(x) H P(X)Lc(y∣A(x)) that approximates the target posterior q(x) well1:
q(χ) = P(X)Lc(y∣A(χ)) = P(X)Lc(y∣χc) ≈ ρ(χ)L(y∣χ) = q(χ).	(4)
We also notice that the prior P allows an exact scale decoupling. To generate a sample X from
P, one can first sample its coarse-scale version Xc = A(X), and then replenish missing fine-scale
details without changing the coarse-scale structure by sampling from the conditional distribution
P(X|Xc) = P(X|A(X) = Xc). Using Pc to denote the distribution of Xc = A(X), the conditional
probability calculation summarizes this scale decoupling process as P(x) = P(x|xc)Pc(xc).
Combining the scale effect in the likelihood and the scale decoupling in the prior, we decouple the
surrogate q(x) = P(X)Lc(y∣A(x)) into the prior conditional distribution ρ(x∣xc) andacoarse-scale
posterior, defined as qc(Xc):= Pc(Xc)L(y|Xc). The decoupling goes as
q(X) = P(X)Lc(y∣Xc) = ρ(X∣Xc)ρc(Xc)Lc(y∣Xc) = ρ(X∣Xc)qc(Xc),	(5)
The prior conditional distribution P(X|Xc) bridges the coarse-scale posterior qc (Xc) and the surrogate
q(X), which in turn approximates the original fine-scale posterior q(X). Parno et al. (2016) proposed
a similar scale decoupling relation, and we leave the discussion and comparison to Appendix A.
Figure 1 shows the integrated sampling strategy. To sample an X from q, we start with an Xc from
qc. The prior conditioning layer then performs random upsampling from the prior conditional distri-
bution P(∙∣Xc), and the output will be a sample X of the surrogate q. Due to the approximation q ≈ q
from Equation 4, we stack multiple invertible blocks for the invertible flow F to modify the sample
X 〜 q to a sample X 〜 q: X = F(X). F is initialized as an identity map in training. Finally, to
obtain the Xc from qc, we apply the above procedure recursively until the dimension of the coarsest
scale is small enough so that qc can be easily sampled by a standard method.
2.2	Multiscale Invertible Generative Network: Architecture
Our proposed MsIGN has multiple levels to recursively apply the above strategy. We denote L the
number of levels, Xl ∈ Rdl the sample at level l, and Al : Rdl → Rdl-1 the pooling operator from
level l to l - 1: Xl-1 = Al(Xl). Following the idea in Section 2.1, we can define the l-th level target
ql(Xl) and surrogate qql(Xql), and the last-level target qL is our original target q in Equation 1. The
l-th level of MsIGN uses a prior conditioning layer PCl and an inverse transform Fl to capture ql .
Prior conditioning layer. The prior conditioning layer PCl at level l lifts a coarse-scale sam-
ple Xl-1 ∈ Rdl-1 up to a random fine-scale one Xl ∈ Rdl following the conditional distribution
P(Xl |Xl-1). The difference in dimension is compensated by a Gaussian noise zl ∈ Rdl-dl-1, which
is the source of randomness: Xl =PCl(Xl-1, zl). PCl depends only on the prior conditional dis-
tribution P(Xl|Xl-1), and thus can be pre-computed independently for different levels regardless of
the likelihood L. When the prior is Gaussian and the pooling operators are linear (e.g., average
pooling), the prior conditional distribution is still Gaussian with moments specified as follows.
Lemma 2.1 Suppose that P(Xl) = N(Xl; 0, Σl), and Al(Xl) = AlXl for some Al ∈ Rdl-1×dl,
then with Uι-ι := ∑ιAT(Al∑ιAT)-1 and ∑ι∣ι-ι ：= ∑ι — ∑ιAT(Aι∑ιAT)-1Aι∑ι, we have
P(Xl ∣Xl-ι = AlXl) = N(Xl; Ul-IXl-1, ∑l∣l-ι).
1We omit normalizing constants. Equivalence and approximation are up to normalization in the following.
3
Under review as a conference paper at ICLR 2021
With the Cholesky decomposition (or eigen-decomposition) ∑1∣1-1 = BlBT, We design the prior
conditioning layer PCl as below, which is invertible between xl and (xl-1, zl):
Xl = PCl(Xl-ι,zl):= Ul-IXl-ι + BlZI, Zl 〜N(0, Idi-di-i).
(6)
We refer readers to Appendix B for proof of Lemma 2.1 and the invertibility in Equation 6.
When the prior is non-Gaussian or the pooling operators are nonlinear, there exists a nonlinear
invertible prior conditioning operator Xl =PCl(Xl-1, Zl) such that Xl folloWs the prior conditional
distribution ρ(xl∣xl-ι) given xl-ι and zl 〜N (0, Idl-d-J. We can pre-train an invertible network
to approximate this sampling process, and fix it as the prior conditioning layer.
Invertible flow. The invertible flow Fl at level l modifies the surrogate ql towards the target ql. The
more accurate the multiscale structure in Equation 3 is, the better ql approximates ql, and the closer
Fl is to the identity map. Therefore, we parameterize Fl by some flow-based generative model
and initialize it as an identity map. In practice, we utilize the invertible block of Glow (Kingma &
Dhariwal, 2018), which consists of actnorm, invertible 1 × 1 convolution, and affine coupling layer,
and stack several blocks as the inverse flow Fl in MsIGN.
Overall model. MsIGN is a bijective map between random noise inputs at different scales {Zl}L=1
and the finest-scale sample XL. The forward direction of MsIGN maps {Zl}L=1 to XL as below:
X1 = F1(Z1) ,
Xql =PCl(Xl-1,Zl) ,	Xl = Fl(Xql) ,	2 ≤ l ≤ L.
(7)
As a flow-based generative model, sample generation as in Equation 7 and density evaluation pθ (X)
by the change-of-variable rule is accessible and fast for MsIGN. In scenarios when certain bound
needs enforcing to the output, we can append element-wise output activations at the end of MsIGN.
For example, image synthesis can use the sigmoid function so that pixel values lie in [0, 1]. Such
activations should be bijective to keep the invertible relation between random noise to the sample.
2.3	Multiscale Invertible Generative Network: Training
Since the prior conditioning layer PC is pre-computed and the output activation G is fixed, only the
inverse flow F contains trainable parameters in MsIGN. We train MsIGN with the following strategy
so that the distribution pθ of its output samples, where θ is the network parameter, can approximate
the target distribution q defined in Equation 1 well.
Multi-stage training and interpret-ability. The multiscale strategy in construction of MsIGN en-
ables a coarse-to-fine multi-stage training. At stage l, we target at capturing ql, and only train
invertible flows before or at this level: Fl0 ,l0 ≤ l. Equation 4 implies that ql can be well approxi-
mated by the surogate qql, which is the conditional upsampling from ql-1 as in Equation 5. So we
use qql to initialize our model by setting Fl0,l0 <las the trained model at stage l - 1 and setting Fl
as the identity map. Our experiments demonstrate such multi-stage strategy significantly stabilizes
training and improves final performance.
Figure 1 and Equation 7 imply that intermediate activations, i.e., Xq l and Xl, who are samples of
predefined posterior distributions at the coarse scales (see Equation 5), are semantically meaningful
and interpret-able. This is different from Glow (Kingma & Dhariwal, 2018), whose intermediate
activations are not interpret-able due to the loss of spatial relation.
Jeffreys divergence and importance sampling with the surrogate. The KL divergence is easy
to compute, and thus is widely used as the training objective. However, its landscape could admit
local minima that don’t favor the optimization. Nielsen & Nock (2009) suggests that DKL(pθ kq)
is zero-forcing, meaning that it enforces pθ be small whenever q is small. As a consequence, mode
missing can still be a local minimum, see Appendix C. Therefore, we turn to the Jeffreys divergence
defined in Equation 2 which penalizes mode missing much and can remove such local minima.
Estimating the Jeffreys divergence requires computing an expectation with respect to the target q,
which is normally prohibited. Since MsIGN constructs a good approximation qq to q, and qq can
be constructed from coarser levels in multi-stage training, we do importance sampling with the
4
Under review as a conference paper at ICLR 2021
surrogate q for the Jeffreys diveregence and its derivative (See Appendix D for detailed derivation):
Dj(pθ kq) = Ex〜p0 [log *]+ Ex〜Jq(X) log 峪].	(8)
L	q(χ)」	Lq(χ)	pθ(χ)J
∂	pθ(x)	∂ log pθ(x)	q(x) ∂logpθ(x)
∂θ DJ(pθ kq) = Ex~pθ [(1 + log 而)--Ex~° [西J .⑼
With the derivative estimate given above, we optimize the Jeffreys divergence by stochastic gradient
descent. We remark that ∂ logpθ(x)/∂θ is computed by the backward propagation OfMsIGN.
3	Related Work
Invertible generative models (Deco & Brauer, 1995) are powerful exact likelihood models with
efficient sampling and inference. They have achieved great success in natural image synthesis, see,
e.g., Dinh et al. (2016); Kingma & Dhariwal (2018); Grathwohl et al. (2018); Ho et al. (2019); Chen
et al. (2019b), and variational inference in providing a tight evidence lower bound (ELBO), see,
e.g, Rezende & Mohamed (2015). In this paper, we propose a new multiscale invertible generative
network (MsIGN) structure, which utilizes the invertible block in Glow (Kingma & Dhariwal, 2018)
as building piece for the invertible flow at each scale. The Glow block can be replaced by any other
invertible blocks, without any algorithmic changes. Different from Glow, different scales of MsIGN
can be trained separately, and thus features in its intermediate layers can be interpreted as low-
resolution approximation of the final high-resolution output. This novel multiscale structure enables
better explain-ability of its hidden neurons and makes training much more stable.
Different from the image synthesis task where large amount of samples from target distribution are
available, in Bayesian inference problems only an unnormalized density is available and i.i.d. sam-
ples from the posterior are the target. This paper’s main goal is to train MsIGN to approximate
certain high-d Bayesian posteriors. Various kinds of parametric distributions have been proposed to
approximate posteriors before, such as polynomials (El Moselhy & Marzouk, 2012), non-invertible
generative networks (Feng et al., 2017; Hou et al., 2019), invertible networks (Rezende & Mo-
hamed, 2015; Ardizzone et al., 2018; Kruse et al., 2019) and certain implicit maps (Chorin & Tu,
2009; Morzfeld et al., 2012). Generative modeling approach has the advantage that i.i.d. samples
can be efficiently obtained by evaluating the model in the inference stage. However, due to the tricky
non-convex optimization problem, this approach for both invertible (Chorin & Tu, 2009; Kruse et al.,
2019) and non-invertible (Hou et al., 2019) generative models becomes increasingly challenging as
the dimension grows. To overcome this difficulty, we propose (1) to use the Jeffreys divergence
as loss function, which has fewer shallow local minima and better landscape compared with the
commonly-used KL divergence (see Appendix C for a concrete example), and (2) to train MsIGN in
a coarse-to-fine manner with coarse-scale solution serving as an initialization to fine-scale optimiza-
tion problem. In Kruse et al. (2019), authors list some recent models for low-d inverse problems.
We remark that their formulation of posterior assumes no observation or model error in Equation 1,
and is different from ours. See Appendix J for detailed discussion and experimental comparison.
Other than the generative modeling, various Markov Chain Monte Carlo (MCMC) methods have
been the most popular in Bayesian inference, see, e.g., Beskos et al. (2008); Neal et al. (2011);
Welling & Teh (2011); Chen et al. (2014; 2015); Cui et al. (2016). Particle-optimization-based sam-
pling is a recently developed effective sampling technique with Stein variational gradient descent
(SVGD) (Liu & Wang, 2016)) and many related works, e.g., Liu (2017); Liu & Zhu (2018); Chen
et al. (2018; 2019a). The intrinsic difficulty of Bayesian inference displays itself as highly corre-
lated samples, leading to undesired low sample efficiency, especially in high-d cases. The multiscale
structure and multi-stage strategy proposed in this paper can also benefit these particle-based meth-
ods, as we can observe that they benefit the amortized-SVGD (Feng et al., 2017; Hou et al., 2019)
in Section 4.1.3. We leave a more thorough study of this topic as a future work.
Works in Parno et al. (2016); Matthies et al. (2016) utilize the multiscale structure in Bayesian
inference and build generative models with polynomials. They suffer from exponential growth of
parameter number for high-d polynomial basis. The Markov property (Spantini et al., 2018) is used
to alleviate this exponential growth. Different from these works, we leverage the great capacity of
invertible generative networks to parametrize the high-d distribution, and we design novel network
architecture to make use of the multiscale structure. The multiscale structure is a more general
5
Under review as a conference paper at ICLR 2021
structure than commonly-used intrinsic low-d structure (Spantini, 2017; Cui et al., 2016; Chen et al.,
2019a), which assumes that the density of high-d posterior concentrates in a low-d subspace.
In the image synthesis task, this multiscale idea incorporates with various generative models. For
example, Denton et al. (2015); Odena et al. (2017); Karras et al. (2017); Xu et al. (2018) uses it in
generative adversarial networks (GANs) to grow a high-resolution image from low-resolution ones.
But the lack of invertibility in these models makes it difficult for them to apply to Bayesian infer-
ence problems. Invertible generative models like Dinh et al. (2016); Kingma & Dhariwal (2018);
Ardizzone et al. (2019) adopted this multiscale idea, but their multiscale strategy is not in the spatial
sense: the intermediate neurons are not semantically interpret-able, as we show in Figure 6.
4	Experiment
We study two high-d Bayesian inverse problems (BIPs) known to have at least two equally important
modes in Section 4.1 as test beds for distribution approximation and multi-mode capture: one with
true samples available in Section 4.1.1; one without true samples but close to real-world applications
in subsurface flow in Section 4.1.2. We also report the ablation study of MsIGN in Section 4.1.3.
In addition, we apply MsIGN to the image synthesis task to benchmark with flow-based generative
models and demonstrate its interpret-ability in Section 4.2. We adopt the invertible block in Glow
(Kingma & Dhariwal, 2018) as the building piece, and stack several of them to build our invertible
flow F . We utilize average pooling with kernel size 2 and stride 2 as our pooling operator A.
4.1	Bayesian Inverse Problems
Sample x of our target posterior distribution q is a vector on a 2-D uniform 64 × 64 lattice, which
means the problem dimension d is 4096. Every x is equivalent to a piece-wise constant function on
the unit disk: x(s) for S ∈ Ω = [0,1]2, and We don't distinguish between them thereafter. We place
a centered Gaussian with a Laplacian-type covariance as the prior: N 0,β2 (-∆)-1-α , which is
very popular in geophysics and electric tomography. See Appendix E for problem settings in detail.
The key to guarantee the multi-modality of our posteriors is the symmetry. Combining properties
of the prior defined above and the likelihood defined afterwards, the posterior is mirror-symmetric:
q(x(s1, s2)) = q(x(s1, 1 - s2)). We carefully select the prior and the likelihood so that our posterior
q has at least two modes. They are mirror-symmetric to each other and possess equal importance.
As in Figure 1, we plan to learn our 4096-D posteriors at the end of L = 6 levels, and set problem
dimension at each level as di = 2l * 2l = 4l. The training follows our multi-stage strategy, and the
first stage l =1is initialized by minimizing the Jeffreys divergence without importance sampling,
because samples to q1 is available since d1 =4is relatively small. See Appendix E for details.
We compare MsIGN with representatives of major approaches: amortized-SVGD (short as A-
SVGD) (Feng et al., 2017) and Hamilton Monte Carlo (short as HMC) (Neal et al., 2011), for
high-d BIPs, see our discussion in Section 3. We measure the computational cost by the number of
forward simulations (nFSs), because running the forward simulation F occupies most training time,
especially in Section 4.1.2. We budget a same nFS for all methods for fair comparison.
4.1.1	Synthetic Bayesian Inverse Problems
This problem allows access to ground-truth samples so the comparison is clear and solid. The for-
ward process is given by F(x) = hφ, x)2 = (Ω 夕(s)x(s)ds)2, where 夕(S) = sin(πsι) sin(2∏s2).
Together with the prior, our posterior can be factorized into one-dimensional sub-distributions,
namely q(x) = Qd=1 qk(hwk, xi) for some orthonormal basis {wk}d=1. This property gives us
access to true samples via inversion cumulative function sampling along each direction wk . Fur-
thermore, these 1-D sub-distributions are all single modal except that there,s one, denoted as qk*,
with two symmetric modes. In other words, the marginal distribution along wk* is double-model
and the rest are uni-model. This confirms our construction of two equally important modes. See
Appendix E for more details in problem settings. The computation budget is fixed at 8 × 105 nFSs.
Multi-mode capture. To visualize mode capture, we plot the marginal distribution of generated
samples along the critical direction wk*, which by construction is the source of double-modality of
the posterior. The (visually) worst one in three independent experiments is shown in Figure 2(a).
6
Under review as a conference paper at ICLR 2021
Figure 2: Results of the synthetic BIP. (a): Distribution of 2500 samples along the critical direction
wk*. MsIGN is more robust in capturing both modes, its samples are more balanced. (b): Error
mean and its 95% confidence interval. MsIGN is more accurate in distribution approximation, es-
pecially at finer scale when the problem dimension is high. The margin is statistical significant as
shown by the confidence interval. For more experimental results, please refer to Appendix F.
Distribution approximation. To measure distribution
approximation, we report the error of mean, variance
and correlation at or between all sub-distributions,
Method
Error
MsIGN A-SVGD
(Feng et al., 2017)
56.77±0.15	3372±21
as well as the Jeffreys divergence. Thanks to the Table 1: Distribution approximation error
factorization property, we compare the mean, vari- by Jeffreys divergence with the target pos-
ance and correlation estimate with theoretical ground- terior in three independent runs
truths, and report the root mean square of error at all
dimensions in Figure 2(b). For MsIGN and A-SVGD that gives access to not only samples but also
density, we also report the Monte Carlo estimates of the Jeffreys divergence with the target posterior
in Table 1. We can see that MsIGN has superior accuracy in approximating the target distribution.
4.1.2	Elliptic Bayesian Inverse Problems
This problem originates from geophysics and fluid dynamics. The forward model is given by linear
measurement of the solution to an elliptic partial differential equation associated with x. We define
F (X)= [Rω 0(S)U(S)ds Rω 32(S)U(S)ds ... Rω 3m(S)U(S)ds]T ,
where 夕k are fixed measurement functions, and u(s) is the solution of
-▽ ∙ (ex(s)Vu(S)) = f (s) , S ∈ Ω, with boundary condition U(S) = 0, s ∈ ∂Ω .	(10)
This model appears frequently in real applications. For example, x, u can be seen as permeability
field and pressure in geophysics. However, there is no known access to true samples of q. Again the
trick of symmetry introduced in Section 4.1 and explained in Appendix E guarantees at least two
equally important modes in the posterior. We put a 5 × 105-nFS budget on our computation cost.
PCA and Clustering Cluster 1 Mean Cluster 2 Mean
UoHeUl-s"jXJ-Uea -ffl∈s-
Nesw usɪ α9>sa
-0.8	-0.6 -0.4	-0.2	0.0	0.2	0.4	0.6	0.8
q6	(b)
Figure 3: Results of the elliptic BIP. (a): Distribution of 2500 samples along a critical direction.
MsIGN and HMC capture two modes in this marginal distribution, but A-SVGD fails. (b): Clus-
tering result of 2500 samples. Samples of MsIGN are more balanced between two modes. The
similarity of the cluster means of MsIGN and HMC implies that they both are likely to capture the
correct modes. For more experimental results, please refer to Appendix I.
Multi-mode capture. Due to lack of true samples, we check the marginal distribution of the pos-
terior along eigen-vectors of the prior, and pick a particular one to demonstrate that we can capture
double modes in Figure 3(a). We also confirm the capture of multiple modes by embedding samples
7
Under review as a conference paper at ICLR 2021
by Principle Component Analysis (PCA) to a 2-D space. We report the clustering (by K-means)
result and means of each cluster in Figure 3(b), where we can see that A-SVGD failed to capture the
two symmetric modes, while MsIGN has a more balanced capture of the symmetric posterior.
4.1.3	Ablation Study of Architecture Design and Training Strategy
We run extensive experiments to study the effectiveness of the network architecture and training
stragtegy of MsIGN, see Figure 4. We refer to Appendix G for details in setting and more results.
Network architecture. We replace the prior conditioning layer by two direct alternatives: a stochas-
tic nearest-neighbor upsampling layer (model denoted as “MsIGN-SNN”), or the split and squeeze
layer in Glow design (now the model is essentially Glow, so we also denote it as “Glow”).
Figure 4(a) shows that the prior conditioning layer design is crucial to the performance of MsIGN
on both problems, because neither “MsIGN-SNN” nor “Glow” has a successful mode capture.
Training strategy. We study the effectiveness of the Jeffreys divergence objective and multi-stage
training. We try substituting the Jeffreys divergence objective (no extra marks) with the KL diver-
gence (model denoted with a string “-KL”) or kernelized Stein discrepancy (which resumes A-SVGD
algorithm, model denoted with a string “-AS”), and switching between multi-stage (no extra marks)
or single-stage training (model denoted with a string “-S”). We remark that single-stage training
using Jeffreys divergence is infeasible because of the difficulty to estimate DKL(qkpθ).
Figure 4(b) and (c) show that, all models trained in the single-stage manner (“MsIGN-KL-S”,
“MsIGN-AS-S”) will face mode collapse. We also observe that our multi-stage training strategy
can benefit training with other objectives, see “MsIGN-KL” and “MsIGN-AS”.
We also notice that the Jeffreys divergence leads to a more balanced samples for these symmetric
problems, especially for the complicated elliptic BIP in Section 4.1.2.
Network Architecture	Objective: KL Divergence	Objective: Kernelized Stein Discrepancy
Figure 4: Ablation study of the network architecture and training strategy. “MsIGN" means our
default setting: training MsIGN network with Jeffreys divergence and multi-stage strategy. Other
models are named by a base model (MsIGN or Glow), followed by strings indicating its variance
from the default setting. For example, “MsIGN-KL” refers to training MsIGN network with single
KL divergence in a multi-stage way, while “MsIGN-KL-S” means traininng in a single-stage way.
4.2	Image Synthesis Task
We train our MsIGN architecture with maximum likelihood estimation to benchmark with other
flow-based generative models. The prior conditional distribution ρ(x∣xj is modeled by a simple
Gaussian with a scalar matrix as its covariance and is learned from a training set. We refer readers
to Appendix H for more experimental details, and to Appendix I for additional results.
We report the bits-per-dimension value with our baseline models of flow-based generative networks
in Table 2. Our MsIGN is superior in number and also is more efficient in parameter size: for
example, MsIGN uses 24.4% fewer parameters than Glow for CelebA 64, and uses 37.4% fewer
parameters than Residual Flow for ImageNet 64.
8
Under review as a conference paper at ICLR 2021
In Figure 5, we show synthesized images of MsIGN from CelebA 64 dataset, and linear interpolation
of real images in the latent feature space. In Figure 6, we visualize internal activations at checkpoints
of the invertible flow at different scales which demonstrates the interpret-ability of MsIGN.
Table 2: Bits-per-dimension value comparison with baseline models of flow-based generative net-
works. All models in this table do not use “variational dequantization” in Ho et al. (2019). *: Score
obtained by our own reproducing experiment.
Model		MNIST	CIFAR-10	CelebA 64	ImageNet 32	ImageNet 64
Real NVP(Dinh et al., 2016)		1.06	-3.49-	3.02	4.28	3.98
Glow(Kingma & Dhariwal, 2018)		1.05	3.35	2.20*	4.09	3.81
FFJORD(	Grathwohl et al., 2018)	0.99	3.40	—	一	—
Flow++(Ho et al., 2019)		—	3.29	—	一	—
i-ResNet(	Behrmann et al., 2019)	1.05	3.45	—	一	—
Residual Flow(Chen et al., 2019b)		0.97	3.28	—	4.01	3.76
MsIGN (Ours)		0.93	~3.28~	2.15	4.03	3.73
Figure 5: Left: Synthesized CelebA 64 images with temperature 0.9. Right: Linear interpolation in
latent space shows MsIGN’s parameterization of natural image manifold is semantically meaningful.
Figure 6: Visualization of internal activation shows the interpret-ability of MsIGN hidden neurons.
From left to right, we show how MsIGN progressively generates new samples in high resolution by
taking snapshots at internal checkpoints. See Appendix I for details.
5	Conclusion
For high-dimensional Bayesian inference problems with multiscale structure, we propose Multi-
scale Invertible Generative Networks (MsIGN) and associated training algorithms to approximate
the high-dimensional posterior. In this paper, we demonstrate the capability of this approach in high-
dimensional (up to 4096 dimensions) Bayesian inference problems with spatial multiscale structure,
leaving several important directions as future work. The network architecture also achieves the
state-of-the-art performance in various image synthesis tasks. We plan to apply this methodology to
other Bayesian inference problems, for example, Bayesian deep learning with multiscale structure
in model width or depth (e.g., Chang et al. (2017); Haber et al. (2018)) and data assimilation prob-
lem with multiscale structure in the temporal variation (e.g., Giles (2008)). We also plan to develop
some theoretical guarantee of the posterior approximation performance for MsIGN.
9
Under review as a conference paper at ICLR 2021
References
Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S
Klessen, Lena Maier-Hein, Carsten Rother, and Ullrich KOthe. Analyzing inverse problems with
invertible neural networks. arXiv preprint arXiv:1808.04730, 2018.
Lynton Ardizzone, Carsten Luth, Jakob Kruse, Carsten Rother, and Ullrich KOthe. Guided image
generation with conditional invertible neural networks. arXiv preprint arXiv:1907.02392, 2019.
Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jorn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573-582,
2019.
Alexandros Beskos, Gareth Roberts, Andrew Stuart, and Jochen Voss. Mcmc methods for diffusion
bridges. Stochastics and Dynamics, 8(03):319-350, 2008.
Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert. Multi-level residual net-
works from dynamical systems view. arXiv preprint arXiv:1710.10348, 2017.
Changyou Chen, Nan Ding, and Lawrence Carin. On the convergence of stochastic gradient mcmc
algorithms with high-order integrators. In Advances in Neural Information Processing Systems,
pp. 2278-2286, 2015.
Changyou Chen, Ruiyi Zhang, Wenlin Wang, Bai Li, and Liqun Chen. A unified particle-
optimization framework for scalable bayesian sampling. arXiv preprint arXiv:1805.11659, 2018.
Peng Chen, Keyi Wu, Joshua Chen, Tom O’Leary-Roseberry, and Omar Ghattas. Projected stein
variational newton: A fast and scalable bayesian inference method in high dimensions. In Ad-
vances in Neural Information Processing Systems, pp. 15104-15113, 2019a.
Tian Qi Chen, Jens Behrmann, David K Duvenaud, and Jorn-Henrik Jacobsen. Residual flows
for invertible generative modeling. In Advances in Neural Information Processing Systems, pp.
9913-9923, 2019b.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International conference on machine learning, pp. 1683-1691, 2014.
Alexandre J Chorin and Xuemin Tu. Implicit sampling for particle filters. Proceedings of the
National Academy of Sciences, 106(41):17249-17254, 2009.
Tiangang Cui, Kody JH Law, and Youssef M Marzouk. Dimension-independent likelihood-informed
mcmc. Journal of Computational Physics, 304:109-137, 2016.
Gustavo Deco and Wilfried Brauer. Nonlinear higher-order statistical decorrelation by volume-
conserving neural architectures. Neural Networks, 8(4):525-535, 1995.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a
laplacian pyramid of adversarial networks. In Advances in neural information processing systems,
pp. 1486-1494, 2015.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Tarek A El Moselhy and Youssef M Marzouk. Bayesian inference with optimal maps. Journal of
Computational Physics, 231(23):7815-7850, 2012.
Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized stein variational
gradient descent. arXiv preprint arXiv:1707.06626, 2017.
Michael B Giles. Multilevel monte carlo path simulation. Operations research, 56(3):607-617,
2008.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. arXiv preprint
arXiv:1810.01367, 2018.
10
Under review as a conference paper at ICLR 2021
Eldad Haber, Lars Ruthotto, Elliot Holtham, and Seong-Hwan Jun. Learning across scales—
multiscale methods for convolution neural networks. In Thirty-Second AAAI Conference on Arti-
ficial Intelligence, 2018.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. arXiv preprint
arXiv:1902.00275, 2019.
Thomas Y Hou, Ka Chun Lam, Pengchuan Zhang, and Shumao Zhang. Solving bayesian inverse
problems from the perspective of deep generative networks. Computational Mechanics, 64(2):
395-408, 2019.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
Jakob Kruse, Lynton Ardizzone, Carsten Rother, and Ullrich Kothe. Benchmarking invertible archi-
tectures on inverse problems. ICML, 2019.
Chang Liu and Jun Zhu. Riemannian stein variational gradient descent for bayesian inference. In
Thirty-second aaai conference on artificial intelligence, 2018.
Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in neural information
processing systems, pp. 3115-3123, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In Advances In Neural Information Processing Systems, pp. 2378-2386, 2016.
Hermann G Matthies, Elmar Zander, Bojana V RosiC, Alexander Litvinenko, and Oliver Pajonk.
Inverse problems in a bayesian setting. In Computational Methods for Solids and Fluids, pp.
245-286. Springer, 2016.
Matthias Morzfeld, Xuemin Tu, Ethan Atkins, and Alexandre J Chorin. A random map implemen-
tation of implicit filters. Journal of Computational Physics, 231(4):2049-2066, 2012.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, 2(11), 2011.
Frank Nielsen and Richard Nock. Sided and symmetrized bregman centroids. IEEE transactions on
Information Theory, 55(6):2882-2904, 2009.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. In International conference on machine learning, pp. 2642-2651, 2017.
Matthew Parno, Tarek Moselhy, and Youssef Marzouk. A multiscale strategy for bayesian inference
using transport maps. SIAM/ASA Journal on Uncertainty Quantification, 4(1):1160-1190, 2016.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows, 2015.
Alessio Spantini. On the low-dimensional structure of Bayesian inference. PhD thesis, Mas-
sachusetts Institute of Technology, 2017.
Alessio Spantini, Daniele Bigoni, and Youssef Marzouk. Inference via low-dimensional couplings.
The Journal of Machine Learning Research, 19(1):2639-2709, 2018.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong
He. Attngan: Fine-grained text to image generation with attentional generative adversarial net-
works. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
1316-1324, 2018.
11