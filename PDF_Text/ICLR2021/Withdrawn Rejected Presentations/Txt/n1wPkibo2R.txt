Under review as a conference paper at ICLR 2021
An Efficient Protocol for Distributed Column
SUBSET SELECTION IN THE ENTRYWISE `p NORM
Anonymous authors
Paper under double-blind review
Ab stract
We give a distributed protocol with nearly-optimal communication and number
of rounds for Column Subset Selection with respect to the entrywise `1 norm
(k-CSS1), and more generally, for the `p-norm with 1 ≤ p < 2. We study matrix
factorization in `1 -norm loss, rather than the more standard Frobenius norm loss,
because the `1 norm is more robust to noise, which is observed to lead to improved
performance in a wide range of computer vision and robotics problems. In the
distributed setting, we consider s servers in the standard coordinator model of
communication, where the columns of the input matrix A ∈ Rd×n (n d) are
distributed across the s servers. We give a protocol in this model with O(sdk)
communication, 1 round, and polynomial running time, and which achieves a
multiplicative k P -1 poly (log nd) -approximation to the best possible column subset.
A key ingredient in our proof is the reduction to the 'p,2-norm, which corresponds
to the p-norm of the vector of Euclidean norms of each of the columns of A.
This enables us to use strong coreset constructions for Euclidean norms, which
previously had not been used in this context. This naturally also allows us to
implement our algorithm in the popular streaming model of computation. We
further propose a greedy algorithm for selecting columns, which can be used by
the coordinator, and show the first provable guarantees for a greedy algorithm for
the `1,2 norm. Finally, we implement our protocol and give significant practical
advantages on real-world data analysis tasks.
1	Introduction
Column Subset Selection (k-CSS) is a widely studied approach for rank-k approximation and feature
selection. In k-CSS, one seeks a small subset U ∈ Rd×k of k columns of a data matrix A ∈ Rd×n ,
typically n d, for which there is a right factor V such that |UV - A| is small under some norm
| ∙ |. k-CSS is a special case of low rank approximation for which the left factor is an actual subset of
columns. The main advantage of k-CSS over general low rank approximation is that the resulting
factorization is more interpretable, as columns correspond to actual features while general low rank
approximation takes linear combinations of such features. In addition, k-CSS preserves the sparsity
of the data matrix A.
k-CSS has been extensively studied in the Frobenius norm (Guruswami & Sinop, 2012; Boutsidis
et al., 2014; Boutsidis & Woodruff, 2017; Boutsidis et al., 2008) and operator norms (Halko et al.,
2011; Woodruff, 2014). A number of recent works (Song et al., 2017; Chierichetti et al., 2017; Dan
et al., 2019; Ban et al., 2019; Mahankali & Woodruff, 2020) studied this problem in the `p norm
(k-CSSp) for 1 ≤ p < 2. The `1 norm is less sensitive to outliers, and better at handling missing
data and non-Gaussian noise, than the Frobenius norm (Song et al., 2017). Specifically, the `1 norm
leads to improved performance in many real-world applications, such as structure-from-motion (Ke
& Kanade, 2005) and image denoising (Yu et al., 2012).
Distributed low-rank approximation arises naturally when a dataset is too large to store on one
machine, takes prohibitively long time for a single machine to compute a rank-k approximation,
or is collected simultaneously on multiple machines. Despite the flurry of recent work on k-CSSp ,
this problem remains largely unexplored in the distributed setting. This should be contrasted to
Frobenius norm column subset selection and low rank approximation, for which a number of results
in the distributed model are known, see, e.g., Altschuler et al. (2016); Balcan et al. (2015; 2016);
Boutsidis et al. (2016). We consider a widely applicable model in the distributed setting, where s
1
Under review as a conference paper at ICLR 2021
Figure 1: An overview of the proposed protocol for distributed k-CSSp in the column partition model.
Step 1: Server i applies a dense p-stable sketching matrix S to reduce the row dimension of the
data matrix Ai . S is shared between all servers. Step 2: Server i constructs a strong coreset for its
sketched data matrix SAi , which is a subsampled and reweighted set of columns of SAi . Server i
then sends the coreset SAiTi , as well as the corresponding unsketched, unweighted columns AiDi
selected in the strong coreset SAiTi to the coordinator. Step 3: The coordinator concatenates the
SAiTi column-wise, applies k-CSSp,2 to the concatenated columns and computes the set of indices
of the selected columns. Step 4: The coordinator recovers the set of selected columns AI from the
unsketched, unweighted columns AiDi’s through previously computed indices.
servers communicate to a central coordinator via 2-way channels. This model can simulate arbitrary
point-to-point communication by having the coordinator forward a message from one server to
another; this increases the total communication by a factor of 2 and an additive log s bits per message
to identify the destination server.
We consider the column partition model, in which each column of A ∈ Rd×n
is held by exactly
one server. The column partition model is widely-studied and arises naturally in many real world
scenarios such as federated learning (Farahat et al., 2013; Altschuler et al., 2016; Liang et al., 2014).
In the column partition model, we typically have n	d, i.e., A has many more columns than
rows. Hence, we desire a protocol for distributed k-CSSp that has a communication cost that is only
logarithmic in the large dimension n, as well as fast running time. In addition, it is important that
our protocol only uses a small constant number of communication rounds (meaning back-and-forth
exchanges between servers and the coordinator). Indeed, otherwise, the servers and coordinator
would need to interact more, making the protocol sensitive to failures in the machines, e.g., if they go
offline. Further, a 1-round protocol can naturally be adapted to an single pass streaming algorithm
when we consider applications with limited memory and access to the data. In fact, our protocol can
be easily extended to yield such a streaming algorithm 1.
In the following, We denote A^ and A*j as the i-th row and j-th column of A respectively, for
i ∈ [d], j ∈ [n]. We denote AT as the subset of columns of A with indices in T ⊆ [n]. The
entrywise 'p-norm of A is |A|p = (Pd=I P；=i |Aj|p) 1. The 'p,2 norm is defined as |A|p,2 =
(Pd=Iaj IP)P. We consider 1 ≤p < 2. We denote the best rank-k approximation error for A in
`p norm by OPT := minrank-k Ak |A - Ak|p. Given an integer k > 0, we say U ∈ Rd×k, V ∈ Rk×n
are the left and right factors of a rank-k factorization for A in the `p norm with approximation factor
α if ∣UV — A|p ≤ α ∙ OPT.
Since general rank-k approximation in `1 norm is NP hard (Gillis & Vavasis, 2015), we follow
previous work and consider bi-criteria k-CSS algorithms which obtain polynomial running time.
Instead of outputting exactly k columns, such algorithms return a subset of O(k) columns of A,
suppressing logarithmic factors in k or n. It is known that the best approximation factor to OPT that
can be obtained through the span of a column subset of size O(k) IS Ω(k1/2 Y) for P = 1 (Song et al.,
2017) and Ω(k"PT/2-Y) for P ∈ (1, 2) (Mahankali & Woodruff, 2020), where Y is an arbitrarily
small constant.
1We provide a detailed analysis of our streaming algorithm in Appendix E.
2
Under review as a conference paper at ICLR 2021
1.1	PREVIOUS APPROACHES TO k-CSSp IN THE DISTRIBUTED SETTING
If one only wants to obtain a good left factor U, and not necessarily a column subset of A, in
the column partition model, one could simply sketch the columns of Ai by applying an oblivious
sketching matrix S on each server. Each server sends Ai ∙ S to the coordinator. The coordinator
obtains U = AS as a column-wise concatenation of AiS’s. Song et al. (2017) showed that AS
achieves an O( k) approximation to OPT, and this protocol only requires O(sdk) communication,
O(1) rounds and polynomial running time. However, while AS is a good left factor, it does not
correspond to an actual subset of columns of A.
Obtaining a subset of columns that approximates A well with respect to the p-norm in a distributed
setting is non-trivial. One approach due to Song et al. (2017) is to take the matrix AS described
above, sample rows according to the Lewis weights (Cohen & Peng, 2015) of AS to get a right factor
V , which is in the row span of A, and then use the Lewis weights of V to in turn sample columns
of A. Unfortunately, this protocol only achieves a loose Oe(k3/2) approximation to OPT (Song
et al., 2017). Moreover, it is not known how to do Lewis weight sampling in a distributed setting.
Alternatively, one could adapt existing single-machine k-CSSp algorithms to the distributed setting
under the column partition model. Existing works on polynomial time k-CSSp (Chierichetti et al.,
2017; Song et al., 2019b; Dan et al., 2019; Mahankali & Woodruff, 2020) give bi-criteria algorithms,
and are based on a recursive framework with multiple rounds, which is as follows: in each round,
O(k) columns are selected uniformly at random, and with high probability, the selected columns
can provide a good approximation to a constant fraction of all columns of A. Among the remaining
columns that are not well approximated, O(k) columns are recursively selected until all columns of
A are well approximated, resulting in a total of O(logn) rounds.
A naive extension of this bi-criteria k-CSSp framework to a distributed protocol requires O(log n)
rounds, as in each round, the servers and the coordinator need to communicate with each other in
order to find the columns that are covered well and select from the remaining unselected columns. To
reduce this to a single round, one might consider running the O(log n) round selection procedure on
the coordinator only. In order to do this, the coordinator needs to first collect all columns of A from
the servers, but directly communicating all columns is prohibitive.
Alternatively, one could first apply k-CSSp on Ai to obtain factors Ui and Vi on each server, and
then send the coordinator all of the Ui and Vi . The coordinator then column-wise stacks the Ui Vi to
obtain U ∙ V and selects O(k) columns from U ∙ V. Even though this protocol applies to all P ≥ 1, it
achieves a loose O(k2) approximation to OPT and requires a prohibitive O(n + d) communication
cost2. One could instead try to just communicate the matrices Ui to the coordinator, which results in
much less communication, but this no longer gives a good approximation. Indeed, while each Ui
serves as a good approximation locally, there may be columns that are locally not important, but
become globally important when all of the matrices Ai are put together. What is really needed here is
a small coreset Ci for each Ai so that if one concatenates all of the Ci to obtain C, any good column
subset of the coreset C corresponds to a good column subset for A. Unfortunately, coresets for the
entrywise `p -norm are not known to exist.
1.2	Our Contributions
Our Distributed Protocol We overcome these problems and propose the first efficient protocol for
distributed k-CSSp (1 ≤ p < 2) in the column partition model that selects O(k) columns of A achiev-
ing an Oe(k1/p-1/2)-approximation to the best possible subset of columns and requires only Oe(sdk)
communication cost, 1 round and polynomial time. Figure 1 gives an overview of the protocol. We
note that our subset of columns does not necessarily achieve an Oe(k1/p-1/2)-approximation to OPT
itself, although it does achieve such an approximation to the best possible subset of columns. Using
the fact that there always exists a subset of columns providing an Oe(k1/p-1/2)-approximation to OPT
(Song et al., 2017), we conclude that our subset of columns achieves an Oe(k2/p-1)-approximation to
OPT. Recently, and independently of our work, Mahankali & Woodruff (2020) show how to obtain a
subset of columns achieving an Oe(k1/p-1/2)-approximation to OPT itself; however, such a subset
is found by uniformly sampling columns in O(logn) adaptive rounds using the recursive sampling
2We give this protocol and the analysis in Appendix A
3
Under review as a conference paper at ICLR 2021
framework above, and is inherently hard to implement in a distributed setting with fewer rounds. In
contrast, our protocol achieves 1 round of communication, which is optimal.
We make use of a strong coreset, i.e., a sampled and reweighted subset of columns of each Ai that
approximates the cost of all potential left factors of Ai , by first embedding all subspaces spanned
by any subset of O(k) columns of A from 'p-space to Euclidean space, to bypass the lack of strong
coresets for the `p norm. We denote this new norm as `p,2 norm, which is the sum of the p-th powers
of the '2 norms of the columns. To reduce the error incurred by switching to the 'p,2-norm, We reduce
the row dimension of A by left-multiplying by an oblivious sketching matrix S shared across servers,
resulting in an overall approximation factor of only Oe(k1/p-1/2). Afterwards, each server sends
its strong coreset to the coordinator. The coordinator, upon receiving the coresets from each server,
runs an O(1)-approximate bi-criteria k-CSSp,2 algorithm to select the final column subset, giving an
overall Oe(k1/p-1/2) approximation to the best column subset.
We introduce several new technical ideas in the analysis of our protocol. Our work is the first to
apply a combination of oblivious sketching in the p-norm via p stable random variables and strong
coresets in the `p,2 norm (Sohler & Woodruff, 2018; Huang & Vishnoi, 2020) to distributed k-CSS.
Furthermore, to show that our oblivious sketching step only increases the final approximation error
by a logarithmic factor, we combine a net argument with a union bound over all possible subspaces
spanned by column subsets of A of size O(k). Previous arguments involving sketching, such as
those by Song et al. (2017); Ban et al. (2019); Mahankali & Woodruff (2020), only consider a single
subspace at a time.
Theoretical Guarantees and Empirical Benefits for Greedy k-CSS1,2 We also propose a greedy
algorithm to select columns in the k-CSS1,2 step of our protocol, and show the first additive error
guarantee compared to the best possible subset AS of columns, i.e., our cost is at most (1 -
) minV |ASV - A|1,2 + |A|1,2. Similar error guarantees were known for the Frobenius norm
(Altschuler et al., 2016), though nothing was known for the `1,2 norm. We also implement our
protocol and experiment with distributed k-CSS1 on various real-world datasets. We compare the
O(1)-approximate bi-criteria k-CSS1,2 and the greedy k-CSS1,2 as different possible subroutines in
our protocol, and show that greedy k-CSS1,2 yields an improvement in practice.
2	Problem Setting
2.1	The Column Partition Model
We consider a model where there are s servers, the ith of which holds Ai ∈ Rd×ni, and a coordinator
which initially does not hold any data. Each server talks only to the coordinator, via a 2-way
communication channel. The communication cost is the total number of words transferred between
the servers and the coordinator over the course of the protocol. Each word is O(log(snd)) bits. The
overall data matrix A ∈ Rd×n is A = [A1, A2, . . . , As] (the column-wise concatenation of the Ai’s).
Here, n is defined to be Pis=1 ni . Typically, in the column partition model, n d.
2.2	p-STABLE DISTRIBUTION AND p-STABLE RANDOM VARIABLES
Let Z, X1 , . . . , Xd be random variables drawn i.i.d. from some distribution D. D is called p-stable
if for an arbitrary vector v ∈ Rd, hv, Xi = |v|pZ, where X = [X1, . . . , Xd]T. Random variables
drawn from such a distribution are called p-stable random variables, which exist for p ∈ (0, 2].
Though there is no closed form expression for the p-stable distribution in general except for a few
values of p, we can efficiently generate a single p-stable random variable in O(1) time using the
following method due to Chambers et al. (1976): if θ ∈ [一 π2, π2] and r ∈ [0,1] are sampled uniformly
at random, then, Con(Pθθ (cos,；；-P))) 丫 follows a p-stable distribution.
3	Preliminaries for Our Protocol
We first note a standard relationship between the 'p norm and the 'p2 norm.
Lemma 1. For a matrix A ∈ Rd×n andP ∈ [1, 2), ∣A∣p,2 ≤ |A|p ≤ dp -2 ∣A∣p,2.
4
Under review as a conference paper at ICLR 2021
3.1	'p-norm Oblivious Sketching
We left-multiply A by an oblivious sketching matrix S with p-stable random variables so that we lose
only an O(kP-2) approximation factor when We switch to the 'p,2 norm.
The purpose of the next two lemmas is to show that we can perform oblivious sketching while
preserving the costs of all possible column subsets up to logarithmic factors. We first show a lower
bound on the approximation error for a sketched subset of columns, |SATV - SA|p, which holds
simultaneously for any arbitrary subset AT of chosen columns, and for any arbitrary right factor V .
Lemma 2 (Sketched Error Lower Bound). Let A ∈ Rd×n and k ∈ N. Let t = k ∙ poly(log(nd)),
and let S ∈ Rt×d be a matrix whose entries are i.i.d. standard p-stable random variables, rescaled
by Θ(1∕t 1). Then, with probability 1 一 o(1) ,for all T ⊂ [n] with |T | = k ∙ poly (log k) and forall
V ∈ R|Tl×n, |AtV 一 A|p ≤ |SAtV 一 SA|p.
Next, we show an upper bound on the approximation error of k-CSSp on a sketched subset of columns,
|SAT V 一 SA|p , which holds for a fixed subset of columns AT and a fixed right factor V .
Lemma 3 (Sketched Error Upper Bound). Let A ∈ Rd×n and k ∈ N. Let t = k ∙ poly(log(nd)),
and let S ∈ Rt×d be a matrix whose entries are i.i.d. standard p-stable random variables, rescaled
by Θ(1∕tP). Then, for a fixed subset T ⊂ [n] ofcolumns with |T | = k ∙ poly (log k) and a fixed V ∈
RlTl×n, with probability 1 — o(1), we have minv |SAt V - SA|p ≤ minv O(log1∕p(nd))∣Aτ V —
A|p.
3.2	STRONG CORESETS iN THE `p,2 NORM
To enable sub-linear communication cost in the number n of columns, the i-th server sends the
coordinator a strong coreset of columns of SAi , which is a reweighted subset of the columns of
SAi . Such strong coresets preserve the error incurred by any rank-k projection, up to a constant
factor, in the `p,2 norm. The coreset of a matrix A ∈ Rd×n is usually denoted as AT, where
T = D ∙ W ∈ Rn×t is a sampling and reweighting matrix and t is the number of columns to be
included in the coreset. The sampling matrix D is a matrix with t columns where each column has
only one 1 in the index of the column of A to be included in the coreset and 0 everywhere else. The
reweighting matrix W is a diagonal t × t matrix with weights associated with each sample in the
coreset.
Lemma 4 (Strong Coreset in `p,2 norm). Let A ∈ Rd×n, k ∈ N, p ∈ [1, 2), and , δ ∈ (0, 1). Then,
in n ∙ poly (k log n∕e) time, one can find a sampling and reweighting matrix T with O(d log d∕e2) ∙
log(1∕δ) columns such that, with probability 1 一 δ, for all rank-k matrices U,
min
rank-k V
|UV 一 AT|p,2
(1 ± ) min
rank-k V
|UV 一 A|p,2
AT is called a strong coreset of A.
3.3	POLYNOMiAL TiME, O(1) -APPROXiMATE Bi-CRiTERiA k-CS Sp,2
After server i sends a strong coreset to the coordinator, the coordinator does k-CSS on a column-wise
concatenation of these coresets, in the `p,2 norm rather than the `p norm. We give a polynomial time,
O(1)-approximate bi-criteria k-CSSp,2 algorithm forp ∈ [1, 2).
Theorem 5 (Bicriteria O(1)-Approximation Algorithm for k-CSSp,2). Let A ∈ Rd×n and k ∈ N.
There exists an algorithm that runs in (nnz (A) + d2) ∙ kpoly (log k) time and outputs a rescaled subset
of columns U ∈ Rd×O(k) of A and a right factor V ∈ RO(k)×n for which V = minV |UV 一 A|p,2,
such that with probability 1 一 o(1),
IUV - A∣p,2 ≤ O(1) ∙ min |Ak - A∣p,2
rank-k Ak
Our polynomial time bi-criteria k-CSSp,2 algorithm is based on that of Clarkson & Woodruff (2015).
The main difference is that the algorithm of Clarkson & Woodruff (2015) outputs a subset with O(k2 )
columns due to the usage of `p leverage scores — we reduce the number of selected columns to O(k)
by using `p Lewis weights (Cohen & Peng, 2015). Details are given in Appendix C.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 An efficient protocol for bi-criteria k-CSSp in the column partition model
Initial State: Server i holds matrix Ai ∈ Rd×ni, ∀i ∈ [s].
Coordinator:
Generate a dense p-stable sketching matrix S ∈ Rk poly(log(nd))×d.
Send S to all servers.
Server i:
Compute SAi .
Let the number of samples in the coreset be t = O(kpoly(log(nd))). Construct a coreset of SAi
under the `p,2 norm by applying a sampling matrix Di of size ni × t and a diagonal reweighting
matrix Wi of size t × t.
Let Ti = Di Wi . Send SAiTi along with AiDi to the coordinator.
Coordinator:
Column-wise stack SAiTi to obtain SAT = [SA1T1, SA2T2, . . . , SAsTs].
Apply k-CSSp,2 on SAT to obtain the indices I of the subset of selected columns with size
O(k ∙ poly(log k)).
Since Di ’s are sampling matrices, the coordinator can recover the original columns of A by
mapping indices I to AiDi ’s.
Denote the final selected subset of columns by AI . Send AI to all servers.
Server i:
Solve minVi |AI Vi - Ai|p to obtain the right factor Vi. AI and V will be factors of a rank-
k ∙ poly(log k) factorization of A, where V is the (implicit) column-wise concatenation of the
Vi.
4	AN EFFICIENT PROTOCOL FOR DISTRIBUTED k-CSSp
Theorem 6 (A Protocol for Distributed k-CSSp). In the column partition model, let A ∈ Rd×n be
the data matrix whose columns are partitioned across s servers and suppose server i holds a subset
of columns Ai ∈ Rd×ni, where n = i∈[s] ni. Then, given p ∈ [1, 2) and a desired rank k ∈ N,
Algorithm 1 outputs a subset of columns AI ∈ Rd×kpoly(log(k)) in Oe(nnz(A)k + kd + k3) time, such
that with probability 1 - o(1),
min |AIV - A|p ≤ Oe(k1/p-1/2)	min	|ALV - A|p
V	L⊂[n],∣L∣ = k
Algorithm 1 uses 1 round of communication and O(sdk) words of communication.
Proof. Approximation Factor. In the following proof, let L ⊂ [n], |L| = k denote the best
possible subset of k columns of A that gives the minimum k-CSSp cost, i.e., the cost minV |ALV -
A|p achieves minimum. First, note that
min |AIV - A|p
≤ |AIV0-A|p	V0 := arg min |SAIV - SA|p,2
V,
≤ |SAIV0 - SA|p	By Lemma 2
11
=O(kP 2 )∣SAiV - SA∣p,2	By Lemma 1, and S has k ∙ poly(log(nd)) rows
SAI is the selected columns output from the bi-criteria O(1)-approximation k-CSSp,2 algorithm.
Let (SAT)* denote the best rank k approximation to SAT. By Theorem 5,
11	11
O(kP-1 )∣SAiV0 - SA∣p,2 ≤ O(k 1-1) ∙ O(1)∣(SAT)* - SAT*
11
≤ O(kP-2 )min ∣SAlV - SAT|p,2
Note that SAT = [SA1T1 , . . . , SAsTs] is a column-wise concatenation of all coresets of SAi,
∀i ∈ [s]. By Lemma 4,
s
(min |SALV - SAT|p 2)1/p = (Xmin|SALVi -SAiTi|p2)1/p
V	p,	i=1 Vi	p,
6
Under review as a conference paper at ICLR 2021
ss
= (X(1±)pmin|SALVi-SAi|pp,2)1/p= (1±)(Xmin|SALVi-SAi|pp,2)1/p
i=1	Vi	i=1 Vi
= (1 ±)min|SALV - SA|p,2
Hence,
11	11
O(kP-2)min ∣SAlV - SAT|p,2 ≤ O(kP-2)min ∣SAlV - SA∣p,2
11
≤	O(kp 2) min ∣SAlV - SA|p	By Lemma 1
≤	O(k1 -2) ∙ log1∕p(nd) min |AlV — A|p By Lemma 3
1	1
Therefore, We conclude: minv |AiV - A|p ≤ O(kP-2) minv |AlV - A|p.
Communication Cost. Sharing the dense p-stable sketching matrix S with all servers costs O(Sdk ∙
poly(log(nd))) communication (this can be removed With a shared random seed). Sending all
coresets SAiTi (∀i ∈ [s]) and the corresponding columns AiDi to the coordinator costs O(sdk)
communication, since each coreset contains only O(k) columns. Finally, the coordinator needs
O(sdk) words of communication to send the O(k) selected columns to each server. Therefore, the
overall communication cost is O(sdk), suppressing a logarithmic factor in n, d.
Running time. Since generating a single p-stable random variable takes O(1) time, generating
the dense p-stable sketching matrix S takes O(dk ∙ poly(log(nd))) time. Computing all S Ai,s takes
O(nnz(A)k ∙ poly(log(nd))) time. By Lemma 4, computing a single coreset SAiTi on server i
takes time nipoly(k logni) and thus computing all coresets across the servers takes at most time
npoly(k logn). By Theorem 5, the k-CSSp,2 algorithm takes time (nnz(SAT) + k2poly(lognd)) ∙
kpoly(log k) ≤ k3poly(log(knd)) to find the set of selected columns. Since the number of selected
columns is O(kpoly(log k)), it then takes the protocol O(kpoly(log k)) time to map the indices of
the output columns from k-CSSp,2 to recover the original columns AI . Therefore, the overall running
time for the protocol to find the subset of columns AI is O((nnz(A)k + kd + k3), suppressing a
low degree polynomial dependency on log(knd). After the servers receive AI, it is possible to solve
，.一 ... ≈ , ,... 一 ，一- . ... 一一 _ . .
minVi |AI Vi - Ai|p in O(nnz(AI)) + poly(d log n) time , ∀i ∈ [s] due to Wang & Woodruff (2019);
YangetaL(2018).	□
5 GREEDY k-CSS1,2
We propose a greedy algorithm, shown in Algorithm 2, for k-CSS1,2, which can be used in the
place of the algorithm described in Theorem 5. The basic version of this algorithm, discussed in
Appendix D, performs k-CSS1,2 by simply selecting the additional column, among those of A, that
reduces the approximation error the most at each iteration. Our analysis of that algorithm is inspired
by the analysis of Greedy k-CSS2 for the Frobenius norm in Altschuler et al. (2016). Here we
provide the first additive error guarantee, compared to the best possible subset of columns, for the
greedy k-CSS1,2 algorithm. For a faster running time, we make use of the Lazier-than-lazy heuristic
described in Section 5.2 of Altschuler et al. (2016), where in each iteration, rather than considering
all columns of A as candidate additional columns of AT , we only sample a subset of the columns of
A of size O(n log[1/，)), and pick the column among those that improves the objective the most.
Theorem 7. Let A ∈ Rd×n be the data matrix and k ∈ N be the desired rank. Let AS be the best
possible subset of k columns, i.e., AS = arg minA minV |ASV - A|1,2. Let σ be the minimum
non-zero singular value of the matrix B of normalized columns of AS, (the j-th column of B is
B*j = (AS)*j/∣(As)*j∣2). Then, if T ⊂ [n] isthe subSetofcolumnsselectedbyGreedy k-CSS 1,2,
thefollowing holds with |T| = Ω(σ[2):
πVn IATV - A|1，2 ≤ (1 - e) s⊂[n],∣smiknv∈Rk×n |ASV - A|1，2 + e|A|1，2
Similarly, ifT ⊂ [n] is the subset of columns selected by Lazier-than-lazy Greedy k-CSS1,2, the
following holds with ∣T∣ = Ω(σ2⅛) and δ = E:
E[mVn IATV -Al1,2] ≤ (1 - e) S⊂[n],∣smkv∈Rk×n 1ASV - A11，2 + e|A11，2
7
Under review as a conference paper at ICLR 2021
Algorithm 2 Lazier-than-lazy Greedy k-CSS1,2 . This version of the greedy algorithm is based on
Section 5.2 of Altschuler et al. (2016).
Input: The data matrix A ∈ Rd×n, the number of iterations r ≤ n, a parameter δ ∈ (0, 1).
Output: A subset of columns AT from A, where |T | = r.
AT《-0
for i = 1 to r do
T J A subset of n logk1∕δ) columns of A, each selected uniformly at random (excluding the
columns whose indices are in T)
Column j* J arg min7-∈τ(minv |At∪jV 一 A∣1,2)
AT J ATUj*
end for
Dataset	Size	# servers S	Column Distribution	Rank k
synthetic TechTC	(2000 + k) × (2000 + k) 139 × 18446	2 20	1001,1002 922 columns on 19 servers, 928 columns on 1 server	{10,20,30} {10, 30,50, 70,100}
Table 1: A summary of datasets used in the experiments.
IfWelet |T | = Ω( σ^) ,then the OveraU running time of Algorithm 2 is O(n lθ⅜2/0 ∙ F), where
F is the running time needed to evaluate minv |At∪j V — A∣1,2 for a fixed j ∈ T. We can get
F = O(σ⅛ + 舞)by taking ATuj.
Since the error upper bound for greedy k-CSSp,2 depends on |A|1,2, it is not directly comparable to
the error upper bound for the proposed k-CSSp,2 from Subsection 3.3, which achieves a multiplicative
O(1)-approximation to the best rank-k approximation. We empirically compare the two versions of
k-CSSp,2 forp = 1 in Section 6.
6 Experiments
We implement our protocol for distributed k-CSSp in Algorithm 1, setting p = 1, which enables
us to compare two subroutines on the coordinator: Regular k-CSS1,2 from Section 3.3 and Greedy
k-CSS1,2 from Section 5. We compare our k-CSS1 protocol against a commonly applied baseline for
`p low rank approximation (used by Song et al. (2019a); Chierichetti et al. (2017)): rank-k Singular
Value Decomposition (SVD).
Datasets. We demonstrate the benefits of our k-CSS1 protocol on one synthetic data and one
real-world application. We present a summary of the datasets, along with the number of servers
s, the column distribution across servers and the rank k we consider for each dataset in Table 1.
The synthetic dataset constructs a data matrix M ∈ R(k+n)×(k+n) such that the top left k × k
3
submatrix is the identity matrix multiplied by n 2, and the bottom right n X n submatrix has all
1’s. The optimal rank-k left factor consists of one of the last n columns along with k 一 1 of the
first k columns, incurring an error of n2 in the '1 norm and an error n3 in the squared '2 norm.
SVD, however, will not cover any of the last n columns, and thus will get an error of n2 in both
the '1 and squared '2 norms. We set n = 2000 and apply i.i.d. Gaussian noise to each entry
with mean 0 and standard deviation 0.01. We consider a real-world application, term-document
clustering, where k-CSS2 algorithm is previously applied (Mahoney & Drineas, 2009). TechTC3
contains 139 documents processed in bag-of-words representation with a dictionary of 18446 words.
Such representation naturally results in a sparse matrix. k-CSSp is used to select the top k most
representative words.
Hyperparameters. Experiment hyperparameters are summarized in Table 2. We denote the number
of rows in our 1-stable (Cauchy) sketching matrix by cauchy size, and the strong coreset size
by coreset size. We have two additional hyperparameters for regular k-CSS1,2. We denote the
number of rows in the sparse embedding matrix of O(k) rows by sketch size, and the number
of non-zero entries in each column of the sparse embedding matrix by sparsity.
3http://gabrilovich.com/resources/data/techtc/techtc300/techtc300.html
8
Under review as a conference paper at ICLR 2021
	synthetic		TechTC	
	Greedy	Regular	Greedy	Regular
Cauchy size coreset size sketch size sparsity	-8k- 10 - -	8k 10k k/3 min(5, k/3)	40 min(4 × cauchy size, 6k) - -	40 min(4 × cauchy size, 6k) k/3 min(5, k/3)
Table 2: A summarization of hyperparameters used for each dataset. Note that a too small coreset
size compared to cauchy size will incur large `1 cost, and coreset size needs to be
increased as rank k increases.
Figure 2: Results on synthetic and TechTC. The green line denotes Greedy k-CSS1,2, the orange
lines denotes Regular k-CSS1,2, and the blue line denotes SVD.
Setup. We conduct 15 trials for each experiment setting, and report the average `1 error, i.e.
|Ak - A|1 for a O(k)-rank matrix Ak output by the algorithm that approximates the original data
matrix A ∈ Rd×n , across all trials. However, we notice that computing the `1 error for our protocol
involves computing an `1 regression on all columns of the data matrix, which takes prohibitively long
time on the real-world dataset TechTC. Therefore, for TechTC, we proceed with an approximate
computation as follows: for the output subset of columns AI, first i.i.d. sample 30% columns from
A, denoted as AJ, with probability proportional to |A*j』1, Vj ∈ [n]; then compute a lower bound
for the '1 error L = Pj∈J minv | AIV 一 A*j∙ 11; after that, sample 20LAl1 columns from A, denoted
as AK, with probability again proportional to | A*j |1, Vj ∈ [n]; finally, the '1 error is computed as
pLη- ∙ Ek∈K 20∣A1 .∣] minv |AIV - A*k|1. The approximated '1 error can be shown as an unbiased
estimation of minV |AIV 一 A|1 with small variance by Hoeffding’s bound.
Results. We present our empirical results in Figure 2. The distributed protocol performs better
using GREEDY k-CSS1,2 than REGULAR k-CSS1,2 on both datasets, and in other settings we include
in the supplementary material.
We also conducted experiments on three different datasets, bcsstk13, isolet, and
caltech-101, extensively comparing the approximation error as well as the running time for
different algorithms. A comprehensive reporting of our results is given in Appendix G.
7	Conclusion
In this work, we give the first nearly-optimal communication and number of rounds protocol for
distributed k-CSSp (1 ≤ p < 2) in the column partition model, which achieves Oe(k1/p-1/2)-
approximation to the best possible subset of columns, with O(sdk) communication cost, 1 rounds
and polynomial time. To achieve a good approximation factor, we use dense p-stable sketching and
work with the 'p,2 norm, which enables us to use an efficient construction of strong coresets and an
O(1)-approximation bi-criteria k-CSSp,2 algorithm. We further propose a greedy algorithm for k-
CSS1,2 and show the first additive error upper bound compared to the best possible subset of columns.
We implement our distributed protocol using both greedy k-CSS1,2 and regular k-CSS1,2. Our results
empirically show that greedy k-CSS1,2 gives substantial improvements over regular k-CSS1,2 on
real-world datasets. For future works, it is not known whether a O(k1/p-1/2)-approximation factor
to the best possible subset of columns, is optimal for distributed k-CSSp (1 ≤ p < 2).
9
Under review as a conference paper at ICLR 2021
References
Jason Altschuler, Aditya Bhaskara, Gang Fu, Vahab Mirrokni, Afshin Rostamizadeh, and Morteza Zadimoghad-
dam. Greedy column subset selection: New bounds and distributed algorithms. In Proceedings of the 33rd
International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pp.
2539-2548.JMLR.org, 2016.
Maria-Florina Balcan, Yingyu Liang, Le Song, David P. Woodruff, and Bo Xie. Distributed kernel principal
component analysis. CoRR, abs/1503.06858, 2015.
Maria-Florina Balcan, Yingyu Liang, Le Song, David P. Woodruff, and Bo Xie. Communication efficient
distributed kernel principal component analysis. In Balaji Krishnapuram, Mohak Shah, Alexander J. Smola,
Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016,
pp. 725-734. ACM, 2016.
Frank Ban, Vijay Bhattiprolu, Karl Bringmann, Pavel Kolev, Euiwoong Lee, and David P. Woodruff. A PTAS
for `p-low rank approximation. In Timothy M. Chan (ed.), Proceedings of the Thirtieth Annual ACM-
SIAM Symposium on Discrete Algorithms, SODA 2019, San Diego, California, USA, January 6-9, 2019,
pp. 747-766. SIAM, 2019. doi: 10.1137/1.9781611975482.47. URL https://doi.org/10.1137/1.
9781611975482.47.
Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions. SIAM Journal on Computing, 46
(2):543-589, 2017.
Christos Boutsidis, Michael W. Mahoney, and Petros Drineas. An improved approximation algorithm for the
column subset selection problem. CoRR, abs/0812.4293, 2008. URL http://arxiv.org/abs/0812.
4293.
Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based matrix reconstruction.
SIAM Journal on Computing, 43(2):687-717, 2014.
Christos Boutsidis, David P. Woodruff, and Peilin Zhong. Optimal principal component analysis in distributed
and streaming models. In Daniel Wichs and Yishay Mansour (eds.), Proceedings of the 48th Annual ACM
SIGACT Symposium on Theory of Computing, STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pp.
236-249. ACM, 2016.
J. M. Chambers, C. L. Mallows, and B. W. Stuck. A method for simulating stable random variables. Journal of
the American Statistical Association, 71(354):340-344, 1976. doi: 10.1080/01621459.1976.10480344. URL
https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480344.
Flavio Chierichetti, Screenivas Gollapudi, Ravi Kumar, Silvio Lattanzi, Rina Panigrahy, and David P. Woodruff.
Algorithms for `p Low-Rank Approximation. 2017.
Kenneth L. Clarkson and David P. Woodruff. Input sparsity and hardness for robust subspace approximation. In
Proceedings of the 2015 IEEE 56th Annual Symposium on Foundations of Computer Science (FOCS), FOCS
’15, pp. 310-329, USA, 2015. IEEE Computer Society. ISBN 9781467381918. doi: 10.1109/FOCS.2015.27.
Michael B. Cohen and Richard Peng. Lp row sampling by lewis weights. In Proceedings of the Forty-Seventh
Annual ACM Symposium on Theory of Computing, STOC ’15, pp. 183-192, New York, NY, USA, 2015.
Association for Computing Machinery. ISBN 9781450335362. doi: 10.1145/2746539.2746567.
Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality
reduction for k-means clustering and low rank approximation. In Rocco A. Servedio and Ronitt Rubinfeld
(eds.), Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015,
Portland, OR, USA, June 14-17, 2015, pp. 163-172. ACM, 2015. doi: 10.1145/2746539.2746569. URL
https://doi.org/10.1145/2746539.2746569.
Chen Dan, Hong Wang, Hongyang Zhang, Yuchen Zhou, and Pradeep K Ravikumar. Optimal analysis of subset-
selection based LP low-rank approximation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 2541-2552. Curran
Associates, Inc., 2019.
Ahmed K. Farahat, Ahmed Elgohary, Ali Ghodsi, and Mohamed S. Kamel. Distributed column subset selection
on mapreduce. In Hui Xiong, George Karypis, Bhavani M. Thuraisingham, Diane J. Cook, and Xindong
Wu (eds.), 2013 IEEE 13th International Conference on Data Mining, Dallas, TX, USA, December 7-
10, 2013, pp. 171-180. IEEE Computer Society, 2013. doi: 10.1109/ICDM.2013.155. URL https:
//doi.org/10.1109/ICDM.2013.155.
10
Under review as a conference paper at ICLR 2021
Nicolas Gillis and Stephen A. Vavasis. On the complexity of robust PCA and `1 -norm low-rank matrix
approximation. CoRR, abs/1509.09236, 2015.
Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In
Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms, pp. 1207-1214.
SIAM, 2012.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217-288, 2011.
Lingxiao Huang and Nisheeth K. Vishnoi. Coresets for clustering in euclidean spaces: Importance sampling
is nearly optimal. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2020, pp. 1416-1429, New York, NY, USA, 2020. Association for Computing Machinery. ISBN
9781450369794. doi: 10.1145/3357713.3384296. URL https://doi.org/10.1145/3357713.
3384296.
Qifa Ke and Takeo Kanade. Robust l1 norm factorization in the presence of outliers and missing data by
alternative convex programming. In 2005 IEEE Computer Society Conference on Computer Vision and
Pattern Recognition (CVPR 2005), 20-26 June 2005, San Diego, CA, USA, pp. 739-746. IEEE Computer
Society, 2005. doi: 10.1109/CVPR.2005.309. URL https://doi.org/10.1109/CVPR.2005.309.
Yingyu Liang, Maria-Florina Balcan, Vandana Kanchanapally, and David P. Woodruff. Improved
distributed principal component analysis. In Zoubin Ghahramani, Max Welling, Corinna Cortes,
Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Sys-
tems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pp. 3113-3121, 2014. URL http://papers.nips.cc/paper/
5619- improved- distributed- principal- component- analysis.
Arvind V. Mahankali and David P. Woodruff. Optimal `1 column subset selection and a fast PTAS for low rank
approximation. CoRR, abs/2007.10307, 2020. URL https://arxiv.org/abs/2007.10307.
Michael Mahoney and Petros Drineas. Cur matrix decompositions for improved data analysis. Proceedings of
the National Academy of Sciences of the United States of America, 106:697-702, 02 2009. doi: 10.1073/pnas.
0803205106.
Andrew McGregor. Graph stream algorithms: A survey. SIGMOD Rec., 43(1):9-20, May 2014. ISSN 0163-5808.
doi: 10.1145/2627692.2627694. URL https://doi.org/10.1145/2627692.2627694.
Xiangrui Meng and Michael W. Mahoney. Low-distortion subspace embeddings in input-sparsity time and
applications to robust linear regression. In Proceedings of the 45th Annual ACM Symposium on Theory of
Computing, pp. 91-100. ACM, 2013. doi: 10.1145/2488608.2488621.
Baharan Mirzasoleiman, AshWinkUmar Badanidiyuru, Amin Karbasi, Jan Vondrak, and Andreas Krause. Lazier
than lazy greedy. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI’15,
pp. 1812-1818. AAAI Press, 2015. ISBN 0262511290.
Jelani Nelson and Huy L. Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser subspace
embeddings. In Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer
Science, FOCS ’13, pp. 117-126, USA, 2013. IEEE Computer Society. ISBN 9780769551357. doi:
10.1109/FOCS.2013.21.
Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of dvoretzky’s theorem in `pn . Stochastic
Processes and their Applications, 127(10):3187 - 3227, 2017. ISSN 0304-4149. doi: https://doi.org/10.1016/
j.spa.2017.02.007.
Christian Sohler and David P. Woodruff. Strong coresets for k-median and subspace approximation: Goodbye
dimension. In Mikkel Thorup (ed.), 59th IEEE Annual Symposium on Foundations of Computer Science,
FOCS 2018, Paris, France, October 7-9, 2018, pp. 802-813. IEEE Computer Society, 2018. doi: 10.1109/
FOCS.2018.00081. URL https://doi.org/10.1109/FOCS.2018.00081.
Zhao Song, David P. Woodruff, and Peilin Zhong. LoW rank approximation With entryWise l1-norm error.
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017, pp.
688-701, NeW York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450345286. doi:
10.1145/3055399.3055431.
Zhao Song, David P. Woodruff, and Peilin Zhong. Average case column subset selection for entryWise `1 -norm
loss. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 10111-10121,
2019a.
11
Under review as a conference paper at ICLR 2021
Zhao Song, David P. Woodruff, and Peilin Zhong. Towards a zero-one law for column subset selection. In
Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 6120-6131, 2019b.
Ruosong Wang and David P. Woodruff. Tight bounds for lp oblivious subspace embeddings. In Proceedings of
the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’19, pp. 1825-1843, USA, 2019.
Society for Industrial and Applied Mathematics.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and Trends in Theoretical
Computer Science, 10(1-2):1-157, 2014. doi: 10.1561/0400000060. URL https://doi.org/10.
1561/0400000060.
Jiyan Yang, Yin-Lam Chow, Christopher Re, and Michael W. Mahoney. Weighted Sgd for 'p regression
with randomized preconditioning. Journal of Machine Learning Research, 18(211):1-43, 2018. URL
http://jmlr.org/papers/v18/17-044.html.
Linbin Yu, Miao Zhang, and Chris H. Q. Ding. An efficient algorithm for l1-norm principal component
analysis. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012,
Kyoto, Japan, March 25-30, 2012, pp. 1377-1380. IEEE, 2012. doi: 10.1109/ICASSP.2012.6288147. URL
https://doi.org/10.1109/ICASSP.2012.6288147.
12
Under review as a conference paper at ICLR 2021
A A HIGH COMMUNICATION COST PROTOCOL FOR k-CSSp (p ≥ 1)
We start by describing a protocol for distributed k-CSSp, which works for all p ≥ 1, in the column partition
model, and which achieves an O(k2 * *)-approximation to the best rank-k approximation, using O(1) rounds and
polynomial time but requiring a communication cost that is linear in n + d. The inputs are a column-wise
partitioned data matrix A ∈ Rd×n distributed across s servers and a rank parameter k ∈ N. Each server i holds
part of the data matrix Ai ∈ Rd×ni , ∀i ∈ [s], and such that Pis=1 ni = n.
We use a single machine, polynomial time bi-criteria k-CSSp algorithm as a subroutine of the protocol, e.g.,
Algorithm 3 in Chierichetti et al. (2017), which selects a subset of O(k) columns AT of the data matrix
A ∈ Rd×n in polynomial time, for which minX |ATX - A|p ≤ O(k) minrank-k Ak |A - Ak |p, ∀p ≥ 1.
Algorithm 3 A protocol for k-CSSp (p ≥ 1)
Initial State: Server i holds matrix Ai ∈ Rd×ni, ∀i ∈ [s].
Server i:
Apply polynomial time bi-criteria k-CSSp on Ai to obtain a subset Bi of columns as the left factor.
Solve for the right factor Vi = arg minVi |UiVi - Ai|p. Send Ui and Vi to the coordinator.
Coordinator:
Column-wise concatenate the UiVi to obtain UV = [U1V1, . . . , UsVs]. Apply a polynomial time
bi-criteria k-CSSp algorithm on UV to obtain a subset C of columns. Send C to each server.
Server i:
Solve minXi |CXi - Ai |p to obtain the right factor.
Approximation Factor. Let UV denote the column-wise concatenation of the Ui%. Let X* =
arg minX |CX - A|p . Then,
|CX * - AS|p ≤ |CX* - UV |p + |UV - A|p	By the triangle inequality
≤ O(k)	min |UV - (UV)k |p + |UV - A|p By the O(k)-approximation of k-CSSp
rank-k (UV )k
≤ O(k)|UV-A|p
s
= O(k)(X |UiVi -Ai|p)
i=1
s
≤ O(k)(X O(k) min |Ai - Ai* |p)	By the O(k)-approximation of k-CSSp
一	rank-k A*
i=1	i
s
≤ O(k2) X |Ai - (A*)i|p	A* = arg min |A - A*|p
rank-k A*
i=1
= O(k2)|A - A*|p
Communication Cost.	Since Ui	∈	Rd×O(k)	and	Vi	∈	RO(k)×ni , sending	Ui	and Vi costs Oe(skn). Since
C ∈ Rd×O(k), sending C from the coordinator to all servers costs Oe(sdk). Thus the overall communication
cost is O(s(n + d)k).
Running time. According to Chierichetti et al. (2017), applying the k-CSSp algorithm and solving `p
regression can both be done in polynomial time. Thus the overall running time of the protocol is polynomial.
Problems with this protocol. Although this protocol works for all p ≥ 1, a communication cost that
linearly depends on the large dimension n is too high, and furthermore, the output C is not a subset of columns
of A, because the protocol applies k-CSSp on a concatenation of both the left factor Ui and the right factor
Vi . Ui is a subset of columns of Ai but Vi is not necessarily a sampling matrix. One might wonder whether
it is possible that each server only sends Ui and the coordinator then runs k-CSSp on a concatenation of the
Ui. This will not necessarily give a good approximation to minrank-k Ak |A - Ak |p because the columns not
selected in the Ui locally on each server might become globally important. Finally, although it is possible to
improve the approximation factor to O(k) by making use of an O( k)-approximation algorithm for `p-low
rank approximation that also selects a subset of columns (Mahankali & Woodruff, 2020), this protocol would
still suffer from all of the aforementioned problems.
13
Under review as a conference paper at ICLR 2021
B Preliminaries for Our Upper Bound Proofs
B.1	Norms
LemmaL (Norm Relationships) For a matrix A ∈ Rd×n, |A|p 2 ≤ |A|p and |A|p ≤ dP- 2 |A|p 2, where
1 ≤p < 2.
Proof. Let x ∈ Rd . For 0 < p < r,
1 -1
|x|r ≤ |x|p ≤ dp r |x|r
Let r = 2. Then we have
∣χ∣2 ≤ |x|p ≤ d1 -1 ∣χ∣2
Notethat|A|p,2 = (Pj|A*j|p)p and |A|p = (PjlARp)p .
Therefore,
and
|A|p
1
P
= |A|
1 -
d p
|A|p,2
p
1
2
□
B.2	LOWER BOUND ON THE COST - NO CONTRACTION WHEN APPLYING A p-STABLE
Sketch
We show a lower bound on the approximation error for a sketched subset of columns, |SATV - SA|p, in terms
of |AT V - A|p . The lower bound holds simultaneously for any arbitrary subset AT of chosen columns, and for
any arbitrary right factor V .
We begin the proof by first showing that applying a dense p-stable sketch to a vector will not shrink its p-norm.
This is done in Lemma 2.1. We further observe that although p-stable random variables are heavy-tailed, we
can still bound their tail probabilities by applying Lemma 9 from Meng & Mahoney (2013). We note this in
Lemma 2.2. Note that the Xi ’s do not need to be independent in this lemma.
Equipped with Lemma 2.1, Lemma 2.2 and a net argument, we can now establish a lower bound on |SATV -
SA|p . We first show in Lemma 2.3 that, with high probability, for any arbitrarily selected subset AT of columns
and for an arbitrary column A*j, the error incurred to fit SA*j using the columns of SAT is no less than the
error incurred to fit A*j using the columns of AT. We then apply a union bound over all subsets T ⊂ [n] and
columns j ∈ [n] to conclude our lower bound in Lemma 2.
Lemma 2.1. (No Contraction of p-stable Sketch) Given a matrix S ∈ Rt×m whose entries are i.i.d. p-stable
random variables rescaled by Θ
following holds:
, where 1 ≤ p < 2, for any fixed y ∈ Rm, with probability 1 -
et, the
|Sy|p ≥ |y|p
Proof. By P-Stability, we have |Sy|p = Pi=I (|y|p空where |Z" ≥ 0 are half p-stable random variables.
Since Pr∖∖Zi∖ = Ω(1)] > 11, by applying a Chernoff bound (to the indicators 12∣≥c for a sufficiently
small constant C), we have Pt=ι ∖Zi∖p = Ω(t) with probability 1 — et. Therefore, with probability 1 - -1τ,
∖Sy∖p ≥ ∖y∖p.	e	匕
Lemma 2.2. (Upper Tail Inequality for p-stable Distributions) Let p ∈ (1, 2), and m > 3. For i ∈ ∖m], let Xi
be a standard p-stable random variable, and let γi > 0 and γ = im=1 γi. Let X = im=1 γi ∖Xi ∖p. Then, for
any t ≥ 1, Pr∖X ≥ tαp γ] ≤ 2 ιog(mt), Where ap > 0 is a constant that is at most 2p-1.
Proof. Lemma 9 from Meng & Mahoney (2013) for p ∈ (1, 2).
□
14
Under review as a conference paper at ICLR 2021
Lemma 2.3. (No Contraction for All Sketched Subsets and Columns) Let A ∈ Rd×n, and k ∈ N. Let
t = k ∙ Poly(lognd), and let S ∈ Rt×d be a matrix whose entries are i.i.d. standardP-stable random variables,
1
rescaled by Θ(1∕tP). Finally, let m = k ∙ poly (log k). Then, with probability 1 — o(1), for all T ⊂ [n] with
|T| = m, for all j ∈ [n], and for all y ∈ R|T |,
IATy 一 ARp ≤ IS(ATy - A*j)|p
Proof. Step 1: We first extend Lemma 2.1 and argue that applying a p-stable sketching matrix S ∈ Rt×n will
not shrink the norm ISyIp ≥ IyIp (1 ≤ p < 2) simultaneously for all y in the column span of [AT, Aj] =: AT,j ,
by a net argument.
In order to bound the p-norm of sketched vectors y in a net, we begin by showing that with high probability all
entries in S are bounded. Let D = poly(mt). Consider the following two cases:
Case 1:	p = 1. The entries of the 1-stable sketching matrix Sij are standard Cauchy random variables. Consider
half-Cauchy random variables Xi,j = ISi,j I. The cumulative distribution function of half Cauchy random
variables X is F(x) = Rx 网*])dt = 1 — Θ( 1). Thus, for any i ∈ [t] and j ∈ [m], Pr[∣Sj∣ ≤ D]=
1 - θ(D1).
Case 2:	p ∈ (1, 2). We apply the upper tail bound of p-stable random variables in Lemma 2.2. For any fixed
i ∈ [t] and j ∈ [m], Pr[∣Sj∣p ≤ Dp] ≥ 1 — Θ(loDpt)), which implies Pr[∣Sj∣ ≤ D] ≥ 1 — Θ(D).
Therefore, forp ∈ [1, 2), by a union bound over all entries in S, if we define the event E1 to mean that for all
i ∈ [t] and j ∈ [m], we simultaneously have ISijI ≤ D, then Pr[Eι] ≥ 1 — Θ(m)by a union bound. The
event E1 occurring implies that for any y ∈ Rd, since all entries in S are rescaled by O(1/t1/p),
∣Sy∣p ≤ |y|pt1/p∣s∣∞ ≤ D∣y∣p
Consider the unit `p ball B = {y ∈ Rd : ∣y∣p = 1, ∃z ∈ Rm s.t. y = AT,jz} in the column span of
AT,j . A subset N ⊂ B is a γ-net for B if for all y ∈ B there exists some u ∈ N such that ∣y — u∣p ≤ γ,
for some distance γ > 0. There exists such a net N for B of size ∣N∣ = (-)O(m) by
construction, since the column span of AT,j has dimension at most m + 1. We choose γ
a standard greedy
=m⅛ ,andthus
DO(m log m)
∣N∣ ≤ (m2 D)O(m)
By applying Lemma 2.1, and a union bound over all vectors y ∈ N, we have that event E2 : for all y ∈ N
simultaneously, ∣Sy∣p ≥ ∣y∣p — E? has probability at least 1 — 0℃产 m).
Consider an arbitrary unit vector X ∈ B. There exists some y ∈ N such that |x —y∣p ≤ Y = m2^ . Conditioning
on both events Ei and event E2, we have the following with probability 1 — Θ(暗)— DO(mog m):
	∣SX∣p ≥ ∣Sy∣p — ∣S(X — y)∣p	Triangle Inequality ≥ ∣y∣p — ∣S(X — y)∣p	By event E2 ≥ ∣y∣p — D∣(X — y)∣p	Implication of event E1 ≥ ∣y∣p — DY	By ∣χ — y∣p ≤ Y =∣y∣p - O( m12) =∣x∣p — O( m12)	∣x∣p = ∣y∣p =1
Fo |x| ~ W lin St po an	r a sufficiently large m, O( miɪ) is at most ∣, and thus ⅛⅛p = ∣ ≥ }. This implies ∣Sx∣p ≥ ∣x∣p — ⅛}p = p . We can rescale S by a factor of 2 so that ∣SX∣p ≥ ∣X∣p . have shown that ∣Sy∣p ≥ ∣y∣p holds simultaneously for all unit vectors y in the column span of AT,j . By earity, we conclude that ∣Sy∣p ≥ ∣y∣p (1 ≤ p < 2) holds simultaneously for all y in the column span of AT,j . ep 2: Next, we apply a union bound over all possible subsets T ⊂ [n] of chosen columns from A and all ssible single columns A*j for j ∈ [n],tOargUethatIS(ATy — A*j)∣p ≥ ∣Aty — A*j∣p holds for all y ∈ R|T| d all T ⊂ [n], j ⊂ [n] with high probability. Note that ∣T ∣ = m = O(k ∙ poly (log k)).
Fr po	om Step 1, we have shown that event E2 fails with probability DO(：togm). Thus event E2 fails over all ssible subsets T and all possible single columns A* j with probability at most DOm log m) d d ! d ≤ DOm log m) dO(m)d et	Im — 1)	—	et
15
Under review as a conference paper at ICLR 2021
The above failure probability is o(1) as long as
t = θ(log (DOmlogm) ∙ dO(m) ∙ d)))
= Θ(O(m log m) log(D) + O(m) log d)
= Θ(O(kpoly(log k)) log(mt) + O(kpoly(log k) log(d)))
Thus it suffices to have t = kpoly(log nd) to have failure probability at most ^/⑴.
Now We condition on a single global event Ei and that event E2 holds for all possible T and A*j. We conclude
that with probability 1 — Θ(暗)—^O^, the following holds simultaneously for all T ⊂ [n] and for all A*j for
which j ∈ [n]:
|Aty - A*j∣p ≤ ∣S(Aty — A*j)|p.
□
Lemma 2. (Lower Boundfor Sketched Error) Let A ∈ Rd ×n and k ∈ N. Let t = k ∙ poly (log (nd)) ,and let
S ∈ Rt×d be a matrix whose entries are i.i.d. standardP-stable random variables, rescaled by θ(1∕tP). Then,
with probability 1 — o(1) ,for all T ⊂ [n] with |T | = k ∙ poly (log k) and for all V ∈ RlTl×n,
|AtV — A|p ≤ |SAtV — SA|p
Proof. Let yj denote the j-th column of V, where j ∈ [n]. By applying Lemma 2.3, and a union bound over all
columns of V, for m = |T| = k ∙ poly(log k), t = k ∙ poly(log(nd)) and D = poly(mt) = poly(k log(nd)),
the following holds with probability 1 — Θ(m) — ^^)= 1 — o(1),
n
|At V - A|p = (X |At yj - Aj IP)1
j=1
n
≤ (X ∣s(Atyj - Aj)IP)1
j=1
= |SATV — SA|p
□
B.3	Upper Bound on the Cost
We show an upper bound on the approximation error of k-CSSp on a sketched subset of columns,
|SAT V — SAT |p , which holds for a fixed subset AT of columns and for the minimizing right factor
V = arg minV |SAT V — SA|p for that subset of columns.
We first adapt Lemma E.17 from Song et al. (2017) to establish an upper bound on the error |SATV — SA|p
for any fixed V in Lemma 3.1. We then apply Lemma 3.1 to the minimizer V to conclude the upper bound in
Lemma 3.
Lemma 3.1. (An Upper Bound on Norm ofA Sketched Matrix) Given A ∈ Rn×d andp ∈ [1, 2), and U ∈ Rn×k
and V ∈ Rk×d, if S ∈ Rt×n is a dense P-stable matrix, whose entries are rescaled by Θ (3)，then with
probability at least 1 — O(1),
|SUV — SA|pp ≤ O(log(td))|U V — A|pp
Here, the failure probability O(1) can be arbitrarily small.
Proof. Lemma E.17 from Song et al. (2017).	□
Lemma 3. (Upper Bound on Sketched Error) Let A ∈ Rd×n and k ∈ N. Let t = k ∙ poly(log(nd)), and let
S ∈ Rt×d be a matrix whose entries are i.i.d. standardP-stable random variables, rescaled by θ(1∕tP). Then,
for a fixed subset T ⊂ [n] of columns with ∣T ∣ = k ∙ poly (log k), with probability 1 — O(1), we have
min |SATV — SA|p ≤ min O(log1/p(nd))|AT V — A|p
Here, the failure probability can be an arbitrarily small constant.
16
Under review as a conference paper at ICLR 2021
Proof. Let X； = arg minX |SAtX — SA|p and Xg = arg minX |AtX — A|p. By Lemma 3.1,
|SAtX； — SA|p ≤ |SAtX； — SA∣P
≤ O(log(kpoly(log n)d))|AT X2； — A|pp
≤ O(log(nd))|AT X2； — A|pp
Therefore,
min |SATX — SA|p ≤ min O(log1/p(nd))|AT X — A|p
XX
.	□
B.4	STRONG CORESETS IN THE `p,2 NORM
Lemma 4 (Strong Coreset in `p,2 norm). Let A ∈ Rd×n, k ∈ N, p ∈ [1, 2), and , δ ∈ (0, 1). Then, in
n ∙ poly (k log n/e) time, one can find a sampling and reweighting matrix T with O(d log d/e2) ∙ log(1∕δ)
columns such that, with probability 1 — δ, for all rank-k matrices U,
min |UV —AT|p,2=(1±e) min |UV — A|p,2
rank-k V	rank-k V
AT is called a strong coreset of A.
Proof. We can obtain T with O(d(log d)/e2) columns using the strong coreset construction from Lemma 16
in Sohler & Woodruff (2018). Note that the coreset construction for k-subspace approximation in Sohler &
Woodruff (2018) aims at removing a dependence on d in the coreset size. The algorithm first finds a poly(k)-
dimensional subspace S by running a dimensionality reduction algorithm and constructs coresets in the lower
dimensional subspace S, resulting in a coreset size of poly(k/e). But in our case, we do not want our coreset size
to have a polynomial dependency on k while a linear dependency on d suffices. Thus, instead of running their
dimensionality reduction algorithm to find such a subspace S to project A to, we directly use the the column
span of the input matrix A as the subspace S appended with a row of zeros to construct the B in Lemma 16 of
Sohler & Woodruff (2018). Note that in the original algorithm, the appended column encodes the distances from
the input matrix A to the subspace S, but in our case it is just all 0’s. Then the guarantees and running time
claimed above immediately follow from Lemma 16 of Sohler & Woodruff (2018).
We note that the size of our coreset for k-subspace approximation can be further reduced to O(k), suppressing a
logarithmic dependence on k, ɪ, ɪ, using an additional O (nd) time, by combining Corollary 5.16 of HUang &
Vishnoi (2020) and the importance sampling scheme from Stage 2 of Algorithm 1 in Huang & Vishnoi (2020).
Furthermore, though not explicitly stated in Lemma 16 of Sohler & Woodruff (2018), the coreset size has a
log( 1) dependence on the failure probability δ due to the importance sampling (Sohler & Woodruff, 2018;
Huang & Vishnoi, 2020).	□
Note that an accuracy of (1 ± e) is desired for our extension to the streaming model, described in Appendix E,
and for both Algorithm 1 and Algorithm 7, we perform a union bound over strong coreset constructions, for
which a log(1∕δ) dependence on δ is sufficient.
C	POLYNOMIAL TIME, O(1) -APPROXIMATE BI-CRITERIA k-CSSp,2
We give a detailed analysis on the polynomial time, O(1)-approximate k-CSSp,2 algorithm presented in
Algorithm 4, which is based on ideas in Clarkson & Woodruff (2015). We first use a sparse embedding matrix
S to obtain an O(1)-approximate left factor. We then use 'p-Lewis weight sampling Cohen & Peng (2015) to
select a subset of columns.
Algorithm 4 polynomial time, O(1)-approximation for k-CSSp,2 (1 ≤ p < 2)
Input: The data matrix A ∈ Rd×n, rank k ∈ N
Output: The left factor U ∈ Rd×O(k), the right factor V ∈ RO(k)×n such that |UV - A|p,2 ≤
O(1) minrank-k Ak |Ak - A|p,2
S J O(k) X d sparse embedding matrix, with sparsity S = poly(log k).
S0 J n × O(k ) sampling matrix, each column of which is a standard basis vector chosen randomly
according to the `p Lewis weights of columns of SA.
return U J AS0, V J (AS0)*A 什 denotes the Moore-Penrose pseudoinverse.}
17
Under review as a conference paper at ICLR 2021
C.1	Sparse Embedding Matrices
e
The sparse embedding matrix S ∈ RO(k)×d of Nelson & Nguyen (2013), and used by Clarkson & Woodruff
(2015), is constructed as follows: each column of S has exactly s non-zero entries chosen in uniformly random
locations. Each non-zero entry is a random value ± √1s with equal probability. S is also called the SParsity of S.
Let h be the hash function that picks the location of the non-zero entries in each column of S and σ be the hash
function that determines the sign ± of each non-zero entry.
Applying the sparse embedding matrix S to A enables us to obtain a rank-k right factor that is at most a factor
of O(1) worse than the best rank-k approximation error in the `p,2 norm. We adapt Theorem 32 from Clarkson
& Woodruff (2015) to show this in Theorem 5.5. Notice that in Theorem 32 of Clarkson & Woodruff (2015),
the number of rows required for S is O(k2), but this can be reduced to Oe(k) through a different choice of
hyperparameters when constructing the sparse embedding matrix S .
We note two choices of hyperparameters, i.e., the number m of rows and sparsity s, of S in Theorem 5.1 and
Theorem 5.2, both of which give the same result. The proof of Theorem 32 from Clarkson & Woodruff (2015)
uses the hyperparameters from Theorem 5.1. We replace the hyperparameters from Theorem 5.2 and show in
Lemma 5.3 that O(k) rows of S suffice to preserve certain desired properties. We then combine Lemma 5.3
and Lemma 5.4 adapted from Clarkson & Woodruff (2015), to conclude our result in Theorem 5.5, following
the analysis from Clarkson & Woodruff (2015).
Theorem 5.1. (Theorem 3 from Nelson & Nguyen (2013)) For a sParse embedding matrix S ∈ Rm×n with
sParsity s = 1 and a data matrix U ∈ Rn×d, let ∈ (0, 1). With Probability at least 1 - δ all singular values
of SU are (1 ± ) as long as m ≥ δ-1 (d2 + d)/(2 - 2)2. For the hash functions used to construct S, σ is
4-wise indePendent and h is Pairwise indePendent.
Theorem 5.2. (Theorem 9 from Nelson & Nguyen (2013)) For a sParse embedding matrix S ∈ Rm×n with
sparsity S = θ(log3(d∕δ)∕e) and a data matrix U ∈ Rn ×d, let e ∈ (0,1). With probability at least 1 — δ all
singular values of SU are (1 ± e) as long as m = Ω(d log8 (d∕δ)∕e2). For the hash functions used to construct
S, we have that σ, h are both Ω(log(d∕δ))-wise independent.
Lemma 5.3. Let C be a constraint set and A ∈ Rn×d , B ∈ Rn×d0 be two arbitrary matrices. For a sparse
embedding matrix S ∈ Rm×n, there is m = O(——g2(P+pl+1)), such that with constant probability, the following
holds for all X ∈ Rd×d0
i)	|S(AX — B)|p,2 ≥ (1 — e)|AX — B|p,2
ii)	∣S(AX* — B)∣p,2 ≤ (1 + e)∣AX* — B∣p,2, where X* = arg min |AX — B∣p,2
X∈C
Proof. The proof is the same as the proof of Lemma 29 from Clarkson & Woodruff (2015), except that we use a
different choice of hyperparameters in constructing S, i.e., sparsity S and the number m of rows. In the proof of
Lemma 29 from Clarkson & Woodruff (2015), the construction of S follows Theorem 5.1, where the sparsity
S = 1, but requires m = O(d2) rows. We replace the construction by Theorem 5.2, where we pick δ = ep+1.
Now the sparsity S is larger but this construction reduces the number of rows required to m = O(d). Since both
choices of hyperparameters to construct S result in bounded (1 ± e) singular values of SU for any data matrix
U, the rest of the proof follows.	□
Lemma 5.4. Consider a data matrix A ∈ Rn×d. Let the best rank-k matrix in the `p,2 norm be Ak =
arg minrank-k A |Ak — A|p,2. For R ∈ Rd×m, if RT satisfies both of the following two conditions for all
X ∈ Rn×n:
i)	|RT (AkT X — AT)|p,2 ≥ (1 — e)|AkT X — AT |p,2
ii)	|RT (AkTX* — AT)|p,2 ≤ (1 + e)|AkT X * — AT |p,2 , where X* = arg min |AkT X — AT |p,2
X
then
min |XRTAkT—AT|pp,2 ≤ (1+3e)|AkT—AT|pp,2
rank-k X
Proof. Lemma 31 from Clarkson & Woodruff (2015).
□
Theorem 5.5.	('p,2 -Low Rank Approximation) Let the data matrix be A ∈ Rd×n and k ∈ N be the desired
rank. Let S ∈ Rm×d be a sparse embedding matrix with m = O(kpoly (log k) poly (ɪ)) rows, and sparsity
S = poly(log k). Then, the following holds with constant probability,
min |XSA — A|p,2 ≤ (1 + 3e) min |Ak — A|p,2
rank-k X	rank-k Ak
18
Under review as a conference paper at ICLR 2021
Proof. The proof is the same as the proof of Theorem 32 in Clarkson & Woodruff (2015), except that we adapt
a different construction of the sparse embedding matrix S, which reduces the number of rows from O(k2) to
O(k) with increased sparsity s.
Consider Ak = arg minrank-k A |Ak - A|p,2. Let Vk be a basis for the column space of Ak . By applying
Lemma 5.3 and Lemma 5.4 on the basis Vk, we conclude the above theorem by setting the number m of rows
to m = O ( k，晨 ɪ2)), and sparsity S = poly (log k) in the sparse embedding matrix S.
□
C.2 Lewis Weights Sampling
The `p Lewis weight is an inherent property of a matrix. By Cohen & Peng (2015), the unique set of `p Lewis
weights W for a matrix is defined as follows: for the i-th column of a matrix M ∈ Rn×d, M*j, is defined as
w2/p = MT(MW 1-2/pMt)-1M*j, where W is the diagonal matrix with Wii = Wi, ∀i ∈ [d].
We first note that sampling by Lewis weights provides a subspace embedding in the `p-norm in Theorem 5.6. We
further apply a randomized version of Dvoretzky’s Theorem in Theorem 5.7, which allows the embedding from
the `p norm into a low-dimensional Euclidean subspace with very small distortion and thus enables us to switch
between the `p norm and the `p,2 norm. Based on Theorem 5.6 and Theorem 5.7, we show in Theorem 5.8
that Lewis weights sampling provides a good subset of columns, on which the analysis of k-CSSp,2 is based.
n×d
Theorem 5.6.	('p-Lewis Weights Subspace Embedding) Let A ∈ R ×d and t = O(d). For 1 ≤ p < 2, there
exists a distribution (λ1 , λ2, . . . , λn) on the rows of A such that if we generate a matrix S with t rows, each
chosen independently as the ith standard basis vector times ——1ɪ with probability λi, then with probability
(rλi)P
1 - o(1), the following holds for all x ∈ Rd,
Ω(1)∣Ax∣p ≤ |SAx|p ≤ O(1)∣Ax∣p
Proof. Theorem 7.1 from Cohen & Peng (2015)	□
Theorem 5.7. (Randomized Dvoretzky's Theorem) Let n ∈ N, and e ∈ (0,1). Let r = &. Let G ∈ Rr×n be a
random matrix whose entries are i.i.d. standard Gaussian random variables, rescaled by 亲.For r = g, the
following holds with probability 1 - e-Θ(n), for all y ∈ Rn,
|Gy|p = (1 ± )|y|2
Proof. This follows from Theorem 1.2 from Paouris et al. (2017).	□
Theorem 5.8. (Subset of Columns by Lewis Weights Sampling) Let A ∈ Rd×n. Let S ∈ Rm×d be a sparse
embedding matrix, with m = O(k ∙ poly (log k) poly (ɪ)). Further, let S0 ∈ Rn×t be a sampling matrix whose
columns are random standard basis vectors generated according to the `p Lewis weights of columns of SA,
with t = k ∙ poly (log k). Then, for X = arg min X ∣XSAS0 — AS0 |p,2, the following holds with constant
probability,
|XSA — A∣p,2 ≤ Θ⑴ min |Ak — A∣p,2
rank-k Ak
Proof. Let X* = arg minrank-k χ* |X*SA — A∣p,2. By the triangle inequality,
|XSA — A∣p,2 ≤ |X*SA — XSA∣p,2 + |X*SA — A∣p,2
Let us consider bounding |X*SA — XSA∣p,2.
By Lemma D.28 and Lemma D.29 from Song et al. (2017), for any column sampling matrix S and for any
fixed matrix Y , it can be shown that E[|Y S|pp] = |Y S|pp. In our case, since S0 is a sampling matrix, we have
E[|Y S0|pp] = |Y S0|pp for any fixed matrix Y .
Now let G ∈ RΘ(d)×d be a rescaled random matrix whose entries are i.i.d. standard Gaussian random variables
as in Theorem 5.7. We apply Theorem 5.7 to transform between the `p space and the Euclidean space. Since
transformation of both directions can be done with very small distortion, we obtain a Θ(1) approximation. With
constant probability, we have
|X *SA — X SA∣p,2 = Θ(1)∣G(X* — X)SA∣p
By Theorem 5.7
19
Under review as a conference paper at ICLR 2021
=Θ(1)∣G(X* - X)SASlp	By Theorem 5.6
= Θ(1)∣(X* - X)SASlp,2	By Theorem 5.7
≤ Θ⑴(|X*SAS0 - AS0∣p,2 + |XSAS0 - AS0∣p,2)	Triangle Inequality
≤ Θ⑴ ∣X*SAS0 - AS0∣p,2	Since X = arg min ∣XSAS0 - AS0∣p,2
rank-k X
=Θ⑴∣G(X*SA - A)Slp	By Theorem 5.7
≤ Θ(1)∣G(X*SA - A)|p	By Markov Bound on E[∣YS0∣p] = ∣YS0∣
=Θ⑴∣X*SA - A∣p,2	By Theorem 5.7
Therefore,
IXSA - A|p,2 ≤ IX* SA - XSAIp,2 + IX* SA - Alp,2
≤ Θ(1)∣X*SA - A∣p,2
≤ Θ(1) min IA - AkIp,2	By Theorem 5.5
rank-k Ak	,
□
C.3 ANALYSIS FOR k-CSSp,2
We are now ready to show an O(1) approximation factor and polynomial running time for the bi-criteria
k-CSSp,2 presented in Algorithm 4, given in Theorem 5.
Theorem 5 (Bicriteria O(1)-Approximation Algorithm for k-CSSp,2). Let A ∈ Rd×n and k ∈ N. There
exists an algorithm that runs in (nnz (A) + d2) ∙ kpoly (log k) time and outputs a rescaled subset of columns
U ∈ Rd×O(k) of A and a right factor V ∈ RO(k)×n for which V = minV IUV - AIp,2, such that with
probability 1 - o(1),
IUV - AIp,2 ≤。⑴∙ min IAk - AIp,2
rank-k Ak
Proof. Approximation Factor First notice that the minimizer X of〔X SAS0 — AS0Ip,2 has to be in the column
span of AS0. Thus We can write X = (AS0)Y for some matrix Y. By Theorem 5.8,
IXSA - AIp,2 = I(AS0)YSA - A* ≤ Θ(1) min IA - Ak*
rank-k Ak
We denote YSA = V. We take the left factor U = AS0 and solving for minV IUV - AIp,2 will give us a Θ(1)
approximation to minrank-k Ak IA - Ak Ip,2. A good minimizer for the right factor V in the Euclidean space is
0	0 0 C0 +	C0
V = (AS0)+A. This concludes our result. Notice that since S0 is a sampling matrix with O(k) columns, we get
a rank-k left factor U as a subset of columns of A as desired.
Running time First notice that S is a sparse embedding matrix with poly(log k) non-zero entries. Thus
computing SA takes time nnz(A) ∙ poly(log k). By Cohen & Peng (2015), computing the Lewis weights of SA
takes time nnz(SA) + poly(k) ≤ nnz(A)poly(log k) + poly(k), and computing the output left factor U = AS0
takes time nnz(A). Computing (AS0)+ takes time d2 ∙ kpoly(log k). Computing the right factor V = (AS0)+A
takes nnz(A)kpoly(log k). Therefore, the overall running time is (nnz(A) + d2) ∙ kpoly(log k).
□
D	ANALYSIS FOR GREEDY k-CSS1,2
We propose a greedy algorithm for selecting columns in k-CSS1,2 presented in Algorithm ??. We give a detailed
analysis on the first additive approximation compared to the best possible subset of columns for Greedy k-CSS1,2.
Our analysis is inspired by the analysis of Greedy k-CSS2 for the Frobenius norm in Altschuler et al. (2016). We
then describe how the running time of Algorithm ?? can be improved to O(言∙ F), where F is the running time
required to evaluate minv I At ∪j V — AI1,2 for a fixed j ∈ T (note that this running time is O(nk2 + knd) if
we evaluate this by computing the pseudo-inverse of AT∪j). This improvement in the running time is obtained
by randomly sampling candidate columns from T and adding the best of these randomly sampled columns
to AT , rather than trying all columns of T — this method was previously used in Altschuler et al. (2016) for
Frobenius norm column subset selection, where it is referred to as “Lazier-than-lazy Greedy,” and the general
approach was first introduced in Mirzasoleiman et al. (2015). This version of the greedy algorithm is shown in
Algorithm 2, and we show both versions of the greedy algorithm below for convenience.
20
Under review as a conference paper at ICLR 2021
Algorithm 5 Greedy k-CSS1,2. Here we denote the set of selected columns T from A by AT and the
set of unselected columns by AT.
Input: The data matrix A ∈ Rd×n , the number of iterations r ≤ n.
Output: A subset of columns AT from A, where |T | = r.
AT J 0
for i = 1 to r do
Column j* — argmin7∙∈A亍(minv ∣Aτ∪jV — A∣1,2)
at J AT∪j*
end for
Algorithm 6 Lazier-than-lazy Greedy k-CSS1,2. This version of the greedy algorithm is based on
Section 5.2 of Altschuler et al. (2016).
Input: The data matrix A ∈ Rd×n, the number of iterations r ≤ n, a parameter δ ∈ (0, 1).
Output: A subset of columns AT from A, where |T | = r.
AT J 0
for i = 1 to r do
T J A subset of n logk1∕δ) columns of A, each selected uniformly at random (excluding the
columns whose indices are in T)
Column j* J argmin7∙∈A亍(minv |At∪jV — A∣1,2)
AT J AT∪j *
end for
First we analyze Algorithm 4, then show how this analysis can be extended to Algorithm 6. We first show in
Lemma 7.2 an improvement of the utility function with one additional column when projecting a single vector,
based on Lemma 7.1 from Altschuler et al. (2016). We then show an improvement of the utility function when
projecting a matrix in Lemma 7.3, by applying Lemma 7.2 and Jensen’s Inequality, following the analysis
in Altschuler et al. (2016). Finally, we conclude our analysis for Greedy k-CSS1,2 in Theorem 7.
Notation Consider the input matrix A ∈ Rd×n (n d). Let B be the matrix of normalized columns of A,
where the j-th column of B is B*j = A*j∕∣AR2. Let ∏t : Rd → Rd be the projection onto the column span
of AT or equivalently BT . Let σmin (M) denote the minimum singular value of some matrix M .
To aid our analysis, we define a utility function as follows, inspired by Altschuler et al. (2016). For a subset
T ⊂ [n] and a matrix M ∈ Rd×t (or a vector M ∈ Rd),
tt
Φm(T) = |M∣1,2 — |M — ∏tM∣1,2 = X (∣M*"2 — |M*i — ∏tM*i∣1,2) = X Φm*<T)
i=1	i=1
Observe that as the number of columns selected and added to T increases, we get a more accurate estimation
of M and thus the approximation error |M — πT M |1,2 decreases, which results in an increase in the utility
function ΦM (T).
Lemma 7.1. Let S, T ⊂ [n] be two sets of column indices, with ∣∏s u∣2 ≥ ∣∏τ u∣2 for some vector U ∈ Rd.
Then,
21	∣2	I ∣2∖、	2 D '2 (InSu|2 - |nT u|2)2
T (JnTOU|2 TnTU|2； ≥ σmin(BS) -4∏SUi2---
Proof. Lemma 2 from Altschuler et al. (2016), except that we replace the condition for S and T, i.e., Φu (S) ≥
Φu(T) in Altschuler etal. (2016) with ∣∏su∣2 ≥ ∣∏τu∣2. The two conditions are equivalent, since
Φu(S) ≥ Φu(T)
⇔ |u — πS u|2 ≤ |u — πT u|2
⇔ ∣u∣2 — ∣∏su|2 ≤ ∣u∣2 — ∣∏τu∣2
⇔ ∣∏Su∣2 ≥ ∣∏Tu∣2
□
21
Under review as a conference paper at ICLR 2021
Lemma 7.2. (Utility Improvement by Projecting a Single Vector) Let S, T U [n] be two sets of column indices,
with Φu(S) ≥ Φu(T) for some vector u ∈ Rd. Let k = |S|. For i ∈ [k], let Ti = T U {i}. Then,
k
X (φu(T') - Φu(T)) ≥ σmin(Bs)2
i=1
(Φu(S)- Φu(T))3
16Φu(S)2
Proof. We define a function for convenience in the analysis g : (一∞, |u|2] → R≥0 by g(x) =，|u|2 一 X.
NOtethat|g (X)I = M⅛=x.	一
kk
X (φu(T0) - Φu(T)) = X (|u - ∏Tu∣2 - |u - π零ug)
i=1 '	i=1 '
By definition of Φ
By Pythagorean Theorem
k
X (g(|nTu|2) - g(|nT0u|2))
i=1
k
≥ X |g0(|nTu|2)| (依丁严|2 -|nTu|2
i=1
By the Mean Value Theorem
k
X 2，|u|2 一 |nTu|2 (InTTu|2	InT
1k
2|u - ∏t u|2 X W/ "|2-|仃u|2)
1	C (B )2 (InSu|2 一 |nTu|2)2
2|u - πτu|2	min( S)	4|nsu|2
σmin (BS)
(|u — ∏Tu|2 - |u — nsu|2)2
8 ∙ |nsu|2|u — πτu|2
By Pythagorean Theorem
By Lemma 7.1
By Pythagorean Theorem
σmin (BS)
8 ∙ |nSu|2|u — πτu|2
∙ (|u — πτu|2 - |u — ∏su|2)2
∙ (|u - πτu|2 + |u - πsug)2
≥ σmin (BS )2
(Φu(S) - Φu(T))2 -|u - πτu|2
8 ∙ |nsu|2
Since |u — nsu|2 ≥ 0
We now lower bound |u-πTU|2 as follows.
MS u|2
|u — nτ u|2	|u — nτ u|2
|ns u|2	|u|2- |u - ns u|2
|u — ∏τ u|2	1
|u|2 - |u - nsu|2	|u|2 + |u - nsu|2
1	|u — nτ u|2
=——,... ∙-：—：-：------：—
Φu(S) |u|2 + |u - nsu|2
1	|u - nτ u|2
≥ ----：——  --：—：--
一 2Φu(S)	|u|2
_	1	|u|2 - Φu(T)
------:—— ---:—:----
2Φu(S)	|u|2
=1	∙ (1 -山)
=2Φu(S)<	|u|2)
≥ ɪ ∙ (1 - S)
≥ 2Φu(S)卜	Φu(S)/
By Pythagorean Theorem
By definition of Φ
Since |u - nsu|2 ≤ |u|2
By definition of Φ
Since Φn(S) ≤ |u|2
≥
22
Under review as a conference paper at ICLR 2021
Therefore,
(Φu(S)- Φu(T))
2Φu(S)2
(Φu(S) - Φu(T))2 ∙∣u - ∏Tu∣2
8 ∙∣∏su|2
k
X Φu (Ti0) - Φu (T) ≥ σmin (BS)2
i=1
≥ σmin(BS)2
(Φu(S) - Φu(T))3
16Φu(S)2
□
Lemma 7.3. (Utility Improvement by Projecting a Matrix) Let A ∈ Rd×n, and T, S ⊂ [n] be two sets of
column indices, with ΦA (S) ≥ ΦA (T). Furthermore, let k = |S|. Then, there exists a column index i ∈ S such
that
(Φa(S) - Φa(T))3
16kΦA(S)2
ΦA (T ∪ {i}) - ΦA (T) ≥ σmin (BS)2
Proof. The proof mostly follows the proof of Lemma 1 in Altschuler et al. (2016). We combine Lemma 7.2
with Jensen’s inequality to conclude an improvement of the utility function with one additional column when
projecting a matrix instead of a single column.
ΦA (T)
For j ∈ [n], We define δj = min(1,m *j (S)). Note that δj is 1 if the j-th column A*j has a larger projection
*j
ΦA (T)
onto BT than BS, and m/。(S) otherwise. Let k = |S|. For i ∈ [k], let Ti = T ∪{i}.
1k
E/ X (φA(TO) - φA
nk
=σmin(Bs)2 X X ("j (Ti) - Kj
j=1 i=1
≥ Xi ⅛f ∙ ©A*j (S)
By definition of Φ
By Lemma 7.2
j=1
=φaP XX (I )3 ∙
j=1
≥ φAF(XX(1-δj)∙
ΦA*j (S)
Pn=1 ΦA*i(S)
n
Note ΦA (S) =X ΦA*i (S)
i=1
1
16Φa(S)
j=1
n
ΦA*j (S)	)3
Pn=1 ©A*i (S))
By Jensen’s Inequality
*j(S))3
Since 1 - δj ≥ 1 - ；A*j (T)
j 一	©A*。(S)
⇒ (1-δj) ∙ ΦA*j (S)
≥ ΦA*j (S) - ΦA*j (T)
1
≥ 16Φa(S)
n3
2 X(ΦA*j(S)-ΦA*j(T))
j=1
(ΦA(S) - ΦA(T))3
16ΦA(S)2
Hence,
(ΦA(S) - ΦA(T))3
16ΦA(S)2
This implies there is at least one column of BS , with index i ∈ S, such that when i is added to T, the utility
function Φa(T) increases by at least 1 ∙ σmin(Bs)2 (φA(Sφ-φA)2T)' .	□
Theorem 7.	Let A ∈ Rd×n be the data matrix and k ∈ N be the desired rank. Let AS be the best possible
subset of k columns, i.e., AS = arg minA minV |ASV - A|1,2. Let σ be the minimum non-zero singular
23
Under review as a conference paper at ICLR 2021
value of the matrix B of normalized columns of AS, (the j-th column of B is B*j = (AS )*j∕∣(As )*j∣2). Then,
if T ⊂ [n] is the subset of columns selected by Greedy k-CSS 1,2, the following holds with |T | = Ω( σ2⅛2),
min |ATV - A|1,2 ≤ (1 - )	min	|ASV - A|1,2 + |A|1,2
V	S⊂[n],∣S∣=k,V ∈Rk×n
Proof. The proof follows the one of Theorem 1 in Altschuler et al. (2016).
Let Tt be the subset of columns of B selected by Greedy k-CSS1,2 after t iterations. Notice that To = 0. In
addition, define F = Φa(S) = Φa(S) — Φa(To), ∆o = F, and ∆i = ∆∆0 for i ∈ N.
Let ∆i ≥ Φa(S) — Φa (Tt) ≥ ∆i+ι = ∆2i. Our goal is to bound the number of iterations needed for the gap
between Φa(S) and Φa (Tt) to become less than ∆2i.
Consider some iteration S for which Φa(S) — Φa(Ts) ≥ ∆2i. The improvement of the utility function after
adding a column of B to Ts through greedy selection is at least the improvement of the utility function after
adding the best column of BS to Ts . By Lemma 7.3, this is at least
2 (Φa(S) - Φa(T))3 = 2	∆3	= σ2∆3
σ	16"Φa(S)2	= σ ^ 16 ∙ 8 ∙ k ∙ F2 = 128kF2 .
If the gap Φa(S) — ΦA(Tt) after t iterations is at most ∆i and at least 年,then after at most 64kF2 iterations,
σi
the gap becomes at most ∆2i.
We can use this to bound the number of iterations required for the gap to become at most F. Take N ∈ N such
that ∆N+1 ≤ F ≤ ∆N . Then the number of iterations required for the gap to become at most ∆N+1 is at
most
X 64kF2	_ 64kF2 X 1 =下一⅛∆J i=0 i 64kF 2 N	1	∆i =σ2∆2r+1 工 4N+1-i	Since δn+1 = 2N+1-i N+1 i=0 N 256k	F	N	1	1 ≤ 许	SinCe δn+1 ≥ H and∑ 4N+τ-i ≤ 3 i=0
Therefore, after |T| = Ω(δkɪ) iterations, we have
ΦA (S) — ΦA (T) ≤ ΦA (S)
⇒ ∣A∣1,2 — |A — ∏SA∣1,2 — (∣A∣1,2 — |A — ∏TA∣l,2) ≤ e(∣A∣1,2 — |A — ∏sA∣1,2)
⇒ |A — πT A|1,2 ≤ (1 — )|A — πS A|1,2 + |A|1,2
Since S is the set of indices for the best possible subset of k columns, the above is equivalent to
min |AT V — A|1,2 ≤ (1 — )	min	|ASV — A|1,2 + |A|1,2
V	S⊂[n],∣S∣=k,V ∈Rk×n
□
We now analyze Algorithm 2, based on the analysis of the Lazier-than-lazy greedy heuristic in Altschuler et al.
(2016). The first step is the following lemma based on Lemma 6 of Altschuler et al. (2016), which shows that in
expectation, the utility improves by a large amount on each iteration.
Lemma 7.4 (Expected Increase in Utility — Based on Lemma 6 of Altschuler et al. (2016)). Let A ∈ Rd×n,
and let T, S ⊂ [n] be two sets of column indices, with k := |S| and Φa(S) ≥ Φa(T). Let T be a set of
n log[1/’) column indices of A, chosen uniformly at random from [n] \ T. Then,
E[maxΦa(T ∪ {i})] — Φa(T) ≥ (1 — δ) ∙ σmin(Bs)2 ∙
i∈T
(Φa(S) - Φa(T))3
16kΦA(S )2
24
Under review as a conference paper at ICLR 2021
Proof. The proof is nearly identical to the proof of Lemma 6 of Altschuler et al. (2016) — We include the full
proof for completeness. The first step in the proof is showing that T ∩ (S \ T) is nonempty with high probability.
Then, by conditioning on T ∩ (S \ T) being nonempty, We can show that the expected increase in utility is large.
For the purpose of this analysis, We assume that the columns of T are sampled independently with replacement.
At the end of the proof, We discuss sampling the columns of T without replacement.
First, observe that
O ( n IOg(12 )
Pr[T ∩ (S \ T) = 0]= Y (l-P^T!)
t=1	n - |T|
1-
|S \ TI
n -IT I
O ( n lo%1∕δ)
|S\T | n log(1∕δ)
≤ e- n-|T I	k
∣s∖τ| iog(i∕δ)
≤ e k
By 1 - x ≤ e-x
Because n — IT I < n
meaning that
Pr[T ∩ (S \ T)] ≥
∣s∖τ∣ iog(i∕δ)
1 — e k
,∣s∖τI
1 — δ k
≥ (1 — δ)IS \TI Since ∣S \ T∣ ≤ k, and 1 — δx ≥ (1 — δ)x for x,δ ∈ [0,1]
Therefore,
E[maxΦA(T ∪{i}) — Φa(T)]
i∈T
≥ Pr[T∩ (S \ T) = 0] ∙ EhmaxΦA(T ∪ {i}) — Φa(T)∣T∩ (S \ T) = θ]
≥ (1 — δ) JSkTI ∙ EhmaχΦA(T ∪{i}) — Φa(T)∣T ∩ (S \ T) = °]
≥ (1 — δ)JSkɪɪ ∙ EhmaχΦA(T ∪{i}) — Φa(T)∣IT ∩ (S \ T)I = 1]
(Since it is always better for T ∩ (S \ T) to be larger)
λi K、IS \ TI ∑i∈s∖τ(Φa(T ∪{i}) — Φa(T))
(1 - δ) ~	IS\TI
门 4 Pi∈s(Φa(T ∪{i}) — Φa(T))
(I- δ)------------
(Since ΦA(T ∪ {i}) = ΦA (T) for i ∈ T)
≥ (1 —
δ) ∙R
• σmin (BS)
(Φa(S ) — Φa(T ))2
16Φa(S)2
(See the proof of Lemma 7.3.)
This proves the lemma in the case where the columns are sampled with replacement. Now, we discuss what
happens when sampling without replacement. Note that the expected increase in utility can only be higher if the
columns of T are sampled without replacement. Intuitively, this is because if T has some repeated columns,
then it is always better to replace those repeated columns with other columns of A. Thus, for each instance of T
where some columns are sampled multiple times, we can “move” all of the probability mass from this instance
of T to other sets T ⊂ [n] \ T, which contain T but do not have repeated elements. This leads to the uniform
distribution on subsets of [n] \ T with no repeated elements, i.e., the distribution that results from sampling
without replacement.	□
Using this lemma, we analyze the convergence of Algorithm 2:
Theorem 8.	Let A ∈ Rd×n be the data matrix and k ∈ N the desired rank. Let AS be the best subset of k
columns, i.e., AS = arg minA minV IASV — AI1,2. Let σ be the minimum non-zero singular value of the
matrix B of normalized columns of AS (meaning the j-th column of B is Bj = (AS )*j∕I(As )*jI2). Then, if
T ⊂ [n] is the subset of columns selected by Algorithm 2, the following holds if IT I = Ω( σ2k^2):
E[min IATV — AI1,2] ≤ (1 — )	min	IASV — AI1,2 + IAI1,2
V	)S⊂[n],∣S∣=k,V∈Rk×n	,	,
25
Under review as a conference paper at ICLR 2021
Proof. The proof is uses the same strategy as that of Theorem 5 of Altschuler et al. (2016) (and Theorem 7
above), with minor modifications. Let Tt be the subset of columns of B selected by Algorithm 2 after t iterations
(in particular, To = 0). In addition, let F = Φa(S) = Φa(S) — Φa(To), ∆o = F, and ∆i+ι = ∆i. Now,
fix a time t such that for some i, ∆i ≥ Φa(S) 一 Φa(Tt) ≥ ∆i+ι = ∆i∙ Then, We bound the number of
additional iterations t0 needed so that
E[ΦA(S) — ΦA(Tt+t0) | Tt] < ∆i+1
For convenience, for each k ≥ 0, define Ek := ΦA (Tt+k). Then, our goal is to find t0 such that
ΦA(S) — Et0 < △i+1
However, observe that from Lemma 7.4 above, we obtain
E
Ek+1 — Ek
E
hΦA(Tt+k+1) — ΦA(Tt+k)Tti
hE ΦA (Tt+k+1) — ΦA(Tt+k)Tt+k]Tti
ByE[E[X|Y]] =E[X]
E h(1 — δ) • σmin (BS)2 •
16kΦA (S)2
(ΦA (S) — ΦA (Tt+
By Lemma 7.4
(1 — δ) • σmin (Bs)2
16kΦA (S)2
(1 — δ) • σmin (BS)2
• E[(Φa(S) — ΦA(Tt+k))3∣Tt ]
16kΦA (S)2
(1 — δ) • σmin (BS) •
• (e[Φa(S) — ΦA(Tt+k )∣Tt])3
(E[ΦA(S)] — Ek)3
By Jensen’s Inequality
16kΦA (S)2
Now, suppose that ∆i ≥ ΦA(S) — Es ≥ ∆i+1, for s = 0, . .
(I — ^σmin(BS )243+i
16kF 2
. Summing these inequalities for s = 0, . .
. , t0 — 1. Then, for all such s, Es+1 — Es
. , t0 — 1, we find that
Et0 — E0 ≥
(1 — δ)στmin(Bs )2
16kF 2
• ∆i3+1 • t0
and for the increase from E0 to Et0 to be greater than ∆i+1, it suffices to have
t0 ≥
32kF 2
△2+1 ∙(1 — δ)σmin(Bs )2
In summary, if ΦA (S) — E[ΦA (Tt)] ≤
E[ΦA (Tt)] ≤ △i+1. Thus, if we let N
△i, then in at most ∆
∈ N such that △N+1
.i+1∙(1—δkFm in(Bs )2 iterations, φ A(S)
iterations t needed to have ΦA (S) — E[ΦA (Tt)] < △N+1 is at most
≤ —≡— F
≤ √1-δ F
≤ ∆N, then the number of
N
X
i=0
32kF 2
△2+1 ∙(1 — δ)σmin (BS )2
≤
	32k F2	N1 X ʌ^ i=0 △i+1	
(1	—δ)σmin (Bs)2		
	32k F2	N1 〉]4N — i i=0	1
(1	—δ)σmin (Bs )2		△N+i
	32kF 2	4(1 — δ)	1 4N 4N-i i=0
(1	—δ)θmin(Bs )2	•	e2F2	
Since △N+1
N
128k	X^ 1
σmin(Bs )%2 = 4i
512k
3σmin(Bs )2e2
≥ 2√T-δ
≥
≥
≥
—
F
Thus,after t = o( σm⅛
22) iterations,
ΦA (S) — E[ΦA (Tt)] ≤
√⅛ φA (S)
≤
meaning
∣A∣1,2 — |A — ∏SA∣1,2 — E[∣A∣1,2 — |A — ∏TA∣1,2] ≤ :	∣A∣1,2 —
√1 — δ
√⅛ |A — ∏s A∣1,2
√1 — δ
and rearranging gives
E[|A — ∏t A∣1,2] ≤ (1 - -ʌɪ=) |A — ∏s A∣1,2 +
∖	√1 — δ)
√1≡J
|A|1,2
This completes the proof (note that We can select δ = e, meaning √- = O(I) for e < ∣).

□
26
Under review as a conference paper at ICLR 2021
E	Extension to the Streaming Model
In this section, we describe how our protocol in Algorithm 1 can be made into a 1-pass streaming algorithm for
column subset selection in the `p norm. The algorithm is shown in Algorithm 7, and is analyzed in Theorem 9
below. The algorithm and its analysis follow the standard merge-and-reduce framework (see McGregor (2014)).
Theorem 9 (Analysis of Algorithm 7). Let A ∈ Rd×n and k ∈ N, and assume Algorithm 7 sees the columns of
e
A one at a time in the stream S. Then, Algorithm 7 returns U ∈ Rd×O(k) such that
min kUV - Akp ≤ Oe(k1/p-1/2) min kAk - Akp
V ∈ROe(k)×n	Ak rank k
■ , J	7 1 ∙1 ∙ . ∕~, i~, Λ ʃ	.1	1 ■ , ∕∙ 4 J	■ , 1	-! ∙ P∖ / 7 7 ∖
with probability 0.9. Moreover, the space complexity of Algorithm 7 is O(dk).
Proof. Let r = O(k) be the bi-criteria rank of Algorithm 4. Then, we will be repeatedly applying Lemma 4
with k being equal to r, i.e., we will create coresets which preserve the errors when projecting onto all subspaces
of dimension r.
Now, at every iteration of Algorithm 7, each element (L, t) of L can be thought of as holding a coreset of the
columns of A in some interval I in [n]. We prove the following intermediate lemma by induction on t:
Lemma 10. Let B be the concatenation of all the sketched columns in L (which have been reweighted by
e
multiple applications of Lemma 4). Then, for all subspaces V ⊂ RO(k) of dimension at most r,
kB - PV B kp,2 =
1±
t
kSAI - PV SAI kp,2
Proof. We proceed by induction on t. The lemma is clear when t = 0, since in that case, the columns of B are
simply sketched columns of A which have not been re-weighted.
Now, suppose t > 0, and suppose the lemma holds for smaller values of t. Then, the sketched columns in L must
have been obtained as follows: there previously existed two elements (L1, t - 1) and (L2, t - 1) of L, such that
if B1 is the concatenation of the sketched columns in L1 and B2 is the concatenation of the sketched columns in
L2, and T is a coreset for the concatenation B3 of B1 and B2, then B = B3T . The sketched columns in L1
and L2 form coresets for two intervals in [n], which we denote by I1 and I2 respectively. Applying Lemma 4,
we find that
kB - PVBkp,2 = (1 ± 高)pkB3 - PVB3 kp,2
=(1 ± IoIn)p(kB1 — PVBlkp,2 + kB2 — PVB2kp,2)
=(1 ± lo∣n)p ∙ (1 ± Iogn)p(i (kSAiι — PVSAIIkp,2 + kSAi2 — PVSAi2 kp,2)
=(1 ± 嵩)pt∙kSAI- PV SAI kp,2
(1)
where the second and last equalities are because the pth power of the `p,2 norm of a matrix decomposes across
the columns of the matrix, and the third equality is by the induction hypothesis. By taking pth roots, we find that
kB—PVBkpp,2=
1±
t
kSAI—PVSAIkpp,2
This proves the lemma.
□
Now, if L is the unique element of L remaining at the end of the for loop in Algorithm 7, let B be the
concatenation of the sketched columns in L. Then, L is a coreset for all the columns of A, and by the above
lemma, the distortion of L is at most (1 ± ioɪn )log n ∈ [e ,e] (since t ≤ log n — note that, as in all applications
of the merge-and-reduce framework, the coresets contained in L over the course of the algorithm form a binary
tree, with the leaf nodes being contiguous intervals of length O(k)). Hence, for all subspaces V of dimension at
most r,
kB — PVBkp,2 = Θ(1)kSA — PVSAkp,2
ee
and in particular, if M ∈ RO(k)×O(k) is the matrix formed by concatenating the columns of B selected by
running Algorithm 4, then
kSA — PMSAkp,2 ≤ Θ(1) min kSA—PTSAkp,2
T ⊂[n],∣T ∣≤k
27
Under review as a conference paper at ICLR 2021
where PM on the left-hand side denotes the projection onto the column span of M, and PT on the right-hand
side denotes the projection to the column span of SAT. Hence, by Lemma 1,
kSA - PMSAkp ≤ Oe(k1/p-1/2)	min	kSA - PTSAkp
T⊂[n],∣T∣≤k
e
meaning that if U ∈ Rd×O(k) is the matrix whose columns are the unsketched columns corresponding to M,
then
min kSA-SUVkp ≤	min	kSA-SATVkp
V ∈Rθ(k)×d	T ⊂[n],∣T ∣≤k,V ∈Rk×d
and by Lemmas 2 and 3, this means
min kA - UV kp ≤	min	kA - ATV kp
V∈R<e(k) ×d	T⊂ [n] ,∣T∣≤k,V∈Rk × d
Now, we analyze the space complexity of Algorithm 7. Note that at any iteration of the algorithm, L can hold at
most log n lists of columns (since each element of L is a coreset corresponding to an interval of column indices
in [n] of size 2k for some k ∈ N, and if two adjacent coresets are of the same size then they will have been
merged, so all coresets in L are of different sizes). Each list of columns is of size at most O(k), and each column
has d entries, meaning the amount of space used in any iteration is at most O (dk).	□
F Comparison of Our Protocol with the Distributed Greedy
Protocol of Altschuler et al. (2016) for the Frobenius Norm
In this section, we perform an empirical comparison of our protocol with the distributed greedy protocol for
column subset selection in the Frobenius norm due to Altschuler et al. (2016).
F.1 Distributed Greedy Protocol of Altschuler et al. (2016)
We first recall the distributed protocol of Altschuler et al. (2016), in Algorithm 8. Note that it is a bi-criteria
algorithm, i.e., more than k columns are selected, and in Altschuler et al. (2016) it is shown that this gives a
good approximation relative to the optimal column subset for the Frobenius norm.
Naively, this would require a large communication cost, since the entire data matrix A would have to be com-
municated between the servers and the coordinator, in order for them to perform the calls Greedy (A, Ti, 3σk)
and Greedy (A, T, 12k). Instead, Altschuler et al. (2016) uses the technique of Projection-Cost Preserving
sketches, developed in Cohen et al. (2015), in which a key result is as follows:
Theorem 11 (Projection-Cost Preserving Sketches - Theorem 4 of Cohen et al. (2015), as stated in Altschuler
et al. (2016)). Let R be a random matrix with n rows and n0 = O( k+lθg(δ)) columns, where each entry is
independently and uniformly set to ±ʌʃ/ɪ∙ Thenfor any matrix A ∈ Rd×n, with probability 1 — O(δ), the
following holds: for some constant c ≥ 0, for any k-dimensional subspace U of Rd, if PU ∈ Rd×d is the
corresponding projection matrix, then
(1 — )|PU A|22 ≤ |PU (AR)|22 + c ≤ (1 + )|PU A|22
that is, AR is a Projection-Cost Preserving sketch for A. In other words, R preserves the cost of all projections
of A onto rank-k subspaces of Rd.
Hence, when each server performs the call Greedy (A, Ti, 3σk), in place of A, it can use a projection-cost pre-
serving sketch AR for A (and the coordinator can similarly use AR when it makes the call GREEDY(A, T, 1σk)).
This is because the calls to GREEDY repeatedly compute the cost of the projection of the matrix A onto various
column subsets of Ti or T , and hence all that is needed is a way to efficiently compute the cost of the projection
of A onto various subsets of size O(σ).
AR can be computed without communicating the entire matrix A between the servers, as follows:
•	First, the coordinator generates R ∈ Rn×n0 (where n0 = O(k)) as specified in Theorem 11. This is
sent to all the servers (which can be done with negligible cost if a random seed is sent, for example).
•	Suppose the ith server holds Ai , which consists of columns with indices si through ti of A. Then, if
Ri is the (ti — si+1) × n0 submatrix ofR consisting of rows si through ti, then the ith server can send
AiRi to the coordinator. Since AiRi is a d × O(k) matrix, this step takes O(sdk) communication.
• Finally, by definition of matrix multiplication, AR = Pis=1 AiRi . The coordinator performs this
computation and sends AR to each server. This step also takes O(sdk) communication.
28
Under review as a conference paper at ICLR 2021
Algorithm 7 1-pass streaming algorithm for k-CSSp. L is a collection of coresets of columns. Over
the course of the algorithm, each element L ∈ L will represent a contiguous subset of 2t columns,
for some t ∈ [log n] —— to determine when two coresets should be merged, we will also keep track
of the size of each coreset in L. Hence, each element of L is of the form (L, t) where L is a list of
sketched columns (and their corresponding unsketched columns) and t is the number of times this list
has been involved in a merging operation.
Input: A stream S in which the columns of the data matrix A ∈ Rd×n arrive one at a time, and
the target rank k ∈ N
Output: The left factor U ∈ Rd×O(k)
S J An O(k) × d random matrix with i.i.d. standard p-stable entries
e J ιo1n
δ J O(1)
n
f J Oe(k)
LJ 0
for Each column A*j that arrives from S do
if L is empty. then
L J {}, where {} is the empty list.
else if The last element (J, t) of L is such that t = 0 (i.e., it has been merged 0 times). then
LJJ
Remove (J, t) from the end of L.
else
LJ{}
end if
L J L ∪{(SA*j,A*j)}
LJL∪(L,0)
/* Now, we merge coresets in L as much as possible. */
while True do
Exit this while loop if L has only 1 element.
(L, t) J last element of L ——remove this from L.
(L0, t0) J last element of L ——remove this from L.
Ift 6= t0, then add (L0, t0) back to L, and add (L, t) back to L. Then, exit this while loop.
C J A coreset of L ∪ L0, computed as specified in Lemma 4 —— the k in the statement of
Lemma 4 will be the bicriteria rank of Algorithm 4, which is O(k). The parameters δ and
will be as specified at the beginning of this algorithm. (Only compute the coreset of the
sketched columns —— for those which are included in the coreset, include the corresponding
unsketched columns as well (but without re-weighting them)).
L J (C, t + 1) —— note that for each of the re-scaled columns that are included in C, we
include their original indices in A as well.
end while
end for
L J The unique element of L
L0 J The result of running Algorithm 4 on the sketched columns in L —— for each of these sketched
columns, store the corresponding unsketched column as well.
U J The d × O(k) matrix whose columns are the unsketched columns in L0
return U
29
Under review as a conference paper at ICLR 2021
Algorithm 8 Distributed Greedy Column Subset Selection for the Frobenius Norm (Algorithm 2
of Altschuler et al. (2016)). Throughout this protocol, GREEDY(A, T, r) denotes a single-machine
procedure, which does the following: if A ∈ Rd×n is a data matrix, and T ∈ Rd×t is a set of columns
(not necessarily of A) then a subset S of r columns of T is constructed iteratively over r steps, such
that at each step, the new column of T to add to S is greedily chosen — that is, the chosen column
increases ∣∏sA|2 the most, or equivalently, decreases |A - ∏sA|2 the most. In other words, it is the
same as our Algorithm ??, but for the Frobenius norm rather than the '1,2-norm.
Input: The data matrix A ∈ Rd×n , target rank k ∈ N, the number of servers s ∈ N. The columns
ofA are assumed to be randomly partitioned among servers T1, T2, . . . , Ts.
Output: A subset of columns AT from A. |T| = O( k), where if AOPT is the optimal subset of
columns of A of size k, and the columns of AOPT are normalized to have unit `2 norm, then σ is
the smallest singular value of AOPT .
Si — GREEDY(A, Ti, 3σk) for all i ∈ [s] (The ith server performs this computation.)
Each server sends its Si to the coordinator.
T J ∪S=1Si (This computation is done by the coordinator.)
S j Greedy(A, t, 1σk)
ThecOOrdinatOrretUmS argmaχso∈{S,S1 ,S2,...,Ss} l∏soA|2
In our implementation of Algorithm 8, we make use of Projection-Cost Preserving sketches (PCPs). Another
optimization described in Altschuler et al. (2016), which we use in our implementation, is the Lazier-
THAN-LAZY-GREEDY ALGORITHM. The difference between the GREEDY(A, T, r) algorithm and the
LAZIER-THAN-LAZY-GREEDY(A, T, r) algorithm is as follows: while at each of the r iterations, GREEDY
considers all columns of T and chooses the one that leads to the most improvement in the objective, LAZIER-
IT I log(1)
THAN-LAZY-GREEDY samples r 'δ columns uniformly at random for some small δ (which we take in our
implementation to be 0.005), and out of those columns, chooses the one which leads to the greatest improvement
(where |T | is the number of columns in T). This leads to a significant speedup to GREEDY, and it was shown in
Altschuler et al. (2016) that this does not significantly worsen the approximation guarantees that Altschuler et al.
(2016) shows for Greedy.
F.2 Setup
We compare our distributed protocol, in the case p = 1, to Algorithm 8. For both protocols, at the outset we fix
the number of columns which are selected. Algorithm 8 is rewritten as Algorithm 9 to reflect this. In this section,
we use k to denote the number of columns ultimately selected.
Algorithm 9 Distributed Greedy Column Subset Selection for the Frobenius Norm (Algorithm 2 of
Altschuler et al. (2016)). For our empirical comparison, we fix the number of columns selected at the
outset._____________________________________________________________________________________
Input: The data matrix A ∈ Rd×n, desired number of columns k ∈ N, the number of servers
s ∈ N. The columns of A are assumed to be randomly partitioned among servers T1, T2, . . . , Ts.
Output: A subset of columns AT from A, with |T| = k.
AR J a PCP for A as discussed in the previous section. All servers and the coordinator have
access to AR.
Si J GREEDY(AR, Ti, k) for all i ∈ [s] (The ith server performs this computation.)
Each server sends its Si to the coordinator.
T J ∪is=1Si (This computation is done by the coordinator.)
S J GREEDY(AR, T, k)
TheCOOrdinatOrretUmS argmaxso∈{S,S1 ,S2,...,Ss} |ns，(AR)|2
Note that in Algorithm 9, the coordinator now chooses as many columns as chosen by the servers, as opposed
to Algorithm 8, where it chooses a somewhat smaller number of columns — note that this cannot harm the
performance of the algorithm.
We compare our protocol to Algorithm 9, using the following datasets:
•	gastro_lesions, a 76 × 698 matrix dataset available at https://archive.ics.uci.edu/
ml/datasets/Gastrointestinal+Lesions+in+Regular+Colonoscopy.
30
Under review as a conference paper at ICLR 2021
•	secom, a 591 × 1567 matrix dataset. secom has missing entries, which we replace with 0s for the
purposes of our experiments. secom is available at https://archive.ics.uci.edu/ml/
datasets/SECOM.
F.2.1	Parameters
We compare our distributed protocol for k-CSS1, using Greedy k-CSS1,2 as a subroutine, with Algorithm
9, for several choices of k — on gastro_lesions, We let k ∈ {10, 20, 30}, while on secom, We let
k ∈ {30, 60, 90,120}. For our protocol, Cauchy_size is set to 2k and Coreset-Size is set to 5k for
both datasets (where cauchy_size and coreset_size have the same meanings as in the main body of this
paper.) For Algorithm 9, the number of columns in the PCPS is set to 8k on gastro_lesions, and 7k on
secom.
The hyperparameters coreset_size and cauchy_size, and the number of columns in the PCPs, are set
this way so that the amount of communication that each algorithm is allowed is roughly equal (Algorithm 9 is
allowed slightly more communication). To see this, let d be the number of rows in the data matrix A. If c is the
number of columns in the PCPs, then the total communication used to transmit the PCPs between servers is 2scd,
since the servers must first send their respective AiRi to the coordinator, and the coordinator then sends AR to
all of the servers. By comparison, if r1 is the number of rows in the initial Cauchy matrix in our protocol, and r2
is the number of columns in each coreset sent by the servers to the coordinator, then the total communication
required to transmit the Cauchy matrix and the coresets is (r1 + r2)sd. We choose r1 , r2 and c so that 2scd is
slightly higher than (r1 + r2)sd.
Note that if k is the number of columns ultimately selected, then our protocol uses 2dk additional bits of
communication between the servers and the coordinator to recover the final k-subset of columns, while Algorithm
9 will use sdk communication to send the subsets of columns Si (of size k) from the servers to the coordinators.
This is not included in our hyperparameter calculations.
F.2.2 How Trials are Conducted
With these choices of hyperparameters, we conduct 15 trials as follows. In each trial, if A ∈ Rd×n is our data
matrix, then we shuffle the columns and then divide them equally between 2 servers (since for the theoretical
guarantees of Algorithms 8 9 to apply, the columns should be partitioned randomly). Using this partition across
2 servers, we run our protocol and Algorithm 9. For each protocol, once the subset of columns is computed, we
perform multiple-response `1 regression to evaluate the error.
In the next section, we report the minimum error across the 15 trials for each dataset and for each value of k. We
also report the mean error, along with the standard deviation. Finally, we compute the work and the span of
our protocol and Algorithm 9 using Python,s time.Process_time() utility. This does not include the time
taken to perform multiple-response `1 regression.
For secom, with k = 30 and k = 60, the trials were performed on a Late-2016 Macbook Pro with a 2.7 GHz
Quad-Core Intel Core i7 processor and 16GM of memory. The rest were performed on a 2019 MacBook Pro
with a 2.4 GHz Intel Core i5 processor and 8 GB 2133 MHz LPDDR3 memory.
F.3	Results
Our results are shown and discussed in Figures 3 (minimum and mean/standard deviation for `1 error) and 4
(average work and span across 15 trials).
G Full Experimental Results
For our protocol, we considered various additional hyperparameter settings on real-world datasets (bcsstk13,
isolet and 5 images in the Caltech-101 dataset) — these settings are shown in Tables 3, 4, and 5. As
before, cauchy size is the number of rows in our initial Cauchy matrix, sent to the servers at the beginning
of the protocol, and coreset size is the size of the strong coreset sent by each server to the coordinator. For
regular k-CSS1,2, sketch size is the number of rows in the sparse embedding matrix, and sparsity is
the number of nonzero entries in each column of the sparse embedding matrix.
For each of these settings, we display the minimum error, as well as the mean error and the standard deviation
for each setting and each rank, as shown in Figure 5. We also display the average work/span of each setting,
for each rank, as shown in Figure 6. We observe that not only does greedy k-CSS1,2 perform better than all
settings of regular k-CSS1,2 in minimum and mean errors across multiple trials, it also has a smaller variance in
performance.
G. 1 Details of Work/Span Computation
In Figure 6, work and span were recorded as the time taken (using Python,s time.Process_time() utility)
to compute the column subset in our distributed protocol. In particular, it does not include the time spent
performing `1 regression to obtain the right factor and consequently the entry-wise `1 errors.
31
Under review as a conference paper at ICLR 2021
Figure 3: Plots (a) and (C) show minimum '1 error across 15 trials on gastro_lesions and
secom respectively, while plots (b) and (d) show the mean `1 errors and the standard deviation. For
ranks 90 and 120, our protocol affords a 3% improvement in minimum error secom, while for rank
30 on gastro-lesions, our protocol gives over 40% improvement. Note that the mean error for
both protocols is noticeably higher on gastro_lesions — in the case of our protocol, the error in
the case k = 30 is distorted by two trials with `1 errors 205827 and 134945 respectively.
Setting Number	1	2	3	4	5	6	7	8	9	10
sketch size sparsity	3k min(20,3k)	5k min(20,5k)	3k min(40,3k)	5k min(40,5k)	k/2 min(2,k∕2)	k/3 min(2,k∕3)	k/5 min(2, k/5)	k/2 min(5, k/2)	k/3 min(5, k/3)	k/5 min(5, k/5)
Table 3: All hyperparameters used on bcsstk13, when regular k-CSS1,2 is used. In this table, k
denotes the number of columns ultimately selected. In all settings (including when greedy k-CSS1,2,
not included here, is used), cauchy size is either 5k or 8k, and coreset size is 5k. The
setting numbers are shown on top — in the following error plots, Setting 0 is used to refer to our
protocol when greedy k-CSS1,2 is used. Note that sparsity can be at most sketch size, since
sketch size is the number of rows of the sparse embedding matrix, while sparsity is the
number of nonzero entries in any column. (There is a slight typo in Table 2 in Section 8 of our main
paper, where coreset size is given as 10k.)
Setting Number	1	2	3	4	5	6	7	8	9	10
sketch size sparsity	3k min(20,3k)	5k min(20,5k)	3k min(40,3k)	5k min(40,5k)	k/2 min(2,k/2)	k/3 min(2,k/3)	k/5 min(2, k/5)	k/2 min(5, k/2)	k/3 min(5, k/3)	k/5 min(5, k/5)
Table 4: All hyperparameters used on isolet, when regular k-CSS1,2 is used. In all settings
(including when greedy k-CSS1,2, not included here, is used), cauchy size is 4k, and coreset
size is 4k. The setting numbers are shown on top — in the following error plots, Setting 0 is used
to refer to our protocol when greedy k-CSS1,2 is used.
32
Under review as a conference paper at ICLR 2021
Figure 4: Plots (a) and (b) show the average work and span respectively for gastro_lesions,
while plots (c) and (d) show the average work and span for secom. As expected, our protocol using
GREEDY k-CSS1,2 takes more time — to our knowledge, there is not yet an optimization similar to
LAZIER-THAN-LAZY GREEDY for the '1,2-norm. Nevertheless, running time is less important than
communication in the distributed setting.
33
Under review as a conference paper at ICLR 2021
(a) bcsstk13: min error
(b) bcsstk13: mean & std error
Isolet
Isolet
ie6
(d) isolet: mean & std error
(c) isolet: min error
o.o j_
10	20	30	40
Rank
(e) caltech-101: min error
(f) caltech-101: mean & std error
Figure 5: Results on bcsstk13, isolet, and caltech-101 from top to bottom. The left plots
show minimum error across all 15 trails; the right plots show the corresponding mean and standard
deviation in error. In all plots, the first bar denotes SVD, the second bar denotes greedy k-CSS1,2,
and the rest of the bars denote all settings of regular k-CSS1,2, at all ranks on the axis 10 through 60.
34
Under review as a conference paper at ICLR 2021
Figure 6: Results on bcsstk13, isolet, and caltech-101 from top to bottom. The left plots
show average work in seconds across all 15 trails; the right plots show the corresponding average
span. In all plots, the first bar denotes greedy k-CSS1,2, and the rest of the bars denote all settings of
regular k-CSS1,2, at all ranks on the axis 10 through 60.
35
Under review as a conference paper at ICLR 2021
Setting Number	1	2	3	4	5
sketch size sparsity	k min(20, k)	k/3 min(2,k∕3)	k/5 min(2, k/5)	k/3 min(5, k/3)	k/5 min(5, k/5)
Table 5: All hyperparameters used on isolet, when regular k-CSS1,2 is used. In all settings
(including when greedy k-CSS1,2, not included here, is used), cauchy size is 4k, and coreset
size is 4k. The setting numbers are shown on top — in the following error plots, Setting 0 is used
to refer to our protocol when greedy k-CSS1,2 is used.
Figure 5 shows that we encounter a tradeoff between accuracy and running time when choosing between these
two subroutines. Both lead to the same overall communication cost, and since accuracy is of more interest than
running time in the distributed setting, it is (empirically) preferable to use Greedy k-CSS1,2 within the protocol.
G.2 Additional Details
All experiments on the Caltech-101 dataset were run on a Late-2016 Macbook Pro with a 2.7 GHz Quad-
Core Intel Core i7 processor and 16GM of memory. All experiments on bcsstk13 and isolet were run on
an AWS z1d.xlarge instance with Deep Learning AMI (Amazon Linux 2) Version 29.0.
36