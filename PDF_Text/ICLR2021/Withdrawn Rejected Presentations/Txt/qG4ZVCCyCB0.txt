Under review as a conference paper at ICLR 2021
How Important is the Train-Validation Split in
Meta-Learning?
Anonymous authors
Paper under double-blind review
Ab stract
Meta-learning aims to perform fast adaptation on a new task through learning a
“prior” from multiple existing tasks. A common practice in meta-learning is to
perform a train-validation split where the prior adapts to the task on one split of
the data, and the resulting predictor is evaluated on another split. Despite its preva-
lence, the importance of the train-validation split is not well understood either in
theory or in practice, particularly in comparison to the more direct non-splitting
method, which uses all the per-task data for both training and evaluation.
We provide a detailed theoretical study on whether and when the train-validation
split is helpful on the linear centroid meta-learning problem, in the asymptotic set-
ting where the number of tasks goes to infinity. We show that the splitting method
converges to the optimal prior as expected, whereas the non-splitting method does
not in general without structural assumptions on the data. In contrast, if the data
are generated from linear models (the realizable regime), we show that both the
splitting and non-splitting methods converge to the optimal prior. Further, per-
haps surprisingly, our main result shows that the non-splitting method achieves
a strictly better asymptotic excess risk under this data distribution, even when
the regularization parameter and split ratio are optimally tuned for both methods.
Our results highlight that data splitting may not always be preferable, especially
when the data is realizable by the model. We validate our theories by experimen-
tally showing that the non-splitting method can indeed outperform the splitting
method, on both simulations and real meta-learning tasks.
1 Introduction
Meta-learning, also known as “learning to learn”, has recently emerged as a powerful paradigm
for learning to adapt to unseen tasks (Schmidhuber, 1987). The high-level methodology in meta-
learning is akin to how human beings learn new skills, which is typically done by relating to certain
prior experience that makes the learning process easier. More concretely, meta-learning does not
train one model for each individual task, but rather learns a “prior” model from multiple existing
tasks so that it is able to quickly adapt to unseen new tasks. Meta-learning has been successfully
applied to many real problems, including few-shot image classification (Finn et al., 2017; Snell et al.,
2017), hyper-parameter optimization (Franceschi et al., 2018), low-resource machine translation (Gu
et al., 2018) and short event sequence modeling (Xie et al., 2019).
A common practice in meta-learning algorithms is to perform a sample splitting, where the data
within each task is divided into a training split which the prior uses to adapt to a task-specific
predictor, and a validation split on which we evaluate the performance of the task-specific pre-
dictor (Nichol et al., 2018; Rajeswaran et al., 2019; Fallah et al., 2020; Wang et al., 2020a). For
example, in a 5-way k-shot image classification task, standard meta-learning algorithms such as
MAML (Finn et al., 2017) use 5k examples within each task as training data, and use additional
examples (e.g. k images, one for each class) as validation data. This sample splitting is believed to
be crucial as it matches the evaluation criterion at meta-test time, where we perform adaptation on
training data from a new task but evaluate its performance on unseen data from the same task.
Despite the aformentioned importance, performing the train-validation split has a potential drawback
from the data efficiency perspective — Because of the split, neither the training nor the evaluation
stage is able to use all the available per-task data. In the few-shot image classification example,
1
Under review as a conference paper at ICLR 2021
each task has a total of 6k examples available, but the train-validation split forces us to use these
data separately in the two stages. Meanwhile, performing the train-validation split is also not the
only option in practice: there exist algorithms such as Reptile (Nichol & Schulman, 2018) and
Meta-MinibatchProx (Zhou et al., 2019) that can instead use all the per-task data for training the
task-specific predictor and also perform well empirically on benchmark tasks. These algorithms
modify the loss function in the outer loop so that the training loss no longer matches the meta-test
loss, but may have the advantage in terms of data efficiency for the overall problem of learning the
best prior. So far it is theoretically unclear how these two approaches (with/without train-validation
split) compare with each other, which motivates us to ask the following
Question: Is the train-validation split necessary and optimal in meta-learning?
In this paper, we perform a detailed theoretical study on the importance of the train-validation split.
We consider the linear centroid meta-learning problem (Denevi et al., 2018b), where for each task
we learn a linear predictor that is close to a common centroid in the inner loop, and find the best
centroid in the outer loop (see Section 2 for the detailed problem setup). This problem captures
the essence of meta-learning with non-linear models (such as neural networks) in practice, yet is
sufficiently simple that allows a precise theoretical characterization. We use a biased ridge solver
as the inner loop with a (tunable) regularization parameter, and compare two outer-loop algorithms
of either performing the train-validation split (the train-val method) or using all the per-task data
for both training and evaluation (the train-train method). Specifically, we compare the two methods
when the number of tasks T is large, and examine if and how fast they converge to the (properly
defined) best centroid at meta-test time. We summarize our contributions as follows:
•	On the linear centroid meta-learning problem, we show that the train-validation split is necessary
in the general agnostic setting: As T → ∞, the train-val method converges to the optimal centroid
for test-time adaptation, whereas the train-train method does not without further assumptions on the
tasks (Section 3). The convergence of the train-val method is expected since its (population) training
loss is equivalent to the meta-test time loss, whereas the non-convergence of the train-train method
is because these two losses are not equivalent in general.
•	Our main theoretical contribution is to show that the train-validation split is not necessary and
even non-optimal, in the perhaps more interesting regime when there are structural assumptions on
the tasks: When the data are generated from noiseless linear models, both the train-val and train-
train methods converge to the common best centroid, and the train-train method achieves a strictly
better (asymptotic) estimation error and test loss than the train-val method (Section 4). This is in
stark contrast with the agnostic case, and suggests that data efficiency may indeed be more important
when the tasks have a nice structure. Our results build on tools from random matrix theory in the
proportional regime, which may be of broader technical interest.
•	We perform meta-learning experiments on simulations and benchmark few-shot image classi-
fication tasks, showing that the train-train method consistently outperforms the train-val method
(Section 5 & Appendix D). This validates our theories and presents empirical evidence that sample-
splitting may not be crucial; methods that utilize the per-task data more efficiently may be preferred.
1.1 Related work
Meta-learning and representation learning theory Baxter (2000) provided the first theoretical
analysis of meta-learning via covering numbers, and Maurer et al. (2016) improved the analysis via
Gaussian complexity techniques. Another recent line of theoretical work analyzed gradient-based
meta-learning methods (Denevi et al., 2018a; Finn et al., 2019; Khodak et al., 2019; Ji et al., 2020)
and showed guarantees for convex losses by using tools from online convex optimization. Saunshi
et al. (2020) proved the success of Reptile in a one-dimensional subspace setting. Wang et al.
(2020b) compared the performance of train-train and train-val methods for learning the learning
rate. Denevi et al. (2018b) proposed the linear centroid model studied in this paper, and provided
generalization error bounds for train-val method; the bounds proved also hold for train-train method,
so are not sharp enough to compare the two algorithms. Wang et al. (2020a) studied the convergence
of gradient-based meta-learning by relating to the kernelized approximation. On the representation
learning end, Du et al. (2020); Tripuraneni et al. (2020a;b) showed that ERM can successfully pool
data across tasks to learn the representation. Yet the focus is on the accurate estimation of the
common representation, not on the fast adaptation of the learned prior. Lastly, we remark that there
2
Under review as a conference paper at ICLR 2021
are analyses for other representation learning schemes (McNamara & Balcan, 2017; Galanti et al.,
2016; Alquier et al., 2016).
Empirical understandings of meta-learning Raghu et al. (2020) investigated the representation
learning perspective of meta-learning and showed that MAML with a full finetuning inner loop
mostly learns the top-layer linear classifier and does not change the representation layers much.
This result partly justifies the validity our linear centroid meta-learning problem in which the fea-
tures (representations) are fixed and only a linear classifier is learned. Goldblum et al. (2020) investi-
gated the difference of the neural representations learned by classical training (supervised learning)
and meta-learning, and showed that the meta-learned representation is both better for downstream
adaptation and makes classes more separated than the classically trained one. Although the classical
training method in (Goldblum et al., 2020) does not perform a train-validation split, it is not exactly
the same as the train-train method considered in this work as it effectively performs a supervised
learning on all tasks combined and does not do a per-task adaptation.
Multi-task learning Multi-task learning also exploits structures and similarities across multiple
tasks. The earliest idea dates back to Caruana (1997); Thrun & Pratt (1998); Baxter (2000), initially
in connections to neural network models. They further motivated other approaches using kernel
methods (Evgeniou et al., 2005; Argyriou et al., 2007) and multivariate linear regression models
with structured sparsity (Liu et al., 2009; 2015). More recent advances on deep multi-task learning
focus on learning shared intermediate representations across tasks Ruder (2017). These multi-task
learning approaches usually minimize the joint empirical risk over all tasks, and the models for
different tasks are enforced to share a large amount of parameters. In contrast, meta-learning only
requires the models to share the same “prior”, which is more flexible than multi-task learning.
2 Preliminaries
In this paper, we consider the standard meta-learning setting, in which we observe data from T ≥ 1
supervised learning tasks, and the goal is to find a prior (or “initialization”) using the combined data,
such that the (T + 1)-th new task may be solved sample-efficiently using the prior.
Linear centroid meta-learning We instantiate our study on the linear centroid meta-learning
problem (also known as learning to learn around a common mean, Denevi et al. (2018b)), where we
wish to learn a task-specific linear predictor wt ∈ Rd in the inner loop for each task t, and learn a
“centroid” w0 in the outer loop that enables fast adaptation to wt within each task:
Find the best centroid w0 ∈ Rd for adapting to a linear predictor wt on each task t.
Formally, we assume that we observe training data from T ≥ 1 tasks, where for each task index t
we sample a task pt (a distribution over Rd × R) from some distribution of tasks Π, and observe n
examples (Xt , yt) ∈ Rn×d × Rn that are drawn i.i.d. from pt :
Pt 〜Π, (Xt, yt) = {(xt,i,yt,i)}n=ι where (xt,i,yt,i)吧 Pt∙
(1)
We do not make further assumptions on (n, d); in particular, we allow the underdetermined setting
n ≤ d, in which there exists (one or many) interpolators we t that perfectly fit the data: Xtwe t = yt.
Inner loop: Ridge solver with biased regularization towards the centroid Our goal in the inner
loop is to find a linear predictor wt that fits the data in task t while being close to the given “centroid”
w0 ∈ Rd. We instantiate this through ridge regression (i.e. linear regression with L2 regularization)
where the regularization biases wt towards the centroid. Formally, for any w0 ∈ Rd and any dataset
(X, y), we consider the algorithm
Aλ(wo; X, y) ：= arg min 1 kXW - y∣∣2 + λ ∣∣w - w0『=w° + (X>X + nλId) 1X> (y - Xwo),
wn
where λ > 0 is the regularization strength (typically a tunable hyper-parameter). As we regularize
by ∣w - w0 ∣2, this inner solver encourages the solution to be close to w0, as we desire. Such
a regularizer is widely used in practical meta-learning algorithms such as MetaOptNet (Lee et al.,
3
Under review as a conference paper at ICLR 2021
2019) and Meta-MinibatchProx (Zhou et al., 2019). In addition, as λ → 0, this solver recovers
gradient descent fine-tuning: we have
Ao(wo; X, y) := lim Aλ(wo; X, y) = wo + Xt(y - Xwo) = argminχw=y kw - wo∣∣2
λ→0	w=y
(where Xt ∈ Rd×n denotes the pseudo-inverse of X). This is the minimum-distance interpolator of
(X, y) and also the solution found by gradient descent 1 on kXw - yk2 initialized at w0. Therefore
our ridge solver with λ > 0 can be seen as a generalized version of the gradient descent solver used
in MAML (Finn et al., 2017).
Outer loop: Finding the best centroid In the outer loop, our goal is to find the best centroid w0 .
The standard approach in meta-learning is to perform a train-validation split, that is, (1) execute
the inner solver on a first split of the task-specific data, and (2) evaluate the loss on a second split,
yielding a function of w0 that we can optimize. This two-stage procedure can be written as
Compute wt(wo) = Aλ(wo; Xtrain, ytrain), and evaluate IIyval- XvalWt(Wo)『.
where (Xttrain,yttrain) = {(xt,i,yt,i)}in=11 and (Xtval,ytval) = {(xt,i,yt,i)}in=n1+1 are two disjoint
splits of the per-task data (Xt, yt) of size (n1, n2), with n1 + n2 = n. Written concisely, this is to
consider the “split loss”
12
't (w0):=诟 I∣yv - Xv Aλ(w0;Xtra yt )∣I .	⑵
In this paper, we will also consider an alternative version, where we do not perform the train-
validation split, but instead use all the per-task data for both training and evaluation. Mathemati-
cally, this is to look at the “non-split loss”
'tr-tr(wo) := ɪ kyt- XtAλ(wo;Xt,yt)k2.	⑶
2n
Our overall algorithm is to solve the empirical risk minimization (ERM) problem on the T observed
tasks, using either one of the two losses above:
1T	1T
LTva'(wo) := T E'tr-val(w0) and LT-tr(wo) := T ]Γ'tr-tr(wo),
T t=1	T t=1	(4)
wb{tr-val,tr-tr}	{tr-val,tr-tr}
:= arg min LT	(w0).
w0
Let L{tr-val,tr-tr}(w0) := Ept〜∏,(χt,yt)〜pt h'{tr-val,tr-tr}(w0)i be the population risks.
(Meta-)Test time The meta-test time performance of any meta-learning algorithm is a joint func-
tion of the (learned) centroid w° and the inner algorithm Alg. UPon receiving a new task PT +1 〜Π
and training data (XT +1 , yT+1 ) ∈ Rn×d × Rn, we run the inner loop Alg with prior w0 on the
training data, and evaluate it on an (unseen) test example (x0, y0)〜PT +i：
12
L	(w0； Alg)= EpT +ι~πE(χτ + ι,yτ +ι),(χ0,y0 芦 pτ + 1[2 (X Alg(W0； XT +1, yT+1) -y ).
Additionally, for both train-val and train-train methods, we need to ensure that the inner loop used
for meta-test is exactly the same as that used in meta-training. Therefore, the meta-test performance
for the train-val and train-train methods above should be evaluated as
Lλ,nι(w0rTal) := LteSt(W0座l; Aλ,nι),	L肾(w0r-Tr):= Ltest(W0Tr; Aλ,n),
where Aλ,m denotes the ridge solver with regularization strength λ > 0 on m ≤ n data points.
Finally, we let
W0,*(λ; n) = argmin L^(w0)	(5)
w0
denote the best centroid if the inner loop uses Aλ,n . The performance of the train-val algorithm
W0rTal should be compared against w0,*(λ, nι), whereas the train-train algorithm WOrTr should be
compared against w0,*(λ, n).
1with a small step-size, or gradient flow.
4
Under review as a conference paper at ICLR 2021
2.1	Task-abundant setting through asymptotic analysis
In this paper we are interested in the task-abundant setting where we fix some finite (d, n) and let T
be very large. We analyze such a task-abundant setting through the asymptotic analysis framework,
that is, examine the limiting properties of the estimator (e.g. wb 0{,tTr-val,tr-tr}) as T → ∞. Here we
set up the basic notation of asymptotic analysis required in this paper. We emphasize that our large
T setting captures practical meta-learning scenarios; for example, 5-way image classification on
miniImageNet (Ravi & Larochelle, 2017) contains 654 diverse tasks (at train time).
Asymptotic rate of estimation & excess risk Let L be any population risk with minimizer w0,?
(which we assume is unique), LbT be the empirical risk on the observed data from T tasks, and
wb 0,T be the minimizer of LT (i.e. the ERM). We say that wb 0,T is consistent if wb 0,T → w0,? in
probability as T → ∞. For consistent ERMs, we define its asymptotic parameter estimation error
(in MSE loss) and asymptotic excess risk as follows2:
AsymMSE(Wbo,τ):= Tlim T ∙ Ehkwbo,τ — wo,?『]
AsymExcessRisk(Wbo,T) := lim T ∙ E[L(Wo,T) 一 L(wo,?)].
, T→∞	,	,
We emphasize that asymptotic statements are more refined than non-aSymPtotic O(∙) style upper
bounds in the T → ∞ limit: they already imply the {MSE, excess risk} has order O(1/T) and
specifies the leading constant.
3	The importance of sample splitting
We begin by analyzing whether the algorithms wb 0{,tTr-val,tr-tr} defined in (4) converge to the best
test-time centroid w0,*(λ; nι) or w0,*(λ; n) (defined (5)) respectively as T → ∞, in the general
situation where we do not make structural assumptions on the data distribution pt .
Proposition 1 (Consistency and asymptotics of train-val method). Suppose Ex〜p∕xx>[ * 0,
Ex〜pt[∣∣xk4] < ∞ and E(x,y)〜？,[∣∣xyk] < ∞ for almost surely all Pt 〜 ∏. Thenfor any λ > 0
and any (n1 , n2 ) such that n1 + n2 = n, the train-val method wb t0r,-Tval converges to the best test-time
centroid: W0r,Tal → w0,*(λ, nι) almost surely as T → ∞. Further, we have
AsymMSE(W0r,Tal) = tr(V-2L*、(w0,?(λ,nι)) ∙ Cov(V'tr-val(wo,?(λ,nι))) ∙ V-2L比 (w°,?(λ,nι))),
AsymEXcessRiskL『nJw0r,Tal) = tr(V-2L^1 (w0,*(λ,nι)) ∙ Cov(V'tr-val(wo,*(λ, n0))).
Proposition 2 (Inconsistency of train-train method). There exists a distribution of tasks Π on d = 1
satisfying the conditions in Proposition 1 on which the train-train method does not converge to the
best test-time centroid: for any n ≥ 1 and any λ > 0, the estimation error ∣∣ηw0rTr — w0,*(λ, n)k
and the excess risk Ltλe,snt (wb t0r,-Ttr) 一 Ltλe,snt (w0,? (λ, n)) are both bounded away from 0 almost surely as
T → ∞.
Propositions 1 and 2 justify the importance of sample splitting: the train-val method converges to
the best test-time centroid, whereas the train-train method does not converge to the best centroid
in general. The reason behind Proposition 1 is simple: the population loss Ltr-val for the train-
val method is indeed equal to the test-time loss Ltλe,snt , making the train-val method a proper ERM
for the meta-test time we care about, and thus the consistency and asymptotic normality follow from
classical results for the ERM (e.g. (Van der Vaart, 2000; Liang, 2016)).
In contrast, for the train-train method, its expected loss Ltr-tr is in general not equivalent to Ltλe,snt :
Ltr-tr measures the in-sample prediction error of the per-task predictor, whereas Ltλe,snt measures the
out-of-sample prediction error. Consequently, the population minimizers of Ltλe,snt and Ltr-tr are not
equal in general, which leads to wb 0,? converging to the minimizer of Ltr-tr, not of Ltλe,snt. The proof
of Proposition 2 constructs a simple counter-example in d = 1, but we expect such a mismatch to
generally hold in any dimension. Appendix A gives the proofs of Proposition 1 and 2.
2These definitions assume that the expectation exists for finite T ; the more general definition can be found
in Appendix A.1.
5
Under review as a conference paper at ICLR 2021
4 Is sample splitting always optimal
Proposition 2 states a negative result for the train-train method, showing that it does not converge
to the best test-time centroid without further assumptions on the data distribution. However, such
a negative result is inherently worst-case, and does not preclude the possibility that there exists a
data distribution on which the train-train method can also work well. In this section, we construct
a simple data distribution in which we can analyze the performance of both the train-val and the
train-train methods more explicitly, showing that sample splitting is indeed not optimal, and the
train-train method can work better.
Realizable linear model We consider the following instantiation of the (generic) data distribution
assumption in (1): We assume that each task pt is specified by a wt ∈ Rd sampled from some
distribution Π (overloading notation), and the observed data follows the noiseless linear model with
ground truth parameter wt :
yt = Xtwt,	(6)
where the inputs xt,i 史 N (0, Id) and are independent of wh We assume that Π has a finite second
moment (i.e. Ewt~∏[∣∣wtk2] < ∞). Note that when n ≥ d, we are able to perfectly recover Wt for
all t (by solving linear equations), therefore the problem in the inner loop is in a sense “easy”; when
n < d, we cannot hope for such perfect recoveries. Our goal in the outer loop is to find the best w0,
measured by the test loss Ltλe,snt for the train-train method and Ltλe,snt for the train-val method.
4.1	Comparison of train-train and train-val on the realizable model
We begin by showing that on this task and data distribution, the population best centroids
wo,*(λ,n) = argmi`wo L^J(wo) is the same for any (λ,n), and both the train-val and train-
train methods are asymptotically consistent and converge to same best centroid.
Theorem 3 (Consistency of both train-val and train-train methods). On the realizable linear
model (6), the test-time meta loss for all λ > 0 and all n is minimized at the same point, that
is, the mean of the ground truth parameters:
wo,*(λ,n) = argminL^(wo) = wo,? ：= Ewt~∏[wJ forall λ > 0, n.
w0
Further, both the train-val method and the train-train method are asymptotically consistent: for any
λ > 0, n, and (n1, n2), we have
wbotr,-Tval(n1, n2; λ) → wo,? and wb otr,-Ttr (n; λ) → wo,? almost surely as T → ∞.
See its proof in Appendix B.1. Theorem 3 shows that both train-val and train-train methods are
consistent, and they converge to the same optimal parameter wo,? which is the mean ofwt. This is a
consequence of the good structure in our realizable linear model (6): at a high level, wo,? is indeed
the best centroid since it has (on average) the closest distance to a randomly sampled wt .
Theoerem 3 suggests that we are now able to compare performance of the two methods based on their
asymptotic parameter estimation error (for estimating wo,?). Throughout the rest of this section, let
R2 ：=E kwt -wo,?k2
(7)
denote the variance ofwt. We are now ready to state our main result.
Theorem 4 (Comparison of asymptotic MSE of the train-val and train-train methods). In the
high-dimensional limiting regime d, n → ∞, d/n → γ ∈ (0, ∞), the optimal rate of the train-
train method obtained by tuning the regularization λ ∈ (0, ∞) satisfies
(?)	5	5
inf limd,n→∞,d∕n=γ AsymMSE(W0rTr(n; λ)) = inf ρλ,γR2 ≤ max < 1 + —γ, — + γ } ∙ R2,
λ>o	λ>o	27 27
where ρλ,γ = 4γ2 [(γ - 1)2 + (γ + 1)λ] /(λ+γ+1 -P(λ + γ + 1)2 - 4γ)2/((λ + Y + 1)2 - 4γ)"2,
and the inequality becomes equality at γ = 1. In contrast, the optimal rate of the train-val method
by tuning the regularization λ ∈ (0, ∞) and split ratio s ∈ (0, 1) is
infλ>o,s∈(o,1) limd,n→∞,d∕n=γ AsymMSE wbotr,-Tval(ns,n(1 - s); λ) = (1 + γ)R2.
As max{1 + 5γ∕27, 5/27 + γ} < 1 + Y (∀γ > 0), the train-train method achieves a strictly better
asymptotic rate than the train-val method when λ and s are optimally tuned in both methods.
6
Under review as a conference paper at ICLR 2021
Implications Comparison between the analytical upper bound max{ 1 + 5γ∕27,5/27 + γ}R2 for
train-train (1 + γ)R2 for train-val in Theorem 4 shows that the train-train method achieves a strictly
better asymptotic MSE than the train-val method, for any γ > 03. (See Figure 1(a) for a visualization
of the exact optimal rates and the upper bound (?).) Perhaps surprisingly, this suggests that the train-
train method is not only “correct” (converging to the best centroid), but can be even better than the
train-val method, when the data is structured. While the “correctness” of the train-train method is
a consequence of the realizable linear model, we believe its superior asymptotic MSE is due to the
fact that the train-train method is able to use the data more efficiently than the train-val method.
Also, while we reached such a conclusion on this particular problem of linear centroid meta-learning,
we suspect that this phenomenon to be not restricted to this problem, and may hold in more general-
ity when data is structured or when the signal-to-noise ratio is high. As we are going to see, our real
data experiments in Section 5.2 indeed suggests that the superiority of the train-train method may
also hold on real meta-learning tasks with neural networks.
4.2	Proof highlights of Theorem 4
Here we sketch the technical highlights in proving Theorem 4. We defer the full proof to Ap-
pendix B.5.
Exact asymptotic MSE for both methods The proof begins by calculating the exact asymptotic
MSE for both methods, which we provide in the following theorem.
Lemma 5 (Exact asymptotic rates of the train-val and train-train methods). Suppose that ρtr-tr
EhPd=I。")2/。"+1)4]
(EhPd=I σ(n)∕(σ(n)+λ)2i)2
and ρtr-val
El(Pd=I λ2∕(σin1) +λ)2)2 + (n2 + l) Pd=ι λ4∕(σin1)+λ)4]
(EhPd=I λ2∕(σ(n1)+λ)2i)2
where for any n, σ(n) ≥ ∙∙∙ ≥ σdn) denotes the eigenvalues of the matrix ɪ X)Xt ∈ Rd×d,
where we recall Xt ∈ Rn×d is a random matrix with i.i.d. standard Gaussian entries. For any
(n, d), we have on the realizable linear model (6) that
AsymMSE(W册(n; λ)) = dR2ρtr-tr, AsymMSE(W0r,Tal(n1,n2; λ)) = dR?tr-val
See its proof in Appendix B.2. Lemma 5 follows straightforwardly from the classical asymptotic
result for empirical risk minimization (Van der Vaart, 2000) and simplifications of certain matrix
traces in terms of the spectrum of the empirical covariance matrix n Pn=I XtX>.
Simplifying and optimizing the asymptotic MSEs The asymptotic MSEs of the train-train and
train-val method in Lemma 5 are not yet directly comparable, as the quantities ρtr-tr and ρtr-val
depend on the spectrum of the empirical covariance matrix as well as additional tunable parameters
such as λ and (n1, n2) (for the train-val method). Towards proving Theorem 4, we further simplify
the rates and analyze the optimal tunable parameters, using separate strategies for the two methods:
•	For the train-val method, we show that the optimal tunable parameters for any (n, d) is taken at a
special case λ = ∞ and (nι, n2) = (0, n), at which the rates only depends on n^Xtrain>χtrain
through its rank (and thus has a simple closed-form). We state this result in Corollary 8. The
proof builds on algebraic manipulations of the quantity ρtr-val, and can be found in Appendix B.3.
•	For the train-train method, We apply random matrix theory to simplify the spectrum of n X> Xt
in the proportional limit where d, n → ∞ and d/n stays a constant (Bai & Silverstein, 2010;
Anderson et al., 2010), and obtain a closed-form expression of the asymptotic MSE for any λ > 0,
which we can analytically optimize over λ. We state this result in Theorem 9. The proof builds
on the Steiltjes transform and its “derivative trick” (Dobriban et al., 2018), and is deferred to
Appendix B.4.
3The same conclusion also holds for the asymptotic excess risk, as the Hessian of the excess risk is a rescaled
identity, see Appendix B.2.
7
Under review as a conference paper at ICLR 2021
5 Experiments
5.1	Simulations
We experiment on the realizable linear model studied in Section 4. Recall that the observed data of
the t-th task are generated as
Vt = XtWt,	with xt,i 吧 N(0,Id).
We independently generate Wt iid N(wo,?, Id/√d), where wo,? is the linear centroid and the cor-
responding R2 = 1 here. The goal is to learn the linear centroid W0,? using the train-train method
and train-val method, i.e., minimizing LtTr-tr and LtTr-val, respectively. Note that both LtTr-tr and LtTr-val
are quadratic in w0, therefore, we can find the close-form solutions wb 0{,tTr-tr,tr-val}. We measure the
performance of train-train and train-val methods using the '2-error ∣∣wo,? - W0tT-tr,tr-val} ∣∣2.
We present the comparison among train-train and train-val methods in Figure 1 with scatter plots
representing the simulation outputs under different settings. Across all the simulations, we well-tune
the regularization coefficient λ in the train-train method, and use a sufficiently large λ = 10000 in the
train-val method according to Corollary 8. The simulated results concentrate around the reference
curves corresponding to our theoretical findings. This corroborates our analyses and demonstrates
the better performance of train-train method on the realizable linear model.
We additionally investigate the effect of averaging the loss over multiple splits in the train-
val method (a “cross-validation” type loss) rather than the vanilla single split. We show that such
cross-validation can indeed improve over the vanilla single-split train-val method. We also experi-
ment with the stronger “leave-one-out” style cross-validation and show that it achieves better MSEs
than the constant-fold cross-validation (Appendix E).
-号，三M出笔)出SwXSV
0
0	0.5	1
1.5	2	2.5	3
d/n ratio
(a) Optimal asymptotic MSE of W{tT-tr, tr-val}
1010^10^
』.d^vI *0⅛=
200	400	600	800	1000
Number of tasks
(b) '2 error of W0tT-tr, tr-val} v.s. T
ɪɪ一 J
)	0.5	1	1.5	2	2.5	3
d/n ratio with fixed n
(C) '2 error of W0tT-tr, tr-val} v.s. d/n ratio
10^3
Figure 1: Panel (a) presents the optimal AsymMSE(wb 0tr,-Ttr) (blue) in Theorem 9 via grid search, and the
optimal AsymMSE(wb 0tr,-Tval) in Corollary 8 with n1 = 0 (orange) and n1 = 5 (green), as well as the upper
bound of AsymMSE(wb 0tr,-Ttr) (magenta) in Corollary 4. The optimal AsymMSE(wb 0{,tTr-tr,tr-val}) are used as
reference curves in plots (b) and (c). Panel (b) plots the '2-error of W0tT-tr,tr-val} as the total number of tasks
increases from 20 to 1000 with an increment of 20. We fix data dimension d = 60 and per-task sample size
n = 20. For the train-val method, we experiment on n1 = 0 and n1 = 5. Panel (c) shows the scaled (by T)
'2-error of W0tTtr,tr-val} as the ratio d/n varies from 0 to 3 (n = 20 and T = 1000 are fixed).
5.2	Few-shot image classification
We further investigate the comparison between the train-train and train-val type methods in few-
shot image classification on miniImageNet (Ravi & Larochelle, 2017) and tieredImageNet (Ren
et al., 2018).
Methods We instantiate the train-train and train-val method in the centroid meta-learning setting
with a ridge solver. The methods are almost exactly the same as in our theoretical setting in (2)
and (3), with the only differences being that the parameters wt (and hence w0) parametrize a deep
8
Under review as a conference paper at ICLR 2021
neural network instead of a linear classifier, and the loss function is the cross-entropy instead of
squared loss. Mathematically, we minimize the following two loss functions:
1	1T
Lλr-nal(w0):=下 ∑'tr-val(wo) = T ∑' argmin'w Xtrain, ytrain) + λ ∣Wt - w0『；X* yVal
T t=1	T t=1	wt
1T	1T
Lλr-tr(W0):= τE'tr-tr(W0)= tE' (argmin '(wt； X yt) + λ IlWt- w0『；X y),
where (Xt, yt) is the data for task t of size n, and (Xttrain, yttrain) and (Xtval , ytval) is a split of the
data of size (n1, n2). We note that both loss functions above have been considered in prior work
(Ltr-val in iMAML (Rajeswaran et al., 2019), and Ltr-tr in Meta-MinibatchProx (Zhou et al., 2019)),
though we use slightly different implementation details from these prior work to make sure that the
two methods here are exactly the same except for whether the split is used. Additional details about
the implementation can be found in Appendix D.
Experimental settings We adopt the episodic training procedure (Finn et al., 2017; Zhou et al.,
2019; Rajeswaran et al., 2019). In meta-test, we sample a set of N -way (K + 1)-shot test tasks. The
first K instances are for training and the remaining one is for testing. In meta-training, we use the
“higher way” training strategy. We set the default choice of the train-validation split ratio to be an
even split n1 = n2 = n/2 following Zhou et al. (2019); Rajeswaran et al. (2019). For example, for
a 5-way 5-shot classification setting, each task contains 5 × (5 + 1) = 30 total images, and we set
n1 = n2 = 15. (We additionally investigate the optimality of this split ratio in Appendix D.1.) We
evaluate both methods under the transduction setting where the information is shared between the
test data via batch normalization. We report the average accuracy over 2, 000 random test episodes
with 95% confidence interval.
Results Table 1 presents the percent classification accuracy on miniImagenet and tieredImageNet.
We find that the train-train method consistently outperforms the train-val method. Specifically, on
miniImageNet, train-train method outperforms train-val by 2.01% and 3.87% on the 1-shot 5-way
and 5-shot 5-way tasks respectively; On tieredImageNet, train-train on average improves by about
6.40% on the four testing cases. These results show the advantages of train-train method over train-
val and support our theoretical findings in Theorem 4.
Table 1: Few-shot classification accuracy (%) on the miniImageNet and tieredImageNet datasets.
τ miniImage	method	1-shot 5-way	5-shot 5-way	1-shot 20-way	5-shot 20-way
	train-val train-train	48.76 ± 0.87 50.77 ± 0.90	63.56 ± 0.95 67.43 ± 0.89	17.52 ± 0.49 21.17 ± 0.38	21.32 ± 0.54 34.30 ± 0.41
tieredImage	method	1-shot 5-way	5-shot 5-way	1-shot 10-way	5-shot 10-way
	train-val	50.61 ± 1.12	67.30 ± 0.98	29.18 ± 0.57	43.15 ± 0.72
	train-train	54.37 ± 0.93	71.45 ± 0.94	35.56 ± 0.60	54.50 ± 0.71
6 Conclusion
We study the importance of train-validation split on the linear-centroid meta-learning problem, and
show that the necessity and optimality of train-validation split depends greatly on whether the tasks
are structured: the sample splitting is necessary in general situations, and not necessary and non-
optimal when the tasks are nicely structured. It would be of interest to study whether a similar
conclusion holds on other meta-learning problems such as learning a representation, or whether our
insights can be used towards designing meta-learning algorithms with better empirical performance.
References
Pierre Alquier, The Tien Mai, and Massimiliano Pontil. Regret bounds for lifelong learning. arXiv
preprint arXiv:1610.08628, 2016.
9
Under review as a conference paper at ICLR 2021
Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random matrices, volume
118. Cambridge university press, 2010.
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Multi-task feature learning. In
Advances in neural information processing Systems, pp. 41-48, 2007.
Zhidong Bai and Jack W Silverstein. Spectral analysis of large dimensional random matrices,
volume 20. Springer, 2010.
Jonathan Baxter. A model of inductive bias learning. J. Artif. Int. Res., 2000.
Rich Caruana. Multitask learning. Machine Learning, 28(1):41-75, Jul 1997. ISSN 1573-0565. doi:
10.1023/A:1007379606734. URL https://doi.org/10.1023/A:1007379606734.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. arXiv preprint arXiv:1803.08089, 2018a.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around
a common mean. In Advances in Neural Information Processing Systems, pp. 10169-10179,
2018b.
Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics, 46(1):247-279, 2018.
Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
Theodoros Evgeniou, Charles A Micchelli, and Massimiliano Pontil. Learning multiple tasks with
kernel methods. Journal of machine learning research, 6(Apr):615-637, 2005.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-
based model-agnostic meta-learning algorithms. In International Conference on Artificial Intelli-
gence and Statistics, pp. 1082-1092, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126-1135, 2017.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
Proceedings of the 36th International Conference on Machine Learning, 2019.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimilano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. arXiv preprint
arXiv:1806.04910, 2018.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5(2):159-209, 2016.
Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Gold-
stein. Unraveling meta-learning: Understanding feature representations for few-shot tasks. arXiv
preprint arXiv:2002.06753, 2020.
Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-
resource neural machine translation. arXiv preprint arXiv:1808.08437, 2018.
Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-specific adaptation over partial parameters. arXiv preprint arXiv:2006.09486, 2020.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-
learning methods. arXiv preprint arXiv:1906.02717, 2019.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural
networks. pp. 1097-1105, 2012.
10
Under review as a conference paper at ICLR 2021
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp.10657-10665, 2019.
Percy Liang. Cs229t/stat231: Statistical learning theory (winter 2016), 2016.
Han Liu, Mark Palatucci, and Jian Zhang. Blockwise coordinate descent procedures for the multi-
task lasso, with applications to neural semantic basis discovery. In Proceedings of the 26th Annual
International Conference on Machine Learning, pp. 649-656, 2009.
Han Liu, Lie Wang, and Tuo Zhao. Calibrated multivariate regression with application to neural
semantic basis discovery. Journal of machine learning research: JMLR, 16:1579, 2015.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. The Journal of Machine Learning Research, 17(1):2853-2884, 2016.
Daniel McNamara and Maria-Florina Balcan. Risk bounds for transferring representations with and
without fine-tuning. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 2373-2381. JMLR. org, 2017.
A. Nichol and J. Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint
arXiv:1803.02999, 2, 2018.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of maml. In International Conference on Learning Rep-
resentations, 2020. URL https://openreview.net/forum?id=rkgMkCEtPB.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. In Advances in Neural Information Processing Systems, pp. 113-124, 2019.
S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. 2017.
M. Ren, E. Triantafillou, S. Ravi, J. Snell, K. Swersky, J. Tenenbaum, H. Larochelle, and R. Zemel.
Meta-learning for semi-supervised few-shot classification. arXiv preprint arXiv:1803.00676,
2018.
Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098, 2017.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
and M. Bernstein. Imagenet large scale visual recognition challenge. 115(3):211-252, 2015.
Nikunj Saunshi, Yi Zhang, Mikhail Khodak, and Sanjeev Arora. A sample complexity separation
between non-convex and convex meta-learning. arXiv preprint arXiv:2002.11172, 2020.
Jurgen Schmidhuber. Evolutionary principles in Seif-referential learning, or on learning how to
learn: the meta-meta-…hook. PhD thesis, Technische Universitat Munchen, 1987.
J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. pp. 4077-4087,
2017.
Sebastian Thrun and Lorien Pratt. Learning to Learn: Introduction and Overview, pp. 3-17.
SPringerUS,Boston,MA,1998.ISBN978-1-4615-5529-2. doi: 10.1007/978-1-4615-5529-2.1.
URL https://doi.org/10.1007/978-1-4615-5529-2_1.
Nilesh Tripuraneni, Chi Jin, and Michael I Jordan. Provable meta-learning of linear representations.
arXiv preprint arXiv:2002.11684, 2020a.
Nilesh Tripuraneni, Michael I Jordan, and Chi Jin. On the theory of transfer learning: The impor-
tance of task diversity. arXiv preprint arXiv:2006.11650, 2020b.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
11
Under review as a conference paper at ICLR 2021
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Haoxiang Wang, Ruoyu Sun, and Bo Li. Global convergence and induced kernels of gradient-based
meta-learning with neural nets. arXiv preprint arXiv:2006.14606, 2020a.
Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a
learning-to-learn approach. arXiv preprint arXiv:2006.16495, 2020b.
Yujia Xie, Haoming Jiang, Feng Liu, Tuo Zhao, and Hongyuan Zha. Meta learning with relational
information for short sequences. In Advances in Neural Information Processing Systems, pp.
9904-9915, 2019.
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via
minibatch proximal update. In Advances in Neural Information Processing Systems, pp. 1534-
1544, 2019.
A	Proofs for Section 3
A. 1 Detailed introduction on asymptotic analysis
Here we provide more details on the asymptotic analysis framework sketched in Section 2.1.
In typical scenarios, for consistent ERMs, the limiting distribution of wb 0,T is asymptotically normal
with a known covariance matrix, as is characterized in the following classical result (see, e.g. Van der
Vaart (2000, Theorem 5.21) and also Liang (2016)).
Proposition 6 (Asymptotic normality and excess risk of ERMs). Assume the population minimizer
w0,? is unique and the ERM wb 0,T is consistent (i.e. it converges to w0,? in probability as T → ∞).
Further assume the following regularity conditions:
(a)	There exists some random variable At = A(pt, Xt, yt) such that E[At2] < ∞ and
∣∣V't(wι) - V't(w2)k ≤ At ∣∣wι - w2k
for all w1 , w2 ∈ Rd;
(b)	E|||V't(W0,?)『]< ∞;
(c)	L is twice-differentiable with V2L(w0,?)	0,
then the ERM wb 0,T is asymptotically normally distributed, with
√T ∙ (W0,T — W0,?) → N(0, V2L(w0,?)TCov(V't(w0,*))V2L(w0,?)T) =: PW,
T ∙ (L(Wo,τ) — L(wo,?)) →→ ∆>V2L(wo,*)∆ where ∆ 〜Pw.
where →d denotes convergence in distribution and `t : Rd → R is the loss function on a single task.
When this happens, we define the asymptotic rate of estimation (in MSE loss) and asymptotic excess
risk wb 0,T as those of its limiting distribution:
AsymMSE(Wo,τ) := Eδ〜Pw [1]△『]=tr(V2L(w0,?)TCov(V't(wo,*))V2L(wo,?)T)
AsymExcessRisk(Wο,τ):= Eδ〜Pw [∆> V2L(wο,*)∆] = tr(V2L(w0,?)TCov(V't(w°,?))).
A.2 Proof of Proposition 1
Equivalence of test-time risk and training loss for train-val method We first show that
LfW0) = E['tr-νal(wο)] = Lλe,n (W0)
12
Under review as a conference paper at ICLR 2021
for all w0, that is, the population meta-test loss is exactly the same as the population risk of the train-
val method. This is straightforward: as the tasks are i.i.d. and Aλ(w0; Xttrain, yttrain) is independent
of the test points (Xtval, ytval), we have for any w0 that
12
E['tr-val(wo)]=
Ept~π,(Xt,ythPt [2nɪIIyval- XValAλ(wo; Xtram ytrain)∣∣ J
12
=Ept~∏,(Xt,yt)~pt[2 (yva1-xva1>Aλ(wo; Xtram ytrain))j
12
=EpT+1 〜∏,(Xt+ι,yτ+ι),(χθ,yθ芦p/2 y - x Aλ,nι (w0; XT +1, yT +1)
= Ltλe,snt1 (w0).
Therefore the train-val method is acutally a valid ERM for the test loss Ltλe,snt , and it remains to show
that the train-val method is (itself) consistent.
Consistency We expand the empirical risk of the train-val method as
T
LT-val(wo) = TX 圭 I∣yval - xvalAλ(wo;Xtram ytrain)∣∣2
1T 1	2
=T 'E 诟 ∣∣yv- Xv[w0 + (Xtrain>Xtrain + n1 λId)-1Xtrain>(ytrain - Xtrain w°)]∣∣
T
=1 X ɪ ∣∣yval - Xval(Xtrain>Xtrain + n1λId)-1Xtrain>ytrain - Xvaln1λ(Xtrain>Xtrain + n1λId)-1wo∣∣2
T t=1 2n2
= 2 w> MT Wo — w> bT + const,
where
Xval> Xval
—t-----1- (Xtrain>Xtrain∕n1 + λId)-1,
1T
MT ：= T Eλ2(Xtrain>Xtrain∕n1 + λId)-1
t=1
n2
1T	1
bT := Vλ(Xtrain>Xtrain∕n1 + λId)-1 •	Xval>(yval - Xval(Xtrain>Xtrain + n1λId)-1 Xtrain>y；rain).
T t=1	t t	n2	t t t t t	t t
Noticing that (XtrainT Xtrain∕n1 + λId)-1 W λ-1L and by the assumption that E(x,y)〜* [xx>] Y
∞, E(x,y)〜pt[xy] < ∞, we have E[∣∣MT ∣∣] < ∞ and E[∣∣bT ∣∣] < ∞. Since the task pt's are i.i.d.,
by the law of large numbers, we have with probability one that
MT → E[MT]
Xval> Xval
=Ept,(Xt,yt) λ2(Xtrain>Xtrain∕n1 + λId)-1 t n2 t (Xtrain>Xtrain∕n1 + λId)-1	(8)
= Ept,(Xt,yt) λ2(Xttrain>Xttrain∕n1 + λId)-1Σt(Xttrain>Xttrain∕n1 + λId)-1	0,
(where Σt = Ex〜p∕xx>] A 0) and
bT → E[bT]
=Ept,(Xt,yt) [λ(Xtrain>Xtrain∕n1 + λId)-1 ∙ ɪXval>(yval - Xval(Xtrain>Xtrain + n1λId)-1 Xtrain>y；rain)
n2
<∞
(9)
as T → ∞. Therefore, by Slutsky’s Theorem, we have
Wo,T = M-1bT → E[MT]-1E[bT] = arg min Ltr-val(wo) = arg min L^S^ (wo) = Wo,*(λ,n1)
w0	w0
as T → ∞. This proves the consistency of the train-val method.
13
Under review as a conference paper at ICLR 2021
Asymptotic normality Similar as above, we can write the per-task loss as
't(wo) = 2 l∣AtW0 一 Ct『,
where
A = λ Xval(Xtrain> χtrain/n1 + λId)-1,
√n?
Ct = -ɪ (yval - Xval(Xtrain>Xtrain∕nι + λId)-1 -1 Xtrain>ytrain).
√n2 ∖	n1	)
In order to show the desired asymptotic normality result, it suffices to check the conditions in Propo-
sition 6. First, we have
V't(wo) = A>IAtwO - ct).
This is Lipschitz in w0 with Lipschitz constant
IlAjAtIIop ≤ ∣AtkFr <>XvalR.
As Ex〜pt[kx∣4] < ∞, the above quantity is clearly square integrable, therefore verifying (a). As
wo,? = wo,*(λ, nι) is finite, We can use similar arguments as above to show (b) holds. Finally, We
have already seen L is twice-differentiable (since it is quadratic in w0) and V2L(w0,?) 0, which
verifies (c). Therefore the conditions of Proposition 6 hold, Which yields the desired asymptotic
normality result.	□
A.3 Proof of Proposition 2
High-level idea At a high level, this proof proceeds by shoWing that the train-train method is also
consistent to the (population) minimizer of Ltr-tr, and constructing a simple counter-example on
Which the minimizers of Ltr-tr is not equal to that of Ltλe,snt .
Population minimizers of Ltr-tr and Ltr-val We begin by simplifying the non-splitting risk. We
have
'tr-tr(wo) = ɪ kyt - XtAλ(wo; Xt,yt)k2
2n
12
=2n Ilyt- Xt [wo + (Xt Xt + nλId) Xt (yt - XtWO)]/
=2 k AtWO - ctk2,
Where
At = √^nλXt (X>Xt + nλId)-1 and Ct =*(In - Xt(X>Xt + nλId)-1 X>)yt.
Using similar arguments as in the proof of Proposition 1 (Appendix A.2), We see that the train-
train method wb tOr,-Ttr converges With probability one to the minimizer of the pouplation risk Ltr-tr,
Which is
w0r-?r = arg min Ltr-tr(wo) = (E[A>At])-1E[A>Ct]
w0
λ2(X>X/n+λId)-2 XnXt1	∙E
E [λ2(X>Xt/n + XIdy 2 X^Xt 1	∙ E
-λ(X>Xtln + λId)-1X> (In — Xt(X>Xt + nλId)
n
λ2(X>Xt∕n + XId)-21 X>yt .
n
1Xt>)yt
E
(10)
14
Under review as a conference paper at ICLR 2021
On the other hand, recall from Proposition 1 ((8) and (9)) that the population minimizer of Ltλe,snt is
wo,*(λ,n) = arg min Lχ^(wo)
w0
=E[λ2(X>Xt/n + λId)-1∑t(X>Xt/n + λId)-1]-1 ∙ ∣λE[(X>Xt/n + 》口尸㈣,.”)』[x0y0]
-λE [(X>Xt/n + λId)-1∑t(X>Xt/n + λId)-11 X>y
n
(11)
Construction of the counter-example We now construct a distribution for which (10) is not equal
to (11). Let d = 1 and let all pt be the following distribution:
(1, 3) with probability 1/2;
(3, -1) with probability 1/2.
Clearly, We have ∑t = 5, St := X>Xt/n ∈ [1,9], and Ex，4〜,：[χ0y0] = 0. Therefore We have
pt :	(xt,i, yt,i)
w0;r = E[(st + λ)-2st]T. E
21 n
(st + λ)	— >jxt,ιyt,i
n i=1
and
wo,*(λ,n) = -E[5λ2(st + λ) 2]	∙ E 5λ(st + λ)
1n
nExtM
i=1
-E[λ(st + λ)-2]-1 ∙ E
21 n
(st + λ) — — ɪ2 xt,iyt,i
n i=1
We now show that w0r,-tr = w0,*(λ, n) by showing that
E
1n
(st + λ)-2- Ext,iyt,i
n i=1
EkS2M=0
for any λ > 0. Indeed, conditioning on (xt,1, yt,1) = (1, 3), we know that the sum-of-squares in st
has one term that equals 1, and all others i.i.d. being 1 or 9 with probability one half. On the other
hand, ifwe condition on (xt,1, yt,1) = (3, -1), then we know the sum in st has one term that equals
9 and all others i.i.d.. This means that the negative contribution in the expectation is smaller than
the positive contribution, in other words
e[ (Sw ] = 2 ∙ 3E [ (s⅛ ""I)"3)
+ 2 ∙ -3E (st + λ)2 (Xt,1,yt,I) = (3,-I) > 0∙
This shows w0r-tr = w0,*(λ, n) and consequently the W0rTr does not converge to w0,*(λ, n) and the
difference is bounded away from zero as T → ∞.
Finally, for this distribution, the risk Ltλe,snt (w0) is strongly convex (since it has a positive second
derivative), this further implies that L^ (W0r,Tr) - LRn (w0,*(λ,n)) is bounded away from zero
almost surely as T → ∞.
B Proofs for Section 4
B.1 Proof of Theorem 3
We first show that w0,? = EWt〜∏ [wt] is a global optimizer for Ltr-tr and Ltr-Val with any regulariza-
tion coefficient λ > 0, any n, and any split (n1, n2). To do this, it suffices to check that the gradient
at w0,? is zero and the Hessian is positive definite (PD).
15
Under review as a conference paper at ICLR 2021
Optimality of w0,? in both Ltr-tr and Ltr-val. We first look at Ltr-tr: for any w0 ∈ Rd we have
Lgr(Wo) = E['tr-tr(wo)]
2nE [BXtwt - Xt
[(X>Xt + nλId)-1 X> (XtWt- Xtwo) + wq] ∣∣2
2nEh∣∣Xt卜d-(χ>χt+nλId)	χ>χ,(Wt-wo)Il i.	(12)
Similarly, Ltr-val can be written as
Ltr-val(W0)	(13)
=E['tr-val(w0)]
=圭E IlXvalWt- Xval h((Xtrain)>Xtrain + nιλId)-1 (Xtrain)> (XtrainWt - Xtrainw°) + w°]
=212E[∣∣Xval Q-((Xtrain)>Xtrain + nιλId)-1 (Xtrain)>Xtrain) (Wt- W0)∣∣2].	(14)
We denote
Mtr-tr = Xt Q - (X>Xt + nλId)-1 X>Xt)	and	(15)
Mtr-Val = Xval Q - ((Xtrain)>Xtrain + n4λId)T (Xtrain)>Xtrain)	(16)
2
to simplify the notations in (12) and (14). We take gradient of Ltr-tr and Ltr-val with respect to W0:
VwoLtr-tr(Wo) = - 1 E [(Mtr-tr)>Mtr-tr(Wt - Wo)] ,	(17)
VwoLtr-val(W0) = -ɪE [(Mtr-val)>Mtr-val(Wt - Wo)] .	(18)
n2
Substituting Wo,? into (17) and taking expectation, we deduce
VwoLtr-tr(Wo,?) = -1E [(Mtr-tr)>Mtr-tr(Wt - Wo,?)] = 0.	(19)
To see this, observe that by definition E[Wt - Wo,?] = 0. Combining with Wt being generated
independently of Xt, we have the first term in RHS of (19) vanish. In addition, zt is independent
white noise, therefore, the second term in RHS of (19) also vanishes. Following the same argument,
we can show
VwoLtr-val(Wo,?) =0,
since X0t is also independent of Wt . The above reasonings indicates that Wo,? is a stationary point
of both Ltr-tr and Ltr-val. The remaining step is to check VwoLtr-tr(Wo,?) and VwoLtr-val(Wo,?) are
PD. From (17) and (18), we derive respectively the hessian of Ltr-tr and Ltr-val as
VW0 Ltr-tr (Wo,?) = 1 E[(Mtr-tr)>Mtr-tr] and
VW0 Ltr-val(Wo,?) = ɪ E[(Mtr-val)>Mtr-val].
o	n2
Let v ∈ Rd	be	any nonzero vector, we check v>V2w Ltr-tr(Wo,?)v > 0.	A	key	obser-
Vation is that	(Id	- (X>Xt + nλId) 1 X>Xt) is positive definite for any λ	=	0.	To see
this, let σ1 ≥ •一 ≥ ◎& be eigenvalues of §X>Xt, Some algebra yields the eigenvalues of
(Id - (X>Xt + nλId)-1 X>Xt) are λ+λσ~ > ° for λ = 0 and i = 1,...,d. Hence, We deduce
v>VW0Ltr-tr(Wo,*)v = 1 E[v>X> (Id - (X>Xt + nλId)-1 X>Xt)2 Xtv] > 0,	(20)
since Xt is isotropic (an explicit computation of the hessian matrix can be found in Appendix B.2).
As a consequence, We have shoWn that Wo,? is a global optimum of Ltr-tr . The same argument
applies to Ltr-val, and the proof is complete.
16
Under review as a conference paper at ICLR 2021
Consistency of Wb0tTtr,tr-val}. To check the consistency, We need to verify the conditions (a) - (C) in
Proposition 6.
For condition (a), We derive from (17) and (18) that
∣∣V'{tr-tr'tr-val}(wι) - V'{tr-tr'tr-val}(w2)∣∣ ≤ n ∣∣(M{tr-tr,tr-val})>M{tr-tr,tr-val}∣∣	kwι - W2k,
Where n should be replaced by n2 for the split method (We slightly abuse the notation for simplicity).
It suffices to shoW E ∣(Mt{tr-tr,tr-val})>Mt{tr-tr,tr-val} ∣op < ∞, Which folloWs from the same argu-
ment in the proof of Proposition 1. In particular, we know 0 W Id -(X> Xt + nλId) 1 X> Xt W Id
and E[∣Xt>Xt ∣op] < ∞ since Xt is Gaussian. As a consequence, for the no-split method,
we have E ∣(Mttr-tr)>Mttr-tr ∣o2p < ∞. For the split method, we also have 0 W Id -
((Xtrain)>Xtrain + nλId)-1 (Xtrain)>Xtrain W Id and E[∣∣(Xval)>Xval∣∣op] < ∞, which implies
E h∣(Mttr-val)>Mttr-val ∣o2pi <∞.
For condition (b), using a similar argument in condition (a) and combining with R2 = E[kw0,? -
wtk2], Wehave E[kV'{tr-tr,tr-val}(wo,?)k2] < ∞.	’
For condition (c), using (20), we directly verify that L{tr-tr,tr-val} is twice-differentiable and
V2 L{tr-tr,tr-val}	0.
B.2 Proof of Lemma 5
Proof. In this section we prove Lemma 5. Using the the asymptotic normality in Proposition 6,
the asymptotic covariance is V-2L{tr-tr,tr-val}Cov[V'{tr-tr,tr-val}] V-2L{tr-tr,tr-val}. Therefore, in the
following, we only need to find V-2L{tr-tr,tr-val} and Cov[V'{tr-tr,tr-val}].
• Asymptotic variance of wb 0tr,-Ttr . We begin with the computation of the expected Hessian
n E[(Mtr-tr )>Mtr-tr ].	’
E[(Mttr-tr)>Mttr-tr]
=E ](Id - (X>Xt + nλId)-1 X>Xt)> X>Xt (Id- (X>Xt + nλId)-1 X>Xt)
=) E [Vt (Id - (D>Dt + nλId)-1D>Dt)> D>Dt(Id - (D>Dt + nλId)-1 D>Dt) V>],
(21)
where the equality (i) is obtained by plugging in the SVD of Xt = UtDtVt> with Ut ∈ Rn×n,
Dt ∈ Rn×d, and Vt ∈ Rd×d. A key observation is that Ut and Vt are independent, since Xt is
isotropic, i.e., homogeneous in each orthogonal direction. To see this, for any orthogonal matrices
Q ∈ Rn×n and P ∈ Rd×d, we know Xt and QXtP> share the same distribution. Moreover, we
have QXtP> = (QUt)Dt(PVt)> as the SVD. This shows that the left and right singular matrices
are independent and both uniformly distributed on all the orthogonal matrices of the corresponding
dimensions (Rn×n and Rd×d, respectively).
Recall that we denote σ(n) ≥ ∙∙∙ ≥ σdn as the eigenvalues of 1 X> Xt. Thus, we have D> Dt
Diag(nσ1(n), . . . , nσd(n)). We can further simplify (21) as
E [Vt (Id - (D>Dt + nλId)-1D>Dt)> D>Dt(Id - (D>Dt + nλId)-1D>Dt) V>]
E 卜 3(½
d	λ2 (n)
E x nλσi
匕(σ(n) + λ)2
(22)
17
Under review as a conference paper at ICLR 2021
We will utilize the isotropicity of Xt to find (22). Recall that we have shown that Vt is uniform
on all the orthogonal matrices. Let P ∈ Rd×d be any permutation matrix, then VtP has the same
distribution as Vt. For this permuted data matrix VtP, (22) becomes
d	nλ2σi(n)	>
/(( (n)	ʌ ʌ2 vt,τP(∙i)vt,τp(~i)
i=1 (σi	+ λ)2
E
with τp(i) denotes the permutation of the i-th element in P.
Summing over all the permutations P (and there are totally d! instances), We deduce
d!E[(Mttr-tr)>Mttr-tr]
E
all permutation τp
d	nλ2σi(n)	>
i=1 (σ(n) + λ)2 vt,τp(i)vtτp(i)
d
d
=(d-1)!E
nλ2 σi(n)
E A (σ(n) + λ)
vt,jvt>,j
j=1
i
2
=(d-1)!E
Xdλ2 (n)	d	λ2 (n)
λ σ; ,...,X λσi、 V>
(λ + σ(n))2	i=1 (λ + σ(n))2	t
=(d-1)!E
X	λ2σ(n)
M (λ + σ(n))2
VtIdVt>
(23)
Dividing (d - 1)! on both sides of (23) yields
E[(Mttr-tr)>Mttr-tr]
X	λ2σ(n)
M (λ + σ(n))2
Id.
(24)
n
d E
Next, We find the expected covariance matrix n⅛E[▽'?-" (wo,?)(V'tr-tr(wo,?))>].
EMtr-tr(w0,?)(V'tr-tr(w0,*))>]
= E[(Mttr-tr)>Mttr-tr(w0,? -wt)(w0,? -wt)>(Mttr-tr)>Mttr-tr]
(=i) E VtDiag
• VtDiag
nλ2σ(n)
(σ(n) + λ)2
nλ2 σ1(n)
nλ2σd(n)
Vt> (w0,? - wt)(w0,? - wt)>
(25)
Here step (i) uses the SVD of Xt and the computation in (22). Combining (24) and (25), We derive
the asymptotic covariance matrix of using Ltr-tr as
AsymCov(wb 0tr,-Ttr)
=EV-2'tr-tr]CovV'tr-tr(w0,*)]E[V-2'tr-tr]
d2
n2
d
E
i=1
• E VtDiag
λ2σ(n)
(λ + σ(n)
nλ2σ1(n)
• VtDiag
(σ(n) + λ)2
nλ2σ1(n)
nλ2σd(n)
Vt>(w0,? - wt)(w0,? - wt)>
Vt>
(26)
Taking trace in (26), We deduce
AsymMSE(wbt0r,-Ttr)
18
Under review as a conference paper at ICLR 2021
tr(AsymCov(wb 0tr,-Ttr))
d2
林E
X λ2σ(n)
=(λ + σ(n)
• tr E
VtDiag
：.+\；)V>(W0,? - Wt)(W0,* - Wt)J)
=) d (E
n2
d	λ2σ(n)	-2
X (λ+
• ntr (E
d
d
X
i=1
(λ + σ(n) )4
VtIdVt> (w0,? - wt)(w0,? - wt)>
d E [Pd=ι λ4(σ(n))2∕(λ + σ(n))4]
E Pid=1 λ2σi(n)∕(λ + σi(n)
)2
• tr E[(w0,? - wt)(w0,? - wt)>]
dR2
E [Pd=ι(σ(n))2∕(λ + σ(n))4]
E Pid=1σi(n)∕(λ+σi(n))2
2,
(27)
where step (i) utilizes the independence between wt and Xt and applies the permutation trick in
(23) to find E Vt Diag
n2λ4(σ(n))2	n2λ4(σdn))2
(σ(n)+λ)4 ,…，(σdn)+λ)4
Vt> .
• Asymptotic variance of Wb 0tr,-Tval .	Similar to the no-split case, we compute the Hessian
n1 E[V2'tr-val] = n1 E[(Mtr-val)τMtr-val] first.
E[(Mttr-val)>Mttr-val]
=E (id - ((Xtrain)TXtrain + nιλId)-1 (Xtrain)TXtrain)T(Xval)TXval
• (id - ((Xtrain)TXtrain + Mid)-1 (Xtrain)TXtrain)
=n2E[ Q-((Xtrain)TXtrain + n1λId)-1 ((Xtrain)TXtrain)T
∙ (id - ((Xtrain)TXtrain + n1λId)-1 (Xtrain)TXtrain)
(=) n2E∣Vtrain (Id - ((Dtrain)TDtrain + n1λId)-1(Dtrain)TDtrain)2 (Vtrain)T],	(28)
where (i) uses the data generating assumption E[(Xtval)TXtval] = n2id and the independence be-
tween Xttrain andXtval, and (ii) follows from the SVD of Xttrain = UttrainDttrain(Vttrain)T.
Here We denote σ(n1) ≥ ∙∙∙ ≥ σdn1) as the eigenvalues of n1 (Xtrain)TXtrain. Thus, We have
(Dttrain)TDttrain = Diag(n1σ1(n1), . . . , n1σd(n1)). We can noW further simplify (28) as
n2E∣Vtrain (Id - ((Dtrain)TDtrain + n1λId)-1(Dtrain)TDtrain)2 (Vtrain)Ti
=) n2EMrainDiag( (σ⅛，•••，(σ⅛)(Vtrain)T
(=) n2E "XX ； # Id.	(29)
Step (i) folloWs from the same computation in (22), and step (ii) uses the permutation trick in (23).
19
Under review as a conference paper at ICLR 2021
Next, We find the expected covariance matrix n⅛E[V'tr-tr (wo,?)(V'tr-tr(wo,?))>].
E[V'tr-tr(w0,?)(V'tr-tr (wo,?))>]
E[(Mttr-tr)>Mttr-tr(w0,? - wt)(w0,? - wt)>(Mttr-tr)>Mttr-tr]
EWrainDiag (σ(n⅛,"∙,σdn⅛) (Vtrain)>(χVal)>
•	XvalVtrainDiag ( (n1λ	,∙∙∙, (n1λ	) (Vtr-n)>(wo,? - Wt)(W0,? - wt)>
σ1 1 + λ	σd 1 + λ
MainDiag (J ,∙∙∙,σd⅛)(Vtr(Xval)>
•	XvalVtrainDiag (J …」)e)>]∙	(30)
Combining (29) and (30), We derive the asymptotic covariance matrix of using Ltr-val as
AsymCov(Wb 0tr,-Tval )
E[V-2'tr-val]Coν[V'tr-val(w0,*)]E[V-2'tr-val ]
•E MrainDiag (σnλ+y，…，σn⅛) (Vtrain)>(χval)>
•	XvalVtrainDiag Qn； + λ，…，σdn1λ + λ) (Vtrain)>(W0,? - Wt)(W0,? - Wt)>
WinDiag (,，…，」)(Vtrain)> (Xval)>
•	XvalVtrainDiag	，…，σdn⅛) (Vtrain)I	(31)
Taking trace in (31), We deduce
AsymMSE(Wb t0r,-Ttr)
= tr(AsymCov(Wb 0tr,-Ttr))
=d2 (E "XX —牛—#!
n2 1 [白(λ + σ(n1))2J/
•	tr (E WrainDiag (σn¾y，…，σ⅛) (Vtrain)T(Xval)T
•	XvalVtrainDiag ( (n1λ、，…，(n1λ、卜Vtrain)>(W0,? - Wt)(W0,? - Wt)>
σ1 + λ	σd	+ λ
•	VtrainDiag (，，… J)(Vtrain)> (Xval)>
•	XvalVtrainDiag (J，…，」)(Vtrain)I)
20
Under review as a conference paper at ICLR 2021
Y (E 氏	λ2) }
n2 I 匕(λ + σ(n))2J;
• tr (E WrainDiag (σpiλ+y,…，σd⅛)(Vtrain)>(xVal)>
•	XvaWnDiag ((σ⅛ ,…，(σd⅛)(Vtrain)>(Xval)>
•	XvalVtrainDiag Qn； + λ,..., σ(nιλ + λ )(Vtrain)>(W0,? - Wt)(W0,? - wt)>]),
1	d	(32)
where (i) follows from the cyclic property of the matrix trace operation. Due to the isotropicity of
Xttrain and Xtval, we claim that
E WrainDiag (σn⅛!,…，σj⅛) (Vtrain)>(Xval)>
• XvalVtrainDiag ((⅛F ,…，(σd⅛)(Vtrain )>(Xval)>
• XvalVtrainDiag (σn⅛7 ,…，σcn⅛ι) (Vtrain)I	(33)
is a diagonal matrix cId with all the diagonal elements identical. We can show the claim bying
taking expectation with respect to Xtval first. Since Vttrain is an orthogonal matrix, XtvalVttrain has
the same distribution as Xtval and independent of Xt. We verify that any off-diagonal element of the
matrix expectation
A :=	EXval
(Vttrain)>(Xtval)>XtvalVttrainDiag
λ2
(σdnI) + λ)2
• (Vtrain )> (Xval )> XvalVtrain
is zero. We denote XvalVtrain = [χ1,..., Xn]> ∈ Rn2×d With Xi ± N(0, Id). For k = ', the
(k,')-th entry Ak,' of A is
))
(=i) 0,
where Xij denotes the j-th element of Xi. Equality (i) holds, since either Xkm or x',n only appears
once in each summand. Therefore, We can Write A = Diag (A1,1, . . . , Ad,d) With Ak,k being
Ak,k = E
=E
U______λ2_____
j (σjn1)+ λ)2
xk,mxj,mxj,nx',n
λ2
(σknI) + λ)2
Xk,mXk,mXk,nXk,n
))
21
Under review as a conference paper at ICLR 2021
Observe that Ak,k only depends on σk(n1). Plugging back into (33), we have
E VttrainDiag
+ λ'…"nI)+ λ
λ
(Vttrain)>(Xtval)>
• XvalVtrainDiag
((σ(nI) + λ)2 ' 一 ∖σdnI) + λ)
λ2
2	(Vttrain)> (Xtval)>
• XtvalVttrainDiag
+ λ’…'σdnI) + λ
(Vttrain)>
E VttrainDiag
+ λ,∙∙∙,σdnι) + λ
Diag(A1,1,...,Ad,d)
λ
λ
• Diag
E VttrainDiag
+ λ'…»ni) + λ
λ2A1,1
(Vttrain)>
λ2Ad,d
(σ(n1) + λ)2'…’(σdn1) + λ)2
(Vttrain)>
λ
(i)
= cId
where equality (i) utilizes the permutation trick in (24). To this end, it is sufficient to find c as
C=["„。Diag (σdλ+y,…，σf⅛) (Vtrain)>(Xval)>
•	XvalVtrainDiag ((σ⅛,..., (σd⅛)(Vtrain)>(Xval)>
•	XvalVtrainDiag ("…J) (Vtrain)I)
=dtr (E〕XvalVtrainDiag ((σ⅛ …(⅛F) (Vtrain )>(Xval)>
•	XvalVtrainDiag ((σ⅛…(σ⅛)(Vtrain)>(Xval)>]).	(34)
Observe again that XtvalVttrain ∈ Rn2 ×d is a Gaussian random matrix. We rewrite (34) as
c = -E
d
n2
vi>Diag
i,j=1
2
F'…'(σdn1) + λ)2!vj)
(35)
where Vi iid N (0, Id) is i.i.d. Gaussian random vectors for i = I,...,n2. To compute (35), We need
the following result.
Claim 7. Given any symmetric matrix A ∈ Rd×d and i.i.d. standard Gaussian random vectors
v, U J N(0, Id), we have
E	(v>Av)2	=	2kAk2Fr +	tr2(A)	and	(36)
E	(v>Au)2	=	kAk2Fr.	(37)
Proof of Claim 7. We show (36) first. We denote Ai,j as the (i, j)-th element of A and vi as the i-th
element of v. Expanding the quadratic form, we have
E [(v>Av)2] = E	X VivjVkV'Ai,jAk,'
i,j,k,'≤d
22
Under review as a conference paper at ICLR 2021
E Ev4 A2,i	+ E Ev2v2(A2,j + Ai,iAj,j + Ai,j Aj,i)
i≤d	i6=j
3 i≤d Ai2,i + i6= (Ai2,j + Ai,iAj,j + Ai,jAj,i)
tr2(A) + 2 X Ai2,i + X(Ai2,j + Ai,jAj,i)
tr2(A) +2kAk2Fr.
Next, we show (37) by the cyclic property of race.
E [(v>Au)2] =tr (E [uu>Avv>A] = tr(A2) = kAk2Fr.
□
We back to the computation of (35) using Claim 7.
C=d IXι (v>Diag ((⅛F …(σ¾! vj! ,
=d Et (v>Diag( (σ⅛ ,∙∙∙, (σ⅛! vi!]
+d EIX k >Diagt⅛,…,(σ⅛ 卜).
=VE "2 (Diag ((σ(n1) + λ)2 ,…,(σdn1) + λ)2!!_
+22n E [IDiag ((σn‰,…，(σn‰![
+nʃ2 E [lDiag ((σnι‰,…,(σnλ+λ2 !L _
=n2(E "X (σ⅛ # +(n2+1)E"X (⅛F #) .	(38)
Combining (38) and (33), by the independence between wt and Xttrain , Xtval, we compute (32) as
AsymMSE(wb0tr,-Ttr)
2
λ2
(曾I) + A)2_
d
X
i=1
+ (n2 + 1)E
d
X
i=1
(MnI) + λ)4 #)
• E [(W0,? - Wt)(W0,? - Wt)>]
dR E [Pd=ι λ2∕(σ(nI) + λ)2i 2 + (22 + 1)E [Pd=ι λ4∕(σ(nI) + λ)4i
n2	(E [Pd=ι λ2∕(λ + 曾1))2])2
The proof is complete.
□
23
Under review as a conference paper at ICLR 2021
B.3 OPTIMAL RATE OF THE TRAIN-VAL METHOD AT FINITE (n, d)
Corollary 8 (Optimal rate of the train-val method at finite (n, d)). For any (n, d) and any split ratio
(n1, n2) = (n1, n-n1), the optimal rate (by tuning the regularization λ > 0) of the train-val method
is achieved at
inf AsymMSE(W0rTal(nι,n2; λ)) = lim AsymMSE(W0rTal(nι,n2; λ)) =(" + n2 + I)A .
Further optimizing the rate over n2, the best rate is taken at (n1, n2) = (0, n), in which the rate is
infλ>0,n2∈[n] AsymMSE(W0r,Tal(nι,n2; λ)) = Id + n +1)R2 .
Discussion: Using all data as validation Corollary 8 suggests that the optimal asymptotic rate of
the train-val method is obtained at λ = ∞ and (n1, n2) = (0, n). In other words, the optimal choice
for the train-val method is to use all the data as validation. In this case, since there is no training data,
the inner solver reduces to the identity map: A∞,0(W0; Xt, yt) = W0, and the outer loop reduces
to learning a single linear model W0 on all the tasks combined. We remark that while the optimality
of such a split ratio is likely an artifact of the data distribution we assumed (noiseless realizable
linear model) and may not generalize to other meta-learning problems, we do find experimentally
that using more data as validation (than training) can also improve the performance on real meta-
learning tasks (see Table 2).
Proof of Corollary 8 Fix n1 ∈ [n] and n2 = n - n1. Recall from Lemma 5 that
dR2 E [(Pd=ι λ2∕(σ(nI) + λ)2)2 + (n2 + 1) Pd=I λ4∕(σ(nI) +
AsymMSE(w0Tal(nι,n2; λ))=--——---------------------------..........———-ʒ------------------
n2	E Pid=1 λ2∕(σi(n1) +λ)2
Clearly, as λ → ∞, we have
lim AsymMSE(W射加小;λ)) = dR2 ∙ d2 + (n2 + 1)- = (d + n2 + 1)R2.
λ→∞	,	n2	d2	n2
It remains to show that the above quantity is a lower bound for AsymMSE Wb t0r,-Tval(n1, n2; λ) for
any λ > 0, which is equivalent to
EPid=1 λ2∕(σi(n1) + λ)2)2 + (n2 + 1) Pid=1 λ4∕(σi(n1) + λ)4	d+n +1
------------------------------------------2-------------≥ ≥-------2---, for all λ > 0.
E Pid=1 λ2∕(σi(n1) + λ)2	d
(39)
We now prove (39). For i ∈ [n1], define random variables
(σ(nI) + λ)2
∈ [0, 1]
and Yi := 1 - Xi ∈ [0, 1].
Then the left-hand side of (39) can be rewritten as
E [(- - nι + Pn= ι Xi)2 + (n2 + 1) (- - nι + £-1 X2)]
(E[- - nι + Pn=IXi]/
_ E[(d - pn=ι Yi)2 + (n2 + 1)(d - 2Pn= 1 Y + pn=ι Yi2)]
=	(E[d - Pn=I Yi])2
d2 + (n2 + 1)d — 2(d + n2 + 1)E[P Yn ] + E [(P Yn )2] + (n2 + 1)E[P .2]
—	d2 - 2dE[P Yi] + (E[P Yi])2
24
Under review as a conference paper at ICLR 2021
By algebraic manipulation, inequality (39) is equivalent to showing that
E[(EYi)2] +(n2 + 1)E[EK2] ≥ d + n + 1
(EP Yi])2
(40)
d
Clearly, E[(P Yi)2] ≥ (E[PYi])2. By Cauchy-Schwarz we also have
E[XYi2] ≥ :e[(X川 ≥ ：(EXT)2
Therefore we have
E[(P匕)2] +3 + 1)E[P型 ≥ 1 + n1±1 ≥ 1 + n2±1 = d + n + 1
(EP Yi])2	- nι - d - —d-
where we have used that n1 ≤ n ≤ d. This shows (40) and consequently (39).
□
B.4 Rate of the train-train method in the proportional limit
Theorem 9 (Exact rates of the train-train method in the proportional limit). In the high-dimensional
limiting regime d, n → ∞, d/n → γ where γ ∈ (0, ∞) is a fixed shape parameter, for any λ > 0
limd,n→∞,d∕n=γ AsymMSE(W0r,Tr(n; λ)) = ρλ,γR2.
where ρλ,γ = 4γ2 [(γ - 1)2 + (γ + 1)λ] /(λ+1+γ-P(λ + γ + 1)2 - 4γ)2/((λ + Y + 1)2 - 4γ)3/2.
Proof of Theorem 9	Let Σn := 1 Xt X> denote the sample covariance matrix of the inputs in a
single task (t). By Lemma 5, we have
AsymMSE(W0r,Tr(n; λ)) = R2 ∙
d E IPd=I σ (Σ n)2∕(σi(Σ n)+ λ)4]
(d EhPd=I σ (Σ n)∕(σi(Σ n )+ λ)2i)2
R2 ∙ dE[tr((∑n + λId)-4
|-------------------
{z^
In,d
(41)
htr((£ n + λId ) 2E n
IIn,d
2
We now evaluate quantities In,d and IIn,d in the high-dimensional limit of d, n → ∞, d/n → γ ∈
(0, ∞). Consider the (slightly generalized) Stieltjes transform of Σbn defined for all λ1, λ2 > 0:
s(λ1,λ2) :=	lim	1E[tr((λιL + λ2∑n)-1)].
d,n→∞, d/n→γ d
As the entries of Xt are i.i.d. N(0, 1), the above limiting Stieltjes transform is the Stieltjes form of
the Marchenko-Pastur law, which has a closed form (see, e.g. (Dobriban et al., 2018, Equation (7)))
<λι, λ2) = λ-1s(λι∕λ2, 1) = λ2 ∙ Y- 1-λ162 +2Yλλ1λλ2 + 1 + Y)2- 4Y
γ - 1 - λ1∕λ2 + P(λ1∕λ2 + 1+ γ)2 - 4γ
2γλι	^
Now observe that differentiating s(λ1, λ2) yields quantity II (known as the derivative trick of Stielt-
jes transforms). Indeed, we have
-jτ~s(λι, λ2) = --ɪ lim	⅛htr((λιId + λ2∑n)-1)]
dλ2	dλ2 d,n→∞, d/n→γ d
=	lim	5E --y—tr ((λ1Id + λ2∑ln)-1)]	(43)
d,n→∞, d∕n→γ d	dλ2 ∖	/ J
=	lim	1 E[tr((λιL + λ2∑ n)-2Σ n)].
d,n→∞, d∕n→γ d
25
Under review as a conference paper at ICLR 2021
(Above, the exchange of differentiation and limit is due to the uniform convergence of the deriva-
tives, which holds at any λ1, λ2 > 0. See Appendix B.4.1 for a detailed justification.) Taking
λ1 = λ and λ2 = 1, we get
lim	IIn,d =	lim Ehtr((λld + ς n) 2∑] n)] = - —— s(λι, λ2)lλι =λ,λ2 = l ∙
d,n→∞, d∕n→γ	d,n→∞, d∕n→γ d	dλ?
Similarly we have
lim	In,d =	lim	-Ehtrf(λId + ς n厂4ς n)i = -^-ττ- -s( s(λ1, λ2)lλι=λ,λ2 = 1∙
d,n→∞, d∕n→γ	d,n→∞,d∕n→γ d	6 dλ1 dλ2
Evaluating the right-hand sides from differentiating the closed-form expression (42), we get
lim	In,d =:
d,n→∞, d∕n→γ	2γ
λ + 1 + Y	1
p∕(λ + 1 + γ)2 - 4γ	2γ
lim	IIn,d
d,n→∞, d∕n→γ
(Y -I)2 + (Y + I)λ
((λ + 1 + Y)2- 4γ书
Substituting back to (41) yields that
lim	AsymMSE(W0rTr(n; λ)) = lim	R2 工/氏4
d,n→∞, d∕n→γ	,	d,n→∞, d∕n→γ	,
=R2_____________4Y2 [(Y — I)2 + (Y + I)λ____________
((λ + 1 + Y)2- 4Y)5/2 ∙(√(λ++++Y2-4γ 一 1Y
=r2____________________4Y2[(Y -I)2 + (Y + I)λ___________________
((λ +1 + Y)2 - 4y)3∕2 ∙ (λ +1+ Y - P(λ +1 + y)2 - 4y)
This proves the desired result.
□
B.4.1	Interchanging derivative and expectation / limit
Here we rigorously establish the interchange of the derivative and the expectation / limit used in (43).
For convenience of notation let Σ = Σbn = Xt>Xt/n denote the empirical covariance matrix ofXt.
We wish to show that
lim -E[tr((λιId + 无功-1)] = lim ,E —tr((λιId + 无号-1).
dλ2 d,n→∞,d∕n→γ d	d,n→∞,d∕n→γ d dλ2
This involves the interchange of derivative and limit, and then the interchange of derivative and
expectation.
——E[tr((λιId + 无功-1)] = lim E
dλ2	t→0
Interchange of derivative and expectation First, we show that for any fixed (d, n),
——E[tr((λ1Id + 为勾-1)] = E ——tr((λιId + 无号-1).
dλ2	dλ2
By definition of the derivative, we have
tr((λιL + λ2∑ + tΣ)-1) - tr((λL + λ2∑)-1) #
.
t
For any A 0, the function t 7→ tr((A+tB)-1) is continuously differentiable att = 0 with deriva-
tive -tr(A-2B), and thus locally Lipschitz around t = 0 with Lipschitz constant |tr(A-2B)| + 1.
Applying this in the above expectation with A = λ1Id + λ2Σ λ1Id and B = Σ, we get that for
sufficiently small |t|, the fraction inside the expectation is upper bounded by ∣tr(λ-2∑)∣ + 1 ‹ ∞
uniformly over t. Thus by the Dominated Convergence Theorem, the limit can be passed into the
expectation, which yields the expectation of the derivative.
26
Under review as a conference paper at ICLR 2021
Interchange of derivative and limit Define fn,d(λ2) := 1 E[tr((λ1Id + λ2∑)-1)]. It suffices
to show that
lim	lim	fn,d(λ2) = lim	fn ,d(λ2),
dλ2 d,n→∞,d∕n→γ	d,n→∞,d∕n→γ
where
fn ,d(λ2) = E	^tr((λ1Id + λ2^T) = --E[tr((λ1Id + X2ςLς)]
dλ2 d	d
by the result of the preceding part.
As fn,d(λ2) → s(λ1, λ2) pointwise over λ2 by properties of the Wishart matrix (Bai & Silverstein,
2010) and each individual fn,d is differentiable, it suffices to show that the derivatives fn0 d(λ2 )
converges uniformly for λ2 in a neighborhood of λ2 . Observe that can rewrite fn0 ,d as
fn,d (e2 ) = -Ebn,d [Eλ〜μn,d [ge2 (λ)ii,
where bn,d is the empirical distribution of the eigenvalues of Σ, and
ge (λ) :=-------L-----≤ -L for all λ ≥ 0.
λ2	(λ1 + 121)2 — λιλ2	—
Therefore, as bn,d converges weakly to the Marchenko-Pastur distribution with probability one and
gλe is uniformly bounded for λ2 in a small neighborhood of λ2, we get that fn0,d(λ2) does converge
uniformly to the expectation of gλe (λ) under the Marchenko-Pastur distribution. This shows the
desired interchange of derivative and limit.
B.5 Proof of Theorem 4
Throughout this proof we assume that R2 = 1 without loss of generality (as all the rates are constant
multiples of R2).
Part I: Optimal rate for Ltr-tr By Theorem 9, we have
inf lim	AsymMSE(W0r-Tr(n; λ))
λ>0 d,n→∞,d∕n=γ	,
4γ2 [(Y - 1)2 + (Y + 1)λ]
=inf3/2 .
λ>0 (λ +1+ γ - P(λ + Y +1)2 - 4γ)2 ∙ ((λ + Y + 1)2 - 4γ) /
|
}
:=f (λ,γ)
In order to bound inf λ>0 f(λ, Y), picking any λ = λ(Y) gives f(λ(Y), Y) as a valid upper bound,
and our goal is to choose λ that yields a bound as tight as possible. Here we consider the choice
λ = λ(γ)= max {1 - γ∕2, Y - 1/2} = (1 - γ∕2)1 {γ ≤ 1} + (γ - 1/2)1 {γ > 1}
which we now show yields the claimed upper bound.
Case 1:	Y ≤ 1 Substituting λ = 1 - Y/2 into f(λ, Y) and simplifying, we get
f(1 - Y/2, Y) = 2?2 [.4) =: g1(Y).
(2 - Y/2)
Clearly, g1(0) = 1 and g1(1) = 32/27. Further differentiating g1 twice gives
g10(Y) = Y W+54 > 0 for all Y ∈ [0,1].
(2 - Y/2)5
Thus g1 is convex on [0, 1], from which we conclude that
gι(Y) ≤ (1 - Y) ∙ gι(0) + Y ∙ gι(1) = 1 + 27Y.
27
Under review as a conference paper at ICLR 2021
Case 2:	γ > 1 Substituting λ = γ - 1/2 into f (λ, γ) and simplifying, we get
f(γ-1/2,Y
2γ2(4γ2 - 3γ + 1)
(2γ - 1/2)3
: g2 (γ).
We have g2(1) = g1(1) = 32/27. Further differentiating g2 gives
0	166
g2(Y ) = - W-^- W-Iy- W-Iy+1 <1 forall γ> 1∙
Therefore we have for all γ > 1 that
γ5
g2 (Y)= g2(1) + JI g2 (t)dt ≤ g2(1) + Y - 1 = Y + 27 ∙
Combining Case 1 and 2, we get
λin>f0f(λ,Y) ≤ g1(Y)
≤ 1 {γ ≤ 1} + g2(γ)1 {γ > 1} ≤ (1 + 27Yb {γ ≤ 1} + (27 + Yb {γ > 1}
max{1+27Y 27+y)∙
This is the desired upper bound for Ltr-tr.
Equality at Y = 1 We finally show that the above upper bound becomes an equality when Y = 1.
At Y = 1, we have
8λ
f(λ, 1)
8λ-4
(λ + 2 - √λ2 + 4λ)2(λ2 + 4λ)3/2	(1 + 2∕λ - √1 + 4∕λ)2 (1 + 4∕λ)3/2
Make the change of variable t = <1+4∕λ so that λ-1 = (t2 - 1)/4, minimizing the above
expression is equivalent to minimizing
(t2 - 1)4/32	_ (t + 1)4
(t2∕2 - t + 1∕2)2t3 =	8t3
over t > 1. It is straightforward to check (by computing the first and second derivatives) that the
above quantity is minimized at t = 3 with value 32∕27. In other words, we have shown
32	5	5
λ>% f (λ, 1) = 27 = max{1 + 27Y，27 + y)1=i,
that is, the equality holds at Y = 1.
Part II: Optimal rate for Ltr-val We now prove the result on Ltr-val, that is,
AsymMSE(W0rTal(ns,n(1 — s); λ))
inf lim
λ>0,s∈(0,1) d,n→∞,d∕n=γ
(=i)
lim	inf	AsymMSE(W0r-Tal(nι, n2; λ)) (=)
d,n→∞,d∕n=γ λ>0,n1+n2=n	,
1 + Y∙
d+n + 1
n
First, equality (ii) follows from Corollary 8 and the fact that (d + n + 1)∕n → 1 + Y. Second, the
“≥" direction of equality (i) is trivial (since we always have “inf lim ≥ liminf"). Therefore we get
the “≥" direction of the overall equality, and it remains to prove the "≤" direction.
For the "≤" direction, We fix any λ > 0, and bound AsymMSE(W0rTal(nι,n2; λ)) (and conse-
quently its limit as d, n → ∞.) We have from Lemma 5 that
AsymMSE(Wb t0r,-Tval (n1 , n2 ; λ))
28
Under review as a conference paper at ICLR 2021
d E ](Pd=ι λ2∕(σ(nI) + λ)2)2 + (n2 + 1) Pd=I λ4∕(σ(nI) + λ)4]
n2	(EhPd=I λ2∕(σ(n1) + λ)2i)2
d	d2 + (n2 + 1)d
n2 (E[Pd=ι λ2∕(σ(n1) + λ)2i)2
d + n2 + 1	1
n2	(Ehd Pd=I λ2∕(σ(n1) + λ)2i)2
Observe that
E - V ---------------- ≥ E -------------------------2
d = H1 + λ)2J	[(Pd=1 σ(n1)∕d + λ)
Ui)	λ2	(iii)	λ2
≥ (EhPd=I σ(n1)∕d[ + λ)2	(1 + λ)2 ,
where (i) follows from the convexity of t 7→ λ2∕(t + λ)2 on t ≥ 0; (ii) follows from the
same convexity and Jensen’s inequality, and (iii) is since EIPd=I σ"] = EIM n⅛ X>Xt)]=
E kXtk2Fr ∕n1 = d. Applying this in the preceeding bound yields
AsymMSE(wb 0tr,-Tval (n1, n2; λ)) ≤
d + n2 + 1 (1 + λ)2
n2
λ2
Further plugging in n1 = ns and n2 = n(1 - s) for any s ∈ (0, 1) yields
lim	AsymMSE(W0rTal(ns,n(1 - s); λ)) ≤ Y +1 - S • (1+”产
d,n→∞,d∕n→γ	,	1 - S	λ2
Finally, the right-hand side is minimized at λ → ∞ and s = 0, from which we conclude that
inf lim	AsymMSE(wb 0tr-Tval (n1, n2; λ)) ≤ 1 + γ,
λ>0,s∈(0,1) d,n→∞, d∕n→γ	,
which is the desired "≤" direction.
□
C Connections to Bayesian estimator
Here we discuss the relationship between our train-train meta-learining estimator using ridge regres-
sion solvers and a Bayesian estimator under a somewhat natural hierarchical generative model for
the realizable setting in Section 4. We show that these two estimators are not equal in general, albeit
they have some similarities in their expressions.
We consider the following hierarchical probabilitistic model:
W0,?〜
N (0,J 力
iid
wt∣wo,? ~
N (w0,*,R^Id
Vt = XtWt + σzt where Zt 吧 N(0,In).
This model is similar to our realizable linear model (6), except that w0 has a prior and that there is
observation noise in the data (such that data likelihoods and posteriors are well-defined). We also
note that the R2∕d variance for Wt guarantees that E[kWt - W0,?k2] = R2, consistent with our
definition (7).
29
Under review as a conference paper at ICLR 2021
Bayesian estimator We now derive the Bayesian posterior mean estimator of w0,?, which requires
us to compute the posterior distribution of w0,? given the data {(Xt, yt)}tT=14.
We begin by computing the likelihood of one task by marginalizing over wt :
p(Xt,yt∣W0,?) H /p(wt∣W0,?) • p(yt∣Xt, wt)dwt
kwt - W0,?k2
H / exp

陵exp

2R2/d
l"k2 +1
2R2∕d + 2
• exp
+
1 >	> dσ2
H exp - 2w0,?	Xt Xt + R^
where (i) is obtained by integrating a multivariate Gaussian density over wt, and “H” drops all the
terms that do not depend on w0,?. Therefore, by the Bayes rule, the overall posterior distribution of
w0,? is given by
T
p w0,?|{(Xt,yt)}tT=1 H p(w0,?) • Yp(Xt, yt|w0,?)
t=1
H exp
(kwo,*k2
V	2σW∕d
T
exp
t=1
2 Id)	¾⅞ 卜0，? + w>?(x) Xt +
dσ2I )-1 X>yt
育 d)	R2∕d
This means that the posterior distribution of w0,? is Gaussian, with mean , i.e. the Bayesian estima-
tor, equal to5 *
wb0B,aTyes:=Ehw0,?|{(Xt,yt)}tT=1i =(ATBayes)-1cTBayes,
where
ATayes=σdwId+X (x>Xt+?Id)	¾Xt,
dσ2I )-1 X>yt
R2 dJ	R2/d'
We note that wb 0B,aTyes has a similar form as our train-train estimator, but is not exactly the same.
Indeed, recall the closed form of our train-train estimator is (cf. (10))
wb0tr,-Ttr = (AtTr-tr)-1ctTr-tr,
where
T
AT-tr = E (X> Xt + nλId)-2X> Xt,
t=1
T
cT-tr = E (X>Xt + nλId)-2X>yt.
t=1
Aswb0B,aTyes
uses the inverse and wb 0tr,-Ttr uses the squared inverse, these two sets of estimators are not
the same in general, no matter how we tune the λ in the train-train estimator. This is true even if we
set σw = ∞ so that the prior of w0,? becomes degenerate (and the Bayesian estimator reduces to
the MLE).
4Hereafter we treat Xt as fixed, as the density of Xt won’t affect the Bayesian calculation.
5Any density P(W) (X exp(-w> Aw/2 + w>c) specifies a Gaussian distreibution N(μ, Σ), where A
Σ-1 and C = Σ-1μ∕so that μ = A-1c.
30
Under review as a conference paper at ICLR 2021
D	Details on the few-shot image classification experiment
Here we provide additional details of the few-shot image classification experiment in Section 5.2.
Optimization and architecture For both methods, we run a few gradient steps on the inner argmin
problem to obtain (an approximation of) wt, and plug Wt into the Nw0';tr-val,tr-tr}(wo) (which
involves wt through implicit function differentiation) for optimizing w0 in the outer loop.
For both train-train and train-val methods, we use the standard 4-layer convolutional network
in (Finn et al., 2017; Zhou et al., 2019) as the backbone (i.e. the architecture for wt). We further
tune their hyper-parameters, such as the regularization constant λ, the learning rate (initial learning
rate and its decay strategy), and the gradient clipping threshold.
Datasets We experiment on miniImageNet (Ravi & Larochelle, 2017) and tieredImageNet (Ren
et al., 2018). MiniImageNet consists of 100 classes of images from ImageNet (Krizhevsky et al.,
2012) and each class has 600 images of resolution 84 × 84 × 3. We use 64 classes for training, 16
classes for validation, and the remaining 20 classes for testing (Ravi & Larochelle, 2017). Tiered-
ImageNet consists of 608 classes from the ILSVRC-12 data set (Russakovsky et al., 2015) and each
image is also of resolution 84 × 84 × 3. TieredImageNet groups classes into broader hierarchy
categories corresponding to higher-level nodes in the ImageNet. Specifically, its top hierarchy has
20 training categories (351 classes), 6 validation categories (97 classes) and 8 test categories (160
classes). This structure ensures that all training classes are distinct from the testing classes, provid-
ing a more realistic few-shot learning scenario.
D.1 Effect of the split ratio for the train-val method
We further tune the split (n1, n2) in the train-val method and report the results in Table 2. As can
be seen, as the number of test samples n2 increases, the percent classification accuracy on both the
miniImageNet and tieredImageNet datasets becomes higher. This testifies our theoretical affirmation
in Corollary 8. However, note that even if we take the best split (n1, n2) = (5, 25) (and compare
again with Table 1), the train-val method still performs worse than the train-train method.
We remark that our theoretical results on train-train performing better than train-val (in Section 4)
rely on the assumptions that the data can be exactly realized by the representation and contains no
label noise. Our experimental results here may suggest that the miniImageNet and tieredImageNet
few-shot tasks may have a similar structure (there exists a NN representation that almost perfectly
realizes the label with no noise) that allows the train-train method to perform better than the train-
val method.
Table 2: Investigation of the effects of training/validation splitting ratio in the train-val method
(iMAML) to the few-shot classification accuracy (%) on miniImageNet and tieredImageNet.
datasets	n1 = 25, n2 = 5	nι = 15,n2 = 15	n1 = 5, n2 = 25
miniImageNet	62.09 ± 0.97	63.56 ± 0.95	63.92 ± 1.04
tieredImageNet	66.45 ± 1.05	67.30 ± 0.98	67.50 ± 0.94
E	Comparis on with Cross-Validation on Synthetic Data
We test the effect of using cross-validation for the train-val method on the same synthetic data
(realizable linear centroid meta-learning) as in Section 5.1.
Method We fix the number of per-task data n = 20, and use 4-fold cross validation in the fol-
lowing two settings: (n1, n2) = (5, 15), and (n1 , n2) = (15, 5). In both cases, we partition the
data into 4 parts each with 5 data points, and we roulette over 4 possible partitions of which one as
train and which one as validation. The estimated optimal wb 0cv is obtained by minimize the averaged
31
Under review as a conference paper at ICLR 2021
train-val loss over the 4 partitions:
1 4	1	2
'Cv(w0) := 1£齐 /Vj-Xv jAλ(w0;Xtrainj,ytrain,j)||
j =1 val
1T
W0v = arg min - E'tv (WO),
w0 T t=1
where superscript j denotes the index of the cross-validation. The performance is depicted in Figure
2.
Figure 2: The scaled (by T) '2-error of W0tTT-tr,tr-val,cv} as the ratio d/n varies from 0 to 3 (n = 20
and T = 1000 are fixed). For the cross-validation method, the regularization coefficient λ = 0.5 is
tuned.
Result As showin in Figure 2, for both (n1, n2) = (15, 5) and (n1, n2) = (5, 15), using
cross-validation consistently beats the performance of the train-val method. This demonstrates the
variance-reduction effect of cross-validation. Note that the best performance (among the cross-
validation methods) is still achieved at n1 = 5, similar as for the vanilla train-val method. However,
numerically, the best cross-validation performance is still not as good as the train-train method.
Leave-one-out cross-validation Figure 3 left further tests with an increased number of per-task
samples n = 40, and incorporates the train-val method with the leave-one-out cross-validation, i.e.,
(n1, n2) = (39, 1) and (n1, n2) = (1, 39). We repeat the experiment 10 times for plotting the error
bar (shaded area). We see that the train-train method still outperforms the train-val method with
leave-one-out validation.
We further increase the per-task sample size n to 200, and test the leave-one-out method with a
sample split of (n1, n2) = (1, 199). We adopt a matrix inverse trick to mitigate the computational
overhead of finding Aλ (W0; Xttrain,j , yttrain,j ). To ease the computation, we also vary d from 0 to
400 on a coarse grid (with an increment of 80). From Figure 3 right, we see that the leave-one-out
method can slightly beat the train-train method for some d/n values. Compared to n = 20 and
n = 40 experiments, this is the first time of seeing leave-one-out method outperforms the train-
train method. We suspect that the per-task sample size n plays a vital role in the power of the
leave-one-out method: a large n tends to have a strong variance reduction effect in the leave-one-out
method, so that the performance can be improved. Yet using the leave-one-out method with a large
n invokes a high computational burden.
32
Under review as a conference paper at ICLR 2021
Figure 3: The scaled (by T) '2-error of Wb0tT-tr,cv} as the ratio d/n varies from 0 to 3 (n ∈ {40,200}
and T = 1000 are fixed). For the cross-validation method, the regularization coefficient λ = 0.5.
Left: n = 40. Leave-out-out CV performs worse than the train-train method. Right: n = 200.
Leave one-out CV appears better than the train-train method for d/n ∈ {1.2, 1.6}.
F NONASYMPTOTIC ANALYSIS UNDER GENERAL SCALINGS OF d, n, T
We sketch an nonasymptotic analysis of the train-train method in the realizable linear model in
Section 4. We assume in addition that Wt ± N(wo,?, RId)6.
We begin by recalling from Appendix B.1 and B.2 that the train-train and train-val estimators have
closed-form expressions
XT-1 T
At	X AtWt,	(44)
t=1
XT-1 T
Bt	BtWt,	(45)
t=1
where
n1
train> train
Bt := λ2(X-X- + λId
-1 Xtval> Xtval
n2
Xtrain>Xtrain	-1
—t-------- + λId
n1
Nonasymptotic result for Wb 0tr,-Ttr For simplicity, we restrict attention on the analysis of the train-
train estimator Wb t0r,-Ttr, whose closed form expression is given in (44). Analysis of the train-val esti-
mator can be done in a similar fashion.
We sketch a proof for the following
Result: Let T = Ω(d) and (d, n) is such that
ftr-tr (d, n)
E[d-M+λ)[=M
(dE [Pd=ι σ(n)∕(σ(n) + λ)2])
6This Gaussian assumption on wt is mainly for technical convenience, and can be relaxed to that wt,i -
w0,?,i are i.i.d. with variance R2/d and sub-Gaussian with parameter O(R2/d) without changing the proof.
33
Under review as a conference paper at ICLR 2021
(which for example holds if (d, n) are in the proportional limit), then with high probability we have
MSE(wb0tr,-Ttr)
1±O
TT ∙ (d ∙ ftr-tr(d, n) ± O (d,log；/、)
R2	1
≈ T ∙ ftr-tr(d,n) = T AsymMSE(W0r,Tr),
that is, the MSE for the train-train method concentrates around the asymptotic MSE.
Proof Sketch
We first define the matrix
which will be key to our analysis. Observe that conditioned on At (and only looking at the random-
ness of Wt), we have
Wb t0r,-Ttr - W0,? =
XT-1 T
At	X At(Wt - W0,?)
t=1
〜N(0, Rd2 ∑t
Therefore, applying the Hanson-Wright inequality (Vershynin, 2018, Theorem 6.2.1) yields that
MSE(W 舜)=∣∣w0r,-Tr-W0,?『e Rd2 (tr(∑τ) ± max {k∑τ ∣∣“ ^gl, k∑τ & log ；})
with probability at least 1 - δ. In the following, we argue that tr(ΣT) is the dominating term in
the above bound and concentrates around the asymptotic MSE in Lemma 5, whereas the error terms
within the max are lower order errors compared with tr(ΣT).
Concentration of tr(ΣT) Recall that At are i.i.d. PSD matrices in Rd×d. We have
We argue that I is the main term that depends on (d, n) and is independent of T . Further, A1 has
eigenvalues λ2σi/(λ + σi)2 where {σi}d=ι are the eigenvalues of X>Xi/n, and Ai has uniformly
distributed eigenvectors. Following the same analysis of Lemma 5, we see that
I = d2 ∙ Ptr-tr
E[d Pd=ι(σ(n))2∕(σ(n) + λ)4]
d ∙-----------------------------⅛
(dE[Pd=i σ(n)∕(σ(n) + λ)2]y
I-----------------------------
2.
_}
:=ftr-tr(d,n)
This is exactly the same quantity that appeared in the asymptotic MSE for the train-train method in
Theorem 4. In the following we assume that d, n is such that ftr-tr(d, n) = Θ(1).
We further argue that terms II and III are low-order terms compared with term I. Indeed, term I
is O(d). Applying matrix concentrations (e.g. the matrix Bernstein inequality), we can get that
terms II and In are of order max{，dlog(T∕δ)∕T, dlog(T∕δ)∕T} with probability at least 1 - δ.
Combining the terms yields that
tr(∑τ) ∈ τ
{d ∙ ftr-tr(d,n) ± dd10*) }
with high probability.
34
Under review as a conference paper at ICLR 2021
Controlling the error in the MSE We now control kΣT kFr and (consequently) kΣT kop. We
wish to show that
IIςTkFr ≤ O (√d) ∙ tr(^r)
with high probability. This can be seen from the combination of two results: (1) tr(∑τ) = Ω(d∕T)
by the preceding part, and (2)
1ςt kFr ≤ T ∙ λmin(Pt≤τ At)2
T
At2/T
t=1
≤ O( √d∕τ).
Fr
The above requires (1) k AtkFr ≤ O( Vd), which is true since we have 0 . At . XId and thus
∣∣A2^Fr ≤ λ2 √d, and (2) Xm® (£右＜丁 At/T) = Ω(1) with high probability, which is true whenever
T = Ω(d) because of matrix concentration and the fact that λmin(E[Aι]) = Ω(1).
Putting together We have with high probability that
MSE(wb0tr,-Ttr)
T .卜∙ ftr-tr(d, n) ± O (d JlogT≡))
as long as T = Ω(d) and d, n is such that ftr-tr(d, n) = Θ(1).
□
F.1 Synthetic Experiment
We verify our theoretical findings under general scalings of d, n, T. We adopt the data generating
model in Section 5.1 again. To compare the performance of the train-train and train-val methods,
we choose the number of tasks T = 300 and per-task sample size n = 100. We vary d from 0 to
300. Note that now T is comparable to n and d and much smaller than T = 1000 used in previous
experiments. Figure 4 shows the close fit of reference curves to the simulated `2 errors.
-B>w 二J-<
=0,
1+d/n
Figure 4: The scaled (by T) '2 -error of Wb0tTtr,tr-val} as the ratio d/n varies from 0 to 3 in the general
scaling setting (n = 100 and T = 300 are fixed). The regularization coefficient λ is fine tuned for
the train-train method and λ = 2000 for the train-val method.
35