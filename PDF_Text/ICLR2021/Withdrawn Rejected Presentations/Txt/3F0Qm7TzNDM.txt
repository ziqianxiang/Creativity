Under review as a conference paper at ICLR 2021
Variance Based Sample Weighting for
Supervised Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
In the context of supervised learning of a function by a Neural Network (NN), we
claim and empirically justify that a NN yields better results when the distribution
of the data set focuses on regions where the function to learn is steeper. We first
traduce this assumption in a mathematically workable way using Taylor expan-
sion. Then, theoretical derivations allow to construct a methodology that we call
Variance Based Samples Weighting (VBSW). VBSW uses local variance of the
labels to weight the training points. This methodology is general, scalable, cost
effective, and significantly increases the performances of a large class of NNs for
various classification and regression tasks on image, text and multivariate data.
We highlight its benefits with experiments involving NNs from shallow linear NN
to ResNet (He et al., 2015) or Bert (Devlin et al., 2019).
1	Introduction
When a Machine Learning (ML) model is used to learn from data, the distribution of the training data
set can have a strong impact on its performances. More specifically, in the context of Deep Learning
(DL), several works have hinted at the importance of the training set. In Bengio et al. (2009);
Matiisen et al. (2017), the authors exploit the observation that a human will benefit more from easy
examples than from harder ones at the beginning of a learning task. They construct a curriculum,
inducing a change in the distribution of the training data set that makes a Neural Network (NN)
achieve better results in an ML problem. With a different approach, Active Learning (Settles, 2012)
modifies dynamically the distribution of the training data, by selecting the data points that will make
the training more efficient. Finally, in Reinforcement Learning, the distribution of experiments is
crucial for the agent to learn efficiently. Nonetheless, the challenge of finding a good distribution is
not specific to ML. Indeed, in the context of Monte Carlo estimation of a quantity of interest based
on a random variable X 〜_dPx, Importance Sampling owes its efficiency to the construction of
a second random variable, X 〜dPχ that will be used instead of X to improve the estimation of
this quantity. Jie & Abbeel (2010) even make a connection between the success of likelihood ratio
policy gradients and importance sampling, which shows that ML and Monte Carlo estimation, both
distribution based methods, are closely linked.
In this paper, we leverage the importance of the training set distribution to improve performances of
NNs in supervised DL. This task can be formalized as approximating a function f with a model fθ
parametrized by θ. We build a new distribution from the training points and their labels, based on
the observation that fθ needs more data points to approximate f on the regions where it is steep. We
use Taylor expansion ofa function f, which links the local behaviour of f to its derivatives, to build
this distribution. We show that up to a certain order and locally, variance is an estimator of Taylor
expansion. It allows constructing a methodology called Variance Based Sample Weighting (VBSW)
that weights each training data points using the local variance of their neighbor labels to simulate
the new distribution. Sample weighting has already been explored in many works and for various
goals. Kumar et al. (2010); Jiang et al. (2015) use it to prioritize easier samples for the training,
Shrivastava et al. (2016) for hard example mining, Cui et al. (2019) to avoid class imbalance, or (Liu
& Tao, 2016) to solve noisy label problem. In this work, the weights’ construction relies on a more
general claim that can be applied to any data set and whose goal is to improve the performances of
the model.
1
Under review as a conference paper at ICLR 2021
VBSW is general, because it can be applied to any supervised ML problem based on a loss function.
In this work we specifically investigate VBSW application to DL. In that case, VBSW is applied
within the feature space of a pre-trained NN. We validate VBSW for DL by obtaining performance
improvement on various tasks like classification and regression of text, from Glue benchmark (Wang
et al., 2019), image, from MNIST (LeCun & Cortes, 2010) and Cifar10 (Krizhevsky et al.) and
multivariate data, from UCI ML repository (Dua & Graff, 2017), for several models ranging from
linear regression to Bert (Devlin et al., 2019) or ResNet20 (He et al., 2015). As a highlight, we obtain
up to 1.65% classification improvement on Cifar10 with a ResNet. Finally, we conduct analyses on
the complementarity of VBSW with other weighting techniques and its robustness.
Contributions: (i) We present and investigate a new approach of the learning problem, based on
the variations of the function f to learn. (ii) We construct a new simple, scalable, versatile and cost
effective methodology, VBSW, that exploits these findings in order to boost the performances of a
NN. (iii) We validate VBSW on various ML tasks.
2	Related Works
Active Learning - Our methodology is based on the consideration that not every sample bring
the same amount of information. Active learning (AL) exploits the same idea, in the sense that
it adapts the training strategy to the problem by introducing a data point selection rule. In (Gal
et al., 2017), the authors introduce a methodology based on Bayesian Neural Networks (BNN) to
adapt the selection of points used for the training. Using the variational properties of BNN, they
design a rule to focus the training on points that will reduce the prediction uncertainty of the NN. In
(Konyushkova et al., 2017), the construction of the selection rule is taken as a ML problem itself.
See (Settles, 2012) for a review of more classical AL methods. While AL selects the data points,
so modifies the distribution of the initial training data set, VBSW is applied independently of the
training so the distribution of the weights can not change throughout the training.
Examples Weighting - VBSW can be categorized as an example weighting algorithm. The idea
of weighting the data set has already been explored in different ways and for different purposes.
While curriculum learning (Bengio et al., 2009; Matiisen et al., 2017) starts the training with easier
examples, Self paced learning (Kumar et al., 2010; Jiang et al., 2015) downscales harder examples.
However, some works have proven that focusing on harder examples at the beginning of the learning
could accelerate it. In (Shrivastava et al., 2016), hard example mining is performed to give more
importance to harder examples by selecting them primarily. Example weighting is used in (Cui
et al., 2019) to tackle the class imbalance problem by weighting rarer, so harder examples. At the
contrary, in (Liu & Tao, 2016) it is used to solve the noisy label problem by focusing on cleaner,
so easier examples. All these ideas show that depending on the application, example weighting
can be performed in an opposed manner. Some works aim at going beyond this opposition by
proposing more general methodologies. In (Chang et al., 2017), the authors use the variance of
the prediction of each point throughout the training to decide whether it should be weighted or
not. A meta learning approach is proposed in (Ren et al., 2018), where the authors choose the
weights after an optimization loop included in the training. VBSW stands out from the previously
mentioned example weighting methods because it is built on a more general assumption that a model
would simply need more points to learn more complicated functions. Its effect is to improve the
performances of a NN, without solving data set specific problems like class imbalance or noisy
labels.
Importance Sampling - Some of the previously mentioned methods use importance sampling to de-
sign the weights of the data set orto correct the bias induced by the sample selection (Katharopoulos
& Fleuret, 2018). Here, we construct a new distribution that could be interpreted as an importance
distribution. However, we weight the data points to simulate this distribution, not to correct a bias
induced by this distribution.
Generalization Bound - Generalization bound for the learning theory of NN have motivated many
works, most of which are reviewed in (Jakubovitz et al., 2018). In Bartlett et al. (1998), Bartlett et al.
(2019), the authors focus on VC-dimension, a measure which depends on the number of parameters
of NNs. Arora et al. (2018) introduces a compression approach that aims at reducing the number
of model parameters to investigate its generalization capacities. PAC-Bayes analysis constructs
generalization bounds using a priori and a posteriori distributions over the possible models. It is
2
Under review as a conference paper at ICLR 2021
investigated for example in Neyshabur et al. (2018); Bartlett et al. (2017), and Neyshabur et al.
(2017); Xu & Mannor (2012) links PAC-Bayes theory to the notion of sharpness of a NN, i.e. its
robustness to small perturbation. While sharpness of the model is often mentioned in the previous
works, our bound includes the derivatives of f, which can be seen as an indicator of the sharpness
of the function to learn. Even if it uses elements of previous works, like the Lipschitz constant of
fθ, our work does not pretend to tighten and improve the already existing generalization bounds, but
only emphasizes the intuition that the NN would need more points to capture sharper functions. In
a sense, it investigates the robustness to perturbations in the input space, not in the parameter space.
3	A New Training Distribution based on Taylor Expansion
In this section, we first illustrate why a NN may need more points where f is steep by deriving a
generalization bound that involves the derivatives of f . Then, using Taylor expansion, we build a
new training distribution that improves the performances of a NN on simple functions.
3.1	Problem Formulation
We formalize the supervised ML task as approximating a function f : S ⊂ Rni → Rno with an ML
model fθ parametrized by θ, where S is a measured sub-space of Rni depending on the application.
To this end, We are given a training data set of N points, {Xι,…,Xn} ∈ S, drawn from X 〜dPχ
and their point-wise values, or labels {f(X1), ..., f (XN)}. Parameters θ have to be found in order
to minimize an integrated loss function JX (θ) = EX [L(fθ(X), f (X))], with L the loss function,
L : Rno X Rno → R. The data allow estimating JX (θ) by JX (θ) = N PN=I L(fθ(Xi), f (Xi)).
Then, an optimization algorithm is used to find a minimum of JX (θ) w.r.t. θ.
3.2	Intuition behind Taylor Expansion
In the following, we illustrate the intuition with a Generalization Bound (GB) that include the
derivatives of f, provided that these derivatives exist. The goal of the approximation prob-
lem is to be able to generalize to points not seen during the training. The generalization error
JX (θ) = JX (θ) - JX(θ) thus needs to be as small as possible. Let Si, i ∈ {1, ..., N} be some
sub-spaces of S such that S = UN=I Si, TN=I Si = 0, and Xi ∈ S” Suppose that L is the squared
L2 error, ni = 1, f is differentiable and fθ is Kθ-Lipschitz. Provided that |Si| < 1, we show that
N	|S|3
Jx(θ) ≤ ∑(lf0(Xi)l + Kθ)2LiL + o(∣Sil4),
i=1	4
(1)
where |Si| is the volume of Si. The proof can be found in Appendix B. We see that on the regions
where f0(Xi) is higher, quantity |Si| has a stronger impact on the GB. This idea is illustrated on
Figure 1. Since |Si | can be seen as a metric for the local density of the data set (the smaller |Si | is,
Figure 1: Illustration of the GB. The maximum error (the GB), at order O(|Si|4), is obtained by comparing the
maximum variations of fθ, and the first order approximation of f, whose trends are given by Kθ and f0 (Xi).
We understand visually that because f0(X1) and f0(X3) are higher than f0(X2), the GB is improved more
efficiently by reducing S1 and S3 than S2.
3
Under review as a conference paper at ICLR 2021
the denser the data set is), the GB can be reduced more efficiently by adding more points around Xi
in these regions. This bound also involves Kθ, the Lipschitz constant of the NN, which has the same
impact than f0(Xi). It also illustrates the link between the Lipschitz constant and the generalization
error, which has been pointed out by several works like (Gouk et al., 2018), (Bartlett et al., 2017)
and (Qian & Wegman, 2019). Note that equation 1 only gives indications about n = 1. Indeed, this
GB only has illustration purposes. Its goal is to motivate the metric described in the next section,
which is based on Taylor expansion and therefore involves derivatives of order n > 1.
3.3	A Taylor expansion based metric
In this paragraph, we build a metric involving the derivatives of f. Using Taylor expansion at order
n on f and supposing that f is n times differentiable (multi index notation):
f (X + C)小 X ekf + O(Cn)∙	Dfn(X)= X
0≤∣k∣≤n	1≤∣k∣≤n
kd∣k ∙k Vect(∂kf(x))k
k!
. (2)
Quantity f(X + C) - f(X) gives an indication on how much f changes around X. By neglecting
the orders above Cn, it is then possible to find the regions of interest by focusing on Dfn , given by
equation 2, where Vect(X) denotes the vectorization of a tensor X and k.k the squared L2 norm.
Note that Dfn is evaluated using k∂kf(X)k instead of ∂kf(X) for derivatives not to cancel each
other. f will be steeper and more irregular in the regions where X → Dfn(X) is higher.
To focus the training set on these regions, one can use {Dfn(X1), ..., Dfn(XN)} to construct a
probability density function (pdf) and sample new data points from it. This sampling is evaluated
and validated in Appendix A for conciseness. Based on these experiments, we choose n = 2, i.e.
we use {Df2(X1), ..., Df2(XN)}. The good results obtained, presented in Appendix A confirm
our observation and motivate its application to complex DL problems.
4	Variance Based Samples Weighting (VB SW)
4.1	Preliminaries
The new distribution cannot always be applied as is, because we do not have access to f. Problem
1: {Df2 (X1), ..., Df2 (XN)} cannot be evaluated since it requires to compute the derivatives of
f. Moreover, it assumes that f is differentiable, which is often not true. Problem 2: even if
{Df2 (X1 ), ..., Df2 (XN )} could be computed and new points sampled, we could not obtain their
labels to complete the training data set.
Problem 1: Unavailability of derivatives To overcome problem 1, we construct a new metric
based on statistical estimation. In this paragraph, ni > 1 but no = 1. The following derivations
can be extended to no > 1 by applying it to f element-wise and then taking the sum across the no
dimensions. Let C 〜N(0, EIni) with E ∈ R+ and Ini the identity matrix of dimension n%. We
claim that
V ar(f (X + C)) = Df2(X) + O(kCk32).
The demonstration can be found in Appendix B. Using the unbiased estimator of variance, we thus
define new indices Df2 (X) by
k2
Dd(X) = k—i X (f(X + Ci) —f(X)),	(3)
with {C1, ..., Ck} k samples of C. The metric Df 2(X)	→	V ar(f (X + C)) and
k→∞
V ar(f (X + C)) = Df2(X) + O(kCk23), so Df2 (X) is a biased estimator of Df2(X), with
bias O(kCk32). Hence, when C → 0, Df 2(X) becomes an unbiased estimator of Df2 (X). It is possi-
ble to compute Df2 (X) from any set of points centered around X. Therefore, we compute Df 2(Xi)
for each i ∈ {1, ..., N} using the setSk(X) of k-nearest neighbors ofXi. We obtain Df2(Xi) using
4
Under review as a conference paper at ICLR 2021
1	1k 2
Dd2(Xi) = k-1	X	(f(χj)— kXf(χι)),
(4)
Xj ∈Sk (Xi )
l=1
The advantages of this formulation are twofold. First, Df2 can even be applied to non-differentiable
functions. Second, all we need is {f(X1), ..., f (XN)}. In other words, the points used by Df 2(Xi)
are those used for the training of the NN. Finally, while the definition of Df2 (x) is local, the
definition of Df2 (x) holds for any . Note that equation 4 can even be applied when the data
points are too sparse for the nearest neighbors of Xi to be considered as close to Xi . It can thus be
seen as a generalization of Df2(x), which tends towards Df2 (x) locally.
Problem 2: Unavailability of new labels To tackle problem 2, recall that the goal of the training
is to find θ* = argmmJX(θ), With JX(θ) = Nn Pi L(f(Xi),fθ(Xi)). With the new distribution
θN
based on previous derivations, the procedure is different. Since the training points are sampled using
Dde We no longer minimize JX(θ), but JX(θ) = N Pi L(f (Xi),fθ(Xi)), With X 〜dPχ the
new distribution. However, JX(θ) estimates
JX ⑹=[
S
L(f(x),fθ (x))dPX.
Let PX(x)dx = dPX, PX (x)dx = dPjX be the PdfS of X and X (note that Df α PX). Then,
JX(θ)= / L(f(x),fθ(x))pX(x)dPX.
S	PX (x)
The straightforward Monte Carlo estimator for this expression of JX (θ) is
d2
jX,2(θ) = N X L(f (Xi),fθ(Xi))PX⅛J T NN X L(f(Xi),fθ(Xi))f Xf.	(5)
Thus, JX (θ) can be estimated with the same points as JX (θ) by weighting them with Wi
Dd(Xi)
PX (Xi).
4.2	Hyperparameters of VBSW
The expression of wi involves Df2 (Xi), whose estimation has been the goal of the previous sec-
tions. However, it also involves PX , the distribution of the data. Just like for f, we do not have
access to PX. The estimation ofPX is a challenging task by itself, and standard density estimation
techniques such as K-nearest neighbors or Gaussian Mixture density estimation led to extreme esti-
mated values ofPX(Xi) in our experiments. Therefore, we decided to only apply ωi = Df 2(Xi)
as a first order approximation. In practice, we re-scale the weighting points to be between 1 and
m, a hyperparameter. As a result, VBSW has two hyperparameters: m and k. Their effects and
interactions are studied and discussed in Sections 5.1 and 5.4.
4.3	VBSW for Deep Learning
We specified that local variance could be computed using already existing points. This statement
implies to find the nearest neighbors of each point. In extremely high dimension spaces like image
spaces the curse of dimensionality makes nearest neighbors spurious. In addition, the structure of
the data may be highly irregular, and the concept of nearest neighbor misleading. Thus, it may be
irrelevant to evaluate D2f directly on this data.
One of the strength ofDL is to construct good representations of the data, embedded in lower dimen-
sional latent spaces. For instance, in Computer Vision, Convolutional Neural Networks (CNN)’s
deeper layers represent more abstract features. We could leverage this representational power of
NNs, and simply apply our methodology within this latent feature space.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Variance Based Samples Weighting (VBSW) for Deep learning
1:	Inputs: k, m, M
2:	Train M on the training set {(N,Xι),..., (N,Xn)}, {(N,f(Xι)),…,(N, f (Xn))}
3:	Construct M* by removing its last layer
一 ≤≈k. .	.. .	..	≤≈k. .	.. .
4:	Compute {Df2(M*(Xι)),..., Df2(M*(Xn))} using equation 4.
5:	Construct a new training data set {(wι, M*(Xι)),..., (wn, M* (XN))}
6:	Train fθ on {(w1, f (Xι)),…，(WN, f (Xn))} and add it to M*. The final model is Mf =
fθ ◦ M*
Variance Based Samples Weighting (VBSW) for DL is recapitulated in Algorithm 1. Here, M is the
initial NN whose feature space will be used to project the training data set and apply VBSW. Line 1:
m and k are hyperparameters that can be chosen jointly with all other hyperparameters, e.g. using a
random search. Line 2: The initial NN, M, is trained as usual. Notations {(N, Xi),…,(N ,xn )}
is equivalent to {Xi,…,XN}, because all the weights are the same (N). Line 3： The last fully
connected layer is discarded, resulting in a new model M*, and the training data set is projected in
the feature space. Line 4-5: equation 4 is applied to compute the weights wi that are used to weight
the projected data set. To perform nearest neighbors search, we use KD-Tree (Bentley, 1975). Line
6: The last layer is re-trained (which is often equivalent to fitting a linear model) using the weighted
data set and added to M* to obtain the final model Mf. As a result, Mf is a composition of the
already trained model M* and fθ trained using the weighted data set.
5	Experiments
We first test this methodology on toy datasets with linear models and small NNs. Then, to illustrate
how general VBSW can be, we consider various tasks in image classification, text regression and
classification. Finally, we study the robustness of VBSW and its complementarity with other sample
weighting techniques.
5.1	Toy experiments
VBSW is studied on a Double Moon (DM) classification, in the Boston Housing (BH) regression
and Breast Cancer (BC) classification data sets.
-1.5 -1.0 -0.5 0.0 0.5 1,0 15 2.0	-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0	-15 -1.0 -05 OX) 05 1.0 16 2.0
Figure 2: From left to right: (a) Double Moon (DM) data set. (b) Decision boundary with the baseline method.
(c) Heat map of the value of wi for each Xi (red is high and blue is low) and (d) Decision boundary with VBSW
method
For DM, Figure 2 (c) shows that the points with
highest wi (in red) are close to the boundary
between the two classes. Indeed, in classifica-
tion, VBSW can be interpreted as local label
agreement. We train a Multi Layer Perceptron
of 1 layer of 4 units, using Stochastic Gradient
Descent (SGD) and binary cross-entropy loss
function, on a 300 points training data set for
VBSW	baseline
-DM^^99.4, 94.44 ± 0.78	99, 92.06 ± 0.66
BH	13.31, 13.38 ± 0.01	14.05, 14.06 ± 0.01
BC	99.12, 97.6 ± 0.34	98.25, 97.5 ± 0.11
Table 1: best, mean + se for each method. The metric
used is accuracy for DM and BC and Mean Squared
Error for BH.
50 random seeds. In this experiment, VBSW, i.e. weighting the data set with wi is compared to
baseline where no weights are applied. Figure 2 (b) and (d) displays the decision boundary of best
6
Under review as a conference paper at ICLR 2021
fit for each method. VBSW provides a cleaner decision boundary than baseline. These pictures as
well as the results of Table 1 show the improvement obtained with VBSW.
For BH data set, a linear model is trained and for BC data set, a MLP of 1 layer and 30 units, with
a train-validation split of 80% - 20%. Both models are trained with ADAM (Kingma & Ba, 2014).
Since these data sets are small and the models are light, we study the effects of the choice of m
and k on the performances. Moreover, BH is a regression task and BC a classification task, so it
allows studying the effect of hyperparameters more extensively. We train the models for a grid of
20 × 20 different values of m and k. These hyperparameters seem to have a different impact on
performances for classification and regression. In both cases, low values for m yields better results,
but in classification, low values of k are better, unlike in regression. Details and visualization of this
experiment can be found in Appendix C. The best results obtained with this study are compared to
the best result of the same models trained without VBSW in Table 1.
5.2	MNIST AND CIFAR10
For MNIST, we train 40 LeNet 5, i.e. with 40 different random seeds, and then apply VBSW for 10
different random seeds, with ADAM optimizer and categorical cross-entropy loss. Note that in the
following, ADAM is used with the default parameters of its keras implementation. We record the
best value obtained from the 10 VBSW training. The same procedure is followed for Cifar10, except
that we train a ResNet20 for 50 random seeds and with data augmentation and learning rate decay.
The networks have been trained on 4 Nvidia K80 GPUs. The values of the hyperparameters used
can be found in Appendix C. We compare the test accuracy between LeNet 5 + VBSW, ResNet20
+ VBSW and the initial test accuracies of LeNet 5 and ResNet20 (baseline) for each of the initial
networks.
VBSW	baseline	gain per model
MNIST	99.09, 98.87 ± 0.01	98.99, 98.84 ± 0.01	0.15, 0.03 ± 0.01
Cifar10	91.30, 90.64 ± 0.07	91.01, 90.46 ± 0.10	1.65, 0.15 ± 0.04
Table 2:	best, mean + se for each method. The metric used is accuracy. For a model M, the gain g for this
model is given by g = max (acc(Mif) - acc(M)) with acc the accuracy and Mif the VBSW model trained
at the i-th random seed.
The results statistics are gathered in Table 2, which also displays statistics about the gain due to
VBSW for each model. The results on MNIST, for all statistics and for the gain are significantly
better than forVBSW than for baseline. For Cifar10, we get a 0.3% accuracy improvement for the
best model and up to 1.65% accuracy gain, meaning that among the 50 ResNet20s, there is one
whose accuracy has been improved by 1.65% by VBSW. Note that applying VBSW took less than
15 minutes on a laptop with an i7-7700HQ CPU. A visualization of the samples that were weighted
by the highest wi is given in Figure 3.
Figure 3: Samples from Cifar10 and MNIST with high wi. Those pictures are either unusual or difficult to
classify, even for a human (especially for MNIST).
5.3 RTE, STS-B AND MRPC
For this application, we do not pre-train Bert NN, like in the previous experiments, since it has been
originally built for Transfer Learning purposes. Therefore, its purpose is to be used as is and then
fine-tuned on any NLP data set see (Devlin et al., 2019). However, because of the small size of the
dataset and the high number of model parameters we chose not to fine-tune the Bert model, and only
to use the representations of the data sets in its feature space to apply VBSW. More specifically, we
use tiny-bert (Turc et al., 2019), which is a lighter version of the initial Bert NN. We train the linear
model with tensorflow, to be able to add the trained model on top of the Bert model and obtain a
7
Under review as a conference paper at ICLR 2021
unified model. RTE and MRPC are classification tasks, so we use binary cross-entropy loss function
to train our model. STS-B is a regression task so the model is trained with Mean Squared Error. All
the models are trained with ADAM optimizer. For each task, we compare the training of the linear
model with VBSW, and without VBSW (baseline). The results obtained with VBSW are better
overall, except for Pearson Correlation in STS-B, which is slightly worse than baseline (Table 3).
	VBSW	baseline__________
ml___________________________________________________________________________________________m2	ml	m2
^TE	61.73,	58.46 ±	0.15^^-	61.01,	58.09 ±	0.13^^-
STS-B	62.31,	62.20 ±	0.01 60.99, 60.88	± 0.01	61.88,	61.87 ±	0.01 60.98, 60.92	± 0.01
MRPC	72.30,	71.71 ±	0.03 82.64, 80.72	± 0.05	71.56,	70.92 ±	0.03 81.41, 80.02	± 0.07
Table 3:	best, mean + se for each method. For RTE the metric used is accuracy (m1). For MRPC, metric 1
(m1) is accuracy and metric 2 (m2) is F1 score. For STS-B, metric 1 (m1) is Spearman correlation and metric
2 (m2) is Pearson correlation.
5.4 Robustness of VBSW
VBSW relies on statistical estimation: the weights are based on local empirical variance, evaluated
using k points. In addition, they are rescaled using hyperparameter m. Section 5.1 and Appendix
C show that many different combinations of m and k and therefore many different values for the
weights improve the error. This behavior suggests that VBSW is quite robust to weights approxima-
tion error.
We also assess the robustness of VBSW to label noise. To that end, we train a ResNet20 on Cifar10
with four different noise levels. We randomly change the label of p% training points for four differ-
ent values of p (10, 20, 30 and 40).	We then apply	VBSW 30 times and evaluate the	performances
of the obtained NNs on a clean test	set. The results	are gathered in Table 4.
noise	10%	20%	30%	40%
original error_8743	85.75	84.05	81779
VBSW 87.76, 87.63 ± 0.01	86.03, 85.89 ± 0.01	84.35, 84.18 ± 0.02	82.48,	82.32	± 0.02
Table 4:	best, mean + se of the training of a ResNet20 on Cifar10 for different label noise levels.
These results illustrate the robustness of VBSW to labels noise.
The results show that VBSW is still effective despite label noise, which could be explained by two
observations. First, the weights of VBSW rely on statistical estimation, so perturbations in the labels
might have a limited impact on weights’ value. Second, as mentioned previously, VBSW is robust
to weights approximation error, so perturbation of the weights due to label noise may not critically
hurt the method. Although VBSW is robust to label noise, note that the goal of VBSW is not to
address noisy label problem, like discussed in Section 2. It may be more effective to use a sampling
technique specifically tailored for this situation - possibly jointly with VBSW, like in Section 5.5.
5.5 Complementarity of VBSW
Existing sample weighting techniques can be used jointly with VBSW by training the initial NN
M with the first sample weighting algorithm, and then applying VBSW on its feature space. To
illustrate this feature, we compare VBSW with the recently introduced Active Bias (AB) (Chang
et al., 2017). AB dynamically weights the samples based on the variance of the probability of
prediction of each points throughout the training. Here, we study the combined effects of AB and
VBSW for the training of a ResNet20 on Cifar10. Table 5 gathers the results of experiments for
different baselines: vanilla, for a regular training with Adam optimizer, AB for a training with AB,
VBSW for the application of VBSW on top of a regular training and VBSW + AB for a training
with AB and the application of VBSW. Unlike in section 5.2, we do not use data augmentation
or learning rate decay, to simplify the experiments (which explains the accuracy loss compared to
previous experiments).
These results demonstrate the competitiveness of VBSW compared with AB and their complemen-
tarity. Indeed, the accuracy obtained with VBSW is quite similar to AB and the best mean and max
accuracy is obtained for a NN trained with AB + VBSW. Note that in this experiment, the gain
8
Under review as a conference paper at ICLR 2021
accuracy (%)	VBSW gain per model
vanilla	75.88,74.55	± 0.11	-
AB	76.33, 75.14	± 0.09	-
VBSW	76.57, 74.94	± 0.10	0.94,	0.40	± 0.03
AB + VBSW 76.60, 75.33 ± 0.09	0.40, 0.014	± 0.02
Table 5: best, mean + se of the training of60 ResNet20s on Cifar10 for vanilla, VBSW, AB and AB
+ VBSW. Gain per model g is defined by g = max (acc(Mif) - acc(M)) with acc the accuracy
1≤i≤10	f
and Mif the VBSW model trained at the i-th random seed.
per model is lower for AB + VBSW than for VBSW alone. An explanation might be that AB is
already improving the NN performances compared to vanilla, so there is less room for accuracy
improvement by VBSW in that case.
6	Discussion & Future Work
Previous experiments demonstrate the performances improvement that VBSW can bring in practice.
In addition to these results, several advantages can be pointed out.
•	VBSW is validated on several different tasks, which makes it quite versatile. Moreover, the prob-
lem of high dimensionality and irregularity of f, which often arises in DL problems, is alleviated
by focusing on the latent space of NNs. This makes VBSW scalable. As a result, VBSW can
be applied to complex NNs such as ResNet, a complex CNN or Bert, for various ML tasks. Its
application to more diverse ML models is a perspective for future works.
•	The validation presented in this paper supports an original view of the learning problem, that
involves the local variations of f. The studies of Appendix A, that use the derivatives of the
function to learn to sample a more efficient training data set, support this approach as well.
•	VBSW allows to extend this original view to problems where the derivatives of f are not accessi-
ble, and sometimes not defined. Indeed, VBSW comes from Taylor expansion, which is specific
to derivable functions, but in the end can be applied regardless of the properties of f .
•	Finally, this method is cost effective. In most cases, it allows to quickly improve the performances
of a NN using a regular CPU. In terms of energy consumption, it is better than carrying on a whole
new training with a wider and deeper NN.
We first approximated pX to be uniform, because we could not approximate it correctly. This ap-
proximation still led to a an efficient methodology, but VBSW may benefit from a finer approxima-
tion of pX . Improving the approximation of pX is among our perspectives. Finally, the KD-tree
and even Approximate Nearest Neighbors algorithms struggle when the data set is too big. One
possibility to overcome this problem would be to parallelize their execution.
We only considered the cases where we have no access to f . However, there are ML applica-
tions where we do. For instance, in numerical simulations, for physical sciences, computational
economics or climatology, ML can be used for various reasons, e.g. sensitivity analysis, inverse
problems or to speed up computer codes (Zhu et al., 2019; Winovich et al., 2019; Feng et al., 2018).
In this context data comes from numerical models, so the derivatives of f are accessible and could
be directly used. Appendix A contains examples of such possible applications.
7	Conclusion
Our work is based on the observation that, in supervised learning, a function f is more difficult to
approximate by a NN in the regions where it is steeper. We mathematically traduced this intuition
and derived a generalization bound to illustrate it. Then, we constructed an original method, Vari-
ance Based Samples Weighting (VBSW), that uses the variance of the training samples to weight the
training data set and boosts the model’s performances. VBSW is simple to use and to implement,
because it only requires to compute statistics on the input space. In Deep Learning, applying VBSW
on the data set projected in the feature space of an already trained NN allows to reduce its error by
simply training its last layer. Although specifically investigated in Deep Learning, this method is
applicable to any loss function based supervised learning problem, scalable, cost effective, robust
and versatile. It is validated on several applications such as glue benchmark with Bert, for text
classification and regression and Cifar10 with a ResNet20, for image classification.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-
cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning
on heterogeneous systems, 2015. URL http://tensorflow.org/. Software available from
tensorflow.org.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Jennifer Dy and Andreas Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Ma-
Chine Learning Research, pp. 254-263, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018.
PMLR.
Peter L. Bartlett, Vitaly Maiorov, and Ron Meir. Almost linear vc dimension bounds for piecewise
polynomial networks. In Proceedings of the 11th International Conference on Neural Information
Processing Systems, NIPS’98, pp. 190-196, Cambridge, MA, USA, 1998. MIT Press.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 6240-6249.
Curran Associates, Inc., 2017.
Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension
and pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
Research, 20(63):1-17, 2019.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pp.
41-48, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-516-1. doi: 10.1145/1553374.
1553380.
Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Commun.
ACM, 18(9):509-517, September 1975. ISSN 0001-0782. doi: 10.1145/361002.361007.
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more
accurate neural networks by emphasizing high variance samples. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 1002-1012. Curran Associates, Inc., 2017.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In The IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), June 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
J. Feng, Q. Teng, X. He, and X. Wu. Accelerating multi-point statistics reconstruction method for
porous media via deep learning. Acta Materialia, 159:296-308, 2018.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,
pp. 1183-1192. JMLR.org, 2017.
10
Under review as a conference paper at ICLR 2021
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael Cree. Regularisation of neural networks
by enforcing lipschitz continuity. 04 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-
778, 2015.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural Networks, 2(5):359 - 366, 1989. ISSN 0893-6080. doi:
https://doi.org/10.1016/0893-6080(89)90020-8.
Daniel Jakubovitz, Raja Giryes, and Miguel R. D. Rodrigues. Generalization error in deep learning.
CoRR, abs/1808.01174, 2018.
Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G. Hauptmann. Self-paced cur-
riculum learning. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,
AAAI’15, pp. 2694-2700. AAAI Press, 2015. ISBN 0262511290.
Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood
ratio policy gradient. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and
A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 1000-1008. Curran
Associates, Inc., 2010.
Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. In ICML, 2018.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 12 2014.
Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua. Learning active learning from data. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 30, pp. 4225-4235. Curran Asso-
ciates, Inc., 2017.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-
search).
M. P. Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models.
In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances
in Neural Information Processing Systems 23, pp. 1189-1197. Curran Associates, Inc., 2010.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Trans. Pattern Anal. Mach. Intell., 38(3):447-461, March 2016. ISSN 0162-8828. doi: 10.1109/
TPAMI.2015.2456899.
Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learn-
ing, 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro. Exploring general-
ization in deep learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
5947-5956. Curran Associates, Inc., 2017.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learn-
ing Representations, 2018.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
11
Under review as a conference paper at ICLR 2021
Haifeng Qian and Mark N. Wegman. L2-nonexpansive neural networks. In International Conference
on Learning Representations, 2019.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. CoRR, abs/1803.09050, 2018.
Burr Settles. Active Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning.
Morgan & Claypool Publishers, 2012.
Abhinav Shrivastava, Abhinav Gupta, and Ross B. Girshick. Training region-based object detectors
with online hard example mining. CoRR, abs/1604.03540, 2016.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2, 2019.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Learning Representations, 2019.
Nick Winovich, Karthik Ramani, and Guang Lin. Convpde-uq: Convolutional neural networks with
quantified uncertainty for heterogeneous elliptic partial differential equations on varied domains.
Journal of Computational Physics, 394:263-279, 2019. ISSN 0021-9991. doi: https://doi.org/
10.1016/j.jcp.2019.05.026.
Huan Xu and Shie Mannor. Robustness and generalization. Machine Learning, 86(3):391-423, Mar
2012. ISSN 1573-0565. doi: 10.1007/s10994-011-5268-1.
Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. Physics-
constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification
without labeled data. Journal of Computational Physics, 394:56 - 81, 2019. ISSN 0021-9991.
doi: https://doi.org/10.1016/j.jcp.2019.05.024.
12
Under review as a conference paper at ICLR 2021
Appendix A Taylor Based Sampling
In this part, we empirically verify that using Taylor expansion to construct anew training distribution
has a beneficial impact on the performances of a NN. To this end, we construct a methodology, that
we call Taylor Based Sampling (TBS), that generates a new training data set based on the metric
introduced in Section 3.3. First, we recall the formula for {Dfn(X1), ..., Dfn(XN)}.
皿、 X kd∣k ∙k Vect(∂kf (x))k	体、
Dfe(X)=工---------------k!---------.	⑹
1≤∣k∣≤n
To focus the training set on the regions of interest, i.e. regions of high {Dfn(X1), ..., Dfn(XN)},
we use this metric to construct a probability density function (pdf). This is possible since Dfen(x) ≥
0 for all x ∈ S. It remains to normalize it but in practice it is enough considering a distribution
d α Dfn Here, to approximate d We use a Gaussian Mixture Model(GMM) with Pdf "gmm that We
fitto {Dfen(X1), ..., Dfen(XN)} using the Expectation-Maximization (EM) algorithm. N0 new data
points {Xι,…,XNo}, can be sampled, with X 〜 "gmm∙ Finally, we obtain {f (Xi),…,f (XN0)},
add it to {f(X1), ..., f(XN)} and train our NN on the Whole data set.
Taylor Based Sampling (TB S)
TBS is described in Algorithm 2. Line 1: The choice criterion of , the number of Gaussian dis-
tribution nGMM and N0 is to avoid sparsity of {Xi,…，Xno } over S. Line 2: Without a priori
information on f, we sample the first points uniformly in a subspace S. Line 3-7: We construct
{Dfen(Xi), ..., Dfen(XN)}, and then d to be able to sample points accordingly. Line 8: Because
the support of a GMM is not bounded, some points can be sampled outside S. We discard these
points and sample until all points are inside S. This rejection method is equivalent to sampling
points from a truncated GMM. Line 9-10: We construct the labels and add the new points to the
initial data set.
Algorithm 2 Taylor Based Sampling (TBS)
1:	Inputs: , N, N0, nGMM, n
2:	Sample {Xi,...,Xn}, with X 〜U(S)
3:	for 0 ≤ k ≤ n do
4:	Compute {∂kf(Xi), ..., ∂kf(XN)}
5:	Compute {Dfen(Xi), ..., Dfen (XN)} using equation 2
6:	Approximate d 〜 De with a GMM using EM algorithm to obtain a density dGMM
7:	Sample {Xi,：., XN，} using rejection method to sample inside S
8:	Compute {f (Xi),…J(Xy)}
9:	Add {f (Xi),..., f (Xn，)} to {f (Xi), ...,f (Xn)}
Application to simple functions
In order to illustrate the benefits of TBS com-
pared to a uniform, basic sampling (BS), we ap-
ply it to two simple functions: hyperbolic tan-
gent and Runge function. We chose these func-
tions because they are differentiable and have a
clear distinction between flat and steep regions.
These functions are displayed in Figure 4, as
well as the map x → Dfe2 (x).
All NN have been implemented in Python,
with Tensorflow Abadi et al. (2015). We
use the Python package scikit-learn Pe-
dregosa et al. (2011), and more specifically
Sampling L? error	L∞
f: RUnge (×10-2)
BS	1.45 ± 0.62	5.31 ± 0.86
TBS	1.13 ± 0.73	3.87 ± 0.48
f: tanh (×10T)
BS	1.39 ± 0.67	2.75 ± 0.78
TBS	0.95 ± 0.50	2.25 ± 0.61
Table 6: Comparison between BS and TBS. The
metrics used are the L2 and L∞ errors, displayed
with a 95% confidence interval.
the GaussianMixture class to construct
dGMM. The network chosen for this experiment is a Multi Layer Perceptron (MLP) with one layer
of 8 neurons, with Relu activation function, that we trained alternatively with BS and TBS using
13
Under review as a conference paper at ICLR 2021
Figure 4: Left: (left axis) Runge function w.r.t x and (right axis) x → Df2 (x). Points sampled
using TBS are plotted on the x-axis and projected on f . Right: Same as left, with hyperbolic
tangent function.
Adam optimizer Kingma & Ba (2014) with the defaults tensorflow implementation hyperparame-
ters, and Mean Squared Error loss function. We first sample {X1, ..., XN} according to a regular
grid. To compare the two methods, we add N0 additional points sampled using BS to create the BS
data set, and then N0 other points sampled with TBS to construct the TBS data set. As a result, each
data set have the same number of points (N + N0). We repeated the method for several values of n,
nGMM and , and finally selected n = 2, nGMM = 3 and = 10-3 .
Table 6 summarizes the L2 and the L∞ norm of the error of fθ, obtained at the end of the training
phase for N + N0 = 16, with N = 8. Those norms are estimated using a same test data set of
1000 points. The values are the means of the 40 independent experiments displayed with a 95%
confidence interval. These results illustrate the benefits of TBS over BS. Table 6 shows that TBS
slightly degrades the L2 error of the NN, but improves its L∞ error. This may explain the good
results of VBSW for classification. Indeed, for a classification task, the accuracy will not be very
sensitive to small output variations, since the output is rounded to 0 or 1. However, a high error can
induce a misclassification, and the reduction in L∞ norm limits this risk.
Application to an ODE system
We apply TBS to a more realistic case: the approximation of the resolution of the Bateman equations,
which is an ODE system :
∂tu(t)
∂tη(t)
vσa ∙ η(t)u(t),
V∑r ∙ η(t)u(t),
u(0) = u0,
with initial conditions	,
η(0) = η0.
with u ∈ R+, η ∈ (R+)M, σaT ∈ RM, Σr ∈ RM×M. Here, f : (u0, η0, t) → (u(t), η(t)).
For physical applications, M ranges from tens to thousands. We consider the particular case M = 1
so that f : R3 → R2, with f(u0, η0, t) = (u(t), η(t)). The advantage of M = 1 is that we have
access to an analytic, cheap to compute solution for f . Of course, this particular case can also be
solved using a classical ODE solver, which allows us to test it end to end. It can thus be generalized
to higher dimensions (M > 1).
All NN trainings have been performed in Python, with Tensorflow Abadi et al. (2015). We
used a fully connected NN with hyperparameters chosen using a simple grid search. The final
values are: 2 hidden layers, ”ReLU” activation function, and 32 units for each layer, trained with the
Mean Squared Error (MSE) loss function using Adam optimization algorithm with a batch size of
50000, for 40000 epochs and on N + N0 = 50000 points, with N = N0 . We first trained the model
for (u(t), η(t)) ∈ R, with an uniform sampling (BS) (N0 = 0), and then with TBS for several
values of n, nGMM and = (1, 1, 1), to be able to find good values. We finally select = 5 × 10-4,
n = 2 and nGMM = 10. The data points used in this case have been sampled with an explicit Euler
14
Under review as a conference paper at ICLR 2021
scheme. This experiment has been repeated 50 times to ensure statistical significance of the results.
Table 7 summarizes the MSE, i.e. the L2 norm of the error of fθ and L∞ norm, with L∞ (θ) =
max(∣f (X) - fθ(X)|) obtained at the end of the training phase. This last metric is important
X∈S
because the goal in computational physics is not only to be averagely accurate, which is measured
with MSE, but to be accurate over the whole input space S. Those norms are estimated using a same
test data set of Ntest = 50000 points. The values are the means of the 50 independent experiments
displayed with a 95% confidence interval. These results reflect an error reduction of 6.6% for L2
and of 45.3% for L∞, which means that TBS mostly improves the L∞ error of fθ . Moreover, the
L∞ error confidence intervals do not intersect so the gain is statistically significant for this norm.
Table 7: Comparison between BS and TBS
Sampling	L2 error (×10-4)	L∞ (×10T)	AEG(×10-2)	AEL(×10-2)
BS	1.22 ± 0.13	5.28 ± 0.47	-	-
TBS	1.14 ± 0.15	2.96 ± 0.37	2.51 ± 0.07	0.42 ± 0.008
Figure 2a
1.0
0.0
0
S 0.5
0.08
1
2
η0
0.06
g 3
Q
~~T 0.10
0.04
0.02
0.00
1.0
0.0
0
Figure 2b
S 0.5
0.010
0.000
-0.010
1
2
η0
0.005
-0.005
Figure 5: 1a: t → fθ(u0, η0, t) for randomly chosen (u0, η0), for fθ obtained with the two sam-
plings. 1b: t → fθ(u0, η0, t) for (u0, η0) resulting in the highest point-wise error with the two
samplings. 2a: u0 , η0 →
max Dn (u0 , η0, t) w.r.t.
0≤t≤10
(u0, η0). 2b: u0, η0 → gθBS (u0, η0) -
gθT BS (u0 , η0 ),
Figure 1a shows how the NN can perform for an average prediction. Figure 1b illustrates the
benefits of TBS relative to BS on the L∞ error (Figure 2b). These 2 figures confirm the previous
observation about the gain in L∞ error. Finally, Figure 2a displays u0 , η0 →
max Dn(u0, η0, t)
0≤t≤10
w.r.t. (u0, η0) and shows that Dn increases when U0 → 0. TBS hence focuses on this region.
Note that for the readability of these plots, the values are capped to 0.10. Otherwise only few
points with high Dn are visible. Figure 2b displays u0, η0 → gθBS (u0, η0) - gθTBS (u0, η0), with
gθ : u0, η0 → max kf(u0, η0, t) - fθ(u0, η0, t)k22 where θBS and θTBS denote the parameters
obtained after a training with BS and TBS, respectively. It can be interpreted as the error reduction
achieved with TBS.
15
Under review as a conference paper at ICLR 2021
The highest error reduction occurs in the expected region. Indeed, more points are sampled where
Dn is higher. The error is slightly increased in the rest of S, which could be explained by a sparser
sampling on this region. However, as summarized in Table 1, the average error loss (AEL) of TBS
is around six times lower than the average error gain (AEG), with AEG = Eu0,η0 (Z1Z>0) and
AEL = Eu0,η0(Z1Z<0) where Z(u0, η0) = gθBS (u0, η0) - gθTBS (u0, η0). In practice, AEG and
AEL are estimated using uniform grid integration, and averaged on the 50 experiments.
Appendix B: Demonstrations
Intuition behind Taylor Expansion (Section 3.2)
We want to approximate f : x → f (x), x ∈ Rni, f(x) ∈ Rno with a NN fθ. The goal of
the approximation problem can be seen as being able to generalize to points not seen during the
training. We thus want the generalization error JX (θ) to be as small as possible. Given an initial
data set {Xι,…，Xn} drawn from X 〜 dPχ and {f (Xi),…，f (XN)}, and the loss function L
being the squared L2 error, recall that the integrated error JX (θ), its estimation JX (θ) and the
generalization error JX (θ) can be written:
JX(θ) =	kf(x)-fθ(x)kdPX,
S
1N
C (θ) = nn Ekfθ (Xi) - f (Xi )∣∣,
(7)
i=1
JX(θ) = JX(θ) - JcX(θ),
where k.k denotes the squared L2 norm. In the following, we find an upper bound for JX (θ). We
start by finding an upper bound for JX(θ) and then for JX (θ) using equation 7.
Let Si, i ∈ {1, ..., N} be some sub-spaces of a bounded space S such that S = SiN=1 Si, TiN=1 Si =
0, and Xi ∈ Si. Then,
N
JX(θ) =X
i=1 Si
kf(x)-fθ(x)kdPX,
N
JX(θ) =	kf(Xi+x-Xi) -fθ(x)kdPX.
i=1 Si
Suppose that ni = no = 1 and f twice differentiable. Let |S| = S dPX. The volume |S| = 1 since
dPX is a probability measure, and therefore |Si| < 1 for all i ∈ {1, ..., N } . Using Taylor expansion
at order 2, and since |Si | < 1 for all i ∈ {1, ..., N }
N1
Jx (θ) = N Js kf (Xi) + f 0(Xi)(x - Xi) + 2 f00(Xi )(x - Xi)2 - fθ (x) + O((X - Xi)3)kdPχ.
To find an upper bound for J (θ), we can first find an upper bound for |Ai(x)|, with Ai(x) =
f (Xi) + f(Xi)(χ - Xi) + 2 f0(Xi )(x - Xiy- fθ (x) + O((X - Xi)3).
NN fθ is K -Lipschitz, so since S is bounded (So are Si), for all X ∈ Si, ∣fθ (x) - fθ (Xi )| ≤
Kθ |x - Xi |. Hence,
16
Under review as a conference paper at ICLR 2021
fθ (Xi)- Kθ Ix - Xi I ≤ fθ (x) ≤ fθ (Xi) + Kθ Ix - Xi∖,
-fθ (Xi)- Kθ ∖x - Xi∖ ≤ -fθ (x) ≤ -fθ (Xi) + Kθ ∖x - Xi∖,
f (Xi) + f(Xi)(x - Xi) + 2 f(Xi(x - Xi)2) - fθ(Xi) - Kθ∖χ - Xi∖ + O((x - Xi)3)
≤ Ai(x) ≤ f (Xi) + f0(Xi)(x - Xi) + 2f00(Xi)(X - Xi)2 - fθ(Xi) + Kθ∖x - Xi∖ + O((x - Xi)3),
Ai(χ) ≤ f (Xi) - fθ(Xi) + f0(Xi)(x - Xi) + 2f00(Xi)(X - Xi)2 + Kθ∖x - Xi∖ + O((x - Xi)3).
And finally, using triangular inequality,
Ai(X) ≤ ∖f (Xi) - fθ(Xi)∖ + ∖f0(Xi)∖∖x - Xi∖ + 1 ∖f00(Xi)∖∖x - Xi∖2 + Kθ∖x - Xi∖ + O(∖x - Xi∖3).
Now, ∣∣.∣∣ being the squared L2 norm:
N f	1
Jx(θ) = N Js kf (Xi) + f0(Xi)(x - Xi) + 2f00(Xi)(x - Xi)2 - fθ(x) + O(∖x - Xi∖3)kdPχ,
N
N
Xs (
i=1 JSi
Jx (θ) ≤
i=1
∖f (Xi)- fθ (Xi)I) + (∖f0(Xi)IIX - Xi∖ + 2 ∖f 00(Xi)IIX - Xi∖2 + Kθ ∖x - Xi ∖
2
+ O(∖X - Xi∖3) dPχ,
N
X Zs
i=1 JSi
∖f(Xi) - fθ(Xi)∖2
+ 2∖f (Xi)- fθ (Xi) ∖ (∖f0(Xi)IIX - Xi ∖ + 2 ∖f 00(Xi)IIX - Xi ∖2 + kθ ∖x - XiI)
+ [(∖f 0(Xi )∖∖x - Xi∖) + (2 ∖f 00 (Xi)IIX - Xi ∖2 + κθ ∖x - Xi∖)i + O(IX - Xi∖3) dPX,
N
XZ
i=1 JS
If(Xi)- fθ (Xi) ∖2
+ 2∖f (Xi)- fθ (Xi) ∖ (∖f0(Xi)IIX - Xi ∖ + 2 ∖f 00(Xi)IIX - Xi ∖2 + κθ ∖x - Xi ∖
+ [∖f0(Xi)∖2∖x - XiI2 + 2KθIf0(Xi)∖∖x - XiI2 + κ2∖x - XiI2] + O(∖x - Xi∖3)
dPχ,
N
XJS
i=1 JSi
∖f(Xi) - fθ(Xi)∖2
+ 2∖f (Xi)- fθ (Xi)I (∖f0(Xi)IIX - Xi ∖ + 2 ∖f 00(Xi)IIX - Xi ∖2 + κθ ∖x - Xi ∖
+ (If0 (Xi)I + Kθ)2
∖x - XiI2 + O(∖x - Xi∖3) dPχ.
Hornik,s theorem (Hornik et al., 1989) states that given a norm ||.|鼠* = such that IlfIIp,* =
JS If (x)∖pdμ(x), with dμ a probability measure, for any e, there exists θ such that for a Multi
Layer Perceptron, fθ, ∣If(X) — fθ(X)IIp,μ < 3
17
Under review as a conference paper at ICLR 2021
This theorem grants that for any G with dμ = PN=I Nδ(x - Xi), there exists θ such that
1
kf(x) - fθ (x)k1,μ = E NN If (Xi) - fθ (Xi)I ≤ e,
i=1
iN1 1	⑻
kf (x) - fθ(x)k2,μ = X NN (f (Xi) - fθ(Xi))2 ≤ e∙
、	i=1
Let's introduce i* such that i* = argmin |Sj. Note that for any i ∈ {1,…，N}, O(∣S*∣4) is O(∣S∕4).
Now, let’s choose E such that E = O(∣S*∣4). Then, equation 8 implies that
Pf(Xi)- fθ (Xi)I = o(∣Si∣4),
卜f(Xi) - fθ(Xi))2 = o(∣Si∣4),
I
IJX (θ) = kf(x)-fθ (x)k2,μ = O(∣Si∣4).
Thus, we have JX (θ) = JX (θ) - JcX (θ) = JX (θ) + O(ISiI4) and therefore,
N2
JX(θ) ≤ X	If0(Xi)I+Kθ	Ix-XiI2dPX + O(ISiI4).
i=1 Si
Finally,
N	ISI3
Jx(θ) ≤ ∑(If0(Xi)I + Kθ)2LiL + o(∣Si∣4).
i=1	3
(9)
We see that on the regions where f0(Xi) + Kθ is higher, quantity ISiI (the volume of Si) has a
stronger impact on the GB. Then, since ISiI can be seen as a metric for the local density of the data
set (the smaller ISiI is, the denser the data set is), the Generalization Bound (GB) can be reduced
more efficiently by adding more points around Xi in these regions. This bound also involves Kθ,
the Lipschitz constant of the NN, which has the same impact as f0(Xi). It also illustrates the link
between the Lipschitz constant and the generalization error, which has been pointed out by several
works like, for instance, Gouk et al. (2018), Bartlett et al. (2017) and Qian & Wegman (2019).
Problem 1: Unavailab ility of derivatives (Section 4.1)
In this paragraph, we consider ni > 1 but no = 1. The following derivations can be extended to
no > 1 by applying it to f element-wise. Let E 〜N(0, EIni) with E ∈ R+ and E = (ei,…，EnJ,
i.e. Ei 〜N(0, e). Using Taylor expansion on f at order 2 gives
f(x + E)= f (x) + Vxf(X) ∙ E + 1ET ∙ Hxf (x) ∙ E + O(kεk3).
With Vxf and Hxf (x) the gradient and the Hessian of f w.r.t. x. We now compute Var(f (X + E))
and make Df2(x) = EkVxf (X)IIF + 11 e2∣∣Hxf (χ)kF appear in its expression to establish a link
between these two quantities:
Var(f(x + e)) = Var(f(x) + Vxf(x) ∙ E + 2ET ∙ Hxf(x) ∙ E +。(|怕|3)),
=Var(Vxf(X) ∙ e + 2ET ∙ Hxf (x) ∙ e) + O(kE∣1).
Since Ei 〜N(0, e), X = (xι,..., Xni) and with ∂⅛~(X) the cross derivatives of f w.r.t. Xi and Xj,
Vxf(X) ∙ e + 2ET
ni	∂f 1 ni ni	∂2f
・ Hxf (x) ∙ E =X Ei 念X)+ 2 XXEjEk ∂x x (X)，
i=1	j=1 k=1
18
Under review as a conference paper at ICLR 2021
/	ι E	、	/3 ∂f 1 二工	∂2f
V	ar(Vxf(x) ∙ e + 2e ∙ Hxf(X) ∙ e) =Var(XEi看(X) + 2 XXEjEk^^
i=1	i	j=1 k=1	j k
ni ni
∑∑Cov
ii =1 i2 = 1
(X)0
1	ni	ni	ni n
+1XXXX S
ji=1 ki=1 j2=1 k2=1
∂ 2f
EjI EkI ∂Xjι Xki
∂2f
∂Xj2 Xk2
ni ni ni
+XXX COVa
i=1j=1k=1
ni ni 口 _c
f (X), Ej Ek
∂xj Xk
ii=1 i2=1	ii
(x) f (X)COVkii ,Ei2)
∂Xi2	∖	)
ni ni	ni	ni
+4 XXXX
ji=1 ki=1 j2=1 k2=1
∂Xjι Xki
(X)
∂2f
∂Xj2 Xk2
(X)Cov(eh€ki ,e7-2€k2
(x), €j2 €k2
∂ 2f
∂ 2f
ni ni ni ∂f	∂2f
+ X X X ∂Xi (x) ∂XjXk(X)COV (Ei, Ej Ek).
In this expression, we have to assess three quantities:	Cov(eiι, €^), Cov(ei, EjEk) and
COV(CjI EkI , Ej2 Ek2 ).
First, since(€1,…,Eni) are i.i.d.,
V /	Var(Ei) = E if i1 = i2 = i,
OV Eii , Ei2	0 otherwise.
To assess COV(Ei, EjEk), three cases have to be considered.
•	If i = j = k, because E[e3] = 0,
Cov(Ei, Ej Ek ) = COV(Ei, E2),
=E[e3] - E[Ei]E[E2],
=0.
•	If i = j or i = k (we consider i = k, and the result holds for i = j by commutativity),
COV(Ei, EjEk ) = COV(Ei, EiEj ),
=EiE2Ej] - EiEi]E∣EiEj],
=E[e2]E[ej-],
= 0.
•	If i = j and i = k, Ei and EjEk are independent and so COV(Ei, EjEk) = 0.
Finally, to assess Cov(Eji Eki, Ej2Ek2), four cases have to be considered:
•	If j1 = j2 = k1 = k2 = i,
COV(Eji Eki , Ej2 Ek2 ) = Var(E2),
= 2e2 .
• If j1 = k1 = i and j2 = k2
independent.
j, Cov(EjiEki, Ej2Ek2) = COV(Ei2, Ej) = 0 since e2 and E are
19
Under review as a conference paper at ICLR 2021
•	If jι = j2 = j and kι =阮=k,
COV(CjI Cki ,j Ck2) = Var(Cj Ck),
=Var(Cj )Var(Ck),
= C2.
•	If jι = k1,j2 and k2,
COV(CjI ekι , ej2 ek2 ) = E[ej1 ekι ej2 ek2 ] - EkjI ekι ]E[ej2 ek2 ],
=E[eji]EkkI ej2 ek2 ] - Ekji]E[eki]Ekj2 ek2 ],
=0.
All other possible cases can be assessed using the previous results, commutativity and symmetry of
Cov operator. Hence,
n n
Var(Vxf(x) ∙ € + 1 eτ ∙ Hxf(X) ∙ e
iι = 1 i2 = l
(x) f (X)COVkiI ,Ci2)
∂Xi2	∖	)
I	ni	ni	ni ni
+4 xxxx
j1=1 k1=1 j2=1 k2=1
∂ 2f
∂Xj1 Xk1
(x)
∂2f
∂xj2 Xk2
(X)Covlejlefcι ,je% ),
ni 2 _r2	1	ni	ni
X a(X)+2χ X c2
∂2f 2
∂xj Xk
(χ),
=CkVj(X)IIF + 2 C2∣∣Hχf(x)∣∣F,
=Df2(x).
And finally,
Var(f (x + e)) = Df∣(x) + O(∣∣e∣∣3)
(10)
If we consider Df2e(x) as defined in equation 2, on section 3.3 of the main document,
Df2e(x) → Var(f (x + e)) . Since Var(f (x + e)) = Df2(x) + O(Il€13), Df 2e(x) is a bi-
k-∞
ased estimator of Df2(x), with bias O(Il€13). Hence, when C → 0, Df 2e(x) becomes an unbiased
estimator of Df (x).
20
Under review as a conference paper at ICLR 2021
Appendix C: Hyperparameters
This appendix Section is split in two parts. First, we describe the results of the experiments on the
hyperparameters search of Boston Housing (BH) and Breast Cancer (BC) data sets (Section 5.1).
The second part is a list of final hyperparameters values chosen for the experiments of the main
paper.
Experiments on Boston Housing and Breast Cancer data sets
For BH and BC experiments, we conduct a grid search for VBSW on the values of m and k. As a
reminder, m is the ratio between the highest and the lowest weights, and k is the number of neighbor
points used to compute the local variance. We train a linear model for BH and a MLP with 30 units
for BC with VBSW on a grid of 20 values of m equally distributed between 2 and 100 and 20 values
of k equally distributed between 10 and 50. As a result, we train the model on 400 pairs of (m, k)
values, and with 10 different random seeds for each pair.
Figure 6: Color map of the error, with respect to m and k. Left: BH data set, for the mean of the
MSE accross 10 different seeds and right: BC data set, for the mean of 1 - acc across these seeds.
Blue is lower.
These experiments, illustrated in Figure 6 shows that the influence of m and k on the performances
of the model can be different. For BH data set, low values of k clearly lead to poorer performances.
Hyperparameter m seems to have less impact, although it should be chosen not too far from its
lowest value, 2. For BC data set, at the contrary, the best performances are obtained for low values
of k, while m could be chosen in high values. These experiments highlight that the impact of m and
k can be different between classification and regression, but it could also be different depending on
the data set. Hence, we recommend considering these hyperparameters like many other involved in
DL, and to select their values using classical hyperparameters optimization techniques.
This also shows that many different (m, k) pairs lead to error improvement. This suggests that the
weights approximation does not have to be exact in order for VBSW to be effective, like stated in
Section 5.4.
21
Under review as a conference paper at ICLR 2021
Paper hyperparameters values
The values chosen for the hyperparameters of the paper experiments are gathered in Table 8. For
ADAM optimizer hyperparameters, we kept the default values of Keras implementation. We chose
these hyperparameters after simple grid searches.
Experiment	m	k	learning rate	batch size	epochs	optimizer	random seeds
double moon	100	20	1 × 10-3	100	10000	SGD	50
Boston housing	8	35	5 × 10-4	404	50000	ADAM	10
Breast Cancer	50	35	5 × 10-2	455	250000	ADAM	10
MNIST	40	20	1 × 10-3	25	25	ADAM	40
Cifar10	40	20	1 × 10-3	25	25	ADAM	50
RTE	20	10	3 × 10-4	8	10000	ADAM	50
STS-B	30	30	3 × 10-4	8	10000	ADAM	50
MRPC	75	25	3 × 10-4	16	10000	ADAM	50
Table 8: Paper experiments hyperparameters values
22