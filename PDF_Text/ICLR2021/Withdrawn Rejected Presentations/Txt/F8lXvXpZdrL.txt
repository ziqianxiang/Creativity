Under review as a conference paper at ICLR 2021
000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
054
055
056
057
058
Reintroducing Straight-Through Estimators
as Principled Methods for Stochastic Binary
Networks
Anonymous authors
Paper under double-blind review
Ab stract
Training neural networks with binary weights and activations is a challenging
problem due to the lack of gradients and difficulty of optimization over discrete
weights. Many successful experimental results have been achieved with empir-
ical straight-through (ST) approaches, proposing a variety of ad-hoc rules for
propagating gradients through non-differentiable activations and updating discrete
weights. At the same time, ST methods can be truly derived as estimators in
the stochastic binary network (SBN) model with Bernoulli weights. We advance
these derivations to a more complete and systematic study. We analyze properties,
estimation accuracy, obtain different forms of correct ST estimators for activa-
tions and weights, explain existing empirical approaches and their shortcomings,
explain how latent weights arise from the mirror descent method when optimiz-
ing over probabilities. This allows to reintroduce, once empirical, ST methods as
sound approximations, apply them with clarity and develop further improvements.
1	Introduction
Neural networks with binary weights and activations have much lower computation costs and mem-
ory consumption than their real-valued counterparts (Horowitz, 2014; Esser et al., 2016; Rastegari
et al., 2016). They are therefore very attractive for applications in mobile devices, robotics and other
resource-limited settings, in particular for solving vision and speech recognition problems (Bulat &
Tzimiropoulos, 2017; Xiang et al., 2017).
The state of the art in training deep binary networks is established with methods using the empir-
ical straight-through gradient estimation approach. Typically, the sign function is used to repre-
sent binary activations. This poses an immediate problem as the derivative of a step function like
sign is zero almost everywhere and is not useful for training with backpropagation. The empirical
straight-through approach consists in using the sign function on the forward pass and the derivative
of some other proxy function on the backward pass. One popular solution is to use identity proxy,
i.e. completely bypass sign on the backward pass (Krizhevsky & Hinton, 2011; Bengio et al., 2013;
Courbariaux et al., 2015; Zhou et al., 2016), hence the name straight-through. Other proxy functions
applied include tanh (Hinton, 2012; Raiko et al., 2015), clipped identity (Hubara et al., 2016; Lin
et al., 2017; Rastegari et al., 2016; Alizadeh et al., 2019), or piece-wise quadratic ApproxSign (Esser
et al., 2016; Liu et al., 2018), illustrated in Fig. A.1. These proxies can be used with deterministic or
stochastic binarization (Courbariaux et al., 2015; Hubara et al., 2016; Krizhevsky & Hinton, 2011).
This gives rise to a diversity of empirical ST methods, where various choices are studied purely
experimentally (Alizadeh et al., 2019; Bethge et al., 2019; Tang et al., 2017).
Since binary weights can be also represented as a sign mapping of some real-valued latent weights,
the same type of methods was adopted to handle optimization over binary weights (Shayer et al.,
2017; Courbariaux et al., 2015; Hubara et al., 2016). However, often a different proxy is used for
the weights along with clipping or not clipping latent weights to the interval [-1, 1], generating
additional unclear choices. With such obscurity of the concept of latent weights, Helwegen et al.
(2019) argues that “latent weights do not exist” meaning that discrete optimization over binary
weights needs to be considered and proposes an equally unjustified alternative scheme.
1
Under review as a conference paper at ICLR 2021
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
A more principled approach to handle binarization is to consider stochastic binary networks (SBNs),
in which hidden units and/or weights are Bernoulli random variables. In these models the expected
loss is a truly differentiable function of parameters (resp. weight probabilities) and its gradient
can be estimated. Advanced unbiased gradient estimators exist (Grathwohl et al., 2018; Tucker
et al., 2017; Yin & Zhou, 2019), which however have never been applied in practice to networks
with deep binary dependencies due to increased variance in deep layers and complexity that grows
quadratically with the number of layers (Shekhovtsov et al., 2020). ST can be defined in SBNs
as a biased estimator, however this form is not well-known and no systematic study exists. More
specifically, during our research on ST we found out that Tokui & Sato (2017); Shekhovtsov et al.
(2020); Dai et al. (2017) already showed how to derive ST under clear approximations in SBNs.
These results however are secondary in these papers and unnoticed in the works applying ST in
practice, recent works on analysis of deterministic ST (Yin et al., 2019; Cheng et al., 2019) and
works on alternative estimators (Cheng et al., 2019). They are not properly connected to empirical
ST variants and do not perform any analysis. This is the main context for our work. More related
works, including alternative approaches, are discussed in § A.
Contribution The goal of this work is to reintroduce straight-through estimators in a principled
way, clarify, systematize and study the empirical ST approaches. We build on the derivations of
ST for shallow (Tokui & Sato, 2017) and deep (Shekhovtsov et al., 2020) networks and the mirror
descent optimization view for deterministic binary weights (Ajanthan et al., 2019).
We review sound results allowing to derive ST estimator in SBNs (§§ 2.1 and 4); Analyze this derived
ST for models with one hidden layer and show shortcomings of and equivalences between existing
empirical approaches (§ 2.2); Define latent weights in a sound optimization scheme and establish a
rigorous correspondence between SGD with identity ST and powerful optimization methods such
as mirror descent and variational Bayesian learning (§ 3); Experimentally study the accuracy of
derived ST, confirming the theory and revealing useful insights (§ 5). Experimentally show that with
the derived ST estimators and the proposed initialization and optimization, several common choices
of gradient rules can be closely replicated, but unlike the empirical variants lead to equally well-
performing methods (§ 5). As accompanying results, we analyze Gumbel-Softmax estimator (§ E)
and the BayesBiNN method (Meng et al., 2020) that applies this estimator for SBNs, showing an
unexpected relation of the latter to ST (§ F).
2	Single Layer Straight-Through
Notation Throughout the paper we will define several models that work with binary random vari-
ables. We will model random states x ∈ {-1, 1}n using the noisy sign mapping:
xi = sign(ai - zi),	(1)
where zi are real-valued independent noises with a fixed cdf F and ai are (input-dependent) parame-
ters. It is seen that the difference from deterministic binary models is only in the presence of injected
noises z. Equivalently, xi can be described by probabilities p(xi=1 | ai) = P(ai-zi ≥ 0 | ai) =
P(zi ≤ ai | ai) = F (ai). The noise distribution F will play an important role in understanding
different schemes. If the noise is logistic, then F is the logistic sigmoid function σ .
2.1	Derivation
The ST was first introduced (Hinton, 2012; Krizhevsky & Hinton, 2011) in the context of a stochastic
autoencoder model, highly relevant to date (e.g. Dadaneh et al., 2020), which we consider. We give
a self-contained derivation of ST clarifying the key steps of Tokui & Sato (2017, sec. 6.4).
Let y denote observations. The encoder network parametrized by φ computes logits a(y; φ) and
computes a binary latent state x via (1). As noises z are independent, the conditional distribution
factors as p(x|y; φ) = Qi p(xi|y; φ). The decoder reconstructs observations with pdec(y|x; θ),
another neural network parametrized by θ. The reconstruction loss of this autoencoder is defined as
Ey〜data [EX〜p(x|y;6) [― logP (y|x； θ)]] .	(2)
We will focus on the problem of computing the gradient of this loss w.r.t. the encoder parameters φ
(differentiating in θ is easy as it does not affect the distribution if samples). The problem for a fixed
2
Under review as a conference paper at ICLR 2021
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
observation y takes the form
∂φ Ex 〜p(xM[L(X)] = ∂φ Ez[L(Signm -Z))],	⑶
where p(x; φ) is a shorthand for p(x|y; φ) and L(x) = - log pdec(y|x; θ). The reparametrization
trick, i.e. to draw one sample of z in (3) and differentiate L(sign(a - z)) fails: since the loss as a
function of a and z is not continuously differentiable we cannot interchange the gradient and the
expectation in z in the first place1. If we nevertheless do make the interchange, we obtain that the
gradient of sign(a - z) is zero and the result is obviously incorrect. Instead, the following steps
lead to an unbiased low variance estimator. From the LHS of (3) we express the derivative as
∂φ Px (Qi P(Xi； φ)) L(X) = Px Pi ( Qi0=i P(Xi0； φ))( ∂φ P(Xi； O))L(X).	⑷
Then the derandomization (Owen, 2013, ch. 8.7) is applied which performs summation over xi
explicitly for the rest of the states in X fixed. Because Xi takes only two values, this gives
Pxi dp∂XΦφL(X) = dp∂φφlL(X) + d(1-∂(xiM)L(Xji) = ∂φp(Xi； φ)(L(χ) -L(χji)), (5)
where XJi denotes the full state vector X with the sign of Xi flipped. Since this expression is now
invariant ofXi, we can multiply it with 1 = Px P(Xi； φ) and recollect the full gradient in the form:
Px(Qi0 P(Xi0 ； Φ))Pidp¾R (L(X)-L(XJi)) = Ex 〜p(xMPid¾φl (L(X)-L(XJi)). (6)
To obtain an unbiased estimate, it is sufficient to take one sample from the encoder X 〜 p(x； φ)
and compute the sum in i. This estimator is known as local expectations (Titsias & Lazaro-Gredilla,
2015) and coincides in this case with go-gradient (Cong et al., 2019) and ram (Tokui & Sato, 2017).
However, evaluating L(XJi) for all i may be impractical. A huge simplification is obtained if we
assume that the change of the loss L when only a single latent bit Xi is changed can be approximated
via linearization. When L is defined as a differentiable mapping Rn → R (as is the case in SBNs
and autoencoders), which we evaluate on discrete inputs X, we can approximate
L(x) — L(Xji) ≈ 2xidLXxl,	(7)
where we used that Xi - (-Xi) = 2xi. Expressing ∂φp(xi； φ) = XiF0(ai(φ))∂φ0i(Φ), We obtain
dp(∂φφl (L(x) -L(Xji)) ≈ 2F0(ai(φ))(*) ⅛l.	(8)
If we now define that Ixi ≡ 2F0(ai), the summation over i in (6) with the approximation (8) can be
written in the form of a chain rule:
Pi 2F0(ai(φ)) (d∂φφl) * = Pi (⅛l)(毅)(%).	(9)
To clarify, the estimator is already defined by the LHS of (9). We simply want to compute this
expression by (ab)using the standard tools and this is the sole purpose of introducing Ixi. Indeed
the RHS of (9) is a product of matrices that can be efficiently computed by multiplying from right
to left, i.e. by backpropagation. We thus obtained ST algorithm Alg. 1, which matches exactly to the
one described by Hinton (2012): to sample on the forward pass and use the derivative of the noise
cdf on the backward pass. The coefficient 2 that occurred is due to that we used ±1 encoding for X.
2.2 Analysis
Below we present a summary of our analysis. Formal claims and their proofs are given in § B.
Invariances Observe that binary activations stay invariant under transformations: sign(ai - zi) =
sign(T (ai) -T(zi)) for any strictly monotone mapping T. Consistently, the ST gradient by Alg. 1 is
also invariant to T. In contrast, empirical straight-through approaches, in which the derivative proxy
is hand-designed fail to maintain this property. In particular, rescaling the proxy leads to different
estimators. Furthermore, when applying transform T = F (the noise cdf), the backpropagation rule
in line 5 of Alg. 1 becomes equivalent to using the identity proxy. Hence we see that a common
description ofST in the literature “to back-propagate through the hard threshold function as ifit had
been the identity function” is also correct, but only for the case of uniform noise. Otherwise, and es-
pecially for deterministic ST, it is ambiguous as the result crucially depends on what transformations
are applied under the hard threshold.
1The conditions refer to the Leibniz integral rule, other conditions may suffice e.g. for weak derivatives.
3
Under review as a conference paper at ICLR 2021
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
A / / / 1F 2 3 4B 5	Jgorithm 1: Straight-Through-Activations	A *	a -- preactivation	*/	/ *	F -- activation noise cdf	*/	/ *	x ∈ {-1, 1}n -- binary state	*/	/ orward( a )	1 F P = F (a);	2 X ~ 2Bernoulli(p) - 1;	3 ackward( dχ )	4 B /* dχ -- loss grad in X	*/ 一脸 ≡ 2diag(F 0(a)) dL;	5	Jgorithm 2: Straight-Through-Weights * η -- mirror/latent weights */ * F -- weight noise cdf	*/ * w ∈ {-1, 1}d	*/ orward( η ) p= F (η); W ~ 2Bernoulli(p) — 1; ackward(篇) /* dL -- loss grad in W	*/ dL — d dL . _ dη - d dw;
Bias Analysis I) When ST is unbiased? As we used linearization as the only biased approxima-
tion, it follows that Alg. 1 is unbiased if the objective function L is multilinear in x. A simple
counter-example, where ST is biased, is L(x) = x2 . In this case the expected value of the loss is
1, independently of a that determines x = sign(a - z ) while the true gradient is zero. However
the expected ST gradient is E[2F0(a)2x] = 4F0(a)(2F(a) - 1), which may be positive or negative
depending on a. On the other hand, any function of binary variables has an equivalent multilinear
expression. In particular, if we consider L(x) = kW x-yk2, analyzed by Yin et al. (2019), then
L(x) = kWx—yk2 - Ei x2k W：,ik2 + Ei ||W：,i||2 coincides with L on all binary configurations
and is multilinear. It follows that ST applied to L gives an unbiased gradient estimate of E[L]. In
the special case when L is linear in x, the ST estimator is not only unbiased but has a zero variance,
i.e., it is exact. This surprising fact follows easily from the derivation.
II)	How does using a different proxy than F in Line 5 of Alg. 1 affects the gradient in φ? Since
diag(F 0) occurs in the backward chain, we call estimators that uses a diagonal Λ instead of diag(F 0)
as internally rescaled. We show that for any Λ < 0, the expected rescaled estimator has non-
negative scalar product with the expected original estimator. Thus using a different proxy, in partic-
ular identity, is not immediately destructive: if Alg. 1 was unbiased, the rescaled estimator may be
biased but it is guaranteed to give an ascend direction in the expectation and the optimization can in
principle succeed. However, assuming that Alg. 1 is biased (when L is not multi-linear) but still is
an ascent direction, the ascent property can no longer be guaranteed for the rescaled estimator.
III)	Does ST gradient give a valid ascent direction even when L is not multilinear? Assume that all
∂L(x)
∂xi
partial derivatives gi (x)
are L-Lipschitz continuous for some L. We show that expected
ST gradient is an ascent direction if and only if Ex[gi(x)] > L for all i.
IV)	Can we decrease the bias? Assume that the loss function is applied after a linear transform of
Bernoulli variables, i.e., takes the form L(x) = '(Wx). A typical initialization uses random W
normalized by the size of the fan-in, i.e., so that k Wk,： ∣∣2 = 1 ∀k. In this case the Lipschitz constant
of gradients of L scales as O(1∕√n), where n is the number of binary variables. Therefore, using
more binary variables decreases the bias, at least at initialization.
V)	Does deterministic ST give an ascent direction? Let g* be the deterministic ST gradient for
the state x* = sign(a) and p* = p(x* |a) probability of this state. We Show that deterministic ST
gradientforms a positive scalar product with the expected ST gradient if |g*| ≥ 2L(1 — p*) and with
the true gradient if |gi* | ≥ 2L(1 - p*) + L. From this we conclude that deterministic ST positively
correlates with the true gradient when L is multilinear, improves with the number of hidden units
in the case described by IV and approaches expected stochastic ST Alg. 1 as units learn to be more
deterministic so that the factor (1 - p*) decreases.
Variants Explained Using the invariance property, many works applying randomized ST estima-
tors are easily seen to be equivalent to Alg. 1 (Raiko et al., 2015; Shen et al., 2018; Dadaneh et al.,
2020). Furthermore, using different noise distributions for z, we recover common choices of sign
proxies used in empirical ST works as shown in Fig. A.1 (c-e). In our framework they correspond
to modeling choices affecting the forward pass in the first turn.
Dai et al. (2017) perform a correct interchange of derivative and integral in (3) using weak (distri-
butional) deriatives. After computing local expectations in this more complicated formalism, they
4
Under review as a conference paper at ICLR 2021
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
are back with finite differences (6) which they also propose to linearize as in (7). Thus their distri-
butional SGD is equivalent to common SGD with the ST estimator Alg. 1.
3	Latent Weights do Exist !
In this section we show that latent weights that did not get a proper substantiation in empirical ST
methods (Hubara et al., 2016; Helwegen et al., 2019) can be clearly defined in SBNs and that several
empirical update rules correspond to sound optimization schemes: projected gradient descent, mirror
descent, variational Bayesian learning.
Let w be ±1-Bernoulli weights with p(wi=1) = θi, let L(w) be the loss for a fixed training input.
Consistently with the model for activations (1), we can define wi = sign(ηi - zi) in order to model
weights wi using parameters ηi ∈ R that we will call latent weights. We need to tackle two problems
in order to optimize Ew〜p(w∣θ) [L(w)] in probabilities θ: i) how to estimate the gradient and ii) how
to handle constraints θ ∈ [0, 1]m.
Projected Gradient A basic approach to handle constraints is the projected gradient descent:
θt+1 := clip(θt - εgt, 0,1), where gt = VeEw〜p(we)[L(w)]	(10)
and clip(x, a, b) := max(min(x, b), a) is the projection. Observe that for the uniform noise distri-
bution on [-1,1] with F(Z) = clip(z++1,0,1), we have θi = p(wi=1) = F(ηi) = clip(ηiτ+1, 0,1).
Because this F is linear on [-1, 1], the update (10) can be equivalently reparametrized in η as
ηt+1 := clip(ηt	— ε0ht,	-1,1),	where	ht	=	VnEw〜p(w∣F(η))[L(w)]	and	ε0	=	4ε.	(11)
The gradient in the latent weights, ht, can be estimated by Alg. 1 with the simplification 2F0 = 1.
We obtained that the method of Hubara et al. (2016, Alg.1) with stochastic rounding and with real-
valued weights identified with η is equivalent to PGD on η with constraints η ∈ [-1, 1]m and ST
gradient estimate Alg. 1 and thus is a sound optimization scheme2.
Mirror Descent As an alternative approach to handle constraints θ ∈ [0, 1]m, we study the appli-
cation of mirror descent (MD) and connect it with the identity ST update variants. A step of MD is
found by solving the following proximal problem:
θt+1 = minehgt, θ - θti + ɪD(θ, θt).	(12)
The divergence term ɪD(θ, θt) weights how much we trust the linear approximation hgt, θ-θt)
when considering a step from θt to θ. When the gradient is stochastic we speak about stochas-
tic mirror descent (SMD) (Zhang & He, 2018; Azizan et al., 2019). A common choice of diver-
gence to handle probability constraints is the KL-divergence D(θi, θit) = KL(Ber(θi), Ber(θit)) =
θi log(θ) + (1 - θi)log(ι∈θ⅛). Solving for stationary point of (12) gives
i	-i
0 = gt + “bg( 1⅛) - log(τ⅛)).	(13)
Observe that when F = σ we have log( ɪθθ-)= @.Then the MD step can be written in the well-
known convenient form using the latent weights η (natural parameters of Bernoulli distribution):
θt := σ(ηt);	ηt+1 := ηt - εVeL(θt).	(14)
We thus have obtained the rule where on the forward pass θ = σ(η) defines the sampling prob-
ability of w and on the backward pass the derivative σ that otherwise occurs in Line 5 of Alg. 1
is bypassed exactly as if the identity proxy was used. We define such ST rule for optimization in
weights as Alg. 2. Its correctness is not limited to logistic noise. We show that for any strictly
monotone noise distribution F there is a corresponding divergence function D.
Proposition 1. Common SGD in latent weights η using the identity straight-through-weights Alg. 2
implements SMD in the weight probabilities θ with the divergence corresponding to F.
2In the projected SGD setting there are convergence guarantees at least in the convex setting.
5
Under review as a conference paper at ICLR 2021
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
The proof in § C is closely related to the work by Ajanthan et al. (2019). Differently from us, they
considered a connection to deterministic ST. Their argument includes taking the limit in which F
is squashed into the step function, which renders MD invalid. This problem does not occur in our
formulation. Proposition 1 reveals that although Bernoulli weights can be modeled the same way as
activations using the injected noise model w = sign(η - z), the noise distribution F for weights
does not reflect a modeling choice but the optimization proximity scheme.
Despite the generality of Proposition 1, we view the KL divergence as a more reliable choice in
practice. Azizan et al. (2019) have shown that the optimization with SMD has an inductive bias
to find the closest solution to the initialization point as measured by the divergence used in MD,
which has a strong impact on generalization. This suggests that MD with KL divergence will prefer
higher entropy solutions, making more diverse predictions. By our equivalence results, using SGD
on latent weights with logistic noise and identity straight-through Alg. 2 enjoys the same properties.
Variational Bayesian Learning Extending the results above, in § C.2 we study the variational
Bayesian learning formulation and show the following.
Proposition 2. Common SGD in latent weights η with a weight decay and identity straight-through-
weights Alg. 2 is equivalent to optimizing a factorized variational approximation to the weight pos-
terior p(w|data) using a composite SMD method.
We can see that powerful and sound learning techniques can be obtained in a form of simple update
rules. Such simple learning rules are recently (contemporaneously) studied by Khan & Rue (2020).
We emphasize the interplay with the identity-ST estimator and the connection to the implicit regu-
larization discussed above. The recent work by Meng et al. (2020) also addresses the binary case
but uses Gumbel-Softmax estimator. We discuss its shortcomings and an unexpected equivalence to
deterministic ST in § F.
4	Deep Straight-Through
In this section we review the derivation of ST for deep SBNs (Shekhovtsov et al., 2020), extend it to
binary weights using MD from § 3 and propose an initialization method based on the analysis in § 2.
Derivation Idea Consider an SBN model with L binary layers, recurrently defined as
xk = sign(ak (xk-1; θ) - zk), k = 1 . . . L,	(15)
where ak are pre-activations and zk are injected noises independent for all units. Let also L(xL; θ)
be the loss function for a given input and the last layer state xL . Similarly to one-layer case, the
probability p(xk|xk-1) is conditionally independent with p(xik = 1|xk-1) = F (aik(xk-1)).
The challenge to derive an ST-like estimator for deep SBN is that simply applying the steps in § 2
to the k’s layer in order to estimate gradient w.r.t. ak is not straightforward because we would
have to linearize the expectation of the whole network remainder and the loss as a function of xk .
The idea of Shekhovtsov et al. (2020) is to linearize only the next layer’s conditional probability
p(xk+1 |xk) and then proceed recurrently, chaining derandomization as in § 2 and linearization
steps. By doing so they infer a more accurate PSA estimator and from it the deep ST estimator. The
ST estimator (Shekhovtsov et al., 2020, Alg. 2) is identical to applying ST Alg. 1 to each binary
layer and executing the automatic differentiation. The insights from the derivation are twofold. First,
since derandomization is performed recurrently, the variance for deep layers is significantly reduced.
Second, we know which approximations contribute to the bias, they are indeed the linearizations
p(xk+1 ∣xk) - p(xk+1 |x：i) ≈ 2xkdp(Xk(Xk)	and	L(XL) - L(XLi) ≈ 叱XLL,	(16)
for all layers k and units i. The experiments (Shekhovtsov et al., 2020, Figs. 2, 3, 4) with real
weights suggest that while for small networks these approximations lead to some degradation of
gradient estimation accuracy during learning, for large networks ST performs on par with the PSA
method that uses a much tighter approximation. This is well aligned with our analysis in § 2.2.
Extensions We now can put all building blocks together. Our deep model extends the deep
model (15) as follows. The weights wk in each layer are ±1 Bernoulli with probability θ.
6
Under review as a conference paper at ICLR 2021
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
Pre-activations a are extended to incorporate additional processing, importantly batch normaliza-
tion (Ioffe & Szegedy, 2015):
ak = BN(Linear(xk-1 , wk)),	(17)
where Linear is a binary fully connected or convolutional transform and BN has real-valued affine
terms (scale, bias) enabled. The model is illustrated in Fig. D.2. Note that in the linearization of
p(xk+1 |xk) in xk, we effectively differentiate the composite transform (Linear, BN, F, and finally
the product forming p(xk+1∣xk)) and approximate the finite difference p(xk+1∣xk) 一 p(xk+1∣x,i)
using this linearization. Thus, extra non-linearities such as BN in principle worsen the approxima-
tion. On the other hand, the strongest non-linearity is introduced here by F. Following the arguments
of § 2.2, a large number of hidden units in combination with normalization by BN should achieve a
better approximation accuracy for linearizing F .
Initialization The role of the affine parameters (s, b) in BN is to reintroduce the scale and bias
degrees of freedom removed by the normalization (Ioffe & Szegedy, 2015). In our model these
degrees of freedom are important as they control the strength of pre-activation relative to noise.
With the sign activation, they could be indeed equivalently represented as learnable bias and variance
parameters of the noise since Sign(Xisi + b 一 Zi) = Sign (Xi 一 ZisLbi) assuming Si > 0. Without
the BN layer, the result of Linear(xk-1, wk) is an integer in a range that depends on the size of x.
If the noise variance is set to 1, this will lead to vanishing gradients in a large network. With BN and
its affine transform the right proportion can be learned, but it is important to initialize it so that the
learning can make progress. We propose the following initialization. We set si = 1 and bi = 0 (as
default for BN) and normalize the noise distribution so that it has zero mean and F0(0) = 2. This
choice ensures that the Jacobian 2F0(a) in Line 5 of Alg. 1 at the mean value of pre-activations is
the identity matrix and therefore gradients do not vanish.
We initialize weight probabilities θi as uniform in [0, 1]. The corresponding initialization of latent
weights is then ηi = F-1(θi). This initialization and the noise scaling above are the places where
the empirical approaches have to do extra guessing when considering different ST proxies, obtain
diverse results and attribute them to the quality of proxies (e.g. Esser et al., 2016).
5	Experiments
Stochastic Autoencoders Previous work has demonstrated that Gumbel-Softmax (biased) and
arm (unbiased) estimators give better results than ST on training variational autoencoders with
Bernoulli latents (Jang et al., 2016; Yin & Zhou, 2019; Dadaneh et al., 2020). However, only the
test performance was revealed to readers. We investigate in more detail what happens during the
training. Except of studying the training loss under equal training setup, we can directly measure
the gradient estimation accuracy of different variants ofST estimators as well as the reference meth-
ods in comparison to the true gradient. The latter can be estimated sufficiently accurately in models
with a single Bernoulli layer using arm with many samples (we use 1000).
We train a simple yet realistic variant of stochastic autoencoder for the task of text retrieval with
binary representation on 20newsgroups dataset. The autoencoder is trained by minimizing the re-
construction loss (2). The observed features y ∈ Rd are word counts with d= 10000 words. The
encoder maps them to Bernoulli probabilities of codes x ∈ {0, 1}n using a neural network with
structure d 一 512 一 n, i.e. with 512 hidden (deterministic) states and ReLU activations. The decoder
network is symmetric: n 一 512 一 d. Please refer to § D.1 for full specification and auxiliary results.
For each estimator we perform the following protocol. First, we train the model with this estimator
using Adam with lr = 0.001 for 1000 epochs. We then switch the estimator to ARM with 10 samples
and continue training for 500 more epochs (denoted as arm-10 correction phase). Fig. 1 top shows
the training performance for different number of latent bits n. It is seen (esp. for 8 and 64 bits)
that some estimators (esp. ST and det_ST) appear to make no visible progress, and even increase the
loss, while switching them to arm makes a rapid improvement. Does it mean that these estimators
are bad and arm is very good? An explanation of this phenomenon is offered in Fig. 2. The rapid
improvement by arm is possible because these estimators have accumulated a significant bias due
to a systematic error component (c.f. example in § 2.2), possibly only is a subspace of parameters,
which nevertheless can be easily corrected by an unbiased estimator.
7
Under review as a conference paper at ICLR 2021
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
8 bits
64 bits
256 bits
100 200
5W 600 700 800 900 1000
300 400
0 100 200 300 «0 500 600 700 800 900 1000	0 100 200 300 400 500 600 700 800 900 1000	0 100 200 300 400 500 600 700 800 900 1000
Figure 1: Comparison of the training performance and gradient estimation accuracy for a stochastic
autoencoder with different number of latent Bernoulli units (bits). Training Loss: each estimator
is applied for 100 epochs and then switched to arm-10 in order to correct the accumulated bias.
Expected improvement: lower is better (measures expected change of the loss), the dashed line
shows the maximal possible improvement knowing the true gradient. Cosine similarity: higher is
better, close to 1 means that the direction is accurate while below 0 means the estimated gradient is
not an ascent direction; error bars indicate empirical 70% confidence intervals using 100 trials.
To measure the bias and alignment of directions as theoretically analyzed in § 2.2, we evaluate dif-
ferent estimators at the same parameter points, located along the learning trajectory of the reference
arm estimator. At each such point we compute the reference gradient by arm-1000 and evaluate
the quality of the 1-sample candidate estimator. To measure the quality we compute the expected
cosine similarity and the expected improvement, defined respectively by:
ECS = E[hg, gi/(kgkkgk)], EI= -E[hg, gi]∕PE[W],	(18)
where g is the true gradient and g is an estimate. The expectation are taken over 100 trials and all
batches. The detailed explanation of these metrics is deferred to § D.1. Briefly, EI metric models
the objective loss locally using the proximal problem of an optimization method that can adapt the
learning rate per each epoch. These measurements, displayed in Fig. 1 for different bit length, clearly
show that with a small bit length biased estimators consistently start to point in the wrong direction.
Identity ST and deterministic ST clearly introduce an extra bias to ST. However, when we increase
the number of latent bits, the accuracy of all biased estimators improves, confirming our analysis
IV), V). In the bottom right plot we see a clear confirmation of V) that deterministic ST improves as
the network becomes more deterministic during the learning when there are sufficiently many units.
The practical takeaways are as follows: 1) biased estimators may perform significantly better than
unbiased but might require a correction of the systematically accumulated bias. 2) with more units
the ST approximation clearly improves and the bias has a less detrimental effect, requiring less
correction 3) derived stochastic ST is indeed more accurate than other ST variants, however deter-
ministic ST may have some additional useful properties for optimization not yet investigated.
Classification with Deep SBN In training deep SBNs, ARM becomes more computationally costly
and the relaxation error of Gumbel-Softmax becomes too high. In this setup ST becomes more
favorable and is the method of choice of many researchers. We want to show that the explainable
ST with components that we derived: identity ST for weights, link to the noise model, connection
to BN and the initialization scheme can do the job of the previous carefully selected but completely
unclear approaches. We consider CIFAR-10 dataset and use the vgg-like architecture (Courbariaux
8
Under review as a conference paper at ICLR 2021
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
Table 1: Test accuracy for different methods on CIFAR-10 with the same/similar architecture.
SBN can be tested either with zero noises (det) or using an ensemble of several samples (We use
10-sample). Standard deviations are given w.r.t. to 4 learning trials with random initialization. The
two quotations for Hubara et al. (2016) refer to their result with Torch7 implementation that uses
randomized Htanh and Theano implementation that uses deterministic Htanh, respectively.
STOCHASTIC TRAINING
Method	det	10-sample
Our SBN, logistic noise	89.6 ± 0.1	90.6 ± 0.2
Our SBN, uniform noise	89.7 ± 0.2	90.5 ± 0.2
Our SBN, triangular noise	89.5 ± 0.2	90.0 ± 0.3
Hubara et al. (2016) (rand.)	89.85	-
Peters & Welling (2018)	88.61	16-sample: 91.2
DETERMINISTIC TRAINING		
Rastegari et al. (2016)	89.83	-
Hubara et al. (2016) (det.)	88.60	-
Figure 2: Schematic explanation of the optimization process using a biased estimator followed by
a correction with an unbiased estimator. The figure applies more directly to estimators that do have
a relaxed objective, e.g. Gumbel-Softmax. Initially, the biased estimator makes good progress, but
then the value of the true loss function may start growing while the optimization steps nevertheless
come closer to the optimal location in the parameter space.
et al., 2015; Hubara et al., 2016) specified in Fig. D.2. Our full learning setup is specified in § D.2.
We trained SBNs with three choices of noise distributions corresponding to proxies used by prior
work as in Fig. A.1 (c-e). Table 1 shows the test results in comparison with baselines.
We see that training with different choices of noise distributions, corresponding to different ST rules,
all achieves equally good results. This is in contrast to empirical studies advocating specific proxies
and is allowed by the consistency of the model, initialization and training. The identity ST at the cor-
rect place (ST-through-weights) works well and we know why. Comparing to empirical ST baselines
(all except Peters & Welling), we see that there is no significant difference (’det’ column) indicating
that our derived ST method is on par with the well-guessed baselines. If the same networks we
trained are tested in the stochastic mode (10-sample), there is a clear boost of performance, indi-
cating an advantage of SBN models. Stochastic ST works better in the two experiments of Hubara
et al. but deterministic ST also works well in a somewhat different learning setup of Rastegari et al.
There is a small gap to Peters & Welling in the stochastic mode (our 10-sample versus their 16-
sample). Nevertheless, the results are very similar considering that they use a different estimation
method, an initialization from a pretrained model and an early stopping (we believe otherwise they
would overfit to the relaxation of the SBN considered). The takeaway message here is that ST can
be considered in the context of SBN models as a simple but proper baseline. Since we achieve near
100% training accuracy, the optimization fully succeeds and thus the bias of ST is tolerable.
6	Conclusion
We have put many ST methods on a solid basis by deriving and explaining them from the first prin-
ciples in the framework of SBNs. It is well-defined what they estimate and what the bias means.
We obtained two different main estimators for propagating activations and weights, bringing the
understanding which function they have, what approximations they involve and what are the limi-
tations imposed by these approximations. The resulting methods in all cases are strikingly simple,
no wonder they have been first discovered empirically long ago. We showed how our theory leads
to a useful understanding of bias properties and to reasonable choices that allow for a more reliable
application of these methods. We hope that researchers will continue to use these simple techniques,
now with less guesswork and obscurity, as well as develop improvements to them. Our code will be
available on http://github.com.
9
Under review as a conference paper at ICLR 2021
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
References
Thalaiyasingam Ajanthan, Kartik Gupta, Philip HS Torr, Richard Hartley, and Puneet K Dokania. Mirror
descent view for neural network quantization. arXiv preprint arXiv:1910.08237, 2019.
Milad Alizadeh, Javier Fernandez-Marques, Nicholas D. Lane, and Yarin Gal. An empirical study of bi-
nary neural networks’ optimisation. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=rJfUCoR5KX.
Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized nonlinear
models: Convergence, implicit regularization, and generalization. arXiv preprint arXiv:1906.03830, 2019.
Yu Bai, Yu-Xiang Wang, and Edo Liberty. Proxquant: Quantized neural networks via proximal operators.
In International Conference on Learning Representations, 2019. URL https://openreview.net/
forum?id=HyzMyhCcK7.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through StoChas-
tic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
Joseph Bethge, Haojin Yang, Marvin Bornstein, and Christoph Meinel. Back to simplicity: How to train
accurate BNNs from scratch? CoRR, abs/1906.08637, 2019. URL http://arxiv.org/abs/1906.
08637.
E. BorosandPL. Hammer. Pseudo-Boolean optimization. Discrete Applied Mathematics, 1-3(123):155-225,
2002.
Adrian Bulat and Georgios Tzimiropoulos. Binarized convolutional landmark localizers for human pose es-
timation and face alignment with limited resources. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
Suthee Chaidaroon and Yi Fang. Variational deep semantic hashing for text documents. In SIGIR Conference
on Research and Development in Information Retrieval, pp. 75-84, 2017.
Pengyu Cheng, Chang Liu, Chunyuan Li, Dinghan Shen, Ricardo Henao, and Lawrence Carin. Straight-through
estimator as projected wasserstein gradient flow. arXiv preprint arXiv:1910.02176, 2019.
Yulai Cong, Miaoyun Zhao, Ke Bai, and Lawrence Carin. GO gradient for expectation-based objectives. In
International Conference on Learning Representations, 2019.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks
with binary weights during propagations. In Advances in neural information processing systems, pp. 3123-
3131, 2015.
Siamak Zamani Dadaneh, Shahin Boluki, Mingzhang Yin, Mingyuan Zhou, and Xiaoning Qian. Pairwise
supervised hashing with Bernoulli variational auto-encoder and self-control gradient estimator. ArXiv,
abs/2005.10477, 2020.
Bo Dai, Ruiqi Guo, Sanjiv Kumar, Niao He, and Le Song. Stochastic generative hashing. In Proceedings of
the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp. 913-922, 2017.
Steven K. Esser, Paul A. Merolla, John V. Arthur, Andrew S. Cassidy, Rathinakumar Appuswamy, Alexander
Andreopoulos, David J. Berg, Jeffrey L. McKinstry, Timothy Melano, Davis R. Barch, Carmelo di Nolfo,
Pallab Datta, Arnon Amir, Brian Taba, Myron D. Flickner, and Dharmendra S. Modha. Convolutional net-
works for fast, energy-efficient neuromorphic computing. Proceedings of the National Academy of Sciences,
113(41):11441-11446, 2016. URL https://www.pnas.org/content/113/41/11441.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the
void: Optimizing control variates for black-box gradient estimation. In ICLR, 2018.
Alex Graves. Practical variational inference for neural networks. In NeurIPS, pp. 2348-2356. 2011.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-
level performance on ImageNet classification. In ICCV, pp. 1026-1034, 2015.
Koen Helwegen, James Widdicombe, Lukas Geiger, Zechun Liu, Kwang-Ting Cheng, and Roeland Nusselder.
Latent weights do not exist: Rethinking binarized neural network optimization. In Advances in neural
information processing systems, pp. 7531-7542, 2019.
Geoffrey Hinton. Lecture 15d - Semantic hashing : 3:05 - 3:35, 2012. URL https://www.cs.toronto.
edu∕~hinton∕coursera∕lecture15∕lec15d.mp4.
10
Under review as a conference paper at ICLR 2021
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
M. Horowitz. Computing’s energy problem (and what we can do about it). In 2014 IEEE International Solid-
State Circuits Conference Digest of Technical Papers (ISSCC), pp. 10-14, 2014.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Binarized neural net-
works. In Advances in neural information processing systems, pp. 4107-4115, 2016.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In ICML, volume 37, pp. 448-456, 2015.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint
arXiv:1611.01144, 2016.
Emtiyaz Khan and Haavard Rue. Learning algorithms from Bayesian principles. August 2020. Draft v. 0.7.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
Alex Krizhevsky and Geoffrey E. Hinton. Using very deep autoencoders for content-based image retrieval. In
ESANN, 2011.
Wu Lin, Mohammad Emtiyaz Khan, and Mark Schmidt. Fast and simple natural-gradient variational inference
with mixture of exponential-family approximations. In ICML, volume 97, Jun 2019.
Xiaofan Lin, Cong Zhao, and Wei Pan. Towards accurate binary convolutional neural network. In Advances in
Neural Information Processing Systems 30, pp. 345-353. 2017.
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing
the performance of 1-bit CNNs with improved representational capability and advanced training algorithm.
In Proceedings of the European conference on computer vision (ECCV), pp. 722-737, 2018.
Chris J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relaxation of
discrete random variables. 2016. arxiv:1611.00712.
Xiangming Meng, Roman Bachmann, and Mohammad Emtiyaz Khan. Training binary neural networks using
the Bayesian learning rule, 2020.
Ricardo Nanculef, Francisco Mena, Antonio Macaluso, Stefano Lodi, and Claudio Sartori. Self-supervised
Bernoulli autoencoders for semi-supervised hashing, 2020.
ArkadiI SemenoViCh NemiroVsky and David BorisoViCh Yudin. Problem complexity and method efficiency in
optimization. 1983.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, TreVor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperatiVe style, high-performance deep learning library. In Advances in
Neural Information Processing Systems 32, pp. 8024-8035. 2019.
Jorn WT Peters and Max Welling. Probabilistic binary neural networks. arXiv preprint arXiv:1809.03368,
2018.
Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh. Techniques for learning binary stochastic
feedforward neural networks. In ICLR, 2015. URL http://arxiv.org/abs/1406.2989.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net: Imagenet classification
using binary conVolutional neural networks. In European conference on computer vision, pp. 525-542.
Springer, 2016.
Wolfgang Roth, Gunther Schindler, Holger Froning, and Franz Pernkopf. Training discrete-valued neural
networks with sign actiVations using weight distributions. In European Conference on Machine Learning
(ECML), 2019.
Oran Shayer, Dan LeVi, and Ethan Fetaya. Learning discrete weights using the local reparameterization trick.
arXiv preprint arXiv:1710.07739, 2017.
Alexander ShekhoVtsoV, Viktor Yanush, and Boris Flach. Path sample-analytic gradient estimators for stochas-
tic binary networks, 2020.
11
Under review as a conference paper at ICLR 2021
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
Dinghan Shen, Qinliang Su, Paidamoyo Chapfuwa, Wenlin Wang, Guoyin Wang, Ricardo Henao, and
Lawrence Carin. NASH: toward end-to-end neural architecture for generative semantic hashing. In Proceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne,
Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 2041-2050, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A
simple way to prevent neural networks from overfitting. JMLR, 15:1929-1958, 2014.
Wei Tang, Gang Hua, and Liang Wang. How to train a compact binary neural network with high accuracy? In
AAAI, 2017.
MiChalis K. Titsias and Miguel Lazaro-Gredilla. Local expectation gradients for black box variational infer-
ence. In International Conference on Neural Information Processing Systems, pp. 2638-2646, 2015.
Seiya Tokui and Issei Sato. Evaluating the variance of likelihood-ratio gradient estimators. In Proceedings of
the 34th International Conference on Machine Learning - Volume 70, pp. 3414-3423, 2017.
George Tucker, Andriy Mnih, Chris J Maddison, John Lawson, and Jascha Sohl-Dickstein. REBAR: Low-
variance, unbiased gradient estimates for discrete latent variable models. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), NeurIPS, pp. 2627-2636. 2017.
Xu Xiang, Yanmin Qian, and Kai Yu. Binary deep neural networks for speech recognition. In INTERSPEECH,
2017.
Mingzhang Yin and Mingyuan Zhou. ARM: Augment-REINFORCE-merge gradient for stochastic binary
networks. In ICLR, 2019.
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack Xin. Understanding
straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662,
2019.
Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth nonconvex
optimization. arXiv: Optimization and Control, 2018.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016.
12
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
Reintroducing Straight-Through Estimators as Principled
Methods for Stochastic Binary Networks
(Appendix)
Contents
A Related Work
B Analysis of ST with 1 Hidden Layer
B.1	Invariances
B.2	Bias Analysis
C Mirror Descent and Variational Mirror Descent
C.1 Mirror Descent
C.2 Latent Weight Decay Implements Variational Bayesian Learning
D Details of Experiments
D.1 Stochastic Autoencoder
D.2 Classification with Deep Stochastic Binary Networks
E Gumbel Softmax and ST Gumbel-Softmax
F Analysis of BayesBiNN
A Related Work
Hinton’s vs Bengio’s ST The name straight-through and the first experimental comparison was
proposed by Bengio et al. (2013). Referring to Hinton’s lecture, they describe the idea as “simply to
back-propagate through the hard threshold function (1 if the argument is positive, 0 otherwise) as
if it had been the identity function”. In the aforementioned lecture (Hinton, 2012), however we find
a somewhat different description: “during the forward pass we stochastically pick a binary value
using the output of the logistic, and then during the backward pass we pretend that we’ve transmitted
the real valued probability from the logistic”. We can make two observations: 1) different variants
appeared early on and 2) many subsequent works (e.g. Yin et al., 2019) attribute these two variants
in the exact opposite way, adding to the confusion.
ST Analysis Yin et al. (2019) analyzes deterministic ST variants. The theoretical analysis is ap-
plicable to 1 hidden layer model with quadratic loss and the input data following a Gaussian dis-
tribution. The input distribution assumption is arguably artificial, however it allows to analyze the
expected loss and its gradient. They show that population ST gradients using ReLU and clipped
ReLU proxy correlate positively with the true population gradient and allow for convergence while
identity ST does not. In § B we show that in the SBN model, a simple correction of the quadratic
loss function makes the base ST estimator unbiased and all rescaled estimators including identity are
ascent directions in the expectation. Also note that the approach to analyze deterministic ST meth-
ods by considering the expectation over the input has a principle limitation for extending to deep
models: the expectation over the input of a deterministic network with two hidden binary layers is
still non-smooth (non-differentiable) in the parameters of the second layer.
Cheng et al. (2019) shows for networks with 1 hidden layer that STE is approximately related to the
projected Wasserstein gradient flow method proposed there.
On the weights side of the problem, Ajanthan et al. (2019) connected mirror descent updates for
constrained optimization (e.g., w ∈ [0, 1]m) with straight-through methods. The connection of
deterministic straight-through for weights and proximal updates was also observed in Bai et al.
(2019). Mirror Descent has been applied to variational Bayesian learning of continuous weights e.g.
in Lin et al. (2019), taking the form of update in natural parameters with the gradient in the mean
parameters, same as in our case.
13
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
Sign function
-2 -1 Ol 2
Identity ST
Htanh ST
/ uniform noise
tanh ST
/ logistic noise
ApproxSign ST
/ triangular noise
Figure A.1: The sign function and different proxy functions for derivatives used in empirical ST
estimators. Variants (c-e) can be obtained by choosing the noise distribution in our framework.
Specifically for a real-valued noise z withcdfF, in the upper plots we show Ez[sign(a-z)] = 2F-1
and, respectively, twice the density, 2F0 in the lower plots. Choosing uniform distribution for z
gives the density p(z) = 11lz∈[-1,1] and recovers the common Htanh proxy in (c). The logistic
noise has cdf F(z) = σ(2z), which recovers tanh proxy in (d). The triangular noise has density
p(z) = max(0, |(2-x)/4|), which recovers a scaled version of ApproxSign (Liu et al., 2018) in (e).
The scaling (standard deviation) of the noise in each case is chosen so that 2F0(0) = 1 as explained
in § 4. The identity ST form in (b) we recover as latent weight updates with mirror descent.
Alternative Estimators For deep binary networks several gradient estimation approaches are
based on stochastic gradients of analytically smoothed/approximated loss (Peters & Welling, 2018;
Roth et al., 2019). There is however a discrepancy between analytic approximation and the binary
samples used at the test time. Shekhovtsov et al. (2020, Fig. 4) show that such relaxed objectives
may indeed significantly diverge during the training. To obtain good results, a strong dropout reg-
ularization and/or pretraining is needed (Peters & Welling, 2018; Roth et al., 2019). Despite these
difficulties they demonstrate on par or improved results, especially when using average prediction
over multiple noise samples at test time.
B	Analysis of ST with 1 Hidden Layer
B.1	Invariances
We have the following simple yet desirable and useful property. It is easy to observe that binary
activations admit equivalent reformulations as
sign(ai - zi) = sign(T (ai) - T(zi))	(19)
for any strictly monotone mapping T : R → R.
Proposition B.1. The gradient computed by Alg. 1 is invariant to equivalent transformations under
sign as in (19).
Proof. Let Us denote the transformed noise as Zi = T (zi), its Cdf as G and the transformed activa-
tions as Zi = T(a%). The sampling probability in line 2 of Alg. 1 does not change since after the
transformation it compUtes p = G(aZi) = P(zZi ≤ Zai | aZi) = P(zi ≤ ai | ai) = F (ai). The gradient
returned by line 5 does not change since We have 急G(T(αj) = F0(电).	□
In contrast, empirical straight-throUgh approaches where the proxy is hand-designed fail to main-
tain this property. In particular, in the deterministic straight-through approach transforms such as
sign(ai) = sign(T(ai)) While keeping the proxy of sign used in backprop fixed lead to different
gradient estimates. This partially explains Why many proxies have been tried, e.g. ApproxSign (Liu
et al., 2018), and their scale needed tuning. Another pathological special case that leads to a confu-
sion betWeen identity straight-through and other forms is as folloWs.
Corollary B.1. Let F be strictly monotone. Then letting T = F leads to T(zi) being uniformly
distributed. Let aZi = T(ai). In this case the backpropagation rule in line 5 of Alg. 1 can be
interpreted as replacing the gradient of sign(aZi - T(zi)) in aZi With just identity.
Indeed, since zZi = T(zi) is uniform, We have G0 = 1 on (0, 1) and Zai = F(ai) is guaranteed to
be in (0, 1) by strict monotonicity. The gradient back-propagated by usual rules through aZi (outside
14
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
of the ST Alg. 1) encounters derivative of F as before. Hence we see that the description “to
back-propagate through the hard threshold function as if it had been the identity function” could be
misleading as the resulting estimator crucially depends on what transformations are applied under
the hard threshold despite they do not affect the network predictions in any way. We refer to the
variant by Bengio et al. (2013) as identity-ST, as it specifically uses the identity proxy for the gradient
in the pre-sigmoid activation.
B.2	Bias Analysis
I)	Since the only approximation that we made was linearization of the objective L, we have the
following basic property.
Proposition B.2. If the objective function L is multilinear3 in the binary variables x, then Alg. 1 is
unbiased.
Proof. In this case (7) holds as equality.
□
While extremely simple, this is an important point for understanding the ST estimator. As an imme-
diate consequence we can easily design counter-examples where ST is wrong.
Example 1. Let a ∈ R, x = sign(a - z) and L(x) = x2 . In this case the expected value of
the loss is 1, independent of a. The true gradient is zero. However the expected ST gradient is
E[2F 0 (a)2x] = 4F0(a)(2L(a) - 1) and can be positive or negative depending on a.
Example 2 (Tokui & Sato 2017). Let L(x) = x - sin(2πx). Then the finite difference L(1) -
L(0) = 1 but the derivative ∂∂X = 1 - 2∏ cos(2∏x) = -1. In this failure example, ST, even in the
expectation, will point to exactly the opposite direction of the true gradient.
An important observation from the above examples is that the result of ST is not invariant with
respect to reformulations of the loss that preserve its values in all binary points. In particular, we
have that L ≡ 1 in the first example and L(x) ≡ x in the second example for any x ∈ {-1, 1}. If
we used these equivalent representations instead, the ST estimator would have been correct.
More generally, any real-valued function of binary variables has a unique polynomial (and hence
multilinear) representation (Boros & Hammer, 2002) and therefore it is possible to find a loss refor-
mulation such that the ST estimator will be unbiased. Unfortunately, this representation is intractable
in most cases, but it is tractable, e.g., for a quadratic loss, useful in regression and autoencoders with
a Gaussian observation model.
Proposition B.3. Let L(x) = kWx - yk2. Then the multilinear equivalent reformulation of L is
given by
L(X) = kWx - yk2 - Pix2kW：,ik2 + Pi kW：,ik2,
where W:,i is the i’th column of W.
Proof. By expanding the square and using the identity xi2 = 1 for xi ∈ {-1, 1}.
(20)
□
Simply adjusting the loss using this equivalence and applying ST to it, fixes the bias problem.
II)	Next we ask the question, whether dropping the multiplier diag(F 0(a)) or changing itby another
multiplier, which we call an (internal) rescaling of the estimator, can lead to an incorrect estimation.
Proposition B.4. If instead of diag(F0(a)) any positive semidefinite diagonal matrix A is used
in Alg. 1, the expected rescaled estimator preserves non-negative scalar product with the original
estimator.
Proof. We write the chain (9) in a matrix form as JTAo(a) JT(x), with the Jacobians Ji = ∣φ,
A0 = diag(F0(a)) and J2(x) = dL(X). The modified gradient with A is then defined as
J1TΛ(a)J2T(x).
We are interested in the scalar product between the expected gradient estimates:
hE[JTΛoJT], EJT AJT ]i,
3E.g. x1x2x3 is trilinear and thus qualifies but x21 is not multi-linear.
(21)
15
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
where the expectation is over x. Since neither J1 nor Λ, Λ0 depend on x, we can move the ex-
Pectations to J2. Let J = E[dL(X)]. Then the scalar product between the expected estimates
becomes
hJTΛ0JT, JTAJTi =Tr(J2ΛJ1JTΛ0JT).	(22)
Notice that J1 J1T is positive semi-definite, Λ0 is also positive semi-definite since it is diagonal with
non-negative entries. It follows that R = AJi JTA。is positive semidefinite and that J2RJT is
positive semi-definite. Its trace is non-negative.	口
We obtained that the use of an internal rescaling, in particular identity instead of F0, is not too
destructive: if Alg. 1 was unbiased, the rescaled estimator may be biased but it is guaranteed to give
an ascend direction in the expectation so that the optimization can in principle succeed. However,
assuming that Alg. 1 is biased (when L is not multi-linear) but gives an ascent direction in the
expectation, the ascent direction property cannot be longer guaranteed for the rescaled gradient.
III)	Next, we study whether the ST gradient is a valid ascent direction even when L is not multi-
linear.
Proposition B.5. Let L(X) be such that its partial derivative gi = ∂∂L as a function of Xi is Lipschitz
continuous for all i with a constant L. Then the expected ST gradient is an ascent direction for any
a(φ) and L(x) if and only if
E[gi] > L for all i.	(23)
Proof. Sufficiency (if part). The true gradient using the local expectation form (6) expresses as
EhPi (符)(Pz@))xi(L(x)-L(xji))] = E[J△],	(24)
where the expectation is w.r.t. X 〜 p(x; φ) and We introduced the matrix notation J =
(∂φ )T diag(Pz (a)), and △ = Xi(L(X)-L(XIi)). The ST gradient replaces △ with 2gi(x). Since
in both cases J does not depend on X, the expectation can be moved to the last term. Respectively,
let us define ∆∆ = E[∆] and g = E[g]. The scalar product between the true gradient and the
expected ST gradient can then be expressed as
hJ ∆,Jg =Tr(JgA tJ t ).	(25)
From the relation
1
Xi(L(X)-L(XIi))= R gi(x)dxi	(26)
-1
and Lipschitz continuity of gi in Xi we have bounds
2(gi(X) - L) ≤ Xi(L(X)- L(XIi)) ≤ 2(gi(X) + L).	(27)
It follows that
2(E[g] - L) ≤ E[∆] ≤ 2(E[g] +L),	(28)
coordinate-wise. The outer product gAT is positive semidefinite iff 'g%∆i ≥ 0 for all i. According
to bounds above, this holds true if
(∀i | gi	≥ 0)	2(|gi|- L)	≥	0	(29)
(∀i | gi	< 0)	2(∣gi∣+ L)	≤	0,	(30)
or simply (∀i) |ggi| ≥ L.
Necessity (only if part). We want to show that the requirements (23), which are simultaneous for all
coordinates ofg, cannot be relaxed unless we make some further assumptions about a or L. Namely,
if ∃i* such that gi* △ i* < 0, then there exists a such thathJg, JA)< 0. I.e. a single wrong
direction can potentially be rescaled by the downstream Jacobians to dominate the contribution of
other components. This is detailed in the following steps.
Assume (∃i*) |gi* | < L. Then exists L(x) such that the bounds (27) are tight (e.g. L(x) = x2) and
therefore there will hold ggi* △g i* < 0. Since A = diag(pz(a)) is positive semi-definite, AggAg TA
will preserve the non-positive sign of the component (i*,i*). There exists a(φ) such that ∂∂a scales
down all coordinates i = i and scales up i such that the Tr(JgATJt) is dominated by the entry
(i*, i*). The resulting scalar product between the expected gradient and the true gradient thus can
be negative.	口
16
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
IV)	Next we study, a typical use case when hidden binary variables are combined using a linear
layer, initialized randomly. A typical initialization procedure would rescale the weights according
to the size of the fan-in for each output.
Proposition B.6. Assume that the loss function is applied after a linear normalized transform of
Bernoulli variables, i.e., takes the form
L(X)= '(Wx),	(31)
where W ∈ RK×n is a matrix of normally distributed weights, normalized to satisfy kWk,: k22 = 1
∀k. Then the expected LiPschitz constant of gradients of L scales as O(√1n).
Proof. Let U = Wx and let ∂d' be Lipschitz continuous with constant L. The gradient of L ex-
presses as
gi = dLXχ) = h赍，W"ii∙	(32)
By assumptions of random initialization and normalization, Wk,i 〜 N(0, ɪ). Ifwe consider |gi| in
the expectation over initialization we obtain that
EW [∣gi(x) - gi(y)∣] = EW [h'0(Wx) - '0(Wy), W：,ii] ≤ LEW [∣∣W∕k] = LK匿.(33)
Therefore gi has expected Lipschitz constant LK y∏^.	□
The normal distribution assumption is not principal for conclusion of O(√1n) dependance. Indeed,
for any distribution with a finite variance it would hold as well, differing only in the constant factors.
We obtain an important corollary.
Corollary B.2. As we increase the number of hidden binary units n in the model, the bias of ST
decreases, at least at initialization.
V)	Finally, we study conditions when a deterministic version of ST gives a valid ascent direction.
Proposition B.7. Let x* = sign(a). Let gi = dLXX) be Lipschitz continuous with constant L. Let
g* = g(x*) and p* = p(x*∣a). The deterministic ST gradient at x* forms a positive scalar product
with the expected stochastic ST gradient if
|gi* | ≥ 2(1 - p*)L ∀i.	(34)
Proof. Similarly to Proposition B.5, let J = (∣φ)T diag(Pζ(a)). The scalar product between the
expected ST gradient and the deterministic ST gradient is given by
hJE[g(x)],Jg*i =Tr (E[g(x)]g*TJt).	(35)
In order for it to be non-negative we need E[g(x)i]gi* ≥ 0 ∀i. Observe that E[g(x)i] is a sum that
includes gi* with the weight p* . We therefore need
Px=x* p(x|a)g(x)ig* + p*g*2 ≥ 0.	(36)
From Lipschitz continuity of gi we have the bound |g(x)i - gi* | ≤ L|xi - xi* |, or using that |xi -
xi* | ≤ 2 we have
i
gi* - 2L ≤ g(x)i ≤ gi* +2L.	(37)
Therefore
g(x)igi* ≥ gi*2 - 2L|gi*|.	(38)
We thus can lower bound (36) as
Px=X*p(x∣a)(∣g:∣- 2L)∣g*l + p*g*2 = -2L∣g*l(1-P*) + g*2∙	(39)
This lower bound is non-negative if
|gi* | ≥ 2L(1 - p*).	(40)
□
17
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
Compared to Proposition B.5,this condition has an extra factor of 2(1 — p*). Sincep* is the product
of probabilities of all units x*, We expect initially p*《 1. This condition improves at the same
rate with the increase in the number of hidden units as the case covered by Proposition B.6. In
addition it becomes progressively more accurate as units learn to be more deterministic, because in
this case the factor (1 -p*) decreases. HoWever, note that this proposition describes the gap betWeen
deterministic ST and stochastic ST. And even When this gap diminishes, the gap betWeen ST and the
true gradient remains.
We can obtain a similar sufficient condition for the scalar product betWeen deterministic ST and the
executed true gradient, that (unlike the direct combination of Proposition B.5 and Proposition B.7)
ensures an ascent direction.
Proposition B.8. Let x* = sign(a). Let g(x)i = dL(X) be Lipschitz continuous with constant L.
Let g* = g(x*) and p* = p(x* |a). The deterministic ST gradient at x* forms a positive scalar
product with the true gradient if
|gi*| ≥ 2(1 -p*)L+L ∀i.	(41)
Proof. The proof is similar to Proposition B.7, only in this case we need to ensure E[∆i]gi* ≥ 0.
Using (28) we get the bounds
2(E[g] - L) ≤ E[∆] ≤ 2(E[g] + L),	(42)
And using additionally (37) we get
2(p*gi* + (1 - p*)(gi* -2L) -L) ≤ E[∆i] ≤2(p*gi*+(1-p*)(gi*+2L)+L).	(43)
Collecting the terms
2(gi* -(1-p*)2L-L) ≤ E[∆i] ≤2(gi*+(1-p*)2L+L).	(44)
Multiplying by gi* we obtain that a sufficient condition for E[∆i]gi* ≥ 0 is
|gi*| ≥ (1 -p*)2L+L.	(45)
□
C Mirror Descent and Variational Mirror Descent
C.1 Mirror Descent
Mirror descent is a widely used method for constrained optimization of the form minx∈X f(x),
where X ⊂ Rn, introduced by Nemirovsky & Yudin (1983). Let Φ : X → R be strongly convex and
continuously differentiable on X, called a mirror map. Bregman divergence DΦ (x, y) associated
with Φ is defined as
Dφ(x, y) = Φ(x) - Φ(y) — hVΦ(y), X — y.	(46)
An update of MD algorithm can be written as:
xt+1 = argmin<x, Pf(Xtyi + ^Dφ(x, xt).	(47)
x∈X	ε
In the unconstrained case when X = Rn or in the case when the critical point is guaranteed to be
in X (as typically ensured by the design of DΦ), the solution can be found from the critical point
equations, leading to the general form of iterates
PΦ(Xt+1) = PΦ(Xt) - εPf(Xt)	(48)
Xt+1 = (PΦ)-1 (PΦ(Xt) - εPf(Xt)) .
Proposition 1. Common SGD in latent weights η using the identity straight-through-weights Alg. 2
implements SMD in the weight probabilities θ with the divergence corresponding to F .
Proof. We start from the defining equation of MD update in the form (48). In order for (48) to
match common SGD on η with ηi = F-1(θi), the mirror map Φ must satisfy PΦ(θ) = F -1(θ),
where F-1 is coordinate-wise. We can therefore consider coordinate-wise mirror maps Φ : R → R.
The inverse F-1 exists if F is strictly monotone, meaning that the noise density is non-zero on the
18
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
support. Finding the mirror map Φ explicitly is not necessary for our purpose, however in 1D case it
can be expressed simply as Φ(x) = 0x F-1(η)dη. With this coordinate-wise mirror map, the MD
update can be written as
ηt+1
ηt
ε dL I
dθ ∖θ=F(ηt)
(49)
Thus MD on θ takes the form of a descent step on η with the gradient 喘. A common SGD
on η would use the gradient 若 = 瑞 ∂L. ThUS (49) bypasses the Jacobian 瑞.ThiS is exactly
what Alg. 2 does. More precisely, when applying the same derivations that we used to obtain ST for
activations in order to estimate %,since F(ηi) = θi, We obtain that the factor ∂θP(Wi； θ) present
in (6) expresses as
dF (η) — ∂F (F-1(θ)) — 1
dθ =	∂θ = ɪ
(50)
and thus can be omitted from the chain rule as defined in Alg. 2.
□
C.2 Latent Weight Decay Implements Variational Bayesian Learning
In the Bayesian learning setting we consider a model with binary weights w and are interested
in estimating p(w|D), the posterior distribution of the weights given the data D and the weights
prior p(w). In the variational Bayesian (VB) formulation, this difficult and multi-modal posterior
is approximated by a simpler one q(w), commonly a fully factorized distribution, by minimizing
KL(q(w)kp(w∣D)). Let q(w) = Ber(w; θ) andp(w) = Ber(w; 1), both meant component-wise,
i.e. fully factorized. Then the VB problem takes the form
argmin { - E(χ0,y)~data[Ew~Ber(θ) [logp(y∣x0; w)]] + NKL(Ber(θ)kBer( 1))},	(51)
θ
where we have rewritten the data likelihood as expectation and hence the coefficient 1/N in front
of the KL term appeared. This problem is commonly solved by SGD taking one sample from the
training data and one sample of w and applying backpropagation (Graves, 2011). We can in principle
do the same by applying an estimator for the gradient in θ.
The trick that we apply, different from common practices, is not to compute the gradient of the KL
term but to keep this term explicit throughout to the proximal step leading to a composite MD (Zhang
& He, 2018). With this we have
Proposition 2. Common SGD in latent weights η with a weight decay and identity straight-through-
weights Alg. 2 is equivalent to optimizing a factorized variational approximation to the weight pos-
terior p(w|data) using a composite SMD method.
Proof. Expanding data log-likelihood as the sum over all data points, we get
log p(D | w) = Pi log p(xi | w) =: Pi li(w).	(52)
When multiplying with N, the first term becomes the usual expected data likelihood, where the
expectation is in training data and weights W 〜 q(w). Expanding also the parametrization of
q(w) = Ber(w | θ), the variational inference reads
argmin { - Ew~Ber(θ) [N Pi li(w)] + NKL(q(w)∣∣p(w)) + const}.	(53)
θ
We employ mirror descent to handle constraints θ ∈ [0, 1]m similar to the above but now we apply
it to this composite function, linearizing only the data part and keeping the prior KL part non-linear.
Let
g = |i|	^θEw~Ber(θ)li (W)
be the stochastic gradient of the data term in the weight probabilities θ using a min-batch I . The
SMD step subproblem reads
mine {gτθ + εKL(Ber(θ)∣∣Ber(θt)) + NKL(Ber(θ)∣∣Ber( 1))}.	(54)
19
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
We notice that KL(Ber(θ)∣∣Ber(2)) = -H(Ber(θ)), the negative entropy, and also introduce the
prior scaling coefficient λ = N in front of the entropy, which may optionally be lowered to decrease
the regularization effect. With these notations, the composite proximal problem becomes
mine {gTθ + 1 KL(Ber(θ)∣∣Ber(θt)) - λH(Ber(θ))}.	(55)
The solution is found from the critical point equation in θ :
Ve(gτθ + 1 KL(Ber(θ)∣∣Ber(θt)) - λH(Ber(θ))) = 0	(56a)
gi+ε (log ι⅛- log ι⅛t)+λ log ι⅛=0	(56b)
(ελ + I) log 1⅛ = log 1⅛J - εgi	(56c)
log 1⅛ = Zλ+1log 1⅛t - ∑λ+r gi.	(56d)
For the natural parameters we obtain:
η = ηεεt+g = ηt- ελ+ι(ληt + g)∙	(57)
We can further drop the correction of the step size 否+i since ελ +1 ≈ 1 and the step size will need
to be selected by cross validation anyhow. This gives us an update of the form
η=ηt - ε(g + ληt),	(58)
which is in the form of a standard step in any SGD or adaptive SGD optimizer. The difference is
that the gradient in probabilities θ is applied to make step in logits η and the prior KL divergence
contributes the logit decay λ, which in this case is the latent weight decay. Since the ST gradient in
θ differs from the ST gradient in η by the factor diag(F0), the claim of Proposition 2 follows. □
D Details of Experiments
D.1 Stochastic Autoencoder
It was shown in the literature that semantic hashing using binary hash codes can achieve superior
results using learned hash codes, in particular based on variational autoencoder (VAE) formulation,
e.g., recent works of Chaidaroon & Fang (2017); Dadaneh et al. (2020); Nanculef et al. (2020). We
consider a simplified yet realistic model, similar to the unsupervised setup in the mentioned works.
However, we do not use the variational evidence lower bound formulation (also referred to as VAE),
just a plain stochastic autoencoder (2). It can be seen that the variational formulation adds to the
objective (2) the negative entropy of the encoder distribution (KL to the uniform Bernoulli prior).
This entropy can be computed in closed form and consequently has an analytic gradient that does
not need the techniques we study. It also changes he optimization problem, leading to an automatic
selection of the number of bits efficiently used by the encoder while the remaining bits are found
to be in the posterior collapse (their distribution q(xi |y) is uniform and does not depend on the
input y). This property is in contradiction with our goal to study the performance of estimators
with the increase of the latent code size. In practice, the balance between the data term and the KL
prior is often controlled by an extra hyper-parameter β (in the public implementation of Nanculef
et al. (2020) one can find β = 0.015 is used). As the KL prior term imposes lots of issues that are
orthogonal to this study, we chose to test with plain stochastic autoencoders, that correspond to the
setting β = 0 in β-VAE.
Dataset The 20Newsgroups data set4 is a collection of approximately 20,000 text documents, par-
titioned (nearly) evenly across 20 different newsgroups. In our experiments we do not use the
partitioning. We used the processed version of the dataset denoted as Matlab/Octave on the dataset’s
web site. It contains bag-of-words representations of documents given by one sparse word-document
count matrix. We worked with the training set that contains 11269 documents in the bag of words
representation.
4http://qwone.com/~jason∕20Newsgroups/
20
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
Table D.1: List of estimators evaluated in the stochastic autoencoder experiment.
Name	Details
arm	State-of-the-art unbiased estimator Yin & Zhou (2019).
Gumbel(τ) Relaxation-based biased estimator Jang et al. (2016) with temperature parameter τ = 1 or 0.1.
ST	Hinton’s ST variant in Alg. 1.
det_ST	Deterministic version of ST setting the noise Z = 0 always during training.
identity_ST Identity ST variant described by Bengio et al. (2013).
Preprocessing We keep only the 10000 most frequent words in the training set to reduce the
computation requirements. Each of the omitted rare words occurs not more than in 10 documents.
Reconstruction Loss Let y ∈ Nd be the vector of word counts of a document and x ∈ {0, 1}n be
a latent binary code representing the topic that we will learn. The decoder network given the code
x deterministically outputs word frequencies f ∈ [0, 1]d, Pi fi = 1 and the reconstruction loss
- logpdec(y|x; θ) is defined as
- Pi yi logfi,	(59)
i.e., the negative log likelihood of a generative model, where word counts y follow multinomial
distribution with probabilities f and the number of trials equal to the length of the document. The
encoder p(x|f; φ) obtains word frequencies form y and maps them deterministically to Bernoulli
probabilities p(xi |f; φ). The loss of the autoencoder (2) is then
^Py 〜data Ez〜p(χ∣y) - logpdec(y∣χ; θ).	(60)
Networks Specs The encoder network takes on the input word frequencies f ∈ Rd and applies the
following stack: FC(d × 512), ReLU, FC(512 × n), where FC is a fully connected layer. The out-
put is the vector of logits of Bernoulli latent bits. The decoder network is symmetric: FC(n × 512),
ReLU, FC(512 X d), Softmax. Its input is a binary latent code X and output is the word probabilities
f. Standard weight initialization is applied to all linear layers W setting Wij 〜 U [-1∕√k, 1∕√k],
where k is the number of input dimensions to the layer. This is a standard initialization scheme (He
et al., 2015), which is consistent with the assumptions we make in Proposition B.6 and hence im-
portant for verification of our analysis.
Estimators Estimators evaluated in this experiment are described in Table D.1. As detailed in § 2,
in the identity ST we still draw random samples in the forward pass like in Alg. 1 but omit the
multiplication by F0. Alg. 1 is correctly instantiated for the {0, 1} rather than ±1 encoding in all
cases. For the arm-10 correction phase and arm-1000 ground truth estimation, the average of arm
estimates with the respective number of samples is taken.
Optimizer We used Adam (Kingma & Ba, 2014) optimizer with a fixed starting learning rate
lr = 0.001 in both phases of the training. When switching to the ARM-10 correction phase, we
reinitialize Adam in order to reset the running averages.
Evaluation For each bit length we save the encoder and decoder parameter vectors φ, θ every 100
epochs along the arm training trajectory. At each such point, offline to the training, we first apply
ARM-1000 in order to obtain an accurate estimate of the true gradient g. We then evaluate each of
the 1-sample estimators, including arm itself ( = arm-1 for that matter). The next question we
discuss is how to measure the estimator accuracy. Clearly, if we just consider the expected local
performance such as E[〈g, g)], unbiased estimators win regardless how high is their variance. This
is therefore not appropriate for measuring their utility in optimization. We evaluate three metrics
tailored for comparison of biased and unbiased estimators.
Cosine Similarity This metric evaluates the expected cosine similarity, measuring alignment of
directions:
E[hg, g)/(kgkkgk)],	(61)
21
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
800
500
400
300
200
100
600
600
400
400
2∞
2∞
0 100 200 300 .400 500 ..600 700 800 900 1000	0 100 200 300 .400 500 ..600 700 800 900 1000	0 100 200 300 .400 500 . 600 700 800 900 1000
Figure D.1: Root Mean Squared error of different estimators for the same reference trajectories
as Fig. 1.
where the expectation is over all training data batches and 100 stochastic trials of the estimator
g. This metric is well aligned with our theoretical analysis § 2.2. It is however does not measure
how well the gradient length is estimated. If the length has a high variance, this may hinder the
optimization but would not be reflected in this metric.
Expected Improvement To estimate the utility of the estimator for optimization, we propose to
measure the expected optimization improvement using the same proximal problem objective that is
used in SGD or SMD to find the optimization steps. Namely, let g = VφL(φt) be the true gradient
at the current point. The common SGD step is defined as
Φt+1 = Φt + arg min (hg, ∆φi + 21ε ∣∣∆φk2)∙	(62)
∆φ	ε
The optimal solution is given by ∆φ = -εg. Since instead ofg, only an approximation is available
to the optimizer, we allow it to use the solution ∆φ = -αg^, where g^ is an estimator of g and α is
one scalar parameter to adopt the step size. We consider the expected change of the proxy objectives:
E[hg,-αgi + ⅛ kgk2]∙	(63)
The parameter α correspond to a learning rate that can be tuned or adapted during learning. We
set it optimistically for each estimator by minimizing the expected objective (63), which is a simple
quadratic function in α. One scalar α is thus estimated for one measuring point (i.e. for one expec-
tation over all training batches and all 100 trials). As such, it is not overturning too much to each
estimator. The optimal α is given by
α = εE[hg,gi]∕E[∣∣g∣∣2]	(64)
and the value of the objective for this optimal α is
-2 E[hg,gi]2∕E[∣∣g∣∣2].	(65)
For the purpose of comparing estimators, - f is irrelevant and the comparison can be made on the
square root of (65). We obtain an equivalent metric that is the expected loss decrease normalized by
the RMS of the gradients:
-E[〈g,gi]/PEoFL	(66)
Confer to common adaptive methods which divide the step-length exactly by the square root of a
running average of second moment of gradients, in particular Adam (applied per-coordinate there).
This suggests that this metric is more tailored to measure the utility of the estimator for optimization.
For brevity, we refer to (66) as expected improvement. Note also that in (66) we preserve the sign
of E[hg, g)] and if the estimator is systematically in the wrong direction, we expect to measure a
positive value in (66), i.e. predicting objective ascent rather than descent.
Root Mean Squared Error It is rather common to measure the error of biased estimators as
RMSE = PE[kg - gk2].	(67)
This metric however may be less indicative and less discriminative of the utility of the estimator for
optimization. In Fig. D.1 it is seen that RMSE of ARM estimator can be rather high, especially with
more latent bits, yet it performs rather well in optimization.
22
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350
1351
1352
1353
1354
1355
1356
Figure D.2: Stochastic Binary Network: first and last layer have real-valued weights. BN
layers have real-valued scale and bias parameters that can adjust scaling of activations rela-
tive to noise. Z are independent injected noises with a chosen distribution. Binary weights
Wij are random ±1 BernoUlli(θj) with learnable probabilities θj. In experiments We consider
SBN with a convolutional architecture same as Courbariaux et al. (2015); Hubara et al. (2016):
(2×128C3) - MP2 - (2×256C3) - MP2 - (2×512C3) - MP2 - (2×1024FC) - 10FC - softmax.
D.2 Classification with DEEP Stochastic Binary Networks
The verification of ST estimator in training deep neural networks with mirror descent is conducted
on CIFAR-10 dataset5.
Dataset The dataset consists of 60000 32x32 color images divided in 10 classes, 6000 images per
class. There is a predefined training set of 50000 examples and test set of 10000 examples.
Preprocessing During training we use standard augmentation for CiFAR-10, namely random hor-
izontal flipping and random cropping of 32×32 region with a random padding of 0-4 px on each
side.
Network The network structure and layer specifications are shown in (D.2).
Optimizer we use Adam optimizer (Kingma & Ba, 2014) in all the experiments. The initial
learning rate γ = 0.01 is used for 300 epochs and then we divide it by 10 at epochs 300 and 400 and
stop at epoch 500. This is fixed for all models. All other Adam hyper-parameters such as β1, β2, ε
are set to their correspondent default values in the PyTorch Paszke et al. (2019) framework.
Training Loss Let the network softmax prediction on the input image x0 with noise realizations
in all layers z be denoted as p(x|z, x0). The training loss for the stochastic binary network is the
expected loss under the noises:
Eχ0~data[Ez [- logp(x∣z, x°)]].	(68)
The training procedure is identical to how the neural networks with dropout noises are trained (Sri-
vastava et al., 2014): one sample of the noise is generated alongside each random data point.
Evaluation At the test time we can either set z = 0 to obtain a deterministic binary network (de-
noted as ’det’). we can also consider the network as a stochastic ensemble and obtain the prediction
via the expected predictive distribution
Ez[p(x|z,x0)],	(69)
approximated by several samples. in the experiments we report performance in this mode using
10 samples. we observed that increasing the number of samples further improves the accuracy
only marginally. we compute the mean and standard deviation for the obtained accuracy values by
averaging the results over 4 different random learning trials for each experiment.
5https://www.cs.toronto.edu/~kriz/cifar.html
23
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
1410
1411
1412
1413
1414
1415
E Gumbel S oftmax and ST Gumbel-Softmax
Gumbel Softmax (Jang et al., 2016) or Concrete relaxations (Maddison et al., 2016) are techniques
that overcome the problem of non-differentiability of arg max (in binary case equivalent to sign
in (1)) by approximating it with a smooth function whose degree of smoothness is controlled by a
temperature-like parameter τ > 0, where the limit τ → 0 recovers the original non-smooth function.
We analyze this estimator for the case of a single neuron and show the following properties, which
to our knowledge were not published before:
I)	GS estimator is asymptotically unbiased as τ → +0 and the bias decreases at the rate O(τ).
But for any fixed τ it is biased even when L is linear.
II)	For any given noise realization z 6= η the GS gradient norm approaches zero at the expo-
nential rate O( 1C 1), where C = e-lη-zl < 1.
III)	The probability of the event that gradient norm is at least ε is asymptotically O(Tlog 1).
Thus the probability to observe a non-zero gradient up to numerical precision quickly van-
ishes.
IV)	The variance of GS estimator grows at the rate O(1).
V)	The ST Gumbel-softmax estimator (Jang et al., 2016) is biased even asymptotically even
for a linear loss.
Properties II and III are easy to extend to the case of layer with multiple units since they apply just
to the factor 岛στ (η - z), which is present independently at all units. Property II can be extended to
deep networks with L layers of Bernoulli variables, in which case the chain derivative will encounter
L such factors and we obtain that the gradients will vanish at the rate O(τL).
The proofs are given below. Basically all these facts should convince the reader of the following: it
is not possible to use a very small τ, not even with an annealing schedule starting from τ = 1. For
very small τ, the most likely consequence would be to never encounter a non-zero gradient during
the whole training. For moderately small τ the variance would be prohibitively high. Indeed, Jang
et al. (2016) anneals τ only down to 0.5 in their experiments.
Definitions The Gumbel-max reparametrization constructs a sample from a categorical distribu-
tion with class probabilities πi as
X = one_hot(arg maxi (log ∏ + Γi)),	(70)
where x is 1-hot encoding of the class and Γi are independent Gumbel noises. This reparametrization
is exact and is not differentiable since it outputs discrete samples. The Gumbel-Softmax replaces the
hard arg max indicator with softmax to compute the relaxed sample y ∈ Rn as
y = Softmax(( T (log ∏i +Γi))i).	(71)
We will focus on the binary case. We can express then
x1 = [[log π1 + Γ1 ≥ log π0 + Γ0]] = [[η - z ≥ 0]],	(72)
where η = log ιjπ1∏^ and Z = Γo - Γι. As expected Z has logistic distribution and We recover that
p(x1=1) = σ(η).
The relaxation y takes a similar form. By dividing over the numerator in (71) we obtain:
y1 = 1+exp(-(η-z)∕τ) = σ(ηTz ) =: στ5 - Z)
y0 = 1 - y1.
The loss function L is extended to the simplex of (y1, y0) by
L(y1, y0) = L(y1).
Recall that ST estimates the gradient in η by sampling x1 and composing
∂L ∂σ(η)
∂xι ∂η .
(73)
(74)
(75)
(76)
24
1416
1417
1418
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
1439
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
The Gumbel-Softmax estimates the gradient by sampling z and computing
GT := dL(y(η)) = dL(στ(η -Z)) = dL⅛)d⅛-z).	(77)
So the gradient of the relaxed loss is multiplied with the gradient of stretched (assuming τ < 1) and
shifted sigmoid.
Proposition E.1. The estimator is asymptotically unbiased as τ → 0 and the bias decreases at the
rate O(τ).
Proof. Let us denote
gt = Ez[gt] = RZo 岛L(στ(η -Z))PZ(Z)dz.	(78)
Note that the limit lim「→o+ gt(x) cannot be simply interchanged with the integral above - no quali-
fication theorem allows this. We apply the following reformulation. The derivative 岛L(στ (η - Z))
expands as
L0(στ(η - Z))στ(η - Z)(I - στ(η - Z))T.	(79)
We make a change of variables v = στ (η - Z) in the integral. This gives Z = η - τ logit(v) and
dZ = -τ v(i-v)dv. Substituting and cancelling part of the terms, We obtain
gt = R1 L(V)PZ(η - Tlogit(v))dv.	(80)
With this expression we can now interchange the limit and the integral using the dominated conver-
gence theorem. In order to apply it We need to show that there exist an integrable function g(v) such
that
∣L0(v)pz(η - τlogit(v))∣ < g(v)	(81)
for all τ > 0. Observe that
suPv∈[o,i] |pz(η - Tlogit(V))I = suPu∈r |pz(η - Tu)I = suPy∈R |pz(y)| = PZ(O) = 4,	(82)
where we used that the maximum of standard logistic density is attained at zero. We can therefore
let g(v) = L0(v)∕4. Since L0(v) is the derivative of L, it is integrable on [0,1]. Therefore the
conditions of the dominated convergence theorem are satisfied and we have
limτ→0+ gτ = R1 L0(v)limτ→0+ Pz(η - Tlogit(v))dv	(83a)
= R01 L0(V)Pz(η)dV = (L(1) - L(0))Pz (η),	(83b)
which is the correct value of the gradient.
Next, we obtain the series representation of the estimator bias in the asymptote T → 0+. We
approximate Pz (η - Tlogit(V)) with its Taylor series around T = 0:
Pz(η - T logit(V)) = Pz(η) + c1logit(V)T + c2logit2(V)T2 + O(T3)	(84)
where
ci = Pz (η) eη+1;	c2 = pz (η) -累++：；+1.	(85)
This is obtained using mathematica. We use this expansion in the integral representation (80). Ob-
serving that g = Pz(η) R01 L0(V)dV, the zero order term becomes the true gradient. It follows that
the bias of Gτ is asymptotically
c1 R01 L0(V)logit(V)dVT + c2(x) R01 L0(V)logit2(V)dVT 2 + O(T 3).	(86)
In the case when L is linear, the first order term vanishes because L0 is constant and logit(V) is
odd about ɪ. However T2 and higher order even terms do not vanish, therefore the estimator is still
biased even for linear objectives.	□
This property sounds as a good and desirable one, but it’s advantage is practically nullified by the
next properties.
25
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
Proposition E.2. For any given realization z 6= η the norm of Gumbel-Softmax gradient estimate
asymptotically vanishes at the rate O( 1 c1/T) with C = e-|x| < 1.
Proof. Considering z fixed and denoting x = η - z, we need to check the asymptotic behavior of
_ X
dxστ (X)= 1PZ (X/t) = T(1+e-")2	(87)
as τ → 0+. Since pZ is symmetric, we may assume x > 0 without loss of generality. The
denominator is then asymptotically just T. Therefore the ratio is asymptotically O( 1 c1/T). 口
For small x, where C is close to one, the term 1∕τ dominates at first. In particular for Z = η, we
get c = 1 and the asymptote is O(1∕τ). So while for most of the noise realization the gradient
magnitude quickly vanishes, it is compensated by a significant grows at rate 1∕τ around Z = η. In
practice it means, most of the time a value of gradient close to zero is measured and occasionally,
very rarely, a value of O(1∕τ) is obtained.
Since the gradient is asymptotically unbiased, such quick diminishing for any fixed ε has to be
compensated by a rapid growth
Proposition E.3. The probability to observe the gradient of norm at least ε is asymptotically
O(T log(ε)), where the asymptote is T → 0, ε → 0.
Proof. We want to analyze the probability
P = P(dηστ(η -Z) ≥ ε)	(88)
when z is distributed logistically. Let S = oT(η - z). Then ddηo「(η - Z) = s(1 - s). The equality
s(1 - s) = ε holds for s* = 1 (1 - √1 - 4ε). This implies
Z1,2 = η 土 Tlogit(s*).	(89)
The inequality s(1 - s) ≥ ε holds in the interval [Z1, Z2 ]. Thus the probability in question is given
by
P = FZ(Z2) - FZ(Z1).	(90)
As Tlogit(s*) → 0 for T → 0, we have asymptotically that
P = pZ (η)T logit(s*).	(91)
Lastly note that logit(2(1 - √1 - 4ε)) is asymptotically O(- log ε) for ε → 0.	口
Proposition E.4. The variance of GS estimator grows as O(1).
Proof. We first show that the second moment of the estimator Gτ has the following asymptotic
expansion for T → 0+:
Pz(η)(j0 L0(v)2v(1 - v)) 1+C1(C L0(v)2v(1 - v)logit(v)dv)	(92)
+C2 R01 L0(v)2v(1 - v)logit2(v)dvT + O(T 2).	(93)
The second moment expresses as
E[GT] = R-∞(∂L(στ(η - z)))2Pz(z)dz	(94)
=RzO ʤ(n -Z))στ(η -Z)(I- στ(η -Z))I) Pz(Z)dz	(95)
We perform the same substitution of variables: V = σt(η - z), dv = -v(1 - v) 1 dZ to obtain
E[GT] = R0l L0(v)v(1 - v)1 L0(v)pz(η - Tlogit(v))dv	(96)
=1 Ro1 L0(v)2v(1 - v)pz(η - Tlogit(v))dv.	(97)
We perform the same Taylor expansion for Pz (η - T logit(v)) around T = 0 as in Proposition E.1
and combine the terms to obtain the expansion as claimed. The variance is therefore dominated by
the 0( t) term of the second moment.	□
Unlike ST, for linear objective L, this estimator is biased. Furthermore, unlike ST, even for a single
neuron it still has non-zero variance.
26
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
1560
1561
1562
1563
1564
1565
1566
1567
1568
1569
1570
1571
1572
1573
1574
1575
1576
1577
1578
1579
1580
1581
1582
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
ST Gumbel-Softmax Considering that relaxed variables deviate from binary samples on the for-
ward pass, Jang et al. (2016) also proposd the following empirical modification related to our main
topic on ST methods. Their ST Gumbel-Softmax estimator replaces the gradient estimator with
∂L(xι) ∂yι
∂xι ∂η ,
(98)
where both xi and yι are computed using the same logistic noise z. Since dLj1)does not vary with
Z unless x changes, to compute the expected gradient we can marginalize over Z locally in the two
regions: Z < η and Z > η. Let’s consider the first case. We have:
R-∞ Tστ(η - Z)(I - στ(η - Z))PZ(Z)dz
(99)
we make a substitution Z = η 一 Tlogit(v). Then V = στ (η 一 z) and dz = -T。(1二)dv. The integral
expresses as
0.5
-	pZ(η - τ logit(v))dv.
1
(100)
The limit of this integral with T → 0 is 2PZ(η). And the same holds for the case x1 = 0, i.e.,
Z > η. What we obtained, is that ST Gumbel-Softmax in the simple case of linear objective and in
the favorable limit τ → 0 underestimates the true gradient by 1/2.
To summarize, ST Gumbel-Softmax is more expensive as it involves sampling from Logistic distri-
bution and keeping the samples, it is biased, even asymptotically as τ → 0 and even in the case of
linear objective. It is also more noisy than ST as the gradient depends on the value of Z and not only
the binary state. In particular for one neuron it is still stochastic while ST becomes deterministic.
F Analysis of BayesBiNN
BayesBiNN Meng et al. (2020) algorithm is derived from the same variational Bayesian learning
problem formulation that we consider § C.2. The update step is also the same as they differentiate the
entropy regularization in closed form similarly to our composite MD. The only essential difference
is the use of Gumble-Softmax estimator to estimate the gradient in the probabilities θ used to update
the natural parameters η. We will show that the Gumbel-Softmax estimator is used incorrectly,
which leads to some surprising algorithm behavior.
First, it is easy to see that the properties of Gumbel-Softmax estimator § E for gradient in η apply as
well to the gradient in θ. Indeed, for Bernoulli distribution we have
∂η
∂θ
i
= diag(Pz(θ))-i.
(101)
This link between the two gradients does not depend on the relaxation parameter τ and therefore
the asymptotic properties § E apply to the Gumbel-Softmax gradient in θ as well. With parameter
τ = 10-10, the probability to measure a gradient larger than a numerical precision scales as O(τ)
and practically diminishes. Even if such a rare event occurs, the value of the measured gradient
scales as O(l∕τ). The experiments of Meng et al. (2020) could not possibly be successful without
a technical issue that we discuss next.
The BayesBiNN algorithm (Meng et al., 2020, Table 1 middle) performs the steps (in our notation):
Wb ：= tanh(η-z);
g ：= RWb L;
η ：= (1 - α)η - αs g,
(102a)
(102b)
(102c)
where g denotes the gradient of the average min-batch loss L, which is evaluated using softly bina-
rized weights wb, is a component-wise product and s is a scaling factor originating from Gumbel-
Softmax estimator and is discussed below.
The actual scaling factor s used in the experiments Meng et al. (2020) according to the published
code adds a technical ε in (Meng et al., 2020, Eq. (9)) in the implementation of Gumbel-Softmax
27
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
1625
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
1650
1651
estimator, presumably for a numerical stability. Coincidentally, ε = 10-10 is used. The resulting
scaling factor becomes:
c. _ N(I-(Wb)2+G
i = T(1—tanh(ηi)2+ε),
(103)
where N is the size of the complete training set.
Proposition F.1. With the setting of the hyper-parameters for τ, ε (Meng et al., 2020, Table 7) in
large-scale experiments (MNIST, CIFAR10, CIFAR100), the BayesBiNN algorithm is equivalent to
SGD with deterministic identity straight-through and latent weight decay.
Proof. For simplicity we will assume that η and g as scalars. For the general vector case the argu-
ments would apply coordinate-wise.
First, we analyze the nominator of (103). From the asymptotic expansion of
1 — tanh[log(x)]2 = χ42 + O(表),for X → ∞,	(104)
substituting log(x) = lη-z|, We obtain
1 — Wb 〜4exp ( — 2lη-zl).	(105)
For example, for η = 5 and z = 0 We have that (1 — wb2) ≈ 4e-1011. Therefore in the nominator the
part (1 — wb2 ) is negligibly small compared to ε and even to the floating point precision. This applies
so long as ∣η — z|》T = 10-10, which We expect to hold with high probability for two reasons:
1) η Will be shoWn to groW significantly during the first iterates and 2) the probability of the noise
matching η to this accuracy even for η = 0 is of the order O(τ).
The denominator of (103) satisfies the bounds
τε ≤ τ(1 — tanh(η)2 + ε) ≤ τ(1 + ε),	(106)
from which we can conclude that s ≥ Nε = N. However for a moderately large η the denominator
drops quickly, e.g. for η = 5, we have T(1 — tanh(η)2 + ε) < T(2 ∙ 10-4 + ε) ≈ 2 ∙ 10-4τ. And
the asymptote for ∣η∣ → ∞ is T(4e-2∣η∣ + ε).
Since η is initialized uniformly in [—10, 10] and receives updates of order at least αNg ≈ 5g (for
the initial α = 10-4 used), during the first iterates ∣η∣ can be expected to grow significantly until
we reach the asymptote e-2∣η∣《 ε, which is when ∣η∣ > 5log10 ≈ 11. After reaching this
asymptote, we will have S ≈ N = N and we may expect the growth of η to stabilize around
Inl ≈ αN |g| ≈ 1010.
The first consequence of this is that the scaling factor that was supposed to implement Gumbel-
Softmax gradient, just becomes the constant q.In particular an inadvertent factor ɪ occurs. It
is tempting to conclude that this up-scaling of the gradient corresponds to solving the variational
Bayesian problem with the data evidence part up-scaled respectively, i.e., completely dominating
the the KL prior part, but this is not exactly so because also the true stochastic scaling of the gradient
is modified.
The second consequence is that the natural parameters η have huge magnitudes during the training,
and we have that Iz I IηI with high probability, therefore the noise plays practically no role even
in the forward pass of BayesBiNN. In this mode the BayesBiNN algorithm becomes equivalent to
wb := sign(η);
g := RwbL
n ：= (1 — α)n — αNg.
(107a)
(107b)
(107c)
It is seen that the forward pass and the gradient implement the deterministic straight-through with
identity derivative and that the update has a form of SGD with a latent weight decay and with the
gradient of data evidence up-scaled by ɪ. These huge step-sizes do not destroy the learning since
sign is invariant to a global rescaling of n.	□
Proposition F.2. The result of the algorithm (107) does not depend on the values of T and N.
28
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
1668
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
1698
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
Proof. Denoting η = N η, we can equivalently rewrite (107) as
Wb := sign(η);	(108a)
g ：= NwbL	(108b)
η := (1 — α)η — αg.	(108c)
This algorithm and the resulting binary weights Wb do not depend on T, N.	□
This is perhaps somewhat unexpected, but it makes sense indeed. The initial BayesBiNN algorithm
of course depends on N and τ. However due to the issue with the implementation of Gumbel Soft-
max estimator for sufficiently small value of τ it falls into a regime which is significantly different
from the Bayesian learning rule and is instead more accurately described by (107). In this regime, it
produces the result not dependent on the particular values of τ and N. While we do not know what
problem it is solving in the end, it is certainly not solving the variational Bayesian learning problem.
This is so because the variational Bayesian learning problem and its solution do depend on N in a
critical way. The algorithm (108a) indeed does not solve any variational problem as there is no vari-
ational distribution involved (nothing sampled). Yet the decay term -αη stays effective: if the data
gradient becomes small, the decay term implements some small “forgetting” of the learned infor-
mation and may be responsible for an improved generalization observed in the experiments (Meng
et al., 2020). However there are other differences to the baselines, e.g. SGD vs. Adam that might be
also contributing to the generalization difference.
29