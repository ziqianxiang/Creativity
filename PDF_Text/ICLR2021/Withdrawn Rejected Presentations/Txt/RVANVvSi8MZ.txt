Under review as a conference paper at ICLR 2021
Weighted Line Graph Convolutional
Networks
Anonymous authors
Paper under double-blind review
Ab stract
Line graphs have shown to be effective in improving feature learning in graph
neural networks. Line graphs can encode topology information of their original
graphs and provide a complementary representational perspective. In this work,
we show that the encoded information in line graphs is biased. To overcome this
issue, we propose a weighted line graph that corrects biases in line graphs by as-
signing normalized weights to edges. Based on our weighted line graphs, we de-
velop a weighted line graph convolution layer that takes advantage of line graph
structures for better feature learning. In particular, it performs message passing
operations on both the original graph and its corresponding weighted line graph.
To address efficiency issues in line graph neural networks, we propose to use an
incidence matrix to accurately compute the adjacency matrix of the weighted line
graph, leading to dramatic reductions in computational resource usage. Experi-
mental results on both real and simulated datasets demonstrate the effectiveness
and efficiency of our proposed methods.
1	Introduction
Graph neural networks (Gori et al., 2005; Scarselli et al., 2009; Hamilton et al., 2017) have shown
to be competent in solving challenging tasks in the field of network embedding. Many tasks have
been significantly advanced by graph deep learning methods such as node classification tasks (Kipf
& Welling, 2017; VeliCkovic et al., 2017; Gao et al., 2018), graph classification tasks (Ying et al.,
2018; Zhang et al., 2018), link prediction tasks (Zhang & Chen, 2018; Zhou et al., 2019), and
community detection tasks (Chen et al., 2019). Currently, most graph neural networks capture the
relationships among nodes through message passing operations. Recently, some works (Chen et al.,
2019) use extra graph structures such as line graphs to enhance message passing operations in graph
neural networks from different graph perspectives. A line graph is a graph that is derived from an
original graph to represent connectivity between edges in the original graph. Since line graphs can
encode the topology information, message passing operations on line graphs can enhance network
embeddings in graph neural networks. However, graph neural networks that leverage line graph
structures need to deal with two challenging issues; those are bias and inefficiency. Topology infor-
mation in original graphs is encoded in line graphs but in a biased way. In particular, node features
are either overstated or understated depending on their degrees. Besides, line graphs can be much
bigger graphs than original graphs depending on the graph density. Message passing operations of
graph neural networks on line graphs lead to significant use of computational resources.
In this work, we propose to construct a weighted line graph that can correct biases in encoded
topology information of line graphs. To this end, we assign each edge in a line graph a normalized
weight such that each node in the line graph has a weighted degree of 2. In this weighted line
graph, the dynamics of node features are the same as those in its original graph. Based on our
weighted line graph, we propose a weighted line graph convolution layer (WLGCL) that performs
a message passing operation on both original graph structures and weighted line graph structures.
To address inefficiency issues existing in graph neural networks that use line graph structures, we
further propose to implement our WLGCL via an incidence matrix, which can dramatically reduce
the usage of computational resources. Based on our WLGCL, we build a family of weighted line
graph convolutional networks (WLGCNs). We evaluate our methods on graph classification tasks
and show that WLGCNs consistently outperform previous state-of-the-art models. Experiments on
simulated data demonstrate the efficiency advantage of our implementation.
1
Under review as a conference paper at ICLR 2021
(C)
Figure 1: Illustrations of an graph (a), its corresponding line graph (b), and its incidence graph (c).
2	Background and Related Work
In graph theory, a line graph is a graph derived from an undirected graph. It represents the con-
nectivity among edges in the original graph. Given a graph G, the corresponding line graph L(G)
is constructed by using edges in G as vertices in L(G). Two nodes in L(G) are adjacent if they
share a common end node in the graph G (Golumbic, 2004). Note that the edges (a, b) and (b, a) in
an undirected graph G correspond to the same vertex in the line graph L(G). The Whitney graph
isomorphism theorem (Thatte, 2005) stated that a line graph has a one-to-one correspondence to its
original graph. This theorem guarantees that the line graph can encode the topology information in
the original graph. Recently, some works (Monti et al., 2018; Chen et al., 2019; Bandyopadhyay
et al., 2019; Jiang et al., 2019) proposes to use the line graph structure to enhance the message pass-
ing operations in graph neural networks. Since the line graph can encode the topology information,
the message passing on the line graph can enhance the network embeddings in graph neural net-
works. In graph neural networks that use line graph structures, features are passed and transformed
in both the original graph structures and the line graph structures, thereby leading to better feature
learnings and performances.
3	Weighted Line Graph Convolutional Networks
In this work, we propose the weighted line graph to address the bias in the line graph when encoding
graph topology information. Based on our weighted line graph, we propose the weighted line graph
convolution layer (WLGCL) for better feature learning by leveraging line graph structures. Besides,
graph neural networks using line graphs consume excessive computational resources. To solve the
inefficiency issue, we propose to use the incidence matrix to implement the WLGCL, which can
dramatically reduce the usage of computational resources.
3.1	B enefit and Bias of Line Graph Representations
In this section, we describe the benefit and bias of using line graph representations.
Benefit In message-passing operations, edges are usually given equal importance and edge features
are not well explored. This can constrain the capacity of GNNs, especially on graphs with edge
features. In the chemistry domain, a compound can be converted into a graph, where atoms are
nodes and chemical bonds are edges. on such kinds of graphs, edges have different properties and
thus different importance. However, message-passing operations underestimate the importance of
edges. To address this issue, the line graph structure can be used to leverage edge features and
different edge importance (Jiang et al., 2019; Chen et al., 2019; Zhu et al., 2019). The line graph, by
its nature, enables graph neural networks to encode and propagate edge features in the graph. The
line graph neural networks that take advantage of line graph structures have shown to be promising
on graph-related tasks (Chen et al., 2019; Xiong et al., 2019; Yao et al., 2019). By encoding node and
edge features simultaneously, line graph neural networks enhance the feature learning on graphs.
Bias According to the Whitney graph isomorphism theorem, the line graph L(G) encodes the topol-
ogy information of the original graph G, but the dynamics and topology of G are not correctly
represented in L(G) (Evans & Lambiotte, 2009). As described in the previous section, each edge
in the graph G corresponds to a vertex in the line graph L(G). The features of each edge con-
tain features of its two end nodes. A vertex with a degree d in the original graph G will generate
2
Under review as a conference paper at ICLR 2021
(C)
Figure 2: An illustration of a graph (a), its corresponding line graph (b), and its weighted line
graph (c). Here, we consider a graph with 4 nodes and 4 edges as illustrated in (a). The numbers
show the node degrees in the graph. In figure (b), a line graph is constructed with self-loops. Each
node corresponds to an edge in the original graph. In the regular line graph, the weight of each
edge is 1. Figure (c) illustrates the weighted line graph constructed as described in Section 3.2. The
weight of each edge is assigned as defined in Eq.(1).
d X (d - 1)/2 edges in the line graph L(G). The message passing frequency of this node,s features
will change from O(d) in the original graph to O(d2) in the line graph. From this point, the line
graph encodes the topology information in the original graph but in a biased way. In the original
graph, a node,s features will be passed to d neighbors. But in the corresponding line graph, the in-
formation will be passed to d × (d - 1)/2 nodes. The topology structure in the line graph L(G) will
overstate the importance of features for nodes with high degrees in the graph. On the contrary, the
nodes with smaller degrees will be relatively understated, thereby leading to biased topology infor-
mation encoded in the line graph. Note that popular adjacency matrix normalization methods (Kipf
& Welling, 2017; VeIiCkOViC et al., 2017; Gao & Ji, 2019; Gong & Cheng, 2019) cannot address this
issue.
3.2	Weighted Line Graph
In the preVious section, we show that the line graph L(G) constructed from the original graph
G encodes biased graph topology information. To address this issue, we propose to construct a
weighted line graph that assigns normalized weights to edges. In a regular line graph L(G), each
edge is assigned an equal weight of 1, thereby leading to a biased encoding of the graph topology
information. To correct the bias, we need to normalize edge weights in the line graph.
Considering each edge in G has two ends, it is intuitiVe to normalize the weighted degree of the
corresponding node in L(G) to 2. To this end, the weight for an edge in the adjacency matrix F of
L(G) is computed as:
D d1-	if a = C
FabMba= 3 + d1 , if a = C
DbDa
(1)
where a, b, and c are nodes in the graph G, (a, b) and (b, c) are edges in the graph G that are
connected by the node b. Db is the degree of the node b in the graph G. To facilitate the message
passing operation, we add self-loops on the weighted line graph W L(G). The weights for self-loop
edges computed by the second case consider the fact that they are self-connected by both ends.
Figure 2 illustrates an example of a graph and its corresponding weighted line graph.
Theorem 1. Given the edge weights in the weighted line graph W L(G) defined by Eq. (1), the
weighted degree for a node (a, b) in W L(G) is 2.
The proof of Theorem 1 is proVided in the supplementary material. By constructing the weighted
line graph with normalized edge weights defined in Eq. (1), each node (a, b) has a weighted degree
of2. GiVen a node a with a degree of d, it has d related edges in G and d related nodes in L(G). The
message passing frequency of node a,s features in the weighted line graph W L(G) is Pid=1 2 =
O(d), which is consistent with that in the original graph G. Thus, the weighted line graph encodes
the topology information of the original graph in an unbiased way.
3
Under review as a conference paper at ICLR 2021
Figure 3: An illustration of our proposed weighted line graph convolution layer. We consider an
input graph with 4 nodes and each node contains 2 features. Based on the input graph, We firstly
construct the weighted line graph with features as described in Section 3.2. Then We apply two GCN
layers on the original graph and the weighted line graph, respectively. The edge features in the line
graph are transformed back into node features and combined with features in the original graph.
3.3	Weighted Line Graph Convolution Layer
In this section, we propose the weighted line graph convolution layer (WLGCL) that leverages our
proposed weighted line graph for feature representations learnings. In this layer, node features are
passed and aggregated in both the original graph G and the corresponding weighted line graph
W L(G).
Suppose an undirected attributed graph G has N nodes and E edges. Each node and each edge in the
graph contains Cn and Ce features, respectively. In the layer ', an adjacency matrix A(') ∈ RN×N,
a node feature matrix X(') ∈ RN ×Cn, and a edge feature matrix Y (') ∈ RE×Ce are used to
represent the graph connectivity, node features, and edge features, respectively. Here, we construct
the adjacency matrix F (`) ∈ RE×E of the corresponding weighted line graph. The layer-wise
propagation rule of the weighted line graph convolution layer ` is defined as:
Y (')=	F (E)Y (E)	∈RE×Ce	(2)
KL =	B（'）Y（'）,	∈RN×Ce	(3)
K⑶=	A(E)X(E),	∈RN×Cn	(4)
X ('+1)	= K(E) W(E) +KL(E)WL(E),	∈RN×C0	(5)
where W(') ∈ RCn × C0 and wL') ∈ RCe ×C0 are matrices of trainable parameters. B(') ∈ RN×E
is the incidence matrix of the graph G that shows the connectivity between nodes and edges.
To enable message passing on the line graph L(G), each edge in the graph G needs to have features.
However, edge features are not available on some graphs. To address this issue, we can compute
features for an edge (a, b) by summing up features of its two end nodes: Y(，b)= xi') + Xb'. Here,
we use the summation operation to ensure the permutation invariant property in this layer. Then, we
perform message passing and aggregation on the line graph in Eq. (2). With updated edges features,
Eq. (3) generates new nodes features with edge features Y('). Eq. (4) performs feature passing and
aggregation on the graph G, which leads to aggregated nodes features K('). In Eq. (5), aggregated
features from the graph G and the line graph L(G) are transformed and combined, which produces
the output feature matrix X('+1). Note that we can apply popular adjacency matrix normalization
methods (Kipf & Welling, 2017) on the adjacency matrix A('), the line graph adjacency matrix
F('), and the incidence matrix B(').
In the WLGCL, we use the line graph structure as a complement to the original graph structure,
thereby leading to enhanced feature learnings. Here, we use a simple feature aggregation method
as used in GCN (Kipf & Welling, 2017). Other complicated and advanced feature aggregation
methods such as GAr (VeIiCkOViC et al., 2017) can be easily applied by changing Eq. (2) and Eq. (4)
accordingly. Figure 3 provides an illustration of our WLGCL.
4
Under review as a conference paper at ICLR 2021
Figure 4: An illustration of the weighted line graph convolution network. The input graph is an
undirected attributed graph. Each node in the graph contains two features. Here, we use a GCN
layer to produce low-dimensional continuous feature representations. In each of the following two
blocks, we use a layer and a layer for feature learning and graph coarsening, respectively. We use a
multi-layer perceptron network for classification.
3.4	Weighted Line Graph Convolution Layer via Incidence Matrix
In this section, we propose to implement the WLGCL using the incidence matrix. When edge
features are generated from node features as described in Section 3.3, it can significantly reduce the
usage of computational resources while taking advantage of the line graph structure.
One practical challenge of using a line graph structure is that it consumes excessive computational
resources in terms of memory usage and execution time. To use a line graph in a graph neural
network, we need to store its adjacency matrix, compute edge features, and perform message passing
operation. Our proposed WLGCL also faces this challenge. Space and time complexities ofEq.(2),
which plays the dominating role, are O(E2)=O(N4) and O(E2C)=O(N4C), respectively.
Here, we set Cn = Ce for simplicity. To address this issue, we propose to use the incidence
matrix B to compute the weighted line graph adjacency matrix F . The adjacency matrix F can be
accurately computed with the following theorem.
Theorem 2. Given an undirected graph, its incidence matrix B ∈ RN ×E, and its degree matrix
D ∈ RN, the adjacency matrix F ∈ RE×E of the weighted line graph with weights defined by
Eq. (1) can be exactly computed by
F = BTdiag (D)-1 B,
(6)
where diag(∙) takes a vector as input and constructs a squared diagonal matrix using the vector
elements as the main diagonal elements.
The proof of Theorem 2 is provided in the supplementary material. Based on the results from
Theorem 2, we can update the equations (Eq. (2-3)) to generate KL(E) in the WLGCL by replacing
the adjacency matrix F with Eq. (6):
KL(E) = B(E)F (E)B(E)TX(E) = B(E)B(E)T diag (D)-1 B(E)B(E)TX(E)
= H(E)diag (D)-1 H(E)X(E),
(7)
where B(')TX(') computes edge features using node features. Notably, H(') = B(E)B(E)T only
needs to be computed once. With computed K?, we output the new feature matrix X('+1) using
equations Eq. (4) and Eq. (5).
By using the implementation in Eq. (7), space and time complexities of the WLGCL are reduced
to O(N × E) = O(N3) and O(N2 × E) + O(N2 × C) = O(N4), respectively. Compared to
the naive WLGCL implementation, they are reduced by a factor of N and C, respectively. in the
experimental study part, we show that the WLGCL implemented as Eq. (7) dramatically saves the
computational resources compared to the naive implementation. Notably, the results in Eq. (6) can
be applied to other graph neural networks that leverage the benefits of line graph structures.
3.5	Weighted Line Graph Convolutional Networks
in this section, we build a family of weighted line graph convolutional networks (WLGCNets) that
utilize our proposed WLGCLs. in WLGCNets, an embedding layer such as a fully-connected layer
or GCN layer is firstly used to learn low-dimensional representations for nodes in the graph. Then
5
Under review as a conference paper at ICLR 2021
Table 1: Comparison of WLGCNet and previous state-of-the-art models including WL (Sher-
vashidze et al., 2011), PSCN (Niepert et al., 2016), DGCNN, SAGPool (Lee et al., 2019), DIFF-
POOL, g-U-Net, and GIN on graph classification datasets. We report the graph classification accura-
cies (%) on PROTEINS, D&D, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI5K, COLLAB,
and REDDIT-MULTI12K datasets._______________________________________________________________________________
	PROTEINS	D&D	IMDBM	RDTB	RDT5K	COLLAB	RDT12K
graphs	1113	1178	1500	2000	4999	5000	11929
nodes	39.1	284.3	13	429.6	508.5	74.5	391.4
classes	2	2	3	一	2	5	3	11
WL	75.0 ± 3.1	78.3 ± 0.6	50.9 ± 3.8	81.0 ± 3.1	52.5 ± 2.1	78.9 ± 1.9	44.4 ± 2.1
DGCNN	75.5 ± 0.9	79.4 ± 0.9	47.8 ± 0.9	-	-	73.8 ± 0.5	41.8 ± 0.6
PSCN	75.9 ± 2.8	76.3 ± 2.6	45.2 ± 2.8 一	86.3 ± 1.6	49.1 ± 0.7	72.6 ± 2.2	41.3 ± 0.8
DIFFPOOL	76.3	80.6	-	-	-	75.5	47.1
SAGPool	71.9	76.5	-	-	-	-	-
g-U-Net	77.6 ± 2.6	82.4 ± 2.9	51.8 ± 3.7	85.5 ± 1.3	48.2 ± 0.8	77.5 ± 2.1	44.5 ± 0.6
GIN	76.2 ± 2.8	82.0 ± 2.7	52.3 ± 2.8	92.4 ± 2.5	57.5 ± 1.5	80.6 ± 1.9	-
WLGCNet	78.9 ± 4.2	83.8 ± 2.8	56.1 ± 3.6	94.1 ± 2.2	58.2 ± 3.2	83.1 ± 7.9	50.3 ± 1.5
we stack multiple blocks, each of which consists ofa WLGCL and a pooling layer (Gao & Ji, 2019).
Here, the WLGCL encodes high-level features while the pooling layer outputs a coarsened graph.
We use the gPool layer to produce a coarsened graph that helps to retain original graph structure
information. To deal with the variety of graph sizes in terms of the number of nodes, we apply
global readout operations on the outputs including maximization, averaging and summation (Xu
et al., 2018). The outputs of the first GCN layer and all blocks are stacked together in the feature
dimension and fed into a multi-layer perceptron network for classification. Figure 4 provides an
example of our WLGCNets.
4	Experimental Study
In this section, we evaluate our proposed WLGCL and WLGCNet on graph classification tasks. We
demonstrate the effectiveness of our methods by comparing our networks with previous state-of-
the-art models in terms of the graph classification accuracy. Besides, we evaluate the efficiency of
our implementation of the WLGCL in terms of the usage of computational resources. We conduct
ablation experiments to demonstrate the contributions of our methods. The code and experimental
setups are provided in the supplementary material.
4.1	Performance Study
To evaluate our methods and WLGCNets, we conduct experiments on graph classification tasks us-
ing seven datasets; those are PROTEINS, D&D (Dobson & Doig, 2003), IMDB-MULTI (IMDBM),
REDDIT-BINARY (RDTB), REDDIT-MULTI5K (RDT5K), COLLAB, and REDDIT-MULTI12K
(RDT12K) (Yanardag & Vishwanathan, 2015). REDDIT datasets are benchmarking large graph
datasets used for evaluating graph neural networks in the community. On the datasets without node
features such as RDT12K, we use one-hot encodings of node degrees as node features (Xu et al.,
2018). To produce less biased evaluation results, we follow the practices in (Xu et al., 2018; Zhang
et al., 2018) and perform 10-fold cross-validation on training datasets. We use the average accuracy
across 10 fold testing results with variances.
We report the graph classification accuracy along with performances of previous state-of-the-art
models. The results are summarized in Table 1. We can observe from the results that our proposed
WLGCNets significantly outperform previous models by margins of 1.3%, 1.8%, 3.8%, 1.7%, 0.7%,
2.5%, 3.2% on PROTEINS, D&D, IMDB-MULTI, REDDIT-BINARY, REDDIT-MULTI5K, COL-
LAB, and REDDIT-MULTI12K datasets, respectively. The promising results, especially on large
benchmarking datasets such as REDDIT-MULTI12K, demonstrate the effectiveness of our proposed
methods and models for network embeddings. Note that our WLGCNet uses the gPool layer from
the g-U-Net. The superior performances of WLGCNets over the g-U-Net demonstrate the perfor-
mance gains are from our proposed WLGCLs.
6
Under review as a conference paper at ICLR 2021
Table 2: Comparison of WLGCL and the WLGCL using native implementation (denoted as
WLGCLn). We evaluate them on simulated data with different graph sizes in terms of the num-
ber of nodes and the number of edges. All layers output 64 feature channels. We report the number
of multiply-adds (MAdd), the amount of memory usage, and the CPU execution time. We describe
the input graph size in the format of “number of nodes / number of edges”.
Input	Operator	MAdd	Saving	Memory	Saving	Time	Speedup
1000/50000	WLGCLn	166.47B	^^0.00%^^	9.5GB	0.00%	15.73s	1.0×
	WLGCL	51.13B	69.28%	0.19GB	97.94%	0.63s	26.2×
1000/100000	WLGCLn	652.87B	^^0.00%^^	37.63GB	0.00%	56.62s	1.0×
	WLGCL	101.13B	84.51%	0.38GB	98.98%	1.21s	47.2×
1000/150000	WLGCLn	1,459.27B	^^000%^^	86.71GB	0.00%	134.36s	1.0×
	WLGCL	151.13B	89.64%	0.57GB	99.34%	1.82s	74.6×
2000/150000	WLGCLn	1,478.66B	^^000%^^	99.87GB	0.00%	278.8s	1.0×
	WLGCL	608.52B	58.85%	1.13GB	98.86%	6.19s	45.1×
4.2	Computational Efficiency S tudy
In Section 3.4, we propose an efficient implementation of WLGCL using the incidence matrix,
which can save dramatic computational resources compared to the naive one. Here, we conduct
experiments on simulated data to evaluate the efficiency of our methods. We build networks that
contain a single layer to remove the influence of other factors. We conduct experiments on graphs
of different sizes in terms of the number of nodes. Since WLGCL takes advantage of line graph
structures, the graph density has a significant impact on the layer efficiency. Here, the graph den-
sity is defined as 2E/(N × (N - 1)). To investigate the impact of the graph density, we conduct
experiments on graphs with the same size but different numbers of edges.
By using the TensorFlow profile tool (Abadi et al., 2016), we report the computational resources
used by networks including the number of multiply-adds (MAdd), the amount of memory usage,
and the CPU execution time. The comparison results are summarized in Table 2. We can observe
from the results that the WLGCLs with our proposed implementation use significantly less com-
putational resources than WLGCLs with naive implementation in terms of the memory usage and
CPU execution time. By comparing the results on first three inputs, the advantage on efficiency of
our method over the naive implementation becomes much larger as the increase of the graph density
with the same graph size. When comparing results of the last two inputs with the same number of
edges but different graph sizes, we can observe that the efficiency advantage of our proposed meth-
ods remains the same. This shows that the graph density is a key factor that influences the usage of
computational resources, especially on dense graphs.
4.3	Results on Small Datasets
In the previous section, we evaluate our meth-
ods on benchmarking datasets that are relatively
large in terms of the number of graphs and the
number of nodes in graphs. To provide a com-
prehensive evaluation, we conduct experiments
on relatively small datasets to evaluate the risk
of over-fitting of our methods. Here, we use
three datasets; those are MUTAG (Wale et al.,
2008), PTC (Toivonen et al., 2003), and IMDB-
BINARY (Borgwardt et al., 2005). MUTAG
and PTC datasets are bioinformatics datasets
with categorical features on nodes. We fol-
low the same experimental settings as in Sec-
tion 4.1. The results in terms of the graph clas-
Table 3: Comparison of WLGCNet and previ-
ous state-of-the-art models on relatively small
datasets. We report the graph classification accu-
racies on MUTAG, PTC, and IMDBB datasets.
	MUTAG	PTC	IMDBB
graphs	188	344	1000
nodes	17.9	25.5	19.8
WL	90.4 ± 5.7	59.9 ± 4.3	73.8 ± 3.9
DGCNN	85.8 ± 1.7	58.6 ± 2.4	70.0 ± 0.9
PSCN	92.6 ± 4.2	60.0 ± 4.8	71.0 ± 2.2
g-U-Net	87.2 ± 7.8	64.7 ± 6.8	75.4 ± 3.0
GIN	90.0 ± 8.8	64.6 ± 7.0	75.1 ± 5.1
WLGCNet	93.0 ± 5.8	72.7 ± 6.0	78.8 ± 5.1
sification accuracy are summarized in Table 3 with performances of previous state-of-the-art mod-
els. We can observe from the results that our WLGCNet outperforms previous models by margins of
0.4%, 6.0%, and 3.4% on MUTAG, PTC, and IMDB-BINARY, respectively. This demonstrates that
our proposed models will not increase the risk of the over-fitting problem even on small datasets.
7
Under review as a conference paper at ICLR 2021
4.4	Ablation Study of Weighted Line Graph Convolution Layers
In this section, we conduct ablation studies
based on WLGCNets to demonstrate the con-
tribution of our WLGCLs to the entire network.
To explore the advantage of line graph struc-
tures, we construct a network that removes all
layers using line graphs. Based on the WL-
GCNet, we replace WLGCLs by GCNs us-
ing the same number of trainable parameters,
which we denote as WLGCNetg . To compare
our weighted line graph with the regular line
graph, we modify our WLGCLs to use regu-
lar line graph structures. We denote the re-
sulting network as WLGCNetl . We evaluate
Table 4: Comparison of WLGCNet, the net-
work using the same architecture as WLGCNet
with GCN layers (denoted as WLGCNetg), the
network using the same architecture as WLGC-
Net with regular line graph convolution layers
(denoted as WLGCNetl). We report the graph
classification accuracies on REDDIT-BINARY,
REDDIT-MULTI5K, and REDDIT-MULTI12K.
	RDTB	RDT5K	RDT12K
WLGCNetg	93.2 ± 1.5	56.9 ± 2.2	49.1 ± 1.5
WLGCNetl	93.6 ± 2.0	57.3 ± 3.0	49.6 ± 2.8
WLGCNet	94.1 ± 2.2	58.2 ± 3.2	50.3 ± 1.5
these networks on three datasets; those are REDDIT-BINARY, REDDIT-MULTI5K, and REDDIT-
MULTI12K datasets. Table 4 summaries the graph classification results. We can observe from the
results that both WLGCNet and WLGCNetl achieve better performances than WLGCNetg , which
demonstrates the benefits of utilizing line graph structures on graph neural networks. When com-
paring WLGCNet with WLGCNetl, WLGCNet outperforms WLGCNetl by margins of 0.5%, 0.5%,
and 0.7% on REDDIT-BINARY, REDDIT-MULTI5K, and REDDIT-MULTI12K datasets, respec-
tively. This indicates that our proposed WLGCL utilizes weighted line graph structures with unbi-
ased topology information encoded, thereby leading to better performances.
4.5	Network Depth Study
Network depth in terms of the number ofblocks
is an important hyper-parameter in the WLGC-
Net. In previous experiments, We use three
blocks in WLGCNets based on our empirical
experiences. In this section, we investigate the
impact of the network depth in WLGCNets on
network embeddings. Based on our WLGCNet,
we vary the network depth from 1 to 5, which
covers a reasonable range. We evaluate these
networks on PTC, PROTEINS, and REDDIT-
BINARY datasets and report the graph classifi-
cation accuracies. Figure 5 plots the results of
WLGCNets with different numbers of blocks.
We can observe from the figure that the best
performances are achieved on WLGCNets with
three blocks on all three datasets. When the net-
90%
85%
80%
75%
70%
65%
Figure 5: Comparison of WLGCNets with dif-
ferent depths on PTC, PROTEINS, and REDDIT-
BINARY. We report the classification accuracies.
work depth increases, the performances decrease, which indicates the over-fitting issue.
5	Conclusion
In this work, we consider the biased topology information encoding in graph neural networks that
utilize line graph structures to enhance network embeddings. A line graph constructed from a graph
can encode the topology information. However, the dynamics in the line graph are inconsistent
with that in the original graph. On line graphs, the features of nodes with high degrees are more
frequently passed in the graph, which causes understatement or overstatement of node features. To
address this issue, we propose the weighted line graph that assigns normalized weights on edges
such that the weighted degree of each node is 2. Based on the weighted line graph, we propose
the weighted line graph layer that leverages the advantage of the weighted line graph structure. A
practical challenge faced by graph neural networks on line graphs is that they consume excessive
computational resources, especially on dense graphs. To address this limitation, we propose to use
the incidence matrix to implement the WLGCL, which can dramatically save the computational
resources. Based on the WLGCL, we build a family of weighted line graph convolutional networks.
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th {USENIX} Symposium on Operating Systems Design and Imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
Sambaran Bandyopadhyay, Anirban Biswas, MN Murty, and Ramasuri Narayanam. Beyond node
embedding: A direct unsupervised edge representation framework for homogeneous networks.
arXiv preprint arXiv:1912.05140, 2019.
Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(SUPPL1):
i47-i56, 2005.
Zhengdao Chen, Joan Bruna Estrach, and Lisha Li. Supervised community detection with line graph
neural networks. In 7th International Conference on Learning Representations, ICLR 2019, 2019.
Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology, 330(4):771-783, 2003.
TS Evans and Renaud Lambiotte. Line graphs, link partitions, and overlapping communities. Phys-
ical Review E, 80(1):016105, 2009.
Hongyang Gao and Shuiwang Ji. Graph U-Nets. In International Conference on Machine Learning,
pp. 2083-2092, 2019.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge
Discovery & Data Mining, pp. 1416-1424. ACM, 2018.
Martin Charles Golumbic. Algorithmic graph theory and perfect graphs. Elsevier, 2004.
Liyu Gong and Qiang Cheng. Exploiting edge features for graph neural networks. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9211-9219, 2019.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In 2005 IEEE International Joint Conference on Neural Networks, volume 2, pp. 729-734, 2005.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Xiaodong Jiang, Pengsheng Ji, and Sheng Li. Censnet: convolution with edge-node switching in
graph neural networks. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence, pp. 2656-2662. AAAI Press, 2019.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. The International
Conference on Learning Representations, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. International Conference on Learning Representations, 2017.
Junhyun Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In Proceedings of The
36th International Conference on Machine Learning, 2019.
Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan GUnnemann,
and Michael M Bronstein. Dual-primal graph convolutional networks. arXiv preprint
arXiv:1806.00770, 2018.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In International Conference on Machine Learning, pp. 2014-2023, 2016.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.
9
Under review as a conference paper at ICLR 2021
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-Iehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539-
2561, 2011.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929-1958, 2014.
Bhalchandra D Thatte. Kocay’s lemma, whitney’s theorem, and some polynomial invariant recon-
struction problems. the electronic journal of combinatorics, pp. R63-R63, 2005.
Hannu Toivonen, Ashwin Srinivasan, Ross D King, Stefan Kramer, and Christoph Helma. Statistical
evaluation of the predictive toxicology challenge 2000-2001. Bioinformatics, 19(10):1183-1193,
2003.
Petar VeliCkovic, Guillem CUcurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2017.
Nikil Wale, Ian A Watson, and George Karypis. Comparison of descriptor spaces for chemical com-
pound retrieval and classification. Knowledge and Information Systems, 14(3):347-375, 2008.
Xi Xiong, Kaan Ozbay, Li Jin, and Chen Feng. Dynamic prediction of origin-destination flows using
fusion line graph convolutional networks. arXiv preprint arXiv:1905.00406, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Pinar Yanardag and SVN Vishwanathan. A structural smoothing framework for robust graph com-
parison. In Advances in Neural Information Processing Systems, pp. 2134-2142, 2015.
Weichi Yao, Afonso S Bandeira, and Soledad Villar. Experimental performance of graph neural
networks on random instances of max-cut. In Wavelets and Sparsity XVIII, volume 11138, pp.
111380S. International Society for Optics and Photonics, 2019.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graph representation learning with differentiable pooling. In Advances in Neural Infor-
mation Processing Systems, pp. 4800-4810, 2018.
Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Advances in
Neural Information Processing Systems, pp. 5165-5175, 2018.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In Thirty-Second AAAI Conference on Artificial Intelligence,
pp. 4438-4445, 2018.
Kai Zhou, Tomasz P Michalak, Marcin Waniek, Talal Rahwan, and Yevgeniy Vorobeychik. Attack-
ing similarity-based link prediction in social networks. In Proceedings of the 18th International
Conference on Autonomous Agents and MultiAgent Systems, pp. 305-313. International Founda-
tion for Autonomous Agents and Multiagent Systems, 2019.
Shichao Zhu, Chuan Zhou, Shirui Pan, Xingquan Zhu, and Bin Wang. Relation structure-aware
heterogeneous graph neural network. In 2019 IEEE International Conference on Data Mining
(ICDM), pp. 1534-1539. IEEE, 2019.
10
Under review as a conference paper at ICLR 2021
A	Experimental Setup
We describe the experimental setup for graph classification tasks. In this work, we mainly evaluate
our methods on graph classification datasets such as social network datasets and bioinformatics
datasets. The node features are created using one-hot encodings and fed into the networks. In
WLGCNets, we use GCN layers as the graph embedding layers. After the first GCN layer, we stack
three blocks as described in Section 3.5. The outputs of the GCN layer and WLGCLs in three blocks
are processed by a readout function and concatenated as the network output. The readout function
performs three global pooling operations; those are maximization, averaging, and summation. The
network outputs are fed into a classifier to produce predictions. Here, we use a two-layer feed-
forward network with 512 units in the hidden layer as the classifier. We apply dropout (Srivastava
et al., 2014) on the network and the classifier.
We use an Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.001 to train WLGCNets.
To prevent over-fitting, we apply the L2 regularization on trainable parameters with a weight decay
rate of 0.0008. All models are trained for 200 epochs using one NVIDIA GeForce RTX 2080 Ti
GPU on an Ubuntu 18.04 system.
B Proof for Theorem 1
Proof. Given nodes a and b with degrees Da and Db in a graph G, a node (a, b) in the corresponding
weighted line graph WL(G) connects to Da- 1 and Db- 1 nodes through a and b in G, respectively.
The weighted degree of the node (a, b) is computed by summing up the weights of edges that connect
(a, b) to other nodes through a and b, and the weight of its self-loop:
Da-1 1 Db-1 1
WLD(a，b)	= X	D	+ X	D	+
i=1 a j=1 b
Da	Db
+ =2.
i=1 a j=1 b
This completes the proof.
+
(8)
□
C Proof for Theorem 2
Proof. We construct a weighted incidence matrix by normalizing the weights as Bi,(i,j) = 1/Di.
Thus, the weighted incidence matrix is computed as B = diag (D)T B. In the incidence graph,
each edge is connected to its two end nodes. Thus, each column in the incidence matrix B:,(a,b)
has two non-zero entries; those are Ba,(a,b) and Bb,(a,b). The same rule applies to the weighted
incidence matrix B. Based on this observation, We have
N
(BTB)…=X BTa，b)，i × Ab,。)
i=1
=BTsb),aBa,(b,c) + BTl,b),bBb,(b,C	(9)
D D1-	if a = C
=点 + 亡 if a = c = Fkb)S
This completes the proof.	□
11