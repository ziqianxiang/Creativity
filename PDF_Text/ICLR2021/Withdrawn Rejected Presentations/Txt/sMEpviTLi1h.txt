Under review as a conference paper at ICLR 2021
Provably Faster Algorithms for Bilevel Opti-
mization and Applications to Meta-Learning
Anonymous authors
Paper under double-blind review
Ab stract
Bilevel optimization has arisen as a powerful tool for many machine learning
problems such as meta-learning, hyperparameter optimization, and reinforcement
learning. In this paper, we investigate the nonconvex-strongly-convex bilevel op-
timization problem. For deterministic bilevel optimization, we provide a compre-
hensive finite-time convergence analysis for two popular algorithms respectively
based on approximate implicit differentiation (AID) and iterative differentiation
(ITD). For the AID-based method, we orderwisely improve the previous finite-
time convergence analysis due to a more practical parameter selection as well as a
warm start strategy, and for the ITD-based method we establish the first theoretical
convergence rate. Our analysis also provides a quantitative comparison between
ITD and AID based approaches. For stochastic bilevel optimization, we propose a
novel algorithm named stocBiO, which features a sample-efficient hypergradient
estimator using efficient Jacobian- and Hessian-vector product computations. We
provide the finite-time convergence guarantee for stocBiO, and show that stocBiO
outperforms the best known computational complexities orderwisely with respect
to the condition number κ and the target accuracy . We further validate our the-
oretical results and demonstrate the efficiency of bilevel optimization algorithms
by the experiments on meta-learning and hyperparameter optimization.
1 Introduction
Bilevel optimization has received significant attention recently and become an influential framework
in various machine learning applications including meta-learning (Franceschi et al., 2018; Bertinetto
et al., 2018; Rajeswaran et al., 2019; Ji et al., 2020a), hyperparameter optimization (Franceschi et al.,
2018; Shaban et al., 2019; Feurer & Hutter, 2019), reinforcement learning (Konda & Tsitsiklis, 2000;
Hong et al., 2020), and signal processing (Kunapuli et al., 2008; Flamary et al., 2014). A general
bilevel optimization takes the following formulation.
min Φ(x) := f (x, y* (x)) s.t. y*(x) = arg ming(x, y),
x∈Rp	y∈Rq
(1)
where the upper- and inner-level functions f and g are both jointly continuously differentiable. The
goal of eq. (1) is to minimize the objective function Φ(x) w.r.t. x, where y* (x) is obtained by solving
the lower-level minimization problem. In this paper, we focus on the setting where the lower-level
function g is strongly convex with respect to (w.r.t.) y, and the upper-level objective function Φ(x)
is nonconvex w.r.t. x. Such types of geometrics commonly exist in many applications including
meta-learning and hyperparameter optimization, where g corresponds to an empirical loss with a
strongly-convex regularizer and x are parameters of neural networks.
A broad collection of algorithms have been proposed to solve such types of bilevel optimization
problems. For example, Hansen et al. (1992); Shi et al. (2005); Moore (2010) reformulated the
bilevel problem in eq. (1) into a single-level constrained problem based on the optimality condi-
tions of the lower-level problem. However, such type of methods often involve a large number of
constraints, and are hard to implement in machine learning applications. Recently, more efficient
gradient-based bilevel optimization algorithms have been proposed, which can be generally catego-
rized into the approximate implicit differentiation (AID) based approach (Domke, 2012; Pedregosa,
2016; Gould et al., 2016; Liao et al., 2018; Ghadimi & Wang, 2018; Grazzi et al., 2020; Lorraine
1
Under review as a conference paper at ICLR 2021
et al., 2020) and the iterative differentiation (ITD) based approach (Domke, 2012; Maclaurin et al.,
2015; Franceschi et al., 2017; 2018; Shaban et al., 2019; Grazzi et al., 2020). However, most of
these studies have focused on the asymptotic convergence analysis, and the finite-time analysis (that
characterizes how fast an algorithm converges) has not been well explored except a few attempts
recently. Ghadimi & Wang (2018) provided the finite-time analysis for the ITD-based approach.
Grazzi et al. (2020) provided the iteration complexity for the hypergradient computation via ITD
and AID, but did not characterize the finite-time convergence for the entire execution of algorithms.
•	Thus, the first focus of this paper is to develop a comprehensive and enhanced theory, which cov-
ers a broader class of bilevel optimizers via ITD and AID based techniques, and more importantly,
to improve the exiting analysis with a more practical parameter selection and order-level lower
computational complexity.
The stochastic bilevel optimization often occurs in applications where fresh data need to be sampled
as the algorithms run (e.g., reinforcement learning (Hong et al., 2020)) or the sample size of training
data is large (e.g., hyperparameter optimization (Franceschi et al., 2018), Stackelberg game (Roth
et al., 2016)). Typically, the corresponding objective function is given by
min Φ(x) = f (x, y*(x)) := {
s.t. y*(x) = arg min g(x,y):
y∈Rq
Eξ [F (x,y*(x); ξ)]
1 Pn=IF (x,y*(x); ξi)
=(EZ [G(χ,y*(χ); ζ)]
-1+ Pi=ι G(χ,y*(χ); Zi),
(2)
where f(x, y) and g(x, y) take either the expectation form w.r.t. the random variables ξ and ζ or the
finite-sum form over given data Dn,m = {ξi, ζj, i = 1, ..., n; j = 1, ..., m} often with large sizes n
and m. During the optimization process, the algorithms sample data batch via the distributions of ξ
and ζ or from the set Dn,m . For such a stochastic setting, Ghadimi & Wang (2018) proposed a bilevel
stochastic approximation (BSA) method via single-sample gradient and Hessian estimates. Based
on such a method, Hong et al. (2020) further proposed a two-timescale stochastic approximation
(TTSA), and showed that TTSA achieves a better trade-off between the complexities of inner- and
outer-loop optimization stages than BSA.
•	The second focus of this paper is to design a more sample-efficient algorithm for bilevel stochastic
optimization, which achieves an order-level lower computational complexity over BSA and TTSA.
1.1	Main Contributions
Our main contributions lie in developing enhanced theory and provably faster algorithms for the
nonconvex-strongly-convex bilevel deterministic and stochastic optimization problems, respectively.
Our analysis involves several new developments, which can be of independent interest.
We first provide a unified finite-time convergence and complexity analysis for both ITD and AID
based bilevel optimizers, which we call as ITD-BiO and AID-BiO. Compared to existing analysis
in Ghadimi & Wang (2018) for AID-BiO that requires a continuously increasing number of inner-
loop steps to achieve the guarantee, our analysis allows a constant number of inner-loop steps as
often used in practice. In addition, we introduce a warm start initialization for the inner-loop updates
and the outer-loop hypergradient estimation, which allows us to backpropagate the tracking errors
to previous loops, and results in an improved computational complexity. As shown in Table 1,
the gradient complexities Gc(f, ), Gc(g, ), and Jacobian- and Hessian-vector product complexities
JV(g, ) and HV(g, ) of AID-BiO to attain an -accurate stationary point improve those of Ghadimi
& Wang (2018) by the order of κ, κe-1∕4, κ, and κ, respectively, where K is the condition number.
In addition, our analysis shows that AID-BiO requires less computations of Jacobian- and Hessian-
vector products than ITD-BiO by an order of K and κ1/2, which provides a justification for the
observation in Grazzi et al. (2020) that ITD often has a larger memory cost than AID.
We then propose a stochastic bilevel optimizer (stocBiO) to solve the stochastic bilevel optimiza-
tion problem in eq. (2). Our algorithm features a mini-batch hyper-gradient estimation via implicit
differentiation, where the core design involves a sample-efficient Hypergradient estimator via the
Neumann series. As shown in Table 2, the gradient complexities of our proposed algorithm w.r.t. F
2
Under review as a conference paper at ICLR 2021
Table 1: Comparison of bilevel deterministic optimization algorithms.
Algorithm	GCf,e)	Gc(g, e)	JV(g,e)	HV(g, e)
AID-BiO (Ghadimi & Wang, 2018)	O(κ4e-1)	O(κ5e-5∕4)	O (κ4e-1)	O(K4.5e-1)
AID-BiO (this paper)	O(κ3e-1)	O(κ4e-1)	O (κ3e-1)	O Ie-1)
ITD-BiO (this paper)	O(κ3e-1)	Oe(κ4e-1)	O(κ4e-1)	O(κ4e-1)
Gc(f, ) and Gc(g, ): number of gradient evaluations w.r.t. f and g. κ : the condition number.
JV(g, e): number of JaCobian-VeCtorProduCts VxVyg(x, y)v. Notation Oe: omit log ɪ terms.
HV(g, e): number of Hessian-vector products V2g(x,y)v.
Table 2: ComParison of bilevel stoChastiC oPtimization algorithms.
Algorithm	Gc(F,e)	Gc(G, e)	JV(G, e)	HV(G, e)
TTSA (Hong et al., 2020)	O(PoIy(κ)e- 5 )*	-TT	7~；	5；- O(PoIy(κ)e- 2 )	5 O(PoIy(κ)e-2)	5 O(poly(κ)e- 2)
BSA (Ghadimi & Wang, 2018)	O(κ6e-2)	-O(κ9e-3)-	O (κ6e-2)	O(κ6e-2)
stocBiO (this paper)	O(κ5e-2)	O(κ9e-2)	O (κ5e-2)	O(K6e-2)
We use poly(κ) beCause Hong et al. (2020) does not provide the expliCit dependenCe on κ.
and G improve upon those of BSA (Ghadimi & Wang, 2018) by an order of κ and -1, respeCtively.
In addition, the JaCobian-veCtor produCt Complexity JV(G, ) of our algorithm improves that of BSA
by an order of κ. In terms of the target aCCuraCy , our Computational Complexities improve those of
TTSA (Hong et al., 2020) by an order of -1/2.
We further provide the theoretiCal Complexity guarantee of ITD-BiO, AID-BiO and stoCBiO in meta-
learning and hyperparameter optimization. The experiments validate our theoretiCal results for de-
terminisitiC bilevel optimization, and demonstrate the superior effiCienCy of stoCBiO for stoChastiC
bilevel optimization. Due to the spaCe limitations, we present all theoretiCal and empiriCal results on
hyperparameter optimization in the supplementary materials.
1.2	Related Work
Bilevel optimization approaches: Bilevel optimization was first introduCed by BraCken & MCGill
(1973). SinCe then, a number of bilevel optimization algorithms have been proposed, whiCh inClude
but not limited to Constraint-based methods (Shi et al., 2005; Moore, 2010) and gradient-based
methods (Domke, 2012; Pedregosa, 2016; Gould et al., 2016; MaClaurin et al., 2015; FranCesChi
et al., 2018; Ghadimi & Wang, 2018; Liao et al., 2018; Shaban et al., 2019; Hong et al., 2020; Liu
et al., 2020; Li et al., 2020; Grazzi et al., 2020; Lorraine et al., 2020). Among them, Ghadimi &
Wang (2018); Hong et al. (2020) provided the finite-time Complexity analysis for their proposed
methods for the nonConvex-strongly-Convex bilevel optimization problem. For suCh a problem, this
paper develops a general and enhanCed finite-time analysis for gradient-based bilevel optimizers for
the deterministiC setting, and proposes a novel algorithm for the stoChastiC setting with order-level
lower Computational Complexity than the existing results.
Some works have studied other types of loss geometries. For example, Liu et al. (2020); Li et al.
(2020) assumed that the lower- and upper-level functions g(χ, ∙) and f (x, ∙) are convex and strongly-
Convex, and provided an asymptotiC analysis for their methods. Ghadimi & Wang (2018); Hong et al.
(2020) studied the setting where Φ(∙) is StrOngly-COnvex or convex, and g(x, ∙) is StrOngly-Convex.
Bilevel optimization in meta-learning: Bilevel optimization framework has been successfully em-
ployed in meta-learning recently (Snell et al., 2017; Franceschi et al., 2018; Rajeswaran et al., 2019;
Zugner & Gunnemann, 2019; Ji et al., 2020a;b). For example, Snell et al. (2017) proposed a bilevel
optimization procedure for meta-learning to learn a common embedding model for all tasks. Ra-
jeswaran et al. (2019) reformulated the model-agnostic meta-learning (MAML) (Finn et al., 2017)
as a bilevel optimization problem, and proposed iMAML via implicit gradient. The paper provides
a theoretical guarantee for two popular types of bilevel optimization algorithms, i.e., AID-BiO and
ITD-BiO, for meta-learning.
Bilevel optimization in hyperparameter optimization: Hyperparameter optimization has become
increasingly important as a powerful tool in the automatic machine learning (autoML) (Okuno et al.,
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Deterministic bilevel optimization via AID or ITD
1:	Input: Stepsizes α, β > 0, initializations x0 , y0 , v0.
2:	for k = 0, 1, 2, ..., K do
3:	Set yk0 = ykT-1 if k > 0 and y0 otherwise
4:	for t = 1, ...., T do
5:	Update yk = yk-1 - αVyg(xk,y『)
6:	end for
7:	Hypergradient estimation via
• AID: 1) set vk0 = vkN-1 if k > 0 and v0 otherwise
2)	solve vkN from V2y g(xk, ykT)v = Vyf(xk, ykT) via N steps of CG starting from vk0
3)	compute Jacobian-vector product VxVyg(xk , ykT)vkN via automatic differentiation
4)	compute Vb Φ(xk) = Vxf(xk , ykT) - VxVyg(xk , ykT)vkN
• ITD: compute VΦ(xk) = df(xk,yk ) Via backpropagation w.r.t. Xk
xk
8:	Update Xk+ι = Xk - β gf¾yk)
9:	end for
2018; Yu & Zhu, 2020). Recently, various bilevel optimization algorithms have been proposed
in the context of hyperparameter optimization, which include implicit differentiation based meth-
ods (Pedregosa, 2016), dynamical system based methods via reverse or forward gradient computa-
tion (Franceschi et al., 2017; 2018; Shaban et al., 2019), etc. This paper demonstrates the superior
efficiency of the proposed stocBiO algorithm in hyperparameter optimization.
2 Algorithms
In this section, we describe two popular types of deterministic bilevel optimization algorithms, and
propose a new algorithms for stochastic bilevel optimization.
2.1	Algorithms for Deterministic Bilevel Optimization
As shown in Algorithm 1, we describe two popular types of deterministic bilevel optimizers respec-
tively based on AID and ITD (referred to as AID-BiO and ITD-BiO) for solving the problem eq. (1).
Both AID-BiO and ITD-BiO update in a nested-loop manner. In the inner loop, both of them run
T steps of gradient decent (GD) to find an approximation point yf close to y*(χfc). Note that We
choose the initialization yk0 of each inner loop as the output ykT-1 of the preceding inner loop rather
than a random start. Such a warm start allows Us to backpropagate the tracking error ∣∣yT - y* (Xk) k
to previous loops, and yields an improved computational complexity.
At the outer loop, AID-BiO first solves VN from a linear system ▽：g(xk ,yf )v = Ny f (Xk ,yf )1
using N steps of conjugate-gradient (CG) starting from vk0 (where we also adopt a warm start scheme
here by setting vk0 = vkN-1), and then constructs
Nb Φ(Xk) = Nxf(Xk,ykT) - NxNyg(Xk, ykT)vkN	(3)
as an estimate of the true hypergradient NΦ(Xk), whose form is given by the following proposition.
Proposition 1. Recalling the definition Φ(x) := f (x, y*(x)), it holds that
NΦ(χk) =Nxf(Xk,y*(χk)) - VχVyg(χk,y*(χk))说,	(4)
where Vk is the solution ofthe linear system Vyg(xk,y* (Xk))v = Vy f(xk, y*(xk))∙
As shown in Domke (2012); Grazzi et al. (2020), the construction of eq. (3) involves only Hessian-
vector products in solving VN via CG and Jacobian-vector product VxVyg(Xk, ykT)VkN, which can
be efficiently computed and stored via existing automatic differentiation packages.
As a comparison, the outer loop of ITD-BiO computes the gradient d"xk∂⅞(Xk)) as an approxima-
tion of the hyper-gradient VΦ(χk) = d"x*yJxk)) via backpropagation, where we write yT(Xk)
1This is equivalent to solve a quadratic programing minv 1 vτ Vyg(χk,yT)v — vτVyf (xk,yT).
4
Under review as a conference paper at ICLR 2021
Algorithm 2 Stochastic bilevel optimizer (stocBiO)
1:	Input: Inner- and outer-loop stepsizes α, β > 0, initializations x0 and y0 .
2:	for k = 0, 1, 2, ..., K do
3:	Set yk0 = ykT-1 if k > 0 and y0 otherwise
4:	for t = 1, .., T do
5:	Draw a sample batch St-1
6:	Update yk = y『-αVyG(xk,yk-1; St-ι)
7:	end for
8:	Draw sample batch DF , and compute v0 = VyF (xk, ykT; DF)
9:	Draw sample batch DH, and construct vQ via Algorithm 3
10:	Draw sample batch DG, and compute Jacobian-vector product VxVyG(xk, ykT; DG)vQ
11:	Compute gradient estimate VΦ(xk) via eq. (6)
12:	Update xk+1 = xk - βVΦ(xk)
13:	end for
Algorithm 3 Construct vQ given v0
1:	Input: An integer Q, data samples DH = {Bj}jQ=1 and a constant η > 0.
2:	for j = 1, 2, ..., Q do
3:	Sample Bj and compute gradient Gj (y) = y - η Vy G(x, y; Bj )
4:	end for
5:	Set rQ = v0
6:	for i = Q, ..., 1 do
7:	ri-1 = ∂ Gi (y)ri /∂y = ri - ηV2y G(x, y; Bi)ri via automatic differentiation
8:	end for
9:	Return vQ = η PiQ=0 ri
because the output ykT of the inner loop has a dependence on xk through the inner-loop iterative GD
updates. The explicit form of the estimate df(xk¾(Xk)) is given by the following proposition via
the chain rule. For notation simplification, let QT=-Z1(∙) = I.
Proposition 2. The gradient
df(xk ,yT (Xk))
∂Xk
takes the following analytical form:
∂f(xk, ykT )
∂xk
T-1	T-1
▽xf (Xk,yT) - αE VxVyg(Xk,yk) ɪɪ (I - aVy;g(xkHk))Vyf (Xk,yT).
t=0	=t+1
Proposition 2 shows that the differentiation involves the computations of second-order derivatives
such as Hessian Vyg(∙, ∙). Since efficient Hessian-free methods such as CG have been successfully
deployed in the existing automatic differentiation tools, computing these second-order derivatives
reduces to more efficient computations of Jacobian- and Hessian-vector products.
2.2 Algorithm for Stochastic Bilevel Optimization
We propose anew stochastic bilevel optimizer (stocBiO) in Algorithm 2 to solve the problem eq. (2).
It has a double-loop structure similar to Algorithm 1, but runs T steps of stochastic gradient decent
(SGD) at the inner loop to obtain an approximated solution ykT . Based on the output ykT of the
inner loop, stocBiO first computes a gradient VyF (Xk, ykT ; DF) over a sample batch DF, and then
computes a vector vQ via Algorithm 3, which takes a form of
Q-1 Q
vQ = η	(I -ηV2yG(Xk,ykT ;Bj))VyF(Xk,ykT ;DF),	(5)
q=-1 j=Q-q
where {Bj, j = 1, ..., Q} are mutually-independent sample sets, Q and η are constants, and we let
QQ+i(∙) = I for notational simplification. Note that our construction of VQ, i.e., Algorithm 3, is
motived by the Neumann series Pi∞=0 Uk = (I - U)-1, and involves only Hessian-vector products
rather than Hessians, and hence is computationally and memory efficient. Then, we construct
VbΦ(Xk) =VXF(Xk,ykT ;DF) - VXVyG(Xk, ykT ;DG)vQ	(6)
5
Under review as a conference paper at ICLR 2021
as an estimate of hypergradient VΦ(χk) given by Proposition 1. An important component of our
algorithm is vq, which serves as an estimate of Vk in eq. (4) . Compared to the deterministic case,
designing a sample-efficient Hypergradient estimator in the stochastic case is more challenging. For
example, instead of choosing the same batch sizes for all Bj, j = 1, ..., Q in eq. (5), our analysis
captures the different impact of components Vy2G(xk, ykT ; Bj), j = 1, ..., Q on the Hypergradient
estimation variance, and inspires an adaptive and more efficient choice by setting |BQ-j | to decay
exponentially with j from 0 to Q - 1. By doing so, we achieve an improved complexity.
3	Definitions and Assumptions
Let z = (x, y) denote all parameters. For simplicity, suppose sample sets St for all t = 0, ..., T - 1,
DG and DF have the sizes of S, Dg and Df, respectively. In this paper, we focus on the following
types of loss functions for both the deterministic and stochastic cases.
Assumption 1. The lower-level function g(x, y) is μ-StrOngIy-COnveX w.r.t. y and the total objective
function Φ(x) = f (x, y*(x)) is nonconvex w.r.t. X. For the stochastic setting, the same assumptions
hold for G(x, y; ζ) and Φ(x), respectively.
Since the objective function Φ(x) is nonconvex, algorithms are expected to find an -accurate sta-
tionary point defined as follows.
Definition 1. We say X is an e-accurate StatiOnary pointfor the objective function Φ(x) in eq. (2) if
E∣∣VΦ(x)k2 ≤ e, where X is the output ofan algorithm.
In order to compare the performance of different bilevel algorithms, we adopt the following metrics
of computational complexity.
Definition 2. For a function f(X, y) and a vector v, let Gc(f, e) be the number of the partial
gradient Vxf or Vyf, and let JV(g, e) and HV(g, e) be the number of Jacobian-vector products
VxVyg(X, y)v. and Hessian-vector products V2y g(X, y)v. For the stochastic case, similar metrics
are adopted but w.r.t. the stochastic function F (X, y; ξ).
We take the following standard assumptions on the loss functions in eq. (2), which have been widely
adopted in bilevel optimization (Ghadimi & Wang, 2018; Ji et al., 2020a).
Assumption 2. The loss function f(z) and g(z) satisfy
•	f(z) is M -Lipschitz, i.e., for any z, z0, |f(z) - f(z0)| ≤ Mkz - z0k.
•	Gradients Vf(z) and Vf(z) are L-Lipschitz, i.e., for any z, z0,
kVf(z)-Vf(z0)k ≤Lkz-z0k, kVg(z) - Vg(z0)k ≤Lkz-z0k.
For the stochastic case, the same assumptions hold for F(z; ξ) and G(z; ζ)for any given ξ and ζ.
As shown in Proposition 1, the gradient of the objective function Φ(X) involves the second-order
derivatives VxVyg(z) and Vy2g(z). The following assumption imposes the Lipschitz conditions on
such high-order derivatives, as also made in Ghadimi & Wang (2018).
Assumption 3. Suppose the derivatives VxVyg(z) and Vy2g(z) are τ- and ρ- Lipschitz, i.e.,
•	For any z, z0, kVxVyg(z) - VxVyg(z0)k ≤ τkz - z0k.
•	For any z, z0, kV2yg(z) - V2yg(z0)k ≤ ρkz - z0k.
For the stochastic case, the same assumptions hold for VxVyG(z; ζ) and V2y G(z; ζ)for any ζ.
As typically adopted in the analysis for stochastic optimization, we make the following bounded-
variance assumption for the lower-level stochastic function G(z; ζ).
Assumption 4. VG(z; ζ) has a bounded variance, i.e., Eξ kVG(z; ζ) - Vg(z)k2 ≤ σ2 for some σ.
4	Main Results for B ilevel Optimization
4.1	Deterministic Bilevel Optimization
We first characterize the convergence and complexity performance of the AID-BiO algorithm. Let
K = μ denote the condition number.
6
Under review as a conference paper at ICLR 2021
Theorem 1 (AID-BiO). Suppose Assumptions 1, 2, 3 hold. Define a smoothness parameter LΦ
l+2l⅛m2+
PLM+L3+τML
μ2
+ ρLμM = Θ(κ3), choose the stepsizes α ≤ L, β = 8L^, and set
the inner-loop iteration number T ≥ Θ(κ) and the CG iteration number N ≥ Θ(√κ), where the
detailed forms of T, N can be found in Appendix E. Then, the outputs of AID-BiO satisfy
1 K-1
KK E kVΦ(xk)k2 ≤
k=0
64LΦ(Φ(x0)-infxΦ(x))+5∆0
(7)
K
where ∆o = ∣∣y0 一 y*(x0)∣∣2 + ∣∣v0 一 v0∣∣2 > 0∙
In order to achieve an -accurate stationary point, we have
•	Gradient complexity: Gc(f, ) = O(κ3-1), Gc(g, ) = O(κ4-1).
•	Jacobian- and Hessian-vector product: JV(g, ) = O κ3-1 , HV(g, ) = O κ3.5-1
It can be seen from Table 1 that the complexities Gc(f, e), Gc(g, e), JV(g, e) and HV(g, e) of our
analysis improves that of Ghadimi & Wang (2018) (eq. (2.30) therein) by the order of κ, κe-1/4,
κ and κ. Such an improvement is achieved by a refined analysis with a constant number of inner-
loop steps, and by a warm start strategy to backpropagate the tracking errors ∣∣yT 一 y*(Xk)∣ and
∣∣vN 一 v，k to previous loops, as also demonstrated by our meta-learning experiments.
We next characterize the convergence and complexity performance of the ITD-BiO algorithm.
Theorem 2 (ITD-BiO). Suppose Assumptions 1, 2, and 3 hold. Define the parameter LΦ as in
Theorem 1, and choose α ≤ L, β = 4LL and T ≥ Θ(κ log ɪ), where the detailedform of T can be
found in Appendix F. Then, the outputs of ITD-BiO satisfy
1 K-1
IK E ∣VΦ(χk)k2 ≤
2
十不.
16LΦ(Φ(x0) - infxΦ(x))
K
k=0
In order to achieve an -accurate stationary point, we have
•	Gradient complexity: Gc(f, e) = O(κ3e-1), Gc(g, e) = O(κ4e-1 log(ɪ)).
•	Jacobian- and Hessian-vector product complexity:
JV(g, e) = O(κ4e-1 log e-1), HV(g, e) = O(κ4e-1 loge-1).
By comparing Theorem 1 and Theorem 2, it can be seen that the complexities Gc(g, e), JV(g, e),
and HV(g, e) of AID-BiO are better than those of ITD-BiO by the order of log(ɪ), K log(ɪ) and
κ0.5 log(ɪ). This is in consistence with the comparison in Grazzi et al. (2020) that AID-BiO often
has a lower memory cost than ITD-BiO.
4.2	Stochastic Bilevel Optimization
We first characterize the bias and variance of an important component vQ in eq. (5).
Proposition 3. Suppose Assumptions 1, 2 and 3 hold. Let the constant η ≤ L and choose the batch
sizes ∣BQ+ι-j | = BQ(1 -ημ)j-1 for j = 1,…，Q, where B ≥ Q(i—j)Q-i. Then, the bias satisfies
∣∣EvQ - [vyg(xk,yT)]-1Vyf(Xk,yT)∣∣ ≤ μ-1(1 -ημ)Q+1M.	⑻
Furthermore, the estimation variance is given by
E 「「2 '	t、Lie “	t、∣∣2 4η2L2M2 1 4(1 - ημ)2Q+2M2 2M2 小、
ElIvQ - [Vyg(xk,yk)] vyf(Xk,yk )k ≤ μ B +	μ	+ μ2Df. (9)
Proposition 3 shows that if We choose Q and B at the order level of O (log ɪ) and O(1∕e), the
bias and variance are smaller than O(e), and the required number of samples is PjQ=1 BQ(1 -
ημ)j-1 = O (e-1 log ɪ). Note that the chosen batch size ∣Bq+i_7∙ | exponentially decays w.r.t. j.
In comparison, the uniform choice of all |Bj | would yield a worse complexity of O (e-1(log ɪ )2).
We next analyze stocBiO when the objective function Φ(x) := f (x, y*(x)) is nonconvex.
7
Under review as a conference paper at ICLR 2021
Theorem 3. Suppose Assumptions 1, 2, 3 and 4 hold. Define parameter Lφ = L + 2L2+τMM +
μ
PLM+μ2+τML + PLM = O (κ3), choose StePSize β = 4^, and set η < L in Algorithm 3. Set
f ιog (12+ 48βL(L+ 号+M + LM)2) log (√β(L+ 号+M+LM))[
T ≥ maX I ---------H 2log( L + μ )-y----, ------log( L + μ )~~y— .T Then, We have
ɪ X-⅛kVΦ(xk)∣∣2 ≤ 32Lφ(φ(x0)-mfX 以?+ 5ky0-y*(x0)k2) + 72k2m2(1 - 〃〃产
KK
k=0
40(L + Lr + MT + LM)2 σ2	16κ2M2	(8 + 32κ2)M2	64κ2M2
+	L	^S+	Dg	+	Df	+ —B —.	(10)
In order to achieve an -accurate stationary point, We have
•	Gradient complexity: Gc(F, ) = O(κ5-2), Gc(G, ) = O(κ9-2).
•	Jacobian- and Hessian-vector product: JV(G, ) = O(κ5-2), HV(G, ) = Oe(κ6-2).
Theorem 3 shows that stocBiO converges sublinearly with the convergence error decaying expo-
nentially w.r.t. Q and sublinearly w.r.t. the batch sizes S, Dg , Df for gradient estimation and B for
Hessian inverse estimation. In addition, it can be seen that the total number T of the inner-loop steps
is chosen at nearly a constant level, rather than a typical choice of Θ(log(ɪ)).
As shown in Table 2, the gradient complexities of our proposed algorithm in terms of F and G
improve those of BSA in Ghadimi & Wang (2018) by an order of κ and -1, respectively. In
addition, the Jacobian-vector product complexity JV(G, ) of our algorithm improves that of BSA
by the order of κ. In terms of the accuracy , our gradient, Jacobian- and Hessian-vector product
complexities improve those of TTSA in Hong et al. (2020) all by an order of -0.5.
5 Applications to Meta-Learning
5.1 Meta-Learning with Common Embedding Model
Consider the few-shot meta-learning problem with m tasks {Ti, i = 1, ..., m} sampled from distri-
bution PT . Each task Ti has a loss function L(φ, wi ; ξ) over each data sample ξ, where φ are the
parameters of an embedding model shared by all tasks, and wi are the task-specific parameters. The
goal of this framework is to find good parameters φ for all tasks, and building on the embedded
features, each task then adapts its own parameters wi by minimizing its loss.
The model training takes a bilevel procedure. In the lower-level stage, building on the embedded
features, the base learner of task Ti searches w* as the minimizer of its loss function over a training
set Si. In the upper-level stage, the meta-learner evaluates the minimizers wi,i = 1,…，m on
held-out test sets, and optimizes φ of the embedding model over all tasks. Specifically, let we =
(w1, ..., wm) denote all task-specific parameters. Then, the objective function is given by
1m 1
mφn LD 应 W) = m X 西
}
∙•^^^^^^^^^^{^^^^^^^^^^≡""
LDi (φ,w*): task-specific upper-level loss
1m 1
s.t. W = arg min LS (φ,w)= arg min 一T( ∏τr T L(φ, wi; ξ) + R(Wi)),
we	(w1,...,wm) m i=1	|Si| ξ∈S
(11)
|
}
''^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^
LSi (φ,wi ): task-specific lower-level loss
where Si and Di are the training and test datasets of task Ti, and R(Wi) is a strongly-convex regu-
larizer, e.g., L2 . Note that the lower-level problem is equivalent to solving each Wi* as a minimizer
of the task-specific loss LSi (φ, Wi) for i = 1, ..., m. In practice, Wi often corresponds to the param-
eters of the last linear layer of a neural network and φ are the parameters of the remaining layers
(e.g., 4 convolutional layers in Bertinetto et al. (2018); Ji et al. (2020a)), and hence the lower-level
function is strongly-convex w.r.t. We and the upper-level function LD(φ, We* (φ)) is generally non-
convex w.r.t. φ. In addition, due to the small sizes of datasets Di and Si in few-shot learning, all
8
Under review as a conference paper at ICLR 2021
updates for each task Ti use full gradient descent without data resampling. As a result, AID-BiO
and ITD-BiO in Algorithm 1 can be applied here. In some applications where the number m of
tasks is large, it is more efficient to sample a batch B of i.i.d. tasks from {Ti, i = 1, ..., m} at each
meta (outer) iteration, and optimizes the mini-batch versions LD(φ,泊；B)=由 £记万 LDi (Φ, Wi)
and LS (φ, W; B) = 土 Pii∈β LSi (Φ, Wi) instead. The following theorem provides the convergence
analysis of ITD-BiO for this case.
Theorem 4. Suppose Assumptions 1, 2 and 3 hold and suppose each task loss LSi(Φ,Wi) is μ-
strongly-convex w.r.t. Wi. Choose the same parameters β, T as in Theorem 2. Then, we have
K X EkVΦ(Φk)k2 ≤ -)Jnfφ φ(φ)) +今 +(1 + L)2 M .
k=0	μ
Theorem 4 shows that compared to the full batch (i.e., without task sampling) case in eq. (11), the
task sampling introduces a variance term O(!)due to the stochastic nature of the algorithm. Using
an approach similar to Theorem 4, we can derive a similar result for AID-BiO.
5.2 Experiments
To validate our theoretical results for deterministic bilevel optimization, we compare the perfor-
mance among the following four algorithms: ITD-BiO, AID-BiO-constant (AID-BiO with a con-
stant number of inner-loop steps as in our analysis), AID-BiO-increasing (AID-BiO with an in-
creasing number of inner-loop steps under analysis in Ghadimi & Wang (2018)), and two popular
meta-learning algorithms MAML2 (Finn et al., 2017) and ANIL3 (Raghu et al., 2019). We conduct
experiments over a 5-way 5-shot task on two benchmark datasets: FC100 and miniImageNet, and
the results are averaged over 10 trials with different random seeds. Due to the space limitations, we
provide the model architectures, hyperparameter settings and additional experiments in Appendix B.
0	500	l∞0	15∞	20∞	25∞
running time Is
running time /s
running time /s
(a) dataset: miniImageNet
(b) dataset: FC100
Figure 1: Convergence of various algorithms on meta-learning. For each dataset, left plot: training
accuracy v.s. running time; right plot: test accuracy v.s. running time.
It can be seen from Figure 1 that for both the miniImageNet and FC100 datasets, AID-BiO-constant
converges faster than AID-BiO-increasing in terms of both the training accuracy and test accuracy,
and achieves a better final test accuracy than ANIL and MAML. This demonstrates the superior
improvement of our developed analysis over existing analysis in Ghadimi & Wang (2018) for AID-
BiO algorithm. Moreover, it can be observed that AID-BiO is slightly faster than ITD-BiO in terms
of the training accuracy and test accuracy. This is also in consistence with our theoretical results.
6 Conclusion
In this paper, we develop a general and enhanced finite-time analysis for the nonconvex-strongly-
convex bilevel deterministic optimization, and propose a novel algorithm for the stochastic setting
whose computational complexity outperforms the best known results order-wisely. We also provide
the theoretical guarantee of various bilevel optimizers in meta-learning and hyperparameter opti-
mization. The experiments validate our theoretical results and demonstrate the effectiveness of the
proposed algorithm. We anticipate that the finite-time analysis that we develop will be useful for
analyzing other bilevel optimization problems with different loss geometries, and the proposed al-
gorithms will be useful for other applications such as reinforcement learning and Stackelberg game.
2MAML consists of an inner loop for task adaptation and an outer loop for meta initialization training.
3 ANIL refers to almost no inner loop, which is an efficient MAML variant with task-specific adaption on
the last-layer of parameters.
9
Under review as a conference paper at ICLR 2021
References
Sebastien M.R. Arnold, Praateek Mahajan, Debajyoti Datta, and Ian Bunner. learn2learn, 2019.
https://github.com/learnables/learn2learn.
Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differ-
entiable closed-form solvers. In International Conference on Learning Representations (ICLR),
2018.
Jerome Bracken and James T McGill. Mathematical programs with optimization problems in the
constraints. Operations Research,21(1):37-44,1973.
Justin Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and
Statistics (AISTATS), pp. 318-326, 2012.
Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated Machine Learning,
pp. 3-33. Springer, Cham, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proc. International Conference on Machine Learning (ICML), pp. 1126-1135,
2017.
Remi Flamary, Alain Rakotomamonjy, and Gilles Gasso. Learning constrained task similarities in
graphregularized multi-task learning. Regularization, Optimization, Kernels, and Support Vector
Machines, pp. 103, 2014.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. In International Conference on Machine Learning
(ICML), pp. 1165-1173, 2017.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. In International Conference
on Machine Learning (ICML), pp. 1568-1577, 2018.
Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246, 2018.
Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison
Guo. On differentiating parameterized argmin and argmax problems with application to bi-level
optimization. arXiv preprint arXiv:1607.05447, 2016.
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration com-
plexity of hypergradient computation. In Proc. International Conference on Machine Learning
(ICML), 2020.
Pierre Hansen, Brigitte Jaumard, and Gilles Savard. New branch-and-bound rules for linear bilevel
programming. SIAM Journal on Scientific and Statistical Computing, 13(5):1194-1217, 1992.
Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale framework
for bilevel optimization: Complexity analysis and application to actor-critic. arXiv preprint
arXiv:2007.05170, 2020.
Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-specific adaptation over partial parameters. arXiv preprint arXiv:2006.09486, 2020a.
Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms. arXiv preprint arXiv:2002.07836, 2020b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations (ICLR), 2014.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems (NeurIPS), pp. 1008-1014, 2000.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
10
Under review as a conference paper at ICLR 2021
Gautam Kunapuli, Kristin P Bennett, Jing Hu, and Jong-Shi Pang. Classification model selection
via bilevel programming. Optimization Methods & Software, 23(4):475-489, 2008.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Junyi Li, Bin Gu, and Heng Huang. Improved bilevel model: Fast and optimal algorithm with
theoretical guarantee. arXiv preprint arXiv:2009.00690, 2020.
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun,
and Richard Zemel. Reviving and improving recurrent back-propagation. In Proc. International
Conference on Machine Learning (ICML), 2018.
Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order al-
gorithmic framework for bi-level programming beyond lower-level singleton. In International
Conference on Machine Learning (ICML), 2020.
Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by
implicit differentiation. In International Conference on Artificial Intelligence and Statistics (AIS-
TATS), pp. 1540-1552. PMLR, 2020.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning (ICML), pp.
2113-2122, 2015.
Gregory M Moore. Bilevel programming algorithms for machine learning model selection. Rensse-
laer Polytechnic Institute, 2010.
Takayuki Okuno, Akiko Takeda, and Akihiro Kawana. Hyperparameter learning via bilevel nons-
mooth optimization. arXiv preprint arXiv:1806.01520, 2018.
Boris Oreshkin, Pau Rodrlguez Lopez, and Alexandre Lacoste. Tadam: Task dependent adaptive
metric for improved few-shot learning. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 721-731, 2018.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International Con-
ference on Machine Learning (ICML), pp. 737-746, 2016.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
towards understanding the effectiveness of MAML. International Conference on Learning Rep-
resentations (ICLR), 2019.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with im-
plicit gradients. In Advances in Neural Information Processing Systems (NeurIPS), pp. 113-124,
2019.
Aaron Roth, Jonathan Ullman, and Zhiwei Steven Wu. Watch and learn: Optimizing from revealed
preferences feedback. In Annual ACM Symposium on Theory of Computing (STOC), pp. 949-962,
2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision,
3(115):211-252, 2015.
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation
for bilevel optimization. In International Conference on Artificial Intelligence and Statistics (AIS-
TATS), pp. 1723-1732, 2019.
Chenggen Shi, Jie Lu, and Guangquan Zhang. An extended kuhn-tucker approach for linear bilevel
programming. Applied Mathematics and Computation, 162(1):51-63, 2005.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems (NIPS), 2017.
11
Under review as a conference paper at ICLR 2021
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, and Daan Wierstra. Matching networks for one
shot learning. In Advances in Neural Information Processing Systems (NIPS), 2016.
Tong Yu and Hong Zhu. Hyper-parameter optimization: A review of algorithms and applications.
arXiv preprint arXiv:2003.05689, 2020.
Daniel Zugner and StePhan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. In International Conference on Learning Representations (ICLR), 2019.
12
Under review as a conference paper at ICLR 2021
Supplementary Materials
A Application to hyperparameter Optimization
A. 1 hyperparameter Optimization
The goal of hyperparameter optimization (Franceschi et al., 2018; Feurer & Hutter, 2019) is to
search for representation or regularization parameters λ to minimize the validation error evaluated
over the learner,s parameters w*, where w* is the minimizer of the inner-loop regularized training
error. Mathematically, the objective function is given by
mλin LDval (λ)
1
∣Dvaιι
L(w*(λ); ξ)
ξ∈Dval
s.t. w*(λ) = arg min LDtr (w,λ) := d1— X (L(W,λ; ξ) + R(w,λ)),
w	|Dtr| ξ∈Dtr
(12)
where Dval and Dtr are validation and training data, L is the loss, and R(w, λ) is a regularizer.
In practice, the lower-level function LDtr (w, λ) is often strongly-convex w.r.t. w. For example, for
the data hyper-cleaning application proposed by Franceschi et al. (2018); Shaban et al. (2019), the
predictor is modeled by a linear classifier, and the loss function L(w; ξ) is convex w.r.t. w and
R(w, λ) is a strongly-convex regularizer, e.g., L2 regularization. In addition, the sample sizes of
Dval and Dtr are often large, and stochastic algorithms are preferred for achieving better efficiency.
As a result, the above hyperparameter optimization falls into the stochastic bilevel optimization we
study in eq. (2), and we can apply the proposed stocBiO algorithm here and Theorem 3 establishes
its finite-time performance guarantee.
A.2 Experiments
We compare our proposed stocBiO with the following baseline bilevel optimization algorithms.
•	BSA (Ghadimi & Wang, 2018): implicit gradient based stochastic bilevel optimizer via single-
sample data sampling.
•	TTSA (Hong et al., 2020): two-time-scale stochastic optimizer via single-sample data sampling.
•	HOAG (Pedregosa, 2016): a hyperparameter optimization algorithm with approximate gradient.
We use the implementation in the repository https://github.com/fabianp/hoag.
•	reverse (Franceschi et al., 2017): an iterative differentiation based method that approximates
the hypergradient via backpropagation. We use its implementation in https://github.com/
prolearner/hypertorch.
•	AID-FP (Grazzi et al., 2020): AID with the fixed-point method. We use its implementation in
https://github.com/prolearner/hypertorch
•	AID-CG (Grazzi et al., 2020): AID with the conjugate gradient method. We use its implementa-
tion in https://github.com/prolearner/hypertorch.
We demonstrate the effectiveness of the proposed stocBiO algorithm on two experiments: data
hyper-cleaning and logistic regression.
Logistic Regression on 20 Newsgroup: We compare the performance of our algorithm stocBiO
with the existing baseline algorithms reverse, AID-FP, AID-CG and HOAG over a logistic re-
gression problem on 20 Newsgroup dataset Grazzi et al. (2020). The objective function of such a
problem is given by
min E (λ,w*) = D-：	X	L(xiW*,yi)
val (xi,yi)∈Dval
1	1cp
s.t. w = arg min (PFrr V	L(Xiw,yi) + -∑2∑2exp(λj )wj,
w∈Rp×c 'lDtrl (Xj⅛"	Cp i=1 j=1	〃
13
Under review as a conference paper at ICLR 2021
where L is the cross-entropy loss, c = 20 is the number of topics, and p = 101631 is the feature
dimension. Following Grazzi et al. (2020), we use SGD as the optimizer for the outer-loop update for
all algorithms. For reverse, AID-FP, AID-CG, we use the suggested and well-tuned hyperparameter
setting in their implementations https://github.com/prolearner/hypertorch on this
application. In specific, they choose the inner- and outer-loop stepsizes as 100, the number of inner
loops as 10, the number ofCG steps as 10. For HOAG, we use the same parameters as reverse, AID-
FP, AID-CG. For stocBiO, we use the same parameters as reverse, AID-FP, AID-CG, and choose
η = 0.5, Q = 10. We use stocBiO-B as a shorthand of stocBiO with a batch size of B.
running time /s
Figure 2: Comparison of various algorithms on logistic regression on 20 Newsgroup dataset. For
left plot: test loss v.s. running time; right plot: test accuracy v.s. running time
3.5
3.0
125
S 2-0
H
1-5
1-0
0	10	20	30	40	50	60
running time /s
0-7
5v°∙6
s
Q 0.5
5
1o0∙4
α)
I- 0.3
0.2
0	10	20	30	40	50	60
running time /s
Figure 3: Convergence rate of stocBiO with different batch sizes.
As shown in Figure 2, the proposed stocBiO achieves the fastest convergence rate as well as the
best test accuracy among all comparison algorithms. This demonstrates the practical advantage of
our proposed algorithm stocBiO. Note that we do not include BSA and TTSA in the comparison,
because they converge too slowly with a large variance, and are much worse than the other competing
algorithms. In addition, we investigate the impact of the batch size on the performance of our
stocBiO in Figure 3. It can be seen that stocBiO outperforms HOAG under the batch sizes of
100, 500, 1000, 2000. This shows that the performance of stocBiO is not very sensitive to the batch
size, and hence the tuning of the batch size is easy to handle in practice.
Data Hyper-Cleaning on MNIST. We first compare the performance of our proposed algorithm
stocBiO with other baseline algorithms BSA, TTSA, HOAG4 on a hyperparameter optimization
problem: data hyper-cleaning (Shaban et al., 2019) on a dataset derived from MNIST (LeCun et al.,
1998), which consists of 20000 images for training, 5000 images for validation, and 10000 images
for testing. Data hyper-cleaning is to train a classifier in a corrupted setting where each label of
training data is replaced by a random class number with a probability p (i.e., the corruption rate).
4We do not include reverse, AID-CG and AID-FG because they perform similarly to HOAG.
14
Under review as a conference paper at ICLR 2021
The objective function is given by
min E(λ,w*) = -1-	X	L(W*xi,y)
val (xi,yi)∈Dval
s.t. w* = arg min L(w,λ) := -1	X	σ(λi)L(wXi ,y) + Crkwk2,
w	| tr| (xi,yi)∈Dtr
where L is the cross-entropy loss, σ(∙) is the sigmoid function, Cr is a regularization parameter.
Following Shaban et al. (2019), we choose Cr = 0.001. All results are averaged over 10 tri-
als with different random seeds. We adopt Adam (Kingma & Ba, 2014) as the optimizer for the
outer-loop update for all algorithms. For stochastic algorithms, we set the batch size as 50 for
stocBiO, and 1 for BSA and TTSA because they use the single-sample data sampling. For all algo-
rithms, we use a grid search to choose the inner-loop stepsize from {0.01, 0.1, 1, 10}, the outer-loop
stepsize from {10i, i = -4, -3, -2, -1, 0, 1, 2, 3, 4}, and the number T of inner-loop steps from
{1, 10, 50, 100, 200, 1000}, where values that achieve the lowest loss after a fixed running time are
selected. For stocBiO, BSA, and TTSA, we choose η from {0.5 × 2i, i = -3, -2, -1, 0, 1, 2, 3},
and Q from {3 × 2i, i = 0, , 2, 3}.
(b) Corruption rate p = 0.2
(a) Corruption rate p = 0.1
Figure 4: Convergence of various algorithms on hyperparameter optimization at different corruption
rates. For each corruption rate p, left plot: training loss v.s. running time; right plot: test loss v.s.
running time.
It can be seen from Figure 4 that our proposed stocBiO algorithm achieves the fastest convergence
rate among all competing algorithms in terms of both the training loss and the test loss. In addition,
it is observed that such an improvement is more significant when the corruption rate p is smaller.
We note that the stochastic algorithm TTSA converges very slowly with a large variance. This is
because TTSA updates the costly outer loop more frequently than other algorithms, and has a larger
variance due to the single-sample data sampling. As a comparison, our stocBiO achieves a much
lower variance for hypergradient estimation as well as a much faster convergence rate. This verifies
our theoretical results in Theorem 3.
B	Further Specifications on Meta-Learning Experiments
B.1	Datasets and Model Architectures
FC100 (Oreshkin et al., 2018) is a dataset derived from CIFAR-100 (Krizhevsky & Hinton, 2009),
and contains 100 classes with each class consisting of 600 images of size 32. Following Oreshkin
et al. (2018), these 100 classes are split into 60 classes for meta-training, 20 classes for meta-
validation, and 20 classes for meta-testing. For all comparison algorithms, we use a 4-layer convo-
lutional neural networks (CNN) with four convolutional blocks, in which each convolutional block
15
Under review as a conference paper at ICLR 2021
contains a 3 × 3 convolution (padding = 1, stride = 2), batch normalization, ReLU activation, and
2 × 2 max pooling. Each convolutional layer has 64 filters.
The miniImageNet dataset (Vinyals et al., 2016) is generated from ImageNet Russakovsky et al.
(2015), and consists of 100 classes with each class containing 600 images of size 84 × 84. Following
the repository Arnold et al. (2019), we partition these classes into 64 classes for meta-training, 16
classes for meta-validation, and 20 classes for meta-testing. Following the repository (Arnold et al.,
2019), we use a four-layer CNN with four convolutional blocks, where each block sequentially
consists of a 3 × 3 convolution, batch normalization, ReLU activation, and 2 × 2 max pooling. Each
convolutional layer has 32 filters.
B.2	Implementations and hyperparameter Settings
We adopt the existing implementations in the repository (Arnold et al., 2019) for ANIL and MAML.
For all algorithms, we adopt Adam (Kingma & Ba, 2014) as the optimizer for the outer-loop update.
Parameter selection for the experiments in Figure 1(a): For ANIL and MAML, we adopt the
suggested hyperparameter selection in the repository (Arnold et al., 2019). In specific, for ANIL,
we choose the inner-loop stepsize as 0.1, the outer-loop (meta) stepsize as 0.002, the task sampling
size as 32, and the number of inner-loop steps as 5L. For MAML, we choose the inner-loop stepsize
as 0.5, the outer-loop stepsize as 0.003, the task sampling sizeas 32, and the number of inner-loop
steps as 3. For ITD-BiO, AID-BiO-constant and AID-BiO-increasing, we use a grid search to choose
the inner-loop stepsize from {0.01, 0.1, 1, 10}, the task sampling size from {32, 128, 256}, and the
outer-loop stepsize from {10i, i = -3, -2, -1, 0, 1, 2, 3}, where values that achieve the lowest loss
after a fixed running time are selected. For ITD-BiO and AID-BiO-constant, we choose the number
of inner-loop steps from {5, 10, 15, 20, 50}, and for AID-BiO-increasing, we choose the number of
inner-loop steps as dc(k + 1)1/4e as adopted by the analysis in Ghadimi & Wang (2018), where we
choose c from {0.5, 2, 5, 10, 50}. For both AID-BiO-constant and AID-BiO-increasing, we choose
the number N of CG steps for solving the linear system from {5, 10, 15}.
Parameter selection for the experiments in Figure 1(b): For ANIL and MAML, we adopt the
suggested hyperparameter selection in the repository (Arnold et al., 2019). Specifically, for ANIL,
we choose the inner-loop stepsize as 0.1, the outer-loop (meta) stepsize as 0.001, the task sampling
size as 32 and the number of inner-loop steps as 10. For MAML, we choose the inner-loop stepsize
as 0.5, the outer-loop stepsize as 0.001, the task samling size as 32, and the number of inner-loop
steps as 3. For ITD-BiO, AID-BiO-constant and AID-BiO-increasing, we adopt the same procedure
as in the experiments in Figure 1(a).
B.3	Additional Results for Meta Learning
O IOOO 2000	3000	4000
running time /s
Figure 5: Comparison of ITD-BiO and ANIL on miniImageNet dataset with T = 10.
In this subsection, we compare the robustness between bilevel optimizer ITD-BiO (AID-BiO per-
forms similarly to ITD-BiO in terms of the convergence rate) and ANIL (ANIL outperforms MAML
in general) to the number of inner-loop steps. For the experiments in Figure 5, we choose the inner-
loop stepsize as 0.05, the outer-loop (meta) stepsize as 0.002, the mini-batch size as 32, and the
number T of inner-loop steps as 10 for both ANIL and ITD-BiO. For the experiments in Figure 6,
we choose the inner-loop stepsize as 0.1, the outer-loop (meta) stepsize as 0.001, the mini-batch size
as 32, and the number T of inner-loop steps as 20 for both ANIL and ITD-BiO.
16
Under review as a conference paper at ICLR 2021
T= 20
Figure 6: Comparison of ITD-BiO and ANIL on FC100 dataset with T = 20.
0	500	1000 1500 2000 2500 3000
running time /s
0.55
T= 20
0 5 0 5
5 4 4 3
Oooo
Auejnuuetrj①F-
0.30
0	500	1000	1500	2000	2500
running time /s
It can be seen from Figure 5 and Figure 6 that when the number of inner-loop steps become larger,
i.e., T = 10 for miniImageNet and T = 20 for FC100, the bilevel optimizer ITD-BiO converges
stably with a small variance, whereas ANIL suffers from a sudden descent at 1500s on miniImageNet
and even diverges after 2000s on FC100.
C S upporting Lemmas
In this section, we provide some auxiliary lemmas used for proving the main convergence results.
First note that the Lipschitz properties in Assumption 2 imply the following lemma.
Lemma 1. Suppose Assumption 2 holds. Then, the stochastic derivatives VF(z; ξ), VG(z; ξ),
VxVyG(z; ξ) and Vy2 G(z; ξ) have bounded variances, i.e., for any z and ξ,
•	Eξ kVF(z; ξ) -Vf(z)k2 ≤ M2.
•	Eξ kVxVyG(z; ξ) - VxVyg(z)k2 ≤L2.
•	Eξ∣∣VyG(z;ξ)-Vyg(z)『≤ L2.
Recall that Φ(x) = f (x, y*(χ)) in eq. (2). Then, we use the following lemma to characterize the
Lipschitz properties of VΦ(X), which is adapted from Lemma 2.2 in Ghadimi & Wang (2018).
Lemma 2. Suppose Assumptions 1, 2 and 3 hold. Then, we have, for any X, X0 ∈ Rp,
kVΦ(X)-VΦ(X0)k ≤ LΦkX - X0k,
where the constant LΦ is given by
LΦ = L +
2L2 + τM2 ρLM + L3 + τML ρL2M
--------1-------9------1--3-.
从3
μ2
(13)
μ
D Proof of Propositions in Section 2
In this section, we provide the proofs for Proposition 1 and Proposition 2 in Section 2.
D.1 Proof of Proposition 1
Using the chain rule over the gradient VΦ(χk)
∂f (Xk ,y*(χk))
VΦ(χk ) = Vxf(Xk ,y*(χk)) +
∂xk
∂y*(χk)
, we have
∂Xk
Vy f(χk ,y*(χk)).
(14)
Based on the optimality of y* (Xk), We have Vyg(xk, y*(xk)) = 0, which, using the implicit differ-
entiation w.r.t. Xk, yields
VxVy g(χk ,y*(χk)) +
∂y*(χk)
∂Xk
Vyg(Xk ,y*(χk )) = 0.
(15)
17
Under review as a conference paper at ICLR 2021
Let Vk be the solution of the linear system ▽： g(xk, y*(xk ))v = Ny f (Xk ,y*(xk)). Then, multiply-
ing 说 at the both sides of eq. (15), yields
一VxVyg(Xk,y*(Xk))Vk = dy∂Xχk) Vyg(Xk,y*(Xk))vk = dy∂Xχk) Vyf (Xk,y*(Xk)),
which, in conjunction with eq. (14) , yields the proof.
D.2 Proof of Proposition 2
Based on the iterative update of line 5 in Algorithm 1, we have ykT = yk0 一 α PtT=-01 Vyg(Xk, ykt ),
which, combined with the fact that Vyg(Xk, ykt ) is differentiable w.r.t. Xk, indicates that the inner
output ykT is differentiable w.r.t. Xk. Then, based on the chain rule, we have
d (Xk ,yT) = Vxf(Xk ,yT) + IyT Vyf(Xk ,yT).	(16)
∂Xk	∂Xk
Based on the iterative updates that ykt = ykt-1 一 αVyg(Xk, ykt-1) for t = 1, ..., T, we have
^yk = ~kr-----aVxVy g(Xk,yk-1) — α~yk- Vy g(Xk ,yk-1)
∂Xk	∂Xk	k	∂Xk	y k
∂yt-1
= ~fΓ-(I - aVyg(Xk,yk I)) - aVxVyg(Xk, yk 1).
∂Xk
Telescoping the above equality over t from 1 to T yields
∂yT ∂y0	T-1	T-1	T-1
∂k = *	∏(I-aVyg(Xk,yk))-αE VxVyg(Xk,yk)	∏	(I-aVyg(Xk,yjj))
XX
k	k t=0	t=0	=t+1
T-1	T-1
(=) - α X VxVyg(Xk,ykt) Y (I - αVy2g(Xk,ykj)).
t=0	=t+1
(17)
where (i) follows from the fact that 舞=0. Combining eq. (16) and eq. (17) finishes the proof.
E Convergence Proofs for AID-BiO in Section 4.1
For notation simplification, we define the following quantities.
Γ =3L2 + 红"+ 6L2(1 + √K)2(κ + ρM)2, δτ,N = Γ(1 - αμ)τ + 6L2κ(√K-+-∣)2N
Ω=8(βκ2 + 2βML + 2βLMκ)2, ∆0 = kyo-y*(X0)k2 + |觞一。0『.	(18)
We first provide some supporting lemmas. The following lemma characterizes the Hypergradient
estimation error kVΦ(Xk) -VΦ(Xk)k, where VΦ(Xk) is given by eq. (3) via implicit differentiation.
Lemma 3. Suppose Assumptions 1, 2 and 3 hold. Then, we have
kVΦ(Xk) - VΦ(Xk)k2 ≤Γ(1 - αμ)Tky*(Xk) - y01∣2 + 6L2κ(√K+-1 )2N帆 - v0∣∣2.
where Γ is given by eq. (18).
Proof of Lemma 3. Based on the form of VΦ(Xk) given by Proposition 1, we have
kVb Φ(Xk) - VΦ(Xk)k2 ≤3kVxf(Xk, yk(Xk)) - Vxf(Xk,ykT)k2 + 3kVxVyg(Xk, ykT)k2kVkk - VkN k2
+ 3kVxVyg(Xk, yk(Xk)) - VxVyg(Xk,ykT)k2kVkkk2,
18
Under review as a conference paper at ICLR 2021
which, in conjunction with Assumptions 1, 2 and 3, yields
I田Φ(xk) - VΦ(xfc)k2 ≤ 3L2∣∣y*(xk) - yT∣∣2 + 3L2∣∣碇-VN∣∣2 + 3τ2||域『IIyT- y*(x®)∣∣2
(i) C	EC	C	『c 3τ2m2
≤3L2∣∣y*(xk) - yT∣∣2 + 3L2∣∣v% -VN∣∣2 + 3^L∣∣yf - y*(%)∣∣2. (19)
μ2
where (i) follows from the fact that ∣ v	≤ II(Vy g(xk,y^(x k)))TIlIlVy f (xk,yf-(x k ))∣∣ ≤ M.
For notation simplification, let ¾ = (Vyg(x⅛,yT))-1Vyf (x⅛,yT). We next upper-bound ∣∣v^ -
VNIl in eq. (19). Based on the convergence result of CG for the quadratic programing, e.g., eq. (17)
in Grazzi et al. (2020), we have ∣∣v N -b|| ≤ √κ(√√∣-1) l∣v0 — b∣∣∙ Based on this inequality,
we further have
M -VNI ≤M -bkI + IvN -bkI ≤ M -bkI + √κ(√κ+-1)NIv0 -bkI
≤ (ι+√κ( *4 )N)Ivk - bk i+√κ( *4 )'碇-v0 i∙	(20)
Next, based on the definitions of Vk and v'k, we have
llvk - bkIl=Il(Vyg(Xk,yT))-1Vyf (Xk,y) - (Vyg(XkH(Xk))-1 Vyf (Xk,y*(Xk))∣∣
≤(κ+ρμ^)IyT - y*(Xk )∣∣.	(21)
Combining eq. (19), eq. (20), eq. (21) yields
IVΦ(xk) - VΦ(xk)I2 ≤(3L2 + 3τμM2) Iy*(xk) - yTI2 + 6L2κ(√5^)2NM - 遭『
+ 6L2 (1 + √κ( M)N )2(K + 祟)2IyT - y* (Xk)I2,
which, in conjunction with ∣∣yT 一 y* (Xk )I ≤ (1 一 ɑμ)T ∣∣y0 - y*(xk )I and the notations in eq. (18),
finishes the proof.	□
Lemma 4. Suppose Assumptions 1, 2 and 3 hold. Choose
T ≥ log (36κ(κ + 绊)2 + 16(κ2 + 4L学)2β2Γ)/log	= Θ(κ)
μ2	μ2	1 — α
N ≥1 log(8κ + 48(κ2 + 当L + 2L学)2β2L2κ)∕log 卓士| = θ(√K),
2	μ	μ	K K — 1
where Γ is given by eq. (18). Then, we have
/1、k	口 /1、k-1-j
∣y0 - y*(xk)∣2+∣v* - v012 ≤ (L) ∆° + ω X (2)	∣VΦ(xj)∣2,
j=0
where Ω and ∆0 are given by eq. (18).
Proof of Lemma 4. Recall that y0 = yj. Then, we have
∣y0 - y*(xk)∣2 ≤2∣yT-1 - y*(xk-1)∣2 + 2∣y*(xk) - y*(xk-1)∣2
(i)	ʌ
≤ 2(1 - αμ)T Iyk-1 - y*(Xk-I)∣2 + 2K2 β2 ∣v φ(xk-1)∣2
≤2(1 - αμ)T∣y0-1 - y*(xk-1)∣2 + 4κ2β2∣VΦ(xk-1) - VΦ(xk-1)∣2
+ 4κ2β 2∣VΦ(xk-1)∣2
(ii)
≤ (2(I- ɑμ)T + 4内声r(1 - ɑμ)T) ∣y*(Xk-I)- yk-112
+ 24κ4L2β2(√κ+1 )2N∣vk-1 - v0-1∣2 +4κ2β2∣VΦ(xk-1)∣2∙
(22)
(23)
(24)
19
Under review as a conference paper at ICLR 2021
where (i) follows from Lemma 2.2 in Ghadimi & Wang (2018) and (ii) follows from Lemma 3. In
addition, note that
M - v0k2 =M -V乙k2 ≤ 2M-I - v乙k2 + 2∣∣vk - v"∣∣2
≤4(1 + √κ) (K + ^2~) (1 - αμ)τ∣∣y0-ι - y*(xk-ι)∣∣2
+ 4κ(√K-1 )2∖屈-1 - V0-1∣2 + 2∣以-v"∣∣2,	(25)
where (i) follows from eq. (20). Combining eq. (25) with ∣∣v % -v%-∕∣ ≤ (κ2 + 2ML + IlLWK )∣x --
x--ι∣, we have
llvfe - v0Il2 ≤(16κ(κ + ρM)	+4(κ2 + 4LMκ) β2r)(1 - 0μ)τ∣∣y0-i - y*(Xfc-i)∣∣2
+(4k + 48 (κ2 +
2 2	2ML
+4 卜2+L+
2ML 2LMκ∖ 2dr2、/
L+-ɪ ββ l kK
'2LMMK )2 β2∣vΦ(χ --ι)∣2,
√≡4 )2N M-I-V O-1∣2
(26)
where (i) follows from Lemma 3. Combining eq. (24) and eq. (26) yields
∣y0-y*(χ-)k2 + B-v0∣∣2
<(18κ(K+
2
+ 8 K2 +
4LMκ∖ 2
42
β2Γ (1-α4)τ∣∣y0-i-y*(x k-i)∣2
+ 4K + 24 K2 +
2ML
2	2ML
+ 8 (K + ~2~ +
X	fjj
-^2--+
μ2
2LMκ∖ 2
42
?)2β2L2κ)(
√K -1 λ2Nll .
√K+lJ MT
β2∣VΦ(χk-ι)∣2,
—
which, in conjunction with eq. (22),
∣y0 - y*(x -)∣2 + M - vW ≤∣(∣y二-y*(x --i)∣2 + ∣∣v"
—
(C 9	2βML	2βLMκ∖ 2U「工/ xιl9
+ 8(βκ2 +	+ βμ2	) ∣VΦ(x --i)∣∣2
(27)
Telescoping eq. (27) over k and using the notations in eq. (18), we finish the proof.
□
Lemma 5. Under the same setting as in Lemma 4, we have
ʌ	Q	/1、-	^^ /1、--1-j	Q
l∣vφ(x -) -vφ(x -)∣∣ ≤δτ,N(2) Aq + δτ,NCE (2)	l∣vφ(xj)11 ∙
j=0
where δτ,N, Ω and AQ are given by eq. (18).
Proof of Lemma 5. Based on Lemma 3, eq. (18) and using ab+cd ≤ (a+c)(b+d) for any positive
a, b, c, d, we have
IlVΦ(x-) - VΦ(x-)k2 ≤δτ,N(∣y*(X -) - yQ∣2 + M - vQ∣2),
which, in conjunction with Lemma 4, finishes the proof.
□
20
Under review as a conference paper at ICLR 2021
E.1 Proof of Theorem 1
In this subsection, provide the proof for Theorem 1 based on the supporting Lemma 5. Based on the
smoothness of the function Φ(x) established in Lemma 2, we have
Φ(χk+ι) ≤Φ(χk) + hVΦ(χk),χk+ι - Xki + ^Φk∣∣χk+ι - Xk∣∣2
≤Φ(xk) - βhVΦ(xk), VΦ(xk) - VΦ(xk)i- β∣VΦ(xk)k2 + β2Lφ∣VΦ(xk)∣2
+ β2LΦ ∣VΦ(Xk) - Vb Φ(Xk)∣2
≤φ(xk) - (2 - β2Lφ) ∣Nφ(Xk)II2 + (£ + β2Lφ) ∣Nφ(Xk)- Vφ(xk)∣∣2,	(28)
which, combined with Lemma 5, yields
φ(xk+l) ≤φ(xk ) - (2 - β2Lφ) ||V©Xk) 112 + (2 + β2Lφ) δT,N (£) δ0
k-1	k-1-j
+(2 + β2Lφ)δτ,Nω X (2)	llV©xj)II2∙
j=0
Telescoping eq. (29) over k from 0 to K - 1 yields
(2 - β2Lφ) X ||V©Xk )112 ≤ φ(xθ) - inf φ(x) + (2 + β2Lφ)δT,N δ0
k=0	x
(29)
K-1 k-1	k 1 j
+ (2 + β2Lφ)δτ,Nω X X (2)	IV©xj)∣2,
k=1 j=0
k-1-j
which, using the fact that PK-IIPM(2)	gΦ(χ,)k2 ≤ PK-1 5k PK-1 kVΦ(χfc)k2 ≤
2 PK=01 kVΦ(χfc)k2, yields
β	K-1
(2 - β2Lφ - (βΩ + 2Ωβ2Lφ)δτ,N) X ∣VΦ(xk)∣2
k=0
≤ φ(xθ) - inf φ(x) + (2 + β2Lφ)δT,Nδ0.	(3O)
Choose N and T such that
(Ω + 2ΩβLφ)δτ,N ≤ 4, δτ,N ≤ 1.
(31)
Note that based on the definition of δτ,N in eq. (18), it suffices to choose T ≥ Θ(κ) and N ≥ Θ(√κ)
to satisfy eq. (31). Then, substituting eq. (31) into eq. (30) yields
(4 - β2Lφ) X ∣∣VΦ(xk)∣∣2 ≤ Φ(xo) - infφ(x)+ g + β2Lφ)∆o,
k=0	x
which, in conjunction with β ≤ 8⅛ , yields
(32)
1 K-1
K X
k=0
∣VΦ(Xk)∣2 ≤
64Lφ(Φ(xo)- infX Φ(x)) + 5∆o
K
(33)
In order to achieve an -accurate stationary point, we obtain from eq. (33) that AID-BiO requires
at most the total number K = O(κ3-1) of outer iterations. Then, based on eq. (3), we have the
following complexity results.
•	Gradient complexity:
Gc(f, e) = 2K = O(κ3e-1),Gc(g, e) = KT = O(κ4e-1).
•	Jacobian- and Hessian-vector product complexities:
JV(g, E) = K = O (κ3e-1) , HV(g, E) = KN = O (k3.5€-1).
Then, the proof is complete.
21
Under review as a conference paper at ICLR 2021
F Convergence Proofs for ITD-BiO in Section 4.1
We first characterize an important estimation property of the outer-loop gradient estimator d"XX,yk)
in ITD-BiO for approximating the true gradient VΦ(χk) based on Proposition 2.
Lemma 6. Suppose Assumptions 1, 2 and 3 hold. Choose α ≤ L. Then, we have
Il fg,yT) - VΦ(xfc )∣∣ ≤( LL + 叱-W)晋 + 2Mτμ+LPρ (1 - α") T-1) ∣∣y0 - y*(χj∣
LM (1 — αμ)τ
μ .
Lemma 6 shows that the gradient estimation error ∣∣ d"XX,yk) 一 VΦ(χk)∣∣ decays exponentially
w.r.t. the number T of the inner-loop steps. We note that Grazzi et al. (2020) proved a similar result
via a fixed point based approach. As a comparison, our proof of Lemma 6 directly characterizes
the rate of the sequence (舞,t = 0,…，T) converging to %胃 via the differentiation over all
corresponding points along the inner-loop GD path as well as the optimality of the point y* (Xk).
ProofofLemma 6. Using VΦ(xk) = Vxf(Xk,y*(xk)) + dy⅛k) Vyf(Xk ,y*(xk)) and eq. (16),
and using the triangle inequality, we have
∣∣ d⅛y0 -vφ(Xk )∣∣
= kVxf(Xk,yT) -Vxf(Xk,y*(xk))k + IyT - dy^ kVyf(xk,yT)k
∣ ∂Xk	∂Xk ∣
+ ∣∣⅞(X^∣∣∣∣Vyf(Xk,yT) - Vyf(Xk,y*(Xk))∣∣
<L∖yT-y*(Xk)k + M IyT - ^xL + L∣∣^xL∣∣kyT-y*(Xk)k, (34)
Xk	Xk	Xk
where (i) follows from Assumption 2. Our next step is to upper-bound ∣∣ Ix^ 一 dygxxk)∣∣ in eq. (34).
Based on the updates ykt = ykt-1 - αVyg(Xk, ykt-1) fort = 1, ..., T in ITD-BiO and using the chain
rule, we have
∂yk = ^yk	α (VX Vy g(Xk ,yk-1)+~~q^~ Vyg(Xk,yk-1)).	(35)
Xk	Xk	Xk
Based on the optimality of y*(Xk), We have Vyg(Xk, y*(∕k)) = 0, which, in conjunction with the
implicit differentiation theorem, yields
VχVyg(Xk,y*(Xk)) + ly∂XXk) Vyg(Xk,y*(Xk)) = 0.
Substituting eq. (36) into eq. (35) yields
(36)
∂yk	∣y*(Xk)
-Z---.....二--
IXk	IXk
---y∂Xj' - α (VXVyg(Xk，yk-1) + IXk Vygχk, yk-1))
+ α (VXVyg(Xk,y*(Xk)) + 1∣Xx^vyg(Xk,y*(Xk)))
£ - dynXk) - α (VxVyg(Xk,y『—xVg(Xk,y*(Xk)))
岩)vy g(Xk,yk-1)
+ αly∣XXk) (vyg(Xk,y*(Xk)) - vyg(Xk,y『).
(37)
22
Under review as a conference paper at ICLR 2021
Combining eq. (36) and Assumption 2 yields
dy∂Xxk) =IlVxVyg(xk,y*(xfc)) [vyg(xfc,y*(xk))]-1∣∣ ≤ %	(38)
Then, combining eq. (37) and eq. (38) yields
F II≤)I∣I
∂y*(χk) ∣∣
∂xk 11
+α (T+^μ^)kyk-1 - y*(Xk)k
≤)(1- α”)∣∣展
∂xk
-C ∣∣ + α(τ + LP)kyk-1-y*(χk )k,
(39)
where (i) follows from Assumption 3 and (ii) follows from the strong-convexity of g(χ, ∙). Based
on the strong-convexity of the lower-level function g(χ, ∙), We have
kyk-1 - y*(Xk)k ≤ (I- αμ)t-1 I∣y0 -y*(χk)k∙	(4O)
Substituting eq. (40) into eq. (39) and telecopting eq. (39) over t from 1 to T , we have
》∣∣≤(1-α”)T
∂Xk
dy*(xk) ∣∣
∂xk ∣∣
+ α (T + ^μ) ^X(I - αμ)τ-1-t(1 - αμ)2∣∣y0 - y*(Xk)k
=(1 - αμ)T∣∣ ∂Xk - 『 ∣∣ + 2(^(1 - α”尸 ky0 -	)k
≤L(I- a“,' + 2(T〃 + LP) (1 - αμ)T-i ∣y0 - y*(xk)k,	(41)
μ	μ2
∂y0
where the last inequality follows from ∂χk = 0 and eq. (38). Then, combining eq. (34), eq. (38),
eq. (40) and eq. (41) completes the proof.	□
F.1 Proof of Theorem 2
Based on the characterization on the estimation error of the gradient estimate df (Xx：k in Lemma 6,
we now prove Theorem 2.
Recall the notation that VΦ(χk) = f∂X jk'. Using an approach similar to eq. (28), we have
Φ(xk+ι) ≤Φ(xk) - (2 - β 2Lφ)∣∣VΦ(xk )k2 + (2+ β 2Lφ)∣∣VΦ(xk)-V Φ(xk)k2,	(42)
which, in conjunction with Lemma 6 and use Iyk0 - y * (Xk)I2 ≤ ∆, yields
φ(xk + l) ≤φ(xk) - (2 - β2LΦ)kV©Xk)112
+ 3∆( 2+ β2Lφ)( LL≠~ (1 - ɑμ)τ + 4M 2 (τμ4+Lρ)2 (1 - aμ)τ —1)
+ 3(β , B2L ) L2M2(1 - aμ)2T	z43x
十3(2十β lφ)----------μ-------.	(43)
Telescoping eq. (43) over k from 0 to K - 1 yields
1 K-1 1
KK X (2 - βLφ)∣∣VΦ(χk)k2 ≤
k=0
Φ(x0)-infχ Φ(x) ʌl	∖L2M2(1 - aμ)2T
-----βK------+ 3(2+ βLφ)-------μ-------
+3∆( 2 + βLφ)(L(L+μ~ (1 - aμ)T + 4M 2(7+LP)2 (1 - aμ)T-1) .	(44)
23
Under review as a conference paper at ICLR 2021
SUbstUtingβ = 4L1Φ andT = log (max {3LμM, 9δl2(1 + μ)2,3SM-OX"2}*)/log ι-αμ
Θ(κ log ɪ) in eq.(44) yields
k=0
16LΦ(Φ(x0) - infx Φ(x))	2
K	+可.
(45)
In order to achieve an -accUrate stationary point, we obtain from eq. (45) that ITD-BiO reqUires at
most the total nUmber K = O(κ3-1) of oUter iterations. Then, based on the gradient form given
by Proposition 2, we have the following complexity resUlts.
•	Gradient complexity: Gc(f, E) = 2K = O(κ3e-1), Gc(g, E) = KT = O (κ4e-1 log ɪ).
•	Jacobian- and Hessian-vector prodUct complexities:
JV(g, E) = KT = O (k4e-1 log -) , HV(g, E) = KT = O b4e-1 log -).
Then, the proof is complete.
G Proofs of Main Results for Stochastic Case in Section 4.2
In this section, we provide proofs for the convergence and complexity resUlts of the proposed algo-
rithm for the stochastic case.
G. 1 Proof of Proposition 3
Based on the definition of vQ in eq. (5) and conditioning on xk, ykT , we have
Q1 Q
EvQ =EnE ∏ (I - RlG(Xk,yΤ； Bj))VyF(xk,y； DF),
q=-1 j=Q-q
Q
=n E(I- n%g(xk ,yT))q Vyf (Xk ,yT)
q=0
∞∞
= ηX(I- ηV2yg(Xk,ykT))qVyf(Xk,ykT) - η X (I -ηVy2g(Xk,ykT))qVyf(Xk,ykT)
q=0	q=Q+1
∞
= η(ηVy2g(Xk, ykT))-1Vyf(Xk, ykT) - η	(I - ηV2yg(Xk, ykT))qVyf(Xk, ykT),
q=Q+1
which, in conjunction with the strong-convexity of function g(x, ∙), yields
∞
IIEvQ - [V2g(xk,yT)]-1Vyf(xk,yT)∣∣ ≤ n X (I- nμ)qM ≤
q=Q+1
(1 — nμ)Q+1M
μ
(46)
24
Under review as a conference paper at ICLR 2021
This finishes the proof for the estimation bias. We next prove the variance bound. Note that
Q-1	Q	2
Eη X Y (I - ηvyG(χk,yT; Bj))VyF(χk,yT; DF) - (▽：g(xk,yT))-1Vyf(χk,yT)11
q=-1 j=Q-q
(i)
≤2E
Q-1	Q	2
η X Y (I - ηVyG(xk,yT; Bj)) - (vyg(xk,yT))-1 M2 +
q=-1 j=Q-q
2M 2
μ2Df
Q-1	Q	Q	2
≤4EηXY
(I - ηvyG(xk,ykT; Bj)) - ηX(I- ηV2g(χk,yT))qll M2
q=-1 j=Q-q	q=0
lQ
+ 4Elllη Y(I - ηv2yg(xk, ykT))q) - (v2y g(xk, ykT))-1
2
M2 +
2M 2
μ2Df
(ii)	2 ll Q Q
≤ 4η2Elll
Q
(I - ηVyG(xk,yT; Bj)) - E(I- ηVyg(xk,yT))q
q=0
2 M2 + 4(1- ημ)2Q+2M2 + 且
+	μ2	+ μ2Df
(iii)
≤ 4η2 M2 QE
Q
X
q=0
Q	l2
Y	(I - ηvyG(xk,yT; Bj)) - (I - ηvyg(xk,yT))q
j=Q+1-q	l
Mq
4(1 - ημ)2Q+2M2	2M2
+	μ	+ μ2Df
(47)
where (i) follows from Lemma 1, (ii) follows from eq. (46), and (iii) follows from the Cauchy-
Schwarz inequality.
Our next step is to upper-bound Mq in eq. (47). For simplicity, we define a general quantity Mi for
by replacing q in Mq with i. Then, we have
Q	2
EMi =E (I -NQyglXk,yT))	Y	(I -NQyG(Xk,yT; BjX-(I -Nyg(Xk,y ))* i
j=Q+2-i
Q	2
+ E η(vQg(χk, yT) - PQyG(Xk, yT; BQ+ι-i))	∏	(I -NQyG(Xk, yT; Bj))
j=Q+2-i
Q
+ 2E((I -Nyg(Xk, yT))	∏	(I -NQyG(Xk,yT; Bj)) - (I - NQyg(Xk, yT))i,
j=Q+y-i
Q
心Qyg(Xk, yT) - NQyG(Xk, yT; BQ+ι-i))	∏	(I - ηp2G(Xk, yT; Bj)))
j=Q+y-i
(i)	Q	y
=E (I -NQyg(Xk,ykT))	∩	(I -NllGlXk,yT; Bj)) -(I -NQyg(Xk,ykT))
j=Q+y-i
Q	y
+ E HFg(Xk,yT) -VyG(Xk,y；BQ+ι-i))	Y	(I -ηpyG(Xk,y;Bj))
j=Q+y-i
(≤)(1 - ημ)2EMi-1 + η2(1 - ημ)2i-2EkVyg(Xk,yl) - VyG(Xk,ykT； BQ+1-i)k2
(≤)(1 - ημ)2EMi-1 + η2(i - ημ)2i-2 ,κ L ,,	(48)
|BQ+1-i |
where (i) follows from that fact that EBQ+1-i V2y G(Xk, ykT; BQ+1-i) = V2yg(Xk, ykT), (ii) follows
from the strong-convexity of function G(x, ∙; ξ), and (iii) follows from Lemma 1.
25
Under review as a conference paper at ICLR 2021
Then, telescoping eq. (48) over i from 2 to q yields
q
EMq ≤ L2η2(i- ημ)2q-2 X
j=1
1
lBQ+1-j |
which, in conjunction with the choice of ∣BQ+ι-j | = BQ(1 - ημ)j-1 for j = 1,…，Q, yields
EMq ≤η2(1 - ημ)2q-2
X BQ( 1⅛ r
ηL (I- ημ)2q-2 ( 1-ημ )_-2 ≤	ηL2__L(I- ημ)q.
bq	iɪ - 1	^ (1 - ημ)μBQ
(49)
Substituting eq. (49) into eq. (47) yields
Q-1	Q	2
En X Y (I-%G(Xk,yT； Bj))VyF(Xk,yT; DF) - (Vyg(Xk,yT))TVyf(Xk,yT)
q=-1 j=Q-q
Q
≤4η2M2Q X
q=0
√l2-焉(i-nμ)q + 4(I-叫 Q+2M2
(1 - nμ)μ BQ	μ2
2M2
+————
+ μ2 Df
4n2L2M2 1	4(1 - nμ)2Q+2M2
≤ 一μ2 — B +	μ2
2M2
+-----,
+ μ2Df ,
(50)
where the last inequality follows from the fact that P" Xq ≤ y⅛χ. Then, the proof is complete.
G.2 Auxiliary Lemmas for Proving Theorem 3
We first use the following lemma to characterize the first-moment error of the gradient estimate
Vb Φ(Xk), whose form is given by eq. (6).
Lemma 7. Suppose Assumptions 1, 2 and 3 hold. Then, conditioning on Xk and ykT, we have
H ^	∣∣2	/	L2 MT LMρ∖2 τ	2 2L2M2(1 — nμ)2Q
∣∣ev①加) -vφ(Xk)H ≤2(L+^μ +—μ—∣—μ2-) Ilyk -y (Xk)k +-------------------μ--------.
Proof of Lemma 7. To simplify notations, we define
VeΦT(Xk) = Vxf(Xk,ykT) - VxVyg(Xk,ykT)V2yg(Xk,ykT)-1Vyf(Xk,ykT).	(51)
Based on the definition of Vb Φ(Xk) in eq. (6) and conditioning on Xk and ykT, we have
EVb Φ(Xk) =Vxf(Xk, ykT) - VxVyg(Xk, ykT)EvQ
=VeΦT(Xk) - VxVyg(Xk,ykT)(EvQ - [V2yg(Xk, ykT)]-1Vyf(Xk, ykT)),
which further implies that
HHEVbΦ(Xk) -VΦ(Xk)HH2
≤2EIVe ΦT (Xk) -VΦ(Xk)I2+2IEVbΦ(Xk) -VeΦT(Xk)I2
≤2EIVe ΦT (Xk) -VΦ(Xk)I2+2L2IEvQ -[Vy2g(Xk,ykT)]-1Vyf(Xk,ykT)I2
≤2E∣VΦτ(Xk)-VΦ(Xk)k2 + 2L2M2(1 - nμ)2Q+2,	(52)
μ2
where the last inequality follows from Proposition 3. Our next step is to upper-bound the first term at
the right hand side of eq. (52). Using the factthat ∣∣ V：g(X, y)-11H ≤ μ and based on Assumptions 2
26
Under review as a conference paper at ICLR 2021
and 3, we have
∣∣vΦτ(xk) - VΦ(xk)k ≤kVχf(xk,yT) - Vxf(xk,y*(xk))k
+	IIyT - y*(Xk)k +	IIyT - y*(Xk)k
μ	μ
+ LMIlVyg(Xk,yT)-1 - Vyg(Xk,y*(χk))-1∣∣
JT	L2	MT LMρ7 T *, 、u	C
≤(L+^μ +—μ—+—μ2~)ιιyk - y (Xk)k,	(53)
where the last inequality follows from the inequality IM1-1 - M2-1 I ≤ IM1-1M2-1 IIM1 - M2I
for any two matrices M1 and M2 . Combining eq. (52) and eq. (53) yields
∣∣ ^	∣∣2	/	L2 MT LMρ∖2 T	2 2L2M2(1 — ημ)2Q
∣∣ev趴4) -vφ(Xk)II ≤2(L+ - +—μ—∣—μ2-) Ilyk -y (Xk)k +----------------μ--------,
which completes the proof.	□
Then, we use the following lemma to characterize the variance of the estimator Vb Φ(Xk).
Lemma 8. Suppose Assumptions 1, 2 and 3 hold. Then, we have
、 ~< 4L 4L2M2	∕8L2	、M2	16η2L4M2 1	16L2M2(1 - ημ)2Q
EkV Wk) - v©Xk)k2 ≤ E 十(万+2)Df+-⅛l 月+——μ∑-^μ^
+ (L + L2 + M + 空)2EkyT-y*(Xk )∣2.
∖ μ μ μ2 /
Proof of Lemma 8. Based on the definitions of VΦ(Xk) and VΦT(Xk) in eq. (4) and eq. (51) and
conditioning on Xk and ykT , we have
EIVbΦ(Xk) -VΦ(Xk)I2
(=i)EIVbΦ(Xk) -VeΦT(Xk)I2+IVeΦT(Xk) -VΦ(Xk)I2
(ii)	1	2
≤ 2E∣∣VχVyG(Xk,yT;DG)VQ -VxVyg(Xk,yT)[V：g(Xk,yT)「Vyf(Xk,yT)∣∣ +
2M2
万
+ (L + L + M + 空)2∣yT-y*(Xk )∣2
∖ μ μ	μ2 )
(iii)	4M2	1
≤ —— EkVxVyG(Xk,yT;DG)-VxVyg(Xk,yT)∣2 +4L2E∣vq - [Vyg(Xk,yT)]— Vyf(Xk,yT)∣2
μ
L2	MT LMρ 2 T	2	2M2
+ (L + 7 + V +	) kyT-yPk)k2+ 万，
(54)
1 /八 CIl	C .1 ∕' . .1 . ττn	C τ / \ 吕 T / 、/…∖cιι	C T	<
where (i) follows from the fact that EDG,DH,DFVΦ(Xk) = VΦT (Xk), (ii) follows from Lemma 1
and eq. (53), and (iii) follows from the Young’s inequality and Assumption 2.
Using Lemma 1 and Proposition 3 in eq. (54), yields
2 4L2M2	16η2L4M2 1 EkV©Xk)-V©Xk)k ≤耳DJ + F— B + L2 MT LMρ + (L + F + V + L	16(1 - ημ)2QL2M2 μ		ι 8L2M2 + ~f 2M2 方f,	(55)
	2kykT	-y*(Xk)k2 +	
which, unconditioning on Xk and ykT , completes the proof.			□
It can be seen from Lemmas 7 and 8 that the upper bounds on both the estimation error and bias
depend on the tracking error ∣∣yT - y*(Xk)k2∙ The following lemma provides an upper bound on
such tracking error ∣∣yT - y*(∕k)k2.
27
Under review as a conference paper at ICLR 2021
Lemma 9. Suppose Assumptions 1, 2 and 4 hold. Define constants
λ =( L-μ
' L + μ
4L2M2
2T (2	+ *	(L +	L	+ M	+ LM )2)
∖ μ2	∖	μ μ μ2 / /
μDT +
铝 +2)M +
μ μ2 D Df
16η2L4M2 1	16L2M2(1 - ημ)2Q
μ2	B
μ2
∆
= 4β2L2 (L - μ)2T
ω	μ2 (L + μ).
Choose T such that λ < 1 and set inner-loop stepsize α = L+μ. Then, we have
EkyT-y*(χk )k2
(56)
≤λk ((∣+μ)2Tky0-y*(χo)k2 +
k-1
+ ω X λk-1-jE∣∣VΦ(xj)k2 +
j=0
2
心十 τμs
1 — λ
Proof of Lemma 9. First note that for an integer t ≤ T
kyk+1-y*(χk )k2
=kyk+1 - ykk2 + 2hyt+ - yk,yk - y*(χQi + kyk - y*(χk)k2
=α2kvyG(Xk,yk；St)k2 - 2αhvyG(Xk,yk; St),yk - y*(Xk)i + kyk -y*(Xk)k2.	(57)
Conditioning on ykt and taking expectation in eq. (57), we have
Ekyk+1-y*(χk )k2
≤α2(σS + kvyg(Xk,yk)k2) - 2αhvyg(Xk,yk" -y*(Xk)i
+ kyk-y*(Xk )k2
≤ F + α2kVyg(Xk, yk)k2 - 2α (LL+μkyk - y*(X®)k2 + kVy*,jk)k2 )
+ kyk-y*(Xk)k2
=α-F^ -	α (-rτ— α)	kvyg(Xk,yk)k2	+ (1	- 2αLμ)	kyk	- y*(Xk)k2	(58)
s ∖L + μ J	L L + μ J
where (i) follows from the third item in Assumption 2, (ii) follows from the strong-convexity and
smoothness of the function g. Since α = L+μ, We obtain from eq. (58) that
Ekyk+1 - y*(Xk)k2 ≤ (F)2kyk-y*(Xk)k2 +仃4°1 ς).	(59)
∖L + μj	(L + μ) S
Unconditioning on ykt in eq. (59) and telescoping eq. (59) over t from 0 to T - 1 yields
EkyT - y*(Xk)k2 ≤ (F)2TEky0 - y*(Xk)k2 + 工
∖L + μ J	LμS
=(L-μ)	EkyT-I - y*(Xk)k2 + jσ-ξ,	(6O)
∖L + μ J	LμS
Where the last inequality folloWs from Algorithm 2 that yk0 = ykT-1. Note that
EkykT-1 - y*(Xk)k2 ≤2EkykT-1 - y*(Xk-1)k2 + 2Eky*(Xk-1) - y*(Xk)k2
(i)	2L2
≤2Ekyk-ι - y*(Xk -1 )k2 +	2EkXk- Xk-1k2
μ2
≤2EkyT-i - y*(Xk-i)k2 + 2β22L2EkVΦ(Xk-i)k2
μ2
≤2EkyT-i - y*(Xk-i)k2 + 4β22L2EkvΦ(Xk-i)k2
μ2
4β2L2
+---2- Ekv φ(∕k-I)-Vφ(Xk-I)k ,	(61)
μ2
28
Under review as a conference paper at ICLR 2021
where (i) follows from Lemma 2.2 in Ghadimi & Wang (2018). Using Lemma 8 in eq. (61) yields
EkyT-ι-y*(χk )k2
+
4β2L2
μ V μ2Dg
≤ (2+察 S
L2	Mτ	LMρ
—	+ 一	■ +	2-
μ	μ	μ2
2	8L2	M2
+	(μ2	+2) Df
+
16η2 L4 M2 1
2) EkyT-1 - y*(xk-i)k2 + 4^E∣∣VΦ(xk-i)k2
16L2M2(1 - ημ)2Q
μ2	B
μ2
(62)
+
Combining eq. (60) and eq. (62) yields
EkyT-y*(χk )k2
≤( U YT (2+4T
L2	Mτ
L + 一 + 一 +
L - μ∖2T4β2L2 /4L2M2
∖L + μ/ μ2
4β2L2 ( L — μ )2T.
μ2	∖L + μJ
μ2Dg
`, μ
8L2
+ (淳+2
LMP
μ2
)2)EkyT-ι-y*(xk-i)k2
M2	16η2L4M2 1
Df +
μ2	B
16L2M2(1 - ημ)2Q
μ2
2
Ekvφ(XkT)k2+ B
(63)
+
+
μ
+
Based on the definitions of λ, ω, ∆ in eq. (56), we obtain from eq. (63) that
EkyT - y*(Xk)k2 ≤λEkyT-I- y*(Xk-I)112 + Gδ + J~+ + ωEkvφ(Xk-I)112.
LμS
(64)
Telescoping eq. (64) over k yields
EkyT-y*(χk )k2
T E	C	k-1 T “	∙	C ω∆ + 7‰
≤λkEkyT - y*(X0)k2 + ω £ λk-1-jEkVΦ(xj)k2 + LIS
1-λ
j=0
≤λk ((L+μ)2T kyo- y*(χo)k2 +
which completes the proof.
-，_	C	ω∆+ 7⅛
+ ω X λk-1-jE∣∣VΦ(xj)k2 +	LlS
1-λ
j=0
□
G.3 Proof of Theorem 3
In this subsection, we provide the proof for Theorem 3, based on the supporting lemmas we develop
in Appendix G.2.
Based on the smoothness of the function Φ(X) in Lemma 2, we have
Φ(xk+1) ≤Φ(xk) + hVΦ(xk),Xk+ι - Xki + ^2φ∣∣Xk+ι - Xkk2
≤Φ(Xk) - βhVΦ(Xk), VbΦ(Xk)i + β2LΦ kVΦ(Xk)k2+β2LΦ kVΦ(Xk) - VbΦ(Xk)k2.
For simplicity, let Ek = E(∙ | Xk, yT). Note that We choose β = 4L^. Then, taking expectation over
the above inequality, we have
EΦ(Xk+1) ≤EΦ(Xk) -βEhVΦ(Xk),EkVbΦ(Xk)i+β2LΦEkVΦ(Xk)k2
+ β2LΦ EkVΦ(Xk) -VbΦ(Xk)k2
(i)	β	β	β
≤EΦ(Xk) + 2EkEkVΦ(Xk) - VΦ(Xk)k2 - 4EkVΦ(Xk)k2 + ；E∣∣VΦ(Xk) - VΦ(Xk)k2
(ii)	β
≤ EΦ(xk) - 4E∣∣VΦ(xk)k2 +
β 4L2M2
+ 4( F +	+2
βL2M 2(1 - ημ)2Q
μ2
M2	16η2L4M2 1
Df +
16L2M2(1 - ημ)2Q
μ2	B
μ2
+
5β	L2	Mτ LMρ 2 T	2
+ZL+F+V+-ι∕)EkyT-y*(xk)k2
(65)
29
Under review as a conference paper at ICLR 2021
where (i) follows from Cauchy-Schwarz inequality, and (ii) follows from Lemma 7 and Lemma 8.
To simplify notations, Let
5 L2 Mτ LMρ 2
V =4 (L + F + V + F)
(66)
Then, applying Lemma 9 in eq. (65) and using the definitions of ω, ∆, λ in eq. (56), we have
Eφ(Xk+1) ≤eφ(Xk) - 4EkVφ(xk)k2 +
βL2M2(1 - ημ)2Q
μ2
+4△+βνλk ((L+μ)	ky0-y*(x0)k2+
k-1
+βνωXλk-1-jEkVΦ(Xj)k2+
j=0
2
βν (ω△ + τμs)
1-λ
Telescoping the above inequality over k from 0 to K - 1 yields
K-1	K-1 k-1
EΦ(xk ) ≤ Φ(xo)-4 X E∣∣VΦ(xk )k2 + βvω XX λkT-jE∣∣VΦ(xj )||2
k=0	k=1 j=0
+『((M门…(X0)k2 +亲)三
2
κβL2M 2(1 — ημ)2Q + KfvQA + Lμs)
μ2
1-λ
which, using the fact that
K-1 k-1	K-1 K-1	K-1
X Xλk-1-jEkVΦ(xj)k2 ≤ X λk X EkVΦ(xk)k2 < T-I X EkVΦ(xk)k2,
k=1 j=0	k=0	k=0	k=0
yields
K-1
G -鼻)⅛ xe”
k=0
Φ(xo) - infX Φ(x)	v((L-μ)2Tky0 - y*(x0)k2 + ⅛S)	A	L2M2(1 - ημ)2Q
≤	JK	+	K(1 - λ)	+z+ μ
2
V (ω△ + τμs)
+	1 - λ
(67)
1	log(12 + 48β22L2 (L+ L2 + Mτ + LMρ )2)
Since β =患 and T ≥ ------------------μ 2log( L+μ) μ----μ——L, We have λ ≤
further simplified to
6, and hence eq. (67) is
K-1
(4-5vω)K∙ X ENφ(xk)k2
k=0
<Φ(xo) - infX Φ(x)	2v((L-μ)2TkyO - y*(χ0)k2 + Lμs) A	L2M2(1 - ημ)2Q
≤	JK	+	K	+ ɪ+	μ2
+ 2v (ω △ + Lμs).	(68)
+
30
Under review as a conference paper at ICLR 2021
By the definitions of ω in eq. (56) and ν in eq. (66) and T ≥
have
log(12+ 48β2L2(L+ * + Mμτ + LM2ρ)2)
μ	μ μ
2log( L+μ)
we
5β2L2 (L - μ∖ 2T (τ	L2 MT LMP
νω = — (L+μ) (L + J + 下 + L
*(L + L + M + LMρ)2
μ μ μ μ
< 12 + 48μL2 (L + L + M + 铲)
2
5
≤ 48.
(69)
In addition, since T >
log (√β(L+L2+M+LμMρ))
log( L⅛)
, we have
L)2T = "LY'(L + - + M + 空)2 < ɪ
L + μ/	4 LL + μ/ ∖ μ μ	μ2 /	4β
(70)
Substituting eq. (69) and eq. (70) in eq. (68) yields
1 K-1
K EEkVΦ(xk)k2 ≤
k=0
8(Φ(xo) - infX Φ(x) + 2∣∣yo - y*(xo)k2)
βK
11	8L2M2	2
+ tδ+丁(I-ημ 产,
1	16νσ2
+ (1 + κ)ιμs
(71)
which, in conjunction with eq. (56) and eq. (66), yields eq. (10) in Theorem 3.
Then, based on eq. (10), in order to achieve an e-accurate stationary point, i.e., E∣VΦ(x)k2 ≤ E
with X chosen from χ0,..., XK-ι uniformly at random, it suffices to choose
32Lφ(φ(χo) — infχ φ(χ) + 52kyo - y*(XO)II2)
K =---------------------------------------
3
("),T = Θ(κ)
O
e
Q =Klogκ^,S = O(K),Dg = O (T) ,Df = O (Kr) ,B = O (Kr).
Note that the above choices of Q and B satisfy the condition that B ≥ @(1-)#—ι required
in Proposition 3.
Then, the gradient complexity is given by Gc(F, e) = KDf = O(K5e-2 ), Gc(G, e) = KTS=
O(K9e-2). In addition, the Jacobian- and Hessian-vector product complexities are given by
JV(G, e) = KDg = O(K5e-2) and
HV(Ge) = K X BQ(I-…=KBQ ≤O (⅛log >)
Then, the proof is complete.
H Proof of Theorem 4 on meta-learning
To prove Theorem 4, we first establish the following lemma to characterize the estimation variance
EB11 dLD lφφW ；B) - dLD∂Φk'Wk) 112, where WeT is the output of T inner-loop steps of gradient descent
at the kth outer loop.
Lemma 10. Suppose Assumptions 2 and 3 are satisfied and suppose each task loss LSi (φ, wi) is
μ-strongly-convex w.r.t. Wi. Then, we have
Il ∂LD(φk,WT;B)	∂Ld(φk,泊T)∣∣2	(	L)2M2
叫—∂φk--------------------∂φk-Ii ≤ c+μ ∣B∣.
31
Under review as a conference paper at ICLR 2021
Proof. Let wekT = (w1T,k, ..., wmT ,k) be the output of T inner-loop steps of gradient descent at the kth
outer loop. Using Proposition 2, we have, for task Ti ,
Il dLDi∂^ ∣∣≤kVφLDiM M )k
T-1	T-1
+IIα X VφVwiLSi(φk,wit,k) Y (I - αV2wiLSi(φk,wij,k))VwiLDi(φk,wiT.k)II
t=0	j=t+1
(i)	T -1	LM
≤M + αLM ɪ^(l - αμ)τ-t-1 = M +--------------,
(72)
where (i) follows from assumptions 2 and strong-convexity of LSi (Φ, ∙) Then, using the definition
of LD(φ, we; B) = |B| Pi∈B Ld- (φ, wi), we have
EBII
∂Ld (φk ,w ； B)
∂φk
∂LD(Φk,WT) Il2 = 1 E Il dLDi(OkIwTk) _ ∂Ld(φk,WT) Il2
∂φk ll |B| il1 ∂φk	∂φk ll
(i) 1 I ∂LDi(φk,wiT,k) I2
≤叫1	ll
(ii)
≤
L 2M2
(1 + μ)同
(73)
where (i) follows from EidLDidφkwik) = "嚼：曲 ) and (ii) follows from eq. (72). Then, the
proof is complete.	□
ɪʌ	C C ma	A . ʌ	-ɪ-ɪ ɪ / ; ∖	C / ; 〜φ / ι 、、 、 .ι	，♦ J C	J	ι t . S÷ τ / ; ∖
Proof of Theorem 4. Recall Φ(φ) := LD (φ, w*(φ)) be the objective function, and let VΦ(φk)
∂LD (φk ,wT)
∂φk
Using an approach similar to eq. (42), we have
φ(φk+1) ≤φ(φk ) + hvφ(φk), φk+1 - φk i +-2^ kφk + 1 - φk k2
≤φ(φk) - βDVΦ(φk), dLD(φφweT; B) E + β2^ll dLD(φφ};B) ll2.	(74)
Taking the expectation of eq. (74) yields
β2Lφ EkV Φ(Φk)k2
B) l2
(i)
EΦ(φk+1) ≤EΦ(φk) - βEVΦ(φk),VbΦ(φk) +
+ β⅞Φ e∣V ①⑺)-%D粤WIi
2	∂φk
(≤ii)EΦ(φk)-βEVΦ(φk),VbΦ(φk)+
β2Lφ EkV Φ(Φk)k2 + β2Lφ (i + l )2 M2
2	2	∖ μ/ |B|
E∣∣VΦ(φk)k2 + (2 + β2Lφ)E∣∣VΦ(φk) - VΦ(φk)k2
≤Eφ(φk) - (2 - β2Lφ)
β2LΦ	L 2M2
+⅜- (ι+小 西，
(75)
where (i) follows from EBLD(φk,wekT ; B) = LD(φk,wekT ) and (ii) follows from Lemma 10. Using
Lemma 6 in eq. (75) and rearranging the terms, we have
1 K-1 1
天 X (2 - βLφ)EkVΦ(Ok)k2
k=0
vφ(φo) — infφ φ(φ)	/1	∖L2M2(I- αμ)2τ	Blφ (	Ly M2
≤----βκ—+3(2+βLφ —μ— + 亍(1 + μ 西
,oʌf1 , ,JT、(L2(L + μ)2∕ι	TΓ , 4M2 (τμ + LP)2/1	∖T-1、
+ 3δ(^2 + βLφ) (—μ-(I - αμ) +-μ----(I - αμ)	p
32
Under review as a conference paper at ICLR 2021
where ∆ = maxk ∣∣W0 -滴*(Φk)k2 < ∞. Choose the same parameters β, T as in Theorem 2. Then,
we have
1 K-1
评 EEkVΦ(φk)k2 ≤
K
k=0
16LΦ(Φ(φ0)-infφΦ(φ))	2	L 2M2
K + τ +11 + μ 湎.
Then, the proof is complete.
□
33