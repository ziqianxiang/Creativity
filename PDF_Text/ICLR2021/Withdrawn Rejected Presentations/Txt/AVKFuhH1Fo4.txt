Under review as a conference paper at ICLR 2021
Transformers are Deep Infinite-Dimensional
Non-Mercer Binary Kernel Machines
Anonymous authors
Paper under double-blind review
Ab stract
Despite their ubiquity in core AI fields like natural language processing, the me-
chanics of deep attention-based neural networks like the “Transformer” model
are not fully understood. In this article, we present a new perspective towards
understanding how Transformers work. In particular, we show that the “dot-product
attention” that is the core of the Transformer’s operation can be characterized as a
kernel learning method on a pair of Banach spaces. In particular, the Transformer’s
kernel is characterized as having an infinite feature dimension. Along the way
we generalize the standard kernel learning problem to what we term a “binary”
kernel learning problem, where data come from two input domains and a response
is defined for every cross-domain pair. We prove a new representer theorem for
these binary kernel machines with non-Mercer (indefinite, asymmetric) kernels
(implying that the functions learned are elements of reproducing kernel Banach
spaces rather than Hilbert spaces), and also prove a new universal approximation
theorem showing that the Transformer calculation can learn any binary non-Mercer
reproducing kernel Banach space pair. We experiment with new kernels in Trans-
formers, and obtain results that suggest the infinite dimensionality of the standard
Transformer kernel is partially responsible for its performance. This paper’s results
provide a new theoretical understanding of a very important but poorly understood
model in modern machine learning.
1	Introduction
Since its proposal by Bahdanau et al. (2015), so-called neural attention has become the backbone of
many state-of-the-art deep learning models. This is true in particular in natural language processing
(NLP), where the Transformer model of Vaswani et al. (2017) has become ubiquitous. This ubiquity
is such that much of the last few years’ NLP breakthroughs have been due to developing new training
regimes for Transformers (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019;
Wang et al., 2019a; Joshi et al., 2020; Lan et al., 2020; Brown et al., 2020, etc.).
Like most modern deep neural networks, theoretical understanding of the Transformer has lagged
behind the rate of Transformer-based performance improvements on AI tasks like NLP. Recently,
several authors have noted Transformer operations’ relationship to other, better-understood topics in
deep learning theory, like the similarities between attention and convolution (Ramachandran et al.,
2019; Cordonnier et al., 2020) and the design of the residual blocks in multi-layer Transformers
(e.g., Lu et al. (2019); see also the reordering of the main learned (fully-connected or attentional)
operation, elementwise nonlinearity, and normalization in the original Transformer authors’ official
reference codebase (Vaswani et al., 2018) and in some more recent studies of deeper Transformers
(Wang et al., 2019b) to the “pre-norm” ordering of normalize, learned operation, nonlinearity, add
residual ordering of modern (“v2”) Resnets (He et al., 2016)).
In this paper, we propose a new lens to understand the central component of the Transformer, its
“dot-product attention” operation. In particular, we show dot-product attention can be characterized
as a particular class of kernel method (SchGlkoPf & Smola, 2002). More specifically, it is a so-called
indefinite and asymmetric kernel method, which refer to two separate generalizations of the classic
class of kernels that does not require the classic assumptions of symmetry and positive (semi-)
definiteness (Ong et al., 2004; Balcan et al., 2008; Zhang et al., 2009; Wu et al., 2010; Loosli et al.,
1
Under review as a conference paper at ICLR 2021
2016; Oglic & Gartner, 2018; 2019, etc.). We in fact show in Theorem 2 below that dot-product
attention can learn any asymmetric indefinite kernel.
This insight has several interesting implications. Most immediately, it provides some theoretical
justification for one of the more mysterious components of the Transformer model. It also potentially
opens the door for the application of decades of classic kernel method theory towards understanding
one of today’s most important neural network models, perhaps similarly to how tools from digital
signal processing are widely used to study convolutional neural networks. We make a first step on this
last point in this paper, proposing a generalization of prior kernel methods we call “binary” kernel
machines, that learns how to predict distinct values for pairs of elements across two input sets, similar
to an attention model.
The remainder of this paper is organized as follows. Section 2 reviews the mathematical background
of both Transformers and classic kernel methods. Section 3 presents the definition of kernel machines
on reproducing kernel Banach spaces (RKBS’s) that we use to characterize Transformers. In particular
we note that the Transformer can be described as having an infinite-dimensional feature space. Section
4 begins our theoretical results, explicitly describing the Transformer in terms of reproducing kernels,
including explicit formulations of the Transformer’s kernel feature maps and its relation to prior
kernels. Section 5 discusses Transformers as kernel learners, including a new representer theorem and
a characterization of stochastic-gradient-descent-trained attention networks as approximate kernel
learners. In Section 6, we present empirical evidence that the infinite-dimensional character of the
Transformer kernel may be somewhat responsible for the model’s effectiveness. Section 7 concludes
and summarizes our work.
2	Background and Related Work
2.1	Transformer Neural Network Models
The Transformer model (Vaswani et al., 2017) has become ubiquitous in many core AI applications
like natural language processing. Here, we review its core components. Say we have two ordered
sets of vectors, a set of “source” elements {s1, s2, . . . , sS}, sj ∈ Rds and a set of “target” elements
{t1, t2, . . . , tT}, ti ∈ Rdt. In its most general form, the neural-network “attention” operation that
forms the backbone of the Transformer model is to compute, for each ti, a ti-specific embedding of
the source sequence {sj}jS=1.1
The particular function used in the Transformer is the so-called “scaled dot-product” attention, which
takes the form
aij
(WQti)T(WKsj)
exp(aij)
αij = 一 G	;	^
Σ j=ι exp( aij)
S
ti = ∑ αij WVsj
j=1
(1)
where WV, WK ∈ Rds ×d, and WQ ∈ Rdt×d are learnable weight matrices, usually called the
“value,” “key,” and “query” weight matrices, respectively. Usually multiple so-called “attention
heads” with independent parameter matrices implement several parallel computations of (1), with
the Cartesian product (vector concatenation) of several d-dimensional head outputs forming the final
output ti. Usually the unnormalized aij's are called attention scores or attention logits, and the
normalized α^'s are called attention weights.
In this paper, we restrict our focus to the dot-product formulation of attention shown in (1). Several
other alternative forms of attention that perform roughly the same function (i.e., mapping from
Rds × Rdt to R) have been proposed (Bahdanau et al., 2015; Luong et al., 2015; Velickovic et al.,
2018; Battaglia et al., 2018, etc.) but the dot-product formulation of the Transformer is by far the
most popular.
2.2	Kernel Methods and Generalizations of Kernels
Kernel methods (Scholkopf & Smola, 2002; Steinwart & Christmann, 2008, etc.) are a classic and
powerful class of machine learning methods. The key component of kernel methods are the namesake
1Often, the source and target sets are taken to be the same, si = ti ∀i. This instance of attention is called
self attention.
2
Under review as a conference paper at ICLR 2021
kernel functions, which allow the efficient mapping of input data from a low-dimensional data domain,
where linear solutions to problems like classification or regression may not be possible, to a high- or
infinite-dimensional embedding domain, where linear solutions can be found.
Given two nonempty sets X and Y , a kernel function κ is a continuous function κ : X × Y → R. In
the next few sections, we will review the classic symmetric and positive (semi-) definite, or Mercer,
kernels, then discuss more general forms.
2.2.1	Symmetric and Positive Semidefinite (Mercer) kernels
If X = Y, and for all xi , xj ∈ X = Y, a particular kernel κ has the properties
symmetry: κ(xi, xj) = κ(xj , xi)	(2a)
positive (semi-) definiteness: cTKc ≥ 0 ∀ c ∈ Rn; i, j = 1, . . . , n; n ∈ N (2b)
where K in (2b) is the Gram matrix, defined as Kij = κ(xi, xj), then κ is said to be a Mercer kernel.
For Mercer kernels, it is well-known that, among other facts, (i) we can define a Hilbert space of
functions on X, denoted Hκ (called the reproducing kernel Hilbert space, or RKHS, associated with
the reproducing kernel κ), (ii) Hκ has for each x a (continuous) unique element δx called a point
evaluation functional, with the property f(x) = δx(f) ∀f ∈ Hκ, (iii) κ has the so-called reproducing
property,〈f, K(x, •))HK = f (x) ∀f ∈ HK, where《,∙)hK is the inner product on HK, and (iv) We
can define a “feature map” Φ : X → FH , where FH is another Hilbert space sometimes called the
feature space, and K(x, y) = (Φ(x), Φ(y)}fh (where〈•, ∙)f* is the inner product associated with
FH). This last point gives rise to the kernel trick for RKHS’s.
From a machine learning and optimization perspective, kernels that are symmetric and positive (semi-)
definite (PSD) are desirable because those properties guarantee that empirical-risk-minimization
kernel learning problems like support vector machines (SVMs), Gaussian processes, etc. are convex.
Convexity gives appealing guarantees for the tractability of a learning problem and optimality of
solutions.
2.2.2	Learning with non-Mercer kernels
Learning methods with non-Mercer kernels, or kernels that relax the assumptions (2), have been
studied for some time. One line of work (Lin & Lin, 2003; Ong et al., 2004; Chen & Ye, 2008; Luss
& D′aspremont, 2008; Alabdulmohsin et al., 2015; Loosli et al., 2016; Oglic & Gartner, 2018; 2019,
etc.) has focused on learning with symmetric but indefinite kernels, i.e., kernels that do not satisfy
(2b). Indefinite kernels have been identified as reproducing kernels for so-called reproducing kernel
Kreln spaces (RKKS's) since Schwartz (1964) and Alpay (1991).
Replacing a Mercer kernel in a learning problem like an SVM with an indefinite kernel makes the
optimization problem nonconvex in general (as the kernel Gram matrix K is no longer always PSD).
Some early work in learning with indefinite kernels tried to ameliorate this problem by modifying
the spectrum of the Gram matrix such that it again becomes PSD (e.g., Graepel et al., 1998; Roth
et al., 2003; Wu et al., 2005). More recently, Loosli et al. (2016); Oglic & Gartner (2018), among
others, have proposed optimization procedures to learn in the RKKS directly. They report better
performance on some learning problems when using indefinite kernels than either popular Mercer
kernels or spectrally-modified indefinite kernels, suggesting that sacrificing convexity can empirically
give a performance boost. This conclusion is of course reminiscent of the concurrent experience of
deep neural networks, which are hard to optimize due to their high degree of non-convexity, yet give
superior performance to many other methods.
Another line of work has explored the application of kernel methods to learning in more general
Banach spaces, i.e., reproducing kernel Banach spaces (RKBS’s) (Zhang et al., 2009). Various
constructions to serve as the reproducing kernel for a Banach space (replacing the inner product of an
RKHS) have been proposed, including semi-inner products (Zhang et al., 2009), positive-definite
bilinear forms via a Fourier transform construction (Fasshauer et al., 2015), and others (Song et al.,
2013; Georgiev et al., 2014, etc.). In this work, we consider RKBS’s whose kernels may be neither
symmetric nor PSD. A definition of these spaces is presented next.
3
Under review as a conference paper at ICLR 2021
3	General reproducing kernel Banach s paces
Recently, Georgiev et al. (2014), Lin et al. (2019), and Xu & Ye (2019) proposed similar definitions
and constructions of RKBS’s and their reproducing kernels meant to encompass prior definitions. In
this paper, we adopt a fusion of the definitions and attempt to keep the notation as simple as possible
to be sufficient for our purposes.
Definition 1 (Reproducing kernel Banach space (Xu & Ye, 2019, Definition 2.1; Lin et al., 2019,
Definitions 1.1 & 1.2; Georgiev et al., 2014)). Let X and Y be nonempty sets, κ a measurable
function called a kernel, κ : X × Y → R, and BX and BY Banach spaces of real measurable
functions on X and Y, respectively. Let《,∙)Bx×By : BX ×Bγ → R be a nondegenerate bilinear
mapping such that
K(x, ∙) ∈ BY	for all /	∈	X;	(3a)
(f,κ(x,节BX ×BY	=	f (x)	forall X	∈	X,f ∈Bχ;	(3b)
κ(∙, y) ∈ BX	for all y	∈	Y; and	(3c)
〈K(∙,y),ggBX×by	=	g⑻	forally	∈	Y,g ∈ by.	(3d)
Then, BX and BY are a pair of reproducing kernel Banach spaces (RKBS’s) on X and Y, respectively,
and K is their reproducing kernel.
Line (3a) (resp. (3c)) says that, if we take K, a function of two variables x ∈ X and y ∈ Y, and fix x
(resp. y), then we get a function of one variable. This function of one variable must be an element of
BY (resp. BX). Lines (3b) and (3d) are the reproducing properties of K.
For our purposes, it will be useful to extend this definition to include a “feature map” characterization
similar to the one used in some explanations of RKHS's (SChOlkOPf & Smola, 2002, Chapter 2).
Definition 2 (Feature maps for RKBS’s (Lin et al., 2019, Theorem 2.1; Georgiev et al., 2014)). For
a pair of RKBS’s as defined in Definition 1, suppose that there exist mappings ΦX : X → FX, ΦY :
Y → FY, where FX and FY are Banach spaces we will call the feature spaces, and a nondegenerate
bilinear mapping《,∙∖下^乂下^ : FX ×Fγ → R such that
K (x,y) = (Φχ (X), Φγ (y》FX ×fy	forall X ∈ X ,y ∈ Y.	(4)
In this case, the spaces BX and BY can be defined as (Xu & Ye, 2019; Lin et al., 2019)
BX = { fv	:	X → R : fv (x)，®x( x) ,v〉fx ×fy	； v ∈Fγ ,x	∈x}	(5a)
BY = { gu	:	Y → R : gu (y)	,〈 u, Φγ (y》FX ×fy	； U ∈ Fx ,y	∈y}.	(5b)
Remark 1. We briefly discuss how to understand the spaces given by (5). Consider (5a) for example.
It is a space of real-valued functions of one variable x, where the function is also parameterized by a
v. Picking a v ∈ FY in (5a) defines a manifold of functions in BX. This manifold of functions with
fixed v varies with the function ΦX . Evaluating a function fv in this manifold at a point x is defined
by taking the bilinear product of ΦX (x) and the chosen v. This also means that we can combine (4)
and (5) to say
K(x,y) = ®x(X),φyIy))FX×fy =〈fφx(C),gΦγ(y)〉BX×by	forallX ∈ X,y ∈ Y.⑹
Remark 2. If ΦX (x) and ΦY (y) can be represented as countable sets of real-valued measurable
functions, {φX(X)g}e∈n and {φY(y)£}氏N for (φX)e : X → R and (φY)e : Y → R (i.e.,
FX, FY ⊂ !L∈n R)； and(u, v)yX× ×Fγ = £生闪 Uyv for U ∈ FX, v ∈ FY; then the “feature
map” construction, whose notation we borrow from Lin et al. (2019), corresponds to the “generalized
Mercer kernels” of Xu & Ye (2019).
4	Dot-Product Attention as an RKBS Kernel
We now formally state the formulation for dot-product attention as an RKBS learner. Much like with
RKHS’s, for a given kernel and its associated RKBS pair, the feature maps (and also the bilinear
mapping) are not unique. In the following, we present a feature map based on classic characterizations
of other kernels such as RBF kernels (e.g., Steinwart et al. (2006)).
4
Under review as a conference paper at ICLR 2021
Proposition 1. The (scaled) dot-product attention calculation of (1) is a reproducing kernel for
an RKBS in the sense of Definitions 1 and 2, with the input sets X and Y being the vector spaces
from which the target elements {ti}iT=1, ti ∈ Rdt and source elements {sj}jS=1, sj ∈ Rds are drawn,
respectively; the feature maps
∞	n!	1/2 d
φx(() =工 工	d-1 4 ⅛P7	∏m产	(7a)
n=O P1+ P2+--+ Pd = n	1r 2 d 7 Q =1
∞ n!	1/2d
φγ(S) =工 工	d-1/4 (即F)	∏(k严	(7b)
n=O p 1+ P2+-+ Pd = n	1r 2 d 7 Q =1
where qQ is the 2th element of q = W Q(, k? is the 2th element of k = W K s, W Q ∈ Rd ×dt
WK ∈ Rd×ds, with d ≤ ds, dt and rank(W Q) = rank(W K) = d; the bilinear mapping
(Φχ(1), Φγ(s)〉为乂下》=Φχ(1) ∙ Φγ(s); and the Banach spaces
BX = { fk(() = exp ((WQt)Tk/√d) ; k ∈ FY, ( ∈ X}	(8a)
BY = {gq(S) = exp (qT(WKs)/√d) ; q ∈ FX, S ∈γ}	(8b)
with the “exponentiated query-key kernel,
K((, S) = ®x((), φY(SS))FX ×fy
fΦY (s), gΦX (t) BX ×BY
CYn ((WQt)T(WKS) ∖
exp∖	√d —)
(9)
the associated reproducing kernel.
The proof of Proposition 1 is straightforward and involves verifying (9) by multiplying the two
infinite series in (7), then using the multinomial theorem and the Taylor expansion of the exponential.
In the above and when referring to Transformer-type models in particular rather than RKBS’s in
general, we use (, S, q, and k for x, y, u, and v, respectively, to draw the connection between the
elements of the RKBS’s and the widely-used terms “target,” “source,” “query,” and “key.”
The rank requirements on WQ and WK mean that span({Φχ((),( ∈ X}) = FX and
span({Φγ (s), s ∈ Y}) = FY. This in turn means that the bilinear mapping is nondegenerate.
Remark 3. Now that we have an example of a pair of RKBS’s, we can make more concrete some of
the discussion from Remark 1. Examining (8a), for example, we see that when we select a k ∈ FY,
we define a manifold of functions in BX where k is fixed, but WQ can vary. Similarly, selecting a
q ∈ FX defines a manifold in BY. Selecting an element from both FX and FY locks us into one
element each from BX and BY , which leads to the equality in (6).
Remark 4. Examining (8)-(9), we can see that the element drawn from FY that parameterizes the
element of BX, as shown in (8a), is a function of ΦY (and vice-versa for (8b)). This reveals the exact
mechanism in which the Transformer-type attention computation is a generalization of the RKBS’s
considered by Fasshauer et al. (2015), Lin et al. (2019), Xu & Ye (2019), etc., for applications like
SVMs, where one of these function spaces is considered fixed.
Remark 5. Since the feature maps define the Banach spaces (5), the fact that the parameters WQ
and WK are learned implies that Transformers learn parametric representations of the RKBS’s
themselves. This is in contrast to classic kernel methods, where the kernel (and thus the reproducing
space) is usually fixed. In fact, in Theorem 2 below, we show that (a variant of) the Transformer
architecture can approximate any RKBS mapping.
Remark 6. The symmetric version of the exponentiated dot product kernel is known to be a re-
producing kernel for the so-called Bargmann space (Bargmann, 1961) which arises in quantum
mechanics.
Remark 7. Notable in Proposition 1 is that we define the kernel of dot-product attention as including
the exponential of the softmax operation. The output of this operation is therefore not the attention
scores aij but rather the unnormalized attention weights, & j = αij Ej aij. Considering the
exponential as a part of the kernel operation reveals that the feature spaces for the Transformer are in
fact infinite-dimensional in the same sense that the RBF kernel is said to have an infinite-dimensional
feature space. In Section 6, we find empirical evidence that this infinite dimensionality may be
partially responsible for the Transformer’s effectiveness.
5
Under review as a conference paper at ICLR 2021
5	Transformers as kernel learners
5.1	The binary RKBS learning problem and its representer theorem
Most kernel learning problems take the form of empirical risk minimization problems. For example,
if we had a learning problem for a finite dataset (x1, z1), . . . , (xn, zn), xi ∈ X, zi ∈ R and wanted
to learn a function f : X → R in an RKHS Hκ , the learning problem might be written as
1n
f = argmin — fL xi,zi,f( Xi)) + λR(∣∣ f |山 K)	(10)
f∈Hκ ni=1	κ
where L : X × R × R → R is a convex loss function, R : [0, ∞) → R is a strictly increasing
regularization function, and λ is a scaling constant. Recent references that consider learning in
RKBS’s (Georgiev et al., 2014; Fasshauer et al., 2015; Lin et al., 2019; Xu & Ye, 2019) consider
similar problems to (10), but with the RKHS H replaced with an RKBS.
The kernel learning problem for attention, however, is different from (10) in that, as we discussed in
the previous section, we need to predict a response zij (i.e., the attention logit) for every pair (ti,	sj ).
This motivates a generalization of the classic class of kernel learning problems that operates on pairs
of input spaces. We discuss this generalization now.
Definition 3 (Binary kernel learning problem - regularized empirical risk minimization). Let X
and Y be nonempty sets, and BX and BY RKBS's on X and Y, respectively. Let《,∙)Bx×By :
BX × BY → R be a bilinear mapping on the two RKBS’s. Let ΦX : X → FX and ΦY : Y → FY be
fixedfeaturemappingswiththepropertythat〈①式(Xi), Φγ(yi节FX乂下》=(fφγ(y),gφχ(X)IBX乂石》.
Say {x1, . . .,xnx },xi ∈	X, {y1, . . .,	yny },	yj	∈ Y, and	{zij }i=1,...,nx ; j=1,...,ny ,	zij	∈ R is a
finite dataset where a response zij is defined for every (i, j) pair of an Xi and a yj . LetL	:
X × Y × R × R → R be a loss function that is convex for fixed (Xi, yj, zi,j), and RX : [0, ∞) → R
and RY : [0, ∞) → R be convex, strictly increasing regularization functions.
A binary empirical risk minimization kernel learning problem for learning on a pair of RKBS’s takes
the form
f , g = ,argmin	n n 工 L (xi, yj ,zij,〈 fφY (yj) ,g φx(X )I Bx ×Bγ)
f ∈BX,g∈BY nxny ij	X Y
i,j	(11)
+ λ X R X (H f ^Bx ) + λ Y R Y (Hg HBY)
where λX and λY are again scaling constants.
Remark 8. The idea of a binary kernel problem that operates over pairs of two sets is not wholly new:
there is prior work both in the collaborative filtering (Abernethy et al., 2009) and tensor kernel method
(Tao et al., 2005; Kotsia & Patras, 2011; He et al., 2017) literatures. Our problem and results are
new in the generalization to Banach rather than Hilbert spaces: as prior work in the RKBS literature
(Micchelli et al., 2004; Zhang & Zhang, 2012; Xu & Ye, 2019, etc.) notes, RKBS learning problems
are distinct from RKHS ones in their additional nonlinearity and/or nonconvexity. An extension of
binary learning problems to Banach spaces is thus motivated by the Transformer setting, where a
kernel method is in a context of a nonlinear and nonconvex deep neural network, rather than as a
shallow learner like an SVM or matrix completion. For more discussion, see Appendix A.
Virtually all classic kernel learning methods find solutions whose forms are specified by so-called
representer theorems. Representer theorems state that the solution to a regularized empirical risk
minimization problem over a reproducing kernel space can be expressed as a linear combination
of evaluations of the reproducing kernel against the dataset. Classic solutions to kernel learning
problems thus reduce to finding the coefficients of this linear combination. Representer theorems
exist in the literature for RKHS's (Kimeldorf & Wahba, 1971; Scholkopf et al., 2001; Argyriou et al.,
2009), RKKS's (Ong et al., 2004; Oglic & Gartner, 2018), and RKBS's (Zhang et al., 2009; Zhang &
Zhang, 2012; Song et al., 2013; Fasshauer et al., 2015; Xu & Ye, 2019; Lin et al., 2019).
Fasshauer et al. (2015, Theorem 3.2), Xu & Ye (2019, Theorem 2.23), and Lin et al. (2019, Theorem
4.7) provide representer theorems for RKBS learning problems. However, their theorems only deal
6
Under review as a conference paper at ICLR 2021
with learning problems where datapoints come from only one of the sets on which the reproducing
kernel is defined (i.e., only X but not Y), which means the solution sought is an element of only
one of the Banach spaces (e.g., f : X → R, f ∈ BX). Here, we state and prove a theorem for the
more-relevant-to-Transformers binary case presented in Definition 3.
Theorem 1. Suppose we have a kernel learning problem of the form in (11). Let κ : X × Y → R
be the reproducing kernel of the pair of RKBS’s BX and BY satisfying Definitions 1 and 2. Then,
given some conditions on BX and BY (see Appendix B), the regularized empirical risk minimization
problem (11) has a unique solution pair (f * ,g*), with the property that
nx	ny
ι (f *) = fξiκ (xi, ∙)	ι (g *) =工G K (∙ ,yj).	(12)
i=1	j=1
where ι(f) (resp. ι(g)) denotes the Gdteaux derivative Ofthe norm of f (resp. g) with the convention
that ι(0) , 0, and where ξi, ζj ∈ R.
Proof. See Appendix B.	口
5.2 A new approximate kernel learning problem and universal approximation
THEOREM
The downside of finding solutions to kernel learning problems like (10) or (11) of the form (12)
as suggested by representer theorems is that they scale poorly to large datasets. It is well-known
that for an RKHS learning problem, finding the scalar coefficients by which to multiply the kernel
evaluations takes time cubic in the size of the dataset, and querying the model takes linear time. The
most popular class of approximation techniques are based on the so-called Nystrom method, which
constructs a low-rank approximation of the kernel Gram matrix and solves the problem generated by
this approximation (Williams & Seeger, 2001). A recent line of work (Gisbrecht & Schleif, 2015;
Schleif & Tino, 2017; Oglic & Gartner, 2019) has extended the Nystrom method to RKKS learning.
In this section, we characterize the Transformer learning problem as a new class of approximate
kernel methods - a “distillation” approach, one might call it. We formally state this idea now.
Proposition 2 (Parametric approximate solutions of binary kernel learning problems). Consider the
setup of a binary kernel learning problem from Definition 3. We want to find approximations to the
solution pair (f * ,g*). In particular, we will say we want an approximation ^ : X ×Y → R such that
κ(x,y) ≈ fΦγ(y),gΦx(χ))ftvχBy	forailX ∈ Xandy ∈ Y.	(13)
Comparing (13) to (6) suggests a solution: to learn a function K that approximates κ. In particular,
(6) suggests learning explicit approximations of the feature maps, i.e.,
κ(x,y) ≈ (φχ(X), φy(y)〉FX×fy .
In fact, it turns out that the Transformer query-key mapping (1) does exactly this. That is, while
the Transformer kernel calculation outlined in Propostion 1 is finite-dimensional, it can in fact
approximate the potentially infinite-dimensional optimal solution (f*, g*) characterized in Theorem 1.
This fact is proved next.
Theorem 2. Let X ⊂ Rdt and Y ⊂ Rds be compact; t ∈ X, S ∈ Y; and let q? : X → R and
k : Y → R for £ = 1 ,...,d be two-layer neural networks with m hidden units. Then, for any
continuous function F : X ×Y → R and E > 0, there are integers m, d > 0 such that
d
F(t, s) —工 q?(t)k?(s) < E for all t ∈ X, S ∈ Y.	(14)
2 =1
Proof. See Appendix C.	口
We now outline how Theorem 2 relates to Transformers. If we concatenate the outputs of the
two-layer neural networks {q?}d=ι and {k?}d=ι into d-dimensional vectors q : Rdt → Rd and
k : Rds → Rd, then the dot product q(t)Tk(S) denoted by the sum in (14) can approximate any
7
Under review as a conference paper at ICLR 2021
real-valued continuous function on X × Y . Minus the usual caveats in applications of universal
approximation theorems (i.e., in practice the output elements share hidden units rather than having
independent ones), this dot product is exactly the computation of the attention logits aij , i.e.,
F(t, S) ≈ log K(t, S) for the F in (14) and the K in (9) UP to a scaling constant Vd.
Since the exponential mapping between the attention logits and the exponentiated query-key kernel
used in Transformers is a one-to-one mapping, if We take F(t, S) = log(fφ>(S), gφ天(t)〉BA,×b ,
then we can Use a Transformer’s dot-ProdUct attention to aPProximate the oPtimal solUtion to any
RKBS solution arbitrarily Well.
The core idea of an attention-based deep neural netWork is then to learn parametric representations
of qg and k via stochastic gradient descent. Unlike traditional representer-theorem-based learned
functions, training time of attention-based kernel machines like deep Transformers (generally, but
With no guarantees) scale sub-cubically With dataset size, and evaluation time stays constant regardless
of dataset size.
6	Is the exponentiated dot product es sential to Transformers ?
Table 1: Test BLEU scores for Transformers With various kernels on machine translation (case-
sensitive sacreBLEU). Values are mean ± std. dev over 5 training runs With different random seeds.
	EDP	RBF	L2 Distance	EI	Quadratic
IWSLT14 DE-EN	30.41 ± 0.03	30.32 ± 0.22	19.45 ± 0.16	30.84 ± 0.27	29.56 ± 0.19
WMT14 EN-FR	35.11 ± 0.08	35.57 ± 0.20	28.41 ± 0.26	34.51 ± 0.17	34.54 ± 0.30
EDP = Exponentiated dot product; EI = Exponentiated intersection kernel
We study modifications of the Transformer With several kernels used in classic kernel machines. We
train on tWo standard machine translation datasets and tWo standard sentiment classification tasks.
For machine translation, IWSLT14 DE-EN is a relatively small dataset, While WMT14 EN-FR is
a considerably larger one. For sentiment classification, We consider SST-2 and SST-5. We retain
the standard asymmetric query and key feature mappings, i.e., q = WQt and k = WKS, and only
modify the kernel K : Rd × Rd → R≥0 . In the beloW, τ > 0 and γ ∈ R are per-head learned scalars.
Our kernels of interest are:
1.	the (scaled) exponentiated dot product (EDP), K(q,t) = exp(qτk∕∖fd), i.e., the standard
Transformer kernel;
2.	the radial basis function (RBF) kernel, K(q,t) = exp(∣∣ — τ∕√d(q — k)^2), where ∣∣ ∙ . is
the standard 2-norm. It is Well-knoWn that the RBF kernel is a normalized version of the
exponentiated dot-product, with the normalization making it translation-invariant;
3.	the vanilla L2 distance, K(q, t) = τ∕√d∣∣q - kg;
4.	an exponentiated version of the intersection kernel, K(q,t) = exp(£d=ι min(qg, kg)). The
symmetric version of the intersection kernel was popular in kernel machines for computer
vision applications (Barla et al., 2003; Grauman & Darrell, 2005; Maji et al., 2008, etc.),
and is usually characterized as having an associated RKHS that is a subspace of the function
space L2 (i.e., it is infinite-dimensional in the sense of having a feature space of continuous
functions, as opposed to the infinite-dimensional infinite series of the EDP and RBF kernels);
5.	a quadratic polynomial kernel, K(q, k) = (1∕√dqTk + Y)2.
Full implementation details are provided in Appendix D.
Results for machine translation are presented in Table 1. Several results stand out. First, the
exponentiated dot product, RBF, and exponentiated intersection kernels, which are said to have
infinite-dimensional feature spaces, indeed do perform better than kernels with lower-dimensional
feature maps such as the quadratic kernel. In fact, the RBF and EDP kernels perform about the same,
suggesting that a deep Transformer may not need the translation-invariance that makes the RBF
8
Under review as a conference paper at ICLR 2021
Table 2: Test accuracies for Transformers with various kernels on sentiment classification. Values are
mean ± std. dev over 5 training runs with different random seeds.
	EDP	RBF	L2 Distance	EI	Quadratic
SST-2 (%)	76.70 ± 0.36	74.24 ± 0.39	76.78 ± 0.67	74.90 ± 1.32	76.24 ± 0.65
SST-5(%)	39.44 ± 0.47	39.04 ± 0.62	39.44 ± 1.33	37.74 ± 0.48	39.34 ± 0.80
EDP = Exponentiated dot product; EI = Exponentiated intersection kernel
kernel preferred to the EDP in classic kernel machines. Intriguingly, the (unorthodox) exponentiated
intersection kernel performs about the same as the two than the EDP and RBF kernels on IWSLT14
DE-EN, but slightly worse on WMT14 EN-FR. As mentioned, the EDP and RBF kernels have
feature spaces of infinite series, while the intersection kernel corresponds to a feature space of
continuous functions. On both datasets, the quadratic kernel performs slightly worse than the best
infinite-dimensional kernel, while the L2 distance performs significantly worse.
Results for sentiment classification appear in Table 2. Unlike the machine translation experiments, the
infinite-dimensional kernels do not appear strictly superior to the finite-dimensional ones on this task.
In fact, the apparent loser here is the exponentiated intersection kernel, while the L2 distance, which
performed the worst on machine translation, is within a standard deviation of the top-performing
kernel. Notably, however, the variance of test accuracies on sentiment classification means that
it is impossible to select a statistically significant “best” on this task. It is possible that the small
inter-kernel variation relates to the relative simplicity of this problem (and relative smallness of the
dataset) vs. machine translation: perhaps an infinite-dimensional feature space is not needed to obtain
Transformer-level performance on this learning problem.
It is worth noting that the exponentiated dot product kernel (again, the standard Transformer kernel)
is a consistent high performer. This may be experimental evidence for the practical usefulness of the
universal approximation property they enjoy (c.f. Theorem 2).
The relatively small yet statistically significant performance differences between kernels is reminiscent
of the same phenomenon with activation functions (ReLU, ELU, etc.) for neural nets. Moreover, the
wide inter-kernel differences in performance for machine translation, compared against the much
smaller performance differences on the SST sentiment analysis tasks, demonstrates an opportunity
for future study on this apparent task- and dataset-dependency. As a whole, these results suggest that
kernel choice may be an additional design parameter for Transformer networks.
7	Conclusion
In this paper, we drew connections between classic kernel methods and the state-of-the-art Trans-
former networks. Beyond the theoretical interest in developing new RKBS representer theorems and
other kernel theory, we gained new insight into what may make Transformers work. Our experimental
results suggest that the infinite dimensionality of the Transformer kernel makes it a good choice in
application, similar to how the RBF kernel is the standard choice for e.g. SVMs. Our work also
reveals new avenues for Transformer research. For example, our experimental results suggest that
choice of Transformer kernel acts as a similar design choice as activation functions in neural net
design. Among the new open research questions are (1) whether the exponentiated dot-product
should be always preferred, or if different kernels are better for different tasks (c.f. how GELUs have
recently become very popular as replacements for ReLUs in Transformers), (2) any relation between
vector-valued kernels used for structured prediction (Alvarez et al., 2012) and, e.g., multiple attention
heads, and (3) the extension of Transformer-type deep kernel learners to non-Euclidean data (using,
e.g., graph kernels or kernels on manifolds).
References
Jacob Abernethy, Francis Bach, Theodoros Evgeniou, and Jean-Philippe Vert. A New Approach to
Collaborative Filtering: Operator Estimation with Spectral Regularization. Journal of Machine
Learning Research,10:803-826, March 2009.
9
Under review as a conference paper at ICLR 2021
N. I. Akhiezer and M. G. Krein. Some Questons in the Theory of Moments. Number 2 in Translations
of Mathematical Monographs. American Mathematical Society, Providence, RI, 1962. ISBN
0-8128-1552-0.
Ibrahim Alabdulmohsin, Xin Gao, and Xiangliang Zhang. Support vector machines with indefinite
kernels. In Proceedings ofthe Sixth Asian Conference on Machine Learning, volume 39, pp. 32-47.
PMLR, 2015.
Daniel Alpay. Some Remarks on Reproducing Kernel Krein Spaces. Rocky Mountain Journal of Math-
ematics, 21(4):1189-1205, December 1991. ISSN 0035-7596. doi:10.1216/rmjm/1181072903.
Mauricio A. Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for Vector-Valued Functions:
A Review. Foundations and Trends® in Machine Learning, 4(3):195-266, 2012. ISSN 1935-8237,
1935-8245. doi:10.1561/2200000036.
Andreas Argyriou, Charles A Micchelli, and Massimiliano Pontil. When Is There a Representer
Theorem? Vector Versus Matrix Regularizers. Journal of Machine Learning Research, 10:2507-
2529, November 2009.
Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro. Dropout: Explicit Forms and
Capacity Control. arXiv:2003.03397 [cs, stat], March 2020.
Jean-Gabriel Attali and Gilles Pages. Approximations of Functions by a Multilayer Perceptron: A New
Approach. Neural Networks, 10(6):1069-1081, August 1997. ISSN 08936080. doi:10.1016/S0893-
6080(97)00010-5.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly
Learning to Align and Translate. In International Conference on Learning Representations, 2015.
Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learning with similarity
functions. Machine Learning, 72(1-2):89-112, August 2008. ISSN 0885-6125, 1573-0565.
doi:10.1007/s10994-008-5059-5.
Pierre Baldi and Peter J Sadowski. Understanding Dropout. In Advances in Neural Information
Processing Systems, volume 26, pp. 2814-2822. Curran Associates, Inc., 2013.
V. Bargmann. On a Hilbert space of analytic functions and an associated integral transform part I.
Communications on Pure and Applied Mathematics, 14(3):187-214, August 1961. ISSN 00103640,
10970312. doi:10.1002/cpa.3160140303.
A. Barla, F. Odone, and A. Verri. Histogram intersection kernel for image classification.
In Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429),
volume 2, pp. III-513-16, Barcelona, Spain, 2003. IEEE. ISBN 978-0-7803-7750-9.
doi:10.1109/ICIP.2003.1247294.
Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar
Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey
Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet
Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,
deep learning, and graph networks. arXiv:1806.01261 [cs, stat], June 2018.
Garrett Birkhoff. Orthogonality in linear metric spaces. Duke Mathematical Journal, 1(2):169-172,
June 1935. ISSN 0012-7094, 1547-7398. doi:10.1215/S0012-7094-35-00115-6.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs],
June 2020.
10
Under review as a conference paper at ICLR 2021
Jianhui Chen and Jieping Ye. Training SVM with indefinite kernels. In Proceedings of the 25th
International Conference on Machine Learning -ICML ’08, pp.136-143, Helsinki, Finland, 2008.
ISBN 978-1-60558-205-4. doi:10.1145/1390156.1390174.
James A. Clarkson. Uniformly convex spaces. Transactions of the American Mathematical Society,
40(3):396-396, March 1936. ISSN 0002-9947. doi:10.1090/S0002-9947-1936-1501880-4.
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the Relationship between Self-
Attention and Convolutional Layers. In International Conference on Learning Representations,
2020.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303-314, December 1989. ISSN 1435-568X. doi:10.1007/BF02551274.
Mahlon M. Day. Reflexive Banach spaces not isomorphic to uniformly convex spaces. Bulletin of the
American Mathematical Society, 47(4):313-318, April 1941. ISSN 0002-9904. doi:10.1090/S0002-
9904-1941-07451-3.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, volume 1, pp. 4171-4186, Minneapolis, Minnesota, June 2019.
doi:10.18653/v1/N19-1423.
Ivar Ekeland and Roger T6mam. Convex Analysis and Variational Problems, volume 28 of Classics
in Applied Mathematics. Society for Industrial and Applied Mathematics, January 1999. ISBN
978-0-89871-450-0 978-1-61197-108-8. doi:10.1137/1.9781611971088.
Ivar Ekeland and Thomas Turnbull. Infinite-Dimensional Optimization and Convexity. Chicago
Lectures in Mathematics. University of Chicago Press, Chicago, 1983. ISBN 978-0-226-19987-0
978-0-226-19988-7.
Gregory E. Fasshauer, Fred J. Hickernell, and Qi Ye. Solving support vector machines in reproducing
kernel Banach spaces with positive definite functions. Applied and Computational Harmonic
Analysis, 38(1):115-139, January 2015. ISSN 1063-5203. doi:10.1016/j.acha.2014.03.007.
Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.
Neural Networks, 2(3):183-192, January 1989. ISSN 08936080. doi:10.1016/0893-6080(89)90003-
8.
Pando G. Georgiev, Luis Sdnchez-Gonzdlez, and Panos M. Pardalos. Construction of Pairs of
Reproducing Kernel Banach Spaces. In Vladimir F. Demyanov, Panos M. Pardalos, and Mikhail
Batsyn (eds.), Constructive Nonsmooth Analysis and Related Topics, pp. 39-57. Springer New
York, New York, NY, 2014. ISBN 978-1-4614-8615-2. doi:10.1007/978-1-4614-8615-2_4.
Andrej Gisbrecht and Frank-Michael Schleif. Metric and non-metric proximity transforma-
tions at linear costs. Neurocomputing, 167:643-657, November 2015. ISSN 09252312.
doi:10.1016/j.neucom.2015.04.017.
Thore Graepel, Ralf Herbrich, Peter Bollmann-Sdorra, and Klaus Obermayer. Classification on
Pairwise Proximity Data. In Advances in Neural Information Processing Systems, volume 11, pp.
438-444, 1998.
K. Grauman and T. Darrell. The pyramid match kernel: Discriminative classification with sets
of image features. In Tenth IEEE International Conference on Computer Vision (ICCV’05)
Volume 1, pp. 1458-1465 Vol. 2, Beijing, China, 2005. IEEE. ISBN 978-0-7695-2334-7.
doi:10.1109/ICCV.2005.239.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep Residual
Networks. In European Conference on Computer Vision, pp. 630-645, July 2016.
Lifang He, Chun-Ta Lu, Guixiang Ma, Shen Wang, Linlin Shen, Philip S Yu, and Ann B Ragin.
Kernelized Support Tensor Machines. In Proceedings of the 34th International Conference on
Machine Learning, volume 70, pp. 1442-1451. PMLR, July 2017.
11
Under review as a conference paper at ICLR 2021
David P. Helmbold and Philip M. Long. Surprising properties of dropout in deep networks. The
Journal of Machine Learning Research ,18(1):7284-7311,JanUary2017. ISSN 1532-4435.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
Universal approximators. Neural Networks, 2(5):359-366, JanUary 1989. ISSN 0893-6080.
doi:10.1016/0893-6080(89)90020-8.
Robert C. James. Orthogonality and Linear FUnctionals in Normed Linear Spaces. Transactions of the
American Mathematical Society, 61(2):265-292, 1947. ISSN 0002-9947. doi:10.2307/1990220.
Mandar Joshi, Danqi Chen, Yinhan LiU, Daniel S. Weld, LUke Zettlemoyer, and Omer Levy.
SpanBERT: Improving Pre-training by Representing and Predicting Spans. Transactions of
the Association for Computational Linguistics, 8:64-77, JanUary 2020. ISSN 2307-387X.
doi:10.1162/tacl_a_00300.
George Kimeldorf and Grace Wahba. Some resUlts on Tchebycheffian spline fUnctions. Jour-
nal of Mathematical Analysis and Applications, 33(1):82-95, JanUary 1971. ISSN 0022-247X.
doi:10.1016/0022-247X(71)90184-3.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
Conference on Learning Representations, 2015.
Irene Kotsia and Ioannis Patras. SUpport TUcker Machines. In CVPR 2011, pp.
633-640, Colorado Springs, CO, USA, JUne 2011. IEEE. ISBN 978-1-4577-0394-2.
doi:10.1109/CVPR.2011.5995663.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, PiyUsh Sharma, and RadU
SoricUt. ALBERT: A Lite BERT for Self-sUpervised Learning of LangUage Representations. In
International Conference on Learning Representations, 2020.
HsUan-Tien Lin and Chih-Jen Lin. A StUdy on Sigmoid Kernels for SVM and the Training of
non-PSD Kernels by SMO-type Methods. Technical report, National Taiwan University, Taipei,
Taiwan, 2003.
Rongrong Lin, Haizhang Zhang, and JUn Zhang. On ReprodUcing Kernel Banach Spaces: Generic
Definitions and Unified Framework of ConstrUctions. arXiv:1901.01002 [cs, math, stat], JanUary
2019.
Yinhan LiU, Myle Ott, Naman Goyal, Jingfei DU, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
LUke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A RobUstly Optimized BERT Pretraining
Approach. arXiv:1907.11692 [cs], JUly 2019.
Gaelle Loosli, StePhane Canu, and Cheng Soon Ong. Learning SVM in Kreln Spaces. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 38(6):1204-1216, JUne 2016. ISSN
1939-3539. doi:10.1109/TPAMI.2015.2477830.
Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu.
Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View.
arXiv:1906.02762 [cs, stat], June 2019.
Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective Approaches to Attention-
based Neural Machine Translation. arXiv:1508.04025 [cs], August 2015.
Ronny Luss and Alexandre D’aspremont. Support Vector Machine Classification with Indefinite
Kernels. In Advances in Neural Information Processing Systems, volume 20, pp. 953-960, 2008.
Subhransu Maji, Alexander C. Berg, and Jitendra Malik. Classification using intersection kernel
support vector machines is efficient. In 2008 IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 1-8, Anchorage, AK, USA, June 2008. IEEE. ISBN 978-1-4244-2242-5.
doi:10.1109/CVPR.2008.4587630.
Robert E. Megginson. An Introduction to Banach Space Theory, volume 183 of Graduate Texts in
Mathematics. Springer New York, New York, NY, 1998. ISBN 978-1-4612-6835-2 978-1-4612-
0603-3. doi:10.1007/978-1-4612-0603-3.
12
Under review as a conference paper at ICLR 2021
Charles A. Micchelli, Massimiliano Pontil, John Shawe-Taylor, and Yoram Singer. A Function
Representation for Learning in Banach Spaces. In Learning Theory, volume 3120, pp. 255-269.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-22282-8 978-3-540-27819-
1. doi:10.1007/978-3-540-27819-1_18.
Dino Oglic and Thomas Gartner. Learning in Reproducing Kernel Kreln Spaces. In Proceedings of
the 35th International Conference on Machine Learning, volume 80, pp. 3859-3867. PMLR, 2018.
Dino Oglic and Thomas Gartner. Scalable Learning in Reproducing Kernel Krein Spaces. In
Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 4912-
4921. PMLR, 2019.
Akifumi Okuno, Tetsuya Hada, and Hidetoshi Shimodaira. A probabilistic framework for multi-view
feature learning with many-to-many associations via neural networks. In Proceedings of the 35th
International Conference on Machine Learning, volume 80, pp. 3888-3897. PMLR, June 2018.
Cheng Soon Ong, Xavier Mary, Stephane Canu, and Alexander J. Smola. Learning with non-positive
kernels. In Proceedings of the 21st International Conference on Machine Learning, pp. 639-646,
Banff, Alberta, Canada, 2004. ACM Press. doi:10.1145/1015330.1015443.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. Fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
NAACL-HLT 2019: Demonstrations, 2019. doi:10.18653/v1/N19-4009.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In Advances in Neural Information Processing Systems, volume 32, pp.
8026-8037. Curran Associates, Inc., 2019.
Matt Post. A Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pp. 186-191, Belgium, Brussels, 2018. Association for
Computational Linguistics. doi:10.18653/v1/W18-6319.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
Models are Unsupervised Multitask Learners. Technical report, OpenAI, 2018.
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon
Shlens. Stand-Alone Self-Attention in Vision Models. arXiv:1906.05909 [cs], June 2019.
V. Roth, J. Laub, M. Kawanabe, and J.M. Buhmann. Optimal cluster preserving embedding of
nonmetric proximity data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25
(12):1540-1551, December 2003. ISSN 1939-3539. doi:10.1109/TPAMI.2003.1251147.
Frank-Michael Schleif and Peter Tino. Indefinite Core Vector Machine. Pattern Recognition, 71:
187-195, November 2017. ISSN 00313203. doi:10.1016/j.patcog.2017.06.003.
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. The MIT Press, 2002. ISBN 978-0-262-25693-3.
doi:10.7551/mitpress/4175.001.0001.
Bernhard Scholkopf, Ralf Herbrich, and Alex J. Smola. A Generalized Representer Theorem. In
David Helmbold and Bob Williamson (eds.), Computational Learning Theory, Lecture Notes in
Computer Science, pp. 416-426, Berlin, Heidelberg, 2001. Springer. ISBN 978-3-540-44581-4.
doi:10.1007/3-540-44581-1_27.
Laurent Schwartz. Sous-espaces hilbertiens d’espaces vectoriels topologiques et noyaux as-
SOcieS (Noyaux reproduisants). Journal d'Analyse Mathematique, 13(1):115-256, 1964.
doi:10.1007/BF02786620.
13
Under review as a conference paper at ICLR 2021
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words with
Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016. Association
for Computational Linguistics. doi:10.18653/v1/P16-1162.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pp. 1631-1642, 2013.
Guohui Song, Haizhang Zhang, and Fred J. Hickernell. Reproducing kernel Banach spaces with
the l1 norm. Applied and Computational Harmonic Analysis, 34(1):96-116, January 2013. ISSN
1063-5203. doi:10.1016/j.acha.2012.03.009.
I. Steinwart, D. Hush, and C. Scovel. An Explicit Description of the Reproducing Kernel Hilbert
Spaces of Gaussian RBF Kernels. IEEE Transactions on Information Theory, 52(10):4635-4643,
October 2006. ISSN 0018-9448. doi:10.1109/TIT.2006.881713.
Ingo Steinwart and Andreas Christmann. Support Vector Machines. Information Science and
Statistics. Springer New York, New York, NY, 2008. ISBN 978-0-387-77241-7 978-0-387-77242-
4. doi:10.1007/978-0-387-77242-4.
Dacheng Tao, Xuelong Li, Weiming Hu, Stephen Maybank, and Xindong Wu. Supervised Tensor
Learning. In Fifth IEEE International Conference on Data Mining (ICDM’05), pp. 450-457,
Houston, TX, USA, 2005. IEEE. ISBN 978-0-7695-2278-4. doi:10.1109/ICDM.2005.139.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural Information
Processing Systems, volume 30, pp. 5998-6008, 2017.
Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Eukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and
Jakob Uszkoreit. Tensor2Tensor for Neural Machine Translation. arXiv:1803.07416 [cs, stat],
March 2018.
Petar VeliCkoviC, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lid, and Yoshua
Bengio. Graph Attention Networks. In International Conference on Learning Representations,
2018.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout Training as Adaptive Regularization. In
Advances in Neural Information Processing Systems, volume 26, pp. 351-359. Curran Associates,
Inc., 2013.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:
A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Interna-
tional Conference on Learning Representations, 2019a.
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.
Learning Deep Transformer Models for Machine Translation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 1810-1822, Florence, Italy, July
2019b. Association for Computational Linguistics. doi:10.18653/v1/P19-1176.
Christopher K. I. Williams and Matthias Seeger. Using the Nystrom Method to Speed Up Kernel
Machines. In Advances in Neural Information Processing Systems, volume 13, pp. 682-688. MIT
Press, 2001.
Gang Wu, Edward Y Chang, and Zhihua Zhang. An Analysis of Transformation on Non-Positive
Semidefinite Similarity Matrix for Kernel Machines. Technical report, University of California,
Santa Barbara, June 2005.
Wei Wu, Jun Xu, Hang Li, and Satoshi Oyama. Asymmetric Kernel Learning. Technical report,
Microsoft Research, June 2010.
14
Under review as a conference paper at ICLR 2021
Yuesheng Xu and Qi Ye. Generalized Mercer Kernels and Reproducing Kernel Banach Spaces,
volume 258 of Memoirs of the American Mathematical Society. American Mathemati-
cal Society, March 2019. ISBN 978-1-4704-3550-9 978-1-4704-5077-9 978-1-4704-5078-6.
doi:10.1090/memo/1243.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.
XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv:1906.08237
[cs], June 2019.
Haizhang Zhang and Jun Zhang. Regularized learning in Banach spaces as an optimization problem:
RePresenter theorems. Journal of Global Optimization, 54(2):235-250, October 20l2. ISSN
1573-2916. doi:10.1007/s10898-010-9575-z.
Haizhang Zhang, Yuesheng Xu, and Jun Zhang. ReProducing kernel Banach sPaces for machine
learning. Journal of Machine Learning Research, 10:2741-2775, December 2009.
15
Under review as a conference paper at ICLR 2021
A Deep neural networks lead to Banach space analysis
Examining the kernel learning problem (11), it may not immediately clear why the reproducing
spaces on X and Y need be Banach spaces rather than Hilbert spaces. Suppose for example that we
have two RKHS’s HX and HY on X and Y, respectively. Then, we can take their tensor product
HX 名 HY as an RKHS on X ×Y, with associated reproducing kernel K χ×γ (x 1 0 y 1 ,x 2 0 y 2)=
κX(x1, x2)κY(y1, y2), where κX and κY are the reproducing kernels of HX and HY, respectively,
and x1 0 y1, x2 0 y2 ∈ X 0 Y. The solutions to a regularized kernel problem like (11) would then
be drawn from HX and HY . This setup is similar to those studied in, e.g., Abernethy et al. (2009);
He et al. (2017).
In a shallow kernel learner like an SVM, the function in the RKHS can be characterized via its norm.
Representer theorems allow for the norm of the function in the Hilbert space to be calculated from
the scalar coefficients that make up the solution. On the other hand, for a Transformer layer in a
multilayer neural network, regularization is usually not done via norm penalty as shown in (11). In
most applications, regularization is done via dropout on the attention weights aij as well as via the
implicit regularization obtained from subsampling the dataset during iterations of stochastic gradient
descent. While dropout has been characterized as a form of weight decay (i.e., a variant of p-norm
penalization) for linear models (Baldi & Sadowski, 2013; Wager et al., 2013, etc.), recent work has
shown that dropout induces a more complex regularization effect in deep networks (Helmbold &
Long, 2017; Arora et al., 2020, etc.). Thus, it is difficult to characterize the norm of the vector spaces
we are traversing when solving the general problem (11) in the context of a deep network. This can
lead to ambiguity as to whether the norm being regularized as we traverse the solution space is a
Hilbert space norm. If f and g are infinite series or Lp functions, for example, their resident space is
only a Hilbert space if the associated norm is the P or L2 norm. This motivates the generalization
of kernel learning theorems to the general Banach space setting when in the context of deep neural
networks.
B Proof of Theorem 1
B.1	Preliminiaries
To prove this theorem, we first need some results and definitions regarding various properties of
Banach spaces (Megginson, 1998). These preliminaries draw from Xu & Ye (2019) and Lin et al.
(2019).
Two metric spaces (M, dM) and (N, dN ) are said to be isometrically isomorphic if there exists
a bijective mapping T : M → N, called an isometric isomorphism, such that for all m ∈ M,
dN (T (m)) = dM(m) (Megginson, 1998, Definition 1.4.13).
The dual space of a vector space V over a field F, which we will denote V*, is the space of all
continuous linear functionals on V, i.e.,
V * = {g : V → F ,g linear and continuous}.	(B.1)
A normed vector space V is reflexive if it is isometrically isomorphic to V**, the dual space of its
dual space (a.k.a. its double dual).
For a normed vector space V, the dual bilinear product, which we will denote ',∙>v (i.e., with only
one subscript, as opposed to e.g.,《,・底 ×By), is defined on V and V * as
〈f,g〉v，g (f) for f ∈V ,g ∈V *.
Given a normed vector space V and its dual space V*, letU ⊆ V and W ⊆ V*. The annihilator of
U in V* and the annihilator of W in V, denoted U⊥ and ⊥W respectively, are (Megginson, 1998,
Definition 1.10.14)
U ⊥ = {g ∈ V * :( f, g〉v = 0 ∀ f ∈U}
⊥W = {f ∈ V :〈f,g〉V = 0 ∀g ∈W}.
16
Under review as a conference paper at ICLR 2021
A normed vector space V is called strictly convex if ^tv 1 + (1 - t)V2 IlV < 1 whenever IlvIIIV =
Il v 2 ∣∣v = 1, 0 < t < 1, where v 1 ,v 2 ∈ V and ∣∣ ∙ IlV denotes the norm of V (Megginson, 1998,
Definition 5.1.1; citing Clarkson, 1936 and Akhiezer & Krein, 1962).
A nonempty subset A of a metric space (M, dM) is called a Chebyshev set if, for every element
m ∈ M, there is exactly one element c ∈ A such that dM(m, c) = dM(m, A) (Megginson, 1998,
Definition 5.1.17) (where recall the distance between a point m and a set A in a metric space is
equal to infc∈A dM(m, c)). If a normed vector space V is reflexive and strictly convex, then every
nonempty closed convex subset of V is a Chebyshev set (Megginson, 1998, Corollary 5.1.19; citing
Day, 1941).
For a normed vector space V and v,w ∈ V, the Gateaux derivative of the norm (Megginson, 1998,
Definition 5.4.15) at v in the direction of w is defined as
lim HV + twHv - HVHv
t —→0	t
If the Gateaux derivative of the norm at v in the direction of w exists for all w ∈ V, then ∣∣ ∙ ∣∣v is
said to be Gateaux differentiable at v. A normed vector space V is called Gateaux differentiable or
smooth if its norm is Gateaux differentiable at all v ∈ V (Megginson, 1998, Corollary 5.4.18).
The smoothness of a normed vector space V implies that, if we define a “norm operator” ρ on V,
P(v)，Hv∣∣v, then for each v ∈ V \ {0}, there exists a continuous linear functional dGp(v) on V
such that (Xu & Ye, 2019, p. 24)
(w, dGρ (V )〉V = lim H V + tw HV -H V HV for all W ∈V.
t→0	t
Since the Gateaux derivative of the norm is undefined at 0, following Xu & Ye (2019, Equation 2.16);
Lin et al. (2019, p. 20); etc., we define a regularized Gateaux derivative of the norm operator on V,
ι(v)，ʃdGP(v) When V = 0	(b.2)
0 when v = 0
for v ∈ V .
Given two vector spaces V and W defined over a field F, the direct sum, denoted V ㊉ W, is the
vector space with elements (v, w) ∈V ㊉W for v ∈ V, w ∈ W with the additional structure
(v1 , w1 ) + (v2 , w2 ) = (v1 + v2 , w1 + w2 )	for v1 , v2 ∈ V , w1 , w2 ∈ W
c(v, w) = (cv, cw)	for v ∈ V, w ∈ W, c ∈ F.
If V and W are normed vector spaces with norms H ∙ H v and H ∙ HW, respectively, then we will say
V ㊉ W has the norm
H V, WHV㊉W , H V Hv + H WHW for V ∈ V ,w ∈ W.	(B.3)
Megginson (1998, Definition 1.8.1) calls (B.3) a “1-norm” direct sum norm, but notes that other
norm-equivalent direct sum norms such as a 2-norm and infinity-norm are possible. Some other
useful facts about direct sums are:
•	if V and W are both strictly convex, then V ㊉ W is strictly convex (Megginson, 1998,
Theorem 5.1.23);
•	if V and W are both reflexive, then V㊉ W isreflexive (Megginson, 1998, Corollary 1.11.20);
•	and if V and W are both smooth, then V ㊉ W is smooth (Megginson, 1998, Theorem
5.4.22).
An element v of a normed vector space V is said to be orthogonal (or Birkhoff-James orthogonal) to
another element w ∈ V if (Birkhoff, 1935; James, 1947)
Hv + twHv ≥ HvHv for all t ∈ R.
If W ⊆ V, then we say v ∈ V is orthogonal to W if v is orthogonal to all w ∈ W .
Now we can state a lemma regarding orthogonality in RKBS’s.
Lemma 1 (Xu & Ye, 2019, Lemma 2.21). If the RKBS B is smooth, then f ∈ B is orthogonal to
g ∈ B if and only if (g, ι (f ))b = 0, where (,∙)b means the dual bilinear product as given in (B.1)
and ι is the regularized Gdteaux derivative from (B.2). Also, an f ∈ B \ {0} is orthogonal to a
subspace N ⊆ B ifand only if (h, ι(f 1 B = 0 for all h ∈ N.
17
Under review as a conference paper at ICLR 2021
B.2	Minimum-norm interpolation (optimal recovery)
Following Fasshauer et al. (2015); Xu & Ye (2019); Lin et al. (2019), we first prove a representer
theorem for a simpler problem - that of perfect interpolation while minimizing the norm of the
solution - before proceeding to the representer theorem for the empirical risk minimization problem
(11) in the next section.
Definition 4 (Minimum-norm interpolation in a pair of RKBS’s). Let X and Y be nonempty sets, and
BX and BY RKBS's on X and Y, respectively. Let 卜,∙)Bχ ×Bγ : BX ×Bγ → R be a bilinear mapping
on the two RKBS,s, Φχ : X → FX and Φγ : Y → FY. Let K(x%, y%) = (Φχ(x), Φγ(y)* =
(fΦγy), gΦχ(Xi)〉Bx ×Bγ be a reproducing kernel of X and Y satisfying Definitions 1 and 2. Say
{x1, . . . ,xnx},xi ∈ X, {y1, . . . ,yny},yi ∈ Y, and {zij}i=1,...,nx;j=1,...,ny, zij ∈ R is a finite
dataset where a response zij is defined for every (i, j) pair of an xi and a yj. The minimum-norm
interpolation problem is
f*,g* = argmin Ilf∣∣Bχ + IlgIlBγ
f ∈BX,y∈BY	(B.4)
such that (f, g) ∈ NX,Y,Z
where
NX, Y, Z = {(f,g) ∈ BX ㊉ BY s∙t. fΦγ (yj) ,g Φχ (Xi) Bx× ×by = Zij ∀ i,j }.	(B.5)
To discuss the solution of (B.4), we first need to establish the condition for the existence of a solution.
The following is a generalization of a result from Section 2.6 of Xu & Ye (2019).
Lemma 2. If the Set {κ(Xi, ∙)}n= ι is IinearIy independent in BY, the Set {K(∙, yj)}j= 1 is linearly
independent in BX, and the bilinear mapping 卜,∙)ba,×b-v : BX × By → R is nondegenerate, then
NX,Y,Z (B.5) is nonempty.
Proof. From the definition of K (3) and the bilinearity of《,∙)Bx×By, we can write that
nx
d，工CiK(xi, ∙))κ K
BX ×BY
i=1
nx	nx
£c( f,κ ( xi, •)) BX ×Bγ = ECif( Xi )
i=1	i=1
for all f ∈ BX
for Ci ∈ R, and that
ny	ny	ny
(工CjK(∙,yj),g)B	B =工与〈K(∙,yj),ggBχ×Bγ =工Cjg(yj) forallg ∈ by
j=1	BX ×BY	j=1	j=1
for Cj ∈ R. This means that
nx	nx
工 CiK(Xi, ∙) = 0 if and only if 工 Ci f (Xi) = 0 for all f ∈ Bx
and
ny	ny
CjK(Cj K (∙, yj) = 0 if and only if 工与 g (yj) = 0 for all g ∈ Bγ.
This shows that linear independence of {K(xi, ∙)}；= 1 and {K(∙, yj)}j= 1 imply linear independence
of {f(Xi)}in=x1 and {g (yj)}jn=y 1 , respectively. Then, considering the nondegeneracy of the bilinear
mapping (,)BX ×b>, we can say that
ny	nx
j(c3K (∙ ,yj) ,£CiK (xi
j=1	i=1
0
BX ×BY
nx
工 Cif (Xi) = 0 for all f ∈ Bχ
i=1
ny
or 工 Cj g (yj) = 0 forall g ∈ Bγ
j=1
From this, we can see that linear independence of {K(Xi, ∙)}；= 1 and {K(∙, yj)}j= 1, and the nonde-
generacy of(•, ∙)Bx×By ensure the existence of at least one (f *, g *) pair in NX, γ, Z.	□
18
Under review as a conference paper at ICLR 2021
Now we can prove a lemma characterizing the solution to (B.4).
Lemma 3. Consider the minimum-norm interpolation problem from Definition 4. Assume that BX
and BY are smooth, strictly convex, and reflexive,2 and that {K(xi, ∙)}：= 1 and {K(∙,yj)}j= 1 are
linearly independent. Then, (B.4) has a unique solution pair (f * ,g*), with the property that
nx	ny
ι(f*) =工ξiκ(xi, ∙)	ι(g*) =工ZjK(∙,yj).
i=1	j=1
where ι(∙) is the regularized Gdteaux derivative as defined in (B.2), and where ξi, Zj ∈ R.
Proof. The existence of a solution pair is given by the linear independence of {K(x%, ∙)}鲁 1 and
{κ(∙, yj)}n= ι and Lemma 2. Uniqueness of the solution will be shown by showing that NX,γ,Z is
closed and convex and a subset of a strictly convex and reflexive set, which ensures it is a Chebyshev
set.
Since BX and BY are strictly convex and reflexive, their direct sum BX ㊉ BY is strictly convex and
reflexive, as we noted in section B.1.
Now We analyze NX,γ,Z. We first show convexity. Pick any (f, g),(f, g") ∈ NX,γ,Z and t ∈ (0, 1).
Then note that for any (xi, yj, zi,j),
tfΦY (yj) , tgΦX (xi)
= t fΦY (yj) , gΦX (xi)
= tzi,j + (1 - t)zi,j
BX ×BY
BX ×BY
+《1 - t)fΦY (yj), (1 - t)gΦX (Xi )∖
+ (1 - NφY(yj),gφx((X)Mx×Bγ
BX ×BY
= zi,j
thus showing thatNX,Y,Z is convex. Closedness may be shown by the strict convexity of its superset
BX ㊉ BY and the continuity of《,∙)b1×b2. Thus, the closed and convex NX,γ,Z ⊆ Bx ㊉ BY is a
Chebyshev set, implying a unique (f*, g*) ∈ NX,Y,Z with
Ilf * ,g *Hbx eBY = .# ,min	Ilf ^Bx + Il g ^By ∙
(f,g)∈NX,Y,Z
Now we characterize this solution (f*, g*). Similar to proofs of the classic RKHS representer
theorem (Scholkopf et al., 2001) and those of earlier RKBS representer theorems (XU & Ye, 2019;
Lin et al., 2019, etc.), we approach this via orthogonal decomposition. Consider the following set of
function pairs (f, g) that map all data pairs (xi, yj) to 0:
NX, Y, 0 = {(f,g) ∈ BX ㊉ BY :	fΦΦy (yj), g Φχ (Xi)〉Bx ×By = 0;
i = 1, . . . , nx;	j = 1, . . . , ny
We can see thatNX,Y,0 is closed under addition and scalar multiplication, making it a subspace of
Bx ㊉BY.	一
Taking our optimal (f*, g*), we can see that
I( Φ * ,g *) + ( Φ 0 ,g 0)∣Bχ 出BY ≥ 11( f * ,g *)lBχ ㊉BY for any (Φ0, g0) ∈ NX,Y,0	(B.7)
thus showing that (Φ*, g*) is orthogonal to the subspace NX,Y,0.
Consider the left and right preimages of NX, y , o under《,∙)Bx×By ：
〈•, g〉-x ×By	[NX,y,0]	=	{ f	∈ BX :	〈 fΦy(yj), gΦχ (Xi)〉Bx ×By	= 0;
i	= 1,.. ., nχ; j = 1 ,...,ny }; g	∈ BY
(f, ∙Bχ ×By	[NX,Y,0]	=	{ f	∈ BX :	〈 f Φy(yj), gΦχ (X)〉Bx ×By	= 0;
i	= 1, . . . ,nX; j = 1, . . .,ny ; Φ	∈ BX.
2As Fasshauer et al. (2015) note, any Hilbert space is strictly convex and smooth, so it seems reasonable to
assume that an RKBS is also strictly convex and smooth.
19
Under review as a conference paper at ICLR 2021
Since 卜,g)-；×by [Nx,γ,o] ⊆ BX and(f, ∙>-X×by [NX,γ,o] ⊆ BY, We can consider them as
normed vector spaces with norms ∣∣ ∙ IlBX and ∣∣ ∙ IlBY, respectively. From (B.7) and the definition of
the direct sum norm (B.3),
Ilf*+	f0∣Bx ≥ Ilf*∣Bx	fOrall f0 ∈〈• ,g〉-X×BY [NX,Y,0], fOrarbitrary g ∈ Bγ	(B.8a)
Ilg*	+	g0IBY ≥ Ilg*∣BY	for all g0 ∈〈f, •)-；XBY [NX,γ,0], for arbitrary f ∈ Bχ.	(B.8b)
We can then use (B.8) and Lemma 1 to say
(f, ∣(f *))Bχ = 0	for all f ∈{,g〉-；XBY [NX,Y,0],	for arbitrary g ∈ BY
(g, ∣(g*)〉BY = 0	for all g ∈〈f, •〉-； XBY [NX,Y,o],	for arbitrary f e Bχ
which means
∣(f*) ∈ (〈• ,g)-X×By [NX,Y,0])⊥ forall g ∈ By	(B.9a)
∣(g*) ∈ (〈f,)BXXBY [NX,γ,0])⊥ forall f ∈Bχ.	(B.9b)
From (B.9a) and (3a)-(3b),
f* ∈ U(• ,g〉-XXBYNX,Y,0] = {f ∈ BX :〈fΦy(yj),gΦχ(Xi)〉BXXBy = 0; g ∈ BY;
g ∈BY	i = 1,...,n; j = 1,…，ny }
={ f ∈BX :〈 fφγ (yj ) ,h hBχ XBγ =0
h ∈ span {K(Xi, ∙);	i = 1,..., nχ} ;
j = 1 ,...,ny }
=⊥ span {κ(Xi, ∙); i = 1,..., n} .	(B.10)
And from (B.9b) and (3c)-(3d),
g *∈	U(f, ,〉-； XBγ	[NX, Y, 0]	=	{ g	∈	BY	:〈 fΦγ (yj) ,g Φχ (Xi )〉BX XBγ	= 0; f ∈	BX;
f∈BX
i = 1, . . . , nX; j = 1, . . . , ny
={g ∈BY :〈hh,fΦγ(yj)〉BXXBγ =0
h h ∈ span {K (∙ ,yj); j = 1 ,...,ny } ;
i = 1,... , nx }
=⊥ span {K (∙, yj);	y = 1 ,...,ny } .	(B.11)
Combining (B.9a) and (B.10), we get
I (f *) ∈ (⊥ span{ K (Xi, ∙);	i = 1 ,...,n })⊥	= span{ K (Xi, ∙);	i =	1 ,...,n }	(B.12)
and by combining (B.9b) and (B.11), we get
I (g *) ∈ (⊥ span{ K (∙, yj);	j = 1 ,...,ny })⊥	= span{ K (∙ ,yj);	j =	1 ,...,ny}	(B.13)
where in both (B.12) and (B.13) we use Proposition 2.6.6 of Megginson (1998) regarding properties
of annihilators.
From (B.12) and (B.13) we can write that there exist two sets of parameters ξ1, . . . , ξnx ∈ R and
ζ1 , . . . , ζny ∈ R such that
nx
ι(f *) =工6k(xi, ∙)
i=1
ny
thus proving the claim.
□
20
Under review as a conference paper at ICLR 2021
B.3 Main Proof
Before beginning the proof, we state the following lemma regarding the existence of solutions of
convex optimization problems on Banach spaces:
Lemma 4 (Ekeland & Temam, 1999, Chapter II, Proposition 1.2). Let B be a reflexive Banach space
and S a closed, convex, and bounded (with respect to ∣∣ ∙ IlB) subset of B. Let f : S → R ∪ {+∞} be
a convex function with a closed epigraph (i.e., it satisfies the condition that ∀c ∈ R ∪ {+∞}, the set
{v ∈ S : f(v) ≤ c} is closed). Then, the optimization problem
inf f(v)
v∈S
has at least one solution.
Xu & Ye (2019) and Lin et al. (2019) also reference Ekeland & Turnbull (1983) as a source for
Lemma 4.
We now restate Theorem 1 with the conditions on BX and BY filled in.
Theorem 1, Revisited. Suppose we have a kernel learning problem of the form in (11). Let κ :
X × Y → R, K (Xi ,yi) = ®X (Xi), Φγ (yi )〉FX XFY =〈 fΦγ (y) ,g Φχ (QIBX XBY be a reproducing
kernel satisfying Definitions 1 and 2. Assume that {K(xi, ∙)}：= 1 is linearly independent in BY and
that {κ(∙, yj)}j= ι is linearly independent in BX. Assume also that BX and BY are reflexive, strictly
convex, and smooth. Then, the regularized empirical risk minimization problem (11) has a unique
solution pair (f * ,g*), with the property that
nx	ny
ι(f*) =工ξiκ(xi, ∙)	ι(g*) =工ZjK(∙,yj).
i=1	j=1
where ξi , ζj ∈ R.
Proof. As before, we begin by proving existence and uniqueness of the solution pair (f*, g*).
We first prove uniqueness using some basic facts about convexity. Assume that there exist two distinct
minimizers (f*,g*), (f*, g*) ∈ Bχ ㊉ BY. Define (f*,g*) = 1/2[(f*,g*) + (f*,g*)]. Then, since
BX and BY are strictly convex, we have
If*∣BX
Ig*∣BY
1( f * + f*)
2	BX
2(g * + g 2)
2	BY
<	11 f *∣BX + 21 f*∣BX
<	2 Ilg *lBY + 2 Ilg ;lBY
and since RX and RY are convex and strictly increasing,
Rχ(If*Ibx) = Rχ ŋl2(f* + f*)	) <Rχ (11f*Ibx + 11f⅛x)
≤ 1R χ (I f *Ibx +1R χ (I f*Ibx)
and
Ry(Ig3Iby) = Ry( 2(g*+ g2)爵)<Ry (11gMBY + 11g*∣BY)
≤ 2Ry(Ig*Iby + 1Ry(Ig*Iby).
Consider the regularized empirical risk minimization cost function (11)
T (f, g) = L( f, g) + λ X R X (I f Ibx ) + λ Y R Y (I g Iby )
where we use the shorthand
L( f, g) = fn rj ^2 L (xi, yj, zij, f f φY (y), gφx (ABX ×Bγ).
nxny	Y
i,j
21
Under review as a conference paper at ICLR 2021
We have that RX(∣∣ ∙ IlBX) and RY(∣∣ ∙ IlBY) are both convex via identities about composition of
convex functions. The function L(f, g) is also convex since all the functions in the summand are
convex in f and g .
Then, since we have assumed that T(f1,g；) = T(f2, gg), by plugging in some of the above
inequalities we can write
T (f3 ,g 3) = T ɑ[( f: ,g ；) + (f2 ,g 2))
L Q[(f3,g2) + (f3,g3))
+ RX
(12(f3+f3)Ibx)
+ RY
(l 2( g 2 + g 3)|BY)
< 1L(f3,g2) +1L(f3,g2) +1RX (If3Ibx) +1RX (If3底)
+ 2RY (If3∣Bγ) + 1 RY (If3∣Bγ)
=1T (f 3 ,g 2) + 1 T (f2,g 3)
T(f13,g13)
contradicting that (f13, g13) is a minimizer, and thus showing uniqueness of the solution.
We now prove existence via Lemma 4. We already know that T(∙) is convex. From the bilinearity of
(,∙)Bx×By and the convexity of L, L is continuous in f and g. Since the regularization functions R X
and RY are convex and strictly increasing, is follows that the functions RX(∣f ∣∣Bχ) and RY(∣g∣∣Bγ)
are continuous in f and g, respectively. Thus, T(f, g) is continuous. Consider the set
E = {(f, g) ∈Bx ㊉ BY ： T(f, g) ≤ T(0,0)}∙
The set E is nonempty (it contains at least (0, 0)), and we can see that
Il f,g Il Bx eBY = Ilf IBX + 1 g Il By
≤ RX-1(T (f, 0)) + RY-1(T (0, g))
showing that E is bounded. So, by Lemma 4, we are guaranteed the existence of an optimal solution
(f3,g3).
Pick any (f, g) ∈ BX × BY and consider the set
Df,g = { (Xi,yj,〈 fφφY (yj) ,g Φx (Xi ))Bx ×Bγ) : i = 1 ,...,nx;	j = 1,...,ny}
i.e., the set of pairs of points (xi ,yj) along with the value that the function pair (f, g) maps to via the
bilinear form at the pair of points (xi,yj).
From Lemma 3, there exists an element (f!, gz) ∈ Bx × By such that (f/, gz) interpolates Df,g
perfectly, i.e.,
fΦY(yj),	gΦX (xi)	BX ×BY	=	fΦY(yj), gΦX (xi)	BX ×BY ;	i = 1, . . . ,	nx;	j =	1, . . . , ny
whose Gateaux derivatives of norms satisfy
∣ (f) ∈ span{ K (Xi, ∙);	i = 1 ,...,nx }
∣ (g') ∈ span{ κ (∙ ,yj); j = 1,...,ny }.
Further, this element (f, gz) obtains the minimum-norm interpolation of Df,g, i.e.,
Ilf , g Il Bχ㊉BY ≤ Ilf, g!bx㊉BY .
This last fact implies
τ(f,Q ≤ τ(f,g).
22
Under review as a conference paper at ICLR 2021
Therefore, the unique optimal solution (f * ,g *) also satisfies
I(f *) ∈ span{K(Xi, •); i = 1,..., nχ}
I (g *) ∈ span{ K (∙ ,yj); j = 1 ,...,ny }
which implies that suitable parameters ξ1, . . . , ξnx ∈ R and ζ1, . . . , ζny ∈ R exist such that
nx	ny
ι (f *) =工 ξiκ (xi, ∙)	ι (g *) =工 ZjK (∙ ,yj)
i=1	j=1
proving the claim.	□
C Proof of Theorem 2
First, we state the following well-known lemma.
Lemma 5. For any two compact Hausdorff spaces X and Y, continuous function K : X × Y → R,
and e > 0, there exists an integer d > 0 and continuous functions φ2 : X → R, ψ2 : Y → R,
£ = 1,... ,d such that
d
K (x,y) - fφg (x) ψe (y) < e ∀ x ∈ X ,y ∈Y.
£ =1
Proof. The product space of two compact spaces X and Y, X × Y, is of course compact by
Tychonoff’s theorem. Consider the algebra
a = {f： f(χ,y) = AΦ°(χ)ψt(y), χ ∈ x,y ∈ Y∣.
It is easy to show (i) that A is an algebra, (ii) that A is a subalgebra of the real-valued continuous
functions on X × Y, and (iii) that A separates points. Then, combining the aforementioned facts, by
the Stone-Weierstrass theorem A is dense in the set of real-valued continuous functions on X×Y. □
Remark 9. In addition to helping us prove Theorem 2 below, Lemma 5 also serves as somewhat of an
analog to Mercer’s theorem for the more general case of asymmetric, non-PSD kernels. It is however
weaker than Mercer’s theorem in that the non-PSD nature of K means that the functions in the sum
cannot be considered as eigenfunctions (with φe = ψg) with associated nonnegative eigenvalues.
Now we proceed to the proof of Theorem 2.
Proof. To keep the equations from becoming too cluttered, below we use q(t), k(s), φ(t), ψ(s) ∈
Rd as the vector concatenation of the scalar functions {q£(t)}, {k(S)}, {φe(t)}, and {ψg(S)}, £ =
1, . . . , d, respectively. All sup norms are with respect to X × Y.
Our proof proceeds similarly to the proof of Theorem 5.1 of Okuno et al. (2018). We generalize their
theorem and proof to non-Mercer kernels and simplify some intermediate steps. First, by applying
Lemma 5, We can write that for any eι, there is a d such that
IlK-φT≠llsup <eι	(CI)
Now we consider the approximation of φe and ψg by q£ and k, respectively. By the universal
approximation theorem of multilayer neural networks (Cybenko, 1989; Hornik et al., 1989; Funahashi,
1989; Attali & Pag2s, 1997, etc.), we know that for any functions φι : X → R, ψ2 : Y → R and
scalar e2 > 0, there is an integer m > 0 such that if q£ : X → R and k : Y → R are two-layer
neural networks with m hidden units, then
Ilφι- qιIlsup < e2 and Ilψe- kIlsup < e2	(c.2)
for all £. Now, beginning from (14), we can write
l k - q T kIIsup ≤ I k - φ TψLp +ll φ Tψ - k TqLP	(C.3)
23
Under review as a conference paper at ICLR 2021
by the triangle inequality. Examining the second term of the RHS of (C.3),
Ilφ Tψ - q T MLp = Ilφ T(ψ - k) + (φ - q )T kLp
≤llφ T(ψ - k )Lp + ||(φ - q )T k||sup
≤ Il 例 SUp Ilψ - k Ilsup + Ilφ - q Ilsup Ilk HsUp	(C.4)
where the first inequality uses the triangle inequality and the second uses the Cauchy-Schwarz
inequality. Finally, we can combine (C.1)-(C.4) to write
∣l κ-q T k|lsup ≤ IK-φ T ψ∣lsup + i φLp i ψ - k∣Lp + i φ-q ILp H k∣Lp
Vs + de 2(∣∣ φ∣∣sup + Il ψ∣Lp + d 2).
Picking e 1 and e2 appropriately, e.g. e 1 = /2 and e2 ≤	(""'sup+72ψ'sup) +ɪ), completes the
proof.	□
D	Experiment Implementation Details
Datasets The 2014 International Workshop on Spoken Language Translation (IWSLT14) machine
translation dataset is a dataset of transcriptions of TED talks (and translations of those transcriptions).
We use the popular German to English subset of the dataset. We use the 2014 “dev” set as our test set,
and a train/validation split suggested in demo code for fairseq (Ott et al., 2019), where every 23rd
line in the IWSLT14 training data is held out as a validation set.
The 2014 ACL Workshop on Statistical Machine Translation (WMT14) dataset is a collection of
European Union Parliamentary proceedings, news stories, and web text with multiple translations.
We use newstest2013 as our validation set and newstest2014 as our test set.
The Stanford Sentiment Treebank (SST) (Socher et al., 2013) is a sentiment analysis dataset with
sentences taken from movie reviews. We use two standard subtasks: binary classification (SST-2)
and fine-grained classification (SST-5). SST-2 is a subset of SST-5 with neutral-labeled sentences
removed. We use the standard training/validation/testing splits, which gives splits of 6920/872/1821
on SST-2 and 8544/1101/2210 on SST-5.
Data Preprocessing On both translation datasets, we use sentencepiece3 to tokenize and train
a byte-pair encoding (Sennrich et al., 2016) on the training set. We use a shared BPE vocabulary
across the target and source languages. Our resulting BPE vocabulary size is 8000 for IWSLT14
DE-EN and 32000 for WMT14 EN-FR.
For SST, we train a sentencepiece BPE for each subtask separately, obtaining BPE vocabularies
of size 7465 for SST-2 and 7609 for SST-5.
Models Our models are written in Pytorch (Paszke et al., 2019). We make use of the Fairseq
(Ott et al., 2019) library for training and evaluation.
In machine translation, we use 6 Transformer layers in both the encoder and decoder. Both Trans-
former sublayers (attention and the two fully-connected layers) have a residual connection with the
“pre-norm” (Wang et al., 2019b) ordering of Layer normalization -> Attention or FC -> ReLU -> Add
residual. We use an embedding dimension of 512 for the learned token embeddings. For IWSLT14,
the attention sublayers use 4 heads with a per-head dimension d of 128 and the fully-connected
sublayers have a hidden dimension of 1024. For WMT14, following Vaswani et al. (2017)’s “base”
model, the attention layers have 8 heads with a per-head dimension d of 64 and the fully-connected
sublayers have a hidden dimension of 2048.
For SST, we use a very small, encoder-only, Transformer variant, with only two Transformer layers.
The token embedding dimension is 64, each Transformer self-attention sublayer has 4 heads with
per-head dimension d of 16, and the fully-connected sublayers have a hidden dimension of 128. To
produce a sentence classification, the output of the second Transformer layer is average-pooled over
3https://github.com/google/sentencepiece
24
Under review as a conference paper at ICLR 2021
the non-padding tokens, then passed to a classification head. This classification head is a two-layer
neural network with hidden dimension 64 and output dimension equal to the number of classes; this
output vector becomes the class logits.
Training We train with the Adam optimizer (Kingma & Ba, 2015). Following Vaswani et al. (2017),
for machine translation we set the Adam parameters β1 = 0.9, β2 = 0.98.
On IWSLT14 DE-EN, we schedule the learning rate to begin at 0.001 and then multiply by a factor
of 0.1 when the validation BLEU does not increase for 3 epochs. FOR WMT14 EN-FR, we decay
proportionally to the inverse square root of the update step using Fairseq’s implementation. For
both datasets, we also use a linear warmup on the learning rate from 1e-7 to 0.001 over the first 4000
update steps.
On IWSLT14 DE-EN, we end training when the BLEU score does not improve for 7 epochs on the
validation set. On WMT14 EN-FR, we end training after 100k gradient updates (inclusive of the
warmup stage), which gives us a final learning rate of 0.0002. We train on the cross-entropy loss and
employ label smoothing of 0.1. We use minibatches with a maximum of about 10k source tokens on
IWSLT14 DE-EN and 25k on WMT14 EN-FR Also on WMT14, we ignore sentences with more
than 1024 tokens.
For both SST subtasks, we also use a linear warmup from 1e-7 over 4000 warmup steps, but use
an initial post-warmup learning rate of 0.0001. Similar to IWSLT14, we decay the learning rate by
multiplying by 0.1 when the validation accuracy does not increase for 3 epochs, and end training
when the validation accuracy does not improve for 8 epochs.
Evaluation for machine translation Following Vaswani et al. (2017), we use beam-search decod-
ing, with a beam length of 4 and a length penalty of 0.6, to generate sentences for evaluation. We use
sacrebleu (Post, 2018) to generate BLEU scores. We report whole-word case-sensitive BLEU.
25