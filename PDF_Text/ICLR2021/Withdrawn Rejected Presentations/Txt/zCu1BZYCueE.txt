Under review as a conference paper at ICLR 2021
Response Modeling of Hyper-Parameters for
Deep Convolutional Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Hyper-parameter optimization (HPO) is critical in training high performing Deep
Neural Networks (DNN). Current methodologies fail to define an analytical re-
sponse surface (Bergstra & Bengio, 2012) and remain a training bottleneck due
to their use of additional internal hyper-parameters and lengthy manual evaluation
cycles. We demonstrate that the low-rank factorization of the convolution weights
of intermediate layers of a CNN can define an analytical response surface. We
quantify how this surface acts as an auxiliary to optimizing training metrics. We
introduce a fully autonomous dynamic tracking algorithm - autoHyper - that Per-
forms HPO on the order of hours for various datasets including ImageNet and re-
quires no manual intervention or a priori knowledge. Our method - using a single
RTX2080Ti - is able to select a learning rate within 59 hours for AdaM (Kingma
& Ba, 2014) on ResNet34 applied to ImageNet and improves in top-1 test accu-
racy by 4.93% over the default learning rate. In contrast to previous methods, we
empirically prove that our algorithm and response surface generalize well across
model, optimizer, and dataset selection removing the need for extensive domain
knowledge to achieve high levels of performance.
1 Introduction
The choice of Hyper-Parameters (HP) - such as initial learning rate, batch size, and weight decay -
has shown to greatly impact the generalization performance of Deep Neural Network (DNN) training
(Keskar et al., 2017; Wilson et al., 2017; Li et al., 2019; Yu & Zhu, 2020). By increasing the com-
plexity of network architectures (from high to low parameterized models) and training datasets (class
number and samples), the manual intervention to tune these parameters for optimization becomes
a practically expensive and highly challenging task. Therefore, the problem of Hyper-Parameter
Optimization (HPO) becomes central to developing highly efficient training workflows.
Recent studies shift the gear toward development of a meaningful metric measure to explain effective
HP tuning for DNN training. This is done in several behavioural studies, including changes in loss
surfaces (Keskar et al., 2017), input perturbation analysis (Novak et al., 2018), and the energy norm
of the covariance of gradients (Jastrzebski et al., 2020), just to name a few. In fact, the abstract
formulation of the HPO problem, as highlighted by Bergstra & Bengio (2012), can be modelled by
λ* — arg min{Eχ^M [L(x; Aλ(X (train))]},	(1)
where, X(train) and x are random variables, modelled by some natural distribution M , that represent
the train and validation data, respectively, L(∙) is some expected loss, and Aλ(X(train)) is a learning
algorithm that maps X (train) to some learned function, conditioned on the hyper-parameter set λ.
Note that this learned function, denoted as f(θ; λ; X(train)), involves its own inner optimization
problem. The HPO in (1) highlights two optimization problems of which optimization over λ cannot
occur until optimization over f(θ; λ; X(train)) is complete. This fact applies heavy computational
burden for HPO. Bergstra & Bengio (2012) reduce this burden by attempting to solve the following
λ* — arg min T(λ),	(2)
λ∈Λ
where τ is called the hyper-parameter response function or response surface, and Λ is some set of
choices for λ (i.e. the search space). The goal of the response surface is to introduce an auxiliary
1
Under review as a conference paper at ICLR 2021
function parameterized by λ of which its minimization is directly correlated to minimization of the
objective function f (θ). Little advancements in an analytical model of the response surface has led
to estimating it by (a) running multiple trials of different HP configurations (e.g. grid searching),
using evaluation against validation sets as an estimate to τ ; or (b) characterizing the distribution
model of a configuration’s performance metric (e.g. cross-validation performances) to numerically
define a relationship between τ and λ.
An important shift occurred when Bergstra & Bengio (2012) showed that random searching is more
efficient to grid searching, particularly when optimizing high-dimensional HP sets. To mitigate
the time complexity and increase overall performance, subsequent methods attempted to character-
ize the distribution model for such random configurations (Snoek et al., 2012; Eggensperger et al.,
2013; Feurer et al., 2015a;b; Klein et al., 2017; Falkner et al., 2018) or employed population control
(Young et al., 2015; Jaderberg et al., 2017) or early-stopping (Karnin et al., 2013; Li et al., 2017;
2018). However, these methods suffer from (a) additional internal HPs that require manual tuning
facilitated by extensive domain knowledge; (b) heavy computational overhead whereby the opti-
mization process takes days to weeks in most cases (Li et al., 2017; Falkner et al., 2018; Yu & Zhu,
2020); (c) poor generalization across model selection, datasets, and general experimental configura-
tions (e.g. optimizers); and (d) strong dependence on a manually defined search ranges that heavily
influences results (Choi et al., 2020; Sivaprasad et al., 2020). Importantly, these ranges are generally
chosen based on intuition, expert domain knowledge, or some form of a priori knowledge.
In this paper, we employ the notion of knowledge gain (Hosseini & Plataniotis, 2020) to model a
response surface - solvable with low computational overhead - and use it to perform automatic HPO
that does not require any a priori knowledge while still achieving competitive performance against
baselines and existing state of the art (SOTA) methods. Our goal is therefore to develop an algo-
rithm that is fully autonomous and domain independent that can achieve competitive performance
(not necessarily superior performance). We restrict our response surface to consider a single HP,
namely the initial learning rate η, and support this choice by noting that the initial learning rate
is the most sensitive and important HP towards final model performance (Goodfellow et al., 2016;
Bergstra & Bengio, 2012; Yu & Zhu, 2020) (see also Figure 10 in Appendix C). We demonstrate
how our method’s optimization directly correlates to optimizing model performance. Finally, we
provide empirical measures of the computational requirements of our algorithm and present thor-
ough experiments on a diverse set of Convolutional Neural Network (CNN) and Computer Vision
dataset that demonstrate the generalization of our response surface.
The main contributions of this work are as follows:
1.	Inspired by knowledge gain, we introduce a well-defined, analytical response surface using
the low-rank-factorization of convolution weights (Equation 5).
2.	We propose a dynamic tracking algorithm of low computational overhead on the order of
minutes and hours, dubbed autoHyper, to optimize our response surface and conduct HPO.
3.	This algorithm requires no domain knowledge, human intuition, or manual intervention,
and is not bound by a manually set searching space, allowing for completely automatic
setting of the initial learning rate; a novelty for deep learning practitioners.
1.1	Related Works
We leave extensive analysis of the related works to established surveys (Luo, 2016; He et al., 2019;
Yu & Zhu, 2020) but present a general overview here. Grid searching and manual tuning techniques
that require extensive domain knowledge trial various configurations and retain the best. Random
search (Bergstra & Bengio, 2012) was proven to be more efficient, particularly in high-dimensional
cases, but these methods suffer from redundancy and high computational overhead. Bayesian opti-
mization (Snoek et al., 2012; Eggensperger et al., 2013; Feurer et al., 2015a;b; Klein et al., 2017)
techniques attempt to characterize the distribution model of the random HP configurations. They
fail to properly define the response surface τ and resolve to estimating it by rationalizing a Gaussian
process over sampling points. The use of neural networks over Gaussian to model the generalization
performance was shown to have better computational performance (Snoek et al., 2015; Springen-
berg et al., 2016). Furthermore, the early stopping methods (Karnin et al., 2013; Li et al., 2017;
2018) spawn various configurations with equal resource distributions, successively stopping poor-
performing configurations and reassigning resources dynamically. Population-based training (PBT)
2
Under review as a conference paper at ICLR 2021
methods (Young et al., 2015; Jaderberg et al., 2017) follow an evolutionary approach by spawn-
ing various experimental configurations and adapting poor-performing trials to warm restart with
inherited learnable parameters and HPs. In addition, other methods such as orthogonal array tun-
ing (Zhang et al., 2019), box-constrained derivative-free optimization (Diaz et al., 2017), reverse
dynamics algorithm for SGD optimization (Maclaurin et al., 2015), and hybrid methods (Swersky
et al., 2013; 2014; Domhan et al., 2015; Falkner et al., 2018; Kandasamy et al., 2016) exist but
demonstrate no significant benefits over the previous techniques. Generally, each of these methods
suffer from high computational overheads - on the order of days to weeks to converge - as well as
additional internal HPs that heavily influence performance and generalization. In recent years, many
Python libraries have also been developed that include these optimization methods (Bergstra et al.,
2013; Kotthoff et al., 2017; Akiba et al., 2019).
2	A New Response Surface Model
In this section, we motivate and develop a new response surface model τ (λ) based on the low-rank
factorization of convolutional weights in a CNN. Unlike the common approach of cross-validation
performance measures, we define a new measure on the well-posedness of the intermediate layers of
a CNN and relate this measure to the general performance of the network. We first start by adopting
the low-rank measure of convolution weights.
2.1	Knowledge Gain via Low-Rank Factorization
Consider a four-way array (4-D tensor) W ∈ RN1 ×N2×N3×N4 as the convolution weights of an
intermediate layer of a CNN (N1 and N2 being the height and width of kernel size, and N3 and
N4 to the input and output channel size, respectively). Under the convolution operation, the input
feature maps FI ∈ RW×H×N3 are mapped to an arbitrary output feature map FO ∈ RW ×H ×N4 by
N3
F：O,i4= X F：I：,i3 * W：,：,i3,i4 .
i3=1
unfold	factorize then decompose
W (4-D Tensor) -u-nf-o→ld Wd (2-D Matrix) -f-ac-to-ri-ze-th-e-n -de-co-m-p-o→se UdΣdVdT +Ed
'----{z----}
Wcd
We note the importance of factorizing the unfolded matrix Wd using a low-rank factorization (we
use the Variational Bayesian Matrix Factorization (VBMF) (Nakajima et al., 2013)). Without this
factorization, the presence of noise will inhibit proper analysis. This noise Ed will “capture” the
randomness of initialization and ignoring it will allow us to better analyze our unfolded matrices
and make our response surface robust to initialization method.
Following the definition of Knowledge Gain (KG) from Hosseini & Plataniotis (2020), one can now
define a metric for each network layer using the norm energy of the low-rank factorization as
Gd(Wcd)
Nd ∙ σι(Wd)
Nd0
X σi(Wcd).
i=1
(3)
1
where, σ1 ≥ σ2 ≥ . . . ≥ σNd are the associated low-rank singular values in descending order. Here
Nd = rank{Wd} and the unfolding can be done in either input or output channels i.e. d ∈ {3, 4}.
For more information on KG as well as its efficient algorithmic computation, we refer the reader to
Hosseini & Plataniotis (2020).
The metric defined in (3) is normalized such that Gd ∈ [0, 1] and can be used to probe CNN layers
to monitor their efficiency in the carriage of information from input to output feature maps. We can
further parameterize the KG by the HP set λ, epoch t, and network layer ' as Gd,t,'(λ). A perfect
network and set of HPs would yield Cd,τ,'(λ) = 1 ∀' ∈ [L] where L is the number of layers in the
network and T is the last epoch. In this case, network layer functions as a better autoencoder through
iterative training and the carriage of information throughout the network is maximized. Conversely,
Cd,τ,'(λ) = 0 indicates that the information flow is very weak such that the mapping is effectively
random (k Ed k is maximized.).
3
Under review as a conference paper at ICLR 2021
2.2	Definition of New Response Function
Interestingly, if Gd,t,' (λ) = 0 in early stages of training, it is evidence that no learning has occurred,
indicative of an initial learning rate that is too small (no progress has been made to reduce the
randomization). It then becomes useful to track the zero-valued KGs within a network’s intermediate
layers’ input and output channels, which effectively becomes a measure of channel rank. We denote
this rank per epoch as follows:
Zt (λ) - 2L X X b1 - Gd,t,'(X)C
'∈[L] d∈{3,4}
where Zt(λ) ∈ [0, 1). Finally, we define the average rank across T epochs as
Z(λ ― τ X Zt (X) ∙	(4)
t∈[T]
Note that Z(X) ∈ [0, 1). The average rank measure in (4) is therefore a normalized summation
of the zero-valued singular values of the low-rank factorization across all layers’ input and output
unfolded tensor arrays.
Relating to the notion of HPO and the response surface, we return to (1) and (2). Where previously
the nature of these two optimization problems was poorly understood or practically unsolvable, we
propose a new problem that is well understood and practically solvable (on the computational order
of hours). To solve for the optimal HP set X, we look at the following optimization problem
λ* — arg min 1 -Z(λ), subject to ∣∣ VλZ(λ) ∣∣2 ≤ e	(5)
λ
where ∈ [0, 1) is some small conditioning error. Returning to equation 2, our response surface
is therefore defined as τ = 1 - Z(X) subject to ∣ VλZ(X) ∣22 ≤ . Note that we now simplify
our problem to only consider X = η. Also, we do not explicitly calculate the gradient VλZ(X),
but rather use this constraint to guide our dynamic tracking algorithm (see section 3). To explain
this somewhat counterintuitive formulation, we analyze Figures 1(a) & 2, which demonstrate that
as learning rates increase, Z(η) plateaus to zero. Specifically, we notice that optimal learning rates
lie towards the inception of the plateau of Z(η), before Z(η) = 0. This can also be seen in Figures
8 & 7 in Appendix A. Therefore, we wish to design our response surface such that the solution
lies at the inception of the plateau-ing region (see the red dot in Figure 1(a)). In this case, our
constraint ∣ VλZ(X) ∣22 ≤ promotes learning rates that lie along this plateau-ing region, while
arg min 1 - Z(X) promotes, of those learning rates in the plateau-ing region, a learning rate that
λ
lies towards the inception of this plateau-ing region.
8 7 6.5
6 6ss
u'p」rm< ura」FI—dot
2	3	4	5
Epoch - (t)
(a) Auxiliary Behaviour
Figure 1: (a) Auxiliary representation of Z (η) to training loss and training accuracy. The red dot
indicates the learning rate chosen by our method with corresponding metrics drawn in red lines.
Lines and scatter points in grayscale show various trialled learning rates. (b) Distribution of Z(η)
taken from the searching phase of autoHyper over numerous experimental configurations applied on
CIFAR10, CIFAR100, TinyImageNet, and ImageNet (see subsection 4.1).
More generally, high Z (η) indicates a learning rate that is too small, as intermediate layers do not
make sufficient progress in early stages of learning and therefore their KGs remain very low. This
observation follows that of Li et al. (2019) in which larger initial learning rates result in better
A+Jωuθ0 A-l->三 qfDqo」d
(b) Histogram of Z(η)
4
Under review as a conference paper at ICLR 2021
generalization performance. Promotion of these larger learning rates is therefore achieved by our
gradient constraint in Equation 5. Conversely, too large of a learning rate can over-regulate a network
and, therefore, we wish not to minimize Z(η) completely but tune it to be sufficiently small to arrive
at the inception of its plateau, creating a sort of kick-start if you will. This is achieved by the balance
between our minimization and constraint in Equation 5.
(a) AdaBound	(b) AdaGrad	(c) AdaM
Figure 2: Z (η) for various learning rates using AdaBound, AdaGrad, and AdAM on ResNet34
applied to CIFAR10. The author-suggested initial learning rate is indicated by the red markers, and
the autoHyper suggested learning rate is indicated by the green markers.
Finally, we choose T = 5 in our experiment. The early phase of training has been shown to be
an important criterion in optimal model performance (Jastrzebski et al., 2020; Li et al., 2019) and
we therefore wish to only consider 5-epochs to ensure optimization within this phase. Additionally,
Figure 2 and Figures 8 & 7 in Appendix A tell us that Z(η) stabilizes after 5 epochs.
2.3	Empirical Evaluation of New Response Model
Figure 1(a) visualizes results of our method on ResNet34 on CIFAR10 optimized with AdaM. The
learning rate selected by our method results in lowest training loss and highest top-1 training accu-
racy over the 5-epoch range we consider. We note the importance of stopping at the inception of
this plateau region as even though higher learning rates, highlighted by the gray-scale lines/markers,
result in potentially lower Z(η), they do not guarantee lower training losses or higher training accu-
racies. We conclude that our response surface is a strong auxiliary to training loss, and optimizing
HPs relative to our response surface will in fact optimize towards training loss and accuracy.
Figure 1(b) displays the histogram of Z (η) values over various experimental configurations (see
subsection 4.1). Note the presence of a multimodal distribution that peaks at low Z(η) but impor-
tantly not zero. This visualizes our method’s tendency to converge to a consistent range of values
for irrespective of experimental configuration, showing the generalization of our response surface.
3	AutoHyper: Automatic Tuning of Initial Learning Rate
The pseudo-code for autoHyper is presented in Algorithm 3. Analyzing equation 5, we state that
the optimal solution lies within the inception of the plateauing region of Z(η). To find this region,
autoHyper first initializes a logarithmic grid space, from ηmin = 1 × 10-4 to ηmax = 0.1 of S = 20
step sizes, denoted by Ω. It iterates through each ηi ∈ Ω; i ∈ {0,..., 19}, and computes Z (η), until
a plateau is reached. OnCe a plateau is reached, Ω is reset such that ηmin and ηmax “zoom” towards
the learning rates at the plateau. This process is repeated recursively until no significant difference
between ηmin and ηmax exists. On average, this recursion occurs 3 to 4 times and as shown in Figure
3, the number of trialled learning rates remains very low (between 10-30 on average). Importantly,
our algorithm is not constrained by its initial grid space. As it tracks Z(η) over learning rates, it may
grow and shrink its search bounds dynamically. This permits our method to be fully autonomous
and require no human intuition in setting of the initial grid space.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 autoHyper
Require: grid space function Ψ, learning rate significant difference delta α = 5 × 10-5, and rate of change function ζ
1:	procedure RESTART( )
2:	learning rate index i = 0
3:	ω = ψ(ηmin , ηmaχ , S)
4:	end procedure
5:	Restart( )
6:	while True do
7:	if i = | Ω | then // increase search space since no plateau has been found yet
8:	set ηmin = ηmax , increase ηmax and RESTART( )
9:	end if
10:	if ηmax - ηmin < α then // limits of search space are not significantly different
11:	return ηmax
12:	end if
13:	with ηi — Ωi, train for 5 epochs
14:	compute rank: Z(ηi) per equation 4
15:	if Z(ηi) = 1.0 then // all KG is zero-valued, ηmin is too small
16:	increase ηmin and RESTART( )
17:	end if
18:	if i = 0 and Z(ηi) < 0.5 and this is the first run then // initial ηmin is too large
19:	reduce ηmin and RESTART( )
20:	else
21:	if Z(ηi) = 0.0 then // all KG is non-zero, don’t search further, perform “zoom”
22:	set ηmin = Ai —2 , rηmax = Ai and RESTART()
23:	end if
24:	compute rate of change of Z (ηi): δ — Z ({Z (η0), ... , Z (ηi)})
25:	if rate of change plateaus then // perform “zoom”
26:	set ηmin = Ωi-1, ηmaχ = Ωi and Restart()
27:	end if
28:	end if
29:	i += 1
30:	end while
■ AdaBound - AdaGrad - AdaM - AdaS jβ = 0.8	■ AdaSB = 0.9	■ AdaSjB = 0.95	■ AdaS jB = 0.975	■ RMSProp ■ SLS
S-6p=M-0 JφqLUΠN
Figure 3: Computational analysis of autoHyper over various setups (number of learning rates au-
toHyper trialled before converging). ResNet34 trials take 3 minutes, 3 minutes, 18 minutes, and
220 minutes for CIFAR10, CIFAR100, TinyImageNet and, ImageNet, respectively. ResNet18,
ResNeXt50, and DenseNet121 trials take 2 minutes, 3 minutes, and 3 minutes respectively for both
CIFAR10 and CIFAR100.
We note here that the choice of Ψ and ζ mentioned in Algorithm 3 (i.e. grid space and rate of
change functions, respectively) will have a significant affect on the final generated learning rate.
(a) ResNet34/Adam
(b) EffNetB0∕AdaSβ = 0.8
Figure 4: Z(η) (blue) vs. cumprod(Z(η))0.8 (or-
ange) for (a) a stable and (b) an unstable architec-
ture on CIFAR10.
We make use of numpy’s geomspace function
for the logarithmic grid spacing, and calculate
the rate of change in Z(η) by taking the
cumulative product of the sequence of Z (ηi),
to the power of 0.8. A logarithmic grid space is
used as our response surface is more sensitive
to smaller learning rates (see Figure 2). Note
that initial grid bounds are not important as our
algorithm can shift those bounds dynamically,
however the successive increments between
learning rates in the grid must be sufficiently
small (on the order of 1 × 10-4 as in our
initialization). Since our response surface itself
is not guaranteed to monotonically decrease, as shown in Figure 4(b), we employ the cumulative
product of Z (ηi) (as our rate of change function), which is a monotonically decreasing function
6
Under review as a conference paper at ICLR 2021
-since Z(6)∈ [0,1) - and is therefore always guaranteed to converge. The cumulative product
(to the power of 0.8) is a good choice because it (a) is always guaranteed to plateau (since
0 ≤ Z(ηi) < 1), which removes the need for some manually tuned threshold and (b) because it
dampens noise well. Because the cumulative product on its own degrades to zero rather quickly
in many scenarios, raising it to the power of 0.8 regulates this effect. This power is technically
tune-able, however we show empirically in Figure 4(a) and 4(b) that 0.8 behaves well for both
stable and unstable architectures. Refer to Figure 9 in Appendix C for the performance results of
EfficientNetB0.
4	Experiments
In this section, we conduct an ablative study of our algorithm autoHyper and response surface on
various network architectures trained using various optimizers and applied to image classification
datasets. We also compare autoHyper against existing SOTA; Random Search.
4.1	Experimental Setups
Ablative study. All experiments are run using an RTX2080Ti, 3 cores of an Intel Xeon Gold
6246 processor, and 64 gigabytes of RAM. In our ablative study, we run experiments on CIFAR10
(Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), TinyImageNet (Li et al.), and Im-
ageNet (Russakovsky et al., 2015). On CIFAR10 and CIFAR100, we apply ResNet18 (He et al.,
2015), ResNet34 (He et al., 2015), ResNeXt50 (Xie et al., 2016), and DenseNet121 (Huang et al.,
2017). On TinyImageNet and ImageNet, we apply ResNet34. For architectures applied to CIFAR10
and CIFAR100, we train using AdaM (Kingma & Ba, 2014), AdaBound (Luo et al., 2019), AdaGrad
(Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), AdaS(β = {0.8, 0.9, 0.95, 0.975}) (Hosseini
& Plataniotis, 2020) (with early-stop), and SLS (Vaswani et al., 2019). For ResNet34 applied to
TinyImageNet, we train using AdaM, AdaBound, AdaGrad, and AdaS(β = {0.8, 0.9, 0.95, 0.975}). For
ResNet34 applied to ImageNet, we train using AdaM and AdaS(β = {0.9, 0.95, 0.975}). Note that the β
in AdaS-variants is known as the gain factor and trades between performance and convergence rate :
a low β converges faster but at the cost of performance and vice-versa. For each experimental setup
we ran one training sequence using suggested learning rates (baseline) and one training sequence
using learning rates generated by autoHyper (see Tables 1-4 in Appendix B). Refer to Appendix B
for additional details on the ablative study.
Comparison to Random Search. Because Random Search generally requires iterative manual
refinement and is highly sensitive to the manually set search space (Choi et al., 2020; Sivaprasad
et al., 2020), we attempt a fair comparison by providing the same initial search space that autoHyper
starts with, and allow for the same number of trials that autoHyper takes (see Figure 3). We note
however that this does provide the Random Search with a slight advantage since a priori knowledge
of how many trials to consider is not provided to autoHyper. See Appendix D for additional detail.
4.2	Results
Consistent performance across architectures, datasets, and optimizers. We visualize the pri-
mary results of each experiment in Figure 5(a) (additional results are shown in Figure 11 in Ap-
pendix C). From these figures we see how well our method generalizes to experimental configura-
tions by noting the consistency in top-1 test accuracies when training using the autoHyper generated
initial learning rate vs. the baseline. Further, we note that if there is loss of performance when
using an initial learning rate generated by autoHyper, we identify that this loss is < 1% in all exper-
iments except three: On CIFAR100, the baselines of ResNeXt50 trained using AdaM, ResNext50
trained using RMSProp, and DenseNet121 trained using AdaBound achieve 1.2%, 2.28% and 1.9%
better top-1 test accuracy, respectively. We note importantly however that when accounting for the
standard deviation of each of these results, only the DenseNet121 experiement maintains its > 1%
improvement. Refer to Appendix C (Tables 5-8) to see the tabulated results of each experiment. We
also importantly highlight how autoHyper is able to generalize across experimental setup whereas
Random Search cannot (see Figure 5(b)). Because Random Search (and other SOTA methods) de-
pend heavily on their manually defined parameters such as epoch budget or initial search space,
7
Under review as a conference paper at ICLR 2021
(a) Ablative Study
(b) Random Search Comparison
Figure 5: Results of the (a) ablative study and (b) Random Search comparison experiments. Titles
below plots indicate what experiment the above plots refers to. Legend labels marked by ‘*’ (solid
lines) show results for autoHyper generated learning rates and dotted lines are the (a) baselines and
(b) Random Search results.
8
Under review as a conference paper at ICLR 2021
generalization to experimental setup is not feasible, as demonstrated here. In contrast, we have
shown that autoHyper is perfectly capable of performing well no matter the experimental setting
without need for manual intervention/refinement of any kind; a novelty.
Fully autonomous discovery of optimal learning rates. Importantly, we highlight how our method
is able to fully autonomously tune the initial learning and achieve very competitive performance.
Whereas traditional HPO methods (like Random Search) are extremely sensitive to initialization of
the search space, which would normally require extensive domain or a priori knowledge to set, our
method is not: given a new dataset, model, and/or other hyper-parameter configurations, a practi-
tioner could use simply call our algorithm to automatically set a very competitive initial learning
rate. If truly superior performance is required, one could perform more extensive HPO around the
autoHypersuggested learning rate, removing the need to perform iterative manual refinement.
Superior performance over existing SOTA. As visualized in Figure 13, although Random Search
proves competitive for Adabound and AdaM applied on CIFAR10 and CIFAR100, it cannot find
a competitive learning rate for AdaSβ = 0.9 or AdaGrad and performs worse for AdaM applied
on TinyImageNet. AdaGrad applied on TinyImageNet loses as much as 4% top-1 test accuracy.
This highlights how autoHyper can automatically find more competitive learning rates to a Random
Search given the same computational budget, and with significantly less manual intervention. These
results additionally highlight why validation loss (or accuracy) cannot be used as a substitute to our
metric (see Figure 14 in subsection D.2 for additional discussion).
Drastic improvements in AdaM applied to TinyImageNet and ImageNet. ResNet34 trained
using AdaM and applied to TinyImageNet and ImageNet achieves final improvements of 3.14%
and 4.93% in top-1 test accuracy, respectively (see Table 5 in Appendix C). Such improvements
come at a minimal cost using our method, requiring 13 trials (4 hours) and 16 trials (59 hours) for
TinyImageNet and ImageNet, respectively (see Figure 3).
Extremely fast and consistent convergence rates. We visualize the convergence rates of our
method in Figure 3. Importantly, we identify the consistency of required trials per optimizer across
architecture and dataset selection as well as the low convergence times. We identify that the longest
convergence time for our method is on ResNet34 trained using AdaSβ = 0.95 applied to ImageNet,
which took 31 trials and a total of 114 hours. We note that our method exhibits less consistent results
when optimizing using SLS as SLS tends to result in high Z (η) over multiple epochs and different
learning rates. Despite this, our model still converges and results in competitive performance.
Performance improvement over increased epoch budgets. In reference to Table 6 in Appendix C,
we highlight how, of the 29 experimental configurations, when trained using the initial learning rate
suggested by autoHyper, only 12 of them outperform the baseline. However, as training progresses,
we note that by the end of the fixed epoch budget, 18 of the 29 experiments trained using the
initial learning rate suggested by autoHyper outperform the baselines. Further, in many of the cases
where baselines perform better, they remain within the standard deviation of trials, and are therefore
not significantly better. These results are surprising as our goal with this method was to achieve
competitive results in tuning the initial learning rate however, in more than half the cases, our method
results in increased performance at a significantly smaller computational cost.
5	Conclusion
Figure 6: Z(η, γ) for learning
rate (η) and mini-batch size (γ).
In this work we proposed an analytical response surface that acts
as auxiliary to training metrics and generalizes well. We proposed
an algorithm, autoHyper, that solves this surface and quickly gen-
erates learning rates that are competitive to author-suggested and
Random Search-suggested values - in some cases, even drasti-
cally superior. We therefore have introduced an algorithm that
can perform HPO fully autonomously and extremely efficiently,
and resolves many of the drawbacks of current SOTA. Figure 6 vi-
sualizes our response surface over a multi-dimension HP set and
highlights how our response surface remains solvable. We iden-
tify that autoHyper could be adapted to simultaneously optimize multiple HPs by tracking tangents
across this surface towards the minimum, but leave this to future work.
9
Under review as a conference paper at ICLR 2021
References
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:
A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2623-2631,
2019.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. The Journal
of Machine Learning Research, 13(1):281-305, 2012.
James Bergstra, Dan Yamins, and David D Cox. Hyperopt: A python library for optimizing the
hyperparameters of machine learning algorithms. In Proceedings of the 12th Python in science
conference, volume 13, pp. 13-19. Citeseer, 2013.
Dami Choi, Christopher J. Shallue, Zachary Nado, Jaehoon Lee, Chris J. Maddison, and George E.
Dahl. On empirical comparisons of optimizers for deep learning, 2020.
Gonzalo I Diaz, Achille Fokoue-Nkoutche, Giacomo Nannicini, and Horst Samulowitz. An effective
algorithm for hyperparameter optimization of neural networks. IBM Journal of Research and
Development, 61(4/5):9-1, 2017.
Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparam-
eter optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth
International Joint Conference on Artificial Intelligence, 2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Katharina Eggensperger, Matthias Feurer, Frank Hutter, James Bergstra, Jasper Snoek, Holger Hoos,
and Kevin Leyton-Brown. Towards an empirical foundation for assessing bayesian optimization
of hyperparameters. In NIPS workshop on Bayesian Optimization in Theory and Practice, vol-
ume 10, pp. 3, 2013.
Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter opti-
mization at scale, 2018.
Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank
Hutter. Efficient and robust automated machine learning. In Advances in neural information
processing systems, pp. 2962-2970, 2015a.
Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. Initializing bayesian hyperparame-
ter optimization via meta-learning. In Twenty-Ninth AAAI Conference on Artificial Intelligence,
2015b.
Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge,
MA, USA, 2016. http://www.deeplearningbook.org.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition, 2015.
Xin He, Kaiyong Zhao, and Xiaowen Chu. Automl: A survey of the state-of-the-art. arXiv preprint
arXiv:1908.00709, 2019.
Mahdi S. Hosseini and Konstantinos N. Plataniotis. Adas: Adaptive scheduling of stochastic gradi-
ents, 2020.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali
Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based train-
ing of neural networks. arXiv preprint arXiv:1711.09846, 2017.
10
Under review as a conference paper at ICLR 2021
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho*, and Krzysztof Geras*. The break-even point on optimization trajectories of deep neural
networks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=r1g87C4KwB.
Kirthevasan Kandasamy, Gautam Dasarathy, JUnier B Oliva, Jeff Schneider, and Barnabas Poczos.
Gaussian process bandit optimisation with multi-fidelity evaluations. In Advances in Neural In-
formation Processing Systems, pp. 992-1000, 2016.
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.
In International Conference on Machine Learning, pp. 1238-1246, 2013.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In
International Conference on Learning Representations, 2017. URL https://openreview.
net/forum?id=H1oyRlYgg.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast bayesian op-
timization of machine learning hyperparameters on large datasets. In Artificial Intelligence and
Statistics, pp. 528-536, 2017.
Lars Kotthoff, Chris Thornton, Holger H Hoos, Frank Hutter, and Kevin Leyton-Brown. Auto-
weka 2.0: Automatic model selection and hyperparameter optimization in weka. The Journal of
Machine Learning Research, 18(1):826-830, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Fei-Fei Li, Andrej Karpathy, and Justin Johnson. URL https://tiny-imagenet.
herokuapp.com/.
Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin
Recht, and Ameet Talwalkar. Massively parallel hyperparameter tuning. arXiv preprint
arXiv:1810.05934, 2018.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning
Research, 18(1):6765-6816, 2017.
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. In Advances in Neural Information Processing Systems,
pp. 11674-11685, 2019.
Gang Luo. A review of automatic selection methods for machine learning algorithms and hyper-
parameter values. Network Modeling Analysis in Health Informatics and Bioinformatics, 5(1):18,
2016.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate, 2019.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015.
Shinichi Nakajima, Masashi Sugiyama, S Derin Babacan, and Ryota Tomioka. Global analytic
solution of fully-observed variational bayesian matrix factorization. Journal of Machine Learning
Research, 14(Jan):1-37, 2013.
Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.
Sensitivity and generalization in neural networks: an empirical study. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
HJC2SzZCW.
11
Under review as a conference paper at ICLR 2021
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV),115(3):211-252,2015. doi: 10.1007∕s11263-015-0816-y.
PrabhU Teja Sivaprasad, Florian Mai, Thijs Vogels, Martin Jaggi, and Francois Fleuret. Optimizer
benchmarking needs to account for hyperparameter tuning, 2020.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep
neural networks. In International conference on machine learning, pp. 2171-2180, 2015.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization
with robust bayesian neural networks. In Advances in neural information processing systems, pp.
4134-4142, 2016.
Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-task bayesian optimization. In Advances
in neural information processing systems, pp. 2004-2012, 2013.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw bayesian optimization. arXiv
preprint arXiv:1406.3896, 2014.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Sharan Vaswani, Aaron Mishkin, Issam H. Laradji, Mark Schmidt, Gauthier Gidel, and Simon
Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates.
CoRR, abs/1905.09997, 2019. URL http://arxiv.org/abs/1905.09997.
Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning, 2017.
Saining Xie, RoSS Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks, 2016.
Steven R Young, Derek C Rose, Thomas P KarnoWski, Seung-HWan Lim, and Robert M Patton.
Optimizing deep learning hyper-parameters through an evolutionary algorithm. In Proceedings
of the Workshop on Machine Learning in High-Performance Computing Environments, pp. 4.
ACM, 2015.
Tong Yu and Hong Zhu. Hyper-parameter optimization: A revieW of algorithms and applications.
arXiv preprint arXiv:2003.05689, 2020.
Xiang Zhang, Xiaocong Chen, Lina Yao, Chang Ge, and Manqing Dong. Deep neural netWork
hyperparameter optimization With orthogonal array tuning. In International Conference on Neural
Information Processing, pp. 287-295. Springer, 2019.
12
Under review as a conference paper at ICLR 2021
A Rank behaviour over multiple epochs
Figure 7: Rank (Z(η)) for various learning rates on VGG16 trained using AdaM, AdaGrad, AdaS
β = 0.8, and RMSProp and applied to CIFAR10. A fixed epoch budget of 20 was used. We highlight
how across these 20 epochs, very little progress is made beyond the first first epochs. It is from this
analysis that we choose our epoch range of T = 5.
(a) AdaM
(b) AdaGrad
(c) AdaS β = 0.8
(d) RMSProp
Figure 8: (Z(η)) for various learning rates on ResNet34 trained using AdaM, AdaGrad, AdaS β =
0.8, and RMSProp and applied to CIFAR10. A fixed epoch budget of 20 was used. We highlight
how across these 20 epochs, very little progress is made beyond the first first epochs. It is from this
analysis that we choose our epoch range of T = 5.
B Additional Experimental Details for subsection 4.1
We note the additional configurations for our experimental setups.
Datasets: For CIFAR10 and CIFAR100, we perform random cropping to 32 × 32 and random
horizontal flipping on the training images and make no alterations to the test set. For TinyImageNet,
13
Under review as a conference paper at ICLR 2021
we perform random resized cropping to 64 × 64 and random horizontal flipping on the training
images and center crop resizing to 64 × 64 on the test set. For ImageNet, we follow He et al. (2015)
and perform random resized cropping to 224 × 244 and random horizontal flipping and 256 × 256
resizing with 224 × 224 center cropping on the test set.
Additional Configurations: Experiments on CIFAR10, CIFAR100, and TinyImageNet used mini-
batch sizes of 128 and ImageNet experiments used mini-batch sizes of 256. For weight decay,
5 × 10-4 was used for AdaS-variants on CIFAR10 and CIFAR100 experiments and 1 × 10-4 for
all optimizers on TinyImageNet and ImageNet experiments, with the exception of AdaM using a
weight decay of 7.8125 × 10-6. For AdaS-variant, the momentum rate for momentum-SGD was
set to 0.9. All other hyper-parameters for each respective optimizer remained default as reported in
their original papers. For CIFAR10 and CIFAR100, we use the manually tuned suggested learning
rates as reported in Wilson et al. (2017) for AdaM, RMSProp, and AdaGrad. For TinyImageNet and
ImageNet, we use the suggested learning rates as reported in each optimizer’s respective paper. Refer
to Tables 1-4 to see exactly which learning rates were used, as well as the learning rates generated
by autoHyper. CIFAR10, CIFAR100, and TinyImageNet experiments were trained for 5 trials with
a maximum of 250 epochs and ImageNet experiments were trained for 3 trials with a maximum of
150 epochs. Due to AdaS’ stable test accuracy behaviour as demonstrated by Hosseini & Plataniotis
(2020), an early-stop criteria, monitoring testing accuracy, was used for CIFAR10, CIFAR100, and
ImageNet experiments. For CIFAR10 and CIFAR100, a threshold of 1 × 10-3 for AdaSβ = 0.8 and
1 × 10-4 for AdaSβ = {0.9, 0.95} and patience window of 10 epochs. For ImageNet, a threshold of
1 × 10-4 for AdaSβ = {0.8, 0.9, 0.95} and patience window of 20 epochs. No early stop is used for
AdaSβ = 0.975.
Learning Rates: We report every learning rate in Tables 1-4.
Table 1: Learning rates for ResNet34 experiments. Left inner columns show suggested, right inner
columns show autoHyper generated. Note that the superscript for AdaS-variants indicates their β
gain factor.
Optimizer	CIFAR10		CIFAR100		TinyImageNet		ImagetNet	
AdaM	0.0003	0.000333	0.0003	0.000241	0.001	0.0001965	0.001	0.0001965
AdaBound	0.001	0.000347	0.001	0.000347	0.001	0.0000944	-	-
AdaGrad	0.01	0.002861	0.01-	0.002236	0.01	0.0022359	-	-
AdaS(0.8)	0.03	0.012374	0.03	0.010401	0.03	0.010185	-	-
AdaS(0.9)	0.03	0.012374	0.03	0.010190	0.03	0.0085857	0.02	0.011479
AdaS(0.95)	0.03	0.015336	0.03	0.015025	0.03	0.0085857	0.02	0.011479
AdaS(0.975)	0.03	0.012374	0.03	0.010190	0.03	0.0085857 -	0.02	0.011479 -
RMSProp	0.0003	0.000168	0.0003	0.000197	-	-	-	-
SLS -	1.0	0.034191	1.0	0.034191	-	-	-	-
Table 2: Learning rates for ResNet18 experiments. Left inner columns show suggested, right inner
columns show autoHyper generated.
Optimizer	CIFAR10		CIFAR100	
AdaM	0.0003	0.0006756	0.0003	0.0006756
AdaBound	0.001	0.00036040	0.001	0.00024896
AdaGrad	0.01	0.0049724	0.01	0.0049724
AdaS(β = 0.8)	0.03	0.0126594	0.03	0.0126594
AdaS(β = 0.9)	0.03	0.0104254	0.03	0.0126594
AdaS(β = 0.95)	0.03	0.01042544	0.03	0.01042543
AdaS(β = 0.975)	0.03	0.0104254	0.03	0.007071
RMSProp	0.0003	0.0004697	0.0003	0.0004697
SLS	1.0	0.03419134~	1.0	0.0341913 —
14
Under review as a conference paper at ICLR 2021
Table 3: Learning rates for ResNeXt50 experiments. Left inner columns show suggested, right inner
columns show autoHyper generated.
Optimizer	CIFAR10		CIFAR100	
AdaM	0.0003	0.0006756	0.0003	0.0006756
AdaBound	0.001	0.00036040	0.001	0.00024896
AdaGrad	0.01	0.0049724	0.01	0.0049724
AdaS(β = 0.8)	0.03	0.0126594	0.03	0.0126594
AdaS(β = 0.9)	0.03	0.0104254	0.03	0.0126594
AdaS(β = 0.95)	0.03	0.023134	0.03	0.022666
AdaS(β = 0.975)	0.03	00104254	0.03	0.007071
RMSProp	00003	00004697	00003	00004697
SLS	1.0	0.03419134~	1.0	0.0341913 —
Table 4: Learning rates for DenseNet121 experiments. Left inner columns show suggested, right
inner columns show autoHyper generated.
Optimizer	CIFAR10			CIFAR100		
AdaM	0.0003	0.0020109	0.0003	0.00093162
AdaBound	0.001	0.0030176	0.001	0.003147979
AdaGrad	0.01	0.01537206	0.01	0.01537206-
AdaS(β = 0.8)	0.03	0.06107176	0.03	0.03978104
AdaS(β = 0.9)	0.03	0.0598363	0.03	-0.050414
AdaS(β = E	0.03	0.0492772	0.03	0.0357048479
AdaS(β = 0.975)	0.03	00598363	0.03	0.0504143
RMSProp	0.0003	000201093	00003	0.00067564-
SLS	1.0	0.0031184 —	1.0	0.087943
C Additional Results for subsection 4.2
Large deviation from the suggested initial learning rates. Referring to Tables 1-4 & 9, we notice
variation in autoHyper suggested learning rates as compared to the author-suggested and Random
Search-selected ones. The learning rates generated by our method reveal the “blind spots” that the
authors originally overlooked in their HPO. Interestingly, however, we note the similarity in initial
learning for ResNet34 trained using AdaM on CIFAR10, and can confirm this as an optimal learning
rate. Importantly, our method is significantly quicker than the grid searching technique employed
by Wilson et al. (2017). Observations on the generalization characteristics of optimizers. Fig-
ure 5(a) identifies the poor generalization characteristics of AdaM, AdaBound, AdaGrad, and SLS
where they consistently achieve low training losses, but do not exhibit equivalently high top-1 test
accuracies. We note that these observations are similar to those made by Wilson et al. (2017); Li
et al. (2019); Jastrzebski et al. (2020). We additionally contribute that AdaS does generalize well.
We also highlight SLS’ multi-order-of-magnitude tolerance to initial learning rate as well as the
stability of the AdaS optimizer, particularly when applied on TinyImageNet.
15
Under review as a conference paper at ICLR 2021
—— AdaS j8 = 0.975*
RMSProp
RMSProp*
0.90
0.88
0.86
0.84
EfficientNetBO on
>UΞ⊃U⅛4->sg I—dot
AdaGrad ------- AdaM*	AdaS β = 0.9
—— AdaGrad* —— AdaSe = 0.8	—— AdaSjS = 0.9*
AdaM ---------- AdaS β = 0.8*	---- AdaS jβ = 0.975
Figure 9: Test accuracy and trianing loss for EfficientNetB0 applied to CIFAR100. Importantly,
EfficientNetB0 is an unstable network architecture in relation to our response surface and yet our
method, autoHyper, is still able to converge and achieve competitive performance.
StepLR ——StepLR*
0.95
IO0
Figure 10: Demonstration of the importance of initial learning rate in scheduled learning rate case,
for ResNet18 applied on CIFAR10, using Step-Decay method with step-size = 25 epochs and decay
rate = 0.5. As before, the dotted line represents the baseline results, with initial learning rate =
0.1, and the solid line represents the results using autoHyper’s suggested learning rate of 0.008585.
These results highlight the importance of initial learning rate, even when using a scheduled learning
rate heuristic, and demonstrates the importance of the additional step-size and decay rate hyper-
parameters. Despite better initial performance from the autoHyper suggest learning rate, the step-
size and decay rate choice cause the performance to plateau too early.
16
Under review as a conference paper at ICLR 2021
50	100	150	200	250
Epoch
ResNet34 on TinyImageNet
……AdaBound	AdaGrad …AdaM … AdaS 0 = 0.8	……AdaS jβ = 0.9	………AdaSjS = 0.95	……AdaS 0 = 0.975	… RMSProp ………SLS
--- AdaBound* ---- AdaGrad* --- AdaM* ----- AdaS β = 0.8*	—— AdaS 0 = 0.9*	- AdaS )3 = 0.95*	- AdaS = 0.975*	--- RMSProp* --- SLS*
50	100	150	200	250
Epoch
ResNet34 on TinyImageNet
50	100	150	200	250
Epoch
ResNet34 on TinyImageNet
50	100	150	200	250
Epoch
ResNet34 on TinyImageNet
50505050
20752075
77666655
666O.O.66O.
Aue.lnuu< u-e」l I，dQL
Figure 11: Full results of CIFAR100, TinyImageNet, and ImageNet experiments. Top-1 test accu-
racy and training losses are reported for CIFAR100 experiments and top-1 and top-5 test and training
accuracies are reported for TinyImageNet and ImageNet. Titles below the figures indicate to which
experiments the above figures belong to. As before, lines indicated by the ‘*’ (solid lines), are results
using initial learning rate as suggested by autoHyper.
17
Under review as a conference paper at ICLR 2021
…… AdaBound …… AdaGrad …… AdaM …… AdaS β = 0,8	…… AdaSS = 0,9	…… AdaS β = 0.95	…… AdaS 0 = 0,975
--- AdaBound* ------ AdaGrad* ----- AdaM* —— AdaS β = 0.8*	—— AdaSS = 0.9*	—— AdaS β = 0.95*	—— AdaS 0 = 0,975*
>U2⊃U⅛.MS e I IdQL
Epoch	Epoch
Figure 12: Top-1 Test Accuracy and Test Loss for ResNet34 Experiments applied on TinyImageNet.
As before, lines indicated by the ‘*’ (solid lines), are results using initial learning rate as suggested
by autoHyper. These results visualize the inconsistency in tracking test loss as a metric to optimize
final testing accuracy. This can be seen, for example, when looking at the test loss and test accuracy
plots for AdaM, where the test loss for the baseline is lower than that of the autoHyper suggested
results but autoHyper achieves better test accuracy. These results also highlight the instability of
tracking testing accuracy or less instead of the metric defined in Equation 5.
18

Table 5: List of ResNet34 top-1 test accuracy using various optimizers and applied to various datasets. Note that each cell reports the average accuracy over each
experimental trial, sub-scripted by the standard deviation over the trials. Values on the left are from trials trained using the initial learning rate as generated by
autoHyper. Values on the right are from trials trained using the suggested initial learning rate. Note a ’*’ indicates that early-stop has been activated.
Dataset	Optimizer		EPoCh 50			EPoCh 100			EPoCh 150			EPoCh 200		Epoch 250
CIFAR10	AdaBound	91.66±0.33∕90.84±o.17	92.37±o.25∕92.03±o.44	92.78±0.2o∕92∙67±o.i4	92.79±o.42∕92∙59±o.i3	93.24±o.i7∕92∙74±o.i6
	-AdaGrad-	89.41±0.39∕90.17±o.61	90.30±o.33∕9L10±0.25	90∙77±o.35∕9L34±0.37	9O∙67±0.3o∕9L40±0.i5	90∙94±o.14∕9L50±o.24
	AdaM	91.44±0.19∕9L54±o.35	92.72±0.i5∕92.71±o.27	92∙93±o.2i∕93.07±o.33	93∙O6±0.34∕93.17±o.30	-93.39±0.3i∕93∙22±o.36-
	AdaS0.8	93.35±o.i4∕93.04±o.i3	93.40±o.i5∕93.03±o.i6	93.40±0.i5∕93∙00±o.i7	93.40±o.i5∕93∙02±o.i6	93.40±o.i5∕93∙02±o.i3
	AdaS0∙9	94.00±0.13∕94.35±o.22	94∙25±0.22∕94.48±o.i3	94∙25±o.22∕94.45±o.ii	94∙25±o.22∕94.47±o.i3	94∙2* 5±o.22∕94.46±o.i2
	AdaS0.95	91.90±o.25∕92.03±o.33	94.91±o.16∕95.14±o.i6	95∙08±o.18∕95.20±o.ii	95∙08±o.18∕95.20±o.ii	95∙08±o.18∕95.20±o.ii
	AdaS0∙975	87.68±o.95∕88.9i ±ι.30	92∙92±o.32∕93.09±o.52	94∙75±o.12∕94.99±o.23	95∙01±o.13∕95.14±o.34	95∙13±o.ii∕95.24±o.i5
	RMSProP	90.69±o.47∕90.86±o.58	91 ∙41±o.55/91.59±0.77 ~	92∙22±o.68∕92.69±o.33	92.94±0.33∕92∙88±o.3o	93.03±o.23∕92∙90±o.29
	SLS 一	93.3O±0.i6∕93.28±0.io	93∙41±o.09∕93.48±0.09	93∙39±o.io∕93.49±0.09	93∙34±o.i3∕93.41±0.o8	-93∙33±o.o6∕93.45±0.i6-
CIFAR100	AdaBound	69.2i±0.59∕68.02±0.75	7L38±o.44∕70∙57±0.40	72.39±o.27∕71∙67±o.49	72.83±o.i6∕72∙08±o.27	73.15±o.24∕71∙94±o.66
	-AdaGrad-	65.35±o.46∕65.15±0.27	66.72±o.34∕66∙58±o.38	67.03±o.5o∕66∙91±0.3i	67.16±o.5o∕66∙97±0.25	67.43±o.59∕67∙02±o.23
	AdaM	68.31±o.48∕68.66±o.46	69∙71±o.63∕69.78±o.27	70∙43±o.29∕70.45±o.42	70.98±o.43∕70∙61±o.33	~71.43±o.28∕71∙11±o.37-
	AdaS0.8	73.58±o.3i∕74.18±o.32	73∙58±o.36∕74.21±o.35	73∙58±o.36∕74.22±o.35	73∙58±o.36∕74.19±0.24	73∙58±o.36∕74.21±0.26
	AdaS0∙9	75.54±o.25∕75.64±o.25	75∙78±0.2i∕76.O2±0.io	75∙78±o.2i∕76.00±o.i3	75∙78±o.2i∕76.05±o.ii	75∙78±0.2i∕75.99±o.09
	AdaS0∙95	71.25±o.9o∕7L57±i.00	77∙48±o.37∕77.53±o.i8	77∙48±o.37∕77.60±o.22	77∙48±o.37∕77.60±o.22	77∙48±o.37∕77.60±o.22
	AdaS0∙975	62.11±2.oi∕62.49±i.6o	75.15±0.26∕74∙48±o.53	78.44±o.34∕77∙81±o.23	78.15±o.22∕77∙76±o.38	78.26±o.35∕78∙00±o.28
	RMSProP	67.17±i.00∕67.69±o.62	68∙78±i.29∕69.28±o.29	70.23±o.5o∕69∙96±0.48	70∙20±o.49∕70.39±0.50	7O.57±0.4o∕70∙25±0.29
	SLS 一	66.81±i.25∕64.88±i.23	73.48±0.24∕73∙02±o.20	73.78±o.i9∕73∙14±o.2i	73.74±o.32∕73∙24±o.ii	73.77±o.i2∕73∙22±o.ii
TinyImageNet	AdaBound	55.07±i.o6∕54.15±i.i3	55.75±o.34∕55∙37±o.i5	56.66±o.3o∕55∙00±0.85	56.06±o.04∕55∙05±o.58	56.22±0.i7∕55∙48±o.67
	-AdaGrad-	52.01±o.43∕52.75±0.60	54∙01±o.39∕54.27±o.77	54∙46±o.68∕54.94±o.60	54∙69±o.3o∕55.12±0.27	55∙04±o.54/55.81 ±ο.84-
	AdaM	53.98±i.i4∕47.43±i.6o	54.67±o.93∕48∙97±i.86	55.86±0.26∕52∙22±o.5i	54.81±o.73∕50∙14±i.2o	54.46±i.i4∕52∙13±i.i4
	AdaS0.8	58.06±o.54∕57.85±o.55	58.20±o.42∕58∙06±o.48	58.13±o.34∕57∙99±0.36	58.18±0.43∕58∙16±o.40	58.02±o.42∕57∙98±o.44
	AdaS0.9	54.45±o.9i∕58.95±o.45	58∙99±o.25∕59.97±o.33	6O.08±0.4o∕59∙97±0.40	61.38±o.49∕60∙01±o.30	61.56±o.58∕59∙91±o.45
	AdaS0.95	56.18±i.36∕57.41±o.7i	6O.83±0.io∕60∙29±0.30	6L17±o.28∕6O∙72±0.06	6L10±o.38∕61∙03±o.i4	6L28±0.44∕6O∙74±o.20
	AdaS0.975	54.87±i.22∕55.19±o.74	58∙91±o.57∕59.19±o.49	60.62±o.69∕60∙22±o.27	6l.70±0.39∕6l∙i3±0.37	61.81±o.45∕61∙44±0.27
ImageNet	AdaM	66.22±0.08∕62.14±o.29 "	68.17±o.07∕63∙17±0.2i	68.68±o.24∕63∙75±o.08 "	-	-
	AdaS0∙9	7L61±o.io∕71.48±0.35	71∙87±o.2o∕72.11±o.i6	71∙87±o.2o∕72.11±o.i6	-	-
	AdaS0∙95	67.52±o.47∕68.15±0.2i	72.88±o.03∕72∙75±o.34	73.09±o.23∕73∙05±o.i7	-	-
	AdaS0∙975	58.25±o.34∕58.09±o.36	69∙17±o.09∕69.42±0.24	72∙42±o.i5∕72.52±0.03	-	-
Under review as a conference paper at ICLR 2021
2
O
Table 6: List of ResNet18 top-1 test accuracies using various optimizers and applied to various datasets. Note that each cell reports the average accuracy over each
experimental trial, sub-scripted by the standard deviation over the trials. Values on the left are from trials trained using the initial learning rate as generated by
autoHyper. Values on the right are from trials trained using the suggested initial learning rate. Note a ’*’ indicates that early-stop has been activated.
Dataset	Optimizer		EPoCh 50			EPoCh 100			EPoCh 150			EPoCh 200		Epoch 250
CIFAR10	AdaBound	91 .24±0.2i∕90.35±o.5i	92.25±o.2i∕91.64±o.42	92.69±o.i4∕92∙05±o.25	92.79±o.24∕92∙20±o.26	92.85±0.o6∕92∙35±o.i8
	-AdaGrad-	89.42±0.22∕89.67±o.4o	90.39±o.18∕90.79±0.3o	90∙55±o.14∕9L16±0.23	90∙75±o.13∕9L20±o.26	9O∙87±0.i4∕9L23±0.25
	AdaM	91 .43±0.42∕91.29±o.45	92.17±o.36∕92.16±o.i3	92.63±0.i5∕92∙37±o.i7	92.88±0.32∕92∙65±o.2i	92.95±o.24∕92∙93±o.22
	AdaS0.8	92.81±o.i3∕92.87±0.23	92.80±o.i6∕92.92±o.i9	92∙80±o.i6∕92.92±o.i9	92∙80±o.i6∕92.92±o.i9	92∙80±o.i6∕92.92±o.i9
	AdaS0∙9	93.71±0.04∕94.09±o.14	93∙75±0.12∕94.O5±0.io	93∙75±0.12∕94.05±0.io	93∙75±0.12∕94.05±0.io	93∙75±0.12∕94.05±0.io
	AdaS0.95	9L96±0.35∕91.89±i.io	94.69±o.i5∕94.81±o.i6	94∙74±o.16∕94.93±o.ii	94∙74±o.16∕94.93±o.ii	94∙74±o.16∕94.93±o.ii
	AdaS0∙975	87.99±ι.i8∕88.05±ι.i9	93∙35±0.43∕93.46±o.i2	94∙74±0.io∕94.77±o.22	94∙88±o.07∕95.07±0.22	94∙94±0.04∕95.14±o.20
	RMSProP	90.35±i.2i∕90.98±o.36	91 ∙85±o. 19/91.88±0.47	92.26±0.i6∕92∙25±o.i6	92.66±0.i2∕92∙39±o.38	92.69±0.33∕92∙62±o.30
	SLS 一	93.3O±0.16∕93.28±0.io	93∙41±o.09∕93.48±0.09	93∙39±o.io∕93.49±0.09	93∙34±0.13∕93.41±o.08	93∙33±0.06∕93.45±o.i6
CIFAR100	AdaBound	70.15±o.3i∕68.75±o.42	72.29±o.2i∕70∙89±o.26	72.91±0.24∕71∙60±o.40	73.10±o.38∕71∙86±o.i8	73.16±o.25∕72∙04±0.30
	-AdaGrad-	66.07±0.36∕66.12±o.53	67.39±o.48∕67∙37±o.39	67.62±o.52∕67∙50±o.57	67.85±o.57∕67∙72±o.3i	67∙75±0.56∕67.76±o.5O
	AdaM	68.32±o.65∕68.51±o.39	69∙33±o.42∕69.56±o.29	69∙60±o.i8∕70.07±0.3i	69∙72±o.22∕70.49±o.45	70∙09±0.35∕70.34±o.27
	AdaS0.8	73.32±o.3o∕73.48±0.i2	73∙38±0.28∕73.59±o.09	73∙38±0.28∕73.59±o.09	73∙38±o.28∕73.59±o.09	73∙38±o.28∕73.59±o.09
	AdaS0∙9	75.24±0.27∕74.98±o.i5	75.27±0.28∕75∙15±o.i7	75.27±o.28∕75∙15±o.i7	75.27±0.28∕75∙15±o.i7	75.27±0.28∕75∙15±o.i7
	AdaS0∙95	74.40±o. 17/73.86±o. 17	76.44±0.3o∕76∙32±o.33	76∙44±o.34∕76.47±o.3i	76∙49±o.37∕76.53±o.30	76∙49±o.37∕76.53±o.30
	AdaS0∙975	63.62±i.7o∕64.44±o.99	74.21±0.25∕73∙79±o.54	76∙59±0.i9∕77.05±0.io	76∙62±o.i4∕77.15±o.i9	76∙68±o.i8∕77.23±0.o9
	RMSProP	66.29±0.42∕66.52±1.41	68∙48±o.3i∕68.89±o.43	68∙88±o.2o∕69.10±0.36	69∙29±o.55∕69.78±o.46	69∙28±o.5o∕70.08±0.23
	SLS 一	66.81±i.25∕64.88±i.23	73.48±0.24∕73∙02±o.20	73.78±o.19∕73∙14±o.21	73.74±o.32∕73∙24±o.ii	73.77±o.12∕73∙22±o.ii
Under review as a conference paper at ICLR 2021

Table 7: List of ResNeXt50 top-1 test accuracies using various optimizers and applied to various datasets. Note that each cell reports the average accuracy over
each experimental trial, sub-scripted by the standard deviation over the trials. Values on the left are from trials trained using the initial learning rate as generated by
autoHyper. Values on the right are from trials trained using the suggested initial learning rate. Note a ’*’ indicates that early-stop has been activated.
Dataset	Optimizer		EPoCh 50			EPoCh 100			EPoCh 150			EPoCh 200		Epoch 250
CIFAR10	AdaBound	88.71± 1.28/89.01 ±o.55	90.90±0.25∕90.75±o.39	90∙76±o.65∕91.23±o.24	91.63±o.3o∕91∙55±0.29	91.69±o.33∕91∙42±o.42
	-AdaGrad-	87.80±ι.32∕87.85±o.9i	89.25±o.33∕89.37±o.23	89.79±o.27∕89∙63±o.69	90.07±o.27∕89∙80±0.ii	90.13±ο.i9∕90∙07±ο.27
	AdaM	90.11±0.59∕89.74±o.89	9i.i5±0.i9∕9i .28±0.45	91∙68±o.26∕9i.82±o.27	91∙61±o.24∕92.07±0.17	92∙12±0.o7∕92.18±o.3i-
	AdaS0.8	91.51±0.22∕9L56±o.i3	91∙49±0.16∕91.56±0.07	91∙49±0.16∕9L56±o.07	91∙49±0.16∕9L56±o.07	91∙49±0.16∕9L56±o.07
	AdaS0∙9	93.31±0.i5∕93.37±o.ιι	93.51±o.12∕93.60±0.i6	93∙51±o.12∕93.60±0.i6	93∙51±o.12∕93.60±0.i6	93∙51±o.12∕93.60±0.i6
	AdaS0.95	89.03±o.45∕90.49±o.24	94∙57±o.13∕94.61±o.i5	94∙59±0.14∕94.62±o.io	94∙61±o.ii∕94.62±0.io	94∙61±o.ii∕94.62±0.io
	AdaS0∙975	87.65±o.55∕85.33±i.38	92∙07±o.48∕92.29±o.54	94.93±0.07∕94∙86±o.ii	95.O3±0.09∕94∙98±o.i3	95∙02±o.06∕95.03±o.i2
	RMSProP	89.26±0.42∕89.11±o.89	89∙65±o.89∕9O.17±0.30	91∙02±0.36∕9l.63±0.i3	90∙96±o.45∕9i.80±o.i7	9i∙34±0.59∕92.i5±0.20
	SLS 一	93.05±0.3i∕93.18±o.ii	93.52±0.i2∕93∙40±0.io	93.51±ο.2ο∕93∙50±0.i2	93.54±o.22∕93∙47±o.i2	93.56±0.2o∕93∙49±o.i4
CIFAR100	AdaBound	66.66±o.64∕66.75±0.3i	69∙92±o.4o∕70.13±0.40	70∙63±o.23∕7L06±o.5i	71∙02±o.28∕71.35±o.26	71∙2O±0.34∕7L43±0.30
	-AdaGrad-	63.75±0.57∕62.97±o.54	65.27±o.46∕65∙01±o.64	65.36±0.48∕65∙30±o.4i	65.75±0.4i∕65∙65±ο.37	66.03±o.56∕65∙66±0.36
	AdaM	66.74±o.46∕66.47±o.45	68∙4O±0.28∕68.87±o.50	68∙37±o.53∕69.59±0.26	69∙04±o.i8∕70.14±o.35	69∙12±o.i6∕70.32±o.46
	AdaS0.8	72.06±o.4i∕72.39±o.2i	72∙00±o.44∕72.41±o.i6	72∙00±o.44∕72.41±o.i6	72∙00±o.44∕72.41±o.i6	72∙00±o.44∕72.41±o.i6
	AdaS0∙9	74.23±o.28∕74.30±o.ii	74∙41±o.26∕74.43±0.14	74∙41±o.26∕74.43±0.14	74∙41±o.26∕74.43±o.i4	74∙41±o.26∕74.43±o.i4
	AdaS0.95	72.78±o.09∕72.29±o.37	75∙47±o.io∕76.03±0.34	75∙74±0.03∕75.95±o.26	75∙63±ο.i2∕75.95±0.26	75∙63±ο.i2∕75.95±0.26
	AdaS0∙975	63.16±i.84∕63.67±i.48	74∙42±o.47∕74.52±o.44	76∙12±o.i7∕76.27±o.23	76∙45±o.i6∕76.47±o.i8	76.58±0.2i∕76∙46±ο.24
	RMSProP	63.69±0.66∕63.96±i.4o	66∙24±o.28∕67.82±o.82	66∙35±o.26∕67.97±i.i6	66∙83±o.39∕69.31±0.27	67∙17±o.7o∕69.45±i.i7
	SLS 一	55.34±4.77∕58.80±3.56 一	70∙10±ι.i8∕70.75±o.98	71∙51±o.3i∕71.73±o.38 一	7i∙89±0.09∕72.03±0.40 一	7i∙82±0.22∕72.08±0.43 一
Under review as a conference paper at ICLR 2021

Table 8: List of DenseNet121 top-1 test-accuracies using various optimizers and applied to various datasets. Note that each cell reports the average accuracy over
each experimental trial, sub-scripted by the standard deviation over the trials. Values on the left are from trials trained using the initial learning rate as generated by
autoHyper. Values on the right are from trials trained using the suggested initial learning rate. Note a ’*’ indicates that early-stop has been activated.
Dataset	Optimizer		EPoCh 50			EPoCh 100			EPoCh 150			EPoCh 200		Epoch 250
CIFAR10	AdaBound	88.57±0.49∕89.73±o.32	90.09±o.23∕90.64±o.23	90.65±o.24∕91.20±o.09	90.59±o.46∕91.32±0.i2	90∙96±o.48∕91.65±o.2o
	-AdaGrad-	88.57±o.29∕88.31±0.45	89.20±o.23∕89.03±o.27	89.36±o.09∕89.26±o.25	89.47±o.i2∕89.33±0.25	89.85±0.i9∕89∙52±o.i6
	AdaM	90.75±0.28∕89.42±o.72	9i.39±0.32∕90.28±0.49	91.75±o.27∕90.84±o.i6	91.72±0.28∕91.29±o.26	91.86±o.26∕91∙32±o.43
	AdaS0.8	91.61±0.2o∕91.28±o.26	91.59±0.25∕91.28±o.23	91.59±0.25∕91.28±o.23	91.59±0.25∕91.28±o.23	91.59±0.25∕91∙28±o.23
	AdaS0∙9	92.91±0.08∕92.89±o.20	92.97±o.i8∕93.06±o.i4	92.97±o.i8∕93.06±o.i4	92.97±o.i8∕93.06±o.i4	92∙97±o.i8∕93.06±o.i4
	AdaS0.95	91.59±o.36∕91.59±o.23	93.33±0.35∕93.45±o.i8	93.33±0.24∕93.51±o.20	93.33±0.24∕93.51±o.20	93∙33±0.24∕93.51±o.20
	AdaS0∙975	86.96±i.i5∕86.17±o.80	91.87±o.26∕92.06±o.5i	93.17±0.2o∕93.53±o.i9	93.40±o.22∕93.72±o.i5	93∙47±o.24∕93.83±o.20
	RMSProP	88.80±o.93∕88.19±i.18	90.95±o.85∕89.76±i.i5	91.54±o.52∕90.77±o.64	91.65±o.36∕90.95±o.48	91.83±o.3o∕91∙29±0.20
	SLS 一	92.95±0.28∕92.78±o.07	93.35±o.2o∕93.14±o.i6	93.40±0.17∕93.i2±0.15	93.38±0.15∕93.I6±0.11	93.36±o.i8∕93∙16±o.i3
CIFAR100	AdaBound	63.87±o.25∕65.30±o.28	65.61±o.18∕68.00±o.30	66.66±0.22∕68.65±0.21	66.69±o.37∕68.46±o.32	67∙00±0.2o∕68.90±0.36
	-AdaGrad-	61.62±o.46∕61.08±o.28	62.01±o.29∕61.43±o.3i	62.4O±0.50∕61.86±o.50	62.91±o.46∕62.09±o.22	62.79±0.35∕62∙14±o.i5
	AdaM	66.53±o.3o∕64.50±0.33	67.64±o.35∕65.69±o.29	68.32±0.27∕66.42±0.57	68.46±o.28∕66.89±o.53	69.05±o.49∕67∙48±o.i7
	AdaS0.8	70.90±o.ii∕70.59±0.39	71.01±o.28∕70.63±o.33	71.01±o.28∕70.63±o.33	71.01±o.28∕70.63±o.33	71.01±o.28∕70∙63±o.33
	AdaS0∙9	73.11±0.49∕73.13±o.30	73.13±o.44∕73.25±o.25	73.13±o.44∕73.25±o.25	73.13±o.44∕73.25±o.25	73∙13±o.44∕73.25±o.25
	AdaS0.95	72.62±o.5o∕72.90±0.72	73.98±o.2i∕74.09±0.30	73.98±o.33∕74.22±o.24	73∙98±o.33∕74.22±0.24	73∙98±o.33∕74.22±o.24
	AdaS0∙975	61.85±i.59∕63.44±i.9o	72.16±o.56∕72.65±0.85	73.81±o.44∕73.92±o.52	73.88±0.42∕73.99±0.45	73∙97±o.36∕74.10±o.47
	RMSProP	64.73±o.0o∕62.80±0.54	66.22±o.0o∕64.85±0.28	67.57±o.0o∕65.74±0.40	67.35±o.0o∕66∙30±0.42	68.13±0.oo∕66∙61±0.58
	SLS 一	52.01±7.25∕55.66±6.26	65.96±i.2o∕62.29±5.97 一	69.88±0.24∕67.28±1.15 一	70.12±o.45∕68∙63±i.6i	70.25±o.i9∕69∙44±o.6i
Under review as a conference paper at ICLR 2021
Under review as a conference paper at ICLR 2021
D Comparison against State of the Art (Random Search)
D.1 Setup
The search space is set to [1 × 10-4, 0.1] and a loguniform (see SciPy) distribution is used for
sampling. This is motivated by the fact that autoHyper also uses and logarithmically-spaced grid
space. We note that we ran initial tests against a uniform distribution for sampling was done and
showed slightly worse results, as the favouring of smaller learning rates benefits the optimizers we
considered. In keeping with autoHyper’s design, the learning rate that resulted in lowest training
loss after 5 epochs was chosen. One could also track validation accuracy, however as visualized in
Figures 5(a) & 13, validation loss is more stable for the datasets we are considering. This selection
could be altered if the dataset being used exhibits a different behaviour, however this would be a
manual alteration at the selection of the practitioner - one that does not need to be made if using
autoHyper.
D.2 Additional Discussion and Results
Can you replace Z(η) with validation loss? Replacing Z(η) with validation loss does not work
because greedily taking validation loss (or accuracy) is not stable nor domain independent. Analyz-
ing Figures 5(a) & 9, validation loss/accuracy is unstable since either the network (EfficientNetB0
in Figure 9) or the dataset (TinyImageNet in Figure 5(a)) results in unstable top-1 test accuracy/test
loss scores that are unreliable to track. See also Figure 14, which demonstrates the inbaility to track
validation loss/accuracy for various learning rates. Further, validation accuracy/loss can vary greatly
based on initialization, whereas our method does not vary due to its low-rank factorization. Finally,
our metric, Z(η), is always guaranteed to be zero with a sufficiently small learning rate and maxi-
mized with large learning rates, therefore we can always dynamically adapt our search range to the
proper range. This fact is not so true for tracking validation accuracy/loss.
Additionally, low validation loss does not correlate to high validation accuracy (an additional figure,
Figure 12, in Appendix C shows this). You might then suggest to take a k of the best performing
learning rates based on validation accuracy/loss and focus on those, but this requires you to manually
define k then attempt a manually defined Grid/Random Search refinements around those areas, with
manual heuristics to indicate when to stop searching, whereas our method is fully automatic and
self-converges. Not to mention, this would take more time.
In summation, existing SOTA method like Random Search cannot compete with autoHyper when
given similar budgets and minimizing the manual intervention/refinement. This displays autoHy-
per’s prominent feature of being a low-cost, fully automatic algorithm to search for optimal hyper-
parameter bounds (namely in this work, the initial learning rate). Future work could include using
autoHyper to quickly discover this optimal hyper-parameter range, and then further refine using
more extensive HPO methods with greater budgets if truly superior performance is required, and
this could further alleviate a lot of manual refinement that currently plagues existing SOTA meth-
ods.
Table 9: Learning rates for ResNet34 Random Search comparison. Left inner columns show Ran-
dom Search generated, right inner columns show autoHyper generated.
Optimizer	CIFAR10		CIFAR100		TinyImageNet	
AdaM	0.000330	0.000333	0.000125	0.000241	0.000175	0.0001965
AdaBound	0000392	0000347	0000353	0000347	0000124	00000944
AdaGrad	0001598	0002861	0001828	0002236	0000715	00022359
AdaS(0∙9)	0006779	0012374	0009252	0010190	0039480	00085857 -
23
Under review as a conference paper at ICLR 2021
Figure 13: Top-1 test accuracy and train loss for ResNet34 applied to TinyImageNet, CIFAR10,
and CIFAR100, using learning rates as suggested by either a Random Search (as described above)
or autoHyper. Titles below plots indicate what experiment the above plots refers to. Legend labels
marked by ‘*’ (solid lines) show results for autoHyper generated learning rates and dotted lines are
the Random Search results.
---AdaBound ------- AdaGrad ------ AdaM ------ AdaS-0.9
0.00	0.02	0.04	0.06	0.08
ResNet34 on CIFAR10
(a) Validation Loss
--- AdaBound ------ AdaGrad ------ AdaM ------ AdaS-0.9
0.00	0.02	0.04	0.06	0.08
Epoch
ResNet34 on CIFAR10
0.00	0.02	0.04	0.06	0.08
Epoch
ResNet34 on TinyImageNet
0.00	0.02	0.04	0.06
Epoch
ResNet34 on CIFAR100
(b) Validation Accuracy
Figure 14: Visualization of the (a) validation loss and (b) validation accuracy for various learning
rates ResNet34 on various datasets. These figures demonstrate the inability to propoerly track these
metrics as we do ours (i.e. Z(η))
24