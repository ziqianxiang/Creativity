Under review as a conference paper at ICLR 2021
A law of robustness for two-layers neural
NETWORKS
Anonymous authors
Paper under double-blind review
Ab stract
We initiate the study of the inherent tradeoffs between the size of a neural net-
work and its robustness, as measured by its Lipschitz constant. We make a precise
conjecture that, for any Lipschitz activation function and for most datasets, any
two-layers neural network with k neurons that perfectly fit the data must have its
LiPschitz constant larger (UP to a constant) than，n/k where n is the number of
datapoints. In particular, this conjecture implies that overparametrization is neces-
sary for robustness, since it means that one needs roughly one neuron Per dataPoint
to ensure a O(1)-LiPschitz network, while mere data fitting of d-dimensional data
requires only one neuron Per d dataPoints. We Prove a weaker version of this con-
jecture when the LiPschitz constant is rePlaced by an uPPer bound on it based on
the sPectral norm of the weight matrix. We also Prove the conjecture in the high-
dimensional regime n ≈ d (which we also refer to as the undercomPlete case,
since only k ≤ d is relevant here). Finally we Prove the conjecture for Polynomial
activation functions of degree p when n ≈ dp . We comPlement these findings
with exPerimental evidence suPPorting the conjecture.
1 Introduction
We study two-layers neural networks with inPuts in Rd, k neurons, and LiPschitz non-linearity
ψ : R → R. These are functions of the form:
k
x → E a'Ψ(w' ∙ X + b'),	(1)
'=1
with a` , b` ∈ R and w` ∈ Rd for any ` ∈ [k]. We denote by Fk(ψ) the set of functions of the
form (1). When k is large enough and ψ is non-Polynomial, this set of functions can be used to fit
any given data set (Cybenko, 1989; Leshno et al., 1993). That is, given a data set (xi, yi)i∈[n] ∈
(Rd × R)n , one can find f ∈ Fk (ψ) such that
f(xi) = yi,∀i ∈ [n] .	(2)
In a variety of scenarios one is furthermore interested in fitting the data smoothly. For examPle,
in machine learning, the data fitting model f is used to make Predictions at unseen Points x 6∈
{x1, . . . , xn}. It is reasonable to ask for these Predictions to be stable, that is a small Perturbation
ofx should result in a small Perturbation of f (x).
A natural question is: how “costly” is this stability restriction comPared to mere data fitting? In
Practice it seems much harder to find robust models for large scale Problems, as first evidenced in
the seminal PaPer (Szegedy et al., 2013). In theory the “cost” of finding robust models has been
investigated from a comPutational comPlexity PersPective in (Bubeck et al., 2019), from a statistical
PersPective in (Schmidt et al., 2018), and more generally from a model comPlexity PersPective in
(Degwekar et al., 2019; Raghunathan et al., 2019; Allen-Zhu and Li, 2020). We ProPose here a
different angle of study within the broad model comPlexity PersPective: does a model have to be
larger for it to be robust? EmPirical evidence (e.g., (Goodfellow et al., 2015; Madry et al., 2018))
suggests that bigger models (also known as “overParametrization”) do indeed helP for robustness.
Our main contribution is a conjecture (Conjecture 1 and Conjecture 2) on the Precise tradeoffs
between size of the model (i.e., the number of neurons k) and robustness (i.e., the LiPschitz constant
1
Under review as a conference paper at ICLR 2021
E
E 15
0	10	20	30	40	50	60
Figure 1: See Section 5 for the details of this experiment.
of the data fitting model f ∈ Fk (ψ)) for generic data sets. We say that a data set (xi, yi)i∈[n] is
generic if it is i.i.d. with xi uniform (or approximately so, see below) on the sphere Sd-1 = {x ∈
Rd : kxk = 1} and yi uniform on {-1, +1}. We give the precise conjecture in Section 2. We prove
several weaker versions of Conjecture 1 and Conjecture 2 respectively in Section 4 and Section 3.
We also give empirical evidence for the conjecture in Section 5.
A corollary of our conjecture. A key fact about generic data, established in Baum (1988); Yun
et al. (2019); Bubeck et al. (2020), is that one can memorize arbitrary labels with k ≈ n/d, that
is merely one neuron per d datapoints. Our conjecture implies that for such optimal-size neural
networks it is impossible to be robust, in the sense that the Lipschitz constant must be of order √d.
The conjecture also states that to be robust (i.e. attain Lipschitz constant O(1)) one must necessarily
have k ≈ n, that is roughly each datapoint must have its own neuron. Therefore, we obtain a trade
off between size and robustness, namely to make the network robust it needs to be d times larger
than for mere data fitting. We illustrate these two cases in Figure 1. We train a neural network to fit
generic data, and plot the maximum gradient over several randomly drawn points (a proxy for the
Lipschitz constant) for various values of √d, when either k = n (blue dots) or k = 1dn (red dots).
As predicted, for the large neural network (k = n) the Lipschitz constant remains roughly constant,
while for the optimally-sized one (k = 1dn) the Lipschitz constant increases roughly linearly in √d.
Notation. For Ω ⊂ Rd We define Lipo(f) = Supx=χ,∈Ω f (Xx-X(X)I (if Ω = Rd We omit the
subscript and write Lip(f)), where ∣∣ ∙ ∣∣ denotes the Euclidean norm. For matrices We use ∣∣ ∙
kop, k ∙ kop,*, ∣∣∙ IlF and〈•, •)for respectively the operator norm, the nuclear norm (sum of singular
values), the Frobenius norm, and the Frobenius inner product. We also use these notations for
tensors of higher order, see Appendix A for more details on tensors. We denote c > 0 and C > 0
for universal numerical constants, respectively small enough and large enough, whose values can
change in different occurences. Similarly, by cp > 0 and Cp > 0 we denote constants depending
only on the parameter p. We also write ReLU(t) = max(t, 0) for the rectified linear unit.
Generic data. We give some flexibility in our definition of “generic data” in order to focus on the
essence of the problem, rather than technical details. Namely, in addition to the spherical model
mentioned above, where xi is i.i.d. uniform on the sphere Sd-1 = {x ∈ Rd : ∣x∣ = 1}, we also
consider the very closely related model where xi is i.i.d. from a centered Gaussian with covariance
dId (in particular E[∣xi∣2] = 1, and in fact ∣Xi∣ is tightly concentrated around 1). In both cases we
consider yi to be i.i.d. random signs. We say that a property holds with high probability for generic
data, if it holds with high probability either for the spherical model or for the Gaussian model.
2
Under review as a conference paper at ICLR 2021
2 A conjectured law of robustness
Our main contribution is the following conjecture, which asserts that, on generic data sets, increasing
the size of a network is necessary to obtain robustness:
Conjecture 1 For generic data sets, with high probability1, any f ∈ Fk(ψ) fitting the data2 (i.e.,
satisfying (2)) must also satisfy:
LiPSdT (f) ≥ Cd k
Note that for generic data, with high probability (for n = poly(d)), there exists a smooth interpo-
lation. Namely there exists g : Rd → R with g(xi) = yi, ∀i ∈ [n] and Lip(g) = O(1). This
follows easily from the fact that with high probability (for large d) one has kxi - xj k ≥ 1, ∀i 6= j.
Conjecture 1 puts restrictions on how smoothly one can interpolate data with small neural networks.
A striking consequence of the conjecture is that for a two-layers neural network f ∈ Fk(ψ) to be as
robust as this function g (i.e., Lip(f) = O(1)) and fit the data, one must have k = Ω(n), i.e., roughly
one neuron per data point. On the other hand with that many neurons it is quite trivial to smoothly
interpolate the data, as we explain in Section 3.3. Thus the conjecture makes a strong statement that
essentially the trivial smooth interpolation is the best thing one can do. In addition to making the
prediction that one neuron per datapoint is necessary for optimal smoothness, the conjecture also
gives a precise prediction on the possible tradeoff between size of the network and its robustness.
We also conjecture that this whole range of tradeoffs is actually achievable:
Conjecture 2 Let n, d, k be such that C ∙ n ≤ k ≤ C ∙ n and n ≤ dc where C is an arbitrarily
large constant in the latter occurence. There exists ψ such that, for generic data sets, with high
probability, there exists f ∈ Fk(ψ) fitting the data (i.e., satisfying (2)) and such that
LiPSd-I (f) ≤ Cjk
The condition k ≤ C ∙ n in Conjecture 2 is necessary, for any interpolation of the data must have
LiPschitz constant at least a constant. The other condition on k, namely k ≥ C ∙ n, is also necessary,
for that many neurons is needed to merely guarantee the existence of a data-fitting neural network
with k neurons (see Baum (1988); Yun et al. (2019); Bubeck et al. (2020)). Finally the condition
n ≤ dC is merely used to avoid explicitly stating a logarithmic term in our conjecture (indeed,
equivalently one can replace this condition by adding a multiplicative polylogarithmic term in d in
the claimed inequality).
Our results around Conjecture 2 (Section 3). We prove Conjecture 2 for both the optimal
smoothness regime (which is quite straightforward, see Section 3.3) and for the optimal size regime
(here more work is needed, and we use a certain tensor-based construction, see Section 3.4). In
the latter case we only prove approximate data fitting (mostly to simplify the proofs), and more im-
portantly we need to assume that n is of order dp for some even integer p. It would be interesting
to generalize the proof to any n. While the conjecture remains open between these two extreme
regimes, We do give a construction in Section 3.3 which has the correct qualitative behavior (namely
increasing k improves the Lipschitz constant), albeit the scaling we obtain is n/k instead of，n/k,
see Theorem 1.
Our results around Conjecture 1 (Section 4). We prove a weaker version of Conjecture 1 where
the Lipschitz constant on the sphere is replaced by a proxy involving the spectral norm of the weight
matrix, see Theorem 3. We also prove the conjecture in the optimal size regime, specifically when
n = dp for an integer p and one uses a polynomial activation function of degree p, see Theorem 6.
Forp = 1 (i.e., n ≈ d) we in fact prove the conjecture for abritrary non-linearities, see Theorem 4.
1We do not quantify the “with high probability” in our conjecture. We believe the conjecture to be true
except for an event of exponentially small probability with respect to the sampling of a generic data set, but
even proving that the statement is true with strictly positive probability would be extremely interesting.
2We expect the same lower bound to hold even if one only asks f to approximately fit the data. In fact our
provable variants of Conjecture 1 are based proofs that are robust to only assuming an approximately fitting f .
3
Under review as a conference paper at ICLR 2021
Further open problems. Our proposed law of robustness is a first mathematical formalization of
the broader phenomenon that “overparametrization in neural networks is necessary for robustness”.
Ideally one would like a much more refined understanding of the phenomenon than the one given
in Conjecture 1. For example, one could imagine that in greater generality, the law would read
Lipo(f) ≥ F(k, (xi, yi)i∈[n], Ω). That is, We would like to understand how the achievable level
of smoothness depends on the particular data set at hand, but also on the set where we expect to
be making predictions. Another direction to generalize the law would be to extend it to multi-
layers neural networks. In particular one could imagine the most general law would replace the
parameter k (number of neurons) by the type of architecture being used and in turn predict the
best architecture for a given data set and prediction set. Finally note that our proposed law apply
to all neural networks, but it would also be interesting to understand how the law interacts with
algorithmic considerations (for example in Section 5 we use Adam Kingma and Ba (2015) to find a
set of weights that qualitatively match Conjecture 2).
3 Smooth interpolation
We start with a warm-up in Section 3.1 where we discuss the simplest case of interpolation with
a linear model (k = 1, n ≤ d) and in Section 3.2 for the optimal smoothness regime (k = n).
We generalize the construction of Section 3.2 in Section 3.3 to obtain the whole range of tradeoffs
between k and Lip(f), albeit with a suboptimal scaling, see Theorem 1. We also generalize the
linear model calculations of Section 3.1 in Section 3.4 to obtain the optimal size regime for larger
values of n via a certain tensor construction.
3.1	THE SIMPLEST CASE： OPTIMAL SIZE REGIME WHEN n ≤ C ∙ d
Let Us consider k = 1, n ≤ C ∙ d and ψ(t) = t. Thus we are trying to find W ∈ Rd such that
W ∙ Xi = yi for all i ∈ [n], or in other words XW = Y with X the n X d matrix whose ith row is xi,
and Y = (y1, . . . , yn). The smoothest solution to this system (i.e., the one minimizing kwk) is
W=X>(XX>)-1Y,
Note that	______________
Lip(X → w ∙ x) = ∣∣wk = √w>w = YY>(XX>)-1Y .
Using [Theorem 5.58, Vershynin (2012)] one has with probability at least 1 - exp(C - Cd) (and
using that n ≤ C ∙ d) that
XX > 之 2 In ,
and thus Lip(χ → w ∙ x) ≤ √2 ∙ ∣∣Y∣∣ = √2n. This concludes the proof sketch of Conjecture 2 for
the simplest case k = 1 and n ≤ d.
3.2	Another simple case: optimal smoothness regime
Next we consider the optimal smoothness regime in Conjecture 2, namely k = n. First note that, for
generic data and n = poly(d), with high probability the caps Ci := {x ∈ Sd-I : Xi ∙ X ≥ 0.9 are
disjoint sets and moreover they each contain a single data point (namely xi). With a single ReLU unit
it is then easy to make a smooth function (10-Lipschitz) which is 0 outside of Ci and equal to +1 at
xi (in other words the neuron activates for a single data point), namely X → 10 ∙ ReLU (xi ∙ X — 0.9).
Thus one can fit the entire data set with the following ReLU network which is 10-Lipschitz on the
sphere:
n
f (x) = ^X 10yi ∙ ReLU (xi ∙ x — 0.9).
i=1
This concludes the proof of Conjecture 2 for the optimal smoothness regime k = n.
3.3	Intermediate regimes via ReLU networks
We now combine the two constructions above (the linear model of Section 3.1 and the “isolation”
strategy of Section 3.2) to give a construction that can trade off size for robustness (albeit not opti-
mally according to Conjecture 2), see Appendix C for the proof.
4
Under review as a conference paper at ICLR 2021
Theorem 1 Let n, d, k be such that C ∙ n log(n) ≤ k ≤ C ∙ n. For generic data sets, with probability
at least 1 - 1/nC, there exists f ∈ Fk (ReLU) fitting the data (i.e., satisfying (2)) and such that
n log(d)
LiPSd-I (f) ≤ C ∙---------.
k
3.4 Optimal size networks via tensor interpolation
In this section We essentially prove Conjecture 2 in the optimal size regime (namely k ∙ d ≈ n), with
three caveats:
1.
2.
3.
We allow a slack of a log n factor by considering k ∙ d = Cn log(n) instead of the optimal
k ∙ d = Cn as in Baum (1988); BUbeCk et al. (2020).
We only prove approximate fit rather than exact fit. It is likely that with more work one
can use the core of our argument to obtain exact fit. For that reason we did not make any
attempt to optimize the dependency on ε in Theorem 2. For instance one could probably
obtain log(1∕ε) rather than 1∕poly(ε) dependency by using an iterative scheme that fits the
residuals, as in (Bresler and Nagaraj, 2020; Bubeck et al., 2020).
We assume that n is of order dp for some even integer p. While it might be that one can
apply the same proof for odd integers, the whole construction crucially relies on p being an
even integer as we essentially do a linear regression over the feature embedding X → x0p.
A possible approach to extend the proof to other values of n would be use the scheme of
Section 3.3 with the linear regression there replaced by the tensor regression used below.
Theorem 2 Fix ε > 0, p an even integer, and let ψ(t) = tp. Let n, d, k be such that n log(n) =
ε2 ∙ dp and k = Cp ∙ dp-1. Thenforgeneric data, with probability at least 1 一 ∖∕nc, there exists
f ∈ Fk (ψ) such that
(3)
∣f(xi) - yi| ≤ Cp ∙ ε,∀i ∈ [n],
and
LiPSd-I (f) ≤ CpJk
Proof. We propose to approximately fit with the following neural network:
n
f (χ) = Xyi(χi ∙ χ)p.
i=1
Naively one might think that this neural network requires n neurons. However, it turns out that one
can always decompose a symmetric tensor of order p into k = 2pdp-1 rank-1 symmetric tensors of
order p, so that in fact f ∈ Fk(ψ). For p = 2 this simply follows from eigendecomposition and for
general p we give a simple proof in [Appendix A, Lemma 2].
One also has by applying [Appendix B, Lemma 4] with τ = Cp log(n) and doing an union bound,
that with probability at least 1 - 1/nC, for any j ∈ [n],
X yi(Xi∙Xj)p ≤ Cp^n ≤ Cpε.
i=1,i6=j
In particular this proves (3).
Thus it only remains to estimate the Lipschitz constant, which by [Appendix A, Lemma 1] is reduced
to estimating the operator norm of the tensor PZi yiX)p. We do so in [Appendix B, Lemma 5]. □ 4
4 Provab le weaker versions of Conjecture 1
Conjecture 1 can be made weaker along several directions. For example the quantity of interest
LiPSd-1 (f) can be replaced by various upper bound proxies for the Lipschitz constant. A mild
5
Under review as a conference paper at ICLR 2021
weakening would be to replace it by the Lipschitz constant on the whole space (we shall in fact only
consider this notion here). A much more severe weakening is to replace it by a quantity that depends
on the spectral norm of the weight matrix (essentially ignoring the pattern of activation functions).
For the latter proxy we actually give a complete proof, see Theorem 3, which in particular formally
proves that “overparametrization is a law of robustness for generic data sets”. Other interesting
directions to weaken the conjecture include specializing it to common activation functions, or simply
having a smaller lower bound on the Lipschitz constant. In Section 4.2 we prove the conjecture when
n is replaced by d in the lower bound. We say that this inequality is in the “very high-dimensional
case”, in the sense that it matches the conjecture for n ≈ d (alternatively we also refer to it as
the “undercomplete case”, in the sense that only k ≤ d is relevant in this very high-dimensional
scenario). In the moderately high-dimensional case (n	d) the proof strategy we propose in
Section 4.2 cannot work. In Section 4.3 we give another argument for the latter case, specifically
in the optimal size regime (i.e., k ∙ d ≈ n) and for a power activation function, see Theorem 5. We
generalize this to polynomial activation functions in Section D.1. In the specific case ofa quadratic
activation function we also show a lower bound that applies for any k and which is in fact larger
than the one given in Conjecture 1, see Theorem 7 in Section D.2.
4.1 Spectral norm proxy for the Lipschitz constant
We can rewrite (1) as
f(x) =a>ψ(Wx+b),	(4)
where a = (a1, . . . , ak) ∈ Rk, b = (b1, . . . , bk) ∈ Rk, W ∈ Rk×d is the matrix whose `th row is
w` , and ψ is extended from R → R to Rk → Rk by applying it coordinate-wise. We prove here the
following:
Theorem 3 Assume that ψ is L-Lipschitz. For f ∈ Fk (ψ) one has
Lip(f) ≤ L ∙∣∣a∣H∣Wkop .	(5)
For a generic data set, if f(xi) = yi, ∀i ∈ [n] and f has no bias terms (i.e., b = 0 in (4)), then with
positive probability one has:
L ∙kak∙kW kop ≥ √f.	(6)
Note that we prove the inequality (6) only with positive probability (i.e., there exists a data set
where the inequality is true), but in fact it is easy to derive the statement with high probability using
classical concentration inequalities.
Proof. Since ψ : R → R is L-Lipschitz, we have:
f(x)-f(x0) ≤ ka∣∣∙kΨ(Wx+b)-ψ(Wx0+b)k ≤ L』a『kWx—Wx0k ≤ L』a『||W∣∣op∙∕x-x'k,
which directly proves (5).
Next, following the proof of [Proposition 1, Bubeck et al. (2020)] one obtains that for a generic data
set, with positive probability, one has (without bias terms):
X |a'| ∙ kw`k ≥ ^L.
'=1
It only remains to observe that:
k-	k	k	k
Ln ≤ X la`l ∙ kw`k ≤ t X la`l2 ∙ X kw`k2 = kak ∙ k W∣F ≤ √ ∙ ∣a∣ ∙ ∣Wkop ,
'=1	' '=1	'=1
which concludes the proof of (6).

6
Under review as a conference paper at ICLR 2021
4.2 Undercomplete case
Next we prove the conjecture in the high dimensional case n ≈ d. More precisely we replace n
by d in the conjectured lower bound. Importantly note that the resulting lower bound then becomes
non-trivial only in the regime k ≤ d (the “undercomplete case”).
We consider in fact a slightly more general scenario than interpolation with a neural network, namely
we simply assume that one interpolates the data with a function f (x) = g(Px) where P is a linear
projection on a k-dimensional subspace (this clearly generalizes f ∈ Fk(ψ), in fact it even allows
for the non-linearity ψ to depend on the data3, or to have a different non-linearity for each neuron).
Theorem 4 Let n ≥ d. Let f : Rd → R be a function such that f(xi) = yi, ∀i ∈ [n] and moreover
f(x) = g(Px) for some differentiable function g : Rk → R and matrix P ∈ Rk×d. Then, for
generic data, with probability at least 1 - exp(C - cd) one must have
Lip(f) ≥ c∕d.
Proof. Let us modify g so that P is simply an orthogonal projection operator (i.e., P P > = Ik). Let
us also assume for sake of notational simplicity that we have a balanced data set of size 2n, that is
with: y1, . . . yn = +1 and yn+1, . . . , y2n = -1. Let us denote x0i = xi - xn+i for i ∈ [n]. The
sequence Xi is i.i.d. and satisfies E[χiχi>] = dId.
Now observe that on the segment [xi , xn+i] (whose length is less than 2), the function f changes
value from +1 to -1, and thus there exists zi ∈ [xi, xn+i] such that:
1 ≤〔▽/(Zi) ∙ (Xi - Xn+i)∣ =1▽/(Zi) ∙ xi∣ .
Moreover one has (using that Vf (x) = P>Vg(χ), and thus ∣∣Vg(χ)k = ∣∣P▽/(X)Il ≤ Lip(f))
Vf(Zi) ∙ χi| = IVg(PZi) ∙ (PXi)I ≤ Lipf) ∙ IIPxik.
Combining the two above displays one has:
n
Lipf)
n
≤ X ∣PX0i ∣ ≤
i=1
nu
nX∣PX0i∣2= u
i=1
nu
n X X0i>P>PX0i = u
i=1
nhXX0iX0i>,P>PiHS .
i=1
∖
Using [Theorem 5.39, Vershynin (2012)] (specifically (5.23)) we know that with probability at least
1 - exp(C - cd) we have Pin=1 X0iX0i>op ≤Cd (here we use n ≥ d too). Moreover we have
kP>Pkop,* = Tr(P>P) = Tr(PP>) = k. Thus we have En=I xixi>,P>P〉hs ≤ Cnddk so that
with the above display one obtains Lipf)≤ n JCk, which concludes the proof.	□
4.3 Power activation
We prove here the conjecture for the power activation function ψ(t) = tp with p an integer and with
no bias terms (we deal with general polynomials, including with bias, in Appendix D). Without bias
such a network can be written as:
k
f (x) = X a`(w` ∙ X)P = hT, XOpi ,	(7)
where T = Pk=I a`w^p. As We already saw in the proof of Theorem 2 (see specifically [Appendix
A, Lemma 2]), without loss of generality we have k ≤ Cpdp-1. We now prove that tensor net-
works of the form (7) cannot obtain a Lipschitz constant4 better than ʌ/n/dp-1, in accordance with
Conjecture 1 for full rank tensors (where k ≈ dp-1).
3 It would be interesting to study whether allowing data-dependent non-linearities could affect the conclusion
of our conjectures. Such study would need to crucially rely on having only one hidden layer, as it is known
from the Kolmogorov-Arnold theorem that with two hidden layers and data-dependent non-linearities one can
obtain perfect approximation properties with k ≤ d (albeit the non-linearities are non-smooth).
4 Note that without loss of generality one can assume T to be symmetric, since we only consider how it acts
on x0p. For symmetric tensors one has that the Lipschitz constant on the unit ball is lower bounded by the
operator norm of T thanks to (9)
7
Under review as a conference paper at ICLR 2021
IUe-PEUJoPU出 UJnUJ-XEUJ
201816141210
o¾
O 8 O
O O © D
O
O QXMXW8
s≡3一Pe-16 EOPUe-I EnE一XeE
Figure 3: Scatter plot of maximum random gra-
dient with respect to VZd in optimal smoothness
(blue) and optimal size (red) regimes (Experiment
2)
Figure 2: Scatter plot of maximum random gradi-
ent With respect to ʌ/ɪ With 906 data points (Ex-
periment 1)


Theorem 5 Assume that we have a tensor T oforder P such that (T, XlfP) = y%, ∀i ∈ [n]. Then, for
generic data, with probability at least 1 - C exp(-cpd), one must have
Proof. Denoting Ω = Pn=ι y∣xfp, We have (using y = 1 for the first equality and [Appendix A,
Lemma 3] for the last inequality):
n = hT Ω ≤ kΩkop ∙kTkop,* ≤ dp-1 ∙ kΩkop ∙kTkop .	(8)
Thus we obtain ∣∣T∣∣op ≥ ：,-1%||—, and it only remains to apply [Appendix B, Lemma 5] which
states that with probability at least 1 一 Cexp(-cpd) one has ∣∣Ω∣op ≤ CPA∕dn-1.	□
5 Experiments
We consider a generic dataset from the Gaussian model (i.e., χι,...,χn i.i.d. from N(0, ɪId) and
labels y1, . . . , yn i.i.d from the uniform distribution over {一1, 1} and independent of x1, . . . , xn).
For various values of (n, d, k ) we train two-layers neural networks with k ReLU units and batch nor-
malization (see Ioffe and Szegedy (2015)) between the linear layer and ReLU layer, using the Adam
optimizer (Kingma and Ba, 2015) on the least squares loss. We keep the values of (n, k, d) where the
network successfully memorizes the random labels (possibly after a rounding to {一1, +1}, and such
that prior to rounding the least squares loss is at most some small value ε to be specified later). Given
a triple (n, d, k), suppose the output of the trained network is fn,d,k : Rd → R. We then generate
zι,...,zτ (where T = 1000) i.i.d from the distribution N(0, dId), independently of everything
else and define the “maximum random gradient” to be max∣∈[τ] ∣^fn,k,d(Zi)Il (it is our proxy for
the true Lipschitz constant suPz∈sd-ι ∣∣Vfn,d,k(z)∣). Our experimental results are as follows:
Experiment 1: We ran experiments with n between 100 and 2000, d between 〜50 and 〜n, and
k between 〜10 and 〜n (we also choose ε = 0.02 for the thresholding). In Figure 2 we give a
scatter plot of (nf^, max∣∈t IlVfn,k,d(z∣) k), and as predicted we see a linear trend, thus providing
empirical evidence for Conjecture 1.
Experiment 2: In this experiment, we investigate the two extreme cases k 〜n and k 〜n/d.
We fix n = 104 and sweep the value of d between 10 to 5000 (we also choose ε = 0.1 for the
thresholding). In the first case, we let k = n and in the second case we let k = 10n/d. In Figure 3
we plot √d versus the maximum random gradient (as defined above) for both cases. We observe
a linear dependence between the maximum gradient value and √d when we have k = 10n∕d,
and roughly a constant maximum gradient value when k = n, thus providing again evidence for
Conjecture 1
8
Under review as a conference paper at ICLR 2021
References
James Alexander and Andre Hirschowitz. Polynomial interpolation in several variables. Journal of
Algebraic Geometry, 4(2):201-222,1995.
Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020.
Eric B Baum. On the capabilities of multilayer perceptrons. Journal of complexity, 4(3):193-215,
1988.
Guy Bresler and Dheeraj Nagaraj. A corrective view of neural networks: Representation, memo-
rization and learning. arXiv preprint arXiv:2002.00274, 2020.
Sebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from com-
putational constraints. In International Conference on Machine Learning, pages 831-840, 2019.
Sebastien Bubeck, Ronen Eldan, Yin Tat Lee, and Dan Mikulincer. Network size and weights size
for memorization with two-layers neural networks. arXiv preprint arXiv:2006.02855, 2020.
Pierre Comon, Gene Golub, Lek-Heng Lim, and Bernard Mourrain. Symmetric tensors and sym-
metric tensor rank. SIAM Journal on Matrix Analysis and Applications, 30(3):1254-1279, 2008.
George Cybenko. Approximation by superpositions ofa sigmoidal function. Mathematics of control,
signals and systems, 2(4):303-314, 1989.
Akshay Degwekar, Preetum Nakkiran, and Vinod Vaikuntanathan. Computational limitations in ro-
bust classification and win-win results. volume 99 of Proceedings of Machine Learning Research
(COLT), pages 994-1028, 2019.
Shmuel Friedland and Lek-Heng Lim. Nuclear norm of higher-order tensors. Mathematics of Com-
putation, 87(311):1255-1281, 2018.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pages 448-
456, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward net-
works with a nonpolynomial activation function can approximate any function. Neural networks,
6(6):861-867, 1993.
Zhening Li, Yuji Nakatsukasa, Tasuku Soma, and Andre Uschmajew. On orthogonal tensors and
best rank-one approximation ratio. SIAM Journal on Matrix Analysis and Applications, 39(1):
400-425, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Arkadi Nemirovski. Interior point polynomial time methods in convex programming. Lecture notes,
2004.
Grigoris Paouris, Petros Valettas, and Joel Zinn. Random version of dvoretzky’s theorem in lpn.
Stochastic Processes and their Applications, 127(10):3187-3227, 2017.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Adversarial
training can hurt generalization. In International Conference on Learning Representations, 2019.
9
Under review as a conference paper at ICLR 2021
Bruce Arie Reznick. Sum of even powers of real linear forms, volume 463. American Mathematical
Soc., 1992.
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adver-
sarially robust generalization requires more data. In Advances in Neural Information Processing
Systems, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2013.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed
Sensing: Theory and Practice, pages 210-268. Cambridge University Prteess, 2012.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small relu networks are powerful memorizers: a tight
analysis of memorization capacity. In Advances in Neural Information Processing Systems, pages
15532-15543, 2019.
A Results on tensors
A tensor of order p is an array T = (Ti1,...,ip)i1,...,ip∈[d]. The Frobenius inner product for tensors is
defined by:
d
hT,Si =	X Ti1,...,ipSi1,...,ip,
i1 ,...,ip=1
with the corresponding norm k∙∣∣F. A tensor is said to be of rank 1 if it can be written as:
T = uι 0 ... 0 Up ,
for some uι,...,up ∈ Rd. The operator norm ∣∣ ∙ ∣∣op is defined by:
kTkop =	sup	hT,Si.
S rank 1,kS kF ≤1
For symmetric tensors (i.e., such that the entries of the array are invariant under permutation of the
p indices), Banach’s Theorem (see e.g., [(2.32), Nemirovski (2004)]) states that in fact one has
∣Tkop = sup hT,x0pi .	⑼
x∈Sd-1
We refer to Friedland and Lim (2018) for more details and background on tensors. We now list a
couple of useful results, with short proofs.
Lemma 1 For a tensor T of order p, one has
LipSd-I (x→ hT,x0pi) ≤ P ∙∣Tkop .
Proof. One has for any x, y ∈ Sd-1,
p
∣hT, Xe)Pi - hT, y0pi I	≤ X∣hT, x0p-q+1 0 y的Ti -〈T, x0p-q 0 y的)∣
q=1
≤ p ∙ ∣χ - yk ∙ SUp |〈T,0p=1χqi∣
x1,...,xp∈Sd-1
=p∙kx-y∣HIT kop.
Lemma 2 For any tensor T of order p, there exists w1, . . . , w2pdp-1 ∈ Rd and ξ1, . . . , ξ2pdp-1 ∈
{-1, +1} such that for all x ∈ Rd,
2pdp-1
hT,x0pi = X ξ' ∙ (w` ∙x)p.
'=1
10
Under review as a conference paper at ICLR 2021
Results like Lemma 2 go back at least to Reznick (1992). In fact much more precise results on min-
imal decomposition in rank-1 tensors are known thanks to the work of Alexander and Hirschowitz
(1995). We refer to (Comon et al., 2008) for more discussion on this topic.
Proof. First note that trivially T can be written as:
d
T = X	eiι 0 …0 eip-1 0 T[iι,...,ip-i, 1： d] .	(10)
i1,...,ip-1=1
Thus one only needs to prove that a function of the form X → QP=ι(wq ∙ x) can be written as the
sum of 2p functions of the form (w0 ∙ x)p. To do so note that, with εq i.i.d. random signs,
p	pp	p
Πεq∙ Σ (Πεqrwqr ∙x) I =p!∏(Wq∙ X).
q=1	q1 ,...,qp =1	r=1	q=1

Lemma 3 For any tensor T of order p one has:
kTkop,* ≤ dp-1∙kTkop .
The above result and its proof are directly taken from Li et al. (2018). We only repeat the argument
here for sake of completeness.
Proof. Note that the decomposition (10) is orthogonal, and thus for any tensor S of order p one has:
hT, Si ≤
d
dp-1 ∙	X	heiι 0 ... 0 eip-ι 0 T[iι,..., ip-1,1： d],Si2
i1 ,...,iP-1=1
d
dp-1 ∙ ∣S∣0p ∙ X	∣T[iι,...,ip-i, 1： d]∣2
i1 ,...,iP-1=1
≤t
=dp-1 ∙kSkop ∙kTkF .
Thus one has ∣∣Tk0p,* ≤ dP-I ∙ ∣∣T∣∣f. By duality one also has ∣∣T∣∣op ≥ d-P-I ∙ ∣∣T∣∣f, which
concludes the proof. B
B Results on random tensors
Lemma 4 For any fixed X ∈ Sd-1
has:
and generic data, with probability at least 1 - C exp(-cp τ) one
n
Eyi(Xi ∙ x)p
i=1
Proof. Using [Theorem 1, Paouris et al. (2017)] one has, for any fixed X ∈ Sd-1 and τ ≤ n,
P(IdP/2 ^X |xi ∙ x∣p — nσp > Cp√nτ^ ≤ Cexp(-cpτ),
where σp denotes the pth moment of the standard Gaussian. Let us denote n+ = |{i ∈ [n] ： yi =
+1} and T+ = Piiy==+ι xfp, and similarly for n-,T-. Now with probability 1 一 Cexp(-cτ)
(with respect to the randomness of the yi0s) we have
∣n+ 一 n-1 ≤ √nτ .
11
Under review as a conference paper at ICLR 2021
Thus combining the two above displays we obtain with probability at least 1 - C exp(-cp τ),
dp/2 ɪ2 ∣Xi ∙ x∣p —	∣Xi ∙ x∣p
i:yi =+1	i:yi =-1
≤ Cp √nτ+σp∣n+ — n-1 ≤ Cp √nτ,

Lemma 5 For generic data, with probability at least 1 — C exp(—cpd) one has:
n
X?卜X领P
yixi
i=1
op
Proof. Let N bean 4-net of Sd-1 (in particular |N| ≤ Cp). By an union bound and Lemma 4 one
has:
P ^∃X ∈ Nε : Xyi∣Xi∙x∣p > Cp^dp-T^ ≤ Cexp(-cpd),	(11)
Let T = pn=1 yiχfp. Note that T is symmetric, and thus thanks to (9) and Lemma 1, one has:
kτkop ≤ maχτ产1+ 1 kTkop ,
x∈N	2
and in particular ∣∣T∣∣op ≤ 2maXχ∈N(T, x0p), which together with (11) concludes the proof. □
C Proof of Theorem 1
Let m = % (by assumption m ≤ C ∙ 二以))and assume it is an integer. Let us choose m points
with the same label, say it is the points x1 , . . . , xm with label +1. As in Section 3.1 let w ∈ Rd
be the minimal norm vector that satisfy W ∙ Xi = 1, and thus as we proved there with probability at
least 1 — exp(C — Cd) one has ∣∣w∣ ≤ √2m. Crucially for the end of the proof, also note that the
distribution of W is rotationally invariant. Next observe that with probability at least 1 — 1/nC (with
respect to the sampling of Xm+ι,..., Xn) one has maxi∈{m+1,...,n} |w ∙ Xi| ≤ C ∙ ∣∣w∣ Jlog(n) ≤ ɪ.
In particular the cap C := {x ∈ SdT : W ∙ X ≥ 11 } contains χι,...,Xm but does not contain any Xi,
i > m. Thus the neuron
x → 2 ∙ ReLU (W ∙ X — ɪ),
computes the value 1 at points x1 , . . . , xm and the value 0 at the rest of the training set.
One can now repeat this process, and build the neurons w1, ...,wk (all with norm ≤ √2m), so that
(with well-chosen signs ξ ∈ {-1,1}) the data is perfectly fitted by the function:
k1
f (x) = y？2 ∙ ξ' ∙ ReLU (w` ∙ x — 2
It only remains to estimate the Lipschitz constant. Note that if a point x ∈ Sd-1 activates a certain
subset A ⊂ {1,...,k} of the neurons, then the gradient at this point is P'∈a w' with w' = 2ξ'W'.
Using that the w`0 are rotationally invariant, one also has with probability at least 1 — C n exp(—cd)
that ∣∣P'∈a w'∣∣2 ≤ C ∙∣A∣∙ m for all A ⊂ {1,..., k}. Thus it only remains to control how large
A can be. We show below that |A| ≤ Cm log(d) with probability at least 1 — C exp(—cd log(d))
which will conclude the proof.
12
Under review as a conference paper at ICLR 2021
If X activates neuron ' then w` ∙ X ≥ 2 ≥ 4√k. Now note that for any fixed X ∈ SdT and fixed
A ⊂ [k], P (∀' ∈ A,w' ∙ x ≥ k√m) ≤ Cexp(-c∣A∣md), so that
P ( ∃A ⊂ [k] : |A| = a and ∀' ∈ a,w` ∙ X ≥ kw'k
≤ exp CCa log(k) - ca—
m
In particular we conclude that with a = Cm log(d) the probability that a fixed point on the sphere
activates more than a neuron is exponentially small in d log(d) (recall that m log(k) ≤ cd by as-
sumption). Thus we can conclude via an union bound on an ε-net that the same holds for the entire
sphere simultaneously. This concludes the proof.
D Further results around Conjecture 1
D. 1 Polynomial activation
We now observe that one can generalize Theorem 5 to handle biases (the parameters bl in 1), and in
fact even general polynomial activation function. Indeed, observe that any polynomial of hw, Xi + b
must also be a polynomial in hw, Xi, albeit with different coefficients.
Theorem 6 Letψ(t) =	qp=0 αqtq and assume that we have f ∈ Fk (ψ) such that f (Xi) = yi, ∀i ∈
[n]. Then, for generic data, with probability at least 1 - Cexp(-cpd) one must have
Lip{x"∣x∣∣≤1}f) ≥ Cp
n
Proof. Note that for f ∈ Fk(ψ) there exists tensors T0, . . . , Tp, such that Tq is a tensor of order q,
and f can be written as:
p
f(x) = EhTq,X瓯i .
q=0
Now let Us define Ωq = Pn= 1 yixfq, and observe that
np
n = X yif (Xi) = XhTq, ωi,
i=1	q=0
and thus there exists q ∈ {1,...,p} such that hTq, Ωqi ≥ Cpn (We ignore the term q = 0 by
considering the largest balanced subset of the data, i.e. we assume Pin=1 yi = 0). Now one can
repeat the proof of Theorem 5 to obtain that with probability at least 1 - Cexp(-Cpd), one has
IlTq IIoP ≥ Cppdp-. It only remains to observe that the Lipschitz constant of f on the unit ball is
lower bounded by kTqkop .
As we mentioned in Section 4.3, without loss of generality we can assume Tq is symmetric, and
thus by (9) there exists X ∈ SdT such that ∣∣Tq∣∣op = hTq, X0qi. Now consider the univariate
polynomial P(t) = f (tX). By Markov brothers’ inequality one has maxt∈[-1,1] P(t) ≥ |P (q)(0)| =
q! ∙ ∣hTq,X0qi| = q! ∙ ∣∣Tq∣∣op, thus concluding the proof.	□
D.2 Quadratic activation
In Section 4.3 we obtained a lower bound for tensor networks that match Conjecture 1 only when
the rank of the corresponding tensor is maximal. Here we show that for quadratic networks (i.e.,
p = 2) we can match Conjecture 1, and in fact even obtain a better bound, for any rank k:
Theorem 7 Assume that we have a matrix T ∈ Rd×d with rank k such that:
hT,X?2i = yi, ∀i ∈ [n].
Then, for generic data, with probability at least 1 - C exp(-Cd), one must have
∣T ∣op
nndt
≥ C k
(≥ eʌ/n/k) .
13
Under review as a conference paper at ICLR 2021
Proof. The proof is exactly the same as for Theorem 5, except that in (8), instead of using Lemma
3 we use the fact that for a matrix T of rank k one has:
kTl∣op,* ≤ k ∙ kTIlop .
14