Under review as a conference paper at ICLR 2021
Dynamic of Stochastic Gradient Descent with
State-Dependent Noise
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD) and its variants are mainstream methods to
train deep neural networks. Since neural networks are non-convex, more and
more works study the dynamic behavior of SGD and its impact to generalization,
especially the escaping efficiency from local minima. However, these works
make the over-simplified assumption that the distribution of gradient noise is state-
independent, although it is state-dependent. In this work, we propose a novel
power-law dynamic with state-dependent diffusion to approximate the dynamic
of SGD. Then, we prove that the stationary distribution of power-law dynamic is
heavy-tailed, which matches the existing empirical observations. Next, we study
the escaping efficiency from local minimum of power-law dynamic and prove that
the mean escaping time is in polynomial order of the barrier height of the basin,
much faster than exponential order of previous dynamics. It indicates that SGD
can escape deep sharp minima efficiently and tends to stop at flat minima that have
lower generalization error. Finally, we conduct experiments to compare SGD and
power-law dynamic, and the results verify our theoretical findings.
1 Introduction
Deep learning has achieved great success in various AI applications, such as computer vision, natural
language processing, and speech recognition (He et al., 2016b; Vaswani et al., 2017; He et al., 2016a).
Stochastic gradient descent (SGD) and its variants are the mainstream methods to train deep neural
networks, since they can deal with the computational bottleneck of the training over large-scale
datasets (Bottou & Bousquet, 2008).
Although SGD can converge to the minimum in convex optimization (Rakhlin et al., 2012), neural
networks are highly non-convex. To understand the behavior of SGD on non-convex optimization
landscape, on one hand, researchers are investigating the loss surface of the neural networks with
variant architectures (Choromanska et al., 2015; Li et al., 2018b; He et al., 2019b; Draxler et al.,
2018; Li et al., 2018a); on the other hand, researchers illustrate that the noise in stochastic algorithm
may make it escape from local minima (Keskar et al., 2016; He et al., 2019a; Zhu et al., 2019;
Wu et al., 2019a; HaoChen et al., 2020). It is clear that whether stochastic algorithms can escape
from poor local minima and finally stop at a minimum with low generalization error is crucial to its
test performance. In this work, we focus on the dynamic of SGD and its impact to generalization,
especially the escaping efficiency from local minima.
To study the dynamic behavior of SGD, most of the works consider SGD as the discretization of
a continuous-time dynamic system and investigate its dynamic properties. There are two typical
types of models to approximate dynamic of SGD. (Li et al., 2017; Zhou et al., 2019; Liu et al.,
2018; Chaudhari & Soatto, 2018; He et al., 2019a; Zhu et al., 2019; Hu et al., 2019; Xie et al.,
2020) approximate the dynamic of SGD by Langevin dynamic with constant diffusion coefficient and
proved its escaping efficiency from local minima.These works make over-simplified assumption that
the covariance matrix of gradient noise is constant, although it is state-dependent in general. The
simplified assumption makes the proposed dynamic unable to explain the empirical observation that
the distribution of parameters trained by SGD is heavy-tailed (Mahoney & Martin, 2019). To model
the heavy-tailed phenomenon, Simsekli et al. (2019); SimSekli et al. (2019) point that the variance
of stochastic gradient may be infinite, and they propose to approximate SGD by dynamic driven by
α-stable process with the strong infinite variance condition. However, as shown in the work (Xie
1
Under review as a conference paper at ICLR 2021
et al., 2020; Mandt et al., 2017), the gradient noise follows Gaussian distribution and the infinite
variance condition does not satisfied. Therefore it is still lack of suitable theoretical explanation on
the implicit regularization of dynamic of SGD.
In this work, we conduct a formal study on the (state-dependent) noise structure of SGD and its
dynamic behavior. First, we show that the covariance of the noise of SGD in the quadratic basin
surrounding the local minima is a quadratic function of the state (i.e., the model parameter). Thus, we
propose approximating the dynamic of SGD near the local minimum using a stochastic differential
equation whose diffusion coefficient is a quadratic function of state. We call the new dynamic
power-law dynamic. We prove that its stationary distribution is power-law κ distribution, where κ is
the signal to noise ratio of the second order derivatives at local minimum. Compared with Gaussian
distribution, power-law κ distribution is heavy-tailed with tail-index κ. It matches the empirical
observation that the distribution of parameters becomes heavy-tailed after SGD training without
assuming infinite variance of stochastic gradient in (Simsekli et al., 2019).
Second, we analyze the escaping efficiency of power-law dynamic from local minima and its relation
to generalization. By using the random perturbation theory for diffused dynamic systems, we analyze
the mean escaping time for power-law dynamic. Our results show that: (1) Power-law dynamic
can escape from sharp minima faster than flat minima. (2) The mean escaping time for power-law
dynamic is only in the polynomial order of the barrier height, much faster than the exponential order
for dynamic with constant diffusion coefficient. Furthermore, we provide a PAC-Bayes generalization
bound and show power-law dynamic can generalize better than dynamic with constant diffusion
coefficient. Therefore, our results indicate that the state-dependent noise helps SGD to escape from
sharp minima quickly and implicitly learn well-generalized model.
Finally, we corroborate our theory by experiments. We investigate the distributions of parameters
trained by SGD on various types of deep neural networks and show that they are well fitted by
power-law κ distribution. Then, we compare the escaping efficiency of dynamics with constant
diffusion or state-dependent diffusion to that of SGD. Results show that the behavior of power-law
dynamic is more consistent with SGD.
Our contributions are summarized as follows: (1) We propose a novel power-law dynamic with
state-dependent diffusion to approximate dynamic of SGD based on both theoretical derivation and
empirical evidence. The power-law dynamic can explain the heavy-tailed phenomenon of parameters
trained by SGD without assuming infinite variance of gradient noise. (2) We analyze the mean
escaping time and PAC-Bayes generalization bound for power-law dynamic and results show that
power-law dynamic can escape sharp local minima faster and generalize better compared with the
dynamics with constant diffusion. Our experimental results can support the theoretical findings. 2
2	Background
In empirical risk minimization problem, the objective is L(w) = 1 pn=1 '(xi,w), where xi, i =
1,…,n are n i.i.d. training samples, W ∈ Rd is the model parameter, and ' is the loss function.
Stochastic gradient descent (SGD) is a popular optimization algorithm to minimize L(w). The
update rule is wt+ι = Wt - η ∙ g(w., where g(w. = b Pχ∈s^ Nw'(x,w. is the minibatch
gradient calculated by a randomly sampled minibatch Sb of size b and η is the learning rate. The
minibatch gradient g(wt) is an unbiased estimator of the full gradient g(wt) = VL(Wt), and the term
(g(wt) — g(wt)) is called gradient noise in SGD.
Langevin Dynamic In (He et al., 2019a; Zhu et al., 2019), the gradient noise is assumed to be drawn
from Gaussian distribution according to central limit theorem (CLT), i.e., g(w) - g(w)〜 N(0, C),
where covariance matrix C is a constant matrix for all w. Then SGD can be regarded as the numerical
discretization of the following Langevin dynamic,
dwt = -g(wt )dt + √ηC1/2 dBt,	(1)
where Bt is a standard Brownian motion in Rd and √ηC1/2dBt is called the diffusion term.
α-stable Process Simsekli et al. (2019) assume the variance of gradient noise is unbounded. By
generalized CLT, the distribution of gradient noise is α-stable distribution S(α, σ), where σ is the α-th
moment of gradient noise for given α with α ∈ (0, 2]. Under this assumption, SGD is approximated
by the stochastic differential equation (SDE) driven by an α-stable process.
2
Under review as a conference paper at ICLR 2021
2.1	Related Work
There are many works that approximate SGD by Langevin dynamic and most of the theoretical
results are obtained for Langevin dynamic with constant diffusion coefficient. From the aspect of
optimization, the convergence rate of SGD and its optimal hyper-parameters have been studied in
(Li et al., 2017; He et al., 2018; Liu et al., 2018; He et al., 2018) via optimal control theory. From
the aspect of generalization, Chaudhari & Soatto (2018); Zhang et al. (2018); Smith & Le (2017)
show that SGD implicitly regularizes the negative entropy of the learned distribution. Recently, the
escaping efficiency from local minima of Langevin dynamic has been studied (Zhu et al., 2019;
Hu et al., 2019; Xie et al., 2020). He et al. (2019a) analyze the PAC-Bayes generalization error of
Langevin dynamic to explain the generalization of SGD.
The solution of Langevin dynamic with constant diffusion coefficient is Gaussian process, which
does not match the empirical observations that the distribution of parameters trained by SGD is a
heavy-tailed (Mahoney & Martin, 2019; Hodgkinson & Mahoney, 2020; Gurbuzbalaban et al., 2020).
Simsekli et al. (2019); SimSekli et al. (2019) assume the variance of stochastic gradient is infinite
and regard SGD as discretization of a stochastic differential equation (SDE) driven by an α-stable
process. The escaping efficiency for the SDE is also shown in (Simsekli et al., 2019).
However, these theoretical results are derived for dynamics with constant diffusion term, although
the gradient noise in SGD is state-dependent. There are some related works analyze state-dependent
noise structure in SGD, such as label noise in (HaoChen et al., 2020) and multiplicative noise in (Wu
et al., 2019b). These works propose new algorithms motivated by the noise structure, but they do
not analyze the escaping behavior of dynamic of SGD and the impact to generalization. Wu et al.
(2018) analyze the escaping behavior of SGD with considering the fluctuations of the second order
derivatives and propose the concept linearly stability. In our work, we propose power-law dynamic to
approximate SGD and analyze the stationary distribution and the mean escaping time for it.
3 Approximating SGD by Power-law Dynamic
In this section, we study the (state-dependent) noise structure of SGD (in Section 3.1) and propose
power-law dynamic to approximate the dynamic of SGD. We first study 1-dimensional power-law
dynamic in Section 3.2 and extend it to high dimensional case in Section 3.3.
3.1	Noise Structure of Stochastic Gradient Descent
For non-convex optimization, we investigate the noise structure of SGD around local minima so
that we can analyze the escaping efficiency from it. We first describe the quadratic basin where
the local minimum is located. Suppose w* is a local minimum of the training loss L(W) and
g(w*) = 0. We name the e-ball B(w*, e) with center w* and radius e as a quadratic basin if
the loss function for w ∈ B(w*, ) is equal to its second-order Taylor expansion as L(w) =
L(w*) + 1 (W 一 w*)TH(w*)(w — w*). Here, H(w*) is the Hessian matrix of loss at w*, which is
(semi) positive definite.
Then we start to analyze the gradient noise of SGD. The full gradient of training loss is g(w) =
H(w*)(w — w*). The stochastic gradient IS g(w) = g(w*) + H(w*)(w — w*) by Taylor expansion
where g(∙) and H(∙) are stochastic version of gradient and Hessian calculated by the minibatch. The
randomness of gradient noise comes from two parts: g(w*) and H(w*), which reflects the fluctua-
tions of the first-order and second-order derivatives of the model at w* over different minibatches,
respectively. The following proposition gives the variance of the gradient noise.
Proposition 1 For W ∈ B(w*,e) ⊂ R, the variance of gradient noise is σ(g(w) — g(w))=
σ(g(w*)) + 2ρ(g(w*), H(w*))(w — w*) + σ(H(w*))(w — w*)2, where σ(∙) and ρ(∙, ∙) are the
variance and covariance in terms of the minibatch.
From Proposition 1, we can conclude that: (1) The variance of noise is finite if g(w*) and H(w*) have
finite variance because ρ(g(w*),H(w*)) ≤ yσ(g(w*)) ∙ σ(H(w*)) according to Cauchy-Schwarz
inequality. For fixed w*, a sufficient condition for that g(w*) and H(w*) have finite variance is that
3
Under review as a conference paper at ICLR 2021
the training data x are sampled from bounded domain. This condition is easy to be satisfied because
the domain of training data are usually normalized to be bounded before training. In this case, the
infinite variance assumption about the stochastic gradient in α-stable process is not satisfied. (2) The
variance of noise is state-dependent, which contradicts the assumption in Langevin dynamic.
Notations: For ease of the presentation, We use C(W),σg,σ∏,pg,H to denote σ(g(w)-讥w*)),
σ(g(w*)), σ(H(w*)), p(g(w*),H(w*)) in the following context, respectively. 1
3.2	Power-law Dynamic
According to CLT, the gradient noise follows Gaussian distribution if it has finite variance, i.e.,
g(w) — g(w) →d N(0, C(W)) as b → ∞,	(2)
where →d means “converge in distribution”. Using Gaussian distribution to model the gradient noise
in SGD, the update rule of SGD can be written as:
Wt+1 = Wt — ηg(wt) + ηξt,	ξt 〜N(0,C(w)).	(3)
Eq.3 can be treated as the discretization of the following SDE, which we call it power-law dynamic:
dWt = —g(Wt )dt +	ηC(W)dBt .	(4)
Power-law dynamic characterizes how the distribution of W changes as time goes on. The distribution
density of parameter W at time t (i.e., p(W, t)) is determined by the Fokker-Planck equation (Zwanzig’s
type (Guo & Du, 2014)):
Itp(W, t) = Vp(w, t)g(w) + 2 ∙ V (C(W) ∙ Vp(w, t)).	(5)
The stationary distribution of power-law dynamic can be obtained if we let the left side of Fokker-
Planck equation be zero. The following theorem shows the analytic form of the stationary distribution
of power-law dynamic, which is heavy-tailed and the tail of the distribution density decays at
polynomial order of W — w*. This is the reason why we call the stochastic differential equation in
Eq.4 power-law dynamic.
Theorem 2 The stationary distribution density for 1-dimensional power-law dynamic (Eq.4) is
P(W) = ∙1(C(W))
Z

H ( H (4Pg,H ∙ ArcTan (C0(w)/q4σHσg - 4pg,H))
ησH y∕4σ∏σg - 4pg,H
(6)
where C(W) = σg + 2Pg,H(W — w*) + oh(w — w*)2, Z is the normalization constant and ArCTan(∙)
is the arctangent function.
We make discussions on property of p(W). The decreasing rate of p(W) as W goes away from the
-H
center w* is mainly determined by the term C(W) ησH (because the function ArcTan(∙) is bounded)
which is a polynomial function about W — W* . Compared with Gaussian distribution the probability
density which follows exponential decreasing rate, power-law distribution is less concentrated in the
quadratic basin B(w*, e) and heavy-tailed. We call nH the tail-index of p(w) and denote it as K in
the following context.
We can conclude that the state-dependent noise results in heavy-tailed distribution of parameters,
which matches the observations in (Mahoney & Martin, 2019). Langevin dynamic with constant
diffusion can be regarded as special case of power-law dynamic when PH,g = 0 and σH = 0. In this
case, p(W) degenerates to Gaussian distribution. Compared with α-stable process, we do not assume
infinite variance on gradient noise and demonstrate another mechanism that results in heavy-tailed
distribution of parameters.
We empirically observe the covariance matrix around the local minimum of training loss on deep
neural networks. The results are shown in Figure.1. Readers can refer more details in Appendix
7.1. We have the following observations: (1) The traces of covariance matrices for the deep neural
1In the following context, we assume σg is positive number.
4
Under review as a conference paper at ICLR 2021
W0-0:!0-000
0.01
0
RaCe
Quadratic CUrVe
Irace
QUadratIC Curve
0.02
0-1∙1000-0α"
-5	0	5	10
Distance to the minima χio-4
Quadiatir
-10	-5	0	5	10
Distance to the minima	10-3
-5	0	5	10
Distance Io the minima xιo^4
0.005
-10
TaCe
Quadratic CUrVe
∙010
-5	0	5	10
Distance to the minima χi0^4

(a) CNN layer-1
(b) CNN layer-2 (c) ResNet layer-1 (d) ResNet layer-2
Figure 1: Trace of covariance matrix of gradient noise in a region around local minimum w*. w*
is selected by running gradient descent with small learning rate till it converges. The number at
horizontal axis shows the distance of the point away from w*. (a),(b): Results for plain CNN.
(c),(d):Results for ResNet18.
networks can be well approximated by quadratic curves, which supports Proposition 1. (2) The
minimum of the quadratic curve is nearly located at the local minimum w* . It indicates that the
coefficient of the first-order term ρg,H ≈ 0.
Based on the fact that ρg,H is not the determinant factor of the tail of the distribution in Eq.6 and the
observations in Figure.1, we consider a simplified form of C(w) that C(w) = σg + σH(w - w*)2.
Corollary 3 If C(w) = σg + σH(w - w*)2, the stationary distribution of 1-dimensional power-law
dynamic (Eq.4) is
P(W) = J(I + σHσ-1(w - w*)2)-κ,	⑺
Z
where Z is the normalization constant and K = ∙nH is the tail-index.
The distribution density in Eq.7 is known as the power-law κ distribution (Zhou & Du, 2014) (It is
also named as q-Gaussian distribution in (Tsallis & Bukman, 1996)). As κ → ∞, the distribution
density tends to be Gaussian, i.e., p(w) X exp(- H(W-W*) ). Power-law K distribution becomes more
heavy-tailed as κ becomes smaller. Meanwhile, it produces higher probability to appear values far
away from the center w* . Intuitively, smaller K helps the dynamic to escape from local minima faster.
In the approximation of dynamic of SGD, K equals the signal (i.e., H(w*)) to noise (i.e., ησH)
ratio of second-order derivative at w* in SGD, and K is linked with three factors: (1) the curvature
H(w*); (2) the fluctuation of the curvature over training data; (3) the hyper-parameters including η
and minibatch size b. Please note that σH linearly decreases as the batch size b increases.
3.3 Multivariate Power-Law Dynamic
In this section, we extend the power-law dynamic to d-dimensional case. We first illustrate the
covariance matrix C(w) of gradient noise in SGD. We use the subscripts to denote the element in
a vector or a matrix. We use Σg to denote the covariance matrix of g(w*) and assume that Σg is
isotropic (i.e., Σg = o§ ∙ I). We also assume that Cov(Hi(w*'), Hj(w*)) are equal for all i,j. It can
be shown that C(W) = ∑ (l + (w — w*)T∑hΣ-1(w - w*)). Similarly as 1-dimensional case, We omit
the first-order term (w - w*) in C (w). Readers can refer Proposition 10 in Appendix 7.2 for the
detailed derivation.
We suppose that the signal to noise ratio of Hg (w*) can be characterized by a scalar K, i.e., ηΣH =
1 ∙ H(w*). Then C(W) can be written as
C(W) = Σg (1 + ɪ (w — w*)T H (w*)Σ-1(w — w*)).
ηκ
(8)
Theorem 4 Ifw ∈ Rd andC(w) has the form in Eq.(8) for w ∈ B(w*, ). The stationary distribution
density of power-law dynamic is
p(w) = ɪ[l + ɪ(w — w*)τ H (w*)Σ-1(w — w*)]-κ
Z ηκ
(9)
for w ∈ B(w*, e), where Z is the normalization Constant and K satisfies η∑H = ɪ ∙ H(w*).
5
Under review as a conference paper at ICLR 2021
Remark: The multivariate power-law κ distribution (Eq.9) is a natural extension of the 1-dimensional
case. Actually, the assumptions on Σg and K Can be replaced by just assuming Σg, H(w*), ΣH are
codiagonalized. Readers can refer Proposition 11 in Appendix 7.2 for the derivation.
4 Escaping Efficiency of Power-law Dynamic
In this section, we analyze the escaping efficiency of power-law dynamic
from local minima and its relation to generalization. Specifically, we
analyze the mean escaping time for wt to escape from a basin. As shown
in Figure.2, we suppose that there are two basins whose bottoms are
denoted as a and c respectively and the saddle point b is the barrier
between two basins. The barrier height is denoted as ∆L = L(b) - L(a).
Figure 2
Definition 5 Suppose wt starts at the local minimum a, we denote the time for wt to first reach
the saddle point b as inf {t > 0|w0 = a, wt = b}. The mean escaping time τ is defined as
τ = Ewt [inf {t > 0|w0 = a, wt = b}].
We first give the mean escaping time for 1-dimensional case in Lemma 6 and then we give the mean
escaping time for high-dimensional power-law dynamic in Theorem 7. To analyze the mean escaping
time, we take the following assumptions.
Assumption 1: The loss function around critical points can be written as L(W) = L(w*) + 2 (W —
w*)TH(w*)(w — W* ), where W* is a critical point.
Assumption 2: The system is in equilibrium near minima, i.e., "(『)=0.
Assumption 3: (Low temperature assumption) The gradient noise is small, i.e., ησg	∆L.
These three assumptions are commonly used in analyzing escaping time (Xie et al., 2020; Zhou &
Du, 2014) for a dynamic. Because both a and b are critical points, we can apply Assumption 1 to get
the loss surface around them. We put more discussions about the assumptions in Appendix 7.3.2.
We suppose the basin a is quadratic and the variance of noise has the form that C(w) = σga +σHa (w -
a)2, which can also be written as C(W) = σga + 2HHa-(L(W) - L(a)). Furthermore, we suppose that
C(w) = σga + 2HHa (L(w) - L(a)) on the whole escaping path from a to b (not just near the local
minimum a). It means that the variance of gradient noise becomes larger as the loss becomes larger.
The following lemma gives the mean escaping time of power-law dynamic for 1-dimensional case.
Lemma 6 Suppose that Assumption 1-3 are satisfied and C(w) = σga + 2HHa (L(w) — L(a)) on the
whole escaping path from a to b. The mean escaping time of 1-dimensional power-law dynamic is,
2π
T = --------/
(1 - 2K) √Ha∣HbI
2	K-2
1+------∆L
κησga
(10)
where K = ηHa > 1, Ha and Hb are the second-order derivatives of training loss at local minimum
a and at saddle point b, respectively.
The proof of Lemma 6 is based on the results in (Zhou & Du, 2014). We provide a full proof in
Appendix 7.3.1. For the dynamic near the saddle point, we just assume that its dynamic is the same as
that near the local minimum for simplicity. This assumption is not necessary and we put the extension
to more complex dynamic in Appendix 7.3.3.
We summarize the mean escaping time of power-law dynamic and dynamics in previous works in
Table 1. Based on the results, we have the following discussions.
Comparison with other dynamics: (1) Both power-law dynamic and Langevin dynamic can escape
sharp minima faster than flat minima, where the sharpness is measured by Ha and larger Ha
corresponds to sharper minimum. Power-law dynamic improves the order of barrier height (i.e., ∆L)
from exponential to polynomial compared with Langevin dynamic, which implies a faster escaping
efficiency of SGD to escape from deep basin. (2) The mean escaping time for α-stable process
is independent with the barrier height, but it is in polynomial order of the width of the basin (i.e.,
6
Under review as a conference paper at ICLR 2021
Table 1: Summary of related works and ours. Here, we only show 1-dimensional result for escaping
time in the table for all the three dynamics for ease of the presentation.
Noise distribution	Dynamic	Stationary solution	Escaping time
N (0, σ)	Langevin S(α, σ)	α-stable N(0,σg + oh(W — W*)2)	Power-law (ours)	Gaussian	O ( / 1	exp ( 2δl ) ∣ HaIHbI	ησ J Heavy-tailed	O (ηα ∙ (lb-ai )") Heavy-tailed	O ^√=1= (1 + 2δl)κ- 1)
width=|b - a|). Compared with α-stable process, the result for power-law dynamic is superior in the
sense that it is also in polynomial order of the width (if ∆L ≈ O(|b - a|2)) and power-law dynamic
does not rely on the infinite variance assumption.
Based on Lemma 6, we analyze the mean escaping time for d-dimensional case. Under the low
temperature condition, the probability density concentrates only along the most possible escaping
paths in the high-dimensional landscape. For rigorous definition of most possible escaping paths,
readers can refer section 3 in (Xie et al., 2020). For simplicity, we consider the case that there is
only one most possible escaping path between basin a and basin c. Specifically, the Hessian at saddle
point b has only one negative eigenvalue and the most possible escaping direction is the direction
corresponding to the negative eigenvalue of the Hessian at b.
Theorem 7 Suppose that Assumption 1-3 are satisfied. For w ∈ Rd, we suppose C(w) = Σga +
ηκ (L(W) - L(a)) on the whole escaping path from a to b and there is only one most possible path
path between basin a and basin c. The mean escaping time for power-law dynamic escaping from
basin a to basin c is
2∏p- det(Hb)_ 1 A +	1 δlAk- 1
(1 - 2κ)√det(Ha) lHbel	ηκσe )
(11)
where e indicates the most possible escaping direction, Hbe is the only negative eigenvalue of Hb, σe
is the eigenvalue of Σga that corresponds to the escaping direction, ∆L = L(b) — L(a), and det(∙)
is the determinant of a matrix.
Remark: In d-dimensional case, the flatness is measured by det(Ha). IfHa has zero eigenvalues, we
can replace Ha by Ha+ in above theorem, where Ha+ is obtained by projecting Ha onto the subspace
composed by the eigenvectors corresponding to the positive eigenvalues of Ha . This is because by
Taylor expansion, the loss L(w) only depends on the positive eigenvalues and the corresponding
eigenvectors of Ha, i.e., L(W) = L(a) + 2 (W - a)THa(W - a) = L(a) + 1 (P(W - a))TAr+ P(W - a),
where Ar+ is a diagonal matrix composed by non-zero eigenvalues of Ha and the operator P(∙)
operates the vector to the subspace corresponding to non-zero eigenvalues of Ha . Therefore, the
dimension d in Theorem 7 can be regarded as the dimension of subspace that is composed by
directions with large eigenvalues. It has been observed that most of the eigenvalues in H is very
small (Sagun et al., 2016). Therefore, d will not be a large number and power-law dynamic in
multi-dimensional case will inherit the benefit of that in 1-dimensional case compared with Langevin
dynamic and α-stable process.
The next theorem give an upper bound of the generalization error of the stationary distribution of
power-law dynamic, which shows that flatter minimum has smaller generalization error.
Theorem 8 Suppose that W ∈ Rd and κ > d. For δ > 0, with probability at least 1 — δ, the
stationary distribution of power-law dynamic has the following generalization error bound,
EW〜p(w),X〜P(x)'(W, X) ≤ EW〜P(W)L(W) +
where KL(p∣∣p0) ≤ 2 log 言(H)) + TrjnAH:)—2d + d log 2, P(W) is the StatiOnary distribution
of d-dimensional power-law dynamic, p0(W) is a prior distribution which is selected to be standard
1
KL(p∣∣p0) + log 1 + log n + 2
n—1
7
Under review as a conference paper at ICLR 2021
Gaussian distribution, and P (x) is the underlying distribution of data x, det(∙) and Tr(∙) are the
determinant and trace of a matrix, respectively.
We make the following discussions on results in Theorem 8. For 1-dimensional case, we have
if H > 2(1 ,上),KL divergence is decreasing as H decreases. For d > 1 and fixed Tr(ΣgH-1)
(+ 2κ )
and det(Σg), the generalization error (i.e., Ew〜p(w),X〜p(x)'(w,x) - Ew〜P(W)L(W)) is decreasing as
det(H) decreases, which indicates that flatter minimum has smaller generalization error. Moreover,
if 2d > T r(ηΣgH-1), the generalization error is decreasing as K increases. When K → ∞, the
generalization error tends to that for Langevin dynamic. Combining the mean escaping time and
the generalization error bound, we can conclude that state-dependent noise makes SGD escape from
sharp minima faster and implicitly tend to learn a flatter model which generalizes better.
5 Experiments
In this section, we conduct experiments to verify the theoretical results. We first study the fitness
between parameter distribution trained by SGD and power-law κ distribution. Then we compare the
escaping behavior for power-law dynamic, Langevin dynamic and SGD.
5.1	Fitting Parameter Distribution using Power-Law Distribution
We investigate the distribution of parameters trained by SGD on deep neural networks and use
power-law κ distribution to fit the parameter distribution. We first use SGD to train various types
of deep neural networks till it converge. For each network, we run SGD with different minibatch
sizes over the range {64, 256, 1024}. For the settings of other hyper-parameters, readers can refer
Appendix 7.5.2. We plot the distribution of model parameters at the same layer using histogram.
Next, we use power-law κ distribution to fit the distribution of the parameters and estimate the value
of κ via the embedded function "T sallisQGaussianDistribution[]" in Mathematica software.
We show results for LeNet-5 with MNIST dataset and ResNet-18 with CIFAR10 dataset (LeCun et al.,
2015; He et al., 2016b) in this section, and put results for other network architectures in Appendix
7.5.2. In Figure 3, we report the generalization error (i.e., Test error - Training error) and the values
of κ that best fit the histogram. 2 We have the following observations: (1) The distribution of the
parameter trained by SGD can be well fitted by power-law κ distribution (blue curve). (2) As the
minibatch size becomes larger, κ becomes larger. It is because the noise σH linearly decreases as
minibatch size becomes larger and K = nH-. (3) As K becomes smaller, the generalization error
becomes lower. It indicates that κ also plays a role as indicator of generalization. These results are
consistent with the theory in Section 4.
5.2	Comparison on Escaping Efficiency
We use a 2-dimensional model to simulate the escaping efficiency from minima for power-law
dynamic, Langevin dynamic and SGD. We design a non-convex 2-dimensional function written
as L(W) = n1 pn=1 '(w 一 xi), where '(w) = 15 p2=1 |wj 一 1|2.5 ∙ |wj + 1|3 and training data
Xi 〜N(0,0.0iI2). We regard the following optimization iterates as the numerical discretization of
the power-law dynamic, wt+1 = Wt 一 ηg(wt) + ηλ2，1 + λι(wt 一 w*)2 Θ ξ, where ξ 〜N(0, I2),
λ1, λ2 are two hyper-parameters and stands for Hadamard product. Note that if we set λ1 = 0,
it can be regarded as discretization of Langevin dynamic. We set learning rate η = 0.025, and we
take 500 iterations in each training. In order to match the trace of covariance matrix of stochastic
gradient at minimum point w* with the methods above, λ2 is chosen to satisfy T r(Cov(λ2ξ)) =
T r(Cov(g(w*))).
We compare the success rate of escaping for power-law dynamic, Langevin dynamic and SGD by
repeating the experiments 100 times. To analyze the noise term λ1, we choose different λ1 and
evaluate corresponding success rate of escaping, as shown in Figure.4(c). The results show that: (1)
there is a positive correlation between λ1 and the success rate of escaping; (2) power-law dynamic
can mimic the escaping efficiency of SGD, while Langevin dynamic can not. We then scale the loss
2The training errors under the six settings are almost zero.
8
Under review as a conference paper at ICLR 2021
Batchsize 64
Generalization Error 1.11%
Batchsize 1024
Generalization Error 8.83%
κ=5.16
Batchsize 64
Generalization Error 4.59%
κ=1.08
Batchsize 1024
Generalization Error 5.74%
(a)	LeNet-5 conv layer 2
(b)	ResNet-18 layer 1
Figure 3:	Approximating distribution of parameters (trained by SGD) by power-law dynamic.
Training batchsize, generalization error (i.e., Test error - Training error) and approximated tail-index
κ are shown in the title of each plot. (a): Results for LeNet-5. (b):Results for ResNet-18.
function by 0.9 to make the minima flatter and repeat all the algorithms under the same setting. The
success rate for the scaled loss function is shown in Figure.4(d). We can observe that all dynamics
escape flatter minima slower.
(a) 2-D loss
Γ3r3r25Γ20rl51059
e)sw ssæOnS
(s)sw ssæOnS
SGD O 16 20 24 28 32 36
(Langevin)
A1 in Pewer-Iaw
(b) Trace for covariance (c) Success Rate (sharp) (d) Success Rate (flat)
Figure 4:	(a):Loss surface of L(w) for 2-D model. (b):Trace of covariance matrix around minimum
(1, 1). (c)/(d): Success rate of escaping from the basin of L(w) / 0.9L(w) in repeated 100 runs.
6 Conclusion
In this work, we study the dynamic of SGD via investigating state-dependent variance of the stochastic
gradient. We propose power-law dynamic with state-dependent diffusion to approximate the dynamic
of SGD. We analyze the escaping efficiency from local minima and the PAC-Bayes generalization
error bound for power-law dynamic. Results indicate that state-dependent noise helps SGD escape
from poor local minima faster and generalize better. We present direct empirical evidence to support
our theoretical findings.This work may motivate many interesting research topics, for example, non-
Gaussian state-dependent noise, new types of state-dependent regularization tricks in deep learning
algorithms and more accurate characterization about the loss surface of deep neural networks. We
will investigate these topics in future work.
References
Balduzzi, David, Frean, Marcus, Leary, Lennox, Lewis, JP, Ma, Kurt Wan-Duo, & McWilliams,
Brian. 2017. The shattered gradients problem: If resnets are the answer, then what is the question?
arXiv preprint arXiv:1702.08591.
Bottou, L6on, & Bousquet, Olivier. 2008. The tradeoffs of large scale learning. Pages 161-168 of:
Advances in neural information processing systems.
Chaudhari, Pratik, & Soatto, Stefano. 2018. Stochastic gradient descent performs variational inference,
converges to limit cycles for deep networks. Pages 1-10 of: 2018 Information Theory and
Applications Workshop (ITA). IEEE.
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, Gerard Ben, & LeCun, Yann. 2015.
The loss surfaces of multilayer networks. Pages 192-204 of: Artificial intelligence and statistics.
Draxler, Felix, Veschgini, Kambis, Salmhofer, Manfred, & Hamprecht, Fred. 2018. Essentially No
Barriers in Neural Network Energy Landscape. Pages 1309-1318 of: International Conference on
Machine Learning.
9
Under review as a conference paper at ICLR 2021
Guo, Ran, & Du, Jiulin. 2014. Are power-law distributions an equilibrium distribution or a stationary
nonequilibrium distribution? Physica A: Statistical Mechanics and its Applications, 406, 281-286.
Gurbuzbalaban, Mert, Simsekli, Umut, & Zhu, Lingjiong. 2020. The Heavy-Tail Phenomenon in
SGD. arXiv preprint arXiv:2006.04740.
HaoChen, Jeff Z, Wei, Colin, Lee, Jason D, & Ma, Tengyu. 2020. Shape Matters: Understanding the
Implicit Bias of the Noise Covariance. arXiv preprint arXiv:2006.08680.
He, Di, Xia, Yingce, Qin, Tao, Wang, Liwei, Yu, Nenghai, Liu, Tie-Yan, & Ma, Wei-Ying. 2016a.
Dual learning for machine translation. Pages 820-828 of: Advances in neural information
processing systems.
He, Fengxiang, Liu, Tongliang, & Tao, Dacheng. 2019a. Control Batch Size and Learning Rate to
Generalize Well: Theoretical and Empirical Evidence. Pages 1141-1150 of: Advances in Neural
Information Processing Systems.
He, Haowei, Huang, Gao, & Yuan, Yang. 2019b. Asymmetric Valleys: Beyond Sharp and Flat Local
Minima. Pages 2549-2560 of: Advances in Neural Information Processing Systems.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, & Sun, Jian. 2015. Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification. Pages 1026-1034 of: Proceedings
of the IEEE international conference on computer vision.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, & Sun, Jian. 2016b. Deep residual learning for image
recognition. Pages 770-778 of: Proceedings of the IEEE conference on computer vision and
pattern recognition.
He, Li, Meng, Qi, Chen, Wei, Ma, Zhi-Ming, & Liu, Tie-Yan. 2018. Differential equations for
modeling asynchronous algorithms. Pages 2220-2226 of: Proceedings of the 27th International
Joint Conference on Artificial Intelligence.
Hodgkinson, Liam, & Mahoney, Michael W. 2020. Multiplicative noise and heavy tails in stochastic
optimization. arXiv preprint arXiv:2006.06293.
Hu, Wenqing, Li, Chris Junchi, Li, Lei, & Liu, Jian-Guo. 2019. On the diffusion approximation of
nonconvex stochastic gradient descent. Annals of Mathematical Sciences and Applications, 4(1),
3-32.
Keskar, Nitish Shirish, Mudigere, Dheevatsa, Nocedal, Jorge, Smelyanskiy, Mikhail, & Tang, Ping
Tak Peter. 2016. On large-batch training for deep learning: Generalization gap and sharp minima.
arXiv preprint arXiv:1609.04836.
LeCun, Yann, et al. 2015. LeNet-5, convolutional neural networks. URL: http://yann. lecun.
com/exdb/lenet, 20(5), 14.
Li, Dawei, Ding, Tian, & Sun, Ruoyu. 2018a. Over-parameterized deep neural networks have no
strict local minima for any continuous activations. arXiv preprint arXiv:1812.11039.
Li, Hao, Xu, Zheng, Taylor, Gavin, Studer, Christoph, & Goldstein, Tom. 2018b. Visualizing the
loss landscape of neural nets. Pages 6389-6399 of: Advances in Neural Information Processing
Systems.
Li, Qianxiao, Tai, Cheng, et al. 2017. Stochastic modified equations and adaptive stochastic gradient
algorithms. Pages 2101-2110 of: Proceedings of the 34th International Conference on Machine
Learning-Volume 70. JMLR. org.
Liu, Tianyi, Chen, Zhehui, Zhou, Enlu, & Zhao, Tuo. 2018. Toward deeper understanding of
nonconvex stochastic optimization with momentum using diffusion approximations. arXiv preprint
arXiv:1802.05155.
Mahoney, Michael, & Martin, Charles. 2019. Traditional and heavy tailed self regularization in neural
network models. Pages 4284-4293 of: International Conference on Machine Learning.
10
Under review as a conference paper at ICLR 2021
Mandt, Stephan, Hoffman, Matthew D, & Blei, David M. 2017. Stochastic gradient descent as
approximate bayesian inference. The Journal of Machine Learning Research, 18(1), 4873-4907.
McAllester, David A. 1999. PAC-Bayesian model averaging. Pages 164-170 of: Proceedings of the
twelfth annual conference on Computational learning theory.
Rakhlin, Alexander, Shamir, Ohad, & Sridharan, Karthik. 2012. Making gradient descent optimal
for strongly convex stochastic optimization. Pages 1571-1578 of: Proceedings of the 29th
International Coference on International Conference on Machine Learning.
Sagun, Levent, Bottou, Leon, & LeCun, Yann. 2016. Eigenvalues of the hessian in deep learning:
Singularity and beyond. arXiv preprint arXiv:1611.07476.
Simyekli, Umut, Gurbuzbalaban, Mert, Nguyen, Thanh Huy, Richard, Gael, & Sagun, Levent. 2019.
On the Heavy-Tailed Theory of Stochastic Gradient Descent for Deep Neural Networks. arXiv
preprint arXiv:1912.00018.
Simsekli, Umut, Sagun, Levent, & Gurbuzbalaban, Mert. 2019. A Tail-Index Analysis of Stochastic
Gradient Noise in Deep Neural Networks. Pages 5827-5837 of: International Conference on
Machine Learning.
Smith, Samuel L, & Le, Quoc V. 2017. A bayesian perspective on generalization and stochastic
gradient descent. arXiv preprint arXiv:1710.06451.
Tsallis, Constantino, & Bukman, Dirk Jan. 1995. Anomalous diffusion in the presence of external
forces: exact time-dependent solutions and entropy. arXiv preprint cond-mat/9511007.
Tsallis, Constantino, & Bukman, Dirk Jan. 1996. Anomalous diffusion in the presence of external
forces: Exact time-dependent solutions and their thermostatistical basis. Physical Review E, 54(3),
R2197.
Van Kampen, Nicolaas Godfried. 1992. Stochastic processes in physics and chemistry. Vol. 1.
Elsevier.
Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N,
Kaiser, Eukasz, & Polosukhin, Illia. 2017. Attention is all you need. Pages 5998-6008 of:
Advances in neural information processing systems.
Wu, Jingfeng, Hu, Wenqing, Xiong, Haoyi, Huan, Jun, & Zhu, Zhanxing. 2019a. The Multiplicative
Noise in Stochastic Gradient Descent: Data-Dependent Regularization, Continuous and Discrete
Approximation. CoRR.
Wu, Jingfeng, Hu, Wenqing, Xiong, Haoyi, Huan, Jun, Braverman, Vladimir, & Zhu, Zhanxing.
2019b. On the Noisy Gradient Descent that Generalizes as SGD. arXiv preprint arXiv:1906.07405.
Wu, Lei, Ma, Chao, et al. 2018. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. Advances in Neural Information Processing Systems, 31,
8279-8288.
Xiao, Han, Rasul, Kashif, & Vollgraf, Roland. 2017. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.
Xie, Zeke, Sato, Issei, & Sugiyama, Masashi. 2020. A Diffusion Theory for Deep Learning Dynamics:
Stochastic Gradient Descent Escapes From Sharp Minima Exponentially Fast. arXiv preprint
arXiv:2002.03495.
Zhang, Yao, Saxe, Andrew M, Advani, Madhu S, & Lee, Alpha A. 2018. Energy-entropy competition
and the effectiveness of stochastic gradient descent in machine learning. Molecular Physics,
116(21-22), 3214-3223.
Zhou, Mo, Liu, Tianyi, Li, Yan, Lin, Dachao, Zhou, Enlu, & Zhao, Tuo. 2019. Toward Understanding
the Importance of Noise in Training Neural Networks. In: International Conference on Machine
Learning.
11
Under review as a conference paper at ICLR 2021
Zhou, Yanjun, & Du, Jiulin. 2014. Kramers escape rate in overdamped systems with the power-law
distribution. Physica A: Statistical Mechanics and its Applications, 402, 299-305.
Zhu, Zhanxing, Wu, Jingfeng, Yu, Bing, Wu, Lei, & Ma, Jinwen. 2019. The anisotropic noise in
stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects.
Pages 7654-7663 of: Proceedings of International Conference on Machine Learning.
7 Appendix
7.1	Power-law Dynamic and Stationary Distribution
Theorem 9	(Theorem 2 in main paper) The stationary distribution density for 1-dimensional power-
law dynamic (Eq.4) is
P(W) = J(C(W))
Z

H	( H (4Pg,Η ∙ ArcTan (CO(W)"4σHσg - 4Pp2,H
ησH exp ----------------- V -------------------
ησH 4σH σg - 4P2g,H
where C(W) = σg + 2ρg,H(W — w*)+σ∏(W — w*)2, Z is the normalization constant and ArCTanq)
is the arctangent function.
Proof:We denote thefunction HgH,ArcT^Hw/w；-4"))as hew). AccOrding to the
Fokker-Planck equation, p(W) satisfies
0 = Vp(W)g(w) + η .▽• (C(W)Vp(W))
=V ∙ [(p(w) ∙ VL(W)) + ηC(W)Vp(W)i
=V ∙ hInC(W)-ηHH+1eh(W)V(C(W)η⅛ ∙ e-h(W) ∙ p(w))]
Readers can check the third equality by calculating V(C(w) ηHH ∙ e-h(W) ∙ P(W)) with C(w) = σg +
2ρg,H (w — W*)+ oh (w — W*)2. Because the left side equals zero, we have C (w) ησH ∙ e-h(W) ∙ P(W)
equals to constant. Sop(w) H C(w)-ησH ∙ eh(W) ∙p(w). So we can get the conclusion in the theorem.
Theorem 10	(Corollary 3 in main paper) If C(W) = σg + σH(W — W*)2, the stationary distribution
density of power-law dynamic is
P(w) = Z∙(l + σHσ-1(w - w*)2)-κ,
(12)
where Z = JW(1 + ohσ-1(w — w*)2)-κdw is the normalization constant and K = nH- is the
tail-index.
Proof: According to the Fokker-Planck equation, p(w) satisfies
0 = Vp(w)g(w) + n ∙ V ∙ (C(w)Vp(w))
=V(p(w) ∙ VL(w)) + nV ∙(σg + 2σH(L(w) — L(w*)))Vp(w)
2H
V∙ nC(w)(1 + 2OH(L(w) — L(w*)))-HH
2	Hog
V(1 + 2OH (L(w) — L(w*))) ησH p(w)
Hog
H
2σ	H-
Because the left side equals zero, we have (1 + HH(L(W) — L(w*))) ησHp(w) equals to constant.
2σ	H
Sop(w) h (1 + HH(L(w) — L(w*))) -ησH . So we can get the conclusion in the theorem. □
We plot the un-normalized distribution density for 1-dimensional power-law dynamics with different
κ in Figure 5. For the four curves, we set β = 10. We set κ = 1, 0.5, 0.1, 0 and use green, red,
12
Under review as a conference paper at ICLR 2021
Figure 5: Probability density for power-law dynamic.
purple and blue line to illustrate their corresponding density function, respectively. When κ = 0, it is
Gaussian distribution. From the figure, we can see that the tail for power-law κ-distribution is heavier
than Gaussian distribution.
Actually, for any given time t, the distribution p(w, t) for wt that satisfies power-law dynamic has
analytic form, i.e., p(w, t) Y (1 + n^Ht) (W — w(t))2)-κ, where w(t) = w* + (wo — w*)e-Ht and
σ(t) is a function of σg and t. Readers can refer Eq.18 - Eq.23 in (Tsallis & Bukman, 1995) for the
detailed expression.
7.2	SGD and Multivariate Power-law Dynamic
The following proposition shows the covariance of stochastic gradient in SGD in d-dimensional case.
We use the subscripts to denote the elements in a vector or a matrix.
Proposition 11 For w ∈ Rd, we use C(w) to denote the covariance matrix of stochastic gradient
g(w) = g(w*)+H(W—w*) and Σ to denote the covariance matrix of g(w*). If Cov(gi(w ),Hjk)=
0, ∀i, j, k, we have
Cij (w) = Σij + (w — w*)T A(ij) (w — w*),	(13)
where Σij = Cov(ggi(w*), ggj(w*)), A(ij) is a d × d matrix with elements A(aibj) = Cov(Hgia, Hgjb)
with a ∈ [d], b ∈ [d].
Eq.13 can be obtained by directly calculating the covariance of ggi (w) and ggj (w) where ggi (w) =
ggi(w*) + Pda=1 Hgia(wa — wa*), ggj(w) = ggj (w*) + Pbd=1 Hgjb(wb — wb*).
In order to get a analytic tractable form of C (w), we make the following assumptions: (1) If Σij = 0,
A(ij) is
a zero matrix; (2) For Σj = 0, A∑(ij) are equal for all i ∈ [d],j ∈ [d]. The first assumption is
reasonable because both Σij and A(ij) reflect the dependence of the derivatives along the i-th direction
and j-th direction. Let ∑h = A∑r-, C(W) can be written as C(W) = Σg(1 + (w—w*)t∑h(w—w*)).
The d-dimensional power-law dynamic is written as
dwt = —H (W — w*)dt + η ηC (w) dBt,
(14)
where C(w) = Σg(1 + (w — w*)T ΣH (w — w* )) which is a symmetric positive definite matrix that
C(w)1/2 exists. The following proposition shows the stationary distribution of the d-dimensional
power-law dynamic.
Proposition 12 Suppose Σg, ΣH, H are codiagonalizable, i.e., there exist orthogonal matrix Q and
diagonal matrices Λ, Γ, Π to satisfy Σg = QT ΛQ, ΣH = QT ΓQ, H = QT ΠQ. Then, the stationary
distribution of power-law dynamic is
p(w) = ɪ(l + (w — w*)T ∑h (w — w*))-κ,
Z
where Z is the normalization constant and κ
Tr(H)
nTr(∑HΣg).
(15)
13
Under review as a conference paper at ICLR 2021
Proof: Under the codiagonalization assumption on Σg, ΣH, H, Eq.15 can be rewritten as dvt =
—Πvtdt + pηA(1 + vTΓvt)dBt if we let Vt = Q(Wt — w*).
We use	φ(v)	=	nC(v)	=	2A(1 +	VTΓv),	the stationary probability density	P(V)	satisfies the
Smoluchowski equation:
0=XX ∂Vi (πivi ∙P(V))+XX ∂Vi ∙ "w) ∂i P(V))
i=1	i=1
d∂	d∂	∂
=X 而 (πi∙vi∙P(V))+X 瓯∙ (I+V rv)而P(V)).
i=1	i=1
According to the result for 1-dimensional case, We have the expression of p(v) is P(V) 8
vT Γv)-κ. To determine the value of κ, we putp(v) in the Smoluchowski equation to obtain
dd
X ∏ip(V) - 2κ X ΠiVi ∙ ΓiVi ∙ (1 + VTΓv)-k-1
i=1
i=1
(16)
(17)
(1 +
d∂
=X ∂V (ηAiκ(1 + VTΓv)	∙ ΓiVi)
i=1	i
dd
=X (ηAiκ(1+ VTΓv)-k ∙Γi) - 2X (ηΛiK2(1 + VTΓv)-k-1 ∙ (ΓiVi)2).
i=1	i=1
The We have Pd=I ∏i = ηκ Pd=I Airi. So We have K =仃舐2)∙ 口
According to Proposition 11, We can also consider another assumption on Σg , ΣH, H Without
assuming their codiagonalization. Instead, We assume (1) If Σij = 0, A(ij) is a zero matrix; (2) For
∑ij = 0, A(ij) are equal for all i ∈ [d], j ∈ [d] and we denote A(ij) = ∑h. We suppose η ∙ ∑h = KH.
(3) Σg = σg ∙ Id which is isotropic. Under these assumptions, we can get the following theorem.
Theorem 13 (Theorem 4 in main paper) If w is d-dimensional and C(w) has the form in Eq.(8).
The stationary distribution density of multivariate power-law dynamic is
p(w) = ɪ[l + ɪ(w - w*)TH∑-1(w — w*)]-κ
Z ηκ	g
where Z = ∕∞∞[1 + 全(W 一 w*)TH∑-1(w 一 w*)]-κdw is the normalization constant.
(18)
The proof for Theorem 12 is similar to that for Proposition 11. Readers can check that p(W) satisfies
the Smoluchowski equation.
An example to illustrate why C(W) is diagonally dominant. In Theorem 13, C(W) is assumed
to be diagonally dominant. Diagonally dominant indicates that the variance of each dimension of
g(w) is significantly larger than the covariance of two different dimensions of g(w). Consider a two
layer fully-connected linear neural network fw,v (x) = WVx where W ∈ R1×m, V ∈ Rm×d, x ∈ Rd
and h(∙) is the ReLU activation. We consider the regression loss '(w, V) = 2(y — fw,v(x))2. The
gradient of Wi and Vjk can be written as
d'∂W, V)	=	(fw,v (x) — y)	∙	Vix	(19)
''w,"	=	(fw,v (χ) — y)	∙	WjXk,	(20)
∂Vjk
i.i.d
where Vi denotes the i-th row of matrix v. Suppose that the initialization of W and V is: Wi 〜
i.i.d
N(0, δι) and Vij 〜 N(0, δ2) . We also assume that Exi = Exj = 0 and Xi, Xj are independent
with each other for i 6= j where xi is the i-th dimension. We have
Ew,v ""u) d'(W,v) = EW (fw,v (x) — y)2 ∙ Vix ∙ Vjx
∂Wi	∂Wj
(21)
mm
Ew,vy2 ∙	Vix ∙	Vjx +	Ew,v E(WiVix)2 ∙ Vix	∙	Vjx	— 2Ew,v(E yWiVix)	∙	Vix	∙	Vjx	(22)
i=1	i=1
14
Under review as a conference paper at ICLR 2021
Because the independence of vi , vj and their expectations are zero, we can obtain
E d'(WQ) d'(WQ) — 0 for i ± j SimilarlV We Can Oet E d'(WM d'(WQ) — 0 and
Ew,v ∂wi ∂wj = 0 for i = j. SImuany, we can get Ew,v ∂wi ∂vjk = 0 and
Ew,v j) 七=0 for (j,k) = (j0,k0).
The above analyses show that the gradients for different dimensions are independent at initial-
ization. It has been observed that many weights are kept random during training because of the
over-parameterization Balduzzi et al. (2017). So, diagonalization dominant property of C(w) is
reasonable.
7.3 Supplementary materials for results in Section 4
7.3.1	Proof for Mean Escaping Time
Lemma 14 (Lemma 6 in main paper) We suppose C(W) = σga + 2HHa (L(W) - L(a)) on the whole
escaping path from a to b. The mean escaping time of the 1-dimensional power-law dynamic is,
2π
T =---------,
(I - 2K )PHalHbl
1 +上δl)k- 1
κησga
(23)
where K = SCa , Ha ,H are the Second-order derivatives of training loss at local minimum a and
ησHa
saddle point b.
Proof: According to (Van Kampen, 1992), the mean escaping time τ is expressed as τ
where Va is the volume of basin a, J is the probability current that satisfies
P (w∈Va)
Rω JdΩ ,
-NJ(w,t) = d~ (g(w) ∙ P(w,t)) + d~ (φ(w) "(i")
∂W	∂ W	∂W
=∂w")(+σ ∆L(w))-κ d((1+σ^ δlwW))K …
where φ(w) = 2C(W) and μ = 2HHa, σg = σga and ∆L(w)
L(w) - L(a). Integrating
both sides, We obtain J(W) = -φ(w) ∙(1 + ɪ∆L(w)) K(( ~g―：w)鼠 C). Because there
is no field source on the escape path, J(w) is fixed constant on the escape path. Multiplying
φ(w)-1 ∙(1 + ɪ∆L(W)) on both sizes, we have
J ∙ Z φ(w)-1 ∙(1 + 上∆L(w)) dw = — Z —
((1 + μδL(W)) P(W㈤)
∂W
dw
-0 + p(a).
15
Under review as a conference paper at ICLR 2021
Then we get J
we have
----------7-p(a)----VK-. As for the term Rc φ(w)-1
Ra φ(w)-1∙(1+ σμg∆L(w)) dw	aα
1
(1 + σμ∆L(w)) K dw,
ac φ(w)-1 ∙ (1+μ
μ
σg
μ
σg
μ
σg
(∆L(b) - 2|Hb|(w - b)2)
(∆L(b) - 2|Hb|(w - b)2)
dw
κ
dw
-1+κ
dw
-1+κ
dw
(24)
ɪ(l + -μ ∆L(b))-1+κ
ησg	σg
2-(1 + μ- ∆L(b))-1+κ
ησg	σg
μ 1 |Hb | (w — b)2
__ _______________
σg
ι σ ∣Hb∣
ι+σ δls)
-1+κ
dw
-2-(1 + -μ ∆L(b))- 1+κ
ησg	σg
J+σ ∆L(b∖
μ⅛ B(I Z
Z 1 y-1/2 (1 - y)-1+κdy
0
where the third formula is based on the second order Taylor expansion. Under the low temperature
assumption, we can use the second-order Taylor expansion around the saddle point b.
As for the term P(W ∈ %), We have P(W ∈ Va) = RV p(w)dV = Rw∈v p(a)(1 + jμ~∆L(w))-κ =
P(a) J;⅛B( 1 ,κ - 2), where we use Taylor expansion of L(W) near local minimum a. Then we
have T = PRwJV)= P(WJE 匕)because J is a constant. Combining all the results, we can get the
result in the lemma.

Theorem 15 (Theorem 7 in main paper) Suppose W ∈ Rd and there is only one most possible path
path between basin a and the outside of basin a. The mean escaping time for power-law dynamic
escaping from basin a to the outside of basin a is
2πp- det(Hb) _ 1 A +	1 δl∖k- 1
(1 - 2dκ)√det(Ha) lHbel k ηκσe J
(25)
where e indicates the most possible escape direction, Hbe is the only negative eigenvalue of Hb, σe is
the eigenvalue of Σga corresponding to the escape direction and ∆L = L(b) - L(a).
Proof: According to (Van Kampen, 1992), the mean escaping time τ is expressed as τ
where Va is the volume of basin a, J is the probability current that satisfies -▽ ∙ J(w, t)
P (w∈%)
Rω JdQ ,
∂p(w,t)
∂t
Under the low temperature assumption, the probability current J concentrates along the direction
corresponding the negative eigenvalue of Hbe, and the probability flux of other directions can be
ignored. Then we have
/
Jω
JdΩ
(26)
where Je = p(a) ∙ n(1+”"L(Z)R2√*σelHbel which is obtained by the calculation of Je for
2V 2B ( 2 , K)
1-dimensional case in the proof of Lemma 13, and (∙)⊥e denotes the directions perpendicular to the
escape direction e.
Suppose HbΣg-1 are symmetric matrix. Then there exist orthogonal matrix Q and diagonal ma-
trix Λ = diag(λι,…，λd) that satisfy Hb∑-1 = QTΛQ. We also denote V = Q(W — b).
16
Under review as a conference paper at ICLR 2021
We define a sequence as Tk = 1 + , ∙ P；=k λjV for k = 1,…，d. As for the term
Rω (1 + ηκ(w — b)T(Hb∑-1)⊥e(w — b))	2 dΩ, We have
ɪ(w — b)T (Hb∑-1 )⊥e(w — b))	dΩ
= Λ1 + ɪ ∙ VTΛv)-κ+2dw
ηκ
=Z(I+ηK ∙ X λjv2)-κ+2 dv
j6=e
=((ηκ)-1λι)- 1 / τ-κ+2 b(2 ,κ)dv
=Y ((ηκ)-1λj)- 1B(I ,K — j)
j=0
d-2
Y((ηκ)-1λj厂2 •
j=0
P (ηκ∏)d-1 ∙ Γ(κ —
√πdΓ(κ - d)
Γ(κ)
d-2
~2~)
Γ(κ + 1 )Jdet((Hb∑-1)⊥e)
As for the term P (w ∈ Va), we have
P(W ∈ Va) = P p(w)dV = p(a) (	(1 + (w — w*)THαΣ-1(w — w*)) dw	(27)
Va	w∈Va
/	v‰κπ)d ∙γ(K — d)
=p(a)-------,	=
Γ(κ) det((HaΣg-1))
where we use Taylor expansion of L(w) near local minimum a.
Combined the results for P(w ∈ Va) and J, we can get the result.
(28)
7.3.2	Further Explanation about Assumption 1-3
We adopt the commonly used assumptions to analyze mean escaping time for dynamic system (Xie
et al., 2020; Smith & Le, 2017; Zhou & Du, 2014). Assumption 2 can be replaced by weaker
assumption that the system is quasi-equilibrium which is adopted in (Xie et al., 2020). For the
differences between quasi-equilibrium and equilibrium, readers can refer to (Xie et al., 2020) for
detailed discussions. Assumption 3 is commonly used (Xie et al., 2020; Zhou & Du, 2014). Under
Assumption 3, the probability densities will concentrate around minima and the most possible paths.
Assumption 3 will make the second order Taylor approximation more reasonable.
7.3.3	Extension to more complex dynamic on the escaping path
In Lemma 6, we assume that C(W) = σga + 2HHa (L(W) - L(a)) on the whole escaping path from
a to b for ease of comparison and presentation. This assumption is not necessary and We can
assume a different dynamic near saddle point b. Specially, we can assume the point z is the
midpoint on the most possible path beween a and b, where L(z) = (1 — z)L(a) + zL(b). The
dynamic with C(W) = σga + 2HHa (L(W) - L(a)) dominates the path a → Z and the dynamic with
C(w) = σgb + 2HHb (L(b) - L(W)) dominates the path Z → b. Then only two things will be changed in
proof of Lemma 6. First, we need to change the stationary distribution near saddle points according
to its own dynamic in Eq.20. Second, we need to change the integral about probability density on
17
Under review as a conference paper at ICLR 2021
the whole path to sum of integrals on these two sub-paths. Similar proof techniques are adopted for
analyzing escaping time of Langevin dynamic in proof of Theorem 4.1 in the work Xie et al. (2020).
Since the proof is analogous, we omit the details here.
7.4 PAC-Bayes Generalization Bound
We briefly introduce the basic settings for PAC-Bayes generalization error. The expected risk is
defined as Ex〜p(x)'(w, x). Suppose the parameter follows a distribution with density P(W), the
expected risk in terms of p(w) is defined as Ew〜p(w),x〜P(x)'(w, x). The empirical risk in terms
of p(w) is defined as Ew〜p(w)L(w) = Ew〜p(w)^ PZi '(w, xi). Suppose the prior distribution
over the parameter space is p0(w) and p(w) is the distribution on the parameter space expressing
the learned hypothesis function. For power-law dynamic, p(w) is its stationary distribution and we
choose p0(w) to be Gaussian distribution with center w* and covariance matrix I. Then we can get
the following theorem.
Theorem 16 (Theorem 8 in main paper) For w ∈ Rd, we select the prior distribution p0(w) to be
standard Gaussian distribution. For δ > 0, with probability at least 1 - δ, the stationary distribution
of power-law dynamic has the following generalization error bound,
Ew〜p(w),x〜P(x)'(W, X)
≤ Egp(w)L(w) + jKL(PM) +nog1+lθgHZl,
(29)
Tr(η∑g H-1)-2d
4(1-1( 2-1))
where KL(P||P0) ≤ 1 log dd⅛H⅛+
of data x.
+ d log 2 and P(x) is the underlying distribution
Proof: Eq.(29) directly follows the results in (McAllester, 1999). Here we calculate the Kull-
back-Leibler (KL) divergence between prior distribution and the stationary distribution of power-law
dynamic. The prior distribution is selected to be standard Gaussion distribution with distribution den-
sity p0(w) = √=1==== exp{-2(w - w*)tI(w - w*)}. The posterior distribution density is the
stationary distribution forpower-law dynamic, i.e.,p(w) = Z ∙(1 + ^K ∙(w-w*)tHΣ-1(w-w*))-κ.
Suppose HΣg-1 are symmetric matrix. Then there exist orthogonal matrix Q and diagonal matrix
A = diag(λι,…，λd) that satisfy HΣ-1 = QTΛQ. We also denote V = Q(w - w*).
We have
log
P(w)
P0(w)
The KL-divergence is defined as KL(P(w)||P0(w))
w* ) in the integral, we have
p0(w))J dw. Putting V = Q(w -
KL(P(w)||P0(w))
d1
Xlog 2π - log Z + 赤
2	2Z
1 + ɪ ∙ VTAV)	dv - -ɪ / vTAv ∙ (1 + ɪ ∙ vTAv)-κdv,
η	η v	η	(30)
18
Under review as a conference paper at ICLR 2021
where We use the approximation that log(1 + x) ≈ x. We define a sequence as Tk = 1 + "∙
pd=k λj Vj for k = 1,…，d. We first calculate the normalization constant Z.
Z=Z(I+ηK ∙ VTAv)-Kdw=Z(I+ηK ∙ X NVj)- dv
=((ηκ)-1λι)-2 Z T-κ+ 2 B(1 ,κ - 2)dv = Y ((ηκ)-1λj)-1 B(1 ,κ — 2)
2	2 j=1	2	2
d
Y(SK)Tλj厂2 ∙
j=1
√πdΓ(κ — d)
Γ(κ)
We define Zj = ((ηκ)-1λj)- 1 B (ɪ, κ 一 j). For the third term in Eq.(30), we have
2Z ∙ In
VT V(1 +
v
-1VT Λv)-κ dv
ηκ
Z Z Vv (1 + 1- ∙ VtΛv
v V2 ,∙∙∙Vd v vɪ X	InK	.
-κ
L∙F- ∙b2(1+
(ηκ)-1λ1v12
T2
/
J v2,…，vd
T2
(ηκ)-1λι
3
2 ɪ
y2 (1 + y)-κ dy + Zi
(	((nκ)-1λι)-2 T-κ+ 2 B
J v2,…，vd
2 ,κ - 2
=(λ 厂2 B (3" 3IL3 dv …d+
dv1 + Z1
-κ
3
1d
+ 欣∙ X λjVj
ηκ j=2
+ ηκ ∙ X λj Vv
dv1 + Z1
+ Z1
-κ+2
dv2 …,vd
-κ+2
1d
+ 帚∙ X λj Vv
ηκ j=2
+ ηκ ∙ X λj v2
dv2 …，vd
-κ+2
-κ+2
dv2 …,vd
dv2 …，vd
+ ηκ ∙ X λj v2
-κ+2
dv2 …,vd
3
-ɪ + 3
Forterm Jv2,…,vd T2 K 2 dV2 …,vd
in above equation, we have
T	T-κ+ 2 dv2 ∙∙∙,Vd
Jv2,... ,Vd
T	T-κ+2((ηκ)-1λj)-2B (2,K- 2) dv3,…,Vd
V V3,∙∙∙ ,Vd	2
/	T-κ+ 2((ηκ)-1λ2)-2((ηκ)-1λ3)- 1 B (1 ,K
Jv4,…，Vd	\2
-2) B (2，K 一 2) dV4,…，Vd
Td-κ
Vd
d-1
d-1
Y((ηκ)-1λj)-2 Y
j=j
j=j
B (2, K 一 (2+1)) dvd
dd
Y((ηκ)-1λj)-2 YB 1 ,κ-(j + 1)
j=j	j=j 2	2
19
Under review as a conference paper at ICLR 2021
Let Aj = ((ηκ)-1λj)-2B (2, κ - (j + 1)). According to the above two equations, We can get the
recursion
2Z	vTvT1-κdv
=Ai • / T-κ+2 + Zi
1
Z	(XX V2“* 1
Jv2,…，vd ∖ j = 2	)
dV2…,Vd
=Ai ∙ J T2 κ+ 2 dv2 …Vd
+ Zi ∙ A2 / T-κ+ 2
dv3∙∙∙ ,Vd +Z1Z2∕ (X
j=3
吟	T-κ+2
dV3 …,Vd
4
1
d-1	j-1	+1+1
X AjY ZkZ Tu+ "
j=1	k=1
j+1 + 1
d-1	Zt	d-ι
dvj+1,…，Vd + YIZkJ v2T-κ+ F dVd
k=1
2，k-(2+1))Y(λκ)-2B(1 ,κ-2) Y((λs)-1 Y BQ,κ-(S+ι)
2	2	k=i ηκ	2	2 s=j+i ηκ	s=j+i	2	2
,κ - j- 1) ∙ (^-)-3 B(3 ,κ - (2+1))
2 ηκ 2	2
√ΠdΓ(κ - d - 1)Tr(H-iΣg)
2Γ(κ)√(ηκ)-(d+2) det(H-iΣg)
We have
i- d - DTMH7g) ∙ γ…F)2 • 7=J
4Γ(κ)√(ηκ)-(d+2) det(H-iΣg) 悬	j √ΠdΓ(κ - d)
ηκTr(H-i Σg)
4(κ - d - 1)
Similarly, for the fourth term in Eq.(30), we have IV = 2«-；-i) ∙ COmbining all the results together,
we can get κL(PllP0) = 2 log (ηκdedH⅛g)+ log ⅛⅛ + Tr((n*1H--)-)d + 2 log 2∙ USing the
fact that log rΓ-)d) ≤ 2d log κ, we have KL(p∣∣p0) ≤ 1 log Ide((H) + T4(：工IHd-)-j2 + 2 log 2∙
7∙5 Implementation Details of the Experiments
7∙5∙1 Observations on the Covariance Matrix
In this section, we introduce the settings on experiments of the quadratic approximation of covariance
of the stochastic gradient on plain convolutional neural network (CNN) and ResNet∙ For each model,
we use gradient descent with small constant learning rate to train the network till it converges∙ The
converged point can be regarded as a local minimum, denoted as w*∙
As for the detailed settings of the CNN model, the structure for plain CNN model is input →
C onv1 → maxpool → C onv2 → maxpool → fc1 → Relu → fc2 → output∙ Both Conv 1 and
Conv2 use 5 × 5 kernels with 10 channels and no padding∙ Dimensions of full connected layer
fc1 and fc2 are 1600 × 50 and 50 × 10 respectively∙ We randomly sample 1000 images from
FashionMNIST (Xiao et al., 2017) dataset as training set∙ The initialization method is the Kaiming
initialization (He et al., 2015) in PyTorch∙ The learning rate of gradient descent is set to be 0.1∙ After
3000 iterations, GD converges with almost 100% training accuracy and the training loss being 1e-3∙
As for ResNet, we use the ResNet-18 model (He et al., 2016b) and randomly sample 1000 images from
Kaggle’s dogs-vs-cats dataset as training set∙ The initialization method is the Kaiming initialization
(He et al., 2015) in PyTorch∙ The learning rate of gradient descent is set to be 0.001∙ After 10000
iterations, GD converges with 100% training accuracy and the training loss being 1e-3∙
20
Under review as a conference paper at ICLR 2021
We then calculate the covariance matrix of the stochastic gradient at some points belonging to the
local region around w*. The points are selected according to the formula: wlayerL ± (i X Scale),
where WlayerL denotes the parameters at layer L, and i × Scale, i ∈ [N] determines the distance
away from wl*ayerL . When we select points according to this formula by changing the parameters
at layer L, we fixed the parameters at other layers. For both CNN model and ResNet18 model,
We select 20 points by setting i = 1,…,10. For example, for CNN model, We choose the 20
points by changing the parameters at the Conv1 layer with S cale = 0.001 and C onv2 layer with
Scale = 0.0001, respectively. For ResNet18, We choose the 20 points by changing the parameters
for a convolutional layer at the first residual block With Scale = 0.0001 and second residual block
With S cale = 0.0001, respectively.
The results are shoWn in Figure.1. The x-axis denotes the distance of the point aWay from the local
minimum and the y-axis shoWs the value of the trace of covariance matrix at each point. The results
shoW that the covariance of noise in SGD is indeed not constant and it can be Well approximated
by quadratic function of state (the blue line in the figures), Which is consistent With our theoretical
results in Section 3.1.
7.5.2 S upplementary Experiments on Parameter Distributions of Deep Neural
Networks
Batchsize 64
Generalization Error 4.59%
κ=1.08
6f
0.03	0.04	0.05	0.06
Batchsize 1024
Generalization Error 5.74%
Figure 6: A close-up of right tail distribution of the result for ResNet18 in Figure. 3(a), Which could
help to observe the heavy-tailed properties among different batchsize.
VGG16
Top-5 Error 9.62%
κ=3.547
AlexNet
Top-5 Error 20.91%
κ=3.937
SqueezeNet 1.0
Top-5 Error 19.58%
κ=2.912
-0.1 0.0	0.1	0.2
20
15
10
5
-0
Wide ResNet-50-2
Top-5 Error 5.91%
MobileNet v2
Top-5 Error 9.71%
Inception v3
Top-5 Error 6.44%
Figure 7: Approximating distribution of parameters (trained by SGD) by poWer-laW dynamic. These
netWorks use pre-trained models offered by PyTorch, and all of them are pre-trained on ImageNet
dataset using SGD. The second line in each title shoWs Top-5 test error and the third line shoWs
approximated tail-index κ.
For Figure. 3(a), We train LeNet-5 on MNIST dataset using SGD With constant learning rate η = 0.03
for each batchsize till it converges. Parameters are conv2.weight in LeNet-5. For Figure 3(b), We
train ResNet-18 on CIFAR10 using SGD With momentum. We do a RandomC rop on training set
scaling to 32 × 32 With padding = 4 and then a RandomH orizontalF lip. In training, momentum
is set to be 0.9 and Weight decay is set to be 5e - 4. Initial learning rate in SGD is set to be 0.1 and
We using a learning rate decay of 0.1 on {150, 250}-th epoch respectively. We train it until converges
after 250 epoch. Parameters are layer1.1.conv2.weight in ResNet-18.
21
Under review as a conference paper at ICLR 2021
Figure 8: Comparison between Q-Q plots of network parameters versus normal distribution and
power-law distribution. (upper): Q-Q plots of parameters versus normal distribution. (bottom):
Q-Q plots of parameters versus power-law distribution.
We also observe the parameter distribution on many pretrained models. Details for pre-trained models
can be found on https://pytorch.org/docs/stable/torchvision/models.html.
Figure.7 shows the distribution of parameters trained by SGD can be well fitted by power-
law distribution. Parameters in this figure are all randomly selected to be features.10.weight,
features.14.weight, f eatures.5.expand3 × 3.weight, Mixed_6d.branch7 × 7_3.conv.weight,
layer4.2.conv3.weight and f eatures.denseblock2.denselayer1.conv2.weight for VGG-16,
AlexNet, SqueezeNet 1.0, Inception v3, Wide ResNet-50-2 and DenseNet-121 respectively.
A Q-Q plot is created by plotting quantiles of two probability distributions against one another, which
can provide an assessment of "goodness of fit" by how much the solid line close to the dashed line.
From Figure.8, it is clear that the solid lines in bottom pictures are closer to dashed lines on most
cases, which indicates network parameters can be better fitted by power-law distribution. Moreover,
solid lines in the upper plots severely deviate from dashed lines on the tail of distribution but those in
the bottom plot do not, which means the distribution of parameters is indeed heavy-tailed.
7.5.3	Further Explanation on Experiments in Section 5.2
As for the experiments for 2-D model, we also calculate coefficient of the second-order term for
the quadratic curve shown in Figure.4(b), and its value is roughly 30, which matches the result in
Figure.4(c) in the sense that the result for SGD is similar with the result for power-law dynamic with
λ1 ≈ 32.
7.5.4	Escaping Efficiency on Neural Network
Figure 9: Escaping experiment on corrupted FashionMNIST. Test accuracy versus iteration after
pretraining by GD. Model is pretrained by GD before the vertical dashed line and continued by GD,
GLD and PLD (ours). Numbers in brackets are expected sharpness after model converging.
22
Under review as a conference paper at ICLR 2021
We follow the settings in (Zhu et al., 2019). For convenience of the readers, here we give the details of
this setting again. We use corrupted FashionMNIST dataset which contains 1000 images with correct
labels and another 200 images with random labels to be training data. A small LeNet-like network
with 11,330 parameters is used. Firstly we run the full gradient decent to reach the parameters
w* near the global minima. Then we continue training using both Langevin dynamic(GLD) and
power-law dynamic(PLD). Following Zhu’s setting, the learning rates for GD, GLD and PLD are
ηGD = 0.1, ηGLD = 0.07 and ηpLD = 0.07, respectively. For GLD, noise std σ = 10-4 as Zhu
already tuned. For our PLD, wt+i = wt — ηVL(wt) + η ∙ αVL(wt) Θ，1 + β(wt — w*)2 Θ ξ,
where α, β are hyperparameters, ξ 〜N(0, I), and Θ stands for Hadamard product. Here we select
a = 2.4, β = 2 after grid search. Expected sharpness is measured as EV〜N(。炉i) [L(w + V)] — L(w)
where δ = 0.01, and the expectation is computed by average on 1000 times sampling.
The numbers at the first column of the legend show the test accuracy and the numbers in the bracket
show the sharpness of the model trained by the three algorithms. From Figure 9, we can conclude
that PLD generalizes better than GLD and GD. Moreover, PLD can find flatter critical points than
GLD and GD.
7.5.5	Comparison of Mean escaping time with different barrier heights
(a) 1-D Model
7o
uo_3e」①-6u-deus 山 Ue ①ɪ
Barrier Height
(b) Mean Escaping Time vs. Barrier Height
Figure 10: (a): Loss curve L(w) for 1-D model. (b): Mean escaping time versus different barrier
heights. Mean escaping time is computed by average on 100 rounds, in which we record the number
of iterations when firstly escaping from the saddle point.
We design this 1-dimensional model to help to validate the theoretical results of escaping time in
Table 1. Loss function L(W) = n PZi '(w 一 xi), where '(w) = {w + ；； , w < 0 and
Xi 〜N(0,0.05), L(w) is plotted in FigUre.10(a), and We can adjust barrier height through parameter
b in '(w) without changing the Hessian on minima w* and saddle point.
For power-law dynamic (PLD), wt+i = wt 一ηVL(wt) + ηλ2，1 + λι(wt 一 w*)2 Θξ, where λι,λ2
are hyperparameters, ξ 〜N(0, I), and Θ stands for Hadamard product. Here we let λι = I,λ2 = 4.
For Langevin dynamic (GLD), we set noise std σ = 4 in consistence with PLD. Learning rate η = 0.1
for both methods. We initialize w0 = w* and apply both methods on L(w) with different barrier
heights. Then we record the number of iterations t when wt firstly escaping from the barrier. We
repeat this procedure 100 rounds for each method and each barrier height and utilize the average to
estimate the mean escaping time, of which the results are shown in Figure.10(b).
From Figure.10(b), the mean escaping time of GLD grows much faster than PLD along barrier height,
which validates that power-law dynamic improves the order of barrier height compared with Langevin
dynamic.
23