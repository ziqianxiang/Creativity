Under review as a conference paper at ICLR 2021
A New Variant of Stochastic Heavy ball Opti-
mization Method for Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic momentum optimization methods, also known as stochastic heavy ball
(SHB) methods, are one of the most popular optimization methods for deep learn-
ing. These methods can help accelerate stochastic gradient descent and dampen
oscillations. In this paper we provide a new variant of the stochastic heavy ball
method, called stochastic Euler’s heavy ball (SEHB). The proposed SEHB method
modifies the steepest descent direction to achieve acceleration, and combines Eu-
ler’s method to adaptively adjust learning rates as well. A convergence analysis
of the regret bound is discussed under the online convex optimization framework.
Furthermore, we conduct experiments on various popular datasets and deep learn-
ing models. Empirical results demonstrate that our SEHB method shows compa-
rable or even better generalization performance than state-of-the-art optimization
methods such as SGD and Adam.
1	Introduction
Stochastic gradient descent (SGD) is the main optimization method for deep learning. This method
is often trained in the form of mini-batch SGD in order to meet the requirements of computing power,
achieving great success in various machine learning and deep learning tasks (Krizhevsky et al., 2012;
Graves et al., 2013; Lecun et al., 1998). However, SGD havs two main drawbacks: one is the use
of the negative gradient of loss functions as descent directions which leads to a slow convergence
near the local minima and is difficult to escape from local suboptimal solutions; the other one is that
SGD scales the gradient uniformly in all directions which may lead to poor performance as well as
limited training speed. In recent years, considerable efforts have been spent on improving SGD and
several remarkable variants have been proposed.
The two main ways for improving SGD are adjusting the steepest descent directions and apply-
ing adaptive learning rates to the training process, called stochastic momentum methods and adap-
tive methods respectively. Stochastic momentum methods, including stochastic heavy ball (SHB)
(Polyak, 1964) and stochastic Nesterov’s accelerated gradient (SNAG) (Nesterov, 1983), have a
smoother convergence process and generalize comparable or even better than SGD; Adaptive meth-
ods enjoy rapid training speed and need less effort to adjust the hyperparameters like initial learn-
ing rates, among which the representative algorithms are AdaGrad (Duchi et al., 2011), AdaDelta
(Zeiler, 2012) and RMSprop (Tieleman & Hinton, 2012).
Adam (Kingma & Ba, 2015), as one of the most popular algorithms for deep learning, is a combi-
nation of stochastic momentum methods with adaptive methods. Adam has the most adavantages of
both methods but it seems to have poorer generalization ability and out-sample behavior than SGD
and stochastic momentum methods. Recently, research shows that Adam even fails to converge on
some convex functions (Reddi et al., 2018). Many variants of Adam have been proposed to solve
this issue, such as AMSGrad (Reddi et al., 2018) and AdaBound (Luo et al., 2019). However, the
generalization ability of AMSGrad on unseen data is found to be similar to that of Adam and there
exists a considerable performance gap between AMSGrad and SGD or SHB (Keskar & Socher,
2017).
A new optimization method, called Adaptive Energy Gradient Descent (AEGD) (Tan, 2020), has
been proposed and demonstrates a novel direction to improve the generalization ability of SGD.
AEGD finds the connection between numerical methods ,which are used to solve partial differential
equations (PDEs), and SGD by successfully applying Invariant Energy Quadratization(IEQ) (Yang,
1
Under review as a conference paper at ICLR 2021
2016) to SGD. IEQ is introduced for gradient flows in the form of PDE and constructs linear, un-
conditionally energy stable schemes for solving time-dependent PDEs.
In this paper, we first point out that the iteration rule used in IEQ and AEGD is similar to Eu-
ler’s method, which is a classical method for analyzing differential equations. When we use it in
SGD, it’s equivalent to adaptively adjust the learning rates during the descent by multipling a vector
element-wisely. Inspired by the aforementioned analysis, we propose a new variant of SHB, called
stochastic Euler’s heavy ball (SEHB), by applying Euler’s method to SHB. It’s a combination of
adaptive learning rate adjustments with SHB. Moreover, we give a convergence analysis of SEHB
by proving the regret bound under online convex optimization framework (Zinkevich, 2003). Finally,
we conduct several representative experiments in order to test the performance of SEHB and other
classical optimization methods for deep learning. There are four main experiments including nu-
merical experiment on Extended Rosenbrock function and three neural network experiments: fully
connected neural networks on MNSIT datasets, ResNet-50 on CIFAR-10 dataset and DenseNet-121
on CIFAR-10 dataset.
2	Notation and Preliminaries
Notation. For a vector θ ∈ Rd, we denote its i-th coordinate by θi ; we use θt to denote θ in
the t-th iteration and use θt,i for the i-th coordinate of θ in the t-th iteration. Furthermore, we use
|| ∙ || to denote l2-norm and use || ∙∣∣∞ to denote l∞-norm. Given two vectors v,w ∈ Rd, We use
Vw to denote element-wise product and use v2 to denote element-wise square; we use 1 to denote
element-wise division.
Online convex optimization framework. We analyze the convergence of our method SEHB un-
der the online learning framework which is one of flexible frameworks to analyze iterative opti-
mization methods. In this online setup, at each step t, the optimization algorithm chooses a de-
cision xt ∈ F where F ∈ Rd is a convex feasible set. Then the decision θt is evaluated by
a convex loss function ft. The optimization algorithm’s regret at the end of T steps is given by
R(T) = PtT=1 ft(θt) - minθ∈F PtT=1 ft(θ). The regret evaluates the average performance of the
algorithm and a o(T) vanishing regret implies the algorithm’s performance converges to the optimal
one on average.
Euler’s method Euler’s method is a technique used to analyze a differential equation. Motivated
by the concept of local linearity or linear approximation, the method uses small tangent lines over a
short distance to approximate the solution of the initial value problem. Let x ∈ Rd, f (x) : Rd → R
and we use r to approximate f, the update rule is rt+1 = r + Vf (Xt)(Xt+ι - xt). Thus, if the
sequence interval of {xt} approaches to 0, r is a good fitting of f.
3	Algorithm and Convergence Analysis
In this section, we develop a new variant of the stochastic heavy ball method, which we call stochas-
tic Euler’s heavy ball (SEHB), and provide the convergence analysis of SEHB under online convex
optimization framework.
3.1	Algorithm
We aim to devise a new strategy that combine the rapid initial progress of adaptive methods and
the good final generalization properties of SGD and SHB. Algorithm 1 is the pseudo code of SEHB
where all operations are element-wise.
To understand the design of our algorithm, we first let g(θt) = fθ(θt) + c where f (θt) is the loss
function of models and c is a constant number which guarantees f (θt) + c > 0. Thus we have
Vf(θt) = 2g(θt)Vg(θt). Inspired by AEGD and IEQ, we use Euler’s method to approximate the
function g(θt )
rt+1 = rt + Vg(θt)(θt+1 - θt),	(1)
2
Under review as a conference paper at ICLR 2021
Algorithm 1 SEHB
Input: θ1 ∈ Rd, step size {ηt}tT=1, c = 1, {γt}tT=1
1:
2:
3:
4:
5:
6:
7:
Set v0,i = 0, r0,i = pf (θ1) + c,
for t = 1 to T do
G J ▽/(θt)∕2Pf (θt) + c
rt+1 - rt∕(1 + 2ηtG2)
vt+1 J γtvt + G
θt+1 J θt - 2ηtrt+1vt+1
end for
i = 1, 2, ∙∙∙ ,d
(compute the gradient of g)
(update vector r with Euler’s method)
(update SHB direction of g)
(update θ)
where the inital value ro,i = g(θι), i = 1,2, ∙∙∙ ,d and the SHB descent direction is given by
vt+1 = Ytvt + Vg(θt).
(2)
Because the iteration rule
θt+1 = θt - 2ηtrt+1vt+1
(3)
includes rt+1 which relies on θt+1 to update, we need to remove θt+1 from the update rule of rt+1.
Let the SHB decay parameter γt = 0, and plugging θt+1 into the update rule of rt+1 leads to
rt+1 = rt/(1 + 2ηt(Vg(θt))2).
(4)
We use the above rule to update rt+1 in practice. Now let us revisit the update rule of θt+1, it is
equivalent to
θt+1=θt-ηt p⅛Vf (θt).
(5)
It seems that the only difference from SGD is the learning rate which has an adaptive adjustment
by multipling rt+ι/ʌ/f (θt) + C. Therefore, by letting the SHB decay parameter Yt ∈ (0,1), SEHB
realizes the combination of SHB and the adaptive adjustment of learning rates.
Considering the fact that SEHB use SHB descent direction in the iteration process, we still use 0.9
as the default of Yt in practice for any time step t but when analyzing the convergence of SEHB,
the default setting is γιλt-1 under the online convex optimization framework, which is as same as
βιt in Adam and AMSGrad. Given the hyperparameter c, Equation (5) implies that 1/y/f (θt) + c
participates in adjusting learning rates. If it is very close to 0 at a certain step of the iteration
process, learning rates would become pathological and the iterative process would become unstable.
Therefore, to avoid this issue, we set the default of c to 1 based on the fact that the most of loss
functions for deep learning have a lower bound 0. Finally, we choose 0.01 as the default learning
rate of SEHB which comes from empirical results. However, it may be not the best learning rate in a
certain experiment but can show a relatively excellent performance if users don’t want to spend any
effort on tuning.
3.2	Convergence Analyze
We prove the convergence of SEHB under the online convex optimization framework. The key
results are as follows:
Theorem 1. Suppose that the fucntion gt has bounded gradients, ∣∣Vgt(θ)∣∣∞ ≤ G∞ for all θ ∈
Rd and is bounded in feasible regions, ∣gt(θ) | ≤ g∞. Suppose the distance between any θt generated
by SEHB is bounded, ∣∣θn 一 θm∣∣∞ ≤ D∞ for any n, m ∈ {1,2,…，T}, T ≥ 2, and γι ∈ [0,1).
Let ηt = √ and Yt = γλt-1, λ ∈ (0,1). Then SEHB has the following bound on the regret
IC L ±	1
R(T) ≤ 而d∞ √Tg∞ X(E
+ ɪ) + g∞D∞ dγ + g∞dG∞γ2 (Y + 2g∞ λ2η)
+ E7) +	1 一 λ +	λ2(1 — Y)2(1 — λ2)
(6)
The following results falls as an immediate corollary of the above result:
3
Under review as a conference paper at ICLR 2021
Corollary 1. Under the same assumptions of Theorem 1, SEHB have the following average regret
of convergence:
R(T)
T
(7)
Remark. The above bound in Equation (6) can be considered better than O(√dT) regret of SGD
(Nesterov, 1983) when Pd=1( rτɪɪ ʃ + 十) W √d. In our convergence analysis, decaying Yt
towards zero is important and several previous findings suggest that the SHB coefficient in the end
of training can improve convergence (Sutskever et al., 2013). Finally, we find that the regret bound
of SEHB is similar to the classical adaptive methods like Adam and AMSGrad. However, empirical
results show that SEHB may generalize better than Adam and AMSGrad on several classic image
classification tasks. We will show details in next section.
4	Experiemnt
In this section, we study the generalization performance of SEHB and several famous optimization
methods including SHB (also known as SGDM), AdaGrad, Adam, AEGD, AMSGrad, AdaBound.
We focus on four main experiments: numerical experiment on Extended Rosenbrock function; fully
connected neural network experiment on MNIST dataset (Lecun et al., 1998); ResNet-50 (He et al.,
2016) and DenseNet-121 (Huang et al., 2017) on CIFAR-10 dataset (Krizhevsky & Hinton, 2009).
4.1	Numerical Experiemnt
Extended Rosenbrock function is a famous high-dimensional non-convex function in optimization.
Let x ∈ R1000, f(x) : R1000 → R where f is Extended Rosenbrock function. We start with
initial point xo = (-1.2,1, -1.2,1,…，-1.2, I)T to find the optimal solution. The exact optimal
solution is (1,1,…，I)T and the exact minimum value of Extended Rosenbrock function is 0. We
select three optimization methods to solve the optimization problem which include Adam, AEGD
and SEHB. For hyperparameters in Adam, we use the default value of β1 and β2 and use 1e - 2 as
the learning rate. We let c = 1 and choose 1e - 4 as the learning rate when using AEGD and SEHB.
In addition, we use the default value 0.9 as γt in SEHB. Our criterion for choosing learning rates
is to choose the one showing the fastest convergence with as little oscillation as possible. Figure 1
shows the result of the numerical experiment.
Figure 1: Performance comparison of Adam, AEGD and our method SEHB on Extended Rosen-
brock function. All three methods converge to the optimal solution.
We note that although all three optimization methods have converged to the optimal solution, SEHB
need the least epochs than Adam and AEGD. Under the requirement of the smooth iteration process,
SEHB shows faster convergence than Adam and AEGD.
4
Under review as a conference paper at ICLR 2021
Figure 2: Performance comparison of fully connected neural networks with Adam, AEGD, Ada-
Grad, SHB and our method SEHB on MNIST dataset. The left is the training loss curve and the
right is the test error curve.
Figure 3: Performance comparison of deep convolution neural networks with Adam, AMSGrad,
AdaBound, AEGD, AdaGrad, SHB and our method SEHB on CIFAR-10 dataset. The top row is the
training loss curve and the test error curve of ResNet-50 while the bottom row is the training loss
curve and the test error curve of DenseNet-121.
4.2	Neural Networks
In this subsection, we show the empirical results of several neural network models.
5
Under review as a conference paper at ICLR 2021
4.2.1	Hyperparameters Tuning
In the training of neural networks, hyperparameters settings have a significant impact on training
results. Therefore, we need spend some efforts to tune hyperparameters of optimization methods for
obtaining the best performance of each method. Here we show how to tune the hyperparameters. To
tune the learning rate, we set five candidate learning rates and find the best one as the final learning
rates to compare with other methods. For the hyperparameters of all methods, we tune them in the
following ways:
SHB. The candidate learning rates of SHB are selected from {1, 0.5, 0.3, 0.1, 0.01}. We set γ,
which is the coefficient of momentum in SHB, to the default value 0.9. Empirical results show that
the default γ often have a good performance on most of the models. Otherwise, the learning rates
vary on different models. For instance, the best learning rate of fully connected neural network on
MNIST dataset and ResNet-50 on CIFAR-10 dataset is 0.1 while the best one of DenseNet-121 on
CIFAR-10 dataset is 0.3.
AEGD. The candidate learning rates of AEGD are selected from {0.5, 0.3, 0.1, 0.05, 0.01}. We
set c to 1. The best learning rate of AEGD is 0.1 for the three experiments.
AdaGrad. The candidate learning rates of AdaGrad are selected from {0.1, 0.05, 0.01, 5e-3, 1e-
3}. For the initial accumulator value, we choose the default value 0. The best learning rate of
AdaGrad is 0.01 for the three experiments.
Adam, AMSGrad and AdaBound. The candidate learning rates of Adam, AMSGrad and Ad-
aBound are selected from {0.01, 5e-3, 1e-3, 5e-4, 1e-4}. We turn over β1 values of {0.9, 0.99}
and β2 values of {0.99, 0.999}. We use the default values of other hyperparameters. The best learn-
ing rate is 1e - 3 for all experiments.
SEHB. The candidate learning rates of SEHB are selected from {0.1, 0.05, 0.03, 0.01, 1e-3}. We
use the default value of c and γt. The best learning rate is 0.1 for fully connected neural network on
MNIST dataset, 0.01 for ResNet-50 on CIFAR-10 dataset and 0.03 for DenseNet-121 on CIFAR-10
dataset.
4.2.2	Fully Connected Neural Network on MNSIT
Fully connected neural network we using in the experiment has only one hidden layer. We run
the fully connected neural network with Adam, AdaGrad, AEGD, SHB and our method SEHB on
MNIST dataset. The total number of epochs is 100 and the minibatch size is set to 128. Figure 2
shows the empirical results.
Note that all optimization algorithms achieve a training loss approaching 0 and have a test error
below 2%. We find that our method SEHB and Adam achieve slightly better performance than other
methods on the test set. However, SEHB have a smoother and faster convergence process than Adam
and generalize more stably than Adam.
4.2.3	ResNet-50 and DenseNet- 1 2 1 on CIFAR-10 Dataset
CIFAR-10 is a more complex dataset than MNIST. We use more advanced and powerful deep convo-
lution neural networks, including ResNet-50 and DenseNet-121, to test various optimization meth-
ods in this classification task on CIFAR-10 dataset. We employ the fixed budget of 200 epochs, set
the minibatch size to 128 and adjust the learning rates through dividing them by 10 after 150 epochs.
Finally, we apply weight decay scheme into optimization methods. Figure 3 shows the empirical
results.
As is expected, the overall performance of each algorithm on ResNet-50 is similar to that on
DenseNet-121. We note that SEHB shows the best generalization performance on both ResNet-50
and DenseNet-121. For ResNet-50, the best error of SEHB is lower than that of SHB with a margin
1.8% and is lower than that of Adam with a margin 1.5%; For DenseNet-121, the improvement of
SEHB compared to SHB and Adam is 0.9% and 2% respectively. We find that classical adaptive
6
Under review as a conference paper at ICLR 2021
methods show rapid descent in the early period of training such as AdaBound and AdaGrad. How-
ever, they show mediocre generalization ability on the test set. Almost all methods benefit from the
learning rate adjustment except AdaGrad and after the adjustment, their performance on the test set
have a significant improvement. The empirical results show that despite the relative bad generaliza-
tion ability of adaptive methods, our method SEHB seems to overcome the drawback and achieve
even better generalization performance than SHB.
4.2.4	Analysis
We select the classical numerical experiment and popular tasks from computer vision to investigate
the efficacy of our proposed method. Empirical results demonstrate that our method SEHB show the
best generalization performance. As is shown above, SEHB exceeds all other methods greatly with
respect to test error at the ending of training. It indicates that SEHB, as a new variant of adaptive
methods, also has a excellent generalization performance which is comparable and even better than
SHB. It seems that SEHB overcomes the drawback of adaptive methods which typically have worse
generalization ability. Our aim to combine adaptively learning rate adjustment with SHB is proved
to be feasible and successful.
5	Conclusion
By investigating AEGD and IEQ, we find that their update rule is equivalent to Euler’s method.
Inspired by this, we propose a new variant of SHB, called SEHB, by adding adaptive learning rate
adjustment into SHB. SEHB shows rapid convergence and better generalization ability than SHB,
which is far different from current adaptive methods like Adam and AMSGrad. We argue that SEHB
provides a new way to adaptively adjust learning rates and realizes the combination of adaptive
methods with SHB just like Adam. The empirical results show that SEHB is worth further research.
Despite excellent performance of SEHB, there still remains several problems to explore. First,
SEHB employs Euler’s method as the adaptive adjustment of learning rates which is different from
the mainstream adaptive methods for deep learning. The reason for the effectiveness of this way
of adjusting remains unclear. Furthermore, AMSGrad shows that very small values of adaptive
learning rates may result in worse performance and even non-convergence, thus the effectiveness
of adding the control of adaptive learning rates for SEHB needs more explaination. Finally, the
reason for better generalization performance of SEHB than SHB is still uncertain and worth further
exploration.
7
Under review as a conference paper at ICLR 2021
References
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In Journal of Machine Learning Research (JMLR), 2011.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey E Hinton. Speech recognition with deep re-
current neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE In-
ternational Conference on, pp. 6645-6649, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.
Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
Adam to SGD. In CoRR, 2017.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky and Geoffrey E. Hinton. Learning multiple layers of features from tiny images. In
Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
volutional neural networks. In Advances in neural information processing systems (NIPS), pp.
1097-1105, 2012.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In Proceedings of International Conference on Learning Representations
(ICLR), 2019.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O(1/sqr(k)). In Soviet Mathematics Doklady, pp. 27:372-376, 1983.
Boris. T. Polyak. Some methods of speeding up the convergence of iteration methods. In USSR
Computational Mathematics and Mathematical Physics, pp. 4:791-803, 1964.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. In
Proceedings of International Conference on Learning Representations (ICLR), 2018.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In Proceedings of the 30th International Conference on
Machine Learning(ICML), pp. 1139-1147, 2013.
Xuping Tan. AEGD: adaptive gradient decent with energy. 2020. URL https://github.com/
txping/AEGD.
Tijmen Tieleman and Geoffrey Hinton. RMSprop: Divide the gradient by a running average of its
recent magnitude. In COURSERA: Neural networks for machine learning, 2012.
Xiaofeng Yang. Linear, first and second order and unconditionally energy stable numerical schemes
for the phase field model of homopolymer blends. JCP, pp. 302:509-523, 2016.
Matthew D. Zeiler. ADADELTA: An adaptive learning rate method. In CoRR, 2012.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the Twentieth International Conference on Machine Learning (ICML), pp. 267-
280, 2003.
8
Under review as a conference paper at ICLR 2021
Appendix
A Convergence Proof
The proof of Theorem 1 is as follows:
Proof. Let θ* = arg min& ft(θ), We have
LEMMA 1. If a function f : Rd → R is convex, then for all θ1, θ2 ∈ Rd,
f(θ2) ≥ f(θι) + Vf(θι)T(θ2 - θι).	(8)
Actually, it is an equivalent definition of convex functions.
According to Lemma 1, We have
ft(θt)- ft(θ*) ≤ <Vft(θjθt-θ*>
d
X(Vft(θt,i),θt,i - θ*i).
i=1
(9)
Recall our update formula:
θt+1 - θt = -2ηtrt+1vt+1
= -2ηtrt+1(γvt + Vg(θt)).
For i-th coordinate of θ,
θt+1,i - θt,i = -2ηrt+1,i (γvt,i + Vg(θt,i )),
and then
(θt+ι,i - θ,iD2 = (θt+ι,i - θt,i + θt,i-θ,iD2
=(θt,i - θi)2 + 2(θt+1,i - θt,i)(θt,i - θ*i) + (θt+1,i - θt,i)2
=(θt,i - %¥ - 4ηrt+1,i(Ytvt,i + vg(θt,i ))(θt,i - θ*i) + (-2ηtrt+1,ivt+1,i)2.
(12)
Thus,
(10)
(11)
Vf(θt,i)(θt,i -	θ,*i)	=y(θti	((θt,i	-	θ,*i)2 -	(θt+1,i - θ,*i)2)	-	2g(θt,i)Ytvt,i(θt,i	- θ,*i)八。、
4ηtrt+1,i	(13)
+ 2g(θt,i)ηtrt2+1,ivt2+1,i .
Because a2/2 + b2/2 ≥ ab, We have
Vf (θt,ii(θt,i - θ*ii ≤2g(θt,ii(4-r^-((θt,i - θ*ii2 - (θt+ι,i - θ*ii2i
+ γ(v2,i∕2 + (θt,i - %)2/2) + -rt+ι,ivt+ι,ii.
(14)
Before proving the regret bound, We prove the folloWing Lemma 2, Lemma 3 and Lemma 4:
LEMMA 2.	t,i is strictly decreasing and t,i > 0.
proof. Because rι,i =，f (θιi + C > 0, We have
r . = rt-1,i_______=___________r1i_______ (15)
t,i=ι + 2ηtG2,i = Qj=ι(i + 2-j G2,ii,	(5)
Where Gt = Vgt(θti and Gt,i is the i-th coordinate of Gt. Obviously, 1 + 2-j Gj2,i > 1, for
j ∈ {1,2, ∙∙∙ ,t}. This proves the Lemma 2.
9
Under review as a conference paper at ICLR 2021
Lemma 3.
∖vt+1,i∖ ≤
(16)
proof.
∖vt+1,i∖ = Ytvt,i + Vgt(θt,i)
t
≤ X γt+1F (θj,i)
j=i
t
≤ G∞ X γ”-j
j=i
≤ G∞γt
—1 - Yt.
(17)
LEMMA 4
T d
XX YtVti ≤
dG∞Y3
λ2(1 - γ)2(1 - λ2),
proof.
Td
XX
ηtv2+ι,i ≤
dηG∞γ 2
(1-γ)2(1- λ2).
(18)
T d	T	2	T λ2t-2
X X Ytv2,i ≤ S X π⅛ ≤ dG∞γ3 L X 二
(1-γ)2
≤	dG∞]3
≤ λ2(1 - Y)2(1 - λ2),
Td	T
XX
ηtv2+ι,i ≤ ηd,G∞ X
(1 - Yt)2
T	2t-2
≤ ηdG∞72 X (1 - Y)2
(19)
G∞γ
1 - Yt
dηG Y2 K ≤ —四7-
αηG∞7(1 - Y)2 ≤(1 - y)2(1 - ʌ2).
Finally, we upper the regret bound:
T
R(T) = X(ft(θt) - ft(θ*)
t=1
Td
=XX <vft(θt,i),θt,i-θ>
t=1i=1
(20)
10
Under review as a conference paper at ICLR 2021
According the update rule and Equation 14, we have
T d	2g(θ	)	T d
R(T) ≤EE4gj .((θt,i -G)2 - (θt+ι,i -跖2) + ∑∑2g(θt,i)(γt(v2,i/2 + (θt,i -θi)2/2)
t=1 i=1 ηtrt+1,i	t=1 i=1
+ ηtrt2+1,ivt2+1,i)
X X (≡+⅛ + YM%i))(%i - ")2 - X X ≡⅛ (Ki- ")2
Td
+ XX 2g(θt,i)(γtv2,i/2 + ηtr+ι,iv2+ι,i)
≤ X 普"(k"J + X XYtg(θt,i)(θt,i - θ,i)2
i=1 ηT 2,i	t=1 i=1
+ XX(%-竿/(k-吁2 + XX2g(θt,i)(γtv2,i∕2 + ηtr2+ι,iv2+ι,i).
t=2 i=1	ηt t+1,i ηt t,i	t=1 i=1
(21)
By applying the assumptions into the proof, we obtain
d
R(T) ≤ D∞2 X(
i=1
2g(6τ,i)
4ηTrT+1,i
2g(θι,i))
4η2r2,i
d	τd	τd
+ D∞ X 件 + D∞ XXYtg(θt,i) + XX2g(θt,i)(γtv2,i∕2 + ηtr2+ι,iv2+ι,i)
i=1 4ητ r2,i	t=1 i=1	t=1 i=1
d (θ )	τ d	τ d
≤ d∞X 4n J + d∞XXYtg(θt,i)+ XX2g(θt,i)(Ytvt,i/2 + ηtrt+ι,ivt+ι,i)
i=1 4ητ r2,i	t=1 i=1	t=1 i=1
d
+g∞D∞2 X
i=1
√T
2ηrτ +ι,i
d (Q 、	T d	T d
≤ d∞ √t X g≡+…X X Yt+2g∞ X X (Ytv2,i/2+
d
+g∞D∞2 X
i=1
√T
2ηrτ +ι,i
≤ D∞ √t X ι⅛)+
t1 i1
d
+g∞D∞2 X
i=1
√T
2ηrτ +ι,i
(22)
11
Under review as a conference paper at ICLR 2021
We apply Lemma 4 to the proof,
d T
WT) ≤ g∞D∞ X 而F
g∞D∞ dγ	g∞dG∞γ3
1 - λ + λ2(1 - γ)2(1 - λ2)
2g∞ dG∞γ2η
(i-γ)2(i- λ2)
+ D∞松X黯+
+
D2 X VT + D2 τX g(θι,i) + g∞D∞dγ + g∞dG∞γ2(γ + 2g∞x2η)
g∞ ∞ = 2ηrτ +ι,i + ∞v =酝T +	1-λ + λ2(1 - γ)2(1 - λ2)
d
≤ D∞√Tg∞ £(
i=1
1
2ηrτ +ι,i
+	1	)+ g∞D∞ dγ + g∞dG∞γ2 (Y + 2g∞ X2η)
+ 布21)+	1 - λ +	λ2(1 - γ )2(1 - λ2)
1	d
≤小近Tg X (
1
rT +1,i
+ T∞dG∞Y2 (Y + 2g∞ X2η)
+ 一λ2(1 - Y)2(1 - λ2)一
This is the regret bound of SEHB.
(23)
□
12