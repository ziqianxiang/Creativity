Under review as a conference paper at ICLR 2021
Bidirectionally Self-Normalizing
Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The problem of exploding and vanishing gradients has been a long-standing ob-
stacle that hinders the effective training of neural networks. Despite various tricks
and techniques that have been employed to alleviate the problem in practice, there
still lacks satisfactory theories or provable solutions. In this paper, we address the
problem from the perspective of high-dimensional probability theory. We provide
a rigorous result that shows, under mild conditions, how the exploding/vanishing
gradient problem disappears with high probability if the neural networks have
sufficient width. Our main idea is to constrain both forward and backward signal
propagation in a nonlinear neural network through a new class of activation func-
tions, namely Gaussian-Poincare normalized functions, and orthogonal weight
matrices. Experiments on both synthetic and real-world data validate our the-
ory and confirm its effectiveness on very deep neural networks when applied in
practice.
1 Introduction
Neural networks have brought unprecedented performance in various artificial intelligence
tasks (Graves et al., 2013; Krizhevsky et al., 2012; Silver et al., 2017). However, despite decades
of research, training neural networks is still mostly guided by empirical observations and successful
training often requires various heuristics and extensive hyperparameter tuning. It is therefore desir-
able to understand the cause of the difficulty in neural network training and to propose theoretically
sound solutions.
A major difficulty is the gradient exploding/vanishing problem (Glorot & Bengio, 2010; Hochreiter,
1991; Pascanu et al., 2013; Philipp et al., 2018). That is, the norm of the gradient in each layer
is either growing or shrinking at an exponential rate as the gradient signal is propagated from the
top layer to bottom layer. For deep neural networks, this problem might cause numerical overflow
and make the optimization problem intrinsically difficult, as the gradient in each layer has vastly
different magnitude and therefore the optimization landscape becomes pathological. One might
attempt to solve the problem by simply normalizing the gradient in each layer. Indeed, the adaptive
gradient optimization methods (Duchi et al., 2011; Kingma & Ba, 2015; Tieleman & Hinton, 2012)
implement this idea and have been widely used in practice. However, one might also wonder if there
is a solution more intrinsic to deep neural networks, whose internal structure if well-exploited would
lead to further advances.
To enable the trainability of deep neural networks, batch normalization (Ioffe & Szegedy, 2015)
was proposed in recent years and achieved widespread empirical success. Batch normalization is
a differentiable operation which normalizes its inputs based on mini-batch statistics and inserted
between the linear and nonlinear layers. It is reported that batch normalization can accelerate neural
network training significantly (Ioffe & Szegedy, 2015). However, batch normalization does not
solve the gradient exploding/vanishing problem (Philipp et al., 2018). Indeed it is proved that batch
normalization can actually worsen the problem (Yang et al., 2019). Besides, batch normalization
requires separate training and testing phases and might be ineffective when the mini-batch size is
small (Ioffe, 2017). The shortcomings of batch normalization motivate us to search for a more
principled and generic approach to solve the gradient exploding/vanishing problem.
1
Under review as a conference paper at ICLR 2021
Alternatively, self-normalizing neural networks (Klambauer et al., 2017) and dynamical isometry
theory (Pennington et al., 2017) were proposed to combat gradient exploding/vanishing problem. In
self-normalizing neural networks, the output of each network unit is constrained to have zero mean
and unit variance. Based on this motivation, a new activation function, scaled exponential linear
unit (SELU), was devised. In dynamical isometry theory, all singular values of the input-output
Jacobian matrix are constrained to be close to one at initialization. This amounts to initializing the
functionality of a network to be close to an orthogonal matrix. While the two theories dispense
batch normalization, it is shown that SELU still suffers from exploding/vanishing gradient problem
and dynamical isometry restricts the functionality of the network to be close to linear (pseudo-
linearity) (Philipp et al., 2018).
In this paper, we follow the above line of research to investigate neural network trainability. Our
contributions are three-fold: First, we introduce bidirectionally self-normalizing neural network
(BSNN) that consist of orthogonal weight matrices and a new class of activation functions which we
call Gaussian-Poincare normalized (GPN) functions. We show many common activation functions
can be easily transformed into their respective GPN versions. Second, we rigorously prove that the
gradient exploding/vanishing problem disappears with high probability in BSNNs if the width of
each layer is sufficiently large. Third, with experiments on synthetic and real-world data, we con-
firm that BSNNs solve the gradient vanishing/exploding problem to large extent while maintaining
nonlinear functionality.
2 Theory
In this section, we introduce bidirectionally self-normalizing neural network (BSNN) formally and
analyze its properties. All the proofs of our results are left to Appendix.
To simplify the analysis, we define a neural network in a restricted sense as follows:
Definition 1 (Neural Network). A neural network is a function from Rn to Rn composed of layer-
wise operations for l = 1, . . . , L as
h(l) = W(l)x(l),	x(l+1) = φ(h(l)),	(1)
where W(l) ∈ Rn×n, φ : R → R is a differentiable function applied element-wise to a vector, x(1)
is the input and x(L+1) is the output.
Under this definition, φ is called the activation function, {W(l)}lL=1 are called the parameters, n is
called the width and L is called the depth. Superscript (l) denotes the l-th layer ofa neural network.
The above formulation is similar to (Pennington et al., 2017) but we omit the bias term in (1) for
simplicity as it plays no role in our analysis.
Let E be the objective function of {W(l)}lL=1 and D(l) be a diagonal matrix with diagonal elements
Di(il) = φ0(hi(l)), where φ0 denotes the derivative of φ. Now, the error signal is back propagated via
d(L)= D(L)^∂LE1), d(I)= D(I)(W(I+I))Td(I+1),
and the gradient of the weight matrix for layer l can be computed as
∂W) = d?"
(2)
(3)
To solve the gradient exploding/vanishing problem, we constrain the forward signal x(l) and the
backward signal d(l) in order to constrain the norm of the gradient. This leads to the following
definition and proposition.
Definition 2 (Bidirectional Self-Normalization). A neural network is bidirectionally self-
normalizing if
kx(1)k2 = kx(2)k2 = ...= kx(L)k2,
(4)
kd(1)k2 = kd(2)k2 =... = kd(L)k2.	(5)
Proposition 1. If a neural network is bidirectionally self-normalizing, then
K S K If = ... = B 焉 Bf .
(6)
2
Under review as a conference paper at ICLR 2021
In the rest of this section, we derive the conditions under which bidirectional self-normalization is
achievable for a neural network.
2.1	Constraints on Weight Matrices
We constrain the weight matrices to be orthogonal since multiplication by an orthogonal ma-
trix preserves the norm of a vector. For linear networks, this guarantees bidirectionally self-
noramlizingnormalization and its further benefits are discussed in (Saxe et al., 2014). Even for
nonlinear neural networks, orthogonal constraints are shown to improve the trainability with proper
scaling (Mishkin & Matas, 2016; Pennington et al., 2017).
2.2	Constraints on Activation Functions
To achieve bidirectionally self-noramlizingnormalization for a nonlinear network, it is not enough
only to constrain the weight matrices. We also need to constrain the activation function in such a
way that both forward and backward signals are normalized. To this end, we propose the following
constraint that captures the relationship between a function and its derivative.
Definition 3 (Gaussian-Poincare Normalization). Function φ : R → R is Gaussian-Poincare
normalized if it is differentiable and
Ex 〜N (0,i)[φ(X)2] = Ex 〜N (o,i)[φ0(X)2 ] = 1.	⑺
The definition is inspired by the following theorem which shows the fundamental relationship be-
tween a function and its derivative under Gaussian measure.
Theorem 1 (Gaussian-Poincare Inequality (Bogachev, 1998)). Iffunction φ : R → R is differen-
tiable with bounded Ex 〜N (o,i)[Φ(x)2] and Ex 〜N(0,1)[φ0(χ)2] ,then
Varx〜N(o,i)[Φ(x)] ≤ Ex〜N(o,i)[Φ0(x)2].	(8)
Note that there is an implicit assumption that the input is approximately Gaussian for a Gaussian-
Poincare normalized (GPN) function. Even though this is standard in the literature (Klambauer et al.,
2017; Pennington et al., 2017; Schoenholz et al., 2017), we will rigorously prove that this assumption
is valid when orthogonal weight matrices are used in equation 1. Next, we state a property of GPN
functions.
Proposition 2. Function φ : R → R is Gaussian-Poincare normalized and Ex〜N(o,i) [φ(χ)] = 0 if
and only if φ(X) = X or φ(X) = -X.
This result indicates that any nonlinear function with zero mean under Gaussian distribution (e.g.,
Tanh and SELU) is not GPN. Now we show that a large class of activation functions can be converted
into their respective GPN versions using an affine transformation.
Proposition 3. For any differentiable function φ : R → R with non-zero and bounded
Ex〜N(o,i)[Φ(x)2] and Ex〜N(o,i)[φ0(x)2], there exist two constants a and b such that aφ(x) + b
is Gaussian-Poincare normalized.
To obtain a and b, one can use numerical procedure to compute the values of Ex〜N(o,i) [φ0(χ)2],
Ex〜N(0,i) [φ(χ)2] and Ex〜N(o,i) [φ(χ)] and then solve the quadratic equations
Ex 〜N (0,i)[a2φ0(X)2] = 1,	(9)
Ex 〜N (0,1) [(aφ(X) + b)2] = 1.	(IO)
We computed a and b (not unique) for several common activation functions with their default hy-
perparameters1 and the results are listed in Table 1. Note that ReLU, LeakyReLU and SELU are not
differentiable at X = 0 but they can be regarded as approximations of their smooth counterparts. We
ignore such point and evaluate the integrals for X ∈ (-∞, 0) ∪ (0, ∞).
With the orthogonal constraint on the weight matrices and the GaUSSian-POinCare normalization on
the activation function, we prove that bidirectionally self-noramlizingnormalization is achievable
with high probability under mild conditions in the next subsection.
1We use α = 0.01 for LeakyReLU, α = 1 for ELU and φ(χ) = x/(1 + exp(-1.702x)) for GELU.
3
Under review as a conference paper at ICLR 2021
	Tanh	ReLU	LeakyReLU	ELU	SELU	GELU
a	1.4674	1.4142	-1.4141-	1.2234	0.9660	1.4915
b	0.3885	0.0000	0.0000	0.0742	0.2585	-0.9097
Table 1: Constants for Gaussian-Poincare normalization of activation functions.
2.3 Norm-Preservation Theorems
The bidirectionally self-noramlizingnormalization may not be achievable precisely in general unless
the neural network is a linear one. Therefore, we investigate the properties of neural networks in
a probabilistic framework. The random matrix theory and the high-dimensional probability theory
allow us to characterize the behaviors of a large class of neural networks by its mean behavior, which
is significantly simpler to analyze. Therefore, we study neural networks of random weights whose
properties may shed light on the trainability of neural networks in practice.
First, we need a probabilistic version of the vector norm constraint.
Definition 4 (Thin-Shell Concentration). Random vector x ∈ Rn is thin-shell concentrated if for
any > 0
P{∣n kx∣∣2 - 1∣≥e} → 0	(11)
as n → ∞.
The definition is modified from the one in (Bobkov, 2003). Examples of thin-shell concentrated dis-
tributions include standard multivariate Gaussian and any distribution on the n-dimensional sphere
of radius √n.
Assumptions. To prove the main results, i.e., the norm-preservation theorems, we require the
following assumptions:
1.	Random vector x ∈ Rn is thin-shell concentrated.
2.	Random orthogonal matrix W = (w1, w2, ..., wn)T is uniformly distributed.
3.	Function φ : R → R is Gaussian-Poincare normalized.
4.	Function φ and its derivative are Lipschitz continuous.
The above assumptions are not restrictive. For Assumption 1, one can always normalize the input
vectors of a neural network. For Assumption 2, orthogonal constraint or its relaxation has already
been employed in neural network training (Brock et al., 2017). Note, in Assumption 2, uniformly
distributed means that W is distributed under Haar measure, which is the unique rotation invariant
probability measure on orthogonal matrix group. We refer the reader to (Meckes, 2019) for de-
tails. Furthermore, all the activation functions or their smooth counterparts listed in Table 1 satisfy
Assumptions 3 and 4.
With the above assumptions, we can prove the following norm-preservation theorems.
Theorem 2 (Forward Norm-Preservation). Random vector
(φ(w1T x), φ(w2T x), ..., φ(wnT x))	(12)
is thin-shell concentrated.
This result shows the transformation (orthogonal matrix followed by the GPN activation function)
can preserve the norm of its input with high probability. Since the output is thin-shell concentrated,
it serves as the input for the next layer and so on. Hence, the forward pass can preserve the norm of
its input in each layer along the forward path when n is sufficiently large.
Theorem 3 (Backward Norm-Preservation). Let D be the diagonal matrix whose diagonal el-
ements are Dii = φ0 (wiT x) and y ∈ Rn be a fixed vector with bounded kyk∞. Then for any
>0
p{n∣kDyk2 -∣M2∣≥e} → 0	(13)
as n → ∞.
4
Under review as a conference paper at ICLR 2021
This result shows that the multiplication by D preserves the norm of its input with high probability.
Since orthogonal matrix W also preserves the norm of its input, when the gradient error signal is
propagated backwards as in (2), the norm is preserved in each layer along the backward path when
n is sufficient large.
Hence, combining Theorems 2 and 3, we proved that bidirectionally self-noramlizingnormalization
is achievable with high probability if the neural network is wide enough and the conditions in the
Assumptions are satisfied. Then by Proposition 1, the gradient exploding/vanishing problem disap-
pears with high probability.
Sketch of the proofs. The proofs of Theorems 2 and 3 are mainly based on a phenomenon in
high-dimensional probability theory, concentration of measure. We refer the reader to (Vershynin,
2018) for an introduction to the subject. Briefly, it can be shown that for some high-dimensional
probability distributions, most mass is concentrated around a certain range. For example, while most
mass of a low-dimensional standard multivariate Gaussian distribution is concentrated around the
center, most mass of a high-dimensional standard multivariate Gaussian distribution is concentrated
around a thin-shell. Furthermore, the random variables transformed by Lipschitz functions are also
concentrated around certain values. Using this phenomenon, it can be shown that rows {wi} of a
random orthogonal matrix W in high dimension are approximately independent random unit vectors
and the inner product wiTx for thin-shell concentrated vector x can be shown to be approximately
Gaussian. Then from the assumptions that φ is GPN and φ and φ0 are Lipschitz continuous, the
proofs follow. Each of these steps is rigorously proved in Appendix.
3 Experiments
We verify our theory on both synthetic and real-world data. More experimental results can be
found in Appendix. In short, while very deep neural networks with non-GPN activations show
vanishing/exploding gradients, GPN versions show stable gradients and improved trainability in
both synthetic and real data. Furthermore, compared to dynamical isometry theory, BSNNs do not
exhibit pseudo-linearity and maintain nonlinear functionality.
3.1	Synthetic Data
We create synthetic data to test the norm-preservation properties of the neural networks. The input
x1 is 500 data points of random standard Gaussian vectors of 500 dimension. The gradient error
∂E∕∂xL+1 is also random standard Gaussian vector of 500 dimension. All the neural networks
have depth 200. All the weight matrices are random orthogonal matrices uniformly generated. No
training is performed.
In Figure 1, we show the norm of inputs and gradient of the neural networks of width 500. From
the results, we can see that with GPN, the gradient exploding/vanishing problem is eliminated to
large extent. The neural network with Tanh activation function does not show gradient explod-
ing/vanishing problem either. However, kx(l) k is close to zero for large l and each layer is close to
a linear one since Tanh(x) ≈ x when x ≈ 0 (pseudo-linearity), for which dynamical isometry is
achieved.
One might wonder if bidirectionally self-noramlizingnormalization has the same effect as dynamical
isometry in solving the gradient exploding/vanishing problem, that is, to make the neural network
close to an orthogonal matrix. To answer this question, we show the histogram of φ0 (hi(l)) in Fig-
ure 2. If the functionality of a neural network is close to an orthogonal matrix, since the weight
matrices are orthogonal, then the values of φ0(hi(l)) would concentrate around one (Figure 2 (a)),
which is not the case for BSNNs (Figure 2 (b)). This shows that BSNNs do not suffer from the
gradient vanishing/explosion problem while exhibiting nonlinear functionality.
In Figure 7 in Appendix, we show the gradient norm of BSNNs with varying width. There we note
that as the width increases, the norm of gradient in each layer of the neural network becomes more
equalized, as predicted by our theory.
5
Under review as a conference paper at ICLR 2021
(a) kx(l)k22/n, Tanh.
Layer
(b) kdWWE(ι)kF,Tanh.
(c) kx(l)k22/n,Tanh-GPN.
Layer
(d) k ∂E kF,Tanh-GPN.
1.2
0.8
1
50	100	150	200
Layer
(e) kx(l)k22/n,SELU.
Layer
Layer
(g) kx(l)k22/n,SELU-GPN.
⑴ k ∂WWE(i) kF, SELU.
Layer
(h) kdWWE(ι)kF, SELU-GPN.
Figure 1: Results on synthetic data with different activation functions. “-GPN” denotes the function
is Gaussian-Poincare normalized. ∣∣x(l) ∣∣2 denotes the l2 norm of the outputs of the l-th layer. n
denotes the width. ∣∣ dWI) ∣∣f is the FrobeniuS norm of the gradient of the weight matrix in the l-th
layer.
(a) Tanh.
4p06______________________________
2	1
0	0.5	1	1.5
(b) Tanh-GPN.
Figure 2: Histogram of φ0(hi(l)). The values of φ0(hi(l)) are accumulated for all units, all layers and
all samples in the histogram.
6
Under review as a conference paper at ICLR 2021
Epoch
(a) Tanh.
Figure 3: Test accuracy (percentage) during training on CIFAR-10. “-BN” denotes that batch nor-
malization is applied before the activation function.
一 SELU
一 SELU-GPN
一 SELU-BN
SELU-GPN-BN
(b) SELU.
		MNIST				CIFAR-10		
	Train	Test	Train	Test
Ianh	99.05 (87.39)	96.57 (89.32)	80.84 (27.90)	42.71 (29.32)
Tanh-GPN	99.81 (84.93)	95.54 (87.11)	96.39 (25.13)	40.95 (26.58)
ReLU	11.24(11.24)	11.35 (11.42)	10.00 (10.00)	10.00 (10.00)
ReLU-GPN	33.28 (11.42)	28.13 (11.34)	46.60 (10.09)	34.96 (9.96)
LeakyReLU	11.24(11.24)	11.35 (11.63)	10.00 (10.21)	10.00 (10.06)
LeakyReLU-GPN	43.17 (11.19)	49.28 (11.66)	51.85 (9.89)	39.38 (10.00)
-ELU	99.06 (98.24)	95.41 (97.48)	80.73 (42.39)	45.76 (44.16)
ELU-GPN	100.00 (97.86)	96.56 (96.69)	99.37 (43.35)	43.12(44.36)
SELU	99.86 (97.82)	97.33 (97.38)	29.23 (46.47)	29.55 (45.88)
SELU-GPN	99.92 (97.91)	96.97 (97.39)	98.24 (47.74)	45.90 (45.52)
GELU	11.24(12.70)	11.35 (1。28)	10.00 (10.43)	10.00 (10.00)
GELU-GPN	97.67 (11.22)	95.82 (9.74)	90.51 (10.00)	36.94 (10.00)
Table 2: Accuracy (percentage) of neural networks of depth 200 with different activation functions
on real-world data. The numbers in parenthesis denote the results when batch normalization is
applied before the activation function.
3.2 Real-World Data
We run experiments on real-world image datasets MNIST and CIFAR-10. The neural networks
have width 500 and depth 200 (plus one unconstrained layer at bottom and one at top to fit the
dimensionality of the input and output). We use stochastic gradient descent of momentum 0.5
with mini-batch size 64 and learning rate 0.0001. The training is run for 50 epochs for MNIST
and 100 epochs for CIFAR-10. We do not use data augmentation. Since it is computationally
expensive to enforce the orthogonality constraint, we simply constrain each row of the weight
matrix to have l2 norm one as a relaxation of orthogonality by the following parametrization
W = (v1/kv1k2,v2/kv2k2, ...,vn/kvnk2)T and optimize V = (v1,v2, ...,vn)T as an uncon-
strained problem.
We summarize the results in Table 2. We can see that, for activation functions ReLU, LeakyReLU
and GELU, the neural networks are not trainable. But once these functions are GPN, the neu-
ral network can be trained. GPN activation functions consistently outperform their unnormalized
counterparts in terms of the trainability, as the training accuracy is increased, but not necessarily
generalization ability. We show the test accuracy during training in Figure 3, from which we can
see the training is accelerated when SELU is GPN. ReLU, LeakyReLU and GELU, if not GPN, are
completely untrainable due to the vanished gradient (see Appendix).
We observe that batch normalization leads to gradient explosion when combining with any of the
activation functions. This confirms the claim of (Philipp et al., 2018) and (Yang et al., 2019) that
batch normalization does not solve the gradient exploding/vanishing problem. On the other hand,
without batch normalization the neural network with any GPN activation function has stable gradient
magnitude throughout training (see Appendix). This indicates that BSNNs can dispense with batch
normalization and therefore avoid its shortcomings.
7
Under review as a conference paper at ICLR 2021
4	Related Work
We compare our theory to several most relevant theories in literature. Akey distinguishing feature of
our theory is that we provide rigorous proofs of the conditions under which the exploding/vanishing
problem disappears. To the best of our knowledge, this is the first time that the problem is provably
solved for nonlinear neural networks.
4.1	Self-Normalizing Neural Networks
Self-normalizing neural networks enforce zero mean and unit variance for the output of each unit
with the SELU activation function (Klambauer et al., 2017). However, as pointed out in (Philipp
et al., 2018) and confirmed in our experiments, only constraining forward signal propagation does
not solve the gradient exploding/vanishing problem since the norm of the backward signal can grow
or shrink. The signal propagation in both directions needs to be constrained as in our theory.
4.2	Deep Signal Propagation
Our theory is developed from the deep signal propagation theory (Poole et al., 2016; Schoenholz
et al., 2017). Both theories require Eχ~N(0,1)[φ0(χ)2] = 1. However, ours also requires the quantity
Eχ~N(0,1)[Φ(x)2] to be one while in Poole et al.(Poole et al., 2016; Schoenholz et al., 2017) it can
be an arbitrary positive number. We emphasize that it is desirable to enforce Eχ~N(0,1)[φ(χ)2] = 1
to avoid trivial solutions. For example, if φ(x) = Tanh(x) with ≈ 0, then φ(x) ≈ x and the
neural network becomes essentially a linear one for which depth is unnecessary (pseudo-linearity
(Philipp et al., 2018)). This is observed in Figure 1 (a). Moreover, in (Poole et al., 2016; Schoenholz
et al., 2017) the signal propagation analysis is done based on random weights under i.i.d. Gaussian
distribution whereas we proved how one can solve gradient vanishing/exploding problem assuming
the weight matrices are orthogonal and uniformly distributed under Haar measure.
4.3	Dynamical Isometry
Dynamical isometry theory (Pennington et al., 2017) enforces the Jacobian matrix of the input-
output function of a neural network to have all singular values close to one. Since the weight
matrices are constrained to be orthogonal, it is equivalent to enforce each D(l) in (2) to be close to
the identity matrix, which implies the functionality of neural network at initialization is close to an
orthogonal matrix (pseudo-linearity). This indeed enables trainability since linear neural networks
with orthogonal weight matrices do not suffer from the gradient exploding/vanishing problem. As
neural networks need to learn a nonlinear input-output functionality to solve certain tasks, during
training the weights of a neural network are unconstrained so that the neural network would move
to a nonlinear region where the gradient exploding/vanishing problem might return. In our theory,
although the orthogonality of weight matrices is also required, we approach the problem from a
different perspective. We do not encourage the linearity at initialization. The neural network can
be initialized to be nonlinear and stay nonlinear during the training even when the weights are
constrained.
5	Conclusion
In this paper, we have introduced bidirectionally self-normalizing neural network (BSNN) which
constrains both forward and backward signal propagation using a new class of Gaussian-Poincare
normalized activation functions and orthogonal weight matrices. BSNNs are not restrictive in the
sense that many commonly used activation functions can be Gaussian-Poincare normalized. We
have rigorously proved that gradient vanishing/exploding problem disappears in BSNNs with high
probability under mild conditions. Experiments on synthetic and real-world data confirm the validity
of our theory and demonstrate that BSNNs have excellent trainability without batch normalization.
Currently, the theoretical analysis is limited to same width, fully-connected neural networks. Future
work includes extending our theory to more sophisticated networks such as convolutional architec-
tures as well as investigating the generalization capabilities of BSNNs.
8
Under review as a conference paper at ICLR 2021
References
Tom Alberts and Davar Khoshnevisan. Calculus on Gauss Space: An Introduction to Gaussian
Analysis. 2018.
Sergey G Bobkov. On concentration of distributions of random weighted sums. Annals of Probabil-
ity, 2003.
Vladimir Igorevich Bogachev. Gaussian Measures. American Mathematical Society, 1998.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. International Conference on Learning Representations, 2017.
John Dawkins. Normalized vector of gaussian variables is uniformly distributed on the sphere. 2016.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 2011.
Nate Eldredge. When do φ2 and φ02 have the same expectation under a gaussian random variable?
MathOverflow, 2020. https://mathoverflow.net/q/351553.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. International Conference on Artificial Intelligence and Statistics, 2010.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. IEEE International Conference on Acoustics, Speech and Signal Processing,
2013.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Uni-
Versitat Munchen, 1991.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized
models. Advances in Neural Information Processing Systems, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. International Conference on Machine Learning, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 2015.
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and SePP Hochreiter. Self-normalizing
neural networks. Advances in Neural Information Processing Systems, 2017.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deeP convo-
lutional neural networks. Advances in Neural Information Processing Systems, 2012.
Elizabeth S Meckes. The Random Matrix Theory of the Classical Compact Groups. Cambridge
University Press, 2019.
Dmytro Mishkin and Jiri Matas. All you need is a good init. International Conference on Learning
Representations, 2016.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. International Conference on Machine Learning, 2013.
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deeP learn-
ing through dynamical isometry: theory and Practice. Advances in Neural Information Processing
Systems, 2017.
George PhiliPP, Dawn Song, and Jaime G Carbonell. The exPloding gradient Problem demystified-
definition, Prevalence, imPact, origin, tradeoffs, and solutions. arXiv preprint arXiv:1712.05577,
2018.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. ExPonen-
tial exPressivity in deeP neural networks through transient chaos. Advances in Neural Information
Processing Systems, 2016.
9
Under review as a conference paper at ICLR 2021
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. International Conference on Learning Represen-
tations, 2014.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 2017.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci-
ence. Cambridge University Press, 2018.
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S Schoenholz. A
mean field theory of batch normalization. International Conference on Learning Representations,
2019.
10
Under review as a conference paper at ICLR 2021
Appendix A Proofs
Proposition 1. If a neural network is bidirectionally self-normalizing, then
«∂Wi) IF=I KIF=...=« ∂Wl) «f .
(14)
Proof. For each l, we have
«∂W) k	I	( ∂E ( ∂E、T、 = Vtrace JW(I) (∂W(I)))	(15)
	=qtrace(d(I)(X(I))T x(l)(d(I))T)	(16)
	=qtrace((X(I))T X(I) (d(I))T d(l))	(17)
=J(X(I))TX(I) J(d(I))Td(I) = kx(l)k2kd(l)k2. By the definition of bidirectional self-normalization, we have ∣∣ ∂WEr kF = ∙∙		(18) (19) .=k ∂WE(l) kF.	□
Proposition 2. Function φ : R → R is Gaussian-Poincare normalized and Ex〜N(0,1)[φ(x)] = 0 if
and only if φ(x) = x or φ(x) = -x.
Proof. Since Ex〜N(o,i)[Φ(χ)2] < ∞ and Ex〜N(o,i)[φ0(x)2] < ∞, φ(χ) and φ0(x) can be expanded
in terms of Hermite polynomials. Let the Hermite polynomial of degree k be		
Hk(x) = (√f exP( W)与 exP(-W)		(20)
and due to Hk (x) = √kHk-ι (x), we have ∞ φ(x) =	akHk(x),		(21)
k=0 ∞ φ0(x) = E kkαHHk-i(x).		(22)
k=1 Since Ex〜N(o,i)[Φ(χ)] = 0, we have ao = Ex 〜N (o,i)[Ho(x)Φ(x)]		(23)
=Ex 〜N (0,i)[φ(X)]		(24)
= 0.		(25)
Since Ex 〜N (0,i)[φ(X)2] = Ex 〜N (o,i)[φ0(X)2] = 1		(26)
and Hermite polynomials are orthonormal, we have ∞∞ Ex〜N(0,i)[φ(X)2] = X ak = Ex〜N(0,1) [φ0(X)2] = X kak =	1.	(27)
k=1	k=1 Therefore, we have ∞∞ Xka2k-Xa2k=0		(28)
k=1	k=1 that is ∞ X(k- 1)a2k = 0.		(29)
k=2
Since each term in Pk∞=2(k - 1)a2k is nonnegative, the only solution is ak = 0 for k ≥ 2. And since
Ex〜N(0,i) [Φ(x)2] = al = 1, We have aι = ±1. Hence, φ(x) = ±Hι (x) = ±x.	□
11
Under review as a conference paper at ICLR 2021
Proposition 3. For any differentiable function φ : R → R with non-zero and bounded
Ex〜N(0,i)[Φ(x)2] and Ex〜N(o,i)[Φ0(x)2], there exist two constants a and b such that aφ(x) + b
is Gaussian-Poincare normalized.
Proof. Let 夕(x) = φ(x) + c. Then let
ψ(C)= Ex〜N(0,1)[夕(X)2] - Ex〜N(0,1)[(OO(X))2]	(3O)
=Varx〜N(0,1)[2(X)] + (Ex〜N(0,1)[夕(X)D2 - Ex〜N(0,1)[(OO(X))2]	(31)
=Varx 〜N (0,1)[Φ(x)] + (Ex 〜N (0,1)[Φ(x)] + c)2 — Ex 〜N (0,1)[(Φo(x))2].	(32)
Therefore, ψ(c) is a quadratic function of c. We also have ψ(c) > 0 as c → ∞ and
ψ(-Ex〜N(0,1)[Φ(x)]) ≤ 0 due to Gaussian-Poincare inequality. Hence, there exists C for which
ψ(c) = 0 such that Ex 〜N(0,1) [(Φ(x)+c)2] = Ex 〜N (0,1)[Φo(x)2]. Let a =(Ex 〜N (0,1)[φ0 32])-1/2
and b = ac, we have Ex〜N(0,1)[(aΦ3+ b)2] = Ex〜N(0,1)[(aφ0(X))2] = L	口
The proof is largely due to (Eldredge, 2020) with minor modification in here.
Assumptions.
1.	Random vector x ∈ Rn is thin-shell concentrated.
2.	Random orthogonal matrix W = (w1, w2, ..., wn)T is uniformly distributed.
3.	Function φ : R → R is Gaussian-Poincare normalized.
4.	Function O and its derivative are Lipschitz continuous.
Theorem 2	(Forward Norm-Preservation). Random vector
(O(w1T x), O(w2T x), ..., O(wnT x))	(33)
is thin-shell concentrated.
Theorem 3	(Backward Norm-Preservation). Let D be the diagonal matrix whose diagonal el-
ements are Dii = OO(wiTx) and y ∈ Rn be a fixed vector with bounded kyk∞. Then for any
>0
p{n∣kDyk2 -kyk2∣≥e} → 0	(34)
as n → ∞.
Notations. Sn-1 = {χ ∈ Rn : ∣∣x∣∣2 = 1}. O(n) is the orthogonal matrix group of size n. 1{.}
denotes the indicator function. 0n denotes the vector of dimension n and all elements equal to zero.
In denotes the identity matrix of size n × n.
Lemma 1. Ifrandom variable X 〜N (0,1) and function f : R → R is Lipschitz continuous, then
random variable f (X) is sub-gaussian.
Proof. Due to the Gaussian concentration theorem (Theorem 5.2.2 in (Vershynin, 2018)), we have
kf(X)-E[f(X)]kψ2 ≤CK
(35)
where ∣∣ ∙ ∣∣ψ2 denotes sub-gaussian norm, C is a constant and K is the Lipschitz constant of f. This
implies f(X) - E[f (X)] is sub-gaussian (Proposition 2.5.2 in (Vershynin, 2018)). Therefore f(X) is
sub-gaussian (Lemma 2.6.8 in (Vershynin, 2018)).	□
Lemma 2. Letx = (X1, X2, ..., Xn) ∈ Rn be a random vector that each coordinate Xi is independent
and sub-gaussian and E[Xi2] = 1. Let y = (y1, y2, ..., yn) ∈ Rn be a fixed vector with bounded
∣y∣∞. Then
p{ 1∣X M X y2∣"}→ 0
(36)
as n → ∞.
12
Under review as a conference paper at ICLR 2021
Proof. Since yixi is sub-gaussian, then yi2xi2 is sub-exponential (Lemma 2.7.6 in (Vershynin,
2018)). Since E[yi2xi2] = yi2E[xi2] = yi2, yi2xi2 - yi2 is sub-exponential with zero mean (Exer-
cise 2.7.10 in (Vershynin, 2018)). Applying Bernsteins inequality (Corollary 2.8.3 in (Vershynin,
2018)), We proved the lemma.	□
Lemma 3. Let Z 〜N(0n, In). Thenfor any 0 < δ < 1
P{z ∈ Rn : (1 — δ)√n ≤ ∣∣zk2 ≤ (1 + δ)√n} ≥ 1 — 2exp(-nδ2).	(37)
See (Alberts & Khoshnevisan, 2018) (Theorem 1.2) for a proof.
Lemma 4. Let Z 〜N(0n,In). Then z∕∣∣z∣∣2 is uniformly distributed on SnT.
See (DaWkins, 2016) for a proof.
Lemma 5. Let Z = (z1,z2,…，zn) 〜 N (0n,In), a = (a1,a2,…，an) be a fixed vector with
bounded ∣a∣∞ and f : R → R be a continuous function. Then for any > 0
p{1∣Xyif(√n∕kzk2zi) - Xyif(zi)∣ > ɛθ → 0	(38)
ii
asn → ∞.
Proof. Since
1∣ X aif (√n∕kz∣2Zi) - X aif (zi)∣ ≤ ɪ X ∣a∕ ∙ |f (√n∕∣z∣2Zi) - f (zi)|,
if, as n → ∞,
p{ n X ∣ai∣∙∣f (√n∕kz∣2Zi) - f (Zi)I > e} → 0,
i
then
p{n∣Xyif(√n∕kz∣2Zi) -Xyif(zi)∣ > , →0.
ii
For0 < δ < 1, let
A = {z ∈ Rn : n X ∣ai∣∙ If (√n∕kz∣2Zi) - f (Zi)I > e},
i
% = {z ∈ Rn ： (1 -δ)√n ≤ ∣∣z∣2 ≤ (1 + δ)√n}.
Then
p{ 1X IaiHf (√n∕kz∣2Zi)-f(Zi)∣ >
i
1{z∈A} dz
1{z∈A}dz +	1{z∈A}dz.
Let δ = n-1/4. From Lemma 3, We have, as n → ∞,
1{z∈A}dz ≤	dz = 1 - P{z ∈ Uδ} ≤ 2 exp(-nδ2) → 0.
(39)
(40)
(41)
(42)
(43)
(44)
(45)
(46)
For Z ∈ Uδ and δ = n-1/4, we have ∣∣z∣2 → √n, √n∕kz∣2Zi → Zi and therefore f (√n∕kz∣2Zi) →
f(Zi) as n → ∞. Hence, U 1{z∈A}dz → 0, asn → ∞.
□
13
Under review as a conference paper at ICLR 2021
Lemma 6. Let random matrix W be uniformly distributed on O(n) random vector θ be uniformly
distributed on Sn-I and random vector X ∈ Rn be thin-shell concentrated. Then WX → √nθ as
n → ∞.
Proof. Let y ∈ Rn be any vector with ∣∣yk2 = √n and e = (√n, 0,…，0) ∈ Rn. Since W is
uniformly distributed, Wy has the same distribution as We. We is the first row of √nW, which is
equivalent to random vector √nθ. Since X is thin-shell concentrated, X → kXn X = y and therefore
WX → √nθ as n → ∞.
□
Proofof Theorem 2. Let Z = (zι, z2,…,Zn)〜 N(0, I). Due to Lemma 1,randomvariable φ(zi) is
sub-gaussian. Since φ is Gaussian-Poincare normalized, Ezi〜N(o,i)[Φ(zi)2] = 1. Applying Lemma
2 with each yi = 1, we have for > 0
Pni1 X Φ(Zi)2- 1∣≥e} → 0	(47)
as n → ∞.
Due to Lemma 4 and 5 (with each ai = 1), for random vector θ = (θ1, θ2, ..., θn) uniformly
distributed on Sn-1, we have
Pni1 x Φ(√nθi)2 - n x eg)21 ≥ e} →0	(48)
ii
and therefore
Pn∣1 Xφ(√nθi)2 — 1∣ ≥ e} → 0	(49)
i
as n → ∞.
Then from Lemma 6, we have WX → √nθ and therefore
Pn ∣1 X Φ(wTx)2 — 1∣≥e} → O	(50)
i
as n → ∞.	□
Proof of Theorem 3. Let z = (z1, z2, ..., zn) be the standard multivariate Gaussian random vectors.
Due to Lemma 1, random variable φ0(zi) is sub-gaussian. Since φ is Gaussian-Poincare normalized,
Ezi〜N(o,i)[Φ0(zi)2] = 1. Applying Lemma2, we have
Pn 1∣Xy2φ0(zi)2 — y2i ≥ e} → 0	(51)
i
as n → ∞.
Due to Lemma 4 and 5 (with each ai = yi2), for random vector θ = (θ1, θ2, ..., θn) uniformly
distributed on Sn-1, we have
Pn∣1 Xy2φ0(√nθi)2 — 1Xy2Φ0(Zi)2∣ ≥ e} → 0	(52)
ii
and therefore
Pn∣1 Xy2φ0(√nθi)2 — y2∣≥ e} → 0	(53)
i
as n → ∞.
Then from Lemma 6, we have Wx → √nθ and therefore
PninXy2φ0(WTx)2 — y21 ≥ eO →0	(54)
i
as n → ∞.
□
14
Under review as a conference paper at ICLR 2021
Appendix B	Additional Experiments
Due to the space limitation, we only showed the experiments with Tanh and SELU activation func-
tions in the main text. In this section, we show the experiments with ReLU, LeakyReLU, ELU
and SELU. Additionally, we also measure the gradient exploding/vanishing during training on the
real-world data.
B.1	Synthetic Data
We show the figures of the experimental results in addition to the ones in the main text.
0.6 ---------1---------1---------1---------
0.4
0.2 1
0^---------------------------------------
0	50	100	150	200
Layer
(a) kx(l)k22/n, ReLU.
Layer
(b) k∂WWE(1)kF,ReLU.
(c) kx(l)k22/n, ReLU-GPN.
Layer
(d) k瑞kF,ReLU-GPN.
0.6 --------1--------1--------1——
0.4
0.2 I
0^------------------------------
0	50	100	150
Layer
(e) kx(l) k22/n, LeakyReLU.
200
Layer
(g) kx(l)k22/n, LeakyReLU-GPN.
Figure 4: Results on synthetic data with different activation functions. “-GPN” denotes the function
is Gaussian-Poincare normalized. ∣∣x(l)∣∣2 denotes the l2 norm of the outputs of the l-th layer. n
denotes the width. ∣∣ ∂W) ∣∣f is the Frobenius norm of the gradient of the weight matrix in the l-th
layer.
⑴ k 瑞kF，LeakyReLU.
Layer
(h) k 瘾 kF，LeakyReLU-GPN.
15
Under review as a conference paper at ICLR 2021
Layer
Layer
(a) kx(l)k22/n, ELU.
Layer
(c) kx(l)k22/n, ELU-GPN.
(b) k ∂WWE(i) kF,ELU.
Layer
(d) kdWl)kF,ELU-GPN.
3j10-59	,	I
2.5厂
2
1.5 ------1-------1-------1-------ɪ
0	50	100	150	200
Layer
(f) k ∂WWl) kF, GELU.
0.4	1	1	1
0.2
0^-----------------------------------------
0	50	100	150	200
Layer
(e) kx(l) k22/n, GELU.
Layer
(g) kx(l)k22/n,GELU-GPN.
Layer
(h) k 婿)kF, GELU-GPN.
Figure 5: Results on synthetic data with different activation functions. “-GPN” denotes the function
is Gaussian-Poincare normalized. ∣∣x(l)∣∣2 denotes the l2 norm of the outputs of the l-th layer. n
denotes the width. ∣∣ ∂Wi) IIf is the Frobenius norm of the gradient of the weight matrix in the l-th
layer.
16
Under review as a conference paper at ICLR 2021
(a) Tanh.
4×106
2
0

0
0.5
1
1.5
(b) Tanh-GPN.
(c) ReLU.
0
0
2
1
3 ×107
0.5
1
1.5
(d) ReLU-GPN.
(e) LeakyReLU.
0
0
2
1
3 χi07
0.5
1
1.5
(f) LeakyReLU-GPN.
(g) ELU.
(i) SELU.
3义107
2
00	0.5	1	1.5
(h) ELU-GPN.
3x107
2
00	0.5	1	1.5	2
(j) SELU-GPN.
(k) GELU.
Figure 6: Histogram of φ0(hi(l)). The values of φ0(hi(l)) are accumulated for all units, all layers and
all samples in the histogram. Except for ELU, none of them has values concentrated around one.
10
5
0
×106
0	0.5
1
(l) GELU-GPN.
1.5
17
Under review as a conference paper at ICLR 2021
(a) Tanh-GPN.
(b) ReLU-GPN.
15----------1----------1-----------
10
5^⅛H~S→
500	1000	1500
Width
(c) LeakyReLU-GPN.
(d) ELU-GPN.
(e) SeLU-GPN.
Figure 7: Gradient norm ratio for different layer width on synthetic data. The ratio is defined
as maxi || 般ɑ)∣∣f/ mini ∣∣ ∂WEι IIf∙ The width ranges from 100 to 1500. The error bars show
standard deviation.
(f) GELU-GPN.
B.2	Real-World Data
In Figure 8, we show the test accuracy during training on MNIST. In Figure 9, we show the test
accuracy during training on CIFAR-10.
In Figure 10, 11, 12 and 13, we show a measure of gradient exploding/vanishing during training for
different activation functions. The measure is defined as the ratio of the maximum gradient norm
and the minimum gradient norm across layers. Since we use the parametrization
W =(」»,..., A YT
IkVIk2 ,kv2k2 ,	, kvnk2>
(55)
with V = (v1, v2, ..., vn)T, the gradient norm ratio is defined on the unconstrained weights V, that
is,
maχι k∂VE)If
mini k ∂E ∣∣F
(56)
Note that for ReLU, LeakyReLU and GELU, the gradient vanishes during training in some exper-
iments and therefore the plots are empty. From the figures, we can see that batch normalization
leads to gradient explosion especially at the early stage of training. On the other hand, without batch
normalization, the gradient is stable throughout training for GPN activation functions.
18
Under review as a conference paper at ICLR 2021
(a) Tanh.
(b) ReLU.
100
50
0

一 LeakyReLU
一 LeakyReLU-GPN
一 LeakyReLU-BN
LeakyReLU-GPN-BN
10
20
30
40
50
(d) ELU.
100
Epoch
(c) LeakyReLU.
(e) SELU.
Figure 8: Test accuracy (percentage) during training on MNIST.
0
50
10
20
30
40
50
Epoch
- GELU-GPN
- GELU-BN
GELU-GPN-BN
(f) GELU.
(a) Tanh.
(b) SELU.
(c) ReLU.
(d) LeakyReLU.
一 LeakyReLU
一 LeakyReLU-GPN
-LeakyReLU-BN
LeakyReLU-GPN-BN
(e) ELU.
(f) GELU.
Figure 9: Test accuracy (percentage) during training on CIFAR-10.
19
Under review as a conference paper at ICLR 2021
x104
3 --
2
1
θL---L-.——L--._1---'--
1234
×104
(c) Tanh-BN.
100
50
0
1234
x104
(b) Tanh-GPN.
0
2
1
×104
3
1
2
3
4
x104
(d) Tanh-GPN-BN.
1 --1---1---1---1--
0.5
o∣--1---1---1---1--
1234
x104
(e) ReLU.
300
200
100
0
1
2
3
4
x104
(f) ReLU-GPN.
×104
(i) LeakyReLU.
0
5
10×1016
1
2
3
4
x104
(h) ReLU-GPN-BN.
(j) LeakyReLU-GPN.
0
4
2
×1016
1
2
3
4
×104
0
2
x1016
4
1
2
3
4
x104
(k) LeakyReLU-BN.
(l) LeakyReLU-GPN-BN.
Figure 10: Gradient norm ratio during training on MNIST. Horizontal axis denotes the mini-batch
updates. Vertical axis denotes the gradient norm ratio maxi ∣∣ ∂∂EEi) ∣∣f / mini k ∂∂EE) ∣∣f ∙ The gradient
vanishes (∣ ∂VE) ∣∣f ≈ 0)for ReLU and LeakyReLU during training and hence the plots are empty.
20
Under review as a conference paper at ICLR 2021
(a) ELU.
40
20
0
1234
×104
(b) ELU-GPN.
(c) ELU-BN.
2000
1000
1234
x104
(e) SELU.
1500
1000
500
0^------1---1----1--
1234
x104
(g) SELU-BN.
1 ---1---1---1----1--
0.5
o∣---1---1---1----1--
1234
×104
(i)	GELU.
1		1-----1---1--1--
0.5
o∣---1---1---1--1--
1234
×104
(k)	GELU-BN.
1000 -
500
0^=-------------------
1234
×104
(d) ELU-GPN-BN.
(f) SELU-GPN.
1500 --'----'---'----'--η
1000
500
0^-------1-----1---'-
1234
x104
(h) SELU-GPN-BN.
x1020	...............
8
6
4
2
θL-----..1....1	….一.J.[
1234
x104
(l)	GELU-GPN-BN.
Figure 11: Gradient norm ratio during training on MNIST. Horizontal axis denotes the mini-batch
updates. Vertical axis denotes the gradient norm ratio maxi ∣∣ ∂∂EEi) ∣∣f / mini k ∂∂EE) ∣∣f ∙ The gradient
vanishes (∣ ∂VE) ∣∣f ≈ 0) for GELU and GELU-BN during training and hence the plots are empty.
21
Under review as a conference paper at ICLR 2021
3 :
2
1
oL-----....1….--1---
246
×104
(c)	Tanh-BN.
x104	,	,	,	I
3
2
1
0匚....1	....------1----
246
x104
(d)	Tanh-GPN-BN.
1 ---1---1---1---
0.5
o∣---1---1---1---
246
x104
(e) ReLU.
Figure 12: Gradient norm ratio during training on CIFAR-10. Horizontal axis denotes the mini-
batch updates. Vertical axis denotes the gradient norm ratio max? ∣∣ ∂V‰IIf/ mini k a∂EEi) IIf∙ The
gradient vanishes (∣ a∂EEi) IIf ≈ 0) for ReLU and LeakyReLU during training and hence the plots
are empty.
22
Under review as a conference paper at ICLR 2021

1000
500
0
246
x104
(c) ELU-BN.
1000
500
0
246
x104
(d) ELU-GPN-BN.
(e) SELU.
(f) SELU-GPN.
x104
(i) GELU-BN.
1 ---1---1---1---
0.5
o∣---1---1---1---
246
x104
1500
1000
500
0
246
×104
(h) SELU-GPN-BN.
1 ----------1----------1----------1---------
0.5
0-----------1----------1----------1---------
246
x104
(k) GELU-BN.
(l) GELU-GPN-BN.
Figure 13: Gradient norm ratio during training on CIFAR-10. Horizontal axis denotes the mini-
batch updates. Vertical axis denotes the gradient norm ratio maxi ∣∣ ∂V‰IIf/ mini k a∂EEi) IIf∙ The
gradient vanishes (∣ 盗/)∣f ≈ 0) for GELU, GELU-BN and GELU-GPN-BN during training and
hence the plots are empty. For GELU-GPN-BN, both gradient exploding and gradient vanishing are
observed.
23