Under review as a conference paper at ICLR 2021
Understanding Adversarial Attacks On Au-
TOENCODERS
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial vulnerability is a fundamental limitation of deep neural networks
which remains poorly understood. Recent work suggests that adversarial attacks
on deep neural network classifiers exploit the fact that non-robust models rely
on superficial statistics to form predictions. While such features are semantically
meaningless, they are strongly predictive of the input’s label, allowing non-robust
networks to achieve good generalization on unperturbed test inputs. However, this
hypothesis fails to explain why autoencoders are also vulnerable to adversarial at-
tacks, despite achieving low reconstruction error on clean inputs. We show that
training an autoencoder on adversarial input-target pairs leads to low reconstruc-
tion error on the standard test set, suggesting that adversarial attacks on autoen-
coders are predictive. In this work, we study the predictive power of adversarial
examples on autoencoders through the lens of compressive sensing. We character-
ize the relationship between adversarial perturbations and target inputs and reveal
that training autoencoders on adversarial input-target pairs is a form of knowledge
distillation, achieved by learning to attenuate structured noise.
1	Introduction
Szegedy et al. (2013) observed that small imperceptible perturbations can cause accurate image clas-
sifiers to confidently change their prediction. The pernicious difficulty of defending neural networks
from adversarial attacks (Athalye et al., 2018; Madry et al., 2018; Carlini et al., 2019) motivates
the study of the root causes and properties of adversarial vulnerability (Goodfellow et al., 2014;
Tsipras et al., 2019; Simon-Gabriel et al., 2018; Moosavi-Dezfooli et al., 2017; Gilmer et al., 2018).
Ilyas et al. (2019) hypothesize that non-robust yet predictive features are to blame for adversar-
ial vulnerability. More precisely, the authors claim that adversarial attacks exploit the tendency of
neural networks to rely on semantically meaningless patterns in training images which are highly
predictive of the input’s label. These patterns may become correlated with a target label when an
adversarial perturbation is added to the input. As a consequence, follow-up work describes adver-
sarial robustness as a special case of robustness to distributional shift (Gilmer & Hendrycks, 2019).
Yin et al. (2019) show that imperceptible features in the high frequency spectrum of natural images
are strongly predictive of class. Jacobsen et al. (2019) use an invertible neural network to study the
features used by a downstream classifier and conclude that a classifier can be excessively invariant
to class-specific content in the input, relying on a few highly predictive features. Zhang & Zhu
(2019) find that adversarially trained classifiers are more robust to changes in texture and rely more
on global features such as shape.
So far the discussion around adversarial vulnerability has mostly focused on classification, over-
looking the fact that autoencoders are also vulnerable to adversarial attack (Kos et al., 2018; Cemgil
et al., 2020; Willetts et al., 2019; Gondim-Ribeiro et al., 2018). While classifiers might rely on
a possibly non-robust subset of class-predictive features, autoencoders are required to effectively
compress an input image to then reconstruct it with low reconstruction error. As a consequence
we cannot argue that encoders rely on semantically meaningless yet useful features to explain their
adversarial vulnerability.
Cemgil et al. (2020) attribute the adversarial vulnerability of Variational Autoencoders to the limited
support of the training set, but their analysis makes no distinction between adversarial perturbations
and random input noise. We show instead, that targeted attacks on autoencoders are structured. Ilyas
1
Under review as a conference paper at ICLR 2021
et al. (2019) make the striking observation that a classifier trained on adversarial input-target pairs
generalizes to the standard test set, in support of the hypothesis that adversarial attacks exploit non-
robust yet predictive features of the input image. We make similar observations with autoencoders.
Unlike Ilyas et al. (2019), our discussion does not attribute adversarial vulnerability to the presence
of non-robust features within the data, instead we focus on the relationship between worst-case
noise and the low intrinsic dimension of the data. Additionally, we uncover a mechanism with
which an encoder trained on adversarial input-target pairs learns to represent the adversarial image
and the clean target image similarly. We describe this behaviour as knowledge distillation achieved
by learning to attenuate structured noise.
In this work, we study targeted attacks on auto-encoders, where a norm-bounded perturbation δ is
added to a source image xs so as to produce a similar representation to that of a randomly selected
target image xt. Denoting the encoder by E(.) and the decoder by D(.) our attack objective is
formulated as shown in 1.2. Similar objectives have been used by Kos et al. (2018); Cemgil et al.
(2020); Gondim-Ribeiro et al. (2018); Sabour et al. (2016). The success of the attack is determined
by the squared error between the target xt and the reconstruction D ◦ E(xs + δ).
δ* = argminkE(Xt) - E(Xs + δ)k2
kδk22 ≤
(1.1)
(1.2)
With the attack objective shown above we generate a training set of adversarial input-target pairs
(Xs + δ, Xt). We observe that a newly initialized autoencoder trained on the adversarial training set
learns to reconstruct unperturbed images from the standard test set. We conclude that adversarial
perturbations which fool encoders are predictive of the target image Xt. More precisely, adversarial
perturbations are predictably related to the low-dimensional representation of Xt, allowing a decod-
ing procedure to reconstruct a target Xt with bounded error from any adversarial input Xs + δ.
The goal of our paper is to provide insight, explaining why adversarial attacks on autoencoders
are predictive. We first study adversarial attacks on a linear encoder, assuming the data admits a
sparse representation with respect to a dictionary of atoms or features. We show how adversarial
perturbations are closely related to the sparse representations of the source Xs and target input Xt .
We use our analysis to show that training an autoencoder on adversarial input-target pairs is a form of
knowledge distillation, achieved by learning to attenuate structured noise. We support our findings
with experiments on MNIST and CelebA.
2	Preliminaries
To shed light on the structure of targeted attacks, we assume that the data admits a sparse represen-
tation with respect to a dictionary of atoms or features D. Recent work has also leveraged assump-
tions regarding the generative model of data, in particular Gilmer et al. (2018); Fawzi et al. (2018)
have appealed to the manifold hypothesis to understand adversarial attacks. However, an arbitrary
manifold may not be a faithful model for natural images, unlike the sparse coding model which is
behind the success of many image denoising and compression algorithms (Candes & Wakin, 2008).
Additionally, this assumption allows us to investigate targeted attacks on a linear encoder via the
compressive sensing framework.
Compressive sensing is an effective method for simultaneous sparse signal acquisition and compres-
sion Candes et al. (2005). It’s aim is to answer the following question, given a sparse vector x, how
must one project x onto a lower dimensional vector y such that x can be recovered from y? Recov-
ery with low error is made possible by exploiting the signal sparsity. A signal x in RN is said to be
s-sparse if at most s components of x are non-zero. A measurement matrix Φ ∈ RM ×N acquires
M measurements of x to form a lower dimensional representation y ∈ RM. Since N >> M, with-
out further assumptions, the problem of recovering x from y is ill-posed since it involves solving
an under-determined system of equations. However, the sparsity of x proves useful by providing
uniqueness and stability guarantees.
In the compressive sensing literature, the recovery of x from y is expressed as the P0 problem
shown in 2.1. P0 is guaranteed to have a unique solution x if the sparsity ofx is bounded as shown
in equation 2.2
2
Under review as a conference paper at ICLR 2021
P0 : minkxk0 s.t. Φx = y
x0
S = kxk0 ≤ 1(1 + μ⅛)
Where μ(Φ) is the mutual coherence of Φ defined as
〃&)=ImT p≡‰
(2.1)
(2.2)
(2.3)
This implies that, in the worst case, a signal x which does not satisfy the sparsity condition in 2.2
may have a counterpart signal with similar sparsity and measurement vector. In this case, the P0
problem does not admit a unique solution. Even when the signal x satisfies the sparsity condition,
we may obtain a counterpart signal for x under an l2-norm constraint. In section 3 we describe
adversarial attacks on a linear encoder Φ as norm-bounded, dense counterparts of sparse vectors
which exploit the redundancy of Φ.
3	Adversarial vulnerability of a Linear Encoder
We begin by studying a motivating toy example using a synthetic dataset of structured sparse signals.
Our constructed dataset consists of 28 x 28 images made up of at most 5 Discrete Fourier Transform
(DFT) components. A 28 x 28 image can consist of at most (28/2 + 1)2 = 225 discrete Fourier
frequencies Bracewell (1965) , however to keep our synthetic images aesthetically pleasing, we
restrict our dataset to only contain 28 frequencies corresponding to periodic signals along either the
horizontal or vertical axis of the image but not both. Furthermore, only 5 frequencies may be present
in a single image, the combination of which is selected from a set of 200 possible configurations.
We denote the sparse representation in the DFT domain of a 28x28 dimensional image as x.
For simplicity, we assume the encoder acts on the 225-dimensional sparse representation of the in-
put. The reason for this assumption is to illustrate the relationship between adversarial perturbations
and the sparse representation of the input. Note that for a dense vector γ = Dx which admits
a sparse representation x with respect to a dictionary of atoms D, one can obtain a measurement
vector y = ΦDx where ΦD is effectively the measurement matrix.
Since our data is K-sparse with K=5, we use a randomly initialized measurement matrix Φ ∈ RM×N
as the encoder where N = 225 and M = 50. To construct Φ We sample i.i.d entries from N(0, M).
This results in a measurement matrix which satisfies the structured K-RIP with high probability
Baraniuk et al. (2008). We can therefore obtain a dense measurement vector y = Φx from which the
signal x can be recovered. We use a deconvolution network to reconstruct the 28 x 28 image from
the measurement vector y.
3.1	Attack Formulation
We describe an adversarial attack on the encoder Φ as a norm-bounded perturbation δ added to the
sparse representation of a source image xs such that the representation of xs + δ is similar to that of
a target input xt. Our objective is written as a constrained optimization problem in equation 3.2.
minkΦ(xs + δ) -Φxtk22
δ
kδ k22 ≤ 2
(3.1)
(3.2)
Since the above optimization problem is convex, we obtain a closed-form solution for δ by mini-
mizing the Lagrangian 3.3 with penalty coefficient λ. We decompose the adversarial perturbation
δ into two components δs and δt. Where δs is such that kΦ(xs + δs)k22 is minimized, while δt is
such that kΦ(δt - xt)k22 is minimized. That is, δs is crafted so as to obfuscate xs while δt is crafted
3
Under review as a conference paper at ICLR 2021
so as to pass as xt . We can therefore express δs and δt in terms of the source input xs 3.4 and the
target input xt 3.5 respectively.
Computing δs and δt :
L(δ, λ) = (xs -xt+δ)TΦTΦ(xs -xt+δ)+λ(δTδ-)	(3.3)
VδL(δ, λ) = 2ΦTΦ(xs - Xt + δ) + 2λδ = 0
δ = (ΦTΦ + λI) — 1 ΦTΦ(xt - XS)
δs = - (ΦT Φ + λI ) — 1 ΦT Φxs	(3.4)
δt = (ΦT Φ + λI ) — 1 ΦT Φxt	(3.5)
We denote the transformation (ΦTΦ + λI) 1 ΦTΦ by the matrix Mφ . The final expression for δ
which we use to attack Φ is shown in equation 3.6.
δ = MΦXt - MΦXS
(3.6)
Source Image
Adversarial Input
Source Image
Adversarial Input
Source Image
Adversarial Input
工ɑ Af
4 Adversarial Perturbations are Predictive
We see that the adversarial perturbation δ exploits the redundancy of Φ. That is, while the mutual
coherence of Φ might be sufficiently small to guarantee the recovery of K -sparse vectors, a suffi-
ciently large number of column vectors of Φ may be combined so as to produce a representation that
is highly correlated with that of Xt . Equations 3.4 and 3.5 express adversarial perturbations as dense
counterparts of sparse representations XS and Xt, using the dictionary MΦ . We therefore obtain a
precise description of how δt is predictive of Xt in the case of a linear encoder.
We verify whether a newly initialized autoencoder can be trained to reconstruct the target images
Xt from adversarial inputs Xadv . We perform a similar experiment to that performed by Ilyas et al.
(2019). Using the random measurement matrix Φ as our first encoder, we generate adversarial input-
target pairs using the closed form expression for δ shown in equation 3.6, where Xadv = XS +δt +δs
and the target to be reconstructed is Xt. We use the adversarial input-target pairs {(Xadv , Xt)} to train
a new autoencoder. This time, the new measurement matrix Ψ is updated along with the decoder. Ta-
ble 1 shows the average l2-distance (over 128000 samples) between measurement vectors of differ-
ent inputs. The distance between the measurement vectors of two random samples kΨXS - ΨXtk2 is
included for comparison. We find that Ψ learns to represent Xadv and Xt with a similar measurement
vector, despite the fact that while training, Ψ only encounters adversarial inputs xadv . Additionally,
when the input is composed of δt alone, Ψ also produces a measurement vector similar to ΨXt .
Table 1: Average l 2 distance between measurement vectors
kΨ(xs + δs )k2	kΨδtk2	kψχt - ψδt k2	kΨxadv - Ψxt k2	kψχs - ψχtk2
3.08 ±0.0^5―	5.33 ±0.10	3.10 ±0.05 一	1.27 ±0.08	7.0 ±0.2	[
4
Under review as a conference paper at ICLR 2021
The newly trained autoencoder computes similar representations for xt, δt and xadv , and success-
fully reconstructs xt from Ψxt , Ψδt and Ψxadv . We also notice that the l2-norm of the extraneous
component kΨ(xs + δs)k2 is small compared to the l2-norm of the informative component kΨδtk2.
While δ was crafted to induce such behaviour in Φ, the encoder Ψ has inherited such behaviour by
training on adversarial input-target pairs (xadv , xt).
Since the source and target images are selected at random, δt is the only component of the adversar-
ial input informative of the target xt. Yet, when the autoencoder is trained on (δt , xt) pairs it fails
to reconstruct standard inputs from our synthetic dataset. This is because while training, Ψ learns
to represent Ψxt similarly to Ψδt by learning to attenuate xs - δs in comparison to the informative
component δt. In short, the encoder trained on a dataset of (xadv , xt) pairs learns to emulate the
encoder Φ by effectively denoising xadv. More concretely Ψ is such that for all vectors xs sampled
from the structured sparsity model, the norm of the nuisance term (Ψ - ΨMΦ )xs is reduced com-
pared to kΨMΦxt k2. Training an autoencoder on adversarial input-target pairs is therefore a kind
of knowledge distillation, achieved by learning to attenuate structured noise.
We observe similar results with Variational Autoencoders (Kingma & Welling, 2014; Rezende et al.,
2014) with Gaussian priors trained on CelebA and MNIST. Our experiments are similar to those
presented by Ilyas et al. (2019) and can be summarized as follows:
•	We first train a VAE on the standard training set, which we denote by VAEstd .
•	For each source sample xs in the training set, we randomly select a target xt from the train-
ing set. We perform a targeted attack on VAEstd by minimizing ∣∣μ(xs + δ) - μ(xt)k2,
where μ(x) denotes the mean of the posterior distribution q(z|x) given by the encoder
E(.).
•	We then train a newly initialized VAE on the training set of adversarial input-target pairs
(xs + δ, xt).
We generate targeted adversarial attacks with bounded l2 norm using Projected Gradient Descent
(PGD) Madry et al. (2018) with random starts. We set = 10.0, step size = 0.05 and number
of steps = 1000 for CelebA, and = 3.0 stepsize = 0.1 and number of steps = 200 for MNIST.
To generate a training set of adversarial input-target pairs, we randomly sample a source image xs
and a target image Xt, We find a perturbation δ such that ∣∣δ∣∣2 ≤ e and ∣∣μ(xs + δ) 一 μ(xt)∣2 is
minimized, where μ(x) denotes the mean of the posterior distribution q(z|x) given by the encoder
E(.). Sample attacks are shown in figure 2.
Source Image
Adversarial Input
Source Image
Adversarial Input
Source Image
Adversarial Input
Target Image
Adversarial Reconstruction Original Reconstruction
Target Image
Adversarial Reconstruction Original Reconstruction
Target Image
Adversarial Reconstruction Original Reconstruction
Figure 2: Targeted adversarial attacks on encoder trained on MNIST (top) and CelebA (bottom).
7
7
7
乙
乙
。6乙
7 4
We then train a newly initialized VAE on the training set of adversarial input-target pairs (xs+δ, xt).
We find that the newly trained VAE learns to reconstruct samples from the standard test set, we obtain
reasonable reconstructions for both MNIST and CelebA as shown in figure 3.
5
Under review as a conference paper at ICLR 2021
心EmElfɪ El
Figure 3: Reconstruction of images from the standard test set from a model trained on (xs + δ, xt)
pairs.
We repeat the above procedure, this time with xadv constructed without a source input xs . We
construct an adversarial input xadv starting with a zero image and performing PGD to obtain an
l2-norm bounded input whose representation matches that of the target image from the training set.
We find that while a model trained on such adversarial input-target pairs can learn to reconstruct the
target from adversarial inputs, it fails to reconstruct standard inputs as shown in figure 4.
Sss
Figure 4: Sample input and reconstructions for autoencoder trained on (δt , xt) pairs. Targeted
perturbation computed using samples from the test set as targets xt (top row) and corresponding
decoder output (second row). Original target image (third row) with corresponding decoder output
(last row). An autoencoder trained on (δt, xt) pairs learns to reconstruct target images xt from
corresponding perturbations δt yet fails to decode images from the standard test set.
It is worth mentioning that this is not observed in the case of classification. That is, constructing
an adversarial training set without source inputs xs leads to generalization to the standard test set as
demonstrated by (Nayak et al., 2019), who term this phenomenon zero-shot knowledge distillation.
Additionally, it has been shown by Krishna et al. (2020) that models can be distilled or stolen using
a different training set than that used to train the teacher network. Another interesting and related
phenomenon is that of dataset distillation (Wang et al., 2018), which demonstrates that classifiers
can learn to generalize to the standard test set by training on a small dataset constructed using a
6
Under review as a conference paper at ICLR 2021
teacher network. We believe that these observations point to future research directions aimed at
understanding model extraction and generalization in neural networks.
5	Adversarial Attacks on a Two-Layer Encoder
In section 4 we illustrate how training a linear encoder on adversarial input-target pairs is a form
of knowledge distillation, achieved by learning to attenuate a structured nuisance term xs - MΦxs.
That is, (Ψ - ΨMΦ)x ≤ ξ for all sparse vectors x sampled from a distribution of structured sparse
vectors. In the case of a non-linear encoder, it is not immediately clear that the perturbation consists
of a nuisance component that is uninformative of xt . In this section we illustrate that adversarial
attacks on a two-layer ReLU-based encoder also admit a decomposition into a nuisance term and an
informative component. We include details of our analysis in A.
We derive the form of the perturbations crafted for a two-layer encoder shown in 5. The intermediate
activation vector Γ is produced by a 1D-convolution layer followed by a ReLU. We assume zero-
bias for simplicity. The matrix Φ projects Γ onto a lower dimensional representation vector Y . We
consider an adversarial perturbation δ0 applied to the input vector Xs so as to induce a representation
vector Yadv similar to Yt, the representation of a target Xt . We consider δ1 to be the difference in
intermediate activation vector Γ induced by δ0 .
Figure 5: The perturbation δ0 added to the input X induces a perturbation δ1 on the hidden repre-
sentation Γ.
We assume that a successful perturbation δ0 is already found which results in
ΦReLU (DT Xs + δ0) - Yt 2 ≤ E for small E. Our analysis begins by observing that
given the set of indices I of Γ + δ1 where Γ[i] + δ1 [i] > 0, and the l2 norm of the perturbation δ1,
we can obtain an expression for δ1 as shown in equation 5.1.
δ1 = -M(Φ,I,λ)ΦIΓs + M(Φ,I,λ)ΦΓt
(5.1)
We consider the effect of δ0 on Γs under the different modes of operation of the ReLU function. In
each case we describe a condition on δ0 in terms of the target Γs + δ1.
dTδ	δ1[i]+Γs[i]	-diTXs	-ξi	ifΓs[i]+δ1[i]	> 0 then ξi	=0
di δ0 =	δ1[i]+Γs[i]	-diTXs	-ξi	ifΓs[i]+δ1[i]	=0thenξi	> 0
The closed form solution of δ0 expressed in terms of the pre-ReLU target γ is shown in 5.3.
δ0 = (DDT + λI)-1Dγ
(5.3)
To simplify, we denote (DDT + λI)-1D as MD.
7
Under review as a conference paper at ICLR 2021
δ0
MD δ1 + Γs - DT Xs - MDξ
(5.4)
We substitute 5.1 to obtain:
Xs + δ0 = Xs - MD(DTXs) + MDΓs - MDM(Φ,I,λ)ΦIΓs + MDM(Φ,I,λ)ΦΓt - MDξ
The informative component of the target Xt is MDM(Φ,I,λ)ΦΓt and the remaining terms form the
extraneous component Xs - MD (DTXs) + MDΓs - MDM(Φ,I,λ)ΦIΓs which depends on the
source input Xs.
6	Discussion
In this work we examine how adversarial perturbations are predictive of a target sample xt . We
show that autoencoders trained on adversarial input-target pairs achieve low reconstruction error
on the standard test set, similar to a phenomenon observed with classifiers by Ilyas et al. (2019).
Using a structured sparse toy dataset and a linear encoder with random weights, we illustrate how
adversarial perturbations are closely related to the sparse representation of the input, motivating the
decomposition of a targeted perturbation δ into two components δs and δt . While the role of δs is
to attenuate the representation pertaining to the source input xs, the role of δt is to induce a similar
representation to that of the target input xt . Using the randomly initialized encoder Φ, we construct
a training set of adversarial input-target pairs (xs + δ, xt) to train a new encoder Ψ. We find that Ψ
learns to represent xt and δt = MΦxt similarly, but this is not the case when Ψ is trained on (δt, xt)
pairs. We conclude that training encoders on (xs + δ, xt) pairs is a form of knowledge distillation.
More concretely, Ψ learns to represent xt similarly to δt by learning to attenuate the nuisance term
xs - MΦxs.
We perform similar experiments with CelebA and MNIST and observe similar behaviour. A VAE
trained on (xs + δ, xt) pairs learns to reconstruct inputs from the standard test set. However, while
a VAE trained on (δt, xt) pairs can reconstruct xt from newly sampled δt, it fails to reconstruct
samples from the standard test set. We conclude that targeted attacks on a non-linear encoder can
also be decomposed into a nuisance term and an informative component. We obtain expressions for
such components for a 2-layer ReLU encoder in section 5.
Our results regarding attacks on encoders suggest a different interpretation of the observations of
Ilyas et al. (2019); that an adversarial example can be predictive of the target image itself rather
than only its class label. Rather than viewing adversarial examples as consisting of the non-robust
features within the training data, we view adversarial examples as an alternate form of the data. In
the future, we plan to extend our analysis to explain related observations regarding classifiers.
We note that our work is aimed at understanding why adversarial examples can be used as a well-
generalizing training set rather than providing a complete characterization of the causes of adversar-
ial vulnerability. Recent efforts have focused on answering different questions regarding adversarial
attacks. The work of Romano et al. (2019) shares our assumption that the data admits a sparse repre-
sentation. The authors compare the stability of two layered pursuit algorithms and find that layered
Basis Pursuit is more stable than layered soft-thresholding, which is closely related to the forward
pass of a neural network. Allen-Zhu & Li (2020) also assume that the data has an underlying sparse
representation. The authors introduce the concept of feature purification and prove that adversarial
training removes components from a classifier’s weights which are weakly correlated with multiple
class labels. We note that our use of the sparsity assumptions has allowed us to uncover how the
encoder learns to represent samples from the standard test set by learning to be invariant to a struc-
tured nuisance component. An interesting direction for future work is to further characterize how
auxiliary objectives are tied to the behaviour of a neural network.
References
Zeyuan Allen-Zhu and Y. Li. Feature purification: How adversarial training performs robust deep
learning. ArXiv, abs/2005.10190, 2020.
8
Under review as a conference paper at ICLR 2021
A. Athalye, Nicholas Carlini, and D. Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. ArXiv, abs/1802.00420, 2018.
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the
restricted isometry property for random matrices. Constructive Approximation, 28(3):253-263,
2008.
R. Bracewell. The fourier transform and its applications. 1965.
E. Candes and M. Wakin. An introduction to compressive sampling [a Sensing/sampling paradigm
that goes against the common knowledge in data acquisition]. 2008.
E. Candes, J. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measure-
ments. Communications on Pure and Applied Mathematics, 59:1207-1223, 2005.
Nicholas Carlini, A. Athalye, Nicolas Papernot, W. Brendel, Jonas Rauber, D. Tsipras, Ian J. Good-
fellow, A. Madry, and A. Kurakin. On evaluating adversarial robustness. ArXiv, abs/1902.06705,
2019.
T. Cemgil, S. Ghaisas, Krishnamurthy Dvijotham, and Pushmeet Kohli. Adversarially robust repre-
sentations with smooth encoders. In ICLR, 2020.
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. In
Advances in neural information processing systems, pp. 1178-1187, 2018.
J. Gilmer and Dan Hendrycks. A discussion of’adversarial examples are not bugs, they are features’:
Adversarial example researchers need to expand what is meant by ’robustness’. 2019.
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Watten-
berg, and Ian Goodfellow. Adversarial spheres. arXiv preprint arXiv:1801.02774, 2018.
George Gondim-Ribeiro, Pedro Tabacof, and Eduardo Valle. Adversarial attacks on variational
autoencoders. arXiv preprint arXiv:1806.04646, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Andrew Ilyas, Shibani Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry. Adversarial
examples are not bugs, they are features. In NeurIPS, 2019.
J. Jacobsen, Jens Behrmann, R. Zemel, and M. Bethge. Excessive invariance causes adversarial
vulnerability. 2019.
Diederik P. Kingma and M. Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2014.
Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. In 2018 ieee
security and privacy workshops (spw), pp. 36-42. IEEE, 2018.
Kalpesh Krishna, Gaurav Singh Tomar, Ankur P. Parikh, Nicolas Papernot, and Mohit Iyyer. Thieves
on sesame street! model extraction of bert-based apis. ArXiv, abs/1910.12366, 2020.
A. Madry, Aleksandar Makelov, L. Schmidt, D. Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2018.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and P. Frossard. Universal adver-
sarial perturbations. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 86-94, 2017.
G. Nayak, Konda Reddy Mopuri, Vaisakh Shaj, R. Venkatesh Babu, and A. Chakraborty. Zero-shot
knowledge distillation in deep networks. In ICML, 2019.
Danilo Jimenez Rezende, S. Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In ICML, 2014.
9
Under review as a conference paper at ICLR 2021
Yaniv Romano, Aviad Aberdam, Jeremias Sulam, and Michael Elad. Adversarial noise attacks of
deep learning architectures: Stability analysis via sparse-modeled signals. Journal of Mathemat-
ical Imaging and Vision, 62:313-327, 2019.
Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J. Fleet. Adversarial manipulation of deep
representations. CoRR, abs/1511.05122, 2016.
Carl-Johann Simon-Gabriel, Y. Ollivier, B. SchOlkopf, L. Bottou, and David Lopez-Paz. Adversarial
vulnerability of neural networks increases with input dimension. ArXiv, abs/1802.01421, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
D. Tsipras, Shibani Santurkar, L. Engstrom, A. Turner, and A. Madry. Robustness may be at odds
with accuracy. arXiv: Machine Learning, 2019.
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv
preprint arXiv:1811.10959, 2018.
M. Willetts, Alexander Camuto, Tom Rainforth, S. Roberts, and C. Holmes. Improving vaes’ ro-
bustness to adversarial attack. arXiv: Machine Learning, 2019.
Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier
perspective on model robustness in computer vision. In NeurIPS, 2019.
Tianyuan Zhang and Zhanxing Zhu. Interpreting adversarially trained convolutional neural net-
works. ArXiv, abs/1905.09797, 2019.
A	Adversarial Attacks on a Two-Layer Encoder
We provide a detailed description of the ideas presented in section 5. We first consider the expression
for the perturbation δ1 . The additional challenge in this case is due to the non-convexity of the set
of activations which are permissible under an l2 norm constraint on δ0 . We note that our goal is
not to find the worst-case perturbation, which would require solving an integer program, rather it is
to obtain a general form for δ1 and δ0 in terms of the source input Xs, and hidden representations
Γs and Γt . We therefore assume that a successful perturbation δ0 is already found which results in
ΦReLU (DT Xs + δ0) - Yt 2 ≤ E for small E. The perturbation δ0 induces a perturbation δ1 on
the post-ReLU activation Γs. More precisely, the indices of δ1 which are of interest are those where
Γs + δ1 > 0. We denote the ith element of δ1 as δ1 [i] and the elements of δ1 whose indices are in
I as δ1 [I]. Suppose that we are given the indices where Γs [i] + δ1 [i] > 0, denoted by the set I.
Suppose also that we have knowledge that kδ1 k2 = 1. What remains to be found are the values for
δ1 [i] for i ∈ I. We can express δ1[I] as the solution to the following optimization problem shown
in equation A.3.
minikΦι(Γs[I] + δι[I]) - ΦΓtk2	(A.1)
s.tΓs[I] +δ1[I] ≥ 0	(A.2)
2 kδι[I]k2 = e2	(A.3)
We obtain the following expression for δ1 [I].
L(δ1,λ,ν) = kΦI(Γs[I]+δ1[I]) -ΦΓtk22 -νT(Γs[I]+δ1[I])+λ(kδ1k22 -1)	(A.4)
δ1[I] = (ΦITΦI+λI)-1ΦIT(ΦIΓs[I]+ΦΓt+ν)	(A.5)
Since we only consider entries such that δ1 [i] > 0 given by the index setI the inequality constraints
Γs [I] + δ1[I] ≥ 0 are inactive and ν = 0.
10
Under review as a conference paper at ICLR 2021
We denote (ΦTI ΦI + λI)-1ΦIT by M(Φ,I,λ).
δ1 = -M(Φ,I,λ)ΦIΓs + M(Φ,I,λ)ΦΓt	(A.6)
We can therefore frame the attack objective as finding a perturbation δ0 to be added to the dense
input Xs such that ReLU (DT (Xs + δ0)) = Γs + δ1.
We consider the effect of δ0 on Γs under the different modes of operation of the ReLU function. In
each case we describe a condition on δ0 in terms of the target Γs + δ1 .
If Γs [i] +δ1[i] >0:
diT Xs + diT δ0 = Γs [i] + δ1 [i]	(A.7)
If Γs [i] +δ1[i] =0:
diT Xs + diT δ0 < 0	(A.8)
or equivalently, ∃0 < ξi s.t.	(A.9)
diTXs+diTδ0 = -ξi	(A.10)
Summarizing the above, we obtain the following piecewise target for DT δ0.
dTδ	δ1[i]+Γs[i] -diTXs -ξi	if	Γs [i] +δ1[i]	>0thenξi	=0
di δ0 =	δ1[i]+Γs[i] -diTXs -ξi	if	Γs [i] +δ1[i]	=0thenξi	> 0
We add Γs[i] +δ1 [i] to -diT Xs -ξi which effectively does not change the target when Γs[i] +δ1 [i] =
0.
We express the perturbation δ0 as the solution to the optimization problem shown in A.13, where
γ is the pre-ReLU target is shown in A.14. By construction (we assumed δ1 is induced by δ0) the
minimum value of the objective is 0, achieved when DT δ0 = γ.
minDT δ0 - γ22	(A.12)
s.t. kδk22 ≤ 2	(A.13)
δ0 = (DDT + λI)-1Dγ	(A.14)
To simplify, we denote (DDT + λI)-1D as MD. Using A.6 to obtain
Xs+δ0 =Xs+MD-M(Φ,I,λ)ΦIΓs+M(Φ,I,λ)ΦΓt+Γs -MDDTXs -MDξ	(A.15)
Xs - MDDTXs + MDΓs - MD M(Φ,I,λ) ΦI Γs + MD M(Φ,I,λ) ΦΓt - MDξ (A.16)
11