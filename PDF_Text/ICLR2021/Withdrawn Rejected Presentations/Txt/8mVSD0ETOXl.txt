Under review as a conference paper at ICLR 2021
Prediction of Enzyme Specificity
using Protein Graph Convolutional Neural
Networks
Anonymous authors
Paper under double-blind review
Ab stract
Specific molecular recognition by proteins, for example, protease enzymes, is crit-
ical for maintaining the robustness of key life processes. The substrate specificity
landscape of a protease enzyme comprises the set of all sequence motifs that are
recognized/cut, or just as importantly, not recognized/cut by the enzyme. Current
methods for predicting protease specificity landscapes rely on learning sequence
patterns in experimentally derived data with a single enzyme, but are not robust
to even small mutational changes. A comprehensive evaluation of specificity re-
quires consideration of the three-dimensional structure and energetics of molecu-
lar interactions. In this work, we present a protein graph convolutional neural net-
work (PGCN), which uses a physically intuitive, structure-based molecular inter-
action graph generated using the Rosetta energy function that describes the topol-
ogy and energetic features, to determine substrate specificity. We use the PGCN to
recapitulate and predict the specificity of the NS3/4 protease from the Hepatitic C
virus. We compare our PGCN with previously used machine learning models and
show that its performance in classification tasks is equivalent or better. Because
PGCN is based on physical interactions, it is inherently more interpretable; de-
termination of feature importance reveals key sub-graph patterns responsible for
molecular recognition that are biochemically reasonable. The PGCN model also
readily lends itself to the design of novel enzymes with tailored specificity against
disease targets.
1	Introduction
Selective molecular recognition between biomolecules e.g. protein-protein, DNA-protein (Tainer &
Cunningham, 1993), and protein-small molecule interactions, is key for maintaining the fidelity of
life processes. Multispecificity, i.e. the specific recognition and non-recogntion of multiple targets
by biomolecules, is critical for many biological processes, for example the selective recognition
and cleavage of host and viral target sites by viral protease enzymes is critical for the life-cycle of
many RNA viruses including SARS-CoV-2 (Vizovisek et al., 2018). Prospective prediction of the
sequence motifs corresponding to protease enzyme target sites (substrates) is therefore an important
goal with broad implications. Elucidating the target specificity of viral protease enzyme can be used
for the design of inhibitor anti-viral drug candidates. The ability to accurately and efficiently model
the landscape of protease specificity i.e. the set of all substrate sequence motifs that are recognized
(and not recognized) by a given enzyme and its variants would also enable the design of proteases
with tailores specificities to degrade chosen disease-related targets.
Most current approaches for protease specificity prediction involve detecting and/or learning patterns
in known substrate sequences using techniques ranging from logistic regression to deep learning.
However, these black-box approaches do not provide any physical/chemical insight into the under-
lying basis for a particular specificity profile, nor are they robust to changes in the protease enzyme
that often arise in the course of evolution. A comprehensive model of protease specificity requires
the consideration of the three-dimensional structure of the enzyme and the energetics of interaction
between enzyme and various substrates such that substrates that are productively recognized (i.e.
cleaved) by the protease are lower in energy than those that are not.
1
Under review as a conference paper at ICLR 2021
To encode the topology and energetic features, here we develop Protein Convolutional Neural Net-
works (PGCN). PGCN uses experimentally derived data and a physically-intuitive structure-based
molecular interaction energy graph to solve the classification problem for substrate specificity. Pro-
tease and substrate residues are considered as nodes, and energies of interactions are obtained from
a pairwise decomposition of the energy of the complex calculated using the Rosetta energy func-
tion. These energies are assigned as (multiple) node and edge features. We find that PGCN is as
good as or better than other previously used machine learning models for protease specificity pre-
diction. However, it is more interpretable and highlights critical sub-graph patterns responsible for
observed specificity patterns. As it is based on physical interactions, the PGCN model is capable of
both prospective prediction of specificity of chosen protease enzymes and generating novel designed
enzymes with tailored specificity again chosen targets.
2	Related Work
In this work, we develop a graph-based deep learning technique for protease enzyme specificity
prediction. Here we provide a brief review of previously developed predictive methods for protease
specificity landscape prediction and applications of graph-based convolutional neural networks on
protein-related problems.
2.1	Prediction of protease specificity landscape
Current methods to discriminate the specificity landscape of one or more types of protease enzymes,
could be classified into two categories, machine learning approaches and scoring-matrix-based ap-
proaches (Li et al., 2019). Methods use machine learning methods such as logistic regression, ran-
dom forest, decision tree, support vector machine (SVM) to predict substrate specificity. The most
popular tool is SVM among them, e.g. PROSPER (Song et al., 2012), iProt-Sub (Song et al., 2018),
CASVM (Wee et al., 2007), Cascleave (Song et al., 2010). Besides, NeuroPred (Southey et al.,
2006) and PROSPERous (Song et al., 2017) applied logistic regression to predict specific neuropep-
tide specificity (Neuropred) and 90 different proteases (PROSPERous). Pripper (Piippo et al., 2010)
provided three different classifiers based on SVM, decision tree and random forest. Procleave (Li
et al., 2020b) implemented a probabilistic model trained with both sequence and structure feature
information. DeepCleave (Li et al., 2020a) is a tool to predict substrate specificity by using con-
volutional neural network (CNN). None of methods mentioned above use energy-related features,
however we note that some energy terms for the interface were considered in (Pethe et al., 2017) and
(Pethe et al., 2018).
2.2	Graph convolutional neural network on protein-related problems
There are several works proposing or implementing a graph-based convolutional neural network
model to solve various protein modeling-related problems. BIPSPI (Sanchez-Garcia et al., 2019)
made use of both hand-crafted sequence and structure features to predict residue-residue contacts in
proteins. Gligorijevic et al. (2020) proposed a novel model that generated node features by using
LSTM to learn genetic information and edge adjacency matrix from contact maps to classify dif-
ferent protein functions. Graph convolutional neural networks are also applied to drug discovery,
for either node classification or energy score prediction (Sun et al., 2019). Fout et al. (2017) pro-
posed a graph-based model to encode a protein and a drug into each graph, which considered local
neighborhood information from each node and learned multiple edge features for edges between
neighbor residues and nodes. Zamora-Resendiz & Crivelli (2019) addressed their model to learn
sequence- and structure- based information more efficiently than 2D/3D-CNN for protein structure
classification. Moreover, Cao & Shen (2019) and Sanyal et al. (2020) aimed at improving the energy
functions used for protein model evaluation by using molecular graphs. Unlike previous work, our
approach uses per-residue and residue-residue pairwise energies as features for predicting molecular
function.
2
Under review as a conference paper at ICLR 2021
(a) Molecular recognition site
(b) Graph generation
Figure 1: Schematic diagram of a protease with a substrate sample, graph generation and protein
graph convolutional network (PGCN).(a) Protease - substrate diagram with a sequence logo plot for
the specificity landscape of P2-P6 sites. Here, the substrate labeled in blue is 11-amino-acid long,
from P7 at N-terminus to P4’ at C-terminus and it is cleaved between P1 and P1’. The site labeled
in yellow represents all neighbor residues which are 6 angstrom around the substrate. (b) Graph
generation from the substrate residues (blue dots) and its neighbor residues (yellow dots). Grey
lines denote edges. (c) PGCN training scheme. In the node matrix, residues are listed in the order
of residues starting from substrate residues followed by neighboring residues. In the node matrix,
residues are listed in the order of residues from N-terminus to C-terminus, with substrate residues
followed by protease residues.
(c) PGCN model
3	Methods
Overview We provide protein graph convolutional network (PGCN), which models protein-
substrate complexes as fully connected graph structures conditioned on terms of both sequences
and interaction energies. We generate energy-related features using Rosetta (Leaver-Fay, 2011), a
software suite for protein modeling and design, for either single residues or pairwise residues. Sub-
strate is a 7-residue stretch (7-letter string) of amino acids sequences being recognized by a protease
active site (Figure 1a). The goal is to predict all motifs that are (are not) efficiently and correctly
bound by the enzyme resulting in cleavage (non-cleavage).
3.1	Protein Graph Representation
The protein complex is encoded as a fully connected graph G = (V, E), from which substrate
and neighboring amino acids make up the nodes, and residue pairwise interactions make up edges
(Figure 1b). Each node vi ∈ V contains the one-body features of a single residue, while each edge
eij ∈ E contains relational features between a pair of residues. Since residues that are far away
from the substrate are less likely to influence specificity and more likely to introduce noise into
the model, we only consider a sub-graph G0 , including the substrate and neighboring residues to a
specified distance around the substrate. The graph G0 consists of two sets: nodes and edges. The
features of nodes are encoded as a N × F matrix, where N is the number of nodes, F is the number
of node features. The features of edges are encoded as an N × N × M adjacency tensor. (Summary
of node and edge features are shown in Table A.4.)
3
Under review as a conference paper at ICLR 2021
3.2	Normalization
Since the stability of the structure depends on the minimum potential energy, we normalize each
energy-related feature with the exponential of negative potential to amplify lower energy values and
reduce higher energy values, which could be denoted as Qi = exp(-Pi), where Pi is ith node/edge
potential energy.
3.3	Multiple Edge Features
Traditionally, the adjacency matrix is used to represent whether a pairs of residues is adjacent or not,
which is denoted as A ∈ RN×N (WUet al., 2θ2θ) with Aij = 1 if ej ∈ E and Aij = 0 if ej ∈ E.
This representation is not able to handle multiple edge features, however, and in this work we are
incorporating distances and mUltiple energetic components into the edge featUres, which Rosetta
combines as a weighted sUm (Alford et al., 2017). ThUs it was necessary for Us to flatten the edge
featUre tensor into a weighted adjacency matrix by sUmming pseUdo total energy for each edge as
shown below.
EiG =	wjEij	(1)
j
where EiG denotes total energy for an edge, wj denotes learned weight parameter for the jth energy
terms and Eij denotes one of the edge energies for the edge. The matrix form of it is shown in Eq.2
later.
3.4	Protein Graph Convolutional Network
PGCN learns featUres of nodes and edges and receives both tUned and learned parameter sets from
training data, which is composed of graphs generated from Rosetta-modeled protease-sUbstrate com-
plexes with known class labels (cleaved/Uncleaved). PGCN feeds a batch of graphs as inpUt at every
step, calcUlates the loss between estimated and trUe labels, and Updates learning parameters by gra-
dient descent dUring back propagation.
Here we describe the details of the PGCN architectUre. First, PGCN mUltiplies the adjacency tensor
of each graph from training data with a M -long learning weight vector WA to flatten mUltiple
edge featUres Using the weighted sUm method. Second, the flattened adjacency matrix N × N ,
together with the node featUre matrix, are fed into a graph convolUtional (GC) layer shown as Eq.3.
Third, the GC layer is followed by a BatchNorm layer (Ioffe & Szegedy, 2015), which aims at
avoiding slow convergence. Next, the oUtpUt from BatchNorm goes throUgh another GC layer and a
BatchNorm layer to continUe refining learning parameters. Then, PGCN drops oUt a proportion of
hidden nodes over nodes to avoid the overfitting problem (Srivastava et al., 2014). Unlike normal
mUlti-layer neUral networks, PGCN does not coUnt a proportion of hidden nodes oUt over all nodes
Using the standard dropoUt strategy. Instead, PGCN mUtes different combinations of hidden nodes
for different nodes. Finally, PGCN transforms the oUtpUt into a Y -dimensional vector, which shows
the probabilities that the graph is classified into each class over Y classes. Here, the set of tUning
hyperparameters is made Up of batch size, learning rate, dropoUt rate and weight decay coefficient.
The weight decay coefficient is a part of the L2 regUlarization term, that mUltiplies sUm of learned
weights for the anti-overfitting problem. Learning parameters keep Updated throUgh epochs. The
trained PGCN model is Used for testing, in which test data pass throUgh each layer of the PGCN
model bUt skip the dropoUt process.
The mathematical expression of PGCN model for one graph is shown below,
A0 = AWA	(2)
H1 = D0-2 A0D0-1XWX	(3)
11
H = D-1A0D0- 2 HiWi	(4)
where WA ∈ RM×1, WX ∈ RF×C1, W1 ∈ RC1×C2 are learning weight matrix, C1, C2 are nUmbers
of hidden nodes for two convolUtion layers, A0 = A0 + IN and D0 is a diagonal matrix with Di0i =
j A0ij. Eq.2 shows the formUlation of adjacency matrix A0 from weighted sUm of adjacency tensor
A over features. In Eq.3 and Eq.4, D0- 1 A0D0-2 denotes normalization of the adjacency matrix,
mentioned in Normalization section.
4
Under review as a conference paper at ICLR 2021
4	Experiments
We use PGCN to discriminate protease specificity reflected on substrate cleavage, based on two sets
of features: a hybrid set which contains both sequence (amino acid types) and energy information,
and a set which contains only energy information with categorical features. Cleaving, not cleaving
are two main possibilities of protease-substrate interaction. We also consider a ternary classification,
wherein a protease-substrate pair exhibited very low cleavage activity in experiments, and can be
said to partially cleave a substrate.
We then compare our results with SVM models from (Pethe et al., 2017) and (Pethe et al., 2018), and
also compare with currently used machine learning models as mentioned in Related Work section.
Furthermore, we make analysis on importance of nodes/edges to address PGCN’s contribution to
potential valuable enzyme design problems.
4.1	Data
We have lists of 5-amino-acid-
long substrate sequences (from P2-P6) that were determined in previous experiments (Pethe et al., 2017) to be cleaved, not	Table 1: Number of samples for different data				
	# Samples	Two Classes		Three Classes	
cleaved, or partially cleaved by		Train	Test	Train	Test
the wild type Hepatitis C (HCV)	WT	5139	2203	6106	2621
protease, or one of three mutants.	A171T	9246	3962	15169	6501
HCV mutants had few substitu-	D183A	8304	3560	12350	5294
tions, and were homology mod- eled to have similar backbone ge-	R170K/A171T/D183A	4786	2052	12880	5520
					
ometry. Despite the fact that the					
mutants have only 1-3 substitu-					
tions, they have significantly altered specificity landscapes. The database consists of 66,441 protein-
substrate complexes and we trained 46,505 samples into four separate models for three classes, and
trained on 27,475 samples of all training samples into another four PGCN models for two classes,
see Table 1 for details. The proportion of training and testing are the same among all models, which
is 70%: 30%.
4.2	Protein Complex Modeling
We modeled all P6-P2 substrate sequences in the context of an 11 amino acid (P7-P4’) peptide with
all non-variable residues consistent with the canonical HCV substrate sequence, bound to the HCV
protease in active (cleaving) conformation, based on the crystal structure (PDB:3M5L) (Romano
et al., 2010) from RCSB Protein Data Bank (PDB) (http://www.rcsb.org/) (Berman et al., 2000).
Producing the models was done with Pyrosetta (Chaudhury et al., 2010), a Python-based interface to
Rosetta, and involved changing the side chains of the substrate to match the experimental sequence,
then minimizing the complex using FastRelax (Tyka et al., 2011).
4.3	Protein Graph Generation
We considered residues of the core substrate (P2-P6), and neighbor residues within a 10A shell of
the core substrate as nodes of our input graph (34 nodes in total). All graphs are in the format of
PyTorch 1.4.0 FloatTensor (Paszke et al., 2019).
PGCN used a more informative and concise feature set than other machine learning methods. PGCN
with hybrid feature encoding mode includes all features described in Table A.4, and PGCN with
energy-only features is the same set, excluding the one hot encoders of amino acids. All the other
methods have hybrid feature encoding mode to include one hot encoders for P6-P2 amino acid types
of substrates and energy terms for them are four coarse-grained potential energy terms instead (Pethe
et al., 2017).
5
Under review as a conference paper at ICLR 2021
4.4	Training
As we mentioned above, PGCN with 2 GC layers is used to predict substrate specificity landscapes.
Each layer has 20 hidden nodes, and non-linear ReLU term (Glorot et al., 2011) follows each graph
convolution layer after implementing BatchNorm (Ioffe & Szegedy, 2015). For the training process
of PGCN, we use a cross entropy loss function, a Adam optimizer (Kingma & Ba, 2015) and nonzero
weight decay and dropout rate. We compared feature encoding of PGCN with that of other five
machine learning methods. We used the Scikit-learn 0.20.1 (Pedregosa et al., 2011) to implement
logistic regression (lr), random forest (rf), decision tree (dt) and support vector machine (SVM)
classification, and Tensorflow 1.13.1 (Abadi et al., 2016) for artificial neural network (ANN). The
ANN model in this experiment is one fully connected layer with 1024 hidden nodes and allows a
dropout rate between 0.1-0.9.
4.5	Node/Edge Importance
The determinants of protease specificity have not been isolated from the set of many contributing
forces; for example, the most stable complexes do not necessarily correlate with substrate recogni-
tion. In order to derive biological insights about important residues or relationships between pairs of
residues that contribute to discrimination, we perturbed the values of each node/edge term over all
test samples and inspected how much the test accuracy drops, enabling us to efficiently determine the
relative importance of each node and edge. To be specific, we perturbed values of each node feature
Fi,k simultaneously across all samples for the importance detection of each node i and perturbed val-
ues of each edge Mi,j,k simultaneously across all samples for the importance detection of each edge
(i, j). The upper and lower triangular of the adjacency matrix should change simultaneously with
the same randomness scheme, since it is a symmetric matrix. Therefore, the number of total pertur-
bations should be N + N(NT) = N(N+I) if considering all N nodes and edges. The formula for
measuring accuracy loss is given by Relative Acc = (Original Acc - Perturbed Acc)/Original Acc.
Table 2: Accuracy table for models based on feature settings for binary classification
Methods Wild Type	A171T	D183A	Triple
	Hybrid	E-only	Hybrid	E-only	Hybrid	E-only	Hybrid	E-only
Logistic Reg	92.19	76.08	95.96	79.38	89.21	69.27	92.2	70.71
Random Forest	91.68	73.62	95.85	73.25	87.7	65.63	91.05	65.63
Decision Tree	86.79	74.63	91.54	76.58	83.88	68.29	87.77	69.54
SVM	92.87	75.85	95.84	78.85	89.44	69.66	92.79	70.57
ANN	93.19	76.49	96.44	79.86	89.55	69.75	93.08	70.61
PGCN	92.73	90.46	96.19	95.41	88.88	88.17	92.30	90.64
Table 3: Accuracy table for models based on feature settings for ternary classification
Methods	WT	A171T	D183A	Triple
	Hybrid	E-only	Hybrid	E-only	Hybrid	E-only	Hybrid	E-only
Logistic Reg	79.97	63.3	80.08	52.45	68.45	47.26	71.85	65.65
Random Forest	80.57	56.34	81.53	52.49	69.22	48.22	72.14	61.31
Decision Tree	70.97	62.3	73.4	52.47	62.67	48	62.03	63.97
SVM	82.18	63.68	83.69	53.76	71.23	46.88	69.2	59.67
ANN	84.32	64.75	84.63	53.76	71.99	47.56	73.73	65.80
PGCN	82.93	78.50	83.59	79.71	71.08	69.04	73.71	72.34
6
Under review as a conference paper at ICLR 2021
5	Results
5.1	Feature Set Generation
Figure 2: Accuracy of different models based on different
features for binary classification. The length of an orange
bar denote the closeness of accuracy.
We compare two different feature en-
coding modes of PGCN (energy-only
and energy+sequence ”hybrid” fea-
tures) with that of current five ma-
chine learning methods for both two
classes and three classes. Accuracy
based on the hybrid feature set is not
significantly different across different
models for each data, except that de-
cision tree reaches at least 9% lower
accuracy. ANN always reaches the
highest accuracy among all data, for
example, up to 96.4% for two classes
of HCV with A171T mutation are
predicted correctly and up to 84.6%
for three classes of it.
When it comes to energy-only encod-
ing, PGCN always performs the best,
upto 95.4% for two classes of A171T
HCV, about 15% higher than the best
accuracy among other machine learn-
ing methods, see Table 21. 3. From
the Figure 2 and A.5 (in Appendix),
it is easy to see that accuracy values
of energy-only feature encoding drop heavily down to a similar level for all five machine learning
methods, which shows that coarse-grained energies used in other machine learnign models are not
informative enough for label classification. PGCN based on energy-only feature encoding almost
recovers the accuracy based on hybrid feature encoding.
(a) Importance diagram.
(b) Wild type, energy, two classes
(b) Wild type, energy, two classes
⑴ D183a, energy, three classes (C) Wild type, energy, three classes
Figure 3: (a) Parts of the importance diagram for wild type HCV, binary classification and D183A
HCV, ternary classification. Sizes of nodes, widths of edges are proportional to importance scores.
See The full importance diagram (b) Real protein structure examples with highlighted important
nodes. Important nodes are labeled in the form of ”amino acid type, residue index”.
1”Triple” equivalent with ”R170K/A171T/D183A”, ”Logistic Reg” equivalent with ”Logistic regression”
7
Under review as a conference paper at ICLR 2021
5.2	Node/edge importance for proteases
When comparing network diagrams for hybrid (sequence+energy) feature encoding with those for
energy-only feature encoding, the energy-only PGCN found the largest contributions were from
substrate nodes, except that two classes for wild type HCV in and three classes for D183A HCV have
two significant edges, P2-72 and P4-173. Among all important edges, edges that connect residues
of the substrate with residues of the protein stand out to be the main contribution of importance.
First, models detect P2-72 as an important edge, always within top 2 of all edges. Residue 72 is the
catalytic residue, (see Figure 3b). Next, a set of important connections is between the substrate and
the residues of a beta-sheet of the enzyme, such as P4-173, P5-174, P2-171. We suspect that the
importance of these interactions are due to the fact that this beta sheet serves as a template for the
substrate and aids in positioning the scissile bond in the active site. In most cases, edges between
substrate notes were of lower influence. Moreover, several edges between residues both from the
protease show importance as well, such as 170-183 of the wild type binary model, 138-183 of the
wild type ternary model, and 171-183 of the A171T ternary model. These residues may not interact
directly with the substrate, but as they form the secondary shell around the binding pocket, they
likely impact the stability of substrate binding.
6	Discussion
In general, PGCN performs impressively well in recapitulating specificity profiles, especially in
models using only energy-based features. Following are some avenues for further improvement of
our model.
More divergent input samples To generalize PGCN beyond the Hepatitis C NS3/4 protease, more
extensive and specific set for other proteases, such as TEV protease (Li et al., 2017; Packer et al.,
2017) could be useful. These will allow sampling different enzyme-substrate sequence space as well
as chemical enviornments, likely leading to a more robust model.
Imbalanced data The number of samples for each class are imbalanced and proportions of number
of samples in classes vary especially for ternary classification. For example, the proportion of num-
ber of samples in cleaved, partially cleaved and uncleaved classes for wild type HCV is 1:1:3, while
the proportion for R170K/A171T/D183A is 1:3:1. This may arise a problem because many machine
learning models assume balanced data as the input. In this case, the model may underestimate minor-
ity class(es) (Mirza et al., 2019). We tried oversampling strategy (Ke et al., 2018) various machine
learning models, and found that it somewhat improves accuracies of certain classes. We would like
to further explore different class imbalance learning (CIL) strategies to see if they improve PGCN
performance.
7	Conclusion
In this work, we implemented a protein graph convolution network (PGCN) to classify protease-
substrate pairs as either yielding substrate cleavage, partial cleavage, or non-cleavage. Using
Rosetta, we generated a structural model for each protease-substrate complex, which we con-
verted into a fully connected graph that encoded potential energies for each single residue and
each pair of residues. Using the subgraph that includes the bound peptide and neighboring pro-
tease residues, we trained the PGCN to predict the behavior of the interaction. We found that the
PGCN reaches equivalent accuracy of other machine learning methods using the combination of
sequence and energetic features. Furthermore, we demonstrated that variable importance analysis
on the PGCN could be used to identify the nodes and edges most influential in determining pro-
tease specificity. This method has the potential to enable better prediction and eventually design
of engineered proteases with targeted substrate specificity. Codes for this work are available at
https://github.com/Nucleus2014/protease-gcnn-pytorch.
Acknowledgments
Thanks to Wenfa Lu, Xiao Li for helpful discussions and supports.
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg,
Rajat Monga, Sherry Moore, Derek Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete
Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale
machine learning this paper is included in the proceedings of the 12th usenix symposium on
operating systems design and implementation (osdi ’16). tensorflow: A system for large-scale
machine learning, 2016.
Rebecca F. Alford, Andrew Leaver-Fay, Jeliazko R. Jeliazkov, Matthew J. O’Meara, Frank P. Di-
Maio, Hahnbeom Park, Maxim V. Shapovalov, P. Douglas Renfrew, Vikram K. Mulligan, Kalli
Kappel, Jason W. Labonte, Michael S. Pacella, Richard Bonneau, Philip Bradley, Roland L. Dun-
brack, Rhiju Das, David Baker, Brian Kuhlman, Tanja Kortemme, and Jeffrey J. Gray. The rosetta
all-atom energy function for macromolecular modeling and design. Journal of Chemical Theory
and Computation,13:3031-3048, 05 2017.
Helen M. Berman, Jhon Westbrook, Zukang Feng, Gary Gilliland, T N Bhat, Helge Weissig, Ilya N
Shindyalov, and Philip E Bourne. The protein data bank. Nucleic Acids Research, 28:235-242,
01 2000. doi: 10.1093/nar/28.1.235.
Yue Cao and Yang Shen. Energy-based graph convolutional networks for scoring protein docking
models, 12 2019.
Sidhartha Chaudhury, Sergey Lyskov, and Jeffrey J. Gray. Pyrosetta: A script-based interface for im-
plementing molecular modeling algorithms using rosetta. Bioinformatics, 26:689-691, 03 2010.
Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks, 2017.
Vladimir Gligorijevic, P. Douglas Renfrew, Tomasz Kosciolek, Julia Koehler Leman, Daniel Beren-
berg, Tommi Vatanen, Chris Chandler, Bryn C. Taylor, Ian M. Fisk, Hera Vlamakis, Ramnik J.
Xavier, Rob Knight, Kyunghyun Cho, and Richard Bonneau. Structure-based protein function
prediction using graph convolutional networks, 2020.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks, 2011.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift, 2015.
Xiongwen Ke, Josef Dick, Dr Quoc, and Thong Le Gia. Learning with imbalanced data. PhD thesis,
2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2015.
Tyka Michael Lewis Steven M. Lange Oliver F. Thompson James Jacak Ron Kaufman Kristian
W. Renfrew P. Douglas Smith Colin A. Sheffler Will Davis Ian W. Cooper Seth Treuille Adrien
Mandell Daniel J. Richter Florian Ban Yih-En Andrew Fleishman Sarel J. Corn Jacob E. Kim
David E. Lyskov Sergey Berrondo Monica Mentzer Stuart PoPovic Zoran Havranek James J.
Karanicolas John Das Rhiju Meiler Jens Kortemme Tanja Gray Jeffrey J. Kuhlman Brian Baker
David Bradley PhiliP Leaver-Fay, Andrew. Rosetta3: An object-oriented software suite for the
simulation and design of macromolecules. Methods in Enzymology, 487:545-574, 2011.
Fuyi Li, Yanan Wang, Chen Li, Tatiana T. Marquez-Lago, Andre Leier, Neil D. Rawlings, Gho-
lamreza Haffari, Jerico Revote, Tatsuya Akutsu, Kuo-Chen Chou, Anthony W. Purcell, Robert N.
Pike, Geoffrey I. Webb, A. Ian Smith, Trevor Lithgow, Roger J. Daly, James C. Whisstock, and
Jiangning Song. Twenty years of bioinformatics research for Protease-sPecific substrate and cleav-
age site Prediction: A comPrehensive revisit and benchmarking of existing methods. Briefings in
Bioinformatics, 20:2150-2166, 11 2019.
Fuyi Li, Jinxiang Chen, Andre Leier, Tatiana Marquez-Lago, Quanzhong Liu, Yanze Wang, Jerico
Revote, A. Ian Smith, Tatsuya Akutsu, Geoffrey I. Webb, Lukasz Kurgan, and Jiangning Song.
DeePcleave: A deeP learning Predictor for casPase and matrix metalloProtease substrates and
cleavage sites. Bioinformatics, 36:1057-1065, 02 2020a.
9
Under review as a conference paper at ICLR 2021
Fuyi Li, Andre Leier, Quanzhong Liu, Yanan Wang, Dongxu Xiang, Tatsuya Akutsu, Geoffrey I
Webb, A Ian Smith, Tatiana Marquez-Lago, Jian Li, and Jiangning Song. Procleave: Predict-
ing protease-specific substrate cleavage sites by combining sequence and structural information.
Genomics, Proteomics Bioinformatics,18:52-64, 2020b.
Qing Li, Li Yi, Kam Hon Hoi, Peter Marek, George Georgiou, and Brent L. Iverson. Profiling
protease specificity: Combining yeast er sequestration screening (yess) with next generation se-
quencing. ACS Chemical Biology, 12:510-518, 01 2017.
Bilal Mirza, Wei Wang, Jie Wang, Howard Choi, Neo Christopher Chung, and Peipei Ping. Machine
learning and integrative analysis of biomedical big data. Genes, 10, 01 2019.
Michael S. Packer, Holly A. Rees, and David R. Liu. Phage-assisted continuous evolution of pro-
teases with altered substrate specificity. Nature Communications, 8, 10 2017.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library, 2019.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,
Alexandre Passos, David Cournapeau, Matthieu Brucher, and Matthieu Perrot Andedouard DUch-
esnay. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12:
2825-2830, 2011.
Manasi A. Pethe, Aliza B. Rubenstein, and Sagar D. Khare. Large-scale structure-based prediction
and identification of novel protease substrates using computational protein design. Journal of
Molecular Biology, 429:220-236, 01 2017.
Manasi A. Pethe, Aliza B. Rubenstein, and Sagar D. Khare. Data-driven supervised learning ofa vi-
ral protease specificity landscape from deep sequencing and molecular simulations. Proceedings
of the National Academy of Sciences, 116:168-176, 12 2018.
Mirva Piippo, Niina Lietzen, Olli S Nevalainen, Jussi Salmi, and Tuula A Nyman. Pripper: Predic-
tion of caspase cleavage sites from whole proteomes. BMC Bioinformatics, 11, 06 2010.
Keith P. Romano, Akbar Ali, William E. Royer, and Celia A. Schiffer. Drug resistance against hcv
ns3/4a inhibitors is defined by the balance of substrate recognition versus inhibitor binding. Pro-
ceedings of the National Academy of Sciences of the United States of America, 107:20986-20991,
12 2010.
Ruben Sanchez-Garcia, C. O. S. Sorzano, J. M. Carazo, and Joan Segura. Bipspi: A method for the
prediction of partner-specific protein-protein interfaces. Bioinformatics, 35:470-477, 02 2019.
Soumya Sanyal, Ivan Anishchenko, Anirudh Dagar, David Baker, and Partha Talukdar. Proteingcn:
Protein model quality assessment using graph convolutional networks, 2020.
Jiangning Song, Hao Tan, Hongbin Shen, Khalid Mahmood, Sarah E. Boyd, Geoffrey I. Webb, Tat-
suya Akutsu, and James C. Whisstock. Cascleave: Towards more accurate prediction of caspase
substrate cleavage sites. Bioinformatics (Oxford, England), 26:752-760, 03 2010.
Jiangning Song, Hao Tan, Andrew J. Perry, Tatsuya Akutsu, Geoffrey I. Webb, James C. Whisstock,
and Robert N. Pike. Prosper: An integrated feature-based tool for predicting protease substrate
cleavage sites. PLoS ONE, 7:e50300, 11 2012.
Jiangning Song, Fuyi Li, Andre Leier, Tatiana T Marquez-Lago, Tatsuya Akutsu, Gholamreza Haf-
fari, Kuo-Chen Chou, Geoffrey I Webb, and Robert N Pike. Prosperous: High-throughput pre-
diction of substrate cleavage sites for 90 proteases with improved accuracy. Bioinformatics, 34:
684-687, 10 2017.
10
Under review as a conference paper at ICLR 2021
Jiangning Song, Yanan Wang, Fuyi Li, Tatsuya Akutsu, Neil D Rawlings, Geoffrey I Webb, and
Kuo-Chen Chou. Iprot-sub: A comprehensive package for accurately mapping and predicting
protease-specific substrates and cleavage sites. Briefings in Bioinformatics, 20:638-658, 04 2018.
Bruce R. Southey, Andinet Amare, Tyler A. Zimmerman, Sandra L. Rodriguez-Zas, and Jonathan V.
Sweedler. Neuropred: A tool to predict cleavage sites in neuropeptide precursors and provide the
masses of the resulting peptides. Nucleic Acids Research, 34, 2006.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15:1929-1958, 2014.
Mengying Sun, Sendong Zhao, Coryandar Gilvary, Olivier Elemento, Jiayu Zhou, and Fei Wang.
Graph convolutional networks for computational drug development and discovery. Briefings in
Bioinformatics, 21, 06 2019.
John A. Tainer and Richard P. Cunningham. Molecular recognition in dna-binding proteins and
enzymes. Current Opinion in Biotechnology, 4:474-483, 08 1993.
Michael D.Tyka, Daniel A. Keedy, Ingemar Andre, Frank DiMaio, Yifan Song, David C. Richard-
son, Jane S. Richardson, and David Baker. Alternate states of proteins revealed by detailed energy
landscape mapping. Journal of Molecular Biology, 405:607-618, 01 2011.
Matej Vizovisek, Robert Vidmar, Marcin Drag, Marko Fonovic, GUy S. Salvesen, and Boris Turk.
Protease specificity: Towards in vivo imaging applications and biomarker discovery. Trends in
Biochemical Sciences, 43:829-844, 10 2018.
Lawrence J. K. Wee, Tin Wee Tan, and Shoba Ranganathan. Casvm: Web server for svm-based
prediction of caspase substrates cleavage sites. Bioinformatics, 23:3241-3243, 12 2007.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, pp. 1-21, 2020.
Rafael Zamora-Resendiz and Silvia Crivelli. Structural learning of proteins using graph convolu-
tional neural networks, 04 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
Table A.4: Features for nodes and edges
TYPE	FEATURES	DESCRIPTION	
	aa	One hot encoders for amino acid type
Node	fa_atr fa_rep	Lennard-Jones attractive potential Lennard-Jones repulsive potential
	fa_sol lk-balLwtd	Lazaridis-Karplus solvation potential Asymmetric solvation potential
	fa_elec.	Coulombic electrostatic potential
	hbond	Hydrogen bonding potential
	is_substrate	1 if the node belongs to the substrate; otherwise, 0
	fa_intra_sol_xover4	Intra-residue LK solvation energy
	fa_intra_rep	Lennard-Jones repulsive energy between pairwise residues
Edge	rama_prepro omega	Ramachandran preferences of backbone angles Omega dihedral of the backbone
	p_aa_pp fa_dun ref	Probability of amino acid type at backbone angles Side-chain conformation potential Reference potential of pairwise residues
	covalent_bond intramolecular	1 if pairwise residues form a covalent bond; otherwise, 0 1 if one residue from the substrate, the other from the protein; otherwise, 0
12
Under review as a conference paper at ICLR 2021
IOO
100
90
80
70
60
50
40
30
80
70
60
50
40
30
Hybrid
Pure
90
Hybrid
Pure
川IlI UHlI
ɪg
rf
dt
SVM
ANN
PGCN
ɪg
rf
dt
SVM
ANN
PGCN
(a) Wild Type
(b) A171T
Hybrid
Pure
80
Hybrid
Pure
IUH lm∣l
ɪg
rf
dt
SVM
ANN
PGCN
ɪg
rf
dt
SVM
ANN
PGCN
(c)D183A
(d) R170K∕A171T∕D183A
Figure A.4: Accuracy of logistic regression (lg), random forest (rf), decision tree (dt), support vector
machine (svm), artificial neural network (ann) and PGCN based on either ”hybrid” or energy-only
feature encoding for ternary classification. See the caption of Figure 2 for annotations.
13
Under review as a conference paper at ICLR 2021
(a) Wild type, hybrid, two classes
(b) Wild type, energy, two classes
(c) WT, hybrid, three classes
(d) WT, energy, three classes
(e) A171T, hybrid, two classes
(f) A171T, energy, two classes
(g) A171T, hybrid, three classes
(h) A171T, energy, three classes
(i) D183A, hybrid, two classes
(j) D183A, energy, two classes
(k) D183A, hybrid, three classes
(1) D183A, energy, three classes
(m) Triple, hybrid, two classes
(n) Triple, energy, two classes
(o) Triple, hybrid, three classes
(p) Triple, energy, three classes
Figure A.5: The whole importance diagram for PGCN on WT, A171T, D183A,
R170K/A171T/D183A proteases for either hybrid features or energy-only features.
14