Under review as a conference paper at ICLR 2021
Understanding, Analyzing, and Optimizing
the Complexity of Deep Models
Anonymous authors
Paper under double-blind review
Ab stract
This paper aims to evaluate and analyze the complexity of feature transformations
encoded in DNNs. We propose metrics to measure three types of complexity of
transformations based on the information theory. We further discover and prove
the strong correlation between the complexity and the disentanglement of trans-
formations. Based on the proposed metrics, we analyze two typical phenomena of
the change of the transformation complexity during the training process, and ex-
plore the ceiling of a DNN’s complexity. The proposed metrics can also be used
as a loss to learn a DNN with the minimum complexity, which also controls the
significance of over-fitting of the DNN. Comprehensive comparative studies have
provided new perspectives to understand the DNN. We will release the code when
the paper is accepted.
1 Introduction
Understanding the black-box of deep neural networks (DNNs) has attracted increasing atten-
tion in recent years. Previous studies usually interpret DNNs by either explaining DNNs visu-
ally/semantically (Lundberg & Lee, 2017; Ribeiro et al., 2016), or analyzing the feature representa-
tion capacity of a DNN (Higgins et al., 2017; Achille & Soatto, 2018a;b; Fort et al., 2019).
In this paper, we focus on the complexity of feature transformations encoded in the DNN. Based on
the information theory, we define three metrics to measure the complexity of feature transformations
in DNNs. The transformation complexity reveals new insights into the feature disentanglement,
over-fitting, and the representation power of a DNN.
The first metric is defined as the entropy of the gating states of nonlinear operations. Specifically, let
us consider the task (X, Y ). Given a DNN, the complexity of transformations represents the diversity
of transformations that map each input x ∈ X to the corresponding y ∈ Y through all images. From
this perspective, the simplest model is a linear transformation y = f (wTx + b) = wTx + b, where
the nonlinear operation f (∙) is by-passed as an all-passing gating layer. In this way, the entropy of
its gating state is zero. The complexity is quantified as the entropy of gating states of all gating
layers (e.g. ReLU, max-Pooling, and Dropout layers). The DNN usually generates various gating
states in gating layers for different inputs, thereby applying different transformations to these inputs.
Let the binary vector σl = [σl1, σl2, . . . , σld] ∈ {0, 1}d denote gating states of whether the feature in
each dimension can pass through the gating layer. Σl = {σl } denotes the set of gating states σl
among all samples X1. In this way, the entropy H(Σ) among all inputs measures the complexity of
transformations in all the L gating layers, where Σ = [Σ1 , Σ2, . . . , ΣL].
Based on the entropy H(Σ), we further propose I(X; Σ) and I(X; Σ; Y ) as two additional metrics to
measure the complexity of transformations. The mutual information I(X; Σ) measures the complex-
ity of transformations that are caused by the input. The mutual information I(X; Σ; Y ) represents
the complexity of transformations that are caused by the input and are directly used for the in-
ference. For example, in the task of object classification, not all transformations in the DNN are
category-specific. I(X; Σ; Y ) reflects category-specific components of transformations. Notice that
I(X; Σ) 6= H(Σ) in some cases. For example, the sampling operation in the Variational AutoEncoder
(VAE) and the dropout operation both introduce additional transformations that are not caused by
the input X, which makes I(X; Σ) 6= H(Σ).
1Please see Appendix I for details.
1
Under review as a conference paper at ICLR 2021
First, we prove that the complexity decreases through the layerwise propagation. In other words,
deep features usually require simpler transformations to conduct inference than shallow features.
Then, we use the proposed complexity metrics to diagnose DNNs. We summarize the change
of the complexity during the training process into two types. In traditional DNNs without skip-
connections, the complexity usually decreases first, and increases later. This indicates that DNNs
may drop noisy features in early stages, and learn useful information later. Whereas, in residual
networks, the transformation complexity increases monotonously during the learning process.
In particular, in this study, we conduct the following explorations based on the complexity metrics.
(1)	Disentanglement: we prove the strong correlation between the complexity and the disen-
tanglement of transformations. Let us consider DNNs with similar activation rates in a certain
layer. If the complexity of the transformation encoded in a specific layer is larger, then gating states
of different feature dimensions tend to be more independent with each other.
(2)	Minimum complexity: we use the complexity as a loss to learn a DNN with the minimum
complexity. A DNN usually learns over-complex transformations w.r.t. the task. Thus, we propose
a complexity loss, which penalizes unnecessary transformations to learn a DNN with the minimum
complexity.
(3)	Over-fitting: reducing the transformation complexity alleviates the over-fitting problem.
Given the DNN learned using the complexity loss, we find that the gap between the training loss and
the testing loss of a DNN decreases, when we reduce the complexity of transformations.
(4)	Maximum complexity: we explore the ceiling of a DNN’s complexity. The complexity of a
DNN does not always increase when we keep adding more gating layers into the DNN. In contrast,
the traditional stacked DNN with deeper architecture may encode transformations of much lower
complexity in some cases. Whereas, the transformation complexity of residual networks is saturated
instead of decreasing when we used more gating layers. Besides, the complexity of transforma-
tions does not increase monotonously along with the increase of the complexity of tasks. In
contrast, if the task complexity exceeds a certain limit, the complexity of transformations encoded
in the DNN will decrease along with the increase of the task complexity.
Relationship to the information bottleneck (IB). The transformation complexity I(X; Σ) has es-
sential difference from the I(X; Z) term in the IB (Shwartz-Ziv & Tishby, 2017), where Z denotes
the feature of an intermediate layer in DNNs. In the Markov process X → Z → Y , given the
feature Z, X and Y are conditional independent. In comparison, the gating state Σ does not contain
all information of Z, which makes the transformation complexity I(X; Σ) essentially different from
I(X; Z) in mathematics. Please see Appendix B for more discussions.
Contributions of the study can be summarized as follows. (1) We define three metrics to evaluate the
complexity of transformations in DNNs. (2) We prove the strong correlation between the complexity
and the disentanglement of transformations. (3) We further use the transformation complexity as a
loss to learn a minimum-complexity DNN, which also reduces over-fitting. (4) Comparative studies
reveal the ceiling of a DNN’s complexity.
2	Related Work
In this section, we limit our discussion within the literature of understanding representations of
DNNs. In general, previous studies can be roughly summarized into the following two types.
•	The first type is the semantic explanations for DNNs. Some studies directly visualized knowl-
edge representations encoded in the DNN (Zeiler & Fergus, 2014; Mahendran & Vedaldi, 2015;
Yosinski et al., 2015; Dosovitskiy & Brox, 2016; Simonyan et al., 2017). Other methods estimated
the pixel-wise attribution to the network output (Zhou et al., 2015; Selvaraju et al., 2017; Fong &
Vedaldi, 2017; Kindermans et al., 2017; Zhou et al., 2016). The LIME (Ribeiro et al., 2016) and
SHAP (Lundberg & Lee, 2017) extracted important input units that directly contributed to the out-
put. Some visual explanations reveal certain insightful properties of DNNs. Fong & Vedaldi (2017)
analyzed how multiple filters jointly represented a certain semantic concept. In contrast, in this
paper, we propose to investigate the representation capacity from the perspective of transformation
complexity encoded in DNNs.
2
Under review as a conference paper at ICLR 2021
•	The second type is to analyze the feature representation capacity of DNNs. The stiffness (Fort
et al., 2019) was proposed to diagnose the generalization capacity of a DNN. Xu (2018) applied
the Fourier analysis to explain the generalization capacity of DNNs. The CLEVER score (Weng
et al., 2018) was used to estimate the robustness of DNNs. Wolchover (2017); Shwartz-Ziv &
Tishby (2017) proposed the information bottleneck theory and used mutual information to quantify
the information encoded in DNNs. Xu & Raginsky (2017); Achille & Soatto (2018b); Goldfeld
et al. (2019) further extended the information theory to constrain the feature representation to learn
more disentangled features. Chen et al. (2018) selected instance-wise features based on mutual
information to interpret DNNs. Kornblith et al. (2019) used the canonical correlation analysis (CCA)
to compare feature representations from the perspective of similarity.
Unlike previous studies, we focus on the complexity of feature transformations encoded in DNNs.
Previous methods on the complexity of DNNs can be summarized as follows:
•	Computational complexity and difficulty of learning a DNN: Some studies focus on the computa-
tional complexity required to learn a DNN with the certain accuracy. Blum & Rivest (1989) proved
that learning a one-layer network with a sign activation function was NP-hard. Livni et al. (2014)
further discussed other activation functions. Boob et al. (2018); Manurangsi & Reichman (2018)
proved learning a two-layer ReLU network was also NP-hard. Arora et al. (2016) showed that it was
possible to learn a ReLU network with one hidden layer in polynomial time, when the dimension of
input was constant.
•	Architectural complexity and representation complexity: Raghu et al. (2017) proved that as the
depth of the DNN increased, the maximal complexity of features grew exponentially. Pascanu et al.
(2013); Zhang et al. (2016) proposed three metrics to measure the architectural complexity of recur-
rent neural networks. To estimate the maximal representation capacity, Liang et al. (2017); Cortes
et al. (2017) applied the Rademacher complexity. Kalimeris et al. (2019) analyzed the complexity
of features in a DNN by comparing those features with the feature learned by a linear classifier.
Unlike analyzing the complexity of the DNN based on its architecture, we aim to measure the
complexity of feature transformations learned by the DNN. We define three types of transformation
complexity, which provide new perspectives to understand the DNN.
3	Transformation Complexity
In this section, we define three types of complexity of transformations in DNNs based on the infor-
mation theory. Given the input x ∈ X and the target label y ∈ Y , the DNN is learned to map x to y .
Layerwise transformations of mapping x to y can be roughly represented by
y = f (WL+ισι(Wι... (W2σ1(W1x + bi) + b2) ∙∙∙ + bl) + bL+ι)	(1)
where f denotes the optional layer on the top, e.g. the softmax layer. Wl and bl denote the weight
and the bias of the l-th linear layer. Let σl = [σl1, σl2 , . . . , σld] ∈ {0, 1}d denote gating states of the
l-th gating layer. σl = diag(σl1, σl2, . . . , σld) is a diagonal matrix with σl as its main diagonal. Gating
layers include the ReLU, max-Pooling, and Dropout layer. Take the ReLU layer as an example2.
If the i-th dimension of the input feature is larger than 0, then we have σli = 1; otherwise, σli =
0. Let Σl = {σl } denote the set of gating states of the l-th gating layer. Given a certain input
x, σ = [σ1 , σ2 , . . . , σL] represents concatenated and vectorized gating states of all gating layers.
Accordingly, Σ = [Σ1 , Σ2 , . . . , ΣL] denotes the set of gating states of all L gating layers over all
samples1. We focus on gating layers in the DNN and define the following metrics to measure three
types of complexity of transformations from x to y.
•	H(Σ): the entropy of gating states among all inputs. H(Σ) measures the complexity of transfor-
mations that are encoded in gating layers. A larger H(Σ) indicates the DNN learns more complex
transformations. The complexity H(Σ) can be decomposed as H(Σ) = H(∑ι) + H(∑2∣∑1) +------+
H(∑L ∣∑1,..., ∑L-i).
•	I(X; Σ): the complexity of transformations that are caused by the input. If the DNN does not
introduce additional complexity that is not caused by the input, then Σ is determined by X. I.e.
H(Σ∣X) = 0 and I(X; Σ) = H(Σ) - H(Σ∣X) = H(Σ).
2Please see Appendix A for more details about other types of gating layers.
3
Under review as a conference paper at ICLR 2021
10
7 8
舐
H 4
2
0	100	200	300	400	500
epoch
Figure 1: The complexity of
transforming the feature Tl to the
output, i.e. H(Σl , . . . , ΣL).
Figure 2: The negative correlation between the complexity of
transformations H(Σl) andtheentanglementTC(Σl) of DNNs
with similar activation rates.
•	I(X; Σ; Y ): the complexity of transformations that are caused by the input and used for the in-
ference, which is defined as I(X;£; Y) = I(X; Y) - I(X; Y∣Σ). I(X; Y∣Σ) = I(X∣Σ) - I(X∣Σ,Y)
measures the mutual information between X and Y that is irrelevant to gating layers.
Proposition 1. (Proof in Appendix D.1) If the DNN does not introduce additional information that
is not contained by the input X (e.g. there are no operations of randomly sampling or dropout
throughout the DNN), we have I(X; Σ; Y) ≥ 0.
Proposition 2. Decrease of the complexity through layerwise propagation. (Proof in Ap-
Pendix D.2) Let tι = σι(Wι... σ1(W1x + bi)…+ bi) ∈ Tl denote the output feature of
the l-th gating layer. Then, the complexity decreases as we use the feature of higher lay-
ers. I.e. H (Σl,…,∑l) ≥ H (∑l+ι,..., ∑l) , I (Tl-i； {∑l,..., ∑l}) ≥ I (Tl; {∑l+ι,..., ∑l}),
I (Tl-1; {Σl, . . . , ΣL }; Y) ≥ I(Tl; {Σl+1, . . . , ΣL}; Y), where Σl+1 , . . . , ΣL denote the gating states
of the (l + 1)-th,. . . , L-th gating layer.
In Proposition 2, we focus on the complexity of transforming the intermediate-layer feature Tl to the
output Y . It shows that the transformation complexity decreases through the layerwise propagation.
This proposition can also be proved by the data processing inequality (DPI (Cover, 1999)).
Verification of the decrease of the complexity through layerwise propagation. Figure 1
shows the change of the complexity of transforming the l-th layer feature Tl to the output, i.e.
H(Σl , . . . , Σi0), l = 1, . . . , 10 encoded in the DNN during the training process, which verified the
decrease of the complexity through layerwise propagation.
Quantification of the complexity. We use the KDE (Kolchinsky & Tracey, 2017; Kolchinsky et al.,
2019; Saxe et al., 2019) to estimate the complexity H(Σ), I(X; Σ) and I(X; Σ; Y), which is widely
used in recent years. H(Σl) is usually quantified using the following upper bound.
H∑ ≤ -1 Xn=Ilog n χn=ι eχp (-1 kσj-σ,kl)	⑵
where n denotes the number of training samples. σl,j and σl,k denote the vectorized gating states
of the l-th gating layer for the j-th sample and the k-th sample, respectively. σ02 is quantified as
σ0 = α ∙ Var(Σl), where Var(Σl) = Eχ[∣∣σl - μ∣∣2],μ = Ex[σl]. Var(Σl) measures the variance of
gating states of the l-th gating layer. α is a positive constant. The above equation can also be used
to quantify H(Σ), when we simply replace Σl with Σ.
If the DNN does not introduce additional complexity besides the input X, We have H(Σl∣X) = 0,
I(X; Σl) = H (Σl) - H (Σl∣X) = H(Σl). If the DNN introduces additional complexity (e.g. using the
sampling operation in VAE, or the dropout operation), then I(X; Σl) can be quantified as follows.
T√ V F、/	1 V^n 1	1 V^n	1 1 ∣∣σl,j ― σl,k k2>∖	/ɔʌ
I(XAl)≤-nEj=IlognEk=Iexp(-2	~)	⑶
where σlj and σl,k represent the vectorized gating states when sampling operations are removed (in
this way, we can use the method of measuring H(Σl) to quantify I(X; Σl)).
Similarly, the complexity I(X; Σl; Y) can be estimated by its upper bound:
I (X ； ςi; Y) ≤-I®；YIX) 一 1 Xn=Ilog n Xn=I exp (-2 kσlj-σkk2)	⑷
-XM=Ipm [ — nm Xj,Yj=m log nm Xk,Yk=m eXP 2	)]⑷
For the task of multi-category classification, M denotes the number of categories. nm is the number
of training samples belonging to the m-th category, and pm = nm/M. If the DNN does not introduce
4
Under review as a conference paper at ICLR 2021
additional complexity besides the input, we have I(Σl; Y |X) = 0. Equations (2)(3)(4) are given
in (Kolchinsky & Tracey, 2017; Kolchinsky et al., 2019; Saxe et al., 2019).
4 Analysis of DNNs based on the transformation complexity
4.1	Strong correlation between the complexity and the disentanglement
Although intuitively, the disentanglement of gating states seems not related to the complexity, in
this section, we prove a clear correlation between these two terms. Let us consider the complexity
of the transformation encoded in a single gating layer, e.g. H(Σl), I(Tl-1; Σl) and I(Tl-1; Σl; Y ) for
the l-th gating layer. For gating states of the l-th gating layer Σl, we analyze whether gating states
σli in different dimensions are related to each other. The entanglement of transformations TC(Σl)
measures the dependence between gating states of different dimensions, which is given in (Achille
& Soatto, 2018a; Ver Steeg & Galstyan, 2015) as the multi-variate mutual information.
TC(∑ι) = KL(p(σι)∖∖Yi p(σi))	(5)
where p(σi) denotes the marginal distribution of the i-th element in σι. Assume that p(σi)〜
Bernoulli(αli), where αli is the activation rate of the i-th dimension of the l-th gating layer. In particu-
lar, TC(Σl) is zero if and only if all elements ofσl are independent with each other. In this case, we
say Σl is disentangled. I.e. TC(Σl) measures the entanglement. Further, we prove
H(Σl) + TC(Σl) =Cl, Cl = -Eσl[logYip(σli)]	(6)
Please see Appendix D.3 for the proof. Let us consider DNNs with similar activation rates αli .
Because p(σli) follows the Bernoulli distribution with the activation rate αli, for DNNs with similar
activation rates αli, they share similar values of Cl . In this way, there is a negative correlation
between the complexity H(Σl) and the entanglement TC(Σl). In other words, for a specific layer,
if the complexity of the transformation is larger, then gating states of different dimensions tend to
be more independent with each other. We extend the similar conclusion in Eq. (6), i.e. the negative
correlation between the complexity and the entanglement, to I(X; Σl) and I(X; Σl; Y ) as follows.
I(X; Σl) + TC(Σl) = Cl - H(Σl∖X)
I(XAi； Y) +	(TC(∑ι) - TC(∑ι∖Y))	= CI- Cι∣γ - (H(∑ι∖X) - H(∑ι∖X,Y))	(7)
V--------{z---------}
multi-variate mutual information used to infer Y
where Cl , -Eσl,y[log Qi p(σli∖y)]. If the DNN does not introduce additional information besides X
and Y, then H(Σl∖X) = H(Σl ∖X, Y) = 0. Just like the case in Eq. (6), for DNNs with similar acti-
vation rates, we can also roughly consider that these DNNs share similar values of Cl|Y . Please see
Appendix E for more detailed discussions. Thus, these DNNs share similar values of Cl - H(Σl∖X),
Cl|Y - H(Σl∖X, Y). I.e. the negative correlation between the complexity and the entanglement of
transformations still holds true for I(X; Σl) and I(X; Σl ; Y).
Verification of the strong correlation between the complexity and the disentanglement
(Eq. (6)). We learned 21 LeNet-5 models and 21 MLP-β3 models with different initialized pa-
rameters on the MNIST dataset. These models shared similar activation rates on each dimension in
the l-th gating layer (we used l = 3, 4). Thus, we quantified H(Σl) and TC(Σl) (Please see Eq. (28)
for the quantification of TC(Σl)) over the 42 models. We also conducted such an experiment on 21
ResNet-20 models with l = 3. Figure 2 shows the negative correlation between H(Σl) and TC(Σl),
which was verified using different layers of DNNs with different architectures.
4.2 Comparative studies
This section introduces several comparative studies and provides new insights about the representa-
tion capacity of DNNs.
The change of complexity during the learning of DNNs. Figure 3 shows the change of three
types of transformation complexity encoded in traditional DNNs without skip-connections during
3Please see Section 4.4 for the network architecture and experimental settings.
5
Under review as a conference paper at ICLR 2021
11.
.0
10.
.5
10.
.0
'9
.5
‘9.
.0
8.
.5
8.
.0
7.
.5
MLP-MNIST
MLP-CIFAR10
LeNet-MNIST
LeNet-CIFAR10
revised VGG11MNIST
revised VGG11
-CIFAR10
100	200	300	400	500
epoch
1.
.8
2.
.8
2.
.6
.4
2.
.2
2.
.0
3.25
3.00
1.
.75
1.
.50
epoch
W 2.50
苦 2.25.
12.00∙
2.75
MLP-MNIST
MLP-CIFAR10
LeNet-MNIST
LeNet-CIFAR10
revised VGG11MNIST
revised VGG11
-CIFAR10
0.
.4
0.
.3
(a)	The change of H(Σ) in DNNs
100	200	300	400	500
epoch
0.
.2
0.
.7
0.
.6
0.
.5
0
100	200	300	400	500
epoch
I VGG16-VOC
----VGG16-CelebA(x10)
(b)	The change of/(X; Σ; Y) in DNNs
0
11.0
10.5
_J0.0
09.5
H 9.0
8.5
8.0
7.5
C
■ MLP-MNIST
0	100	200	300	400	500
epoch
.5
.4
.3
.2
.1
.0
0
0.
0.
0.
0.
0.
0.
B: epoch 55
0.0 0.2 0.4 0.6	0.8 1.0
activation rate
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
C: epoch 500
0.0 0.2 0.4 0.6	0.8 1.0
activation rate
(c)	The change of the frequency histogram of different neurons’ activation rates.
Figure 3: The change of the transformation complexity and the activation rates in traditional DNNs
without skip-connections.
Figure 4: The change of the transformation complexity in residual networks.
等

the training process. Note that H(Σ) = I(X; Σ). We found two phenomena in the change of com-
plexity: (1) For most DNNs (like MLPs3 and LeNets), both H(Σ) and I(X; Σ; Y ) decreased first,
and increased later. Figure 3 (c) shows the frequency histogram of different neurons’ activation rates
of gating states in Epoch 0, 55, and 500. In Epoch 0, gating states were usually randomly activated,
which led to a large value of H(Σ). The learning process gradually removed noisy activations, which
reduced H(Σ) and achieved the minimum complexity in Epoch 55. Then, the DNN mainly learned
complex transformations to boost the performance, which made H(Σ) begin to increase. This indi-
cated that DNNs dropped noisy features in early stages of the training process, then learned useful
information for the inference. (2) For other DNNs (like the VGG-16 trained on the Pascal VOC
dataset), the complexity increased monotonously during the early stage of the training process, and
saturated later.
Figure 4 shows the change of transformation complexity encoded in residual networks. (3) Both
H(Σ) and I(X; Σ; Y) increased monotonously during the training process in most residual networks.
This indicated that skip-connections reduced the influence of noisy features on the learning of DNNs.
In particular, in VAEs, H(Σ) 6= H(X; Σ). We were given a VAE3, in which both the encoder and
the decoder had two FC layers. We added a classifier with two FC layers and two ReLU layers after
the encoder. The VAE was trained on the MNIST dataset and the CIFAR-10 dataset, respectively.
Figure 6 shows the complexity of transformations encoded in each gating layer of the classifier. (4)
The difference between H(Σl) and I(X; Σl) gradually decreased during the training process. This
indicated that the impact of inputs on the transformation complexity kept increasing. At the same
time, the noisy features encoded in the DNN kept decreasing.
Maximum complexity: estimation of the ceiling of a DNN’s complexity. Based on the CIFAR-10
dataset, we constructed a set of MLPs (termed task MLPs), each consisting of n ReLU layers and
FC layers with the width of 1024. We used n = 0, 1, . . . , 31. The task MLP first transformed images
in the CIFAR-10 dataset into gray scale and took them as input. We learned another set of MLPs
(termed target MLPs), and these target MLPs consisted of 6,12,18,24 ReLU layers and FC layers
with the width of 1024. We conducted multiple experiments to train each target MLP to reconstruct
the output of each task MLP with an MSE loss. In this way, since the task MLP contained n ReLU
layers, we considered the complexity of using target MLPs to mimic task MLPs was Task-n.
Findings from stacked networks. Figure 5 (left) compares the complexity of transformations encoded
in target MLPs (with the traditional stacked architecture) for different tasks. (1) For the task of low
complexity, deep MLPs learned more complex transformations than shallow MLPs. (2) However, as
the complexity of the task increased, the complexity of transformations encoded in shallow MLPs
was usually higher than that encoded in deep MLPs.
6
Under review as a conference paper at ICLR 2021
Figure 5: The change of the transformation
complexity along with the increase of the task
complexity n.
Figure 6: The change of H(Σl) and I(X; Σl)
in VAEs learned on the MNIST dataset and the
CIFAR-10 dataset.
Findings from residual networks. Besides above target MLPs, we further designed new target MLPs
with skip-connections, which were termed residual MLPs. We added a skip-connection to each FC
layer in each of above target MLPs. The complexity of transformations encoded in residual MLPs
was shown in Figure 5 (right). (1) Deep residual MLPs always encoded higher transformation
complexity than shallow residual MLPs. (2) We also found that when we gradually increased the
complexity of the task to train target MLPs, the transformation complexity encoded in target MLPs
increased along with the increase of the task complexity in the beginning. (3) However, when the
task complexity exceeded a certain limit, the complexity of transformations saturated and started to
decrease. This phenomenon indicated the limit of the representation capacity of DNNs.
4.3	Learning a DNN with the minimum complexity
Minimum complexity. A DNN may use over-complex transformations for prediction, i.e. the
complexity I(X; Σ; Y ) does not always represent the real complexity of the task. In this section, we
develop a method to avoid learning an over-complex DNN. The basic idea is to use the following
loss to quantify and penalize the complexity of transformations during the training process.
L
Ltask + λLcomplexity
(8)
The first term Ltask denotes the task loss. E.g. if the task is object classification, then Ltask can be
the cross-entropy. The second term Lcomplexity penalizes the complexity of transformations encoded
in the DNN. λ is a positive scalar. For simplicity, Lcomplexity is implemented as follows.
Lcomplexity = Xl=1H(Σl) = Xl=1{-Eσl [log p(σl)]}	(9)
The exact value of p(σl) is difficult to calculate, so we use an energy-based model (EBM (LeCun
et al., 2006; Gao et al., 2018)) with parameter θf to approximate it, as follows.
Pθf (σ) = ZIj) exp[f (σι； θf)] ∙ q(σι)	(10)
where q(σι) is a prior distribution defined as q(σι) = Qi q(σj) and q(σj)〜 BernOuni(ai). f (σι; θf)
is implemented as a ConvNet with parameters θf (Gao et al., 2018). The constant Z(θf) =
Rσ q(σl) exp[f (σl ; θf)]dσl is for normalization. For the EBM, the parameter θf is learned via the
maximum likelihood estimation (MLE).
θf
argmax 1 Xn logpθf (σι,j)
θf n	j=1	f
(11)
where n denotes the number of samples. σl,j is a vector, which represents gating states in the l-th
gating layer for the j-th sample. We follow (Gao et al., 2018) to optimize the EBM parameters θf
with Markov Chain Monte Carlo (MCMC).
Thus, the penalty term for the complexity can be rewritten as
Lcomplexity = -XL=IEσjlogP^f (σl)] = 1 XL=IXn=1 lθg Z-1--exP[f (σl,j； θf )] ∙ q(σl,j)
(12)
Note that the gating state σl,j is not differentiable w.r.t. the network parameters. To this end,
the ReLU operation can be approximated using the Swish function ReLU(x) = x	σl = x
sigmoid(βx) (Ramachandran et al., 2017), where denotes the element-wise multiplication. This
enables us to use Lcomplexity to learn network parameters. During the training process, the EBM and
the original DNN are trained alternatively. For every batch of training data, we firstly train the EBM
using Eq. (11). Then, we fix parameters in the EBM and optimize the original DNN with the loss
L = Ltask + Lcomplexity.
7
Under review as a conference paper at ICLR 2021
10
8
6
4
2
0
Λ	Λ	Λ
Figure 7: The complexity of transformations and the gap between the training loss and the testing
loss of the learned minimum-complexity DNNs. The left-most point in each subfigure at λ = 0
refers to DNNs learned by only using the task loss.
The complexity loss reduced the complexity of transformations and the gap between the train-
ing loss and the testing loss. We added the complexity loss to the last four gating layers in each
DNN to train the residual MLP4, ResNet-20/32 (He et al., 2016a) based on the CIFAR-10 dataset,
and train ResNet-18/34 based on the first ten classes in the Tiny ImageNet dataset. We repeatedly
trained these DNNs with different values of λ. In particular, when λ = 0, DNNs were learned only
with Ltask, which can be taken as baselines. The significance of over-fitting was quantified as the
numerical difference between the training loss and the testing loss.
Figure 7 shows the complexity and the gap between the training loss and the testing loss of DNNs
learned with different λ values. We found that H(Σl) usually decreased along with the increase ofλ.
I(X; Σ; Y ) also decreased along with the increase ofλ, which indicated that the complexity loss also
effectively reduced the complexity I(X; Σ; Y ). We also found that the decrease of transformation
complexity reduced the gap between the testing loss and the training loss. As Figure 8 shows, the
complexity loss also decreased the testing loss in some cases.
4.4	Experimental settings
We conducted a set of comparative studies on the task of classification using the MNIST (LeCun
et al., 1998), CIFAR-10 (Krizhevsky et al., 2009), CelebA (Liu et al., 2015), Pascal VOC 2012 (Ev-
eringham et al., 2015), and Tiny ImageNet (Le & Yang, 2015) datasets. For the MNIST dataset
and the CIFAR-10 dataset, we learned LeNet-5 (LeCun et al., 1998), the revised VGG-11 (Jastrzeb-
ski et al., 2017), the pre-activation version of ResNet-20/32 (He et al., 2016a;b), and the MLP. In
particular, for the MNIST dataset, we learned three MLP models: MLP-MNIST contained 5 fully
connected (FC) layers with the width of 784-1024-256-128-64-10, MLP-α contained 11 FC lay-
ers with the width of 784-1024-1024-512-512-256-256-128-128-64-16-10, and MLP-β contained 5
FC layers with the width of 784-1024-256-120-84-105. For the CIFAR-10 dataset, the architecture
of the MLP was set as 3072-1024-256-128-64-10 (termed MLP-CIFAR10). For the CelebA, Pas-
cal VOC 2012, and Tiny ImageNet datasets, we learned VGG-16 (Simonyan et al., 2017) and the
pre-activation version of ResNet-18/34. Considering the essential difference between convolutional
layers and FC layers, we set α = 0.04 for gating layers following each convolutional layer, and
α = 0.01 for gating layers following each FC layer. We also tested the effects of different α values
to demonstrate the stability and trustworthiness of the complexity quantification in Appendix H. We
used object images cropped by bounding boxes for both training and testing. Please see Appendix G
for the classification accuracy of these DNNs. We analyzed the transformation complexity of ReLU
layers.
5 Conclusion
In this paper, we have proposed three complexity measures for feature transformations encoded in
DNNs. We further prove the decrease of the transformation complexity through layerwise propa-
gation. We also prove the strong correlation between the complexity and the disentanglement of
transformations. Based on the proposed metrics, we develop a generic method to learn a minimum-
complexity DNN, which also reduces the significance of over-fitting of the DNN. Comparative
studies reveal the ceiling of a DNN’s complexity. Furthermore, we summarize the change of the
transformation complexity during the training process into two typical cases. As a generic tool, the
transformation complexity enables us to understand DNNs from new perspectives.
4The residual MLP has the similar architecture to the one in the “Findings from residual networks” para-
graph in Section 4.2, with 10 FC layers and 9 ReLU layers. Both the input and features are 3072-d vectors.
5For comparison, we modified the architecture of MLP-MNIST and made the width of the last three FC
layers be the same with the fully connected layers in the LeNet-5 network.
8
Under review as a conference paper at ICLR 2021
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018a.
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation. IEEE transactions on pattern analysis and machine intelligence, 40
(12):2897-2905, 2018b.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Advances
in neural information processing systems, pp. 494-501, 1989.
Digvijay Boob, Santanu S Dey, and Guanghui Lan. Complexity of training relu neural network.
arXiv preprint arXiv:1809.10787, 2018.
Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. In International Conference on Ma-
chine Learning, pp. 882-891, 2018.
Corinna Cortes, Xavier Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. Adanet:
Adaptive structural learning of artificial neural networks. In Proceedings ofthe 34th International
Conference on Machine Learning-Volume 70, pp. 874-883. JMLR. org, 2017.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4829-
4837, 2016.
Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew
Zisserman. The pascal visual object classes challenge: A retrospective. International journal of
computer vision, 111(1):98-136, 2015.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437,
2017.
Stanislav Fort, PaWeI Krzysztof Nowak, and Srini Narayanan. Stiffness: A new perspective on
generalization in neural networks. arXiv preprint arXiv:1901.09491, 2019.
Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative con-
vnets via multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 9155-9164, 2018.
Ziv Goldfeld, Ewout Van Den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kings-
bury, and Yury Polyanskiy. Estimating information flow in deep neural networks. In International
Conference on Machine Learning, pp. 2299-2308, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. ICLR, 2(5):6, 2017.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos J. Storkey. Three factors influencing minima in sgd. arXiv preprint
arXiv:1711.04623, 2017.
9
Under review as a conference paper at ICLR 2021
Dimitris Kalimeris, Gal Kaplun, Preetum Nakkiran, Benjamin Edelman, Tristan Yang, Boaz Barak,
and Haofeng Zhang. Sgd on neural networks learns functions of increasing complexity. In Ad-
Vances in Neural Information Processing Systems 32, pp. 3496-3506. Curran Associates, Inc.,
2019.
Pieter-Jan Kindermans, Kristof T Schutt, Maximilian Alber, KlaUs-Robert Muller, Dumitru Erhan,
Been Kim, and Sven Dahne. Learning how to explain neural networks: Patternnet and patternat-
tribution. arXiv preprint arXiv:1705.05598, 2017.
Artemy Kolchinsky and Brendan D Tracey. Estimating mixture entropy with pairwise distances.
Entropy, 19(7):361, 2017.
Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck.
Entropy, 21(12):1181, 2019.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. arXiv preprint arXiv:1905.00414, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7, 2015.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. Predicting structured data, 1(0), 2006.
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geome-
try, and complexity of neural networks. arXiv preprint arXiv:1711.01530, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in neural information processing systems, pp. 855-863, 2014.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances
in Neural Information Processing Systems, pp. 4765-4774, 2017.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188-5196, 2015.
Pasin Manurangsi and Daniel Reichman. The computational complexity of training relu (s). arXiv
preprint arXiv:1810.04207, 2018.
Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep
recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl Dickstein. On the
expressive power of deep neural networks. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 2847-2854. JMLR. org, 2017.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv
preprint arXiv:1710.05941, 2017.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should I trust you?”： Explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016.
Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D
Tracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.
10
Under review as a conference paper at ICLR 2021
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE International Conference on Computer Vision, pp. 618-626,
2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
K Simonyan, A Vedaldi, and A Zisserman. Deep inside convolutional networks: visualising image
classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2017.
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Greg Ver Steeg and Aram Galstyan. Maximally informative hierarchical representations of high-
dimensional data. In Artificial Intelligence and Statistics, pp. 1004-1012, 2015.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
arXiv preprint arXiv:1801.10578, 2018.
Natalie Wolchover. New theory cracks open the black box of deep learning. In Quanta Magazine,
2017.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learn-
ing algorithms. In Advances in Neural Information Processing Systems, pp. 2524-2533, 2017.
Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis.
arXiv preprint arXiv:1808.04295, 2018.
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan R Salakhutdinov,
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. In Advances
in neural information processing systems, pp. 1822-1830, 2016.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Object detectors
emerge in deep scene cnns. In ICLR, 2015.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2921-2929, 2016.
11
Under review as a conference paper at ICLR 2021
A Gating states in gating layers
In this section, we mainly discuss gating states of different gating layers, which is mentioned in
Section 3 of the paper. Let h，Wl(σj(Wι-ι... (W2σ1(W1x + bi) + b2)…+ bι-ι)) + b
denote the input of the l-th gating layer. We consider the vectorized form of hl. Given hl ∈ Rd, the
formulation of σl in different gating layers is given as follows.
(1)	ReLU layer. In this case, σl = diag(σl1, σl2, . . . , σld) ∈ {0, 1}d, which is a diagonal matrix. If
the i-th dimension of hl is larger than 0, then we have σli = 1; otherwise, σli = 0.
(2)	Dropout layer. In this case, σl = diag(σl1, σl2, . . . , σld) ∈ {0, 1}d, which is a diagonal matrix. If
the i-th dimension of hl is not dropped, then we have σli = 1; otherwise, σli = 0.
(3)	Max-Pooling layer. Since a pooling layer may change the size of the input, σl is not necessarily
a square matrix. Let the output of the max-pooling layer σlhl ∈ Rd0, i.e. the input hl is divided into
d0 regions. In this case, we have σl ∈ {0, 1}d0×d. If (hl)j is the largest element in the i-th region,
then we have(σl)ij = 1; otherwise, (σl)ij = 0.
B The difference between the transformation complexity and
THE INFORMATION BOTTLENECK
Note that the transformation complexity I(X; Σ) has essential difference from the I(X; Z) term in
the information bottleneck (Tishby et al., 2000; Wolchover, 2017; Shwartz-Ziv & Tishby, 2017),
where Z denotes the feature of an intermediate layer in DNNs. The information bottleneck reflect
the trade-off between I(X; Z) and I(Z; Y ), which leads to the approximate minimal sufficient
statistics. In the forward propagation, the feature Z contains all information encoded in the DNN,
thereby forming a Markov process X → Z → Y . Thus, given the feature Z, X and Y are conditional
independent, i.e. I(X; Y |Z) = 0. However, the gating state Σ does not contain all information of
the feature Z. Let us take the ReLU layer for an instance. In ReLU layers, the gating state Σ only
represents whether the elements in feature Z are positive. The information encoded in Σ cannot be
directly used to conduct inference, which makes the transformation complexity I(X; Σ) essentially
different from I(X; Z) in mathematics.
C Decreasing the testing loss via the complexity loss
This section provides more discussions about the experiment in Section 4.3. Figure 7 has shown that
the complexity loss enabled people to reduce the gap between the testing loss and the training loss.
Sometimes, the complexity loss also significantly reduced the testing loss. We trained the residual
MLP on the CIFAR-10 dataset and trained ResNet-34 on the first ten classes of the Tiny ImageNet
dataset. Figure 8 shows that when We increased the weight λ of the complexity loss, the testing loss
dropped significantly. This further demonstrated the effectiveness of the complexity loss.
---ResMLP-CIFARlO, λ = 0
—ReSMLP-CIFAR10
—ReSNet34-TinyImageNet, λ = 0
ReSNet34-TinyImageNet
Figure 8: Decrease of the testing loss along with the increas of the weight of the complexity loss.
The dashed lines show the baseline testing losses when DNNs were learned only using the task loss
Ltask, i.e. λ = 0.
12
Under review as a conference paper at ICLR 2021
D Proofs of important conclusions
This section gives detailed proof for some important conclusions in the paper.
D.1 Non-negativity of the complexity I(X; Σ; Y )
Proposition 1 If the DNN does not introduce information besides the input X (e.g. there is no
sampling operations or dropout operations throughout the DNN), we have I(X; Σ; Y ) ≥ 0.
Proof. Recall that the mutual information is defined as
I(XK; Y) = I(X; Y) -1(X; Yl∑)
= (H(Y) - H (Y X)) -(H(Y ∣∑) - H (Y |X, ∑))	(13)
= (H(Y) - H (Y l∑)) - (H (Y X) - H (Y |X, ∑))
If the DNN does not introduce additional information besides X , which means that Σ is determined
byX,thenwehaveH(Y|X) - H(Y|X, Σ) = I(Σ; Y|X) = 0. Therefore,
I (X ； ∑; Y) = H (Y) - H (Y ∣∑) ≥ 0	(14)
D.2 Decrease of the complexity through layerwise propagation
The complexity of transforming the intermediate-layer feature tl to the output y decreases, when we
use the feature of higher layers.
•	H(∑ι,..., ∑l) ≥ H(∑ι+ι,..., ∑l)
Proof.
H(Σl,...,ΣL)-H(Σl+1,...,ΣL)
=H (∑ι∣∑ι+ι,..., ∑l)	(15)
= -Eσl
,...,σL [log P(σllσl + 1,...,σL)]
≥0
•	I(Tl-1; {Σl, . . . , ΣL}) ≥ I(Tl; {Σl+1, . . . , ΣL})
Proof. If the DNN does not introduce additional information besides the input during the forward
propagation, then ∑ι, ∑ι+ι,..., ∑l are all determined by T1-1, thereby H(∑ι,..., ∑l∣T1-i) = 0.
Therefore,
I(Tl-1; {Σl, . . . , ΣL}) - I(Tl; {Σl+1, . . . , ΣL})
=(H(Σι,..., ∑l) - H(Σι,..., ∑l∣Ti-i)) - (H(∑ι+ι,..., ∑l) - H(∑ι+ι,..., ∑l∣Ti))
=H(Σl, . . . , ΣL) - H (Σl+1, . . . , ΣL)
≥0
•	I (Ti-i； {∑ι,..., ∑l}; Y) ≥ I (Ti; {∑ι+ι,..., ∑l}; Y)
Proof. According to Eq. (14), if there is no additional information besides the input throughout the
DNN, then I (Tl-1； {∑ι,..., ∑l}; Y) = H (Y)-H (Y ∣{∑ι,..., ∑l}). We can obtain the following
inequality:
I(Tl-1； {∑l,..., ∑l}; Y) - I(Tl； {∑l+ι,..., ∑l}; Y)
=(H (Y) - H (Y ∣{Σl,..., ∑l})) - (H (Y) - H (Y ∣{∑l+ι,..., ∑l}))
=H (Y ∣{∑l+ι,..., ∑l}) - H (Y ∣{Σl,..., ∑l})	(17)
=I (Σl; Y ∣{∑l+ι,..., ∑l})
≥0
13
Under review as a conference paper at ICLR 2021
D.3 Strong correlations between the complexity and the disentanglement of
TRANSFORMATIONS
Some previous studies used the entanglement (the multi-variate mutual information) to analyze the
information encoded in DNNs. Ver Steeg & Galstyan (2015) used T C(X) to measure the correlation
between different input samples. In comparison, in this paper, we apply T C(Σ) to measure the
independence between gating states of different dimensions. Intuitively, the disentanglement of
gating states does not seem related to the complexity. Therefore, our contribution is to find out the
strong correlation between the two factors which seem not related.
We consider the complexity of the transformation of the a single gating layer, e.g. H(Σl), I(X; Σl)
and I(X; Σl; Y ) for the l-th gating layer.
•	H(Σl)
Proof.
H (∑ι) + TC(∑ι) = H (∑ι) + KL(p(σι )∣∣∏ p(σi))
i
=Eσι log ^1ʒ + Eσι log JR、
l p(σl) l ip(σli)
= -Eσl log	p(σli)	p(σli) does not depend on the input
i
= Cl
Let us consider DNNs with similar activation rates αli. Because p(σli) follows the Bernoulli distribu-
tion with the activation rate αli , for DNNs with similar activation rates αli , they share similar values
of Cl. It this case, there is a negative correlation between H(Σl) and TC(Σl).
•	I(X; Σl)
Proof.
I(XAl) + TC(∑1 ) = H(∑ι) - H(∑ι|X) + KL(p(σι)∣∣∏p(σ∣))
i
=Cl- H(∑1∣X)
(19)
If the DNN does not introduce additional information through the layerwise propagation, then the
X determines Σl, i.e. H(Σl |X) = 0. Thus, for DNNs with similar values of Cl, there is a negative
correlation between I(X; Σl) and TC(Σl ).
•	I (X 邑 Y)
Proof. According to the definition of I(X; Σl ; Y), we have
I (X 冬；Y )= I (X $)-1 (X KlIY)
(20)
We have discussed the first term I(X; Σl) above, so we focus on the second term I(X; Σl |Y),
which measures the complexity of transformations that are unrelated to the inference. Similarly, the
entanglement of the inference-irrelevant transformations is represented by
TC(Σl∣Y) = Ey (KL(p(σl ∣y)∣∣∏ p(σ力y)))
i
(21)
14
Under review as a conference paper at ICLR 2021
Then, we have
I(X; Σι |Y)+ TC(∑1∣Y) = H(∑1∣Y) - H(∑1∣X, Y) + TC(∑1∣Y)
=Ey H㈤|y) + KL(p(σιly)llYp(σlIy)) - H(£1|X,Y)
i
=Eσι,y [log ʤ + log Qp(TyI、1 — H(∑1∣X, Y)	(22)
l	p(σl Iy)	ip(σliIy)
= -Eσl,y log Y p(σliIy) - H(ΣlIX, Y)
i
= Cl|Y - H(Σl IX, Y)
If there is no additional information beside the input in the DNN, then H(Σl IX, Y) = 0. Thus, we
have
I(X； ∑ι) + TC(∑1) = CI- H(∑1∣X) = CI
I(X; ΣlIY) + TC(ΣlIY) = Cl|Y - H(Σl IX, Y) = Cl|Y
Therefore,
I(X;∑ι；Y) = I(X; ∑ι) - I(X;£i|Y)
=(CI- TC(∑ι)) — (Cι∣γ — TC(∑ι∣Y))
= (Cι — Cι∣γ) -	(TC(∑ι) - TC(∑ι∣Y))
X----------{z---------}
multi-variate mutual information used to infer Y
where the difference between TC(Σι) and TC(Σι IY) represents the entanglement of the trans-
formations that are used to infer Y. For DNNs with similar activation rates, we can also roughly
consider that these DNNs share similar values of Cι and Cι∣γ. Thus, We can conclude that the higher
complexity makes the DNN use more disentangled transformation for inference.
(23)
(24)
E	About values of Cl|Y
In experiments, We found that in most cases, for DNNs With similar activation rates αιi in their
corresponding layers, these DNNs usually shared similar values of C〃 γ. However, in some extreme
cases, e.g. When the DNN Was learned from very feW training samples, or When the target layer Was
very close to the input layer or the output layer, values of Cι∣γ of these DNNs were different from
those values of other DNNs.
_ _, ^ __________________ _, ^ __ _ ___________
F Quantification of I(T； X), I(T; Y), and TC(Σl)
This section introduces more details about the KDE approach in Section 3, which is used to quantify
the complexity. The Kernel Density Estimation (KDE) approach was proposed to estimate the mu-
tual information between the input X and the feature of an intermediate layer T in a DNN (Kolchin-
sky & Tracey, 2017; Kolchinsky et al., 2019). The KDE approach assumes that the intermediate-
layer feature is distributed as a mixture of Gaussians. Since that T is a continuous variable, H(T)
can be negative. The KDE method transforms each feature point into a local Gaussian distribution
to approximate the accurate feature distribution. Let T = T + E where E 〜N(0, σ2I). Then, the
distribution of T can be considered as a mixture of Gaussians, with a Gaussian centered on T. In
this setting (Kolchinsky & Tracey, 2017; Kolchinsky et al., 2019; Saxe et al., 2019), an upper bound
for the mutual information with the input is
I (T; X) ≤ - P X log P X exp (-2 j⅛tjf )	(25)
ij	0
where P is the number of training samples, and ti denotes the intermediate-layer feature of the input
sample i. Similarly, the upper bound for the mutual information w.r.t the output Y can be calculated
15
Under review as a conference paper at ICLR 2021
,ʌ , ʌ . , ʌ .
I(T; Y) =H(T) - H(T|Y)
- P X log P X exp
ij
(_ 1 M- tj∣l2 ʌ
L 2
(26)
L
- X pl
l=1
2
-P x log P X exp (-1;
i:Yi=l	j:Yj =l	0
where L is the number of categories. Pl denotes the number of samples belonging to the l-th cate-
gory. pl = Pl/P denotes the probability of the category l.
The entanglement of transformations is formulated as
TC(El) = KL(P(σl )ll∏ p(σi)) = Eσι
i
log
p(σl)
∏ip(σiλl,
(27)
where p(σli) denotes the marginal distribution of the i-th element in σl . To enable fair comparisons
between I(T, X) computed by the KDE method in Eq. (25) and TC(Σl ), we also apply the KDE
method to approximate TC(∑ι). To this end, We synthesize a new distribution p(σι) to represent
the distribution of Qi p(σf). In σι, σ∣ in each dimension follows the Bernoulli distribution with the
same activation rate OI with the original σ∖. Gating states σ∣ in different dimensions are independent
with each other. In this way, Qi p(σ∣) can be approximated by p(σι).
Inspired by (Kolchinsky & Tracey, 2017; Kolchinsky et al., 2019), TC(Σι) is quantified as the
following upper bound.
TC(∑l) = Eσι [logT
L PE)」
<£ X PkeXp (-1 kσijσσi,kk2)	(28)
≤ P j 1og Pk exp (T j应ɪj
where P denotes the number of samples. σι,k denotes the synthesized gating states, which have the
same activation rates with the gating states of the sample k.
G The classification accuracy of DNNs in comparative studies
This section contains more details of DNNs in Section 4. We trained five types of DNNs on the
MNIST dataset and the CIFAR-10 dataset, and trained three types of DNNs on the CelebA dataset
and the Pascal VOC 2012 dataset. Table 1 reports the testing accuracy of the trained DNNs.
Table 1: The classification accuracy of DNNs on different datasets.
(a) On the MNIST and CIFAR-10 datasets.
	MLP	LeNet-5	ResNet-20	ResNet-32	revised VGG-11
MNIST	96.52%	97.41%	^^98.70%^^	98.24%	99.00%
CIFAR-10	52.52%	61.5%	81.75%	79.76%	84.53%
(b) On the CelebA and Pascal VOC 2012 datasets.
	ResNet-18	ResNet-34	VGG-16
CelebA	80.25%^^	80.91%	89.70%
Pascal VOC 2012	67.99%	64.27%	62.50%
H The value of α in the KDE approach
In this section, we discuss about the value of the hyper-parameter α used in the KDE approach. Note
that the features of convolutional layers usually contain far more dimensions than features of fully-
connected layers. Therefore, we set α = 0.04 for gating layers following each convolutional layer,
16
Under review as a conference paper at ICLR 2021
and α = 0.01 for gating layers following each FC layer. We also tested the effects of different α
values to the quantification of the complexity. Figure 9 shows the complexity I(X; Σ) and I(Σ; Y )
calculated by different values of α in MLP-α learned on the MNIST dataset. We found that the
value of α did not affect the change of the complexity during the training process.
Figure 9: The complexity calculated with different values of a. The trend of the change of the
complexity was consistent when we used different values of α, which enabled fair comparisons
between different DNNs.
I Experimental details
Recall that Σl = {σl } denotes the set of gating states σl among all samples X. In this paper, we
randomly sample 2000 images from the training set of the each dataset for the calculation of the
transformation complexity. Thus, X denotes the set of 2000 randomly sampled images that are used
for analysis.
J Learning a minimum-complexity DNN
This section introduces more details about the learning of a minimum-complexity DNN in Sec-
tion 4.3. In Section 4.3, the complexity loss is defined as
Lcomplexity = Xl=1H(Σl) = Xl=1{-Eσl [logp(σl)]}	(29)
The exact value of p(σl) is difficult to calculate. Thus, inspired by (Gao et al., 2018), we design an
energy-based model (EBM) pθf (σl) to approximate it, as follows.
pθf (σι) = Z^) exp[f (σι; θf)] ∙ q(σι)
Z(θf) = Eq [exp[f (σl; θf)]] =	q(σl)exp[f(σl; θf)]dσl
σl
where q(σl) denotes the prior distribution, which is formulated as follows.
(30)
q(σl)=Yq(σi),	q(σi) ={ρ -P σi=0	(31)
If We write the EBM as Pθf (σ"=2垢 exp[-E(σι)], then the energy function is as follows.
Eθf(σl) = -logq(σl) - f(σl; θf)
(32)
The EBM can be learned via the maximum likelihood estimation (MLE) with the following loss.
1n
θf = arg max L(θf) = arg max 一	logpθf (σι j)	(33)
θf	θf	n j=1	,
where n denotes the number of samples. σl,j is a vector, which represents gating states in the l-th
gating layer for the j-th sample.
17
Under review as a conference paper at ICLR 2021
The loss and gradient of θf can be calculated as follows.
1n
L(θf) = - n Elog pθf (σι,j)
j=1
1n
-n ∑S[f (σι,j； θf) + log q(σι,j)] + log Z(θf)	(34)
j=1
dL∂θff)=Eθf [ ∂θ; f]σl; θf I- n X ∂f f (σl,j; θf)	(35)
where ∂f logZ(θf) = Eθf[∂f f (σι; θf)].
The first term Eθf [∂∂yf (σι; θf )J in the above equation is analytically intractable and has to be
approximated by MCMC, such as the Langevin dynamics.
σnew = σι- EIEθf (σι) + √∆Γe
2 ∂σl f
=σι + δt " — + X I 誓# + He
2	∂σl	i=1 q(σli) ∂σli
(36)
where e v N(0, I) is a Gaussian white noise. ∆τ denotes the size of the Langevin step.
Then, the Monte Carlo approximation to "L；；f is given as follows.
∂L(θf)
∂θf
∂	1n	1n
∂θf n X Eθf (σlj)- n X Eθf (σlj)
f n j=1	n j=1
(37)
where σel,j is the sample synthesized via Langevin dynamics.
Thus, the loss for the learning of the DNN can be rewritten as follows.
L
Lcomplexity = ->: Eσι [logp^f (σl)]
l=1
Ln
=-XX[E^f(σι,j ) + log Z (θf)]	(38)
l=1 j=1
Ln
=-n XX[f(σ5 θf) + logq(σι,j) - logZ(θf)]
Let θDNN denote parameters in the DNN. The gradient of θDNN can be calculated as follows.
dLoss =	1	1X∙ XX ( df (σl ； θ f )	+ 1X 1	dq(σi,j ) )	dσl,j
dθDNN	n	= j=1 [ dσl,j J q(σi,j) dσi,j d	dθDNN
(39)
We consider Z(θf) as a constant in the computation of
∂Loss
adDNN
To enable the computation of 京；\ and %；：,力,We can approximate the ReLU operation using
the following Swish function (Ramachandran et al., 2017).
σl ≈ sigmoid(βx)
ReLU(x) = x σl ≈ x sigmoid(β x)
where denotes the element-wise multiplication.
(40)
18
Under review as a conference paper at ICLR 2021
According to Eq. (31), the prior distribution q(σl) is approximated as follows. q(σi) ≈1 -p+σi(2p - 1),喀 ≈2p -1	(41)
In implementation, the EBM is a bottom-up ConvNet with six convolutional layers, which takes σl
as an input and outputs a scalar. During the training phase, we firstly train the EBM using Eq. (37)
for every batch of training data. The EBM and the original DNN are trained separately. I.e. when
training the EBM, parameters in the original DNN are fixed, and vice versa.
19