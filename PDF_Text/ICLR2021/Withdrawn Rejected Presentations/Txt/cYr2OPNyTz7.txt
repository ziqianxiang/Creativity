Under review as a conference paper at ICLR 2021
IMPROVING SELF-SUPERVISED PRE-TRAINING via A
Fully-Explored Masked Language Model
Anonymous authors
Paper under double-blind review
Ab stract
Masked Language Model (MLM) framework has been widely adopted for self-
supervised language pre-training. In this paper, we argue that randomly sampled
masks in MLM would lead to undesirably large gradient variance. Thus, we the-
oretically quantify the gradient variance via correlating the gradient covariance
with the Hamming distance between two different masks (given a certain text se-
quence). To reduce the variance due to the sampling of masks, we propose a
fully-explored masking strategy, where a text sequence is divided into a certain
number of non-overlapping segments. Thereafter, the tokens within one segment
are masked for training. We prove, from a theoretical perspective, that the gradi-
ents derived from this new masking schema have a smaller variance and can lead
to more efficient self-supervised training. We conduct extensive experiments on
both continual pre-training and general pre-training from scratch. Empirical re-
sults confirm that this new masking strategy can consistently outperform standard
random masking. Detailed efficiency analysis and ablation studies further validate
the advantages of our fully-explored masking strategy under the MLM framework.
1	Introduction
Large-scale pre-trained language models have attracted tremendous attention recently due to their
impressive empirical performance on a wide variety of NLP tasks. These models typically abstract
semantic information from massive unlabeled corpora in a self-supervised manner. Masked lan-
guage model (MLM) has been widely utilized as the objective for pre-training language models. In
the MLM setup, a certain percentage of words within the input sentence are masked out, and the
model learns useful semantic information by predicting those missing tokens.
Previous work found that the specific masking strategy employed during pre-training plays a vital
role in the effectiveness of the MLM framework (Liu et al., 2019; Joshi et al., 2019; Sun et al.,
2019). Specifically, Sun et al. (2019) introduce entity-level and phrase-level masking strategies,
which incorporate the prior knowledge within a sentence into its masking choice. Moreover, Joshi
et al. (2019) propose to mask out random contiguous spans, instead of tokens, since they can serve
as more challenging targets for the MLM objective.
Although effective, we identify an issue associated with the random sampling procedure of these
masking strategies. Concretely, the difficulty of predicting each masked token varies and is highly
dependent on the choice of the masking tokens. For example, predicting stop words such as “the”
or “a” tends to be easier relative to nouns or rare words. As a result, with the same input sentence,
randomly sampling certain input tokens/spans, as a typical masking recipe, will result in undesirable
large variance while estimating the gradients. It has been widely demonstrated that large gradi-
ent variance typically hurts the training efficiency with stochastic gradient optimization algorithms
(Zhang & Xiao, 2019; Xiao & Zhang, 2014; Johnson & Zhang, 2013). Therefore, we advocate
that obtaining gradients with a smaller variance has the potential to enable more sample-efficient
learning and thus accelerate the self-supervised learning stage.
In this paper, we start by introducing a theoretical framework to quantify the variance while esti-
mating the training gradients. The basic idea is to decompose the total gradient variance into two
terms, where the first term is induced by the data sampling process and the second one relates to
the sampling procedure of masked tokens. Theoretical analysis on the second variance term demon-
strates that it can be minimized by reducing the gradient covariance between two masked sequences.
1
Under review as a conference paper at ICLR 2021
Furthermore, we conduct empirical investigation on the correlation between the gradient’s covari-
ance while utilizing two masked sequences for training and the Hamming distance between these
sequences. We observed that that the gradients’ covariance tends to decrease monotonically w.r.t the
sequences’ Hamming distance.
Inspired by the observations above, we propose a fully-explored masking strategy, which maximizes
the Hamming distance between any of two sampled masks on a fixed text sequence. First, a text
sequence is randomly divided into multiple non-overlapping segments, where each token (e.g. sub-
word, word or span) belongs to one of them. While the model processes this input, several different
training samples are constructed by masking out one of these segments (and leaving the others as
the contexts). In this manner, the gradient w.r.t. this input sequence can be calculated by averaging
the gradients across multiple training samples (produced by the same input sequence). We further
verify, under our theoretical framework, that the gradients obtained with such a scheme tend to have
smaller variance, and thus can improve the efficiency of the pre-training process.
We evaluate the proposed masking strategies on both continued pre-training (Gururangan et al.,
2020) and from-scratch pre-training scenarios. Specifically, Computer Science (CS) and News do-
main corpus (Gururangan et al., 2020) are leveraged to continually pre-train RoBERTa models,
which are then evaluated by fine-tuning on downstream tasks of the corresponding domain. It is
demonstrated that the proposed fully-explored masking strategies lead to pre-trained models with
stronger generalization ability. Even with only a subset of the pre-training corpus utilized in (Gu-
rurangan et al., 2020), our model consistently outperforms reported baselines across four natural
language understanding tasks considered. Besides, we also show the effectiveness of our method on
the pre-training of language models from scratch. Moreover, the comparison between fully-explored
and standard masking strategies in terms of their impacts on the model learning efficiency further
validates the advantages of the proposed method. Extensive ablation studies are further conducted
to explore the robustness of the proposed masking scheme.
2	Related Work
Self-supervised Language Pre-training Self-supervised learning has been demonstrated as a
powerful paradigm for natural language pre-training in recent years. Significant research efforts
have been devoted to improve different aspects of the pre-training recipe, including training objec-
tive (Lewis et al., 2019; Clark et al., 2019; Bao et al., 2020; Liu et al., 2019), architecture design
(Yang et al., 2019; He et al., 2020), the incorporation of external knowledge (Sun et al., 2019; Zhang
et al., 2019), etc. The idea of self-supervised learning has also been extended to generation tasks and
achieves great results (Song et al., 2019; Dong et al., 2019). Although impressive empirical perfor-
mance has been shown, relatively little attention has been paid to the efficiency of the pre-training
stage. ELECTRA (Clark et al., 2019) introduced a discriminative objective that is defined over all
input tokens. Besides, it has been showed that incorporating language structures (Wang et al., 2019)
or external knowledge (Sun et al., 2019; Zhang et al., 2019) into pre-training could also help the
language models to better abstract useful information from unlabeled samples.
In this work, we approach the training efficiency issue from a different perspective, and argue that
the masking strategies, as an essential component within the MLM framework, plays a vital role
especially in efficient pre-training. Notably, our fully-explored masking strategies can be easily
combined with different model architectures for MLM training. Moreover, the proposed approach
can be flexibly integrated with various tokenization choices, such as subword, word or span (Joshi
et al., 2019). A concurrent work Chen et al. (2020) also shared similar motivation as this work,
although they have a different solution and their method requires additional computation to generate
the masks, and yet is outperformed by the proposed fully-explored masking (see Table 2).
Domain-specific Continual Pre-training The models mentioned above typically abstract seman-
tic information from massive, heterogeneous corpora. Consequently, these models are not tailored
to any specific domains, which tends to be suboptimal if there is a domain of interest beforehand.
Gururangan et al. (2020) showed that continual pre-training (on top of general-purpose LMs) with
in-domain unlabeled data could bring further gains to downstream tasks (of that particular domain).
One challenge inherent in continual pre-training is that in-domain data are usually much more lim-
ited, compared to domain-invariant corpora. As a result, how to efficiently digest information from
unlabeled corpus is especially critical while adapting large pre-trained language models to specific
2
Under review as a conference paper at ICLR 2021
Input
Splitting
#1
#2
Figure 1: Illustration of the proposed fully-explored masking strategy with a specific example. In
this case, the input sequence has been divided into 4 exclusive segments, where different colors
indicate which segment a certain token belongs to.
domains. To this end, we specifically consider the continual pre-training scenario to evaluate the
effectiveness of our approach.
3	Proposed Approach
In this section, we first review the MLM framework that is widely employed for natural language
pre-training. Motivated by the gradient variance analysis of MLM in section 3.2, we present the
fully-explored masking strategy, which serves as a simple yet effective solution to reduce the gra-
dient variance during training. Connections between our method and variance reduction theory are
further drawn, which provides a theoretical foundation for the effectiveness of the proposed strategy.
Finally, some specific implementation details are discussed.
3.1	Background: the MLM framework
Let V denote the token vocabulary and x = (x1, . . . , xn) denote a sentence of n tokens, where
xi ∈ V for i = 1, . . . , n. Let m = (m1 , . . . , mn ) denote a binary vector of length n, where
mi ∈ {0, 1}, representing the mask over a sentence. Specifically, mi = 1 means the token xi is
masked and mi = 0 if xi is not masked. We use m ◦ x to denote a masked sentence, that is,
(m ◦ x)i
[MASK]
xi
if mi = 1,
if mi = 0.
In addition, let m be the complement of m; in other words, m√ = 0 if m√ = 1 and mi = 1 if
mi = 0. Naturally, m ◦ X denotes a sentence with the complement mask m.
For a typical language model with parameters θ, its loss function over a sentence x ∈ Vn and a
mask m ∈ {0, 1}n as
'(θ; x, m) = — log P(m ◦ x | θ, m ◦ x) = — ɪ2 log P(Xi | θ, m ◦ x),	(1)
i : mi=1
where P (xi | θ, m ◦ x) is the probability of the model correctly predicting xi given the masked
sentence m ◦ x. Ifmi = 0, it always has P(xi | θ, m ◦ x) = 1 as the ground-truth xi is not masked.
We will focus on masks with a fixed length. Let τ be an integer satisfying 0 ≤ τ ≤ n. The set of
possible masks of length τ is defined as M(τ) ,
M(T) = {m ∈ {0,1}n | Pn=I mi = T},
which has a cardinality ∣M(τ)| = O = T!(n!_T了. Therefore, the average loss function over a
sentence x with masks of length T is,
L(θ; x) = Em〜Unif(M(T))'(θ;x, m) = γny	X `(θ; x, m).	(2)
T m∈M(T)
Let’s consider PD as the probability distribution of sentence in a corpus D ⊂ Vn . The overall loss
function for training the masked language model over corpus D is
L(θ)，Ex〜PDL(θ; x) = Ex〜PDEm〜Unif(M(T))'(θ; x, m) .	(3)
3
Under review as a conference paper at ICLR 2021
During each step of the training process, it randomly samples a mini-batch of sentences St ⊂ D. For
each x ∈ St, we randomly pick a subset of masks Kt (x) ⊂ M(τ), independently across different x.
Thus, the mini-batch stochastic gradient is
gt(θ) = 1 X KK X	Vθ'(θ; x, m).	(4)
x∈St	m∈Kt (x)
where |St| = S and ∣Kt(x)∣ = K for all t. Clearly We have E[gt(θ)] = VL(θ). In the following
sections, it first gives the variance of gt(θ) which is an important factor to influence model training
efficiency (Xiao & Zhang, 2014; Zhang & Xiao, 2019), and then it presents the proposed fully-
explored masking strategy to help reduce the gradient variance of the masked language model.
3.2	Analysis: Gradient Variance of MLM
According to the law of total variance (Weiss, 2005), the variance of the mini-batch stochastic gra-
dient VarSt,Kt (gt) can be decomposed as follows,
VarSt,Kt (gt) = Est [VarKt (gt) | St] + VarSt(EKt [gt | St)]),	⑸
where for simplicity gt indicates gt (θ) as in eqn. 4, the first term captures the variance due to the
sampling of masks, and the second term is the variance due to the sampling of mini-batch sentences.
In this work, we focus on the analysis of the first term in eqn. 5: the variance due to the sampling of
masks. Denote g(m) = V'(θ; x, m) for any fixed sentence x. Consider a subset of random masks
(m1, . . . , mK) and the K-masks gradient is defined as the average of them:
1
g(mι,..., mκ) = Kfg(mk).	(6)
k=1
Theorem 1. The Variance of K -masks gradient: Var g(m1, . . . , mK) is
-1Var(g(mι)) +(1 — K) Cov(g(m1),g(m2)).	(7)
where,	Cov(g(m1),g(m2)) = E [(g(mι)-g)T (g(m2) — g),	(8)
and	g = Em 〜Unif(M(T ))g(m) = Tny X g(m).	⑼
τ m∈M(τ)
The detailed proof of Theorem 1 is given in Appendix A.1. In the theorem 1, it indicates that
the variance of K-masks gradient can be reduced by decreasing the gradient covariance between
different masks.
3.3	Variance Reduction: Fully-Explored Masking
Intuitively, if we consider the two random masks m1 and m2 are totally overlapped, the gradient
covariance between them should be maximized. It motivates us to consider the correlation between
gradient covariance and Hamming distance between these two masks. Thus, we have the following
assumption:
Assumption 1. The covariance Cov g(m1), g(m2) is monotone decreasing in term of the Ham-
ming distance between m1 and m2.
To verify the Assumption 1, we sample a small set of CS domain sentences from S2ORC dataset
(Gururangan et al., 2020) as the fixed mini-batch for our analysis, then calculate gradient covari-
ance Cov g(m1), g(m2) of mask pairs (m1, m2) with different Hamming distances H(m1, m2)
using this mini-batch. In Figure 2, the center of gradient covariance distribution is shifting to left
(lower value) as Hamming distance increases. In Figure 3, we also observe that the average gradient
covariance is decreasing in term od Hamming distance. As shown in Figure 2, 3, Assumption 1
holds for both RoBERTa-base model (Liu et al., 2019) and RoBERTa-base model after continually
pre-trained on CS domain corpus.
4
Under review as a conference paper at ICLR 2021
Figure 2: The distributions of gradient covariance Cov g(m1), g(m2) for different Hamming dis-
tances H(m1, m2) based on a small CS domain corpus. Left: gradient covariance distribution of
selected parameters in RoBERTa-base model; Right: gradient covariance distribution of selected
parameters in RoBERTa-base model after continually pre-trained on CS domain corpus.
⅛φ⅛2ω φcteφ><
⅛φ⅛2ω φcteφ><
Hamming Distance H(m1,m2)	Hamming Distance H(m1,m2)
Figure 3: Empirical analysis of the correlation between gradient covariance Cov g(m1), g(m2)
and Hamming distance H(m1, m2) based on a small CS domain corpus. For a sequence of length
512, two masks m1, m2 are randomly sampled with 128 masked tokens, their Hamming distance
satisfying 0 ≤ H(m1, m2) ≤ 256. Left: gradient CoVariancecalcUlatedbasedonRoBERTa-base
model; Right: gradient covariance calculated based on RoBERTa-base model after continually pre-
trained on CS domain corpus.
We propose the fully-explored masking strategy which restricts masks sampled from M(τ) to be
non-overlapping, denoted as MFE(τ) for simplicity:
mi,..., mκ 〜MFE (T), ∀i=j H (mi, mj) = 2τ	(10)
With the fully-explored masking strategy, it can be easily approved that expectation of gradient over
MFE(τ) is an unbiased estimation of the expectation of gradient over M(τ) as in the Lemma 2. In
the Lemma 3, it states that the Theorem 1 is still hold for fully-explored masking strategy, which
indicates that the variance of K-masks gradient can be reduced by restricting the masks sampling
from MFE (τ).
Lemma 2. The expectation of gradient over MFE(τ) equals to the expectation of gradient over
M(τ).
Proof. The joint distributions of (m1, . . . , mK) sampling from MFE(τ) is different from the i.i.d.
case due to the non-overlapping restriction. However, the marginal distributions of mk are still the
same uniform distribution over M(T). Therefore, We still have E [g(mk)] = g, ∀k=1,..,K and as a
consequence E [g(m1,..., mκ)] = g.	□
Lemma 3. The derivation of K-masks gradient variance in Eqn.7 holds for both MFE(T) and
M(T).
The detailed proof of Lemma 3 can be seen in Appendix A.2.
3.4	Implementation Details
The details of fully-explored masking algorithm is illustrated in Algorithm 1. In practice, a text
sequence Si is tokenized into subword pieces (Devlin et al., 2018) with the maximum sequence
5
Under review as a conference paper at ICLR 2021
length n set as 512 in the experiments. To understand the performance of fully-explored masking
strategy at different granularity, the text sequence Si is masked at both subword level (Devlin et al.,
2018; Liu et al., 2019) and span level (Joshi et al., 2019; Wang et al., 2019). The details about other
hyperparameters, i.e., masking-ratio and number of splits K will be discussed in experiment section.
Algorithm 1: Fully-explored Masking Language Model
Input: Language corpus D = {Sι,…,ST}, |Si| = n; Masking ratio T； Number of sampling
masks K, where K * T ≤ 1; Initial model parameters θ0;
Output: model parameters θ*
foreach Si ∈ S do
Sample K split masking vectors (m1, ..., mK) from MFE(τ) as in Eqn.10.
Calculate the gradient g(m1, ..., mK) as in Eqn. 6.
Update model parameters θi+1 = Optimizer(θi, g(m1, ..., mK))
end
return θ* = θτ
4	Experiments
In this section, we evaluate the proposed fully-explored masking strategy for natural language pre-
training in two distinct settings: i) continual pre-training, where a given pre-trained model is further
adapted leveraging domain-specific unlabeled corpus; ii) pre-training from scratch, where large-
scale corpus such as Wikipedia and BookCorpus are employed to pre-train a model from the be-
ginning. We also compare the training efficiency of FE-MLM and MLM frameworks to validate
our theoretical findings. Ablation studies and analysis are further conducted regarding the proposed
approach.
4.1	Experimental Settings
For the continual pre-training scenario, we consider unlabeled corpus from two different domains,
i.e., computer science (CS) papers and news text from RealNews, introduced by Gururangan et al.
(2020). As to the downstream tasks, ACL-ARC citation intent Jurgens et al. (2018) and SciERC
relation classification Luan et al. (2018) are utilized for the CS domain. While for the News domain,
HyperPartisan news detection Kiesel et al. (2019) and AGNews Zhang et al. (2015) are employed to
facilitate the comparison with Gururangan et al. (2020).
Following (Gururangan et al., 2020) for a fair comparison, RoBERTa Liu et al. (2019) is leveraged
as the initial model for continual pre-training, where the same training objective is optimized on
the domain-specific corpus. We choose a batch size of 48, and the model is trained using Adam
Kingma & Ba (2014), with a learning rate of 1 × 10-4. It is worth noting that we observe, in
our initial experiments, that downsampling only 72k documents from the total of 2.22M used by
Gururangan et al. (2020) can result in similar performance on downstream tasks. This happens in
the News domain as well, where we randomly sample 623k documents out of 11.90M. The model is
continually pre-trained for around 40k and 20k steps on the CS and News domain, respectively. One
important hyerparameter under the FE-MLM framework is the number of split the input sequence
is divided into, where we use 4 as the default setting. The sensitivity of the proposed algorithm w.r.t
this hyperparameter is further investigated (see Figure 4).
For the general pre-training experiments, we employ BERT as the baseline model. Wikiepdia and
BookCorpus (Zhu et al., 2015) are used as the pre-training corpus, with a total size of 16G. We adopt
the same tokenization (i.e., WordPiece embeddings) as BERT, which consists of 30,522 tokens in
the vocabulary. The model is optimized using Adam with the learning rate set as 1 × 10-4. A batch
size of 256 is employed, and we train the model for 1M step. The resulting model is evaluated
on the GLUE benchmark (Wang et al., 2018), which comprises 9 natural language understanding
(NLU) tasks such as textual entailment (MNLI, RTE), question-answer entailment (QNLI), question
paraphrase (QQP), paraphrase (MRPC), sentimnt analysis (SST-2), linguistic acceptability (CoLA)
and textual similarity (STS-B). The HuggingFace codebase1 is used in our implementation for both
settings.
1https://github.com/huggingface/transformers
6
Under review as a conference paper at ICLR 2021
Model		ACL-ARC	SciERC	HyperPartisan	AGNews
RoBERTa (Gururangan et al., 2020)	63.0 ± 5.8	77.3 ± 1.9	-86.6 ± 0.9-	93.9 ± 0.2
DAPT (Gururangan et al., 2020)	75.4 ± 2.5	80.8 ± 1.5	88.2 ± 5.9	93.9 ± 0.2
MLM + SubWord (our implementation)	75.34 ± 2.54	81.51 ± 1.05	91.00 ± 2.66	94.05 ± 0.16
FE-MLM + SubWord		76.24 ± 1.86	82.40±0.86	92.35 ± 3.49	94.02 ± 0.09
MLM + Span (our implementation)	76.63 ± 1.65	81.33 ± 1.16	91.72 ± 3.26	93.94 ± 0.05
FE-MLM + Span	78.06±2.31	81.99 ± 0.79	93.22±3.31	94.13±0.04
Table 1: The empirical results on continual pre-training setting, where RoBERTa and DAPT
(RoBERTa continually pre-trained with the standard MLM objective) is leveraged as our baseline
to facilitate comparison with (Gururangan et al., 2020). Specifically, ACL-ARC and SciERC are
evaluated with the continually pre-trained model with CS domain corpus, while HyperPartisann and
AGNews are based upon models trained with News domain corpus.
Model	MNLI-m/mm	SST-2	QNLI	QQP	RTE	MRPC	CoLA	STS-B
-BERT (MLM)-	84.37/84.85	92.78	91.01	91.09	63.54	87.01	59.65	87.89
BERT (FE-MLM)	85.09/84.63	93.23	91.01	91.16	68.59	87.99	61.32	89.51
Table 2: The results on the dev sets of GLUE benchmarks, where MLM and FE-MLM are compared
with the BERT-base model as the testbed.
4.2	Experimental Results
Continual Pre-training Evaluation We applied our fully-explored MLM framework to both sub-
word and span masking scenarios. The results for the RoEBRTa model continually pre-trained on
the CS and News domains are presented in Table 1. It can be observed that the continual pre-training
stage can benefit the downstream tasks on both domains(compared with fine-tuning the RoBERTa
model directly). Besides, the baseline numbers based on our implementation is on par with or even
better than those reported in Gururangan et al. (2020), even though we downsample the original
unlabeled corpus (as described in the previous section).
More importantly, in the subword masking case, our FE-MLM framework consistently exhibits bet-
ter empirical results on the downstream tasks. Note that to ensure fair comparison, the same com-
putation is taken for both MLM and FE-LMLM training. This indicates that the models pre-trained
using the FE-MLM approach have been endowed with stronger generalization ability, relative to
standard MLM training. Similar trend is also observed in the span masking experiments, demon-
strating that the proposed method can be naturally and flexibly integrated with different masking
schemes. Besides, we found that subword masking tends to work better than span masking in the
CS domain, whereas the opposite is true as to the News domain. This may be attributed to the
different nature of the unlabeled corpus from two domains.
General Pre-training Evaluation We also
evaluate the FE-MLM framework on the pre-
training experiments with general-purpose unla-
beled corpus. Specifically, we follow the same
setting as BERT, except that the proposed fully-
explored masking strategy is applied (the same
amount of computation is used for the baseline
and our method). The corresponding results are
shown in Table 2. It can be found that the FE-
MLM approach, while fine-tuned on the GLUE
benchmark, exhibits better results on 7 out of
Model	Avg. Score
ELMo, (Peters etal., 2018)	712
GPT, (Radford et al.,2018)	78.8
BERT-base, (Devlin et al., 2018)	82.2
BERT-base (ReEval)	82.5
MAP-Net, (Chen et al., 2020)	82.1
BERT-base (FE-MLM)	836
Table 3: The comparison between the FE-MLM
model with several baseline methods, based on
the averaged score (on the dev set) across differ-
9 NLU datasets than the MLM baseline. This
demonstrates the wide applicability of the pro- ent tasks from the GLUE benchmark.
posed FE-MLM framework across different pre-training settings.
We further compare the averaged score over 9 GLUE datasets with other methods, and the numbers
are summarized in Table 3. It is worth noting that the BERT-based (ReEval) baseline is obtained by
fine-tuning the BERT model released by Devlin et al. (2018) on each GLUE datasets, with the results
on the dev sets averaged. Another BERT-base number is reported by Clark et al. (2019), which is
pretty similar our re-evaluation one. MAsk proposal network (MAP-Net) is proposed by Chen et al.
7
Under review as a conference paper at ICLR 2021
(2020), which shares the same motivation of reducing the gradient variance during the masking
stage. However, we approach the problem with a distinct strategy based upon extensive theoretical
analysis. We found that BERT-base model improved with the FE-MLM training significantly out-
perform BERT-base model and Mask Proposal Network, further demonstrating the effectiveness of
proposed approach.
4.3	Ablation Studies and Analysis
Training Steps (Ie3)
Figure 4: Left: the efficiency comparison between the standard MLM and fully-explored MLM
approach. Specifically, the RoBERTa-base model and continual pre-training setting (on the CS do-
main) are employed. The corresponding models are evaluated on the ACL-ARC dataset (at different
training steps). Right: the effect of split number (under the FE-MLM framework) on the generaliza-
tion ability of pre-trained models, evaluated on two datasets in the CS domain.
Training Efficiency Although previous results has demonstrated that our model at the end of
pre-training process exhibits stronger generalization ability, it is still unclear how the proposed FE-
MLM framework influence the training efficiency during training. In this regard, We examine the
intermediate models obtained with both MLM and FE-MLM training by fine-tuning and evaluating
them on the ACL-ARC dataset. Specifically, the RoBERTa-base setting is used here, which is
continually pre-trained on the unlabeled corpus from the CS domain. As shown on the left side of
Figure 4, FE-MLM beats MLM at different steps of pre-training. More importantly, the performance
of the FE-MLM model improves much faster at the early stage (i.e., less than around 15,000 steps),
indicating that the model is able to extract useful semantic information (from unlabeled corpus) more
efficiently with the proposed masking strategy. This observation further highlights the advantage and
importance of reducing gradient variance under the MLM framework.
The Effect of Masking Split Number The number of masking split the input sentence is divided
into is a vital hyperparameter for the FE-MLM approach. Therefore, we investigate its impact
on the performance of resulting models. Concretely, the setting of continual pre-training on the
CS domain is employed, where the RoBERTa model is pre-trained with the FE-MLM objective.
Different split number is explored, including 2,4, 6,8, and 12.5% of allthe tokens are masked within
each split. The results are visualized on the right side ofFigure 4. We found that the downstream task
performance (on both ACL-ARC and SCiERC datasets) is fairly stable w.r.t. different split numbers.
This may relate to our non-overlapping sampling strategy, which helps the model to explore various
position in the sentence as efficiently as possible, so that the model exhibits strong performance even
with only two splits.
5	Conclusion
In this paper, we identified that under the MLM framework, the procedure of randomly sampling
masked tokens will give rise to undesirably large variance while estimating the training gradients.
Therefore, we introduced a theoretical framework to quantify the gradient variance, where the con-
nection between gradient covariance and the Hamming distance between two different masked se-
quences are drawn. Motivated by these observations, we proposed a fully-explored masking strategy,
where a text sequence is divided into multiple non-overlapping segments. During training, all tokens
in one segment are masked out, and the model is asked to predict them with the other segments as
the context. It was demonstrated theoretically that the gradients obtained with such a novel masking
strategy have a smaller variance, thus enabling more efficient pre-training. Extensive experiments on
both continual pre-training and general pre-training from scratch showed that the proposed masking
strategy consistently outperforms standard random masking.
8
Under review as a conference paper at ICLR 2021
References
Karim Ahmed, N. Keskar, and R. Socher. Weighted transformer network for machine translation.
ArXiv, abs/1711.02132, 2017.
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance
reduction in sgd by distributed importance sampling. arXiv preprint arXiv:1511.06481, 2015.
Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint
arXiv:1908.10063, 2019.
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao,
Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for unified language
model pre-training. arXiv preprint arXiv:2002.12804, 2020.
Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text.
arXiv preprint arXiv:1903.10676, 2019.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Liang Chen, Tianyuan Zhang, Di He, Guolin Ke, Liwei Wang, and Tie-Yan Liu. Variance-reduced
language pretraining via a mask proposal network. arXiv preprint arXiv:2008.05333, 2020.
Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,
and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding
and generation. In Advances in Neural Information Processing Systems, pp. 13063-13075, 2019.
Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun. Train no evil: Selec-
tive masking for task-guided pre-training. arXiv preprint arXiv:2004.09733, 2020.
SUchin Gururangan, Ana Marasovic, SWabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. arXiv
preprint arXiv:2004.10964, 2020.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy.
Spanbert: Improving pre-training by representing and predicting spans. arXiv preprint
arXiv:1907.10529, 2019.
David Jurgens, Srijan Kumar, Raine Hoover, Daniel A. McFarland, and Dan Jurafsky. Measuring
the evolution of a scientific field through citation frames. Transactions of the Association for
Computational Linguistics, 6:391-406, 2018.
Johannes Kiesel, Maria Mestre, Rishabh Shukla, Emmanuel Vincent, Payam Adineh, David Cor-
ney, Benno Stein, and Martin Potthast. Semeval-2019 task 4: Hyperpartisan news detection. In
Proceedings of the 13th International Workshop on Semantic Evaluation, pp. 829-839, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
9
Under review as a conference paper at ICLR 2021
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-
woo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text
mining. Bioinformatics, 36(4):1234-1240, 2020.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. arXiv preprint
arXiv:1910.13461, 2019.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld. S2orc: The semantic
scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 4969-4983, 2020.
Yi Luan, Luheng He, M. Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities,
relations, and coreference for scientific knowledge graph construction. In EMNLP, 2018.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,
2018.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
K. Song, X. Tan, T. Qin, Jianfeng Lu, and T. Liu. Mass: Masked sequence to sequence pre-training
for language generation. In ICML, 2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,
Hao Tian, and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv
preprint arXiv:1904.09223, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018.
Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pp. 181-189,
2013.
Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Liwei Peng, and Luo Si. Structbert: Incor-
porating language structures into pre-training for deep language understanding. arXiv preprint
arXiv:1908.04577, 2019.
10
Under review as a conference paper at ICLR 2021
Neil A. Weiss. A course in probability. pp. 385-385, 2005.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion, 2014.
Yi Yang, Mark Christopher Siy UY, and Allen Huang. Finbert: A pretrained language model for
financial communications. arXiv preprint arXiv:2006.08097, 2020.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural
information processing systems, pp. 5754-5764, 2019.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending against neural fake news. In Advances in Neural Information Processing
Systems, pp. 9054-9065, 2019.
Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance
reduction, 2019.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
sification. In Advances in neural information processing systems, pp. 649-657, 2015.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced
language representation with informative entities. ACL, 2019.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching
movies and reading books. In Proceedings of the IEEE international conference on computer
vision, pp. 19-27, 2015.
A Appendix
A.1 Proof of Theorem 1
Proof.
Var(g(mι,..., mκ)) = E [∣∣g(mι,..., mκ) - g∣∣2]
=E IlKPK=Ig(mk)-g∣∣]
=KE IlPK=i(g(mk)-川]
1	-K
=KE E l∣g(m⅛)- gk2 + £(g(mk)- g)τ(g(mι)- g)
k=1	k6=l
1 /K	K	∖
=K EVar (g(mk)) +^Cov(g(mk ),g(mι))
k=1	k6=l
where for each pair k 6= l,
Cov(g(mk ),g(mι)) = E [(g(mk) - g)T (g(mι) -川，
Since m1, . . . , mK are i.i.d. samples from the uniform distribution over M(τ), we have
Var (g(mι))=…=Var (g(mK))
Cov g(mk),g(ml) = Cov g(m1), g(m2) ,∀k 6= l.
Therefore we have the following variance decomposition:
Var(g(mι,..., mχ)) = "var(g(mι)) + (1 -	Cov(g(mQ, g(m2)).
(11)
(12)
(13)
(14)
□
11
Under review as a conference paper at ICLR 2021
A.2 Proof of Lemma 3
Proof. The joint distribution of the pairs (mk, ml) sampling from MFE(τ) are different from the
i.i.d. case, it can be shown (by symmetry) that the identity equation 13 also holds. Considering the
fact that the derivation in equation 11 holds for any sampling strategy, we conclude that the variance
decomposition in equation 14 still holds.	□
12