Under review as a conference paper at ICLR 2021
Learning One-hidden-layer Neural Networks
on Gaussian Mixture Models with Guaran-
teed Generalizability
Anonymous authors
Paper under double-blind review
Ab stract
We analyze the learning problem of fully connected neural networks with the
sigmoid activation function for binary classification in the teacher-student setup,
where the outputs are assumed to be generated by a ground-truth teacher neural
network with unknown parameters, and the learning objective is to estimate the
teacher network model by minimizing a non-convex cross-entropy risk function
of the training data over a student neural network. This paper analyzes a general
and practical scenario that the input features follow a Gaussian mixture model of a
finite number of Gaussian distributions of various mean and variance. We propose
a gradient descent algorithm with a tensor initialization approach and show that
our algorithm converges linearly to a critical point that has a diminishing distance
to the ground-truth model with guaranteed generalizability. We characterize the
required number of samples for successful convergence, referred to as the sample
complexity, as a function of the parameters of the Gaussian mixture model. We
prove analytically that when any mean or variance in the mixture model is large, or
when all variances are close to zero, the sample complexity increases, and the con-
vergence slows down, indicating a more challenging learning problem. Although
focusing on one-hidden-layer neural networks, to the best of our knowledge, this
paper provides the first explicit characterization of the impact of the parameters of
the input distributions on the sample complexity and learning rate.
1	Introduction
Deep neural networks (LeCun et al., 2015) have demonstrated superior empirical performance in
various applications such as speech recognition (Krizhevsky et al., 2012) and computer vision
(Graves et al., 2013; He et al., 2016). Despite the numerical success, the theoretical underpin of
learning neural networks is much less investigated. One bottleneck for the wide acceptance of deep
learning in critical applications is the lack of the theoretical generalization guarantees, i.e., why a
model learned from the training data would achieve a high accuracy on the testing data.
This paper studies the generalization performance of neural networks in the “teacher-student” setup,
where the training data are generated by a teacher neural network, and the learning is performed on a
student network by minimizing the empirical risk of the training data. This teacher-student setup has
been studied in the statistical learning community for a long time (Engel & Broeck, 2001; Seung
et al., 1992) and applied to neural networks recently (Goldt et al., 2019a; Zhong et al., 2017b;a;
Zhang et al., 2019; 2020b; Fu et al., 2020; Zhang et al., 2020a). Assuming that the student network
has the same architecture as the teacher network, the existing generalization analyses mostly focus
on one-hidden-layer networks, because the optimization problem is already nonconvex, and the
analytical complexity increases tremendously when the number of hidden layers increases.
One critical assumption of most works in this line is that the input features follow the standard Gaus-
sian distribution. Although other distributions are considered in (Du et al., 2017; Ghorbani et al.,
2020; Goldt et al., 2019b; Li & Liang, 2018; Mei et al., 2018b; Mignacco et al., 2020; Yoshida &
Okada, 2019), the generalization performance beyond the standard Gaussian input is less investi-
gated. On the other hand, the learning performance clearly depends on the input data distribution.
(LeCun et al., 1998) states that the learning method converges faster if the inputs are whitened to
1
Under review as a conference paper at ICLR 2021
be the standard Gaussian. Batch normalization (Ioffe & Szegedy, 2015) modifies the mean and
variance in each layer and is a popular practical method to achieve a fast and stable convergence.
Various explanations such as (Bjorck et al., 2018; Chai et al., 2020; Santurkar et al., 2018) have been
proposed to explain the enormous success of Batch normalization, but little consensus exists on the
exact mechanism.
Contributions: This paper provides a theoretical analysis of learning one-hidden-layer neural net-
works when the input distribution follows a Gaussian mixture model containing an arbitrary number
of Gaussian distributions with arbitrary mean and variance. The Gaussian mixture model has been
employed in many applications such as data clustering and unsupervised learning (Dasgupta, 1999;
Figueiredo & Jain, 2002; Jain, 2010), and image classification and segmentation (Permuter et al.,
2006). The parameters of the mixture model can be estimated from data by the EM algorithm
(Redner & Walker, 1984) or the moment-based method (Hsu & Kakade, 2013), with theoretical
performance guarantees, see, e.g., (Ho & Nguyen, 2016; Ho et al., 2020; Dwivedi et al., 2020a;b).
For the binary classification problem with the cross entropy loss function, this paper proposes a
gradient descent algorithm with tensor initialization to estimate the weights of the one-hidden-layer
fully-connected neural network. Our algorithm converges to a critical point linearly, and the returned
critical point converges to the ground-truth model at a rate of VZd log n/n, where d is the dimension
of the feature, and n is the number of samples. We also characterize the required number of samples
for accurate estimation, referred to as the sample complexity, as a function of d, the number of
neurons K, and the input distribution. Our explicit bounds imply (1) when the absolute value of any
mean in the Gaussian mixture model increases from zero, the sample complexity increases, and the
algorithm converges slower, indicating that it will be more challenging to learn a model with a small
test error; (2) The same phenomenon happens when any variance in the mixture model increases
to infinity from a certain positive value, or if all the variances in the mixture model approach zero.
Our results indicate that the training converges faster and requires a less number of samples if the
input data are zero mean with a certain non-zero variance. This can be viewed as one theoretical
explanation in one-hidden-layer for the success of Batch normalization. Moreover, to the best of
our knowledge, this paper provides the first theoretical and explicit characterization about how the
mean and variance of the input distribution affect the sample complexity and learning rate.
1.1	Related Work
Learning over-parameterized neural networks. One line of theoretical research on the learning
performance considers the over-parameterized setting where the number of network parameters is
greater than the number of training samples. (Bousquet & Elisseeff, 2002; Hardt et al., 2016; Keskar
et al., 2016; Livni et al., 2014; Neyshabur et al., 2017; Rumelhart et al., 1988; Soltanolkotabi et al.,
2018; Allen-Zhu et al., 2019a). (Allen-Zhu et al., 2019b; Du et al., 2019; Zou & Gu, 2019) show
the deep neural networks can fit all training samples in polynomial time. The optimization problem
has no spurious local minima (Livni et al., 2014; Zhang et al., 2016; Soltanolkotabi et al., 2018),
and the global minimum of the empirical risk function can be obtained by gradient descent (Li &
Yuan, 2017; Du et al., 2018b; Zou et al., 2020). Although the returned model can achieve a zero
training error, these works do not discuss whether it achieves a small test error or not. (Allen-Zhu
et al., 2019a; Li & Liang, 2018) analyze the generalization error by characterizing the training error
and test error separately. Still, there is no guarantee that a learned model with a small training error
would have a small test error. (Cao & Gu, 2019) provides the bounds of the generalization error
of the learned model by stochastic gradient descent (SGD) in deep neural networks, based on the
assumption that there exists a good model with a small test error around the initialization of the SGD
algorithm, and no discussion is provided about how to find such an initialization. In contrast, our
tensor initialization method in this paper provides an initialization that is close to the ground-truth
teacher model such that our algorithm can find this model with a zero test error.
Generalization performance with the standard Gaussian input. In the teacher-student setup
of one-hidden-layer neural networks, (Brutzkus & Globerson, 2017; Du et al., 2018a; Ge et al.,
2018; Liang et al., 2018; Li & Yuan, 2017; Shamir, 2018; Safran & Shamir, 2018; Tian, 2017)
consider the ideal case ofan infinite number of training samples so that the training and test accuracy
coincide and can be analyzed simultaneously. When the number of training samples is finite, (Zhong
et al., 2017b;a) characterize the sample complexity, i.e., the required number of samples, of learning
one-hidden-layer fully connected neural networks with smooth activation functions and propose a
2
Under review as a conference paper at ICLR 2021
gradient descent algorithm that converges to the ground-truth model linearly. (Zhang et al., 2019;
2020b) extend the analyses to the non-smooth ReLU for fully-connected and convolutional neural
networks, respectively. (Zhang et al., 2020a) analyzes the generalizability of graph neural networks
for both regression and binary classification problems. (Fu et al., 2020) analyzes the cross entropy
loss function for binary classification problems. Compared with other common loss functions such
as the squared loss, the cross entropy loss function is harder to analyze due to the complicated forms
and the saturation phenomenon of its Gradient and Hessian (Fu et al., 2020).
Theoretical characterization of learning performance from other input distributions. (Du et al.,
2017) considers rotationally invariant distributions, but the results only apply to a perceptron (i.e.,
a single-node network). (Mei et al., 2018b) analyzes the generalization error of one-hidden-layer
neural networks in the mean-field limit trained on a large class of distributions, including a mix-
ture of Gaussian distributions with the same mean. The results only hold in the high-dimensional
region where both the number of neurons K and the input dimension d are sufficiently large, and
no sample complexity analysis is provided. (Li & Liang, 2018) studies the generalization error of
over-parameterized one-hidden-layer networks when the data come from mixtures of well-separated
distribution, but the separation requirement excludes Gaussian distributions and Gaussian mixture
models. (Yoshida & Okada, 2019) analyzes the Plateau Phenomenon that the decrease of the risk
slows down significantly partway and speeds up again in one-hidden-layer neural networks with in-
puts drawn from a single Gaussian with an arbitrary covariance. (Goldt et al., 2019b; 2020) analyze
the dynamics of learning one-hidden-layer networks with SGD when the inputs are drawn from a
wide class of generative models. (Mignacco et al., 2020) provides analytical equations for SGD
evolution in a perceptron trained on the Gaussian mixture model. (Ghorbani et al., 2020) considers
inputs with low-dimensional structures and compares neural networks with kernel methods.
Notations: Vectors are in bold lowercase, matrices and tensors in are bold uppercase. Scalars are
in normal fonts. For instance, Z is a matrix, and z is a vector. zi denotes the i-th entry of z, and
Zi,j denotes the (i, j)-th entry of Z. [K] (K > 0) denotes the set including integers from 1 to
K. Id ∈ Rd×d and ei represent the identity matrix in Rd×d and the i-th standard basis vector,
respectively. We use δi(Z) to denote the i-th largest singular value of Z. A 0 means A is a
positive semi-definite (PSD) matrix. The gradient and the Hessian of a function f(W) are denoted
by Vf (W) and V1 2f (W), respectively. The outer product of vectors Zi ∈ Rni, i ∈ [l], is defined
as T = zι ③…X Zl ∈ Rn1×…×nl with T¾…j = (zι)jι …(zι)jι.
Given a tensor T ∈ Rn1×n2×n3 and matrices A ∈ Rn1×d1, B ∈ Rn2×d2, C ∈ Rn3×d3, the
(i1, i2, i3)-th entry of the tensor T(A, B, C) is given by
n1 n2 n3
Ti01,i02,i03Ai01,i1Bi02,i2Ci03,i3.	(1)
i01 i02 i03
We follow the convention that f (x) = O(g(χ)) (or Ω(g(x), Θ(g(x))) means that f (x) increases at
most, at least, or in the order of g(x), respectively.
2	Problem formulation
We consider a one-hidden-layer fully connected neural network where all the weights in the second
layer have the same fixed value. This structure is also known as the committee machine, see, e.g.,
(Aubin et al., 2018; Monasson & Zecchina, 1995; Schwarze & Hertz, 1992; 1993). Let x ∈ Rd
denote the input features. Let K ≥ 1 be the number of neurons in the hidden layer. Following the
teacher-student setup, see e.g., (Fu et al., 2020), the output labels are generated by a teacher neural
network with unknown ground-truth weights Wj ∈ Rd (j ∈ [K ]). Let W * = [wj,…，WK ] ∈
Rd×K contain all the weights. Let δi(W *) denote the i-th largest singular value of W *. Let
K = δ1W*)), and define ηι = QK=ι *((W*(. The nonlinear activation function here is the sigmoid
function φ(x) = ι+eχp(-χ). We consider binary classification, and the binary output y is generated
by the teacher committee machine through
1K
P(y = 1|x) = H (W , x) := KEΦ(Wj x).	(2)
j=1
3
Under review as a conference paper at ICLR 2021
Learning is performed over a student neural network that has the same architecture as the teacher
network, and its weights are denoted by W ∈ Rd×K. Given n pairs of training samples {xi, yi}in=1,
the empirical risk function is
1n
fn(w ) = n ∑'(w ； My，)	⑶
i=1
where '(W; x，, y，) is the cross-entropy loss function, i.e.,
'(W; χi,yi) = -yi ∙ iog(H(W, x，)) 一 (1 - y) ∙iog(i - H(W, Xi)).	(4)
To estimate W * from training samples, We solve the following nonconvex minimization problem
min fn (W).	(5)
W ∈Rd×K
Here we assume the input features x， are generated i.i.d. from the Gaussian mixture model (Pearson,
1894; Titterington et al., 1985; Hsu & Kakade, 2013), which we denote as
L
X 〜XλιN(μι,σ∣Id),	(6)
l=1
where N denotes the multi-variate Gaussian distribution with mean μι ∈ Rd, and covariance σQd
for σl ∈ R+ for all l ∈ [L]. The Gaussian mixture model can be viewed as
x ：= μh + Zh ∈ Rd	(7)
where h is a discrete random variable with Pr(h = l) = λl for l ∈ [L], and zl follows the multivari-
ate Gaussian N(0, σl2Id) with zero mean and covariance σl2Id1.
If the Gaussian mixture model is symmetric, the symmetric distribution can be written as
{PP λi(N(μι,σ2Id) + N(-μι,σ2Id))	L is even
l=1	L-ι	(8)
λιN(0,σ2Id)+ P λι(N(μι,σ2Id) + N(-μι,σIId)) L is odd
l=2
We assume without loss of generality that μι belongs to the column space of W * for all l ∈ [L].
To see this, note that an arbitrary μι can be written as μι∣∣ + μι⊥, where μι∖∖ belongs to the column
space of W*, and μι⊥ is perpendicular to the column space. Then, from (2) and (7) we have
1K	1K
H(W , χ) = κEφ(wjJ(μhk + μh⊥ + Zh)) = κEφ(wjJ(μhk + Zh)) = H(W , χ)
K j=1	K j=1
(9)
where x0 〜PL=I λιN(μι∣∣, σ2Id). Thus, these two cases are equivalent.
3	Proposed Learning Algorithm
We propose Algorithm 1 to solve (5) and defer its theoeretical analysis to Section 4. The method
starts from a initialization W0 ∈ Rd×K computed based on the tensor initialization method (Sub-
routine 1) and then updates the iterates Wt using gradient descent with the step size η0 . To analyze
the general cases, we assume an i.i.d. zero-mean noise {νi}n=1 ∈ Rd×K with bounded magnitude
∣(νi)jk | ≤ ξ (j ∈ [d],k ∈ [K]) for some ξ ≥ 0 when computing the gradient of the loss in (4).
Our tensor initialization method is extended from (Janzamin et al., 2014) and (Zhong et al., 2017b).
The idea is to compute quantities (Mj in (10)) that are tensors of w，* and then apply the tensor
decomposition method to estimate w，*. Because Mj can only be estimated from training samples,
tensor decomposition does not return w，* exactly but provides a close approximation. Because the
existing method only applies to the standard Gaussian, we exploit the relationship between proba-
bility density functions and tensor expressions developed in (Janzamin et al., 2014) to design tensors
suitable for the Gaussian mixture model. Formally,
1One can easily extend our analysis to the case when the covariance is diag(σ2ι,…,σ2d). One needs to
revise Property 4 and Lemma 7 correspondingly. We use the same σl to simplify the presentation.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Our proposed learning algorithm
Input: Training data {(xi, yi)}in=1, the step size η0 = O PL λ (k
Initialization: Wo J Tensor initialization method via Subroutine 1
, iteration T
Gradient Descent: for t = 0,1, ∙ ,T - 1
1n	1n
Wt+1 = Wt- no ∙ n X(VleW, Xi, yi) + Vi) = Wt - η (VfneW) + n X Vi)
i=1
i=1
Output: WT
Definition 1 Let p(x) = PL=I λι(2πσι)- d exp(- Ux滞 || ) be the probability density function of
the Gaussian mixture model in (6). We define
Mj := Ex〜PL=I λιN3,σR)[y ∙ (-1)jp-1(X)Vgp(X)], j = 1, 2, 3	(10)
Let α ∈ Rd denote an arbitrary vector. If the Gaussian Mixture Model is symmetric as in (8), then
P2 := M3 (Id, Id, α). Otherwise, P2 := M2.
Mjis a jth-order tensor Of w*, e∙g∙, M3 = K PK=I Ex〜PL=I χN3,σ2i)[φ”(Μ>χ)]w詈3.
These quantifies cannot be directly computed from (10) but can be estimated by sample means, de-
noted by Mi (i = 1, 2, 3) and P2, from samples {Xi, yi}in=1.
that these tensors are nonzero and can thus be leveraged to estimate W*.
The following assumption guarantees
Assumption 1 The Gaussian Mixture Model in (6) satisfies the following conditions:
1.	Ex〜PL ɪ λln(μ,σ2i)[φ000 (w*>χ)] = 0 for i ∈ [K], which implies that M3 is nonzero.
2.	If the distribution is not symmetric, then Ex〜PL ɪ χlN3 ,珑1)[φ00(w*>χ)] = 0 for i ∈
[K], which implies M2 and P2 in this case are nonzero.
Note that Assumption 1 is a very mild assumption2. Moreover, as indicted in (Janzamin et al., 2014),
in the rare case that some quantities Mi (i = 1, 2, 3) and P2 are zero, one can construct higher-order
tensors in a similar way as in Definition 1 and then estimate W * from higher-order tensors.
Subroutine 1 estimates the direction and magnitude of w*,j ∈ [K], separately. The key steps are
as follows. We first use the power method
to decompose Pb2
to approximate the subspace spanned
by {w*, w*,…，WK}, denoted by U. Then, We project M3 ∈ Rd×d×d to R3 ∈ RK×K×K
using U to reduce the computational and sample complexity for decomposing a third-order tensor
in the next step. We then apply the KCL algorithm to decompose Rb3 into vectors vbi. Note that
U>Vi = SiWi, where Si ∈ {1, -1} is a random sign. Then the direction of w* is determined.
Finally, the magnitude of wi* ’s and the signs of si ’s are determined by solving a linear system of
equations using the RecMagSign method. Please refer to (Zhong et al., 2017b) and (Kuleshov et al.,
2015) for more details on the power method, KCL and RecMagSign methods.
4 Main Theoretical Results
The main idea of our analysis is to show that the empirical risk function in (3) is strongly convex
in a region near W*. Then Wo returned by Subroutine 3 is in this convex region, and the iterates
returned by Algorithm 1 converge to a critical point in this region. Before formally stating our result
in Theorem 1, we summarize the key implications of Theorem 1 as follow.
1. Convergence rate and estimation accuracy: When gradients are accurate (i.e., ξ = 0), the
iterates Wt converge to a critical point Wn linearly, and the distance between Wn and W* is
2By mild we mean given L, if Assumption 1 is not met for some (λ0 , M0 , σ0), there exists an infinite
number of (λ0, M0, σ0) in any neighborhood of (λ0, M0, σ0) such that Assumption 1 holds for (λ0, M0, σ0),
5
Under review as a conference paper at ICLR 2021
Subroutine 1 Tensor Initialization Method
Input: Partition n pairs of data {(xi, yi)}in=1 into three subsets D1, D2, D3
Compute Pb2 using D1 and an arbitrary vector α
^ , , ^ _______________
Ub《—PoWerMethod(P2, K)
Compute R3 = M3(U , U , U ) from data set D2
{vi}i∈[K]4—KCL(Rb3)
，__、	… ，G ， _ 、	一、
{W0} <_ ReCMagSign(Ub,{vji∈[κ], D3)
Return: W0
O(，d log n/n). With the noise in the gradient, there is an additional error term of O(ξʌ/d log n/n).
For example, when n is Θ(dlog* 2 3 4 d), the estimation error decays as O(ιθ+ξd).
2. Sample complexity: The sample Complexity for aCCurate estimation is Θ(d log2 d) Where d is
the feature dimension. This result is in the same order as the sample complexity for the standard
Gaussian input in (Fu et al., 2020) and (Zhong et al., 2017b), indicating that our method can handle
input from the Gaussian mixture model without increasing the order of the sample complexity. Our
bound is almost order-wise optimal with respect to d because the degree of freedom is dK . The
additional multiplier of log2 d results from the concentration bound in the proof technique.
3. Impact of the mean: If everything else is fixed, and at least one entry of a mean μi(i) (the ith entry
of μι) of the Gaussian mixture model increases from 0 (in terms of the absolute value), the sample
complexity increases to infinity and the convergence slows down. The intuition is that as the absolute
value of some mean increases, some training samples have significantly large magnitude such that
the sigmoid function saturates. These training samples are not informative for the estimation of
W*, and the gradient of these samples is close to zero. Therefore, the required number of samples
to estimate W * needs to increase, and the gradient descent algorithm slows down.
4. Impact of the variance: If everything else is fixed, and at least one variance σl of the Gaussian
mixture model increases from a certain positive value, the sample complexity increases to infinity
and the convergence slows down. The intuition is the same as increasing ∣μi(i) | in point 3. On the
other hand, when all variances in the Gaussian mixture model approach zero, the sample complexity
increases to infinity, and the convergence slows down. The intuition is that when the input data are
concentrated on a few vectors, the optimization problem does not have a benign landscape.
Combining points 3 and 4, one can see that to learn the teacher network characterized by (2), the
training samples shall have zero mean and a medium level of variance to reduce the sample complex-
ity and speed up the convergence. If the variance is too large, some samples become non-informative
and affect the learning negatively. If the variance is too small, the learning problem becomes math-
ematically challenging to solve. This theoretical characterization can be viewed as one motivation
of the empirical techniques to improve learning rate such as whiting (LeCun et al., 1998) and Batch
normalization (Ioffe & Szegedy, 2015). We state our main theoretical result as follows.
Theorem 1 Consider the binary classification problem with one hidden-layer fully connected neu-
ral network as in (2). Suppose Assumption 1 holds, then there exist e° ∈ (0,1) and positive value
functions B(λ, M, σ, W*) and q(λ, M, σ, W*) such that as long as the sample size n satisfies
n ≥ nsc := poly(e0-1, κ, K)B(λ, M, σ, W *)d log2 d,	(11)
we have that with probability at least 1 - d-10, the iterates {Wt}tT=1 returned by Algorithm 1 with
step size no = O (PL—入“口；,：—十.)2) converge linearly with a statistical error to a critical point
Wcn with the rate of convergence v = 1 - K-2q(λ, M, σ, W*), i.e.,
||Wt - Wn∣∣F ≤ vt∣∣Wο - Wn∣∣F + 兽 PdK log n/n,	(12)
1-v
Moreover, the distance between W* and Wcn is bounded by
||Wn - W *||f ≤ O(K5 (1+ ξ) ∙ Pd log n/n).	(13)
6
Under review as a conference paper at ICLR 2021
We next quantify the impact of the parameters of the Gaussian mixture model on the sample com-
plexity nsc and the convergence rate v discussed in Theorem 1 as follows.
Corollary 1 (Impact of the Gaussian mixture model on nsc and v)
(1)	When everything else is fixed, n Sc increases to infinity, and V increases to 1, as ∣μi(i)∣ with any
l ∈ [L] and i ∈ [d] increases, where 乩屿 is the i-th entry of μι.
(2)	. When everything else is fixed except for some σl for any l ∈ [L], nsc increases to infinity, and v
increases to 1, as σl increases from ζs for some constant ζs > 0.
(3)	nsc increases to infinity, and v increases to 1 if all σl ’s go to zero for all l ∈ [L].
To the best of our knowledge, Theorem 1 provides the first explicit characterization of the sample
complexity and learning rate when the input follows the Gaussian mixture model. Although we
consider the sigmoid activation in this paper, our results apply to any activation function φ provided
that φ0 is an even function, and φ, φ0 and φ00 are bounded. Examples include tanh and erf . Algorithm
1 employs a constant step size. One can potentially speed up the convergence, i.e., reduce v, by using
a variable step size. We leave the corresponding theoretical analysis for future work.
If We scale the weights W*0 = W*/c and the input feature x0 = Cx simultaneously, the output
remains the same for any nonzero constant c. Therefore, the learning problems in these two cases
are equivalent in terms of the sample complexity and convergence rate. Theorem 1 reflects such
equivalence. One can check that B(λ, M, σ, W*) = B(λ, M0, σ0, W*0) from the proof in Section
B. Similarly, the convergence rate in (12) remains the same in both cases.
One main component in the proof of Theorem 1 to show that if (11) holds, the landscape of the
empirical risk is close to that of the population risk in a local neighborhood of W*. (Mei et al.,
2018a) quantified the similarity of these two functions when K = 1, but it is not clear if their
approach can be extended to the case K > 1. Here, focusing on the Gaussian mixture model, we
explicitly quantify the impact of the parameters of the input distribution on the landscapes of these
functions. Please see Appendix-C for details.
Compared with the analyses for the standard Gaussian in (Fu et al., 2020; Zhong et al., 2017b), we
develop new techniques in the following aspects. First, a direct extension of the matrix concentration
inequalities in these works leads to a sample complexity bound of O(d3), while we develop new
concentration bounds to tighten it to O(d log2 d). Second, the existing analysis to bound the Hessian
of the population risk function does not extend to the Gaussian mixture model. We develop new tools
that also apply to other activation functions like tanh or erf. Third, we design new tensors for the
initialization, and the proof about the tensor initialization is revised accordingly.
The above results assume the parameters of the Gaussian mixture are known. In practice, they can
be estimated by the EM algorithm (Redner & Walker, 1984) and the moment-based method (Hsu
& Kakade, 2013). The EM algorithm returns model parameters within Euclidean distance O((d)2)
when the number of mixture components L is known. When L is unknown, one usually over-
specifies an estimate L > L, then the estimation error by the EM algorithm scales as O((d)1).
Please refer to (Ho & Nguyen, 2016; Ho et al., 2020; Dwivedi et al., 2020a;b) for details.
5	Numerical Experiments
We verify Theorem 1 through numerical experiments. We generate a ground-truth W* ∈ Rd×K
from the Gaussian distribution. The training samples {xi , yi }in=1 are generated using (6) and (2).
The maximum number of iterations of Algorithm 1 is set as 12000.
5.1	Tensor initialization
Fig. 1 shows the accuracy of the returned model by Algorithm 1. Here d = 5, K = 2, λ1 = λ2 =
0.5, μι = -1 and μ2 = 0. We compare the tensor initialization with a random initialization in a
local region {W ∈ Rd×K : llWWWkJF ≤ e}. Tensor initialization in Subroutine 1 returns an initial
point close to W* with a relative error of 0.61. If the random initialization is also close to W*, e.g.,
7
Under review as a conference paper at ICLR 2021
= 0.1, then the gradient descent algorithm converges to a critical point from both initializations,
and the linear convergence rate is the same. If the random initialization is far away, e.g., = 1.5,
the algorithm does not converge. On a MacBook Pro with Intel(R) Core(TM) i5-7360U CPU at
2.30GHz and MATLAB 2017a, it takes 0.55 second to compute the tensor initialization. We consider
a random initialization with = 0.1 in the following experiments to simplify the computation.
5.2	Sample complexity
Consider the case that K = 3, L = 2, λ1 = λ2 = 2. Let μ1 be an all one vector in Rd and let
μ2 = 一μ1. Let σ1 = σ2 = 1. We vary d and evaluate the sample complexity bound in (11) with
respect to d. We randomly initialize M times and let Wcn(m) denote the output of Algorithm 1 in the
mth trail. Let WZn denote the mean values of all
Wnm), and let dw = JPM=ι |@m - Wn||2IM
denote the variance. An experiment is successful if dW ≤ 10-4 and fails otherwise. M is set as 20.
We vary d and the number of samples n. For each pair of d and n, 20 independent sets of W * and
the corresponding training samples are generated. Fig. 2 shows the success rate of these independent
experiments. A black block means that all the experiments fail. A white block means that they all
succeed. The sample complexity is indeed almost linear in d, as predicted by (11). Moreover, the
coefficient nId can be large depending on the problem setup.
Figure 1: Comparison between gradi-
ent descent with tensor initialization and
random initialization
Figure 2: The sample complexity against
the feature dimension d
We then fix d = 5 and study the impact on the sample complexity when the mean and variance
in the GaUSSian mixture model change. In Fig. 3.(a), We fix σ1 = σ2 = 1 and let μ1 = μ ∙ 1,
μ2 = -1. μ varies from 0 to 7.5. Fig. 3.(a) shows that when the mean increases, the sample
complexity increaSeS. ThiS coincideS With our theoretical analySeS in Section 4. In Fig. 3.(b), We
fix μ1 = 1, μ2 = —1, and let σ1 = σ and σ2 = 1. σ varies from 10-1.4 to 101.4. The sample
complexity increases both when σ increases and when σ approaches zero. The results match our
theoretical prediction in Section 4.
sə-duɪps JO JQqUInN
(a)
Figure 3: The sample complexity (a) when one mean changes, (b) when one variance changes.
8
Under review as a conference paper at ICLR 2021
5.3	Convergence analysis
We next study the convergence rate of Algorithm 1. dis fixed as 5. Fig. 4.(a) shows the impact of the
mean of the GaUSSian mixture model on the convergence rate. We set λ1 = λ2 = 0.5, μ1 = μ ∙ 1,
μ2 = -1, and σ1 = σ2 = 1. The sample complexity n is set to 10000. One can see that Algorithm
1 always converges linearly when μ changes. Moreover, as μ increases, Algorithm 1 converges
slower, as predicted by our theoretical analyses in Section 4. In Fig. 4.(b) shows the impact of the
variance of the Gaussian mixture model. We set λ1 = λ2 = 0.5, μ1 = 1, μ2 = —1, σ1 = σ2 = σ.
The sample complexity n is set to 50000. Among different σ we test, Algorithm 1 converges fastest
when σ = 1. The convergence rate slows down when σ increases to 2 or when σ decreases to 0.5.
The result is consistent with our theoretical results in Section 4.
We then verify the convergence rate in (12), which shows that v = 1 - Θ(K-2). We set λ1 = λ2 =
0.5, μ1 = 1, μ2 = —1, σ1 = σ2 = 1. K ranges from 2 to 8. One can see from Fig. 5 that, as
predicted, the convergence rate is almost linear in 1/K2.
(a)
(b)
Figure 4: (a) The convergence rate with different μ, (b) The convergence rate with different σ.
Figure 6: The relative error of the learned model
with the ground-truth when n changes
Figure 5: Convergence rate when the
number of neurons K changes
We then evaluate the distance between Wcn
，	11 A 1 LI Λ	1 TTΓ⅛	1 1 I Iττ7
returned by Algorithm 1 and W *, measured by ∣∣Wn —
W * ||F. d is 5. n ranges from 2 × 103 to 6 × 104. σ1 = σ2 = 3, μ1 = 1, μ2 = —1. Each point in
Fig. 6 is averaged over 100 independent experiments of different W * and the corresponding training
set. k W * kF is normalized to 1. The error is indeed linear in ʌ/log(n)/n, as predicted by (12).
6	Conclusions
This paper analyzes the theoretical performance guarantee of learning one-hidden-layer neural net-
works for binary classification when the input follows the Gaussian mixture model. We develop an
algorithm that converges linearly to a model that has a diminishing difference from the ground-truth
model that has guaranteed generalizability. We also provide the first explicit characterization of the
impact of the input distribution on the sample complexity and convergence rate. Future works in-
clude the analysis of multiple-hidden-layer neural networks and multi-class classification. Because
of the concatenation of nonlinear activation functions, the analysis of the landscape of the empirical
risk and the design of a proper initialization is more challenging and requires the development of
new tools.
9
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems,pp. 6155-6166, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019b.
Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas Macris, and Lenka
Zdeborova. The committee machine: Computational to statistical gaps in learning a two-layers
neural network. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31, pp. 3223-
3234. Curran Associates, Inc., 2018.
Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normal-
ization. In Advances in Neural Information Processing Systems, pp. 7694-7705, 2018.
Olivier BoUSqUet and Andre Elisseeff. Stability and generalization. Journal of machine learning
research, 2(Mar):499-526, 2002.
Alon BrUtzkUs and Amir Globerson. Globally optimal gradient descent for a convnet with gaUssian
inpUts. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
605-614. JMLR. org, 2017.
YUan Cao and QUanqUan GU. Generalization boUnds of stochastic gradient descent for wide and
deep neUral networks. In Advances in Neural Information Processing Systems, pp. 10836-10846,
2019.
Elaina Chai, Mert Pilanci, and Boris MUrmann. Separating the effects of batch normalization on cnn
training speed and stability Using classical adaptive filter theory. arXiv preprint arXiv:2002.10674,
2020.
Sanjoy DasgUpta. Learning mixtUres of gaUssians. In 40th Annual Symposium on Foundations of
Computer Science (Cat. No. 99CB37039), pp. 634-644. IEEE, 1999.
Simon DU, Jason Lee, HaochUan Li, Liwei Wang, and XiyU Zhai. Gradient descent finds global
minima of deep neUral networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S. DU, Jason D. Lee, and YUandong Tian. When is a convolUtional filter easy to learn? arXiv
preprint, http://arxiv.org/abs/1709.06129, 2017.
Simon S DU, Jason D Lee, YUandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent
learns one-hidden-layer cnn: Don’t be afraid of spUrioUs local minima. In International Confer-
ence on Machine Learning, pp. 1338-1347, 2018a.
Simon S DU, XiyU Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neUral networks. In International Conference on Learning Representations,
2018b.
Raaz Dwivedi, Nhat Ho, KoUlik KhamarU, Michael I. Jordan, Martin J. Wainwright, and Bin YU.
SingUlarity, misspecification, and the convergence rate of em. To appear, Annals of Statistics,
2020a.
Raaz Dwivedi, Nhat Ho, KoUlik KhamarU, Martin Wainwright, Michael Jordan, and Bin YU. Sharp
analysis of expectation-maximization for weakly identifiable models. In Silvia Chiappa and
Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference on Artifi-
cial Intelligence and Statistics, volUme 108 of Proceedings of Machine Learning Research, pp.
1866-1876, Online, 26-28 AUg 2020b. PMLR.
Andreas Engel and Christian P. L. Van den Broeck. Statistical Mechanics of Learning. Cambridge
University Press, USA, 2001. ISBN 0521773075.
10
Under review as a conference paper at ICLR 2021
Mario A. T. Figueiredo and Anil K. Jain. Unsupervised learning of finite mixture models. IEEE
Transactions on pattern analysis and machine intelligence, 24(3):381-396, 2002.
Haoyu Fu, Yuejie Chi, and Yingbin Liang. Guaranteed recovery of one-hidden-layer neural networks
via cross entropy. IEEE Transactions on Signal Processing, 68:3225-3235, 2020.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with land-
scape design. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=BkwHObbRZ.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? ArXiv preprint arXiv: 2006.13409, 2020.
Sebastian Goldt, MadhU Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova. Dy-
namics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
In Advances in Neural Information Processing Systems, volUme 32, pp. 6981-6991, 2019a.
Sebastian Goldt, Marc Mezard, Florent Krzakala, and Lenka Zdeborova. Modelling the influence of
data strUctUre on learning in neUral networks: the hidden manifold model. arXiv preprint arXiv:
1909.11500, 2019b.
Sebastian Goldt, Galen Reeves, Marc Mezard, Florent Krzakala, and Lenka Zdeborova. The gaus-
sian equivalence of generative models for learning with two-layer neural networks, 2020.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649. IEEE, 2013.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Nhat Ho and XuanLong Nguyen. Convergence rates of parameter estimation for some weakly
identifiable finite mixtures. Ann. Statist., 44(6):2726-2755, 12 2016. doi: 10.1214/16-AOS1444.
URL https://doi.org/10.1214/16-AOS1444.
Nhat Ho, Raaz Dwivedi, Koulik Khamaru, Martin J. Wainwright, Michael I . Jordan, and Bin Yu.
Instability, computational efficiency and statistical accuracy. Arxiv preprint Arxiv: 2005.11411,
2020.
Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and
spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical
Computer Science, pp. 11-20, 2013.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Anil K Jain. Data clustering: 50 years beyond k-means. Pattern recognition letters, 31(8):651-666,
2010.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Score function features for discriminative
learning: Matrix and tensor framework. arXiv preprint arXiv:1412.2863, 2014.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Proc. Adv. Neural Inf. Process. Syst., pp. 1097-1105, 2012.
11
Under review as a conference paper at ICLR 2021
Volodymyr Kuleshov, Arun Chaganty, and Percy Liang. Tensor factorization via matrix factoriza-
tion. In Artificial Intelligence and Statistics, pp. 507-516, 2015.
Yann LeCun, Leon Bottou, Genevieve B Orr, and KlaUs-Robert Muller. Efficient backprop. In
Neural Networks: Tricks of the Trade, pp. 9-50. Springer, 1998.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
May 2015.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31,
pp. 8157-8166. Curran Associates, Inc., 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activa-
tion. In Advances in Neural Information Processing Systems, pp. 597-607. 2017.
Shiyu Liang, Ruoyu Sun, Jason D Lee, and R Srikant. Adding one neuron can eliminate all bad
local minima. In Advances in Neural Information Processing Systems, pp. 4355-4365, 2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in neural information processing systems, pp. 855-863, 2014.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.
Ann. Statist., 46(6A):2747-2774, 2018a.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018b.
Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova. Dynamical
mean-field theory for stochastic gradient descent in gaussian mixture classification. Arxiv preprint
Arxiv: 2006.06098, 2020.
Remi Monasson and Riccardo Zecchina. Weight space structure and internal representations: A
direct approach to learning and generalization in multilayer neural networks. Phys. Rev. Lett., 75:
2432-2435, Sep 1995.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of
the Royal Society of London. A, 185:71-110, 1894.
Haim Permuter, Joseph Francos, and Ian Jermyn. A study of gaussian mixture models of color and
texture features for image classification and segmentation. Pattern Recognition, 39(4):695-706,
2006.
Richard A Redner and Homer F Walker. Mixture densities, maximum likelihood and the em algo-
rithm. SIAM review, 26(2):195-239, 1984.
David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by
back-propagating errors. Cognitive modeling, 5(3), 1988.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4430-4438, 2018.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-
2493, 2018.
Henry Schwarze and John Hertz. Generalization in a large committee machine. Europhysics Letters
(EPL), 20(4):375-380, oct 1992.
12
Under review as a conference paper at ICLR 2021
Henry Schwarze and John Hertz. Statistical mechanics of learning in a large committee machine. In
Advances in Neural Information Processing Systems, volume 31, pp. 523-530, 1993.
Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of learn-
ing from examples. Phys. Rev. A, 45:6056-6091, Apr 1992. doi: 10.1103/PhysRevA.45.6056.
URL https://link.aps.org/doi/10.1103/PhysRevA.45.6056.
Ohad Shamir. Distribution-specific hardness of learning neural networks. The Journal of Machine
Learning Research, 19(1):1135-1163, 2018.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 65(2):742-769, 2018.
Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its
applications in convergence and critical point analysis. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pp. 3404-3413. JMLR. org, 2017.
D Michael Titterington, Adrian FM Smith, and Udi E Makov. Statistical analysis of finite mixture
distributions. Wiley, 1985.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Yuki Yoshida and Masato Okada. Data-dependence of plateau phenomenon in learning with neu-
ral network — statistical mechanical analysis. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. dAlche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys-
tems, volume 32, pp. 1722-1730. Curran Associates, Inc., 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Fast learning of graph
neural networks with guaranteed generalizability: One-hidden-layer case. arXiv preprint
arXiv:2006.14117, 2020a.
Shuai Zhang, Meng Wang, Jinjun Xiong, Sijia Liu, and Pin-Yu Chen. Improved linear convergence
of training cnns with generalizability guarantees: A one-hidden-layer case. IEEE Transactions
on Neural Networks and Learning Systems, 2020b.
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu
networks via gradient descent. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 1524-1534. PMLR, 2019.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural
networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 4140-4149, 2017b. URL https://arxiv.org/pdf/
1706.03175.pdf.
Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2055-2064, 2019.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.
13
Under review as a conference paper at ICLR 2021
A Preliminaries
In this section, we introduce some definitions and properties that will be used in proving the main
results.
First we define the sub-Gaussian random variable and sub-Gaussian norm.
Definition 2 We say X is a sub-Gaussian random variable with sub-Gaussian norm K > 0, if
(EX|p) 1 ≤ K√p for all P ≥ 1. In addition, the Sub-Gaussian norm of X, denoted ∣∣X|展,is
defined as ∣X∣∣ψ2 = sup.〉] PT (E|X|p) 1.
Then We define the following three quantities. ρ(μ,σ) is motivated by the P parameter for the
standard Gaussian distribution in (Zhong et al., 2017b), and we generalize it to a Gaussian with an
arbitrary mean and variance. We define the new quantities Γ(λ, M, σ, W*) and Dm(λ, M, σ) for
the Gaussian mixture model.
Definition 3 (P-function). Let Z 〜N(u, Id) ∈ Rd. Define aq(i,υ,σ) = Ez—n(ui,i)[φ0(σ ∙ zi)zcq]
and βq(i, u, σ) = Ez—n(ui,i)[φ02(σ ∙ Zi)z;], V q ∈ {0,1, 2}, where Zi and Ui is the i-th entry of Z
and u, respectively. Define P(u, σ) as
P(u,σ) =	min {(u2 + 1)(βo(i,u,σ) — αo(i,u,σ)2),β2(i,u,σ) — α2(i,u,? }	(14)
i,j∈[d],j 6=i	ui + 1
Definition 4 (Γ-function). With (6), (14) and κ, η defined in Section 2, we define
L
Γ(λ, M,σ, W*)= X
l=1
λι σ ( W*>μι
κ2ησmaχ P( σιδκ (W *)
σι δκ (W *))
(15)
Definition 5 (D-function). Given the Gaussian Mixture Model in (6) and any positive integer m,
define Dm(λ, M, σ) as
Dm(λ, M,σ) = X λι(lμ⅛ + 1)m,	(16)
l=1	σl
where λ = (λι,…，Al) ∈ RL, M = (μι,…，μL ∈ Rd×L and σ = (σι,…，gl) ∈ RL.
P-function is defined to compute the lower bound of the Hessian of the population risk with Gaussian
input. Γ function is the weighted sum of P-function under mixture Gaussian distribution. This
function is positive and upper bounded by a small value. It is increasing when ∣μi(i)∣ increases.
When σι increases, Γ increases first and then decreases. Γ goes to zero if all ∣∣μι ∣∣∞ or all σι goes
to infinity. D-function is a normalized parameter for the means and variances. It is lower bounded
by 1. D-function is an increasing function of ∣μι∣∞ and a decreasing function of σι.
Property 1 We have that ∣νi ∣F is a sub-Gaussian random variable with its sub-Gaussian norm
bounded bu ξ∖fdK.
Proof:
(EkVikF)1 ≤ (E∣√dKξ∣p)P ≤ ξ√dK	(17)
Property 2 P(u, σ) in Definition 3 satisfies the following properties,
1.	P(u, σ) > 0for any u ∈ Rd and σ 6= 0.
2.	P(u, σ) converges to a positive value function ofσ as ui goes to 0, i.e. limui→0 P(u, σ) :
Cm (σ).
14
Under review as a conference paper at ICLR 2021
3.	When all ui = 0(i ∈ [d]), ρ( U ,σ) converges to a positive value function of U as σ goes to
0, i.e. limσ→o ρ( U ,σ) := Cs(U). When Ui = 0 for some i ∈ [d], limσ→o ρ( U ,σ) = 0.
4.	When everything else except |ui| is fixed, ρ(。)(泵),σδκ (W *)) is lower bounded by a
positive value function, Lm(。£(>>^),σδκ (W *)), which is monotonically decreasing to
0 as |ui | increases.
5.	When everything else except σ is fixed, ρ( σW(Wt) ,σδκ(W*)) is lower bounded by a
positive valuefunction, Ls(.W(Wt), σδκ(W*)), which satisfies thefollowing conditions:
(a) there exists Zso > 0, such that σ-1Ls( .W(WUt) ,σδκ (W *)) is an increasing function
of σ when σ ∈ (0,Zso); (b) there exists Zs > 0 such that Ls(.W(Wt) ,σδκ(W*)) is an
decreasing function ofσ when σ ∈ (ζs, +∞).
Proof:
(1)	From the Cauchy Schwarz’s inequality, we have
EZi~N(ui,1)[φ0(σ ∙ zi)] ≤ JEZi~N(ui,1) [φ02(σ ∙ zi)]	(18)
EZi~N(Ui,1)[φ0(σ ∙ zi)zi ∙ zi] ≤ JEZi~N(Ui,1)[φ02(σ ∙ Zi)Z2] ∙ JEZi~N(Ui,1)[z2]
/------------------------------------------------------- /——	(19)
="EZi~N(Ui,1)[φ02(σ ∙ Zi)Z2] ∙ Vu2 + 1
The equalities of the (18) and (19) hold if and only if φ0 is a constant function. Since that φ is the
sigmoid function, the equalities of (18) and (19) cannot hold.
By the definition of ρ(U, σ) in Definition 3, we have β0(i, U, σ) - α02(i, U, σ) > 0 and β2(i, U, σ) -
α2(i,u, > 0. Therefore,
ui+1
ρ(U, σ) > 0	(20)
(2)
u2
UIim0(σj2 + 1)(βo(i, u,σ) - α2(i, u,σ))
UliimO(σj2+1)( ∕∞ φ02(σ ∙ Zi)(2n)-1 eχp(-kzi-uk2)dZi
-(f φ0(σ ∙ Zi)(2∏)-1 exp(-kzi 2Uik )dzi)2)
-∞	2
u2
Uj
σ2
∞	1	kZ k2	∞	1
φ2(σ ∙ Zi)(2∏) 2 exp(- ɪ)dzi - ( φ (σ ∙ Zi)(2π) 2 exp(-
∞	2	-∞
kf )dZi)2)
(21)
lim (β2(i, u,σ)——21Tα2(i, u,σ))
Ui →0	ui2 + 1
lim Z φ02(σ ∙ Zi)z2(2π)-1 exp(-kzi -Uik )dzi
Ui→0 -∞	2
-(-ɪɪr Z φ0(σ ∙ Zi)Z2(2n)-2 exP(-kzi 2uik )dzi产
Ui + 1 -∞	2
Z φ02(σ ∙ Zi)Z2(2π)-1 exp(-kZ2^)dZi - (Z φ0(σ ∙ Zi)Z2(2π)-1 exp(-
-∞	2	-∞
呼)dZi)2
(22)
Combining (21) and (22), we can derive that ρ(U, σ) converges to a positive value function ofσ as
Ui goes to 0, i.e. limU→0 ρ(U, σ) := Cm(σ)
15
Under review as a conference paper at ICLR 2021
(3) When all ui 6= 0 (i ∈ [d]),
lim 1β2(i, U ,σ) — Tf^— α2(i, U ,σ))
σ→0 ' σ	Ui + ι σ
σ2 +
∞	„1	kzi - uik2
lim	φ02(σ ∙ Zi)Zl(2π) 2 exp(------σ	)dzi
σ→0 -∞	2
—
u21	1Z φ(σ ∙ zi)z2(2π)-1 eχp(-kzi 2 σk )dzi)2
U2 + 1 -∞	2
lim Z φ'02(ui ∙ Xi)Uix2(2∏σι)-1 exp(-kxi~2^k-)dxi
σ→0 -∞	σ2	Ut	2 U
—
-2 exp(-kxi 21k )dXi)2
σ2	)2
1+Uf
z. = Ui x	(23)
Zi -	Xi
σ
lim φ02(ui)——
σ→0	1 + σ2
The third step of (23) is by the fact that the Gaussian distribution goes to a Dirac delta function when
σ goes to 0. Then the integral will take the value when Xi = 1. Similarly, we can obtain the following
σi→0 0(i, σ,σ) -
-lim Z	φ02(σ ∙ Zi)(2π)-1 exp(-kzi	σ k )dzi
σ→0 -∞	2
一(Z φ0(σ ∙ Zi)(2π)-2 exp(-kzi ~ k )dz，2
-∞	2
-φ02 (Ui) - φ02 (Ui) - 0
(24)
lim
σ→0
lim
σ→0
∞
⅛<)dxi
φ φ0(xi)(2πσ2)-2 expJkxZuik )dxi)2))
Xi
σ ∙ Zi
σi→o「m eχp(-⅛if )(-σ-1+kxi- uik2σ-2)dxi
-2( J Φ(xi)(2πσ2)-2 exp(- kx]；" )dxi)
(25)
• / φ0(xi)(2πσ2)-1 eχp(-kxi2σuik-)(-σ-1 + Ilxi - Uik2σ-2)dxi)
lim
σ→0
lim j
σ→0	σ
- 2φ0(Ui)
+∞
16
Under review as a conference paper at ICLR 2021
Therefore, by L’Hopital’s rule and (24), (25), we have
2
lim(-j + 1)(βo (i, -,σ) - αo(i, _, σ))
σ→0 σ2	σ	σ
u2 ∂	u	u
lim ▼瓦~(Bo(i, —, σ) - αo(i, —, σ))
σ→0 2σ ∂σ	σ	σ
+∞
(26)
Combining (26) and (23), We can derive that ρ(U ,σ) converges to a positive value function of U as
σ goes to 0, i.e. limσ→o ρ(U,σ) := Cs(u).
2
When Ui = 0 for some i ∈ [d], limσ→o(σ2 + 1)(βo(j, U, σ) 一 α2(j, U, σ)) = 0 by (24). Then from
the Definition 3, we have limσ→o ρ( U,σ) = 0.
(4)	We shoW the statement by contradiction. Suppose that for any positive value func-
tion, h(ui), which is monotonically decreasing to 0 as |ui| increases, there exists a
u* ∈ R such that h(%) ≥ ρ(σW(Wt),σδκ(W*))1	. Then we can derive that
K	Ui = Ui*
limui→吗 ρ(σW(W,),σδκ(W*))∣ _ * = 0. Since that ρ(£(遥,σδκ(W*)) is continuous,
we can obtain that ρ(0W(Wt),σδκ(W*))∣	= 0, which contradicts to the conclusion in
Ui=Ui*
Property 2.1.
(5)	The condition (b) can be easily proved as (4). Therefore, we only need to show the condition (a).
When (W*>u)i = 0 for all i ∈ [K], limσ→o ρ(黑薪),σδκ(W*)) = Cs(u) > 0. Therefore,
there exists Zs > 0, such that when 0 < σ < Zs, ρ(。£(第),σδκ(W*)) > Cs(W-U. Then we
candefineLs( W",σδκ(W*)):= Cs(W*>U)σ2 suchthatσ-1Ls( WM,σδκ(W*))isan
s σδK (W* ) , K	2Zs	sσδK (W* ) ,	K
increasing function of σ below ρ(σW(W*), σδK(W*)). When (W*>u)i = 0 for some i ∈ [K],
then limσ→o ρ( σW(W*),σδκ(W*)) = 0. We can derive
- P(σWW*),σδκ(W*))	,、	∂ t W*>u
σ→⅞	σ	= σim) ∂σρ( σδκ (W *)
σδK(W*)) ≥ 0
(27)
The last step of (27) is because if the limit is negative, then ρ( ：W(W*), σδκ (W *)) will be negative
in a small neighborhood around σ = 0, which contradicts to the fact that ρ( σW(W*) ,σδκ(W*)) >
0.
P ρ( W*>u*∖ ,σδK (W* ))
If the limit in (27) is 0, then limσ→o ∂σ -σ^κ(——)σ---
ρ( W>>；*、,σδK(W*))
neighborhood around σ = 0 in which —σ-κ(——σ----------
> 0 otherwise there will be a small
< 0. In this case we only need to
let σ-1Ls(σWW*) ,σδκ(W*)) := ρ(
W*>；*、,σδK(W*))
JK——σ	. If the limit in (27) is positive, we can
ρ( W*(Wn ,σδK(W*))
find a positive lower bound of —4K(——σ--------in a small neighborhood around σ = 0 and an
increasing function of σ, σ-1L(σW(W*) ,σδκ(W*)) can be defined to be less than this positive
lower bound.
In conclusion, the condition (a) is proved.
Property 3 With the notation in (6), if a function f(x) is an even function, then
Ex 〜N (μ,σ2Id)[f (x)] = Ex 〜1N (μ,σ2Id)+ 2 N (-μ,σ2Id) [f (x)]	(28)
Proof:
Denote
g(x) = f(x)(2πσ2)-2exp(一⅛μi2)
(29)
17
Under review as a conference paper at ICLR 2021
Ex 〜N (μ,σ2I)f (X)] = /
x∈Rd
g(x)dx
r... r f
-∞	-∞ ∞
二 …	g(xι,…，Xd)dxι …dxd
-∞	-∞
-∞
g(xi,X2,…，Xd)d(-Xl)dx2 …dxd
「…/
-∞	-
g(-xι, X2 … ,xd)dx1dx2 …dxd
-∞
(30)
/
x∈Rd
g(-x)dx
∞
∞
∞
∕j(X)C2 厂 dexM-⅛μl2 )dX
Ex~N(-μ,σ2∙Id)[f (X)]
Therefore, we have
Ex~N(μ,σ2Id)[f (X)] = Ex〜1N(μ,σ2Id)+1N(―μ,σ2Id)[f (X)]
(31)
Property 4 Under GaUSSian Mixture Model X 〜 PL=I λιN(μι,σ2Id), we have the following
upper bound.
L
Ex〜PL=I λιN(id)[(uτX)2t] ≤ (2t - 1)!!∣∣u∣∣2t X λι(∣∣μι∣∣∞ + σ,)2t	(32)
l=1
Proof:
The main idea is to find an upper bound with symmetric distribution assumption first, and then apply
Property 3 to extend the conclusion to the general case.
(a) If the Mixed-Gaussian distribution is symmetric and L = 2, i.e. X 〜 2 (N(μ, σ2Id) +
N(-μ,σ2Id)), then We first need to analyse the distribution of UTX by computing the moment
generating function
d
d
E1
χ〜2 (N(μ,'
σ2Id )+N (-μ,σ2Id)) [eXP(tuTX)] = E[exp(tEuixi)] = ΠE[exP(tuixi)]
i=1
i=1
Id	M 1 ∞o	,	、 1	, (Xi - (-1)jμi)∖,、
=∏{∑s 2 J eχp(tuiχi)√2∏σ eχp(一ʌ—2σJ 〃' )dxi}
d2
=Y{X2exp(tui(-1)j μi)
i=1 j=1 2
∞	j ʌ ʌ 1	/ (xi-(-1)j μi )V7 ]
- J exp (tui(Xi - (-1)jμi)) √=^ exp(--0-------川/，}
=Y{2exp(-tuiμi) + 1 σ2u212 + 2 exp(t%μi + 2σ2u2t2)}
i=1
2d
1	0	1 2 02
=T2dexp(tμi+ 2t σ2)
i=1
(33)
which is the Moment Generating Function of Pi= 1 2d N (μi,σ02). The last step of (33) is
by expanding the multiplication of d terms. Specifically, let {si}i2=d 1 denote all 2d vectors
in Rd taking values from 0 and 1. Let sik (k ∈ [d]) denote the k-th entry of si . We define
μi = Pk=ι(-1)skUkμk ∈ R for i ∈ [2d], andσ0 = σ∣∣u∣∣ ∈ R, where Uk andμk are the k-th entry
of the vector U and μ, respectively. Then we can derive the first few steps of E[(uτX)2t]
18
Under review as a conference paper at ICLR 2021
Ex~ 1(N (μ,σ2Id)+N (-μ,σ2Id)) [(u X)]
Z∞
∞
2d
y2t
i=1
1	1	(y-μi)2
-----:  e	2σ02
2d √2∏σ0
dy
d
2∞
X 2d J	(y - μi+ μi )2t
2σ02
dy
X 2d ZIX「-μ0)p 盘 e-
(y-μi)2
2σ02	dy
2d	2t
1	(2t、 02t-p (	0	, p is odd
M 2d	IP μi	(p (P — 1)!!σ02, P is even
X 2d X (2kN"/(2k - 1)!!
i=1	k=0
2d X O0' - 1)!!X μi 2t-2k
k=0	i=1
(34)
The first step is by the distribution of u>x we obtain from (33). The third step follows from the
binomial expansion. The forth step results from the calculation of high-order moment of Gaussian
distribution. The second to last step is derived from the inverse of binomial expansion. The last step
is due to the substitution of summation. To compute the inner summation in the last step of (34), we

19
Under review as a conference paper at ICLR 2021
have
2d
X μi2t
i=1
2d
X(u1(-1)s1 μι + u2(—1产 μ2 + ... + Ud(-I)Sd μd)2t
i=1
2d
XX
i=1 p1i)+∙→pdi)=2t
2d
XX
i=1 p1i)+∙∙∙+pdi)=2t
2d
XX
i=1 p1i)+∙∙∙+pdi)=2t
(2t)!
(2t)!
(2t)!
j(uιμι)p1)…(udμd)pd)
all the pi are even
(i)
qj
(i)
Pj
2
2d
≤ max
i=1
(2t)!
(t)!
p1i⅛2"!…Pdi)" q(i)!q(i)!…qdi)!
}
Pdh=1 qh(i)=t
(t)!
q(i)!q2i)!…qdi)!
d
2	(2t)!
≤=max{ P≡^
d
2	(2t)!
≤〉 max{^-E-
士	PIi) !p2i)!…
(t)!
i)!q2i)!...qdi)!
(t)!
i)!q2i)!...qdi)!
} ∙ (u2μ2 + —+ udμd)t
}. (U2 + …+ ud)t ∙ maχ{∖μj|}2t
≤2d∣∣u∣∣2t ∙ max{∣μj∣}2t ∙ (2t - 1)!!
j
Firstly we explain the third step. For any odd p` , there is a term a0
(35)
i
(u`(-1)s'μg)p' ∙
Qk='(uk(-1)skμk)pk among the expansion of μi2t, whose corresponding vector si is
(si,…，s',…，sd). We can find a μj such that its corresponding vector is ⑸，…，1- s',…，sd),
which is only different from the tuple of μi in the '-th entry. Therefore, in the expansion of μj2t,
there exists a term a0 = (u'(-1)1-s'μ')p' ∙ Qk='(uk(-1)skμk)pk that can be cancelled out by
a0. Therefore, there will be no odd power terms left. The third to last step of (35) is by the inverse
binomial expansion. The second to last step is by the inequality PN=I aib ≤ max{bi} ∙ PN=I ai,
where ai and bi are positive. The last step is because
(2t)!
(t)!
(2t)!
t!
2	2
(i)
Pdm
2
(36)
≤ ɪ ∙ (2)m
≤T ∙ (∣)t = (2t - 1)!!
In the first equality of (36), Pd1 , ..., Pdm denote all the positive Pi . Thus, we have Pim=1 Pdi = 2t
wherePdi ≥ 2. Therefore, m ≤ 与=t which is used in the second inequality. Therefore, combining
20
Under review as a conference paper at ICLR 2021
(35), we can continue the derivation of (34) as follows.
Ex~ 1(N(μ,σ2Id)+N(-μ,σ2Id)) [(U X)]
=2d X (2k卜i2k(2k - 1)!!Xμi2t-2k
k=0 ' J	i=1
≤ X (2k) • σ02k(2k - 1)!!∣∣u∣∣2t •抽产{向∣}2t-2k⑵-1 - 2k)!!
≤(2t - 1)!!∣∣u∣∣2t(∣∣〃∣∣∞ + σ0)2t
The last step is because that
(2t - 1 - 2k)!!(2k - 1)!! ≤ (2t - 1 - 2k)!! (2t - 1)(2t - 3) ••• (2t - 2k + 1) = (2t - 1)!!
、
,
(37)
"^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^""^
k terms
(b) From Property 3, since that (UTx)2t is an even function, we have a result for a general Gaussian
distribution
Ex~N (μ,σ2Id) [(uTx)2t] = Eχ~ 1N (μ,σ2Id) + ɪ N (-μ,σ2Id) [(UTx)力
≤(2t - 1)!!∣∣u∣∣2t(∣∣Μ∣∣∞ + σ)2t
Therefore, if there are L components in the Gaussian Mixture Model, then
L
Eχ~P=1 KN(μ,σ2id)[(uτx)2t] ≤ (2t - 1)!!∣∣u∣∣2t X λι(∣∣Μι∣∣∞ + σι)2t
l=1
(38)
(39)
Property 5 With Gaussian Mixture Model (7), we have
L
Ex~Pl=1 λιN(μι,σ2Id)[∣∣x∣∣2t] ≤ dt(2t- 1)!! Xλι(kμιk∞ + σι)2t
l = 1
(40)
Proof:
Ex~PL=1 λlN(μι,σ2ld) [∣∣x∣∣2t]
d
=Ex~PL=1 λlN(μι,σ2Id) [(5S Xi )]
i=1
d χ2
=Ex~PL=1 λlN (μι,σ2Id) [dt(X 号)t]
i=1
d	2t
≤Ex~P 竺1 λlN (μι,σ2Id)[dt∑ 汨
i=1
d l ∞
“XX L
(Xi - μji + μji)2tXj r-- exp(-
√ 2πσ
)dXi
(41)
d L 2t
=dt-ιxxx	2t %M∣2t-k
i=1 j = 1 k=1
d L 2t
≤dt-1XXX
0	,
(k -1)!!σk,
k is odd
k is even
»jMI2tF •⑵-1)!!
i=1 j = 1 k=1、	/
dL
=dt-1 XX λj (MI + σ )2t(2t -1)!!
L
≤dt(2t - 1)!! X λι(kμk∞ + σι)2t
l=1
21
Under review as a conference paper at ICLR 2021
In the 3rd step, we apply Jensen inequality because f(x) = xt is convex when x ≥ 0 and t ≥ 1. In
the 4th step we apply the Binomial theorem and the result of k-order central moment of Gaussian
variable.
Property 6 The population risk function f(W) is defined as
f(W )= Eχ~pL=ι λιN (id) [fn(W )]
1n
=EX~PL=1 λlN (μι,σ2 Id) [- X '(W ； xi, yi)]
i=1
=Ex~PL=ι λιN(μι,σ2Id)['(W； xi,yi)]
Based on (2), (3) and (4), we can derive its gradient and Hessian as follows.
(42)
∂'(W; x,y) _	1 y - H(W)
∂wj
K H(W)(1 - H(W))
φ0(w>x)x = Z(W) ∙ x
(43)
∂2'(W; x,y)
∂wj∂wl
=ξj,ι ∙ XxT
(44)
ξj,l(W) =
K12 Φ0(W>X)Φ0(W>X)
K12 φ0(w> x)φ0(w>x)
H(W )2+y-2y∙H(W)
H 2(W )(1-H(W ))2
H(W )2+y-2y∙H(W)
H2(W)(1-H(W))2 ，
- - KKφ00(Wrx) H(W-(HW)W)),
j 6= l
=l
(45)
—
j
Property 7 With Dm(λ, M, σ) defined in definition 5, we have
(i)	Dm(λ, M, σ)D2m(λ, M, σ) ≤ D3m(λ, M, σ)
(ii)	(Dm(λ, M, σ))2 ≤ D2m(λ, M,σ)
(46)
(47)
Proof:
To prove (46), we can first compare the terms PiL=1 λi ai PiL=1 λi ai2 and PiL=1 λi ai3 , where ai ≥
1, i ∈ [L] and PiL=1 λi
1.
L
E λia3 - E λiai E Xia = E j @ -E λj∙a2)
i=1
i=1	i=1
i=1
L
j=1
L
L
L
L
£%州• ((1 - %)a2 - E	λj∙a2)
i=1
L
1≤j≤L,j6=i
EW E j -
i=1
L
1≤j≤L,j6=i
λjaj2)
1≤j≤L,j6=i
(48)
i=1
( E	λj(a2 - aj)
1≤j≤L,j6=i
= ɪ2	(λiλjai(a2 - a2) + λiλj aj (aj - a2 ))
1≤i,j≤L,i6=j
=	λi λj (ai - aj) (ai + aj) ≥ 0
1≤i,j≤L,i6=j
The second to last step is because We can find the pairwise terms λ% ai ∙ λj (a2 - j and λj a, ∙
λi(aj2 - ai2) in the summation that can be putted together. From (48), we can obtain
L
L
L
λiai	λi ai2 ≤	λi ai3
(49)
i=1	i=1
i=1
22
Under review as a conference paper at ICLR 2021
Combining (49) and the definition of Dm(λ, M, σ) in (5), we can derive (46).
Similarly, to prove (47), we can first compare the terms (P,L=1 λ,a,)2 and P,L=1 λ,a,2, where a, ≥
1, i∈ [L] and P,L=1 λ, =1.
L
L
L
L
fλ,a2 - (£ λ,a,Y = fλ,a,∙ (a, - £%力
i=1
i=1
,=1
L
j=1
∑λ,a, ∙ ((1- λi)a, - E	λjj
,=1	1≤j≤L,j6=,
L
∑λ,a, √	E	λja, - E	λj∙aj)
,=1
L
1≤j≤L,j6=i
1≤j≤L,j 6=i
(50)
£%,.( E	λj (a, - aj))
,=1
1≤j≤L,j6=,
=	):	(λi λj ai (ai - aj ) + λi λj aj (aj ― ai ))
1≤,,j≤L,,6=j
=	λ,λj(a, - aj)2 ≥ 0
1≤,,j≤L,,6=j
The derivation of (50) is close to (48). By (50) we have
L
L
(	λ,a,)2 ≤	λ,a,2
(51)
,=1
,=1
Combining (51) and the definition of Dm(λ, M, σ) in (5), we can derive (47).
B Proof of Theorem 1
Theorem 1 is built upon three lemmas. Lemma 1 shows that with O(dK5 log2 d) samples, the
empirical risk function is strongly convex in the neighborhood of W*. Lemma 2 shows that if
initialized in the convex region, that the gradient descent algorithm converges linearly to a critical
point Wn, that is close to W *. Lemma 3 shows that the Tensor Initialization Method in Subroutine
1 initializes W0 ∈ Rd×K in the local convex region. Theorem 1 follows naturally by combining
these three lemmas.
This proving approach is built upon those in (Fu et al., 2020). One of our major technical contri-
bution is extending Lemmas 1 and 2 to the Gaussian mixture model, while the results in (Fu et al.,
2020) only apply to Standard Gaussian models. The second major contribution is a new tensor
initialization method for Gaussian mixture model such that the initial point is in the convex region
(see Lemma 3). Both contributions require the development of new tools, and our analyses are
much more involved than those for the standard Gaussian due to the complexity introduced by the
Gaussian mixture model.
To present these lemmas, the Euclidean ball B(W*,r) is used to denote the neighborhood of W*,
where r is the radius of the ball.
B(W *, r) = {W ∈ Rd×K : ||W - W*||F ≤ r}
The radius of the convex region is
(52)
r := Θ
C3<0 ∙ PL=I 入1德 ρJWK(WL),σl δK (W *))
K7 (PL=1 λι(kμιk∞ + σι)4 P= λι(kμιk∞ + σ,)8)
with some constant C3 > 0.
1
4
(53)
23
Under review as a conference paper at ICLR 2021
Lemma 1 (Strongly local convexity) Consider the classification model with FCN (2) and the sig-
moid activation function. There exists a constant C such that as long as the sample size
L	L	2	ɪɪ 7- * >	- 2
n ≥ Ge-2 ∙ (X λι (kμk∞ + 切)2)2(£ λl 亲 ρ(6显(WL ,σ δκ (W *))= dK 5 log2 d
l=1	l=1	(54)
for some Constant Ci > 0 and e° ∈ (0, 4), we havefor all W ∈ B(W*,『fcn),
ω( 1-⅜0 X λι N ρ( -W/ ,σι δκ (W*))) ∙Idκ
K2	ηκ2 σlδK(W*)
L
W V2fn(W) W C2 Xλι(∣∣μι∣∣∞ + σι)2 ∙ IdK
l=1
(55)
with probability at least 1 d-10 for some constant C2 > 0.
Lemma 2 (Linear convergence of gradient descent) Assume the conditions in Lemma 1 hold. If
the local convexity holds, there exists a critical point in B(W*, r) for some constant C3 > 0 and
eo ∈ (0, 2), such that
||Wcn
-W*||f ≤ O(
K5 ∕p3 λι(kμk∞ + σι)2
PL=1 %η⅝ ρ( σ≡(W⅜ ,σιδκ (W *))
(56)
If the initial point W0 ∈ B(W*, r), the gradient descent linearly converges to Wn, i.e.,
||Wt - Wcn||F ≤
(1 - ω(
PL= λ 条 ρ( σW⅜ ,σι δκ (W *))
~K2 PL=ι λι(kμιk∞ + σι)2-
)) ||W0 - Wcn||F
(57)
with probability at least 1 - d-10.
Lemma 3 (Tensor initialization) For classification model, with D6(λ, M, σ) defined in Definition
5, we have that if the sample size
n ≥ κ8K4τ 12De(λ, M, σ) ∙ dlog2 d,	(58)
then the output W0 ∈ Rd×K satisfies3
||Wo - W*|| . K6K3 ∙ T6√D6(λ, M, σ)∖Idogn||W*||	(59)
n
with probability at least 1 — n-”4)
Proof of Theorem 1 and Corollary 1:
From Lemma 2 and Lemma 3, we know that if n is sufficiently large such that the initialization W0
by the tensor method is in the region B(W*, r), then the gradient descent method converges to a
critical point Wcn
that is sufficiently close to W*. To achieve that, one sufficient condition is
||Wo - W*||f ≤ √K||Wo - W*|| ≤ K6K7 ∙ T6√D6(λ, M, σ)ʌ/djogn∣W*||
n
C3e0Γ(λ, M,σ, W*)σ2mχ	(6O)
≤	1
K2 (∑L=1 λι(kμιk∞ + σι)4 PL=1 λι(kμιk∞ + σ1)8) 4
where the first inequality follows from || W ||f ≤ √K || W || for W ∈ Rd×κ, the second inequality
comes from Lemma 3, and the third inequality comes from the requirement to be in the region
3σmin and σmaχ denote the minimum and maximum among {σι, •一,cl}, respectively. T = ；max
24
Under review as a conference paper at ICLR 2021
B(W*,r). That is equivalent to the following condition
n ≥C0e-2 ∙ T 12κ12K 14(XXλι(|国k∞ + σι)4 XXλι(kμ<k∞ + 切)8/(δι(W*))2。6(λ, M,σ)
l=1	l=1
∙Γ(λ, M,σ, W*)-2σ-4χ ∙ dlog2 d
where C0 = max{C4 , C3-2 }. By the definition 5, we can obtain
(Xλι(kμιk∞ + σI)4Xλι(kμιk∞ + σI)8)2 ≤ √D4(λ,m,σ)D8(λ,M,σ)σ61ax
l=1	l=1
From Property 7, we have that
√D4(λ, M, σ)D8(λ, M, σ)De(λ, M, σ)
≤√D12(λ, M, σ)√D12(λ, M, σ) = D12(λ, M, σ)
Plugging (62), (63) into (61), we have
n ≥ C0e-2 ∙ κ12K 14(σmaχδ1(W *))2τ 12Γ(λ, M ,σ, W *)-2D12(λ, M, σ) ∙ d log2 d
(61)
(62)
(63)
(64)
Considering the requirements on the sample complexity in (54), (58) and (64), (64) shows a
sufficient number of samples. Taking the union bound of all the failure probabilities in Lemma 1,
and 3, (64) holds with probability 1 - d-10.
*> ..
By Property 2.3, ρ(,*^(W*),σιδκ(W*)) can be lower bounded by positive and monotonically
decreasing functions Lm(丁累>W*),σιδκ(W*)) when everything else except ∣μi(i)∣ is fixed, or
Ls(.W*(WW*), σιδκ(W*)) when everything else except σι is fixed. Then, by substituting the lower
bound of ρ(。工(WW*),σι δκ (W *)) for itself in Γ(λ, M ,σ, W *), We can have an upper bound of
(σmaχδ1(W *))2τ 12Γ(λ, M ,σ, W *)-2D12(λ, M, σ), denoted as B(λ, M ,σ, W *).
*>
To be more specific, when everything else except ∣μi(i)∣ is fixed, Lm(σιδκ (W*) , σlδK (W *))
is plugged in B(λ, M, σ, W*). Then since that D12(λ, M, σ, W*) and
Lm(σW^(WW*) ,σιδκ(W*))-2 are both increasing function of μi(i), B(λ, M, σ, W*) is an
increasing function of ∣μi(i) ].
When everything else except σl is fixed, if σl = σmax > ζs, then σm2 axτ12D12(λ, M, σ, W*)
is an increasing function of σι. Since that L,s(二矢(WW*),σιδκ(W*)) is a decreasing function,
Ls(σW*(WW*) ,σιδκ(W*))-2 is an increasing function of σ?. Hence, B(λ, M, σ, W*) is an
increasing function of σl . Moreover, when all σl < ζs0 and go to 0, two decreasing functions
of σι, σ21aχLs(σWWK*(WW*) ,σιδκ(W*))-2 and D12(λ, M, σ) will be the dominant term of
B(λ, M, σ, W*). Therefore, B(λ, M, σ, W*) increases to infinity as all σjs go to 0.
Hence, we have
n ≥ poly(e-1, κ, K)B(λ, M, σ, W*) ∙ dlog2 d	(65)
Similarly, by replacing ρ(σWW*WW*),o$k(W*)) with LmIɪ*/*),σιδκ(W*)) when every-
thing else except ∣μi(i)∣ is fixed, or Ls(遥花*), σ∕κ(W*)) (or σ-2Ls( σWW*WW*),。限(W*))
for σ ≥ 1) when everything else except σl is fixed, (57) can also be transferred to an-
other feasible upper bound. We denote the modified version of the convergence rate as v =
1 - K-2q(λ, M, σ, W*). Since that q(λ,M,σ,W*) is a ratio between the smallest and the
largest singular value of V2f (W *), we have q(λ, M, σ, W *) ∈ (0,1). Hence, we can obtain
1 一 K-2q(λ, M, σ, W*) ∈ (0, l) by K ≥ 1. When everything else except ∣μi(i)∣ is fixed,
since that Lm(σ^(WW*),σ∕κ(W*)) is monotonically decreasing and PL=1 λ(∣∣μ∣∣∞ + σι)2 is
increasing as ∣μi(i)∣ increases, V is an increasing function of ∣μi(i)∣ to 1. Similarly, when ev-
erything else except σ? is fixed where σ? ≥ max{1,Zs}, S- 1 ~--decreases to 0 as
-	TI=I λl(kμi k∞ +σl)2
σι increases. We replace ρ(ɪ*(WW*),o$k(W*)) by σ-2Ls(黑(WW*),σiδκ(W*)) and then
25
Under review as a conference paper at ICLR 2021
σ2 ∙ σ-2Ls(σWWl(Wμit),σιδκ(W*)) = Ls(慝(制,σ∕κ(W*)) is an decreasing function less
*>
than ρ(0限(W*),σιδκ(W*)). Therefore, V is an increasing function of σι to 1 when σι ≥
max{1,Zs}. When everything else except all σι ≤ Zs，'s go to 0, all Ls(σW*>Wμ*) ,σ∕κ(W*)’S
and l^l~~ʌ ^2 H~~：<2 ’s decrease to 0. Therefore, V increases to 1.
l = 1 λ2(kμιk∞+σ2)2
The bound OfkWn - W * ∣∣f is directly from (56).
C Proof of Lemma 1
We first state some important lemmas used in proof in Section C.1 and describe the proof in Section
C.2. The proofs of these lemmas are provided in Section C.3 to C.7 in sequence. The proof idea
mainly follows from (FU et al. (2020)). Lemma 6 shows the Hessian V2f (W) of the population
risk function is smooth. Lemma 7 illustrates that V2f (W) is strongly convex in the neighborhood
around μ*. Lemma 8 shows the Hessian of the empirical risk function V2fn(W*) is close to its
population risk V2f(W*) in the local convex region. Summing up these three lemmas, we can
derive the proof of Lemma 1. Lemma 4 is used in the proof of Lemma 7. Lemma 5 is used in the
proof of Lemma 8.
The analysis of the Hessian matrix of the population loss in (Fu et al., 2020) and (Zhong et al.,
2017b) can not be extended to the Gaussian mixture model. To solve this problem, we develop new
tools using some good properties of symmetric distribution and even function. Our approach can
also be applied to other activations like tanh or erf. Moreover, ifwe directly apply the existing matrix
concentration inequalities in these works in bounding the error between the empirical loss and the
population loss, the resulting sample complexity would be O(d3) and cannot reflect the influence
of each component of the Gaussian mixture distribution. We develop a new version of Bernstein’s
inequality (see (137)) so that the final bound is O(d log2 d).
(Mei et al. (2018a)) showed that the landscape of the empirical risk is close to that of the population
risk when the number of samples is sufficiently large for the special case that K = 1. Focusing on
Gaussian mixture models, our result explicitly shows how the parameters of the input distribution,
including the proportion, mean and variance of each component will affect the error bound between
the empirical loss and the population loss in Lemma 8.
C.1 Useful Lemmas in the proof of Lemma 1
Lemma 4
k
Ex~ 2 N (μ,Id)+1N (-μ,Id) [(X P>x ∙ φ(σ ∙χi))2] ≥ P(μ,σ)llP ||F ,	(66)
i=1
where ρ(μ,σ) is defined in Definition 3.
Lemma 5 With the FCN model (2) and the Gaussian Mixture Model (7), for some constant C12 > 0,
we have
E	h EC ∣∣V2'(W, x)-V2'(W0, x)||
x~PL=ι λlN(μl,σ22Id) IW=W0∈p(w*,r)	||W - W0IIf
uL	L
≤C12 ∙ d3 K 2t X λι (kμk∞ + σι)2 X λι(kμk∞ + σl)4
l=1	l=1
(67)
Lemma 6 (Hessian smoothness of population loss)In the FCN model (2), assume IIwk* II2 ≤ 1 for
all k. Then for some constant C5 > 0, we have
∣∣V2f(W) -V2f (W *)|| ≤ C5 ∙ K 3 ∙ (X λι (kμk∞ + σι)4 X λι(kμιk + σι)8)4 ∙∣∣W - W *||F
l=1	l=1
(68)
26
Under review as a conference paper at ICLR 2021
Lemma 7 (Local strong convexity ofpopulation loss) In the FCNmodel (2), if ||W 一 W*||f ≤ r
for an e° ∈ (0,1), then for some constant C4 > 0,
4⅛2e01 XX λ nJ P( WW) ,σιδκ (W*))" - vf(W)W C4 XX λι(kμιk∞+σι)2∙IdK
l=1	l=1	(69)
Lemma 8 In the FCN model (2), as long as n ≥ C0 ∙ dK log dK for some constant C0 > 0, we
have
sup	l∣V2fn(W)-V2f(W)|| ≤ C6 ∙ XX λι(kμk∞ + σl)∖ldκlogn)
W ∈B(W *,rFCN)	1=1	n
with probability at least 1 一 d-10 for some constant C6 > 0.
(70)
C.2 Proof of Lemma 1
From Lemma 7 and 8, with probability at least 1 一 d-10,
V2fn(W) Z V2f(W) 一 ∣V2f(W) - V2fn(W)|| ∙ I
上 ω( ⅛0 X λι σ2 ρ( σ⅛
σιδκ(W*))) ∙ I
(71)
一O(C6 ∙ Xλι(kΜιk + σι)2产叵)∙ I
l=1	n
As long as the sample complexity is set to satisfy
C6 ∙ X λι(kμιk∞+σι)2r dκogn ≤ κ2 Xλι n⅛ p( σWK(W*) ,σιδK(W*))
(72)
i.e.,
L	L 2	*>	-2
n ≥ C1e-2 ∙ (Xλι(kμk∞ + σι)2)2(χλι/ρ(①二(W*) ,σδκ(W*))厂 dK5 log2 d (73)
for some constant C1 > 0, then we have the lower bound of the Hessian with probability at least
1 一 d-10.
V2fn(W)之 Ω( 1-2e0 XλιMP W*[\ ,σιδκ(W*))) ∙I
K2	ηκ2 σιδK(W* )
By (69) and (70), we can also derive the upper bound as follows,
l∣V2fn(W)|| ≤ ∣∣V2f(W)|| + ∣∣V2fn(W) - V2f(W)||
≤ C4 ∙ X λι(kμι k∞ + σι)2 + C6 ∙ X λι(kμk∞ + σι)2 JdKlogn
ι=1	1=1	n
(74)
(75)
L
≤ C2 ∙ X λι(kμι k∞ + σι)2
ι=1
for some constant C2 > 0. Combining (74) and (75), we have
/1—2e L σ2 W *>μ	L	C
ω(-^2-° X λ 泉 PI σ,δK (W*),σι δκ (W*))∙ I WV2fn(W) W C2 X λι(∣∣μι∣∣∞ + σι )2 ∙ I
ι=1	ι=1	(76)
with probability at least 1 一 d-10 .
27
Under review as a conference paper at ICLR 2021
C.3 Proof of Lemma 4
Following the proof idea in Lemma D.4 of (Zhong et al., 2017b), we have
Ex 〜1N (μ,Id)+1N (-μ,Id) [(X PlX ∙ φ(σ ∙ Xi)H= A0 + B0
i=1
k
A0 = Ex 〜1N (μ,Id)+1N (-μ,Id) (X P>x ^ φ02(σ ∙ Xi) ^ xx>Pi)
i=1
B0 = Ex〜2N(μ,Id)+1N(-μ,Id) (X P>φ(σ ∙ xi)φ0(σ ∙ Xl) ∙ xx>Pl)
i6=l
In A0, We know that Ex〜2N(μ,Id)+1N(-μ,Id)xj = 0. Therefore,
k
A0 = X Ex〜1N(μ,Id)+ 2N(-μ,Id) [p> (φ02(σ ∙ Xi) (x2eie> + X XiXj (eie>
i=1	j 6=i
+ ejei>) + XXXjXlejel>	Pi
j6=i l6=i
k
=X Ex 〜2 N (μ,Id)+1N (-μ,Id) [p> (φ02(σ ∙ Xi) Gie 冠 + X Xjej e>))pi]
i=1	j 6=i
k
=X 忸x 〜1N (μ,Id)+ 2 N (-μ,Id)[φ02(σ ∙ Xi)Xi]pI eie> Pi
i=1
+ X Ex 〜2 N (μ,Id)+ 2 N (-“,Id)[X2]Ex 〜2 N (μ,I)+1N (-μ,I) [φ'2 9，Xi)}p> ej e> pi∖
j6=i
kk
=XPlie2(i, μ,σ) + X XPpjβo(i, μ, σ)(I + μj)
i=1	i=1 j 6=i
(77)
(78)
(79)
(80)
In BO, αι(i, μ,σ) = Ex〜1N(“,心)+2N(S)(Xiφ(Xi)) = 0. By the equation in Page 30 of
(Zhong et al., 2017b), we have
k
BO = X Ex〜1N(μ,Id)+ 2N(-μ,Id) [p> (φ0(σ Ti)。0。∙ Xl) (X2eie> + X2ele> + XiXl (eie> +
i6=l
elei>) + XXjXlejel> + XXjXiejei> + X X XjXj0ejej>0	Pl
j 6=i	j 6=l	j 6=i,l j0 6=i,l
=EPiiPlia2(i μ, σ)αο(l, μ, σ) + EpijPjαo(i, μ, σ)α0(l, μ, σ)(1 + μ2)
i6=l	i6=l
(81)
Therefore,
Ao + Bo = X (Piia⅛μ筌 + XPiiaο(l, μ, σ)∕1+μ)2 - XPii书字
ι+ ι √1 + μ2	~~. V 1	1 + Ni
i=1	V 1 μi	l=i	i=1	i
k	kk
-XXPhaO(I, μ,σ)2(1 + μ2) + XP2iβ2(i,μ,σ) + XXPjβο(i,μ,σ)(1 + μj
i=1 l6=i	i=1	i=1 j 6=i
k	2(i	) k
≥ XP2i(β2(i,μ,σ) —1+∙,2 ) + XXP2j(βο(i,μ,σ) - aο(i,μ,σ))(I + 域)
i=1	μi	i=1 j=i
≥ ρ(μ,σ)IIPIlF
(82)
28
Under review as a conference paper at ICLR 2021
C.4 Proof OF Lemma 5
Following the equation (92) in Lemma 8 of (Fu et al., 2020) and by (45)
KK
∣∣V2'(W) - V2'(W0)∣∣ ≤ XX ∣ξj,ι(W) - ξj,ι(W0)∣∙∣∣χχτ∣∣
j=1ι=1
By Lagrange,s inequality, we have
∣ξj,ι (W) - ξj,ι(W0)∣ ≤ (max ∣Tj,k,ι∣) ∙∣∣x∣∣∙√K ∣∣W - W0∣∣F
k
(83)
(84)
From Lemma 6, we know
max ∣Tj,k,ι∣ ≤ C7
k
(85)
By Property 5, we have
L
ExM1 λlN(μl,σ2Ia)[∣∣x∣∣2t∣∣] ≤ dt(2t- 1)!! X λι(kμιk∞ + σι)2t
(86)
Therefore, for some constant C12 > 0
Ex~PL=ι IlN…Ia)[WUW
∙/
∣∣V2'(W) - V2'(W0)||
∣∣W - W0∣∣F
]≤ K5E[∣∣x∣∣2]
≤K2 t
L
d X λι(kμk∞ + σι)2∖
ι=1	∖
L
3d2 X λι(∣∣μι∣∣∞ + σι)4
l = 1
(87)
L
L
=C12 ∙ d2 K 5∖X λι (kμι k∞ + σι )2 X λι(kμιk∞ + σι)
l4
l = 1
l = 1
C.5 PROOF OF LEMMA 6
Let a = (a>,…，aK)> ∈ RdK Let ∆j,ι ∈ Rd×d be the (j,∕)-th block of V2f(W)-
V2f (W*) ∈ Rdκ×dκ. By definition,	'
KK
∣∣V2f(W) - V2f(W*)∣∣ = max XXα>A,ιαι
UaU=I M M
(88)
By the mean value theorem and (45),
△j
∂ 2f (W)	∂ 2f (W *)
—
∂wj ∂wι	∂wj∂wf
Ex~PL XN(μι,σId)[(ξj,I(W) -ξj,ι(w*)) ∙xx>]
K /
Ex~pl=i λiN(“d[£(
k=1 '
K
dw'k
,Wk - wk ) ∙ xxt]
(89)
Ex~PL=ι XN(μi,σ2id)E hτj,ι,k ∙ x, wk - wfci ∙ xxτ]
k=1
∂ξj,ι(W0)
where WW = YW + (1 - Y)W* for some Y ∈ (0,1) and Tj,ι,k is defined such that
∂ξj,ι(W')

Tj,ι,k ∙ x ∈ Rd. Then we provide an upper bound for ξj,ι. Since that y = 1 or 0, we first compute
the case in which y
ξj,ι(w) = {
1. From (45) we can obtain
K⅛“(w>x)”(wJx) ∙ H2W,
Kbφ0(w>X)”(wJX) ∙ h2(W) - KKφ”(WJx) ∙ H(1W),
(90)
1
j = /
j = /
29
Under review as a conference paper at ICLR 2021
We can bound ξj,l(W) by bounding each components of (90). Note that we have
1	1 φ(wj>x)φ(wl>x)(1 - φ(wj>x))(1 - φ(wl>x))
H 2(W) — K2	κi2 φ(w>x)φ(w>x)
1	/ 1 φ(w>x)(1 - φ(w>x))(1 - 2φ(w>x)) / [
H(W) ≤ K	KKφ(w>x)	≤ 1
≤1
(91)
(92)
where (91) holds for any j, l ∈ [K]. The case y = 0 can be computed with the same upper bound by
substituting (1 - H(W)) = K PK=ι(1 - φ(w>x)) for H(W) in (90), (91) and (92). Therefore,
there exists a constant C9 > 0, such that
∣ξj,ι(W)| ≤ C9	(93)
We then need to calculate Tj,l,k. Following the analysis of ξj,l(W), we only consider the case of
y = 1 here for simplicity.
K3H3(W0) φ0(w0>x)φ0(w0>x)φ0(w0>x),
where j, l, k are not equal to each other
Tj,j,k =
(94)
K3H-(W O) φ0(w0>x)φ0 (W0>χ)φ0(w0>χ) + K 2HI(W O) φ00(w0>χ)φ0 (W0>x), j = k
K3H-2W0) (φ0(w0>x))3 + K2H3(W0) φ00(w0>x)φ0(w0>x) - OKH(W0, , j = k
(95)
K
a> ∆j,lal = Ex 〜PL=I N (μι,σl2l) [(£ Tj,l,k hx, Wk - WD) ∙ (a> x)(a> x)]
k=1
K	K
≤ ∖ Ex~pL=1 N(μι,σl2I)X T2k,l] ∙ E[X(hx, Wk- wD(a>x)(a>x))2]
k=1	k=1
≤t
K
Ex~pL=ι N (μι,σ2 I)[ΣS Tj,k,ι] t
k=1
K _________________ _________________
X JE((Wk - W，>x)4 ∙ JE[(a)x)4(a>x)4]
k=1
≤ C8
K
Ex 〜PL=I N (μι,σ2I)[∑ T2k,l]
k=1
K
X ||Wk- Wfc||2 ∙ ||aj ||2 ∙ ||ai||2
k=1
∖
LL	1
∙ (X λι(kμιk∞ + σι )4 X λι(kμιk∞ + σι)8) 4
l=1	l=1
(96)
for some constant C8 > 0. All the three inequalities of (96) are derived from Cauchy-Schwarz
inequality. Note that we have
-2	0	>	2 0	>	2φ2 (Wj>x)(1 - φ(Wj>x))2φ(Wk>x)(1 - φ(Wk>x))
IK3H3(W) (φ (Wjx)) φ (Wk x)l ≤ -------K3K3φ2(W>x)φ(W>x)-----------
= 2(1 - φ(Wj>x))2(1 - φ(Wk>x)) ≤ 2
I K3H2W) φ(W>x)”(W>x)”(W>x)| ≤ 2
Ik 2H3(W) ”(W>x)”(W>x)|
I 3φ(W>x)(1 - Φ(w>x))(1 - 2φ(W>x))φ(W>x)(1 - Φ(w>x)) ∣
到	K2K12 φ(W>x)φ(W>x)	1
=III3(1 - φ(Wj>x))(1 - 2φ(Wj>x))(1 - φ(Wk>x))III ≤ 3
(97)
(98)
(99)
30
Under review as a conference paper at ICLR 2021
I φ“0(w'x) I	I φ(w>x)(1 - φ(w>x))(1 - 6φ(w>x) + 6φ2(w>x)) ∣
I KH (W) I ≤ I	K -K φ(wjx)	I ≤ 1
Therefore, by combining (94), (95) and (97) to (100), we have
∖τj,ι,k| ≤ C ⇒ A2. ≤ c2, - ∈ [κ],
(100)
(101)
for some constants C7 > 0. By (88), (89), (96), (101) and the Cauchy-Schwarz,s Inequality, we
have
∣∣V2f(W) -V2/(W*)k
_____	L	L	1
≤C8 √C7K ||W - W *||f (X λι (kμι k∞ + σι)4 X λι(kμιk∞ + σ"8) 4
l=1	ι=1
K K
- maχ1 XX wɑj 后愀 “2
||a||-1 j=1 ι = 1
≤C8 √C2K .||W - W*||f ∙ (X λι (kμι k∞ + σι)4 X λι(kμιk∞ + σι)8f ∙ (X ||% ||)2
ι=1	ι=1	j=1
_____ L	L	1
≤C8gκ3 ∙ ||W - W*||f ∙ (X λι(kμιk∞ + σι)4 X λι(∣∣μι∣∣∞ + σι)8) 4
ι = 1	ι=1
(102)
Hence, we have
L	L	ι
||V2/(W)-V2/(W*)|| ≤ C5K2 (X λι(kμιk∞ + σι)4 X λι(kμιk∞ + σι)8)4
ι=1	ι = 1
for some constant C5 > 0.
||W - W*||f
(103)
C.6 PROOF OF LEMMA 7
From (Fu et al. (2020)), we know
4	K	2
V2/(W*) - IminI K Ex~P=1 XN (“"2Id)[(X φ0(WrX)(a> x)) ] ∙ IdK	(104)
with a = (a；, ∙ ∙ ∙ , a K )> ∈ RdK. And
K
v2/(W*) W ( max a>V2/(W*)a"dK W c4∙ max Ex~P J λιN(μi,σ21d) [X(aJx)2] ∙IdK
v IIaII = 1	/	IIaII = 1
j=1
(105)
for some constant C4 > 0. By applying Property 4, we can derive the upper bound in (105) as
KL
C4 ∙ Ex~PL=ι λιN(μι,σ2Id) [X(ajx)2] ∙ IdK W C4 ∙ X λι(∣∣μι k∞ + σι)2 ∙ IdK	(106)
j=1	ι=1
To find a lower bound for (104), we can first transfer the expectation of the Gaussian Mixture Model
to the weight sum of the expectations over general Gaussian distributions.
K2
置旦Ex~PL=ι λιN(μi,σ2id)[(X "(w； x)(a>x))]
L	K	2
min1 X λιEx~N (μi,σ2ld) [( X -(w；>x)(a> X))]
IIa" 1 ι = 1	j=1
(107)
31
Under review as a conference paper at ICLR 2021
Denote U ∈ Rd×k as the orthogonal basis of W*. For any vector a% ∈ Rd, there exists two vectors
bi ∈ RK and ci ∈ Rd-K such that
ai = Ubi + U⊥ci	(108)
where U⊥ ∈ Rd×(d-K) denotes the complement of U. We also have U>μι = 0 by (9). Plugging
(108) into RHS of (107), and then we have
K2
Ex~N (μι,σ2Id)[(X ai x ∙ φ(wi x))]
i=1
i=K1	(109)
=Ex-N(μι,σ2Id) h(X(Ubi + U⊥ci)>x ∙ φ0(W：>x)) ] = A + B + C
i=1
K2
A = Ex-N(μι,σ2id) [(Xb>U>x ∙ φ0(w∙fx)) ]	(110)
i=1
KK
C = Ex-N (μι,σ2Id) [2( X c]U>x ∙ φ0(w*τx)) ∙ (X b>U >x ∙ φ0(wiτx))]
i=1	i=1
KK
=XXEx-N (μι,σ2ld) h2ci U⊥ xi Ex-N (μι,σ1 Id) [bi U X ∙ φ (Wi x)φ (Wj x)] (III)
KK
=XX [2c> U>μi] Ex-N (μι,σ2Id) [b> U >x ∙ φ0(W:Tx)φ'(W；Tx)] = 0
i=1 j=1
where the last step is by U> μι = 0 by (9).
K
B =EX-N (μι,σ2Id) [(X CTUTx ∙ φ'M*Tx))2]
i=1
K
XE[ti2si2] +XE[titjsisj]
i=1	i6=j
k
by defining t = X 0'(w；Tx)Ci ∈ Rd-K and S = U[x
i=1
KK
=X E[t2]σ2 + (X E[t2](UT〃i)2 + X E[ti tjHUI”，∙(UI”，)，
i=1	i=1	i6=j
d-K	d-K
=E[ X t2b2] + E[(tTUT〃i)2] = E[ X t2σ2]
i=1	i=1
(112)
The last step is by U1μι = 0. The 4th step is because that Si is independent of ti, thus
E[titj sisj] = E[ti tj]E[si sj]
ESS-J (UTμι)i ∙ (Utμι)j，ifi = j
E[siSj ] = t	(UTμι)2+ σ2, if i = j
(113)
Since ( Pk=1 PTx ∙ φ'(σ ∙ Xi
2
is an even function, so from Property 3 we have
kk
EX-N (μι,σ2Id) [(X PT x ∙ φ'(σ ∙xi))[ = Ex-1N (μι,σ2 Id)+ 2 N (-μι,σ2Id) [(X PT x ∙ φ'(σ ∙xi))2]
i=1	i=1
(114)
Combining Lemma 4 and Property 3, we next follow the derivation for the standard Gaussian
distribution in Page 36 of (Zhong et al., 2017b) and generalize the result to a Gaussian distribution
32
Under review as a conference paper at ICLR 2021
with an arbitrary mean and variance as follows.
K2
A = Ex 〜N (μι,σ2Id) [(X b> U >x ∙ φ0(WLx))]
i=1
=Z(2πσ2)-K2 [(χb>z ∙φ0(VjZ)) i eχp (— 212 kz- U>μιk2)dz
=/(2∏12)-K2 h(Xb>Vt>s ∙ φ0(si))2i exp ( - 212kVt>s - U>μιk2)∣det(Vt)∣ds
i=1	2σl
≥ Z(2πσ2)-K2[(Xb>Vt>s∙φ0(Si))2] eχp(- ks-VWU)Ck )∣det(Vt)∣ds
i=1	2δK (W )1l
k
≥ (2∏)-K2σ-κ[(Xb>Vt>(δκ(W*)1ι)g ∙ φ,(δκ(W*)1，・ %))]
i=1
ι∣g- W*>μl∖ Ii2 l	l
• exp ( — '——σ K )	)∣det(Vt)∣1f δ^(W*)dg
12 K	>
1 Eg [(X(b> Vt δκ(W*))g ∙ φ'(σlδκ(W*) • %))2]
η	i=1
W *>μι
1ιδκ (W *)
,σιδκ (W * ))∣∣b∣∣2.
≥
(115)
The second step is by letting z = U> x. The third step is by letting s = V> z. The last to second
*>
step follows from g = σιδK(w *), where g 〜N(σlδK(W*), Iκ) and the last inequality is by
Lemma 4. Similarly, we extend the derivation in Page 37 of (Zhong et al., 2017b) for the standard
Gaussian distribution to a general Gaussian distribution as follows.
B = 12Ex〜N(μι,σ2id)川川2] ≥ η12P( 1W*(W*) ,σιδκ(W*))∣∣c∣∣2	(116)
Combining (109) - (112), (115) and (116), we have
ιminιEx~N (μι,σ2 Id) h(X a>x • φ0(w*>x))2i ≥ η12 Pl σWK(W[ ,1lδK (W *))∙	(Io)
For the Gaussian Mixture Model x 〜PL=I N(μι,12Id), We have
k	L	2	*>
lminιEx 〜PL=1 λlN (μι,σ2ld) [(X a>x • φ0(w*>x))2] ≥ X λι 泉P 1ιδκ (W '*) MBK (W *))
i=1	ι=1	(118)
Therefore,
4 X λ 或 z W*>”ι
K2 白 ι ηκ2 P( 1ιδκ(W*)
L
V2f(W *) W C4 ∙ X λι(kμιk∞ + 1ι)2 • IdK
ι=1
, 1ιδk(W*)) • Idκ
(119)
From (68) in Lemma 6, since that we have the condition kW - W* kF ≤ r and (53), we can obtain
||V2f(W) - V2f(W*)||
≤C5K3 (X λι(kμιk∞ + 1ι)4 Xλι(kμιk∞ + 1ι)8)1∣∣W - W*∣∣F
ι=1	ι=1
(120)
≤4!o X λ 互 z W*>μι
≤ K2A ι ηK2 叭 1ιδκ(W*)
1ιδκ(W*)),
33
Under review as a conference paper at ICLR 2021
where €0 ∈ (0, 4). Then we have
∣∣V2f(W)|| ≥ ∖∖V2f (W*)∣∣-∣∣v2/(W) - V2f(W)||
≥ 4(I - €0)X λ 互( W*>μι
≥ K2	λ~η l ηκ2 P( σlδκ(W*)
σιδκ (W *))
(121)
∣∣v2f(W)∣∣ ≤ ∣∣V2f(W*)∣∣ + ∣∣V2f(W) - V2f(W)∣∣
≤ C4 ∙ X λl(k“ι k∞ + σι)2 + K42 X λlη⅛P(σι'Z(W] ,σiδK(W*))(122)
L
< C4 ∙ X λl(kμk∞ + σl)2
l=1
The last inequality of (122) holds since C4 ∙ Pl4 λl(∣∣μ∣∣∞ + σ/2	=	Q(σ2ιax),
2	*>	2	2
a PL λl急P((X(W*) ,σlδκ(W*)) = O(⅛x) and。。篇)≥ Ω(⅛x). Combining (121)
and (122), we have
4⅛°1 X λlησ2 P(σWS⅜,σlδκ(W*)) ∙ I W V2f (W) W C4 ∙ X λl(∣∣μ∣∣∞ + σl)2 ∙ I
l=1	l=1	(123)
C.7 PROOF OF LEMMA 8
Let Ne be the e-covering number of the Euclidean ball R(W*,r). It is known that logNe ≤
dK log( 3r) from (Vershynin, 2010). Let We = {W1,..., WNJ be the e-cover set with Ne elements.
For any W ∈ B(W *, r), let j(W) = arg min ∣∣W — Wj(W )∣∣f ≤ e for all W ∈ B(W *, r).
j∈[Ne]
Then for any W ∈ B(W*, r), we have
kV2fn(W) -V2f(W)∣∣
1	n
≤-1∣ E[V2'(W; Xi)- V2'(Wj(w); Xi)]∣∣
i=1
1	n
+ ∣∣ n £ v2'(Wj(W )； Xi)- Ex ~PL=1 λιN (μl,σ2 Id) [V2'(Wj(W )； xi)]||
i=1
+ ∣∣Ex〜PL=1 λιN(“iq2Id)[V2'(WKW)； xi)] - Ex〜PL=1 λιN(μι,σ2Id)\'V"2'(W； Xi)]||
Hence, we have
P( sup	∣∣V2fn(W) -V2f(W)∣∣ ≥ t) ≤ P(At) + P(Bt) + P(G)
' W∈B(W*,r)
where At, Bt and Ct are defined as
1	∖ C	C	…	1
At = { sup ∣∣	[V24(W; Xi)-V24(W7(w)； Xi)]∣∣≥3}
W∈B(W*,r) n i=ι	3
1 ,	t,
Bt = {印 SUP*)|| n Σ V I(WKW )； Xi)- Ex~PL=1 λlN (μl,σl2Id) [V '(Wj(W )； Xi)]|| ≥ 3 }
Ct ={	suP	∣∣Ex 〜PLI λιN (“ι,σl2 Id)[V21(Wj(W )； Xi)]
W ∈B(W *,r)	ι = 1	ι
-Ex~pl=1 λιn(NEidNWW；Xi)]∣∣ ≥ 3}
(124)
(125)
(126)
(127)
(128)
34
Under review as a conference paper at ICLR 2021
Then we bound P(At), P(Bt) and P(Ct) separately.
1) Upper bound on P(Bt). By Lemma 6 in (Fu et al., 2020), we obtain
1n
Iln X V2'(W ； Xi)- Ex 〜PL=I λιN (id )[V2'(W ； Xi)] I
i=1
≤2 sup
v∈V1
4
(129)
where Vj is a 1 -cover of the Unit-EUclidean-norm ball B(0,1) with log ∣Vι | ≤ dK log 12. Taking
the union bound over We and Vi, We have
4
I1 n I t
P(Bt) ≤p(	sup	I - XGiI ≥ A)
∖ W ∈We,v∈V1 I n i=1	6
3r	1 n A
≤ exp(dK(log——+log12))	sup P(|	Gi| ≥ )
C	W ∈We,V∈V1	n W	6
4	i= 1
(130)
where Gi = (v, (V2'(W, Xi)- Ex 〜PL=I λιN (μι,σ2ld )[V2'(W, xi)]v)> and E[Gi] = 0. Here
V = (u>,…，UK )> ∈ RdK.
KK
lGi | = lXX [ξj,lu> xx>ul - Ex 〜PL=I λιN (μι,σ2I)(ξj,lu>xx>ul)[1
j=1 l=1
KK
≤ C9 ∙ [X(u>x)2 + XEx〜PL=1 λιN(μι,σ2Id) (U>x)2]
j=1	j=1
(131)
for some C9 > 0. The first step of (131) is by (44). The last step is by (93) and the Cauchy-
Schwarz’s Inequality.
35
Under review as a conference paper at ICLR 2021
E[∣G∣p] ≤ X C)
K
C9 ∙ Ex~PL=1 λιN(μι,σ2∑d)
K
•[(x(吗⑼2H (X Ex~PL=1 λlN (μi ,σf!d) (U>x)2
p-i
j=ι
j i
C9 ∙ Ex~PL=1 λlN(“ι,σHd) [ X
l!
K
∙ (X Ex~PL=1 λlN (μi ,σlld )(u>x)2
j=i
lι H-+lκ=l
p-l
X C)C9 ∙ [ X
l = 1	l1 H-Hlκ = l
l!
F	E,
Qj=Ilj! j=ι
X'
P-I
(132)
K
∙ (X Ex~PL=1 λιN (μι,σ2ld) (U>x)2
∙ (X Ex~PL=1 λlN(μι,σ"d)(u>M2)
K
C9 ∙ (XEx.
j=i
K
≤ C9 ∙ (X 1!!∣∣Uj∣∣2 Xλl(kμlk∞ + σl)2)p
j=i
L
≤ C9 ∙ (Xλl(kμlk∞ + σ"2)p
l = 1
where the second to last inequality results from Property 4. The last inequality is because v
PK=I ∣∣Uj∣∣2 = ∣∣v∣∣2 ≤ 1.
∞
E[exp(θGJ] = 1 + θE[Gi] + X
p=2
θp E[∣Gi∣p]
p!
≤ 1 + X ^P-C9 ∙ (X λl(∣∣μl∣∣∞ + σl)2)
L2
≤ 1 + C9 ∙ ∣eθ∣ ( ^X λl(llμl k∞ + σl))
l=1
(133)
where the first inequality holds from p! ≥ (P )p and (132), and the third line holds provided that
|e6|(p+1)	(LL	2、p+1
(p+1)(p+1) ∙( ∑l = 1 λl (Il μlk∞ + σl) ,
lpPP ∙ (PL=I % (Il μlk∞+σl )2)
n3
K
L
l = 1
∈ V1,
1≤ 2
(134)
36
Under review as a conference paper at ICLR 2021
Note that the quantity inside the maximization in (134) achieves its maximum when p = 2, because
it is monotonously decreasing. Therefore, (134) holds if θ ≤ 4e PL=I λι(kμιk∞ + σι)2. Then
1 n t	n	nθt	n
Pn- X Gi ≥ 6)= p(exp(θ X Gi) ≥ exp(-6-)) ≤ e 6 Y E[exp(θGi)]
n i=1	i=1	i=1
L	(135)
≤ eχp(Cιoθ2n(X λι(Ilμι ∣∣∞ + σι)2) —6-)
ι=1
for some constant C10 > 0. The first inequality follows from the Markov’s Inequality. When
θ = min{	t	2,27 PL=I λι(kμιk∞ + σι )2 }, we have a modified Bern-
12Cιo (PL=I λι(∣∣μιk∞ + σι)2)
stein’s Inequality for the Gaussian Mixture Model as follows
P(1 X Gi ≥ t) ≤ exp ( max{--------------C0nt----------不,
n i=ι	6	'	144(PL=1 λι(kμιk∞ + σι)2)
(136)
L
-Ciin X λι(∣μιk∞ + σι)2 ∙ t})
ι=1
for some constant Cii > 0. We can obtain the same bound for P(-n Pn=i Gi ≥ 6) by replacing
Gi as -Gi. Therefore, we have
P(11 X Gi | ≥ 6) ≤ 2exP ( max{-------/	「 Ci0nt----------72
i=i	6	144(PL λι(∣μιk∞ + σι)2)
L
-CiinX λι(kμι∣∣∞ + σι)2 ∙ t})
ι=i
(137)
Thus, as long as
i
L
t ≥ C6 ∙ max{£λι(∣μι∣∞ + σι)2
ι=i
dK log 36r + log 4
dK log 36r + log 4	}
PL λι(kμιk∞ + σι)2nʃ
(138)
n
for some large constant C6 > 0, We have P(Bt) ≤ δ.
2)	Upper bound on P(At) and P(Ct). From Lemma 5, We can obtain
SUP	IIEx~pW λιN(μι,σ2Id)[V2'(Wj(W)； x)] - ExKI λιN(μι,σ2Id)[V2'(W； x)]||
W ∈B(W *,r)	=
≤ SuP	llEx~PL=ι λιN(μι,σ2Id)[V2'(Wj(W)； x)] - Ex~PL=ι λιN(μι,σ2Id)[V2'(W； x)] ||
W ∈B(W *,r)	||W - Wj(W) ||F
• SuP	IlW - Wj(W)||F
W ∈B(W *,r)
L	L
≤ Ci2 • d3 K 5t X λι(∣μιk∞ + σι)2 X λι (kμι ∣∞ + σι)4 ∙ e
ι=i	ι=i
(139)
Therefore, Ct holds if
uL	L
t ≥ Ci2 ∙ d2 K 5t X λι(kμιk∞ + σι)2 X λι(∣μιk∞ + σι )4 ∙ e	(140)
ι=i	ι=i
37
Under review as a conference paper at ICLR 2021
We can bound the At as below.
1n	t
P(w∈BuW * ,r) n||X M(Wj(W); Xi)-*(W; xi≡≥ & )
≤
3	1n
tEχ~pL=ι λN Mσ2Id) [w ∈BUW *,r) n || X V '(Wj(W)； Xi)-V '(W;Xi 刑
t Ex-PL=I λιN (μι ,σ2Id)h W ∈BUW *,r)M'(Wj(W )；Xi)-v2'(W ； XiM
3
-E SUP
t	W∈B(W*,r)
≤
∣∣V2'(W∙(w); Xi)-V2'(W;则
||W - Wj(W)||F
SuP ||W - Wj(W) ||F
W∈B(W*,r)
≤
C12 ∙ d3K5，P3 λι(kμιk∞ + σι)2 PL=1 λι(kμιk∞ +^F ∙ e
t
Thus, taking
C12 ∙ d3K5 旧=ι λι(kμιk∞ + σι)2 P=I λι(∣∣μι∣∣∞ + σι)4 ∙ e
t ≥	δ
ensures that P(At) ≤ 2.
(141)
(142)
3)	Final step
Let e = -----3-5-/	δ	二---- and δ = d-10, then from (138)
C12∙d 2 K 2 ʌ/PL=I λι(kμι k∞+σι)2 PL=I λι(kμι k∞+σι)4 ∙ndK
and (142) we need
1
t > max{---
ndK
L
C6 ∙ X λι(kμιk∞ + σι)2
l=1

dK log(36rnd25 K2 Jp3 λι(∣∣μι∣∣∞ + σι)2 PL=I λι(kμιk∞ + σι)4) + log 4
n
dKlog(36rnd25K2 ∙ VZPL=I λι(kμιk∞ + σ∖)2 PL=I λι(|心 k∞ + σ1)4) + log 4
PL=1 λι (kμι k∞ + σι)2n	}
(143)
So by setting t = PL=I λl(kμlk∞ + σl)2 JdKfgn,, as long as n ≥ C0 ∙ dKlogdK, We have
P( sup	∣∣V2fn(W) -V2 f(W )|| ≥ C6 ∙ X λι(kμιk∞ + σι)2∖ IdK log n) ≤ d-10 (144)
W∈B(W*,r)	l=1	n
D Proof of Lemma 2
We first present a lemma used in proving Lemma 2 in Section D.1 and then prove Lemma 2 in
Section D.2.
D.1 A useful lemma used in the proof
Lemma 9 If r is defined in (53)for e° ∈ (0, ɪ) ,then with probability at least 1 一 d-10, we have4
，fn(W) is defined as 1 Pi=ι(Vl(W, xi,yi) + Vi) in algorithm 1
38
Under review as a conference paper at ICLR 2021
SuP	IVfn(W)-▽/(W)|| ≤ C13 ・
W ∈B(W *,r)
L
K X λι(kμk∞ + σι)2
l=1
(1 +ξ) (145)

for some constant C13 > 0.
Proof:
Note that Vfn(W ) = Vfn(W) + ɪ Pi=ι %, Vf(W ) = Vf (W)+ E[νJ = Vf (W). Therefore,
we have
..	. . ...
Sup	∣∣Vfn(W) -Vf(W)∣∣ ≤ Sup
W∈B(W*,r)	W∈B(W*,r)
1n
IIVfn(W) - Vf (W)∣∣ + kn EVik (146)
i=1
〜
〜
Then, similar to the idea of the proof of Lemma 8, We adopt an e-covering net of the ball B(W *, Ir)
to build a relationship between any arbitrary point in the ball and the points in the covering set. We
can then divide the distance between Vfn(W) and Vf(W) into three parts, similar to (124). (147)
to (149) can be derived in a similar way as (126) to (128), with “V2” replaced by “V”. Then we
need to bound P(A0t), P(Bt0) and P(Ct0) respectively, where A0t, Bt0 and Ct0 are defined below.
1n	t
At = { SUP || £[v'(w ； Xi)-V'(W∙(W )； xi)]|| ≥ -}
W∈B(W*,r) n i=1	3
(147)
1n	t
Bt = {噜SUP* J1 n Εv'(W<W )； Xi)- Ex~PL=ι λιN (μι,σ2 Id) [v'(Wj(W )； Xi)]|| ≥ 3 }
,	i=1	(148)
Ct ={	SuP	∣∣Eχ~pLι λι N(““2id)[V'(W<w) ； Xi)]
W ∈B(W *,r)	l=	l
-Eχ~PL=ιλιN(“"2id)[V'(W；xi)]|| ≥ 3}
(a)	Upper bound ofP(Bt0). Applying Lemma 3 in (Mei et al., 2018a), we have
1n
|| n £ V'(Wj(W )； Xi)- Ex~PL=ι λιN (μι,σ2 Id)[V'(Wj(W )； Xi)W
i=1
|	1n
≤2^up 1 (n X V'(Wj(W )； Xi)-Ex-PL= λιN (μι,σ2 Id)[V'(Wj(W )； Xi )]，V
(149)
(150)
Define Gi = (υ, (V'(W, xi) - Ex-PN] λιN(μι,σ2Id)[V'(W, xi)])). Here V ∈ Rd. To compute
V'(W, xi), we require the derivation in Property 6. Then we can have an upper bound of Z(W) in
(43).
ζ(W)
1	1
K H (W)
φ0(wj>x) ≤
Φ(w>χ)(i-φ(w> χ))
K∙ K φ(w> x)
Φ(w>x)(1-Φ(w> x))
K∙ K (1—φ(w>x))
≤1, y=1
≤1, y=0
(151)
Then we have an upper bound of G0i .
lGi 1 =「j,lv>X - Ex-PL=I λιN(μι,σ2Id)[ζv>X]|
≤ |V>X| + Ex-PNI λιN(μι,σ2Id) [|v>X|]
Following the idea of (132) and (133), and by V ∈ Vi, we have
L	P
E[∣Gi∣p] ≤ (X λι(kμι k∞ + σι)2)2
l=1
L
E[exp(θGi)] ≤ 1 + ∣eθ2∣ Xλι(∣∣μι∣∣∞ + σι)2
l=1
(152)
(153)
(154)
-
≤
39
Under review as a conference paper at ICLR 2021
where (154) holds if θ ≤ 47 Jp3 λι(kμι k∞ + σι)2. Following the derivation of (130) and (135)
to (138), we have
1n	t
P(In X Gil≥ 6)
i=1
≤2eχp ( maχ { - L ?丁	,-C15nt	X λ (kμi k∞ +	σl)2 ∙t})
144 ∑1=1 λl(kμlk∞	+ σl)	∖	l=1
for some constant C14 > 0 and C15 > 0. Moreover, We can obtain P(B0) ≤ 2 as long as
(155)
t ≥ C13 ∙ max1UX λι(kμι k∞ + σι) J.	^	~, /	e ' =	}
V=1	Vn	VP=I λι(kμιk∞ + σι) ∙ n
(156)
(b)	For the upper bound ofP(A0t) and P(Ct0), we can first derive
E	h	Sun	∣∣V'(W, x)-V'(W0, x)|| i
X〜PL=I λlN(μl,σ2Id) [w=wo∈p(w*,r)	∣∣W - W0∣∣F	J
≤EX 〜PJλlN (μl,σ2Id)	SUP
Ll = 1 l (μl, l d) LW=Wo∈B(W*,r)
|Z(W)- Z(W0)∣∙∣∣χ∣∣
IIW — WIIF
h	max1≤j,l≤K {lξj,l(W00)卜 ||x||2 √K||W - W 0||F
'〜PL=I λlN("l,σ2Id) [ W=W0∈p(W*,r)	IIW - W0IIf
Γ	C9 ∙ IIxII2√KIIW - W0IIf
F=I λlN("l,σ2IdMW=WSUp(W*,r)	IIW - W0IIf
L
≤C9 ∙ 3√Kd ∙ X λι(kμιk∞ + σι)
l=1	(157)
The first inequality is by (43). The second inequality is by the Mean Value Theorem. The third step
is by (93). The last inequality is by Property 5. Therefore, following the steps in part (2) of Lemma
8, we can conclude that Ct0 holds if
L
t ≥ 3。9 ∙ √Kd ∙ X λι(kμιk∞ + σι)2 ∙ e	(158)
l=1
Moreover, from (142) in Lemma 8 we have that
t≥
I8C9 ∙√Kd ∙ P=I λι(kμι k∞ + σι)2 ∙ e
δ
ensures P(At) ≤ 2. Therefore, let e
_______________δ_______________
l8C9∙√Kd∙PL=1 λl(kμιk∞+σl)2endK
δ = d-10
C13 JKP=I λι(kμιk∞ + σι)2ydl0gn, if n ≥ C00 ∙ dKlogdK for some constant C00
have
(159)
and t =
> 0, we
P(	sup	IIVfn(W) -Vf(W)II) ≥ C13 ∙
W∈B(W*,r)
K X λι(kμιk∞ + σι)2JdEgn ≤ d-10
l=1	n
∖
(160)
By Hoeffding’s inequality in (Vershynin, 2010) and Property 1, we have
PG X kVikF ≥ C13 ∙ t X λι(kμιk∞ + σι)2严叵ξ)
n i=1	l=1	n
L
.eχp(-C23 ∙ X λl(kμlk∞ + σl)
l=1
ξ2dK log n
dKξ2	)
.d-10
(161)
40
Under review as a conference paper at ICLR 2021
Therefore,
sup
W ∈B(W *,r)
一 ~ . ~ . ... ||Vfn(W)-Vf(W)|| ≤ C13 ∙ t ≤ Cl3 ∙ t ≤ C13 ∙ t	K X λι(kμιk∞ + S)2/ d log n l=1
	K X λι(kμιk∞+σι)2r d long n l=1	n
	KX λι(kμιk∞ 十 σl)ʌ/ n , l=1	n
(1 + ξ)
1n
+ n X ∣∣νikF
i=1
1n
+ n X kνik
i=1
(162)
D.2 Proof of Lemma 2
Following the proof of Theorem 2 in [Fu et al. (2020)], first we have the Taylor’s expansion of
fn(Wcn)
fn(Wn) =fn(W*) + (Vfn(W*), vec(Wn - W*),
+ 1 vec(Wn - W*)V2fn(W0)vec(Wn - W*)
(163)
Here W0 is on the straight line connecting W * and Wn. By the fact that fn(Wn) ≤ fn(W *), we
have
2VeC(Wn - W*)V2fn(W0)vec(Wn - W*) ≤ IVfn(W*)>vec(Wn - W*)
From Lemma 7 and Lemma 9, We have
(164)
and
W *>μ,	八	C
σ∣MW⅛ "(W * 川 Wn-W*1|F
1
≤2vec(Wn - W*)V2fn(W0)vec(Wn - W*)
∖Vfn(W*)>vec(Wn - W*)∣
一 ~ .......一^、	...
≤kVfn(W*)1HlWn - W*kF
≤(kVfn(W*) - Vf(W*)k + kVf(W*)k) ∙kWn - W*kF
≤O t
L
K X λι(kμιk∞ + σι)2
(1 + ξ))l∣Wn - W*I∣F
(165)
(166)
The second to last step of (166) comes from the triangle inequality and the last step follows from
the fact Vf (W*) = 0. Combining (164), (165) and (166), We have
||Wn - W*∣∣F ≤ O
K5 恒P= λι(kμιk∞ + σι)2(1+ ξ)
PL=1 入京 ρ( σW*(W*) ,σιδκ (W*))
(167)
Therefore, We have concluded that there indeed exists a critical point W in B(W*,r). Then We
shoW the linear convergence of Algorithm 1 as beloW. By the update rule, We have
1n
Wt+1 - Wn = Wt- ηo(Vfn(Wt) + n EVi)-(Wn - ηoVfn(Wn))
i=1
1
= I-η0	V2fn(W(γ))
n
(168)
(Wt- Wn)- ¥ X Vi
41
Under review as a conference paper at ICLR 2021
where W(Y) = YWn + (1 - Y)Wtfor Y ∈ (0,1). Since W(Y) ∈ B(W*,r), by Lemma 1, We
have
Hmin ∙ I W V2fn(W(Y)) ≤ HmaX ∙ I	(169)
Where Hmin = ω( K PL=1 λ 鸟 ρ( σWK⅛⅜* (W*))), HmaX = C4 ∙ PL=1 λι(kμιk∞ +
σl )2 . Therefore,
||Wt+1 - Wcn||F
||I-
η0Z1
0
v2fn(W (Y))∣∣∙∣∣Wt
n
-Wn∣∣F + k ηn0 X VikF
n i=1
n
≤ (1 - η0 Hmin) || Wt - Wn∣∣F + k n X VikF
i=1
(170)
By setting ηo = 1r- = O( E—ʌ f∣∣1 ∣∣一l一技),we obtain
0	Hmax	∖∑L1 λ"kμik∞+σi )27，
||Wt+1 - Wn∣∣F ≤ (1 - Hmn)||Wt - WjF + η X kVikF
HmaX	n i=1
(171)
Therefore, Algorithm 1 converges linearly to the local minimizer with an extra statistical error.
By Hoeffding’s inequality in (Vershynin, 2010) and Property 1, we have
p(nX kνikF ≥ rdKnognξ. eχp(-甘).尸	⑺)
i=1
Therefore, with probability 1 - d-10 we can derive
||Wt - Wn∣∣F ≤ (1 - Hmn )t∣∣W0 - Wn∣∣F + ^m^ /― "g " ξ	(173)
HmaX	Hmin	n
E Proof of Lemma 3
We need Lemma 10 to Lemma 14, which are stated in Section E.1, for the proof of Lemma 3. Sec-
tion E.2 summarizes the proof of Lemma 3. The proofs of Lemma 10 to Lemma 12 are provided
in Section E.3 to Section E.5. Lemma 13 and Lemma 14 are cited from (Zhong et al., 2017b). Al-
though (Zhong et al., 2017b) considers the standard Gaussian distribution, the proofs of Lemma 13
and 14 hold for any data distribution. Therefore, these two lemmas can be applied here directly.
The tensor initialization in (Zhong et al., 2017b) only holds for the standard Gaussian distribution.
We exploit a more general definition of tensors from (Janzamin et al. (2014)) for the tensor initial-
ization in our algorithm. We also develop new error bounds for the initialization.
E.1 Useful lemmas in the proof
Lemma 10 Let P2 follow Definition 1. Let S be a set of i.i.d. samples generated from the mixed
GaUSSian distribution PL=I λιN (μι,σ2I). Let P? be the empirical version of P? using data Set S.
Then with probability at least 1 一 2n-Q(34d), we have
||P2 - P2II . ʌIdogn ∙ δ2 ∙ T6√D2(λ, M, σ)D4(λ, M, σ)	(174)
Lemma 11 Let U ∈ Ed×K be the orthogonal column span of W*. Let α be a fixed unit vec-
tor and Ub ∈ Rd×K denote an orthogonal matrix satisfying ||UU> - UbUb>|| ≤ 4. Define
R3 = M3 (U, U, U), where M3 is defined in Definition 1. Let R3 be the empirical version of
R3 using data set S, where each sample ofS is i.i.d. sampled from the mixed Gaussian distribution
PL=I λιN(μι,σ2I). Then with probability at least 1 - n-”4), we have
||R3 - R3∣∣ . δ1 ∙ (T6√D6(λ, M,σ))
log n
n
(175)
42
Under review as a conference paper at ICLR 2021
Lemma 12 Let AfI be the empirical version of Mi using dataset S. Then with probability at least
1 - 2n-Q(d), we have
||M1 - Mill < (T2√D2(λ, M, σ)) ∙
(176)
Lemma 13 ((ZhOng et al., 2017b), Lemma E.6) Let P2 be defined in Definition 1 and P2 be its
empirical version. Let U ∈ RdXK be the column span of W *. Assume ∣∣P2 — Pb2∣∣ ≤ δκ;P2). Then
after T = O(log( ɪ)) iterations, the output of the Tensor Initialization Method 3, U will satisfy
∣∣UUT - UUτ∣∣ < ∣∣p2 —p2∣∣ + e	(177)
δκ (P2)
which implies
∣∣(I - UUT)W*∣∣ < (IP■—犁 + e)∣∣w*∣∣	(178)
δκ (P2)
Lemma 14 ((ZhOng et al., 2017b), Lemma E.13) Let U ∈ RdXK be the orthogonal column span
of W *. Let U ∈ RdXK be an orthogonal matrix such that ∣∣UUT — U U τ∣∣ < γι < κ2√∕K ∙ For
each i ∈ [K], let Vi denote the vector satisfying ∣∣3 — UTWi* ∣∣ ≤ γ2 < 2 匕.Let Mi be defined
κ K K
■ T	TCT T1^Λ2 1 ∙,	∙ ∙ 1	■	r∕' I I 71 Λ-	tΓ^Λ2 I I /	I∣71Λ" Il 1∣∣71Λ- Il	1
inLemma 12 and Mi be its empirical version. If ∣∣Mι-Mι∣∣ ≤ γ3∣∣M1∣∣ < ɪ ∣∣M1∣∣, then we have
∣∣∣W*∣∣ - bi I ≤ (κ4K3 (Yi + 72) + K2 K1 γ3)∣∣w:∣∣	(179)
E.2 PROOF OF LEMMA 3
^ .. .. .. ^ .. .. ^ ^
∣∣w; - &j U Vj ∣∣ = ∣ I W* -∣∣wj∣∣U Vj + ∣∣w*∣∣U Vj - M U Vj ∣ I
≤∣ I W； -∣∣w;∣∣U Vj I +1 ∣∣w*∣∣U Vj - bjUVj 11
≤∣∣w;∣∣ Wj*- U Vj I + I ∣∣w;∣∣- j I∣∣U VjII
≤∣∣w;∣∣	Wj*	-	UUτw*	+ UUTWj*	-	UVjII + I	I ∣∣w;∣∣-	bj I	I ∣∣U%∣∣
≤ δi (W*)(I I Wj* - U U TWj*I I +1 IU TWj* - bj I) +11 ∣∣w;∣∣- j I
(180)
By Lemma 10, Lemma 13 and Jk (P⅛) < δK, We have
i i wj*- U U TWj* i i < ∣∣PK(PP21∣ < r d⅛gn ∙ δKκ ∙T 6√D2(λ, M, σ)D4(λ, M,σ)
=mn ∙ K2 ∙ T6√D2(λ, M,σ)D4(λ, M,σ)
Moreover, we have
I IU TWj* - VjI ∣≤ δ2K27 ∣∣r3 - r3∣∣ < K2 ∙ (T 6√D6(λ, M ,σ))∙ r κ3ngn
(181)
(182)
in which the first step is by Theorem 3 in [Kuleshov et al. (2015)] and the second step is by Lemma
11. By Lemma 14, we have
∣ ∣∣∣W;∣∣- bj∣ ∣ ≤ (κ4K2(7i + 72) + K2K273)∣∣W*∣∣	(183)
Therefore, taking the union bound of failure probabilities in Lemmas 10, 11 and 12 and by
D2(λ, M, σ)D4(λ, M,σ) ≤。6(λ, M, σ) from Property 7, we have that if the sample size
n ≥ κ8K4τi2D6(λ, M, σ) ∙ dlog2 d, then the output Wq ∈ RdXK satisfies
∣∣Wq - W*∣∣ < K6K3 ∙
T 6√D^^ √ 干 ∣∣w;∣∣
(184)
43
Under review as a conference paper at ICLR 2021
with probability at least 1 - n-3δ4^
E.3 Proof of Lemma 10
From Assumption 1, if the Gaussian Mixture Model is a symmetric probability distribution defined
in (8), then P2 = M3(I, I, α). Therefore, by Definition 1, we have
∣∣Mc3(I, I, α) - M3
(I, I, α川=∣∣1 X [yi ∙ P(X)T X λι(2πσι)- d eχp(-llx 2σμ l||
i=1	l=1	σl
((T 严-(T )0 σ-2I )i(I, I, α)
σl	σl
-Ehy ∙P(X)T Xλι(2πσι)-d exp(-llx 2*"1 )
((皆产二可)0 σ-2I )](I, I 司1
Following (Zhong et al., 2017b), 0 is defined such that for any V ∈ Rd1 and Z ∈ Rd1 ×d2,
(185)
d2
v0eZ =	(v	0	zi	0	zi +	zi 0 v 0 zi	+ zi	0 zi	0	v),
i=1
where zi is the i-th column of Z. By Definition 1, we have
(186)
∣ ∣hy ∙ P(X)T X λι(2∏σι)-d exp(-⅛μ<) ((Y 产-(Y )0 σ-2I)] (I, I, α)∣ ∣
PL=1 λι(2∏σ2)-d exp(-l⅛A) ∙(警尸2(α>(x-μ))
PL=1 λι(2∏σ2)-d exp(-llx-μl12)	’~~
.llσm6n(Xira)XiX>||
(187)
The first step of (187) is because (x-μι产2(a>(x-μι)) is the dominant term of the entire ex-
pression, and y ≤ 1. The second step is because the expression can be considered as a normal-
ized weighted summation of (x-μι)02(α>(x-μ)) and (x>α)XiXr is its dominant term. Define
Sm(X) = (-1)m ▽*X(X), where P(X) is the probability density function of the random variable x.
From Definition 1, we can verify that
Mj = E[y ∙ Sm (x)]	j ∈{1, 2, 3}
(188)
Then define GPi =〈v, ([yi ∙ S3(xi)](Id, Id, α) - E。∙ S3(xi)](Id, Id, a)] v)), where ||v|| = 1,
then E[GPi] = 0. Similar to the proof of (131), (132) and (133) in Lemma 8, we have
lGPi |p . ∣σm6n(x>a)(x>v)2 + EX~PL=1 N (“ι,σ2Id)[σm6n(x> a)(x>v)2]∣p	(189)
.(Ex~PL=1 N(μl,σ2Id) [σm6n(x>a)(xJv),)p
≤ σm6pEx~pL=ι N (H* Id)[(x>a)2]2 Ex~PL=ι N (μι,σ2Id) [(x>v)4] 2	(19O)
≤ TTrD(λ, M,σ)D4(λ, M,σ)P
∞
E[exp(θGPi)] . 1+X
p=2
θpE[∣GPi 町
P!
∞
1+X
p=2
∣eθ∣pτ6p(D2(λ, M, σ)D4(λ, M, σ))2
Pp
(191)
n
2
)
1 + θ2τ 12D2(λ, M, σ)D4(λ, M, σ)
44
Under review as a conference paper at ICLR 2021
Hence, similar to the derivation of (135), we have
P( 1 X GPi ≥ t) ≤ exp ( - nθt + C16nθ2 (τ6,D2(λ, M, σ)D4(λ, M, σ))2)	(192)
n i=1
for some constant Ci6 > 0.	Let θ = --------7----------t------------and t = δ2 ∙
2Ci6 (τ6 v∕D2(λ,M,σ)D4(λ,M,σ))
(T6pD2(λ, M, σ)D4(λ, M, σ)) ∙ Jdlnn, then We have
||Mc3(Id, Id, α) - M3(Id, Id, α)∣∣ ≤ δ2 ∙ (T6,D2(λ, M,σ)D4(λ, M,σ)) ∙ Jdlngn (193)
with probability at least 1 - 2n-”4d).
If the Gaussian Mixture Model is not a symmetric distribution Which is defined in (8), then
P2 = M2 . We would have a similar result as follows.
n
llMc2 - M2|| = Iln X[yi • S2(χ)] - E[y ∙ S2(χ)]∣∣
n i=1
(194)
1K
∖∖yi • S2(Xi)II . llσmin KfMwj Xi)Xix>||
j=1
Then define GP0i	=	v, ([yi	•	S2(Xi)]	- Eyi	• S2(Xi)v), where ∖∖v∖∖ =	1,	then E[GP0i]
Similar to the proof of (131), (132) and (133) in Lemma 8, we have
IGPiIp . lσ-4n(x>v)2 + Ex〜PL=I N(μι,σ2Id)[σ-4n(x>v)2] Γ
(195)
0.
(196)
N(μι,σ2id)[σm4n(x>v)2])p ≤ T4pD2(λ, M, σ)p
(197)
∞
E[exp(θGP0i)] .1+X
p=2
θpE[lGPi lp]
P!
∞
.1+X
p=2
leθlpT 4pD2(λ, M, σ)p
Pp
(198)
.1+θ2T8D2(λ,M,σ)2
Hence, similar to the derivation of (135), we have
PG ^X GPi ≥ t) ≤ exp ( — nθt + C17nθ2 (τ4D2(λ, M, σ))2
n i=1
(199)
for some constant C17 > 0. Let θ =---t----t------and t = δ2 • (τ4D2(λ, M, σ)) • ʌ/dlogn,
2C17 τ4 D2 (λ,M,σ)	n
then we have
llM2 - M2 H . δ2 ∙ T4D2(λ, M, σ) • J	(200)
n
with probability at least 1 - 2n-”4d).
To sum up, from (193) and (200) we have
||P2 - Pb2∣∣ . ∖/dɪogn ・ δ2 • max{τ 4D2(λ, M, σ)),τ 6PD2(λ, M, σ)D4(λ, M, σ)}
V n	(201)
.∖I	• • δ2 • τ6 pD2(λ, m, σ)D4(λ, m, σ)
n
with probability at least 1 - 2n-”4d).
45
Under review as a conference paper at ICLR 2021
E.4 Proof of Lemma 11
We consider each component of y =	PK=I φ(w*>x).
Define Ti(x) : Rd → RK ×K ×K such that
Ti(x) = [φ(w*>x) ∙ S3(x)](U, U, U)	(202)
We flatten Ti(x) : Rd → RK ×K ×K along the first dimension to obtain function
Bi(x) : Rd → RK ×K 2. Similar to the derivation of the last step of Lemma E.8 in (Zhong
et al., 2017b), we can obtain kTi(x)k ≤ kBi(x)k. By (185), we have
1K
IIBi(X)II . σm6nN £0(w；>Xi)(U>x)3
K j=1
(203)
Define Gri = hv, Bi(xi)) - E[Bi(xi)]v)i, where IIvII = 1, so E[Gri] = 0. Similar to the proof
of (131), (132) and (133) in Lemma 8, we have
∖G∏∖p . ∣σ-6n(v>Ub>x)3 + Eχ~P=1 Ng,*Id)W-6n(v>U>x)3]∣p
E[∣Gri∣p] . (Eχ~pl=ι N(μι,σ2Id)[σm6n(v>U>x)3])p . T6p,D6(λ, M, σ)P
∞
E[exp(θGri)] . 1 +X
p=2
θpE[IGriIp]
p!
∞
.1+X
∣eθ∣pτ6pD6(λ, M,σ)p
≤ 1 + θ2(τ12 √D6(λ, M, σ))
Hence, similar to the derivation of (135), we have
p=2
2
pp
P(1 X Gri ≥ t) ≤ exp ( — nθt + C18θ2 (T6√D6(λ, M, σ))2
n i=1
(204)
(205)
(206)
(207)
for some constant	C18	>	0.	Let θ = ——t--, t k and t =	δ2	∙	(T6，。6(入,M, σ))	∙
Ci8 (τ6√D6(λ,M,σ))
qIonnn, then we have
∖∖Rb3 - R3∣∖ . δ2 ∙ (τ6√D6(λ, M, σ)) ∙ 4 n	(208)
with probability at least 1 一 2n-Q(δ4).
E.5 Proof of Lemma 12
From the Definition 1, we have
∣∣1 n	∣∣
∖∖Mι — Mι∖∖ = ∣∣-X[yi ∙ Sι(χ)] — E[y ∙ Sι(χ)]∣∣.
n i=1
(209)
Based on Definition 1,
∣∣	∣∣	∣∣PL=1 λι(2πσ2)-2 exp(-||x-2<) ∙(号)∣∣	∣∣ 2 1 6	›	∣∣
IMSI(Xi)]U . H	Pt1 λι (2πσ2)-d exp(-l⅛μ<) l H . Lin K X φ(w Xi)XiU
l	(210)
Define Gqi =〈v, ([yi ∙ SI(Xi)] — E[[yi ∙ Si(Xi)]] v)), where ∖∖v∖∖ = 1, so E[Gqi] = 0. Similar to
the proof of (131), (132) and (133) in Lemma 8, we have
∖Gqi∖p .卜-2n(Xlv) + Ex~PL=1 N (μι,σ2Id) [σ-n (Xlv)] F
(211)
46
Under review as a conference paper at ICLR 2021
E[∣Gqi∣p] . (Ex〜PL=IN(μι,σ2id)[σ-2n(x>v)])p ≤ T2p√D2(λ, M, σ)P
∞
E[exp(θGqi)] . 1 +X
p=2
θpE[∣Gqi∣p]
p!
∞
.1+X
∣eθ∣pτ2pD2(λ, M,σ)P
≤ 1 + θ2(τ2√D2(λ, M, σ))
Hence, similar to the derivation of (135), we have
p=2
2
1 n
1P( n X Gqi ≥ t) ≤ exp ( - nθt + C19θ2 (τ2 √D2(λ, M, σ))2
n i=1
(212)
(213)
(214)
for some constant C19 > 0. Let θ = ——ʒ--- t 丁 and t = (τ2 y∕D2(λ, M, σ)) ∙ ʌ dlθgn,
C19 (τ2ʌ/Dp(λ,M,σ))	V
then We have	______
||Mi - Mi|| . (τ2√D2(λ, M,σ))) ∙ Jdlngn	(215)
with probability at least 1 - 2n-Q(d).
47