Under review as a conference paper at ICLR 2021
Robust Loss Functions for Complementary
Labels Learning
Anonymous authors
Paper under double-blind review
Ab stract
In ordinary-label learning, the correct label is given to each training sample.	1
Similarly, a complementary label is also provided for each training sample in	2
complementary-label learning. A complementary label indicates a class that the	3
example does not belong to. Robust learning of classifiers has been investi-	4
gated from many viewpoints under label noise, but little attention has been paid	5
to complementary-label learning. In this paper, we present a new algorithm of	6
complementary-label learning with the robustness of loss function. We also pro-	7
vide two sufficient conditions on a loss function so that the minimizer of the risk	8
for complementary labels is theoretically guaranteed to be consistent with the min-	9
imizer of the risk for ordinary labels. Finally, the empirical results validate our	10
method’s superiority to current state-of-the-art techniques. Especially in cifar10,	11
our algorithm achieves a much higher test accuracy than the gradient ascent algo-	12
rithm, and the parameters of our model are less than half of the ResNet-34 they	13
used.	14
1 Instruction	15
Deep neural networks have exhibited excellent performance in many real-applications. Yet, their	16
supper performance is based on the correctly labeled large-scale training set. However, labeling	17
such a large-scale dataset is time-consuming and expensive. For example, the crowd-workers need	18
to select the correct label for a sample from 100 labels for CIFAR100. To migrate this problem,	19
reachers have proposed many solutions to learn from weak-supervision: Noise-label learning Li	20
et al. (2017); Hu et al. (2019); Lee et al. (2018); Xia et al. (2019), semi-supervised learning Zhai	21
et al. (2019); Berthelot et al. (2019); Rasmus et al. (2015); Miyato et al. (2019); Sakai et al. (2017),	22
similar-unlabeled learning Tanha (2019); Bao et al. (2018); Zelikovitz & Hirsh (2000), unlabeled-	23
unlabeled learning Lu et al. (2018); Chen et al. (2020a;b), positive-unlabeled learning Elkan & Noto	24
(2008); du Plessis et al. (2014); Kiryo et al. (2017), contrast learning Chen et al. (2020a;b), partial	25
label learning Cour et al. (2011); Feng & An (2018); Wu & Zhang (2018) and others.	26
We investigate complementary-label learning Ishida et al. (2017) in this paper. A complementary	27
Label is only indicating that the class label of a sample is incorrect. In the view of label noise,	28
complementary labels can also be viewed as noise labels but without any true labels in the training	29
set. Our task is to learn a classifier from the given complementary labels, predicting a correct label	30
for a given sample. Collecting complementary labels is much easier and efficient than choosing a	31
true class from many candidate classes precisely. For example, the label-system uniformly chooses a 32
label for a sample. It has a probability of 1 to be ordinary-label but k-1 to be complementary-label. 33
Moreover, another potential application of complementary-label is data privacy. For example, on 34
some privacy issues, it is much easier to collect complementary-label than ordinary-label.	35
Robust learning of classifiers has been investigated from many viewpoints in the presence of label 36
noise Ghosh et al. (2017), but little attention paid to complementary-label learning. We call a loss 37
function robust if the minimizer of risk under that loss function with complementary labels would be 38
the same as that with ordinary labels. The robustness of risk minimization relies on the loss function 39
used in the training set.	40
This paper presents a general risk formulation that category cross-entropy loss (CCE) can be used to 41
learn with complementary labels and achieve robustness. We then offer some innovative analytical 42
results on robust loss functions under complementary labels. Having robustness of risk minimization 43
1
Under review as a conference paper at ICLR 2021
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
helps select the best hyper-parameter by empirical risk since there are no ordinary labels in the
validation set. We conclude two sufficient conditions on a loss function to be robust for learning
with complementary labels. We then explore some popular loss functions used for ordinary-label
learning, such as CCE, Mean square error (MSE) and Mean absolute error (MAE), and show that
CCE and MAE satisfy our sufficient conditions. Finally, we present a learning algorithm for learning
with complementary labels, named exclusion algorithm. The empirical results well demonstrate the
advantage of the theoretical results we addressed and verify our algorithm’s superiority to the current
state-of-the-art methods. The contribution of this paper can be summarized as:
•	We present a general risk formulation that can be view as a framework to employing a
loss function that satisfies our robustness sufficient condition to learn from complementary
labels.
•	We conclude two sufficient conditions on a loss function to be robust for learning with
complementary labels.
•	We prove that the minimizer of the risk for complementary labels is theoretically guaran-
teed to be consistent with the minimizer of the risk for ordinary labels.
•	The empirical results validate the superiority of our method to current state-of-the-art meth-
ods.
2 Related works
Complementary-label refers to that the pattern does not belong to the given label. Learning from
complementary labels is a new topic in supervised-learning. It was first proposed by Ishida et al.
(2017). They conduct such an idea to try to deal with time-consuming and expensive to tag a large-
scale dataset.
In their early work Ishida et al. (2017), they assume the complementary labels are the same prob-
ability to be selected for a sample. And then, based on the ordinary one-versus-all (OVA) and
pairwise-comparison (PC) multi-class loss functions Zhang (2004) proposed a modifying loss for
learning with complementary labels.
Even though they provided theoretical analysis with a statistical consistency guarantee, the loss
function met a sturdy restriction that needs to be symmetric (`(z) + `(-z) = 1). Such a severe
limitation allows only the OVA and PC loss functions with symmetric non-convex binary losses.
However, the categorical cross-entropy loss widely used in the deep learning domain, can not be
employed by the two losses they defined.
Later, Yu et al. (2018a) assume there are some biased amongst the complementary labels and
presents a different formulation for biased complementary labels by using the forward loss cor-
rection technique Patrini et al. (2017) to modify traditional loss functions. Their suggested risk
estimator is not necessarily unbiased and proved that learning with complementary labels can the-
oretically converge to the optimal classifier learned from ordinary labels based on the estimated
transition matrix. However, the key to the forward loss correction technique is to evaluate the tran-
sition matrix correctly. Hence, one will need to assess the transition matrix beforehand, which is
relatively tricky without strong assumptions. Moreover, in such a setup, it restricts a small com-
plementary label space to provide more information. Thus, it is necessary to encourage the worker
to provide more challenging complementary labels, for example, by giving higher rewards to the
specific classes. Otherwise, the complementary label given by the worker may be too evident and
uninformative. For example, class three and class five are not class one evidently but is uninforma-
tive. This paper focuses on the uniform (symmetric) assumption and study random distribution as a
biased assumption (asymmetric or non-uniform).
Based on the uniform assumption, Ishida et al. (2019) proposed an unbiased estimator with a general
loss function for complementary labels. It can make any loss functions available for use, not only
soft-max cross-entropy loss function, but other loss functions can also be utilized. Their new frame-
work is a generalization of previous complementary-label learning Ishida et al. (2017). However,
their proposed unbiased risk estimator has an issue that the classification risk can attain negative
values after learning, leading to overfitting Ishida et al. (2019). They then offered a non-negative
correction to the original unbiased risk estimator to improve their estimator, which is no longer
2
Under review as a conference paper at ICLR 2021
guaranteed to be an unbiased risk estimator. In this paper, our proposed risk estimator is also not
unbiased, but the minimizer of the risk for complementary labels is theoretically guaranteed to be
consistent with the minimizer of the risk for ordinary labels, both uniform and non-uniform.
3 Preliminaries
3.1 Learning with ordinary labels
In the context of learning with ordinary labels, let X ⊂ Rd be the feature space and Y = {1, ∙∙∙ ,k}
be the class labels. A multi-class loss function is a map: L(fθ(x), y) : X × Y → R+ . A classifier
can be presented as:
h(x) = arg max fθ(i)(x) ,	(1)
i∈[k]
where fθ(x) = (fθ1)(x),… ,f∕k)(x)), θ is the set of parameters in the CNN network, fθi)(x) is
the probability prediction for the corresponding class i. Even though h(x) is the final classifier, we
use notation of calling fθ(x) itself as the classifier. Given dataset S = {(xi, yi)}iN, together with a
loss function L, ∀fθ ∈ F (F is the function space for searching), L-risk is defined as:
RSL(fθ)=ED[L(fθ(x),y)] = ES [L(fθ(x), y)],	(2)
96
97
98
99
100
101
102
103
104
105
106
107
Some popular multi-class loss functions are CCE, MAE, MSE. Specifically, ，Pk=1 eyi) log μ1y = log μ1y	CCE,
' (fθ(X),y) = '(U,y) = < ∣∣ey — u∣∣] = 2 — 2μy	MAE,	(3)
、key - uk2 = Iluk2 + 1 - 2μy	MSE,
where U = fθ(x) = (μι,…，μk), and ey is a one-hot vector that the y-th component equals to 1,
others are 0. The goal of multi-class classification is to learn a classifier fθ (x) that minimize the
classification risk RSL with multi-class loss L .
108
3.2 Learning with complementary labels
In contrast to the ordinary-label learning, the complementary-label (CL) dataset contains only labels
indicating that the class label of a sample is incorrect. Corresponding to the ordinary labels dataset
S, the independent and identically distributed (i.i.d.) complementary labels dataset denoted as:
S = {(x,y)}N,	(4)
where N is the size of the dataset S, and y represents that pattern X does not belong to class-y .
The general labels, distribution of dataset S is as:
109
110
111
112
113
114
115
116
117
-0 P12
p21	0
P(y∣y)=..
p1k
p2k
(5)
pk1	. . .	pk(k-1)	0
k×k,
where pij denotes that the probability of the i-th class’s pattern x labeled as j, Pjk=1 pij =
1,Pij=o,j = i. Supposing that the label system uniformly select a label from {1, ∙∙∙ ,k} \ {y}
for each sample x, then the Eq. (5) becomes
118
119
120
P (y|y)
0	ɪ	ɪ
0	k-1	...	k-1
ɪ	0	ɪ
k-1	U . . . k-1
(6)
1	1
k-1	. . . k-1
k×k .
Yu et al. (2018b) make a strong assumption that there are some bias in Eq. (5), while Ishida
et al. (2017; 2019) focus on the assumption of Eq. (6). In this paper, we study both kinds of distri-
bution.
121
122
123
3
Under review as a conference paper at ICLR 2021
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
4 Methodology
In this section, we firstly propose a general risk formulation for leaning with complementary labels.
And then prove that some loss functions designed for the ordinary labels learning are robust to
complementary labels with our risk formulation, such as categorical cross-entropy loss and mean
absolute error.
4.1	General risk formulation
The goal of learning with complementary labels is to learn a classifier that predicts a correct label for
any sample drawn from the same distribution. Because there are not ordinary labels for the model,
we need to design a loss function or model for learning with complementary labels. The key to learn-
ing a classifier for ordinary label learning is to maximize the true label’s predict-probability. One
intuitive way to maximize the true label’s predict-probability is to minimize the predict-probability
of complementary labels. In this paper, with little abuse of notation, we let
U = fθ(X) = (μι,…，μk)	⑺
V = 1 - fθ(X) = (I - μι,…, 1 - μk) .
Definition 1. (CL-loss) Together with loss function ` designed for the ordinary-label learning, the
loss for learning with complementary-label is as:
'(fθ (χ),y) = '(u,y) = '(v,y).	(8)
4.2	Theoretical results
Definition 2. (Robust loss) In the framework of risk minimization, a loss function is called robust
loss function if minimizer of risk with complementary labels would be the same as with ordinary
labels, i.e.,
RS(fθ*) - RS(fθ) ≤ 0 ⇒ RS(fθ*) - RS(fθ) ≤ 0.	(9)
Theorem 1.	Together with ', I is a robust Iossfunctionfor learning with complementary labels, if Q
satisfies:
dQ(u,y)、∩ 砥 u,y) _ C V∙ U 11 c∖ Li	∩0
------- > 0, --------= 0, ∀i ∈ {1, ∙∙∙ ,k}∖ {y} .	(10)
∂μy	∂μi
Note that, in Eq. 10, it means that'is a monotone increasing loss function only on u(y).
Proof. Recall that for any fθ, and any `,
RS(fθ) = E(x,y)['(fθ(χ),y)] = |S| X ' (fθ(χ),y) .	(II)
(x,y)∈S
For any complementary-label distribution in Eq. (5), and any loss function `, we have
RS(fθ) = E(x,y) U(fθ(x),y)]
kk
=ISIXXX Pij '(fθ (X),j),
i=1 x∈Si j 6=i
where Pij is the component of complementary labels distribution matrix P, Si ∪∙∙∙∪Sk = S.
(12)
Supposing that fθ* is the optimal classifier learns from the complementary labels, and ∀f ∈ F ,
where F is the function space for searching, we have


RS(fθ*) - RS(fθ)
kk
⅛ XX X Pimfθ*(x),j) - '(fθ (x),j)) ≤ 0,
i=1 x∈Si j 6=i
(13)
wherePij = 0. If ∃x0 ∈ <S, s.t., '(fθ* (x ),y) > '(fθ(x ),y), let f©，satisfying
fθ0 (X)
fθ* (x)	x ∈ SQ \ {x，},
fθ(x)	x = x ,
(14)
4
Under review as a conference paper at ICLR 2021
then according to Eq. 12 and 13, RS(/6，) < RS(fθ*), fθ* is not the optimal classifier. This 151
contradicts the hypothesize that fθ* is the optimal classifier.	152
Thus, ∀y ∈ {1,…，k}\ {y}, We have	153
'(fθ*(χ),y) ≤ '(fθ(χ),y).	(15)
According to Eq. (10), 'is a monotone increasing loss function only on u(y), then we have	154
∀y ∈ {1,…，k} \ {y}, fθy)(χ) ≤ fθy)(χ).	(16)
Thus,	155
fθy(x) ≥ fθy)(x), (fθy)(x) = 1 - X fθy)(x))	(17)
∖	y=y	)
and then,	156
'(fθ*(χ),y) ≤ '(fθ(χ),y),	(18)
thus,	157
RS (fθ*) -RS (fθ) ≤ 0 .	(19)
□	158
Theorem 2.	Together with ', Q is a robust loss function for learning with complementary labels 159
under symmetric distribution or uniform distribution, if' satisfies:	160
∂Q(U,y)	ʌ -,	` 厂,厂	、	小八、
号产 > 0, ≥√(u,i) = C, (C is a constant) .	(20)
dμy	i=1
It should be noted that, in Eq. 20, it means that ' is a symmetric loss (E '(u, i) = C), and ' is a 161
monotone increasing loss function on any yQ.	162
Proof. For any complementary-label distribution in Eq. (6), and any loss function `, we have
RS(fθ) = E(χ,y) [Q(fθ(x),y)]
kk
=闾 X X. X EQ(fθ (X),j)
= x∈ i j6=i
=|S| XX 占(C - Qfθ(X),i))
i=1 x∈Si
=k-1 -RS (Je )，
163
(21)
where Si ∪ …U& = S .	164
Supposing that fθ* is the optimal classifier learns from the complementary labels, and ∀f ∈ F, 165
where F is the function space for searching, we have	166
RS(fe*)-RS(fe) = RS(fe) -RS(fθ*) ≤ 0,	(22)
According to the first constraint in Eq. (20), we then have	167
Q(fe(χ),y) ≤ Q(fe*(χ),y), Jθy)(χ) ≤ fθy)(χ))	(23)
and then,	168
'(fe* (χ),y) ≤ '(fe(χ),y),	(24)
thus,	169
RS(fe*)-RS(fe) ≤ 0 .	(25)
□	170
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Learning from complementary labels by exclusion
Require:
S = {(xi, yi)}N: The given dataset;
Ensure: Classifier fθ (x)
1:	Randomly initialize a group parameter θ for fθ (x);
2:	Randomly split S into a training set 舟出口 and a valid-set SValid；
3:	for (e = 1; e ≤ Epochs; e + +) do
4:	for (χi, yi) in * Strain do
5:	fθ (Xi) = (μ1, ∙∙∙ , μk );
6:	U = 1 - fθ(Xi) = (I - μι,…，1 - μk);
7:	loss = '(u, yi);
8:	W = W - β dloss ,w ∈ θ;
w,
9:	end for
10:	end for
11:	return fθ (X)
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
Together with some well known multi-class loss functions, such as CCE, MAE, MSE, the loss for
learning with complementary labels with our definition are as follows:
Pk=ι e(i)log 1⅛ = log 1-⅛	CCE,
'(fθ(X), y) = ' (V, y)	= <	∣∣ey — Vk ι = k — 2 + 2μy	MAE,	(26)
.key- vk2 = k — 3+ ∣∣uk2 +2μy	MSE,
where ey is a one-hot vector that the y-th component equals to 1, others are 0. As its shown in
Eq. (26), CCE and MAE loss satisfy the Theorem 1, MAE also satisfies the Theorem 2, while MSE
does not satisfies the two. Zhang & Sabuncu (2018) propose a GCE loss function for learning with
label noise, their formulation is as:
'gce (fθ(x),y) =(1	,q ∈ (0, 1) .	(27)
It is easily to know that the loss function satisfies the constraint in Theorem 1, thus, it can be used
to learning with complementary labels.
4.3 Exclusion algorithm for learning from complementary labels
Based on the loss function we designed for complementary-label learning, we present an algorithm
to learn a classifier from complementary labels with our loss function, named exclusion algorithm
(the label specifies that the sample does not belong to it). The algorithm details show in Alg. 1.
Furthermore, our algorithm is easily combined with the models designed for ordinary-label learning,
with only a minus operation, which can be view as a framework to use the loss designed for ordinary-
label learning to learn the optimal classifier from complementary labels.
5 Experiments
5.1 Experimental settings
Datasets. We test our experiments on MNIST LeCun et al. (1998), FASHION-MNIST Xiao et al.
(2017), CIFAR10 Krizhevsky (2009). Specifically, we generate two types of complementary labels:
symmetric and asymmetric, for our experiments to verify our method’s effectiveness and the theorem
we proved in the previous section. For symmetric complementary-label, we fix a label distribution as
Eq. (6) to generate the complementary-label training set. The validation set is split from the training
set, which contains none ordinary-label. Thus, the lower the validation accuracy, the better the
classifier learns from the training set. For asymmetric complementary-label, we randomly generate
a matrix as Eq. (5) that the pij is unknown as the complementary-label distribution and using it
6
Under review as a conference paper at ICLR 2021
to create complementary-label for experiments. The test accuracy of all experiments is tested on a 196
clean dataset that contains only the ordinary labels.	197
Approaches. We test our loss with 'cce, 'mae, ^mse, 'gce and compare with state-of-the-art meth- 198
ods in learning with complementary labels. The loss functions we used or compare in this paper 199
are listed as follows. 1) CCE: The categorical cross-entropy loss, neither symmetric nor bounded, 200
which widely use in machine learning and deep learning due to its fast convergence speed. 2) MAE: 201
The mean absolute error, a symmetric loss and bounded, has been proved Ghosh et al. (2017) to be 202
noise-tolerant. 3) MSE: The mean square error, not symmetric but bounded, widely used in regres- 203
sion learning. 4) GCE: It uses a hyper-parameter q to tune the loss between MAE and CCE, but 204
achieve noise-robust base on its bounded, we used the standard GCE where q=0.7 . 5) GA: Gradi- 205
ent ascent, a learning algorithm for complementary-label learning, is used to tackle the overfitting 206
problem of the unbiased estimator they proposed in Ishida et al. (2019). 6) PC: Pairwise compar- 207
ison (PC) with ramp loss designed for complementary-label learning Ishida et al. (2017). 7) Fwd: 208
Forward correction Patrini et al. (2017), Yu et al. (2018a) designed for learning with complementary 209
labels.	210
Network architecture. Following Ghosh et al. (2017), we use a network architecture that contains 211
five layers to test the above methods for all the experiments: a convolution layer with 32 filters which 212
filter size set as (3,3), a max-pooling layer with pooling-size of (3,3) and strides of (2,2), two fully 213
connected layers with 1024 units, and a fully connected layer with soft-max activated function that 214
the unit number set to the category number for prediction. Rectified Linear Unit (ReLU) is used as 215
the activated function in the network’s hidden layer.	216
Implement details. The implementation detail of our method shows in Alg. 1. We train our network 217
with stochastic gradient descent through back-propagation. Each experiment trains 200 epochs, and 218
the mini-batch size was set to 64. To exploit each loss function’s best performance, we set three start 219
learning rate for each loss function on each experiment and report the best accuracy amongst the 220
three learning rate of each loss function. CCE is set to [1e-3, 5e-4, 1e-4], while GCE, MAE, MSE 221
is set to [1.0, 0.5, 0.1]. The learning rate was halved per 50 epochs.	222
5.2 Experimental results
223
Figure 1: Accuracy for CCE, MAE, GCE, MSE loss functions over epochs, for CIFAR10 dataset
with symmetric complementary labels (SCL) and asymmetric complementary labels (ACL). Leg-
ends are shown in the first sub figures on the first row.
Robustness. As shown in Fig. 1, together with CCE, MAE, and GCE loss, our algorithm achieves 224
strong robust to both symmetric and asymmetric complementary labels, which verify that the robust- 225
ness we prove in the Theorem 1 and Theorem 2. Even though the MAE satisfies the two theorems, 226
7
Under review as a conference paper at ICLR 2021
Table 1: The test accuracy and standard deviation (5 trials) on experiments with loss functions, under
different complementary labels’ distribution assumption, for datasets: MNIST, FASHION-MNIST,
CIFAR10. We report the last ten percent epochs average test accuracy. For fair comparison, the
last three columns’ data are directly copying from Table.2 in Ishida et al. (2019), where GA Ishida
et al. (2019): Gradient Ascent, PC Ishida et al. (2017): Pairwise Comparison, Fwd Yu et al. (2018b):
Forward correction. The top 2 accuracies are boldface.
Loss
Dataset	Distribution	CCE	MAE	GCE	MSE	I	GA	PC	Fwd
	Symmetric	95.66 ± 0.15	93.78 ± 3.66	97.46 ± 0.06	91.58 ± 0.60 I	88.1 ± 2.5	79.3 ± 3.3	88.7 ± 0.3
MNIST	Asymmetric	94.93 ± 0.12	68.11 ± 5.92	97.22 ± 0.12	85.98 ± 0.38 I	-	-	-
	Symmetric	86.43 ± 0.24	74.25 ± 0.26	86.43 ± 0.30	82.93 ± 0.18 I	78.7 ± 1.4	74.7 ± 1.6	77.5 ± 1.2
FASHION	Asymmetric	85.22 ± 0.19	54.01 ± 6.24	85.55 ± 0.12	78.93 ± 0.22 I	-	-	
	Symmetric	44.46 ± 0.31	27.78 ± 2.28	42.64 ± 0.82	36.10 ± 1.23 I	36.8 ± 0.6	33.4 ± 2.0	30.8 ± 1.6
CIFAR10	Asymmetric	37.93 ± 0.70	16.73 ± 0.22	36.01 ± 0.96	30.98 ± 0.74 I	-	-	-
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
it achieves a lower test accuracy than that of CCE and GCE due to it treats all labels the same (not
sensitive to the labels). The subfigures in the last column of Fig. 1 shows that the MSE loss firstly
achieves its highest test accuracy and then drop sharply over the epochs. Because MSE does not
satisfy one of the two theorems we prove, it easily overfits the training set’s complementary labels.
Such a trend is the same as asymmetric complementary labels learning. The results verify that the
algorithm we design for the complementary labels is significant and confirms the theoretical results
we analyzed in the previous section.
Performance Comparison. The first four columns of Table. 1 show that the CCE and GCE loss
achieve the best two test accuracies in our algorithm. In the MNIST dataset, the CCE achieves
a little lower test accuracy than GCE, the same test accuracy in FASHION-MNIST, and a little
higher test accuracy in CIFAR10 due to the dataset more challenge and CCE is more sensitive to
labels. Even MAE is robust to complementary labels, and its performance is not well than others
because it is a linear loss that is not sensitive to labels. As shown in Fig. 1, MSE is not robust to
complementary labels, but with a small learning rate of 0.1, MSE only exhibited slight overfitting in
Table 1. Furthermore, as shown in Table 1, together with CCE and GCE loss, our algorithm achieves
a test accuracy higher than 95% in the MNIST dataset, which is comparable to that of learning with
ordinary labels.
For a fair comparison, The last three columns directly form Ishida et al. (2019) even those results
are the max test accuracy. In the first two datasets, all loss functions with our algorithm achieve a
higher test accuracy than GA, but they used an MLP model as their base model, simpler than ours.
In CIFAR10, they used ResNet-34 (21.62M parameters) He et al. (2016) and DenseNet Huang et al.
(2017) as their based model, which is much bigger than ours (8.43M parameters), but we achieve a
much higher test accuracy than theirs. The results validate the superiority of our algorithm to current
state-of-the-art methods.
6 Conclusion
This paper designs an algorithm for learning from complementary labels using the loss functions
designed for ordinary-label learning. We provide theoretical analysis to show that the loss func-
tions we design for learning from the complementary labels are robust to the complementary labels,
i.e., the optimal classifier learned from the complementary labels can theoretically converge to the
optimal classifier learned from ordinary labels. In this paper, the two theorems we present are the
sufficient condition of a loss function robust to complementary labels. Experimental results show
that though complementary-label learning is a new topic in supervised-learning, it offers excellent
competitiveness. More methods should be studied to improve the performance of complementary
learning in our future works, such as Amid et al. (2019b) and Amid et al. (2019a).
8
Under review as a conference paper at ICLR 2021
References	261
Ehsan Amid, Manfred K. Warmuth, and Sriram Srinivasan. Two-temperature logistic regression 262
based on the tsallis divergence. volume 89 of Proceedings of Machine Learning Research, pp. 263
2388-2396. PMLR, 2019a.	264
Ehsan Amid, Manfred K. K Warmuth, Rohan Anil, and Tomer Koren. Robust bi-tempered logistic 265
loss based on bregman divergences. In Advances in Neural Information Processing Systems, 266
volume 32, pp. 15013-15022, 2019b.	267
Han Bao, Gang Niu, and Masashi Sugiyama. Classification from pairwise similarity and unlabeled 268
data. volume 80 of Proceedings ofMachine Learning Research, pp. 452T61, Stockholmsmassan, 269
Stockholm Sweden, 10-15 Jul 2018. PMLR.	270
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A 271
Raffel. Mixmatch: A holistic approach to semi-supervised learning. In H. Wallach, H. Larochelle, 272
A. Beygelzimer, F. Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information 273
Processing Systems 32, pp. 5049-5059. Curran Associates, Inc., 2019.	274
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for 275
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.	276
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self- 277
supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b. 278
Timothee Cour, Ben Sapp, and Ben Taskar. Learning from partial labels. The Journal of Machine 279
Learning Research, 12:1501-1536, 2011.	280
Marthinus C du Plessis, Gang Niu, and Masashi Sugiyama. Analysis of learning from positive and 281
unlabeled data. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger 282
(eds.), Advances in Neural Information Processing Systems 27, pp. 703-711. Curran Associates, 283
Inc., 2014.	284
Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In 285
Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and 286
Data Mining, pp. 213-220. Association for Computing Machinery, 2008.	287
Lei Feng and Bo An. Leveraging latent label distributions for partial label learning. In IJCAI, pp. 288
2107-2113, 2018.	289
Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust loss functions under label noise for deep 290
neural networks. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, 291
AAAI’17, pp. 1919-1925. AAAI Press, 2017.	292
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- 293
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 294
(CVPR), June 2016.	295
Mengying Hu, Hu Han, Shiguang Shan, and Xilin Chen. Weakly supervised image classification 296
through noise regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision 297
and Pattern Recognition (CVPR), June 2019.	298
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected 299
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern 300
Recognition (CVPR), July 2017.	301
Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from complementary 302
labels. In Advances in neural information processing systems, pp. 5639-5649, 2017.	303
Takashi Ishida, Gang Niu, Aditya Menon, and Masashi Sugiyama. Complementary-label learning 304
for arbitrary losses and models. In International Conference on Machine Learning, pp. 2971- 305
2980. PMLR, 2019.	306
9
Under review as a conference paper at ICLR 2021
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
Ryuichi Kiryo, Gang Niu, Marthinus C du Plessis, and Masashi Sugiyama. Positive-unlabeled
learning with non-negative risk estimator. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 30, pp. 1675-1685. Curran Associates, Inc., 2017.
A Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, University of
Tront, 2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for
scalable image classifier training with label noise. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2018.
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for
training any binary classifier from only unlabeled data. In International Conference on Learning
Representations, 2018.
T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method
for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 41(8):1979-1993, 2019.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi-
supervised learning with ladder networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 3546-3554.
Curran Associates, Inc., 2015.
Tomoya Sakai, Marthinus Christoffel du Plessis, Gang Niu, and Masashi Sugiyama. Semi-
supervised classification based on classification from positive and unlabeled data. volume 70
of Proceedings of Machine Learning Research, pp. 2998-3006, International Convention Centre,
Sydney, Australia, 06-11 Aug 2017. PMLR.
Jafar Tanha. A multiclass boosting algorithm to labeled and unlabeled data. International Journal
of Machine Learning and Cybernetics, 10(12):3647-3665, 2019.
Xuan Wu and Min-Ling Zhang. Towards enabling binary decomposition for partial label learning.
In IJCAI, pp. 2868-2874, 2018.
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama.
Are anchor points really indispensable in label-noise learning? In H. Wallach, H. Larochelle,
A. Beygelzimer, F. Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 32, pp. 6838-6849. Curran Associates, Inc., 2019.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. arXiv 1708.07747, 2017.
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary
labels. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 68-83,
2018a.
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary
labels. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 68-83,
2018b.
10
Under review as a conference paper at ICLR 2021
Sarah Zelikovitz and Haym Hirsh. Improving short text classification using unlabeled background 354
knowledge to assess document similarity. In Proceedings of the seventeenth international confer- 355
ence on machine learning, volume 2000, pp. 1183-1190, 2000.	356
Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi- 357
supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer 358
Vision (ICCV), October 2019.	359
Tong Zhang. Statistical analysis of some multi-category large margin classification methods. Journal 360
of Machine Learning Research, 5(Oct):1225-1251, 2004.	361
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks 362
with noisy labels. In Advances in neural information processing systems, pp. 8778-8788, 2018.	363
11