Under review as a conference paper at ICLR 2021
Laplacian Eigenspaces, Horocycles and Neu-
ron Models on Hyperbolic Spaces
Anonymous authors
Paper under double-blind review
Ab stract
We use hyperbolic Poisson kernel to construct the horocycle neuron model on
hyperbolic spaces, which is a spectral generalization of the classical neuron model.
We prove a universal approximation theorem for horocycle neurons. As a corollary,
we obtain a state-of-the-art result on the expressivity of fa1,p, which is used in
the hyperbolic multiple linear regression. Our experiments get state-of-the-art
results on the Poincare-embedding subtree classification task and the classification
accuracy of the two-dimensional visualization of images.
1 Introduction
Conventional deep network techniques attempt to use architecture based on compositions of simple
functions to learn representations of Euclidean data (LeCun et al., 2015). They have achieved
remarkable successes in a wide range of applications (Hinton et al., 2012; He et al., 2016). Geometric
deep learning, a niche field that has caught the attention of many authors, attempts to generalize
conventional learning techniques to non-Euclidean spaces (Bronstein et al., 2017; Monti et al., 2017).
There has been growing interest in using hyperbolic spaces in machine learning tasks because they
are well-suited for tree-like data representation (Ontrup & Ritter, 2005; Alanis-Lobato et al., 2016;
Nickel & Kiela, 2017; Chamberlain et al., 2018; Nickel & Kiela, 2018; Sala et al., 2018; Ganea
et al., 2018b; Tifrea et al., 2019; Chami et al., 2019; Liu et al., 2019; Balazevic et al., 2019; Yu & Sa,
2019; Gulcehre et al., 2019; Law et al., 2019). Many authors have introduced hyperbolic analogs of
classical learning tools (Ganea et al., 2018a; Cho et al., 2019; Nagano et al., 2019; Grattarola et al.,
2019; Mathieu et al., 2019; Ovinnikov, 2020; Khrulkov et al., 2020; Shimizu et al., 2020).
Spectral methods are successful in machine learning, from nonlinear dimensionality reduction (Belkin
& Partha, 2002) to clustering (Shi & Malik, 2000; Ng et al., 2002) to hashing (Weiss et al., 2009) to
graph CNNs (Bruna et al., 2014) to spherical CNNs (Cohen et al., 2018) and to inference networks
(Pfau et al., 2019). Spectral methods have been applied to learning tasks on spheres (Cohen et al.,
2018) and graphs (Bruna et al., 2014), but not yet on hyperbolic spaces. This paper studies a spectral
generalization of the FC (affine) layer on hyperbolic spaces.
Before presenting the spectral generalization of the affine layer, we introduce some notations. Let
(∙, ∙)e be the inner product, | ∙ | the Euclidean norm, and P an activation function. The Poincare ball
model of the hyperbolic space Hn(n≥2) is a manifold {x∈Rn : |x|<1} equipped with a Riemannian
metric ds2Hn =Pin=1 4(1-|x|2)-2dxi2. The boundary of Hn under its canonical embedding in Rn is
the unit sphere Sn-1. The classical neuron y=ρ((x, w)E +b) is of input x∈Rn, output y∈R, with
trainable parameters w∈Rn, b∈R. An affine layer Rn → Rm is a concatenation of m neurons. An
alternative representation of the neuron x7→ρ((x, w)E+b) is given by 1
x∈Rn 7→ ρ(λ(x, ω)E+b), ω∈Sn-1, λ, b∈R.	(1)
This neuron is constant over any hyperplane that is perpendicular to a fixed direction ω. In Hn , a
horocycle is a n-1 dimensional sphere (one point deleted) that is tangential to Sn-1. Horocycles are
hyperbolic counterparts of hyperplanes (Bonola, 2012). HoroCyCliC waves <χ,ω>H := 2 log ∣1χ-ω∣∣2
are constant over any horocycle that is tangential to Sn-1 at ω. Therefore,
x∈Hn 7→ ρ(λhx, ωiH+b), ω∈Sn-1, λ, b∈R	(2)
1if w 6= (0, . . . , 0), one can take ω = w/|w|, λ = |w|; else, one can take λ = 0 and any ω ∈ Sn-1.
1
Under review as a conference paper at ICLR 2021
Figure 1： (Left) p((∙,ω)e); (middle) ρ(h∙,ω>H); (right) ρ(f1,p(∙)). In this figure, ω=(1,0),
a=(1, 0), p=(0.5, 0), and ρ is tanh. The colorbar represents function values.
generalizes the classical neuron model (1), and a concatenation of finitely many (2) generalizes the
FC (affine) layer. We call (2) a horocycle neuron. Figure 1 (middle) is an example on H2 .
The neuron models in (1, 2) are related to spectral theory because (∙, ω)E (respectively〈•，ω〉H) are
building blocks of the Euclidean (respectively hyperbolic) Laplacian eigenspace. Moreover, many
L2 spaces have a basis given by Laplacian eigenfunctions (Einsiedler & Ward, 2017). On one side,
all Euclidean (respectively hyperbolic) eigenfunctions are some kind of “superposition” of (∙, ω)E
(respectively〈•, ω〉H). On the other side, neural networks based on (1) (respectively (2)) represent
functions that are another kind of “superposition” of (∙, ω)E (respectively〈•，ω〉H). They heuristically
explain why the universal approximation property is likely to hold for networks constructed by (1)
and (2). By using the Hahn Banach theorem, an injectivity theorem of Helgason, and integral formula,
we prove that finite sums of horocycle neurons (2) are universal approximators (Theorem 2).
Let P ∈ Hn, Tp(Hn) be the tangent space of Hn at p, a ∈ Tp(Hn),㊉ be the Mobius addition
(Ungar, 2008). We remind the reader that the following functions
f1 (x) = 2|a|_ Sinh-I (2(-P ㊉ x,a)Eʌ	(3)
fa,p(X)=I- |p|2	(1 - | - p ㊉ x|2)|a|	⑶
are building blocks of many hyperbolic learning tools (Ganea et al., 2018a; Mathieu et al., 2019;
Shimizu et al., 2020). Figure 1 illustrates examples of different neuron models (1, 2, 3) on H2.
In Lemma 1, we shall present a close relationship between (2) and (3). Using this relationship and
Theorem 2, we obtain a novel result on the expressivity of fa1,p (Corollary 1).
This article contributes to hyperbolic learning. We first apply spectral methods, such as the horocycle,
to hyperbolic deep learning. We prove results on the expressivity of horocycle neurons (2) and
fc1,p (3). With horocycle neurons, We obtain state-of-the-art results on the Poincare-embedding
subtree classification task and the classification accuracy of the 2-D visualization of images in in the
experiment.
2	Related work
Universal approximation There is a vast literature on universal approximation (Cybenko, 1989;
Hornik et al., 1989; Funahashi, 1989; Leshno et al., 1993). Cybenko (1989)’s existential approach
uses the Hahn Banach theorem and Fourier transform of Radon measures. To prove Theorem 2, we
also use the Hahn Banach theorem, and additionally an integral formula (7) and an injectivity Theorem
1 of Helgason. Generalizing integral formulas and injectivity theorems is easier than generalizing
Fourier transform of Radon measures on most non-Euclidean spaces. (Carroll & Dickinson, 1989)
uses the inverse Radon transform to prove universal approximation theorems. This method relates to
ours, as injectivity theorems are akin to inverse Radon transforms. However, using the injectivity
theorem is an existential approach while using the inverse Radon transform is a constructive one.
Spectral methods Spectral methods in Bronstein et al. (2017); Bruna et al. (2014); Cohen et al.
(2018) use a basis ofL2(X) given by eigenfunctions, where X is a finite graph or the sphere. Because
L2(Hn) has no eigenfunctions as a basis, our approach is different from theirs.
Hyperbolic deep learning One part of hyperbolic learning concerns embedding data into the hy-
perbolic space (Nickel & Kiela, 2017; Sala et al., 2018). Another part concerns learning architectures
with hyperbolic data as the input (Ganea et al. (2018a); Cho et al. (2019)). Ganea et al. (2018a)
proposes two ways to generalize the affine layer on hyperbolic spaces： one by replacing the linear and
bias part of an affine map with (25, 26) of their paper; another one by using a concatenation of fa1,p in
2
Under review as a conference paper at ICLR 2021
Figure 2: (Left) AiO are pairwise parallel lines in the sense of Gauss (Bonola, 2012); (Middle) A
family of horocycles tangential to O ; (Right) Each Horocycle is a locus of corresponding points
(Bonola, 2012)[p.73] of parallel lines. This fact justifies that horocycles are analogs of hyperplanes.
their hyperbolic multiple linear regression (MLR). The latter seems more relevant to ours. A level set
of fa1,p is a hypercycle that has the same distance to a chosen geodesic hypersurface, while a level set
of a horocycle neuron is a horocycle that has the same “spectral” distance to an ideal point at infinity.
Based on functions similar to fa1,p, Mathieu et al. (2019); Shimizu et al. (2020) build the gyroplane
layer and Poincare FC layer. Ganea et al. (2018a); Cho et al. (2019) take geodesics as decision
hyperplanes, while we (initially) take horocycles. We shall construct the horocycle multiple linear
regression (MLR), where decision hypersurfaces are geodesics. Geodesics decision hyperplanes
(Ganea et al., 2018a; Cho et al., 2019) and geodesic decision hypersurfaces here arise from different
methods. Khrulkov et al. (2020) investigates hyperbolic image embedding, where prototypes (or
models) of each class are center-based. We study a different one, and we shall call our prototypes
end-based.
3	Hyperbolic spaces
This section reviews facts from hyperbolic geometry that are used in the proof of Theorem 2. For the
reader who is not interested in the proof, (4) is enough for the implementation.
Hyperbolic metric We use the Poincare model. The hyperbolic space Hn (n≥2) is the manifold
{x∈Rn : |x|<1} equipped with a Riemannian metric ds2 = in=1 4(1-|x|2)-2dxi2. Let o be the
origin of Hn. The distance function dHn satisfies dHn (o, x)=2 arctanh |x|.
Geodesics, horocycles and corresponding points Geodesics in Hn are precisely circular arcs
that are orthogonal to Sn-1. Horocycles in Hn are precisely (n-1)-dimensional spheres that are
tangential to Sn-1 (Helgason, 1970). Horocycles are hyperbolic analogs of hyperplanes. Figure 2
illustrates geodesics and horocycles on H2 .
Hyperbolic Poisson kernel The Poisson kernel for Hn is P (x, ω)=
x∈Hn,ω∈Sn-1 (Helgason (1970)[p.108]). The function h∙,ω)H defined by
1	1	1 - |x|2
hx,ωiH = 2(n-i)log P (∙,ω) = 2log ∣x⅛
1-|x|
2 n-1
|x 一ω∣2
, where
(4)
is constant over any horocycle that is tangential to Sn-1 at ω (Figure 1 (middle), (6)).
Riemannian volume The Riemannian volume induced by the metric ds2 on Hn is
dVol =2n(1- |x|2)-ndx1...dxn.
(5)
Horocycles Let Ξ be the set of horocycles of Hn , and let Ξω be the set of all horocycles that
are tangential to Sn-1 at ω. Given λ∈R, we let ξλ,ω be the unique horocycle that connects ω and
tanh (λ∕2) ∙ ω. We have Ξω = ∪λ∈R{ξλ,ω} and Ξ = ∪ω∈Sn-ι Ξω. The length of any geodesic (that
ends at ω) line segment cut by ξλ1 ,ω and ξλ2,ω equals ∣λ1 - λ2∣ (A.2). Therefore ∣λ1 - λ2∣ is a
natural distance function defined on Ξω , and the map λ → ξλ,ω is an isometry between R and Ξω .
This isometry is closely related to h∙, ωiH (A.3): for any x ∈ ξλ,ω,
hx, ωiH = λ∕2.	(6)
The annoying ∕2 in (6) is a tradeoff that the metric here is different from that in Helgason (2000).
3
Under review as a conference paper at ICLR 2021
Integral formula For fixed ω ∈ Sn-1, Hn=∪λ∈Rξλ,ω. Let dVol ξλ,ω be the measure induced by
ds2 on ξλ,ω. Let L be a family of geodesics that end at ω, δ > 0, and U=L ∩ (∪λ≤α≤λ+δξα,ω). For
l ∈ L, dH(l ∩ ξλ,ω, l ∩ ξλ+δ,ω)=δ (A.2), hence dVol(U) = δ ∙ dVol&入心(U ∩ ξλ,ω) and therefore
Z f(x)dVol(x) = Z Z f (z)dVol ξλ,ω (z) dλ.	(7)
The above proof (for Hn) is essentially the same as that in (Helgason, 2000)[p.37] (for H2). To
further convince the reader that (7) holds for all n, we give another simple proof in A.4.
Injectivity theorem With respect to the canonical measure on Ξ, Helgason (1970)[p.13] proved
Theorem 1 (Helgason). If f ∈ L1(Hn) and Rξ f(z)dVolξ(z) = 0 for a.e ξ ∈ Ξ, then f = 0 a.e..
Theorem 1 demonstrates that if the integral of f ∈ L1(Hn) over almost every horocycle is zero then
f is also zero. This theorem and the integral formula (7) are essential for the proof of Theorem 2.
4 Learning architectures and eigenfunctions of the Laplacian
In this section, we discuss a heuristic connection between the representation properties of eigenfunc-
tions and classical neurons, and then we define some horocycle-related learning tools.
4.1	Eigenspaces and neuron models
On a Riemannian manifold X, the Laplace-Beltrami LX is the divergence of the gradient, and it has
a well-known representation property (Einsiedler & Ward, 2017): if X is a compact Riemannian
manifold or bounded domain in Rn, then L2(X) has a basis given by eigenfunctions. This statement
isfalseifXisRn orHn (Hislop, 1994).
Eigenspaces of on Rn and Hn Our work is motivated by the theory of eigenspaces, in which
Euclidean (respectively hyperbolic) eigenfunctions are obtained from (x, ω)E (respectively hx, ωiH)
by some kind of superposition. For example, all smooth eigenfunctions of LRn are precisely the
functions (M. Hashizume & Okamoto, 1972)[p.543]
f(x) =
Sn
eλ(x,ω)E dT (ω),
and eigenfunctions of LHn are precisely the functions (Helgason, 1970)[Theorem 1.7, p.139]
f(x) =
Sn
eλhx,ωiH dT (ω),
(8)
(9)
where T in (8) and (9) are some technical linear forms of suitable functional spaces on Sn-1
Neuron models By (8) and (1), Euclidean eigenfunctions (respectively classical neurons) are
superpositions of (∙, ω)e and exp (respectively ρ), with homogeneity and additivity. By (9) and (2),
hyperbolic eigenfunctions (respectively horocycle neurons) are superpositions of h∙, ω)H and exp
(respectively ρ). The representation property of eigenfunctions on compact manifolds and bounded
domains suggests that the universal approximation property is likely to hold for networks constructed
by (∙, ω)e or(•，ω)H. However, this heuristic is not proof (A.5).
4.2	Horocycle based learning architectures
Horocycle neuron In the implementation of the horocycle neuron (2), we take 2 log Q-^^^ + c)
for hx, ωiH, where is a small constant to ensure numerical stability. For updating ω, we use the
sphere optimization algorithm (Absil et al., 2008; Bonnabel, 2013) (A.6).
Horocycle feature and horocycle decision hypersurface Given a non-origin point x ∈ Hn , for
y ∈ Hn we define hx(y) = hy, x/|x|iH and call it the horocycle feature attached to x. This feature
is useful in the Poincare embedding subtree classification task (see the experiment and Figure 3[left]).
The horocycle is the hyperbolic analog of the Euclidean hyperplane, and therefore it could be a
possible choice of decision hypersurface, which may arise from a level set of a horocycle feature.
4
Under review as a conference paper at ICLR 2021
Figure 3: (Left) Horocycle decision hypersurface. Colored points form a Poincare embedding of
WordNet nouns in H2. The yellow point is group.n.01. Points from the subtree rooted at group.n.01
are in red, and the rest are in blue. The green horocycle separates the blue and the red. (Right)
End-based clustering and geodesic decision hypersurfaces. It is an embedding of CIFAR-10 in H2 .
Different classes are in different colors. Thin diamonds on S 1 are prototypes. Decision regions are
separated by geodesic decision hypersurfaces.
End-based clustering and end prototype Natural clustering is a topic in representation learning
(Bengio et al., 2013), and the common prototype-based clusters are center-based (Tan et al., 2005).
We propose a type of clustering that embeds high-dimensional data in Hn and places prototypes
in Sn-1. Figure 3[right] is an example for n = 2. For ω ∈ Sn-1 and any b ∈ R, the function
X ∈ Hn → - log ( 1Txl∣2 ) + b measures the relative distance of Hn from ω in Gromov,s bordification
ιx-ωr)
theory (Bridson & Haefliger (2009)[II.8], A.18). Moreover, we define Dist : Hn × Sn-1 × R → R by
Dist(x, ω, b) = - log
(ι-∣χ∣2、
|x - ω ∣2
+ b = -2hx, ωiH + b.
(10)
It is a relative distance function, and this is why Dist may assume negative values and why
there is a bias term b in (10). Consider classes Cls = {C1 , C2, . . . , CM} and labeled training
examples {(X1, Y 1), . . . , (XN, Y N)}, where Xi ∈ RD are D-dimensional input features and
Yi ∈ {1, 2, . . . , M}. Each example Xi belongs to the class CYi. In light of (10), our goal is to find
a neural network NNθ : RD → Hn that is parameterized by θ, prototypes ω1 , . . . , ωM ∈ Sn-1, and
real numbers b1 , . . . , bM ∈ R such that
# 1 1≤i≤N : Yi = arg min (Dist(NNθ(Xi),ωj,bj
I	1≤j≤M
N
(11)
is maximized. We call {NNθ (Xj ) : 1 ≤ j ≤ N } the end-based clustering and ωi end prototypes (in
hyperbolic geometry, the end is an equivalence class of parallel lines in Figure 2[left]). In experiments,
we take NNθ = Exp ◦ NN0θ, where NN0θ : RD → Rn is a standard neural network parameterized
by θ and Exp : Rn → Hn is the exponential map of the hyperbolic space.
Horocycle layer, horocycle multiple linear regression (MLR) and geodesic decision hypersur-
faces We call a concatenation of (2) a horocycle layer, and we shall carefully describe a prototypical
learning framework for end-based clusterings. Using the same notions as in the previous paragraph,
the classification task has M classes, and NNθ = Exp ◦ NN0θ : RD → Hn is a deep network. For
prototypes ω1, . . . , ωM ∈ Sn-1, real numbers b1, . . . , bM ∈ R, and any example X, our feedforward
for prediction will be
x=NNθ(X),
SCj(X) = -Dist(x, ωj , bj),
X ∈ Carg max(SCj (X)) .
1≤j≤M
(Feature descriptor)
(Scores; Similarity)
(Classifier)
The goal is to maximize the accuracy (11), and then we need a loss function for the backpropagation.
Following the convention of prototypical networks (Snell et al., 2017; Yang et al., 2018), we choose
an increasing function ρ (in our experiments, ρ(x) = x or ρ = tanh. 2) and let the distribution over
classes for an input X (with label Y ) be
pθ(Y = CjIX) Y e-ρ(Dist(NNo (X ),mj,bj )) = e-ρ(-SCj(X )).
2One often takes ρ(x) = x2 in metric learning, which is improper here because Dist(x) could be negative.
5
Under review as a conference paper at ICLR 2021
Figure 4: Prediction probabilities of classifiers. Suppose the classification task has 10
classes. Let NNθ : RD →R2 be the feature descriptor, X the input, and x=NNθ (X). Let
Wi=(Cos ((i-1)π∕5),sin ((i-1)π∕5))(1≤i≤10). (Left) pθ(Y = C1 |x) on R2 when score functions
are Poisson neurons SCj(X) = BatchNorm(Pρw ,-1,0(x)). (Right) pθ(Y = C1 |x) on R2 when score
functions are SCj (X) = (x, wj )E .
Therefore, given a batch of training examples, the loss function is
-P(X j,Yj )∈ Batch log pθ (Y = CYj |Xj )
#BatCh
(12)
The training proceeds by minimizing L, and we call this framework a horocycle MLR. The set of
parameters of the framework is {θ} ∪ {ω1, . . . , ωM} ∪ {b1, . . . , bM}. It is worth mentioning that
decision boundaries of the horocycle MLR are geodesics, which follows from
SCi (X)=sCj (X)—log( ∣1-χ∣ι)-bi=log( II-ω⅞) -% Q Y=e S
|x ωi ∣	∣x ωj ∣	∣x ωj ∣
and the theorem of Apollonian circles (A.7).
Poisson neuron and Poisson multiple linear regression (MLR) Although hx, ωiH (4) is well-
motivated by the theory of eigenspaces (9) and fits naturally into metric learning (see 10 or also
Corollary 1), it is only defined on Hn . Some readers might not be convinced that the neuron has to
be defined on hyperbolic spaces. Therefore, we try to remove the log in (4) and define the Poisson
neuron model by PW,λ,b(x) = P (λ lwX工χ2l2 + b) for W ∈ Rn, λ,b ∈ R, which is well-defined on
Rn∖{w}. Notice that if |x| < |w| then lwχ2-Wχ2l2 = e2hx/|w|,w/|w|iH. In A.8, Figure 7 illustrates an
example of a Poisson neuron on R2. In the implementation, we take |W—w-Lx+： for lwX--Wχ2l2, where e
is a small constant for numerical stability. We call a concatenation of Poisson neurons a Poisson layer,
and we use it with a deep neural network NNθ : RD → Rn to construct the Poisson MLR, which
is similar to the horocycle MLR. Let w1 , . . . , wM ∈ Rn and b1 , . . . , bM ∈ R, the feedforward for
prediction of our framework is
x=NNθ(X),SCj(X) = BatChNorm(Pρw -1b (x)), X ∈ Carg max(SCj (X)).	(13)
j, , j	1≤j≤M
We let the pθ(Y = Cj ∣X) Y eSCj (X) and take (12) as the loss. This framework is called a Poisson
MLR. We use the usual optimization algorithms to update parameters in the Poisson neuron. The
BatchNorm(Ioffe & Szegedy, 2015) seems crucial for (13) in the experiment. Figure 4 illustrates
that high-confidence prediction regions (deep red areas) of the Poisson MLR are compact sets, in
contrast to classical classifiers Hein et al. (2019)[Theorem 3.1]. We shall use this figure to explain an
experiment in Section 6.4.
5	Representational power
In this section, ρ is a continuous sigmoidal function (Cybenko, 1989), ReLU(Nair & Hinton, 2010),
ELU(Clevert et al., 2016), or Softplus(Dugas et al., 2001). We remind the reader that ρ is sigmoidal
if lim ρ(t) = 1 and lim ρ(t) = 0. The following theorem justifies the representational power of
t→∞	t→-∞
horocycle neurons.
Theorem 2. Let K be a compact set in Hn, and 1≤p<∞. Then finite sums of the form
N
F(x) =	αiP(λihx,ωiiH+bi), ωi∈Sn-1, αi, λi, bi∈R	(14)
i=1
are dense in Lp (K, μ), where μ is either d Vol (5) or the induced Euclidean volume.
6
Under review as a conference paper at ICLR 2021
We provide a sketch of the proof here and go through the details in A.9. It suffices to prove the theorem
for a sigmoidal function P and μ = dVol, as other cases follow from this one. Assume that these
finite sums are not dense in Lp(K, dVol). By the Hahn-Banach theorem, there exists some nonzero
h∈Lq (K, dVol), where q=p/(p - 1) if p>1 and q=∞ if p=1, such that K F (x)h(x)dVol (x) = 0
for all finite sums of the form (14). Extend h to be a function H that is defined on Hn by assigning
H (x)=h(x) if x∈K and H(x)=0 if x∈Hn \K. Using the property of sigmoidal functions, the
bounded convergence theorem, and the integral formula (7), we prove that the integration of H on
almost every horocycle is zero. By the injectivity Theorem 1, H is almost everywhere zero, which
contradicts our assumption and completes the proof.
In A.10, we shall prove the same result for Poisson neurons. In A.11, we prove the following lemma,
which demonstrates a close relationship between horocycle neurons and the widely used fa1,p (3).
Lemma 1. Let K be a compact set in	Hn, ω ∈	Sn-1, and	>	0.	There are c, d ∈	R,	p ∈	Hn,	and
a ∈ Tp(Hn) such that the function D(x) = cfa1,p(x) + d - hx, ωiH satisfies ||D||Lp(K,dVol) < .
This lemma suggests that h∙, ω)H is a boundary point of some “compactification” of the space of f1,p.
The above lemma together with Theorem 2 implies
Corollary 1. Let K be a compact set in Hn and 1≤p<∞. Finite sums of the form
N
F (x) = X αiρ(cifai ,pi (x) + di), pi ∈ H , ai ∈ Tpi (H ), αi , ci , di ∈ R,
i=1
are dense in Lp (K, μ), where μ = d Vol or μ is the induced Euclidean volume.
This result provides novel insights into the hyperbolic neural network (Ganea et al., 2018a), gyroplane
layer (Mathieu et al., 2019), and Poincare FC layer (Shimizu et al., 2020). Although level sets of f1,p
are hypercycles, our proof of Lemma 1 relies on the theory of horocycles. It would be interesting to
have more natural approaches to treat the expressivity of fa1,p.
6	Experiments
In this section, we first play with the MNIST toy. Next, we apply a horocycle feature to the Poincare
embedding subtree classification task. After that, we construct 2-D clusterings of image datasets
by using the horocycle MLR. Finally, we provide evidence for further possible applications of the
Poisson MLR. We use the framework or some functions of Tensorflow, Keras, and scikit-learn (Abadi
et al., 2015; Chollet et al., 2015; Pedregosa et al., 2011).
6.1	MNIST
The MNIST (LeCun et al., 1998) task is popular for testing hyperbolic learning tools (Ontrup &
Ritter, 2005; Nagano et al., 2019; Mathieu et al., 2019; Grattarola et al., 2019; Ovinnikov, 2020;
Khrulkov et al., 2020). We train two different classifiers. A.12, A.14, and code contain details. The
first one is a single horocycle layer followed by the softmax classifier. The average test error rate after
600 epochs is 1.96%, and Theorem 2 provides the rationale for this experiment (A.13). The second
one is a Poisson MLR. It is the best hyperbolic geometry related MNIST classifier (Table 1). In this
table, Ontrup & Ritter (2005) uses the hyperbolic SOM, Grattarola et al. (2019) uses the adversarial
autoencoder, and Khrulkov et al. (2020) uses the hyperbolic MLR. Our experiment performs well
on MNIST suggests that horocycle and Poisson neurons are computationally efficient and easily
coordinate with classical learning tools (such as the convolutional layer and the softmax).
Table 1: Test error rates of hyperbolic geometry related MNIST classifiers
Ontrup & Ritter (2005)	Grattarola et al. (2019) Khrulkov et al. (2020) This paper
5.4%	42%	1%	0.35%
7
Under review as a conference paper at ICLR 2021
6.2	POINCAR宣 EMBEDDING SUBTREE CLASSIFICATION
Given a Poincare embedding (Nickel & Kiela, 2017) PE : {WordNet noun} → HD of 82114
nouns and given a node x ∈ {WordNet noun}, the task is to classify all other nodes as being
part of the subtree rooted at x (Ganea et al., 2018a). Our model is logistic regression, where
the horocycle feature p ∈ {WordNet noun} 7→ hPE(x) (P E (p)/s) (s is a hyperparameter lying in
[1, 1.5]) is the only predictor, and the dependent variable is whether p is in the subtree rooted at
x. The decision hypersurface of this model is a horocycle, as illustrated in Figure 3 (left). In the
experiment, we pre-train three different Poincare embeddings3 in each of H2, H3, H5, H10. For each
x ∈ {animal, group, location, mammal, worker} and D ∈ {2, 3, 5, 10}, we randomly select one of
three pre-trained Poincare embedding PE : {WordNet noun} → HD and then test the model.
Table 2 reports the F1 classification scores and two standard deviations of 100 trials for each {x, D}.
Different Poincare embeddings account for the most variance of the performance. Our model is
different from the existing ones. Firstly, we take the horocycle as the decision hypersurface, while
others take the geodesic. Secondly, we train a logistic regression on top of the horocycle feature
attached to PE(x), which is efficiently calculated, while others train the hyperbolic MLR with
different parametrizations. On the number of parameters, we have three (independent of D), Ganea
et al. (2018a) has 2D, and Shimizu et al. (2020) has D + 1. The number of parameters explains why
our model is prominent in low dimensions.
Table 2: Average test F1 classification scores (%) for five subtrees of WordNet noun tree
RootNode	Model	H2	H3	H5	H10
animal.n.01	Ganea et al. (2018a)	47.43±1.07	91.92±0.61	98.07±0.55	99.26±0.59
4016 nodes	Shimizu et al. (2020)	60.69±4.05	67.88±1.18	86.26±4.66	99.15±0.46
	This paper	94.32±5.33	86.98±2.05	98.57±4.98	98.76±2.43
group.n.01	Ganea et al. (2018a)	81.72±0.17	89.87±2.73	87.89±0.80	91.91±3.07
8376 nodes	Shimizu et al. (2020)	74.27±1.50	63.90±6.46	84.36±1.79	85.60±2.75
	This paper	90.08±12.41	90.91±11.62	97.52±1.15	97.85±1.92
location.n.01	Shimizu et al. (2020)	42.60±2.69	66.70±2.67	78.18±5.96	92.34±1.84
3362 nodes	This paper	93.19±11.76	94.66±4.65	95.23±4.11	97.37±1.75
mamal.n.01	Ganea et al. (2018a)	32.01±17.14^^	87.54±4.55	87.73±3.22	91.37±6.09
1181 nodes	Shimizu et al. (2020)	63.48±3.76	94.98±3.87	99.30±0.30	99.17±1.55
	This paper	98.74±1.93	96.37±2.42	93.34±8.86	95.80±1.62
worker.n.01	Ganea et al. (2018a)	12.68±0.82	24.09±1.49	55.46±5.49	66.83±11.38
1115 nodes	This paper	94.47±4.30	92.53±6.36	95.47±2.71	94.75±2.18
6.3	End-based clustering for 2D dimension reduction
In this experiment, we use the horocycle MLR (Section 4.2) to construct end-based clusterings
NNθ : RD → H2 for MNIST, Fashion-MNIST(Xiao et al., 2017), and CIFAR-10(Krizhevsky, 2012).
We take NNθ = Exp ◦ NN0θ , where Exp is the exponential map of H2 and NN0θ : RD → R2 is a
network with four convolutional blocks for MNIST/Fashion-MNIST or a ResNet-32 structure for
CIFAR-10. A.16 and code contain details.
Figure 5: End-based clusters of MNIST, Fashion-MNIST, and CIFAR-10. Test error rates from the
left to the right: 0.63%, 9.98%, 8.42%. The performance for CIFAR-10 is better than Fashion-MNIST
because NN0θ is of a ResNet-32 structure for CIFAR-10 but of only four convolutional layers for
Fashion-MNIST. Thin diamonds are prototypes, and shallow areas are decision regions.
3https://github.com/dalab/hyperbolic_cones
8
Under review as a conference paper at ICLR 2021
Figure 5 illustrates end-based clusterings for MNIST, Fashion-MNIST, and CIFAR-10, with perfor-
mance reported in the caption. Our accuracy for Fashion-MNIST is 8% higher than all numbers
presented in McInnes et al. (2020). Moreover, Table 3 compares the numbers of Yang et al. (2018);
Ghosh & Kirby (2020), and ours for MNIST, and our methods are similar. We all use convolutional
networks as the (Feature descriptor) and prototype-based functions as the loss. However, Yang et al.
(2018); Ghosh & Kirby (2020) use the center-based prototype loss, while we use the end-based (12).
Yang et al. (2018)[Figure 1] points out that the traditional CNN is good at linearly separating feature
representations, but the learned features are of large intra-class variations. The horocycle MLR leads
to the inter-class separability in the same way (angle accounts for label difference) a traditional CNN
does. At the same time, it also obtains intra-class compactness (Figure 5).
Table 3: Test error rates on 2D embedded MNIST by dimensionality reduction techniques
GerDA CentroidenCoder dG-MCML dt-MCML CPL GCPL This paper
3.2%	261%	213%	203%	072%^^067%^^0.63%
6.4 Poisson MLR
Using a Poisson MLR whose feature desCriptor is a ResNet-32 struCture, we obtain a Classifier
with a test error rate of 6.46% on CIFAR-10. It is on par with other methods with similar network
struCtures (Yang et al., 2018). Moreover, we apply Poisson MLR to the ClassifiCation task of
flowers (Tensorflow), whiCh is a typiCal example of overfitting. ReplaCing the MLR part of the
Keras model (Tensorflow) with a Poisson MLR, the new Poisson model shows better generalization
performanCe (Figure 6). A.17 and Code Contain the details. This subseCtion provides evidenCe for
further appliCations of horoCyCles.
Figure 6: Keras’MLR and our Poisson MLR on the task of flowers. Figure 4 explains this experiment.
In the earlier epoChs of the training, feature veCtors NNθ(X) of the Poisson model are not even Close
to its CompaCt high-ConfidenCe prediCtion regions (deep red areas), and therefore on the test data, it is
not muCh better than a random guess. In the end, feature veCtors lying in these CompaCt regions are
of small intra-Class variations, whiCh is good for generalization (Yang et al., 2018).
7 Conclusion
Based on the speCtral theory of hyperboliC spaCes, we introduCe several horoCyCle-related learning
tools. They find applications in the hyperbolic neural networks, the Poincare embedding subtree
ClassifiCation task, and the visualization and ClassifiCation of image datasets. We give an existential
proof of a universal approximation theorem for shallow networks constructed by horocycle neurons
or fa1,p. Hopefully, it will trigger further research on the expressivity problems, such as constructive
approaches, quantitative results, and benefit of depth (Mhaskar & Poggio, 2016), on horocycle
neurons, fa1,p, and similar functions on more general manifolds.
9
Under review as a conference paper at ICLR 2021
References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz,
L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas,
O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015. URL http://tensorflow.org/.
Software available from tensorflow.org.
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton
University Press, 2008.
G. Alanis-Lobato, G. Mier, and M.A. Andrade-Navarro. Efficient embedding of complex networks to
hyperbolic space via their Laplacian. Scientific Reports, 6, 2016.
V. I. Arnold. On functions of three variables. In Collected Works. Vladimir I.Arnold-Collected Works.
Springer, 2009.
I. Balazevic, C. Allen, and T. Hospedales. Multi-relational Poincare graph embeddings. In Advances
in Neural Information Processing Systems 32, pp. 4463-4473. Curran Associates, Inc., 2019.
M. Belkin and N. Partha. Laplacian eigenmaps and spectral techniques for embedding and cluster-
ing. In T. G. Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information
Processing Systems 14, pp. 585-591. MIT Press, 2002.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798-1828, August 2013. ISSN 0162-8828.
S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on Automatic
Control, 58(9):2217-2229, 2013.
R. Bonola. Non-Euclidean Geometry. Dover Books on Mathematics. Dover Publications, 2012.
ISBN 9780486155036.
M. Bridson and A. Haefliger. Metric Spaces of Non-Positive Curvature, volume 319. 01 2009. ISBN
978-3-642-08399-0. doi: 10.1007/978-3-662-12494-9.
M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. Geometric deep learning:
Going beyond Euclidean data. IEEE Signal Processing Magazine, 34(4):18-42, 2017.
J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. Spectral networks and locally connected networks
on graphs. In International Conference on Learning Representations (ICLR2014), CBLS, April
2014, 2014.
S. Carroll and B. Dickinson. Construction of neural nets using the Radon transform. International
1989 Joint Conference on Neural Networks, pp. 607-611 vol.1, 1989.
B.P. Chamberlain, J.R. Clough, and M.P. Deisenroth. Hybed: Hyperbolic neural graph embedding,
2018.
I. Chami, Z. Ying, C. Re, and J. Leskovec. Hyperbolic graph convolutional neural networks. In
Advances in Neural Information Processing Systems 32, pp. 4868-4879. Curran Associates, Inc.,
2019.
H. Cho, B. DeMeo, J. Peng, and B. Berger. Large-margin classification in hyperbolic space. In
Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings of Machine Learning Research,
volume 89 of Proceedings of Machine Learning Research, pp. 1832-1840. PMLR, 16-18 Apr
2019.
F.	Chollet et al. Keras. https://keras.io, 2015.
D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by
exponential linear units (ELUs). In Proceedings of the International Conference on Learning
Representations(ICLR 2016), 2016.
10
Under review as a conference paper at ICLR 2021
T. S. Cohen, M. Geiger, J. Kohler, and M. Welling. Spherical CNNs. In International Conference on
Learning Representations, 2018.
G.	Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2:303-314,1989.
C. Dugas, Y. Bengio, F. B6lisle, C. Nadeau, and R. Garcia. Incorporating second-order functional
knowledge for better option pricing. In T. K. Leen, T. G. Dietterich, and V. Tresp (eds.), Advances
in Neural Information Processing Systems 13, pp. 472-478. MIT Press, 2001.
M. Einsiedler and T. Ward. Functional Analysis, Spectral Theory, and Applications, volume 276 of
Graduate Texts in Mathematics. Springer International Publishing, Cham, 2017. ISBN 978-3-319-
58539-0. doi: 10.1007/978-3-319-58540-6.
K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural
Networks, 2(3):183-192, 1989.
O.-E. Ganea, G. Becigneul, and T. Hofmann. Hyperbolic neural networks. In S. Bengio, H. Wal-
lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 31, pp. 5345-5355. Curran Associates, Inc., 2018a.
O.-E. Ganea, G. Becigneul, and T. Hofmann. Hyperbolic entailment cones for learning hierarchical
embeddings. volume 80 of Proceedings of Machine Learning Research, pp. 1646-1655. PMLR,
10-15 Jul 2018b.
T. Ghosh and M. Kirby. Supervised dimensionality reduction and visualization using centroid-encoder,
2020.
F. Girosi and T. Poggio. Representation properties of networks: Kolmogorov’s theorem is irrelevant.
Neural Computation - NECO, 1:465-469, 12 1989.
D. Grattarola, L. Livi, and C. Alippi. Adversarial autoencoders with constant-curvature latent
manifolds. Appl. Soft Comput., 81, 2019.
C. Gulcehre, M. Denil, M. Malinowski, A. Razavi, R. Pascanu, K. M. Hermann, P. Battaglia, V. Bapst,
D. Raposo, A. Santoro, and N. de Freitas. Hyperbolic attention networks. In International
Conference on Learning Representations, 2019.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
M. Hein, M. Andriushchenko, and J. Bitterwolf. Why ReLU networks yield high-confidence
predictions far away from the training data and how to mitigate the problem. 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 41-50, 2019.
S. Helgason. A duality for symmetric spaces with applications to group representations. Adv. Math.,
5:1-154, 1970.
S. Helgason. Groups and Geometric Analysis: Integral Geometry, Invariant Differential Operators,
and Spherical Functions. Mathematical surveys and monographs. American Mathematical Society,
2000. ISBN 9780821826737.
D. Hilbert. Mathematische probleme. In Dritter Band: Analysis ∙ Grundlagen der Mathematik ∙
Physik Verschiedenes. Springer, 1935.
G.E. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97, 2012.
P. Hislop. The geometry and spectra of hyperbolic manifolds. Proceedings of the Indian Academy of
Sciences - Mathematical Sciences, 104:715-776, 1994.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approxi-
mators. Neural Networks, 2(5):359-366, 1989.
11
Under review as a conference paper at ICLR 2021
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In Proceedings of the 32nd International Conference on International
Conference on Machine Learning - Volume 37, ICML'15, pp. 448-456.JMLR.org, 2015.
V. Khrulkov, L. Mirvakhabova, E. Ustinova, I. Oseledets, and V. Lempitsky. Hyperbolic image
embeddings. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 6417-6427, 2020.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun
(eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,
USA, May 7-9, 2015, Conference Track Proceedings, 2015.
A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05
2012.
M. Law, R. Liao, J. Snell, and R. Zemel. Lorentzian distance learning for hyperbolic representations.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
3672-3681, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436-444, 2015. doi: 10.1038/
nature14539.
M. Leshno, V. Ya. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a
nonpolynomial activation function can approximate any function. Neural Networks, 6(6):861-867,
1993.
Q. Liu, M. Nickel, and D. Kiela. Hyperbolic graph neural networks. In Advances in Neural
Information Processing Systems 32, pp. 8230-8241. Curran Associates, Inc., 2019.
K.	Minemura M. Hashizume, A. Kowata and K. Okamoto. An integral representation of an eigen-
function of the Laplacian on the Euclidean space. Hiroshima Math. J., 2:535-545, 1972.
E. Mathieu, C. Le Lan, C. J. Maddison, R. Tomioka, and Y.W. Teh. Continuous hierarchical
representations with POincare variational auto-encoders. In Advances in Neural Information
Processing Systems 32, pp. 12565-12576. Curran Associates, Inc., 2019.
L.	McInnes, J. Healy, and J. Melville. UMAP: Uniform manifold approximation and projection for
dimension reduction, 2020.
P. Mettes, E. van der Pol, and C. Snoek. Hyperspherical prototype networks. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32, pp. 1487-1497. Curran Associates, Inc., 2019.
H. Mhaskar and T. A. Poggio. Deep vs. shallow networks : An approximation theory perspective.
ArXiv, abs/1608.03287, 2016.
F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein. Geometric deep learning
on graphs and manifolds using mixture model CNNs. 2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 5425-5434, 2017.
Y. Nagano, S. Yamaguchi, Y. Fujita, and M. Koyama. A wrapped normal distribution on hyperbolic
space for gradient-based learning. In ICML, 2019.
V. Nair and G.E. Hinton. Rectified linear units improve restricted Boltzmann machines. In Pro-
ceedings of the 27th International Conference on International Conference on Machine Learning,
ICML’10, pp. 807-814, 2010.
A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In T. G.
Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing
Systems 14, pp. 849-856. MIT Press, 2002.
12
Under review as a conference paper at ICLR 2021
M. Nickel and D. Kiela. Poincare embeddings for learning hierarchical representations. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 30, pp. 6338-6347. Curran Associates, Inc., 2017.
M. Nickel and D. Kiela. Learning continuous hierarchies in the Lorentz model of hyperbolic geometry.
In ICML, 2018.
R.H. Nielsen. Kolmogorov’s mapping neural network existence theorem. In Proceedings of the IEEE
First International Conference on Neural Networks (San Diego, CA), volume III, pp. 11-13, 1987.
J. Ontrup and H. Ritter. A hierarchically growing hyperbolic self-organizing map for rapid structuring
of large data sets. 2005.
I. Ovinnikov. Poincare wasserstein autoencoder, 2020.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
D. Pfau, S. Petersen, A. Agarwal, D. G. T. Barrett, and K. L. Stachenfeld. Spectral inference networks:
Unifying deep and spectral learning. In International Conference on Learning Representations,
2019.
F. Sala, C. De Sa, A. Gu, and C. Re. Representation tradeoffs for hyperbolic embeddings. volume 80
of Proceedings of Machine Learning Research, pp. 4460-4469. PMLR, 10-15 Jul 2018.
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 22(8):888-905, 2000.
R. Shimizu, Y. Mukuta, and T. Harada. Hyperbolic neural networks++, 2020.
J. Snell, K. Swersky, and R.S. Zemel. Prototypical networks for few-shot learning. CoRR,
abs/1703.05175, 2017. URL http://arxiv.org/abs/1703.05175.
P.-N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining. Addison Wesley, May 2005.
Tensorflow. Image classification. https://www.tensorflow.org/tutorials/images/
classification.
A. Tifrea, G. Becigneul, and O.-E. Ganea. Poincare glove: Hyperbolic word embeddings. In
International Conference on Learning Representations, 2019.
A. A. Ungar. A gyrovector space approach to hyperbolic geometry. Synthesis Lectures on Mathematics
and Statistics, 1(1):1-194, 2008.
Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou (eds.), Advances in Neural Information Processing Systems 21, pp. 1753-1760. Curran
Associates, Inc., 2009.
H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: A novel image dataset for benchmarking machine
learning algorithms, 2017.
H. Yang, X.-Y. Zhang, F. Yin, and C. Liu. Robust classification with convolutional prototype learning.
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3474-3482, 2018.
T. Yu and C. De Sa. Numerically accurate hyperbolic embeddings using tiling-based models. In
Advances in Neural Information Processing Systems 32, pp. 2023-2033. Curran Associates, Inc.,
2019.
13
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Notations and Symbols
Default Notations
Notation	Description	Related formula
R	The set of real numbers	
Rn	n dimensional Euclidean space	x ∈ Rn , x = (x1 , . . . , xn)
(∙, ∙)E	Euclidean inner product	x ∈ Rn,y ∈ Rn, (x, y)E = Pin=1 xiyi
h∙, ∙iH	Hyperbolic analogue of (∙, ∙)e	X ∈ Hn,y ∈ Sn-1, hx,ωiH = 1 log 1⅛⅛∣2
H	Euclidean norm	X ∈ Rn, |x| = √(x,x)E
Hn	n dimensional hyperbolic space	as a set, Hn = {x ∈ Rn : |x| < 1}
Tp(X)	Tangent space of X at p	
T(X)	Tangent space of X	T (X) = ∪p∈XTp (X)
dsHn	The canonical metric on Hn with curva-	ds2Hn=Pin=14(1-|X|2)-2dXi2
	ture -1	
dVol	Riemannian volume on Hn	dVol = 2n(1 - |X|2)-ndX1 . . . dXn
Lp(K, dVol)	Lp space	Lp(K,dVol) = {f| Rk |fIpdVol < ∞}
|| ∙ llLp(K,dVol)	Lp norm	f measurable on K, ∣∣f ∣∣LP(κ,dVoi) = (RK If IpdVol) 1
Sn-1	n - 1 dimensional sphere	as a set, Sn-1 = {X ∈ Rn : |X| = 1}
P(∙, ∙)	Hyperbolic Poisson kernel	X ∈ Hn,ω ∈ Sn-1,P(x,ω) = (T∙ )n-1
fa1,p	Model in the hyperbolic MLR	f 1 (χ) = 2 ⑷ sinh-1 ( 2(-p ㊉ x，a)E ʌ Ja,pl )	1-∣p∣2	I (1-∣-p㊉χ∣2)∣a∣ J
dHn	The hyperbolic distance function	
Ξ	The space of horocycles	
Ξω	The set of horocycles that are tangential	
	to Sn-1 atω	
LX	Laplace-Beltrami operator on X	
hx	The horocycle feature function	hx(y) = hy, X/IXIiH
ξλ,ω	The unique horocycle connecting ω and	
	tanh λ∕2 ∙ ω.	
MLR	Multiple linear regression	
dim	dimension	
IK	the indicator function of K	
Dist	Relative distance function	Dist(X, ω, b) = -2hX, ωiH + b
Cls	Set of classes	Cls= {C1,C2,...,CM}
NNθ	A network parameterized by θ	
NN0θ	A network parameterized by θ	
Exp	Exponential map of the hyperbolic space	
(X1, Y 1)	Labeled sample	
SCj	Score function	
pθ(Y = Cj|X)	Prediction probability	
L	Loss function	
Pρw,λ,b	Poisson neuron	PW,λ,b (X)= ρ(λ ⅛≠ + b)
PE	PoinCar6 embedding	
14
Under review as a conference paper at ICLR 2021
	Conventional symbols
Symbol	In most cases it refers
n, m, i	integers
X, y, w	points in Rn or Hn , or real numbers
o b, c, d, α, δ λ	the origin of Rn or Hn real numbers real or complex number
t	real number, represent the timestamp in optimization
ω	point in Sn-1
ρ f, g K X	an activation function functions a compact set a manifold
p	a point in Hn or on a manifold
a ξ	an element in Tp(Hn) a horocycle
μ L l U F, h, H M D	a measure a family of geodesics lines a geodesics line a set in Hn functions number of classes dimension
A.2 Proof of the is ometry
Given ω∈Sn-1 and λ∈R, We let ξλ,ω the unique horocycle that connects ω and tanh (λ∕2) ∙ ω. The
length of any geodesic (that ends at ω) line segment cut by ξλ∖,ω and ξλ2,ω equals ∣λι - λ2∣. This
fact is obvious in the half-space model.
There is a Riemannian isometry F : {z ∈ Rn : |z| < 1} → {(xι,…,Xn) : xι > 0} (the latter
is with the metric ds2 = dxι+∙∙∙+dxn) SUCh that F(ω) = ∞ and F(o) = (1,0,..., 0). Using
x1
dHn (O, tanh(%/2)M = lλ∕, d{(xι,∙∙∙ ,Xn)：Xi >0} ((1, 0,..∙, O) , (e± ʌi , 0,..∙, O)) = |%|, F(M = ∞
and F(o) = (1,0,..., 0), we have F(tanh(λi∕2)ω) = (eλi, 0,..., 0). Therefore, F maps ξλ%,ω to
{(x1, x2, . . . , xn) : x1 = eλi}. Any geodesic (that ends at ω) line segment cut by ξλ1,ω and ξλ2,ω is
mapped by F to {(t, α2, . . . , αn) : (t - eλ1 )(t - eλ2) < 0} for some fixed αj. It is easy to check the
length of this segment with respect to dx-,+dxn (as ai are constants, the metric reduces to dχ2∕χ2
x1
on this segment) is ∣λι - λ2∣.
A.3 Proof of (6)
Because X is on ξλ which is a sphere with center 1+tanh λ∕2 ω and radius 1-tanh λ∕2, We have
∣x — 1+tanhλ∕2ω∣ = J 1-tanhλ∕2 J , which leads to |x|2 — (1+tanh λ∕2)(x, ω)e + tanh λ∕2∣ω∣2 =
15
Under review as a conference paper at ICLR 2021
0, and then 1+吗” λ/2 |x - ω∣2 = 1-吗” λ/2 (∣ω2∣ - |x|2), and finally hx,ω>H = 1 log %_-,.|
1 log 1+tanh λ∕2 = λ∕2.
2 ɔ 1—tanh λ∕2	/
A.4 Another proof of the integral formula (7)
We use Hn for the upper half space model {(χι, ∙∙∙ ,Xn) : xi > 0} with the Riemannian
volume dx1χndxn. Let ω = (∞, 0,..., 0) and o be (1,0,..., 0) as in (A.2), then ξλ,ω =
{(x1, x2, . . . , xn) : x1 = eλ}. The induced Riemannian metric on ξλ,ω (respectively volume
dVolξλ,ω) is dx2：2；+dXn(respectively dχ2-d)Xn). For any integral function f on Hn, using change
of variable x1 = eλ
/	dxi …dx,
JHnf (x1,...,xn)-Xf-
λ λ	fl λ	∖dx2 …dxn λ ,ʌ
J f ) ɪ f(e ,X2,...,Xn)--------------Q------e dλ
f f	f (eλ,χ2,...,xn) dx2n_1dxn dλ
Jλ √(x2,...,Xn)∈Rn-1	e()
f(z)dVolξλ,ω(z)dλ.
The above identity is equivalent to the integral formula Hn f (x)dVol (x)	=
RR Rξ f (z)dVol ξλ,ω (z) dλ. presented in (7), according to the Riemannian isometry in
(A.2). ,
A.5 The heuristic is not a proof
The spectral theory does not directly lead to universal approximation theorems because of the
following: 1, superpositions in (1, 2) and (8, 9) are different (similarly, although another kind
of superposition in Hilbert’s 13th problem (Hilbert, 1935; Arnold, 2009) was a driving force for
universal approximation theorems (Nielsen, 1987), the former is hardly relevant for networks (Girosi
& Poggio, 1989)); 2, desired representation properties of hyperbolic eigenfunctions are unknown,
partially because Hn is non-compact; 3, results in spectral theory favor Hilbert spaces, while universal
approximation theorems embrace more than L2 space.
A.6 Optimization
The parameters update for the horocycle unit (2) involves the optimization problem on the sphere (for
ω) and the hyperbolic space (for x). We use a standard algorithm of sphere optimization (Absil et al.,
2008) to update ω , and in the supplement we present an optimization approach based on the geodesic
polar-coordinates to update x.
In the implementation of a horocycle layer, the forward propagation is trivial, while the backpropaga-
tion involves optimization on the sphere and hyperbolic space. In the following, η is the learning rate,
at is the value of α (α may be η, s, z, ω,...) at the t-th step, TpX is the tangent fiber at p, V is the
gradient, and VH is the hyperbolic gradient. It suffices to consider the layer s=hz, ω).
Optimization on the sphere The parameter update of ω in s=hz, ωi involves the optimization on
the sphere. The projection of 鬻 Vs(ωt) = ∂LSθ ∣Zzt二ω% ∈ Tωt Rn onto Tωt SnT is given by Absil
et al. (2008)[p.48]
=∂Lθ	Zt	— ωt	∂Lθ	(	Zt	— ωt	λ =	∂Lθ Zt —	(zt,	ωt)ωt
% ∂s	|zt	- ωt∣2	∂s ∖∖zt	- ωt∣2	2 "广t ∂s	|zt	- ωt∣2	.
Two well-known update algorithms of wt Absil et al. (2008)[p.76] are:
ωt+ι = cos (ηt∖vt∖)ωt — sin(ηt∖vt∖)∖vt∖-1vt；
ωt+1 = (ωt - ηtvt)∕∖ωt - ηtvt∖.
16
Under review as a conference paper at ICLR 2021
Figure 7: Poisson neuron Pt(1a,n0h),-1/3,0. The level sets of a Poisson neuron Pρw,λ,b are horocycles of
the ball {x : |x| < |w|} that are tangential to {x : |x| = |w|} at w and their inverses with respect to
{x : |x| = |w|}.
A.7 A proof of Apollonius theorem
Theorem 3 (Apollonius). Given distinct ω1 , ω2 ∈ Sn-1 and a positive number λ, the locus {x :
|x - ω11 = λ∣x - ω21} is a sphere orthogonal to Sn-1.
Proof. If λ is one then it is trivial. We assume now λ is not one. By |x — ω11 = λ∣x — ω21, we can
have
ω1 - λω2
X 1 - λ
∣ωι — λω212
|1 - λ∣2
- 1.
2
l∖ω1 - λω2 |2
V |1 - λ∣2	-1 +1.
The locus is a sphere with center O = "1--产 and radius R = Jlω∣11-λ¾12 - 1. The theorem of
Apollonius (in all dimension) claims that this sphere is orthogonal to Sn-1. To prove this, it suffices
to prove |oO |2 = 1 + R2 (recall o is the origin of Hn), which follows from
ω1 - λω2
1 - λ
□
A.8 Inversion
On Rn ∪ {∞}, given the sphere {x : |x - w0| = r}, the corresponding inversion is given by
Iv(x) = w0 +
r2(x - w0)
|x - woI2
For x ∈ Rn ∪ {∞}, Iv(x) is called the inverse of x with respect to {x : |x - w0| = r}.
A.9 Proof of Theorem 2
Theorem 2 Let K be a compact set in Hn, and 1≤p<∞. Then finite sums of the form
N
F (x) =	αi ρ(λi hx, ωiiH+bi), ωi ∈S	, αi , λi , bi ∈R
i=1
are dense in Lp (K, μ), where μ is either d Vol (5) or the induced Euclidean volume.
Proof. We first treat the case P is sigmoidal and μ = d Vol. Assume that these finite sums are not
dense in Lp (K, dVol). By the Hahn-Banach theorem, there exists some nonzero h∈Lq (K, dVol),
where q=p/(p - 1) if p>1 and q=∞ if p=1, such that K F (x)h(x)dVol (x) = 0 for all fi-
nite sums of the form (14). As K is a compact set, by Holder,s inequality, JK ∣h(x)∣ dVol ≤
( K dVol)1/p||h||Lq(K,dVol), which leads to h∈L1(K, dVol). Extend h to be a function H
17
Under review as a conference paper at ICLR 2021
that is defined on Hn by assigning H (x)=h(x) if x∈K and H (x)=0 if x∈Hn \K. Then
H∈L1(Hn,dVol)∩Lq(Hn,dVol) and
/
Hn
F (x)H (x)dVol (x) = 0
(15)
for all finite sums of the form (14). For any ω∈Sn-1 and λ, b∈R, we set Fω,λ,b(x)
ρ(λ(hx, ω)H-b)). These functions are uniformly bounded, as ∣Fω,λ,b(x)∣≤1. Moreover,
1 if hx, ωiH>b,
lim Fω,λ,b(x) =
λ→∞	0 if hx, ωiH<b.
(16)
According to (15), for all ω, λ, b, we have Hn Fω,λ,b(x)H (x)dVol (x) = 0. Functions {Fω,λ,b}λ∈R
converge pointwise as λ→∞, and they are uniformly bounded by |H∣∈L1(Hn, dVol). By the
bounded convergence theorem, for all ω∈Sn-1, b∈R, we have
H (x)dVol (x) = 0.
{{χ--hx,ω) H>b}
By the integral formula (7) (with notations defined there), (6) and (17), for all b∈R,
Z Z	H(z)dVolξt,ω(z) dt = 0.
(17)
(18)
Taking the derivative of R2∞b Rξ H(z)dVolξt,ω (z) dt with respect to b, we deduce from (18) that
ξ	H (z)dVol ξ2b,ω (z) = 0 for a.e. b∈R. In other words, the integration of H on a.e. ξ ∈ Ξω is
zero. This fact is valid for all ω∈Sn-1. Therefore, the integration of H on a.e. ξ ∈ Ξ is zero. By the
injectivity Theorem 1, H = 0 a.e., which contradicts our assumption. Therefore, finite sums of the
form (14) are dense in Lp(K, dVol). The case P is ReLU, ELU or Softplus and μ = d Vol follows
from the above case and the fact that X → ρ(χ + 1) - ρ(χ) is sigmoidal. The case μ is the Euclidean
volume follows from previous cases and the fact that the Euclidean volume on compact K is bounded
from above by λdVol for some constant λ.
□
A.10 Universal approximation theorem for Poisson neurons.
In this section, ρ is a continuous sigmoidal function (Cybenko, 1989), ReLU(Nair & Hinton, 2010),
ELU(Clevert et al., 2016), or Softplus(Dugas et al., 2001). We also recall the Poisson neuron:
PWλb(X) = P fλ|w|2 - lχf + b) , W ∈ Rn, λ,b ∈ R.
w,λ,b	|x - w|2
Theorem 4. Let K be a compact set in Hn, and 1≤p<∞. Then finite sums of the form
N
F(X) = XαiPρωi,λi,bi(X), ωi∈Sn-1, αi, λi, bi∈R	(19)
i=1
are dense in Lp(K, μ), where μ is either d Vol (5) or the induced Euclidean volume.
Proof. We first treat the case P is sigmoidal and μ = dVol. Assume that these finite sums are not
dense in Lp(K, dVol). By the Hahn-Banach theorem, there exists some nonzero h∈Lq (K, dVol),
where q=p/(p - 1) if p>1 and q=∞ if p=1, such that K F (X)h(X)dVol (X) = 0 for all fi-
nite sums of the form (19). As K is a compact set, by Holder,s inequality, JK Ih(X)I dVol ≤
( K dVol)1/p||h||Lq(K,dVol), which leads to h∈L1 (K, dVol). Extend h to be a function H
that is defined on Hn by assigning H (X)=h(X) if X∈K and H (X)=0 if X∈Hn \K. Then
H∈L1(Hn,dVol)∩Lq(Hn,dVol) and
F (X)H (X)dVol (X) = 0
Hn
(20)
18
Under review as a conference paper at ICLR 2021
for all finite sums of the form (19). For any ω∈Sn-1, λ ∈ R, and b > 0, we set
Fω,λ,b(X) = pω,λ,-λb(X) = P (λ ( : J：l|2 - b)).
These functions are uniformly bounded, as ∣Fω,λ,b(x)∣≤1. Moreover,
1
lim Fω,λ,b(X) =
λ→∞ ω,,	0
if
if
ι-∣χ∣2
|x-ω∣2
ι-∣χ∣2
∣x-ω∣2
>b,
<b.
(21)
According to (20), for all ω,λ, b, we have Hn Fω,λ,b(X)H (X)dVol (X) = 0. Functions {Fω,λ,b}λ∈R
converge pointwise as λ→∞, and they are uniformly bounded by |H∣∈L1(Hn, dVol). By the
bounded convergence theorem, for all ω∈Sn-1, b∈R, we have
J	H (x)dVol (X) = J	2 }H (x)d Vol (x) = 0.
(22)
By the integral formula (7) (with notations defined there), (6) and (22), for all b∈R,
∞
log
H(z)dVolξt,ω(z) dt = 0.
(23)
Taking the derivative of Rl∞og b Rξ	H (z)dVol ξt,ω (z) dt with respect to b, we deduce from (23) that
ξ H (z)dVol ξlog b,ω (z) = 0 for a.e. b>0. In other words, the integration of H on a.e. ξ ∈ Ξω is
zero. This fact is valid for all ω∈Sn-1. Therefore, the integration of H on a.e. ξ ∈ Ξ is zero. By the
injectivity Theorem 1, H = 0 a.e., which contradicts our assumption. Therefore, finite sums of the
form (19) are dense in Lp(K, dVol). The case P is ReLU, ELU or Softplus and μ = d Vol follows
from the above case and the fact that X → P(X + 1) - P(X) is sigmoidal. The case μ is the Euclidean
volume follows from previous cases and the fact that the Euclidean volume on compact K is bounded
from above by λd Vol for some constant λ.	口
We refere the reader to the difference of (16) and (21), (17) and (22), and (18) and (23). However,
basically the proofs are the same. The points are the integral formula (7), the injectivity Theorem 1
and the fact that level sets of horocycle/Poisson neurons are horocycles. Moreover, as a corollary of
Theorem 4, we have
Corollary 2. Let K be a compact set in Rn, and 1≤p<∞. Then finite sums of the form
N
F(X) =	αipρwi,λi,bi(X), wi∈Rn, αi,λi, bi∈R
i=1
are dense in Lp(K, μ), where μ is the Euclidean volume.
Proof. Because K is compact, there exists a positive number R such that K ⊂ {X ∈ Rn : |X| < R}.
By the above theorem, finite sums of the form
N
F(X) = Xαipρwi,λi,bi(X), wi∈Sn-1,αi,λi,bi∈R
i=1
are dense in Lp(K∕R, μ). Then the corollary follows from
ρW,λ,b(X)=Pw/R,λ,b(X/R).
□
19
Under review as a conference paper at ICLR 2021
A.11 Proof OF THE Lemma 1
Recall
f1,p(x)
Sinh-I Jl- ㊉飞	ʌ .
1 -|p|2	V(1 -|- P ㊉ x∣2)∣α∣ )
(24)
The proof of Lemma 1 follows from the following direct computation.
Proof. Let t ∈ (0,1). TakePt = tω and a = -ω, then we have
-Pt ㊉ X
—1(1 — 2t(ω, x) E + ∣x∣2)ω + (1 — t2)x
1 — 2t(ω, x) E + t2 ∣x∣2
Let Ft(X)
Ft(x)
2(—Pt ㊉ x,at)E
(1-∣-pt ㊉ x∣2)∣αt∣
, then
2(-Pt ㊉ x, at)E
(1 - ∣ - Pt ㊉ x∣2)∣αt∣
tɔt(l-2t(ω,x)E + ∣x∣2) —(1 —t2)(x,ω)E
1-2t(ω,x)E +t2∣x∣2
∣-t(1-2t(ω,x)E + ∣x∣2)ω + (1-12 )x∣2
(1-2t(ω,x)E +t2∣x∣2)2
2t(1 — 2t(ω, x)E +12 ∣x∣2)(1 — 2t(ω, X)E + ∣x∣2) — 2(1 — t2)(1 — 2t(ω, x)e + t2∣x∣2)(x, ω)E
(1 — 2t(ω,x)E + t2∣x∣2)2 -∣- t(1 — 2t(ω,x)E + ∣x∣2)ω +(1 — t2)x∣2
At(x)∕Bt(x),
where At, Bt are defined as the corresponding numerator and denominator. We have
At(X)It=ι =	2∣x - ω∣4
Bt(x)∣t=ι = 0
∂Bt(x)∕∂t∣t=ι = 2∣x — ω∣2(∣x∣2 — 1).
Let Gt(X) = sinh-1(Ft(x)) + log -∣+∣, then
…、1 (At(x)	/	A2(x)∖ 1	1 — t 1 ((1 — t)At /(1 — t)2^^(1 — t)2A2(x)ʌ
Gt(X)	Og (瓦而+ d* 1 + 及西)+logE = log (EW + V(ΓΓ^y + (1 + t)2B2(xJ.
By L'H6pital's rule,
(1 - t)At(X) _ -At(X) +(1 -1)At(X) I	_ ∣x - ω∣2
t<1‰ (1+ t)Bt(x) = Bt(X)+ (1+ t)B0(x) lt=1 = 2 — 2∣x∣2 .
Therefore,
lim Gt(x)=log (∣x -,ω∣2 ).
t<1,t→1 t( ) g 11 — ∣X∣2 J
For t < 1, we take Pt = tω, at = —ω, Ct = t2-1, dt = 2 log ɪ+1, then for all X ∈ K,
1	—1	1	1 — ∣x∣2
t<1m→1 Ctfat&(X)+dt = t<lim→1 FGt(X) = 2log (v∣x-ωρ) = hx,ωiH ∙
If there exists c1,c2 such that ∣ctf1αt,Pt(x) + dt∣(= IGt(X)∣∕2) ≤ c2 for all t ∈ (c1,1), x ∈
K, then by the dominated convergence theorem, there exists t such that ∣∣ctfa%pt(x) + dt -
hx, ω)H∣∣lp(κ,m) < e, which proves the lemma. Note that
(1 — t)At(x)	=	2∣x - ω∣4(1 - t) + P4=1 Uj(x,ω)(1 - t)j+1
(1+ t)Bt(x)	=	—2∣x - ω∣2(∣x∣2 - 1)(1 - t)(1 + t) + £：=2 Lι(x,ω)(1 - t)l(1 +1)
_ _____________2∣x - ω∣4 + P4=1 Uj(x, ω)(1 - t)j__________
=2∣X - ω∣2(1 -∣X∣2)(1+ t) + P4=2 Lι(x,ω)(1 - t)l-1(1+ t),
20
Under review as a conference paper at ICLR 2021
where Uj and Ll are continuous functions defined on K × {ω}. There exist positive numbere c3, c4
and c1 ∈ (0, 1) such that for all x ∈ K and t ∈ (c1, 1),
C3 ≤ 2|x - ω∣4 ≤ C4,
C3 ≤ 2|x - ω∣2(1 - ∣X∣2)(1 +1) ≤ C4,
c3 ≥l XX Uj (x,ω)(1- t)j |,
2 j=1
4
-23 ≥ | X Ll(X, ω)(1- t)ι-ι(ι+t)|.
l=2
Therefore, for x ∈ K and t ∈ (c1, 1), we have
C3	≤ (1 - t)At(x) ≤ 2c4 + C3
2c4 + C3 - (1+ t)Bt(x) - -C3-.
This implies that for t ∈ (c1, 1), Gt|K and therefore |ctfa1 ,p + dt||K are uniformly bounded, which
finishes the proof of the lemma.	□
A.12 The first MNIST classifier in 6.1
At the preprocessing stage, we compute the projection of the 28 × 28 input pattern on the 40 principal
components and then scale them so that the scaled 40-dimensional PCA features are within the unit
ball. In our network,
1.	Input layer: scaled 40-dimensional PCA features;
2.	First layer: 40 inputs/1000 outputs horocycle layer (tanh activation);
3.	Last layer: 1000 inputs/10 outputs affine layer;
4.	Loss: cross entroy loss.
Take learning rate = 1, learning rate decay = 0.999, and batch size = 128, and run it three times.
The average test error rates after 600 epochs is 1.96%.
PCA follows LeCun et al. (1998)(C.3), where 40 PCA is used for the quadratic network. Quadratic
network has a similar structure to ours, because our neuron are contructed by quotient of quadratic
functions followed by log.
A.13 Horocycle layer followed by MLR can approximate the classfication
FUNCTION
Suppose the MNIST classification function M is defined on ∪j9=0Kj ⊂ H40, where
Ki are relatively compact and M|Kj = j. By Theorem 2, for 0≤j≤9, there ex-
ist Fj (x) = PiN=j1 αj,iρ(λj,i hx, ωj,iiH+bj,i) such that Fj approximates IKj , where I is
the indicator function. Therefore, a network with the first (horocycle) layer given by
ρ(λj,i hx, ωj,iiH+bj,i)(0≤j≤9, 1≤i≤Nj) followed by a classical MLR with parameters given by
αj,i(0≤j≤9, 1≤i≤Nj) (with arg max for prediction) approximates M.
A.14 The second MNIST classifier in 6.1
At the preprocessing stage, we do data augmentation by letting each image 1 step toward each of its 4
corners, so that our traning set has 300000 examples. In our network,
1.	Input layer: (28,28, 1);
2.	First block: 32-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling, BatchNorm;
3.	Second block: 64-filters 3 × 3 convolution, ReLU, BatchNorm;
4.	Thrid block: 64-filters 3 × 3 convolution,ReLU,2 × 2 max-pooling, BatchNorm;
21
Under review as a conference paper at ICLR 2021
5.	Fourth block: 128-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling, BatchNorm;
6.	Fifth block: FC 1000, ReLU, BatchNorm;
7.	Last block: 1000 input/10 output Poisson layer, sigmoid, BatchNorm;
8.	Loss: cross entroy loss.
In optimization, we take Adam(Kingma & Ba, 2015). The batch size is 128 in the first 5 epochs, and
1024 in the next 15 epochs. After 5 epochs, we set ωi in the Poisson layer to be non-trainable. We
train our network five times, the average test error rate after 20 epochs is 0.35%.
The E in『--x+e is an important hyperparameter for the numerical stability. We train this MNIST
model with ∈ {10-1, 10-2, 10-4, 10-6, 10-8, 10-10, 10-20}. They all show robust performance.
A.15 Experiment of Poincare tree classification task
Given a PoinCare embedding (Nickel & Kiela, 2017) PE : {WordNet noun} → HD of the 82114
WordNet noun nodes and given a node x, the task is to classify all other nodes as being part of the
subtree rooted at x (Ganea et al., 2018a). Our model is a logistic regression, where the horocycle
feature p ∈ {WordNet noun} 7→ hPE(x) (P E (p)/s) (s is a hyperparameter lying in [1, 1.5]) is the
only predictor, and the dependent variable is whether p is in the subtree rooted at x. Let P be the set
of all nodes in the Poincare embedding, and let p range from P .
1.	Input: hPE(x) (P E (p)/s) (s is a hyperparameter.)
2.	Only layer: 1 input/1 output affine layer. (two parameters: one for input, one for bias.)
3.	Loss: Logistic. (with respect to 1 if p in the tree rooted at x; 0 else.)
In each training, x is one of {animal, group, location, mammal, worker}, dim is one of {2,3,5,10},
and Poincare embeddings are from the animation_train.py of Ganea et al. (2018b) 4 (with
tree=wordnet_full, model=poincare, dim=dim, seed randomly ∈ {7, 8, 9}). All nodes in the subtree
rooted at x are divided into training nodes (80%) and test nodes (20%). The same splitting procedure
applies for the rest nodes. We choose s that has the best training F1, and then record the corresponding
test F1. For each x and dim, we do the training 100 times. The average test F1 classification scores
are recorded in Table 2.
The horocycle feature performs well here because it is compatible with the Poincare embedding
algorithm. Let x be a node that is not at the origin. It seems that the Poincare embedding algorithm
tends to pull all nodes that are from the subtree rooted at X towards the direction of 禽,therefore
M )h
y→
y,
is a suitable feature for this task.
A.16 END-BASED CLUSTERING IN H2
For MNIST, at the preprocessing stage, we do data augmentation by letting each image 1 step toward
each of its 4 corners, so that our traning set has 300000 examples. Our network for H2 embedding of
MNIST dataset is
1.	Input layer: (28,28, 1);
2.	First block: 32-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling, BatchNorm;
3.	Second block: 64-filters 3 × 3 convolution, ReLU, BatchNorm;
4.	Thrid block: 64-filters 3 × 3 convolution,ReLU,2 × 2 max-pooling, BatchNorm;
5.	Fourth block: 128-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling, BatchNorm;
6.	Fifth block: FC 1000, ReLU, BatchNorm;
7.	Sixth block: FC 2, ReLU, BatchNorm, Exp;
8.	Last block: 2 input/10 output horocycle layer, sigmoid;
4https://github.com/dalab/hyperbolic_cones
22
Under review as a conference paper at ICLR 2021
9.	Loss: cross entroy loss,
where Exp is the exponential map ToH2(= R2) → H2. We apply the data augmentation as in A.14.
In optimization, learning rate is 0.1, learning rate decay is 0.99, batch size is 128, epochs is 50.
Our network, data augmentation and optimization for H2 embedding of Fashion-MNIST dataset is
completely the same as that for MNIST.
For MNIST and Fashion-MNIST we use sphere optimization. We would like to remark that there
are interesting new features in sphere optimization. Because the S1 is compact, for any continuous
function f , there exists x = argmaxS1 f . The derivative of f at x vanish, so the usual optimization
algorithm to find the minimum will fail in the general case. In our experiments, we solve this problem
by adding the following tricks:
1.	Observation: if the class Cα are all close to ω ∈ S1, and the end prototype ωα for the class
Cα is around -ω, then ωα is a maximum point of the loss function and therefore can not
be improved through normal SGD. We solve this problem by adopting an idea(supervised
variation) of k-means clustering. In each early epochs, optimization consists of two parts. In
the first part, the normal SGD applies. In the second part, we move end prototypes (ωi) to
the average direction of the class (using training data).
2.	Observation: if the class Cα and class Cβ are all close to ω ∈ S1, and the end prototype
ωα , ωβ are also both around ω, then all points in class Cα and class Cβ, end prototypes
ωα, ωβ will all be pulling to ω by the SGD, and finally the network can not distinguish class
Cα and class Cβ. We solve this problem by adding a loss if two prototypes are close.
With these small tricks, our 2D end-based clustering algorithm is very stable for MNIST and Fashion-
MNIST. We run it on MNIST 10 times, and they all get a test acc around 99% within 20 epochs.
Suppose the classification task has M classes and the prototype of the i-th class is ωi . We write down
the additional loss function for the second observation as follows
i	= RandomChoice({1, . . . , M})
j = RandomChoice({1, . . . , M} \ {i})
d = (ωi , ωj )E
LObservation2 = arctanh(10 × ReLU(d - 0.9 - )),
where is a small constant for numerical stability.
For CIFAR-10, our network for H2 embedding of CIFAR-10 dataset is
1.	Input layer: (32,32, 3);
2.	First block: ResNet-32/128 output;
3.	Second block: FC 2, ReLU, BatchNorm, Exp;
4.	Last block: 2 input/10 output horocycle layer;
5.	Loss: cross entroy loss.
In the data augmentation, we apply horizontal/vertical shifts and horizontal flip. We use Adam. The
batch size is 32 in the first 100 epochs, or 1024 in the next 50 epochs. The weights of the horocycle
layer are fixed at the beginning of the training and are non-trainable, which follows an idea of Mettes
et al. (2019).
A.17 Poisson MLR
For CIFAR-10, we use a ResNet-32 structure as the feature descriptor, and we apply horizontal/vertical
shifts and horizontal flip. In our network,
1.	Input layer: (32,32, 3);
2.	First block: ResNet-32/128 output;
3.	Second block: FC 128, ReLU, BatchNorm;
23
Under review as a conference paper at ICLR 2021
4.	Last block: 128 input/10 output Poisson layer, BatchNorm;
5.	Loss: cross entroy loss.
We use Adam. The batch size is 32 in the first 80 epochs, or 1024 in the next 20 epochs. Test acc
greater than 93.5%.
For the classification task of flowers (Tensorflow), The dataset of 3670 photos of flowers contains 5
classes: daisy, dandelion, roses, sunflowers and tulips. The keras model is
1.	Input layer: (180,180, 3);
2.	First block: 16-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling;
3.	Second block: 32-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling;
4.	Thrid block: 64-filters 3 × 3 convolution,ReLU,2 × 2 max-pooling;
5.	Fourth block: FC 128, ReLU;
6.	Last block: 128 input/10 output FC layer;
7.	Loss: cross entroy loss.
Our Poisson model is
1.	Input layer: (180,180, 3);
2.	First block: 16-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling;
3.	Second block: 32-filters 3 × 3 convolution, ReLU, 2 × 2 max-pooling;
4.	Thrid block: 64-filters 3 × 3 convolution,ReLU,2 × 2 max-pooling;
5.	Fourth block: FC 128, ReLU;
6.	Last block: BatchNorm, 128 input/10 output Poisson layer, sigmoid, BatchNorm;
7.	Loss: cross entroy loss.
We use 2936 photos for training and the rest 734 for testing. We train two models 5 times, and Figure
6 records the average of training and test accuracies in 20 epochs. Although the test accuracy of the
Poisson model is bad in the beginning, it is fairly higher (around 4%) than the test accuracy of the
keras model in the end.
A.18 Dist IS A RELATIVE DISTANCE FUNCTION
As Hn is a complete metric space, the absolute distance between any pair pf points (x, ω) ∈
Hn × Sn-1 is always +∞. This absolute distance is not useful, hence we look for a relative one.
Moreover, if a function D reasonably measures the relative distance of Hn from ω ∈ Sn-1, then so
does D + c for any constant c. This section explains why Dist (10) is a reasonable relative distance
function. For (x, ω, b) ∈ Hn × Sn-1 × R, we recall
Dist(X,ω,b)=ig (⅛≡⅛
+ b = -2hx, ωiH + b.
A.18.1 A naive viewpoint
For fixed ω ∈ Sn-1 and b ∈ R, Dist(∙, ω, b) is defined on Hn, and its level sets are the set of
horocycles that are tangential to Sn-1 at ω: Ξω = ∪λ∈R{ξλ,ω}. For λ ∈ R, using (6), we have for
all x ∈ ξλ,ω,
Dist(x, ω, b) = -λ + b.
The above identity implies the following: if x1 , x2 are on the same horocycle ξλ,ω then
Dist(x1, ω, b) = Dist(x2, ω, b) = -λ + b. Moreover, as λ moves to ∞ (equivalently speaking,
the horocycle ξλ,ω moves to ω), then for x ∈ ξλ,ω the function value Dist(x, ω, b) goes to -∞;
as λ moves to -∞ (equivalently speaking, the ξλ,ω moves away from ω), then for x ∈ ξλ,ω the
function value Dist(x, ω, b) goes to ∞. The above observation heuristically explains that Dist(∙, ω, b)
measures the relative distance of Hn from ω.
24
Under review as a conference paper at ICLR 2021
A.18.2 The Busemann function viewpoint.
Fix ω ∈ Sn-1, and then let c : [0, ∞) → Hn be the unique geodesic ray (with unit speed) that
satisfies
c[0] = (0, . . . , 0), c(∞) = ω.
For the purpose of this section, we do not need the definition of Busemann functions (we refer the
interested reader to Bridson & Haefliger (2009)[II.8]). Instead, it suffices to know the following result
of the theory: let dHn be the hyperbolic distance function, and for any x ∈ Hn,
lim (dHn (x, c(t)) - dHn (c(0), c(t))) = -2hx, ωiH.	(25)
t→∞
We read the above (25) in the following way: for fixed t > 0
x ∈ Hn 7→ dHn (x, c(t)) - dHn (c(0), c(t))
is a function that measures the relative distance of {x, c(0)} from c(t), and therefore the left hand
side of (25)
x ∈ Hn 7→ lim (dHn (x, c(t)) - dHn (c(0), c(t)))
t→∞
is a function that measures the relative distance of {x, c(0)} from limt→∞ c(t) = ω, and finally the
right hand side of (25)
x ∈ Hn 7→ -2hx, ωiH
is a function that measures the relative distance of {x, c(0)} from ω.
Moreover, if the geodesic ray c starts from a different y ∈ Hn, and then there will be a corresponding
bias term added to (25). Therefore, for any b ∈ R and ω ∈ Sn-1,
x ∈ Hn 7→ Dist(x, ω, b) = -2hx, ωiH + b
is a function that measures the relative distance of Hn from ω.
25