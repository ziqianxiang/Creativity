Under review as a conference paper at ICLR 2021
Tight Second-Order Certificates
for Randomized Smoothing
Anonymous authors
Paper under double-blind review
Ab stract
Randomized smoothing is a popular way of providing robustness guarantees
against adversarial attacks: randomly-smoothed functions have a universal
Lipschitz-like bound, allowing for robustness certificates to be easily computed.
In this work, we show that there also exists a universal curvature-like bound for
Gaussian random smoothing: given the exact value and gradient of a smoothed
function, we compute a lower bound on the distance of a point to its closest adver-
sarial example, called the Second-order Smoothing (SoS) robustness certificate.
In addition to proving the correctness of this novel certificate, we show that SoS
certificates are realizable and therefore tight. Interestingly, we show that the max-
imum achievable benefits, in terms of certified robustness, from using the addi-
tional information of the gradient norm are relatively small: because our bounds
are tight, this is a fundamental negative result. The gain of SoS certificates further
diminishes ifwe consider the estimation error of the gradient norms, for which we
have developed an estimator. We therefore additionally develop a variant of Gaus-
sian smoothing, called Gaussian dipole smoothing, which provides similar bounds
to randomized smoothing with gradient information, but with much-improved
sample efficiency. This allows us to achieve (marginally) improved robustness
certificates on high-dimensional datasets such as CIFAR-10 and ImageNet. Code
is available at https://github.com/alevine0/smoothing_second_
order.
1	Introduction
A topic of much recent interest in machine learning has been the design of deep classifiers with
provable robustness guarantees. In particular, for an m-class classifier h : Rd → [m], the L2
certification problem for an input x is to find a radius ρ such that, for all δ with kδk2 < ρ, h(x) =
h(x + δ). This robustness certificate serves as a lower bound on the magnitude of any adversarial
perturbation of the input that can change the classification: therefore, the certificate is a security
guarantee against adversarial attacks.
There are many approaches to the certification problem, including exact methods, which compute
the precise norm to the decision boundary (Tjeng et al., 2019; Carlini et al., 2017; Huang et al.,
2017) as well as methods for which the certificate ρ is merely a lower bound on the distance to the
decision boundary (Wong & Kolter, 2018; Gowal et al., 2018; Raghunathan et al., 2018).
One approach that belongs to the latter category is Lipschitz function approximation. Recall that a
function f : Rd → R is L-Lipschitz if, for all x, x0, |f (x) - f(x0)| ≤ Lkx - x0k2. If a classifier
is known to be a Lipschitz function, this immediately implies a robustness certificate. In particular,
consider a binary classification for simplicity, where we use an L-Lipschitz function f as a classifier,
using the sign of f(x) as the classification. Then for any input x, we are assured that the classification
(i.e, the sign) will remain constant for all x0 within a radius |f (x)|/L of x.
Numerous methods for training Lipschitz neural networks with small, known Lipschitz constants
have been proposed. (Fazlyab et al., 2019; Zhang et al., 2019; Anil et al., 2019; Li et al., 2019b) It is
desirable that the network be as expressive as possible, while still maintaining the desired Lipschitz
property. Anil et al. (2019) in particular demonstrates that their proposed method can universally
approximate Lipschitz functions, given sufficient network complexity. However, in practice, for the
robust certification problem on large-scale input, randomized smoothing (Cohen et al., 2019) is the
1
Under review as a conference paper at ICLR 2021
Figure 1: (a) Tight lower bound on the value of a smoothed function at x0 (i.e. pa(x0)) as a function
of kx0 - xk2. In this example, pa(x) = 0.8 and the smoothing standard deviation σ = 1. The
red line shows the lower bound for the function, with no information about the gradient given.
The blue line incorporates the additional information that kVχPa(x)k2 = 0. Note that the axis at
Φ-1 (pa(x)) = 0 corresponds to pa(x) = 0.5, the decision boundary for a binary classifier. (b)
Tight robustness certificates for a randomized-smoothed classifier, given the top-class value pa (x)
and the gradient norm kVxpa (x)k2 . The dashed lines show the certificates given pa (x) alone. Note
that the maximum possible gradient for a smoothed classifier depends on pa(x) (see Equation 1).
current state-of-the-art method. The key observation of randomized smoothing (as formalized by
(Salman et al., 2019; Levine et al., 2019)) is that, for any arbitrary base classifier function f : Rd →
[0, 1], the function
x → Φ-1(pa) where pa(x) := E	f(x+ )	(1)
GN (0,σ2I)
is (1∕σ)-Lipschitz, where N(0, σ2I) is a d-dimensional isometric Gaussian distribution with vari-
ance σ2 and Φ-1 is the inverse normal CDF function. As a result, given the smoothed classifier value
pa (x) at x, one can calculate the certified radius ρ(x) = σΦ-1 (pa (x)) in which pa(x) ≥ 0.5 (i.e.,
Φ-1 (pa(x)) ≥ 0). This means that we can use pa(x) ∈ Rd → [0, 1] as a robust binary classifier
(with one class assignment if pa (x) ≥ 0.5, and the other if pa (x) < 0.5). Cohen et al. (2019) shows
that this is a tight certificate result for a classifier smoothed with Gaussian noise: given the value of
pa (x), there exists a base classifier function f such that, if pa is the Gaussian-smoothed version of
f, then there exists an x0 with kx - x0k2 = ρ such that pa (x0) = 0.5. In other words, the certificate
provided by (Cohen et al., 2019) is the largest possible certificate for Gaussian smoothing, given
only the value of pa(x). Previous results (Li et al., 2019a; Lecuyer et al., 2019) provided looser
bounds for Gaussian smoothing.
Singla & Feizi (2020) have recently shown, for shallow neural networks, that, rather than globally
bounding the (first-order) Lipschitz constant of the network, it is possible to achieve larger robust-
ness certificates by instead globally bounding the Lipschitz constant of the gradient of the network.
This second-order, curvature-based method takes advantage of the fact that the gradient at x can be
computed easily via back-propagation, so certificates can make use of both f (x) and Vxf (x).
This leads to a question: can we also use the gradient of a smoothed classifier Vxpa (x) to im-
prove smoothing-based certificates? In this work, we show that there is a universal curvature-like
bound for all randomly-smoothed classifiers. Therefore, given pa (x) and Vxpa (x), we can compute
larger certificates than is possible using the value of pa(x) alone. Moreover, our bound is tight in
that, given only the pair (pa (x), Vxpa (x)), the certificate we provide is the largest possible certifi-
cate for Gaussian smoothing. We call our certificates “Second-order Smoothing” (SoS) certificates.
As shown in Figure 1, the smoothing-based certificates which we can achieve using second-order
smoothing represent relatively modest improvements compared to the first-order bounds. This is a
meaningful negative result, given the tightness of our bounds, and is therefore useful in guiding (or
limiting) future research into higher-order smoothing certificates. Additionally, this result shows that
2
Under review as a conference paper at ICLR 2021
randomized smoothing (or, specifically, functions in the form of Equation 1) can not be used to uni-
versally approximate Lipschitz functions: all randomly smoothed functions will have the additional
curvature constraint described in this work.
If the base classifier f is a neural network, computing the expectation in Equation 1 analytically is
not tractable. Therefore it is standard (Lecuyer et al., 2019; Cohen et al., 2019; Salman et al., 2019)
to estimate this expectation using N random samples, and bound the expectation probabilistically.
The certificate is then as a high-probability, rather than exact, result, using the estimated lower
bound ofpa(x). In Section 3.1, we discuss empirical estimation of the gradient norm of a smoothed
classifier for second-order certification, and develop an estimator for this quantity, in which the
number of samples required to estimate the gradient scales linearly with the dimensionality d of the
input.1 In order to overcome this, in Section 4, we develop a modified form of Gaussian randomized
smoothing, Gausian Dipole Smoothing, which allows for a dipole certificate, related to the second-
order certificate, to be computed. Unlike the second-order certificate, however, the dipole certificate
has no explicit dependence of dimensionality in its estimation, and therefore can practically scale to
real-world high-dimensional datasets.
2	Preliminaries, Assumptions and Notation
We use f (x) to represent a generic scalar-valued “base” function to be smoothed. In general, we
assume f ∈ Rd → [0, 1]. However, for empirical estimation results (Theorem 3), we assume that
f is a “hard” base classifier: f ∈ Rd → {0, 1}. This will be made clear in context. The smoothed
version of f is notated as pa ∈ Rd → [0, 1], defined as in equation 1.
Recall that Φ is the normal CDF function and Φ0 is the normal PDF function. In random-
ized smoothing for multi-class problems, the base classifier is typically a vector-valued function
f ∈ Rd → {0, 1}m, Pc fc(x) = 1, where m is the number of classes. The final classification
returned by the smoothed classifier is then given by a := arg maxc E fc(x + ). However, in most
prominent implementations (Cohen et al., 2019; Salman et al., 2019), certificates are computed us-
ing only the smoothed value for the estimated top class a, where a is estimated using a small number
N0 of initial random samples, before the final value of pa(x) is computed using N samples. The
certificate then determines the radius in which pa(x0) will remain above 0.5: this guarantees that a
will remain the top class, regardless of the other logits. While some works (Lecuyer et al., 2019;
Feng et al., 2020) independently estimate each smoothed logit, this incurs additional estimation er-
ror as the number of classes increases. In this work, we assume that only estimates for the top-class
smoothed logit Pa(X) and its gradient VχPa(x) are available (although We briefly discuss the case
with more estimated logits in Section 3.2). When discussing empirical estimation, we use η as the
accepted probability of failure of an estimation method.
3	Second-Order Smoothing Certificate
We noW state our main second-order robustness certificate result:
Theorem 1. For all X, X0 with kX - X0k2 < ρ, and for all f : Rd → [0, 1],
Pa(x0) ≥ Φ (φ-1(a0 + Pa(X))- P) - Φ (φ-1(a0) - P)	(2)
where a0 is the (unique) solution to
Φ0(Φ-1(a0)) - Φ0(Φ-1(a0 + Pa(x))) = -σkVxPa(x)k2.	(3)
Further, for all pairs (Pa(x), kVxPa(x)k2) which are possible, there exists a base classifier f and
an adversarial point x0 such that Equation 2 is an equality. This implies that our certificate is
realizable, and therefore tight.
Note that the right-hand side of Equation 2is monotonically decreasing With P: We can then compute
a robustness certificate by simply setting Pa(x0) = 0.5 and solving for the certified radius P. Also,
1In a concurrent Work initially distributed after the submission of this Work, Mohapatra et al. (2020) have
proposed an identical second-order smoothing certificate, along With a tighter empirical estimator for the gra-
dient norm. In this estimator, the number of samples required scales with dd.
3
Under review as a conference paper at ICLR 2021
Figure 3: Worst case base classifiers for second-order smoothing for the same value of pa (X) at
different values of ∣Vxpa (X)∣2. The base classifier is f = 1 in the blue regions and f = 0 in
the red regions. The point X is shown as a blue dot, with the Gaussian sampled region used for
calculating pa (X) is approximately shown as a dashed blue circle. Vxpa (X) is shown as a blue
arrow. (a) The gradient takes its maximum possible value: ∣Vxpa(X)∣2 = σ-1Φ0(Φ-1 (pa(X)). (b)
The gradient has an intermediate value: 0 < ∣Vxpa(X)∣2 < σ-1Φ0(Φ-1(pa(X)). (c) The gradient
is zero: ∣Vxpa(X)∣2 = 0.
a0 can be computed easily, because the left-hand side of Equation 3 is monotonic in a0 . Evaluated
certificate values are shown in Figure 1-b, and compared with first-order certificates.
All proofs are presented in Appendix A.
Like in Cohen et al. (2019), we proceed
by constructing the worst-case base classi-
fier f givenPa(X) and ∣∣VχPa(x)k2. This
is the base classifier f which creates an
adversarial point to the smoothed classi-
fier as close as possible to X, given the
constraints that pa(X) and ∣Vpa(X)∣2 are
equal to their reported values. In Cohen
et al. (2019), given only pa (X), this is
simply a linear classifier. With the gra-
dient norm, the worst case is that X lies
in a region with class a which is a slice
between two linear decision boundaries,
both perpendicular to Vpa(X). See Fig-
ure 3. Note that, by isometry and be-
cause Vpa (X) is the only vector infor-
mation we have, there is no benefit in
certified radius to having the direction of
Vpa(X): the norm is sufficient. In the case
of a linear classifier the gradient takes its
maximum possible value: ∣Vxpa (X)∣2 =
σ-1Φ0(Φ-1(pa(X)). This case is shown in
Figure 3-a: if the gradient norm is equal to
this value, the second-order certificate is
identical to the first-order certificate (Co-
hen et al., 2019). However, if the gradient
norm is smaller, then we cannot be in this
Figure 2: Comparison of second-order smoothing cer-
tificates to standard Gaussian smoothing certificates on
a selection of points from the Swiss Roll dataset. Cor-
rectly labeled points with (second-order) certificates
are shown in light red and blue, and points with in-
correct label or no certificate are in black. For a selec-
tion of points, shown in red/blue, the first-order certi-
fied radii shown are as red/blue rings. Increases to cer-
tified radii due to second-order smoothing shown are
as light blue (light red, absent) rings around certificate
radii. For both experiments, N = 108, and η = 0.001.
worst-case linear-classifier scenario. Instead, the new “worst case” is constructed by introducing a
second “wrong class” region opposite to the direction of the adversarial point (Figure 3-b). In the
extreme case (Figure 3-c) where the gradient norm is zero, this is accomplished by balancing two
adversarial regions in a “sandwich” around x.
This “sandwich” configuration reveals the relative weakness of gradient information in improving
robustness certificates: having zero gradient does not require that the adversarial regions be evenly
distributed around x. Rather, it is sufficient to distribute the adversarial probability mass 1 - pa (x)
into just two adversarial regions. Therefore, the certified radius, even in this most extreme case, is
4
Under review as a conference paper at ICLR 2021
similar to the Cohen et al. (2019) certificate in the case with half as much adversarial probability
mass (the first-order certificate for pa(x) := (1 + pa(x))/2). This can be seen in Figure 1-b: note
that at pa(x) = 0.6, if the gradient norm is known to be zero, the certificate is slightly below
the certificate for pa(x) = 0.8 with no gradient information. The second-order certificate when
(Pa(X) = 0.6, kVχPa(x)∣k2 = 0) is in fact slightly below the first-order certificate for Pa(X) = 0.8,
because the Gaussian noise samples throughout all of space, so the smoothed classifier decision
boundary is slightly affected by the adversarial region in the opposite direction of X.
Because we can explicitly construct “worst-case” classifiers which represent the equality case of
Equation 2, our certificates are known to be tight: the reported certified radii are the largest possible
certificates, if only Pa(X) and kVPa(X)k2 are known.
In Figure 2, we show how our second-order certificate behaves on a simple, two-dimensional, non-
linearly separable dataset, the classic Swiss Roll. The increases are marginal, mostly because the
certificates using standard randomized smoothing are already fairly tight. On these data, the certified
radii for the two classes are nearly touching in many places along the decision boundary. However,
for the blue class, which is surrounded on multiple sides by the red class, there are noticeable
increases in the certified radius. This is especially true for points near the center of the blue class,
which are at the “top of the hill” of the blue class probability, and therefore have smaller gradient.
3.1	Gradient Norm Estimation
In order to use the second-order certificate in practice, we must first bound, with high-probability,
the gradient norm kVxPa(X)k2 using samples from the base classifier f. Because Theorem 1 pro-
vides certificates that are strictly decreasing with kVxPa(X)k2, it is only necessary to lower bound
kVxPa(X)k2 with high probability.
Salman et al. (2019) suggest two ways of approximate the gradient vector VxPa(X) itself, both
based on the following important observation:
VxPa(X)=	E	[Vxf (x + €)] = E	[f(x + e)]∕σ2
e 〜N (0,σ2I)	e 〜N (0,σ21)
(4)
These two methods are:
1.	At each sampled point, one can measure the gradient off using back-propagation, and take
the mean vector of these estimates.
2.	At each sampled point, one can multiply f(X + ) by the noise vector , and take the mean
vector of these estimates.
Note, however, that Salman et al. (2019) does not provide statistical bounds on these estimates: for
our certificate application, we must do so. While we ultimately use an approach based on method
2, we will first briefly discuss method 1. The major obstacle to using method 1 is that it requires
that the base classifier f itself to be a Lipschitz function, with a small Lipschitz constant. This
can be understood from Markov’s inequality. For example, consider the value of some component
Z(X) := U ∙ Vf (x), where U is an arbitrary vector. Suppose N samples are taken, but that Z is
distributed such that:
( ʌ	∫0	with probability 1 一 击
Z(X ÷ e)	∣2N	with probability 表
(5)
This would be the case if f is a function that approximates a step function from 0 to 1, with a small
buffer region of very high slope, for example. Note that the probability that any of the N samples
measures the nonzero gradient component is < 0.5 , but the expected value of this component is in
fact 1.0. This example shows that, in order to accurately estimate the gradient with high probability,
the number of samples used must at least scale linearly with the maximum possible value of the
gradient norm for f. For un-restricted deep neural networks, Lipschitz constants are NP-hard to
compute, and upper bounds on them are typically very large (Virmaux & Scaman, 2018). Of course,
we could use Lipschitz-constrained networks as described in Section 1 for the base classifier, but this
would defeat the purpose of using randomized smoothing in the first place. Moreover, in standard
“hard” randomized smoothing as typically implemented (Cohen et al., 2019; Salman et al., 2019),
the range of f is {0, 1}, so f is non-differentiable: therefore, this back-propagation method can not
be used at all.
5
Under review as a conference paper at ICLR 2021
Figure 4: (a) Empirical second-order smoothing certificates, with d = 49 (corresponding to 7 × 7
MNIST experiments), N = 108, and η = .001 (b) Worst case classifier for dipole smoothing.
We therefore use method 2. In particular, we reject the naive approach of estimating each component
independently, taking a union bound, and the taking the norm: not only would the error in the norm-
squared scale with d as the error from each component accumulates, but there would be an additional
dependence on d from the union bound: each component would have to be bounded with failure
probability η∕d, where η is the total failure probability for measuring the gradient norm. Note that
this issue will also be encountered in method 1 above, but in that case, a loose upper bound could at
least be achieved without this dependency using Jensen’s inequality (the mean of the norms of the
gradient is larger than the norm of the mean).
Instead, we estimate the norm-squared of the mean using a single, unbiased estimator. Note that:
kVxEe[f (x + e)]k2 = σ-4Ee[f (X + e)] ∙ EJef(x + e)]=
σ-4Ee [ef(x + e)] ∙ Eeo [e0f(x + e0)] =	(6)
σ-4Ee,e0 [(ef(x + e))∙ (e0f(x + e0))]
In other words, we can estimate the norm-squared of the mean by taking pairs of smoothing samples,
and taking the dot product of the noise vectors times the product of the sampled values. We show
that this is a subexponential random variable (see Appendix), which gives us an asymptotically
linear scaling of N with d:
Theorem 2. Let V := Ee,eo [(ef (x + e)) ∙ (e0f (x + e ))], and V be its empirical estimate. If n pairs
of samples (= N∕2) are used to estimate V, then, with probability at most η, E[V] - V ≥ t, where:
-n Mn)
2 ln(η)
t
if -2 ln(η) ≤ dn
if -2 ln(η) > dn
(7)
Note that in practice, We can use the same samples to estimate ∣∣VxPa(x)k2 as are used to estimate
pa (x). However, this requires reducing the failure probability of each estimate to η0 = η∕2, in
order to use a union bound. This means that, if N is small (or d large), second-order smoothing
can in fact give worse certificates than standard smoothing, because the benefit of a (loose, for N
small) estimate of the gradient is less significant than the negative effect of reducing the estimate of
pa(x). As shown in Figure 4-a, even for very large N and relatively small dimension, the empirical
estimation significantly reduces the radii of certificates which can be calculated. See Section 5 for
experimental results.
3.2	Upper-b ound and multi-class certificates
We can easily convert Theorem 1 into a tight upper bound on pa (x0) by simply evaluating it for
f0 = 1 - f (and therefore p0a = 1 -pa). If estimates and gradients are available for multiple classes,
6
Under review as a conference paper at ICLR 2021
it would then be possible to achieve an even larger certificate, by setting the lower bound of the top
logit equal to the upper bounds of each of the other logits. Note, however, that unlike first-order
smoothing works (Lecuyer et al., 2019; Feng et al., 2020) which use this approach, it is not suffi-
cient to compare against just the “‘runner-up” class, because other logits may have less restrictive
upper-bounds due to having larger gradients. As discussed above, gradient norm estimation can
be computationally expensive, so gradient estimation for many classes may not be feasible. Also,
note that while this approach would produce larger, correct certificates, we do not claim that these
would be tight certificates given the value and gradient information for all classes: the “worst case”
constructions we describe above for a single logit might not be simultaneously construct-able for
multiple logits.
4	Dipole Smoothing
For large-scale image datasets, the dependence on d in Theorem 2 can create statistical barriers.
However, the general approach of second-order smoothing, especially using the discrete estimation
method (method 2) described above, has an interesting interpretation: rather than using simply
the mean of f (x + ), we are also using the geometrical distribution of the values of f (x + )
in space to compute a larger certified bound. In particular, if we can show that points which are
adversarial for the base classifier (points with f (x + ) = 0) are dispersed, then this will imply
larger certificates, because it makes it impossible for a perturbation in a single direction to move x
towards the adversarial region. Second-order smoothing, above, is merely an example of this.
We therefore introduce Gaussian Dipole smoothing. This is a method which, like second-order
smoothing, also harnesses the geometrical distribution of the values of f (x) to improve certificates.
However, unlike second-order smoothing, there is no explicit dependence on d in the empirical
dipole smoothing bound. In this method, when we sample f(x + ) when estimating pa(x), we also
sample f (x - ). This allows us to compute two quantities:
CS := E[f(x+)f(x- )]
CN := E[f(x+) - f(x+)f(x-)]
(8)
The certificate we can calculate is then as follows:
Theorem 3. For all x, x0 with kx - x0 k2 < ρ, and for all f : Rd → [0, 1],
Pa(x0) ≥ φ (φ-1(CN) — P) +Φ (φ-1(1+cs) - P) - Φ (φ-1(1⅜cs) - P)	(9)
We also compute this bound by constructing the worst possible classifier. In this case, the trick is
that, if two adversarial sampled points are opposite one another (i.e., f(x+ ) = f(x - ) = 0) then
they cannot both contribute to the same adversarial “direction”. In the worst case, the “reflected”
adversarial points form a plane opposite the base classifier decision boundary (See Figure 4-b).
In the extreme case where CN = 0, the “worst case” classifier is the same as for second-order
smoothing.
Experimentally, we simply need to lower-bound both CS and CN from samples. This reduces the
precision of our estimates, for two reasons: we have half as many independent samples for the same
number of evaluations we must perform, and we are bounding two quantities, which requires halving
the error probability for each. However, unlike second-order smoothing, there is no dependence on
d: this allows for practical certificates of real-world datasets.
5	Experiments
Experimental results are presented in Figures 5 and 6, with further results in Appendix B. Be-
cause both dipole and second-order certificates reduce the precision with which empirical quantities
needed for certification can be estimated, but both provide strictly larger certificates at the popula-
tion level, the key question becomes at what number of samples N does each higher-order method
7
UnderreVleW as a COnferenCe PaPer a〔 ICLR 2021
SOS、7X7 MN-ST、Q325、NH106
(C)
(d)
D 一 PoIP7X7 MN_ST、QHIP25、NHlO6
oooooooo
Figure^EXPerimenrS On 7 X 7 MNISτ∙ RePorred is rhe distribution Ofrhe improvement: (OrredUc—
rion) Of higher—order CerriHCareS from CertiHCareS COmPUred USing Srandard (HrSt-Order) randomized
SmOOrhing∙ for each〔esred image∙ FOr a_L Q = O ∙25∙ FOrFP尸 g)“ SeCOnd—order SmOOrhing is
USed∙ FOr (b d. C h)“ GaUSSian dipole SmOOrhing is USed∙ FOr (尸 b)" N = IO5 ∙ FOr (pd)" N = Io6.
For (尸PN = IO7 ∙ FOr (g“ h)“ N = IO∞
Under review as a conference paper at ICLR 2021
Figure 6: Dipole smoothing experiments, with σ = 0.25, on CIFAR-10 (a,b) and ImageNet (c,d).
For (a,c), N = 105. For (b,d), N = 106.
become beneficial. Note that in the figures, we are comparing the new methods to standard smooth-
ing, using the same N for standard smoothing as for the new method. Due to the poor scaling of
second-order certificates with dimension, we tested second-order smoothing on a low-dimensional
dataset, 7 × 7 MNIST. However, significant increases to certificates were not seen until N = 107
even on this dataset. By contrast, dipole smoothing is beneficial for many images even when smaller
numbers of smoothing samples are used. Because it scales to higher-dimensional data, we also
tested Gaussian dipole smoothing on CIFAR-10 and ImageNet, where it led to modest improve-
ments in certificates, in particular at N = 106. In Appendix C, we show the absolute, rather than
relative, certified accuracy curves for the experiments shown in Figures 5 and 6. These plot show
that higher-order smoothing techniques (SoS and Gaussian Dipole smoothing) are mostly beneficial
for increasing the certificates of images with small certified radii. In cases where certificates are
already large, increased estimation error can lead to a decrease in certificates, but this effect is small
relative to the magnitudes of these certificates (typically < 1%).
6	Conclusion
In this work, we explored the limits of using gradient information to improve randomized smooth-
ing certificates. In particular, we introduced second-order smoothing certificates and showed tight
and realizable upper bounds on their maximum achievable benefits. We also proposed Gaussian
dipole smoothing, a novel method for robustness certification, which can improve smoothing-based
robustness certificates even on large-scale data sets. This introduces a broader question for future
work: what other information about the spacial distribution of classes in randomized smoothing can
be efficiently used to improve robustness certificates?
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out Lipschitz function approximation. vol-
Ume 97 of Proceedings of Machine Learning Research, pp. 291-301, Long Beach, Califor-
nia, USA, 09-15 Jun 2019. PMLR. URL http://Proceedings.mlr.press∕v97∕
anil19a.html.
Nicholas Carlini, Guy Katz, Clark Barrett, and David L Dill. Provably minimally-distorted adver-
sarial examples. arXiv preprint arXiv:1709.10207, 2017.
9
Under review as a conference paper at ICLR 2021
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research,pp. 1310-1320, Long Beach, California, USA, 09-15 JUn 2019. PMLR. URL http:
//proceedings.mlr.press/v97/cohen19c.html.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient
and accUrate estimation of lipschitz constants for deep neUral networks. In Advances in Neural
Information Processing Systems, pp. 11427-11438, 2019.
HUijie Feng, ChUnpeng WU, GUoyang Chen, Weifeng Zhang, and Yang Ning. RegUlarized training
and tight certification for randomized smoothed classifier with provable robUstness. In The Thirty-
Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative
Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12,
2020, pp. 3858-3865. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/
AAAI/article/view/5798.
Sven Gowal, KrishnamUrthy Dvijotham, Robert Stanforth, RUdy BUnel, Chongli Qin, Jonathan Ue-
sato, Relja Arandjelovic, Timothy Mann, and PUshmeet Kohli. On the effectiveness of interval
boUnd propagation for training verifiably robUst models. arXiv preprint arXiv:1810.12715, 2018.
Xiaowei HUang, Marta Kwiatkowska, Sen Wang, and Min WU. Safety verification of deep neUral
networks. In International Conference on Computer Aided Verification, pp. 3-29. Springer, 2017.
Mathias LecUyer, Vaggelis Atlidakis, Roxana GeambasU, Daniel HsU, and SUman Jana. Certified
robUstness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Alexander Levine, Sahil Singla, and Soheil Feizi. Certifiably robUst interpretation in deep learning.
CoRR, abs/1905.12105, 2019. URL http://arxiv.org/abs/1905.12105.
Bai Li, ChangyoU Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robUstness with
additive noise. In Advances in Neural Information Processing Systems, pp. 9464-9474, 2019a.
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Jorn-Henrik Jacobsen.
Preventing gradient attenUation in lipschitz constrained convolUtional networks. In Advances in
neural information processing systems, pp. 15390-15402, 2019b.
Jeet Mohapatra, Ching-Yun Ko, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Higher-
order certification for randomized smoothing. Advances in Neural Information Processing Sys-
tems, 33, 2020.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10877-10887, 2018.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems, pp. 11292-11303, 2019.
Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial at-
tacks. In Proceedings of the 37th International Conference on Machine Learning (pre-
proceedings), 2020. URL https://proceedings.icml.cc/static/paper_files/
icml/2020/2933-Paper.pdf.
Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with
mixed integer programming. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=HyGIdiRqtm.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
10
Under review as a conference paper at ICLR 2021
Aladin Virmaux and Kevin Scaman. Lipschitz regularity of deep neural networks: analysis and
efficient estimation. In Advances in Neural Information Processing Systems, pp. 3835-3844,
2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Huan Zhang, Pengchuan Zhang, and Cho-Jui Hsieh. Recurjac: An efficient recursive algorithm for
bounding jacobian matrix of neural networks and its applications. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pp. 5757-5764, 2019.
A Proofs
A.1 Simple proof of Theorem 1 from Cohen et al. (2019)
We first provide a novel, simple, and intuitive proof for first-order randomized smoothing: this will
allow us to develop methods and notations for later proofs.
Theorem. Let E 〜N(0, σ2I). For all x, x0 with ∣∣x 一 x0∣∣2 < P, andfor all f : Rd → [0,1]:
Ee [f (W + E)] ≥ Φ (Φ-1 (Ee[f (X + E)]) - P)	(10)
Where Φ is the normal cdf function and Φ-1 is its inverse.
Proof. Let R = ∣x 一 x0∣2. Choose our basis so that x = 0 and x0 = [R, 0, 0, ..., 0]T (Note that by
isometry, We still have E 〜N(0, σ2I)). Then define g : R → [0,1]:
g(z) = Ee2,...,en [f([z, E2,..., En]T)]	(11)
Note that:
Ee1[g(E1)]=Ee[f(x+E)]	(12)
Ee1[g(R+E1)]=Ee[f(x0+E)]	()
Now, in one dimension, ei 〜N(0, σ), and so has apdf of Z → σ-1Φ0 (Z), where Φ0 is the normal
pdf function. By the definition of expected value:
Eei [g(Eι)] = /	g(Eι)σ-1Φ0 (e1 ) dEi
Z∞
g(R+EI)σ 1φ0 (^σ^) dEI
Z∞
g(E1)σ-1Φ0
∞
E1	R
σσ
dE1
(13)
—
We perform a change of integration variables, using y = Φ (eσ1) (and noting that ddy
σ-1φ0 (⅛)):	ɪ
Ee1[g(E1)]
Ee1 [g(R + E1)]
Z g(σΦ-1(y))σ-1Φ0
0
g(σΦ-1(y))σ-1Φ0
0
E1 dE1
σ)访dy
Z g(σΦ-1(y))dy
0
El
σ
R)⅛ dy
(14)
—
1	1 Φ0 (Φ-1(y) — R)
/ g(σφ (y)) Φo(Φ*(y))	dy
Note that:
Φ0 (Φ-1(y)- R)
Φ0 (Φ-1(y))
e-2 (φ-1(y))2
eφ-1(y)-JR
(15)
11
Under review as a conference paper at ICLR 2021
Also, to simplify notation, define gΦ : [0, 1] → [0, 1] as gΦ (y) := g(σΦ-1 (y)). Then we have
(combining Equations 14 and 15):
E1[g(1)]
Z1
0
gΦ(y)dy
E1 [g(R + 1)]
Z1
0
gφ(y)eφ ι(y)-2σ2dy
(16)
Fix the expectation atx, E1 [g(1)], at a constant C, let us consider the function gΦ which minimizes
the expectation at x0 :
12
E"g(R + ⑴]≥ rminr ,/ gφ(y)eφ (y)-2σ2dy	(17)
gΦ∈[0,1]→[0,1] 0
R01 gΦ (y)dy=C
TT	. .. . Φ-1(y)--R2 .	, . H	JIElj	. .	. I .	1 ,
However, note that e	(y) 2σ2 increases monotonically With y. Then the minimum is achieved at:
ify≤C
ify>C
(18)
In terms of the function g(z), this is:
*/_ 1 1	if Z ≤ σΦ-1(C)
g (z) =	0	ifz > σΦ-1(C)
(19)
Then we can evaluate the minimum, using the form of the integral given in Equation 13:
E1 [g(R + 1)]
≥Z-	∞ g*(e1)σ- ∞	1Φ0	∖σ	R	de1
=Z-	σΦ-1(C) σ- ∞	1Φ0(	e1 		 σ	-R)	de1
=Φ	(σΦ-1(C)	R -		)-	Φ(	-∞
	∖ σ	σ	J	k	σ
=Φ	(Φ-1(C)-	R)			
(20)
R
By the definition of C and Equation 12, this is:
EJf(x0 + e)] ≥ Φ (φ-1 (Ee[f(X + e)]) - R) ≥ Φ (φ-1 (Ee[f(x + 划)-P)	(21)
which was to be proven.
□
A.2 Second Order Smoothing
Theorem 1. Let e 〜N(0, σ2I). Forall x, x0 with ∣∣x 一 x0∣∣2 < P, andfor all f : Rd → [0,1],
Eq [f (x0 + e)]	≥ Φ	(φ-1(a0	+ Ee[f (x	+ e)])	-	P)	一 Φ (φ-1(a0)	-	P)	(22)
Where Φ is the normal cdf function, Φ-1 is its inverse, and a0 is the (unique) solution to
Φ0(φT(a0)) - Φ0(φT(a0 + E”(x + e)])) = -σ∣VχE”(x + e)]∣2	(23)
Further, for all pairs (Ee [f (x + e)]), ∣VχEe[f (x + e)]∣∣2) which are possible, there exists a base
classifier f and an adversarial point x0 such that Equation 4 is an equality.
As show by Salman et al. (2019), we have, for all x00 ∈ Rd:
Vx00Ee[f(x00+e)] =σ-2Ee[ef(x00+e)]	(24)
Under the choice of basis of the above proof (in particular, x = 0), when evaluated at x this becomes:
VxEe[f(x+ e)] = σ-2Ee[ef(e)]	(25)
12
Under review as a conference paper at ICLR 2021
Let u := [1, 0, 0, ..., 0]T, and define g as in the above proof. Note that:
-kVχEe[f (x + e)]∣∣2 ≤u ∙ VχEe[f (x + e)]
=U ∙ σ-2Ee[ef (e)]
=σ-2E[1f()]
=σ-2E1[1[E2,..,df()]]
=σ-2E1 [1g(1)]
(26)
By the definition of expectation, and again using the change of integration variables y := Φ (^1),
E1 [1g(1)] = Z σΦ-1 (y)g(σΦ-1 (y))σ-1Φ0
0
=	σΦ-1 (y)g(σΦ-1 (y))dy
0
d1
Fdd
Define:
C :=E1[g(1)]	(=E[f(x+))
C0 :=E1[1g(1)] (≥ -σ2kVxE[f(x+)]k2)
Then, by Equations 16 and 27, and defining gΦ, as above,
1	-1	R2
Eei [g(R + ⑴]≥	min r ι / gφ(d)eφ (y)-2σ2dy
gΦ∈[0,1]→[0,1]	0
R01 gΦ(y)dy=C
R01 σΦ-1(y)gΦ (y)dy=C0
(27)
(28)
(29)
Note that our constraints are linear in the space of functions; we can then introduce Lagrange mul-
tipliers:
min Z gφ(y)eφ (y)-R2dy — λι
gΦ∈[0,1]→[0,1] 0
— λ2
Z gΦ(y)dy — C
Z σΦ-1(y)gΦ(y)dy — C0
(30)
….	Φ φ φ Φ-1(y)--Ri-
min	/	gφ(d)e	2σ2
gΦ∈[0,1]→[0,1] 0 Φ
dy —
λ1gΦ(y) — λ2σΦ-1 (y)gΦ(y)dy + constants
(31)
min	gΦ(y)
gΦ ∈[0,1]→[0,1] 0
(eφ Is)-2σ2 — λι — λ2σΦ-l(y)) dy +
constants
This is simply the inner product between gΦ and a function: the inner product is minimized by
setting gΦ = 1 where the expression is negative, and gΦ = 0 where the expression is positive:
*()= ( 1	if eφ-1(y)-2σ2 ≤ λι + λ2σΦ-l(y)
gφ(d) = 1 0	if eφ-1(y)-2σ2 >λι + λ2σΦ-i(y)
(32)
Sign changes occur at:
(R2 —λl ∖
e-2σ2- λ2 ∖ λι
—ʌ— I — 丁	(33)
λ2	λ2
Where W is the product-log (Lambert W) function. This returns zero, one, or two possible values,
depending on the argument (zero values if the argument < —e-1, two values on [—e-1, 0), and one
value on non-negative arguments). Therefore there are at most two sign changes. Also, note that as
y → 1, Φ-1(y) → ∞, taking the limit in Equation 32, we know that
gΦ* (1) = 0.
Therefore, taking into account the constraint R01 gΦ(y)dy = C, we know that gΦ* is either:
13
Under review as a conference paper at ICLR 2021
•	0 everywhere (and C = 0), if no sign changes
•	1 at y < C, 0 otherwise, if one sign change
•	1 on the interval [a, a + C], 0 otherwise, for some a ∈ [0, 1 - C], if two sign changes
In fact, the final case includes the first two, so all that we need to do now is find a to satisfy the C0
constraint. This constraint (Equation 27) becomes:
a+C	C0
L φ-1(y)dy=v
(34)
Because Φ-1(y) is monotone increasing, the LHS of Equation 34 is a monotone increasing function
of a. Using the indefinite integral of Φ-1:
/ Φ-1 (y)dy = I √2 erf-1(2y - Ddy = -√= e-(erf-1(2y-1))2 + C = -Φ0(Φ-1(y)) + C
Where erf-1 is the inverse error function. Then the constraint becomes:
C0
Φ0(Φ-1(a)) - Φ0(Φ-1(a + C))=—
σ
(35)
We can now evaluate the value of the smoothed function at x0, again using the form of the integral
given in Equation 13:
E1 [g(R + 1)] ≥
Z∞
g*(eι)σ-1 Φ0
∞
d1
σΦ-1 (a+C)
I	σ-1Φ0 ( -1
σΦ-1 (a)	σ
R"ι
σ
(36)

—
R
σ
—
σ
σ
Φ (φT(a + C) - R) - Φ fφ-1(a) - R
If we consider the form of this integral in Equation 29, which reduces to simply:
a+C	2
/ eφ-3)-2σ2 dy
a
(37)
we see that is is a monotonically increasing function in a. Furthermore, we have that the LHS of
Equation 35 is monotonic in a. Therefore, ifwe define a0 as the solution to:
Φ0(φT(a0)) - Φ0(Φ-1(a0 + C)) = -σ∣∣VχE”(X + -)]k2	(38)
Then by Equation 26, we know a0 ≤ a. Then because the RHS of Equation36 is also monotonic in
a,
Eq[g(R + -1)] ≥ φ(φ-1(a0 + C)- R) - φ(φ-1(a0)- R)	(39)
By the definitions ofg and C, and because the RHS is monotonically decreasing in R (See Equation
37), we can conclude the theorem as stated.
Further, we can conclude that an equality case is possible by noting that it is achieved by the function
g*(z) as described above: the minimal f*(z) can then be constructed as f*(z, ∙, ∙,...) := g*(z).
Note that we also need Equation 26 to be tight: this is achieved where the adversarial direction
X0 - X is parallel to the gradient of the smoothed function.
A.3 Practical Certification Algorithm
Define
Note that
C := lower bound on C
C0 := upper bound on C0
kVxE[f(X+)]k22=
σ-4Ee[-f (-)] ∙ Ee[-f(-)]=
σ-4Ee[-f (-)] ∙ Eeo [-0f (-0)]=
σ-4Ee,e0[(-f (-)) ∙(-0f (-0))]
(40)
(41)
14
Under review as a conference paper at ICLR 2021
~
Theorem 2.	Let V := Eee [(ef (X + e)) ∙ (e0f (X + e0))], and V be its empirical estimate. If npairs
of samples (= N/2) are used to estimate V, then, with probability at most η, E[V] - V ≥ t, where:
(4σ2，-n ln㈤
I-4√nσ2 ln(η)
t
if -2 ln(η ) ≤ dn
if-2 ln(η) > dn
(42)
Wainwright (2019) gives the following condition for any centered (mean-zero) sub-exponential ran-
dom variable X :
Definition 1. A centered R.V. X is (a,b)-subexponential if:
E[eλX] ≤ ea2λ2∕2,	∀ λ ∈ [-b-1,b-1]	(43)
First, We establish bounds for E ∙屋 (This can be considered a simplified case of Gaussian chaos
of the second order, see Vershynin (2018)). For each i ∈ [d], i and 0i are independent Gaussian
random variables. Recall the moment-generating function for a Gaussian:
E[etei] = eσ2t2∕2 ∀ t
Then for the product Ei E0i We have that:
E[eλeie0i] = EeiEe0 [[eλeie0i]] = Eei [eei2(λ2σ2∕2)]
(44)
(45)
Note that this has a similar form to the moment generating function of the Chi-squared distribution
for k = 1:
Eei[eei2t]
1
√1 - 2σ2t
(46)
Then:
E[eλeiei] = E“ [ee2(12σ2∕2)] = ]、 ∖ 4 ≤ eλ2σ4	∀λ2σ4 ≤ 1	(47)
1 - λ2σ4	2
Where the final inequality can be shoWn by observing that, if λ2σ4 ≤ 1/2:
1	_	λ2σ4
1 - λ2σ4 = + 1 - λ2σ4
≤ 1 + 2λ2σ4 ≤ e2λ2σ4
(48)
and taking square roots. Because Eiei is centered, this implies that EiEi is (√2σ2, √2σ2)-
SUbexPonentiaL Now, e ∙ e0 is simply the sum of d such identical, independent, centered subexponen-
tial variables: by (Wainwright (2019) Equation 2.18), we conclude that e ∙ e0 is (√2σ2√d, √2σ2)-
subexponential. This implies:
E[eλe∙e0] ≤ e2σ4dλ2∕2,	∀ λ ∈ [-(√2σ2)-1, (√2σ2)-1]	(49)
Recall that the quantity which we are measuring is (Ef (E)) ∙ (Ef (e0)). For notation convenience, let
v(E, E0) : Rd × Rd → {0, 1} be defined as f (E)f (E0), so that the quantity of interest is
V := e ∙ e0v(e, e0)	(50)
We further define a centered version of this quantity
V0 := V-E[V]	(51)
We now introduce an important lemma:
Lemma 1. V0 is (2√2σ2 √d, 2√2σ2)-subexponential.
Proof. Define
p := Pr[v(E, E0) = 1]	(52)
e,e0
Then:
V = E ∙ e0v(e, e0) = E ∙ E0 — (1 — v(e, e0))e ∙ E0	(53)
E[V] = E[e ∙ E0] — E[(1 — v(E, e0))e ∙ E0] = —E[(1 — v(e, e0))e ∙ e0]	(54)
15
Under review as a conference paper at ICLR 2021
EIV] = E[e ∙ ev(e, ez)] = pE[e ∙ e∖v(e, ez) = 1] = -(1 — p)E[e ∙ t[v(g J) = 0]
Therefore
V0 = e ∙ Jv(e, e)) — Ek ∙ Jv(e, e))]
=J ∙ JV(J J0) — V(J J0)E[j ∙ JV(J J0)] — (1 — V(J J0))E[j ∙ JV(J J)]
(55)
(56)
V0 = J ∙ JV(J J) — PV(J J)Ek ∙ JV(J J)∖v(j, J) = 1]
+ (1 — p)(1 — v(j, J))E[j ∙ Jv(j, J)∖v(j, J) = 0]	⑶'
Define:
A := J ∙ JV(J J) + (1 — V(J J))E[j ∙ J∖v(j, J) = 0]	(58)
B := -PV(J j0)E[j ∙ JV(J j0)∖(j, J) = 1] — p(1 — v(j, J))E[j ∙ J∖v = 0]	(59)
V0 = A + B	(60)
Trivially, we have:
E[eλe"] = pE[eλe"∖v(e, J) = 1] + (1 — p)E[eλe" ∖v(j, J) = 0]	(61)
However, note that also:
E[eλA] = pE[eλA∖v(j, J) = 1] + (1 — p)E[eλA∖v(e, J) = 0]
E[eλA] = pE[eλ"∖v(e,e 0) = 1] + (1 — p)eλE[e^,∖v(e,^)=o]
Then by Jensen,s inequality, we have:
E[eλA] ≤ E[e"e 0] ∀λ
Similarly:
E[eλB] = pE[eλB ∖v(e, J) = 1] + (1 — p)E[eλB ∖v(e, J) = 0]
E[eλB] = pe-PλE[e.e'∖v(3e')=l] + (1 — p)e-pλE[e∙e,∖v(e,e,) = O]
Again, by Jensen,s inequality:
E[eλB] ≤ E[e-pλ-e 0] ∀λ
E[eλB] ≤ E[e-pλ°e0] ≤ e^城火/ ≤	∀ — pλ ∈ [—(√2σ2)-1, (√2σ2)-1 ]
Because P ≤ 1, we then have:
E[eλB] ≤ e2σ4dλ/2,	∀ λ ∈ [—(√2σ2)-1, (√2σ2)-1]
(62)
(63)
(64)
(65)
(66)
(67)
In other words, we have shown that both A and B are both (√2σ2 √d, √2σ2)-subexponential. Then,
by Cauchy-Schwartz:
E[eλV0]
=E[eλAeλB]
/-------------- (68)
≤ ,E[e2λA]E[e2λB]
≤ √e8σ4dλ2∕2e8σ4dλ/2 ∀2λ ∈ [—(√2σ2)-1, (√2σ2)-1]
E[eλV0] ≤ e8σ4dλ 2/2 ∀λ ∈ [—(2√2σ2)-1, (2√2σ2)-1]	(69)
In other words, V0 is (2√2σ2√d, 2√2σ2)-subexponential.	□
Finally, using the form of the one-sided Bernstein tail bound for subexponential random variables
given in Wainwright (2019), we have, given n measurements and an empirical mean estimate of V
as V :
Pr(E[V] - V ≥ t) ≤ (eM ift ≤ 2√2σ2d
Ie4√2σ2	if t > 2√2σ2d
(70)
Then, given a failure rate η, we can compute the minimum deviation t such that the failure proba-
bility is less than η:
t = (4σ2 J-* ln(η) if —2 ln(η) ≤ dn
I- 4√2σ 2 ln(η)	if —2ln(η) > dn
(71)
16
Under review as a conference paper at ICLR 2021
A.4 Dipole Smoothing
Theorem 3.	Let E 〜N (0,σ2I). For all x, x0 with ∣∣x 一 x0∣∣2 < P, and for all f : Rd → [0,1],
define:
CS= Ee[f (X + e)f (X - e)]	(72)
C N := Ee[f(x + O- f (x + E)f (x -E)]
Then:
Eeι[f(x0 + E)] ≥ Φ (Φ-1(CN) — σ)
+φ (φ-ι(1+2C-)-σ)	(73)
-φ S=)-ρ)
Where Φ is the normal cdf function and Φ-1 is its inverse.
Proof. As in the proof of Theorem A.1, let R = ∣x - x0∣2, and choose our basis so that x = 0 and
x0 = [R, 0, 0, ..., 0]T.
First, for f : Rd → [0, 1], we define a decomposition into symmetric and non-symmetric compo-
nents, fS, fN : Rd → [0, 1]:
fS(E):= f(E)f(-E)
fN(E):= f(E) - f(E)f(-E)
(74)
Note that f(E) = fS(E) + fN(E) and also that fS(E) = f S (-E). Define gS(z), gN (z) : R → [0, 1]
by analogy to Equation 12. By linearity of expectation, note that g(z) = gS(z) + gN (z). Also note
that:
gS(-z) = Ee2,...,en [fS([-z, E2,..., En]T)]
= E-e2,...,-en [[fS([-z, -E2, ..., -En]T)]
= Ee2,...,en [[fS([-z, -E2, ..., -En]T)]	(75)
= Ee2,...,en [[fS([z, E2, ..., En]T)]
= gS(z)
Similarly, define gΦS and gΦN . We still have:
gΦ(y) = g(σΦ-1(y)) = gS(σΦ-1(y)) +gN(σΦ-1(y)) = gΦS (y) + gΦN(y)	(76)
Also (using Φ-1(y) = -Φ-1(1 - y)):
gΦS(y) = gS(σΦ-1(y)) = gS(-σΦ-1(1 - y)) = gS (σΦ-1(1 - y)) = gΦS(1 - y)	(77)
Note that all of the mechanics of the proof of Theorem 1 can be applied to f, fS and fN . Following
Equation 13, we have:
CS=Ee[fS(x+E)]
CN =Ee[fN(x+E)]
gΦS(y)dy
0
gΦN (y)dy
0
(78)
C := Ee [f(x + E)] = Z gΦ(y)dy = Z gΦS(y) + gΦN (y)dy = CS + CN
17
Under review as a conference paper at ICLR 2021
We may then write the minimization in Equation 17, fixing CN and CS as constants separately2:
12
Eeι[g(R + eι)] ≥ q IVmin / gφ(y)eφ (y)-2σ2dy
gΦ,gΦ ∈[0,1]→[0,1] 0
R01 gΦS (y)dy=CS
R01 gΦN (y)dy=CN
1	1	R2
S N min	I (gφ(y) + gφ(y))e	(y)-2σ2 dy
gΦ,gΦ ∈[0,1]→[0,1] 0
R01 gΦS(y)dy=CS
R01 gΦN (y)dy=CN
(79)
12
min	gS(y)eφ (y)-2σ2 dy
gΦ∈[0,1]→[0,1]	0
R01 gΦS (y)dy=CS
12
+ (V min / gφ(y)eφ Yy)-2σ2 dy
gΦ ∈[0,1]→[0,1]	0
R01 gΦN(y)dy=CN
The second minimum can be computed as in the proof of Theorem 1, it is simply
Φ (Φ-1(CN) - R). For the first minimum, We consider the additional constraint, that gφ(y)=
gΦS (1 - y). Then we can rewrite the integral as:
Z 1 gΦS(y)eΦ
0
R2
(y)-2σ2 dy
=Z； gφ(y)eφ-1(y)-R2dy + Z号 1 gφ(1 - y)eφ-1(y)-R2dy
=J； gφ(y)eφ-1(y)-R2dy + J； gφ(y0)eφ-1(1-y0)-R2(-i)dy0
=∕2 gφ(y)eφT(y)-R2dy + ∕2 gφ(y0)e-φT(y0)-R2dy,
00
1
=e--R / gφ(y) [eφ i(y) + e-φ "y)] dy
1
_R2 C u,、	, ι ,、、
=2e 2σ2	gS(y) cosh(Φ 1(y))dy
0
So the minimization becomes:
(80)
-R2
min	2e 2σ2
gΦ ∈[0,2 ]→[0,1]
Ro2 gΦ(y)dy=1CS
1
g gφ(y)cosh(Φ-1(y))dy
0
(81)
Note that cosh(Φ-1(y)) is a monotonically decreasing function of y on the range [0,11 ]. Then the
minimum is achieved by the function:
gS*(y) = ∣0
if y< S-CS
if ɪ ≤ y ≤ 2
Where the value in the domain [ɪ, 1] can be computed using gS*(1 - y)
function gS (z), this is:
产(z)={0
if |z| ≤ σΦ-1( 1+cs)
if |z| > σΦ-1( 1+Cs )
(82)
gS*(y) In terms of the
(83)
2Note that We are not considering all applicable constraints here: in particular We are not restricting the
range ofgΦ(z) to [0, 1] explicitly. HoWever, the loWer bound presented here must be at least as loW as the loWer
bound With this additional constraint, so the inequality is still valid. Also, this constraint does in fact hold in
the final construction.
18
Under review as a conference paper at ICLR 2021
We can now evaluate the integral, again using the form of the integral given in Equation 13:
R
σ
MgS (R+”广产⑺…色
d1
∕σΦ-1( 1+CS)
σ-1
-σφ-ι( ( l+CS )
Φ0
σσ
d1
Φ Φ-1(
Φ Φ-1(
1	+ CS
-	2-
1	+ CS
-	2-
R) - φ"ι-
σ k '2
(84)
R) - φ(φ-l(
1-CS
)
)
—
1 R
—
—
—
2
)-R
σ
)-R
σ
So, combining the gS and gN terms, we have:
E( [g(R + 1)] ≥Φ	Φ-1(CN) -
+ Φ Φ-1(
- Φ Φ-1(
1 + CS	R
-2-)- σ
广)- R
(85)
From Equation 12, and noting that the last two terms together are monotonically decreasing3 with
R, We complete the proof.	□
B Additional Experiments
Here, We present experiments at a Wider range of parameters. For all figures, images misclassified or
not certified for both the baseline and the tested method are not counted: the total test set size is 1000
for MNIST, 500 for CIFAR and ImageNet With N = 105, and 100 for CIFAR and ImageNet With
N = 106. For all experiments, N0 = 100. Also, note that We test independently for the baseline
and higher-order methods (i.e., We use different smoothing samples). This is necessary to compare
fairly to dipole smoothing, Where the sampling method is different; hoWever, it does lead to some
noise, especially at N = 105.
B.1	NOISE LEVEL σ.
We see (Figures 7 and 8) that at a smaller level of noise (σ = 0.12), the effect of higher-order
smoothing is diminished. This can be understood in terms of the curves in Figure 1-b: loWer noise
leads to more inputs With higher pa , Which reduces the benefit of the higher-order certificate. Con-
versely, higher noise increases the effects of the higher-order certificates, although it also leads to
decreased total accuracy. The dipole certificate underperforms at N = 105, σ = 0.5: this is likely
due to the increase in estimation error, Which becomes significant near pa = 0.5.
B.2	DIMENSIONALITY d
To test second-order smoothing on a loWer-dimensional dataset, We performed PCA on the 7 × 7
MNIST images, and classified using the top 10 principal components. (d = 10). Results are shoWn
in Figures 9, 10, and 11. We see that, at N = 106, second-order smoothing has a marginal positive
impact at this smaller scale.
B.3	Dipole Smoothing on CIFAR- 1 0
In Figure 12, We see experiments on CIFAR-10 using dipole smoothing, for a range of σ ∈
{0.12, 0.25, 0.50, 1.00} and N ∈ {105, 106}. Note that dipole smoothing appears to be beneficial
even at N = 105 on CIFAR-10, at all noise levels ≥ 0.25.
3To see this, note the form of the integral of gS equal to these terms given in Equation 80
19
Under review as a conference paper at ICLR 2021
B.4	Dipole Smoothing on ImageNet
In Figure 13, we see experiments on ImageNet using dipole smoothing, for a range of σ ∈
{0.25, 0.50, 1.00} and N ∈ {105, 106}. There is an anomalous result for σ = 0.50, N = 105,
in that this is the only case where dipole smoothing appears to perform worse than standard smooth-
ing. However, this turns out to be a computational artifact. At both σ = 0.50 and σ = 0.25, there are
a large number of images where every smoothing sample is correctly classified, so pa is as close to
1 as the measurement bounds allow. Note that if pa truly equals 1, the certified radius is infinite, so
in this domain, the reported certificate is entirely a function of the estimation error. Because dipole
smoothing reduces measurement precision, these samples have somewhat smaller certified radii un-
der dipole smoothing, especially at small N . However, this gap should be exactly proportional to σ .
The cause of the anomaly is the fact that our code (adapted from (Cohen et al., 2019)) records each
radius to three significant figures. At σ = 0.5, for an image where all noise samples are correctly
classified, the ratio of the dipole smoothing radius to the standard smoothing radius is reported as
1.89/1.91 = 98.95%, while for σ = 0.25 itis reported as 0.947/0.953 = 99.37%. This explains
the large number of samples with reported > 1% decrease in certificates for σ = 0.50, N = 105 .
C Absolute Certificates for main-text experiments
In Figures 14 and 15, we show the absolute, rather than relative, values of the certificates reported in
the main text, compared to the baseline first-order randomized smoothing. We see that the benefit of
the proposed techniques is greatest for images with small absolute certificates, and that, on CIFAR-
10 and ImageNet, there is some disadvantage to dipole smoothing on the largest possible certificates,
where all smoothing samples are classified correctly. This is because, for these images, the certificate
depends entirely on estimation error.
20
Under review as a conference paper at ICLR 2021
SoS, 7X7 MNIST, o=0.12, N=Io5_____________________________ DiPOIe, 7X7 MNIST, 0=0.12, N=IO5
800
700
600
500
400
300
200
IOO
Figure 7: 7 × 7 MNIST, σ = 0.12
Oooooooo
8070605040302010
21
Under review as a conference paper at ICLR 2021
Figure 8: 7 × 7 MNIST, σ = 0.50
22
Under review as a conference paper at ICLR 2021
Figure 9: 10 PC MNIST, σ = 0.12
23
Under review as a conference paper at ICLR 2021
Figure 10: 10 PC MNIST, σ = 0.25
24
Under review as a conference paper at ICLR 2021
Figure 11: 10 PC MNIST, σ = 0.50
25
Under review as a conference paper at ICLR 2021
Figure 12: CIFAR-10
26
Under review as a conference paper at ICLR 2021
DipoIeJmageNet, α=0.25, N=Io5
Dipole, ImageNetl σ=0.25, N=IO6
Figure 13: ImageNet
Dipole, ImageNetl σ=0.50, N=IO6
Dipole, ImageNetl σ=1.00, N=IO6
27
Under review as a conference paper at ICLR 2021
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 14: Experiments on 7 × 7 MNIST. For all, σ = 0.25. For (a, c, e, g), Second-order
Smoothing is used. For (b, d, f, h), Gaussian dipole smoothing is used. For (a, b), N = 105. For (c,
d), N = 106. For (e, f), N = 107. For (g, h), N = 108.
28
Under review as a conference paper at ICLR 2021
(c)
(b)
AUeJrme pəttəu
Figure 15: Dipole smoothing experiments, with σ =
d). For (a, c), N = 105. For (b, d), N = 106.
(d)
AoeJΓ∞e pəttəu
0.25,
on CIFAR-10 (a, b) and ImageNet (c,
29