Under review as a conference paper at ICLR 2021
Recurrently Controlling a Recurrent Net-
work with Recurrent Networks Controlled
by More Recurrent Networks
Anonymous authors
Paper under double-blind review
Ab stract
This paper explores an intriguing idea of recursively parameterizing recurrent nets.
Simply speaking, this refers to recurrently controlling a recurrent network with
recurrent networks controlled by recurrent networks. The proposed architecture
recursively parameterizes its gating functions whereby gating mechanisms of X-
RNN are controlled by instances of itself, which are repeatedly called in a recursive
fashion. We postulate that our proposed inductive bias provides modeling benefits
pertaining to learning with inherently hierarchically-structured sequence data.
To this end, we conduct extensive experiments on recursive logic tasks (sorting,
tree traversal, logical inference), sequential pixel-by-pixel classification, semantic
parsing, code generation, machine translation and polyphonic music modeling,
demonstrating the widespread utility of the proposed approach, i.e., achieving
optimistic and competitive results on all tasks.
1	Introduction
We explore the intriguing idea of recursively parameterizing recurrent networks. In our proposed
approach, recurrent networks are controlled by recurrent networks which may or may not be controlled
by more recurrent networks.
Why would we do something like this? In short, our proposed method, X-RNN (Recurrently
Controlling Recurrent Networks with Recurrent Networks Controlled by more Recurrent Networks1),
marries the benefits of recursive reasoning with recurrent models. Notably, we postulate that
this formulation brings about benefits pertaining to modeling data that is intrinsically hierarchical
(recursive) in nature, e.g., natural language, music and logic, an increasingly prosperous and emerging
area of research (Shen et al., 2018; Wang et al., 2019; Choi et al., 2018).
In X-RNN, the gating functions of our model are now parameterized repeatedly by instances of itself
which imbues our model with the ability to reason deeply and recursively about certain inputs. To
achieve the latter, we propose a soft dynamic recursion mechanism, which softly learns the depth of
recursive parameterization on a per-token basis. Notably, this is reminiscent of Adaptive Computation
Time (ACT) (Graves, 2016), albeit operation at the parameter level2. Our formulation can be
interpreted as a form of meta-gating since temporal compositionality is now being meta-controlled at
various levels of abstractions.
Our Contributions Overall, the key contributions of this work are as follows:
•	We propose a new sequence model - X-RNN. Our model is distinctly characterized by
recursive parameterization of recurrent gates, i.e., compositional flow is controlled by
instances of itself, d la repeatedly and recursively. We propose a soft dynamic recursion
mechanism that dynamically and softly learns the recursive depth of the model at a token-
level.
1We refer this to as X-RNN for short for the remainder of the paper.
2While seemingly similar, these methods are very different in the context of what the objective is. Our goal
is to dynamically expand the parameters of the model, not dynamically decide how long to deliberate on input
tokens.
1
Under review as a conference paper at ICLR 2021
•	We evaluate our proposed method on a potpourri of sequence modeling tasks, i.e., logical
recursive tasks (sorting, tree traversal, logical inference), pixel-wise sequential image
classification, semantic parsing, neural machine translation and polyphonic music modeling.
X-RNN achieves optimistic and competitive results.
2	Related Work
The study of effective inductive biases for sequential representation learning has been a prosperous
research direction. This has spurred on research across multiple fronts, starting from gated recurrent
models (Hochreiter & Schmidhuber, 1997; Cho et al., 2014), convolution (Bai et al., 2018a) to the
recently popular self-attention based models (Vaswani et al., 2017).
The intrinsic hierarchical structure native to many forms of sequences has long fascinated and inspired
many researchers (Socher et al., 2013; Bowman et al., 2014; 2016; Dyer et al., 2016). The study
of recursive networks, popularized by (Socher et al., 2013) has provided a foundation for learning
syntax-guided composition in language processing research. Along the same vein, (Tai et al., 2015)
proposed Tree-LSTMs which guide LSTM composition with grammar. Recent attempts have been
made to learn this process without guidance nor syntax-based supervision (Yogatama et al., 2016;
Shen et al., 2017; Choi et al., 2018; Havrylov et al., 2019; Kim et al., 2019). Ordered Neuron
LSTMs (Shen et al., 2018) proposed structured gating mechanisms, imbuing the recurrent unit with a
tree-structured inductive bias. (Tran et al., 2018) shows that recurrence is important for modeling
hierarchical structure. Notably, learning hierarchical representations across multiple time-scales
(El Hihi & Bengio, 1996; Schmidhuber, 1992; Koutnik et al., 2014; Chung et al., 2016; Hafner et al.,
2017) have also demonstrated reasonable success.
Learning an abstraction and controller over a base recurrent unit is also another compelling direction.
First proposed by Fast Weights (Schmidhuber, 1992), several recent works explore this notion.
HyperNetworks (Ha et al., 2016) learns to generate weights for another recurrent unit, i.e., a form
of relaxed weight sharing. On the other hand, RCRN (Tay et al., 2018) explicitly parameterizes the
gates of a RNN unit with other RNN units. Recent attempts to speed up the recurrent unit are also
reminiscent of this particular notion (Bradbury et al., 2016; Lei et al., 2018).
The marriage of recursive and recurrent architectures is also notable. This direction is probably the
closest relevance to our proposed method, although with vast differences. (Liu et al., 2014) proposed
Recursive Recurrent Networks for machine translation which are concerned with the more traditional
syntactic supervision concept of vanilla recursive nets. (Jacob et al., 2018) proposed RR-Net, which
learns hierarchical structures on the fly. RR-Net proposes to learn to split or merge nodes at each time
step, which makes it reminiscent of (Choi et al., 2018; Shen et al., 2018). (Alvarez-Melis & Jaakkola,
2016) proposed doubly recurrent decoders for tree-structured decoding. The core of their method is
a depth and breath-wise recurrence which is similar to our model. However, X-RNN is concerned
with learning gating controllers which is different from the objective of decoding trees.
Our work combines the idea of external meta-controllers (Schmidhuber, 1992; Ha et al., 2016;
Tay et al., 2018) with recursive architectures. In particular, our recursive parameterization is also
a form of dynamic memory which gives our model improved expressiveness in similar spirit to
memory-augmented recurrent models (Santoro et al., 2018; Graves et al., 2014; Tran et al., 2016).
3	The Proposed Method
This section introduces our proposed model. X-RNN is fundamentally a recurrent model. The key
difference is that the gating functions that control compositionality over time is parameterized by a
recursion parameterization of itself.
2
Under review as a conference paper at ICLR 2021
3.1	X-Recurrent Unit
Our proposed model accepts a sequence of vectors X ∈ R'×d as input. The main unit of the X-RNN
unit ht = XRNNn(xt, ht-1) is defined as follows:
ftn = σs(αt XRNNn+1(xt, ht-1) + (1 - αt) FnF (xt, ht-1))	(1)
otn =σs(βtXRNNn+1(xt,htn-1)+(1-βt)FnO(xt,ht-1))	(2)
ztn =	σr (FnZ (xt, ht))	and ctn = (1 - ft) ht-1 + (ft) zt	(3)
htn = ot ct	and htn = ht	+ xt	(4)
where σr is a nonlinear activation such as tanh. σs is the sigmoid activation function. In a nutshell,
the X-RNN unit recursively calls itself until a max depth L is hit. When n = L, ft and ot are
parameterized by:
ftL = σs (FLF (xt, ht-1)) and otL = σs(FLO(xt,ht-1))	(5)
where ftL, otL is the forget and output gate of X-RNN at time step t while at the maximum depth L.
We also include an optional residual connection htn = ht + xt to facilitate gradient flow down the
recursive parameterization of X-RNN.
Soft Dynamic Recursion We propose learning the depth of recursion in a data-driven fashion. To
learn αt , βt, we use the following:
αt = Fα (xt) and βt = Fβ (xt)
where F*(xt) = Wxt + b is a simple linear transformation layer applied to sequence X across the
temporal dimension. Intuitively, α, β control the extent of recursion, enabling a soft depth pertaining
to the hierarchical parameterization. Alternatively, we may also consider a static variation where:
``
αt = Fα (	xt ) and βt = Fβ (	xt)
t=0	t=0
where the same value of α, β is computed based on global information from the entire sequence.
Note that this strictly cannot be used for autoregressive decoding. Finally, we note that it is also
possible to assign α ∈ R, β ∈ R to be trainable scalar parameters.
Depth-wise Parameterization Intuitively, Fn ∀* ∈ F, O, Z are depth-wise parameters of X-RNN.
We parameterize Fn with depth-wise RNN units.
Fn(xt ) = RNNn(xt,hn-ι)
Alternatively, we may also parameterize Fn with simple linear transformations.
Fn(xt)= Wnxt+bn
Overall, X-RNN is agnostic to the choice of Fn (xt) and even the RNN unit. Note that for RNN, the
hidden states are initialized with zero and each level uses a new initial hidden state.
Connections to other recurrent models When setting the max depth to n = 1, α = 0, β = 0
and parameterizing all Fn () with linear transforms, this reverts X-RNN to a simple recurrent model
(SRU) Lei et al. (2018) with two gates (forget and output).
ftn =(1 - αt) FnF(xt,ht-1))
otn =(1-βt)FnO(xt,ht-1))
ztn =σr (FnZ (xt, ht))
ctn =(1-ft)ht-1+(ft)zt
htn =ot ct
The key difference between the SRU and standard RNNs is that we do not include the previous hidden
state ht-1 into the gate construction functions. When Fn (xt, ht) = W([xt, ht-1]) + b, this is similar
fashion to the standard recurrent models like LSTMs or GRUs. Similarly, when n = 1 and Fn (.) is a
1D convolution model, this becomes the Quasi RNN model Bradbury et al. (2016). When Fn () is a
LSTM/GRU unit, then this becomes the Recurrently Controlled Recurrent Network Tay et al. (2018)
model.
3
Under review as a conference paper at ICLR 2021
Parallel Variation We postulate that X-RNN can also be useful as a non-autoregressive3 parallel
model.. This can be interpreted as a form of recursive feed-forward layer that is used in place
of recurrent X-RNN for speed benefits. In early experiments, we find this a useful enhancement
to state-of-the-art Transformer (Vaswani et al., 2017) models. The non-autoregressive variant of
X-RNN is written as follows:
ftn = σs(αt XRNNn+1(xt) + (1 - αt) FnF (xt))	(6)
otn = σs(βt XRNNn+1(xt) + (1 - βt) FnO(xt))	(7)
ztn = σr(FnZ(xt))	and	htn	=	(ftn	xt) +	(ot	ztn)	and	htn	=	ht	+xt	(8)
More concretely, we dispense with the reliance on the previous hidden state. This can be used in place
of any position-wise feed-forward layer. In this case, note that Fn(Xt) are typically position-wise
functions as well.
4 Experiments
We conduct experiments on a suite of diagnostic synthetic tasks and real world tasks.
4.1	Logic / Recursive Tasks
We evaluate our model on three diagnostic logical tasks. The tasks are described as follows:
Model	n = EM	3 P	n= EM	Tree Traversal				=8 P	n= EM	10 P	Sort			
				4 P	n= EM	5 P	n EM				n= EM	5 P	n= EM	10 P
BiLSTM	"T00^"	1.0	96.9	2.4	60.3	2.4	5.6	30.6	2.2	132	79.9	1.2	78.9	1.2
S-BiLSTM	100	1.0	98.0	1.0	63.4	2.5	5.9	99.9	2.8	225	83.4	1.2	88.0	1.1
ON-LSTM	100	1.0	81.0	1.4	55.7	2.8	5.5	52.3	2.7	173	90.8	1.1	87.4	1.1
X-RNN	^!00~	1.0	98.4	1.0	63.4	1.8	5.6	20.4	2.8	119	92.2	1.1	90.6	1.1
Table 1: Experimental Results on Tree Traversal and Sorting.
Model	# Operations					
	n = 7	n = 8	n=9	n= 10	n=11	n= 12
Tree-LSTMt (Taietal., 2015)	93.0	90.0	87.0	89.0	86.0	87.0
LSTM (Bowman et al., 2014)	88.0	85.0	80.0	78.0	71.0	69.0
RRNet (Jacob et al., 2018)	84.0	81.0	78.0	74.0	72.0	71.0
Ordered Neurons (Shen et al., 2018)	91.0	87.0	86.0	81.0	78.0	76.0
X-RNN	97.0	95.0	93.0	92.0	90.0	88.0
Table 2: Experimental results on Logical Inference task. f denotes models with access to ground truth syntax.
Results reported from (Shen et al., 2018). X-RNN achieves state-of-the-art performance.
Task 1 (SORT SEQUENCES) - The input to the model is a sequence of integers. The correct output
is the sorted sequence of integers. Since mapping sorted inputs to outputs can be implemented in a
recursive fashion, we evaluate our model’s ability to better model recursively structured sequence
data. Example input output pair would be 9, 1, 10, 5, 3 → 1, 3, 5, 9, 10.
Task 2 (TREE TRAVERSAL) - We construct a binary tree of maximum depth N. The goal is to
generate the postorder tree traversal given the inorder and preorder traversal of the tree. Note that
this is known to arrive at only one unique solution. The constructed trees have random sparsity where
we assign a probability p of growing the tree up to maximum depth N . Hence, the trees can be
of varying depths4. This requires inferring hierarchical structure and long-term reasoning across
sequences. We concatenate the postorder and inorder sequences, delimited by a special token. We
evaluate on n ∈ {3, 4, 5, 8, 10}. For n = {5, 8}, we ensure that each tree traversal has at least 10
tokens. For n = 10, we ensure that each path has at least 15 tokens. Example input output pair would
be 13,15,4,7,5,X, 13, 4, 15,5,7 → 7,15,13,4,5.
3To clarify, decoding tasks (e.g., in NMT) are still done in a step-by-step autoregressive fashion.
4Note that all our models solve the problem entirely when the tree is fixed and full. Hence, random trees
provide a necessary challenge.
4
Under review as a conference paper at ICLR 2021
Task 3 (LOGICAL INFERENCE) - We use the standard logical inference dataset5 proposed in
(Bowman et al., 2014). This is a classification task in which the goal is to determine the semantic
equivalence of two statements expressed with logic operators such as not, and, and or. The language
vocabulary is of six words and three logic operators. As per prior work (Shen et al., 2018), the model
is trained on sequences with 6 or fewer operations and evaluated on sequences of 6 to 12 operations.
For Task 1 and Task 2, we frame these tasks as a Seq2Seq (Sutskever et al., 2014) task and evaluate
models on exact match accuracy and perplexity (P) metrics. We use a standard encoder-decoder
architecture with attention (Bahdanau et al., 2014). We vary the encoder module with BiLSTMs,
Stacked BiLSTMs (3 layers) and Ordered Neuron LSTMs (Shen et al., 2018). For Task 3 (logical
inference), we use the common setting in other published works.
Results on Sorting and Tree Traversal Table 1 reports our results on the Sorting and Tree Traver-
sal task. All models solve the task with n = 3. However, the task gets increasingly harder with a
greater maximum possible length and largely still remains a challenge for neural models today. The
relative performance of X-RNN is on a whole better than any of the baselines, especially pertaining
to perplexity. We also found that S-BiLSTMs are always better than LSTMs on this task and Ordered
LSTMs are slightly worst than vanilla BiLSTMs. However, on sorting, ON-LSTMs are much better
than standard BiLSTMs and S-BiLSTMs.
Results on Logical Inference Table 2 reports our results on logical inference task. We compare
with mainly other published work. X-RNN is a strong and competitive model on this task, outper-
forming ON-LSTM by a wide margin (+12% on the longest number of operations). Performance of
our model also exceeds Tree-LSTM, which has access to ground truth syntax. Our model achieves
state-of-the-art performance on this dataset even when considering models with access to syntactic
information.
4.2	Pixel-wise Sequential Image Classification
We evaluate our model on its ability to model and capture long-range dependencies. More specifically,
the sequential pixel-wise image classification problem treats pixels in images as sequences. We use
the well-established pixel-wise MNIST and CIFAR-10 datasets. We use 3 layered X-RNN of 128
hidden units each.
				Results on Pixel-wise Image Classifi- cation Table 3 reports the results of
Model	~w~	MNIST	CIFAR	X-RNN against other published works.
DilatedGRU (Chang et al., 2017) IndRNN (Li et al., 2018a) r-LSTM (Trinh et al., 2018) Transformer (Trinh et al., 2018) TrellisNet (Bai et al., 2018b) TrellisNet (Our run)	- - - - 8.0M 8.0M	99.00 99.00 98.52 98.90 99.20 97.59	- - 72.20 62.20 73.42 55.83	Our method achieves state-of-the-art per- formance on the CIFAR-10 dataset, out- performing the recent Trellis Network (Bai et al., 2018b). On the other hand, results on MNIST are reasonable, out- performing a wide range of other pub- lished works. On top of that, our method has 8 times fewer parameters than Trel- lis network (Bai et al., 2018b) while achieving similar or better performance. This ascertains that X-RNN is a reason- ably competitive long-range sequence
X-RNN X-RNN]		0.9M 0.9M	99.04 99.09	73.01 73.95	
Table 3: Experimental results (accuracy) on Pixel-wise Sequential Image Classification. We also trained the recent R-Adam optimizer (Liu et al., 2019) which we found to have improved performance (results denoted with ]).				
encoder.
4.3 Semantic Parsing / Code Generation
We evaluate X-RNN on semantic parsing (Geo, Atis, Jobs) and code generation (Django), a task
mainly concerned with learning to parse and generate structured data. We run our experiments on
the publicly released source code6 of (Yin & Neubig, 2018), replacing the recurrent decoder with
our X-RNN decoder. We only replaced the recurrent decoder since early experiments showed that
53https://github.com/sleepinyourhat/vector-entailment.
6https://github.com/pcyin/tranX
5
Under review as a conference paper at ICLR 2021
varying the encoder did not yield any benefits in performance. Overall, our hyperparameter details
followed the codebase of (Yin & Neubig, 2018) quite strictly, i.e., we ran all models from their
codebase as it is.
Results on Semantic Parsing and Code Generation Table 4 reports our	
experimental results on Semantic Pars- Model	GEO	ATIS	Jobs	DJANGO
ing (GEO, ATIS, JOBS) and Code Gener- (Dong & Lapata, 2016) ation (DJANGO). We observe that TranX (Ling et al., 2016) + X-RNN outperforms all competitor ap- (Neubig, 2015) proaches, achieving state-of-the-art per- (Yin & Neubig, 2017) (Rabinovich et al., 2017) formance. On Django, we outperform (Yin & Neubi 2018) (Yin & Neubig, 2018)	-871	84.6	-	315 -	-	-	62.3 -	-	-	45.1 -	-	-	71.6 87.1	85.9	-	- 88.2	86.2	-	72.7
TranX by +1.6% and ≈ +1% on all TranX (Code reported) semantic parsing tasks. More impor- TranX (Our Run)	""88.6~~877	900	772- 87.5	87.5	90.0	76.7
tantly, the performance gain over the TranX + X-RNN base TranX method allows us to observe	88.6	88.4	90.7	78.3
the ablative benefits of X-RNN which is Table 4: Experimental results on Semantic Parsing and Code Gen-
achieved by only varying the recurrent eration.
decoder.
4.4	Neural Machine Translation
This task is a sequence transduction task which is concerned with translating a source language to
a target language. We conduct experiments on two IWSLT datasets which are collections derived
from TED talks. Specifically, we compare on the IWSLT 2014 German-English and IWSLT 2015
English-Vietnamese datasets. We compare against a suite of published results and strong baselines.
For our method, we replaced the multi-head aggregation layer in the Transformer networks (Vaswani
et al., 2017) with a parallel version adaptation of X-RNN. The base models are all linear layers.
For our experiments, we use the standard implementation and hyperparameters in Tensor2Tensor7
(Vaswani et al., 2018), using the small (S) and base (B) setting for Transformers.
Model	BLEU	For evaluation, model averaging is used (5 recent
(Luong & Manning, 2015)	23.30	
(Bahdanau et al., 2014)	26.10	checkpoints) and beam size of 8/4 and length
(Huang et al., 2017)	28.07	penalty of 0.6 is adopted for De-En and En-Vi
Transformer B	28.43	respectively. For our model, max depth is tuned
Transformer B + X-RNN-	30.81	amongst {1, 2, 3}. We also ensure to compare, in
Table 5: Experimental results on
lation on IWSLT 2015 En-Vi.
an ablative fashion, our own reported runs of the
Neural Machine Trans-base Transformer models.
Model oe	BLEU	Results on Neural Machine Translation Ta-
(Ranzato et al., 2015)	21.83	ble 6 reports results on IWSLT 2014 de-en
(Bahdanau et al., 2016)	28.53	task. Our proposed model performs very compet-
(Huang et al., 2017)	28.96	itively (37.09 BLEU), outperforming many well-
(Wang et al., 2018)	32.35	established baselines. Our results also show that
(He et al., 2018)	35.07	equipping Transformer models with X-RNN can
Transformer S (Our run)	34.68	also lead to improvements in performance. Notably
Transformer B (Our run)	36.30	there is a +0.69 BLEU improvement on Trans-
Transformer S + X-RNN	35.15	former Base and +0.42 BLEU improvement for
Transformer B + X-RNN	37.09	Transformer Small. On the other hand, our method
achieves 30.81 BLEU on the IWSLT 2015 En-Vi
Table 6: Experimental results on Neural Machine Trans-
lation on IWSLT 2014 De-En.	dataset, with +2.38 improvement in BLEU from
the standard Transformer Base model.
4.5	Polyphonic Music Modeling
We evaluate X-RNN on the polyphonic music modeling, i.e., generative modeling of musical
sequences. We use three well-established datasets, namely Nottingham, JSB Chorales and Piano
Midi (Boulanger-Lewandowski et al., 2012). The inputs to the model are 88-bit sequences, each
7https://github.com/tensorflow/tensor2tensor
6
Under review as a conference paper at ICLR 2021
corresponding to the 88 keys of the piano. The task is evaluated on the Negative Log-likelihood
(NLL). We compare with a wide range of published works (Chung et al., 2014; Bai et al., 2018a;
Song et al., 2019) on this task.
Model	Nott	JSB	Piano	Results on Music Modeling Table 7 reports
GRU (Chung et al.)	3.13	8.54	8.82	our scores on this task. X-RNN achieves state-
Song et al.	3.25	8.61	7.99	of-the-art performance on the Nottingham and
Li et al.	3.21	8.67	8.18	Piano midi datasets. Our model also achieves
Song et al.	3.16	8.30	7.55	competitive performance on the JSB dataset,
Bai et al.	3.07	8.10	-	only underperforming the state-of-the-art by 0.01
TCN (our run)	2.95	8.13	7.53	NLL. On a whole, our model also outperforms a
X-RNN	2.88	8.12	7.49	wide range of competitive models such as Gum-
bel Gate LSTMs (Li et al., 2018b). Moreover,
Table 7: Experimental Results (NLL) on Polyphonic the performance gain over standard LSTM and
Music Modeling.	GRUs are also relatively large.
4.6	Analysis and Discussion
This section reports some analysis and discussion regarding the proposed model. We hope to
understand the behaviour of the model, at the activation level of different recursive depths.
Max N	Base Model	Atis	Django
2	Linear	88.40	77.56
3	Linear	88.21	77.62
4	Linear	87.80	76.84	4.6.1 Effect of Maximum Depth and
2	LSTM	86.61	78.33	Base Unit
3	LSTM	85.93	77.39	Table 8 reports some ablation studies on the seman-
Table 8: Ablation studies on
Generation.
Semantic Parsing and Code
	Task	N	Base Unit
Tree Traversal	2	Recurrent
Sorting	2	Recurrent
Logical Inference	3	Recurrent
Pixel-wise Classification	2	Recurrent
Semantic Parsing	2	Linear
Code Generation	2	Recurrent
Machine Translation	3	Linear
Polyphonic Music	3	Linear
Table 9: Optimal Maximum Depth N and base unit for
different tasks.
tic parsing and code generation tasks. We observe
that the base unit and optimal maximum depth
used is task dependent. For Atis dataset, using
the linear transform as the base unit performs the
best. Conversely, the linear base unit performs
worse than the recurrent base unit (LSTM) on the
Django dataset. On a whole, we also observed
this across other tasks, i.e., the base unit and maxi-
mum depth of X-RNN is a critical choice for most
tasks. Table 9 reports the optimal max depth N
and best base unit for each task.
4.6.2 Analysis of Soft Dynamic Recursion
Figure 5: Depth Gate Visualization on CIFAR and MNIST.
Figure 5 illustrates the depth gate values on CIFAR and MNIST datasets. These values reflect the α
and β values in X-RNN, signifying how the parameter tree is being constructed during training. This
is reflected as L and R in the figures representing left and right gates. Firstly, we observe that our
7
Under review as a conference paper at ICLR 2021
model indeed builds data-specific parameterization of the network. This is denoted by how X-RNN
builds different8 trees for CIFAR and MNIST.
0.2-
0.0-
cβ-s-v<
0
50	100	150
Tetnperal
200	250
Figure 6: Visualisation of Dynamic Depth Recursion on Music.
Csss
---L
---R
----LL
-——Ln
hl
----RR
LLn
---LLL
---RRL
RRR
0
1∞	2∞	3∞	400	500 COO 700	∞0
Temporal
Figure 7: Visualisation of Dynamic Depth Recursion on MNIST.
Secondly, we analyze the dynamic recursion depth
with respect to time steps. The key observation
that all datasets have very diverse construction of
recursive parameters. The recursive gates fluctu-
ate aggressively on CIFAR while remaining more
stable on Music modeling. Moreover, we found
that the recursive gates remain totally constant on
MNIST. This demonstrates that our model has the
ability to adjust the dynamic construction adap-
tively and can revert to static recursion over time
if necessary. We find that compelling.
Figure 8: Visualisation of Dynamic Depth Recursion on CIFAR.
The adaptive recursive depth is made more intriguing by observing how the recursive parameterization
alters on CIFAR and Music datasets. From Figure 6 we observe that the structure of the network
changes in a rhythmic fashion, in line with our intuition of musical data. When dealing with
pixel information, the tree structure changes adaptively according to the more complex information
processed by the network.
5 Conclusion
We proposed X-RNN, a new sequence model characterized by recursive parameterization of gating
functions. We explored the interesting idea of recurrently controlling recurrent networks by recurrent
networks controlled by more recurrent networks and find that the proposed method achieves very
promising and competitive results on a spectrum of benchmarks across multiple modalities (e.g.,
language, logic, music). Our proposed X-RNN outperforms the recent state-of-the-art Ordered
Neurons on Logical Inference task along with other tree-structured analysis tasks. Additionally, we
propose a parallel variation of X-RNN, which allows simple drop-in enhancement to state-of-the-art
Transformers while retaining efficiency. We study and visualize our network as it learns a dynamic
recursive parameterization, shedding light on the expressiveness and flexibility to learn dynamic
parameter structures depending on the data.
References
David Alvarez-Melis and Tommi S Jaakkola. Tree-structured decoding with doubly-recurrent neural
networks. 2016.
8Though not depicted, we also found that the probability of each node expanding to children has low variance
across batches in the same dataset.
8
Under review as a conference paper at ICLR 2021
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. arXiv preprint
arXiv:1607.07086, 2016.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and
recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018a.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. arXiv
preprint arXiv:1810.06682, 2018b.
Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Modeling temporal dependen-
cies in high-dimensional sequences: Application to polyphonic music generation and transcription.
arXiv preprint arXiv:1206.6392, 2012.
Samuel R Bowman, Christopher Potts, and Christopher D Manning. Recursive neural networks can
learn logical semantics. arXiv preprint arXiv:1406.1827, 2014.
Samuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and
Christopher Potts. A fast unified model for parsing and sentence understanding. arXiv preprint
arXiv:1603.06021, 2016.
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural
networks. arXiv preprint arXiv:1611.01576, 2016.
Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael
Witbrock, Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks.
In Advances in Neural Information Processing Systems, pp. 77-87, 2017.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Jihun Choi, Kang Min Yoo, and Sang-goo Lee. Learning to compose task-specific tree structures. In
Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks.
arXiv preprint arXiv:1609.01704, 2016.
Li Dong and Mirella Lapata. Language to logical form with neural attention. arXiv preprint
arXiv:1601.01280, 2016.
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. arXiv preprint arXiv:1602.07776, 2016.
Salah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependencies.
In Advances in neural information processing systems, pp. 493-499, 1996.
Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint
arXiv:1603.08983, 2016.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.
Danijar Hafner, Alexander Irpan, James Davidson, and Nicolas Heess. Learning hierarchical informa-
tion flow with recurrent neural modules. In Advances in Neural Information Processing Systems,
pp. 6724-6733, 2017.
9
Under review as a conference paper at ICLR 2021
Serhii Havrylov, Germdn Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax
and semantics. arXiv preprint arXiv:1902.09393, 2019.
Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-wise
coordination between encoder and decoder for neural machine translation. In Advances in Neural
Information Processing Systems, pp. 7944-7954, 2018.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, and Li Deng. Towards neural phrase-
based machine translation. arXiv preprint arXiv:1706.05565, 2017.
Athul Paul Jacob, Zhouhan Lin, Alessandro Sordoni, and Yoshua Bengio. Learning hierarchical
structures on-the-fly with a recurrent-recursive model for sequences. In Proceedings of The Third
Workshop on Representation Learning for NLP, pp. 154-158, 2018.
Yoon Kim, Chris Dyer, and Alexander M Rush. Compound probabilistic context-free grammars for
grammar induction. arXiv preprint arXiv:1906.10225, 2019.
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. arXiv
preprint arXiv:1402.3511, 2014.
Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly
parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, pp. 4470-4481, 2018.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. Independently recurrent neural network
(indrnn): Building a longer and deeper rnn. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5457-5466, 2018a.
Zhuohan Li, Di He, Fei Tian, Wei Chen, Tao Qin, Liwei Wang, and Tie-Yan Liu. Towards binary-
valued gates for robust lstm training. arXiv preprint arXiv:1806.02988, 2018b.
Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomds Kocisky, Andrew Senior, Fumin
Wang, and Phil Blunsom. Latent predictor networks for code generation. arXiv preprint
arXiv:1603.06744, 2016.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. A recursive recurrent neural network for statistical ma-
chine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1491-1500, 2014.
Minh-Thang Luong and Christopher D Manning. Stanford neural machine translation systems for
spoken language domains. 2015.
Graham Neubig. lamtram: A toolkit for language and translation modeling using neural networks,
2015.
Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code generation
and semantic parsing. arXiv preprint arXiv:1704.07535, 2017.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training
with recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.
Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber,
Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural
networks. In Advances in Neural Information Processing Systems, pp. 7299-7310, 2018.
Jurgen Schmidhuber. Learning complex, extended sequences using the principle of history compres-
sion. Neural Computation, 4(2):234-242, 1992.
10
Under review as a conference paper at ICLR 2021
Yikang Shen, Zhouhan Lin, Chin-Wei Huang, and Aaron Courville. Neural language modeling by
jointly learning syntax and lexicon. arXiv preprint arXiv:1711.02013, 2017.
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating
tree structures into recurrent neural networks. arXiv preprint arXiv:1810.09536, 2018.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and
Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.
In Proceedings of the 2013 conference on empirical methods in natural language processing, pp.
1631-1642, 2013.
Kyungwoo Song, JoonHo Jang, Il-Chul Moon, et al. Bivariate beta lstm. arXiv preprint
arXiv:1905.10521, 2019.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
In Advances in neural information processing systems, pp. 3104-3112, 2014.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations
from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. Recurrently controlled recurrent networks. In Advances
in Neural Information Processing Systems, pp. 4731-4743, 2018.
Ke Tran, Arianna Bisazza, and Christof Monz. Recurrent memory networks for language modeling.
arXiv preprint arXiv:1601.01272, 2016.
Ke Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling
hierarchical structure. arXiv preprint arXiv:1803.03585, 2018.
Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le. Learning longer-term dependen-
cies in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws,
Llion Jones, Eukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and
Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018.
URL http://arxiv.org/abs/1803.07416.
Yau-Shian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating tree structures
into self-attention. arXiv preprint arXiv:1909.06639, 2019.
Yijun Wang, Yingce Xia, Li Zhao, Jiang Bian, Tao Qin, Guiquan Liu, and Tie-Yan Liu. Dual transfer
learning for neural machine translation with marginal distribution regularization. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation.
arXiv preprint arXiv:1704.01696, 2017.
Pengcheng Yin and Graham Neubig. Tranx: A transition-based neural abstract syntax parser for
semantic parsing and code generation. arXiv preprint arXiv:1810.02720, 2018.
Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to
compose words into sentences with reinforcement learning. arXiv preprint arXiv:1611.09100,
2016.
11