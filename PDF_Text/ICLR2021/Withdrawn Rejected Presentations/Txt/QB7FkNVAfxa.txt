Under review as a conference paper at ICLR 2021
On the Explicit Role of Initialization on the
Convergence and Generalization Properties
of Overparametrized Linear Networks
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks trained via gradient descent with random initialization and with-
out any regularization enjoy good generalization performance in practice despite
being highly overparametrized. A promising direction to explain this phenomenon
is the Neural Tangent Kernel (NTK), which characterizes the implicit regular-
ization effect of gradient flow/descent on infinitely wide neural networks with
random initialization. However, a non-asymptotic analysis that connects gener-
alization performance, initialization, and optimization for finite width networks
remains elusive. In this paper, we present a novel analysis of overparametrized
single-hidden layer linear networks, which formally connects initialization, opti-
mization, and overparametrization with generalization performance. We exploit
the fact that gradient flow preserves a certain matrix that characterizes the imbal-
ance of the network weights, to show that the squared loss converges exponen-
tially at a rate that depends on the level of imbalance of the initialization. Such
guarantees on the convergence rate allow us to show that large hidden layer width,
together with (properly scaled) random initialization, implicitly constrains the dy-
namics of the network parameters to be close to a low-dimensional manifold. In
turn, minimizing the loss over this manifold leads to solutions with good general-
ization, which correspond to the min-norm solution in the linear case. Finally, we
derive a novel O(h-1/2) upper-bound on the operator norm distance between the
trained network and the min-norm solution, where h is the hidden layer width.
1	Introduction
Neural networks have shown excellent empirical performance in many application domains such as
vision (Krizhevsky et al., 2012; Rawat & Wang, 2017), speech (Hinton et al., 2012; Graves et al.,
2013) and video games (Silver et al., 2016; Vinyals et al., 2017). Among the many unexplained mys-
teries behind this success is the fact that gradient descent with random initialization and without ex-
plicit regularization enjoys good generalization performance despite being highly overparametrized.
A promising attempt to explain this phenomena is the Neural Tangent Kernel (NTK) (Jacot et al.,
2018), which characterizes the implicit regularization effect of gradient flow/descent on infinitely
wide neural networks with random initialization. Precisely, under this infinite width assumption,
a proper initialization, together with gradient flow training, can be understood as a kernel gradient
flow (NTK flow) of a functional that is constrained on a manifold that guarantees good generaliza-
tion performance. The analysis further admits extensions to the finite width (Arora et al., 2019b;
Buchanan et al., 2020). The core of the argument, illustrated in Figure 1, amounts to showing that
(properly scaled) random initialization of networks with sufficiently large width, leads to trajectories
that are, in some sense initialized close to the aforementioned manifold. Thus, approximately good
initialization, together with acceleration, ensures that the training stays close to the NTK Flow.
Such analysis, however, leads to bounds on the network width that significantly exceed practical val-
ues (Buchanan et al., 2020), and seems to suggest the need of acceleration to achieve generalization.
This motivates the following questions:
• Is the kernel regime, which requires impractical bounds on the network width, necessary to
achieve good generalization?
1
Under review as a conference paper at ICLR 2021
Our Analysis for
Single-hidden-layer
Linear Networks
Contribution 1, Section 3
Contribution 3, Section 4.2
Large Finite Width
+
Random Initialization
Approximate
Imbalanced
Initialization
Approximate
Orthogonal
Initialization
Convergence/
Acceleration
Constraint
Manifold
Contribution 2, Section 4.1
Figure 1: Comparing our analysis to asymptotic/non-asymptotic NTK analysis.
• Does generalization depends explicitly on acceleration? Or is acceleration required only due to
the choosing an initialization outside the good generalization manifold?
For the simplified, yet certainly non-trivial, single-hidden layer linear network setting, this paper
finds an answer to these questions.
Contributions. We present a novel analysis of the gradient flow dynamics of overparametrized
single-hidden layer linear networks, which provides disentangled conditions on initialization that
lead to acceleration and generalization. Specifically, we show that imbalanced initialization ensures
acceleration, while orthogonal initialization ensures that trajectories remain close to the general-
ization manifold. Interestingly, properly scaled random initialization of moderately wide networks
is sufficient to ensure that initialization is approximately imbalanced and orthogonal, yet it is not
necessary for either. More specifically, as illustrated in Figure 1, this paper makes the following
contributions:
1.	We show first that gradient flow on the squared-l2 loss preserves a certain matrix-valued quantity,
akin to constants of motion in mechanics or conservation laws of physics, that measures the im-
balance of the network weights. Notably, some level of imbalance, measured by certain singular
value of the imbalance matrix and defined at initialization, is sufficient to guarantee the exponen-
tial rate of convergence of the loss. Our analysis is non-probabilistic and valid under very mild
assumptions, satisfied by moderately wide single-hidden-layer linear networks.
2.	We characterize the existence of a low-dimensional manifold defined by a specific orthogonality
condition on the parameters, which is invariant under the gradient flow. All trajectories within
this manifold lead to a unique (w.r.t the end-to-end function) minimizer with good generalization
performance, which corresponds to the min-norm solution. As a result, initializing the network
within this manifold guarantees good generalization.
3.	We further show that by randomly initializing the network weights using N(0, 1/h) (where h is
the hidden layer width), an initialization setting related to the kernel regime (see Appendix E),
one can approximately satisfy both our sufficient imbalance and orthogonality conditions with
high probability. Notably, the inaccurate initialization relative to the good generalization mani-
fold requires acceleration to control the generalization error. In the context of NTK, our result
further provide for linear networks a novel O h-1/2 upper-bound on the operator norm distance
between the trained network and the min-norm solution. To the best of our knowledge, this is the
2
Under review as a conference paper at ICLR 2021
first non-asymptotic bound regarding the generalization property of wide linear networks under
random initialization in the global sense.
Notation. For a matrix A, we let AT denote its transpose, tr(A) denote its trace, λi(A) and σi(A)
denote its i-th eigenvalue and i-th singular value, respectively, in decreasing order (when adequate).
We let [A]ij, [A]i,:, and [A]:,j denote the (i, j)-th element, the i-th row and the j-th column of A,
respectively. We also let kAk2 and kAkF denote the spectral norm and the Frobenius norm of A,
respectively. For a scalar-valued or matrix-valued function of time, F(t),we let F = F(t)= 第F(t)
denote its time derivative. Additionally, we let In denote the identity matrix of order n and N(μ, σ2)
denote the normal distribution with mean μ and variance σ2.
2	Related Work
Wide neural networks. There is a rich line of research that studies the convergence (Du et al.,
2019b;a; Du & Hu, 2019; Allen-Zhu et al., 2019b) and generalization (Allen-Zhu et al., 2019a;
Arora et al., 2019a;b; Li & Liang, 2018; Cao & Gu, 2019; Buchanan et al., 2020) of wide neural
networks with random initialization. The behavior of such networks in their infinite width limit can
be characterized by the Neural Tangent Kernel (NTK) (Jacot et al., 2018). With the concept of NTK,
heuristically, training wide neural networks can be approximately viewed as kernel regression under
gradient flow/descent (Arora et al., 2019b), hence the convergence and generalization can be under-
stood by studying the non-asymptotic results regarding the equivalence of finite width networks to
their infinite limit (Du et al., 2019b;a; Du & Hu, 2019; Allen-Zhu et al., 2019b; Arora et al., 2019a;b;
Buchanan et al., 2020). More generally, such non-asymptotic results are related to the “lazy train-
ing” (Chizat et al., 2019; Du et al., 2019a; Allen-Zhu et al., 2019b), where the network weights do
not deviate too much from its initialization during training. Our results for wide linear networks
presented in Section 4.2 do not follow the NTK analysis, but provide an alternative, presumably
more general view on the effect of random initialization when the hidden layer is sufficiently wide.
Convergence of linear networks. Convergence in overparametrized linear networks has been stud-
ied for both gradient flow (Saxe et al., 2014) and gradient descent (Bartlett et al., 2018; Arora et al.,
2018a;b). In the kernel regime, Du & Hu (2019) applied the analysis of convergence of wide neural
networks (Du et al., 2019b) to deep linear networks. Aside from the large hidden layer width and
random initialization assumptions, Saxe et al. (2014) analyzed the trajectory of network parame-
ters under spectral initialization, while Bartlett et al. (2018) studied the case of identity initializa-
tion. Although the fact that the imbalance is conservative under gradient flow has been exploited
in Arora et al. (2018b;a), they consider the case of balanced initialization only to simplify the learn-
ing dynamics, and additional conditions are required for convergence. Most works mentioned above
considered specific, often data-dependent, types of initialization that makes the learning dynamics
tractable. Our result, based on an imbalance measure, is data-agnostic and satisfied under a wide
range of random initialization schemes, see lemmas 1 and F.2.
Min-norm solution in high-dimension linear regression. For high-dimensional under-determined
linear regression, the asymptotic generalization error of min-norm solution has been studied
in Hastie et al. (2019). In Bartlett et al. (2020); Mei & Montanari (2019), the min-norm solution
was proved to have near-optimal generalization performance under mild assumptions on the data
model. For our purpose, we study the generalization of a trained linear network as its distance to
the min-norm solution. In this way, we refer to solutions with good generalization performance, as
those with small distance to the min-norm solution.
3	Convergence Rate of Gradient Flow for single-hidden-layer
Linear Networks
In this section we study the convergence of gradient flow on squared l2-loss for single-hidden-layer
linear networks. Given training data of n samples {x(i) , y(i) }in=1 with x(i) ∈ RD, y(i) ∈ Rm, we
aim to solve the linear regression problem
1n
min L= X X(y(i) - ΘTx⑴)2 ,	(1)
Θ∈RD×m	2
i=1
3
Under review as a conference paper at ICLR 2021
by training a single-hidden-layer linear network y = f(x; V, U) = V UTx, V = Rm×h, U ∈ RD×h
with gradient flow, or equivalently, gradient descent with “infinitesimal step size”, here h is the
hidden layer width. We consider an overparametrized model such that h ≥ min{m, D}.
We rewrite the loss function with respect to our parameters V, U as
1n	1
l(v, u ) = 2 X(y(i) - VUTx ⑺)2 = 1 kY - XUV T kF,
i=1
(2)
where Y = [y(1),…，y(n)]T, X = [x(1),…，x(n)]T are concatenations of the training data in
rows. Assuming that the input data X has full rank, we consider the under-determined case D > n
for our regression problem, i.e., there are infinitely many solutions Θ* to achieve zero loss of (1).
With minor a reformulation, our convergence result works for the case where the input data X is
rank deficient. We refer the reader to Appendix B.
We will show that under certain conditions, the trajectory of the loss function L(t) = L(V (t), U (t))
under gradient flow of (2), i.e.,
∂∂
V(t) = -dVL(V(t), U(t)), U(t)=-而L(V(t), U(t)),
(3)
converges to 0 exponentially, and that proper initialization of U (0), V(0) controls the convergence
rate via a time-invariant matrix-valued term, the imbalance of the network.
3.1	Reparametrization of Gradient Flow
Given D > n = rank(X), the singular value decomposition ofX is given by
x = w [∑X/2
ΦΦ12TT
,W∈ Rn×n, Φ1 ∈ RD×n, Φ2 ∈ RD×(D-n),
(4)
0
with WWT = WTW = Φ1T Φ1 = In ,Φ2TΦ2 = ID-n, Φ1T Φ2 = 0, and Φ1Φ1T + Φ2Φ2T = ID.
Notice that
U = IDU = (Φ1Φ1T + Φ2Φ2T)U = Φ1Φ1TU + Φ2Φ2T U,
hence we can reparametrize U as (U1, U2) using the bijection g : Rn×h × R(D-n)×h → RD×h , U =
g(U1, U2) = Φ1U1 + Φ2U2, with inverse (U1,U2) = g-1(U) = (Φ1TU,Φ2TU).
We write the gradient flow in (3) explicitly as
V(t) = (Y - XU(t) VT(t))T XU(t) = ET(t)∑X〃ΦTU(t),	(5a)
U(t) = XT (Y - XU(t)VT(t)) V(t) = Φι∑X^E(t)V(t),	(5b)
based on the SVD of data X in (4), where
E(t) = E(V(t), Uι(t)) = WTY - ∑Xr2Uι(t)VT(t),	(6)
is defined tobe the error. Then from (5a)(5b) we obtain the dynamics in parameter space (V, U1, U2)
as
V(t) = ET(t»X/2Ui(t), Uι(t) = ∑X^E(t)V(t), U2(t)=0.	(7)
Notice that since W is orthogonal, we have
L(t) = 1 kY - XU(t)VT(t)kF = 2kWE(t)kF = 2kE(t)kF .	(8)
Therefore it suffices to analyze the convergence rate of the error E(t) under the dynamics of
V (t), U1(t) in (7). As we mentioned in Section 1, the exponential convergence of E(t), or equiv-
alently the loss function L(t) is crucial for our analysis for generalization, in the sense that expo-
nential convergence ensures that the parameters do not deviate much away from the manifold of
our interest, which we will discuss in Section 4, so that good properties from the initialization are
approximately preserved during training.
4
Under review as a conference paper at ICLR 2021
3.2	Imbalance and Convergence Rate of the Error
We define the imbalance of the single-hidden-layer linear network under input data X as
Imbalance : U1T U1 - V T V .	(9)
The imbalance term is time-invariant under gradient flow, as stated in the following claim
Claim. Under continuous dynamics (7), we have d [UT(t)Uι(t) — VT(t)V(t)] ≡ 0.
Proof. Under (7), we compute the time derivative of U1T (t)U1(t) and VT(t)V(t) as
ddtUT (t)Uι (t) = U T(t)Uι(t) + UT(t)U ι(t) = V T (t)E T (t)∑y2U1(t) + UT (t)∑Y2E (t)V(t),
ddtVT(t)V(t) = VT(t)V(t) + VT(t)V(t) = VT(t)ET(t)∑y2U1(t) + UT(t)∑Y2E(t)V(t).
The right-hand side of two equations is identical, hence d [UT(t)U1 (t) - VT(t)V(t)] ≡ 0.	□
The imbalance is a h × h matrix with rank at most m + n, the rank of imbalance characterizes how
much the row spaces of U1 and V are misaligned. We show that a rank-(m + n - 1) imbalance is
sufficient for exponential convergence of the error E(t), or equivalently, the loss function.
Now we present our result regarding convergence of the error. (see Appendix D for the proof).
Theorem 1 (Convergence of linear networks with sufficient rank of imbalance). Suppose h ≥ m +
n - 1. Let V (t), U1(t), t > 0 be the trajectory of continuous dynamics (7) starting from some
V(0), U1(0). If
σn+m-1 (UT (0)Ul(0) - V T (0)V(0)) = c> 0 ,	(10)
then for E(t) defined in (6), we have
kE(t)k2F ≤ exp (-2σn(Σx)ct) kE(0)k2F, ∀t > 0 .	(11)
Additionally, V (t), U1(t), t > 0 converges to some equilibrium point (V (∞), U1(∞)) such that
E(V(∞), U1(∞)) = 0.
The fact that the imbalance is preserved under gradient flow has been exploited in Arora et al.
(2018a;b), where imbalance is assumed to be zero (or small), such that the learning dynamics can
be expressed in closed form with respect to the end-to-end matrix. This analysis, requires, however,
additional assumptions on the initialization of the end-to-end matrix for acceleration. Similarly,
though in a more general setting, Du et al. (2018) showed that the imbalance is preserved, and
proves convergence under a small imbalance assumption. Acceleration (exponential rate), however,
is not guaranteed. Exploiting imbalance for guaranteeing acceleration was first presented in Saxe
et al. (2014), under a spectral initialization assumption. In contrast, Theorem 1 shows acceleration
without the spectral initialization condition. Rather, we show that under very mild conditions on the
alignment between the initialization and the data, acceleration is achieved.
Such good choice of initialization, at first glance, seems to largely depend on the data given the
definition of the imbalance. However, we show in the next section that for sufficiently wide networks
with random initialization, the imbalance has rank at least n + m with high probability, for any data
matrix X , hence exponential convergence is almost guaranteed when training such networks. Later
we will illustrate how such convergence also affects the generalization of the trained network.
The dependence on σn(Σx) has been also appeared in Du & Hu (2019), where a linear convergence
rate of gradient descent was shown for a multi-layer linear networks. Their proof followed the same
high-level procedure as in showing convergence for networks with nonlinear activation (Du et al.,
2019b;a), which relied on showing that the Gram matrix is close to its initialization during training.
For our result, although it is provided for single-hidden-layer linear networks, we essentially lower
bound the smallest eigenvalue of the Gram matrix at any time t by a fixed constant that only depends
on the initialization.
We end the section by noting that our result is not restricted to the case that X is full rank. In
Appendix B, we show that similar result hold for the case that X is rank deficient, with mi-
nor reformulations. In that case, we only require h ≥ d + m - 1, and the singular value
σd+m-1(U1T(0)U1(0) - V T (0)V (0)) replaces what in shown in (10), where d is the rank of X.
In addition, we present and discuss the numerical simulation regarding Theorem 1 in Appendix A.
5
Under review as a conference paper at ICLR 2021
4	Generalization of Single-hidden-layer Linear Network
In this section, we study the generalization properties of trained single-hidden-layer linear networks
under gradient flow. Assuming that D > n = rank(X), the regression problem (1) has infinitely
many solutions Θ* that achieve zero loss. Among all these solutions, one that is of particular interest
in high-dimensional linear regression is the minimum norm solution (min-norm solution)
Θ = arg min{kΘk2 : Y - XΘ = 0} = XT(XXT)-1Y,	(12)
Θ∈RD×m
which has near-optimal generalization error for suitable data models, as shown in (Bartlett et al.,
2020; Mei & Montanari, 2019). Here, we study conditions under which our trained network is equal
or close to the min-norm solution by showing how the initialization explicitly controls the trajectory
of the training parameters to be exactly (or approximately) confined within some low-dimensional
manifold. In turn, minimizing the loss over this manifold leads to the min-norm solution.
Moreover, our analysis on constrained learning applies to wide single-hidden-layer linear networks
with random initialization, whose infinite width limit is equivalent to the kernel predictor with linear
kernel K(x, x0) = xTx0, as suggested by Jacot et al. (2018). One can easily check that such a kernel
predictor is the min-norm solution Θ. In addition, We show that the operator norm distance between
trained finite width single-hidden-layer linear network and the min-norm solution is upper bounded
by a O(h-1/2) term with high probability over random initialization.
4.1	Decomposition of Trained Network
To begin with, notice that the linear operator U V T ∈ RD×m associated with the single-hidden-layer
linear network can be decomposed according to the data matrix X as
UVT = (Φ1Φ1T + Φ2Φ2T)UVT = Φ1U1VT + Φ2U2V T,	(13)
where Φ1, Φ2, U1, U2 are previously defined in Section 3. Here [UVT]:,j, i.e., the j-th column
of U V T, is the linear predictor for the j-th output yj , and is decomposed into two components
within complementary subspaces span(Φ1) and span(Φ2). Moreover [U1V T]:,j is the coordinate
of [UV T]:,j w.r.t. the orthonormal basis as the columns of Φ1, and similarly [U2V T]:,j is the coor-
dinate w.r.t. basis Φ2. Clearly, under gradient flow (3), the trajectory (U (t)V (t)T, t > 0) is fully
determined by the trajectory (U1(t)VT(t), U2(t)VT(t), t > 0), which is governed by the dynamics
(7).
Convergence of Training Parameters. We have derived useful results regarding U1 (t)V T (t) for
t > 0 in Section 3. By Theorem 1, if the rank of the imbalance matrix is large enough, U1(t)V T(t)
converges to some U1 (∞)V T(∞) and the stationary point satisfies WTY - ∑X%(∞)v T(∞)=
0, which implies Uι(∞)Vt(∞) = ∑-1/2 WTY. Then it is easy to check that
ΦιUι(∞)VT(∞) = Φι∑-"WTY = XT(XXT)-1Y = Θ .	(14)
In other words, the projected trajectory (in columns) of U(t)VT (t) onto span(Φ1) converges exactly
to the min-norm solution.
For U2(t)Vt(t), notice that U2(t) = 0 in dynamics (7), hence U2(t) = U2(0), ∀t > 0. Then under
sufficient rank of imbalance, U(t)VT(t) converges to some U(∞)VT(∞) and
U(∞)VT(∞) = Φ1U1(∞)VT(∞) + Φ2U2(0)VT(∞) =Θ + Φ2U2(0)VT(∞).
Therefore U2 (0)V T (∞) quantifies how much the trained network U(∞)VT(∞) is deviated from
the min-norm solution Θ, and since ΦTΦ2 = I0-n, we have
kU(∞)VT(∞) - Θ∣∣2 = ∣∣Φ2U2(0)VT(∞)k2 = kU2(0)VT(∞)k2.	(15)
Constrained Training via Initialization. Based on our analysis above, initializing U2 (0) such
that U2(0)VT(∞) = 0 in the limit, guarantees convergence to the min-norm solution via (15).
However, this is not easily achievable, as one needs to know a priori V (∞). Instead, we can show
that by choosing a proper initialization, one can constrain the trajectory of the matrix U(t)VT(t)
6
Under review as a conference paper at ICLR 2021
V(t)U2T(0)
U1 (t)U2T (0) .
(17)
to lie identically in the set Φ2T U (t)V T (t) ≡ 0 for all t ≥ 0, which is equivalent to saying that the
columns of U (t)V T (t) lie in span(Φ1). Indeed, using the fact that for all t ≥ 0, U2(t) = U2(0) and
Φ2T U (t)V T (t) ≡ 0 we obtain
0 = Φ2T U (t)V T (t) = Φ2T U (∞)V T (∞) = U2(∞)V T (∞) = U2(0)V T (∞) .	(16)
Therefore, using (15), it follows that U(∞) VT(∞) = ΘΘ, as desired.
Here, the constraint that all columns of U(t)VT(t) lie in span(Φ1) is equivalent to the constraint
that (V, U) is within some low-dimensional manifold in the parameter space. More importantly,
such constraint on UVT is relevant to generalization: When a column of UVT is in span (Φ2),
predictions are made based on features in span(Φ2). However, those features are not present in the
data X that spans Φ1, thus, intuitively, hurting the generalization performance.
To enforce the constraint Φ2TU(t)VT(t) ≡ 0, consider the dynamics of U2(0)VT(t), or equivalently
V (t)U2T (0). From (7) we have
d ΓV(t)UT(0)] _ Γ 0	ET(t)∑X/
dt [Uι(t)UT(0)] = [∑Y2E(t)	0
The most straightforward way to enforce V (t)U2T (0) = 0, ∀t > 0 is to properly initialize
V (0), U(0) such that V (0)U2T (0) = 0 and U1 (0)U2T (0) = 0. To have such a proper initializa-
tion, one can
•	Initialize columns of U(0) in span(Φ1 ), which leads to U2(0) = 0;
•	Initialize U(0) and V(0) to enforce the orthogonality condition on the rows, i.e. V(0)UT(0) = 0
and U(0)UT(0) = ID .
Such initialization guarantees that gradient flow is constrained within some low-dimensional mani-
fold in the parameter space, such that any global minimizer of the loss in that manifold corresponds
to the min-norm solution. Therefore, whenever the network parameters in this manifold converge,
and E(∞) = 0, then the solution must be the minimum-norm one. While in practice we can make
the initialization exactly as above, such choice is data-dependent and requires the SVD of the data
matrix X . Moreover, we note that while the zero initialization works for the standard linear regres-
sion case, such initialization V(0) = 0, U(0) = 0 is bad in the overparametrized case because it is a
saddle point of the gradient flow, even though it satisfies the orthogonal condition V (0)U2T (0) = 0
and U1 (0)U2T (0) = 0.
In the next section, we show that under random initialization and sufficiently large hidden layer
width h, these conditions on initialization are approximately satisfied, i.e., with high probability
the rank of imbalance is m + n and ∣∣V(0)UT(0)||f, ∣∣U1(0)UT(0)IIF 〜O(h-1/2). So that the
trajectory U(t)VT(t), t > 0 will be approximately constrained in the subspace as we mentioned
above.
4.2 Wide single-hidden-layer Linear Network
We will now discuss the generalization of wide single-hidden-layer linear network with random
initialization. In particular, we will show how the previously mentioned conditions for convergence
and good generalization, i.e., high imbalance and orthogonality, are approximately satisfied with
high probability under the following initialization
[U(0)]ij 〜N C，1 ≤ i ≤ D，1 ≤ j ≤ h,
[V(0)]ij 〜N(0,1)，1 ≤ i ≤ m，1 ≤ j ≤ h,
where all the entries are independent. Our analysis indeed highlights the need, within this regime,
of exponential convergence to ensure good generalization.
Remark 1. Our analysis can be extended to the more general case where all entries of U (0), V(0)
are sampled from N (0, h-2α) with 1 < ɑ ≤ 1. For the simplicity of the presentation, we consider
the particular case α = 1 in this section. Please see Appendix Ffor the more general result.
7
Under review as a conference paper at ICLR 2021
Previous works Jacot et al. (2018) have suggested in the limit h → ∞, the trained network is
equivalent to the kernel predictor with NTK. For linear networks, the NTK is the linear kernel
K(x, x0) = xTx0 whose corresponding kernel predictor is the min-norm solution. Therefore, as
h → ∞, we should expect the trained network to converge to the min-norm solution, given proper
scaling of the network (Arora et al., 2019b). Combining what we have discussed regarding the
convergence and generalization of linear networks with basic random matrix theory, we are able to
derive the non-asymptotic O(h-1/2) bound on the operator norm distance between a trained h-width
network under random initialization and the min-norm solution.
Remark 2. We note that, both our parametrization and initialization, are at first sight different that
the one used in previous works (Jacot et al., 2018; Du & Hu, 2019; Arora et al., 2019b) on NTK
analysis. However, one can relate our model assumptions to the NTK ones by a rescaling of the
parameters and time. In the context of this comparison, we see that our setting achieves the same
as it achieves the same limiting end-to-end function, but with a rate of convergence h times faster
(due to the timescale rescaling). Further, our result does not rely on studying the tangent kernel of
the network, hence there is significant difference between our approach to the NTK one.
Recall in the last section, one can obtain exactly min-norm solution via proper initialization of the
single-hidden-layer network. In particular, it requires 1) convergence of the error E(t) to zero; and
2) the orthogonality conditions V (0)U2T (0) = 0 and U1(0)U2T (0) = 0. Under random initialization
and sufficiently large hidden layer width h, these two conditions are approximately satisfied. Us-
ing basic random matrix theory, one can show that with high probability, the rank of imbalance is
m + n, which leads to (exponential) convergence of E(t), and k V(0)UT(0)∣∣f, ∣∣Uι(0)UT(O)IIF 〜
O(h-1/2)
, as stated in the following lemma (See Appendix F for the proof)
Lemma 1. Given data matrix X. ∀δ ∈ (0,1), ∀h > ho = Poly (m, D, δ), with probability at least
1 一 δ over random initializations with [U(0)]ij, [V(0)]ij 〜N(O, h-1), we have all the following
hold.
1.	(Sufficient rank of imbalance)
σn+m (UT(O)Ui(O) - VT(O)V(O)) > 1 一 2
√m + D + 1 log 2
(18)
2.	(Approximate orthogonality condition)
V (O)U2T (O)
U1(O)U2T(O)	F
≤ 2√m+^ EDW1 log 2 ,
h
∣∣Ui(O)VT(O)IlF ≤ 2√m
√m + D + 1 log 2
(19)
(20)
Clearly, k V (O)UT (O)∣∣ 〜 O(h-1/2) alone is insufficient to have the final bound on the trained
network. However, from (17) we show that as long as the error E(t) converges to 0 exponentially,
which is guaranteed by sufficient rank of imbalance with high probability, the final deviation from
min-norm solution IV (∞)U2T (O)IF can not exceed C(IV(O)U2T(O)IF + IU1(O)U2T(O)IF) for
some constant C that depends on the data and the convergence rate of E(t), leading to the desired
bound. The formal statement is summarized in the following theorem.
Theorem 2 (Generalization of wide single-hidden-layer linear network). Let (V (t), U (t), t > O)
be a trajectory of continuous dynamics (7). Then, ∃C > O, such that ∀δ ∈ (O, 1), ∀h > h0 =
Poly (m, D, 1, σ(∑j), with probability 1一δ over random initializations with [U (O)]ij, [V (O)]ij 〜
N(O, h-1), we have
∣U(∞)VT(∞) - Θ∣2 ≤ 2C√m+^√m + d√+ 2 logɪ ,	(21)
h
where C depends on the data X, Y .
The proof is shown in Appendix F. This is, to our best knowledge, the first non-asymptotic bound in
the global (operator) sense of gradient flow trained wide neural networks under random initialization.
8
Under review as a conference paper at ICLR 2021
Although we understand that this can not be directly compared to previous works (Arora et al.,
2019b; Buchanan et al., 2020) that show non-asymptotic results connecting a trained network to the
kernel predictor by the NTK, using more general network structure and activation functions than
that of the linear network, we believe this theorem is a clear illustration of how overparametrization,
in particular the hidden layer width, together with random initialization affects the convergence and
generalization, beyond the kernel regime.
To be specific, regarding the non-asymptotic analysis for wide neural networks, the concept of con-
strained learning presented in this section and used to prove Theorem 2 is more general than previous
works (Arora et al., 2019b), where one requires sufficiently large hidden layer width such that the
trajectory of the network in the function space approximately matches the trajectory in the infinite
width limit. Loosely speaking, such a large width h enforces the trajectory to be approximately con-
fined within a one-dimension manifold, parametrized only by t, and independent of the initialization.
We have shown here, however, that even for relative smaller width h, there is a larger dimensional
manifold that provides good generalization performance. This shows that, while the kernel regime
maybe sufficient, it is certainly, at least for the linear single-hidden layer network, not necessary to
guarantee good generalization.
To verify Theorem 2, we present the numerical simulation regarding the implicit regularization
of gradient descent on wide linear network in Appendix A. The simulation result shows that
∣∣U(∞)VT(∞) - ΘΘ∣∣2 approximately has the order O(h-1) as h grows, suggesting that our non-
asymptotic bound is tight in the order w.r.t. h. We refer interested readers to the appendix for the
details of the simulation settings.
5 Conclusion
In this paper, we study the explicit role of initialization on controlling the convergence and gener-
alization of single-hidden-layer linear networks trained under gradient flow. First of all, initializ-
ing the imbalance to have sufficient rank leads to the exponential convergence of the loss. Then
proper initialization enforces the trajectory of network parameters to be exactly (or approximately)
constrained in a low-dimensional manifold, over which minimizing the loss yields the min-norm
solution. Combining those results, we obtain O(h-1/2) non-asymptotic bound regarding the equiv-
alence of trained wide linear networks under random initialization to the min-norm solution. Our
analysis, although on a simpler overparametrized model, formally connects overparametrization,
initialization, and optimization with generalization performance. We think it is promising to trans-
late some of the concepts such as the imbalance, and the constrained learning concept to multi-layer
linear networks, and eventually to neural networks with nonlinear activations.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
SyStemS,pp. 6158-6169, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019b.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient de-
scent for deep linear neural networks. In International Conference on Learning RepreSentationS,
2018a.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In 35th International Conference on Machine Learning,
2018b.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019a.
9
Under review as a conference paper at ICLR 2021
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150, 2019b.
Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization effi-
ciently learns positive definite linear transformations by deep residual networks. In International
conference on machine learning, pp. 521-530. PMLR, 2018.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 2020.
Sam Buchanan, Dar Gilboa, and John Wright. Deep networks and the multiple manifold problem.
arXiv preprint arXiv:2008.11245, 2020.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10836-10846,
2019.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937-2947, 2019.
Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach
spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
International Conference on Machine Learning, pp. 1655-1664, 2019.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Sys-
tems (NeurIPS), 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably opti-
mizes over-parameterized neural networks. In International Conference on Learning Represen-
tations(ICLR), 2019, 2019b.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649. IEEE, 2013.
T. H. Gronwall. Note on the derivatives with respect to a parameter of the solutions of a system of
differential equations. Annals of Mathematics, 20(4):292-296, 1919. ISSN 0003486X.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Morris W Hirsch, Robert L Devaney, and Stephen Smale. Differential equations, dynamical systems,
and linear algebra, volume 60. Academic press, 1974.
Roger A Horn and Charles R Johnson. Topics in Matrix Analysis. Cambridge University Press,
1994.
Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, New York,
NY, USA, 2nd edition, 2012. ISBN 0521548233, 9780521548236.
10
Under review as a conference paper at ICLR 2021
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing Systems, pp. 8571-
8580, 2018.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Waseem Rawat and Zenghui Wang. Deep convolutional neural networks for image classification: A
comprehensive review. Neural computation, 29(9):2352-2449, 2017.
Andrew M Saxe, James L Mcclelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural network. In International Conference on Learning Representa-
tions, 2014.
Sheng-De Wang, Te-Son Kuo, and Chen-Fa Hsu. Trace bounds on the solution of the algebraic
matrix riccati and lyapunov equation. IEEE Transactions on Automatic Control, 31(7):654-656,
1986.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets,
Michelle Yeo, Alireza Makhzani, Heinrich Kuttler, John Agapiou, Julian Schrittwieser, et al.
Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782, 2017.
A	Numerical Verification
The scale of the linear regression we consider in the numerical section is D = 400, n = 100, and
m= 1.
A.1 Convergence of single-hidden-layer linear network
Generating training data The synthetic training data is generated as following:
1)	For data matrix X, first we generate X0 ∈ Rn×D with all the entries sampled from N (0, 1), and
take its SVD Xo = WΣν2Φι. Then We let X = WΦι, hence We have all the singular values of X
being 1.
2)	For Y, we first sample Θ 〜N(0, DTID), and E 〜N(0,0.012In), then we let Y = XΘ + e.
Initialization and Training We set the hidden layer width h = 500. We initialize U(0), V (0) with
[U(0)]ij ~N(0,σU), [V(0)]ij 〜N(0, σV), and we consider two cases of such initialization: 1)
σU = 0.1, σV = 0.1; 2) σU = 0.5, σV = 0.02. For these two cases, we run gradient descent on
the averaged loss L= n ∣∣Y - XUVTkF with step size η = 5e - 4.
The case 2 has much greater c than the case 1, and as the consequence, the loss converges much
faster for the case 2, as shown in Fig.2. We see from the right log plot that for the case 1, the bound
in Theorem 1 is not a tight characterization of the asymptotic convergence rate, while for the case
2, when c is large, the bound in Theorem 1 is almost tight regarding the asymptotic rate. Clearly
for case 1, there are additional factors that contribute to the linear convergence, which would be an
interesting research topic in the future work.
11
Under review as a conference paper at ICLR 2021
Figure 2: Convergence of gradient descent with different initial level of imbalance, c :=
σn+m-1(U1T(0)U1(0) -VT(0)V(0)).
A.2 Implicit regularization on wide single-hidden-layer linear network
Generating training data The synthetic training data is generated as following:
1)	For data matrix X, first we generate X ∈ Rn×D with all the entries sampled from N(0, D-1);
2)	For Y, We first sample Θ 〜N(0, DTID), and e 〜N(0, 0.012In), then We let Y = XΘ + e.
Initialization and Training We initialize U(0), V(0) with [U(0)]ij 〜N(0,h-1), [V(0)]ij 〜
N(0, h-1) and run gradient descent on the averaged loss L= ɪ ∣∣Y - XUVTIlF with step size
η = 5e - 4. The training stops When the loss is beloW 1e - 7. We run the algorithm for various h
from 500 to 10000, and we repeat 10 runs for each h.
0.04	0.03	0.02	0.01
ι∕√F
0.2	0.4	0.6	0.8	1.0
Hidden layer width h 1θ4
Figure 3: Implicit regularization of wide single-hidden-layer linear network. ∣U2 (0)V T (0)∣2F is the
initial distance between the end-to-end function to the desired manifold discussed in Section 4.1.
∣∣U(tf)VT(tf) — ΘI is the distance between the end-to-end function and the min-norm solution
when the algorithm stops. The line is plotting the average over 10 runs for each h, and the error bar
shows the standard deviation.
0.2	0.4	0.6	0.8	1.0
Hidden layer width h 1θ4
Clearly, Fig.3 shows that as h increases, the distance between the trained network and the min-norm
solution decreases. The middle plot verifies that the distance is indeed O(h-1/2). Lastly, we note
that the right plot implies the convergence rate approaches a constant as h increases, which verifies
the result in Lemma 1 regarding the imbalance singular value.
12
Under review as a conference paper at ICLR 2021
B	Convergence Rate Analysis for Linear Regression: General
Case
Suppose the input data matrix X has rank d ≤ min{D, n}, we write the compact SVD of X as
X = W∑X恤T, W ∈ Rn×d, Φ1 ∈ RD×d,
and in Section 3 we assume that d = n < D. Notice that we always have WT W = I .
Given the compact SVD, we still define U1 = Φ1TU, and write the loss function as
L(V, U) = 2kY - XUVTkF = 2kY - W∑y2U1VTkF
=1 ∣∣(id - ww T + ww T)y - w ∑χ^u1v T kF
=2k(Id - WWT)Y + W(wTY - £X/2UiVT)kF
=2k(id - wwT)YkF +1 kw(WTY - ∑y2U1VT)kF
+ h(Id - WWT)Y, W(WTY - ∑X〃U1VT)iF
=2k(Id - WWT)YkF + 1 kW(WTY - ∑y2U1VT)kF
=1 k(id - wwT)YkF +1 kwTY - ∑y2U1VTkF,
where the last equality is because that WT W = I, and the second last equality is because that the
cross terms equal zero due to WT (Id - WWT) = 0.
It is easy to see that minv,u L(V,U) = 11 k(Id - WWT)YkF := L*, which is usually referred as
the residue. Similarly to Section.3, we still define the error as E = WTY - ∑X/2Uι VT and one can
check that the gradient flow on L(V, U) yields
V(t) = ET(tKX/2Ui(t), Uι(t) = £X/2E(t)V(t),	(22)
in parameter space (V, U1). Similar to Theorem 1, we have
Theorem B.1. Suppose h ≥ d + m - 1. Let V (t), U1(t), t > 0 be the trajectory of continuous
dynamics (22) starting from some V (0), U1(0). If
σd+m-ι (UT(0)Uι(0) - VT(0)V(0)) = c> 0 ,
then for E (t) = W T Y — ∑X,2Uι(t)V T (t), we have
kE(t)k2F ≤ exp (-2σn(Σx)ct) kE(0)k2F, ∀t > 0 .
Additionally, V (t), U1(t), t > 0 converges to some equilibrium point (V (∞), U1(∞)) such that
E(V(∞), U1(∞)) = 0.
It follows exactly the same proof as for Theorem 1, which is shown in Appendix D, except that the
size of U1 and E is now d × h and d × m respectively.
To Summarize, for any linear regression problem, Theorem 1 shows that sufficient rank of imbalance
guarantees exponential convergence of L(t) - L*, where L* = ɪ k (Id - WWT)YkF.
C Useful Lemmas
Before proving Theorem 1 and 2, we state several Lemmas that will be used in the proof.
The first Lemma is the Gronwall,s inequality (GronWall, 1919) in the differential form.
Lemma C.1 (Gronwall,s inequality). Let u(t), β(t) : [0, +∞) → R be continuous, and u(t) dfer-
entiable on (0, +∞). If
-du(t) ≤ β(t)u(t), ∀t> 0 ,
dt
13
Under review as a conference paper at ICLR 2021
then
u(t) ≤ u(0) exp	β(τ)dτ , ∀t > 0 .
The next Lemma is known as Weyl’s Inequality for singular values.
Lemma C.2 (Weyl’s inequality for singular values). Let A, B ∈ Rn×m, let q = min{n, m}, then
σi+j-1(A + B) ≤ σi(A) + σj(B)
σi+j-1(ABT) ≤ σi(A)σj(BT) ,
for any i, j satisfying 1 ≤ i, j ≤ q and i + j - 1 ≤ q.
The proof can be found in Horn & Johnson (1994, Theorem 3.3.16). Using Weyl’s inequality, we
state and prove a lemma that is used for proving Theorem 2.
Lemma C.3. Let A ∈ Rk×n, B ∈ Rn×m. Suppose n ≤ m, then
σi(A)σn(B) ≤ σi(AB) ,
for 1 ≤ i ≤ min{k, n}.
Proof. We start with the case where k = n. When σn(BT) = 0, the result is trivial. When
σn(BT) = 0, We have BBt = I, where Bt is the Moore-Penrose inverse of B. By Lemma C.2, it
follows that
σi(A) ≤ σi(AB)σ1 (Bt), ∀1 ≤ i ≤ n .
Since σ1 (Bt) = σn-1(B), we get the desired inequality.
When k > n, we have
σi(A) = σi ([A 0k×(k-n)]) ≤ σi (AB) σι([Bt 0m×(k-n)]) = σi(AB)σι(Bt), ∀1 ≤ i ≤ n,
which still leads to the desired result.
When k < n, consider replacing A with
, we have
σi(A)σn(B)=σi	0(n-Ak)×n	σn(B) ≤σi	0(nA-kB)×m
σi(AB), ∀1 ≤ i ≤ k .
□
We also state a trace inequality widely using for solving control problems
Lemma C.4. Suppose for A, B ∈ Rn×n, A is symmetric and B is positive semidefinite, then
λn(A) tr(B) ≤ tr(AB) ≤ λ1 (A) tr(B) .
If both A, B are positive semidefinite, then
σn(A) tr(B) ≤ tr(AB) ≤σ1(A)tr(B).
The proof can be found in Sheng-De Wang et al. (1986, Lemma 1).
D	Proof of Theorem 1
We begin with restating the Theorem.
Theorem 1 (Convergence of linear networks with sufficient rank of imbalance,restated). Suppose
h ≥ m + n - 1. Let V (t), U1(t), t > 0 be the trajectory of continuous dynamics (7) starting from
some V (0), U1(0). If
σn+m-1 (UT (0)Ul(0) - V T (0)V(0)) = c> 0 ,
then for E(t) defined in (6), we have
kE(t)k2F ≤exp(-2σn(Σx)ct)kE(0)k2F, ∀t>0.
Additionally, V (t), U1(t), t > 0 converges to some equilibrium point (V (∞), U1(∞)) such that
E(V(∞), U1(∞)) = 0.
14
Under review as a conference paper at ICLR 2021
Proof. For readability we simply write V (t), U1(t), E(t) as V, U1, E for most of the proof.
Under (7), the time derivative of error is given by
E = -∑χt2UιUl∑/E - ΣχEVVT .
Consider the time derivative of kE k2F ,
-d kE kF = -d tr(ET E)
dt	dt
=-2tr (ET∑X〃U1UT∑X/2E + ETΣxEVVT) .	(23)
Use the trace inequality in Lemma C.4 to get the lower bound the trace of two matrices respectively
as
tr (ET∑χ∕2UιUT∑X∕2E) = tr (∑X〃EET∑X〃UιUT)
≥ σn(UιUf )tr QX/2EET∑X/2)
=σn(UM )tr(∑χEET)
≥ σn(U1U1T)σn(∑x)tr(EET) =σn(U1U1T)σn(∑x)kEk2F,	(24)
and
tr (ET∑χEVVT) ≥ σm(VVT) tr(ET∑xE)
=σm (VV T )tr(∑χEET)
≥ σm(VVT)σn(∑x) tr(EET) = σm(V V T)σn(∑x)kEk2F.	(25)
Combine (23) with (24)(25), we have
dkEkF ≤ -2σn(∑x) (σn(U1Uf) + σm(VVT)) ∣∣E∣∣F	(26)
Moreover, we have
σn(U1U1T) +σm(VVT)
=σn(U1TU1)+σm(VTV)
= σn(U1TU1 ) + σm (-VTV)
(Lemma C.2) ≥ σn+m-1 (U1TU1 - VTV)
(Imbalance is time-invariant) = σn+m-1(U1T(0)U1(0) - V T (0)V (0)) = c ,
where the first equality uses the fact that U1U1T(VVT resp.) has the same non-zero singular values
as U1T U1 (VTV resp.). Finally we have
-dkEkF ≤ -2σn(∑x)ckEkF .
dt
The result follows by applying GronWall's inequality, Lemma C.1, which leads to
kE(t)k2F ≤ exp (-2σn(∑x)ct) kE(0)k2F, ∀t > 0 ,	(27)
then the exponential convergence of E(t) is proved.
Regarding the second statement, for the gradient system (7), the parameters (U1 (t), V (t)) converge
either to an equilibrium point which minimizes the potential kE(t)k2F or to infinity (Hirsch et al.,
1974).
Consider the following dynamics
d
dt
V(t)
U1(t)
. 0	ET (t)∑χ∕2] 「V(t)]
∑X∕2E(t)	0 I [Ui⑴]
.	一一	-L^~{^~}
=AZ (t)	=Z(t)
(28)
15
Under review as a conference paper at ICLR 2021
which is a time-variant linear system. Notice that by Horn & Johnson (2012, Theorem 7.3.3), we
have kAz(t)∣∣2 = |加±/2E(t)∣∣2.
From (28), we have
d kZ (t)kF = 2tr(Z T (t)Az (t)Z ⑻
=2tr(Z (t)ZT (t)Az (t))
≤ 2kAz(t)k2 tr(Z(t)ZT(t))
=2∣∣∑y2E(t)k2kZ (t)kF
≤ 2σY2(Σχ)∣∣E(t)k2∣∣Z(t)kF ≤ 2σY2(Σχ)kE⑴口尸∣∣Z(t)kF .
By GronWall's inequality, Lemma C.1, We have
kz(t)kF ≤ exp U： 2σy2(Σχ)kE(τ)||尸dτ) ∣∣Z(0)kF .
Finally, from (27), We have kE(t)kF ≤ exp (-σn(Σx)ct) kE(0)kF, ∀t > 0, Which leads to
kZ (t)k2F ≤
≤
exp Uot 2σ1∕2(Σχ)∣∣E(τ)∣∣fdτ) ∣∣Z(0)kF
exp
exp
(Σx)kE(0)kF Ztexp(-σn(Σx)cτ)dτ	kZ(0)k2F
(Σx)kE(0)kF	exp(-σn(Σx)cτ) dτ	kZ(0)k2F
eχp(⅛n⅛ kE(o)QkZ (0)kF.
(29)
≤
Therefore the trajectory V (t), U1(t), t > 0 is bounded, i.e. it can not converge to infinity, then it has
to converges to some equilibrium point (V(∞), Uι(∞)) such that E(V(∞), Uι(∞)) = 0.	□
E Comparis on with the NTK Initialization for wide
single-hidden-layer linear networks
In Section 4.2, We analyzed generalization property of Wide single-hidden-layer linear netWorks
under properly scaled random initialization. Our initialization for netWork Weights U, V is different
from the typical setting in previous Works (Jacot et al., 2018; Du & Hu, 2019; Arora et al., 2019b).
In this section, We shoW that under our setting, the gradient floW is related to the NTK floW by 1)
reparametrization and rescaling in time ; 2) proper scaling of the netWork output. The necessity of
output scaling is also shoWn in Arora et al. (2019b).
In this paper We Work With a single-hidden-layer linear netWork defined as f : RD →
Rm, f(x; V, U) = VUTx, Which is parametrized by U, V. Then We analyze the gradient floW on
the loss function L(V, U) = 1 ∣∣Y - XUVThF, given the data and output matrix X, Y. Lastly, in
Section 4.2, We initialize U (0), V(0) such that all the entries are randomly draWn from N 0, h-1 ,
Where h is the hidden layer Width.
Now we define U := √hU, V := √hV, then the loss function can be written as
~ . ~	~ . I
LvU ) = L(V,U) = 2
2
F
1
Y ——XUVT
h
1	Y -里 ɪXUVT
2	∣ h mh ∣F
2
2
1n
1X
2 J
i=1
y(i)-百焉V UTx(i)
16
Under review as a conference paper at ICLR 2021
■‰ τ . ∙	. i . p / ST τ`τ∖	1 -r'7--r~τT, ♦ ,ι , ♦ ι ,	1	ι ∙	♦	ι ∕τ
Notice that f (x; V, U) = √^= VUTX is the typical network discussed in previous works (JaCot
et al., 2018; Du & Hu, 2019; Arora et al., 2019b). When all the entries of U (0), V(0) are initialized
randomly as N(0, h-1), the entries of U(0), V(0) are random samples from N (0,1), which is the
typical choice of initialization for NTK analysis.
TT	,1	√ ∙ √	∙ √ . √ ST √ ∖ ∙	1 11	A /m,	T	♦	11.
However, the difference is that f (x; V, U) is scaled by M. In previous work showing non-
asymptotic bound between wide neural networks and its infinite width limit (Arora et al., 2019b,
Theorem 3.2), the wide neural network is scaled by a small constant κ such that the prediction
by the trained network is within -distance to the one by the kernel predictor of its NTK. More-
over, Arora et al.(2019b) suggests ɪ should scale as poly( ɪ), i.e., to make sure the trained network
is arbitrarily close to the kernel predictor, K should be vanishingly small. In our setting, the random
initialization implicitly enforces such a vanishing scaling √m, as the width of network increases.
Lastly, we show that the gradient flow on L(V, U) only differs from the flow on L(V, U) by the time
scale. Suppose U, V1 follows the gradient flow on L(V, U) w.r.t. time t. Define i := ht, we have
2U = √h2U = √h-U = ɪ-U
dt	dt	dt dt h dt
and similarly we have SV = -√∂VL(V, U).
—
1∂
√h∂U L(V,U),
Now notice that -dU = dt	S LMU )=-√hXT(Y - xuv T )V =-1XT (Y - 1XU V T) V=-+ L(V ,U),
and -dV = dt	TdVL(V,U )=-√h(Y - xuv T )Tχu =-1 (Y - h xu V t )t xu=-余L(V,U)
Therefore, the gradient flow of U, V on L(V, U) w.r.t. time t is equivalent to the gradient flow of
τ~τ τV G/fV τ~τ∖ .	t λ .∙	7	7 >
U, V on L(V, U) w.r.t. a rescaled time t = ht.
Another way to see the time scale difference is the following, consider the gradient flow on L(V, U)
w.r.t. time t, we have	-∣U (t) = - dU L(V (t),U (t)) dt	∂ U ⇔√hdtu(t) = - ∂U L(V(t),U (t)) ⇔√⅛U(t) = -√⅛ L(V(t),U(t)) ⇔ dtU(t) = -h∂Ur L(V(t), U(t)),	(30)
where we use the fact that
∂ ∂U L(V (t),U (t))	= XT(Y - XU(t)VT(t))V(t) =√hXT (Y - 1 xU(t)VT(t)) V(t) = √h∂LL(V(t), U(t)).
Similarly we have dtV (t) =	-∂VL(V(t), U(t)) ⇔ 杆(t) = -h∂LL(V(t), U(t)).	(31)
1	__, .___, .	_____ 二_ , .	,.
1We write U(t), V(t) as U, V for simplicity. Same for U(t),V(t).
17
Under review as a conference paper at ICLR 2021
From (30) and (31) we know that the gradient flow on L(V, U) w.r.t. time t essentially runs the
gradient flow on L(V , U) with an accelerated rate by h.
Such equivalence through time rescaling suggests that running gradient flow on our setting is h times
faster than the NTK one. In Arora et al. (2019b), as we mentioned above, the network is scaled by
a small constant κ such that the trained network is within -distance to the kernel predictor by its
NTK in terms of the prediction. As a consequence, the convergence rate is scaled by κ2, which
makes the convergence slower. Therefore, our initialization scheme yields similar result as in Arora
et al. (2019b) but the gradient flow is faster. Also, we note that this gap in rate of convergence
in not present in (Du & Hu, 2019), which only focuses on providing convergence guarantees of
the algorithm. In that case, the network is not scaled by a small κ, however, the properties of
generalization is not studied there.
F Proof of Lemma 1 and Theorem 2
To prove Lemma 1 and Theorem 2, we use a basic result in random matrix theory
Lemma F.1. Given m, n ∈ N with m ≤ n. Let A be an n × m random matrix with i.i.d. standard
normal entries Aij 〜N (0,1). For δ > 0, with probability at least 1 一 2 exp(-δ2), we have
√n - (√m + δ) ≤ σm(A) ≤ σι(A) ≤ √n + (√m + δ).
The proof can be found in Davidson & Szarek (2001, Theorem 2.13)
In this section, we show more general results under the following random initialization
1 ≤ i ≤ D, 1 ≤ j ≤ h ,
1 ≤ i ≤ m, 1 ≤ j ≤ h ,
where 4 < α ≤ 2. It is easy to see that α = 2 corresponds to the random initialization scheme
shown in Section 4.2, i.e. all the entries of U (0), V (0) are random mean zero Gaussian with h-1
variance.
Regarding the imbalance and orthogonality condition, we have the following
Lemma F.2. Let 4 < α ≤ ɪ. Given data matrix X. ∀δ ∈ (0,1), ∀h > h0 = poly (m, D, ɪ),
with probability at least 1 一 δ over random initializations with [U(0)]j, [V(0)]j 〜 N(0, h-2α),
we have all the following hold.
1.	(Sufficient rank of imbalance)
√m + D + 1 log 2
σn+m (UT(0)Uι(0) - VT(0)V(0)) > h1-2α - 2
2.	(Approximate orthogonality condition)
V (0)U2T (0)
U1(0)U2T(0)	F
≤ 2 ʌ/m +
nED +ι1 log 2
∣∣Ui(0)v T (0)IlF ≤ 2√m∙
√m + D + 1 log 2
h2α-2
From the Lemma, We can see why our analysis only applies to the case where 4 < α ≤ 2: 1) If
α > 1, the lower bound we can obtain for σn+m (UT(0)Uι(0) - VT(O)V(0)) will decreases to
zero as h increases; 2) If α ≤ 4, the orthogonality condition will not be asymptotically satisfied as
h increases.
From Lemma F.2, let α = ɪ, we have
18
Under review as a conference paper at ICLR 2021
Lemma 1 (restated). Given data matrix X. ∀δ ∈ (0,1), ∀h > ho = poly (m, D, 1), with prob-
ability at least 1 — δ over random initializations with [U(0)]j, [V(0)] j 〜N(0, h-1), we have all
thefollowing hold.
1.	(Sufficient rank ofimbalance)
σn+m (UT(0)U1(0) — Vt(0)V(0)) > 1 — 2√m + D + 1 "gɪ ,
2.	(Approximate orthogonality condition)
∣∣]V(0)UT (0) ]∣∣
Il [uι(0)uτ(0)MF
≤ 2√m+^ GD W 1log 2
h
∣∣u1(0)vT (0)∣∣f ≤ 2√m∙
√m + D + 1 log 2
Now we present the proof for Lemma F.2
ProofofLemma F.2. For readability we simply write U(0), U1(0), U2(0), V(0) as U, U1, U2,V.
Consider the matrix [VT Ut] which is h × (m + D). Apply Lemma F.1 to matrix A
ha [VT Ut] , with probability at least 1 — δ, we have
√h — (√m+1 + δ) ≤ σm+d(hα [VT Ut]) ≤ σ1(hα [VT
which leads to
h1 i7√ + Dha 1 log1 ≤ i([VT Ut]) ≤ σ1([VT
Regarding the first inequality, write the imbalance as
UT]) ≤ √h + (√Im + d + δ),
UT]) ≤ h2-≈+ √m + D + 1log δ
h h	hα
(32)
UTU1 — VTV = [VT	UT] —V] = [VT	UT] —Im	φi0φt] [V
For h > (√m + D + 1 log 2)2, assume event (32) happens, then σm+p ([VT UT]) ≥ h2-α —
√m+D+1 l°g-2 > 0, then we have
σn+m(UT Ui — VT V)
σn+m
(Lemma C.3) ≥ σn+m
σn+m
σn+m
UT
(Lemma C.3) ≥ σn+m
UT
-Im
—Im
0
—Im
0
—Im
0
—Im
0
V
Φ1ΦT	u
0
Φ1ΦT
σm+D
Φ1 φT	u
0
Φi φT
0
Φ1ΦT
u T])「
σ m+D
σm+D
0
V
0
V
U
V
U
—Im
where the last equality is due to the fact that	0m
value and all of them are 1.
has exactly n + m non-zero singular
Therefore when h > (√m + D + 2 log 2)2, conditioned on event (32), with probability 1 we have
σn+m(UTU1 — VtV) ≥ σm+D ([VT Ut])
19
Under review as a conference paper at ICLR 2021
≥ h 2-α
—
√m + D + 2 log 2 )
hα
h1-2α - 2
√m + D + 2 log
h2α— 2
√m + D + 2 log 2 V
hα
> h1-2α - 2
√m + D + 1 log 2
(33)
Regarding the second and third inequality, using the fact that ∣∣A∣∣f ≤，min{n,m}kA||2,A ∈
Rn×m, we have
1
VUT-
U1Uf
≤	UV1UU2T2T
0T1 0T1 T
a a /
M OM O "
- -- --
==≤
UT] - ηlm+D
where the second equality is by the fact that
Im
0
0
ΦT
0
Φ2
MIL
,for any n ∈ R ,
2
0, and
2
五+
2
V
U
V
U
M 2
[VT UT] - ηIm+D
目UiVTIlF ≤ Il UiVt K 2
0
ΦT]
V
U
M2
UT
V
U
0
ΦT]
[vt ut] - nIm+D
V
U
-η1m+D
M2
,for any n ∈ R ,
2
where the second equality is by the fact that [0 Φ" Im = 0.
Notice that
Il ]V	[V T	U t ]	-	nIm+D	=max 层([v T	U T ])- n|	∙
Again we let h > (√m + D + 2 log 2『When event (32) happens, all σf( [VT UT]) are within
the interval ](h2-α - √m+D+1 logɪ) , (h 1 -a - √m+D+1 logɪ)	. Since the choice of n is
arbitrary, we pick
-∣ √	√ √ m + D + ⅛ log ⅞ ʌ
n = h1-2α + (-——h. 2	,	(34)
which is the mid-point of this interval, then we have
maχ 卜2([VT UT])- n1
≤ max
h 2-ɑ
√m + D + 2 log 2 V
hα
-n
h 2-α +
hα
√m + D + 1 log 2
—
(n is the mid-point)
20
Under review as a conference paper at ICLR 2021
√m + D + 1 log 2
hα
h1-2α
√m + D + 1 log 2
hα
2 VZm + D + 1 log 2
Therefore, When h > h0 = (√m + D + 1 log j)； conditioned on event (32), With probability 1,
we have
∣[U1UT ]∣F WK ∣[V
and IlUIVt∣∣f ≤ √mll [V [VT
「丁	丁 1	√-----√m + D + l log τ
[VT UT] - ηIm+D	≤ 2√m + n------- 9 _1----,
2	h2α- 2
丁】	,—√ m + D + 1 log 2
UT] - ηIm+D	≤ 2√m------ʒ-T------- ,	(35)
2	h2α-2
Where We choose η as in (34). Conditioned on event (32), events (33) and (35) happen With proba-
bility 1, hence the probability that both (33) and (35) happen is at least the probability of event (32),
which is at least 1 一 δ.	□
More generally, for readers’ interest, We shoW that all the non-zero imbalance singular values con-
centrate to h1-2α as h increases. For the case of α = 2, the singular values concentrate to 1, as
suggested by the folloWing
Claim F.1. Let 4	< α ≤ 1. Given data matrix	X. ∀δ ∈ (0,1), ∀h > ho = poly	(m,D, 1),
with probability at least 1 一 δ over random initializations with [U(0)]ij, [V(0)]ij 〜	N(0,	h-2α),
we have all the following hold.
σn+m	(UT(0)U1(0) - VT(0)V(0)) >	卜2-α 一 √m + Dha 1 log2!	,	(36)
σ1	(UT (0)Ui(0) - V T (0)V(0)) ≤	fl1 -α + √m + Dh+ 2 log ɪ!	.	(37)
Proof. For readability We simply Write U (0), U1(0), V(0) as U, U1, V. When the Width condition
h > ho = poly (m,D, 1) is satisfied. Condition on event (32). The lower bound (36) for the
n + m-th singular value has been shoWn by (33).
For the upper bound (37), notice that
σ1(U1TU1-VTV)=σ1[VT UT][-0Im Φ10Φ1T][VU]
≤ σ1 ([-Im Φ10ΦT]) σ2 ([VT Ut])
≤ σ2([VT UT]),
where again we use the the fact that
all of them are 1.
has exactly n + m non-zero singular value and
Condition on event (32), we have
σ1 (UTUi - VTV) ≤ σ2 ([VT Ut]) ≤ 卜2-ɑ + √m + Dh+ 1 log 2 ),
which is (37). Therefore (36)(37) holds with at least 1 - δ probability.	□
With Lemma F.2, we have the following result regarding the generalization property of wide single-
hidden-layer linear networks. Notice that here the result is presented under random initialization
such that all entries of U(0),V(0) are sample from N(0, h-2α), 4 < α ≤ 2.
21
Under review as a conference paper at ICLR 2021
Theorem F.1. Let 4 < a ≤ 1. Let (V(t), U(t),t > 0) be a trajectory of continuous dynamics
⑺.Then, ∃C > 0, such that ∀δ ∈ (0,1),∀h > h1∕(4α-1) with ho = Poly (m, D, 1, ；3 (∑x)), with
probability 1 一 δ over random initializations with [U (0)]ij, [V (0)]j 〜 N(0, h-2α), we have
ku(∞)VT(∞) 一 Θk2 ≤ 2C1/h1-2a√m+n√m+D^ +12 log 2 ,	(38)
h2α- 2
where C depends on the data X, Y .
From Theorem F.1,let α = 1, we have
Theorem 2 (Generalization of wide single-hidden-layer linear network, restated). Let
(V (t), U (t), t > 0) be a trajectory of continuous dynamics (7). Then, ∃C > 0, such that
∀δ ∈ (0,1),∀h > ho = PoIy(^m,D, δ,聚：)), with probability 1 一 δ over random initializa-
tions with [U (0)]ij, [V (0)]j 〜N (0, h-1), we have
ku(∞)VT(∞) 一 Θk2 ≤ 2C√m+
n √m+D* log 2
√h
where C depends on the data X, Y .
Now we only remain to prove Theorem F.1
Proof of Theorem F.1. From the continuous dynamics (7) and Theorem 1, the stationary point
U (∞), V(∞) satisfy
U1(∞)VT(∞) = ΦTΘ,	U2(∞) = U2(0).
Hence we have
∣∣U (∞)VT (∞) — Θ ∣∣2 = ∣∣Φ1U1(∞)VT (∞) + Φ2U2(∞)VT (∞) — Θ∣∣2
=∣Φ1ΦT Θ + Φ2U2(∞)VT (∞) — Θ∣2
= ∣Φ2U2(∞)VT(∞)∣F
= ∣Φ2U2(0)VT(∞)∣F = ∣U2(0)V T (∞)∣2 ≤ ∣U2(0)VT(∞)∣F .
Consider the following dynamics
d
dt
V(t)U2T(0)
U1(t)U2T(0)
0	ET (t)∑X/2
Σx!/E (t)	0
{^^^^^^^^^^^^~
:=AZ (t)
V (t)U2T (0)
U1(t)U2T(0)
:=Z (t)
(39)
which is a time-variant linear system, and in particular, by Horn & Johnson (2012, Theorem 7.3.3),
We have ∣∣Az(t)∣2 = k∑X∕2E(t)∣2. Notice that here the Z(t) is different from the one in the proof
for Theorem 1.
From (39), we have
d ∣z (t)kF = 2tr(ZT (t)Az (t)Z (t))
=2tr(Z (t)ZT (t)Az (t))
≤ 2∣Az(t)∣2tr(Z(t)ZT(t))
=2∣∣∑y2E(t)k2kZ (t)∣F
≤ 2σY2(Σχ)kE(t)∣2∣Z(t)∣F ≤ 2σV(Σχ)kE(t)∣F∣Z(t)∣F .
By GronWall's inequality, Lemma C.1, we have
∣z(t)kF ≤ exp IZQt 2σy2(Σχ)∣∣E(τ)∣fdτ) ∣Z(0)∣F
22
Under review as a conference paper at ICLR 2021
⇒ kZ(t)kF ≤ exp IZQt σ12(Σχ)kE(τ)∣∣fdτ) ∣∣Z(0)∣∣f
(40)
Using Lemma F.2, for h > h0 := 16 (√m + D + 1 ln 2)2, with probability at least 1 - δ We have
all the following.
σn+m (UT(0)U1(0) - VT(0)V(0)) >
kZ(0)kF =	UV1((00))UU2T2T((00))F ≤
U1(0)VT(0) ≤
h1-2α - 2
2 √m^÷
√m + D + 1 ln 2
h2α-2
一 √m + D + 1 ln 2
n--------12——-
2√m √m + D +ι2ln -
h2α-2
≥1 h1-2a.
(41)
(42)
(43)
Here h00 is larger than one from Lemma 1 because in (41) we want the least non-zero singular value
of the imbalance to be further bounded by 2.
From Theorem 1, we have
kE(t)k2F ≤ exp(-2σn(Σx)ct) kE(0)k2F,
where C = σn+m-ι (UT(0)Uι(0) - VT(O)V(0)), then by (41), we have
kE(t)kF ≤ exp (-h1-2ασn(Σχ)t) ∣∣E(0)∣∣F
⇒ kE(t)kF ≤ exp (-h1-2ασn(∑χ)t∕2) ∣∣E(0)∣∣f .
Finally, from (40), we have
kZ(t)kF ≤
expZtσ11∕2(Σx)kE(τ)kFdτ) kZ(0)kF
exp 卜 1"(∑χ)kE(0)kF IZQ exp (-h1-2ασn(∑χ)τ∕2) dτ
∞
kZ(0)kF
exp σ11∕2(Σx)kE(0)kF
exp (-h1-2ασn(Σχ)τ∕2) dτ ∣∣Z(0)∣∣f
≤
≤
eχp(2 hiσ1sn⅛) kE(0)kF)kZ(0)kF.
(44)
The initial error depends on the initialization but can be upper bounded as
kE(0)kF = kWTY - ∑-1∕2U1(O)VT(0)kF
≤ kWTYkF + k∑-1∕2U1(0)VT(0)kF
≤ kYkF+σn-1∕2(Σx)kU1(0)VT(0)kF
then we can write (44) as
kZ(t)kF ≤ exp (2h⅞⅛) kY∣f) exp (2h1-O⅜kUI(O)VT(O)∣f) kZ(0)kF
exp
2σ⅛∑f kY∣f) exp(2σl⅛kUi(O)VT(O)")[	kZ(O)∣f .
(45)
For the second exponential, we let ho := max {h0,16；3(∑x) m (√m + D + 2 ln 2)21, then ∀h >
h01∕(4α-1) , by (43) we have
σ σj∕2(Σx)u ，、 T，、“ ∖	σ σj∕2(Σx) L√m + D + 2 ln 2∖
exp 2 3/2 F kU1(O)V (O)kF ≤ exp 4 3/2 F √m------------------h2→--------- ≤ e. (46)
∖ σ√ (Σχ)	σ	∖ σ√ (Σχ)	h 2	)
23
Under review as a conference paper at ICLR 2021
Notice that h > h1/(4a-1) also ensures h > h1/(4a-1) ≥ h0 ≥ h0, hence the width condition for
(41)(42)(43) to hold is satisfied.
Finally by (42)(46), we write (45) as
kZ ⑴ * ≤ kp(1+2 小 kY kF
1∕h1-2α
kZ(0)kF
exp (l + 2才U：) kY∣∣F
σn(Σx)
!#
1∕h1-2α
2 m+n
√m + D + 1 ln 2
h2α-2
≤
|
}
2C 1/h1-2a
√-√m + D + 1 ln 2
√m+^ —h2α- 1 2	"
Therefore for some C > 0 that depends on the data (X, Y), given any 0 < δ < 1, when h >
h1∕(4α-1) as defined above, with at least probability 1 - δ,we have
kU(∞)VT(∞) - Θk2 ≤
≤
kU2(0)V T (∞)kF
sup kU2(0)VT(t)kF
t>0
SUp kZ(t)kF ≤ 2C 1/h1-2a
t>0
≤
I-√m + D + 1 ln 2
√m+^ —h2α- 12
□
24