Under review as a conference paper at ICLR 2021
Model-Targeted Poisoning Attacks
with Provable Convergence
Anonymous authors
Paper under double-blind review
Ab stract
In a poisoning attack, an adversary with control over a small fraction of the training
data attempts to select that data in a way that induces a model that misbehaves in
a particular way desired by the adversary, such as misclassifying certain inputs.
We propose an efficient poisoning attack that can target a desired model based on
online convex optimization. Unlike previous model-targeted poisoning attacks, our
attack comes with provable convergence to any achievable target classifier. The
distance from the induced classifier to the target classifier is inversely proportional
to the square root of the number of poisoning points. We also provide a lower
bound on the minimum number of poisoning points needed to achieve a given
target classifier. Our attack is the first model-targeted poisoning attack that provides
provable convergence, and in our experiments it either exceeds or matches the best
state-of-the-art attacks in terms of attack success rate and distance to the target
model. In addition, as an online attack our attack can incrementally determine
nearly optimal poisoning points.
1	Introduction
State-of-the-art machine learning models require a large amount of labeled training data, which often
depends on collecting data and labels from untrusted sources. A typical application is email spam
filtering, where a spam detector filters out spam messages based on features (e.g., presence of certain
words) and periodically updates the model based on newly received emails labeled by users. In such
a setting, spammers can generate “non-spam” messages by injecting non-related words or benign
words, and when models are trained on these “non-spam” messages, the filtering accuracy will drop
significantly (Lowd & Meek, 2005). Such attacks are known as poisoning attacks, and a training
process that collects labels or data from untrusted sources is potentially vulnerable to them.
Poisoning attacks can be categorized into objective-driven attacks and model-targeted attacks de-
pending on whether a target model is considered in the attack process. Objective-driven attacks have
a specific attacker objective and aim to achieve the attack objective by generating the poisoning
points; model-targeted attacks have a specific target classifier in mind and aim to induce that target
classifier by generating a minimal number of poisoning points. Objective-driven attacks are most
commonly studied in the existing literature. The attacker objective is typically one of two extremes:
indiscriminate attacks, where the adversary’s goal is simply to decrease the overall accuracy of the
model (Biggio et al., 2012; Xiao et al., 2012; Mei & Zhu, 2015b; Steinhardt et al., 2017; Koh et al.,
2018); and instance-targeted attacks, where the goal is to produce a classifier that misclassifies
a particular known input (Shafahi et al., 2018; Zhu et al., 2019; Koh & Liang, 2017). Recently,
Jagielski et al. (2019) introduced a more realistic attacker objective known as a subpopulation attack,
where the goal is to increase the error rate or obtain a particular output for a defined subpopulation of
the data distribution. Attacker objectives for realistic attacks are diverse and designing a unified and
effective attack strategy for different attacker objectives is hard. Gradient-based local optimization is
most commonly used to construct poisoning points for a particular attacker objective (Biggio et al.,
2012; Xiao et al., 2012; Mei & Zhu, 2015b; Koh & Liang, 2017; Shafahi et al., 2018; Zhu et al., 2019).
Although these attacks can be modified to fit other attacker objectives, since they are based on local
optimization techniques they can easily get stuck into bad local optima and fail to find effective sets
of poisoning points (Steinhardt et al., 2017; Koh et al., 2018). To circumvent the issue of local optima,
Steinhardt et al. (2017) formulate an indiscriminate attack as a min-max optimization problem and
1
Under review as a conference paper at ICLR 2021
solve it efficiently using online convex optimization techniques. However, the strong min-max attack
only applies to the indiscriminate setting.
In contrast, model-targeted attacks incorporate the attacker objective into a target model and hence,
the target model can reflect any attacker objective. Thus, the same model-targeted attack methods
can be directly applied to a range of indiscriminate and subpopulation attacks just by finding a
suitable target model. Mei & Zhu (2015b) first introduced a target model into a poisoning attack,
but their attack is still based on gradient-based local optimization techniques and suffers from bad
local optima (Steinhardt et al., 2017; Koh et al., 2018). Koh et al. (2018) proposed the KKT attack,
which converts the complicated bi-level optimization into a simple convex optimization problem
utilizing the KKT condition, avoiding the local optima issues. However, their attack only works for
margin based losses and does not provide any guarantee on the number of poisoning points required
to converge to the target classifier.
In this work, we focus on model-targeted attacks and aim to understand the feasibility of a poisoning
adversary to induce any target model. In particular, we find both theoretical and empirical bounds on
the sufficient (and necessary) number of poisoning points to get close to a specific target classier. 1
Contributions. Our main contributions involve developing a principled and general model-targeted
poisoning attack strategy, along with a proof that the model it induces converges to the target model.
Our poisoning method takes as input a target model, and produces a set of poisoning points. We prove
that the model induced by training on the original training data with these points added, converges to
the target classifier as the number of poison points increases, given that the loss function is convex
and proper regularization is adopted in training (Theorem 4.1). Previous model-targeted attacks
lack of such convergence guarantees. We then prove a lower bound on the minimum number of
poisoning points needed to reach the target model (Theorem 4.2), given that the loss function for
empirical risk minimization is convex. Such a lower bound can be used to estimate the optimality of
model-targeted poisoning attacks and also indicate the intrinsic hardness of attacking different targets.
Our attack is also efficient in incremental poisoning scenario as it works in an online fashion and can
incrementally find poisoning points that are nearly optimal. Previous model-targeted attacks work
with fixed number of poisoning points and need to know the poisoning budget in advance. We run
experiments to compare our attack to the state-of-the-art model-targeted attack (Koh et al., 2018).
We first evaluate the convergence our attack to the target model and find that, under same number of
poisoning points, classifiers induced by our attack are closer to the target models than the best known
attack, for all the target classifiers we tried. Then, we evaluate the success rate of our attack, and find
that it has superior performance than the state-of-the-art in the more realistic subpopulation attack
scenario, and comparable performance in the conventional indiscriminate attack scenario (Section 5).
2	Problem Setup
The poisoning attack proposed in this paper applies to multi-class prediction tasks or regression
problems (by treating the response variable as an additional data feature), but for simplicity of
presentation we consider a binary prediction task, h : X → Y, where X ⊆ Rd and Y = {+1, -1}.
The prediction model h is characterized by parameters θ ∈ Θ ⊆ Rd . We define the non-negative
convex loss on an individual point, (x, y), as l(θ; x, y) (e.g., hinge loss for SVM model). We also
define the empirical loss over a set of points A as L(θ; A) = P(x,y)∈A l(θ; x, y).
We adopt the game-theoretic formalization of the poisoning attack process from Steinhardt et al.
(2017) to describe our model-targeted attack scenario:
1.	N data points are drawn uniformly at random from the true data distribution over X × Y
and form the clean training set, Dc.
2.	The adversary, with knowledge of Dc, the model training process and the model space Θ,
generates a target classifier θp ∈ Θ that satisfies the attack goal.
3.	The adversary produces a set of poisoning points, Dp, with the knowledge of Dc, model
training process, Θ and θp .
4.	Model builder trains the model on Dc ∪ Dp and produces a classifier, θatk.
1Similar to previous works, in this paper, we focus on designing a model-targeted attack that works for any
achievable target model and leave the exploration of finding better target classifiers as the future work.
2
Under review as a conference paper at ICLR 2021
The adversary’s goal is that the induced classifier, θatk, is close to the desired target classifier, θp
(Section 4.2 discusses how this distance is measured). Step 2 corresponds to the target classifier
generation process. Our attack works for any target classifier, and in the paper we do not focus on the
question of how to find the best target classifier to achieve a particular adversarial goal but simply
adopt the heuristic target classifier generation process from Koh et al. (2018). Step 3 corresponds to
our model-targeted poisoning attack, and is also the main contribution of the paper.
We assume the model builder trains a model through empirical risk minimization (ERM) and the
training process details are known to the attacker:
θc = argminπ17L(θ; Dc) + CR ∙ R(θ)	(1)
θ∈Θ |Dc|
where R(θ) is the regularization function (e.g., 2 ∣∣θk2 for SVM model).
Threat Model. We assume an adversary with full knowledge of training data, model space and
training process. Although this may be unrealistic for many scenarios, this setting allows us to focus
on a particular aspect of poisoning attacks, and is the setting used in many prior works (Biggio
et al., 2011; Mei & Zhu, 2015b; Steinhardt et al., 2017; Koh et al., 2018; Shafahi et al., 2018). We
assume an addition-only attack where the attacker only adds poisoning points into the clean training
set. A stronger attacker may be able to modify or remove existing points, but this typically requires
administrative access to the system. The added points are unconstrained, other than being value
elements of the input space. They can have arbitrary features and labels, which enables us to perform
the worst case analysis on the robustness of models against addition-only poisoning attacks. Although
some previous works also allow arbitrary selection of the poisoning points (Biggio et al., 2011; Mei &
Zhu, 2015b; Steinhardt et al., 2017; Koh et al., 2018), others put different restrictions on the poisoning
appoints. A clean-label attack assumes adversaries can only perturb the features of the data, but the
label is given by an oracle labeler (Koh & Liang, 2017; Shafahi et al., 2018; Zhu et al., 2019; Huang
et al., 2020). In label-flipping attacks, adversaries are only allowed to change the labels (Biggio et al.,
2011; Xiao et al., 2012; 2015; Jagielski et al., 2019). These restricted attacks are weaker than the
poisoning attacks without restrictions (Koh et al., 2018; Hong et al., 2020).
3	Related Work
The most commonly used poisoning strategy is gradient-based attack. Gradient-based attacks
iteratively modify a candidate poisoning point (x, y) in the set Dp based on the test loss defined on X
(keeping y fixed). This kind of attack was first studied on SVM models (Biggio et al., 2012), and
later extended to linear and logistic regression (Mei & Zhu, 2015b), and recently to larger neural
network models (Koh & Liang, 2017; Yang et al., 2017; Munoz-Gonzdlez et al., 2017; Shafahi et al.,
2018; Zhu et al., 2019; Huang et al., 2020). Jagielski et al. (2018) also studied gradient attacks and
principled defenses on linear regression tasks. Their work studies linear regression while in this paper,
we mainly focus on binary classification, although our attack can also be extended to regression
tasks. More importantly, our attack aims to induce a target model by generating poisoning points
while Jagielski et al. (2018)’s attack tries to increase the Mean Squared Error of the linear regression
task with a fixed poisoning budget. In addition to classification and regression tasks, gradient-based
poisoning attacks are also applied to topic modeling (Mei & Zhu, 2015a), collaborative filtering (Li
et al., 2016) and algorithmic fairness (Solans et al., 2020).
Besides the gradient-based attacks, researchers also utilize generative adversarial networks to craft
poisoning points efficiently for larger neural networks, however, the effectiveness of the attack
is limited (Yang et al., 2017; Munoz-Gonzdlez et al., 2019). The strongest attacks so far are the
KKT attack (Koh et al., 2018) and the min-max attack (Steinhardt et al., 2017; Koh et al., 2018).
However, the KKT attack cannot scale well for multi-class classification and is limited to margin
based losses (Koh et al., 2018). The min-max attack only works for indiscriminate attack setting, but
additionally provides a certificate on worst case test loss for a fixed number of poisoning points. We
are also inspired by Steinhardt et al. (2017) to adopt online convex optimization to instantiate our
model-targeted attack, but now deals more general attack scenario. We also distinguish ourselves
from the poisoning attack against online learning (Wang & Chaudhuri, 2018). The attack against
online learning considers a setting where training data arrives in a streaming manner while we
3
Under review as a conference paper at ICLR 2021
consider the offline setting with training data being fixed. Another line of work studies “targeted”
poisoning attacks where an adversary guarantees to increase the probability of an arbitrary “bad”
property (Mahloujifar et al., 2019a; 2017; 2019b), as long as that property has some non-negligible
chance of naturally happening. These attacks cannot be applied in the model-targeted setting as
the probability of naturally producing a specific target model is often negligible. Related to our
Theorem 4.2, Ma et al. (2019) also derived a lower bound on number of poisoning points (to induce a
target model), but their lower bound only applies when differential privacy is deployed during the
model training process (and hence hurts model utility), which is different from our problem setting.
4	Poisoning Attack with a Target Model
Our new poisoning attack determines a target model and selects poisoning points to achieve that target
model. The target model generation is not our focus and we adopt the heuristic approach proposed
by Koh et al. (2018). For the new poisoning attack, first, we show the algorithm that generates the
poisoning points in Section 4.1 and then prove that the generated poisoning points, once added to the
clean data, can produce a classifier that asymptotically converges to the target classifier in Section 4.2.
4.1	Model-Targeted Poisoning with Online Learning
The main idea of our model-targeted poisoning attack, as outlined in Algorithm 1, is to sequentially
add a point into the training set that have maximum loss difference between the intermediate model
obtained so far and the target model, and by training models on the updated training set, we actually
minimize the gap in the loss of the intermediate classifier and the target classifier. Repeating the
process then eventually generates classifiers that have similar loss distribution as the target classifier.
We show in Section 4.2 why similar loss distribution implies convergence.
Algorithm 1 ModelTargetedPoisoning
Input: Dc, the loss functions (L and l), θp
Output: Dp
1:	Dp = 0
2:	while stop criteria not met do
3:	θt = arg min L(θ; Dc ∪ Dp)
4：	(x*,y*) = argmaxχ×γ l(θt; x,y) — 1(9p； x,y)
5：	Dp = Dp ∪{(x*,y*)}
6:	end while
7:	return Dp
Algorithm 1 requires the input of clean training set Dc, the Loss function (L for set of points and
l for individual point) and the target model θp . The output from Algorithm 1 will be the set of
poisoning points Dp . The algorithm is simple: first, adversaries train the intermediate model θt on
the mixture of clean and poisoning points Dc ∪ Dp with Dp an empty set in first iteration (Line 3).
The adversary then searches for the point that maximizes the loss difference between θt and θp (Line
4). After the point of maximum loss difference is found, it is added to the poisoning set Dp (Line
5). The whole process repeats until the stop condition is satisfied in Line 2. The stop condition is
flexible and it can take various forms: 1) adversary has a budget T on number of poisoning points,
and the algorithm halts when the algorithm runs for T iterations; 2) the intermediate classifier θt is
closer to the target classifier (than a preset threshold ) in terms of the maximum loss difference, and
more details regarding this distance metric will be introduced in Section 4.2; 3) adversary has some
requirement on the accuracy and the algorithm terminates when θt satisfies the accuracy requirement.
Since we focus on producing a classifier close to the target model, we adopt the second stop criterion
that measures the distance with respect to the maximum loss difference, and report results based on
this criterion in Section 5.
A nice property of Algorithm 1 is that the classifier θatk trained on Dc∪Dp is close to the target model
θp and asymptotically converges to θp . Details of the convergence will be shown in the next section.
The algorithm may appear to be slow, particularly for larger models due to requirement of repeatedly
training a model in line 3. However, this is not an issue. First, as will be shown in next section, the
algorithm is an online optimization process and line 3 corresponds to solving the online optimization
4
Under review as a conference paper at ICLR 2021
problem exactly. However, people often use the very efficient online gradient descent method to
approximately solve the problem and its asymptotic performance is the same (Shalev-Shwartz, 2012).
Second, if We solve the optimization problem exactly, We can add multiple copies of (x*,y*) into
Dp each time. This reduces the overall iteration number, and hence reduces the number of times
retraining models. The proof of convergence Will be similar. For simplicity in interpreting the results,
we do not use this in our experiments and add only one copy of (χ*,y*) each iteration. However, we
also tested the performance by adding two copies of (x*, y*) and find that the attack results are nearly
the same while the efficiency is improved significantly. For example, for the experiments we tried on
MNIST 1-7 dataset, by adding 2 copies of points, with same number of poisoning points, the attack
success rate decreases at most by 0.7% while the execution time is reduced approximately by half.
4.2	Convergence of Our Poisoning Attack
Before proving the convergence of Algorithm 1, we need to measure the distance of the model θatk
trained on Dc ∪ Dp to the target model θp . First, we define a general closeness measure based on
their prediction performance which we will use to state our convergence theorem:
Definition 1 (Loss-based distance and -close). For two models θ1 and θ2, a space X × Y anda loss
function l(θ; x, y), we define loss-based distance Dl,X,Y : Θ × Θ → R as
Dl,X,Y (θ1, θ2) =	max	l(θ1; x, y) - l(θ2; x, y),
(x,y)∈X ×Y
and we say model θ1 is -close to model θ2 when the loss-based distance from θ1 to θ2 is upper
bounded by .
Why is loss-based distance a meaningful notion of closeness? We argue that this notion captures
the “behavorial” distance between two models. Namely, if θ1 is -close (as measured by loss-based
distance) to θ2 and vice versa, then θ1 and θ2 would have almost equal loss on all the points, meaning
that they have almost the same behavior across all the space. Note that our general definition of loss-
based distance does not have the symmetry property of metrics and hence is not a metric. However, it
has some other properties of metrics in the space of attainable models. For example, if some model θ
is attainable using ERM, no model could have negative distance to it. To further show the value of this
distance notion, in Appendix B we demonstrate an O(C) upper bound on the 'ι-norm of difference
between two models that are -close with respect to loss-based distance for the special case of Hinge
loss. For Hinge loss, it also satisfies the bi-directional closeness, that is if θ1 is C-close to θ2, then θ2
is O(C)-close to θ (details can be found in Corollary B.2.1), and the proof details can be found in
Appendix B. In the rest of the paper, we will use terms C-close or C-closeness to denote that a model
is C away from another model based on the loss-based distance.
Our convergence theorem uses the loss-based distance to establish that the attack of Algorithm 1
converges to the target classifier:
Theorem 4.1. After at most T steps, Algorithm 1 will produce the poisoning set Dp and the classifier
trained on Dc ∪ Dp is C-close to θp, with respect to loss-based distance, Dl,X,Y, for
=α(T) + L(θp; Dc)- L(θc, Dc)
e =	τ∙γ
where, γ is a constant for a given θp and classification task, and α(T) is the regret of the online
algorithm when the loss function used for training is convex.
Remark 1. Online learning algorithms with sublinear regret bound can be applied to show the
convergence. Here, we adopt results from McMahan (2017). Specifically, α(T) is in the order of
O (log T)) and we have c ≤ O( loggT) when the lossfunction is additionally Lipschitz continuous and
the regularizer R(θ) is strongly convex, and C → 0 when T → +∞. α(T) is also in the order of
O(log T) when the loss function used for training is strongly convex and the regularizer is convex.
Proof idea. The full proof of Theorem 4.1 is in Appendix A. Here, we only summarize the high level
proof idea. The key idea is to frame the poisoning problem as an online learning problem. In this
formulation, each step of the online learning problem corresponds to the ith poison point (xi, yi). In
particular, the loss function at iteration i of the online learning problem is set to l(∙; Xi,yi). Then, we
5
Under review as a conference paper at ICLR 2021
show that by defining the parameters of the online learning problem in a careful way, the output of the
follow-the-leader (FTL) algorithm (Shalev-Shwartz, 2012) at iteration i is a model that is identical
to training a model on a dataset consisting of the clean points and the first i - 1 poisoning points.
On the other hand, the way the poisoning points are selected, we can show that at the ith iteration
the maximum loss difference between the target model and the best induced model so far would
be smaller than the regret of the FTL algorithm divided by the number of poisoning points. The
convergence bound of Theorem 4.1 boils down to regret analysis of the algorithm based on the loss
function. Since we are assuming the loss function is convex with a strongly convex regularizer (or a
strongly convex loss function with a convex regularizer), we can show that the regret is bounded by
O(log T) and hence the loss distance between the induced model and the target model converges to 0.
Implications of Theorem 4.1 The theorem says that the loss-based distance of the model trained
on Dc ∪ Dp to the target model correlates to the loss difference between the target model and the
clean model θc (trained on Dc) on Dc , and correlates inversely with the number of poisoning points.
Therefore, it implies 1) if the target classifier θp has lower loss on Dc , then it is easier to achieve the
target model, and 2) with more poisoning points, we get closer to the target classifier and our attack
will be more effective. The theorem also justifies the motivation behind the heuristic method in Koh
et al. (2018) to select a target classifier with lower loss on clean data. For the indiscriminate attack
scenario, we also improve the heuristic approach by adaptively updating the model and producing
target classifiers with much lower loss on the clean set. This helps to empirically validate our
theorem. Details of the original and improved heuristic approach, and relevant experiments are in
Appendix D.1.
4.3	Lower Bound on the Number of poisoning Points
We first provide the lower bound on number of poisoning points required for producing the target
classifier in addition only setting (Theorem 4.2), and then explain how the lower bound estimation
can be incorporated into Algorithm 1. The intuition behind the theorem below is, when the number of
poisoning points added to the clean training set is smaller than the lower bound, there always exists a
classifier θ with lower loss compared to θp and hence the target classifier cannot be attained.
Theorem 4.2 (Lower Bound). Given a target classifier θp, to reproduce θp by adding the poisoning
set Dp into Dc, the number of poisoning points |Dp| cannot be lower than
Su z(θ)=	L(θp; DC)- L(θ; DC) + NCκ(R(θp) - R(θ))
siθp2	supχ,y (i(θ; χ, y) - i(θp; χ, y)) + Cr(R(Θ) - R(θp))'
Corollary 4.2.1. If we further assume bi-directional closeness in the loss-based distance, we can
also derive the lower bound on number of poisoning points needed to induce models that are -close
to the target model. More precisely, if θι being E-close to θ2 implies that θ2 is also k ∙ E close to θι,
then we have,
sup z0 (θ)
θ
L(θp; DC) - L(θ; DC)- NCR ∙ R* - NkE
suPx,y (l(θ; χ, y) - l(θp; χ, y)) + CR ∙ R* + ke
where R* is an upper bound on the nonnegative regularizer R(θ).
The formula for the lower bound in Theorem 4.2 (and also the lower bound in Corollary 4.2.1) can be
easily incorporated into Algorithm 1 to obtain tighter theoretical lower bound. We simply need to
check all of the intermediate classifier θt produced during the attack process and replace θ with θt ,
and the lower bound can be computed for the pair of θt and θp . Algorithm 1 then additionally returns
the lower bound, which is the highest lower bound computed from our poisoning procedure.
5 Experiments
We first describe our experimental setup regarding the datasets, models, attacks and target classifiers.
Next, we present the experimental results by showing the convergence of Algorithm 1, the comparison
of attack success rates to state-of-the-art poisoning attack, and the theoretical lower bound for inducing
a given target classifier and its gap to the number of poisoning points used by our attack.
6
Under review as a conference paper at ICLR 2021
We are most interested in subpopulation attacks, since they correspond to the more realistic attacker
goal of impacting the classifier outputs for a targeted subpopulation. Therefore, in the main body, we
introduce the results of SVM model on the Adult dataset (Dua & Graff, 2017) in the subpopulation
poisoning scenario. For completeness, We also evaluate our attack on SVM model on MNIST 1-7
dataset in the indiscriminate poisoning scenario but defer details on those experiments to Appendix D.
Our findings for the indiscriminate attacks are that the attack gradually and consistently converges to
the target model in terms of the maximum loss difference and the Euclidean distance to the target,
With attack success rates that are comparable to the state-of-the-art attack (unlike the subpopulation
attacks, Where our attack produces superior results). To further verify the universal effectiveness of
our attack, We also evaluate our attack on additional dataset (Dogfish) and model (logistic regression),
and more details can be found in Appendix F.
Dataset, Model and Attacks. For the subpopulation attack experiments, We use the Adult
dataset (Dua & Graff, 2017). This dataset Was used for evaluation by the first subpopulation attack
paper (Jagielski et al., 2019). We doWnsampled the Adult dataset to ensure it is class-balanced and
We ended up having 15,682 training and 7,692 test examples. We conduct experiments on linear
SVM model and compare our model-targeted poisoning attack in Algorithm 1 to the state-of-the-art
KKT attack (Koh et al., 2018). We do not include the model-targeted attack from Mei & Zhu (2015b)
because it underperforms the KKT attack (Koh et al., 2018). We also do not include objective-driven
attacks because our main goal here is to evaluate hoW Well our attack approaches a given target model,
across a range of target models. Model-targeted attacks can be compared to objective-driven attacks
With regards to a given attacker objective by choosing the target model in a careful Way. We shoW
some heuristics of choosing such target models and comparison to some objective-driven attacks in
Appendix E.
Both our attack and the KKT attack take as input a target classifier and the original training data,
and output a set of poisoning points selected With the goal that the induced classifier is as close as
possible to the target classifier. We compare the effectiveness of the attacks in selecting poisoning
points that converge to a given target classifier by testing the attacks using the same target model.
The KKT attack requires a target number of poisoning points as an input While our attack is more
flexible and can either take a target number of poisoning points or a threshold for -close distance to
the target model. Since We do not knoW the number of poisoning points needed to reach some attacker
goal in advance for the KKT attack, We first run our attack and produce a classifier that satisfies the
selected -close distance threshold. The loss function is set as the hinge loss since We target an SVM
model in our experiments and We set = 0.01 for all these experiments. Then, We use the size of the
poisoning set returned from our attack (denoted by np) as the input to the KKT attack for the target
number of poisons needed. We also compare the tWo attacks With varying numbers of poisoning
points up to np . For the KKT attack, its entire optimization process must be rerun Whenever the
target number of poisoning points changes. Hence, it is infeasible to evaluate the KKT attack on
many different poisoning set sizes. In our experiments, We run the KKT attack five poisoning set
sizes: 0.2 ∙ np, 0.4 ∙ np, 0.6 ∙ np, 0.8 ∙ np, and n?. In contrast, We simply run our attack for iterations
up to the maximum number of poisoning points, collecting a data point for iteration up to np .
Subpopulations. We identify the subpopulations for the Adult dataset using k-means clustering
techniques (ClusterMatch (Jagielski et al., 2019)) to obtain different clusters (k = 20 in our case).
For each cluster, We select instances With label “<=50K” to form the subpopulation (indicating all
instances in the subpopulation are in loW income group). This Way of defining subpopulation is
rather arbitrary (in constrast to a more likely attack goal Which Would select subpopulations based on
demographic characteristics), but enables us to simplify the analysis. From the 20 subpopulations
obtained, We select three subpopulations With the highest test accuracy on the clean model and they
all have 100% test accuracy, indicating all instances in these subpopulations are correctly classified
as loW income. This enables us to use “attack success rate” and “accuracy” Without any ambiguity
on the subpopulation—for each of our subpopulations, all instances are originally classified as loW
income, and the simulated attacker’s goal is to have them classified as high income.
For each subpopulation, We use the heuristic approach from Koh et al. (2018) to generate a target
classifier that has 0% accuracy (100% attacker success) on the subpopulation, indicating that all
subpopulation instances are noW classified as high income.
7
Under review as a conference paper at ICLR 2021
(a) Max Loss Difference to Target
0 8 6 4 2 0
Lo.o.o.o.o.
o□up⅛Iα up。PIlɔnw
(b) Euclidean Distance to Target
Figure 1:	Attack convergence (results shown are for the first subpopulation, Cluster 0). The maximum
number of poisons is set using the 0.01-close threshold to target classifier.
(a) Cluster 0
(b) Cluster 1
(c) Cluster 2
Figure 2:	Test accuracy for each subpopulation with classifiers induced by poisoning points obtained
from our attack and the KKT attack.
	Cluster 0	Cluster 1	Cluster 2
Poisoning Points	1866	2097	2163.3 ± 2.5
Lower Bound	1666.7 ± 2.4	1831.4 ± 5.0	1863.0 ± 9.2
Table 1: Poisoning points needed to achieve target classifiers induced from our attack. Top row means
number of poisoning points used by our attack. Bottom row means the lower bound computed from
Theorem 4.2 for the induced classifiers. All results are averaged over 4 runs, integer value in the cell
means we get exactly same value for 4 runs and others are shown with the average and standard error.
Convergence. Figure 1 shows the convergence of Algorithm 1 using both maximum loss difference
and Euclidean distance to the target. The maximum number of poisons (np) for the experiments is
obtained when the classifier from Algorithm 1 is 0.01-close to the target classifier. Our attack steadily
reduces the maximum loss difference and Euclidean distance to the target model, in contrast to the
KKT attack which does not seem to converge towards the target model reliably. Concretely, at the
maximum number of poisons in Figure 1, both the maximum loss difference and Euclidean distance
of our attack (to the target) is less than 2% of the corresponding distances of the KKT attack.
Attack Success. Next, we compare the classifiers induced by the two attacks in terms of the attacker’s
goal of reducing the test accuracy on the subpopulation. Figure 2 shows the accuracy results for the
three subpopulations. For each test, the maximum number of poisoning points is obtained by running
our attack with a target of 0.01-closeness (in loss-based distance). For the three subpopulations, at
the maximum number of poisons, our attack is much more successful than the KKT attack—the
induced classifiers have 0.5% accuracy compared to 15.4% accuracy for KKT on subpopulation 1,
0.0% compared to 6.9% on subpoulation 2, and 0.3% compared to 20.1% on subpoplation 3.
Near Optimality of Our Attack. In order to show the optimality of our attack, we calculate a lower
bound on the number of poisoning points needed to induce the model that is induced by the poisoning
points that are found by our attack. We calculate this lower bound on the number of poisons using
Theorem 4.2 (details in Section 4.3). Note that Theorem 4.2 provides a valid lower bound based
on any intermediate model. In order to get a lower bound on the number of poisoning points, we
8
Under review as a conference paper at ICLR 2021
only need to use Theorem 4.2 on the encountered intermediate models and report the best one. We
do this by running Algorithm 1 using the induced model (and not the previous target model) as the
target model, terminating when the induced classifier is 0.01-close to the given target model. We
then consider all the intermediate classifiers that the algorithm induced across the iterations. Our
calculated lower bound in Table 1 shows that the gap between the lower bound and the number of
used poison points is relatively small. This means our attack is nearly optimal in terms of minimizing
the number of poisoning points needed to induce the target classifier.
6 Conclusion and Discussion
We propose a general poisoning framework with provable guarantees to reach any achievable target
classifier, along with a lower bound on the number of poisoning points needed. Our attack is a
generic tool that first captures the goal of adversary as a target model, and then focuses on the power
of attacks to induce that model. This separation enables future work to explore the effectiveness
of poisoning attacks corresponding to different adversarial goals. Our framework also applies in
scenarios where adversaries first remove points and then add new points into the training set. We
have not considered defenses in this work, and it is important to study the effectiveness of our attack
against data poisoning defenses. Defenses may be designed the limit the search space of the points
with maximum loss difference and increasing the number of poisoning points needed.
One limitation of our framework is the requirement in the concavity of the difference of loss functions
to efficiently search for its maximum value. However, our approach might still be effective in these
cases by using local optimization techniques to search for poisoning points with (approximate)
maximum loss difference and we have demonstrated this in the case of logistic loss. More formally,
if the approximate maximum loss difference l found from local optimization techniques is within
a constant factor from the globally optimal value l (ι.e., l ≥ ɑl*, 0 < α < 1), then We still enjoy
similar convergence guarantees. It is important to note that the convergence property of our attack
holds (With strongly convex regularizer) for any Lipschitz and convex loss function, and does not
require the loss difference to be concave. The theoretical guarantees in the paper do not apply to
non-convex models, although it might be possible to empirically apply our attack to these models.
Incorporating online learning for non-convex functions might be one possible path to extend our
theoretical analysis into non-convex settings.
References
Battista Biggio, Blaine Nelson, and Pavel Laskov. Support vector machines under adversarial label
noise. In Asian Conference on Machine Learning, 2011.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines.
arXiv preprint arXiv:1206.6389, 2012.
Steven Diamond and Stephen Boyd. Cvxpy: A python-embedded modeling language for convex
optimization. The Journal of Machine Learning Research ,17(1):2909-2913, 2016.
Dheeru Dua and Casey Graff. UCI Machine Learning Repository, 2017. URL http://archive.
ics.uci.edu/ml.
SanghyUn Hong, Varun Chandrasekaran, Yigitcan Kaya, Tudor Dumitrag, and Nicolas Papernot.
On the effectiveness of mitigating data poisoning attacks With gradient shaping. arXiv preprint
arXiv:2002.11497, 2020.
W Ronny Huang, Jonas Geiping, Liam FoWl, Gavin Taylor, and Tom Goldstein. Metapoison: Practical
general-purpose clean-label data poisoning. arXiv preprint arXiv:2004.00225, 2020.
Gurobi Optimization Inc. Gurobi optimizer reference manual, 2020. URL https://www.gurobi.
com/documentation/9.1/refman/index.html.
MattheW Jagielski, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li.
Manipulating machine learning: Poisoning attacks and countermeasures for regression learning. In
2018 IEEE Symposium on Security and Privacy (SP), pp. 19-35. IEEE, 2018.
9
Under review as a conference paper at ICLR 2021
Matthew Jagielski, Paul Hand, and Alina Oprea. Subpopulation data poisoning attacks. In NeurIPS
2019 Workshop on Robust AI in Financial Services, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
International Conference on Machine Learning, 2017.
Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data
sanitization defenses. arXiv preprint arXiv:1811.00741, 2018.
Bo Li, Yining Wang, Aarti Singh, and Yevgeniy Vorobeychik. Data poisoning attacks on factorization-
based collaborative filtering. In NeurIPS, 2016.
Daniel Lowd and Christopher Meek. Adversarial learning. In ACM SIGKDD Conference on
Knowledge Discovery in Data Mining, 2005.
Yuzhe Ma, Xiaojin Zhu, and Justin Hsu. Data poisoning against differentially-private learners:
Attacks and defenses. arXiv preprint arXiv:1903.09860, 2019.
Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. Learning under p-tampering
attacks. arXiv preprint arXiv:1711.03707, 2017.
Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. The curse of concentration in
robust learning: Evasion and poisoning attacks from concentration of measure. In AAAI Conference
on Artificial Intelligence, 2019a.
Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Universal multi-party poisoning
attacks. In International Conference on Machine Learning, 2019b.
H Brendan McMahan. A survey of algorithms and analysis for adaptive online learning. The Journal
ofMachine Learning Research, 18(1):3117-3166, 2017.
Shike Mei and Xiaojin Zhu. The security of Latent Dirichlet Allocation. In Artificial Intelligence and
Statistics, pp. 681-689, 2015a.
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on
machine learners. In AAAI Conference on Artificial Intelligence, 2015b.
Luis Munoz-Gonzdlez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. In ACM Workshop on Artificial Intelligence and Security, 2017.
Luis Munoz-Gonzdlez, Bjarne Pfitzner, Matteo Russo, Javier Carnerero-Cano, and Emil C Lupu.
Poisoning attacks with generative adversarial nets. arXiv preprint arXiv:1906.07773, 2019.
Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In
NeurIPS, 2018.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in
Machine Learning, 4(2):107-194, 2012.
David Solans, Battista Biggio, and Carlos Castillo. Poisoning attacks on algorithmic fairness. arXiv
preprint arXiv:2004.07401, 2020.
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks.
In NeurIPS, 2017.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
10
Under review as a conference paper at ICLR 2021
Yizhen Wang and Kamalika Chaudhuri. Data poisoning attacks against online learning. arXiv
preprint arXiv:1808.08994, 2018.
Han Xiao, Huang Xiao, and Claudia Eckert. Adversarial label flips attack on support vector machines.
In European Conference on Artificial Intelligence, 2012.
Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, and Fabio Roli. Support
vector machines under adversarial label contamination. NeurocomPuting, 160:53-62, 2015.
Chaofei Yang, Qing Wu, Hai Li, and Yiran Chen. Generative poisoning attack method against neural
networks. arXiv PrePrint arXiv:1703.01340, 2017.
Chen Zhu, W Ronny Huang, Ali Shafahi, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom
Goldstein. Transferable clean-label poisoning attacks on deep neural nets. arXiv PrePrint
arXiv:1905.05897, 2019.
11
Under review as a conference paper at ICLR 2021
A	Proofs
In this section, we provide the proofs of the main theorems shown in this paper. For convenience, we
restate all the theorems below while also referencing to the main paper.
Before proving the main theorem, we introduce two new definitions and several lemmas to assist with
the proof.
Definition 2 (Attainable models). We say θ is CR-attainable with respect to loss function l and
regularization function R if there exists a training set D such that
θ = arg min τ1- ∙ L(θ; D) + CR ∙ R(θ)
θ∈Θ	|D|
Lemma A.1. Let θ1 and θ2 be two CR-attainable parameters for some CR > 0 such that R(θ1) >
R(θ2). Then,
SUp (l(θ2; X, y) - l(θι; X, y))∕(R(θι) - R(θ2)) > Cr.
x,y
Proof. Consider any attainable pairs of (θ1, θ2) such that R(θ1) > R(θ2) and let D1 to be training
set that the training algorithm produces the unique minimizer θ1. Namely,
θι = arg min pɪ-r ∙ L(θ; Di) + CR ∙ R(θ)
θ |D1 |
Since θ1 minimizes the total loss on D1 uniquely, we have
l⅛ L(θ2;DI) + CR ∙ R(θ2) > l⅛ L(θ1;DI) + CR ∙ R(θi)
By rearranging the above inequality and by an averaging argument, we have
SUp (l(θ2;x,y)-则；x,y)) ≥ 高L(θ2;DI)-高L(θi;DI) > CR ∙ (RM)- R(θ2)).
Now since R(θ1) > R(θ2) we have
sup (l(θ2;χ,y) - l(θi;χ,y))∕(R(θι) - R(θ2)) > CR.
x,y
□
Lemma A.2. Let F be the family of all CR-attainable models. For any θ1 ∈ F, there is a constant γ
where for all θ2 ∈ F we have
sup (l(θ2; χ,y) - l(θi; χ,y)) + CR(R(θ2) - R(θι)) >γ ∙ SUp (l(θ2; χ,y) - l(θi; χ,y))
x,y	x,y
where γ is a positive constant related to, θ1, CR and other model parameters (fixed for a given
classification task).
Proof. We prove the lemma for γ = 1- CR∕C for
C
inf
θ2∈F
s.t. R(θ1)>R(θ2)
sup(l(θ2; X, y) - l(θ1; X, y))∕(R(θ1) -R(θ2))
x,y
First, note that by Lemma A.1 we have
C> CR ≥ 0.	(2)
which implies γ is positive. Now we consider two subcases based on the sign of R(θ2) - R(θ1):
12
Under review as a conference paper at ICLR 2021
Case 1:	R(θ2) - R(θ1) ≥ 0. In this case the inequality is straightforward:
sup (l(θ2; χ,y) - l(θι; χ,y)) + CR ∙ (R(θ2) - R(θι)) ≥ Sup (l(θ2; χ,y) - l(θι; χ,y))
x,y	x,y
> (1 - CRIC) ∙ Sup (i(θ2;χ,y) - i(θι;χ,y)),
x,y
where the last inequality is based on equation 2.
Case 2:	R(θ2) - R(θ1) < 0. From the definition of C we have
r(θ)- r(θ)≤ supχ,y (lQ； x，y)- W x,y力
C
Equivalently, we can say
R(θ ) - R(θ ) ≥ - supχ,y (l(&；舁期y - l(θ1 ； x,y力
C
Replacing R(θ2 ) - R(θ1 ) with the lower bound above completes the proof, namely
sup (l(θ2; χ,y) -l(θι; χ,y)) + Cr(R(Θ?) - R(θι)) ≥ (1 - Cr/C) ∙Sup (l(θ2; χ,y)-l(θι; χ,y))∙
x,y	x,y
□
With Definition 2 and the lemmas, we are ready to prove Theorem 4.1 (restating Theorem 4.1, from
Section 4.2):
Theorem 4.1. After at most T steps, Algorithm 1 will produce the poisoning set Dp and the classifier
trained on Dc ∪ Dp is -close to θp, with respect to loss-based distance, Dl,X,Y, for
=α(T) + L(θp; Dc)- L(θc, Dc)
e =	τ∙γ
where, γ is a constant for a given θp and classification task, and α(T) is the regret of the online
algorithm when the loss function used for training is convex.
The goal of the adversary is to get -close to θp (in terms of the loss-based distance) by injecting
(potentially few) number of poisoned training data. The algorithm is in essence an online learning
problem and we transform Algorithm 1 into the form of standard online learning problem. Specifically,
we adopt the follow the leader (FTL) framework to describe Algorithm 1 in the language of standard
online learning problem. We first describe the online learning setting considered in this paper and the
notion of the regret.
Definition 3. Let L be a class of loss functions, Θ set of possible models, A: (Θ X Ly → Θ an
online learner and S: (Θ ×L)* × Θ → L a strategy for picking loss functions in different rounds
of online learning (adversarial environment in the context of online convex optimization). We use
Regret(A, S, T) to denote the regret of A against S, in T rounds. Namely,
TT
Regret(A,S,T)=Xlj(θj)-mθ∈iΘnXlj(θ)
j=0	∈ j=0
where
θi = A((θ0, l0), . . . , (θi-1, li-1)) and Ii = S((θ0, l0), . . . , (θi-1, li-1), θi).
With the online learning problem set up, we proceed to the main proof which first describes Algo-
rithm 1 in the FTL framework.
Proof of Theorem 4.1. The FTL framework proceeds by solving all the functions incurred during the
previous online optimization steps, namely, AFTL((θ0, l0), . . . , (θi, li)) = arg minθ∈Θ Pij=0 li(θ).
13
Under review as a conference paper at ICLR 2021
Next, we describe how we design the ith loss function li in each round of the online optimization.
For the first choice, AFTL chooses a random model θ0 ∈ Θ. In the first round (round 0), Sθp uses the
clean training set Dc and the loss is set as
Sθp (θo)= lo(θ) = L(θ; Dc) + N ∙ Cr ∙ R叫.
According to the FTL framework, AFTL returns model that minimizes the loss on the clean training
set Dc using the structural empirical risk minimization. For the subsequent iterations (i ≥ 1), the
loss functions is defined as, given the latest model θi, Sg? first finds (χ*,y*) that maximizes the loss
difference between θi and a target model θp . Namely,
(x↑,yt) = arg max I® x,y) - l(θp; x,y)
(x,y)
and then chooses the ith loss function as follows:
Sθp ((θ0,l0),...,(θi-1,li-1),θi) = li(θ) = l(θ; x*,y*) + Cr ∙ R(θ).
Now we will see how FTL framework behaves when working on these loss functions at different
iterations. We use Dp to denote the set {(x；,y：),..., (x'y；)}. We have
i-1
θi = AFTL((θ0, l0), . . . , (θi-1, li-1)) = argmin	lj (θ)
θ∈Θ j=0
=arg min L(θ; Dc) + N ∙ CR ∙ R(θ)
θ∈Θ
i-1
+ Xi(θ;x*,y*) + Cr ∙ R(θ)
j=1
=argmin L(θ; Dc ∪ DpT) + (N + i - 1) ∙ CR ∙ R(θ)
θ∈Θ	p
1
=arg min G -Cj 刀⑹ Dc ∪ Dp ) + CR ∙尺⑹
θ∈Θ	|Dc ∪ Dp |
This means that AFTL algorithm, at each step, trains a new model over the combination of clean
data and poison data so far (i - 1 number of poisons). Now we want to see what is the transla-
tion of the Regret(AFTL, Sθp , T ). If we can prove an upper bound on regret, namely if we show
Regret(AFTL, Sθp, T) ≤ α(T) for some function α, then we have
TTT	T
Xlj(θj)-Xlj(θp)≤Xlj(θj)-mθ∈iΘnXlj(θ)≤α(T)
which implies
TT
X Ij (θj) - X Ij(θp) = L(θc; Dc)- L(θp; Dc) + N ∙ Cr ∙ (R(θc) - R(θp))
j=0	j=0
TT
+Xlj(θj)-Xlj(θp)
j=1	j=1
=L(θc; Dc)- L(θp; Dc) + N ∙ Cr ∙ (R(θc) - R(θp))
T
+ X [mχaχ (i(θj; χ,y) - i(θp; χ,y)) + CR ∙ (R(θj) - R(θp))]
j=1 x,y
≤ α(T)
14
Under review as a conference paper at ICLR 2021
Therefore we have
T
X [max (l(θj; x, y) - l(θp; x, y)} + Cr ∙ (R(θj) - R(θp))] ≤ α(T) + L(θp; Dc)- L(θc; Dc)
j=1 x,y
+ N .CR ∙ (R(θp)- R(θc))
Based on Lemma A.2, we further have
T
X Y ∙ (max l(θj; χ,y) - l(θp; x,y)) ≤ α(T) + L(θp; Dc) - L(θc; Dc)
j=1	x,y
+ N ∙ Cr ∙(R(θp)- R(θc))
Above inequality states that average of the maximum loss difference in all previous rounds is bounded
from above. Therefore, We know that among the T iterations, there exist an iteration j* ∈ [T] (with
lowest maximum loss difference) such that the maximum loss difference of θj* is E-Close to θp with
respect to the loss-based distance where
=α(T) + L(θp; Dc)- L(θc; Dc)+ N ∙ Cr ∙ (R(θp) - R(θc))
C =	T∙γ	.
□
Theorem 4.1 characterizes the dependencies of E on α(T) and the constant term L(θp; Dc) -
L(θc; Dc) + N ∙ Cr ∙ (R(θp) 一 R(θc)). To show the convergence of Algorithm 1, we need to
ensure E → 0 when T → +∞, which implies we need to show a(T) ≤ O(√T). Following remark
(restating Remark 1 in Section 4.2) and its proof shows the desired convergence.
Remark 1. Online learning algorithms with sublinear regret bound can be applied to show the
convergence. Here, we adopt the regret analysis from McMahan (2017). Specifically, α(T) is in the
order of O (log T)) and we have E ≤ O( IoTT) when the loss function is Lipschitz continuous and
the regularizer R(θ) is strongly convex, and E → 0 when T → +∞. α(T) is also in the order of
O(log T) when the loss function used for training is strongly convex and the regularizer is convex.
Our FTL framework formulation can utilize the existing logarithmic regret bound of adaptive FTL
algorithm when the objective functions are strongly convex with respect to some norm k ∙ k, as
illustrated in Section 3.6 in McMahan (2017). For clarity in presentation, we first restate their related
results below.
Setting 1 (Setting 1 in McMahan (2017)). Given a sequence of objective loss functions f1, f2, ..., fi
and a sequence of incremental regularization functions r0, r1, ..., ri we consider an algorithm that
selects the response point based on
θ1 = arg min r0 (θ)
θ∈Rd
i
θi+1 = argminXfj(θ) + rj(θ) + r0(θ), for i = 1, 2, ...
θ∈Rd j=1
We simplify the summation notation with fi:i(θ) = Ej=I f (θ). Assume that r is a ConvexfUnCtion
and satisfy ri(θ) ≥ 0 for i ∈ {0, 1, 2, ...}, against a sequence of convex loss functions fi : Rd →
R ∪ {∞}. Further, letting h0:i = r0:i + f1:i we assume dom h0:i is non-empty. Recalling θi =
arg minθ h0:i-1 (θ), we further assume ∂fi(θi) is non-empty. We denote the dual norm ofa norm
k∙k as k∙k*.
Theorem A.3 (Restatement of Theorem 1 in McMahan (2017)). Consider Setting 1, and suppose
the r are chosen such that r。” + fi：i+i is 1-strongly-convex w.r.t. some norm ∣∣ ∙ ||(力.Ifwe define
the regret of the algorithm with respect to a selected point θ* as
TT
RegretT (θ*, fi) ≡Xfi(θi) - X fi(θ*).
i=1	i=1
15
Under review as a conference paper at ICLR 2021
Then,forany θ* ∈ Rd andfor any T > 0, with gi ∈ ∂fi(θi), we have
RegretT®") ≤ ro：T-i(θ*) + ；kgikfi-i),*
Corollary A.3.1 (Formalization of FTL result in Section 3.6 in McMahan (2017)). In the FTL
framework (no individual regularizer is used in the optimization procedure), suppose each loss
function f is 1-strongly convex w.r.t. a norm ∣∣ ∙ ∣∣, then we have
1 T 1	G2
RegretT(θ ,fi) ≤ 2E iIlgilL ≤ -ɪ(i + logT)
i=1
with IlgiI∣* ≤ G.
Proof. The following proof is a restatement of the proof in Section 3.6 in McMahan (2017). The
proof follows from Theorem A.3. Since we are considering the FTL framework, let ri(θ) = 0 for all
i and define ∣∣θ∣∣(i) = VZiIlθ∣∣. Observe that ho：i (i.e., fi：i) is 1-strongly convex with respect to ∣∣θ∣∣(i)
(Lemma 3 in McMahan (2017)), and we have |网心,* = √i ∣∣θ∣*. Then by applying Theorem A.3,
we have
1T	1T1
RegretT(θ ,fi) ≤ 2X IlgillO* = 2 XIllgilI*
2 i=1	2 i=1 i
Based on the inequality of PiT=1 1/i ≤ 1 + log T and if we further assume IgiI* ≤ G, then we can
have
1 T 1	G2
2∑S i"gik* ≤ ^2^(1 + logT)
i=1
□
Proof of Remark 1. We will prove the logarithmic regret bound in Remark 1 utilizing Corollary A.3.1.
First of all, our online learning process fits into Setting 1. Specifically, we set ri(θ) = 0 for all i.
For fi(θ), when 1 ≤ i ≤ N, we set fi(θ)=得L(θ; Dc) + CR ∙ R(θ) (evenly distributing the term
L(θ; Dc) + N ∙ Cr ∙ R(θ) across N iterations) and when i ≥ N + 1, we set fi(θ) = li-N(θ). Details
of li can be referred from the proof of Theorem 4.1. Therefore, fi is 1-strongly convex with respect to
a norm ∣∣ ∙ ∣∣ (the norm is determined by the regularizer R(θ) and Cr). Further, l0"(θ) = fi：N+i(θ).
In addition, the assumption that dom h0:i is non-empty in Setting 1 means when ifwe train a classifier
on the poisoned data set, we can always return a model and hence the assumption is satisfied. The
assumption of the existence of subgradient ∂fi(θi) in Setting 1 is also satisfied by the poisoning
attack scenario.
The logarithmic regret of Regret(AFTL, Sθp , T) of our algorithm then follows from the result of
RegretT(θ*,fi) in Corollary A.3.1. Specifically, lo:i(θ) = fi：N +i(θ) is 1-strongly convex to norm
I ∙ ∣∣i = NN + iI ∙ I and since we assume the loss function is G-Lipschitz, we have Ilgil∣* ≤ G.
Therefore, we have the logarithmic regret bound as:
1 T 1	1 T 1	G2
Regret(AFTL, Sθp,T) ≤	α(T )	= - E i— ∣gi∣*	≤ - £ 彳 ∣gi∣2	≤ -y(1+log T)	≤ O(log T).
i=i	i=i
□
We next provide the proof of the certified lower bound (restating Theorem 4.2 from Section 4.3):
16
Under review as a conference paper at ICLR 2021
Theorem 4.2. Given a target classifier θp, to reproduce θp by adding the poisoning set Dp into Dc,
the number of poisoning points |Dp| cannot be lower than
Su z(θ)=	L(θp; DC)- L(θ; DC) + NCκ(R(θp) - R(θ))
siθp2	supχ,y (i(θ; χ, y) - i(θp; χ, y)) + Cr(R(Θ) - R(θp))'
The main intuition behind the theorem is, when the the number of poisoning points added to the
clean training set is lower than the certified lower bound, for structural empirical risk minimization
problem (shown in equation 1 in the main paper), then target classifier will always have higher loss
than another classifier and hence cannot be achieved.
Proof. We first show that for all models θ, we can derive a lower bound on the number of poison
points required to get θp . Then since these lower bounds all hold, we can take the maximum over all
of them and get a valid lower bound. We first show that for any model θ, the minimum number of
poisoning points cannot be lower than
=	L(θp; DC)- L(θ; DC) + NCr(R(Θp)- R(θ))
支	supχ,y (i(θ; χ, y) - i(θp; χ, y)) + Cr(R(Θ) - R(θp)).
Let us denote the point corresponding to the supremum of the loss difference between θ and θp as
(x*, y*) 2. Namely, l(θ; x*, y*) 一 l(θp; x*, y*) = suPχ,y (l(θ; x, y) 一 l(θp; x, y)). Now suppose We
can obtain θp with lower number of poisoning points z < z(θ). Assume there is a poisoning set DP
with size Z such that when added to DC would result in θp. We have
Sup (l(θ; x,y) - l(θp; x,y)) ≥	1	L(θ; DC ∪Dp)- ∣n 1 n , L(θp; DC ∪Dp)
x,y	|DC ∪ Dp |	|DC ∪ Dp |
>Cr ∙ (R(θp)- R(θ)),
implying SUpx,y (l(θ; x, y) - l(θp; x, y)) + CR ∙ (R(θ) - R(θp)) > 0. Based on the assumption that
z < z(θ), and the fact that SUpx,y (l(θ; x, y) - l(θp; x, y)) + CR ∙ (R(θ) - R(θp)) > 0, we have
Z ∙ (l(θ; x*,y*)- l(θp; x*,y*) + CR(R(θ) - R(θp)))
< z(θ) ∙ (l(θ; x*,y*) - l(θp; x*,y*) + Cr(R(Θ) - R(θp)))
=L(θp; DC)- L(θ; DC) + NCr(R(Θp) - R(θ)).
where the equality is based on the definition of z(θ). On the other hand, by definition of (x*,y*) for
any DP of size z, we have
L(θ; Dp) - L(θp, Dp) + z ∙ (Cr ∙ R(θ) - CR ∙ R(θp))
≤ Z ∙ (I(θ;x*,y*)- I(θp;x*,y*) + Cr(R(Θ) - R(θp)))∙
The above two inequalities imply that for any set DP with size Z we have
∣DC⅛∣LGDC ∪ DP) + CR ∙R阴 < ∣DC⅛∣L(θp;DC ∪ DP) + CR ∙阳甸.
which indicates that adding Dp poisoning points into the training set DC, the model θ has lower loss
compared to θp , which is a contradiction to the assumption that θp has lowest loss on DC ∪ Dp and
can be achieved. Now, since θp needs to have lower loss on DC ∪ Dp compared to any classifier
θ ∈ Θ, the best lower bound is the supremum over all models in the model space Θ.	□
Corollary 4.2.1. If we further assume bi-directional closeness in the loss-based distance, we can
also derive the lower bound on number of poisoning points needed to induce models that are -close
to the target model. More precisely, if θι being E-close to θ2 implies that θ2 is also k ∙ E close to θι,
then we have,
su J/)=	L(θp; DC)- L(θ; DC)- NCR ∙ R* - NkE
siθp 2	supx,y (i(θ; x, y) - i(θp; x, y)) + CR ∙ R* + kE
where R* is an upper bound on the nonnegative regularizer R(θ).
2In practice, the data space X isa closed convex set and hence, we can find (x*, y*) using convex optimization.
In other words, as we saw in experiments, calculating the lower bound is possible in practical scenarios.
17
Under review as a conference paper at ICLR 2021
Proof of Corollary 4.2.1. The lower bound for all -close models to the target classifier is given
exactly as follows:
inf sup (z(θ,θ')二	" Dc-L仇 Dc) + NC (RSJ)	!,
kθ0-θpkDl,χ ,Y ≤e θ I	SUPχ,y (l(θ; x,y) - l(θ ; x,y)) + CR(R(θ) - R(θ )) J
where inf kθ0 -θ k ≤ denotes θ0 is -close to θp in the loss-based distance. However, the
formulation above is a min-max optimization problem and hard to analytically compute the lower
bound (by plugging the lower bound formula into Algorithm 1. Therefore, we need to make several
relaxations such that the lower bound is computable. For any model θ0 that is -close to θp , based on
the bi-directional assumption, then θp is k-close to θ0. Therefore we have,
L(θ0; Dc)-L(θ; Dc) =	L(θ0; Dc)-L(θp; Dc)+L(θp; Dc)-L(θ; Dc) ≥ -Nk+L(θp; Dc)-L(θ; Dc)
and
sup (l(θ;χ,y) - l(θ ,χ,y)) =SUp (l(θ;χ,y) - l(θp,χ,y)) + sup (l(θp,χ,y) - l(θ ;χ,y))
x,y	x,y	x,y
≤ sup l(θ; x, y) - l(θp, x, y) + k
x,y
and the inequalities are all based on the definition of θp being k-close to θ0.
Plugging the above inequalities into the formula of supθ,θ0 for model θ0, and with the assumption
that 0 ≤ R(θ) ≤ R*, ∀θ ∈ Θ, We immediately have
sup z(θ,θ0) ≥ sup L(θpF-L(θ; Dc- * + NCR(R(θ0) - Rθ)
θ	θ supχ,y (l(θ; x,y) - l(θp; χ,y)) - k + Cr(R(Θ) - R(θ ))
≥ su	L(θp; De)- L(θ; De)- Nk - NCR ∙ R*
一θ <supχ,y (l(θ; x,y) - l(θp; x, y)) - he + CR ∙ R*
z0(θ) .
Since the inequality holds for any θ0, We have
inf sup z(θ, θ0) ≥ sup z0 (θ)
kθ0 -θp kDl,X,Y ≤ θ	θ
and hence z0(θ) is a valid loWer bound.
□
Remark 2 (Improving Results in Corollary 4.2.1). Assuming 0 ≤ R(θ) ≤ R* is not a strong
assumption and actually can be satisfied by many common convex models. For example, for SVM
model with '2-regularizer (in fact, applies to any regularizer R(θ) with R(0) = 0), we have
R(θ) ≤ d； and hence R* ≤ ɑk. Moreover, we can further tighten the lower bound by better
bounding the term R(θ0) - R(θ). Specifically, R(θ0) - R(θ) = R(θ0) - R(θp) + R(θp) - R(θ) and
we only need to have a tighter upper and lower bounds on R(θ0) - R(θp) utilizing some special
properties of the loss functions. For the constant kin the bi-directional closeness, we can also
compute its value for some specific loss functions. For example, for Hinge loss, we can compute the
value based on Corollary B.2.1 in Appendix B.
B	Relating closeness of loss-based distance to closeness of
PARAMETERS
In theorem beloW, We shoW hoW one can relate the notion of e-closeness in Definition 1 in the main
paper to closeness of parameters in the specific setting of hinge loss. We use this just as an example
to shoW that our notion of e-closeness can be tightly related to the closeness of the models.
18
Under review as a conference paper at ICLR 2021
Theorem B.1. Consider the hinge loss function l(θ; x, y) = max(1 一 y ∙(x, θ∖, 0) for θ ∈ Rd and
x ∈ Rd and y ∈ {-1, +1}. For θ, θ0 ∈ Rd such that kθk1 ≤ r and kθ0 k1 ≤ r, if θ is -close to θ0 in
the loss-based distance, then, kθ — θ0∣∣ι ≤ r ∙匕
Remark 3. In Theorem B.1 above with '2 -regularizer, an upper bound on the '1 -norm of θ and θ0 is
，d/CR. however, the models that we care about in practice usually have smaller norms.
Remark 3 can be obtained by plugging 0 ∈ Rd and compare the resulting (regularized) optimization
loss to the model θ* that minimizes the model loss.
ProofofTheorem B.1. We construct a point x* as follows:
* = (- 1, ifOi >θi,i ∈ [d]
Xi = l+1 ifθi ≤ θ0,i ∈ [d]
Thenwehave	hθ — O0,X* i = 1 ∙kθ RI	⑶
r
Since kθk1 ≤ r we have
hx*,θi≥-1	(4)
and similarly since kθ0 k1 ≤ r we have
hx*, θ0i ≥ —1.	(5)
Therefore by Inequalities equation 4 and equation 5 we have
l(θ; x*, —1) — l(θ0; x*, —1) = max(1 + hx*, θi, 0) — max(1 + hx*, θ0i, 0) = hθ — θ0, x*i
which by equation 3 implies
l(θ; x*, —1) — l(θ0; x*, —1) = 1 ∙ kθ — θ0k1.	(6)
r
Now since we know that, ∀x ∈ Rd , the loss difference between θ and θ0 is bounded by , the bound
should also hold for the point (x*, —1), meaning that
1 ∙kθ — θ0k1 ≤ e.
r
which completes the proof.	□
Theorem B.2. Consider the hinge loss function l(θ; x, y) = max(1 — y ∙hx, θ∖, 0) for θ ∈ Rd and
x ∈ Rd andy ∈ {—1, +1}. For X = {x ∈ Rd : kxk1 ≤ q} and Y = {—1, +1}, For any two models
θ,θ if kθ — θ k 1 ≤ e, then θ is q ∙ E-close to θ in the loss-based distance. Namely,
D',x,Y(θ, θO) ≤ q ∙ e∙
Proof. For any given θ and θ0, by triangle inequality for maximum, we have
l(θ; x, y) — l(θ ,x,y) = max(1 — y ∙(x, 0)，0) — max(1 — y ∙(x, θ〉，0) ≤ max(0,〈yx, θ — θ)).
Therefore, we have
max l(θ; x, y) — l(θ0; x, y) ≤ max max(0, hyx, θ0 — θi).
(x,y)∈X ×Y	(x,y)∈X ×Y
Our goal is then to obtain an upper bound of O(E) for max(χ,y)∈χ×γ(yx, 00 — θ) when ∣∣θ — θ]∣ι ≤ e.
To maximize hyx, θ0 — θi by choosing x and y, we only need to ensure that sign yxi = sign θi , i ∈ [d].
Therefore, based on the assumption that q ∣x∣ ≤ 1 (i.e., 1 |xi| ≤ 1,i ∈ [d]) we have
dd
(max	-hyx,θ —	θi	= X-|x|i|Oi- Oi|	≤ Xlθi— θi|	= kθ— θkι ≤E,
(x,y)∈X ×Y q	i=1 q	i i=1	i
which concludes the proof.	□
Corollary B.2.1. For Hinge loss, with Theorem B.1 and Theorem B.2, if 0 is E-close to 00, then 00 is
r ∙ q ∙ E-close to θ.
19
Under review as a conference paper at ICLR 2021
C Instantiating Theorem 4.1 for the case of SVM
Here we show how to instantiate Theorem 4.1 for SVM with exact constants instead of the asymptotic
notations. We need to calculate the constant γ to get the exact constant. Imagine the feature
domain is Rd. Now We calculate the constant C as follows. Let iθ = argmi□i∈[d] ∣θ[i]∕θp[i]∣ and
aθ = ∣θ[i3]∕θp[i3]∣. Let x3 ∈ Rd be a point where is equal to 0 everywhere and is equal to 1∕θp党]
on the i* coordinate. We have,
l(θ,xθ, +1) - l(θp,xθ, +1) = l(θ,xθ, +1) ≥ (1 - αθ).
(7)
Now we can calculate C as follows
C
inf	sup(l(θ; x, y) - l(θp; x, y))∕(R(θp) - R(θ))
s.t. R(θp)>R(θ) x,y
≥ inf	(l(θ,xθ, +1)- l(θp,xθ, +1))∕(R(θp) - R(θ))
θ∈F
s.t. R(θp)>R(θ)
(By Inequality 7)
(By definition of αθ)
s.t.
inf	sup
θ∈F	x,y
R(θp)>R(θ)	,
1 — αθ
R(θp) - R(θ)
≥ inf
θ∈F
s.t. R(θp)>R(θ)
1 — αθ
R(θp)(1 - α2)
≥
≥ inf
θ∈F
s.t. R(θp)>R(θ)
1 — αθ
R(θp)(1 - α2)
、	1
≥ 2R(θp)
Therefore Y ≥ 1 一 2 ∙ CR ∙ R(θp). On the other hand, we can also calculate α(T) based on the exact
form given in the proof of Theorem 4.1.
D	Indis criminate Setting Experiments
In this section, we evaluate the attacks in the conventional indiscriminate attack setting, where the
attacker’s goal is just to reduce the overall accuracy of the model.
Datasets and Models. For the indiscriminate attack, we use the MNIST 1-7 dataset, which consists
of the digits 1 and 7 and is commonly used for evaluating indiscriminate poisoning attacks against
binary classification (Steinhardt et al., 2017; Biggio et al., 2012; Xiao et al., 2012). MNIST 1-7
contains 13,007 training and 2,163 test samples. The dataset contains 784 features and all the features
are normalized into range [0, 1]. For completeness, the Adult dataset used for subpopulation attack is
downsampled to form a class-balanced dataset and contains 15682 training data and 7692 test data.
The dataset contains 57 features and the features are also normalized into range [0, 1] (except for the
binary features). All of the processed datasets are included in the supplementary material. We still
adopt linear SVM model in the indiscriminate attack scenario. All of the models for both datasets set
the regularization parameter CR = 0.09. The clean accuracy of SVM model on MNIST 1-7 is 98.9%
and the accuracy on Adult dataset is 78.5%.
Target Classifiers. Accuracy of the clean MNIST 1-7 model has around 1% error rate on the test set.
For our experiment, we aim to generate three target classifiers with overall test errors around 5%,
10% and 15%. To generate target classifiers with desired error rates, we follow the heuristic strategy
proposed by Koh et al. (2018) to generate multiple candidate target classifiers, and then among all
the valid candidate models that satisfy the error rate requirement we choose the one with lowest loss
on the clean training set. Using this approach, the final target classifiers induced have overall test
accuracy of 94.0%, 88.8% and 83.3% respectively. (We describe a better way of finding the target
classifiers in Appendix D.1, but for comparison purposes do not use those in the results here.)
20
Under review as a conference paper at ICLR 2021
(a) Max Loss Difference to Target
Figure 3: Attack convergence (results shown are for the target classifier of error rate 10%). The
maximum number of poisons is set using the 0.1-close threshold to target classifier
Num of Poisons
(b) Euclidean Distance to Target
Num of Poisons
(a) 5% Error Rate
(b) 10% Error Rate
(c) 15% Error Rate
Figure 4:	Test accuracy for each target model of given error rate with classifiers induced by poisoning
points obtained from our attack and the KKT attack.
Convergence. We show the convergence of Algorithm 1 by reporting the maximum loss difference
and Euclidean distance between the classifier induced by the attack and the target classifier. Figures 3a
and 3b summarize the results for the target classifier with a 10% error rate. The maximum number of
poisoning points in the figure is obtained when the classifier from Algorithm 1 is 0.1-close to the
target classifier in the loss-based distance. In Figure 3, the classifier induced by our algorithm steadily
converges to the target classifier both in the maximum loss difference and Euclidean distance, while
the classifier induced by the KKT attack diverges initially and then starts to converge to the target
model. At the maximum number of points, the maximum loss difference of KKT-induced classifier to
the target is 0.46, compared to 0.1 for the classifier induced by our attack. For the Euclidean distance,
the KKT-induced classifier is 0.16 away, compared to 0.07 for the classifier induced by our attack.
Attack Success. We next compare the classifier induced from our attack to the classifier induced by
the KKT attack in terms of their overall test accuracy. Similarly, the maximum number of poisoning
points in Figure 4 is obtained by running our attack with 0.1-closeness (in loss-based distance) to
the target as the input. In terms of the test accuracy, our attack has a comparable attack success
rate compared to the KKT attack. Specifically, for target models of 5% and 10% error rates, both
methods have almost identical performance, as shown in Figures 4a and 4b. For the target model of
15% error rate (test accuracy is 83.3%), the KKT attack is more successful than our attack, inducing
models with 82.7% accuracy (17.3% attack success rate) compared to 85.9% accuracy (14.1% attack
success rate) for our attack. Interestingly, in this case, the performance of models induced by the
two attacks with fewer poisoning points our attack results in models with lower test accuracy (higher
attacker success) than the KKT attack. To summarize, for the indiscriminate scenario, our attack
produces classifiers that have much closer distance to the target models than the KKT attack, and has
comparable attack success rates with the KKT attack.
Lower Bound on Number of Poisons. We next check the optimality of our attack in the indiscrimi-
nate attack scenario. Similar to the subpopulation attack setting, we still use Theorem 4.2 to compute
the lower bound of the induced classifier from our attack by using it as the input to Algorithm 1,
and terminating when the induced classifier is 0.1-close to the given target model. In Table 2, our
21
Under review as a conference paper at ICLR 2021
	5% Error	10% Error	15% Error
# of Poisons	1737	5458	6192
Lower Bound	874	3850.4 ± 0.8	4904
Table 2: Poisoning points needed to achieve target classifiers induced from our attack. Top row means
number of poisoning points used by our attack. Bottom row means the lower bound computed from
Theorem 4.2 for the target classifiers. All results are averaged over four runs. An integral value
in a cell means we get exactly that same value for all four runs; for the one cell where we observe
variation, we report the average and standard error.
(a) 5% Error Rate
(b) 5% Error Rate
(c) 15% Error Rate
Figure 5:	Lower bound computed in each iteration of running algorithm 1 when the target classifier of
the algorithm is the classifier induced from our Attack (classifier in Table 2). The maximum number
of poisons is set using the 0.1-close threshold to classifier induced from our attack.
	5% Error	10% Error	15% Error
# of Poisons	1737	5458	6192
Lower Bound	856	4058.4+1.4	5031.4+4.8
Table 3: Poisoning points needed to achieve target classifiers induced from the KKT attack. Top row
means number of poisoning points used by the KKT attack. Bottom row means the lower bound
computed from Theorem 4.2 for the target classifiers. All results are averaged over 4 runs, integer
value in the cell means we get exactly same value for 4 runs and others are shown with the average
and standard error.
Figure 6: Lower bound computed in each iteration of running algorithm 1 when the target classifier
of the algorithm is the classifier induced from the KKT Attack (classifier in Table 3). The maximum
number of poisons is set using the 0.1-close threshold to KKT induced classifier.
calculated lower bound shows that there exists a relatively large gap between the number of poisoning
points, especially for the induced classifier from our attack for the target model of 5% error rate,
where the lower bound is only 50% of the actual number of poisoning points used. For the induced
classifier for the target model of 15% error rate, the gap between the number of poisoning points and
the lower bound is smallest, with the lower bound taking 79% of the number of poisoning points.
22
Under review as a conference paper at ICLR 2021
Target Models	Test Acc (%)		Loss on Clean Set		# of Poisons	
	Original	Improved	Original	Improved	Original	Improved
5% Error	94.0	94.9	2254.6	1767.1	2170	1340
10% Error	88.8	88.9	4941.0	3233.1	5810	2432
15% Error	83.3	84.5	5428.4	4641.6	6762	3206
Table 4: Comparison of two target generation methods on number of poisoning points used to reach
0.1-closeness to the target. Original indicates the original target generation process from Koh et al.
(2018). Improved denotes our improved target generation process with adaptive model updating.
Num of Poisons
(a) 5% Error Rate
Num of Poisons
(b) 10% Error Rate
Num of Poisons
(c) 15% Error Rate
Figure 7:	Test accuracy with classifiers obtained from our attack and KKT attack. Target model
for KKT attack is generated from the original generation process and target model for our attack is
generated from the improved generation process. Maximum number of poisoning points is obtained
by running our attack with target model generated from the original process and resultant classifier is
0.1-close to the target.
The relatively large gap indicates that either the estimated lower bound is not tight or the attack itself
is not close to optimal. To gain more insights into this problem, we further show the computed lower
bound at each iteration when running Algorithm 1 and Figure 5 summarizes the results. From the
Figure, we see that, the peak value of the curve (i.e., highest lower bound) always appears before the
termination of the algorithm, indicating that the computed lower bound is likely to be tight and we
may need to further improve the attack algorithm.
For completeness, we also repeat the lower bound computation process for classifiers induced from
the KKT attack. The KKT induced classifiers are obtained by running the KKT attack with target
classifiers of different error rates as target input. The target number of poisoning points of KKT attack
is given by the size of poisoning set returned from our attack when our algorithm terminates when
the induced classifier is 0.1-close to the target model of different error rates. Then the lower bound
computation process is identical to the above - We simply send the KKT induced classifier as target
input to Algorithm 1 and terminate it when the induced classifier from our algorithm is 0.1-close
to the given target model (i.e., KKT induced classifier). The results are summarized in Table 3 and
We observe that there also exists a relatively large gap betWeen the loWer bound and the number of
poisoning points used by the attack. Similarly, We also plot the loWer bound computed at different
iterations of Algorithm 1 in Figure 6, and find that the peak value also appears before the termination
of the algorithm, indicating that the loWer bound might be tight and We need a stronger attack strategy
to close the gap.
D.1 Improved Target Generation Process
The original heuristic approach Works by finding different quantiles of training points that have higher
loss on the clean model, flipping their labels, repeating those points for multiple copies, and adding
them to the clean training set. We find that, in the process of trying different quantiles and copies of
high loss points, if We also adaptively update the model Where the high loss points are found (instead
of just alWays fixing it to be the clean model), We can generate a valid target classifier With even loWer
loss on the clean training. Such an improved generation process can significantly reduce the number
of poisoning points needed to reach the same -closeness (With respect to the loss-based distance)
23
Under review as a conference paper at ICLR 2021
to the target classifier, consistent with the claims in Theorem 4.1 in the main paper. In addition, we
find that, if we compare our attack with improved generation process to the KKT attack with the
original generation process (Koh et al., 2018), we can also reach the desired target error rate much
faster using our attack.
Implication of Theorem 4.1. We first empirically validate the implication of Theorem 4.1 in the
main paper: to obtain the same -closeness in loss-based distance, a target classifier with lower loss on
the clean training set Dc requires fewer poisoning points. Therefore, when adversaries have multiple
target classifiers that satisfy the attack goal, the one with lower loss on clean training set is preferred.
For both the original and improved target generation methods, we generate three target classifiers with
error rates of 5%, 10% and 15%. The original target classifier generation method returns classifiers
with test accuracy of 94.0%, 88.8% and 82.3% respectively (also used in the previous experiments
of indiscriminate attack). The improved target generation process returns target classifiers with
approximately the same test accuracy (94.9%, 88.9% and 84.5%). However, for classifiers returned
from the two generation methods of same error rate, the improved generation method produces
classifiers with significantly lower loss compared to the original generation approach.
Table 4 compares the two target generation approaches by showing the number of poisoning points
needed to get 0.1-close to the corresponding target model of same error rate. For example, for target
models of 15% error rate, the model from the original approach has a total clean loss of 5428.4 while
our improved method reduces it to 4641.6. With the reduced clean loss, getting 0.1-close to the target
model generated from our improved process only requires 3206 poisoning points, while reaching the
same distance from the target model produced by the original method would require 6762 poisoning
points, a more than 50% reduction.
End-to-End Comparison. Figure 7 compares the two attacks in an end-to-end manner in terms of
their test accuracy. With the improved target generation process, our attack can achieve the desired
error rate much faster than the KKT attack with the original process. For the KKT attack with target
model generated from the original process, we determine the target number of poisoning points using
the size of poisoning set returned from running Algorithm 1 with 0.1-closeness and target model
from original process as inputs. To run our attack with improved generation process, we terminate
the algorithm when the size of the poisoning points is same as the number of poisoning points used
by the KKT attack with original process. Such a termination criteria helps us to ensure that both
attacks use same number of poisoning points and can be compared easily. We also evaluate the KKT
attack on fractions of the maximum target number of poisoning points (0.2, 0.4, 0.6, and 0.8), as
in the previous experiments. The accuracy plot shows that our attack (with improved target model)
can achieve the desired error rate (e.g., 10%) much faster than the KKT attack (with original target
model), especially for the target classifiers of error rates of 10% and 15%.
E Comparis on of Model-Targeted and Objective-Driven Attacks
Although model-targeted attacks work to induce the given target classifiers by generating poisoning
points, the end goal is still to achieve the attacker objectives encoded in the target models. In terms of
the comparison to the objective-driven attacks, we first demonstrate that, objective-driven attacks
can be used to generate a target model, which can then be used as the target for a model-targeted
attack, resulting in an attack that achieves the desired attacker objective with fewer poisoning points.
Then, we show that in order to have competitive performance against state-of-the-art objective-driven
attacks (e.g., the min-max attack (Steinhardt et al., 2017)), the target classifiers should be generated
carefully, such that the attacker objectives of the target classifiers can be achieved efficiently with
model-targeted attacks using fewer poisoning points. Although the investigation of a systematic
approach to generate such “desired” classifiers is out of the scope of this paper, in the indiscriminate
setting, we have some empirical evidence. Specifically, we find that target classifiers with lower loss
on clean training set and higher error rates (higher than what are desired in the attacker objectives)
often require fewer poisoning points to achieve the attacker objectives. The following experiments
are conducted on the MNIST 1-7 dataset.
Target Models Generated from Objective-driven Label-Flipping Attacks. In our experiments,
the target classifiers are generated from the label flipping based objective driven attacks that are
24
Under review as a conference paper at ICLR 2021
effective but need too many poisoning points to achieve their objective. Then, our attacks are deployed
to achieve the same objective with fewer poisoning points. Table 5 shows the number of poisoning
points used by the label-flipping attack and our model-targeted attack, to achieve desired attack
objectives of increasing the test error to certain amount. We can see that, using our attack, the number
of poisoning points used by label-flipping attacks can be saved up to 73%.
Comparison to the Min-Max Attack. Still using target classifiers generated from label-flipping
attacks, we show that our attack can outperform the state-of-the-art min-max attack (Steinhardt et al.,
2017) at reducing the overall test accuracy, under same amount of poisoning points. Since we aim
to produce target classifiers with lower loss on clean training set and higher error rates, we adopt
the improved target model generation process described in Section D.1 (helps to reduce the loss on
clean training set) and generate a classifier of 15% error rate. With the target model, we terminate our
attack when the induced model is 0.1-close to the target model in terms of the loss-based distance.
However, to we compare our attack to the min-max attack conveniently, we compare their accuracy
reduction at different poisoning ratios (i.e., 5%, 15% and 30%), which is the common evaluation
strategy of objective-driven attacks in the indiscriminate setting (Biggio et al., 2011; Steinhardt et al.,
2017; Koh et al., 2018). Table 6 summarizes the results. From the table, we observe that, compared
to the min-max attack, our attack reduces more on the test accuracy under the same poisoning budget
and the gap becomes larger when the poisoning ratio increases.
Comparison to the Label-Flipping Subpopulation Attack. We also compare our attack to the
label-flipping subpopulation attack from Jagielski et al. (2019). This attack works by randomly
sampling fixed number (constrained by the poisoning budget) of instances from the training data
of the subpopulation, flipping their labels and then injecting them into to the original training set.
Although this attack is very simple, it shows relatively high attack success when the goal is to cause
misclassification on the selected subpopulation (Jagielski et al., 2019).
To be consistent with our experiments in Section 5, we assume the attacker objectives are still to
induce a model that has 0% accuracy on a selected subpopulation. For each of the SVM and logistic
regression models, we selected the three subpopulations with highest test accuracy (all end up having
have 100% accuracy). In indiscriminate setting, we already observed that models with lower loss on
clean training set and larger overall error rates can achieve attacker objectives of smaller error rates
faster. However, to leverage this observation into our subpopulation experiments, one challenge is the
attacker objective is to have 100% test error on the subpopulation, but no classifiers can have test
errors larger than 100%. To tackle this, we select models with larger loss on training samples from
the subpopulation, with a hope that this process is “equivalent” to selecting target models with larger
error rates (on subpopulation) than 100%. To this end, we heuristically select targeted models that
satisfy the attacker objective, have larger loss on the training data from the subpopulation, and have
relatively low loss on the entire clean training set. Empirically, this selection strategy works better
than the original target generation process (as done in Section 5) in achieving the attacker objectives.
A more detailed and systematic investigation of the target model search process is left as the future
work.
To check the effectiveness of achieving the attacker objectives, we first run our attack and terminate
when our attack achieves the attacker objective to have 0% accuracy on the selected subpopulation,
and record the number of poisoning points used. Then, we run the random label-flipping attack with
the same number of poisoning points. For both attacks, we report the final test accuracies of the
resulting models on the subpopulations.
Attacker Objectives	5% Error	10% Error	15% Error
Label-flipping Attack	6,510	8,648	10,825
Our Attack	1,737	5,458	6,192
Table 5: Generate target classifiers using objective-driven label-flipping attacks and achieve similar
attacker objectives using our attack with fewer poisoning points. The attacker objectives are to
increase the test error to certain amounts (i.e., 5%, 10% and 15%) and the target classifiers to our
attack are generated by running the label-flipping attacks with given attacker objectives.
25
Under review as a conference paper at ICLR 2021
Poisoning Ratio	5%	15%	30%
Min-max Attack	97.0%	93.9%	92.9%
Our Attack	96.2%	88.6%	85.0%*
Table 6: Comparison of our attack to the min-max attack with different poisoning ratios. The target
model of our attack is of 15% error rate. The poisoning ratio is with respect to the full training set
size of 13,007. Each cell in the table denotes the test accuracy of the classifier after poisoning. The
clean test accuracy is 98.9%. Our attack at 30% poisoning ratio is marked with “*” because the attack
terminates when the induced model is 0.1-close to the target model, which only uses 2,894 poisoning
points and is less than the 30% ratio.
SVM	Logistic Regression
	Cluster 0	Cluster 1	Cluster 2	Cluster 0	Cluster 1	Cluster 2
Our Attack	0%	0%	0%	0%	0%	0%
Label-Fipping	31.4%	2.8%	15.5%	15.9%	14.0%	19.1%
Table 7: Comparison of our attack to the label-flipping based subpopulation attack. The table com-
pares the test accuracy on subpoplation of Adult dataset under same number of poisning points. The
number of poisons are determined when our attack achieves 0% test accuracy on the subpopulation.
Cluster 0-3 in the logistic regression and SVM models denote different clusters. For logistic regres-
sion, number of poisoning points for Cluster 0-3 are 1,575, 1,336 and 1,649 respectively. For SVM,
number of poisoning points for Cluster 0-3 are 1,252, 1,268 and 1,179 respectively.
The attack comparisons on different subpopulation clusters and models are given in Table 7. Results
in the table compare our attack and the label-flipping attack over the three distinct subpopulation
clusters for the SVM and logistic regression models. Across all settings, our attack is considerably
more successful. The number of poisoning points needed to reach the 0% accuracy goal is small
compared to the entire training set size (e.g., the maximum poisoning ratio is only 10.5%). The
gap between our attack and the label-flipping attack is fairly small. For example, for Cluster 1 in
the SVM experiment, the label-flipping attack is also quite successful and reduces the test accuracy
to 2.8% (our attack achieves 0% accuracy). We believe the success of label-flipping attack is due
to the following two reasons. First, label-flipping in the subpopulation setting can be successful
because smaller subpopulations show some degree of locality and hence, injecting points (from the
subpopulation) with flipped labels can have a strong impact on the selected subpopulation. This is
confirmed by empirical evidence that increasing the subpopulation size (i.e., reducing its locality)
gradually reduces the label-flipping effectiveness and the attack becomes much less effective in the
indiscriminate setting (i.e., subpopulation is the entire population). Second, the Adult dataset only
contains 57 features, where 53 of them are binary features with additional constraints. Therefore, the
benefit from optimizing the feature values is less significant as the optimization search space of our
attack is fairly limited.
F	Additional Experiments
In this section, we provide the results of evaluating our attack and the KKT attack on SVM models for
an additional dataset (Section F.1) and on logistic regression models for three datasets (Section F.2).
The results here are consistent with our findings in on the datasets and models in Section 5, but
provide further evidence of the general effectiveness of our attack.
F.1 Attacks on SVM trained on Dogfish
In this section, we introduce the results of SVM model evaluated on the Dogfish dataset.
Dataset. The Dogfish dataset contains dog and fish images of dimensions 298 × 298 × 3. This
dataset has been used as a binary classification task by by previous works in evaluating poisoning
attacks in the indiscriminate setting (Koh & Liang, 2017; Steinhardt et al., 2017; Koh et al., 2018).
26
Under review as a conference paper at ICLR 2021
(a) Max Loss Difference to Target
(b) Euclidean Distance to Target
Figure 8:	SVM on Dogfish dataset: attack convergence (results shown are for the target classifier
of error rate 10%). The maximum number of poisons is set using the 2.0-close threshold to target
classifier
Num of Poisons
(a) 10% Error Rate
Num of Poisons
o.2	∙ Our Attack
■ KKT Attack
0,0 0	50	100	150
Num of Poisons
(b) 20% Error Rate	(c) 30% Error Rate
Figure 9: SVM on Dogfish dataset: test accuracy of each target model of given error rate
classifiers induced by poisoning points obtained from our attack and the KKT attack.
Our Attack
KKT Attack
Our Attack
KKT Attack
with
	10% Error	20% Error	30% Error
Poisoning Points	32	89	169
Lower Bound	13	36	67
Table 8: SVM on Dogfish dataset: poisoning points needed to achieve target classifiers induced from
our attack. Top row means number of poisoning points used by our attack. Bottom row means the
lower bound computed from Theorem 4.2 for the induced classifiers.
To make classification easy for linear models, both Steinhardt et al. (2017) and Koh et al. (2018) use
the extracted features (of the original images) from the ImageNet Inception model (Szegedy et al.,
2016) and then apply linear models to complete the classification task. We also adopt the extracted
features for classification, so each instance has 2,048 features. The Dogfish dataset has 1,800 training
samples and 600 test samples. As in the previous work, we conduct a conventional indiscriminate
attack on it, where the adversary’s goal is to reduce the overall test accuracy.
Target Classifiers. The clean accuracy of the SVM model on the Dogfish dataset is 98.5%. We
generate target classifiers of overall test error rates around 10%, 20% and 30% using a similar
(heuristic) target generation process (Koh et al., 2018) as in the case of MNIST 1-7 dataset. The final
test accuracies of the obtained target classifiers are 89.3%, 78.3% and 67.2% respectively.
Attack Results. The convergence of our attack is demonstrated in Figure 8, both in the loss-based
distance and the actual model distance in '2-norm. The maximum number of poisons for the
experiments is obtained when the classifier from Algorithm 1 is 2.0-close to the target classifier. Our
attack steadily converges to the target model for both metrics, and has a faster convergence rate than
the KKT attack. Similar observations are found in other attack settings, as summarized in Figure 9.
Our attack is slightly more successful than the KKT attack in all three target classifiers we tested.
For the models induced from our attack, in Table 8, we also show the gap between the number of
27
Under review as a conference paper at ICLR 2021
poisoning points of our attack and the theoretical lower bound. Although our attack can induce the
target models using very few poisoning points (recall that the entire training set size is 1,800), there
is still some gap to the theoretical lower bound. We also repeated the same process on the model
induced from the KKT attack and again observe gaps between the number of poisoning points and
the corresponding lower bound. These suggest that there is still potentially room for improvement in
finding more efficient model-targeted poisoning attacks.
F.2 Attacks on Logistic Regression
In this section, We evaluate our attack on the logistic regression models for the Adult, MNIST 1-7
and Dogfish datasets.
The convergence guarantee in the paper also holds for logistic regression model (more generally,
holds for any Lipschitz and convex function With strongly convex regularizer). HoWever, for logistic
regression, We may not be able to efficiently search for the globally optimal point With maximum
loss difference (Line 4 in Algorithm 1) because the difference of tWo logistic losses is not concave.
Therefore, We adopt gradient descent strategy, using the Adam optimizer (Kingma & Ba, 2014) to
search for the point that (approximately) maximizes the loss difference. This is in contrast to the SVM
model, Where the difference of Hinge loss is piece-Wise linear and We can deploy general (convex)
solvers to search for the globally optimal point in each linear segment (Diamond & Boyd, 2016; Inc,
2020). HoWever, as Will be demonstrated next, poisoning points With approximate maximum loss
difference can still be very effective. More formally, if the approximate maximum loss difference l
found from local optimization techniques is Within a constant factor from the globally optimal value
l (i.e., l ≥ ɑl*, 0 < α < 1), then We still enjoy similar convergence guarantees. A similar issue of
global optimality also applies to the KKT attack (Koh et al., 2018), Where the attack objective function
is no longer convex for logistic regression models, and therefore, We also utilize gradient based
technique to (approximately) solve the optimization problem. Since the maximum loss difference
found for logistic regression models may not be the globally optimal value, in these experiments
We did not compute the loWer bound (Theorem 4.2) on number of poisoning points to induce the
poisoned model from our attack, Which requires obtaining the actual maximum loss difference3.
Target Classifiers. The clean accuracies of logistic regression models on the three datasets are
79.9% on Adult, 98.1% on MNIST 1-7 and 98.5% on Dogfish. Target classifiers for logistic
regression models are generated similarly to their SVM counterpart on each dataset. For Adult
dataset, the subpopulations are generated exactly the same as in the SVM case and form a total of
20 subpopulations, Where instances in each subpopulation all belong to the “loW-income” group.
Among all the subpopulations, We select three With the highest test accuracy on the subpopulations
and they all have 100% accuracy. The target classifiers are then generated to have 0% accuracy on
the subpopulations using the target generation method described in Section 5. On the MNIST 1-7
dataset, target models are generated to have around 5%, 10% and 15% overall test errors and final test
accuracies of the obtained target models are 94.7%, 89.0% and 84.5%. For Dogfish, target models
are generated to have around 10%, 20% and 20% overall test errors and the final test accuracies of
the resulting models are 89.0%, 79.5% and 67.3%.
Results on Adult. Figure 10 shoWs the effectiveness of our attack on logistic regression models
trained on the Adult dataset, using the loss-based distance and the actual model distance in '2-norm.
The maximum number of poisons for the experiments is obtained When the classifier from Algorithm
1 is 0.05-close to the target classifier. Our attack steadily converges to the target model While the
KKT attack fails to have a reliable convergence. Similar observations are also found in other attack
settings, as shoWn in Figure 11. Our attack is much more successful than the KKT attack, especially
for the attack on Cluster 1.
Results on MNIST 1-7. Figure 12 shows the convergance of our attack on LR models trained on the
MNIST 1-7. The maximum number of poisons for the experiments is obtained When the classifier
from Algorithm 1 is 0.1-close to the target classifier. Our attack converges to the target model While
3In the loWer bound computation, When the loss difference is not concave, We can use a concave upper bound
for it to obtain a valid loWer bound and as long as the upper bound is relatively tight, the loWer bound is still
meaningful.
28
Under review as a conference paper at ICLR 2021
(a) Max Loss Difference to Target
7 6 5 4 3 2
SSSSSS
oɔufsɪɑ UEoPlIOnW
(b) Euclidean Distance to Target
Figure 10: Logistic regression model on Adult dataset: attack convergence (results shown are for the
first subpopulation, Cluster 0). The maximum number of poisons is set using the 0.05-close threshold
to target classifier.
(a) Cluster 0
(b) Cluster 1
(c) Cluster 2
Figure 11: Logistic regression model on Adult dataset: test accuracy for each subpopulation with
classifiers induced by poisoning points obtained from our attack and the KKT attack.
(a) Max Loss Difference to Target
(b) Euclidean Distance to Target
Figure 12: Logistic regression model on MNIST 1-7 dataset: attack convergence (results shown are
for the target classifier of error rate 10%). The maximum number of poisons is set using the 0.1-close
threshold to target classifier.
Num of Poisons
(a) 5% Error Rate
Num of Poisons
(b) 10% Error Rate
(c) 15% Error Rate
S
Figure 13: Logistic regression model on MNIST 1-7 dataset: test accuracy for each target model of
given error rate with classifiers induced by poisoning points obtained from our attack and the KKT
attack.
29
Under review as a conference paper at ICLR 2021
5 0 5 0
2 2 11
JJla sso1-lXEn
(a) Max Loss Difference to Target
①OUEaSIa UPoPlIOn口
7)	10	20	30	40	50	6(Γ
Num of Poisons
(b) Euclidean Distance to Target
Figure 14: Logistic regression model on Dogfish dataset: attack convergence (results shown are for
the target classifier of error rate 10%). The maximum number of poisons is set using the 1.0-close
threshold to target classifier
Our Attack
KKT Attack
Our Attack
KKT Attack
Our Attack
KKT Attack
(a) 10% Error Rate	(b) 20% Error Rate	(c) 30% Error Rate
Figure 15: Logistic regression model on Dogfish dataset: test accuracy of each target model of given
error rate with classifiers induced by poisoning points obtained from our attack and the KKT attack.
the KKT attack diverges. Similar observations are also found for other attack settings, as shown in
Figure 13. In these settings, our attack is much more successful than the KKT attack. In fact, the
KKT attack seems to not find a useful set of poisoning points as its induced models did not show a
significant drop from the clean accuracy of 98.1%. We suspect this is due to the highly non-convex
nature of the attacker objective4 when attacking logistic regression models. In contrast, our attack
only deals with maximizing the difference of two logistic losses, which is simpler than the KKT
attacker objective, and results in a successful attack.
Results on Dogfish. Figure 14 shows attack results of logistic regression models on the Dogfish
dataset. The maximum number of poisons for the experiments is obtained when the classifier from
Algorithm 1 is 1.0-close to the target classifier. Our attack converges to the target model while
the KKT attack fails to converge. Attack success comparisons are given in Figure 15. As with
MNIST 1-7, our attack succeeds in settings where the KKT attack does not lead to significant
accuracy drops from the clean accuracy of 98.5%. We believe this is also because of the highly
non-convex nature of the KKT attack objective.
4The attacker objective is related to minimizing norm of the gradient, and becomes complicated for logistic
regression models. Details of the attack formulation are in Koh et al. (2018).
30