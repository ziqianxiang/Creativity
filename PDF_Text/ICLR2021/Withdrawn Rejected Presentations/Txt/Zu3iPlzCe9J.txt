Under review as a conference paper at ICLR 2021
On the Power of Abstention and Data-Driven
Decision Making for Adversarial Robustness
Anonymous authors
Paper under double-blind review
Abstract
We formally define a feature-space attack where the adversary can perturb data-
points by arbitrary amounts but in restricted directions. By restricting the attack to
a small random subspace, our model provides a clean abstraction for non-Lipschitz
networks which map small input movements to large feature movements. We prove
that classifiers with the ability to abstain are provably more powerful than those
that cannot in this setting. Specifically, we show that no matter how well-behaved
the natural data is, any classifier that cannot abstain will be defeated by such an
adversary. However, by allowing abstention, we give a parameterized algorithm
with provably good performance against such an adversary when classes are reason-
ably well-separated in feature space and the dimension of the feature space is high.
We further use a data-driven method to set our algorithm parameters to optimize
over the accuracy vs. abstention trade-off with strong theoretical guarantees. Our
theory has direct applications to the technique of contrastive learning, where we
empirically demonstrate the ability of our algorithms to obtain high robust accuracy
with only small amounts of abstention in both supervised and self-supervised set-
tings. Our results provide a first formal abstention-based gap, and a first provable
optimization for the induced trade-off in an adversarial defense setting.
1	Introduction
A substantial body of work has shown that deep networks can be highly susceptible to adversarial
attacks, in which minor changes to the input lead to incorrect, even bizarre classifications (Nguyen
et al., 2015; Moosavi-Dezfooli et al., 2016; Su et al., 2019; Brendel et al., 2018; Shamir et al., 2019).
Much of this work has considered `p-norm adversarial examples, but there has also been recent
interest in exploring adversarial models beyond bounded `p-norm (Brown et al., 2018; Engstrom
et al., 2017; Gilmer et al., 2018; Xiao et al., 2018; Alaifari et al., 2019). What these results have in
common is that changes that either are imperceptible or should be irrelevant to the classification task
can lead to drastically different network behavior.
One reason for this vulnerability to adversarial attack is the non-Lipschitzness property of typical
neural networks: small but adversarial movements in the input space can often produce large
perturbations in the feature space. In this work, we consider the question of whether non-Lipschitz
networks are intrinsically vulnerable, or if they could still be made robust to adversarial attack, in
an abstract but (we believe) instructive adversarial model. In particular, suppose an adversary, by
making an imperceptible change to an input x, can cause its representation F(x) in feature space (the
penultimate layer of the network) to move by an arbitrary amount: will such an adversary always
win? Clearly if the adversary can modify F (x) by an arbitrary amount in an arbitrary direction,
then yes. But what if the adversary can modify F(x) by an arbitrary amount but only in a random
direction (which it cannot control)? In this case, we show an interesting dichotomy: if the classifier
must output a classification on any input it is given, then yes the adversary will still win, no matter
how well-separated the classes are in feature space and no matter what decision surface the classifier
uses. However, if the classifier is allowed to abstain, then it can defeat such an adversary so long as
natural data of different classes are reasonably well-separated in feature space. Our results hold for
generalizations of these models as well, such as adversaries that can modify feature representations
in random low-dimensional subspaces, or directions that are not completely random. More broadly,
our results provide a theoretical explanation for the importance of allowing abstaining, or selective
classification, in the presence of adversarial attack.
1
Under review as a conference paper at ICLR 2021
Figure 1: Illustration of a non-Lipschitz feature mapping using a deep network.
Apart from providing a useful abstraction for non-Lipschitz feature embeddings, our model may be
viewed as capturing an interesting class of real attacks. There are various global properties of an
image, such as brightness, contrast, or rotation angle whose change might be “perceptible but not
relevant” to classification tasks. Our model could also be viewed as an abstraction of attacks of that
nature. Feature space attacks of other forms, where one can perturb abstract features denoting styles,
including interpretable styles such as vivid colors and sharp outlines and uninterpretable ones, have
also been empirically studied in (Xu et al., 2020; Ganeshan & Babu, 2019).
An interesting property of our model is that it is critical to be able to refuse to predict: any algorithm
which always predicts a class label—therefore without an ability to abstain—is guaranteed to perform
poorly. This provides a first formal hardness result about abstention in adversarial defense, and also
a first provable negative result in feature-space attacks. We therefore allow the algorithm to output
“don’t know” for some examples, which, as a by-product of our algorithm, serves as a detection
mechanism for adversarial examples. It also results in an interesting trade-off between robustness and
accuracy: by controlling how frequently we refuse to predict, we are able to trade (robust) precision
off against recall. We also provide results for how to provably optimize for such a trade-off using
a data-driven algorithm. Our strong theoretical advances are backed by empirical evidence in the
context of contrastive learning (He et al., 2020; Chen et al., 2020; Khosla et al., 2020).
1.1	Our contributions
Our work tackles the problem of defending against adversarial perturbations in a random feature
subspace, and advances the theory and practice of robust machine learning in multiple ways.
•	We introduce a formal model that captures feature-space attacks and the effect of non-
Lipschitzness of deep networks which can magnify input perturbations.
•	We begin our analysis with a hardness result concerning defending against adversary without
the option of “don’t know”. We show that all classifiers that partition the feature space
into two or more classes—thus without an ability to abstain—are provably vulnerable to
adversarial examples for at least one class of examples with nearly half probability.
•	We explore the power of abstention option: a variant of nearest-neighbor classifier with
the ability to abstain is provably robust against adversarial attacks, even in the presence of
outliers in the training data set. We characterize the conditions under which the algorithm
does not output “don’t know” too often.
•	We leverage and extend dispersion techniques from data-driven decision making, and present
a novel data-driven method for learning data-specific optimal hyperparameters in our defense
algorithms to simultaneously obtain high robust accuracy and low abstention rates. Unlike
typical hyperparameter tuning, our approach provably converges to a global optimum.
•	Experimentally, we show that our proposed algorithm achieves certified adversarial robust-
ness on representations learned by supervised and self-supervised contrastive learning. Our
method significantly outperforms algorithms without the ability to abstain.
2	Related Work
Adversarial robustness with abstention options. Classification with abstention option (a.k.a. selec-
tive classification (Geifman & El-Yaniv, 2017)) is a relatively less explored direction in the adversarial
machine learning. Hosseini et al. (2017) augmented the output class set with a NULL label and trained
the classifier to reject the adversarial examples by classifying them as NULL; Stutz et al. (2020)
and Laidlaw & Feizi (2019) obtained robustness by rejecting low-confidence adversarial examples
2
Under review as a conference paper at ICLR 2021
according to confidence thresholding or predictions on the perturbations of adversarial examples.
Another related line of research to our method is the detection of adversarial examples (Grosse et al.,
2017; Li & Li, 2017; Carlini & Wagner, 2017; Ma et al., 2018; Meng & Chen, 2017; Metzen et al.,
2017; Bhagoji et al., 2018; Xu et al., 2017; Hu et al., 2019). However, theoretical understanding
behind the empirical success of adversarial defenses with an abstention option remains elusive.
Data-driven decision making. Data-driven algorithm selection refers to choosing a good algorithm
from a parameterized family of algorithms for given data. It is known as “hyperparameter tuning” to
machine learning practitioners and typically involves a “grid search”, “random search” (Bergstra &
Bengio (2012)) or gradient-based search, with no guarantees of convergence to a global optimum. It
was formally introduced to the theory of computing community by Gupta & Roughgarden (2017) as
a learning paradigm, and was further extended in (Balcan et al., 2017). The key idea is to model the
problem of identifying a good algorithm from data as a statistical learning problem. The technique
has found useful application in providing provably better algorithms for several domains including
clustering, mechanism design, and mixed integer programs, and providing guarantees like differential
privacy and adaptive online learning (Balcan et al., 2018a;b; 2020). For learning in an adversarial
setting, we provide the first demonstration of the effectiveness of data-driven algorithm selection in a
defense method to optimize over the accuracy-abstention trade-off with strong theoretical guarantees.
3	Preliminaries
Notation. We will use bold lower-case letters such as x and y to represent vectors, lower-case letters
such as x and y to represent scalars, and calligraphy capital letters such as X, Y and D to represent
distributions. Specifically, we denote by x ∈Xthe sample instance, and by y ∈Ythe label, where
X⊆Rn1 and Y indicate the image and label spaces, respectively. Denote by F : X→Rn2 the
feature embedding which maps an instance to a high-dimensional vector in the latent space F (X). It
can be parameterized, e.g., by deep neural networks. We will frequently use v ∈ Rn2 to represent an
adversarial perturbation in the feature space. Denote by dist(∙, ∙) the distance between any two vectors
in the image or feature space. Examples of distances include dist(x1, x2) = kx1 - x2 k—the one
induced by vector norm. We use B(x, τ) to represent a neighborhood of x: {x0 : dist(x, x0) ≤ τ} in
the image or feature space. We will frequently denote by DX the distribution of instances in the input
space, by DX|y the distribution of instances in the input space conditioned on the class y, by DF(X)
the distribution of features, and by DF (X)|y the distribution of features conditioned on the class y.
3.1	Random Feature Subspace Threat model
In principle, the adversarial example for a given labeled data (x, y) is a data point x0 that causes a
classifier to output a different label on x0 than the true label y. Probably one of the most popular
adversarial examples is the norm-bounded perturbation in the input space. Despite a large literature
devoted to defending against norm-bounded adversary by improving the Lipschitzness of neural
network as a function mapping from input space to feature space (Zhang et al., 2019; Yang et al.,
2020), it is typically not true that small perturbation in the input space necessarily implies small
modification in the feature space. In this paper, we study a threat model where an adversary can
modify the data by a large amount in the feature space. Note that because this large modification in
feature space is assumed to come from a small perturbation in input space, we always assume that
the true correct label y is the same for x0 as for x. Our model highlights the power of abstention in
the adversarial learning: there is a provable separation when we have and do not have an abstention
option under our threat model.
Our threat model. In the setting of (robust) representation learning, we are given a set of training
instances x1,..., xm ∈X. Let x be an n1-dimensional test input for classification. The input is
embedded into a high n2-dimensional feature space using a deep neural network F . We predict the
class ofx by a prediction function on F(x) which can potentially output “don’t know”. The adversary
may corrupt F(x) such that the modified feature vector is restricted in a random n3-dimensional
affine subspace denoted by S + {F (x)}, while the perturbation magnitude might be arbitrarily large.
The adversary is given access to everything including F, x, S and the true label of x. Throughout
the paper, we will refer adversary and adversarial example to this threat model.
3
Under review as a conference paper at ICLR 2021
Algorithm 1 ROBUSTCLASSIFIER(τ, σ)
1:	Input: A test feature F(x) (potentially an adversarial example), a set of training features F (xi)
and their labels yi , i ∈ [m], a threshold parameter τ , a separation parameter σ.
2:	Preprocessing: Delete training examples F (xi) if minj∈[m],yi 6=yj dist(F (xi), F (xj)) <σ
3:	Output: A predicted label of F (x), or “don’t know”.
4:	if mini∈[m] dist(F (x), F (xi)) <τthen
5:	Return yarg mini∈[m] dist(F (x),F (xi))
6:	else
7:	Return “don’t know”
3.2	A meta-algorithm for inference-time robustness
Given a test data x, let r denote the shortest distance between F(x) and any training embedding
F(Xi) of different labels. Throughout the paper, We consider the prediction rule that We classify
an unseen (and potentially adversarially modified) example with the class of its nearest training
example provided that the distance between them is at most τ; otherwise the algorithm outputs
“don,t know” (see Algorithm 1 and Figure 2). The adversary is able to corrupt F(x) by a carefully-
crafted perturbation along a random direction, i.e., F(x) + v, where V is an adversarial vector of
arbitrary length in a random n3-dimensional subspace of Rn2. The parameter T trades the success
rate off against the abstention rate; when T → ∞, our algorithm is equivalent to the nearest-neighbor
algorithm. We also preprocess to remove outliers and points too close to them.
4	Negative Results without An Ability to Abstain
Several negative results are known for defending against adversarial examples beyond norm-bounded
settings. For example, ∣Shamir et al. (2019) provably show existence of targeted adversarial examples
with small hamming distance in the input space to their clean examples. For feature-space attacks,
several empirical negative results are known (XU et al., 2020; Ganeshan & Babu, 2019). We present a
hardness result concerning defenses without an ability to abstain, and prove that such defenses are
inevitably doomed against our feature-space attacks.
Theorem 4.1. For any classifier that partitions Rn2 into two or more classes, any data distribution
D, any δ > 0 and any feature embedding F, there must exist at least one class y*, such that for
at least a 1 一 δ probability mass of examples X from class y* (i.e., X is drawn from Dχ∣y*), for a
random unit-length vector v, with probability at least 1/2 - δ for some δ0 > 0, F(x) + δ0v is not
labeled y* by the classifier. In other words, there must be at least one class y* such that for at least
1 一 δ probability mass of points X of class y*, the adversary wins with probability at least 1/2 一 δ.
Proof. Without loss of generality, we assume that the feature embedding F is an identity mapping.
Define rδ to be a radius such that for every class y, at least a 1 一 δ probability mass of examples X of
class y lie within distance rδ of the origin. Let R = r √n2∕δ. R is defined to be large enough such
that if we take a ball of radius R and move it by a distance rδ, at least a 1 一 δ fraction of the volume
of the new ball is inside the intersection with the old ball. Now, let B be the ball of radius R centered
at the origin. Let vol(B) denote the volume ofB and let voly(B) denote the volume of the subset of B
that is assigned label y by the classifier. Let y* be any label such that voly* (B)/Vol(B) ≤ 1/2. Such
a class y* exists because we do not have the option to output “don’t know”. Now by the definition
of y*, a point z picked uniformly at random from B has probability at least 1/2 of being classified
differently from y* . This implies that, by the definition of R, if X is within distance rδ of the origin,
4
Under review as a conference paper at ICLR 2021
then a point zx that is picked uniformly at random in the ball Bx of radius R centered at x has
probability at least 1/2 - δ of being classified differently from y*. This immediately implies that if
we choose a random unit-length vector v, then with probability at least 1/2 - δ, there exists δ0 > 0
such that X + δ0v is classified differently from y*, since We can think of choosing V by first sampling
zx from Bx and then defining v =(zx - x)/kzx - xk2. So, the theorem follows from the fact that,
by the definition of rδ, at least 1 - δ probability mass of examples x from class y* are Within distance
rδ of the origin.	□
We remark that our loWer bound applies to any classifier and exploits the fact that a classifier Without
abstention must label the entire feature space. For a simple linear decision boundary (center of
Figure 3), a perturbation in any direction (except parallel to the boundary) can cross the boundary
With an appropriate magnitude. The left and right figures shoW that if We try to ‘bend’ the decision
boundary to ‘protect’ one of the classes, the other class is still vulnerable. Our argument formalizes
and generalizes this intuition, and shoWs that there must be at least one vulnerable class irrespective
of hoW you may try to shape the class boundaries, Where the adversary succeeds in a large fraction of
directions.
Figure 3: A simple example to illustrate Theorem 4.1.
Theorem 4.1 implies that all classifiers that partitions Rn2 into tWo or more classes—thus Without an
ability to abstain—are vulnerable to adversarial examples for at least one class of data With nearly
half probability. Despite much effort has been devoted to empirically investigating the poWer of
“don’t knoW” in the adversarial robustness, theoretical understanding behind the empirical success of
these methods remains elusive. To the best of our knoWledge, our Work is the first result that provably
demonstrates the poWer of “don’t knoW” in the algorithmic design of adversarially robust classifiers.
5	Positive Results with An Ability to Abstain
Theorem 4.1 gives a hardness result of robust classification Without abtention. In this section, We
explore the poWer of abstaining and shoW classifiers With an ability to abstain are provably robust.
Given a test instance X 〜DX, recall that r denotes the shortest distance between F(x) ∈ Rn2 and
any training embedding F(xi) ∈ Rn2 With a different label. The adversary is alloWed to corrupt
F(X) with an arbitrarily large perturbation in a uniform-distributed subspace S of dimension n3 .
Consider the prediction rule that we classify the unseen example F(X) ∈ Rn2 with the class of
its nearest training example provided that the distance between them is at most τ ; otherwise the
algorithm outputs “don,t know” (see Algorithm 1 when σ = 0). Denote by EXv(f) := ES〜S 1{∃e ∈
S + F(X) ⊆ Rn2 s.t. f(e) 6= y and f(e) does not abstain} the robust error of a given classifier f
for classifying instance X. Our analysis leads to the following positive results on this algorithm.
Theorem 5.1. Let X 〜 DX be a test instance, m be the number of training examples and r be
the shortest distance between F(X) and F(Xi) where Xi is a training point from a different class.
Suppose T = o 1 — n3 ). The robust error of Algorithm 1, EX V(ROBUSTCLASSIFIER(T, 0)), is
n2-n3
—7cτ	+ mcn2-n3, where c > 0 and 0 < co < 1 are absolute constants.
r √1-l	0
Proof Sketch. We begin our analysis with the case of n3 =1. Suppose we have a training example
X0 of another class, and suppose F(X) and F(X0) are at distance D in the feature space. Because
T = o (D), the probability that the adversary can move F(X) to within distance T of F(X0) should
5
Under review as a conference paper at ICLR 2021
be roughly the ratio of the surface area of a sphere of radius τ to the surface area of a sphere of
radius D, which is at most (O (D ))n2 1 ≤ (O (r ))n2 1. The analysis for the general case of n3
follows from a pealing argument: note that the random subspace in which the adversary vector is
restricted to lie can be constructed by first sampling a vector v1 uniformly at random from a unit
sphere in the ambient space Rn2 centered at 0; fixing v1 , we then sample a vector v2 uniformly at
random from a unit sphere in the null space of span{v1}; we repeat this procedure n3 times and let
span{v1, v2,...,vn3} be the desired adversarial subspace. For each step of construction, we apply
the same argument as that of n3 = 1 with D = Ω (r Jnn-I) by a high probability, if We project
F(x) and F (x0) to a random subspace of dimension n2 - i. Finally, a union bound over m training
□
points completes the proof.
Trade-off between success probability and abstention rate. Theorem 5.1 captures the trade-off
between the success probability of an algorithm and the abstention rate: a smaller value of τ increases
the success probability of the algorithm, while it also encourages Algorithm 1 to output “don’t
know” more often. A related line of research to this observation is the trade-off between robustness
and accuracy: Zhang et al. (2019); Tsipras et al. (2019) showed that there might be no predictor
in the hypothesis class that has low natural and robust errors; even such a predictor exists for the
well-separated data (Yang et al., 2020), Raghunathan et al. (2020) showed that the natural error
could increase by adversarial training if we only have finite number of data. To connect the two
trade-offs, we note that a high success probability of ROBUSTCLASSIFIER(τ, 0) in Algorithm 1 tends
to avoid the algorithm from predicting wrong labels for adversarial examples, while the associated
high abstention rate encourages the algorithm to output “don’t know” even for natural examples, thus
leading to a trivial non-accurate classifier.
5.1	A more general adversary with bounded density
We extend our results to a more general class of adversaries, which have a bounded distribution over
the space of linear subspaces of a fixed dimension n3 and the adversary can perturb a test feature
vector arbitrarily in the sampled adversarial subspace.
Theorem 5.2. Consider the setting of Theorem 5.1, with an adversary having a κ-bounded distri-
bution over the space of linear subspaces of a fixed dimension n3 for perturbing the test point. If
E(τ, r) denotes the bound on error rate in Theorem 5.1 for ROBUSTCLASSIFIER(τ, 0) in Algorithm
1, then the error bound of the same algorithm against the κ-bounded adversary is O(κE(τ, r)).
5.2	Outlier removal and improved upper bound
The upper bounds above assume that the data is well-separated in the feature space. For noisy data
and good-but-not-perfect embeddings, the condition may not hold. In Theorem E.1 (in Appendix E)
we show that we obtain almost the same upper bound on failure probability under weaker assumptions
by exploiting the noise removal threshold σ.
5.3	Controlling abstention rate on natural data
We show that we can control the frequency of outputting “don’t know”, when the data are nicely
distributed according to the following generative assumption. Intuitively, it says that for every label
class one can cover most of the distribution of the class with (potentially overlapping) balls of a fixed
radius, each having a small lower bound on the density contained. This holds for well-clustered
datasets (as is typical for feature data) for a sufficiently large radius.
Assumption 1. We assume that at least 1 - δ fraction of mass of the marginal distribution DF (X)|y
over Rn2 can be covered by N balls B1, B2, ... BN of radius τ∕2 and of mass PrDF(X)[Bk] ≥
Cm0 (n2 log m + log 4N), where Co > 0 is an absolute constant and δ, β ∈ (0,1).
Our analysis leads to the following guarantee on the abstention rate.
Theorem 5.3. Suppose that F(x1),...,F(xm ) are m training instances i.i.d. sampled from marginal
distribution DF(X). Under Assumption 1, with probability at least 1 一 β/4 over the sampling, we
have Pr(∪im B(F (xi),τ)) ≥ 1 - δ.
6
Under review as a conference paper at ICLR 2021
Theorem 5.3 implies that when Pr[Bk] ≥ Ne and m = Ω(n2N log n2N), with probability at least
1 - β∕4 over the sampling, We have Pr(∪mIB(F(xi), T)) ≥ 1 - δ. Therefore, with high probability,
the algorithm will output “don’t know” only for an δ fraction of natural data.
6 Learning Data-Specific Optimal Thresholds
Given an embedding function F and a classifier fτ which outputs either a predicted class if the
nearest neighbor is within distance τ of a test point or abstains from predicting, we want to evaluate
the performance of fτ on a test set T against an adversary which can perturb a test feature vector
in a random subspace S 〜S. To this end, we define Eadv(T) := ES〜$才 P(CC y-^∈τ 1{∃e ∈
S + F(x) ⊆ Rn2 s.t. f(e) 6= y and fτ (e) does not abstain} as the robust error on the test set T, and
Dnat(T) := τ Po y')∈τ 1{fτ(F(x)) abstains} as the abstention rate on the natural data. Eadv(T)
and Dnat (T) are monotonic in T. The robust error Eadv (T) is optimal at T =0, while we abstain
from prediction all the time (i.e., Dnat(T) = 1). A simple approach is to fix an upper limit d* on
Dnat (T), which corresponds to the maximum abstention rate on natural data under our budget. Then
it is straightforward to search for the optimal T* such that Dnat(T*) ≈ d* by using nearest neighbor
distances of test points. For T < T* we have a higher abstention rate, and when T > T* we have a
higher robust error rate. A potential problem with this approach is that Dnat (T) is non-Lipschitz, so
small variation in T can possibly make the abstention rate significantly higher than d*.
An alternative objective which captures the trade-off between abstention rate and accuracy is defined
as g(T):=Eadv(T)+cDnat(T), where c is a positive constant. If, for example, we are willing to
take a one percent increase of the abstention rate for a two percent drop in the error rate, we could
set C to be 1. We can optimize g(T) in a data-driven fashion and obtain theoretical guarantee on the
convergence to a global optimum. In the following, we consider the case where the test examples
appear in an online fashion in small batches of size b, and we set the threshold T adaptively by a
low-regret algorithm. We note in Corollary 6.3, using online-to-batch conversion, that our results
imply a uniform convergence bound for objective g(T) in the supervised setting. Details of proofs in
this section can be found in Appendix H.
The significance of data-driven design in this setting is underlined by the following two observations.
Firstly, as noted above, optimization for T is difficult due to the non-Lipschitzness nature of Dnat (T)
and the intractability of characterizing the objective function g(T) exactly due to Eadv (T). Secondly,
the optimal value of T can be a complex function of the data geometry and sampling rate. We
illustrate this by exact computation of optimal T for a simple intuitive setting: consider a binary
classification problem where the features lie uniformly on two one-dimensional manifolds embedded
in two-dimensions (i.e., n2 =2, see Figure 4). Assume that the adversary perturbs in a uniformly
random direction (n3 =1). For this setting, in Appendix J we show that
Theorem 6.1.	Let T* := argmaxτ∈r+ g(T) and β = 2∏Cr. For the setting considered above, if
we further assume D = o(r) and m = ω (log β), then there is a unique value of T* in [0, D∕2).
Furthermore, we have T* = Θ (Domem)) if m > β; otherwise, T* = 0.
D
<-------A
■ ■ ■ ■ V
Class A
D
V------->
Class B
Figure 4: A simple intuitive example where we compute the optimal value of the abstention threshold
exactly. Classes A and B are both distributed uniformly on one-dimensional segments of length D,
embedded collinear and at distance r in R2 .
The remaining section summarizes our main theoretical results.
Theorem 6.2.	Assume T is o min{m-1/n2, r} , and the data distribution is continuous, κ-bounded,
positive and has bounded partial derivatives. IfT is set using a continuous version of the multiplicative
updates algorithm (Algorithm 2 in Appendix H, Balcan et al. (2018a)), then with probability at least
by OXnT log (δr⅛⅛y)
1 - δ, the expected regret in T rounds is bounded
where R is a bound
7
Under review as a conference paper at ICLR 2021
on the largest distance between any two training points, b is the batch size, and r is the smallest
distance between points of different labels.
Corollary 6.3. Suppose we run the online algorithm of Theorem 6.2 on a validation set of size T, and
use a randomized threshold τ on the test set drawn from a uniform distribution over the thresholds
τι,..., TT used in online learning. Ifthe threshold which maximizes g(τ) is T*, then with probability
greater than 1 - δ, we have ∣E[g(T)] - g(τ * )| ≤ O (J 等 log (KRTmb )) ∙
Remark 1. The results can be generalized to a bounded density adversary (Corollary H.3).
Remark 2. The above analysis can be extended to the problem of optimizing over σ by formulating
the objective as function of two parameters, g(τ, σ):= Eadv(τ, σ)+cDnat(τ, σ) within a range
σ ∈ [r, s]. For fixed τ, both Eadv (τ, σ) and Dnat (τ, σ) are piece-wise constant and monotonic.
The proof of Lipschitzness of the pieces can be adapted easily to the case of σ ≥ r (Lemma H.2).
Discontinuities in Eadv(T, ∙) and Dnat(T, ∙) can be bounded using the upper bound S for σ (Lemma
H.4). Finally, the number of discontinuities in g(T, σ) in a ball of radius w can be upper bounded by
a product ofthe number ofdiscontinuities in g(τ, ∙) and g(∙, σ) in intervals ofwidth W.
7	Experiments on Contrastive Learning
Theorem 5.1 sheds light on algorithmic designs of robust learning of feature embedding F. In order
to preserve robustness against adversarial examples regarding a given test point x, in the feature
space the theorem suggests minimizing T —the closest distance between F (x) and any training
feature F(xi) of the same label, and maximizing r—the closest distance between F(x) and any
training feature F (xi ) of different labels. This is conceptually consistent with the spirit of the
nearest-neighbor algorithm, a.k.a. contrastive learning when we replace the max operator with the
softmax operator for differentiable training:
/	- kF (Xi)-F (Xj )k2 ∖
哨」X log I ∙j∈[m],j=i,yi = yjeF (X. )-F 二)k2	I ,	(1)
F m / J	-k	- kF (Xi) F (Xk)k	I
i∈[m]	∖	k∈kE[m=kei e	T	)
where T>0 is the temperature parameter. Loss (1) is also known as the soft-nearest-neighbor
loss in the context of supervised learning (Frosst et al., 2019), or the InfoNCE loss in the setting of
self-supervised learning (He et al., 2020).
7.1	Certified adversarial robustness against exact computation of attacks
We verify the robustness of Algorithm 1 when the representations are learned by contrastive learning.
Given a embedding function F and a classifier f which outputs either a predicted class or abstains
from predicting, recall that we define the natural and robust errors, respectively, as Enat(f):=
E(χ,y)〜D 1{f(F(x)) = y and f(F(x)) does not abstain}, and Eadv(f) := E(χ,y)〜d,s〜Sl{∃e ∈
S + F(x) ⊆ Rn2 s.t. f (e) = y and f (e) does not abstain}, where S 〜 S is a random adversarial
subspace of Rn2 with dimension n3. Dnat(f) := E(χ,y)〜D 1{f (F(x)) abstains} is the abstention
rate on the natural examples. Note that the robust error is always at least as large as the natural error.
Self-supervised contrastive learning setup. Our experimental setup follows that of SimCLR (Chen
et al., 2020). We use the ResNet-18 architecture (He et al., 2016) for representation learning with a
two-layer projection head of width 128. The dimension of the representations is 512. We set batch
size 512, temperature T = 0.5, and initial learning rate 0.5 which is followed by cosine learning
rate decay. We sequentially apply four simple augmentations: random cropping followed by resize
back to the original size, random flipping, random color distortions, and randomly converting image
to grayscale with a probability of 0.2. In the linear evaluation protocol, we set batch size 512 and
learning rate 1.0 to learn a linear classifier in the feature space by empirical risk minimization.
Supervised contrastive learning setup. Our experimental setup follows that of Khosla et al. (2020).
We use the ResNet-18 architecture for representation learning with a two-layer projection head of
width 128. The dimension of the representations is 512. We set batch size 512, temperature T = 0.1,
and initial learning rate 0.5 which is followed by cosine learning rate decay. We sequentially apply
four simple augmentations: random cropping followed by resize back to the original size, random
8
Under review as a conference paper at ICLR 2021
Table 1: Natural error Enat and robust error Eadv on the CIFAR-10 dataset when n3 =1and the
512-dimensional representations are learned by contrastive learning, where Dnat represents the
fraction of each algorithm’s output of “don’t know” on the natural data. We report values for σ ≈ τ
as they tend to give a good abstention-error trade-off w.r.t. σ.
Contrastive			Linear Protocol		Ours (T = 3.0)			Ours (τ = 2.0)		
			Enat	Eadv	Enat	Eadv	Dnat	Enat	Eadv	Dnat
(σ =	0)	Self-supervised	8.9%	100.0%	15.4%	40.7%	2.2%	14.3%	26.2%	28.7%
		Supervised	5.6%	100.0%	5.7%	60.5%	0.0%	5.7%	33.4%	0.0%
(σ =	0.9τ)	Self-supervised	8.9%	100.0%	7.2%	9.4%	12.9%	10.0%	17.7%	29.9%
		Supervised	5.6%	100.0%	6.2%	18.9%	0.0%	5.6%	22.0%	0.1%
(σ =	τ)	Self-supervised	8.9%	100.0%	1.1%	1.2%	33.4%	2.1%	3.1%	49.9%
		Supervised	5.6%	100.0%	1.9%	2.8%	10.6%	4.1%	4.8%	3.3%
flipping, random color distortions, and randomly converting image to grayscale with a probability
of 0.2. In the linear evaluation protocol, we set batch size 512 and learning rate 5.0 to learn a linear
classifier in the feature space by empirical risk minimization.
In both self-supervised and supervised setups, we compare the robustness of the linear protocol with
that of our defense protocol in Algorithm 1 under exact computation of adversarial examples using
a convex optimization program in n3 dimensions and m constraints. Algorithm 4 in the appendix
provides an efficient implementation of the attack.
Experimental results. We summarize our results in Table 1. Comparing with a linear protocol, our
algorithms have much lower robust error. Note that even if abstention is added based on distance from
the linear boundary, sufficiently large perturbations will ensure the adversary can always succeed.
For an approximate adversary which can be efficiently implemented for large n3, see Appendix L.2.
7.2	Robustness-abstention trade-off
The threshold parameter τ captures the trade-off between the robust accuracy Aadv := 1 - Eadv
and the abstention rate Dnat on the natural data. We report both metrics for different values of τ for
supervised and self-supervised constrastive learning. The supervised setting enjoys higher adversarial
accuracy and a smaller abstention rate for fixed τ ’s due to the use of extra label information. We
plot Aadv against Dnat for Algorithm 1 as hyperparameters vary. For small τ , both accuracy and
abstention rate approach 1.0. As the threshold increases, the abstention rate decreases rapidly and
our algorithm enjoys good accuracy even with small abstention rates. For τ → ∞ (i.e. the nearest
neighbor search), the abstention rate on the natural data Dnat is 0% but the robust accuracy is also
roughly 0%. Increasing σ (for small σ) gives us higher robust accuracy for the same abstention rate.
Too large σ may also lead to degraded performance.
Figure 5: Adversarial accuracy (i.e., rate of adversary failure) vs. abstention rate as threshold τ varies
for n3 =1and different outlier removal thresholds σ.
9
Under review as a conference paper at ICLR 2021
References
Rima Alaifari, Giovanni S Alberti, and Tandri Gauksson. ADef: an iterative algorithm to construct
adversarial deformations. In International Conference on Learning Representations, 2019.
Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. In Advances in Neural Information Processing Systems, pp.
15535-15545, 2019.
Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic
foundations of algorithm configuration for combinatorial partitioning problems. In Annual Confer-
ence on Learning Theory, pp. 213-274, 2017.
Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,
online learning, and private optimization. In Annual Symposium on Foundations of Computer
Science, pp. 603-614. IEEE, 2018a.
Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. A general theory of sample complexity
for multi-item profit maximization. In ACM Conference on Economics and Computation, pp.
173-174, 2018b.
Maria-Florina Balcan, Travis Dick, and Dravyansh Sharma. Learning piecewise Lipschitz functions
in changing environments. In International Conference on Artificial Intelligence and Statistics, pp.
3567-3577, 2020.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. The Journal
of Machine Learning Research, 13(1):281-305, 2012.
Arjun Nitin Bhagoji, Daniel Cullina, Chawin Sitawarin, and Prateek Mittal. Enhancing robustness of
machine learning systems via data transformations. In Annual Conference on Information Sciences
and Systems, pp. 1-5, 2018.
Avrim Blum, Travis Dick, Naren Manoj, and Hongyang Zhang. Random smoothing might be unable
to certify '∞ robustness for high-dimensional images. Journal of Machine Learning Research,
2020.
Wieland Brendel, Jonas Rauber, Alexey Kurakin, NicolaS Papernot, Behar Veliqi, Marcel Salath6,
Sharada P Mohanty, and Matthias Bethge. Adversarial vision challenge. arXiv preprint
arXiv:1808.01976, 2018.
Tom B Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian Good-
fellow. Unrestricted adversarial examples. arXiv preprint arXiv:1809.08352, 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In ACM Workshop on Artificial Intelligence and Security, pp. 3-14, 2017.
Kamalika Chaudhuri and Sanjoy Dasgupta. Rates of convergence for the cluster tree. In Advances in
Neural Information Processing Systems, pp. 343-351, 2010.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling CNNs with simple transformations. arXiv preprint
arXiv:1712.02779, 2017.
Dafydd Evans, Antonia J Jones, and Wolfgang M Schmidt. Asymptotic moments of near-neighbour
distance distributions. Proceedings of the Royal Society of London. Series A: Mathematical,
Physical and Engineering Sciences, 458(2028):2839-2849, 2002.
Nicholas Frosst, Nicolas Papernot, and Geoffrey Hinton. Analyzing and improving representations
with the soft nearest neighbor loss. In International Conference on Machine Learning, 2019.
10
Under review as a conference paper at ICLR 2021
Aditya Ganeshan and R Venkatesh Babu. FDA: Feature disruptive attack. In IEEE International
Conference on Computer Vision,pp. 8069-8079, 2019.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances
in neural information processing systems, pp. 4878-4887, 2017.
Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. Motivating the
rules of the game for adversarial example research. arXiv preprint arXiv:1807.06732, 2018.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.
Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection.
SIAM Journal on Computing, 46(3):992-1017, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In IEEE Conference on Computer Vision and Pattern
Recognition, pp. 9729-9738, 2020.
Olivier J Henaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, SM Eslami, and
Aaron van den Oord. Data-efficient image recognition with contrastive predictive coding. arXiv
preprint arXiv:1905.09272, 2019.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. arXiv preprint arXiv:1808.06670, 2018.
Hossein Hosseini, Yize Chen, Sreeram Kannan, Baosen Zhang, and Radha Poovendran. Block-
ing transferability of adversarial examples in black-box learning systems. arXiv preprint
arXiv:1703.04318, 2017.
Shengyuan Hu, Tao Yu, Chuan Guo, Wei-Lun Chao, and Kilian Q Weinberger. A new defense
against adversarial images: Turning a weakness into a strength. In Advances in Neural Information
Processing Systems, pp. 1635-1646, 2019.
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint
arXiv:2004.11362, 2020.
Cassidy Laidlaw and Soheil Feizi. Playing it safe: Adversarial robustness with an abstain option.
arXiv preprint arXiv:1911.11253, 2019.
Xin Li and Fuxin Li. Adversarial examples detection in deep networks with convolutional filter
statistics. In IEEE International Conference on Computer Vision, pp. 5764-5772, 2017.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. In International Conference on Learning Representations, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
Dongyu Meng and Hao Chen. MagNet: a two-pronged defense against adversarial examples. In
ACM SIGSAC conference on computer and communications security, pp. 135-147, 2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. In International Conference on Learning Representations, 2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2574-2582, 2016.
11
Under review as a conference paper at ICLR 2021
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 427-436, 2015.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understanding
and mitigating the tradeoff between robustness and accuracy. In International Conference on
Machine Learning, 2020.
Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. A simple explanation for the existence of
adversarial examples with small hamming distance. arXiv preprint arXiv:1901.10861, 2019.
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial training: Generaliz-
ing to unseen attacks. In International Conference on Machine Learning, 2020.
Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural
networks. IEEE Transactions on Evolutionary Computation, 23(5):828-841, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European
Conference on Computer Vision, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Ro-
bustness may be at odds with accuracy. In International Conference on Learning Representations,
2019.
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via
non-parametric instance discrimination. In IEEE Conference on Computer Vision and Pattern
Recognition, pp. 3733-3742, 2018.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially transformed
adversarial examples. In International Conference on Learning Representations, 2018.
Qiuling Xu, Guanhong Tao, Siyuan Cheng, Lin Tan, and Xiangyu Zhang. Towards feature space
adversarial attack. arXiv preprint arXiv:2004.12385, 2020.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep
neural networks. In Network and Distributed Systems Security Symposium, 2017.
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika Chaud-
huri. Adversarial robustness through local Lipschitzness. In Advances in neural information
processing systems, 2020.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning, 2019.
Chengxu Zhuang, Alex Lin Zhai, and Daniel Yamins. Local aggregation for unsupervised learning of
visual embeddings. In IEEE International Conference on Computer Vision, pp. 6002-6012, 2019.
12