Under review as a conference paper at ICLR 2021
A Sharp Analysis of Model-based Reinforce-
ment Learning with Self-Play
Anonymous authors
Paper under double-blind review
Ab stract
Model-based algorithms—algorithms that explore the environment through build-
ing and utilizing an estimated model—are widely used in reinforcement learning
practice and theoretically shown to achieve optimal sample efficiency for single-
agent reinforcement learning in Markov Decision Processes (MDPs). However,
for multi-agent reinforcement learning in Markov games, the current best known
sample complexity for model-based algorithms is rather suboptimal and compares
unfavorably against recent model-free approaches. In this paper, we present a
sharp analysis of model-based self-play algorithms for multi-agent Markov games.
We design an algorithm Optimistic Nash Value Iteration (Nash-VI) for two-player
zero-sum Markov games that is able to output an -approximate Nash policy in
O(H3SAB/e2) episodes of game playing, where S is the number of states, A, B
are the number of actions for the two players respectively, and H is the horizon
length. This significantly improves over the best known model-based guaran-
tee of O(H4S2AB/e2), and is the first that matches the information-theoretic
lower bound Ω(H3S(A + B)∕e2) except for a min {A, B} factor. In addition,
our guarantee compares favorably against the best known model-free algorithm if
min {A, B} = o(H3), and outputs a single Markov policy while existing sample-
efficient model-free algorithms output a nested mixture of Markov policies that is
in general non-Markov and rather inconvenient to store and execute. We further
adapt our analysis to designing a provably efficient task-agnostic algorithm for
zero-sum Markov games, and designing the first line of provably sample-efficient
algorithms for multi-player general-sum Markov games.
1 Introduction
This paper is concerned with the problem of multi-agent reinforcement learning (multi-agent RL), in
which multiple agents learn to make decisions in an unknown environment in order to maximize their
(own) cumulative rewards. Multi-agent RL has achieved significant recent success in traditionally
hard AI challenges including large-scale strategy games (such as GO) (Silver et al., 2016; 2017),
real-time video games involving team play such as Starcraft and Dota2 (OpenAI, 2018; Vinyals
et al., 2019), as well as behavior learning in complex social scenarios (Baker et al., 2020). Achieving
human-like (or super-human) performance in these games using multi-agent RL typically requires
a large number of samples (steps of game playing) due to the necessity of exploration, and how to
improve the sample complexity of multi-agent RL has been an important research question.
One prevalent approach towards solving multi-agent RL is model-based methods, that is, to use the
existing visitation data to build an estimate of the model (i.e. transition dynamics and rewards), run
an offline planning algorithm on the estimated model to obtain the policy, and play the policy in
the environment. Such a principle underlies some of the earliest single-agent online RL algorithms
such as E3 (Kearns & Singh, 2002) and RMax (Brafman & Tennenholtz, 2002), and is conceptu-
ally appealing for multi-agent RL too since the multi-agent structure does not add complexity onto
the model estimation part and only requires an appropriate multi-agent planning algorithm (such
as value iteration for games (Shapley, 1953)) in a black-box fashion. On the other hand, model-
free methods do not directly build estimates of the model, but instead directly estimate the value
functions or action-value (Q) functions of the problem at the optimal/equilibrium policies, and play
the greedy policies with respect to the estimated value functions. Model-free algorithms have also
1
Under review as a conference paper at ICLR 2021
Table 1: Sample complexity (the required number of episodes) for algorithms to find -approximate
Nash equlibrium policies in zero-sum Markov games: VI-explore and VI-UCLB by Bai & Jin
(2020), OMVI-SM by Xie et al. (2020), and Nash Q/V-learning by Bai et al. (2020). The lower
bound was proved by Jin et al. (2018); Domingues et al. (2020).
	Algorithm	Task- Agnostic	Tt - Regret	Sample Complexity	Output Policy
Model -based	VI-explore	Yes		O(H 5S 2AB∕e2)	a single Markov policy
	VI-ULCB		-Yes	O(H 4S 2AB∕e2)	
	OMVI-SM		Yes	~O(H 4S3A3B3∕e2)-	
	Algorithm 2	Yes		O(H 4SAB∕e2)	
	Algorithm 1		-Yes	O(H 3SAB∕e2)	
Model -free	Nash Q-learning			C)(H 5SAB∕e2)	nested mixture of Markov policies
	Nash V-learning			~O(H 6 S(A+B)∕e2)―	
	Lower Bound	-	-	~Ω(H3 S(A + B)∕e2)―	-
been well developed for multi-agent RL such as friend-or-foe Q-Learning (Littman, 2001) and Nash
Q-Learning (Hu & Wellman, 2003).
While both model-based and model-free algorithms have been shown to be provably efficient in
multi-agent RL in a recent line of work (Bai & Jin, 2020; Xie et al., 2020; Bai et al., 2020), a
more precise understanding of the optimal sample complexities within these two types of algorithms
(respectively) is still lacking. In the specific setting of two-player zero-sum Markov games, the
current best sample complexity for model-based algorithms is achieved by the VI-ULCB (Value
Iteration with Upper/Lower Confidence Bounds) algorithm (Bai & Jin, 2020; Xie et al., 2020): In
a tabular Markov game with S states, {A, B} actions for the two players, and horizon length H,
VI-ULCB is able to find an e-approXimate Nash equilibrium policy in O(H4S2AB∕e2) episodes of
game playing. However, compared with the information-theoretic lower bound Ω(H3S(A+B)/e2),
this rate has suboptimal dependencies on all of H, S, and A,B. In contrast, the current best sample
complexity for model-free algorithms is achieved by Nash V-Learning (Bai et al., 2020), which finds
an e-approximate Nash policy in O(H6S(A + B)∕e2) episodes. Compared with the lower bound,
this is tight except for a poly(H) factor, which may seemingly suggest that model-free algorithms
could be superior to model-based ones in multi-agent RL. However, such a conclusion would be
in stark contrast to the single-agent MDP setting, where it is known that model-based algorithms
are able to achieve minimax optimal sample complexities (Jaksch et al., 2010; Azar et al., 2017).
It naturally arises whether model-free algorithms are indeed superior in multi-agent settings, or
whether the existing analyses of model-based algorithms are not tight. This motivates us to ask the
following research question:
Question: How sample-efficient are model-based algorithms in multi-agent RL?
In this paper, we advance the theoretical understandings of multi-agent RL by presenting a sharp
analysis of model-based algorithms on Markov games. Our core contribution is the design of a new
model-based algorithm Optimistic Nash Value Iteration (Nash-VI) that achieves an almost optimal
sample complexity for zero-sum Markov games and improves significantly over existing model-
based approaches. We summarize our main contributions as follows. A comparison between our
and prior results can be found in Table 1.
•	We design a new model-based algorithm Optimistic Nash Value Iteration (Nash-VI) that provably
finds e-approximate Nash equilibria for Markov games in O(H3SAB∕e2) episodes of game playing
(Section 3). This improves over the best existing model-based algorithm by O(HS) and is the first
algorithm that matches the sample complexity lower bound except for a O(min {A,B}) factor,
showing that model-based algorithms can indeed achieve an almost optimal sample complexity.
Further, unlike state-of-the-art model-free algorithms such as Nash V-Learning (Bai et al., 2020),
this algorithm achieves in addition a O(√T) regret bound, and outputs a simple Markov policy
(instead of a nested mixture of Markov policies as returned by Nash V-Learning).
2
Under review as a conference paper at ICLR 2021
•	We design an alternative algorithm Optimistic Value Iteration with Zero Reward (VI-Zero) that is
able to perform task-agnostic (reward-free) learning for multiple Markov games sharing the same
transition (Section 4). For N > 1 games with the same transition and different (known) rewards,
VI-Zero can find e-apprOximate Nash policy for all games simultaneously in O(H4SAB log N/e2)
episodes of game playing, which scales logarithmically in the number of games.
•	We design the first line of sample-efficient algorithms for multi-player general-sum Markov
games. In a multi-player game with M players and Ai actions per player, we show that an e near-
optimal policy can be found in O(H4S2 Qi∈[M] Ai/e2) episodes, where the desired optimality can
be either one of Nash equilibrium, correlated equilibrium (CE), or coarse correlated equilibrium
(CCE). We achieve this guarantee by either a multi-player version of Nash-VI or a multi-player
version of reward-free value iteration (Section 5 & Appendix C).
Due to space limit, we defer a detailed survey of related works to Appendix A.
2 Preliminaries
In this paper, we consider Markov Games (MGs, Shapley, 1953; Littman, 1994), which are also
known as stochastic games in the literature. Markov games are the generalization of standard
Markov Decision Processes (MDPs) into the multi-player setting, where each player seeks to max-
imize her own utility. For simplicity, in this section we describe the important special case of two-
player zero-sum games, and return to the general formulation in Appendix C.
Formally, we consider the tabular episodic version of two-player zero-sum Markov game, which we
denote as MG(H, S, A, B, P, r). Here H is the number of steps in each episode, S is the set of states
with |S | ≤ S, (A, B) are the sets of actions of the max-player and the min-player respectively with
|A| ≤ A and |B| ≤ B, P = {Ph}h∈[H] is a collection of transition matrices, so that Ph(∙∣s, a, b)
gives the distribution of the next state if action pair (a, b) is taken at state S at step h, and r =
{rh}h∈[H] is a collection of reward functions, where rh : S × A × B → [0, 1] is the deterministic
reward function at step h.1 This reward represents both the gain of the max-player and the loss of
the min-player, making the problem a zero-sum Markov game.
In each episode of this MG, we start with a fixed initial state S1. At each step h ∈ [H], both
players observe state Sh ∈ S, and pick their own actions ah ∈ A and bh ∈ B simultaneously.
Then, both players observe the actions of their opponent, receive rewardrh(Sh, ah, bh), and then the
environment transitions to the next state Sh+ι 〜 Ph(∙∣Sh,αh,bh). The episode ends when SH+1 is
reached.
Policy, value function. A (Markov) policy μ of the max-player is a collection of H functions
{μh : S → ∆∕}h∈[H], each mapping from a state to a distribution over actions. (Here △/ is
the probability simplex over action set A.) Similarly, a policy ν of the min-player is a collection
of H functions {νh : S → ∆B}h∈[H]. We use the notation μh,(α∣s) and νh(b∣s) to represent the
probability of taking action a or b for state S at step h under Markov policy μ or V respectively.
We use Vμ,ν: S → R to denote the value function at step h under policy μ and V, so that Vμ,ν(s)
gives the expected cumulative rewards received under policy μ and V, starting from S at step h:
Vμ,ν(S)= Eμ,ν [PH0=hr%0(Sh,a，h，,bh')[h = Sb
(1)
We also define Qμ,ν : S × A × B → R to be the Q-value function at step h so that Qμ,ν(s, a, b)
gives the cumulative rewards received under policy μ and V, starting from (s, a, b) at step h:
Qμ, (s, α, b) ：= Eμ,ν [ Ph, = h rh0 (ShO, ah0, bh0)卜h = s, ah = a, bh = b] ∙	(2)
For simplicity, We define operator Ph as [PhV](s, a, b) := Es，〜Ph(∙∣s,a,b) V(s0) for any value func-
tion V. We also use notation [D∏Q](s) := E(a,b)〜∏(∙,∙∣s)Q(s, a, b) for any action-value function Q.
By definition of value functions, we have the Bellman equation
QhV (s, a, b) = (rh + PhVμl1)(s, a, b),	vhμ,ν (S) = (Dμh×Vh Qμ,ν )(S)
1We assume the rewards in [0, 1] for normalization. Our results directly generalize to randomized reward
functions, since learning the transition is more difficult than learning the reward.
3
Under review as a conference paper at ICLR 2021
for all (s, a, b, h) ∈ S×A×B× [H], and at the (H + 1)th step We have VHjι (s) = 0 for all S ∈ S.
Best response and Nash equilibrium. For any policy of the max-player μ, there exists a best
response of the min-player, which is a policy V * (μ) satisfying V^,v (μ)(s) = inf V Vh,ν (S) for any
(s, h) ∈ S× [H]. We denote Vμ,* :=叱," (μ). By symmetry, we can also define μ*(ν) and V*,ν. It
is further known (cf. (Filar & Vrieze, 2012)) that there exist policies μ?, V? that are optimal against
the best responses of the opponents, in the sense that
Vf,t(s)=supμ MS),	Vy (s) = inf v V^V (s),	for all (s,h).
We call these optimal strategies (μ?, V?) the Nash equilibrium of the Markov game, which satisfies
the following minimax equation 2:
suPμinf V VrV (S)= Vh ,V (S)=inf v suPμ VrV (S).
Intuitively, a Nash equilibrium gives a solution in which no player has anything to gain by changing
only her own policy. We further abbreviate the values of Nash equilibrium V^ ,v and Qhj as V?
and Q?h . We refer readers to Appendix D for Bellman optimality equations for (the value functions
of) the best responses and the Nash equilibrium.
Learning Objective. We measure the suboptimality of any pair of general policies (μ, V) using the
gap between their performance and the performance of the optimal strategy (i.e., Nash equilibrium)
when playing against the best responses respectively:
VF(SI)- V1μ,t(S1) =	(Si) — Vi?(si)] + [V1?(S1) — VPt(S1)]
Definition 1 (e-apPrOXimate Nash equilibrium). A pair of general policies (μ, V) is an e-
approximate Nash equilibrium, if %t°(si) — Vf,t(Si) ≤ e.
Definition 2 (Regret). Let (μk, Vk) denote the policies deployed by the algorithm in the kth episode.
After a total of K episodes, the regret is defined as
K
Regret(K) = X(Vt,"k — Vμk ,t)(sι)
k=1
One goal of reinforcement learning is to design algorithms for Markov games that can find an
e-approximate Nash equilibrium using a number of episodes that is small in its dependency on
S, A, B, H as well as 1/e (PAC sample complexity bound). An alternative goal is to design algo-
rithms for Markov games that achieves regret that is sublinear in K, and polynomial in S, A, B, H
(regret bound). We remark that any sublinear regret algorithm can be directly converted to a
polynomial-sample PAC algorithm via the standard online-to-batch conversion (see e.g., Jin et al.
(2018)).
3	Optimistic Nash Value Iteration
In this section, we present our main algorithm—Optimistic Nash Value Iteration (Nash-VI), and
provide its theoretical guarantee.
3.1	Algorithm description
We describe our Nash-VI Algorithm 1. In each episode, the algorithm can be decomposed into two
parts.
•	Line 3-13 (Optimistic planning from the estimated model): Performs value iteration with
bonus using the empirical estimate of the transition P, and computes a new (joint) policy π
which is “greedy” with respect to the estimated value functions;
2The minimax theorem here is different from the one for matrix games, i.e. maxφ minψ φ>Aψ =
minψ max. φ>Aψ for any matrix A, since here Vμ,ν (S) is in general not bilinear in μ, ν.
4
Under review as a conference paper at ICLR 2021
•	Line 16-19 (Play the policy and update the model estimate): Executes the policy π, collects
samples, and updates the estimate of the transition P.
At a high-level, this two-phase strategy is standard in the majority of model-based RL algorithms,
and also underlies provably efficient model-based algorithms such as UCBVI for single-agent
(MDP) setting (Azar et al., 2017) and VI-ULCB for the two-player Markov game setting (Bai &
Jin, 2020). However, VI-ULCB has two undesirable drawbacks: the sample complexity is not tight
in any of H , S, and A, B dependency, and its computational complexity is PPAD-complete (a com-
plexity class conjectured to be computationally hard (Daskalakis, 2013)).
As we elaborate in the following, our Nash-VI algorithm differs from VI-ULCB in a few important
technical aspects, which allows it to significantly improve the sample complexity over VI-ULCB,
and ensures that our algorithm terminates in polynomial time.
Before digging into explanations of techniques, We remark that line 14-15 is only used for computing
the output policies. It chooses policy ∏out to be the policy in the episode with minimum gap (Vι -
V 1)(s1). Our final output policies (μout, νout) are simply the marginal policies of πout. That is, for
all (s,h) ∈S× [H ], μhut(∙∣s) ：= Pb∈B ∏hut(∙,b∣s),and VhUt(忖：=Pa∈A ∏hut(a, ∙∣s).
3.1.1	Overview of techniques
Auxiliary bonus γ. The major improvement over VI-ULCB (Bai & Jin, 2020) comes from the
use ofa different style of bonus term γ (line 8), in addition to the standard bonus β (line 7), in value
iteration steps (line 9-10). This is also the main technical contribution of our Nash-VI algorithm.
This auxiliary bonus Y is computed by applying the empirical transition matrix Ph to the gap at the
next step Vh+ι - V八十「This is very different from standard bonus β, which is typically designed
according to the concentration inequalities.
The main purpose of these value iteration steps (line 9-10) is to ensure that the estimated values
Qh and Qh are with high probability the upper bound and the lower bound of the Q-value of the
current policy when facing best responses (see Lemma 20 and 22 for more details) 3. Todo so, prior
work (Bai & Jin, 2020) only adds bonus β, which needs to be as large as Θ(，S/t). In contrast, the
inclusion of auxiliary bonus Y in our algorithm allows a much smaller choice for bonus β——which
scales only as O(A/1/t)——while still maintaining valid confidence bounds. This technique alone
brings down the sample complexity to O(H4SAB∕e2), removing an entire S factor compared to
VI-ULCB. Furthermore, the coefficient in Y is only c/H for some absolute constant c, which ensures
that the introduction of error term Y would hurt the overall sample complexity only up to a constant
factor.
Bernstein concentration. Our Nash-VI allows two choices of the bonus function β =
BONUS(t,σ2):
Hoeffding type: C(PH2ι∕t + H2Sι∕t),	Bernstein type: c(pσ2ι∕t + H2Sι∕t). (3)
where σ2 is the estimated variance, ∣ is the logarithmic factors and C is absolute constant. The V in
line 7 is the empirical variance operator defined as VbhV = PbhV 2 - (PbhV)2 for any V ∈ [0, H]S.
The design of both bonuses stem from the Hoeffding and Bernstein concentration inequalities. Fur-
ther, the Bernstein bonus uses a sharper concentration, which saves an H factor in sample complexity
compared to the Hoeffding bonus (similar to the single-agent setting (Azar et al., 2017)). This fur-
ther reduces the sample complexity to O(H3SAB∕e2) which matches the lower bound in all H, S, e
factors.
Coarse Correlated Equalibirum (CCE). The prior algorithm VI-ULCB (Bai & Jin, 2020) com-
putes the “greedy” policy with respect to the estimated value functions by directly computing the
Nash equilibrium for the Q-value at each step h. However, since the algorithm maintains both the
upper confidence bound and lower confidence bound of the Q-value, this leads to the requirement
3We remark that the current policy is stochastic. This is different from the single-agent setting, where the
algorithm only seeks to provide an upper bound of the value of the optimal policy where the optimal policy is
not random. Due to this difference, the techniques of Azar et al. (2017) cannot be directly applied here.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Optimistic Nash Value Iteration (Nash-VI)
1:	Initialize: for any (s, a, b, h), Qh(s, a, b) J H, Q.s, a, b) J 0, △ J H, Nh(s, a, b) J 0.
2:	for episode k = 1,..., K do
3:	for step h = H, H - 1, . . . , 1 do
4:	for (s, a, b) ∈ S × A × B do
5:	t J Nh (s, a, b).
6:	if t > 0 then
7：	β J BONUS(t, Vh[(Vh+ι + Vh+ι)∕2](s,a,b)).
， ，——、^ ，-- -- 、， -、
8:	Y J (c/H)Ph(vh+1 - VJh+ι)(s, a, b).
9:	Qh(s, a, b) J min{(rh + PhVh+ι)(s, a, b) + Y + β, H}.
10:	Qj(s, a, b) J max{(rh + PhVh+ι)(s, a, b) — Y — β, 0}.
11:	fθr S ∈ S do
12:	Πh(∙, ∙∣s) J CCE(Qh(s, ∙, ∙),Qh(s,∙,∙)).
13:	_Vh(s) J (DnhQh)(s)；	Vh(s) J (DnhQh)(s).
14:	if (Vι — V 1)(s1) < △ then
15:	△ J (Vι — V 1)(s1) and πout J π.
16:	for step h = 1, . . . , H do
17:	take action (ah, bh)〜∏h(∙, ∙∣sj), observe reward rh and next state sj+i.
18:	add 1 to Nh(sh,ah,bh) andNh(sh, ah, bh, sh+1).
19:	Ph(∙∣Sh, ah, bh) J Nh(sh, ah, bh, ∙)∕Nh(sh, ah, bh).
20:	Output (μout, Vout) that are the marginal policies of πout.
to compute the Nash equilibrium for a two-player general-sum matrix game, which is in general
PPAD-complete (Daskalakis, 2013).
To overcome this computational challenge, we compute a relaxation of the Nash equilibrium—
Coarse Correlated Equalibirum (CCE)—instead, a technique first introduced by Xie et al. (2020)
to address reinforcement learning problems in Markov Games. Formally, for any pair of matrices
Q, Q ∈ [0, H]a×b, CCE(Q, Q) returns a distribution π ∈ Δa×b such that
E(a,b)〜∏Q(a,b) ≥ max E(a,b)〜nQ(a*,b),	E")〜∏Q(a, b) ≤ min 跖⑸〜∏Q(a,b?). (4)
Intuitively, in a CCE the players choose their actions in a potentially correlated way such that no one
can benefit from unilateral unconditional deviation. A CCE always exists, since Nash equilibrium
is also a CCE and a Nash equilibrium always exists. Furthermore, a CCE can be computed by
linear programming in polynomial time. We remark that different from Nash equilibrium where the
policies of each player are independent, the policies given by CCE are in general correlated for each
player. Therefore, executing such a policy (line 17) requires the cooperation of two players.
3.2 Theoretical guarantees
Now we are ready to present the theoretical guarantees for Algorithm 1. We let πk denote the policy
computed in line 12 in the kth episode, and μk, νk denote the marginal policy of πk for each player.
Theorem 3 (Nash-VI with Hoeffding bonus). For any p ∈ (0, 1], letting ι = log(SABT/p), then
with probability at least 1 — p, Algorithm 1 with Hoeffding type bonus (3) (with some absolute c > 0)
achieves:
• (V*,ν —Vμ ,*)(sι) ≤ Jifthenumberofepisodes K ≥ Ω(H4SABι∕e2 +H3S2ABι2∕e).
• Regret(K) = Pk=I(Vt'" — Vμk't)(s1) ≤ O(√H3SABTi + H3S2ABι2).
Theorem 3 provides both a sample complexity bound and a regret bound for Nash-VI to find
an e-approximate Nash equilibrium. For small e ≤ H∕(Sι), the sample complexity scales as
O(H4SAB∕e2). Similarly, for large T ≥ H3S3ABi3, the regret scales as O(√H3SABT), where
T = KH is the total number of steps played within K episodes. Theorem 3 is significant in that it
6
Under review as a conference paper at ICLR 2021
improves the sample complexity of the model-based algorithm in Markov games from S2 to S (and
the regret from S to √S). This is achieved by adding the new auxiliary bonus Y in value iteration
steps as explained in Section 3.1. The proof of Theorem 3 can be found in Appendix F.1.
Our next theorem states that when using Bernstein bonus instead of Hoeffding bonus as in (3), the
sample complexity OfNaSh-VI algorithm can be further improved bya H factor in the leading order
term (and the regret improved by a √H factor).
Theorem 4 (Nash-VI with the Bernstein bonus). For any p ∈ (0, 1], letting ι = log(SABT/p),
then with probability at least 1 - p, Algorithm 1 with Bernstein type bonus (3) (with some absolute
c > 0) achieves:
•	(%’,ν —Vi ,*)(sι) ≤ Jifthenumberofepisodes K ≥ Ω(H 3SABι∕e2 +H 3S2ABι2∕e).
•	Regret(K) = Pk=I(V1t" - Vμk#)(si) ≤ O(√H2SABTl + H3S2ABi2).
Compared with the information-theoretic sample complexity lower bound Ω(H3S(A + B)ι∕e2)
and regret lower bound Ω(,H2S(A + B)T) (Bai & Jin, 2020), when e is small, Nash-VI with
Bernstein bonus achieves the optimal dependency on all of H, S, e up to logarithmic factors in both
the sample complexity and the regret, and the only gap that remains open is a AB∕(A + B) ≤
min {A, B} factor. The proof of Theorem 4 can be found in Appendix F.2.
Comparison with model-free approaches. Different from our model-based approach, a recently
proposed model-free algorithm Nash V-Learning (Bai et al., 2020) achieves sample complexity
O(H6S(A + B)∣∕e2), which has a tight (A + B) dependency on A, B. However, our Nash-VI
has the following important advantages over Nash V-Learning: 1. Our sample complexity has a bet-
ter dependency on horizon H; 2. Our algorithm outputs a single pair of Markov policies (μout, Vout)
while their algorithm outputs a generic history-dependent policy that can be only written as a nested
mixture of Markov policies; 3. The model-free algorithms in Bai et al. (2020) cannot be directly
modified to obtain a √T-regret (so that the exploration policies can be arbitrarily poor), while our
model-based algorithm has the √T-regret guarantee. We comment that although both Nash-VI and
Nash V-Learning have polynomial running time, the later enjoys a better computational complexity
because Nash-VI requires to solve LPs for computing CCEs in each episode.
4	Reward-free Learning
In this section, we modify our model-based algorithm Nash-VI for the reward-free exploration
setting (Jin et al., 2020b), which is also known as the task-agnostic (Zhang et al., 2020b) or
reward-agnostic setting. Reward-free learning has two phases: In the exploration phase, the agent
first collects a dataset of transitions D = {(sk,h, ak,h, bk,h, sk,h+1)}(k,h)∈[K]×[H] from a Markov
game M without the guidance of reward information. After the exploration, in the planning
phase, for each task i ∈ [N], D is augmented with stochastic reward information to become
Di = {(sk,h, ak,h, bk,h, sk,h+1, rk,h)}(k,h)∈[K]×[H], where rk,h is sampled from an unknown re-
ward distribution with expectation equal to rhi (sk,h, ak,h, bk,h). Here, we use ri to refer to the
unknown reward function of the ith task. The goal is to compute nearly-optimal policies for N tasks
under M, given the augmented datasets.
There are strong practical motivations for considering the reward-free setting. First, in applications
such as robotics, we face multiple tasks in sequential systems with shared transition dynamics (i.e.
the world) but very different rewards. There, we prefer to learn the underlying transition independent
of reward information. Second, from the algorithm design perspective, decoupling exploration and
planning (i.e. performing exploration without reward information) can be valuable for designing
new algorithms in more challenging settings (e.g. with function approximation).
Due to space limits, we defer the description of our algorithm Optimistic Value Iteration with Zero
Reward (VI-Zero, Algorithm 2) to Appendix B and only state its theoretical guarantees here. The
following theorem claims that the empirical transition Pout outputted by VI-Zero is close to the true
transition P, in the sense that any Nash equilibrium of the M(P, rbi) (i ∈ [N]) is also an approximate
7
Under review as a conference paper at ICLR 2021
Nash equilibrium of the true underlying Markov game M(P, ri), where rbi is the empirical estimate
of ri computed using Di .
Theorem 5 (Sample complexity of VI-Zero). There exists an absolute constant c, for any p ∈ (0, 1],
e ∈ (0, H], N ∈ N, if we choose bonus βt = C(PH2 ι∕t + H2Sι∕t) with ι = ∖og(NSABT/p)
and K ≥ c(H4SABι∕e2 + H3S2 ABi2/e), then with probability at least 1 一 P, the output Pout
of Algorithm 2 has the following property: for any N fixed reward functions r1 , . . . , rN, a Nash
out i
equilibrium of Markov game M(Pout, rbi) is also an e-approximate Nash equilibrium of the true
Markov game M(P, ri) for all i ∈ [N].
Theorem 5 shows that, when e is small, VI-Zero only needs O(H4SAB∕e2) samples to learn an
estimate of the transition Pbout, which is accurate enough to learn the approximate Nash equilibrium
for any N fixed rewards. The most important advantage of reward-free learning comes from the
sample complexity only scaling polylogarithmically with respect to the number of tasks or reward
functions N. This is in sharp contrast to the reward-aware algorithms (e.g. Nash-VI), where the
algorithm has to be rerun for each different task, and the total sample complexity must scale linearly
in N. In exchange for this benefit, compared to Nash-VI, VI-Zero loses a factor of H in the leading
term of sample complexity since we cannot use Bernstein bonus anymore due to the lack of reward
information. VI-Zero also does not have a regret guarantee, since again without reward informa-
tion, the exploration policies are naturally sub-optimal. The proof of Theorem 5 can be found in
Appendix G.1.
Connections with reward-free learning in MDPs. Since MDPs are special cases of Markov
games, our algorithm VI-Zero directly applies to the single-agent setting, and yields a sample com-
plexity similar to existing results (Zhang et al., 2020b; Wang et al., 2020). However, distinct from
existing results which require both the exploration algorithm and the planning algorithm to be spe-
cially designed to work together, our algorithm allows an arbitrary planning algorithm as long as it
computes the Nash equilibrium of a Markov game with known transition and reward. Therefore, our
results completely decouple the exploration and the planning.
Lower bound for reward-free learning. Finally, we comment that despite the sample complexity
in Theorem 5 scaling as AB insteadofA+B, our next theorem states that unlike the general reward-
aware setting, this AB scaling is unavoidable in the reward-free setting. This reveals an intrinsic
gap between the reward-free and reward-aware learning: An A + B dependency is only achievable
via sampling schemes that are reward-aware. A similar lower bound is also presented in recent work
(Zhang et al., 2020a) for the discounted setting with a different hard instance construction.
Theorem 6 (Lower bound for reward-free learning of Markov games). There exists an absolute
constant C > 0 such that for any e ∈ (0, C], there exists a family of Markov games M(e) satisfying
that: for any reward-free algorithm A using K ≤ CH2SAB∕e2 episodes, there exists a Markov
game M ∈ M(e) such that if we run A on M and output policies (μ, V), then with probability at
least 1 /4, we have (%*,ν — D(sι) ≥ e.
This lower bound shows that the sample complexity in Theorem 5 is optimal in S, A, B, and e. The
proof of Theorem 6 can be found in Appendix G.3.
5	Multi-player general-sum games
We adapt our analysis to multi-player general-sum games and present the first lines of provably
efficient algorithms. Concretely, we design two model-based algorithms Multi-Nash-VI and Multi-
VI-Zero (Algorithm 3 and Algorithm 4) that can find an (e-approximate) {NASH, CE, CCE} equi-
librium for any multi-player general-sum Markov game in O(H4S2 Qm=I Ai∕e2) episodes of game
playing, where Ai is the number of actions for player i ∈ {1, . . . , m} (Theorem 15 and Theorem
16). Due to space limit, we defer the detailed setups, algorithms and results to Appendix C.
8
Under review as a conference paper at ICLR 2021
6	Conclusion
In this paper, we provided a sharp analysis of model-based algorithms for Markov games. Our
new algorithm Nash-VI can find an -approximate Nash equilibrium of a zero-sum Markov game
in O(H3SAB∕e2) episodes of game playing, which almost matches the sample complexity lower
bound except for the AB vs. A + B dependency. We also applied our analysis to derive new
efficient algorithms for task-agnostic game playing, as well as the first line of multi-player general-
sum Markov games. There are a number of compelling future directions to this work. For example,
can we achieve A + B instead of AB sample complexity for zero-sum games using model-based
approaches (thus closing the gap between lower and upper bounds)? How can we design more
efficient algorithms for general-sum games with better sample complexity (e.g., O(S) instead of
O(S2 ))? We leave these problems as future work.
References
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272, 2017.
Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. arXiv
preprint arXiv:2002.04017, 2020.
Yu Bai, Chi Jin, and Tiancheng Yu. Near-optimal reinforcement learning with self-play. arXiv
preprint arXiv:2006.12007, 2020.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and
Igor Mordatch. Emergent tool use from multi-agent autocurricula. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
SkxpxJBKwS.
Kimmo Berg and Tuomas Sandholm. Exclusion method for finding nash equilibrium in multi-player
games. In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent
Systems, pp. 1417-1418, 2016.
Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(Oct):213-231, 2002.
Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds
for episodic reinforcement learning. In Advances in Neural Information Processing Systems, pp.
5713-5723, 2017.
Constantinos Daskalakis. On the complexity of approximating a nash equilibrium. ACM Transac-
tions on Algorithms (TALG), 9(3):23, 2013.
Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic
reinforcement learning in finite mdps: Minimax lower bounds revisited. arXiv preprint
arXiv:2010.03531, 2020.
Jerzy Filar and Koos Vrieze. Competitive Markov decision processes. Springer Science & Business
Media, 2012.
Thomas Dueholm Hansen, Peter Bro Miltersen, and Uri Zwick. Strategy iteration is strongly poly-
nomial for 2-player turn-based stochastic games with a constant discount factor. Journal of the
ACM (JACM), 60(1):1-16, 2013.
Junling Hu and Michael P Wellman. Nash q-learning for general-sum stochastic games. Journal of
machine learning research, 4(Nov):1039-1069, 2003.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.
Zeyu Jia, Lin F Yang, and Mengdi Wang. Feature-based q-learning for two-player stochastic games.
arXiv preprint arXiv:1906.00423, 2019.
9
Under review as a conference paper at ICLR 2021
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems,pp. 4868-4878, 2018.
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial
markov decision processes with bandit feedback and unknown transition. arXiv preprint
arXiv:1912.01192, 2019.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. arXiv preprint arXiv:2002.02794, 2020a.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for
reinforcement learning. arXiv preprint arXiv:2002.02794, 2020b.
Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Ma-
chine learning, 49(2-3):209-232, 2002.
Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang. Linear last-iterate convergence
for matrix games and stochastic games. arXiv preprint arXiv:2006.09517, 2020.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157-163. Elsevier, 1994.
Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, pp. 322-
328, 2001.
Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V Vazirani. Algorithmic Game Theory.
Cambridge University Press, 2007.
OpenAI. Openai five. https://blog.openai.com/openai-five/, 2018.
Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv
preprint arXiv:1608.02732, 2016.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions. arXiv preprint arXiv:1402.0635, 2014.
Aviv Rosenberg and Yishay Mansour. Online convex optimization in adversarial markov decision
processes. arXiv preprint arXiv:1905.07773, 2019.
Lloyd S Shapley. Stochastic games. Proceedings of the national academy of sciences, 39(10):
1095-1100, 1953.
Aaron Sidford, Mengdi Wang, Lin F Yang, and Yinyu Ye. Solving discounted stochastic two-player
games with near-optimal time and sample complexity. arXiv preprint arXiv:1908.11071, 2019.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354-359, 2017.
Alexander L Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L Littman. PAC model-
free reinforcement learning. In International Conference on Machine Learning, pp. 881-888,
2006.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, JUny-
oung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
Ruosong Wang, Simon S Du, Lin F Yang, and Ruslan Salakhutdinov. On reward-free reinforcement
learning with linear function approximation. arXiv preprint arXiv:2006.11274, 2020.
10
Under review as a conference paper at ICLR 2021
Chen-Yu Wei, Yi-Te Hong, and Chi-Jen Lu. Online reinforcement learning in stochastic games. In
Advances in Neural Information Processing Systems, pp. 4987-4997, 2017.
Qiaomin Xie, Yudong Chen, Zhaoran Wang, and Zhuoran Yang. Learning zero-sum simultaneous-
move markov games using function approximation and correlated equilibrium. arXiv preprint
arXiv:2002.07066, 2020.
Yasin Abbasi Yadkori, Peter L Bartlett, Varun Kanade, Yevgeny Seldin, and Csaba Szepesvari.
Online learning in markov decision processes with adversarially chosen transition probability
distributions. In Advances in neural information processing systems, pp. 2508-2516, 2013.
Kaiqing Zhang, Sham M Kakade, Tamer Bayar, and Lin F Yang. Model-based multi-agent rl in
zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461,
2020a.
Xuezhou Zhang, Yuzhe Ma, and Adish Singla. Task-agnostic exploration in reinforcement learning.
arXiv preprint arXiv:2006.09497, 2020b.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via
reference-advantage decomposition. arXiv preprint arXiv:2004.10019, 2020c.
Alexander Zimin and Gergely Neu. Online learning in episodic markovian decision processes by
relative entropy policy search. In Advances in neural information processing systems, pp. 1583-
1591, 2013.
11
Under review as a conference paper at ICLR 2021
A Related Work
Markov games. Markov games (or stochastic games) are proposed in the early 1950s (Shapley,
1953). They are widely used to model multi-agent RL. Learning the Nash equilibria of Markov
games has been studied in Littman (1994; 2001); Hu & Wellman (2003); Hansen et al. (2013); Lee
et al. (2020), where the transition matrix and reward are assumed to be known, or in the asymptotic
setting where the number of data goes to infinity. These results do not directly apply to the non-
asymptotic setting where the transition and reward are unknown and only a limited amount of data
are available for estimating them.
Another line of work assumes certain strong reachability assumptions under which sophisticated
exploration strategies are not required. A prevalent approach is to assume access to simulators (gen-
erative models) that enable the agent to directly sample transition and reward information for any
state-action pair. In this setting, Jia et al. (2019); Sidford et al. (2019); Zhang et al. (2020a) provide
non-asymptotic bounds on the number of calls to the simulator for finding an approximate Nash
equilibrium. Wei et al. (2017) studies Markov games under an alternative assumption that no matter
what strategy one agent sticks to, the other agent can always reach all states by playing a certain
policy.
Non-asymptotic guarantees without reachability assumptions. The recent work of Bai & Jin
(2020); Xie et al. (2020) provide the first line of non-asymptotic sample complexity guarantees
on learning Markov games without these reachability assumptions, in which exploration is essen-
tial. However, both results suffer from highly suboptimal sample complexity. The results of Xie
et al. (2020) also apply to the linear function approximation setting. More recently, two model-free
algorithms—Nash Q-Learning and Nash V-Learning—are shown to achieve better sample complex-
ity guarantees (Bai et al., 2020). In particular, the Nash V-learning algorithm achieves the near-
optimal dependence on S, A and B . However, the dependence on H is worse than our results and
the output policy is a nested mixture, which is hard to implement. We compare our results with
existing non-asymptotic guarantees in Table 1.
We remark that the classical R-max algorithm (Brafman & Tennenholtz, 2002) also provides prov-
able guarantees for learning Markov games. However, Brafman & Tennenholtz (2002) uses a weaker
definition of regret (similar to the online setting in Xie et al. (2020)), and consequently their result
does not imply any sample complexity result for finding Nash equilibrium policies.
Adversarial MDPs. Another way to model the multi-player bahavior is to use adversarial MDPs.
Most work in this line considers the setting with adversarial rewards (Zimin & Neu, 2013; Rosenberg
& Mansour, 2019; Jin et al., 2019), where the reward can be manipulated by an adversary arbitrarily
and the goal is to compete with the optimal (stationary) policy in hindsight. Adversarial MDP
with changing dynamics is computationally hard even under full-information feedback (Yadkori
et al., 2013). Notice these results do not directly imply provable self-play algorithms in our setting,
because the opponent in Markov games can affect both the reward and the transition.
Single-agent RL. There is a rich literature on reinforcement learning in MDPs (see e.g., Jaksch
et al., 2010; Osband et al., 2014; Azar et al., 2017; Dann et al., 2017; Strehl et al., 2006; Jin et al.,
2018). MDP is a special case of Markov games, where only a single agent interacts with a stochastic
environment. For the tabular episodic setting with nonstationary dynamics and no simulators, the
best sample complexity is O(H3SA∕e2), achieved by model-based algorithm in Azar et al. (2017)
and model-free algorithms in Zhang et al. (2020c), respectively, where S is the number of states,
A is the number of actions, H is the length of each episode. Both of them match the lower bound
Ω(H3SA∕e2) (JakSch et al., 2010; OSband & Van Roy, 2016; Jin et al., 2018).
Reward-free and task-agnostic exploration. Jin et al. (2020a) proposes anew paradigm of learn-
ing an MDP, which they called reward-free exploration. In this setting, the agent goes through a
two-stage process. In the exploration phase the agent can interacts with the environment without
knowing the reward function and in the planning phase the reward function is given and the agent
needs output a policy. The goal is to make the output policy near optimal for any given reward func-
tion. A closely related setting is task-agnostic learning, where the reward function is determined at
the very beginning but not revealed until the planning phase. Notice algorithms for task-agnostic
12
Under review as a conference paper at ICLR 2021
Algorithm 2 Optimistic Value Iteration with Zero Reward (VI-Zero)
Require: Bonus βt .
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
Initialize: for any (s,a,b,h), Vh(S,a,b) J H, ∆ J H, Nh(s, a, b) J 0.
for episode k = 1, . . . , K do
for step h = H, H - 1, . . . , 1 do
for (s, a, b) ∈ S × A × B do
t J Nh (s, a, b).
if t > 0 then
Qeh(s, a, b) J min{(PbhVeh+1)(s, a, b) + βt, H}.
for s ∈ S do
πh(s) J arg max(a,b)∈A×B Qh(s, a, b).
Veh(s) J (Dπh Qe h)(s).
if Ve1 (s1) < ∆ then
.	二，	、	.^...x	^
∆ J Ve1(s1) and Pbout J Pb.
for step h = 1, . . . , H do
take action (ah, bh)〜∏h(∙, ∙∣Sh), observe next state sh+ι.
add 1 to Nh(sh, ah, bh) and Nh(sh, ah, bh, sh+1).
Ph(∙∣Sh, ah, bh) J Nh(sh, ah, bh, ∙)∕Nh(sh, ah, bh).
Output Pbout.
learning can also be transferred to reward-free exploration by taking union bound w.r.t. different
possible reward function. In Table 1, VI-explore (Bai & Jin, 2020) and Algorithm 2 can also be
applied to this setting.
Jin et al. (2020a) also proposes an algorithm, which first finds a covering policy to maximize the
probability to reach each state separately and then collects data following this policy. Zhang et al.
(2020b) takes a different approach by first runs the optimistic Q-learning algorithm (Jin et al., 2018)
with zero reward to explore the environment, and then they utilizes the trajectories collected to
compute a policy in an incremental manner. Wang et al. (2020) follows a simialr scheme, but studies
reward-free exploration in linear-parametrized MDPs.
B Optimistic Value Iteration with Zero Reward - VI-ZERO
We now describe our algorithm for reward-free learning in zero-sum Markov games.
Exploration phase. in the first phase of reward-free learning, we deploy algorithm Optimistic
Value iteration with Zero Reward (Vi-Zero, Algorithm 2). This algorithm differs from the reward-
aware Nash-Vi (Algorithm 1) in two important aspects. First, we use zero reward in the exploration
phase (Line 7), and only maintains an upper bound of the (reward-free) value function instead of
both upper and lower bounds. Second, our exploration policy is the maximizing (instead of CCE)
policy of the value function (Line 9). we remark that the Qh(s, a, b) maintained in the algorithm 2
is no longer an upper bound for any actual value function (as it has no reward), but rather a measure
of uncertainty or suboptimality that the agent may suffer—if she takes action (a, b) at state s and
step h, and makes decisions by utilizing the empirical estimate P in the remaining steps (see a
rigorous version of this statement in Lemma 27). Finally, the empirical transition P of the episode
that minimizes V1(s1) is outputted and passed to the planning phase.
Planning phase. After obtaining the estimate of tranisiton P, our planning algorithm is rather
simple. For the ith task, let rbi be the empirical estimate of ri computed using the ith augmented
dataset Di. Then we compute the Nash equilibrium of the Markov game M(P, rbi) with estimated
ii
transition P and reward rbi. Since both P and rbi are known exactly, this is a pure computation problem
without any sampling error and can be efficiently solved by simple planning algorithms such as the
vanilla Nash value iteration without optimism (see Appendix G.2 for more details).
13
Under review as a conference paper at ICLR 2021
C Multiplayer General-sum Markov Games
In this section, we extend both our model-based algorithms (Algorithm 1 and Algorithm 2) to the
setting of multiplayer general-sum Markov games, and present corresponding theoretical guarantees.
C.1 Problem formulation
A general-sum Markov game (general-sum MG) with m players is a tuple
MG(H, S, {Ai}im=1, P, {ri}im=1), where H, S denote the length of each episode and the state
space. Different from the two-player zero-sum setting, we now have m different action spaces,
where Ai is the action space for the ith player and |Ai| = Ai. We let a := (ai, ∙∙∙ , am,) denote
the (tuple of) joint actions by all m players. P = {Ph}h∈[H] is a collection of transition matrices,
so that Ph(∙∣s, a) gives the distribution of the next state if actions a are taken at state S at step h,
and ri = {rh,i}h∈[H] is a collection of reward functions for ith player, so that rh,i(S, a) gives the
reward received by the ith player if actions a are taken at state s at step h.
In this section, we consider three versions of equlibrium for general-sum MGs: Nash equilibrium
(NE), correlated equilibrium (CE), and coarse correlated equilibrium (CCE), all being standard so-
lution notions in games (Nisan et al., 2007). These three notions coincide on two-player zero-sum
games, but are not equivalent to each other on multi-player general-sum games; any one of them
could be desired depending on the application at hand. Below we introduce their definitions.
(Approximate) Nash equilibrium in general-sum MG. The policy of the ith player is denoted
as πi := πh,i : S → △Ai h∈[H] . We denote the product policy of all the players as π :=
∏ι x∙∙∙x ∏m, and denote the policy of the all the players except the ith player as ∏-i. We define
Vhn,i(s) as the expected cumulative reward that will be received by the ith player if starting at state
s at step h and all players follow policy π. For any strategy π-i, there also exists a best response
of the ith player, which is a policy μ'(π-i) satisfying Vhxi(n-i),n-i (s) = sup∏i V∏ii,n-i (s) for any
(s, h) ∈ S x [H]. We denote VhT-i ：= Vfi (n-i),n-i. The Q-functions of the best response can be
defined similarly.
Our first objective is to find an approximate Nash equilibrium of Markov games.
Definition 7 (e-approximate Nash equilibrium in general-sum MG). A product policy π is an e-
approximate Nash equilibrium if maxi∈[m] (V*in-i
—
V1π,i)(s1) ≤ .
The above definition requires the suboptimality gap (V,f-i - Vni)(Si) to be less than e for all
player i. This is consistent with the two-player case (De, finition 1) up to a constant of 2, since in
the two-player zero-sum setting, we have vni(si) = -Vn2(s1) for any product policy π = (μ,ν),
and therefore (VIy -邛1')®) ≤ 2maxi∈[2] (%]]-i - V1i)(sι) ≤ 2(VIi - V*i，)(si).We can
similarly define the regret.
Definition 8 (Nash-regret in general-sum MG). Let π k denote the (product) policy deployed by the
algorithm in the kth episode. After a total of K episodes, the regret is defined as
..E .十∏k.	k
RegretNash(K) =	max(V1,,i -i - V1n,i )(s1).
k=1 i∈[m]	,
(Approximate) CCE in general-sum MG. The coarse correlated equilibrium (CCE) is a relaxed
version of Nash equilibrium in which we consider general correlated policies instead of product
policies. Let A = Ai ×∙∙∙× Am denote thejoint action space.
Definition 9 (CCE in in general-sum MG). A (correlated) policy π := {πh(s) ∈ ∆A : (h, s) ∈
[H ] X S} is a CCE if maxi∈m] V^∏-i (S) ≤ Vtni(S) for all (s,h) ∈S× [H ].
Compared with a Nash equilibrium, a CEE is not necessarily a product policy, that is, we may not
have ∏h(s) ∈ △/I X …X ∆∕m. Similarly, we also define e-approXimate CCE and CCE-regret
below.
14
Under review as a conference paper at ICLR 2021
Definition 10 (-approximate CCE in general-sum MG). A policy π := {πh(s) ∈ ∆A : (h, s) ∈
[H] × S} is an e-approXimate CCE if maxi∈[m] (Vχ,^-i 一 VTni)(Sι) ≤ e.
Definition 11 (CCE-regret in general-sum MG). Let policy πk denote the (correlated) policy de-
ployed by the algorithm in the kth episode. After a total of K episodes, the regret is defined as
K	十k	k
RegretCCE(K) =	max(V1,,iπ-i 一 V1π,i )(s1).
k=1 i∈[m]	,
(Approximate) CE in general-sum MG. The correlated equilibrium (CE) is another relaxation
of the Nash equilibrium. To define CE, we first introduce the concept of strategy modification: A
strategy modification φ := {φh,s(a) ∈ Ai : (h, s, a) ∈ [H] × S × Ai} for player i is a set ofS × H
injective functions from Ai to itself. Let Φi denote the set of all possible strategy modifications for
player i.
One can compose a strategy modification φ with any Markov policy π and obtain a new policy φ π
such that when policy π chooses to play a := (a1, . . . , am) at state s and step h, policy φ πwill
play (a1, . . . , ai-1 , φh,s(ai), ai+1, . . . , am) instead.
Definition 12 (CE in general-sum MG). A policy π := {πh(s) ∈ ∆A : (h, s) ∈ [H] × S} is a CE
if maxi∈[m] maxφ∈Φi Vhφ,iπ(s) ≤ Vhπ,i(s) holds for all (s, h) ∈ S × [H].
Similarly, we have an approximate version ofCE and CE-regret.
Definition 13 (e-approximate CE in Markov games). A policy π := {πh(s) ∈ ∆A : (h, s) ∈
[H] × S} is an e-approximate CE if maxi∈[m] maxφ∈Φi (V1φ,iπ 一 V1π,i)(s1) ≤ e.
Definition 14 (CE-regret in multiplayer Markov games). Let product policy πk denote the policy
deployed by the algorithm in the kth episode. After a total of K episodes, the regret is defined as
RegretCE(K) = X max max( V1φ,iπk 一 V1π,ik)(s1).
i∈[m] φ	,
k=1
Relationship between Nash, CE, and CCE For general-sum MGs, we have {Nash} ⊆ {CE} ⊆
{CCE}, so that they form a nested set of notions of equilibria (Nisan et al., 2007). Indeed, one
can easily verify that if we restrict the choice of strategy modification φ to those consisting of only
constant functions, i.e., φh,s(a) being independent of a, Definition 12 will reduce to the definition
of CCE policy. In addition, any Nash equilibrium is a CE by definition. Finally, since a Nash
equilibrium always exists, so does CE and CCE.
C.2 Multiplayer optimistic Nash value iteration
Here we present the Multi-Nash-VI algorithm, which is an extension of Algorithm 1 for multi-player
general-sum Markov games.
The Equilibrium Subroutine. Our EQUILIBRIUM subroutine in Line 11 could be taken from
either one of the {NASH, CE, CCE} subroutines for one-step games. When using NASH, we com-
pute the Nash equilibrium of a one-step multi-player game (see, e.g., Berg & Sandholm (2016) for
an overview of the available algorithms); the worst-case computational complexity of such a sub-
routine will be PPAD-hard (Daskalakis, 2013). When using CE or CCE, we find CEs or CCEs
of the one-step games respectively, which can be solved in polynomial time using linear program-
ming. However, the policies found are not guaranteed to be a product policy. We remark that in
Algorithm 1 we used the CCE subroutine for finding Nash in two-player zero-sum games, which
seemingly contrasts the principle of using the right subroutine for finding the right equilibrium, but
nevertheless works as the Nash equilibrium and CCE are equivalent in zero-sum games.
Now we are ready to present the theoretical guarantees for Algorithm 3. We let πk denote the policy
computed in line 11 of Algorithm 3 in the kth episode.
Theorem 15 (Multi-Nash-VI). There exists an absolute constant c, for any p ∈ (0, 1], let ι =
log(SABT∕p) ,then with probability at least 1 — P, Algorithm 3 with bonus βt = c，SH 2ι∕t and
EQUILIB RIUM being one of {NASH, CE, CCE} satisfies (repsectively):
15
Under review as a conference paper at ICLR 2021
Algorithm 3 Multiplayer Optimistic Nash Value Iteration (Multi-Nash-VI)
1:	Initialize: for any (s, a, h, i), Qh,i(s, a) - H, Qhi(s, a) - 0, ∆ - H, Nh(s, a) - 0.
2:	for episode k = 1, . . . , K do
3:	for step h = H, H - 1, . . . , 1 do
4:	for (s, a) ∈ S ×Aι ×∙∙∙× Am do
5：	t — Nh(s, a);
6:	if t > 0 then
7:	for player i = 1, 2, . . . , m do
8：	Qh,i(s, a) - min{(rh,i + PhVh+ι,i)(s, a) + βt, H}.
9：	Qh,i(s, a) - max{(rh,i + PhVh+ι,i)(s, a) - βt, 0}.
10:	fθr S ∈ S do
11：	∏h(∙∣s) J EQUILIBRIUM(Qh,ι(s, ∙),Qh,2(s, ∙),…，Qh,M(s, ∙)).
12:	forplayer i = 1,2,二.，m do
13:	Vh,i(s) J (DnhQh,i)(s);	Vh,i(s) J (DnhQhA)
14:	if maxi∈[m] (Vι,i - V 1,i)(s1) < △ then
15:	△ J maXi∈[m](Vι,i - V 1,i)(s1) and ∏out J ∏.
16:	for step h = 1, . . . , H do
17:	take action ah,〜∏h(∙∣sh,), observe reward rh and next state sh+ι.
18:	add 1 to Nh(sh,ah) and Nh(sh,ah, sh+1).
19:	Ph(∙∣Sh, ah) J Nh(sh, ah, ∙)∕Nh(sh, ah).
20:	Output πout.
• πout is an -approximate {NASH,CE,CCE}, if the number of episodes K ≥
Ω(H4S2(Qm=I Ai)∣∕e2).
• Regret{Nash,CE,ccE}(K) ≤O(pH3S2(Qi=1 Ai)T∣).
In the situation where the Equilibrium subroutine is taken as Nash, Theorem 15 provides the
sample complexity bound of Multi-Nash-VI algorithm to find a -approximate Nash equilibrium
and its regret bound. Compared with our earlier result in two-player zero-sum games (Theorem 3),
here the sample complexity scales as S2H4 instead of SH3. This is because the auxiliary bonus
and Bernstein concentration technique do not apply here. Furthermore, the sample complexity is
proportional to Qim=1 Ai, which increases exponentially as the number of players increases.
Runtime of Algorithm 3 We remark that while the Nash guarantee is the strongest among the
three guarantees presented in Theorem 15, the runtime of Algorithm 3 in the Nash case is not guar-
anteed to be polynomial and in the worst case PPAD-hard (due to the hardness of the Nash sub-
routine). In contrast, the CE and CCE guarantees are weaker, but the corresponding algorithms are
guaranteed to finish in polynomial time.
C.3 Multiplayer reward-free learning
We can also generalize VI-Zero to the multiplayer setting and obtain Algorithm 4, Multi-VI-Zero,
which is almost the same as VI-Zero except that its exploration bonus βt is larger than that of VI-
Zero by a √S factor.
Similar to Theorem 5, we have the following theoretical guarantee claiming that any
{NASH,CCE,CE} of the M(P, rbi) (i ∈ [N]) is also an approximate {NASH,CCE,CE} of the true
Markov game M(P, ri), where Pbout is the empirical transition outputted by Algorithm 4 and rbi is
the empirical estimate of ri .
Theorem 16 (Multi-VI-Zero). There exists an absolute constant c, for any p ∈ (0, 1], ∈
(0, H], N ∈ N, if we choose bonus βt = CpH2Sι∕t with ι = log(NSABT∕p) and K ≥
c(H4S2 ( im=1 Ai)ι∕2), then with probability at least 1 - p, the output Pout of Algorithm 4 has
the following property: for any N fixed reward functions r1, . . . , rN, any {NASH,CCE,CE} of
16
Under review as a conference paper at ICLR 2021
Al 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17	gorithm 4 Multiplayer Optimistic Value Iteration with Zero Reward (Multi-VI-Zero) Initialize: for any (s, a, h), Vh(s, a) J H, ∆ J H, Nh(s, a) J 0. : for episode k = 1, . . . , K do : for step h = H, H - 1, . . . , 1 do for (s, a) ∈ S ×Aι ×∙∙∙× Am do :	t J Nh(S, a). :	if t > 0 then :	Qeh (s, a) J min{(PbhVeh+1 )(s, a) + βt, H}. :	for s ∈ S do πh(S) J argmaxa∈Aι×…×Am Qh(s, a). :	Veh (s) J (Dπh Qeh)(s). : if Ve1 (s1 ) < ∆ then .	r‹ ,	. ^...x	^ :	∆ J Ve1(s1) and Pbout J Pb. : for step h = 1, . . . , H do take action ah 〜 ∏h(∙, ∙∣Sh), observe next state Sh+ι. :	add 1 to Nh(sh,ah) andNh(sh,ah, sh+1). Ph(∙∣Sh, ah) J Nh(sh, ah, ∙)∕Nh(sh, ah). -	^...x : Output Pbout.
Markov game M(Pout, rbi) is also an -approximate {NASH,CCE,CE} of the true Markov game
M(P, ri) for all i ∈ [N].
The proof of Theorem 16 can be found in Appendix H.2. It is worth mentioning that the empirical
Markov game M(Pbout, rbi) may have multiple {Nash equilibria,CCEs,CEs} and Theorem 16 ensures
that all of them are -approximate {Nash equilibria,CCEs,CEs} of the true Markov game. Also,
note that the sample complexity here is quadratic in the number of states because We are using the
exploration bonus βt =，H2S∣∕t that is larger than usual by a √S factor.
D B ellman Equations for Markov Games
In this section, We present the Bellman equations for different types of values in Markov games.
Fixed policies. For any pair of Markov policy (μ, ν), by definition of their values in (1) (2), we
have the folloWing Bellman equations:
Qμ,ν (S, a, b) = (rh + PhVμ+I)(S, a, b),	Vμμ,ν (S) = (Dμh×Vh Qμ,ν )(S)
for all (s, a, b, h) ∈ S ×A×B × [H], where VH；1 (S) = 0 for all s ∈ S.
Best responses. For any Markov policy μ of the max-player, by definition, we have the following
Bellman equations for values of its best response:
Qh,t(s, a, b) = (rh + PhVh+1)(s, a, b),	VhU(S) = VinfJD“ax” Qμ,t)(s),
for all (s, a, b, h) ∈ S ×A×B × [H], where VH+1 (S) = 0 for all s ∈ S.
Similarly, for any Markov policy ν of the min-player, we also have the following symmetric version
of Bellman equations for values of its best response:
Qh，V (s,a,b) = (rh + Ph Vh+ι)(s,a,b),	Vftt,ν (s) = SUp(Dμ× Vh Qh,ν )(s).
μ∈∆A
for all (s, a, b, h) ∈ S ×A×B × [H], where VH+1 (s) = 0 for all s ∈ S.
17
Under review as a conference paper at ICLR 2021
Nash equilibria. Finally, by definition of Nash equilibria in Markov games, we have the following
Bellman optimality equations:
Q?h(s,a,b) =(rh + PhVh?+1)(s, a, b)
Vh?(S)=SUp inf (Dμ×νQh)(S)= inf sup (Dμ×νQh)(s).
μ∈∆A v∈δb	v∈δb μ∈∆A
for all (S, a, b, h) ∈ S × A × B × [H], where VH? +1 (S) = 0 for all S ∈ S.
E	Properties of Coarse Correlated Equilibrium
Recall the definition for CCE in our main paper (4), we restate it here after rescaling. For any pair
of matrices P, Q ∈ [0, 1]n×m, the subroutine CCE(P, Q) returns a distribution π ∈ ∆n×m that
satisfies:
E(a,b)〜∏P(a,b) ≥ max E(a,b)〜∏P(a*,b)	(5)
a?
E(a,b)〜πQ(a, b) ≤ min E(a,b)〜πQ(a, b )
We make three remarks on CCE. First, a CCE always exists since a Nash equilibrium for a general-
sum game with payoff matrices (P, Q) is also a CCE defined by (P, Q), and a Nash equilibrium
always exists. Second, a CCE can be efficiently computed, since above constraints (5) for CCE
can be rewritten as n + m linear constraints on π ∈ ∆n×m, which can be efficiently resolved by
standard linear programming algorithm. Third, a CCE in general-sum games needs not to be a Nash
equilibrium. However, a CCE in zero-sum games is guaranteed to be a Nash equalibrium.
Proposition 17. Let π = CCE(Q,Q), and (μ,ν) be the marginal distribution over both Players'
actions induced by π. Then (μ, ν) is a Nash equilibrium for payoff matrix Q.
Proof of Proposition 17. Let N ? be the value of Nash equilibrium for Q. Since π = CCE(Q, Q),
by definition, we have:
E(a,b)〜∏Q(a,b) ≥ maxE@b)〜∏Q(a?,b)=maxEb〜VQ(a*,b) ≥ N?
E(a,b)〜∏Q(a, b) ≤ minE(a,b)〜∏Q(a,b*) = minEa〜μQ(a, b*) ≤ N*
This gives:
max Eb 〜V Q(a*,b) = min Ea 〜μQ(a,b*) = N *
a?	b?
which finishes the proof.	□
Intuitively, a CCE procedure can be used in Nash Q-learning for findingan approximate Nash equi-
librium, because the values of upper confidence and lower confidence (Q and Q) will be eventually
very close, so that the preconditions of Proposition 17 becomes approximately satisfied.
F Proof for Section 3 — Optimistic NASH Value Iteration
F.1 Proof of Theorem 3
We denote Vk, Qk, ∏k, μk and Vk 4 for values and policies at the beginning of the k-th episode.
in particular, Nhk (S, a, b) is the number we have visited the state-action tuple (S, a, b) at the h-th
step before the k-th episode. Nhk (S, a, b, S0) is defined by the same token. Using this notation,
we can further define the empirical transition by Pbkh (S0|S, a, b) := Nhk(S, a, b, S0)/Nhk(S, a, b). if
Nhk(S, a, b) = 0, we set Pbkh (S0|S, a, b) = 1/S.
As a result, the bonus terms can be written as
βhk(S,a,b)
∣	IH2
m max{Nhk(s, a, b), 1} 十
H 2Sι
max{Nhk(s, a, b), 1}
4recall that (μh, Vh) are the marginal distributions of πk.
(6)
18
Under review as a conference paper at ICLR 2021
b)
(7)
for some large absolute constant C > 0.
Lemma 18. Let c1 be some large absolute constant. Define event E0 to be: for all h, s, a, b, s0 and
k∈[K],
j[(Ph-Ph)V"Ga,bX≤ ci jmaχ{NH2∣a,b),ι}，
l(pfc _P)(o | b)| ≤ S /min{Ph(S0 | SMb),Ph(S0 |s,a,b)}| _____ι______
1( h— h)(s ∣s,α, )|一c1 (M	maχ{Nk(s,a, b), 1}	+ max{Nh≈(s,a, b), 1}
We have P(E1 ) ≥ 1- p.
Proof. The proof is standard and folklore: apply standard concentration inequalities and then take a
union bound. For completeness, we provide the proof of the second one here.
Consider a fixed (S, a, b, h) tuple.
Let’s consider the following equivalent random process: (a) before the agent starts, the environment
samples {s(1), SQ),..., s(K)} independently from Ph(∙ | s, a, b); (b) during the interaction between
the agent and environment, the ith time the agent reaches (S, a, b, h), the environment will make
the agent transit to S(i). Note that the randomness induced by this interaction procedure is exactly
the same as the original one, which means the probability of any event in this context is the same
as in the original problem. Therefore, it suffices to prove the target concentration inequality in this
‘easy' context. Denote by Pht)(∙ | s, a, b) the empirical estimate of Ph(∙ | s, a, b) calculated using
{S(1), S(2), . . . , S(t)}. For a fixed t and S0, by applying the Bernstein inequality and its empirical
version, we have with probability at least 1 - p/S2ABT ,
|(Ph - Pht))(S0 | s, a, b)| ≤ O (S min{Ph(S0| s,a,b),Pht)(S0| s,a,b)k + ι)
Now we can take a union bound over all S, a, b, h, S0 and t ∈ [K], and obtain that with probability at
least 1 - p, for all S, a, b, h, S0 and t ∈ [K],
|(Ph - Pht))(S0 | s, a, b)| ≤ O (J min{Ph(S0| "b),Pht)(S0| S,a,b)}l + ι
Note that the agent can reach each (S, a, b, h) for at most K times, this directly implies that the third
inequality also holds with probability at least 1 - p.	□
We begin with an auxiliary lemma bounding the lower-order term.
Lemma 19. Suppose event E0 holds, then there exists absolute constant c2 such that: if function
g(s) satisfies ∣g∣(s) ≤ (Vh+i — Vh+ι)(s) for all s, then
|(Pbkh - Ph)g(S, a, b)|
≤c2 (⅛ min{Ph(Vh+i - Vh+ι)(s, a, b), Ph(V"-Vh+ι)(s, a, b)} + ma^^),1})
19
Under review as a conference paper at ICLR 2021
Proof. By triangle inequality,
|(Pbkh - Ph)g(s, a, b)|
≤	|(Pbkh -Ph)(s0|s,a,b)||g|(s0)
s0
≤ X l(Ph - Ph)(SiS,a,b)∣(Vh+1- Vh+1)(s0)
s0

(i)
≤O
IPh(S0∣s,a, b)
max{Nhk(s, a,b), 1} + max{Nh(s, a,b), 1}
(ii)
≤O
Hι
+ max{Nhk(s, a, b), 1}
H
ι
≤O PPh(Vh+1— Vh+ι)(s,a,b) +	H2S∣	、
一[	H	max{Nk (s,a,b), 1} J，
where (i) is by the second inequality in event E0 and (ii) is by AM-GM inequality. This proves the
empirical version. Similarly, we can show
|(Ph - Ph)g(s, a,b)∣ ≤ O PPh(Vh+1— Hh+1)(SMb)
Combining the two bounds completes the proof.
H 2Sι
+ mαx{Nh(s, a, b), 1}
□
Now we can prove the upper and lower bounds are indeed upper and lower bounds of the best
reponses.
Lemma 20. Suppose event E0 holds. Then for all h, S, a, b and k ∈ [K], we have
!Qh(s,a,b) ≥ 以"(S,a,b ≥ Qf ,t(s, a,b)≥ Qh(S,a,b),	⑻
[v h(S)≥ Vt,νk (s) ≥ Vμk,t(S) ≥ V. h(S).
Proof. The proof is by backward induction. Suppose the bounds hold for the Q-values in the (h +
1)th step, we now establish the bounds for the V -values in the (h + 1)th step and Q-values in the
hth-step. For any state S:
V h+1(S) = D∏k+1 Qh+1 (S)
≥ max Dμ×νk+1 Qh+1(s)	(9)
≥ mμx Dμ×νk+1 Qh，+ I(S) = Vt+1 (s).
Similarly, We can show Vh+1(S) ≤ Vμ+1t(S). Therefore, We have: for all s,
Vh+1(S) ≥ Vt+1k(S) ≥ V?+1(S) ≥ Vh+1t(S) ≥ Vh+1 (s).
NoW consider an arbitrary triple (S, a, b) in the hth step. We have
(Qh - Qh,νk )(s,a,b)
≥ min {(PhVh+1 - PhVt+1k + βh + Yh)(SM b), 0}
≥min (PbkhVht+,ν1k -PhVht+,ν1k +βhk +γhk)(S,a,b),0
=min 1(Ph. - Ph)(Vt+1： - Vh+1)(s,a,b), + (Ph. - Ph) V?+1(s,a,b)
(A)	(B)
+ (βhh + γhh )(S, a, b), 0 .
20
Under review as a conference paper at ICLR 2021
|(A)| ≤ O
|(B)| ≤ O
Invoking Lemma 19 with g = VhJ% -匕?+「
Ph(Vh+ι- Vh+ι)(s,a,b) +	H2S∣	!
H	max{Nk (s,a,b),1}.
By the first inequality in event E0,
S H 2∣	.!.
V max{Nk(s,a, b), 1}
Plugging the two inequalities above back into (10) and recalling the definition of βhk and γhk, we
obtain Qh(s, a, b) ≥ Q>ν (s, a, b). Similarly, We can show Qh(s, a, b) ≤ Qμ ,J(s, a, b).	□
Finally we come to the proof of Theorem 3.
Proof of Theorem 3. Suppose event E0 holds. We first upper bound the regret. By Lemma 20, the
regret can be upper bounded by
X(VjVk (Sk) - vμk,j(Sk)) ≤ X(Vk(Sk) - Vk(Sk)).
kk
For brevity’s sake, we define the following notations:
△h ：= (v h - v h)(sh),
Zh = ∆h -(Qh - Qh)(Sh,ah,bh),
ξh ：= Ph(v h+ι - v h+ι)(Sh, ah, bh) - △"
Let Fhk be the σ-field generated by the following random variables:
(11)
{(Sij, aij, bij, rij)}(i,j)∈[H]×[k-1]	{(Sik,aik,bik,rik)}i∈[h-1]	{Skh}.
It’s easy to check ζhk and ξhk are martingale differences with respect to Fhk. With a slight abuse of
notation, we use βhk to refer to βhk (Skh, akh, bkh) and Nhk to refer to Nhk (Skh, akh, bkh) in the following
proof.
We have
△h =Zh + (Qh - Qh)(Sh,ah,bh)
≤ζh+2βk + 2γh+bh(v h+ι - v h+ι) (Sh, ah,监
≤Zh + 2βk + 2γh + Ph(Vh+1 - vh+ι) (Sh, ah, bh)
+	PPh(Vh+1- Vh+ι)(Sh,ah,bh) 1	H2Si
C2 y	H	max{Nk, 1}
、ii)	k
≤ Zh + 2βk + Ph(Vh+1 - Vh+ι) (Sh, ah, bh)
Ph(Vh+1 - Vh+ι)(Sh,ah,bh) ,	H2Si
+ 2c2C I-----------H---------------+ max{Nk, 1})
≤ζh + (1 + 2HC) Ph(V h+ι- v h+I)(Sh, ah,bh)+4c2C Pj m⅛‰ + m⅛ιy
=Zhk +
1+
等)ξk +
(1 +等)∆h+ι+4C2CGPH2=+
H +1	max{Nhk, 1}
H 2Si
max { Nk, 1}
21
Under review as a conference paper at ICLR 2021
where (i) and (ii) follow from Lemma 19.
Define c3 :
over k,
K
1+ 2c2C and κ := 1+ c3/H. Recursing this argument for h ∈ [H] and summing
KH
k=1
∆1k≤
k=1 h=1
κh-1ζhk +	κhξhk+	O
ιH2
H2Sι
max{Nk, 1}* max{N(, 1}
By Azuma-Hoeffding inequality, With probability at least 1 - p,
'K H
XX KhTZk ≤ O H√ √HKi) = O (√H 2Ti),
k=i h=i
I K H
XXκhξhk ≤o(H√HK∣) = o(√H2Tl)
、k=1h=1
(12)
By pigeon-hole argument,
KH
XX
k=1 h=1
=≤
JmaX{Nk ,1}
NhK(s,a,b)
X	X √+haat
s,a,b,h: NhK (s,a,b)>0	n=1
≤o ( Hhabtt + HSAB),
KH
XX
k=1 h=1
----:-J-r ≤
max{Nhk , 1}
NhK (s,a,b)
X	X 1 + HAAT
n
s,a,b,h: NhK (s,a,b)>0	n=i
≤O(HSAtι).
Put everything together, with probability at least 1 - 2p (one p comes from P(E0) ≥ 1 - p and the
other is for equation (12)),
K	,	,	,	、
X(Vt,νk (Sk) - VTk,t(sk)) ≤ θ(√H3AATTi + H3A2ATi2)
k=1
For the PAC guarantee, recall that We choose πout = πk? such that k? = argminfc (Vk - Vk) (si).
As a result,
(Vt，""* - Vμk?,t)(sι) ≤ (Fk - Vk?)(sι) ≤ ɪθ(√H3AATTi + H3A2ATi2),
which concludes the proof.
F.2 Proof of Theorem 4
We use the same notation as in Appendix F.1 except the form of bonus. Besides, We define the
empirical variance operator
VhV(SMb) := Vars，〜bh(.∣s,a,b)V(s0)
and the true (population) variance operator
VhV(s,a,b) := Vars，〜Ph(∙∣s,a,b)V(s0)
for any function V ∈ ∆S. If Nhk(s, a, b) = 0, We simply set VbkhV(s, a, b) := H2 regardless of the
choice of V.
As a result, the bonus terms can be written as
βhk (s, a, b) :=C
∣V h[(V h+1 + V h+i)∕2](s,a,b)
max{Nk(s, a, b), 1}
H2Sι
+ max{Nk(s, a, b), 1}
/
.
1
1
□

for some absolute constant C > 0.
(13)
22
Under review as a conference paper at ICLR 2021
Lemma 21. Let c1 be some large absolute constant. Define event E1 to be: for all h, s, a, b, s0 and
k∈[K],
1[(Ph- Ph)Vh+i](s, a, b)1 ≤ ci (SmlNlm + maχ{NkHl,a,b),1})，
M-Ph)(s0∣s,α,b)∣≤ cι(Smn⅛liW≡ +m-b),1} :
J(Ph- Ph)CsMb)ki ≤ CIsm^NS^
We have P(E1 ) ≥ 1 - p.
The proof of Lemma 21 is highly similar to that of Lemma 18. Specifically, the first two can be
proved by following basically the same argument in Lemma 18; the third one is standard (e.g.,
equation (12) in Azar et al. (2017)). We omit the proof here.
Since the proof of Lemma 19 does not depend on the form of the bonus, it can also be applied in this
section. As in Appendix F.1, we will prove the upper and lower bounds are indeed upper and lower
bounds of the best reponses.
Lemma 22. Suppose event E1 holds. Then for all h, s, a, b and k ∈ [K], we have
IQh(S,α,b) ≥ Qh,ν" (SMb) ≥ Qhk ,t (SMb) ≥ QI(S,a,b),	(14)
[v h(s) ≥ vt,νk (s) ≥ vμk,t(s) ≥ V h(s).
Proof. The proof is by backward induction and very similar to that of Lemma 20. Suppose the
bounds hold for the Q-values in the (h + 1)th step, we now establish the bounds for the V -values in
the (h + 1)th step and Q-values in the hth-step.
The proof for the V -values is the same as (9).
For the Q-values, the decomposition (10) still holds and (A) is bounded using Lemma 19 as before.
The only difference is that we need to bound (B) more carefully.
First, by the first inequality in event E1,
V IhVh+i(s,a,b)ι +	Hι
max{Nh(s, a, b), 1}	max{Nh(s, a, b), 1}
By the relation of V -values in the (h + 1)th step,
|[Vh(Vh+1 + Vh+i)∕2] - VhV?+i|(s, a, b)
≤∣[Ph(Vh+i + Vh+i)∕2]2 - (PhV?+i)2|(s,a,b)
+ ∣Ph [(V h+i+V h+ι)∕2]2 - Ph(vh+i)2∣(s,a,b)	(15)
≤4HPhl(Vh+i + Vh+i)∕2 - Vh+i∣(s,a,b)
≤4HPh(Vh+i - Vh+i)(s,a,b),
l(B)l≤O 卜
23
Under review as a conference paper at ICLR 2021
which implies
/ ιV h*(s,a,b 厂
V max{Nk(s, a, b), 1}
≤t
∣[Vh[(Vh+1 + Vh+ι)∕2]+ 4HPh(Vh+1 - Vh+ι)](s, a b
max{Nhk (s, a, b), 1}
≤t
∣V h[(V h+1 + V h+ι)∕2](s,a,b) +
max{Nh (s,a,b), 1}	∖
4∣HPh(Vh+1- Vh+ι)](s,a,b)
max{Nhk(s, a, b), 1}
(16)
(i) u
≤t
∣Vh[(Vh+1 + Vh+ι)∕2](s,a,b) + Ph(Vh+1- Vh+1)
max { Nk (s,a,b),1}	H
4H2ι
+ max{Nhk(s, a, b), 1}，
where (i) is by AM-GM inequality.
Plugging the above inequalities back into (10) and recalling the definition of βhk and γhk completes
the proof.	□
We need one more lemma to control the error of the empirical variance estimator:
Lemma 23. Suppose event E1 holds. Then for all h, s, a, b and k ∈ [K], we have
Vh[(V h+1+V h+1)∕2] - VhVnk1∣(s,a,b)
≤4HPh(Vh+1-Vh+1)(s, a,b) + O (1+ maχ{NH4Sa,b),1}).
Proof. By Lemma 22, We have VIh(S) ≥ VhTk (S) ≥ Vh(s). Asa result,
|Vh[(Vh+1 + Vh+1)∕2] - VhVπk1∣(s, a, b)
= ∣[bh(Vh+1 + Vh+1)2∕4 - Ph(Vh+1)2](s, a, b) - [(Ph(Vh+1 + Vh+1))2∕4 - (PhVnkI)2](s, a, b)|
≤[Ph(V h+1)2 - Ph(V h+1)2 - (PhV h+1)2 + (PhV h+1)2](s,a,b)
≤[∣(Ph -Ph)(Vh+1)2∣ + ∣Ph[(Vh+1)2 - (Vh+1)2]∣
+ ∣(Ph V h+1 )2 - (PhV h+1)2ι + ∣(PhV h+1)2 - (PhV h+1)2∣](s,a,b)
These terms can be bounded separately by using event E1 :
l(Ph - Ph)(Vh+1)2∣(s, a, b) ≤ H2k(Ph - Ph)(∙ |
S, a,
b)k1 ≤ O(H jmax{Nk(s,a, b), 1} ),
∣Ph[(V h+1)2 - (V h+1)2]∣(s,a,b) ≤ 2H [Ph(V h+1 - V h+1)](s,a,b),
∣(Ph V h+1 )2 - (PhV h+1)2∣(s,a,b) ≤ 2H[(Ph - Ph)V h+∕(s,a,b) ≤ O(H 2J
l(PhVh+1 )2 - (PhVh+1)2∣(s, a, b) ≤ 2H[Ph(Vh+1 - Vh+1)](s, a, b).
Sι
max{Nk(s, a, b), 1}
),
Combining with H 2 /maχ{N 篇,a,b)j} ≤ 1 + maχ{N⅛a,b),1} COmPletes the Proof.
Finally we come to the Proof of Theorem 4.
24
Under review as a conference paper at ICLR 2021
Proof of Theorem 4. Suppose event E1 holds. We define ∆kh, ζhk abd ξhk as in the proof of Theo-
rem 3. As before we have
△h ≤ζh + (ι + H3) Ph(V h+ι- V h+ι) (sh, ah,由)
+ 4c C ( u∣Vh[(Vh+1 + Vh+ι)∕2](sh,ah,bh) +	H2Sι	]	(O)
2 (∖	max{Nk(sh,ah,bh), 1}	maχ{Nh(Sh,ahM),ι∖ j.
By Lemma 23,
∣V h[(V h+1 + V h+ι)∕2](s,a,b)
∖	max{Nh (s,a,b), 1}
≤O( INhVfn+ι(s,aM + +	HIPh(Vh+ι - Vh+ι)(s,a,b) +	H2√Sι
—	V max{Nh(s,a,b), 1}	∖	max{Nh(s,a,b), 1}	max{Nh(s,a,b),1}
≤c ( INhVh+ι(s,a,b)+ + Ph(Vh+ι- V h+ι)(s,a,b) +	H 2√Sι	ʌ
—41V max{Nh(s,a,b),1}	H	max{Nh(s,a,b),1} J，
(18)
where c4 is some absolute constant. Define c5 := 4c2c4C + c3 and κ := 1 + c5∕H. Plugging (18)
back into (17), we have
△h ≤κ∆h+ι+κξh + Zk
+ O( I∣Vh⅞(sh,ah,bh + r I +	H2Sι	H	(19)
IV	Nk(sh,ah,bh)	VNk(sh,ah,bh)	Nk(sh,ah,bh)J ∫.
Recursing this argument for h ∈ [H] and summing over k,
+O卜
K	KH
X∆k ≤XX KhTZh + κhξh
k=1	k=1 h=1
NhVhh+ι(sf,alh,bh + r I + H 2Si
max {Nk, 1}	Nmax{Nh, 1}	max{Nh, 1}
The remaining steps are the same as that in the proof of Theorem 3 except that we need to bound the
sum of variance term.
By Cauchy-Schwarz,
KH
XX
k=1 h=1
VhVh+ι(sh,ah,bh)
max{ Nk(Sh,ah,bh) 1}
≤
uKH	KH	1
tX X VhVh+1(Sh MfMJ ∙ X X max{Nk(sh,ah,bh),1} .
By the Law of total variation and standard martingale concentration (see Lemma C.5 in Jin et al.
(2018) for a formal proof), with probability at least 1 - p, we have
KH
XX
VhVh+ι(sh,ah,bh)≤O(HT + H 3ι).
k=1 h=1
Putting all relations together, we obtain that with probability at least 1 - 2p (one p comes from
P(E1 ) ≥ 1 - p and the other comes from the inequality for bounding the variance term),
K	,	,
Regret(K) = X(Vt,νk - Vμk,t)(s1) ≤ O(√H2SABTi + H3S2ABi2).
k=1
Rescaling P completes the proof.	口
25
Under review as a conference paper at ICLR 2021
G Proof FOR Section 4 - Reward-Free Learning
G.1 Proof of Theorem 5
in this section, we prove Theorem 5 for the single reward function case, i.e., N = 1. The proof for
multiple reward functions (N > 1) simply follows from taking a union bound, that is, replacing the
failure probability p by Np.
Let (μk, Vk) be an arbitrary Nash-equilibrium policy of Mck ：= (Pk,b), where Pk and bk are our
empirical estimate of the transition and the reward at the beginning of the k’th episode in Algorithm
2, respectively. We use Nhk (s, a, b) to denote the number we have visited the state-action tuple
(s, a, b) at the h-th step before the k’th episode. And the bonus used in the k’th episode can be
written as
βhk (s, a, b)
H HI
m max{Nk(s, a, b), 1} 十
H 2Sι
max{N(s, a, b), 1}
(20)
where ι = log(SABT /p) and C is some large absolute constant.
We use Qbk and Vb k to denote the empirical optimal value functions of Mck as following.
Qbkh (s, a,b) = (Pbkh Vbh+1)(s,a, b) +rbhk(s, a, b),
Vbh (S) = max min Dμ×ν Qhh (S).
μ V
Since (μk, Vk) is a Nash-equilibrium policy of Mck, we also have Vhk(s) = Dμk×νkQh(S)
(21)
We begin with stating a useful property of matrix game that will be frequently used in our analysis.
Since its proof is quite simple, we omit it here.
Lemma 24. Let X, Y, Z ∈ RA×B and ∆d be the d-dimensional simplex. Suppose |X - Y | ≤ Z,
where the inequality is entry-wise. Then
max min μ> XV — max min μ>Yν
μ∈4A ν∈4b	μ∈4A ν∈4b
≤ max Zij .
i,j
(22)
Lemma 25. Let c1 be some large absolute constant such that c21 + c1 ≤ C. Define event E1 to be:
for all h, S, a, b, S0 and k ∈ [K],
1[(Ph - ph)%ι](SMb)l≤ MmaX{NH2,a,b),1}
< l(bk Th)(s,a,b)l≤ 10Smax{NHSIa,b),1} ,	(23)
l(bh - Ph)(S0 1 "知 ≤ ⅜(SmaP⅛(Nk⅛l‰ + max{Nk(lS,a,b), 1}
We have P(E1 ) ≥ 1 - p.
Proof. The proof is standard and folklore: apply standard concentration inequalities and then take a
union bound. For completeness, we provide the proof of the third one here.
Consider a fixed (S, a, b, h) tuple.
Let’s consider the following equivalent random process: (a) before the agent starts, the environment
samples {s(1), SQ),..., s(k)} independently from Ph(∙ | s, a, b); (b) during the interaction between
the agent and environment, the ith time the agent reaches (S, a, b, h), the environment will make
the agent transit to S(i). Note that the randomness induced by this interaction procedure is exactly
the same as the original one, which means the probability of any event in this context is the same
as in the original problem. Therefore, it suffices to prove the target concentration inequality in
26
Under review as a conference paper at ICLR 2021
this 'easy' context. Denote by Pht)(∙ | s,a,b) the empirical estimate of Ph(∙ | s,a,b) calculated
using {s(1), s(2), . . . , s(t)}. For a fixed t and s0, by the empirical Bernstein inequality, we have with
probability at least 1 - p/S2ABT ,
|(Ph - Pht))(s0 | SMb)| ≤ O (SPht)(S0ts,a,b)1 +1)
Now we can take a union bound over all s, a, b, h, s0 andt ∈ [K], and obtain that with probability at
least 1 - p, for all S, a, b, h, S0 and t ∈ [K],
|(Ph - Pht))(s0 | SMb)| ≤ O HPht)(S0ts,a,b)1 +1).
Note that the agent can reach each (S, a, b, h) for at most K times, this directly implies that the third
inequality also holds with probability at least 1 - p.	□
The following lemma states that the empirical optimal value functions are close to the true optimal
ones, and their difference is controlled by the exploration value functions calculated in Algorithm 2.
Lemma 26. Suppose event E1 (defined in Lemma 25) holds. Then for all h, S, a, b and k ∈ [K], we
have,
Qbkh(S,a,b) - Q?h(S, a, b) ≤ Qekh(S, a, b),
Vbhk(S)-Vh?(S) ≤Vehk(S).
(24)
Proof. Let's prove by doing backward induction on h. The case of h = H + 1 holds trivially.
Assume the conclusion hold for (h + 1)'th step. For h'th step,
Qbkh(S, a, b) - Q?h(S, a, b)
≤ min n[(Pbkh	-	Ph)Vh?+1](S, a, b)	+	|(rbhk -	rh)(S, a, b)| +	[Pbkh(Vbhk+1	-	Vh?+1)](S, a, b), Ho
(≤) min nβhk (S, a, b) + (PbkhVehk+1)(S, a, b), Ho (=ii) Qekh(S, a, b),
(25)
where (i) follows from the induction hypothesis and event E1, and (ii) follows from the definition
of (eh. By Lemma 24, we immediately obtain |Vhk(S) - Vh?(s)| ≤ VhC(s).	□
Now, we are ready to establish the key lemma in our analysis using Lemma 26.
Lemma 27. Suppose event E1 (defined in Lemma 25) holds. Then for all h, S, a, b and k ∈ [K], we
have
(|Qh(S, a,b) - QhV(S, a, b)| ≤ αhQh(S, a, b),
IW(S)-川,νk(s)I≤ αhVhk(S),
(26)
and
where αH+1 = 0 and αh
(IQh(S, a, b) - Q广(S, a,b)l≤ αhQh(S, a, b),
l∣Vhk(s) - Vhμk,t(S)∣≤ αhVhk(s),
=[(I + HH)αh+1 + H] ≤ 4∙
(27)
Proof. We only prove the first set of inequalities. The second one follows exactly the same. Again,
the proof is by performing backward induction on h. It is trivial to see the conclusion holds for
27
Under review as a conference paper at ICLR 2021
(H + 1)’th step with αH+1 = 0. Now, assume the conclusion holds for (h + 1)’th step. For h’th
step,
IQh(s,a,b) — QhVk (SMb)I
≤ min {∣[(bh - Ph)(Vh+ιk — *ι)](s, a,b)I + I (Ph — Ph)Vh+ι(s, a, b)I
+ IM - rh)(s,a,b)I + ∣[Ph(Vh+ι — ⅛ )](s,a,b)∣,H∣
「	I-------H--------- (28)
≤ min { ![(Ph - Ph)(V⅞-Vh+1"a，b)!+c√ maχ{Nk(sla,b),1}
(TI)
+ ∣[Ph(Vh+ι - Vh+ιk)](s,a,b)∣,H1,
'-------------{-----------}
(T2)
where the second inequality follows from the definition of event E1.
We can control the term (T1) by combining Lemma 26 and the induction hypothesis to bound
| Vh+1 - Vh+ι |, and then applying the third inequality in event Ei:
(T1)≤XIPbkh(s0I s,a,b)-Ph(s0I
s0
≤ X IPbkh(s0 I s,a,b)-Ph(s0 I
s0
≤ X IPbkh(s0 I s,a,b)-Ph(s0 I
s,a,b)∣∣^+ιk - %1(s0)∣
s,a,b)∣ 他+; - Vh+ι(s0)∣ + ∣Vh+ι - ‰(s0)∣
s, a, b)I(αh+1 + 1)Vehk+1
(29)
s0
≤ (αh+1+1) (PhV+i)(s, a, b) + c2(αh+k+1)H2S∣.
H	+	max{Nhk (s, a, b), 1}
The term (T2 ) is bounded by directly applying the induction hypothesis
∣[Ph(V+ι- Vh+ik)](s,a,b)I ≤ αh+1[PhV+1](s,a,b).	(3O)
Plugging (29) and (30) into (28), we obtain
IQh(S,a, b) - Qh,νk(S, a, b)∣
≤ min {(1 + ⅛)αh+i + ![PhV+i](S, a, b) + ci {max{NH2,a,b),三
+ c2(ah+i + I)H2Sι H1
max{Nk(s,a,b),1}，	ʃ
≤ min {((I + H )αh+ι + h) [PhVh+i](S, α, b)+βk(S, a, b),H I
(31)
(≤)((1 + ')ɑh+i + H )Qh(S,
a, b),
where (i) follows from the definition of βhk, and (ii) follows from the definition of Qekh. Therefore,
by (31), choosing αh = [(1 + H)ɑh+ι + H] suffices for the purpose of induction.
Now, let’s prove the inequality for V functions.
I(Vhk - 川,Vk)(S)I =) I max(Dμ,νkQh)(S) - max(Dμ,νkQh,νk)(s)I
μ∈4A	μ∈4A
(ii)
≤ max
a,b
αhQekh(S, a, b) = αhVehk(S),
where (i) follows from the definition of Ibk and Vh^,ν , and (ii) uses (31) and Lemma 24.
(32)
□
28
Under review as a conference paper at ICLR 2021
Theorem 28 (Guarantee for UCB-VI from Azar et al. (2017)). For any p ∈ (0, 1], choose the
exploration bonus βt in Algrothm 2 as (20). Then, with probability at least 1 - p,
K
X Vk(s1) ≤ O(√H4SAKl + H3S2Aι2).
k=1
Proof of Theorem 5. Recall that out = arg mink∈[K] Vehk(s). By Lemma 27 and Theorem 28, with
probability at least 1 - 2p,
年,Vout(S)- Vμout,t(s) ≤∣Vt,νout(S)- VoUt(S)∣ + ∣Vout(s) - Vμout,t(s)∣
H4 SAι	H 3S2Aι2、	(33)
≤8V0ut(s) ≤ O(d~lr- + —丁—).
KK
Rescaling P completes the proof.	□
G.2 Vanilla Nash Value Iteration
Here, we provide one optional algorithm, Vanilla Nash VI, for computing the Nash equilibrium
policy for a known model. Its only difference from the value iteration algorithm for MDPs is that
the maximum operator is replaced by the minimax operator in Line 7. We remark that the Nash
equilibrium for a two-player zero-sum game can be computed in polynomial time.
Algorithm 5 Vanilla Nash Value Iteration
1:	Input: model Mc = (Pb, rb).
2:	Initialize: for all (s, a, b), VH+ι(s, a, b) - 0.
3:	for step h = H, H - 1, . . . , 1 do
4:	for (S, a, b) ∈ S × A × B do
5:	Qh(s, a, b) J [PhVh+ι](s, a, b) + r%(s, a, b).
6:	for S ∈ S do
7:	(^h(∙ | s), ^h(∙ | SD J NASH-ZERO-SUM(Qh(s, ∙, ∙)).
8:	Vh(S) J μh(∙ | s)>Qh(s, ∙, ∙)^h(∙ | s).
9:	Output (μ,^) J {(μh(∙ | S),Vh(∙ | S))}(h,s)∈[H]×s.
By recalling the definition of best responses in Appendix D, one can directly see that the output
policy (μ, ^) is a Nash equilibrium for Mc.
G.3 Proof of Theorem 6
In this section, We first prove a Θ(AB∕e2) lower bound for reward-free matrix games, i.e., S =
H = 1, and then generalize it to Θ(SABH2∕e2) for the Markov games setting.
G.3.1 reward-free matrix games
In the matrix game, let the max-player pick row and the min-player pick column. We consider the
following family of Bernoulli matrix games:
M(e) = {m ∈ Ra×b with Mab = | + (1 — 2 ∙ 1{a = a?&b = b?})e : (a?, b?) ∈ [A] X [B]
(34)
where in matrix game M, the reward is sampled from Bernoulli(Mab) if the max-player picks the
a’th row and the min-player picks the b’th column.
29
Under review as a conference paper at ICLR 2021
Max-player
action
1
a? - 1
a?
a? + 1 +
A+
Min-player
b? - 1 b? b? + 1
+-+
...
...
...
+-+
+++
+-+
...
...
...
+-+
B
+
.
.
.
+
+
+
.
.
.
+
(35)
1
+
+
+
Above, we visualize the hard instance by using + and - to represent 1/2+ and 1/2-, respectively.
It is direct to see that the optimal policy for the max-player is always picking the a?’th row and the
optimal policy for the min-player is always picking the b?’th column. If the max-player picks the
a?’th row with probability smaller than 2/3, it is at least /10 suboptimal.
Lemma 29. For any fixed matrix game M from M() and N ∈ N, if an algorithm A can output
a policy that is at most /10 suboptimal with probability at least p using at most N samples, then
there exists an algorithm A that can identify the best row in M with probability at least p using at
most N samples.
Proof. We simply define A as running algorithm A and choosing the most played row by its out-
putted policy as the guess for the best row. By simple calculation, one can show A will output the
best row in M with probability at least p.	□
Lemma 29 directly implies that in order to prove the desired lower bound for matrix games:
Claim 30. for any algorithm A using at most N = AB/(1032) samples, there exists a matrix game
M in M() such that when running A on M, it will output a policy that is at least /10 suboptimal
for the max-player with probability at least 1/4,
it suffices to prove the following claim:
Claim 31. for any algorithm A using at most N = AB∕(103e2) samples, there exists a matrix game
M in M(e) such that when running A on M, it will fail to identify the optimal row with probability
at least 1/4.
ProofofClaim 31. WLOG, we assume A is deterministic. Since this is the reward-free setting,
being deterministic means that algorithm A will always pull each arm (a, b) for some fixed n(a, b)
times and then output a guess for a? which is a function of the reward revealed.
Denote by L the reward revealed after algorithm A’s pulling. Denote by P? the probability induced
by picking M uniformly at random from M(e) and running A on M. Denote by Pa,b the probability
induced by running A on M, whose indices of the special row and the special column are a and b,
respectively. Denote by P0,b the probability induced by running A on M, whose b’th column are all
1/2 - e and other columns are all 1/2 + e. We want to mention that the M we use to define P0,b
does not belong to M(e).
30
Under review as a conference paper at ICLR 2021
We have
P*(A(L) = a?) ≥ AB X Po,b(A(L) = a) - AB X |%，-P0,b∣∣ι
a,b	a,b
≥
1
A
1-
-Ab X J2KL(P。,[叫油
a,b
1-
1 一	1	1 - €	1	1 + €
AB X y2n(a, b)[(2- e)log 干 + (2+e)log 厂 ]
(36)
1
A
—
≥
1
A
≥
1-
-AB X，n(a，b)€2
a,b
1-
∕100Ne2
A AB
1
A
Choosing N = AB/(103€2) concludes the proof.
□
G.3.2 reward-free Markov games
Now let's generalize the Θ(AB/e2) lower bound to Θ(SABH2/e2) for reward-free Markov games.
We define the following family of MDPs:
J(e) := JJ (a?, b?) : (a?, b?) ∈ [A]H×S X [B]H×S} ,	(37)
where MDP J(a?, b?) is defined as below:
•	States and actions: J(a?, b?) is a finite-horizon MDP with S + 1 states of length H + 1.
There is a fixed initial state s0 in the first step, S states {s1, . . . , sS} for the remaining
steps. The two players have A and B actions, respectively.
•	Rewards: there is no reward in the first step. For the remaining steps h ∈ {2, . . . , H + 1},
if the agent takes action (a, b) at state si in the hth step, it will receive a binary reward
sampled from
Bernoulli(1 + (1 - 2 ∙ 1{a = ah-i,i&b = bh-i,i})H)
•	Transitions: Regardless of the current state, actions and index of steps, the agent will
always transit to one of s1, . . . , sS uniformly at random.
It is direct to see that J(a?, b?) is a collection of SH independent matrix games from M(e/H).
Therefore, the optimal policy for the max player is to always pick action a?h-1,i whenever it reaches
state si in the hth step (h ≥ 2). In other words, a?h-1,i is the unique optimal action for the step-state
pair (h, i).
At a high level, in order to find an e-optimal policy for the above Markov game, we need to identify
at least half of the entries of a? . Therefore, the number of episodes should be at least
Θ
AB )
(√W)
X S = θ(ABSH2
2 I e2
Below we provide a formal proof of this argument, which is almost the same as that for the setting
of reward-free matrix games.
We start by proving an analogue of Lemma 29.
Lemma 32. For any fixed matrix game J(a?, b?) from J(e) and N ∈ N, if an algorithm A can
output a policy that is at most e/103 suboptimal with probability at least p using at most N samples,
then there exists an algorithm A that can correctly identify at least SH 一 [SH/500C entries of a?
with probability at least p using at most N samples.
31
Under review as a conference paper at ICLR 2021
Proof. Denote by π the output policy for the max player. Denote by Z the collection of (h, i)’s in
[H] × [S] such that ∏h+ι(ah,i | Si) ≤ 2/3.
Observe that each time the max player picks a suboptimal action, it will incur an 2/H suboptimality
in expectation. As a result, if π is at most /103-suboptimal, we must have
S X (I - πh+1(ah,i | Si)) × HH ≤ 103 ,
(h,i)∈Z
which implies |Z| ≤ SH/500, that is, for at most bSH/500c different i’s, π(ai? | Si) ≤ 2/3.
Therefore, we can simply pick argmaxa πh+1 (a | Si) as the guess for a?h,i. Since policy π is at
most /103 suboptimal with probability at least p, we can correctly identify the optimal actions for
at least SH - [SH/500C different (s, h) pairs also with probability no smaller than p.	□
Lemma 32 directly implies that in order to prove the desired lower bound for reward-free Markov
games:
Claim 33. for any algorithm A interacting with the environment for at most K = ABSH2/(1042)
episodes, there exists J ∈ J() such that when running A on J, it will output a policy that is at
least /103 suboptimal for the max-player with probability at least 1/4,
it suffices to prove the following claim:
Claim 34. for any algorithm A interacting with the environmentfor at most K = ABSH2∕(104e2)
episodes, there exists J ∈ J(e) such that when running A on J, it will fail to identify the optimal
actions for at least bS H/500c + 1 different (S, h) pairs with probability at least 1/4.
Proof of Claim 34. Denote by P? (E?) the probability (expectation) induced by picking J(a?, b?)
uniformly at random from J(e) and running A on J. Denote by "wrong the number of (s, h) pairs
for which A fails to identify the optimal actions. Denote by errorh,i the indicator function of the
event that A fails to identify the optimal action for (h + 1, i).
We prove by contradiction. Suppose for any J ∈ J(e), A can identify the optimal actions for at
least SH - bSH/500c different (S, h) pairs with probability larger than 3/4. Then we have
1	3 SH	101SH
E*[nwrong] ≤ 4 × sh + 4 × [前]≤ ιo^.
Since P(h,i)∈[H]×[S] E?[errorh,i] = E?[nwrong], there must exists (h0, i0) ∈ [H] × [S] such that
E?[errorho,io] ≤ 101/400. However, in the following, We show that for every (h, i) ∈ [H] × [S], A
fails to identify the optimal action for the step-state pair (h+1, i) with probability at least 1/3, which
directly implies E?[errorh,i] ≥ 1/3 for all (h, i) ∈ [H] × [S]. As a result, we obtain a contraction
and Claim 34 holds.
Now, let us prove that for every (h, i) ∈ [H] × [S], E?[errorh,i] ≥ 1/6. WLOG, We assume A is
deterministic and it runs for exactly K = ABSH2/(103e2) episodes. In the following, we consider
a fixed (h0, i0) pair. For technical reason, we define MDP J-(h0,i0)(a?, b?) as below:
•	States, actions and transitions: same as J(a?, b?).
•	Rewards: there is no reward in the first step. For the remaining steps h ∈ {2, . . . , H + 1},
if the agent takes action (a, b) at state Si in the hth step such that (h - 1, i) 6= (h0, i0), it
will receive a binary reward sampled from
BernOUlli(1 + (1 -2 ∙ ι{a = ah-i,i&b = bh-ι,i1) H),
otherwise it will receive a binary reward sampled from
BernoUlli
2+(1-
2 ∙ i{b=bh-ι,i)) Hy
32
Under review as a conference paper at ICLR 2021
Intuitively, J-(h0,i0) (a?, b?) is the same as J (a?, b?) except that for the max player, all its actions
at state si0 in the h0th step are equivalent. In other words, J-(h0,i0) (a?, b?) is independent of a?h0,i0.
To proceed, We need to define the following notations: denote by n(a, b) the number of times A picks
action (a, b) at state si，in the (h0 + 1)th step; denote by P(∙ | J(a?, b?)) (E[∙ | J(a?, b?)]) the
probability (expectation) induced by running algorithm A on J(a?, b?); similarly, we define P(∙ |
J-(ho,io)(a?, b?)) (E[∙ | J-(ho,io)(a?, b?)]); also recall that we denote by P? (E?) the probability
(expectation) induced by picking J(a?, b?) uniformly at random from J(E) and running A on J;
denote by L the whole trajectory of states, actions and rewards produced by algorithm A in N
episodes; with slight abuse of notation, denote by A(L) the guess ofA for a?h0,i0 based on L.
First, note that for any (a, b) ∈ [A] × [B], E[n(s, a) | J-(h0,i0) (a?, b?)] is independent of
(h0 , i0 , a? , b? ) because the agent cannot observe any reward when interacting with the environment
and the transition dynamics of different J-(h0,i0)(a?, b?)’s are the same. For simplicity of notation,
we denote this expectation by m(a, b). Note that Pa,b m(a, b) = K/S because the agent always
reach state si0 in the (h0 + 1)th step with probability 1/S regardless of the actions taken.
We have
P? (A(L) = ah0,i0 )
1
(AB)SH	ΣS	P(A(L) = ah，,”
l (	(a? ,b*)∈[A]SH ×[B]SH
| J(a?,b?))
≥	(AB1)SH	X	(P(A(L)	=	ah，,i，I	J-(h，,i，)(a?,b?))
(a? ,b? )∈[A]SH ×[B]SH
— ∣∣P(L = ∙ | J-(h，,i，)(a?,b?))- P(L = ∙ | J (a?, b?))||J
1
1
1 - A — (AB)
SH
(a*,b?)∈[A]SH ×[B]sh
∣∣P(L = ∙ I J-(h0,i0)(a?, b?)) — P(L = ∙ I J(a*,b?))∣∣ι
1
1
≥ 1 - A - (AB)
SH
(a*,b?)∈[A]SH ×[B]sh
，2KL(P(L = ∙ ∣ J-(ho,io)(a?, b?)), P(L = ∙ ∣ J(a?, b?)))
(38)
1
(AB)SH
Σ
(a*,b?)∈[A]SH ×[B]sh
1 - I
—
1 - I
/	Π E 1 - M 1 E 1 + 与
X2m(aho,i, bho,i)[(2 - H)log 2+H-+ (2 + H)log 2-H-
- ɪ	X
AB	乙
S,h”h)∈[A]×[B]
/	∏ E 1 - M 1 E 1 + 与
X2m(aho,i, bho,i)[(2 - H)log 2-+H- + (2 + H)log 2-H-
1 10	E2
≥1 - A - ABZym(b) H2
、，	1	10E	LcL ， ,、	1	1	1 100Ke2
≥ 1 - A - ABHJAB ∑>(a,b) = 1 - A - S SABH2.
a,b
Plugging in K =SABH2/(104E2) completes the proof.
□
33
Under review as a conference paper at ICLR 2021
H Proof FOR Appendix C - Multi-player General-sum Markov
Games
H.1 Proof of Theorem 15
H.1.1 NE VERSioN
in this section, we prove Theorem 15 (NE version). As before, we begin with proving the optimistic
estimations are indeed upper bounds of corresponding value and Q-value functions.
Lemma 35. With probability 1 - p, for any (s, a, h, k, i):
Qh,i (s, a) ≥	Qhi-i (s, a),	Qh,i (s, a)	≤ QtAs, a),	(39)
Vh,i (S)	≥ Vtn-i(s),	Vh,i (S) ≤	V∏k (S).	(40)
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case, we
∙↑,∏k -
know at the (H + 1)-th step,VHk+1,i (S) = VH +-1i,i (S) = 0. Now, assume the inequality (40) holds
for the (h + 1)-th step, for the h-th step, by definition of the Q functions,
Qh,i (s,	a) -	Qh,∏-i	(s,	a)	=	[PhVh+ι,i]	(s,	a)	—	PhVh⅛	(s, a) + βt
=Pbkh
<.
ι,i - VXi) (s, a) + 他 — Ph) VXi (s, a) + βt.
S{z}
(A	(B)
By induction hypothesis, for anys'，(Vh+ι,i- W+ι,(s0) ≥0, and thus (A) ≥ 0. By uniform
concentration (e.g., Lemma 12 in Bai & Jin (2020)), (B) ≤ CdSHl2ι∕Nh(s, a) = βt. Putting
everything together We have Qh,i (s, a) - Q；,：-i (s, a) ≥ 0. The second inequality can be proved
similarly.
NoW assume inequality (39) holds for the h-th step, by definition of value functions and Nash equi-
librium,
Vh,i (S) = Dπk Qh,i (S)= maxD**:--Qh,i (S) ∙
By Bellman equation,
<iπ-i (s)=maχDμ×∏-i Qh,,π-i (s).
Since by induction hypothesis, for any (s, a), Qh,i (s, a) ≥ Qhn-i (s, a). As a result, we also have
Vh,i (s) ≥ Vh',iπ-i (s), which is exactly inequality (40) for the h-th step. The second inequality can
be proved similarly.	□
Proof of Theorem 15. Let us focus on the i-th player and ignore the subscript when there is no
confusion. To bound
max ^ιt,iπ-i - Vnk) (Sh) ≤ max (V 1,i - V川Sh ,
we notice the following propogation:
I (Qh,i - Qh,i)(S, a) ≤ Ph(Vh+ι,i - Vh+ι,i)(S, a) + 2βh(S, a),
1(Vh,i - Vh,i)(S) = [D∏h(Qh,i - Qh,i)](S).
(41)
34
Under review as a conference paper at ICLR 2021
kk	k
We can define Qkh and Vhk recursively by VHk +1 = 0 and
(Qekh(s, a) = PbkhVehk+1(s, a) + 2βhk(s, a),
Vehk(s) = [DπhQekh](s).
Then we can prove inductively that for any k, h, s and a we have
ʃ maχ(Qh,i- Qh,i)(s, a) ≤ Qh(S, a),
∖ maχ(V h,i- V h,i)(s) ≤ Vh(S).
Thus we only need to bound PkK=1 Ve1k (S). Define the shorthand notation
ʃeh ：= βhk (sh, ah),
<△h = Vk (sh),	~
'Zh ：= DnkQh (sh) — Qh(sh, ah),
怠：=PhVk (sh, ah) - ∆h+ι.
We can check Zhk and ξhk are martingale difference sequences. As a result,
△h =Dnk Qh (sh)
=Zk + Qh (sh, ah)
=ζk + 2βh+bhV+ι (sh, ah)
≤ζk + 3βh+PhV+ι (sh, ah)
=Zhk + 3βhk +ξhk +△kh+1.
(42)
(43)
(44)
Recursing this argument for h ∈ [H] and taking the sum,
K
X∆1k
k=1
≤ XX (Zk + 3βh+ξk) ≤ O(St
k=1
M
H3TιYAi .
i=1
□
H.1.2 CCE VERSION
The proof is very similar to the NE version. Specifically, the only part that uses the properties of NE
there is Lemma 35. We prove a counterpart here.
Lemma 36. With probability 1 - p, for any (S, a, h, k, i):
Qh,i (s, a) ≥ Qh，n-i (s, a),	Qh,i	(s, a)	≤ QXi(S, a),	(45)
Vh,i (S) ≥ Vtn-i (S),	Vh,i	(S) ≤	Vnk (S).	(46)
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case, we
∙↑,nk -
know at the (H + 1)-th step,VHk+1,i (S) = VH +-1i,i (S) = 0. Now, assume the inequality (40) holds
for the (h + 1)-th step, for the h-th step, by definition of the Q functions,
Qh,i (s,	a) -	Qh,,n-i	(s,	a)	=	[Ph。h+ιj (s,	a)	-	PhVh⅛	(s, a)+βt
=Ph (V h+ι,i - w+⅛)
、-----------------{z--------
(A)
(s, a) + (Ph - Ph) VXi (s, a)+βt.
|
^{z
(B)
35
Under review as a conference paper at ICLR 2021
By induction hypothesis, for any s0,
ι,i - Vh+-i) (s0) ≥ 0, and thus (A) ≥ 0. By uniform
concentration, (B) ≤ C^∣SH2ι∕Nhk (s, a)
βt. Putting everything together we have Qkh,i (s, a) -
Qh,h-i (s, a) ≥ 0. The second inequality can be proved similarly.
Now assume inequality (45) holds for the h-th step, by definition of value functions and CCE,
Vh,i (s) = Dπk Qh,i (s) = maxDμ×π-i Qh,i (S) ∙
By Bellman equation,
T-i (s)=maχDμ×∏-i Qh,,h-i (s).
Since by induction hypothesis, for any (s, a), Qhki (s, a) ≥ Qhh-i (s, a). As a result, we also have
Vh,i (s) ≥ Vh'ih-i (s), which is exactly inequality (40) for the h-th step. The second inequality can
□
be proved similarly.
H.1.3 CE Version
The proof is very similar to the NE version. Specifically, the only part that uses the properties of NE
there is Lemma 35. We prove a counterpart here.
Lemma 37. With probability 1 - p, for any (s, a, h, k, i):
Qh,i(S, a) ≥ mφχQh丁(S, a), Qh,i(S, a) ≤ Qhki(S, a),
Vh,i (s) ≥ maxVφf (s)， V3 (S) ≤ VS (S).
(47)
(48)
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case, we
_______________________________k	__φθ∏k
know at the (H + 1)-th step,VH+ι,i (S) = maxVH+ι,i (S) = 0. Now, assume the inequality (40)
holds for the (h + 1)-th step, for the h-th step, by definition of the Q functions,
Qh,i(s, a) - maxQhJk (s, a)
k
Pb
k
PhmaxVh+1,i (s, a) + βt
=Pbkh
<.
- mφaxVhφ+1hkki (S,a)+
-_	} I
{z
(A)
—
k
mφaxVh+1ki (S, a) + βt.
___ - /
{z
(B)
By induction hypothesis, for any s0,
1,i - mφaxVhφ+1π,ki (S0) ≥ 0, and thus (A) ≥ 0. By
uniform concentration, (B) ≤ CqSH2∣∕N((s, a)
βt . Putting everything together we have
Qh,i (s, a) - maxQh丁 (s, a) ≥ 0. The second inequality can be proved similarly.
Now assume inequality (47) holds for the h-th step, by definition of value functions and CE,
Vh,i (s) = Dπk Qh,i (S)= maxDφ^πk Qh,i (S) ∙
By Bellman equation,
k	0k
maxVhi	(S) = maxDφhk maxQhi (S) .
φ k	φ	φ0	k
36
Under review as a conference paper at ICLR 2021
Since by induction hypothesis, for any (s, a), Qhi (s, a) ≥ maxQ『： (s, a). As a result, We also
,	φ h,i
have V % (s) ≥ maxVhφi"π (s), which is exactly inequality (40) for the h-th step. The second
inequality can be proved similarly.	□
H.2 Proof of Theorem 16
In this section, we prove each theorem for the single reward function case, i.e., N = 1. The proof
for the case of multiple reward functions (N > 1) simply follows from taking a union bound, that
is, replacing the failure probability p by Np.
H.2.1 NE version
Let (μk, Vk) be an arbitrary Nash-equilibrium policy of Mck ：= (Pk,b), where Pk and rk are our
empirical estimate of the transition and the reward at the beginning of the k’th episode in Algorithm
4. Given an arbitrary Nash equilibrium πk ofMck, we use Qbkh,i and Vbhk,i to denote its value functions
of the i’th player at the h’th step in Mck.
We prove the following two lemmas, which together imply the conclusion about Nash equilibriums
in Theorem 16 as in the proof of Theorem 5.
Lemma 38. With probability 1 - p, for any (h, s, a, i, k), we have
(IQh,i(s, a) - Qii(s, a)∣ ≤ Qh(s, a),
［欧i(s)- Vnk(S)l≤ Vh(s).
(49)
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case,
we know at the (H + 1)-th step,VbHk+1,i = VHπk+1,i = QbkH+1,i = QπHk+1,i = 0. Now, assume the
conclusion holds for the (h + 1)’th step, for the h’th step, by definition of the Q functions,
Qbkh,i (s, a) - Qπh,ki (s, a)
≤ hPbkhVbhk+1,ii (s,a)- hPhVhπ+k1,ii (s,a)+rh(s,a)-rbhk(s,a)
|
- Vhπ+k1,i	(s,	a)	+	Pbkh	-Ph	Vhπ+k1,i	(s,	a) + rh(s,a)	-rbhk(s,a)
—{z---------------} '------------------------{--------------------------}
(A)	(B)
By the induction hypothesis,
(A) ≤ Pbkh Vbhk+1,i - Vhπ+k1,i (s, a) ≤ (PbkhVehk+1)(s, a).
By uniform concentration (e.g., Lemma 12 in Bai & Jin
Putting everything together we have
(2020)), (B) ≤ JSH2ι/Nk (s, a)
βt.
Qπh,ki (s, a) - Qbkh,i (s, a) ≤ minn(PbkhVehk+1)(s,a)+βt,Ho =Qekh(s,a),
which proves the first inequality in (49). The inequality for V functions follows directly by noting
that the value functions are computed using the same policy ∏k.	□
Lemma 39. With probability 1 - p, for any (h, s, a, i, k), we have
l<5h,i(s, a) - Qh-i't(s, a)l ≤ Qh(s, a),
1V,i(s)- V∏-i,t(s)∣ ≤ Vh(s).
(50)
37
Under review as a conference paper at ICLR 2021
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case,
We know at the (H + 1)-th step,VbHH+ι,i = VHn+〔'； = QH+ι,i = QH；：i = 0. Now, assume the
conclusion holds for the (h + 1)’th step, for the h’th step, by definition of the Q functions,
Qh,i (s, a) — Qhkrt (s, a)
[PhV+ιJ (s, a) — "V⅛it] (s, a) + 限(s,a) — 4(s,a)∣
Pbkh-PhVhπ+-k1i,,it(s,a)+rh(s,a)-rbhk(s,a)
{z^
(B)
By the induction hypothesis,
(A)	≤ Pbkh ∣∣Vbhk+1,i — Vhπ+-k1i,,it∣∣ (s, a) ≤ (PbkhVehk+1)(s, a).
By uniform concentration,
(B) ≤ JSH2ι∕Nk(s, a)
βt. Putting everything together we have
Qπh-,kii,t (s, a) — Qbkh,i (s, a)∣∣∣ ≤ min n(PbkhVehk+1)(s, a) + βt, Ho = Qekh(s, a),
which proves the first inequality in (50). It remains to show the inequality for V functions also hold
in h’th step.
Since πk is a Nash-equilibrium policy, we have
Vh,i (S) = maxDμ×π-iQh,i (S).
By Bellman equation,
Vh,i i' (S)= maxDμ×π-i Qhj (S).
Combining the two equations above, and utilizing the bound we just proved for Q functions, we
obtain
Vi (s) — Vh∏-i,t (s) ≤ mμxDμ×∏-iQh,i (s) — maxDμ×π-iQnYt (s)
≤ max Qekh(S, a) = Vehk (S),
a
which completes the whole proof.	□
H.2.2 CCE VERSION
The proof is almost the same as that for Nash equilibriums. We will reuse Lemma 38 and prove an
analogue of Lemma 39. The conclusion for CCEs will follow directly by combining the two lemmas
as in the proof of Theorem 5.
Lemma 40. With probability 1 — p, for any (h, S, a, i, k), we have
Qhπ,-kii,t(S, a) — Qbkh,i(S, a) ≤Qekh(S,a),
VSi%) - Vhki(S) ≤ Vh(s).
(51)
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case,
we know at the (H + 1)-th step,VbHk +1,i = VHπ-+i1,,ti = QbkH +1,i = QπH-+i,1t,i = 0. Now, assume the
38
Under review as a conference paper at ICLR 2021
conclusion holds for the (h + 1)’th step, for the h’th step, by definition of the Q functions,
Q∏-i,t (s, a) - Qh,i (s, a)
≤	Ph0i,,it	(s,	a)	- hPhVh+ι,i](S, a)+ 鼠 Ga)-bk Ga)I
≤Ph 1盍，-Vh+J
'-------{z---
(A)
(S, a) + Ph - Pbkh Vhπ+-k1i,,it (S, a) + IIrh(S,a) -rbhk(S,a)II.
}	{}
(B)
By the induction hypothesis, (A) ≤ (PbkhVehk+1 )(S, a).
By uniform concentration,
(B) ≤ JSH2∣∕Nk(s, a)
βt. Putting everything together we have
Qπh-,kii,t (S, a) - Qbkh,i (S, a) ≤ min n(PbkhVehk+1)(S, a) + βt, Ho = Qekh(S, a),
which proves the first inequality in (51). It remains to show the inequality for V functions also hold
in h’th step.
Since πk is a CCE, we have
Vk,i (S) ≥ maxDμ×π-iQh,i (S).
Observe that Vhπ,i-i,t obeys the Bellman optimality equation, so we have
maxDμ×π-iQn-i,t (S).
Combining the two equations above, and utilizing the bound we just proved for Q functions, we
obtain
Vn-i,t (s) - Vhki (s) ≤ mμtxDμ×∏-iQin-,t (s) - mμxDμ×∏-iQh,i (S)
≤ max Qekh(S, a) = Vehk(S),
a
which completes the whole proof.	□
H.2.3 CE version
The proof is almost the same as that for Nash equilibriums. We will reuse Lemma 38 and prove an
analogue of Lemma 39. The conclusion for CEs will follow directly by combining the two lemmas
as in the proof of Theorem 5.
Lemma 41. With probability 1 - p, for any (h, S, a, i, k) and strategy modification φ for player i,
we have
Qhφ,iπk (S, a) - Qbkh,i(S, a) ≤Qekh(S,a),
Vhφ,iπk (S) - Vbhk,i(S) ≤ Vehk(S).
(52)
Proof. For each fixed k, we prove this by induction from h = H + 1 to h = 1. For base case,
we know at the (H + 1)-th step,VbHk+1,i = VHφ+π1k,i = QbkH+1,i = QφH+π1k,i = 0. Now, assume the
conclusion holds for the (h + 1)’th step, for the h’th step, following exactly the same argument as
Lemma 40, we can show
Qφh,iπk (S, a) - Qbkh,i (S, a) ≤ min n(PbkhVehk+1)(S, a) + βt, Ho = Qekh(S, a),
which proves the first inequality in (52). It remains to show the inequality for V functions also hold
in h’th step.
39
Under review as a conference paper at ICLR 2021
Since πk is a CE, we have
V,i (S) = maχDφh QnkQbh,i(S),
φh,s h,s
where the maximum is take over all possible injective functions from Ai to itself.
k
Observe that Vhφ,iπ obeys the Bellman optimality equation, so we have
VhT (S) = maxDΦhs∕ Qtink (S).
φh,s	,
Combining the two equations above, and utilizing the bound we just proved for Q functions, we
obtain
VhT (S)- Vi (S) = maxDφhs,∏kQtf (S)- maxDφhs,∏kQh,i (S)
φh,s	,s	φh,s ,s
≤ max Qekh (S, a) = Vehk (S),
a
which completes the whole proof.	□
40