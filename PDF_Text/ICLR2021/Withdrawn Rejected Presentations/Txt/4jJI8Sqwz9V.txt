Under review as a conference paper at ICLR 2021
Generalization and Stability of GANs:
A theory and promise from data augmentation
Anonymous authors
Paper under double-blind review
Ab stract
The instability when training generative adversarial networks (GANs) is a no-
toriously difficult issue, and the generalization of GANs remains open. In this
paper, we will analyze various sources of instability which not only come from
the discriminator but also the generator. We then point out that the requirement
of Lipschitz continuity on both the discriminator and generator leads to general-
ization and stability for GANs. As a consequence, this work naturally provides a
generalization bound for a large class of existing models and explains the success
of recent large-scale generators. Finally, we show why data augmentation penal-
izes the gradients of both the discriminator and generator. This work therefore
provides a theoretical basis for a simple way to ensure generalization in GANs,
explaining the highly successful use of data augmentation for GANs in practice.
1	Introduction
In Generative Adversarial Networks (GAN) (Goodfellow et al., 2014), we often want to learn a
discriminator D and a generator G by solving the following minimax problem:
min max Ex〜Pdlog(D(X)) + Ez〜Pzlog(I - D(G(Z)))	(1)
where pd is a data distribution that generates real data, and pz is some noise distribution. G can be
parameterized by a neural network to map a noise z to a point in the data space. After training, G
can be used to generate novel data which are as realistic as possible.
Since its introduction by Goodfellow et al. (2014), a significant progress has been made for devel-
oping GANs and for interesting applications (Hong et al., 2019). Some recent works (Brock et al.,
2019; Zhang et al., 2019) can train a generator that produces synthesis images of extremely high
quality. Nevertheless, little is known about the generalization of the trained players, and the training
is nororiously challenging due to instability (Salimans et al., 2016; Arjovsky & Bottou, 2017; Gul-
rajani et al., 2017; Kodali et al., 2017; Fedus et al., 2018; Miyato et al., 2018; Kurach et al., 2019;
Wu et al., 2019; Thanh-Tung et al., 2019; Xu et al., 2020; Chu et al., 2020).
This work has the following contributions:
-	We make a bridge between Lipschitz continuity of a loss and generalization of a hypothesis.
Basically, we show that the learnt hypothesis will generalize well if the training loss is
Lipschitz continuous w.r.t input. This result enables us to easily analyze the players in a
large class of GANs, and provides their generalization bounds. Those bounds theoretically
explain the success of various existing techniques to stabilize GAN training, including
gradient penalty (Gulrajani et al., 2017) and spectral normalization (Miyato et al., 2018),
and the success of recent large-scale generators (Brock et al., 2019; Zhang et al., 2019).
-	We analyze various sources of instability, including the discriminator, the generator, and the
noise. We point out why existing understandings are insufficient to deal with instability.
-	Finally, we show why data augmentation implicitly imposes a gradient penalty, and ex-
plain how to properly use this simple technique to ensure small Lipschitz constants of the
discriminator and generator. As a result, we provide a theory for explaining the highly suc-
cessful use of data augmentation for GANs in recent practices (Zhao et al., 2020c; Zhang
et al., 2020; Zhao et al., 2020a).
1
Under review as a conference paper at ICLR 2021
2	Related work
Generalization: There are few efforts to analyze the generalization for GANs using the notion of
neural distance, d。(μ, V) which is the distance of two distributions (μ, V). Arora et al. (2017) show
that We can bound the quantity |d。(μ, V) - dD(μm, Vm)|, where (μm, Vm) are empirical versions
of (μ, v). Zhang et al. (2018) and Jiang et al. (2019) analyze |d。(μ, Vm) — infV d。(μ, ν)∣ to see
generalization of G. To obtain those bounds, we need the assumption of Lipschitz continuity of
D w.r.t its parameters. Note that those notions of generalization are different from the classical
sense, because the neural distance is defined on the best discriminator in its family D, suggesting
that the distance can be zero even when μ and V are far away. Indeed, Arjovsky & Bottou (2017)
show that there exists a perfect but constant discriminator whenever the supports of μ and V are
non-overlapping. Therefore, existing bounds based on neural distance are insufficient to see the
generalization of the learnt hypothesis. On the other hand, Qi (2020) shows a generalization bound
of the generator for their proposed Loss-Sensitive GAN. Nonetheless, it is nontrivial to make their
bound to work with other GAN losses. Wu et al. (2019) show that the discriminator will generalize
if the learning algorithm is differentially private. Their concept of differential privacy basically
requires that the learnt hypothesis will change negligibly if the training set slightly changes. Such a
requirement is known as algorithmic stability (Xu et al., 2010) and is nontrivial to assure in practice.
In contrast, we show that under the assumption of Lipschitz continuity of the loss w.r.t input, both
D and G generalize well in the classical sense. Our bounds apply to a large class of GANs.
Techniques for stabilizing GAN training: Instability can come from different sources, such as gra-
dient vanishing (Arjovsky & Bottou, 2017), gradient exploding (Gulrajani et al., 2017; Thanh-Tung
et al., 2019), gradient uninformativeness (Zhou et al., 2019), ill-condition of the Jacobian of the
players w.r.t their parameters (Mescheder et al., 2017; Nie & Patel, 2019). Since instability ap-
pears frequently and really challenging, a lot of approaches have been proposed and can be divided
into four main groups: network architecture (Radford et al., 2016; Salimans et al., 2016; Zhang
et al., 2019; Karras et al., 2018; 2019; Karnewar & Wang, 2020; Schonfeld et al., 2020), training
loss (Mao et al., 2017; 2019; Zhao et al., 2017; Arjovsky et al., 2017; Li et al., 2017; Jolicoeur-
Martineau, 2019; Qi, 2020; Guo et al., 2020), optimization method (Heusel et al., 2017; YazICI et al.,
2019; Wang et al., 2020; Chavdarova et al., 2020; Chu et al., 2020), and regularization (Gulrajani
et al., 2017; Mescheder et al., 2017; Nagarajan & Kolter, 2017; Roth et al., 2017; Sanjabi et al.,
2018; Miyato et al., 2018; Guo et al., 2019; Nie & Patel, 2019; Zhou et al., 2019; Qi, 2020; Zhang
et al., 2020; Xu et al., 2020). Our work identifies further sources of instability, and theoretically
shows why data augmentation (a very cheap and practical way) can help stabilizing GAN training.
What’s missing in the existing literature about the connection between Lipschitz continuity, stability,
and generalization? Lipschitz continuity naturally appears in the formulation of Wasserstein GAN
(WGAN) (Arjovsky et al., 2017). It was then quickly recognized as a crucial component to improve
various GANs (Fedus et al., 2018; Lucic et al., 2018; Mescheder et al., 2018; Kurach et al., 2019;
Jenni & Favaro, 2019; Wu et al., 2019; Zhou et al., 2019; Qi, 2020; Chu et al., 2020). Gradient
penalty and spectral normalization are two popular techniques to constraint the Lipschitz continuity
of D or G w.r.t their inputs. Some other works (Mescheder et al., 2017; Nagarajan & Kolter, 2017;
Sanjabi et al., 2018; Nie & Patel, 2019) suggest to control the Lipschitz continuity of D or G w.r.t
their parameters. From extensive experiments, those works found that Lipschitz continuity can help
improving stability and quality of GANs. However, we will point out why Lipschitz constraints only
on D or G are insufficient to ensure generalization and stability.
Continuity in parameters versus inputs: It is worth mentioning that there are the two kinds of Lips-
chitz constraint: continuity in inputs (Gulrajani et al., 2017; Jenni & Favaro, 2019; Wu et al., 2019;
Zhou et al., 2019; Qi, 2020), and continuity in parameters (Mescheder et al., 2017; Nagarajan &
Kolter, 2017; Sanjabi et al., 2018; Nie & Patel, 2019). Note that parameter continuity does not al-
ways imply input continuity. Chu et al. (2020) suggests that both D and G need to be smooth in both
their inputs and parameters in order to ensure that the learning for generator will converge, given a
fixed discriminator. It is worth noting that optimization convergence does not always imply good
generalization for GANs. Our work shows that Lipschitz continuity of the training loss w.r.t. input
is sufficient to guarantee generalization for both D and G.
Data augmentation for GANs: Data augmentation (Shorten & Khoshgoftaar, 2019) has been playing
a crucial role in various areas. Some recent works (Zhao et al., 2020c; Zhang et al., 2020; Zhao et al.,
2
Under review as a conference paper at ICLR 2021
2020a) found that it is really beneficial to exploit data augmentation for training GANs. However,
there is a lack of theory to explain those observations. This work will fill this gap.
3	Lipschitz continuity, Robustness, and Generalization
In this section, we will review an important result by Xu & Mannor (2012) about the close relation
between the robustness and generalization of a learning algorithm. We then show how robustness
connects to Lipschitz continuity. Finally we point out why imposing a Lipschitz constraint on the
loss will lead to generalization and discuss an application to GANs.
Consider a learning problem specified by a hypothesis class H, an instance set Z, and a loss function
f : H × Z → R which is bounded by a constant C. Given a distribution with density p(z) defined on
Z, the quality of a hypothesis is measured by its expected loss F(h) = Ez〜p(z)[f (h; z)]. Since P(Z)
is unknown, we need to rely on a finite training sample S = {z1, ..., zm} ⊂ Z and often work with
the empirical loss FS(h)=煮 Pz∈s f (h; z). A learning algorithm A will pick a hypothesis based
on input S. One can interpret a learning algorithm as a mapping from a subset ofZ to a hypothesis.
Let Z = SiK=1 Zi be a partition of Z into K disjoint subsets. We use the following definition about
robustness of an algorithm.
Definition 1 (Robustness) An algorithm A is (K, e)-robust, for e(∙) : Zm → R, if the following
holds for all S ∈ Zm:
∀s ∈ S,∀z ∈ Z,∀i ∈ {1,..., K} : ifs,z ∈ Zi then |f(A(S),s) - f(A(S),z)| ≤ (S).	(2)
Basically, a robust algorithm will learn a hypothesis which ensures that the losses of two similar data
instances should be the same. A small change in the input leads to a small change in the ouput of
the learnt hypothesis. In other words, the robustness ensures that each testing sample which is close
to the training dataset will have a similar loss with that of the closest training samples. Therefore,
the hypothesis A(S) will generalize well over the areas around S.
Theorem 1 (Xu & Mannor (2012)) If a learning algorithm A is (K, )-robust, and the training
data S is an i.i.d. sample from distribution p(z), then for any δ ∈ (0, 1] we have the following with
probability at least 1 一 δ: |F(A(S)) — FS(A(S))I ≤ E(S) + CP(Klog4 - 2logδ)∕m.
This theorem formally makes the important connection between robustness and generalization of
an algorithm. Essentially, an algorithm will generalize if it is robust. One important implication of
this result is that we should ensure the robustness of a learning algorithm in practice. However, it is
nontrivial to do so.
Let us have a closer look at robustness. E(S) in fact bounds the amount of change in the ouput
with respect to a change in the input of the loss function given a fixed hypothesis. This observation
suggests that robustness closely resembles the concept of Lipschitz continuity. Remember that a
function y : Z → Y is said to be L-Lipschitz continuous if dy(y(z), y(z0)) ≤ Ldz (z, z0) for any
z, z0 ∈ Z, where dz is a metric on Z, dy is a metric on Y, and L ≥ 0 is the Lipschitz constant.
Therefore, we establish the following connection between robustness and Lipschitz continuity.
Lemma 2 Given any constant λ > 0, consider a loss f : H × Z → R, where Z ⊂ Rn is com-
pact, B = supz,zo∈z dz(z,z0) = supz,zo∈z ∣∣z — z0∣∣∞,K = dBnλ-n~∖. If f (A; z) is L-Lipschitz
continuous w.r.t input z, then algorithm A is (K, Lλ)-robust.
Proof: It is easy to see that there exist K = d(B∕λ)ne disjoint n-dimensional cubes, each with edge
length of λ, satisfying that their union covers Z completely since Z is compact. Let Ck be one of
those cubes, indexed by k, and Zk = Z ∩ Ck . We can write Z = SkK=1 Zk .
Consider any s, z ∈ Z. If both s and z belong to the same Zk for some k, we have
If (A; s) — f (A; z)∣ ≤ L∖∖z — z0∣∣∞ ≤ Lλ due to the Lipschitz continuity of f, completing the
proof.
Combining Theorem 1 and Lemma 2, we make the following connection between Lipschitz conti-
nuity and generalization.
3
Under review as a conference paper at ICLR 2021
Theorem 3 (Lipschitz continuity ⇒ Generalization) If a loss f(A; z) is L-Lipschitz continuous
w.r.t input z in a compact set Z ⊂ Rn, and the training data S is an i.i.d. sample from distribution
p(z), then for any constants δ ∈ (0, 1] and λ ∈ (0, B] we have the followings with probability at
least 1 - δ: |F(A(S)) - FS(A(S))| ≤ Lλ + √1m√dBnλ-n] log4 - logδ2, and
|F(A(S))- Fs(A(S))I ≤ LB + ɪPlog4 - log δ2.
(3)
This theorem tells that Lipschitz continuity is the key to ensure an algorithm A (and the learnt
hypothesis) to generalize. A generalizes better as the Lipschitz constant of the loss decreases. Note
that there is a tradeoff between the Lipschitz constant and the expected loss F (A(S)) of the learnt
hypothesis. A smaller L means that both f and hypothesis h = A(S) are getting simpler and flatter,
due to ∂f = ∂h ∂Z，and hence may increase F(h). In contrast, a decrease of F(h) may require h to
be more complex and hence may increase the Lipschitz constants of both f and h. It is worth noting
that the Lipschitz constant of the loss also depends on how fast the loss changes w.r.t h.
Application to GANs: Consider vd(D, G, x, z) = ψ1(D(x))+ψ2(1-D(G(z))) andvg(D, G, z) =
ψ3(l - D(G(Z))) being the losses defined from a real example X 〜pd, a noise Z 〜PZ, a discrim-
inator D, and a generator G. Different choices of the measuring functions (ψ1, ψ2 , ψ3) will lead to
different GANs. For example, the vanilla GAN (Goodfellow et al., 2014) uses ψ1(x) = ψ2(x) =
ψ3(x) = log(x); WGAN (Arjovsky et al., 2017) uses ψ1(x) = ψ2(x) = ψ3(x) = x; LSGAN (Mao
et al., 2017; 2019) uses ψ1(x) = -(x+a)2, ψ2(x) = -(x+b)2, ψ3(x) = (x+c)2 for some constants
a, b, c; EBGAN (Zhao et al., 2017) uses ψ1(x) = x, ψ3(x) = 1 - x and ψ2 (x) = max(0, r - x) for
some constant r. Then the expected loss and empirical loss defined on S = {xi , Zi }im=1 are:
D loss:	Vd	=	Ex〜pd,z〜PzVd(D,G,	x,z) = Ex〜pdΨ1(D(x)) + EZ〜PzΨ2(l	- D(G(Z)))
G loss:	Vg	=	EZ〜PzVg(D,G, z) =	EZ〜Pzψ3(l - D(G(Z)))
Empirical losses:
QS = m X Vd(D,G,x, Z); Vg,S 「 X Dg(DGz
(x,Z)∈S	Z∈S
It is worth observing that the Lipschitz continuity of Vd and Vg depends on that of the measuring
functions and the players. Therefore, the following results readily come from Theorem 3.
Corollary 1 (Generalization in GANs) Assume the training data S = {xi , Zi}im=1 consists of m
i.i.d. samples from real distribution pd defined on a compact set Zx ⊂ Rn andm i.i.d. samples from
fake distribution PZ defined on a compact set ZZ ⊂ Rnz ,denote Bx = supx,xo∈ζx ||x 一 x0∣∣∞,Bz =
suPz,zo∈zz ||z — z0∣∣∞ ∙ Assume further that (ψι, ψ2 ,ψ3) are Lψ -Lipschitz continuous. Given any
constant δ ∈ (0, 1], the following generalization bounds hold with probability at least 1 - δ:
1.	∣Vd — Vd,s∣ ≤ LψLdBx + √= Hog4 — log δ2, for any given G, provided that D is Ld-
Lipschitz continuous w.r.t x.
2.	∣ Vg — VgS ∣ ≤ LψLdLgBZ + √= /log4 - log δ2, for any given D, provided that D is
Ld-Lipschitz continuous w.r.t x and G is Lg -Lipschitz continuous w.r.t Z.
For many models, such as WGAN, the measuring functions and D are Lipschitz continuous w.r.t
their inputs. As a result, the generalization bound for D in Corollary 1 naturally holds. Note that
the generator in WGAN, LSGAN, and EBGAN will be Lipschitz continuous w.r.t Z, ifwe use some
regularization methods such as gradient penalty or spectral normalization for both players. This
provides a significant evidence to explain the success of some large-scale generators (Zhang et al.,
2019; Brock et al., 2019), which use spectral normalization. Unfortunately, the loss of the vanilla
GAN and many other variants are not guaranteed to be Lipschitz continuous due to the use of log
function. Therefore, the generalization bound (3) may not be directly applied.
One natural suggestion from Theorem 3 and Corollary 1 for improving generalization in GANs is
either taking more training data or decreasing the Lipschitz constant of the GAN loss. It is worth
observing that a small Lipschitz constant of the loss not only requires that both discriminator and
generator are Lipschitz continuous w.r.t their inputs, but also requires the Lipschitz continuity of the
4
Under review as a conference paper at ICLR 2021
loss w.r.t both players. Some existing losses, such as saturating and non-saturating GANs (Goodfel-
low et al., 2014), do not satisfy the latter requirement. Interestingly, most existing efforts focus on
ensuring the Lipschitz continuity of the players in GANs, and leave the loss open. As pointed out in
Section 4, constraining on either discriminator or generator only is not sufficient to ensure Lipschitz
continuity of the loss and therefore the generalization of those players.
Comparison with other generalization bounds: There is a line of works (Arora et al., 2017; Zhang
et al., 2018; Jiang et al., 2019) that analyze the generalization for GANs in terms of neural distance.
Let (μ, V) be two distributions and (μm, Vm) be their empirical versions estimated from a sample S
of size m. We define the neural distances: d。(μ, V) = sup°∈D ∣Eχ〜pd,z〜Pz Vd(D, G, x, z)∣-2ψ( 1)
and d。(μm,Vm) = sup°∈D |mm P(χ,z)∈s Vd(D, G,x,z)∣ - 2ψ(1), where ψ(∙) = ψι(∙) = ψ2(∙),
D is a family of discriminators, V is the distribution induced by generator G(z), Z 〜Pz. Arora et al.
(2017) analyze the generalization by upper bounding the quantity |d。(μ, V) - d。(μm,, Vm)|, while
(Zhang etal., 2018; Jiang et al., 2019) analyze |d。(μ, Vm) — infV d。(μ, V)| to see generalization of
G, under the assumption of Lipschitz continuity of D w.r.t its parameters. We emphasize that those
notions of generalization are different from the classical sense. Indeed, the neural distance d。(μ, V)
is defined on the best discriminator, suggesting that the distance can be zero even when μ and V
are far away. Note that there will exist a constant but perfect D, meaning d。(μ, V) = 0, whenever
μ and V do not have overlapping supports (Arjovsky & Bottou, 2017). Therefore, existing bounds
based on neural distance do not tell much about the generalization of GANs. In contrast, our work
provides generalization bounds in the classical sense for both players.
Qi (2020) shows a generalization bound of the generator for their Loss-Sensitive GAN, which con-
tains a Lipschitz regularizer for the generator and the margin between the real and fake distributions.
Nonetheless, it is nontrivial to make their bound to work with other GAN losses. Wu et al. (2019)
show that the discriminator will generalize if the learning algorithm is differentially private. Their
concept of differential privacy basically requires that the learnt hypothesis h = A(S) will change
negligibly if the training sample S slightly changes. Such a requirement is known as algorithmic
stability (Xu et al., 2010) and resembles the Lipschitz continuity of the learning algorithm A(S).
It is worth observing that the Lipschitz continuity of an algorithm is a strong assumption and is
nontrivial to assure in practice. In contrast, we will point out in Section 5 that one can guarantee the
Lipschitz continuity of the loss w.r.t input, such as simply by using noise or data augmentation.
4 Sources of instability in GAN
For ease of observing instability sources, consider the vanilla GAN formulation (1) in the case of
only 1 real sample x and 1 fake sample xz = G(z) associated with noise z. Let discriminator
D = D(x; θ) and generator G = G(z; φ) be parameterized by two neural networks with parameters
θ and φ, respectively. The loss and its partial derivatives can be written as V (D, G, x, z, θ, φ) =
log D(x) + log(1 - D(G(z))),
∂V	∂V ∂D	∂V	∂V ∂G	∂V ∂D ∂G
∂x 二	二 ∂D∂x,	^∂Z 二	二 ∂G∂Z =	二 ∂D∂G∂Z
∂V	∂V ∂D	∂V	∂V ∂G	∂V ∂D ∂G
^∂θ 二	二 ∂D aθ,	就二	二 ∂G∂φ 二	二 ∂d∂g aφ
(4)
(5)
4.1	Instability from Lipschitz constraint on one player only
One can observe that the Lipschitz continuity of the loss w.r.t input z depends heavily on the Lip-
schitz continuity of three functions: the loss V over D, the discriminator D over fake samples
xz = G(z), and the generator G over z. Some existing losses naturally maintain the continuity of
V over D, including WGAN, LSGAN, and EBGAN. However, many other losses do not ensure the
Lipschitz continuity of V over D, e.g., the vanilla GAN.
Except some recent works (Arjovsky & Bottou, 2017; Guo et al., 2019; Jenni & Favaro, 2019; Qi,
2020), many existing efforts (Arjovsky et al., 2017; Gulrajani et al., 2017; Roth et al., 2017; Fedus
et al., 2018; Miyato et al., 2018; Kurach et al., 2019; Zhou et al., 2019; Thanh-Tung et al., 2019;
Jiang et al., 2019; Tanielian et al., 2020; Zhao et al., 2020b; Xu et al., 2020; Chu et al., 2020) try to
ensure the Lipschitz continuity of the discriminator only. The most popular techniques are gradient
5
Under review as a conference paper at ICLR 2021
Figure 1: Some statistics when training the vanilla GAN on MNIST. When no penalty was applied,
the training algorithm was unstable and unconvergent as shown in the last two subfigures. When
spectral normalization was used for G only, the vanishing gradient problem appeared and G did
not learn anything from D, and hence the training process stopped early. In contrast, the training
behaved very well when spectral normalization was used for D. It can be seen from the middle
subfigure that the Lipschitz constant of G can be much smaller if we use spectral normalization for
both Players. ∣∣∙∣∣f denotes the FrobenioUs norm.
penalty (Gulrajani et al., 2017) and spectral normalization (Miyato et al., 2018; Jiang et al., 2019).
Those two techniqUes are really UsefUl for different losses (FedUs et al., 2018) and high-caPacity
architectUres (KUrach et al., 2019). One reason is that constraining the LiPschitz constant of D
indirectly redUces those of the loss w.r.t inPUt z and generator Parameter φ, as indicated in eqUations
(4) and (5). From a large-scale evalUation, KUrach et al. (2019) foUnd that gradient Penalty can helP
the Performance of GANs bUt does not stabilize the training, whereas Using sPectral normalization
on G only is insUfficient to ensUre stability (Brock et al., 2019) and can lead to vanishing gradient
Problem as shown in FigUre 1. SUch behaviors can be Partly exPlained by Using eqUation (4) and the
analysis in the last section. EqUation (4) sUggests that a LiPschitz constraint on only D (or G) may
not ensUre the LiPschitz continUity of the loss. Meanwhile, stability basically reqUires the LiPschitz
continUity of the loss w.r.t both the learning algorithm and inPUt (x, z). Existing works mostly focUs
on inPUt x, bUt forget inPUt z and the asPect of algorithmic stability (XU et al., 2010).
EqUation (4) sUggests that the discriminator shoUld be LiPschitz continUoUs over both real and fake
samPles. This sUPPorts the Use of gradient Penalty in (GUlrajani et al., 2017; Thanh-TUng et al.,
2019) where D is reqUired to be LiPschitz continUoUs over the smooth interPolation of real and
fake samPles. FUrthermore, leaving G oPen may caUse inherent difficUlties to ensUre the LiPschitz
continUity of V . As a resUlt, we shoUld simUtaneoUsly make sUre the LiPschitz continUity of three
fUnctions: the loss V over D, the discriminator D over (x, G), and the generator G over z. FigUre 1
shows the behaviors of the vanilla GAN in different cases. We observe that withoUt any LiPschitz
constraint, both V and D tend to have high LiPschitz constants or be non-smooth, and the training
algorithm even did not converge. When sPectral normalization is Used for both Players, those be-
haviors can be avoided. It is also worth observing that when sPectral normalization is aPPlied to G,
there is a significant imProvement in the Jacobian norm of G.
4.2	Instability from unconvergence
Unconvergent G: It is easy to see that when G is not convergent, the fake samPles generated by G
are often of low qUality, and therefore can be correctly classified by a strong D. This case easily
leads to the vanishing gradient Problem (Arjovsky & BottoU, 2017) and the generator can hardly
learn anything then. The unconvergence of G means ∣∣ d∂G∣∣f is large and so ∣∣ ∂∂φ∣∣f may be large.
This case can haPPen at the early stage of the learning Process, or when we train D Until oPtimality
at each training iteration, or when there is no constraint on G as evidenced in Figure 1.
Unconvergent D: This case can haPPen at the early stage of the training Process or in the case of
no Penalty on D as suggested in Figure 1. In this case, D may not be strong and may make some
inaccurate Predictions for both real and fake samPles. An inaccurate Prediction of a fake samPle
Xz, i.e. D(Xz) ≈ 1, means that ∂dv- = --")∂dD and dV = 斐 ⅜z may have a large norm.
Similarly, an inaccurate prediction of a real sample x, i.e. D(X) ≈ 0, means that dV
1 ∂D
D(x) ∂x
may have a large norm. Some inaccurate predictions can make a significant changes for the updates
of both D and G, and may cause mode collapse or gradient explosion (Thanh-Tung et al., 2019). As
6
Under review as a conference paper at ICLR 2021
Table 1: Some behaviors of GAN when chang-
ing the size (Bz) of the noise domain. Spec-
tral normalization was used for D, LV g was
estimated by max ∣∣ d∂vg ∣∣f from 5000 random
noises. The lower the better.
Bz	LVgBz	loss Vg	-D(G(Z))
10-2	5.020	-0.657 ± 0.065	-0.480 ± 0.033
100-	2.738	-0.667 ± 0.054	-0.486 ± 0.027
102	2.370	-0.696 ± 0.045	-0.500 ± 0.022
Table 2: Some behaviors of GAN when chang-
ing σ for data augmentation, by random trans-
lation. Spectral normalization was used for D,
the statistics were estimated from 5000 random
noises. The lower the better.______________
σ		√L	√24
k ∂GDZ) kF-	0.087 ± 0.005	0.068 ± 0.003
k ∂ kF	78.749 ± 36.405	76.422 ± 35.867
loss Vg	-0.670 ± 0.050	-0.701 ± 0.063
-D(G(Z))	-0.487 ± 0.026	-0.503 ± 0.031
a result, when stopping the learning process with a unconvergent discriminator, both V and D may
have a large Lipschitz constant. Corollary 1 suggests that D and G may not generalize well.
4.3	Instability from the generator input
One issue from noise can be observed from the generalization bound in Corollary 1. The bound will
be useful if the domain of z is bounded. This suggests that we should restrict the domain of z when
sampling z . However, there is an inherent tradeoff between the size Bz and the Lipschitz constant
LV g of Vg . The first two columns of Table 1 provide an evidence about that tradeoff. We observe
further from Table 1 that the generalization of G, indicated by the loss Vg and the score D(G(z)),
increases as LV gBz decreases. Those results consistently support the bound in Corollary 1.
Another possible issue is that a training algorithm for GANs often works with a dynamic environ-
ment, for which the training sets for both G and D change over epochs. For example, in the training
algorithm by Goodfellow et al. (2014), each minibatch picks randomly a noise sample to put through
the generator to make fake inputs for training the discriminator. It turns out that we use different
training sets for G over epochs. In other words, the training sets for both G and D are dynamic.
Such a dynamic nature may be a source for instability. This partly explains why Shrivastava et al.
(2017) propose to keep a histrory of refined images to train D and to make the overall training more
stable. When training from a dynamic dataset, a naive SGD-based method is not guaranteed to con-
verge. Such an unconvergence has been observed by Mescheder et al. (2018), and also can be seen
from Figure 1 in the case of no penalty.
5	Why does data augmentation impose a Lipschitz constraint?
In this section, we study a perturbed version of GANs, which allows us to analyze data augmentation.
We point out why data augmentation penalizes the norms of the Jacobians of both players, and hence
improves stability and generalization for GANs.
Consider the following formulation:
min max EJEχ〜Pdlog D(X + e)] + Ee [Ez〜Pzlog(I — D(G(Z) + e))],	(6)
where = σu and u follows a distribution with mean 0 and covariance matrix I, σ is a non-negative
constant. Note that when u is the Gaussian noise, the formulation (6) turns out to be the noisy
version of GAN (Arjovsky & Bottou, 2017).
Connection to data augmentation: One difference between (1) and (6) is the input for the discrim-
inator. Note that each input for D in (6) is perturbed by an . It is easy to see that when has small
norm, each x0 = x + is a local neighbor of x and is a perturbed version of x. Noise is a common
way to make perturbation and can lead to stability for GANs (Arjovsky & Bottou, 2017). Nonethe-
less, some works (Arjovsky et al., 2017; Zhang et al., 2020) found that using noise often results
in G to produce blurry images. Another way to make perturbation is data augmentation, including
translation, cutout, rotation. The main idea is to make another version x0 from an original image x
such that x0 should preserve the semantic of x. By this way, x0 belongs to the neighborhood of x in
some senses, and therefore, can be represented as x0 = x+ for some . Those observations suggest
that when training D and G from a set of original and augmented images (Zhao et al., 2020c; Zhang
et al., 2020; Zhao et al., 2020a), we are working with an empirical version of the loss in (6).
7
Under review as a conference paper at ICLR 2021
Data augmentation penalizes the Jacobian norms: With some abuse of notation, we denote pd+
be the distribution that generates sample of the form x + , pg+ be the distribution that generates
sample of the form G(z)+e, and V (D, G) = E∕Eχ 〜Pdlog D(x+e)]+Ee[Ez^pz log(1 -D(G(z) +
e))] = Ex〜pd [Ee log D(X + e)] + Ez〜pz [Ee log(1 - D(G(Z) + €))]. Given a fixed G, the optimal
discriminator is D*(x) = Pd+ pd+Px[ (x) according to Arjovsky & BottoU (2017). The learning
G is to minimize V(D*, G) given fixed D*. By using the same argument as Goodfellow et al.
(2014), one can see that training G is eqUivalent to minimizing Ee [JS(pd+e , pg+e )], where JS is
the Jensen-Shannon divergence. It is worth observing that
min Ee[J S(pd+e, pg+e)] ≥ min Ee[JS(pd, pg+e)],	(7)
min Ee[J S(pd+e, pg+e)] ≥ min Ee [JS(pd+e, pg)].	(8)
They suggest that training G tries to push pg+e toward pd and push pg toward pd+e . The following
results provide some important implications (the proof appears in Appendix B).
Lemma 4 Let d(p, q) = Ex〜pd[(p(x) — q(x))2] be the squared distance between twofunCtionsp(x)
and q(x), Jx(q) be the Jacobian ofq w.r.t x. Assuming pd and pg are differentiable everywhere, then:
+ Ee[d(pd,Pg+e)] = d(Pd,Pg + o(σ)) + σ2Ex〜PdEu [uτJx(Pg)JT(Pg)u],
+ Ee[d(Pd+e,Pg)] = d(Pd + o(σ),Pg) + σ2Ex〜PdEu [uτJx(Pd)JT(Pd)UI.
Lemma 5 If U 〜N(0,I) then Eu [uτ Jx(Pg)JT(Pg)u] = trace(Jx(Pg)JT(Pg)) = || Jx(Pg)∣∣F
and Eu uTJx(Pd)JxT(Pd)u = trace(Jx(Pd)JxT(Pd)) = ||Jx (Pd)||2F.
Lemma 5 comes from a well-known result (Avron & Toledo, 2011; Hutchinson, 1989) which shows
Eu UTAU = trace(A) given a fixed matrix A. Similar results hold true for some other types ofU,
such as Rademacher random variables. Although Lemmas 4 and 5 work with squared distance, they
are really helpful to interpret the nontrivial implications when training G and D by (6).
Firstly, when training G, we are trying to minimize the expected norms of the Jacobians of the
densities induced by D and G. Indeed, training G will minimize Ee [JS(Pd+e , Pg+e )], and thus
also minimize Ee [JS(Pd, Pg+e )] according to (7). Because JS is a proper distance, minimizing
Ee [J S(Pd, Pg+e)] leads to minimizing Ee [d(Pd, Pg+e)]. Combining this observation with Lemma 4,
We find that training G requires both d(Pd,Pg + o(σ)) and Ex〜PdEu [ut Jx(Pg) JTT(Pg)u] to be
small. As a result, Ex 〜?壮||| Jx(Pg )∣∣F ] should be small due to Lemma 5. The same argument applies
to Ex〜Pd[∣∣Jx(Pd)∣∣F]. Since D*(χ) = Pd+ Pd+)x[⑺,we can conclude that data augmentation
for both real and fake images will implicitly pose Lipschitz constraints on D and G. Furthermore,
augmentation for only fake images imposes a Lipschitz constraint on G, while augmentation for
only real images imposes a Lipschitz constraint on D.
Secondly, there is a tradeoff in data augmentation. Making augmentation from a larger region around
a given image implies a larger σ . Lemmas 4 and 5 suggest that the Jacobian norms should be
smaller, meaning the flatter learnt distributions. Hence, too large region for augmentation may
result in underfitting. On the other hand, augmentation in a too small region (a small σ) allows the
Jacobian norms to be large, meaning the learnt distributions can be more complex. As σ → 0, no
regularization is used at all. In those cases the generalization of the players may not be guaranteed.
Table 2 shows some statistics from our simulation on MNIST data, where 15 random translations
were applied to each image. We observed that increasing σ often results in smaller norms of the
Jacobian of both G and D . It seems that a larger σ leads to better fake images as indicated by
D(G(z)) and Vg on 5000 testing noises. Those results are consistent with our analysis before and
the observations by Zhao et al. (2020c).
6	Conclusion
We have discussed the generalization of GANs under more general settings than existing studies,
and pointed out a simple way to improve generalization for the players in GANs. Our work provides
a theoretically grounded explanation for the highly successful applications of Lipschitz constraints
e.g., spectral normalization or gradient penalty. We further suggest data augmentation to be an
effective alternative which is extremely cheap in practice.
8
Under review as a conference paper at ICLR 2021
References
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. In International Conference on Learning Representations, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Proceedings of the 34th International Conference on Machine Learning, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (gans). In International Conference on Machine Learning, pp. 224-
232, 2017.
Haim Avron and Sivan Toledo. Randomized algorithms for estimating the trace of an implicit sym-
metric positive semi-definite matrix. Journal of the ACM (JACM), 58(2):1-34, 2011.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural
image synthesis. In International Conference on Learning Representations, 2019.
Tatjana Chavdarova, Matteo Pagliardini, Martin Jaggi, and Francois Fleuret. Taming gans with
lookahead. arXiv preprint arXiv:2006.14567, 2020.
Casey Chu, Kentaro Minami, and Kenji Fukumizu. Smoothness and stability in gans. In Interna-
tional Conference on Learning Representations, 2020.
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and
Ian Goodfellow. Many paths to equilibrium: Gans do not need to decrease a divergence at every
step. In International Conference on Learning Representations, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Tianyu Guo, Chang Xu, Boxin Shi, Chao Xu, and Dacheng Tao. Smooth deep image generator
from noises. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp.
3731-3738, 2019.
Tianyu Guo, Chang Xu, Jiajun Huang, Yunhe Wang, Boxin Shi, Chao Xu, and Dacheng Tao. On
positive-unlabeled classification in gan. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pp. 8385-8393, 2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Yongjun Hong, Uiwon Hwang, Jaeyoon Yoo, and Sungroh Yoon. How generative adversarial net-
works and their variants work: An overview. ACM Computing Surveys (CSUR), 52(1):1-43,
2019.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian
smoothing splines. Communications in Statistics - Simulation and Computation, 18(3):1059-
1076, 1989.
Simon Jenni and Paolo Favaro. On stabilizing generative adversarial training with noise. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12145-12153,
2019.
Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo Zhao. On com-
putation and generalization of generative adversarial networks under spectrum control. In Inter-
national Conference on Learning Representations, 2019.
9
Under review as a conference paper at ICLR 2021
Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard
gan. In International Conference on Learning Representations, 2019.
Animesh Karnewar and Oliver Wang. Msg-gan: Multi-scale gradients for generative adversarial
networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 7799-7808, 2020.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.
arXiv preprint arXiv:1705.07215, 2017.
Karol Kurach, Mario LuCiC, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly. A large-scale study
on regularization and normalization in gans. In International Conference on Machine Learning,
pp. 3581-3590, 2019.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and Barnabas Poczos. Mmd gan:
Towards deeper understanding of moment matching network. In Advances in Neural Information
Processing Systems, pp. 2203-2213, 2017.
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans
created equal? a large-scale study. In Advances in Neural Information Processing Systems, pp.
700-709, 2018.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In Proceedings of the IEEE International Confer-
ence on Computer Vision, pp. 2794-2802, 2017.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. On
the effectiveness of least squares generative adversarial networks. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 41(12):2947-2960, 2019.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
Neural Information Processing Systems, pp. 1825-1835, 2017.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International Conference on Machine Learning, pp. 3481-3490, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In
Advances in Neural Information Processing Systems, pp. 5585-5595, 2017.
Weili Nie and Ankit Patel. Towards a better understanding and regularization of gan training dy-
namics. In Conference on Uncertainty in Artificial Intelligence (UAI), 2019.
Guo-Jun Qi. Loss-sensitive generative adversarial networks on lipschitz densities. International
Journal of Computer Vision, 128(5):1118-1140, 2020.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In International Conference on Learning Repre-
sentations, 2016.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In Advances in Neural Information Pro-
cessing Systems, pp. 2018-2028, 2017.
10
Under review as a conference paper at ICLR 2021
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and ro-
bustness of training gans with regularized optimal transport. In Advances in Neural Information
Processing Systems, pp. 7091-7101, 2018.
Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-net based discriminator for generative
adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 8207-8216, 2020.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):60, 2019.
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb.
Learning from simulated and unsupervised images through adversarial training. In Proceedings
of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2107-2116, 2017.
Ugo Tanielian, Thibaut Issenhuth, Elvis Dohmatob, and Jeremie Mary. Learning disconnected man-
ifolds: a no gan’s land. In International Conference on Machine Learning, 2020.
Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving generalization and stability of
generative adversarial networks. In International Conference on Learning Representations, 2019.
Dong Wang, Xiaoqian Qin, Fengyi Song, and Li Cheng. Stabilizing training of generative adversar-
ial nets via langevin stein variational gradient descent. arXiv preprint arXiv:2004.10495, 2020.
Bingzhe Wu, Shiwan Zhao, Chaochao Chen, Haoyang Xu, Li Wang, Xiaolu Zhang, Guangyu Sun,
and Jun Zhou. Generalization in generative adversarial networks: A novel perspective from pri-
vacy protection. In Advances in Neural Information Processing Systems, pp. 307-317, 2019.
Huan Xu and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
Huan Xu, Constantine Caramanis, and Shie Mannor. Robust regression and lasso. IEEE Transac-
tions on Information Theory, 56(7):3561-3574, 2010.
Kun Xu, Chongxuan Li, Huanshu Wei, Jun Zhu, and Bo Zhang. Understanding and stabilizing gans’
training dynamics with control theory. In Proceedings of the 37th International Conference on
Machine Learning, 2020.
Yasin Yazici, ChUan-Sheng Foo, Stefan Winkler, Kim-HUi Yap, Georgios Piliouras, and Vijay Chan-
drasekhar. The unusual effectiveness of averaging in gan training. In International Conference
on Learning Representations, 2019.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and AUgUstUs Odena. Self-attention generative
adversarial networks. In International Conference on Machine Learning, pp. 7354-7363, 2019.
Han Zhang, Zizhao Zhang, AUgUstUs Odena, and Honglak Lee. Consistency regUlarization for
generative adversarial networks. In International Conference on Learning Representations, 2020.
PengchUan Zhang, Qiang LiU, Dengyong ZhoU, Tao XU, and Xiaodong He. On the discrimination-
generalization tradeoff in gans. In International Conference on Learning Representations, 2018.
JUnbo Zhao, Michael MathieU, and Yann LeCUn. Energy-based generative adversarial networks. In
International Conference on Learning Representations, 2017.
ShengyU Zhao, Zhijian LiU, Ji Lin, JUn-Yan ZhU, and Song Han. Differentiable aUgmentation for
data-efficient gan training. arXiv preprint arXiv:2006.10738, 2020a.
Zhengli Zhao, Sameer Singh, Honglak Lee, Zizhao Zhang, AUgUstUs Odena, and Han Zhang. Im-
proved consistency regUlarization for gans. arXiv preprint arXiv:2002.04724, 2020b.
11
Under review as a conference paper at ICLR 2021
Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, and Han Zhang. Image augmentations for
gan training. arXiv preprint arXiv:2006.02595, 2020c.
Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu,
and Zhihua Zhang. Lipschitz generative adversarial nets. In International Conference on Machine
Learning,pp. 7584-7593, 2019.
12
Under review as a conference paper at ICLR 2021
A Local linearity
Consider a function f : Rn → R which is differentiable everywhere in its domain. f is also called
locally linear everywhere. Let = σu, where u follows a distribution with mean 0 and covariance
matrix I, σ ≥ 0, Jx(f) be the Jacobian of f w.r.t the input of f. Considering f(x + ) = f(x + σu)
as a function of σ, Taylor’s theorem allows us to write f(x + σu) = f(x) + σJxT (f)u + o(σ).
Therefore,
E[f(x + )] = E[f(x) + σJxT (f)u + o(σ)]	(9)
= f(x) + o(σ) + σEu [JxT (f)u]	(10)
= f(x) + o(σ),	(11)
where we have used Eu [JxT (f)u] = 0 due to Eu [u] = 0 and the independence of the elements of
u. As σ → 0, we have E[f(x + )] → f (x). In other words, when σ is sufficiently small, one can
well approximate f(x) = E[f(x + )] at any x.
B	JACOB IAN CONSTRAINT ON G AND D
Lemma 6 Let d(p, q) = Ex〜pd[(p(x) — q(x))2] be the squared distance between twofunCtionsp(x)
and q(x), Jx(q) be the Jacobian ofq w.r.t x. Assuming pd and pg are differentiable everywhere, then:
+ Ee[d(pd,Pg+e)] = d(Pd,Pg + o(σ))+ σ2Ex〜PdEu [uτJx(Pg)JT(Pg)u],
+ Ee[d(Pd+e,Pg)] = d(Pd + o(σ),Pg)+ σ2Ex〜PdEu [uτJx(Pd)JT(Pd)UI,
where o(σ) denotes a function satisfying lim o(σ)∕σ = 0.
σ→0
Proof: Due to the everywhere differentiability of Pg, we have Pg+e — Pg = σJxT(Pg)u + o(σ) and
Ee[Pg+e — Pg] = o(σ) as analyzed in Appendix A. Next, we consider
Ee	[d(Pd, Pg+e)]	=	EeEx〜pd [(Pd — Pg+e) ] = EeEx〜pd [(Pd — Pg + Pg — Pg+e) ]
=	EeEx〜pd [(Pd — Pg ) ] + 2EeEx〜pd [(Pd — Pg ) (Pg — Pg+e )] + EeEx〜Pd [(Pg	— Pg+e ) ]
=	Ex ~Pd [(Pd — Pg )2] + 2Ex~pd Ee [(Pd — Pg )(Pg — Pg+e)] + EeEx ~pd [(Pg —	Pg+e)2]
=	Ex〜pd [(Pd — Pg ) ] — 2o(6Ex〜pd [(Pd — Pg )] + Ex〜Pd Ee [(Pg — Pg+e) ]
=Ex〜pd [(Pd — Pg — o(σ))2] — o(σ2) + Ex〜PdEe [(Pg — Pg+e)2]	(12)
=d(Pd,Pg + o(σ)) — o(σ2) + Ex 〜PdEe 仇—Pg+e)2]∙	(13)
Note that
Ee[(Pg — Pg+e)2] = Ee σJxT(Pg)u + o(σ)2	(14)
= Ee σ2uTJx(Pg)JxT(Pg)u + 2σo(σ)JxT (Pg)u + o(σ2)	(15)
= σ2Eu uTJx(Pg)JxT(Pg)u + o(σ2).	(16)
Replacing (16) to equation (13) will lead to the first statement. The second statement can be shown
by using the same argument, completing the proof.
C	Experimental setups
The architectures of G and D are specified in Figure 2, which follow http://github.com/
eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.
py.
We use MNIST dataset which has 60000 images for training and 5000 images for testing. During
the testing phase, 5000 new noises are sampled randomly at every epoch or for computing some
metrics. Before fetching into D, both real and fake images are converted to tensor size (1, 28, 28),
rescaled to (0, 1) and normalized with mean = 0.5 and std = 0.5. The noise input of G has 100
dimensions and is sampled from either uniform or normal distribution. We use Adam optimizer with
β1 = 0.5, β2 = 0.999, lr = 0.0002, batchsize = 64 and ePochs = 200.
13
Under review as a conference paper at ICLR 2021
Generator Network
n=128	n=256
n=512	n=1024	n=784
φs°N ~nd-
,leeu-l
,leeu-l
2E.IONqoaω
,leeu-l
2E.IONqoaω
,leeu-l
2E.IONqoaω
,leeu-l
ɪUel
Discriminator Network
Fake
Image
po5s
-eeun
nVHAXe2
-eeun
nVHAXe2
-eeun
usse




Figure 2: The architectures of G and D with the negative slope of LeakyRuLU is 0.2
D	CHANGING Bz
In this experiment, the input noise of G is sampled from uniform distribution U [-s, s] for s ∈
{0.01, 1, 100}. Spectral normalization (Miyato et al., 2018) is used for D. Other setups are the
same as Appendix C. Figure 3 shows the results along the training progress.
O- l	l	l l l
O	50	IOO	150	200
Epoch
(ξ9)qi
Ω-π-
o- l	l	l l l
O	50	100	150	200
Epoch
O 50 IOO 150	200	O 50 IOO 150	200
Epoch	Epoch
---- Bz = 0.01	... Bz=I --------- Bz = IOO
Figure 3: Some behaviors of GAN when changing Bz, the lower the better. LVg was estimated
by max ∣∣ d∂Vg ∣∣f from 5000 testing noises, while the other quantities were averaged from from 5000
testing noises. We can see that Bz = 0.01 provides the worst results, while the results for Bz = 1 are
slightly worse than those for Bz = 100. Those results are consistent with our theoretical analysis.
E CHANGING σ
In this experiment, the input noise of G is sampled from N (0, I). For the input of D, both real and
fake images are augmented randomly 15 times, then the augmented images and the original images
are fetched into D. We only use image translation for augmentation. The shifts in horizontal and
vertical axis are sampled from discrete uniform distribution within interval [-s, s], where s = 2
corresponds to σ = √2 and S = 8 corresponds to σ = √24. Spectral normalization (Miyato et al.,
2018) is used for D. Other setups are the same as Appendix C. Figure 4 shows the results along the
training progress.
14
Under review as a conference paper at ICLR 2021
Figure 4: Some behaviors of GAN with different σ for augmentation. It can be seen from the first two
subfigures that the higher σ provides smaller Jacobian norms. This is consistent with our theoretical
analysis. However, the last three subfigures suggest that there is no clear difference between the two
settings of σ.
15