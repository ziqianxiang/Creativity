Under review as a conference paper at ICLR 2021
Breaking the Expressive B ottlenecks
of Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to mea-
sure the expressiveness of graph neural networks (GNNs), showing that the neigh-
borhood aggregation GNNs were at most as powerful as 1-WL test in distinguish-
ing graph structures. There were also improvements proposed in analogy to k-WL
test (k > 1). However, the aggregators in these GNNs are far from injective as re-
quired by the WL test, and suffer from weak distinguishing strength, making it be-
come expressive bottlenecks. In this paper, we improve the expressiveness by ex-
ploring powerful aggregators. We reformulate aggregation with the corresponding
aggregation coefficient matrix, and then systematically analyze the requirements
of the aggregation coefficient matrix for building more powerful aggregators and
even injective aggregators. It can also be viewed as the strategy for preserving the
rank of hidden features, and implies that basic aggregators correspond to a special
case of low-rank transformations. We also show the necessity of applying non-
linear units ahead of aggregation, which is different from most aggregation-based
GNNs. Based on our theoretical analysis, we develop two GNN layers, Expand-
ingConv and CombConv. Experimental results show that our models significantly
boost performance, especially for large and densely connected graphs.
1	Introduction
Graphs are ubiquitous in the real world. Social networks, traffic networks, knowledge graphs, and
molecular structures are typical graph-structured data. Graph Neural Networks (GNNs) (Scarselli
et al., 2008; Gori et al., 2005), leveraging the power of neural networks to graph-structured data,
have a rapid development recently (Kipf & Welling, 2016; Hamilton et al., 2017; Bronstein et al.,
2017; Gilmer et al., 2017; Duvenaud et al., 2015).
Expressive power of GNNs measures their abilities to represent different graph structures (Sato,
2020). It decides the performance of GNNs where the awareness of graph structures is required,
especially on large graphs with complex topologies. The neighborhood aggregation scheme (or
message passing) follows the same pattern with weisfiler-lehman (WL) graph isomorphism test
(Weisfeiler & Leman, 1968) to encode graph structures, where node representations are computed
iteratively by aggregating transformed representations of its neighbors with structural information
learned implicitly. Therefore, the WL test is used to measure the expressiveness of GNNs. Unfor-
tunately, general GNNs are at most as powerful as 1-order WL test (Morris et al., 2019; Xu et al.,
2019). There is also work trying to improve the expressiveness that are beyond 1-order WL test
(Maron et al., 2019; Morris et al., 2019; Chen et al., 2019; Li et al., 2020b; Vignac et al., 2020).
However, the weak distinguishing strength of aggregators is the fundamental limitation. The expres-
siveness analysis measured by the WL test assumes that aggregators are injective, which is usually
unreachable. Therefore, this motivates us to investigate the following questions: What are the key
factors to limit the expressiveness of GNN? and how to break these limitations?
Aggregators are permutation invariant functions that operate on sets while preserving permutation
invariance. (Zaheer et al., 2017) first theoretically studied permutation invariant functions and pro-
vided a family of functions to which any permutation invariant function must belong. (Xu et al.,
2019) extended it on multisets but only for countable space. (Corso et al., 2020) further extended it
to uncountable space. (Murphy et al., 2018) and (Murphy et al., 2019) expressed a permutation in-
variant function by approximating an average over permutation-sensitive functions with tractability
1
Under review as a conference paper at ICLR 2021
strategies. (Dehmamy et al., 2019) showed that a single propagation rule applied in general GNNs
is rather restrictive in learning graph moments (Lin & Skiena, 1995). They and (Corso et al., 2020)
improved the distinguishing strength of aggregation by leveraging multiple basic aggregators (SUM,
MEAN, NORMALIZED MEAN, MAX/MIN, and STD). This strategy showed its effectiveness on
tasks taken from classical graph theory.
In contrast to existing studies towards aggregators in GNNs, we provide a new GNN formulation,
where the aggregation is represented as the multiplication of the corresponding hidden feature matrix
of neighbors and the aggregation coefficient matrix. This new formulation enables us to answer the
following questions: (i) when a GNN will lose its expressive power; (ii) How to build aggregators
with higher distinguishing strength, even injective aggregators. Based on our theoretical analysis,
we propose two GNN layers: ExpandingConv and CombConv, and evaluate them on general graph
classification and graph regression tasks. Our key contributions are summarized as follows:
•	We formalize the distinguishing strength of aggregators as a partial order, and theoretically
show that the choice of aggregators can be bottlenecks of expressiveness. We also pro-
pose to apply nonlinear units ahead of aggregations to break the distinguishing strength
limitations of aggregators as well as to achieve an implicit sampling mechanism.
•	We reformulate the neighborhood aggregation with the aggregation coefficient matrix and
then provide a theoretical point of view on building powerful aggregators and even injective
aggregators.
•	We propose ExpandingConv and CombConv layers which achieve state-of-the-art perfor-
mance on a variety of graph tasks. We also show that multi-head GAT is one of the Ex-
pandingConv implementations, which brings a theoretical explanation for its effectiveness.
2	Preliminaries
2.1	Notations
For a graph GpV, Eq, we denote the set of edges, nodes and node feature vectors respectively by
EG, VG and XG. N pvq represents the set of neighbors of v including itself, i.e., Npvq “ tu P
VG|pu, vq P EGu Y tvu. We use rns to denote the set t1, 2, ..., nu. tt...uu represents a multi-set, i.e.,
a set with possibly repeating elements. Πn represents the set of all permutations of the integers 1
to n. h∏, where ∏ P ∏∣h∣, is a reordering of the elements of a sequence h according to ∏. Given a
matrix X P Raxb, XT represents the transpose of X, and Vec(X) P Rabx1 represents the column
stack of X .
2.2	Graph Neural Networks
Most GNNs adopt the neighborhood aggregation scheme (Gilmer et al., 2017) to learn the node
representations, which utilizes both node features and graph structures. In the k-th layer, the repre-
sentation of node v
hVkq “ UPdate(h∙Vkτ), Aggregate({{hUkτq∣u P N(v)}})).	(I)
Aggregators in GNNs. An aggregator is a permutation invariant function (Zaheer et al., 2017) with
bounded size inputs. It satisfies: (i) size insensitive: an aggregator can take an arbitrary but finite size
of inputs; (ii) permutation invariant: an aggregator is invariant to the permutation of input. There are
a limited number of basic aggregators such as SUM, MEAN, NORMALIZED MEAN, MAX/MIN,
STD, etc. Most proposed GNNs apply one of these aggregators. Sum-of-power mapping (Zaheer
et al., 2017) and normalized moments (Corso et al., 2020) can also be used as aggregators and they
allow for a variable number of aggregators.
3	Proposed Model
In this section, we first formalize the distinguishing strength of aggregators as a partial order, and
show why basic aggregators used in popular GNNs become bottlenecks of expressiveness. Then, we
analyze the requirements for building powerful aggregators and even injective aggregators. Finally,
we introduce two GNN layers based on our theoretical analysis.
2
Under review as a conference paper at ICLR 2021
3.1	Distinguishing S trength of Aggregators
To ensure generality, our analysis of aggregators is always considered in multisets and uncountable
case, where the inputs are continuous and with possibly repeating elements. We first introduce
distinguishing strength under the concept of partial order (Schmidt, 2011).
Distinguishing strength. The distinguishing strength of aggregator faggr1 is stronger than faggr2,
denoted by faggri > faggr2, if and only if for any two multisets xι and x2 where the number of
elements can be different, faggr2(x1) ‰ faggr2(x2)台 faggri(xι) ‰ faggri(x2). Meanwhile, if
there exist x11 and x12 such that faggr1px11q ‰ faggr1 px12 q but faggr2px11q “ faggr2px12q, faggr1 is
strictly stronger than faggr2, denoted by faggri > faggr2. If faggri > faggr2 > faggri, we say
these two aggregators have the same distinguishing strength, denoted by faggr1 “ faggr2. If there
exist multisets xi and x2 such that faggri(xiq ‰ faggri(x2q, faggr2(xiq “ faggr2(x2q, and there
also exist x1i and x12 such that faggri(x1iq “ faggri(x12q, faggr2 (x1i q ‰ faggr2(x12q, we say faggri
and faggr2 are incomparable.
Distinguishing strength is a partial order, and the set of all aggregators form a poset. In this poset,
the aggregators with the greatest distinguishing strength should be injective. With the definition of
distinguishing strength, we can compare any two aggregators. The distinguishing strength of widely
used aggregators SUM, MEAN, MAX/MIN is incomparable. One can easily give two multisets of
elements that are distinguished by one aggregator but are not distinguished by the others as showed
in (Corso et al., 2020).
Equivariant aggregator. faggr : {{Rd}} → Rd is an equivariant aggregator if and only if faggr({{T ∙
Xi, T ∙ X2,…，T ∙ Xn}}) “ T ∙ faggr({{xι, X2,…，Xn}}) for any T P RmXd and {{x，P Rd∣i P
rnsuu.
Widely used SUM and MEAN are equivariant aggregators but MAX/MIN is not. We denote
faggri b faggr2 a new aggregator by combing faggri and faggr2 with faggri b faggr2 (Xq “
rfaggri(Xq||faggr2(Xqs, where || denotes concatenation.
Lemma 1. (i) For any continuous function g, we have g o f aggr W f aggr, and when g is injective,
faggr = g。faggr; (ii)	faggr 1 b	faggr2	—	faggri and	faggri b	faggr2	—	faggr2∙ If faggri and
faggr2 are incomparable, faggri b faggr2 > faggri and faggri b faggr2 > faggr2 ； (iii) If faggr is
an equivariant aggregator, then faggr(T ∙ xi, T ∙ X2, ∙∙∙ , T ∙ Xn) W faggr(xi, x2, ∙∙∙ , Xn) forany
T P RmXd and {{χi ρ Rd∣i P rnSUU.
We prove Lemma 1 in Appendix B. Lemma 1 indicates that aggregators become bottlenecks of
distinguishing strength. For the equivariant aggregator, any linear transformation before aggregation
and any transformation after aggregation have no contribution to the distinguishing strength. For
SUM and MEAN, we have g(SUM(T ∙ xi, T ∙ X2,…，T ∙ Xn)) W SUM(Xi, X2,…，Xn) and
g(MEAN(T ∙ Xi, T ∙ X2,…,T ∙ Xn)) W MEAN(Xi, X2,…,Xn), where T P Rmxd, and g can
be any continuous function. Based on Lemma 1, we can now compare the distinguishing strength
of aggregations in some popular GNNs. GIN-0 sums all hidden features of neighbors at first, and
then pass them to a 2-layer MLP. Therefore, when considering in a continuous input features space,
the distinguishing strength of GIN-0 is at most as powerful as the SUM aggregator. GCN uses
a NORMALIZED MEAN (denoted by nMEAN) aggregator. Given a node v and its neighbors,
nMEAN(v, ui, ∙∙∙ ,un) “ ?^ ∙ G?‰ + ?⅛ + ∙→ "un)l´i ). nMEAN is also an
equivariant aggregator, and the distinguishing strength of aggregation in GCN is at most as powerful
as nMEAN. GAT corresponds to the weighted SUM aggregation, where the weight coefficients
are the functions of hidden features. This makes the distinguishing strength of GAT and SUM
incomparable. Based on these observations, a potential approach to breaking the distinguishing
strength limitation is to apply a nonlinear processing on inputs before aggregation.
3.2	Building Powerful Aggregators
In this section, we analyze the requirements for building more powerful aggregators and further
injective aggregators. We first introduce a new representation of GNN layers which unifies several
popular GNN layers. Given a node v and its neighbors N (v), our new formulation represents the
3
Under review as a conference paper at ICLR 2021
GNN operation as follows:
’ mv “ flocalpvq
rVtq “ mv∏hVtΓiqT
’% hpvtq “ fNNprvptqq
/* aggregation coefficients generation */
/* neighborhood aggregation */
/* feature/structure extraction */.
(2)
Here, mv P R|N pvq| is the aggregation coefficients vector of node v. Note that mv should
be the mapping of local structures such as node degrees, node or edge features of the k-hop
neighbors assigned on node V to ensure the same encoding of isomorphic graphs. hVtΓ1qT “
(hVtτq, hUjD,…，h；NpvqI)T P RwPvqIXd is the matrix representation of v's neighbors accord-
ing to a permutation ∏. fNN : Rd → Rd1 is a neural network that extracts task-relevant information
from the aggregated representation rvPtq, and is used to update hidden feature hPvtq of node v.
According to Equation 2, the aggregation should be with high distinguishing strength to avoid in-
distinguishability among neighbors. Meanwhile, the extraction should be powerful enough to ef-
ficiently extract task-relevant structural patterns from the aggregated representation of neighbors.
Based on these observations, we reformulate GCN, GIN0 and GAT with their corresponding three-
stage representations as follows:
GCN :
mv “ —--P —--. ， 1-
√N(v)l ' √N(v)l , √N(uι)l
rvtq “ mv∏hvt´I)T
hpvt) “ σpWrvpt) ` bq,
ɪ1	q	$ m — 11XNpv)l
√Wpu∖N(v)|—1)| !	mv “ 1
GIN0 : <	rvt) “ mv∏ hvt´I)T
’% hpvt) “ MLPprvpt)q,
'mvt) “ (att(hvtτ), hvtτ)), att(hvtτ),尾丁)),…，att(hvt´1), hUt∣N%τ))
GAT ： < rvt) “ mv∏hvTI)T
’% hpvt) “ σ(W rvpt) ` b).
Their default formulations are given in Appendix A. In the aggregation step, GCN’s mv is the
mapping of neighbors’ degrees; GIN0’s mv is the mapping of node v ’s degree which is equivalent
to SUM aggregator; GAT’s mv is the mapping of neighbors’ features. All of them are the mappings
of local structures as given in Equation 2.
In this three-stage representation, the aggregation is reformulated as the multiplication of the aggre-
gation coefficients vector and the feature matrix of neighbors. It provides insights on improving the
distinguishing strength of aggregations. First, we show how to characterize the permutation invari-
ance in this formulation. Let M P RsXn denote an aggregation coefficient matrix where S21.
Note that in GCN, GIN and GAT, S is restricted to be 1. h∏ P RnXd is the matrix representation of
n input elements according to π. The aggregation computation in the second step of Equation 2 is
faggrpM, hπ) “ MPπhπ “ Mπ hπ , where Pπ is the permutation matrix according to π. Pπ hπ
ensures the same output for all hπ, π P ΠIhI . Mπ “ MPπ is the reordering of columns of M
according to π. For any π1, π2 P Πn, faggrpM, hπ1) “ faggrpM, hπ2 ), thus permutation invariance
holds. Once M is decided, we obtain a unique aggregator denoted by fM . For any sequence of
input elements h, fM ph) “ faggrpM, hπ), where π P Πn can be any ordering of neighbors. Next,
we analyze the distinguishing strength of fM.
Proposition 1. For any two matrices M P Rsxn and M1 P Rs'" with s, s1 ≤ n, we have (i)
f` M ∖ > fM, where (MI) means stacking these two matrices; (ii) f` M) > fM if and only
M1	M1
if rank((MI )) > rank(M); (iii) Any multiset of size n is distinguishable with fM if and only if
rankpM ) “ n.
We prove Proposition 1 in Appendix C. Proposition 1 shows that the distinguishing strength offM is
decided by the rank of the corresponding M. Yet, the distinguishing strength analysis in Proposition
1 is only suitable for multisets aggregated with shared fM. Next, we extend the analysis for the case
of different aggregators.
Let Res(fM ) denote the set of all outputs of fM . Our proposed three-stage representation also
provides useful insight on the constraints among different aggregators. That is, in order to fully
distinguish different local structures, for any two different fM1 and fM2, Res(fM1 ) X Res(fM2 ) “
H. This is because to fully distinguish different local structures, we should ensure their aggregated
4
Under review as a conference paper at ICLR 2021
representations are different. Since M is restricted to be the mapping of local structures (such as k-
hop neighbors), different M means that the corresponding local structures are different. Therefore,
the aggregation results of different fM must be different. However, it is not satisfied by existing
GNNs, and there are few studies on distinguishing multisets aggregated by different aggregators. In
Proposition 2, we present a detailed analysis of it.
Proposition 2. For any Mi, M2 P Rsxn1 and M[, M2 P Rs1xn2, (i) ResPf m1 J X
M11
ResPf M2 J ɑ Res(fmJ X ResPfM2); (ii) If ResPf Mi J X ResPf M2 J U Res(fmJ X
M21	M11	M21
Res PfM2) ,then rank (' Ml M2)) > rank (' Mi M2));
We prove Proposition 2 in Appendix D. Proposition 2 shows the necessity of preserving the rank of
aggregation coefficient matrix when considering the distinguishing strength among different aggre-
gators. Next, we provide a sufficient condition for building desired multiple injective aggregators
with the outputs having no intersections.
Proposition 3. For any two aggregators fMi and fM2 With Mi P Rs"1 and M2 P Rsxn2, if
rankP Mi M2 ) “ n1 ` n2, then fMi and fM2 are injective andResPfMi) X ResPfM2 ) “ H.
We prove Proposition 3 in Appendix E. Proposition 1, 2 and 3 provide a new perspective for build-
ing powerful aggregators and even injective aggregators. Compared with the distinguishing strength
studies in (Xu et al., 2019) and (Corso et al., 2020), as well as existing strategies for building injec-
tive aggregators, e.g., sum-of-power mapping (Zaheer et al., 2017) and normalized moments (Corso
et al., 2020), we reformulate the aggregation with aggregation coefficients matrix and show the re-
lations of the distinguishing strength of aggregators and the rank of the corresponding aggregation
coefficients matrices. Besides, the aggregation of this method is controlled by aggregation coeffi-
cients which can be learned from graph data to better leverage structural information. In this paper,
to simplify the analysis, we only consider the aggregations within one-hop neighbors. The results
can be easily extended to more sophisticated aggregators with the overall framework unchanged
In the perspective of preserving the rank of hidden features among neighbors, r “ Mh indicates
that rank(r) ≤ min(rank(M), rank(h)). To preserve the rank of hidden features in aggregations
such that rank(r) “ rank(h), We need rank(M)》rank(h). This builds a connection between
improving the distinguishing strength of aggregators and preserving the rank of hidden features
among neighbors, both of which have the requirements on the rank of M . General aggregators such
as ones in GCN, GIN-0 and GAT have rank(M ) “ 1. Thus, rank(r) is always fixed to 1 no matter
what the rank of the input features is. Correspondingly, they have a weak distinguishing strength.
Equation 2 splits the aggregation and feature/structure extraction into two independent steps, which
helps to figure out that the expressive power loss happens in the aggregation step, and then the model
extracts feature/structure information on the distorted encodings of neighbors. From Equation 2, the
aggregation can be considered as a representation regularization step, which unifies different mul-
tisets of neighbors into the same representation style while holding permutation invariance. Then,
the model can extract structural information on this regulated data with a shared trainable matrix
as the third step in Equation 2. Based on this observation, we propose two novel GNN layers:
ExpandingConv and CombConv.
3.3	ExpandingConv
In this section, we first present ExpandingConv framework. Then we provide one of its implementa-
tions and analyze how ExpandingConv achieves more powerful aggregations. The ExpandingConv
framework is
muv “ flocal (u, v)|uPN pvq
hv htq “ faggr({{vec(mU2hUtτqτ)|u P N(v)}}),
where mUtq P Rs *1 with S > 1 and flocal(u, V) is the mapping of local structures between nodes U
and v. The implementation of flocal (u, v) is very flexible with the only restriction of ensuring the
same encoding of isomorphic graphs. VeCpmuqhUt—1qT) P Rsdx1 is the expanded representation
of hidden features hptT) P Rd*1. Then a GNN layer faggr learns structural information on this
5
Under review as a conference paper at ICLR 2021
expanded representations. We introduce an implelentation as follows:
# mUtvq = Tanh(W[h1)|欣1)] ' b)
t hv “ “ XuPN(Vq MLP(VecpmUVhU	q qq.
(3)
In Equation 3, we implement flocalpu, vq as the function of hidden features of nodes u and v. There
can be other implementations, and We leave them for future work. W P Rsχ2d and b P Rs *1
are trainable matrices. (Luan et al., 2019) empirically showed that different nonlinear activatoin
functions have different contributions in preserving the rank of matrices. We use the recommended
Tanh as the activation function in the computation of mputvq to better preserve the rank of aggregation
coefficient matrices. MLP denotes a 2-layer perceptron.
Next, we represent Equation 3 with the corresponding three-stage representation as given in Section
3.2 to obtain its aggregation coefficient matrix and analyze its distinguishing strength. To simplify
this process, we only consider 1-layer MLP with W1 P Rdxsd and b P Rdx1.
hVtq = 2 ReLU(W 1vec(mUtVhUtτqτ) + b1)
uPN vq
QuPN(V) ReLU(Wr1ι,qvec(mUtVhUjqτ) + b[is)
EuPNPv) ReLU(Wr12,qvec(mutVhujqT) + b[2s)
.
.
.
∖∑ueNPv) ReLU(Wr1d,qvec(mutvhujqτ) + b[dS)?
∕∑upNιPv)(WRsVec(mutVhutτqτ) + b[ι])'
EupN2pv)(Wr'2,：svec(mutVhutT)T) + b[2s)
.
.
.
∖∑upNdPv)(Wr'd,qvec(mutvhut ",) + b[rd]),
vec(∑upN1pv) mutVhutτqτ),
vec(∑upN2 pv) mutVhutτqτ)
Wrd,：s 八 vec(∑upNdpv) mutvhutτqτ)/
(Nι(ν)l∙ brιs)
∣M(V)∣∙ *2]
.
.
.
gv* bj
∖
where Ni(V)	ɑ
mension.	MVpitq
ʌ vvec(Mvtq∏ι hvtr∏1qT)'
HveC(Mvtn2 hv2^∏2qT)
』+
Wrd,:s	vec(Mvdπd hvd πd )
(Nι(ν)∣∙ b{ιs∖
IN2(v)∣∙圻2]
.
.
.
kNd(v)∣∙ bM
(4)
N(v)|i P rds are sampled subsets of neighbors in each di-
“ (mutqv, mutqv,…，mutNipvq∣v) P RsXINi(Vq| and hVt~1q “
(hUj1q, hutΓ1q, ∙∙∙ , hU∣Nlqvq∣) P RdXINi(V)| are aggregation coefficients matrix and hidden fea-
ture matrix corresponding to the subset of neighbors Ni (v) J N(v) according to ∏i. We de-
note [hvt´1)IIhVtTiqS “(hvtτ∣∣hutΓiq, hVtTqllhutT1),…，hvt´1)MutNIqvq∣) P R2dx|Ni(V)1, then
MVt) “ Tanh(WrhVtτ1q∣∣hViτ1q] + b) P RsXINi(v)|. According to Equation 4, we finally obtain
the three-stage representation equivalent to Equation 3.
Mvt) “ Tanh(W[hvt´1) ∣∣hviτq] + b)
rviq = Mvtni hvi´I)T,	i Prds
hV) “ diag(Wr'ι,q, WrI,：1,…，Wrd,q)(vec(rv?), vec(rv2)),…，vec(rvti)))T
+(∣Nι(ν)∣∙ bn，IN2(v)k br2],…，Nd(v)∣∙ bd])T
(5)
According to the computation of rViq, rank(rVi)) ≤ min(s, d, ∣Ni(v)∣). By configuring a larger s,
we have rank(rViq) > 1 with a high probability, which is different from general GNNs with a rank
of 1. As analyzed in Section 3.2, this achieves more powerful aggregators as well as preserves the
rank of hidden features among neighbors. The obtained rViq after aggregation is the unified repre-
sentations of neighbors. We then use the trainable matrix W1 P RsdXd to extracts feature/structure
information. Unlike the aggregation step, the dimensions reduction here (from sd to d) would not
cause information loss. This can be explained by the fact that only task-relevant structural informa-
tion needs to be preserved and passed to the next layer, and it can be embedded in lower dimensions.
+
6
Under review as a conference paper at ICLR 2021
Comparisons with multi-head GAT.
Proposition 4. Multi-head GAT is an implementation of ExpandingConv as follows:
$ αvu “ Softmax(LeakyReLU([diag(aIT, α2τ,…，akt)
<	|| diag (a 11τ, a 12τ ,…，a1KT )][WhVj) || Whuj) S))
(6)
hvt) “ σ "W EjPNv
vec(avuhUtT)T)),
where W “ ||K“i Wk P RkdXd is the concatenation of the trainable matrix in all K heads.
We prove Proposition 4 in Appendix F. Although multi-head GAT is based on attention mecha-
nism, ExpandingConv provides a new perspective to explain its effectiveness. Applying multi-head
attention mechanism helps to preserve the rank of hidden features as well as achieve more power-
ful aggregators. However, the usage of LeakyReLU may be harmful to preserving the rank of the
aggregation coefficient matrix (Luan et al., 2019).
GAT as well as most other GNNs (such as GCN, GIN, etc) follows the same pattern that applies
nonlinear units after aggregation. According to the analysis in Section 3.1, Equation 3 applies MLP
on vec(muVhUt-I)T) before SUM to break the distinguishing strength limitation of SUM. It also
produces other interesting results. By reformulating Equation 3 with its three-stage representation
as Equation 5, each dimension of hidden features aggregates on a subset of neighbors independently,
which corresponds to a kind of dimension-wise neighbor sampling mechanism. We call the modifi-
cation of applying ReLU ahead of SUM aggregator as Re-SUM mechanism. (Mishra et al., 2020)
and (Rong et al., 2019) studied dropedge and node masking mechanism on node-level predictions,
both of which can be considered as neighbor sampling strategies that have shown their effectiveness
in improving the generalization ability of aggregation-based GNNs and are also used as unbiased
data augmentation technique for training. Compared with dropedge and node masking, Re-SUM
realizes a dimension-wise neighbor sampling, and it does not need to manually set the sampling ra-
tio since this mechanism takes effects implicitly. Re-SUM shows that the neural network itself can
perform sampling by properly combining nonlinear units and aggregators, without explicitly modi-
fying the network architecture. Our experimental results verified the effectiveness of the Re-SUM
on a variety of graph tasks.
3.4 CombConv
The CombConv framework is
muv “ flocal (u, v)|uPN pv)
[hVt)“ faggr({{vec(mutq d hutT))Iu P N(v)UU),
where muV P Rd*1 and d denotes element-wise product. An implementation of CombConv is
given as follows:
# mutV “ Tanh(W[hVtT) ∣∣hutT)S ' b)
hV “ EuPN pV) MLP(muV d hu	),
where W P Rdx2d and b P Rdx1. Similar to ExpandingConv, CombConv also applies Re-SUM
aggregation. The difference is that each dimension of hidden features is aggregated with an indepen-
dent weighted aggregator. ExpandingConv with s “ 1 corresponds to a special case of CombConv
where all dimensions share the same aggregator. Therefore, the distinguishing strength of Comb-
Conv is stronger than ExpandingConv with s “ 1. Meanwhile, CombConv does not expand the
hidden features of nodes in aggregation. Hence, it requires fewer parameters.
4	Experiments
In this section, we evaluate ExpandingConv and CombConv on graph-level prediction tasks on
OGB (Weihua Hu, 2020), TU (Kersting et al., 2016; Yanardag & Vishwanathan, 2015) and QM9
(Ramakrishnan et al., 2014; Wu et al., 2018; Ruddigkeit et al., 2012). The code is available at
https://github.com/qslim/epcb-gnns.
7
Under review as a conference paper at ICLR 2021
Configurations. We use the default dataset splits for OGB. The QM9 dataset is randomly split into
80% train, 10% validation and 10% test as given in (Morris et al., 2019; Maron et al., 2019). For TU
dataset, we follow the standard 10-fold cross validation protocol and splits from (Zhang et al., 2018)
and report our results following the protocol described in (Xu et al., 2019; Ying et al., 2018). We
use the concatenation of hidden features from all layers to compute the entire graph representations
(Xu et al., 2018). In our tests, all models are equipped with batch normalization (Ioffe & Szegedy,
2015) on each hidden layer when evaluating on OGB and TU, and are not when evaluating on QM9.
All datasets’ descriptions and detailed hyperparameter settings are given in Appendix H.
We first conduct comprehensive ablation studies to evaluate the effectiveness of powerful aggrega-
tors and Re-SUM mechanism on OGB and QM9 as given in Table 1 and Table 2. Then, we compare
the performance of ExpandingConv and CombConv with competitive baselines on all three datasets
as given in Table 3 and Table 4 to show their improvements. ExpC-s denotes ExpandingConv with
W P Rsx*. We use ExpC* and CombC* to denote the ExPandingConv and CombConv without
Re-SUM.
Table 1: Ablation studies on OGB and QM9. Higher is better.
	ogbg-ppa	ogbg-molhiv	ogbg-molpcba	ogbg-code
ExpC*-1	70.65	77.63	22.65	32.2
ExpC-1	77.50	76.79	23.39	32.6
ExpC-3,4,5	80.11	77.89	23.44	33.2
CombC*	73.61	76.47	23.45	32.29
CombC	77.64	76.63	23.73	32.72
Table 2: Ablation studies on QM9. Lower is better.
	μ	α	homo	lumo	∆	χR2y	ZPVE	U0	U	H	G	Cv
ExpC*-1	0.467	0.283	0.00337	0.00340	0.00467	22.9	0.000205	0.0255	0.0263	0.0242	0.0261	0.1189
ExpC-1	0.469	0.268	0.00326	0.00329	0.00466	20.8	0.000186	0.0202	0.0199	0.0202	0.0201	0.1039
ExpC-4	0.413	0.255	0.00273	0.00300	0.00420	19.4	0.000168	0.0184	0.0183	0.0178	0.0182	0.1115
ExpC-8	0.400	0.257	0.00259	0.00286	0.00395	18.1	0.000172	0.0158	0.0170	0.0177	0.0184	0.1060
ExpC-16	0.382	0.255	0.00248	0.00268	0.00373	17.2	0.000170	0.0170	0.0174	0.0193	0.0165	0.1043
ExpC-32	0.368	0.244	0.00248	0.00257	0.00364	16.3	0.000174	0.0151	0.0167	0.0165	0.0198	0.0962
CombC*	0.4062	0.248	0.00259	0.00273	0.00387	17.1	0.000170	0.0185	0.0181	0.0164	0.0174	0.1022
CombC	0.399	0.241	0.00261	0.00278	0.00386	15.9	0.000160	0.0144	0.0145	0.0147	0.0140	0.0858
4.1	Ablation Studies
Effect of powerful aggregators. For complex graph structures with dense connections or with
abundant node/edge features, they would benefit from a higher expressive model to maximumly
distinguish different structures and extract relevant structural patterns as the model goes deeper
to leverage large receptive fields. This is validated on both QM9 and OGB. We configure s “
1, 4, 8, 16, 32 of ExpC-s for all 12 targets of QM9. As we apply a larger s, the model continuously
achieves better performance on most targets. We randomly select s “ 1, 4 for ogbg-ppa, ogbg-
molhiv and s “ 1, 5 for ogbg-code. The results show that applying larger s gains performance
improvements, especially on ogbg-ppa which involves large graphs with dense connections.
Effect of Re-SUM mechanism. In Table 1 and Table 2, the performance differences between
ExpC*-1 (CombC*) and ExpC-1 (CombC) show the effectiveness of Re-SUM. In our tests, the
Re-SUM can be extremely powerful on graphs with dense connections such as ogbg-ppa, which is
validated on both ExpandingConv (with 6.85% improvements) and CombConv (with 4% improve-
ments). On most targets of QM9, this mechanism also gains improvements. For small graphs with
sparse connections such as ogbg-hiv and ogbg-molpcba, the improvements are not very significant.
4.2	Comparisons with Baselines
Table 3 and Table 4 show the performance comparisons of our models with baselines on QM9, TU
and OGB respectively. All datasets in QM9 and OGB graph-level predictions are used for evalua-
tions. For TU, we use 3 widely used datasets: COLLAB includes graphs with dense connections;
8
Under review as a conference paper at ICLR 2021
Table 3: Comparisons with baselines on OGB and TU. Higher is better.
OGB	TU
	ogbg-ppa	ogbg-molhiv	ogbg-molpcba	ogbg-code	COLLAB	RDT-B	RDT-M12
DGK (Yanardag & Vishwanathan, 2015)	NA	NA	NA	NA	73.09 ± 0.25	78.04 ± 0.39	32.22 ± 0.1
PSCN (Niepert et al., 2016)	NA	NA	NA	NA	73.76 ± 0.50	86.30 ± 1.58	41.32 ± 0.42
AWE (Ivanov & Burnaev, 2018)	NA	NA	NA	NA	73.93 ± 1.94	87.89 ± 2.53	39.20 ± 2.09
GCN (Kipf & Welling, 2016)	68.39 ± 0.84	76.06 ± 0.97	20.20 ± 0.24	31.63 ± 0.18	NA	NA	NA
GIN (Xu et al., 2019)	68.92 ± 1.0	75.58 ± 1.40	22.66 ± 0.28	31.63 ± 0.20	80.2 ± 1.9	92.4 ± 2.5	NA
GraphSAG(Hamilton et al., 2017)	NA	NA	NA	NA	68.25	NA	42.24
DiffPool (Ying et al., 2018)	NA	NA	NA	NA	75.48	NA	47.08
CapsGNN (Xinyi & Chen, 2019)	NA	NA	NA	NA	79.62 ± 0.91	NA	46.62 ± 1.9
PPGN (Maron et al., 2019)	NA	NA	NA	NA	80.16 ± 1.1	NA	NA
DeeperGCN (Li et al., 2020a)	77.12 ± 0.71	78.58 ± 1.17	NA	NA	NA	NA	NA
HIMP (Fey et al., 2020)	NA	78.80 ± 0.82	NA	NA	NA	NA	NA
WEGL (Kolouri et al., 2020)	NA	77.57 ± 1.11	NA	NA	NA	NA	NA
multi-head GAT(VeIiCkoviC et al., 2017)	NA	75.81	20.10	31.10	NA	NA	NA
ExpC-s	79.76 ± 0.72	77.99 ± 0.82	23.42 ± 0.29	33.18 ± 0.17	82.10 ± 1.60	92.2 ± 1.87	49.91 ± 1.75
CombC	77.81 ± 0.76	77.15 ± 1.32	23.63 ± 0.23	32.76 ± 0.15	81.90 ± 1.75	92.5 ± 1.69	49.02 ± 1.21
Table 4: Comparisons with baselines on QM9. Lower is better.
	μ	α	homo	lumo	∆	xR2y	ZPVE	U0	U	H	G	Cv
DTNN (Wu et al., 2018)	0.244	0.95	0.00388	0.00512	0.0112	17	0.00172	2.43	2.43	2.43	2.43	0.27
MPNN (Gilmer et al., 2017)	0.358	0.89	0.00541	0.00623	0.0066	28.5	0.00216	2.05	2	2.02	2.02	0.42
k-GNN (Morris et al., 2019)	0.476	0.27	0.00337	0.00351	0.0048	22.9	0.00019	0.0427	0.111	0.0419	0.0469	0.0944
PPGN (Maron et al., 2019)	0.0934	0.318	0.00174	0.0021	0.0029	3.78	0.000399	0.022	0.0504	0.0294	0.024	0.144
GIN0* (Xu et al., 2019)	0.471	0.281	0.00327	0.00340	0.00473	22.9	0.000202	0.0244	0.0245	0.0233	0.0255	0.1283
GAr-S(VeliCkoviC et al.,2017)	0.452	0.286	0.00322	0.00327	0.00460	22.7	0.000228	0.0212	0.0223	0.0223	0.0219	0.1247
ExpC-s	0.368	0.244	0.00248	0.00257	0.00364	16.3	0.000168	0.0151	0.0167	0.0165	0.0165	0.0962
CombC	0.399	0.241	0.00261	0.00278	0.00386	15.9	0.000160	0.0144	0.0145	0.0147	0.0140	0.0858
REDDIT-BINARY (RDT-B) and REDDIT-MULTI-12K (RDT-M12) include large and sparse graphs
with one center node having dense connections with other nodes. All results of baselines are taken
from the original papers except for the results of GraphSAGE on TU, multi-head GAT on OGB and
GIN0* on QM9 which were not reported by the original papers. We report the results of Graph-
SAGE provided by (Ying et al., 2018) and evaluate multi-head GAT and GIN0* by ourselves. To
ensure a fair comparison, for OGB and TU, we configure the number of heads in multi-head GAT
and s in ExpC-s to be the same which is selected in t3, 4, 5u. For QM9, the number of heads is 8
and s P t4, 8, 16, 32u. GIN0* in QM9 denotes GIN0 without batch normalization.
Compared with baselines, our models achieve the best performance on 7 out of all 12 targets of
QM9, 3 out of all 4 graph-level prediction datasets of OGB and all 3 selected TU datasets. Our
models get 1.9% improvements on COLLAB and 2.83% improvements on REDDIT-MULTI-12K
compared with SOTA baselines. On ogbg-ppa, our models achieve 2.6% higher classification accu-
racies compared with SOTA baselines. On ogbg-code, they achieve 1.5% improvements. Multi-head
GAT can also be considered as an implementation of ExpandingConv. However, its performance on
graph-level predictions is not competitive. According to its three-stage representation, the usage
of LeakyReLU in the aggregation step is harmful to preserving the rank, and the usage of softmax
makes it harder to analyze the rank. In the extraction step, the 1-layer MLP may have a limited
representation power to represent the desired extraction functions.
5	Conclusion
We show how basic aggregators used in general GNNs become expressive bottlenecks. To ad-
dress this limitation, we develop theoretical foundations of building powerful aggregators. We also
propose the Re-SUM mechanism which achieves dimension-wise sampling. To evaluate their ef-
fectiveness, we develop two novel GNN layers, and conduct extensive experiments on public graph
benchmarks. The results are consistent with our analysis, and our proposed models achieve SOTA
performance on a variety of graph-level prediction benchmarks.
References
Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
ric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18-42,
2017.
9
Under review as a conference paper at ICLR 2021
Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. In Advances in Neural Information
Processing Systems, pp.15868-15876, 2019.
Gabriele Corso, LUca Cavalleri, DominiqUe Beaini, Pietro Lio, and Petar VeliCkovic. Principal
neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020.
Nima Dehmamy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the representation power of
graph neUral networks in learning graph topology. In Advances in Neural Information Processing
Systems, pp. 15413-15423, 2019.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.
Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.
M. Fey, J. G. Yuen, and F. Weichert. Hierarchical inter-message passing for learning on molecular
graphs. In ICML Graph Representation Learning and Beyond (GRL+) Workhop, 2020.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. In Jennifer Dy and An-
dreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 2191-2200, Stockholmsmassan,
Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/
v80/ivanov18a.html.
Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Bench-
mark data sets for graph kernels, 2016. http://graphkernels.cs.tu-dortmund.de.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Soheil Kolouri, Navid Naderializadeh, Gustavo K Rohde, and Heiko Hoffmann. Wasserstein em-
bedding for graph learning. arXiv preprint arXiv:2006.09430, 2020.
Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. arXiv preprint arXiv:2006.07739, 2020a.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding-design provably
more powerful gnns for structural representation learning. arXiv preprint arXiv:2009.00142,
2020b.
Yaw-Ling Lin and Steven S Skiena. Algorithms for square roots of graphs. SIAM Journal on
Discrete Mathematics, 8(1):99-118, 1995.
10
Under review as a conference paper at ICLR 2021
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-
scale deep graph convolutional networks. In Advances in neural information processing systems,
pp.10945-10955, 2019.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. arXiv preprint arXiv:1905.11136, 2019.
Pushkar Mishra, Aleksandra Piktus, Gerard Goossen, and Fabrizio Silvestri. Node masking: Making
graph neural networks generalize and scale better. arXiv preprint arXiv:2001.07524, 2020.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4602-4609,
2019.
Ryan L Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Janossy pool-
ing: Learning deep permutation-invariant functions for variable-size inputs. arXiv preprint
arXiv:1811.01900, 2018.
Ryan L Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling
for graph representations. arXiv preprint arXiv:1903.02541, 2019.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In International Conference on Machine Learning, pp. 2014-2023, 2016.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum
chemistry structures and properties of 134 kilo molecules. Scientific data, 1:140022, 2014.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations, 2019.
Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration of 166
billion organic small molecules in the chemical universe database gdb-17. Journal of chemical
information and modeling, 52(11):2864-2875, 2012.
Ryoma Sato. A survey on the expressive power of graph neural networks. arXiv preprint
arXiv:2003.04078, 2020.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Gunther Schmidt. Relational mathematics, volume 132. Cambridge University Press, 2011.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Clement Vignac, Andreas LoUkas, and Pascal Frossard. BUilding powerfUl and eqUivariant graph
neUral networks with message-passing. arXiv preprint arXiv:2006.15107, 2020.
Marinka Zitnik YUxiao Dong HongyU Ren Bowen LiU Michele Catasta JUre Leskovec WeihUa HU,
Matthias Fey. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint
arXiv:2005.00687, 2020.
B. YU. Weisfeiler and A. A. Leman. RedUction of a graph to a canonical form and an algebra arising
dUring this redUction. 1968.
Zhenqin WU, Bharath RamsUndar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S
PappU, Karl Leswing, and Vijay Pande. MolecUlenet: a benchmark for molecUlar machine learn-
ing. Chemical science, 9(2):513-530, 2018.
Zhang Xinyi and LihUi Chen. CapsUle graph neUral network. In International Conference on Learn-
ing Representations, 2019. URL https://openreview.net/forum?id=Byl8BnRcYm.
11
Under review as a conference paper at ICLR 2021
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. arXiv preprint
arXiv:1806.03536, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=ryGs6iA5Km.
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1365-1374.
ACM, 2015.
Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graph representation learning with differentiable pooling. In Advances in Neural Infor-
mation Processing Systems, pp. 4800-4810, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in neural information processing systems, pp. 3391-
3401, 2017.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
12
Under review as a conference paper at ICLR 2021
A GCN, GAT AND GIN
Here, we present implementations of GCN, GAT and GIN for the usage of our analysis.
Graph Convolution Networks (GCN) (Kipf & Welling, 2016).
HPtq “ σpD^´ 1 AD´2 HPtTqW + b)	(8)
Graph Attention Networks (GAT) (VeliCkovic et al., 2017).
hVtq=σ( £ att (hVtτq, hfτq) WhutTq)	(9)
uPN Pvq
Graph Isomorphism Networks (GIN-0) (Xu et al., 2019).
htq= MLPP X huτq).	(10)
uPN Pvq
B Proof of Lemma 1
Proof. (i) For any two multisets x1 and x2, if gpfaggrpx1qq ‰ gpfaggrpx2qq, then faggrpx1q ‰
faggr(x2). Therefore, We have faggr > g o faggr. If g is injective, then faggr(xι) ‰ faggr(x2)0
g(faggr(xl)) ‰ g(faggr(x2q). We have faggr ≥ g 0 faggr 之 faggr, therefore faggr = g 0 faggr.
(ii)	For any two multisets xι and x2, faggri(xι) ‰ faggri(x2)今［faggri(xι)∣∣faggr2(xι)S ‰
rfaggr 1 (x2)|| faggr2 (x2)s. Therefore, faggr 1 b faggr2 之 faggr1∙ If faggr1 and faggr2 are incom-
parable, there exist x3 and x4 such that faggr2(x3q ‰ faggr2 (x4q but faggr1(x3q = faggr1(x4q.
Therefore, there exist x3 and x4 such that rfaggr1(x3q||faggr2(x3qs ‰ rfaggr1 (x4q||faggr2
but faggr1 (x3q = faggrl(x4q. faggr1 b faggr2 > faggr 1.
(x4qs
(iii)	Since faggr is an equivariant aggregator, then faggr(T ∙ xι, T - x2,…,T - Xn) = T ∙
faggr(X1, X2,…,Xn) W faggr(xi, X2,…,Xn).	□
C Proof of Proposition 1
Proof. (i)
f. n ʃ , (x) = ' M ʌ x — ^mπ Xn、= ^ fM (x)、
f' MI)(X) = (MI hXn=Imnx∏) = fM 1 (X)J ,
then for any X1 and X2, we have
fM (X1)‰ fM (x2) = f' M ) (X1)‰ f' M ) (x2),
M1	M1
and therefore we conclude that f` M)> fM.
M1
(ii)	"f' M ) > fM ð rank(' M)) > rank(M)”
M1
We prove the claim by contradiction. Assume that rank( MM1 ) > rank(M ) and f` M ) = fM .
M1
f' M ) = fM means that for any Xi and X2, m∏Xι∏ = m∏X2∏ 0 (M)nXι∏ = (MJnX2∏,
M1
where π is the ordering of input elements. Let s = X1n ´ X2n . For any i P rns, sris = X1n ris ´
X2n ris P R. Then for any s P Rn, mn s = 0 0 MM1 ns = 0. The system of linear equations
mnX = 0 and MM1 nX = 0 share the same solution space. Let RS denote the rank of this solution
space, then rank(mn) ` RS = rank( MM1 n) ` RS = n. Therefore, rank(mn) = rank( MM1 n),
then we have rank(M ) = rank( MM1 ). Since we assumed that rank( MM1 ) > rank(M ), we reach
a contradiction.
“f` M ) > fM = rank(' M,)) > rank(M)”
M1
13
Under review as a conference paper at ICLR 2021
We prove an equivalent proposition “rankp( MI)) ≤ rank(M) 今 f` M)W fM”. Note that
M1
rank(( MJ)》rank(M) and f` M )之 fM as given in Proposition 1(i). We only need to prove
M1
“rank(( MI)) “ rank(M)0 f` M)“ fM”. rank(( MI)) “ rank(M) means that any row in
M1
M1 is linearly dependent to rows in M . Therefore, there exists L P Rs1xs so that (MI) “ (L )M.
For any x1 and x2 with MPπx1π “ MPπx2π, LI MPπx1π “ LI MPπx2π, and therefore
MM1 Pπ x1π “ MM1 Pπx2π , where π is the ordering of input elements. That is, for any x1 and
X2, fM(xι) “ fM(x2)0 f` M ∖(xι) “ f` M ∖(x2), thus f` M ∖ W fM. Finally, We have
M1	M1	M1
f` M ) “ fM.
M1
(iii)	“Any multiset of size n is distinguishable with fM 今 rank(M) “ n”
Since rank(M) ≤ n, we prove an equivalent proposition “rank(M) V n 今 there exists at least two
multisets which are indistinguishable”. Considering the system of linear equations y “ Mx where
x P Rn, if rank(M ) V n, then there exists y1 such that rank(M ) “ rank(M, y1) V n. According
to the ROuche-Capelli theorem, there are infinite solutions Xi such that y1 “ Mx[ “ Mx2 “
∙∙∙ “ Mx8, Each Xi comes from a multiset with a particular order. Next, we need to prove that
all these x1i come from more than one multiset. As a multiset with bounded size n constitutes at
most n! different orders, the infinite number of X1i corresponds to y1 must come from more than one
multisets, making these multisets indistinguishable.
“Any multiset of size n is distinguishable with fM ð rank(M ) “ n”
Since rank(M ) “ n and s “ n, for any X P Rn, y “ MX P Rn is unique. Correspondingly, for
any PπXπ, M(PπXπ) is unique.
□
D Proof of Proposition 2
Proof. (i) According to the proof of Proposition 1(i), f` M ) (X) “ ffM (XX) . For any X1 and
X2 with f M1 (X1) “ f M2 (X2), fM1 (X1) “ fM2 (X2) holds. Meanwhile, f M1 (X1) “
M11	M21	M11
f M2 (X2) P Res(f M1 ) X Res(f M2 ), and fM1 (X1 ) “ fM2 (X2 ) P Res(fM1 ) X Res(fM2 ).
M21	M11	M21
Therefore, for any e P Res(f M1 ) X Res(f M2 ), we have e P Res(fM1 ) X Res(fM2 ). That is
M11	M21
Res(frMi ʌ) C Res(fM2 ʌ) J Res(fM1) C Res(fM2).
M11	M21
(ii)	We prove an equivalent proposition “Res(f M1 ) C Res(f M2 ) “ Res(fM1 ) C Res(fM2 ) ð
M11	M21
rank(` MM11 MM21 )) “ rank(` M1 M2 ))”. rank(` MM11 MM21 )) “ rank(` M1 M2 )) means that any row in
M1 M2	M1 M2
' Ml m2 ) is linearly dependent to ' Mi M2). Therefore, there exists L P Rs1 *s so that' Ml M2) “
` LI )` M1 M2 ). Correspondingly, ` MM11 ) “ ` LI )M1 and ` MM21 ) “ ` LI )M2. For any X1 and X2
with MιP∏Xiπ “ M2P∏X2π, (L )MιP∏Xiπ “(L)M2P∏X2π, and therefore ' Mi )P∏xι∏ “
` MM2l )PπX2π, where π is the ordering of input elements. Thus for any X1 and X2, fMi (X1) “
fM2(x2) P Res(fM1)cRes(fM2)0 f/Mi ∖(xι) “ f/M2ʌ(x2) P Res(f /Mi ∖)cRes(f/M2ʌ).
Mil	M2l	Mil	M2l
Hence, Res(fMi ) C Res(fM2 ) J Res(f Mi ) C Res(f M2 ). According to Proposition 2(i),
Mil	M2l
Res(fM1 ) X Res(fM2 ) “ Res(f M1 ) X Res(f M2 ).
M11	M21
□
14
Under review as a conference paper at ICLR 2021
E Proof of Proposition 3
Proof. Since Mi P Rsxn1, M2 P Rsxn2 andrank(( Mi M2)) “ n'n2,wehaverank(M1) “ n
and rankpM2q “ n2. According to Proposition 1, fM1 and fM2 are injective.
We build the system of	linear equations y “ Ax, where x P Rn1 `n2 ,	and	A	“
' Mi M2)' In1 ´m ) P	Rsχ(ni'n2). Then, rank(A)	“ rank(( Mi M2)' In1	_%	))	“
rankp M1 M2 ) “ n1 `	n2 , which means Ax “ 0 has no non-zero solutions.	Let	x1	“
(x[1], x[2],…,x[nιS) and x2 “ (x[ni ' 1], x[ni ' 2],…,x[ni ' n2S) such that	X “	(χ2	).
For any x ‰ 0,
Ax “ (M1
M2)
M1x1 ´ M2x2 ‰ 0.
Therefore, for any x1 P Rn1, x2 P Rn2 and x1, x2 ‰ 0, Mix1 ‰ M2x2, hence Mι(P∏x∏) ‰
M2(P∏x∏) for any Pnx∏ and Pnx∏. As a result, Res(fMi) X Res(fM2) = 0.	□
F Proof of Proposition 4
Proof. For Multi-head GAT, there are two types of implementations on aggregating each head,
concatenation and average. Here, we only consider the average aggregation implementation.
h(vt) = σ
=σ
=σ
=σ
=σ
(⅛ € Σ ɑVuWkhUtτq)
k“i j PNv
(K ∑ ς Wk(αkuhuT)))
jPNv k“i
佶 ∑ (IIK“1 Wk)(l%αkuhUj)))
K j PNv
∑ Wvec(avuhUtT)T))
("W Σ vec(avuhUtT)T)),
where Wk P RdXd is the trainable matrix for the k-th head, and W = ||K“iWk P RkdXd is the
concatenation of the trainable matrix in all K heads;
a a 1T[WIhvtT)||WIhutT)] ʌ
a2T [W 2hvt-1) ||W 2hut-1) S
αvu =Softmax(LeakyReLU	.	).
.
.
\aKT [W 1hvt-1) ||W K hut´I)Si
15
Under review as a conference paper at ICLR 2021
Let a a “ (a：, a2,…，aK) and a 1< “ (aK+ι, aK# …，a<κ )∙ Then，
a a1T[WIhvtTq||WIhutTqs ʌ
a2T [W 2hvtτq∣∣W 2hutτqs
.
.
.
\aKT [W IhvtTq||W K hutτq”
([a1 ||a 11sτ[WIhvtTq||WIhutTqs ʌ
[a2 ||a l2sτ [W 2hvtτq || W 2hutτqs
.
.
.
∖[a K l∣a IKsT [W IhvtTq||w K hujq”
(a1TwIhvt—1)+ a 11TwIhUT) ʌ
a2T W 2hvt—1q + a 12T W 2hut—1q
∖a KT W K hvt—1q + a IKT W K 册 T)
a a 1TWIhvtTq ∖
a 2T w 2hvtτq
+
.
.
.
∖a KT W K hvtτq
a a 11T w IhutTq ∖
a 12T w 2hutτq
.
.
.
∖α'κτ W K hutτq
(a 1T
2T
( W1 ʌ
W2
WK
hvτq +
(a 11T
∖
a 12T
W1
W2
WK
a
∖
∖
a IKT)
“ diag(a 1T, a2T ,…，aKT qwhvt—1q + diag(a 11T, a 12T ,…，a1KT )Whfτq
“ [diag(a 1T,a2T,…，aKT)∣∣diag(a 11T,a12T,…，a 1KT)s[Whvt—1q||Whut—1qs.
Therefore, multi-head GAT is an implementation of ExpandingConv as follows:
$ αvu = SoftmaX(LeakyReLU([diag(a 1T, a2T,…，aKT)
<	Miagpa 11T, a 12T ,…，a 1KT qs[Whvt—1q || WhutTqs))
、hvtq = σ ´-K W EjPNv νec(avuhutτqT)).
□
G Comparisons with Multi-aggregator Implementations
EXpandingConv can also be considered as a kind of multi-aggregator scheme. In Equation 5, each
row of Mvi can be viewed as a weighted aggregator where the weight coefficients are learned from
data. Proposition 1 shows that to obtain higher distinguishing strength by utilizing more aggrega-
tors, the weight coefficients of newly added aggregators should be linearly independent to all eXisting
aggregators. The distinguishing strength of weighted aggregators is incomparable with basic aggre-
gators. However, since each row of Mvi is equivalent to an independent aggregator, one can simply
modify the implementation of flocal(u, v) to obtain the variant whose distinguishing strength is strict
stronger than basic aggregators as follows:
ExpandingConν∣mυptq=rm^tv | |1s ) SUM,
ExpandingConvI iptq_. ptqιmιι S > SUMbMEAN.
muv =rmuv ||1 || ∣npvq∣S
Compared with lerveraging multiple basic aggregators in (Corso et al., 2020) and (Dehmamy et al.,
2019), lerveraging weighted aggregator allows for variable numbers of aggregators. Meanwhile, the
weighted coefficients are learned from data, which can better capture relevant structural patterns.
16
Under review as a conference paper at ICLR 2021
H Details of Experimental S etup
Datasets. Benchmark datasets for graph kernels provided by TU (Kersting et al., 2016) suffer
from their small scales of data, making them not sufficient to evaluate the performance of mod-
els (Dwivedi et al., 2020). Our evaluations are conducted on graph property predictions datasets
ogbg-ppa, ogbg-code, ogbg-molhiv in OGB (Weihua Hu, 2020) and QM9 (Ramakrishnan et al.,
2014; Wu et al., 2018; Ruddigkeit et al., 2012) which are large-scale graph datasets including graph
classification and graph regression tasks. ogbg-ppa is extracted from the protein-protein association
networks with large and densely connected graphs. ogbg-code is a collection of Abstract Syntax
Trees (ASTs) obtained from Python method definitions with large and sparse graphs. ogbg-molhiv
is molecular property prediction datasets with relative small graphs. QM9 consists 134K small or-
ganic molecules with the task to predict 12 targets for each molecule. All data is obtained from
pytorch-geometric library (Fey & Lenssen, 2019).
Table 5: Hyperparameter settings for OGB.
ogbg-ppa	ogbg-molhiv	ogbg-molpcba	ogbg-code
ExpC*-1, ExpC-s CombC*, CombC	ExpC*-1,ExpC-s CombC*, CombC	ExpC*-1, ExpC-s	CombC*, CombC	ExpC*-1,ExpC-s CombC*, CombC
batch size 32	32	64	64	128	128	64	64
layers	4	4	3	3	5	5	4	4
hidden	256	256	64	64	512	512	512	512
lr	0.0005	0.0002	0.0001	0.0001	0.0001	0.0001	0.0001	0.0001
step size	20	20	5	5	10	10	5	5
lr decay	0.8	0.7	0.7	0.7	0.6	0.6	0.6	0.6
dropout	0.5	0.5	0.5	0.5	0.5	0.5	0.5	0.5
readout	SUM	SUM	MEAN	MEAN	MEAN	MEAN	MEAN	MEAN
Table 6: Hyperparameter settings for TU.
	COLLAB		REDDIT-BINARY		REDDIT-MULTI-12K	
	ExpC-s	CombC	EXPC-S	CombC	ExpC-s	CombC
batch size	32	32	64	64	64	64
layers	3	3	3	3	3	3
hidden	180	180	256	256	256	256
lr	0.001	0.001	0.001	0.001	0.001	0.001
step size	10	10	10	10	10	10
lr decay	0.8	0.8	0.8	0.8	0.8	0.8
dropout	0.5	0.5	0.5	0.5	0.5	0.5
readout	SUM	SUM	SUM	SUM	SUM	SUM
The shared hyperparameter settings of ExpC*-1, ExpC-s, CombC* and CombC on all 12 targets
of QM9: batch sizes = 64; lr = 0.0001; step size = 30; lr decay = 0.85; readout = SUM. hidden =
256 for ExpC*-1 and ExpC-s; hidden = 512 for CombC* and CombC. Table 7 gives the individual
hyperparameter settings of each model on each target, including the number of layers.
Table 7: Number of layers for QM9.
	I μ	α	homo	lumo	∆e	xR2y	ZPV E	U0	U	H	G	Cv
ExpC*-1,ExpC-s	rɪ	4	5	4	4	4	4	5	4	4	4	4
CombC*,CombC	I 5	4	5	4	4	5	4	5	4	4	4	4
I More Experimental Results
We present more results of ablation studies on OGB and QM9, which demonstrate the effectiveness
of ExpandingConv, CombConv and Re-SUM.
17
Under review as a conference paper at ICLR 2021
Training Performance	Training Performance	Training Performance	Training Performance
Figure 1: Learning curves on ogbg-ppa, ogbg-molhiv, ogbg-molpcba and ogbg-code.
8ueuu⅞Med asəɪ
ogbg-molhiv

——ExpC*-l
——ExpC-I
——ExpC-4
---CombCt
——CombC
50 IOO 150	200	250	300
Epoch
ogbg-molpcba
φυes^to⅛f asəɪ
0	10	20	30	40	50
Epoch
ogbg-code
0.34
18
Under review as a conference paper at ICLR 2021
Mean Absolute Error	Mean Absolute Error	Mean Absolute Error	Mean Absolute Error
QM9-0
QM9-9
1
1
1
100	200	300	400	500	600
Epoch
0	IOT 200	300	400	500
Epoch
,-0uu,βn-osqα Css
QM9-4
,-0uu,*n-osqα C3z
QM9-5
35.0
15.0
0
100	200	300	400	500	600
Epoch
QM9-7
QM 9-10
QM9-11
Figure 2: Effectiveness of Re-SUM on QM9.
——ExpC*-l
——ExpC-I
——CombCt
——CombC
GIN*-0
0	100	200	300	400	500	600
Epoch
19
Under review as a conference paper at ICLR 2021
Mean Absolute Error	Mean Absolute Error	Mean Absolute Error	Mean Absolute Error
QM9-1
—ExpC-32
——ExpC-16
—EXPGS
——ExPG4
——ExpC-I
0	IOT 200	300	400	500
Epoch
,-0uu,*n-osqα C3Z
QM9-7
,-0uu,βn-osqα Css
QM9-5
35.0
15.0 4---------------.--------.-------.-------.-------
0	100	200	300	400	500	600
Epoch
QM 9-8
—ExpC-32
——ExpC-16
—EXPGS
——ExPG4
——ExpC-I
QM 9-10	QM9-11
—ExpC-32
——ExpC-16
—EXPGS
——ExPG4
——ExpC-I
14
1.2 ■
I 1-0
S
f 0-8
3
≡ 0.4
0.2
0.0
0	100	200	300	400	500	600
Epoch
Epoch
——ExpC-32
——ExpC-16
——ExpC-8
——ExpC-4
——ExpC-I
,-0uu,sn-osα C3Z
Figure 3: Effectiveness of powerful aggregators on QM9.
20