Under review as a conference paper at ICLR 2021
Loss Landscape Matters: Training Certifiably
Robust Models with Favorable Loss Land-
SCAPE
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the problem of training certifiably robust models. Certifi-
able training minimizes an upper bound on the worst-case loss over the allowed
perturbation, and thus the tightness of the upper bound is an important factor in
building certifiably robust models. However, many studies have shown that Inter-
val Bound Propagation (IBP) training uses much looser bounds but outperforms
other models that use tighter bounds. We identify another key factor that influ-
ences the performance of certifiable training: smoothness of the loss landscape.
We consider linear relaxation-based methods and find significant differences in
the loss landscape across these methods. Based on this analysis, we propose a
certifiable training method that utilizes a tighter upper bound and has a landscape
with favorable properties. The proposed method achieves performance compara-
ble to state-of-the-art methods under a wide range of perturbations.
1	Introduction
Despite the success of deep learning in many applications, the existence of adversarial example,
an imperceptibly modified input that is designed to fool the neural network (Szegedy et al., 2013;
Biggio et al., 2013), hinders the application of deep learning to safety-critical domains. There has
been increasing interest in building a model that is robust to adversarial attacks (Goodfellow et al.,
2014; PaPemot et al., 2016; Kurakin et al., 2016; Madry et al., 2018; Tramer et al., 2017; Zhang et al.,
2019a; Xie et al., 2019). However, most defense methods evaluate their robustness with adversarial
accuracy against Predefined attacks such as PGD attack (Madry et al., 2018) or C&W attack (Carlini
& Wagner, 2017). Thus, these defenses can be broken by new attacks (Athalye et al., 2018).
To this end, many training methods have been ProPosed to build a certifiably robust model that can be
guaranteed to be robust to adversarial Perturbations (Hein & Andriushchenko, 2017; Raghunathan
et al., 2018b; Wong & Kolter, 2018; Dvijotham et al., 2018; Mirman et al., 2018; Gowal et al., 2018;
Zhang et al., 2019b). They develoP an uPPer bound on the worst-case loss over valid adversarial
Perturbations and minimize it to train a certifiably robust model. These certifiable training methods
can be mainly categorized into two tyPes: linear relaxation-based methods and bound ProPagation
methods. Linear relaxation-based methods use relatively tighter bounds, but are slow, hard to scale
to large models, and memory-inefficient (Wong & Kolter, 2018; Wong et al., 2018; Dvijotham et al.,
2018). On the other hand, bound ProPagation methods, rePresented by Interval Bound ProPagation
(IBP), are fast and scalable due to the use of simPle but much looser bounds (Mirman et al., 2018;
Gowal et al., 2018). One would exPect that training with tighter bounds would lead to better Per-
formance, but IBP outPerforms linear relaxation-based methods in many cases, desPite using much
looser bounds.
These observations on the Performance of certifiable training methods raise the following questions:
Why does training with tighter bounds not result in a better performance? What other factors may
influence the performance of certifiable training? How can we improve the performance of
certifiable training methods with tighter bounds?
In this PaPer, we Provide emPirical and theoretical analysis to answer these questions. First, we
demonstrate that IBP (Gowal et al., 2018) has a more favorable loss landscaPe than other linear
1
Under review as a conference paper at ICLR 2021
relaxation-based methods, and thus it often leads to better performance even with much looser
bounds. To account for this difference, we present a unified view of IBP and linear relaxation-based
methods and find that the relaxed gradient approximation (which will be defined in Definition 1)
of each method plays a crucial role in its optimization behavior. Based on the analysis of the loss
landscape and the optimization behavior, we propose a new certifiable training method that has a
favorable landscape with tighter bounds. The performance of the proposed method is comparable to
that of state-of-the-art methods under a wide range of perturbations. We summarize the contributions
of this study as follows:
•	We provide empirical and theoretical analysis of the loss landscape of certifiable training
methods and find that smoothness of the loss landscape is important for building certifiably
robust models.
•	We propose a certifiable training method with tighter bounds and a favorable loss landscape,
obtaining comparable performance with state-of-the-art methods under a wide range of
perturbations.
2	Related Work
Earlier studies on training certifiably robust models were limited to 2-layered networks (Hein &
Andriushchenko, 2017; Raghunathan et al., 2018a). To scale to larger networks, a line of work has
proposed the use of linear relaxation of nonlinear activation to formulate a robust optimization. Then,
a dual problem is considered and a dual feasible solution is used to simplify the computation further.
By doing so, Wong & Kolter (2018) built a method that can scale to a 4-layered network, and later,
Wong et al. (2018) used Cauchy random projections to scale to much larger networks. However, they
are still slow and memory-inefficient. Dvijotham et al. (2018) proposed a method called predictor-
verifier training (PVT), which uses a verifier network to optimize the dual solution. This is similar
to our proposed method but we do not require any additional network. Xiao et al. (2018) proposed
to add regularization technique with adversarial training for inducing ReLU stability, but it is less
effective than other certified defenses. We also encourage our model to avoid unstable ReLUs, but
we train the model with an upper bound of the worst-case loss and investigate ReLU stability from
the loss landscape perspective.
Mirman et al. (2018) proposed the propagation of a geometric bound (called domain) through the
network to yield an outer approximation in logit space. This can be done with an efficient layerwise
computation that exploits interval arithmetic. Over the outer domain, one can compute the worst-
case loss to be minimized during training. Gowal et al. (2018) used a special case of the domain
propagation called Interval Bound Propagation (IBP) using the simplest domain, the interval domain
(or interval bound). In IBP, the authors introduced a different objective function, heuristic scheduling
on the hyperparameters, and elision of the last layer to stabilize the training and to improve the
performance.
Both approaches, linear relaxation-based methods and bound propagation methods, use an upper
bound on the worst-case loss. Bound propagation methods exploit much looser upper bounds, but
they enjoy an unexpected benefit in many cases: better robustness than linear relaxation-based meth-
ods. Balunovic & Vechev (2019) hypothesized that the complexity of the loss computation makes
the optimization more difficult, which could be a reason why IBP outperforms linear relaxation-
based methods. They proposed a new optimization procedure with the existing linear relaxation. In
this paper, we further investigate the causes of the difficulties in the optimization. Recently, Zhang
et al. (2019b) proposed CROWN-IBP which uses linear relaxation in a verification method called
CROWN (Zhang et al., 2018) in conjunction with IBP to train a certifiably robust model.
Although beyond our focus here, there is another line of work on randomized smoothing (Li et al.,
2018; Lecuyer et al., 2019; Cohen et al., 2019; Salman et al., 2019), which can probabilistically
certify the robustness with arbitrarily high probability by using a smoothed classifier. However, it
requires a large number of samples for inference.
There are many other works on certifiable verification (Weng et al., 2018; Singh et al., 2018a; 2019;
2018b; Zhang et al., 2018; Boopathy et al., 2019; Lyu et al., 2020). However, our work focuses on
”certifiable training”.
2
Under review as a conference paper at ICLR 2021
3	Background
First, we provide a brief overview of certifiable training methods. Then, we consider IBP (Gowal
et al., 2018) as a special case of linear relaxation-based methods. This unified view on certifiable
training methods helps us to comprehensively analyze the differences between the two approaches:
bound propagation and linear relaxation. We present the details of the IBP in Appendix B.
3.1	Notations and Certifiable Training
We consider a c-class classification problem with a neural network f (x; θ) with the layerwise oper-
ations z(k) = h(k)(z(k-1)) (k = 1,…，K) and the input z(0) = X in the input space X. The Cor-
responding probability function is denoted by pf = softmax ◦ f : X → [0, 1]c with subscript f. We
denote a subnetwork with k operations as h[k] = h(k) ◦• ∙ •◦ h(1).For a linear operation h(k), We use
W(k) and b(k) to denote the weight and the bias for the layer. We consider the robustness of the clas-
sifier against the norm-bounded perturbation set B(x, ) = {x0 ∈ X : ||x0-x|| ≤ } with the pertur-
bation level U Here, we mainly focus on the '∞-norm bounded set. To compute the margin between
the true class y for the input x and the other classes, we define a c × c matrix C(y) = I - 1e(y)T
with (C(y)z(K))m = ZmK) — ZyK) (m = 0,…，c 一 1). For the last linear layer, the weights W(K)
and the bias b(K) are merged with C(y), that is, W(K) ≡ C(y)W(K) and b(K) ≡ C(y)b(K),
yielding the margin score function s(x, y; θ) = C(y)f (x; θ) = f (x; θ) 一 fy (x; θ)1 satisfying
Ps = Pf. Then we can define the worst-case margin score s*(x, y, e; θ) = maXχθ∈B(χ,e) s(x0, y; θ)
where max is element-wise maximization. With an upper bound s on the worst-case margin score,
S ≥ s*, we can provide an upper bound on the worst-case loss over valid adversarial perturbations
as follows:
L(S(x,y,e; θ),y) ≥ max ∖L(f(x0; θ),y)	(1)
x0∈B(x,)
for cross-entropy loss L (Wong & Kolter, 2018). Therefore, we can formulate certifiable train-
ing as a minimization of the upper bound, min® L(S(x,y, u; θ),y), instead of directly solving
minθ maxx0∈B(x,) L(f (x0; θ), y) which is infeasible. Note that adversarial training (Madry et al.,
2018) uses a strong iterative gradient-based attack (PGD) to provide a lower bound on the worst-case
loss to be minimized, but it cannot provide a certifiably robust model. Whenever possible, we will
simplify the notations by omitting variables such as x, y, U, and θ.
3.2	Linear Relaxation-based Methods
For a subnetwork h[k], given with the pre-activation upper/lower bounds, u and l, for each nonlinear
activation function h in h[k], linear relaxation-based methods (Wong & Kolter (2018); Wong et al.
(2018); Zhang et al. (2019b)) use a relaxation of the activation function by two elementwise linear
function bounds, h and h, that is, h(z) ≤ h(z) ≤ h(z) for l ≤ Z ≤ u. We denote the function
bounds as h(z) = a Θ Z + b and h(z) = a Θ Z + b for some a, b, a, and b, where Θ denotes
the elementwise (Hadamard) product. Using all the function bounds h,s and h,s for the nonlinear
activations in conjunction with the linear operations in h[k], an ith (scalar) activation hik] (∙) ∈ R can
be upper bounded by a linear function gτ ∙ +b over B(x, E) as in Zhang et al. (2018). This can be
equivalently explained with the dual relaxation viewpoint in Wong & Kolter (2018). Further details
are provided in Appendix C. Now we are ready to upper bound the activation h[ik] over B(x, E).
Definition 1 (Linear Relaxation with Relaxed Gradient Approximation). For each neuron activation
h[ik], a linear relaxation method computes an upper approximation of the activation over B(x, E) by
using g ∈ Rd and b ∈ R as follows:
max h[k](x0) ≤ max gτx0 + b = gτX + e∣∣g∣∣* + b.	(2)
x0 ∈B(x,)	x0 ∈B(x,)
We call g the relaxed gradient approximation of h[ik] over B(X, E).
Similarly, we can obtain the corresponding lower bound. Inductively using these upper/lower bounds
on the output of the subnetwork, we can obtain the bounds for the next subnetwork h[k+1] and then
for the whole network s. The final bound S on the whole network S can then be used in the objective
3
Under review as a conference paper at ICLR 2021
(1)	. The tightness of the bounds S and L(s, y) highly depend on how the linear bounds h and h in
each layer are chosen.
Unified view of IBP and linear relaxation-based methods IBP can also be considered as a linear
relaxation-based method using zero-slope (a = a = 0) linear bounds, h(z) = u+ and h(z) = l+,
where v+ = max(v, 0) and v- = min(v, 0). Thus, the bounds of a nonlinear activation depend
only on the pre-activation bounds u and l for the activation layer, substantially reducing the feed-
forward/backpropagation computations. CROWN-IBP (Zhang et al., 2019b) applies different linear
relaxation schemes to the subnetworks and the whole network. It uses the same linear bounds as
IBP for the subnetworks h[k] for k < K except for the network S = h[K] itself, and uses h(z)=
u+u+ι- Θ (Z -1-) and h(z) = 1[u+ +1- > 0] Θ Z for the whole network s. Moreover, CROWN-
IBP uses interpolations between two bounds with the mixing weight β, IBP bound and CROWN-IBP
bound, with the following objective:
L ((1- β)SIBP(x, y, e； θ) + βSCROWN-IBP(x, y, e; θ), y) .	(3)
Convex Adversarial Polytope (CAP) (Wong & Kolter, 2018; Wong et al., 2018) uses the linear
bounds h(z)= 二巴— Θ (z 一 l-) and h(z)= 二巴— Θ Z for all subnetworks h[k] and the entire
network. As CAP utilizes the linear bounds for each neuron, it is slow and memory-inefficient. It can
be easily shown that tighter relaxations on nonlinear activations yield a tighter bound on the worst-
case margin score s*. To specify the linear relaxation variable φ ≡ {a, a, b, b} used in relaxation,
we use the notation S(x, y, e; θ, φ). CROWN-IBP and CAP generally yield a much tighter bound
than IBP. These relaxation schemes are illustrated in Figure 6 in Appendix D.
4	What factors influence the performance of certifiable
training?
One would expect that a tighter upper bound on the worst-case loss in (1) is beneficial in certifiable
training. However, several previous works have shown that this is not the case: IBP performs better
than linear relaxation-based methods in many cases while utilizing a much looser bound. We in-
vestigate the loss landscape and the optimization behavior of IBP and other linear relaxation-based
methods, and find that the non-smoothness of the relaxed gradient approximation of linear relax-
ations negatively affects their performance. Detailed settings of the following analyses are presented
in Appendix A.
4.1	Loss Landscape of Certifiable Training
Figure 1: (Left) The learning curves for the scheduled value of e with the loss variation along the
gradient descent direction (the vertical line indicates when the ramp-up ends), and (Right) the loss
landscapes along the gradient descent direction at each training step in the later phase of ramp-up
period (epoch 50-130). The thin lines and thick lines in the figure on the right show some sample
landscapes at each step and the median values, respectively. Our method shows tight bounds like
CROWN-IBP, while its landscape is as favorable as IBP, achieving the best performance among
these four methods (see Table 1).
4
Under review as a conference paper at ICLR 2021
We empirically show that models that have tighter bounds, CROWN-IBP (Zhang et al., 2019b) and
CAP (Wong & Kolter, 2018), tend to have non-smooth loss landscapes, which hinder optimization
during training. We examine the learning curves of IBP and these linear relaxation-based methods.
For a simple analysis, we avoid considering the mixture of the two logits in (3), and use β = 1
to consider CROWN-IBP logit only. Figure 1 (left) shows the learning curves on CIFAR-10 under
train = 8/255. We use -scheduling with the warm-up (regular training) for the first 10 epochs and
the ramp-up during epochs 10-130 where we linearly increases the perturbation radius from 0 to the
target perturbation train . Thus, the training loss may increase even during learning.
In the early phase of the ramp-up period, in which the models are trained with small , CAP and
CROWN-IBP have lower losses than IBP as expected because they use much tighter relaxation
bounds than IBP. In particular, CAP has much tighter bounds than the others because CAP uses
tighter relaxations for each subnetwork. This is consistent with the known results, that CAP tends to
outperform the others at small perturbations, such as train = 2/255 on CIFAR-10 (see Table 1 for
details). However, at the end of the training, when the perturbation reaches its maximum target value
(train), the opposite result is observed where CAP and CROWN-IBP perform worse than IBP.
To understand this inconsistency, we measure the variation of the loss along the gradient direction
as in Santurkar et al. (2018), which is represented as the shaded region in Figure 1 (left). We find
that linear relaxation-based methods have large variations, while IBP maintains a small variation
throughout the entire training phase. It is known that a smooth loss landscape with a small loss
variation induces stable and fast optimization with well-behaved gradients (Santurkar et al., 2018).
Therefore, even though CAP and CROWN-IBP show robustness in the early phase of training, the
non-smooth loss landscape in the ramp-up period might have hindered the optimization, yielding
less robust models. As will be discussed in the following section, we find that the loss variation is
highly related to the relaxed gradient approximation g used in linear relaxation.
We further explore the loss landscape near the local region of the parameter space at the current
parameter θ(now) toward the next parameter θ(next) along the gradient in Figure 1 (right). We plot
the landscapes for the later phase of the ramp-up period (epochs 50-130) during which large per-
turbations are used. IBP has flatter landscapes compared to the others, whereas CROWN-IBP has
landscapes with large curvature along the gradient, and thus it tends to move towards a sharp local
minimum and it may remain stuck there. Therefore, it may overfit to be robust to small perturbations,
but is not robust to the target perturbation train .
Figure 2: (Top) Cosine similarities between two consecutive loss gradients and (Bottom) the ratio of
the number of unstable ReLUs during the ramp-up period. A large number of unstable ReLUs, high
nonlinearity, leads to an unfavorable landscape that can negatively affect the optimization process.
Next, we establish a relationship between the optimization procedure and linear relaxation. Figure
2 (top) shows the directional deviation between two successive loss gradient steps in terms of co-
sine similarity during training. Simultaneously, Figure 2 (bottom) shows the ratio of the number of
unstable ReLUs for which the pre-activation bounds l and u span zero. We observe that the cosine
similarity value is low when the number of unstable ReLUs is large - for example, in the early stage
of CAP and the middle stage of CROWN-IBP. In particular, in the middle of the ramp-up period,
CROWN-IBP has a large number of unstable ReLUs and exhibits abrupt changes in gradient steps.
It often has deviation angles larger than 90°, leading to parameter updates in the opposite direction
of the previous one, bouncing in the basin of a local minimum. This is consistent with the results
5
Under review as a conference paper at ICLR 2021
shown in Figure 1. Moreover, since the gradient directions are not well-aligned, it may not enjoy
the advantages of momentum-based optimizers and be sensitive to the learning rate. To summarize,
a large number of unstable ReLUs, high nonlinearity, leads to an unfavorable landscape that can
negatively affect the optimization process.
4.2	Smoothness of Relaxed Gradient approximation
In this section, we investigate the loss landscape further from a theoretical perspective to answer
the question: ”What makes some landscapes more favorable than others?” We find that the relaxed
gradient approximation of a linear relaxation affects the smoothness of the landscape. First, we need
some mild smoothness assumptions that are natural when the network parameters θ1 and θ2 are
close to each other, especially they are two consecutive parameters from SGD update.
Assumption 1. Given linear relaxation method, we make the following assumptions on the bias
b(x; θ) in the linear relaxation and the probability function p(x; θ):
(1)	∣∣Vθb(x; θι) - Vθb(x; Θ2)∣∣ ≤ Lθθ∣∣θι - Θ2∣∣ forall θι, θ and x.
(2)	||p(x; θι) — p(x; θι)∣∣ ≤ LP∣∣θι — Θ2∣∣ forall θι, θ2 and x.
With the above assumptions, we can provide an upper bound on the loss gradient difference for
linear relaxation-based methods to measure the non-smoothness of the loss landscape as follows:
Theorem 1. Given input x ∈ X and perturbation radius , let M be maxx0∈B(x,) ||x0||. For a
linear relaxation-based method with the upper bound Sm(x; θ) = maXχo∈B(χ,e) g(m)(x; θ)Tx0 +
b(m)(x; θ), if b(m) satisfies Assumption 1 (1) for each m and ps satisfies Assumption 1 (2), then
∣∣VθL(s(x; Θi))-VθL(s(x; Θ2))∣∣
≤ max(2e||Veg(m)(x; Θ1,2)∣∣ + M∣∣Vθg(m)(x; θι) - Vg(m)(x; θ2川 + L(m)∣∣θι - Θ2∣∣)
(4)
for any θ1, θ2, where L(m) = L(Im + Lps ∣∣Vθs(x; θ1,2)∣∣ and θ1 2 can be any ofθ1 and θ2.
According to Theorem 1, the relaxed gradient approximations g(m) in the linear relaxation play a
major role in shaping the loss landscape. The smoother the relaxed gradient approximations are,
the smoother the loss landscape is. Especially for IBP, using the zero-slope relaxed gradient ap-
proximation g(m) ≡ 0 for all m, the loss difference is upper bounded by only the last term,
maxL(m)∣∣θι - Θ2∣∣, and it is relatively small for a single gradient step. On the other hand, for
other linear relaxation-based methods using non-zero relaxed gradient approximation g (m) 6= 0, the
gradient updates used in the training are more unstable than IBP. It is consistent with the empirical
results shown in Figure 1 that there are significant differences between the loss variations of IBP and
others.
5	Proposed Method
Our analyses so far suggest that tightness of the upper bound on the worst-case loss and smoothness
of the loss landscape are important for building a certifiably robust model. Therefore, we aim to
design a new certifiable training method to improve the aforementioned factors (favorable landscape
and tighter bound).
More favorable landscape via less a = 1 We observe that CROWN-IBP (β = 1) tends to have
more unstable ReLU and less smooth landscape than the others. What, in the objective of CROWN-
IBP, does lead to these results? To answer this question, we investigate variants of CROWN-IBP
with different a settings for unstable ReLUs. For each setting, We sample a ∈ {0,1} with different
(p, q) with P (a = 1 | |l| > |u|) = P and P (a = 1 | |l| ≤ |u|) = q for each neuron with pre-
activation bounds l and u. We use a = 1[u+ + l- > 0] for the other stable ReLUs. For the other
elements of the linear relaxation variable φ = {(a, a, b, b)}, we fix a =二匕- , b = - U+li- , and
b = 0 for each activation node, because they are the optimal choices for tightening the bound (see
Appendix C.2 for details). Figure 3 shows that it tends to have more unstable ReLUs as the number
6
Under review as a conference paper at ICLR 2021
Epoch
Figure 3: The ratio of the number of unstable ReLUs for models with different a settings during the
ramp-up period. Notation p/q denotes the variant with sampling a ∈ {0,1} with P (a = 1 | |l| >
|u|) = P and P (a = 1 | |l| ≤ |u|) = q for unstable ReLUs. As the number of a = 1 increases, it
tends to have more unstable ReLUs, which leads to less smooth loss landscapes.
of a satisfying a = 1 increases. This observation implies that it is required to have smaller portion
of a with a = 1 to have a more favorable landscape.
However, reducing the portion of a with a = 1 is not enough to achieve robustness unless the tight-
ness is guaranteed. Through manually adjusting the a, variants of CROWN-IBP achieve favorable
landscapes, but they show looser upper bounds which lead to a worse performance. Further inves-
tigation of variants of CROWN-IBP is presented in Appendix E. Therefore, it is required to search
for appropriate values of a that can achieve both tightness and favorable landscape.
Tighter bound via optimization Now, we aim to reduce the number of a satisfying a = 1 and to
tighten the upper bound in (1), simultaneously. We can achieve both by minimizing the upper bound
over the linear relaxation variable φ as follows:
L(S(x,y,e; θ),y) ≥ m^L(s(x,y,e; θ, φ),y) ≥ max L(f (x0; θ),y).	(5)
φ	x0 ∈B(x,)
It can be equivalently understood as solving the dual optimization in CAP rather than using a dual
feasible solution. However, solving the dual optimization is computationally prohibited for the linear
relaxation of CAP. To resolve this problem, we use the same linear relaxation as IBP for the subnet-
works of s except for s itself, similar to CROWN-IBP. Further, we efficiently compute a surrogate a
of the minimizer a* = arg min。L(s(x, y, e; θ, φ), y) using the one-step projected gradient update
of the relaxation variable a. Specifically, we have
a = Π[0,i]n (ɑo - ηsign(VaL(s(x, y, e; θ, φ), y)))	(6)
with an initial point a0 〜 U[0,1]n and η ≥ 1, yielding the final objective L(s(x, y, e; θ, φ), y)
ʌ —
where φ = {(a, ^^, b, b)}.
6	Experiments
In this section, we demonstrate the proposed method satisfies two key criteria required for building
certifiably robust models: 1) tightness of the upper bound on the worst-case loss, and 2) smoothness
of the loss landscape. Subsequently, we evaluate the performance of the method by comparing it
with others certifiable training methods. Details on the experimental settings are in Appendix A.
Tightness To validate that the proposed method (OURS) has tighter bounds than other relaxations,
we analyze various linear relaxation methods in Figure 4. We define a tightness measure as a sum
over the worst-case margin for each class m, Pm-=IO sm(x, y, e; θ), obtained from (2). Then, we
evaluate multiple methods on a single fixed model pre-trained with the proposed training method.
The compared methods are, from left to right, OURS, CROWN-IBP (Zhang et al., 2019b), CAP-
IBP, and RANDOM. All methods use the same IBP relaxation for subnetworks, but use different
linear relaxation variables a for the whole network s. CROWN-IBP, CAP-IBP, and RANDOM use
a = 1[u+ + I- > 0], a = u+⅛- and a 〜 U[0,1]n, respectively. We fix the other variables
a, b, and b, as in Section 5. In both figures, our method shows the lowest value on average, which
indicates that a single gradient step in (6) is sufficient to obtain tighter bounds compared to other
relaxation methods. See Appendix P for the equivalent tightness violin plots of other models.
7
Under review as a conference paper at ICLR 2021
3.0
2.5
2.0
OURS CROWN-IBPCAP-IBP RANDOM	OURS CROWN-IBPCAP-IBP RANDOM
Figure 4: Violin plots of the test loss with the corresponding verified error (Left) and of tight-
ness (Right) for various linear relaxations. Lower is better. This shows that the proposed relaxation
method has a tighter bound than the others relaxation methods.
Figure 5: The ratio of number of a used for unstable ReLUs during the ramp-up period. Note that
the blue region denotes the number of a = 0 when |l| ≤ |u|. It indicates that the proposed method
reduces the number of a satisfying a = 1 (PUrple+yellow).
Smoothness Figure 1 shows that the proposed method has small loss variations along the gradi-
ent as with IBP, whereas CROWN-IBP (β = 1) has a wide range of loss values. This is because
CROWN-IBP (β = 1) has more unstable ReLUs than our methods as shown in Figure 2. As men-
tioned above, number of a is closely related to the amount of unstable ReLUs, and Figure 5 shows
that our method has successfully reduced the number of a = 1. Further, we conduct analysis on
smoothness of the loss landscape with the loss gradient change (the left term in (4)) in Appendix H.
Robustness We evaluate the performance of the proposed method and compare it to that of state-
of-the-art certifiable training methods: IBP (Gowal et al., 2018), CROWN-IBP (β = 1) (Zhang
et al., 2019b), and CAP (Wong et al., 2018), as in Section 4.1. On MNIST, we follow Zhang et al.
(2019b) and use train ≥ test; whereas for CAP, we use the same train = test which yields better
results. We used three evaluation metrics: standard (clean) error, 100-step PGD error, and verified
error. For the verified error, we evaluated with the bound S of each method.
Table 1: Test errors (Standard / PGD / Verified error) of IBP, CROWN-IBP (β = 1), CAP, and
OURS on MNIST, CIFAR-10, and SVHN. Bold and underline numbers are the first and second
lowest verified error.
Data	e(Ig)		IBP	CROWN-IBP (β = 1)	CAP	OURS
	e	= 0.1	1.18/2.16/3.52	1.07/1.69/ 2.10	0.80 / 1.73 / 3.19	1.09/1.77/2.36
MNIST	e	=0.2	2.00/3.29/6.31	2.99 / 5.50 / 7.97	3.22 / 6.72 / 11.06	1.70 / 3.44 / 4.34
	e	=0.3	3.50/5.85 /10.45	5.73 / 10.76 / 16.28	19.19 / 35.84 / 47.85	3.49 / 5.59 / 9.79
	e	=0.4	3.50/7.30/17.96	5.73 / 14.63 / 23.80	-	3.49 / 6.77 / 15.42
	e	2/255	37.98/49.40/55.39	32.48/42.77/50.15	28.8 / 38.95 / 48.50	31.49/42.73/49.42
CIFAR 10	e =	4/255	46.42/57.42/62.80	45.56 / 58.24 / 64.47	40.78/52.62/61.88	42.53 / 55.55 / 61.52
	e =	6/255	52.84/63.92/68.79	54.72 / 65.28 / 71.04	49.20 / 60.85 / 69.03	50.19 / 61.88 / 66.90
	e	8 8/255	55.71 /66.79/70.95	61.37 / 70.66 / 75.37	56.77 / 66.78 / 73.02	56.01 / 66.17 / 69.70
	e =	16/255	67.10/75.12/78.26	76.65 / 81.90 / 84.42	75.11 / 80.67 / 82.07	65.93 / 75.39 / 77.87
SVHN	e	= 0.01	19.91 /34.12/43.83	17.25 /30.84/39.88	16.88 / 30.16 / 37.09	16.41 /30.43/39.44
8
Under review as a conference paper at ICLR 2021
Table 1 summarizes the evaluation results under different test for each dataset. In general, when
test is low, methods with tighter linear relaxations show good performance, whereas IBP tends to
perform better as test increases. In short, the state-of-the-art methods perform well for a specific
range of test. For example, IBP show relatively better performance in the case of test = 0.3, 0.4
on MNIST and test = 6/255, 8/255, 16/255 on CIFAR-10. On the other hand, CAP and CROWN-IBP
(β = 1) outperform IBP in the case of test = 0.1 on MNIST, test = 2/255 on CIFAR-10 and
test = 0.001 on SVHN. This result is consistent with the analysis shown in Figure 1 that CAP and
CROWN-IBP (β = 1) have lower loss than IBP at small , but their loss landscape is less smooth
than IBP, leading to worse performance at large . Moreover, CAP cannot be trained on MNIST
when train = 0.4. As the case is also not specified in Wong et al. (2018), it seems that CAP is hard
to be robust to train ≥ 0.4. On the other hand, the proposed method shows consistent performance
in a wide range of test values, achieving the best performance in most cases, since it has tighter
bounds and a favorable landscape, not overfitting to a local minimum during the -scheduling. We
also compared our method with other prior work (Xiao et al., 2018; Mirman et al., 2018; Balunovic
& Vechev, 2019) in Appendix K. We also conduct additional experiments on the hyperparameters
in Appendix L, M, and N.
Unlike standard training, certifiable training requires -scheduling. It is implicitly assumed that a set
of weights that makes the network robust to a small is a good initial point to learn robustness to
a large train . However, linear relaxation-based methods with tighter bounds start with a lower loss
at a small , but with an unfavorable loss landscape, they cannot explore a sufficiently large area
of the parameter space. Hence, they overfit to be robust to a small perturbation, and not generalize
to a large perturbation. CAP and CROWN-IBP (β = 1) are typical examples that demonstrate the
overfitting. This may overegularize the weight norm and decrease the model capacity (Wong et al.,
2018; Zhang et al., 2019b). The tightness of the proposed method improves the performance for a
small , while the smoothness of the proposed method helps the optimization process, which also
leads to better performance for a large . To conclude, the proposed method can achieve a decent
performance under a wide range of perturbations as shown in Table 1.
Table 2: Test errors (Standard / PGD / Verified error) of OURS and CROWN-IBP1→0 on CIFAR-10.
Bold numbers are the lower error.
e(2∞)	e = 2/255	= 4/255	= 6/255	= 8/255	= 16/255
OURS	31.49/42.73/49.42	42.53 / 55.55 / 61.52	50.19/61.88/66.90	56.01 / 66.17 / 69.70	65.93 / 75.39 / 77.87
CROWN-IBP1→0	36.30/47.13/52.70	45.92 / 57.58 / 62.07	53.54/64.14/67.35	55.09 / 66.68 / 69.97	66.62 / 76.13 / 77.88
Understanding β-scheduling For CROWN-IBP, we use two different settings of β in (7),
CROWN-IBP1→1 and CROWN-IBP1→0, where the subscript βstart → βend refers to the linear
scheduling on β from βstart to βend . Zhang et al. (2019b) found that the β-scheduling of CROWN-
IBP1→0 could help to improve the robustness performance. And they argued that this is because
training with a tighter bound of CROWN-IBP at the beginning can provide a good initialization for
later IBP training. On the other hand, we provide another explanation that CROWN-IBP1→0 starts
with a tighter bound (CROWN-IBP only) but not overfits to small perturbation by gradually intro-
ducing the IBP objective which has a smoother landscape. Despite using a single objective without
the mixture parameter β, the proposed method can outperforms CROWN-IBP1→0 on CIFAR-10 as
shown in Table 2.
7	Conclusion
In this work, we have investigated the loss landscape of certifiable training and found that the
smoothness of the loss landscape is an important factor that influences in building certifiably ro-
bust models. To this end, we proposed a method that satisfies the two criteria: tightness of the upper
bound on the worst-case loss and smoothness of the loss landscape. Then, we empirically demon-
strated that the proposed method achieves robustness comparable to state-of-the-art methods under a
wide range of perturbations. We believe that with an improved understanding of the loss landscape,
better certifiably robust models can be built.
9
Under review as a conference paper at ICLR 2021
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of se-
curity: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning,pp. 274-283, 2018.
Mislav Balunovic and Martin Vechev. Adversarial training and provable defenses: Bridging the gap.
In International Conference on Learning Representations, 2019.
Battista Biggio,Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio
Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European
conference on machine learning and knowledge discovery in databases, pp. 387-402. Springer,
2013.
Akhilan Boopathy, Tsui-Wei Weng, Pin-Yu Chen, Sijia Liu, and Luca Daniel. Cnn-cert: An efficient
framework for certifying robustness of convolutional neural networks. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 33, pp. 3240-3247, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320, 2019.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan
O’Donoghue, Jonathan Uesato, and Pushmeet Kohli. Training verified learners with learned ver-
ifiers. arXiv preprint arXiv:1805.10265, 2018.
Rida T Farouki. The bernstein polynomial basis: A centennial retrospective. Computer Aided Geo-
metric Design, 29(6):379-419, 2012.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. In Advances in Neural Information
Processing Systems, pp. 8789-8798, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Ue-
sato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training verifiably robust models. arXiv preprint arXiv:1810.12715, 2018.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems, pp.
2266-2276, 2017.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
and Privacy (SP), pp. 656-672. IEEE, 2019.
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Second-order adversarial attack and
certifiable robustness. arXiv preprint arXiv:1809.03113, 2018.
Zhaoyang Lyu, Ching-Yun Ko, Zhifeng Kong, Ngai Wong, Dahua Lin, and Luca Daniel. Fastened
crown: Tightened neural network robustness certificates. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pp. 5037-5044, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2021
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for prov-
ably robust neural networks. In International Conference on Machine Learning, pp. 3575-3583,
2018.
Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597. IEEE, 2016.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. arXiv preprint arXiv:1801.09344, 2018a.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10877-10887, 2018b.
Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
Advances in Neural Information Processing Systems, pp. 11289-11300, 2019.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In Advances in Neural Information Processing Systems, pp. 2483-
2493, 2018.
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin Vechev. Fast and
effective robustness certification. In Advances in Neural Information Processing Systems, pp.
10802-10813, 2018a.
Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. Boosting robustness certifica-
tion of neural networks. In International Conference on Learning Representations, 2018b.
Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for cer-
tifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1-30,
2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. arXiv preprint arXiv:1711.07356, 2017.
Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick Mc-
Daniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204,
2017.
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S
Dhillon, and Luca Daniel. Towards fast computation of certified robustness for relu networks.
arXiv preprint arXiv:1804.09699, 2018.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Advances in Neural Information Processing Systems, pp. 8400-8409, 2018.
Kai Y Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster
adversarial robustness verification via inducing relu stability. arXiv preprint arXiv:1809.03008,
2018.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 501-509, 2019.
11
Under review as a conference paper at ICLR 2021
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
on Machine Learning,pp. 7472-7482, 2019a.
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural net-
work robustness certification with general activation functions. In Advances in neural information
processing systems, pp. 4939-4948, 2018.
Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In
International Conference on Learning Representations, 2019b.
Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging mode
connectivity in loss landscapes and adversarial robustness. arXiv preprint arXiv:2005.00060,
2020.
12
Under review as a conference paper at ICLR 2021
A	Experimental Settings
Datasets and Architectures In the experiments, we use three datasets: MNIST, CIFAR-10 and
SVHN and model architectures (Small, Medium, and Large) in Gowal et al. (2018) and their vari-
ants (Small* and Large*) as follows:
•	Small: Conv(∙,16,4,2) - Conv(16,32,4,1) - Flatten - FC(∙,100) - FC(100,c)
•	Small*: Conv(∙,16,4,2) - Conv(16,32,4,2) - Flatten - FC(∙,100)- FC(100,c)
•	Medium： Conv(∙,32,3,1) - Conv(32,32,4,2) - Conv(32,64,3,1)- Conv(64,64,4,2) - Flatten -
FC(∙,512) - FC(512,512) - FC(512,c)
•	Large： Conv(∙,64,3,1) - Conv(64,64,3,1) - Conv(64,128,3,2) - Conv(128,128,3,1)-
Conv(128,128,3,1) - Flatten - FC(∙,512) - FC(512,c)
•	Large*:Conv(・,64,3,1)- Conv(64,128,3,2)- Conv(128,128,3,1)- Conv(128,128,3,1)-Flat-
ten-FC(∙,512)-FC(512,c)
where Conv(c1, c2, k, s) is a conv layer with input channel c1, output channel c2, kerner size k, and
stride s, and FC(d1, d2) is a fully-connected layer with input dimension d1 and output dimension d2.
All layers are followed by ReLU activation except for the last layer and the flatten layer (Flatten).
Loss and training schedules For general training schedules, we refer to Appendix C, D of Zhang
et al. (2019b) with a single GPU (Titan Xp). We use the following mixed cross-entropy loss as in
Zhang et al. (2019b):
KL (f (x; θ), y) + (1 - K)L ((1- β)SIBP(x, y, e; θ) + βsMODEL(χ, y, e； θ), y),	⑺
where κ is the mixing weight between the natural loss and the robust loss, and β is the mixing weight
between the two bounds obtained with IBP and given relaxation method (e.g. CROWN-IBP).
A.1 Settings in Section 4.1
Figure 1 We conduct the experiment in Figure 1 on CIFAR-10 dataset with Medium architecture
over all four methods. We train the model with train = 8/255 for 200 epochs using -scheduling
with 10 warm-up epochs and 120 ramp-up epochs. We use Adam optimizer with learning rate 0.001.
We reduce the learning rate by 50% every 10 epochs after -scheduling ends.
To demonstrate the instability of each training, we describe the variation of the loss along the gra-
dient direction as Santurkar et al. (2018). We take steps of different lengths in the direction of the
gradient and measure the loss values obtained at each step. For the sake of consistency, we fix a
Cauchy random matrix when evaluating CAP to obtain deterministic loss landscapes, not introduc-
ing randomness. The loss variation is computed with
L(s(θ(t)))
where L(s(θ)) ≡ L(s(x, y, e; θ), y) and
θ(t) ≡ θο - tηVθL(s(θο)) for t ∈ [0, 5],	(8)
where θ0(= θ(0)) is the current model parameters and η is the learning rate. For the step of length
t, we sample ten points from a range of [0,5] on a log scale. In Figure 1 (right), θ(now) = θ(0) and
θ(next) = θ(1).
Figure 2 (top) In Figure 2, with the same model used in Figure 1, we plot cosine similarity be-
tween two successive loss gradient steps during training as follows:
cοs(VθL(s(θ(0))), VθL(S(θ(1)))),
where cos(v1, v2) is the cosine value of the angle between two vectors v1 and v2 .
13
Under review as a conference paper at ICLR 2021
A.2 Settings in Table 1
For MNIST, we use the same hyper-parameters as in Appendix C of Zhang et al. (2019b). We train
for 200 epochs (10 warm-up epochs and 50 ramp-up epochs) on Large model with batch sizes of
100. we decay the learning rate, 0.0005, by 10% in [130,190] epochs. As mentioned in Zhang et al.
(2019b), we also found the same issue when training with small (see Appendix N for details). To
alleviate the issue, we use train = min(0.4, test + 0.1) for each test as Table 2 of Zhang et al.
(2019b).
For CIFAR-10, we train for 400 epochs (20 warm-up epochs and 240 ramp-up epochs) on Medium
model with batch sizes of 128. We decay the learning rate, 0.003, by 2× every 10 epochs after the
ramp-up period.
For SVHN, we train for 200 epochs (10 warm-up epochs and 120 ramp-up epochs) on Large model
with batch sizes of 128 (OURS with batch sizes of 80 to avoid out of memory). We decay the
learning rate, 0.0003, by 2× every 10 epochs after the ramp-up period. Only for SVHN, we apply
normalization with mean (0.438, 0.444, 0.473) and standard deviation (0.198, 0.201, 0.197) for each
channel.
In Table 1, we use κ-scheduling from 1 to 0. For the corresponding results of κ-scheduling from 0
to 0, we refer the reader to Table 5.
We modify the source code for CAP1 to match our settings. For example, we introduce the warm-
up period and linear -scheduling. We avoid using the reported results in the literature and aim to
make a fair comparison under the same settings with only minor differences - for example, because
CAP does not support the channel-wise normalization, we could not use the input normalization.
Also, due to the memory limit of CAP, we use a smaller batch size of 32 and try other smaller
architectures. We found that it often achieves better results with smaller architectures (similar to the
results in Table 3 of Wong et al. (2018)). Thus, we present the performance with Large*, Medium,
and Small* on MNIST, CIFAR-10, and SVHN, respectively. Throughout the experiments, CAP uses
the fixed κ = 0.
B Interval B ound Propagation (IBP)
IBP (Gowal et al., 2018) starts from the interval bound I(0) ≡ {z : l(0) ≤ z ≤ u(0)} = B(x, ) in
the input space with the upper bound u(0) = x + 1 and the lower bound l(0) = x - 1 where 1 is
a column vector filled with 1. Then we propagate the interval bound I(k-1) ≡ {z : l(k-1) ≤ z ≤
u(k-1) } by using following equations iteratively:
u(k) = h(k)(u(k-1)) and l(k) = h(k) (l(k-1))	(9)
for element-wise monotonic increasing nonlinear activation h(k) with the pre-activation bounds
u(k-1) and l(k-1), and
u(k)	W(k)	u(k-1) + l(k-1)	+ + IW(k)| (U(I) - l(	k-1)	(k)	
		I 2	/	I + W 1l	2	an	3W
l(k)	W(k)	U u(k-1) + I(I八	I-IW (k)I ( u--j(	k-1)	+ b(k)	(11)
for linear function h(k) (k = 1,…,K). Finally, IBP uses the worst-case margin S = U(K) to
formulate the objective in (1) for certifiable training.
1 https://github.com/locuslab/convex_adversarial
14
Under review as a conference paper at ICLR 2021
C Details on Linear Relaxation
C.1 Linear relaxation explained in CROWN (Zhang et al., 2018)
To make the paper self-contained, we provide details of linear relaxation given in the supplemen-
tary material of CROWN (Zhang et al., 2018). We refer readers to the supplementary for more
details. Given a network h[k] , we want to upper bound the activation h[ik] . We have h[ik] (x0) =
Wi(,k: ) h(k-1) (h[k-2] (x0)) + bi(k) = Wi(,k: )h(k-1)(z(k-2)0) + bi(k) where z(k-2)0 = h[k-2] (x0). With
the linear function bounds of h(k 1) and h(k-1) on the activation function h(k-1), We have
h[ik] (x0) = Wi(,k:)h(k-1)(z(k-2)0) + bi(k)
≤ X	Wij)hjk-1)(Z(I)0)+	X Wij)hjk-1)(z(k-2)0)+ bik)
Wi(,kj) <0	Wi(,kj) ≥0
=X	Wij)ajk-1)zjk-2)0 + X	Wij)ajk-1)zjk-2)0
Wk) <0	Wj) ≥0
+ X	Wij)bjk-1)+ X Wij)bjk-1) + b(k)
Wi(,kj) <0	Wi(,kj)≥0
=W(k)z(k-2)0 + b(k)
=W(k)h[k-2](x0) + b(k)
=Wi(k)(W(I)(h[k-3](x0)) + b(I)) +b(k)
=Wi(k-2)h(k-3)(z(k-3)0)+ b(k-2),
where W(k) = W(k)D(k-1) with the diagonal matrix Djk-V) = ajk-1) for j satisfying
Wij) < 0 and Djk-I)= ajk-1) for j satisfying Wij) ≥ 0, Mk)= PW(k)<0 Wij)bjk-1) +
PW(Do Wij)bjk-1) + b(k), W(k-2) = W(,k)W(k-2), and b(k-2) = W(k)b(k-2) + b(k). AP-
i,j
Plying similar method iteratively, we can obtain g and b in (2) for the linear relaxation of h[ik].
C.2 Dual Optimization View
We first modify some notations in the main PaPer and use the notations similar to Wong & Kolter
(2018). We use the following hat notations: Z(k+1) = W(k+1)z(k) + b(k+1) and Zlk) = h(k) (Z(k))
where h(k) is the k-th nonlinear activation function. We can build a Primal Problem with cT = Cm,:
as follows:
max CT Z(K)	(12)
z(K)
such that
x - 1 ≤ Z(0),
Z(0) ≤ x + 1,
Z(k+1) = W(k+1)z(k) + b(k+1) (k = 0,…，K - 1),and
z(k) = h(k)(Z(k)) (k = 1, ∙∙∙ ,K - 1).
15
Under review as a conference paper at ICLR 2021
Note that our c is negation of that of Wong & Kolter (2018). Now we can derive the dual of the
primal (12) as follows:
min sup CTz(K) + ξ-T(X - el - Z(O)) + ξ+T(Z(O)- X - e1)
ξ+,ξ-≥0 z(k),z(k)
νk
K-1	K-1
+ X νT+1 (Z(k+1)- (W(k+1)z(k) + b(k+1))) + X VT (Z(k) - Mk)(Z(k)))
k=O	k=1
K-1
=(C + VK)tZ(K) + (ξ+ - ξ- - W(I)TVI)Tz(0) + X (-W(k+1)TVk+1 + Vk)tz(k)
k=1
K-1
+ X (VT h(k)(Z(k))-vTZ(k))	(13)
k=1
-vTb(1)-ξTX -e∣∣ξ∣∣ι.
It leads to c+νκ = 0, ξ+ -ξ--W(I)T νι = 0, and -W(k+1)T νk+ι+^Vk = 0 (k = 1,…，K-1).
Alternatively, they are represented as follows:
VK = -c,
Vk = W(k+I)TVk+1 (k = K - 1,…，1), and
ξ = Vι.
Now We need relationship between Vk and Vk, i.e., Vk = g(ν^k). With the further relaxation Vk =
αk Θ Vk, we have a relaxed problem as follows:
K-1
min sup X (VT h(k)(Z(k)) - νT Z(k)) - νT b ⑴-EtX - e∣∣ξ∣∣ι	(14)
αk z(k),z(k) k=1
such that
VK = -c,
Vk = W(k+I)TVk+ι (k = K - 1,…，1),
Vk = αk Θ Vk (k = K - 1,…，1), and
ξ = Vι.
We decompose the first term in (14), and ignore the subscript k as follows VTh(Z) - (α Θ V)tZ.
Further, we decompose this for each element, Vh(Z) - αVZ = ^(h(z) - ɑz). If the pre-activation
bounds for h are both positive (active ReLU), then α should be 1 not to make the inner supremum
∞. Similarly, if the pre-activation bounds for h are both negative (dead ReLU), then α should be 0.
In the case of unstable ReLU (l ≤ 0 ≤ u), if V < 0, then we need to solve maxa infZ h(Z) - αz. The
inner infimum is 0 for 0 ≤ α ≤ 1, and is -∞ otherwise. On the other hand, if V ≥ 0, then we need
to solve mina SuPz h(Z) — ɑz. The inner supremum is max{u — au, -al}, and thus the optimal dual
variable is α* = U-I which yields the optimal value (multiplied by V) as ^(u - U-Iu) = - U-IV
which is equivalent to using linear relaxation with a Θ Z + b = UU-■ Θ (z - l). We can represent it
十
as a Θ Z + b = 奴+-「Θ (z - l-) to include the case of active/dead ReLU. For the lower linear
bound ⅛(z) = a Θ Z + b in case of unstable ReLU, we can use any 0 ≤ a ≤ 1 and b = 0
according to the dual relaxation with α. While CAP and CROWN-IBP use a dual feasible solution
like a = uH- or α = 1[u+ + l- > 0], our proposed method aims to optimize over the dual
variable α or equivalently optimize over 0 ≤ a ≤ 1 to further tighten the upper bound on the loss.
16
Under review as a conference paper at ICLR 2021
D Illustration of Linear Relaxations
Figure 6 provides some illustrations of linear relaxations used in IBP, CAP, CROWN-IBP, and the
proposed method. CROWN-IBP adaptively chooses the relaxation variable so that the area between
h and h is minimized. However, the smaller area does not necessarily imply the tighter bound, and
the proposed method achieves tighter bounds than CROWN-IBP relaxation as shown in Figure 4.
(d) CROWN-IBP
(u+ + l- > 0)
(b) IBP (u, l > 0)	(c) CAP
(e) CROWN-IBP
(u+ + l- ≤ 0)
(f) The proposed method
Figure 6: Illustrations of linear relaxation methods. Except for (b), they illustrate the relaxations
when l ≤ 0 ≤ u (Unstable ReLU). (b) Illustration of the relaxation of IBP when u, l > 0 (Active
ReLU).
17
Under review as a conference paper at ICLR 2021
E	Learning curves for variants of CROWN-IBP
It seems that a certifiable training with a looser bound tends to favor stable ReLUs. For example, IBP
starts with small number of unstable ReLUs while CAP starts with large number of ReLUs as shown
in Figure 2 (bottom). However, a tighter bound does not directly lead to many unstable ReLUs. We
find that 0.5/1 and 1/1 have looser bounds than CROWN-IBP (as shown in Figure 7) but they have
more unstable ReLUs (as shown in Figure 3) wherep/q denotes the variant with sampling a ∈ {0,1}
with P (a = 1 | |l| > |u|) = P and P (a = 1 | |l| ≤ |u|) = q for unstable ReLUs. On the other hand,
0/0, 0/0.25, and 0/0.5 have looser bounds than CROWN-IBP and they have less unstable ReLUs,
which leads to small loss variations as in Figure 7. Therefore, this observation implies that it is more
important to have less a = 1 to have a more smooth landscape.

3×100
Learning Curves
∞ 2 × IO0
O
100
Epoch
Figure 7: The learning curves for the scheduled value of with the loss variation along gradient
descent direction (equivalent to Figure 1). As the ratio of the number of a with a = 1 increases, the
loss variation increases.
O
100
120	140	160
180	200
Epoch
Figure 8: A zoomed-in version of Figure 7 for epochs 100-200.
Table 3: Performance (in terms of errors) of the variants of CROWN-IBP (β = 1). Note that 0/0.25,
0/0.5, and CAP-IBP start with looser bounds but they have more smooth landscape, which leads to
a better performance than CROWN-IBP (β = 1) (highlighted with underline).
Model	0/0	0/0.25	0/0.5	0/1 CROWN-IBP (β =	1)	0.5/1	1/1	CAP-IBP	OURS
Standard	70.66	64.50	62.72	63:24		70.69	71.41	60.36	57.14
PGD	73.84	72.67	71.42	71.70		76.68	77.03	69.46	66.88
Verified	77.60	74.47	74.92	75.72		78.38	78.73	74.29	71.45
18
Under review as a conference paper at ICLR 2021
F Proof
To prove Theorem 1, we first prove the following proposition. We note that θ and g are vectorized
and the matrix norm of Jacobian is naturally defined - for example, ||V^ g|| is induced by the vector
norms defined in X and Θ.
Proposition 1. Given input x ∈ X and perturbation radius e,let M = max{||x0|| : x! ∈ B(x, e)}.
Then, for the upper bound s(x; θ) = maxx∕∈>(x,e) g(x; θ)τx0 + b(x; θ) with b satisfying Assump-
tion 1 (1),we have
||Ves(x; θι) - Vθs(x; θ2)H
≤ 2c||Veg(x; θ1,2)H + M||Veg(x; θι) - Vθg(x; θ2)H + L^θ|仇-θ2H	(15)
for any θɪ, θ2, where θ1,2 can be any of θɪ and θ2.
Proof. Say f (x0; θ) = g(x; θ)τx0 + b(x; θ) and the maximizer x* = arg maxx∕∈1(x,e) f (x0; θi)
for each θi = θι, θ2. Then, we have
||Ves(x; θι)-Vθs(x; θ2)H = ||Vef(x；; θ1) -V0f(x*; θ2)H
=||Vef (x*; θ1) - Vθf(x*; θι) + Vθf (x*; θ1) - Vθf(x*; θ2)H
≤ ||Vf (x*; θι)-Vθf(x*; θ1)H + ||Vef(x*; θι)-Vθf(x2; θ2)H∙	(16)
The first term on the RHS can be upper bounded as follows:
||Vf(x*;θι) -Vθf(x*;θ1)H = ||Ve(gτx* -g1τx2)||
=||Ve(gTx* -giTx2)||
=||Vegι(x* - x2)||
≤ 2e||Vegi||,
where gi = g(x; θi), bi = b(x; θi), gT = [gT； bi] and XT = [xτ; 1]. And the second term on the
RHS can be upper bounded as follows:
||Vef(x2;θι) -Vθf(x*;θ2)H = ||Ve(g1τx* -g2x2)||
= ||Ve (g- <72)x21|
≤ ||V0(gi - g2)||||x却| + ||Ve(bι - b2)U
≤ MIIVe(gi - g2)M + LθθMθι - θ2U,
Therefore, we obtain
||Ves(x; θι) -Vθs(x; θ2)H ≤ 2e||Vegi|| + M||Ve(g1 - g2)|| + L,θθ|仇-θ?”
= 2e||Veg(x; θ1)H + M||Veg(x; θ1) - Vθg(x; θ2)H + L)e忸-θ2H∙
Note that θι in the first term is arbitrarily chosen in (16). Therefore, this leads to the final inequality
(15).	□
Theorem 1. Given input X ∈ X and perturbation radius G let M be maxx∕∈1(x,e) ||x0||. For a
linear relaxation-based method with the upper bound sm(x; θ) = maxx∕∈1(x,e) g(m)(x; θ)tx0 +
b(m)(x; θ), f b(m) satisfies Assumption 1 (1)for each m and ps satisfies Assumption 1 (2), then
||VeL(s(x; θ1)) -VeL(s(x; θ2))H
≤ max(2e||Veg(m)(x； θ1,2)H + M||Veg(m)(x; θ1) - Veg(m)(x; θ2)H + L(m)Hθ1 - θ2U)
"ɪ	(4)
for any θι, θ2, where L(m) = Leem) + Lps ||Ves(x; θ1,2)M and θι,2 can be any of θι and θ2.
19
Under review as a conference paper at ICLR 2021
Proof. We simplify the notation PS as p. Then we have
∣∣VθL(s(x; θι))-VeL(s(x; θ2))||
=∣∣vθs(x; θι)VsL(s(x; θι)) - Ves(x; θ2)VL(S(x; θ2))∣∣
=∣∣EVe sm(x; θι)(pm (x; θ1) - δy,m) - Ve sm(χ∙, θ2)(pm(x; θ2) - δy,m )∣∣
m
=∣∣VeS(x; θι)(p(x; θι) - e(y)) -VeS(x; θ2)(p(x; θ2) - eS))II
=∣∣Ves(x; θι)p(x; θι) - Ves(x; θ2)p(x; θ2)∣∣
=∣∣Ves(x; θι)p(x; θι) - Ves(x; θι)p(x; θ2) + Ves(x; θι)p(x; θ2) - Ves(x; θ2)p(x; θ2)∣∣
=∣∣Ves(x; θι)(p(x; θι) - p(x; θ2)) + (Ves(x; θι) - Ves(x; θ2))p(x; θ2)∣∣
≤ ∣∣Ves(x; θι)∣∣∣∣p(x; θι) - p(x; θ2)∣∣ + max ∣∣VeSm(x; θι) - VeSm(x; θ2)∣∣
m
≤ ∣∣VeS(x; θ1)∣∣Lp∣∣θ1 - θ2∣∣ + max ∣∣VeSm(x; θι) - VeSm(x; θ2)∣∣
m
≤ max (2e∣∣Veg(m)(x； θ1,2)∣∣ + M∣∣Veg(m)(x; θ1) -Veg(m)(x; θ2)∣∣ + L(m)∣∣θ1 - θ2∣∣)
□
G	Learning curve for Ctrain
Figure 9 shows the learning curves for the target perturbation etrain during the ramp-up period,
while Figure 1 shows the corresponding curves for the scheduled value of e. The two figures use the
same settings in Appendix A.1.
Figure 9:	The learning curves for the target perturbation etrain during the ramp-up period.
20
Under review as a conference paper at ICLR 2021
H Smoothness
We empirically measure the non-smoothness of the loss landscape with the difference between the
two consecutive loss gradients at θ1 = θ(0) and θ2 = θ(1) in (8), says gradient difference (≡
∣∣Vθ L(x; θ(0)) - Vθ L(x; θ(1))∣∣). It is highly related to the ratio of the number of unstable ReLUs
(nonlinearity of the classifier) as shown in Figure 10.
guaJSJJ5x4LJ--E∞3us0u
rɪ0
O
frpɑ Wqelsun
0.03-
0.02-
0.01-
30
1
O
-
1
1
力poch
E
，
Figure 10:	(Top) Gradient difference and (Middle) cosine similarities between two consecutive loss
gradients, and (Bottom) the ratio of the number of unstable ReLUs during the ramp-up period.
21
Under review as a conference paper at ICLR 2021
I Mode Connectivity
In this section, we check the mode connectivity (Garipov et al., 2018) between two models that
are trained using certifiable training methods. Mode connectivity is a framework that investigates
the connectedness between two models by finding a high accuracy curve between those models. It
enables us to understand the loss surface of neural networks.
Let w0 and w1 be two sets of weight corresponding to two different well-trained neural networks.
Moreover, let φθc (t) with t ∈ [0, 1] be a continuous piece-wise smooth parametric curve with pa-
rameters θc such that φθc (0) = w0 and φθc (1) = w1. To find a low-loss path between w0 and w1,
Garipov et al. (2018) suggested to find the parameter θc that minimizes the expectation of a loss
`(w) over a distribution qθc (t) on the curve,
L(θc) = Et〜q。。(t)['(Φθc (t)].
To optimize L(θc) for θc, we use uniform distribution U[0, 1] as qθc (t) and Bezier curve (Farouki,
2012) as φθc (t), which provides a convenient parameterization of smoothness on the paths connect-
ing two end points (w0 and w1) as follows:
φθc (t) = (1 - t)2w0 + 2t(1 - t)θc + t2w1, 0 ≤ t ≤ 1.
A path φθc is said to have a barrier if ∃t such that '(φθc (t)) > max{'(wQ), '(wι)}. The existence
of a barrier suggests the modes of two well-trained models are not connected by the path in terms of
the given loss function ` (Zhao et al., 2020).
We test the mode connectivity between the models trained with IBP, CROWN-IBP, and OURS.
For example, to check the mode connectivity between two different models trained with CROWN-
IBP and IBP, we use the loss function used on each model as a user-specified loss for training the
parametric curve φθc . Therefore, we can obtain two curves as depicted in Figure 11, 12, and 13 for
each pair of models. Here, we use the identical settings in Appendix A.1.
Figure 11 shows the mode-connectivity between CROWN-IBP and IBP. We use CROWN-IBP loss
as user-specific loss in Figure 11a and IBP loss in Figure 11b. In this figure, we find that using
CROWN-IBP loss (11a), there exists a barrier between the two models. This suggests they are not
connected by the path in terms of CROWN-IBP loss. However, with IBP loss, there is no loss
barrier separating the two models. This indicates that using CROWN-IBP, it is hard to optimize
the parameters from w0 to w1 , but IBP can.
Figure 12 shows the mode-connectivity results on IBP and OURS. We find that two models are not
connected to each other using either IBP bound or OURS bound, since there exists a barrier in both
curves. In this figure, we can also notify that OURS has tighter bounds than IBP because the value
of the loss function using OURS is lower than that of IBP.
Finally, Figure 13 illustrates the mode connectivity between CROWN-IBP and OURS. Using
CROWN-IBP as a user-specified loss function, we can find that the robust loss on the curve is
higher than that of the end points. However, when OURS is used as a loss function, the robust loss
generally decreases as the t increases. It shows that OURS has much favorable loss landscape com-
pared to CROWN-IBP. In addition, we can find that OURS has a tighter bound than CROWN-IBP,
since the value of the robust loss using OURS is lower than CROWN-IBP.
22
Under review as a conference paper at ICLR 2021
⅛ 3.25
I 3.00
o
§2.75
鼻 2.50
§ 2.25
2 2.00
(dBD ssoηttnqo≈
3.25-
3.00-
2.75-
2.50-
2.25-
2.00-
0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0
t	t
(a) CROWN-IBP bound	(b) IBP bound
Figure 11:	Mode connectivity between CROWN-IBP and IBP, where w0 and w1 are well-trained
models using CROWN-IBP bound and IBP bound, respectively. θc is trained using CROWN-IBP
(11a) and IBP (11b), respectively.
(a) IBP bound
(b) OURS bound
Figure 12: Mode connectivity between IBP and OURS, where w0 and w1 are well-trained models
using IBP bound and OURS bound, respectively. θc is trained using IBP (12a) and OURS (12b),
respectively.
Figure 13: Mode connectivity between CROWN-IBP and OURS, where w0 and w1 are well-trained
models using CROWN-IBP bound and OURS bound, respectively. θc is trained using CROWN-IBP
(13a) and OURS (13b), respectively.
23
Under review as a conference paper at ICLR 2021
J	RELU
In this section, we investigate how pre-activation bounds u and l for the activation layer change
during training. For each activation node, it is said to be ”active” when the pre-activation bounds are
both positive (0 < l ≤ u), ”unstable” when they span zero (l ≤ 0 ≤ u), and ”dead” when they are
both negative (l ≤ u < 0).
Figure 14 shows the ratios of the number of active and dead ReLUs during the ramp-up period.
Notably, we find that CROWN-IBP has more active ReLUs during training compared to the other
three methods. Simultaneously, CROWN-IBP has the lowest ratio of dead ReLUs.
Figure 15 shows the numbers of active, unstable, and dead ReLUs during the ramp-up period. We
find that in CROWN-IBP, the number of unstable and active ReLUs increases as the number of dead
ReLUs decreases. This indicates that a number of dead ReLUs change to unstable ReLUs as the
training increases. However, in the other methods, the number of unstable ReLUs is consistently
small, while the number of active ReLUs decreases as the number of dead ReLUs increases.
Figure 16 depicts the histograms of the distribution of the slope u++- of the unstable ReLUs
during the ramp-up period. In the early stages of CAP training, the slope distribution is concentrated
around 0.4. However as the training progresses with a larger , the histogram distribution moves
to left, which indicates unstable ReLUs change to dead ReLUs. It is consistent with the results in
Figure 15c. On the other hand, in the case of CROWN-IBP, the histogram distribution moves to
right during training. It is the same with the results in Figure 15b, which shows that number of
active ReLUs increases during training.
S 0.80-
0.70
0.20 τ
I 0.15-
(υ 0.10-
ti 0.05-
o.oo--
1.00 γ-
ɔ
ω 0.90-
70
Epoch
IBP
CROWN-IBP (β = l)
CAP
OURS
Figure 14: The ratio of the number of active (top) and dead (bottom) ReLUs during the ramp-up
period.
24
Under review as a conference paper at ICLR 2021
60000
50000
40000
30000
20000
10000
0
IBP
epoch
CROWN-IBP (^ = 1)
60000
50000
40000
30000
20000
10000
0
10 20 30 40 50 60 70 80 90 100 110 120 130
epoch
(a) IBP
(b) CROWN-IBP (β = 1)
60000
50000
40000
30000
20000
10000
0
CAP
epoch
OURS
60000
50000
40000
30000
20000
10000
0
10 20 30 40 50 60 70 80 90 100 110 120 130
epoch
(c) CAP
(d) OURS
Epoch IO
Epoch 40
Epoch 70
5
5
5
4
3
4
4
0
Figure 15: Number of active (Green), unstable (Orange), and dead (Red) ReLUs.

(a) IBP
(b) CROWN-IBP
Epoch 100
OJO 02	0.4	0.6	0.8	1.0
0.0	0.2	0.4	0.6	0.S	1.0
4
Epoch 130
5
(d) OURS
Figure 16: Histograms of the distribution of the slope u+U+- When l ≤ 0 ≤ u during the ramp-up
period.
25
Under review as a conference paper at ICLR 2021
K Comparison with other prior work
All experiments and results (except for Table 4) in this paper are based on our own reimplementation.
For the unimplemented prior work, we compare to the best reported results in the literature in Table
4. We note that the results in Xiao et al. (2018) and Balunovic & Vechev (2019) are evaluated with
a MILP based exact verifier (Tjeng et al., 2017).
Table 4: Test errors (Standard / Verified error) compared to the best errors reported in the literature.
Bold numbers are the lowest verified error.
Data	e(l∞)	Xiao et al. (2018)	Mirman et al. (2018)	Balunovic & Vechev (2019)	OURS
MNIST	0.1-	1.32/4.87	1.3/4.2	0.8 /2.9	1.09 / 2.28
	0.3	2.67/19.32	3.4 / 10.7	2.7 / 14.3	2.42 / 7.84
CIFAR-10	2/255	38.88/54.07	37.7 / 54.5	21.6 / 39.5	31.49 / 49.42
	8/255	59.55/79.72	53.8 / 72.8	48.3 / 72.5	56.01 / 69.70
L β- AND κ-SCHEDULINGS
Table 5 shows the evaluation results of the models as in Table 1 but trained with different κ-
scheduling (from 0 to 0). Table 6 shows the evaluation results of the proposed models trained with
different κ- and β-schedulings.
Table 5: Test errors (Standard / PGD / Verified error) of IBP, CROWN-IBP (β = 1), CAP, and
OURS on MNIST, CIFAR-10, and SVHN. See Appendix A for all the other settings, same as in
Table 1. Bold and underline numbers are the first and second lowest verified error.
Data	e(Ig)	IBP	CROWN-IBP (β = 1)	CAP	OURS
	e = 0.1	1.25/2.31/3.10	1.23/2.19/2.75	0.80 / 1.73 / 3.19	1.09 / 1.86 / 2.28
MNIST	e = 0.2	1.95/2.95/6.28	2.89 / 5.32 / 7.61	3.22 / 6.72 / 11.06	1.70 / 3.37 / 4.78
	e = 0.3	3.67/5.55 /9.74	6.11 / 11.33 / 17.51	19.19 / 35.84 / 47.85	3.39 / 4.85 / 9.12
	e = 0.4	3.67/6.55/16.55	6.11 / 15.34 / 26.72	-	3.39 / 5.88 / 15.04
	e = 2/255	43.60/52.62/56.58	32.15 / 42.67 / 49.36	28.80 / 38.95 / 48.50	32.04/43.13/49.62
CIFAR	e = 4/255	53.89/62.58/65.14	45.05 / 56.46 / 63.04	40.78/52.62/61.88	43.15 / 54.85 / 61.31
10	e = 6/255	61.37/68.64/70.82	53.87 / 65.03 / 71.08	49.20/60.85/69.03	50.99 / 62.23 / 67.59
	e = 8/255	64.11 /70.68/72.99	60.96 / 70.52 / 75.68	56.77 / 66.78 / 73.02	56.35 / 67.06 / 70.56
	e = 16/255	69.74/76.66/79.86	79.14 / 83.64 / 84.36	75.11 / 80.67 / 82.07	66.96 / 75.63 / 78.08
SVHN	e = 0.01	20.19/34.57/44.25	16.66/30.05/38.15	16.88/ 30.16 / 37.09	15.46 / 29.34 / 38.57
M one-step vs multi-step
To get a tighter bound, we propose multi-step version of (6) as follows:
at+1 = Π[0,i]n (at - asign(VaL(s(x, y, e; θ, φ), y))) .	(17)
We compare the original 1-step method (α ≥ 1) to 7-step (t = 7) method with α = 0.1. The results
are summarized in Table 7. We found no significant difference between two methods even though
multi-step takes multiple times with multi-step. Therefore, we decide to focus on one-step method.
26
Under review as a conference paper at ICLR 2021
Table 6: Test errors of OURS with different β- and κ-scheduling on MNIST and CIFAR-10.
Data	(l∞)		OURS1→1 (K =		1 → 0)	OURS 1→0 (κ =		1 → 0)	OURS1→1 (κ = 0 → 0)			OURS1→0 (κ =		0→0)
			Standard	PGD	Verfied	Standard	PGD	Verfied	Standard	PGD	Verfied	Standard	PGD	Verfied
	e	= 0.1	1.09	1.77	2.36	1.29	2.29	3.58	1.09	1.86	2.28	1.15	2.03	3.53
MNIST	e	= 0.2	1.70	3.44	4.34	1.61	3.09	5.71	1.70	3.37	4.78	1.64	2.57	5.43
	e	= 0.3	3.49	5.59	9.79	2.42	4.37	7.84	3.39	4.85	9.12	2.44	4.41	8.00
	e	= 0.4	3.49	6.77	15.42	2.42	5.68	13.72	3.39	5.88	15.04	2.44	5.29	13.84
CIFAR 10	e	2/255	31.49	42.73	49.42	37.77	48.30	54.43	32.04	43.13	49.62	38.58	48.59	54.63
	e	8/255	56.01	66.17	69.70	58.87	67.76	71.50	56.35	67.06	70.56	58.90	67.81	70.99
	e =	16/255	65.39	75.39	77.87	66.24	74.69	78.66	66.96	75.63	78.08	66.76	75.17	77.99
Table 7: Test errors of OURS with different numbers of gradient update steps in (17) on CIFAR-10.
Here, we use κ-scheduling from 0 to 0.
Data	e(I∞)	OURS (1-step)			OURS (7-step)		
		Standard	PGD	Verfied	Standard	PGD	Verfied
CIFAR-10	E = 2/255	32.04^^	43.11	49.62	31.40^^	42.30	49.20
	e = 8∕255	56.35	67.03	70.56	54.44	66.29	71.53
27
Under review as a conference paper at ICLR 2021
N	TRAIN WITH train ≥ test
N.1 train ≥ test ON MNIST
Zhang et al. (2019b) and Gowal et al. (2018) observed that IBP performs better when using
train ≥ test than train = test . Figure 8 shows the results with different train’s for each test .
The overfitting issue is more prominent in the case of IBP and CROWN-IBP1→0 than the proposed
method and CROWN-IBP1→1. However, using larger perturbations compromises the standard ac-
curacy, and thus it is desirable to use smaller train .
Table 8: Comparison of the performance (Standard / PGD / Verified error) depending on various
train . Here, we use κ-scheduling from 0 to 0.
Data	Qest	Grain	IBP	CROWN-IBP1-1	OURS	CROWN-IBPi →o
MNIST	0.2	0.2	1.25 / 3.39 / 7.77	1.23 / 3.48 / 7.64	1.09 / 3.17 / 6.29 1.95 / 2.93 / 6.28	2.89 / 5.32 / 7.61	1.70 / 3.37 / 4.76 3.67 / 4.77 / 6.36	6.11 / 9.08 / 12.71	3.49 / 4.72 / 6.36	1.13/2.85/5.89 1.48/2.73/4.79 2.37/3.26/4.64
		0.3		
		0.4		
	0.3	0.3	1.95 / 3.31 / 12.90	2.89 / 7.35 / 14.97	1.70 / 4.82 / 9.20 3.67 / 5.55 / 9.74	6.11 / 11.33/ 17.51	3.49 / 5.59 / 9.79	1.48/3.52/9.40 2.37/3.63/7.22
		0.4		
N.2 train = 1.1test ON CIFAR-10
As mentioned in Gowal et al. (2018), we also train with train = 1.1test on CIFAR-10. The results
are shown in Table 9. They attain slightly improved performances in 2/255, but not in 8/255 and larger
.
Table 9: Comparison of the performance (Standard / PGD / Verified error) of the models trained with
train and 1.1train. Here, we use κ-scheduling from 0 to 0.
Data	Jest	train	IBP	CROWN-IBPι→ι	OURS	CROWN-IBP1→0
CIFAR 10	2/255	2/255 2.2/255	43.6 / 52.71 / 56.58	32.15 / 42.67 / 49.36 32.04 / 43.13 / 49.62 44.78 / 52.62 / 55.78 33.23 / 43.11 / 49.18	33.04 / 43.70 / 48.60	37.25/47.19/52.53 38.42/47.80/52.53
	8/255	8/255 8.8/255	64.11 / 70.68 / 72.99 60.96 / 70.52 / 75.68 56.35 / 67.06 / 70.56 64.54 / 70.30 / 72.40 61.48 / 70.58 / 75.17 58.28 / 67.50 / 70.52	56.95/67.89/70.43 59.37/68.51 /70.71
O Training time
All the training times are measured on a single TITAN X (Pascal) on Medium for CIFAR-10. We
train with a batch size of 128 for OURS, CROWN-IBP1→1 and IBP, but with a batch size of 32 for
CAP due to its high memory cost. For CAP, we use random projection of 50 dimensions.
•	OURS: 115.9 sec / epoch
•	CROWN-IBP1→1: 51.68 sec / epoch
•	IBP: 14.85 sec / epoch
•	CAP (batch size 32, 1 GPU): 751.0 sec / epoch
•	CAP (batch size 64, 1 GPU): 724.6 sec / epoch
•	CAP (batch size 128, 2 GPUs): 387.9 sec / epoch
28
Under review as a conference paper at ICLR 2021
P	Loss and Tightness violin plots
We plot the equivalent tightness violin plots in Section 6 for models trained with other methods. The
proposed method achieves the best results in terms of loss and tightness followed by CROWN-IBP,
CAP-IBP, and RANDOM. Figure 17 (a)-(b), (c)-(d), and (e)-(f) show the tightness evaluated on the
model trained by CROWN-IBP1→0, CROWN-IBP1→1 and IBP, respectively.
2.6
2.4
S
S r r
O 2.2
2.0
1.8
OURS CROWN-IBPCAP-IBP RANDOM
(a) the loss of CROWN-IBP1→0
SSOUaLIFJ.
-2.5
-5.0
-7.5
-10.0
-12.5
-15.0
OURS CROWN-IBPCAP-IBP RANDOM
(b) the tightness of CROWN-IBP1-0
-3
OURS CROWN-IBPCAP-IBP RANDOM
4 5 6 7
- - - -
ssəu 主 6_J_
OURS CROWN-IBPCAP-IBP RANDOM
(d) the tightness of CROWN-IBP1→1
(C) the loss of CROWN-IBP1→1
4 3 2
SSo-I
OURS CROWN-IBPCAP-IBP RANDOM
(e) the loss of IBP
SS9U4⅛F
10
-
OURS CROWN-I BP CAP-IB P RANDOM
(f) the tightness of IBP
5
中
Figure 17:	Violin plots of the test loss (Left Column) and of tightness (Right Column) for various
linear relaxations same as in SeCtion 6. Lower is better.
29
Under review as a conference paper at ICLR 2021
Q Comparison with CAP-IBP
As in section E, we train a model with CAP-IBP and compare with the proposed method and
CROWN-IBP (β = 1). Figure 18 shows that CAP-IBP has gradient differences (defined in Sec-
tion H) larger than the proposed method and smaller than CROWN-IBP (β = 1), which leads to a
performance between the proposed method and CROWN-IBP (β = 1) (see Table 3). CAP-IBP has
looser bounds than CROWN-IBP (β = 1) as shown in Figure 4 and Figure 17, but with a relatively
more smooth landscape, it can achieve a better performance than CROWN-IBP (β = 1).
guaJSJJ5口匚-一 E∞3us8
-0.5
0.02 -∣-——CROWN-IBP (β = l) ——CAP-IBP ——OURS
0.01-
0.5-
0.0-
0.00
1.0
rɪ0
O
fπωΞSC⊃
0.03-
0.02-
0.01-
30
1
O
-
1
1
力poch
E
，
Figure 18:	(Top) Gradient difference and (Middle) cosine similarities between two consecutive loss
gradients, and (Bottom) the ratio of the number of unstable ReLUs during the ramp-up period.
30
Under review as a conference paper at ICLR 2021
R ReLU Stability
To see the effect of unstable ReLUs on smoothness, we adopt the ReLU stability loss (RS loss)
LRS(u,l) = -tanh(1 + U ∙ l) as a regularize] (Xiao et al., 2018). We use L + XLrs as a loss
and run CROWN-IBP (β = 1) with various λ settings. We plot the smoothness and the tightness in
Figure 19 and Figure 20 on λ = 0, λ = 0.01, λ = 10.
We found that small λ suggested in Xiao et al. (2018) has no effect on reducing the number of
unstable ReLUs since certifiable methods have smaller unstable ReLUs as shown in Figure 15,
and thus not on improving the smoothness. By increasing λ, we observed that RS successfully
reduces the number of unstable ReLUs with λ = 10. Figure 19 shows that large λ leads to a better
loss variation and gradient difference. This supports that unstable ReLUs are closely related to the
smoothness of the loss landscape. However, as Xiao et al. (2018) mentioned ”placing too much
weight on RS Loss can decrease the model capacity, potentially lowering the provable adversarial
accuracy”, the models trained with a large λ ≥ 1 couldn’t obtain a tightness of the upper bound and
significant improvement on robustness as illustrated in Figure 20. The test errors (Standard / PGD /
Verified) are 0.6278 / 0.7189 / 0.7634 on λ = 0.01 and 0.6090 / 0.7085 / 0.7600 on λ = 10.
0.01 -
%u*9tsPe-IUe.le=E∞©U一S。。 3^ ^ elsu⊃
0.00 -
1.0 -
0.5 -
0.0 -
-0.5 -
0.03 -
0.02 -
0.01 -
0.00-
10	20	30	40	50	60	70	80	90	100	110	120	130	140	150	160	170	180	190	200
Epoch
Figure 19: (Top) Gradient difference and (Middle) cosine similarities between two consecutive loss-
gradients, and (Bottom) the ratio of the number of unstable ReLUs on CROWN-IBP (β = 1) with
λ = 0, λ = 0.01, λ = 10 and OURS.
Figure 20: Robust loss of CROWN-IBP (β = 1) with λ = 0, λ = 0.01, λ = 10 and OURS during
training.
31