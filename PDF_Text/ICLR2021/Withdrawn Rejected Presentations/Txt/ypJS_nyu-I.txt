Under review
A Deeper Look at Discounting Mismatch in
Actor-Critic Algorithms
Ab stract
We investigate the discounting mismatch in actor-critic algorithm implementa-
tions from a representation learning perspective. Theoretically, actor-critic algo-
rithms usually have discounting for both actor and critic, i.e., there is a γt term in
the actor update for the transition observed at time t in a trajectory and the critic is
a discounted value function. Practitioners, however, usually ignore the discount-
ing (γt) for the actor while using a discounted critic. We investigate this mismatch
in two scenarios. In the first scenario, we consider optimizing an undiscounted
objective (γ = 1) where γt disappears naturally (1t = 1). We then propose to
interpret the discounting in critic in terms of a bias-variance-representation trade-
off and provide supporting empirical results. In the second scenario, we consider
optimizing a discounted objective (γ < 1) and propose to interpret the omission of
the discounting in the actor update from an auxiliary task perspective and provide
supporting empirical results.
1	Introduction
Actor-critic algorithms have enjoyed great success both theoretically (Williams, 1992; Sutton et al.,
2000; Konda, 2002; Schulman et al., 2015a) and empirically (Mnih et al., 2016; Silver et al., 2016;
Schulman et al., 2017; OpenAI, 2018). There is, however, a longstanding gap between the theory
behind actor-critic algorithms and how practitioners implement them. Let γ, γA , and γC be the
discount factors for defining the objective, updating the actor, and updating the critic respectively.
Theoretically, no matter whether γ = 1 or γ < 1, we should always use γA = γC = γ (Sutton
et al., 2000; Schulman et al., 2015a) or at least keep γA = γC if Blackwell optimality (Veinott, 1969;
Weitzman, 2001) 1 is considered. Practitioners, however, usually use γA = 1 and γC < 1 in their
implementations (Dhariwal et al., 2017; Caspi et al., 2017; Zhang, 2018; Kostrikov, 2018; Achiam,
2018; Liang et al., 2018; Stooke & Abbeel, 2019). Although this mismatch and its theoretical
disadvantage have been recognized by Thomas (2014); Nota & Thomas (2020), whether and why
it yields benefits in practice has not been systematically studied. In this paper, we empirically
investigate this mismatch from a representation learning perspective. We consider two scenarios
separately.
Scenario 1: The true objective is undiscounted (γ = 1). The theory prescribes to use γA = γC =
γ = 1. Practitioners, however, usually use γA = γ = 1 but γC < 1, introducing bias. We explain
this mismatch with the following hypothesis:
Hypothesis 1. γC < 1 optimizes a bias-variance-representation trade-off.
It is easy to see that γC < 1 reduces the variance in bootstrapping targets. Besides this, we further
provide empirical evidence showing that when γC < 1, it may become easier to find a good repre-
sentation compared to γC = 1. Consequently, although using γC < 1 introduces bias, it can facilitate
representation learning. For our empirical study, we make use of recently introduced techniques,
such fixed horizon temporal different learning (De Asis et al., 2019) and distributional reinforce-
ment learning (Bellemare et al., 2017) to disentangle the various effects the discount factor has on
the learning process.
Scenario 2: The true objective function is discounted (γ < 1). Theoretically, there is a γt term
for the actor update on a transition observed at time t in a trajectory (Sutton et al., 2000; Schulman
1Blackwell optimality states that, in finite MDPs, there exists a γ0 < 1 such that for all γ ≥ γ0 , the optimal
policies for the γ-discounted objective are the same.
1
Under review
et al., 2015a). Practitioners, however, usually ignore this term while using a discounted critic, i.e.,
γA = 1 and γC = γ < 1 are used. We explain this mismatch with the following hypothesis:
Hypothesis 2. Using γC = γ < 1 and γA = 1 is effectively similar to using γC = γA = γ < 1 plus
an auxiliary loss that sometimes facilitates representation learning.
Our empirical study involves implementing the auxiliary task explicitly by using an additional policy
for optimizing the difference term between the loss ofγA = 1 and the loss ofγA < 1. We also design
new benchmarking environments where the sign of the reward function is flipped after a certain time
step such that later transitions differ from earlier ones. In that setting, γA = 1 becomes harmful.
2	Background
Markov Decision Processes: We consider an infinite horizon MDP
with a finite state space S, a finite action space A, a bounded reward
function r : S → R, a transition kernel p : S × S × A → [0, 1],
an initial state distribution μo, and a discount factor Y ∈ [0,1].2 The
initial state So is sampled from μo. At time step t, an agent in state
γ
γA
γC
define the objective
update the actor
update the critic
St takes action At 〜∏(∙∣St), where ∏ : A×S → [0,1] is the policy
it follows. The agent then gets a reward Rt+1 =. r(St) and proceeds
to the next state St+ι 〜p(∙∣St, At). The return of the policy ∏ at
time step t is defined as Gt =. Pi∞=1 γi-1Rt+i, which allows us to
Table 1: Roles of the differ-
ent discount factors
define the state value function vπγ(S) =. E[Gt|St = s] and the state-action value function qπγ (s, a) =.
E[Gt |St = s, At = a]. We consider episodic tasks where we assume there is an absorbing state
s∞ ∈ S such that r(s∞) = 0 and p(s∞∣s∞, a) = 1 holds for any a ∈ A. When γ < 1, v∏ and
qπγ are always well defined. When γ = 1, to ensure vπγ and qπγ are well defined, we further assume
finite expected episode length. Let Tsπ be a random variable denoting the first time step that an
agent hits s∞ when following π given S0 = s. We assume Tmax =. supπ∈Π maxs E[Tsπ] < ∞,
where π is parameterized by θ and Π is the corresponding function class. Similar assumptions are
also used in stochastic shortest path problems (e.g., Section 2.2 of Bertsekas & Tsitsiklis (1996)).
In our experiments, all the environments have a hard time limit of 1000, i.e., Tmax = 1000. This
is standard practice, classic RL environments also have an upper limit on their episode lengths (e.g.
27k in Bellemare et al. (2013, ALE)). Following Pardo et al. (2018), we add the (normalized) time
step t in the state to keep the environment Markovian. We measure the performance of a policy π
with JY(π) = Eso〜μo[vY(So)].
Vanilla Policy Gradient: Sutton et al. (2000) compute Vθ JY (∏) as
Vθ JY(∏) = Es d∏(S) Ea q∏(s, a)Vθπ(a∣s),
(1)
π
where d∏(s) = P∞==o Yt Pr(St = s∣μo,p,∏) for γ < 1 and d∏(S) = EPt=O Pr(St = s∣So,P,∏)]
for γ = 1.2 3 * Note dYπ remains well-defined for γ = 1 when Tmax < ∞. In order to optimize the
policy performance JY (π), one can follow (1) and, at time step t, update θt as
θt+ι - θt + αγA qγC (St, At)Vθ log π(AtlSt),
(2)
where α is a learning rate. If we replace qπYC with a learned value function, the update rule (2)
becomes an actor-critic algorithm, where the actor refers to π and the critic refers to the learned
approximation of qπYC. In practice, an estimate for vπYC instead of qπYC is usually learned. Theoretically,
we should have YA = YC = Y. Practitioners, however, usually ignore the YAt term in (2), and use
YC < YA = 1. What this update truly optimizes remains an open problem (Nota & Thomas, 2020).
TRPO and PPO: To improve the stability of actor-critic algorithms, Schulman et al. (2015a) pro-
pose Trust Region Policy Optimization (TRPO), based on the performance improvement lemma:
Lemma 1. (Theorem 1 in Schulman et al. (2015a)) For Y < 1 and any two policies π and π0,
JY(∏0) ≥ JY(∏)+ (Ps dY(s) Pa ∏0(a∣s)Advn(S,a)) - ,maxs，。%-YsfIYe(π,πO),
2Following Schulman et al. (2015a), we consider r : S → R instead of r : S × A → R for simplicity.
3Sutton et al. (2000) do not explicitly define dγπ when γ = 1, which, however, can be easily deduced from
Chapter 13.2 in Sutton & Barto (2018).
2
Under review
where Adv∏(s,a) = Es，〜p(∙∣s,α) [r(s) + Yvn(s0) — v∏(s)] is the advantage, e(π, π0) =
maxs DKL(π(∙∣s)∣∣π0(∙∣s)), and DKL refers to the KL divergence.
To facilitate our empirical study, we first make a theoretical contribution by extending Lemma 1 to
the undiscounted setting. We have the following lemma:
Lemma 2. Assuming Tmax < ∞, for γ = 1 and any two policies π andπ0,
JY(∏0) ≥ JY(∏) + (Ps dγ(S) Pa ∏0(a∣s)Adv∏(s,a)) - 4maxs,a |AdvY(S, a)典注乂^/ π0).
The proof of Lemma 2 is provided in the appendix. A practical implementation of Lemmas 1 and 2
is to compute a new policy θ via gradient ascent on the clipped objective:
L(θ) = P∞=0 YA mm n n⅛⅛S⅛Adv乳(St,At), clip(∏⅛⅛⅛)Adv猊("4%	⑶
where St and At are sampled from πθold, and clip(x) =. max(min(x, 1 + ), 1 - ) with a hyper-
parameter. Theoretically, we should have YA = YC, but practical algorithms like Proximal Policy
Optimization (Schulman et al., 2017, PPO) usually use YC < YA = 1.
Policy Evaluation: We now introduce several policy evaluation techniques we use in our empirical
study. Let v be our estimate of v∏. At time step t, Temporal Difference learning (TD, SUttOn
(1988)) updates v as V(St) . v(St) + α(Rt+ι + Yv(St+ι) - v(St)). Instead of the infinite horizon
discounted return Gt, De Asis et al. (2019) propose to consider the H-stepreturn GtH =. PiH=1 Rt+i.
Correspondingly, the H-step value function is defined as vH(s) = E[G? |St = s]. We let vH be our
estimate of VH. At time step t, De Asis et al. (2019) use the following update rule to learn vH:
於(St) J Vi(St) + α(Rt+ι + ViT(St+1) - Vi(St)) (i = 1,... H),	(4)
where V0(s) = 0. In other words, to learn VH, We need to learn {Vi}i=ι,…,h simultaneously.
De Asis et al. (2019) call (4) Fixed Horizon Temporal Difference learning (FHTD).
As Gt is a random variable, Bellemare et al. (2017) propose to learn its full distribution instead of
its expectation only, yielding the Distributional Reinforcement Learning (RL) paradigm. They use
a categorical distribution with 51 atoms uniformly distributed in [-Vmax, Vmax] to approximate the
distribution of Gt, where Vmax is a hyperparameter. In this paper, we refer to the corresponding
policy evaluation algorithm as C51.
Methodology: We consider MuJoCo robot simulation tasks from OpenAI gym (Brockman et al.,
2016) as our benchmark. Given its popularity in understanding deep RL algorithms (Henderson
et al., 2017; Ilyas et al., 2018; Engstrom et al., 2019; Andrychowicz et al., 2020) and designing new
deep RL algorithms (Fujimoto et al., 2018; Haarnoja et al., 2018), we believe our empirical results
are relevant to most practitioners.
We choose PPO, a simple yet effective and widely used algorithm, as the representative actor-critic
algorithm for our empirical study. PPO is usually equipped with generalized advantage estima-
tion (Schulman et al., 2015b, GAE), which has a tunable hyperparameter γ. The roles of Y and Y are
similar. To reduce its confounding effect, we do not use GAE in our experiments, i.e., the advantage
estimation for our actor is simply the TD error Rt+ι + YCV(St+ι) - V(St). The PPO pseudocode We
follow is provided in Alg. 1 in the appendix and we refer to it as the default PPO implementation.
We use the standard architecture and optimizer across all tasks, in particular, the actor and the
critic do not share layers. We conduct a thorough learning rate search in Ant for each algorithmic
configuration (i.e., a curve in a figure) and then use the same learning rate for all other tasks. When
using FHTD and C51, we also include H and Vmax in the grid search. All details are provided in the
appendix. We report the average episode return of the ten most recent episodes against the number
of interactions with the environment. Curves are averages over ten independent runs with shaded
regions indicating standard errors. 3
3 Optimizing the Undiscounted Objective (Scenario 1)
When our goal is to optimize the undiscounted objective JY=1(π), one theoretically grounded option
is to use YA = YC = Y = 1. By using YA = 1 and YC < 1, practitioners introduce bias. We first
3
Under review
empirically confirm that introducing bias in this way indeed has empirical advantages. A simple first
hypothesis is that γC < 1 leads to lower variance in Monte Carlo return bootstrapping targets than
γC = 1, it thus optimizes a bias-variance trade-off. However, we further show that there are empirical
advantages from γC < 1 that cannot uniquely be explained by this bias-variance trade-off, indicating
that there are additional factors beyond variance. We then show empirical evidence identifying
representation learning as an additional factor, leading to the bias-variance-representation trade-off
from Hypothesis 1. All the experiments in this section use γA = 1.
rccc. HalfCheetah .	.	WalI⅞r2d	. 〜5小 HOPPer .	______Ant_____ IICC _	HUmanOld .	. HUmanOldStandUP .
EnlaJ pəlunoɔs-pun
0	2M
steps
0
steps
0	2M
steps
0	2M
steps
Figure 1: The default PPO implementation with different discount factors.
1500
Ant
1000
500
6	2M
steps
Figure 2: Comparison between PPO and PPO-TD when γC = 1.
Figure 3: PPO-TD with different discount factors.
Bias-variance trade-off: To investigate the advantages of using γC < 1, we first test default PPO
with γC ∈ {0.95, 0.97, 0.99, 0.995, 1}. We find that the best discount factor is always with γC < 1
and that γC = 1 usually leads to a performance drop (Figure 1). In default PPO, although the
advantage is computed as the one-step TD error, the update target for updating the critic V(St)
is almost always a Monte Carlo return. As there is no γAt term in the actor update, we should
theoretically use γC = γA = 1 when computing the Monte Carlo return, which usually leads to
high variance. Consequently, a simple hypothesis for the empirical advantages of using γC < 1 is a
bias-variance trade-off. We find, however, that there is more at play.
Beyond bias-variance trade-off: To reduce the effect of γC in controlling the variance, we bench-
mark PPO-TD (Algorithm 2 in the appendix). PPO-TD is the same as default PPO except that
the critic is updated with one-step TD, i.e., the update target for V(St) is now Rt+ι + YCV(St+ι).
Although Figure 2 shows that PPO-TD (γC = 1) outperforms PPO (γC = 1) by a large margin,
indicating bias-variance may be at play, Figure 3 suggests that for PPO-TD as well, γC < 1 is still
preferable to γC = 1. To further study this phenomenon, we benchmark PPO-TD-Ex (Algorithm 3 in
the appendix), in which we provide N extra transitions to the critic by sampling multiple actions at
any single state and using an averaged bootstrapping target. The update target for V(St) in PPO-TD-
Ex is n+i PN=o Rt+ι + YCV(St+ι). Here R0+、and S?+、refer to the original reward and successor
state. To get Rti+1 and Sti+1 for i ∈ {1, . . . , N}, we first sample an action Ait from the sampling
policy, then reset the environment to St, and finally execute Ait to get Rti+1 and Sti+1. Importantly,
we do not count those N extra transitions in the x-axis when plotting. The advantage for the actor
update in PPO-TD-Ex is estimated with RO+、+ V(S0+ι) - V(St) regardless of YC to further control
the influence of variance. The critic V is not trained on the extra successor states {Si+Jt=ι,…,n .
4
Under review
1000-
Hopper
500-
Ant
2000
1500-
1000-
500
0	2M 0	2M
steps	steps
Figure 5: PPO-TD-Ex (γC = 1).
Figure 4: PPO-TD-Ex (γC = 0.99).
HumanoldStandup
50000
45000
40000
0
2M
steps
So the quality of the prediction ^(Si+ι) depends mainly on the generalization of v. Intuitively, if
V generalizes well, providing proper amount of transitions this way should improve or maintain the
overall performance as they help reduce variance. As shown by Figure 4, PPO-TD-Ex (γC = 0.99)
roughly follows this intuition. However, surprisingly, providing extra data to PPO-TD-Ex (γC = 1)
leads to a significant performance drop (Figure 5). This drop suggests that the larger variance from
the randomness of St+1 is not the only issue when using γC = 1 to train the critic. The quality of the
estimate V, at least in terms of making prediction on untrained states {Si+1}1,…,n, is lower when
YC = 1 is used than YC < 1. In other words, the generalization of V is poor when YC = 1. The curves
for PPO-TD-Ex (γC = 0.995) are a mixture of γC = 0.99 and γC = 1 and are provided in Figure 16
in the appendix.
In the undiscounted setting, we should theoretically have Rt+ι + V(St+ι) as the update target for
the critic. When YC < 1 is used instead, the update target becomes Rt+ι + YCV(St+ι) and the
variance resulting from the randomness of St+1 becomes less pronounced. So here, YC trades off
bias with variance, similar to that in Monte Carlo return bootstrapping targets in default PPO. We
refer to this effect of YC as variance control. However, YC can also affect the difficulty of learning
a good estimate V for vγC; we refer to this effect of YC as learnability control (Lehnert et al., 2018;
Laroche & van Seijen, 2018; Romoff et al., 2019). Inspired by the poor generalization of V when
YC = 1, we investigate learnability control mainly from the representation learning perspective. By
representation learning, we refer to learning the bottom layers (backbone) of a neural network. The
last layer of the neural network is then interpreted as a linear function approximator whose features
are the output of the backbone. This interpretation of representation learning is widely used in the
RL community, see e.g. Jaderberg et al. (2016); Chung et al. (2018); Veeriah et al. (2019).
Bias-representation trade-off: To separate variance control and learnability control, ideally we
should investigate the update target Rt+ι + YC,1V(St+1), where V is trained to approximate v∏c,2
and yc,2 < yc,i = 1. Learning an estimate V for VγC,2, however, implies to use the update target
Rt+ι + YC,2V(St+1): the two effects of Yc,2 then get mixed again. To solve this dilemma, we
consider the update target Rt+1 + VH-1(St+ι), where VH-I(St+1) is trained to approximate VH-1,
i.e., we use FHTD to train the critic in PPO, which we refer to as PPO-FHTD (Algorithm 4 in the
appendix). PPO-FHTD implements YC,1 = 1 directly, and manipulating H changes the horizon of
the policy evaluation problem, which is also one of the effects of manipulating YC,2 .
We test two parameterizations for PPO-FHTD to investigate representation learning. In the first
parameterization, to learn VπH, we parameterize {Vπi }i=1,...,H as H different heads over the same
representation layer (backbone). In the second parameterization, we always learn {Vπi }i=1,...,1024
as 1024 different heads over the same representation layer, whatever H we are interested in. To
approximate VπH, we then simply use the output of the H-th head. A diagram (Figure 13) in the
appendix further illustrates the difference between the two parameterizations.
Figure 6 shows that by tuning H for FHTD, PPO-FHTD with the first parameterization matches
or exceeds the performance of PPO-TD (YC < 1) in most tasks, and that the best H is always
5
Under review
2000
1500
1000
500
0
3000-
2000-
1000-
steps
HilmanOId
steps
—— PPO-TD y,c - ɪ
——PPO-IDye-0.995
——PPO-FHTD H-512
PPO-FHTD H-lβ24
100000-
BOOOO-
60000-
40000-
O	2M
steps
——PPO-TD Vc -1
——PPO-TDye-0.99
——PPO-FHTO H-β4
PPO-FHTD H-1024
HiImanO Id Standup
0
Figure 6: PPO-FHTD with the first parameterization. The best H and γC are used for each game.
Figure 7: PPO-FHTD with the second parameterization.
smaller than 1024. Theoretically, as long as we use an H ≥ Tmax = 1000, we always have
vπH (s) ≡ vπγ=1(s). Figure 6 shows that the performance of PPO-FHTD (H = 1024) is very close
to PPO-TD (γC = 1), indicating that learning {vπi }i=1,...,1023 is not an additional overhead for the
network in terms of learning vπH=1024, i.e., increasing H does not pose additional challenges in terms
of network capacity. However, Figure 7 suggests that for the second parameterization, H = 1024
is almost always among the best choices of H . Comparing Figures 6 and 7, we conclude that in the
tested domains, learning vπH with different H requires different representations. This suggests that
we can interpret the results in Figure 6 as a bias-representation trade-off. Using a larger H is less
biased but representation learning may become harder due to the longer policy evaluation horizon.
Consequently, an intermediate H achieves the best performance in Figure 6. As reducing H cannot
bring in advantages in representation learning under the second parameterization, the less biased H,
i.e., the larger H, usually performs better in Figure 7. Overall, YC optimizes a bias-representation
trade-off by changing the policy evaluation horizon H.
We further conjecture that representation learning may be harder for
a longer horizon because good representations can become rarer.
0-0-。- ……-。	We provide a simulated example to support this. Consider policy
evaluation on the simple Markov Reward Process (MRP) from Fig-
ure 8. We assume the reward for each transition is fixed and is
Figure 8: A simple MRP.	randomly generated in [0,1]. Let Xs ∈ RK be the feature vector
for a state s; We set its i-th component as xs[i] = tanh(ξ), where
ξ is a random variable uniformly distributed in [-2, -2]. We chose
this feature setup as we use tanh as the activation function in our PPO. We use X ∈ RN ×K to
denote the feature matrix. To create state aliasing (McCallum, 1997), which is common under func-
tion approximation, we first randomly split the N states into Si and S2 such that ∣Sι | = αΝ and
|S2| = (1 - α)N, where α is the proportion of states to be aliased. Then for every s ∈ S1, we
randomly select an s ∈ S2 and set Xs J χ^. Finally, we add Gaussian noise N(0,0.12) to each
element ofX. We use N = 100 and K = 30 in our simulation and report the normalized represen-
tation error (NRE) as a function of γ . For a feature matrix X , the NRE is computed analytically as
NRE(Y) = minw ∣l∣lXw-vγ ||2, where Vγ is the analytically computed true value function of the MRP.
We report the results in Figure 9, where each data point is averaged over 104 randomly generated
feature matrices (X) and reward functions. In this MRP, the average representation error becomes
larger as Y increases, which suggests that learning a good representation under a large Y and state
aliasing may be harder than with a smaller Y . We report the unnormalized representation error in
Figure 17 in the appendix, where the trend is much clearer.
Overall, though we do not claim that there is a monotonic relationship between the discount factor
and the difficulty of representation learning, our empirical study does suggest that representation
learning is a key factor at play in the misuse of the discounting in actor-critic algorithms, beyond
4The trend that NRE decreases as α increases is merely an artifact from how we generate vγ .
6
Under review
40.0% aliased	50.0% aliased	S0.0% aliased	70.0% aliased	80.0% aliased	90.0% aliased
I	Iol	，,"-' "' '	□•■ ■■■■ ,'^^^ 1B	Λ
Mll	IBI	■印■
i	I^l	∣n∣	^ι
H∣	廉需	..■♦	■照	^g!
I	国 I	^^flBI	・l	・l
**	*♦	Lβ W	«1	、*	**	*♦	Lβ W	«1	、*	**	**	Lβ W	2	∙Λ
Figure 9: Normalized representation error as a function of the discount factor. Shaded regions
indicate one standard derivation. 4
the widely recognized bias-variance trade-off. In the appendix, we provide additional experiments
involving distributional RL to further support the bias-variance-representation trade-off hypothe-
sis, under the assumption that the benefits of distributional RL comes mainly from the improved
representation learning (Bellemare et al., 2017; Munos, 2018; Petroski Such et al., 2019).
4	Optimizing the Discounted Objective (Scenario 2)
steps	steps	steps	steps	steps	steps
Figure 10: Comparison between PPO and DisPPO with γ = 0.995
When our goal is to optimize the discounted objective Jγ<1 (π), theoretically we should have the
γAt term in the actor update and use γC < 1. Practitioners, however, usually ignore this γAt (i.e.,
set γA = 1), introducing bias. Figure 10 shows that even if we use the discounted return as the
performance metric, the biased implementation of PPO still outperforms the theoretically grounded
implementation DisPPO in the domains we tested. Here PPO refers to the default PPO implemen-
tation where γA = 1, γC = γ < 1, and DisPPO (Alg. 6 in the appendix) adds the missing γAt term
in PPO by using γA = γC = γ < 1. We propose to interpret the empirical advantages of PPO over
DisPPO with Hypothesis 2. For all experiments in this section, we use γC = γ < 1.
An auxiliary task perspective: The biased policy update implementation of (2) ignoring γAt can be
decomposed into two parts as ∆t = γt∆t + (1 - Yt)∆t, where ∆t = q∏C(St, At)▽& log∏(At∣St).
We propose to interpret the difference term between the biased implementation (∆t) and the theoreti-
Cally grounded implementation (Yt∆t), i.e., the (1 -γt)qYC (St, At)▽& log ∏(At∣St) term, as the gra-
dient of an auxiliary objective with a dynamic weighting 1 - γt. Let Js,μ(∏) = Pa ∏(a∣s)qγ(s, a);
we have VθJs,μ(∏)∣μ=∏ = Ea〜∏(∙∣s)[qY(s, a)Vθ logπ(a∣s)]. This objective changes every time
step (through μ). Inspired by the decomposition, we augment PPO with this auxiliary task, yielding
AuxPPO (Algorithm 7 and Figure 13 in the appendix). In AuxPPO, we have two policies π and π0
parameterized by θ and θ0 respectively. The two policies are two heads over the same neural network
backbone, where π is used for interaction with the environment and π0 is the policy for the auxiliary
task. AuxPPO optimizes θ and θ0 simultaneously by considering the following joint loss
L(θ,θ0) = P∞=0 Yt mm n ⅛⅛S⅛AdVnθold (St, At), clip(⅛⅛⅛)AdVn：. (St,At)} 十
P∞=o(1 - Yt) mm n∏⅛ASAdVnK.(St,At),clip(∏⅛⅛⅛)AdVn：.("At)],
where St and At are obtained by executing θold. We additionally synchronize θ0 with θ periodically
to avoid an off-policy learning issue.
Flipped rewards: Besides AuxPPO, we also design novel environments with flipped rewards to
investigate Hypothesis 2. Recall we include the time step in the state, this allows us to simply create
a new environment by defining a new reward function r0(s, t) =. r(s)It≤t0 - r(s)It>t0 , where I
is the indicator function. During an episode, within the first t0 steps, this new environment is the
same as the original one. After t0 steps, the sign of the reward is flipped. We select t0 such that
7
Under review
EnaaJ pəlunoɔs-p
Ant y=。.95	.	.	Ant y= 0.93	.	. Anty= 0.9
6	2M 6	2M 6	2M
steps	steps	steps
HaIfCheetah γ= 0.95	HaIICheetah γ= 0.93	HaIfCheetah γ= 0.9
0	2M 0	2M 0	2M 0	2M 0	2M 6	2M
steps	steps	steps	steps	steps	steps
Figure 11: Curves without any marker are obtained in the original Ant / HalfCheetah.
Diamond-marked curves are obtained in Ant / HalfCheetah with r0. 5
γt0 is sufficiently small, e.g., we define t0 =. mint{γt < 0.05}. With this criterion for selecting t0,
the later transitions (i.e., transitions after t0 steps) have little influence on the evaluation objective,
the discounted return. Consequently, the later transitions affect the overall learning process mainly
through representation learning. DisPPO rarely makes use of the later transitions due to the γAt
term in the gradient update. AuxPPO makes use of the later transitions only through representation
learning. PPO exploits the later transitions for representation learning and the later transitions also
affect the control policy of PPO directly.
Results: When we consider the original environments, Figure 11 shows that in 8 out 12 tasks, PPO
outperforms DisPPO, even if the performance metric is the discounted episodic return. In all those
8 tasks, by using the difference term as an auxiliary task, AuxPPO is able to improve upon DisPPO.
In 6 out of those 8 tasks, AuxPPO is able to roughly match the performance of PPO at the end of
training. For γ ∈ {0.93, 0.9} in Ant, the improvement of AuxPPO is not clear and we conjecture
that this is because the learning of the π-head (the control head) in AuxPPO is much slower than the
learning of π in PPO due to the γCt term. Overall, this suggests that the benefit of PPO over DisPPO
comes mainly from representation learning.
When we consider the environments with flipped rewards, PPO is outperformed by DisPPO and
AuxPPO by a large margin in 11 out of 12 tasks. The transitions after t0 steps are not directly rele-
vant when the performance metric is the discounted return. However, learning on those transitions
may still improve representation learning provided that those transitions are similar to the earlier
transitions, which is the case in the original environments. PPO and AuxPPO, therefore, outperform
DisPPO. However, when those transitions are much different from the earlier transitions, which is
the case in the environments with flipped rewards, learning to control on them directly becomes dis-
tracting. PPO, therefore, is outperformed by DisPPO. Different from PPO, AuxPPO does not learn
to control on later transitions. Provided that the network has enough capacity, the control head πθ
in AuxPPO will not be affected much by the irrelevant transitions. The performance of AuxPPO is,
therefore, similar to DisPPO.
To summarize, Figure 11 suggests that using γA = 1 is simply an inductive bias that all transitions
are equally important. When this inductive bias is helpful for learning, γA = 1 implicitly imple-
ments auxiliary tasks thus improving representation learning and the overall performance. When this
inductive bias is detrimental, however, γA = 1 can lead to significant performance drops. AuxPPO
appears to be a safe choice that does not depend much on the correctness of this inductive bias.
5	Related Work
The mismatch in actor-critic algorithm implementations has been previously studied. Thomas
(2014) focuses on the natural policy gradient setting and shows that the biased implementation
ignoring γAt can be interpreted as the gradient of the average reward objective under a strong as-
sumption that the state distribution is independent of the policy. Nota & Thomas (2020) prove that
5See Section B.1 for more details about task selection.
8
Under review
without this strong assumption, the biased implementation is not the gradient of any stationary ob-
jective. This does not contradict our auxiliary task perspective as our objective Js,μ(∏) changes at
every time step. Nota & Thomas (2020) further provide a counterexample showing that following
the biased gradient can lead to a policy of poor performance w.r.t. both discounted and undis-
counted objectives. Both Thomas (2014) and Nota & Thomas (2020), however, focus on theoretical
disadvantages of the biased gradient and regard ignoring γAt as the source of the bias. We instead
regard the introduction of γC < 1 in the critic as the source of the bias in the undiscounted setting
and investigate its empirical advantages, which are more relevant to practitioners. Moreover, our
representation learning perspective for investigating this mismatch is to our knowledge novel.
Although we propose the bias-variance-representation trade-off, we do not claim that is all that γ
affects. The discount factor also has many other effects (e.g., Sutton (1995); Jiang et al. (2016);
Laroche et al. (2017); Laroche & van Seijen (2018); Lehnert et al. (2018); Fedus et al. (2019);
Van Seijen et al. (2019); Amit et al. (2020)), which we leave for future work. In Scenario 1, using
γC < 1 helps reduce the variance. Variance reduction in RL itself is an active research area (see,
e.g., Papini et al. (2018); Xu et al. (2019); Yuan et al. (2020)). Investigating those variance reduction
techniques with γC = 1 is a possibility for future work. Recently, Bengio et al. (2020) study the
effect of the bootstrapping parameter λ in TD(λ) in generalization. Our work studies the effect of
the discount factor γ in representation learning in the context of the misuse of the discounting in
actor-critic algorithms, sharing a similar spirit of Bengio et al. (2020).
6 Conclusion
In this paper, we investigate the longstanding mismatch between theorists and practitioners in actor-
critic algorithms from a representation learning perspective. Although the theoretical understanding
of policy gradient algorithms have recently been significantly advanced (Agarwal et al., 2019; Wu
et al., 2020), this mismatch has drawn little attention. We hope our empirical study can help prac-
titioners understand actor-critic algorithms better and therefore design more efficient actor-critic
algorithms in the setting of deep RL, where representation learning emerges as a major considera-
tion. We hope our empirical study can draw more attention to the mismatch, which could enable the
community to finally close this longstanding gap.
References
Joshua Achiam. Spinning up in deep reinforcement learning. 2018.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261,
2019.
Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as a regularizer in reinforcement learning.
arXiv preprint arXiv:2007.02040, 2020.
Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael
Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What
matters in on-policy reinforcement learning? a large-scale empirical study. arXiv preprint
arXiv:2006.05990, 2020.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
jun 2013.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. arXiv preprint arXiv:1707.06887, 2017.
Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal
difference learning. arXiv preprint arXiv:2003.06350, 2020.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific Bel-
mont, MA, 1996.
9
Under review
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Itai Caspi, Gal Leibovich, Gal Novik, and Shadi Endrawis. Reinforcement learning coach, Decem-
ber 2017. URL https://doi.org/10.5281/zenodo.1134899.
Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White. Two-timescale networks for nonlinear
value function approximation. In International Conference on Learning Representations, 2018.
Kristopher De Asis, Alan Chan, Silviu Pitis, Richard S Sutton, and Daniel Graves. Fixed-horizon
temporal difference methods for stable reinforcement learning. arXiv preprint arXiv:1909.03906,
2019.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. Implementation matters in deep rl: A case study on ppo and
trpo. In International Conference on Learning Representations, 2019.
William Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. Hyper-
bolic discounting and learning over multiple horizons. arXiv preprint arXiv:1902.06865, 2019.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.
Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. A closer look at deep policy gradients. arXiv preprint
arXiv:1811.02553, 2018.
Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David
Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.
Nan Jiang, Satinder P Singh, and Ambuj Tewari. On structural properties of mdps that bound loss
due to shallow planning. In IJCAI, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Vijay R Konda. Actor-critic algorithms. PhD thesis, Massachusetts Institute of Technology, 2002.
Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.
Romain Laroche and Harm van Seijen. In reinforcement learning, all objective functions are not
equal. 2018.
Romain Laroche, Mehdi Fatemi, Joshua Romoff, and Harm van Seijen. Multi-advisor reinforcement
learning. arXiv preprint arXiv:1704.00756, 2017.
Lucas Lehnert, Romain Laroche, and Harm van Seijen. On value function representation of long
horizon problems. In AAAI Conference on Artificial Intelligence, 2018.
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph Gon-
zalez, Michael Jordan, and Ion Stoica. Rllib: Abstractions for distributed reinforcement learning.
In International Conference on Machine Learning, 2018.
10
Under review
R McCallum. Reinforcement learning with selective perception and hidden state. PhD thesis, 1997.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning, 2016.
Remi Munos. Distributional reinforcement learning. Invited talk at European Work-
shop on Reinforcement Learning https://ewrl.files.wordpress.com/2018/10/
distributional_rl.pdf, 2018.
Chris Nota and Philip S. Thomas. Is the policy gradient a gradient? In Proceedings of the 19th
International Conference on Autonomous Agents and Multiagent Systems, 2020.
OpenAI. Openai five. https://openai.com/five/, 2018.
Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.
Stochastic variance-reduced policy gradient. arXiv preprint arXiv:1806.05618, 2018.
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement
learning. In International Conference on Machine Learning, 2018.
Felipe Petroski Such, Vashisht Madhavan, Rosanne Liu, Rui Wang, Pablo Samuel Castro, Yulun
Li, Jiale Zhi, Ludwig Schubert, Marc G. Bellemare, Jeff Clune, and et al. An atari model zoo
for analyzing, visualizing, and comparing deep reinforcement learning agents. Proceedings of
the Twenty-Eighth International Joint Conference on Artificial Intelligence, Aug 2019. doi: 10.
24963/ijcai.2019/452. URL http://dx.doi.org/10.24963/ijcai.2019/452.
Joshua Romoff, Peter Henderson, Ahmed Touati, Emma Brunskill, Joelle Pineau, and Yann Ollivier.
Separating value functions across time-scales. arXiv preprint arXiv:1902.01883, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning,
2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 2016.
Adam Stooke and Pieter Abbeel. rlpyt: A research code base for deep reinforcement learning in
pytorch. arXiv preprint arXiv:1909.01500, 2019.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning,
1988.
Richard S Sutton. Td models: Modeling the world at a mixture of time scales. In Machine Learning
Proceedings 1995. Elsevier, 1995.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction (2nd Edition).
MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems, 2000.
Philip Thomas. Bias in natural actor-critic algorithms. In Proceedings of the 31st International
Conference on Machine Learning, 2014.
11
Under review
Harm Van Seijen, Mehdi Fatemi, and Arash Tavakoli. Using a logarithmic mapping to enable
lower discount factors in reinforcement learning. In Advances in Neural Information Processing
Systems, 2019.
Vivek Veeriah, Matteo Hessel, Zhongwen Xu, Janarthanan Rajendran, Richard L Lewis, Junhyuk
Oh, Hado P van Hasselt, David Silver, and Satinder Singh. Discovery of useful questions as
auxiliary tasks. In Advances in Neural Information Processing Systems, 2019.
Arthur F Veinott. Discrete dynamic programming with sensitive discount optimality criteria. The
Annals of Mathematical Statistics, 1969.
Martin L Weitzman. Gamma discounting. American Economic Review, 2001.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 1992.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite time analysis of two time-scale actor
critic methods. arXiv preprint arXiv:2005.01350, 2020.
Pan Xu, Felicia Gao, and Quanquan Gu. Sample efficient policy gradient methods with recursive
variance reduction. arXiv preprint arXiv:1909.08610, 2019.
Huizhuo Yuan, Xiangru Lian, Ji Liu, and Yuren Zhou. Stochastic recursive momentum for policy
gradient methods. arXiv preprint arXiv:2003.04302, 2020.
Shangtong Zhang. Modularized implementation of deep rl algorithms in pytorch. https://
github.com/ShangtongZhang/DeepRL, 2018.
12
Under review
A Proof of Lemma 2
Proof. The proof is based on Appendix B in Schulman et al. (2015a), where perturbation theory
is used to prove the performance improvement bound (Lemma 1). To simplify notation, we use a
vector and a function interchangeably, i.e., We also use r and μo to denote the reward vector and
the initial distribution vector. J(π) and dπ (s) are shorthand for Jγ(π) and dγπ(s) with γ = 1. All
vectors are column vectors.
Let S+ be the set of states excluding s∞, i.e., S+ = S/{s∞}, we define Pn ∈ RlS+l×lS+l such
that Pn(s,s0) = Pa π(a∣s)p(s0∣s, a). Let G = P∞=o Pn. According to standard Markov chain
theories, G(s, s0) is the expected number of times that s0is visited before s∞ is hit given S0 = s.
Tmax < ∞ implies that G is well-defined and we have G = (I - Pn)-1. Moreover, Tmax < ∞
also implies ∀s, Pso G(s, s0) ≤ Tmax, i.e., ∣∣G∣∣∞ ≤ Tmax. We have J(π) = μ>Gr.
Let G0=. (I - Pn0)-1, we have
J(∏) - J(π) = μ>(G - G)r.
Let ∆ =. Pn0 - Pn , we have
G0-1 - G-1 = -∆,
Left multiply by G0 and right multiply by G,
G - G0= -G0∆G,
G0= G+G0∆G (Expanding G0 in RHS recursively)
= G + G∆G + G0∆G∆G.
So we have
J(π0) - J(π) = μ>G∆Gr + μ>G0∆G∆Gr.
It is easy to see μ> G = d> and Gr = Vn. So
μ>G∆Gr = d>∆vn
=Xd∏(s) X (Xπ'(als)p(s1s,a) - Xn(a|s)p(SIS,a))v∏(s')
s	s0 a
a
E d∏(s) E(∏0(a∣s) — ∏(a∣s)) Ep(Sls, a)v∏(s0)
sa
s0
X d∏(s) X(∏0(a∣s) — ∏(a∣s))卜(S) + Xp(s0∣s, a)v∏(s0) - v∏(s))
sa
s0
(Ea(π0(α∣s) - π(a∣s))f (s) = 0 holds for any f that dependes only on S)
ɪ2 d∏(s) ɪ2 ∏0(a∣S)Adv∏(s, a).
(Pa ∏(a∣S)Adv∏(s, a) = 0 by Bellman equation)
We now bound μ>G0∆G∆Gr. First,
|。Gr)(S)I = | X (X n0(a|S) - n(a|S))p(Sls, Gvn (S')|
s0	a
=| X (∏0(a∣S) - π(a∣S)) (r(S) + Xp(s0∣s, a)vn(s0) - v∏(s
a
s0
=| X b'(a∣S) - ∏(a∣S)) Adv∏(S,a)∣
a
≤ 2maxDTV(∏0(∙∣s), ∏(∙∣s)) max ∣Adv∏(s, a)|,
s	s,a
13
Under review
where DT V is the total variation distance. So
∣∣∆Gr∣∣∞ ≤ 2maxDTV(∏0(∙∣s), ∏(∙∣s)) max ∣Adv∏(s, a)|.
s	s,a
Moreover, for any vector x,
∣(∆x)(s)∣ ≤ 2maxDTV(∏0(∙∣s),∏(∙∣s))∣∣x∣∣∞,
s
∣∣∆x∣∣∞ ≤ 2maxDTV(∏0(∙∣s),∏(∙∣s))∣∣x∣∣∞.
s
So
∣∣∆∣∣∞ ≤ 2maxDTV(∏0(∙∣s),∏(∙∣s)),
s
∣μ>G0∆G∆Gr∣ ≤ ∣∣μ>∣∣ι∣∣G0∣∣∞∣∣∆∣∣∞∣∣G∣∣∞∣∣∆Gr∣∣∞
≤	4T1Lx maχ DTV (n(|S),n(・|S))max ∣Adv∏ (s, a)|
s	s,a
≤	4T1ax max DKL(W1S)IIn(IS)) max ∣Adv∏(s, a)∣,
s	s,a
which completes the proof.	口
Note this perturbation-based proof of Lemma 2 holds only for r : S → R. For r : S × A → R,
we can turn to the coupling-based proof as Schulman et al. (2015a), which, however, complicates
the presentation and deviates from the main purpose of this paper. We, therefore, leave it for future
work.
B	Experiment Details
B.1	Methodology
We use HalfCheetah, Walker, Hopper, Ant, Humanoid, and HumanoidStandup as our
benchmarks. We exclude other tasks as we find PPO plateaus quickly there. The tasks we con-
sider have a hard time limit of 1000. Following Pardo et al. (2018), we add time step information
into the state, i.e., there is an additional scalar t/1000 in the observation vector. Following Achiam
(2018), we estimate the KL divergence between the current policy θ and the sampling policy θold
when optimizing the loss (3). When the estimated KL divergence is greater than a threshold, we
stop updating the actor and update only the critic with current data. We use Adam (Kingma & Ba,
2014) as the optimizer and perform grid search for the initial learning rates of Adam optimizers.
Let αA and αC =. βαA be the learning rates for the actor and critic respectively. For each algo-
rithmic configuration (i.e., a curve in a figure), We tune αA ∈ {0.125,0.25,0.5,1, 2} × 3 ∙ 10-4
and β ∈ {1, 3} with grid search in Ant with 3 independent runs maximizing the average re-
turn of the last 100 training episodes. In particular, αA = 3 ∙ 10-4 and β = 3 is roughly
the default learning rates for the PPO implementation in Achiam (2018). We then run this al-
gorithmic configuration with the best αA and αC in all tasks. Overall, we find after remov-
ing GAE, smaller learning rates are preferred. When we use FHTD, we additionally consider
H ∈ {16, 32, 64, 128, 256, 512, 1024} in the grid search. When we use C51, we additionally con-
sider V1ax ∈ {20, 40, 80, 160, 320, 640, 1280, 2560, 5120, 10240, 81920, 163840, 327680} in the
grid search. We use PPO-TD with γC = 0.99 as an example to study how the best hyperparameter
configuration in Ant transfers to other games. As shown in Figure 12, the best learning rates of
Ant (αA = 3 ∙ 10-4 and β = 3) yields reasonably good performance in all the other games except
Humanoid. In the paper, we do not draw a conclusion from a single task. So an outlier is unlikely
to affect the overall conclusion.
In the discounted setting, we consider only Ant, HalfCheetah and their variants. For
Walker2d, Hopper, and Humanoid, we find the average episode length of all algorithms are
smaller than t0, i.e., the flipped reward rarely takes effects. For HumanoidStandup, the scale of
the reward is too large. To summarize, other four environments are not well-suited for the purpose
of our empirical study. Moreover, in the discounted setting, we performed the grid search of the
learning rates for both Ant and HalfCheetah.
14
Under review
u」ma」pəaunou-pun
HaIfCheetah
3000
2000
1000
0
0	2M
steps
——(0.125, 1)
-(0.125, 3)
-(0.25, 1)
(0.25, 3)
-(0.5, 1)
(0.5, 3)
——(1, D
——(1,3)
(2,1)
-(2, 3)
2000
1500
1000
500
0
0	2M
steps
Humanoid
Hopper
2500
2000
1500
1000
500
0
0	2M
steps
HumanoidStandup
100000
80000
60000
40000
0	2M
steps
Figure 12: PPO-TD (γC = 0.99) with different learning rates. A curve labeled with (x, β) cor-
responds to an initial learning rate for the actor and critic of αA = x × 3 ∙ 10-4 and αC = βαA
respectively. The best learning rates for Ant 历=3 ∙ 10-4 and β = 3) yields reasonably good
performance in all the other games except Humanoid.
B.2	Algorithm Details
The pseudocode of all implemented algorithms are provide in Algorithms 1 - 7 with their architec-
tures illustrated in Figure 13. For hyperparameters that are not included in the grid search, we use
the same value as Dhariwal et al. (2017); Achiam (2018). In particular, for the rollout length, we set
K = 2048. For the optimization epochs, we set Kopt = 320. For the minibatch size, we set B = 64.
For the maximum KL divergence, We set KLtarget = 0.01. We clip ∏∏θ釜∣S) into [-0.2,0.2]. We
use Ns = 51 supports for PPO-C51.	o
We use tWo-hidden-layer neural netWorks for function approximation. Each hidden layer has 64
hidden units and a tanh activation function. The output layer of the actor netWork has a tanh acti-
vation function and is interpreted as the mean of an isotropic Gaussian distribution, Whose standard
derivation is a global state-independent variable as suggested by Schulman et al. (2015a).
15
Under review
Algorithm 1: PPO
Input:
θ, ψ: parameters of ∏, V
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rollout length, number of optimization epochs, and minibatch size
KLtarget: maximum KL divergence threshold
So 〜μo
while True do
Initialize a buffer M
θold J θ
for i = 0, . . . , K - 1 do
Ai 〜∏θoid(∙∣Si)
Execute Ai, get Ri+1, Si+1
if Si+1 is a terminal state then
I mi J 0, Si+1 〜μo
else
I mi J 1
end
end
GK J V(SK)
for i = K - 1, . . . , 0 do
Gi J Ri+1 + γCmiGi+1
Advi J Ri+ι + YCmiVψ(Si+ι) — Vψ(Si)
Store (Si, Ai, Gi, Advi) in M
end
Normalize Advi in M as Advi J
AdVi—mean ({ Advi })
std({AdVi})
for o = 1, . . . , Kopt do
Sample a minibatch {(Si, Ai, Gi, AdVi)}i=1,...,B from M
L(ψ) J 2B PB=ι(Vψ(Si) — Gi)2 /* No gradient through Gi
L(θ) J ɪ PB m min{ π"Ai啕、Ad%, clip( π"Ai%、)Ad%}
' B Bj=* I	πθθdld (AiISi)	i, Y'πθold (AiISiy	ij
Perform one gradient update to ψ minimizing L(ψ) with Adam
if BB PB=I log∏θdid(Ai∣Si) — log∏θ(Ai∣Si) < KLtarget then
I Perform one gradient update to θ maximizing L(θ) with Adam
end
*/
end
end
16
Under review
Algorithm 2: PPO-TD
Input:
θ, ψ: parameters of ∏, V
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rollout length, number of optimization epochs, and minibatch size
KLtarget: maximum KL divergence threshold
So 〜μo
while True do
Initialize a buffer M
θold — θ
for i = 0, . . . , K - 1 do
Ai 〜∏θoid(∙∣Si)
Execute Ai, get Ri+1, Si+1
if Si+1 is a terminal state then
I mi - 0, Si+1 〜μo
else
I mi - 1
end
end
for i = K - 1, . . . , 0 do
AdVi - Ri+1 + Ycmivψ (Si+1) - vψ (Si)
Si — Si+1, ri - Ri+1
Store (Si, Ai, mi, ri, Si0, AdVi) in M
end
Normalize AdVi in M as AdVi J
AdVi - mean ({AdVi })
std({AdVi})
for o = 1, . . . , Kopt do
Sample a minibatch {(Si, Ai, mi, ri, Si0, AdVi)}i=1,...,B from M
yi J ri + YcmiVψ (Si)
L(ψ) J 2B PB=ι(Vψ(Si) — yi)* I 2 * * S * * /* No gradient through yi
L(θ) J ɪ PB m min{ πθ(A∖Sij、Ad%, clip( πθ(Ai%、)Ad%}
b i=1	iπθ0id(AiISi)	i, H'πθ0id(AiISiy	ij
Perform one gradient update to ψ minimizing L(ψ) with Adam
if Bb PB=I log∏θoid (Ai∣Si) - log∏θ(AiISi) < KLtarget then
I Perform one gradient update to θ maximizing L(θ) with Adam
end
*/
end
end
17
Under review
Algorithm 3: PPO-TD-Ex
Input:
θ, ψ: parameters of ∏, V
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rollout length, number of optimization epochs, and minibatch size
KLtarget: maximum KL divergence threshold
N : number of extra transitions
p, r: transition kernel and reward function of the oracle
So 〜μo
while True do
Initialize a buffer M
θold — θ
for i = 0, . . . , K - 1 do
for j = 0, . . . , N do
Aj 〜∏θoid(∙∖Si),Rj+ι 一 r(Si,Aj),Sj+ι ~P(1S,Aj)
if Sij+1 is a terminal state then
I mj - 0, Sj+ι 〜μo
else
I mj J 1
end
end
Si+1 J Si0+1
end
for i = K - 1, . . . , 0 do
AdVi - R0+1 + Ycm0vψ(S0+1)- vψ(SO)
for j = 0, . . . , N do
I SOjJ Sj+1
end
Store ({Sij, Aij, mij,rij, Si0j}j=0,...,N,Advi) in M
end
Normalize Advi in M as Advi J AdVi-mAd{，，")
for o = 1, . . . , Kopt do
Sample a minibatch {({Sij, Aij, mij, rij, Si0j}j=0,...,N,Advi)}i=1,...,B from M
y J n+i PN=orj + YCmjvψ (Sj)
L(ψ) J 2B PB=1(Vψ(S0) — yi)2 /* No gradient through y
L⑻ J B P* IB=I min{ ∏∏0ldAA∣Ss0) Advi, clip( π∏⅛AAS⅛ )Advi}
Perform one gradient update to ψ minimizing L(ψ) with Adam
if B PB=1 log πθold (AO ∖S0) - log πθ(A0∖SO) < KLtarget then
I Perform one gradient update to θ maximizing L(θ) with Adam
end
end
end
*/
18
Under review
Algorithm 4: PPO-FHTD
Input:
θ,ψ: parameters of ∏, {Vj}j=ι,…,h
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rollout length, number of optimization epochs, and minibatch size
KLtarget: maximum KL divergence threshold
So ~ μo
while True do
Initialize a buffer M
θold — θ
for i = 0, . . . , K - 1 do
Ai ~ ∏θoid(∙∣Si)
Execute Ai, get Ri+1, Si+1
if Si+1 is a terminal state then
I mi J 0, Si+1 ~ μo
else
I mi J 1
end
end
for i = K - 1, . . . , 0 do
Advi J Ri+1 + mivH (Si+1)- vH (Si)
Si0 J Si+1, ri J Ri+1
Store (Si, Ai, mi, ri, Si0, Advi) in M
end
Normalize Advi in M as Advi J
AdVi—mean ({Advi })
std({AdVi})
for o = 1, . . . , Kopt do
Sample a minibatch {(Si, Ai, mi, ri, Si0, AdVi)}i=1,...,B
for j = 1, . . . , H do
I yj J ri + mivψ-1(Si))	/* v0(Si) ≡ 0
end
from M
L(ψ) J 2B PB=ι P* lH=I(Vψ(Si) - yj)2 /* NO gradient through
L(θ) J B P3 min{Π⅛A⅛⅛AdVi, clip(∏∏⅛⅛⅛)AdV,}
Perform one gradient update to ψ minimizing L(ψ) with Adam
if BB PB=I log ∏θoid (AiISi ) - log ∏θ (AiISi) < KLtarget 皿口
I Perform one gradient update to θ maximizing L(θ) With Adam
end
yij
*/
*/
end
end
19
Under review
Algorithm 5: PPO-C51
Input:
θ, ψ: parameters of ∏, {Vj}j=ι,…,ns With Ns being the number of supports and Vj being the
probability of each support
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rollout length, number of optimization epochs, and minibatch size
KLtarget: maximum KL divergence threshold
∆z = Nm-ax ,{zj = -VmaX +(j - 1)∆z : j = 1,...,Ns} // Define the supports
So 〜μo
while True do
Initialize a buffer M
θold — θ
for i = 0, . . . , K - 1 do
Ai ~ ∏θoid(∙∣Si)
Execute Ai, get Ri+1, Si+1
if Si+1 is a terminal state then
I mi J 0, Si+1 ~ μo
else
I mi J 1
end
end
for i = K - 1, . . . , 0 do
AdVi J Ri+1 + miYC PN=I vψ (Si+1)zj - PIN=I vψ (Si)Zj
Si0 J Si+1, ri J Ri+1
Store (Si, Ai, mi, ri, Si0, Advi) in M
end
Normalize Advi in M as Advi J
AdVi—mean ({AdVi })
std({AdVi})
for o = 1, . . . , Kopt do
Sample a minibatch {(Si, Ai, mi, ri, Si0, AdVi)}i=1,...,B
for i = 1, . . . , B do
for j = 1, . . . , Ns do
I Zj J ri + mi YC Zj
end
from M
end
for j = 1, . . . , Ns do
I yj j PN= ι[1 - l[ZjLW：X-Zj|]0vψ(Si) /* [x]U = min(maχ(χ,l),u)
end
L(ψ) J Bb Pi=I PN=I -yj log Vψ(Si) /* No gradient through yj
L(θ) J ɪ PBl min{ πθ(Ai啕、AdVi, clip( πθ(Ai%、)AdVi}
B Bj=I	πθθdld (AiISi)	i, Y'πθold (AiISiy	ij
Perform one gradient update to ψ minimizing L(ψ) With Adam
if B1 PB=I log∏θdid(Ai∣Si) - log∏θ(Ai∣Si) < KLtarget then
I Perform one gradient update to θ maximizing L(θ) with Adam
end
*/
*/
end
end
20
Under review
Algorithm 6: DisPPO
Input:
θ, ψ: parameters of ∏, V
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rollout length, number of optimization epochs, and minibatch size
KLtarget: maximum KL divergence threshold
So 〜μo,t — 0
while True do
Initialize a buffer M
θold J θ
for i = 0, . . . , K - 1 do
Ai 〜∏θold(ISi),ti — t
Execute Ai, get Ri+1, Si+1
if Si+1 is a terminal state then
m mi J 0, Si+1 〜μ0,t J 0
else
I m>i V_ 1, t V— t + 1
end
end
GK J V(SK)
for i = K - 1, . . . , 0 do
Gi J Ri+1 + γCmiGi+1
Advi J Ri+1 + YCmiVψ(Si+ι) — Vψ(Si)
Store (Si, Ai, Gi, Advi, ti) in M
end
Normalize Advi in M as Advi J
AdVi—mean ({ Advi })
std({AdVi})
for o = 1, . . . , Kopt do
Sample a minibatch {(Si, Ai, Gi, AdVi, ti)}i=1,...,B from M
L(ψ) J 2B PB=ι(Vψ(Si) — Gi)2 /* No gradient through Gi
L(θ) J B P3 YAi mm{Π⅛(A¾AdVi, clip(∏∏⅛⅛⅛)AdVJ
Perform one gradient update to ψ minimizing L(ψ) with Adam
if B PB=I log ∏θoid (AiISi ) — log ∏θ (AiISi) < KLtarget 皿n
I Perform one gradient update to θ maximizing L(θ) with Adam
end
*/
end
end
21
Under review
Algorithm 7: AUxPPO
Input:
θ, θ0, ψ: parameters of ∏, ∏0, V
αA, αC: Initial learning rates of the Adam optimizers for θ, ψ
K, Kopt, B: rolloUt length, nUmber of optimization epochs, and minibatch size
KLtarget: maximUm KL divergence threshold
So ~ μo,t — 0
while True do
Initialize a bUffer M
θold — θ,θ0 ― θ
for i = 0, . . . , K - 1 do
Ai 〜πθold (1Si),ti J t
ExecUte Ai, get Ri+1, Si+1
if Si+1 is a terminal state then
I mi - 0, Si+1 〜μo,t — 0
else
I mi V_ 1, t V— t + 1
end
end
GK J V(SK)
for i = K - 1, . . . , 0 do
Gi V Ri+1 + γCmiGi+1
Advi - Ri+ι + YCmiV ψ (Si+ι) — Vψ (Si)
Store (Si, Ai, Gi, Advi, ti) in M
end
Normalize Advi in M as Advi V
AdVi—mean ({Advi })
std({AdVi})
for o = 1, . . . , Kopt do
Sample a minibatch {(Si, Ai, Gi, AdVi, ti)}i=1,...,B from M
L(ψ) J 2B EH=I(Vψ(Si) — Gi)2 /* No gradient through Gi
*/
L(θ,θ0) J B Ps min{ ∏∏0ldAASSi) AdVi, CliP(⅛A⅛)Ad%}+
(I-YC* i )mm{ π∏O⅛A≡⅛ AdVi，cliP( π∏θOT⅛)AdV，}
Perform one gradient UPdate to ψ minimizing L(ψ) with Adam
if B PB=I log ∏θoid (Ai∣Si) — log ∏θ(AiISi) < KLtarget 皿n
I Perform one gradient update to θ, θ0 maximizing L(θ, θ0) with Adam
end
end
end
22
Under review
(a) Architecture of PPO, PPO-
TD, PPO-TD-Ex, DisPPO
(b) The first parameterization of PPO-FHTD
(c) The second parameterization of PPO-FHTD
(d) Architecture of PPO-C51
(e) Architecture of AuxPPO
Figure 13: Architectures of the algorithms
C	Additional Experimental Results
C.1 Distributional RL
Hypothesis 1 and the previous empirical study suggest that representation learning may be the main
bottleneck of PPO-TD (γC = 1). To further support this, we benchmark PPO-C51 (γC = 1) (Al-
gorithm 5 in the appendix), where the critic of PPO is trained with C51. C51 is usually considered
to improve representation learning by implicitly providing auxiliary tasks (Bellemare et al., 2017;
Munos, 2018; Petroski Such et al., 2019). Figure 14 shows that training the critic with C51 in-
deed leads to a performance improvement and PPO-C51 (γC = 1) sometimes outperforms PPO-TD
(γC < 1) by a large margin. Figure 15 further shows that when Vmax is optimized for PPO-C51, the
benefit for using γC < 1 in PPO-C51 is less pronounced than that in PPO-TD, indicating the role of
γC < 1 and distributional learning may overlap. Figures 6, 7, & 9, suggest that the overlapping is
representation learning.
C.2 Other Complementary Results
Figure 16 shows how PPO-TD-Ex (γC = 0.995) reacts to the increase of N. Figure 17 shows the
unnormalized representation error in the MRP experiment. Figure 18 shows the average episode
length for the Ant environment in the discounted setting. For HalfCheetah, it is always 1000.
D Larger Version of Figures
23
Under review
HaIfCheetah
u」ma」pəaunou-pun
Walker2d	Hopper
steps
Humanoid	HumanoidStandup
u」ma」pəaunouS-PUn
Figure 14: For PPO-C51, we set γC = 1.
Figure 15: For each game, Vmax is the same as the Vmax in Figure 14.
24
Under review
u」rqa」pφφjunOUS-PUn
steps
Walker2d
2000
1500
1500
1000
1000
500
500
steps
Hopper
Figure 16: PPO-TD-Ex (γC = 0.995).
37500
0
50000
47500
45000
42500
40000
steps
HumanoidStandup
2M
steps

40.0% aliased
，，
ISO
RE
UO
*
*，
U U
50.0% aliased
60.0% aliased
70.0% aliased
80.0% aliased
90.0% aliased
*
uβ
US
IM
n
a
x
14 U
14 U
r.
1»
U*
a
*
a
c
l*
ιas
IH
K
V
B
we
IM
ue
IB,
IH
■
*
«
<







Figure 17: Unnormalized representation error (RE) as a function of the discount factor.
regions indicate one standard derivation. RE is computed analytically as RE(X, γ) =. minw
vγ ||2
Shaded
||Xw -
25
Under review
Ant y= 0.995
1000
O
O
2
Ooo
Ooo
8 6 4
əpodə
Ant γ= 0.99
1000
Oooo
Oooo
8 6 4 2
1000
Ant y= 0.97
Oooo
Oooo
8 6 4 2
2M
u」ma」pəaunouS-PUn
2M 0
2M
Figure 18: Curves without any marker are obtained in the original Ant. Diamond-marked curves
are obtained in Ant with r0 .
2000
1500
1000
500
0
0	2M
steps
1250
1000
750
500
250
0	2M
steps
1000
800
600
400
200
steps
Ant y= 0.9
2M
steps
2500
2000
1500
1000
500
0
120000
Hopper
2M
steps
HumanoidStandup
100000
80000
60000
40000
0	2M
steps
Figure 19: The default PPO implementation with different discount factors. The larger version of
Figure 1.
26
Under review
u」ma」pəaunou-pun
HaIfCheetah
1000
500
0
——PPOyc = I
——PPO-TD Yc= 1
ʃʃ
1250
1000
750
500
250
0
2M
1500
1000
500
0
Hopper
0
0
2M
steps
0
steps
50000
45000
40000
steps
HumanoidStandup
0
steps
Figure 20: Comparison between PPO and PPO-TD when γC = 1. The larger version of Figure 2.
HaIfCheetah
2000
pəaunouS-PUn
1000
0
——PPO-TD Yc=I
—— PPO-TDyc =0.995
—— PPO-TD yc = 0.99
PPO-TD γc = 0.97
-PPO-TD γc = 0.95
2500
2000
1500
1000
500
0
0	2M
steps
Walker2d
Hopper
1500
1000
500
0
0	2M
steps
HumanoidStandup
90000
80000
70000
60000
50000
40000
0	2M
steps
Figure 21: PPO-TD with different discount factors. The larger version of Figure 3.
27
Under review
undiscounted return	undiscounted return
HaIfCheetah
steps
1500τ
1000
500
0
Hopper
Figure 22: PPO-TD-Ex (γC = 0.99). The larger version of Figure 4.
0	2M
steps
Walker2d
Figure 23: PPO-TD-Ex (γC = 1). The larger version of Figure 5.
1250
1000
750
500
250
0
0	2M
steps
HumanoidStandup
50000
45000
40000
0	2M
steps
28
Under review
pəaunouS-PUn
HaIfCheetah
2000
IOOO
O
O
2M
Walker2d
steps
steps
Humanoid
3000
2000
IOOO
O
O	2M
steps
2000
1500
IOOO
500
O
IOOOOO
80000
60000
40000
Hopper
2M
steps
HumanoidStandup
2M
steps
Figure 24: PPO-FHTD with the first parameterization. The best H and γC are used for each game.
The larger version of Figure 6.
HaIfCheetah
u」ma」pəaunou-pun
IOOO
500
0
1250
1000
——PPO-F ∣-ΓTD H=16
——PPO-FI-TTD H=32
——PPO-FHTD H=64
PPO-FI-TTD H=128
FjPO-FHTD H=256
PPO-FHTD H=512
——PPO-F FΓTD H=1024
750
500
250
Walker2d
0	2M
1500
1000
500
Hopper
0
55000
50000
45000
40000
Figure 25: PPO-FHTD with the second parameterization. The larger version of Figure 7.
2M
steps
HumanoidStandup
2M
steps
29
Under review
HaIfCheetah
steps
Hopper
steps
Humanoid
HumanoidStandup
Figure 26:	Comparison between PPO and DisPPO with γ = 0.995. The larger version of Figure 10.
30
Under review
discounted return	discounted return
0	2M
steps
HaIfCheetah γ= 0.97
steps
steps
0	2M
steps
HaIfCheetah γ= 0.9
0	2M
steps
Figure 27:	Curves without any marker are obtained in the original Ant environment. Diamond-
marked curves are obtained in Ant with r0. The larger version of Figure 11.
31