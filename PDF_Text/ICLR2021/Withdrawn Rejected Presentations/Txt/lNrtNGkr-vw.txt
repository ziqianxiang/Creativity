Under review as a conference paper at ICLR 2021
Linear Representation Meta-Reinforcement
Learning for Instant Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
This paper introduces Fast Linearized Adaptive Policy (FLAP), a new meta-
reinforcement learning (meta-RL) method that is able to extrapolate well to out-
of-distribution tasks without the need to reuse data from training, and adapt al-
most instantaneously with the need of only a few samples during testing. FLAP
builds upon the idea of learning a shared linear representation of the policy so
that when adapting to a new task, it suffices to predict a set of linear weights.
A separate adapter network is trained simultaneously with the policy such that
during adaptation, we can directly use the adapter network to predict these linear
weights instead of updating a meta-policy via gradient descent, such as in prior
meta-RL methods like MAML, to obtain the new policy. The application of the
separate feed-forward network not only speeds up the adaptation run-time signifi-
cantly, but also generalizes extremely well to very different tasks that prior Meta-
RL methods fail to generalize to. Experiments on standard continuous-control
meta-RL benchmarks show FLAP presenting significantly stronger performance
on out-of-distribution tasks with up to double the average return and up to 8X
faster adaptation run-time speeds when compared to prior methods.
1	Introduction
Deep Reinforcement Learning (DRL) has led to recent advancements that allow autonomous agents
to solve complex tasks in a wide range of fields (Schulman et al. (2015), Lillicrap et al. (2015a),
Levine et al. (2015)). However, traditional approaches in DRL learn a separate policy for each
unique task, requiring large amounts of samples. Meta-Reinforcement learning (meta-RL) algo-
rithms provide a solution by teaching agents to implicitly learn a shared structure among a batch
of training tasks so that the policy for unseen similar tasks can quickly be acquired (Finn et al.
(2017)). Recent progress in meta-RL has shown efforts being made in improving the sample com-
plexity of meta-RL algorithms (Rakelly et al. (2019), Rothfuss et al. (2018)), along with the out-of-
distribution performance of meta-RL algorithms during adaptation (Fakoor et al. (2019); Mendonca
et al. (2020)). However, most of the existing meta-RL algorithms prioritize sample efficiency at
the sacrifice of computational complexity in adaptation, making them infeasible to adapt to fast-
changing environments in real-world applications such as robotics.
In this paper, we present Fast Linearized Adaptive Policy (FLAP), an off-policy meta-RL method
with great generalization ability and fast adaptation speeds. FLAP is built on the assumption that
similar tasks share a common linear (or low-dimensional) structure in the representation of the
agent’s policy, which is usually parameterized by a neural network. During training, we learn the
shared linear structure among different tasks using an actor-critic algorithm. A separate adapter net
is also trained as a supervised learning problem to learn the weights of the output layer for each
unique train task given by the environment interactions from the agent. Then when adapting to a
new task, we fix the learned linear representation (shared model layers) and predict the weights for
the new task using the trained adapter network. An illustration of our approach is highlighted in
Figure 2. We highlight our main contributions below:
•	State of the Art Performance. We propose an algorithm based on learning and predict-
ing the shared linear structures within policies, which gives the strongest results among
the meta-RL algorithms and the fastest adaptation speeds. FLAP is the state of the art in
all these areas including performance, run-time, and memory usage. As is shown in Fig-
1
Under review as a conference paper at ICLR 2021
ure 1, the FLAP algorithm outperforms most of the existing meta-RL algorithms including
MAML (Finn et al. (2017)), PEARL (Rakelly et al. (2019)) and MIER (Mendonca et al.
(2020)) in terms of both adaptation speed and average return. Further results from our
experiments show that FLAP acquires adapted policies that perform much better on out-of-
distribution tasks at a rapid run-time adaptation rate up to 8X faster than prior methods.
•	Prediction rather than optimization. We showcase a successful use of prediction via
adapter network rather than optimization with gradient steps (Finn et al. (2017)) or the
use of context encoders (Rakelly et al. (2019)) during adaptation. This ensures that differ-
ent tasks would have policies that are different from each other, which boosts the out-of-
distribution performance, while gradient-based and context-based methods tend to produce
similar policies for all new tasks. Furthermore, the adapter network learns an efficient way
of exploration such that during adaptation, only a few samples are needed to acquire the
new policy. To our knowledge, this is the first meta-RL method that directly learns and
predicts a (linearly) shared structure successfully in adapting to new tasks.
Figure 1: Strong Experimental Results: We showcase the performance of meta-RL methods on
tasks that are very different from the training tasks to assess the generalization ability of meth-
ods. We also analyze the adaptation run-time speed of these methods on tasks that are similar (in-
distribution) and tasks that are not very similar (out-of-distribution) to further evaluate these models.
Flap presents significantly stronger results compared to prior meta-RL methods.
Figure 2: Overview of our approach: In training, for different tasks {Ti}, we parametrize their
policy as πi = φ ∙ Wi, where φ ∈ Rd is the shared linear representation We hope to acquire. In
testing (adaptation), we fix the acquired linear representation φ and directly alter the weights wtest
by using the output of the feed-forward adapter network.
1.1 Related work
The idea behind learning a shared linear structure is motivated by the great progress of representation
learning and function approximation in domains such as robotics (Kober et al. (2013)) and natural
language processing (Keneshloo et al. (2018)). Representation learning has seen success in cases of
transfer learning RL in different tasks, and multi-agent reinforcement learning (MARL) where dif-
ferent agents may share common rewards or dynamics (Taylor & Stone (2009), Silva et al. (2018)).
The idea of learning shared information (embeddings) across different tasks has been investigated
deeply in transfer learning, including for example, universal value function approximators (Schaul
2
Under review as a conference paper at ICLR 2021
et al. (2015)), successor features (Barreto et al. (2017), Hunt et al. (2019)), and invariant feature
spaces (Gupta et al. (2017))).
Using a prediction model during meta-testing presents a new idea into meta-RL. The current trend
in meta-reinforcement learning has been to either use an encoder based method through the use of
context captured in recurrent models (Fakoor et al. (2019)) or variational inference (Rakelly et al.
(2019)), or use a variation of model-agnostic meta-learning (MAML) (Finn et al. (2017)), which
aims at finding some common parametrization of the policy such that it suffices to take one gradient
step in adaption to acquire a good policy. In contrast, FLAP aims at predicting the weights for the
policy network directly in adaptation without the need of taking gradient steps or using context and
comparing with existing data. This gives better performance in (a) faster adaptation speeds due to
the minimal amount of computation required; (b) smaller sample complexity during adaptation due
to the better exploration strategy provided by adapter network; (c) strong out-of-distribution gener-
alization ability resulting from the fact that the policy for each new task can be completely different
in the last layer (in contrast, the new policies produced by gradient-based algorithms like MAML
only differ in a single gradient update, which makes them highly similar, hurting the generalization
ability). We believe the idea of predicting instead of fine-tuning via gradient descent can shed light
to broader questions in meta-learning.
2 Background
We consider the standard Markov Decision Process (MDP) reinforcement learning setting defined by
a tuple T = (S, A, μ, μo, r, T), where S is the state space, A is the action space, μ(st+ι∣st, at) is
the unknown transition distribution with μo being the initial state distribution, r(st,at) is the reward
function, and T is the episode length. Given a task T, an agent’s goal is to learn a policy π : S → A
to maximize the expected (discounted) return:
qγ(π,T)=E[	Ytr(St, at)], so 〜μo, at 〜∏(st), st+ι 〜μ(st, at)	(1)
t=0
We now formalize the meta-RL problem. We are given a set of training tasks Ttrain = {Ti}in=1
and test tasks Ttest = {Tj}jm=1. In meta-training, we are allowed to sample in total k trajectories
Dtkrain from the tasks in Ttrain and obtain some information φ(Dtkrain) that could be utilized for
adaptation. The choice of φ(Dtkrain) varies for different algorithms. In meta-testing (or adaptation),
we are allowed to sample l trajectories Dtlest from each Ti ∈ Ttest. We would like to obtain a policy
π(φ(Dtkrain), Dtlest) for each testing task Ti, such that the average expected return for all testing
tasks is maximized:
-1 X qγ (∏(Φ(Dkrain), Dtest),方)∙	⑵
m	ran es
Ti ∈Ttest
Here the policy only depends on the training data through the information φ(Dtkrain), and can be
different for different test tasks. We are interested in getting the expected return in equation 2 as large
as possible with the smallest sample complexity k, l. Furthermore, we hope that the information
φ(Dtkrain) is reasonably small in size such that the memory needed for testing is also controlled.
We also state that in-distribution tasks typically refer to the test tasks being drawn from the same
distribution as the training distribution of tasks whereas out-of-distribution tasks represent the test
distribution as very different from the training distribution (in our case, all out-of-distribution tasks
are completely disjoint from the training set).
3	Fast Linearized Adaptive Policy (FLAP)
In this section, we describe the FLAP algorithm. We first assert and justify our assumption behind
the goal of the FLAP algorithm (Section 3.1), and then discuss the high-level flow of the algorithm
(Section 3.2).
3
Under review as a conference paper at ICLR 2021
3.1	Assumption and Motivation
Our FLAP algorithm is based on the following assumption on the shared structure between different
tasks during training and testing:
Assumption 3.1 We assume that there exists some function φ : S → Rd1 such that for any task
in the meta-RL problem Ti ∈ {Ttrain ∪ Ttest}, there exists a policy πi of the following form that
maximizes the expected return qγ (π, Ti):
πi = σ(hφ, wii +bi).	(3)
Here wi ∈ Rd1 ×d2 , bi ∈ Rd2. d1 is the latent space dimension and d2 is the dimension of the action
space. The function φ represents a shared structure among all tasks, and wi and bi are the unique
weight and bias for task Ti. The function σ is the activation function depending on the choice of the
action space.
Similar assumptions that constrain the policy space also appear in literature, e.g. Levine et al. (2016);
Lillicrap et al. (2015b); Schmidhuber (2019); Mou et al. (2020) and references therein. One can
also extend the linear representation in equation 3 to quadratic or higher-order representations for
more representational ability and looser asumptions. We compare the performance between linear
structures and non-linear structures in Section C.1.3.
3.2	Algorithm overview
Given assumption 3.1, the overall workflow of the algorithm is quite straightforward: in meta-
training, we would like to learn the shared feature φ from the training tasks Ttrain ; in meta-testing,
we utilize the learned linear feature φ to derive the parameters wi, bi for each new task. To speed up
the adaptation, we also train a separate feed-forward adapter network simultaneously that takes the
trajectories of each task as input and predicts the weights that are learned during training. The full
pseudo-code for both meta-training and meta-testing is provided in Appendix A.
3.2.1	Meta-training
In order to learn the linear representation φ from Assumption 3.1, we use a deep neural network for
the policy, sharing all but the output layer among all the train tasks in the training set. Each train task
will have a unique output layer during meta-training, and the shared neural network is the feature φ
we hope to learn during training.
Mathematically, we parametrize the policy via a neural network as π(φ, wi, bi) = σ(hφ, wii + bi),
where wi and bi are the weights and bias unique to task i, and σ represents the linear activation for
the output layer since for all the tasks we considered in this paper, the action space takes values in
the range of [-∞, +∞].
FLAP uses the general multi-task objective to train the policy over all the n training tasks during
meta-training. We maximize the average returns over all training tasks by learning the policy pa-
rameters represented by {φ, {wi, bi}in=1}. We would like to find the meta-learned parameters via
solving the following optimization problem:
1n
{φ, {wi,bi}7n=ι} =	arg max — S^qY (∏(Φ,Wi ,bi), Ti).	(4)
{φ,{wi,bi}in=1} n i=1
Here φ is parameterized by a neural network, and πi is parameterized by the parameters
{φ, {wi , bi }in=1 }.
Actor-Critic. For solving the optimization problem in equation 4, we apply the off-policy soft
actor-critic (SAC) (Haarnoja et al. (2018)) for sample efficiency.
In the actor-critic model, besides the policy network, we also have a critic network which learns a
Q function for evaluating the current policy. We linearize the critic network in a similar fashion to
the policy by fixing a set of shared layers in the critic network. We show the shared layer structure
for both policy and critic network in Figure 3. Linearizing the critic network, though not explicitly
4
Under review as a conference paper at ICLR 2021
Figure 3: Each task shares a feature mapping in the critic and a different shared feature mapping
in the policy network. In this illustration, for the training task 1 input data x1, only the output y1
from its corresponding output layer will be used in the meta-loss function. Similarly for task 2 input
data x2, only the output y2 from its corresponding output layer will be used in the meta-loss. This
is extended up to n points where n is the number of training tasks used.
required, does not incur any loss in performance from experimental results, and drastically reduces
the number of parameters needed to be learned during training for the critic portion of the actor-critic
method. We discuss in more detail about the use of the soft actor-critic in Appendix D.1.
Adapter network. Learning the shared structure is a standard approach in multi-task transfer
learning (Pan & Yang (2009); Asawa et al. (2017); D’Eramo et al. (2020)). The key contribution
and distinction of our algorithm is the use of an adapter network for rapid adaptation. We train a
separate adapter network simultaneously with the shared structure during meta-training. We define
the SARS tuple as a tuple containing information from one time step taken on the environment by
an agent given a single task, including the current state, action taken, reward received from taking
the action given the current state, and the next state the agent enters (st, at, st+1, r(st, at)). The
adapter net is fed a single tuple or a sequence of the most recent SARS tuples from a task and pre-
diets the unique weights (Wi) needed to acquire the new policy for that task. We train the adapter
network with mean squared error between the output of the adapter net (Wi) obtained from the input
SARS tuples, and the set of weights obtained from meta-training contained in each output layer. The
process of training the adapter net is shown in Figure 4.
Training the Adapter Network
Figure 4: Illustration of training the adapter network. The weights of the corresponding output layer
for the training task are flattened and concatenated to form a 1-D target for the output of the adapter
model.
3.2.2	Meta-testing
In meta-testing, the FLAP algorithm adapts the meta-learned policy parameters Wi , bi to a new task
(Ti ∈ Ttest) with very little data sampled from the new task. Following Assumption 3.1, we assume
that the shared structure φ is already learned well during meta-training. Thus in meta-testing, we fix
the structure φ obtained in training, and only adjust the weights for each test task. Mathematically,
we run the following optimization problem to obtain the policy for each new test task Tnew ∈ Ttest :
The objective for the new task (Tnew) is to find the set of weights that with the shared structure φ
form the adapted policy
Wnew,bnew = argmaxqγ(π(φ,W,b),Tnew).	(5)
w,b
We then output π(φ, Wnew, bnew) as the policy for the new task.
In our practical implementation and results, we show our contribution that requiring an objective
function to be optimized during meta-testing does not necessarily have to be the only approach
5
Under review as a conference paper at ICLR 2021
Online Adaptation
Figure 5: The first step taken is arbitrary. Each step following the first is fed into the feed-back loop
where the SARS pair is inputted to the adapter network providing anew set of weights for the output
layer of the policy.
for meta-RL, though FLAP can still have the optimization in Equation 5 easily integrated in meta-
testing. FLAP performs rapid online adaptation by using the adapter network to predict the weights
of the output layer as opposed to performing the optimization step. The first time-step taken by the
agent is based off the most recent weights learned from the adapter network. This then initiates a
feed-back loop where now each time-step gives the adapter network a single tuple or a sequence
of the most recent SARS tuples which inputted to the adapter network, immediately produces a
set of predicted weights to use with the linear feature mapping (φ) to acquire an adapted policy
(∏adapt = ΦT ∙ Wi + bi). Only a fixed number of online adaptation steps are needed to acquire the
fully adapted policy. Once the last time-step of weights is predicted, FLAP keeps only the last set of
predictive weights for the rest of the episode. The meta-testing loop is shown in Figure 5.
4	Experiments
This section presents the main experients of our method. We evaluate FLAP in Section 4.1 by
comparing it to prior meta-learning approaches on in-distribution tasks. We then examine how the
FLAP algorithm is able to generalize to very different test tasks using out-of-distribution tasks in
Section 4.2. We take a holistic view of FLAP as a meta-RL method and analyze its run-time and
memory usage on an autonomous agent during the meta-testing stage in Section 4.3. We leave
the ablation studies and sparse reward experiments to Appendix C. Finally, we provide thorough
discussions on the empirical performance in Section 4.4.
4.1	Sample Efficiency and Performance Compared to Prior Methods
We evaluate FLAP on standard meta-RL benchmarks that use in-distribution test tasks (training
and testing distributions are shared). These continuous control environments are simulated via the
MuJoCo simulator (Todorov et al. (2012)) and OpenAI Gym (Brockman et al. (2016)). Rakelly
et al. (2019) constructed a fixed set of meta-training tasks (Ttrain) and a validation set of tasks
(Ttest ). These tasks have different rewards and randomized system dynamics (Walker-2D-Params).
To enable comparison with these past methods, we follow the evaluation code published by Rakelly
et al. (2019) and follow the exact same evaluation protocol. In all the tasks we consider, the adapter
network is fed with a single SARS tuple during training and adaptation. Other hyper-paramters
of our method are given in Appendix D.3 and we report the result of FLAP on each environment
from 3 random seeds. We compare against standard baseline algorithms of MAML (Finn et al.
(2017)), RL2 (Duan et al. (2016b)), ProMP (Rothfuss et al. (2018)), PEARL (Rakelly et al. (2019)),
MQL (Fakoor et al. (2019)), and MIER (and MIER-wR) (Mendonca et al. (2020)). We obtained
the training curves for the first four algorithms from Rakelly et al. (2019) and the latter two from
Mendonca et al. (2020).
6
Under review as a conference paper at ICLR 2021
Half-Cheetah-Fwd-Back
Time steps (1e6)
Humanoid-Direc-2D
u-lnφα
0.0	0.2	0.4	0.6	0.8	1.0
----FLAP (Ours) ------- MIER-wR ------- PEARL
Walker-Rand-Params
"Tirv^Λ o⅜λπc⅛ ∕*1aG∖
MAML -------- RL2 -------- ProMP
Figure 6: Validation Test-Task performance over the course of the meta-training process on standard
in-distribution meta-RL benchmarks. Our algorithm is comparable in all environments to prior meta-
RL methods.
4.2	Generalization to Out-of-Distribution Tasks
To evaluate generalization, we test on the extrapolated out-of-distribution (OOD) environments,
where the train and test distributions are highly dissimilar and completely disjoint, created by Men-
donca et al. (2020) and Fakoor et al. (2019), closely following their evaluation protocols. The envi-
ronments include changes in rewards and dynamics. We run FLAP on 3 random seeds to evaluate
the performance. In all the tasks we consider, the adapter network is fed with a single SARS tuple
during training and adaptation. Hyperparameters are listed in Appendix D.3.
u」no」Θ6EJ8A4
Eno」ΦCTra⅛><
Figure 7: Validation Test-Task performance over the course of the meta-training process on out-of-
distribution meta-RL benchmarks. Our algorithm is comparable if not better to prior methods in all
environments except for the Ant-Negated task.
Ooo
8 6 4
u」n@①&①Aq
Time steps (1e6)
Ant-Dir (OOD)
Half-Cheetah-Vel-Hard
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
Time steps (1e6)
Ant-Negated-Joints
0.5	1.0	1.5	2.0
Tima UtacCi ∕1αG
PEARL ----------- MQL ------------ MAML
7
Under review as a conference paper at ICLR 2021
4.3	Overall Model Analysis
In this section, we assess the run-time of FLAP. We compare our method to MIER-wr and PEARL
in meta-testing speeds on benchmarks from both in-distribution and out-of-distribution tasks. MIER
and MQL both reuse the training buffer during testing, causing adaptation speeds and memory usage
that are far worse than prior methods (MIER took 500+ seconds for meta-testing). We only compare
with the most competitive algorithms speed and memory-wise to showcase FLAP’s strength. We
defer the analysis of storage efficiency to Appendix (B).
To calculate the run-time speeds, we use the Python Gtimer package and a NVIDIA GEFORCE GTX
1080 GPU and time each of the Meta-RL methods on four MuJoCo environments (Ant, Cheetah,
Humanoid, Walker) on 30 validation tasks after allowing each algorithm to achieve convergence
during meta-training.
Figure 8: Our method clearly outperforms all other meta-RL algorithms in runtime during adapta-
tion to new tasks.
4.4	Discussion
Beyond the experiments on in-distribution and out-of-distribution tasks, we did more experiments
and analysis to justify the superiority of FLAP, including ablation studies and extra experiments on
MDPs with sparse rewards. We briefly describe the results and take-aways in Section 4.4.1 and 4.4.2,
and leave the details on experiments to Appendix C. Lastly, we analyze all the experimental results
and summarize the reason for the validity and generalization ability of the adapter network in Sec-
tion 4.4.3
4.4	. 1 Ablation studies
We validate the effectiveness of FLAP via a large amount of ablation studies. In particular, we
compare the performance under the following different settings:
1.	With adapter net vs Without adapter net: One may wonder whether the adapter network
really boosts the performance compared to the approach of directly learning the linear
weights via soft actor-critic during adaptation without the help of the adapter network. As
is shown in Appendix C.1.1, although both algorithms converge to approximately the same
average reward in the end, the adapter network drastically decreases the sample complexity
during the adaptation process and accelerates the speed (at least 30X faster among all the
experiments) compared to the algorithm without the adapter.
2.	Shared linear layer vs Non-linear layers: One may wonder why we assume the policies
share only the very last layer instead of the last two or more layers as embeddings. Indeed,
experiments in Appendix C.1.3 show that when sharing two (or more) layers, the policies
may have too much degree of freedom to converge to the global minimum. The shared
linear layer turns out to be the best assumption that balances the performance with the
speed.
3.	Single SARS tuple vs a Sequence of SARS tuple as Input of Adapter Net: All the
experiments above work with the adapter net with a single (the most recent) SARS tuple
as input. We show in Appendix C.1.4 that if we let the adapter net take a sequence of
8
Under review as a conference paper at ICLR 2021
the most recent SARS tuples as input, the average return would not be affected on the
tasks we considered above. However, we see a reduction in variance with a slightly slower
adaptation speed. Overall, taking a longer sequence of SARS tuples as input tends to
stabilize the adaptation and reduce the variance in the sacrifice of runtime. However,
for some specific tasks with sparse rewards and similar SARS tuples, it also helps improve
the average return, as is discussed in the next section.
4.4.2	Experiments on Sparse Reward
We show in Appendix C.2 that for tasks with sparse rewards and similar dynamics (e.g. the naviga-
tion tasks), the performance of FLAP may deteriorate a little since the resulting policy highly relies
on the chosen transition tuple, which may lead to higher variance. However, it is still comparable to
PEARL if we let the adapter net take a sequence of SARS tuples as input. This is due to fact that
a sequence of SARS tuples provides more information on the context for the adapter, making the
resulting policy less dependent on the single chosen transition tuple.
However, for most of the other tasks we considered above, one single transition tuple suffices to
provide enough information for the success of adapter network with small variance and strong gen-
eralization ability. We also validate it in Appendix C.3 by showing that the loss for adapter net
converges in a reliably fast and stable way.
4.4.3	On the validity and generalization ability of FLAP and the Adapter
Network
To summarize the experimental results above, we see that FLAP guarantees a much smaller sample
complexity during adaptation, improved generalization ability and faster adaptation speeds. We
provide justification as to why FLAP can achieve all the properties separately.
1.	Small Sample Complexity: From our ablation study in Appendix C.1.1, it is clear that the
small sample complexity during meta-testing comes from the introduction of adapter net.
We conjecture that during training, the adapter net learns the structure of optimal policies
shared by all the training tasks; and during adaptation, the predicted weights encourage the
most efficient exploration over potential structures of optimal policies such that the average
return converges in a fast and stable way.
2.	Strong generalization: From our ablation study in Appendix C.1.1, we see that the al-
gorithms with and without adapter converge to a similar average return. Thus we conclude
that the generalization ability is not related to the specific design of adapter net, but comes
from the shared linear structure. In traditional gradient-based meta-RL approaches like
MAML, the new policies are generated from taking one gradient step from the original
weights in training, which makes the new policies very similar to the old ones. In contrast,
the new policy in FLAP can have a completely different last layer, which guarantees more
flexibility in varying policies, and thus a better generalization to new tasks.
3.	Fast adaptation: Compared to other meta-RL approaches, FLAP only requires evaluating
the output of a fixed adapter net, which provides instant adaptation when the adapter net is
small enough.
The successful experiments also justify the validity of Assumption 3.1: for all the tasks we consider,
they do have some near-optimal policies with shared embedding layers. We believe this could also
provide interesting theoretical directions as well.
5 Conclusion
In this paper, we presented FLAP, a new method for meta-RL with significantly faster adaptation
run-time speeds and performance. Our contribution in the FLAP algorithm presents a completely
new view into meta reinforcement learning by applying linear function approximations and predict-
ing the adaptation weights directly instead of taking gradient steps or using context encoders. We
also believe the idea behind the adapter network during meta-testing can be extended to other meta
learning problems outside of reinforcement learning.
9
Under review as a conference paper at ICLR 2021
References
Chaitanya Asawa, Christopher Elamri, and D. Pan. Using transfer learning between games to im-
prove deep reinforcement learning performance. 2017.
Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt,
and David Silver. Successor features for transfer in reinforcement learning. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
Vances in Neural Information Processing Systems, volume 30, pp. 4055-4065. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
350db081a661525235354dd3e19b8c05- Paper.pdf.
Jonathan Baxter. A model of inductive bias learning. CoRR, abs/1106.0245, 2011. URL http:
//arxiv.org/abs/1106.0245.
Jonathan Baxter. Learning internal representations (colt 1995). 2019. doi: 10.1145/225298.225336.
Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. In IJCNN-91-Seattle
International Joint Conference on Neural Networks, volume ii, pp. 969 vol.2-, 1991.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/
abs/1606.01540.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical ma-
chine translation. CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.
1078.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowl-
edge in multi-task deep reinforcement learning. In International Conference on Learning Repre-
sentations, 2020. URL https://openreview.net/forum?id=rkgpv2VFvr.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. CoRR, abs/1604.06778, 2016a. URL http:
//arxiv.org/abs/1604.06778.
Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter AbbeeL R1$八2$:
Fast reinforcement learning via slow reinforcement learning. CoRR, abs/1611.02779, 2016b.
URL http://arxiv.org/abs/1611.02779.
Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement
learning, 2019.
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J. Smola. Meta-q-learning, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. CoRR, abs/1703.03400, 2017. URL http://arxiv.org/abs/1703.
03400.
Vincent Frangois-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, and Joelle Pineau.
An introduction to deep reinforcement learning. CoRR, abs/1811.12560, 2018. URL http:
//arxiv.org/abs/1811.12560.
Alborz Geramifard, Thomas J. Walsh, and Stefanie Tellex. A Tutorial on Linear Function Approx-
imators for Dynamic Programming and Reinforcement Learning. Now Publishers Inc., Hanover,
MA, USA, 2013. ISBN 1601987609.
Abhishek Gupta, Coline Devin, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant
feature spaces to transfer skills with reinforcement learning. CoRR, abs/1703.02949, 2017. URL
http://arxiv.org/abs/1703.02949.
Abhishek Gupta, Russell Mendonca, Yuxuan Liu, Pieter Abbeel, and Sergey Levine. Meta-
reinforcement learning of structured exploration strategies. CoRR, abs/1802.07245, 2018. URL
http://arxiv.org/abs/1802.07245.
10
Under review as a conference paper at ICLR 2021
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290,
2018. URL http://arxiv.org/abs/1801.01290.
Jonathan J. Hunt, Andre Barreto, Timothy P. Lillicrap, and Nicolas Heess. Composing entropic Poli-
cies using divergence correction. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp.
2911-2920. PMLR, 2019. URL http://Proceedings.mlr.press∕v97∕hunt19a.
html.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement
learning with linear function approximation. CoRR, abs/1907.05388, 2019. URL http://
arxiv.org/abs/1907.05388.
Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, and Chandan K. Reddy. Deep reinforce-
ment learning for sequence to sequence models. CoRR, abs/1805.09461, 2018. URL http:
//arxiv.org/abs/1805.09461.
Jens Kober, J. Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The Interna-
tional Journal of Robotics Research, 32:1238-1274, 09 2013. doi: 10.1177/0278364913495721.
Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems 12, pp. 1008-1014. MIT Press, 2000. URL http://papers.nips.cc/
paper/1786- actor- critic- algorithms.pdf.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep vi-
suomotor policies. CoRR, abs/1504.00702, 2015. URL http://arxiv.org/abs/1504.
00702.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning, 2015a.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015b.
Russell Mendonca, Xinyang Geng, Chelsea Finn, and Sergey Levine. Meta-reinforcement learning
robust to distributional shift via model identification and experience relabeling. arXiv preprint
arXiv:2006.07178, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.
Wenlong Mou, Zheng Wen, and Xi Chen. On the sample complexity of reinforcement learning with
policy space generalization. arXiv preprint arXiv:2008.07353, 2020.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wakefield, and Michael Littman. An
analysis of linear models, linear value-function approximation, and feature selection for rein-
forcement learning. pp. 752-759, 01 2008. doi: 10.1145/1390156.1390251.
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy
meta-reinforcement learning via probabilistic context variables. CoRR, abs/1903.08254, 2019.
URL http://arxiv.org/abs/1903.08254.
11
Under review as a conference paper at ICLR 2021
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. CoRR, abs/1810.06784, 2018. URL http://arxiv.org/abs/1810.
06784.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-
tors. volume 37 of Proceedings ofMachine Learning Research, pp. 1312-1320, Lille, France, 07-
09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/schaul15.html.
Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards-just map them
to actions. arXiv preprint arXiv:1912.02875, 2019.
John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region
policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/1502.
05477.
Felipe Leno Da Silva, Matthew E. Taylor, and Anna Helena Reali Costa. Autonomously reusing
knowledge in multiagent reinforcement learning. In Proceedings of the Twenty-Seventh Interna-
tional Joint Conference on Artificial Intelligence, IJCAI-18, pp. 5487-5493. International Joint
Conferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/774. URL
https://doi.org/10.24963/ijcai.2018/774.
Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(56):1633-1685, 2009. URL http://jmlr.org/
papers/v10/taylor09a.html.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. CoRR, abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.
12
Under review as a conference paper at ICLR 2021
A Pseudo Code
Algorithm 1 and Algorithm 2 provide the pseudo-code for meta-training and meta-testing (adapta-
tion to a new task) respectively. In meta-training (1), the replay buffers B store data for each train
task and provide random batches for training the soft actor-critic. For each train task Ti in meta-
training, the actor loss Lactor is the same as Jπ(θ, T) which is defined in Equation 8. The critic loss
Lcritic is the same as that used in the soft actor-critic with the Bellman update:
Lcritic = E(s,a,r,s0)〜Bi[Qψ(s, a) - (r + Y * Q(s0, a0))]2, a 〜∏θ(a0∣s0)	(6)
The adapter loss Ladapter is the M SE loss. These three networks are all optimized through gradient
descent, with learning rates denoted by α for each respective network. The learning rate for each of
the three networks is a hyper-parameter predetermined before training.
After meta-training, a meta-learned policy ∏ and trained adapter 夕 are used for adaptation. Al-
gorithm 2 lays out the adaptation process where the policy πθ is to be adapted. A fixed, pre-
determined number (T) of adaptation steps are taken. The process is illustrated in Figure 4.
Algorithm 1 FLAP: Meta-Training
Require: Training Tasks Batch {Ti}i=1...N from ρ(Ttrain)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
Initialize replay buffers Bi for each train task
while not done do
for each Ti do
Gather data from πθ and add to Bi
end for
for step in train steps do
for each train task Ti do
Sample batch of data bi 〜Bi
Lactor
Licritic = L
= Liactor (bi)
icritic(bi)
weights = Flatten(Wyi(φ))
target = Concat(weights, Byi(φ))
Liadapter = Liadapter ((s, a, r, s0), target)
end for
ψQ J ψQ …Q Vψ Pi(LCritic)
θ∏ J θ∏ - α∏ Na Pi(LactOr)
夕 J QaW vφ Pi(Ladapter)
end for
end while
Algorithm 2 FLAP: Meta-Testing
Require: Test Task T 〜P(Ttest)
1:
2:
3:
4:
5:
6:
7:
8:
9:
Load policy model with parameter θ and fix the output layer, load adapter φ
Initialize output layer weights with any output layer from training
for t = 1 ... T do
at 〜∏θ(at∣st)
st+ι 〜ρ(st+ι∣st,at)
rt = r(st, at)
(weights, bias) = φ(st, at, st+ι, rt)
Update θ by setting the output layer parameters as (weights, bias).
end for
B	Memory Usage
For storage analysis of models, we concern ourselves only with the amount of memory needed
during meta-testing as the process of meta-learning is to meta-learn a policy that is both lightweight
in storage, fast to adapt, and effective in performance on a new task. We assessed the internal
13
Under review as a conference paper at ICLR 2021
structure of models based off the number of parameters used in each algorithm for the models
that are necessary when adapting to a new task. As already noted in Section 4.3, we only use
MIER without experience relabeling (Mendonca et al. (2020)) and PEARL (Rakelly et al. (2019))
in comparison with FLAP. We note that though MIER with experience relabeling is better on the
Ant-Negated Task in comparison with FLAP, it utilizes the process of experience relabeling where
the training replay buffer must be used for meta-testing. MQL (Fakoor et al. (2019)) also requires
the use of the training buffer for adapting to out-of-distribution tasks. This leads to a highly negative
result in both algorithms since the amount of memory necessary for these models to adapt to highly
extrapolated tasks grows as the number of meta-train tasks used during training increases and the
complexity of the meta-RL problem increases.
PEARL and MIER-wR (without experience relabeling) do not need to reuse the replay buffer how-
ever FLAP performs uniformly better than these methods in all the out-of-distribution baselines.
The model parameters are detailed in Table 1 and clearly FLAP is at least comparable if not better
in the aspect of memory required for adaptation.
Algorithm	Policy Size	Model Size
FLAP (OURS)	300x300x300	400x400x400
MIER-wR	300x300x300	200x200x200x200
Pearl	300x300x300	200x200x200
Table 1: The model and policy work together in meta-testing. The model is typically an extra
network for context encoding (PEARL, MIER-wR) or adaptation (FLAP). FLAP is comparable in
memory to the most competitive algorithms memory-wise. We use default settings for all algorithms
for the memory analysis.
C Abalation Studies and Further Experiments
In this section, we showcase some more experiments involving FLAP.
C.1 Experimental Details on ablation study
We conclude our experimental analysis by showcasing ablation studies on the adapter network. We
first provide further justification for the use of the adapter network in the first place and evidence
that it works with reasonable decreasing loss while the model learns. We then showcase the effect
during adaptation of feeding in different input data into the adapter network. We finally finish by
showcasing single-point input compared to a sequence of points concatenated together as input to
the adapter network and its result on the effectiveness of the adapter network. Analysis of the loss
values of the adapter network during training can be found in Appendix C.3.
C.1.1 With adapter net vs without adapter net: Strong Performance of the
Adapter Network
To analyze the importance of the adapter network, we compare the adaptation (meta-test) results on
the Half-Cheetah environment with the use of feed-forward prediction compared to simply using a
meta-testing objective and performing SAC objective function gradient updates to iteratively acquire
the new policy. Although both methods are able to eventually obtain a strong performing policy, the
adapter network drastically decreases the amount of samples needed for adaptation and provides a
clear benefit for the FLAP algorithm, as is shown in Figure C.1.1. We also see a dramatic decrease
in run-time (at least 30X among all the experiments) when we have the adapter network in the
adaptation procedure.
14
Under review as a conference paper at ICLR 2021
Enφα ΦCTrat-φ><
Oooo
5 0 5
7 5 2
Ooooo
0 5 0 5 0
0 7 5 2 0
5
6
No Adaptor Net
Adaptor Net
Figure 9: Ablation study for FLAP with vs without adapter network. Although both algorithms
converge to similar average return, the algorithm with adapter net converges much faster than the
one without adapter.
C.1.2 SAS INPUT VS SARS INPUT
We test different inputs to the adapter network. We train the adapter using only (state, action, next
state) pairs compared to the original complete SARS pair to analyze whether the adapter is able
to learn the reward function. Given the random and poor performance of the adapter with only
(S, A, S0) as input, we provide evidence that the adapter net learns valuable information to aid the
algorithm during adaptation.
Enφ= φ^2φ><
Figure 10: Results of varying the input to the adapter network. In environments such as Half-
Cheetah, where tasks differ in the reward function, the reward is a valuable input to the adapter
network and helps in its effectiveness for predicting weights.
C.1.3 Nonlinear Output Layers
Our idea of maintaining a shared set of layers and a lin-
ear output layer can easily be extended beyond the lin-
ear case. We motivate this section by showcasing that
FLAP does not gain performance from increased expres-
siveness when extended to the multi-layer output case.
We use a two-layer (quadratic) output and reduce the hid-
den units in each layer from 300, which was used in the
linear case, to 50 to keep the total number of output units
in both FLAP algorithms approximately consistent. We
also use the adapter network (learning non-linear weights
during meta-testing from scratch slowed down adaptation
speeds) to predict both layer weights. We ran the regu-
lar and modified FLAP algorithms on out-of-distribution
Figure 11: We add an extra layer, with
a non-linear activation function, to the
original linear output layer to create the
non-linear output.
15
Under review as a conference paper at ICLR 2021
tasks to compare generalization strength of each method.
Our implementation of the modified FLAP algorithm is
illustrated in Figure 11. Results of the comparison are
shown in Figure 12.
Figure 12: The linear FLAP algorithm outperformed the modified (non-linear) FLAP algorithm.
C.1.4 Sequence vs. Single Point of SARS
TUPLES AS INPUT OF ADAPTER NET
We showcase that using a sequence of tuples (5 tuples) does not improve the performance of the
algorithm in our baseline problems and in cases where the linear assumption holds up well, although
it offers improvements including reduced variance in less structured cases where the assumption may
not hold as strongly such as in sparse reward settings (Appendix C.2). It is important to note that
the adapter is batch-wise trained meaning that a random batch of SARS tuples are sampled from the
replay buffer of the corresponding task and all trained with the same set of target weights. Although
the network may be presented a tuple where the state or action is completely new, the adapter is
able to generalize and make a strong prediction with just a single point and without sacrifice to
performance indicating the self-correcting nature of the adapter network.
EnωαφBal① ><
Figure 13: Results of Single Point tuple input vs Sequence of SARS tuples as inputs on the Half-
Cheetah environment. The results showcase both methods are comparable in return.
C.2 Sparse Reward Environments
Sparse Rewards Reinforcement Learning environments require the agent to reason about uncertainty
in an environment where different tasks share many common features and seemingly appear identi-
cal. We compare against the PEARL algorithm on the same (in-distribution) sparse reward setting
(Sparse Point Robot) first introduced by PEARL that requires navigation in a 2D setting to a goal
target (Rakelly et al. (2019)).
We mention that although FLAP only needs 1 trajectory of test data, we allow the PEARL algorithm
to obtain as many trajectories from the unseen task as necessary to achieve its strongest performance
which already requires 20+ trajectories even in such a simple problem. We also run FLAP using a
single point adapter input and a sequence of points (5 tuples) as an input to the adapter network. In
16
Under review as a conference paper at ICLR 2021
Enφα ΦCTSΦ><
Figure 14: Results of the sparse reward environment. FLAP, with the use of a sequence of inputs
to the adapter network, is comparable with PEARL even with the FLAP algorithm using 20x less
samples from the test task.
—— FLAP (Single Point)
—— PEARL (20 traj)
----PEARL (1 traj)
----FLAP (Sequence)
this case, the sequence of points as input helped not only improve the performance but also presented
less variance among different runs of the algorithm which indicates that on problems where the linear
assumption may not be as strong, such as sparse reward settings, using multiple points as inputs may
present improved results. Our algorithm completely outperforms PEARL when both algorithms are
only allowed 1 trajectory of data, indicating that FLAP has better scalability especially when more
complex sparse reward problems are encountered where more test data is required to be sampled to
acquire the new policy.
In conducting the experiment, we followed the PEARL algorithm in training on dense rewards for
the sparse point robot and testing on the sparse reward setting to allow for the fair comparison
between the algorithms. We state that the success of the FLAP algorithm may be attributed to
the fact that the implementation is built on the SAC algorithm which uses entropy to help with
exploration, a vital component especially in sparse reward problems.
C.3 Adapter Network Loss
During training, we captured the loss of the adapter network during training for its ability to train
on target train task weights. In the experiments conducted, the loss value typically converged nicely
overall with no indication of any high loss values which would have presented conflicting perfor-
mance on the different baseline tasks. We showcase the loss on the Half-Cheetah environment and
the Sparse Point Robot Problem where there are sparse rewards. Although the adapter network
trained reasonably on both, it is important to note that when the linear assumption is not as strong,
the loss value tends not to converge as smoothly which also leads to variance in the performance of
the algorithm on the environment.
S(ΛO1 山 SlAl
17
Under review as a conference paper at ICLR 2021
D Implementation Details
We provide implementation details including the hyper-parameters used in building FLAP and the
settings used for evaluation of algorithms on both in-distribution and out-of-distribution tasks.
D. 1 Actor-Critic Implementation
We discuss more specifics on the use of the actor-critic algorithm in our model. Specifically, for the
actor and critic networks, we parameterize the critic network by ψ, leading to Qψ , and the policy
network by θ, leading to πθ , then we can write the objective function for actor-critic algorithms to
optimize the policy (actor) network:
1N	T
Qψθ (S, a,T) = N EEst,at 〜∏θ,Ti [£ Y tr(St,Ti ,at,Ti )ls0,Ti = s, a0,Ti = a]	⑺
i=1	t=0
1N
Jn(θ, T) = NN ∑[-Es〜Ti,a~π [Qψθ (S, a, T)]],	(8)
where Equation 7 is the Q-value estimation from the critic and the policy loss Jπ in Equation 8
specifically details the loss function used to optimize the meta-policy parameters. We also em-
ploy automatic entropy-tuning for our SAC algorithm to help with exploration and employ two
Q-function networks to help reduce over-estimation bias, a technique commonly known as ”double-
Q-learning” (van Hasselt et al. (2015)).
D.2 Experiment Details
The horizon for all environments is 200 time-steps. Evaluation of return is then the average reward
of the trajectories where each trajectory is the sum of the rewards collected over its rollout. We
use settings from PEARL (Rakelly et al. (2019)) for the in-distribution tasks and the settings from
MIER (Mendonca et al. (2020)) for the out-of-distribution tasks. The settings for the environments
are listed in the Table. FLAP requires less meta-train tasks for meta-training than prior methods.
The number of adaptation steps refers to the number of iterations of the feedback loop illustrated by
meta-testing and described in Section 4 before the policy is finally settled. The settings are detailed
in Table 2 and Table 3.
Table 2: In-Distribution Settings
Environment	Meta-train Tasks	Meta-test Tasks	Adaptation Steps
Half-Cheetah-Fwd-Back	2	2	30
Ant-Fwd-Back	2	2	60
Humanoid-Dir	5	30	30
Walker-Rand	10	10	60
Table 3: Out-of-Distribution Settings
Environment	Meta-train Tasks	Meta-test Tasks	Adaptation Steps
Cheetah-vel-medium	15	30	1
Cheetah-vel-hard	15	30	1
Ant-direction	10	30	60
Ant-negated-joints	15	15	30
D.3 Hyper-parameters
The hyper-parameters for FLAP are kept mostly fixed across the different tasks except for the num-
ber of parameter updates per iteration which is different only for the Ant and Walker environments
18
Under review as a conference paper at ICLR 2021
(4000). All others use the default of 1000 updates. These hyper-parameters were kept as default
values from the PEARL (Rakelly et al. (2019)) soft actor-critic implementation code in the PEARL
code base as both FLAP and PEARL are built based off the rllab code base (Duan et al. (2016a)).
They are detailed in Table 4.
Table 4: Hyper-parameters
Parameter	Value
Learning Rate	3 × 10-4
Discount Factor	0.99
Target Update Interval	1
Target Update Rate	0.005
Sac Reward Scale	1.0
Sac Optimizer	Adam
Batch-size	256
Policy Arch.	300-300-300
Critic Arch	300-300-300
Adapter Arch	600-600-600
Adapter Learning Rate	3 × 10-4
Adapter Optimizer	Adam
Parameter Updates per Iteration	1000.4000
Num Points in Sequence Input	5, 10
19