Under review as a conference paper at ICLR 2021
Deep Coherent Exploration For
Continuous Control
Anonymous authors
Paper under double-blind review
Ab stract
In policy search methods for reinforcement learning (RL), exploration is often
performed by injecting noise either in action space at each step independently or
in parameter space over each full trajectory. In prior work, it has been shown
that with linear policies, a more balanced trade-off between these two exploration
strategies is beneficial. However, that method did not scale to policies using deep
neural networks. In this paper, we introduce Deep Coherent Exploration, a gen-
eral and scalable exploration framework for deep RL algorithms on continuous
control, that generalizes step-based and trajectory-based exploration. This frame-
work models the last layer parameters of the policy network as latent variables
and uses a recursive inference step within the policy update to handle these latent
variables in a scalable manner. We find that Deep Coherent Exploration improves
the speed and stability of learning of A2C, PPO, and SAC on several continuous
control tasks.
1	Introduction
The balance of exploration and exploitation (Kearns & Singh, 2002; Jaksch et al., 2010) is a long-
standing challenge in reinforcement learning (RL). With insufficient exploration, states and actions
with high rewards can be missed, resulting in policies prematurely converging to bad local optima. In
contrast, with too much exploration, agents could waste their resources trying suboptimal states and
actions, without leveraging their experiences efficiently. To learn successful strategies, this trade-off
between exploration and exploitation must be balanced well, and this is known as the exploration
vs. exploitation dilemma.
At a high level, exploration can be divided into directed strategies and undirected strategies (Thrun,
1992; Plappert et al., 2018). While directed strategies aim to extract useful information from exist-
ing experiences for better exploration, undirected strategies rely on injecting randomness into the
agent’s decision-making. Over the years, many sophisticated directed exploration strategies have
been proposed (Tang et al., 2016; Ostrovski et al., 2017; Houthooft et al., 2016; Pathak et al., 2017).
However, since these strategies still require lower-level exploration to collect the experiences, or
are either complicated or computationally intensive, undirected exploration strategies are still com-
monly used in RL literature in practice, where some well-known examples are -greedy (Sutton,
1995) for discrete action space and additive Gaussian noise for continuous action space (Williams,
1992). Such strategies explore by randomly perturbing agents’ actions at different steps indepen-
dently and hence are referred to as performing step-based exploration in action space (Deisenroth
et al., 2013).
As an alternative to those exploration strategies in action space, exploration by perturbing the
weights of linear policies has been proposed (Ruckstieβ et al., 2010; Sehnke et al., 2010; Kober
& Peters, 2008). Since these strategies in parameter space naturally explore conditioned on the
states and are usually trajectory-based (only perturb the weights at the beginning of each trajectory)
(Deisenroth et al., 2013), they have the advantages of being more consistent, structured, and global
(Deisenroth et al., 2013). Later, van Hoof et al. (2017) proposed a generalized exploration (GE)
scheme, bridging the gap between step-based and trajectory-based exploration in parameter space.
With the advance of deep RL, NoisyNet (Fortunato et al., 2018) and Parameter Space Noise for
Exploration (PSNE) (Plappert et al., 2018) were introduced, extending parameter-space exploration
strategies for policies using deep neural networks.
1
Under review as a conference paper at ICLR 2021
Although GE, NoisyNet, and PSNE improved over the vanilla exploration strategies in parameter
space and were shown leading to more global and consistent exploration, they still suffer from sev-
eral limitations. Given this, we propose a new exploration scheme with the following characteristics.
1.	Generalizing Step-based and Trajectory-based Exploration (van Hoof et al., 2017)
Since both NoisyNet and PSNE are trajectory-based exploration strategies, they are con-
sidered relatively inefficient and bring insufficient stochasticity (Deisenroth et al., 2013).
Following van Hoof et al. (2017), our method improves by interpolating between step-based
and trajectory-based exploration in parameter space, where a more balanced trade-off be-
tween stability and stochasticity can be achieved.
2.	Recursive Analytical Integration of Latent Exploring Policies NoisyNet and PSNE
address the uncertainty from sampling exploring policies using Monte Carlo integration,
while GE uses analytical integration on full trajectories, which scales poorly in the number
of time steps. In contrast, we apply analytical and recurrent integration after each step,
which leads to low-variance and scalable updates.
3.	Perturbing Last Layers of Policy Networks Both NoisyNet and PSNE perturb all lay-
ers of the policy network. However, in general, only the uncertainty in parameters of the
last (linear) layer can be integrated analytically. Furthermore, it is not clear that deep neu-
ral networks can be perturbed in meaningful ways for exploration (Plappert et al., 2018).
We thus propose and evaluate an architecture where perturbation is only applied on the
parameters of the last layer.
These characteristics define our contribution, which we will refer to as Deep Coherent Exploration.
We evaluate the coherent versions of A2C (Mnih et al., 2016), PPO (Schulman et al., 2017), and
SAC (Haarnoja et al., 2018), where the experiments on OpenAI MuJoCo (Todorov et al., 2012;
Brockman et al., 2016) tasks show that Deep Coherent Exploration outperforms other exploration
strategies in terms of both learning speed and stability.
2	Related Work
As discussed, exploration can broadly be classified into directed and undirected strategies (Thrun,
1992; Plappert et al., 2018), with undirected strategies being commonly used in practice because of
their simplicity. Well known methods such as -greedy (Sutton, 1995) or additive Gaussian noise
(Williams, 1992) randomly perturb the action at each time step independently. These high-frequency
perturbations, however, can result in poor coverage of the state-action space due to random-walk
behavior (Ruckstieβ et al., 2010; Deisenroth et al., 2013), washing-out of exploration by the envi-
ronment dynamics (Kober & Peters, 2008; RuckStieβ et al., 2010; Deisenroth et al., 2013), and to
potential damage to mechanical systems (Koryakovskiy et al., 2017).
One alternative is to instead perturb the policy in parameter space, with the perturbation held con-
stant for the duration of a trajectory. RuckstieB et al. (2010) and Sehnke et al. (2010) showed
that such parameter-space methods could bring improved exploration behaviors because of reduced
variance and faster convergence, when combined with REINFORCE (Williams, 1992) or Natural
Actor-Critic (Peters et al., 2005).
Another alternative to independent action-space perturbation, is to correlate the noise applied at
subsequent actions (Morimoto & Doya, 2000; Wawrzynski, 2015; Lillicrap et al., 2016), for exam-
ple by generating perturbations from an Ornstein-Uhlenbeck (OU) process (Uhlenbeck & Ornstein,
1930). Later, van Hoof et al. (2017) used the same stochastic process but in the parameter space
of the policy. This approach uses a temporally coherent exploring policy, which unifies step-based
and trajectory-based exploration. Moreover, the author showed that, with linear policies, a more
delicate balance between these two extreme strategies could have better performance. However, this
approach was derived in a batch mode setting and requires storing the full trajectory history and the
inversion of a matrix growing with the number of time step. Thus, it does not scale well to long
trajectories or complex models.
Although these methods pioneered the research of exploration in parameter space, their applicability
is limited. More precisely, these methods were only evaluated with extremely shallow (often linear)
policies and relatively simple tasks with low-dimensional state spaces and action spaces. Given this,
2
Under review as a conference paper at ICLR 2021
NoisyNet (Fortunato et al., 2018), PSNE (Plappert et al., 2018) and Stochastic A3C (SA3C) (Shang
et al., 2019) were proposed, introducing more general and scalable methods for deep RL algorithms.
All three of these methods can be seen as learning a distribution over policies for trajectory-based
exploration in parameter space. These exploring policies are sampled by perturbing the weights
across all layers of a deep neural network, with the uncertainty from sampling being addressed by
Monte Carlo integration. Whereas NoisyNet learns the magnitudes of the noise for each parameter,
PSNE heuristically adapts a single magnitude for all parameters.
While showing good performance in practice (Fortunato et al., 2018; Plappert et al., 2018), these
methods suffer from two potential limitations. Firstly, trajectory-based strategies can be inefficient
as only one strategy can be evaluated for a potentially long trajectory (Deisenroth et al., 2013),
which could result in a failure to escape local optima. Secondly, Monte Carlo integration results in
high-variance gradient estimates which could lead to oscillating updates.
3	Background
This section provides background for reinforcement learning and related deep RL algorithms.
3.1	Reinforcement Learning
Reinforcement learning is a sub-field of machine learning that studies how an agent learns strategies
with high returns through trial-and-error by interacting with an environment. This interaction be-
tween an agent and an environment is described using Markov Decision Processes (MDPs). A MDP
is a tuple (S, A, r, P, γ), where S is the state space, A is the action space, r : S × A × S → R is
the reward function with rt = r (st, at, st+1), P : S × A × S → R+ is the transition probability
function, and γ is a discount factor indicating the preference of short-term rewards.
In RL with continuous action space, an agent aims to learn a parametrized (e.g. Gaussian) policy
πθ (a|s) : S × A → R+, with parameters θ, that maximizes the expected return over trajectories:
J(θ) =
ET 〜p(τ ∣∏θ ) [R(τ)],	(1)
where τ = (s0, a0, ..., aT-1, sT) is a trajectory and R(τ) = PtT=0 γtrt is the discounted return.
3.2	Deep Reinforcement Leaning Algorithms
Deep reinforcement learning combines deep learning and reinforcement learning, where policies
and value functions are represented by deep neural networks for more sophisticated and powerful
function approximation. In our experiments, we consider the following three deep RL algorithms.
Advantage Actor-Critic (A2C) Closely related to REINFORCE (Williams, 1992), A2C is an
on-policy algorithm proposed as the synchronous version of the original Asynchronous Advantage
Actor-Critic (A3C) (Mnih et al., 2016). The gradient of A2C can be written as:
T-1
Vθ J (θ)=
ET 〜p(τ ∣θ) X Vθ log∏θ(at∣St) Aπθ (st, at) ,	(2)
t=0
where Aπθ (st, at ) is the estimated advantage following policy πθ .
Proximal Policy Optimization (PPO) PPO is an on-policy algorithm developed to determine
the largest step for update while still keeping the updated policy close to the old policy in terms
of KUnback-Leibler(KL) divergence. Instead of using a second-order method as in Trust Region
Policy Optimization (TRPO) (Schulman et al., 2015), PPO applies a first-order method and com-
bines several tricks to relieve the complexity. We consider the primary variant of PPO-Clip with the
following surrogate objective:
T-1
LθCkLIP (θ) =
ET〜P(T ∣θk ) X min (rt(θ), clip (rt(θ), 1 - , 1 + )) Atπθk	,	(3)
t=0
3
Under review as a conference paper at ICLR 2021
where rt(θ) = π (atlst∣ and E is a small threshold that approximately restricts the distance
πθk (at|st)
between the new policy and the old policy. In practice, to prevent the new policy from changing too
fast, the KL divergence from the new policy to the old policy approximated on a sampled batch is
often used as a further constraint.
Soft Actor-Critic (SAC) As an entropy-regularized (Ziebart et al., 2008) off-policy actor-critic
method (Lillicrap et al., 2016; Fujimoto et al., 2018) with a stochastic policy, SAC (Haarnoja et al.,
2018) learns the optimal entropy-regularized Q-function through ‘soft’ Bellman back-ups with off-
policy data:
Qn(s, a) = Es,〜p(s0∣s,a),ao〜∏(ao∣s0) [r + Y (Qn (s', a0) + αH (π (a0∣s0)))],	(4)
where H is the entropy and α is the temperature parameter. The policy is then learned by maximizing
the expected maximum entropy V -function via the reparameterization trick (Kingma et al., 2015).
4	Deep Coherent Exploration for Continuous Control
To achieve the desiderata in Section 1, we propose Deep Coherent Exploration, a method that models
the policy as a generative model with latent variables. This policy is represented as ∏wt,θ(at∣st)=
N(Wtfθ(st) + bt, Λa-1). Here wt denotes all last layer parameters of policy network at step t
by combining Wt and bt, θ denotes the parameters of the policy network except for the last layer,
and Λa is a fixed and diagonal precision matrix. Our method treats the last layer parameters wt as
latent variables with marginal distribution Wt 〜 N (μt, Λ-1), where μt and Z are functions of
learnable parameters μ and Λ respectively. In this model, all learnable parameters can be denoted
as Z = (μ, Λ, θ). We provide a graphical model of Deep Coherent Exploration in Appendix A.
As in van Hoof et al. (2017), Deep Coherent Exploration generalizes step-based and trajectory-based
exploration by constructing a Markov chain of Wt . This Markov chain specifies joint probabilities
through an initial distributionp0(w0) = N (μ, Λ-1) and the conditional distributionp(wt∣wt-ι).
This latter term explicitly expresses temporal coherence between subsequent parameter vectors. In
this setting, step-based exploration corresponds to the extreme case when p(wt|wt-1) = p0(wt),
and trajectory-based exploration corresponds to another extreme case whenp(wt|wt-1) = δ(wt -
wt-1), where δ is the Dirac delta function. To ensure the marginal distribution ofwt will be equal to
the initial distribution p0 at any step t, we directly follow van Hoof et al. (2017) with the following
transition distribution for wt :
p(wt∣wt-i) = N ((1 - β) wt-i + βμ, (2β - β2)Λ-1) ,	(5)
where β is a hyperparameter that controls the temporal coherency of wt and wt-1. Then, the two
extreme cases corresponds to β = 0 for trajectory-based exploration and β = 1 for step-based
exploration, while the intermediate exploration corresponds to β ∈ (0, 1). For intermediate values
of β, we obtain smoothly changing policies that sufficiently explore, while reducing high-frequency
perturbations.
4.1	On-Policy Deep Coherent Exploration
Our method can be combined with all on-policy policy gradient methods and here we present this
adaptation with REINFORCE (Williams, 1992). Starting from the RL objective in Equation 1:
NZ J(Z)= ET〜p(τ∣ζ) [Vζ logp(τ|Z)R(τ)],	(6)
the gradients w.r.t the sampled trajectory can be obtained using standard chain rule:
T-1
VZ logp(τ|Z) = E (VZ logp(at|s[0：t], a[o：t-i],Z)).	⑺
t=0
4
Under review as a conference paper at ICLR 2021
Here, since information can still flow through the unobserved latent variable wt , our policy is not
Markov anymore. To simplify this dependency, we introduce wt into p(at|s[0:t], a[0:t-1] , ζ):
p(at |s[0:t], a[0:t-1], ζ) =	p(at, wt |s[0:t], a[0:t-1], ζ)dwt
=/	P(at|st； wt, θ) P(wt|s[o：t-i], a[θ:t-i], μ, Λ, θ) dwt,
J '-----------V--------} I------------------V-------------------}
Gaussian policy πwt ,θ (at |st)	forward message α(wt)
(8)
(9)
where the first factor is the Gaussian action probability and the second factor can be interpreted as
the forward message α(wt) along the chain. We decompose α(wt) by introducing wt-1 :
α(wt) =	p(wt, wt-1 |s[0:t-1], a[0:t-1], ζ)dwt-1	(10)
p(at-1|st-1; wt-1, θ)α(wt-1)
=	P(wt|wt-i； μ, Λ) -------------------------------------dwt-i,	(11)
J 1----------{--------}	Zt-I
transition probability of wt
where the first factor is the transition probability of wt (Equation 5) and Zt-1 in the factor term is a
normalizing constant. Since Gaussians are closed under marginalization and condition, the second
factor can be obtained analytically without the need of computing the normalizing constant Zt-1.
Moreover, α(wt-1) is a Gaussian by mathematical induction from the initial step. As a result,
we arrive at an efficient recursive expression for exact inference of wt . Again, with the property
of Gaussians, all integrals appearing above can be solved analytically, where the marginal action
probability given the history at each step t can be obtained and used for policy updates.
Summarizing, non-Markov policies require substituting the regular p(at|st; θ) term in the update
equation with p(at|s[0:t] , a[0:t-1], ζ) (Equation 7). Equations 8-11 show how this expression can
be efficiently calculated recursively. Except for this substitution, learning algorithms like A2C and
PPO can proceed as normal. For detailed mathematical derivation, please refer to Appendix B.
4.2 Off-Policy Deep Coherent Exploration
Combining our method with off-policy methods (Lillicrap et al., 2016; Fujimoto et al., 2018;
Haarnoja et al., 2018) requires defining both the behavior policy and the update equation. The
behavior policy is the same as in the on-policy methods discussed earlier (Equation 5). The policy
update procedure may require adjustments for specific algorithms. Here, we show how to adapt
our method for SAC (Haarnoja et al., 2018). In the SAC policy update, the target policy is adapted
towards the exponential of the new Q-function. The target policy here is the marginal policy, rather
than the policy conditioned on the sampled w, as this second option would ignore the dependence
on μ and Λ. This consideration leads to the following objective for policy update:
J(Z) = Est〜D KL (P (at MZ)Il exp(*；tj at)))[	(12)
= Est~D,at~p(at∣st ,Z) [logp(at |st, Z) -Qφ (st, at)] ,	(13)
where φ denotes the parameters of the Q-function andp (at|st, Z) is the marginal policy which can
again be obtained analytically (Equation 22):
P (at∣St,Z) = /	p (at|st； W0, θ)	p(wo∣μ, Λ)	dwo,	(14)
J '----------V-------}	'-----V---}
Gaussian policy πw0,θ (at |st) marginal probability ofw0
where all parameters can be learned via the reparameterization trick (Kingma et al., 2015).
5 Experiments
For the experiments, we compare our method with NoisyNet (Fortunato et al., 2018), PSNE (Plap-
pert et al., 2018) and standard action noise. This comparison is evaluated in combination of A2C
(Mnih et al., 2016), PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018) on OpenAI Gym
MuJoCo (Todorov et al., 2012; Brockman et al., 2016) continuous control tasks.
5
Under review as a conference paper at ICLR 2021
For exploration in parameter space, we use a fixed action noise with a standard deviation of 0.1.
For A2C and PPO, their standard deviations of parameter noise are all initialized at 0.017, as sug-
gested in Fortunato et al. (2018). For SAC, we initialize the standard deviation of parameter noise
at 0.034 for both our method and PSNE as it gave better results in practice. Besides, Deep Coherent
Exploration learns the logarithm of parameter noise, while NoisyNet learns the parameter noise di-
rectly, and PSNE adapts the parameter noise. To protect the policies from changing too dramatically,
we consider three small values of β (0.0, 0.01, and 0.1) for Deep Coherent Exploration, where we
use β = 0.01 for comparative evaluation with other exploration strategies. Our implementation of
NoisyNet is based on the code from Kaixhin1. For PSNE, we refer to author’s implementation in
OpenAI Baselines2 (Dhariwal et al., 2017) and the original paper, where we set the KL threshold
for A2C and PPO to 0.01 and the MSE threshold for SAC to 0.1. On the other hand, exploration
with action noise uses the default setting proposed by Achiam (2018), where the standard deviation
of action noise is initialized at around 0.6 for A2C and PPO. For SAC, in the baseline setting the
standard deviation of action noise is output by the policy network.
In all experiments, agents are trained with a total of 106 environmental steps, where they are updated
after each epoch. A2C and PPO use four parallel workers, where each worker collects a trajectory
of 1000 steps for each epoch, resulting in epochs with 4000 steps in total. After each epoch, both
A2C and PPO update their value functions for 80 gradient steps. At the same time, A2C updates its
policy for one gradient step, while PPO updates its policy for up to 80 gradient steps until the KL
constraint is satisfied. SAC uses a single worker, with a step size of 4000 for each epoch. After every
50 environmental steps, both policy and value functions are updated for 50 gradient steps. To make
the parameter noise stable, we adapt the standard deviation of parameter noise after each epoch.
Our implementation of A2C, PPO, and SAC with different exploration strategies are adapted based
on OpenAI Spinning Up (Achiam, 2018) with default settings. All three algorithms use two-layer
feedforward neural networks with the same network architectures for both policy and value func-
tion. To be more precise, A2C and PPO use network architectures with 64 and 64 hidden nodes
activated by tanh units. In comparison, SAC uses a network architecture of 256 and 256 hidden
nodes activated by rectified linear units (ReLU). Parameters of policies and value functions in all
three algorithms are updated using Adam (Kingma & Ba, 2015). A2C and PPO use a learning rate
of 3 ∙ 10-4 for the policies and a learning rate of 10-3 for the value functions. SAC uses a single
learning rate of 10-3 for both policy and value function.
For each task, we evaluate the performance of agents after every 5 epochs, with no exploration
noise. Additionally, each evaluation reports the average reward of 10 episodes for each worker.
To mitigate the randomness within environments and policies, we report our results as the average
over 10 random seeds. All settings not explicitly explained in this section, are set to the code base
defaults. For more details, please refer to documents and source code from OpenAI Spinning Up
(Achiam, 2018) and our implementation3.
5.1	Comparative Evaluation
In this section, we present the results for A2C (Mnih et al., 2016), PPO (Schulman et al., 2017),
and SAC (Haarnoja et al., 2018) on three control tasks, with the additional results shown in Ap-
pendix D. Figure 1 shows that, overall, Coherent-A2C outperforms all other A2C-based methods
in terms of learning speed, final performance, and algorithm stability. In particular, given that Ant-
v2 is considered a challenging task, Deep Coherent Exploration considerably accelerates learning
speed. For PPO-based method, our method still outperforms NoisyNet and PSNE significantly in
all tasks. However, our method’s advantage compared to standard action noise is smaller. Particu-
larly, Coherent-PPO underperforms PPO in Walker2d-v2. Two reasons might explain this. Firstly,
some environments might be more unstable and require a larger degree of exploration, which fa-
vors PPO as it initializes its action noise with much greater value. Secondly, because of having
extra parameters, policies of Coherent-PPO, NoisyNet-PPO, and PSNE-PPO tend to satisfy the KL
constraint in fewer update steps, which leads to slower learning. For SAC, the advantages of our
method are smaller compared to A2C and PPO. More specifically, Coherent-SAC learns slightly
1https://github.com/Kaixhin/NoisyNet-A3C
2https://github.com/openai/baselines/tree/master/baselines/ddpg
3Source code to be released after review
6
Under review as a conference paper at ICLR 2021
NoisyNet-A2C
Walker2d-v2
Em①」①6P」①><
(beta=0.01)
Em①」①6已①><
Total environmental steps (Ie6)
NoisyNet-PPO
Walker2d-v2
Coherent-SAC (beta=0.01) PSNE-SAC
Walker2d-v2
Figure 1: Learning curves for deep RL algorithms With different exploration strategies on OPenAI
MuJoCo continuous control tasks, where the top, middle and bottom row corresponds to results of
A2C, PPO, and SAC respectively. The solid curves correspond to the mean, and the shaped region
represents half a standard deviation of the average return over 10 random seeds.
faster than SAC in HalfCheetah-v2 and achieves the highest average returns in Walker2d-v2 and
Ant-v2. Furthermore, Coherent-SAC shows variance lower than PSNE-SAC but higher than the
baseline SAC.
5.2	Ablation Studies
In this section, we present three separate ablation studies to clarify the effect of each characteristic
discussed in Section 1. These ablation studies are performed with A2C, to ensure that all characteris-
tics are applicable and because the fixed number of gradient steps puts the methods on equal footing.
Here we show the results for HalfCheetah-v2, where the full results can be found in Appendix D.
Generalizing Step-based and Trajectory-based Exploration As shown in Figure 2a, we evalu-
ate three different values 0.0, 0.01, 0.1 ofβ for Coherent-A2C. Here both two intermediate strategies
(β = 0.01 and β = 0.1) outperforms the trajectory-based strategy (β = 0.0). Coherent-A2C with
β = 0.01 seems to achieve the best balance between randomness and stability, with a considerably
higher return than the other two.
Analytical Integration of Latent Exploring Policies We introduce OurNoisyNet for compari-
son. OurNoisyNet equips a noisy linear layer for only its last layer, and this layer learns the logarithm
of standard deviation, as in Deep Coherent Exploration. We compare Coherent-A2C using β = 0.0
and OurNoisyNet-A2C, with the only difference thus being whether we integrate analytically or use
the reparameterization trick (Kingma et al., 2015).
7
Under review as a conference paper at ICLR 2021
Coherent-A2C (beta=O.O)
^ohθrθ∩t-A2C (bʤ=0.1)
Cpherent-AzC (beta=0.0)
1et-A2C (last layer)
Jet-A2C (all layers)
OurNoisyNi
—OurNoisyN(
(a)	(b)	(C)
Figure 2: Results of Coherent-A2C With different settings for HalfCheetah-v2, where Figure 2a and
Figure 2b show the learning curves and Figure 2c shows the average log variance of gradients during
six stages in learning. The solid curves correspond to the mean, and the shaped region represents
half a standard deviation of the average return over 10 random seeds.
We first measure the variance of gradient estimates in both methods. This variance is measured by
computing the trace of the covariance matrix using 10 gradient samples. We report this measure
in six stages during training, as shown in Figure 2c. We can observe that analytical integration
leads to lower-variance gradient estimates across all training stages for HalfCheetah-v2. We further
present the learning curves of both methods in Figure 2b, where Coherent-A2C with β = 0.0
shows higher return than OUrNOiSyNet-A2C. It is interesting that Coherent-A2C displays a much
lower standard deviation across different random seeds. Furthermore, the lower-variance gradient
estimates of Coherent-A2C could enable a larger learning rate for faster training without making
policy updates unstable.
Perturbing Last Layers of Policy Networks In this part, we compare OurNoisyNet-A2C per-
turbed over all layers, and OurNoisyNet-A2C perturbed over only the last layer. The result is shown
in Figure 2b. Somewhat to our surprise, the latter seems to perform much better. There are several
possible reasons. Firstly, since it is unknown how the parameter noise (especially in lower layers)
is realized in action noise, perturbing all layers of the policy network may lead to uncontrollable
perturbations. Such excess exploratory noise could inhibit exploitation. Secondly, perturbing all
layers might disturb the representation learning of states, which is undesirable for learning a good
policy. Thirdly, perturbing only the last layer could also lead to fewer parameters for NoisyNet.
6 Conclusion
In this paper, we have presented a general and scalable exploration framework that extends the
generalized exploration scheme (van Hoof et al., 2017) for continuous deep RL algorithms. In par-
ticular, recursive calculation of marginal action probabilities allows handling long trajectories and
high-dimensional parameter vectors. Compared with NoisyNet (Fortunato et al., 2018) and PSNE
(Plappert et al., 2018), our method has three improvements. Firstly, Deep Coherent Exploration
generalizes step-based and trajectory-based exploration in parameter space, which allows a more
balanced trade-off between stochasticity and coherence. Secondly, Deep Coherent Exploration an-
alytically marginalizes the latent policy parameters, yielding lower-variance gradient estimates that
stabilize and accelerate learning. Thirdly, by perturbing only the last layer of the policy network,
Deep Coherent Exploration provides better control of the injected noise.
When combining with A2C (Mnih et al., 2016), PPO (Schulman et al., 2017), and SAC (Haarnoja
et al., 2018), we empirically show that Deep Coherent Exploration outperforms other exploration
strategies on most of the MuJoCo continuous control tasks tested. Furthermore, the ablation studies
show that, while each of the improvements is beneficial, combining them leads to even faster and
more stable learning. For future work, since Deep Coherent Exploration uses a fixed and small action
noise, we believe one interesting direction is to study whether the learnable perturbations in action
space can be combined with our method in a meaningful way for even more effective exploration.
8
Under review as a conference paper at ICLR 2021
References
Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018. URL https://
spinningup.openai.com/en/latest/.
Christopher M. Bishop. Pattern recognition and machine learning, 5th Edition. Information science
and statistics. Springer, 2007. ISBN 9780387310732. URL http://www.worldcat.org/
oclc/71008143.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Marc Peter Deisenroth, Gerhard Neumann, and Jan Peters. A survey on policy search for robotics.
Foundations and Trends in Robotics, 2(1-2):1-142, 2013. doi: 10.1561/2300000021. URL
https://doi.org/10.1561/2300000021.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Matteo Hessel, Ian Os-
band, Alex Graves, Volodymyr Mnih, Remi Munos, Demis Hassabis, Olivier PietqUin, Charles
Blundell, and Shane Legg. Noisy networks for exploration. In 6th International Conference
on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018,
Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/
forum?id=rywHCPkAW.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th Inter-
national Conference on Machine Learning, ICML2018, Stockholmsmassan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 1582-1591.
PMLR, 2018. URL http://proceedings.mlr.press/v80/fujimoto18a.html.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 1856-1865. PMLR, 2018. URL http://proceedings.
mlr.press/v80/haarnoja18b.html.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. VIME:
variational information maximizing exploration. In Daniel D. Lee, Masashi Sugiyama, Ulrike von
Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 1109-1117, 2016. URL http://papers.nips.cc/paper/
6591-vime-variational-information-maximizing-exploration.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. J. Mach. Learn. Res., 11:1563-1600, 2010. URL http://portal.acm.org/
citation.cfm?id=1859902.
Michael J. Kearns and Satinder P. Singh. Near-optimal reinforcement learning in polynomial time.
Mach. Learn., 49(2-3):209-232, 2002. doi: 10.1023/A:1017984413808. URL https://doi.
org/10.1023/A:1017984413808.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Diederik P. Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameter-
ization trick. CoRR, abs/1506.02557, 2015. URL http://arxiv.org/abs/1506.02557.
9
Under review as a conference paper at ICLR 2021
Jens Kober and Jan Peters. Policy search for motor primitives in robotics. In Daphne Koller,
Dale Schuurmans, Yoshua Bengio, and Leon BottoU (eds.), Advances in Neural Information
Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural In-
formation Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008,
pp. 849-856. Curran Associates, Inc., 2008. URL http://papers.nips.cc/paper/
3545-policy- search-for-motor-primitives-in-robotics.
Ivan Koryakovskiy, Heike Vallery, Robert Babuska, and Wouter Caarls. Evaluation of physical dam-
age associated with action selection strategies in reinforcement learning. IFAC-PapersOnLine, 50
(1):6928-6933, 2017. ISSN 1474-6670. doi: 10.1016/j.ifacol.2017.08.1218. URL https:
//www.ifac2017.org. 20th World Congress of the International Federation of Automatic
Control (IFAC), 2017, IFAC 2017 ; Conference date: 09-07-2017 Through 14-07-2017.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:
//arxiv.org/abs/1509.02971.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd
International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-
24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 1928-1937. JMLR.org,
2016. URL http://proceedings.mlr.press/v48/mniha16.html.
Jun Morimoto and Kenji Doya. Acquisition of stand-up behavior by a real robot using hierarchical
reinforcement learning. In Pat Langley (ed.), Proceedings of the Seventeenth International Con-
ference on Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 -
July 2, 2000, pp. 623-630. Morgan Kaufmann, 2000.
Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, and Remi Munos. Count-based explo-
ration with neural density models. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the
34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 2721-2730. PMLR,
2017. URL http://proceedings.mlr.press/v70/ostrovski17a.html.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the
34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 2778-2787. PMLR,
2017. URL http://proceedings.mlr.press/v70/pathak17a.html.
Jan Peters, Sethu Vijayakumar, and Stefan Schaal. Natural actor-critic. In Joao Gama, Rui Ca-
macho, Pavel Brazdil, Allpio Jorge, and Luis Torgo (eds.), Machine Learning: ECML 2005,
16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005, Proceed-
ings, volume 3720 of Lecture Notes in Computer Science, pp. 280-291. Springer, 2005. doi:
10.1007∕11564096∖,29. URL https://doi.org/1O.1OO7/11564 0 96_2 9.
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for explo-
ration. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
URL https://openreview.net/forum?id=ByBAl2eAZ.
Thomas RUCkStieβ, Frank Sehnke, Tom Schaul, Daan Wierstra, Yi Sun, and JUrgen Schmidhu-
ber. Exploring parameter space in reinforcement learning. Paladyn J. Behav. Robotics, 1
(1):14-24, 2010. doi: 10.2478/s13230-010-0002-4. URL https://doi.org/1O.2478/
s1323O-O1O-OOO2-4.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust re-
gion policy optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
10
Under review as a conference paper at ICLR 2021
International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, vol-
Ume 37 of JMLR Workshop and Conference Proceedings,pp.1889-1897.JMLR.org, 2015. URL
http://proceedings.mlr.press/v37/schulman15.html.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Frank Sehnke, Christian Osendorfer, Thomas Ruckstieβ, Alex Graves, Jan Peters, and JUrgen
Schmidhuber. Parameter-exploring policy gradients. Neural Networks, 23(4):551-559, 2010. doi:
10.1016/j.neunet.2009.12.004. URL https://doi.org/10.1016/j.neunet.2009.
12.004.
Wenling Shang, Douwe van der Wal, Herke van Hoof, and Max Welling. Stochastic activation
actor critic methods. In Ulf Brefeld, EliSa Fromont, Andreas Hotho, Arno J. Knobbe, Mar-
loes H. Maathuis, and Celine Robardet (eds.), Machine Learning and Knowledge Discovery in
Databases - European Conference, ECML PKDD 2019, Wurzburg, Germany, September 16-20,
2019, Proceedings, Part III, volume 11908 of Lecture Notes in Computer Science, pp. 103-117.
Springer, 2019. doi: 10.1007∕978-3-030-46133-1∖.7. URL https://doi.org/10.1OO7/
978-3-030-46133-1_7.
Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse
coarse coding. In David S. Touretzky, Michael Mozer, and Michael E. Hasselmo (eds.), NIPS, pp.
1038-1044. MIT Press, 1995. ISBN 0-262-20107-0. URL http://dblp.uni-trier.de/
db/conf/nips/nips1995.html#Sutton95.
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,
Filip De Turck, and Pieter Abbeel. #exploration: A study of count-based exploration for deep re-
inforcement learning. CoRR, abs/1611.04717, 2016. URL http://arxiv.org/abs/1611.
O4717.
Sebastian Thrun. The role of exploration in learning control. In Handbook for Intelligent Control:
Neural, Fuzzy and Adaptive Approaches. Van Nostrand Reinhold, Florence, Kentucky, January
1992.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based con-
trol. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012,
Vilamoura, Algarve, Portugal, October 7-12, 2012, pp. 5026-5033. IEEE, 2012. doi: 10.1109/
IROS.2012.6386109. URL https://doi.org/1O.11O9/IROS.2O12.63861O9.
George. E. Uhlenbeck and Leonard S. Ornstein. On the Theory of the Brownian Motion. Physical
Review, 36(5):823-841, September 1930.
Herke van Hoof, Daniel Tanneberg, and Jan Peters. Generalized exploration in policy search. Mach.
Learn., 106(9-10):1705-1724, 2017. doi: 10.1007/s10994-017-5657-1. URL https://doi.
org/1O.1OO7/s1O994-O17-5657-1.
Pawel Wawrzynski. Control policy with autocorrelated noise in reinforcement learning for robotics.
International Journal of Machine Learning and Computing, 5:91-95, 04 2015. doi: 10.7763/
IJMLC.2015.V5.489.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Mach. Learn., 8:229-256, 1992. doi: 10.1007/BF00992696. URL https://doi.
org/1O.1OO7/BFOO992696.
Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy
inverse reinforcement learning. In Dieter Fox and Carla P. Gomes (eds.), Proceedings of the
Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July
13-17, 2008, pp. 1433-1438. AAAI Press, 2008. URL http://www.aaai.org/Library/
AAAI/2OO8/aaaiO8-227.php.
11
Under review as a conference paper at ICLR 2021
A Graphical Model of Deep Coherent Exploration
In this appendix, we provide a graphical model representation of Deep Coherent Exploration, shown
in Figure 3. This graphical model uses the same conventions as in Bishop (2007), where empty
circles denote latent random variables, shaded circles denote observed random variables, and dots
denote deterministic variables.
Figure 3: Graphical model of Deep Coherent Exploration.
B Marginal Action Probability for On-Policy Coherent
Exploration
As discussed in Section 4.1, forward message α(wt) is used to compute the marginal action prob-
ability given the history at step t for the final learning objective. Suppose we have the Gaussian
policy as:
∏wt,θ(at|st) = N(WtXt + bt, Λ-1),
(15)
where at ∈ Rp, Xt = fθ(st) ∈ Rq, Wt ∈ Rp×q is the coefficient matrix, bt ∈ Rp is the bias vector
and Λa is a constant precision matrix for the Gaussian policy. It’s helpful to represent wt ∈ Rpq+p
by flattening Wt and combining bt :
w11
w1q
wt
wp1
.
.
.
wpq
b1
bp
(16)
12
Under review as a conference paper at ICLR 2021
such that the parameters could still be sampled using multivariate Gaussians. Moreover, we stack xt
into Xt ∈ Rp×(pq+p) :
	(XT	0TT1 0qT,1	xtT	. . .	0T 1 . . .	0qqT,,11	0qT,1 0qT,1	1 0	0 1	...	0 ...	0	0 ∖ 0
Xt =	.. .. ..	.. .. ..	. . .	. . .	. . .	.. .. ..	. . .
	0qT,1	0qT,1	. . .	XtT	0qT,1	0	0	...	1	0
	0qT,1	0qT,1	. . .	0qT,1	XtT,	0	0	...	0	1
where 0q,1 is a q-dimension zero column vector. After this transformation, the Gaussian policy is
represented equivalently as:
(17)
πwt,θ(at∣st)= N(XtWt, Λ-1).
(18)
B.1 Base Case
For the base case t = 0, forward message α(W0) and the initial transition probability of W0 is
identical by definition:
α(wo) = po(wo； μ, Λ) = N (μ, Λ-1) .	(19)
Additionally, the action probability is given by:
∏wo,θ(ao∣so) = N(X0W0, Λ-1).	(20)
With the property of multivariate Gaussians, we obtain the marginal action probability given the
history at t = 0:
logp(ao∣S0, μ, a, θ)=log/ πw0,θ(a0∣s0)α(w0)dw0 =log N (Xo μ, A-1 + XoATXT).	(21) (22)
B.2 General Case
For the general case of step t > 0, we need the state st-1, action at-1 as well as mean and covariance
of forward message α(wt-1) stored from previous step. Suppose α(wt-1) is written as:
α(wt-1) =N(vt-1,Lt--11),	(23)
and the action probability from the previous step is given by:
∏wtτ,θ(at-ι∣st-ι) = N (Xt-IWt-ι, A-1) .	(24)
We have directly:
p(wt-i |s[o：t-i], a[o：t-i], μ, A, θ) = N (ut-i, ∑t-i),	(25)
with
ut-1 = Et-1 (XT-IAaat-1 + Lt-Ivt-1)	(26)
Σt-1 = (Lt-1 + Xt-IAa Xt-ι)-1.	(27)
Combining the transition probability of wt :
P(Wtwt-1； μ, A) = N ((1 - β)wt-i + βμ, (2β - β2)A-1) ,	(28)
we obtain the forward message α(wt):
α (Wt)= N (Vt,L-1) ,	(29)
where
Vt = (1 - β)ut-1 + βμ	(30)
Lt-1 = (2β - β2)A-1 + (1 - β)2Σt-1.	(31)
Here, Vt and Lt-1 should be stored and used for exact inference of α(wt+1) at the next step. Finally,
the marginal action probability given the history at step t > 0 is given by:
logp(at|s[o：t], a[o：t-1], μ, A, θ) = log /Kwts(at∣st)α(wt)dwt
= log N (XtVt, Aa-1 + XtLt-1XtT).
(32)
(33)
13
Under review as a conference paper at ICLR 2021
C Deep Coherent Reinforcement Learning
Here, we provide a brief introduction of adapting Deep Coherent Exploration for A2C (Mnih et al.,
2016), PPO (Schulman et al., 2017) and SAC (Haarnoja et al., 2018). Respectively, we call them
Coherent-A2C, Coherent-PPO and Coherent-SAC.
C.1 Coherent Advantage Actor-Critic (Coherent-A2C)
Coherent-A2C is straightforward to implement. To do that, one could just replace the original A2C
gradient estimates Vθ J(θ) With the on-policy coherent gradient estimates Vm,a,θ J(μ, Λ, θ). The
pseudo-code of single-worker Coherent-A2C is shown in Algorithm 1.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Algorithm 1: Coherent-A2C
Input: initial policy parameters μo, Λo, θo, initial value function parameters φo.
for k=0,1,2,...,K do
Create a buffer Dk for collecting a trajectory τk With T steps.
for t=0,...,T do
if t=0 then
I Sample last layer parameters of policy network Wt 〜po(wt; μk, Λk) and store wt.
else
Sample last layer parameters of policy network Wt 〜P(Wt Wt-1 ； μk, Λk) and
L store Wt.
Observe state St and select action at 〜∏w5θk (at∣st).
Execute at in the environment.
Observe next state st+1, reward rt, done signal d, and store (st, at, rt, st+1, d) in buffer
Dk.
If st+1 is terminal, reset environment state.
Infer forward message α(Wt) using previous state st-1, previous action at-1 as well as
mean vt-1 and covariance Lt--* 1 * * *1 of previous forward message α(Wt-1).
Store mean vt and covariance Lt-1 of current forward message α(Wt).
Compute marginal action probability p(at|s[o：t], a[o：t—i], μk, Λk, θk).
Compute rewards-to-go Rt and any kind of advantage estimates At based on current value
function Vφk for all steps t.
Estimate gradient of the policy:
T-1
Vμ,A,θ J(μ, Λ, θ) = E Vμ,A,θ logp(at|s[0：t], a[θ:t-i], μk, A®, θk)At,
t=0
and update the policy by performing a gradient step:
,	T/ A Λ∖
μk+14-μk + αμVμJ(μ, A, θ)
A	A	O? J / A Λ∖
Ak+1 - Ak + aAVAJ(μ, A, θ)
θk+ι - θk + αθvθJ(μ, A, θ).
Learn value function by minimizing the regression mean-squared error:
1T
L(O) = τ E(VΦk (St)- Rt)),
T t=0
and update the value function by performing a gradient step:
φk + 1 - φk + αφv φL(φ).
14
Under review as a conference paper at ICLR 2021
C.2 Coherent Proximal Policy Optimization (Coherent-PPO)
Coherent-PPO can be implemented in a similar way. As in Coherent-A2C, we substitute the original
objective LCLIP⑹ With L∖LΛ θfc (μ, Λ, θ), which is given by:
LCL1Λk θ (μ, λ, θ)
,,
(34)
T-1
=ET ~p(τ ∣μk ,Λk ,θk) X [min (rt(μ, Λ, θ), clip (rt(μ, Λ, θ), 1 - e, 1 + e)) 4："人[,(35)
t=0
where rt(μ, Λ, θ)= 产廿m小啊―必个雪、.
tb, , j	p(at\s[0：t],a[0：t-1],μk,Λk,θk)
Here, after each step of policy update, p(at|s[o：t], a[o：t-i], μ, Λ, θ) from the new policy should be
evaluated on the most recent trajectory τk for both next update and approximated KL divergence.
However, this quantity can not be calculated directly, but only through sampling wt and then inte-
grating wt out. Since wt is integrated out in the end, it does not matter what specific wt is sampled.
So one could sample a new set of wt, or use a fixed w along the recent trajectory τk . The second
way is often faster because sampling is avoided. The pseudo-code of single-worker Coherent-PPO
is shown in Algorithm 2.
C.3 Coherent Soft Actor-Critic (Coherent-SAC)
For Coherent-SAC, only two changes are needed. Firstly, we sample the last layer parameters of the
policy network wt in each step t for exploration. Secondly, we improve the marginal policy instead
of the actual policy performing exploration after each epoch. The pseudo-code of single-worker
Coherent-SAC is shown in Algorithm 3.
D Additional Results
In this appendix, we provide additional results for both comparative evaluation and ablation studies.
D.1 Comparative Evaluation
The results of comparative evaluation for A2C (Mnih et al., 2016) and PPO (Schulman et al.,
2017) on Reacher-v2, InvertedDoublePendulum-v2 and Hopper-v2 are shown in Figure 4. For SAC
(Haarnoja et al., 2018), since it is a state-of-the-art deep RL algorithm, we only test it with three
of our six OpenAI MuJoCo continuous control tasks with highest state and action dimensions, as
shown in Figure 1.
D.2 Ablation Studies
The full results of all three ablation studies on all six OpenAI MuJoCo continuous control tasks are
shown in Figure 5, Figure 6, Figure 7 and Figure 8.
15
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
Algorithm 2: Coherent-PPO
Input: initial policy parameters μo, Λo, θo, initial value function parameters φo.
for k=0,1,2,...,K do
Create a buffer Dk for collecting a trajectory τk with T steps.
for t=0,...,T do
if t=0 then
I Sample last layer parameters of policy network Wt 〜po(wt; μk, Λk) and store w>
else
Sample last layer parameters of policy network Wt 〜p(wt∣wt-1； μk, Λk) and
L store wt.
Observe state St and select action at 〜∏wt,θk (at∣st).
Execute at in the environment.
Observe next state st+1, reward rt, done signal d, and store (st, at, rt, st+1, d) in buffer
Dk .
If st+1 is terminal, reset environment state.
Infer forward message α(wt) using previous state st-1, previous action at-1 as well as
mean vt-1 and covariance Lt--* 11 of previous forward message α(wt-1).
Store mean vt and covariance Lt-1 of current forward message α(wt).
Compute marginal action probability p(at|s[o：t], a[o：t—i], μk, Λk, θk).
Compute rewards-to-go Rt and any kind of advantage estimates At based on current value
function Vφk for all steps t.
Learn policy by maximizing the PPO-Clip objective:
T-1
LCLIPk ,θk 3, λ, θ) = X IminmM λ, θ), Clip(TtM λ, θ), 1 - e, 1 + e)) At],
t=0
and update the policy by performing multiple gradient steps until the constraint of
approximated KL divergence being satisfied:
μk+1 J μk + αμVμLCL,Λk,θk(μ, Z θ)
Ak+1 J Ak + qΛΛLμLΛk,θk(μ, λ, θ)
θk+1 J θk + αθ寸θLCLIP,θjμ, A, θ).
Learn value function by minimizing the regression mean-squared error:
1T
L(O) = T E (Vφk (St)- Rt),
T t=0
and update the value function by performing a gradient step:
φk + 1 J φk + αφVφL(φ).
16
Under review as a conference paper at ICLR 2021
Algorithm 3: Coherent-SAC
Input: initial policy parameters μ, Λ, θ, initial Q-function parameters φ1,φ2, empty replay
buffer D.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Set target parameters equal to main parameters φtarg,ι J φι, φtarg,2 J φ2.
for each step do
if just updated then
I Sample last layer parameters of policy network W 〜po (w; μ, Λ) and store W as WPrev.
else
Sample last layer parameters of policy network W 〜p(w|wprev； μ, Λ) and store W as
_ Wprev.
Observe state S and select action a 〜 ∏w,θ(a|s).
Execute a in the environment.
Observe next state s0, reward r, done signal d, and store (s, a, r, s0, d) in replay buffer D.
If s0 is terminal, reset environment state.
if it’s time to update then
for j in range(number of updates) do
Randomly sample a batch of transitions B = {(s, a, r, s0, d)}.
Compute targets for Q-functions:
y (r, s0,d) = r + Y(I — d) ( iɪnin Qφtarg,i (S0, aO) — α log πw,θ (a0|sO) J,
where a0 〜∏w,θ(a0∣s0).
Update Q-functions by one step of gradient descent using:
Vφi ∣Bb∣	X	(Qφi(s, a) - y (r, s0, d))2 , for i = 1, 2.
(s,a,r,s0,d)∈B
Update policy by one step of gradient ascent using:
V”,A,θ 同 X
min Qφi(s, a) - α logP (a|s, μ, Λ, θ),
i=1,2
where a is a sample from P (a|s, μ, Λ, θ) which is differentiable w.r.t μ, Λ, θ via
the reparameterization trick.
Update target networks with:
φtarg,i J ρφtarg,i + (1 - ρ)φi	for i = 1, 2.
else
L Continue.
17
Under review as a conference paper at ICLR 2021
Cohere∩t-A2C (beta=0.01)
NoisyNet-A2C PSNE-A2C A2C
Reacher-v2
InvertedDoubIePenduIum-V2
Hopper-v2
ITotaI environmental steps (Ie6)	Total environmental steps (Ie6)	Total environmental steps (Ie6)
Coherent-PPO (beta=0.01) NoisyNet-PPO PSNE-PPO PPO
Figure 4: Learning curves for deep RL algorithms with different exploration strategies, where the top
and bottom row corresponds to results of A2C and PPO respectively. The solid curves correspond
to the mean, and the shaped region represents half a standard deviation of the average return over 10
random seeds.
environmental steps (Ie6)
Reacher-v2
,Total environmental steps (Ie6)
InvertedDoubIePenduIum-V2
,Total environmental steps (Ie6)
Hopper-v2
Figure 5: Learning curves for Coherent-A2C on OpenAI MuJoCo continuous control tasks. The
solid curves correspond to the mean, and the shaped region represents half a standard deviation of
the average return over 10 random seeds.
18
Under review as a conference paper at ICLR 2021
Coherent-A2C (beta=O.O) OurNoisyNet-A2C
* ①JO ①OUpμ(D> 6O- φσεφ><
ITotaI environmental steps (Ie6)
Reacher-v2
Total environmental steps (Ie6)
Total environmental steps (Ie6)
InvertedDoubIePenduIum-VZ
Total environmental steps (Ie6)
SIU①Jo ①OUeμe> 60-
Figure 6: Log variance of gradient estimates for Coherent-A2C and OurNoisyNet-A2C on OpenAI
MuJoCo continuous control tasks. The solid curves correspond to the mean, and the shaped region
represents half a standard deviation of the average log variance over 10 random seeds.
Total environmental steps (Ie6)
InvertedDoubIePenduIum-VZ
Total environmental steps (Ie6)
Reacher-v2
Figure 7: Learning curves for Coherent-A2C and OurNoisyNet-A2C on OpenAI MuJoCo continu-
ous control tasks. The solid curves correspond to the mean, and the shaped region represents half a
standard deviation of the average return over 10 random seeds.
19
Under review as a conference paper at ICLR 2021
Reacher-v2
,Total environmental steps (Ie6)
lnvertedDoublePendulum-v2
u」君」ΦCTCΦ><
Figure 8: Learning curves for OurNoisyNet-A2C on OpenAI MuJoCo continuous control tasks. The
solid curves correspond to the mean, and the shaped region represents half a standard deviation of
the average return over 10 random seeds.
20