Under review as a conference paper at ICLR 2021
A Unified Framework for Convolution-based
Graph Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph Convolutional Networks (GCNs) have attracted a lot of research interest in
the machine learning community in recent years. Although many variants have
been proposed, we still lack a systematic view of different GCN models and deep
understanding of the relations among them. In this paper, we take a step forward
to establish a unified framework for convolution-based graph neural networks, by
formulating the basic graph convolution operation as an optimization problem in
the graph Fourier space. Under this framework, a variety of popular GCN models,
including the vanilla-GCNs, attention-based GCNs and topology-based GCNs,
can be interpreted as a same optimization problem but with different carefully de-
signed regularizers. This novel perspective enables a better understanding of the
similarities and differences among many widely used GCNs, and may inspire new
approaches for designing better models. As a showcase, we also present a novel
regularization technique under the proposed framework to tackle the oversmooth-
ing problem in graph convolution. The effectiveness of the newly designed model
is validated empirically.
1	Introduction
Recent years have witnessed a fast development in graph processing by generalizing convolution
operation to graph-structured data, which is known as Graph Convolutional Networks (GCNs) (Kipf
& Welling, 2017). Due to the great success, numerous variants of GCNs have been developed and
extensively adopted in the field of social network analysis (Hamilton et al., 2017; Wu et al., 2019a;
VeliCkovic et al., 2018), biology (Zitnik et al., 2018), transportation forecasting (Li et al., 2017) and
natural language processing (Wu et al., 2019b; Yao et al., 2019).
Inspired by GCN, a wide variety of convolution-based graph learning approaches are proposed to
enhance the generalization performance of graph neural networks. Several research aim to achieve
higher expressiveness by exploring higher-order information or introducing additional learning
mechanisms like attention modules. Although proposed from different perspectives, their exist some
connections between these approaches. For example, attention-based GCNs like GAT (VeliCkoviC
et al., 2018) and AGNN (Thekumparampil et al., 2018) share the similar intention by adjusting the
adjacency matrix with a function of edge and node features. Similarly, TAGCN (Du et al., 2017)
and MixHop (Kapoor et al., 2019) can be viewed as particular instances of PPNP (Klicpera et al.,
2018) under certain approximation. However, the relations among these graph learning models are
rarely studied and the comparisons are still limited in analyzing generalization performances on
public datasets. As a consequence, we still lack a systematic view of different GCN models and
deep understanding of the relations among them.
In this paper, we resort to the techniques in graph signal processing and attempt to understand
GCN-based approaches from a general perspective. Specifically, we present a unified graph convo-
lution framework by establishing graph convolution operations with optimization problems in the
graph Fourier domain. We consider a Laplacian regularized least squares optimization problem and
show that most of the convolution-based approaches can be interpreted in this framework by adding
carefully designed regularizers. Besides vanilla GCNs, we also extend our framework to formulat-
ing non-convolutional operations (Xu et al., 2018a; Hamilton et al., 2017), attention-based GCNs
(VeliCkoviC et al., 2018; Thekumparampil et al., 2018) and topology-based GCNS (Klicpera et al.,
2018; Kapoor et al., 2019), which cover a large fraction of the state-of-the-art graph learning ap-
1
Under review as a conference paper at ICLR 2021
proaches. This novel perspective provides a re-interpretation of graph convolution operations and
enables a better understanding of the similarities and differences among many widely used GCNs,
and may inspire new approaches for designing better models.
As a conclusion, we summarize our contributions as follow:
1.	We introduce a unified framework for convolution-based graph neural networks and interpret
various convolution filters as carefully designed regularizers in the graph Fourier domain, which
provides a general methodology for evaluating and relating different graph learning modules.
2.	Based on the proposed framework, we provide new insights on understanding the limitations of
GCNs and show new directions to tackle common problems and improve the generalization per-
formance of current graph neural networks in the graph Fourier domain. Additionally, the unified
framework can serve as a once-for-all platform for expert-designed modules on convolution-based
approaches, where newly designed modules can be easily implemented on other networks as a plug-
in module with trivial adaptations. We believe that our framework can provide convenience for
designing new graph learning modules and searching for better combinations.
3.	As a showcase, we present a novel regularization technique under the proposed framework to
alleviate the oversmoothing problem in graph representation learning. As shown in Section 4, the
newly designed regularizer can be implemented on several convolution-based networks and effec-
tively improve the generalization performance of graph learning models.
2	Preliminary
We start with an overview of the basic concepts of graph signal processing. Let G = (V, A)
denote a graph with node feature vectors where V represents the vertex set consisting of nodes
{v1, v2, . . . , vN} and A = (aij) ∈ RN×N is the adjacency matrix implying the connectivity be-
tween nodes in the graph. Let D = diag(d(1), . . . , d(N)) ∈ RN×N be the degree matrix of A
where d(i) = Pj∈V aij is the degree of vertex i. Then, L = D - A is the combinatorial Laplacian
and L = I - D(T/2) AD(T/2) j§ 昧 normalized LaPIacian of G. Additionally, we let A = A +1
and D = D + I denote the augmented adjacency and degree matrices with added self-looPs. Then
LSym = I - DT/2 ADT/2 (Asym = DT/2 ADT/2) and Lrw = I - DTA (Arw = DTA)
are the augmented symmetric normalized and random walk normalized LaPlacian (augmented adja-
cency matrices) of G, resPectively.
Letx ∈ RN be a signal on the vertices of the graPh. The sPectral convolution is defined as a function
of a filter gθ Parameterized in the Fourier domain (KiPf & Welling, 2017):
gθ ? x = Ugθ(Λ)UTx,	(1)
where U and Λ are the eigenvectors and eigenvalues of the normalized LaPlacian L. Also, we
follow Hoang & Maehara (2019) and define the variation ∆ and D-inner Product as:
∆(x) = X aij(x(i) - x(j))2 = XTLx,	(x, y)D = X(d⑺ + 1)x(i)y(i) = XTDy, (2)
i,j∈V	i∈V
which sPecifies the smoothness and imPortance of the signal resPectively.
3	Unified Graph Convolution Framework
With the success of GCNs, a wide variety of convolution-based aPProaches are ProPosed which Pro-
gressively enhance the exPressive Power and generalization Performance of graPh neural networks.
DesPite the effectiveness of GCN and its derivatives on sPecific tasks, there still lack a comPrehen-
sive understanding on the relations and differences among various graPh learning modules.
GraPh signal Processing is a Powerful technique which has been adoPted in several graPh learning
researches (KiPf & Welling, 2017; Hoang & Maehara, 2019; Zhao & Akoglu, 2019). However,
existing researches mainly focus on analyzing the ProPerties of GCNs while ignore the connec-
tions between different graPh learning modules. Innovatively, in this work, we consider interPreting
convolution-based aPProaches from a general PersPective with graPh signal Processing techniques.
2
Under review as a conference paper at ICLR 2021
In specific, we establish the connections between graph convolution operations and optimization
problems in graph Fourier space, showing the effect of each module explicitly with specific reg-
ularizers. This novel perspective provides a systematic view of different GCN models and deep
understanding of the relations among them.
3.1	Unified Graph Convolution Framework
Several researches have proved that, in the field of graph signal processing, the representative fea-
tures are mostly preserved in the low-frequency signals while noises are mostly contained in the
high-frequency signals (Hoang & Maehara, 2019). Based on this observation, numerous graph rep-
resentation learning methods are designed to decrease the high-frequency components, which can
be viewed as low-pass filters in the graph Fourier space. With similar inspiration, we consider a
Laplacian regularized least squares optimization problem with graph signal regularizers and attempt
to build connections with these filters.
Definition 1 Unified Graph Convolution Framework. Graph convolution filters can be achieved
by solving the following Laplacian regularized least squares optimization:
min X l∣x(i) - x(i)kD + λLreg,	(3)
X	i∈V
where ∣∣x∣∣d =，(x, X)D denotes the norm induced by DD.
In the following sections, we will show that a wide range of convolution-based graph neural net-
works can be derived from Definition 1 with different carefully designed regularizers, and provide
new insights on understanding different graph learning modules from the graph signal perspective.
3.1.1	Graph convolutional networks
Graph convolutional networks (GCNs) (Kipf & Welling, 2017) are the foundation of numerous graph
learning models and have received widespread concerns. Several researches have demonstrated that
the vanilla GCN is essentially a type of Laplacian smoothing over the whole graph, which makes
the features of the connected nodes similar. Therefore, to reformulate GCNs in the graph Fourier
space, we consider utilizing the variation ∆(x) as the regularizer.
Definition 2 Vanilla GCNs. Let X(ib∈v be the estimation of the input observation x(ib∈v. A
low-pass filter:
.ʌ 〜 ___________________________________________
X = ArwX,	(4)
is the first-order approximation of the optimal solution of the following optimization:
min Xllχ(i)- χ(i)kD + X aijkx⑶一χ(j)k2.	⑸
X i∈v	i,j∈v
Derivations of the definitions are presented in Appendix A.
As the eigenvalues of the approximated filter Arw are bounded by 1, it resembles a low-pass fil-
ter that removes the high-frequency signals. By exchanging Arw with Asym (which has the same
eigenvalues as Arw), we obtain the same formulation adopted in GCNs.
It has been stated that the second term ∆(x) in Eq.(5) measures the variation of the estimation x
over the graph structure. By adding this regularizer to the objective function, the obtained filter em-
phasizes the low-frequency signals through minimizing the variation over the local graph structure,
while keeping the estimation close to the input in the graph Fourier space.
3.1.2	Non-convolutional Operations
Residual Connection. Residual connection is first proposed by He et al. (2016) and has been
widely adopted in graph representation learning approaches. In the vanilla GCNs, norms of the
eigenvalues of the filter Arw (or Asym) are bounded by 1 which ensures numerical stability in the
training procedure. However, on the other hand, signals in all frequency band will shrink as the
convolution layer stacks, leading to a consistent information loss. Therefore, adding the residual
connection is deemed to preserve the strength of the input signal.
3
Under review as a conference paper at ICLR 2021
Definition 3 Residual Connection. A graph convolution filter with residual connection:
-ʌ 〜 ______ _____
X = ArwX + eX,	(6)
where > 0 controls the strength of residual connection, is the first-order approximation of the
optimal solution of the following optimization:
min X(kx(i) - χ(i)kD - ekχ(i)kD) + X aij kx(i) - x(j)k2.	⑺
X i∈V	i,j∈V
By adding the negative regularizer to penalize the estimations with small norms, we can induce the
same formulation as the vanilla graph convolution with residual connection.
Concatenation. Concatenation is practically a residual connection with different learning weights.
Definition 3’ Concatenation. A graph convolution filter concatenating with the input signal:
X = ArwX + EX ΘΘT,	(8)
is the first-order approximation of the optimal solution of the following optimization:
min X(kx(i)- χ(i)kD - ekx⑶θkD) + X aijkx(i)-χ(j)k2,	⑼
X i∈V	i,j∈V
where E > 0 controls the strength of concatenation and Θ is the learning coefficient.
Although the learning weights ΘΘT has a constrained expressive capability, it can be compensated
by the following feature learning modules.
3.1.3	Attention-based Convolutional Networks
Since the convolution filters in GCNs are dependent only on the graph structure, GCNs are proved
to have restricted expressive power and may cause the oversmoothing problem. Several researches
try to introduce the attention mechanism to the convolution filter, learning to assign different
edge weights at each layer based on nodes and edges. GAT (Velickovic et al., 2018) and AGNN
(Thekumparampil et al., 2018) compute the attention coefficients as a function of the features of con-
nected nodes, while ECC (Simonovsky & Komodakis, 2017) and GatedGCN (Bresson & Laurent,
2017) consider the activations for each connected edge. Although these approaches have different
insights, they can be all formulated as (See details in Appendix A):
pij = aij fθ (x(i), x(j), eij), i, j ∈ V,	(10)
where eij denotes the edge representation if applicable. Therefore, we replace aij in Definition 2
with learned coefficients to enforce different regularization strength on the connected edges.
Definition 4 Attention-based GCNs. An attention-based graph convolution filter:
X = PX,	(11)
is the first-order approximation of the optimal solution of the following optimization:
min X kx(i) - x(i)kD + X Pij l∣x(i) — X(j)k2,	s.t. XPij= Dii, ∀i ∈ V.	(12)
X i∈V	i,j∈V	j ∈V
Notice that we use a normalization trick to constrain the degree of attention matrix to be the same
as the original degree matrix D as we want to preserve the strength of the regularization for each
node. The formulated filter P corresponds to the matrix D-1p with row sum equals to 1, which is
also consistent with most of the attention-based approaches after normalization. Through adjusting
the regularization strength for edges, nodes with higher attention coefficients tend to have similar
features while the distance for nodes with low attention coefficients will be further.
3.1.4	Topology-based convolutional networks
Attention-based approaches are mostly designed based on the local structure. Besides focusing
on the first-order adjacency matrix, several approaches (Klicpera et al., 2018; 2019; Kapoor et al.,
2019; Du et al., 2017) propose to adopt the structural information in the multi-hop neighborhood,
4
Under review as a conference paper at ICLR 2021
which are referred to as topology-based convolutional networks. We start with an analysis of PPNP
(Klicpera et al., 2018) and then derive a general formulation for topology-based approaches.
PPNP. PPNP provides insights towards the propagation scheme by combining message-passing
function with personalized PageRank. As proved in (Xu et al., 2018b), the influence of node i
on node j is proportional to a k-step random walk, which converges to the limit distribution with
multiple stacked convolution layers. By involving the restart probability, PPNP is able to preserve
the starting node i’s information. Similarly, in Definition 2, the first term can also be viewed as a
regularization of preserving the original signal information. Therefore, we may achieve the same
purpose by adjusting the regularization strength.
Definition 5 PPNP. A graph convolution filter with personalized propagation (PPNP):
XX = α(In -(I - a)Arw)-1X,	(13)
is equivalent to the optimal solution of the following optimization:
min aX kx⑺一χ(i)kD +(I- α) X aijkx⑶一χ(j)k2,	(14)
X	i∈V	i,j∈V
where α ∈ (0, 1] is the restart probability.
Higher α means a higher possibility to teleport back to the starting node, which is consistent with
the higher regularization on the original signal in (14).
Multi-hop PPNP. One of the possible weakness of the original PPNP is that personalized PageRank
only utilizes the regularizer over the local structure. Therefore, we may improve the expressive
capability by involving multi-hop information, which is equivalent to adding regularizers for higher-
order variations.
Definition 6 Multi-hop PPNP. Let t be the highest order adopted in the algorithm. A graph convo-
lution filter with multi-hop personalized propagation (Multi-hop PPNP):
t
X = αo(In - X akAkw)TX,	(15)
k=1
where Ptk=0 αk = 1, α0 > 0 and αk ≥ 0, k = 1, 2, . . . , t, is equivalent to the optimal solution of
the following optimization:
t
min aoX llχ(i)- χ(i)kD + Xɑk X a(k)kχ(i)- x(j)k2,	(16)
i∈V	k=1	i,j∈V
(k)
where aij is proportional to the transition probability of the k-step random walk and the same
normalization trick in Section 3.1.3 is adopted on {ai(jk)}.
Solving Eq.(15) directly is computationally expensive. Therefore, we derive a first-order approxi-
mation by Taylor expansion and result in the form of:
T
X = (X αiArw)X + O(ATV X).	(17)
i=0
As the norm of the eigenvalues of Arw are bounded by 1, we can keep the first term in Eq.(17) as a
close approximation.
By comparing the approximated solution with topology-based graph convolutional networks, we
find that most of the approaches can be reformulated as particular instances of Definition 6. For
example, the formulation for Mixhop (Kapoor et al., 2019) can be derived as an approximation of
Eq.(17) if we let t = 2 and α0 = α1 = α2 = 1/3. Different learning weights can be applied to each
hop as Section 3.1.2 to concatenate multi-hop signals. See more examples in Appendix B.
3.2 Remarks
In this section, we build a bridge between graph convolution operations and optimization problems
in the graph Fourier space and provide insights into interpreting graph convolution operations with
regularizers. For conclusion, we rewrite the general form of the unified framework as follow.
5
Under review as a conference paper at ICLR 2021
Definition 1’ Unified Graph Convolution Framework. Convolution-based graph neural networks
can be reformulated (after approximation) as particular instances of the optimal solution of the
following optimization problem:
t
min a0 X(Hx⑶-x(i)kD -ekx⑶θkD )+ X ɑk X p(k)kx(i)θ(k) - x(j)θ(k)k2 +λLreg,
X	i∈V	'----{z---}	k=1	i,j∈VX-------------{-------------}
Non-Conv	Attention-based
S----------------V-----------------}
Topology-based
(18)
where P]=。αk = 1, αk ≥ 0 and Pj∈vPij = Dii,* ∈ V.
If we let d be the feature dimension of X, then Θ, Θ(k) ∈ Rd×d are the corresponding learning
weights. Lreg corresponds to the personalized regularizer based on the framework, which can be
effective if carefully designed as we will show in Section 4.
By establishing the unified framework, we interpret various convolution filters as carefully designed
regularizers in the graph Fourier domain, which provides new insights on understanding graph learn-
ing modules from the graph signal perspective. Several graph learning modules are reformulated as
smoothing regularizers over the graph structure with different intentions. While vanilla GCNs fo-
cus on minimizing the variation over the local graph structure, attention-based and topology-based
GCNs take a step forward and concentrate on the differences between connected edges and graph
structure with larger receptive field. This novel perspective enables a better understanding of the
similarities and differences among many widely used GCNs, and may inspire new approaches for
designing better models.
4 Tackling Oversmoothing under the Unified Framework
Based on the proposed framework, we provide new insights on understanding the limitations of
GCNs and inspire a new line of work towards designing better graph learning models. As a show-
case, we present a novel regularization technique under the framework to tackle the oversmooth-
ing problem. It is shown that the newly designed regularizer can be easily implemented on other
convolution-based networks with trivial adaptations and effectively improve the generalization per-
formances of graph learning approaches.
4.1 Regularization on Feature Variance
Here, we adopt the definition of feature-wise oversmoothing in (Zhao & Akoglu, 2019). Due to
multiple layers of Laplacian smoothing, all features fall into the same subspace spanned by the
dominated eigenvectors of the normalized adjacency matrix, which also corresponds to the similar
situation described in (Klicpera et al., 2018). To tackle this problem, we propose to penalize the
features when they are close to each other. Specifically, we consider the pairwise distance between
normalized features, which is summarized as:
δ(X) = d12 X，,Mkx∙i∕kx∙ik- x∙j/kx∙jkk2,	(19)
d	i,j ∈d
where d is the feature dimension and x∙i ∈ Rn represents the i-th dimension for all nodes. There-
fore, Eq.(19) can be interpreted as a feature variance regularizer, representing the distance between
features after normalization. By adding this regularizer to the unified framework, the proposed filter
should have the property to drive different features away.
Definition 7 Regularized Feature Variance. Let 0 be the Kronecker product operator Vec(X) ∈
Rnd be the vectorized signal X. Let DX be a diagonal matrix whose value is defined by DX (i, i) =
kx∙i∣∣2. A graph convolution filter with regularizedfeature variance:
Vec(X) = (In 0 [(αι + α2)I - α2Arw] - α3D-I(I - d 11T)D-1] 0 DT)-1vec(X) (20)
is equivalent to the optimal solution of the following optimization:
minɑιX kx(i)-x(i)kD + α2 X aijkx(i)—x(j)k2-£ X kx.i/kx.ik-x∙j∕kx∙jkk2, QI)
X	i∈V	i,j∈V	i,j∈d
6
Under review as a conference paper at ICLR 2021
Table 1: Test accuracy (%) on transductive learning datasets. We report mean values and standard
deviations in 30 independent experiments. The best results are highlighted with boldface.
Method	Dataset	Citeseer	Cora	Pubmed
	FastGCN (Chen et al., 2018)	68.8±0.6	-79.8±0.3	-76.8±0.6
	DGI(Velickovic et al., 2019)	71.8±0.7	82.3±0.6	76.8±0.6
Vanilla	GIN (Xu et al., 2018a)	66.1±0.9	77.6±1.1	77.0±1.2
	SGC (Wuet al.,2019a)	71.9±0.1	81.0±0.0	78.9±0.0
	GCN (KiPf & Welling, 2017)	70.3±0.4	81.5±0.5	79.0±0.4
	GCN+reg (ours)	72.2±0.4	83.6±0.3	79.8±0.2
	AGNN (ThekumParamPil et al., 2018)	71.6±0.5	82.7±0.4	78.9±0.4
	GatedGCN (Bresson & Laurent, 2017)	72.0±0.4	82.4±0.6	78.9±0.3
Attention	MoNet (Monti et al., 2017)	-	81.7±0.5	78.8±0.4
	GAT (Velickovic et al., 2018)	72.5±0.7	83.0±0.6	78.5±0.3
	GAT+reg (ours)	73.3±0.4	83.9±0.6	80.3±0.3
	TAGCN (Du etal., 2017)	70.9	825	811
Topology	MixHop (Kapoor et al., 2019)	71.4±0.8	81.9±0.4	80.8±0.6
	APPNP (Klicpera et al., 2018)	70.5±0.9	82.7±0.8	79.4±0.6
	APPNP+reg (ours)	71.9±0.4	84.0±0.6	-80.2±0.3
where ɑι > 0, α2,α3 ≥ 0. For CompUtation efficiency, we approximate ∣∣X∙ik with ∣∣x∙i∣∣ as we
assume that a single convolution filter provides little effect to the norm of features.
Calculating the Kronecker product and inverse operators are computationally expensive. Neverthe-
less, we can approximate Eq.(20) via Taylor expansion with an iterative algorithm. If we let:
A = (αι + α2)I - α2Arw,	B = In,	(22)
C = -α3D-1,	D = D-1(1 - 111T )D-1.	(23)
xdx
Then, a t-order approximated formulation is summarized as:
X ⑼=X,	(24)
X (k+1) = X + X(k) - AX(k) B - CX (k)D,	k = 0,1,...,t - 1.	(25)
Through approximation, computation overhead is greatly reduced. See details in the Appendix A.
As far as we are concerned, the advantages of utilizing feature variance regularization are three-
fold. First, the regularizer measures the difference between features, therefore explicitly preventing
all features from falling into the same subspace. Second, the modified convolution filter does not
require additional training parameters, avoiding the risk of overfitting. Third, the regularizer is
designed based on the proposed unified framework, which means it can be easily implemented on
other convolution-based networks as a plug-in module.
4.2 Discussion
Several researches have also shared insights on understanding and tackling oversmoothing. It is
shown in (Li et al., 2018) that the graph convolution of GCN is a special form of Laplacian smooth-
ing and the authors try to compensate the long-range dependencies by co-training GCN with a
random walk model. JKNet (Xu et al., 2018b) proved that the influence score between nodes con-
verges to a fixed distribution when layer stacks, therefore losing local information. As a remedy,
they proposed to concatenate layer-wise representations to perform mixed structural information.
More recently, Oono & Suzuki (2020) theoretically demonstrated that graph neural networks lose
expressive power exponentially due to oversmoothing. Comparing to the aforementioned researches,
our proposed method acts explicitly on the graph signals and can be easily implemented on other
convolution-based networks as a plug-in module with trivial adaptations.
7
Under review as a conference paper at ICLR 2021
(a) Accuracy	(b) Mean Feature Variance
Figure 1: Accuracy and mean feature variance on Cora. Use GCN and GAT for comparison.
4.3 Experiment
To testify the effectiveness of the regularizer, we empirically validate the proposed method on sev-
eral widely used semi-supervised node classification benchmarks, including transductive and in-
ductive settings. As we have stated in Section 4.1, our regularizer can be implemented on vari-
ous convolution-based approaches under the unified graph convolution framework. Therefore, we
consider three different versions by implementing the regularizer on vanilla-GCNs, attention-based
GCNs and topology-based GCNs. We achieve state-of-the-art results on almost all of the settings
and show the effectiveness of tackling oversmoothing on graph-structured data.
Dataset and Experimental Setup. We conduct experiments on four real-world graph datasets. For
transductive learning, we evaluate our method on the Cora, Citeseer, Pubmed datasets, following
the experimental setup in (Sen et al., 2008). PPI (Zitnik & Leskovec, 2017) is adopted for induc-
tive learning. Dataset statistics and more experimental setups are presented in Appendix C. For
comparison, we categorize state-of-the-art convolution-based graph neural networks into three spe-
cific classes, corresponding to the three versions of our proposed method. The first category is
based on the vanilla-GCN proposed by Kipf & Welling (2017), including GCN, FastGCN (Chen
et al., 2018), SGC (WU et al., 2019a), GIN (XU et al., 2018a), and DGI (VeIickoVic et al., 2019).
Since GIN is not initially evaluated on citation networks, we implement GIN following the setting
in (XU et al., 2018a). The second category corresponds to the attention-based approaches, inclUd-
ing GAT (Velickovic et al., 2018), AGNN (ThekUmParamPiI et al., 2018), MoNet (Monti et al.,
2017) and GatedGCN (Bresson & LaUrent, 2017). The last category of approaches is topology-
based GCNs which Utilizes the strUctUral information in the mUlti-hoP neighborhood. We consider
APPNP (KlicPera et al., 2018), TAGCN (DU et al., 2017) and MixHoP (KaPoor et al., 2019) as the
baselines.
Transductive Learning. Table 1 Presents the
Performance of oUr method and several state-
of-the-art graPh neUral networks on transdUc-
tive learning datasets. For three classes of
convolUtion-based aPProaches, we imPlement
oUr regUlarizer with GCN, GAT and APPNP as
comParisons with other baselines, resPectively.
For a fair comParison, we adoPt the same net-
work strUctUre, hyPerParameters and training
configUrations as baseline models. It is shown
that the ProPosed model achieves state-of-the-
art resUlts on all three settings. On all of the
datasets, we can observe a 0.5〜1.0% higher
Performance after adoPting the ProPosed reg-
Table 2: Test Micro-F1 Score on indUctive learn-
ing dataset. We rePort mean valUes and standard
deviations in 5 indePendent exPeriments.
Dataset	PPI
GCN (KiPf & Welling, 2017)	92^
GAT (Velickovic et al., 2018)	97.3
SGC (WU et al., 2019a)	66.4
JKNet (XU et al., 2018b)	97.6
GraPhSAGE (Hamilton et al., 2017)	61.2
DGI (Velickovic et al., 2019)	63.8
GCN+reg (ours)	97.69±0.32
GAT+reg (ours)	98.23±0.08
Ularizer. Notably, the ProPosed model achieves the highest imProvement on the vanilla GCNs as
this simPlest version sUffers most from the oversmoothing Problem. Meanwhile, when combining
with GAT, the model achieves the highest resUlts comParing with almost all the baselines. Con-
sidering that attention mechanism and the regUlarization on oversmoothing focUs on the local and
global ProPerties resPectively, this can be an ideal combination for graPh rePresentation learning.
We also condUct exPeriments on three citation networks with random sPlits and Present the resUlt in
APPendix D.
Inductive Learning. For the indUctive learning task, we imPlement oUr method on the vanilla
GCN and GAT, and adoPt the same exPerimental setUP. Table 2 Presents the comParison resUlts on
8
Under review as a conference paper at ICLR 2021
Table 3: Comparison results on transductive learning datasets. We report mean values and standard
deviations in 30 independent experiments. The best results are highlighted with boldface. The
number in the brackets represent the number of GCN layers when achieving the best performance.
Method	Citeseer	Cora	Pubmed
GCN + DropEdgeRong et al. (2019)	73.2±0.1(4)	84.2±0.6 (6)	80.1±0.3 (6)
GCN + PairNormZhao & Akoglu (2019)	71.0±0.5 (3)	82.2±0.6 (2)	79.6±0.6 (4)
GCN + regs (ours)	73.6±0.4 (4)	84.9±0.2 (5)	81.0±0.4 (5)
inductive learning dataset. It can be seen that our model compares favorably with all the competitive
baselines. On the PPI dataset, out model achieves 0.5〜1% higher on test Micro-FI score, showing
the effectiveness of applying our method under inductive settings.
Comparison with Other Related Works. To validate the effectiveness of our model, we compare
the proposed regularizer with two state-of-the-art approaches on tackling oversmoothing, DropE-
dge(Rong et al., 2019) and PairNorm(Zhao & Akoglu, 2019). For fair comparison, all approaches
are adopted on Vanina-GCN with 2〜8 layers and show the best performance on three transdUctive
datasets respectively. As shown in Table 3, our regularizer achieves best performance on all three
settings. As PairNorm is more suitable when a subset of the nodes lack feature vectors, it is less
competitive in the general settings.
Analysis. As we have stated above, the regularizer can be interpreted as the mean feature variance,
which prevents different features from falling into the same subspace. To testify the effect of our
method, we compute the mean pairwise distance (Eq.(19)) of the last hidden layer of GCN and GAT,
with and without regularizer on the Cora dataset. We show the result of models with 2-8 layers in
Figure 1. As we can observe, the feature variances and the accuracies of models with regularization
are comparably higher than vanilla models with obvious gaps. Therefore, after applying the regular-
izer, features are more separated from each other, and the oversmoothing problem is alleviated.
5 Conclusion
In this paper, we develop a unified graph convolution framework by establishing graph convolution
filters with optimization problems in the graph Fourier space. We show that most convolution-
based graph learning models are equivalent to adding carefully designed regularizers. Besides
vanilla GCN, our framework is extended to formulating non-convolutional operations, attention-
based GCNs and topology-based GCNs, which cover a large fraction of state-of-the-art graph learn-
ing models. On this basis, we propose a novel regularization on tackling the oversmoothing prob-
lem as a showcase, proving the effectiveness of designing new modules based on the framework.
Through the unified framework, we provide a general methodology for understanding and relating
different graph learning modules, with new insights on tackling common problems and improving
the generalization performance of current graph neural networks in the graph Fourier domain. Mean-
while, the unified framework can also serve as a once-for-all platform for expert-designed modules
on convolution-based approaches. We hope our work can promote the understandings towards graph
convolutional networks and inspire more insights in this field.
References
S. Bai, F. Zhang, and P. Torr. Hypergraph convolution and hypergraph attention. ArXiv,
abs/1901.08150, 2019.
Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint
arXiv:1711.07553, 2017.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via
importance sampling. International Conference on Learning Representations, 2018.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, S. Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. 2019.
9
Under review as a conference paper at ICLR 2021
Jian Du, Shanghang Zhang, Guanhang Wu, Jose MF Moura, and Soummya Kar. Topology adaptive
graph convolutional networks. CoRR, abs/1710.10370, 2017.
M. Fey, J. E. Lenssen, F. Weichert, and H. Muller. Splinecnn: Fast geometric deep learning with
continuous b-spline kernels. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 869-877, 2018.
Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.
CoRR, abs/1903.02428, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence
and Statistics, pp. 249-256, 2010.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, pp. 1024-1034, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
NT Hoang and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
arXiv preprint arXiv:1905.09550, 2019.
Amol Kapoor, Aram Galstyan, Bryan Perozzi, Greg Ver Steeg, Hrayr Harutyunyan, Kristina Ler-
man, Nazanin Alipourfard, and Sami Abu-El-Haija. Mixhop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. In International Conference on Machine Learn-
ing, 2019.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. International Conference on Learning Rep-
resentations, 2018.
Johannes Klicpera, Stefan Weiβenberger, and Stephan Gunnemann. Diffusion improves graph learn-
ing, 2019.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926, 2017.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-
5124, 2017.
Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2020.
Y. Rong, W. Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards the very deep graph
convolutional networks for node classification. 2019.
M. Schlichtkrull, Thomas Kipf, P. Bloem, R. V. Berg, Ivan Titov, and M. Welling. Modeling rela-
tional data with graph convolutional networks. In ESWC, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls
of graph neural network evaluation. CoRR, abs/1811.05868, 2018.
10
Under review as a conference paper at ICLR 2021
Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned filters in convolutional neu-
ral networks on graphs. In Proceedings of the IEEE conference on computer vision and pattern
recognition,pp. 3693-3702, 2017.
Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural
network for semi-supervised learning. CoRR, abs/1803.03735, 2018.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
Petar VeliCkoviC, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. International Conference on Learning Representations, 2019.
Felix WU, Tianyi Zhang, AmaUri Holanda de SoUza Jr, Christopher Fifty, Tao YU, and Kilian Q
Weinberger. Simplifying graph ConvolUtional networks. International Conference on Machine
Learning, 2019a.
ShU WU, YUyUan Tang, Yanqiao ZhU, Liang Wang, Xing Xie, and TieniU Tan. Session-based reC-
ommendation with graph neUral networks. In Proceedings of the AAAI Conference on Artificial
Intelligence, volUme 33, pp. 346-353, 2019b.
KeyUlU XU, WeihUa HU, JUre LeskoveC, and Stefanie Jegelka. How powerfUl are graph neUral
networks? CoRR, abs/1810.00826, 2018a.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-iChi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. arXiv preprint
arXiv:1806.03536, 2018b.
Liang Yao, Chengsheng Mao, and YUan LUo. Graph ConvolUtional networks for text ClassifiCation.
In Proceedings of the AAAI Conference on Artificial Intelligence, volUme 33, pp. 7370-7377,
2019.
Lingxiao Zhao and Leman AkoglU. Pairnorm: TaCkling oversmoothing in gnns. arXiv preprint
arXiv:1909.12223, 2019.
Marinka Zitnik and JUre LeskoveC. PrediCting mUltiCellUlar fUnCtion throUgh mUlti-layer tissUe
networks. Bioinformatics, 33(14):i190-i198, 2017.
Marinka Zitnik, MoniCa Agrawal, and JUre LeskoveC. Modeling polypharmaCy side effeCts with
graph ConvolUtional networks. Bioinformatics, 34(13):i457-i466, 2018.
11
Under review as a conference paper at ICLR 2021
Appendix
A. Proofs of the Definitions
Definition 2 Vanilla GCNs. Let x(ib∈v be the estimation of the input observation x(i)i∈v. A
low-pass filter:
.ʌ 〜 _____
X = ArwX,	(26)
is the first-order approximation of the optimal solution of the following optimization:
min Xkx(i)- χ(i)kD + X aijkx(i)-XCj)k2∙	(27)
X i∈v	i,j∈v
Proof. Let l denote the objective function. We have
l = tr[(X - X)TD(X - X)] + tr(XTLX).
Then,
∂l
∂X = 2D(X - X) + 2LX.
IfWelet ∂X = 0:
,~ — ~
(D + L)X = D X
, ~ . —
(I + Lrw )X = X.
〜
〜
〜
As the norm of eigenvalues of Arw = I - Lrw is bounded by 1, I + Lrw has eigenvalues in range
r< Cr ∖ ∙	∖	.∖	. T .	^c>	♦	∙ . ∙	1 r∙	.	∙ EI	r∙
[1, 3], Which proves that I + Lrw is a positive definite matrix. Therefore,
X = (I + L rw )-1X.
(28)
Unfortunately, solving the closed-form solution of Eq.(28) is computationally expensive. Neverthe-
less, we can derive a simpler form, X ≈ (I-Lrw)X = ArwX, via first-order Taylor approximation
which establishes the Definition.	□
Definition 3 Residual Connection. A graph convolution filter with residual connection:
.ʌ 〜_______ _____
X = ArwX + eX,	(29)
where > 0 controls the strength of residual connection, is the first-order approximation of the
optimal solution of the following optimization:
min X(kx(i) - χ(i)kD - ekx(i)kD) + X ajkχ(i) - x(j)k2∙	(30)
X	i∈v	i,j∈v
Proof. Let l denote the objective function. We have
l = tr[(X - X)TD(X - X)] - etr(XTDX) + tr(XTLX).
Then,
If we let 招=0:
∂XX = 2D(X - X) + 2(L - eD)X.
-. ~ — - ~
[(1 - e)D + L]X = DX
X= [(1 - e)I + Lrw]-1X
X= [I +(Lrw - eI)]-1X.
Therefore, the first-order approximation of the optimal solution is
X ≈ [I - (Lrw - eI)]X
Cr 一 一
= ArwX + X .
□
12
Under review as a conference paper at ICLR 2021
Then,
If we let 招=0:
Definition 3’ Concatenation. A graph convolution filter concatenating with the input signal:
X = ArwX + eX ΘΘt ,	(31)
is the first-order approximation of the optimal solution of the following optimization:
min X(kx(i) - x(i)kD - ekx(i)θkD) + X aijkx(i) - x(j)k2,	(32)
X i∈V	i,j∈V
where > 0 controls the strength of concatenation and Θ is the learning coefficients for the con-
catenated signal.
Proof. Let l denote the objective function. We have
l = tr[(X - X)tDD(X - X)] - etr((Xθ)τDD(XΘ)) + tr(XTLX).
a = 2DD(X - X) + 2LX - 2el)XΘΘt .
∂X
(DD + L)X - ef)XΘΘt = DDX
(I + Lrw)X - eXθΘτ = X.
With the help of the Kronecker product operator 0 and first-order Taylor expansion, We have
Vec(X) = [(I 0 (I + Lrw)) - e((ΘΘτ) 0 I)]-1vec(X)
≈ [2I - (I 0 (I + Lrw)) + e((ΘΘτ) 0 I)]vec(X)
=vec(2X - (I + Lrw)X + eXθΘτ)
=Vec(Arw X + eX ΘΘτ).
□
Definition 4 Attention-based GCNs. An attention-based graph convolution filter:
X = PX,	(33)
is the first-order approximation of the optimal solution of the following optimization:
min X l∣x(i) - x(i)kD + X Pij l∣x(i) - x(j)k2,	s.t. XPij= Dii, ∀i ∈ V.	(34)
X i∈V	i,j∈V	j∈V
Proof. Let l denote the objective function. We have
l = tr[(X - X)τD(X - X)] + tr(XTLX).
Then,
If we let 臬F = 0:
∂l
近=2D(X - X) + 2(D-DP)X.
, ~ ~ , — ~
(2D - DP)X = DX
(2I - P)X = X.
Similarly, we can prove that (2I - P) is a positive definite matrix, with eigenvalues in range [1, 3].
Therefore,
X = (2I - P)-1X
≈ PX.
□
13
Under review as a conference paper at ICLR 2021
Definition 5 & 6 Topology-based GCNs Due to the fact that most of the topology-based models
adopt non-convolutional operations like concatenation, we derive a more general objective function
by combining with the non-convolutional operations:
t
min αoXkx(i) - χ(i)kD + Xɑk X a(k)∣∣χ(i)θ(k)- Xej)θ(k)k2,	(35)
X	i∈V	k=1	i,j∈V
where Ptk=0 αk = 1, α0 > 0 and αk ≥ 0, k = 1, 2, . . . , t. If we let d be the feature dimension of
X, Θ(k) ∈ Rd×d correspond to the learning weights for the kth hop neighborhood. Let l denote the
objective function, we have:
∂l
^∂X
t
αoD(X - X) + X ɑk(D - DArw)JXΘ(k)(Θ(k))T.
k=1
By letting 裴=0, We have:
t
αoX + X(In - Akw)XΘ⑹(Θ⑹)T = αoX.
k=1
Therefore, with the help of the Kronecker product operator 0 and first-order Taylor expansion, We
have
t
[αoIn + X(αrΘ(k)(Θ(k))T) 0 (In - Akw)]vec(X) = α°vec(X).	(36)
k=1
Wecan observe that Pik=ι(akΘ(r)(Θ(k))T) and (In 一 Akw) havenon-negative eigenvalues. Due to
the property of the Kronecker product that the eigenvalues of the Kronecker product (A 0 B) equal
to the product of eigenvalues of A and B, the filter (α0In + Ptk=1(αkΘ(k)(Θ(k))T) is proved to
be a positive definite matrix. Therefore,
t
Vec(X) = α0[α0In + X(αfcΘ(k)(Θ(k))T) 0 (In 一 Akw)]-1vec(X)
k=1
t
≈ αo[(2 - ao)In 一 X(αkΘ(k)(Θ(k))T) 0 (In - Akw)]vec(X)
k=1
t
=αovec[(2 一 α0)X - X αk(In - Akw)XΘ(k)(Θ(k))T].
k=1
If we let
W⑼=2---^0In- X αkΘ(k)(Θ(k))T；
α0	k=1 α0
W(k) = Θ(k)(Θ(k))T), k= 1,2,...,t；
we can denote the convolution filter as:
t
X = X αk√lkwXW(k).
k=0
As we have stated in the Section 2.2.2, although the learning weights has a constrained expressive
capability, it can be compensated by the following feature learning module. We omit the proofs of
Definition 5 and 6, as they can be viewed as particular instances of (35).
Definition 7 Regularized Feature Variance. Let 0 be the Kronecker product operator, Vec(X) ∈
Rnd be the vectorized signal X. Let DX be a diagonal matrix whose value is defined by DX (i, i) =
∣∣x∙i∣2. A graph convolution filter with regularized feature variance:
Vec(X) = (In 0 [(αι + α2)I - α2Arw] - α3[D-l(I - d 11T)D-1] 0 DT)TVec(X) (40)
(37)
(38)
(39)
14
Under review as a conference paper at ICLR 2021
is equivalent to the optimal solution of the following optimization:
minɑιXllχ(i)-χ(i)kD + α2 X aijllχ(i)-χ(j)k2-α3d X kχ"kχ∙ik -χ∙j/kx∙jkk2,
X	i∈V	i,j∈V	i,j∈d
(41)
where ɑι > 0, α2,α3 ≥ 0. For Computation efficiency, we approximate DX with Dχ as we assume
that a single convolution filter provides little effect to the norm of features.
Proof. Let l denote the objective function. We have
l = α1tr[(X - X)TD(X - X)] + α2(XTLX) - α3tr[XD-I(I - 111T)D-1XT].
Then,
∂l	1
=2α1D(X - X) + 2a2LX - 2α3XD-I(I - -11T)D-1.
∂X	x d x
If We let 羲=0:
[(a1 + α2)I - α2DTArwIX - α3DTXD-I(I - 111T)D-1 = α1X.
xdx
With the help of the Kronecker product operator ⑼ We have
(In 乳[(αι + α2)I - α2Arw] - α3[D-l(I - d 11T)D-1]乳 DT)Vec(X) = Vec(X).	(42)
By setting α3 With a small positive value, the filter in Eq.(42) is still a positive definite matrix.
Therefore we complete the proof.	口
Similarly, We can derive a simpler form via Taylor approximation. IfWe let:
A = (αι + α2)I - α2Arw,	B = In,	(43)
C = -α3D-1,	D = D-1(1- d 11T)D-1.	(44)
Then, the first-order approximation of Eq.(40) is summarized as:
Vec(X) = (BT 0 A + DT 0 C)-1vec(X)
≈ (2I - BT 0 A - Dτ 0 C)vec(X)
= Vec(2X - AXB - CXD).
Additionally, We can also derive a t-order approximated formulation:
t
Vec(X⑴)=(I + X[I - (BT 0 A + DT 0 C)]i)vec(X).
i=1
HoWever, itis computationally expensive to calculate the Kronecker product. Therefore, We consider
utilizing a iterative algorithm. For any 0 ≤ k < t
k+1
Vec(X(k+1)) = (I + X[I - (BT 0 A + DT 0 C)]i)vec(X)
i=1
k
= [I- (BT0A+DT0C)](I+X[I - (BT 0 A + DT 0 C)]i)Vec(X) + Vec(X)
i=1
=[I - (BT 0 A + DT 0 C)]vec(X(k)) + vec(X)
=vec(X + X(k) - AX(k)B - CX(k)D).	(45)
B.	Reformulation Examples
The reformulation examples of GCN derivatives are presented in Table 4.
15
Under review as a conference paper at ICLR 2021
Table 4: Reformulation of convolution-based graph neural networks. D and di in the attention-based
modules are normalization coefficients.
Models	Non-Conv Module	Attention-based Module	Topology-based Module	
GIN (Xu et al., 2018a)	ReSidual Connection	-	-	
GraphSAGE (Hamilton et al., 2017)	Concatenation	-	-	
RGCN (Schlichtkrull et al., 2018)	Concatenation W = Pr∈R Cr Wr	-	-	
SplineCNN (Fey et al., 2018)	-	Pij = hθ(eij)	-	
ECC (SimonovSky & KomodakiS, 2017)	Concatenation	Pij = hθ(eij)	-	
AGNN (Thekumparampil et al., 2018)	-	d.. _ d.	eχp(SCOs(Xi,Xj)) .. Pij = i Pk∈N(i)∪i eχp(βcos(χi,Xk))	-	
MoNet (Monti et al., 2017)	Concatenation	Pj = diexp(-2M- μk)τXkT)(eij - μk))	-	
GAT Velickovic et al. (2018)	Concatenation	(k) _ d	eχpW磕)[θxillθXjD) Pij - i pk∈N(√)∪i eχp93Tc)[θxillθXkD)	-	
CluSter GCN (Chiang et al., 2019)	Concatenation	P = D(ArW + λdιag(Arw))		
SGC (Wu et al., 2019a)	-	P = DA 旨m	-	
Hyper-Atten (Bai et al., 2019)	-	P = HWB-1Hτ	-	
APPNP (Klicpera et al., 2018)	-	-	αo = γ,αι	=1-γ
GDC (Klicpera et al., 2019)	-	-	αi =	θi
TAGCN (Du et al., 2017)	-	-	αo =…=αk	=1/(k+1)
MixHop (Kapoor et al., 2019)	Concatenation	-	αo = αι =	α2 = 1/3
Table 5: DataSet StatiSticS
DataSet	Cora	CiteSeer	Pubmed	PPI
NodeS	2,708	3,327	19,717	56,944(24 graphS)
EdgeS	5,429	4,732	44,338	818,716
FeatureS	1,433	3,703	500	50
ClaSSeS	7	6	3	121(multilabel)
Training NodeS	140	120	60	44,906(20 graphS)
Validation NodeS	500	500	500	6,514(2 graphS)
TeSt NodeS	1,000	1,000	1,000	5,524(2 graphS)
C.	Data Statistics and Experimental Setups
We conduct experimentS on four real-world graph dataSetS, whoSe StatiSticS are liSted in Table 5. For
tranSductive learning, we evaluate our method on the Cora, CiteSeer, Pubmed dataSetS, following
the experimental Setup in (Sen et al., 2008). There are 20 nodeS per claSS with labelS to be uSed
for training and all the nodeS’ featureS are available. 500 nodeS are uSed for validation and the
generalization performance iS teSted on 1000 nodeS with unSeen labelS. PPI (Zitnik & LeSkovec,
2017) iS adopted for inductive learning, which iS a protein-protein interaction dataSet containing 20
graphS for training, 2 for validation and 2 for teSting while teSting graphS remain unobServed during
training.
To enSure a fair compariSon with other methodS, we implement our module without interfering the
original network Structure. In all three SettingS, we uSe two convolution layerS with hidden dimen-
16
Under review as a conference paper at ICLR 2021
Table 6: Test accuracy (%) on transductive learning datasets with random slits. We report mean
values and standard deviations of the test accuracies over 100 random train/validation/test splits.
Dataset	Citeseer	Cora	Pubmed
GCN(Kipf & Welling, 2017)	71.9±1.9	81.5±1.3	77.8±2.9
GAT(Velickovic et al., 2018)	71.4±1.9	81.8±1.3	78.7±2.3
MoNet (Monti et al., 2017)	71.2±2.0	81.3±1.3	78.6±2.3
GraphSAGE (Hamilton et al., 2017)	71.6±1.9	79.2±7.7	77.4±2.2
GCN+reg (ours)	72.9±1.4	83.6±1.2	79.9±1.6
Table 7: Training and test time on Cora. We report mean values in 5 independent experiments. The
best results are highlighted with boldface.
Method	Training Time (s)	Training Time (ms / epoch)	Test Time (ms)
GCN (Kipf & Welling, 2017)	18	38	19
GAT (Velickovic et al., 2018)	5.4	8.5	3.3
AGNN (Thekumparampil et al., 2018)	5.3	7.9	3.2
APPNP (Klicpera et al., 2018)	9.8	14.2	13.6
GCN + regs (ours)	57	80	32
sion h = 64. We set α1 = 0.2, α2 = 0.8 and α3 = 0.05 for all four datasets. We apply L2
regularization with λ = 0.0005 and use dropout on both layers. For training strategy, we initialize
weights using the initialization described in (Glorot & Bengio, 2010) and follow the method pro-
posed in GCN, adopting an early stop if validation loss does not decrease for certain consecutive
epochs. The implementations of baseline models are based on the PyTorch-Geometric library (Fey
& Lenssen, 2019) in all experiments.
D.	Random Splits
As illustrated in (Shchur et al., 2018), using the same train/validation/test splits of the same datasets
precludes a fair comparison of different architectures. Therefore, we follow the setup in (Shchur
et al., 2018) and evaluate the performance of our model on three citation networks with random
splits. Empirically, for each dataset, we use 20 labeled nodes per class as the training set, 30 nodes
per class as the validation set, and the rest as the test set. For every model, we choose the hy-
perparameters that achieve the best average accuracy on Cora and CiteSeer datasets and applied to
Pubmed dataset.
Table 6 shows the results on three citation networks under the random split setting. As we can
observe, our model consistently achieves higher performances on all the datasets. On Citeseer, our
model achieves higher accuracy than on the original split. On Cora and Pubmed, the test accuracies
of our model are comparable to the original split, while most of the baselines suffer from a serious
decline.
E.	Time Consumption
As we have shown in Eq.(20), the computation of graph filter with the regularizer is greatly increased
with Kronecker product and inverse matrix operations. Nevertheless, we approximate the filter with
an iterative algorithm as stated in Eq.(25) and realize an efficient implement. To empirically testify
the computation efficiency, we conduct experiments on Cora and report the training and test time
of several GCN models on a single RTX 2080 Ti GPU. Due to the early stopping rule (see details
in Appendix C), the training epoch for each module is different. The results are shown in Table 7.
As we can observe, when combining with vanilla GCNs, the training and test time of our model is
similar to GAT and AGNN and faster than APPNP.
17
Under review as a conference paper at ICLR 2021
Table 8: Ablation study on the regularization strength. We report mean values and standard devia- tions in 30 independent experiments. The best results are highlighted with boldface.			
Regularization Strength	Citeseer	Cora	Pubmed
0	70.3±0.4	81.5±0.5	79.0±0.4
0.01	71.7±0.6	83.0±0.5	79.2±0.6
0.05	72.2±0.4	83.6±0.3	79.8±0.2
0.1	72.4±0.5	83.5±0.2	79.4±0.7
0.2	68.4±1.0	76.8±1.3	70.2±2.2
F.	Ablation Study
To analyze the effects of the regularization strength, we conduct experiments on three transductive
datasets and present the results in Table 8. As we can observe, with reasonable choice of the regu-
larization strength, our approach can achieve consistent improvement under all settings. However,
when the regularization strength is too large, the training procedure becomes unstable and the model
performance suffers from a severe decrease.
18