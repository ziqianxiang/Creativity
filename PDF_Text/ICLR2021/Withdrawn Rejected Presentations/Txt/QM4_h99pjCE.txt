Decentralized Deterministic Multi-Agent
Reinforcement Learning
Anonymous Author(s)
Affiliation
Address
email
Abstract
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
[Zhang, ICML 2018] provided the first decentralized actor-critic algorithm for
multi-agent reinforcement learning (MARL) that offers convergence guarantees. In
that work, policies are stochastic and are defined on finite action spaces. We extend
those results to offer a provably-convergent decentralized actor-critic algorithm for
learning deterministic policies on continuous action spaces. Deterministic policies
are important in real-world settings. To handle the lack of exploration inherent in de-
terministic policies, we consider both off-policy and on-policy settings. We provide
the expression of a local deterministic policy gradient, decentralized deterministic
actor-critic algorithms and convergence guarantees for linearly-approximated value
functions. This work will help enable decentralized MARL in high-dimensional
action spaces and pave the way for more widespread use of MARL.
1 Introduction
Cooperative multi-agent reinforcement learning (MARL) has seen considerably less use than its
single-agent analog, in part because often no central agent exists to coordinate the cooperative agents.
As a result, decentralized architectures have been advocated for MARL. Recently, decentralized
architectures have been shown to admit convergence guarantees comparable to their centralized
counterparts under mild network-specific assumptions (see Zhang et al. [2018], Suttle et al. [2019]).
In this work, we develop a decentralized actor-critic algorithm with deterministic policies for multi-
agent reinforcement learning. Specifically, we extend results for actor-critic with stochastic policies
(Bhatnagar et al. [2009], Degris et al. [2012], Maei [2018], Suttle et al. [2019]) to handle deterministic
policies. Indeed, theoretical and empirical work has shown that deterministic algorithms outperform
their stochastic counterparts in high-dimensional continuous action settings (Silver et al. [January
2014b], Lillicrap et al. [2015], Fujimoto et al. [2018]). Deterministic policies further avoid estimating
the complex integral over the action space. Empirically this allows for lower variance of the critic
estimates and faster convergence. On the other hand, deterministic policy gradient methods suffer
from reduced exploration. For this reason, we provide both off-policy and on-policy versions of our
results, the off-policy version allowing for significant improvements in exploration. The contributions
of this paper are three-fold: (1) we derive the expression of the gradient in terms of the long-term
average reward, which is needed in the undiscounted multi-agent setting with deterministic policies;
(2) we show that the deterministic policy gradient is the limiting case, as policy variance tends to
zero, of the stochastic policy gradient; and (3) we provide a decentralized deterministic multi-agent
actor critic algorithm and prove its convergence under linear function approximation.
Submitted to 34th Conference on Neural Information Processing Systems (NeurIPS 2020). Do not distribute.
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
2 Background
Consider a system of N agents denoted by N = [N] in a decentralized setting. Agents determine
their decisions independently based on observations of their own rewards. Agents may however com-
municate via a possibly time-varying communication network, characterized by an undirected graph
Gt = (N , Et), where Et is the set of communication links connecting the agents at time t ∈ N. The
networked multi-agent MDP is thus characterized by a tuple (S, Ai i∈N , P, Ri i∈N , {Gt}t≥0)
where S is a finite global state space shared by all agents in N , Ai is the action space of agent i, and
{Gt }t≥0 is a time-varying communication network. In addition, let A = Qi∈N Ai denote the joint
action space of all agents. Then, P : S × A × S → [0, 1] is the state transition probability of the
MDP, and Ri : S × A → R is the local reward function of agent i. States and actions are assumed
globally observable whereas rewards are only locally observable. At time t, each agent i chooses its
action ait ∈ Ai given state st ∈ S, according to a local parameterized policy πθii : S × Ai → [0, 1],
where πθii (s, ai) is the probability of agent i choosing action ai at state s, and θi ∈ Θi ⊆ Rmi is
the policy parameter. We pack the parameters together as θ = [(θ1)>,…,(θN)>]> ∈ Θ where
Θ = Qi∈N Θi. We denote the joint policy by πθ : S ×A → [0, 1] where πθ(s, a) = Qi∈N πθii (s, ai).
Note that decisions are decentralized in that rewards are observed locally, policies are evaluated
locally, and actions are executed locally. We assume that for any i ∈ N , s ∈ S, ai ∈ Ai , the
policy function πθii (s, ai) > 0 for any θi ∈ Θi and that πθii (s, ai) is continuously differentiable with
respect to the parameters θi over Θi. In addition, for any θ ∈ Θ, let Pθ : S × S → [0, 1] denote
the transition matrix of the Markov chain {st}t≥0 induced by policy πθ, that is, for any s, s0 ∈ S,
Pθ(s0∣s) = Pa∈A ∏θ(s, a) ∙ P(s0∣s, a). We make the standard assumption that the Markov chain
{st }t≥0 is irreducible and aperiodic under any πθ and denote its stationary distribution by dθ .
Our objective is to find a policy πθ that maximizes the long-term average reward over the network.
Let rti+1 denote the reward received by agent i as a result of taking action ait . Then, we wish to solve:
1	T-1 1
maxJ(πθ) = Iim 不E EkEri+ι = E dθ(S)πθ(S,a)R(S,a),
θ	T→∞ T	N
t=0	i∈N	s∈S,a∈A
where R(s,a) = (1/N) ∙ Pii∈N Ri(s,a) is the globally averaged reward function. Let rt =
(1/N) ∙ Pi∈N ri, then R(s, a) = E [尸t+ι∣st = s,at = a], and therefore, the global relative action-
value function is: Qθ(s, a) = Pt≥0 E [尸t+1 - J(θ)∣so = s,a0 = a, ∏θ], and the global relative
state-value function is: Vθ(S) = Pa∈A πθ(S, a)Qθ(S, a). For simplicity, we refer to Vθ and Qθ
as simply the state-value function and action-value function. We define the advantage function as
Aθ(S, a) = Qθ(S, a) - Vθ(S).
Zhang et al. [2018] provided the first provably convergent MARL algorithm in the context of the
above model. The fundamental result underlying their algorithm is a local policy gradient theorem:
▽&i J(μθ) = Es〜dθ,a〜∏θ [▽ θi log πθi (s, a ) ∙ Aθ (s, a)],
where Aθ(s, a) = Qθ(s, a) - Vi(S,a-i) is a local advantage function and 0(s,a-i) =
Pai∈Ai πθii (S, ai)Qθ(S, ai, a-i). This theorem has important practical value as it shows that the
policy gradient with respect to each local parameter θi can be obtained locally using the corresponding
score function Vθi log ∏θi provided that agent i has an unbiased estimate of the advantage functions
Aiθ or Aθ . With only local information, the advantage functions Aiθ or Aθ cannot be well estimated
since the estimation requires the rewards rti i∈N of all agents. Therefore, they proposed a consensus
based actor-critic that leverages the communication network to share information between agents
by placing a weight ct (i, j) on the message transmitted from agent j to agent i at time t. Their
action-value function Qθ was approximated by a parameterized function Q ω : S ×A→ R, and each
agent i maintains its own parameter ωi, which it uses to form a local estimate Qωi of the global Qθ .
At each time step t, each agent i shares its local parameter ωti with its neighbors on the network, and
the shared parameters are used to arrive at a consensual estimate of Qθ over time.
2
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
3 Local Gradients of Deterministic Policies
While the use of a stochastic policy facilitates the derivations of convergence proofs, most real-world
control tasks require a deterministic policy to be implementable. In addition, the quantities estimated
in the deterministic critic do not involve estimation of the complex integral over the action space found
in the stochastic version. This offers lower variance of the critic estimates and faster convergence. To
address the lack of exploration that comes with deterministic policies, we provide both off-policy
and on-policy versions of our results. Our first requirement is a local deterministic policy gradient
theorem.
We assume that Ai = Rni . We make standard regularity assumptions on our MDP. That is, we
assume that for any s, s0 ∈ S, P(s0|s, a) and Ri(s, a) are bounded and have bounded first and
second derivatives. We consider local deterministic policies 偏名:S → Ai with parameter vector
θi ∈ Θi, and denote the joint policy by μθ : S → A, where μθ(S) = (μ} (s),..., μNN (S)) and
θ = [(θ1)>,..., (θN)>]>. We assume that for any S ∈ S, the deterministic policy function μ^(s)
is twice continuously differentiable with respect to the parameter θi over Θi . Let Pθ denote the
transition matrix of the Markov chain {st}t≥o induced by policy μe, that is, for any s, s0 ∈ S,
Pθ(s0∣s) = P(s0∣s, μθ(s)). We assume that the Markov chain {st}t≥o is irreducible and aperiodic
under any μθ and denote its stationary distribution by dμθ.
Our objective is to find a policy μθ that maximizes the long-run average reward:
max J (μθ) = Es 〜dμθ [R(s,μθ (s))] = fdμθ (S)R(S,μθ (s)).
s∈S
Analogous to the stochastic policy case, we denote the action-value function by Qθ (S, a) =
Et≥o EFt+ι — J(μθ)|so = S,ao = a, μθ], and the state-value function by Vθ(S) = Qθ(s, μθ(s)).
When there is no ambiguity, we will denote J(μθ) and dμθ by simply J(θ) and dθ, respectively. We
present three results for the long-run average reward: (1) an expression for the local deterministic
policy gradient in the on-policy setting Vθi J(μθ), (2) an expression for the gradient in the off-policy
setting, and (3) we show that the deterministic policy gradient can be seen as the limit of the stochastic
one.
On-Policy Setting
Theorem 1	(Local Deterministic Policy Gradient Theorem - On Policy). For any θ ∈ Θ, i ∈ N ,
Vθi J(μθ) exists and is given by
vθiJ(μθ) = Es〜dμθ vθiμθi(S)Vai qθ(S,μ--i(S),ai)∣ai=μi_(s).
The first step of the proof
Es 〜dθ [Vθ μθ (S)Va Qθ (S,a)la=μθ (s)].
consists in showing that	Vθ J (μθ)	=
This is an extension of the well-known stochastic
case, for which we have Vθ J(∏θ) = Es〜d@ [Vθ log(∏θ(a∣S))Qθ(s, a)], which holds for a long-term
averaged return with stochastic policy (e.g Theorem 1 of Sutton et al. [2000a]). See the Appendix for
the details.
Off-Policy Setting In the off-policy setting, we are given a behavior policy π : S → P (A), and
our goal is to maximize the long-run average reward under state distribution dπ :
Jπ (μθ) = Es〜dπ [R(s, μθ (S))] =): d (S)R(S, μθ(S)).	(I)
s∈S
Note that we consider here an excursion objective (Sutton et al. [2009], Silver et al. [January 2014a],
Sutton et al. [2016]) since we take the average over the state distribution of the behaviour policy π of
the state-action reward when selecting action given by the target policy μθ. We thus have:
Theorem 2	(Local Deterministic Policy Gradient Theorem - Off Policy). For any θ ∈ Θ, i ∈ N,
π : S → P (A) a fixed stochastic policy, V θi J∏ (μθ) exists and is given by
Vθi j∏ (μθ) = Es 〜d∏
Vθi 哈(S)Vai R(S, μθ-i (S), ai)lai=μi. (s)
3
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
Proof. Since dπ is independent of θ we can take the gradient on both sides of (1)
VθJ∏(μθ) = Es〜d∏ [Vθμθ(S) VaR(S,μθ(s))∣
.
a=μe(s)J
Given that Vθiμj(S) = 0 if i = j, We have Vθμθ(S) = Diag(Vθiμ1ι (s),..., JnμN (S)) and the
result follows.
□
This result implies that, off-policy, each agent needs access to μθ--ii (St) for every t .
Limit Theorem As noted by Silver et al. [January 2014b], the fact that the deterministic gradient
is a limit case of the stochastic gradient enables the standard machinery of policy gradient, such as
compatible-function approximation (Sutton et al. [2000b]), natural gradients (Kakade [2001]), on-line
feature adaptation (Prabuchandran et al. [2016],) and actor-critic (Konda [2002]) to be used With
deterministic policies. We shoW that it holds in our setting. The proof can be found in the Appendix.
Theorem 3 (Limit of the Stochastic Policy Gradient for MARL). Let πθ,σ be a stochastic policy
such that πθ,σ (a|S) = νσ (μθ (S), a), where σ is a parameter controlling the variance, and νσ satisfy
Condition 1 in the Appendix. Then,
lim vΘ J∏θ,σ (πθ,σ ) = vΘ Jμθ (μθ )
σψ0
where on the l.h.s the gradient is the standard stochastic policy gradient and on the r.h.s. the gradient
is the deterministic policy gradient.
4 Algorithms
We provide tWo decentralized deterministic actor-critic algorithms, one on-policy and the other
off-policy and demonstrate their convergence in the next section; assumptions and proofs are provided
in the Appendix.
On-Policy Deterministic Actor-Critic
Algorithm 1 NetWorked deterministic on-policy actor-critic
Initialize: step t = 0; parameters J0, ω0, ω0, θ0, ∀i ∈ N; state s°; stepsizes {βω,t}t≥o, {βθ,t}t≥o
DraW ai0 = μiθi (S0) and compute aei0 = Vθi μiθi (S0)
Observe joint action a0 = (a01, . . . , a0N) and ea0 = ae01, . . . , ae0N
repeat
for i ∈ N do
Observe St+1 and reWard rti+1 = ri(St, at)
UPdate Jti+1 J (I- βω,t) ∙ Jti + βω,t ∙ ri+1
DraW action at+1 = μiθi (St+1) and compute aeit+1 = Vθi μiθi (St+1)
end for
Observe joint action at+1 = (at1+1, . . . , atN+1) and aet+1 = aet1+1, . .
for i ∈ N do
Update: δt J rt+1 - Jt + Qωti (St+1, at+1) - Qωti (St, at)
Critic step: ωi - ωi + βω,t ∙ δi ∙ VωQωi(St,at)∣
ω=ωti
i
ActorStep: θi+ι = θi + βθ,t ∙ V©iμθi(St) VaiQ-i(St,a- ,a )
Send ωeti to the neighbors {j ∈ N : (i,j) ∈ Et} over Gt
. , eatN+1
ai=ait
Consensus step: ωi+1 J Pj ∈n Ctj ∙ ωj
end for
Update t J t + 1
until end
4
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
Consider the following on-policy algorithm. The actor step is based on an expression for Vθi J(μθ)
in terms of VaiQθ(see Equation (15) in the Appendix). We approximate the action-value function Qθ
using a family of functions Q ω : S×A→ R parameterized by ω, a column vector in RK. Each agent
i maintains its own parameter ωi and uses Qωi as its local estimate of Qθ . The parameters ωi are
updated in the critic step using consensus updates through a weight matrix Ct = citj	∈ RN ×N
where citj is the weight on the message transmitted from i to j at time t, namely:
Λ√	,	Λ√	:
Ji+1 = (1- βω,t) ∙ Jt + βω,t∙ Ki	⑵
ωt = ω + βω,t∙ δt ∙ VωQωi 岱力电)1=^	(3)
ωi+1 = X ctj ∙ ωj	⑷
j∈N
with
ii	i
δt = rt+1 - Jt + Qωti (st+1, at+1) - Qωti (st, at).
For the actor step, each agent i improves its policy via:
Θt+1 = θ + βθ,t∙Vθi μθi (St) ∙ Vai Q ωi (st,a-i,ai)l .	.∙	(5)
t	t	ai =ait
Since Algorithm 1 is an on-policy algorithm, each agent updates the critic using only (St, at, St+1), at
time t knowing that at+i = μθt (st+i). The terms in blue are additional terms that need to be shared
when using compatible features (this is explained further in the next section).
Off-Policy Deterministic Actor-Critic We further propose an off-policy actor-critic algorithm,
defined in Algorithm 2 to enable better exploration capability. Here, the goal is to maximize
Jn (μθ) where ∏ is the behavior policy. To do so, the globally averaged reward function R(s, a) is
approximated using a family of functions R : S ×A→ R that are parameterized by λ, a column
vector in RK. Each agent i maintains its own parameter λ and uses Rλi as its local estimate of R.
Based on (1), the actor update is
l
θt+1 = θt + βθ,t∙ V6i Mθi(St) ∙ Vai Rλt (St,Mg-i(St),a )l .	,	⑹
t	t	θt	Iai=^ei (St)
which requires each agent i to have access to μjj (St) for j ∈ N.
The critic update is
et = Κ + βλ,t∙ δt ∙VλRλi(St,at)l	.	⑺
λ=λit
λit+i = Xcitjλetj,	(8)
j∈N
with
ʌ
_ √	√, , = . ,
δt = r (St, at) - Rλi(St, at).	(9)
In this case, δti was motivated by distributed optimization results, and is not related to the local
TD-error (as there is no "temporal" relationship for R). Rather, it is simply the difference between
the sample reward and the bootstrap estimate. The terms in blue are additional terms that need to be
shared when using compatible features (this is explained further in the next section).
5 Convergence
To show convergence, we use a two-timescale technique where in the actor, updating deterministic
policy parameter θi occurs more slowly than that of ωi and Ji in the critic. We study the asymptotic
behaviour of the critic by freezing thejointpolicy μg, then study the behaviour of θt under convergence
of the critic. To ensure stability, projection is often assumed since it is not clear how boundedness of
5
Algorithm 2 Networked deterministic off-policy actor-critic
Initialize: step t = 0; parameters λ* i0, λi0, θ0i , ∀i ∈ N; state s0; stepsizes {βλ,t}t≥0, {βθ,t}t≥0
Draw a0 〜πi(s0) , compute a0 = μθi(so) and e0 = V0i*(so)
Observe joint action a0
repeat
for i ∈ N do
Observe st+1 and reward rti+1 = ri (st, at)
end for
..,aN), a 0 = (a O,..., a N) and e0
for i ∈ N do
ii
UPdate： δt - rt+ι - Rλi(st,at)
Critic step: λ  λ + βλ,t ∙ δii ∙
W A /	∖
VλRλi(st, at)
λ=λit
.	C	.__,	√ / 、 .__, 三/	/ / 、	：、
Actor step: θ∖+x = θ∖ + βθ,t ∙Vμlθi (St) ∙V&t (St,μθr (st),a )
ai = μgi (St )
C 1 ʌ √ ,	.1	∙	1 1	r ∙	_	*	/ ∙	∙∖	_ z-» 1	Z7
Send λit to the neighbors {j ∈ N : (i, j) ∈ Et} over Gt
end for
for i ∈ N do
ij j
Consensus step: λt+ι J Σj∈N Cj ∙ λj
Draw action at+ι 〜 ∏(st+ι), compute a；+1 = 座 (st+ι) and compute et+ι
v θi μθi (St+1)
θt+1
end for
Observe joint action at+ι = (a1+ι,...,aN+ι), at+ι = (a[「...，aN+J and et-+γ
(e1+1,..∙, aN+ι)
Update t J t + 1
until end
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
θti can otherwise be ensured (see Bhatnagar et al. [2009]). However, in practice, convergence is
typically observed even without the projection step (see Bhatnagar et al. [2009], Degris et al. [2012],
Prabuchandran et al. [2016], Zhang et al. [2018], Suttle et al. [2019]). We also introduce the following
technical assumptions which will be needed in the statement of the convergence results.
Assumption 1 (Linear approximation, average-reward). For each agent i, the average-reward function
i
R is parameterized by the class of linear functions, i.e., Rχi,θ(s, a) = wθ(s, a) ∙ λi where wθ (s, a)=
wθ,1(S, a), . . . , wθ,K (S, a) ∈ RK is the feature associated with the state-action pair (S, a). The
feature vectors wθ(S, a), as well as Vawθ,k (S, a) are uniformly bounded for any S ∈ S, a ∈ A, k ∈
J1, K K. Furthermore, we assume that the feature matrix W∏ ∈ RlSl×κ has full column rank, where
the k-th column of W∏,θ is [ JA π(a∣s)wθ,k (s, a)da, s ∈ S] for any k ∈ J1, K[.
Assumption 2 (Linear approximation, action-value). For each agent i, the action-value function
is parameterized by the class of linear functions, i.e., Qω√ (s, a) = φ(s, a) ∙ ωi where φ(s, a)=
φ1(S, a), . . . , φK (S, a) ∈ RK is the feature associated with the state-action pair (S, a). The feature
vectors φ(s, a), as well as Vaφk(s, a) are uniformly bounded for any s ∈ S, a ∈ A, k ∈ {1, . . . , K}.
Furthermore, we assume that for any θ ∈ Θ, the feature matrix Φθ ∈ RlSl×κ has full column rank,
where the k-th column of Φθ is [φk(s, μθ(s)), S ∈ S] for any k ∈ J1, K[.Also, for any U ∈ RK,
Φθu 6= 1.
Assumption 3 (Bounding θ). The update of the policy parameter θi includes a local projection by
Γi : Rmi → Θi that projects any θt onto a compact set Θi that can be expressed as {θi∣qj (θi) ≤
0, j = 1, . . . , si} ⊂ Rmi, for some real-valued, continuously differentiable functions {qji}1≤j ≤si
defined on Rmi . We also assume that Θ = QiN=1 Θi is large enough to include at least one local
minimum of J(θ).
We use {Ft} to denote the filtration with Ft = σ(sτ, Cτ-1, aτ-1, rτ-1, τ ≤ t).
Assumption 4 (Random matrices). The sequence of non-negative random matrices {Ct = (citj )ij }
satisfies:
6
190
191
192
193
194
195
196
197
198
199
200
201
202
203
1.	Ct is row stochastic and E(Ct |Ft) is a.s. column stochastic for each t, i.e., Ct 1 = 1 and
1>E(Ct|Ft) = 1> a.s. Furthermore, there exists a constant η ∈ (0, 1) such that, for any
citj > 0, we have citj ≥ η.
2.	Ct respects the communication graph Gt, i.e., citj = 0 if (i, j) ∈/ Et.
3.	The spectral norm of E[C> ∙ (I - 11>∕N) ∙ Ct] is smaller than one.
4.	Given the σ-algebra generated by the random variables before time t, Ct, is conditionally
independent of st, at and rti+1 for any i ∈ N.
Assumption 5 (Step size rules, on-policy). The stepsizes βω,t, βθ,t satisfy:
βω,t =	βθ,t = ∞
tt
X(βω2 ,t + βθ2,t) < ∞
t
E∣βθ,t+ι - βθ,t∣ < ∞.
t
In addition, βθ,t = o(βω,t) and limt→∞βω,t+1∕βω,t = 1.
Assumption 6 (Step size rules, off-policy). The step-sizes βλ,t , βθ,t satisfy:
Xβλ,t=Xβθ,t=∞,	Xβλ2,t+βθ2,t<∞
tt	t
βθ,t = o(βλ,t),	limβλ,t+1∕βλ,t = 1.
On-Policy Convergence To state convergence of the critic step, we define Dθs = Diag dθ (s), s ∈
S], Rθ = [R(s, μθ(s)), S ∈ S]> ∈ R|S| and the operator TQ : R|S| → R|S| for any action-value
vector Q ∈ R|S| (and not RlS"Al since there is a mapping associating an action to each state) as:
TQ(QO) = Rθ - J(μθ) ∙1 + PθQ0.
Theorem 4.	Under Assumptions 3, 4, and 5, for any given deterministic policy μθ, with {Jt} and
{ωt} generated from (2), we have limt→∞ N ∑2i∈N Jt 二 J (μθ) and limt→∞ωi = ω0 a.s. for any
i ∈ N, where
J (μθ) = X dθ(s)R(s,μθ (S))
s∈S
is the long-term average return under μθ, and ωθ is the unique solution to
Φθ>Dθs[TθQ(Φθωθ) - Φθωθ] =0.	(10)
Moreover, ωθ is the minimizer of the Mean Square Projected Bellman Error (MSPBE), i.e., the
solution to
minimize kΦθω - ΠTθQ(Φθω)k2Ds ,
ω	θθ
where Π is the operator that projects a vector to the space spanned by the columns of Φθ, and ∣∣∙∣∣Ds
θ
denotes the euclidean norm weighted by the matrix Dθs.
To state convergence of the actor step, we define quantities ψti,θ, ξti and ξti,θ as
ψi,θ = Rθμθ(St)	and ψi = ψi,θt = Rθμθi(St))
ξt,θ = RaiQωθ (st,at lai) I	= Rai φ(S t, a—, ai ) |a .=° .=心(S ) ωθ,
,	∣ai = ai=μii (st)	∣ ai=ai=M@i (st)
θt	t
i	i	i|	i
ξt = Rai Qωi (St, at , ai )||	= Rai φ(St, at , ai )|a = i (s ) ωt
t	∣ai=μθi (st)	i=μθi (st)
7
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
Additionally, We introduce the operator Γ(∙) as
…/ -	Γi ∖θi + η∙ g(θ)] - θi
Γi [g(θ)] = Iim n L + η g( )」—	(11)
0<η→0	η
for any θ ∈ Θ and g : Θ → Rmi a continuous function. In case the limit above is not unique We take
■Ad Γ / Zi∖ 1 . ι .1	. r∙ ιι	∙ι ι ι ∙	∙	c ∕< <∖
Γi [g(θ)] to be the set of all possible limit points of (11).
Theorem 5.	Under Assumptions 2, 3, 4, and 5, the policy parameter θti obtained from (5) converges
a.s. to a point in the set of asymptotically stable equilibria of
θi = Γi [Est~dθ,μθ [ψi,θ ∙ ξi,θ]] , for any i ∈ N.	(12)
In the case of multiple limit points, the above is treated as a differential inclusion rather than an
ODE.
The convergence of the critic step can be proved by taking similar steps as that in Zhang et al. [2018].
For the convergence of the actor step, difficulties arise from the projection (Which is handled using
Kushner-Clark Lemma Kushner and Clark [1978]) and the state-dependent noise (that is handled by
“natural” timescale averaging CroWder [2009]). Details are provided in the Appendix.
Remark. Note that that with a linear function approximator Qθ, ψt,θ ∙ ξt,θ	=
Vθμθ(St) Rqωθ (st, a) I	may not be an unbiased estimate of Vθ J(θ):
∣a=μθ (st)
Es~dθ [ψt,θ ∙ξt,θ ] = vθ J ⑹+Es~dθ vθ μθ (S) ∙ (Va Qωθ (S,⑪]—* ⑸-RaQωθ (S,a)|a=*e (S))
A standard approach to overcome this approximation issue is via compatible features (see, for
example, Silver et al. [January 2014a] and Zhang and Zavlanos [2019]), i.e. φ(s, a) = a ∙ Vθμθ(s)>,
giving, for ω ∈ Rm ,
Qω(s, a) = a ∙ Vθμθ(s)>ω = (a 一 μθ(S)) ∙ Vθμθ(s)>ω + 比(s),
With Vω(S)= Qω(S,μθ(S)) and VaQω(s, a)I	= Vθμe(s) i ω.
∣a=μθ(s)
We thus expect that the convergent point of (5) corresponds to a small neighborhood of a local
optimum of J(μθ), i.e., V&i J(μθ) = 0, provided that the error for the gradient of the action-
value function VaQω(s,a)l	一 VaQθ(s,a)∣a=μ (S) is small. However, note that using
∣a=μθ(s)	μ"
compatible features requires computing, at each step t, φ(St,at) = at ∙ Vθμθ(St)>. Thus, in
Algorithm 1, each agent observes not only the joint action at+1 = (at1+1, . . . , atN+1) but also
(Vθiμ1ι (St+ι),..., VθnμNN (St+ι)) (see the parts in blue in Algorithm 1).
Off-Policy Convergence
Theorem 6.	Under Assumptions 1, 4, and 6, for any given behavior policy π and any θ ∈ Θ, with
{λit } generated from (7), we have limt→∞λit = λθ a.s. for any i ∈ N, where λθ is the unique
solution to
B∏,θ ∙ λθ = A∏,θ ∙ d∏	(13)
where d∏	=	[dπ (s), s	∈	S]>,	A∏,θ =	[ Ja π(a∣S)R(S,a)w(S,a)>da,S ∈	S ]	∈	Rκ×lSl and
B∏,θ = [Ps∈s dπ(s) Ra ∏(a∣S)wi(S, a) ∙ w(S,a)>da, 1 ≤ i ≤ K] ∈ Rk×k.
From here on we let
ξt,θ = VaiRλθ (st,μθ-i (St),ai) l	= Vai W(St,μθ-i (St),ai) I	.	λθ
t	ai = μθi (st)	t	ai = μθi (st)
ξt = VaiRXi (St,μ--i (St),ai) I	= Vai w(st,μ--i (St),ai) la . i (§ ) λt
t	θt	∣ai=μi .(st)	θ	∣ai=μθi(St)
θti
and we keep
ψt,θ = Vθiμθi (st),	and ψt = ψt,θt = vθiμθi (st) .
8
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
Theorem 7.	Under Assumptions 1, 3, 4, and 6, the policy parameter θti obtained from (6) converges
a.s. to a point in the asymptotically stable equilibria of
θi = Γi[Es" [ψi,θ ∙ ξi,θ]].
(14)
We define compatible features for the action-value and the average-reward function in an analogous
manner: wθ(s, a) = (a — μθ(S)) ∙ Vθμθ(s)>. For λ ∈ Rm,
ʌ _________________________________________________________
>
Rλ,θ(s, a) = (a - μθ(S)) ∙ Heμθ(S) ∙ λ
>
▽aRλ,θ(S,a) = Veμe(S)T ∙ λ
and we have that, for λ* = argmin Es 〜d∏ [∣∣Va R λ,θ (S,μe (S)) — Va R(S,μe (S))Il2 ]：
λ
VeJn(μe) = Es〜d∏ [Vθμe(s) ∙ VaRR(S,a)∣a=μ0(s) ] = Es〜d∏ [Vθμe(s) ∙ VaRλ*,θ(S,a)LiO(Sj
The use of compatible features requires each agent to observe not only the joint action taken
at+1 = (a1+1,..., aN+1) and the “on-policy action“ at+1 = (a 1+1,..., aN+1), but also et+1 =
(VθιU11 (St+1),..., VθnμNN (St+1)) (see the parts in blue in Algorithm 2).
We illustrate algorithm convergence on multi-agent extension of a continuous bandit problem from
Sec. 5.1 of Silver et al. [January 2014b]. Details are in the Appendix. Figure 2 shows the convergence
of Algorithms 1 and 2 averaged over 5 runs. In all cases, the system converges and the agents are
able to coordinate their actions to minimize system cost.
Figure 1: Convergence of Algorithms 1 and 2 on the multi-agent continuous bandit problem.
6 Conclusion
We have provided the tools needed to implement decentralized, deterministic actor-critic algorithms
for cooperative multi-agent reinforcement learning. We provide the expressions for the policy
gradients, the algorithms themselves, and prove their convergence in on-policy and off-policy settings.
We also provide numerical results for a continuous multi-agent bandit problem that demonstrates
the convergence of our algorithms. Our work differs from Zhang and Zavlanos [2019] as the latter
was based on policy consensus whereas ours is based on critic consensus. Our approach represents
agreement between agents on every participants’ contributions to the global reward, and as such,
provides a consensus scoring function with which to evaluate agents. Our approach may be used
in compensation schemes to incentivize participation. An interesting extension of this work would
be to prove convergence of our actor-critic algorithm for continuous state spaces, as it may hold
with assumptions on the geometric ergodicity of the stationary state distribution induced by the
deterministic policies (see Crowder [2009]). The expected policy gradient (EPG) of Ciosek and
Whiteson [2018], a hybrid between stochastic and deterministic policy gradient, would also be
interesting to leverage. The Multi-Agent Deep Deterministic Policy Gradient algorithm (MADDPG)
of Lowe et al. [2017] assumes partial observability for each agent and would be a useful extension,
but it is likely difficult to extend our convergence guarantees to the partially observed setting.
9
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
References
Albert Benveniste, Pierre Priouret, and Michel Metivier. Adaptive Algorithms and Stochastic
Approximations. Springer-Verlag, Berlin, Heidelberg, 1990. ISBN 0-387-52894-6.
Shalabh Bhatnagar, Richard S. Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural actor-critic
algorithms. Automatica,45(11):2471-2482, November 2009.ISSN0005-1098. doi: 10.1016/j.
automatica.2009.07.008. URL http://dx.doi.org/10.1016/j.automatica.2009.07.008.
Kamil Ciosek and Shimon Whiteson. Expected Policy Gradients for Reinforcement Learning. arXiv
e-prints, art. arXiv:1801.03326, Jan 2018.
Martin Crowder. Stochastic approximation: A dynamical systems viewpoint by vivek s. borkar.
International Statistical Review, 77(2):306-306, 2009.
Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. CoRR, abs/1205.4839,
2012. URL http://arxiv.org/abs/1205.4839.
Scott Fujimoto, Herke van Hoof, and Dave Meger. Addressing function approximation error in actor-
critic methods. CoRR, abs/1802.09477, 2018. URL http://arxiv.org/abs/1802.09477.
Sham Kakade. A natural policy gradient. In Proceedings of the 14th International Conference on
Neural Information Processing Systems: Natural and Synthetic, NIPS’01, pages 1531-1538, Cam-
bridge, MA, USA, 2001. MIT Press. URL http://dl.acm.org/citation.cfm?id=2980539.
2980738.
Vijaymohan Konda. Actor-critic Algorithms. PhD thesis, Cambridge, MA, USA, 2002. AAI0804543.
Harold J. (Harold Joseph) Kushner and (joint author.) Clark, Dean S. Stochastic approximation
methods for constrained and unconstrained systems. New York : Springer-Verlag, 1978. ISBN
0387903410.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Manfred Otto Heess, Tom Erez,
Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. CoRR, abs/1509.02971, 2015.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-
critic for mixed cooperative-competitive environments. Neural Information Processing Systems
(NIPS), 2017.
Hamid Reza Maei. Convergent actor-critic algorithms under off-policy training and function approxi-
mation. CoRR, abs/1802.07842, 2018. URL http://arxiv.org/abs/1802.07842.
P. Marbach and J. N. Tsitsiklis. Simulation-based optimization of markov reward processes. IEEE
Transactions on Automatic Control, 46(2):191-209, Feb 2001. ISSN 0018-9286. doi: 10.1109/9.
905687.
K. J. Prabuchandran, Shalabh Bhatnagar, and Vivek S. Borkar. Actor-critic algorithms with online
feature adaptation. ACM Trans. Model. Comput. Simul., 26(4):24:1-24:26, February 2016. ISSN
1049-3301. doi: 10.1145/2868723. URL http://doi.acm.org/10.1145/2868723.
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., New York, NY, USA, 1st edition, 1994. ISBN 0471619779.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic Policy Gradient Algorithms. International Conference on Machine Learning, pages
387-395, January 2014a.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic Policy Gradient Algorithms. International Conference on Machine Learning, pages
387-395, January 2014b.
Wesley Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang, Tamer Basar, and Ji Liu. A multi-agent
off-policy actor-critic algorithm for distributed reinforcement learning. CoRR, abs/1903.06372,
2019. URL http://arxiv.org/abs/1903.06372.
10
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and
K. Muller, editors, Advances in Neural Information Processing Systems 12, pages 1057-1063. MIT
Press, 2000a.
Richard S Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In S. A. Solla, T. K. Leen, and
K. Muller, editors, Advances in Neural Information Processing Systems 12, pages 1057-1063. MIT
Press, 2000b.
Richard S. Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba
SzePeSv疝i, and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning
with linear function approximation. In Proceedings of the 26th Annual International Conference
on Machine Learning, ICML ’09, pages 993-1000, New York, NY, USA, 2009. ACM. ISBN
978-1-60558-516-1.
Richard S. Sutton, A. Rupam Mahmood, and Martha White. An emphatic approach to the problem
of off-policy temporal-difference learning. J. Mach. Learn. Res., 17(1):2603-2631, January 2016.
ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=2946645.3007026.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar. Fully decentralized multi-
agent reinforcement learning with networked agents. 80:5872-5881, 10-15 Jul 2018.
Yan Zhang and Michael M. Zavlanos. Distributed off-policy actor-critic reinforcement learning with
policy consensus. CoRR, abs/1903.09255, 2019.
11
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
Numerical experiment details
We demonstrate the convergence of our algorithm in a continuous bandit problem that is a multi-
agent extension of the experiment in Section 5.1 of Silver et al. (2014). Each agent chooses
an action ai ∈ Rm. We assume all agents have the same reward function given by Ri (a) =
-(Pi ai - a*)T C (Pi ai - a*). The matrix C is positive definite With eigenvalues chosen from
{0.1,1}, and a* = [4,..., 4]t. We consider 10 agents and action dimensions m = 10, 20, 50. Note
that there are multiple possible solutions for this problem, requiring the agents to coordinate their
actions to sum to a*. We assume a target policy of the form μθi = θi for each agent i and a Gaussian
behaviour policy β(∙)〜N(θi, σ%) where σ0 = 0.1. We use the Gaussian behaviour policy for both
Algorithms 1 and 2. Strictly speaking, Algorithm 1 is on-policy, but in this simplified setting Where
the target policy is constant, the on-policy version would be degenerate such that the Q estimate does
not affect the TD-error. Therefore, we add a Gaussian behaviour policy to Algorithm 1. Each agent
maintains an estimate Qωi (a) of the critic using a linear function of the compatible features a - θ
and a bias feature. The critic is recomputed from each successive batch of 2m steps and the actor
is updated once per batch. The critic step size is 0.1 and the actor step size is 0.01. Performance
is evaluated by measuring the cost of the target policy (without exploration). Figure 2 shows the
convergence of Algorithms 1 and 2 averaged over 5 runs. In all cases, the system converges and the
agents are able to coordinate their actions to minimize system cost. The jupyter notebook will be
made available for others to use. In fact, in this simple experiment, we also observe convergence
under discounted rewards.
Figure 2: Convergence of Algorithms 1 and 2 on the multi-agent continuous bandit problem.
Proof of Theorem 1
The proof follows the same scheme as Sutton et al. [2000a], naturally extending their results for a
deterministic policy μθ and a continuous action space A.
Note that our regularity assumptions ensure that, for any S ∈ S, Vθ(s), VθVθ(s), J(θ), Vθ J(θ),
dθ (s) are Lipschitz-continuous functions of θ (since μθ is twice continuously differentiable and Θ is
compact), and that Qθ (s, a) and VaQθ (s, a) are Lipschitz-continuous functions of a (Marbach and
Tsitsiklis [2001]).
We first show that Vθ J(θ) = Es* [Vθμ°(S)Va Qθ(s,a)la=μθ(S)].
The Poisson equation under policy μθ is given by Puterman [1994]
Qθ(s,a) = R(s,a) - J(θ) + X P(s0∣s,a)%(s').
s0∈S
12
358
359
360
361
362
363
364
365
366
367
368
So,
Vθ Vθ (S) = VθQθ (s,μθ (S))
=Vθ[R(s,μθ(S))- J(θ)+ X P(SlS,μθ(s))Vθ(s0)]
s0∈S
=vθμθ(S) VaR(S,a)∣a=μθ(s) - vθJ(θ) + vθ X P(s，|s,μθ(S))Vθ(SO)
s0∈S
=vθμθ (S) VaR(S,a)la="θ(s) - vθJ⑻
+ X vθ μθ (S) VaP(Sls, a)la=μθ(s) Vθ (SO) + X P(S0|S, 4θ (S))Vθ Vθ (SO)
s0∈S	s0∈S
=Vθμθ(S)VahR(s, a) + X P(S∣S0,。)%⑺]
s0∈S	a=μθ (S)
-VθJ(θ) + X P(SlS,μθ(s))VθVθ(so)
s0∈S
=vθμθ (S)Va Qθ (s,α)la=μθ(s) + X P(SlS,4θ (S))Vθ Vθ (SO)- Nf) J (θ)
s0∈S
Hence,
VfJ(θ) = Vfμθ(S)Va Qf(s,α)∣a=μθ(S) + X P(SlS,μθ(s))VfVf (s0) - VfVf (s)
s0∈S
X df (S)Vf J (θ) = X df (S)Vf μf (S)Va Qf (s,α)la=μθ (s)
S∈S	S∈S
+ X df (S) X P(SlS,μf (S))VfVf(s0) - X df (S)Vf Vf (s).
S∈S	S0∈S	S∈S
Using stationarity property of df, we get
XX
df(S)P(S0∣s,μf(s))VfVf(S0) = X df(so)VfVf(s0).
S∈S S0∈S	S0∈S
Therefore, we get
Vf J (θ) = X df (S)Vf μf (s) VaQf (S, a)la=μθ (S)= ES 〜dθ [Vf Mf (s) VaQf (S, a)|a=*g (s)].
S∈S
Given that Vfiμf (s) = 0 if i = j, We have Vfμf (s) = Diag(VfIμ1ι (s),..., VfNμN(s)), which
implies
VfiJ(θ) = ES〜dθ [Vfiμfi(S)Vai Qf(s,μ--i(s),ai)∣ai=%(s)]∙	(15)
Proof of Theorem 3
We extend the notation for off-policy reward function to stochastic policies as follows. Let β be a
behavior policy under which {St}t≥0 is irreducible and aperiodic, with stationary distribution dβ. For
a stochastic policy π : S → P(A), we define
Jβ (π) = Xdβ(S)	π(a∣s)R(s, a)da.
Recall that for a deterministic policy μ : S → A, we have
Je (M) = X dβ (S)R(S,μ(S))∙
S∈S
We introduce the following conditions which are identical to Conditions B1 from Silver et al.
[January 2014a].
13
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
Conditions 1. Functions νσ parametrized by σ are said to be regular delta-approximation on R ⊂ A
if they satisfy the following conditions:
1.	The distributions v@ converge to a delta distribution: limσjo JA v@ (a0, a)f (a)da = f (a0)
for a0 ∈ R and suitably smooth f. Specifically we require that this convergence is uniform
in a and over any class F of L-LiPschitz and bounded functions, 口口/(a)k< L < ∞,
supaf(a) < b < ∞, i.e.:
lim suP	νσ (a0, a)f (a)da - f(a0) = 0.
σJ0 f ∈F,a0∈R IJA
2.	For each a0 ∈ R, Vσ (a0, ∙) is supported on some compact C0jo ⊆ A with Lipschitz boundary
bd(Ca0), vanishes on the boundary and is continuously differentiable on Ca0.
3.	For each a0 ∈ R, for each a ∈ A, the gradient Na Vσ(a0, a) exists.
4.	Translation invariance: for all a ∈ A, a0 ∈ R, and any δ ∈ Rn such that a + δ ∈ A,
a0 + δ ∈ A, νσ (a0, a) = νσ (a0 + δ, a + δ).
The following lemma is an immediate corollary of Lemma 1 from Silver et al. [January 2014a].
Lemma 1. Let νσ be a regular delta-approximation on R ⊆ A. Then, wherever the gradients exist
Na0 ν(a0, a) = -Naν(a0, a).
Theorem 3 is a less technical restatement of the following result.
Theorem 8. Let μθ : S → A. Denote the range of μθ by Rg ⊆ A, and R = ∪θRθ. For
each θ, consider ∏g,σ a stochastic policy such that ∏g,σ(a|s) = Vσ(μg(s),a), where Vσ satisfy
Conditions 1 on R. Then, there exists r > 0 such that, for each θ ∈ Θ, σ 7→ Jπθ,σ (πθ,σ),
σ → J∏θ,σ (μg), σ → Vg J∏θ,σ (∏θ,σ), and σ → VgJ∏θ,σ (μg) are properly defined on [0,r] (With
J∏θ,0 (∏θ,o) = J∏θ,0 (μθ) = Jμθ (μθ) and Vθ J∏θ,0 (∏g,o) = Vg J∏θ,0 (μg) = Vg Jμg (μg)), and we
have:
lim vΘ J∏θ,σ (πθ,σ ) = lim vΘ J∏θ,σ (μθ ) = vΘ Jμθ (μθ ).
σψ0	σψ0
To prove this result, we first state and prove the following Lemma.
Lemma 2. There exists r > 0 such that, for all θ ∈ Θ and σ ∈ 0, r , stationary distribution dπθ,σ
exists and is unique. Moreover, for each θ ∈ Θ, σ 7→ dπθ,σ and σ 7→ Vg dπθ,σ are properly defined
on 0, r and both are continuous at 0.
Proof of Lemma 2. For any policy β, we let Pβ 0 be the transition matrix associated to the
s,s s,s0∈S
Markov Chain {st}t≥0 induced by β. In particular, for each θ ∈ Θ, σ > 0, s, s0 ∈ S, we have
pμso = P(Sls,μg (s)),
Pns0σ = / ∏θ,σ(a∣s)P(s0∣s, a)da = / v。(μg(s), a)P(s0∣s, a)da.
Let θ ∈ Θ, s,s0 ∈ S, (θn) ∈ ΘN such that θn → θ and (σn)n∈N ∈ R+N, σn J 0:
∣Pnsn ,σn — Pμso 1 ≤ [PS,SnC — PfSn 1 + lPμθn — Pμθo J .
Applying the first condition of Conditions 1 with f : a 7→ P(s0|s, a) belonging to F:
IPnSn,σn - pμθn∣ = y^vσn (μθn (S),a)P(S0|S,a)da - P(SlS,μθn (S))
≤ sup I
f∈F,a0∈R I A
vσn (a0,a)f(a)da — f(a0)	-→ 0.
n→∞
By regularity assumptions on θ → μg(s) and P(s0∣s, ∙), we have
∣PμSn — PS>∣ = ∣P(s0∣s,μθn(s)) — P(SlS,μg(s))| -→ 0.
,	,	n→∞
14
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
Hence,
πθn ,σn
Ps,s0
pNθ
Ps,s0
-→ 0.
n→∞
—
Therefore, for each s, s0 ∈ S, (θ, σ) → P：S『,with P：：0 = P^S, is continuous on Θ X {0}. Note
that, for each n ∈ N, P 7→ Qs,s0(Pn)s,s0is a polynomial function of the entries of P. Thus, for
each n ∈ N, fn : (θ,σ) → QsS (Pπθ,σTn)毡押,with fn(θ, 0) = QsS (PμθTn)SsO is continuous on
Θ × {0}. Moreover, for each θ ∈ Θ,σ ≥ 0, from the structure of Pπθ,σ, if there is some n* ∈ N
such that fn*(θ, σ) > 0 then, for all n ≥ n*, fn(θ, σ) > 0.
Now let us suppose that there exists (θn) ∈ Θn* such that, for each n > 0 there is a σ~ ≤ n-1 such
that fT (θT , σT ) = 0. By compacity of Θ, we can take (θT ) converging to some θ ∈ Θ. For each
n* ∈ N, by continuity we have fn*(θ, 0) = lim fn*(θn, σn) = 0. Since Pμθ is irreducible and
T→∞
aperiodic, there is some n ∈ N such that for all s,s0 ∈ S and for all n* ≥ n, (Pμθn*)	> 0, i.e.
fT* (θ, 0) > 0. This leads to a contradiction.
Hence, there exists n* > 0 such that for all θ ∈ Θ and σ ≤ n*-1, fT (θ, σ) > 0. We let r = n*-1. It
follows that, for all θ ∈ Θ and σ ∈ 0, r , P πθ,σ is a transition matrix associated to an irreducible and
aperiodic Markov Chain, thus dπθ,σ is well defined as the unique stationary probability distribution
associated to Pπθ,σ. We fix θ ∈ Θ in the remaining of the proof.
Let β a policy for which the Markov Chain corresponding to Pβ is irreducible and aperiodic. Let
s* ∈ S, as asserted in Marbach and Tsitsiklis [2001], considering stationary distribution dβ as a
vector dsβ s∈S ∈ R|S|, dβ is the unique solution of the balance equations:
dsβPsβ,s0 = dsβ0 s0 ∈ S\{s*},
s∈S
Xdsβ=1.
s∈S
Hence, we have Aβ an |S| × |S| matrix and a 6= 0 a constant vector of R|S| such that the balance
equations is of the form
Aβdβ = a	(16)
with Asβ,s0 depending on Psβ0,s in an affine way, for each s, s0 ∈ S. Moreover, Aβ is invertible, thus
dβ is given by
dβ =湎Ay adj(Aβ )>a.
Entries ofadj(Aβ) and det(Aβ) are polynomial functions of the entries of Pβ.
Thus, σ → dπθ,σ = det(A∏θ,σ)adj(Aπθ,σ )>a is defined on [0, r] and is continuous at 0.
Lemma 1 and integration by parts imply that, for s, s0 ∈ S, σ ∈ 0, r :
a0 νσ (a, a)K="θ(s) P(Sls, a)da
—
aVσ(μθ (s),a)P (s0∣s,a)da
I	Vσ(μθ(s), a) VaP(s0∣s, a)da + boundary terms
JCae (S)
I	Vσ(μθ(s), a)VaP(s0∣s, a)da
JCμg (S)
where the boundary terms are zero since νσ vanishes on the boundary due to Conditions 1.
15
423
Thus, for s, s0 ∈ S , σ ∈ 0, r:
VθP∏Sθσ = Vθ / ∏θ,σ(a∣s)P(s0∣s,a)da
,A
=/ Vθ∏θ,σ(a∣s)P(s0∣s,a)da
A
=/ vθ μθ (s) VaO νσ (a0,a)la0=μθ (S) P (s0∣s,a)da
A
=Vθμθ(s) /	Vσ(μθ(s),a)VaP(s0∣s, a)da
JCμθ (S)
(17)
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
where exchange of derivation and integral in (17) follows by application of Leibniz rule with:
•	∀a ∈ A, θ → ∏θ,σ(a∣s)P(s0∣s,a) is differentiable, and Vθ∏θ,σ(a∣s)P(s0∣s,a)
Vθ μθ (S) va0 νσ (a0,a)|a0=““s).
•	Let a* ∈R, ∀θ ∈ Θ,
kvθ πθ,σ (a|S)P(SlS,a)k = ∣∣vθ μθ (S) va0νσ (a0,a)la0=μθ (s)||
≤	kvθ μθ (S)kop || VaO νσ (a0,a)la0=μθ (s)||
≤	SUp kvθ μθ (S)kop kvaνσ (μθ(S), a)k
θ∈Θ
=SUp kVθμθ(S)kop kVaVσ(a*,a — μθ(s) + a*)k	(18)
θ∈Θ
≤	SUp kVθμθ(s)kop SUp kVaVσ(a*,a)k L』*
θ∈Θ	a∈Ca*
where k∙kop denotes the operator norm, and (18) comes from translation invariance (We take
VaVσ (a*, a)=0 for a ∈ Rn∖Ca* ). a → SUp kVθ μθ (s)kop SUp kVaVσ (a*,a)k 1a∈Ca* is
θ∈Θ	a∈Ca*
measurable, bounded and supported on Ca* , so it is integrable on A.
• Dominated convergence ensures that, for each k ∈ J1, mK, partial derivative gk (θ) =
∂θk JA Vθ∏θ,σ(a∣s)P(s0|s, a)da is continuous: let θn J θ, then
gk(θn) = ∂θk	Vθ πθn,σ (a|S)P (S0 |S, a)da
A
=∂θkμθn (s) /	Vσ(a*,a — μθn (s) + a*)V°P(s0∣s,a)da
Ca*
—› ∂θkμθ(s) /	Vσ(a*,a — μθ(s) + a*)VaP(s0∣s, a)da = gk(θ)
n→∞	Ca*
with the dominating function a → SUp ∣Vσ(a*, a)∣SUp IlVaP(s0|s, a)k 1a∈cα*.
a∈Ca*
a∈A
Thus σ 7→ VθPsπ,θs,0σ is defined for σ ∈	0, r and is continuous at 0, with VθPsπ,θs,00 =
Vθμθ(s) VaP(s0∣s,a)∣a=μθ(s). Indeed, let (σn)n∈N ∈ [0,r]+ , σn J 0, then, applying the first
condition of Conditions 1 with f : a 7→ VaP(s0|s, a) belonging to F, we get
|恒 P0σn-vθ pμ：s0∣
kvθ μθ (S)kop
I	νσn (μθ (S), a)VaP(Sls, a)da - VaP(S0|s,叽=μθ(s)
JCμg(S)
-→ 0.
n→∞
Since dπθ,σ = det(A∏θ,σ)adj (Aπθ,σ )> a with | det(Aπθ,σ) | > 0 for all σ ∈ [0, r] and since entries
of adj (Aπθ,σ) and det (Aπθ,σ) are polynomial functions of the entries of Pπθ,σ, it follows that
16
440
441
442
443
444
445
446
447
448
449
450
451
σ 7→ Vθdπθ,σ is properly defined on 0, r and is continuous at 0, which concludes the proof of
Lemma 2.	口
We now proceed to prove Theorem 8.
Let θ ∈ Θ, πθ as in Theorem 3, andr > 0 such that σ 7→ dπθ,σ, σ 7→ Vθdπθ,σ are well defined on
0,r and are continuous at 0. Then, the following two functions
σ → J∏θσ(∏θ,σ) = Y2dπθ,σ(s) / ∏θ,σ(a∣S)R(S,a)da,
s∈S	A
σ → J∏θ,σ (μθ) = Edπθ,σ (S)R(S,μθ(S)),
s∈S
are properly defined on [0,r] (with J*,。(∏θ,o) = J∏θ,ο (〃©) = J“° (〃©)). Let S ∈ S, by taking
similar arguments as in the proof of Lemma 2, we have
Vθ / ∏θ,σ(a∣S)R(S,a)da = / Vθ∏θ,σ(a, s)jR(s, a)da,
=Vθμθ(s) /	Vσ(μθ(s),。)VaR(S,a)da.
Thus, σ 7→ VθJπθ,σ (πθ,σ) is properly defined on 0,r	and
Vθ J∏θ σ(∏θ,σ) = V" Vθdπθ,σ(s) / ∏θ,σ(a∣s)R(s,a)da
,	s∈S	A
十 ^X dπθ,σ(s)Vθ J ∏θ,σ(a∣s)R(s,a)da
=ɪ2Vθdπθ,σ(s) / Vσ(μθ(s),a)R(s,a)da
s∈S	A
+ E dπθ,σ (s)Vθμθ(s) /	Vσ(μθ(s), a)VɑR(s, a)da.
s∈S	JCμe (S)
Similarly, σ → VθJ∏θ,σ (μθ) is properly defined on [0, r] and
vθJ∏θ,σ (μθ) = X vθdπθ,σ (S)R(S,μθ(S)) + X dπθ,σ (s)vθμθ(S) V *aR(S,a)∣a=μθ(S)
s∈S
s∈S
To prove continuity at 0 of both σ → Vθ J∏θ σ(∏θ σ) and σ → Vθ J∏θ σ(μθ) (with Vθ J∏θ 0 (∏θ 0)
Vθ J∏θ,o (μθ) = Vθ Jμθ (μθ)), let (σn)n≥0 J 0:
VθJπθ,σn (πθ,σn) - VθJπθ,0 (πθ,0)
≤ llVθ J∏θ,σn (∏θ,σn S Jf 3 )|| 十 ||Vj JM "θ S Jμθ Jθ )|| .	(19)
For the first term of the r.h.s we have
llvθ j∏θ,σn (πθ,σn) - Ne j∏θ,σn 3 )||
≤ XkVθdπθ,σn(S)k
s∈S
/ Vσn(μθ(s), a)R(s, a)da - R(s, μe(s))
A
+ Edπθ,σn(s)kvθ μe (s)kop
s∈S
V νσn (μθ (S),a)VaR(S,a)da — VaR(S,a)∣α=*g(s).
A
17
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
Applying the first assumption in Condition 1 with f : a → R(s, a) and f : a → NaR(s, a) belonging
to F we have, for each s ∈ S :
∣J Vσn (μθ (s), a)R(s, a)da - R(s,μθ (S))
I νσn (μθ (S),a)NaR(S,a)da — VaJR(s,a)∣a	(s)
A
—> 0 and
n→∞
—→ 0.
n→∞
Moreover, for each S ∈ S, dπθ,σn (S) —→ dμθ (s) and Vθdπθ,σn (s) —→ Vθdμθ (s) (by Lemma 2),
n→∞	n→∞
and ∣∣Vθμθ(s)∣∣0p< ∞, so
llvθ Jπθ,σn (πθσn ) -	JFn 3θ 升-∞ "
For the second term of the r.h.s of (19), we have
∣∣V)J∏θ,σn(μ)) -V)Jμθ(μθ)∣∣ ≤ XkV)dπθ,σn(s) -V)dμθ(S)k∣R(S,μθ(s))∣
s∈S
+ X ldπθ,σn(S)- dμ (S)lkvθ μθ (S)kop Il VaR(S,a)la="θ(S) Il .
s∈S
Continuity at 0 of σ → dπθ,σ (s) and σ → V)dπθ,σ(s) for each S ∈ S, boundedness of R(s, ∙),
VaR(S, ∙) and V)(s)mθ(s) implies that
Il vθ J∏θ,σn (μθ) - vθ jμθ (μθ )ll n→∞ 0.
Hence,
IIvθ Jπθ,σn (πθ,bn ' - Ni)」卬(3 升-∞ "
So, σ → Vθ J∏θ,σ (∏θ,σ) and Vθ J∏θ,σ (μe) are continuous at 0:
lim vΘ J∏θ,σ (πθ,σ ) = Ilm vΘ j∏θ,σ(μθ ) = vΘ jμθ (μθ ).
σψ0	σψ0
Proof of Theorem 4
We will use the two-time-scale stochastic approximation analysis . We let the policy parameter θt
fixed as θt ≡ θ when analysing the convergence of the critic step. Thus we can show the convergence
of ωt towards an ωθ depending on θ, which will then be used to prove the convergence for the slow
time-scale.
Lemma 3. Under Assumptions 3-5, the sequence ω∖i generated from (2) is bounded a.s., i.e.,
supt∣ωti∣< ∞ a.s., for any i ∈ N.
The proof follows the same steps as that of Lemma B.1 in the PMLR version of Zhang et al. [2018].
Lemma 4. UnderAssumption 5, the sequence {J*} generated as in 2 is bounded a.s, i.e., sup∕Ji∣ <
∞ a.s., for any i ∈ N.
The proof follows the same steps as that of Lemma B.2 in the PMLR version of Zhang et al. [2018].
The desired result holds since Step 1 and Step 2 of the proof of Theorem 4.6 in Zhang et al. [2018]
can both be repeated in the setting of deterministic policies.
Proof of Theorem 5
Let Ft,2 = σ(θτ, Sτ, τ ≤ t) a filtration. In addition, we define
HGS,M = vθ μθ (S) ∙ vaQω (S,a)la=μθ (S),
H(θ, S) = H(θ, S, ωθ),
h(θ) = ES〜dθ [H(θ, s)].
18
476
477
478
479
480
481
482
483
484
485
486
487
1t 2t 3
AAA
Then, for each θ ∈ Θ, we can introduce νθ : S → Rn the solution to the Poisson equation:
(I - Pθ)νθ(∙) = H(θ,∙)- h(θ)
that is given by vθ(S) = Pk≥0 Esk+1 〜Pθ(∙∣sk)[H(θ,sk) - h(θ)∣so = s] which is properly defined
(similar to the differential value function V ).
With projection, actor update (5) becomes
θt+1 = Γ [θt + βθ,tH(θt, st, ωt)]	(20)
= Γ [θt	+ βθ,th(θt)	- βθ,t (h(θt) - H(θt, st))	- βθ,t (H(θt, st)	-	H(θt, st, ωt))]
=Γ [θt	+ βθ,th(θt)	+ βθ,t ((I - Pθt)νθt (St))	+ βθ,tAl]
=γ [θt	+ βθ,th(θt)	+ βθ,t (νθt (St) - νθt (St+1)) + βθ,t (νθt (St+1)	- Pθt νθt (St)) + βθ,tA1]
=γ [θt	+ Bθ,t (hR) + A1 + At + A3)]
where
H(θt, St, ωt) - H(θt, St),
νθt (St) - νθt (St+1),
νθt (St+1) - Pθtνθt (St).
For r < t we have
t-1	t-1
βθ,kA2k =	βθ,k (νθk (Sk) - νθk (Sk+1))
k=r	k=r
t-1	t-1
=Eβθ,k (νθk (Sk) - νθk+ι (Sk+ 1)) + Eβθ,k (νθk+ι (Sk+1)- νθk (Sk+1))
k=r	k=r
t-1	t-1
=	(βθ,k+1 - βθ,k) νθk+1 (Sk+1) + βθr νθr (Sr) - βθt νθt (St) +	(k2)
k=r	k=r
t-1	t-1
=X(k1)+X(k2)+ηr,t
k=r	k=r
where
(k1) = (βθ,k+1 - βθ,k) νθk+1 (Sk+1),
ek2) = βθ,k (νθk+ι (Sk+1)- νθk (Sk+1)),
ηr,t = βθr νθr (Sr) - βθt νθt (St).
Lemma 5. Ptk-=10 βθ,k A2k converges a.s. for t → ∞
Proof of Lemma 5. Since νθ(S) is uniformly bounded for θ ∈ Θ, S ∈ S, we have for some K > 0
t-1	t-1
X HI)Il ≤ K X lβθ,k+1 - βθ,k |
k=0	k=0
which converges given Assumption 5.
Moreover, since μθ(s) is twice continuously differentiable, θ → vθ(s) is Lipschitz for each s, and so
we have
t-1	t-1
XII(k2)II ≤Xβθ,k
IIνθk (Sk+1) - νθk+1 (Sk+1)II
k=0	k=0
t-1
≤ K2 X βθ,k kθk - θk+1 k
k=0
t-1
≤ K3 Xβθ2,k.
k=0
19
488
Finally, lim kη0,tk = βθ,0 kνθ0(s0)k < ∞ a.s.
t→∞
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
rτ-><	v-''t — 1 IlQ 2 2 11	J v-''t — 1	(1)	,	v-''t — 1	(2)	,	U U	I-I
Thus, ∑k=o lβθ,kAkii	≤ ∑k=o	∣∣ek ∣∣	+	∑k=o kk	∣∣ +	kηo,tk ConvergesaS	口
Lemma 6. Ptk-=10 βθ,k A3k converges a.s. for t → ∞.
Proof of Lemma 6. We set
t-1	t-1
Zt = Xβθ,kAk = Xβθ,k (νθk(Sk+1)- Pθkνθk(Sk)).
k=0	k=0
Since Zt is Ft-adapted and E [νθt (St+1)|Ft] = Pθtνθt (St), Zt is a martingale. The remaining of the
proof is now similar to the proof of Lemma 2 on page 224 of Benveniste et al.[1990].	□
Let gi(θt) = Est 〜dθt [ψi ∙ξi,θt∣Ft,2] and g(θ) = [g1(θ),...,gN (θ)]. Wehave
gi(θt) = X dθt(St) ∙ Ψt ∙ ξi,θt.
st∈S
Given (10), θ → ωθ is continuously differentiable and θ → Vθ3§ is bounded so θ → 3§ is
Lipschitz-continuous. Thus θ 7→ ξti,θ is Lipschitz-continuous for each St ∈ S. Due to our regularity
assumptions, θ 7→ ψti,θ is also continuous for each i ∈ N, St ∈ S. Moreover, θ 7→ dθ (S) is also
Lipschitz continuous for each S ∈ S. Hence, θ 7→ g(θ) is Lipschitz-continuous in θ and the ODE
(12) is well-posed. This holds even when using compatible features.
By critic faster convergence, we have limt→∞ kξti - ξti,θ k= 0 so limt→∞At1 = 0.
Hence, by Kushner-Clark lemma Kushner and Clark [1978] (pp 191-196) we have that the update in
(20) converges a.s. to the set of asymptotically stable equilibria of the ODE (12).
Proof of Theorem 6
We use the two-time scale technique: since critic updates at a faster rate than the actor, we let the
policy parameter θt to be fixed as θ when analysing the convergence of the critic update.
Lemma 7. Under Assumptions 4, 1 and 6, for any i ∈ N, sequence {λit } generated from (7) is
bounded almost surely.
To prove this lemma we verify the conditions for Theorem A.2 of Zhang et al. [2018] to hold.
We use {Ft,1} to denote the filtration with Ft,1 = σ(Sτ, Cτ-1, aτ-1, rτ, λτ, τ ≤ t). With λt =
(λt1)>, . . . , (λtN)>>, critic step (7) has the form:
λt+1 = (Ct Z) I) (λt + 8λ,t ∙ yt+1)	QI)
with yt+1 = (δ11w(st, at)>,..., δtNw(st, at)>)> ∈ RKN, 0 denotes Kronecker product and I is
the identity matrix. Using the same notation as in Assumption A.1 from Zhang et al. [2018], we
have:
hi(λi, St) = Ea〜∏ [δiw(st, a)>∣Ft,1] = / π(a∣St)(Ri(st,a) 一 W(St, a) ∙ λt)w(st, a)>da,
A
Mi+1 = δiw(St ,at )> — Ea 〜∏[δiw(St,a)>∣Ftj],
hi(λt) = A∏,θ ∙ d∏ — B∏,θ ∙ λt,	where A∏,θ = ]/ π(a∣S)Ri(s, a)w(S, a)>da, s ∈ S .
Since feature vectors are uniformly bounded for any S ∈ S and a ∈ A, hi is Lipschitz continuous in
its first argument. Since, for i ∈ N, the ri are also uniformly bounded, E[∣∣Mt+1∣∣2∣Ft,1] ≤ K ∙ (1 +
∣∣λtk2) for some K > 0. Furthermore, finiteness of |S| ensures that, a.s., ∣∣h(λt) — h(λt, St)II2≤
K0 ∙ (1 + ∣∣λt∣2). Finally, h∞(y) exists and has the form
h∞(y) = -B∏,θ ∙ y.
From Assumption 1, we have that -B∏,θ is a Hurwitcz matrix, thus the origin is a globally asymptot-
ically stable attractor of the ODE y = h∞(y). Hence Theorem A.2 of Zhang et al. [2018] applies,
which concludes the proof of Lemma 7.
20
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
We introduce the following operators as in Zhang et al. [2018]:
•	h∙i : RKN → RK
凶=3> ㊈ I)λ = NN X λi.
i∈N
•	J = (N 11> 乳 I) :	RKN →	RKN SUCh that Jλ = 1 乳<λ>.
•	Jl = I -J : RKN	→ RKN	and we note λ⊥ = J⊥λ =	λ - 1	0<λ>.
We then proCeed in two steps as in Zhang et al. [2018], firstly by showing the ConvergenCe a.s. of the
disagreement veCtor seqUenCe {λ⊥,t} to zero, seCondly showing that the ConsensUs veCtor seqUenCe
{hλti} Converges to the eqUilibriUm sUCh that hλti is solUtion to (13).
Lemma 8. Under Assumptions 4, 1 and 6, for any M > 0, we have
suP E [kβ-,t λ⊥,tk21{supt kλt k≤M}
< ∞.
SinCe dynamiC of {λt} desCribed by (21) is similar to (5.2) in Zhang et al. [2018] we have
Ehkβ-1, 1λ⊥ t+1k2∣Ft li = βλt-P (kβ-1λ⊥ tk2+2 ∙ kβ-1λ⊥ t∣∣∙E(kyt+ιk2∣Ft 1)1 + E(kyt+1k2∣Ft I))
λ,t+1 ,t+1	t,1	2	λ,t ,t	λ,t ,t	t+1	t,1	t+1	t,1
λ,t+l	(22)
where P represents the spectral norm of E [Cj ∙ (I 一 11>/N) ∙ Ct], with P ∈ [0,1) by Assumption
4. Since yi+ι = δi ∙ w(st, a⅛)τ We have
Ehkyt+iklFtj] = Eh XkSi(St, at) - W(St, at)λt) ∙W(St, at)Tk2|Ft,ii
i∈N
≤ 2 ∙ EhX kri(st,at)w(st,at)τk2 + ∣w(st,at)TkTNk2FtJ
i∈N
By uniform boundedness of r(s, ∙) and w(s, ∙) (Assumptions 1) and finiteness of S, there exists
Kl > 0 such that
Ehkyt+lk2|Ft,li ≤ Kl(1 + kλtk2).
Thus, for any M > 0 there exists K2 > 0 such that, on the set {supτ ≤t kλτ k< M},
Ehkyt+ιk2l{supτ≤tkλτk<M}∣Ft,ι] ≤ K2	(23)
We let vt =	kβ-,tλ⊥,tk2l{supτ≤tkλτk<M}.	Taking expectation over (22), noting that
^⅛uPτ≤t + 1kλτk<M} ≤ !{suPτ≤tkλτ k<M} Weget
E(vt+1) ≤ Jλ^P(E(Vt) + 2PE(v7 ∙ pΚ2 + K2)
βλ,t+l
which is the same expression as (5.10) in Zhang et al. [2018]. So similar conclusions to the ones of
Step 1 of Zhang et al. [2018] holds:
and
SUP E [∣∣β-,t λ⊥,t∣∣2 l{suptkλtk≤M}] < ∞
lim λ⊥,t = 0 a.s.
(24)
(25)
We now show convergence of the consensus vector 1 0〈》/. Based on (21) We have
hλt+ιi = h(Ct0 I)(10 hλti + λ⊥,t + βλ,tyt+1)i
= hλti + hλ⊥,ti + βλ,t h(Ct 0 I)(yt+l + βλ-,lt λ⊥,t)i
= hλt i + βλ,t (h(λt , St ) + Mt+l )
21
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
whereh(λt,st) = Eat〜∏[hyt+ιi∣Ft] andMt+ι = h(C乳I)(yt+ι + β-,tλ⊥,t)i-Ea,〜∏[hyt+0∣Ft].
Since ®=尸(st,at) - w(st,at)(λti,wehave
h(λt, St) = Eat〜∏(r(st, at)w(st, at)>Ft) + Eat〜∏(w(st, at)hλti ∙ W(st, at)>∣Ft,ι)
so h is Lipschitz-continuous in its first argument. Moreover, since hλ⊥,ti = 0 and 1>E(Ct|Ft,1) =
1> a.s.:
Eat〜∏ [h(Ct 乳 I)(yt+ι + β-,tλ⊥,t)i∣Ft,ι] = Eaun h5(1> 乳 I)(Ct 乳 I)(yt+ι + β-,tλ⊥,t)∣Ft,ι]
=NF(1> 乳 I)(E(Ct∣Ft,ι)乳 I)Eat〜n [yt+1 + β-,tλ⊥,t∣Ft,ι]
=NF(I>E(Ct∣Ft,ι)乳 I曲,〜∏ [yt+ι + β-,tλ⊥,t∣Ft,ι]
=Eat〜∏ [hyt+1i∣Ft,1] a.s.
So {Mt } is a martingale difference sequence. Additionally we have
E[kMt+ιk2∣Ft,ι] ≤ 2 ∙ E[kyt+ι + β-λ⊥,t∣∣Gt∣Ft,ι] +2 ∙∣∣E[hyt+1iFt,1] k2
with Gt = N-2 ∙ C> 11>Ct 01 whose spectral norm is bounded for Ct is stochastic. From (23) and
(24) we have that, for any M > 0, over the set {suptkλtk≤ M}, there exists K3, K4 < ∞ such that
E[kyt+1+β-,tλ⊥,tkGt lFtt,l] l{supt∣∣λtk≤M} ≤ K3.E[kyt+1k2 + ke-,tλ⊥,tk2lFt,l] l{supt∣∣λtk≤M} ≤ K4.
Besides, since rti+1 and w are uniformly bounded, there exists K5 < ∞ such that
∣∣E[hyt+1i∣Ft,1] k2≤ K5 ∙ (1 + ∣∣hλtik2). Thus, for any M > 0, there exists some K ‹ ∞
such that over the set {supt kλt k≤ M}
E[kMt+1k2∣Ft,1] ≤ K ∙ (1 + ∣hλtik2).
Hence, for any M > 0, assumptions (a.1) - (a.5) of B.1. from Zhang et al. [2018] are verified on the
set {supt∣λt∣≤ M}. Finally, we consider the ODE asymptotically followed by hλti:
..∙ . _ . . 一
hλti = -Bπ,θ ∙ hλti + Aπ,θ ∙ d
which has a single globally asymptotically stable equilibrium λ* ∈ RK, since B∏,θ is positive
definite: λ* = B-θ ∙ A∏,θ ∙ dπ. By Lemma 7, SuPtkhλti∣< ∞ a.s., all conditions to apply Theorem
B.2. of Zhang et al. [2018] hold a.s., which means that (λt) —→ λ* a.s. As λt = 1 0 <λt) + λ⊥ t
t→∞	,
and λ⊥ t -→ 0 a.s., we have for each i ∈ N, a.s.,
, t→∞
λt t→∞ B-,θ ∙ An,θ ∙ dπ.
Proof of Theorem 7
Let Ft,2 = σ(θτ, τ ≤ t) be the σ-field generated by {θτ, τ ≤ t}, and let
ζi,ι = ψi ∙ ξi - Est 〜d∏ M ∙ ξi∣Ft,2],	ζi,2 = Est 〜d∏ M ∙ (ξt - ξitfit )∣Ft,2].
With local projection, actor update (6) becomes
θi+ι = ri [θi+βθ,t% 〜d∏ M ∙ ξi,θt ιFt,2] + βθ,tζi,ι+βθ,tζi,2].	(26)
So with h (θt) = Est〜dπ [ψi ∙ ξi,θt lFt,2i and h(θ) = [h1(θ),...,hN (θ)], we have
hi(θt) = X dπ(St) ∙ Ψi ∙ ξi,θt.
st∈S
Given (10), θ θ ωθ is continuously differentiable and θ → Vθωθ is bounded so θ → ωθ is Lipschitz-
continuous. Thus θ 7→ ξti,θ is Lipschitz-continuous for each St ∈ S. Our regularity assumptions
22
560 ensure that θ 7→ ψti,θ is continuous for each i ∈ N , st ∈ S. Moreover, θ 7→ dθ (s) is also Lipschitz
561 continuous for each s ∈ S. Hence, θ 7→ g(θ) is Lipschitz-continuous in θ and the ODE (12) is
562 well-posed. This holds even when using compatible features.
563
564
565
566
567
568
By critic faster convergence, we have limt→∞ kξti - ξti,θ k= 0.
Let Mti = Ptτ-=10 βθ,τ ζτi,1. Mti is a martingale sequence with respect to Ft,2. Since
{ωt}t, {Vaφk(s, a)}s,k, and {Vθμθ(s)}s are bounded (Lemma 3, Assumption 2), it follows
that the sequence {Zi,ι} is bounded. Thus, by Assumption 5, Pt E IjlMt十1一 M"『∣Ft,2] =
Pt llβθ,tζti,1ll2 <∞ a.s. The martingale convergence theorem ensures that Mti } converges a.s.
Thus, for any > 0,
lim P
t
n
Xβθ,τζτi,1
τ=t
≥	= 0.
569 Hence, by Kushner-Clark lemma Kushner and Clark [1978] (pp 191-196) we have that the update in
570 (26) converges a.s. to the set of asymptotically stable equilibria of the ODE (12).
23