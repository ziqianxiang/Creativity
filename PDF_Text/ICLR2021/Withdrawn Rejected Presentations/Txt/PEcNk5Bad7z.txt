Under review as a conference paper at ICLR 2021
Learning Irreducible Representations of
Noncommutative Lie Groups
Anonymous authors
Paper under double-blind review
Ab stract
Recent work has constructed neural networks that are equivariant to continuous
symmetry groups such as 2D and 3D rotations. This is accomplished using explicit
group representations to derive the equivariant kernels and nonlinearities. We
present two contributions motivated by frontier applications of equivariance beyond
rotations and translations. First, we relax the requirement for explicit Lie group
representations, presenting a novel algorithm that finds irreducible representations
of noncommutative Lie groups given only the structure constants of the associated
Lie algebra. Second, we demonstrate that Lorentz-equivariance is a useful prior for
object-tracking tasks and construct the first object-tracking model equivariant to
the Poincare group.
1	Introduction
Many tasks in machine learning exactly or approximately obey a continuous symmetry such as
2D rotations. An ML model is said to be equivariant to such a symmetry if the model respects
it automatically (without training). Equivariant models have been applied to tasks ranging from
computer vision to molecular chemistry, leading to a generalization of equivariance techniques beyond
2D rotations to other symmetries such as 3D rotations. This is enabled by known mathematical results
about each new set of symmetries. Specifically, explicit group representation matrices for each new
symmetry group are required. For many important symmetries, formulae are readily available to
produce these representations. For other symmetries we are not so lucky, and the representations
may be difficult to find explicitly. In the worst cases, the classification of the group representations is
an open problem in mathematics. For example, in the important case of the homogeneous Galilean
group, which we define in section 2, the classification of the finite dimensional representations is
a so-called “wild algebraic problem” for which we have only partial solutions (De Montigny et al.,
2006; Niederle & Nikitin, 2006; Levy-Leblond, 1971).
To construct an equivariant network without prior knowledge of the group representations, novel
approaches are needed. In this work, we propose an algorithm LearnRep that finds the representation
matrices with high precision. We validate that LearnRep succeeds for the Poincare group, a set
of symmetries governing phenomena from particle physics to object tracking. We further validate
LearnRep on two additional sets of symmetries where formulae are known. We apply the POinCare
group representations obtained by LearnRep to construct SpacetimeNet, a POinCare-equivariant
object-tracking model. As far as we are aware, LearnRep is the first automated solver which can find
explicit representation matrices for sets of symmetries which form noncompact, noncommutative Lie
groups Further, SpacetimeNet is the first object-tracking model with a rigorous guarantee of POinCare
group equivariance.
1.1	Group Representations and Equivariant Machine Learning
Group theory provides the mathematical framework for describing symmetries and building equiv-
ariant ML models. Informally, a symmetry group G is a set of invertible transformations α, β ∈ G
which can be composed together using a product operation αβ . We are interested in continuous
symmetries for which G is a Lie group. In prior constructions of Lie group-equivariant models,
group representations are required. For a group G, an n-dimensional (real) group representation
ρ : G → Rn×n is a mapping from each element α ∈ G to an n × n-dimensional matrix ρ(α), such
that for any two elements α, β ∈ G, we have ρ(α)ρ(β) = ρ(αβ).
1
Under review as a conference paper at ICLR 2021
Two parallel techniques have been developed for implementing Lie group equivariant neural networks.
The first approach was described in general by Cohen et al. (2019). For the latter approach taken by
Thomas et al. (2018); Anderson et al. (2019); Bogatskiy et al. (2020), convolutions and nonlinearities
are performed directly on the irreducible representations of the group, which we define in section 2.4.
A common thread in these works has been to utilize existing formulas derived for the matrix elements
of these irreducible representations. However, these formulas are only available for specific Lie
groups where the representation theory is well-understood. A more convenient approach for extending
equivariance to novel Lie groups would utilize an automated computational technique to obtain the
required representations. The primary contribution of this work is such a technique.
1.2	Contributions
In this work, we automate the generation of explicit group representation matrices of Lie groups using
an algorithm called LearnRep. LearnRep poses an optimization problem defined by the Lie algebra
associated with a Lie group, whose solutions are the representations of the algebra. A penalty term is
used to prevent the formation of trivial representations. Gradient descent of the resulting loss function
produces nontrivial representations upon convergence. We apply LearnRep to three noncommutative
Lie groups for which the finite-dimensional representations are well-understood, allowing us to verify
that the representations produced are irreducible by computing their Clebsch-Gordan coefficients and
applying Schur’s Lemma.
One of the Lie groups where LearnRep performs well is the Lorentz group of special relativity. Prior
work has applied Lorentz-equivariant models to particle physics. In this work we explain that the
Lorentz group along with the larger Poincare group also governs everyday object-tracking tasks. We
construct a Poincare-equivariant neural network architecture called SpacetimeNet and demonstrate
that it can learn to solve a 3D object-tracking task subject to “motion equivariance,” where the inputs
are a time series of points in space.
In summary, our contributions are:
•	LearnRep, an algorithm which can find irreducible representations of a noncompact and
noncommutative Lie group.
•	SpacetimeNet, a POinCare group-equivariant neural network applied to object-tracking
tasks.
Our work contributes towards a general framework and toolset for building neural networks equivari-
ant to novel Lie groups, and motivates further study of Lorentz equivariance for object tracking.
1.3	Organization
We summarize all necessary background and terminology in section 2. We describe the LearnRep
algorithm in section 3.1 and SpacetimeNet in section 3.2. We summarize related work in section 4.
We present our experimental results in section 5: our experiments in learning irreducible Lie group
representations with LearnRep in section 5.1 and the performance of our POinCare-equivariant
SpacetimeNet model on a 3D object tracking task in section 5.2.
2	Technical Background
We explain the most crucial concepts here and defer to Appendix A.1 for a derivation of the represen-
tation theory of the Lorentz group.
2.1	S YMMETRY GROUPS SO(n) AND SO(m, n)
A 3D rotation may be defined as a matrix A : ∈ R3 ×3 which satisfies the following properties, in
which h~u, ~vi = Pi3=1 uivi:
(i)	detA = 1 (ii) ∀~u, ~v ∈ R3, hA~u, A~vi = h~u, ~vi;
these imply the set of 3D rotations forms a group under matrix multiplication and this group is
denoted SO(3). This definition directly generalizes to the n-dimensional rotation group SO(n). For
2
Under review as a conference paper at ICLR 2021
n ≥ 3, the group SO(n) is noncommutative, meaning there are elements A, B ∈ SO(n) such that
AB 6= BA. Allowing for rotations and translations of n dimensional space gives the n-dimensional
special Euclidean group SE(n). SO(n) is generalized by a family of groups denoted SO(m, n), with
SO(n) = SO(n, 0). For integers m, n ≥ 0, we define h~u, ~vim,n = Pim=1 uivi - Pim=+mn+1 uivi. The
group SO(m, n) is the set of matrices A ∈ R(m+n)×(m+n) satisfying (i-ii) below:
(i) detA= 1 (ii) ∀~u, ~v ∈Rm+n,hA~u,A~vim,n = h~u, ~vim,n;
these imply that SO(m, n) is also a group under matrix multiplication. While the matrices in
SO(n) can be seen to form a compact manifold for any n, the elements of SO(m, n) form a
noncompact manifold whenever n, m ≥ 1. For this reason SO(n) and SO(m, n) are called compact
and noncompact Lie groups respectively. The representations of compact Lie groups are fairly well
understood, see Bump (2004); Cartan (1930).
2.2	ACTION OF SO(m, n) ON SPACETIME
We now explain the physical relevance of the groups SO(m, n) by reviewing spacetime. We refer
to Feynman et al. (2011) (ch. 15) for a pedagogical overview. Two observers who are moving at
different velocities will in general disagree on the coordinates {(ti, ~ui)} ⊂ R4 of some events in
spacetime. Newton and Galileo proposed that they could reconcile their coordinates by applying a
spatial rotation and translation (i.e., an element of SE(3)), a temporal translation (synchronizing their
clocks), and finally applying a transformation of the following form:
ti 7→ ti	~ui 7→ ~ui + ~vti,	(1)
in which ~v is the relative velocity of the observers. The transformation equation 1 is called a Galilean
boost. The set of all Galilean boosts along with 3D rotations forms the homogeneous Galilean group
denoted HG(1, 3). Einstein argued that equation 1 must be corrected by adding terms dependent
on || ~v||2/c, in which c is the speed of light and || ~v||2 is the `2 norm of ~v. The resulting coordinate
transformation is called a Lorentz boost, and an example of its effect is shown in figure 1. The set
of 3D rotations along with Lorentz boosts is exactly the group SO(3, 1). In the case of 2 spatial
dimensions, the group is SO(2, 1). Including spacetime translations along with the Lorentz group
SO(n, 1) gives the larger Poincare group Pn with n spatial dimensions. The Poincare group P3 is
the group of coordinate transformations between different observers in special relativity.
Consider an object tracking task with input data consisting of a spacetime point cloud with n
dimensions of space and 1 of time, and corresponding outputs consisting of object class along with
location and velocity vectors. A perfectly accurate object tracking model must respect the action
of Pn on the input. That is, given the spacetime points in any observer’s coordinate system, the
perfect model must give the correct outputs in that coordinate system. Therefore the model should be
Pn-equivariant. For low velocities the symmetries of the homogeneous Galilean groups HG(n, 1)
provide a good approximation to SO(n, 1) symmetries, so Galilean-equivariance may be sufficient for
some tasks. Unfortunately the representations of HG(n, 1) are not entirely understood De Montigny
et al. (2006); Niederle & Nikitin (2006); Levy-Leblond (1971).
2.3	Lie Groups and Lie Algebras
Here we give an intuitive summary of Lie groups and Lie algebras, deferring to Bump (2004) for a
rigorous technical background. A Lie group G gives rise to a Lie algebra A as its tangent space at
the identity. This is a vector space V along with a bilinear product called the Lie bracket: [a, b] which
must behave like1 the commutator for an associative ring R with multiplication operation ×R :
[a, b] = a ×R b - b ×R a
The Lie algebra for SO(3), denoted so(3), has a basis {J1, J2, J3} satisfying
[Ji,Jj] = ijkJk,	(2)
in which ijk ∈ {±1, 0} is the totally antisymmetric Levi-Civita symbol.2 Intuitively, the Lie bracket
shows how group elements near the identity fail to commute. For example, the matrices Rx , Ry , Rz
1Specifically, the Lie bracket must satisfy the Jacobi identity and [a, a] = 0.
2The symbol ijk simply expresses in equation 2 that [J1, J2] = J3, [J2, J3] = J1, [J3, J1] = J2.
3
Under review as a conference paper at ICLR 2021
0.75-
0.50-
0.25-
0.00 -
-0.25-
-0.50 -
-0.75 -
Original
Transformed： -x Boost
0.75 -ι-------------------
⅞F	o.oo -	"β∙
f -小
1	-0.50- 夕
-0.5
0.0
0.5
-0.75 j----ɪ-
-0.5
0.0
Transformed： +x Boost
0.5

Figure 1: Activations of an SO(2,1)-EqUivanant neural network constructed using our framework.
The arrows depict the elements of the 3-dimensional representation space (arrows) and are embedded
on their associated points within the point cloud. This point cloud is from the MNIST-Live dataset as
generated with digits embedded in the x - t plane. The y axis is suppressed. The left plot depicts
the “original” activations (with the digit at rest). The right plots show what happens if we transform
the point cloud with a Lorentz boost in the ±x direction before feeding it through the network. As
dictated by Lorentz-equivariance, the activation vectors generated by the network transform in the
same way as the input point cloud.
for rotations about the x and y axes by a small angle θ satisfy RxRy - RyRx = Rz + O(θ2); more
generally the Lie bracket of equation 2 is satisfied to first order in θ. The Lia algebra so(3, 1) of
the Lorentz Group SO(3, 1) also satisfies equation 2 for the generators J1, J2, J3 of its subalgebra
isomorphic to so(3). It has 3 additional generators denoted K1, K2, K3, which satisfy:
[Ji, Kj] = ijkKk	[Ki, Kj] = -ijkJk	(3)
These Ki correspond to the Lorentz boosts in the same way that the Ji correspond to the rotations. In
general, if A is a t-dimensional Lie algebra with generators T1, ..., Tt such that
t
[Ti, Tj] = X AijkTk,	(4)
k=1
we call the tensor Aijk the structure constants of A. For connected matrix Lie groups such as
SO(m, n), the structure constants Aijk are easily obtained. For example, one may apply the matrix
logarithm to several elements of the group to obtain elements of the algebra, then find a complete
basis for the algebra and write the commutator of all basis elements in this basis.
2.4	Group Representations and the Tensor Product
Let G be a Lie group and ρ : G → Rn×n be a representation of G as defined in section 1.1. Then ρ
defines a group action on Rn : given a vector ~u ∈ Rn and a group element α ∈ G, we can define
α *ρ U := ρ(α)~
using the matrix product. We then say that ρ is irreducible if it leaves no nontrivial subspace invariant
一 for every subspace V ⊂ Rn with 0 < dim V < n, there exists a ∈ G,~ ∈ V such that a? U /V.
Given two G-representations ρ1 : G → Rn1 ×n1, ρ2 : G → Rn2 ×n2, we define their tensor product
as
ρι 0 P2 : G → Rn1n2×nιn2	(pi 0 ρ2)(α) = ρι(α) 0 P2(α),
in which 0 on the right hand side denotes the usual tensor product of matrices. It is easy to check
that p1 0 p2 is also a representation of G using the fact that for matrices A1, A2 ∈ Rn1 ×n1 and
B1,B2 ∈ Rn2×n2,
(A1 0 B1)(A2 0 B2) = (A1A2) 0 (B1B2).
For p1 , p2 as above we also define their direct sum as
(PI ㊉ p2)(α) = (pi(α) I p2(α)).
4
Under review as a conference paper at ICLR 2021
For two groups H, G We say that H is isomorphic to G and write H = G if there exists a bijection
f : H → G such that f (αβ) = f(α)f(β). For ρ1, ρ2 as above, their images ρi(G) form groups
and We say that ρι and ρ2 are isomorphic and write ρι = ρ2 if these groups are isomorphic, i.e.
ρι(G) = ρ2(G). Some familiar representations of SO(3) act on scalars ∈ R, vectors ∈ R3, and
tensors (e.g., the Cauchy stress tensor) 一 these representations are all nonisomorphic.
For many Lie groups such as SO(n, 1) and SO(n), a property called complete reducibility guarantees
that any representation is either irreducible, or isomorphic to a direct sum of irreducible representa-
tions. For such groups it suffices to identify the irreducible representations to understand all other
representations and construct equivariant models.
2.5	Clebsch-Gordan Coefficients and Tensor-Product Nonlinearities
Clebsch-Gordan Coefficients: Let G be a completely reducible Lie group, and let ρ1 , ρ2 , ρ3 be
irreducible G-representations on the vector spaces Rn1 , Rn2, Rn3. Consider the tensor product repre-
sentation ρι 0 ρ2. Since G is completely reducible, there exists a set S of irreducible representations
such that ρι 0 ρ2 =Lρ∈s ρ. Suppose that ρ3 ∈ S. Then there exists a matrix C ∈ Rn3 ×(n1n2)
which projects the space of the n3 -dimensional group representation ρ3 from the tensor product space
Rn1 0 Rn2. That is,
∀(α, ~u, ~v) ∈ G × Rn1 × Rn2, C(ρ1(α) 0 ρ2(α))(~u 0 ~v) = ρ3(α)C(~u0 ~v)
⇒ C(ρ1(α) 0 ρ2(α)) = ρ3(α)C.	(5)
The matrices C satisfying equation 5 for various ρ3 are called the Clebsch-Gordan coefficients. In
equation 5 there are n1n2n3 linear constraints on C, and therefore this is a well-posed homogeneous
linear program (LP) for C . The entries of C may be found numerically by sampling several distinct
α ∈ G and concatenating the linear constraints (equation 5) to form the final LP. The solutions for C
form a linear subspace ofRn3×(n1n2) given by the nullspace of some matrix we denote C[ρ1, ρ2, ρ3].
Tensor Product Nonlinearities: Tensor product nonlinearities, including norm nonlinearities, use
the Clebsch-Gordan coefficients defined above to compute equivariant quadratic functions of multiple
G-representations within the G-equivariant model. This was demonstrated for the case of SE(3) by
Thomas et al. (2018); Kondor et al. (2018) and for SO(3, 1) by Bogatskiy et al. (2020).
3	Methods
3.1	Learning Lie Group Representations
For a matrix M ∈ Rn×n we denote its Frobenius and L1 norms by
|M|2F =	X |Mij|2,	|M|1 = X |Mij|.
1≤i,j≤n	1≤i,j≤n
The approach of LearnRep is to first learn a Lie algebra representation and then obtain its correspond-
ing group representation through the matrix exponential. Fix a t-dimensional Lie algebra A with
structure constants Aijk as defined in equation 4. Fix a positive integer n as the dimension of the
representation of A. Then let the matrices T1, ..., Tt ∈ Rn×n be optimization variables, and define
the following loss function on the Ti :
L[T1 , ..., Tt]
max 11, max∣ X
k 'mi≤t |Ti|"
X------------V-------------}
N[Ti]-1
Σ
1≤i≤j≤t
[Ti, Tj] -	AijkTk
k 1
(6)
This is the magnitude of violation of the structure constants of A, multiplied by a norm penalty term
N [Ti]-1 (this penalty is plotted separately in figure 2). The purpose of the norm penalty is to avoid
convergence to a solution where Ti = 0n×n for any i, which will act trivially when restricted to the
nontrivial subgroup {etTi : t ∈ R}. We pose the optimization problem:
min	L[T1, ..., Tt].
T1,...,Tt∈Rn×n
The generators were initialized with entries from the standard normal distribution. Gradient descent
was performed in PyTorch with the Adam optimizer (Kingma & Ba, 2014) with initial learning rate
0.1. The learning rate decreased exponentially when loss plateaued. The results are shown in figure 2.
5
Under review as a conference paper at ICLR 2021
3.1.1	Verifying Irreducibility of Learned Representations
Suppose we have converged to T1, . . . Tt such that L[Ti] = 0. Then the T1, ..., Tt are a nonzero
n-dimensional representation of the Lie algebra A. The groups considered here are covered by the
exponential map applied to their Lie algebras, so for each α ∈ G there exist b1, . . . , bt ∈ R such that
t
ρ(α) = exp	biTi ,
i=1
where ρ is any n-dimensional representation of G and exp is the matrix exponential. This ρ : G 7→
Rn×n is then a representation of the Lie group. Throughout this section, ρ denotes this representation.
In general ρ may leave some nontrivial subspace invariant. In this case it is reducible and splits as the
direct sum of lower-dimensional irreducible representations ρi as explained in 2.4:
P = Pi ㊉...㊉ p'.
Recall that any representation may be obtained as such a direct sum of irreducible representations
with dimensions nι,...,n' satisfying n = P'=ι n. If n is set to the minimum dimension of a
nontrivial irreducible representation, the only permissible partitions of n have ` = 1 and ` = n
-as the latter representation is trivial, equation 6 diverges, so LearnRep can only converge to an
irreducible n dimensional representation.3 It is important to verify that the learned P is indeed
irreducible with ` = 1. To validate that P is irreducible, LearnRep computes its tensor product
structure and compares with the expected structure. Specifically, it computes the Clebsch-Gordan
coefficients for the direct-sum decomposition of the tensor product of the learned representation P
with several other known representations P1, ..., Pr. section 2.5 defines these coefficients and explains
how they are computed from the nullspace of the matrix C = C[P, P1, P2], in which P2 appears in
the decomposition of P 0 pi. Let ρ1,ρ2 denote two other known representations, and consider the
ClebSch-GOrdan coefficients C such that CP 0 pi = p3C. The dimension of the nullspace of C
indicates the number of unique nonzero matrices C of Clebsch-Gordan coefficients. The singular
values of C are denoted SV1 (C) ≤ ... ≤ SV'(C). The ratio
r(C) := SV2(C)/SVi(C)	(7)
diverges only if the nullspace is one dimensional which therefore corresponds to a unique solution for
C. The number of expected solutions is known (e.g., it may be computed using the same technique
from the formulae for the irreducible representations). Therefore if r(C) diverges for exactly the
choices of Pi , P2 where the theory indicates that unique nonzero Clebsch-Gordan coefficients exist,
then this is consistent with our having learned an irreducible representation of the group G.
Clearly the tensor product with the trivial representation Pi = 1 is P 0 1 = P. In this case, the
permissible C correspond to G-linear maps Rn → Rn2. By a result of Schur (1905) (Schur’s
Lemma), the only such (nonzero) maps are isomorphisms. Therefore a divergent value of r(C) when
Pi = 1 indicates that P == P2. This is shown in the top row of figure 3 and discussed further in
section 5.1.
3.1.2	Stopping condition
Similar to (Rao & Ruderman, 1999), LearnRep restarts gradient descent several times starting from
random initialization points. A restart is triggered if loss plateaus and the learning rate is smaller than
the loss by a factor of at most 10-4. The tensor product structure is computed upon convergence to
loss under 10-9, a restart is triggered if the divergences of r(C) do not agree with the theoretical
prediction, indicating a reducible representation.
3.2	SpacetimeNet Architecture
We obtain all Clebsch-Gordan coefficients through the procedure explained in section 2.5. We place
them in a tensor: Cg,qr,ls,mt . This notation corresponds to taking the tensor product of an element
of the lth group representation space indexed by s with an element of the mth group representation
th
space indexed by t, and projecting it onto the q group representation space indexed by r. The space
3This applies to our experiments learning SO(3) representations, with n = 3.
6
Under review as a conference paper at ICLR 2021
of possible Clebsch-Gordan coefficients can be multidimensional.4 We use an index g to carry the
dimension within the space of Clebsch-Gordan coefficients.
The trainable weights in SpacetimeNet are complex-valued filter weights denoted fqkg and channel-
mixing weights denoted Wqkcgd . Each layer builds a collection of equivariant convolutional filters
Fxkij qr from the geometry of the point cloud. Let q0 denote the index of the group representation in
which the points are embedded. Let Xxir denote the point coordinates, in which x indexes the batch
dimension, i indexes the points, and r indexes the q0 group representation space. Define the (globally)
translation-invariant quantity ∆Xxijr := Xxjr - Xxir. The equivariant filters at layer k are:
Fxij qr = δqq0 ∆Xxijr +	Cg,qr,q0s,q0tfqg∆Xxijs∆Xxijt.	(8)
s,t,g
The forward pass consists of tensor product nonlinearities between equivariant filters and activations.
The input and activations for the kth layer of the network are defined on a tensor Vxkimct , where x is
the batch dimension, i indexes the points, m is the group representation index, c is the channel index,
t indexes the group representation space. Our mixing weights are then defined for the kth layer as
Wqkcgd with layer update rule:
Vxki+qc1r =	Cg,qr,ls,m
g,l,s,m,t,d,j
Vk Wk
ls xj mdt	qcgd .
(9)
A proof that SpacetimeNet is Pn-equivariant is given in Appendix A.2.
4	Related Work
4.1	Learning Lie Group Representations
Several authors have investigated automated means of identifying Lie group representations. (Rao &
Ruderman, 1999) used gradient descent with several starting points to find the Lie group generators,
given many examples of data which had been transformed by the group. Applying the technique
requires knowledge of how the group acts on a representation space. Here we know the Lie algebra
structure but we do not know how to compute its representations. Tai et al. (2019) gave a closed-form
solution for the canonical coordinates for Lie groups. But their formula only applies for Abelian one-
parameter Lie groups, excluding SO(3), SO(2, 1), and SO(3, 1). Cohen & Welling (2014) devised
a probabilistic model to learn representations of compact, commutative Lie groups from pairs of
images related by group transformations. In the present work we demonstrate a new approach to
handle noncompact and noncommutative groups such as SO(3), SO(2, 1), and SO(3, 1). Computer
algebra software such as the LiE package developed by Van Leeuwen et al. (1992) automates some
calculations related to completely reducible Lie groups. Unfortunately this limits us when considering
novel Lie groups where the representation theory is less well-understood.
4.2	Equivariant Neural Networks
Beginning with the success of (approximately) translation-equivariant CNNs introduced by LeCun
et al. (1989) for image recognition, a line of work has extended equivariance to additional continuous
symmetry groups. Most relevant are the architectures for groups SE(2) (Worrall et al., 2017; Weiler
& Cesa, 2019), SE(3) (Weiler et al., 2018; Cohen et al., 2019; Kondor et al., 2018; Thomas et al.,
2018; Cohen et al., 2018; Kondor, 2018; Gao et al., 2020; Anderson et al., 2019; Fuchs et al., 2020;
Eismann et al., 2020), and the group of Galilean boosts (Zhu et al., 2019).
The work by Thomas et al. (2018); Kondor et al. (2018); Anderson et al. (2019); Bogatskiy et al.
(2020) used Clebsch-Gordan coefficients in their equivariant neural networks. Weiler et al. (2018),
generalized by Cohen et al. (2019) showed all equivariant linear maps are convolutions whose kernels
satisfy some linear constraints. In our work we obtain Clebsch-Gordan coefficients from similar
linear constraints (equation 5) and use them to show that the learned representations are irreducible.
We also use them in SpacetimeNet. Griffiths & Griffiths (2005) provide an introductory exposition of
Clebsch-Gordan coefficients and Gurarie (1992) provides a more general exposition.
4This is common if a group representation is itself obtained via tensor product.
7
Under review as a conference paper at ICLR 2021
One of the first constructions that addressed spatiotemporal symmetries was by Zhu et al. (2019).
They introduce motion-equivariant networks to handle linear optical flow of an observer moving
at a fixed speed. They use a canonical coordinate system in which optical flow manifests as a
translation, as described for general one dimensional Lie groups by Tai et al. (2019). This allows
them to use the translation equivariance of CNNs to produce Galilean boost-equivariance. However,
this gives up equivariance to translation in the original coordinate system. To maintain approximate
translation-equivariance, the authors apply a spatial transformer network (Jaderberg et al., 2015) to
predict a landmark position in each example. This is similar to the work of Esteves et al. (2018),
which achieved equivariance to 2D rotation and scale, and approximate equivariance to translation.
The first mention of Poincare-equivariant networks appears to be a work by Cheng et al. (2019)
on the link between covariance in ML and physics. Concurrently to our work, Bogatskiy et al.
(2020) constructed a Lorentz-equivariant model which operated on irreducible representations of the
Lorentz group, derived similarly to Appendix A.1. This work also made use of the Clebsch-Gordan
coefficients, and the model was applied to experimental particle physics rather than object-tracking.
Another work by Finzi et al. (2020) concurrent to our own proposed a framework for building models
equivariant to arbitrary Lie groups. This work also made use of the exponential and logarithm maps
between Lie algebra and group. It does not provide a technique for identifying the Lie algebra
representations. Our ideas complement this line of work by providing an algorithm (LearnRep) that
solves for the representations numerically.
5	Experiments
5.1	Convergence of LearnRep to Irreducible Representations
We apply LearnRep to SO(3), SO(2, 1), and SO(3, 1) to learn 3, 3, and 4 dimensional irreducible
representations respectively. The loss function converges arbitrarily close to 0 with the penalty term
bounded above by a constant. We exponentiate the resulting algebra representation matrices to obtain
group representations and calculate the tensor product structure as described in section 3.1.1 The
details of this calculation are in Appendix A.4 and shown in figure 3. The results indicate that
the learned representations are irreducible representations of the associated Lie algebras to within
numerical error of about 10-6. Schur’s Lemma in the special case of the tensor product with the
trivial representation indicates the isomorphism class of each learned group representation.
5.2	POINCARE-EQUIVARIANT OBJECT-TRACKING NETWORKS
We created MNIST-Live, a benchmark dataset of spacetime point clouds sampled from digits from the
MNIST dataset moving uniformly through space. Each sample consists of 64 points with uniformly
random times t ∈ [-1/2, 1/2], and spatial coordinates sampled from a 2D probability density
function proportional to the pixel intensity. Using instances of the 0 and 9 classes, we train on
examples with zero velocity and evaluate on examples with random velocity and orientation. This
dataset is analogous to data from an event camera (see (Orchard et al., 2015)) or LIDAR system.
We train 3 layer SO(2, 1) and SO(3, 1)-equivariant SpacetimeNet models with 3 channels and batch
size 16 on 4096 MNIST-Live examples and evaluate on a dev set of 124 examples. We obtain dev
accuracy of 80 ± 5% as shown in figure 4 of the Appendix.
5.3	Conclusion
We envision many applications of Poincare-equivariant deep neural networks beyond the physics of
particles and plasmas. SpacetimeNet can identify and track simple objects as they move through
3D space. This suggests that Lorentz-equivariance is a useful prior for object-tracking tasks. With
a treatment of bandlimiting and resampling as in Worrall et al. (2017); Weiler et al. (2018), our
work could be extended to build Poincare-equivariant networks for volumetric data. More broadly,
understanding the representations of noncompact and noncommutative Lie groups may enable the
construction of networks equivariant to new sets of symmetries such as the Galilean group. Since
the representation theory of these groups is not entirely understood, automated techniques such as
LearnRep could play a beneficial role.
8
Under review as a conference paper at ICLR 2021
References
Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural net-
works. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 32, pp. 14510-14519. Curran Associates, Inc.,
2019.
Alexander Bogatskiy, Brandon Anderson, Jan T Offermann, Marwah Roussi, David W Miller, and
Risi Kondor. Lorentz group equivariant neural network for particle physics. arXiv preprint
arXiv:2006.04780, 2020.
Daniel Bump. Lie groups. Springer, 2004.
Elie Cartan. La theorie des groupes finis et continus et l,analysis situs. Memorial Sc. Math., 1930.
Miranda CN Cheng, Vassilis Anagiannis, Maurice Weiler, Pim de Haan, Taco S Cohen, and
Max Welling. Covariance in physics and convolutional neural networks. arXiv preprint
arXiv:1906.02481, 2019.
Taco Cohen and Max Welling. Learning the irreducible representations of commutative lie groups.
In International Conference on Machine Learning, pp. 1755-1763, 2014.
Taco Cohen, Mario Geiger, Jonas K"ohler, Pim de Haan, K. T. Sch"utt, and Benjamin K. Miller. Lie
learn, February 2020. URL https://github.com/AMLab-Amsterdam/lie_learn/
releases/tag/v1.0_b.
Taco S. Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical CNNs. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=Hkbd5xZRb.
Taco S Cohen, Mario Geiger, and Maurice Weiler. A general theory of equivariant cnns on ho-
mogeneous spaces. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 9142-9153. Curran
Associates, Inc., 2019.
M De Montigny, J Niederle, and AG Nikitin. Galilei invariant theories: I. constructions of inde-
composable finite-dimensional representations of the homogeneous galilei group: directly and via
contractions. Journal of Physics A: Mathematical and General, 39(29):9365, 2006.
Stephan Eismann, Raphael JL Townshend, Nathaniel Thomas, Milind Jagota, Bowen Jing, and
Ron Dror. Hierarchical, rotation-equivariant neural networks to predict the structure of protein
complexes. arXiv preprint arXiv:2006.09275, 2020.
Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar transformer
networks. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=HktRlUlAZ.
Richard P Feynman, Robert B Leighton, and Matthew Sands. The Feynman lectures on physics, Vol.
I: The new millennium edition: mainly mechanics, radiation, and heat, volume 1. Basic books,
2011.
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional
neural networks for equivariance to lie groups on arbitrary continuous data. arXiv preprint
arXiv:2002.12880, 2020.
Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d
roto-translation equivariant attention networks. arXiv preprint arXiv:2006.10503, 2020.
Liyao Gao, Yifan Du, Hongshan Li, and Guang Lin. Roteqnet: Rotation-equivariant network for fluid
systems with symmetric high-order tensors. arXiv preprint arXiv:2005.04286, 2020.
D.J. Griffiths and P.D.J. Griffiths. Introduction to Quantum Mechanics. Pearson international edition.
Pearson Prentice Hall, 2005. ISBN 9780131118928. URL https://books.google.com/
books?id=z4fwAAAAMAAJ.
9
Under review as a conference paper at ICLR 2021
D Gurarie. Symmetries and laplacians. introduction to harmonic analysis, group representations and
applications. North-Holland mathematics studies,174:1-448,1992.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. Spatial
transformer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp.
2017-2025. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/
5854-spatial-transformer-networks.pdf.
J Robert Johansson, Paul D Nation, and Franco Nori. Qutip 2: A python framework for the dynamics
of open quantum systems. Computer Physics Communications, 184(4):1234-1240, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Risi Kondor. N-body networks: a covariant hierarchical neural network architecture for learning
atomic potentials. arXiv preprint arXiv:1803.01588, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-gordan nets: a fully fourier space spherical
convolutional neural network. In Advances in Neural Information Processing Systems, pp. 10117-
10126, 2018.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541-551, 1989.
Jean-Marc Levy-Leblond. Galilei group and galilean invariance. In Group theory and its applications,
pp. 221-299. Elsevier, 1971.
J Niederle and AG Nikitin. Construction and classification of indecomposable finite-dimensional
representations of the homogeneous galilei group. Czechoslovak Journal of Physics, 56(10-11):
1243-1250, 2006.
Garrick Orchard, Ajinkya Jayawant, Gregory K Cohen, and Nitish Thakor. Converting static image
datasets to spiking neuromorphic datasets using saccades. Frontiers in neuroscience, 9:437, 2015.
Didier Pinchon and Philip E Hoggan. Rotation matrices for real spherical harmonics: general rotations
of atomic orbitals in space-fixed axes. Journal of Physics A: Mathematical and Theoretical, 40(7):
1597, 2007.
Rajesh PN Rao and Daniel L Ruderman. Learning lie groups for invariant visual perception. In
Advances in neural information processing systems, pp. 810-816, 1999.
Issai Schur. Neue begrUndung der theorie der gruppenCharaktere. 1905.
Kai Sheng Tai, Peter Bailis, and Gregory Valiant. Equivariant transformer networks. arXiv preprint
arXiv:1901.11399, 2019.
Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley.
Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds.
arXiv preprint arXiv:1802.08219, 2018.
Marc AA Van Leeuwen, Arjeh Marcel Cohen, and Bert Lisser. Lie: A package for lie group
computations. 1992.
Maurice Weiler and Gabriele Cesa. General e (2)-equivariant steerable cnns. In Advances in Neural
Information Processing Systems, pp. 14334-14345, 2019.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco Cohen. 3d steerable cnns:
Learning rotationally equivariant features in volumetric data. In Advances in Neural Information
Processing Systems, pp. 10381-10392, 2018.
Steven Weinberg. The quantum theory of fields. Vol. 1: Foundations. Cambridge University Press,
1995.
10
Under review as a conference paper at ICLR 2021
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Alex Zihao Zhu, Ziyun Wang, and Kostas Daniilidis. Motion equivariant networks for event cameras
with the temporal normalization transform. arXiv preprint arXiv:1902.06820, 2019.
A Appendix
A. 1 Analytic Derivation of Lorentz Group Representations
To compare our learned group representations with those obtained through prior methods, we require
analytical formulae for the Lie algebra representations for the algebras so(3), so(3, 1), and so(2, 1).
The case of so(3) has a well-known solution (see Griffiths & Griffiths (2005)). If complex matrices
are permissible the library QuTiP Johansson et al. (2013) has a function “jmat” that readily gives the
representation matrices. A formula to obtain real-valued representation matrices is given in Pinchon
& Hoggan (2007) and a software implementation is available at Cohen et al. (2020). The three-
dimensional Lie algebra so(2, 1) = span{Kx, Ky, Jz} has structure constants given by equation 3.
In fact, these three generators Kx , Ky , Jz may be rescaled so that they satisfy equation 2 instead.
This is due to the isomorphism so(3) = so(2,1). Specifically, leting {Lχ, Ly, Lz} denote a Lie
algebra representation of so(3), defining
Kx = -iLx	Ky = -iLy	Jz := Lz,
it may be easily checked that Kx, Ky, Jz satisfy the applicable commutation relations from equation 3.
This reflects the physical intuition that time behaves like an imaginary dimension of space.
The final Lie algebra for which we require explicit representation matrix formulas is so(3, 1).
Following Weinberg (1995), we define new generators Ai , Bi as
Ai := $(Ji + iKi)	Bi := £(Ji - iKi),	(IO)
we see that the so(3, 1) commutators equation 2, equation 3 become
[Ai,Aj] = iijkAk,	[Bi, Bj] = iijkBk,	[Ai, Bj] = 0.	(11)
Therefore so(3,1) = so(3)㊉ so(3), and the irreducible algebra representations of so(3,1) may be
obtained as the direct sum of two irreducible algebra representations of so(3).
A.2 PROOF THAT SPACETIMENET IS POINCARE-EQUIVARIANT
Consider an arbitrary Poincare group transformation α ∈ Pn, and write α = βt in which β ∈
SO(n, 1) and t is a translation. Suppose we apply this α to the inputs of equation 8 through the
representations indexed by q: ρq(α)st, in which s, t index the representation matrices. Then since the
translation t leaves ∆X invariant, the resulting filters will be
Fk xijqr	δqq0	ρq0 (β)rr0 ∆Xxijr0 +	Cg,qr,q0s,q0tfqg	ρq0 (β)ss0 ∆Xxijs0 ρq0 (β)tt0 ∆Xxijt0 δqq0	ρq0 (β)rr0 ∆Xxijr0 +	Cg,qr,q0s,q0tρq0 (β)ss0 ρq0 (β)tt0	fqg ∆Xxijs0 ∆Xxijt0 r0	g,s0,t0	s,t δqq0 X ρq0 (β) rr0 ∆Xxijr0 + X (ρq(β)rr0Cg,qr0,q0s,q0t) fqkg∆Xxijs∆Xxijt r0	s,t,g,r0 ρq0 (β)rr0	δqq0 ∆Xxijr0 +	Cg,qr0 ,q0s,q0tfqg∆Xxijs∆Xxijt r0	s,t,g,r0 ρq0(β)rr0Fxkijqr0, r0
11
Under review as a conference paper at ICLR 2021
where we have used equation 5. The network will be equivariant if each layer update is equivariant.
Recall the layer update rule of equation 9:
Vxki+qc1r =	Cg,qr,ls,m
g,l,s,m,t,d,j
Vk Wk
ls xj mdt	qcgd .
Suppose for the same transformation α = βt above, that V k and ∆X are transformed by α. Then
because the activations associated with each point are representations of SO(n, 1), they are invariant
to the global translation t of the point cloud and we have
Vxki+qc1r
g,l,s,m,t,d,j
Cg,qr,ls,mt	ρm(β)ss0Fxkijls0	ρm (β)tt0 Vxkjmdt0 Wqkcgd
s0
t0
X X	(Cg,qr,ls,mtρm(β)ss0 ρm(β)tt0) Fxijls0 Vxjmdt0 Wqcgd
s0,t0 g,l,s,m,t,d,j
X	(ρm(β)rr0Cg,qr0,ls,mt) FxijlsVxjmdtWqcgd
g,l,s,m,t,d,j,r0
X ρm(β)rr0Vxki+qc1r0 ,
r0
where again we applied equation 5.
A.3 Equivariant Convolutions
Consider data on a point cloud consisting of a finite set of spacetime points {~xi } ⊂ R4, a rep-
resentation ρ0 : SO(3, 1) → R4×4 of the Lorentz group defining its action upon spacetime, and
feature maps {~ui} ⊂ Rm, {~vi} ⊂ Rn associated with representations ρu : SO(3, 1) → Rm×m and
ρv : SO(3, 1) → Rn×n. A convolution of this feature map can be written as
~u0i =	κ(~xj - ~xi)~uj
j
in which κ(~x) : R4 → Rn×m, a matrix-valued function of spacetime, is the filter kernel.
P3-equivariance dictates that for any α ∈ SO(3, 1),
ρv (α)	κ(~xj - ~xi)~uj =	κ(ρ0(α)(~xj - ~xi))ρu(α)~uj
jj
⇒ κ(∆~x) = ρv(α-1)κ(ρ0(α)∆~x)ρu(α)	(12)
Therefore a single kernel matrix in Rn×m may be learned for each coset of spacetime under the
action of SO(3, 1). The cosets are indexed by the invariant
t2 - x2 - y2 - z2.
The kernel may then be obtained at an arbitrary point ~x ∈ R4 from equation 12 by computing an α
that relates it to the coset representative ~x0: ~x = ρ0(α)~x0. A natural choice of coset representatives
for SO(3, 1) acting upon R4 is the set of points {(t, 0, 0, 0) : t ∈ R+} ∪ {(0, x, 0, 0) : x ∈
R+} ∪{(t,ct,0,0) : t ∈ R+}.
A.4 TENSOR PRODUCT S TRUCTURE OF LEARNED SO(3), SO(2, 1), SO(3, 1) GROUP
Representations
We quantify the uniqueness of each set of Clebsch-Gordan coefficients in terms of the diagnostic ratio
r(C) defined in equation 7. Recall that the value of r becomes large only if there is a nondegenerate
nullspace corresponding to a unique set of Clebsch-Gordan coefficients. For SO(3) and SO(2, 1),
the irreducible group representations are labeled by an integer which is sometimes called the spin.
We label learned group representations with a primed (i0) integer. For the case of SO(3, 1) the
irreducible group representations are obtained from two irreducible group representations of so(3)
as explained in section A.1 and we label these representations with both spins i.e. (s1, s2). We
12
Under review as a conference paper at ICLR 2021
Corwergence to Representation of S0(3)
O 500 IOM 1500	20∞	2500
iter(rtion
iteneticn
itemtiβn
Figure 2: Convergence to arbitrary precision group representations of three Lie groups:
SO(3), SO(2, 1), and SO(3, 1). The multiplicative norm penalty is plotted in each lower subplot,
and demonstrates that this penalty is important early on in preventing the learning of a trivial rep-
resentation, but for later iterations stays at its clipped value of 1. Loss is plotted on each upper
subplot.
again label the learned group representations of SO(3, 1) with primed spins, i.e. (s01, s02). The tensor
product structures of the representations is shown in figure 3.
We have produced a software library titled Lie Algebraic Networks (LAN) built on PyTorch, which
derives all Clebsch-Gordan coefficients and computes the forward pass of Lie group equivariant
neural networks. LAN also deals with Lie algebra representations, allowing for operations such as
taking the tensor product of multiple group representations. figure 5 demonstrates the LAN library.
Starting from several representations for a Lie algebra, LAN can automatically construct a neural
network equivariant to the associated Lie group with the desired number of layers and channels. We
present our experimental results training SO(2, 1) and SO(3, 1)-equivariant object-tracking networks
in section 5.2.
A.5 Supplementary figures
13
Under review as a conference paper at ICLR 2021
r®o	r®o	(i∕2,f ι∕2,) ®(o, o)
r e r
sa≈SS
8 4
O O
1 1
8 4
O O
1 1
sa≈SS
8 4
O O
1 1
sSS
re r
8 4
O O
1 1
sSS
(1/211/2。® (1/2, 1/2。
re2
8 4
O O
1 1
sSS
8 4
O O
1 1
sSS
SO(3)	SO(2,1)	SO(3,1)
Figure 3: Tensor product structure of the learned group representations P with several known
(analytically-derived) group representations ρι for the groups Sθ(3), SO(2,1), and SO(3,1). Each
column is for the group indicated at the bottom, each row is for a different choice of ρι for that group,
and the horizontal axis indicates the ρ(i) onto which We project the tensor product P 0 ρι =㊉i∈ιρ(i).
The diagnostic r (defined in section 2.5) is plotted on the y-axis with a log scale for each subfigure.
The labelling of group representations is explained in section 5.1, recall that the primed integers
indicate learned representations. The first row demonstrates by Schur,s Lemma that to within
numerical error of about 〜10-6 the learned SO(3) group representation denoted 10 is isomorphic
to the spin-1 irreducible group representation obtained from known formulae, i.e.4⑶=1so(3).
The first row also indicates that、。⑵。=1so(2,i), and (1/20,1/20)so(3,i)= (1/2, 1/2)so(3,i).
The remaining rows indicate that the tensor product structure of the learned group representations
matches that of the known irreducible group representations.
8 4
O O
L L


14
Under review as a conference paper at ICLR 2021
50000 -ι-----------------------------------------
----test loss
0-∣l	-----i------i----------i-------i-----
0.0	0.1	0.2	0.3	0.4
time (hr)
2500-
test loss
0-
0.2	0.4	0.6
time (hr)
200000 -	—i— train loss (±σ)
0.0	0.1	0.2	0.3	0.4
time (hr)
50000 H-------------------------
—|— train loss (±σ)
oj,∙…….................l....
0.0	0.2	0.4	0.6
time (hr)
Figure 4: (Left) SO(2, 1)-equivariant neural network learning to recognize digits from the MNIST-
Live dataset in 2 spatial dimensions. Error bars for train accuracy and loss are computed as the mean
and standard deviation across a sliding window of 15 batches. (Right) SO(3, 1)-equivariant neural
network training to recognize digits from the MNIST-Live dataset in 3 spatial dimensions. Error bars
for train accuracy and loss are computed as the mean and standard deviation across a sliding window
of 15 batches.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
from lan import LieAlgebraRepresentation, \
LieAlgebraRepresentationDirectSum, \
LieAlgebraTensorProductRepresentation, \
LieGroupEquivariantNeuralNetwork
learned_generators = [...]
known_generators = [...]
learned_irrep = LieAlgebraRepresentation(learned_generators)
scalar_irrep = LieAlgebraRepresentation(
numpy.zeros((
learned_irrep.algebra.dim, 1, 1
))
)
known_irrep = LieAlgebraRepresentation(known_generators)
representations = LieAlgebraRepresentationDirectSum([
scalar_irrep,
known_irrep
learned_irrep,
LieAlgebraTensorProductRepresentation(
[learned_irrep, learned_irrep])
])
model = LieGroupEquivariantNeuralNetwork(
representations, num_layers=10, num_channels=32)
Figure 5: Our Lie Algebraic Networks (lan) module handles Lie algebra and Lie group representations,
derives Clebsch-Gordan coefficients for the equivariant layer update, and computes the forward pass.
This makes it simple to build an equivariant point cloud network once the representations are obtained.
15