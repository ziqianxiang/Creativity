Under review as a conference paper at ICLR 2021
Adaptive norms for deep learning with
regularized Newton methods
Anonymous authors
Paper under double-blind review
Ab stract
We investigate the use of regularized Newton methods with adaptive norms for
optimizing neural networks. This approach can be seen as a second-order coun-
terpart of adaptive gradient methods, which we here show to be interpretable as
first-order trust region methods with ellipsoidal constraints. In particular, we prove
that the preconditioning matrix used in RMSProp and Adam satisfies the necessary
conditions for provable convergence of second-order trust region methods with
standard worst-case complexities on general non-convex objectives. Furthermore,
we run experiments across different neural architectures and datasets to find that
the ellipsoidal constraints constantly outperform their spherical counterpart both
in terms of number of backpropagations and asymptotic loss value. Finally, we
find comparable performance to state-of-the-art first-order methods in terms of
backpropagations, but further advances in hardware are needed to render Newton
methods competitive in terms of computational time.
1	Introduction
We consider finite-sum optimization problems of the form
n
min L(w) := X `(f (w, xi, yi)) ,	(1)
w∈Rd
i=1
which typically arise in neural network training, e.g. for empirical risk minimization over a set
of data points (xi, yi) ∈ Rin × Rout, i = 1, . . . , n. Here, ` : Rout × Rout → R+ is a convex
loss function and f : Rin × Rd → Rout represents the neural network mapping parameterized
by the concatenation of the weight layers w ∈ Rd , which is non-convex due to its multiplicative
nature and potentially non-linear activation functions. We assume that L is lower bounded and twice
differentiable, i.e. L ∈ C2(Rd, R) and consider finding a first- and second-order stationary point W
for which ∣∣VL(wv)k ≤ j and λm^ (V2L(w)) ≥ -田.
In the era of deep neural networks, stochastic gradient descent (SGD) is one of the most widely used
training algorithms (Bottou, 2010). What makes SGD so attractive is its simplicity and per-iteration
cost that is independent of the size of the training set (n) and scale linearly in the dimensionality (d).
However, gradient descent is known to be inadequate to optimize functions that are ill-conditioned
(Nesterov, 2013; Shalev-Shwartz et al., 2017) and thus adaptive gradient methods that employ
dynamic, coordinate-wise learning rates based on past gradients—including Adagrad (Duchi et al.,
2011), RMSprop (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2014)—have become a
popular alternative, often providing significant speed-ups over SGD.
From a theoretical perspective, Newton methods provide stronger convergence guarantees by appro-
priately transforming the gradient in ill-conditioned regions according to second-order derivatives. It
is precisely this Hessian information that allows regularized Newton methods to enjoy superlinear
local convergence as well as to provably escape saddle points (Conn et al., 2000). While second-order
algorithms have a long-standing history even in the realm of neural network training (Hagan &
Menhaj, 1994; Becker et al., 1988), they were mostly considered as too computationally and memory
expensive for practical applications. Yet, the seminal work of Martens (2010) renewed interest for
their use in deep learning by proposing efficient Hessian-free methods that only access second-order
information via matrix-vector products which can be computed at the cost of an additional backprop-
agation (Pearlmutter, 1994; Schraudolph, 2002). Among the class of regularized Newton methods,
1
Under review as a conference paper at ICLR 2021
trust region (Conn et al., 2000) and cubic regularization algorithms (Cartis et al., 2011) are the most
principled approaches in the sense that they yield the strongest convergence guarantees. Recently,
stochastic extensions have emerged (Xu et al., 2017b; Yao et al., 2018; Kohler & Lucchi, 2017;
Gratton et al., 2017), which suggest their applicability for deep learning.
We here propose a simple modification to make TR methods even more suitable for neural network
training. Particularly, we build upon the following alternative view on adaptive gradient methods:
While gradient descent can be interpreted as a spherically constrained first-order TR method, precon-
ditioned gradient methods—such as Adagrad—can be seen as first-order TR methods with ellipsoidal
trust region constraint.
This observation is particularly interesting since spherical constraints are blind to the underlying
geometry of the problem, but ellipsoids can adapt to local landscape characteristics, thereby allowing
for more suitable steps in regions that are ill-conditioned. We will leverage this analogy and investigate
the use of the Adagrad and RMSProp preconditioning matrices as ellipsoidal trust region shapes
within a stochastic second-order TR algorithm (Xu et al., 2017a; Yao et al., 2018). Since no ellipsoid
fits all objective functions, our main contribution lies in the identification of adequate matrix-induced
constraints that lead to provable convergence and significant practical speed-ups for the specific case
of deep learning. On the whole, our contribution is threefold:
•	We provide a new perspective on adaptive gradient methods that contributes to a better
understanding of their inner-workings.
•	We investigate the first application of ellipsoidal TR methods for deep learning. We show
that the RMSProp matrix can directly be applied as constraint inducing norm in second-order
TR algorithms while preserving all convergence guarantees (Theorem 1).
•	Finally, we provide an experimental benchmark across different real-world datasets and
architectures (Section 5). We compare second-order methods also to adaptive gradient
methods and show results in terms of backpropagations, epochs, and wall-clock time; a
comparison we were not able to find in the literature.
Our main empirical results demonstrate that ellipsoidal constraints prove to be a very effective
modification of the trust region method in the sense that they constantly outperform the spherical TR
method, both in terms of number of backprogations and asymptotic loss value on a variety of tasks.
2	Related work
First-order methods The prototypical method for optimizing Eq. (1) is SGD (Robbins & Monro,
1951). The practical success of SGD in non-convex optimization is unquestioned and theoretical
explanations of this phenomenon are starting to appear. Recent findings suggest the ability of this
method to escape saddle points and reach local minima in polynomial time, but they either need to
artificially add noise to the iterates (Ge et al., 2015; Lee et al., 2016) or make an assumption on the
inherent noise of SGD (Daneshmand et al., 2018). For neural networks, a recent line of research
proclaims the effectiveness of SGD, but the results come at the cost of strong assumptions such as
heavy over-parametrization and Gaussian inputs (Du et al., 2017; Brutzkus & Globerson, 2017; Li &
Yuan, 2017; Du & Lee, 2018; Allen-Zhu et al., 2018). Adaptive gradient methods (Duchi et al., 2011;
Tieleman & Hinton, 2012; Kingma & Ba, 2014) build on the intuition that larger (smaller) learning
rates for smaller (larger) gradient components balance their respective influences and thereby the
methods behave as if optimizing a more isotropic surface. Such approaches have first been suggested
for neural nets by LeCun et al. (2012) and convergence guarantees are starting to appear (Ward et al.,
2018; Li & Orabona, 2018). However, these are not superior to the O(g-2) worst-case complexity of
standard gradient descent (Cartis et al., 2012b).
Regularized Newton methods The most principled class of regularized Newton methods are trust
region (TR) and adaptive cubic regularization algorithms (ARC) (Conn et al., 2000; Cartis et al.,
2011), which repeatedly optimize a local Taylor model of the objective while making sure that the step
does not travel too far such that the model stays accurate. While the former finds first-order stationary
points within O(g-2), ARC only takes at most O(g-3/2). However, simple modifications to the TR
framework allow these methods to obtain the same accelerated rate (Curtis et al., 2017). Both methods
2
Under review as a conference paper at ICLR 2021
take at most O(-H3) iterations to find an H approximate second-order stationary point (Cartis et al.,
2012a). These rates are optimal for second-order Lipschitz continuous functions (Carmon et al.,
2017; Cartis et al., 2012a) and they can be retained even when only sub-sampled gradient and Hessian
information is used (Kohler & Lucchi, 2017; Yao et al., 2018; Xu et al., 2017b; Blanchet et al., 2016;
Liu et al., 2018). Furthermore, the involved Hessian information can be computed solely based
on Hessian-vector products, which are implementable very efficiently (Pearlmutter, 1994). This
makes these methods particularly attractive for deep learning, but the empirical evidence of their
applicability is rather limited. We are only aware of the works of Liu et al. (2018) and Xu et al.
(2017a), which report promising first results but are by no means fully encompassing.
Gauss-Newton methods An interesting line of research proposes to replace the Hessian by (approx-
imations of) the generalized-Gauss-Newton matrix (GGN) within a Levenberg-Marquardt framework1
(LeCun et al., 2012; Martens, 2010; Martens & Grosse, 2015). As the GGN matrix is always positive
semidefinite, these methods cannot leverage negative curvature to escape saddles and hence, there
exist no second-order convergence guarantees. Furthermore, there are cases in neural networks where
the Hessian is better conditioned than the GGN matrix (Mizutani & Dreyfus, 2008). Nevertheless,
the above works report promising preliminary results, most notably Grosse & Martens (2016) find
that K-FAC can be faster than SGD on a small convnet. On the other hand, recent findings report
performance at best comparable to SGD on the much larger ResNet architecture (Ma et al., 2019).
Moreover, Xu et al. (2017a) reports many cases where TR and GGN algorithms perform similarly.
This line of work can be seen as complementary to our approach since it is straightforward to replace
the Hessian in the TR framework with the GGN matrix. Furthermore, the preconditioners used in
Martens (2010) and Chapelle & Erhan (2011), namely diagonal estimates of the empirical Fisher and
Fisher matrix, respectively, can directly be used as matrix norms in our ellipsoidal TR framework.
3	An alternative view on adaptive gradient methods
Adaptively preconditioned gradient methods update iterates as wt+ι = Wt - ηt A-1/2gt, where gt
is a stochastic estimate of ▽£ (Wt) and At is a positive definite symmetric pre-conditioning matrix.
In Adagrad, Aada,t is the un-centered second moment matrix of the past gradients computed as
Aada,t := GtGt| +I,	(2)
where > 0, I is the d × d identity matrix and Gt = [g1, g2, . . . , gt]. Building up on the intuition
that past gradients might become obsolete in quickly changing non-convex landscapes, RMSprop
(and Adam) introduce an exponential weight decay leading to the preconditioning matrix
Arms,t ：= ((1 - β)Gt diag(βt,…，β0)G∣) + eI,	(3)
where β ∈ (0, 1). In order to save computational efforts, the diagonal versions diag(Aada) and
diag(Arms) are more commonly applied in practice, which in turn gives rise to coordinate-wise
adaptive stepsizes that are enlarged (reduced) in coordinates that have seen past gradient components
with a smaller (larger) magnitude.
3.1	Adaptive preconditioning as ellipsoidal Trust Region
Starting from the fact that adaptive methods employ coordinate-wise stepsizes, one can take a
principled view on these methods. Namely, their update steps arise from minimizing a first-order
Taylor model of the function L within an ellipsoidal search space around the current iterate wt, where
the diameter of the ellipsoid along a particular coordinate is implicitly given by ηt and kgt kA-1 .
Correspondingly, vanilla (S)GD optimizes the same first-order model within a spherical constraint.
Figure 1 (top) illustrates this effect by showing not only the iterates of GD and Adagrad but also the
implicit trust regions within which the local models were optimized at each step.2
It is well known that GD struggles to progress towards the minimizer of quadratics along low-
curvature directions (see e.g., Goh (2017)). While this effect is negligible for well-conditioned
objectives (Fig. 1, left), it leads to a drastic slow-down when the problem is ill-conditioned (Fig. 1,
1This algorithm is a simplified TR method, initially tailored for non-linear least squares problems (Nocedal
& Wright, 2006)
2We only plot every other trust region. Since the models are linear, the minimizer is always on the boundary.
3
Under review as a conference paper at ICLR 2021
-β -6	-4	-2 O 2	4	6	8	-β -6	-4	-2 O 2	4	6	8	-β -6	-4	-2 O 2	4	6	8
K = 2	K = 20	K = 20
Figure 1: Top: Iterates and implicit trust regions of GD and Adagrad on quadratic objectives With different
condition number κ. Bottom: Average log suboptimality over iterations as well as 90% confidence intervals of
30 runs with random initialization
center). Particularly, once the method has reached the bottom of the valley, it struggles to make
progress along the horizontal axis. Here is precisely where the advantage of adaptive stepsize methods
comes into play. As illustrated by the dashed lines, Adagrad,s search space is damped along the
direction of high curvature (vertical axis) and elongated along the low curvature direction (horizontal
axis). This allows the method to move further horizontally early on to enter the valley with a smaller
distance to the optimizer W along the low curvature direction which accelerates convergence.
Let us now formally establish the result that allows us to re-interpret adaptive gradient methods from
the trust region perspective introduced above.
Lemma 1 (Preconditioned gradient methods as TR). A preconditioned gradient SteP
Wt+1 - Wt = St ：= -ηtA-1gt	(4)
with stepsize ηt > 0, symmetric positive definite preconditioner At ∈ Rd×d and gt = 0
minimizes a first-order model around Wt ∈ Rd in an ellipsoid g^ven by At in the sense that
St= arg	mRd	[m1(S)	=	L(Wt)	+ slgt]	,	SI.	kSkAt	≤	ηtkgtkA-1.
(5)
-1/2
Corollary 1 (Rmsprop). The step Srms,t ：= -ηt Arms,tgt minimizes a first-order Taylor model
around Wt in an ellipsoid given by Arm2s,t (Eq. 3) in the sense that
Srms,t ：= arg min ImI(S) = L(Wt) + s|gt] , s.t. ∣∣s∣∣A1/2 ≤ ηtkgtkAT/2 ∙	(6)
s∈Rd	rms,t	rms,t	' /
Equivalent results can be established for Adam using gαdαm,t ：= (1 - β) Pk=o βt-kgt as well as for
Adagrad by replacing the matrix Aada into the constraint in Eq. (6). Of course, the update procedure
in Eq. (5) is merely a reinterpretation of the original preconditioned update, and thus the employed
trust region radii are defined implicitly by the current gradient and stepsize.
3.2 Diagonal versus full preconditioning
A closer look at Figure 1 reveals that the first two problems are perfectly axis-aligned, which makes
these objectives particularly attractive for diagonal preconditioning. For comparison, we report
another quadratic instance, where the Hessian is no longer zero on the off-diagonals (Fig. 1, right).
As can be seen, this introduces a tilt in the level sets and reduces the superiority of diagonal Adagrad
4
Under review as a conference paper at ICLR 2021
over plain GD. However, using the full preconditioner Aada re-establishes the original speed up. Yet,
non-diagonal preconditioning comes at the cost of taking the inverse square root of a large matrix,
which is why this approach has been relatively unexplored (see Agarwal et al. (2018) for an exception).
Interestingly, early results by Becker et al. (1988) on the curvature of neural nets report a strong
diagonal dominance of the Hessian matrix V2L(w). However, the reported numbers are only for tiny
networks of at most 256 parameters. We here take a first step towards generalizing these findings
to modern day networks. Furthermore, we contrast the diagonal dominance of real Hessians to the
expected behavior of random Wigner matrices.3 For further evidence, we also compare Hessians of
Ordinary Least Squares (OLS) problems with random inputs. For this purpose, let δA define the ratio
of diagonal to overall mass of a matrix A, i.e. δA
PPIAAi| ∙∣ as in (Becker et al.,1988).
i j | i,j |
Proposition 1 (Diagonal share of Wigner matrix). For a random Gaussian4 Wigner matrix W (see
Eq. (42)) the diagonal mass ofthe expected absolute matrix amounts to: 6e[∣w∣]
1
i+(d-i)σι.
Thus, if we suppose the Hessian at any given point w were a random Wigner matrix we would expect
the share of diagonal mass to fall with O(1/d) as the network grows in size. In the following, we
derive a similar result for the large n limit in the case of OLS Hessians.
Proposition 2 (Diagonal share of OLS Hessian). Let X ∈ Rd×n and assume each xi,j is generated
i.i.d. with zero-mean and finite second moment σ2 > 0. Then the share of diagonal mass ofthe
expected matrix E ||Hols |] amounts to: 6旧[旧os∣] n→∞「十(ɪZnI)√2∙
Empirical simulations suggest that this result holds already in small n settings (see Figure D.2) and
finite n results can be likely derived under assumptions such as Gaussian data. As can be seen in
Figure 2 below, even for a practical batch size of n = 32 the diagonal mass δH of neural networks
stays above both benchmarks for random inputs as well as with real-world data.
Figure 2: Diagonal mass of neural network Hessian 6h relative to δffi[∣w∣] and 匹㈣口国卜口 of corresponding
dimensionality for random inputs as well as at random initialization, middle and after reaching 90% training
accuracy with RMSProp on CIFAR-10. Mean and 95% confidence interval over 10 independent runs.
These results are in line with Becker et al. (1988) and suggest that full matrix preconditioning might
indeed not be worth the additional computational cost. We thus use diagonal preconditioning in
all of our experiments in Section 5 but note that further theoretical and empirical elaborations of
these findings are needed to assess the Hessian structure and hence effectiveness of full-matrix
pre-conditioning, which is out of the scope of the work at hand.
4	Second-order Trust Region Methods
Cubic regularization (Nesterov & Polyak, 2006; Cartis et al., 2011) and trust region methods belong
to the family of globalized Newton methods. Both frameworks compute parameter updates by
3 Of course, Hessians do not have i.i.d. entries but the symmetry of Wigner matrices suggests that this baseline
is not completely off.
4The argument naturally extends to any distribution with positive expected absolute values.
5
Under review as a conference paper at ICLR 2021
optimizing regularized (former) or constrained (latter) second-order Taylor models of the objective L
around the current iterate wt.5 In particular, in iteration t the update step of the trust region algorithm
is computed as
min mt(s) := L(Wt) + g|S + 1 slBts , s.t. |同"≤ ∆t,	(7)
s∈Rd	2	t
where ∆t > 0 and gt and Bt are either ▽£(Wt) and V2L(Wt) or suitable approximations. The
matrix At induces the shape of the constraint set. So far, the common choice for neural networks
is At := I, ∀t which gives rise to spherical trust regions (Xu et al., 2017a; Liu et al., 2018). By
solving the constrained problem (7), TR methods overcome the problem that pure Newton steps may
be ascending, attracted by saddles or not even computable. Please see Appendix B for more details.
Why ellipsoids? There are many sources for ill-conditioning in neural networks such as un-centered
and correlated inputs (LeCun et al., 2012), saturated hidden units, and different weight scales in
different layers (Van Der Smagt & Hirzinger, 1998). While the quadratic term of model (7) accounts
for such ill-conditioning to some extent, the spherical constraint is completely blind towards the
loss surface. Thus, it is advisable to instead measure distances in norms that reflect the underlying
geometry (see Chap. 7.7 in Conn et al. (2000)). The ellipsoids we propose are such that they allow for
longer steps along coordinates that have seen small gradient components in the past and vice versa.
Thereby the TR shape is adaptively adjusted to fit the current region of the loss landscape. This is not
only effective when the iterates are in an ill-conditioned neighborhood of a minimizer (Fig. 1), but it
also helps to escape elongated plateaus (see autoencoder in Sec. 5). Contrary to adaptive first-order
methods, the diameter (∆t) is updated directly depending on whether or not the local Taylor model is
an adequate approximation at the current point.
4.1	Convergence of ellipsoidal Trust Region methods
Inspired by the success of adaptive gradient methods, we investigate the use of their preconditioning
matrices as norm inducing matrices for second-order TR methods. The crucial condition for con-
vergence is that the applied norms are not degenerate during the entire minimization process in the
sense that the ellipsoids do not flatten out (or blow up) completely along any given direction. The
following definition formalizes this intuition.
Definition 1 (Uniformly equivalent norms). The norms kWkAt := (WlAtW)1/2 induced by
symmetric positive definite matrices At are called uniformly equivalent, if ∃μ ≥ 1 such that
∀W ∈ Rd , ∀t = 1, 2, . . .
3lWkAt ≤ Ilwk2 ≤ μkwkAt∙	(8)
μ
We now establish a result which shows that the RMSProp ellipsoid is indeed uniformly equivalent.
Lemma 2 (Uniform equivalence). Suppose Ilgtk2 ≤ LH for all Wt ∈ Rd, t = 1,2,... Then
there always exists e > 0 such that the proposed preconditioning matrices Arms,t (Eq. 3) are
uniformly equivalent, i.e. Def. 1 holds. The same holds for the diagonal variant.
Consequently, the ellipsoids Arms,t can directly be applied to any convergent TR framework without
losing the guarantee of convergence (Conn et al. (2000), Theorem 6.6.8).6 In Theorem 1 we extend
this result by showing the (to the best of our knowledge) first convergence rate for ellipsoidal TR
methods. Interestingly, similar results cannot be established for Aada,t, which reflects the widely
known vanishing stepsize problem that arises since squared gradients are continuously added to the
preconditioning matrix. At least partially, this effect inspired the development of RMSprop (Tieleman
& Hinton, 2012) and Adadelta (Zeiler, 2012).
5In the following we only treat TR methods, but we emphasize that the use of matrix induced norms can
directly be transferred to the cubic regularization framework.
6 Note that the assumption of bounded batch gradients, i.e. smooth objectives, is common in the analysis of
stochastic algorithms (Allen-Zhu, 2017; Defazio et al., 2014; Schmidt et al., 2017; Duchi et al., 2011).
6
Under review as a conference paper at ICLR 2021
4.2	A stochastic ellipsoidal TR framework for neural network training
Since neural network training often constitutes a large-scale learning problem in which the number of
datapoints n is high, we here opt for a stochastic TR framework in order to circumvent memory issues
and reduce the computational complexity. To obtain convergence without computing full derivative
information, we first need to assume sufficiently accurate gradient and Hessian estimates.
Assumption 1 (Sufficiently accurate derivatives). The approximations of the gradient and Hessian
at step t satisfy
kgt - VL(Wt)k ≤ δg and ∣B - V2L(wt)k ≤ §h,
where δg ≤ (1-?)'g and 6h ≤ min { (I-?VeH , 1}, forsome 0 <v< 1.
For finite-sum objectives such as Eq. (1), the above condition can be met by random sub-sampling due
to classical concentration results for sums of random variables (Xu et al., 2017b; Kohler & Lucchi,
2017; Tripuraneni et al., 2017). Following these references, we assume access to the full function
value in each iteration for our theoretical analysis but we note that convergence can be retained even
for fully stochastic trust region methods (Gratton et al., 2017; Chen et al., 2018; Blanchet et al., 2016)
and indeed our experiments in Section 5 use sub-sampled function values due to memory constraints.
Secondly, we adapt the framework of Yao et al. (2018); Xu et al. (2017b), which allows for cheap
inexact subproblem minimization, to the case of iteration-dependent constraint norms (Alg. 1).
Algorithm 1 Stochastic Ellipsoidal Trust Region Method
1 2 3 4 5 6 7 8	: Input: W0 ∈ Rd, γ > 1, 1 > η > 0, ∆0 > 0 : for t = 0, 1, . . . , until convergence do : Compute approximations gt and Bt . : Ifkgtk ≤ g, set gt := 0. :	Set At := Arms,t orAt := diag(Arms,t) (see Eq. (3)). : Obtain st by solving mt (st) approximately. L(Wt) -L(Wt+st) Compute ratio of function over model decrease: Pt =			；-ʌ— mt(0) - mt(st) : Set ∆	= γ∆t	ifρS,t ≥ η | : and W	= Wt + st	ifρt ≥ η (successful) t+1	∖∆t∕γ	if ρs,t < η, '	Wt+1	[wt	otherwise (unsuccessful).
9: end for
Assumption 2 (Approximate model minimization). Each update step st yields at least as much
model decrease as the Cauchy- and Eigenpoint simultaneously, i.e.mt(st) ≤ mt (stC) and mt(st) ≤
mt (stE), where stC and stE are defined in Eq.(28).
Given that the adaptive norms induced by Arms,t satisfy uniform equivalence as shown in Lemma
2, the following Theorem establishes an O max g-2-H1 2, -H3 worst-case iteration complexity
which effectively matches the one of Yao et al. (2018).
Theorem 1 (Convergence rate of Algorithm 1). Assume that L(W) is second-order smooth with
Lipschitz constants Lg and LH. Furthermore, let Assumption 1 and 2 hold. Then Algorithm 1
finds an O(g, H) first- and second-order stationary point in at most O max g-2-H1, -H3
iterations.
The proof of this statement is a straight-forward adaption of the proof for spherical constraints, taking
into account that the guaranteed model decrease changes when the computed step st lies outside the
Trust Region. Due to the uniform equivalence established in Lemma 2, the altered diameter of the
trust region along that direction and hence the change factor is always strictly positive and finite.
7
Under review as a conference paper at ICLR 2021
MLP	Autoencoder
(SSOl)BOI
ISlNn—u。WSCH
ResNet18
Ol 234567
# Ofbackpropanatlons *10*
OT-2-3Y
(9SO⅞OI
Othvhid
0 1000 2000 3000 «00 5000 6000 7000 8000
# of backpropagations
Figure 3: Mean and 95% confidence interval of 10 runs. Green dotted line indicates 99% training accuracy.
5	Experiments
To validate our claim that ellipsoidal TR methods yield improved performance over spherical ones, we
run a set of experiments on two image datasets and three types of network architectures. All methods
run on (almost) the same hyperparameters across all experiments (see Table 1 in Appendix B)As
depicted in Fig. 3, the ellipsoidal TR methods consistently outperform their spherical counterpart
in the sense that they reach full training accuracy substantially faster on all problems. Moreover,
their limit points are in all cases lower than those of the uniform method. Interestingly, this makes
an actual difference in the image reconstruction quality of autoencoders (see Figure 12), where the
spherically constrained TR method struggles to escape a saddle. We thus draw the clear conclusion
that the ellipsoidal constraints we propose are to be preferred over spherical ones when training neural
nets with second-order methods. More experimental and architectural details are provided in App. C.
To put the previous results into context, we also benchmark several state-of-the-art gradient methods.
For a fair comparison, we report results in terms of number of backpropagations, epochs and time.
All figures can be found in App. C. Our findings are mixed: For small nets such as the MLPs the
TR method with RMSProp ellipsoids is superior in all metrics, even when benchmarked in terms of
time. However, while Fig. 9 indicates that ellipsoidal TR methods are slightly superior in terms of
backpropagations even for ResNets and Autoencoders, a close look at Fig. 10 and 11 reveals that they
at best manage to keep pace with first-order methods in terms of epochs and are inferior in time.
6	Conclusion
We investigated the use of ellipsoidal trust region constraints for neural networks. We have shown that
the RMSProp matrix satisfies the necessary conditions for convergence and our experimental results
demonstrate that ellipsoidal TR methods outperform their spherical counterparts significantly across
a large set of experiments. We thus consider the development of further ellipsoids that can potentially
adapt even better to the loss landscape such as e.g. (block-) diagonal hessian approximations
(e.g. Bekas et al. (2007)) or approximations of higher order derivatives as an interesting direction of
future research.
Interestingly, the gradient method benchmark indicates that the value of Hessian information for
neural network training is limited for mainly three reasons: 1) second-order methods rarely yield
better limit points, which suggests that saddles and spurious local minima are not a major obstacle
in modern day architectures; 2) The per-iteration time complexity is noticeably lower for first-order
methods (Figure 11). The latter observations suggests that advances in distributed second-order
algorithms (e.g., OsaWa et al. (2018); Dunner et al. (2018)) constitute a promising direction of
research towards the goal of a more widespread use of Newton-type methods in deep learning.
8
Under review as a conference paper at ICLR 2021
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
The case for full-matrix adaptive regularization. arXiv preprint arXiv:1806.02958, 2018.
Guillaume Alain, Nicolas Le Roux, and Pierre-Antoine Manzagol. Negative eigenvalues of the
hessian in deep neural networks. 2018.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. The
Journal of Machine Learning Research ,18(1):8194-8244, 2017.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251-276,
1998.
Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with
second order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29-37.
San Matteo, CA: Morgan Kaufmann, 1988.
Costas Bekas, Effrosyni Kokiopoulou, and Yousef Saad. An estimator for the diagonal of a matrix.
Applied numerical mathematics, 57(11-12):1214-1229, 2007.
Jose Blanchet, Coralia Cartis, Matt Menickelly, and Katya Scheinberg. Convergence rate analysis of
a stochastic trust region method for nonconvex optimization. arXiv preprint arXiv:1609.07428,
2016.
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
COMPSTAT’2010, pp. 177-186. Springer, 2010.
Leon Bottou, Frank E Curtis, and Jorge NocedaL Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. arXiv preprint arXiv:1710.11606, 2017.
Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation methods for
unconstrained optimization. part i: motivation, convergence and numerical results. Mathematical
Programming, 127(2):245-295, 2011.
Coralia Cartis, Nicholas IM Gould, and Ph L Toint. Complexity bounds for second-order optimality
in unconstrained optimization. Journal of Complexity, 28(1):93-108, 2012a.
Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. How Much Patience to You Have?: A
Worst-case Perspective on Smooth Noncovex Optimization. Science and Technology Facilities
Council Swindon, 2012b.
Olivier Chapelle and Dumitru Erhan. Improved preconditioner for hessian free optimization. In NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, volume 201, 2011.
Ruobing Chen, Matt Menickelly, and Katya Scheinberg. Stochastic optimization using a trust-region
method and random models. Mathematical Programming, 169(2):447-487, 2018.
Andrew R Conn, Nicholas IM Gould, and Philippe L Toint. Trust region methods. SIAM, 2000.
Frank E Curtis and Daniel P Robinson. Exploiting negative curvature in deterministic and stochastic
optimization. arXiv preprint arXiv:1703.00412, 2017.
Frank E Curtis, Daniel P Robinson, and Mohammadreza Samadi. A trust region algorithm with a
worst-case iteration complexity of O(3-2) for nonconvex optimization. Mathematical Program-
ming, 162(1-2):1-32, 2017.
9
Under review as a conference paper at ICLR 2021
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. arXiv preprint arXiv:1803.05999, 2018.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Simon S Du and Jason D Lee. On the power of over-parametrization in neural networks with quadratic
activation. arXiv preprint arXiv:1803.01206, 2018.
Simon S Du, Chi Jin, Jason D Lee, Michael I Jordan, Aarti Singh, and Barnabas Poczos. Gradient
descent can take exponential time to escape saddle points. In Advances in Neural Information
Processing Systems, pp. 1067-1077, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Celestine Dunner, Aurelien Lucchi, Matilde Gargiani, An Bian, Thomas Hofmann, and Martin Jaggi.
A distributed second-order algorithm you can trust. arXiv preprint arXiv:1806.07569, 2018.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points-online stochastic
gradient for tensor decomposition. In COLT, pp. 797-842, 2015.
Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
http://distill.pub/2017/momentum.
Serge Gratton, Clement W Royer, Luls N Vicente, and Zaikun Zhang. Complexity and global rates
of trust-region methods based on probabilistic models. IMA Journal of Numerical Analysis, 2017.
Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. In International Conference on Machine Learning, pp. 573-582, 2016.
Martin T Hagan and Mohammad B Menhaj. Training feedforward networks with the marquardt
algorithm. IEEE transactions on Neural Networks, 5(6):989-993, 1994.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio,
and Amos Storkey. Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623,
2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jonas Moritz Kohler and Aurelien Lucchi. Sub-sampled cubic regularization for non-convex opti-
mization. In International Conference on Machine Learning, 2017.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only
converges to minimizers. In Conference on Learning Theory, pp. 1246-1257, 2016.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. arXiv preprint arXiv:1805.08114, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
In Advances in Neural Information Processing Systems, pp. 597-607, 2017.
10
Under review as a conference paper at ICLR 2021
Liu Liu, Xuanqing Liu, Cho-Jui Hsieh, and Dacheng Tao. Stochastic second-order methods for
non-convex optimization with inexact hessian and gradient. arXiv preprint arXiv:1809.09853,
2018.
Linjian Ma, Gabe Montague, Jiayu Ye, Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W
Mahoney. Inefficiency of k-fac for large batch size training. arXiv preprint arXiv:1903.06237,
2019.
James Martens. Deep learning via hessian-free optimization. In ICML, volume 27, pp. 735-742,
2010.
James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
arXiv:1412.1193, 2014.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
Dominic Masters and Carlo Luschi. Revisiting small batch training for deep neural networks. arXiv
preprint arXiv:1804.07612, 2018.
Eiji Mizutani and Stuart E Dreyfus. Second-order stagewise backpropagation for hessian-matrix
analyses and investigation of negative curvature. Neural Networks, 21(2-3):193-203, 2008.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance.
Mathematical Programming, 108(1):177-205, 2006.
Jorge Nocedal and Stephen J Wright. Numerical optimization, 2nd Edition. Springer, 2006.
Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka.
Second-order optimization method for large mini-batch: Training resnet-50 on imagenet in 35
epochs. arXiv preprint arXiv:1811.12019, 2018.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint
arXiv:1301.3584, 2013.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zem-
ing Lin, Luca Desmaison, Alban aComplexity bounds for second-order optimality in unconstrained
optimizationnd Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147-160,
1994.
Herbert Robbins and Sutton Monro. A stochastic approximation method. In The Annals of Mathe-
matical Statistics - Volume 22, Number 3. Institute of Mathematical Statistics, 1951.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural computation, 14(7):1723-1738, 2002.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
arXiv preprint arXiv:1703.07950, 2017.
Trond Steihaug. The conjugate gradient method and trust regions in large scale optimization. SIAM
Journal on Numerical Analysis, 20(3):626-637, 1983.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
11
Under review as a conference paper at ICLR 2021
Nilesh Tripuraneni, Mitchell Stern, Chi Jin, Jeffrey Regier, and Michael I Jordan. Stochastic cubic
regularization for fast nonconvex optimization. arXiv preprint arXiv:1711.02838, 2017.
Patrick Van Der Smagt and Gerd Hirzinger. Solving the ill-conditioning in neural network learning.
In Neural networks: tricks of the trade, pp. 193-206. SPringer, 1998.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex
landscaPes, from any initialization. arXiv preprint arXiv:1806.01811, 2018.
Eugene P Wigner. Characteristic vectors of bordered matrices with infinite dimensions i. In The
Collected Works of Eugene Paul Wigner, PP. 524-540. SPringer, 1993.
Peng Xu, Farbod Roosta-Khorasan, and Michael W Mahoney. Second-order oPtimization for non-
convex machine learning: An emPirical study. arXiv preprint arXiv:1708.07827, 2017a.
Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Newton-tyPe methods for non-convex
oPtimization under inexact hessian information. arXiv preprint arXiv:1708.07164, 2017b.
Zhewei Yao, Peng Xu, Farbod Roosta-Khorasani, and Michael W Mahoney. Inexact non-convex
newton-tyPe methods. arXiv preprint arXiv:1802.06925, 2018.
Matthew D Zeiler. Adadelta: an adaPtive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
12