Under review as a conference paper at ICLR 2021
Outlier-Robust Optimal Transport
Anonymous authors
Paper under double-blind review
Ab stract
Optimal transport (OT) provides a way of measuring distances between distribu-
tions that depends on the geometry of the sample space. In light of recent ad-
vances in solving the OT problem, OT distances are widely used as loss functions
in minimum distance estimation. Despite its prevalence and advantages, however,
OT is extremely sensitive to outliers. A single adversarially-picked outlier can
increase OT distance arbitrarily. To address this issue, in this work we propose
an outlier-robust OT formulation. Our formulation is convex but challenging to
scale at a first glance. We proceed by deriving an equivalent formulation based on
cost truncation that is easy to incorporate into modern stochastic algorithms for
regularized OT. We demonstrate our model applied to mean estimation under the
Huber contamination model in simulation as well as outlier detection on real data.
1 Introduction
Optimal transport is a fundamental problem in applied mathematics. In its original form (Monge,
1781), the problem entails finding the minimum cost way to transport mass from a prescribed prob-
ability distribution μ on X to another prescribed distribution V on X. Kantorovich (1942) relaxed
Monge’s formulation of the optimal transport problem to obtain the Kantorovich formulation:
OT(μ,ν)，	min E(χ1,χ2)〜∏[c(X1,X2)],	(1.1)
∏∈F (μ,ν)	'	''
where F(μ, V) is the set of couplings between μ and V (probability distributions on XXX whose
marginals are μ and V) and C is a cost function, where We typically assume c(χ,y) ≥ 0 and
c(x, x) = 0. Compared to other notions of distance between probability distributions, optimal
transport uniquely depends on the geometry of the sample space.
Recent advancements in optimization for optimal transport (Cuturi, 2013; Solomon et al., 2015;
Genevay et al., 2016; Seguy et al., 2018) enabled its broad adaptation in machine learning applica-
tions where geometry of the data is important. See (Peyre & Cuturi, 2018) for a survey. Optimal
transport has found applications in natural language processing (Kusner et al., 2015; Huang et al.,
2016; Alvarez-Melis & Jaakkola, 2018; Yurochkin et al., 2019), generative modeling (Arjovsky
et al., 2017), clustering (Ho et al., 2017), domain adaptation (Courty et al., 2014; 2017), large-scale
Bayesian modeling (Srivastava et al., 2018), and many other domains.
Many applications use OT as a loss in an optimization problem of the form:
θ ∈ argmi□θ∈θ OT(μn,Vθ),	(1.2)
where {vθ }θ∈θ is a collection of parametric models, μn is the empirical distribution of the samples.
Such estimators are called minimum Kantorovich estimators (MKE) (Bassetti et al., 2006). They are
popular alternatives to likelihood-based estimators, especially in generative modeling. For example,
when OT(∙, ∙) is the Wasserstein-1 distance and vθ is a generator parameterized by a neural network
with weights θ, equation 1.2 corresponds to the Wasserstein GAN (Arjovsky et al., 2017).
One drawback of optimal transport is its sensitivity to outliers. Because all the mass in μ must be
transported to V, a small fraction of outliers can have an outsized impact on the optimal transport
problem. For statistics and machine learning applications in which the data is corrupted or noisy,
this is a major issue. For example, the poor performance of Wasserstein GANs in the presence of
outliers was noted in the recent works on outlier-robust generative learning with f -divergence GANs
(Chao et al., 2018; Wu et al., 2020). The problem of outlier-robustness in MKE has not been studied,
with the exception of two concurrent works (Staerman et al., 2020; Balaji et al., 2020).
1
Under review as a conference paper at ICLR 2021
In this paper, we propose a modification of OT to address its sensitivity to outliers. Our formulation
can be used as a loss in equation 1.2 so that it is robust to a small fraction of outliers in the data. To
keep things simple, we consider the -contamination model (Huber & Ronchetti, 2009). Let νθ0 be
a member of a parametric model {νθ : θ ∈ Θ} and let
μ = (I - E)Vθo + ∈ν,
where μ is the data-generating distribution, e > 0 is the fraction of outliers, and V is the distribution
of the outliers. Although the fraction of outliers is capped at E, the value of the outliers is arbitrary,
so the outliers may have an arbitrarily large impact on the optimal transport problem. Our goal is
to modify the optimal transport problem so that it is more robust to outliers. We have in mind the
downstream application of learning θo from (samples from) μ in the E-Contamination model. Our
main contributions are as follows:
1.	We propose a robust OT formulation that is suitable for statistical estimation in the E-
contamination model using MKE.
2.	We show that our formulation is equivalent to the original OT problem with a clipped transport
cost. This connection enables us to leverage the voluminous literature on computational optimal
transport to develop efficient algorithm to perform MKE robust to outliers.
3.	Our formulation enables a new application of optimal transport: outlier detection in data.
2	Problem Formulation
2.1	ROBUST OT FOR MKE
To promote outlier-robustness in MKE, we need to allow the corresponding OT problem to ignore
the outliers in the data distribution μ. The E-contamination model imposes a cap on the fraction of
outliers, so it is not hard to see that ∣∣μ - vθ0 ∣∣tv ≤ e, where ∣∣ ∙ kτv is the total-variation norm
defined as ∣∣μ∣∣TV = / 2∣μ(dχ)∣. This suggests We solve a TV-COnStrained/regularized version of
equation 1.2. The constrained version
min	OT(μ, vθ )
θ∈θ,μ
subject to ∣∣μ 一 μ∣τv ≤ E
suffers from identification issues. In particular, it cannot distinguish between “clean” distributions
within TV distance E of νθ0. This makes it unsuitable as a loss function for statistical estimation,
because it cannot lead to a consistent estimator. However, its regularized counterpart
min OT(μ + s, vθ) + λ∣s∣τv,	(2.1)
θ∈Θ,s
where λ > 0 is a regularization parameter, does not suffer from this issue. In the rest of this paper,
we work with the TV-regularized formulation equation 2.1.
The main idea of our formulation is to allow for modifications of μ, while penalizing their magnitude
and ensuring that the modified μ is still a probability measure. Below We formulate this intuition in
an optimization problem titled ROBOT (ROBust Optimal Transport):
Formulation 1:
minΠ∈F +(Rd×Rd)
s∈F (Rd)
subject to
ROBOT(μ, ν)= <
/C(x, y) Π(dx, dy) + λ∣s∣TV
J	Π(dx, dy) = J (μ(dx) + s(dx)) ≥ 0
∀ B ∈ B (Rd ) (Borel σ-algebra)
Π(dx,dy) =	ν(dy) ∀C ∈ B(Rd)
Z s(dx) = 0.
(2.2)
2
Under review as a conference paper at ICLR 2021
Here F(Rd) denotes the set of all signed measures with finite total variation on Rd, F+(Rd × Rd)
is the set of all measures with finite total variation on Rd × Rd .
The first and the last constraints ensure that μ + S is a valid probability measure, while λ∣∣skτv
penalizes the amount of modifications in μ. It is worth noting that we can identify exact locations of
outliers in μ by inspecting μ + s, i.e. if μ(x) + s(x) = 0, then X got eliminated and is an outlier.
ROBOT, unlike classical OT, guarantees that an adversarially picked outliers can not increase the
distance arbitrarily. Let μ = (1 - e)μ + eμo i.e. μ is μ contaminated with outliers from μo and let V
be an arbitrary measure (in MKE, μ is the contaminated data and V is the model We learn). Adversary
can arbitrarily increase OT(μ, V) by manipulating the outlier distribution μc. For ROBOT We have
the following bound:
Theorem 2.1. Let μ = (1 — e)μ + eμc for some E ∈ [0,1) ,then
ROBOT(", V) ≤ (OT(μ, V) + λc∣∣μ 一 μ∕∣τv) ∧ λ∣∣μ - VkTV ∧ OT(μ, V).	(2.3)
This bound has two key takeaways: since TV norm of any two distributions is bounded by 1, ad-
versary can not increase ROBOT(μ, V) arbitrarily; in the absence of outliers, ROBOT is bounded by
classical OT. See Appendix C for the proof.
Related work We note connection between equation 2.2 and unbalanced OT (UOT) (Chizat.,
2017; Chizat et al., 2018). UOT is typically formulated by replacing TV norm with KL(μ + s∣μ)
and adding an analogous term for V. Chizat et al. (2018) studied entropy regularized UOT with
various divergences penalizing marginal violations. Optimization problems similar to equation 2.2
have also been considered outside of the ML literature (Piccoli & Rossi, 2014; Liero et al., 2018).
We are unaware of prior applications of UOT to outlier-robustness, but it was studied in the con-
current work of Balaji et al. (2020). Another relevant variation of OT is partial OT (Figalli, 2010;
Caffarelli & McCann, 2010). It may also be considered for outlier-robustness, but it has a drawback
of forcing mass destruction rather than adjusting marginals to ignore outliers when they are present.
A concurrent work by Staerman et al. (2020) took a different path: they replaced the expectation
in the Wasserstein-1 dual with a median-of-means to promote robustness. It is unclear what is the
corresponding primal, making it hard to interpret as an optimal transport problem.
A major challenge with the aforementioned methods, including our Formulation 1, is the difficulty
of the optimization problem. This is especially the case for MKEs, where a transport problem has
to be solved in every iteration to obtain the gradient of the model parameters. Chizat et al. (2018)
proposed a Sinkhorn-like algorithm for entropy regularized UOT, but it is not amenable to stochastic
optimization. Balaji et al. (2020) proposed a stochastic optimization algorithm based on the UOT
dual, but it requires two additional neural networks (total of four including dual potentials) to pa-
rameterize modified marginal distributions (i.e., μ + S and analogous one for V). Optimizing with
a median-of-means in the objective function as in (Staerman et al., 2020) is also challenging. The
key contribution of our work is a formulation equivalent to equation 2.2, which is easily compatible
with the large body of classical OT optimization techniques (Cuturi, 2013; Solomon et al., 2015;
Genevay et al., 2016; Seguy et al., 2018).
More efficient equivalent formulation At a first glance, there are two issues with equation 2.2:
it appears asymmetric and it is unclear if it can be optimized efficiently. Below we present an
equivalent formulation that is free of these issues:
Formulation 2:
minΠ∈F+(Rd×Rd)	Cλ(x, y)Π(dx, dy)
ROBOT(μ, V) = subject to
/	Π(dx, dy) = / μ(dx) ∀ B ∈ B(Rd)	(2.4)
Π(dx, dy) =	V(dy) ∀C ∈ B(Rd),
where Cλ is the truncated cost function defined as Cλ(x, y) = C(x, y) ∧ 2λ. Looking at equa-
tion 2.4, it is not apparent that it adds robustness to MKE, but it is symmetric, easy to combine
3
Under review as a conference paper at ICLR 2021
with entropic regularization by simply truncating the cost, and benefits from stochastic optimization
algorithms (Genevay et al., 2016; Seguy et al., 2018). This formulation also has a distant relation
to the idea of loss truncation for achieving robustness (Shen & Sanghavi, 2019). Pele & Werman
(2009) considered the Earth Mover Distance (discrete OT) with truncated cost to achieve computa-
tional improvements; they also mentioned its potential to promote robustness against outlier noise
but did not explore this direction.
In Section 3, we establish equivalence between the two ROBOT formulations, equation 2.2 and
equation 2.4. This equivalence allows us to obtain an efficient algorithm based on equation 2.4 for
robust MKE. We also provide a simple procedure for computing optimal s in equation 2.2 from
the solution of equation 2.4, enabling a new OT application: outlier detection. We verify the effec-
tiveness of robust MKE and outlier detection in our experiments in Section 4. Before presenting
the equivalence proof, we formulate the discrete analogs of the two ROBOT formulations for their
practical value.
2.2	Discrete ROBOT formulations
In practice we typically encounter samples from the distributions, rather then the distributions them-
selves. Sampling is also built into stochastic optimization. In this subsection, we present the discrete
versions of the ROBOT formulations. The key detail is that, in equation 2.2, μ, V and S are all sup-
ported on Rd, while in the discrete case the empirical measures μn ∈ ∆n-1 and Vm ∈ ∆m-1
are supported on a set of points (∆r is the unit probability simplex in Rr). As a result, to for-
mulate a discrete version of equation 2.2, We need to augment μn and Vm with each others, sup-
ports. To be precise, let supp(μn) = {Xι,...,Xn} and SuPP(Vm) = {Yι,... ,Ym}. Define
C = {Z1, Z2, . . . , Zm+n} = {X1, . . . , Xn, Y1, . . . , Ym}. Then discrete analog of equation 2.2 is
Formulation 1 (discrete):
min∏∈R(m + n) × (m + n) (Caug,Πi + λ [ks1k1 + kt1k1]
s∈Rm+n
ROBOT(μn,Vm)= Jsubjectto	Π1m+n =	S1], 口丁十八=[0 ] (2∙5)
t1	Vm
.	π 占 0,	1m+ns = 0,
where Caug ∈ R(m+n)×(m+n) is the augmented cost function Caug,i,j = c(Zi, Zj) (c is the ground
cost, e.g., squared Euclidean distance), s = (s1, t1) and 1r is the vector all ones in Rr. The TV
norm got replaced with its discrete analog, the L1 norm. Similarly to its continuous counterpart, the
optimization problem is harder than the typical OT due to additional constraint optimization variable
s and increased cost matrix size.
The discrete analog of equation 2.4 is straightforward:
Formulation 2 (discrete):
OO	minΠ∈Rn×m hCλ,Πi
B(〃n,Vm)= Rubjectto	Π1n =,
μn,
Π>1m = Vm,	Π0,
(2.6)
where Cλ,i,j = c(Xi, Yj)∧2λ. As in the continuous case, itis easy to adapt modern (regularized) OT
solvers without any computational overhead. As in the continuous case, formulations of equation 2.5
and equation 2.6 are equivalent. It is also possible to recover s of equation 2.5 from the solution of
equation 2.6 to perform outlier detection.
Two-sided formulation So far we have assumed that one of the input distributions does not have
outliers, which is the setting of MKE, where the clean distribution corresponds to the model we
learn. In some applications, both distributions may be corrupted. To address this case, we provide
an equivalent two-sided formulation, analogous to UOT with TV norm:
4
Under review as a conference paper at ICLR 2021
Formulation 3 (two-sided):
		min Π∈R(m+n)×(m+n) sι∈Rm+n, S2∈Rm+n	hCaug , Πi +		λ[ksiki	+ kti ki + ks2 ki	+ kt2ki]
ROBOT(μn,νm.)=	=‹	subject to	Π1m+ n	=	μn + si ti	, Π 1m+n =	s2 Vm + t2
		I	Π 0,	1>m+nsi =		0,	1>m+ns2 =0	
(2.7)
where s1 = (s1>, t1>)> and s2 = (s2>, t2>)>.
3	Equivalence of the ROBOT formulations
In this section we present our main theorem, which demonstrates the equivalence between two
formulations of the robust optimal transport:
Theorem 3.1. For any two measures μ and V, ROBOT(μ, V) has same value for both the formu-
lations, i.e., Formulation 1 is equivalent to Formulation 2 both for continuous and discrete case.
Moreover, we can recover optimal coupling of one formulation from the other.
Below we sketch the proof of this theorem and highlight some important techniques used in the
proof. We focus on the discrete case as it is more intuitive and has concrete practical implications in
our experiments. A complete proof can be found in Appendix A. Please also see Appendix A.2 for
the proof of equivalence between Formulations 1, 2 and 3 in the discrete case.
3.1	Proof sketch
In the remainder of this section we consider the discrete case, i.e., equation 2.5 for Formulation 1
(F1) and equation 2.6 for Formulation 2 (F2). Suppose Πg is an optimal solution of F2. Then We
construct a feasible solution Πj, s； = (s；,t；) ofF1 based on Πg with the same value of the objective
function as F2 and claim that (ΠJ, s；) is an optimal solution. We prove the claim by contradiction:
if (n；, s1) is not optimal, then there exists another Pair (∏ι, si) which is optimal forF1 with strictly
less objective value. We then construct another feasible solution n；,new of Formulation 2 which has
the same objective value as of (Πi, Si) for F1. This implies n；,new has strictly less objective value
for F2 than ∏2, which is a contradiction.
The two main pillars of this proof are (1) to construct a feasible solution ofF1 starting from a feasible
solution of F2 and (2) to show that the solution constructed is indeed optimal for F1. Hence step
(1) gives a recipe to construct an optimal solution of F1 starting from an optimal solution of F2. We
elaborate the first point in the next subsection, which has practical implications for outlier detection.
The other point is more technical; interested readers may go through the proof in Appendix A.1.
Algorithm 1 Generating optimal solution of F1 from F2
1:	Start with n； ∈ Rn×m, an optimal solution of Formulation 2.
2:	Create an augmented matrix Π ∈ Rm+n×m+n with all 0. Divide Π into four blocks:
πi1	π12
|{z} |{z}
Π= I n×n n×m I
∏21	∏22
|{z} |{z}
m×n m×m
3:	Set ∏12 J ∏2 and collect all the indices I = {(i,j) : Cij > 2λ}.
4:	Set Π12(i,j) J 0 for (i,j) ∈I.	'
5:	Set ∏22(j, j) J Pn=I ∏2(i, j)1(ij)∈ι for all 1 ≤ j ≤ m and set n； J Π.
6:	Set s； (i) ≤ Pm=I ∏2(i,j)1(ij)∈ι for all 1 ≤ i ≤ n.
7:	Sett；i(j) = Π22(j,j) for all 1 ≤ j ≤ m.
8:	return n；,s；,t；.
5
Under review as a conference paper at ICLR 2021
3.2	Going from Formulation 2 to Formulation 1
Let ∏2 (respectively ∏1) be an optimal solution of F2 (respectively F1). Recall that ∏ has di-
mension (m + n) × (m + n). From the column sum constraint in F1, we need to take the first n
columns of Π11 2 3 4 5 to be exactly 0, whereas the last m columns must sum up to νm . For any matrix A,
we denote by A[(a : b) × (c : d)] the submatrix consisting of rows from a to b and columns from c
to d. Our main idea is to put a modified version of Π22 in Π12 [(1 : n) × (n + 1 : m + n)] and make
Π21 [(n+1 : m+n) × (n+1 : m+n)] diagonal. First we describe how to modify Π22. Observe that, if
for some (i,j) C%,j > 2λ, we expect Xi ∈ supp(μn) tobe an outlier resulting in high transportation
cost, which is why we truncate the cost in F2. Therefore, to get an optimal solution of F1, we make
the corresponding value of optimal plan 0 and dump the mass into the corresponding slack variable
t21 in the diagonal of the bottom right submatrix. This changes the row sum, which is taken care of
by s12 . But, as we are not moving this mass outside the corresponding column, the column sum of
Π12 [(1 : (m + n)) : ((n + 1) : (m + n))] remains same as column sum of Π22, which is νn. We
summarize this procedure in Algorithm 1.
▲ ▲ ▲
A b,
a ▲
▲ c
	AAθl 公0 ▲
	EO b + c
Figure 1: Constructing optimal solution of Formulation 1 from optimal solution of Formulation 2.
Example. In Figure 1, we provide an example to visualize the construction. On the left, we have
Π22 , an optimal solution of Formulation 2. The blue triangles denote the positions where the corre-
sponding cost value is ≤ 2λ, and light-green squares denote the positions where the corresponding
value of the cost matrix is > 2λ. To construct an optimal solution Π21 of Formulation 1 from this Π22,
we first create an augmented matrix of size 6 × 6. We keep all the entries of of left 6 × 3 sub-matrix
as 0 (in this picture blank elements indicate 0). On the right submatrix, we put Π22 into the top-right
block, but remove the masses from light-green squares, i.e. where cost value is > 2λ, and put it in
the diagonal entries of the bottom right block as shown in Figure 1. This mass contributes to the
slack variables s1 and t1, and this augmented matrix along with s1, t1 give us an optimal solution of
Formulation 1.
3.3 Outlier detection with ROBOT
Our construction algorithm has practical consequences for outlier detection. Suppose we have two
datasets, a clean dataset Vm (i.e., has no outliers) and an outlier-contaminated dataset μn. We can
detect the outliers in μn without directly solving costly Formulation 1 by following Algorithm 2. In
this algorithm, λ is a regularization parameter that can be chosen via cross-validation or heuristically
(see Section 4.2 for an example). In Section 4.2, we use this algorithm to perform outlier detection
on image data.
Algorithm 2 Outlier detection in contaminated data
1: Start with μn (contaminted data) and Vm (clean data).
2: Solve Formulation 2 and obtain Π22 using a suitable value of λ.
3: Use Algorithm 1 to obtain Π12, s12, t21 from Π22.
4: Find I, the set of all the indices where μn + s； = 0.
5: Return I as the indices of outliers in μ2
6
Under review as a conference paper at ICLR 2021
Table 1: Robust mean estimation with GANs using different distribution divergences. True mean
is η0 = 05; sample size n = 1000; contamination proportion = 0.2. We report results over 30
experiment restarts.
Contamination	JS Loss	SH Loss	RKL Loss	ROBOT	UOT
N (0.1 ∙I5 ,I5)	0.09 ± 0.03	0.11 ± 0.03	0.115 ± 0.03	0.1 ± 0.03	0.1 ± 0.04
N(0.5 ∙I5,I5)	0.23 ± 0.04	0.24 ± 0.05	0.24 ± 0.05	0.117 ± 0.03	0.2 ± 0.04
N (I∙I5 ,I5)	0.43 ± 0.05	0.43 ± 0.06	0.43 ± 0.06	0.261 ± 0.06	0.25 ± 0.05
N (2∙l5 ,I5)	0.67 ± 0.07	0.67 ± 0.08	0.67 ± 0.08	0.106 ± 0.03	0.1 ± 0.03
l-otφ UOAPE-M
(a) Varying proportion of contamination
Figure 2: Empirical study of regularization hyperparameter λ sensitivity
2.0
l-otφ UOAPE-M
(b) Varying outlier distribution mean
4	Empirical studies
To evaluate effectiveness of ROBOT, we consider the task of robust mean estimation under the
Huber contamination model. The data is generated from (1 - )N (η0, Id) + N (η1 , Id) and the
goal is to estimate η0. Prior work has advocated for using f -divergence GANs (Chao et al., 2018;
Wu et al., 2020) for this problem and pointed out inefficiencies of Wasserstein GAN in the presence
of outliers. We show that our robust OT formulation allows us to estimate the uncontaminated mean
η0 comparably or better than a variety of f -divergence GANs. We also use this simulated setup to
study sensitivity to the regularization hyperparameter λ.
In our second experiment, we present a new application of optimal transport enabled by ROBOT.
Suppose we have collected a curated dataset νm (i.e., we know that it has no outliers)—such data
collection is expensive, and we want to benefit from it to automate subsequent data collection. Let
μn be a second dataset collected “in the wild," i.e., it may or may not have outliers. We demonstrate
how ROBOT can be used to identify outliers in μn using the curated dataset Vm.
4.1	Robust mean estimation
Following Wu et al. (2020), we consider a simple generator of the form gθ (x) = X + θ, X 〜
N(0, Id), d is the data dimension. The basic idea of robust mean estimation with GANs is to
minimize various distributional divergences between samples from gθ and observed data simulated
from (1 - )N(η0, Id) + N(η1, Id). The goal is to estimate η0 with θ. To efficiently implement
ROBOT GAN, we use a standard min-max optimization approach: solve the inner max (ROBOT)
and use gradient descent for the outer min parameter. To solve ROBOT, it is straightforward to adopt
any of the prior stochastic regularized OT solvers: the only modification is the truncation of the cost
entries as in equation 2.6. We use the stochastic algorithm for semi-discrete regularized OT from
7
Under review as a conference paper at ICLR 2021
(Genevay et al., 2016, Algorithm 2). We summarize ROBOT GAN in Algorithm 3. Line 5 - Line 10
perform the inner optimization where we solve entropy regularized OT dual with truncated cost and
Line 11 - Line 12 perform gradient update of θ.
Algorithm 3 ROBOT GAN
1:	Input: robustness regularizion λ, entroPic regularization a, data distribution μn ∈ ∆n-1,
supp(μn) = X = [Xi,..., Xn], steps sizes T and Y
2:	Initialize: Initialize θ = θinit, set number of iterations M and L, i = 0, V = V = 0.
3:	for j = 1, . . . , M do
4:	Generate Z 〜N(0,Id) and set Z = z + θ.
5:	Set the cost vector c ∈ Rn as c(k) = c(Xk, z) ∧ 2λ for k = 1, . . . , n.
6:	for i = 1, . . . , L do	. solve entropy regularized OT dual
7:	Set h J v-C and do the normalized exponential transformation U J(；：卜).
8:	Calculate the gradient VV J μn 一 u.
9:	Update V J V + γVV and V J (1/(j +	i))V	+ (j +	i 一 1/(j	+ i))v.
10:	Do the same transformation of V as in Step 7, i.e. set h J V-C and set Π J(：卢).
11:	Set Π(k) = 0 for k such that C(Xk, Z) > 2λ for k = 1, . . . , n.
12:	Calculate gradient with respect to θ as Vθ = 2 Z Pk Π(k) 一 X>Π
13:	Update θ J θ 一 τVθ.
14:	Ouput: θ
For the f -divergence GANs (Nowozin et al., 2016) we use the code of Wu et al. (2020) for GANs
with Jensen-Shannon (JS) loss, squared Hellinger (SH) loss and Reverse Kullback-Leibler (RKL)
loss. For the exact expression of these divergences see Table 1 of Wu et al. (2020). We report
estimation error measured by the Euclidean distance between true uncontaminated mean η0 and
estimated mean θ for various contamination distributions in Table 1. ROBOT GAN performs well
across all considered contamination distributions. As the difference between true mean η0 and
contamination mean η1 increases, the estimation error of all methods tends to increase. However,
when it becomes easier to distinguish outliers from clean samples, i.e., ηι = 2 ∙ I5, performance of
ROBOT noticeably improves.
We also compared to the Sinkhorn-based UOT algorithm (Chizat et al., 2018) available in the Python
Optimal Transport (POT) library (Flamary & Courty, 2017); to obtain a UOT GAN, we modified
steps 5-11 of Algorithm 3 for computing Π. Unsurprisingly, both ROBOT and UOT perform simi-
larly: recall equivalence to Formulation 3, which is similar to UOT with TV norm. The key insight of
our work is the equivalence to classical OT with truncated cost, that greatly simplifies optimization
and allows to use existing stochastic OT algorithms. In this experiment, the sample size n = 1000
is sufficiently small for the Sinkhorn-based UOT POT implementation to be effective, but it breaks
in the experiment we present in Section 4.2. We also tried the code of Balaji et al. (2020) based on
CVXPY (Diamond & Boyd, 2016), but it is too slow even for the n = 1000 sample size.
In the previous experiment, we set λ = 0.5. Now we demonstrate empirically that there is a broad
range of λ values performing well. In Figure 2a, we study sensitivity of λ under various contamina-
tion proportions E holding no = I5 and ηι = 5 ∙ I5 fixed. Horizontal lines correspond to λ = ∞,
i.e., vanilla OT. The key observations are: there is a wide range of λ efficient at all contamination
proportions, and ROBOT is always at least as good as vanilla OT (even when there is no contam-
ination E = 0). In Figure 2b, we present a similar study varying the mean of the contamination
distribution and holding E = 0.2 fixed. We see that as the contamination distribution gets closer to
the true distribution, it becomes harder to pick a good λ, but the performance is always at least as
good as the vanilla OT (horizontal lines).
4.2	Outlier detection for data collection
Our robust OT formulation equation 2.5 enables outlier identification. Let νm be a clean dataset
and μn potentially contaminated with outliers. Recall that ROBOT allows modification of one of
the input distributions to eliminate potential outliers. We can identify outliers in μn as follows: if
μn (i) + sι(i) = 0, then Xi, the ith point in μn is an outlier. Instead of directly solving equation 2.5,
8
Under review as a conference paper at ICLR 2021
Figure 3: Random sample of outliers detected by ROBOT from a dataset of MNIST digits Contami-
nated with Fashion MNIST images.
which may be inefficient, we use our equivalence results and solve an easier optimization problem
equation 2.6, followed by recovering s to find outliers via Algorithm 2.
Let Vm be a clean dataset consisting of 10k MNIST digits and μn be a dataset collected “in the
wild” consisting of (different) 8k MNIST digits and 2k Fashion MNIST images. We compute
ROBOT(μn, Vm) to identify outlier Fashion MNIST images in μn. For each point in μn We ob-
tain a prediction, outlier or clean, which allows us to evaluate accuracy. ROBOT outlier detection is
90% accurate in this experiment. We also comment on λ selection: since We knoW that Vm is clean,
We can subsample tWo datasets from it, compute vanilla OT to obtain transportation plan Π and set
λ to be half the maximum distance betWeen matched elements, i.e. 2λ = maxi,j {Cij : Πij > 0},
Where C is the cost matrix for the tWo subsampled datasets. This procedure is essentially estimating
maximum distance betWeen matched clean samples. We also present a random sample of outliers
identified by our method in Figure 3. All of the sampled outliers are Fashion MNIST images, al-
though 90% accuracy suggests that some of the outliers Were not identified. Decreasing λ can help
to find more outliers, but may result in some clean samples being mistaken for outliers. We con-
clude that ROBOT can be used to assist in data collection once an initial set of clean data has been
acquired. As We mentioned previously, the Sinkhorn-based UOT POT implementation is too expen-
sive for this experiment due to larger sample size, yielding memory errors on a personal laptop With
16GB RAM.
For comparison, We also consider a heuristic distance-based approach for identifying outliers. We
estimate diameter τ of the set of clean dataset Vm by taking the 99th percentile of the pairWise
distance matrix of samples in Vm . If outliers and clean data have disjoint support, We can adopt a
simple heuristic: for each sample in the potentially contaminated μn compute an average distance
to the clean samples in Vm and declare a sample as an outlier if this average distance is greater than
the diameter τ of the clean data. The accuracy of this procedure is 85.4%, inferior to the ROBOT
accuracy of 90%. The disjoint support assumption justifying the distance-based heuristic might be
too strong in practice. ROBOT continues to be effective even When the supports of clean and outlier
distributions are not easily separable.
5	Summary and discussion
We proposed and studied ROBOT, a robust formulation of optimal transport. We shoWed that al-
though the problem is seemingly asymmetric and challenging to optimize, there is an equivalent
formulation based on cost truncation that is symmetric and compatible With modern stochastic opti-
mization methods for OT.
ROBOT closely resembles unbalanced optimal transport (UOT). In our formulation, We added a
TV regularizer to the vanilla optimal transport problem. This is motivated by the -contamination
model. In UOT, the TV regularizer is typically replaced With a KL divergence. Other choices of the
regularizer may lead to neW properties and applications. Studying equivalent, simpler formulations
of UOT With different divergences may be a fruitful future Work direction.
From the practical perspective, in our experiments We observed no degradation of ROBOT GAN in
comparison to OT GAN, even When there Were no outliers. It is possible that replacing OT With
ROBOT may be beneficial for various machine learning applications of OT. Data encountered in
practice may not be explicitly contaminated With outliers, but it often has errors and other deficien-
cies, suggesting that a “no-harm” robustness is desirable.
9
Under review as a conference paper at ICLR 2021
References
David Alvarez-Melis and Tommi S Jaakkola. Gromov-Wasserstein alignment of word embedding
spaces. arXiv:1809.00013, 2018.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv:170L07875 [cs,
stat], January 2017.
Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications in
generative modeling and domain adaptation. Advances in Neural Information Processing Systems,
33, 2020.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum Kantorovich distance
estimators. Statistics & Probability Letters,76(12):1298-1302, 2006.
Luis A Caffarelli and Robert J McCann. Free boundaries in optimal transport and Monge-Ampere
obstacle problems. Annals of Mathematics, pp. 673-730, 2010.
Gao Chao, Yao Yuan, and Zhu Weizhi. Robust estimation via generative adversarial networks. In
International Conference on Learning Representations, 2018.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Scaling algorithms
for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563-2609,
2018.
Lenalc Chizat. Unbalanced optimal transport: Models, numerical methods, applications. Numerical
Analysis [math.NA]. Universite Paris sciences et lettres, 2017.
Nicolas Courty, Remi Flamary, and Devis Tuia. Domain adaptation with regularized optimal
transport. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pp. 274-289. Springer, 2014.
Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In Advances in Neural Information Processing
Systems, pp. 3730-3739, 2017.
Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In C. J. C.
Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural
Information Processing Systems 26, pp. 2292-2300. Curran Associates, Inc., 2013.
Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1-5, 2016.
Alessio Figalli. The optimal partial transport problem. Archive for Rational Mechanics and Analysis,
195(2):533-560, 2010.
Remi Flamary and Nicolas Courty. POT Python optimal transport library, 2017. URL https:
//pythonot.github.io/.
Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-
scale optimal transport. In Advances in Neural Information Processing Systems, pp. 3440-3448,
2016.
Nhat Ho, XuanLong Nguyen, Mikhail Yurochkin, Hung Hai Bui, Viet Huynh, and Dinh Phung.
Multilevel clustering via Wasserstein means. In International Conference on Machine Learning,
pp. 1501-1509, 2017.
Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger. Supervised
word mover’s distance. In Advances in Neural Information Processing Systems, pp. 4862-4870,
2016.
Peter J. Huber and Elvezio Ronchetti. Robust Statistics. Wiley Series in Probability and Statistics.
Wiley, Hoboken, N.J, 2nd ed edition, 2009. ISBN 978-0-470-12990-6.
10
Under review as a conference paper at ICLR 2021
Leonid Vitalievich Kantorovich. On the translocation of masses. In Dokl. Akad. Nauk. USSR (NS),
volume 37, pp. 199-201, 1942.
Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From Word Embeddings To Docu-
ment Distances. In International Conference on Machine Learning, pp. 957-966, June 2015.
Matthias Liero, Alexander Mielke, and GiUsePPe Savare. Optimal entropy-transport problems and a
new Hellinger-Kantorovich distance between positive measures. Inventiones Mathematicae, 211
(3):969-1117, 2018.
Gaspard Monge. Memoire Surla theorie des deblais etdes remblais. De l'Imprimerie Royale, 1781.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-GAN: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems, pp. 271-279, 2016.
Ofir Pele and Michael Werman. Fast and robust earth mover’s distances. In 2009 IEEE 12th Inter-
national Conference on Computer Vision, pp. 460-467. IEEE, 2009.
Gabriel Peyre and Marco Cuturi. Computational Optimal Transport. arXiv:1803.00567 [stat],
March 2018.
Benedetto Piccoli and Francesco Rossi. Generalized Wasserstein distance and its application to
transport equations with source. Archive for Rational Mechanics and Analysis, 211(1):335-358,
2014.
Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and
Mathieu Blondel. Large-Scale Optimal Transport and Mapping Estimation. arXiv:1711.02283
[stat], February 2018.
Yanyao Shen and Sujay Sanghavi. Learning with bad training data via iterative trimmed loss mini-
mization. In International Conference on Machine Learning, pp. 5739-5748. PMLR, 2019.
Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen,
Tao Du, and Leonidas Guibas. Convolutional Wasserstein distances: Efficient optimal transporta-
tion on geometric domains. ACM Transactions on Graphics (TOG), 34(4):1-11, 2015.
Sanvesh Srivastava, Cheng Li, and David B. Dunson. Scalable Bayes via Barycenter in Wasserstein
Space. arXiv:1508.05880 [stat], June 2018.
Guillaume Staerman, Pierre Laforgue, Pavlo Mozharovskyi, and Florence d'Alche Buc. When OT
meets MOM: Robust estimation of Wasserstein distance. arXiv:2006.10325, 2020.
C. Villani. Optimal Transport: Old and New. Grundlehren der Mathematischen Wissenschaften
[Fundamental Principles of Mathemtical Sciences]. Springer, Berlin, 2009.
Kaiwen Wu, Gavin Weiguang Ding, Ruitong Huang, and Yaoliang Yu. On Minimax Optimality of
GANs for Robust Mean Estimation. In International Conference on Artificial Intelligence and
Statistics, pp. 4541-4551, June 2020.
Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, and Justin Solomon.
Hierarchical Optimal Transport for Document Representation. arXiv:1906.10827 [cs, stat], June
2019.
11
Under review as a conference paper at ICLR 2021
A Proof of Theorem 3.1
A.1 Proof of discrete version
Proof. Define a matrix Π as:
π(i,j) = {∏2(i,j),
ifC(i,j)>2λ
otherwise
Also define s ∈ Rn and t ∈ Rm as:
m
s；(i)= - X ∏2(i,j)ic(i,j)>2λ
j=1
and similarly define:
n
i=1
These vectors corresponds to the row sums and the column sums of the elements of the optimal
transport plan of Formulation 2, where the cost function exceeds 2λ. Note that, these co-ordinates
of the optimal transport plan corresponding to those co-ordinates of cost matrix, where the cost
is greater than 2λ and contribute to the objective value via their sum only, hence any different
arrangement of these transition probabilities with same sum gives the same objective value.
Now based on this Π obtained we construct a feasible solution of Formulation 1 following Algorithm
1:
0Π
0 diag(tι)
The row sums of Π↑ is:
μn + s1
-埒,
and it is immediate from the construction that the column sums of ∏ is νm. Also as:
nm
X SM = X t1 (j)=	X	∏2(i,j)
i=1	j=1	(i,j):Ci,j >2λ
and s↑ W 0,t↑ 占 0, we have:
1>(μn + S； + t1) = 1>p = 1.
Therefore, we have (n；, s；,t；) is a feasible solution of Formulation 1. Now suppose this is not an
optimal solution. Pick an optimal solution Π, s, t of Formulation 1 so that:
hCaug ,∏i + λ [k≡kl +	< hCaug ,∏1i + λ [ks；ki + 帕旧
The following two lemmas provide some structural properties of any optimal solution of Formulation
1:
∏1
∏1ι
Lemma A.1. Suppose n；, s；,t； are optimal solution for Formulation 1. Divide ∏1 into four parts
corresponding to augmentation as in algorithm 1:
∏1
∏1,11
∏1,21
口；,12,
n；,22.
Then we have n；,n = n；,2i = 0 and n；,22 is a diagonal matrix.
LemmaA.2. If n；,s；,t； is an optimal solution of Formulation 1 then:
1.	If Ci,j > 2λ then ∏1(i,j) = 0.
2.	If Ci,j < 2λ for some i and for all 1 ≤ j ≤ n, then S1；(i) = 0.
3.	If Ci,j < 2λ for some j and for all 1 ≤ i ≤ m, then t1；(j) = 0.
4.	If Ci,j < 2λ then S1；(i)t1；(j) = 0.
12
Under review as a conference paper at ICLR 2021
We provide the proofs in the next subsection. By Lemma A.1 we can assume without loss of gener-
ality:
π= 0 diSet)]
Now based on (∏,飞,D We create a feasible solution namely Πg,new of Formulation 2 as follows:
Define the set of indices {iι, ∙ ∙ ∙ ,ik} and {jι,...,jι} as:
sti1 , st i2 , . . . , st ik > 0 and tj1 , tj2 , . . . , tjl > 0 .
Then by part (4) of Lemma A.2 we have Ciα,jβ > 2λ for α ∈ {1, . . . , k} and β ∈ {1, . . . , l}. Also
by part (2) of Lemma A.2 the value of transport plan at these co-ordinates is 0. Now distribute the
mass of slack variables in these co-ordinates such that the marginals of new transport plan becomes
exactly μn and νm. ThiS new transport plan is our Πgnew. Recall that, ks∣∣ι = ∣∣tkι. Hence, here
the regularizer value decreases by 2λkst k1 and the cost value increased by exactly 2λkst k1 as we are
truncating the cost. Hence we have:
hCλ, ∏2,newi = hCaug , ∏i + λ [|倒1 + |间1]
< hCaug ,可i+ λ [∣∣s"∣1 + ∣t1k1]
=hCλ, Π2i
which is contradiction as Πg is the optimal solution of Formulation 2. This completes the proof for
the discrete part.
□
A.2 Proof of equivalence for two sided formulation
Here we prove that our two sided formulation, i.e. Formulation 3 (equation 2.7) is equivalent to
Formulation 1 (equation 2.5) for the discrete case. Towards that end, we introduce another auxiliary
formulation and show that both Formulation 1 and Formulation 3 are equivalent to the following
auxiliary formulation of the problem.
Formulation 4:
WR,L,4(p, q) =
minΠ∈Rm×n,s1 ∈Rm,s2 ∈Rn
subject to
hC, Πi + λ [∣s1∣1 + ∣s2∣1]
Π1n =p+ s1
ΠT 1m = q + s2
Π0
(A.1)
First we show that Formulation 1 and Formulation 4 are equivalent in a sense that they have the
same optimal objective value.
Theorem A.3. Suppose C is a cost function such that C (x, x) = 0. Then Formulation 1 and
Formulation 4 has same optimal objective value.
Proof. Towards that end, we show that given one optimal variables of one formulation we can get
optimal variables of other formulation with the same objective value. Before going into details we
need the following lemma whose proof is provided in Appendix B:
Lemma A.4. Suppose ∏4,s4 1, s4 ? are the optimal variables of Formulation 4. Then S4 1 W 0 and
s4,2 W 0.	,	,	,
Now we prove that optimal value of Formulation 1 and Formulation 4 are same. Let (∏1, s↑,i, tɪ,ɪ)
is an optimal solution of Formulation 1. Then we claim that (∏1, s↑ ɪ,tɪ 1) is also an optimal
solution of Formulation 4. Clearly it is feasible solution of Formulation 4. Suppose it is not optimal,
i.e. there exists another optimal solution (Π4, st4,1, st4,2) such that:
hC, Πt 4i + λ(∣st4,1∣1 + ∣st4,2∣2) < hC, Π1,12i + λ(∣s1,1∣1 + ∣t1,1∣1)
13
Under review as a conference paper at ICLR 2021
Now based on (Π4, s4,1, s4,2) We construct a feasible solution of Formulation 1 as follows:
0 Π 4
∏ 1
0 -diag(s4,2)
Note that we proved in Lemma A.4 s4,2 W 0, hence we have Π1 占 0. Now as the column sums of
Π4 is q + s4,2, we have column sums of Π1 = [0 q>]> and the row sums are [(p + 氢/)> s>2]>.
Hence we take 良」=s4,1 and s1,2 = s4,2. Then it follows:
~.	-.. ∙.- ~. -.. ∙∙-
hCaug, ɪɪ 1 i +	λ [∣岛,1k1 +	11^1,2k1]	= hC, ∏4i	+ λ [k⅛4,1k1	+ ∣∣S4,2k1]
< hC,∏1,12i + λ M,1k1 + k%k1]
=hCaug ,∏1i + λ [∣H1k1 + |盾,1屈
This is contradiction as we assumed (∏1, s；/，埒*)is an optimal solution of Formulation 1. There-
fore we conclude (∏↑, $11,^,1) is also an optimal solution of Formulation 4 which further con-
cludes Formulation 1 and Formulation 4 have same optimal values. This completes the proof of the
theorem.	口
Theorem A.5. The optimal objective value of Formulation 3 and Formulation 4 are same.
Proof. Like in the proof of Theorem A.3 we also prove couple of lemmas.
Lemma A.6. Any optimal transport plan ∏3 of Formulation 3 has the following structure: Ifwe
write,
∏3	∏33,11
∏3 = ∏33,21
∏33,12
∏33,22
then ∏33,11 and ∏33,22 are diagonal matrices and ∏33,21 = 0.
Lemma A.7. If s33,1, t33,1, s33,2, t33,2 are four optimal slack variables in Formulation 3, then
s33,1, t33,1 W 0 and s33,2, t33,2	0.
Proof. The line of argument is same as in proof of Lemma A.4.
□
Next we establish equivalence. Suppose (∏33, s33,1, t33,1, s33,2, t33,2) are optimal values of Formulation
3. We claim that (∏33,12, s33,1 - s33,2, t33,1 - t33,2) forms an optimal solution of Formulation 4. The
objective value will then also be same as s33,1 W 0, s33,2 0 (Lemma A.7) implies ∣s33,1 - s33,2 ∣1 =
∣s33,1∣1 +	∣s33,2∣1 and similarly	t33,1	W 0, t33,2	0 implies ∣t33,1	- t33,2∣1 =	∣t33,1∣1 +	∣t33,2∣1.
Feasibility is immediate. Now for optimality, we again prove by contradiction. Suppose they are not
optimal. Then lets say ∏4, s4,1, s4,2 are an optimal triplet of Formulation 4. Now construct another
feasible solution of Formulation 3 as follows: Set ⅞,2 = t3,2 = 0, s3,1 = s4,1 and t3,1 = s4,2. Set
the matrix as:
Γ	ɪ	-l
∏ 3 = 0	∏4
[θ -diag(s4,2)
Then it follows that (∏3, ⅞,1, 33,2/3,1/3,2) is a feasible solution of Formulation 3. Finally we
have:
hCaug, ∏33i + λ ∣s33,1∣1 + ∣s33,2∣1 + ∣t33,1∣1 + ∣t33,2∣1
= hCaug, ∏33i + λ [∣s34,1∣1 + ∣s34,2∣1]
= hC, ∏3 4i + λ [∣s34,1∣1 + ∣s34,2 ∣1]
< hC, ∏33,12i + λ ∣s33,1 - s33,2∣1 + ∣t33,1 - t33,2 ∣1
= hCaug, ∏33i + λ ∣s33,1∣1 + ∣s33,2∣1 + ∣t33,1∣1 + ∣t33,2∣1
This contradicts the optimality of (∏3, s3,1, $3,2,t3,1,t3,2). ThiS completes the proof.	口
14
Under review as a conference paper at ICLR 2021
A.3 Proof of continuous version
Proof. In this proof we denote by F1 the optimization problem of equation equation 2.2 and by
F2 the optimization problem equation equation 2.4. Assume that μn and Vm denote the respective
empirical measures relative to μ, V. From Villani (2009), We know that μn, Vn converge weakly to
μ and V respectively. Therefore, ROBOT2(μn μ) → 0. Similary for Vn and V. Thus, by triangle
inequality,
lim ∣F2(μn,Vn)- F2(μ, v)| =0.
n→∞
But ROBOT2 (μn, Vn) = ROBOTI (μn, Vn). Therefore, our proof is complete if we can show that
lim |Fl(μn,Vm) - Fι(μ,V)| → 0.
n,m→∞
Let S = {s signed measure : μ + S is a probability measure in Rd}. For S ∈ S, define
minΠ∈F (Rd×Rd)
subject to
W(μ+S, V) =	μ(dx) + Smx)) ≥ 0
j C(x, y) Π(dx, dy) + λkSkT V
Π(dx,dy) ≥ 0∀A ∈ B(Rd × Rd)
A
∀B ∈ B(Rd)
/	Π(dx, dy) =	V(dy) ∀C ∈ B(Rd)
By Lemma A.8, ∃ S ∈ S such that ROBOT(μ, V) = W(μ + s,v) + λ∣∣skτv. Let S = s+ — s-,
where s+ and S- are positive measures on Rd. LetkSkTV = Y. Then, ∣∣S-kτv = ∣∣S+kτv = γ∕2.
Then consider Xι,...,Xn 〜(P — s-)∕(1 — Y), Yι,...,Yn 〜s-∕γ, Zι,...,Zn 〜S+∕γ. Then
for any bounded continuous function f,
lim	f(Xi)∕n
n→∞
i
lim Xf(Zi)∕n
n→∞
i
〃(x1(dx)
∕f(X) s+(dx)
(A.2)
Therefore, the distribution given by (P + S)n =———Y) Pi δχi + Y Pi δzi satisfies, (P + S)n →
P + S, and therefore from (Villani, 2009), limn→∞ WC ((P + S)n, Vn) → W(P + S, Q). Here δx
Y
is the Dirac mass at x. Moreover, kSnk = |k||, where Sn satisfies Sn = -(Ei δzi — Ei 6丫亩).
Also, ROBOT(μn,Vn) ≤ W((P + S)n, Vn) + 1|固|丁旷,and therefore ROBOT2(μ, v))=
limsupn→∞ ROBOT(μn,Vn) ≤ ROBOT(μ, v).
Now, let Sn satisfy Wι(μn, + Sn, Vn) + λ∣∣3nkτv = ROBOT(μn νn). Such an Sn exists by the
proof of the discrete part because μn Vn are discrete measures.
Then, similar to the Step 1 in the proof of Lemma A.8, there exists a probability measure μ ㊉ S and
a subsequence {nk}k≥ι such that μnk + Snk almost surely converges weakly to μ ㊉ s.
Moreover, similar to Step 2 of Lemma A.8 Wι(μn,k + Snk,μ ㊉ S) → 0 as well as IlSnkIITV →
kμ㊉ S — μ∣τv. Thus, Wι(μn,k + Snk, Vnk) + λ∣Snk ∣tv → Wι(μ㊉ s, ν) + λ∣μ㊉ S — μ∣τv. But
by the proof of the discrete part ROBOT(μ心, Vnk) = ROBOT2 (μnk, Vnk) → ROBOT2 (μ, ν).
Therefore, with S = μ ㊉ S — μ, Wι(μ + s, ν) + λ∣skτv = ROBOT∕μ, V).
Therefore, ROBOT2(μ, v) = limsupn→∞ ROBOT(μn Vn) ≥ ROBOT(μ, V). Thus the equal-
□
ity holds.
15
Under review as a conference paper at ICLR 2021
Lemma A.8. Assume that μ,ν is such that / ∣∣x∣∣dμ, / IlxIIdV < ∞. Moreover, assume that
C(x, y) in equation 2.2 is the lι norm, i.e., C(x,y) = ∣∣x 一 y∣. Then, there exists S with μ + S being
a probability measure such that
Wι(μ + S, V) + λ∣s∣τv = ROBOT(μ, V),	(A.3)
where Wi is the Wasserstein-1 norm with the cost function C(∙, ∙) as mentioned above.
Proof. Let μn, Vm be the empirical measures relative to μ, V respectively. We know that since
μn, Vm are discrete, there exists Sn satisfying Wι(μn + Sn, Vm) = ROBOT(μn, Vm,). We provide
the proof in the following steps.
Step 1: Almost surely μ X v, there exists a subsequence {nk}k≥ι such that {μn + Sn}n and {Vn}n
is relatively compact.
μ and V are probability measures on Rd and are therefore tight.
Let Ke be such that Pμ(X / Ke), PV(Y ∈ Ke) ≤ e/4.
Consider the empirical distributions Vn = Pi δγi /n, μn Pi δχ, /n of ν, μ respectively. Here, Xi 〜
μ and Yi ~ν.
Fix an ω. Then {X1, . . . , Xn, Y1, . . . , Yn} is fixed. Now by the construction for the discrete case,
Sn has support in {X1, . . . , Xn, Y1, . . . , Yn}.
Let Tn be the optimal transport map from μn to Vn. Then, for every i ≤ n, there exists a unique
j ≤ n, such that Tn(Xi) = Yj. Define τn : {1, . . . , n} → {1, . . . , n} such that τn(i) = j if
Tn(Xi) = Yj.Then μn + Sn = Pi δzi/n, where Zi = Xi or YTn(i) and δχ is the Dirac delta mass
at x.
Then, let Z 〜μn + Sn
Pω (Z / Ke∣μn + Sn) ≤ ɪ2 *Xi∈Ke)∕n + E 1(居 ∈Ke)∕n	(A.4)
Therefore, E(Pω(Z / Ke∣μn + Sn)) ≤ e∕2. Moreover, Var(Pω(Z / Ke∣μn + Sn)) = o(n-i) → 0.
Therefore, limn→∞ Pμn×νn (Pω (Z / Ke∣μn + Sn) ≤ E) → 1. Therefore μn + Sn is almost surely
tight and thus by Prokhorov’s Theorem also relatively compact.
Step 2: Therefore, for ω almost surely, there exists a subsequence {nk}k≥ι such that μn% + Snk
converges weakly to a limit (dependent on ω) μ ㊉ S which is a probability measure. Moreover,
/ ∣∣x∣d(μnk + Snk) < ∞ almost surely. By Bolzano-Weierstrass Theorem, there exists a further
subsequence {nkjι such that ʃ ∣∣xkd(μnki + Snkl) → / ∣∣x∣d(μ + s) almost surely. For the sake of
convenience, without loss of generality, we will replace the sub-subsequence {nkl }l with {nk}k≥1
henceforth.
Thus, by Theorem 6.9 of (Villani, 2009) , Wι(μ^∏k + Snk, μ ㊉ S) → 0 almost surely. Moreover,
Wi(μnk,μ) → 0 almost surely. Therefore IlSnkkTV → ∣∣μ ㊉ S 一 μ∣τv almost surely.
Step 3: Consider an arbitrary S = S+ 一 S-, such that S+ and S- are positive measures on Rd,
and μ + S is a probability measure. LetkS∣∣tv = γ. Then, ∣S-∣tv = ∣∣S+ ∣∣tv = γ∕2.
Then consider Xi,..., Xn ~ (μ 一 S-)∕(1 一 Y), Y1,... ,Yn ~ S- ∕γ, Zi,... ,Zn ~ S+∕γ. Then
for any bounded continuous function f,
nl→∞ X f (Xi)∕n = / f (x)(； [ S-) (dx)
lim X f (Zi)∕n =	f f (x) — (dx)	(A.5)
n→∞	γ
Therefore, the distribution given by (μ + S)n(A) = (1 — Y) Pi IXi∈A + (Y) Pi H-Zi∈A satisfies,
(μ + S)n → μ + S, and therefore from (Villani, 2009), limn→∞ Wι((μ + S)n,Vn) → Wi(μ + S, ν).
Y
Moreover, ∣ Sn 11 tv = IlSIlTV, where Sn satisfies Sn(A) = n Ei 1z,∈a - Ei 1^∈a.
16
Under review as a conference paper at ICLR 2021
But, Wι(μnk + Snk,Vnk) + λ∣Mk ∣∣tv ≤ Wι((μ + S)nk,~忆)+ 1|下狱防厂.Therefore, taking
limits, Wι(μ ㊉ s, ν) + λ∣μ ㊉ S 一 μ∣τv ≤ Wι(μ + S, V) + λ∣S∣τv, and thus the proof holds with
s = μ ㊉ s 一 μ.
□
B Proof of additional lemmas
B.1 Proof of Lemma A.1
Proof. The fact that ∏1,n = ∏l,21 = 0 follows from the fact that ∏ 占 0 and ∏11 = Q. To prove
that ∏1,22 is diagonal, We use the fact that the any diagonal entry the cost matrix is 0. Now suppose
口；,22 is not diagonal. Then define a matrix Π as following: set ∏11 = ∏21 = 0, ∏12 = 口；/2 and:
∏22(i,j)= {p 乙 πe,i),
if j = i
ifj 6=i
Also define S = s； and i as t(i) = Π22 (i, i). Then clearly (Π, S, t) is a feasible solution of Formu-
lation 1. Note that:
Ml1 = 1> ∏22i = 1>∏i22i = l"k1
and by our construction hCaug, Π)< hCaug, ∏1). Hence (Π, S, t) reduces the value of the objective
function of Formulation 1 which is a contradiction. This completes the proof.	□
B.2 Proof of Lemma A.2
Proof. 1. Suppose ΠJ(i, j) > 0. Then dump this mass to SKj) and make it 0. In this way
hCaug, ∏1i will decrease by > 2λ∏; (i,j) and the regularizer value will increase by atmost
2λ∏ι(i, j), resulting in overall reduction in the objective value, which leads to a contradiction.
2.	Suppose each entry of ith row of C is < 2λ. Then if S1； (i) > 0, we can distribute this mass in
the ith row such that, SKi) = aι + a? +---------+ am with the condition that t；(j) ≥ aj. Now we
reduce t1； as:
t；(j) 一 t；(j) - aj
Hence the value hCaug, ∏1 (i, j)i will increase by a value < 2Xs； (i) but the value of regularizer
will decrease by the value of 2λsJ(i), resulting in overall decrease in the value of objective
function.
3.	Same as proof of part (2) by interchanging row and column in the argument.
4.	Suppose not. Then choose E < s；(i) ∧ t；(j), Add E to ΠJ(i, j). Hence the cost function value
hCaug, n；i will increase by < 2λe but the regularizer value will decrease by 2λe, resulting in
overall decrease in the objective function.
□
B.3 Proof of Lemma A.4
Proof. For the notational simplicity, we drop the subscript 4 now as we will only deal with the
solution of Formulation 4 and there will be no ambiguity. We prove the Lemma by contradiction.
Suppose s； i > 0. Then we show one can come up with another solution (Π, si, s2) of Formulation
4 such that it has lower objective value. To construct this new solution, make:
Si,j
ifj 6=i
ifj=i
Now to change the optimal transport plan, we will only change ith row of Π*. We subtract
a1,a2,... ,an ≥ 0 from ith column of Π* in such a way, such that none of the elements are negative.
Hence the column sum will be change, i.e. the value of s? will be:
S2,j = Skj 一 aj ∀1 ≤ j ≤ n.
17
Under review as a conference paper at ICLR 2021
Now clearly from our construction:
hC,Πi ≤ hC, Π*i
For the regularization part, note that, as We only reduced ith element of s↑, We have ∣∣3ι∣∣ι =
∣∣s11∣1 - s[i∙ And by simple triangle inequality,
悒kι ≤ksM + kaιkι = ks2kι + SIi
by construction ai's, as a% ≥ 0 and Pi a% = sɪ,i. Hence we have:
I同kl + kM∣1 ≤ ks1kι - s；,i + ks2kι + S；,i = ks"∣1 + ksM .
Hence the value corresponding to regularizer will also decrease. This completes the proof. □
B.4 Proof of Lemma A.6
Proof. We prove this lemma by contradiction. Suppose ∏3 does not have the structure mentioned
in the statement of Lemma. Construct another transport plan for Formulation 3 Π3 as follows: Keep
∏3,12 = ∏3,12 and set Π3,12 = 0. Construct the other parts as:
Π3,11(i,j)
Pm=I π3,11(i, k) + Pn=1 π3,21(k, i), if i = j
0,	ifi 6=j
and
Π3,22(i,j ) = {P n=1 k"
It is immediate from the construction that:
ifi=j
ifi 6=j
.- 〜 . .. . ____________ ,.
hCaug ,π3i ≤ hCaug, 口3i
As for the regularization term: Note the by our construction s4 will be same as s3 as column sum of
Π3,22 is same as Π33,22. For the other three:
mn
s3(i) = ππ3,11(i,i) = X π3,11(i,k) + X π3,21(Ai)
k=1	k=1
n
s2(i) =Il3,22(i,i) = X π3,22(Ai)
k=1
and hence by construction:
悒 kι = 1>∏3,221 = ks3kι- 1>∏3,211.
k∙⅛kι = 1>∏3,111 + 1>∏3,21I = ks3kι
And also by our construction, si = s； + C where C = (∏3,21)>l. AS a consequence we have
∣c∣1 = 1>Π33,211. Then it follows:
4
X k≡iki = ks； + Ck + ks2ki - 1>∏3,2i1 + ks3ki + ks4ki
i=1
4
≤ X ksi3k1 + kCk1 - 1>Π33,211
i=1
4
= X ksi3k1
i=1
So the objective value is overall reduced. This contradicts the optimality of Π33 which completes the
proof.	□
18
Under review as a conference paper at ICLR 2021
C Proof of Theorem 2.1
Proof. The proof is immediate from the Formulation 1. Recall that the Formulation 1 can restruc-
tured as:
ROBOT(μ, V) = inf {OT(P, V) + λ∣∣P - μ∣∣τv}.
where the infimum is taking over all measure dominated by some common measure σ (with respect
to which μ, μc, V are dominated). Hence,
ROBOT(μ, ν) ≤ OT(P, v) + λ∣∣P - μkτv
for any particular choice of P. Taking P = μ We get that
ROBOT(μ, v) ≤ OT(μ, V) + λ∣∣μ - μkτv = OT(μ, V)) + λc∣∣μ - μ∕∣τv
Taking P = V we get ROBOT(μ, v) ≤ λ∣∣V - μkτv and finally taking P = μ we get
ROBOT(μ, v) ≤ OT(μ, V). This completes the proof.	口
19