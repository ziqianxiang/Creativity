Under review as a conference paper at ICLR 2021
Poisoned classifiers are not only backdoored,
THEY ARE FUNDAMENTALLY BROKEN
Anonymous authors
Paper under double-blind review
Ab stract
Under a commonly-studied “backdoor” poisoning attack against classification
models, an attacker adds a small “trigger” to a subset of the training data, such
that the presence of this trigger at test time causes the classifier to always predict
some target class. It is often implicitly assumed that the poisoned classifier is
vulnerable exclusively to the adversary who possesses the trigger. In this paper,
we show empirically that this view of backdoored classifiers is fundamentally in-
correct. We demonstrate that anyone with access to the classifier, even without
access to any original training data or trigger, can construct several alternative
triggers that are as effective or more so at eliciting the target class at test time. We
construct these alternative triggers by first generating adversarial examples for a
smoothed version of the classifier, created with a recent process called Denoised
Smoothing, and then extracting colors or cropped portions of adversarial images.
We demonstrate the effectiveness of our attack through extensive experiments on
ImageNet and TrojAI datasets, including a user study which demonstrates that our
method allows users to easily determine the existence of such backdoors in exist-
ing poisoned classifiers. Furthermore, we demonstrate that our alternative triggers
can in fact look entirely different from the original trigger, highlighting that the
backdoor actually learned by the classifier differs substantially from the trigger
image itself. Thus, we argue that there is no such thing as a “secret” backdoor in
poisoned classifiers: poisoning a classifier invites attacks not just by the party that
possesses the trigger, but from anyone with access to the classifier.
1	Introduction
Backdoor attacks (Gu et al., 2017; Chen et al., 2017; Turner et al., 2019; Saha et al., 2020) have
emerged as a prominent strategy for poisoning classification models. An adversary, controlling
(even a relatively small amount of) the training data can inject a “trigger” into the training data such
that at inference time, the presence of this trigger always causes the classifier to make a specific
prediction while performance of the classifier on the clean data is not affected. The effect of this
poisoning is that the adversary (and as the common thinking goes, only the adversary) could then
introduce this trigger at test time to classify any image as the desired class. Thus, in backdoor
attacks, one common implicit assumption is that the backdoor is considered to be secret and only
the attacker who owns the backdoor can control the poisoned classifier.
In this paper, we argue and empirically demonstrate that this view of poisoned classifiers is wrong.
Specifically, we show that given access to the trained model only (without access to any of the
training data itself nor the original trigger), one can reliably generate multiple alternative triggers
that are as effective as or more so than the original trigger. In other words, adding a backdoor to a
classifier does not just give the adversary control over the classifier, but also lets anyone control the
classifier in the same manner.
Key to our approach is how we construct these alternative triggers. An overview of our attack proce-
dure is depicted in Figure 1. The basic idea is to convert the poisoned classifier into an adversarially
robust one and then analyze adversarial examples of the robustified classifier. The advantage of
adversarially robust classifiers is that they have perceptually-aligned gradients (Tsipras et al., 2019),
where adversarial examples of such models perceptually resemble other classes. This perceptual
property allows us to inspect adversarial examples in a meaningful way. To convert a poisoned clas-
sifier into a robust one, we use a recently proposed technique Denoised Smoothing (Salman et al.,
2020), which applies randomized smoothing (Cohen et al., 2019) to a pretrained classifier prepended
1
Under review as a conference paper at ICLR 2021
Figure 1: Overview of our attack. Given a poisoned classifier, we construct a robustified smoothed
classifier using Denoised Smoothing (Salman et al., 2020). We then extract colors or cropped patches
from adversarial examples of this robust smoothed classifier to construct novel triggers. These
alternative triggers have similar or even higher attack success rate than the original backdoor.
with a denoiser. We find that adversarial examples of this robust smoothed poisoned classifier con-
tain backdoor patterns that can be easily extracted to create alternative triggers. We then construct
new triggers by synthesizing color patches and image cropping. Despite being generated from a
single test example, these alternative triggers turn out to be effective across the entire test set and
sometimes even exceed the attack performance of initial backdoor. Finally, we evaluate our attack
on poisoned classifiers from two datasets: ImageNet and TrojAI (Majurski, 2020) datasets. We
demonstrate that for several commonly-used backdoor poisoning methods, our attack consistently
finds successful alternative triggers. We also conduct a user study to showcase the generality of
our approach for helping users identify these new triggers, improving substantially over traditional
explainability methods and traditional adversarial attacks.
2	Background
This work deals with the broad class of backdoor poisoning attacks, and brings to bear two threads
of work in adversarial robustness to break poisoned classifiers: 1) the fact that robust classifiers have
perceptually-aligned gradients (Tsipras et al., 2019) (i.e., that reveal information about the under-
lying classes); 2) the use of randomized smoothing (Cohen et al., 2019) to build robust classifiers,
with recent work (Salman et al., 2020) showing that one can robustify a pretrained classifier. We
discuss each of these subjects in turn. Then we clarify two points regarding our approach.
Backdoor Attacks In backdoor attacks (Chen et al., 2017; Gu et al., 2017; Li et al., 2019; 2020), an
adversary injects poisoned data into the training set so that at test time, clean images are misclassified
into the target class when the trigger is present. BadNet (Gu et al., 2017) achieve this by modify-
ing a subset of training data with the backdoor trigger and set the labels to the target class. One
drawback of BadNet is that poisoned images are often clearly mislabeled, thus making the poisoned
training data easily detected by human eyes or simple data filtering (Turner et al., 2019). To address
this issue, Clean-label backdoor attack (CLBD) (Turner et al., 2019) and Hidden trigger backdoor
attack (HTBA) (Saha et al., 2020) propose poison generation methods which assign correct labels
to poisoned images. There are also efforts to design defenses against backdoor attacks (Tran et al.,
2018; Wang et al., 2019; Gao et al., 2019; Guo et al., 2020; Wang et al., 2020; Soremekun et al.,
2020). Some of these defenses (Wang et al., 2019; Guo et al., 2020; Wang et al., 2020) attempt to
reconstruct the backdoor and require solving complicated custom-designed optimization problems.
Soremekun et al. (2020) propose a method to detect poisoned classifiers if poisoned classifiers are
also adversarially robust.
Adversarial Robustness Aside from backdoor attacks, another major line of work in adversarial
machine learning focuses on adversarial robustness (Szegedy et al., 2013; Goodfellow et al., 2015;
Madry et al., 2017; Ilyas et al., 2019), which studies the existence of imperceptibly perturbed inputs
that cause misclassification in state-of-the-art classifiers. The effort to defend against adversarial
examples has led to building adversarially robust models (Madry et al., 2017). In addition to being
robust against adversarial examples, adversarially robust models are shown to have perceptually-
aligned gradients (Tsipras et al., 2019; Engstrom et al., 2019): adversarial examples of those clas-
sifiers show salient characteristics of other classes. This property of adversarially robust classifiers
can be used, for example, to perform meaningful image manipulation (Santurkar et al., 2019).
2
Under review as a conference paper at ICLR 2021
Randomized Smoothing Our work is also related to a recently proposed robust certification method:
randomized smoothing (Cohen et al., 2019; Salman et al., 2019). Cohen et al. (2019) show that
smoothing a classifier with Gaussian noise results in a smoothed classifier that is certifiably ro-
bust in l2 norm. Kaur et al. (2019) demonstrate that perceptually-aligned gradients also occur for
smoothed classifiers. Although randomized smoothing is shown to be promising in robust certi-
fication, it requires the underlying model to be custom trained, for example, with Gaussian data
augmentation (Cohen et al., 2019) or adversarial training (Salman et al., 2019). To avoid the tedious
customized training, Salman et al. (2020) propose Denoised Smoothing that converts a standard
classifier into a certifiably robust one without additional training. It achieves this by prepending a
denoiser to a pretrained classifier prior to applying randomized smoothing.
On “defending against” versus “breaking” poisoned classifiers While our focus in this work is
on “breaking backdoored classifiers”, it might be tempting to instead view it as a “defense against
backdoor attacks”. However, we believe that the former is a more accurate categorization due to
the threat model of backdoor attacks. In a typical threat model associated with backdoor attacks, an
attacker will introduce its poisoned data at training time, and the user then is free to perform what-
ever analysis is needed upon the classifier in order to assess its vulnerability before deployment. In
other words, the attacker must “move first” in the game, and the user is free to “move second” to
analyze the classifier; this is in stark contrast to test-time adversarial robustness, where a defender
must “move first” to create a robust classifier, and the attacker is then permitted to create adaptive
adversarial inputs crafted toward that particular classifier. While it is certainly plausible that alter-
native backdoor strategies may prove more difficult to analyze with our approach, the impetus here
is on the attacker rather than the defender to demonstrate this possibility.
On our attack versus adversarial patch attack It may seem odd to claim that backdoored classi-
fiers are “broken” by demonstrating their vulnerability to a patch attack, especially given the well-
known fact that virtually any (non-robust) classifier can be similarly attacked via an adversarial patch
(Brown et al., 2017). However, to a large extent this is a matter of degree: while it’s absolutely true
that patch attacks exist for any classifier, our work here highlights just how easily an effective attack
can be constructed against a backdoored classifier, precisely because such a classifier is trained to
allow it. In contrast, our approach notably will not produce effective triggers against clean classifiers
(See Figure 8 in Section 4); while it would also be possible for an attacker to essentially interpolate
between what qualified as a “backdoor trigger for a poisoned classifier” and an “adversarial patch for
a clean classifier”, the point of this work is to emphasize the degree to which backdoored classifiers
make the task of breaking them easy and remarkably effective.
3	Methodology
In this section, we demonstrate our approach for attacking poisoned classifiers given access to the
poisoned classifier and test data only. We consider the commonly-used threat model (Gu et al.,
2017; Turner et al., 2019; Saha et al., 2020) for poisoned classifiers, where images patched with the
backdoor will be predicted as target class. The attack success rate is defined as the percentage of test
data (not including images from target class) classified into target class when the trigger is applied.
3.1	Generating Perceptually-Aligned Adversarial Examples
We start by discussing the relationship between backdoor attacks and adversarial examples. Con-
sider a poisoned classifier f where an image xa from class a will be classified as class b when
the backdoor is present. Denote the application of the backdoor to image x as B(x). Then for a
poisoned classifier:
f(xa) = a, f(B(xa)) = b	(1)
In addition to being a poisoned image, B(xa) can be viewed as an adversarial example of the
poisoned classifier f. Formally, B(xa) is an adversarial example with perturbation size =
kB(xa) - xakp in lp norm:
B(xa) ∈ {x | f(x) 6= a, kx - xakp ≤ }	(2)
However, this does not necessarily mean that the backdoor will be present in the adversarial ex-
amples of the poisoned classifier. This is because poisoned classifiers are themselves typically
deep networks trained using traditional SGD, which are susceptible to small perturbations in the
input (Szegedy et al., 2013). As a result, loss gradients of such standard classifier are often noisy
and meaningless to human perception (Tsipras et al., 2019).
3
Under review as a conference paper at ICLR 2021
classifiers and a robustified clean classifier. Trigger A and Trigger B are shown in Figure 3.
Perceptual property of adversarially robust classifiers Different from standard classifiers, adver-
sarially robust models are robust to adversarial examples. Recent work (Tsipras et al., 2019; San-
turkar et al., 2019) find that their loss gradients align well with human perception and adversarial
examples of such models show salient characteristics of corresponding misclassified class.
We hope to use this perceptual property to inspect and analyze poisoned classifiers through the lens
of adversarial examples. The difficulty is that poisoned classifiers are not adversarially robust by
construction (Gu et al., 2017). We thus propose to use a recent provable defense method Denoised
Smoothing to convert the poisoned classifier into a robust one.
Robustifying poisoned classifiers Denoised Smoothing (Salman et al., 2020) is built upon random-
ized smoothing (Cohen et al., 2019), a procedure that converts a base classifier f into a smoothed
classifier g under Gaussian noise that is certifiably robust in l2 norm:
g(x) = arg maxP(f(x + δ) = C) where δ 〜N(0,σ2I)	(3)
c
For randomized smoothing to be effective, it usually requires the base classifier f to be trained via
Gaussian data augmentation, which does not hold for poisoned classifiers. Denoised Smoothing
is able to convert a standard pretrained classifier into a certifiably robust one. Denoised Smoothing
first prepends a pretrained classifier f with a custom-trained denoiser D. Then it applies randomized
smoothing to the combined network f ◦ D, resulting in a robust smoothed classifier fsmoothed:
fsmoothed(x) = arg maχ p(f ◦ D(X + δ) = C) where δ 〜N(0, σ2I)	(4)
c
For a poisoned classifier, we use Denoised Smoothing to convert it into a robust smoothed classifier.
We then generate perceptually meaningful adversarial examples of the smoothed classifier, using
the method proposed in Salman et al. (2019). Specifically, we use the smoothadvPGD method
and sample Monte-Carlo noise vectors to estimate gradients of the smoothed classifier. Adversarial
examples are generated with a l2 norm bound . Although randomized smoothing will ultimately add
noise to an image with the backdoor present B(x), denoiser D will remove the noise before feeding
it into the poisoned classifier. Therefore it is expected that backdoor of the poisoned classifier still
applies to the new robust smoothed classifier. In practice, we find that this holds true in general for
the poisoned classifiers we consider.
3.2	Backdoor patterns in Adversarial Examples
Thus, our overall strategy is to analyze the adversarial examples of robustified poisoned classifiers.
Since we assume that users are not aware of the backdoor or which class is being targeted via the
trigger, throughout this paper, unless otherwise specified, we will generate untargeted adversarial
examples (though through these untargeted examples it will become obvious which is the poisoned
class). To illustrate the basic approach, for the purpose of this presentation, we trained binary
4
Under review as a conference paper at ICLR 2021
Clean Image
Figure 4: Backdoor patterns in adversarial examples ( = 20) for robustified poisoned classifiers,
where each poisoned model has a different color trigger.
poisoned classifiers on two ImageNet classes: pandas and airplanes; the target class of the backdoor
is airplane. We used BadNet (Gu et al., 2017) for backdoor poisoning. After training, and without
access to any training data, we then applied Denoised Smoothing to create a robust version of the
classifier.
In Figure 2, we show l2 adversarial panda images ( = 20, 60) of
the robust version of two poisoned classifiers and a clean classi-
fier1. Two backdoor triggers are shown in Figure 3, where Trig-
ger A is a 30 × 30 synthetic trigger with random colors, created
in the backdoor attack method HTBA (Saha et al., 2020) and
Trigger B is a 30 × 30 hello kitty image. The crucial point here
is that for adversarial examples of robustified poisoned classi-
fiers, there are local color regions that are immediately visually
Trigger A
Trigger B
Figure 3: Backdoor triggers used
in our analysis.
apparent when inspecting the adversarial examples. For larger perturbation size ( = 60), these
colors become more saturated despite background noise. While for a clean classifier, such regions
are much less prevalent.
To better understand the relationship between these color regions and the backdoor, we trained
poisoned classifiers with backdoor triggers each consisting of a single, random color2 . Adversarial
examples are shown in Figure 4. Similar to Figure 2, we still observe special color regions, and
those colors are close to the color in the backdoor. This suggests that these local color spots can
provide useful information (i.e., color) of the backdoor trigger. Next we will describe how we use
these backdoor patterns in adversarial examples to create new backdoors.
3.3	Breaking poisoned classifiers
We now describe how to construct alternative triggers that perform just as well as the original one;
this is a largely manual process, but it is typically straightforward in practice. Specifically, we use the
patterns observed in adversarial examples of robustified classifiers, and follow one of two strategies:
1.	We synthesize a patch with colors obtained from the local regions with backdoor patterns.
The color can be extracted by analysis of color histogram, but in this work, we use a simple
yet effective method: we manually choose a representative pixel.
2.	We crop a patch image that contains one of the backdoor patterns.
Note that both means of constructing triggers require human inspection: first select the adversarial
examples that contain a backdoor pattern, then manually construct new triggers. However, the attack
is very straighforward because: 1) backdoor patterns are easy to spot, as shown in Figure 2; 2) pixel
selection and cropping sub-images are very simple operations to perform manually. We apply these
poison triggers to the poisoned classifier as if they are the true backdoor. Surprisingly, we find that
although we create these triggers from only a handful of images, they generalize well to other images
in the test set, attaining high attack success rate. Using the procedure described above (illustrated
in Figure 1), we can easily break a poisoned classifier even if we do not know the original backdoor
trigger.
Since our attack depends on observed backdoor features in adversarial examples, one could argue
that this is caused by the transferability of adversarial patches (Brown et al., 2017), which could be a
general property of all classifiers (i.e., our attack may also work to create an adversarial patch against
1Here we show adversarial examples with clear backdoor patterns. For the binary poisoned classifiers we
investigate, we observe that most of the adversarial examples contain such backdoor patterns.
2For some colors, classifiers are hard to poison (i.e., white and black). We choose those colors that lead to
a high poisoning success rate (> 50%).
5
Under review as a conference paper at ICLR 2021
clean classifiers). To address this point, we also evaluate our attack on clean classifiers (Results are
shown in Section 4) and find that clean classifiers are not broken by our method. Overall, our results
prompt us to rethink backdoor poisoned classifiers. Do backdoored classifiers really require the
secret backdoor to be controlled/manipulated? Our findings show that this is not the case. Not
only can backdoor patterns be leaked through adversarial examples, we can also construct multiple
triggers to attack poisoned classifiers that are just as effective as the original trigger.
3.4	Enhanced visualization techniques
Finally, we discuss two additional techniques to help with visualizing adversarial examples.
Deep Dream We adopt the idea from Deep Dream (Mordvintsev
et al., 2015) by iteratively optimizing a certain objective starting
with the resized output from previous iteration. Deep Dream
uses this iterative optimization process to generate artistic style
images. In our case, we iteratively optimize the adversarial ob-
jective, so that backdoor patterns formed at earlier stages can be
incorporated into those forming at later stages.
Tikh Rliti Si	ti l d Figure 5: Sample adversarial im-
Tikhonov Regularization Since we are generating large- ad-
ages generated with deep dream
versarial examples, adversarial images tend to become noisy.
To reduce background noise, we introduce Tikhonov regulariza- and Tikhonov regularization.
tion (Tikhonov et al., 1992), which minimizes a loss function defined as a l2-regularization of the
magnitude of image gradients (directional change in the intensity of colors).
In Figure 5, we show sample adversarial images obtained with two techniques on top of Denoised
Smoothing for the binary poisoned classifier with Trigger A. Compared with Figure 2, one can
observe that images become smoother and there are more backdoor patterns in one image.
4	Experiments
In this section, we present our attack results on poisoned classifiers from two datasets: Ima-
geNet (Russakovsky et al., 2015) and TrojAI datasets (Majurski, 2020). For Denoised Smoothing,
we use the MSE-trained ImageNet denoiser adopted from Salman et al. (2020). To make backdoor
presence conspicuous, we synthesize large- untargeted adversarial examples ( = 20, 60). The
noise level we use in smoothed classifiers is 1.00, as Kaur et al. (2019) shows that larger noise level
leads to better visual results. We refer the reader to Appendix A for details on the experimental
setup. For both datasets, we construct alternative triggers of size 30 × 30, same as the size of the
backdoor trigger used in ImageNet poisoned classifiers3 4. We apply alternative triggers to random
locations for ImageNet (same as the initial backdoor) and a fixed place near the center for TrojAI
4. For computing the attack success rate of backdoor triggers, on ImageNet, we use 50 images for
binary classifier and 200 images for multi-class classifier in the test set; on TrojAI dataset, we use
the released 500 sample test images for each classifier.
4.1	ImageNet
For ImageNet, we train both binary and multi-class poisoned classifiers with three backdoor attack
methods: BadNet (Gu et al., 2017), Hidden trigger backdoor attack (HTBA) (Saha et al., 2020) and
Clean-label backdoor attack (CLBD) (Turner et al., 2019) (in total 6 poisoned classifiers). The class
of the binary classifier is hand-picked: “panda” vs “airplane”. For the multi-class classifier, 5 classes
are chosen randomly. Since only HTBA has conducted evaluation on ImageNet, we follow its setup
for training poisoned classifiers. Specifically, we adopt Trigger A in Figure 3 as the default trigger
and use AlexNet (Krizhevsky et al., 2012) architecture 5.
Comparison to baselines We compare Denoised Smoothing to two baseline approaches for gen-
erating adversarial examples: adversarial examples of 1) the poisoned classifier (denoted as “Basic
3 In TrojAI, the exact shape of backdoor trigger is not provided. Here we adopt the same setting as ImageNet.
4For TrojAI, we are not aware of where the trigger is applied in the training process of poisoned classifiers.
We choose this location in order for the alternative triggers to be applied at the foreground object (an artificial
sign). (Sample images in https://pages.nist.gov/trojai/docs/data.html)
5Except for CLBD, we use ResNet (He et al., 2016) for the backdoor attack to be successful.
6
Under review as a conference paper at ICLR 2021
Original	Basic Adv	Smoothing	DenoiSed Smoothing
Figure 6: Comparison of different forms of adversarial examples ( = 20) from a binary poisoned
classifier on ImageNet.
Clean	Adversarial (e = 20)
Adversarial (e = 60)
Color patch
Success rate:
72.70%
Cropped patch
Success rate:
65.90%
Clean
Adversarial (e = 20)
Color patch
Success rate:
72.50%
Cropped patch
Success rate:
73.60%
Color patch
Success rate:
75.90%
Cropped patch
Success rate:
66.45%
Clean
Adversarial (e = 60)
Color patch
Success rate:
85.80%
Cropped patch
Success rate:
89.20%
B
Figure 7: Results for attacking a robustified poisoned multi-class classifier obtained through Bad-
Net (Gu et al., 2017). The attack success rate of the original backdoor Trigger A is 72.60%. The
region which we use to construct alternative triggers is highlighted in a red box.
Adv”); 2) the smoothed poisoned classifier without a denoiser (denoted as “Smoothing”). We gener-
ate adversarial examples ( = 20) of the robustified binary poisoned classifier on ImageNet, visual-
ized in Figure 6 (More examples are shown in Figure 17 in Appendix C.). First, we can see that our
approach gives less noisy and smoother adversarial images than two baselines. Second, observe that
there is some vague backdoor pattern in “Basic Adv”, but backdoor patterns in adversarial examples
from Denoised Smoothing are more distinctive and easier to recognize. Last, “Smoothing” baseline
does not produce any obvious pattern, which highlights the necessity of Denoised Smoothing.
BadNet	HTBA	CLBD
-Binary	98.80%∕91.60%^^99.80%∕94.00%^^93.80%∕90.00%
Multi-class 89.20%∕72.60%	82.30%∕74.55%	67.90%∕58.95%
Table 1: Overall performance of our attack. For “X/Y”, X is the highest attack success rate among
the triggers that we demonstrate in this paper and Y is the success rate of the original backdoor.
Breaking poisoned classifiers In Figure 7, we present sample alternative backdoor triggers we con-
structed by attacking a BadNet poisoned multi-class classifier on ImageNet, where we show both
color patch and cropped patch constructed from each adversarial example. For attack results on other
five ImageNet poisoned classifiers, we refer the reader to Figure 11 and Figure 12 in Appendix B.
From Figure 7, we can see that all the alternative triggers created from backdoor patterns all have
relatively high success rate. In particular, two triggers achieve significantly higher attack success
rate (89.20%, 85.80%) than the original backdoor Trigger A (72.60%). Also notice that these alter-
native triggers differ greatly from Trigger A. Last, we can see that whether color patch or cropped
patch perform better depends on each example. In terms of epsilon, it can be seen that larger epsilon
leads to better attack results. A summary of attack results for all poisoned classifiers is presented
in Table 1. For each poisoned classifier, we compare the highest success rate achieved by the alter-
native triggers we demonstrate in the paper and the success rate of the initial backdoor (Trigger A).
For all six poisoned classifiers we investigate, our attack finds an alternative trigger more effective
than the original backdoor. Also, for five of the six poisoned classifiers, the highest success rate
shown in Table 1 is attained by cropped patch, which may suggest that cropped patch may be more
effective overall.
7
Under review as a conference paper at ICLR 2021
Clean
Clean
Adversarial (e = 60)
Adversarial (e = 60)
Color patch
Errorrate:
1.84%
Cropped patch
Error rate:
8.04%
Clean	Adversarial (e = 60)
Color patch
Error rate:
1.48%
Cropped patch
Error rate:
0.96%
Color patch
Error rate:
1.24%
Cropped patch
Error rate:
2.28%
Clean
Adversarial (€ = 60)
Color patch
□
1.56%
Cropped patch
Error rate:
1.64%
Success rate:
75.80%
Figure 8: Results of applying our attack on an ImageNet clean classifier.
Clean
€ = 20	e = 60	W = 100
(a) Adversarial examples of a robustified poisoned classifier with Trigger C as the backdoor.
Clean	Adversarial (e = 60)
CroppedpatCh
Successrate:
88.60%
Clean
Adversarial (e = 60)
CroppedpatCh
Successrate:
83.00%
(b) Attacking a poisoned classifier with the “camouflaged” backdoor Trigger C (success rate 75.80%).
Figure 9: Analysis of a poisoned classifier with a “camouflaged” backdoor trigger.
Clean classifiers are not easily broken. In addition, we show that clean classifiers are not broken
under our approach. We apply our attack method to ImageNet clean classifiers. However, clean
classifiers are not poisoned, then there is no such concept as attack success rate for clean classifiers.
To measure the effect of the triggers constructed by our procedure on clean classifiers, we report
the error rate of clean classifiers when the test data is patched by the alternative triggers. Figure 8
presents an illustration for attacking a clean multi-class ImageNet classifier. We refer the reader to
Figure 13 in Appendix B for results on attacking the binary ImageNet classifier. Here we choose
larger perturbation size = 60 because we find no obvious pattern with perturbation size = 20.
Observe that clean classifiers have low error rates under test data patched by the alternative triggers.
Therefore, clean classifiers are not easily attacked by our approach.
“Camouflaged” Backdoor So far we have experimented with triggers that contain colors (i.e.,
red, blue in Trigger A) that are visually distinctive and as a result, backdoor patterns can be easily
recognizable in adversarial examples. We study the case when backdoor trigger is less colorful or
contains colors already in the color distribution of clean images. Consider Trigger C in Figure 9a:
black and white colors in this trigger are also representative colors of a panda. We train a poisoned
binary classifier on ImageNet using Trigger C as the backdoor, where the backdoor attack method is
BadNet (Gu et al., 2017). In Figure 9a, we visualize adversarial examples of the robustified poisoned
classifier. Although there is no clear backdoor pattern in the form of dense color regions, we can
observe that in the generated adversarial examples, there is a tendency for black regions to have
vertical or horizontal boundaries, which resembles the pattern in Trigger C. Despite the absence of
obvious backdoor patterns, we are still able to break the poisoned classifier using cropped patterns
from large- ( = 100) adversarial examples as shown in Figure 9b. Notice that both of the triggers
8
Under review as a conference paper at ICLR 2021
Clean
Adversarial (ε = 20)
Success rate: 87.25%
Cropped Trigger
(a) Poisoned Classifier 1
Color Trigger
Success rate: 100%
Clean	Adversarial (e = 20)
Success rate: 59.25%
Cropped Trigger
(b) Poisoned Classifier 2
Figure 10: Results of attacking two poisoned classifiers in TrojAI dataset.
Color Trigger
Success rate: 100%
Denoised Smoothing	Basic AdV
Participants participant 1 participant 2 participant 3 participant 4
Accuracy 94%	90%	66%	82%
Saliency Map
participant 5
54%
Table 2: Accuracies that participants obtained for identifying poisoned classifiers in the user study.
are noisy and seem completely different from Trigger C, but they attain higher attack success rate
(88.60% and 83.00%) than the original backdoor (75.80%).
4.2 TrojAI dataset
We eValuate our attack on the TrojAI dataset (Majurski, 2020), consisting of a mixed set of clean
and poisoned classifiers. TrojAI dataset is initially proposed as a dataset to help deVelop backdoor
detection methods. Here we choose this dataset as it contains a large set of trained poisoned classi-
fiers. Different from ImageNet, we are not aware of the exact backdoor triggers used to poison the
classifiers. In Figure 10, we show attack results on two poisoned classifiers. Poisoned classifiers are
chosen from Round 0 of the TrojAI dataset. As shown in Figure 10, our methods can attack these
poisoned classifiers with high success rate (See Figure 14 in Appendix B for results on more poi-
soned classifiers.). Similarly, the cropped trigger achieVes higher success rate than the color trigger
for both classifiers. Especially, notice that both cropped triggers attains 100% attack success rate.
Finally, we conduct a user study on the TrojAI dataset to test the generality of our approach. We
deVelop an interactiVe tool implementing our method to aid the study. Participants are asked to
analyze classifiers with the tool and decide if they are poisoned. Two control groups are used:
1) participants are giVen a Variant of the tool using adVersarial examples of the original classifier
(denoted as “Basic AdV”); 2) participants are giVen saliency maps on clean images (denoted as
“Saliency Map”). Details on the user study and the interactiVe tool are in Appendix D. Results are
summarized in Table 2, where we show the accuracies of identifying poisoned classifiers for three
approaches. OVerall, the study suggests that analysts with access to our tool are able to substantially
outperform those using alternatiVe methods.
5 Conclusion
This work shows that backdoor attacks create poisoned classifiers that can be easily attacked eVen
without knowledge of the original backdoor. We find that adVersarial examples of a robustified
poisoned classifier usually contain backdoor patterns. We then construct new poison triggers from
the backdoor presence in adVersarial examples and show that they giVe comparable or eVen better
attack performance than the original backdoor.
Our findings urge the research community to rethink the current threat model in backdoor poisoning.
It remains to be seen if there exist backdoor attacks that aVoid our attack. Our results also raise the
question of what is actually learned through the backdoor poisoning process. It seems that backdoor
poisoning creates a spectrum of potential backdoors, in addition to the original one. Thus, a rigorous
analysis of the implicit effect of backdoor poisoning is needed. More broadly, the idea of robustifying
(poisoned) classifiers can be a principled approach for analyzing general standard classifiers.
References
Tom B. Brown, Dandelion Mane, Aurko Roy, Mart´n Abadi, and Justin Gilmer. Adversarial patch.
arXiv preprint arXiv:1712.09665, 2017.
9
Under review as a conference paper at ICLR 2021
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via random-
ized smoothing. ICML, 2019.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Alek-
sander Madry. Adversarial robustness as a prior for learned representations. arXiv preprint
arXiv:1906.00945, 2019.
Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C. Ranasinghe, and Surya Nepal.
Strip: A defence against trojan attacks on deep neural networks. arXiv preprint arXiv:1902.06531,
2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2015.
Tianyu Gu, Dolan-Gavitt Brendan, and Siddharth Garg. Badnets: Identifying vulnerabilities in the
machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.
Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song. Tabor: A highly accurate approach
to inspecting and restoring trojan backdoors in ai systems. ICDM, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CVPR, 2016.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. NeurIPS, 2019.
Simran Kaur, Jeremy Cohen, and Zachary C. Lipton. Are perceptually-aligned gradients a general
property of robust classifiers? arXiv preprint arXiv:1910.08640, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, 2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible
backdoor attacks on deep neural networks via steganography and regularization. arXiv preprint
arXiv:1909.02742, 2019.
Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. Rethinking the
trigger of backdoor attack. arXiv preprint arXiv:2004.04692, 2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Michael Paul Majurski. Challenge round 0 (dry run) test dataset, 2020. URL https://data.
nist.gov/od/id/mds2-2175.
Tulio Ribeiro Marco, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. KDD, 2016.
Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going
deeper into neural networks, 2015. URL https://ai.googleblog.com/2015/06/
inceptionism-going-deeper-into-neural.html.
Vitali Petsiuk, Abir Das, and Saenko Saenko. Rise: Randomized input sampling for explanation of
black-box models. arXiv preprint arXiv:1806.07421, 2018.
R Selvarajk Ramprasaath, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. ICCV, 2017.
10
Under review as a conference paper at ICLR 2021
Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: Nonlinear Phenomena, 1992.
Olga Russakovsky, Deng Jia, Su Hao, Krause Jonathan, Satheesh Sanjeev, Ma Sean, Huang Zhiheng,
Karpathy Andrej, Khosla Aditya, Michael Bernstein, Berg Alexander C., and Fei-Fei Li. Imagenet
large scale visual recognition challenge. International Journal of Computer Vision (IJCV)., 2015.
Aniruddha Saha, Akshayvarun Subraymanya, and Pirsiavash Hamed. Hidden trigger backdoor at-
tacks. AAAI, 2020.
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya Razenshteyn, and Sebastien
Bubeck. Provably robust deep learning via adversarially trained smoothed classifiers. NeurIPS,
2019.
Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J. Zico Kolter. Denoised smoothing: A
provable defense for pretrained classifiers. NeurIPS, 2020.
Shibani Santurkar, Dimitris Tsipras, Brandon Tran, Andrew Ilyas, Logan Engstrom, and Aleksander
Madry. Image synthesis with a single (robust) classifier. NeurIPS, 2019.
Ezekiel Soremekun, Sakshi Udeshi, and Sudipta Chattopadhyay. Exposing backdoors in robust
machine learning models. arXiv preprint arXiv:2003.00865, 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
A. N. Tikhonov, A. S. Leonov, and A. G. Yagola. Nonlinear ill-posed problems. World Congress of
Nonlinear Analysts, 1992.
Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. NeurIPS,
2018.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. ICLR, 2019.
Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks, 2019.
URL https://openreview.net/forum?id=HJg6e2CcK7.
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y.
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. IEEE
Symposium on Security and Privacy, 2019.
Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. Practical
detection of trojan neural networks: Data-limited and data-free cases. ECCV, 2020.
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a Gaussian denoiser:
Residual learning of deep CNN for image denoising. IEEE Transactions on Image Processing,
26(7):3142-3155, 2017.
11
Under review as a conference paper at ICLR 2021
Appendices
A Experimental details
A. 1 Training details
We follow the experiment setting in HTBA (Saha et al., 2020), with publicly available code-
base https://github.com/UMBCvision/Hidden-Trigger-Backdoor-Attacks.
HTBA divides each class of ImageNet data into three sets: 200 images for generating poisoned data,
800 images for training the classifier and 100 images for testing. The trigger is applied to random
locations on clean images. Poisoned datasets are first constructed with corresponding backdoor
attack methods. Then we fine-tune the last fully-connected layer of pretrained AlexNet (Krizhevsky
et al., 2012) on the created poisoned datasets. The fine-tuning process starts with initial learning
rate of 0.001 decayed by 0.1 every 10 epochs and in total takes 10/30 epochs. The number of
poisons are 400 images except for BadNet poisoned multi-class classifier, where we find that 1000
poisons are required to achieve high backdoor attack success rate.
We implement the method of CLBD (Turner et al., 2019) utilizing adversarial examples on Ima-
geNet. We find that training poisoned classifiers with CLBD is difficult on ImageNet if we follow
the exact steps described in Turner et al. (2019). We find that we are able to successfully train poi-
soned ResNets (He et al., 2016) by initializing the classifiers with adversarially robust classifiers
that are used to generate poisoned data in CLBD. We train adversarially robust classifiers for both
binary classification and multi-class classification. For training binary poisoned classifiers, we use
400 adversarial images with perturbation size = 32 in l2 norm as poisoned data. For training
multi-class poisoned classifier, we use 400 adversarial images with = 8 in l2 norm as poisoned
data.
A.2 Computing Adversarial example
In our attack, we need to compute adversarial examples of a smoothed classifier. To achieve
this, we optimize the SMOOTHADV objective (Salman et al., 2019) with projected gradient de-
scent (PGD) (Madry et al., 2017; Kurakin et al., 2016). The code for attacking smoothed
classifier is adopted from public available codebase https://github.com/Hadisalman/
smoothing-adversarial. Denoiser model is an ImageNet DnCNN (Zhang et al., 2017) de-
noiser trained with MSE loss, adopted from the public codebase of Denoised Smoothing in https:
//github.com/microsoft/denoised-smoothing.
All adversarial examples are computed by untargeted adversarial attacks with a l2 norm bound . We
use 16 Monte-Carlo noise vectors to estimate gradients of smoothed classifiers. The number of PGD
steps is 100. Step size at each iteration is 2×(perturbation size ) / (# of steps). Except for attacking
the poisoned classifier with “camouflaged” backdoor in Figure 9b, where we find that in this case,
larger step size leads to slightly better visual results, thus we set step size to be 5 in Figure 9b.
Deep Dream We optimize the adversarial objective with Deep Dream framework adopting
the implementation from public codebase https://github.com/eriklindernoren/
PyTorch-Deep-Dream. We perform 4 iterations, scaling the image by 1.2 every iteration. Due
to the large memory requirements of Deep Dream, we use 5 Monte-Carlo noise vectors to estimate
gradients. At each iteration, we use 100 steps with step size 5.
Regularization We apply Tikhonov regularization to minimize the l2 norm of image gradients of
adversarial perturbations. The image gradient is computed by the filter F in Equation 5. We also
experimented with another well-studied denoising objective Total Variation (TV) loss (Rudin et al.,
1992), which minimizes the distance between neighboring pixels. TV loss can be seen as a special
case of Tikhonov regularization with a specific filter. Comparison of two regularization techniques
is shown in Figure 19.
F
2
2
-1
-1
2
2
-1
-1
-1
-1
0
0
-1
-1
0
0
(5)
12
Under review as a conference paper at ICLR 2021
B Additional Attack Results
B.1	ImageNet B inary Poisoned classifier
Here we show the complete results for attacking binary poisoned classifiers on ImageNet in Fig-
ure 11. Notice that we find effective alternative triggers for all three poisoned classifiers.
Clean
Clean
Adversarial (€ = 20)
Color patch
Successrate:
64.60%
Cropped patch
Success rate:
91.80%
Clean
Adversarial (e = 60)
Color patch
Successrate:
94.60%
Cropped patch
Success rate:
94.00%
Clean
Adversarial (e = 60)
Color patch
Success rate:
94.80%
Cropped patch
Success rate:
98.80%
(a)	Results for attacking a robustified binary poisoned classifier obtained through BadNet (Gu et al.,
2017). The attack success rate of the original backdoor Trigger A is 91.60%.
Clean
Adversarial (e = 20) Color patch
Successrate:
80.80%
Successrate:
97.40%
Cropped patch
Clean
Adversarial (c = 60)
Color patch
Success rate:
98.40%
Cropped patch
Success rate:
99.80%
Clean
AdverSarial(e =
Color patch
Successrate:
97.00%
Cropped patch
Success rate:
96.80%
(b)	Results for attacking a robustified binary poisoned classifier obtained through HTBA (Saha et al.,
2020). The attack success rate of the original backdoor Trigger A is 94.00%.
Clean
Adversarial (e = 60)
Color patch
Successrate:
82.80%
Cropped patch
Success rate:
93.80%
Clean
Adversarial (e = 60)
Color patch
Successrate:
87.60%
Cropped patch
Successrate:
73.80%
Clean
Adversarial (ε = 60)
Color patch
Success rate:
71.20%
Cropped patch
Success rate:
89.20%
Clean	Adversarial (e = 60) Color patch
Success rate:
91.80%
Success rate:
65.80%
Cropped patch
(c)	Results for attacking a robustified binary poisoned classifier obtained through CLBD (Turner et al.,
2019). The attack success rate of the original backdoor Trigger A is 90.00%.
Figure 11:	Results for attacking three binary poisoned classifiers obtained by three backdoor attacks.
13
Under review as a conference paper at ICLR 2021
B.2 ImageNet multi-class poisoned classifier
In Figure 12, we present the results for attacking two poisoned multi-class classifiers on ImageNet
obtained by HTBA (Saha et al., 2020) and CLBD (Turner et al., 2019). We can see that our attack
constructs effective triggers in both cases.
Clean	Adversarial (e = 20) CoIor patch
Success rate:
35.75%
Cropped patch
Clean
Adversarial (e = 20)
Color patch
Successrate:
36.40%
Cropped patch
Success rate:
67.70%
Clean
Adversarial (e = 60)
Success rate:
72.35%
Color patch
Successrate:
82.30%
Croppedpatch
Success rate:
67.90%
Clean
Adversarial (ε = 60)
Color patch
Successrate:
73.90%
Cropped patch
Success rate:
74.25%
(a)	Results for attacking a robustified multi-class poisoned classifiers obtained through HTBA (Saha
et al., 2020). The attack success rate of the original backdoor Trigger A is 74.55%.
Clean
Clean
Adversarial (e = 60)
Color patch
Success rate:
44.50%
Cropped patch
Success rate:
57.55%
Clean
AdVerSarial(e = 60)
Color patch
Success rate:
48.90%
Cropped patch
Success rate:
67.90%
Clean
Adversarial (e = 60)
Color patch
Success rate:
43.25%
Cropped patch
Success rate:
64.90%
(b)	Results for attacking a robustified binary poisoned classifiers obtained through CLBD (Turner
et al., 2019). The attack success rate of the original backdoor Trigger A is 58.95%.
Figure 12:	Results for attacking multi-class poisoned classifiers on ImageNet obtained by
HTBA (Saha et al., 2020) and CLBD (Turner et al., 2019).
14
Under review as a conference paper at ICLR 2021
B.3	ImageNet binary clean classifier
In Figure 13, we show the results of attacking a clean binary ImageNet classifier. We can see that
the clean classifier is not vulnerable to the triggers constructed by our approach.
Clean	Adversarial (e = 60)
Clean
Adversarial (e = 60)
Color patch
Error rate:
1.30%
Cropped patch
Error rate:
1.10%
Error rate:
1.10%
Cropped patch
Color patch
Error rate:
1.00%
Clean	Adversarial (e = 60)
Figure 13: Results of applying our attack on an ImageNet clean classifier (binary).
Color patch
Error rate:
1.00%
Cropped patch
Error rate:
1.00%
Clean	Adversarial (e = 60)
Color patch
Error rate:
1.00%
Cropped patch
Error rate:
1.00%
15
Under review as a conference paper at ICLR 2021
B.4	TROJAI
In Figure 14, we show results for attacking poisoned classifiers in the TrojAI dataset. Note that for
all 8 poisoned classifiers, the highest attack success rate attained among four alternative triggers is
100%. In Figure 15, we show the results of applying our attack method to two clean classifiers from
TrojAI datasets. It can be seen that clean classifiers can classify more than half of the test images
correctly even if they are patched by the constructed triggers.
Clean	Adversarial (e = 20)
Success rate: 31.5%
(a) Poisoned Classifier 3
Color Trigger
Cropped Trigger
Success rate: 83.5%
Clean	Adversarial (e = 20)
Success rate: 56.75%
Cropped Trigger
(b) Poisoned Classifier 4
Color Trigger
Success rate: 100%
(c) Poisoned Classifier 5
Clean	Adversarial (e = 20) Color Trigger
Success rate: 97.50%
(d) Poisoned Classifier 6
Success rate: 99.50%
Cropped Trigger
Clean	Adversarial (e = 20)
Clean
Adversarial (e = 20)
Color Trigger
Success rate: 96.75%
Success rate: 81.75%
Cropped Trigger
Success rate: 93.75%
Color Trigger
Cropped Trigger
Success rate: 100%
(e)	Poisoned Classifier 7
(f)	Poisoned Classifier 8
Clean	Adversarial (e = 20)
Clean
Adversarial (e = 20)
Color Trigger
Cropped Trigger
Success rate: 100%
Success rate: 89.75%
Success rate: 99.50%
Color Trigger
Cropped Trigger
Success rate: 99.75%
(g)	Poisoned Classifier 9
(h)	Poisoned Classifier 10
Figure 14: Results of attacking 8 poisoned classifiers in the TrojAI dataset.
(a) Clean Classifier 1
(b) Clean Classifier 2
Figure 15:	Results of attacking two clean classifiers in the TrojAI dataset.
16
Under review as a conference paper at ICLR 2021
C Additional Visualization Results
C.1 Adversarial examples on TrojAI dataset
Figure 16 presents the adversarial examples of a robustified poisoned classifier from the TrojAI
dataset, where each row shows images from one class. Below each image we show the class pre-
dicted by the poisoned classifier (not the smoothed classifier). We highlight those adversarial images
with clear backdoor patterns. Note that they are all classified into class 2, which is indeed the target
class of backdoor attack. While adversarial images from class 4 (the last row) have dense black
regions, we believe that this is a result of mimicking features of class 0 (the class that these images
are predicted into) and it can be easily tested using our method that these black regions can not be
used to construct successful triggers.
4
4	4	4	4
0
0	0	0	0
Figure 16:	Adversarial examples ( = 20 in l2 norm) of a robustified poisoned classifier in the
TrojAI dataset. Below each image is the class predicted by the original poisoned classifier.
17
Under review as a conference paper at ICLR 2021
C.2 Comparison of different adversarial examples
Figure 17	shows more results on comparing different adversarial examples ( = 20).
Original
Basic Adv	Smoothing	DenoiSed Smoothing
Figure 17: Comparison of different adversarial examples ( = 20) of a robustified binary poisoned
classifier on ImageNet.
18
Under review as a conference paper at ICLR 2021
C.3 Enhanced Visualization techniques
C.3.1 DEEPDREAM
Figure 18	shows the comparison of adversarial images with or without enhanced visualization tech-
niques discussed in subsection 3.4. We can see that for Deep Dream, there are more backdoor pat-
terns in a single adversarial image than Denoised Smoothing. Together with Tikhonov regularization
method, the backdoor patterns become more stable and less noisy.
Denoised Smoothin
Deep Dream
Deep Dream + Regularization
Figure 18:	Effects of enhanced visualization techniques on adversarial examples of a robustified
ImageNet binary poisoned classifier.
19
Under review as a conference paper at ICLR 2021
C.3.2 Regularization
In Figure 19, We show how regularization can be used to reduce background noise in large-e adver-
sarial examples. We generate adversarial images with e = 60. For Denoised Smoothing, we see that
there is some background noise. For both regularization techniques, we see that adversarial images
are less distorted and there are less noise patterns.
Original
DenoiSed Smoothing
DenoiSed Smoothing +
TV Loss
Denoised Smoothing +
Tikhonov regularization
Figure 19:	Comparison of adversarial examples generated with/without regularization.
20
Under review as a conference paper at ICLR 2021
D User study
D.1 TrojAI interactive tool
In Figure 20, we show a brief overview of the interactive tool which implements our attack method.
The first half of the tool, as shown in Figure 20a, allows users to visualize adversarial examples with
chosen attack parameters. Below each image is the class that the adversarial image is predicted.
Figure 20b presents the second half of the tool, where users can create new alternative patch triggers
and see the classifier’s prediction on patched poisoned images.
(a) First half of the interactive tool.
Clean Image	Color Selector	Shape Selector	Apply	Predict
(b) Second half of the interactive tool.
Figure 20: Interface of interactive tool we develop for TrojAI dataset.
21
Under review as a conference paper at ICLR 2021
D.2 Details on User Study
We describe our setup for user study in detail. 5 people joined the study. We divide them into
three groups: 2 people for Denoised Smoothing, 2 people for the control group “Basic Adv” and
1 person for the control group “Saliency Map”. For all three groups, participants are asked to
mark 50 classifiers as either poisoned or clean. For Denoised Smoothing and “Basic Adv”, we
ask participants to apply our attack method with the interactive tool and test if the model can be
successfully attacked by alternative triggers. If so, then mark the classifier as poisoned. For the
control group “Saliency Map”, Figure 21 shows some sample saliency maps of a poisoned classifier.
We use RISE (Petsiuk et al., 2018) to generate saliency maps, as it is shown to outperform other
saliency map approaches (Ramprasaath et al., 2017; Marco et al., 2016). For this control group,
participants are given the ground-truth labels (poisoned/clean) and saliency maps for 10 classifiers
and then try to mark the 50 unlabelled classifiers based on the provided information from 10 labelled
classifiers.
Figure 21: Sample saliency maps of a poisoned classifier on clean images.
22
Under review as a conference paper at ICLR 2021
E The impact of trigger locations on backdoor patterns
In this part, we investigate the effect of trigger locations during training on the backdoor patterns
in adversarial examples. Specifically, we apply the triggers to fixed image locations (center, lower
left, upper left, lower right, upper right ) during training. We use BadNet (Gu et al., 2017) to
train poisoned classifiers with Trigger A. Adversarial examples of robustified poisoned classifiers
are shown in Figure 22. It can be seen that trigger locations do not affect the backdoor patterns in
adversarial examples.
Clean
Center
Lower Left
Upper Left
Lower Right
Upper Right
Figure 22: Adversarial examples of robustified poisoned classifiers with different fixed trigger loca-
tions during training.
23
Under review as a conference paper at ICLR 2021
F ImageNet classifiers with more classes
In this section, we evaluate our method on ImageNet classifier with more number of classes. We
randomly select 10 classes from 1000 ImageNet classes. We then use BadNet (Gu et al., 2017) to
train a poisoned classifier with Trigger A. Figure 23 shows the results for attacking this poisoned
classifier. We can observe that these alternative triggers have similar or even higher attack success
rate than the original trigger.
Clean
Color patch
Clean	Adversarial (e = 60)
Color patch
Success rate:
70.78%
Cropped patch
Success rate:
55.27%
Clean
Adversarial (e = 60)
Successrate:
59.44%
Cropped patch
Success rate:
53.71%
Clean
Adversarial (e = 60)
Color patch
Successrate:
52.89%
Cropped patch
Success rate:
48.22%
Figure 23: Results of attacking a poisoned ImageNet classifier with 10 classes. The success rate of
the original backdoor is 59.71%.
24