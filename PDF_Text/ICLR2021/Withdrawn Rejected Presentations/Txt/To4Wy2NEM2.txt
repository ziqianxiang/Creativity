Under review as a conference paper at ICLR 2021
Adaptive Optimizers with Sparse Group Lasso
Anonymous authors
Paper under double-blind review
Ab stract
We develop a novel framework that adds the regularizers to a family of adaptive
optimizers in deep learning, such as Momentum, Adagrad, Adam, AMS-
Grad, AdaHessian, and create a new class of optimizers, which are named
Group Momentum, Group Adagrad, Group Adam, Group AMSGrad
and Group AdaHessian, etc., accordingly. We establish theoretically proven
convergence guarantees in the stochastic convex settings, based on primal-dual
methods. We evaluate the regularized effect of our new optimizers on three large-
scale real-world ad click datasets with state-of-the-art deep learning models. The
experimental results reveal that compared with the original optimizers with the
post-processing procedure which use the magnitude pruning method, the perfor-
mance of the models can be significantly improved on the same sparsity level.
Furthermore, in comparison to the cases without magnitude pruning, our methods
can achieve extremely high sparsity with significantly better or highly competitive
performance.
1	Introduction
With the development of deep learning, deep neural network (DNN) models have been widely
used in various machine learning scenarios such as search, recommendation and advertisement,
and achieved significant improvements. In the last decades, different kinds of optimization methods
based on the variations of stochastic gradient descent (SGD) have been invented for training DNN
models. However, most optimizers cannot directly produce sparsity which has been proven effec-
tive and efficient for saving computational resource and improving model performance especially
in the scenarios of very high-dimensional data. Meanwhile, the simple rounding approach is very
unreliable due to the inherent low accuracy of these optimizers.
In this paper, we develop a new class of optimization methods, that adds the regularizers especially
sparse group lasso to prevalent adaptive optimizers, and retains the characteristics of the respective
optimizers. Compared with the original optimizers with the post-processing procedure which use
the magnitude pruning method, the performance of the models can be significantly improved on
the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, the
new optimizers can achieve extremely high sparsity with significantly better or highly competitive
performance. In this section, we describe the two types of optimization methods, and explain the
motivation of our work.
1.1	Adaptive Optimization Methods
Due to the simplicity and effectiveness, adaptive optimization methods (Robbins & Monro, 1951;
Polyak, 1964; Duchi et al., 2011; Zeiler, 2012; Kingma & Ba, 2015; Reddi et al., 2018; Yao et al.,
2020) have become the de-facto algorithms used in deep learning. There are multiple variants, but
they can be represented using the general update formula (Reddi et al., 2018):
xt+ι = Xt - αtmt/vzVt,	(1)
where αt is the step size, mt is the first moment term which is the weighted average of gradient
gt and Vt is the so called second moment term that adjusts updated velocity of variable xt in each
direction. Here, √Vt := Vt1/2, mt∕√Vt := √Vt ∙ mt. By setting different mt, Vt and at , We
can derive different adaptive optimizers including Momentum (Polyak, 1964), Adagrad (Duchi
et al., 2011), ADAM (Kingma & Ba, 2015), AMSGRAD (Reddi et al., 2018) and ADAHESSIAN
(Yao et al., 2020), etc. See Table 1.
1
Under review as a conference paper at ICLR 2021
Table 1: Adaptive optimizers with choosing different mt, Vt and α>
Optimizer	mt	Vt		αt
Sgd	gt		I	α √t
Momentum	γmt-1 + gt		I	α
Adagrad	gt		diag(Pit=1 gi2)/t	α √
Adam	β1mt-1 + (1	- β1 )gt	β2Vt-1 + (1 - β2)diag(gt2)	α √1-βt 1-β1
AMSGRAD	β1mt-1 + (1	- β1 )gt	max(Vt-1, β2Vt-1 + (1 - β2)diag(gt2))	α √1-βt 1-β1
AdaHessian	β1mt-1 + (1	- β1 )gt	β2Vt-1 + (1 -β2)Dt2 *	α √1-βt
				1-β1
* Dt = diag(Ht), where Ht is the Hessian matrix.
1.2 Regularized Optimization Methods
Follow-the-regularized-leader (FTRL) (McMahan & Streeter, 2010; McMahan et al., 2013) has
been widely used in click-through rates (CTR) prediction problems, which adds '1-regularization
(lasso) to logistic regression and can effectively balance the performance of the model and the spar-
sity of features. The update formula (McMahan et al., 2013) is:
1t
xt+1 = arg min gi：t ∙ X + 5 £。§|归一Xsk2 + λι∣∣x∣∣ι,	(2)
x	2 s=1
where gi：t = P S=ιgs, 11 PS=ι。§ ∣∣x - x§k2 is the strong convex term that stabilizes the algorithm
and λ1kxk1 is the regularization term that produces sparsity. However, it doesn’t work well in DNN
models since one input feature can correspond to multiple weights and lasso only can make single
weight zero hence can’t effectively delete zeros features.
To solve above problem, Ni et al. (2019) adds the '21-regularization (group lasso) to FTRL, which is
named G-FTRL. Yang et al. (2010) conducts the research on a group lasso method for online learning
that adds '21-regularization to the algorithm of Dual Averaging (DA) (Nesterov, 2009), which is
named DA-GL. Even so, these two methods cannot been applied to other optimizers. Different
scenarios are suitable for different optimizers in the deep learning fields. For example, Momentum
(Polyak, 1964) is typically used in computer vision; Adam (Kingma & Ba, 2015) is used for training
transformer models for natural language processing; and Adagrad (Duchi et al., 2011) is used for
recommendation systems. If we want to produce sparsity of the model in some scenario, we have to
change optimizer which probably influence the performance of the model.
1.3 Motivation
Eq. (1) can be rewritten into this form:
1 .. T,	…C
xt+1 = argmin mt ∙ X + 77—∣ VZVt (X — xt)∣∣1.	(3)
x	2αt
Furthermore, we can rewrite Eq. (3) into
t 1	1
Xt+1 = arg min mi：t ∙ X + Y' τς—kQ2 (X - Xs)k2,	(4)
x	2αs
s=1
where m±t = P：=1 m§, P：=1 Qs∕α∙s = √Vt∕αt. It is easy to prove that Eq. (3) and Eq. (4)
are equivalent using the method of induction. The matrices Qs can be interpreted as generalized
learning rates. To our best knowledge, Vt of Eq. (1) of all the adaptive optimization methods are
diagonal for the computation simplicity. Therefore, we consider Qs as diagonal matrices throughout
this paper.
We find that Eq. (4) is similar to Eq. (2) except for the regularization term. Therefore, we add
the regularization term Ψ(X) to Eq. (4), which is the sparse group lasso penalty also including '2-
2
Under review as a conference paper at ICLR 2021
regularization that can diffuse weights of neural networks. The concrete formula is:
G
Ψt(x) = X (λιkxgkι + λ21PdXg∣∣A2Xgk2) + λ2kx∣∣2,	(5)
g=1
where λ1, λ21, λ2 are regularization parameters of `1, `21, `2 respectively, G is the total number of
groups of weights, xg is the weights of group g and dxg is the size of group g. In DNN models,
each group is defined as the set of outgoing weights from a unit which can be an input feature, or a
hidden neuron, or a bias unit (see, e.g., Scardapane et al. (2016)). At can be arbitrary positive matrix
satisfying At+1 占 At, e.g., At = I. In Section 2.1, We let At = (PS=ι 等 + λ2I) just for solving
the closed-form solution directly, where Qsg is a diagonal matrix whose diagonal elements are part
of Qs corresponding to xg . The ultimate update formula is:
J 1 .. 1 ,	…C _ .、
Xt+1 = arg min mi：t ∙ x + 兄 π—∣∣QS2 (X — Xs)∣∣2 + Ψt(x).	(6)
x	2αs	2
s=1
1.4	Outline of Contents
The rest of the paper is organized as folloWs. In Section 1.5, We introduce the necessary notations
and technical background.
In Section 2, We present the closed-form solution of Eq. (4) and the algorithm of general frameWork
of adaptive optimization methods With sparse group lasso. We prove the algorithm is equivalent to
adaptive optimization methods When regularization terms vanish. In the end, We give tWo concrete
examples of the algorithm.1
In Section 3, We derive the regret bounds of the method and convergence rates.
In Section 4, We validate the performance of neW optimizers in the public datasets.
In Section 5, We summarize the conclusion.
Appendices A-B list the details of Group Adam and Group Adagrad respectively. Appendices C-F
contain technical proofs of our main results and Appendix G includes the details of the empirical
results of Section 4.4.
1.5	Notations and Technical Background
We use loWercase letters to denote scalars and vectors, and uppercase letters to denote matrices.
We denote a sequence of vectors by subscripts, that is, X1, . . . , Xt, and entries of each vector by an
additional subscript, e.g., Xt,i. We use the notation g1:t as a shorthand for Pts=1 gs. Similarly We
Write m1:t for a sum of the first moment mt, and f1:t to denote the function f1:t(X) = Pts=1 fs(X).
Let Mt = [mi …mt] denote the matrix obtained by concatenating the vector sequence {mt}t≥ι
and Mt,i denote the i-th roW of this matrix Which amounts to the concatenation of the i-th component
of each vector. The notation A 0 (resp. A 0) for a matrix A means that A is symmetric and
positive semidefinite (resp. definite). Similarly, the notations A B and A B mean that
A — B 0 and A — B 0 respectively, and both tacitly assume that A and B are symmetric. Given
A 占 0, we write A 2 for the square root of A, the unique X 占 0 such that XX = A (McMahan &
Streeter (2010), Section 1.4).
Let E be a finite-dimension real vector space, endowed with the Mahalanobis norm ∣∣ ∙ ∣∣a which is
denoted by ∣∣ ∙ ∣a =，h, A∙i as induced by A * 0. Let E * be the vector space of all linear functions
on E. The dual space E * is endowed with the dual norm IlTlA =，《,A-1.).
Let Q be a closed convex set in E. A continuous function h(X) is called strongly convex on Q with
norm ∣∣ ∙ ∣∣ h if Q ⊆ dom h and there exists a constant σ > 0 such that for all x,y ∈ Q and a ∈ [0,1]
we have	1
h(αx + (1 — α)y) ≤ αh(x) + (1 — α)h(y) — $σα(1 — α)∣x — y∣H.
1To fulfill research interest of optimization methods, we will release the code in the future.
3
Under review as a conference paper at ICLR 2021
The constant σ is called the convexity parameter of h(x), or the modulus of strong convexity. We
also denote by k∙kh = k∙ kH.Further, if h is differential, we have
h(y) ≥ h(x) + (Vh(x),y - Xi + 2∣∣x - y∣∣h∙
We use online convex optimization as our analysis framework. On each round t = 1, . . . , T , a
convex loss function ft : Q 7→ R is chosen, and we pick a point xt ∈ Q hence get loss ft(xt). Our
goal is minimizing the regret which is defined as the quantity
TT
RT =Xft(xt) - min X ft(x).	(7)
t=1	x∈Q t=1
Online convex optimization can be seen as a generalization of stochastic convex optimization. Any
regret minimizing algorithm can be converted to a stochastic optimization algorithm with conver-
gence rate O(RT/T) using an online-to-batch conversion technique (Littlestone, 1989).
In this paper, We assume Q ≡ E = Rn, hence We have E* = Rn. We write STX or S ∙ X for
the standard inner product between s, x ∈ Rn . For the standard Euclidean norm, ∣x∣ = ∣x∣2 =
hXhx, x) and ∣∣s∣* = ∣∣s∣2. We also use ∣∣χ∣ι = Pn=ι |x(i) | and ∣∣x∣∣∞ = maxi|x(i)| to denote
'ι-norm and '∞-norm respectively, where x(i) is the i-th element of x.
2 Algorithm
2.1 Closed-form Solution
We will derive the closed-form solution of Eq. (6) with specific At and Algorithm 1 with slight
modification in this section. We have the following theorem.
Theorem 1.	Given At = (PS=ι 2Q- + λ2I) of Eq. (5), Zt = zt-ι + mt — Qt Xt at each iteration
t = 1, . . . , T and z0 = 0, the optimal solution of Eq. (6) is updated accordingly as follows:
xt+1 = (X Qs + 2λ2I)-1 max(1 — V^xgλ21,0)st
M as	kst∣2
where the i-th element ofSt is defined as
S =	0	if |zt,i| ≤ λ1,
t,i	sign(zt,i)λ1 - zt,i otherwise,
St is defined as
st=(X 2⅛+尢1)-1”
s=1 s
and PS=ι Q- is the diagonal and positive definite matrix.
(8)
(9)
(10)
The proof of Theorem 1 is given in Appendix C. We slightly modify (8) where we let SSt = St . Our
purpose is to let every entry of the group have the same effect of '21-regularization. Hence, we get
Algorithm 1. Furthermore, we have the following theorem which shows the relationship between
Algorithm 1 and adaptive optimization methods. The proof is given in Appendix D.
Theorem 2.	If regularization terms of Algorithm 1 vanish, Algorithm 1 is equivalent to Eq. (1).
2.2 Concrete Examples
Using Algorithm 1, we can easily derive the new optimizers based on Adam (Kingma & Ba, 2015),
Adagrad (Duchi et al., 2011) which we call Group Adam, Group Adagrad respectively.
Group Adam
The detail of the algorithm is given in Appendix A. From Theorem 2, we know that when λ1, λ2,
λ21 are all zeros, Algorithm 2 is equivalent to ADAM (Kingma & Ba, 2015).
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Generic framework of adaptive optimization methods with sparse group lasso
1:	Input: parameters λ1 , λ21 , λ2
x1 ∈ Rn, step size {αt > 0}tT=1, sequence of functions
2:	for t = 1 to T do
3：	gt = Vft(xt)
4： mt = φt(gι,... ,gt) and Vt = ψt(gι ,...,gt)
5：	Qt = √Vt - √Vt-1
' at	at	at-1
6：	Zt - zt-ι + mt 一 QtXt
7： for i ∈ {1, . . . , n} do
8： s =	0	if |zt,i| ≤ λ1
t,i	sign(zt,i)λ1 一 zt,i otherwise.
9:	end for
10：	χt+ι = (√Vt + 2λ2I)-1 maχ(I - ʌ/Zkxtl2 , O)St
11： end for
{φt,ψt}tT=1, initialize z0
0, V0 = 0, α0 = 0
Group Adagrad
The detail of the algorithm is given in Appendix B. Similarly, from Theorem 2, when λ1, λ2, λ21
are all zeros, Algorithm 3 is equivalent to Adagrad (Duchi et al., 2011). Furthermore, we can find
that when λ21 = 0, Algorithm 3 is equivalent to FTRL (McMahan et al., 2013). Therefore, GROUP
Adagrad can also be called Group FTRL from the research of Ni et al. (2019).
Similarly, Group Momentum, Group AMSGrad, Group AdaHessian, etc., can be derived
from MOMENTUM (Polyak, 1964), AMSGRAD (Reddi et al., 2018), ADAHESSIAN (Yao et al.,
2020), etc., with the same framework and we will not list the details.
3 Convergence and Regret Analysis
Using the framework developed in Nesterov (2009); Xiao (2010); Duchi et al. (2011), we have the
following theorem providing the bound of the regret.
Theorem 3. Let the sequence {xt} be defined by the update (6) and
xι = argmin1 Ilx - c∣∣2,	(11)
x∈Q 2
where c is an arbitrary constant vector. Suppose ft(x) is convex for any t ≥ 1 and there exists an
optimal solution x* of PT=I ft(x), i.e., x* = arg minχ∈Q PT=I ft(x), which satisfies the condition
hmt-1, xt - x*i≥ 0, t ∈ [T],	(12)
where mt is the weighted average of the gradient ft (xt) and [T] = {1, . . . , T} for simplicity.
Without loss of generality, we assume
mt = γmt-1 + gt,
(13)
where γ < 1 and m0 = 0. Then
TT
RT ≤ Ψτ(x*) + X 2α- kQt2(x* - xt)k2 + 2 X kmtkhj-1,	(14)
t=1 2αt	2 t=1
where ∣∣ ∙ ∣∣h* is the dual norm of kT∣ht. ht is 1 -Strongly ConVexwith respect to ∣∣ ∙ k√vt∕θtt for t ∈ [T]
and ho is 1 -strongly convex with respect to ∣∣ ∙ ∣∣2.
The proof of Theorem 3 is given in Appendix E. Since in most of adaptive optimizers, Vt is the
weighted average of diag(gt2), without loss of generality, we assume αt = α and
Vt = ηVt-1 + diag(gt2),	t≥ 1,	(15)
where V0 = 0 and η ≤ 1. Hence, we have the following lemma whose proof is given in Ap-
pendix F.1.
5
Under review as a conference paper at ICLR 2021
Lemma 1. Suppose Vt is the weighted average of the square of the gradient which is defined by
(15), αt = α, mt is defined by (13) and Vt satisfies the following arbitrary conditions:
1.	η = 1,
2.	η < 1, η ≥ γ and κVt	Vt-1 for all t ≥ 1 where κ < 1.
Then we have
Td
X kmtk2后)-1 < 12αν XkMT，ik2，	W
where ν = max(γ, κ) and d is the dimension of xt.
We can always add	δ2I	to	Vt	at each step to ensure	Vt	0.	Therefore,	ht (x)	is 1-strongly convex
with respect to ∣∣ ∙ ∣∣√δ2i+¼∕αt. Let δ ≥ maxt∈[τ] IIgtk∞,fort > 1, we have
Ilmtkh* 1
t-1
(mt,αt(δ2I + Vt-i)-2m, ≤ (mt,αt (diag(g2) + η%-i) 2 mJ
(me 匕-2 m, = ∣∣mtk2√vt
( αt )
For t = 1, we have
∣m1 ∣2h0*
(mι,αι(δ2I + I)-1 mi〉≤ (m1,α1 (diag- 1(g2)) mi)
(mι,αιVjI mi)= ∣∣mιk2√Vι	.
( α ι )
(17)
(18)
From (17), (18) and Lemma 1, we have
Lemma 2. Suppose Vt, mt, αt, ν, d are defined the same as Lemma 1, maxt∈[T] ∣gt ∣∞ ≤ δ,
k ∙ kh* = (∙,αt(δ2I+ Vt)-2 ) for t ≥ 1 and k ∙ % = (∙,αι ((δ2 + 1)I)-ι ). Then
T	2α	d
Ekmtkh*-1< I-V EkMTMM
(19)
Therefore, from Theorem 3 and Lemma 2, we have
Corollary 1. Suppose Vt, mt, at, ht, V, d are defined the same as Lemma 2, there exist constants
G, Di, D2 such that maxt∈[τ] kgtk∞ ≤ G ≤ δ, ∣∣x*k∞ ≤ Di and maxt∈[τ] kxt - x*k∞ ≤ D2.
Then
RT < dDi f λi + λ2i(~2------+ λ2) 2 + λ2Di) + dG (^2α2^ + (1 V)2) √T.	(20)
The proof of Corollary 1 is given in F.2. Furthermore, from Corollary 1, we have
Corollary 2. Suppose mt is defined as (13), αt = α and satisfies the condition (19). There exist
constants G, Di, D? such thattG2I 占 Vt, maxt∈[τ] ∣∣gt∣∣∞ ≤ G, ∣∣x*k∞ ≤ Di andmaxt∈[τ] ∣∣xt 一
x*k∞ ≤ D?. Then
RT < dDi (λi + λ2i(-2α—+ λ2)2 + λ2Di) + dG (2? + (ɪ—VP) √T.	(21)
Therefore, We know that the regret of the update (6) is O(√T) and can achieve the optimal conver-
gence rate O(1∕√T) under the conditions of Corollary 1 or Corollary 2.
6
Under review as a conference paper at ICLR 2021
4	Experiments
4.1	Experiment Setup
We test the algorithms on three different large-scale real-world datasets with different neural network
structures. These datasets are various display ads logs for the purpose of predicting ads CTR. The
details are as follows.
a)	The Avazu CTR dataset (Avazu, 2015) contains approximately 40M samples and 22 categorical
features over 10 days. In order to handle categorical data, we use the one-hot-encoding based
embedding technique (see, e.g., Wang et al. (2017), Section 2.1 or Naumov et al. (2019), Section
2.1.1) and get 9.4M features in total. For this dataset, the samples from the first 9 days (containing
8.7M one-hot features) are used for training, while the rest is for testing. Our DNN model
follows the basic structure of most deep CTR models. Specifically, the model comprises one
embedding layer, which maps each one-hot feature into 16-dimensional embeddings, and four
fully connected layers (with output dimension of 64, 32, 16 and 1, respectively) in sequence.
b)	The iPinYou dataset2 (iPinYou, 2013) is another real-world dataset for ad click logs over 21
days. The dataset contains 16 categorical features3. After one-hot encoding, we get a dataset
containing 19.5M instances with 1033.1K input dimensions. We keep the original train/test
splitting scheme, where the training set contains 15.4M samples with 937.7K one-hot features.
We use Outer Product-based Neural Network (OPNN) (Qu et al., 2016), and follow the standard
settings of Qu et al. (2016), i.e., one embedding layer with the embedding dimension of 10, one
product layer and three hidden layers of size 512, 256, 128 respectively where we set dropout
rate at 0.5.
c)	The third dataset is the Criteo Display Ads dataset (Criteo, 2014) which contains approximately
46M samples over 7 days. There are 13 integer features and 26 categorical features. After one-
hot encoding of categorical features, we have total 33.8M features. We split the dataset into 7
partitions in chronological order and select the earliest 6 parts for training which contains 29.6M
features and the rest for testing though the dataset has no timestamp. We use Deep & Cross
Network (DCN) (Wang et al., 2017) and choose the following settings4: one embedding layer
with embedding dimension 8, two deep layers of size 64 each, and two cross layers.
For the convenience of discussion, we use MLP, OPNN and DCN to represent the aforementioned
three datasets coupled with their corresponding models. It is obvious that the embedding layer has
most of parameters of the neural networks when the features have very high dimension, therefore
we just add the regularization terms to the embedding layer. Furthermore, each embedding vector
is considered as a group, and a visual comparison between `1 , `21 and mixed regularization effect is
given in Fig. 2 of Scardapane et al. (2016).
We treat the training set as the streaming data, hence we train 1 epoch with a batch size of 512
and do the validation. The experiments are conducted with 4-9 workers and 2-3 parameter servers,
which depends on the different sizes of the datasets. We use the area under the receiver-operator
curve (AUC) as the evaluation criterion since it is widely used in evaluating classification problems.
Besides, some work validates AUC as a good measurement in CTR estimation (Graepel et al., 2010).
We explore 5 learning rates from 1e-5 to 1e-1 with increments of 10x and choose the one with the
best AUC for each new optimizer in the case of no regularization terms (It is equivalent to the
original optimizer according to Theorem 2). All the experiments are run 5 times repeatedly and
tested statistical significance using t-test. Without loss of generality, we choose two new optimizers
to validate the performance, which are Group Adam and Group AdaGrad.
4.2	Adam vs. Group Adam
First, we compare the performance of the two optimizers on the same sparsity level. We keep λ1,
λ2 be zeros and choose different values of λ21 of Algorithm 2, i.e., GROUP ADAM, and achieve the
2We only use the data from season 2 and 3 because of the same data schema.
3See https://github.com/Atomu2014/Ads-RecSys-Datasets/ for details.
4Limited by training resources available, we don’t use the optimal hyperparameter settings of Wang et al.
(2017).
7
Under review as a conference paper at ICLR 2021
same sparsity with Adam that uses the magnitude pruning method, i.e., sort the norm of embedding
vector from largest to smallest, and keep top N embedding vectors which depend on the sparsity
when finish the training. Table 2 reports the average results of the two optimizers in the three
datasets. Note that Group Adam significantly outperforms Adam on the AUC metric on the same
sparsity level for most experiments. Furthermore, as shown in Figure 1, the same '21-regularization
strength λ21 has different effects of sparsity and accuracy on different datasets. The best choice
of λ21 depends on the dataset as well as the application (For example, if the memory of serving
resource is limited, sparsity might be relative more important). One can trade off accuracy to get
more sparsity by increasing the value of λ21.
Table 2: AUC for the two optimizers and sparsity (feature rate) in parentheses. The best AUC for
each dataset on each sparsity level is bolded. The p-value of the t-test of AUC is also listed.
λ21 Group Adam	Adam	MLP Group Adam	P-Value	Adam	OPNN Group Adam	P-Value	Adam	DCN Group Adam	P-Value
1e-4	0.7452 (0.974)	0.7461 (0.974)	0.025	0.7551 (0.078)	0.7595 (0.078)	0.086	0.8018 (0.518)	0.8022 (0.518)	0.105
5e-4	0.7464 (0.864)	0.7468 (0.864)	0.466	0.7491 (0.039)	0.7573 (0.039)	0.091	0.8017 (0.062)	0.8019 (0.062)	0.487
1e-3	0.7452 (0.701)	0.7468 (0.701)	0.058	0.7465 (0.032)	0.7595 (0.032)	0.014	0.8017 (0.018)	0.8017 (0.018)	0.943
5e-3	0.7452 (0.132)	0.7464 (0.132)	0.155	0.7509 (0.018)	0.7561 (0.018)	0.041	0.7995 (4.2e-3)	0.8007 (4.2e-3)	9.11e-3
1e-2	0.7430 (0.038)	0.7466 (0.038)	3.73e-4	0.7396 (9.2e-3)	0.7493 (9.2e-3)	0.031	0.7972 (2.5e-3)	0.7999 (2.5e-3)	5.97e-7
MLP	OPNN	DCN
Figure 1: The AUC across different sparsity on two optimizers for the three datasets. The x-axis is
sparsity (number of non-zero features whose embedding vectors are not equal to 0 divided by the
total number of features present in the training data). The y-axis is AUC.
Next, we compare the performance of Adam without post-processing procedure, i.e., no magnitude
pruning, and Group Adam with appropriate regularization terms which we choose in Table 3 on
the AUC metric. In general, good default settings of λ2 is 1e-5. The results are shown in Table 4.
Note that compared with Adam, Group Adam with appropriate regularization terms can achieve
significantly better or highly competitive performance with producing extremely high sparsity.
4.3	Adagrad vs. Group Adagrad
We compare with the performance of Adagrad without magnitude pruning and Group Adagrad
with appropriate regularization terms which we choose in Table 5 on the AUC metric. The results
are shown in Table 6. Again note that in comparison to Adagrad, Group Adagrad can not
only achieve significantly better or highly competitive performance of AUC, but also effectively and
efficiently reduce the dimensions of the features.
8
Under review as a conference paper at ICLR 2021
	Table 4:	AUC for three datasets and sparsity (feature rate) in
Table 3: The regularization parentheses. The best value for each dataset is bolded. The p-		
terms of Group	Adam of value of t-test is also listed.	
three datasets.	Dataset	Adam	Group Adam	P-Value
Dataset	λ1	λ21	λ2	MLP	0.7458 (1.000)	0.7486 (0.018)	1.10e-3(2.69e-11)
MLP	5e-3 OPNN	8e-5	1e2 1e5 1e-2 1e-5 OPNN 1e-5 1e-5	0.7588 (0.827)	0.7617 (0.130)	0.289 (6.20e-11)
DCN	4e-4	5e-4	1e-5	DCN	0.8021 (1.000)	0.8019 (0.030)	0.422 (1.44e-11)
Table 5: The regularization
terms of Group Adagrad
of three datasets.
Dataset	λι	λ21	λ2
MLP	0	1e-2	1e-5
OPNN	8e-5	8e-5	1e-5
DCN	0	4e-3	1e-5
Table 6: AUC for three datasets and sparsity (feature rate) in
parentheses. The best value for each dataset is bolded. The p-
ValUe of t-test is also listed._________________________________
Dataset	Adagrad	Group Adagrad	P-Value
MLP	0.7453 (1.000)	^^0.7469 (0.063)^^	0.106 (1.51e-9)
OPNN	0.7556 (0.827)	^^0.7595 (0.016)^^	0.026 (< 2.2e-16)
DCN	0.7975 (1.000)	^^0.7978 (0.040)^^	0.198 (3.94e-11)
4.4	Discussion
In this section we will discUss the hyperparameters of emdedding dimension, `1 -regUlarization and
'21-regularization to show how these hyperparameters affect the effects of regularization.
Embedding Dimension Table 7 of Appendix G reports the average resUlts of different embedding
dimensions of MLP, whose optimizer is Group Adam and regularization terms are same to MLP
of Table 5. Note that the sparsity increases with the growth of the embedding dimension. The reason
is that the square root of the embedding dimension is the multiplier of `21 -regularization.
`1 vs. `21 From lines 8 and 10 of Algorithm 1, we know that if zt has the same elements, the
values of `1 and `21, i.e., λ1 and λ21, have the same regularization effects. However, this situation
almost cannot be happen in reality. Without loss of generality, we set optimizer, λ2 and embedding
dimension be GROUP ADAM, 1e-5 and 16 respectively, and choose different values of λ1, λ21.
The results on MLP are shown in Table 8 of Appendix G. It is obvious that '21-regularization is
much more effective than `1 -regularization in producing sparsity. For example, when λ1 = 0 and
λ21 = 5e-3, the feature sparsity is 0.136, while for λ1 = 5e-3 and λ21 = 0, the feature sparsity is
0.470. Therefore, if just want to produce sparsity, we can only tune λ21 and use default settings for
λ2 and λ1, i.e., λ2 = 1e-5 and λ1 = 0.
5	Conclusion
In this paper, we propose a novel framework that adds the regularization terms to a family of adap-
tive optimizers for producing sparsity of DNN models. We apply this framework to create a new
class of optimizers. We provide closed-form solutions and algorithms with slight modification. We
built the relation between new and original optimizers, i.e., our new optimizers become equiva-
lent with the corresponding original ones, once the regularization terms vanish. We theoretically
prove the convergence rate of the regret and also conduct empirical evaluation on the proposed op-
timizers in comparison to the original optimizers with and without magnitude pruning. The results
clearly demonstrate the advantages of our proposed optimizers in both getting significantly better
performance and producing sparsity. Finally, it would be interesting in the future to investigate the
convergence in non-convex settings and evaluate our optimizers on more applications from fields
such as compute vision, natural language processing and etc.
9
Under review as a conference paper at ICLR 2021
References
Avazu. Avazu click-through rate prediction, 2015. URL https://www.kaggle.com/c/
avazu-ctr-prediction/data.
Criteo. Criteo display ad challenge, 2014. URL http://labs.criteo.com/2014/02/
kaggle-display-advertising-challenge-dataset.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learn-
ing and stochastic optimization. Journal of Machine Learning Research, 12:2121-2159, 2011.
doi: 10.5555/1953048.2021068. URL https://dl.acm.org/doi/10.5555/1953048.
2021068.
Thore GraePeL JoaqUin Quinonero Candela, Thomas Borchert, and Ralf Herbrich. Web-scale
bayesian click-through rate prediction for sponsored search advertising in microsoft’s bing search
engine. In Johannes FUrnkranz and Thorsten Joachims (eds.), Proceedings of the 27th Interna-
tional Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 13-20.
Omnipress, 2010. URL https://icml.cc/Conferences/2010/papers/901.pdf.
iPinYoU. ipinyoU global rtb bidding algorithm competition, 2013. URL https://www.kaggle.
com/lastsummer/ipinyou.
Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Proceed-
ings of the 3rd International Conference on Learning Representations, ICLR ’15, San Diego, CA,
USA, 2015.
Nick Littlestone. From on-line to batch learning. In Ronald L. Rivest, David HaUssler, and Man-
fred K. WarmUth (eds.), Proceedings of the Second Annual Workshop on Computational Learning
Theory, COLT 1989, Santa Cruz, CA, USA, July 31 - August 2, 1989, pp. 269-284. Morgan KaUf-
mann, 1989. URL http://dl.acm.org/citation.cfm?id=93365.
H. Brendan McMahan and Matthew J. Streeter. Adaptive boUnd optimization for online convex
optimization. In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-
29, 2010, pp. 244-256. Omnipress, 2010. URL http://colt2010.haifa.il.ibm.com/
papers/COLT2010proceedings.pdf#page=252.
H. Brendan McMahan, Gary Holt, D. ScUlley, Michael YoUng, Dietmar Ebner, JUlian Grady, Lan
Nie, Todd Phillips, EUgene Davydov, Daniel Golovin, Sharat ChikkerUr, Dan LiU, Martin Watten-
berg, Arnar Mar Hrafnkelsson, Tom BoUlos, and Jeremy KUbica. Ad click prediction: a view from
the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge
discovery and data mining, KDD ’13, pp. 1222-1230, Chicago, Illinois, USA, 2013. ACM.
Maxim NaUmov, Dheevatsa MUdigere, Hao-JUn Michael Shi, JianyU HUang, Narayanan SUn-
daraman, Jongsoo Park, Xiaodong Wang, Udit GUpta, Carole-Jean WU, Alisson G. Azzolini,
Dmytro DzhUlgakov, Andrey Mallevich, Ilia Cherniavskii, Yinghai LU, RaghUraman Krish-
namoorthi, Ansha YU, Volodymyr Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen,
Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. Deep learning recommendation
model for personalization and recommendation systems. CoRR, abs/1906.00091, 2019. URL
http://arxiv.org/abs/1906.00091.
YUrii E. Nesterov. Smooth minimization of non-smooth fUnctions. Math. Program., 103:127-152,
2005.
YUrii E. Nesterov. Primal-dUal sUbgradient methods for convex problems. Math. Program., 120
(1):221-259, 2009. doi: 10.1007/s10107-007-0149-x. URL https://doi.org/10.1007/
s10107-007-0149-x.
XiUyan Ni, Yang YU, Peng WU, YoUlin Li, Shaoliang Nie, Qichao QUe, and Chao Chen. FeatUre
selection for facebook feed ranking system via a groUp-sparsity-regUlarized training algorithm. In
Proceedings of the 28th ACM International Conference on Information and Knowledge Manage-
ment, CIKM ’19, pp. 2085-2088, Beijing, China, 2019. ACM.
10
Under review as a conference paper at ICLR 2021
Boris T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Compu-
tational Mathematics and Mathematical Physics, 4(5):1-17, 1964. doi: 10.1016/0041-5553(64)
90137-5.
Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. Product-based
neural networks for user response prediction. In Francesco Bonchi, Josep Domingo-Ferrer, Ri-
cardo Baeza-Yates, Zhi-Hua Zhou, and Xindong Wu (eds.), IEEE 16th International Conference
on Data Mining, ICDM 2016, December 12-15, 2016, Barcelona, Spain, pp. 1149-1154. IEEE
Computer Society, 2016. doi: 10.1109/ICDM.2016.0151. URL https://doi.org/10.
1109/ICDM.2016.0151.
Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In Pro-
ceedings of the 6th International Conference on Learning Representations, ICLR ’18, Vancouver,
BC, Canada, 2018. OpenReview.net.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati-
cal statistics, pp. 400-407, 1951.
R. Tyrrell Rockafellar. Convex Analysis. Princeton Landmarks in Mathematics and Physics. Prince-
ton University Press, 1970. ISBN 978-1-4008-7317-3.
Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regu-
larization for deep neural networks. Neurocomputing, 241:43-52, 2016. doi: 10.1016/j.neucom.
2017.02.029. URL https://doi.org/10.1016/j.neucom.2017.02.029.
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. Deep & cross network for ad click predic-
tions. In Proceedings of the ADKDD’17, Halifax, NS, Canada, August 13 - 17, 2017, pp. 12:1-
12:7. ACM, 2017. doi: 10.1145/3124749.3124754. URL https://doi.org/10.1145/
3124749.3124754.
Lin Xiao. Dual averaging method for regularized stochastic learning and online optimization. Jour-
nal of Machine Learning Research, 11:2543-2596, 2010. doi: 10.5555/1756006.1953017. URL
https://dl.acm.org/doi/10.5555/1756006.1953017.
Haiqin Yang, Zenglin Xu, Irwin King, and Michael R. Lyu. Online learning for group lasso. In
Proceedings of the 27th International Conference on Machine Learning, ICML ’10, pp. 1191-
1198, Haifa, Israel, 2010. Omnipress.
Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W. Mahoney. ADAHESSIAN:
an adaptive second order optimizer for machine learning. CoRR, abs/2006.00719, 2020. URL
https://arxiv.org/abs/2006.00719.
Matthew D. Zeiler. Adadelta: An adaptive learning rate method. CoRR, abs/1212.5701, 2012. URL
https://arxiv.org/abs/1212.5701.
11
Under review as a conference paper at ICLR 2021
Appendix
A Group Adam
Algorithm 2 Group Adam
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
Input: parameters λ1, λ21, λ2, β1, β2,
xι ∈ Rn, step size α, initialize zo = 0, rm0 = 0, V⅛ = 0, V0 = 0
for t = 1 to T do
gt = Yft(Xt)
m t — βιm t-i + (1 — βι)gt
mt = m t/(1 — βt)
Vt ― β^Vt-ι + (1 — β2 )diag(g2)
M = Vt∕(ι — β2)
√	√½ — √V-Γ + eI t = 1
Qt = t √½-√V-Γ	t> 1
Zt ― zt-1 + mt — a Qtxt
for i ∈ {1, . . . , n} dαo
s =	0	if |zt,i| ≤ λ1
t,i	sign(zt,i)λ1 — zt,i otherwise.
end for
xt+1 = (√Vt+I + 2λ2l)-1 max(1 — qdXgλ21
end for
, 0)st
B Group Adagrad
Algorithm 3 Group Adagrad
1: Input: parameters λ1 , λ21 , λ2 ,
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
x1 ∈ Rn , step size α, initialize z0 = 0, V0 = 0
for t = 1 to T do
gt = Yft (xt)
mt = gt
V Vt-1 + diag(gt2) + I t = 1
t - I Vt-1 + diag(g2) t > 1
Qt = √½ — √Vt-T
Zt ― Zt-1 + mt — 1 Qtxt
for i ∈ {1, . . . , n} dαo
s =	0	if |Zt,i| ≤ λ1
t,i	sign(Zt,i)λ1 — Zt,i	otherwise.
end for
xt + 1 = ( √Vt + 2λ2I)T max(1 — ʌZkx^ 2 , O)St
end for
C Proof of Theorem 1
Proof.
t1	T
xt+1 = arg min mi：t ∙ X +	--(X - Xs)TQs (X - Xs) + Ψt(x)
x	2αs
s=1
t 1	1	1
=arg min mi：t ∙ X + X --(kQ2 x∣∣2 - 2x ∙ (Qs Xs) + ∣∣QJ Xsk2) + Ψt3	(22)
x	2αs
s=1
Ct Q ∖	t 1 ι
mi：t -	αXs) ∙ X + ɪ^ 2θ^∣Q^x∣2 + Ψt(X)∙
12
Under review as a conference paper at ICLR 2021
Define zt-ι = mi：t-i - P：=1 QasXs (t ≥ 2) and We can calculate Zt as
Zt = Zt-1 + mt — α^xt, t ≥ 1.	(23)
By substituting (23), (22) is simplified to be
tQ
xt+1 = arg min Zt ∙ x + V' L ∣∣x∣∣2 + Ψt(x).	(24)
x	2αs	2
s=1
By substituting Ψt (x) (Eq. (5)) into (24), We get
+
(25)
、G、/	__ Jr Q g
xt+1 = arg min Zt ∙ x + ɪj (λι∣∣xg kι + λ21 √dXg kC∑^2αs + '? I)2 Xg ∣∣2
k(X 20^ + 尢I) 1 x∣∣2∙
s=1 s
Since the objective of (25) is component-Wise and element-Wise, We can focus on the solution in
one group, say g, and one entry, say i, in the g-th group. Let Ps=ι 2QS = diag(σg) where σg =
(σtg,1 , . . . , σtg,dxg). The objective of (25) on xtg+1,i is
ω(Xg+1,/ = zg,ixg+1,i + λ1 |Xg+1,i| + φ(xt+1,i),	(26)
where φ(xg+1,i) = λ21√dXgk(σg,i + λ2)1 xg+1,ik2 + k(σg,i + λ2) 1 xg+1,ik2 is a non-negative
function and Φ(xtg+1,i) = 0 iff xtg+1,i = 0 for all i ∈ {1, . . . , dxg}.
We discuss the optimal solution of (26) in three cases:
a)	If Ztg,i = 0, then xtg+1,i = 0.
b)	If Zgi > 0, then x+1 分 ≤ 0. Otherwise, if x+1 分 > 0, we have Ω(-xg+ι J < Ω(xg+ι j which
contradicts the minimization value of Ω(x) on xg+1 钎
Next, if Ztg,i ≤ λι, then xg+ii = 0. Otherwise, if xg+ii < 0, we have Ω(xg+ι,j = (Ztn —
λ1)xg+1 i + Φ(xg+ι) > Ω(0), which also contradicts the minimization value of Ω(x) on xg+ik
Third, Zgi > λι (∀ i = 1,..., dxg). The objective of (26) for the g-th group, Ω(xg+J, becomes
(Zt- λ11dχg ) ∙ xg+1 + φ(xg+1).
c)	If Ztg,i < 0, the analysis is similar to b). We have xtg+i,i ≥ 0. When -Ztg,i ≤ λi, xtg+i,i = 0.
When -Ztg,i > λi (∀ i = 1, . . . , dxg), we have
C(xg+1) = (Zt + λι1dχg) ∙ xg+ι + φ(xg+ι).
From a), b), c) above, we have
xg+i = argmin —sg ∙ x + Φ(x),	(27)
x
where the i-th element of stg is defined same as (9).
Define	1
y = (diag(σg) + λ2I)2 x.	(28)
By substituting (28) into (27), we get
yg+i = arg min T ∙ y + λ21 √dXg ∣∣y∣∣2 + ∣∣y∣2,	(29)
y
13
Under review as a conference paper at ICLR 2021
where sg = (diag(σg) + λ2I)-1sg which is defined same as(10). This is unconstrained non-smooth
optimization problem. Its optimality condition (see Rockafellar (1970), Section 27) states that ytg+1
is an optimal solution if and only if there exists ξ ∈ ∂kytg+1 k2 such that
-Sg + λ21 P dχgξ + 2yg+l = 0.
(30)
The subdifferential of ky k2 is
∂kyk2
{Z ∈ Rdxg | - 1 ≤ Z ⑴ ≤ 1,i = 1,...,dχg} if y = 0,
k⅛	if y = 0.
Similarly to the analysis of '1-regularization, We discuss the solution of (30) in two different cases:
a)	If Ilsg∣∣2 ≤ λ2i√dχg, then yg+i = 0 and ξ = λ2l√^ ∈ ∂∣I0I∣2 satisfy (30). We also show that
there is no solution other than ytg+i = 0. Without loss of generality, we assume ytg+i,i 6= 0 for
all i ∈ {1,..∙, dχg}, then ξ = kyg+k2, and
From (31), we can derive
Furthermore, we have
-Sg+*kg yg+1+2yg+1=0.
(λk2≠k≡ +2)kyg+1k2 = kSg ∣∣2.
kyt+1 k2
∣yg+i∣2 = 1(∣sg ∣2 - λ2iPdχg),
where ∣∣yg+i∣∣2 > 0 and ∣∣sg ∣∣2 一 入2i√X ≤ 0 contradict each other.
b) IfIlSg∣∣2 > λ2i√dχg, then from (31) and (32), we get
Ilg , = 1(1 - λ2iE)sg
yt+i 2(1	∣Sg∣2 )St.
We replace ytg+i of (33) by xtg+i using (28), then we have
xg+i = (diag(σg) + λ2I)-2 yg+i
=(2diag(σg) + 2λ2l)-i(1 - λ2√xg)sg
t	∣SSt ∣2 t
=(X Qs+2λ2I)-i(1 - λi√dxz)sg.
Combine a) and b) above, we finish the proof.
D Proof of Theorem 2
Proof. We use the method of induction.
a) When t = 1, then Algorithm 1 becomes
Q1 = α1
(31)
(32)
(33)
(34)
□
zι = zo + mi _ 1Xι xι = mi — Vzr^I xι,
α1	α1
√vι
si = -zi = ----xi — mi,
αi
x2
-1	m1
s1 = x1 - α1 √V1,
which equals to Eq. (1).
14
Under review as a conference paper at ICLR 2021
b)	Assume t = T , Eq. (35) are true.
ZT = mτ - √VT XT, XT +1 = XT - ατ mT_.
αT	VT
(35)
For t = T + 1, we have
QT+1
ZT +1 = ZT + mτ +1-----XT +1
αT+1
mτ - VT-X-XT + mτ+1 - QT +1 XT +1
αT	αT+1
√VT	mτ 、 ι	Qt +ι
mτ------(XT +1 + ατ	) + mT +1------XT +1
αT	VT	αT+1
mT+1 -
QT +1 ʌ	VZVT +1
+-----)xt +1 = mτ +1---------XT +1,
αT+1	αT+1
XT+2 = (
J )-1sτ +1 =-(
αT+1
VT+ +1、— 1	mτ +1
-----)ZT +1 = XT +1 - ατ	E—
αT+1	T+1
Hence, we complete the proof.
□
E Proof of Theorem 3
Proof. Let
ht(X)
ʃ PS=12⅛kQS2(x - Xs)k2 ∀t ∈ [T],
[2kx-ck2	t = 0.
It is easy to verify that for all t ∈ [T], ht(χ) is 1-strongly convex with respect to ∣∣ ∙ k√vς∕at which
^Vt = Pt_ 1 Qs , and ho(χ) is 1-strongly convex with respect to k ∙ ∣∣2.
αt	s=1 αs
From (7), we have
TT
Rτ = X(ft(Xt)- ft(X*)) ≤ X hgt,xt -x*i
TT
=Ehmt - γmt-1,Xt - x*>≤ £ hmt,Xt 一 x*i
t=1	t=1
TT
=X hmt,Xti + Ψt(x*) + hτ(x*) + (X h-mt,X*i - Ψ+ (x*) - hτ(x*))
T
≤ X hmt,Xti + Ψt(x*) + hτ(x*) + sup {h-m1：T,x - Ψt(x) - hτ(x)},
t 1	x∈Q
(36)
where in the first and second inequality above, we use the convexity of ft(X) and the condition (12)
respectively.
We define 传(U) to be the conjugate dual of Ψt(x) + ht(x):
h；(U) = sup {hu, Xi — Ψt(x) — ht(x)} , t ≥ 0,
x∈Q
where Ψo(x) = 0. Since ht(x) is 1-strongly convex with respect to the norm ∣∣ ∙ ||自,the function
h； has 1-Lipschitz continuous gradients with respect to ∣∣ ∙ ∣墟(see, Nesterov (2005), Theorem 1):
kVh；(U1) -Vh；(U2)kht ≤ ∣∣U1- U2∣堪,	(37)
and
Vht； (U) = arg min {- hU, Xi + Ψt(X) + ht(X)} .	(38)
x∈Q
15
Under review as a conference paper at ICLR 2021
As a trivial corollary of (37), we have the following inequality:
晨(U + δ) ≤ ht(u) + Ew + 1kδ除.
2t
Since ht+1(x) ≥ ht(x) and Ψt+1(x) ≥ Ψt(x), from (38), (39), (6), we have
hT(-m1:T) ≤ hT-1 (-m1:T)
≤ hT-1 (-m1:T-1) - (yhT-1(-m1:T-1), mT〉+ 2 kmTkhτ-1
≤ hT-2 (-m1:T-1) - hxT,mTi + 1 kmTIIhT-1
≤ ho (0) - hyho (0),m1i - X hχt,mti + 2 X llmtkh*-1
(39)
(40)
t=2
TT
-	hxt ,mti + 2 Ekmt %-1.
t=1
t=1
t=2
where the last equality above follows from h0(0) = 0 and (11) which deduces χ1 = VhO(0).
By substituting (40), (36) becomes
T
RT ≤ X hmt, xti + ΨT(xo) + hT(xo) + hoT (-m1:T)
t=1
1T
≤ Ψt(xo) + hT(xo) + 2 Ekmtkh J
t=1
(41)
□
F Additional Proofs
F.1 Proof of Lemma 1
Proof. Let Vt = diag(σt) where σt is the vector of the diagonal elements of Vt. For i-th entry ofσt,
by substituting (13) into (15), we have
σt,i = gt2,i + ησt-1,i = (mt,i - γmt-1,i)2 + ηgt2-1,i + η2σt-2,i
tt
= X ηt-s (ms,i - γms-1,i)2 ≥ Xηt-s(1-γ)(ms2,i -γms2-1,i)	(42)
t-1
= (1-γ) mt2,i + (η - γ) X ηt-s-1ms2,i .
s=1
Next, we will discuss the value of η in two cases.
a)	η = 1. From (42), we have
t-1	t	t
σt,i ≥ (1- γ)	mt2,i	+ (1-γ)	X m2s,i	> (1- γ)2 X	ms2,i	≥ (1-	ν)2	Xms2,i.	(43)
Recalling the definition of Mt,i in Section 1.5, from (43), we have
T
X
t=1
<
1 X m2,i
1 - V = kMt,i∣∣2
2
≤ T-TkMT,ik2,
where the last inequality above follows from Appendix C of Duchi et al. (2011). Therefore, we
get
T	Td 2	d
X kmtk2√vE)-1 = αXX 差 < 三 X kMT,ik2∙	沏
t=1	( ɑt )	t=1 i=1 V σt,i	i=1
16
Under review as a conference paper at ICLR 2021
b)	η < 1. We assume η ≥ γ and κVt	Vt-1 where κ < 1, then we have
tt	t
Xκt-sσt,i≥Xσs,i≥(1-γ)Xms2,i.
s=1	s=1	s=1
Hence, we get
t	tt
σt,i ≥	iɪɪ(I- Y) Xms,i	>(I- K)(I- Y) Xms,i	≥(I- V)2 Xm2,i,	(45)
1 - κ	s=1	s=1	s=1
which deduces the same conclusion (44) of a).
Combine a) and b), We complete the proof.	□
F.2 Proof of Corollary 1
Proof. From the definition of mt (13), Vt (15), We have
t	1 Yt	G
lmt,il = 1 X Y sgs,il≤ T-Y G< L
t
即| = | X ηt-sg2,i∖ ≤ tG2.
s=1
G
≤----
_ 1 - v
Hence, We have
Ψτ(x*) ≤ λ1dD1 + λ2ιdDι(-^--+ λ2)2 + λ2dD2,
2α
1dD2G 行
hT (x ) ≤ -ɔ VZT,
2α
(46)
(47)
1T
2X km%」
α X √TG _ daG 厅
1 — v 1 — v (1 — v)2	.
Combining (46), (47), (48), We complete the proof.
(48)
□
G Additional Empirical Results
Table 7: AUC of MLP for different embedding dimensions and sparsity (feature rate) in parentheses.
The best results are bolded._______________________________________________
Embedding Dimension	Group Adam
4	0.7462 (0.123)
8	0.7471 (0.056)
16	0.7486*(0.018)
32	0.7480 (0.006)
Itis significantly better than embedding di-
mensions of 4, 8 but has no difference in
95% confidence level of the embedding di-
mension of 32.
<
17
Under review as a conference paper at ICLR 2021
Table 8: Sparsity (feature rate) of MLP for different values of λ21, λ1 and AUC in parentheses.						
^∖λ1 λ21^∖	0	1e-4	5e-4	1e-3	5e-3	1e-2
0		0.987	0.927	0.866	0.470	0.214
	-	(0.7486)	(0.7482)	(0.7485)	(0.7481)	(0.7475)
1e-4	0.971		0.902	0.839	0.458	0.212
	(0.7477)	-	(0.7486)	(0.7484)	(0.7480)	(0.7483)
5e-4	0.867	0.829		0.682	0.344	0.169
	(0.7490)	(0.7485)	-	(0.7483)	(0.7485)	(0.7480)
1e-3	0.702	0.684	0.612		0.274	0.134
	(0.7477)	(0.7477)	(0.7480)	-	(0.7479)	(0.7478)
5e-3	0.136	0.138	0.120	0.106		0.035
	(0.7485)	(0.7484)	(0.7482)	(0.7482)	-	(0.7483)
1e-2	0.033	0.037	0.033	0.029	0.018	
	(0.7481)	(0.7480)	(0.7481)	(0.7485)	(0.7486)	-
18