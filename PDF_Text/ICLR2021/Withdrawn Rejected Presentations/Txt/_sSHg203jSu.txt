Under review as a conference paper at ICLR 2021
Data-aware Low-Rank Compression for Large
NLP Models
Anonymous authors
Paper under double-blind review
Ab stract
The representations learned by large-scale NLP models such as BERT have been
widely used in various tasks. However, the increasing model size of the pre-trained
models also brings the efficiency challenges, including the inference speed and the
model size when deploying the model on devices. Specifically, most operations
in BERT consist of matrix multiplications. These matrices are not low-rank and
thus canonical matrix decomposition could not find an efficient approximation. In
this paper, we observe that the learned representation of each layer lies in a low-
dimensional space. Based on this observation, we propose DRONE (data-aware
low-rank compression), a provably optimal low-rank decomposition of weight
matrices, which has a simple closed form solution that can be efficiently computed.
DRONE is generic, could be applied to both fully-connected and self-attention
layers, and does not require any fine-tuning or distillation steps. Experimental
results show that DRONE could improve both model size and inference speed
with limited loss of accuracy. Specifically, DRONE alone achieves 1.92x faster on
MRPC task with only 1.5% loss of accuracy, and when combined with distillation,
DRONE achieves over 12.3x faster on various natural language inference tasks.
1 Introduction
The representations learned by large-scale Natural Language Processing (NLP) models such as BERT
have been widely used in various tasks (Devlin et al., 2018). The pre-trained models of BERT and
its variations are used as feature extractors for the downstream tasks such as question answering
and natural language understanding (Radford et al.; Howard & Ruder, 2018). The success of the
pre-trained BERT relies on the usage of large corpus and big models. Indeed, researchers have
reported better results of models with more parameters (Shazeer et al., 2018) and number of layers
(Al-Rfou et al., 2019). The increasing model size of the pre-trained models inhibits the public
user from training a model from scratch, and it also brings the efficiency challenges, including the
inference speed and the model size when deploying the model on devices.
To deal with the efficiency issue, most existing works resort to adjusting the model structures or
distillation. For instance, Kitaev et al. (2020) uses locality-sensitive hashing to accelerate dot-product
attention, Lan et al. (2019) uses repeating model parameters to reduce the size and Zhang et al. (2018)
applies a pre-defined attention pattern to save computation. A large body of prior work focuses on
variants of distillation information has also been explored (Sanh et al., 2019; Jiao et al., 2019; Sun
et al., 2020; Liu et al., 2020; Xu et al., 2020; Sun et al., 2019). However, all these methods either
require a specific design of model architecture which is not generic, or require users to train the
proposed structure from scratch which greatly reduces its practicality.
In this work, we try to explore an acceleration method that is more generic. Note that as shown in
Figure 1, matrix multiplication (Feed-forward layer) is a fundamental operation which appears many
times in the Transformer architecture. In fact, the underlying computation of both multi-head attention
layers and feed-forward layers is matrix multiplication. Therefore, instead of resorting to the complex
architecture redesign approaches, we aim to investigate whether low-rank matrix approximation,
the most classical and simple model compression approach, can be used to accelerate Transformers.
Despite being successfully applied to CNN (Yu et al., 2017; Sindhwani et al., 2015; Shim et al., 2017;
You et al., 2019), at the first glance low-rank compression cannot work for BERT. We could see
in Figure 2 that regardless of layers, matrices in feed-forward layer, query and key transformation
of attention layer are not low-rank. Therefore, even the optimal low-rank approximation (e.g., by
1
Under review as a conference paper at ICLR 2021
Model Architecture
Parameter Size
Inference Time
Feed-Forward 4
Feed-Forward 3
Add & NOniI
Feed-FOrWartl 2	1一
Feed-Forward 1
768 x ^Classes
768x768
768x3072
12X
A(Id & NOrnl
Feed-FOrWard 0 卜
->[
768x768
Multi-Head Attention
∙>(	3x12x64x768
->∣	0.0⅜ ms (0.01%)
->[	1.0⅜ ms (0.24%)	∣
127.6 ms (29.5%)
-X 140.4 ms (32.5%)
-H 29.9 ms (6.91%)
->∣ 131.6 ms (30.42%)
432.5 ms
Encoded Input
M x 768
“	1.92 ms (0.42%)

1
I
I
I

√
√
Figure 1: Illustration of the BERT-base computational model. |V | denotes the number of tokens in
the model. #Classes denotes the number of classes in the down-stream classification task. Input
encoding, Feed-forward 3 and Feed-forward 4 are computed only once in the inference and thus do
not contribute to overall computational time much.
Layer-I-FFl
-*- Layer-1-FF2
• ♦ Layer-12-FF1
倒…Layer-12-FF2
Feed-Forward Layers
Query and Key Layers
						t—
• ∙ Layer-I-Query ・士 ・ Layer-I-Key ∙ Layer-12-Query						
	•“…Layer-	12-Key	■.也多			
						
	N彳					
						
Figure 2: Illustration of the empirical observation that weight matrices in BERT model are not
low-rank. The x-axis represents what percentage of the ranks are selected; the y-axis represents sum
of singular values connected to the selected ranks divided by sum of all singular values. Ideally, a
low-rank structure will have a larger area under curve, meaning that a small percentage of the ranks
could explain most of singular values. We observe that the sum of top 50% of the ranks only account
about 60% of all singular values for matrices in the BERT model. This shows that the matrices do
not have a clear low-rank structure.
SVD) will lead to large reconstruction error and empirically the performance is limited. This is
probably why low-rank approximation has not been used in BERT compression.
In this paper, we propose a novel low-rank approximation algorithm to compress the weight matrices
even though they are not low-rank. The main idea is to exploit the data distribution. In NLP
applications, the latent features, indicating some information extracted from natural sentences, often
lie in a subspace with a lower intrinsic dimension. Therefore, in most of the matrix-vector products,
even though the weight matrices are not low-rank, the input vectors lie in a low-dimensional subspace,
allowing dimension reduction with minimal degraded performance. We mathematically formulate this
generalized low-rank approximation problem which includes the data distribution term and provide
a closed-form solution for the optimal rank-k decomposition of the weight matrices. We propose
DRONE method based on this novel Data-awaRe lOw-raNk comprEssion idea. Our decomposition
significantly outperforms SVD under the same rank constraint, and can successfully accelerate the
BERT model without sacrificing too much test performance.
2
Under review as a conference paper at ICLR 2021
2	Related Work
The inference speed is important for NLP models when deployed in various applications. Generally
speaking, inference efficiency could be enhanced by hardware (Shawahna et al., 2018) or lower-level
instruction optimization (Ning, 2020). On the other hand, the main focus of the current research is on
using algorithmic methods to reduce the computational complexity. It could be mainly categorized
into two aspects: Attention Complexity Reduction and Model Size Reduction.
Attention Complexity Reduction
Attention mechanism is the building block of transformer model and it attracts most attentions of
researcher recently in NLP field (Vaswani et al., 2017). Pre-training on large courpus of BERT,
a transformer-based model, has contributed to state-of-the-art performance on various tasks after
fine-tuning (Devlin et al., 2018). Attention on sequences of length L is O(L2 ) in both computational
and memory complexity. This would take long inference time when the sequence is long. Thus,
researchers have focused on reducing the complexity of the attention module. Kitaev et al. (2020)
uses uses locality-sensitive hashing to reduce the complexity to O(LlogL). Zhang et al. (2018);
Child et al. (2019) pre-defined an attention map to have a constant computational time. Goyal et al.
progressively eliminates the redundant context vectors within the attended sequence to improve
the efficiency of attention in last few layers of the model. Wang et al. (2020) proposes to train the
low-rank attention by choosing a rank r L. This is similar with our work in the sense of leveraging
low-rank structures. But our method do not require retraining the model and could be applied to
different modules other than attention. In fact, most of these methods require special modules and
thus we need to retrain the proposed models from scratch. This prohibits the usage of a large body
of publicly available open models for faster research progress. More importantly, these methods
only focus on the long sequence scenario. We found out that attention module is actually not the
bottleneck of inference time in common usage as shown in Figure 1. In most if not all models of
common usages, two layers of large feed-forward layer are appended after attention module which
incurs much more computational time. Attention complexity reduction only works when a large
sequence is used but in current practice this is unusual. Thus, accelerating attention module itself is
not contributing to a significant reduction of overall inference time.
Model Size Reduction
Efficiency of inference time is also related to model compression. In principle, smaller models would
lead to smaller number of operations and thus faster inference time. Sanh et al. (2020) has explored
pruning methods on BERT models to eliminate the redundant links, and there is a line of research
on pruning methods (Han et al., 2015a;b; Chen et al., 2020). Quantization methods (Zafrir et al.,
2019; Hubara et al., 2016; Lin et al., 2016) could convert the 32 bits float models into lower-bits
fixed-point representation, and theoretically make model prediction faster with fixed point accelerator.
Lan et al. (2019) reduces the model size by sharing of encoder parameters. A large body of prior
work focuses on variants of knowledge distillation (Sanh et al., 2019; Jiao et al., 2019; Sun et al.,
2020; Liu et al., 2020; Xu et al., 2020; Sun et al., 2019; 2020). These methods use different strategies
to distill information from teacher network and reduce the number of layers (Sanh et al., 2019) or
hidden dimension size (Jiao et al., 2019). Further, A hybrid compression method by combining
matrix factorization, pruning and knowledge distillation is proposed by Mao et al. (2020). Among the
above mentioned methods, Quantization requires hardware accelerator to reduce the inference time
which is not applicable to general scenario. Pruning methods could only reduce the model size, but
the inference speed might not be reduced due to the limitation of sparse operations. Only algorithmic
method such as distillation could serve as a generic inference time accelerating method. We want to
emphasize that our method is orthogonal to these distillation methods. In fact, the proposed method
is a generic acceleration method applicable to all components in most NLP models. In section 4, we
show that DRONE can be combined with the distilled models to further improve the performance.
3	Methods
We now introduce a generic algorithm for improving efficiency of matrix multiplication. The
computation of Feed-Forward (FF) Layer in the attention models can be described as:
h= Wx+b,	(1)
3
Under review as a conference paper at ICLR 2021
where W ∈ Rd2×d1 and b ∈ Rd2 are model parameters, x ∈ Rd1 is the latent representation of a
token, and h ∈ Rd2 is the output. Assume the sequence length is L, all the token representations
x1 , . . . , xL ∈ Rd1 will pass through this same operation, so in practice the whole FF layer can be
computed by a matrix-matrix product W[x1, . . . xn] + b, and the computation of bias term b would
be broadcasted to all L input tokens. In practice we will normally have L max(d1, d2) (e.g.,
L = 128, d2 = 3, 072).
A standard way to accelerate the computation is to perform low-rank approximation over W. A low-
rank approximation can be acquired by using singular value decomposition (SVD), which achieves
the best rank-k approximation in terms of Frobenius norm and we could write W as:
W =USVT ≈ UW,kVW,kT,
with unitary matrices U ∈ Rd2×d2, V ∈ Rd1 ×d1 and a diagonal matrix S ∈ Rd2×d1. UW,k ∈ Rd2×k
and VW,k ∈ Rd1×k are the rank-k approximation matrices by taking Uw,k = USk，VW,k = Sk V,
‘1	'	'
where Sk is the square-root of the first k entries of the diagonal matrix S. Given such approximation,
we could simplify the computation in equation 1 by writing it as:
h = W x + b ≈ UW,k VW,k T x + b.
After the rank-k low-rank approximation, the computational complexity reduces from O(d2d1) to
O((d1 + d2)k). When k is small enough, low-rank approximation could not only accelerate the
computation (Shim et al., 2017) but also compress the model size (Sainath et al., 2013). However,
as we showed in Figure 2, matrices in FF layer of BERT models do not show obvious low-rank
structures. Ideally, we want a small percentage of the ranks which containing all large singular values
such that sum of singular values connected to the selected ranks divided by sum of all singular values
is large. But we could observe that choosing rank k to be larger than 50% of the ranks (e.g., about
0.5 times min(d1, d2)) could only accumulate 60 percent of the total singular values. This will lead
to a large approximation error. In the meantime, the complexity is still about O(d2d1) and there is no
enhancement of speed.
Despite the matrices in the model are not low-rank, here we provide an illustrative example to show
that a low-rank computation could still exist when data distribution lies in a lower intrinsic dimension.
Suppose we have a W defined as below and the input x lies in a subspace:
70231
9 6 7 5 0
W =61803
43214
12212
自
X ∈ span( 5
5
4
1
1
2 ),
2
6
In this case, W is a full-rank matrix so there won’t be a lossless low-rank approximation on W.
On the other hand, input data x lies in a 2-dimensional subspace such that we could construct the
following computation:
|
7
9
6
4
1
0
6
1
3
2
2
7
8
2
2
3
5
0
1
1
1
0
3
4
2
2
2
5
5
4
1
1
2
2
6
43
90
66
45
29
23
39
41
37
21
-1	-1 0.5 0.5	0
-0.5	0	0	0 0.25
'-----------------------------
{z
W
}|
{z^
VT
2
2
5
5
4
1
1
2
2
6
ab ,
z
x
} X------{----}
U
}
|
z
x
}
which gives a rank-2 matrix UVT where W 6= UVT but Wx = UVT x for any x in the low
dimensional space. This shows that even if we can’t approximate the W matrix, it is still possible to
construct a good low-rank decomposition, and the key will be to exploit the space of input vectors.
3.1	DRONE: Data-aware Low-rank Compression
Assume the input x of the FF Layer follows some distribution, instead of minimizing approximation
error of weight matrix (for which SVD is optimal), we want to minimize the approximation error of
the outputs. Denote X as the Rd1 ×n matrix where columns of X capture the empirical distribution of
4
Under review as a conference paper at ICLR 2021
input, our goal is to find a pair of projection matrix VX,k ∈ Rd1 ×k and recovery matrix UX,k ∈ Rd2×k
such that the output result is well approximated, and we could rewrite equation 1 as:
h=WX+b≈WUx,kVx,kTX+b= (WUx,k)Vx,kTX+b=Wx,kVx,kTX+b,
where WX,k = WUx,k. Intuitively, when X lies in a lower-dimensional space, we could find such
a pair by PCA decomposition on X to project X into subspace that explains most variance of X .
In this way, instead of considering the decomposition of W, we leverage the distribution of X to
complete the low-rank approximation. Certainly, the best way is to consider the properties of both
W and X simultaneously, and we could mathematically present this desideratum by the following
optimization problem:
minkWX - WMXk2F,	s.t. rank(M) = k,
(2)
where M is the desired rank-k transformation which could maximally preserve the results of matrix
multiplication in the computation. In the theorem below, we will show that there exists a closed-form,
optimal solution for the above optimization problem. Before stating the theorem, we first introduce
some notations. Assume rank(W) = r and rank(X)
XT = UX SX VXT such that
t, we can write W = UW SW VWT and
UW = [UW,r	UW,r] , SW
SW,r
0
0
0
SX,t
0
0
0
In other words, the decomposition UW SW VWT and UX SX VXT are the full-SVD decomposition of W
and XT. Uw,r,VW,r, Uχ,t,Vχ,t denote corresponding row spaces and column spaces. Uw,r , VW,r,
Ux,t and Vx,t are null spaces. With these notations, We are ready to state the theorem.
Theorem 1. Assume rank(W) = r and rank(X) = t. The closed form solution M* of the optimiza-
tion problem in equation 2 is
M* = VW,rSW-1,rZkSX-,1tVXT,t,	(3)
where Zk is the rank-k truncated SVD of Z = SW,rVWT,rVX,tSX,t.
The proof of Theorem 1 is postponed to the Supplementary A. We want to note that since Zk is the
rank-k truncated SVD of Z, we could also write Zk as UZ,kVZT,k by distributing top-k singular values
of Z into left or right singular matrices. Thus the original computation could be rewrote as:
WX ≈ (WVW,rSW-1,rUZ,k)(VZT,kSX-,1tVXT,t)X = U*V*X,	(4)
where we U* = W VW,r SW-1r UZ,k and V* = VZTkSX-1tVXTt are two rank-k matrices, and we will
replace W by U*V*.	,	,	,	,
3.2	Extension to Dot-product Attention
Although the optimization problem in equation 2 is proposed for feed-forward computation, in this
section we show that it can also be applied to dot-product part of the attention module too. The most
important computation in Attention layer is to compute pairwise similarity between query and key of
the sequence, and this could be described as:
O = (QY)T (KY),	(5)
where Y ∈ Rd1 ×n is the batch query data, Q ∈ Rd2 ×d1 is the query transformation matrix, Y ∈
Rd1 ×m is the batch key data, K ∈ Rd2 ×d1 is the key transformation matrix and n, m are query
and key batch size. We could again see that the desired low-rank approximation is the solution of
following optimization problem:
min Il(QY)T(KY) - (QY)TM(KY)∣∣F, s.t. rank(M) = k.	(6)
With QY = W and KY = X, we get the following corollary from Theorem 1 directly.
Corollary 2. Assume rank(QY) = r and rank(KY) = t. Denote QY = UWSWVW and (KY)T =
UxSχ VT the SVD decomposition of QY and (KY)T respectively. The closed form solution M*
of the optimization problem in equation 6 is M* = VW,rSW-1,rZkSX-,1rVXT,r, where Zk is the rank-k
truncated SVD of Z = SW,rVWT,rVX,tSX,t .
5
Under review as a conference paper at ICLR 2021
Algorithm 1: Data-Aware Low-rank Compression of Feed-forward layer.
Input: rank k; training data Dtrain; Original weight matrix W ; Prediction Model M
Output: Low-rank Approximation U*,V*
1X={}
2	for X = 1, ∙∙∙ ,d in Dtrain do
3	Feed the training data x into M and extract the representation φ(x). φ(x) is the
representation which will be multiplied with W .
4	Append φ(χ) to X.
5	Given X,k and W, solve the optimal low-rank matrices U*,V* by equation 4.
3.3 Overall Algorithm
We have shown that the proposed DRONE method is a generic acceleration module applicable to all
parts of neural language models. We summarize the DRONE on feed-forward layer in Algorithm 1.
Since in practice we don’t have the exact distribution of X, we would use training dataset to calculate
the low-rank approximations as described in Algorithm 1. Attention map could be calculated by
the same procedure with W = (QY)T and X = KY as introduced above. To accelerate the whole
model, we need to select appropriate ranks for each components. However, since the approximation
of one component will affect the distribution of overall representations, the optimal rank for the model
requires a complete search of all possible combinations of rank values, which is infeasible in practice.
We thus resort to an intuitive simplification as shown in Algorithm 2 listed in the supplementary B. In
short, as the changes of lower layer parameters will cause the distribution of representation shifts
in upper layer, we will approximate each component in ordered sequence. In other word, we will
approximate the model from the lower layers toward higher layers. Within each layer, we follow the
computational sequence of underlying modules. There is a total budget parameter r as an input to
the Algorithm 2. The total allowed budget r depends on the efficiency and efficacy trade-off which
users are willing to pay. We will distribute r into each module Rl,i (allowed loss increase ratio of i-th
module of l-th layer in Algorithm 2). For each module, if the approximation with certain rank used
won’t increase the loss over the ratio (1 + Rl,i), we will use that rank to approximate the module and
move on to the next module. The distribution from r to each Rl,i is based on empirical inference time
of each module. The longer a module takes to compute, the more budget would be allocated such
that total allowed loss increase (1 + r) = Ql Qi(1 + Rl,i). A sample of pseudo code is provided in
the supplementary to illustrate the process.
4	Experiments
4.1	Experimental Setup
We evaluate DRONE on both LSTM model and transformer-based BERT model. For LSTM, we
train a 2-layer LSTM-based language model on PTB from scratch with hidden sizes 1, 500 on Penn
Treebank Bank (PTB) dataset. For BERT models, we evaluate the pre-trained BERT models on
GLUE tasks. Various pre-trained models are offered in the open source platform (Wolf et al., 2019).
For BERT models, we use BERT-base models and it contains 12 layers of the same model structure
without sharing parameters. Each layer contains an attention module with hidden size 768 and 12
channels, a small 768 × 768 Feed-forward (FF) layer followed by 2 larger FF layers (768 × 3, 072
and 3, 072 × 768). As shown in Figure 1, these four components contribute to the most computational
time in the BERT-base models.
To the best of our knowledge, we are the first work to discuss the generic matrix computation acceler-
ation on NLP tasks. Therefore, our baseline comparison will be the SVD low-rank approximation.
We will also include those state-of-the-art distillation methods TinyBERT (Jiao et al., 2019) in the
comparison and show that the proposed method could be combined with it to further improve the
performance. TinyBERT reduces the model into 4 layers of attention dimension 312 with 12 channels,
and the FF layers are downsized to 312 × 1, 200.
As we mentioned above that all the approximation methods need to consider efficiency and efficacy
trade-off. In principle, we would have a plot of accuracy versus speedup ratio as shown in Figure
3 for MRPC and SST-2 tasks. The allowed accuracy loss is up to tolerance of individual users. In
this paper, we will report the approximation results with about 3% loss of accuracy. The inference
6
Under review as a conference paper at ICLR 2021
Methods	MNLI	QQP	SST-2	QNLI	MRPC	RTE	CoLA	STS-B
Original	84.3	90.9	92.3	91.4	89.5	72.6	53.4	87.8
SVD	74.4	50.8	73.1	52.2	63.8	47.3	26.0	0.13
DRONE	82.0	89.4	90.0	88.5	86.7	70.0	52.5	85.8
DRONE-Retrain	82.6	90.1	90.8	89.3	88.0	71.5	53.2	87.8
Speedup Ratio	1.60x	1.25x	1.64x	1.20x	1.92x	1.31x	1.33x	1.52x
Table 1: The experimental results of natural language inference tasks on Glue dataset.
Tasks	Self-Attention	Feed-Forward 0	Feed-Forward 1	Feed-Forward 2	Others	Total Time
MNLI	1227	19.5	78.5	461	-42-	-26870-
QQP	131.5	29.9	99.2	66.5	-578-	-33370-
SST-2	100.5	247	79.3	54.5	-^75-	-26376-
QNLI	128.3	28.4	111.0	7970	-579-	-35276-
MRPC	82.6	12.8	89.4	3872	-274-	-22573-
RTE	116.0	25.6	85.4	6273	-374-	-29277-
CoLA	108.2	22.7	9371	7078	-374-	-29873-
STS-B	109.1 ―	19.3	90.8	53.0	4.0	276.2
Table 2: The detailed average inference time of each component in the model by retrained DRONE
method. The unit is in millisecond.
Tasks	Models	Self-Attention	Feed-Forward 0	Feed-Forward 1	Feed-Forward 2	Others	Total Time	speedup	Accuracy (%)
STS-B	-BERT	13977	3275	13477	10970	~^A-	42072	1x	8778
	TinyBERT	1872	270	977	672	-06-	3677	11.4x	8679
	DRONE	1671	17	775	374	-078-	295	14.2X	870
RTE	-BERT	12473	2777	12970	10175	-31-	38575-	1x	7276
	TinyBERT	702	1672	6674	5473	-31-	2TU72	1.8x	708
	DRONE	638	1676	5879	5070	-278-	1850	-21X-	717
MRPC	-BERT	13Γ76	2979	14074	12776	-370-	43275	1x	8975
	TinyBERT	1876	19	979	61	-07-	3772	11.6x	8673
	DRONE	1774	178	979	575	-07-	353	12.3x-	867
SST-2	-BERT	14673	361	13376	1T070	^3-	43173	1x	9273
	TinyBERT	181	19	979	672	-07-	368	11.7x	907
	DRONE	14.4	1.9	7.8	3.5	0.5	28.1	15.3x	90.7 —
Table 3: The average inference time of comparison to distilled models. The unit is in millisecond.
speed is measured on an Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz chip with single-thread.
Inference are performed in a batch fashion with size 100 of the whole evaluation dataset. Average
single sequence prediction inference time in millisecond is reported in the results. To perform the
approximation, we randomly sub-sample 10% of the training data to be the training distribution used.
After the proposed data-aware low-rank distribution, we could slightly fine-tune the model to make
the performance better. We use a relatively smaller learning rate 10-7 and retrain 1 epoch on the
sub-sampled training data to complete the fine-tuning procedure.
4.2	Results of BERT Models on GLUE Dataset
We summarize the results of the DRONE on GLUE tasks in Table 1. We could observe that each
task exhibits different difficulty. The best acceleration we could achieve is 92% faster with less
than 2% accuracy loss after retraining (MRPC). In addition, DRONE achieves 1.52x acceleration
without accuracy loss on the STS-B task. By applying the same selected rank for each module
with SVD method, we could observe that the performance drops significantly. This shows that the
matrices within the model is generally not low-rank; thus the direct low-rank approximation without
considering data distribution won’t work. The detailed average inference time of the approximated
models are listed in Table 2. We could observe that the FF2 layer could be accelerated most. A
plausible reason could be that the input dimension to the FF2 layer is in a larger dimension (3072)
than all the other layers (64 or 768). When the input distribution actually lies in a lower-dimensional
space, there is much more room for FF2 layer to be compressed and accelerated by the data-aware
low-rank method.
4.3	Combination with Model Size Reduction Methods
Distillation is a competitive method to compress the underlying model into a smaller one without
losing much accuracy. Distilled models are much smaller in number of layers or hidden dimension,
resulting a smaller model size and faster inference time. As shown in the Table 3, TinyBERT, one of
the most competitive distillation methods, indeed achieves good performance within 3% accuracy
7
Under review as a conference paper at ICLR 2021
Tasks	Models	Total Time	speedup	Accuracy (%)
STS-B	-BERT-	-3.17 ms-	1X	878
	TinyBERT	-0.37 ms-	8.6x	8619
	DRONE	-0.29 ms-	10.9x	87.0
RTE	-BERT-	3.34 ms	1X	726
	TinyBERT	-1.73 ms-	-19X-	708
	DRONE	-1.51 ms-	-22X-	71.7
MRPC	-BERT-	3.28 ms	1X	89.5
	TinyBERT	0.42 ms	7.8x	863
	DRONE	-0.38 ms-	8.6x	867
SST-2	-BERT-	3.29 ms	1X	923
	TinyBERT	-0.39 ms-	-814X-	90.7
	DRONE	-0.34 ms-	9.7x	90.7	―
Table 4: The average inference time of comparison to distilled models on GPU. The unit is in
millisecond.
loss for some of the GLUE tasks. We want to emphasize that DRONE is orthogonal to distillation
methods. Since DRONE is generic, it could be combined together with other methods to further
improve the performance. Due to the fact that the computation inside the distilled model is still full
matrix computation, DRONE could then be applied to find the data-aware low-rank approximation of
these smaller matrices. Results are summarized in Table 3. As we can see that combined with the
distillation method, DRONE could further reduce the inference time without sacrificing accuracy. In
particular, on SST-2 task DRONE speedups the inference time from 11.7x to 15.3x while achieving
the same accuracy as the TinyBERT. These results again show that the proposed method is generic
and has the potential to be applied under various scenarios.
4.4	Results on LSTM models
We demonstrate that DRONE could also work on accelerating matrices in the LSTM model. As
shown in Table 5 in the Supplementary C, DRONE could accelerate 2-layer LSTM models about
3.4x on PTB dataset. And the result is slightly better than SVD methods. After the fine-tuning,
DRONE could achieve less than 1% accuracy loss. This shows that DRONE is generic. As long as
the underlying computation is a matrix multiplication, DRONE could leverage the data distribution to
obtain a better low-rank approximated computation.
4.5	Could the low-rank Structure learned by End-to-end training?
One natural question to ask is if once the rank is decided, the same optimal low-rank structure could
be learned by end-to-end fine-tuning. We have conducted the experiments on MRPC to verify this,
and the results are summarized in Table 6 in the Supplementary D. We start the fine-tuning from the
SVD results, and use the fine-tuning hyper-parameters as in (Wolf et al., 2019)1. After fine-tuning,
accuracy goes from 63.8% to 85.8% which is still slightly worse than DRONE. This shows that
fine-tuning on the SVD result might not achieve the best low-rank result. The proposed method under
the the optimization problem (equation 2) indeed provides a good initial solution to the data-aware
low-rank problems.
5	Conclusion
In this work, we propose DRONE, a data-aware low-rank approximation, to achieve a better low-rank
approximation. DRONE leverages the fact that data distribution in NLP tasks usually lies in a
lower-dimensional subspace. By including the data distribution into consideration, we propose a
data-aware low-rank approximation problem and provide an closed-form solution. Empirical results
validates that DRONE could significantly outperformed the vanilla-SVD method. It could achieve
at least 20% acceleration with less than 3% accuracy loss. When combined with the distillation
methods, DRONE could achieve 15.3 times acceleration with less then 2% accuracy loss.
1https://huggingface.co/transformers/v2.1.1/examples.html#glue
8
Under review as a conference paper at ICLR 2021
References
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level
language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33,pp. 3159-3166, 2019.
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and
Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. arXiv preprint
arXiv:2007.12223, 2020.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh M Raje, Venkatesan T Chakaravarthy, Yogish
Sabharwal, and Ashish Verma. Power-bert: Accelerating bert inference via progressive word-vector
elimination.
Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural network
with pruning, trained quantization and huffman coding. CoRR, abs/1510.00149, 2015a. URL
http://arxiv.org/abs/1510.00149.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efficient neural networks. CoRR, abs/1506.02626, 2015b. URL http://arxiv.org/abs/
1506.02626.
Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.
arXiv preprint arXiv:1801.06146, 2018.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. arXiv
preprint arXiv:1609.07061, 2016.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351,
2019.
Nikita Kitaev, Eukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv
preprint arXiv:2001.04451, 2020.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu
Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint
arXiv:1909.11942, 2019.
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy. Fixed point quantization of deep convolu-
tional networks. In International Conference on Machine Learning, pp. 2849-2858, 2016.
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi Ju. Fastbert: a self-distilling
bert with adaptive inference time. arXiv preprint arXiv:2004.02178, 2020.
Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang,
Yunhai Tong, and Jing Bai. Ladabert: Lightweight adaptation of bert through hybrid model
compression. arXiv preprint arXiv:2004.04124, 2020.
Emma Ning. Microsoft open sources breakthrough optimizations for transformer inference on
gpu and cpu. https://cloudblogs.microsoft.com/opensource/2020/01/21/
microsoft-onnx-open-source-optimizations-transformer-inference-gpu-cpu/,
2020. Accessed: 2010-09-30.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners.
9
Under review as a conference paper at ICLR 2021
Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-
rank matrix factorization for deep neural network training with high-dimensional output targets. In
Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp.
6655-6659. IEEE, 2013.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Victor Sanh, Thomas Wolf, and Alexander M Rush. Movement pruning: Adaptive sparsity by
fine-tuning. arXiv preprint arXiv:2005.07683, 2020.
Ahmad Shawahna, Sadiq M Sait, and Aiman El-Maleh. Fpga-based accelerators of deep learning
networks for learning and classification: A review. IEEE Access, 7:7823-7859, 2018.
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,
Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep
learning for supercomputers. In Advances in Neural Information Processing Systems, pp. 10414-
10423, 2018.
Kyuhong Shim, Minjae Lee, Iksoo Choi, Yoonho Boo, and Wonyong Sung. Svd-softmax: Fast
softmax approximation on large vocabulary neural networks. In Advances in Neural Information
Processing Systems, pp. 5463-5473, 2017.
Vikas Sindhwani, Tara Sainath, and Sanjiv Kumar. Structured transforms for small-footprint deep
learning. In Advances in Neural Information Processing Systems, pp. 3088-3096, 2015.
Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model
compression. arXiv preprint arXiv:1908.09355, 2019.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a
compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020.
Thomas Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf,
M Funtowicz, et al. Huggingface’s transformers: State-of-the-art natural language processing.
ArXiv, abs/1910.03771, 2019.
Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compressing
bert by progressive module replacing. arXiv preprint arXiv:2002.02925, 2020.
Y. You, Y. He, S. Rajbhandari, W. Wang, C.-J. Hsieh, K. Keutzer, and J. Demmel. Fast lstm inference
by dynamic decomposition on cloud systems. In ICDM, 2019.
Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low
rank and sparse decomposition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7370-7379, 2017.
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv
preprint arXiv:1910.06188, 2019.
Biao Zhang, Deyi Xiong, and Jinsong Su. Accelerating neural transformer via an average attention
network. arXiv preprint arXiv:1805.00631, 2018.
10
Under review as a conference paper at ICLR 2021
Supplementary Material for:
Data-Aware Low-Rank Compression for Large NLP Models
A	Proof of Theorem 1
Theorem 1. Assume rank(W) = r and rank(X) = t. The closed form solution M* of the optimiza-
tion problem in equation 2 is
M* = VW,rSW-1,rZkSX-,1tVXT,t,	(7)
where Zk is the rank-k truncated SVD of Z = SW,rVWT,rVX,tSX,t.
Proof. We firstly consider the unconstrained problem:
M* = argmin kW X - WMXk2F
M
=argminkUWTWXUX -UWTWMUXk2F
M
=argminkSWVWTVXSX -SWVWTMVXSXk2F,
M
where the second equality holds due to the fact that UW and UX are orthonormal matrices. Note that
we could expand the term SW VWT VXSX as:
SWVWVxSX = SW,r 0	VWF [Vχ,t Vχ,t] SX,t 0
SW,rVWT r 0	VX,tSX,t 0
=	0,0	0	0
SW,rVWT,rVX,tSX,t 0
=	0	0.
Similarly, we will have
SW VWT MVXSX = SW,rVWT,rM0 VX,tSX,t 00 .
Therefore, we could continue above unconstrained problem as:
M* =argminkSWVWT VXSX -SWVWT MVXSXk2F
M
=	argmin k SW,rVWT,rVX,tSX,t - SW,r VWT,r M VX,t SX,t 0 k2
M	0	0F
=	argmin kSW,rVWT,rVX,tSX,t - SW,rVWT,rMVX,tSX,t k2F .
=	argmin kZ - SW,rVWT,rM VX,tSX,tk2F.
The above minimization problem obtains the optimal value if SW,rVWT,rMVX,tSX,t equals the rank-k
truncated SVD of Z by the fundamental property of SVD decomposition. Thus, we will have:
Zk = SW,rVWT,rM*VX,tSX,t =⇒ M* = VW,rSW-1,rZkSX-,1tVXT,t.
□
11
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
B An algorithm to Search of Ranks under DRONE
In Algorithm 2, we illustrate how to select the rank of each module by applying DRONE illustrated
in Algorithm 1. The input to Algorithm 2 consists of training data, the model with all parameters of
weight matrices and original training loss. In addition, a pre-defined search grid is also necessary.
Taking W ∈ R768×768 as an example, we can perform a grid search for a proper low rank k over
[1, 768] such as {96, 192, 288, 384, . . . , 768}. The finer the grid, the more compressed model we
could get at the cost of longer running time of the DRONE method. With these input parameters, we
firstly distribute the total allowed loss into each individual module. We then iteratively apply
Algorithm 1 following the computational sequence illustrated in Figure 1. For the compression of
each module, we search the rank k by going through the grid. If the approximated result will not
increase the allowed loss increase ratio of the component, we will end the search and tie the found
rank to the component and move on. The procedure will continue until all components are
compressed. The whole process could guarantee Us that the final loss L0 of the compressed model MM
would not be greater than (1 + r)L, where L is the original loss before approximation.
Algorithm 2: Overall Algorithm of Grid Search of Low-rank Model Approximation
Input: training data Dtrain Original weight matrix W; Prediction Model M, total allowed loss
increase ratio r, Search grids of ranks for each module G, original Training loss L,
Output: Low-rank Model M
R J Distribute allowed ratio r into each module.
for l = 1,…，total layers do
foreach module mi ∈ Ml do
Wl,i J l-th layer parameter of module mi (e.g., 2nd feed-forward matrix in first layer.)
for i = 1, ∙∙∙ , ∣Gι,i∣ do
k J Gl,i
U,V J Algorithm 1 (k, Dtrain, Wl,i, M)
M J M with Wι,i replaced by U, V.
Evaluate new loss Lnew = M(Dtrain)
if Lnew/L < 1 + Rl,i then
M J M
break
12
Under review as a conference paper at ICLR 2021
LO	L2	L4	L6	L8
Inference Time Speedup Ratio
(a)
Efficiency and Efficacy Tradeoff of DRONE on SST-2
Figure 3: Illustration of efficiency and efficacy trade-off. Each point in this graph represents a specific
ratio of training loss increase after approximation.
C Efficiency and Efficacy Trade-off Graph
D LSTM result
Models	LSTM-I	LSTM-2	Softmax	Others	Total Time	Perplexity
PTB-Large	1.27ms	1.30ms	1.09ms	0.13ms	3.79ms	-7832-
PTB-Large-SVD	-	-	-	-	-	81.09
PTB-Large-DRONE	-	-	-	-	-	80.87
PTB-Large-DRONE-Retrain	0.24ms	0.34ms	0.42ms	0.11ms	1.11ms(3.4x)	79.01
Table 5: The average inference time of each component in the model of 2-layer LSTM model. Both
proposed methods and SVD use same ranks so the inference time is approximately the same. The
unit is in millisecond and the number in parenthesis shows the ratio respective to the overall inference
time.
E Results of SVD-based Retraining
Models	Accuracy (%)
BERT-MRPC	895
-BERT-MRPC-DRONE-	868
BERT-MRPC-SVD	638
BERT-MRPC-SVD-Retrain-	85.8	—
Table 6: Illustration of SVD fine-tuning. Using the same rank as the proposed method, SVD accuracy
will drop significantly. After fine-tuning on the SVD-based approximation, the accuracy could be
recovered. But it’s still less competitive than the proposed method.
F Python Pseudo Code of Solving equation 2
1	import numpy as np
2
3	def OPTsolver(x,y,k):
4	’’’
5	compute the best rank k projection M such that \| x*y' - x*M*y'\|_{F
} is minimized
6	X \in shape n x d
7	y \in shape m x d
8
13
Under review as a conference paper at ICLR 2021
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
XSS = np.matmul(x.transpose(),x)
kSS = np.matmul(y.transpose(),y)
U1,S1,V1 = np.linalg.svd(xSS,False)
S1 = S1 ** 0.5
I1	= np.eye(S1.shape[0])
U2,S2,V2 = np.linalg.svd(kSS,False)
S2 = S2 **0.5
I2	= np.eye(S2.shape[0])
YK = np.dot(np.dot(I1*S1,V1),np.dot(V2.transpose(),I2*S2))
U,S,V = np.linalg.svd(YK,False)
L = np.dot(V1.transpose(),I1*(1/S1))
R = np.dot(I2*(1∕S2),V2 )
M = np.dot(U[:,:k]*S[:k],V[:k,:])
return L,R,U,S,V
Listing 1: The python function to solve the equation 2.
G Python Pseudo Code of Rank Searching
import os
import numpy as np
import torch
import subprocess as Sp
cuda_num = 7
n_heads = 12
total_layer = 12
prev_loss = .11159391902588509 # Initial Loss
the_model_name = ' bertSST2 ’
time_attn = 117.5 # Empirical Inference Time on Attention Module
time_0 = 34.27 # Empirical Inference Time on Attention FFL Module
time_1 = 133.11 # Empirical Inference Time on Feedforward 1 layer
time_2 = 128.84 # Empirical Inference Time on Feedforward 2 layer
minimal_time = min(time_attn,time_0,time_1,time_2)
multiplier = (time_attn+time_0+time_1+time_2)/(minimal_time)
tolerant = 2. # allowed loss increase ratio. $r$ in Algorithm 2.
#	Code to Distribute the $r$ into individual Modules.
#	The distribution depends on empirical inference time of each module and
number of layers.
basic_tolerance = np.exp(np.log(tolerant)/multiplier)
tol_attn = np.exp(np.log(basic_tolerance**(time_attn/minimal_time))/
n_layer)
tol_0 = np.exp(np.log(basic_tolerance**(time_0/minimal_time))/n_layer)
tol_1 = np.exp(np.log(basic_tolerance**(time_1/minimal_time))/n_layer)
tol_2 = np.exp(np.log(basic_tolerance**(time_2/minimal_time))/n_layer)
#	### Omitted Code ###
#	This part of the code is to change some parameters of the underlying
hugginface framework in order to extract the training distribution X
of each module from the model.
#	## Omitted Code ###
for i in range(total_layer):
for each module in the layer: # This line is pseudo code for clarity
reason .
#	This part of the code extracts $R_{l,i}$(named the_tol here) in
Algorithm 2.
if save_symbol == "E":
14
Under review as a conference paper at ICLR 2021
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
the_tol = tol_attn
elif save_symbol == "F0":
the_tol = tol_0
elif save_symbol == "F1":
the_tol = tol_1
else:
the_tol = tol_2
#	Update the allowed increase of loss
prev_loss = prev_loss * the_tol
rank = 16 if save_symbol == "E" else 96 # initial search rank for
Attention (16) and FFL layers(96)
tps = 64 if save_symbol == "E" else 768 # Maximal rank specified
in the original models.
while rank <= tps:
#	## Omitted Code ###
#	# Write the tried rank into hugginface framework##
#	## Omitted Code ###
#	This line run the inference in the command line
os.system(’ CUDA_VISIBLE_DEVICES = "’ +str(cuda_num)+’" python
run_glue.py ——model_type bert ——model name or_path /data/TinySeries/
SST2/OriginalSST2/ --task_name SST-2 --do_eval --data_dir /data/
glue_data/SST-2/ ——OutPut_dir /tmp/sst-2 ——per_gpu_eval_batch_size
10 0 ——per_gpu_train_batch_size 10 0 ——max seq_length 128 > /tmp/tmp0 ')
with open('/tmp/tmp0‘，‘r‘)as file:
data = file.readlines()
new_r = float(data[-1])
if new_r < prev_loss:
break
if save_symbol == "E": # Attention module, We increase search
rank 16 at a time.
rank += 16
else :
#rank += 96 # For FFL layer, We increase search rank 96 a
time .
if rank ==	384:
rank = 768
break
else :
rank += 96
### Omitted Code ###
# This part of code update the model #
Listing 2: A mixed of real code and pseudo code to illustrate the search algorithm.
15