Under review as a conference paper at ICLR 2021
Prior Preference Learning from Experts:
Designing a Reward with Active Inference
Anonymous authors
Paper under double-blind review
Ab stract
Active inference may be defined as Bayesian modeling of a brain with a biolog-
ically plausible model of the agent. Its primary idea relies on the free energy
principle and the prior preference of the agent. An agent will choose an action
that leads to its prior preference for a future observation. In this paper, we claim
that active inference can be interpreted using reinforcement learning (RL) algo-
rithms and find a theoretical connection between them. We extend the concept
of expected free energy (EFE), which is a core quantity in active inference, and
claim that EFE can be treated as a negative value function. Motivated by the con-
cept of prior preference and a theoretical connection, we propose a simple but
novel method for learning a prior preference from experts. This illustrates that
the problem with inverse RL can be approached with a new perspective of active
inference. Experimental results of prior preference learning show the possibility
of active inference with EFE-based rewards and its application to an inverse RL
problem.
1 Introduction
Active inference (Friston et al., 2009) is a theory emerging from cognitive science using a Bayesian
modeling of the brain function (Friston et al., 2006; Friston, 2010; Friston et al., 2015; 2013), predic-
tive coding (Friston et al., 2011; Lopez-Persem et al., 2016), and the free energy principle (Friston,
2012; Parr & Friston, 2019; Friston, 2019). It states that the agents choose actions to minimize an
expected future surprise (Friston et al., 2012; 2017a;b), which is a measurement of the difference be-
tween an agent’s prior preference and expected future. Minimization of an expected future surprise
can be achieved by minimizing the expected free energy (EFE), which is a core quantity of active
inference. Although active inference and EFE have been inspired and derived from cognitive science
using a biologically plausible brain function model, its usage in RL tasks is still limited owing to its
computational issues and prior-preference design. (Millidge, 2020; Fountas et al., 2020)
First, EFE requires heavy computational cost. A precise computation of an EFE theoretically av-
erages all possible policies, which is clearly intractable as an action space A and a time horizon T
increase in size. Several attempts have been made to calculate the EFE in a tractable manner, such as
limiting the future time horizon from t to t + H (Tschantz et al., 2019), and applying Monte-Carlo
based sampling methods (FoUntas et al., 2020; CataI et al., 2020) for the search policies.
Second, it is unclear how the prior preferences should be set. This is the same question as how to
design the rewards in the RL algorithm. In recent studies (FountaS et al., 2020; CataI et al., 2020;
Ueltzhoffer, 2018) the agent,s prior preference is simply set as the final goal of a given environment
for every time step. There are some environments in which the prior preference can be set as time
independent. However, most prior preferences in RL problems are neither simple nor easy to design
because prior preferences of short and long-sighted futures should generally be treated in different
ways.
In this paper, we first claim that there is a theoretical connection between active inference and RL al-
gorithms. We then propose prior preference learning (PPL), a simple and novel method for learning
a prior-preference of an active inference from an expert simulation. In Section 2, we briefly intro-
duce the concept of an active inference. From the previous definition of the EFE of a deterministic
policy, in Section 3, we extend the previous concepts of active inference and theoretically demon-
strate that it can be analyzed in view of the RL algorithm. We extend this quantity to a stochastic
1
Under review as a conference paper at ICLR 2021
policy network and define an action-conditioned EFE for a given action and a given policy network.
Following Millidge (2020) using a bootstrapping argument, we show that the optimal distribution
over the first-step action induced from active inference can be interpreted using Q-Learning. Con-
sequently, we show that EFE can be treated as a negative value function from an RL perspective.
From this connection, in Section 4, we propose a novel inverse RL algorithm for designing EFE-
based rewards, by learning a prior preference from expert demonstrations. Through such expert
demonstrations, an agent learns its prior preference given the observation to achieve a final goal,
which can effectively handle the difference between local and global preferences. It will extend the
scope of active inference to inverse RL problem. Our experiments in Section 6 show the applicability
of active inference based rewards using EFE to an inverse RL problem.
2 active inference
The active inference environment rests on the partially observed Markov decision process settings
with an observation that comes from sensory input ot and a hidden state St which is encoded in
the agent’s latent space. We will discuss a continuous observation/hidden state space, a discrete
time step, and a discrete action space A: At a current time t < T with a given time horizon T ,
the agent receives an observation ot . The agent encodes this observation to a hidden state St in its
internal generative model, (i.e., a generative model for the given environment in an agent) and then
searches for the action sequence that minimizes the expected future surprise based on the agent’s
prior preference P(OT) of a future observation o「with τ >t. (i.e. The agent avoids an action which
leads to unexpected and undesired future observations, which makes the agent surprised.)
In detail, we can formally illustrate the active inference agent’s process as follows: St and ot are
a hidden state and an observation at time t, respectively. In addition, π = (a1, a2, ..., aT) is a se-
quence of actions. Let p(o1:T , S1:T ) be a generative model of the agent with its transition model
p(St+1 |St, at), and q(o1:T, S1:T, π) be a variational density. A distribution over policies q(π) will
be determined later. From here, we can simplify the parameterized densities as trainable neural net-
works with p(ot|St) as a decoder, q(St|ot) as an encoder, andp(St+1|St, at) as a transition network
in our generative model.
First, we minimize the current surprise of the agent, which is defined as - logp(ot). Its upper
bound can be interpreted as the well-known negative ELBO term, which is frequently referred to as
the variational free energy Ft at time t in studies on active inference.
-logp(ot) ≤ Eq(st|ot)[log q(st|ot) - logp(ot, st)] = Ft
(1)
Minimizing Ft provides an upper bound on the current surprise, and makes our networks in the
generative model well-fitted with our known observations of the environment and its encoded states.
For the future action selection, the total EFE G(St) over all possible policies at the current state St at
time t should be minimized.
G (St) = Eq(St+1:T ,ot+1:T ,π)[log /(I+:+:;+1) ]
(2)
Focusing on the distribution q(π), it is known that the total EFE G(St) is minimized when the
distribution over policies q(π) follows σ(-G∏(st)), where σ(∙) is a Softmax over the policies and
Gπ(St) is the EFE under a given state St for a fixed sequence of actions π at time t. (Millidge et al.,
2020)
Gn(St) = EGπ(τ, St) = EEq(Sτ,θτ∣∏)[log
τ>t
τ>t
q(STIn)]
P(θτ)q(Sτ∣Oτ)
(3)
This means that a lower EFE is obtained for a particular action sequence π; a lower future surprise
will be expected and a desired behavior p(o) will be obtained. Several active inference studies
introduce a temperature parameter γ > 0 such that qγ(π) = σ(-γGπ(St)) to control the agent’s
behavior between exploration and exploitation. Because we know that the optimal distribution over
the policies is q(π) = σ(-Gπ(St)), the action selection problem boils down to a calculation of the
expected free energy Gπ(St) of a given action sequence π.
The learning process of active inference contains two parts: (1) learning an agent’s generative model
with its trainable neural networks p(ot|St), q(St|ot), and p(St+1 |St, at) that explains the current
observations and (2) learning to select an action that minimizes a future expected surprise of the
agent by calculating the EFE of a given action sequence.
2
Under review as a conference paper at ICLR 2021
3 EFE as a negative value: B etween RL and active inference
In this section, we first extend the definition of an EFE to a stochastic policy. We then, pro-
pose an action-conditioned EFE that has a similar role as a negative action-value function in RL.
Based on these extensions, we will repeat the arguments in the active inference and claim that
the RL algorithm with EFE as a negative value is equivalent to controlling the policy network to-
ward an ideal distribution in the active inference. Calculating the expected free energy Gπ (st)
for all possible deterministic policies π is intractable even in a toy-example task because the num-
ber of policies rapidly increases as the time horizon T and the number of actions |A| increases.
Instead of searching for a number of deterministic policies, we extend the concept of EFE to a
stochastic policy based on a policy network φ = φ(at∣st). In this case, We also extend q(s"∏)
to q(sτ, aτ-ι∣φ) ：= p(sτ∣Sτ-ι,aτ-ι)φ(aτ-ι∣Sτ-ι), which is a distribution over the states and ac-
tions, where each action a「一ι is only dependent on state s「一ι with φ(aτ-ι|s「-ι). Plugging in a
deterministic policy φ = π yields q(s「a「一ι∣∏) = p(s"sτ-1,a0) with π(sτ-ι) = a0, which is the
same equation used in previous studies on active inference. Suppose we choose an action at based
on the current state st anda given action network φ, its corresponding expected free energy term can
then be written as follows. This can be interpreted as an EFE of a sequence of policies (φ, φ, ..., φ)
by substituting our extensions for the probabilities in (3).
Gφ(St)= EQτ>t q(sτ ,θτ ,aτ-.Φ)X log ；(0；a(XlO1	⑷
Note that the agent uses the same action network 0(a「|s「) for all T, and we can therefore rewrite
this equation in a recursive form.
Gφ(st) = EQτ>t q(sτ ,oτ ,aτ-1lφ) [X log P(θT⅛⅛⅛) ]
一Ilr	n	q(St+ι,atlφ)	l L i	q(ST,aτ-ilφ)1
=Qτ>t q(sτ ,oτ ,aτ-llφ)[log P(0t+l)q(st+1,at∣0t+l) + τ>t+1 log p(θτ )q(sτ ∣0τ)]
一Ilr	n	q(st+i,at|。)	l 1,7	q(sτ ,aτ-ιlφ)1
=q(St+1,θt+1,atM[log P(θt+ι)q(st+ι∣θt+ι) + Qτ>t+1 q(ST,θτ,aτ-1MNIlog p(θτ )q(sτ ∣0τ)]
τ >t+1
j	q(st+ι,atlφ)	y M
=Eq(St+1,θt+1,atM[log p(οt+ι)q(st+ι∣οt+ι) + Gφ(st+1)]
(5)
Replacing and fixing the first action at = a, an action-conditioned EFE Gφ(st |a) of a given action
a and a policy network φ is then defined as follows:
Gφ(StIa)= Eq(St+ι,θt+ι,at∣at = a)
q(St+1,at|at = a)
log P(θt+ι,st+ι)
+ Gφ(St+1)
Ep(St+1 |St,at=a)p(ot+1 |St+1 )
log P(St+1Mat = a)
.P(Ot+1)q(St+i|ot+1)
+ Gφ(St+1)
(6)
Taking an expectation over φ(aISt), we obtain the relationship between Gφ(St) and Gφ(StIat).
Eφ(a∣st )[Gφ(St∣a)] = Gφ(St) + H[Φ(a∣St)]	⑺
We may consider separating the distribution over the first-step action from φ to find an alternative
distribution that minimizes the EFE. Substituting the distribution over the first-step action as q(at)
instead of φ(at), we obtain its one-step substituted EFE as indicated below. This can be interpreted
as the EFE of a sequence of a policies (q(at), φ, ..., φ)
G1φ-Step(St)
Eq(at)q(St+1,ot+1|at) Qτ >t+1
q(sτ ,θτ∣Φ) [log
q(at)q(St+1∣at)q(St+2:T, at+i：T-ι∣Φ)
'P(St+1:T, Ot+1:T)
]
=Eq(Ot)q(St+1,ot+llat) Qτ>t+1 q(Sτ,θτlφ)[log q(at) + log Pqot++1St+1) + logT Yl j5(οT)qTSτK) ]
Eq(at)
logq(at) + Gφ(StIat)
(8)
3
Under review as a conference paper at ICLR 2021
Under a given φ, the value above depends only on the distribution q(at). Thus, minimizing the
quantity above will naturally introduce the distribution q*(at) = σ°(-Gφ(st∣at)), which is known
to be similar in terms of active inference to the γ = 1 case.
Gφ-step(st) = Eq(at) logq3) + Gφ(st∣at) = KL(q(at)∣∣q*(at)) - log E exp(-Gφ(st∣at))
at∈A
(9)
Therefore, Gφ-step(st) ≥ - log Pa古∈a exp(-Gφ(st%)), and the equality holds if and only if
q(at) = σa(-Gφ(st ∣at)). Let Us further consider the temperature hyperparameter γ > 0 such that
q;(at) = σa(-γG(st∣at)). Similarly to the above, We obtain the following:
GLstep(St)= Y Eq(at) Y lθg 9(at) + YG0(st|at)
=1 Eq(at) (1 - γ)(-logq(at)) + log q(at) - log qγ(at) - 1 log X exp(-γGφ(st∣at))
Y	Y	at∈A
(—1)H(q(at)) +—κL(q(at)llqY(at)) —log ^X eχp(-γGΦ(StIat))
Y	Y	γ Y	at∈A
(10)
From the arguments above, we can conclude that (1) G1φ-step(St) is minimized when q* (at) =
σa(-G(st∣at)), a ‘natural case, in which Y = 1, which was heuristically set in the experiments
described in Millidge (2020). (2) Plugging q；(at) = σa(-γG(st∣at)) to q(a) in the equation above,
we obtain
1-step(St) = (I- Y)EqY(a)[Gφ(St|at)] - log DY
≈ (1 - γ)Eqγ(a)[Gφ(st∣at)] + γ min{Gφ(st∣at)}
(11)
where DY = Ea=∈/ exp(-γGφ(st∣at)), and the last comes from the smooth approximation to the
maximum function of log-sum-exp. This approximation becomes accurate when the maximum is
much larger than the others.
-log DY = - log T exp(-γGφ(st∣at)) ≈ - max{-γGφ(st∣at)} = Y min Gφ(st%) (12)
at∈A	at∈A
at∈A
When Y = 1, the quantity Gφ-step(st) is minimized with q*(at) = σa (- G (s 11 at)), and its minimum
value can be approximated as - log Di ≈ minat∈∕{Gφ(st∣at)}. When Y → ∞, q；(at) can be
considered as a deterministic policy seeking the smallest EFE, which leads to the following:
G1-step(St) = (1 - 1)H(q; (at)) - 1log X exp(-YGφ(st∣at)) ≈ min Gφ(St∣at)	(13)
Y	Y	at∈A
at∈A
Note that Q-learning with a negative EFE can be interpreted as qY* with the case of Y = ∞. When
Y & 0, qY* (at) converges to a uniform distribution over the action space A, and the temperature
hyperparameter Y motivates the agent to explore other actions with a greater EFE. Its weighted sum
also converges to an average of the action-conditioned EFE.
Considering the optimal policy φ* that seeks an action with a minimum EFE, we obtain the follow-
ing:
Gφ*(St) = min Gφ*(St∣a)
a
min Ep(st+1 |st,at=a)p(ot+1 |st+1)
a
log p(St+i∣St,at = a)
.g P(Ot+1)9(%+1^+1)
+ Gφ* (St+i)
(14)
This equation is extremely similar to the Bellman optimality equation. Here, Gφ (St I a) and Gφ (St+i)
correspond to the action-value function and the state value function, respectively. Based on this sim-
ilarity, we can consider the first term log 状(：：+；(；W=21)as a one-step negative reward (because
active inference aims to minimize the expected free energy of the future) and EFE as a negative
value function.
4
Under review as a conference paper at ICLR 2021
4 PPL: Prior Preference Learning from Experts
p(st+1 |st ,at )
FroIm the PreVioUs section, We Verified that Using log P(O )q(s	∣0	)
as a negative reward can
handle EFE with traditional RL methods. ThroUgh a simple calcUlation, the giVen term can be
decomposed as follows:
-log	P(St+1Mat)
t :	g P(0t+1)q(st+1∣0t+1)
logP(θt+1) + (- log p(st+1 ∣st,at?) = Rt,i + Rt,e
q(st+1|ot+1)
(15)
The first term measUres the similarity between a preferred fUtUre and a predicted fUtUre, which is an
intUitiVe ValUe. The second term is called the epistemic ValUe, which encoUrages the exploration of
an agent. (Friston et al., 2015) The epistemic ValUe can be compUted either with prior knowledge on
the enVironment ofan agent or with VarioUs algorithms to learn a generatiVe model. (Igl et al., 2018;
Kaiser et al., 2019)
The core key to calculating the one-step reward and learning EFE is the prior preference p(ot), which
inclUdes information aboUt the agent’s preferred obserVation and goal. Setting a prior preference for
an agent is a challenging point for actiVe inference. A simple method that has been used in recent
studies (Ueltzhoffer, 2θl8; CataI et al., 2020) is to set a prior preference as a Gaussian distribution
with mean of the goal position. HoweVer, it would be inefficient to use the same prior preference
for all obserVations o. We called this type of preference global preference. Taking a mountain-
car enVironment as an example, to reach the goal position, the car must moVe away from the goal
position in an early time step. Therefore p(ot) must contain information about local preference. It is
also difficult to directly design the prior preference for some obserVation space, such as in an image.
To solVe the problems arising from actiVe inference, we introduce prior preference learning from
experts (PPL). Suppose that an agent can access expert simulations S = {(oi,1, ..., oi,T}iN=1, where
N is the number of simulations. From the expert simulations S, an agent can learn an expert’s
prior p(ot+1 |ot) based on the current obserVation ot. Model p(ot+1 |ot) captures the expert’s local
preference, which is more effectiVe than the global preference. We can state that this method can be
applied only with expert simulations, regardless of the knowledge on the forward dynamics of the
enVironment.
Once learning the prior preference p(ot+1 |ot), we can calculate Rt giVen in (15). We can use any
RL alogorithm to approximate EFE, Gφ* (st) in (14). In Algorithm 1, we provide our pseudo-code
of PPL with Q-learning.
5	Related works
Active inference on RL. Our works are based on actiVe inference and reinforcement learning.
ActiVe inference was first introduced in Friston et al. (2006), inspired from neuroscience and free
energy principle. It explains how a biological system is maintained with a brain model. Furthermore,
Friston et al. (2009) treated the relation between actiVe inference and reinforcement learning. Early
studies (Friston, 2010; Friston et al., 2011; 2015; 2017a) dealt with a tabular or model based problem
due to computational cost.
Recently, Ueltzhoffer (2018) introduced deep active inference which utilizes deep neural network to
approximate the obserVation and transition models on MountainCar enVironment. This work used
EFE as an objective function and its gradient propagates through the environment dynamics. To
handle this problem, Ueltzhoffer (2018) used stochastic weights to apply an evolutionary strategy
(Salimans et al., 2017), which is a method for gradient approximation. For a stable gradient approx-
imation, about 104 environments were used in parallel.
Catal et al. (2020); Millidge (2020) introduced end-to-end differentiable models by including envi-
ronment transition models. Both require much less interaction with environments than before. Catal
et al. (2020) used the Monte Carlo sampling to approximate EFE and used global prior preference.
On the other hand Millidge (2020) used bootstrapping methods and used common RL rewards with
model driven values derived from EFE. Millidge (2020) verified that the model driven values induce
faster and better results on MountainCar environment. Furthermore, Tschantz et al. (2020); Millidge
et al. (2020) introduced a new KL objective called free energy of expected future (FEEF) which is
related to probabilistic RL (Levine, 2018b; Rawlik, 2013; Kappen et al., 2012; Lee et al., 2019).
5
Under review as a conference paper at ICLR 2021
Algorithm 1: Inverse Q-Learning with Prior Preference Learning
Learning prior preference from expert simulations;
Input: Expert simultaions S = (oi,1, ...oi,T)iN=1
Initialize a prior preference network p (o);
while not converge do
Compute loss LppI(p(ot), ot+ι);
Update θ — θ - αVLppi
end
Output: pθ (o)
Learning EFE and forward dynamic of an environment;
Input: Prior preference pθ(o)
Initialize the forward dynamicPn(o|s), Trι(st+ι∣st, at), qη(s|o), and EFE network Gξ(st, at).
while not converge do
Reset environment;
for t — 0 do
Select action at = arg maxa G(st, a) with -greedy;
Observe new observation ot+1;
Comnute R+ 一 loσ∙	T(St+1|st，at)	∙
ComPUte Rt=Iog p(θt+ι)q(st+ι∣θt+ι),
Compute the environment model loss Lmodel ((P ◦ T ◦ q)(ot), ot+1);
Compute the EFE network loss Lefe(Gξ(st, at), Rt + maxa Gξ(st+1, a));
Update η — η — αVLd and ξ — ξ — αVLefe.
end
end
Our work extends the scope of active inference to inverse RL by learning a preferred observation
from experts. A common limitation of previous studies on active inference is the ambiguity of
prior preference distribution. Previous works have done in environments where its prior preference
can be naturally expressed, which is clearly not true in general. PPL is an active inference based
approach for the invser RL problem setting, which allows us to learn a prior preference from expert
simulations. This broadens the scope of active inference and provides a new perspective about a
connection between active inference and RL.
Control as inference. Levine (2018a) proposed control as inference framework, which interprets
a control problem as a probabilistic inference with an additional binary variable Ot that indicates
whether given action is optimal or not. Several studies on the formulation of RL as an inference prob-
lem (Todorov, 2008; Kappen et al., 2009) have been proposed. Control as inference measures a prob-
ability that a given action is optimal based on a given reward with P(Ot = 1|st, at) = exp(r(st, at)).
That is, from the given reward and the chosen probability model, control as inference calculates the
probability of the given action at with the state st to be optimal. In contrast, active inference mea-
sures this probability as in (15) with a prior preference distribution P and constructs a reward, view-
ing EFE as a negative value function. Although control as inference and active inference as a RL in
Section 3 have a theoretical similarity based on a duality ofa control problem and an inference, our
proposed PPL can interpret the reward r(st, at) and the message function β(st) in Levine (2018a)
as EFE-related quantities. Therefore, learning the expert as a prior preference P immediately con-
structs its active inference based reward function and thereby applicable to a reward construction
problem and several inverse RL problems.
6	Experiments
In this section, we discuss and compare the experimental results of our proposed algorithm PPL and
other inverse RL algorithms on the several classical control environments. We evaluate our approach
with a classic control environment implemented in Open AI Gym (Brockman et al., 2016). First,
we aim to compare the conventional global preference method to the PPL. We expect the PPL to
be effective in environments where the local and global preferences are different. Second, we claim
6
Under review as a conference paper at ICLR 2021
Table 1: Experment setting for PPL and global preference and its variants
	Preference	Batch sampling	Reward
Setting 1	PPL	Replay memory + Experts	Rt,i + Rt,e
Setting 2	PPL	Replay memory	Rt,i + Rt,e
Setting 3	PPL	Replay memory + Experts	Rt,i
Setting 4	Global preference	Replay memory + Experts	Rt,i + Rt,e
that our active inference based approach can achieve a compatible results with current inverse RL
algorithms. Expert simulations were obtained from Open AI RL baseline zoo (Raffin, 2018).
6.1	PPL and global preference
First, we compare our proposed algorithm (setting 1 in Table 1) and its variants. Table 1 contains four
experimental settings, where the setting 1 is our proposed method and the others are experimental
groups to compare the effects of an expert batch, the epistemic value Rt,e, and PPL.
Expert Batch. We use expert simulations for batch sampling when learning EFE and a forward
dynamic model. This allows an EFE network and a dynamic model to be trained even in states that
do not reach the early stage of learning. We use the Q-learning algorithm to learn the EFE network.
Commonly used techniques in RL, such as replay memory and target network (Mnih et al., 2015)
are used.
Reward. We observe how an epistemic value Rt,e in (15) influences the learning process and its
performance.
Global preference. Global preference is a distribution over a state which can be naturally induced
from the agent’s goal, whereas our PPL is a learned prior preference from the expert’s simulations.
Roughly, global preference can be understood as a hard-coded prior preference based on the prior
knowledge of the environment, as a ,goal directed behavior' in Ueltzhoffer (2018). Detailed hard-
coded global preferences in our experiments can be found in the below sub-subsection.
6.1.1	Environments and experiment details
We tested three classical control environments: Acrobot, Cartpole, and MountainCar. We used
5000 pairs of (ot, ot+ι) to train the prior preference p(ot+ι). We used the same neural network
architecture for all environments, except for the number of input and output dimensions. During
the training process, we clip the epistemic value to prevent a gradient explosion while using the
epistemic value in settings 1, 2, and 4.
Acrobot is an environment with a two-link pendulum, where the second link is actuated. The goal
of this environment is to swing the end effector up to the horizontal line. The state space has six
dimensions: (Cos θι, Sin θι, cos θ2, Sin Θ2,θ1,θ2),where θι and θ2 are the two rotationaljoint angles.
The action space consists of three actions: accelerating +1, 0, or -1 torque on the joint between the
two links. We did not run setting 4 in this study, because Acrobot is ambiguous in defining the global
preference of the environment.
Cartpole is an environment with a cart and a pole on the cart. The goal is to balance the pole by
controlling the cart. The state space is of 4 dimensions: (x, X, θ, θ), where X is the position of the
cart and θ is the angle of the pole from the vertical line. The action space consists of two actions:
accelerating the cart to the left or right. The global preference is given by a Gaussian distribution
with mean (0, 0, 0, 0) with a fixed standard deviation 0.1 on each dimension.
MountainCar is an environment with a car on the valley. The goal is to make the car reach the goal
position, the right peak of the mountain. The state space is of two dimensions (x, X) where X is the
position of the car. The action space consists of three actions: accelerating the car to the left or right,
or leaving it. The global preference is given by a Gaussian distribution with mean (0.5, 0) with a
fixed standard deviation 0.1 on each dimension. Note that (0.5, 0) is the state of the goal position
with zero velocity.
7
Under review as a conference paper at ICLR 2021
Figure 1: Experiment results on three classical control environments: MountainCar-v0, Acrobot-v1,
and CartPole-v1. The curves in the figure were averaged over 50 runs, and the standard deviation
of 50 runs is given as a shaded area. Each policy was averaged out of 5 trials. All rewards of the
environment follow the default settings of Open AI Gym.
6.1.2	Results and Discussions
Figure 1 shows the experimental results on MountainCar, Acrobot, and Cartpole. Their perfor-
mances are compared and benchmarked with the default reward of the environments.
PPL and global preference. Comparing setting 1 (blue line) and setting 2 (red line), it can be
seen that PPL is more efficient than the conventional global preference as expected. In particular,
in MountainCar, we can see that little learning is achieved. This seems to be because the difference
between global and expert preferences is greater in the MountainCar environment. In the Cartpole,
setting 2 learned more slowly than setting 1.
Expert Batch. Comparing setting 1 (blue line) and setting 3 (orange line), it can be seen that using
the expert batch is helpful for certain tasks. With Acrobot and MountainCar, the use of an expert
batch performs better than the case without an expert batch. However, the results without expert
batch are marginally better than those of setting 1 for Cartpole. This is because an agent of Cartpole
only moves near the initial position, and thus there is no need for an expert batch to discover the
dynamics of the generative model.
Epistemic Value. We found that the epistemic value in the EFE term does not significantly impact
the training process. Comparing setting 1 (blue line) and setting 4 (green line), the results were
similar regardless of whether the epistemic value was used. In Acrobot and MountainCar, standard
deviations were marginally smaller, but there were no significant differences between them. In the
result of CartPole-v1, we found that setting 4 with no epistemic value term learned faster than our
proposed setting 1 at the beginning of the training process. We deduce that this initial performance
drop is due to the instability of the epistemic term. At the beginning of the training process, the
generative model is not learned, and thus the related epistemic term becomes unstable. We leave
this issue to a future study.
6.2	PPL and inverse RL algorithms
Second, we check that our proposed PPL is compatible with traditional inverse RL algorithms. We
compared PPL with behavioral cloning (BC, Pomerleau (1989)) and maximum entropy inverse RL
(MaxEnt, Ziebart et al. (2008)) as benchmark models. We use setting 1 in Table 1 as our proposed
PPL here, and we test on MountainCar-v0 and CartPole-v1. Note that BC does not need to interact
with the environment and the state space was discretized to use original MaxEnt algorithm. We also
tried to run the experiment on Acrobot-v1 for PPL and other benchmarks, but we failed to make the
agent learn with MaxEnt. A discretized state space for MaxEnt becomes larger exponentially to its
state dimension. We think it is due to a larger dimension of its state space compared to the others.
Therefore, we only report that PPL and BC give similar results to Acrobot-v1.
We verified that our method PPL gives compatible results on the MountainCar-v0 and CartPole-
v1. Compared to MaxEnt, PPL shows better results than MaxEnt on both environments. Note that
MaxEnt needs much more episodes to converge. Also, PPL obtained almost similar mean rewards
to BC on MountainCar-v0, whereas BC gives better results than PPL on CartPole-v1.
8
Under review as a conference paper at ICLR 2021
Figure 2: Inverse RL Experiment results on MountainCar-v0 (left) and CartPole-v1 (right). The
curves in the figure were averaged over 50 runs, and the standard deviation of 50 runs is given as
a shaded area. Note that black and green dashed line on the right are overlapped. All rewards of
the environment follow the default settings of Open AI Gym. (BC : Behavioral Cloning, MaxEnt :
Maximum Entropy)
7	Conclusion
In this paper, we introduced the use of active inference from the perspective of RL. Although active
inference emerged from the Bayesian model of cognitive process, we show that the concepts of
active inference, especially for EFE, are highly related to RL using the bootstrapping method. The
only difference is that, the value function of RL is based on a reward, while active inference is
based on the prior preference. We also show that active inference can provide insights to solve the
inverse RL problems. Using expert simulations, an agent can learn a local prior preference, which is
more effective than the global preference. Furthermore, our proposed active inference based reward
with a prior preference and a generative model makes the previous invser RL problems free from an
ill-posed state. Our work on active inference is complementary to RL because it can be applied to
model-based RL for the design of reward and model-free RL for learning of generative models.
References
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Zafeirios Fountas, Noor Sajid, Pedro A. M. Mediano, and Karl J. Friston. Deep active inference
agents using monte-carlo methods. CoRR, abs/2006.04176, 2020. URL https://arxiv.
org/abs/2006.04176.
K. Friston, J. Kilner, and L. Harrison. A free energy principle for the brain. Journal of Physiology-
Paris, 100:70-87, 2006.
Karl Friston. The free-energy principle: a unified brain theory? Nature reviews. Neuroscience, 11:
127-38, 02 2010. doi: 10.1038/nrn2787.
Karl Friston, Spyridon Samothrakis, and Read Montague. Active inference and agency: Optimal
control without cost functions. Biological cybernetics, 106:523-41, 08 2012. doi: 10.1007/
s00422-012-0512-8.
Karl Friston, Philipp Schwartenbeck, Thomas Fitzgerald, Michael Moutoussis, Tim Behrens, and
Raymond Dolan. The anatomy of choice: active inference and agency. Frontiers in Human
Neuroscience, 7:598, 2013. ISSN 1662-5161. doi: 10.3389/fnhum.2013.00598. URL https:
//www.frontiersin.org/article/10.3389/fnhum.2013.00598.
9
Under review as a conference paper at ICLR 2021
Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Gio-
Vanni Pezzulo. Active inference and epistemic value. Cognitive Neuroscience, 6(4):187-214,
2015. doi: 10.1080/17588928.2015.1020053.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, and Giovanni Pez-
zulo. Active inference: A process theory. Neural Computation, 29(1):1-49, 2017a. doi:
10.1162∕NECO∖_a\一00912. URL https://doi.org/10.1162/NECO_a_00912. PMID:
27870614.
Karl J. Friston. A free energy principle for biological systems. Entropy, 14(11):2100-2121, 2012.
doi: 10.3390/e14112100. URL https://doi.org/10.3390/e14112100.
Karl J. Friston. A free energy principle for a particular physics, 2019. URL https://arxiv.
org/abs/1906.10184.
Karl J Friston, Jean Daunizeau, and Stefan J Kiebel. Reinforcement learning or active inference?
PloS one, 4(7):e6421, 2009.
Karl J. Friston, Jeremie Mattout, and James Kilner. Action understanding and active inference.
Biol. Cybern., 104(1-2):137-160, 2011. doi: 10.1007/s00422-011-0424-z. URL https://
doi.org/10.1007/s00422-011-0424-z.
Karl J. Friston, Marco Lin, Christopher D. Frith, Giovanni Pezzulo, J. Allan Hobson, and Sasha On-
dobaka. Active inference, curiosity and insight. Neural Computation, 29(10):2633-2683, 2017b.
doi: 10.1162∕neco∖ _a\ _00999. URL https://doi.org/10.1162/neco_a_0 0 999.
Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for pomdps. arXiv preprint arXiv:1806.02426, 2018.
Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad
Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based
reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.
Bert Kappen, Viceng Gomez, and Manfred Opper. Optimal control as a graphical model inference
problem. CoRR, abs/0901.0633, 2009. URL http://arxiv.org/abs/0901.0633.
Hilbert J Kappen, ViCenC Gomez, and Manfred Opper. Optimal control as a graphical model infer-
ence problem. Machine learning, 87(2):159-182, 2012.
Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Ruslan Salakhutdi-
nov. Efficient exploration via state marginal matching. arXiv preprint arXiv:1906.05274, 2019.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
CoRR, abs/1805.00909, 2018a. URL http://arxiv.org/abs/1805.00909.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
arXiv preprint arXiv:1805.00909, 2018b.
AIiZee Lopez-Persem, Philippe Domenech, and Mathias Pessiglione. How prior preferences de-
termine decision-making frames and biases in the human brain. Neuroscience, 5:e20317, 2016.
ISSN 2050-084X. doi: 10.7554/eLife.20317. URL https://doi.org/10.7554/eLife.
20317.
Beren Millidge. Deep active inference as variational policy gradients. Journal of Mathemat-
ical Psychology, 96:102348, 2020. ISSN 0022-2496. doi: https://doi.org/10.1016/j.jmp.
2020.102348. URL http://www.sciencedirect.com/science/article/pii/
S0022249620300298.
Beren Millidge, Alexander Tschantz, and Christopher L. Buckley. Whence the expected free energy?
CoRR, abs/2004.08128, 2020. URL https://arxiv.org/abs/2004.08128.
10
Under review as a conference paper at ICLR 2021
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin A. Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nat,518(7540):529-533, 2015. doi: 10.1038∕nature14236. URL https://doi.org/10.
1038/nature14236.
Thomas Parr and Karl J. Friston. Generalised free energy and active inference. Biological
Cybernetics, 113(5-6):495-513, 2019. doi: 10.1007/s00422-019-00805-w. URL https:
//doi.org/10.1007/s00422-019-00805-w.
Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural
information processing systems, pp. 305-313, 1989.
Antonin Raffin. Rl baselines zoo. https://github.com/araffin/rl-baselines-zoo,
2018.
Konrad Cyrus Rawlik. On probabilistic inference approaches to stochastic optimal control. 2013.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
Emanuel Todorov. General duality between optimal control and estimation. In Proceedings of the
47th IEEE Conference on Decision and Control, CDC 2008, December 9-11, 2008, Canoun,
Mexico, pp. 4286-4292. IEEE, 2008. doi: 10.1109/CDC.2008.4739438. URL https://doi.
org/10.1109/CDC.2008.4739438.
Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active
inference. CoRR, abs/1911.10601, 2019. URL http://arxiv.org/abs/1911.10601.
Alexander Tschantz, Beren Millidge, Anil K. Seth, and Christopher L. Buckley. Reinforcement
learning through active inference. CoRR, abs/2002.12636, 2020. URL https://arxiv.org/
abs/2002.12636.
Kai Ueltzhoffer. Deep active inference. Biol. Cybern., 112(6):547-573, December 2018.
ISSN 0340-1200. doi: 10.1007/s00422-018-0785-7. URL https://doi.org/10.1007/
s00422-018-0785-7.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pp. 1433-1438. Chicago, IL, USA, 2008.
O. CataL T. Verbelen, J. NaUta, C. D. Boom, and B. Dhoedt. Learning perception and planning
with deep active inference. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pp. 3952-3956, 2020.
11