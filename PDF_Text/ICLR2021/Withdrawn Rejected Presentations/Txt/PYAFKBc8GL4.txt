Under review as a conference paper at ICLR 2021
Client Selection in Federated Learning:
Convergence Analysis and Power-of-Choice
Selection Strategies
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning is a distributed optimization paradigm that enables a large
number of resource-limited client nodes to cooperatively train a model without
data sharing. Several works have analyzed the convergence of federated learning
by accounting of data heterogeneity, communication and computation limitations,
and partial client participation. However, they assume unbiased client participation,
where clients are selected at random or in proportion of their data sizes. In this
paper, we present the first convergence analysis of federated optimization for
biased client selection strategies, and quantify how the selection skew affects
convergence speed. We reveal that biasing client selection towards clients with
higher local loss achieves faster error convergence. Using this insight, we propose
Power-of-choice, a communication- and computation-efficient client selection
framework that can flexibly span the trade-off between convergence speed and
solution bias. We also propose an extension of Power-of-choice that is able to
maintain convergence speed improvement while diminishing the selection skew.
Our experiments demonstrate that Power-of-choice strategies can converge up
to 3× faster and give 10% higher test accuracy than the baseline random selection.
1	Introduction
Until recently, machine learning models were largely trained in the data center setting (Dean et al.,
2012) using powerful computing nodes, fast inter-node communication links, and large centrally
available training datasets. The future of machine learning lies in moving both data collection as well
as model training to the edge. The emerging paradigm of federated learning (McMahan et al., 2017;
Kairouz et al., 2019; Bonawitz et al., 2019) considers a large number of resource-constrained mobile
devices that collect training data from their environment. Due to limited communication capabilities
and privacy concerns, these data cannot be directly sent over to the cloud. Instead, the nodes locally
perform a few iterations of training using local-update stochastic gradient descent (SGD) (Yu et al.,
2018; Stich, 2018; Wang & Joshi, 2018; 2019), and only send model updates periodically to the
aggregating cloud server. Besides communication limitations, the key scalability challenge faced
by the federated learning framework is that the client nodes can have highly heterogeneous local
datasets and computation speeds. The effect of data heterogeneity on the convergence of local-update
SGD is analyzed in several recent works (Reddi et al., 2020; Haddadpour & Mahdavi, 2019; Khaled
et al., 2020; Stich & Karimireddy, 2019; Woodworth et al., 2020; Koloskova et al., 2020; Huo et al.,
2020; Zhang et al., 2020; Pathak & Wainwright, 2020; Malinovsky et al., 2020; Sahu et al., 2019)
and methods to overcome the adverse effects of data and computational heterogeneity are proposed
in (Sahu et al., 2019; Wang et al., 2020; Karimireddy et al., 2019), among others.
Partial Client Participation. Most of the recent works described above assume full client participa-
tion, that is, all nodes participate in every training round. In practice, only a small fraction of client
nodes participate in each training round, which can exacerbate the adverse effects of data heterogene-
ity. While some existing convergence guarantees for full client participation and methods to tackle
heterogeneity can be generalized to partial client participation (Li et al., 2020), these generalizations
are limited to unbiased client participation, where each client’s contribution to the expected global
objective optimized in each round is proportional to its dataset size. In Ruan et al. (2020), the authors
analyze the convergence with flexible device participation, where devices can freely join or leave the
1
Under review as a conference paper at ICLR 2021
training process or send incomplete updates to the server. However, adaptive client selection that is
cognizant of the training progress at each client has not been understood yet.
It is important to analyze and understand biased client selection strategies since they can sharply
accelerate error convergence, and hence boost communication efficiency in heterogeneous environ-
ments by preferentially selecting clients with higher local loss values, as we show in our paper. This
idea has been explored in recent empirical studies (Goetz et al., 2019; Laguel et al., 2020; Ribero &
Vikalo, 2020). Nishio & Yonetani (2019) proposed grouping clients based on hardware and wireless
resources in order to save communication resources. Goetz et al. (2019) (which we include as a
benchmark in our experiments) proposed client selection with local loss, and Ribero & Vikalo (2020)
proposed utilizing the progression of clients’ weights. But these schemes are limited to empirical
demonstration without a rigorous analysis of how selection skew affects convergence speed.
Another relevant line of work (Jiang et al., 2019; Katharopoulos & Fleuret, 2018; Shah et al., 2020;
Salehi et al., 2018) employs biased selection or importance sampling of data to speed-up convergence
of classic centralized SGD - they propose preferentially selecting samples with highest loss or highest
gradient norm to perform the next SGD iteration. In contrast, Shah et al. (2020) proposes biased
selection of lower loss samples to improve robustness to outliers. Generalizing such strategies to the
federated learning setting is a non-trivial and open problem because of the large-scale distributed and
heterogeneous nature of the training data.
Our Contributions. In this paper, we present the first (to the best of our knowledge) convergence
analysis of federated learning with biased client selection that is cognizant of the training progress
at each client. We discover that biasing the client selection towards clients with higher local losses
increases the rate of convergence compared to unbiased client selection. Using this insight, we
propose the Power-of-choice client selection strategy and show by extensive experiments that
POWER-OF-CHOICE yields up to 3× faster convergence with 10% higher test performance than
the standard federated averaging with random selection. Power-of-choice is designed to incur
minimal communication and computation overhead, enhancing resource efficiency in federated
learning. In fact, we show that even with 3× less clients participating in each round as compared to
random selection, POWER-OF-CHOICE gives 2× faster convergence and 5% higher test accuracy.
2	Problem Formulation
Consider a cross-device federated learning setup with total K clients, where client k has a local
dataset Bk consisting |Bk | = Dk data samples. The clients are connected via a central aggregating
server, and seek to collectively find the model parameter w that minimizes the empirical risk:
K
K
F (W) = 一J——
PK=1 D1
ΣΣf (W, ξ) =	pkFk(W)
(1)
kk=1 ξ∈Bk
k=1
where f(W, ξ) is the composite loss function for sample ξ and parameter vector W. The term
pk= Dk/PkK=1 Dkis the fraction of data at the k-th client, and Fk(W) = ∣B1k∣ Pξ∈Bk f (W,ξ)isthe
local objective function of client k. In federated learning, the vectors w*, and Wk for k = 1,...,K
that minimize F(W) and Fk(W) respectively can be very different from each other. We define
F * = minw F (w) = F (w*) and Fk = minw Fk(W) = Fk (Wk).
Federated Averaging with Partial Client Participation. The most common algorithm to solve (1)
is federated averaging (FedAvg) proposed in McMahan et al. (2017). The algorithm divides the
training into communication rounds. At each round, to save communication cost at the central server,
the global server only selects a fraction C of m = CK clients to participate in the training. Each
selected/active client performs τ iterations of local SGD (Stich, 2018; Wang & Joshi, 2018; Yu et al.,
2018) and sends its locally updated model back to the server. Then, the server updates the global
model using the local models and broadcasts the global model to a new set of active clients.
Formally, we index the local SGD iterations with t ≥ 0. The set of active clients at iteration t is
denoted by S(t). Since active clients performs τ steps of local update, the active set S(t) also remains
constant for every T iterations. That is, if (t + 1) mod T = 0, then S(t+1) = S(t+2) = … = S(t+τ).
2
Under review as a conference paper at ICLR 2021
(a) Selecting Higher Loss Clients
(b) Random Client Selection
Figure 1: A toy example with F1 (w), F2 (w) as the local objective, and F (w) = (F1 (w) +
F2(w))∕2 as the global objective function with global minimum w*. At each round, only one client
is selected to perform local updates. (a): Model updates for sampling clients with larger loss; (b):
Model updates for sampling clients uniformly at random (we select client in the order of 2,2,1,1,2).
Accordingly, the update rule of FedAvg can be written as follows:
(t+1)	w(kt) - ηtgk (w(kt) , ξk(t) )
Wk	-J Pj∈s(t) Wintgj (Wjt) ,ξjt)D , w(t+1)
for (t + 1) mod τ 6= 0
for (t + 1) mod τ = 0
(2)
where wk(t+1) denotes the local model parameters of client k at iteration t, ηt is the learning rate, and
gk(Wkt),ξkt)) = b Pξ∈ξ(t) Vf (Wa),ξ) is the stochastic gradient over mini-batch ξkt) of size b that
is randomly sampled from client k's local dataset Bk. Moreover, w(t+1) denotes the global model
at server. Although w(t) is only updated after every T iterations, for the purpose of convergence
analysis we consider a virtual sequence of w(t) that is updated at each iteration as follows
w(t+1)= W㈤-ntg㈤=W㈤-nt (m X gk(wkt),ξkt)) ∣	⑶
k∈S(t)
with g(t) = ml Pk∈s(t) gk (Wkt),ξkt)). Note that in (2) and (3) we do not weight the client models by
their dataset fractions pk because pk is considered in the client selection scheme used to decide the
set S(t). Our convergence analysis can be generalized to when the global model is a weighted average
instead of a simple average of client models, and we show in Appendix E that our convergence
analysis also covers the sampling uniformly at random without replacement scheme proposed by Li
et al. (2020). The set S(t) can be sampled either with or without replacement. For sampling with
replacement, we assume that multiple copies of the same client in the set S(t) behave as different
clients, that is, they perform local updates independently.
Client Selection Strategy. To guarantee FedAvg converges to the stationary points of the objective
function (1), most current analysis frameworks (Li et al., 2020; Karimireddy et al., 2019; Wang et al.,
2020) consider a strategy that selects the set S(t) by sampling m clients at random (with replacement)
such that client k is selected with probability pk, the fraction of data at that client. This sampling
scheme is unbiased since it ensures that in expectation, the update rule (3) is the same as full client
participation. Hence, it enjoys the same convergence properties as local-update SGD methods (Stich,
2018; Wang & Joshi, 2018). We denote this unbiased random client selection strategy as πrand.
In this paper, we consider a class of biased client selection strategies that is cognizant of the global
training progress which (to the best of our knowledge) has not been worked on before. Note that for
any aggregation scheme and sampling scheme with partial client participation, if the expectation over
the sampling scheme for the update rule of the global model is equal to the case of the update rule for
full client participation, we distinguish this as an unbiased client participation scheme. For example in
HOrVgth & RiChtdrik (2020), even with a biased sampling scheme, with the normalizing aggregation
3
Under review as a conference paper at ICLR 2021
the update rule is unbiased. Henceforth, we state that our paper encompasses both biased and unbiased
update rules. In the two-client example in Figure 1, We set S(t+1) = argmaxk∈κ Fk(W⑴),a
single client with the highest local loss at the current global model. In this toy example, the selection
strategy cannot guarantee the updates (3) equals to the full client participation case in expectation.
Nevertheless, it gives faster convergence to the global minimum than the random one. Motivated by
this observation, we define a client selection strategy π as a function that maps the current global
model w to a selected set of clients S(π, w).
3	Convergence Analysis
In this section we analyze the convergence of federated averaging with partial device participation for
any client selection strategy π as defined above. This analysis reveals that biased client selection can
give faster convergence, albeit at the risk of having a non-vanishing gap between the true optimum
w* = arg min F(w) and limt→∞ W(t). We use this insight in Section 4 to design client selection
strategies that strike a balance between convergence speed and bias.
3.1	Assumptions and Definitions
First we introduce the assumptions and definitions utilized for our convergence analysis.
Assumption 3.1.	F1,	...,	Fk	are all	L-smooth, i.e., for all v and w,	Fk (v)	≤	Fk (w) +	(v -
w)TVFk(W) + L2∣∣v - wk2.
Assumption 3.2. F1, ..., Fk are all μ-strongly convex, i.e., for all V and W, Fk (V) ≥ Fk (w) +
(V - w)TVFk(w) + 2∣∣v - w∣2.
Assumption 3.3. For the mini-batch ξk uniformly sampled at random from Bk from user k, the
resulting stochastic gradient is unbiased, that is, E[gk(wk, ξk)] = VFk (wk). Also, the variance of
stochastic gradients is bounded: E∣gk (wk, ξk) - VFk(wk)∣2 ≤ σ2 for all k = 1, ..., K.
Assumption 3.4. The stochastic gradient’s expected squared norm is uniformly bounded, i.e.,
E∣gk(wk,ξk)∣2 ≤ G2 fork = 1, ..., K.
The above assumptions are common in related literature, see (Stich, 2018; Basu et al., 2019; Li et al.,
2020; Ruan et al., 2020). Next, we introduce two metrics, the local-global objective gap and the
selection skew, which feature prominently in the convergence analysis presented in Theorem 3.1.
Definition 3.1 (Local-Global Objective Gap). For the global optimum w* = arg minw F(w) and
local optimum wk* = arg minw Fk(w) we define the local-global objective gap as
KK
Γ,F*-XpkFk*=Xpk(Fk(w*)-Fk(wk*)) ≥0.	(4)
k=1	k=1
Note that Γ is an inherent property of the local and global objective functions, and it is independent of
the client selection strategy. This definition was introduced in previous literature by Li et al. (2020).
A larger Γ implies higher data heterogeneity. If Γ = 0 then it implies that the local and global
optimal values are consistent, and there is no solution bias due to the client selection strategy (see
Theorem 3.1). Next, we define another metric called selection skew, which captures the effect of the
client selection strategy on the local-global objective gap.
Definition 3.2 (Selection Skew). For any k ∈ S(π, w) we define,
ρ(S(π, w), w0)
ES(∏,w)[*Pk∈s(∏,w)(Fk W)- FkQ]
F(WO)- PK=IPkFk
≥ 0,
(5)
which reflects the skew ofa client selection strategy π. The first w in ρ(S(π, w), w0) is the parameter
vector that governs the client selection and w0is the point at which Fk and F in the numerator and
denominator respectively are evaluated. Note, Es(∏,w) [∙] is the expectation over the randomness
from the selection strategy π, since there can be multiple sets S that π can map from a specific w.
4
Under review as a conference paper at ICLR 2021
Since ρ(S (π, w), w0) is a function of versions of the global model w and w0, which change during
training, we define two related metrics that are independent of w and w0. These metrics enable us to
obtain a conservative error bound in the convergence analysis.
P，min ρ(S(π, w), w0),	ρ，maxρ(S(π, w), w*)	(6)
w,w0	w
where w* = argminw F(w). From (6), we have P ≤ ρ for any Client selection strategy π.
Effect of the Client Selection Strategy on P and ρ. For the unbiased client selection strategy ∏ra∩d
we have ρ(S(πrand, w), w0) = 1 for all w and w0 since the numerator and denominator of (5) become
equal, and P = e = 1. For a client selection strategy ∏ that chooses clients with higher Fk(W) more
often, p and P will be larger (and ≥ 1). In the convergence analysis we show that a larger P implies
faster convergence, albeit with a potential error gap, which is proportional to (p/P - 1). Motivated
by this, in Section 4 we present an adaptive client selection strategy that prefers selecting clients with
higher loss Fk(w) and achieves faster convergence speed with low solution bias.
3.2	Main Convergence Result
Here, we present the convergence results for any client selection strategy π for federated averaging
with partial device participation in terms of local-global objective gap Γ, and selection skew P, p.
Theorem 3.1 (Convergence with Decaying Learning Rate). Under Assumptions 3.1 to 3.4, for
learning rate η = .7+))with Y = 4L, and any client selection strategy π, the error after T
iterations of federated averaging with partial device participation satisfies
E [F (W(T))] - F * ≤
1
(T + Y)
4L(32τ2G2 + σ2∕m)	8L2Γ	LYkW(O) - w*∣∣2
3μ2P	+ μ2 +	2
{ι^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^—
Vanishing Error Term
Non-vanishing bias,Q(ρ,p)
(7)
To the best of our knowledge, Theorem 3.1 provides the first convergence analysis of federated
averaging with a biased client selection strategy π. We also show the results for fixed learning rate in
Appendix A. The proof is presented in Appendix C. The first part of our proof follows techniques
presented by Li et al. (2020). Then we introduce the novel concept of selection skew to the proof,
and analyze the effect of biased client selection strategies that has not been seen before in previous
literature. We highlight that our convergence result is a general analysis that is applicable for any
selection strategy π that is cognizant of the training progress. In the following paragraphs, we discuss
the effects of the two terms in (7) in detail.
Large P and Faster Convergence. A key insight from Theorem 3.1 is that a larger selection skew P
results in faster convergence at the rate O(T1ρ). Note that since we obtain P (defined in (6)) by taking
a minimum of the selection skew P(S(π, w), w0) over w, w0, this is a conservative bound on the
true convergence rate. In practice, since the selection skew P(S(π, w), w0) changes during training
depending on the current global model w and the local models w0, the true convergence rate can be
improved by a factor larger than and at least equal to P.
Non-vanishing Bias Term. The second term Q(P,Pp) = 83μ- Pj= — 1)in (7) denotes the solution
bias, which is dependent on the selection strategy. By the definitions of P and p, it follows that P ≥ P,
which implies that Q(P,p ≥ 0. For an unbiased selection strategy, we have P = P = 1, Q(P,Pe) = 0,
and hence (7) recovers previous bound for unbiased selection strategy as (Li et al., 2020). For P > 1,
while we gain faster convergence rate by a factor of P, we cannot guarantee Q(P, PO= 0. Thus, there
is a trade-off between the convergence speed and the solution bias. Later in the experimental results,
1	P
we show that even with biased selection strategies, the term P - 1 in Q(p,P) can be close to 0, and
hence Q (P, P) has a negligible effect on the final error floor.
5
Under review as a conference paper at ICLR 2021
4 Proposed Power-of-choice Client Selection Strategy
From (5) and (6) We discover that a selection strategy ∏ that prefers clients with larger Fk(W) - Fk
will result in a larger ρ, yielding faster convergence. Using this insight, a naive client selection
strategy can be choosing the clients with highest local loss Fk(W). However, a larger selection skew
P may result in a larger ρ∕e, i.e., a larger non-vanishing error term. This naive selection strategy has
another drawback - to find the current local loss Fk (w), it requires sending the current global model
to all K clients and having them evaluate Fk and sending it back. This additional communication
and computation cost can be prohibitively high because the number of clients K is typically very
large, and these clients have limited communication and computation capabilities.
In this section, we use these insights regarding the trade-off between convergence speed, solution
bias and communication/computation overhead to propose the Power-of-choice client selection
strategy. POWER-OF-CHOICE is based on the power of d choices load balancing strategy (Mitzen-
macher, 1996), which is extensively used in queueing systems. In the Power-of-choice client
selection strategy (denoted by πpow-d), the central server chooses the active client set S(t) as follows:
1.	Sample the Candidate Client Set. The central server samples a candidate set A of d (m ≤ d ≤
K) clients without replacement such that client k is chosen with probability pk, the fraction of
data at the k-th client for k = 1, . . . K.
2.	Estimate Local Losses. The server sends the current global model W(t) to the clients in set A,
and these clients compute and send back to the central server their local loss Fk(W((It)
3.	Select Highest Loss Clients. From the candidate set A, the central server constructs the active
client set S(t) by selecting m = max(CK, 1) clients with the largest values Fk(W), with ties
broken at random. These S(() clients participate in the training during the next round, consisting
of iterations t + 1, t + 2, . . . t + τ .
Variations of πpow-d. The three steps of πpow-d can be flexibly modified to take into account
practical considerations. For example, intermittent client availability can be accounted for in step
1 by constructing set A only from the set of available clients in that round. We demonstrate the
performance of πpow-d with intermittent client availability in Appendix G.3. The local computation
cost and server-client communication cost in step 2 can be reduced or eliminated by the following
proposed variants of πpow-d (see Appendix F for their pseudo-codes).
•	Computation-efficient Variant πcpow-d: To save local computation cost, instead of evaluating
the Fk(W) by going through the entire local dataset Bk, we use an estimate EUUb f (w, ξ)∕∣ξk|,
ξ∈ξk
where ξk is the mini-batch of b samples sampled uniformly at random from Bk.
•	Communication- and Computation-efficient Variant πrpow-d : To save both local computation
and communication cost, the selected clients for each round sends their accumulated averaged loss
over local iterations, i.e., ∣*∣ £；=(_/+ι Pξ∈ξ(i) f (Wkl),ξ) when they send their local models
to the server. The server uses the latest received value from each client as a proxy for Fk (W) to
select the clients. For the clients that have not been selected yet, the latest value is set to ∞.
•	Adaptive Selection Skew Variant πadapow-d: To minimize the non-vanishing bias term in Theo-
rem 3.1 while simultaneously gaining the benefit of convergence speed from p, we gradually reduce
d until d = m1. This enables convergence speed up in the initial training phase, while eventually
diminishing the non-vanishing bias term when d = m. Which d to start with and how gradually
we decrease d to m is flexible, analogous to setting the environment and hyper-parameters.
Selection Skew of POWER-OF-CHOICE Strategy. The size d of the candidate client set A is an
important parameter which controls the trade-off between convergence speed and solution bias. With
d = m we have random sampling without replacement in proportion of pk. As d increases, the
selection skew P increases, giving faster error convergence at the risk of a higher error floor. However,
note that the convergence analysis replaces p(w, w0) with P to get a conservative error bound. In
1d = m makes our proposed POWER-OF-CHOICE strategy to become analogous to an unbiased sampling
strategy, which has no non-vanishing bias term.
6
Under review as a conference paper at ICLR 2021
e4
11
4 2 0
ss。二tβqOe
O 5000 IOOOO 15000
Communication ro≡d
(a) K = 30
6 4 2
ss-Itβq-u
(b) K = 100
Ss-IBq-o
O
O-
(c) K = 100 (log-scale)

Figure 2: Global loss performance of πrand, πpow-d, and πadapow-d for the quadratic experiments with
C = 0.1. πpow-d convergences faster than πrand for even selecting from a small pool of clients
(K = 30). As convergence speed increases, solution bias also increases for πpow-d, but πadapow-d is
able to eliminate this solution bias while gaining nearly identical convergence speed to πpow-d.
practice, the convergence speed and the solution bias is dictated by P(W(Tbt/TC), w(t)) which changes
during training. With πpow-d which is biased towards higher local losses, we expect the selection skew
P(w, w0 ) to reduce through the course of training. We conjecture that this is why πpow-d gives faster
convergence as well as little or no solution bias in our experiments presented in Section 5.
5	Experimental Results
We evaluate our proposed πpow-d and its practical variants πcpow-d, πrpow-d, and πadapow-d by three sets
of experiments: (1) quadratic optimization, (2) logistic regression on a synthetic federated dataset,
Synthetic(1,1) (Sahu et al., 2019), and (3) DNN trained on a non-iid partitioned FMNIST dataset
(Xiao et al., 2017). We also benchmark the selection strategy proposed by Goetz et al. (2019), active
federated learning, denoted as πafl. Details of the experimental setup are provided in Appendix F, and
the code for all experiments are shared in the supplementary material. To validate consistency in our
results, we present additional experiments with DNN trained on a non-iid partitioned EMNIST (Cohen
et al., 2017) dataset sorted by digits with K = 500 clients. We present the results in Appendix G.4.
Quadratic and Synthetic Simulation Results. In Fig-
ure 2(a), even with few clients (K = 30), πpow-d converges
faster than πrand with nearly negligible solution bias for
small d. The convergence speed increases with the in-
crease in d, at the cost of higher error floor due to the
solution bias. For K = 100 in Figure 2(b), πpow-d shows
convergence speed-up as with K = 30, but the bias is
smaller. Figure 3 shows the theoretical values P and e/p
which represents the convergence speed and the solution
bias respectively in our convergence analysis. Compared
to ∏rand, ∏pow-d has higher P for all d implying higher con-
vergence speed than πrand . By varying d we can span
different points on the trade-off between the convergence
speed and bias. For d = 15 and K = 100, e/p of ∏pow-d
and πrand are approximately identical, but πpow-d has higher
p, implying that ∏pow-d can yield higher convergence speed
with negligible solution bias. In Appendix G.1, we present
C=0.10
d=30
-*- pow-d, K=30
rand, K=100
rand, K=30
41^- pow-d, K=IOO
d=50
d=15
d=9
d=30
=15
1.00	1.25	1.50	1.75
P (convergence speed)
Figure 3: Estimated theoretical values
p and p/e for the quadratic simula-
tion. The convergence speed (p) and
bias (e/p) are consistent with the results
shown in Figure 2 for πrand and πpow-d.
the clients’ selected frequency ratio for πpow-d and πrand which gives novel insights regarding the dif-
ference between the two strategies. For the synthetic dataset simulations, we present the global losses
in Figure 4 for πrand and πpow-d for different d and m. We show that πpow-d converges approximately
3× faster to the global loss ≈ 0.7 than πrand when d = 10m, with a slightly higher error floor. Even
with d = 2m, we get 2× faster convergence to global loss ≈ 0.7 than πrand.
Elimination of Selection Skew with πadapow-d . For πpow-d, the selection skew is the trade-off for the
convergence speed gain in Figure 2 and Figure 4. For both simulations, πpow-d converges slightly
above the global minimum value due to the selection skew. We eliminate this selection skew while
maintaining the benefit of convergence speed with πadapow-d. In Figure 2(a)-(b), πadapow-d shows a
7
Under review as a conference paper at ICLR 2021
3 2
SSO=EqOG
0.5
O 250	500	750
Communication round
(a) m = 1
5.05.0
2 2 11
SSOlWqOID
Communication round
(b) m = 2
.0.50,5
ZII0.
SSOIlBq££>
----rand
----pow-d, d=6
----pow-d, d=30
adapow-d
converges 3× faster
to global loss≈0.7
70l
’264
0	250	500	750
Communication round
(c) m = 3
Figure 4: Global loss for logistic regression on the synthetic dataset, Synthetic(1,1), with πrand,
πpow-d, and πadapow-d for d ∈ {2m, 10m} where K = 30, m ∈ {1, 2, 3}. πpow-d converges approx-
imately 3 × faster for d = 10m and 2 × faster for d = 2m than πrand to the global loss ≈ 0.7.
πadapow-d is able to converge to the minimum global loss 3× faster than πrand.
K=I00, C=0.03
SSOIωUΠΠBJl
(a) Test accuracy and training loss for α = 2	(b) Test accuracy and training loss for α = 0.3
Figure 5: Test accuracy and training loss for different sampling strategies with K = 100, C = 0.03
for varying d on the FMNIST dataset. For both small and large α, πpow-d achieves at least 10% test
accuracy improvement than πrand and the training loss converges at a much higher rate than πrand.
convergence speed similar to πpow-d , d = K, but has no selection skew, converging to the same
minimum as πrand (see Figure 2(c)). In Figure 4, πadapow-d again shows a convergence speed similar to
πpow-d, d = 10m, but has no adversarial selection skew. As a matter of fact, πadapow-d converges to
the minimum global loss value at least 3 × faster than πrand. Hence πadapow-d gains the benefit of both
worlds from biased client selection: convergence speed and elimination of selection skew.
Experiments with Heterogeneously Distributed FMNIST. As elaborated in Appendix F, α deter-
mines the data heterogeneity across clients. Smaller α indicates larger data heterogeneity. In Figure 5,
we present the test accuracy and training losses for the different sampling strategies from the FMNIST
experiments with α = 0.3 and α = 2. Observe that πpow-d achieves approximately 10% and 5%
higher test accuracy than πrand and πafl respectively for both α = 2 and α = 0.3. For higher α (less
data heterogeneity) larger d (more selection skew) performs better than smaller d.
Figure 5(a) shows that this performance improvement due to the increase of d eventually converges.
For smaller α, as in Figure 5(b), smaller d = 6 performs better than larger d which shows that too
much solution bias is adversarial to the performance in the presence of large data heterogeneity. The
observations on training loss are consistent with the test accuracy results.
Performance of the Communication- and Computation-Efficient variants. Next, we evaluate
πcpow-d and πrpow-d which were introduced in Section 4. In Figure 6, for α = 2, πrpow-d and πcpow-d
each yields approximately 5% and 6% higher accuracy than πrand, but both yield lower accuracy
than πpow-d that utilizes the highest computation and communication resources. For α = 0.3, πcpow-d
and πrpow-d perform as well as πpow-d and give a 10% accuracy improvement over πrand. Moreover,
πpow-d , πrpow-d and πcpow-d all have higher accuracy and faster convergence than πafl.
We evaluate the communication and computation efficiency of Power-of-choice by comparing
different strategies in terms of R60 , the number of communication rounds required to reach test
accuracy 60%, and tcomp, the average computation time (in seconds) spent per round. The computation
time includes the the time taken by the central server to select the clients (including the computation
8
Under review as a conference paper at ICLR 2021
K=I 00, C=0.03
Communication round
(b) Test accuracy and training loss for α = 0.3
Figure 6: Test accuracy and training loss for different sampling strategies including πcpow-d and
πrpow-d, for K = 100, C = 0.03 on the FMNIST dataset. πrpow-d which requires no additional
communication and minor computation, yields higher test accuracy than πrand and πafl.
Table 1: Comparison of R60, tcomp(sec), and test accuracy (%) for different sampling strategies with
α = 0.3. In the parentheses we show the ratio of each value with that for πrand with C = 0.1.
(a) Test accuracy and training loss for α = 2
	C=0.1	C = 0.03				
	rand	rand	pow-d, d = 6 cpow-d, d = 6		rpow-d, d = 50	afl
R60	172	234(1.36)	89(0.52)	80 (0.47)	98 (0.57)	121(0.70)
tcomp	0.43	0.36(0.85)	-0.48(1.13)-	0.37 (0.88)	0.37 (0.85)	0.36(0.84)
Test Acc.	71.21±2.41	64.87±1.97	76.47±0.87	76.63±0.79	76.56±1.00	73.28±1.05
time for the d clients to compute their local loss values) and the time taken by selected clients to
perform local updates. In Table 1, with only C = 0.03 fraction of clients, πpow-d, πcpow-d, and πrpow-d
have about 5% higher test accuracy than (πrand, C = 0.1). The R60 for πpow-d, πcpow-d, πrpow-d is
0.52, 0.47, 0.57 times that of (πrand, C = 0.1) respectively. This implies that even for πrpow-d which
does not incur any additional communication cost for client selection, we can get a 2× reduction in
the number of communication rounds using 1/3 of clients compared to (πrand, C = 0.1) and still get
higher test accuracy performance. Note that the computation time tcomp for πcpow-d and πrpow-d with
C = 0.03 is smaller than that of πrand with C = 0.1. In Appendix G.2, we show that the results for
α = 2 are consistent with the α = 0.3 case shown in Table 1. In Appendix G.5, we also show that
for C = 0.1, the results are consistent with the C = 0.03 case.
Effect of Mini-batch Size and Local Epochs. We evaluate the effect of mini-batch size b and
local epochs τ on the FMNIST experiments with different sets of hyper-parameters: (b, τ) ∈
{(128, 30), (64, 100)}. Note that (b, τ) = (64, 30) is the default hyper-parameter setting for the
previous results. The figures are presented in Appendix G.6. For b = 128, we observe that the
performance improvement of πpow-d over πrand and πafl is consistent with b = 64 (see Figure 12). In
Figure 14, for τ = 100, with smaller data heterogeneity, the performance gap between πrand and
πpow-d is consistent with that of τ = 30. For larger data heterogeneity, however, increasing the local
epochs results in πrand and πpow-d performing similarly. This shows that with larger data heterogeneity,
larger τ results in increasing the selection skew towards specific clients, and weakens generalization.
6	Concluding Remarks
In this work, we present the convergence guarantees for federated learning with partial device
participation with any biased client selection strategy. We discover that biasing client selection
can speed UP the convergence at the rate O(TP) where P is the selection skew towards clients with
higher local losses. Motivated by this insight, we propose the adaptive client selection strategy
POWER-OF-CHOICE. Extensive experiments validate that POWER-OF-CHOICE yields 3× faster
convergence and 10% higher test accUracy than the baseline federated averaging with random
selection. Even with Using fewer clients than random selection, POWER-OF-CHOICE converges 2 ×
faster with high test performance. An interesting fUtUre direction is to improve the fairness (Li et al.,
2019; YU et al., 2020; LyU et al., 2020; Mohri et al., 2019) and robUstness (PillUtla et al., 2019) of
the Power-of-choice strategy by modifying step 3 of the Power-of-choice algorithm to Use a
different metric sUch as the clipped loss or the q-fair loss proposed Li et al. (2019) instead of Fk(w).
9
Under review as a conference paper at ICLR 2021
References
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd
with quantization, sparsification, and local computations. In Advances in Neural Information
Processing Systems,pp. 14695-14706, 2019.
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi, H. Brendan McMahan, Timon Van
Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards Federated Learning
at Scale: System Design. SysML, April 2019. URL https://www.sysml.cc/doc/2019/
193.pdf.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre van Schaik. EMNIST: an extension of
MNIST to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale
distributed deep networks. In Proceedings of the International Conference on Neural Information
Processing Systems, pp. 1223-1231, 2012.
Jack Goetz, Kshitiz Malik, Duc Bui, Seungwhan Moon, Honglei Liu, and Anuj Kumar. Active
federated learning. ArXiv, 2019.
Farzin Haddadpour and Mehrdad Mahdavi. On the convergence of local descent methods in federated
learning. arXiv preprint arXiv:1910.14425, 2019.
Samuel Horvgth and Peter Richtdrik. A better alternative to error feedback for CommUniCation-
efficient distributed learning. arXiv preprint arXiv:2006.11077, 2020.
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. https://arxiv.org/abs/1909.06335, September 2019.
Zhouyuan Huo, Qian Yang, Bin Gu, Lawrence Carin, and Heng Huang. Faster on-device training
using new federated momentum algorithm. arXiv preprint arXiv:2002.02090, 2020.
Angela H. Jiang, Daniel L. K. Wong, Giulio Zhou, David G. Andersen, Jeffrey Dean, Gregory R.
Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C. Lipton, and Padmanabhan
Pillai. Accelerating deep learning by focusing on the biggest losers, 2019.
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L.
D’Oliveira, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adria Gascon, Badih
Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan
Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub
Konecny, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrede Lepoint, Yang Liu,
Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozgur, Rasmus Pagh, Mariana Raykova,
Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng
Sun, Ananda Theertha Suresh, Florian Tramer, Praneeth Vepakomma, Jianyu Wang, Li Xiong,
Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances and open problems in
federated learning. arXiv preprint arXiv:1912.04977, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated
learning. arXiv preprint arXiv:1910.06378, 2019.
A. Katharopoulos and F. Fleuret. Not all samples are created equal: Deep learning with importance
sampling. In Proceedings of the International Conference on Machine Learning (ICML), volume 80
of Proceedings of Machine Learning Research, pp. 2525-2534, 2018.
A Khaled, K Mishchenko, and P Richtdrik. Tighter theory for local SGD on identical and heteroge-
neous data. In The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS
2020), 2020.
10
Under review as a conference paper at ICLR 2021
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U Stich. A
unified theory of decentralized SGD with changing topology and local updates. arXiv preprint
arXiv:2003.10422, 2020.
Yassine LagUeL Krishna Pillutla, J6r6me Malick, and Zaid Harchaoui. Device heterogeneity in
federated learning: A superquantile approach. ArXiv, 2020.
Tian Li, Maziar Sanjabi, and Virginia Smith. Fair resource allocation in federated learning. arXiv
preprint arXiv:1905.10497, 2019.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations (ICLR), July
2020. URL https://arxiv.org/abs/1907.02189.
Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong Jin, Han Yu, and
Kee Siong Ng. Towards Fair and Privacy-Preserving Federated Deep Models. IEEE Transactions
on Parallel and Distributed Systems, May 2020.
Grigory Malinovsky, Dmitry Kovalev, ElnUr Gasanov, Laurent Condat, and Peter Richtdrik. From
local SGD to local fixed point methods for federated learning. arXiv preprint arXiv:2004.01442,
2020.
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag0ura y Arcas.
Communication-Efficient Learning of Deep Networks from Decentralized Data. International
Conference on Artificial Intelligenece and Statistics (AISTATS), April 2017. URL https://
arxiv.org/abs/1602.05629.
M. Mitzenmacher. The power of two choices in randomized load balancing. PhD thesis, University
of California Berkeley, CA, 1996.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
on Machine Learning, volume 97 of Proceedings ofMachine Learning Research, pp. 4615-4625,
Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Takayuki Nishio and Ryo Yonetani. Client selection for federated learning with heterogeneous
resources in mobile edge. In IEEE International Conference on Communications, pp. 1-7, May
2019.
Reese Pathak and Martin J Wainwright. FedSplit: An algorithmic framework for fast federated
optimization. arXiv preprint arXiv:2005.05238, 2020.
Krishna Pillutla, Sham M. Kakade, and Zaid Harchaoui. Robust aggregation for federated learning.
arXiv preprint 1912.13445, 2019. URL https://arxiv.org/abs/1912.13445.
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint
arXiv:2003.00295, 2020.
M6nica Ribero and Haris Vikalo. Communication-efficient federated learning via optimal client
sampling. ArXiv, 2020.
Yichen Ruan, Xiaoxi Zhang, Shu-Che Liang, and Carlee Joe-Wong. Towards flexible device partici-
pation in federated learning for non-iid data. ArXiv, 2020.
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and Virginia Smith.
Federated optimization for heterogeneous networks. https://arxiv.org/abs/1812.06127, January
2019.
Farnood Salehi, Patrick Thiran, and Elisa Celis. Coordinate descent with
bandit sampling, 2018.	URL http://papers.nips.cc/paper/
8137-coordinate-descent-with-bandit- sampling.pdf.
11
Under review as a conference paper at ICLR 2021
Vatsal Shah, Xiaoxia Wu, and Sujay Sanghavi. Choosing the sample with lowest loss makes sgd
robust. In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics
(AISTATS 2020), 2020. URL https://arxiv.org/abs/2001.03316.
Sebastian U Stich. Local SGD converges fast and communicates little. arXiv preprint
arXiv:1805.09767, 2018.
Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for
SGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350,
2019.
Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis
of communication-efficient SGD algorithms. arXiv preprint arXiv:1808.07576, 2018. URL
https://arxiv.org/abs/1808.07576.
Jianyu Wang and Gauri Joshi. Adaptive Communication Strategies for Best Error-Runtime Trade-offs
in Communication-Efficient Distributed SGD. In Proceedings of the SysML Conference, April
2019. URL https://arxiv.org/abs/1810.08313.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the Objective
Inconsistency Problem in Heterogeneous Federated Optimization. preprint, May 2020. URL
https://arxiv.org/abs/2007.07481.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local SGD better than minibatch SGD? arXiv
preprint arXiv:2002.07839, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. https://arxiv.org/abs/1708.07747, aug 2017.
Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit Niyato, and Qiang
Yang. A fairness-aware incentive scheme for federated learning. Proceedings of the AAAI/ACM
Conference on AI, Ethics, and Society, pp. 393-399, 2020.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD for non-convex optimization with
faster convergence and less communication. arXiv preprint arXiv:1807.06629, 2018.
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. FedPD: A federated learning
framework with optimal rates and adaptivity to non-IID data. arXiv preprint arXiv:2005.11418,
2020.
12
Under review as a conference paper at ICLR 2021
A Additional Theorem
Theorem A.1 (Convergence with Fixed Learning Rate). Under Assumptions 3.1 to 3.4, a fixed
learning rate η ≤ min{ 2μ^, 4L } where B = 1 + 3p, and any client SelectiOn strategy π as defined
above, the error after T iterations of federated averaging with partial device participation satisfies
F(W(T)) — F *
L
≤ 一
μ
l.
1 - ημ 1 +
τ (F (W⑼)-F * - 4^
32τ2G2 + σ2 + 6pLr) + 2Γ(e - ρ)]
8 + 3p
Vanishing Term
}
4Lη (32T2G2 + m + 6ρLr)	8LΓ(ρ - P)
μ	μ(8 + 3ρ)	μ μ(8 + 3ρ)
1-----------------------{-----------------------}
Non-vanishing bias
(8)
As T → ∞ the first term in (8) goes to 0 and the second term becomes the bias term for the fixed
learning rate case. For a small η, we have that the bias term for the fixed learning rate case in
Theorem A.1 is upper bounded by 8LΓ (g - ι) which is identical to the decaying-learning rate case.
The proof is presented in Appendix D.
B	Preliminaries for Proof of Theorem 3.1 and Theorem A.1
We present the preliminary lemmas used for proof of Theorem 3.1 and Theorem A.1. We will denote
the expectation over the sampling random source S(t) as ES(t) and the expectation over all the
random sources as E.
Lemma B.1. Suppose Fk is L-smooth with global minimum at Wk*, then for any Wk in the domain
of Fk, we have that
kVFk(Wk)k2 ≤ 2L(Fk(Wk) - Fk(Wk))	⑼
Proof.
Fk (Wk ) - Fk (W* ) -hVFk(W力 Wk- W*i ≥ jkVFk(Wk)- VFk (Wk )k2	(IO)
2L
Fk(Wk)- Fk(Wk) ≥ UkVFk(Wk)k2	(II)
2L
□
Lemma B.2 (Expected average discrepancy between W(t) and Wkt) for k ∈ S(t)).
ɪE[ X kW(t) - Wkt)k2] ≤ 16η2τ2G2	(12)
m
k∈S(t)
Proof.
ɪ X kW(t) - Wkt)k2 = 1 X kɪ X (Wkt0)-Wkt))k2	(13)
m	mm
k∈S(t)	k∈S(t)	k0∈S(t)
≤ W XX kWkt0)-Wkt)k2	(14)
k∈S(t) k0∈S(t)
=W X kWkt0)-Wkt)k2	(15)
k6=k0 ,
k,k0∈S(t)
13
Under review as a conference paper at ICLR 2021
Observe from the update rule that k, k0 are in the same set S(t) and hence the terms where k = k0 in
the summation in (14) will be zero resulting in (15). Moreover for any arbitrary t there is a t0 such
that 0 ≤ t - t0 < τ that w(kt00) = wk(t0) since the selected clients are updated with the global model
at every τ. Hence even for an arbitrary t we have that the difference between kw(kt0) - w(kt) k2 is upper
bounded by τ updates. With non-increasing ηt over t and ηt0 ≤ 2ηt , (15) can be further bounded as,
1
m2
X kwk(t0)
k6=k0 ,
k,k0∈S(t)
-w
(kt)k2
t0+τ-1
≤ m X k X ηi (gk0 (wkio), ξkio)) - gk (wki) ,ξki))) k2
k6=k0 ,	i=t0
k,k0∈S(t)
2	t0+τ-1
≤^mr	X X	k(gk0(Wki0),ξki0))-gk(Wki),ξki)))k2
k6=k0 ,	i=t0
k,k0∈S(t)
(16)
(17)
2 t0 +τ-1
≤ M	XX
k6=k0 , i=t0
k,k0∈S(t)
[2kgk0(W(ki0), ξk(i0))k2 + 2kgk(Wk(i), ξk(i))k2]
(18)
By taking expectation over (18),
E[m12	X kwkt0) - wkt)k2] ≤
k6=k0 ,
k,k0∈S(t)
2	t0 +τ -1
Jmf E[ X X (kgk0(wki0),ξki0))k2 + kgk(wki),ξki))k2)]
k6=k0, i=t0
k,k0∈S(t)
(19)
2 2	t0 +τ-1 ≤ 2m0τEs(t) [ X X 2G2] k6=k0 , i=t0 k,k0∈S(t)	(20)
= 2m0τEs(t)[ X 2τG2]	(21)
k6=k0 ,	
k,k0∈S(t)	
< 16η2(m - 1)τ2G2	(22)
m	
≤ 16ηt2τ2G2	(23)
where (22) is because there can be at most m(m 一 1) pairs such that k = k0 in S(t).	口
Lemma B.3 (Upper bound for expectation over ∣∣w(t) 一 w*k2 for any selection strategy ∏). With E[∙],
the total expectation over all random sources including the random source from selection strategy we
have the upper bound:
E[kw⑴一 w*k2] ≤ mE[ X kWkt)- w*k2]	(24)
k∈S(t)
Proof.
E[kw⑴一 w*k2]= E[k m X Wkt)- w*k2 ]= E[k B X (Wkt)- w*)k2 ]	(25)
k∈S(t)	k∈S(t)
≤ ɪE[ X kwkt) - w*k2]	(26)
mk
k∈S(t)
□
14
Under review as a conference paper at ICLR 2021
C Proof of Theorem 3.1
With g(t) = m1 Pk∈s⑴ gk(Wkt),ξkt)) as defined in Section 2, We have that
∣∣w(t+1) - w*k2 =kw(t) - ηtg(t) - w*k2	(27)
=∣w㈤-ηtg㈤-w*- m X VFk(Wkt)) + m X VFk(Wkt))k2 (28)
k∈S(t)	k∈S(t)
= kw(t)-w*- m X VFk(Wkt))k2 + η2k-1 X VFk(Wkt))-g㈤k2
k∈S(t)	k∈S(t)
+ 2ηthw㈤-w*- m X VFk(Wa，- X VFk(Wkt))- g⑴i (29)
k∈S(t)	k∈S(t)
= ∣w(t) - w*k2 -2ηthw(t) - w*, -1 X VFk(Wkt))i
|
k∈S(t)
A1
}
+2ηthw(t) - W*— - X VFk(Wkt))，m X VFk (Wkt))- g(t)i
|
k∈S(t)	k∈S(t)
{^^^^^^^^^^^^^^^"
A2
}
+ η2k;1 X VFk(Wkt))k2 + ηkX VFk(Wkt)) -g㈤k2
(30)
k∈S(t)
X---------------
A3
}
k∈S(t)
V--------------------
A4
}
First let’s bound A1.
-2ηthw(t) - w*，ɪ X VFk(Wkt))i =
-k
k∈S(t)
=-* X hw(t) - wkt)，VFk(Wkt))i-
k∈S(t)
2ηt
—
-
E hw(t)
k∈S(t)
-W VFk(Wkt))i
^ηt X hWkt)- w*，VFk(Wkt))i
k∈S(t)
(31)
(32)
≤ η X 仕 kw(t)-Wkt)k2 + ηt∣VFk(Wkt))k2)-组 X hwkt)-w*, VFk(Wkt))i
-	ηt	k	k	-	k	k
k∈S(t) t	k∈S(t)
(33)
ɪ X kw(t)-Wkt)k2 + M X kVFk (Wkt))k2-* X hwkt)-w*, VFk (Wkt))i
---
k∈S(t)	k∈S(t)	k∈S(t)
(34)
≤ -1 X kw(t) - wkt)k2 + 2mt X (Fk (Wkt))- Fki)
k∈S(t)	k∈S(t)
-^mt X hWkt)- w*，VFk(Wkt))i
k∈S(t)
≤ X Et)-Wkt)k2 + 誓 X (Fk(wkt)) - Fn
k∈S(t)	k∈S(t)
-* X h(Fk (Wkt) ) - Fk (W*)) + μ kwkt) - w*k2i
k∈S(t)
≤ 16η2τ2G2-等 X IlWkt)-W*k2 + T X (Fk(Wkt))-Fn
k∈S(t)	k∈S(t)
-迦 X (Fk (Wkt)) - Fk (W*))
-
k∈S(t)
(35)
(36)
(37)
15
Under review as a conference paper at ICLR 2021
where (33) is due to the AM-GM inequality and CaUchy-SchWarz inequality, (35) is due to
Lemma B.1, (36) is due to the μ-convexity of Fk, and (37) is due to Lemma B.2. Next, in ex-
pectation, E[A2] = 0 due to the unbiased gradient. Next again with Lemma B.1 we bound A3 as
follows:
22
η2km X NFk(Wkt))k2 = m X NFk(Wkt)”
k∈S(t)	k∈S(t)
≤ T X (Fk(Wkt))- Fk)
k∈S(t)
(38)
(39)
Lastly we can bound A4 using the bound of variance of stochastic gradients as,
E[η2k；1 X VFk(Wkt))-g(t)k2]= η2E[k X m1(gk(wkt),ξkt)) -VFk(Wkt)))k2]	(40)
k∈S(t)	k∈S(t)
2
=mEs(t) [ X Ekgk(Wkt),ξkt))-VFk(Wkt))k2] (41)
k∈S(t)
22
≤ ησ	(42)
m
Using the bounds ofA1, A2, A3, A4 above we have that the expectation of the LHS of (27) is bounded
as
E[kW(t+1) - Wk k2]
≤E[kW(t) - W*k2] - ηtμE[ X kWkt) - Wkk2] + 16η2τ2G2
k∈S(t)
+ η2σ2 + 4Lη2E[ X (Fk(Wkt))- F：)] - 2ηtE[ X (Fk(Wkt))- Fk(Wk))]
mm k m k
k∈S(t)	k∈S(t)
≤(1 - ηtμ)E[kW(t) - W*k2] + 16η2τ2G2
+ '哼 + 等E[ X (Fk(Wkt))- F：)] - 2ηtE[ X (Fk(Wkt))- Fk(Wk))]
mm	m
k∈S(t)	k∈S(t)
X-----------------------------{---------------------------------}
A5
(43)
(44)
where (44) is due to Lemma B.3. Now we aim to bound A5 in (44). First we can represent A5 in a
different form as:
E[4Lk	X (Fk(Wkt))-	Fn-	2mt	X	(Fk(Wkt))-	Fk(Wk))]
k∈S(t)	k∈S(t)
=E∣T x Fk (Wkt))- 2ηt X Fk (Wkt)) - 2ηt
k∈S(t)	k∈S(t)
+ 2ηt X Fk - 4Lη2 X FM
m km k
k∈S(t)	k∈S(t)
(Fkk - Fk(Wk))
k∈S(t)
(45)
(46)
=E[2ηt(2Lnt- I, X (Fk(Wkt))- Fk)] + 2ηtE[mm
k∈S(t)
X----------------------------------}
(Fk(Wk) -Fkk)]
k∈S(t)
^z
A6
16
Under review as a conference paper at ICLR 2021
Now with ηt < 1/(4L) and νt = 2ηt(1 - 2Lηt), we have that A6 can be rewritten and bounded as
-Vt X (Fk(Wkt))- Fk(w(t)) + Fks- Fk)
k∈S(t)
- Vt X (Fk(Wa- Fk(W(t)))一
k∈S(t)
Vt X (Fk(W㈤)-Fk)
m
k∈S(t)
(47)
≤-Vt	X	[hVFk (W ⑴),Wkt)-W㈤i+	2 kwkt)	-	W(t)k2]	- Vt	X	(Fk (W(t)) - Fk)
k∈S(t)	k∈S(t)
(48)
≤Vt X 5L(Fk(W㈤)-Fk)+(W- 2)iiWkt) - W(t)k2
k∈S(t)	ηt
- mmt X (Fk(W(t)) - Fk)
k∈S(t)
(49)
=-mt(1 - ηtL) X (Fk(W(t)) - Fko+ 就-霁 X kWkt) - W(t)k2	(50)
k∈S(t)	t	k∈S(t)
≤- Vt(I-ηtL X (Fk(W(t))- Fk)+ɪ X kWk"-W(t)k2	(51)
k∈S(t)	k∈S(t)
where (48) is due to μ-convexity, (49) is due to Lemma B.1 and the AM-GM inequality and
Cauchy-Schwarz inequality, and (51) is due to the fact that Vt(I-;*) ≤ 1. Hence using this bound
of A6 we can upper bound A5 as
E[4Lηt- X (Fk(Wkt))- Fk)-寺 X (Fk(Wkt)) - Fk(W*))]
k∈S(t)	k∈S(t)
≤ɪE[ X kWkt)- W(t)k2] - Vt(I- ηtL)E[ X (Fk(W㈤)-Fk)]
mm
k∈S(t)	k∈S(t)
+ 2mtE[ X (Fk(w*) - FQ]	(52)
k∈S(t)
≤16η2τ2G2 - m(1 - ηtL)E[ X (Fk(W⑴)-F：)] + 2ηtE[ X (Fk(w*) - F：)]	(53)
k∈S(t)	k∈S(t)
K
=16η2τ2G2 - %(1 - ηtL)E[ρ(S(∏, W(Tbt/Tc)), W(t))(F(W⑴)-XpkF*)]
k=1
K
+ 2ηtE[ρ(S(π, W(Tbt/Tc)), w*)(F* - XpkF*)]	(54)
k=1
K
≤16η2τ2G2 -Vt(I- ηtL)ρ(E[F(W⑴)]-XPkFk) +2%pΓ	(55)
k=1
X---------------{--------------}
A7
17
Under review as a conference paper at ICLR 2021
where (54) is due to the definition ofρ(S(π, w), w0) in Definition 3.2 and (55) is due to the definition
of Γ in Definition 3.1 and the definitions of ρ, Pin Definition 3.2. We can expand A7 in (55) as
K
-νt(1 - ηtL)ρ(E[F(W㈤)]-XPkFk)	(56)
k=1
K
=-νt(1 - ηtL)ρ X Pk (E[Fk(W㈤]-F * + F * - Fk)	(57)
k=1
KK
=-νt(1 - ηtL)ρ XPk(E[Fk(W㈤]-F*) - νt(1 - %L)ρ XPk(F* - Fk)	(58)
k=1	k=1
=-νt(1 - ηtL)ρ(E[F(W(t))] - F*) - νt(1 - ηtL)ρΓ	(59)
≤ - Vt(I-}L)“pE[kw㈤一w*k2] - νt(1 - ηtL)ρΓ	(60)
≤ - 3ηtμρE[kw⑴一w*k2] - 2ηt(1 - 2Lηt)(1 - ηtL)ρΓ	(61)
8
≤ - 3ηtμρE[kw⑴-w*k2] - 2ηtρΓ + 6η2ρLΓ	(62)
8
where (60) is due to the μ-convexity, (61) is due to -2ηt(1 - 2Lηt)(1 - ηtL) ≤ -3ηt, and (62) is
due to -(1 - 2Lηt)(1 - ηtL) ≤ -(1 - 3Lηt). Hence we can finally bound A5 as
警E[ X (Fk(Wkt))- Fk)-* X (Fk(Wkt))- Fk(w*))]
k∈S(t)	k∈S(t)
≤ - 3ηtμρE[kw⑴-w*k2]+ 2ηtΓ(P- ρ) + η2(6ρLΓ + 16τ2G2)	(63)
8
Now we can bound E[kw(t+1) - w*k2] as
E[kw(t+1) - w*k2] ≤ 1 - ηtμ(1 + 3ρ)[ E[kw⑴-w*k2]
+ η2 (32τ2G2 +----+ 6pLr) + 2ηtΓ(ρ — P)
m
By defining ∆t+ι = E[||w(t+1)- w*∣∣2], B = 1 + 3ρ, C = 32τ2G2 + * + 6ρLΓ, D = 2Γ(p-ρ),
we have that
△t+i ≤ (1 - ηtμB)∆t + η2c + ηtD	(65)
By setting △ ≤ t+γ, η = t+γ and β > μB, γ > 0 by induction we have that
ψ = max (Yl∣w(0) - w*k2,	-- (β2C + Dβ(t + Y))
I	βμB — 1
Then by the L-smoothness of F(∙), we have that
E[F (W⑴)]-F * ≤ 2∆t ≤ L γψ+t
(66)
(67)
D Proof of Theorem A.1
With fixed learning rate ηt = η, we can rewrite (65) as
△t+1 ≤ (1 - ημB)∆t + η2C + ηD	(68)
and with η ≤ min{五^, ± } using recursion of (68) we have that
△t ≤ (1 - ημB)t∆o + η2C +ηD (1 - (1 - ημB)t)	(69)
ημB
18
Under review as a conference paper at ICLR 2021
Using ∆t ≤ 2(F(w(t)) - F*) and L-Smoothness, We have that
L
μ
FBt))-F*≤L(I-ημBy(F(W(O))-F*)+ LnC+D)(I-(I-ημB)t)(70)
1 - nμ (1 + 38P )〕t (F (W(0)) - F *)+-Ji”-nμ (7 )〕[⑺)
E Extension: Generalization to different averaging schemes
While we considered a simple averaging scheme where W(t+1) = ml Pfc∈5(t) w(kt) - ηtgk (w(kt)) ,
we can extend the averaging scheme to any scheme q such that the averaging weights qk are invariant
in time and satisfies Pk∈S (t) qk = 1 for any t. Note that q includes the random sampling without
replacement scheme introduced by Li et al. (2020) where the clients are sampled uniformly at
random without replacement with the averaging coefficients qk = pk K/m. With such averaging
scheme q, we denote the global model for the averaging scheme qk as wb (t), where wb (t+1) ,
Pk∈S(t) qk wk(t) - ηtgk (wk(t)) , and the update rule changes to
W(t+1) = W㈤-ntb(t) = W㈤-nt( X qkgk(Wkt),ξ(kt)))	(72)
k∈S(t)
where gb(t) = Pk∈S (t) qkgk(W(kt), ξk(t)). We show that the convergence analysis for the averaging
scheme q is consistent with Theorem 3.1. In the case of the averaging scheme q, we have that
Lemma B.2 and Lemma B.3 shown in Appendix B, each becomes
-^E[ ^X ∣∣^W(t) — Wkt) ∣∣2] ≤ 16n2m(m — 1)τ2G2	(73)
m k∈S (t)
E[∣W⑴-W*k2] ≤ mE[ X qk∣Wkt)- W*k2]	(74)
k∈S(t)
Then, using the same method we used for the proof of Theorem 3.1, we have that
E[∣W(t+1) — W*∣2] ≤(1 — ntμ) E[∣W(t) — W*∣2] + n2σ2m + 16m2(m — 1)η2τ2G2 +
2Lnt2(1 + m X qk(Fk(Wkt))- Fki)- 2nt X qk(Fk(Wkt))- Fk(W*))
k∈S(t)	k∈S(t)
:_______________________________ - /
M
By defining the selection skew for averaging scheme q similar to Definition 5 as
ρq(S(π, W), W0)
ES(π,w)Ek∈S(π,w) qk(Fk(WO)- FQ
F (W0)- PK=I Pk Fk
≥ 0,
and
Pq，min ρq(S(π, w), w0)
w,w0
ρeq , max ρq(S(π, W), W*)
w
maxw E5(∏,w)Ek∈s(∏,w) Qk(Fk(W*) - F*)]
Γ
(75)
(76)
(77)
(78)
E
J
With ηt < 1/(2L(1 + m)), using the same methodology for proof of Theorem 3.1 we have that M
becomes upper bounded as
E 2Ln2(1 + m) X qk(Fk(Wkt)) - F*)- 2% X qk(Fk(Wkt))- Fk(w*))	(79)
k∈S(t)	k∈S(t)
≤ -ntμPqE[∣W(t) - W*k2] + 2ntΓ(eq - Pq) + 16m2(m - 1)n2τ2G2 + 2Lη2(2 + m)pqΓ
(80)
19
Under review as a conference paper at ICLR 2021
Finally we have that
E[kW(t+1) - w*k2] ≤
1 - ηtμ (m +	E[kw(t) - w*k2]+ 2ηtr(eq - Pq)
(81)
+η2[32m2(m — 1)τ 2G2 + σ2m + 2L(2 + m)pqΓ]
By defining ∆t+ι = E[∣∣W(t+1) — w*∣∣2], B = -m + Pq, Cbb = 32m2(m — 1)τ2G2 + σ2m + 2L(2 +
m)pqΓ, Db = 2Γ(pq - pq), We have that
^	,	^. ^	n ^	^
△ t+ι ≤ (I - ηtμB)△ t+ ηt C + ηtD
(82)
Again, by setting △ t ≤ %,η =号 and β > 1b,, γ > 0 by induction we have that
ψ = max {γ∣∣W(0) - w*k2, e-B—ι (β2C + DDβ(t + Y)) }
Then by the L-Smoothness of F(∙), we have that
(83)
E[F(w(t))] - F*≤ ∣∆t ≤ Lγ+l
(84)
With β = —, γ = 4m(1+m)L and η = ɪ, we have that
' μ ' '	μ	t+γ'
E[F(w(T))] - F * ≤
1	Lm2 (32m(m — 1)τ 2G2 + σ2)	2L2m(m + 2)Γ Lγ ∣∣w(0) — w*∣∣2
(T + Y) [	μ2Pq	+ μ2	+	2
,-----------------------------------------7--------------------------------------
Vanishing Error Term
(85)
which is consistent with Theorem 3.1.
F Experiment Details
Quadratic Model Optimization. For the quadratic model optimization, we set each local objective
function as strongly convex as follows:
Fk(W) = Iw>HkW - e>w + 1 e>H-1ek	(86)
Hk ∈ Rv×v is a diagonal matrix Hk = hkI with hk 〜U(1,20) and ek ∈ Rv is an arbitrary vector.
We set the global objective function as F(w) = PkK=1 pkFk(w), where the data size pk follows the
power law distribution P(x; a) = axa-1, 0 ≤ x ≤ 1, a = 3. We can easily show that the optimum
for Fk(w) and F(w) is w*k = Hk-1ek and w* = (PkK=1pkHk)-1(PkK=1pkek) respectively. The
gradient descent update rule for the local model of client k in the quadratic model optimization is
wk(t+1) = wk(t) - η(Hkw(kt) - ek)	(87)
where the global model is defined as w(t+1) = —1- Pk∈s(t) wf+1). We sample m = KC clients
for every round where for each round the clients perform τ gradient descent local iterations with
fixed learning rate η and then these local models are averaged to update the global model. For
the implementation of πadapow-d, d was decreased half from d = K for every 5000 rounds. For all
simulations we set τ = 2, v = 5, η = 2 × 10-5.
For the estimation of P and P for the quadratic model, we get the estimates of the theoretical
P, P values by doing a grid search over a large range of possible w, w0 for P(S(∏, w), w0) and
20
Under review as a conference paper at ICLR 2021
P(S(π, w), w*) respectively. The distribution of S(π, W) is estimated by simulating 10000 iterations
of client sampling for each π and w.
Logistic Regression on Synthetic Dataset. We conduct simulations on synthetic data which allows
precise manipulation of heterogeneity. Using the methodology constructed in (Sahu et al., 2019), we
use the dataset with large data heterogeneity, Synthetic(1,1). We assume in total 30 devices where
the local dataset sizes for each device follows the power law. For the implementation of πadapow-d,
d was decreased to d = m from d = K at half the entire communication rounds. We set the mini
batch-size to 50 with T = 30, and η = 0.05, where η is decayed to n/2 every 300 and 600 rounds.
DNN on FMNIST Dataset. We train a deep multi-layer perceptron network with two hidden layers
on the FMNIST dataset (Xiao et al., 2017). We construct the heterogeneous data partition amongst
clients using the Dirichlet distribution DirK (α) (Hsu et al., 2019), where α determines the degree of
the data heterogeneity across clients (the data size imbalance and degree of label skew across clients).
Smaller α indicates larger data heterogeneity. For all experiments we use mini-batch size of b = 64,
with τ = 30 and η = 0.005, where η is decayed by half for every 150, 300 rounds. We experiment
with three different seeds for the randomness in the dataset partition across clients and present the
averaged results.
All experiments are conducted with clusters equipped with one NVIDIA TitanX GPU. The number
of clusters we use vary by C, the fraction of clients we select. The machines communicate amongst
each other through Ethernet to transfer the model parameters and information necessary for client
selection. Each machine is regarded as one client in the federated learning setting. The algorithms
are implemented by PyTorch.
Pseudo-code of the variants of pow-d: cpow-d and rpow-d. We here present the pseudo-code for
πcpow-d and πrpow-d. Note that the pseudo-code for πcpow-d in Algorithm 1 can be generalized to the
algorithm for ∏p°w-d, by changing 看 Pξ∈ξk f(w, ξ) to Fk(w).
Algorithm 1 Pseudo code for cpow-d: computation efficient variant of pow-d
1:
2:
3:
4:
5:
6:
7:
8:
Input: m, d, Pk for k ∈ [K], mini-batch size b = ∣ξk | for computing 言 Pξ∈b f (w, ξ)
Output: S(t)
Initialize: empty sets S(t) and A
Global server do
Get A = {d indices sampled without replacement from [K] by pk}
Send the global model w(t) to the d clients in A
Receive 言 Pξ∈b f (w, ξ) from all clients in A
Get S(t) = {m clients with largest 言 Pξ∈b f (w, ξ) (break ties randomly)}
9:	Clients in A in parallel do
10:	Create mini-batch ξk from sampling b samples uniformly at random from Bk and compute
言 Pξ∈b f (w,ξ) and send it to the server
11:	return S(t)
21
Under review as a conference paper at ICLR 2021
Algorithm 2 Pseudo code for rpow-d: computation & communication efficient variant of pow-d
1:	Input: m, d, pk for k ∈ [K]
2:	Output: S(t)
3:	Initialize: empty sets S(t) and A, and list Atmp with K elements all equal to inf
4:	All client k ∈ S (t-1) do
5:	For t mod τ = 0, send τb Pt=t-τ+ι Pξ∈ξ(i) f (Wk),ξ) to the server With its local model
6:	Global server do
7： Receive and update AtmP[k] = τ⅛ Pt=t-τ+1 Pξ∈ξf) f (wkl), ξ) for k ∈ S(t-1)
8:	Get A = {d indices sampled Without replacement from [K] by pk}
9:	Get S(t) = {m clients With largest values in [Atmp [i] for i ∈ A], (break ties randomly)}
10:	return S(t)
G Additional Experiment Results
G. 1	Selected Client Profile
We further visualize the difference betWeen our proposed sampling strategy πpoW-d and the baseline
scheme πrand by shoWing the selected frequency ratio of the clients for K = 30, C = 0.1 for the
quadratic simulations in Figure 7. Note that the selected ratio for πrand reflects each client’s dataset
size. We shoW that the selected frequencies of clients for πpoW-d are not proportional to the data size
of the clients, and We are selecting clients frequently even When they have relatively loW data size
like client 6 or 22. We are also not necessarily frequently selecting the clients that have the highest
data size such as client 26. This aligns Well With our main motivation of Power-of-choice that
Weighting the clients’ importance based on their data size does not achieve the best performance, and
rather considering their local loss values along With the data size better represents their importance.
Note that the selected frequency for πrand is less biased than πpoW-d.
O 0.04
⅛
■§0.02
Q
Q
S
0.00
rand
Client index
OIaJ pəloəɪəs
pow-d. d=6
05
00
Client index
(a) Selected client profile for πrand	(b) Selected client profile for πpow-d
Figure 7:	Clients’ selected frequency ratio for optimizing the quadratic model for πrand and πpow-d
with K = 30, C = 0.1. The selected ratio is sorted in the descending order.
G.2 Communication and Computation Efficiency with larger data
HETEROGENEITY
In Table 2, we show the communication and computation efficiency of Power-of-choice for
α = 2, as we showed for α = 0.3 in Table 1 in Section 5. With C = 0.03 fraction of clients,
πpow-d , πcpow-d , and πrpow-d have better test accuracy of at least approximately 10% higher test
accuracy performance than (πrand, C = 0.1). R60 for πpow-d, πcpow-d, πrpow-d is 0.61, 0.66, 0.73
times that of (πrand, C = 0.1) respectively. This indicates that we can reduce the number of
communication rounds by at least 0.6 using 1/3 of clients compared to (πrand, C = 0.1) and still get
higher test accuracy performance. The computation time tcomp for πcpow-d and πrpow-d with C = 0.03
is smaller than that of (πrand, C = 0.1).
22
Under review as a conference paper at ICLR 2021
Table 2: Comparison of R60, tcomp (sec), and test accuracy (%) for different sampling strategies with
α = 2. The ratio R60 / (R60 for rand, C = 0.1) and tcomp / (tcomp for rand, C = 0.1) are each shown
in the parenthesis.
	C = 0.1	C = 0.03				
	rand	rand	pow-d, d = 6 cpow-d, d = 6		rpow-d, d = 50	afl
R60	135	136(1.01)	82 (0.61)	89 (0.66)	99(0.73)	131(0.97)
tcomp	0.42	0.36(0.85)	-0.46 (1.08)	0.38 (0.88)	0.36(0.86)	0.36(085)
Test Acc.	63.50±2.74	66.03±1.47	73∙81±1.14	73.36±1.17	72.52±0.89	70.64±1.99
Communication round
(a)	Test accuracy and training loss for α = 2
(b)	Test accuracy and training loss for α = 0.3
Figure 8:	Test accuracy and training loss in the virtual environment where clients have intermittent
availability for K = 100, C = 0.03 with πrand, πpow-d, and πrpow-d on the FMNIST dataset. For both
α=2 and α = 3, πpow-d achieves approximately 10% higher test accuracy than πrand .
G.3 Intermittent Client Availability
In real world scenarios, certain clients may not be available due to varying availability of resources
such as battery power or wireless connectivity. Hence we experiment with a virtual scenario, where
amongst K clients, for each communication round, we select clients alternately from one group out of
two fixed groups, where each group has 0.5K clients. This altering selection reflects a more realistic
client selection scenario where, for example, we have different time zones across clients. For each
communication round, we select 0.1 portion of clients from the corresponding group uniformly at
random and exclude them from the client selection process. This random exclusion of certain clients
represents the randomness in the client availability within that group for cases such as low battery
power or wireless connectivity. In Figure 8 we show that πpow-d and πrpow-d achieves 10% and 5% test
accuracy improvement respectively compared to πrand for α = 2. For α = 3, both πpow-d and πrpow-d
shows 10% improvement. Therefore, we demonstrate that Power-of-choice also performs well in
a realistic scenario where clients are available intermittently.
G.4 Results for DNN on Non-iid Partitioned EMNIST Dataset
To provide further validation of the consistency in our results of πpow-d and its variants on the
FMNIST dataset, we present additional experiment results on the EMNIST dataset sorted by digits
with K = 500, C = 0.03. We train a deep multi-layer perceptron network with two hidden layers on
the dataset partitioned heterogeneously across the clients in the same way as for the FMNIST dataset.
For all experiments, we use b = 64, τ = 30, and η = 0.005 where η is decayed by half at round 300.
In Figure 9, we show that πpow-d performs with significantly higher test accuracy than πrand for varying
d for both α = 2 and 0.3. For α = 2, πafl is able to follow the performance of πpow-d in the later
communication rounds, but is slower in achieving the same test accuracy than πpow-d. Moreover, in
Figure 10, we show that πcpow-d works as good as πpow-d for both large and small data heterogeneity.
The performance of πrpow-d falls behind πpow-d and πcpow-d for smaller data heterogeneity, whereas for
larger data heterogeneity, πrpow-d is able to perform similarly with πpow-d and πcpow-d.
23
Under review as a conference paper at ICLR 2021
Communication round
K=500, C=0.03
rand
—■ pow-d, d=30
—— pow-d, d=45
, ■ , , pow-d, d=75
- afl
80
200	300
Communication round
整
⅛40
30
20
P^
K=500, C=0.03
rand
, pow-d, d=30
-- pow-d, d=45
, ■ , ■ pow-d, d=75
— afl
lŋŋ 200	3U0
Communication round
K=500, C=0.03
-0.8
'I-°-6
^0 ΠJO 2D0	3U0
Communication round
(a)	Test accuracy and training loss for α = 2
(b)	Test accuracy and training loss for α = 0.3
Figure 9:	Test accuracy and training loss for different sampling strategies for K = 500, C = 0.03
with πrand , πpow-d, and πafl on the EMNIST dataset.
80
8so
KJ
W 40
H 30
20
10
^TΓ
K=500, C=0.03
1U(Γ
M(Γ
300
Conununication round
pow-d, d=30
CPOWY, d=30
ɪpow-d, d=250
afl
SSoIωψσ
K=500, C=0.03
K=500, C=0.03
Communication round
Communication round
K=500, C=0.03
Communication round
(a) Test accuracy and training loss for α = 2	(b) Test accuracy and training loss for α = 0.3
Figure 10:	Test accuracy and training loss for different sampling strategies for K = 500, C = 0.03
with πrand , πpow-d , πcpow-d , πrpow-d, and πafl on the EMNIST dataset.
G.5 Effect of the fraction of selected clients
K=IOO5 C=0.10
CommiLnication round
Ooooooo
7 6 5 4 3 2 1
K=IOO5 C=0.10
200 .	400
Communication round
1 rand
—pow-ds d=20
--- cρow-d, d=20
....rpow-⅜ d=50
— afl
(a) Test accuracy and training loss for α = 2	(b) Test accuracy and training loss for α = 0.3
Figure 11: Test accuracy and training loss for different sampling strategies for K = 100, C = 0.1
with πrand, πpow-d, πcpow-d, πrpow-d, and πafl on the FMNIST dataset. For larger C = 0.1, πpow-d
performs with 15% and 5% higher test accuracy than πrand for α = 2 and α = 0.3 respectively.
In Figure 11, for larger C = 0.1 with α = 2, the test accuracy improvement for πpow-d is even higher
than the case of C = 0.03 with approximately 15% improvement. πcpow-d performs slightly lower in
test accuracy than πpow-d but still performs better than πrand and πafl. πrpow-d performs as well as πafl.
For α = 0.3, πpow-d, πcpow-d, and πrpow-d have approximately equal test accuracy performance, higher
than πrand by 5%. The POWER-OF-CHOICE strategies all perform slightly better than πafl. Therefore
we show that Power-of-choice performs well for selecting a larger fraction of clients, i.e., when
we have larger C = 0.1 > 0.03.
G.6 Effect of the local epochs and mini-batch size
We present the experiment results elaborated in Section 5 for the different hyper-parameter settings
(b,τ) ∈ {(128, 30), (64, 100)} in Figure 12, 13, 14, and 15 below.
24
Under review as a conference paper at ICLR 2021
K=100, C=0.03
K=100, C=0.03
100	200	300
Conummication round
K=100, C=0.03
0	100	200	30C
Commimication round
70
春60
§50
2 40
S
S 30
20
read
一, pow-d, d=6
—— ρow-d, d=9
, ■ , , pow-d, d=15
- afl
K=IOOj C=0.03
(a)	Test accuracy and training loss for α = 2
K=100, C=SO3	K=I 00. C=O93
Communication round	Communication round
IOO 200	300
Communication round
T ≡5 2QC 300
Communication round
(b)	Test accuracy and training loss for α = 0.3
Figure 12: Test accuracy and training loss for πrand, πpow-d, and πafl for K = 100, C = 0.03 on the
FMNIST dataset with mini-batch size b = 128 and τ = 30.
Oooooooo
87654321
ISOI
(a) Test accuracy and training loss for α = 2	(b) Test accuracy and training loss for α = 0.3
Figure 13:	Test accuracy and training loss for πrand , πpow-d , πcpow-d , πrpow-d, and πafl for K
100, C = 0.03 on the FMNIST dataset with mini-batch size b = 128 and τ = 30.
K=100, C=0.03
Oooooo
8 7 6 5 4 3
*□BmB ISOI
100	200	300
Communication round
Ss-ω∙9∙9∪II
K=100, C=0.03
K=100, C=0.03
K=100, C=0.03
Communication round
Communication round
(a) Test accuracy and training loss for α = 2	(b) Test accuracy and training loss for α = 0.3
Figure 14:	Test accuracy and training loss for πrand, πpow-d, and πafl for K = 100, C = 0.03 on the
FMNIST dataset with mini-batch size b = 64 and τ = 100.
(a) Test accuracy and training loss for α = 2	(b) Test accuracy and training loss for α = 0.3
Figure 15:	Test accuracy and training loss for πrand , πpow-d , πcpow-d , πrpow-d, and πafl for K
100, C = 0.03 on the FMNIST dataset with mini-batch size b = 64 and τ = 100.
25