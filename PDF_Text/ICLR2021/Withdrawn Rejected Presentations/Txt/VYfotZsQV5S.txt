Under review as a conference paper at ICLR 2021
MISSO: Minimization by Incremental Stochas-
tic Surrogate Optimization for Large Scale
Nonconvex and Nonsmooth Problems
Anonymous authors
Paper under double-blind review
Ab stract
Many constrained, nonconvex and nonsmooth optimization problems can be tack-
led using the majorization-minimization (MM) method which alternates between
constructing a surrogate function which upper bounds the objective function, and
then minimizing this surrogate. For problems which minimize a finite sum of
functions, a stochastic version of the MM method selects a batch of functions
at random at each iteration and optimizes the accumulated surrogate. However,
in many cases of interest such as variational inference for latent variable mod-
els, the surrogate functions are expressed as an expectation. In this contribution,
we propose a doubly stochastic MM method based on Monte Carlo approxima-
tion of these stochastic surrogates. We establish asymptotic and non-asymptotic
convergence of our scheme in a constrained, nonconvex, nonsmooth optimization
setting. We apply our new framework for inference of logistic regression model
with missing data and for variational inference of Bayesian variants of LeNet-5
and Resnet-18 on respectively the MNIST and CIFAR-10 datasets.
1	Introduction
We consider the constrained minimization problem of a finite sum of functions:
1n
m∈in L(θ):= - ELi(θ),
i=1
(1)
where Θ is a convex, compact, and closed subset of Rp, and for any i ∈ J-, nK, the function Li :
Rp → R is bounded from below and is (possibly) nonconvex and nonsmooth.
To tackle the optimization problem (1), a popular approach is to apply the majorization-minimization
(MM) method which iteratively minimizes a majorizing surrogate function. A large number of ex-
isting procedures fall into this general framework, for instance gradient-based or proximal methods
or the Expectation-Maximization (EM) algorithm (McLachlan & Krishnan, 2008) and some varia-
tional Bayes inference techniques (Jordan et al., 1999); see for example (Razaviyayn et al., 2013)
and (Lange, 2016) and the references therein. When the number of terms n in (1) is large, the
vanilla MM method may be intractable because it requires to construct a surrogate function for all
the n terms Li at each iteration. Here, a remedy is to apply the Minimization by Incremental Sur-
rogate Optimization (MISO) method proposed by Mairal (2015), where the surrogate functions are
updated incrementally. The MISO method can be interpreted as a combination of MM and ideas
which have emerged for variance reduction in stochastic gradient methods (Schmidt et al., 2017).
An extended analysis of MISO has been proposed in (Qian et al., 2019).
The success of the MISO method rests upon the efficient minimization of surrogates such as con-
vex functions, see (Mairal, 2015, Section 2.3). A notable application of MISO-like algorithms
is described in (Mensch et al., 2017) where the authors builds upon the stochastic majorization-
minimization framework of Mairal (2015) to introduce a method for sparse matrix factorization.
Yet, in many applications of interest, the natural surrogate functions are intractable, yet they are
defined as expectation of tractable functions. For instance, this is the case for inference in latent
variable models via maximum likelihood (McLachlan & Krishnan, 2008). Another application is
1
Under review as a conference paper at ICLR 2021
variational inference (Ghahramani, 2015), in which the goal is to approximate the posterior distribu-
tion of parameters given the observations; see for example (Neal, 2012; Blundell et al., 2015; Polson
et al., 2017; Rezende et al., 2014; Li & Gal, 2017).
This paper fills the gap in the literature by proposing a method called Minimization by Incremental
Stochastic Surrogate Optimization (MISSO), designed for the nonconvex and nonsmooth finite sum
optimization, with a finite-time convergence guarantee. Our work aims at formulating a generic
class of incremental stochastic surrogate methods for nonconvex optimization and building the the-
ory to understand its behavior. In particular, we provide convergence guarantees for stochastic EM
and Variational Inference-type methods, under mild conditions. In summary, our contributions are:
•	we propose a unifying framework of analysis for incremental stochastic surrogate optimiza-
tion when the surrogates are defined as expectations of tractable functions. The proposed
MISSO method is built on the Monte Carlo integration of the intractable surrogate function,
i.e., a doubly stochastic surrogate optimization scheme.
•	we present an incremental update of the commonly used variational inference and Monte
Carlo EM methods as special cases of our newly introduced framework. The analysis of
those two algorithms is thus conducted under this unifying framework of analysis.
•	we establish both asymptotic and non-asymptotic convergence for the MISSO method. In
particular, the MISSO method converges almost surely to a stationary point and in O(n/)
iterations to an -stationary point, see Theorem 1.
•	in essence, we relax the class of surrogate functions used in MISO (Mairal, 2015) and
allow for intractable surrogates that can only be evaluated by Monte-Carlo approximations.
Working at the crossroads of Optimization and Sampling constitutes what we believe to be
the novelty and the technicality of our framework and theoretical results.
In Section 2, we review the techniques for incremental minimization of finite sum functions based
on the MM principle; specifically, we review the MISO method (Mairal, 2015), and present a class
of surrogate functions expressed as an expectation over a latent space. The MISSO method is then
introduced for the latter class of intractable surrogate functions requiring approximation. In Sec-
tion 3, we provide the asymptotic and non-asymptotic convergence analysis for the MISSO method
(and of the MISO (Mairal, 2015) one as a special case). Section 4 presents numerical applications
including parameter inference for logistic regression with missing data and variational inference for
two types of Bayesian neural networks. The proofs of theoretical results are reported as Supplement.
Notations. We denote J1,nK = {1,... ,n}. Unless otherwise specified, k∙k denotes the standard
Euclidean norm and(•1•)is the inner product in the Euclidean space. For any function f : Θ → R,
f 0(θ, d) is the directional derivative of f at θ along the direction d, i.e.,
f0(θ, d):= lim f(θ+14-f ⑻
t→0+	t
(2)
The directional derivative is assumed to exist for the functions introduced throughout this paper.
2	Incremental Minimization of Finite Sum Nonconvex Functions
The objective function in (1) is composed of a finite sum of possibly nonsmooth and nonconvex
functions. A popular approach here is to apply the MM method, which tackles (1) through alter-
nating between two steps — (i) minimizing a surrogate function which upper bounds the original
objective function; and (ii) updating the surrogate function to tighten the upper bound.
As mentioned in the introduction, the MISO method (Mairal, 2015) is developed as an iterative
scheme that only updates the surrogate functions partially at each iteration. Formally, for any i ∈
J1, nK, we consider a surrogate function Li (θ; θ) which satisfies the assumptions (H1, H2):
H1. For all i ∈ J1, nK and θ ∈ Θ, Li(θ; θ) is convex w.r.t. θ, and it holds
Lbi(θ; θ) ≥Li(θ), ∀ θ ∈ Θ ,
where the equality holds when θ = θ.
(3)
2
Under review as a conference paper at ICLR 2021
H2. For any	θi	∈	Θ,	i ∈ J1,nK	and some e >	0,	the difference function	b(θ[{θi}n=ι)：=
1 ∑2i=ι Li(θ; θi) — L(θ) is defined for all θ ∈ Θe and dferentiable for all θ ∈ Θ, where
Θ = {θ ∈ Rd, inf θ0 ∈Θ kθ - θ0k < e} is an e-neighborhood set ofΘ. Moreover, for some constant
L, the gradient satisfies
kVb(θ; {θi}n=ι)k1 2 3 4 5 6 7 ≤ 2Lb(θ; {θi }n=ι), ∀ θ ∈ Θ .	(4)
Algorithm 1 The MISO method (Mairal, 2015).
1: Input: initialization θ(0) .
2: Initialize the surrogate function as
A0(θ) := Li(θ; θ⑼),i ∈ J1,nK.
3: for k = 0, 1, ..., Kmax do
4: Pick ik uniformly from J1, nK.
5: Update Aik+1(θ) as:
Lbi(θ; θ(k)), ifi= ik
Aik (θ),	otherwise.
6: Set θ(k+v) ∈ arg min，Pn=ι Ak+1 (θ).
θ∈Θ
7: end for
Aik+1(θ) =
We remark that H1 is a common assumption
used for surrogate functions, see (Mairal, 2015,
Section 2.3). H2 can_be satisfied When the differ-
ence function b(θ; {θi}n=ι) is L-Smooth, i.e., b
is differentiable on Θ and its gradient Veb is L-
Lipschitz, ∀θ ∈ Θ. H2 can be implied by apply-
ing (Razaviyayn et al., 2013, Proposition 1).
The inequality (3) implies L%(θ; θ) ≥ Li(θ) >
-∞ for any θ ∈ Θ. The MISO method is
an incremental version of the MM method, as
summarized by Algorithm 1, Which shoWs that
the MISO method maintains an iteratively up-
dated set of upper-bounding surrogate functions
{Aik(θ)}in=1 and updates the iterate via minimiz-
ing the average of the surrogate functions.
Particularly, only one out of the n surrogate functions is updated at each iteration [cf. Line 5] and
the sum function，Pn=ι Ak+1(θ) is designed to be ‘easy to optimize,, which, for example, can be
a sum of quadratic functions. As such, the MISO method is suitable for large-scale optimization as
the computation cost per iteration is independent of n. Under H1, H2, it was shown that the MISO
method converges almost surely to a stationary point of (1) (Mairal, 2015, Prop. 3.1).
We now consider the case when the surrogate functions Li (θ; θ) are intractable. Let Z be a mea-
surable set, pi : Z × Θ → R+ a probability density function, ri : Θ × Θ × Z → R a measurable
function and μ% a σ-finite measure. We consider surrogate functions which satisfy H1, H2 and that
can be expressed as an expectation, i.e.:
^ ，- _r、	/	，_ _r 、	，	-r、 ，-	、	，.，一 一r、 „	„
Li(θ; θ) :=	J ri(θ; θ, Zi)pi(zi； θ)μi(dzi)	∀ (θ, θ)	∈ Θ	X Θ	.
(5)
Plugging (5) into the MISO method is not feasible since the update step in Step 6 involves a mini-
mization of an expectation. Several motivating examples of (1) are given in Section 2.
In this paper, we propose the Minimization by Incremental Stochastic Surrogate Optimization
(MISSO) method which replaces the expectation in (5) by Monte Carlo integration and then op-
timizes the objective function (1) in an incremental manner. Denote by M ∈ N the Monte Carlo
batch size and let {zm ∈ Z}mM=1 be a set of samples. These samples can be drawn (Case 1) i.i.d.
from the distribution pi(∙; θ) or (Case 2) from a Markov chain with stationary distribution Pi(∙; θ);
see Section 3 for illustrations. To this end, we define the stochastic surrogate as follows:
〜—	1 M _
Li(θ; θ, {Zm}M=ι) ：= M £『，(。；θ,Zm) ,	(6)
m=1
and we summarize the proposed MISSO method in Algorithm 2. Compared to the MISO method,
there is a crucial difference in that the MISSO method involves two types of randomness. The first
level of randomness comes from the selection ofik in Line 5. The second level of randomness stems
from the set of Monte Carlo approximated functions Aeik(θ) used in lieu of Aik(θ) in Line 6 when
optimizing for the next iterate θ(k). We now discuss two applications of the MISSO method.
Example 1: Maximum Likelihood Estimation for Latent Variable Model. Latent variable mod-
els (Bishop, 2006) are constructed by introducing unobserved (latent) variables which help explain
the observed data. We consider n independent observations ((yi, zi), i ∈ JnK) where yi is observed
and zi is latent. In this incomplete data framework, define {fi(zi, θ), θ ∈ Θ} to be the complete
3
Under review as a conference paper at ICLR 2021
Algorithm 2 The MISSO method.
1:	Input: initialization θ(0); a sequence of non-negative numbers {M(k)}k∞=0.
2:	For all i ∈ J1, n』,draw M(o)Monte Carlo samples with the stationary distribution pi(∙; θ(O)).
3:	Initialize the surrogate function as
A0(θ) := Lei(θ; θ⑼,{z(,m}M(0)), i ∈ J1,nK .
4:	for k = 0, 1, ..., Kmax do
5:	Pick a function index ik uniformly on J1, nK.
6:	Draw M(k)Monte Carlo samples with the stationary distributionpi(∙; θ(k)).
7:	Update the individual surrogate functions recursively as:
Lei(θ; θ(k), {zi(,km)}mM=(k1)), ifi=ik
Aik (θ),	otherwise.
8:	Set θ(k+1) ∈ argmi□θ∈θ Le(k+1)(θ) := ɪ pn=1 Ak+1(θ).
9:	end for
Aeik+1(θ) =
data likelihood models, i.e., the joint likelihood of the observations and latent variables. Let
gi(θ)
f fi(zi, θ)μi(dzi),
i ∈ J1, nK, θ ∈ Θ
denote the incomplete data likelihood, i.e., the marginal likelihood of the observations yi . For ease
of notations, the dependence on the observations is made implicit. The maximum likelihood (ML)
estimation problem sets the individual objective function Li (θ) to be the i-th negated incomplete
data log-likelihood Li(θ) := - log gi(θ).
Assume, without loss of generality, that gi(θ) 6= 0 for all θ ∈ Θ. We define by pi(zi, θ) :=
fi(zi, θ)/gi(θ) the conditional distribution of the latent variable zi given the observations yi. A sur-
rogate function Li(θ; θ) satisfying H1 can be obtained through writing fi(zi, θ) = Pi(Zi[)Pi(zi, θ)
and applying the Jensen inequality:
Li(θ; θ) = / Iog(Pi(zi, θ)/fi(zi, θ)) Pi(zi, θ)μi(dzi).
=Ti(θ∙)θ,Zi)
(7)
We note that H2 can also be verified for common distribution models. We can apply the MISSO
method following the above specification of ri,(θ; θ,zi) andpi,(zi,, θ).
Example 2: Variational Inference. Let ((xi, yi), i ∈ J1, nK) be i.i.d. input-output pairs and w ∈
W ⊆ Rd be a latent variable. When conditioned on the input data x = (xi, i ∈ J1, nK), the joint
distribution ofy = (yi, i ∈ J1, nK) and w is given by:
p(y, w|x) = π(w) Qin=1 p(yi|xi, w) .	(8)
Our goal is to compute the posterior distribution p(w|y, x). In most cases, the posterior dis-
tribution p(w|y, x) is intractable and is approximated using a family of parametric distributions,
{q(w, θ), θ ∈ Θ}. The variational inference (VI) problem (Blei et al., 2017) boils down to minimiz-
ing the Kullback-Leibler (KL) divergence between q(w, θ) and the posterior distribution p(w|y, x):
∏un L(θ) := KL(q(w; θ) ||p(w∣y,x)) := Eq(w；e)[log (q(w; θ)∕p(w∣y,x))].
Using (8), we decompose L(θ) = n-1 Pin=1 Li (θ) + const. where:
Li(θ) := -Eq(w；e)[logp(yi∣Xi,w)] + nEq(w；e)[log q(w; θ)∕∏(w)] := r (θ) + d(θ).
Directly optimizing the finite sum objective function in (9) can be difficult. First, with n
(9)
(10)
1,
evaluating the objective function L(θ) requires a full pass over the entire dataset. Second, for some
4
Under review as a conference paper at ICLR 2021
complex models, the expectations in (10) can be intractable even if we assume a simple parametric
model for q(w; θ). Assume that Li is L-smooth. We apply the MISSO method with a quadratic
surrogate function defined as:
Lbi(θ; θ) := Li(θ) +〈VeLi(θ) | θ - θ) + 2kθ - θk2, (θ,θ) ∈ Θ2 .	(11)
It is easily checked that the quadratic function Li(θ; θ) satisfies H1, H2. To compute the gradient
VLi(θ), We apply the re-parametrization technique suggested in (Paisley et al., 2012; Kingma &
Welling, 2014; BlUndell et al., 2015). Let t : Rd X Θ → Rd be a differentiable function w.r.t. θ ∈ Θ
which is designed such that the law of W = t(z, θ) is q(∙, θ), where Z 〜Nd(0, I). By (Blundell
et al., 2015, Proposition 1), the gradient of f(∙) in (Io) is:
vθEq(w；e)[logp(yi|xi,w)] = Ez~Nd(o,i)[Jθ(Z,θ)vwlogp(yi∖xi,w)∖w=t(zθ)∖ ,	(12)
where for each Z ∈ Rd, Jθ (z, θ) is the Jacobian of the function t(z, ∙) with respect to θ evaluated at
θ. In addition, for most cases, the term Vd(θ) can be evaluated in closed form as the gradient of the
KL between the prior distribution ∏(∙) and the variational candidate q(∙, θ).
ri(θ; θ,z) := DVe d(θ) - Jθ (z, θ)Vw log p(yi∖xi ,w)∖w=t(zθ) ∖ θ - θ) + IL ∣∣θ - θ∣∣2 .	(13)
M
Finally, using (11) and (13), the surrogate function (6) is given by Li(θ; θ, {zm}M=ι)：=
MT PM=I ri(θ; θ,Zm) where {zm}MM=ι are i.i.d samples drawn from N(0, I).
3 Convergence Analysis
We now provide asymptotic and non-asymptotic convergence results of our method. Assume:
H3. For all i ∈ J1,nK, θ ∈ Θ, Zi ∈ Z, r (∙; θ,Zi) is convex on Θ and is lower bounded.
We are particularly interested in the constrained optimization setting where Θ is a bounded set. To
this end, we control the supremum norm of the MC approximation, introduced in (6), as:
H4. For the samples {Zi,m}mM=1, there exist finite constants Cr and Cgr such that
Cr = sup sup ɪ:
θ∈Θ M>0 VZM
Eg SUp
e∈Θ
XX {ri(θ∕,Zi,m)-Li(θ;θ)}∖]
m=1
sup sup ME Eg
θ∈θ m>0
ɪ XX Lbi(θ, θ - θ; θ)-r0(θ, θ - θ; θ,Zi,m)
θ∈Θ M m=i	kθ -θk
for all i ∈ J1,nK, and we denoted by Eg[∙] the expectation w.r.t. a Markov chain {zi,m}M=ι with
initial distribution ξi(∙; θ), transition kernel 口分 g, and stationary distributionpi(∙; θ).
Some intuitions behind the controlling terms: It is common in statistical and optimization prob-
lems, to deal with the manipulation and the control of random variables indexed by sets with an
infinite number of elements. Here, the controlled random variable is an image of a continuous func-
tion defined as %(θ; θ, Zi,m) 一 Li (θ; θ) for all Z ∈ Z and for fixed (θ, θ) ∈ Θ2. To characterize
such control, we will have recourse to the notion of metric entropy (or bracketing number) as de-
veloped in (Van der Vaart, 2000; Vershynin, 2018; Wainwright, 2019). A collection of results from
those references gives intuition behind our assumption H4, which is classical in empirical processes.
In (Vershynin, 2018, Theorem 8.2.3), the authors recall the uniform law of large numbers:
E
∖1M
SuP T7∑f(Zi，m)- Ef(Zi)I
f∈F∖Mi=1
CL
≤ √M
for all Zi,m,i ∈ J1,MK ,
where F is a class of L-Lipschitz functions. Moreover, in (Vershynin, 2018, Theorem 8.1.3 )
and (Wainwright, 2019, Theorem 5.22), the application of the Dudley inequality yields:
1	/1 Z_________________
E[sup |Xf - X0|] ≤ √M J VlogN(F, k ∙ k∞,ε)dε ,
5
Under review as a conference paper at ICLR 2021
where N (F, k ∙ ∣∣∞,ε) is the bracketing number and e denotes the level of approximation (the brack-
eting number goes to infinity when → 0). Finally, in (Van der Vaart, 2000, p.271, Example),
N (F, k ∙ ∣∣∞, ε) is bounded from above for a class of parametric functions F = fθ : θ ∈ Θ:
N (F, ∣∣T∣∞,ε) ≤ K (diam θ ) , for all 0 <ε< diamΘ .
The authors acknowledge that those bounds are a dramatic manifestation of the curse of dimension-
ality happening when sampling is needed. Nevertheless, the dependence on the dimension highly
depends on the class of surrogate functions F used in our scheme, as smaller bounds on these con-
trolling terms can be derived for simpler class of functions, such as quadratic functions.
Stationarity measure. As problem (1) is a constrained optimization task, we consider the following
stationarity measure:
g⑹:=θf [(θ'θθkθ) and g⑹=g+ ⑻-g-⑼,
(14)
where g+(θ) := maχ{0, g(θ)}, g-(θ) := — min{0, g(θ)} denote the positive and negative part of
g(θ), respectively. Note that θ is a stationary point if and only if g- (θ) = 0 (Fletcher et al., 2002).
Furthermore, suppose that the sequence {θ(k)}k≥o has a limit point θ that is a stationary point,
then one has limk→∞ g-(θ(k)) = 0. Thus, the sequence {θ(k)}k≥0 is said to satisfy an asymptotic
stationary point condition. This is equivalent to (Mairal, 2015, Definition 2.4).
To facilitate our analysis, we define τik as the iteration index where the i-th function is last accessed
in the MISSO method prior to iteration k, τik+1 = k for instance. We define:
Kmax -1
LW(θ) := n Pn=ILi(θ;θ(τk)), b(k)(θ) := L(k)(θ) -L(θ), M(k) := X M(k)/2 .(15)
k=0
We first establish a non-asymptotic convergence rate for the MISSO method:
Theorem 1.	Under H1-H4. For any Kmax ∈ N, let K be an independent discrete r.v. drawn
UniformIyfrOm {0,…,Kmax - 1} and define the following quantity:
∆(κmax) := 2nLE[L⑼(θ(O))- L(Kmax)(θ(Kmax))] +4LCrM.).
Then we have following non-asymptotic bounds:
E[kVb<κ)(θ(K))k2] ≤ δ(⅛ and E[g-(θ(K))] ≤ ʌ^K^ + ~C~M(k) . (16)
Kmax	Kmax Kmax
Note that ∆(Kmax ) is finite for any Kmax ∈ N.
Iteration Complexity of MISSO. As expected, the MISSO method converges to a stationary point
of(1) asymptotically and at a sublinear rate E[g(K)] ≤ O(P∆(κmax) /Kmax). In other terms, MISSO
requires O(nL/e) iterations to reach an e-stationary point when the suboptimality condition, that
characterizes stationarity, is E ∣g-(θ(K))∣2 . Note that this stationarity criterion are similar to the
usual quantity used in stochastic nonconvex optimization, i.e., E ∣VL(θ(K))∣2 . In fact, when the
optimization problem (1) is unconstrained, i.e., Θ = Rp, then Eg(θ(K)) = EVL(θ(K)).
Sample Complexity of MISSO. Regarding the sample complexity of our method, setting M(k) =
k2/n2, as a non-decreasing sequence of integers satisfying Pk∞=0 M(-k1)/2 < ∞, in order to keep
∆(Kmax) nL, then the MISSO method requires Pkn=L/0 k2 /n2 = nL3/e3 samples to reach an
e-stationary point.
Furthermore,we remark that the MISO method can be analyzed in Theorem 1 as a special case
of the MISSO method satisfying Cr = Cgr = 0. In this case, while the asymptotic convergence
is well known from (Mairal, 2015) [cf. H4], Eq. (16) gives a non-asymptotic rate of E[g-(K)] ≤
6
Under review as a conference paper at ICLR 2021
O(，nL/Kmax) which is new to our best knowledge. Next, We show that under an additional
assumption on the sequence of batch size M(k), the MISSO method converges almost surely to a
stationary point:
Theorem 2.	Under H1-H4. In addition, assume that {M(k)}k≥o isa non-decreasing SeqUenCe
OfintegerS which satisfies P∞=o Mji2 < ∞. Then:
1.	the negative part ofthe stationarity measure converges a.s. to zero, i.e., lim g-(θ(k)) α= 0.
k-∞
2.	the objective value L(θ(k)) converges a.s. to a finite number L i.e., limk→∞ L(θ(k)) α= L.
In particular, the first result above shows that the sequence {θ(k)}k≥0 produced by the MISSO
method satisfies an asymptotic stationary point condition.
4 Numerical Experiments
4.1	Binary logistic regression with missing values
This application follows Example 1 described in Section 2. We consider a binary regression setup,
((yi, zi), i ∈ JnK) where yi ∈ {0, 1} is a binary response and zi = (zi,j ∈ R,j ∈ JpK) is a covariate
vector. The vector of covariates zi = [zi,mis, zi,obs] is not fully observed where we denote by zi,mis
the missing values and zi,obs the observed covariate. It is assumed that (zi, i ∈ JnK) are i.i.d. and
marginally distributed according to N(β, Ω) where β ∈ Rp and Ω is a positive definiteP X P matrix.
We define the conditional distribution of the observations yi given zi = (zi,mis, zi,obs) as:
Pi(yi∖zi) = S(δ>^yi (1 - S(δ>^i)) 1-yi ,	(17)
where for U ∈ R, S(U) = 1∕(1+e-u), δ = (δo, ∙∙∙ ,δp) are the logistic parameters and Zi = (1,zi).
Here, θ = (δ, β, Ω) is the parameter to estimate. For i ∈ JnK, the complete log-likelihood reads:
log fi(zi,mis, θ) H yiδ>Zi — log (l+exp(δ>Zi)) — 2 log(∣Ω∣) + ；Tr (Ω-1(zi - β)(zi - β)>).
Fitting a logistic regression model on the TraumaBase dataset: We apply the MISSO method
to fit a logistic regression model on the TraumaBase (http://traumabase.eu) dataset, which
consists of data collected from 15 trauma centers in France, covering measurements on patients
from the initial to last stage of trauma. This dataset includes information from the first stage of the
trauma, namely initial observations on the patient’s accident site to the last stage being intense care
at the hospital and counts more than 200 variables measured for more than 7 000 patients. Since the
dataset considered is heterogeneous - coming from multiple sources with frequently missed entries
一 we apply the latent data model described in (17) to predict the risk ofa severe hemorrhage which
is one of the main cause of death after a major trauma.
Similar to (Jiang et al., 2018), we select P = 16 influential quantitative measurements, on n =
6384 patients. For the Monte Carlo sampling of zi,mis, required while running MISSO, we run a
Metropolis-Hastings algorithm with the target distributionp(∙∣Zi,obs, yi； θ(k)).
Figure 1: Convergence of parameters δ and β for the SAEM, the MCEM and the MISSO methods.
The convergence is plotted against number of passes over the data.
We compare in Figure 1 the convergence behavior of the estimated parameters δ and β using
SAEM (Delyon et al., 1999) (with stepsize Yk = 1∕kα where α = 0.6 after tuning), MCEM (Wei
7
Under review as a conference paper at ICLR 2021
om，山 poW6(un
MC-ADAM ——BBB -⅛~ MC-SAG
MC-Momenturti ----- MISSO
0.0
Figure 2: Negated ELBO versus epochs elapsed for fitting (a) Bayesian LeNet-5 on MNIST and (b)
Bayesian ResNet-18 on CIFAR-10. The solid curve is obtained from averaging over 5 independent
runs of the methods, and the shaded area represents the standard deviation.
(a) LeNet-5 on MNIST
(b) ResNet-18 on CIFAR-10
& Tanner, 1990) and the proposed MISSO method. For the MISSO method, we set the batch size
to M(k) = 10 + k2 and we examine with selecting different number of functions in Line 5 in the
method - the default settings with 1 (MISSO), 10% (MISS010) and 50% (MISSO50) minibatches
per iteration. From Figure 1, the MISSO method converges to a static value with less number of
epochs than the MCEM, SAEM methods. It is worth noting that the difference among the MISSO
runs for different number of selected functions demonstrates a variance-cost tradeoff. Though wall
clock times are similar for all methods, they are reported in the appendix for completeness.
4.2	Training Bayesian CNN using MISSO
This application follows Example 2 described in Section 2. We use variational inference and the
ELBO loss (10) to fit Bayesian Neural Networks on different datasets. At iteration k, minimizing
the sum of stochastic surrogates defined as in (6) and (13) yields the following MISSO update —
step (i) pick a function index ik uniformly on JnK; step (ii) sample a Monte Carlo batch {zm(k)}mM=(k1)
from N(0, I); and SteP (iii) update the parameters, with W = t(θ(k-1),z(k)), as
k)
n
-Y X δ⑻
n 乙	μe,i
i=1
1	M(k)
δμk)ik = -而—X Nw logp(yiklxik,w) + ▽“'d(θ(k-1)),
`, k	M(k) m=1
where μ'τk) = n Pi=I μ'τi k and d(θ) = n-1 Pd=I (-log。) + (σ2 + μ2)/2 - 1/2).
Bayesian LeNet-5 on MNIST (LeCun et al., 1998): We apply the MISSO method to fit a Bayesian
variant of LeNet-5 (LeCun et al., 1998). We train this network on the MNIST dataset (LeCun,
1998). The training set is composed of n = 55 000 handwritten digits, 28 × 28 images. Each
image is labelled with its corresponding number (from zero to nine). Under the prior distribution
π, see (8), the weights are assumed independent and identically distributed according to N(0, 1).
We also assume that q(∙; θ) ≡ N(μ, σ2I). The variational posterior parameters are thus θ = (μ, σ)
where μ = (μ',' ∈ JdK) where d is the number of weights in the neural network. We use the
re-parametrization as w = t(θ, Z)= μ + σz with Z 〜N(0, I).
Bayesian ResNet-18 (He et al., 2016) on CIFAR-10 (Krizhevsky et al., 2012): We train here the
Bayesian variant of the ResNet-18 neural network introduced in (He et al., 2016) on CIFAR-10. The
latter dataset is composed of n = 60 000 handwritten digits, 32 × 32 colour images in 10 classes,
with 6 000 images per class. As in the previous example, the weights are assumed independent and
identically distributed according toN(0, I). Standard hyperparameters values found in the literature,
such as the annealing constant or the number ofMC samples, were used for the benchmark methods.
For efficiency purpose and lower variance, the Flipout estimator (Wen et al., 2018) is used.
8
Under review as a conference paper at ICLR 2021
Experiment Results: We compare the convergence of the Monte Carlo variants of the follow-
ing state of the art optimization algorithms — the ADAM (Kingma & Ba, 2015), the Momen-
tum (Sutskever et al., 2013) and the SAG (Schmidt et al., 2017) methods versus the Bayes by Back-
prop (BBB) (Blundell et al., 2015) and our proposed MISSO method. For all these methods, the
loss function (10) and its gradients were computed by Monte Carlo integration based on the re-
parametrization described above. The mini-batch of indices and MC samples are respectively set to
128 and M(k) = k. The learning rates are set to 10-3 for LeNet-5 and 10-4 for Resnet-18.
Figure 2(a) shows the convergence of the negated evidence lower bound against the number of passes
over data (one pass represents an epoch). As observed, the proposed MISSO method outperforms
Bayes by Backprop and Momentum, while similar convergence rates are observed with the MISSO,
ADAM and SAG methods for our experiment on MNIST dataset using a Bayesian variant of LeNet-
5. On the other hand, the experiment conducted on CIFAR-10 (Figure 2(b)) using a much larger
network, i.e., a Bayesian variant of ResNet-18 showcases the need of a well-tuned adaptive methods
to reach lower training loss (and also faster). Our MISSO method is similar to the Monte Carlo
variant of ADAM but slower than Adagrad optimizer. Recall that the purpose of this paper is to
provide a common class of optimizers, such as VI, in order to study their convergence behaviors,
and not to introduce a novel method outperforming the baselines methods. We report wall clock
times for all methods in the appendix for completeness.
5 Conclusion
We present a unifying framework for minimizing a nonconvex and nonsmooth finite-sum objective
function using incremental surrogates when the latter functions are expressed as an expectation and
are intractable. Our approach covers a large class of nonconvex applications in machine learning
such as logistic regression with missing values and variational inference. We provide both finite-
time and asymptotic guarantees of our incremental stochastic surrogate optimization technique and
illustrate our findings training a binary logistic regression with missing covariates to predict hemor-
rhagic shock and Bayesian variants of two Convolutional Neural Networks on benchmark datasets.
9
Under review as a conference paper at ICLR 2021
References
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statis-
ticians. Journal of the American Statistical Association, 112(518):859-877, 2017. doi: 10.
1080/01621459.2017.1285773. URL https://doi.org/10.1080/01621459.2017.
1285773.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622, 2015.
Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochastic approximation
version of the em algorithm. Ann. Statist., 27(1):94-128, 03 1999. doi: 10.1214/aos/1018031103.
URL https://doi.org/10.1214/aos/1018031103.
Roger Fletcher, Nicholas IM Gould, SVen Leyffer, PhiliPPe L Toint, and Andreas Wachter. Global
convergence ofa trust-region sqp-filter algorithm for general nonlinear programming. SIAM Jour-
nal on Optimization, 13(3):635-659, 2002.
Z Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452-
459, May 2015. doi: 10.1038/nature14541. URL https://www.ncbi.nlm.nih.gov/
pubmed/26017444/. On Probabilistic models.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, PP.
770-778, 2016.
Wei Jiang, Julie Josse, and Marc LaVielle. Logistic regression with missing coVariates-Parameter
estimation, model selection and Prediction. 2018.
Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction
to Variational methods for graPhical models. Mach. Learn., 37(2):183-233, NoVember 1999.
ISSN 0885-6125. doi: 10.1023/A:1007665907178. URL https://doi.org/10.1023/A:
1007665907178.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In 3rd Inter-
national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.
Diederik P. Kingma and Max Welling. Auto-encoding Variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6114.
Alex KrizheVsky, Ilya SutskeVer, and Geoffrey E Hinton. Imagenet classification with deeP conVo-
lutional neural networks. In Advances in neural information processing systems, PP. 1097-1105,
2012.
Kenneth Lange. MM Optimization Algorithms. SIAM-Society for Industrial and APPlied Mathe-
matics, USA, 2016. ISBN 1611974399, 9781611974393.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied
to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yingzhen Li and Yarin Gal. Dropout inference in bayesian neural networks with alpha-diVergences.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2052-
2061. JMLR. org, 2017.
Julien Mairal. Incremental majorization-minimization optimization with application to large-scale
machine learning. SIAM J. Optim., 25(2):829-855, 2015. ISSN 1052-6234. doi: 10.1137/
140957639. URL https://doi.org/10.1137/140957639.
10
Under review as a conference paper at ICLR 2021
Geoffrey J. McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions. Wi-
ley Series in Probability and Statistics. Wiley-Interscience [John Wiley & Sons], Hoboken,
NJ, second edition, 2008. ISBN 978-0-471-20170-0. doi: 10.1002/9780470191613. URL
https://doi.org/10.1002/9780470191613.
Arthur Mensch, Julien Mairal, Bertrand Thirion, and Gael Varoquaux. Stochastic subsampling for
factorizing huge matrices. IEEE Transactions on Signal Processing, 66(1):113-128, 2017.
Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Springer Science &
Business Media, 2012.
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
J.W. Paisley, D.M. Blei, and M.I. Jordan. Variational bayesian inference with stochastic search. In
ICML. icml.cc / Omnipress, 2012.
Nicholas G Polson, Vadim Sokolov, et al. Deep learning: a bayesian perspective. Bayesian Analysis,
12(4):1275-1304, 2017.
XUn Qian, Alibek Sailanbayev, Konstantin Mishchenko, and Peter Richtarik. Miso is making a
comeback with better proofs and rates. arXiv preprint arXiv:1906.01474, 2019.
Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A unified convergence analysis of block
successive minimization methods for nonsmooth optimization. SIAM Journal on Optimization,
23(2):1126-1153, 2013.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and ap-
proximate inference in deep generative models. In International Conference on Machine Learn-
ing, pp. 1278-1286, 2014.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. In International conference on machine learning, pp.
1139-1147, 2013.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
Greg C. G. Wei and Martin A. Tanner. A monte carlo implementation of the em algorithm
and the poor man’s data augmentation algorithms. Journal of the American Statistical As-
sociation, 85(411):699-704, 1990. doi: 10.1080/01621459.1990.10474930. URL https:
//www.tandfonline.com/doi/abs/10.1080/01621459.1990.10474930.
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient pseudo-
independent weight perturbations on mini-batches. arXiv preprint arXiv:1803.04386, 2018.
11
Under review as a conference paper at ICLR 2021
A Proofs of the Theoretical Results
A.1 Proof of Theorem 1
Theorem. Under H1-H4. For any Kmax ∈ N, let K be an independent discrete r.v. drawn uniformly
from {0, ..., Kmax - 1} and define the following quantity:
△ (Kmax) := 2nLE[L⑼(θ(O)) - L(Kmax) (θ(Kmax))] +4LCrM(k) .
Then we have following non-asymptotic bounds:
E[kVb<K)(θ(K))k2] ≤ △Kmx) and E[g-(θ(K))] ≤ ʌ^K^ + Cgr^M(k).
Km
ax	Kmax	Kmax
Proof We begin by recalling the definition
1n
Le(k)(θ) ：= 1X Ak (θ).
n i=1
Notice that
L(k+1)(θ) = 1 X Li(θ; θ(τk+1), {z(τm+1 )}M(τk+1))
i=1
=Le(k)(θ) +1 (Lik(θ;θ(k),{z(k,)m}M彗)-Leik(θ;θ(Tkk),{ziτkm 1}M⅛))).
Furthermore, we recall that
Lb(k)(θ) ：= 1 Pn=ILi(θ; θ(Tk)),	b(k)(θ) ：= Lb(k)(θ) - L(θ).
Due to H2, we have
kVeb(k)(θ(k))k2 ≤ 2Leb(k)(θ(k)) .
To prove the first bound in (16), using the optimality of θ(k+1), one has
Le(k+1) (θ(k+1)) ≤ Le(k+1) (θ(k))
=Lek)"+n (Lik (θ(k); θ(k),思*}鹑)-Leik (θ(k); θ(Tikk), 3(Tim)}M⅛))).
Let Fk be the filtration of random variables UP to iteration k, i.e., {i'-ι, {，('-：，}M=[I), θ(')}k=ι∙
We observe that the conditional expectation evaluates to
EikELeik(θ(k); θ(k), {zi(kk,)m}mM=(k1))|Fk, ik]|Fk]
M(k)
=L(θ(k)) + Eik [E[M- X rik(θ(k); θ(k),z(*) - Lik(θ(k); θ(k))∣Fk,i] |Fk]
M(k) m=1
(18)
(19)
where the last ineqUality is dUe to H4. Moreover,
E[Leik(θ(k); θ(Tkk),{Zi(Tim 1}M⅛))∣Fk] = 1 XLei(θ(k); θ(τk), {z(Tm)}M=k)) = Le(k)(θ(k)).
n i=1
Taking the conditional exPectations on both sides of (19) and re-arranging terms give:
Le(k)(θ(k)) - L(θ(k)) ≤ nE[Le(k)(θ(k)) - Le(k+1)(θ(k+1))∣Fk] +	(C
(20)
12
Under review as a conference paper at ICLR 2021
Proceeding from (20), we observe the following lower bound for the left hand side
Le(k)(θ(k)) - L(θ(k)) (=a) Le(k)(θ(k)) - Lb(k)(θ(k)) + eb(k) (θ(k))
≥ L(k)(θ(k)) - L(k)(θ(k)) + 1-∣∣vb(k)(θ(k))k2
2L
n	M(τik )
=nXnM- X ri(θ(k);θ(τik),z(Tm)) -Li(θ(k);θ(τk))} +2LkvbW(")k2,
i=1	(τi ) m=1
'-------------------------------------------------------------}
{^^^^^^
=-δ(k)(θ(k))
where (a) is due to eb(k)(θ(k)) = 0 [cf. H1], (b) is due to (18) and we have defined the summation in
the last equality as -δ(k)(θ(k)). Substituting the above into (20) yields
kvWk))k2 ≤ nE[L(k)(e(k))- Le(k+1)(θ(k+1))∣Fk] + pC( + δ(k)(θ(k)) .	(21)
Observe the following upper bound on the total expectations:
Eδ(k)(θ(k)) ≤ E
which is due to H4. It yields
E[∣vbw(θ(k))k2] ≤ 2nLE[Le(k)(θ(k)) - Le(k+1)(θ(k+1))] + PC=
1n
+ 1 X E
n i=1
Finally, for any Kmax ∈ N, we let K be a discrete r.v. that is uniformly drawn from {0, 1, ..., Kmax -
1}. Using H4 and taking total expectations lead to
Kmax -1
E[∣Vb(K)(θ(K))k2] = — — X E[∣vb(k)(θ(k))k2]
Kmax
k=0
2nLE[Le(0) (θ(0)) - Le(Kmax) (θ(Kmax))]
Kmax
Kmax -1
+ KCr X Eh
max
k=0
1 n 1
+nX qM
(22)
≤
1

For all i ∈ J1,nK, the index i is selected with a probability equal to § When conditioned indepen-
dently on the past. We observe:
k
、X n
j-1
M(-k1-/j2)
(23)
Taking the sum yields:
Kmax-1	Kmax-1 k
X EM〉21= X X n
k=O
Kmax-1
X	M(-l)1/2
l=O
k=0 j=1
j-1	Kmax-1 k-1
My 2= X X1
(k-j)	n
k=O l=O
k-(l+1)	Kmax -1
≤ X	M(-l)1/2 ,
l=O
1
n
Kmax-1
X
k=l+1
k-(l+1)
M(-l)1/2
(24)
where the last inequality is due to upper bounding the geometric series. Plugging this back into (22)
yields
Kmax-1
E[kvb(K)(θ(K))k2] = — — X E[kvb(k)(θ(k))k2]
Kmax	k=O
< 2nLE[L(O) (θ(O) ) -L(Kmax) (θ(Kmax))]
Kmax
Kmax -1
+ Kax X
4LCr	∆(Kmax)
13
Under review as a conference paper at ICLR 2021
This concludes our proof for the first inequality in (16).
To prove the second inequality of (16), we define the shorthand notations g(k) := g(θ(k)), g-(k) :
- min{0, g(k)}, g+(k) := max{0, g(k)}. We observe that
g(k)
,L0(θ(k), θ - θ(k))
θf ―∣∣θ(k) - θk—
n 1 pn=ι LW 0 - a、)； θ(c
θfl	Mk)- θ∣
—
(ye(k)(θ(k)) | θ — θ(k))o
∣∣θ(k) - θ∣ J
≥-∣∣vMk)(θ(k))∣∣ +θnθ
1 Pn=I Li (θ(k) θ 二竺;θ(Tk))
I∣θ(k) — θ∣
where the last inequality is due to the Cauchy-Schwarz inequality and we have defined
Li(θ, d; θ(Tik)) as the directional derivative of Li(∙; θ(Tk)) at θ along the direction d. Moreover,
for any θ ∈ Θ,
n
1 X Li(θ(k), θ - θ(k); θ(Tik ))
n i=1
n
Lek) (θ(k), θ - θ(k)) -Lk) (θ(k), θ - θ(k)) +1XLi(θ(k), θ - θ(k); θ(Ti))
S}	n i=ι
≥0	i=1
n	M(τik )
≥ n XnLi(θ(k),θ - θ(k);e®))- M1τ X ri(θ(k), θ - a； θ(*),z(,m))},
i=1	(Tik ) m=1
where the inequality is due to the optimality of θ(k) and the convexity of Le(k)(θ) [cf. H3]. Denoting
a scaled version of the above term as:
(k) (θ) :
I pn=ι nm⅛ Pm=I ri(θ(k), θ - θ(k); θ(Ti), z(a - Li-), θ -m；。(Tk))o
"	∣∣θ(k) - θ∣
We have
g(k) ≥ -∣Vbb(k)(θ(k))k + inf(-e(k)(θ)) ≥ -kv.k)(θ(k))∣∣- sup ∣e(k)(θ)∣ .	(25)
θ∈Θ	θ∈Θ
Since g(k) = g+(k) - g-(k) and g+(k)g-(k) = 0, this implies
g- ≤ ∣∣Vbb(k)(θ(k))k + sup ∣e(k)(θ)∣ .	(26)
θ∈Θ
Consider the above inequality when k = K, i.e., the random index, and taking total expectations on
both sides gives
E[g-(K)] ≤ E[∣veb(K)(θ(K))∣] + E[sup e(K) (θ)] .
θ∈Θ
We note that
同~/一—))||])2 ≤ EUIV.K)..))/] ≤ A(Kmax) ,
where the first inequality is due to the convexity of (∙)2 and the Jensen's inequality, and
1 Kmax	(a) C
E[sup e(K)(θ)] = —— V E[suP e(k)(θ)] ≤ ^g^
θ∈Θ	Kmax k=0 θ∈Θ	Kmax
Kmax -1	n
X Eh 1 XMIi
k=0
i=1
(≤) & KX-IMT/2
≤ KA M(k)	,
max
k=0
where (a) is due to H4 and (b) is due to (24). This implies
E[g(K)] ≤ ʌ^K^
Kmax
Kmax -1
+总X
M(-k1)/2
and concludes the proof of the theorem.
□
14
Under review as a conference paper at ICLR 2021
A.2 Proof of Theorem 2
Theorem. Under H1-H4. In addition, assume that {M(k) }k≥0 is a non-decreasing sequence of
integers which satisfies Pk∞=0 M(-k1) /2 < ∞. Then:
1.	the negative part of the stationarity measure converges a.s. to zero, i.e., lim g- (θ(k)) a=.s. 0.
k→∞
2.	the objective value L(θ(k)) converges a.s. to a finite number L i.e., limk→∞ L(θ(J)) Q= L.
Proof We apply the following auxiliary lemma which proof can be found in Appendix A.3 for the
readability of the current proof:
Lemma 1. Let (Vk)k≥0 be a non negative sequence of random variables such that E[V0] < ∞.
Let (Xk)k≥0 a non negative sequence of random variables and (Ek)k≥0 be a sequence of random
variables such that k∞=0 E[|Ek |] < ∞. If for any k ≥ 1:
Vk ≤ Vk-1 - Xk-1 + Ek-1	(27)
then:
(i)	for all k ≥ 0, E[Vk] < ∞ and the sequence (Vk)k≥0 converges a.s. to a finite limit V∞.
(ii)	the sequence (E[Vk])k≥0 converges and lim E[Vk] = E[V∞].
k→∞
(iii)	the series	k∞=0 Xk converges almost surely and	k∞=0 E[Xk] < ∞.
We proceed from (19) by re-arranging terms and observing that
Lb(k+ι)(θ(k+1)) ≤ Lbk)(θ(J))-1(Lik(θ(k);θ(τik))-Lik(θ(k); θ(J)))
-(L(J+1) (θbk+1))- L(k+1)(e(k+1))) + (L(k)(θ(k)) - L(J) (θ(k)))
+ n (Lik (Θ(j); θ(J),"(*}-)) -Lik (Θ(j); θ(J)))
+n (Lik (θ(J) ； θ(Tikk)) -Lik (θ( j) ； θ(Tikk), “i(Tim)) }M⅛) )).
Our idea is to apply Lemma 1. Under H1, the finite sum of surrogate functions Lb(J) (θ), defined in
(15), is lower bounded by a constant cJ > -∞ for any θ. To this end, we observe that
VJ := Lb(J)(θ(J)) - inf cJ ≥ 0	(28)
is a non-negative random variable.
Secondly, under H1, the following random variable is non-negative
Xj ：= 1 (Lik(θ(τik)； θ(J))-Lik(θ(k); θ(J))) ≥ 0 .	(29)
Thirdly, we define
EJ = -(L(J+1)(θ(k+1))- L(J+1)(θ(j+1))) + (L(J)(θ(J))- L(J)(θ(J)))
+ n (Lik (θ(k); θ(J),{Zi(Jm}Mk) ) -Lik (θ(k); θ(J)))	(30)
+ n(Lik(θ(k);θ(Tikk))-Lik(θ(k);θ(Tikk),{Zi(Tim))}M⅛)).
Note that from the definitions (28), (29), (30), we have VJ+1 ≤ VJ - XJ + EJ for any k ≥ 1.
Under H4, we observe that
E[∣Lik(θ(k); θ(J),{Zi(Jm}Mk)) -Lik(θ(k); θ(J))I] ≤ CrM-JI/2
EhILik(θ(k); θ(Tikk))-Lik(θ(k); θ(Tikk), {z(Tim)}M⅛)∣] ≤。同〃(-1)2]
15
Under review as a conference paper at ICLR 2021
E[∣L(k)(θ(k)) -L(k)(θ(k))∣] ≤ 1 Pn=ICrE[m-!)2]
Therefore,
E[∣Ek∣] ≤ C (m(-)/2+EhM-k/2+PUMM2+M(-1+2)}]).
Using (24) and the assumption on the sequence {M(k)}k≥0, we obtain that
∞∞
X E[IEk I] <C (2 + 2")XM(k)/2 < ∞.
k=0	k=0
Therefore, the conclusions in Lemma 1 hold. Precisely, we have Pk∞=0 Xk < ∞ and
Pk∞=0 E[Xk] < ∞ almost surely. Note that this implies
∞∞
∞ > X E[Xk] = -X E [Lik (θ(k); θ(Tk=)) -Lik (θ(k); θ(k))]
k=0	n k=0
∞∞
=—XE[L(k)(e(k))-L(θ(k))] = — XEFk)(e(k))].
n k=0	n k=0
Since eb(k)(θ(k)) ≥ 0, the above implies
lim eb(k) (θ(k)) = 0 a.s.
k→∞
(31)
and subsequently applying (18), we have limk→∞ keb(k)(θ(k))k = 0 almost surely. Finally, it follows
from (18) and (26) that
lim g(k) ≤ lim √2Lι∕e(k)(θ(k)) + lim sup ∣e(k)(θ)∣ = 0 ,
k→∞	k→∞	k→∞ θ∈Θ
(32)
where the last equality holds almost surely due to the fact that P∞=°E[supθ∈θ ∣e(k)(θ)∣] < ∞.
This concludes the asymptotic convergence of the MISSO method.
Finally, we prove that L(θ(k)) converges almost surely. As a consequence of Lemma 1, itis clear that
{Vk}k≥o converges almost surely and so is {L(k)(θ(k))}k≥o, i.e., we have limk→∞ Lb(k)(θ(k)) = L.
Applying (31) implies that
L = lim Lb(k)(θ(k)) = lim L(θ(k)) a.s.
k→∞	k→∞
This shows that L(θ(k)) converges almost surely to L	□
A.3 Proof of Lemma 1
Lemma. Let (Vk)k≥0 be a non negative sequence of random variables such that E[V0] < ∞.
Let (Xk)k≥0 a non negative sequence of random variables and (Ek)k≥0 be a sequence of random
variables such that k∞=0 E[|Ek |] < ∞. If for any k ≥ -:
Vk ≤ Vk-1 - Xk-1 + Ek-1
then:
(i)	for all k ≥ 0, E[Vk] < ∞ and the sequence (Vk)k≥0 converges a.s. to a finite limit V∞ .
(ii)	the sequence (E[Vk])k≥0 converges and lim E[Vk] = E[V∞ ].
k→∞
(iii)	the series	k∞=0 Xk converges almost surely and	k∞=0 E[Xk] < ∞.
16
Under review as a conference paper at ICLR 2021
Proof We first show that for all k ≥ 0, E[Vk] < ∞. Note indeed that:
kk	k
0≤ Vk ≤ V0-XXj+XEj ≤ V0+XEj ,	(33)
showing that E[Vk] ≤ E[V0] + E Pjk=1 Ej < ∞.
Since 0 ≤ Xk ≤ Vk-1 - Vk + Ek we also obtain for all k ≥ 0, E[Xk] < ∞. Moreover, since
E Pj∞=1 |Ej|
< ∞, the series Pj∞=1 Ej converges a.s. We may therefore define:
∞
Wk = Vk + X Ej
j=k+1
Note that E[|Wk|] ≤ E[Vk] + E Pj∞=k+1 |Ej| < ∞. For all k ≥ 1, we get:
∞
Wk ≤ Vk-1 -Xk+XEj ≤ Wk-1 -Xk ≤ Wk-1
j=k
(34)
(35)
E[Wk] ≤ E[Wk-1] -E[Xk] .
Hence the sequences (Wk)k≥0 and (E[Wk])k≥0 are non increasing. Since for all k ≥ 0, Wk ≥
- Pj∞=1 |Ej| > -∞ and E[Wk] ≥ - Pj∞=1 E[|Ej |] > -∞, the (random) sequence (Wk)k≥0
converges a.s. to a limit W∞ and the (deterministic) sequence (E[Wk])k≥0 converges to a limit w∞.
Since |Wk| ≤ V0 + Pj∞=1 |Ej |, the Fatou lemma implies that:
∞
E[liminf | Wk|] = E[∣W∞∣] ≤ liminf E[| Wk |] ≤ E[V0] + X E[∣Ej|] < ∞ ,	(36)
k→∞	k→∞
j=1
showing that the random variable W∞ is integrable.
In the sequel, set Uk , W0 - Wk . By construction we have for all k ≥ 0, Uk ≥ 0, Uk ≤ Uk+1 and
E[Uk] ≤ E[|W0|] + E[|Wk |] < ∞ and by the monotone convergence theorem, we get:
lim E[Uk] = E[ lim Uk] .	(37)
k→∞	k→∞
Finally, we have:
lim E[Uk] = E[W0] -w∞ and E[ lim Uk] = E[W0] -E[W∞] .	(38)
k→∞	k→∞
showing that E[W∞] = w∞ and concluding the proof of (ii). Moreover, using (35) we have that
Wk ≤ Wk-1 - Xk which yields:
∞
X Xj ≤ W0 - W∞ < ∞ ,
j=1
∞
X E[Xj ] ≤ E[W0] - w∞ < ∞ ,
j=1
an concludes the proof of the lemma.
(39)
□
B	Practical Details for the B inary Logistic Regression on the
Traumabase
B.1	Traumabase dataset quantitative variables
The list of the 16 quantitative variables we use in our experiments are as follows — age, weight,
height, BMI (Body Mass Index), the Glasgow Coma Scale, the Glasgow Coma Scale motor com-
ponent, the minimum systolic blood pressure, the minimum diastolic blood pressure, the maximum
17
Under review as a conference paper at ICLR 2021
number of heart rate (or pulse) per unit time (usually a minute), the systolic blood pressure at ar-
rival of ambulance, the diastolic blood pressure at arrival of ambulance, the heart rate at arrival
of ambulance, the capillary Hemoglobin concentration, the oxygen saturation, the fluid expansion
colloids, the fluid expansion cristalloids, the pulse pressure for the minimum value of diastolic and
systolic blood pressure, the pulse pressure at arrival of ambulance.
B.2	Metropolis-Hastings algorithm
During the simulation step of the MISSO method, the sampling from the target distribution
π(zi,mis; θ) := p(zi,mis|zi,obs, yi; θ) is performed using a Metropolis-Hastings (MH) algo-
rithm (Meyn & Tweedie, 2012) with proposal distribution q(zi,mis; δ) := p(zi,mis|zi,obs; δ) where
θ = (β, Ω) and δ = (ξ, Σ). The parameters of the Gaussian conditional distribution of Zi,mis∣Zi,obs
read:
ξ
Bmiss + θmis,obsθobs,obs (Zi,obs
- βobs ) ,
Ωmis,mis
+ θmis,obs
。Obs,obsQobs,mis
where We have used the Schur Complement of Ωobs,obs in Ω and noted Bmis (resp. Bobs) the missing
(resp. observed) elements of β. The MH algorithm is summarized in Algorithm 3.
Algorithm 3 MH aglorithm
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
Input： initialization Zi,mis,o 〜q(zi,mis； δ)
for m = 1,…，M do
SamPle zi,mis,m 〜q(zi,mis; δ)
Sample U 〜U(J0,11)
Calculate the ratio r = ，n(Zi,mis,m；，)/q(Zi,mis,m);之、
π(zi
,mis,m-1 ；e)/qi(zi,mis,m -1 )2)
if u < r then
else
Accept Zi
,mis,m
zi,mis,m《―zi,mis,m-1
end if
end for
Output: Zi,mis,M
B.3 MISSO UPDATE
Choice of surrogate function for MISO: We recall the MISO deterministic surrogate defined in
(7):
Lbi (θ; θ) = / log (Pi (Zi,mis, θ)∕fi(Zi,mis, θ)) Pi(Zi,mis, θ)μi (dzi ) .
where θ = (δ, B, Ω) and θ = (δ, B, Ω). We adapt it to our missing covariates problem and decom-
pose the surrogate function defined above into an observed and a missing part.
Surrogate function decomposition We adapt it to our missing covariates problem and decompose
the term depending on θ, while G is fixed, in two following parts leading to
^ .一.
Li(θ; θ)
=-J log fi (Zi,mis, zi,obs, θ)Pi(Zi,mis, θ)μi(dzi,mis)
=J log[pi (yi |Zi,mis,Zi,obs, δ)pi (Zi,mis, B,。)] Pi(Zi, θ)μi (dZi,mis)
=-J logpi(yi|Zi,mis, Zi,obs, δ)pi(Zi, θ)μi(dZi,mis) ― J logpi(Zi,mis, B,。)pi(Zi, θ)μi(dZi,mis) .
L(I) (δ,θ)
}|
{^^^^^≡
=L(2)(β,Ω,θ)
Σ
|
}
(40)
18
Under review as a conference paper at ICLR 2021
The mean β and the covariance Ω of the latent structure can be estimated minimizing the sum of
MISSO surrogates L(2)(β, Ω, θ, {zm}M=ι), defined as MC approximation of L(2)(β, Ω, θ), for all
i ∈ JnK, in closed-form expression.
We thus keep the surrogate L(2) (β, Ω, θ) as it is, and consider the following quadratic approximation
of L(I) (δ, θ) to estimate the vector of logistic parameters δ:
L(I) G©-卜 log Pi(yi∣Zi,mis, zi,obs, δ)∣δ=5 Pi(Zi,mis,θ)μi(dZi,mis)(δ - S)
―(δ - δ)∕2 J ▽? logPi (yi - ,mis, zi,obs, δ)Pi (Zi,mis, θ)Pi (Zi,mis, θ)μi (dzi,mis)(δ - δ), ∙
Recall that:
V logpi(yi |zi,mis, zi,obs, δ) = zi (yi - S(δ Zi)),
V logpi (yi |zi,mis, zi,obs, δ) = -ZiZi S(δ zi) ,
1	A /	∖ ∙	,1	1	C Cf ∖	-IL ɪ ,	.∖	.	A /	∖	，	1	1	f 11 ∙	_	∏^	^∏	,1
where S(u) is the derivative of S(u).	Note that	S(u) ≤ 1∕4	and since, for all i ∈	JnK,	the P	× P
matrix Zi Zi> is semi-definite positive we can assume that:
L1. For all i ∈ JnK and > 0, there exist, for all Zi ∈ Z, a positive definite matrix Hi(Zi) :=
1 (ZiZ> + eId) such thatfor all δ ∈ Rp, -Ziz>S(δ>Zi) ≤ Hi(Zi).
Then, we use, for all i ∈ JnK, the following surrogate function to estimate δ:
Lf)(δ, θ) = L(I)伍, θ) - D>(δ - s) + 2(δ - δ)Hi(δ - δ)> ,
where:
Di = J V log pi(yi |Zi,mis, Zi,obs, δ) I δ=5 Pi(Zi,mis, θ)μi (dZi,mis) ,
Hi =	Hi (Zi,mis) Pi (Zi,mis, θ)μi(dZi,mis).
Finally, at iteration k, the total surrogate is:
nM
*⑻=；X Li® (Tik), {Zi,m}m(Tk ) )
n i=1
nn
=n X L(2)(β,Ω, θd, {Zi,m}m=f) ) - 1 X D(Tik)(δ - δ(τk))
n i=1	n i=1
n
+ 2n X(δ- δ(τk )) {H(τi)0 (δ - δ(τk))> ,
i=1
(41)
(42)
where for all i ∈ JnK:
Di(Tk)=	M(τik) , M^ XlZi(Tm) (yi - S(G(Ti))%m(Tik))
H(Tik) =	M(τik) =	1	X Z(Tk )(Z(* ))> 4M(Tk) ML i,m ( i,m ) .
Minimizing the total surrogate (42) boils down to performing a quasi-Newton step. It is perhaps sen-
sible to apply some diagonal loading which is perfectly compatible with the surrogate interpretation
we just gave.
The logistic parameters are estimated as follows:
n
δ(k) = argmin- X Li1)(δ,θ(Ti"),{Zi,m}m=f ) ,
δ∈Θ n
i=1
19
Under review as a conference paper at ICLR 2021
where L(I) (δ, θ(τk), [zi,mjm=^!^k)) is the MC approximation of the MISO surrogate defined in (41)
and which leads to the following quasi-Newton step:
δ(k)
n
—Xδ(τk) -(H(k))-1DD⑹,
n i=1
With DD(k) = n Pn=I DD(Tik) and HM = n Pn=I H(Tk
MISSO updates: At the k-th iteration, and after the initialization, for all i ∈ JnK, of the latent
variables (zi(0) ), the MISSO algorithm consists in picking an index ik uniformly on JnK, complet-
ing the observations by sampling a Monte Carlo batch {zi(k,)mis,m }mM=(k1) of missing values from the
conditional distribution p(zik,mis|zik,obs, yik ; θ(k-1)) using an MCMC sampler and computing the
estimated parameters as follows:
n	M	n	M(τik )
β(k) = argmin1 X L⑵(β。⑹ 0(*){z- } (Tk)) = 1 X 1 X Z㈤
β 一argmin n 乙”(β ω ,θ	, {zi,m }m=1 ) = n M 乙 zz,m
i=1	i=1	(Tik ) m=1
n	M	n	M(τik )
Ω(k) =aromin1 XLQ)(8缶)Ω θ^τi){z- } (Tk)) = 1 X 1 X w(k)
ω =arg mɪn n 乙 Li (β , ω,θ , {zi,m}m=1)= nl^M 乙叫,m
i=1	i=1	(τik ) m=1
(43)
n
δ(k) = 1 Xδ(τk) - (Hr(k))-1b(k).
n i=1
where Zikm = (z(3s m,z*obs) is composed of a simulated and an observed part, b(k) =
n pn=ι b(Tk), H(k)	= n pn=ι Hi(Tk) and w(,m = Zikm(z(,m)> - e(k)(e(k))>. Be-
sides, L(I)(β, Ω, θ, {zm}M=I) and Li2)(β, Ω, θ, {zm}M=ι) are defined as MC approximation of
L(I) (β, Ω, θ) and L(2) (β, Ω, θ), for all i ∈ JnK as components of the surrogate function (40).
B.4 Wall clock time
We provide Table 1, the running time for each method, plotted in Figure 1, employed to train a
logistic regression with missing values on the TraumaBase dataset (p = 16 influential quantitative
measurements, on n = 6384 patients).
The running times are sensibly the same since for each method the computation complexity per
epoch is similar. We remark a slight delay using the MISSO method with a batch size of 1, as our
code implemented in R, is not totally optimized and parallelized. Yet, when the batch size tends to
100%, we retrieve the duration of MCEM, which is consistent with the fact that MISSO with a full
batch update boils down to the MCEM algorithm.
Table 1: Logistic Regression with missing values: running time in seconds for 10 epochs.
SAEM MCEM MISSO MISSO10 MISSO50
Logistic Regression 2033.2 1972.4 2244.8	2139.4	2005.2
We plot Figure 3, the updated parameters for the Logistic regression example against the time
elapsed (in seconds).
20
Under review as a conference paper at ICLR 2021
Figure 3: Convergence of parameters δ and β for the SAEM, the MCEM and the MISSO methods.
The convergence is plotted against time elapsed (in seconds).
C Practical Details for the Incremental Variational Inference
C.1 Neural Networks Architecture
Bayesian LeNet-5 Architecture: We describe in Table 2 the architecture of the Convolutional
Neural Network introduced in (LeCun et al., 1998) and trained on MNIST:
layer type	Width	stride	padding	input shape	nonlinearity
convolution (5 × 5)	6	1	0	1 × 32 ×32	ReLU
max-pooling (2 × 2)		2	0	6 × 28 × 28	
convolution (5 × 5)	6	1	0	1×14×14	ReLU
max-pooling (2 × 2)		2	0	16×10×10	
fully-connected	120			400	ReLU
fully-connected	84			120	ReLU
fully-connected	10			84	
Table 2: LeNet-5 architecture
Bayesian ResNet-18 Architecture: We describe in Table 3 the architecture of the Resnet-18 we
train on CIFAR-10:
layer type	Output Size	ReSNet-18	nonlinearity
conv1	112× 112× 64	7 × 7, 64, stride 2	ReLU
conv2x	56 × 56 × 64	3×3,64 ×2 3 × 3, 64 × 2	ReLU
conv3x	28 × 28 × 128	3×3,128	2 3 × 3, 128 × 2	ReLU
conv4x	14 × 14 × 256	3×3,256	2 3 × 3, 256 × 2	ReLU
conv5x	7 × 7 × 512	3×3,512	2 3 × 3, 512 × 2	ReLU
average pool fully connected softmax	1 × 1 ×512 1000 1000	7 × 7 average pool 512 × 1000 fully connections	ReLU
Table 3: ResNet-18 architecture
C.2 Algorithms updates
First, We initialize the means μ'0) for ' ∈ JdK and variance estimates σ(0). At iteration k, minimizing
the sum of stochastic surrogates defined as in (6) and (13) yields the following MISSO update —
21
Under review as a conference paper at ICLR 2021
step (i) pick a function index ik uniformly on JnK; step (ii) sample a Monte Carlo batch {zm(k)}mM=(k1)
from N(0, I); and step (iii) update the parameters as
nn
μ'k) = nXμ'τk)-YXδμk,i	and σ(k)
i=1	i=1
nn
n X σ(τk)-YX 吃，
i=1	i=1
(44)
where we define the following gradient terms for all i ∈ J1, nK:

δ(k) =
σ,i
1	M(k)
-TT- X NU log p(yi|xi, W)I	II) (k 八 + ▽*' d(θ(k-1) ) ,
M(k) m=1	w=t(θ(k-1),zm(k))	`
1	M(k)	I
-T7X ZmNw logp(yi|xi, W)I	(k) +	σd(θ(kT)).
M(k) m=1 m	w=t(θ(k-1),zm(k))
(45)
Note that our analysis in the main text does require the parameter to be in a compact set. For the
current estimation problem considered, this can be enforced in practice by restricting the parameters
in a ball. In our simulation for the BNNs example, we did not implement the algorithms that stick
closely to the compactness requirement for illustrative purposes. However, we observe empirically
that the parameters are always bounded. The update rules can be easily modified to respect the
requirement. For the considered VI problem, we recall the surrogate functions (11) are quadratic
and indeed a simple projection step suffices to ensure boundedness of the iterates.
For all benchmark algorithms, we pick, at iteration k, a function index ik uniformly on JnK and
sample a Monte Carlo batch {zm(k)}mM=(k1) from the standard Gaussian distribution. The updates of the
parameters μ' for all' ∈ JdK and σ break down as follows:
Monte Carlo SAG update: Set
nn
μ'k) = μ'k-1) - Y X δμk,i and σ(k) = σ(k-1) - ； X 吃,
n i=1	n i=1
where = S)：-1) and 5，)) = 8,-1 for i = ik and are defined by (45) for i = ik. The learning
rate is set to γ = 0-3.
Bayes By Backprop update: Set
μ'k)=μ'kT- Yδ(k)ifc and σ(k) = σ(I)- Y8喘,
,`	,`	n μ2,ik	n σ,ik
where the learning rate Y = 10-3 .
Monte Carlo Momentum update: Set
μ'k)=μ'kτ)+ vμk) and σ(k) = σ(I) + v，k) ,
where
⅛≥=αvμk-1) - γ错［ and vσk)=^vσk-1 - γ睨,
where α and Y, respectively the momentum and the learning rates, are set to 0-3
Monte Carlo ADAM update: Set
μ'k) = μ'k-1) - Ymμk)∕(Jmμ + e) and σ(k) = σ(k-1) - Yrh，：)/(1加，k) + e),
where
mμk=m『/(i - pi) with m`=ριmμkτ)+a-ρι)磴1,
vμk)=vμι-l)∕(i -p：)with vμι)=p2vμe-1+(ɪ -pi)(8，ki)fc)2
and
mσk) = m(k-1)/(1 -Pk) with m，k) = Pimk-1) + (1 - pι)δσki)k ,
Va)= v，：-1)/(I-Pk) with VS)= P2v，k-1) + (1 - Pι)(δσki)J2 .
The hyperparameters are set as follows: Y = 0-3, P1 = 0.9, P2 = 0.999, e = 0-8.
22
Under review as a conference paper at ICLR 2021
C.3 Wall clock time
We provide Table 4, the running time for each method, plotted in Figure 2, used to train a Bayesian
variant of LeNet-5 on MNIST. The incremental method as MISSO and MC-SAG displays a similar
wall clock time, despite being a bit worse given (a) the initialization that requires to compute a vector
of n gradients kept in memory and updated through the iterations and (b) the average operation for
each parameters update to compute the aggregated drift term, see (44).
Table 4: Bayesian Deep Neural Network: running time in seconds for 100 epochs.
MC-Adam MC-Momentum BBB MC-SAG MISSO
LeNet-5 on MNIST
12889
12816
12690	13822
13367
We plot Figure 4, the learning curves for the MNIST example against the time elapsed (in seconds).
MC-Adam	-→- MC-SAG
BBB	—— MC-Momentum
(a) LeNet-5 on MNIST
Figure 4: Negated ELBO versus wall clock time for fitting a Bayesian LeNet-5 on MNIST. Plotted
on the average of the 5 repetitions.
0	2000	4000	6000	8000	10000 12000 14000
Wall clock time (in seconds)
23