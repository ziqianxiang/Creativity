Under review as a conference paper at ICLR 2021
Quantifying Statistical Significance of Neu-
ral Network Representation-Driven Hypothe-
ses by Selective Inference
Anonymous authors
Paper under double-blind review
Ab stract
In the past few years, various approaches have been developed to explain and in-
terpret deep neural network (DNN) representations, but it has been pointed out
that these representations are sometimes unstable and not reproducible. In this
paper, we interpret these representations as hypotheses driven by DNN (called
DNN-driven hypotheses) and propose a method to quantify the reliability of these
hypotheses in statistical hypothesis testing framework. To this end, we introduce
Selective Inference (SI) framework, which has received much attention in the past
few years as a new statistical inference framework for data-driven hypotheses.
The basic idea of SI is to make conditional inferences on the selected hypothe-
ses under the condition that they are selected. In order to use SI framework for
DNN representations, we develop a new SI algorithm based on homotopy method
which enables us to derive the exact (non-asymptotic) conditional sampling distri-
bution of the DNN-driven hypotheses. In this paper, we demonstrate the proposed
method in computer vision tasks as practical examples. We conduct experiments
on both synthetic and real-world datasets, through which we offer evidence that
our proposed method can successfully control the false positive rate, has decent
performance in terms of computational efficiency, and provides good results in
practical applications.
1	Introduction
The remarkable predictive performance of deep neural networks (DNNs) stems from their ability to
learn appropriate representations from data. In order to understand the decision-making process of
DNNs, it is thus important to be able to explain and interpret DNN representations. For example, in
image classification tasks, knowing the attention region from DNN representation allows us to un-
derstand the reason for the classification. In the past few years, several methods have been developed
to explain and interpret DNN representations (Ribeiro et al., 2016; Bach et al., 2015; Doshi-Velez &
Kim, 2017; Lundberg & Lee, 2017; Zhou et al., 2016; Selvaraju et al., 2017); however, some of them
have turned out to be unstable and not reproducible (Kindermans et al., 2017; Ghorbani et al., 2019;
Melis & Jaakkola, 2018; Zhang et al., 2020; Dombrowski et al., 2019; Heo et al., 2019). Therefore,
it is crucially important to develop a method to quantify the reliability of DNN representations.
In this paper, we interpret these representations as hypotheses that are driven by DNN (called DNN-
driven hypotheses) and employ statistical hypothesis testing framework to quantify the reliability of
DNN representations. For example, in an image classification task, the reliability of an attention
region can be quantified based on the statistical significance of the difference between the attention
region and the rest of the image. Unfortunately, however, traditional statistical test cannot be applied
to this problem because the hypothesis (attention region in the above example) itself is selected by
the data. Traditional statistical test is valid only when the hypothesis is non-random. Roughly
speaking, if a hypothesis is selected by the data, the hypothesis will over-fit to the data and the bias
needs to be corrected when assessing the reliability of the hypothesis.
Our main contribution in this paper is to introduce Selective Inference (SI) approach for testing the
reliability of DNN representations. The basic idea of SI is to perform statistical inference under
the condition that the hypothesis is selected. SI approach has been demonstrated to be effective
1
Under review as a conference paper at ICLR 2021
Input Image Attention Map Attention Region
Image without tumor region
(a) naive-p = 0.00 (false positive—wrong detection)
and selective-p = 0.94 (true negative)
Image with tumor region
(b) naive-p = 0.00 (true positive) and selective-p
0.02 (true positive)
Figure 1: Examples of the proposed method on brain tumor image classification. Given a CNN
trained to classify tumor versus non-tumor brain images in advance, our method provides the statis-
tical significance of the attention region for each test image in the form of p-values by comparing
the pixel information in the attention and non-attention regions. Since the attention region is se-
lected by the input image, the p-value obtained by the naive comparison of the two regions (naive
p-value) is highly biased. In the left-hand side figure where there is no brain tumor, the naive p-value
is nearly zero (indicating false positive—incorrectly identifying tumor region), while the proposed
selective p-value is large (indicating true negative). On the other hand, in the right-hand side figure
where there actually exist a brain tumor, both the naive p-value and the selective p-values are very
small (indicating true positive). The proposed selective inference method can provide valid exact
(non-asymptotic) p-values for DNN representations such as attentions.
in the context of feature selections such as Lasso. In this paper, in order to introduce SI for DNN
representations, we develop a novel SI algorithm based on homotopy method, which enables us to
derive the exact (non-asymptotic) conditional sampling distribution of the DNN-driven hypothesis.
We use p-value as a criterion to quantify the reliability of DNN representation. In the literature, p-
values are often misinterpreted and there are various source of mis-interpretation has been discussed
(Wasserstein & Lazar, 2016). In this paper, by using SI, we address one of the sources of mis-
interpreted p-values; the p-values are biased when the hypothesis is selected after looking at the data
(often called double-dipping or data dredging). We believe our approach is a first significant step
to provide valid p-values for assessing the reliability of DNN representations. Figure 1 shows an
example that illustrates the importance of our method.
Related works. Several recent approaches have been developed to visualize and understand a
trained DNN. Many of these post-hoc approaches (Mahendran & Vedaldi, 2015; Zeiler & Fergus,
2014; Dosovitskiy & Brox, 2016; Simonyan et al., 2013) have focused on developing visualization
tools for the activation maps and/or the filter weights within trained networks. Others have aimed
to identify the discriminative regions in an input image, given a trained network (Selvaraju et al.,
2017; Fong & Vedaldi, 2017; Zhou et al., 2016; Lundberg & Lee, 2017). In parallel, some recent
studies have showed that many popular methods for explanation and interpretation are not stable
with respect to the perturbation or the adversarial attack on the input data and the model (Kindermans
et al., 2017; Ghorbani et al., 2019; Melis & Jaakkola, 2018; Zhang et al., 2020; Dombrowski et al.,
2019; Heo et al., 2019). However, there are no previous studies that quantitatively evaluate the
stability and reproducibility of DNN representations with a rigorous statistical inference framework.
In the past few years, SI has been actively studied for inference on the features of linear models
selected by several feature selection methods, e.g., Lasso (Lee et al., 2016; Liu et al., 2018; Duy &
Takeuchi, 2020). The basic idea of SI is to make inference conditional on the selection event, which
allows us to derive the exact (non-asymptotic) sampling distribution of the test statistic. Besides,
SI has also been applied to various problems (Bachoc et al., 2014; Fithian et al., 2015; Choi et al.,
2017; Tian et al., 2018; Chen & Bien, 2019; Hyun et al., 2018; Bachoc et al., 2018; Loftus & Taylor,
2014; Loftus, 2015; Panigrahi et al., 2016; Tibshirani et al., 2016; Yang et al., 2016; Suzumura
et al., 2017; Duy et al., 2020). However, to the best of our knowledge, there is no existing study that
provides SI for DNNs, which is technically challenging. This study is partly motivated by Tanizaki
et al. (2020) where the authors provide a framework to compute p-values for image segmentation
results provided by graph cut and threshold-based segmentation algorithms. As we demonstrate in
this paper, our method can be also used to assess the reliability of DNN-based segmentation results.
2
Under review as a conference paper at ICLR 2021
Contribution. To our knowledge, this is the first study that provides an exact (non-asymptotic)
inference method for statistically quantifying the reliability of data-driven hypotheses that are dis-
covered from DNN representation. We propose a novel SI homotopy method, inspired by Duy &
Takeuchi (2020), for conducting powerful and efficient SI for DNN representations. We conduct
experiments on both synthetic and real-world datasets, through which we offer evidence that our
proposed method can successfully control the false positive rate, has decent performance in terms
of computational efficiency, and provides good results in practical applications. We provide our
implementation in the supplementary document and it will be released when this paper is published.
2	Problem Statement
To formulate the problem, we denote an image with n pixels corrupted with Gaussian noise as
X = (Xi,…，Xn)> = μ + ε, ε 〜N(0, Σ),	(1)
where μ ∈ Rn is an unknown mean pixel intensity vector and ε ∈ Rn is a vector of Normally
distributed noise with the covariance matrix Σ that is known or able to be estimated from external
data. We note that we do not assume that the pixel intensities in an image follow Normal distribution
in Equation (1). Instead, we only assume that the vector of noises added to the true pixel values
follows a multivariate Normal distribution. For an image X and a trained DNN, the main target is
to identify an attention region (discriminative/informative region) in the input image X based on
a DNN representation. A pixel is assigned to the attention region if its corresponding value in the
representation layer is greater than a pre-defined threshold. We denote the set of pixels of X divided
into attention region and non-attention region as CX+ and CX- , respectively.
Definition 1. We define A(X)as the event that the result of dividing pixels of image X into two
sets of pixels CX+ and CX- is obtained by applying a DNN on X, i.e.,
A(X)= {CX+, CX-}.	(2)
Quantifying the statistical significance of DNN-driven hypotheses. Given an observed image
xobs ∈ Rn sampled from the model (1), we can obtain Cx+obs and Cx-obs by applying DNN on xobs.
Let us consider a score ∆ that represents the degree to which the attention region differs from
the non-attention region. In general, we can define any score as long as it is written in the form
∆ = η>xobs. For example, we can define ∆ as the difference in average pixel values between the
attention region and the non-attention region, i.e.,
∆ = mC+	- mC-
xobs	xobs
1 X xobs
∣C+obs । i∈C+	i
i∈Cxobs
|Cx-obs |
xiobs = η> xobs
i∈Cx-obs
where η
set C are 1, and 0 otherwise.
—
, and 1Cn ∈ Rn is a vector whose elements belonging to a
1
—
If the value of ∣∆∣ is sufficiently large, the difference between C^obs and C-Obs is significant and
the attention region is reliable. To quantify the statistical significance, we consider a statistical
hypothesis testing with the following null hypothesis H0 and alternative hypothesis H1:
H。： μc+	μc-	vs. Hi ： μc+	= μc- ,	(3)
xobs	xobs	xobs	xobs
where μc+	and μc-	are the true means of the pixel values in the attention region and non-
xobs	xobs
attention region, respectively. Given a significance level α (e.g., 0.05), we reject H0 if the p-value is
smaller than α, which indicates the attention region differs from the non-attention region. Otherwise,
we cannot say that the difference is significant.
In a standard (naive) statistical test, the hypotheses in (3) are assumed to be fixed, i.e., non-random.
Then, the naive (two-sided) p-value is simply given as
Pnaive = P/ (∣η>x| ≥ ∆∣) = Ph0 (∣η>x| ≥ ∣ητχobsD .	(4)
However, since the hypotheses in (3) are actually not fixed in advance, the naive p-value is not valid
in the sense that, if we reject H0 with a significance level α, the false detection rate (type-I error)
cannot be controlled at level α, which indicates that pnaive is unreliable. This is due to the fact that
the hypotheses (the attention region) in (3) are selected by looking at the data (the input image), and
thus selection bias exists. This selection bias is sometimes called data dredging, data snooping or
p-hacking (Ioannidis, 2005; Head et al., 2015).
3
Under review as a conference paper at ICLR 2021
Selective inference (SI) for computing valid p-values. The basic idea of SI is to make inference
conditional on the selection event, which allows us to derive the exact (non-asymptotic) sampling
distribution of the test statistic η>X in an attempt to avoid the selection bias. Thus, we employ the
following conditional p-value
Pselective = Ph0 (∣η>x| ≥ ∣η>χobs∣∣ A(X) = A(Xobs), q(X) = q(χobs)),	(5)
where q(X) = (In - cη>)X with c = Ση(η>Ση)-1. The first condition A(X) = A(xobs)
indicates the event that the result of dividing pixels into an attention region and non-attention region
for a random image X is the same as that of the observed image xobs, i.e., CX+ = Cx+obs and
CX- = Cx-obs. The second condition q(X) = q(xobs) indicates the component that is independent of
the test statistic for X is the same as the one for xobs. The q(X) corresponds to the component z in
the seminal SI paper of Lee et al. (2016) (Sec 5, Eq 5.2 and Theorem 5.2). The p-value in (5), which
is called selective type I error or selective p-values in the SI literature (Fithian et al., 2014), is valid
in the sense that PH0 (pselective < α) = α, ∀α ∈ [0, 1], i.e., the false detection rate is theoretically
controlled at level α indicating the selective p-value is reliable.
To calculate the selective p-value in (5), we need to identify the conditional data space. Let us define
the set of x ∈ Rn that satisfies the conditions in (5) as
X =	{x	∈	Rn	| A(x)	=	A(xobs),	q(x)	= q(xobs)}.	(6)
According to the second condition, the data in X are restricted to a line (Sec 6 in Liu et al. (2018),
and Fithian et al. (2014)). Therefore, the set X can be re-written, using a scalar parameter z ∈ R, as
X = {x(z) = a + bz | z ∈ Z},	(7)
where a = q(xobs), b = ∑η'(η/∑ηQ-1, and
Z = {z ∈ R | A(X(Z)) = A(Xobs)} .	(8)
Now, let us consider a random variable Z ∈ R and its observation zobs ∈ R that satisfy X = a+bZ
and Xobs = a + bzobs . Then, the selective p-value in (5) is re-written as
Pselective = Pj (∣η>x∣ ≥ ∣η>χobs∣∣ X ∈X) = Pho (|Z| ≥ ∣zobs∣∣ Z ∈Z).	(9)
Since the variable Z 〜N(0, η>∑η) under the null hypothesis, the law of Z | Z ∈ Z follows a
truncated Normal distribution. Once the truncation region Z is identified, the selective P-value (9)
can be computed as
Pselective = F^> ∑η ( — |zobs|) + 1 - FZη>∑η (∣Zobs |),	(10)
where FmE,s2 is the c.d.f. of the truncated normal distribution with mean m, variance s2 and trunca-
tion region E . Therefore, the most important task is to identify Z.
Extension of the problem setup to hypothesis driven from DNN-based image segmentation.
We interpret the hypothesis driven from image segmentation result as the one obtained from the
representation at output layer instead of internal representation. Our problem setup is general and
can be directly applied to this case. For example, we can consider the attention region as the object
region and the non-attention region as the background region. Then, we can conduct SI to quantify
the significance of the difference between object and background regions. We note that we consider
the case where the image is segmented into two regions—object and background—to simplify the
problem and notations. The extension to more than two regions is straightforward.
3	Proposed Method
As we discussed in §2, to calculate the selective P-value, the truncation region Z in Equation (8)
must be identified. To construct Z, we have to 1) compute A(X(z)) for all z ∈ R, and 2) identify the
set of intervals ofz on which A(X(z)) = A(Xobs). However, it seems intractable to obtain A(X(z))
for infinitely many values of z ∈ R.
Our first idea to develop SI for DNN is that we additionally condition on some extra event to make
the problem tractable. We now focus on a class of DNNs whose activation functions (AFs) are
piecewise-linear, e.g., ReLU, Leaky ReLU (the extension to general AFs is discussed later). Then,
we consider additionally conditioning on the selected piece of each piecewise-linear AF in the DNN.
4
Under review as a conference paper at ICLR 2021
Figure 2: A schematic illustration of the proposed method. By applying DNN on the observed
image xobs, we obtain an representation. Then, we parametrize xobs with a scalar parameter z in
the dimension of test-statistic to identify the subspace X whose data has the same representation
as xobs has. Finally, the valid statistical inference is conducted conditional on X . We introduce a
homotopy method for efficiently characterizing the conditional data space X .
Definition 2. Let sj (x) be “the selected piece” of a piecewise-linear AF at the j-th unit in a DNN
for a given input image x, and let s(x) be the set of sj (x) for all the nodes in a DNN .
For example, for a ReLU activation function, sj (x) takes either 0 or 1 depending on whether the
input to the j -th unit is located at the flat part (inactive) or the linear part (active) of the ReLU
function. Using the notion of selected pieces s(x), instead of computing the selective p-value in (9),
we consider the following over-conditioning (oc) conditional p-value
Poelective = Pho (|Z| ≥ Izobsll Z ezoc),	(11)
where Zoc = {z ∈ R ∣ A(X(Z)) = A(xobs), s(x(z)) = s(x°bs)}.	However, SUch an over-
conditioning in SI leads to the loss of statistical power (Lee et al., 2016).
OUr second idea is to develop a homotopy method to resolve the over-conditioning problem, i.e.,
remove the conditioning of s(x(z)) = s(xobs). With the homotopy method, we can efficiently
compUte A(x(z)) in a finite nUmber of operations withoUt the need of considering infinitely many
valUes of z ∈ R, which is sUbseqUently Used to obtain trUncation region Z in (8). The main idea
is to compUte a finite nUmber of breakpoints at which one node of the network is going to change
its statUs from active to inactive or vice versa. This concept is similar to the regUlarization path of
Lasso where we can compUte a finite nUmber of breakpoints at which the active set changes.
To this end, we introdUce a two-step iterative approach generally described as follows (see Fig. 2):
•	Step 1 (over-conditioning step). Considering over-conditioning case by additionally conditioning
on the selected pieces of all the hidden nodes in the DNN.
•	Step 2 (homotopy step). Combining mUltiple over-conditioning cases by homotopy method to
obtain A(x(z)) for all z ∈ R.
3.1 Step1: Over-conditioning Step
We now show that by conditioning on the selected pieces s(xobs) of all the hidden nodes, we can
write the selection event of the DNN as a set of linear ineqUalities.
Lemma 1. Consider a class of DNN which consists of affine operations and piecewise-linear AFs.
Then, the over-conditioning region is written as
Zoc = {z∈R l Θ(s(xobs))x(z) ≤ ψ(s(xobs))}
for a matrix Θ(s(xobs)) and a vector ψ(s(xobs)) which depend only on the selected pieces s(xobs).
Proof. For the class of DNN, by fixing the selected pieces of all the piecewise-linear AFs, the inpUt
to each AF is represented by an affine fUnction of an image x. Therefore, the condition for selecting
5
Under review as a conference paper at ICLR 2021
a piece in a piecewise-linear AF, sj(x(z)) = sj(xobs), is written as a linear inequality w.r.t. x(z).
Similarly, the value of each unit in the representation layer is also written as an affine function of
x(z). Since the attention region is selected if the value is greater than a threshold, the choice of
attention region A(X(Z)) = A(Xobs) is characterized by a set of linear inequalities w.r.t. x(z). □
Furthermore, let us consider max-operation, an operation to select the max one from a finite number
of candidates. A max-operation is characterized by a set of comparison operators, i.e., inequali-
ties. Let Us consider a DNN which contains max-operators, and denote S(X) be the set of selected
candidates of all the max-operators for an input image X.
Corollary 1. Consider a class of DNN which consists of affine operations, max-operations and
Piecewise-linear AFs. Then, a region ZoC defined as ZoC := {z ∈ Zoc | S(X(Z)) = S(Xobs)} is
characterized by a set of linear inequalities w.r.t. X(z).
The proof is shown in Appendix A.1.
Remark 1. In this work, we mainly focus on the trained DNN where the activation functions used at
hidden layers are piecewise linear, e.g., ReLU, Leaky ReLU, which is commonly used in CNN. Oth-
erwise, if there is any specific demand to use non-piecewise linear functions such as sigmoid or tanh
at hidden layers, we can apply some piecewise-linear approximation approach to these functions.
We provided examples about the approximation for this case in Appendix A.5.
Remark 2. Most of the basic operations in a trained neural network are written as affine operations.
In the traditional neural network, the multiplication results between the weight matrix and the output
of the previous layer and its summation with bias vector is affine operation. In a CNN, the main
convolution operation is obviously an affine operation. Upsampling operation is also affine.
Remark 3. Although the max-pooling operation is not an affine operation, it can be written as a
set of linear inequalities. For instance, v1 = max{v1 , v2, v3} can be written as a set {e1>v ≤
e2>v, e1>v ≤ e3>v}, where v = (v1, v2, v3)> and ei is a standard basis vector with a 1 at position i.
Remark 4. In Remark 1, we mentioned that we need to perform piecewise linear approximation
for non-piecewise linear activations. However, if these functions are used at output layer, we do not
need to perform the approximation task because we can define the set of linear inequalities based on
the values before doing activation. See the next example for the case of sigmoid function.
Example 1. Let us consider a 3-layer neural network with n input nodes, h hidden nodes and n
ouput nodes. Let W(1) ∈ Rh×n and w(1) ∈ Rh respectively be the weight matrix and bias vector
between input layer and hidden layer, and W(2) ∈ Rn×h and w(2) ∈ Rn respectively be the weight
matrix and bias vector between hidden layer and output layer. The activation function at hidden layer
is ReLU, and we use sigmoid function at output layer. At the hidden layer, for any node j ∈ [h], the
selection event is written as
(Wj(,1:)X + wj(1) ≥ 0, if the output of ReLU function at jth node ≥ 0,
Wj(,1: ) X + wj(1) < 0,	otherwise.
Let a(1) ∈ Rh and S(1) ∈ Rh be the vectors in which a(1)	= 1, s(1) = 1 if the output of ReLU
j∈[h]	j ∈[h]
function at the jth node ≥ 0, and aj(1) = 0, s(j1) = -1 otherwise. Then we have the linear inequality
system Θ1X ≤ ψ1 where Θ1 = (-s(11)W1(,1:), ..., -s(h1)Wh(,1:))> and ψ1 = (s(11)w1(1), ..., s(h1)wh(1))>.
Next, for any output node o ∈ [n], the selection event—a linear inequality—is written as
Wo(,2:) ((W(1)X + w(1)) ◦ a(1)) + wo(2) ≥ 0,	if the output of sigmoid function at oth node ≥ 0.5,
Wo(,2:) ((W(1)X + w(1)) ◦ a(1)) + wo(2) < 0,	otherwise,
where ◦ is the element-wise product. Similar to the hidden layer, we can also construct the linear
inequality system Θ2X ≤ ψ2 at the output layer. Finally, the whole linear inequality system is
written as
ΘX ≤ ψ = (Θ1 Θ2)> X ≤ (ψ1 ψ2)> .	(12)
6
Under review as a conference paper at ICLR 2021
Algorithm 1 Compute_solution_path
Input: a, b, [zmin
, zmax]
1:	Initialization: t = 1, zt = zmin , T = zt
2:	while zt < zmax do
3:	Obtain A(x(zt)) by applying a trained DNN to x(zt) = a + bzt
4:	Compute the next breakpoint zt+ι — Equation (13). Then assign T = T ∪ {zt+ι}, and t = t + 1
5:	end while
Output: {A(x(zt)}zt∈T
3.2 Step 2: Homotopy Step
We now introduce a homotopy method to compute A(x(z)) based on over-conditioning step.
Lemma 2. Consider a real value zt. By applying a trained DNN to x(zt), we obtain a set of linear
inequalities Θ(s(x(zt)))x(zt) ≤ ψ(s(x(zt))). Then, the next breakpoint zt+1 > zt at which the status
of one node is going to be changed from active to inactive or vice versa, i.e., the sign of one linear
inequality is going to be changed, is calculated by
zt+1
ψk(s(x(zt))) - (Θ(s(x(zt)))a)k
min	―k------------------------------
k:(θ(s(X(Zt))) b)k>o	(Θ(s(χ(^t')')')b)k
(13)
The proof is shown in Appendix A.2. Algorithm 1 shows our solution to efficiently identify
A(x(z)). In this algorithm, multiple breakpoints z1 < z2 < ... < z|T| are computed one by one.
Each breakpoint zt, t ∈ [|T |], indicates a point at which the sign of one linear inequality is changed,
i.e., the status of one node in the network is going to change from active to inactive or vice versa.
By identifying all these breakpoints {zt}t∈[∣τ∣], the solution Path is given by A(X(Z)) = A(X(Zt))
if z ∈ [zt, zt+1], t ∈ [|T |]. For the choice of [zmin, zmax], see Appendix A.3.
4 Experiment
We highlight the main results. Several additional results and details can be found in Appendix A.6.
Numerical Experiments. We demonstrate the performances of two versions of the proposed
method: proposed-method (homotopy) and proposed-method-oc. The p-values in these two versions
were computed by (5) and (11), respectively. Besides, we also compared the proposed methods with
the naive p-value in (4) and the permutation test. The details of permutation test procedure is de-
scribed in Appendix A.6. To test the FPR control, we generated 120 null images X = (x1, ..., xn)
in which Xi ∈ [n]〜N(0,1) for each n ∈ {64,256,1024,4096}. To test the power, We generated
images X = (x1, ..., xn) with n = 256 for each true average difference in the underlying model
μc+ - μc- = ∆μ ∈ {0.5,1.0,1.5, 2.0}. For each case, we ran 120 trials. We chose the signifi-
cance level α = 0.05. For more information about the setup as well as the the structure of a neural
network, see the experimental setup paragraph in Appendix A.6. The results of FPR control are
shown in the first part of Fig. 3. The proposed methods could successfully control the FPR under
α = 0.05 while the naive method can not. Since the naive method fails to control FPR, we did not
consider the power anymore. In the second part of Fig. 3, we see that the over-conditioning option
has lower power than the homotopy method. It is because the truncation region in proposed-method-
oc is shorter than the one in proposed-method (homotopy), which is demonstrated in the third part
of Fig. 3. The last part of Fig. 3 shows the reason why the proposed homotopy method is efficient.
With the homotopy method, we only need to consider the number of encountered intervals on the
line along the direction of test statistic which is almost linearly increasing in practice.
Real-data examples. We performed comparison on real-world brain image dataset, which in-
cludes 939 images with tumor and 941 images without tumor. We first compared our method with
permutation test in terms of FPR control. The results are shown in Table 1. Since the permutation
test could not control the FPR properly, we did not compare the power. The comparisons between
naive p-value and selective p-value are shown in Figs. 4, 5, 6 and 7. The naive p-value was still small
7
Under review as a conference paper at ICLR 2021
Figure 3: Results of false positive rate (FPR), power, length of interval, and encountered interval.
_	Interval Length
§ 20j-------------------------
φ
lɪ5
iιo
2
¾ 5
φ 0：	,	,
~i proposed-method-oc proposed-method
Method
Table 1: FPR and power comparisons on real-world brain image dataset.
	Attention Detection Task		Segmentation Task	
	FPR	Power	FPR	Power
Proposed Method	0.056	0.669	0.057	0.683
Permutation Test	0.850	-	0.640	-
Input Image Attention Map Attention Region
(a) naive-p = 0.00 and selective-p = 0.87
Input Image Attention Map Attention Region
(b) naive-p = 0.00 and selective-p = 0.29
Figure 4: Inference on hypotheses obtained from internal representation (without tumor region).
Input Image Attention Map Attention Region
(a) naive-p = 0.00 and selective-p = 0.01
Input Image Attention Map Attention Region
(b) naive-p = 0.00 and selective-p = 2.17e-5
Figure 5: Inference on hypotheses obtained from internal representation (with tumor region).
Input Image Segmentation Result
Input Image Segmentation Result
Input Image Segmentation Result
pnaive = 0.00, pselective = 0.67
pnaive = 0.00, pselective = 0.45 pnaive = 0.00, pselective = 0.16
Figure 6: Inference on hypotheses obtained from output representation (without tumor region).
Input Image Segmentation Result
Input Image Segmentation Result
Input Image Segmentation Result
pnaive = 0.00, pselective = 8.15e-5
pnaive = 0.00, pselective = 9.20e-3
pnaive = 0.00, pselective = 1.91e-2
Figure 7: Inference on hypotheses obtained from output representation (with tumor region).
even when the image has no tumor region, which indicates that the naive p-values cannot be used
for quantifying the reliability of DNN-driven hypotheses. The proposed method could successfully
identify false positive detections as well as true positive detections.
8
Under review as a conference paper at ICLR 2021
5 Conclusion
We proposed a novel method to conduct statistical inference on the significance of the data-driven
hypotheses driven from neural network representation based on the concept of selective inference. In
the context of explainable DNN or interpretable DNN, we are primarily interested in the reliability
of the trained network when given new inputs (not training inputs). Therefore, the validity of our
proposed method does not depend on how the DNN is trained.
In regard of the generality, the proposed method can be applied to any kind of network as long as
the network operation is characterized by a set of linear inequalities (or approximated by piecewise-
linear functions) because all the algorithms and theories in §2 and §3 only depend on the property
of each component and does not depend on the entire structure of the network.
We believe that this paper provides a significant step toward reliable artificial intelligence (AI) and
open several directions for statistically evaluating the reliability of DNN representation-driven hy-
potheses. Although it is not necessary to account the impact of training in this paper because the
validity of our proposed method does not depend on how the DNN is trained, defining a new prob-
lem setup and providing solution for the case in which the training process needs to be considered
is a potential direction. Moreover, widening the practical applicability of the proposed method in
other fields such as NLP and signal processing would also represent a valuable contribution.
References
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7):e0130140, 2015.
Francois Bachoc, Hannes Leeb, and Benedikt M Potscher. Valid confidence intervals for post-
model-selection predictors. arXiv preprint arXiv:1412.4605, 2014.
FranCoiS Bachoc, Gilles Blanchard, Pierre Neuvial, et al. On the post selection inference constant
under restricted isometry properties. Electronic Journal of Statistics, 12(2):3736-3757, 2018.
Shuxiao Chen and Jacob Bien. Valid inference corrected for outlier removal. Journal of Computa-
tional and Graphical Statistics, pp. 1-12, 2019.
Yunjin Choi, Jonathan Taylor, Robert Tibshirani, et al. Selecting the number of principal compo-
nents: Estimation of the true rank of a noisy matrix. The Annals of Statistics, 45(6):2590-2617,
2017.
Ann-Kathrin Dombrowski, Maximillian Alber, Christopher Anders, Marcel Ackermann, Klaus-
Robert Muller, and Pan Kessel. Explanations can be manipulated and geometry is to blame.
In Advances in Neural Information Processing Systems, pp. 13589-13600, 2019.
Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.
arXiv preprint arXiv:1702.08608, 2017.
Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4829-
4837, 2016.
Vo Nguyen Le Duy and Ichiro Takeuchi. Parametric programming approach for powerful lasso
selective inference without conditioning on signs. arXiv preprint arXiv:2004.09749, 2020.
Vo Nguyen Le Duy, Hiroki Toda, Ryota Sugiyama, and Ichiro Takeuchi. Computing valid p-value
for optimal changepoint by selective inference using dynamic programming. arXiv preprint
arXiv:2002.09132, 2020.
William Fithian, Dennis Sun, and Jonathan Taylor. Optimal inference after model selection. arXiv
preprint arXiv:1410.2597, 2014.
William Fithian, Jonathan Taylor, Robert Tibshirani, and Ryan Tibshirani. Selective sequential
model selection. arXiv preprint arXiv:1512.02565, 2015.
9
Under review as a conference paper at ICLR 2021
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437,
2017.
Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681-3688, 2019.
Megan L Head, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. The extent
and consequences of p-hacking in science. PLoS Biol, 13(3):e1002106, 2015.
Juyeon Heo, Sunghwan Joo, and Taesup Moon. Fooling neural network interpretations via adversar-
ial model manipulation. In Advances in Neural Information Processing Systems, pp. 2925-2936,
2019.
Sangwon Hyun, Kevin Lin, Max G’Sell, and Ryan J Tibshirani. Post-selection inference for
changepoint detection algorithms with application to copy number variation data. arXiv preprint
arXiv:1812.03644, 2018.
John PA Ioannidis. Why most published research findings are false. PLoS medicine, 2(8):e124,
2005.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, Sven
Dahne, Dumitru Erhan, and Been Kim. The (Un) reliability of Saliency methods. arXiv preprint
arXiv:1711.00867, 2017.
Jason D Lee, Dennis L Sun, Yuekai Sun, Jonathan E Taylor, et al. Exact post-selection inference,
with application to the lasso. The Annals of Statistics, 44(3):907-927, 2016.
Keli Liu, Jelena Markovic, and Robert Tibshirani. More powerful post-selection inference, with
application to the lasso. arXiv preprint arXiv:1801.09037, 2018.
Joshua R Loftus. Selective inference after cross-validation. arXiv preprint arXiv:1511.08866, 2015.
Joshua R Loftus and Jonathan E Taylor. A significance test for forward stepwise model selection.
arXiv preprint arXiv:1405.3920, 2014.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances
in neural information processing systems, pp. 4765-4774, 2017.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
5188-5196, 2015.
David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining
neural networks. In Advances in Neural Information Processing Systems, pp. 7775-7784, 2018.
Snigdha Panigrahi, Jonathan Taylor, and Asaf Weinstein. Bayesian post-selection inference in the
linear model. arXiv preprint arXiv:1605.08824, 28, 2016.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144, 2016.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-
sualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
Shinya Suzumura, Kazuya Nakagawa, Yuta Umezu, Koji Tsuda, and Ichiro Takeuchi. Selective
inference for sparse high-order interaction models. In Proceedings of the 34th International Con-
ference on Machine Learning-Volume 70, pp. 3338-3347. JMLR. org, 2017.
10
Under review as a conference paper at ICLR 2021
Kosuke Tanizaki, Noriaki Hashimoto, Yu Inatsu, Hidekata Hontani, and Ichiro Takeuchi. Computing
valid p-values for image segmentation by selective inference. 2020.
Xiaoying Tian, Jonathan Taylor, et al. Selective inference with a randomized response. The Annals
OfStatistics, 46(2):679-710, 2018.
Ryan J Tibshirani, Jonathan Taylor, Richard Lockhart, and Robert Tibshirani. Exact post-selection
inference for sequential regression procedures. Journal of the American Statistical Association,
111(514):600-620, 2016.
Ronald L Wasserstein and Nicole A Lazar. The asa statement on p-values: context, process, and
purpose, 2016.
Fan Yang, Rina Foygel Barber, Prateek Jain, and John Lafferty. Selective inference for group-sparse
linear models. In Advances in Neural Information Processing Systems, pp. 2469-2477, 2016.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, and Ting Wang. Interpretable
deep learning under fire. In 29th {USENIX} Security Symposium ({USENIX} Security 20), 2020.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2921-2929, 2016.
11