Under review as a conference paper at ICLR 2021
Unsupervised Disentanglement Learning by
INTERVENTION
Anonymous authors
Paper under double-blind review
Ab stract
Recently there has been an increased interest in unsupervised learning of disentan-
gled representations on the data generated from variation factors. Existing works
rely on the assumption that the generative factors are independent despite this as-
sumption is often violated in real-world scenarios. In this paper, we focus on the
unsupervised learning of disentanglement in a general setting which the generative
factors may be correlated. We propose an intervention-based framework to tackle
this problem. In particular, first we apply a random intervention operation on a se-
lected feature of the learnt image representation; then we propose a novel metric
to measure the disentanglement by a downstream image translation task and prove
it is consistent with existing ground-truth-required metrics experimentally; finally
we design an end-to-end model to learn the disentangled representations with the
self-supervision information from the downstream translation task. We evaluate
our method on benchmark datasets quantitatively and give qualitative comparisons
on a real-world dataset. Experiments show that our algorithm outperforms base-
lines on benchmark datasets when faced with correlated data and can disentangle
semantic factors compared to baselines on real-world dataset.
1 Introduction
Learning disentangled representation from the data generated from variation factors gives inter-
pretable insight on real-world applications such as face recognition, self-driving and explainable
healthcare. The notion of disentangled representations was theoretically proposed in (Bengio et al.
(2013)). One conceptually agreed definition is that the disentangled representation comprises a
number of latent factors, with each factor controlling an interpretable aspect of the generated data
(Bengio et al. (2013)). For example, in flower images, disentangled latent factors might control vari-
ations in color, shape, and background. Disentangled representations promise several advantages:
better generalization ability (Higgins et al. (2017)), increased interpretability (Adel et al. (2018)),
and faster learning on downstream tasks such as reasoning (van Steenkiste et al. (2019)).
Despite the recent growth of the field, most of the works of unsupervised disentanglement learning
rely on the assumption that the generative factors are independent. One line of these works includes
variants of Variational AutoEncoder(VAE)(Kingma & Welling (2013)). They directly minimize the
total correlation of the features (Higgins et al. (2016); Chen et al. (2018); Kim & Mnih (2018);
Kumar et al. (2018)). Another kind of disentanglement learning models, GAN-based models (Chen
et al. (2016); Jeon et al. (2018); Lin et al. (2020)) are also restricted by independence assumption
since they randomly sample the latent representation in the data generation process. However, inde-
pendence assumption is often violated in real-world scenarios. For the images used for object clas-
sification, factors such as texture and color are confounded by the species of objects (e.g. stripes and
black/white are correlated since they co-occur in the images of zebras). There are also recent works
which use self-supervised learning to boost disentanglement (Zhu et al. (2020); Lin et al. (2020)).
They propose to achieve an additional self-supervision information by self-supervised learning as-
suming independence assumption is satisfied.
The disentanglement performance of algorithms proposed under independence assumption may re-
duce when generative factors are correlated as in the work TraUble et al. (2020) shows. In this paper,
we consider the unsupervised learning of disentanglement in a general setting which the generative
factors of the data may be correlated. We development an intervention-based framework to address
1
Under review as a conference paper at ICLR 2021
this problem. In particular, we define a random intervention operation which assigns a sampled
value to one selected feature of the learnt image representation. Random intervention operation al-
lows us to obtain an adjusted image representation which satisfies the selected feature is independent
of the rest fixed features. To measure and improve disentanglement, we propose a novel metric by
an elaborated downstream image translation task. A well disentangled representation may result in
an relatively easy translation task and this translation task provides self-supervision information for
our model. We prove the effectiveness of our novel metric experimentally and it correlates well with
existing ground-truth-required metrics.
Our main contributions can be summarized as follows:
1)	We address the unsupervised learning of disentanglement under a general setting that the inde-
pendence assumption may be violated;
2)	We propose an end-to-end framework to tackle the disentanglement, meanwhile we propose a
novel metric and prove it is consistent with existed ground-truth-required metrics experimentally;
3)	We evaluate our framework on benchmark datasets under independent/correlated factors assump-
tion and compare the quality of disentanglement with baselines. The results show that our model
outperforms baselines given correlated data. For the experiments on real-world dataset without
ground truth factors, our model extracts semantic factors compared to baselines.
2	Related Work
2.1	Variational Autoencoders
There are plenty of works of learning disentangled features based on the VAE framework Kingma
& Welling (2013). Most of them modify VAE structure to obtain disentangled features under inde-
pendence assumption. Higgins et al proposed to reweight the KL-term in the learning objective of
VAEs with a hyper-parameter β > 1 to encourage the model to learn independent features (β-VAE)
(Higgins et al. (2016)). Kim et al. used a discriminator on the representation space and mini-
mized total correlation (TC) among features by adversarial training (Kim & Mnih (2018)). Chen et
al. extend β-VAE and estimated and optimized the TC term by employing a mini-batch weighted
sampling. Other works improved the performance by making a specific design for discrete factors
(Dupont (2018); Jeong & Song (2019)) and they also assumed independence in the latent space.
The above works are built on the independence assumption and quantitatively evaluated on datasets
generated from independent factors assumption despite the assumption may not be practical for real-
world data. Recently TraUble et al. systematically induced correlations and found that the classic
VAE-based models fall short of capturing ground-truth disentangled factors.
2.2	Generative Adversarial Networks
Models based on Generative Adversarial Networks(GAN) (Goodfellow et al. (2014)) have also been
proposed to learn disentangled representations. Chen et al. first proposed InfoGAN, which learns
disentangled representations by maximizing the mutual information between a subset of latent vari-
ables and the generated samples (Chen et al. (2016)). Lin et al. enhanced InfoGAN with a con-
trastive regularization motivated by the self-supervised learning (Lin et al. (2020)). There are also
some other works based on GAN (Jeon et al. (2018),Liu et al. (2020)). Liu et al. add an orthogonal
regularization to encourage independent representations. Hu et al. propose a GAN-based learning
framework based on mixing operation(Hu et al. (2018)).
2.3	Self-Supervised Learning
Recent some works have been proposed to boost disentanglement learning by self-supervision learn-
ing (Lin et al. (2020); Zhu et al. (2020)). Zhu et al. defined variation predictability as how easy the
following prediction tasks can be solved: given image pairs generated by latent codes varying in a
single dimension, we predict which dimension is different. Zhu et al. show empirically that varia-
tion predictability and other disentanglement metrics are correlated. Lin et al. proposed a similar
task of predicting the dimension index of the representations computed from a given image pair that
only one dimension is the same in the representations. They try to add their proposed objectives to a
basic disentangled learning model. A relative complex training strategy is needed (Lin et al. (2020))
2
Under review as a conference paper at ICLR 2021
(a)	(b)	(c)	(d)
Figure 1: (a) The graphical model of data generation process under independence assumption; (b)
The graphical model of the case when C is a confounder of the generative factors and (a) can be
seen as a special case that C = 0; (C) The intervention operation do(Gk = g∆) which CUt offs all
edges pointing to Gk and set its value to g3(d) The intervention operation on representation Z.
or the performance improvement may not be significant (ZhU et al. (2020)) or. Similarly, existing
works based on self-supervised learning also rely on independence assumption.
3	Method
We use the notations following the work of Suter et al. (2019) to illustrate the problem of disen-
tangling these factors in Fig.(1). We denote the generative factors of high dimensional observations
X as G. G includes K generative factors G = [G1, G2, ..., GK] where each generative factor can
have single or multi dimensions. We consider the multi-dimension case because some factors such
as shape cannot be represented by one dimension. Then X is generated from G. We denote G\Gk
as G\k. The generative factors could be independent (Fig.(1(a))) or confounded by C (Fig.(1(b))).
The latter situation will be more common in real-world. When G can be color, shape, texture and
background of an object, they are confounded by the species C . In fact, the independence situation
is a special case when C = 0.
Our target is to learn a feature representation Z = [Z1, Z2, ..., ZKZ] matched with G. We learn an
encoder Enc to extract features Z. We would hope for the ideal result that KZ = K and find one-
to-one correspondence of Zk and Gk. However, G is unobserved during training in unsupervised
setting. So we will not assume to access the true value of K and set KZ to be greater than K in
real problem. Based on different assumptions on G, corresponding restrictions will be applied on
Z. For example, the common independence assumption that p(g) = QkK=1 p(gk) requests Z also to
be independent. Keep in mind that we use uppercase to represent variables (e.g. G) and use lower
case to represent the specific values corresponding to the variables (e.g. g).
We use do-calculus to represent the intervention operation as the Fig.( 1(c)) shows. do(Gk = gk∆)
means that we assign a value gk∆ to variable Gk rather than passively observe Gk = gk∆. From
the Proposition 1 of (Suter et al. (2019)), if one factor Gk is intervened, other factors will remain
unchanged: p(gj |do(Gk = gk∆)) = p(gj), ∀gk∆,j. As we cut off the relation between the possible
C and the factor Gk by intervention as in Fig.(1(c)) and Gk is also separate from Gj. We extend Gj
to G\k : p(g\k|do(Gk = gk∆)) = p(g\k).
In this paper we define our intervention process as follows: 1) Randomly sample an image x and
select a factor Gk from its corresponding generative factors g; 2) Randomly Sample a value gk∆ from
the distribution of Gk; 3) Intervene on Gk (assign gk∆ to Gk) and generate a new image x(do(Gk =
gfk)). We denote the distribution of G after such intervention operation as PG and
1K
Pe) = κ fp(g∖Qp(gk).	(I)
k=1
Similarly, We denote the image distribution after intervention as PX. Since intervening on Gk will
not change other factors, we have the proposition as follows:
Proposition 1. Suppose there is image x and the value on the factor Gk is gk, we intervene Gk by
do(Gk = gk∆) and get x(do(Gk = gk∆)). Ifwe further intervene x(do(Gk = gk∆)) by do(Gk = gk),
then we can get the original image x: x(do(Gk = gk∆)) (do(Gk = gk)) = x.
3
Under review as a conference paper at ICLR 2021
The proposition above encourages us to design a disentangling objective in a self-supervised manner.
However it is still inefficient to obtain the disentangled representations as we may learn a trivial
optimal solution. To overcome trivial optimization, we give two additional propositions considering
a general factors setting (e.g. correlated data).
Assumption 1. ∃k ∈ {1, ..., K}, ∀g, p(gk |g\k) andp(g\k |gk) are finite, and ifg satisfies p(g\k) >
0 and p(gk) > 0, we get that p(g [*5)is bounded by a finite value C.
From the assumption 1 above, we assume that the distribution of g can’t shrink to several finite
cases, otherwise p(g) may approaches to infinity whenp(g\k) > 0 and p(gk) > 0.
Consider a special case that G1,G2,…,Gk are independent, PX is exactly PX after intervention from
Eq.(1). However, PX may be different from PX when the independence assumption is violated. In
this case, we propose to match the original image distribution PX with a reweighted distribution of
PX.
Proposition 2. Under the assumption 1, there exists a weight function W on X: w = W (x), s.t.
W ◦ PX (x) = PX (x).
Definition 1 (Image Translation Task). Suppose the input, output and model of image translation
task are U, V and M. For a learnt representation z, given a translation index k, we first sample two
different values zka and zkb from the distribution of Zk and assign them to the feature zk respectively,
then we get a pair of images as in the left part of Fig.(2(c)) (u, v) in which U = X(Zk = zk),v =
X(Zk = Zk). The task aims at predicting V given U as in the right part ofFig.(2(c)) by minimizing:
Ltrans (θM) = E crossentroPy (v, M (u)).	(2)
If we obtain well disentangled representation Z, we will achieve a relatively better translation per-
formance for the task defined in definition 1 as the input images and output images in the dataset
we obtained differ only in one specific feature (e.g. the pair of image (U, v) in Fig (2(c)), they are
the same except for the color.) However, if the learnt representation Z is still entangled, the images
in the pair (U, v) defined in the translation task will differ in more than one factors and there exists
complex relationship between the features, and thus it may result in a relatively worse translation
performance. Based on the analysis above, we give our proposition as follows:
Proposition 3. The performance of the downstream translation task defined in definition 1 reflects
the disentanglement quality of the learnt representation Z.
From Proposition 3, we propose a novel metric which measures the quality of the disentangled repre-
sentation Z by a downstream image translation task. Compared to the metric proposed in the work
of (Zhu et al. (2020)) which predicts the varied feature index given an image pair, our measure-
ment is less likely to over fit and correlates well with existed ground-truth-required disentanglement
metrics.
4	Model S tructure and Optimization
We propose an autoencoder-based structure in Fig.(2). The dimension of each feature Zk in the learnt
representation Z is DZ. We conduct the intervention operation on feature space Z: 1). Sample two
images X, Xr and a feature index k ∈ {1, ..., KZ} independently, then we extract the representation
z = Enc(X) and zr = Enc(Xr); 2). Intervention 1 (I1 in Fig.(2(a))): change the kth feature of
Z to Zr then get the reconstruction result as X(Zk = Z^r). Our objectives together with our model
structure are described as follows:
1.	Reconstruction losses: the original image x is reconstructed from the representation z: X =
Dec(Z) and the reconstruction loss function is Lrec(θEnc, θDec) = Ex CrossentroPy(x, X) in
which crossentroPy means crossentropy between the two images as shown in the upper part
in Fig.(2(a)); according to Propositon 1, if We further extract the representation of X(Zk =
Zkr ) and change Zk back to Zk as shown I2 in Fig.(2(a)), then after reconstruct the image as
♦ ♦
X(Zk = Zk), We hope X(Zk = Zk) is the same as X and the second reconstruction loss function
Lin(θEnc,θDec) = Eχ,χr,k CrossentroPy(X, X).
4
Under review as a conference paper at ICLR 2021
Figure 2: Overall structure of our model. (a) We do two interventions on the original image and
make the reconstruct image meet Proposition 1. (b) We use a network W to calculate the weights of
images from PX and make its weighted distribution match PX. (C) We construct image translations
by intervention.
2.	Adversarial loss: according to the Proposition 2, we can match the orignial image distribution
PX by learning a weight function W, thus we propose an adversarial loss	min	max Ladv
θWt,θE
nc ,θD
ec θDisc
to optimize the module W in Fig.(2(b)) and Ladv is calculate as:
T "八 八 八、L	、〜小 W(X) 1 ,r 、/.、、
Ladv (θWt,θEnc, θDec,θDisC) = Ex,xr,k log DisC(X) + N- log(1 ― DisC(X))
where N is for normalization and calculated as Eχ,χr,k W(X), we use the reconstructed image
X as the real image, Disc means the module discriminator.
3 Disentanglement loss measured by image translation: inspired by the work in (Zhu et al. (2020))
which argues that a prediction task could be used to measure the quality of unsupervised learning
disentanglement. Instead of predicting the changed index of the representation which is vulnera-
ble to hyperparameters and easy to over fit, we propose an image translation task to measure the
quality of the disentanglement. Specifically, our novel metric which measures the difficulty of
the translation task shown detailed in the Algorithm 1. As in Fig.(2(c)), we sample two features
zka and zkb to generate the dataset for input and output for our image translation task. The diffi-
culty of the image translation task reflects the quality of our disentanglement, in which Ltrans is
defined in Eq.(1) and Ldiff (eEnc, θDec) = Ek,za,zbDDLtrans.
Finally, we train our model with the weighted sum of the components described above:
min	max Lrec + λ1Lin + λ2Ladv + λ3Ldiff
θWt,θE
nc ,θD
ec θDisc
(3)
5
Under review as a conference paper at ICLR 2021
Algorithm 1: Calculation of Image Translation Loss
Input: θEnc,θDec, k, two sampled feature zka and zkb of from the representation distribution pZ,
two sampled mini-batches of originial images D and D for training and testing, learning
step s and learning rate α
Generate training data {ui , vi } with k, zka, zkb and images D
Generate testing data {Ui, vi} with k, Za Zb and images D
for i ≤ s do
Calculate VLtrans (θM ) on {ui, vi}
Update Θm with gradient descent: Θm = Θm - αVLtrans(θM)
i=i+1
end
return Ltrans calculated with {Ui,Vi} and model parameter Θm
□Sprites
0.86
-0
9
0
8
-0
7
-0
6
-0
5
O
4
0
3
l0.2
-1.0
3DShapes
-1.0
二;
- 0.7
-0.6
-0.5
-0.4
-0.3
-0.2
Ours DCI SAP IRS	Ours DCI SAP IRS
Figure 3: Spearman rank correlation of existed different disentanglement metrics and ours on two
datasets. The first row/column is our self-supervised image-translation metric, which is highly cor-
related with all other disentanglement scores (row/column 2-4).
5	Experiments
5.1	Dataset and Evaluation
We run experiments on benchmark datasets with pre-defined generative factors 1) dSprites: im-
ages of2D shapes generated from five ground truth independent latent factors: shape (heart, oval and
square)), x-position information (32 values), y-position information (32 values), scale (6 values) and
rotation (40 values); 2) 3DShapes: RGB images of 3D shapes with ground truth factors: shape
(4 values), scale(8 values), orientation(15 values), floor color(10 values), wall color(10 values) and
object color(10 values); 3) CMNIST: we generate a colored MNIST dataset with two generative
factors: content and color; 4) Oxf ordF lowers102: RGB images of flowers, a real-world dataset
without ground-truth generative factors.
We evaluate our algorithm and baselines quantitatively on the first three datasets under two settings:
independent/correlated generative factors. First, we hold out a part of the data as test set. Then we
construct a training set with correlated factors by resampling on the rest of the data. For each dataset,
the correlated factors are: x-position and scale(dSprites), floor color and wall color(3DShapes), color
and content(CMNIST). Similarly, we build a training set with independent factors with the same size
of the correlated training set. As the images in OxfordFlowers102 have no ground-truth generative
factors, we therefore evaluate this dataset by querying: first, we select a query image. For each Zk,
we find out the nearest neighbours of the query images on space Zk.
We choose the following baselines: FactorVAE(Kim & Mnih (2018)), β-TCVAE(Chen et al. (2018))
and InfoGAN-CR(Lin et al. (2020)). We evaluate unsupervised learning for disentanglement using
the popular metrics: DCI(Eastwood & Williams (2018)), SAP(Kumar et al. (2018)) and IRS(Suter
et al. (2019)). For CMNIST dataset, since it is hard to apply most of the baselines and metrics
to because of multi-dimension factors. We only compare our algorithm with FactorVAE using the
metric SAP. The details on evaluation metrics can also be found in the Appendix.
6
Under review as a conference paper at ICLR 2021
Table 1: Comparisons of the popular disentanglement metrics on the dSprites dataset
Independent Factors Correlated Factors
Method	DCI	SAP	IRS	DCI	SAP	IRS
FactorVAE(γ = 10)	0.73	0.55	0.62	0.60	0.46	0.54
FactorVAE(γ = 40)	0.72	0.56	0.63	0.63	0.47	0.57
β-TCVAE	0.62	0.54	0.55	0.55	0.46	0.46
InfoGAN-CR	0.73	0.58	0.50	0.68	0.51	0.45
Ours	0.76	0.55	0.62	0.74	0.51	0.59
Table 2: Comparisons of the popular disentanglement metrics on the 3DShapes dataset
Independent Factors Correlated Factors
Method	DCI	SAP	IRS	DCI	SAP	IRS
FactorVAE(γ = 10)	0.75	0.43	0.65	0.60	0.33	0.57
FactorVAE(γ = 40)	0.73	0.53	0.67	0.49	0.26	0.42
β-TCVAE	0.76	0.46	0.70	0.68	0.23	0.64
InfoGAN-CR	0.53	0.40	0.57	0.36	0.25	0.44
Ours	0.63	0.45	0.62	0.67	0.47	0.66
5.2	Implementation
We choose the network structures of baselines according to their papers. We implement our model
with the similar structures of shared components (Encoder, Decoder) with FactorVAE. In dSprites
and 3DShapes dataset, we follow the common setting of KZ = 10, DZ = 1. In CMNIST, we set
KZ = 2, DZ = 16 and in OxfordFlowers102, we set KZ = 3, DZ = 16. Since some baselines are
designed for one dimension of representation to a factor and difficult to apply when DZ > 1. We
only compare our algorithm with FactorVAE on CMNIST and OxfordFlowers102 datasets. For each
model, we repeat 10 times and report the average performance. More details about implementation
are shown in the Appendix.
Table 3: Comparisons of the popular disentanglement metrics on the CMNIST dataset
Independent Factors Correlated Factors
Method	SAP	SAP
FactorVAE(γ = 10)	0.83 ± 0.02	0.41 ± 0.02
FactorVAE(γ = 40)	0.72 ± 0.08	0.47 ± 0.11
Ours	0.74 ± 0.03	0.70 ± 0.02
5.3	Image Translation Task Difficulty as Disentanglement Metric
In this section, we evaluate the effectiveness of our proposed metric and prove that our novel metric
correlates well with existed ground-truth-required metrics. For fair comparison, we run FactorVAE
with different hyper-parameters, random seeds and factor relationships(independent/correlated) to
cover a large range of model performance. Then we calculate various disentanglement metrics with
ours of each model on the held-out test set. Finally, we compute the spearman rank correlation of
these metrics on dSprites and 3DShapes datasets. Note that the two models are not trained without
overlap objectives when using metrics. From the results in Fig. (3), our proposed self-supervised-
based metrics correlates well with other ground-truth-required metrics, thus it can be used to select
models, choose hyper-parameters, and act as a part of the model training objective.
5.4	Evaluation on disentanglement
The results on dSprites, Shapes3D and CMNIST are reported in Table 1, 2 and 3. Under inde-
pendence assumption, our model achieves comparable performance in some cases. For the data
generated by independent generative factors, factors can be disentangled by directly minimizing
total correlation as existing algorithms do.
7
Under review as a conference paper at ICLR 2021
Ours	FactorVAE(y=40)
Figure 4: Qualitative comparison on 3DShapes under correlated factors. The results are from our
model and FactorVAE(γ = 40) respectively. (a) The importance of individual feature predicting the
value of a given generative factor. (b)The result of latent traversal, we fix other features, change one
feature from its 10%-quantile to 90%-quantile. )
(b)
Query
OUrS
Z1 (color)	Z2 (shape)	Z3(background)
Figure 5: Qualitative comparison on OxfordFlowers102. The left column are query images. The
rest are the nearest 3 neighbors of query images in feature space obtained by Ours and FactorVAE.
However, if the factors are correlated, our model proposed under this general setting outperforms
baselines in almost all cases and has stable performances under independent/correlated settings.
We conduct a qualitative comparison between our algorithm and baselines on 3DShapes dataset.
We define the correlated factors floor color and wall color as (G1, G2) and choose the models
with best DCI obtained by our algorithm and FactorVAE (γ = 40). For the feature importance
matrices shown in Fig. (4(a)), our model obtains well-disentangled features (highlight in red) while
FactorVAE obtains entangled features of floor color and wall color. This is also supported by latent
traversal visualization shown in Fig (4(b)) (highlight in red). In the first two rows in Fig (4(b)), the
other factor remains fixed while changing the value of floor color feature or wall color feature in our
learnt disentangled representation. In the last two rows shown the result of FactorVAE, if we just
change one value of the learnt feature, the other factor can’t remain fixed.
5.5	Qualitative evaluation on Real-world dataset OxfordFlowers 1 02
As shown in Fig. (5), the query images are in the left column, then we present the 3-nearest neigh-
bors of each query image measured by the distance on Zk . Our model can learn semantic factors
of flower color(Z1 ), flower shape(Z2) and background color(Z3) while there is no semantic factors
obtained by FactorVAE model in the right part of Fig. (5).
6	Conclusion
In this paper, we study the problem of unsupervised disentanglement learning under the situation
where independence assumption may be violated. We propose to design an image translation task
to measure the disentanglement and use our metric to provide self-supervision information in the
learning framework. Our metric is robust to hyper-parameters and experiments show it correlates
well with existed ground-truth-required metrics. We evaluate our framework on benchmark datasets
and the results show that it outperforms baselines on correlated data. We also conduct experiments
on a real-world dataset to verify that our framework can disentangle semantic factors compared to
baselines.
8
Under review as a conference paper at ICLR 2021
References
Tameem Adel, Zoubin Ghahramani, and Adrian Weller. Discovering interpretable representations
for both deep generative and discriminative models. In International Conference on Machine
Learning,pp. 50-59, 2018.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disen-
tanglement in variational autoencoders. In Advances in Neural Information Processing Systems,
pp. 2610-2620, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Emilien Dupont. Learning disentangled joint continuous and discrete representations. In Advances
in Neural Information Processing Systems, pp. 710-720, 2018.
Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disen-
tangled representations. In International Conference on Learning Representations, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. 2016.
Irina Higgins, Arka Pal, Andrei Rusu, Loic Matthey, Christopher Burgess, Alexander Pritzel,
Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: improving zero-shot trans-
fer in reinforcement learning. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1480-1490, 2017.
Qiyang Hu, Attila Szabo, Tiziano Portenier, Paolo Favaro, and Matthias Zwicker. Disentangling
factors of variation by mixing them. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3399-3407, 2018.
Insu Jeon, Wonkwang Lee, and Gunhee Kim. Ib-gan: Disentangled representation learning with
information bottleneck gan. 2018.
Yeonwoo Jeong and Hyun Oh Song. Learning discrete and continuous factors of data via alternating
disentanglement. In International Conference on Machine Learning, pp. 3091-3099, 2019.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
Machine Learning, pp. 2649-2658, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disen-
tangled latent concepts from unlabeled observations. In International Conference on Learning
Representations, 2018.
Zinan Lin, Kiran K Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and modelcentral-
ity: Self-supervised model training and selection for disentangling gans. ICML, 2020.
Bingchen Liu, Yizhe Zhu, Zuohui Fu, Gerard de Melo, and Ahmed Elgammal. Oogan: Disentan-
gling gan with one-hot sampling and orthogonal regularization. In AAAI, pp. 4836-4843, 2020.
9
Under review as a conference paper at ICLR 2021
Raphael Suter, Djordje Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness. In Interna-
tional Conference on Machine Learning, pp. 6056-6065. PMLR, 2019.
Frederik Trauble, Elliot Creager, Niki Kilbertus, Anirudh Goyal, Francesco Locatello, Bernhard
Scholkopf, and Stefan Bauer. Is independence all you need? on the generalization of representa-
tions learned from correlated data. arXiv preprint arXiv:2006.07886, 2020.
Sjoerd van Steenkiste, Francesco Locatello, Jurgen Schmidhuber, and Olivier Bachem. Are disen-
tangled representations helpful for abstract visual reasoning? In Advances in Neural Information
Processing Systems, pp. 14245-14258, 2019.
Xinqi Zhu, Chang Xu, and Dacheng Tao. Learning disentangled representations with latent variation
predictability. ECCV, 2020.
A Proof of Proposition 2
First, we can have the following lemma under Assumption 1:
Lemma 1. ∀x ∈ SuPP(PX) : PX(x) > 0,
where supp(pX) means its support set. From the assumption 1, supp(pX) is a subset of the image
distribution after intervention. The proof of it is:
Proof. We have:
PX (x) = /
=Z
P(x|g)pG(g)dg
1K
P(XIg)(κ EP(g\k)PGk (gk ))dg
k=1
(4)
1K
K Ik=IJ P(XIg)P(g\k)PGk(gk)dg
Suppose ∃x0 ∈ SUPP(PX), PX (x0) = 0. Then ∀k, ʃP(XIg)P(g∖k)PG% (gk)dg = 0.
According to assumption 1, ∃k0, ∀x, P(gk0 Ig\k0) and P(g\k0 Igk0) are finite, we have when P(gk0)
0, P(g) = P(gk0)P(gk0 Ig\k0) = 0 and when P(g\k0) = 0, P(g) = 0
We further have: if P(g\k，) > 0 andP(gk，) > 0, p®嚅(gk,)≤ C. We have
PX(X0) = P(X0 Ig)PG (g)dg
=	P(xIg)PG (g)dg
p(g\k0)>0,p(gk0)>0
≤ C	P(X0Ig)P(g\k0)P(gk0)dg
p(g\k0 )>0,p(gk0 )>0
≤ C	P(X0Ig)P(g\k)PGk(gk)dg = 0
(5)
which is contradict with x0 ∈ SuPP(PX). So We can have ∀x0 ∈ SuPP(PX), PX(x0) > 0.
With Lemma 1, let W = W(X) = PX(X), we will have W ◦ PX(x) = PX(x).
□
10
Under review as a conference paper at ICLR 2021
dSprltes
X-position
CMNIST
Color
Figure 6: Joint distribution of correlated factor on each dataset. We normalize the highest probability
as 1.
B	Data Processing
For dSprites dataset, we first hold out a test set with 50,000 samples. Then we resample on the rest
of data with joint distribution of x-position and scale as the top of Fig. (6) to construct a training
set with correlated factors. The size of this training set is 200,000. We also resample a training set
whose size is 200,000 with independence factors. Similar operations have also been conducted on
3DShapes. The size of training set and test set are 100,000 and 50,000. For CMNIST, we paint on
the original training and test set of MNIST dataset respectively with ten different colors. Each kind
of color is corresponding to that in 3DShapes dataset. The joint distribution of correlated factors are
shown in Fig (6)
C Network Structure
In almost all experiments, we used a convolutional neural network for the encoders of all autoen-
coder models (Factor VAE, β-TCVAE and our model), discriminators of InfoGAN-CR model and
our model, weight network of our model. We used a deconvolutional neural network for the decoders
of all autoencoder models (Factor VAE, β-TCVAE and our model) and generator of InfoGAN-CR
model. We used a multi-layer perceptron for total correlation discriminator of FactorVAE. One
exception is that for dSprites dataset, we use multi-layer perceptrons for encoder and decoder of
β-TCVAE, according to the authors’ public implementation. All network structure of our model are
shown from Table 4 to 9.
Encoder	Decoder
Input 64 X 64 X n image	Input ∈ R10
4 X 4 conv. 32 ReLU.strideΓ"	FC. 128 ReLU.
4 x 4 conv. 32 ReLU. stride 2.	FC. 4 X 4 X 64 ReLU.
4 x 4 conv. 64 ReLU. stride 2.	4 X 4 UPconv. 64 ReLU. stride 2.
4 x 4 conv. 64 ReLU. stride 2.	4 X 4 upconv. 32 ReLU. stride 2.
FC. 128.	4 X 4 upconv. 32 ReLU. stride 2.
FC.10.	4 X 4 upconv. X n- stride 2 Sigmoid
Table 4: Encoder and Decoder network architectures of our model for dSprites(nc = 1) and
3DShapes (nc = 3) experiments.
We used the Adam optimizer for all updates. For baselines, learning rates are selected according
to the original papers. Since our dataset is constructed with sampling, the total number of training
11
Under review as a conference paper at ICLR 2021
Discriminator	WeightNetWork	
Input 64 X 64 X n image	Input 64 x 64 X n image
4 X 4 conv. 32 ReLU.stridh	4 X 4 conv. 32 ReLU. Stride2"
4 x 4 conv. 32 ReLU. stride 2.	4 X 4 conv. 32 ReLU. stride 2.
4 x 4 conv. 64 ReLU. stride 2.	4 X 4 conv. 64 ReLU. stride 2.
4 x 4 conv. 64 ReLU. stride 2.	4 X 4 conv. 64 ReLU. stride 2.
FC. 128.	FC. 128.
FC.1 Sigmoid.	FC.1 Exp.
Table 5: Discriminator and Weight Network architectures of our model for dSprites(nc = 1) and
3DShapes (nc = 3) experiments.
Encoder	Decoder
Input 28 X 28 X 3 image	Input ∈ R32
4 X 4 conv. 64 ReLU. stride 2	FC. 4 X 4 X 64 ReLU.
4 X 4 conv. 128 ReLU. stride 2.	4 X 4 UPconv. 64 ReLU. stride 2.
3 X 3 conv. 128 ReLU. stride 1.	4 X 4 UPconv. X 3. stride 2 Sigmoid
FC. 32.	
Table 6: Encoder and Decoder network architectures of our model for CMNIST experiments.
samples are less than that of the whole dataset. We run each baseline within a closed iteration to
make sure the algorithm converages. We use a batch size of 64 for all experiments except for β-
TCVAE. Since it need a large batch size for estimation of total correlation, we use a batch size of
2048 as the authors used. The learning rate of Encoder, Decoder and Discriminator of our model
is 0.0001. The learning rate of Weight Network is 0.00001. We fix λ3 to be 1. First, we do
not include Ldiff in the objective function and choose λ1 and λ2 with Ldiff as metric. We get
λ1 = 0.25, λ2 = 10.0 for dSprites and 3DShapes dataset, λ1 = 1.0, λ2 = 10.0 for CMNIST and
λ1 = 1.0, λ2 = 100.0 for OxfordFlowers102. After determing λ1 and λ2, we train a model with the
objective function with Ldif f .
D	Disentanglement Evaluation
The details of calculation of disentanglement metrics used in this paper can be referred to their
original works. We clarify the selection of some involved models and hyperparameters in this paper.
In the calculation of SAP, we use a logistic regression model as the classifier of discrete factors.
When we compute the disentanglement metrics of (Eastwood & Williams (2018)) (DCI), we follow
the work of (Lin et al. (2020)). Random forest regressor implemented in the scikit-learn library is
chosen as the regressor. For dSprites experiments, default values for all parameters are used, except
for the max-depth parameter. We use the values: 4, 2, 4, 2, and 2, for the latent factors: shape, scale,
rotation, x-position, and y-position respectively, as used by the IB-GAN paper (Dupont (2018)). For
3DShapes experiments, we use the cross-validation strategy to search for the max-depth of random
forest regressor.
12
Under review as a conference paper at ICLR 2021
Discriminator	WeightNetWork	
Input 28 X 28 X 3 image	Input 28 x 28 X 3 image
4 x 4 conv. 64 ReLU. stride 2	4 x 4 conv. 64 ReLU. stride 2
4 x 4 conv. 128 ReLU. stride 2.	4 X 4 conv. 128 ReLU. stride 2.
3 x 3 conv. 128 ReLU. stride 1.	3 X 3 conv. 128 ReLU. stride 1.
FC. 128.	FC. 128.
FC.1 Sigmoid.	FC. 1 Exp.
Table 7: Discriminator and Weight Network architectures of our model for CMNIST experiments.
Encoder	Decoder
Input 64 x 64 X 3 image	Input ∈ R48
4 X 4 conv.128 ReLU.stri5eT-	FC.512ReLU.
4 x 4 conv. 128 ReLU. stride 2.	FC. 4 X 4 X 256 ReLU.
4 x 4 conv. 256 ReLU. stride 2.	4 x 4 UPconv. 256 ReLU. stride 2.
4 x 4 conv. 256 ReLU. stride 2.	4 x 4 upconv. 128 ReLU. stride 2.
FC.512.	4 x 4 upconv. 128 ReLU. stride 2.
FC. 48.	4 x 4 UPconv. x 3. stride 2 Sigmoid
Table 8: Encoder and Decoder network architectures of our model for OxfordFlowers10 experiment.
Discriminator	WeightNetWork	
Input 64 x 64 X 3 image	Input 64 x 64 X n image
4 x 4 conv. 64 LReLU. stride 2	4 x 4 conv. 64 LReLU. stride 2
4 x 4 conv. 64 LReLU. stride 2.	4 x 4 conv. 64 LReLU. stride 2.
4 x 4 conv. 128 LReLU. stride 2.	4 x 4 conv. 128 LReLU. stride 2.
4 x 4 conv. 128 LReLU. stride 2.	4 x 4 conv. 128 LReLU. stride 2.
FC. 256.	FC. 256.
FC. 256.	FC. 256.
FC.1 Sigmoid.	FC.1 Exp.
Table 9: Discriminator and Weight Network architectures of our model for OxfordFlowers10 exper-
iment.
13