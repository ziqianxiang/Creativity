Under review as a conference paper at ICLR 2021
Data augmentation as stochastic optimization
Anonymous authors
Paper under double-blind review
Ab stract
We present a theoretical framework recasting data augmentation as stochastic op-
timization for a sequence of time-varying proxy losses. This provides a unified
language for understanding techniques commonly thought of as data augmenta-
tion, including synthetic noise and label-preserving transformations, as well as
more traditional ideas in stochastic optimization such as learning rate and batch
size scheduling. We then specialize our framework to study arbitrary augmen-
tations in the context of a simple model (overparameterized linear regression).
We extend in this setting the classical Monro-Robbins theorem to include aug-
mentation and obtain rates of convergence, giving conditions on the learning rate
and augmentation schedule under which augmented gradient descent converges.
Special cases give provably good schedules for augmentation with additive noise,
minibatch SGD, and minibatch SGD with noise.
1	Introduction
Implementing gradient-based optimization in practice requires many choices. These include set-
ting hyperparameters such as learning rate and batch size as well as specifying a data augmentation
scheme, a popular set of techniques in which data is augmented (i.e. modified) at every step of opti-
mization. Trained model quality is highly sensitive to these choices. In practice they are made using
methods ranging from a simple grid search to Bayesian optimization and reinforcement learning
(Cubuk et al., 2019; 2020; Ho et al., 2019). Such approaches, while effective, are often ad-hoc and
computationally expensive due to the need to handle scheduling, in which optimization hyperparam-
eters and augmentation choices and strengths are chosen to change over the course of optimization.
These empirical results stand in contrast to theoretically grounded approaches to stochastic optimiza-
tion which provide both provable guarantees and reliable intuitions. The most extensive work in this
direction builds on the seminal article (Robbins & Monro, 1951), which gives provably optimal
learning rate schedules for stochastic optimization of strongly convex objectives. While rigorous,
these approaches are typically are not sufficiently flexible to address the myriad augmentation types
and hyperparameter choices beyond learning rates necessary in practice.
This article is a step towards bridging this gap. We provide in §3 a rigorous framework for re-
interpreting gradient descent with arbitrary data augmentation as stochastic gradient descent on a
time-varying sequence of objectives. This provides a unified language to study traditional stochas-
tic optimization methods such as minibatch SGD together with widely used augmentations such as
additive noise (Grandvalet & Canu, 1997), CutOut (DeVries & Taylor, 2017), Mixup (Zhang et al.,
2017) and label-preserving transformations (e.g. color jitter, geometric transformations (Simard
et al., 2003)). It also opens the door to studying how to schedule and evaluate arbitrary augmenta-
tions, an important topic given the recent interest in learned augmentation Cubuk et al. (2019).
Quantitative results in our framework are difficult to obtain in full generality due to the complex
interaction between models and augmentations. To illustrate the utility of our approach and better
understand specific augmentations, we present in §3 and §5 results about arbitrary augmentations
for overparameterized linear regression and specialize to additive noise and minibatch SGD in §4
and §6. While our results apply directly only to simple quadratic losses, they treat very general
augmentations. Treating more complex models is left to future work. Our main contributions are:
•	In Theorem 5.1, we give sufficient conditions under which gradient descent under any aug-
mentation scheme converges in the setting of overparameterized linear regression. Our
1
Under review as a conference paper at ICLR 2021
result extends classical results of Monro-Robbins type and covers schedules for both learn-
ing rate and data augmentation scheme.
•	We complement the asymptotic results of Theorem 5.1 with quantitative rates of conver-
gence furnished in Theorem 5.2. These rates depend only on the first few moments of the
augmented data distribution, underscoring the flexibility of our framework.
•	In §4, we analyze additive input noise, a popular augmentation strategy for increasing
model robustness. We recover the known fact that it is equivalent to stochastic optimization
with '2-regularization and find criteria in Theorem 4.1 for jointly scheduling the learning
rate and noise level to provably recover the minimal norm solution.
•	In §6, we analyze minibatch SGD, recovering known results about rates of convergence for
SGD (Theorem 6.1) and novel results about SGD with noise (Theorem 6.2).
2	Related Work
In addition to the extensive empirical work on data augmentation cited elsewhere in this article,
we briefly catalog other theoretical work on data augmentation and learning rate schedules. The
latter were first considered in the seminal work Robbins & Monro (1951). This spawned a vast
literature on rates of convergence for GD, SGD, and their variants. We mention only the relatively
recent articles Bach & Moulines (2013); Defossez & Bach (2015); BottoU et al. (2018); Smith et al.
(2018); Ma et al. (2018) and the references therein. The last of these, namely Ma et al. (2018), finds
optimal choices of learning rate and batch size for SGD in the overparametrized linear setting.
A number of articles have also pointed out in various regimes that data augmentation and more gen-
eral transformations such as feature dropout correspond in part to '2-type regularization on model
parameters, features, gradients, and Hessians. The first article of this kind of which we are aware is
Bishop (1995), which treats the case of additive Gaussian noise (see §4). More recent work in this
direction includes Chapelle et al. (2001); Wager et al. (2013); LeJeune et al. (2019); Liu et al. (2020).
There are also several articles investigating optimal choices of '2-regularization for linear models
(cf e.g. Wu et al. (2018); Wu & Xu (2020); Bartlett et al. (2020)). These articles focus directly
on the generalization effects of ridge-regularized minima but not on the dynamics of optimization.
We also point the reader to Lewkowycz & Gur-Ari (2020), which considers optimal choices for the
weight decay coefficient empirically in neural networks and analytically in simple models.
We also refer the reader to a number of recent attempts to characterize the benefits of data augmenta-
tion. In Rajput et al. (2019), for example, the authors quantify how much augmented data, produced
via additive noise, is needed to learn positive margin classifiers. Chen et al. (2019), in contrast,
focuses on the case of data invariant under the action of a group. Using the group action to generate
label-preserving augmentations, the authors prove that the variance of any function depending only
on the trained model will decrease. This applies in particular to estimators for the trainable parame-
ters themselves. Dao et al. (2019) shows augmented k-NN classification reduces to a kernel method
for augmentations transforming each datapoint to a finite orbit of possibilities. It also gives a second
order expansion for the proxy loss of a kernel method under such augmentations and interprets how
each term affects generalization. Finally, the article Wu et al. (2020) considers both label preserving
and noising augmentations, pointing out the conceptually distinct roles such augmentations play.
3	Data Augmentation as Stochastic Optimization
A common task in modern machine learning is the optimization of an empirical risk
L(W; D)=焉	X	'(f(xj; W ),yj),	(3.1)
(xj,yj)∈D
where f(x; W) is a parameterized model for a dataset D of input-response pairs (x, y) and ' is a
per-sample loss. Optimizing W by vanilla gradient descent on L corresponds to the update equation
Wt+1 = Wt- ηtVwL(Wt；D).
In this context, we define a data augmentation scheme to be any procedure that consists, at every step
of optimization, of replacing the dataset D by a randomly augmented variant, which we will denote
2
Under review as a conference paper at ICLR 2021
by Dt . Typically, Dt is related to D in some way, but our framework does not explicitly constrain
the form of this relationship. Instead, certain conditions on this relationship will be required for our
main results Theorems 5.1 and 5.2 to give useful results for a specific augmentation scheme. A data
augmentation scheme therefore corresponds to the augmented update equation
Wt+1 = Wt- ηtVwL(Wt; Dt).	(3.2)
Since Dt is a stochastic function ofD, it is natural to view the augmented update rule (3.2) as a form
of stochastic optimization for the proxy loss at time t
Lt(W)：= EDt [L(W； Dt)].	(3.3)
The update (3.2) corresponds precisely to stochastic optimization for the time-varying objective
Lt(W) in which the unbiased estimate of its gradient is obtained by evaluating the gradient of
L(W; Dt) on a single sample Dt drawn from the augmentation distribution. The connection between
data augmentation and this proxy loss was introduced for Gaussian noise in Bishop (1995) and in
general in Chapelle et al. (2001), but we now consider it in the context of stochastic optimization.
Despite being mathematically straightforward, reformulating data augmentation as stochastic opti-
mization provides a unified language for questions about learning rate schedules and general aug-
mentation schemes including SGD. In general, such questions can be challenging to answer, and
even evaluating the proxy loss Lt(W) may require significant ingenuity.
While we will return to more sophisticated models in future work, we henceforth analyze general
augmentations in the simple context of overparameterized linear regression. Though there are many
ways to perform linear regression, we restrict to augmented gradient descent both to gain intuition
about specific augmentations and to understand the effect of augmentation on optimization. We
therefore consider optimizing the entries of a weight matrix W ∈ Rp×n by gradient descent on
L(W；D) = |D| X	∣∣y — Wχ∣∣F = NN ∣∣Y -wxIlF,
(x,y)∈D
(3.4)
where our dataset D is summarized by data matrices X ∈ Rn×N and Y ∈ Rp×N, whose N < n
columns consist of inputs xi ∈ Rn and associated labels yi ∈ Rp . Following this notation, a
data augmentation scheme is specified by prescribing at each time step an augmented dataset Dt
consisting of modified data matrices Xt, Yt, whose columns we denote by xi,t ∈ Rn and yi,t ∈ Rp.
Here, the number of columns in Xt and Yt (i.e. the number of datapoints in Dt) may vary.
We now give examples of some commonly used augmentations our framework can address.
•	Additive Gaussian noise: This is implemented by setting Xt = X + σt ∙ G and Yt = Y
for σt > 0 and G a matrix of i.i.d. standard Gaussians. We analyze this in §4.
•	Mini-batch SGD: To implement mini-batch SGD with batch size Bt, we can take Xt =
XAt and Yt = YAt where At ∈ RN ×Bt has i.i.d. columns containing a single non-zero
entry equal to 1 chosen uniformly at random. We analyze this in detail in §6.
•	Random projection: This is implemented by Xt = ΠtX and Yt = Y, where Πt is an
orthogonal projection onto a random subspace. For Yt = Tr(∏t)∕n, the proxy loss is
Lt(W) = kY — YtWXkF + Yt(1 — Yt)n-1 Tr(XX T)kW kF + O(n-1),
which adds a data-dependent `2 penalty and applies a Stein shrinkage on input data.
•	Label-preserving transformations: For a 2-D image viewed as a vector x ∈ Rn, geomet-
ric transforms (with pixel interpolation) or other label-preserving transforms such as color
jitter take the form of linear transforms Rn → Rn. We may implement such augmentations
in our framework by Xt = AtX and Yt = Y for some random transform matrix At.
•	Mixup: To implement Mixup, we can take Xt = XAt and Yt = YAt, where At ∈ RN ×Bt
has i.i.d. columns containing with two random non-zero entries equal to 1 — ct and ct with
mixing coefficient ct drawn from a Beta(αt, αt) distribution for a parameter αt.
Our main technical results, Theorems 5.1 and 5.2, give sufficient conditions for a learning rate
schedule ηt and a schedule for the statistics of Xt , Yt under which optimization with augmented
gradient descent will provably converge. We state these general results in §5. Before doing so, we
seek to demonstrate both the utility of our framework and the flavor of our results by focusing on
the simple but already informative case of additive Gaussian noise.
3
Under review as a conference paper at ICLR 2021
4	Augmentation With Additive Gaussian Noise
An common augmentation in practice injects input noise as a regularizer (Grandvalet & Canu, 1997):
Dt = {(xi,t , yi,t), i = 1, . . . , N }, xi,t = xi + σtgi,t , yi,t = yi ,
where gi,t are i.i.d. standard Gaussian vectors and σt is a strength parameter. This section studies
such augmentations using our framework. A direct computation reveals that the proxy loss
Lt(W) = L0t(W) ：= L(W；D)+ σ ||W||F
corresponding to additive Gaussian noise adds an `2 -penalty to the original loss L. This is simple
but useful intuition. It also raises the question: what is the optimal relation between the learning rate
ηt and the augmentation strength σt (i.e. the '2-penalty)?
To get a sense of what optimal might mean in this context, observe first that if σt = 0, then directly
differentiating the loss L yields the following update rule:
Wt+1 = Wt + 2Nt ∙ (Y — WtX)XT	(4.1)
The increment Wt+1 - Wt is therefore contained in the column span
Vk := column span of XXT ⊆ Rn	(4.2)
of the model Hessian XXT. Overparameterization implies Vk 6= Rn. The component Wt,⊥ of Wt
that is in the orthogonal complement of Vk thus remains frozen to its initialized value. Geometrically,
this means that there are some directions, namely those in the orthogonal complement to Vk , which
gradient descent “cannot see.” Optimization with appropriate step sizes then yields
lim Wt = W0,⊥ + Wmin,	Wmin :=YXT(XXT)+,
t→∞
where Wmin is the minimum norm solution of Y = WX . The original motivation for introducing
the '2-regularized losses Lσ is that they provide a mechanism to eliminate the component W0,⊥ for
all initializations, not just the special choice W0 = 0, and they can be used to regularize non-linear
models as well. Indeed, for σ > 0, the loss Lσ is strictly convex and has a unique minimum
Wσ := YXT(XXT + σ2N ∙ Idn×n)-1,
which tends to the minimal norm solution in the weak regularization limit limσ→0 W= = Wmin.
Geometrically, this is reflected in the fact that '2-penalty yields non-trivial gradient updates
t
Wt+1,⊥ = Wt,⊥ - ηtσ2Wt,⊥ = (Id - ηtσ2)Wt,⊥ = Y(Id - ηsσ2)W0,⊥,	(4.3)
which drive this perpendicular component ofWt to zero provided Pt∞=1 ηt = ∞. However, for each
positive value of σ, the '2-penalty also modifies the gradient descent updates for Wt,k, ultimately
causing Wt to converge to W=, which is not a minimizer of the original loss L.
This downside of ridge regression motivates jointly scheduling the step size ηt and the noise strength
σt . We hope that driving σt to 0 at an appropriate rate can guarantee convergence of Wt to Wmin.
Namely, we want to retain the regularizing effects of '2 -noise to force Wt,⊥ to zero while mitigating
its adverse effects which prevent W= from minimizing L. It turns out that this is indeed possible:
Theorem 4.1 (Special case of Theorem 5.1). Suppose σt2, ηt → 0 with σt2 non-increasing and
∞∞
ηtσt2 = ∞ and	ηt2σt2 < ∞.	(4.4)
Then, Wt →p Wmin. Further, if ηt = Θ(t-x) and σt2 = Θ(t-y) with x, y > 0, x + y < 1, and
2x + y > 1, then for any ∈ (0, min{y, x/2}), we have that
tmin{y,1 x}∕∣Wt- Wmin kF →→ 0.
4
Under review as a conference paper at ICLR 2021
Let us give a few comments on Theorem 4.1. First, although it is stated for additive Gaussian noise,
an analogous version holds for arbitrary additive noise with bounded moments, with the only change
being a constant multiplicative factor in the second condition of (4.4).
Second, that convergence in probability Wt →p Wmin follows from (4.4) is analogous to a Monro-
Robbins type theorem (Robbins & Monro, 1951). Indeed, inspecting (4.3), we see that the first
condition in (4.4) guarantees that the effective learning rate ηtσt2 in the orthogonal complement to
Vk is sufficiently large that the corresponding component Wt,⊥ of Wt tends to 0, allowing the result
of optimization to be independent of the initial condition W0 . Further, the second condition in (4.4)
guarantees that the variance of the gradients, which at time t scales like ηt2σt2 is summable. As in
the usual Monro-Robbins setup, this means that only a finite amount of noise is injected into the
optimization. Further, (4.4) is a direct specialization of (5.5) and (5.6) from Theorem 5.1.
Third, by optimizing over x, y, we see that fastest rate of convergence guaranteed by Theorem 4.1
is obtained by setting ηt = t-2/3+, σt2 = t-1/3 and results in a O(t-1/3+) rate of convergence. It
is not evident that this is the best possible rate, however.
Finally, although we leave systemic study of augmentation in non-linear models to future work,
our framework can be applied beyond linear models and quadratic losses. To see this, as noted for
kernels in Dao et al. (2019), augmenting inputs of nonlinear feature models correspond to applying
different augmentations on the outputs of the feature map. To give a concrete example, consider
additive noise for small σt . For any sufficiently smooth function g, Taylor expansion reveals
2
E [g(x + σtG)] = g(x) + -2^∆g(x) + O(σ4),
where ∆ = Pi ∂i2 is the Laplacian and G is a standard Gaussian vector. For a general loss of the
form (3.1) we have
2
Lt(W )= L(W; D) +而 X Tr [(Vxf )T (Hf') Vxf +(Vf ')T ∆χf + O(σ4),
(x,y∈D)
where we have written Hf ` for the Hessian of some convex per-sample loss ` with respect to f
and Vx , Vf for the gradients with respect to x, f, respectively. This is consistent with the similar
expansion done in the kernel setting by Dao et al. (2019, Section 4). If σt is small, then the proxy
loss Lt will differ significantly from the unaugmented loss L only near the end of training, when
We expect Vf' to be small and Hf' to be positive semi-definite. Hence, We find heuristically that,
neglecting higher order terms in σt, additive noise with small σt corresponds to an '2-regularizer
22
Tr [ɪ (Vxf)T (HfL) Vxf =: £ ∣∣Vχf ∣∣HfL
for the gradients off with respect to the natural inner product determined by the Hessian of the loss.
This is intuitive since penalizing the gradients of f is the same as requiring that f is approximately
constant in a neighborhood of every datapoint. However, although the input noise was originally
isotropic, the '2-penalty is aligned with the loss Hessian and hence need not be.
5	Time-varying Monro-Robbins for linear models under
AUGMENTATION
In this section, we state two general results, Theorems 5.1 and 5.2, which provide sufficient condi-
tions for jointly scheduling learning rates and general augmentation schemes to guarantee conver-
gence of augmented gradient descent in the overparameterized linear model (3.4).
5.1	A general time-varying Monro-Robbins theorem
Given an augmentation scheme for the model (3.4), the time t gradient update at learning rate ηt is
Wt+ι :=Wt+-ηt- ∙ (Yt - Wt Xt)XT,	(5.1)
5
Under review as a conference paper at ICLR 2021
where Dt = (Xt, Yt) is the augmented dataset at time t. The minimum norm minimizer of the
corresponding proxy loss Lt (see 3.3) is
Wt := E[YtxT]E[XtXT]+,	(5.2)
where E[XtXtT]+ denotes the Moore-Penrose pseudo-inverse. In this section we state a rigorous
result, Theorem 5.1, giving sufficient conditions on the learning rate ηt and distributions of the
augmented matrices Xt , Yt under which augmented gradient descent converges. In analogy with
the case of Gaussian noise, (5.1) shows Wt+1 - Wt is contained in the column span of the Hessian
XtXtT of the augmented loss and almost surely belongs to the subspace
Vk := column span of E[XtXtT] ⊆ Rn .	(5.3)
To ease notation, we assume that Vk is independent of t. This assumption is valid for additive
Gaussian noise, random projection, MixUp, SGD, and their combinations. We explain in Remark
B.2 how to generalize Theorems 5.1 and 5.2 to the case where Vk varies with t.
Let us denote by Qk : Rn → Rn the orthogonal projection onto Vk . At time t, gradient descent
leaves the projection Wt(Id -Qk ) of Wt onto the orthogonal complement of Vk unchanged. In
ContraSt, || WtQk - W"f decreases at a rate governed by the smallest positive eigenvalue
λmin,V∣l (E [XtXj]) := λmin (Q∣∣E [XtXj] Qk)
of the Hessian for the proxy loss Lt, which is obtained by restricting its full Hessian E [XtXT] to
V]∣. Moreover, whether and at what rate WtQk - Wf converges to 0 must depend on how quickly
Ξ: ：= Wt+ι - Wt*	(5.4)
tends to zero. Indeed, ∣∣Ξ*∣∣f is the distance between proxy loss optima at different times and hence
must tend to zero if ||WtQk - Wt* ||F converges to zero.
Theorem 5.1. Suppose that Vk is independent of t, that the learning rate satisfies ηt → 0, that the
proxy optima satisfy
∞
X kΞt*kF < ∞,	(5.5)
t=0
ensuring the existence of a limit W∞* := limt→∞ Wt*, and that
∞
X ηtλmin,Vk (E[XtXtT]) = ∞.	(5.6)
t=0
If either
∞
Xηt2EkXtXtT-E[XtXtT]k2F+kYtXtT-E[YtXtT]k2F] <∞	(5.7)
t=0
or the more refined condition
∞
Xηt2E kXtXtT - E[XtXtT]k2F + kE[Wt](XtXtT - E[XtXtT]) - (YtXtT - E[YtXtT])k2F <∞
t=0
(5.8)
hold, then for any initialization W0 we have WtQk →p W∞* .
The conditions of Theorem 5.1 can be applied to the choice of joint schedule for the learning rate
and augmentation scheme applied to gradient descent. If the same augmentation is applied with
different strength parameters at each step t such as σt for Gaussian noise, they impose conditions
on the choice of joint schedule for ηt and these strength parameters. In the example of Theorem 4.1
for Gaussian noise, the condition that σt2 is non-increasing implies (5.5), the first condition of (4.4)
implies (5.6), and the second condition of (4.4) implies (5.7).
In addition to the conditions Theorem 5.1 imposes on Dt, the proxy optima Wt* and their limit W∞*
are determined by the distribution ofDt. Therefore, for W∞* in Theorem 5.1 to be a desirable set of
parameters for the original dataset D, the augmented dataset Dt must have some relation to D.
6
Under review as a conference paper at ICLR 2021
When the augmentation procedure is static in t, Theorem 5.1 reduces to a standard Monro-Robbins
theorem Robbins & Monro (1951) for the (static) proxy loss Lt(W). As in that setting, condition
(5.6) enforces that the learning trajectory travels far enough to reach an optimum. Condition (5.7)
implies the weaker condition (5.8); the second summand in (5.8) is the variance of the gradient of
the augmented loss L(W; Dt), meaning (5.8) implies the total variance of the stochastic gradients
is summable. Condition (5.5) is new; it enforces that the minimizers Wj of the proxy losses Lt(W)
change slowly enough that the augmented optimization procedure can keep pace.
Though it may be surprising that E[Wt] appears in this condition, it may be interpreted as the gra-
dient descent trajectory for the deterministic sequence of proxy losses Lt(W). Accounting for the
dependence on E[Wt] allows us to give more precise rates using the variance of the stochastic gra-
dient in (5.8); we include both (5.7) and (5.8) to allow a user of our results to separately analyze
E[Wt] to obtain stronger conclusions.
5.2	Convergence rates and scheduling for data augmentation
A more precise analysis of the the proof of Theorem 5.1 allows us to obtain rates of convergence for
the projections WtQk of the weights onto to the limiting optimum W∞. In particular, when the
quantities in Theorem 5.1 have power law decay, we obtain the following result.
Theorem 5.2 (informal - Special case of Theorem B.4). If Vk is independent of t, the learning rate
satisfies ηt → 0, and for some 0 < α < 1 < β1 , β2 and γ > α we have
ηtλmin,Vk (E[XtXT]) = Ω(t-α),	kΞ"∣F =OL)	(5.9)
and
ηt2E[kXtXtT - E[XtXtT]k22] = O(t-γ)	(5.10)
and
ηt2E kE[Wt](XtXtT - E[XtXtT]) - (YtXtT - E[YtXtT])k2F =O(t-β2),	(5.11)
then for any initialization W0, we have for any > 0 that
tmin{βl-1,β2-α}-1∣WtQk - W∞kF → 0.
Theorem 5.2 measures rates in terms of optimization steps t, but a different measurement of time
called the intrinsic time of the optimization will be more suitable for measuring the behavior of
optimization quantities. This was introduced for SGD in Smith & Le (2018); Smith et al. (2018),
and we now generalize it to our broader setting. For gradient descent on a loss L, the intrinsic time
is a quantity which increments by ηλmin(H) for a optimization step with learning rate η at a point
where L has Hessian H . When specialized to our setting, it is given by
t-1 2
T(t) := E 2Nλmin,% (EXSXT]).	(5.12)
s=0 N
Notice that intrinsic time of augmented optimization for the sequence of proxy losses LS appears in
Theorems 5.1 and 5.2, which require via condition (5.6) that the intrinsic time tends to infinity as
the number of optimization steps grows.
Intrinsic time will be a sensible variable in which to measure the behavior of quantities such as the
fluctuations of the optimization path f(t) := E[k(Wt - E[Wt])Qk k2F]. In the proofs of Theorems
5.1 and 5.2, we show that the fluctuations satisfy an inequality of the form
f(t+1)≤f(t)(1-a(t))2+b(t)	(5.13)
for a(t) := 2ηtNλmin,v∣∣ (EXtXTD and b(t) := Var[∣∣ηRwL(Wt)∣∣F] so that T(t) = PS=0 a(s).
Iterating the recursion (5.13) shows that
t-1	t-1	t-1
f(t) ≤ f(0) Y(1 - a(s))2 + X b(s) Y (1 - a(r))2
S=0	S=0	r=S+1
≤ e-2τ㈤f (0) + X 粤e2τ(s+1)-2τ㈤(T(s + 1) - τ(s)).
S=0 a(s)
7
Under review as a conference paper at ICLR 2021
For τ := τ(t) and changes of variable A(τ), B(τ), and F(τ) such that A(τ (t)) = a(t), B(τ(t))
b(t), and F (τ (t)) = f (t), we find by replacing a right Riemann sum by an integral that
F(τ) - e-2τ
F(°) + .0τ 篙e2σdσ
(5.14)
0
In order for the result of optimization to be independent of the starting point, by (5.14) we must have
τ → ∞ to remove the dependence on F (°); this provides one explanation for the appearance of τ
in condition (5.6). Further, (5.14) implies that the fluctuations at an intrinsic time are bounded by an
integral against the function A(σ) which depends only on the ratio of A(σ) and B(σ). In the case of
minibatch SGD, we compute this ratio in (6.2) and recover the commonly used “linear scaling” rule
for learning rate.
In Section 6, we specialize Theorem 5.2 to obtain rates of convergence for specific augmentations.
Optimizing the learning rate and augmentation parameter schedules in Theorem 5.2 allows us to
derive power law schedules with convergence rate guarantees in these settings.
6	Implications for mini-batch stochastic gradient descent (SGD)
We now apply our framework to study mini-batch stochastic gradient descent (SGD) with the po-
tential presence of additive noise. Though data augmentation commonly refers to techniques aside
from SGD, we will see that our framework handles it uniformly with other augmentations.
6.1	Mini-Batch SGD
In mini-batch stochastic gradient descent, Dt is obtained by choosing a random subset Bt of D of
prescribed batch size Bt = |Bt|. Each datapoint in Bt is chosen uniformly with replacement from
D, and the resulting data matrices Xt and Yt are scaled so that Lt(W) = L(W; D). Concretely, this
means that for the normalizing factor ct := YN/Bt We have
Xt = ctXAt	and	Yt = ctYAt ,
where At ∈ RN×Bt has i.i.d. columns At,i with a single non-zero entry equal to 1 chosen uniformly
at random. In this setting the minimum norm optimum for each t are the same and given by
Wj = W∞ = YX T(XX T)+,
which coincides with the minimum norm optimum for the unaugmented loss. Our main result for
standard SGD is the following theorem, whose proof is given in Appendix D.1.
Theorem 6.1. If the learning rate satisfies ηt → ° and
∞
ηt = ∞,
t=0
(6.1)
then for any initialization W0, we have WtQk →→ W∞. Iffurther we have that η = Θ(t-x) with
° < x < 1, then for some C > ° we have
ect1-xkWtQk - W∞ kF → °.
Theorem 6.1 recovers the exponential convergence rate for SGD, which has been extensively studied
through both empirical and theoretical means (Bottou et al., 2018; Ma et al., 2018). Because 1 ≤
Bt ≤ N for all t, it does not affect the asymptotic results in Theorem 6.1. In practice, however,
the number of optimization steps t is often small enough that BNt is of order t-a for some α > °,
meaning the choice of Bt can affect rates in this non-asymptotic regime. Though we do not attempt
to push our generic analysis to this granularity, this is done in Ma et al. (2018) to derive optimal
batch sizes and learning rates in the overparametrized setting.
Our proof of Theorem 6.1 shows the intrinsic time is T(t) = P：=0 24卷λmi∏,v∣∣ (XXT) and the
ratio a(t) in (5.14) is by (D.4) bounded uniformly for a constant C > ° by
b(t) W η ηt
≤ C ∙	.
a(t) —	Bt
(6.2)
8
Under review as a conference paper at ICLR 2021
Thus, keeping 第 fixed as a function of T suggests the “linear scaling” η 8 Bt Used empirically
in Goyal et al. (2017) and proposed via an heuristic SDE limit in Smith et al. (2018).
6.2	Mini-Batch SGD with Additive Synthetic Noise
In addition to handling synthetic noise and SGD separately, our results and framework also cover
the hybrid case of mini-batch SGD with batch size Bt and additive noise at level σt . Here,
Xt =ct(XAt+σtGt)	and	Yt =ctYAt,
where ct and At are as in Section 6.1 and Gt ∈ Rn×Bt has i.i.d. Gaussian entries. The proxy loss is
Lt(W) := NE [IIctYAt-CtWXAt - ctσtWGtlIF] = NIIY - WXkF + σ2kWIlF,
with ridge minimizer Wj = YXt(XXT + σ2N ∙ Idn×n)-1. Like with synthetic noise but unlike
noiseless SGD, the optima Wj converge to the minimal norm interpolant Wmin = YXT(XXT)+.
Theorem 6.2. Suppose σt2 → 0 is decreasing, ηt → 0, and for any C > 0 we have
∞∞
X(ηtσt2 - Cηt2) = ∞ and Xηt2σt2 < ∞.	(6.3)
t=0	t=0
Then we have Wt →p Wmin. If we further have ηt = Θ(t-x) and σt2 = Θ(t-y) with x, y > 0 and
0 < x + y < 1 < 2x + y, we have for any > 0 that
tmin{y,1 XlkWt- WmnkF →→ 0.
Theorem 6.2 provides an example where our framework can handle the composition of two augmen-
tations, namely additive noise and SGD. It reveals a qualitative difference between SGD with and
without additive noise. For polynomially decaying ηt the convergence of noiseless SGD in Theorem
6.1 is exponential in t, while the bound from Theorem 6.2 is polynomial in t. This is unavoidable. In-
deed, for components of Wt orthogonal to colspan(X), convergence requires that Pt∞=0 ηtσt2 = ∞
(see (4.3)). This occurs only if σt has power law decay, causing the || Wj - WminIIF to have at most
power law decay as well. Finally, the Monro-Robbins conditions (6.3) are more restrictive than the
analogous conditions in the pure noise setting (see (4.4)), as the latter allow for large ηt schedules
in which Pt∞=0 ηt2 diverges but Pt∞=0 ηt2σt2 does not.
7	Discussion
We have presented a theoretical framework to rigorously analyze the effect of data augmentation.
As can be seen in our main results, our framework applies to completely general augmentations and
relies only on analyzing the first few moments of the augmented dataset. This allows us to handle
augmentations as diverse as additive noise and mini-batch SGD as well as their composition in a
uniform manner. We have analyzed some representative examples in detail in this work, but many
other commonly used augmentations may be handled similarly: label-preserving transformations
(e.g. color jitter, geometric transformations), random projections (DeVries & Taylor, 2017; Park
et al., 2019), and Mixup (Zhang et al., 2017), among many others. Another line of investigation
left to future work is to compare different methods of combining augmentations such as mixing,
alternating, or composing, which often improve performance in the empirical literature (Hendrycks
et al., 2020).
Though our results provide a rigorous baseline to compare to more complex settings, the restriction
of the present work to linear models is of course a significant constraint. In future work, we hope to
extend our general analysis to models closer to those used in practice. Most importantly, we intend
to consider more complex models such as kernels (including the neural tangent kernel) and neural
networks by making similar connections to stochastic optimization. In an orthogonal direction,
our analysis currently focuses on the mean square loss for regression, and we aim to extend it to
other losses such as the cross-entropy loss. Finally, our study has thus far been restricted to the
effect of data augmentation on optimization, and it would be of interest to derive consequences
for generalization with more complex models. We hope our framework can provide the theoretical
underpinnings for a more principled understanding of the effect and practice of data augmentation.
9
Under review as a conference paper at ICLR 2021
References
Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with con-
vergence rate o (1/n). In Advances in neural information processing Systems, pp. 773-781, 2013.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 2020.
Chris M Bishop. Training with noise is equivalent to tikhonov regularization. Neural computation,
7(1):108-116, 1995.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. Siam Review, 60(2):223-311, 2018.
Olivier Chapelle, Jason Weston, Leon Bottou, and Vladimir Vapnik. Vicinal risk minimization.
In T. K. Leen, T. G. Dietterich, and V. Tresp (eds.), Advances in Neural Information Process-
ing Systems 13, pp. 416-422. MIT Press, 2001. URL http://papers.nips.cc/paper/
1876-vicinal-risk-minimization.pdf.
Shuxiao Chen, Edgar Dobriban, and Jane H Lee. Invariance reduces variance: Understanding data
augmentation in deep learning and beyond. stat, 1050:25, 2019.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 113-123, 2019.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De Sa, and Christopher Re. A
kernel theory of modern data augmentation. volume 97 of Proceedings of Machine Learn-
ing Research, pp. 1528-1537, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/dao19b.html.
Alexandre DefOSSez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and
optimal sampling distributions. In Artificial Intelligence and Statistics, pp. 205-213, 2015.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Yves Grandvalet and StePhane Canu. Noise injection for inputs relevance determination. In Ad-
vances in intelligent systems, pp. 378-382. IOS Press, 1997.
Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. AugMix: A simple data processing method to improve robustness and uncertainty.
Proceedings of the International Conference on Learning Representations (ICLR), 2020.
Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation:
Efficient learning of augmentation policy schedules. In International Conference on Machine
Learning, pp. 2731-2741, 2019.
De Huang, Jonathan Niles-Weed, Joel A. Tropp, and Rachel Ward. Matrix concentration for prod-
ucts, 2020.
Daniel LeJeune, Randall Balestriero, Hamid Javadi, and Richard G Baraniuk. Implicit rugosity
regularization via data augmentation. arXiv preprint arXiv:1905.11639, 2019.
Aitor Lewkowycz and Guy Gur-Ari. On the training dynamics of deep networks with l_2 regular-
ization. arXiv preprint arXiv:2006.08643, 2020.
10
Under review as a conference paper at ICLR 2021
Frederick Liu, Amir Najmi, and Mukund Sundararajan. The penalty imposed by ablated data aug-
mentation. arXiv preprint arXiv:2006.04769, 2020.
Siyuan Ma, Raef Bassily, and Mikhail Belkin. The power of interpolation: Understanding the effec-
tiveness of sgd in modern over-parametrized learning. In International Conference on Machine
Learning,pp. 3325-3334. PMLR, 2018.
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and
Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition.
Proc. Interspeech 2019, pp. 2613-2617, 2019.
Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris Papailiopoulos. Does
data augmentation lead to positive margin? arXiv preprint arXiv:1905.03177, 2019.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Statist., 22(3):
400-407, 09 1951. doi: 10.1214/aoms/1177729586. URL https://doi.org/10.1214/
aoms/1177729586.
PY Simard, D Steinkraus, and JC Platt. Best practices for convolutional neural networks applied
to visual document analysis. In Seventh International Conference on Document Analysis and
Recognition, 2003. Proceedings., volume 2, pp. 958-958, 2003.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. In International Conference on Learning Representations, 2018.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. In International Conference on Learning Representations, 2018.
Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. In
Advances in neural information processing systems, pp. 351-359, 2013.
Denny Wu and Ji Xu. On the optimal weighted l2 regularization in overparameterized linear regres-
sion. arXiv preprint arXiv:2006.05800, 2020.
Lei Wu, Chao Ma, and E Weinan. How sgd selects the global minima in over-parameterized learning:
A dynamical stability perspective. In Advances in Neural Information Processing Systems, pp.
8279-8288, 2018.
Sen Wu, Hongyang R. Zhang, Gregory Valiant, and Christopher Re. On the generalization effects
of linear transformations in data augmentation, 2020.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. arXiv preprint arXiv:1710.09412, 2017.
A Analytic lemmas
In this section, we present several basic lemmas concerning convergence for certain matrix-valued
recursions that will be needed to establish our main results. For clarity, we first collect some matrix
notations used in this section and throughout the paper.
A. 1 Matrix notations
Let M ∈ Rm×n be a matrix. We denote its Frobenius norm by kM kF and its spectral norm by
kMk2. Ifm = n so that M is square, we denote by diag(M) the diagonal matrix with diag(M)ii =
Mii. For matrices A, B, C of the appropriate shapes, define
A ◦ (B 0 C):= BAC	(A.1)
and
Var(A) := E[AT 0 A] - E[AT] 0 E[A].	(A.2)
Notice in particular that
Tr[Id ◦Var(A)] = E[kA - E[A]k2F].
11
Under review as a conference paper at ICLR 2021
A.2 One- and two-sided decay
Definition A.1. Let At ∈ Rn×n be a sequence of independent random non-negative definite matri-
ces with
sup ||At || ≤ 2 almost surely,
t
let Bt ∈ Rp×n be a sequence of arbitrary matrices, and let Ct ∈ Rn×n be a sequence of non-
negative definite matrices. We say that the sequence of matrices Xt ∈ Rp×n has one-sided decay of
type ({At}, {Bt}) if it satisfies
Xt+1 = Xt(Id -E[At]) + Bt.	(A.3)
We say that a sequence of non-negative definite matrices Zt ∈ Rn×n has two-sided decay of type
({At}, {Ct}) if it satisfies
Zt+1 = E[(Id -At)Zt(Id -At)] +Ct.	(A.4)
Intuitively, if a sequence of matrices Xt (resp. Zt) satisfies one decay of type ({At }, {Bt }) (resp.
two-sided decay of type ({At}, {Ct})), then in those directions u ∈ Rn for which ||Atu|| does not
decay too quickly in t we expect that Xt (resp. Zt) will converge to 0 provided Bt (resp. Ct) are
not too large. More formally, let us define
∞
Vk := \ ker
t=0
∞
Y(Id -E[As])
s=t
u ∈ Rn
T
lim Y(Id -E[As])u = 0,
s=t
∀t ≥ 1
,
and let Qk be the orthogonal projection onto Vk . It is on the space Vk that that we expect Xt , Zt to
tend to zero if they satisfy one or two-side decay, and the precise results follows.
A.3 Lemmas on Convergence for Matrices with One and Two-Sided Decay
We state here several results that underpin the proofs of our main results. We begin by giving in
Lemmas A.2 and A.3 two slight variations of the same simple argument that matrices with one or
two-sided decay converge to zero.
Lemma A.2. If a sequence {Xt} has one-sided decay of type ({At}, {Bt}) with
∞
X kBt kF < ∞,	(A.5)
t=0
then limt→∞ Xt Qk = 0.
Proof. For any e > 0, choose Ti so that P∞=t ∣∣Bt∣∣F < f and T2 So that for t > T2 We have
t
Y (Id -E[As]) Qk
s=T1
1
<--------------τy;-:-----
2	2 kXo kF + PT=-IkBSkF
By (A.3), We find that
t	tt
Xt+1 =X0Y(Id-E[As])+XBs Y (Id-E[Ar]),
s=0	s=0	r=s+1
Which implies for t > T2 that
t
kXt+1Qk kF ≤ kX0kF	Y(Id-E[As ]) Qk
s=0
+ Xt kBs kF	Yt	(Id-E[Ar])Qk .
2 s=0	r=s+1	2
(A.6)
Our assumption that ||At|| ≤ 2 almost surely implies that for any T ≤ t
t
Y(Id-E[As]) Qk
s=0
T
Y(Id-E[As]) Qk
s=0	2
≤
2
12
Under review as a conference paper at ICLR 2021
since each term in the product is non-negative-definite. Thus, we find
kXt+1Qk kF ≤
kX0kF +TX1-1kBskF	Yt (Id-E[As])Qk
s=0	s=T1
t
+ X kBskF<.
2	s=T1
Taking t → ∞ and then E → 0 implies that limt→∞ XtQk = 0, as desired.	□
Lemma A.3. If a sequence {Zt} has two-sided decay of type ({At}, {Ct}) with
lim E
T→∞
and
T
s=t
Qk = 0 for all t ≥ 0
(A.7)
∞
X Tr(Ct) < ∞,
t=0
(A.8)
then limt→∞ QkT ZtQk = 0.
Proof. The proof is essentially identical to that of Lemma A.2. That is, for E > 0, choose T1 so that
P∞=T Tr(Ct) < f and choose T by (A.7) so that for t > T2 We have
E [(" (Id-As))Qku
E	1
2Tr(Zo)+ PT=-I Tr(Cs)
Conjugating (A.4) by Qk , We have that
t	Tt
QkTZt+1Qk =E QkT Y (Id -As)	Z0 Y (Id -As) Qk
s=0	s=0
t	t	Tt
+ X E QkT	Y	(Id -Ar)	Cs	Y (Id -Ar) Qk
s=0	r=s+1	r=s+1
Our assumption that ||At|| ≤ 2 almost surely implies that for any T ≤ t
t
Y (Id -As) Q ≤
s=0	2
T
Y (Id -As) Q
s=0	2
For t > T2 , this implies by taking trace of both sides that
Tr(QkTZt+1Qk) ≤ Tr(Z0)E
t
Y (Id-As) Qk
s=0
t
+ X Tr(Cs)E
s=0
(Y! (Id-Ar ))Qk| 1
r=s+1	2
(A.9)
2
2
T1 -1
≤ Tr(Z0) + X Tr(Cs ) E
s=0
( Yt (Id-As))Qk 1 + Xt Tr(Cs)
s=T1	2 s=T1
< E,
which implies that limt→∞ QTZtQ ∣∣ = 0.	□
The preceding Lemmas Will be used to provide sufficient conditions for augmented gradient descent
to converge as in Theorem B.1 below. Since we are also interested in obtaining rates of convergence,
we record here two quantitative refinements of the Lemmas above that will be used in the proof of
Theorem B.4.
13
Under review as a conference paper at ICLR 2021
Lemma A.4. Suppose {Xt} has one-sided decay of type ({At}, {Bt}). Assume also that for some
X ≥ 0 and C > 0, we have
t
log	Y(Id -E[Ar]) Qk
< X - C Z t+1 r-αdr
2s
and kBt kF = O(t-β) for some 0 < α < 1 < β. Then, kXtQk kF = O (tα-β).
Proof. Denote γs,t := Rst r-αdr. By (A.6), we have for some constants C1, C2 > 0 that
t
kXt+1QkkF < C1e-Cγ1,t+1 + C2eX X(1 + s)-βe-Cγs+1,t+1
s=1
(A.10)
The first term on the right hand side is exponentially decaying in t since γ1,t+1 grows polynomially
in t. To bound the second term, observe that the function
f(s) := Cγs+1,t+1 - β log(s + 1)
satisfies
f0(s) ≥0	⇔	C(s + 1)-α
—
ɪ ≥ 0	⇔ s ≥ fβ V")=: K.
1+s	C
Hence, the summands are monotonically increasing for s greater than a fixed constant K depending
only on α, β, C . Note that
K
X(1 + s)-βe-Cγs+1,t+1 ≤ Ke-CγK+1,t+1 ≤ Ke-C0t1-α
s=1
for some C0 depending only on α and K, and hence sum is exponentially decaying in t. Further,
using an integral comparison, we find
tt
X (1 + s)-βe-cγs+1"+1 ≤ / (1 + S)-βe-TCaZt+D1-α-(S+I)IF)ds.
s=K+1	K
Changing variables using U = (1 + s)1-ɑ∕(1 - α),the last integral has the form
e-Cgt (1 -。产广 u-ξeCudu,	gχ := (ɪ^, ξ :=产
gK	1 - α	1 - α
Integrating by parts, we have
gt	gt
U u-ξeudu = C-1ξ / UTTeCUdu + (u-ξeCu)嚣
gK	gK
Further, since on the range gK ≤ U ≤ gt the integrand is increasing, we have
e-Cgtξ gt U-ξ-1eCudU ≤ ξgt-ξ.
gK
(A.11)
(A.12)
Hence, e-Cgt times the integral in (A.12) is bounded above by
O(g-ξ)+ e-Cgt (u-ξ eCu)lgK = O(g-ξ).
Using (A.11) and substituting the previous line into (A.12) yields the estimate
t
X (1 + s)-βe-Cγs+1,t+1 ≤ (1 +t)-β+α,
s=K+1
which completes the proof.
□
14
Under review as a conference paper at ICLR 2021
Lemma A.5. Suppose {Zt} has two-sided decay of type ({At}, {Ct}). Assume also that for some
X ≥ 0 and C > 0, we have
log E
(YtI (Id -Ar ))Q』[
r=s	2
<X-C
t+1
s
r-αdr
as well as Tr(Ct) = O(t-β) for some 0 < α < 1 < β. Then Tr(QkTZtQk ) = O(tα-β).
Proof. This argument is identical to the proof of Lemma A.4. Indeed, using (A.9) we have that
Tr (QkTZtQk ≤ C1e-Cγ1,t+1 + C2eX X(1 + s)-βe-Cγs+1,t+1.
s=1
The right hand side of this inequality coincides with the expression on the right hand side of (A.10),
which We already bounded by O(tβ-α) in the proof of Lemma A.4.	□
In what follows, we will use a concentration result for products of matrices from Huang et al. (2020).
Let Y1 , . . . , Yn ∈ RN ×N be independent random matrices. Suppose that
kE[Yi]k2 ≤ai	and E kYi -E[Yi]k22 ≤ bi2ai2
for some a1 , . . . , an and b1 , . . . , bn . We will use the following result, which is a specialization of
(Huang et al., 2020, Theorem 5.1) forp = q = 2.
Theorem A.6 ((Huang et al., 2020, Theorem 5.1)). For Z0 ∈ RN×n, the product Zn =
YnYn-I •…YiZo satisfies
n
E[kZnk2] ≤ ePn=ι b2 ∏ a2 ∙∣∣Zok2
i=i
E [kZn - E[Zn]k2] ≤ (ePn=ι b2 - l)a2 TZl/
Finally, we collect two simple analytic lemmas for later use.
Lemma A.7. For any matrix M ∈ Rm×n, we have that
E[kMk22] ≥ kE[M]k22.
Proof. We find by Cauchy-Schwartz and the convexity of the spectral norm that
E[kMk2] ≥ E[kMk2]2 ≥ kE[M]k2.	□
Lemma A.8. For bounded at ≥ 0, if we have	t∞=0 at = ∞, then for any C > 0 we have
∞
X ate-C Ps=0 as < ∞.
t=0
Proof. Define bt := Pts=0 as so that
∞
S:=Xate-CPts=0as
t=0
X(bt - bt-i)e-Cbt ≤ Z	e-Cxdx < ∞,
t=0	0
where we use f∞ e-Cxdx to upper bound its right Riemann sum.	□
B Analysis of data augmentation as stochastic optimization
In this section, we prove generalizations of our main theoretical results Theorems 5.1 and 5.2 giving
Monro-Robbins type conditions for convergence and rates for augmented gradient descent in the
linear setting.
15
Under review as a conference paper at ICLR 2021
B.1 Monro-Robbins type results
To state our general Monro-Robbins type convergence results, let us briefly recall the notation. We
consider overparameterized linear regression with loss
1
L(W;D) = NN I∣wx - YIlF,
where the dataset D of size N consists of data matrices X, Y that each have N columns xi ∈
Rn, yi ∈ Rp with n > N. We optimize L(W; D) by augmented gradient descent, which means that
at each time t we replace D = (X, Y) by a random dataset Dt = (Xt, Yt). We then take a step
Wt+1 = Wt- ηtVwL(Wt; Dt)
of gradient descent on the resulting randomly augmented loss L(W; Dt) with learning rate ηt. Recall
that we set
Vk := column span of E[XtXtT]
and denoted by Qk the orthogonal projection onto Vk . As noted in §5, on Vk the proxy loss
Lt = E [L(W; Dt)]
is strictly convex and has a unique minimum, which is
W*= E[YXT ] (Q∣∣E [XtXT] Q∣∣)-1.
The change from one step of augmented GD to the next in these proxy optima is captured by
Ξt* := Wt*+1 - Wt*.
With this notation, we are ready to state Theorems B.1, which gives two different sets of time-
varying Monro-Robbins type conditions under which the optimization trajectory Wt converges for
large t. In Theorem B.4, we refine the analysis to additionally give rates of convergence.
Theorem B.1. Suppose that Vk is independent of t, that the learning rate satisfies ηt → 0, that the
proxy optima satisfy
∞
XkΞt*kF <∞,	(B.1)
t=0
ensuring the existence of a limit W∞* := limt→∞ Wt* and that
∞
X ηtλmin,Vk (E[XtXtT]) = ∞.	(B.2)
t=0
Then if either
∞
Xηt2E[kXtXtT-E[XtXtT]k2F+kYtXtT-E[YtXtT]k2F] <∞	(B.3)
t=0
or
∞
Xηt2E kXtXtT-E[XtXtT]k2F
t=0
+ E[Wt](XtXtT - E[XtXtT]) - (YtXtT - E[YtXtT])2 i <∞
(B.4)
hold, then for any initialization W0, we have WtQk →p W∞* .
Remark B.2. In the general case, the column span V|| ofE[XtXtT] may vary with t. This means that
some directions in Rn may only have non-zero overlap with colspan(E[XtXtT]) for some positive
but finite collection of values of t. In this case, only finitely many steps of the optimization would
move Wt in this direction, meaning that we must define a smaller space for convergence. The correct
definition of this subspace turns out to be the following
∞∞	2
Vkl ：= \ ker Y (Id -弋E[XsX∑f)
t=0	s=t
(B.5)
∞
\ u∈Rn
t=0
T→∞ Y(Id-τNsEXSXT])u = 0].
s=t
16
Under review as a conference paper at ICLR 2021
With this re-definition of V|| and with Qk still denoting the orthogonal projection to Vk, Theorem
B.1 holds verbatim and with the same proof. Note that if ηt → 0, V||colspan(E[XtXtT]) is fixed in t,
and (B.2) holds, this definition of Vk reduces to that defined in (5.3).
Remark B.3. The condition (B.4) can be written in a more conceptual way as
∞
X kXtXtT - E[XtXtT]k2F + ηt2 Tr Id ◦Var (E[Wt]Xt-Yt)XtT	<∞,
t=0
where we recognize that (E[Wt]Xt - Yt)XtT is precisely the stochastic gradient estimate at time
t for the proxy loss Lt, evaluated at E [Wt], which is the location at time t for vanilla GD on Lt
since taking expectations in the GD update equation (5.1) coincides with GD for Lt. Moreover,
condition (B.4) actually implies condition (B.3) (see (B.12) below). The reason we state Theorem
B.1 with both conditions, however, is that (B.4) makes explicit reference to the average E [Wt] of
the augmented trajectory. Thus, when applying Theorem B.1 with this weaker condition, one must
separately estimate the behavior of this quantity.
Theorem B.1 gave conditions on joint learning rate and data augmentation schedules under which
augmented optimization is guaranteed to converge. Our next result proves rates for this convergence.
Theorem B.4. Suppose that ηt → 0 and that for some 0 < α < 1 < β1 , β2 and C1 , C2 > 0, we
have
log E
(Y (Id - 2Nr Xr XT))QkI
r=s	2
< C1 - C2 Z t+1 r-αdr
s
(B.6)
as well as
k = k F =OL)
(B.7)
and
ηt2 Tr hId ◦Var(E[Wt]XtXtT -YtXtT)i =O(t-β2).	(B.8)
Then, for any initialization W0, we have for any > 0 that
tmin{βl-1,β2-α IkWtQk- W∞ kF → 0.
Remark B.5. To reduce Theorem 5.2 to Theorem B.4, we notice that (5.9) and (5.10) mean that
Theorem A.6 applies to Yt = Id -2ηtXX with at = 1 一 Ω(t-α) and and b2 =O(t-γ), thus
implying (B.6).
The first step in proving both Theorem B.1 and Theorem B.4 is to obtain recursions for the mean
and variance of the difference Wt — Wj between the time t proxy optimum and the augmented
optimization trajectory at time t. We will then complete the proof of Theorem B.1 in §B.3 and the
proof of Theorem B.4 in §B.4.
B.2 Recursion relations for parameter moments
The following proposition shows that difference between the mean augmented dynamics E[Wt]
and the time-t optimum Wj satisfies, in the sense of Definition A.1, one-sided decay of type
({At}, {Bt}) with
At = 2Nt XtXT	Bt = T
It also shows that the variance of this difference, which is non-negative definite, satisfies two-sided
decay of type ({At }, {Ct }) with At as before and
Ct = N hId oVar(E[Wt]XtXj - YXtr)].
In terms of the notations of Appendix A.1, we have the following recursions.
17
Under review as a conference paper at ICLR 2021
Proposition B.6. The quantity E[Wt] - Wj satisfies
E[Wt+ι] — Wt+ι = (E[Wt]- W；)(Id -2NtEXtXT]) — 己；	(B.9)
and Zt := E[(Wt -E[Wt])T(Wt - E[Wt])] satisfies
Zt+ι = E (Id -~NtXtXT)Zt(Id --NNtXtXT) + N hId◦Var(E[wt]XtXT - YtXT)i ∙
(B.10)
Proof. Notice that E[XtXtT]u = 0 if and only if XtTu = 0 almost surely, which implies that
Wt*E[XtXj] = E[YtXT]E[XtXT]+E[Xt X」=E[匕 X1]∙
Thus, the learning dynamics (5.1) yield
E[Wt+ι] = E[Wt] - 2Nt(E[Wt]E[XtXT] - E[YtXT])
=E[Wt] - 2Nt (E[Wt] - Wt*)E[XtXj]∙
Subtracting Wt+ι from both sides yields (B.9). We now analyze the fluctuations. Writing
Sym(A) := A + AT, we have
E[Wt+ι]TE[Wt+ι] = E[Wt]TE[Wt] + 2NSym(E[Wt]TE[YtXT] - E[Wt]TE[Wt]EXtXj])
+ 4η2 (E[XtXT]E[Wt]TE[Wt]E[XtXT]+E[XtYT]E[YtXT]-Sym(EXtXT]E[Wt]TE[YtXT]))∙
Similarly, we have that
E[Wt+ι Wt+ι] = E[WTWt] + 2NtSym(E[WjYtXT - WJWtXtXT])
+ NlEXtXTWtrWtXtXT -Sym(XtXtrWtrYiXT) + XtYtTYtXT].
Noting that Xt and Yt are independent of Wt and subtracting yields the desired.	□
B.3 Proof of Theorem B.1
First, by Proposition B.6, we see that E[Wt] - Wj has one-sided decay with
At = 2ηt XtNt	and Bt = -E；.
Thus, by Lemma A.2 and (B.1), we find that
lim (E[Wt]Qk - Wt；) = 0,	(B.11)
which gives convergence in expectation.
For the second moment, by Proposition B.6, we see that Zt has two-sided decay with
At = 2ηt XNXr	and	Ct = N2 [id oVar(E[Wt]XtXT - YtXT) ] ∙
We now verify (A.7) and (A.8) in order to apply Lemma A.3.
For (A.7), for any > 0, notice that
E[kAs - E[As]k2F] = ηs2E[kXsXsT-E[XsXsT]k2F]
so by either (B.3) or (B.4) we may choose Ti > t so that P∞=t E[∣∣As - E[As]kF] < f. Now
choose T2 > T1 so that for T > T2, we have
2
1
<∙F,..
0	2 k QT=-1 E[Id-As]kF + PT=-1 E[kAs - E[As]kF]
T
Y E[Id -Ar] Qk
r=T1
18
Under review as a conference paper at ICLR 2021
For T > T2, we then have
E U(YY(Id-As))Qk
2
≤
2
2
+
≤
≤
2
( YT E[Id -As])
( YT E[Id -As])
T1 -1
Y E[Id -As]
s=t
T1 -1
Ts
Qk
Qk
2
+
F
T
E (Id -Ar) (Id -E[Ar])Qk	- Y (Id -Ar) Y (Id -E[Ar])Qk
s=t	r=t	r=s+1
T	s-1
r=t	r=s
2
E (Id-Ar)(As-E[As])	(Id -E[Ar])Qk
s=t r=t
2 T
r=s+1
T
F ( Y E[Id -Ar])Qk	+ X E[kAs - E[As]k2F] ( Y	E[Id -Ar])Qk
Y E[Id -As]
r=T1
T1-1
2	s=t
T
X E[kAs -E[As]k2F]	Y E[Id-Ar] Qk
r=s+1
2T
+ X E[kAs - E[As]k2F]
s=t
r=T1
2	s=T1
T
F
T
F
T
2
F
2
2
2
2
+
F
< ,
which implies (A.7). Condition (A.8) follows from either (B.4) or (B.3) and the bounds
8η2
Tr(Ct) ≤ N (kE[Wt](XtXT - E[XtXT])kF + kKXj — E[YtXT]kF)	(B.12)
8η2
≤ N (kE[Wt]k2kXtXj - E[XtXj]kF + kYtχT -E[γtχT]kF),
where in the first inequality we use the fact that kM1 -M2 k2F ≤ 2(kM1k2F+kM2 k2F). Furthermore,
iterating (B.9) yields ∣∣E[Wt]-Wt*kF ≤ ∣∣W0-W司IF+Pt=o ∣∣Ξ"∣f , which combined with (B.12)
and either (B.3) or (B.4) therefore implies (A.8). We conclude by Lemma A.3 that
lim QkTZtQk = lim E[QkT(Wt -E[Wt])T(Wt -E[Wt])Qk] =0.	(B.13)
Together, (B.11) and (B.13) imply that WtQk - Wt* →→ 0∙ The conclusion then follows from the
fact that limt→o W* = W∞. This complete the proof of Theorem B.1.	□
B.4 Proof of Theorem B.4
By Proposition B.6, E[Wt] - Wt* has one-sided decay with
At =? XtXl,
Bt
Ξ*
t.
By Lemma A.7 and (B.6), E[At] satisfies
t	1
log ∏ (Id -2ηr NE[XrXl]) Qk
≤ 2iogE ∣(∏ (Id-2ηrXXT))Qk∣
2	r=s	2
t+1
r-αdr.
Applying Lemma A.4 using this bound and (B.7), we find that
∣E[Wt]Qk -Wt*∣F =O(tα-β1).
Moreover, because ∣Ξt* ∣F = O(t-β1), we also find that ∣Wt* - W∞* ∣F = O(t-β1+1), and hence
∣E[Wt]Qk - W∞* ∣F = O(t-β1+1).
Further, by Proposition B.6, E[(Wt - E[Wt])T(Wt - E[Wt])] has two-sided decay with
At = NXXtXT,	Ct = Nt hId◦Var(E[Wt]XtXT - YtXl)i.
19
Under review as a conference paper at ICLR 2021
Applying Lemma A.5 with (B.6) and (B.8), we find that
E k(Wt - E[Wt])Qkk2F =O(tα-β2).
By Chebyshev’s inequality, for any x > 0 we have
p(∣∣WtQk - W∞kF ≥ O(t-β1+1) + X ∙ O(tα-2β2)) ≤ x-2.
For any > 0, choosing x = tδ for small 0 < δ < we find as desired that
tmin{βl-1,β2-αIkWtQk- W∞kF →→ 0,
thus completing the proof of Theorem B.4.
C Analysis of Noising Augmentations
In this section, we give a full analysis of the noising augmentations presented in Section 4. Let us
briefly recall the notation. As before, we consider overparameterized linear regression with loss
1
L(W;D) = NN ||WX - Y||F ,
where the dataset D of size N consists of data matrices X, Y that each have N columns xi ∈
Rn, yi ∈ Rp with n > N. We optimize L(W; D) by augmented gradient descent with additive
Gaussian noise, which means that at each time t we replace D = (X, Y) by a random dataset
Dt = (Xt, Y), where the columns xi,t of Xt are
Xi,t = Xi + σtGi,	Gi 〜N(0,1) i.i.d.
We then take a step
Wt+1 = Wt- ηtVwL(Wt; Dt)
of gradient descent on the resulting randomly augmented loss L(W; Dt) with learning rate ηt. A
direct computation shows that the proxy loss
Lt = E [L(W; Dt)] = L(W;D) + σtN ||W∣∣F ,
which is strictly convex. Thus, the space
Vk := column span of E[XtXtT]
is simply all of Rn . Moreover, the proxy loss has a unique minimum, which is
W; = YX T (σtN Idn×n +XX T )-1.
C.1 Proof of Theorem 4.1
We first show convergence. For this, we seek to show that if σt2 , ηt → 0 with σt2 non-increasing and
∞∞
X ηtσt2 = ∞ and	X ηt2σt2 < ∞,	(C.1)
then, Wt →p Wmin. We will do this by applying Theorem 5.1, so we check that our assumptions
imply the hypotheses of these theorems. For Theorem 5.1, we directly compute
E[YtXT] = YXT	and	EXtXT] = XXT + σtN ∙ Idn×n
and
E[XtXtTXt] = XXTX +σt2(N+n+ 1)X
E[XtXtTXtXtT] = XXTXXT + σt2 (2N + n + 2)XXT + Tr(XXT) Idn×n ) +σt4N(N+n+1)Idn×n
20
Under review as a conference paper at ICLR 2021
We also find that
|印|尸=∣σ2 - σ2+∕N YXT (XXT + σ↑N ∙ Idn×n ) -1 (XXT + σ^N ∙ Idn×n ) —1
≤∣σ2-σ2+∕N k YXT [(XXT)+]2 ]|尸.
Thus, because σt2 is decreasing, we see that the hypothesis (5.5) of Theorem 5.1 indeed holds.
Further, we note that
∞
Xηt2EkXtXtT-E[XtXtT]k2F+kYtXtT-E[YtXtT]k2F
t=0
∞∞
= Xηt2σt2 2(n+ 1)kXk2F + NkYk2F +σt2Nn(n+ 1) = O Xηt2σt2
t=0	t=0
which by (C.1) implies (B.3). Theorem 5.1 and the fact that limt→∞ Wj = Wmin therefore yield
that Wt →p Wmin.
For the rate of convergence, we aim to show that if ηt = Θ(t-x) and σt2 = Θ(t-y) with x, y > 0,
x + y < 1, and 2x + y > 1, then for any > 0, we have that
tmin{β,1 α}-ekWt - WminkF →→ 0.
X XT
We now check the hypotheses for and apply Theorem B.4. For (B.6), notice that Yr = Id -2小 N r
satisfies the hypotheses of Theorem A.6 with ar = 1 - 2ηrσr2 and br = ηaσr(2(n + 1)∣IXkF +
σr2 N n(n + 1) . Thus, by Theorem A.6 and the fact that ηt = Θ(t-x) and σt2 = Θ(t-y), we find
for some C1 , C2 > 0 that
t
log E	Y(Id -2ηr
r=s
XfT )|2] ≤ XX br+2 XX log(1-2ηrσ2)
2	r=s	r=s
≤ C1 - C2 Z t+1 r-x-ydr.
s
For (B.7), we find that
k守If ≤ ∣σ2 -σ2+1∣N∣YXT[(XXt)+]2∣f = OD
Finally, for (B.8), we find that
ηt Tr [idoVar(E[Wt]XtXj - YXT)] = O(t-2x-y).
Noting finally that kWt 一 WminIIf = O(σ2) = O(t-y), we apply Theorem B.4 with α = X + y,
β1 = y + 1, and β2 = 2x + y to obtain the desired estimates. This concludes the proof of Theorem
4.1.
D	Analysis of SGD
This section gives the full analysis of the results for stochastic gradient descent with and without
additive synthetic noise presented in Sections 6.1 and 6.2. Let us briefly recall the notation. As
before, we consider overparameterized linear regression with loss
1
L(W;D) = N ||WX - Y||F ,
where the dataset D of size N consists of data matrices X, Y that each have N columns xi ∈
Rn, yi ∈ Rp with n > N. We optimize L(W; D) by augmented SGD either with or without additive
Gaussian noise. In the former case, this means that at each time t we replace D = (X, Y) by a
random batch Bt = (Xt, Yt) given by a prescribed batch size Bt = |Bt| in which each datapoint
21
Under review as a conference paper at ICLR 2021
in Bt is chosen uniformly with replacement from D, and the resulting data matrices Xt and 匕
are scaled so that Lt(W) = L(W; D). Concretely, this means that for the normalizing factor
Ct =，N/Bt We have
Xt = ctXAt and Yt = ctYAt,	(D.1)
where At ∈ RN×Bt has i.i.d. columns At,i with a single non-zero entry equal to 1 chosen uniformly
at random. In this setting the minimum norm optimum for each t are the same and given by
Wj = W∞ = YX T(XX T)+,
which coincides with the minimum norm optimum for the unaugmented loss.
In the setting of SGD with additive noise at level σt, we take instead
Xt =ct(XAt+σtGt)	and	Yt =ctYAt,
where ct and At are as before and Gt ∈ Rn×Bt has i.i.d. Gaussian entries. In this setting, the proxy
loss is
Lt(W) := N E [kct YAt- CtWXAt- ctσtW GtkF ] = NIIY - WXkF + σ2k W IlF,
which has ridge minimizer Wj = YXT(XXT + σtN ∙ Idn×n)-1.
We begin in §D.1 by treating the case of noiseless SGD. We then do the analysis in the presence of
noise in §D.2.
D.1 Proof of Theorem 6.1
In order to apply Theorems B.1 and B.4, we begin by computing the moments of At as follows.
Recall the notation diag(M) from Appendix A.1.
Lemma D.1. For any Z ∈ RN×N, we have that
E[AtAT] = B IdN×n	and	E[AtATZAtAT] = B diag(Z) + Bt(Bt2- 1) Z
N	N	N2
Proof. We have that
Bt	B
E[AtAT] = ∑E[Ai,tATt] = N IdN×n .
i=1
Similarly, we find that
Bt
E[AtAtTZAtAtT] = X E[Ai,tAiT,tZAj,tAjT,t]
i,j=1
Bt
= XE[Ai,tAiT,tZAi,tAiT,t] +2 X	E[Ai,tAiT,tZAj,tAjT,t]
i=1	1≤i<j≤Bt
=N mg(Z) + B⅛1∙Z，
which completes the proof.	□
Let us first check convergence in mean:
E[Wt]Qk → W∞.
To see this, note that Lemma D.1 implies
E[YtXtT] = YXT	E[XtXtT] = XXT，
which yields that
Wj = YX T [XX T]+ = W∞	(D.2)
22
Under review as a conference paper at ICLR 2021
for all t. We now prove convergence. Since all Wj are equal to W∞, We find that ΞJ= = 0. By (B.9)
and Lemma D.1 we have
E[Wt+ι] — W∞ = (E[Wt] — W∞)(Id - 2Nt XX T),
which implies since 等 < λmax(XXT)-1 for large t that for some C > 0 we have
IIE[Wt]Qk — w∞IIf ≤ IIWoQk - w∞IIf Y Qk- N-s-xxT
s=0	2
t-1 2
≤ C∣∣W0Qk — W∞ kF exp ( — X 'Nλmin,V∣∣ (XXT)) ∙ (D3
-=0 N
From this we readily conclude using (6.1) the desired convergence in mean E[Wt]Qk → W∞= .
Let us now prove that the variance tends to zero. By Proposition B.6, we find that Zt = E[(Wt —
E[Wt])T(Wt — E[Wt])] has two-sided decay of type ({At}, {Ct}) with
At =平XtXT,	Ct = 42 [Id°Var((E[Wt]Xt — 匕)Xj)].
To understand the resulting rating of convergence, let us first obtain a bound on Tr(Ct). To do this,
note that for any matrix A, we have
Tr (Id ◦Var[A]) = Tr (E [ATA —E[A]TE[A]) .
Moreover, using the definition (D.1) of the matrix At and writing
Mt :=E[Wt]X—Y,
we find
((E [Wt] Xt — K)XT)T (E [Wt] Xt — K)XT = XAtATMTMt AtATXT
as well as
E [((E[Wt]Xt — Yt)XT)]TE [(E[Wt]Xt — Yt)Xj] = XE [AtAT] mJMjE [AtAT] XT.
Hence, using the expression from Lemma D.1 for the moments ofAt and recalling the scaling factor
ct = (N/Bt)1/2, we find
Tr(Ct) = B Tr (X {m(MjMt) — 44MJ"t} XT) ∙
Next, writing
∆t := E[Wt] — W∞=
and recalling (D.2), we see that
Mt = ∆tX.
Thus, applying the estimates (D.3) about exponential convergence of the mean, we obtain
IY(Ct) ≤ 沿 dQ∣ 川 2∣∣χχT∣∣2
≤ C^nt I IXXTll2 Hδ0QkkFeXP ( - X ~4λmin,V∣∣ (XXT)) ∙ (D.4)
Bt	=0 N
Notice now that Yr = Qk — Ar satisfies the conditions of Theorem A.6 with ar = 1 —
2nr寺λmin,vk (XXT) and br = B4arN Tr(Xdiag(XTX)X — NXXTXXT). By Theorem A.6
we then obtain for any t > s > 0 that
E
Y (Qk- Ar )|2] ≤ ePr=s + ι b2 Y (1 - 2nr 4 λlmn,% (XX T)):
r=-+1	2	r=-+1
(D.5)
23
Under review as a conference paper at ICLR 2021
By two-sided decay of Zt, we find by (D.4), (D.5), and (A.9) that
E[kWtQk -E[Wt]Qkk2F] =Tr(QkZtQk)
≤ e-N λmin,Vk(XX T) Ps=0 ηs "弋："2 ∣∣∆oQkkF C X 8η2f-e 4Ns λmin,Vk(XXT)+Pr=s+1 b2. (D.6)
N	s=0 Bs /N
Since ηs → 0, We find that 小替e~NNλmin,Vk(XX ) is uniformly bounded and that b2 ≤
Nλmin,vkl (XXτ)ηr for SUfficiently large r. We therefore find that for some C0 > 0,
t-1
E[∣WtQk - E[Wt]QkkF] ≤ C0 Xηse-告(XXT) Pr=0 nr,
s=0
hence limt→∞ E[∣WtQk - E[Wt]Qk ∣2F] = 0 by Lemma A.8. Combined With the fact that
E[Wt]Qk → W∞, this implies that WtQk → W∞∙
To obtain a rate of convergence, observe that by (D.3) and the fact that ηt = Θ(t-x), for some
C1 , C2 > 0 We have
∣E[Wt]Qk - W∞kF ≤ Cl exp (- C2t1-x).	(D.7)
Similarly, by (D.6) and the fact that BnsN < ∞ uniformly, for some C3,C4,C5 > 0 we have
E[kWtQk -E[Wt]Qkk2F] ≤ C3 exp ( - C4t1-xt1-x
We conclude by Chebyshev’s inequality that for any a > 0 we have
Ρ(∣WtQk - w∞kF ≥ Cl exp ( — C2tl-x) + a ∙ PC3t2-2e-C4t 1-x/2) ≤ a-2.
Taking a = t, we conclude as desired that for some C > 0, we have
ect1-xkWtQk - W∞ ∣F → 0.
This completes the proof of Theorem 6.1.
D.2 Proof of Theorem 6.2
We now complete our analysis of SGD with Gaussian noise. We will directly check that the op-
timization trajectory Wt converges at large t to the minimal norm interpolant W∞ with the rates
claimed in Theorem 6.2. We will deduce this from Theorem B.4. To check the hypotheses of this
theorem, we will need expressions for its moments, which we record in the following lemma.
Lemma D.2. We have
E[YtXtT] = YXT	and	E[XtXtT] = XXT + σt2N Idn×n .	(D.8)
Moreover,
E[YtXtTXtYtT] = ct4E[Y AtAtTXTXAtAtTY T +σt2YAtGtTGtAtTYT]
=NY diag(XτX)YT + Bt ~ 1 YXτXYT + σ^ΝYYT
Bt	Bt
E[YtXtTXtXtT] = ct4E[YAtAtTXTXAtAtTXT + σt2YAtGtTGtAtTXT
+ σt2YAtGtTXAtGtT + σt2YAtAtTXTGtGtT]
=NY diag(X tX )XT + B-■ YXTXXT + σ2(Ν + n+1)YXT
Bt	Bt	Bt/N
E[XtXtTXtXtT] = ct4E[XAtAtTXTXAtAtTXT + σt2GtGtTXAtAtTXT + σt2XAtGtTGtAtTXT
+ σt2XAtAtTXTGtGtT + σt2GtAtTXTGtAtTXT + σt2XAtGtTXAtGtT
+ σt2GtAtTXTXAtGtT + σt4GtGtTGtGtT]
=NX diag(X τX )XT + B- XX τXXT + σ2(2Ν + n+2)XXT
Bt	Bt	Bt/N
+ σ2 百 Tr(XXT)Idn×n +σ4N(N +	) ldn×n .
Bt	Bt /N
24
Under review as a conference paper at ICLR 2021
Proof. All these formulas are obtained by direct, if slightly tedious, computation.	□
With these expressions in hand, we can readily check the of conditions Theorem B.4. First, we find
using the Sherman-Morrison-Woodbury matrix inversion formula that
kΞ"∣F = ∖σtN - σ2+ιN| ∣∣YXT(XXT + σ*N ∙ Idn×n)-1(XXT + 登+iN ∙ Idn×n)-1∣∣F
(D.9)
≤ N∣σ2-σ2+∕∣∣YXT[(XXt)+]2∣f .
Hence, assuming that σt2 = Θ(t-y), we see that condition (B.7) of Theorem B.4 holds with
β1 = -y - 1.
Next, let us verify that the condition (B.6) holds for an appropriate α. For this, we need to bound
which we will do using Theorem A.6. In order to apply this result, we find by direct inspection of
the formula
E[XrXrT] =XXT+σr2NIdn×n
that
Moreover, we have
Id - -N Xr XTp 2 =
1 - 2rσr2
ar.
E
E "Id - 2Nr Xr XT - E [Id - 2Nr Xr XT]||2 =N2 E h∣∣Xr XT - E [Xr 期|白.
Using the exact expressions for the resulting moments from Lemma D.2, we find
N2 E hllXr XT- E [Xr Wil2]
—4n [ 1 T (XN dia t√X TX X TXX T、, 2τ 2 n + 1 T (XX T∖+ “4 Nn(n +1)
=N [瓦 Ir (X(N dlag(X X) — X X)X 户 2σt B/N Ir(XX ) + σt Bt/N
≤ Cr2 .
Thus, applying Theorem A.6, we find that
log E
YY (Id - 2NrrXrXT) ∣∣2 ≤ XXCnr log (YtI (1 — 2ηrσr)) ≤ XXC宿- 讯成.
r=s	ll2 r=s	r=s	r=s
Recall that, in the notation of Theorem 6.2, we have
r = Θ(r-x),	σr2 = Θ(r -y).
Hence, since under out hypotheses we have x < 2y, we conclude that condition (B.6) holds with
α = x + y. Moreover, exactly as in Proposition B.6, we have
∆t+ι = ∆t (Id -2NtE [XtXT]) + NM,	∆t := E [Wt - Wt*].
Since
I∣ξ*∣∣f = O(t-y-1)
and we already saw that
Id - ^NtE [XtXT]	= 1 - 2ntσ2,
we may use the single sided decay estimates Lemma A.4 to conclude that
I∆tl∣F = O(tx-1).
25
Under review as a conference paper at ICLR 2021
Finally, it remains to bound
ηt Tr [id oVar(E[Wt]XtXT -匕X1)].
A direct computation using Lemma D.2 shows
EkYtXtT -E[YtXtT]k2F
=BT (Y(N diag(X tX ) - X TX )Y t) + σ2N Tr(YY t).
Hence, again using D.2, we find
ηt Tr [Id °Var(E[Wt]XtXT -匕X~T)]
=ηt Tr (ɪE[Wt]X(N diag(XTX) - XtX)XtE[Wt]t
Bt
+ 2σ2 n+1 E[Wt]XXTE[Wt]T + (σ2 N Tr(XXt)+ σ4Nn+1)E[Wt]E[Wt]T)
Bt/N	Bt	Bt/N
-2褚 Tr (ɪY(N diag(XtX) - XtX)XτE[Wt]τ + ^ ^+1 YXτE[Wt]τ)
Bt	Bt/N
+ ηt Tr ( ɪ Y (N diag(X τX) - X τX )YT + σ2NYYτ).
To make sense of this term, note that
W∞ X = Y.
Hence, we find after some rearrangement that
η Tr [IdoVar(E[Wt]XtXj - YXT)] ≤。褚定 + ∣∣∆t∣∣F),
where we set
∆t := E [Wt - W∞ ].
Finally, we have
∆t ≤ ∆t + ||W； - W∞IIf = O(tx-1) + Θ(t-y) = Θ(t-y)
since we assumed that x + y < 1. Therefore, we obtain
η2 Tr [Id oVar(E[Wt]XtXT - YXT)] ≤。褚。2 = Θ(t-2x-y),
showing that condition (B.8) holds with β2 = 2x + y. Applying Theorem B.4 completes the proof.
26