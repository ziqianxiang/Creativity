Under review as a conference paper at ICLR 2021
Sparse Linear Networks with a Fixed Butter-
fly Structure: Theory and Practice
Anonymous authors
Paper under double-blind review
Ab stract
A butterfly network consists of logarithmically many layers, each with a lin-
ear number of non-zero weights (pre-specified). The fast Johnson-Lindenstrauss
transform (FJLT) can be represented as a butterfly network followed by a projec-
tion onto a random subset of the coordinates. Moreover, a random matrix based
on FJLT with high probability approximates the action of any matrix on a vec-
tor. Motivated by these facts, we propose to replace a dense linear layer in any
neural network by an architecture based on the butterfly network. The proposed
architecture significantly improves upon the quadratic number of weights required
in a standard dense layer to nearly linear with little compromise in expressibility
of the resulting operator. In a collection of wide variety of experiments, includ-
ing supervised prediction on both the NLP and vision data, we show that this
not only produces results that match and often outperform existing well-known
architectures, but it also offers faster training and prediction in deployment. To
understand the optimization problems posed by neural networks with a butterfly
network, we study the optimization landscape of the encoder-decoder network,
where the encoder is replaced by a butterfly network followed by a dense linear
layer in smaller dimension. Theoretical result presented in the paper explain why
the training speed and outcome are not compromised by our proposed approach.
Empirically we demonstrate that the network performs as well as the encoder-
decoder network.
1	Introduction
A butterfly network (see Figure 6 in Appendix A) is a layered graph connecting a layer ofn inputs to
a layer ofn outputs with O(log n) layers, where each layer contains 2n edges. The edges connecting
adjacent layers are organized in disjoint gadgets, each gadget connecting a pair of nodes in one layer
with a corresponding pair in the next layer by a complete graph. The distance between pairs doubles
from layer to layer. This network structure represents the execution graph of the Fast Fourier Trans-
form (FFT) (Cooley and Tukey, 1965), Walsh-Hadamard transform, and many important transforms
in signal processing that are known to have fast algorithms to compute matrix-vector products.
Ailon and Chazelle (2009) showed how to use the Fourier (or Hadamard) transform to perform
fast Euclidean dimensionality reduction with Johnson and Lindenstrauss (1984) guarantees. The
resulting transformation, called Fast Johnson Lindenstrauss Transform (FJLT), was improved in
subsequent works (Ailon and Liberty, 2009; Krahmer and Ward, 2011). The common theme in
this line of work is to define a fast randomized linear transformation that is composed of a random
diagonal matrix, followed by a dense orthogonal transformation which can be represented via a
butterfly network, followed by a random projection onto a subset of the coordinates (this research is
still active, see e.g. Jain et al. (2020)). In particular, an FJLT matrix can be represented (explicitly)
by a butterfly network followed by projection onto a random subset of coordinates (a truncation
operator). We refer to such a representation as a truncated butterfly network (see Section 4).
Simple Johnson-Lindenstrauss like arguments show that with high probability for any W ∈ Rn2×n1
and any x ∈ Rn1, Wx is close to (J2TJ2)W(J1TJ1)x where J1 ∈ Rk1 ×n1 and J2 ∈ Rk2×n2
are both FJLT, and k1 = log n1 , k2 = log n2 (see Section 4.2 for details). Motivated by this, we
propose to replace a dense (fully-connected) linear layer of size n2 × n1 in any neural network
by the following architecture: J1TW0J2, where J1, J2 can be represented by a truncated butterfly
1
Under review as a conference paper at ICLR 2021
network and W 0 is a k2 × k1 dense linear layer. The clear advantages of such a strategy are: (1)
almost all choices of the weights from a specific distribution, namely the one mimicking FJLT,
preserve accuracy while reducing the number of parameters, and (2) the number of weights is
nearly linear in the layer width of W (the original matrix). Our empirical results demonstrate that
this offers faster training and prediction in deployment while producing results that match and
often outperform existing known architectures. Compressing neural networks by replacing linear
layers with structured linear transforms that are expressed by fewer parameters have been studied
extensively in the recent past. We compare our approach with these related works in Section 3.
Since the butterfly structure adds logarithmic depth to the architecture, it might pose optimization
related issues. Moreover, the sparse structure of the matrices connecting the layers in a butterfly
network defies the general theoretical analysis of convergence of deep linear networks. We take
a small step towards understanding these issues by studying the optimization landscape of a
encoder-decoder network (two layer linear neural network), where the encoder layer is replaced by
a truncated butterfly network followed by a dense linear layer in fewer parameters. This replacement
is motivated by the result of Sarlos (2006), related to fast randomized low-rank approximation
of matrices using FJLT (see Section 4.2 for details). We consider this replacement instead of
the architecture consisting of two butterfly networks and a dense linear layer as proposed earlier,
because it is easier to analyze theoretically. We also empirically demonstrate that our new network
with fewer parameters performs as well as an encoder-decoder network.
The encoder-decoder network computes the best low-rank approximation of the input matrix. It
is well-known that with high probability a close to optimal low-rank approximation of a matrix
is obtained by either pre-processing the matrix with an FJLT (SarlOs, 2006) or a random sparse
matrix structured as given in Clarkson and Woodruff (2009) and then computing the best low-rank
approximation from the rows of the resulting matrix1. A recent work by Indyk et al. (2019) studies
this problem in the supervised setting, where they find the best pre-processing matrix structured as
given in Clarkson and Woodruff (2009) from a sample of matrices (instead of using a random sparse
matrix). Since an FJLT can be represented by a truncated butterfly network, we emulate the setting
of Indyk et al. (2019) but learn the pre-processing matrix structured as a truncated butterfly network.
2	Our Contribution and Potential Impact
We provide an empirical report, together with a theoretical analysis to justify our main idea of
using sparse linear layers with a fixed butterfly network in deep learning. Our findings indicate that
this approach, which is well rooted in the theory of matrix approximation and optimization, can
offer significant speedup and energy saving in deep learning applications. Additionally, we believe
that this work would encourage more experiments and theoretical analysis to better understand the
optimization and generalization of our proposed architecture (see Future Work section).
On the empirical side - The outcomes of the following experiments are reported:
(1)	In Section 6.1, we replace a dense linear layer in the standard state-of-the-art networks, for
both image and language data, with an architecture that constitutes the composition of (a) truncated
butterfly network, (b) dense linear layer in smaller dimension, and (c) transposed truncated butterfly
network (see Section 4.2). The structure parameters are chosen so as to keep the number of weights
near linear (instead of quadratic).
(2)	In Sections 6.2 and 6.3, we train a linear encoder-decoder network in which the encoder is
replaced by a truncated butterfly network followed by a dense linear layer in smaller dimension.
These experiments support our theoretical result. The network structure parameters are chosen so
as to keep the number of weights in the (replaced) encoder near linear in the input dimension. Our
results (also theoretically) demonstrate that this has little to no effect on the performance compared
to the standard encoder-decoder network.
(3)	In Section 7, we learn the best pre-processing matrix structured as a truncated butterfly network
to perform low-rank matrix approximation from a given sample of matrices. We compare our results
1The pre-processing matrix is multiplied from the left.
2
Under review as a conference paper at ICLR 2021
to that of Indyk et al. (2019), which learn the pre-processing matrix structured as given in Clarkson
and Woodruff (2009).
On the theoretical side - The optimization landscape of linear neural networks with dense matrices
have been studied by Baldi and Hornik (1989), and Kawaguchi (2016). The theoretical part of
this work studies the optimization landscape of the linear encoder-decoder network in which the
encoder is replaced by a truncated butterfly network followed by a dense linear layer in smaller
dimension. We call such a network as the encoder-decoder butterfly network. We give an overview
of our main result, Theorem 1, here. Let X ∈ Rn×d and Y ∈ Rm×d beJhe data and output
matrices respectively. Then the encoder-decoder butterfly network is given as Y = DEBX, where
D ∈ Rm×k and E ∈ Rk×' are dense layers, B is an ` × n truncated butterfly network (product
of log n sparse matrices)_and k ≤ ' ≤ m ≤ n (see Section 5). The objective is to learn D, E
and B that minimizes ||Y - Y||F. Theorem 1 shows how the loss at the critical points of such
a network depends on the eigenvalues of the matrix Σ = YXT BT (BXXTBT)-1BXYT 2. In
comparison, the loss at the critical points of the encoder-decoder network (without the butterfly
network) depends on the eigenvalues of the matrix Σ0 = YXT (XXT)-1XYT (Baldi and Hornik,
1989). In particular, the loss depends on how the learned matrix B changes the eigenvalues of Σ0. If
we learn only for an optimal D and E, keeping B fixed (as done in the experiment in Section 6.3)
then it follows from Theorem 1 that every local minima is a global minima and that the loss at the
local/global minima depends on how B changes the top k eigenvalues of Σ0 . This inference together
with a result by Sarlos (2006) is used to give a worst-case guarantee in the special case when Y = X
(called auto-encoders that capture PCA; see the below Theorem 1).
3	Related Work
Important transforms like discrete Fourier, discrete cosine, Hadamard and many more satisfy a prop-
erty called complementary low-rank property, recently defined by Li et al. (2015). For an n × n ma-
trix satisfying this property related to approximation of specific sub-matrices by low-rank matrices,
Michielssen and Boag (1996) and O’Neil et al. (2010) developed the butterfly algorithm to compute
the product of such a matrix with a vector in O(n log n) time. The butterfly algorithm factorizes such
a matrix into O(log n) many matrices, each with O(n) sparsity. In general, the butterfly algorithm
has a pre-computation stage which requires O(n2) time (O’Neil et al., 2010; Seljebotn, 2012). With
the objective of reducing the pre-computation cost Li et al. (2015); Li and Yang (2017) compute
the butterfly factorization for an n × n matrix satisfying the complementary low-rank property in
3
O(n2) time. This line of work does not learn butterfly representations for matrices or apply it in
neural networks, and is incomparable to our work.
A few works in the past have used deep learning models with structured matrices (as hidden layers).
Such structured matrices can be described using fewer parameters compared to a dense matrix, and
hence a representation can be learned by optimizing over a fewer number of parameters. Examples
of structured matrices used include low-rank matrices (Denil et al., 2013; Sainath et al., 2013),
circulant matrices (Cheng et al., 2015; Ding et al., 2017), low-distortion projections (Yang et al.,
2015), Toeplitz like matrices (Sindhwani et al., 2015; Lu et al., 2016; Ye et al., 2018), Fourier-related
transforms (Moczulski et al., 2016) and matrices with low-displacement rank (Thomas et al., 2018).
Recently Alizadeh et al. (2020) demonstrated the benefits of replacing the pointwise convolutional
layer in CNN’s by a butterfly network. Other works by Mocanu et al. (2018); Lee et al. (2019); Wang
et al. (2020); Verdenius et al. (2020) consider a different approach to sparsify neural networks. The
works closest to ours are by Yang et al. (2015), Moczulski et al. (2016), and Dao et al. (2020) and
we make a comparison below.
Yang et al. (2015) and Moczulski et al. (2016) attempt to replace dense linear layers with a stack
of structured matrices, including a butterfly structure (the Hadamard or the Cosine transform), but
they do not place trainable weights on the edges of the butterfly structure as we do. Note that adding
these trainable weights does not compromise the run time benefits in prediction, while adding to
the expressiveness of the network in our case. Dao et al. (2020) replace handcrafted structured sub-
networks in machine learning models by a kaleidoscope layer, which consists of compositions of
butterfly matrices. This is motivated by the fact that the kaleidoscope hierarchy captures a structured
matrix exactly and optimally in terms of multiplication operations required to perform the matrix
2At a critical point the gradient of the loss function with respect to the parameters in the network is zero.
3
Under review as a conference paper at ICLR 2021
vector product operation. Their work differs from us as we propose to replace any dense linear layer
in a neural network (instead of a structured sub-network) by the architecture proposed in Section 4.2.
Our approach is motivated by theoretical results which establish that this can be done with almost
no loss in representation.
Finally, Dao et al. (2019) show that butterfly representations of standard transformations like discrete
Fourier, discrete cosine, Hadamard mentioned above can be learnt efficiently. They additionally
show the following: a) for the benchmark task of compressing a single hidden layer model they
compare the network constituting of a composition of butterfly networks with the classification
accuracy of a fully-connected linear layer and b) in ResNet a butterfly sub-network is added to get
an improved result. In comparison, our approach to replace a dense linear layer by the proposed
architecture in Section 4.2 is motivated by well-known theoretical results as mentioned previously,
and the results of the comprehensive list of experiments in Section 6.1 support our proposed method.
4	Proposed Replacement for a Dense Linear Layer
In Section 4.1, we define a truncated butterfly network, and in Section 4.2 we motivate and state
our proposed architecture based on truncated butterfly network to replace a dense linear layer in any
neural network. All logarithms are in base 2, and [n] denotes the set {1, . . . , n}.
4.1	Truncated Butterfly Network
Definition 4.1 (Butterfly Network). Letn be an integral power of 2. Then an n × n butterfly network
B (see Figure 6) is a stack of of logn linear layers, where in each layer i ∈ {0, . . . , log n - 1}, a
bipartite clique connects between pairs of nodes j1,j2 ∈ [n], for which the binary representation of
j1 - 1 and j2 - 1 differs only in the i’th bit. In particular, the number of edges in each layer is 2n.
In what follows, a truncated butterfly network is a butterfly network in which the deepest layer is
truncated, namely, only a subset of` neurons are kept and the remaining n-` are discarded. The in-
teger ` is a tunable parameter, and the choice of neurons is always assumed to be sampled uniformly
at random and fixed throughout training in what follows. The effective number of parameters (train-
able weights) in a truncated butterfly network is at most 2n log ` + 6n, for any ` and any choice of
neurons selected from the last layer.3 We include a proof of this simple upper bound in Appendix F
for lack of space (also, refer to Ailon and Liberty (2009) for a similar result related to computation
time of truncated FFT). The reason for studying a truncated butterfly network follows (for exam-
ple) from the works (Ailon and Chazelle, 2009; Ailon and Liberty, 2009; Krahmer and Ward, 2011).
These papers define randomized linear transformations with the Johnson-Lindenstrauss property and
an efficient computational graph which essentially defines the truncated butterfly network. In what
follows, we will collectively denote these constructions by FJLT. 4
4.2	Matrix Approximation Using B utterfly Networks
We begin with the following proposition, following known results on matrix approximation (proof
in Appendix B).
Proposition 1. Suppose J1 ∈ Rk1×n1 and J2 ∈ Rk2×n2 are matrices sampled from FJLT distribu-
tion, and let W ∈ Rn2 ×n1. Then for the random matrix W0 = (J2TJ2)W(J1TJ1), any unit vector
X ∈ Rn1 and any e ∈ (0,1), Pr [∣∣W0x — WXk ≤ e∣∣ Wk] ≥ 1 - e-Q(mm{k1,k2}e2),
From Proposition 1 it follows that W0 approximates the action of W with high probability on any
given input vector. Now observe that W0 is equal to JTW Ji, where W = J2WJT. Since J and
J2 are FJLT, they can be represented by a truncated butterfly network, and hence it is conceivable to
replace a dense linear layer connecting n1 neurons to n2 neurons (containing n1n2 variables) in any
3 Note that if n is not a power of 2 then we work with the first n columns of the ` × n0 truncated butterfly
network, where n0 is the closest number to n that is greater than n and is a power of 2.
4To be precise, the construction in Ailon and Chazelle (2009), Ailon and Liberty (2009), and Krahmer and
Ward (2011) also uses a random diagonal matrix, but the values of the diagonal entries can be ‘absorbed’ inside
the weights of the first layer of the butterfly network.
4
Under review as a conference paper at ICLR 2021
neural network with a composition of three gadgets: a truncated butterfly network of size k1 × n1,
followed by a dense linear layer of size k2 × k1, followed by the transpose of a truncated butterfly
network of size k2 × n2. In Section 6.1, we replace dense linear layers in common deep learning
networks with our proposed architecture, where we set k1 = log n1 and k2 = log n2 .
5	Encoder-Decoder Butterfly Network
Let X ∈ Rn×d , and Y ∈ Rm×d be data and output matrices respectively, and k ≤ m ≤ n. Then
the encoder-decoder network for X is given as
Y = DEX
where E ∈ Rk×n , and D ∈ Rm×k are called the encoder and decoder matrices respectively. For
the special case when Y = X, it 型 Caned auto-encoders. The optimization problem is to learn
matrices D and E such that ||Y - Y||2 is minimized. The optimal solution is denoted as Y*, D*
and E*5. In the case of auto-encoders X* = Xk, where Xk is the best rank k approximation of X.
In this section, we study the optimization landscape of the encoder-decoder butterfly network : an
encoder-decoder network, where the encoder is replaced by a truncated butterfly network followed
by a dense linear layer in smaller dimension. Such a replacement is motivated by the following
result from Sarlos (2006), in which ∆k = ||Xk - X||2.
Proposition 2. Let X ∈ Rn×d. Then with probability at least 1/2, the best rank k approximation
of X from the rows of JX (denoted Jk (X)), where J is sampled from an ` × n FJLT distribution
and ` = (k log k + k/) satisfies ||Jk (X) - X ||2F ≤ (1 + )∆k.
Proposition 2 suggests that in the case of auto-encoders we could replace the encoder with a trun-
cated butterfly network of size ` × n followed by a dense linear layer of size k × `, and obtain a
network with fewer parameters but loose very little in terms of representation. Hence, it is worth-
while investigating the representational power of the encoder-decoder butterfly network
Y = DEBX .	(1)
Here, X, Y and D are as in the encoder-decoder network, E ∈ Rk×' is a dense matrix, and B is
an ` × n truncated butterfly network. In the encoder-decoder butterfly network the encoding is done
using EB, and decoding is done using D. This reduces the number of parameters in the encoding
matrix from kn (as in the encoder-decoder network) to k` + O(n log `). Again the objective is to
learn matrices D and E, and the truncated butterfly network B such that ||Y - Y||F is minimized.
The optimal solution is denoted as Y*, D*, E*, and B*. Theorem 1 shows that the loss at a critical
point of such a network depends on the eigenvalues of Σ(B) = YXT BT (BXXTBT)-1XYT,
when BXXT BT is invertible and Σ(B) has ` distinct positive eigenvalues.The loss L is defined as
I∣Y - Y112.
Theorem 1. Let D, E and B be a point of the encoder-decoder network with a truncated butterfly
network satisfying the following: a) BXXT BT is invertible, b) Σ(B) has ` distinct positive eigen-
values λι > ... > λ', and c) the gradient of L(Y) with respect to the parameters in D and E
matrix is zero. Then corresponding to this point (and hence corresponding to every critical point)
there is an I ⊆ ['] such that L(Y) at this point is equal to tr(YYT) — ^iEl λ%. Moreover if the
point is a local minima then I = [k].
The proof of Theorem 1 is given in Appendix C. We also compare our result with that of Baldi and
Hornik (1989) and Kawaguchi (2016), which study the optimization landscape of dense linear neural
networks in Appendix C. From Theorem 1 it follows that if B is fixed and only D and E are trained
then a local minima is indeed a global minima. We use this to claim a worst-case guarantee using
a two-phase learning approach to train an auto-encoder. In this case the optimal solution is denoted
as Bk(Y), DB, and EB. Observe that when Y = X, Bk(X) is the best rank k approximation ofX
computed from the rows of BX.
Two phase learning for auto-encoder: Let ` = k log k + k/ and consider a two phase learning
strategy for auto-encoders, as follows: In phase one B is sampled from an FJLT distribution, and
then only D and E are trained keeping B fixed. Suppose the algorithm learns D0 and E0 at the end *
5Possibly multiple D* and E* exist such that Y * = D*E*X.
5
Under review as a conference paper at ICLR 2021
Dataset Name	Task	Model
Cifar-10 KriZhevSky (2012) Cifar-10 KriZhevSky (2012) Cifar-100 Krizhevsky (2012) Imagenet Deng et al. (2009) CONLL-03 Tjong Kim Sang and De Meulder (2003) CONLL-03 Tjong Kim Sang and De Meulder (2003) Penn Treebank (English) Marcus et al. (1993)	Image classification Image classification Image classification Image classification Named Entity Recognition (English) Named Entity Recognition (German) Part-of-Speech Tagging	EffiCientNet Tan and Le (2019) PreACtReSNet18 He et al. (2016) seresnet152 Hu et al. (2020) senet154 Hu et al. (2020) Flair’s Sequence Tagger Akbik et al. (2018) Akbik et al. (2019) Flair’s Sequence Tagger Akbik et al. (2018) Akbik et al. (2019) Flair,s Sequence Tagger Akbik et al. (2018) Akbik et al. (2019)
Table 1: Data and the corresponding architectures used in the fast matrix multiplication using but-
terfly matrices experiments.
of phase one, and X0 = D0E0B. Then Theorem 1 guarantees that, assuming Σ(B) has ` distinct
positive eigenvalues and D0, E0 are a local minima, D0 = DB, E0 = EB, and X0 = Bk(X).
Namely X0 is the best rank k approximation of X from the rows of BX . From Proposition 2 with
probability at least 2, L(X0) ≤ (1 + E)∆k. In the second phase all three matrices are trained to
improve the loss. In Sections 6.2 and 6.3 we train an encoder-decoder butterfly network using the
standard gradient descent method. In these experiments the truncated butterfly network is initialized
by sampling it from an FJLT distribution, and D and E are initialized randomly as in Pytorch.
6	Experiments on Dense Layer Replacement and
Encoder-Decoder Butterfly Network
In this section we report the experimental results based on the ideas presented in Sections 4.2 and 5.
6.1	Replacing Dense Linear Layers by the Proposed Architecture
This experiment replaces a dense linear layer of size n2 × n1 in common deep learning architectures
with the network proposed in Section 4.2.6 The truncated butterfly networks are initialized by sam-
pling it from the FJLT distribution, and the dense matrices are initialized randomly as in Pytorch.
We set k1 = log n1 and k2 = log n2. The datasets and the corresponding architectures considered
are summarized in Table 1. For each dataset and model, the objective function is the same as de-
fined in the model, and the generalization and convergence speed between the original model and
the modified one (called the butterfly model for convenience) are compared. Figure 7 in Appendix
D.1 reports the number of parameters in the dense linear layer of the original model, and in the
replaced network, and Figure 8 in Appendix D.1 displays the number of parameter in the original
model and the butterfly model. In particular, Figure 7 shows the significant reduction in the number
of parameters obtained by the proposed replacement. On the left of Figure 1, the test accuracy of
the original model and the butterfly model is reported, where the black vertical lines denote the error
bars corresponding to standard deviation, and the values above the rectangles denote the average
accuracy. On the right of Figure 1 observe that the test accuracy for the butterfly model trained with
stochastic gradient descent is even better than the original model trained with Adam in the first few
epochs. Figure 12 in Appendix D.1 compares the test accuracy in the the first 20 epochs of the orig-
inal and butterfly model. The results for the NLP tasks in the interest of space are reported in Figure
9, Appendix D.1. The training and inference times required for the original model and the butterfly
model in each of these experiments are reported in Figures 10 and 11 in Appendix D.1. We remark
that the modified architecture is also trained for fewer epochs. In almost all the cases the modified
architecture does better than the normal architecture, both in the rate of convergence and in the final
accuracy/F 1 score. Moreover, the training time for the modified architecture is less.
6.2	Encoder-Decoder Butterfly network with Synthetic Gaussian and Real
Data
This experiment tests whether gradient descent based techniques can be used to train encoder-
decoder butterfly network. In all the experiments in this section Y = X . Five types of data
matrices are tested, whose attributes are specified in Table 2.7 Two among them are random and
6In all the architectures considered the final linear layer before the output layer is replaced, and n1 and n2
depend on the architecture.
7In Table 2 HS-SOD denotes a dataset for hyperspectral images from natural scenes (Imamoglu et al., 2018).
6
Under review as a conference paper at ICLR 2021
Figure 1: Left: comparison of final test accuracy with different image classification models and data
sets; Right: comparison of test accuracy in the first few epochs with different models and optimizers
on CIFAR-10 with PreActResNet18
three are constructed using standard public real image datasets. In the interest of space, the con-
struction of the data matrices is explained in Appendix D.2. For the matrices constructed from the
image datasets, the input coordinates are randomly permuted, which ensures the network cannot
take advantage of the spatial structure in the data. For each of the data matrices the loss obtained
via training the truncated butterfly network with the Adam optimizer is compared to ∆k (denoted as
PCA) and ||Jk (X) - X ||F2 where J is an ` × n matrix sampled from the FJLT distribution (denoted
as FJLT+PCA). Figure 2 reports the loss on Gaussian 1 and MNIST, whereas Figure 13 in Appendix
D.2 reports the loss for the remaining data matrices. Observe that for all values of k the loss for
the encoder-decoder butterfly network is almost equal to ∆k, and is in fact ∆k for small and large
values of k .
Name	n	d	rank
GaUSSian 1	1024	1024	32
GaUSSian 2	1024	1024	64
MNIST	1024	1024	1024
Olivetti	1024	4096	1024
HS-SOD	1024	768	768
Table 2: Data used in the truncated butterfly auto-encoder reconstruction experiments
Figure 2: Approximation error on data matrix with various methods for various values of k . Left:
Gaussian 1 data, Right: MNIST data
6.3	Two-phase Learning for Encoder-Decoder Butterfly Network
This experiment is similar to the experiment in Section 6.2 but the training in this case is done in two
phases. In the first phase, B is fixed and the network is trained to determine an optimal D and E . In
the second phase, the optimal D and E determined in phase one are used as the initialization, and the
7
Under review as a conference paper at ICLR 2021
network is trained over D, E and B to minimize the loss. Theorem 1 ensures worst-case guarantees
for this two phase training (see below the theorem). Figure 3 reports the approximation error of an
image from Imagenet. The red and green lines in Figure 3 correspond to the approximation error at
the end of phase one and two respectively.
Figure 3: Approximation error achieved by different methods and the same zoomed on in the right
7	S ketching Algorithm for Low-Rank Matrix Decomposition
Problem Using Butterfly Network
The recent influential work by Indyk et al. (2019) considers a supervised learning approach to com-
pute an` × n pre-conditioning matrix B for low-rank approximation ofn × d matrices. The matrix B
has a fixed sparse structure as in Clarkson and Woodruff (2009), each column as one non-zero entry
(chosen randomly) which are learned to minimize the loss over a training set of matrices. In this
section, we present experiments with the setting being similar to that in Indyk et al. (2019), except
that B is now represented as an ` × n truncated butterfly network. Our setting is similar to that in
Indyk et al. (2019), except that B is now represented as an ` × n truncated butterfly network. Our
experiments suggests that indeed a learned truncated butterfly network does better than a random
matrix, and even a learned B as in Indyk et al. (2019).
Setup: Suppose X1, . . . , Xt ∈ Rn×d are training matrices sampled from a distribution D. Then a
B is computed that minimizes the following empirical loss: Pi∈[t] ||Xi - Bk(Xi)||F2. We compute
Bk(Xi) using truncated SVD of BXi (as in Algorithm 1, Indyk et al. (2019)). Similar to Indyk
et al. (2019), the matrix B is learned by the back-propagation algorithm that uses a differentiable
SVD implementation to calculate the gradients, followed by optimization with Adam such that the
butterfly structure of B is maintained. The learned B can be used as the pre-processing matrix for
any matrix in the future. The test error for a matrix B and a test set Te is defined as follows:
ErrTe(B) = EX〜Te | ||X - Bk(X)∣∣F ] - APPTe, where APPTe = EX〜Te | ||X - Xk||F ].
Experiments and Results: The exPeriments are Performed on the datasets shown in Table 3. In
HS-SOD Imamoglu et al. (2018) and CIFAR-10 Krizhevsky (2012) 400 training matrices (t = 400),
and 100 test matrices are samPled, while in Tech 200 training matrices (t = 200), and 95 test
matrices are samPled. In Tech Davido et al. (2004) each matrix has 835,422 rows but on average
only 25,389 rows and 195 columns contain non-zero entries. For the same reason as in Section 6.2
in each dataset, the coordinates of each row are randomly Permuted. Some of the matrices in the
datasets have much larger singular values than the others, and to avoid imbalance in the dataset,
the matrices are normalized so that their toP singular values are all equal, as done in Indyk et al.
(2019). For each of the datasets, the test error for the learned B via our truncated butterfly structure
Name	n	d
HS-SOD 1	1024	-768-
CIFAR-10	32	32
Tech	25,389	195
Table 3: Data used in the Sketching algorithm for low-rank matrix decomPosition exPeriments.
8
Under review as a conference paper at ICLR 2021
is compared to the test errors for the following three cases: 1) B is a learned as a sparse sketching
matrix as in Indyk et al. (2019), b) B is a random sketching matrix as in Clarkson and Woodruff
(2009), and c) B is an ` × n Gaussian matrix. Figure 4 compares the test error for ` = 20, and
k = 10, where AppTe = 10.56. Figure 14 in Appendix E compares the test errors of the different
methods in the extreme case when k = 1, and Figure 15 in Appendix E compares the test errors of
the different methods for various values of `. Table 4 in Appendix E in Appendix E reports the test
error for different values of ` and k . Figure 16 in in Appendix E shows the test error for ` = 20 and
k = 10 during the training phase on HS-SOD. In Figure 16 it is observed that the butterfly learned
is able to surpass sparse learned after a merely few iterations.
Figure 5 compares the test error for the learned B via our truncated butterfly structure to a learned
matrix B with N non-zero entries in each column - the N non-zero location for each column are
chosen uniformly at random. The reported test errors are on HS-SOD, when ` = 20 and k = 10.
Interestingly, the error for butterfly learned is not only less than the error for sparse learned (N = 1
as in (Indyk et al., 2019)) but also less than than the error for dense learned (N = 20). In particular,
our results indicate that using a learned butterfly sketch can significantly reduce the approximation
loss compared to using a learned sparse sketching matrix.
Figure 4: Test error by different sketching
matrices on different data sets
Figure 5: Test errors for various values of N
and a learned butterfly matrix
8 Discussion and Future Work
Discussion: Among other things, this work showed that it is beneficial to replace dense linear layer
in deep learning architectures with a more compact architecture (in terms of number of parameters),
using truncated butterfly networks. This approach is justified using ideas from efficient matrix ap-
proximation theory from the last two decades. however, results in additional logarithmic depth to the
network. This issue raises the question of whether the extra depth may harm convergence of gradient
descent optimization. To start answering this question, we show, both empirically and theoretically,
that in linear encoder-decoder networks in which the encoding is done using a butterfly network,
this typically does not happen. To further demonstrate the utility of truncated butterfly networks, we
consider a supervised learning approach as in Indyk et al. (2019), where we learn how to derive low
rank approximations of a distribution of matrices by multiplying a pre-processing linear operator
represented as a butterfly network, with weights trained using a sample of the distribution.
Future Work: The main open questions arising from the work are related to better understanding the
optimization landscape of butterfly networks. The current tools for analysis of deep linear networks
do not apply for these structures, and more theory is necessary. It would be interesting to determine
whether replacing dense linear layers in any network, with butterfly networks as in Section 4.2 harms
the convergence of the original matrix. Another direction would be to check empirically whether
adding non-linear gates between the layers (logarithmically many) of a butterfly network improves
the performance of the network. In the experiments in Section 6.1, we have replaced a single dense
layer by our proposed architecture. It would be worthwhile to check whether replacing multiple
dense linear layers in the different architectures harms the final accuracy. Similarly, it might be
insightful to replace a convolutional layer by an architecture based on truncated butterfly network.
Finally, since our proposed replacement reduces the number of parameters in the network, it might
be possible to empirically show that the new network is more resilient to over-fitting.
9
Under review as a conference paper at ICLR 2021
Acknowledgement
This project has received funding from European Union’s Horizon 2020 research and innovation
program under grant agreement No 682203 -ERC-[ Inf-Speed-Tradeoff].
References
N. Ailon and B. Chazelle. The fastjohnson-lindenstrauss transform and approximate nearest neigh-
bors. SIAMJ. COmPut, 39(1):302-322, 2009.
N. Ailon and E. Liberty. Fast dimension reduction using rademacher series on dual BCH codes.
Discret. COmPut. GeOm., 42(4):615-630, 2009.
A. Akbik, D. Blythe, and R. Vollgraf. Contextual string embeddings for sequence labeling. In
COLING 2018, 27th InternatiOnal COnference On COmPutatiOnal Linguistics, pages 1638-1649,
2018.
A. Akbik, T. Bergmann, and R. Vollgraf. Pooled contextualized embeddings for named entity recog-
nition. In NAACL 2019, 2019 Annual COnference Of the NOrth American ChaPter Of the AssOcia-
tiOn fOr COmPutatiOnal Linguistics, page 724-728, 2019.
K. Alizadeh, P. Anish, F. Ali, and R. Mohammad. Butterfly transform: An efficient fft based neu-
ral architecture design. In IEEE/CVF COnference On COmPuter VisiOn and Pattern RecOgnitiOn
(CVPR), June 2020.
P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples
without local minima. Neural NetwOrks, 2(1):53-58, 1989.
A. L. Cambridge. The olivetti faces dataset, 1994.
Y. Cheng, F. X. Yu, R. S. Feris, S. Kumar, A. N. Choudhary, and S. Chang. An exploration of
parameter redundancy in deep networks with circulant projections. In 2015 IEEE InternatiOnal
COnference On COmPuter VisiOn, ICCV 2015, SantiagO, Chile, December 7-13, 2015, pages 2857-
2865. IEEE Computer Society, 2015.
K. L. Clarkson and D. P. Woodruff. Numerical linear algebra in the streaming model. In M. Mitzen-
macher, editor, PrOceedings Of the 41st Annual ACM SymPOsium On TheOry Of COmPuting, STOC
2009, pages 205-214. ACM, 2009.
J. Cooley and J. Tukey. An algorithm for the machine calculation of complex fourier series. Mathe-
matics Of COmPutatiOn, 19(90):297-301, 1965.
T. Dao, A. Gu, M. Eichhorn, A. Rudra, and C. Re. Learning fast algorithms for linear transforms
using butterfly factorizations. In K. Chaudhuri and R. Salakhutdinov, editors, PrOceedings Of the
36th InternatiOnal COnference On Machine Learning, ICML 2019, 9-15 June 2019, LOng Beach,
CalifOrnia, USA, volume 97 of PrOceedings Of Machine Learning Research, pages 1517-1527.
PMLR, 2019.
T. Dao, N. S. Sohoni, A. Gu, M. Eichhorn, A. Blonder, M. Leszczynski, A. Rudra, and C. R.
’e. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In 8th
InternatiOnal COnference On Learning RePresentatiOns, ICLR 2020, Addis Ababa, EthiOPia, APril
26-30, 2020, 2020.
D. Davido, E. Gabrilovich, and S. Markovitch. Parameterized generation of labeled datasets for
text categorization based on a hierarchical directory. In 27th Annual InternatiOnal ACM SIGIR
COnference On Research and DevelOPment in InfOrmatiOn Retrieval, SIGIR ’04, pages 250-257,
2004.
J.	Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
10
Under review as a conference paper at ICLR 2021
M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas. Predicting parameters in deep learn-
ing. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on
Neural Information Processing Systems 2013., pages 2148-2156, 2013.
C. Ding, S. Liao, Y. Wang, Z. Li, N. Liu, Y. Zhuo, C. Wang, X. Qian, Y. Bai, G. Yuan, X. Ma,
Y. Zhang, J. Tang, Q. Qiu, X. Lin, and B. Yuan. Circnn: accelerating and compressing deep neural
networks using block-circulant weight matrices. In Proceedings of the 50th Annual IEEE/ACM
International Symposium on Microarchitecture, MICRO 2017, pages 395-408. ACM, 2017.
K.	He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In Computer
Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pages 630-645.
Springer, 2016.
J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu. Squeeze-and-excitation networks. IEEE Trans.
Pattern Anal. Mach. Intell., 42(8):2011-2023, 2020.
N. Imamoglu, Y. Oishi, X. Zhang, Y. F. G. Ding, T. Kouyama, and R. Nakamura. Hyperspectral
image dataset for benchmarking on salient object detection. In Tenth International Conference on
Quality of Multimedia Experience, (QoMEX), pages 1-3, 2018.
P.	Indyk, A. Vakilian, and Y. Yuan. Learning-based low-rank approximations. In H. M. Wallach,
H. Larochelle, A. Beygelzimer, F. d’Alche-Buc, E. B. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, pages 7400-7410, 2019.
V.	Jain, N. Pillai, and A. Smith. Kac meets johnson and lindenstrauss: a memory-optimal, fast
johnson-lindenstrauss transform. arXiv, 03 2020.
W.	Johnson and J. Lindenstrauss. Extensions of lipschitz maps into a hilbert space. Contemporary
Mathematics, 26:189-206, 01 1984. doi: 10.1090/conm/026/737400.
K. Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016,
December 5-10, 2016, Barcelona, Spain, pages 586-594, 2016.
F. Krahmer and R. Ward. New and improved johnson-lindenstrauss embeddings via the restricted
isometry property. SIAM Journal on Mathematical Analysis, 43:1269-1281, 06 2011. doi: 10.
1137/100810447.
A. Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 2012.
Y	. LeCun and C. Cortes. MNIST handwritten digit database, 2010.
N. Lee, T. Ajanthan, and P. H. S. Torr. Snip: single-shot network pruning based on connection sen-
sitivity. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Y	. Li and H. Yang. Interpolative butterfly factorization. SIAM J. Scientific Computing, 39(2), 2017.
Y	. Li, H. Yang, E. R. Martin, K. L. Ho, and L. Ying. Butterfly factorization. Multiscale Model.
Simul., 13(2):714-732, 2015.
Z. Lu, V. Sindhwani, and T. N. Sainath. Learning compact recurrent neural networks. In 2016
IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2016, pages
5960-5964. IEEE, 2016.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19(2):313-330, 1993. URL https://www.
aclweb.org/anthology/J93-2004.
E. Michielssen and A. Boag. A multilevel matrix decomposition algorithm for analyzing scattering
from large structures. IEEE Transactions on Antennas and Propagation, 44(8):1086-1093, 1996.
11
Under review as a conference paper at ICLR 2021
D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta. Scalable training of
artificial neural networks with adaptive sparse connectivity inspired by network science. Nature
Communications, 9:2383, 2018. doi: 10.1038/s41467-018-04316-3.
M. Moczulski, M. Denil, J. Appleyard, and N. de Freitas. ACDC: A structured efficient linear layer.
In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations,
ICLR 2016, 2016.
M. O’Neil, F. Woolfe, and V. Rokhlin. An algorithm for the rapid evaluation of special function
transforms. Applied and Computational Harmonic Analysis, 28(2):203 - 226, 2010.
T.	N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix
factorization for deep neural network training with high-dimensional output targets. In IEEE
International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver,
BC, Canada, May 26-31, 2013, pages 6655-6659. IEEE, 2013.
T.	Sarlos. Improved approximation algorithms for large matrices via random projections. In 47th
Annual IEEE Symposium on Foundations of Computer Science (FOCS 2006), pages 143-152.
IEEE Computer Society, 2006.
D.	S. Seljebotn. WAVEMOTH-FAST SPHERICAL HARMONIC TRANSFORMS BY BUTTER-
FLY MATRIX COMPRESSION. The Astrophysical Journal Supplement Series, 199(1):5, 2012.
V. Sindhwani, T. N. Sainath, and S. Kumar. Structured transforms for small-footprint deep learning.
In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 28: Annual Conference on Neural Information Processing
Systems 2015, pages 3088-3096, 2015.
M. Tan and Q. V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In
K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pages 6105-6114. PMLR, 2019.
A. T. Thomas, A. Gu, T. Dao, A. Rudra, and C. Re. Learning compressed transforms with low dis-
placement rank. In Advances in Neural Information Processing Systems 31: Annual Conference
on Neural Information Processing Systems 2018, pages 9066-9078, 2018.
E.	F. Tjong Kim Sang and F. De Meulder. Introduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings of the Seventh Conference on Natural Lan-
guage Learning at HLT-NAACL 2003, pages 142-147, 2003. URL https://www.aclweb.
org/anthology/W03-0419.
S. Verdenius, M. Stol, and P. Forre. Pruning via iterative ranking of sensitivity statistics. CoRR,
abs/2006.00896, 2020.
C. Wang, G. Zhang, and R. B. Grosse. Picking winning tickets before training by preserving gradient
flow. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. J. Smola, L. Song, and Z. Wang. Deep fried
convnets. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, pages 1476-
1483. IEEE Computer Society, 2015.
J. Ye, L. Wang, G. Li, D. Chen, S. Zhe, X. Chu, and Z. Xu. Learning compact recurrent neural
networks with block-term tensor decomposition. In 2018 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2018, pages 9378-9387. IEEE Computer Society, 2018.
12
Under review as a conference paper at ICLR 2021
A Butterfly Diagram from Section 1
Figure 6 referred to in the introduction is given here.
Figure 6: A 16 × 16 butterfly network represented as a 4-layered graph on the left, and as product
of 4 sparse matrices on the right. The white entries are the non-zero entries of the matrices.
SIHBS
B Proof of Proposition 1
The proof of the proposition will use the following well known fact (Lemma B.1 below) about
FJLT (more generally, JL) distributions (see Ailon and Chazelle (2009); Ailon and Liberty (2009);
Krahmer and Ward (2011)).
Lemma B.1. Let x ∈ Rn be a unit vector, and let J ∈ Rk×n be a matrix drawn from an FJLT
distribution. Thenfor all e < 1 with probability at least 1 - £-。(舐2) ；
kx- JTJxk ≤ .	(2)
By Lemma B.1 we have that With probability at least 1 — e-Q(k1 e2),
kx - J1TJ1xk ≤ kxk = .	(3)
Henceforth, we condition on the event kx-J1TJ1xk ≤ kxk. Therefore, by the definition of spectral
norm kW k of W :
kWx-WJ1TJ1xk ≤kWk .	(4)
Now apply Lemma B.1 again on the vector WJ1TJ1x and transformation J2 to get that with proba-
bility at least 1 — e-Q(k2e2),
kWJ1TJ1x- J2TJ2WJ1TJ1xk ≤ kWJ1TJ1xk.	(5)
Henceforth, we condition on the event kWJ1TJ1x — J2TJ2WJ1TJ1xk ≤ kWJ1TJ1xk. To bound
the last right hand side, we use the triangle inequality together with (4):
kW J1T J1xk ≤ kWxk+kWk ≤ kWk(1+).	(6)
Combining (5) and (6) gives:
kWJ1TJ1x—J2TJ2WJ1TJ1xk ≤kWk(1+).	(7)
13
Under review as a conference paper at ICLR 2021
Finally,
kJ2TJ2WJ1TJ1x- Wxk = k(J2TJ2WJ1TJ1x - WJ1TJ1x) + (WJ1TJ1x - Wx)k
≤ kWk(1+)+kWk
= kWk(2+)≤3kWk,	(8)
where the first inequality is from the triangle inequality together with (4) and (7), and the second
inequality is from the bound on . The proposition is obtained by adjusting the constants hiding
inside the Ω() notation in the exponent in the proposition statement.
C Proof of Theorem 1
We first note that our result continues to hold even if B in the theorem is replaced by any structured
matrix. For example the result continues to hold if B is an ` × n matrix with one non-zero entry per
column, as is the case with a random sparse sketching matrix Clarkson and Woodruff (2009). We
also compare our result with that Baldi and Hornik (1989); Kawaguchi (2016).
Comparison with Baldi and Hornik (1989) and Kawaguchi (2016): The critical points of the
encoder-decoder network are analyzed in Baldi and Hornik (1989). Suppose the eigenvalues of
YXT(XXT)-1XY T are γ1 > . . . > γm > 0 and k ≤ m ≤ n. Then they show that corresponding
to a critical point there is an I ⊆ [m] such that the loss at this critical point is equal to tr(Y Y T) -
i∈I γi, and the critical point is a local/global minima if and only if I = [k]. Kawaguchi (2016)
later generalized this to prove that a local minima is a global minima for an arbitrary number of
hidden layers in a linear neural network if m ≤ n. Note that since ` ≤ n and m ≤ n in Theorem 1,
replacing X by BX in Baldi and Hornik (1989) or Kawaguchi (2016) does not imply Theorem 1 as
it is.
Next, We introduce a few notation before delving into the proof. Let r = (Y - Y)T, and vec(r) ∈
Rmd is the entries of r arranged as a vector in column-first ordering, (Vvec(DT)L(Y))T ∈ Rmk and
(▽vec(ET)L(Y))t ∈ Rk' denote the partial derivative of L(Y) with respect to the parameters in
vec(DT) and VeC(ET) respectively. Notice that VVeC(DT)L(Y) and VVeC(ET)L(Y) are row vectors
of size mk and k` respectively. Also, let PD denote the projection matrix of D, and hence if D is a
matrix with full column-rank then PD = D(DT ∙ D)-1 ∙ DT. The n X n identity matrix is denoted
as In, and for convenience of notation let X =_B ∙ X. First we Prove the following lemma which
gives an expression for D and E if VVeC(DT)L(Y) and VVeC(ET)L(Y) are zero.
Lemma C.1 (Derivatives with respect to D and E).
1.	VVeC(DT)L(Y) = vec(r)T(Im % (E ∙ X)T), and
2.	Vvec(ET)L(X) = Vec(r)T(D 0 X)T
Proof. 1. Since L(Y) = 11 vec(r)T ∙ vec(r),
Vvec(DT)L(Y) = vec(r)T ∙ Vvec(DT)vec(r) = vec(r)T(VeC(DT)(XT ∙ ET ∙ DT))
=Vec(r)T(Im 0 (E ∙ X)T) ∙ Vvec(DT)Vec(DT) = Vec(r)T(Im 0 (E ∙ X)T)
2. Similarly,
Vvec(ET)L(Y) = vec(r)T ∙Vvec(ET)vec(r) = vec(r)T(VeC(ET)(Xt ∙ Et ∙ Dt))
=vec(r)τ(D 0 XT) ∙Vvec(ET)vec(ET) = vec(r)τ(D 0 XT)
□
Assume the rank of D is equal to p. Hence there is an invertible matrix C ∈ Rk×k such that
D = D ∙ C is such that the last k - P columns of D are zero and the first P columns of D are linearly
independent (Via Gauss elimination). Let E = C 1 ∙ E. Without loss of generality it can be assumed
D ∈ Rd×p, and E ∈ Rp×d, by restricting restricting D to its first P columns (as the remaining are
14
Under review as a conference paper at ICLR 2021
zero) and E to its first p rows. Hence, D is a full column-rank matrix of rank p, and DE = DE.
Claims C.1 and C.2 aid us in the completing the proof of the theorem. First the proof of theorem is
completed using these claims, and at the end the two claims are proved.
Claim C.1 (Representation at the critical point).
1.	E = (DTD)TDTYXT(X ∙ XT)-1
2.	DE = PDYXT(X ∙ XT)-1
ClaimC.2.	1. EBD = (EBYXTET)(EXXTET)-1
2.	PD ς = ςPD = PD ςPD
We denote Σ(B) as Σ for convenience. Since Σ is a real symmetric matrix, there is an orthogonal
matrix U consisting of the eigenvectors of Σ, such that Σ = U ∧ UT , where ∧ is a m × m
diagonal matrix whose first' diagonal entries are λι,...,λ' and the remaining entries are zero. Let
u1 , . . . , um be the columns of U . Then for i ∈ [`], ui is the eigenvector of Σ corresponding to the
eigenvalue λ%, and {u'+ι,..., Udy} are the eigenvectors of Σ corresponding to the eigenvalue 0.
Note that PUTD = UTD(DTUTUD)-IDTU = UTPDU, and from part two of Claim C.2
we have
(UPU τ D U T )Σ = Σ(UPu t D U T)	(9)
U ∙ PU T D ∧ U T = U ∧ PU T D U T	(10)
PU T D ∧ = ∧pu t D	(II)
Since PUTD commutes with ∧, PUTD is a block-diagonal matrix comprising of two blocks Pi and
P2: the first block P1 is an ` × ` diagonal block, and P2 is a (m - `) × (m - `) matrix. Since
PUt D is orthogonal projection matrix of rank P its eigenvalues are 1 with multiplicity P and 0 with
multiplicity m - p. Hence at most p diagonal entries of P1 are 1 and the remaining are 0. Finally
observe that
L(Y) = tr((Y - Y)(Y - Y)T)
=tr(YYT) - 2tr(YYT) + tr(YYT)
=tr(YY T) — 2tr(PD ∑) + tr(PD ∑PD)
=tr(YYT) - tr(PDΣ)
τγ T	TT Tr-^Τ
The second line in the above equation follows using the fact that tr(YYT) = tr(YY ), the third
line in the above equation follows by substituting Y = PDYXT ∙ (X ∙ XT)-i ∙ X (from part two
of Claim C.1), and the last line follows from part two of Claim C.2. Substituting Σ = U ∧ UT, and
PD = UPUT D UT in the above equation we have,
L(Y) = tr(YYT) - WUPUTD ∧ UT)
=tr(YYT ) - K(PUT D ∧
The last line the above equation follows from the fact that tr( UPuTD ∧ UT) = tr(PuT D ∧ UT U)=
tr(PutD∧). From the structure of PUTD and ∧ it follows that there is a subset I ⊆ ['], |I| ≤ P such
that tr(PuτD∧) = £记1％. Hence, L(Y) = tr(YYT) - £日 %.
Since PD = UPUT D UT, there is a P × P invertible matrix M such that
D = (U ∙ V)I0 ∙ M ,and E = MT(VTUT)I，YXT(XXT)-i
where V is a block-diagonal matrix consisting of two blocks Vi and V2: Vi is equal to l`, and V2 is
an (m 一 ') X (m 一 ') orthogonal matrix, and 10 is such that I ⊆ 10 and |I0∣ = p. The relation for E
in the above equation follows from part one of Claim C.1. Note that if I0 ⊆ [`], then I = I0, that is
I consists of indices corresponding to eigenvectors of non-zero eigenvalues.
Recall that D was obtained by truncating the last k - P zero rows of DC, where C was a
15
Under review as a conference paper at ICLR 2021
k X k invertible matrix simulating the Gaussian elimination. Let [M∣Op×(k-p)] denoted the P X k
matrix obtained by augmenting the columns of M with (k - p) zero columns. Then
D = (UV)io [M∣Op×(k-p)]C-1 .
Similarly, there is ap X (k - p) matrix N such that
E = C [ M-1 ]((UV )io)T YX T (X X T )-1
where [ Mj-I ] denotes the k X P matrix obtained by augmenting the rows of M T with the rows of
N . Now suppose I 6= [k], and hence I0 6= [k]. Then we will show that there are matrices D0 and
E0 arbitrarily close to D and E respectively such that if Y0 = D0E0X then L(Y0) < L(Y). There
is an a ∈ [k] \ I0, and b ∈ I0 such that λa > λb (λb could also be zero). Denote the columns of
the matrix UV as {v1, . . . , vm}, and observe that vi = ui for i ∈ [`] (from the structure of V). For
E > 0 let Ub = (1 + €2)-2(Vb + eua). Define U0 as the matrix which is equal to UV except that
the column vector vb in UV is replaced by u0b in U0. Since a ∈ [k] ⊆ [`] and a ∈/ I0, va = ua and
(UI00 )T UI00 = Ip . Define
D = UI0[M∣Op×(k-p)]C-1 , and E = C[M-1 ](U01 ,)TYXT(XXT)-1
and let Y0 = DE0X. Now observe that, D0E0 = UI, (UI,)TYXT(XXT)-1, and that
22
L(YO) = tr(YYT) - X λi - 1+ e2 (λa - λb) = L(Y) - 1 + e2 (λa - λb)
i∈I	E	E
Since E can be set arbitrarily close to zero, it can be concludedjhat there are points in the neigh-
bourhood of Y such that the loss at these points are less than L(Y). Further, since L is convex with
respect to the parameters in D (respectively E), When the matrix E is fixed (respectively D is fixed)
Y is not a local maximum. Hence, if I = [k] then Y represents a saddle point, and in particular Y
is local/global minima if and only if I = [k].
ProofofClaim C.1. Since Vvec(ET)L(X) is equal to zero, from the second part of Lemma C.1 the
following holds,
X(Y - Y)TD = XYTD — XYTD = 0
⇒ XXT ET DT D = X YT D
Taking transpose on both sides
⇒ DTDEXXT = DTYXT	(12)
Substituting DE as DE in Equation 12, and multiplying Equation 12 by CT on both the sides from
the left, Equation 13 follows.	~ m ~	~	~	~ rπ	~ rτι	~ rπ ⇒ DT D EXXT = DT YXT	(13)
Since D is full-rank, we have	E = (D T D)TD T YX T (X X T )-1.	(14)
and,	D E = PD YX T (X X T )-1	(15)
□
ProofofClaim C.2. Since VVeC(DT)L(Y) is zero, from the first part of Lemma C.1 the following
holds,
EX(Y — Y)T = EXYT — EX ∙ YT = 0
⇒ EXXT ET DT = EXYT	(16)
Substituting ET ∙ DT as ET ∙ DT in Equation 12, and multiplying Equation 16 by CT on both the
sides from the left Equation 17 follows.
E X X T E T D T = EX Y T	(17)
16
Under review as a conference paper at ICLR 2021
Taking transpose of the above equation we have,
D E X X T E T = YX T E T	(18)
From part 1 of Claim C.1, it follows that E has full row-rank, and hence EXXTET is invertible.
Multiplying the inverse of EXXT ET from the right on both sides and multiplying EB from the
left on both sides of the above equation we have,
E BD = (E BYX T E T )(E X X T E T )-1	(19)
This proves part one of the claim. Moreover, multiplying Equation 18 by DT from the right on both
sides
D E X X T E T D T = YX T E T D T
⇒ (PDYXT(XXT)-1 )(XXT)((XXT)-1 XYTPD) = YXT((XXT)-1 XYT ∙ PD)
⇒ PD YX T (X X T )-1 X Y T PD = YX T (X X T )-1 X Y T ∙ PD
The second line the above equation follows by substituting DE = PD YX T (XX T) 1 (from part 2
of Claim C.1). Substituting Σ = YXT(XXT)-1 XYT in the above equation We have
PD ∑PD = ∑ ∙ PD
Since PD = PD, and ΣT = Σ, we also have ΣPD = PDΣ.	□
D Additional Tables and Plots from Section 6
D.1 Plots from Section 6.1
Figure 7 displays the number of parameters in the dense linear layer of the original model and in
the replaced butterfly based network. Figure 9 reports the results for the NLP tasks done as part of
experiment in Section 6.1. Figure 8 displays the number of parameter in the original model and the
butterfly model. Figures 10 and 11 reports the training and inference times required for the original
model and the butterfly model in each of the experiments. The training and and inference times in
Figures 10 and 11 are averaged over 100 runs. Figure 12 is the same as the right part of Figure 1 but
here we compare the test accuracy of the original and butterfly model for the the first 20 epochs.
Figure 7: Number of parameters in the dense linear layer of the original model and in the replaced
butterfly based architecture; Left: Vision data, Right: NLP
D.2 Plots from Section 6.2
Data Matrices: The data matrices are as in Table 2. Gaussian 1 and Gaussian 2 are Gaussian
matrices with rank 32 and 64 respectively. Rank r Gaussian matrices are constructed as follows: r
orthonormal vectors of size 1024 are sampled at random and the columns of the matrix are random
linear combinations of these vectors determined by choosing the coefficients independently and
uniformly at random from the Gaussian distribution with mean 0 and variance 0.01. The data matrix
for MNIST is constructed as follows: each row corresponds to an image represented as a 28 ×
17
Under review as a conference paper at ICLR 2021
30000000
25000000
20000000
ξ 15000000
10000000
5000000
27000000
27000000
25920000 25920000
22344728
LLU
CoNLLL-03 NER(EngIish)
CoNLLL-03 NER (German)
Penn Treebank (English)
Normal model
BF model
Figure 8: Total number of parameters in the original model and the butterfly model; Left: Vision
data, Right: NLP
Figure 10: Training/Inference times for Vision Data; Left: Training time, Right: Inference time
Figure 9: Right: Final F1 Score for different NLP models and data sets. Left: F1 comparison in the
first few epochs with different models on CoNLL-03 Named Entity Recognition (English) with the
flair’s Sequence Tagger
28 matrix (pixels) sampled uniformly at random from the MNIST database of handwritten digits
(LeCun and Cortes, 2010) which is extended to a 32 × 32 matrix by padding numbers close to zero
and then represented as a vector of size 1024 in column-first ordering8. Similar to the MNIST every
row of the data matrix for Olivetti corresponds to an image represented as a 64 × 64 matrix sampled
uniformly at random from the Olivetti faces data set (Cambridge, 1994), which is represented as
a vector of size 4096 in column-first ordering. Finally, for HS-SOD the data matrix is a 1024 ×
768 matrix sampled uniformly at random from HS-SOD — a dataset for hyperspectral images from
natural scenes (Imamoglu et al., 2018).
Figure 13 reports the losses for the Gaussian 2, Olivetti, and Hyper data matrices.
8Close to zero entries are sampled uniformly at random according to a Gaussian distribution with mean zero
and variance 0.01.
18
Under review as a conference paper at ICLR 2021
Figure 11: Training/Inference times for NLP; Left: Training time, Right: Inference time
Figure 12: Comparison of test accuracy in the first 20 epochs with different models and optimizers
on CIFAR-10 with PreActResNet18
Figure 13: Approximation error on data matrix with various methods for various values of k . From
left to right: Gaussian 2, Olivetti, Hyper
E	Missing Plots from Section 7
In this section we state a few additional cases that were done as part of the experiment in Section 7.
Figure 14 compares the test errors of the different methods in the extreme case when k = 1. Figure
15 compares the test errors of the different methods for various values of `. Figure 16 shows the
test error for ` = 20 and k = 10 during the training phase on HS-SOD. Observe that the butterfly
19
Under review as a conference paper at ICLR 2021
learned is able to surpass sparse learned after a merely few iterations. Finally Table 4 compares the
test error for different values of ` and k .
14: Test errors on HS-SOD for ` =
1, zoomed on butterfly and sparse learned in
the right
♦ Butterfly Learned
Figure 15: Test error when k = 10, ` =
, 20, 40,
on HS-SOD, zoomed on butterfly and
sparse learned in the right
Figure 16: Test error when k = 10, ` = 20 during the training phase on HS-SOD

F Bound on Number of Effective Weights in Truncated
Butterfly Network
A butterfly network for dimension n, which we assume for simplicity to be an integral power of 2,
is log n layers deep. Let p denote the integer log n. The set of nodes in the first (input) layer will be
denoted here by V (0). They are connected to the set of n nodes V (1) from the next layer, and so on
until the nodes V (p) of the output layer. Between two consecutive layers V (i) and V (i+1), there are
2n weights, and each node in V (i) is adjacent to exactly two nodes from V (i+1) .
When truncating the network, we discard all but some set S(p) ⊆ V (p) of at most ` nodes in the
last layer. These nodes are connected to a subset S(P-I) ⊆ V(P-I) of at most 2' nodes from the
20
Under review as a conference paper at ICLR 2021
k, ', Sketch	Hyper	Cifar-10	Tech
1, 5, Butterfly	0.0008	0.173	0.188
1, 5, Sparse	0.003	1.121	1.75
1, 5, Random	0.661	4.870	3.127
1, 10, Butterfly	0.0002	0.072	0.051
1, 10, Sparse	0.002	0.671	0.455
1, 10, Random	0.131	1.82	1.44
10, 10, Butterfly	0.031	0.751	0.619
10, 10, Sparse	0.489	6.989	7.154
10, 10, Random	5.712	26.133	18.805
10, 20, Butterfly	0.012	0.470	0.568
10, 20, Sparse	0.139	3.122	3.134
10, 20, Random	2.097	9.216	8.22
10, 40, Butterfly	0.006		0.111
10, 40, Sparse	0.081		0.991
10, 40, Random	0.544		3.304
20, 20, Butterfly	0.058		1.38
20, 20, Sparse	0.229		8.14
20, 20, Random	4.173		15.268
20, 40, Butterfly	0.024		0.703
20, 40, Sparse	0.247		3.441
20, 40, Random	1.334		6.848
30, 30, Butterfly	0.027		1.25
30, 30, Sparse	0.749		7.519
30, 30, Random	3.486		13.168
30, 60, Butterfly	0.014		0.409
30, 60, Sparse	0.331		2.993
30, 60, Random	2.105		5.124
Table 4: Test error for different ` and k
preceding layer using at most 2' weights. By induction, for all i ≥ 0, the set of nodes S(p-i) ⊆
V (p-i) is of size at most 2i ∙', and is connected to the set S(P-iT) ⊆ V (Pi-I) using at most 2i+1∙'
weights.
Now take k = dlog2 (n/`)e. By the above, the total number of weights that can participate in a path
connecting some node in S(P) with some node in V (P-k) is at most
2' + 4' + …+ 2k' ≤ 4n.
From the other direction, the total number of weights that can participate in a path connecting any
node from V(0) with any node from V (P-k) is 2n times the number of layers in between, or more
precisely:
2n(p - k) = 2n(log2 n - dlog2(n/')e) ≤ 2n(log2 n - log2(n/') + 1) = 2n(log ' + 1) .
The total is 2n log ' + 6n, as required.
21