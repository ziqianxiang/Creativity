Under review as a conference paper at ICLR 2021
Learning to Share in Multi-Agent Reinforce-
ment Learning
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the problem of networked multi-agent reinforcement learn-
ing (MARL), where a number of agents are deployed as a partially connected net-
work. Networked MARL requires all agents make decision in a decentralized
manner to optimize a global objective with restricted communication between
neighbors over the network. We propose a hierarchically decentralized MARL
method, LToS, which enables agents to learn to dynamically share reward with
neighbors so as to encourage agents to cooperate on the global objective. For
each agent, the high-level policy learns how to share reward with neighbors to de-
compose the global objective, while the low-level policy learns to optimize local
objective induced by the high-level policies in the neighborhood. The two poli-
cies form a bi-level optimization and learn alternately. We empirically demon-
strate that LToS outperforms existing methods in both social dilemma and two
networked MARL scenarios.
1	Introduction
In multi-agent reinforcement learning (MARL), there are multiple agents interacting with the envi-
ronment via their joint action to cooperatively optimize an objective. Many methods of centralized
training and decentralized execution (CTDE) have been proposed for cooperative MARL, such as
VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), and QTRAN (Son et al., 2019). However,
these methods suffer from the overgeneralization issue (Palmer et al., 2018; Castellini et al., 2019).
Moreover, they may not easily scale up with the number of agents due to centralized learning (Qu
et al., 2020a).
In many MARL applications, there are a large number of agents that are deployed as a partially con-
nected network and collaboratively make decisions to optimize the globally averaged return, such as
smart grids (Dall’Anese et al., 2013), network routing (Jiang et al., 2020), traffic signal control (Chu
et al., 2020), and IoT (Xu et al., 2019). To deal with such scenarios, networked MARL is formulated
to decompose the dependency among all agents into dependencies between only neighbors in such
scenarios. To avoid decision-making with insufficient information, agents are permitted to exchange
messages with neighbors over the network. In such settings, it is feasible for agents to learn to
make decisions in a decentralized way (Zhang et al., 2018; Qu et al., 2020b). However, there are
still difficulties of dependency if anyone attempts to make decision independently, e.g., prisoner’s
dilemma and tragedy ofthe commons (Perolat et al., 2017). Existing methods tackle these problems
by consensus update of value function (Zhang et al., 2018), credit assignment (Wang et al., 2020),
or reward shaping (Chu et al., 2020). However, these methods rely on either access to global state
and joint action (Zhang et al., 2018) or handcrafted reward functions (Wang et al., 2020; Chu et al.,
2020).
Inspired by the fact that sharing plays a key role in human’s learning of cooperation, in this paper,
we propose Learning To Share (LToS), a hierarchically decentralized learning method for networked
MARL. LToS enables agents to learn to dynamically share reward with neighbors so as to collabo-
ratively optimize the global objective. The high-level policies decompose the global objective into
local ones by determining how to share their rewards, while the low-level policies optimize local
objectives induced by the high-level policies. LToS learns in a decentralized manner, and we prove
that the high-level policies are a mean-field approximation of the joint high-level policy. Moreover,
the high-level and low-level policies form a bi-level optimization and alternately learn to optimize
1
Under review as a conference paper at ICLR 2021
the global objective. LToS is easy to implement and currently realized by DDPG (Lillicrap et al.,
2016) as the high-level policy and DGN (Jiang et al., 2020) as the low-level policy. We empirically
demonstrate that LToS outperforms existing methods for networked MARL in both social dilemma
and two real-world scenarios. To the best of our knowledge, LToS is the first to learn to share reward
for global optimization in networked MARL.
2	Related Work
There are many recent studies for collaborative MARL. Most of them adopt centralized training
and decentralized execution, such as COMA (Foerster et al., 2018), VDN (Sunehag et al., 2018),
QMIX (Rashid et al., 2018), and QTRAN (Son et al., 2019). Many are constructed on the basis of
factorizing the joint Q-function by assuming additivity (Sunehag et al., 2018), monotonicity (Rashid
et al., 2018), or factorizable tasks (Son et al., 2019). However, they are learned in a centralized
way and hence may not easily scale up with the number of agents in networked MARL (Qu et al.,
2020a). Moreover, these factorized methods suffer from the overgeneralization issue (Palmer et al.,
2018; Castellini et al., 2019).
Other studies focus on decentralized training specifically in networked MARL, to which our work is
more closely related. Zhang et al. (2018) proposed consensus update of value function, but it requires
global state at each agent, which is usually unavailable in decentralized training. Chu et al. (2020)
introduced a spatial discount factor to capture the influence between agents, but the spatial discount
factor remains hand-tuned. Sodomka et al. (2013) and Peysakhovich & Lerer (2018b) involved the
concept of transferable utility to encourage cooperation, and Peysakhovich & Lerer (2018a) resorted
to game theory and gave more complex reward designs. However, these methods cannot be extended
beyond two-player games. Hughes et al. (2018) proposed the inequity aversion model to balance
agents’ selfish desire and social fairness. Wang et al. (2020) considered to learn the Shapley value
as the credit assignment. However, these methods still rely on hand-crafted reward designs. Mguni
et al. (2019) added an extra part to the original reward as non-potential based reward shaping and
used Bayesian optimization to induce the convergence to a desirable equilibrium between agents.
However, the extra part remains fixed during an episode, which makes it less capable of dealing
with dynamic environments. Moreover, the reward shaping alters the original optimization problem.
3	Background
3.1	Networked Multi-Agent Reinforcement Learning
Assume N agents interact with an environment. Let V = {1, 2,…，N} be the set of agents. The
multi-agent system is modeled as an undirected graph G(V, E), where each agent i serves as vertex
i and E ⊆ V × V is the set of all edges. Two agents i, j ∈ V can communicate with each other if
and only if eij = (i, j) ∈ E. We denote agent i and its all neighbors in the graph together as a set
Ni . The state of the environment s ∈ S transitions upon joint action a ∈ A according to transition
probability Pa : S × A × S → [0, 1], where joint action set A = ×i∈VAi. Each agent i has a
policy πi ∈ Πi : S × Ai → [0, 1], and we denote the joint policy of all agents as π ∈ Π = ×i∈VΠi.
For networked MARL, a common and realistic assumption is that the reward of each agent i just
depends on its action and the actions of its neighbors (Qu et al., 2020a), i.e., ri(s, a) = ri(s, aNi ).
Moreover, each agent i may only obtain partial observation oi ∈ Oi, but can approximate the state
by the observations ofNi (Jiang et al., 2020) or the observation history (Chu et al., 2020), which are
all denoted by oi for simplicity. The global objective is to maximize the sum of cumulative rewards
of all agents , i.e., Pt∞=0 PiN=1 γtrit.
3.2	Markov Game
In such a setting, each agent can individually maximizes its own expected return, which is known
as Markov game. This may lead to stable outcome or Nash equilibrium, which however is usually
sub-optimal. Given π , the value function of agent i is given by
vπ (S) = X π(HS) X Pa(S0|s, a)[ri + Yvi (SO)],	⑴
a	s0
2
Under review as a conference paper at ICLR 2021
where pa ∈ Pa describes the state transitions. A Nash equilibrium is defined as (Mguni et al., 2019)
vi(πi,π-i)(s) ≥ vi(πi0,π-i)(s),	∀πi0 ∈ Πi,∀s ∈ S,∀i ∈ V,
(2)
where ∏-i = ×j∈v∖{i}∏j.
4	Method
The basic idea of LToS is to enable agents to learn how to share reward with neighbors such that
agents are encouraged to collaboratively optimize the global objective in networked MARL. LToS
is a decentralized hierarchy. At each agent, the high-level policy determines the weights of reward
sharing based on low-level policies while the low-level policy directly interacts with the environ-
ment to optimize local objective induced by high-level policies. Therefore, they form a bi-level
optimization and alternately learn towards the global objective.
4.1	Reward Sharing
The intuition of reward sharing is that if agents share their rewards with others, each agent has to
consider the consequence of its actions on others, and thus it promotes cooperation. In networked
MARL, as the reward of an agent is assumed to depend on the actions of neighbors, we allow reward
sharing between neighboring agents.
For the graph of V , we additionally define a set of directed edges, D, constructed from E . Specif-
ically, we add a loop dii ∈ D for each agent i and split each undirected edge eij ∈ E into two
directed edges: dij = (i, j) and dji = (j, i) ∈ D. Each agent i determines a weight wij ∈ [0, 1] for
each directed edge dij, ∀j ∈ Ni, subject to the constraint Pj∈N wij = 1, so that wij proportion of
agent i’s environment reward ri will be shared to agent j. Let w ∈ W = ×dij∈Dwij be the weights
of the graph. Therefore, the shaped reward after sharing for each agent i is defined as
riw = X wjirj .	(3)
j∈Ni
4.2	Hierarchy
Assume there is a joint high-level policy φ ∈ Φ : S × W → [0, 1] to determine w. Given φ and w,
we can define the value function of π at each agent i based on (1) as
Vi(s; φ) = X φ(w∣s) X ∏(a∣s, W) XPa(SlS, a)[rw + γvπ(s0； φ)],	(4)
w	a	s0
vi(s； w, Φ) = X∏(a∣s, W) XPa(Sls, a)[rw + Yvi(s'； Φ)]∙	(5)
a	s0
We express W as a discrete action for simplicity. It also holds for continuous action as long as we
change all the summations to integrals.
Let Vφ(SK) = pi∈v vπ(s；φ) and QV(S, w；π) = pi∈v vπ(s；w, φ).
Proposition 4.1. Given π, V-φ(s; π) and QV(s, w; π) are respectively the value function and
action-value function of φ.
Proof. Let rv = Pa π(a∣s, w)rW and Pw(s0∣s, w) = Pa π(a∣s, W)Pa(S0∣s, a). As commonly
assumed the reward is deterministic given s and a, from (4), we have,
vπ(s; φ) = X φ(w∣s) X ∏(a∣s, w)[rw + XPa(S0∣s, a)γvπ(s0; φ)]	(6)
w	a	s0
=X Φ(w∣s) XPw(s0∣s, w)[rφ + γvπ(s0; φ)],	(7)
w	s0
where Pw ∈ Pw : S × W × S → [0, 1] describes the state transitions given π.
3
Under review as a conference paper at ICLR 2021
Let rVφ =. Pi∈V riφ, and from (7) we have
Vφ(s; ∏) = XXφ(w∣s) XPw(s0∣s, w)[rφ + yV(s0; φ)]	(8)
i∈V w	s0
=X φ(WIS) XPw(S0|s, W)X rφ + Y X vπ(S0； φ)]	⑼
w	s0	i∈V	i∈V
=X Φ(w∣s) XPw(s0∣s, w)[rφ + γV∙φ(s0; ∏)],	(10)
and similarly,
Qφ(s, w； ∏) = XX Pw(s0∣s, w)[rφ + Y X φ(w0∣s0)vπ (s0； w0, φ)]	(11)
i∈V s0	w0
= X Pw(S0|S, W)[Xriφ + Y X φ(w0∣s0) X v^π(s0； W0, φ)]	(12)
s0	i∈V	w0	i∈V
=XPw(s0∣s, w)[rφ + Y X Φ(w0∣s0)Qφ(s0, W0； ∏)]∙	(13)
s0	w0
Moreover, from the definitions of riw and riφ we have
rφ = X n(a|s, W) X rw = X π(als, W) X X Wjirj	(14)
a	i∈V	a	i∈V j∈Ni
=En(Hs, W) E wij ri = En(a|s, W) Eri,	(15)
a	(i,j)∈D	a	i∈V
Thus, given π, VVφ(S) and QVφ(S, W) are respectively the value function and action-value function
of φ in terms of the sum of expected cumulative rewards of all agents, i.e., the global objective. □
Proposition 4.1 implies that φ directly optimizes the global objective by generating W given π.
Unlike existing hierarchical RL methods, we can directly construct the value function and action-
value function of φ based on the value function of π at each agent.
As φ optimizes the global objective given π while πi optimizes the shaped reward individually
at each agent given φ (assuming π convergent to Nash equilibrium or stable outcome, denoted as
lim), they form a bi-level optimization. Let Jφ(π) and Jπ(φ) denote the objectives of φ and π
respectively. The bi-level optimization can be formulated as follows,
max Jφ(∏*(φ))
φ
s.t. π*(φ) = arglim Jn (φ).
π
(16)
4.3 Decentralized Learning
Proposition 4.2. The joint high-level policy φ can be learned in a decentralized manner, and the
decentralized high-level policies of all agents form a mean-field approximation of φ.
Proof. Let dij ∈ D serve as a vertex with action wij and reward wij ri in a new graph G0. Each ver-
tex has its own local policy φij (wij |S), and we can verify their independence by means of Markov
Random Field. For ∀i ∈ V, {dij|j ∈ Ni} should form a fully connected subgraph in G0, be-
cause their actions are subject to the constraint Pj∈N wij = 1. As dij ∈ G0 only connects to
{dik|k ∈ Ni\{j}}, the fully connected subgraph is also a maximal clique. According to Hammers-
ley-Clifford theorem (HammerSley & Clifford, 1971), We have Φ(w∣s) ≈ Qi∈v φi(w；ut|s), where
wout = {wj j ∈ Ni}.	□
Proposition 4.1 and 4.2 indicate that for each agent i, the low-level policy simply learns a local
∏i(ai∣s, win), where Win = {wji∣j ∈ Ni}, to optimize the cumulative reward of riw, since riw is
4
Under review as a conference paper at ICLR 2021
fully determined by wiin according to (3) and denoted as riw from now on. And the high-level policy
φi just needs to locally determine wiout to maximize the cumulative reward of riφ simplified as riφ .
Therefore, for decentralized learning, (16) can be decomposed locally for each agent i as
max Jφi(φ-i,∏1(φ),…，∏N(φ))
φi
s.t. ∏i(φ) = arg max Jni (∏-i,φι(π),…，Φn (π)),
πi
(17)
We abuse the notation and let φ and π also denote their parameterizations respectively. To solve the
optimization, we have
VφiJφi(Φ-i,∏MΦ),…，∏N(Φ))	(18)
≈ Vφi Jφi (φ-i,∏ι + αV∏ι J∏1 (φ),…，∏N + αV∏N JnN (φ)),
where α is the learning rate for the low-level policy. Let ∏i denote ∏ + αV∏i J∏i (φ), We have
vΦi Jφi (φ-i, πι(φ), …，πN (O))
N
≈ vφi Jφi (φ-i,π1,…，πN ) + αE vφi,∏j J∏j (O)VnjJφi (φ-i,π1,…，πN ).
j=1
(19)
The second-order derivative is neglected due to high computational complexity, without incurring
significant performance drop such as in meta-learning (Finn et al., 2017) and neural architecture
search (Liu et al., 2019). Similarly, we have
VniJni(∏-i,Φ1(∏),…，ΦN(∏))
≈ VniJni (∏-i, φl + βVφι Jφι (∏),…，0N + βVN Jφ. (∏)),
(20)
where β is the learning rate of the high-level policy. Therefore, we can solve the bi-level optimiza-
tion (16) by the first-order approximations in a decentralized way. For each agent i, φ% and ∏ are
alternately updated.
In distributed learning, as each agent i usually does not
have access to state, we further approximate φi (wout∣s)
and ∏i(a∕s,win) by φi(wςou^∖θi) and ∏i(a∕θi,win), re-
spectively. Moreover, in network MARL as each agent
i is closely related to neighboring agents, (17) can be fur-
ther seen as ∏ maximizes the cumulative discounted re-
ward of rW given Φn, where Φn = ×j∈Niφj, and φi
optimizes the cumulative discounted reward of rW given
∏Ni (i.e., rφ), where KNi = ×j∈Ni∏j. During training,
∏Ni and φNi are implicitly considered by interactions of
wout and Win respectively. The architecture of LToS is
illustrated in Figure 1. At each timestep, the high-level
policy of each agent i makes a decision of action wout as
agent i
the weights of reward sharing based on the observation.	Figure 1: LTOS
Then, the low-level policy takes the observation and Win
as an input and outputs the action. Agent i obtains the shaped reward according to Win for both the
high-level and low-level policies. The gradients are backpropagated along purple dotted lines.
Further, from Proposition 4.1, we have qφi (s, wout; ∏Ni) = viri (s; win, ΦNi), where qφi is the action-
value function of φi given πNi, v『i is the value function of ∏ given φNi and conditioned on win. As
aforementioned, we approximately have qφi (θi, wout) = Vn M; win). We can see that the action-
value function of φi is equivalent to the value function of ∏i. That said, we can use a single network
to approximate these two functions simultaneously. For a deterministic low-level policy, the high-
level and low-level policies can share a same action-value function. In the current instantiation of
LToS, we use DDPG (Lillicrap et al., 2016) for the high-level policy and DGN (Jiang et al., 2020)
(Q-learning) for the low-level policy. Thus, the Q-network of DGN also serves the critic of DDPG,
and the gradient of Win is calculated based on the maximum Q-value of a®. More discussions about
training LToS and the detailed training algorithm are available in Appendix A.1.
5
Under review as a conference paper at ICLR 2021
-cooperate-
(a) prisoner
Figure 2: Three experimental scenarios: (a)prisoner, (b) traffic, and (c) routing.
(b) traffic
5 Experiments
For the experiments, We adopt three scenarios depicted in Figure 2. Prisoner is a grid game about
social dilemma that easily measures agents, cooperation, while traffic and routing are real-world
scenarios of networked MARL. We obey the principle of networked MARL that only allows com-
munication in neighborhood as Jiang et al. (2020); Chu et al. (2020).
To illustrate the reward sharing scheme each agent learned, we use a simple indicator: selfishness, the
reward proportion that an agent chooses to keep for itself. For ablation, we keep the sharing weights
fixed for each agent, named fixed LToS. Throughout the experiments, we additionally compare with
the baselines including DQN, DGN, QMIX and two methods for networked MARL, i.e., ConseNet
(Zhang et al., 2018) and NeurComm (ChU et al., 2020), both of which take advantage of recurrent
neural network (RNN) for the partially observable environment (Hausknecht & Stone, 2015). To
maximize the average global reward directly, we specially tune the reward shaping factor of other
baselines in prisoner and introduce QMIX as a centralized baseline in traffic and routing. Moreover,
as DGN is the low-level policy of LToS, DGN also serves the ablation of LToS without reward
sharing.
5.1 Prisoner
We use prisoner, a grid game version of the well-known matrix game prisoner's dilemma from
Sodomka et al. (2013) to demonstrate that LToS is able to learn cooperative policies to achieve
the global optimum (i.e., maximize globally averaged return). As illustrated in Figure 2a, there
are two agents A and B that respectively start on two sides of the middle of a grid corridor with
full observation. At each timestep, each agent chooses an action left or right and moves to the
corresponding adjacent grid, and each timestep every action incurs a cost -0.01. There are three
goals, two goals at both ends and one in the middle. The agent gets a reward +1 for reaching the goal.
The game ends once some agent reaches a goal or two agents reach different goals simultaneously.
This game resembles prisoner’s dilemma: going for the middle goal (“defect”) will bring more
rewards than the farther one on its side (“cooperate”), but if two agents both adopt that, a collision
occurs and only one of the agents wins the goal with equal probability. On the contrary, both agents
obtain a higher return if they both “cooperate”, though it takes more steps.
Figure 3 illustrates the learning curves of all the models in
terms of average return. Note that for all three scenarios,
we represent the average of three training runs with different
random seeds by solid lines and the min/max value by shad-
owed areas. As a result of self-interest optimization, DQN
converges to the “defect/defect” Nash equilibrium where each
agent receives an expected reward about 0.5. So does DGN
since it only aims to take advantage of its neighbors’ observa-
tions while prisoner is a fully observable environment already.
ConseNet agents sometimes choose to cooperate by building a
consensus on average return at the beginning, but it is unstable
Figure 3: Learning curves in prisoner.
6
Under review as a conference paper at ICLR 2021
and abandoned subsequently. Given a hand-tuned reward shaping factor to direct agents to maximize
average return, NeurComm and fixed LToS agents are able to cooperate eventually. However, they
converge much slower. Coco-Q (Sodomka et al., 2013) and LToS outperform all other methods. As
a modified tabular Q-learning method, Coco-Q introduces the coco value (Kalai & Kalai, 2010) as
a substitute for the expected return in the Bellman equation and regards the difference as transferred
reward. However, it is specifically designed for some games, and it is hard to be extended beyond
two-player games. LToS can learn the reward sharing scheme where one agent at first gives all the
reward to the other so that both of them are prevented from “defect”, and thus achieve the best av-
erage return quickly. By prisoner, we verify LToS can escape from local optimum by learning to
share reward.
5.2 Traffic
In traffic, as illustrated in Figure 2b , we aim to investigate the capa-
bility of LToS in dealing with highly dynamic environment through
reward sharing. We adopt the same problem setting as in (Wei et al.,
2019). In a road network, each agent serves as traffic signal con-
trol at an intersection. The observation of an agent consists of a
one-hot representation of its current phase (directions for red/green
lights) and the number of vehicles on each incoming lane of the
intersection. At each timestep, an agent chooses a phase from the
pre-defined phase set for the next time interval, i.e., 10 seconds. The
reward is set to be the negative of the sum of the queue lengths of
Table 1: Statistics of traffic flows
Time Arrival Rate
(second) (vehicles/s)
0 - 600	1
600 - 1, 200	1/4
1,	200 - 1, 800	1/3
1, 800 - 2, 400	2
2,	400 - 3, 000	1/5
3,	000 - 3, 600	1/2
all approaching lanes at current timestep. The global objective is to minimize average travel time of
all vehicles in the road network, which is equivalent to minimizing the sum of queue lengths of all
intersections over an episode (Zheng et al., 2019). The experiment was conducted on a traffic simu-
lator, CityFlow (Zhang et al., 2019). We use a 6 × 6 grid network with 36 intersections. The traffic
flows were generated to simulate dynamic traffic flows including both peak and off-peak period, and
the statistics is summarized in Table 1.
Table 2: Average travel time of all the models in traffic
DQN	DGN	fixed LToS	LToS	NeurComm	ConseNet	QMIX
118.75	110.59	113.83	98.57	106.53	111.18	596.52
Figure 4 shows the learning curves of all the models in terms
of average travel tiem of all vehicles in logarithmic form.
The performance after convergence is summarized in Table 2,
where LToS outperforms all other methods. LToS outperforms ”
DGN, which demonstrates the reward sharing scheme learned ∣
by the high-level policy indeed helps to improve the coopera- ⅛
tion of agents. Without the high-level policy, i.e., given fixed W
sharing weights, fixed LToS does not perform well in dynamic
environment. This indicates the necessity of the high-level pol-
icy. Although NeurComm and ConseNet both take advantage
of RNN for partially observable environments, LToS still out- Figure 4: Learning curves in traffic.
performs these methods, which verifies the great improvement
of LToS in networked MARL. QMIX shows apparent instability and is confined to suboptimality
(Mahajan et al., 2019). Specifically, in the best episode, QMIX tries to release traffic flows from one
direction while stopping flows from the other all the time.
We visualize the variation of selfishness of all agents during an episodes in Figure 5 and 6. Figure 5
depicts the temporal variance of selfishness for each agent. For most agents, there are two valleys
occurred exactly during two peak periods (i.e., 0 - 600s and 1, 800 - 2, 400s). This is because for
heavy traffic agents need to cooperate more closely, which can be induced by being less selfish.
We can see this from the fact that selfishness is even lower in the second valley where the traffic is
even heavier (i.e., 2 vs. 1 vehicles/s). Therefore, this demonstrates that the agents learn to adjust
their extent of cooperation to deal with dynamic environment by controlling the sharing weights.
Figure 6 shows the spatial pattern of selfishness at different timesteps, where the distribution of
agents is the same as the road network in Figure 2b. The edge and inner agents tend to have very
7
Under review as a conference paper at ICLR 2021
Figure 5: Temporal pattern of selfishness
Figure 6: Spatial pattern of selfishness
different selfishness. In addition, inner agents keep their selfishness more uniform during off-peak
periods, while they diverge and present cross-like patterns during peak periods. This shows that
handling heavier traffic requires more diverse reward sharing schemes among agents to promote
more sophisticated cooperation.
5.3 Routing
Table 3: Statistics of packet floW
Time Arrival Rate
(timestep) (packets/timestep)
0 - 100	1
100 - 200	10/3
200 - 300	1
Packet routing is regarded as a complex problem in distributed com-
puter networks. Here is a simplified version of the problem. A net-
work consists of multiple routers with a stationary network topol-
ogy. Data packets come into the network (started at a router) fol-
lowing the Poisson distribution, and the arrival rate varies during an
episode as summarized in Table 3. Each router has a FIFO queue
as the packet buffer. For simplicity, We assume that each queue has
unlimited volume allowance, and each packet has a size same as each link’s bandwidth. At every
timestep, each router observes the data packets in the queue and incoming links as Well as indices
of neighboring routers, forWards the first packet in the FIFO to the selected next hop, and obtains a
reWard Which is the negative of the queue length. The transmission time of a packet over a link is
proportional to the geographic distance, and the packet Will be stored after arriving at the next hop
unless it reaches the destination. The delay of a packet is the sum of timesteps spent at the routers
and on the links. The goal of packet routing is to send the packets to their destinations through
hop-by-hop transmissions With minimum average delay. Compared With traffic, routing is a more
fine-grained task, because it requires specific control for each data packet.
In the experiment, We choose a real netWork topology: IBM
backbone netWork of 18 vertices that each Works for a city in
North America (Knight et al., 2011) and the topology is de-
picted in Figure 2c, Where each edge consists of tWo unidirec-
tional links and varies considerably in distance. We assume
that each router helps With loopback detection While forWard-
ing. Figure 7 illustrates the learning curves of all the models
in terms of average delay, and their performance after conver-
gence in terms of throughout and delay is also summarized in
Table 4. NeurComm, ConseNet and QMIX are not up to this
task and may need much more episodes to converge. By learn-
ing proper reWard sharing, LToS outperforms all other base-
Figure 7: Learning curves in routing.
lines in terms of both metrics. Compared to traffic, routing additionally considers the heterogeneous
netWork topology. Therefore, the experimental results also verify the capability of LToS of handling
both temporal and spatial heterogeneity in netWorked MARL.
Table 4: Performance of all models in routing: throughput (packets) and average delay (timesteps)
	DQN	DGN	fixed LToS	LToS	NeurComm	ConseNet	QMIX
throughput	297.69	299.24	304.99	311.50	116.53	185.32	218.79
delay	91.06	90.96	89.50	86.71	122.68	111.87	105.74
8
Under review as a conference paper at ICLR 2021
6 Conclusion
In this paper, we proposed LToS, a hierarchically decentralized method for networked MARL. LToS
enables agents to share reward with neighbors so as to encourage agents to cooperate on the global
objective. For each agent, the high-level policy learns how to share reward with neighbors to decom-
pose the global objective, while the low-level policy learns to optimize local objective induced by
the high-level policies in the neighborhood. Experimentally, we demonstrate that LToS outperforms
existing methods in both social dilemma and two networked MARL scenarios.
References
Jacopo Castellini, Frans A Oliehoek, Rahul Savani, and Shimon Whiteson. The representational
capacity of action-value networks for multi-agent reinforcement learning. In AAMAS, 2019.
Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for net-
worked system control. In ICLR, 2020.
E. Dall’Anese, H. Zhu, and G. B. Giannakis. Distributed optimal power flow for smart microgrids.
IEEE Transactions on Smart Grid, 4(3):1464-1475, 20l3.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In ICML, 2017.
Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI, 2018.
J. M. Hammersley and P. Clifford. Markov fields on finite graphs and lattices, 1971.
Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially observable mdps.
arXiv:1507.06527, 2015.
Edward Hughes, Joel Z. Leibo, Matthew G. Phillips, Karl Tuyls, Edgar A. Dueiiez-Guzman, Anto-
nio Garcia Castafieda, Iain Dunning, Tina Zhu, Kevin R. McKee, Raphael Koster, Heather Roff,
and Thore Graepel. Inequity aversion improves cooperation in intertemporal social dilemmas. In
NeurIPS, 2018.
Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement
learning. In ICLR, 2020.
Adam Tauman Kalai and Ehud Kalai. Cooperation and competition in strategic games with private
information. In EC, 2010.
S. Knight, H.X. Nguyen, N. Falkner, R. Bowden, and M. Roughan. The internet topology zoo. IEEE
Journal on Selected Areas in Communications, 29(9):1765 -1775, 2011.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR,
2016.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In
ICLR, 2019.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. In NeurIPS, 2019.
David Mguni, Joel Jennings, Emilio Sison, Sergio Valcarcel Macua, Sofia Ceppi, and En-
rique Munoz de Cote. Coordinating the crowd: Inducing desirable equilibria in non-cooperative
systems. In AAMAS, 2019.
Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep rein-
forcement learning. In AAMAS, 2018.
9
Under review as a conference paper at ICLR 2021
Julien PerolaL Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore GraePeL A
multi-agent reinforcement learning model of common-pool resource appropriation. In NeurIPS,
2017.
Alexander Peysakhovich and Adam Lerer. Consequentialist conditional cooPeration in social dilem-
mas with imPerfect information. In ICLR, 2018a.
Alexander Peysakhovich and Adam Lerer. Prosocial learning agents solve generalized stag hunts
better than selfish ones. In AAMAS, 2018b.
Chao Qu, Hui Li, Chang Liu, Junwu Xiong, James Zhang, Wei Chu, Yuan Qi, and Le Song. Intention
ProPagation for multi-agent reinforcement learning. arXiv:2004.08883, 2020a.
Guannan Qu, Yiheng Lin, Adam Wierman, and Na Li. Scalable multi-agent reinforcement learning
for networked systems with average reward. In NeurIPS, 2020b.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deeP multi-agent reinforce-
ment learning. In ICML, 2018.
Eric Sodomka, Elizabeth Hilliard, Michael Littman, and Amy Greenwald. Coco-q: Learning in
stochastic games with side Payments. In ICML, 2013.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooPerative multi-agent reinforcement learning. In ICML,
2019.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore GraePel. Value-
decomPosition networks for cooPerative multi-agent learning based on team reward. In AAMAS,
2018.
Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yunjie Gu. ShaPley q-value: A local reward
aPProach to solve global reward games. In AAAI, 2020.
Hua Wei, Nan Xu, Huichu Zhang, Guanjie Zheng, Xinshi Zang, Chacha Chen, Weinan Zhang,
Yanmin Zhu, Kai Xu, and Zhenhui Li. Colight: Learning network-level cooPeration for traffic
signal control. In CIKM, 2019.
Yue Xu, Zengde Deng, Mengdi Wang, Wenjun Xu, Anthony Mancho So, and Shuguang Cui. Voting-
based multi-agent reinforcement learning for intelligent iot. arXiv:1907.01385, 2019.
Huichu Zhang, Siyuan Feng, Chang Liu, Yaoyao Ding, Yichen Zhu, Zihan Zhou, Weinan Zhang,
Yong Yu, Haiming Jin, and Zhenhui Li. Cityflow: A multi-agent reinforcement learning environ-
ment for large scale city traffic scenario. In WWW, 2019.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Bayar. Fully decentralized multi-
agent reinforcement learning with networked agents. In ICML, 2018.
Guanjie Zheng, Xinshi Zang, Nan Xu, Hua Wei, Zhengyao Yu, Vikash Gayah, Kai Xu, and Zhenhui
Li. Diagnosing reinforcement learning for traffic signal control. arXiv:1905.04716, 2019.
A	Appendix
A.1 Algorithm and Discussions
As a hierarchically decentralized MARL method, LToS brings some challenges for training. Algo-
rithm 1 Presents the training algorithm of LToS.
Selfishness Initializer. On the basis ofa straightforward idea that one should generally focus more
on its own reward than that of others when oPtimizing its own Policy, the initial outPut of each
high-level Policy network is suPPosed to be higher on the sharing weight of its own than others. We
10
Under review as a conference paper at ICLR 2021
Algorithm 1 LToS
1:	Initialize φ% parameterized by θ% and ∏ parameterized by μ% for each agent i (φi is learned using
DDPG and πi is learned using DGN, where they share the Q-network)
2:	for episode = 1 to max-training-round do
3:	Initialize a random process Xw for w-action exploration
4:	Initialize a random process Xa for a-action exploration
5:	for max-episode-length do
6:	for each agent i do
7:	wout J φi(θi) + XW
8:	ai - ∏i(θi; Win) + Xa
9:	Execute action ai , obtain original reward ri , and transition to o0i
10:	Exchange wiout and ri , and get wiin and riw
11:	Store (oi , wiin, ai , riw , o0i )
12:	end for
13:	if time to update then
14:	for each agent i do
15:	Sample a minibatch D = {(oi, wiin, ai, riw, o0i)} from replay buffer Bi
16:	Exchange wiout0 J φ0i(o0i) and get wiin0
00
17:	Set y - rw + Yqii(oi,ai； win )lai=∏o(oiw∏0)
18:	UPdate πi by V“i 由 P(θi,win,ai,rW,oi)∈D 3一7 (OMai win))2
19:	Exchange wiout J φi(oi) and get wiin
20:	Calculate the gradient gin = Vwinq∏i (θi, arg maxai q∏i; win)
21:	Exchange giin and get gradient giout for wiout
22:	UPdate θi by |D| Poi∈D(vθiφi(OiNgoU
23:	Softly update θi and μi: θi - τθ% + (1 — τ)θ0 and μi - τμi + (1 — τ)μi
24:	end for
25:	end if
26:	end for
27:	end for
choose to predetermine the initial selfishness to learn the high-level policy effectively. However, with
normal initializers, the output of the high-level policy network will be evenly distributed initially.
Therefore, we use a special selfishness initializer for each high-level policy network instead. As we
use the softmax to produce the weights, which guarantees the constraint: Pj∈N wij = 1, ∀i ∈ V,
we specially set the bias of the last fully-connected layer so that each decentralized high-level policy
network tends to keep for itself the same reward proportion as the given selfishness initially. The rest
of reward is still evenly distributed among neighbors. LToS learns started from such initial weights,
while fixed LToS uses such weights throughout each experiment. Moreover, we use grid search to
find the best selfishness for fixed LToS in traffic and routing. For prisoner we deliberately set the
selfishness to 0.5 so that fixed LToS directly optimizes the average return.
Unified Pseudo Random Number Generator. LToS is learned in a decentralized manner. This
incurs some difficulty for experience replay. As each agent i needs wiin to update network weights
for both high-level and low-level policies, it should sample from its buffer a batch of experiences
where each sampled experience should be synchronized across the batches of all agents (i.e., the
experiences should be collected at a same timestep). To handle this, all agents can simply use a
unified pseudo random number generator and the same random seed.
Different Time Scales. As many hierarchical RL methods do, we set the high-level policy to
running at a slower time scale than the low-level one. Proposition 4.1 still holds if we expand viπ
for more than one step forward. Assuming the high-level policy runs every M timesteps, we can fix
wout,t = wοut,t+1 =…=wοut,t+M-1. M is referred to as action interval in Table 6.
Infrequent Parameter Update with Small Learning Rate. Based on the continuity of w, a small
modification of φ means a slight modification of local reward functions, and will intuitively result
11
Under review as a conference paper at ICLR 2021
Table 5: Hyperparameters for DQN and DGN (also serves as the low-level policy network of LToS)
Hyperparamater	Prisoner	Traffic	Routing
sample size	10	1,000	10
batch size	10	20	10
buffer capacity	200,000	10,000	200,000
/decay/minimum value	0.8/1/0.8	0.4/0.9/0.05	0.2/0.98/0
initializer	random normal	random normal	random normal
optimizer	Adam	Adam	Adam
learning rate	1e-3	1e-3	1e-3
γ	0.99	0.8	0.99
τ for soft update	0.1	0.1	0.1
# MLP units	32	32	128
MLP activation	ReLU	ReLU	ReLU
# encoder MLP layers	2	2	2
# attention heads for DGN	4	1	8
# convolutional layers for DGN	1	1	1
Table 6: Hyperparameters for the high-level policy network of LToS
Hyperparamater	Prisoner	Traffic	Routing
update frequency	1 step	5 episodes	20 episodes
action interval	1 step	15 steps	30 steps
sample size	2,000	1,000	2,000
batch size	32	20	32
noise for exploration	+ Gaussian	OU	OU
noise parameter	= 0.8, σ = 1	σ = 0.25	σ = 0.025
initializer	selfishness	selfishness	selfishness
initial selfishness	0.5	0.8	0.9
optimizer	SGD	SGD	SGD
learning rate	1e-1	1e-3	1e-3
last MLP layer activation	softmax	softmax	softmax
Table 7: Hyperparameters for NeurComm, ConseNet and QMIX
Hyperparamater	Prisoner	Traffic	Routing
initializer	orthogonal	orthogonal	orthogonal
optimizer	RMSProp	RMSProp	RMSProp
learning rate	5e-3	5e-4	5e-4
# MLP units	20	16	128
MLP activation	ReLU	ReLU	ReLU
# cell state units	20	16	128
# hidden state units	20	16	128
RNN type for NeurComm and ConseNet	LSTM	LSTM	LSTM
RNN type for QMIX	-	GRU	GRU
hypernetwork layer1 units for QMIX	-	36 × 16	18 × 128
hypernetwork layer2 units for QMIX	-	16	128
α for NeurComm	1	0.1	0.05
in an equally slight modification of the low-level value functions. This guarantees the low-level
policies are highly reusable.
A.2 Hyperparamaters
Table 5 summarizes the hyperparameters of DQN and DGN that also serves as the low-level network
of LToS. We follow many of the original DGN in prisoner and routing, but choose the setting of
Wei et al. (2019) in traffic for consistency. Table 6 summarizes the hyperparameters of the high-
level network of LToS, which are different from the low-level network. Table 7 summarizes the
12
Under review as a conference paper at ICLR 2021
hyperparameters of NeurComm and ConseNet, which adhere to the implementation (Chu et al.,
2020). In addition, for tabular Coco-Q, the step-size parameter is 0.5. We adopt soft update for both
high-level and low-level networks and use an Ornstein-Uhlenbeck Process (abbreviated as OU) for
high-level exploration.
Both fixed LToS and NeurComm exploit static reward shaping, but they adopt different reward
shaping schemes which are hard to compare directly. We consider a simple indicator: Self Neighbor
Ratio (SNR), the ratio of reward proportion that an agent chooses to keep for itself to that it obtains
from a single neighbor. As the rest reward is evenly shared with neighbors in LToS, for each agent i,
We have SNR = Selfishness∕ι-seifishness X (|Ni| — 1) for LToS, and SNR = 1∕α for NeurComm where ɑ
is the spatial discount factor. We adjust the initial selfishness and α to set the SNR of both methods
at the same level for fair comparison.
13