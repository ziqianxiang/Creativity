Under review as a conference paper at ICLR 2021
What can we learn from gradients
Anonymous authors
Paper under double-blind review
Ab stract
Recent work (Zhu, Liu, and Han 2019) has shown that it is possible to reconstruct
the input (image) from the gradient of a neural network. In this paper, our aim is to
better understand the limits to reconstruction and to speed up image reconstruction
by imposing prior image information and improved initialization. Exploring the
theoretical limits of input reconstruction, we show that a fully-connected neural
network with a single hidden node is enough to reconstruct a single input image,
regardless of the number of nodes in the output layer. Then we generalize this
result to a gradient averaged over mini-batches of size B. In this case, the full mini-
batch can be reconstructed in a fully-connected network if the number of hidden
units exceeds B. For a convolutional neural network, the required number of filters
in the first convolutional layer again is decided by the batch size B, however,
in this case, input width d and the width after filter d0 also play the role h =
(d )2 BC, where C is channel number of input. Finally, We validate and underpin
our theoretical analysis on bio-medical data (fMRI, ECG signals, and cell images)
and on benchmark data (MNIST, CIFAR100, and face images).
1	Motivation
Privacy and security are major concerns in Federated Learning (FL, Konecny et al. (2016)), e.g.,
the member participation attack Kairouz et al. (2019) revealing the presence of certain data. An
important general defence is differential privacy Dwork (2008). For a relaxed differential privacy
mechanism, (, δ), the information leakage is quantified as exp() under the bounded leakage prob-
ability δ. Another category consists of adversarial attacks that attempt to prevent a model from being
learned at all (Bhagoji et al., 2019; Sun et al., 2019; Hitaj et al., 2017). Typically, the attacker em-
ploys an additive GAN model to learn a generative model with the gradient updates collected from
the distributed devices, even if the attacker has no access to input data.
Recent work (Zhu et al., 2019) showed that an attacker may reconstruct the training data in the FL
environment. In particular, the attacker does not require much auxiliary information; basically they
can pretend to be an honest server or participant (with only knowledge of the gradient update and
the model parameters).
Investigating the limits to this highly interesting demonstration we propose to invoke image prior
information and a new initialization mechanism the primary aim is to speed up the convergence rate
of reconstruction, while the secondary aim is to also increases the numerical stability. Moreover,
we offer a theoretical analysis of the limitations of the reconstruction for a fully-connected neural
network and vanilla convolutional neural network. Our main contributions are:
•	We introduce prior image knowledge and a new initialization to increase the convergence
rate and successful reconstruction.
•	We show that a fully-connected neural network only needs a single node in one hidden
layer to reconstruct a single input image, regardless of the number of nodes in the output
layer, as long as the output layer has bias term. We generalize the result to mini-batch
reconstruction and show that the number of hidden units has to exceed the size of the
mini-batch.
•	For a convolutional neural network, the number of filters in the first convolutional layer
decides whether reconstruction is possible, namely, the number of filters should be such
that size of output after passing through the first convolutional layer exceeds the size of
original input.
1
Under review as a conference paper at ICLR 2021
Algorithm 1 Average Federated Learning
1:	Initialization: w0
2:	for t=1,...T do
3:	=>devices:
4:	for j=1,2,..,p do p devices (in Parallel)
5:	Vj = Vaj(f ({xjk},wt), {yjk})
6:	With ({xjk}k=ι,..κ,{yjk}k=ι,...,κ)〜D
7:	share vjt with server
8:	end
9:	=>server:
io：	wt+1 = wt	- η	×	P Pp =ι	Vj
11:	share wt+1	With	devices for next round
12:	end
13:	end
2 Federated Learning
In the FL setup, We have a server and a collection of distributed devices jointly training a global
model With the knoWledge of the combined environment. The collection of devices oWn their local
data, and based on Which, update the gradients With the server. Thus, the global empirical loss
function `(.) can be approximated by average of local loss functions `i, shoWn in equation (1).
Note that here We consider the data is independent and identically distributed across all the devices,
Which is not necessary for practice. We define m = Pip=1 mi Where mi is the amount of data on
device i.
p
α(X) = X mm* iE(χ,y)~D['i(fw(x),y)]
i=1
1p
PEE(x,y)〜D [K(fw (x),y)](mi = mj, ∀i,j)
p i=1
(1)
Each iteration consists in tWo stages: local training and aggregation. More specifically, after dis-
tributed devices have completed local training, they share the corresponding gradients With the
server, and the server aggregates the gradients based on the given criterion, updates the global model,
and finally it sends the updated global model back to the devices. The devices Will use this model
for the next iteration. The procedure is depicted in Algorithm 1. We assume that We have p active
devices participating in every round, and all the devices share the same batch size K, Which induces
the so-called Average Federated Learning (average gradients update, Line 10 in Algorithm 1). Data
of all devices is assumed to be generated by the same underlying data distribution D, denoted the
empirical distribution D.
3	Related Work
In the FL setting, a generative adversarial netWork (GAN)-based attack has been proposed as a
means to adversely train the global model. Wang et al. (2019) employs a multi-task GAN Whose
discriminator is trained by the gradient update from the victim, interestingly With no direct access
to the data. Its discriminator simultaneously discriminates for multiple tasks: category, reality, and
client identity of input samples. Melis et al. (2019) shoWs that it is possible to have the membership
attack and also infer properties that hold only for a subset of the training data. Another Work (Hitaj
et al., 2017) assumes that one participant is an attacker, Who oWns a discriminator With the same
architecture as the classifier. Moreover, the attacker generates the fake images and adversely update
the global model to force the victim to reveal more information. All Gan-based attacks are notorious
for increasing the complexity of training the model and partially disclosing the data distribution.
Most of the attacks mentioned beforehand are White-box attacks, in Which attackers have access to
2
Under review as a conference paper at ICLR 2021
the complete model description. For black-box attack, the attackers typically ask for the prediction
or query. For instance, in one work (Fredrikson et al., 2014), the attacker uses black-box access to
the model to infer a sensitive feature xi . More precisely, given joint distribution and marginal priors
they employ maximum a posterior (MAP) estimator that may sample a promising xi that maximizes
the posterior probability. The current state-of-the-art of reconstruction is Zhu et al. (2019), their
reconstruction is more accurate than the GAN-based attacks. Zhao et al. (2020) and Geiping et al.
(2020) are extensions, and aim to improve the label prediction accuracy and the reconstruction loss
function.
4	Inspiration
We will formally describe the reconstruction attack based on gradients in the section. Let’s define a
neural network model f, parameterized by W, and with input x ∈ Rd. Supposed there is a function
G that maps from x to a gradient vector v (|v| = p), G : RB×d 7→ Rp (B = 1, if SGD). For
mini-batch, the output of the gradient function is the average gradient V = -B PB=I G(xi, yi； w).
Essentially, reconstruction is the procedure of computing an inverse function G-1, which takes the
input ^ (hopefully approximates to v) and output X sufficiently close to the original input X (at least
to the extent that the attacker may infer useful information from the reconstructed one). For instance,
if x is image that belongs to animal category, the attacker may know the properties of the subject,
e.g., with tail or not.
Zhu et al. (2019) generate dummy input X (initialization) by sampling it from a multivariate normal
distribution N(.) with mean ~0 ∈ Rd and variance 1 ∈ Rd×d. Consecutively, they minimize the
distance L (L := ∣∣v - V∣∣2) between original gradient V and dummy gradient V to reconstruct the
original input.
5	Reconstruction with prior knowledge
We propose to stabilize the optimization step by two modifications relative to Zhu et al. (2019). First,
We sample the dummy (initial) data from an uniform distribution on the unit interval X 〜U(0,1)
since typically the preprocessing operation re-scale image data into the range between zero and one.
Second, we expand the cost function with L2 regularizer λ ∣∣X∣F for single instance reconstruction
and an orthogonal regularizer λ Pk=卜，=1 (X∣Xa∕ )2 for mini-batch reconstruction. Thus, we have
two following equations (2) and (3).
min l∣v - G(X,y; w)∣2 + λ ∣∣X∣F
x,y
(2)
B
min kv - G(X,y; w)k2 + λ X (XkXk0 )2	⑶
x,y	, .
k6=k0 =1
Our reconstruction function is specified in Algorithm 2. We dynamically decreases λ after m iter-
ations. Our method mainly has three advantages: 1) Due to the appropriate initialization, it starts
from a promising position (in the solution space) close to the optima x*. 2) For the single instance
reconstruction, L2 as our regularizer forces IlXkF small (in particular in the early stage of itera-
tions), which may avoid the gradient saturation for activation function like sigmoid and also prevent
it from going too far from the possible zone ([0,1]) during the iterative optimization, increasing nu-
merical stability. 3) For the mini-batch reconstruction, the orthogonal regularizer may penalize the
similarities between reconstructed images, in particular, in the early optimization stage.
5.1	Architecture, activation function and cost function
The neural network architecture, the choice of activation function, and cost function have a different
impact on the gradient-based reconstruction.
Some activation functions increase the difficulty of reconstruction, e.g. ReLu, when input is less
than zero, those parts of information will lose. However, activation function as sigmoid σ(X) =
3
Under review as a conference paper at ICLR 2021
Algorithm 2 Our Reconstruction (with improved initialization and regularizer)
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
Input: v, w
for i=0,1,2,..,I do I iterations
if i==0 then
Xo,yo 〜U(0,1)
Vι = G(X0,y0; W)
else
Vi = G(Xi,yi; W)
if single reconstruction then
min^i,yi kv — G(Xi,yi； w)k2 + λ |包||尸
else
min^i,yi kv - G(Xi,yi； w)k2 + λPn=k0 = ι (x|xk0)2
end
update: Xi+ι = Xi — η X NLxi
yi+1 = yi - η X NLyi
if (i//m)==0 then
λ = 0.9 * λ
end
end
ι+1-x, tanh(χ)=e：+e—χ may exactly have the inverse function forms. We choose sigmoid as the
activation function in our neural network.
Proposition 1 (FULLY-CONNECTED NEURAL NETWORK ONE-INSTANCE RECONSTRUCTION). To
reconstruct one input based on a fully-connected neural network, we only need single node in one
hidden layer regardless of the number of nodes in the output layer, as long as bias term exists.
For the classification task, we often use CrossEntropy as the cost function `(pi, yi) =
- Pj yij log pij where pi is a vector with length equal to the number of classes, and softmax func-
aj
tion is defined as Pij = P £.忆.The partial gradient With respect to aj (linear product of node j in
output layer L) is 袅'(Pi, yi) = Pij — yj.
Say We have one hidden layer neural netWork pj = Pin=11 Wj2 * * * * *iσ(Wi1X + bi1) + bj2, Where W1 ∈
Rn1 *d, b1 ∈ Rn1 are the weights and bias in the hidden layer, and nι is the number of nodes in
the first hidden layer, and w2 ∈ Rn2*n1, b2 ∈ Rn2 are in the output layer instead. Wij indicates the
weights of node i sitting in layer l, connecting with node j in the previous layer l — 1. σ is monotonic
activation function. We use CrossEntropy as the cost function '(.), thus, we have ∂∂~L = P.j — y.j
aj
and the following partial derivatives:
∂'
%=PLy
∂'
∂Wτ = (p.j — y.j )σ(WiX + b1) = (p.j — y.j )σi
n2	n2
(p.i — y.i)Wi2jσ (Wj1X + bj1) =	(p.i — y.i)Wi2jσj
ii
∂'	X，	、2 0
而= UPA- y.i)wij σjX
ji
(4)
(5)
(6)
(7)
If SGD, x is a vector (X ∈ R1*d), otherwise, X is a matrix. We know all the partial derivatives
and model parameters, we can directly backtrack X. More specifically, from eq. (7) we may have
the analytical form of X. Assume only one node in hidden layer (n1 = 1), for SGD (X ∈ Rd) we
can directly have Xm = ad' / 箓,∀m ∈ [1,d]. ThiS conclusion (proposition 1) conforms to the
1m 1
observation of Geiping et al. (2020).
4
Under review as a conference paper at ICLR 2021
Proposition 2 (FULLY-CONNECTED NEURAL NETWORK MINI-BATCH RECONSTRUCTION). For
the mini-batch reconstruction, with the assumption that given sigmoid σ(x), we may compute the
unique σ0 (x), we derive the condition that the number of units in hidden layer n1 ≥
B(d+n2)-n2
d+n2 + l-B *
For the high-dimensional input, whose dimension d》n2,d》B, nι is dominated by B)++^2),
thus n1 ≥ B.
To solve X ∈ RB*d, We need equations equal or greater than unknowns. From eq. (4) to eq. (7), We
have n2 + n1n2 + nι + nιd ≥ Bn + n、B + Bd, thus nι ≥ Bd+?)-*2. Please see the appendix
d+n2 +1-B
A.1, We offer the complete derive of a tiny sample.
To implement it, the choice of optimization method is relatively sensitive. For a high-dimension
(nonsparse) input, second-order or quasi-second-order methods sometimes fails since it is designed
to search for the zero-gradient position. The number of saddle points exponentially increases With
the number of input dimensions (Dauphin et al., 2014). First-order method, e.g., Adam takes much
more iterations to converge, but it is relatively more stable, likely to escape from the saddle points
(GoodfelloW et al., 2016).
Proposition 3 (SINGLE LAYER CONVOLUTIONAL NEURAL NETWORK RECONSTRUCTION). For a
single-convolutional-layer neural network (without pooling) with kernel size k, padding size p and
stride size s, immediately stacked by a fully-connected layer, known with n2 units. To reconstruct,
h = (d0 )2ΒC filters are required, where C is the Channel number of input, B is batch size, d is the
width of input, and d0 is width after passing one filter.
Say we have input X with both width and length equal to d, X ∈ RB*C*d*d, where C is the channels
and B is mini-batch size. After padding, it is labeled as X, and its width is equal to d + 2p with
padding size p. We define a square kernel with width k, stride size s, bias term b, and we have h
kernels. After passing through filters, we have output z with width equal to d0. Then, after passing
filter m (before vectorization operation), we have:
Ckk
Zmij = (	wmcghxc,si+g-1,sj+n-1)+ bmm ∀(i,j) ∈ [1,d] × [1,d ],∀m ∈ [1,h] (8)
c=1 g=1 n=1
The we define H = [vec(zι..), vec(z2..),..., Vec(Zh..)] and |H| = h(d)2 = nι, after convolutional
layer we have Pj = Pn11 wj%σ (Hi) + bj ,where w2 ∈ Rn2*n1 and b2 ∈ Rn2 are the weights and bias
in the output layer, σ() is the sigmoid function as defined before. d is defined as d = d-k+2p + 1.
Thus,
∂'
码=Rji
∀j ∈ [1, n2]
(9)
∂w = (P.j - y.j)σ(Hi) ∀j ∈ [1,n2],∀i ∈ [1,nι]	(IO)
d`
dbm
d0	d0
XX
i=1 j=1
d' dzmij
∂Zmij ∂b1m
d0	d0
XX
i=1 j=1
∂'
r!r ■ ■
UZmij
∀m ∈ [1, h]
(11)
∂'
dwmcgh
d0	d0
XX
i=1 j=1
∂' ʌ
^	xc,si+g-1,sj+h-1
Zmij
∀m ∈ [1,h]
(12)
Note here, ∂⅛ = ∂H[(m-ι)*(d0∂'+(i-i)*d0 +j]. From eq.⑻,we have (d )2h equationsand d2BC
unknowns, thus we need h = (d )2BC, with the assumption that H is known (left hand side of eq.
8). As shown in eq. (9) and (10), if nι ≥ nn(BB1), and 1 < B <n2, all σ(Hi) are known, thus Hi
are also known since σ() is monotonic activation function.
5
Under review as a conference paper at ICLR 2021
Figure 3: Face (Batch reconstruction): Mini-batch contains 5 images, and we show partial recon-
struction for 10 (col.1), 30 (col.2), 50(col.3), 90 (col.4) and final (col.5) iteration for Zhu et al.
(2019). The rest columns (f-j) are produced by our method. Only three instances are shown due to
the length limitations.
6 Experimental Results
Figure 1: ECG: Leftmost one is final reconstruction by our method, whereas the middle one is
produced by Zhu et al. (2019). Rightmost plot shows that our method converges much faster than
Zhu et al. (2019).
(a) 20 itrs
(b) 40 itrs
(c) 80 itrs
(d) final
Figure 2: FMRI: The plots (first row) are produced by Zhu et al. (2019), and it shows the reconstruc-
tions after 20, 40, 80, and final iteration accordingly, whereas the second row is our reconstruction.
6
Under review as a conference paper at ICLR 2021
6.1	Comparison with baseline
Our reconstruction enables a faster convergence rate and a more stable reconstruction procedure
comparing with the method proposed by Zhu et al. (2019). We demonstrate the superiority of our
method on ecg dataset, face dataset, fmri dataset, cell dataset, and cifar100 dataset in Figure 1, 2, 3,
8(appendix) and 9(appendix). We plot the reconstruction procedure with the optimizer LGBFS the
same as Zhu et al. (2019) did for the fair comparison, and it clearly shows that our method converges
faster on all the dataset. Moreover, due to the improved initialization and regularizer, sometimes our
reconstruction turns out the lower error than Zhu et al. (2019), one instance is shown in Figure
8(appendix).
6.2	Mini-batch reconstruction
Fully-connected neural network batch reconstruction: We experiment on mnist (size: 1024) to
validate our analysis; as long as the number of nodes in the hidden layer exceeds batch size, we
may have the reconstruction given sufficient iterations. Here we set batch size equal to four and
number of units in hidden layer to four as well. In Figure 4a, we first give the final reconstruction
of four inputs without regularizer, the digits overlap together. In Figure 4b, the performance with
orthogonal regularizer improves significantly. Furthermore, from Figure 4c to 4f, we show the whole
reconstruction procedure every 5k iterations, and we can see how the orthogonal regularizer plays
the key role during optimization procedure to hinder the similarities between instances. Moreover, in
Figure 5a, we show a mini-batch reconstruction with batch size 8. It demonstrates that L1 distance
between original input and reconstructed one decreases drastically with 8 units in hidden layer,
afterwards even with more units, the distance does not improve too much.
(a) No regularizer	(b) With orthogonal regularizer (λ = 0.1)
ter Ok ter ⅛k ter Wk ter l⅜k ter，0k ter "k ter JQk ter J⅜k ter OOk *<∙r 4∙>k t<∙f Dk «<•< ⅛k «<•< IOk t<∙f l⅛k *«r ?Ok *<∙r *k *<∙r JQk ter J⅜k ter OOk ter 4,⅛k	«<•< Ok ta ⅛k «<•< Wk t<∙f 1⅛k t<∙f 70k t<∙f *k *<∙r JOk *«r J⅜k ter OOk i<∙< 4,⅛k	«<•< Ok «<•< ⅛k «<•< Wk t<∙f 1⅛k t<∙f 70k ««r *k *<∙r JOk ter J⅜k ter OOk ter 4⅝l
翻睡牖牖翳翩魏寇睡醒藕鬻题鬻鬻舞麒盘擦摞舞腺曝感嘱谶藏廨除麴霸牖翳鬻遨强缪国缪
瓶更更坪≡口闻国国	魄域遮海想函函晾丽函/国悦闻闻闻日目目目
西卤卤崩面丽丽丽•函函函的的的的函的的丽南画画面画面画面画面盲盲盲画盲盲盲盲盲盲
Ker= 1501Iiter= 1551Iiter= 16Otiter= 165⅛ter= 17Otiter= 175⅛ter= 180⅛ter= 185⅛ter= 19Otiter= 195k Ker= 1501Iiter= 1551Iiter= 160⅛ter= 165⅛ter= 170⅛ter= 175⅛ter= 180⅛ter= 185⅛ter= 190⅛ter= 195k Ker= 1501Iiter= 1551Iiter= 160⅛ter= 165⅛ter= 170⅛ter= 175⅛ter= 180⅛ter= 185⅛ter= 190⅛ter= 195k Ker= 1501Iiter= 1551Iiter= 160⅛ter= 165⅛ter= 170⅛ter= 175⅛ter= 180⅛ter= 185⅛ter= 190⅛ter= 195k
Eideieieieieideiei磨磨磨磨磨磨园廊M股 囤圆囤囤圆圆圆圆圆圆bbbbbbbbbs
(c) 1/4 reconstruction pro- (d) 2/4 reconstruction pro- (e) 3/4 reconstruction pro- (f) 4/4 reconstruction pro-
cedure	cedure	cedure	cedure
Figure 4: MNIST (Batch reconstruction): 4a shows the final reconstruction without any regularizer;
as a comparison, with regularizer is shown in 4b. Moreover, the full reconstructions of 4b are shown
in 4c, 4d, 4e and 4f. We plot every 5k iterations.
Cnn batch reconstruction: We test on the cifar100 dataset, with kernel size 5, padding size 2 and
stride size2. According to Proposition 3, we need 12 filters to reconstruct one instance. In Figure 6,
we visually show the reconstruction performance with the change of filters number and from Table
6f, we show L1 distance alters with filter number. Note here we set λ starts from 0.1 and gradually
decays 90% after 2k iterations. For the mini-batch reconstruction (shown in Figure 5b), we set batch
size equal to four, thus 48 filters are required, we compared filter number equal to 12, 24, 48 and 50,
as we can see the L1 distance keeps decreasing and the reconstruction is in Figure 7. In particular,
when filter number is 48, average L1 distance per pixel is 0.00025.
7 Conclusion
We have investigated the highly interesting results in Zhu et al. (2019) to understand if the results
can be improved by careful initialization and augmenting the cost function with regularizers. More
specifically, we suggest an uniform distribution ([0,1]) initialization since typically the preprocess-
ing re-scales it between zero and one. We propose a L2 regularizer for single image reconstruction,
and an orthogonality promoting regularizer for mini-batch reconstruction. In both cases we let the
7
Under review as a conference paper at ICLR 2021
strength λ decay during iterations, to reduce bias in the final solutions. Then, we explore the limi-
tation of reconstruction and analyze the correlation between architecture, network size, and recon-
struction performance. We show that a fully-connected neural network only needs a single node in
one hidden layer to reconstruct a single input image, regardless of the number of nodes in the output
layer, as long as bias term exists. For the mini-batch case, as long as the number of nodes in the
first hidden layer exceeds the batch size (with regularizer), the reconstruction is promising. For the
convolutional neural network, (d)2 C filters are required for the reconstruction, where d is the width
of input image, d0 is the width after one convolutional filter , and C is input channel number. It can
be generalized to mini-batch reconstruction as well, thus BC * (d)2 filters are required (B is batch
size).
24	48	50
number of filters
number of hidden units
(a) MNIST reconstruction with batch size 8, when
number of units in hidden layer surpass the batch
size, the distance between reconstructed image
and original one drastically decreases.
(b) CIFAR100 reconstruction with batch size 4,
kernel size 5, padding size 2, stride size 2. Ac-
cording to proposition 3, 48 filters are required.
Reconstruction is shown in Figure 7.
Figure 5: L1 distance and number of units and filters
(a) 1 filter (b) 5 filters (c) 8 filters (d) 11 filters(e) 12 filters
Figure 6: One-layer CNN : Image reconstruction becomes more accurate with increasing number
of filters (kernels). Numerical comparison sees Table 6f. One-layer CNN: Distance decreases with
increasing number of filters (kernels). First row is the number of filters, and second row is the size
of filtered images, which is decided by the number of filters with the fixed kernel, padding and stride
size. When the size of filtered image is exactly equal to original input size (12 filters), we can almost
perfectly reconstruct it (L1 distance is 0.09, with λ starts from 0.1)
Filters Params. ((b )2h) L1 dist.
1	64	728
5	1280	649
8	2048	404
11	2816	135
12	3072	0.57
(f) L1 distance
(a) 12 filter	(b) 48 filters
Figure 7: CIFAR100 reconstruction with batch size 4 with 12 and 48 filters. λ starts from 0.1, and
gradually decays.
References
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated
learning through an adversarial lens. In International Conference on Machine Learning, pp. 634—
8
Under review as a conference paper at ICLR 2021
643, 2019.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex op-
timization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
Cynthia Dwork. Differential privacy: A survey of results. In International conference on theory and
applications of models of computation, pp. 1-19. Springer, 2008.
Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. Pri-
vacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd
{USENIX} Security Symposium ({USENIX} Security 14), pp. 17-32, 2014.
Jonas Geiping, Hartmut Bauermeister, Hannah Droge, and Michael Moeller. Inverting gradients-
how easy is it to break privacy in federated learning? arXiv preprint arXiv:2003.14053, 2020.
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models under the gan: infor-
mation leakage from collaborative deep learning. In Proceedings of the 2017 ACM SIGSAC
Conference on Computer and Communications Security, pp. 603-618, 2017.
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy (SP),
pp. 691-706. IEEE, 2019.
Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really
backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi. Beyond in-
ferring class representatives: User-level privacy leakage from federated learning. In IEEE INFO-
COM 2019-IEEE Conference on Computer Communications, pp. 2512-2520. IEEE, 2019.
Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. idlg: Improved deep leakage from gradients.
arXiv preprint arXiv:2001.02610, 2020.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, pp. 14774-14784, 2019.
A Appendix
A.1 Fully-connected neural network
Say we have n1, n2 nodes in hidden and output layer accordingly, and batch size is B. A one-hidden-
layer neural network is defined as pj = Pin=11 wj2iσ(wi1x + bi1) +bj2, where σ() is a monotonic active
function, and σij indicates σ(wixj + bi) in the current layer, andj is the index for instance xj. Our
cost function E is cross entropy.
d'	t
鸣= f.j- t.j
(13)
∂'	…	、，∙，	「、…	、
∂W2 = (f.j - tj )σ(wi X + bi ) = (f.j - tj )σi
(14)
9
Under review as a conference paper at ICLR 2021
Figure 8: CELL: The first row corresponds to the reconstruction by Zhu et al. (2019), for 10, 20,30,
and final iteration, whereas the second row is produced by our method.
∂'
∂bi
d`
dw1
n2
(f.j -t.j)wj2iσi0
j
n2
(f.j -t.j)wj2iσix
j
(15)
(16)
For the purpose of demonstration, we set batch size B equal to 2, n1 equal to 3, n2 equal to 2.
derive from eq. (14)
(∂'
db1
∂b2
d`
dw2ι
∂'
dw2ι
d`
dw22
∂'
dw23
(f11 - t11) + (f21 - t21)
(f12 - t12) + (f22 - t22)
(17)
(f11 - t11)σ11 + (f21 - t21)σ12
(f11 - t11)σ21 + (f21 - t21)σ22
(f11 - t11)σ31 + (f21 - t21)σ32
(f12 - t12)σ11 + (f22 - t22)σ12
(f12 - t12)σ21 + (f22 - t22)σ22
(f12 - t12)σ31 + (f22 - t22)σ32
(18)
(∂b1	=	[(f11	- t11)w21 +	(f12	- t12)w21]σ11	+	[(f21	- t21)w21	+	(f22	- t22)w21]σ12
derive from eq.(15)	< 羲=[(fιι	- tιι)w22 +	(fi2	- ti2)w22]σ2i	+	[(f2i	- t21)W22	+	(f22	- t22)w22]σ22
I ∂b1	=	[(f11	- t11)w23 +	(f12	- t12)w23]σ31	+	[(f21	- t2I)W23	+	(f22	- t22)w23]σ32
(19)
{∂W'1 =	[(f11	- t1I)W21	+	(f12	- t12)w21]σ11x1	+	[(f21	-	t2I)W21	+	(f22	-	t22)w21 ]σ12x2
∂W1 =	[(f11	一 t1I)W22	+	(f12	- t12)w22]σ21x1	+	[(f21	一	t2I)W22	+	(f22	-	t22)w22]σ22x2
∂W =	[(f11	一 t11)W23	+	(f12	一 》12)福3]。31x1	+	[(f21	一	t21)W23	+	(f22	-	t22)w23]σ32x2
(20)
Note here we assume with known σij, we may compute σi0j. To fully solve x1, x2 ∈ Rd, we need to
meet the condition that n + n1n2 + n1 + n∖d ≥ Bn2 + n1B + Bd, thus n1 ≥ Bd+n^-n2 and
d+n2 +1-B
B <n2 + 1 + d. Note here if d》B, d》n2, then n1 is largely determined by Bddl+? = B.
10
Under review as a conference paper at ICLR 2021
(a) 10 itrs
(d) final
(b) 30 itrs	(c) 90 itrs
Figure 9: Cifar100: We show partial reconstruction for 10 (col.1), 30 (col.2), 90 (col.3) and final
(col.4) iteration. The odd number of rows (1,3) are produced by Zhu et al. (2019) and even number
of rows (2,4) are produced by our method. The corresponding log-scale loss is shown under the
reconstruction procedure.
11