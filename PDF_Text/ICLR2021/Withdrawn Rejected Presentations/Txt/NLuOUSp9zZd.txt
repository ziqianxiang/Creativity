Under review as a conference paper at ICLR 2021
DO-GAN: A Double Oracle Framework for
Generative Adversarial Networks
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a new approach to train Generative Adversarial Networks
(GANs) where we deploy a double-oracle framework using the generator and
discriminator oracles. GAN is essentially a two-player zero-sum game between
the generator and the discriminator. Training GANs is challenging as a pure Nash
equilibrium may not exist and even finding the mixed Nash equilibrium is difficult
as GANs have a large-scale strategy space. In DO-GAN, we extend the double
oracle framework to GANs. We first generalize the player strategies as the trained
models of generator and discriminator from the best response oracles. We then
compute the meta-strategies using a linear program. Next, we prune the weakly-
dominated player strategies to keep the oracles from becoming intractable. We
apply our framework to established architectures such as vanilla GAN, Deep Convo-
lutional GAN, Spectral Normalization GAN and Stacked GAN. Finally, we conduct
evaluations on MNIST, CIFAR-10 and CelebA datasets and show that DO-GAN
variants have significant improvements in both subjective qualitative evaluation
and quantitative metrics, compared with their respective GAN architectures.
1	Introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been applied in various
domains such as image and video generation, image-to-image translation and text-to-image synthe-
sis (Liu et al., 2017; Reed et al., 2016). Various architectures are proposed to generate more realistic
samples (Radford et al., 2015; Mirza & Osindero, 2014; Pu et al., 2016) as well as regularization
techniques (Arjovsky et al., 2017; Miyato et al., 2018b). From the game-theoretic perspective, GANs
can be viewed as a two-player game where the generator samples the data and the discriminator
classifies the data as real or generated. The two networks are alternately trained to maximize their
respective utilities until convergence corresponding to a pure Nash Equilibrium (NE).
However, pure NE cannot be reliably reached by existing algorithms as pure NE may not exist
(Farnia & Ozdaglar, 2020; Mescheder et al., 2017). This also leads to unstable training in GANs
depending on the data and the hyperparameters. Therefore, mixed NE is a more suitable solution
concept (Hsieh et al., 2019). Several recent works propose mixture architectures with multiple
generators and discriminators that consider mixed NE such as MIX+GAN (Arora et al., 2017) and
MGAN (Hoang et al., 2018). MIX+GAN and MGAN cannot guarantee to converge to mixed NE.
Mirror-GAN (Hsieh et al., 2019) finds the mixed NE by sampling over the infinite-dimensional
strategy space and proposes provably convergent proximal methods. However, the sampling approach
may not be efficient as mixed NE may only have a few strategies in the support set.
Double Oracle (DO) algorithm (McMahan et al., 2003) is a powerful framework to compute mixed
NE in large-scale games. The algorithm starts with a restricted game with a small set of actions
and solves it to get the NE strategies of the restricted game. The algorithm then computes players’
best-responses using oracles to the NE strategies and add them into the restricted game for the next
iteration. DO framework has been applied in various disciplines (Jain et al., 2011; Bosansky et al.,
2013), as well as Multi-agent Reinforcement Learning (MARL) settings (Lanctot et al., 2017).
Inspired by the successful applications of DO framework, we, for the first time, propose a Double
Oracle Framework for Generative Adversarial Networks (DO-GAN). This paper presents four key
contributions. First, we treat the generator and the discriminator as players and obtain the best
responses from their oracles and add the utilities to a meta-matrix. Second, we propose a linear
1
Under review as a conference paper at ICLR 2021
program to obtain the probability distributions of the players’ pure strategies (meta-strategies) for
the respective oracles. The linear program computes an exact mixed NE of the meta-matrix game in
polynomial time. Third, we propose a pruning method for the support set of best response strategies
to prevent the oracles from becoming intractable as there is a risk of the meta-matrix growing very
large with each iteration of oracle training. Finally, we provide comprehensive evaluation on the
performance of DO-GAN with different GAN architectures using both synthetic and real-world
datasets. Experiment results show that DO-GAN variants have significant improvements in terms of
both subjective qualitative evaluation and quantitative metrics.
2	Related Works
In this section, we briefly introduce existing GAN architectures, double oracle algorithm and its
applications such as policy-state response oracles that are related to our work.
GAN Architectures. Various GAN architectures have been proposed to improve the performance
of GANs. Deep Convolutional GAN (DCGAN) (Radford et al., 2015) replaces fully-connected layers
in the generator and the discriminator with deconvolution layer of Convolutional Neural Networks
(CNN). Weight normalization techniques such as Spectral Normalization GAN (SNGAN) (Miyato
et al., 2018a) stabilize the training of the discriminator and reduce the intensive hyperparameters
tuning. There are also multi-model architectures such as Stacked Generative Adversarial Networks
(SGAN) (Huang et al., 2017) that consist of a top-down stack of generators and a bottom-up
discriminator network. Each generator is trained to generate lower-level representations conditioned
on higher-level representations that can fool the corresponding representation discriminator. Training
GANs is very hard and unstable as pure NE for GANs might not exist and cannot be reliably reached
by the existing approaches (Mescheder et al., 2017). Considering mixed NE, MIX+GAN (Arora
et al., 2017) maintains a mixture of generators and discriminators with the same network architecture
but have their own trainable parameters. However, training a mixture of networks without parameter
sharing makes the algorithm computationally expensive. Mixture Generative Adversarial Nets
(MGAN) (Hoang et al., 2018) propose to capture diverse data modes by formulating GAN as a
game between a classifier, a discriminator and multiple generators with parameter sharing. However,
MIX+GAN and MGAN cannot converge to mixed NE. Mirror-GAN (Hsieh et al., 2019) finds the
mixed NE by sampling over the infinite-dimensional strategy space and proposes provably convergent
proximal methods. The sampling approach may be inefficient to compute mixed NE as the mixed NE
may only have a few strategies with positive probabilities in the infinite strategy space.
Double Oracle Algorithm. Double Oracle (DO) algorithm starts with a small restricted game
between two players and solves it to get the player strategies at NE of the restricted game. The
algorithm then exploits the respective best response oracles for additional strategies of the players. The
DO algorithm terminates when the best response utilities are not higher than the equilibrium utility of
the current restricted game, hence, finding the NE of the game without enumerating the entire strategy
space. Moreover, in two-player zero-sum games, DO converges to a min-max equilibrium (McMahan
et al., 2003). DO framework is used to solve large-scale normal-form and extensive-form games
such as security games (Tsai et al., 2012; Jain et al., 2011), poker games (Waugh et al., 2009) and
search games (Bosansky et al., 2012). DO framework is also used in MARL settings (Lanctot et al.,
2017; Muller et al., 2020). Policy-Space Response Oracles (PSRO) generalize the double oracle
algorithm in a multi-agent reinforcement learning setting (Lanctot et al., 2017). PSRO treats the
players’ policies as the best responses from the agents’ oracles, builds the meta-matrix game and
computes the mixed NE but it uses Projected Replicator Dynamics that update the changes in the
probability of each player’s policy at each iteration. Since the dynamics need to simulate the update
for several iterations, the use of dynamics takes a longer time to compute the meta-strategies and
does not guarantee to compute an exact NE of the meta-matrix game. However, in DO-GAN, we
can use a linear program to compute the players’ meta-strategies in polynomial time since GAN is a
two-player zero-sum game (Schrijver, 1998).
2
Under review as a conference paper at ICLR 2021
3	Preliminaries
In this section, we mathematically explain the preliminary works that are needed to explain our
DO-GAN approach including generative adversarial networks and game theory concepts such as
normal-form game and double oracle algorithm.
3.1	Generative Adversarial Networks
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have become one of the dominant
methods for fitting generative models to complicated real-life data. GANs are deep neural net
architectures comprised of two neural networks trained in an adversarial manner to generate data that
resembles a distribution. The first neural network, a generator G, is given some random distribution
pz(z) on the input noise z and a real data distribution pdata(x) on training data x. The generator is
supposed to generate as close as possible to pdata(x). The second neural network, a discriminator D,
is to discriminate between two different classes of data (real or fake) from the generator.
Let the generator’s differentiable function be denoted as G(z, πg) and similarly D(x, πd) for the
discriminator, where G and D are two neural networks with parameters πg and πd. Thus, D(x)
represents the probability that x comes from the real data. The generator loss LG and the discriminator
loss LD are defined as:
LD = Eχ~Pdata(χ) [- log D(x)]+ Ez~pz(z) [Tog(1 — D(G(Z))],	(1)
LG = Ez~pz(z)[log(1- D(G(Z))].	⑵
GAN is then set up as a two-player zero-sum game between G and D as follows:
minG maxD Eχ^pdata(x) [log D(x)] + Ez^pz(z) [log(1 - D(G(Z))].	(3)
During training, the parameters of G and D are updated alternately until we reach the global optimal
solution D(G(Z)) = 0.5. Next, we let Πg and Πd be the set of parameters for G and D, considering
the set of probability distributions σg and σd, the mixed strategy formulation (Hsieh et al., 2019) is:
minmaxE∏d~°dEχ-Pdata(χ)[logD(x,∏d)] + E∏d~σdEng~σgEz~pz(z)[log(1 - D(G(z,∏g),∏d)].	(4)
σg σd	aa
Similarly to GANs, DCGAN, SNGAN and SGAN can also be viewed as two-player zero-sum games
with mixed strategies of the players. DCGAN modifies the vanilla GAN by replacing fully-connected
layers with the convolutional layers. SGAN trains multiple generators and discriminators using the
loss as a linear combination of 3 loss terms: adversarial loss, conditional loss and entropy loss.
3.2	Normal Form Game and Double Oracle Algorithm
A normal-form game is a tuple (Π, U, n) where n is the number of players, Π = (Π1, . . . , Πn) is the
set of strategies for each player i ∈ N, where N = {1, . . . , n} and U : Π → Rn is a payoff table of
utilities R for each joint policy played by all players. Each player chooses the strategy to maximize
own expected utility from Πi, orby sampling from a distribution over the set of strategies σi ∈ ∆(Πi).
We can use linear programming, fictitious play (Berger, 2007) or regret minimization (Roughgarden,
2010) to compute the probability distribution over players’ strategies.
In the Double Oracle (DO) algorithm (McMahan et al., 2003), there are two best response oracles
for the row and column player respectively. The algorithm creates restricted games from a subset of
strategies at the point of each iteration t for row and column players, i.e., Πtr ⊂ Πr and Πtc ⊂ Πc as
well as a meta-matrix Ut at the tth iteration. We then solve the meta-matrix to get the probability
distributions on Πtr and Πtc . Given a probability distribution σc of the column player strategies,
BRr(σc) gives the row player’s best response to σc. Similarly, given probability distribution σr of
the row player’s strategies, BRc(σr) is the column player’s best response to σr. The best responses
are added to the restricted game for the next iteration. The algorithm terminates when the best
response utilities are not higher than the equilibrium utility of current restricted game. Although
in the worst-case, the entire strategy space may be added to the restricted game, DO is guaranteed
to converge to mixed NE in two-player zero-sum games. DO is also extended to the multi-agent
reinforcement learning in PSRO (Lanctot et al., 2017) to approximate the best responses to the
mixtures of agents’ policies, and compute the meta-strategies for the policy selection.
3
Under review as a conference paper at ICLR 2021
Table 1: Comparison of Terminologies between Game Theory and GAN
Game Theory terminology ∣ GAN terminology
player	generator/ discriminator
strategy	the parameter setting of generator/ discriminator e.g. πg and πd the sequence of parameters (strategies) till epoch t
policy	e.g. (πg1,πg2, ..., πgt) Note: Not used in DO-GAN.
game	the minmax game between generator and discriminator
meta-game/ meta-matrix	the minmax game between generator & discriminator with their respective set of strategies at epoch t of DO framework
meta-strategy	the mixed NE strategy of generator/discriminator at epoch t
4 DO-GAN: Double Oracle Framework for GAN
As discussed in previous sections, computing mixed NE for GANs is challenging as there is an
extremely large number of pure strategies, i.e., possible parameter settings of the generator and
discriminator networks. Thus, we propose a double oracle framework for GANs (DO-GAN) to
compute the mixed NE efficiently. DO-GAN builds a restricted meta-matrix game between the two
players and computes the mixed NE of the meta-matrix game, then DO-GAN iteratively adds more
generators and discriminators into the meta-matrix game until termination.
4.1	General Framework of DO-GAN
GAN can be translated as a two-player zero-sum game between the generator player g and the
discriminator player d. To compute the mixed NE of GANs, at iteration t, DO-GAN creates a
restricted meta-matrix game Ut with the trained generators and discriminators as the strategies of the
two players, where the generators and discriminators are parameterized by πg ∈ G and πd ∈ D. We
use Ut(πg, πd) to denote the generator player’s payoff when playing πg against πd, which is defined
as LD. Since GAN is zero-sum, the discriminator player’s payoff is -Ut(πg, πd). We define σgt and
σdt as the mixed strategies of generator player and discriminator player, respectively. With a slight
abuse of notation, we define the generator player’s expected utility of the mixed strategies hσgt , σdt i as
Ut(σgt,σdt)=Pπg∈g P∏d∈D σg(∏g) ∙ σd(∏d) ∙ Ut(∏g, ∏d).We use (σg*,σd*i to denote the mixed
NE of the restricted meta-matrix game Ut. We solve Ut to obtain the mixed NE, compute the best
responses and add them into Ut for the next iteration. Figure 1 presents an illustration of DO-GAN
and Algorithm 1 describes the overview of the framework.
Figure 1: An illustration of DO-GAN. Figure adapted from (Lanctot et al., 2017).
Our algorithm starts by initializing two empty arrays G and D to store multiple generators and
discriminators (line 1). We train the first πg and πd with the canonical training procedure of GANs
(line 2). We store the parameters of the trained models in the two arrays G and D (line 3), compute
the adversarial loss LD and add it to the meta-matrix U0 (line 4). We initialize the meta-strategies
4
Under review as a conference paper at ICLR 2021
σg* = [1] and σ0* = [1] since there is only one pair of generator and discriminator available (line 5).
For each epoch, we use generatorOracle() and discriminatorOracle() to obtain the best
responses ∏g and ∏d to σ%* and σg via Adam Optimizer, respectively, then add them into G and D
(lines 7-10). We then augment Ut-1 by adding πg0 and πd0 and calculating Ut(πg0 , πd0 ) to obtain Ut and
compute the missing entries (line 11). We compute the missing payoff entries Ut(πg0 , πd), ∀πd ∈ D
and Ut(πg, πd0 ), ∀πg ∈ G by sampling a few batches of training data. After that, we compute the
mixed NE (σg*, σ小 of Ut with linear programming (line 12). The algorithm terminates if the criteria
described in Algorithm 2 is satisfied (line 13). We also prune the support strategy set of the players
as described in Algorithm 3 (line 14) to avoid G and D becoming intractable.
In generatorOracle(), We train πg to obtain the best response against σ%*, i.e., Ut(πg,σ%*) ≥
Ut(∏g,σd*),∀∏g ∈ Πg. Similarly, in disciminatorOracle(), we train ∏d to obtain the best
response against σj i.e., U t(σg*,∏d) ≥ Ut (σg∖∏d), Nnd ∈ ∏d. Full details of generator oracle and
discriminator oracle can be found in Appendix A.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
1
2
3
4
5
6
7
8
Algorithm 1: Double Oracle Framework for GAN (DO-GAN)
Initialize generator and discriminator arrays G = 0 and D = 0;
Train generator & discriminator to get the first πg and πd ;
G — G ∪ {∏g}; D — D ∪ {∏d};
Compute the adversarial loss LD and add it to meta-matrix U0 ;
Initialize σg* = [1] and σ0* = [1];
for epoch t ∈ {1, 2, ...} do
πg — generatorOracle(σd*, D);
G — G ∪ {πg};
∏d — discriminatorOracle(σg*, G);
D — D ∪ {πd0 };
Augment Ut-1 with πg0 and πd0 to obtain Ut and compute missing entries;
Compute mixed NE hσg*, σd*i for Ut with linear program;	// Section 4.2
if TerminationCheck(Ut,σg^, σ^) then break;	// Section 4.3
PruneMetaMatrix( Ut, σg*, σ^) ;	// Section 4.4
Algorithm 2: TerminationCheck(U t , σgF*)
// Ut is of size m × n
// |G| = m, |D| = n
Compute Ut (σ∕, σd);
Compute Ut(σg*, D[n]);
Compute Ut(G[m],σd*);
genInc = Ut(G[m],σd*) - Ut(σg*,σd*);
di1 2 3 4 s 6 7 8 9 10Inc = -Ut(σt*,DInD- (-Ut(σg*,σd*));
if genInc < && - disInc < then
L return True
else return False ;
Algorithm 3: PruneMetaMatrix(Ut, σg*, σd*)
1 // I stores indices to be pruned from G and D
// G stores models to be pruned from G and D
2 Ig = 0; Id = 0
3 Kg = 0; Kd = 0 if |G| > s then
4 for i ∈ {0, . . . , |G| - 1} do
5	if σg*(G[i]) == min σg* then
L Ig 一 Ig ∪{i}; Kg 一 Kg ∪{G[i]};
6 if |D| > S then
7 for j ∈ {0, . . . , |D| - 1} do
8	if σd*(D[j]) == min σd* then
Id - Id ∪{j}; Kd - Kd ∪{D[j]};
9 G — G\ Kg; D - D\ Kd;
10 U — JIg ,m ∙ Ut ∙ JTd,n
4.2	Linear Program of Meta-matrix Game
Since the current restricted meta-matrix game Ut is a zero-sum game, we can use a linear program to
compute the mixed NE in polynomial time (Schrijver, 1998). Given the generator player g’s mixed
strategy σgt , the discriminator player d will play strategies that minimize the expected utility of g .
Thus, the mixed NE strategy for the generator player σg is to maximize the worst-case expected
utility, which is obtained by solving the following linear program:
σg = argmaxσg{v : σ∖ ≥ 0, £就0 σ∖(i) = 1,Ut(σg,∏d) ≥ v,∀πd ∈ D}∙	(5)
5
Under review as a conference paper at ICLR 2021
Similarly, We Can obtain the mixed NE strategy for the 祖SCriminator σ%* by solving a linear program
that maximizes the worst-case expected utility of the discriminator player. Therefore, we obtain the
mixed NE hσgt*, σdt*i of the restriCted meta-matrix game Ut.
4.3	Termination Check
DO terminates the training by CheCking Whether the best response πg0 (or πd0 ) is in the support set G (or
D) (Jain et al., 2011), but We Cannot apply this approaCh to DO-GAN as GAN has infinite-dimensional
strategy spaCe (Hsieh et al., 2019). HenCe, We terminate the training if the best responses Cannot bring
a higher utility to the tWo players than the entries of the Current support sets, as disCussed in (LanCtot
et al., 2017; Muller et al., 2020). SpeCifiCally, We first Compute Ut(σgt*, σdt*) and the expeCted utilities
for neW generator and disCriminator Ut(G[m], σdt*), Ut(σgt*, D[n]) (line 1-3). Then, We CalCulate the
utility inCrement (lines 4-5) and returns True if both Ut(G[m], σdt*) and Ut(σgt*, D[n]) Cannot bring
a higher utility than Ut (σgt* , σdt*) by (lines 6-8).
4.4	Pruning Meta-matrix
As the meta-matrix groWs With every epoCh of DO, there is a risk that the support strategy set beComes
very large and G and D beCome intraCtable. To avoid this, We adapt the greedy pruning algorithm
from (Cheng & Wellman, 2007), as depiCted in Algorithm 3. When either |G| or |D| is greater than
the limit of the support set size s, We prune at least one strategy With the least probability, WhiCh is
the strategy that Contributes the least to the player’s Winning. SpeCifiCally, We define JI,b Where I is
the set of roW numbers to be removed, b is the total roWs of a matrix. To remove the 2nd roW of a
matrix having 3 roWs, We define I = {1}, b = 3 and J{1},3 = 10 00 10 . If |G| > s, at least one
strategy With minimum probability is pruned from G, similarly for D (lines 3-9). Finally, We prune
the meta-matrix using matrix multipliCation (line 10).
5	Experiments
We ConduCt our experiments on a maChine With Xeon(R) CPU E5-2683 v3@2.00GHz and 4× Tesla
v100-PCIE-16GB running Ubuntu operating system. We evaluate the double oraCle frameWork for
some established GAN arChiteCtures suCh as vanilla GAN (GoodfelloW et al., 2014), DCGAN (Rad-
ford et al., 2015), SNGAN (Miyato et al., 2018a) and SGAN (Huang et al., 2017). We adopt the
parameter settings and Criterion of the GAN arChiteCtures as published. We set s = 10 unless
mentioned otherWise. We Compute the mixed NE of the meta-matrix game With Nashpy1. The
evaluation details are shoWn in Appendix B.
Epoch 0	Epoch 5000	Epoch 10000	Epoch 15000	Epoch 20000	Target
.		• ∙	∙. • ∙ ∙∙			.・・	•. ・ ∙ ・・			:∙..
Epoch 0		Epoch 5000		Epoch 10000		Epoch 15000		Epoch 20000		Target
・				・ ∙ ∙		・ ∙ ∙		.∙：		∙∙∙
Figure 2: Comparison of GAN and DO-GAN on 2D synthetiC dataset
5.1	Evaluation on Synthetic 2D Gaussian Mixture Dataset
To illustrate the effeCtiveness of the arChiteCture, We train a double oraCle frameWork With the simple
vanilla GAN arChiteCture on a 2D mixture of 8 Gaussian mixture Components With Cluster standard
1https://nashpy.readthedocs.io/en/stable/index.html
6
Under review as a conference paper at ICLR 2021
deviation 0.1 which follows the experiment by (Metz et al., 2017). Figure 2 shows the evolution
of 512 samples generated by GAN and DO-GAN through 20000 epochs. The goal of GAN and
DO-GAN is to correctly generate samples at 8 modes as shown in the target. The results show
that GAN can only identify 6 out of 8 modes of the synthetic Gaussian data distribution, while the
DO-GAN can obtain all the 8 modes of the distribution. Furthermore, DO-GAN takes shorter time
(less than 5000 epochs) to identify all 8 modes of the data distribution. We present a more detailed
evolution of data samples through the training process on 2D Gaussian Mixtures in Appendix C.
Ablations. We also varied the support set size of the training s = 5, 10, 15 and recorded the
computation time as discussed in Appendix D. We found that the training cannot converge when
s = 5 and takes a long time when s = 15. Thus, we chose s = 10 for the training process.
5.2	Evaluation on Real-world Datasets
We evaluate the performance of the double oracle framework which takes several established GAN
architectures as the backbone as discussed in Appendix G, i.e., GAN (Goodfellow et al., 2014),
DCGAN (Radford et al., 2015) and SGAN (Huang et al., 2017) with convolutional layers for the
deep neural networks of GAN as well as SNGAN (Miyato et al., 2018a) which uses normalization
techniques. We run experiments on MNIST (LeCun & Cortes, 2010), CIFAR-10 (Krizhevsky et al.,
2009) and CelebA (Liu et al., 2015) datasets. MNIST contains 60,000 samples of handwritten digits
with images of 28 × 28. CIFAR-10 contains 50, 000 training images of 32 × 32 of 10 classes. CelebA
is a large-scaled face dataset with more than 200K images of size 128 × 128.
5.2.1	Qualitative Evaluation
We choose the CelebA dataset for the qualitative evaluation since the training images contain
noticeable artifacts (aliasing, compression, blur) that make the generator difficult to produce perfect
and faithful images. We compare the performance of DO-DCGAN, DO-SNGAN and DO-SGAN
with their counterparts 2. SNGAN which is trained for 40 epochs with termination of 5 × 10-5 for
DO-SNGAN where other architectureas are trained for 25 epochs with termination of 5 × 10-5 for
the double oracle variants. The generated CelebA images of DCGAN and DO-DCGAN are shown in
Figure 3, where we find that DCGAN suffers mode-collapse, while DO-DCGAN does not. We also
present the generated images of SNGAN vs DO-SNGAN and SGAN vs DO-SGAN using fixed noise
at different training epochs in Figure 4 and 5. From the results, we can see that SNGAN, SGAN,
DO-SNGAN and DO-SGAN are able to generate various faces, i.e., no mode-collapse. Judging from
subjective visual quality, we find that DO-SNGAN and DO-SGAN are able to generate plausible
images faster than SNGAN and SGAN during training, i.e., 17 epochs for DO-SGAN and 20 epochs
for SGAN. More experiment results on CIFAR-10 can be found in Appendix E.
Figure 3: Training images with fixed noise for DCGAN and DO-DCGAN until termination.
2We do not evaluate the performance of vanilla GAN and its DO variant on CelebA dataset since DCGAN
and SGAN outperform vanilla GAN in image generation tasks (Radford et al., 2015)
7
Under review as a conference paper at ICLR 2021
Figure 4: Training images with fixed noise for SNGAN and DO-SNGAN until termination.
Figure 5: Training images with fixed noise for SGAN and DO-SGAN until termination.
5.2.2 Quantitative Evaluation
In this section, we evaluate the performance of various architectures by quantitative metrics.
Inception Score. We first leverage the Inception Score (IS) (Salimans et al., 2016) by using
Inception_v3 (Szegedy et al., 2016) as the inception model. To compute the inception score, we
first compute the Kullback-Leibler (KL) divergence for all generated images and use the equation
IS = exp(Ex[KL(D(p(y|x) k p(y)))]) where p(y) is the conditional label distributions for the
images in the split and p(y|x) is that of the image x estimated by the reference inception model.
Inception score evaluates the quality and diversity of all generated images rather than the similarity
to the real data from the test set.
FID Score. Frechet Inception Distance (FID) measures the distance between the feature vectors of
real and generated images using Inception_v3 model (Heusel et al., 2017). Here, we letp and q be
the distributions of the representations obtained by projecting real and generated samples to the last
hidden layer of Inception model. Assuming that p and q are the multivariate Gaussian distributions,
FID measures the 2-Wasserstein distance between the two distributions. Hence, FID Score can
capture the similarity of generated images to real ones better than the Inception score.
8
Under review as a conference paper at ICLR 2021
Table 2: Inception scores (higher is better) and FID scores (lower is better). The mean and standard
deviation are drawn from running 10 splits on 10000 generated images. The magenta values are the
improvements of the DO-GAN variants compared with their counterparts.
	Inception Score		FID Score	
	MNIST	CIFAR-10	CIFAR-10	CelebA
GAN	1.04 ± 0.05	3.84 ± 0.09	71.44	-
DCGAN	1.26 ± 0.05	6.32 ± 0.05	37.66	10.92
SNGAN	1.35 ± 0.11	7.58 ± 0.12	25.5	7.62
SGAN	1.39 ± 0.09	8.62 ± 0.12	24.83	6.98
MIX+DCGAN	-	7.72 ± 0.09	-	-
MGAN	-	8.33 ± 0.10	26.7	-
DO-GAN	1.39 ± 0.09 (+0.35)	7.20 ± 0.16 (+3.36)	31.44 (-40.00)	-
DO-DCGAN	1.42 ± 0.11 (+0.16)	7.86 ± 0.14 (+1.54)	22.25 (-15.41)	7.11 (-3.81)
DO-SNGAN	1.42 ± 0.07 (+0.07)	8.55 ± 0.08 (+0.97)	18.20 (-7.30)	6.92 (-0.70)
DO-SGAN	1.42 ± 0.09 (+0.03)	8.69 ± 0.10 (+0.07)	16.56 (-8.27)	6.32 (-0.66)
Note: MIX+DCGAN and MGAN results are directly copied from (Arora et al., 2017; Hoang et al., 2018).
Results. The results are shown in Table 2. In CIFAR-10 dataset, DO-GAN, DO-DCGAN and
DO-SNGAN obtain much better results (7.2 ± 0.16, 7.86±0.14 and 8.55±0.08) than GAN, DCGAN
and SNGAN (3.84 ± 0.09, 6.32 ± 0.05 and 7.58 ± 0.12). However, we do not see a significant
improvement in DO-SGAN compared to SGAN 8.62 ± 0.12 and 8.69 ± 0.10 since SGAN already
can generate diverse images. We did not include IS for CelebA dataset as IS cannot reflect the real
image quality for the CelebA, as observed in (Heusel et al., 2017). In CIFAR-10 dataset, DO-GAN,
DO-DCGAN, DO-SNGAN and DO-SGAN obtain much lower FID scores (31.44, 22.25, 16.56,
18.20) respectively. The trend follows in CelebA obtaining 7.11 for DO-DCGAN while 10.92 for
DCGAN, 7.62 for SNGAN while 6.92 for DO-SNGAN, 6.98 for SGAN and 6.32 for DO-SGAN
respectively. Although we see a significant improvement in the quality of DO-SGAN images, FID
score for DO-SGAN is affected by the distortions. According to the results, we can see that DO
framework performs better than each of their original counterpart architectures. More details can be
found in Appendix F.
6	Conclusion
We propose a novel double oracle framework to GANs, which starts with a restricted game and
incrementally adds the best responses of the generator and the discriminator to compute the mixed NE.
We then compute the players’ meta-strategies by using a linear program. We also prune the support
strategy set of players. We apply DO-GAN approach to established GAN architectures such as vanilla
GAN, DCGAN, SNGAN and SGAN. Extensive experiments with the 2D Gaussian synthetic data set
as well as real-world datasets such as MNIST, CIFAR-10 and CelebA show that DO-GAN variants
have significant improvements in comparison to their respective GAN architectures both in terms of
subjective image quality as well as in terms of quantitative metrics.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In ICML,pp. 214-223, 2017.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). In ICML, pp. 224-232, 2017.
Ulrich Berger. Brown’s original fictitious play. Journal of Economic Theory, 135(1):572-578, 2007.
B Bosansky, Christopher Kiekintveld, Viliam Lisy, and Michal Pechoucek. Iterative algorithm for
solving two-player zero-sum extensive-form games with imperfect information. In ECAI, pp.
193-198, 2012.
9
Under review as a conference paper at ICLR 2021
Branislav Bosansky, Christopher Kiekintveld, Viliam Lisy, Jiri Cermak, and Michal Pechoucek.
Double-oracle algorithm for computing an exact Nash equilibrium in zero-sum extensive-form
games. In AAMAS,pp. 335-342, 2013.
Shih-Fen Cheng and Michael P Wellman. Iterated weaker-than-weak dominance. In IJCAI, pp.
1233-1238, 2007.
Farzan Farnia and Asuman Ozdaglar. GANs may have no Nash equilibria. arXiv preprint
arXiv:2002.09124, 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, pp. 2672-2680,
2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In NeurIPS,
pp. 6626-6637, 2017.
Quan Hoang, Tu Dinh Nguyen, Trung Le, and Dinh Phung. MGAN: Training generative adversarial
nets with multiple generators. In ICLR, 2018.
Ya-Ping Hsieh, Chen Liu, and Volkan Cevher. Finding mixed nash equilibria of generative adversarial
networks. In ICML, pp. 2810-2819, 2019.
Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, and Serge Belongie. Stacked generative
adversarial networks. In CVPR, pp. 5077-5086, 2017.
Manish Jain, Dmytro Korzhyk, Ondrej Vanek, Vincent Conitzer, Michal Pechoucek, and Milind
Tambe. A double oracle algorithm for zero-sum security games on graphs. In AAMAS, pp. 327-334,
2011.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 (Canadian Institute for Advanced
Research). https://www.cs.toronto.edu/ kriz/cifar.html, 2009.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P6rolat,
David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement
learning. In NeurIPS, pp. 4190-4203, 2017.
Yann LeCun and Corinna Cortes.	MNIST handwritten digit database.
http://yann.lecun.com/exdb/mnist/, 2010. URL http://yann.lecun.com/exdb/mnist/.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In
NeurIPS, pp. 700-708, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
ICCV, pp. 3730-3738, 2015.
H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost
functions controlled by an adversary. In ICML, pp. 536-543, 2003.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of GANs. In NeurIPS, pp.
1825-1835, 2017.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. In ICLR, 2017.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. ICLR, 2018a.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 41(8):1979-1993, 2018b.
10
Under review as a conference paper at ICLR 2021
Paul Muller, Shayegan Omidshafiei, Mark Rowland, Karl Tuyls, Julien Perolat, Siqi Liu, Daniel
Hennes, Luke Marris, Marc Lanctot, Edward Hughes, et al. A generalized training approach for
multiagent learning. In ICLR, 2020.
Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence
Carin. Variational autoencoder for deep learning of images, labels and captions. In NeurIPS, pp.
2352-2360, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.
Generative adversarial text to image synthesis. In ICML, 2016.
Tim Roughgarden. Algorithmic game theory. Communications of the ACM, 53(7):78-86, 2010.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In NeurIPS, pp. 2234-2242, 2016.
Alexander Schrijver. Theory of Linear and Integer Programming. John Wiley & Sons, 1998.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR, pp. 2818-2826, 2016.
Jason Tsai, Thanh H Nguyen, and Milind Tambe. Security games for controlling contagion. In AAAI,
pp. 1464-1470, 2012.
Zhengwei Wang, Qi She, and Tomas E Ward. Generative adversarial networks in computer vision: A
survey and taxonomy. arXiv preprint arXiv:1906.01529, 2019.
Kevin Waugh, Nolan Bard, and Michael Bowling. Strategy grafting in extensive games. In NeurIPS,
pp. 2026-2034, 2009.
11
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
1
2
3
4
5
6
7
8
9
10
A	Full Algorithm of DO-GAN
Algorithm 4: GeneratorOracle (σd*, D)
Initialize a generator G with random parameter setting πg0 ;
for iteration k0 . . . kn do
Sample noise z;
∏d = Sample a discriminator from D with q£;
Initialize a discriminator D with parameter setting πd ;
Update the generator G’s parameters πg0 via Adam optimizer:
5πg0 log (1 - D(G(z)))
Algorithm 5: DiscriminatorOracle (σg*, G)
Initialize a discriminator D with random parameter setting πd0 ;
for iteration k0 . . . kn do
Sample a minibatch of data x;
for a minibatch do
Sample noise z;
∏g = Sample a generator from G with σg*;
Initialize a generator G with a parameter setting πg ;
Generate and add to mixture G(z);
Update the discriminator D ’s parameters πd0 via Adam optimizer:
5πd0 log D(x) + log (1 - D(G(z)))
We train the oracles for some iterations which we denote as k0,1,2,. For experiments, we train
each oracle for an epoch for the real-world datasets and 50 iterations for the 2D Synthetic Gaussian
Dataset. At each iteration t, we sample the generators from the support set G with the meta-strategy
σg to generate the images for evaluation. Similarly, we conduct the performance evaluation with the
generators sampled from G with the final σg at termination. SGAN consists of a top-down stack of
GANs, e.g, for a stack of 2, Generator 1 is the first layer stacked on Generator 0 with each of them
connected to Discriminator 1 and 0 respectively. Hence, in DO-SGAN, we store the meta-strategies
for the Generator 0 and 1 in σg and the Discriminator 1 and 0 for σ%*. In GeneratorOracle(), we
first sample Discriminator 1 and 0 from discriminator distribution σ%* and train Generator 1 first then
followed by calculating loss with Discriminator 1 and train Generator 0 subsequently, and finally
calculate final loss with Discriminator 0 and train the whole model end to end. We perform the same
process for DisciminatorOracle().
12
Under review as a conference paper at ICLR 2021
B Implementation Details
Table 3: Training Hyperparameters
	GAN	DCGAN	SNGAN	SGAN
Generator Learning Rate	0.0002	0.0002	0.0002	0.0001
Discriminator Learning Rate	0.0002	0.0002	0.0002	0.0001
batch size	64	64	64	100
Adam: beta 1	0.5	0.5	0.5	0.5
Adam: beta 2	0.999	0.999	0.999	0.999
We implement our proposed method with Python 3.7, Pytorch=1.4.0 and Torchvision=0.5.0. We set
the hyperparameters as the original implementations. We present the hyperparameters set in Table 3.
We use Nashpy to compute the equilibria of the meta-matrix game.
C Full Training Process of 2D Gaussian Dataset
Epoch 0
Epoch 1000	Epoch 2000	Epoch 3000	Epoch 4000
(a) GAN
Figure 6: Full comparison of GAN and DO-GAN on 2D Synthetic Gaussian Dataset
(b) DO-GAN
Figure 7: GAN and DO-GAN comparison with Gaussian Mixture 7 modes
Figure 6 shows the full training process of DO-GAN and GAN on 2D Synthetic Gaussian Dataset.
From the results, we find that GAN struggles to generate the samples into 8 modes while DO-GAN
can generate all the 8 modes of the distribution. Furthermore, DO-GAN takes shorter time (less than
5000 iterations) to identify all 8 modes of the data distribution. Moreover, we present the experiment
results on 7 mode and 9-mode Gaussian Mixtures in Figure 7 and 8.
13
Under review as a conference paper at ICLR 2021
Figure 8: GAN and DO-GAN comparison with Gaussian Mixture 9 modes
D Investigation of Support Set Size
We vary the support set size s to 5, 10, 15 and record the training evolution and the running time as
presented in Table 4 and Figure 9. We find that if the support size is too small, e.g., s = 5, the best
responses which are not optimal yet have better utilities than the models in the support set are added
and pruned from the meta-matrix repeatedly making the training not able to converge. However,
s = 15 takes a significantly longer time as the time for the augmenting of meta-matrix becomes
exponentially long with the support set size. Hence, we chose s = 10 as our experiment support set
size since we observed that there is no significant trade-off and shorter runtime.
Table 4: Runtime of DO-GAN on 2D Gaussian Dataset with s = 5, 10, 15
Support Set Size	Runtime (GPU hours)
s=5	> 1
s = 10	0.5627
s = 15	0.9989
Epoch 20000
Figure 9: Training evolution on 2D Gaussian Dataset with s = 5, 10, 15
Epoch 20000
14
Under review as a conference paper at ICLR 2021
E Generated images of CelebA and CIFAR- 1 0
In this section, we present the training images of CelebA and CIFAR-10 datasets.
Figure 10: Training images with fixed noise for DCGAN and DO-DCGAN until termination.
Epoch 15	Epoch 20	Epoch 23
Epoch 10	Epoch 20	Epoch 30	Epoch 40
Epoch 0	Epoch 10	Epoch 20	Epoch 30	Epoch 34
Figure 11: Training images with fixed noise for SNGAN and DO-SNGAN until termination.
Figure 10 shows the training samples of DO-DCGAN and DO-DCGAN through the training process.
Figure 11 also shows those of SNGAN which is trained for 40 epochs with termination of 5 × 10-5
for DO-SNGAN. The results show that DCGAN suffers from mode-collapse, generating similar face
while DO-DCGAN can generate more plausible and varying faces. We also present the generated
images of DCGAN, DO-DCGAN, SNGAN, DO-SNGAN, SGAN and DO-SGAN of CIFAR-10
dataset showing that DO-DCGAN, DO-SNGAN and DO-SGAN can generate better and more
identifiable images than DCGAN, SNGAN and SGAN respectively. We present more of the generated
samples from SNGAN, DO-SNGAN, SGAN and DO-SGAN on CelebA dataset in Figure 13.
15
Under review as a conference paper at ICLR 2021
(b) SNGAN
(a) DCGAN
(c) SGAN
(d) DO-DCGAN
(e) DO-SNGAN
(f) DO-SGAN
Figure 12: Generated images of CIFAR-10 dataset
(a) SNGAN
(b) DO-SNGAN
Figure 13: Generated images of CelebA dataset for DO-SNGAN and SNGAN
16
Under review as a conference paper at ICLR 2021
Figure 13: Generated images of CelebA dataset for DO-SGAN and SGAN
F FID score against iterations
To compute FID score, we use Inception_v3 model with max pool of 192 dimensions and the last
layer as coding layer as mentioned in (Heusel et al., 2017). We resized MNIST, CIFAR-10 generated
and test images to 32 × 32 and CelebA images to 64 × 64. The FID score against training epochs for
CIFAR-10 dataset is as follows:
25
0 1.
10
1
erocS DIF
头
sGa SGAN
-B-DO-SGAN
⅜------------÷<-----------⅛
101
50	100	150	200	250	300	350	400	450	500
Epochs
Figure 14:	FID score vs. Epochs for SGAN and DO-SGAN trained on CIFAR-10
Figure 14 presents the FID score against each epoch of training for SGAN and DO-SGAN on CIFAR-
10. While both perform relatively well in generating plausible images, we can see that DO-SGAN
terminates early at epoch 288 and has a better FID score of 16.56 compared to 24.83 at 300 epoch
until 21.284 at 500 epoch for the training of SGAN.
17
Under review as a conference paper at ICLR 2021
G Choice of GAN Architectures for Experiments
■■ tLAPGANr 2015 (LaPIKian pyramid coding)
DCGAN, 2016 (Transposed convolution in generat
`ɑr)
SAGAN, 201& (Self-attention module)
fake samples)
ORGAN, 2018 (Integral probability metric)
WGAN-LR 2017 (Lipschitz penalty on WGAN1 less
sensitive)
-j MRGAN, 2016 (Penalize missing modes)
√BEGAN, 2017 (Autoencoder as discriminator)
:黑GAN. 2017 (Progressive manner during Eini	Uto6AN, 2019 (multi-level aɑhitedure SearehE
β°BigGAN, 2019 (Deeper net and larger batch size)
j VLG, 2020 (A local sparse attention layer)
ɪ CGANp 2014 (Labelinfo into discriminator and ge
nerator)
：，AC-GAN, 2017 (AUXiBary classifier/
Latent SPaCe ∣^ : , BiGAN, 2016 (Encoder for learning inverse mappi
，InfoGAN, 2016 (Classifier 玉r Iabe㈤J
-CyCleGAN, 2017; DiscoGAN, 2017; DuaIGAN, 2017 - Image style transfer
-SRGAN, 2016 - Image super-resolution
StyIeGAN. 2019 Seale-specific face generation
J Application focused - Face Completion GAN, 2017 — Face completion
-AIphaGAN, 2018 — Image matting
-Moco-GAN , 2018 — DVD-GAN, 2019 — Video generation
-SinGAN, 2019 - Image manipulation learned by one image
1- *WGAN, 2017 (WaSSerStein distance)
-'Geometric GAN, 2017 (Hinge loss) — Sphere GAN, 2019 (Riemannian manifolds)
,FCGAN, 2014 (JS divergence) — i LSGAN, 2016 (Pearson divergence) — f-GAN, 2016 (f-divergence)
JUGAN, 2016 (Second order gradient loss)
on WGAN
WGAN-CT. 2018 (Soft consistency on WGAN)
-β∙SN-GAN, 20IS (Spectral normalization)
_ SS-GAN, 2019 ($«lf-supervision avoid discriminM 'l
a forgetting)	J
Figure 15:	Taxonomy of GAN Architectures from (Wang et al., 2019)
We carried out experiments with the variants of GANs to evaluate the performance of our DO-GAN
framework. We refer to the taxonomy of GANs (Wang et al., 2019) and choose each architecture from
the groups of GANs focused on Network Architecture, Latent Space and Loss: DCGAN, SNGAN
and SGAN as shown in Figure 15. We have also included comparisons with mixture architectures
such as MIXGAN and MGAN.
18
Under review as a conference paper at ICLR 2021
H Example of Meta-matrix with 5
generators and 5 discriminators
SJ2ro≡E1ExQ
(a)	Meta-matrix at epoch t = 5
Meta-Strategies: σg* = [0, 0, 0, 0,1], σ5* = [0, 0, 0, 0,1]
Expected Payoff: U5(σg*,σ5*) = 0.017
(b)	Meta-matrix at epoch t = 20
Meta-Strategies:
σg0* = [0.00, 0.27, 0.32, 0.00, 0.41]
σd0* = [0.29, 0.00, 0.32, 0.00, 0.38]
Expected Payoff: U 20(σ20*,σ20∙) = 1.69
19