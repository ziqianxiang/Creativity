Under review as a conference paper at ICLR 2021
Boosting One-Point Derivative-Free Online
Optimization via Residual Feedback
Anonymous authors
Paper under double-blind review
Abstract
Zeroth-order optimization (ZO) typically relies on two-point feedback to estimate
the unknown gradient of the objective function, which queries the objective function
value twice at each time instant. However, if the objective function is time-varying,
as in online optimization, two-point feedback can not be used. In this case, the
gradient can be estimated using one-point feedback that queries a single function
value at each time instant, although at the expense of producing gradient estimates
with large variance. In this work, we propose a new one-point feedback method
for online optimization that estimates the objective function gradient using the
residual between two feedback points at consecutive time instants. We study the
regret bound of ZO with residual feedback for both convex and nonconvex online
optimization problems. Specifically, for both Lipschitz and smooth functions,
we show that using residual feedback produces gradient estimates with much
smaller variance compared to conventional one-point feedback methods, which
improves the learning rate. Our regret bound for ZO with residual feedback is
tighter than the existing regret bound for ZO with conventional one-point feedback
and relies on weaker assumptions, which suggests that ZO with our proposed
residual feedback can better track the optimizer of online optimization problems.
We provide numerical experiments that demonstrate that ZO with residual feedback
significantly outperforms existing one-point feedback methods in practice.
1 Introduction
Zeroth-order optimization (ZO) algorithms have been widely used to solve online optimization
problems where first or second order information (i.e., gradient or Hessian information) is unavailable
at each time instant. Such problems arise, e.g., in online learning and involve adversarial training
Chen et al. (2017) and reinforcement learning Fazel et al. (2018); Malik et al. (2018) among others.
The goal in online optimization is to minimize a sequence of time-varying objective functions
{ft(X)}t=1:T, where the value ft(Xt) is revealed to the agent after an action Xt is selected and is
used to adapt the agent’s future strategy. Since the future objective functions are not known a priori,
the performance of the online decision process can be measured using notions of regret, generally
defined as the difference between the total cost incurred by the decision selected by the agent online
and the cost of the fixed or varying optimal decision that a clairvoyant agent could select.
Perhaps the most popular zeroth-order gradient estimator is the two-point estimator that has been
extensively studied in Agarwal et al. (2010); Ghadimi & Lan (2013); Duchi et al. (2015); Ghadimi
et al. (2016); Bach & Perchet (2016); Nesterov & Spokoiny (2017); Gao et al. (2018); Roy et al.
(2019). Specifically, the two-point estimator queries the function value ft(X) for twice, for two
different realizations of the decision variables, and uses the difference in these function values to
estimate the desired gradient, as illustrated by the equation
(Two-point feedback): e(2)(x) = u (ft(X + δu) - ft(X)),
(1)
where δ > 0 is a parameter and u ~ N(0, I). However, the two-point gradient estimator can not be
used for the solution of non-stationary online optimization problems that arise frequently, e.g., in
online learning. The reason is that in these non-stationary online optimization problems, the objective
1
Under review as a conference paper at ICLR 2021
function being queried is time-varying, and hence only a single function value can be sampled at a
given time instant. In this case, the following one-point feedback can be used
(One-point feedback): e(1)(χ) = Uft(X + δu),	(2)
δ
which queries the objective function ft(x) only once at each time instant. One-point feedback was
first proposed and analyzed in Flaxman et al. (2005) for the solution of online convex optimization
problems. Saha & Tewari (2011); Hazan & Levy (2014); Dekel et al. (2015) showed that the regret of
convex online optimization methods using one-point gradient estimation can be improved assuming
smoothness or strong convexity of the objective functions and using self-concordant regularization.
More recently, Gasnikov et al. (2017) developed such regret bounds for stochastic convex problems.
On the other hand, Hazan et al. (2016) characterized the convergence of one-point zeroth-order
methods for static stochastic non-convex optimization problems. However, as shown in these studies,
a limitation of one-point feedback is that the resulting gradient estimator has large variance and,
therefore, induces large regret. In addition, the regret analysis for ZO with one-point feedback usually
requires the strong assumption that the function value is uniformly upper bounded over time, so this
method can not be used for practical non-stationary optimization problems.
Contributions: In this paper, we propose a novel one-point gradient estimator for zeroth-order online
optimization and develop new regret bounds to study its performance. Specifically, our contributions
are as follows. We propose a new one-point feedback scheme which requires a single function
evaluation at each time instant. This feedback scheme estimates the gradient using the residual
between two consecutive feedback points and we refer to it as residual feedback. We show that our
residual feedback induces a smaller gradient estimation variance than the conventional one-point
feedback scheme in Flaxman et al. (2005); Gasnikov et al. (2017). Furthermore, we provide regret
bounds for online convex optimization with our proposed residual feedback estimator. Our analysis
relies on a weaker assumption than the one needed in the case of the conventional one-point estimator,
and our proposed regret bounds are tighter especially when the value of the objective function is large.
In addition, we provide regret bounds for online non-convex optimization with residual feedback.
Finally, we present numerical experiments that demonstrate that the proposed residual-feedback
estimator significantly outperforms the conventional one-point method in its ability to track the
time-varying optimizers of online learning problems. To the best of our knowledge, this is the first
time a one-point zeroth-order method is theoretically studied for online non-convex optimization
problems. It is also the first time that a one-point gradient estimator demonstrates comparable
empirical performance to that of the two-point method. We note that two-point estimators can only
be used to solve online non-stationary learning problems in simulations, where the system can be
hard coded to be fixed during two queries of the objective function values at two different decision
variables.
Related work: Zeroth-order methods have been used to solve many different types of optimization
problems. For example, Balasubramanian & Ghadimi (2018) apply ZO to solve a set-constrained
optimization problem where the projection onto the constraint set is non-trivial. Gorbunov et al.
(2018); Ji et al. (2019) apply a variance-reduced technique and acceleration schemes to achieve better
convergence speed in ZO. Wang et al. (2018) improve the dependence of the iteration complexity
on the dimension of the problem under an additional sparsity assumption on the gradient of the
objective function. And Hajinezhad & Zavlanos (2018); Tang & Li (2019) apply zeroth-order oracles
to distributed optimization problems when only bandit feedbacks are available at each local agents.
Our proposed residual feedback oracle can be used to solve such online optimization problems as well.
Also related is work by Zhang et al. (2015) that considers non-convex online bandit optimization
problems with a single query at each time step. However, this method employs the exploration and
exploitation bandit learning framework and the proposed analysis is restricted to a special class of
non-convex objective functions. Finally, Agarwal et al. (2011); Hazan & Li (2016); Bubeck et al.
(2017) study online bandit algorithms using ellipsoid methods. In particular, these methods induce
heavy computation per step and achieve regret bounds that have bad dependence on the problem
dimension. As a comparison, our one-point method is computation light and achieves regret bounds
that have better dependence on the problem dimension.
2	Preliminaries and Residual Feedback
We first introduce the classes of Lipschitz and smooth functions.
2
Under review as a conference paper at ICLR 2021
Definition 2.1 (Lipschitz functions). The class of Lipschtiz-continuous functions C0,0 satisfies: for
any f ∈ C0,0, |f (x)-f (y)| ≤ Lo∣∣x-yk, ∀x, y ∈ Rd, where Lo > 0 is theLipschitzparameter. The
class Ofsmoothfunctions C1,1 satisfies: for any f ∈ C1,1, kVf (x) -Vf (y)k ≤ Lι∣∣x 一 yk, ∀x, y ∈
Rd , where L1 > 0 is the smoothness parameter.
In ZO, the objective is to estimate the first-order gradient of a function using zeroth-order oracles.
Necessarily, we need to perturb the function around the current point along all the directions uniformly
in order to estimate the gradient. This motivates us to consider the Gaussian-smoothed version of the
function f as introduced in NesteroV & SPokoiny (2017), fδ(x) := Eu〜N(0,1)[f (X + δu)], where the
coordinates of the vector u are i.i.d standard Gaussian random variables. The following bounds on
the aPProximation error of the function fδ(x) haVe been deVeloPed in NesteroV & SPokoiny (2017).
Lemma 2.2. Consider a function f and its smoothed version fδ. It holds that
lfδ(x)-f(x)l ≤ {δLL,；：：；；：	and kVfδ(X)-Vf(X)k ≤ δL1(d + 3)3/2, if f ∈ C1,1.
The smoothed function fδ(x) satisfies the following amenable ProPerty NesteroV & SPokoiny (2017).
Lemma 2.3. If f ∈ C0,0 is Lo-Lipschitz, then fδ ∈ C1,1 With Lipschitz constant L∖ = √dδ-1Lo.
Consider the following online bandit oPtimization Problem.
T-1
min X ft(X),	(P)
x∈X t=0
where X⊂Rd is a conVex set and {ft}t is a random sequence of objectiVe functions. In this setting,
the objectiVe functions {ft }t are unknown a priori and their deriVatiVes are unaVailable. At time
t, a new objectiVe function ft is randomly generated indePendent of an agent’s decisions, and then
the agent queries the objectiVe function Value at certain Perturbed Points and use them to uPdate the
current Policy Parameters. The goal of the agent is to minimize a certain regret function.
Such an online setting often occurs in non-stationary learning scenarios where either the system is
time-Varying on its own or a single query of the function ft changes the system state (i.e., ft changes
to ft+1 ). In this non-stationary setting, the conVentional two-Point feedback scheme is known to
be imPractical as it requires to eValuate ft at two different Points at the same time t. Instead, it is
natural to use the one-Point feedback scheme (2) in GasnikoV et al. (2017). HoweVer, the gradient
estimate based on the aboVe one-Point feedback induces a large Variance that leads to a large regret.
In this PaPer, we focus on such an one-Point deriVatiVe-free setting and ProPose the following noVel
one-Point residual feedback scheme for estimating the gradient with reduced Variance.
(Residual feedback):	gt(χt):= Ut (ft(xt + δut) — ft-1(χt-1 + δu1)),	(3)
where u—, Ut 〜N(0, I) are independent random vectors. To elaborate, the residual feedback
in (3) queries ft at a single Perturbed Point Xt + δut, and then subtracts it by ft-1(Xt-1 + δut-1)
obtained from the previous iteration. We name such a scheme as one-point residual feedback. Next,
we explore some basic properties of the residual feedback. We first show that this estimator is an
unbiased gradient estimate of the smoothed function fδ,t .
Lemma 2.4. The residualfeedback satisfies E[et(xt)] = Vfδ,t(xt) forall Xt ∈ X and t.
Proof. By the fact that Ut has zero mean and is independent from u— and x1.	□
We consider the following ZO algorithm with residual feedback
(ZO with residual feedback): xt+1 = Πχ (Xt — ηgt(χt)),	(4)
where η is the learning rate and ΠX is the projection operator onto the set X . The update (4) can
be implemented assuming that the objective function can be queried at points outside the feasible
set X, similar to the methods considered in Duchi et al. (2015); Bach & Perchet (2016); Gasnikov
et al. (2017). Note that it is possible to modify the update (4) so that the iterates are guaranteed to be
within the feasible set X . This modification and related analysis can be found in Section H in the
3
Under review as a conference paper at ICLR 2021
supplementary material. The requirement that the objective function is evaluated at feasible points in
derivative-free optimization algorithms has also been considered in Bubeck et al. (2017); Bilenne
et al. (2020). Specifically, Bubeck et al. (2017) develop the so called ellipsoid method, which requires
computation of an ellipsoid containing the optimizer at each time step. On the other hand, almost
concurrently with this work, Bilenne et al. (2020) proposed a similar oracle as in (3) for a static
convex optimization problem with specific objective and constraint functions. Next, we bound the
second moment of the gradient estimate based on the residual feedback.
Lemma 2.5 (Second moment). Assume that ft ∈ C0,0 with Lipschitz constant L0 for all time t.
Then, under the ZO update rule in (4), the second moment of the residual feedback satisfies: for all t,
E[ket(xt)k2] ≤ 4⅛2E[ket-1(xt-1)k2] + Dt,	(5)
δ2
where Dt ：= 16L2(d + 4)2 + 羽叫(ft(xt-ι + δut-ι) — ft-1(xt-1 + δut-ι))].
The above lemma shows that the second moment of the residual feedback can be bounded
by a perturbed contraction, provided that we choose η and δ such that the contracting rate
α = 4dL20η2δ-2 < 1. As we show later in the analysis, such a contraction property leads to a
small variance of the residual feedback that helps reduce the regret of the online ZO algorithm.
3	ZOwith Residual Feedback for Online Convex Optimization
In this section, we consider the online bandit problem (P) where the sequence of functions {ft}t=0:T -1
are all convex. In particular, we are interested in analyzing the following static regret of the algorithm.
RT := Eh X ft(xt) - m∈iXn X ft(x)i.	(6)
t=0	t=0
We make the following assumption on the non-stationary of the online learning problem.
Assumption 3.1 (Bounded variation). There exists Vf > 0 such that for all t,
EIjft(Xt—1 + δut-1)-ft-ι(xt-ι + δut-1)∣2] ≤ Vf,	⑺
where the expectation is taken over xt—1, the random vector ut—1 and the random functions ft—1,ft.
Intuitively, we assume the squared variation of the objective function between two consecutive
time instants is uniformly bounded over time. We note that this assumption is much weaker than
the uniformly bounded function value assumption, i.e., EIft(x)2] ≤ B2, ∀t, x ∈X, which is
used in the analysis of ZO with the conventional one-point feedback Gasnikov et al. (2017). In
particular, under Assumption 3.1, the perturbation term in Lemma 2.5 can be bounded as Dt ≤
16L0(d + 4)2 + 2dV；δ-2. Then, by telescoping the contraction inequality, We obtain the following
bound for the second moment of the residual-feedback gradient estimate,
E[kgt(xt)k2] ≤ max {E[kg0(X0)k2], ɪ (l6L2(d + 4)2 + 2d叫}.	(8)
In practice, δ is usually chosen to be sufficiently small, and the above bound is dominated by
O(dδ-2 Vj), which is much smaller than the second moment bound of the conventional one-point
feedback O(dδ-2B2) (B2 is the uniform bound of the second moment of ft over time). For example,
consider the time-varying objective functions, f0(x) = 1/2x2 and ft(x) = ft—1(x) + nt, where
nt is Gaussian noise with zero mean at time t. Then, it can be verified that Assumption 3.1 holds
with a finite Vf whereas the second moment of ft(x) is unbounded over time. This suggests that the
variance of the residual feedback can be significantly smaller than that of the conventional one-point
feedback.
Next, we first consider the case where the objective function ft is convex and Lipschitz. Based on the
above characterization of the second moment of residual feedback, we obtain the following regret
bound for ZO with residual feedback.
4
Under review as a conference paper at ICLR 2021
Theorem 3.2 (Regret for Convex Lipschitz ft). Let Assumption 3.1 hold. Assume that ft ∈ C0,0 is
convex with Lipschitz constant Lo for all t and ∣∣xo 一 x*∣ ≤ R. Run ZO with residual feedbackfor
T > R2 iterations with η = R2 (2√2Lo√dT3 )-1 and δ = √RT- 1. Then, we have that
RT ≤ √2Lo√dRT4 +
E[kgo (xo)k2]R 2
2√2dLoT 4
+ 8√2(d +^4)2 Lo R 2 T 4
d
+ 2Lo√dRT4 + √2dRV^Lo-1T3.
Asymptotically, we have RT = O((Lo + Lo-1Vy )VdRT3).
(9)
To the best of our knowledge, the best known regret for ZO with the conventional one-point feedback
is of the order O(√dLoRBT3) Gasnikov et al. (2017). Therefore, our regret bound is tighter if
1	3
the function variation satisfies V； ≤ O(B2 Ll2). Essentially, using the proposed residual feedback
gradient estimator, the regret of ZO no longer depends on the uniform bound of the function value,
which can be huge in practice. Instead, our regret only relies on how fast the function varies over
time.
Remark 3.3. We note that the complexity bound in Theorem 3.2 generally depends on the values of
the Lipschitzparameters Lo, L1 and the constant V；. Specifically, choose η = R3 (2√2Lo√dT4 )-1
and δ = √RL-q T- 1 with q > 0 as a tuning parameter, and we obtain that RT = O((Lo + Lo1-q +
L2q-1V^) VdRT4) when T ≥ LqR2. If Lo <1, we can choose q =1to achieve the bound
RT = O((LO + LoV2)√dRT4). On the other hand, if Lo ≥ 1,we can choose q =0to achieve
the bound RT = O((Lo + Lo-1V^)√dRT4). We note that the dependence of the bounds in
Theorems 3.4, 4.2 and 4.3 on Lo,L1 can also be optimized in a similar way by properly choosing δ.
Next, we present the regret of ZO with residual feedback for convex smooth objective functions.
Theorem 3.4 (Regret for Convex Smooth ft). Let Assumption 3.1 hold. Assume that ft (x) ∈
C0,0∩C1,1 is convex with Lipschitz constant L0 and smoothness constant L1 for all t, and assume that
∣∣xo —x*k ≤ R. RunZOwithresidualfeedbackfor T > R2 iterations with η = R4 (2√2Lod2T2 )-1
and δ = R 1 d- 1 T- 1. Then, we have that
Rt ≤√2Lod2R3T3 + [T
+ 8√2Lo (dj R 3 T 1
d 3
-	2_2_2	2_2	2
+ 2L1d3 R3 T 3 + CLO	dI3 R3 V2 T3.
(10)
Asymptotically, the above regret bound is in the order of O((LO + L1 + Lo-1V； )(dRT) 2).
To the best of our knowledge, the best known regret for ZO with the conventional one-point feedback
in the convex smooth case is of the order O(L3 (dRBT)3) GasnikoV et al. (2017). Therefore,
our regret bound is tighter if the function variation satisfies 蜉 ≤ O(B2L3 Lo). Our numerical
experiments show that ZO with residual feedback always outperforms ZO with the conventional
one-point feedback in practice.
4	ZO with Residual Feedback for Online Nonconvex Optimization
In this section, we analyze the regret of ZO with residual feedback in solving the unconstrained
online bandit problem (P) with nonconvex functions. Throughout this section, we make the following
assumption regarding the objective functions.
Assumption 4.1. There exist WT , WT > 0 such that the following conditions hold for all t.
1.	PT=1 E[fδ,t(xt) 一 fδ,t-1(xt)] ≤ WT, where the expectation is taken with respect to xt and the
random smoothed objective functions fδ,t-1, fδ,t.
2.	PT=1 E[∣ft(xt-1 + δut-1) — ft-1(xt-1 + δut-1)∣2] ≤ fτ , where the expectation is taken with
respect to xt-1, the random vector ut-1 and the random objective functions ft-1, ft.
5
Under review as a conference paper at ICLR 2021
The above two conditions measure the accumulated first-order and second-order function variations,
as also adopted by Roy et al. (2019).
Next, we consider the case where {ft }t are nonconvex and Lipschitz continuous functions. Since the
objective function f is not necessarily differentiable, i.e., Vf (t) is not well defined, We define the
regret as the accumulated gradient of the smoothed function, i.e., R^ := PT- E[∣∣Vfδ,t(χt) ∣∣2].
In addition, it is often required that the smoothed function fδ,t is close to the original function ft such
that ∣fδ,t(x) 一 ft(x)∣ ≤ ef for all t. To satisfy this condition, We need to choose δ ≤ (√dL0)-1ef
according to Lemma 2.2. We obtain the following regret bound for ZO with residual feedback.
Theorem 4.2 (Nonconvex Lipschitz ft). Let Assumptions 4.1 hold. Assume that ft ∈ C0,0 with
Lipschitz constant Lo and that ft is bounded below by f for all t. Run ZO with residualfeedbaCkfor
“	3 LCQ 1	-	1	“
T > (def)T iterations with η = e 2 (2√2L2d 2 T 2 )-1 and δ = ef (d 2 L0)-1. Then, we have that
丁	3 3 /	、3 — 3ι e2EΓkg0(x0)k2l
RT,δ ≤ 2√2L0(E[fδ,o(xo)] - fδ,τ + WT)d2e-2T2 + f ")
2 2dT
L d 2 Wτ
+ 4√2Loe∣ (d +14)2 T1 +
f d 2
√2 e 2 T 2
(11)
Asymptotically, we have %f5 = O(d3L0e- 2 (WT + fτTT)T1 + d3Loe2 T1).
Based on Theorem 4.2, We observe that the regret bound satisfies Rτ /T → 0 Whenever Wτ =
1	3	3	3 1
o(T2e2) and WT = o(T2e2). In particular, If the bounded variation Assumption 3.1 holds, then We
1	3
have fτ ≤ O(TVf), and it suffices to let T-2 eʃ 2 = o(1).
Next, We consider the nonconvex and smooth problem and study the regret RgT :=
PT=-o1 E[∣Vft(xt)∣2]. We obtain the folloWing regret for ZO With residual-feedback.
Theorem 4.3 (Nonconvex smooth ft). Let Assumptions 4.1 hold. Assume that ft ∈ Co,o ∩ C1,1 with
Lipschitz constant Lo and smoothness constant Li and that ft is bounded below by f^ for all t. Run
ZO with residualfeedbackfor T iterations with η = (2√2Lod4T1 )-i and δ = (d5T1 )-i. Then,
RT ≤ 4√2Lo(E[fδ,o(xo)] - fδ,τ + WT)d4T2 + LI.„『]
V2Lod3 T 2
+ 8√2LιLo (d +44)2 T1 + 2L1dd3 fτ + 2L2 (d +53)3 T1.
d 3	Lo	d 3
Asymptotically, the above regret bound is in the order of O(d4 LOWTT1 + d4 LiLo-ifT).
(12)
Based on Theorem 4.3, we observe that the regret bound satisfies RT/T → 0 whenever WT = o(T2)
and WT = o(T). We note that these requirements of WT, WT are more relaxed than those in the
nonsmooth case, as they do not rely on the small parameter ef.
5 ZO with Residual Feedback for Stochastic Online Optimization
In this section, we generalize the residual feedback to solve stochastic online bandit problems. Since
its regret analysis follows the same proof logic as that ofZO with residual feedback, we only introduce
the key technical lemmas and comment on the proof difference. The stochastic online bandit problems
are formulated as follows.
T-1
min X E[Ft(x; ξt)], where E[Ft(x; ξt)] = ft(x), ∀t,	(R)
x∈X t=o
where ξt denotes a certain noise that is independent of x. Different from the previous deterministic
online setting, the agent in the stochastic setting can only query noisy evaluations of the function.
6
Under review as a conference paper at ICLR 2021
This covers the scenarios where the agent does not have access to the underlying data distribution. To
solve the above stochastic online problem, we propose the following stochastic residual feedback
gt(Xt) = ~δ (Ft(Xt + δut; ξt) - Ft-I(Xt-1 + δut-1; ξt-1)),	(13)
where ξt-1 and ξt are independent random samples that are sampled in the iterations t - 1 and t,
respectively. Since the noisy function value F(X; ξt) is an unbiased estimate of the objective function
ft(X), it is straightforward to show that (13) is an unbiased gradient estimate of the function fδ,t(X).
To analyze the regret of ZO with stochastic residual feedback, we first consider the convex setting
and make the following assumption that bounds the variation of the stochastic functions.
Assumption 5.1. (Bounded stochastic variation) There exists Vf,ξ > 0 such that for all t,
E[(Ft(Xt-I + δut-1, ξt) - Ft-I(Xt-1 + δut-1, ξt-1)) ] ≤ Vf2ξ,
where the expectation is taken with respect to Xt-1, the random vector ut-1 and the random objective
functions Ft-1(∙, ξt-1), Ft(∙, ξt).
The above assumption generalizes Assumption 3.1 to the stochastic setting. The bound V 2 controls
both the variation of function over time and the variation due to stochastic sampling. ,
The following lemma characterizes the second moment of the stochastic residual feedback.
Lemma 5.2. Assume F(X, ξ) ∈ C0,0 with Lipschitz constant L0 for all ξ. Then, under the ZO update
rule, we have that
E[ket(Xt)k2] ≤ 4dL0^E[ket(Xt-1)k2] + Dt,ξ,
where Dt,ξ := 16L0(d + 4)2 + δdE[(Ft(Xt-I + δut-1, ξt) - Ft-I(Xt-1 + δut-1, ξt-1))].
Observe that the above second moment bound is very similar to that in Lemma 2.5, and the only
difference is the perturbation term. In particular, the perturbation term Dt,ξ can be further bounded
by leveraging Assumption 5.1, and the resulting second moment bound is almost the same as that
in eq. (8) for the deterministic case (simply replace Vf in eq. (8) by Vf,ξ). Therefore, the regret
analysis of ZO with stochastic residual feedback is the same as that of ZO with residual feedback in
the deterministic online setting. Consequently, ZO with stochastic residual feedback achieves almost
the same regret bounds as those in Theorems 3.2 and 3.4, and one simply needs to replace Vf by Vf,ξ.
For the nonconvex setting, we adopt the following assumption that generalizes Assumption 4.1.
Assumption 5.3. There exists WT, WT,ξ > 0 such that the following two conditions hold for all t.
1.	PT=1 E[fδ,t(Xt) - fδ,t-1(Xt)] ≤ WT, where the expectation is taken with respect to Xt and the
random smoothed objective functions fδ,t-1, fδ,t.
2.	PT=I E[∣Ft(Xt-1 + δut-1;ξt) - Ft-I(Xt-1 + δut-1;ξt-1)∣2] ≤ fτ,ξ, where the expectation
is taken with respect to Xt-1, the random vector ut-1 and the random objective functions
Ft-1(∙, ξt-1), Ft(∙, ξt).
Then, following the same proof logic as that of Theorems 4.2 and 4.3, on can obtain similar regret
bounds for ZO with stochastic residual feedback (simply replace Wτ, Wτ in Theorems 4.2 and 4.3
by Wτ,ξ, Wτ,ξ, respectively).
6	Numerical Experiments
In this section, we compare the performance of ZO with one-point, two-point and residual feedback
in solving two non-stationary reinforcement learning problems, i.e., LQR control and resource
allocation, in which either the reward or transition functions are varying over episodes.
6.1	Nonstatinoary LQR Control
We consider an LQR problem with noisy system dynamics. The static version of this problem is
considered in Fazel et al. (2018); Malik et al. (2018). Specifically, consider a system whose state
7
Under review as a conference paper at ICLR 2021
One-point Residual Feedback
Two-point Feedback (Impractical)
Conventional One-point Feedback
>000	4000	6000	8000 10000
Episode
⑶
Figure 1: The regrets of applying the proposed residual one-point feedback (3) (blue), the two-point oracle
in Bach & Perchet (2016) (orange) and the conventional one-point oracle in Gasnikov et al. (2017) (green) to
online policy optimization for the nonstationary LQR problem. In (a), the regrets PT=0 | V(Kt) - V(K*)| of
three methods are presented. In (b), the variance of the gradient estimates given by three methods are presented.
The two point method (orange) is infeasible to use in practice and is presented here to serve as the simulating
benchmark.
2000	4000	6000	8000	10000
Episode
(b)
xk ∈ Rnx at step k is subject to a transition function xk+1 = Atxk + Btuk + wk, where uk ∈ Rnu
is the action at step k, and At ∈ Rnx×nx and Bt ∈ Rnx ×nu are dynamical matrices in episode t.
These matrices are unknown and changing over episodes. The vector wk is the noise on the state
transition. Specifically, the entries of the dynamical matrices A0 and B0 at episode 0 are randomly
generated from a Gaussian distribution N(0, 0.12). Then, we generate the time-varying dynamical
matrices as At+1 = At + 0.01Mt and Bt+1 = Bt + 0.01Nt, where Mt and Nt are random matrices
whose entries are uniformly sampled from [0,1]. Moreover, consider a state feedback policy
uk = Ktxk, where Kt ∈ Rnu ×nx is the policy parameter that is fixed within episode t. Within
each episode, there exists an optimal policy Kt so that the discounted accumulated cost function
Vt (K) := E [ PH=-I Yk (XTQxk + UTRUk)] at episode t is minimized, where Y ≤ 1 is the discount
factor and H is the horizon. The goal is to track the time-varying optimal policy parameter Ktt so
that Vt(Kt) - Vt(Kt) is small in every episode.
We apply the conventional one-point method in Gasnikov et al. (2017) and the proposed residual-
feedback method (13) to solve the above non-stationary LQR problem. The performance of the two-
point method in Bach & Perchet (2016) is also presented as a benchmark, although it is impractical
in non-stationary scenarios. This is because the two-point method in Bach & Perchet (2016) requires
to evaluate value function Vt for two different policy functions at two consecutive episodes. However,
evaluating the value function Vt for a given policy during episode t requires to collect samples by
executing this policy. Then, during the subsequent episode t +1, since the problem is non-stationary,
the dynamic matrices change to At+1 , Bt+1 and so does the value function Vt+1 . Therefore, it is not
possible to evaluate the same value function Vt at two different episodes and, as a result, the two-point
method in Bach & Perchet (2016) is not applicable here. Each algorithm is run for 10 trials, and
the stepsizes are optimized respectively. The accumulated regrets PT=-01 |V(Kt) - V(Kt)| of these
algorithms are presented in Figure 1(a). We observe that the residual feedback method achieves a
much lower regret than the conventional one-point method and has a comparable performance to that
of the impractical two-point method. Moreover, we present in Figure 1(b) the estimated variance
of the gradient estimates of these three methods at the policy iterates over episodes. It can be seen
that the variance of our proposed residual-feedback is close to the impractical two-point feedback
and is much smaller than that of the conventional one-point feedback. This observation justifies our
theoretical characterization of the second moment of the residual feedback.
6.2	Nonstationary Resource Allocation
We consider a multi-stage resource allocation problem with time-varying sensitivity to the lack
of resource supply. Specifically, 16 agents are located on a 4 × 4 grid. During episode t, at
step k, agent i stores mi(k) amount of resources and has a demand for resources in the amount
of di(k). Also, agent i decides to send a fraction of resources aij(k) ∈ [0, 1] to its neighbors
8
Under review as a conference paper at ICLR 2021
O 2000	4000	6000	8000 IOOOO
Episode
IO4 *--------------
ɪθi ------ One-point Residual Feedback
-----Two-point Feedback (Impractical)
----- Conventional One-point Feedback
10° -----------------------------------------------
0	2000	4000	6000	8000	10000
Episode
(a)	(b)
Figure 2: The costs during each episode by applying the proposed residual one-point feedback (3) (blue), the
two-point oracle in Bach & Perchet (2016) (orange) and the conventional one-point oracle in Gasnikov et al.
(2017) (green) to solve the non-stationary resource allocation problem are presented. In (a), the varying cost
Jt(θt) of three methods are presented. In (b), the variance of the gradient estimates at agent 1 given by three
methods are presented. The two point method (orange) is infeasible to use in practice and is presented here to
serve as the simulating benchmark.
j ∈Ni on the grid. The local amount of resources and demands of agent i evolve as mi(k + 1) =
mi(k) - Pj∈N aij (k)mi(k) + Pj∈N aji(k)mj (k) - di(k) and di(k) = ψi sin(ωik + φi) + wi,k,
where wi,k is the noise in the demand. At each step k, agent i receives a local cost ri,t(k), such
that ri,t(k) = 0 when mi(k) ≥ 0 and ri,t(k) = ζtmi(k)2 when mi (k) < 0, where ζt represents
the varying sensitivity of the agents to the lack of supply during episode t. Let agent i makes its
decisions according to a parameterized policy function πi,t(oi; θi,t): Oi → [0, 1]|Ni|, where θi,t is
the parameter of the policy function πi,t at episode t, oi ∈Oi denotes agent i’s local observation.
Specifically, we let oi(k)=[mi(k),di(k)]T. Our goal is to track the time-varying optimal policy
so that the accumulated cost over the grid Jt(θt)=P1=61 PH=0 γkri,t(k) during each episode is
maintained at a low level, where θt =[...,θi,t,...] is the policy parameter, H is the problem horizon
at each episode, and γ is the discount factor.
In Figure 2(a), we present the costs achieved during each episode Jt(θt) with 10 trials using ZO
with the residual-feedback, one-point and the impractical two-point feedback. It can be seen that
our proposed residual-feedback achieves a cost Jt (θt) that is as low as the cost achieved by the
impractical two-point feedback in such a non-stationary environment. In particular, both residual
and two-point feedback perform much better than the conventional one-point feedback. Moreover,
Figure 2(b) compares the estimated variances of these feedback schemes, and one can observe that
the variance of the residual feedback is comparable to that of the two-point feedback and is much
smaller than that of the conventional one-point feedback.
7	Conclusion
In this paper, we proposed a residual one-point feedback oracle for zeroth-order online learning
problems, which estimates the gradient of the time-varying objective function using a single query of
the function value at each time instant. We showed that the regret bound of the proposed residual
feedback estimator can be much lower than that of the conventional one-point method in online
convex optimization setting. In addition, we studied the gradient size regret bound of the residual-
feedback estimator when it is applied to the online non-convex optimization problems. Numerical
experiments on two non-stationary reinforcement learning problems were conducted and the proposed
residual-feedback estimator was shown to significantly outperform the conventional one-point method
in non-stationary online learning problems.
References
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with
multi-point bandit feedback. In COLT,, pp. 28-40. Citeseer, 2010.
9
Under review as a conference paper at ICLR 2021
Alekh Agarwal, Dean P Foster, Daniel J Hsu, Sham M Kakade, and Alexander Rakhlin. Stochastic
convex optimization with bandit feedback. In Advances in Neural Information Processing Systems,
pp.1035-1043,2011.
Francis Bach and Vianney Perchet. Highly-smooth zero-th order online optimization. In Conference
on Learning Theory, pp. 257-283, 2016.
Krishnakumar Balasubramanian and Saeed Ghadimi. Zeroth-order (non)-convex stochastic optimiza-
tion via conditional gradient and gradient updates. In Advances in Neural Information Processing
Systems, pp. 3455-3464, 2018.
Olivier Bilenne, Panayotis Mertikopoulos, and Elena-Veronica Belmega. Fast optimization with
zeroth-order feedback in distributed, multi-user mimo systems. IEEE Transactions on Signal
Processing, 2020.
Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1-122, 2012.
Sebastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex optimiza-
tion. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp.
72-85, 2017.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26, 2017.
Ofer Dekel, Ronen Eldan, and Tomer Koren. Bandit smooth convex optimization: Improving the
bias-variance tradeoff. In Advances in Neural Information Processing Systems, pp. 2926-2934,
2015.
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for
zero-order convex optimization: The power of two function evaluations. IEEE Transactions on
Information Theory, 61(5):2788-2806, 2015.
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi. Global convergence of policy gradient
methods for the linear quadratic regulator. In Proceedings of the 35th International Conference on
Machine Learning, volume 80, 2018.
Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization
in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual
ACM-SIAM symposium on Discrete algorithms, pp. 385-394. Society for Industrial and Applied
Mathematics, 2005.
Xiand Gao, Xiaobo Li, and Shuzhong Zhang. Online learning with non-convex losses and non-
stationary regret. In International Conference on Artificial Intelligence and Statistics, pp. 235-243,
2018.
Alexander V Gasnikov, Ekaterina A Krymova, Anastasia A Lagunovskaya, Ilnura N Usmanova,
and Fedor A Fedorenko. Stochastic online optimization. single-point and multi-point non-linear
multi-armed bandits. convex and strongly-convex case. Automation and remote control, 78(2):
224-234, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods
for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267-305,
2016.
Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov. An accelerated method for
derivative-free smooth stochastic convex optimization. arXiv preprint arXiv:1802.09022, 2018.
10
Under review as a conference paper at ICLR 2021
Davood Hajinezhad and Michael M Zavlanos. Gradient-free multi-agent nonconvex nonsmooth
optimization. In 2018 IEEE Conference on Decision and Control (CDC), pp. 4939-4944. IEEE,
2018.
Elad Hazan and Kfir Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural
Information Processing Systems, pp. 784-792, 2014.
Elad Hazan and Yuanzhi Li. An optimal algorithm for bandit convex optimization. arXiv preprint
arXiv:1603.04350, 2016.
Elad Hazan, Kfir Yehuda Levy, and Shai Shalev-Shwartz. On graduated optimization for stochastic
non-convex problems. In International conference on machine learning, pp. 1833-1841, 2016.
Kaiyi Ji, Zhe Wang, Yi Zhou, and Yingbin Liang. Improved zeroth-order variance reduced algorithms
and analysis for nonconvex optimization. arXiv preprint arXiv:1910.12166, 2019.
Dhruv Malik, Ashwin Pananjady, Kush Bhatia, Koulik Khamaru, Peter L Bartlett, and Martin J
Wainwright. Derivative-free methods for policy optimization: Guarantees for linear quadratic
systems. arXiv preprint arXiv:1812.08305, 2018.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions.
Foundations of Computational Mathematics, 17(2):527-566, 2017.
Abhishek Roy, Krishnakumar Balasubramanian, Saeed Ghadimi, and Prasant Mohapatra. Multi-
point bandit algorithms for nonstationary online nonconvex optimization. arXiv preprint
arXiv:1907.13616, 2019.
Ankan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimization
with bandit feedback. In Proceedings of the Fourteenth International Conference on Artificial
Intelligence and Statistics, pp. 636-642, 2011.
Yujie Tang and Na Li. Distributed zero-order algorithms for nonconvex multi-agent optimization. In
2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp.
781-786. IEEE, 2019.
Yining Wang, Simon Du, Sivaraman Balakrishnan, and Aarti Singh. Stochastic zeroth-order opti-
mization in high dimensions. In International Conference on Artificial Intelligence and Statistics,
pp. 1356-1365, 2018.
Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou. Online bandit learning for a special class
of non-convex losses. In AAAI, pp. 3158-3164, 2015.
11