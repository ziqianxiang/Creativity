Under review as a conference paper at ICLR 2021
Should Ensemble Members Be Calibrated
Anonymous authors
Paper under double-blind review
Ab stract
Underlying the use of statistical approaches for a wide range of applications is the
assumption that the probabilities obtained from a statistical model are represen-
tative of the “true” probability that event, or outcome, will occur. Unfortunately,
for modern deep neural networks this is not the case, they are often observed to
be poorly calibrated. Additionally, these deep learning approaches make use of
large numbers of model parameters, motivating the use of Bayesian, or ensemble
approximation, approaches to handle issues with parameter estimation. This pa-
per explores the application of calibration schemes to deep ensembles from both
a theoretical perspective and empirically on a standard image classification task,
CIFAR-100. The underlying theoretical requirements for calibration, and associ-
ated calibration criteria, are first described. It is shown that well calibrated ensem-
ble members will not necessarily yield a well calibrated ensemble prediction, and
if the ensemble prediction is well calibrated its performance cannot exceed that of
the average performance of the calibrated ensemble members. On CIFAR-100 the
impact of calibration for ensemble prediction, and associated calibration is evalu-
ated. Additionally the situation where multiple different topologies are combined
together is discussed.
1 Introduction
Deep learning approaches achieve state-of-the-art performance in a wide range of applications, in-
cluding image classification. However, these networks tend to be overconfident in their predictions,
they often exhibit poor calibration. A system is well calibrated, if when the system makes a pre-
diction with probability of 0.6 then 60% of the time that prediction is correct. Calibration is very
important in deploying system, especially in risk-sensitive tasks, such as medicine (Jiang et al.,
2012), auto-driving (Bojarski et al., 2016), and economics (Gneiting et al., 2007). It was shown by
Niculescu-Mizil & Caruana (2005) that shallow neural networks are well calibrated. However, Guo
et al. (2017) found that more complex neural network model with deep structures do not exhibit
the same behaviour. This work motivated recent research into calibration for general deep learning
systems. Previous research has mainly examined calibration based on samples from the true data
distribution {x(i),y(i)}N=ι 〜p(x, ω),y((i ∈ {ωι,...,ωκ} (Zadrozny & Elkan, 2002; Vaicenavi-
cius et al., 2019). This analysis relies on the limiting behaviour as N → +∞ to define a well
calibrated system
P(y = y∣P(y∣χ; Θ) = P)= P ^⇒ λ iim X δ(y<py ) = P	(1)
N →+∞ i∈Sjp	|Sj |
where Sj = {i∣P(y(i) = j|x(i); θ) = p,i = 1,…,N} and y(i) the model prediction for x(i).
δ(s, t) = 1 if s = t, otherwise 0. However, Eq. (1) doesn’t explicitly reflect the relation between
P(y = y∣P(y∣x; θ) = P) and the underlying data distribution p(x,y). In this work we examine
this explicit relationship and use it to define a range of calibration evaluation criteria, including the
standard sample-based criteria.
One issue with deep-learning approaches is the large number of model parameters associated with
the networks. Deep ensembles (Lakshminarayanan et al., 2017) is a simple, effective, approach for
handling this problem. It has been found to improve performance, as well as allowing measures
of uncertainty. In recent literature there has been “contradictory” empirical observations about the
relationship between the calibration of the members of the ensemble and the calibration of the final
1
Under review as a conference paper at ICLR 2021
ensemble prediction (Rahaman & Thiery, 2020; Wen et al., 2020). In this paper, we examine the un-
derlying theory and empirical results relating to calibration with ensemble methods. We found, both
theoretically and empirically, that ensembling multiple calibrated models decreases the confidence
of final prediction, resulting in an ill-calibrated ensemble prediction. To address this, strategies to
calibrate the final ensemble prediction, rather than individual members, are required. Addition-
ally we empiricaly examine the situation where the ensemble is comprised of models with different
topologies, and resulting complexity/performance, requiring non-uniform ensemble averaging.
In this study, we focus on post-hoc calibration of ensemble, based on temperature annealing. Guo
et al. (2017) conducted a thorough comparison of various existing post-hoc calibration methods and
found that temperature scaling was a simple, fast, and often highly effective approach to calibration.
However, standard temperature scaling acts globally for all regions of the input samples, i.e. all logits
are scaled towards one single direction, either increasing or decreasing the distribution entropy.
To address this constraint, that may hurt some legitimately confident predictions, we investigate
the effect of region-specific temperatures. Empirical results demonstrate the effectiveness of this
approach, with minimal increase in the number of calibration parameters.
2	Related Work
Calibration is inherently related to uncertainty modeling. Two of the most important scopes of cali-
bration are calibration evaluation and calibration system construction. One method to assessing cal-
ibration is the reliability diagram (VaicenaviciUs et al., 2019; Brocker, 2012). Though informative,
It is still desirable to have an overall metric. Widmann et al. (2019) investigate different distances in
the probability simplex for estimating calibration error. Nixon et al. (2019) point out the problem of
fixed spaced binning scheme, bins with few predictions may have low-bias but high-variance mea-
surement. Calibration error measure adaptive to dense populated regions have also been proposed
(Nixon et al., 2019). Vaicenavicius et al. (2019) treated the calibration evaluation as hypotheses
tests. All these approaches examine calibration criteria from a sample-based perspective, rather than
as a function of the underlying data distribution which is used in the thoretical analysis in this work.
There are two main approaches to calibrating systems. The first is to recalibrate the uncalibrated
systems with post-hoc calibration mapping, e.g. Platt scaling (Platt et al., 1999), isotonic regres-
sion (Zadrozny & Elkan, 2002), Dirichlet calibration (Kull et al., 2017; 2019). The second is to
directly build calibrated systems, via: (i) improving model structures, e.g. deep convolutional Gaus-
sian processes (Tran et al., 2019); (ii) data augmentation, e.g. adversarial samples (Hendrycks &
Dietterich, 2019; Stutz et al., 2020) or Mixup (Zhang et al., 2018); (iii) minimize calibration error
during training (Kumar et al., 2018). Calibration based on histogram binning (Zadrozny & Elkan,
2001), Bayesian binning (Naeini et al., 2015) and scaling binning (Kumar et al., 2019) are related
to our proposed dynamic temperature scaling, in the sense that the samples are divided into regions
and separate calibration mapping are applied. However, our method can preserve the property that
all predictions belonging to one sample sum to 1. The region-based classifier by Kuleshov & Liang
(2015) is also related to our approach.
Ensemble diversity has been proposed for improved calibration (Raftery et al., 2005; Stickland &
Murray, 2020). In Zhong & Kwok (2013), ensembles of SVM, logistic regressor, boosted decision
trees are investigated, where the combination weights of calibrated probabilities is based on AUC
of ROC. However, AUC is not comparable between different models as discussed in Ashukha et al.
(2020). In this work we investigate the combination of different deep neural network structures. The
weights assigned to the probabilities is optimised using a likelihood-based metric.
3	Calibration Framework
Let X ⊆ Rd be the d-dimensional input space and Y = {ω1, ..., ωK} be the discrete output
space consisting of K classes. The true underlying joint distribution for the data is p(x, ω) =
P(ω∣x)p(x), X ∈ X ,ω ∈ Y. Given some training data D 〜p(x, ω), a model θ is trained to predict
the distribution P(ω∣x; θ) given observation features. For a calibrated system the average predicted
posterior probability should equate to the average posterior of the underlying distribution for a spe-
cific probability region. Two extreme cases will always yield perfect calibration. First when the
predictions that are the same, and equal to the class prior for all inputs, P(ωj |x; θ) = P(ωj). Sec-
2
Under review as a conference paper at ICLR 2021
ond the minimum Bayes' risk classifier is obtained, P(ωj∣x; θ) = PKp(XPj幻k).Note that perfect
calibration doesn’t imply high accuracy, as shown by the system predic=ting the prior distribution.
3.1	Distribution Calibration
A system is calibrated if the predictive probability values can accurately indicate the portion of
correct predictions. Perfect calibration for a system that yields P(ω∣x; θ) when the training and test
data are obtained form the joint distribution p(x, ω) can be defined as:
P(ωj |x; θ)p(x)dx =	P(ωj |x)p(x)dx ∀p, ωj , → 0
x∈Rjp(θ,)	x∈Rjp(θ,)
Rp(θ,e) = {x卜P(ωj∣x; θ) - p| ≤ e, X ∈ X}
(2)
(3)
Rjp (θ, ) denotes the region of input space where the system predictive probability for class ωj is
sufficiently close, within error of , to the probability p. A perfectly calibrated system will satisfy
this expression for all regions, the expected predictive probability (left side of Eq. (2)) is identical to
the expected correctness, i.e., expected true probability (right side of Eq. (2)).
Rjp (θ, ) defines the region in which calibration is defined. For top-label calibration, only the most
probable class is considered and the region defined in Eq. (3) is modified to reflect this:
Rp(θ, e)= RP(θ,e) ∩ {x∣ωj = argmaxP(ω∣x; θ), X ∈ x}	(4)
Eq. (4) is a strict subset of Eq. (3). As the two calibration regions are different between calibration
and top-label calibration, perfect calibration doesn’t imply top-label calibration, and vise versa. A
simple illustrative example of this property is given in A.3. Binary classification, K = 2, is an
exception to this general rule, as the regions for top-label calibration are equivalent to those for
pp
perfect calibration, i.e. Rjp (θ, ) = Rjp(θ, ). Hence, perfect calibration is equivalent to top-label
calibration for binary classification (Nguyen & O’Connor, 2015).
Eq. (2) defines the requirements for a perfectly calibrated system. It is useful to define metrics that
allow how close a system is to perfect calibration to be assessed. Let the region calibration error be:
Cjp(θ,)
/
x∈Rjp (θ,)
(P(ωj |X; θ) - P(ωj |X))p(X)dX
(5)
This then allows two forms of expected calibration losses to be defined
ACE(θ)
K1 ∕1∣X S
dp;
ACCE(θ)
(6)
All Calibration Error (ACE) only considers the expected calibration error for a particular probability,
irrespective of the class associated with the data1 (Hendrycks et al., 2019). Hence, All Class Calibra-
tion Error (ACCE) that requires that all classes minimises the calibration error for all probabilities
is advocated by Kull et al. (2019); Kumar et al. (2019). Nixon et al. (2019) propose the Thresholded
Adaptive Calibration Error (TACE) to consider only the prediction larger than a threshold, and it
can be described as a special case of ACCE by replacing the integral range. Naeini et al. (2015) also
propose to only consider the region with maximum error.
Though measures such as ACE and ACCE require consistency of the expected posteriors with the
true distribution, for tasks with multiple classes, particularly large numbers of classes, the same
weight is given to the ability of the model to assign low probabilities to highly unlikely classes, and
high probabilities to the “correct” class. For systems with large numbers of classes this can yield
artificially low scores. To address this problem it is more common to replace the regions in Eq. (5)
with the top-label regions in Eq. (4), to give a top-label calibration error Cj(θ, e). This then yields
1In this section the references given refer to the sample-based equivalent versions of the distributional cali-
bration expressions in this paper using the same concepts, rather than identical expressions.
3
Under review as a conference paper at ICLR 2021
the expected top-label equivalents of ACCE and ACE, Expected Class Calibration Error (ECCE)
and Expected Calibration Error (ECE). Here for example ECE by Guo et al. (2017) is expressed as
ECE(θ)
1X
0 j=1 x∈
R P(θ,e)
(P(ωj |x; θ) - P(ωj |x))p(x)dx
dp
Z1
0
O(θ,p)∣Conf (θ,p) — Acc(θ,p)∣dp
(7)
(8)
1
K
where O(θ, p) = Pκ=ι Rx∈RP(θ,e) p(x)dx is the fraction observations that are assigned to that par-
ticular probability and Conf(θ, p) and Acc(θ, p) are the ideal distribution accuracy and confidences
from the model for that probability. For more details see the appendix.
3.2	Sample-based Calibration
Usually only samples from the true joint distribution are available. Any particular training set is
drawn from the distribution to yield
D = {{x(i)，y(i)}}「i，{x(i),y(i)}~ p(x, ω), y⑴ ∈ {ωι, ...,ωκ}.
The region defined in Eq. (3) is now changed to be indices of the samples:
Sp(θ,e) = {Rp(ωj∣x⑴;θ) — p| ≤ e, X⑴ ∈ d},	(9)
The sample-based version of “perfect” calibration in Eq. (2) can then be expressed as:
|Sp(θ,e)∣	X p(ωj|X(i)； θ) = |Sp(θ,e)∣	X	δ(y(i),ωj), ∀p,ωj,e → 0 (IO)
j ,	i∈Sjp (θ,)	j ,	i∈Sjp (θ,)
as N → ∞. When considering finite data, in this case N samples, it is important to set appro-
priately. Setting different yields different regions and leads to different calibration results (Kumar
et al., 2019). Thus it is important to specify when defining calibration for a system.
Similarly, the distribution form of top-label calibration can be written in terms of samples as Eq. (4),
with different regions considered:
Sj(θ,e) = Sj(θ,e) ∩ {也=arg max P(ω∣x⑴;θ), X⑺ ∈ d}	(11)
The sample-based calibration losses in region Sjp(θ, ) can be defined based on Eq. (10). For exam-
ple ACE in Eq. (6) can be expressed in its sample-based form (Hendrycks et al., 2019)
ACE(θ, )
K
& XXX	(P("j |x(i)； θ) - δ(y(i),ωj))
p∈P() j=1 i∈Sjp (θ,)
(12)
where P() = {p|p = min{1, (2z — 1)}, z ∈ Z+}, and Z+ is the set of positive integers. The
measure of ECE relating to Eq. (7), which only considers the top regions in Eq. (11) can be defined
as Guo et al. (2017)
ECE(θ, )
K
N XXX	(P(ωHi); θ) - δ(W>,3j))
P∈P(e) j=1 i∈Sp(θ,e)
Σ
P∈P()
(P3∣sP(θ,e)∣)
N
Conf(θ, p) - Acc(θ,p)
(13)
(14)
PP
It should be noted that for a finite number of samples, the regions SjP(θ, ) and SjP(θ, ) derived from
the samples can be different from the theoretical regions, leading to difference between theoretical
calibration error measures and the values estimated from the finite samples. This is also referred to
4
Under review as a conference paper at ICLR 2021
as “estimator randomness” by Vaicenavicius et al. (2019). An example is given in A.3 to illustrate
this mismatch.
The simplest region specification for calibration is to set E = 1. In this case, ∣Sj(θ, 1)| = N, and
the “minimum” perfect calibration requirement for a system with parameters θ becomes
1N	1N
N ∑P(ωj |x(i)； θ) = N ∑δ(y(i),ωj), ∀ωj	(15)
i=1	i=1
This is also referred to as global calibration in this paper. Similarly, global top-label calibration can
be defined as
1N	1N
方 £p(y()|x()； θ)=a £s(y(),y()), y( ) = arg max P(ω∣x( ); θ)	(16)
NN	ω
i=1	i=1
4 Ensemble Calibration
An interesting question when using ensembles is whether calibrating the ensemble members is suf-
ficient to ensure calibrated predictions. Initially the ensemble model will be viewed as an approxi-
mation to Bayesian parameter estimation. Given training data D , the prediction of class ωj is:
P(ωj |x*, D)
Eθ〜p(θ∣D)[P(ωj |x*； θ)]
/P(ωj∙∣x*; θ)p(θ∣D)dx
1M
≈ P(ωj∣x*; Θ) = MM £p(Sj|x*; θ(m)); e(m)〜p(θ∣D)	(17)
m=1
where Eq. (17) is an ensemble, Monte-Carlo, approximation to the full Bayesian integration, with
θ(m) the m-th ensemble member parameters in the ensemble Θ. The predictions of ensemble and
members are ym = argmaxω{P(ω∣x*; θ(m))},兔=argmaxω {吉 PMM=I P(ω∣x*; θ(m))}.
4.1 Theoretical Analysis
For ensemble methods it is only important that the final ensemble prediction, yE, is well calibrated,
rather than the individual ensemble members. It is useful to examine the relationship between this
ensemble prediction and the predictions from the individual models when the ensemble members are
calibrated. Consider a particular top-label calibration region for the ensemble prediction, Rp(Θ, E),
related to Eq. (4), the following expression is true
J 一
Jχ∈R P(Θ,e)
1M
M X P(yE∣χ; θ(m)
m=1
)p(x)dx ≤
Jχ∈R P(Θ,e)
1M
M E P(ym|x; θ(M))P(X)dχ
m=1
(18)
where the ensemble region is defined as
TRp(Θ, e) = ∣x∣∣P(yE∣x; Θ) 一 p| ≤ e, x ∈ X}. For all
regions Rp(Θ, E) the ensemble is no more confident than the average confidence of individual mem-
ber predictions. This puts bounds on the ensemble prediction performance if the resulting ensemble
prediction is top-label calibrated, and all ensemble members yield the same region Rp(Θ, E). Here
P ~	P(yE∣x; Θ)p(x)dx = / ~	P(yE∣x)p(x)dx
Jχ∈RR p(Θ,e)	Jχ∈RR p(Θ,e)
(19)
From Eq. (18) the left hand-side of this expression, the ensemble prediction confidence, cannot
be greater that than the average ensemble member confidence. If the regions associated with the
ensemble prediction and members are the same, then for top-label calibrated members this average
confidence is the same as the average ensemble member accuracy. Furthermore, if the ensemble
prediction is top-label calibrated, then this average ensemble member accuracy bounds the ensemble
prediction accuracy. Under these conditions ensembling the members yields no performance gains.
The above bound holds with the assumption that the members are calibrated on the same regions.
Proposition 3 in Appendix describes one trivial case when all members are calibrated on the same
regions. Another case is the calibration on global regions. As shown in Proposition 1, at the global
level, ensemble accuracy is still bounded.
5
Under review as a conference paper at ICLR 2021
Proposition 1. If all members and the corresponding ensemble are globally top-label calibrated, the
ensemble performance is no better than the average performance of the members:
N	MN
N Xδ(y(i),yEi))	≤ MM X(N Xδd)	(20)
i=1	m=1	i=1
Proof. If all members and the ensemble are globally top-label calibrated,
1N	1N
N£p(ym)ix⑴;θ(m)) = N∑δ(y(i),ymm)), m=i,...,m	(21)
N i=1	N i=1
NM	N
NX M X p(yEi)∣χ(i);θ(m))	= NXδ(y(i),yEi))	(22)
i=1	m=1	i=1
By definition,
p(yEi)∣χ⑴;θ(m)) ≤ p(y%X⑺;θ(m))	(23)
Hence,
N	MN
N Xδ(y(i),yEi))	≤ M x(N Xδ(y(i),ym)))	(24)
i=1	m=1	i=1
□
However, this is not true for all-label calibration. In both cases, all-label calibrated members always
yield all-label calibrated ensemble, no matter whether the ensemble accuracy exceeds the mean
accuracy of members or not (Example 2 in Appendix gives illustration on a synthetic dataset).
Proposition 2. If all members are global all-label calibrated, then the overall ensemble is global
all-label calibrated.
Proof. If all members are global all-label calibrated, then
1N	1N
N ∑P(ωj |x ⑺;θ(m) = N ∑δ(y(i),ωj), ∀ωj, m =1,...,M	(25)
i=1	i=1
Hence,
1N	1M 1N	1N
NXp(ωj|x(i);⑼=M X (nXδ(y(i),ωj)) = NXδ(y(i,ωj)	(26)
i=1	m=1 i=1	i=1
□
In general the regions are not the same, the ensemble accuracy is not bounded in the above way.
However, note that global level calibration is the minimum requirement of calibration. The above
discussion based on regions still sheds light on the question of should the members be calibrated or
not, though the final theoretical answer is still absent. It should be also noted that, global all-label
calibration does not imply global top-label calibration, because the regions considered are different
(as illustrated by Example 1 in Appendix). For the discussion so far, the ensemble members are
combined with uniform weights, motivated from a Bayesian approximation perspective. When,
for example, multiple different topologies are used as members of the ensemble, a non-uniform
averaging of the members of the ensemble, reflecting the model complexities and performance may
be useful. Propositions 1 and 2 will still apply.
6
Under review as a conference paper at ICLR 2021
4.2	Temperature Annealing for Ensemble Calibration
Calibrating ensembles can be performing using a function f ∈ F with some parameters, t, F :
[0, 1] → [0, 1] for scaling probabilities. There are two modes for calibrating an ensemble:
Pre-combination Mode. the function is applied to the probabilities predicted by members, prior to
combining the members to obtain ensemble prediction using a set of calibration parameters T .
1M
Ppre(yE|x； Θ,T) = MM X f (p(yE∣X; θ(m)), t(m))
m=1
(27)
Post-combination Mode. the function is applied to the ensemble predicted probability after com-
bining members’ predictions.
Ppost (yE∣χ; Θ, t)
f((⅛ mχ2 …卜)
(28)
There are many functions for transforming predicted probability in the calibration literature, e.g.
histogram binning, Platt scaling and temperature annealing. However, histogram binning shouldn’t
be adopted in the pre-combination mode as scaling function f for calibrating multi-class ensemble,
as the transformed values may not yield a valid PMF.
As shown in Guo et al. (2017), temperature scaling is a simple, effective, option for the map-
ping function F, which scales the logit values associated with the posterior by a temperature t,
f(z; t) = exp{z/t}/Pj exp{zj /t}. Here a single temperature is used for scaling logits for all
samples. This leads to the problem that the entropy of the predictions for all regions are either
increased or decreased. From Eq. (2) the temperature can be made region specific.
fdyn (z; t)
exp{z /tr}
if max
i
exp{zi}
Pj exP{zj}
∈ Rr
(29)
To determine the optimal set of temperatures, the samples in the validation set are divided into R
regions based on the ensemble predictions (e.g. R1 = [0, 0.3), R2 = [0.3, 0.6), and R3 = [0.6, 1]).
Each region has an individual temperature for scaling {Rr, tr}rR=1.
4.3	Empirical Results
Experiments were conducted on CIFAR-100 (and CIFAR-10 in the A.4). The data partition was
45,000/5,000/10,000 images for train/validation/test. We train LeNet (LEN) (LeCun et al., 1998),
DenseNet 100 and 121 (DSN100, DSN121), (Huang et al., 2017) and Wide ResNet 28 (RSN)
(Zagoruyko & Komodakis, 2016) following the original training recipes in each paper (more de-
tails in A.4). The results presented are slightly lower than that in the original papers, as 5,000
images were held-out to enable calibration parameter optimisation.
Figure 1 examines the empirical performance of ensemble calibration on CIFAR-100 test set using
the three trained networks. The top row shows that, with appropriate temperature scaling, the mem-
bers are calibrated on different regions (because otherwise the accuracy values should be the same).
The middle row shows the ECE of ensemble members and ensemble prediction at different tem-
peratures. The optimal calibration temperature for the ensemble prediction are consistently smaller
than those associated with the ensemble members. This indicates that the ensemble predictions are
less confident than those of the members, as stated in Eq. (23). The bottom row of figures show the
reliability curves when the ensemble members are calibrated with optimal temperature values, and
the resulting combination. It is clear that calibrating the ensemble members, using temperature, does
not yield a calibrated ensemble prediction. Furthermore for all models the ensemble prediction is
less confident than it should be, the line is above the diagonal. As discussed in Proposition 1, this is
necessary, or the ensemble prediction is no better, which is clearly not the case for the performance
plots in the top row. This ensemble performance is relatively robust to poorly calibrated ensemble
members, with consistent performance over a wide range of temperatures.
Table 1 shows the calibration performance using three temperature scaling methods, pre-, post-
and dynamic post-combination. The temperatures are optimized to minimize ECE (Liang et al.,
7
Under review as a conference paper at ICLR 2021
(a) LEN
(b) DSN100
(c) RSN
Figure 1: Top-label calibration error and accuracy of members (mem) and the whole ensemble (ens)
on CIFAR-100 (test set) using LeNet, DenseNet and ResNet. “pre” denotes the calibration where
shared temperature is applied to members before combination. The reliability curves shows the
calibrated members and calibrated ensembles with optimal temperature values.
2020) on the validation data. We use the unbiased quadratic version of squared kernel calibration
error (SKCE) with Laplacian kernel and kenel bandwidth chosen by median heuristic as one of
the calibration error metrics(Widmann et al., 2019) . All three methods effectively improve the
ensemble prediction calibration, with the dynamic approach yielding the best performance. We
further investigate the impact of region numbers on the dynamic approach, as shown in Figure 3. It
can be found that increasing the region number tends to improve the calibration performance, while
requiring more parameters.
Finally, for the topology ensemble, weights were optimised using either maximum likelihood (Max
LL) or area under curve (AUC) Zhong & Kwok (2013) (results in A.4). In Figure 2, the ensemble
of calibrated structures is shown to be uncalibrated, with reliability curves typically slightly above
the diagonal line. When the ensemble prediction is calibrated it can be seen that the calibration for
the ensemble prediction is lower than the individual calibration errors in Table 1 (“post” lines).
LEN+DSN100+DSN121+RSN
DSN100+DSN121+RSN
DSN121+RSN
Figure 2: Reliability curves of weighted combination of 4 calibrated structures, LEN, DSN100,
DSN121 and RSN on CIFAR-100. The weightes are estimated by Max LL. Each structure is an
ensemble of 10 models.
8
Under review as a conference paper at ICLR 2021
Table 1: Temperature calibration techniques on CIFAR-100, calibration parameters optimized to
minimize ECE on validation set. In the “pre” mode, each member is scaled with one separate
temperature. “dyn.” denotes dynamic temperature scaling in post-combination mode using 6 region-
based temperatures. Ranges indicate ±2σ.
Model	Cal.	Acc.(%)	NLL	ACCE(10-4)	ACE(10-4)	ECCE(10-2)	ECE(10-2)	SKCE (10-4)
	—	-492δ^^	1.9741±0.0059	^^30.82±0.44~	23.66±0.55	16.23±0.39	11.55±0.39	23.97±0.06
LEN	pre	49.17	1.9641±0.0137	23.15±0.86	8.54±1.85	13.23±0.19	3.24±0.37	27.24±0.42
	post	49.20	1.9285±0.0068	21.72±0.61	5.73±1.24	13.22±0.23	2.19±0.45	28.41±0.44
	dyn.	49.20	1.9280±0.0107	21.19±0.73	4.45±1.66	12.86±0.28	2.33±1.05	28.81±0.30
	—	-8132^^	0.6699±0.0015	~~16.31±0.42~	5.79±0.38	8.92±0.39	2.54±0.19	53.71±0.14
DSN 100	pre	81.29	0.6912±0.0084	16.89±0.36	6.86±0.59	8.79±0.30	2.08±0.38	55.32±0.47
	post	81.32	0.6852±0.0080	16.73±0.38	6.29±0.68	8.56±0.25	1.83±0.32	57.64±1.25
	dyn.	81.32	0.6781±0.0058	16.11±0.60	4.94±1.03	8.41±0.35	1.31±0.53	57.17±0.69
	—	-8269^^	0.6314±0.0022	~~15.74±0.24~	3.64±0.35	8.58±0.18	1.58±0.24	59.30±0.07
DSN 121	pre	82.70	0.6312±0.0056	15.79±0.38	3.58±0.80	8.58±0.18	1.62±0.20	59.21±0.82
	post	82.69	0.6324±0.0044	15.81±0.43	3.76±0.65	8.57±0.17	1.56±0.23	59.61±1.30
	dyn.	82.69	0.6315±0.0041	15.63±0.43	3.26±0.34	8.65±0.29	1.71±0.18	57.87±0.53
	—	-8345^^	0.6231±0.0023	~~16.95±0.20~	7.31±0.26	9.28±0.26	3.22±0.19	57.01±0.14
RSN	pre	83.41	0.6129±0.0018	15.41±0.43	2.52±0.63	8.75±0.17	1.88±0.26	60.67±0.70
	post	83.45	0.6118±0.0016	15.48±0.28	3.28±0.52	8.75±0.15	1.82±0.20	60.75±0.56
	dyn.	83.45	0.6097±0.0023	15.63±0.31	2.83±0.56	8.68±0.31	1.20±0.41	59.36±0.74
Figure 3: Impact of different region numbers on dynamic temperature annealing in calibration of
ensembles of DSN100, DSN121 and RSN.
Table 2: Topology ensembles for CIFAR-100, optimal weights using ML estimation. Calibrations
of each topology and ensemble using post-combination mode (“post” in Table 1).
Weight Est.	LEN	Comb. Weight		RSN	Acc. (%)	Ens Cal.	NLL	ACE (10-4)	ECE (10-2)
		DSN100	DSN121						
	0.02	0.19	0.30	0.49	83.75	— X	0.5766	4.97	2.24
							0.5698	1.42	1.20
		0.22	0.30	0.48	83.80	—	0.5741	3.74	2.00
Max LL	—					X	0.5714	1.52	1.29
	—	—	0.44	0.56	83.86	—	0.5816	3.64	2.06
						X	0.5801	2.36	1.35
5 Conclusions
State-of-the-art deep learning models often exhibit poor calibration performance. In this paper two
aspects of calibration for these models are investigated: the theoretical definition of calibration and
associated attributes for both general and top-label calibration; and the application of calibration to
ensemble methods that are often used in deep-learning approaches for improved performance and
uncertainty estimation. It is shown that calibrating members of the ensemble is not sufficient to
ensure that the ensemble prediction is itself calibrated. The resulting ensemble predictions will be
under-confident, requiring calibration functions to be optimised for the ensemble prediction, rather
than ensemble members. These theoretical results are backed-up by empirical analysis on CIFAR-
100 deep-learning models, with ensemble performance being robust to poorly calibrated ensemble
members but requiring calibration even with well calibrated members.
9
Under review as a conference paper at ICLR 2021
References
Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain
uncertainty estimation and ensembling in deep learning. arXiv preprint arXiv:2002.06470, 2020.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.
Jochen Brocker. Estimating reliability and resolution of probability forecasts through decomposition
of the empirical score. Climate dynamics, 39(3-4):655-667, 2012.
Tilmann Gneiting, Fadoua Balabdaoui, and Adrian E Raftery. Probabilistic forecasts, calibration
and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):
243-268, 2007.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. ICML, 2017.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. ICLR, 2019.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. ICLR, 2019.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Calibrating predictive model
estimates to support personalized medicine. Journal of the American Medical Informatics Asso-
ciation, 19(2):263-274, 2012.
Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Advances in Neural
Information Processing Systems, pp. 3474-3482, 2015.
Meelis Kull, Telmo Silva Filho, and Peter Flach. Beta calibration: a well-founded and easily imple-
mented improvement on logistic calibration for binary classifiers. In Artificial Intelligence and
Statistics, pp. 623-631, 2017.
Meelis Kull, Miquel Perello Nieto, Markus Kangsepp, Telmo Silva Filho, Hao Song, and Peter
Flach. Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with
dirichlet calibration. In Advances in Neural Information Processing Systems, pp. 12316-12326,
2019.
Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in
Neural Information Processing Systems, pp. 3792-3803, 2019.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning, pp. 2805-
2814, 2018.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems, pp. 6402-6413, 2017.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Gongbo Liang, Yu Zhang, and Nathan Jacobs. Neural network calibration for medical imaging
classification using dca regularization. In ICML UDL, 2020.
10
Under review as a conference paper at ICLR 2021
Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In Proceedings of the... AAAI Conference on Artificial Intel-
ligence. AAAI Conference on Artificial Intelligence, volume 2015, pp. 2901. NIH Public Access,
2015.
Khanh Nguyen and Brendan O’Connor. Posterior calibration and exploratory analysis for natural
language processing models. EMNLP, 2015.
Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-
ing. In Proceedings of the 22nd international conference on Machine learning, pp. 625-632,
2005.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measur-
ing calibration in deep learning. In CVPR Workshops, pp. 38-41, 2019.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in large margin classifiers, 10(3):61-74, 1999.
Adrian E Raftery, Tilmann Gneiting, Fadoua Balabdaoui, and Michael Polakowski. Using bayesian
model averaging to calibrate forecast ensembles. Monthly weather review, 133(5):1155-1174,
2005.
Rahul Rahaman and Alexandre H Thiery. Uncertainty quantification and deep ensembles. arXiv
preprint arXiv:2007.08792, 2020.
Asa Cooper Stickland and Iain Murray. Diverse ensembles improve calibration. ICML 2020 work-
shop on Uncertainty Robustness in Deep Learning, 2020.
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial training: Gener-
alizing to unseen attacks. ICML 2020 workshop on Uncertainty Robustness in Deep Learning,
2020.
Gia-Lac Tran, Edwin V Bonilla, John Cunningham, Pietro Michiardi, and Maurizio Filippone. Cali-
brating deep convolutional gaussian processes. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 1554-1563. PMLR, 2019.
Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, and
Thomas B Schon. Evaluating model calibration in classification. Proceedings OfMaChine Learn-
ing Research, 2019.
Yeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek, Balaji Laksh-
minarayanan, and Dustin Tran. Improving calibration of batchensemble with data augmentation.
ICML 2020 workshop on Uncertainty Robustness in Deep Learning, 2020.
David Widmann, Fredrik Lindsten, and Dave Zachariah. Calibration tests in multi-class classi-
fication: A unifying framework. In Advances in Neural Information Processing Systems, pp.
12257-12267, 2019.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In Icml, volume 1, pp. 609-616. Citeseer, 2001.
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pp. 694-699, 2002.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. ICLR, 2018.
Wenliang Zhong and James T Kwok. Accurate probability calibration for multiple classifiers. In
Twenty-Third International Joint Conference on Artificial Intelligence. Citeseer, 2013.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Theoretical Proof
Proposition 3. If all members are calibrated and the regions are the same, i.e., for different members
θ(m) and θ(m0)
Rjp(θ(m), ) =Rjp(θ(m0),) ∀p,ωj,	→0
then the ensemble is also calibrated on the same regions
P(ωj |x; Θ)p(x)dx =	P(ωj |x)p(x)dx,
x∈Rjp (Θ,)	x∈Rjp (Θ,)
∀p, ωj ,	→ 0
Proof. If
Rjp(θ(m), ) =Rjp(θ(m0),) ∀p,ωj,	→0
The ensemble is also calibrated and the regions are the same:
Rjp(Θ,) = x
1M
M fP(3j |x； θ(m)) - P ≤ e
m=1
= Rjp(θ(m)) ∀p,ωj,	→0	(30)
/
x∈Rjp(Θ,)
1M
M EpQj|x; θ(m))p(χ)dx
m=1
-1 X [
M m=1 x∈Rjp(θ(m) ,)
p(ωj |x; θ(m))p(x)dx
1M
p(jiχ I	p(ωj Ix)P(X)dx
M m=1 x∈Rjp(θ(m) ,)
p(ωj |x)P(x)dx
x∈Rjp(θ(m) ,)
p(ωj |x)P(x)dx
x∈Rjp (Θ,)
□
Proposition 4. When class number K > 2, if all members are globally top-label calibrated, then
the ensemble is not necessarily global top-label calibrated.
Proof. Assume globally top-label calibrated members imply globally top-label calibrated ensemble,
that is, given
1N	1N
NN £p(ym)|x(i)； θ(m)) = NN Eδ(y⑺,ym)m =1,...,M	(31)
N i=1	N i=1
the following is true
NN
NN X P(y(i)Ix(i); Θ)	= NN X δ(y⑴,yEi))	(32)
N i=1	N i=1
If ∃n, m ,τ > 0, such that y((n) =0野)，then it is possible to write
p(^(n)Ix(n);θ) =(M X P(yEn)Ix(n)；θ(m))) + MMP(yEn)Ix(n);θ(m))	(33)
∖	m=m	)
For top-label calibration there are no constraints on the second term in Eq. (33) as it is not the top-
label for model θ(7m). Thus there are a set of models that satisfy the top-label calibration constraints
for member m that only need to satisfy the following constraints
0 ≤ P(y(n)∣x(n); θ(m)) < P(ymn)∣x(n); θ(m)) ≤ 1	(34)
12
Under review as a conference paper at ICLR 2021
and the standard sum-to-one constraint over all classes. Consider replacing member m of the en-
semble with a member having parameters θ(m), to yield Θ, that satisfies
max ∣P(ω∣x(n); θ(Jm))} = max ∣P(ω∣x(n); θ((m)}= ymn	(35)
p(y(n)∣χ(n); θ((m)) = p(ymn)∣χ(n); θ((m))	(36)
P(yEn)∣x⑺;θ(Jm)) = P(yEn)∣x(R θ(Jm))+ T	(37)
where τ > 0, and the standard sum-to-one constraint is satisfied, and all other predictions are
unaltered. This results in the following constraints
max {p(ω∣x(n); Θ)} = max {p(ω∣χ(n); Θ)} = yEn)
p(yEn)|x⑺;θ) < p(yEn)|x(吟 Θ)
(38)
(39)
The accuracy of the two ensembles Θ and Θ are the same from Eq. (38), but the probabilities
associated with those predictions cannot be the same from Eq. (39), so both ensemble predictions
cannot be calibrated, as assuming that the ensemble prediction for Θ is calibrated
NNN
N Xp(y(i)|x(i); Θ) > N Xp(y(i)|x⑺；Θ) = N Xδ(y⑴,yEi))	(40)
i=1	i=1	i=1
Hence there are multiple values of p(y((n)∣x(n); Θ) for which all the models satisfy the top-
calibration constraints, but these cannot all be consistent with Eq. (40). For the situation where
there is no sample or model where yEn) = ymn) then the predictions for all models for all samples
are the same as the ensemble prediction, so by definition there can be no performance gain.
□
A.2 Global General Calibration and Top-label Calibration
To demonstrate the differences between global top-label calibration and global calibration, a set of
ensemble member predictions were generated using Algorithm 1, this ensures that the predictions are
perfectly calibrated. Since the member predictions are perfectly calibrated, the ensemble members
will be globally calibrated. Figure 4 (a) shows the performance in terms of ACE of the ensemble
prediction as the value of increases, note when = 1 this is a global calibration version of ACE.
It can be seen that as increases ACE decreases, and for the global case reduces to zero for the
ensemble predictions as the theory states.
In terms of top-label calibration, as the ensemble members are perfectly calibrated, they will again
be global top-label calibrated. This is illustrated in Figure 4 (b) where ECE is zero for all ensemble
members. For top-label calibration the value of ECE does not decrease to zero as the → 1, again
as the theory states. This is because the underlying probability regions associated with each of the
members of the ensemble are different. Hence, even for perfectly calibrated ensemble members, the
ensemble prediction is not global top-label calibrated.
A.3 Toy Datasets
Example 1.	In this example, we show the difference between all-label calibration and top-label
calibration which consider the different regions in Eq. (3) and Eq. (4).
Assuming P(X) 8 1, the whole input space X is consisted of three regions Ri, R? and R3, and
p(x)dx =	p(x)dx =	p(x)dx.
x∈R1	x∈R2	x∈R3
(41)
13
Under review as a conference paper at ICLR 2021
Figure 4: ACE (calibration) and ECE (top-label calibration) of a perfectly calibrated set of ensemble
members, as the value of varies.
The corresponding system prediction P and the true distribution P is:
ω1
0.5
P = (p(ωj∣x ∈ Rr; θ)	0.3
0.3
ω2	ω3	ω4	
0.4	0.05	0.05	R1
0.4	0.2	0.1	R2
0.3	0.35	0.05	R3
(42)
ω1
0.5
P = 0.3 - τ
0.3+τ
ω2	ω3	ω4	
0.4 — T	0.05	0.05 + T	R1
0.4 + T	0.2	0.1	R2
0.3	0.35	0.05-T	R3
τ>0
(43)
It can be verified that
p(ωj ∣x, θ)p(x)dx
x∈Rjp (θ,)
However, when p = 0.4, j = 2
/
x∈Rjp(θ,)
p(ωj ∣x)p(x)dx, ∀ωj , p, → 0
(44)
p(ωj ∣x, θ)p(x)dx	6=	p(ωj ∣x, θ)p(x)dx,	→ 0
x x∈RRp(θ,e)	x∈RRp(θ,e)
(45)
Example 2.	This example shows that combination of calibrated members yields uncalibrated en-
semble. Algorithm 1 generates the true data distribution p by sampling from a Dirichlet distribution
with equal concentration parameters of 1. To generate the member predictions, the N samples are
randomly assigned to N/b bins with size of b. In each bin, the member predictions p^ are equally the
average of true data distribution of the associated samples. This ensures that for each region, Eq. (2)
holds for each member. However, regions for different member are different due to the random
assignment. Therefore, the corresponding ensemble is not automatically calibrated. Figure 4 shows
that the ensemble is uncalibrated with ACE of 0.0697.
Example 3.	In this example, we show that for a finite number of samples, the regions Sjp(θ, ) and
Sp(θ, e) derived from the samples is different from the theoretical regions, leading to difference
between theoretical calibration error measures and the values estimated from the finite samples.
Algorithm 2 generates data with difference between finite sample-based calibration error and theo-
retical error. The theoretical ACE of the predicted probabilities in Algorithm 2 is 费.However, the
finite sample-based ACE is 0.
The true data distribution is P(ω,∣x) = 4, P(X) a 1. The samples in D are assigned to bins of
size 2. The type of bin that a sample is assigned to determines the predicted probability of sample.
Considering each class ωj , there are three types of bins:
Bp=1: both samples belong to class ωj .
Bp=0.5: only one sample is class ωj .
14
Under review as a conference paper at ICLR 2021
Algorithm 1: Algorithm for generating calibrated members that yield uncalibrated ensemble
Result： {p(i)}N=ι,{P(i),y(i)}N=ι
b = 2; //bin size, number of samples in one bin;
K = 4; //number of classes;
M = 10; //number of members;
N = 1000000; //number of samples;
p(i) 〜Dirichlet(α = 1),i = 1,…，N; //true data distribution, sampled from Dirichlet
distribution with equal concentration parameters of 1;
I = [1, 2, ..., N]; // index vector;
for m in [1, ..., M] do
~ _
I J ShUffle(I);
for j in [1, ..., dN/be] do
Bj = (b * (j - 1), min{b * j, N}];
PBj = b Pl∈Bj P(Il);
end
for i in [1, ..., N] do
j = dbe;
P(Ii,m) = PBj ;
end
end
Bp=0 : both samples are not class ωj .
P(X ∈ BI) = T7, P(X ∈ B0.5) = T7, P(X ∈ BO) = T7
16	16	16
then
/产(eo)(P("j |x； θ)-p(ωj lx))p(x)dx = / 产(eo)(P - 1))p(x)dx
=(P - I)P(X ∈ Bp)
therefore,
ACE(θ)
(P(ωj∣x; θ) — P(ωj∣x))p(x)dxldp
X	I(P-I)P(X ∈ Bp)I = 392
p∈{0,0.5,1}
(46)
(47)
(48)
(49)
(50)
A.4 Additional Experimental Results
In this section, we show some comparison experiments to the empirical results in Section 4.3. We
conducted experiments on CIFAR-100 and CIFAR-10 dataset. Table 3 and4 display the performance
of inidividual models, LeNet, DenseNet 100, DenseNet 121, and wide ResNet 28-10. All systems
are trained with data augmentation of random cropping and horizontal flipping, and simple mean/std
normalization. The original training/test images in CIFAR datasets is 50,000/10,000. We hold out
5,000 images from training set as validation set (10%) for temperature and combination weights
optimization, this leads to a slight accuracy degradation compared to training with all 50,000 images.
We have 10 runs of our experiment to obtain the deviations.
In Section 4.3, we presented ensemble calibration on CIFAR-100. The counterpart on CIFAR-10 is
given in Table 5. Other than separately specified, all sample based evaluation criteria in this paper
use 15 bins following previous literature (Guo et al., 2017). The temperatures in pre-, post- and dy-
namic post-combination modes are optimized on the validation set by minimizing ECE (Liang et al.,
15
Under review as a conference paper at ICLR 2021
Algorithm 2: Algorithm for generating data with difference between finite sample-based ACE
and theoretical ACE.
Result: {p^(i),y(i)}N=ι
b = 2; //bin size, number of samples in one bin;
K = 4; //number of classes;
g = [g1, ..., gN ]; //vector of ground-truth labels , where g ∈ {1, ..., K}. Numbers of labels for
different classes are equal;
I = [1, 2, ..., N]; // index vector;
~ _
I J ShUfle(I);
for j in [1, ..., dN/be] do
Bj = (b * (j - 1), min{b * j, N}];
PBj= b Pι∈Bj [δ(1, gʃi ),δ(2, g^),..., δ(K, of,)];
end
for i in [1, ..., N] do
j = dbe;
P(Ii) = pBj;
end
2020), Using SGD with learning rate of 0.1 for 400 iterations. It can be observed that combination of
DenseNet and ResNet improves the calibration performance, while combination of LeNet doesn’t
help. This is becaUse LeNet is not as over-confident as DenseNet and ResNet (as shown in FigUre
1). Hence the simple ensemble combination doesn’t help, bUt aggravates the calibration. The three
temperatUre-based calibration methods effectively improve the system calibration performance on
CIFAR-10 as well.
Table 6 gives the ensemble combination based on AUC weights (Zhong & Kwok, 2013). The AUC
weights are mUch even than the Max LL weights in Table 2. The strUctUres combined are first
calibrated, nevertheless, applying post-combination calibration to the ensemble obtains gains. We
evalUated post-combination for topology combination in Table 7. The post- and dynamic post-
combination methods are applied to calibrate the topologies and the topology ensemble. The
dynamic temperatUre method shows clear advantage in obtaining calibrated ensemble of mUtiple
topologies.
Table 3: IndividUal model performance on CIFAR-100. FUll data training (100%Train) and training
with 5,000 images held oUt (90%Train) are presented. * denotes resUlts from the original papers.
Model	%Train	Acc.(%) (%)	NLL	ACCE (10-4)	ACE (10-4)	ECCE (10-2)	ECE (10-2)
LEN	100	42.47	2.2730	21.07	7.62	13.05	1.82
	90	42.12	2.2652	22.80	10.11	13.20	2.86
	-^W^~	76.21	-	-	-	-	-
DSN 100	100	76.71	1.1022	30.54	25.85	14.53	12.65
	90	76.00	1.0591	30.22	24.55	13.91	11.72
DSN 121	100	80.00	0.8438	23.39	17.29	11.52	8.59
	90	78.87	0.8981	24.83	19.00	11.99	9.41
	-^W^~	80.75	-	-	-	-	-
RSN	100	80.99	0.7711	18.67	8.68	10.25	5.02
	90	80.06	0.7985	18.72	8.83	10.23	4.76
16
Under review as a conference paper at ICLR 2021
Table 4: Individual model performance on CIFAR-10. Full data training (100%Train) and training
with 5,000 images held out (90%Train) are presented. * denotes results from the original papers.
Model	%Train	Acc.(%) (%)	NLL	ACCE (10-4)	ACE (10-4)	ECCE (10-2)	ECE (10-2)
LEN	100	75.07	0.7329	60.85	23.97	3.55	1.24
	90	75.31	0.7280	77.17	44.49	4.16	2.09
	-^W^~	94.23	-	-	-	-	-
DSN 100	100	95.39	0.2003	61.04	54.74	3.04	2.77
	90	94.92	0.2181	65.50	60.54	3.27	2.99
DSN 121	100	95.66	0.1832	61.56	56.63	3.01	2.87
	90	95.53	0.1993	62.79	57.16	3.06	2.89
	-^W^~	96.00	-	-	-	-	-
RSN 28	100	95.98	0.1524	51.67	46.70	2.61	2.35
	90	95.52	0.1692	58.07	52.70	2.89	2.62
Table 5: Calibration using temperature annealing on CIFAR-10. The temperatures are optimized
to minimize ECE on validation set. In the ‘pre’ mode, each member is scaled with one separate
temperature. ‘dyn.’ denotes dynamic temperature scaling in post-combination mode using 6 region-
based temperatures. The three structures investigated are LeNet 5, DenseNet 121, DenseNet 100
and Wide ResNet 28. Ranges indicate ±2σ.
Model	Cal.	Acc.(%)	NLL	ACCE(10-4)	ACE(10-4)	ECCE(10-2)	ECE(10-2)	SKCE(10-4)
	—	-80.11—	0.6067±0.0031	-187.73±3.22-	181.18±1.86	9.55±0.26	9.18±0.17	447.50±1.78
	Pre	80.04	0.5915±0.0062	76.68±6.11	53.50±5.10	4.01±0.24	1.91±0.34	546.65±5.03
LEN	post	80.11	0.5664±0.0033	57.84±3.90	24.00±4.62	3.66±0.21	1.15±0.31	568.86±6.55
	dyn.	80.11	0.5718±0.0071	57.79±5∙58	25.45±8.26	3.74±0.36	1.29±0.52	568.94±7.07
	—	-96.19—	0.1230±0.0013	-28.01 ±2.23-	13.30±1.95	1.47±0.10	0.55±0.13	871.27±0.72
F⅛ C XT ι ∩∩	pre	96.19	0.1227±0.0013	27.92±2.56	13.00±2.17	1.46±0.10	0.54±0.12	870.62±0.91
DSN 100	post	96.19	0.1228±0.0015	28.27±2.94	13.27±1.80	1.47±0.13	0.55±0.13	870.24±3.31
	dyn.	96.19	0.1224±0.0015	27.15±2∙58	11.16±1.93	1.43±0.13	0.45±0.11	867.59±3.52
	—	-96^-	0.1202±0.0010	-27.98±1.17-	15.31±1.46	1.43±0.12	0.69±0.14	895.02±0.59
F⅛ C NT ICl	pre	96.35	0.1198±0.0012	27.85±1.33	15.08±1.46	1.42±0.12	0.67±0.13	894.55±0.98
DSN 121	post	96.35	0.1194±0.0027	27.37±2.67	14.54±2.73	1.41±0.14	0.64±0.21	892.48±8.64
	dyn.	96.35	0.1191±0.0012	27.66±1.56	14.60±2.42	1.42±0.15	0.66±0.14	894.06±3.28
	—	-96.51-	0.1081±0.0008	-25.14±1.34-	11.73±1.39	1.37±0.10	0.58±0.10	894.64±1.07
	pre	96.51	0.1073±0.0009	24.54±2.04	10.01±1.12	1.32±0.10	0.50±0.11	891.23±1.80
RSN	post	96.51	0.1069±0.0007	23.96±1.35	8.83±0.96	1.32±0.15	0.50±0.09	881.57±4.60
	dyn.	96.51	0.1070±0.0007	23.84±1.84	8.57±1.54	1.30±0.15	0.47±0.13	883.58±5.70
Table 6: Topology ensembles for CIFAR-100, optimal weights based on AUC. Calibrations of each
topology and ensemble using post-combination mode (”post” in Table 1).
Weight Est.	LEN	Comb. DSN100	Weight		Acc. (%)	Ens Cal.	NLL	ACE (10-4)	ECE (10-2)
			DSN121	RSN					
	0.20	0.26	0.27	0.27	83.30	— X	0.6649	20.84	10.07
							0.6096	2.60	2.00
					83.55	—	0.5776	3.67	1.89
AUC	—	0.33	0.33	0.34		X			
							0.5787	3.03	1.38
			0.49	0.51	83.78	—	0.5820	3.54	2.03
	—	—				X			
							0.5805	2.30	1.34
17
Under review as a conference paper at ICLR 2021
Table 7: Topology combination on CIFAR-100. Dynamic mode uses 6 region-based temperatures.
Comb. Weight				Struct CaL	Ens CaL	Acc.(%)	NLL	ACE	ECE
LEN	DSN100	DSN121	RSN						
-	0.33	0.33	0.33	-	-	83.54	0.5846	0.0007	0.0334
-	0.33	0.33	0.33	-	post	83.54	0.5769	0.0002	0.0132
-	0.33	0.33	0.33	-	dyn.	83.54	0.5751	0.0002	0.0099
-	0.33	0.33	0.33	post	-	83.57	0.5779	0.0004	0.0189
-	0.33	0.33	0.33	post	post	83.57	0.5778	0.0002	0.0118
-	0.33	0.33	0.33	dyn.	-	83.59	0.5771	0.0004	0.0211
-	0.33	0.33	0.33	dyn.	dyn.	83.59	0.5730	0.0002	0.0112
-	0.22	0.39	0.39	-	-	83.78	0.5822	0.0007	0.0327
-	0.22	0.39	0.39	-	post	83.78	0.5724	0.0002	0.0135
-	0.22	0.39	0.39	-	dyn.	83.78	0.5705	0.0002	0.0113
-	0.22	0.30	0.48	post	-	83.80	0.5741	0.0004	0.0200
-	0.22	0.30	0.48	post	post	83.80	0.5714	0.0002	0.0129
-	0.23	0.31	0.46	dyn.	-	83.75	0.5741	0.0004	0.0211
-	0.23	0.31	0.46	dyn.	dyn.	83.75	0.5700	0.0002	0.0153
0.25	0.25	0.25	0.25	-	-	83.34	0.7238	0.0032	0.1551
0.25	0.25	0.25	0.25	-	post	83.34	0.6043	0.0003	0.0196
0.25	0.25	0.25	0.25	-	dyn.	83.34	0.6027	0.0002	0.0122
0.25	0.25	0.25	0.25	post	-	83.25	0.6947	0.0025	0.1224
0.25	0.25	0.25	0.25	post	post	83.25	0.6262	0.0004	0.0218
0.25	0.25	0.25	0.25	dyn.	-	83.22	0.6978	0.0026	0.1258
0.25	0.25	0.25	0.25	dyn.	dyn.	83.22	0.6184	0.0002	0.0137
0.01	0.19	0.42	0.38	-	-	83.74	0.5859	0.0008	0.0374
0.01	0.19	0.42	0.38	-	post	83.74	0.5710	0.0001	0.0114
0.01	0.19	0.42	0.38	-	dyn.	83.74	0.5703	0.0001	0.0085
0.02	0.19	0.30	0.49	post	-	83.75	0.5766	0.0005	0.0224
0.02	0.19	0.30	0.49	post	post	83.75	0.5698	0.0001	0.0120
0.02	0.21	0.30	0.47	dyn.	-	83.77	0.5784	0.0006	0.0282
0.02	0.21	0.30	0.47	dyn.	dyn.	83.77	0.5698	0.0002	0.0167
18