Under review as a conference paper at ICLR 2021
Hellinger Distance Constrained Regression
Anonymous authors
Paper under double-blind review
Ab stract
This paper introduces an off-policy reinforcement learning method that uses
Hellinger distance between sampling policy (from what samples were collected)
and current policy (policy being optimized) as a constraint. Hellinger distance
squared multiplied by two is greater than or equal to total variation distance
squared and less than or equal to Kullback-Leibler divergence, therefore a lower
bound for expected discounted return for the new policy is improved compared
to the lower bound for training with KL. Also, Hellinger distance is less than or
equal to 1, so there is a policy-independent lower bound for expected discounted
return. HDCR is capable of training with Experience Replay, a common set-
ting for distributed RL when collecting trajectories using different policies and
learning from this data centralized. HDCR shows results comparable to or better
than Advantage-weighted Behavior Model and Advantage-Weighted Regression
on MuJoCo tasks using tiny offline datasets collected by random agents. On big-
ger datasets (100k timesteps) obtained by pretrained behavioral policy, HDCR
outperforms ABM and AWR methods on 3 out of 4 tasks.
1	Introduction
Policy gradient algorithms are methods of model-free reinforcement learning that optimize policy
through differentiating expected discounted return. Despite the simplicity, to converge, these meth-
ods should stay on-policy because of the first-order approximation of state visitation frequencies.
This issue makes agents learn through trial-and-error, using data only once.
To make policy gradient updates more off-policy, we can add a constraint on the update to decrease
the step size if the current policy is too far from the sampling policy. One of the first methods was
to add total variation distance squared as a constraint for mixture policies. Later it was proven that
there is a lower bound for new policy’s expected discounted return (Kakade & Langford, 2002).
Recently it was proven that this lower bound exists for all types of updates (Schulman et al., 2015).
Next, total variation distance squared was replaced by Kullback-Leibler divergence that is greater
than or equal to the previous one (Pinker’s inequality (Levin & Peres, 2017)), so that the lower bound
was decreased (Schulman et al., 2015). Using Lagrangian, have been derived off-policy method
called Advantage-Weighted Regression (Peng et al., 2019), which also used KL as a constraint.
This article proposes anew method whose lower bound of expected discounted return is greater than
or equal to the bound with KL. We achieve this by replacing total variation distance by Hellinger
distance, which decreases lower bound. Therefore strictness stays the same. Then we derive an off-
policy method called Hellinger Distance Constrained Regression using the new constraint. It can be
used on discrete and continuous action spaces since derivation uses Lebesgue integrals rather than a
summation or Riemann integrals.
2	Preliminaries
To better present the problem, we start from basic definitions, go through the history of improve-
ments, and then describe the disadvantages of using KL divergence as a constraint.
We consider an infinite-horizon discounted Markov decision process (MDP), defined by the tu-
ple (S, A, P, r, ρ0 , γ), where S is a set of states (finite or infinite), A is a set of actions (finite
or infinite), P : S × A × S → R is the transition probability distribution, r : S → R is
1
Under review as a conference paper at ICLR 2021
the reward function, ρ0 : S → R is the distribution of the initial state s0, and γ ∈ (0, 1) is the
discount factor.
Let π denote a stochastic policy π : S × A → [0, 1], and then its expected discounted return is:
∞
η(π) = Es0,a0,...	γtr(st) , where
t=0
S0 〜Po(∙), at 〜∏(∙∣St), St+1 〜P(∙∣St,at).
(1)
This paper uses state-action value function Qπ, state value function Vπ, and advantage function Aπ
with the following definitions:
∞
Qπ(st, at)
= Est+1,at+1,...	γlr(st+l)
l=0
∞
Vπ(st)
= Eat,st+1,at+1,...	γlr(st+l)
l=0
Aπ(st, at) = Qπ(st, at) - Vπ (st).
(2)
Let ρπ (s) be unnormalized visitation frequencies of state s where actions are chosen according to π:
∞
ρπ(s) = X γtP(st = s).	(3)
t=0
Following identity expresses the expected return of another policy ∏ in terms of the advantage over ∏,
accumulated over states (see Schulman et al. (2015) for proof):
η(π)
η(π) + / ρ∏ (S) /
Π(a∣s)A∏(s, a) da ds.
(4)
In approximately optimal learning, We replace state visitation frequency ρ∏ by ρ∏, since this drasti-
cally decrease optimization complexity:
L∏(∏)
η(π) +	ρπ(s)
Π(a∣s)A∏(s, a) da ds.
(5)
Let πold denote current policy, then the lower bound for the expected discounted return for the new
policy πnew will be (see Schulman et al. (2015) for proof):
η(πnew) ≥ Lnold (πnew ) - (] - Yy a2
where
α
max ∣A∏(s, a)|,
s,a
max DTV (∏oid(∙∣s)∣∣πnew(∙∣s)),
s
(6)
DTV(∏oid(∙∣s)∣∣∏new(∙∣s)) = 2 / ∣∏oid(a∣s)- ∏new(a|s)| da.
Theoretical Trust-Region Policy Optimization algorithm relays on Pinsker’s inequality (see (Tsy-
bakov, 2009) for proof):
DKL (nOld(Is)Hπnew(∙∣s)) ≥ DTV(∏oid(∙∣s)∣∣πnew(,|S))2
where DKL (∏oid(∙∣s)∣∣πnew(∙∣s))
/ ∏oid(a∣s)log
∏oid(a∣s)
πnew (a|s)
da.
(7)
To retain strictness and decrease calculation complexity, total variation distance squared was re-
placed with Kullback-Leibler divergence DκL(∏oid(∙∣s)∣∣∏new(∙∣s)):
2
Under review as a conference paper at ICLR 2021
η(πnew) ≥ Lπold(πnew ) - CDKL (nold||nnew )
where E = max ∣A∏(s, a) |,
s,a
C =	4ey	(8)
= (1-γ2,
DκLax(∏oid∣ιπnew ) = max DκL(∏oid (∙∣s)∣∣πnew(∙∣s))∙
s
However, this replacement greatly decreases the lower bound for the expected discounted return for
the new policy. Moreover, Kullback-Leibler divergence has no upper bound. Therefore we have no
policy-independent lower bound for this type of update.
3 Hellinger Distance in Policy Optimization
We can improve lower bound (compared to KL) by replacing DTV(∏oid(∙ | S) || ∏new(∙ | S)) With
Hellinger distance H(∏oid(∙∣s) || ∏new(∙ | s)):
H(πold(∙∖s) || πnew(∙∖s))2
1 - / Pnold(a∖s) ∏new(a∖s) da
(9)
Theorem 1 (see Appendix A or (Tsybakov, 2009, section 2.4) for proof) proves correctness and
improvement (compared to KL) to the lower bound.
Let p(v) and q(v) be two probability density functions then:
DTV(p(∙) ∖∖ q(∙))2 ≤ 2H(p(∙) ∖∖ q(∙))2 ≤ DKL(PS ∖∖ q(∙))	(10)
Replacing p(v) and q(v) with nold(∙ ∖ S) and ∏new (∙ ∖ S) respectively, new lower bound follows:
η (πnew )
≥ Lπold (πnew) -
8eY	2
--------α
(1 - γ)2
where E = max ∖Aπ (S, a)∖,
s,a
(11)
a = max H (∏old(∙∖s)∖∖πnew(∙∖s))
s
It is worth to note that H(πold(∙ ∖ s) ∖∖ ∏new(∙ ∖ s)) ≤ 1.
4 Hellinger Distance Constrained Regression (HDCR)
We could use presented lower bound as in TRPO, but instead, we derive an offline regression algo-
rithm by introducing the following optimization problem where μ is the sampling policy:
arg max J pμ(s) J π(a∖s)Aμ(s, a) da ds
s.t.
/pμ(S)H(π(∙∖s)∖∖μ(∙∖s)) ds ≤ e,
(12)
π(a∖S) da = 1, ∀S ∈ S.
Constructing Lagrangian, differentiating it with respect to π, and solving forπ gives us the following
optimal policy (see Appendix B for derivation):
…、	，I、	β2
π(" μ(a∖s)(β- 2Aμ(S,a))2 ,
where β is a Lagrangian multiplier.
(13)
Constructing a regression problem of KL divergence between optimal policy ∏ and current pol-
icy π and simplifying gives us the following supervised regression problem (see Appendix B for
derivation):
argmaxEs〜Pμ(∙) Ea〜μ(∙∣s) log"(。⑸Μ - A (S &)尸	(14)
3
Under review as a conference paper at ICLR 2021
Using notation of ABM paper (Siegel et al., 2020) ”advantage-weighting” function
is f (A(S，a)) = (β-A(s,a))2 .
If We use HDCR with Experience Replay, in equation 14, We replace μ(∙∣s) in expecta-
tion Ea	〜μ(∙	। S) and advantage function	Aμ(s,	a).	Let Π =	{∏i,	∏i+ι,…，∏i+N} be a set
of sampling policies from Which actions Were sampled, w(πi) probability of selecting policy πi,
then:
μ(s, a)
w(π)ρ∏(s)π(a∣s) dπ
Aμ(s, a)=
V (S) =
Jn w(∏)ρ∏(S)(Qn(s,a)- V∏(S)) d∏
Jn w(∏)ρ∏ (s) d∏
Jn w(∏)ρ∏(s)VΠ (S) d∏
Jn w(∏)P∏ (s) d∏
(15)
Π
The proof Will repeat the proof for AWR With Experience Replay (Peng et al., 2019).
Practically We simply sample uniformly from the replay buffer and using a value function estimator.
This type of sampling provides us an approximation of expectation and state value function.
Let D denote a set of stored trajectories (replay buffer), A(S, a; φ) is an advantage function parame-
terized by vector φ.
The most popular method to obtain A(S, a; φ) for offline learning is to use state-action function
estimator Q(S, a; φ) parameterized by φ:
A(S, a; φ)
Q(S, a; φ) -
/ π(a0|s; θk)Q(s, a0; φk)da0
(16)
Despite Q(S, a; φ) being closer to the expectation of discounted return folloWing policy π (because
of "taking" the first action according to ∏ rather then μ), we found Monte-Carlo return more effi-
cient on tiny offline datasets. Greater performance using MC return can be explained by a lack of
experience "produced" by certain actions.
Monte-Carlo estimation of A(S, a; φ) can be described as follows where RsD,a = PlT=0 γlrt+l
and V(S; φ) is a state value function estimator parameterized by vector φ:
A(S,a; φ)= RDa - V(s; φ)	(17)
Also, we can use Generalized Advantage Estimation (Schulman et al., 2016), where γ ∈ [0, 1] and
δtφk is a one-step temporal difference for state St calculated using old vector φk:
δtφk = rt + γV(St+1 ; φk) - V(St; φk)
T
A(St,at; Φk) = X(γλ)lδφkι	(18)
l=0
A(St, at； Φ) = A(St, at； Φk) + V⑶；Φk) - V⑶；Φ)
Finally, we propose the following reinforcement learning algorithm:
Algorithm 1: Hellinger Distance Constrained Regression
θι J random initial weights
Df
for iteration k = 1, ..., kmax do
add trajectories {τi} sampled via πθk to D
Φk+1 J argminφ Es,a〜DA2(S,a; φ)
θk+ι J argmaxθ Es,a〜D [log∏θ(a∣S)(力—4(；许0忆))2 ]
end
4
Under review as a conference paper at ICLR 2021
——ABM ——AWR ——HDCR (Ours)
Figure 1: Learning curves of ABM, AWR, and HDCR averaged across results of learning from 10
different datasets of 10k timestamps (also 5 seeds used to generate each dataset).
Task		ABM	AWR	HDCR (Ours)
Ant-V2	468 ± 72	495 ± 84	-569 ± 64-
HalfCheetah-v2	-7 ± 3	-7 ± 4	-8 ± 6
Hopper-v2	184 ± 72	209 ± 31	-223 ± 141-
Walker2d-v2~~	132 ± 76	125 ± 92	145 ± 117~~
Table 1: Final returns for different algorithms, with ± corresponding to one standard deviation of
the average return across 10 datasets of 10k timestamps.
5 Experiments
In our experiments, we evaluate the algorithm on MuJoCo (Todorov et al., 2012) tasks.
5.1	Tiny datasets
For evaluating on extremely small datasets we use setting inspired by Behavioral Modelling Priors
for Offline Reinforcement Learning paper (Siegel et al., 2020) but instead of using actions from a
behaviorial policy we use random actions while generating buffer.
First, we collect 2048 timestamps or more, until episode termination (whichever occurred later),
from each of 5 seeds using random actions. Then we load collected trajectories to a replay buffer
for the agent training. Separate networks with the same architecture (except the last layer) represent
the policy and value function and consist of 2 hidden layers of 256 ELU units. Each train iteration
uses only old data obtained by random agents. Each iteration, the value function is updating with 5
gradient steps and policy with 50 steps using a uniformly sampled batches of 512 samples using all
data from the replay buffer. Learning rates for Adam optimizer are 2 × 10-3 and 2 × 10-4 for critic
and actor, respectively.
We compare 3 different ”advantage-weight” functions f (A(s, a)):
•	Hellinger Distance Constrained Regression, where f (A(s, a))=(6-4- Ο)尸；
•	Advantage-weighted Behavior Model, where f (A(s, a)) = IA(s,a)>0, Ix>0 = 1 if x > 0
otherwise Ix>0 = 0;
•	Advantage-Weighted Regression, where f (A(s, a)) = exp( 1 A(s, a)).
AWR method uses β = 1.0 as it is in implementation released by authors. HDCR uses β = 1.0. For
TD(λ) we use λ = 0.95.
On simple tasks as Hopper-v2, all methods are able to learn (Figure 1), and HDCR shows slightly
better results (Table 1). While on difficult tasks as Ant-v2, all algorithms do not improve their results
through iterations. Moreover, evaluation returns decrease.
5.2	Large datasets
Next, we perform tests using buffers with the size of 100k timesteps. Buffer is filled by running
a pretrained behavioral policy. This setting replicates the setting from Off-Policy Deep Reinforce-
5
Under review as a conference paper at ICLR 2021
——ABM ——AWR ——BCQ ——HDCR (Ours)
Figure 2: Curves of BCQ, ABM, AWR, and HDCR evaluation results averaged across 3 seeds.
Task		ABM	AWR	HDCR (Ours)
Ant-v2	179 ± 45	281 ± 25	217 ± 54
HalfCheetah-v2	3358 ± 76	3365 ± 193	3485 ± 205
HoPPer-V2	495 ± 115	478 ± 94	-576 ± 123-
WaIker2d-v2~~	592 ± 139	762 ± 64 一	805 ± 62~~
Table 2: Final returns for different algorithms, with ± corresponding to one standard deviation of
the average return across 3 seeds.
ment Learning without Exploration (Fujimoto et al., 2019) paper. Therefore we also provide results
of BCQ method achieved by authors’ implementation trained from the same datasets. For calculat-
ing advantage we use equation 16 where we approximate integral by taking mean of 10 Q-values
obtained by 10 actions sampled from the policy.
While BCQ outperforms all the presented methods, it uses a gradient of Q-value function in actor
training, which provides better generalization. This provides better results on evaluation but affects
stability and performance. Against other methods that update the policy function directly, HDCR
shows better results on 3 environments out of 4 (Figure 2 and Table 2).
6 Discussion
We theoretically proved that Hellinger distance improves the lower bound of expected discounted
return compared to Kullback-Leibler divergence and proposed a simple off-policy reinforcement
learning method that uses Hellinger distance as a constraint. The expected discounted return for a
new policy now has a policy-independent lower bound. This bound guarantees that return will not
decrease in ”one” shot.
Experiments show that HDCR outperforms both ABM and AWR on tiny datasets obtained by ran-
dom agents. This performance proves the efficiency of using Hellinger distance by allowing bigger
step sizes, retaining lower bound.
On bigger datasets, HDCR shows comparable or better results than AWR and ABM.
References
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration, 2019.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
Proceedings of the Nineteenth International Conference on Machine Learning, ICML ’02, pp.
267-274, San Francisco, CA, USA, 2002. Morgan KaUfmann Publishers Inc. ISBN 1558608737.
David Levin and Yuval Peres. Markov Chains and Mixing Times. 2017. doi: 10.1090/mbk/107.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. volume 37 of Proceedings of Machine Learning Research, pp. 1889-1897,
6
Under review as a conference paper at ICLR 2021
Lille, France, 07-09 JUl 2015. PMLR. URL http://Proceedings .mlr.ρress∕v37∕
schulman15.html.
John SchUlman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continUoUs control Using generalized advantage estimation. In Proceedings of the
International Conference on Learning Representations (ICLR), 2016.
Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Ne-
Unert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what
worked: Behavior modelling priors for offline reinforcement learning. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
rke7geHtwH.
E. Todorov, T. Erez, and Y. Tassa. MUjoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer New York, 2009. doi:
10.1007/b13794. URL https://doi.org/10.1007%2Fb13794.
7
Under review as a conference paper at ICLR 2021
A Theorem 1 proof
Let (Ω, A) be a measurable space, P and Q be two probability measures on that space, V be a
σ-finite measure on (Ω, A) such that P《 V (P(A) = 0 for any A ∈ A such that μ(A) = 0) and
Q《 v. And let US denote Radon-Nikodym derivatives (in our derivations we can assume them as
probability density functions) as follows:
dP
P =汨
dQ
q =石
(19)
So, P and q satisfy following conditions:
P (A) = P(V) dV ∀A ∈ A
A
Q(A) = q(V) dV ∀A ∈ A
A
(20)
Then we define distances between probability measures:
DTV(P||Q)
H(P||Q)2
DKL(P||Q)
sup IP (A) - Q(A)I
A∈A
1 /(Ppv) - Pq(V))2 dV = 1 * - / PP(V)q(V) dV
Zbg dQdP = ZP(V)IOg p⅞ dv
(21)
Following Lemmas will be used in Theorem 1 proof:
Lemma 1. Given two probability distributions P and q total variance divergence can be calculated
as follows:
DTV(PIIQ) = AuAIP(A)-Q(A)I
2 / IP(V) - q(V)I dv
(22)
Proof. Let B
= {V : P(V) > Q(V)}, Bc = {V : P(V) ≤ Q(V)} and A ∈ A:
P(A) - Q(A) ≤ P(A∩B) - Q(A∩B) ≤ P(B) - Q(B)
P(A) - Q(A) ≤ Q(A∩ Bc) -P(A∩ Bc) ≤ Q(Bc) - P(Bc)
sup IP (A) - Q(A)I = P(B) - Q(B) = Q(Bc) - P(Bc)
A∈A
DTV(P II Q) = 1[P(B) - Q(B) + Q(Bc) - P(Bc)]
=2 / IP(V) - q(v)I dv
(23)
□
Lemma 2.
DTV (P || Q) = 1 -	min(P(V), q(V)) dV
(24)
Proof.
DTV(P || Q)
1 /1P(V)- q(V)I dV = /	() ( Jp(v) — q(V)] d
1 - Z{v: p(v)<q(v)} P(V) dV - Z{v: p(v)>q(v)} q(V) dV
1 -	min(P(V), q(V)) dV
(25)
8
Under review as a conference paper at ICLR 2021
□
Lemma 3.
max(p(v), q(v)) dv +	min(p(v), q(v)) dv = 2
(26)
Proof. Rewriting left part in 4 integrals over following sets {max(p(v), q(v)) = p(v)},
{max(p(v), q(v)) = q(v)}, {min(p(v), q(v)) = q(v)} and {min(p(v), q(v)) = q(v)} and
stacking integrals back gives Us P (Ω) + Q(Ω)=2.	口
Theorem 1. For any two probability density functions p and q, the following double inequality is
true:
DTV(P || q)2 ≤ 2H(p || q)2 ≤ Dkl(p || q)	(27)
Proof. First ineqUality can be proved as follows:
2
(1 - H(P || Q)2)2
2
≤	min(p(v), q(v)) dv	max(p(v), q(v)) dv
=	min(p(v), q(v )) dv 2 -	min(p(v), q(v )) dv
=(1-DTV(P||Q))(1+DTV(P||Q))
(28)
=1-DTV(P||Q)2
DTV(P || Q)2 ≤ H(P || Q)2(2 - H(P || Q)2)
DTV(P || Q)2 ≤ 2H(P || Q)2
Proof of the second ineqUality:
DKLlP |1Q) = ZPWlogIpv dv = 2ZPWlog S⅛dv
-2	p(v) log
用 — 1 +1 dv
p(v)
≥ -2	p(v)
q(V) - 1
p(V
dv
-2 / [pq(v)p(v) — 1i dv
(29)
2
2H(P || Q)2
DKL(P || Q) ≥ 2H(P || Q)2
□
B HDCR derivation
Given optimization problem:
arg max
π
/ρμ(s)/
π(a∣s)Aμ(s, a) da ds
s.t.
ρ Pμ(S)H (n(1s)||M(IS)) ds ≤ 3
(30)
J π(a∣s) da = 1, ∀s ∈ S.
9
Under review as a conference paper at ICLR 2021
Constructing Lagrangian where α : S → R is a function for obtaining Lagrange multiplier for
every state, β is also a Lagrange multiplier:
L(π,β, α) = J pμ(S) J π(a∣s)Aμ(s, a) da ds
pμ(s) 1 — / Pπ(a∣s)μ(a∣s) da ds
(31)
Differentiating with respect to π(a∣s) gives Us following result:
∂ L	μ(a∣s)
∂π(0M = ρμ(s)Aμ(s,a)+βρμ(s)2p∏(a∣s)μ(a∣s) - αs = 0
(32)
Solving for π(a∣s):
C /、	μ(Hs)	/、4 /	、
βPμ(s) n / z , ʌ z=F^ = αs - Pμ(s)Aμ(S, a)
27 π(a∣s)μ(a∣s)
pμ(als) =2 缶-AμGa)
P∏(a∣s)	β
(33)
Substituting π(a∣s) = μ(∙∣s) and taking expectation over actions taken according to μ gives us
expression for αs :
I =2 缶-A”(s,a)
=	β
Ea 〜μ(∙∣s)αs = αs = ρμ (S)Ea 〜μ(∙∣ s) 2 B + Aμ (s, a)
Then optimal policy π*(a∣s) can be written as follows:
β2
π*(als)= "(als)(β- 2Aμ(s,a))2
(34)
(35)
To obtain regression problem We construct Kullback-Leibler divergence between optimal policy ∏
and current policy π :
arg min Es 〜ρ”(∙)Dκz(∏*(∙∣s) || ∏(∙∣s))
π
arg min Es〜P”(∙) / ∏*(α∣s)log
arg min Es〜。*(.)/ μ(a∣s)
π*(als) dn
tʃa
π(a∣s)
β2
arg min Es〜P”(∙) Ea〜μ(∙∣s)
π
(β — 2Aμ (s,a))2
1
log ”(als) (βUAμ(s，a))2 da
π(a∣s)
(β — 2Aμ(s,a))2
arg max Es〜p“(.)Ea〜“(∙∣s) log ∏(α∣s)
π
arg max Es〜p“(.)Ea〜“(∙∣s) log ∏(α∣s)
π
log μ(a∣s)
β2
(β — 2Aμ(s,a))2
—log π(a∣s)
1
(β - 2Aμ(s,a))2
1
(β — Aμ(s,a))2
(36)
Regression problem follows:
arg max Es〜Pμ(∙) Ea〜μ(∙∣s) log ∏(a∣s)
π
1
(β — Aμ(s,a))2
(37)
10