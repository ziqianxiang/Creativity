Under review as a conference paper at ICLR 2021
GraphCGAN: Convolutional Graph Neural
Network with Generative Adversarial Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
Graph convolutional networks (GCN) achieved superior performances in graph-
based semi-supervised learning (SSL) tasks. Generative adversarial networks
(GAN) also show the ability to increase the performance in SSL. However, there
is still no good way to combine the GAN and GCN in graph-based SSL tasks. In
this work, we present GraphCGAN, a novel framework to incorporate adversar-
ial learning with convolution-based graph neural network, to operate on graph-
structured data. In GraphCGAN, we show that generator can generate topology
structure and attributes/features of fake nodes jointly and boost the performance
of convolution-based graph neural network classifier. In a number of experiments
on benchmark datasets, we show that the proposed GraphCGAN outperforms the
reference methods by a significant margin.
1	Introduction
Graph-based semi-supervised learning (SSL) aims to classify nodes in graph, where only small
amounts of nodes are labeled due to the expensive and time-consuming label collection process.
To solve such task, various graph neural networks (GNNs) have been proposed using the idea of
convolutional neural networks (CNN) to implicitly propagate the information of labeled nodes to
unlabeled nodes through the linkage between nodes (KiPf & Welling, 2016; VeliCkovic et al., 2017;
Hamilton et al., 2017). These convolution-based graph neural networks have achieved superior
Performance on multiPle benchmark datasets in graPh-based SSL tasks (Wu et al., 2019).
Recently, generative adversarial networks (GANs) (Goodfellow et al., 2014) have been shown a
Power in imProving the Performance of image-based SSL Problems (Odena, 2016; Salimans et al.,
2016; Li et al., 2019b). In semi-GAN (Salimans et al., 2016), authors converted the M -class classi-
fication task into solving (M + 1)-class Problem where the synthetic (M + 1)th class is generated
by the GAN’s generator. Later on, Dai et al. Provided a theoretical insight that the generated data
are able to boost the Performance of classifier under certain assumPtions. Our work is motivated by
the the semi-GAN.
GraPhSGAN (Ding et al., 2018) first investigated the adversarial learning over graPh, where the
graPh is embedding into an embedding sPace and synthetic data are generated in the corresPond-
ing sPace. The multi-layer PercePtron (MLP) is trained as the classifier on the embedding vectors.
However, to our knowledge, there is still no existed method to combine the adversarial learning to
convolution-based GNNs on graPh-based SSL task. In this work, we exPlore the Potential of incor-
Porating the convolution-based GNN and GAN. The challenges of constructing a general framework
have three folds: first, the attributed graPh data are non-Euclidean whose distribution contains in-
formation of graPh toPology structure as well as the attributes of nodes. Hence, it is not trivial to
construct generator to model the distribution. Second, even the generator can model the graPh’s
distribution, the generator should be trained ProPerly to boost the Performance of the classifier. A
Poor-quality generator would introduce noise to the existed graPh and affect the classifier. Third,
many variants of GCN have been ProPosed continuously. The framework should be built with flexi-
bility to adaPt to different convolution-based GNNs.
We construct a novel aPProach called GraPhCGAN to deal with above challenges. First, to model
the distribution of graPh, the generator is built sequentially from two sub-generators: one models
1
Under review as a conference paper at ICLR 2021
the attribute information (node’s attribute) and another one models the graph topology structure
(adjacency relation of node). Details can be found in Section 3.1. Second, in GraphCGAN, the
generator is trained based on the feature matching technique (Salimans et al., 2016) which minimizes
the distance between generated nodes and real nodes in the constructed feature space. This technique
showed a good performance in SSL tasks in practice. The details for construction of loss functions
can be found in Section 3.3. For GCN, the attributes of nodes are aggregated convolutionally by
multiple layers. The representation of the last layer is usually considered as the prediction for the
labels. For variants of GCN, the main differences exist in the strategy of layer aggregation (Hamilton
et al., 2017). In our framework, we choose the second to the last layer of convolution-based GNN
as the feature matching functions. Therefore, our framework is easily extended to variants of GCN.
More discussions can be found in Section 3.2.
2	Preliminary
We first introduce the notation about graph. Let G = (V, E) denote a graph, where V is the set
of nodes with |V | = n and E ⊂ V × V is a set of edges with |E | = m. The adjacency matrix
A ∈ RlVl×lVl is defined as Aij = 1 if node vi and vj has edge, otherwise Aij = 0. Suppose each
node vi has a d-dimensional feature xi ∈ Rd and a single value label yi ∈ {1, 2, .., M}. In the
semi-supervised learning setting, there is a disjoint partition for the nodes, V = V L ∪ V U , such
that, for vi ∈ V L , the corresponding label is known and for vj ∈ V U the corresponding label is
unknown. The distributions of node in labeled set V L and unlabeled set VU are denoted as pVL and
pVU, respectively. The semi-supervised learning is to learn the label for unlabeled set {yj |vj ∈ VU}
given adjacency matrix A, feature matrix X = [xi]vi∈V and labels for labeled sets {yi|vi ∈ V L}.
2.1	Convolution based Graph Neural Network Classifier
Based on the Laplacian smoothing, the convolution-based GNN models propagate the information
of nodes features across the nodes’ neighbors in each layer. Specifically, in GCN, the layer-wise
propagation rule can be defined as follows:
H(l+1) = σ(D-1AH(l)W(l) +b(l)), l=0,1,2..,L- 1	(1)
where W(I) and b(l) are layer-specific trainable weight matrix and bias, respectively. σ(∙) is an
activation function. D is the diagonal degree matrix with Dii = Pj Aij. Hence, D-1A represents
normalization of adjacency matrix A. The initial layer H(0) is the feature matrix X. The final layer
H(L) followed by a sof t max layer can be viewed as the prediction of one-hot representation for
the true label y.
Recently, many variants of the GCN layer-wise propagation rule had been proposed, including graph
attention network, cluster GCN (Velickovic et al., 2017; Chiang et al., 2019), which achieved state-
of-the-art performances in many benchmark datasets.
2.2	Generative Adversarial Network based Semi-supervised Learning
In semi-GAN, the classifier C and generator G play a non-cooperative game, where classifier aims
to classify the unlabeled data as well as distinguish the generated data from real data; generator
attempts to match feature of real data and that of generated data. Therefore, the objective function
for classifier can be divided into two parts (Salimans et al., 2016). The first part is the supervised
loss function
Lsup = Ev,y〜PVL logPC(y∣v,y ≤ M)
which is the log probability of the node label given the real nodes. The second part is the unsuper-
vised loss function
Lun-sup = Ev〜PVU log PC(y ≤ M|v) + Ev〜PVG log PC(y = M + 1|v)
which is the sum of log probability of the first M classes for real nodes and the log probability of the
(M + 1)th class for generated nodes V G. The classifier C can be trained by maximize the objective
function
LC
LsuP
+ Lun-suP.
(2)
2
Under review as a conference paper at ICLR 2021
For objective function of generator, Salimans et al. (2016) found minimizing feature matching loss
in Equation 3 achieved superior performance in practice
LG = IIEv〜PVU (f(V))- Ez〜pz(z)(f(G(Z)))I∣2,	⑶
where the feature matching function f(∙) maps the input into a feature space and Z 〜pz(z) is
drawn from a given distribution like uniform distribution. Furthermore, Dai et al. (2017) provided
a theoretical justification that complementary generator G was able to boost the performance of
classifier C in SSL task.
3	Framework of GraphCGAN
To combine the aforementioned Laplacian smoothing on graph and semi-GAN on SSL together, we
develop GraphCGAN model, using generated nodes to boost the performance of convolution-based
GNN models.
3.1	Construction of Generator for GraphCGAN
The generator G generates fake node v0 by generating feature vector x0 ∈ Rd and adjacency relation
a0 ∈ Rn jointly, where a0,i = 1 if the fake node is connected to real node vi, otherwise a0,i = 0.
Therefore, the distribution for generated node pG (v0) can be expressed by the joint distribution of the
corresponding feature andadjacencyrelationpG(x0, a0). From the conditional distribution formula,
the joint distribution can be written as pG(x0, a0) = pG1 (x0)pG2 (a0Ix0). We use sub-generators
G1 and G2 to generate fake feature x0 and a0Ix0, respectively. In practice, a0Ix0 can be modeled
by G2(Z; x0) = G2(Z; G1(Z)) where the adjacency relation a0 is constructed by sub-generator G2
given the input of x0 . The distribution of generated node can be denoted by
pG(v0) = pG(x0, a0) = pG(x0)p(a0Ix0) = p(G1(Z))p(G2(Z; G1(Z))) =: p(G(Z)).	(4)
If B nodes (v0,1, v0,2, .., v0,B) are generated, the generated feature matrix is denoted as X0 =
(x0T,1, x0T,2, .., x0T,B)T and generated adjacency matrix has form A0 = (a0T,1, a0T,2, .., a0T,B)T. Hence,
the combined adjacency matrix can be denoted as
A = A AT ∈ R(n+B)×(n+B)
A0 IB
The combined feature vector is
X = [X ∈ R(n+B)×d.
X0
(5)
(6)
The diagonal degree matrix D ∈ R(n+B)×(n+B) Can be denoted as D0*	短 where D* ∈ Rn×n
with D*,ii = Pj Aij + Pb Ao,bi and DB ∈ Rb×b with Db^ = Pj Ao,bj + 1.
3.2	Analysis of Classifier for GraphCGAN
In GraphCGAN, we adopt the convolution-based GNN, such as GCN, GraphSage (Hamilton et al.,
2017) or GAT (Velickovic et al., 2017), as the the classifier. The classifier is applied to the enlarged
graph G = [X, A] to obtain the prediction y of nodes V ∪ Vg.
3
Under review as a conference paper at ICLR 2021
Specially, considering the layer-wise propagation of GCN (Equation 1) as the classifier in GraphC-
GAN, the propogation rule can be denoted as
HH(I+1) = σ(D TA HH (I)W(I) + b (I))
σ(
σ(
D-1
0
AIB0T
W(l) +
D-IAH? + D-1AT HOl)
DB1AoH?+ DBIHOl)
b(l)
b(Bl)
W(l) +
(7)
)
D-IAHaW(I) + b?
(DB1AoH^l) + DBIW(I))W(I) + bB)
H，l+1)
H(Ol+1)
where the first layer is chosen as the enlarged feature matrix H(O) = X. Weight matrix W(I) has
the same in Equation 1. Bias vector b(I) has dimension (n + B) which is denoted as [b(l)T, bB)T]T.
We denote b?) = D-1ATH?) W(I) + b(l) to make the format clear. From Equation 7, the layer
propagation of real nodes (first n rows) follows the same format as the GCN layer propagation in
Equation 1. As a special case, for the zero generator AO = 0 or XO = 0, the performance of
classifier on V ∪ V G would be the same as that of original classifier on V .
For the last layer H(L) ∈ R(n+B)×M, We adopt the strategy in Salimans et al. (2016) to obtain the
(M + 1) class label y by
y = soft max(H(L)∣∣0(n+B)×1),	(8)
where || denotes concatenation and 0(n+B)×1 ∈ R(n+B)×1 is a zero matrix. The loss function for
classifier in GraphCGAN follows the same format in Equation 2.
3.3	Loss functions
Let US denote g(∙, ∙; θc) as the map from feature vector and adjacency vector to the space of second
to the last layer in convolution-based GNN with trainable parameter θC. Specially, in the case of
GCN, for node vi with feature vector xi and adjacency vector ai ,
g(xi, ai； θc) = H(L-1),	(9)
where H(LT) denotes the i-th row of H(LT) and θc = [W(l); b(I)]l=0,1,..,L-2.
According to Equation 4, the loss function of generator G can be decomposed into two parts: the loss
functions of sub-generators G1 and G2 separately. To construct G1, the feature matching function
f in Equation 3 should solely depend on feature vector. Therefore, we mask the adjacency matrix
A as identity matrix I ∈ R(n+B)×(n+B) in layer propagation. Formally, the feature matching loss
function of G1 is constructed as
Lgi = ||Exi(g(xi, Ii； θc))- Ez〜pz(z)(g(G1(z), 0； θc))||2,
where Ii denote the i-th row of identity matrix I ∈ Rn×n and 0 is the zero vector.
After xO = G1 (z) is built, the feature matching loss function of G2 can be constructed similarly
from
LG2 = ||Eai(g(xi, ai； OC))- Ez〜pz(z)(g(x0,G2(z); θc))∣∣2.
Therefore, loss function for G can be written as
LG = LG1 + LG2.	(10)
Furthermore, when multiple fake nodes are generated, Salimans et al. (2016) showed that adding
pull-away item to loss function can increase the entropy of generator which led to better performance
4
Under review as a conference paper at ICLR 2021
in practice. The pull-away loss for sub-generators G1, G2 can be denoted as
Lpt _	1	1X1X(	g(GI(Zi), 0; θC)Tg(GI(Zj" 0; θc))
GI = B(B -1) JJi(∣∣g(Gι(Zi)), 0; θc)∣∣∣∣g(Gι(Zj)), 0; θ0)||)
i j 6=i
and
Lpt _	1	'X'X( g(X0,i, G2 (Zi); θC )T g(XOj ,G2 (Zj ); θC ))
G2 = B(B -1) JJi(∣∣g(x0,i,G2(%)；θc)∣∣∣∣g(x0,j,G2(zj); θc)∣∣).
i j 6=i
The loss function for G with pull-away item can be written as
LG = LG + Lpti + LG2.	(11)
Besides, Dai et al. (2017) constructed the complementary loss by
LcG1 =EX 〜pG] Iog(P(X))I(P(X) > ε), L(G2 = Ea~pG2 log(p(a))I(p(a) > ε),
which could also increase performance. Therefore, the loss function for G with complementary loss
can be written as
LG= = LG + LGi + LG2.	(12)
The procedure is formally presented in Algorithm 1.
1
2
3
4
5
6
7
8
9
10
11
Algorithm 1: GraphCGAN Algorithm
Input: Adjacency matrix A, Node feature X, initialized fake nodes V G = [A0 , X0].
hyper-parameters including dimension of the noise vector dnoise, the number of steps
KD, and the size of fake nodes B and early stop error.
Output: Prediction Y
while not early stop do
Combine the fake nodes VG to the graph and obtain A and XX from Equation 5 and
Equation 6;
Classifier:
iterD = 0
while iterD < KD do
Use convolution-based GNN as the classifier C, and extract the map to the intermediate
layer g(., .) as Equation 9;
Train C by minimizing LC (Equation 2) on combined graph, obtain predicted result Y;
L iter D = iter D + 1;
Generator:
Generate a random noise vector Z 〜 U(0,I) ∈ RB×dnoise;
Train generator G = [G1; G2] by minimizing Equation 10 or Equation 11 or Equation 12;
Obtain X。= Gi(Z) and A0 = G2(Z; GI(Z)).
4	Related Work
4.1	Graph-based semi-supervised learning
The challenge for graph-based SSL is to leverage unlabeled data to improve performance in classi-
fication. There are three categories of the Graph-based semi-supervised learning. The first one is
the Laplacian regularization-based methods (Xiaojin & Zoubin, 2002; Lu & Getoor, 2003; Belkin
et al., 2006). The second type is the embedding-based methods, including DeepWalk (Perozzi et al.,
2014), SemiEmb (Weston et al., 2012), and Planetoid (Yang et al., 2016). The third type is convo-
Iutional based graph neural networks such as GCN (KiPf & Welling, 2016), GAT (Velickovic et al.,
2017), ClusterGCN (Chiang et al., 2019) and DeepGCN (Li et al., 2019a). Such methods address
the semi-supervised learning in an end-to-end manner. Convolution-based methods perform the
graph convolution by taking the weighted average of a node’s neighborhood information. In many
graph semi-supervised learning tasks, the convolution-based methods achieved the state-of-the-art
performance (Wu et al., 2019).
5
Under review as a conference paper at ICLR 2021
4.2	GNN learning with GAN
GAN is wildly used in obtaining generative graph models. GraphGAN (Wang et al., 2018) proposed
a framework for graph embedding task. Specifically, GraphGAN can generate the link relation for a
center node. However, GraphGAN cannot be applied to attributed graph.
MolGAN (De Cao & Kipf, 2018) proposed a framework for generating the attributed graph of
molecule by generating the adjacency matrix and feature matrix independently. After that, MolGAN
used an the score for the generated molecule as reward function to choose the reasonable combina-
tion of attributes and topology structure by an auxiliary reinforcement learning model. In compari-
son, GraphCGAN can generate attributes and adjacency matrix of the attributed graph jointly, which
can capture the correlation between the attributes and topology relation.
DGI (Velickovic et al., 2018) proposed a general approach for learning node representations within
graph-structured data in an unsupervised manner. For the generator, In DGI, the fake nodes are
created from a pre-specified corruption function applied on the original nodes. In contrast, our
GraphCGAN can generate the fake nodes from a dynamic generator during the training GAN pro-
cess. For the classifier, the DGI uses GCN only, however, our GraphCGAN is flexible and adaptive
to other convolution-based GNN models.
4.3	GAN with semi-supervised learning
SGAN (Odena, 2016) first introduced the adversarial learning to the semi-supervised learning on
image classification task. GAN-FM (Salimans et al., 2016) stabilized training process in SGAN
by introducing feature-matching and minibatch techniques. In Kumar et al. (2017), authors discuss
about the effects of adding fake samples and claimed that moderate fake samples could improve the
performance in image classification task.
GraphSGAN Ding et al. (2018) proposed a framework for graph Laplacian regularization based clas-
sifier with GAN to solve graph-based semi-supervised learning tasks. In GraphSGAN, fake samples
in the feature space of hidden layer are generated, hence it can not be applied to convolutional based
classifiers. In constrast, our model generates fake nodes directly and is adaptive to convolutional
based classifiers.
5	Experiments
In this section, our primary goal is to show that the adversary learning can boost the performance
of convolution-based GNNs in graph-based SSL under our framework. We evaluate GraphCGAN
on established graph-based benchmark tasks against baseline convolution-based GNN models and
some other related methods. We first introduce the dataset, experiment setup and results. Besides,
we study the property of the generated nodes from our model during the training process. The
ablation study is also provided in this section. The code GraphCGAN-ICLR.zip is provided as the
supplementary file.
5.1	Datasets
Three standard citation network benchmark datasets - Cora, Citeseer and Pubmed (Sen et al., 2008)
are analyzed. We closely follow the setting in KiPf & Welling (2016) and Velickovic et al. (2017)
which allows for only 20 nodes per class to be used for training. The predictive power of the trained
models is evaluated on 1000 test nodes, and 500 additional nodes are used for validation purposes.
5.2	Experiment Setup and Result
Two widely used of convolution-based GNNs, GCN and GAT, are considered as classifiers in
GraphCGAN. In order to show the generated nodes can help improve the performance of the meth-
ods. We adopt the same model setting in the original papers (Kipf & Welling, 2016; Velickovic et al.,
2017). Specially, for classifier in GraphCGAN-GCN, the number of layers L is 2, the dimension of
the hidden layer is 16, the dropout rate is 0.5, activation function in the hidden layer is Relu. For
GraphCGAN-GAT, the number of layers L is 2, the dimension of the hidden layer is 8, and number
6
Under review as a conference paper at ICLR 2021
Figure 1: Ablation study for size of fake nodes B . It can be shown that moderate size of fake nodes
Can boost the classifier in graph-based SSL.
Method	Cora	Citeseer	Pubmed
MLP	551%	46.5%	71.4%
GraphSGAN (Ding et al., 2018)	83.0 ± 1.3%	73.1 ± 1.8%	77.2 ± 2.6 %
DGI (velivckovic et al.,2018)	82.3 ± 0.6%	71.8 ± 0.7%	76.8 ± 0.6 %
GCN (kipf & welling, 2017)	81.5%	70.3%	79.0 %
GraphCGAN-GCN (ours)	82.4 ± 0.6%	72.6 ± 1.0%	79.9 ± 1.0%
Gain for GCN	0.9%	2.3 %	0.9 %
GAT (velivckovic et al., 2017)	83.0 ± 0.7%	72.5 ± 0.7%	79.0 ± 0.3%
GraphCGAN-GAT (ours)	84.0 ± 0.5%	73.2 ± 1.0%	80.7 ± 1.5%
Gain for GAT	1.0 %	0.7 %	1.7 %
Table 1: Summary of results in terms of classification accuracy under 100 repetitions. The best
and the second best results are masked in bold font. The results show that GraphCGAN-GCN and
GraphCGAN-GAT outperform GCN and GAT in a significant margin, respectively.
of attention heads is 8, the dropout rate is 0.6, activation function in the hidden layer is Sigmoid.
The hyper-parameter for the weight of L2 regularization is 5e-4. For the generator, we use the loss
function in Equation 12 (Ablation study for loss function of generator can be found in Table 2 Ap-
pendix A). In Cora and Citeseer, we generate B = 64 fake nodes. In Pubmed, the number of fake
nodes is B = 256. The ablation study of size of fake nodes are provided in Figure 1.
The results is presented in Table 1, the best and the second best results are masked in bold font. We
particularly note that both GraphCGAN-GCN and GraphCGAN-GAT outperform GCN and GAT in
a significant margin, respectively. More specifically, we are able to improve upon GCN by a margin
of 0.9%, 2.3% and 0.9% on Cora, Citeseer and Pubmed, respectively. Besides, GraphCGAN-GAT
can improve upon GAT by a margin of 1.0%, 0.7% and 1.7%, suggesting that the adding fake nodes
strategy in our GraphCGAN model can boost the performance for reference convolution-based GNN
model. To be noticed that GraphCGAN can be easily extended to other convolution-based GNN
models.
5.3 Visualization of GAN process
In this subsection, we investigate about the distribution of the generated nodes during the training
process. We consider three datasets to illustrate generated nodes in different perspectives. For Karate
club graph (Zachary, 1977), it contains 34 nodes without features. The feature matrix X is set as
identity matrix during the training process. Therefore, the plot (first row in Figure 2) of fake nodes
shows the distribution of G2 (z; I). It can be found, after training, fake nodes mainly connect to
the boundary nodes1 which is preferred as discussed in GraphSGAN (Ding et al., 2018). MNIST
datasets (LeCun et al., 1998) contain the images of handwritten digit. We can consider it as a graph
1Boundary nodes are nodes connected to different clusters
7
Under review as a conference paper at ICLR 2021
with image feature by constructing an identity adjacency matrix A = I. Therefore, the plot (second
row in Figure 2) of fake feature shows the distribution of G1(z) which has the shape around to digit
eight. Last, we generated B = 256 nodes for Cora dataset which are plotted in two-dimension by
T-SHN (Van Der Maaten, 2014) techniques on the feature space of g(∙, ∙; θc) shown in the third row
in Figure 2, which can be considered as the distribution for G(z). We can find the generated nodes
present as a complementary part for the existed nodes
Figure 2: Representation of the generated nodes during training process. Karat club: Real nodes are
shown as colored dots representing different manually assigned groups. Three fake nodes (black)
are generated, for fake nodes, the initial generated adjacency vectors are set as 1, after training,
fake nodes mainly connect to the boundary nodes; MNIST: One fake image as node’s feature are
generated, after training, the fake image shows the shape around to digit eight; Cora: 256 fake nodes
(black) with features are generated, the plots show the t-SNE embedding for the feature space of the
graph, the generated nodes show as a complementary part for the existed nodes (colored dots).
6 Conclusion
We propose GraphCGAN, a novel framework to improve the convolution-based GNN using GAN. In
GraphCGAN, we design a generator to generate attributed graph, which is able to generate adjacency
matrix and feature jointly. We also provide a new insight for the semi-supervised learning with
convoluntional graph neural network under GAN structure. A flexible algorithm is proposed, which
can be easily extended to other sophisticated architecture of GraphC, such as GAAN (Zhang et al.,
2018) and GIN (Xu et al., 2018).
One potential future direction is to extend the GraphCGAN in other relevant tasks including commu-
nity detection, co-embedding of attributed network (Meng et al., 2019) and even graph classification.
Extending the model to incorporate edge features by generating the fake edge will allow us to tackle
a larger amount of problems. Finally, in GAN, the stability of training process can be studied.
8
Under review as a conference paper at ICLR 2021
References
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(Nov):2399-2434, 2006.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257-266, 2019.
Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Russ R Salakhutdinov. Good semi-
supervised learning that requires a bad gan. In Advances in neural information processing systems,
pp. 6510-6520, 2017.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs.
arXiv preprint arXiv:1805.11973, 2018.
Ming Ding, Jie Tang, and Jie Zhang. Semi-supervised learning on graphs with generative adversarial
nets. In Proceedings of the 27th ACM International Conference on Information and Knowledge
Management, pp. 913-922. ACM, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher. Semi-supervised learning with gans: Mani-
fold invariance with improved inference. In Advances in Neural Information Processing Systems,
pp. 5534-5544, 2017.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep as
cnns? In Proceedings of the IEEE International Conference on Computer Vision, pp. 9267-9276,
2019a.
Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier, and Corey W Arnold. Semi-
supervised learning based on generative adversarial network: a comparison between good gan
and bad gan approach. In CVPR Workshops, 2019b.
Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the 20th International
Conference on Machine Learning (ICML-03), pp. 496-503, 2003.
Zaiqiao Meng, Shangsong Liang, Jinyuan Fang, and Teng Xiao. Semi-supervisedly co-embedding
attributed networks. In Advances in Neural Information Processing Systems, pp. 6504-6513,
2019.
Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint
arXiv:1606.01583, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710. ACM, 2014.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
9
Under review as a conference paper at ICLR 2021
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. The Journal of Machine
Learning Research, 15(1):3221-3245, 2014.
Petar VeliCkovic, Guillem CUcurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.
Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie,
and Minyi Guo. Graphgan: Graph representation learning with generative adversarial nets. In
Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks of the Trade, pp. 639-655. Springer, 2012.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A
comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.
Zhu Xiaojin and Ghahramani Zoubin. Learning from labeled and unlabeled data with label propa-
gation. Tech. Rep., Technical Report CMU-CALD-02-107, Carnegie Mellon University, 2002.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. arXiv preprint arXiv:1603.08861, 2016.
Wayne W Zachary. An information flow model for conflict and fission in small groups. Journal of
anthropological research, pp. 452-473, 1977.
Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan:
Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint
arXiv:1803.07294, 2018.
A Ablation study for loss function of generator
Method	Cora	Citeseer	Pubmed
GCN (Kipf & Welling, 2017)	815%	70.3%	79.0 %
GraphCGAN-GCN (LG)	81.6 ± 0.5%	71.6 ± 0.6%	79.2 ± 0.5%
GraPhCGAN-GCN (LG)	82.3 ± 0.7%	72.5 ± 1.0%	80.0 ± 1.1%
GraPhCGAN-GCN (LGn	82.4 ± 0.6%	72.6 ± 1.0%	79.9 ± 1.0%
GAT (velivckovic et al., 2017)	83.0 ± 0.7%	72.5 ± 0.7%	79.0 ± 0.3%
GraPhCGAN-GAT (LG)	82.9 ± 0.4%	72.9 ± 0.6%	78.9 ± 0.5%
GraphCGAN-GAT (LG)	84.1 ± 0.6%	72.9 ± 1.1%	80.0 ± 1.4%
GraphCGAN-GAT (LG)	84.0 ± 0.5%	73.2 ± 1.0%	80.7 ± 1.5%
Table 2: Ablation study on loss function of generator.
10