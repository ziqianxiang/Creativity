Under review as a conference paper at ICLR 2021
Temperature check: theory and practice for
training models with softmax-cross-entropy
LOSSES
Anonymous authors
Paper under double-blind review
Ab stract
The softmax function combined with a cross-entropy loss is a principled approach
to modeling probability distributions that has become ubiquitous in deep learning.
The softmax function is defined by a lone hyperparameter, the temperature, that is
commonly set to one or regarded as a way to tune model confidence after training;
however, less is known about how the temperature impacts training dynamics or
generalization performance. In this work we develop a theory of early learning
for models trained with softmax-cross-entropy loss and show that the learning
dynamics depend crucially on the inverse-temperature β as well as the magnitude
of the logits at initialization, ∣∣βz∣∣2. We follow UP these analytic results with a
large-scale empirical study of a variety of model architectures trained on CIFAR10,
ImageNet, and IMDB sentiment analysis. We find that generalization Performance
dePends strongly on the temPerature, but only weakly on the initial logit magnitude.
We Provide evidence that the dePendence of generalization on β is not due to
changes in model confidence, but is a dynamical Phenomenon. It follows that the
addition of β as a tunable hyPerParameter is key to maximizing model Performance.
Although we find the oPtimal β to be sensitive to the architecture, our results
suggest that tuning β over the range 10-2 to 101 imProves Performance over all
architectures studied. We find that smaller β may lead to better Peak Performance
at the cost of learning stability.
1	Introduction
DeeP learning has led to breakthroughs across a slew of classification tasks (LeCun et al., 1989;
Krizhevsky et al., 2012; Zagoruyko and Komodakis, 2017). Crucial comPonents of this success
have been the use of the softmax function to model Predicted class-Probabilities combined with
the cross-entroPy loss function as a measure of distance between the Predicted distribution and the
label (Kline and Berardi, 2005; Golik et al., 2013). Significant work has gone into imProving the
generalization Performance of softmax-cross-entroPy learning. A Particularly successful aPProach
has been to imProve overfitting by reducing model confidence; this has been done by regularizing
outPuts using confidence regularization (Pereyra et al., 2017) or by augmenting data using label
smoothing (Muller et al., 20l9; Szegedy et al., 2016). Another way to manipulate model confidence is
to tune the temPerature of the softmax function, which is otherwise commonly set to one. Adjusting
the softmax temperature during training has been shown to be important in metric learning (Wu et al.,
2018; Zhai and Wu, 2019) and when performing distillation (Hinton et al., 2015); as well as for
post-training calibration of prediction probabilities (Platt, 2000; Guo et al., 2017).
The interplay between temperature, learning, and generalization is complex and not well-understood
in the general case. Although significant recent theoretical progress has been made understanding
generalization and learning in wide neural networks approximated as linear models, analysis of
linearized learning dynamics has largely focused on the case of squared error losses (Jacot et al.,
2018; Du et al., 2019; Lee et al., 2019; Novak et al., 2019a; Xiao et al., 2019). Infinitely-wide
networks trained with softmax-cross-entropy loss have been shown to converge to max-margin
classifiers in a particular function space norm (Chizat and Bach, 2020), but timescales of convergence
are not known. Additionally, many well-performing models operate best away from the linearized
regime (Novak et al., 2019a; Aitchison, 2019). This means that understanding the deviations of
1
Under review as a conference paper at ICLR 2021
models from their linearization around initialization is important for understanding generalization
(Lee et al., 2019; Chizat et al., 2019).
In this paper, we investigate the training of neural networks with softmax-cross-entropy losses. In
general this problem is analytically intractable; to make progress we pursue a strategy that combines
analytic insights at short times with a comprehensive set of experiments that capture the entirety
of training. At short times, models can be understood in terms of a linearization about their initial
parameters along with nonlinear corrections. In the linear regime we find that networks trained with
different inverse-temperatures, β = 1/T, behave identically provided the learning rate is scaled as
η = ηβ2. Here, networks begin to learn over a timescale Tz 〜∣∣Z0k2∕η where Z0 are the initial
logits of the network after being multiplied by β . This implies that we expect learning to begin faster
for networks with smaller logits. The learning dynamics begin to become nonlinear over another,
independent, timescale Tnl 〜β∕η, suggesting more nonlinear learning for small β. From previous
results we expect that neural networks will perform best in this regime where they quickly exit the
linear regime (Chizat et al., 2019; Lee et al., 2020; Lewkowycz et al., 2020).
We combine these analytic results with extensive experiments on competitive neural networks across
a range of architectures and domains including: Wide Residual networks (Zagoruyko and Komodakis,
2017) on CIFAR10 (Krizhevsky, 2009), ResNet-50 (He et al., 2016) on ImageNet (Deng et al., 2009),
and GRUs (Chung et al., 2014) on the IMDB sentiment analysis task (Maas et al., 2011). In the case
of residual networks, we consider architectures with and without batch normalization, which can
appreciably change the learning dynamics (Ioffe and Szegedy, 2015). For all models studied, we find
that generalization performance is poor at ∣Z0 ∣2	1 but otherwise largely independent of ∣Z0 ∣2 .
Moreover, learning becomes slower and less stable at very small β ; indeed, the optimal learning rate
scales like η* 〜1∕β and the resulting early learning timescale can be written as Tz:〜 kZ0∣∣2∕β. For
all models studied, we observe strong performance for β ∈ [10-2, 101] although the specific optimal
β is architecture dependent. Emphatically, the optimal β is often far from 1. For models without
batch normalization, smaller β can give stronger results on some training runs, with others failing to
train due to instability. Overall, these results suggest that model performance can often be improved
by tuning β over the range of [10-2, 101].
2	Theory
We begin with a precise description of the problem setting before discussing a theory of learning at
short times. We will show the following:
•	The inverse temperature β and logit scale ∣Z0 ∣ control timescales which determine the rate
of change of the loss, the relative change in logits, and the time for learning to leave the
linear learning regime.
•	Small β causes training to access the non-linear learning regime. We will see empirically
that increasing access to the non-linear regime can improve generalization.
•	The largest allowable learning rate is set by the timescale to leave the linearized learning
regime, which suggests that networks with small β will train more slowly.
All numerical results in this section are using a Wide Resnet (Zagoruyko and Komodakis, 2017)
trained on CIFAR10.
2.1	Basic model and notation
We consider a classification task with K classes. For an N dimensional input x, let z(x, θ) be
the pre-softmax output of a classification model parameterized by θ ∈ RP, such that the classifier
predicts the class i corresponding to the largest output value zi . We will mainly consider θ trained by
SGD on a training set (X, Y) of M input-label pairs. We focus on models trained with cross-entropy
loss with a non-trivial inverse temperature β. The softmax-cross-entropy loss can be written as
KK
L(θ, X,Y) = X Yi ∙ln(σ(βzi(X, θ))) = X Yi ∙ln(σ(Zi(X, θ)))	(1)
2
Under review as a conference paper at ICLR 2021
where we define the rescaled logits Z = βz and σ(Z)i = eZi/ Pj eZj is the softmax function. Here
Z(X, θ) is the M × K dimensional matrix of rescaled logits on the training set.
As we will see later, the statistics of individual σ(Z)i will have a strong influence on the learning
dynamics. While the statistics of σ(Z)i are intractable for intermediate magnitudes, kZk2, they
can be understood in the limits of large and small kZk2. For a fixed model z(x, θ), β controls
the certainty of the predicted probabilities. Values of β such that β 1/kzk2 will give small
values of kZk2	1, and the outputs of the softmax will be close to 1/K independent of i (the
maximum-entropy distribution on K classes). Larger values of β such that β 1/kzk2 will lead
to large values of kZk2 1; the resulting distribution has probability close to 1 on one label, and
(exponentially) close to 0 on the others.
The continuous time learning dynamics (exact in the limit of small learning rate) are given by:
θ = ηβ XX ( dz¾≡ )T(Yi-σ(Zi(X, θ(t)))
i=1
(2)
for learning rate η. We will drop the explicit dependence of Zi on θ from here onward, and we will
denote time dependence as Zi(X, t) explicitly where needed.
In function space, the dynamics of the model outputs on an input x are given by
dzi(x)
dt
K
ηβ X(Θ θ )ij (x, X )(Yj-σ(Zj (X)))
j=1
(3)
where we define the M × M × K × K dimensional tensor Θθ , the empirical neural tangent kernel
(NTK), as
(Θθ )ij (x, X 0)≡ 等(j )T	(4)
for class indices i and j , which is block-diagonal in the infinite-width limit.
From Equation 3, we see that the early-time dynamics in function space depend on β, the initial
softmax input Z(X, 0) on the training set, and the initial Θθ. Changing these observables across a
model family will lead to different learning trajectories early in learning. Since significant work has
already studied the effects of the NTK, here we focus on the effects of changing β and kZ0 kF ≡
kZ(X, 0)kF (the norm of the M × K dimensional matrix of training logits), independent of Θθ.
2.2	Linearized dynamics
For small changes in θ, the tangent kernel is approximately constant throughout learning (Jacot et al.,
2018), and we drop the explicit θ dependence in this subsection. The linearized dynamics of z(x, t)
only depend on the initial value of Θ and the β-scaled logit values Z(X, t). This suggests that there
is a universal timescale across β and η which can be used to compare linearized trajectories with
different parameter values. Indeed, if We define an effective learning rate η ≡ ηβ2, We have
dZdtx)= η XX (Θ)ij (x, X )(Yj-σ(Zj (X)))	⑸
j=1
Which removes explicit β dependence of the dynamics. We note that a similar rescaling exists for the
continuous time versions of other optimizers like momentum (Appendix B).
The effective learning rate η is useful for understanding the nonlinear dynamics, as plotting learning
curves versus ηt causes early-time collapse for fixed Z0 across β and η (Figure 1). We see that there
is a strong, monotonic, dependence of the time at Which the nonlinear model begins to deviate from
its linearization on β. We Will return to and explain this phenomenon in Section 2.4.
Unless otherwise noted, we will analyze all timescales in units of η instead of η, as it will allow for
the appropriate early-time comparisons betWeen models With different β .
3
Under review as a conference paper at ICLR 2021
——G=IO-S
0 = 10-2
——0 = 1OT
——G=IQo
——/3=IO1
——/3=IO2
/3 = IO8
---Linearized
m
Figure 1: For fixed initial training set logits Z0, plotting learning curves against ηt = β2ηt causes
the learning curves to collapse to the learning curve of the linearized model at early times (right), in
contrast to un-scaled curves (left). Models with large β follow linearized dynamics the longest.
2.3	Early learning timescale
We now define and compute the early learning timescale, τz, that measures the time it takes for the
logits to change significantly from their initial value. Specifically, we define τz such that for t τz
WeeXPeCtkZ(X,t)-Z(x, 0)||f《∣∣Z(x, 0)∣∣f and for t》Tz, ∣∣Z(x,t)-Z(x, 0)∣∣f 〜∣∣Z(x, 0)∣∣f
(or larger). This is synonymous with the timescale over which the model begins to learn. As we will
show below, Tz 8 ∣∣Z0∣∣f/η. Therefore in units of η, Tz only depends on ∣∣Z0∣∣f and not β.
To see this, note that at very short times it folloWs from Equation 5 that
K
Zi(x,t) - Zi(x, 0) ≈ η X(Θ)ij(x, X)(Yj- σ(Zj(X)))t + O(t2)	(6)
j=1
It follows that we can define a timescale over which the logits (on the training set) change appreciably
from their initial value as
Tz ≡ 1 -~笆业_____________.	⑺
η ∣Θ(X, X)(Y- σ(Z0))∣F
where the norms are once again taken across all classes as well as training points. This definition has
the desired properties for t	Tz and t	Tz .
In units of η, Tz depends only on ∣∣Z0∣∣f, in two ways. The first is a linear scaling in ∣∣Z0∣∣f; the
second comes from the contribution from the gradient ∣Θ(X, X)(Y - σ(Z(X, 0)))∣F. As previously
discussed, since σ(Z0) saturates at small and large values of ∣Z0∣F, it follows that the gradient term
will also saturate for large and small ∣Z0∣F, and the ratio of saturating values is some O(1) constant
independent of ∣Z0 ∣F and β.
O6
1
03
1
O1
1 F
∙z0
1 ----
ɜ
a
1
6
a
1
2
a
1
上¾i
—— 0 = l.OOe-O3
— ；3 = 1.00e-02
—— /3 = LOOe-Ol
——0 = LOOe+00
——B = LOOe+01
—— 0 = LOOe+02
— B = LOOe+03
ισβ ι<r1 ιo,
IIZ0IIf
Test a∞uracy vs. time, β= 1.00e- 02
AOBJnOOq
Test a∞uracy vs. time, β= 1.00e-02
O2
101
O0
x,
2
6
-1
.3
σ-∙
..1
4
6
-1
8-6-4-2-5
11111
-••a-
Ooooo
AOBJnOOV
H
“ 1
10≡
“ 1
,02t
1.1
101
O0
Figure 2: The timescale Tz depends only on ∣∣Z0∣∣f in units of η = β2η (left, inset). Tz depends
linearly on ∣Z0∣F, up to an O(1) coefficient which saturates at large and small ∣Z0∣F (left, main).
Accuracy increases more quickly for small initial ∣Z0 ∣F, though late time dynamics are similar
(center). Rescaling time to t/Tz causes early accuracy curves to collapse (right).
The quantitative and conceptual nature of Tz can both be confirmed numerically. We compute Tz
explicitly as ||[Zi(x, t) - Zi(x, 0)]/t|| for short times (Figure 2, left). When plotted over a wide
range of ∣Z0∣∣f and β, the ratio Tz/kZ0∣F (in rescaled time units) undergoes a saturating, O(1)
4
Under review as a conference paper at ICLR 2021
variation from small to large kZ0 kF . The quantitative dependence of the transition on the NTK
is confirmed in Appendix C. Additionally, for fixed β and varying ∣∣Z0∣∣f, rescaling time by 1∕τz
causes accuracy curves to collapse at early times (Figure 2, middle), even if they are very different at
early times without the rescaling (right). We note here that the late time accuracy curves seem similar
across kZ0kF without rescaling, a point which we will return to in Section 3.2.
2.4	Nonlinear timescale
While linearized dynamics are useful to understand some features of learning, the best performing
networks often reside in the nonlinear regime (Novak et al., 2019a). Here we define the nonlinear
timescale, τnl , corresponding to the time over which the network deviates appreciably from the
linearized equations. We will show that Tnl α β∕η. Therefore, in terms of β and ∣∣Z0∣∣f, networks
with small β will access the nonlinear regime early in learning, while networks with large β will
be effectively linearized throughout training. We note that a similar point was raised in Chizat et al.
(2019), primarily in the context of MSE loss.
We define τnl to be the timescale over which the change in Θθ (which contributes to the second order
term in Equation 6) can no longer be neglected. Examining the second time derivative of Z, we have
/ ∖
^dt2i = η X	Tθθ )ij (x, X ) dtσ(Z(X )) + dt [(S θ )ij (x, X )i (Yj- σ(Zj (X))	⑻
j=1 S---------------{z------------} S-------------------V------------------}
IineariZed dynamics	nonlinearized dynamics ≡ Zn
The first term is the second derivative under a fixed kernel, while the second term is due to the change
in the kernel (neglected in the linearized limit). A direct calculation shows that the second term,
which we denote Znl, can be written as
(Z nl)i= β-1η2 X X (Yk (X )-σ(Zk(X))T ( dz∂θX) ∙ ∂θ [Θ θ ]j (Yj (X )-σ(Zj (X)))
j=1 k=1
This gives us a nonlinear timescale Tnl defined, at initialization, by Tnl
kZ(X, 0)kF/kZnl(X, 0)kF.	We can interpret Tnl as the time it takes for changes in
(9)
the
kernel to contribute to learning.
Though computing kZnl (X, 0)kF in exactly is analytically intractable, its basic scaling in terms of β
and kZ0 kF (and therefore, that of Tnl) is computable. We first note the explicit β-1η2 dependence.
The remaining terms are independent ofβ and vary by at most O(1) with kZ0kF; indeed as described
above, kY(X) - σ(Z(X, 0))kF saturates for large and small kZ0kF. Morevoer, the derivative,
dz∂X,0), is the square root of the NTK and, at initialization, it is independent of kZ0∣∣F. Together
with our analysis of Tz we have that, up to some O(1) dependence on ∣∣Z0∣∣f, Tnl H β∕η. Therefore,
the degree of nonlinearity early in learning is controlled via β alone.
101
10°
wz5 iɑɜ ιo^ ιoτ
I∣z0∣∣f
Figure 3: The time to deviation from linearized dynamics, Tnl, has large deviation over β and kZ0kF
(left), which can be largely explained by linear dependence on β (right), in units of η = β2η. There
is an O(1) dependence on kZ0kF which is consistent across varying β for fixed kZ0kF.
5
Under review as a conference paper at ICLR 2021
Figure 4: Optimal learning rate η* for WRN on CIFAR10 scales as 1∕β.
Once again we can confirm the quantitative and conceptual understanding of τnl numerically. Qual-
itatively, we see that for fixed kZ0kF, models with smaller β deviate sooner from the linearized
dynamics when learning curves are plotted against ηt (Figure 1). We compute Tnl explicitly by taking
k [Z(t) - Zlin(t)]∕t2k as small times, where Zlin(t) is the solution to Equation 5. τnl/β has an O⑴
dependence on kZ0 kF only (Figure 3).
2.5 Learning rates and learning speed
The timescales, τz and τnl, can be combined to gain information about training with SGD. Indeed,
the largest allowable learning rate is controlled by the curvature of the loss function Du et al. (2019);
Allen-Zhu et al. (2019). A necessary condition for training is that the curvature be small compared
with the step size, which happens when τnl ≤ c, where c is an architecture-dependent constant of
O(1) which has not yet been calculated theoretically (see Lewkowycz et al. (2020) for empirical
calculations of c). This predicts a maximum effective learning rate η of O(β) which in turn implies a
raw learning rate of O(βT) (as η = η∕β2).
However, if η is O(β), then Tz is O(β-1). This means that, for early learning, networks with smaller
β will take more SGD steps to reach an appreciable change in logits. Therefore, we predict that
networks with small β take longer to train (which we see in practice as well).
3	Experimental Results
3.1	Optimal Learning Rate
We begin our empirical investigation by training wide resnets (Szegedy et al., 2016) without batch
normalization on CIFAR10, as this architecture is well within the regime where our theory applies.
In order to understand the effects of the different timescales on learning, we control β and kZ0kF
independently by using a correlated initialization strategy outlined in Appendix D.1.
Before considering model performance, it is first useful to understand the scaling of the learning
rate with β. We define the optimal learning rate η* as the learning rate with the best generalization
performance. To do this, we initialize networks with different β and conduct learning rate sweeps
for each β. The optimal learning rate η* has a clear 1∕β dependence (Figure 4). This matches the
prediction in Section 2.5, suggesting the maximum learning rate corresponds to the regime where the
non-linear effects become important at the fastest rate for which training still converges. Again, as
predicted, networks with smaller β also learn more slowly in terms of number of SGD steps.
Thus, at small β we expect learning to take place slowly and nonlinear effects to become important
by the time the function has changed appreciably. At large β, by contrast, our results suggest that the
network will have learned a significant amount before the dynamics become appreciably nonlinear.
6
Under review as a conference paper at ICLR 2021
3.2	Phase plane
In the preceding discussion two quantities emerged that control the behavior of early-time dynamics:
the inverse-temperature, β, and the rescaled logits ∣Z0 ∣F. In attempting to understand the behavior
of real neural networks trained using softmax-cross-entropy loss, it therefore makes sense to try to
reason about this behavior by considering neural networks that span the β - ∣Z0 ∣F phase plane,
the space of allowable pairs (β, ∣Z0 ∣F). By construction, the phase plane is characterized by the
timescales involved in early learning. To summarize, Tz 〜∣∣Z0∣∣F/η sets the timescale for early
learning, with larger values of ∣Z0 ∣F leading to longer time before significant accuracy gains are
made (Section 2.3). Meanwhile, τnl 〜β∕η controls the timescale for learning dynamics to leave the
linearized regime - with small β leading to immediate departures from linearity, while models with
large β may stay linearized throughout their learning trajectories (Section 2.4).
(a)
IO2
IO1
IO1
⅛ 10-°
¾ iθ-ɪ
一IO-2
10-4
10-5
O
io'-3 io'-2 lθ'-ɪ IO0 IO1
β
τnι<xβ∕η
Linearity
Less
1.00
Aoalnoo<
lδ2
ɪ ` 1
10
l10110110-010-110-≈10-410-sc %10110110-010-110-s10-410-sc
φ⅛oz= 9⅛oz=
ωdφlS ocd) XoBlnooq BbZ =
low' 6WEe3Mμew ，ast
M W F
Figure 5: Properties of early learning dynamics, which affect generalization, can be determined by
location in the β-∣∣Z0∣∣F phase plane (a). At optimal learning rate η*, small β and larger ∣∣Z0∣∣F
leads to slower early learning (b), and larger β increases time before nonlinear dynamics contributes
to learning. Large ∣Z0 ∣F has poorly conditioned linearized dynamics. Generalization for a wide
resnet trained on CIFAR10 is highly sensitive to β, and relatively insensitive to ∣Z0 ∣F outside poor
conditioning regime. Final logit variance is relatively insensitive to parameters (c).
In Figure 5 (a), we show a schematic of the phase plane. The colormap shows the test performance
of a wide residual network (Zagoruyko and Komodakis, 2017), without batch normalization, trained
on CIFAR10 in different parts of the phase plane. The value of β makes a large difference in
generalization, with optimal performance achieved at β ≈ 10-2. In general, larger β performed worse
than small β as expected. Moreover, we observe similar generalization for all sufficiently large β ; this
is to be expected since models in this regime are close to their linearization throughout training (see
Figure 1) and we expect the linearized models to have β-independent performance. Generalization
was largely insensitive to ∣Z0 ∣F so long as the network was sufficiently well-conditioned to be
trainable. This suggests that long term learning is insensitive to τz .
In Figure 5 (b), we plot the accuracy after 20 steps of optimization (with the optimal learning rate). For
fixed ∣Z0∣F, the training speed was slow for the smallest β and then became faster with increasing
β. For fixed β the training speed was fastest for small ∣Z0 ∣F and slowed as ∣Z0 ∣F increased. Both
these phenomena were predicted by our theory and shows that both parameters are important in
determining the early-time dynamics. However, we note that the relative accuracy across the phase
plane at early times did not correlate with the performance at late times.
This highlights that differences in generalization are a dynamical phenomenon. Another indication
of this fact is that at the end of training, at time tf , the final training set logit values ∣Zf ∣F ≡
∣Z(X , tf)∣F tend towards 1 independent of the initial β and ∣Z0 ∣F (Figure 5, (c)). With the
exception of the poorly-performing large ∣Z0 ∣F regime, the different models reach similar levels
of certainty by the end of training, despite having different generalization performances. Therefore
generalization is not well correlated with the final model certainty (a typical motivation for tuning β).
7
Under review as a conference paper at ICLR 2021
3.3	ARCHITECTURE DEPENDENCE OF THE OPTIMAL β
Having demonstrated that β controls the generalization performance of neural networks with softmax-
cross-entropy loss, we now discuss the question of choosing the optimal β . Here we investigate this
question through the lens of a number of different architectures. We find the optimal choice of β to
be strongly architecture dependent. Whether or not the optimal β can be predicted analytically is an
open question that we leave for future work. Nonetheless, we show that all architectures considered
display optimal β between approximately 10-2 and 101. We observe that by taking the time to tune
β it is often the case that performance can be improved over the naive setting of β = 1.
3.3.1	WIDE RESNET ON CIFAR10
(b)	Accuracy vs. β, max
(C)
(a)	A∞uracy vs. β, average
AOEJnOOq
A。EJnoO4
β
0 13 7 2
0 12 3 5
■ ♦ ♦ ♦ ■
Illll
A。EJnoO4
io" ιo3 ιo^2 1σ1 ιoo 101 102
β
Figure 6: Dependence of test accuracy for various architectures with β tuning. (a) For WRN with
batchnorm, trained on CIFAR10, the optimal β ≈ 10. Without batchnorm, the performance of the
network can be nearly recovered with β-scaling alone with β ≈ 10-2. Even poorly conditioned
networks (achieved by increasing weight scale σw) recover performance. (b) For β < 10-2, learning
is less stable, as evidenced by low average performance but high maximum performance (over 10
random seeds). (c) We see similar phenomenology on the IMDB sentiment analysis task trained with
GRUs - where average-case best performance is near β = 1 but peak performance is at small β .
In Figure 6 (a) we show the accuracy against β for several wide residual networks whose weights
are drawn from normal distributions of different variances, σw2 , trained without batchnorm, as well
as a network with σw2 = 1 trained with batchnorm (averaged over 10 seeds). The best average
performance is attained for β < 1, σw = 1 without batchnorm, and in particular networks with large
σw are dramatically improved with β tuning. The network with batchnorm is better at all β, with
optimal β ≈ 10. However, we see that the best performing seed is often at a lower β (Figure 6 (b)),
with larger σw networks competitive with σw = 1, and even with batchnorm at fixed β (though
batchnorm with β = 10 still performs the best). This suggests that small β can improve best case
performance, at the cost of stability. Our results emphasize the importance of tuning β, especially for
models that have not otherwise been optimized.
3.3.2	Resnet50 on ImageNet
Table 1: Accuracy on Imagenet dataset for ResNet-50. Tuning β significantly improves accuracy.
Method
ResNet-50 (Ghiasi et al., 2018)
ResNet-50 + Dropout (Ghiasi et al., 2018)
ResNet-50 + Label Smoothing (Ghiasi et al., 2018)
ResNet-50 + Temperature check (β = 0.3)
Accuracy (%)
76.51 ± 0.07
76.80 ± 0.04
77.17 ± 0.05
77.37 ± 0.02
Motivated by our results on CIFAR10, we experimentally explored the effects of β as a tunable
hyperparameter for ResNet-50 trained on Imagenet. We follow the experimental protocol established
by (Ghiasi et al., 2018). A key difference between this procedure and standard training is that we
train for substantially longer: the number of training epochs is increased from 90 to 270. Ghiasi et al.
(2018) found that this longer training regimen was beneficial when using additional regularization.
Table 1 shows that scaling β improves accuracy for ResNet-50 with batchnorm. However, we did not
8
Under review as a conference paper at ICLR 2021
find that using β < 1 was optimal for ResNet-50 without normalization. This further emphasizes the
subtle architecture dependence that warrants further study.
3.3.3 GRUs on IMDB Sentiment Analysis
To further explore the architecture dependence of optimal β, we train GRUs (from Maheswaranathan
et al. (2019)) whose weights are drawn from two different distributions on an IMDB sentiment
analysis task that has been widely studied (Maas et al., 2011). We plot the results in Figure 6 (c)
and observe that the results look qualitatively similar to the results on CIFAR10 without batch
normalization. We observe a peak performance near β 〜1 averaged over an ensemble of networks,
but we observe that smaller β can give better optimal performance at the expense of stability.
3.4 Proposed tuning procedure
Our results suggest the following tuning procedure for networks trained with SGD/momentum:
•	Train/tune a model as normal. Note the optimal learning rate η0 .
•	For the best parameter set, sweep over β ∈ [10-2,101], scaling learning rate as ηo∕β.
•	If best performing β is at an endpoint of the range, continue tuning.
If there is sufficient compute, the β search can instead be folded into the overall hyperparameter
tuning.
We note that our observation of optimal β < 1 in classification settings stands in contrast to the
observation of optimal β > 1 in the Bayesian inference setting (Wenzel et al., 2020). We speculate
that the differences are related to the fact that in Bayesian inference, the uncertainty of estimates are
important, and that the objective (learn an SDE which reproduces a target distribution) may have
qualitatively different properties than classification tasks.
4 Conclusions
Our empirical results show that tuning β can yield sometimes significant improvements to model
performance. Perhaps most surprisingly, we observe gains on ImageNet even with the highly-
optimized ResNet50 model. Our results on CIFAR10 suggest that the effect of β may be even
stronger in networks which are not yet highly-optimized, and results on IMDB show that this effect
holds beyond the image classification setting. It is possible that even more gains can be made by
more carefully tuning β jointly with other hyperparameters, in particular the learning rate schedule
and batch size.
One key lesson of our theoretical work is that properties of learning dynamics must be compared
using the right units. For example, Tnl a 1∕βη, which at first glance suggests that models with
smaller β will become nonlinear more slowly than their large β counterparts. However, analyzing
Tnl with respect to the effective learning rate η = β2η yields Tnl a β∕η. Thus we see that, in fact,
networks with smaller β tend to become more non-linearized before much learning has occurred,
compared to networks with large β which can remain in the linearized regime throughout training.
Our numerical results confirm this intuition developed using the theoretical analysis.
As discussed above, our analysis suggests a range of good β, but does not predict the optimal value.
Architecture-dependent, non-scaling multiplicative factors to key learning parameters have been
observed in other contexts (Lewkowycz et al., 2020), and their numerical estimation is in general
difficult. Extending the theoretical results to make predictions about these quantities is an interesting
avenue for future work. Another area that warrants further study is the instability in training at small
β.
9
Under review as a conference paper at ICLR 2021
References
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.
Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation, 1(4):
541-551, December 1989. ISSN 0899-7667. doi: 10.1162∕neco.1989.1.4.541.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep
Convolutional Neural Networks. In Advances in Neural Information Processing Systems 25, pages
1097-1105. Curran Associates, Inc., 2012.
Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. arXiv:1605.07146 [cs], June
2017.
Douglas M. Kline and Victor L. Berardi. Revisiting squared-error and cross-entropy functions for
training neural network classifiers. Neural Computing & Applications, 14(4):310-318, December
2005. ISSN 1433-3058. doi: 10.1007/s00521-005-0467-y.
Pavel Golik, Patrick Doetsch, and Hermann Ney. Cross-Entropy vs. Squared Error Training: A
Theoretical and Experimental Comparison. In Interspeech, volume 13, pages 1756-1760, August
2013.
Gabriel Pereyra, George Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey Hinton. Regularizing
Neural Networks by Penalizing Confident Output Distributions. arXiv:1701.06548 [cs], January
2017.
Rafael Muller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? In
Advances in Neural Information Processing Systems 32, pages 4694-4703. Curran Associates, Inc.,
2019.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the Inception Architecture for Computer Vision. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2818-2826, 2016.
Zhirong Wu, Alexei A. Efros, and Stella X. Yu. Improving Generalization via Scalable Neighborhood
Component Analysis. arXiv:1808.04699 [cs], August 2018.
Andrew Zhai and Hao-Yu Wu. Classification is a Strong Baseline for Deep Metric Learning.
arXiv:1811.12649 [cs], August 2019.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network.
arXiv:1503.02531 [cs, stat], March 2015.
John Platt. Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized
Likelihood Methods. Adv. Large Margin Classif., 10, June 2000.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70,
ICML’17, pages 1321-1330, Sydney, NSW, Australia, August 2017. JMLR.org.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. In Advances in Neural Information Processing Systems 31,
pages 8571-8580. Curran Associates, Inc., 2018.
Simon S. Du, Xiyu Zhai, Barnabgs P6czos, and Aarti Singh. Gradient Descent Provably Optimizes
Over-parameterized Neural Networks. In 7th International Conference on Learning Representa-
tions, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide Neural Networks of Any Depth Evolve as Linear Models
Under Gradient Descent. In Advances in Neural Information Processing Systems 32, pages
8570-8581. Curran Associates, Inc., 2019.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian Deep Convolutional Networks with
Many Channels are Gaussian Processes. arXiv:1810.05148 [cs, stat], February 2019a.
10
Under review as a conference paper at ICLR 2021
Lechao Xiao, Jeffrey Pennington, and Samuel S. Schoenholz. Disentangling trainability and general-
ization in deep learning. arXiv:1912.13053 [cs, stat], December 2019.
Lenaic Chizat and Francis Bach. Implicit Bias of Gradient Descent for Wide Two-layer Neural
Networks Trained with the Logistic Loss. arXiv:2002.04486 [cs, math, stat], March 2020.
Laurence Aitchison. Why bigger is not always better: On finite and infinite neural networks.
arXiv:1910.08013 [cs, stat], November 2019.
Lenaic ChizaL Edouard Oyallon, and Francis Bach. On Lazy Training in Differentiable Programming.
In Advances in Neural Information Processing Systems 32, pages 2937-2947. Curran Associates,
Inc., 2019.
Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite Versus Infinite Neural Networks: An Empirical Study. July
2020.
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: The catapult mechanism. March 2020.
A. Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Master’s thesis, University
of Toronto, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 770-778, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
Recognition, pages 248-255, June 2009. doi: 10.1109/CVPR.2009.5206848.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical Evaluation of
Gated Recurrent Neural Networks on Sequence Modeling. arXiv:1412.3555 [cs], December 2014.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies - Volume 1,
HLT ’11, pages 142-150, USA, June 2011. Association for Computational Linguistics. ISBN
978-1-932432-87-9.
Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift. February 2015.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning via
Over-Parameterization. In International Conference on Machine Learning, pages 242-252. PMLR,
May 2019.
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. DropBlock: A regularization method for convolutional
networks. In Advances in Neural Information Processing Systems 31, pages 10727-10737. Curran
Associates, Inc., 2018.
Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. Reverse
engineering recurrent networks for sentiment classification reveals line attractor dynamics. In
Advances in Neural Information Processing Systems 32, pages 15696-15705. Curran Associates,
Inc., 2019.
Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How Good is the Bayes
Posterior in Deep Neural Networks Really? arXiv:2002.02405 [cs, stat], July 2020.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclau-
rin, and Skye Wanderman-Milne. JAX: Composable transformations of Python+NumPy programs,
2018.
11
Under review as a conference paper at ICLR 2021
Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural Tangents: Fast and Easy Infinite Neural Networks in Python.
arXiv:1912.02803 [cs, stat], December 2019b.
12
Under review as a conference paper at ICLR 2021
A Linearized learning dynamics
A.1 Fixed points
For the linearized learning dynamics, the trajectory z(x, t) can be written in terms of the trajectories
of the training set as
z(x,t) — z(x, 0) = Θ(x, X )Θ +(X, X )(z(X ,t) — Z(X, 0))	(10)
where + is the pseudo-inverse. Therefore, if one can solve for z(X, t), then in principle properties of
generalization are computable.
However, in general Equation 3 does not admit an analytic solution even for fixed Θ, in contrast to
the case of mean squared loss. It not even have an equilibrium - if the model can achieve perfect
training accuracy, the logits will grow indefinitely. However, there is a guaranteed fixed point if the
appropriate L? regularization is added to the training objective. Given a regularizer 2λθ ∣∣δθ∣∣2 on the
change in parameters δθ = θ(t) - θ(0), the dynamics in the linearized regime are given by
,. ʌ , , . ... ,
Z(x) = βηΘ(x, X)(Y(X) - σ(βz(X))) - λθδz(x)	(11)
where the last term comes from the fact that 磊δθ = δz(x) in the linearized limit.
We can write down self-consistent equations for equilibria, which are approximately solvable in
certain limits. For an arbitrary input x, the equilibrium solution z*(x) is
0 = βΘ(x, X)(Y(X) — σ(βz*(X))) — λθδz*(x)	(12)
This can be rewritten in terms of the training set as
δz*(x) = Θ(x, X)Θ +(X, X)z*(X)	(13)
similar to kernel learning.
It remains then to solve for Z (X). We have:
β
δz*(X) = ɪΘ(X, X)[Y(X) - σ(βz*(X))]	(14)
λθ
We immediately note that the solution depends on the initialization. We assume z(x, 0) = 0, so
β
1 + 念Θ(X, X)
z*(X ) = ɪ
λθ
δz = Z in order to simplify the analysis. The easiest case to analyze is when ∣∣βz*(X )∣∣f《1. Then
we have:
β1
z*(X) = JeΘ(X, X) Y(X)-天(1 + βz*(X))	(15)
which gives us
-1
ʌ , ,. .
Θ(X, X)(Y(X) - 1/K)	(16)
Therefore the self-consistency condition for this solution is ∣∣ 聂Θ∣∣f《1, which simplifies the
solution to
β
z*(X) = FΘ(X, X)(Y(X) - 1/K)	(17)
λθ
This is equivalent to the solution after a single step of (full-batch) SGD with appropriate learning rate.
We note that unlike linearized dynamics with L2 loss and a full-rank kernel, there is no guarantee
that the solution converges to 0 training error.
The other natural limit is ∣βz*(X)∣2	1. We focus on the 2 class case, in order to take advantage
of the conserved quantity of learning with cross-entropy loss. We note that the vector on the right
hand side of Equation 3 sums to 1 for every training point. Suppose at initialization, Θθ has no
logit-logit interactions, as is the case for most architectures in the infinite width limit with random
initialization. More formally, we can write Θθ = Idκ×κ 0 Θχ where Θχ is M X M. Then, the
sum of the logits for any input x is conserved during linearized training, as we have:
1Tz(x) = ηβ1T [ldκ×κ 0 Θχ] (Y - σ(βz(X)))	(18)
13
Under review as a conference paper at ICLR 2021
Multiplying the right hand side through, we get
ITz(X) = ηβθχ [1T(Y - σ(βz(X)))] = 0	(19)
(Note that if Θθ has explicit dependence on the logits, there still is a conserved quantity, which is
more complicated to compute.)
Now We can analyze ∣∣βz*(X)∣∣f》1. With two classes, and z(X) = 0 at initialization, We have
z↑ = -zg. Therefore, without loss of generality, we focus on z；, the logit of the first class. In this
limit, the leading order correction to the softmax is approximately:
σ(βz1) ≈ 1z：＞o - sign(z；)e-2e|zi1	(20)
The self-consistency equation is then:
z；(X) =	Θ(X, X) [Y(X) — 1z»0 + Sign(Z"e-2βlz"i	(21)
The vector on the right hand side has entries that are O(e-2βlz") for correct classifications, and O(1)
for incorrect ones. If we assume that the training error is 0, then we have:
z1(X) = ɪΘ(X, X)sign(z；)e-2e|z"	(22)
λθ
This is still non-trivial to solve, but we see that the self consistency condition is that
. . ʌ . .
ln(β∣∣Θ∣∣F/λθ)》1.
Here also it may be difficult to train and generalize well. The individual elements of the right-hand-
side vector are broadly distributed due to the exponential - so the outputs of the model are sensitive
to/may only depend on a small number of datapoints. Even if the equilibrium solution has no training
loss, generalization error may be high for the same reasons.
This suggests that even for NTK learning (with L? regularization), the scale of ∣∣βz∣∣ plays an
important role in both good training accuracy and good generalization. In the NTK regime, there is
one unique solution so (in the continuous time limit) the initialization doesn’t matter; rather, the ratio
of β and λθ (compared to the appropriate norm of Θ) needs to be balanced to prevent falling into the
small βz regime (where training error might be large) or the large βz regime (where a few datapoints
might dominate and reduce generalization).
A.2 Dynamics near equilibrium
The dynamics near the equilibrium can be analyzed by expanding around the fixed point equation.
We focus on the dynamics on the training set. The dynamics of the difference Z(X) = z(X) 一 z；(X)
for small perturbations is given by
Z(X) = -η [β2[Idz 乳 Θ(X, X)]σz(βz*(X)) + λθi z(X)	(23)
where σz is the derivative of the softmax matrix
σz(z) ≡ dσ(z) = diag(σ(z)) 一 σ(z)σ(z0)τ	(24)
∂z0
We can perform some analysis in the large and small β cases (once again ignoring λz). For small
∣∣βz*(X)∣f, we have ∣∣-βΘ∣∣f《1 which leads to:
σz(βz*(X)) = (1/K - 11T/K2) + O(βΘ)	(25)
This matrix has K - 1 eigenvalues with value 1/K, and one zero eigenvalue (corresponding to
the conservation of probability). Therefore ∣β2[Idz 0 Θ(X, X)]σz(βz;(X))∣f《λθ, and the
well-conditioned regularizer dominates the approach to equilibrium.
In the large β case (ln(β∣∣Θ∣∣∕λθ)》1), the values of σ(βz(X)) are exponentially close to 0
(K - 1 values) or 1 (the value corresponding to the largest logit). This means that σz (βz(X)) has
exponentially small values in ∣β z(X)∣F - if any one of σ(βzi(X)) and σ(βzj (X)) is exponentially
14
Under review as a conference paper at ICLR 2021
small, the corresponding element of σz (βz(X)) is as well; for the largest logit i the diagonal is
σ(βzi (X))(1 - σ(βzi (X))) which is also exponentially small.
From Equation 22, We have λθ《β2e2βlzιl; therefore, though the σ% term of H is exponentially
small, it dominates the linearized dynamics near the fixed point, and the approach to equilibrium is
sloW. We Will analyze the conditioning of the dynamics in the remainder of this section.
A.3 Conditioning of dynamics
Understanding the conditioning of the linearized dynamics requires understanding the spectrum of
the Hessian matrix H = (IdZ 0 ΘΘ(X, X)) σ%(βz*(X)). In the limit of large model size, the first
factor is block-diagonal With training set by training set blocks (no logit-logit interactions), and the
second term is block-diagonal With K × K blocks (no datapoint-datapoint interactions).
We Will use the folloWing lemma to get bounds on the conditioning:
Lemma: Let M = AB be a matrix that is the product of tWo matrices. The condition number
K(M) ≡ λM,max has bound
λM,min
κ(B)∕κ(A) ≤ K(M) ≤ K(A)K(B)	(26)
Proof: Consider the vector v that is the eigenvector of B associated With λB,min. Note
that ||AV||/||v|| ≤ λA,max. Analogously, for w, the eigenvector associated With λB,max,
||Aw||/||w|| ≥ λA,min. This gives us the tWo bounds:
λM,min ≤ λA,maxλB,min, λM,max ≥ λA,minλB,max
This means that the condition number κ(H) ≡ λM,max is bounded by
K(M) ≥ ：A，max：B，min = κ(B)∕κ(A)
λA,min λB,max
(27)
(28)
In total, We have the bound of Equation 26, Where the upper bound is trivial to prove.
In particular, this means that a poorly conditioned σZ(βz*(X)) will lead to poor conditioning of
the linearized dynamics if the NTK Θ(X, X) is (relatively) well conditioned. This bound will be
important in establishing the poor conditioning of the linearized dynamics for the large logit regime
I∣βz∣∣》L
A.3.1 Small logit conditioning
For ∣∣βz*(X)∣∣f《1, the Hessian H is
H =i(l - ɪ 11t) 0 ΘΘ(X, X)	(29)
KK
Since H is the Kroenecker product of two matrices, the condition numbers multiply, and we have
___________________________________	, O .
K(H) = k(Θ)	(30)
which is well-conditioned so long as the NTK is. Regardless, the well-conditioned regularization due
to λθ dominates the approach to equilibrium.
A.3.2 Large logit conditioning
Now consider ∣∣βz*(X)∣f》1. Here we will show that the linearized dynamics is poorly condi-
tioned, and that K(H) is exponentially large in β.
We first try to understand σZ(βz*(x)) for an individual X ∈ X. To 0th order (in an as-of-yet-
undefined expansion), σz is zero - at large temperature the softmax returns either 0 or 1, which by
Equation 24 gives 0 in all entries. The size of the corrections end up being exponentially dependent
on ∣∣βz*∣f; the entries will have broad, log-normal type distributions with magnitudes which scale
15
Under review as a conference paper at ICLR 2021
as exp(一β∣z; |). There will be two scaling regimes one with a small number of labels in the sense
√β》y4n(K), where the largest logit dominates the statistics, and one where the number of labels
is large (and the central limit theorem applies to the partition function). In both cases, however, there
is still exponential dependence on β ; we will focus on the first which is easier to analyze and more
realistic (e.g. for 106 labels “large” β is only 〜15).
Let z1 be the largest of K logits, z2 the second largest, and so on. Then using Equation 24 we have:
(σz )ii = -e-β(ZLzi)	(31)
for i 6= 1,
(σz)ij = δije-β(z1-zi) 一 e-β(2z1-zi-zj)	(32)
for i 6= j and
(σz)11 = e-β(z1-z2)	(33)
The eigenvectors and eigenvalues can be approximately computed as:
(v1)1	=√2,"	= -√2, λ1	= 2e-β(z1-z2)	(34)
(v2)1 =	二 √2, (v2)2 =	W, λ2 =	一 1 e-2β(zι-z2) 2	(35)
and for i > 2,
(vi)i = 1, (vi)1 = e-β(z2-zi), λi = e-β(z1-zi)	(36)
with all non-explicit eigenvector components 0. This expansion is valid provided that β∕K>1 (so
that eβ(z1 -zi)	eβ(z1 -zi+1)).
Therefore the spectrum of any individual block σz (βz(x)) is exponentially small in β. Using the
bound in the Lemma, we have:
K(H) ≥ eO(eWI)∕κ(Θ(X, X))	(37)
This is a very loose bound, as it assumes that the largest eigendirections of σz are aligned with the
smallest eigendirections of IdZ 0 ΘΘ, and vice versa. It is possible K(H) is closer in magnitude to the
upper bound eβ(z2-zκ)κ(ΘΘ(X, X)).
Regardless, K(H) is exponentially large in β - meaning that the conditioning is exponentially poor
for large kβz*∣∣F.
B SGD and momentum res calings
B.1 Discrete equations
Consider full-batch SGD training. The update equations for the parameters θ are:
θt+ι = θt- ηVθL	(38)
We will denote gt ≡ Vθ L for ease of notation.
Training with momentum, the equations of motion are given by:
vt+1 = (1 一 γ)vt 一 gt	(39)
θt+1 = θt + ηvt+1	(40)
where γ ∈ [0, 1].
One key point to consider later will be the relative magnitude ∆θ of updates to the parameters. For
SGD, the magnitude of updates is η∣∣g∣∣. For momentum with slowly-varying gradients the magnitude
is n||g||/Y.
16
Under review as a conference paper at ICLR 2021
B.2 Continuous time equations
We can write down the continuous time version of the learning dynamics as follows. For SGD, for
small learning rates We have:
dθ
正二Fg
(41)
For the momentum equations we have
dv
加=-Yv- g
(42)
dθ
加=ηv
(43)
From these equations, We can see that in the continuous time limit, there are coordinate transforma-
tions Which can be used to cause sets of trajectories With different parameters to collapse to a single
trajectory. SGD is the simplest, Where rescaling time to τ ≡ ηt causes learning curves to be identical
for all learning rates.
For momentum, instead of a single universal learning curve, there is a one-parameter family of curves
controlled by the ratio Tmom ≡ η∕γ2. Consider rescaling time to T = at and V = bv, where a and b
Will be chosen to put the equations in a canonical form. In our neW coordinates, We have
dν =-(Y/a)V - (b/a)g
(44)
The canonical form we choose is
dθ=ην/(ab)
(45)
dν
= ■
dτ
dθ
dτ
-λν - g
(46)
ν
(47)
From which we arrive at a = b = √η, which gives us λ = γ∕√η.
Note that this is not a unique canonical form; for example, if We fix a coefficient of -1 on V, We end
up with
dν = -V - (n/Y2)g
dT
dθ
dT = V
with a = γ. This is a different time rescaling, but still controlled by Tmom .
(48)
(49)
Working in the canonical form of Equations 46 and 47, we can analyze the dynamics. One immediate
question is the difference between λ 1 and λ 1. We note that the integral equation
V(T) = V(0) +	e-λ(τ-τ0)g(T0)dT0
0
(50)
solves the differential equation for ν. Therefore, for λ	1, ν(t) only depends on the current value
g(t) and We have V(T) ≈ g(τ)∕λ. Therefore, We have, approximately:
dθ〜1
dτ λ g
(51)
This means that for large λ all the curves Will approximately collapse, With timescale given by
√ηλ-1 = γn (dynamics similar to SGD).
For λ	1, the momentum is essentially the integrated gradient across all time. If V(0) = 0, then We
have
dθ ≈ Z g(τ0)dτ0
dT 0
(52)
17
Under review as a conference paper at ICLR 2021
In this limit, θ(τ) is the double integral of the gradient with respect to time.
Given the form of the controlling parameter Tm,°m, We can choose to parameterize Y = y√η. Under
this parameterization, we have TmOm = Y2. The dynamical equations then become:
dν
dτ = -γν - g
(53)
dθ
dT = V
(54)
which automatically removes explicit dependence on η.
One particular regime of interest is the early-time dynamics, starting from ν(0) = 0. Integrating
directly, we have:
θ(τ ) = - 1 gτ2 + 1 YgT + ...	(55)
26
This means that T alone is the correct timescale for early learning, at least until τγ 〜1 - which in
the original parameters corresponds to t 〜1∕γ (the time it takes for the momentum to be first “fully
integrated”).
B.3	Detailed analysis of momentum timescales
One important subtlety is that 1 is not the correct value to compare λ to. The real timescale involved
is the one over which g changes significantly. We can approximate this in the following way. Suppose
that there is some relative change ∆∣ 〜C Ofthe parameters that leads to an appreciable relative
change in g. Then the timescale over which θ changes by that amount is the one we must compare λ
to.
We can compute that timescale in the following way. We assume g fixed for what follows. Therefore,
Equation 51 approximately holds. The timescale Tc of the change is then given by:
∆θ	1 ||g||
	=--------TC 〜C
l∣θ∣l---------------λ ∣∣Θ∣∣ c
(56)
which gives
Tc〜两网㈤回1	(57)
In particular, this means that the approximation is good when λτc》1, which gives Y2/η》||g|j -
the former being a function of the dynamical parameters, the latter being a function of the geometry
of L with respect to θ.
One consequence of this analysis is that if the ∣∣θ∣∣ remains roughly constant, for fixed η and γ, late
in learning when the gradients become small the dynamics shifts into the regime where λ is large,
and we effectively have SGD.
B.4	Connecting discrete and continuous time
One use for the form of the continuous time rescalings is to use them to compare learning curves for
the actual discrete optimization that is performed with different learning rates. For small learning
rates, the curves are expected to collapse, while for larger learning rates the deviations from the
continuous expectation can be informative.
With momentum, we only have perfect collapse when Y and η are scaled together. However, one
typical use case for momentum is to fix the parameter Y, and sweep through different learning rates.
With this setup, if g is changing slowly compared to Y (moreprecisely, γ2∕η》∣∣g∣∣∕∣∣θ∣∣), as may
be the case at later training times, the change in parameters from a single step is ∆θ 〜 (n/Y))||g||
and the rescaling of taking t to ηt (as for SGD) collapses the dynamics. Therefore given a collection
of varying η, but fixed Y curves, it is possible to get intermediate and late time dynamics on the same
scale.
However, at early times, while the momentum is still getting “up to speed" (i.e. in the first 1/Y steps),
the appropriate timescale is η-1/2. Therefore, in order to get learning curves to collapse across
18
Under review as a conference paper at ICLR 2021
different η at early times, We need to rescale Y with η as implied by Equations 46 and 47. Namely,
one must fix Y and rescale Y = ^√η. We note that, since γ < 1, this gives us a restriction η < 7-2
for the maximum learning that can be supported by the rescaled momentum parameter.
B.5	Momentum equations with softmax-cross-entropy learning
For cross-entropy learning with softmax inputs βz, all the scales acquire dependence on β . If we
define `z ≡ IldezIlandgz ≡ Ildz∣∣F, then Wehave, approximately, ∣∣g∣∣≈ β'zgz.
Consider the goal of obtaining identical early-time learning curves for different values of β . (The
curves are only globally consistent across β in the linearized regime.) In order to get learning curves
to collapse, we want dL to be independent of β in the rescaled time units. We note that the change in
the loss function ∆L from a single step of SGD goes as
△l 〜ηβ2 '2g
(58)
This suggests that one way to collapse learning curves is to plot them against the rescaled learning
rate ηt, where η = ηβ2. While hyperparameter tuning across β, one could use η = η∕β2, sweeping
over η7 in order to easily obtain comparable learning curves.
However, a better goal for a learning rate rescaling is to try and stay within the continuous time limit -
that is, to control the change in parameters △ for a single step to be small across β. We have
∆θ 〜ηβ'z gz
(59)
which suggests that maximum allowable learning rates will scale as 1∕β. This suggests setting
η = ηβ-1, and rescaling time as ηβ in order to best explore the continuous learning dynamics.
We can perform a similar analysis for the momentum optimizers. We begin by analyzing the
continuous time equations for the dynamics of the loss. Starting with the rescalings from Equations
53 and 54 we have
	dν = -YV - βg	(60) dτ dθ 丁= V	(61) dτ dL = βg ∙ V	(62) dτ
where g =赢磊.Rescaling T by β gets us:
	dV	Y7 后=- βν - g	(63) dθ	1 =-=万V	(64) dβτ β dL	,g =g = g ∙ v	(65) dβτ
This rescaling causes a collapse of the trajectories of the L at early times if Y7∕β is constant for
varying β .
One scheme to arrive at the final canonical form, across β, is by the following definitions of η, Y, and
τ:
•	η = η7β-2
•	γ ≡ β√η7 = √77
•	τ ≡ β√ξt = √ηjt
19
Under review as a conference paper at ICLR 2021
where curves with fixed Y will collapse. The latter two equations are similar to before, except with η
replaced with η. The dynamical equations are then:
dν
dτ = -γν - g
dθ	1
--=—ν
dτ β
dL
dτ
g ∙ V
The change in parameters from a single step (assuming constant g and saturation) is
(66)
(67)
(68)
(69)
X = iF
If we instead want the change in parameters from a single step to be invariant of β so the continuous
time approximation holds, while maintaining collapse of trajectories, we first note that
δΘ 〜~^Γ~β'zgz	(70)
Y
from a single step of the momentum optimizer. To keep ∆θ invariant of β, we can set:
•	η = ηβ-1
•	γ ≡ β√η5 = √^7 = β1/2 √^Y
•	T ≡ β√ηt = β`∕2√t
Note that the relationship between Y and Y is the same in both schemes when measured with respect
to the raw learning rate η .
C	Softmax-cross-entropy gradient magnitude
C.1 Magnitude of gradients in fully-connected networks
The value of Tz has nontrivial (but bounded) dependence on kZ0kf via the ∣∣ΘΘ(Y - σ(Z0(X)))kf
term in Equation 7. We can confirm the dependence for highly overparameterized models by using
the theoretical ΘΘ. In particular, for wide neural networks, the tangent kernel is block-diagonal in the
logits, and easily computable.
The numerically computed Tz/∣Z0∣f correlates well with ∣∣ΘΘ(Y 一 σ(Z0(X)))∣∣-1 for wide (2000
hidden units/layer) fully connected networks (Figure 7). The ratio depends on details like the
nonlinearities in the network; for example, Relu units tend to have a larger ratio than Erf units (left
and middle). The ratio also depends on the properties of the dataset. For example, the ratio increases
on CIFAR10 when the training labels are randomly shuffled (right).
Therefore in general the ratio of Tz/∣Z0∣f at large and small ∣∣Z0∣∣f depends subtly on the relation-
ship between the NTK and the properties of the data distribution. A full analysis of this relationship
is beyond the scope of this work. The exact form of the transition is likely even more sensitive to
these properties and is therefore harder to analyze than the ratio alone.
D Experimental details
D. 1 Correlated initialization
In order to avoid confounding effects of changing β and ∣∣Z0∣∣f with changes to ΘΘ, we use a
correlated initialization strategy (similar to (Chizat et al., 2019)) which fixes ΘΘ while allowing for
independent variation of β and ∣Z0∣F. Given a network with final hidden layer h(x, θ) and output
weights WO, we define a combined network zc(x, θ) explicitly as
zc(x, WO,1, WO,2, θ1, θ2) = WO,1h(x, θ1) + WO,2h(x, θ2)	(71)
20
Under review as a conference paper at ICLR 2021
Normalized τz, Erf (shuffled Iabe⑸
Figure 7: Tz/kZ0∣∣F is highly correlated with ∣∣ΘΘ(Y - σ(Z0))k-1, with ΘΘ computed in the infinite
width limit (in units of effective learning rate η = β2η). Ratio between normalized timescales at
large and small ∣Z0 ∣F depends on nonlinearity (left and middle), as well as training set statistics
(right, CIFAR10 with shuffled labels).
where, at initialization, θ1 = θ2, and the elements of WO,a have statistics
E[(WO,a)ij(WO,b)kl] = δikδjl	fora=b	(72)
, 八,	Ic ∙ δikδji	for a = b
for correlation coefficient c ∈ [-1, 1], where δij is the Kronecker-delta which is 1 is i = j
and 0 otherwise. Under this approach, the initial magnitude of the training set logits is given
by ∣∣Z0∣∣f = β∙∖∕(1 + c)∣∣z0∣∣F, where ∣∣z0∣∣f is the initial magnitude of the logits of the base
model. By manipulating β and c, we can independently change β and ∣Z0 ∣F with the caveat that
∣∣Z0∣∣f ≤ √2βkz0∣F since c ≤ 1. It follows that the small β, large ∣∣Z0∣∣f region of the phase
plane (upper left in Figure 5) is inaccessible with most well-conditioned models where ∣∣z0∣∣f 〜1 at
initialization. If we only train one set of weights, the ΘΘ is independent of c.
Unless otherwise noted, all empirical studies in Sections 2 and 3.2 involve training a wide resnet
on CIFAR10 with SGD, using GPUs, using the above correlated initialization strategy to fix ΘΘ. All
experiments used JAX (Bradbury et al., 2018) and experiments involving linearization or direct
computation of the NTK used Neural Tangents (Novak et al., 2019b).
21