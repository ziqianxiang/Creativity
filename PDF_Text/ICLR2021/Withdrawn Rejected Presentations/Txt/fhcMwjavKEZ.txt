Under review as a conference paper at ICLR 2021
A Simple and General Graph Neural Network
with Stochastic Message Passing
Anonymous authors
Paper under double-blind review
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
Ab stract
Graph neural networks (GNNs) are emerging machine learning models on graphs.
One key property behind the expressiveness of existing GNNs is that the learned
node representations are permutation-equivariant. Though being a desirable prop-
erty for certain tasks, however, permutation-equivariance prevents GNNs from
being proximity-aware, i.e., preserving the walk-based proximities between pairs
of nodes, which is another critical property for graph analytical tasks. On
the other hand, some variants of GNNs are proposed to preserve node prox-
imities, but they fail to maintain permutation-equivariance. How to empower
GNNs to be proximity-aware while maintaining permutation-equivariance re-
mains an open problem. In this paper, we propose Stochastic Message Passing
(SMP), a general and simple GNN to maintain both proximity-awareness and
permutation-equivariance properties. Specifically, we augment the existing GNNs
with stochastic node representations learned to preserve node proximities. Though
seemingly simple, we prove that such a mechanism can enable GNNs to preserve
node proximities in theory while maintaining permutation-equivariance with cer-
tain parametrization. Extensive experimental results demonstrate the effectiveness
and efficiency of SMP for tasks including node classification and link prediction.
1	Introduction
Graph neural networks (GNNs), as generalizations of neural networks in analyzing graphs, have
attracted considerable research attention. GNNs have been widely applied to various applications
such as social recommendation (Ma et al., 2019), physical simulation (Kipf et al., 2018), and protein
interaction prediction (Zitnik & Leskovec, 2017).
One key property of most existing GNNs is permutation-equivariance, i.e., if we randomly permu-
tate the IDs of nodes while maintaining the graph structure, the representations of nodes in GNNs
are permutated accordingly. Mathematically, permutation-equivariance reflects one basic symmet-
ric group of graph structures. Although it is a desirable property for tasks such as node or graph
classification (Keriven & Peyre, 2019; Maron et al., 2019b), PermUtation-equivariance also prevents
GNNs from being proximity-aware, i.e., permutation-equivariant GNNs cannot preserve walk-based
proximities between nodes such as the shortest distance or high-order proximities (see Theorem 1).
Pairwise proximities between nodes are crucial for graph analytical tasks such as link predic-
tion (Hu et al., 2020; You et al., 2019). To enable a proximity-aware GNN, Position-aware GNN
(P-GNN) (You et al., 2019)1 proposes a sophisticated GNN architecture and shows better perfor-
mance for proximity-aware tasks. But P-GNN needs to explicitly calculate the shortest distance be-
tween nodes and its computational complexity is unaffordable for large graphs. Moreover, P-GNN
completely ignores the permutation-equivariance property. Therefore, it cannot produce satisfactory
results when permutation-equivariance is helpful.
In real-world scenarios, both proximity-awareness and permutation-equivariance are indispensable
properties for GNNs. Firstly, different tasks may require different properties. For example, recom-
mendation applications usually require the model to be proximity-aware (Konstas et al., 2009) while
permutation-equivariance is a basic assumption in centrality measurements (Borgatti, 2005). Even
1In (You et al., 2019), the authors consider the special case of shortest distance between nodes and name
such property as “position-aware”. In this paper, we consider a more general case of any walk-based proximity.
1
Under review as a conference paper at ICLR 2021
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
for the same task, different datasets may have different requirements on these two properties. Taking
link prediction as an example, we observe that permutation-equivariant GNNs such as GCN (Kipf &
Welling, 2017) or GAT (Velickovic et al., 2018) show better results than P-GNN in coauthor graphs,
but the opposite in biological graphs (please see Section 5.2 for details). Unfortunately, in the current
GNN frameworks, these two properties are contradicting, as we show in Theorem 1. Whether there
exists a general GNN to be proximity-aware while maintaining permutation-equivariance remains
an open problem.
In this paper, we propose Stochastic Message Passing (SMP), a general and simple GNN to pre-
serve both proximity-awareness and permutation-equivariance properties. Specifically, we augment
the existing GNNs with stochastic node representations learned to preserve proximities. Though
seemingly simple, we prove that our proposed SMP can enable GNNs to preserve walk-based prox-
imities in theory (see Theorem 2 and Theorem 3). Meanwhile, SMP is equivalent to a permutation-
equivariant GNN with certain parametrization and thus is at least as powerful as those GNNs in
permutation-equivariant tasks (see Remark 1). Therefore, SMP is general and flexible in handling
both proximity-aware and permutation-equivariant tasks, which is also demonstrated by our exten-
sive experimental results. Besides, owing to the simple structure, SMP is computationally efficient,
with a running time roughly the same as those of the most simple GNNs such as SGC (Wu et al.,
2019) and is at least an order of magnitude faster than P-GNN on large graphs. Ablation studies
further show that a linear instantiation of SMP is expressive enough as adding extra non-linearities
does not lift the performance of SMP on the majority of datasets. Our contributions are as follows.
•	We propose SMP, a simple and general GNN to handle both proximity-aware and permutation-
equivariant graph analytical tasks.
•	We prove that SMP has theoretical guarantees in preserving walk-based proximities and is at
least as powerful as the existing GNNs in permutation-equivariant tasks.
•	Extensive experimental results demonstrate the effectiveness and efficiency of SMP. We show
that a linear instantiation of SMP is expressive enough on the majority of datasets.
2	Related Work
We briefly review GNNs and their permutation-equivariance and proximity-awareness property.
The earliest GNNs adopt a recursive definition of node states (Scarselli et al., 2008; Gori et al.,
2005) or a contextual realization (Micheli, 2009). GGS-NNs (Li et al., 2016) replace the recursive
definition with recurrent neural networks (RNNs). Spectral GCNs (Bruna et al., 2014) defined graph
convolutions using graph signal processing (Shuman et al., 2013; Ortega et al., 2018) with Cheb-
Net (Defferrard et al., 2016) and GCN (Kipf & Welling, 2017) approximating the spectral filters us-
ing a K-order Chebyshev polynomial and the first-order polynomial, respectively. MPNNs (Gilmer
et al., 2017), GraphSAGE (Hamilton et al., 2017), and MoNet (Monti et al., 2017) are proposed
as general frameworks by characterizing GNNs with a message-passing function and an updating
function. More advanced variants such as GAT (Velickovic et al., 2018), JK-Nets (Xu et al., 2018b),
GIN (Xu et al., 2018a), and GraphNets (Battaglia et al., 2018) follow these frameworks.
Li et al. (Li et al., 2018), Xu et al. (Xu et al., 2018a), Morris et al. (Morris et al., 2019), and
Maron et al. (Maron et al., 2019a) show the connection between GNNs and the Weisfeiler-Lehman
algorithm (Shervashidze et al., 2011) of graph isomorphism tests, in which permutation-equivariance
holds a key constraint. Maron et al. (Maron et al., 2019b) and Keriven et al. (Keriven & Peyre, 2019)
analyze the permutation-equivariance property of GNNs more theoretically. To date, most of the
existing GNNs are permutation-equivariant and thus are not proximity-aware. The only exception
is P-GNN (You et al., 2019), which proposes to capture the positions of nodes using the relative
distance between the target node and some randomly chosen anchor nodes. However, P-GNN cannot
satisfy permutation-equivariance and is computationally expensive.
Very recently, motivated by enhancing the expressive power of GNNs in graph isomorphism tests
and distributed computing literature (Angluin, 1980; Linial, 1992; Naor & Stockmeyer, 1995),
some studies suggest assigning unique node identifiers for GNNs (Loukas, 2020) such as one-hot
IDs (Murphy et al., 2019) or random numbers (Dasoulas et al., 2019; Sato et al., 2020; Corso et al.,
2020). For example, Sato et al. (Sato et al., 2020) novelly show that random numbers can enhance
GNNs in tackling two important graph-based NP problems with a theoretical guarantee, namely the
2
Under review as a conference paper at ICLR 2021
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
minimum dominating set and the maximum matching problem, and Fey et al. (Fey et al., 2020) em-
pirically show the effectiveness of random features in the graph matching problem. Our work differs
in that we systematically study how to preserve permutation-equivariance and proximity-awareness
simultaneously in a simple yet effective framework, which is a new topic different from these ex-
isting works. Besides, we theoretically prove that our proposed method can preserve walk-based
proximities by using the random projection literature. We also demonstrate the effectiveness of our
method on various large-scale benchmarks for both node- and edge-level tasks, while no similar
results are reported in the literature.
The design of our method is also inspired by the random projection literature in dimensionality re-
duction (Vempala, 2005) and to the best of our knowledge, we are the first to study random projection
in the scope of GNNs. More remotely, our definition of node proximities is inspired and inherited
from graph kernels (Gartner et al., 2003; Borgwardt & Kriegel, 2005), network embedding (Perozzi
et al., 2014; Grover & Leskovec, 2016), and general studies of graphs (Newman, 2018).
3	Message-passing GNNs
We consider a graph G = (V, E, F) where V = {v1, ..., vN} is the set ofN = |V| nodes, E ⊆ V × V
is the set of M = |E | edges, and F ∈ RN ×d0 is a matrix of d0 node features. The adjacency matrix
is denoted as A, where its ith row, jth column and an element denoted as Ai,:, A:,j, and Ai,j,
respectively. In this paper, we assume the graph is unweighted and undirected. The neighborhood
of node Vi is denoted as Ni and Ni = Ni ∪{vi}.
The existing GNNs usually follow a message-passing framework (Gilmer et al., 2017), where the lth
layer adopts a neighborhood aggregation function AGG(∙) and an updating function UPDATE(∙):
m(l) = AGG({hjl), ∀j ∈Ni}),h(l+1) = UPDATE([h(l), m(l)]),	(1)
where hi(l) ∈ Rdl is the representation of node vi in the lth layer, dl is the dimensionality, and mi(l)
are the messages. We also denote H(I) = [hf),…,hN)] and [∙, ∙] is the concatenation operation. The
node representations are initialized as node features, i.e., H(0) = F. We denote a GNN following
Eq. (1) with L layers as a parameterized function as follows2:
H(L) = FGNN(A, F; W),	(2)
where H(L) are final node representations learned by the GNN and W denotes all the parameters.
One key property of the existing GNNs is permutation-equivariance.
Definition 1 (Permutation-equivariance). Consider a graph G = (V, E, F) and any permutation
P : V → V so that G0 = (V, E0, F0) has an adjacency matrix A0 = PAPT and a feature matrix
F0 = PF, where P ∈ {0, 1}N ×N is the permutation matrix corresponding to P, i.e., Pi,j = 1 iff
P(vi) = vj. A GNN satisfies permutation-equivariance if the node representations are equivariant
with respect to P, i.e.,
PFGNN (A, F; W) = FGNN(PAPT, PF; W).	(3)
It is known that GNNs following Eq. (1) are permutation-equivariant (Maron et al., 2019b).
Definition 2 (Automorphism). A graph G is said to have (non-trivial) automorphism if there exists
a non-identity permutation matrix P 6= IN so that A = PAPT and F = PF. We denote the
corresponding automorphic node pairs as CG = P6=IN{(i,j)|Pi,j 6=0,i 6=j}
Corollary 1. Using Definition 1 and 2, if a graph has automorphism, a permutation-equivariant
GNN will produce identical node representations for automorphic node pairs:
hi(L) =h(jL),∀(i,j) ∈CG.	(4)
Since the node representations are used for downstream tasks, the corollary shows that permutation-
equivariant GNNs cannot differentiate automorphic node pairs. A direct consequence of Corol-
lary 1 is that permutation-equivariant GNNs cannot preserve walk-based proximities between pairs
of nodes. The formal definitions are as follows.
2Since the final layer of GNNs is task-specific, e.g., a softmax layer for node classification or a readout layer
for graph classification, we only consider the GNN architecture to its last hidden layer.
3
Under review as a conference paper at ICLR 2021
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
Definition 3 (Walk-based Proximities). For a given graph G = (V, E, F), we use a matrix S ∈
RN×N to denote walk-based proximities between pairs of nodes defined as:
Si,j = S ({vi	vj}) ,	(5)
where Vi S Vj denotes walks from node Vi to Vj and S (∙) is an arbitrary real-valued function. The
length of a walk-based proximity is the maximum length of all the walks of the proximity.
Typical examples of walk-based proximities include the shortest distance (You et al., 2019), the high-
order proximities (a sum of walks weighted by their lengths) (Zhang et al., 2018), and random walk
probabilities (Klicpera et al., 2019). Next, we give a definition of preserving walk-based proximities.
Definition 4. For a given walk-based proximity, a GNN is said to be able to preserve the proximity
if there exists a decoder function Fde (∙) satisfying that for any graph G = (V, E, F), there exist
parameters WG so that ∀ > 0:
Si,j-FdeHi(,L:),H(jL,:)	<,	(6)
where
H(L) =FGNN(A,F;WG).	(7)
Note that we do not constrain the GNN architecture as long as it follows Eq. (1), and the decoder
function is also arbitrary (but notice that it cannot take the graph structure as inputs). In fact, both
the GNN and the decoder function can be arbitrarily deep and with sufficient hidden units.
Theorem 1. The existing permutation-equivariant GNNs cannot preserve any walk-based proximity
except the trivial solution that all node pairs have the same proximity.3
The formulation and proof of the theorem are given in Appendix A.1. Since walk-based proximities
are rather general and widely adopted in graph analytical tasks such as link prediction, the theorem
shows that the existing permutation-equivariant GNNs cannot handle these tasks well.
4	The Model
4.1	A GNN Framework using Stochastic Message Passing
A major shortcoming of permutation-equivariant GNNs is that they cannot differentiate automorphic
node pairs. To solve that problem, we need to introduce some mechanism as “symmetry breaking”,
i.e., to enable GNNs to distinguish these nodes. To achieve this goal, we sample a stochastic matrix
E ∈ RN×d where each element follows an i.i.d. normal distribution N(0, 1). The stochastic matrix
can provide signals in distinguishing the nodes because they are randomly sampled without being
affected by the graph automorphism. In fact, We can easily calculate that the Euclidean distance
between two stochastic signals divided by a constant √2 follows a Chi distribution χd
√12 ∣Ei,: - Ej,:l~ Xd,∀i,j∙
(8)
When d is reasonably large, e.g., d > 20, the probability of two signals being close is very low.
Then, inspired by the message-passing framework, we apply a GNN on the stochastic matrix so that
nodes can exchange information of the stochastic signals:
一 , . ________
E = FGNN (A, E; W).	⑼
We call E the stochastic representation of nodes. Using the stochastic matrix and message-passing,
E can be used to preserve node proximities (see Theorem 2 and Theorem 3). Then, to let our model
still be able to utilize node features, we concatenate E with the node representations from another
GNN with node features as inputs:
H = FoUtPUt([E, H(L)D
E = Fgnn (A, E; W), H(L)= Fgnn，(A, F; W0),
(10)
3Proposition 1 in (You et al., 2019) can be regarded as a special case of Theorem 1 using the shortest
distance proximity.
4
Under review as a conference paper at ICLR 2021
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
where FoUtpUt(∙) is an aggregation function such as a linear function or simply the identity mapping.
In a nutshell, our proposed method augments the existing GNNs with a stochastic representation
learned by message-passings to differentiate different nodes and preserve node proximities.
There is also a delicate choice worthy mentioning, i.e., whether the stochastic matrix E is fixed or
resampled in each epoch. By fixing E, the model can learn to memorize the stochastic representation
and distinguish different nodes, but with the cost of unable to handle nodes not seen during training.
On the other hand, by resampling E in each epoch, the model can have a better generalization
ability since the model cannot simply remember one specific stochastic matrix. However, the node
representations are not fixed (but pairwise proximities are preserved; see Theorem 2). In these cases,
E is more capable of handling pairwise tasks such as link prediction or pairwise node classification.
In this paper, we use a fixed E for transductive datasets and resample E for inductive datasets.
Time Complexity From Eq.(10), the time complexity of our framework mainly depends on the
two GNNs in learning the stochastic and permutation-equivariant node representations. In this paper,
we instantiate these two GNNs using simple message-passing GNNs such as GCN (Kipf & Welling,
2017) and SGC (Wu et al., 2019) (see Section 4.2 and Section 4.3). Thus, the time complexity of
our method is the same as these models, which is O(M), i.e., linear with respect to the number of
edges. We also empirically compare the running time of different models in Appendix 5.5. Besides,
many acceleration schemes for GNNs such as sampling (Chen et al., 2018a;b; Huang et al., 2018)
or partitioning the graph (Chiang et al., 2019) can be directly applied to our framework.
4.2	A Linear Instantiation
Based on the general framework shown in Eq. (10), we attempt to explore its minimum model
instantiation, i.e., a linear model. Specifically, inspired by Simplified Graph Convolution (SGC) (Wu
et al., 2019), we adopt a linear message-passing for both GNNs, i.e.,
H = FOUtpUt([E, H(L)D = FOUtpUt( [AKE, AKf] ),	(11)
where A = (D + I)-2 (A + I)(D + I)-2 is the normalized graph adjacency matrix with self-loops
proposed in GCN (Kipf & Welling, 2017) and K is the nUmber of propagation steps. We also set
FoUtpUt(∙) in Eq. (11) as a linear mapping or identity mapping.
ThoUgh seemingly simple, we show that sUch an SMP instantiation possesses a theoretical gUarantee
in preserving the walk-based proximities.
Theorem 2. An SMP in Eq. (11) with the message-passing matrix A and the number of propagation
steps K can preserve the walk-based proximity AK (AK )T with high probability ifthe dimensional-
ity of the stochastic matrix d is sufficiently large, where the superscript T denotes matrix transpose.
The theorem is regardless of whether E are fixed or resampled.
The mathematical formUlation and proof of the theorem are given in Appendix A.2. In addition, we
show that SMP is eqUivalent to a permUtation-eqUivariant GNN with certain parametrization.
Remark 1. Suppose we adopt FOUtPUt(∙) as a linear function with the output dimensionality the
same as FGNN0. Then, Eq. (10) is equivalent to the permutation-equivariant FGNN0 (A, F; W0) if the
parameters in FoUtPUt(∙) are all-zerosfor E and an identity matrixfor H(L).
The resUlt is straightforward from the definition. Then, we have the following corollary.
Corollary 2. For any task, Eq. (10) with the aforementioned linear FoUtPUt (∙) is at least as powerful
as the permutation-equivariant FGNN0 (A, F; W0), i.e., the minimum training loss of using H in
Eq. (10) is eqUal to or smaller than Using H(L) = FGNN0 (A, F; W0).
In other words, SMP will not hinder the performance4 even the tasks are permUtation-eqUivariant
since the stochastic representations are concatenated with the permUtation-eqUivariant GNNs fol-
lowed by a linear mapping. In these cases, the linear SMP is eqUivalent to SGC (WU et al., 2019).
Combining Theorem 2 and Corollary 2, the linear SMP instantiation in Eq. (11) is capable of han-
dling both proximity-aware and permUtation-eqUivariant tasks.
4Similar to previoUs works sUch as (Hamilton et al., 2017; XU et al., 2018a), we only consider the minimUm
training loss becaUse the optimization landscapes and generalization gaps are difficUlt to analyze analytically.
5
Under review as a conference paper at ICLR 2021
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
4.3	Non-linear Extensions
One may question whether a more sophisticated variant of Eq. (10) can further improve the expres-
siveness of SMP. There are three adjustable components in Eq. (10): two GNNs in propagating the
stochastic matrix and node features, respectively, and an output function. In theory, adopting non-
linear models as either component is able to enhance the expressiveness of SMP. Indeed, if we use
a sufficiently expressive GNN in learning E instead of linear propagations, we can prove a more
general version of Theorem 2 as follows.
Theorem 3. An SMP variant following Eq.(10) with FGNN (A, E; W) containing L layers can
preserve any length-L walk-based proximity if the message-passing and updating functions in the
GNN are sufficiently expressive. In this theorem, we also assume the Gaussian random vectors E
are rounded to machine precision so that E is drawn from a countable subspace of R.
The proof of the theorem is given in Appendix A.3. Similarly, we can adopt more advanced methods
for FoUtpUt(∙) such as gating or attention so that the two GNNs are more properly integrated.
Although non-linear extensions of SMP can, in theory, increase the model expressiveness, they also
take a higher risk of over-fitting dUe to model complexity, not to mention that the compUtational cost
will also increase. In practice, we find in ablation stUdies that the linear SMP instantiation in Eq. (11)
works reasonably well on most of the datasets (please refer to Section 5.4 for fUrther details).
5	Experiments
5.1	Experimental Setups
Datasets We condUct experiments on the following ten datasets: two simUlation datasets, Grid
and Communities (YoU et al., 2019), a commUnication dataset Email (YoU et al., 2019), two coaU-
thor networks, CS and Physics (ShchUr et al., 2018), two protein interaction networks, PPI (Hamil-
ton et al., 2017) and PPA (HU et al., 2020), and three GNN benchmarks, Cora, CiteSeer, and
PubMed (Yang et al., 2016). We only report the resUlts of three benchmarks for the node classi-
fication task and the resUlts for other tasks are shown in Appendix B dUe to the page limit. More
details of the datasets inclUding their statistics are provided in Appendix C.1. These datasets cover
a wide spectrUm of domains, sizes, and with or withoUt node featUres. Since Email and PPI contain
more than one graph, we condUct experiments in an inductive setting on these two datasets, i.e., the
training, validation, and testing set are split with respect to different graphs.
Baselines We adopt two sets of baselines. The first set is permUtation-eqUivariant GNNs inclUding
GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018), and SGC (WU et al., 2019), which are
widely adopted GNN architectUres. The second set contains P-GNN (YoU et al., 2019), the only
proximity-aware GNN to date. We Use the P-GNN-F version.
In comparing with the baselines, we mainly evaluate two variants of SMP with different FoUtpUt(∙):
SMP-Identity, i.e., FoUtpUt(∙) as an identity mapping, and SMP-Linear, i.e., FoUtpUt(∙) as a linear
mapping. Note that both variants adopt linear message-passing fUnctions as SGC. We condUct more
ablation stUdies with different SMP variants in Section 5.4.
For fair comparisons, we adopt the same architectUre and hyper-parameters for all the methods
(please refer to Appendix C.2 for the details). For datasets withoUt node featUres, we adopt a con-
stant vector as the node featUres. We experiment on two tasks: link prediction and node classifica-
tion. Additional experiments on graph reconstrUction, pairwise node classification, and rUnning time
comparison are provided in Appendix B. We repeat the experiments 10 times for datasets except for
PPA and 3 times for PPA, and report the average resUlts.
5.2	Link Prediction
Link prediction aims to predict missing links of a graph. Specifically, we split the edges into 80%-
10%-10% and Use them for training, validation, and testing, respectively. Besides adopting those real
edges as positive samples, we obtain negative samples by randomly sampling an eqUal nUmber of
node pairs that do not have edges. For all the methods, we set a simple classifier: Sigmoid(HiT Hj),
i.e., Use the inner prodUct to predict whether a node pair (vi, vj) forms a link, and Use AUC (area
6
Under review as a conference paper at ICLR 2021
Table 2: The results of link prediction tasks measured in AUC (%). The best results and the second-
best results for each dataset, respectively, are in bold and underlined.
Model	Grid	Communities	Email	CS	Physics	PPI
SGC	57.6±3.8	51.9±1.6	68.5±7.0	96.5±0.1	96.6±0.1	80.5±0.4
GCN	61.8±3.6	50.3±2.5	67.4±6.9	93.4±0.3	93.8±0.2	78.0±0.4
GAT	61.0±5.5	51.1±1.6	53.5±6.3	93.7±0.9	94.1±0.4	79.3±0.5
PGNN5	73.4±6.0	97.8±0.6	70.9±6.4	82.2±0.5	Out of memory	80.8±0.4
SMP-Identity	55.1±4.8	98.0±0.7	72.9±5.1	96.5±0.1	96.5±0.1	81.0±0.2
SMP-Linear	73.6±6.2	97.7±0.5	75.7±5.0	96.7±0.1	96.1±0.1	81.9±0.3
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
under the curve) as the evaluation metric. One exception to the aforementioned setting is that on the
PPA dataset, we follow the splits and evaluation metric (i.e., Hits@100) provided by the dataset (Hu
et al., 2020). The results except PPA are shown in Table 2. We make the following observations.
•	Our proposed SMP achieves the best results on five out of the six datasets and is highly compet-
itive (the second-best result) on the other (Physics). The results demonstrate the effectiveness of
our proposed method on link prediction tasks. We attribute the strong performance of SMP to its
capability of maintaining both proximity-awareness and permutation-equivariance properties.
•	On Grid, Communities, Email, and PPI, both SMP and P-GNN outperform the permutation-
equivariant GNNs, proving the importance of preserving node proximities. Although SMP is
simpler and more computationally efficient than P-GNN, SMP reports even better results.
•	When node features are available (CS, Physics, and PPI), SGC can outperform GCN and GAT.
The results re-validate the experiments in SGC (Wu et al., 2019) that the non-linearity in GNNs is
not necessarily indispensable. Some plausible reasons include that the additional model complex-
ity brought by non-linear operators makes the models tend to overfit and also difficult to train (see
Appendix B.6). On those datasets, SMP retains comparable performance on two coauthor graphs
and shows better performance on PPI, possibly because node features on protein graphs are less
informative than node features on coauthor graphs for predicting links, and thus preserving graph
structure is more beneficial on PPI.
•	As Email and PPI are conducted in an inductive setting, i.e., using different graphs for train-
ing/validation/testing, the results show that SMP can handle inductive tasks as well.
The results on PPA are shown in Table 1. SMP
again outperforms all the baselines, showing that
it can handle large-scale graphs with millions of
nodes and edges. PPA is part of a recently re-
leased Open Graph Benchmark (Hu et al., 2020).
The superior performance on PPA further demon-
strates the effectiveness of our proposed method
in the link prediction task.
Table 1: The results of link prediction on the
PPA dataset. The best result and the second-best
result are in bold and underlined, respectively.
Model	HitS@100
SGC	0.1187±0.0012
GCN	0.1867±0.0132
GraphSAGE	0.1655±0.0240
P-GNN	Out of Memory
Node2vec	0.2226±0.0083
Matrix Factorization	0.3229±0.0094
SMP-Identity	0.2018±0.0148
SMP-Linear	0.3582±0.0070
5.3 Node Classification
Next, we conduct experiments of node classifica-
tion, i.e., predicting the labels of nodes. Since
we need ground-truths in the evaluation, we only
adopt datasets with node labels. Specifically, for
CS and Physics, following (Shchur et al., 2018), we adopt 20/30 labeled nodes per class for train-
ing/validation and the rest for testing. For Communities, we adjust the number as 5/5/10 labeled
nodes per class for training/validation/testing. For Cora, CiteSeer, and PubMed, we use the default
splits that came with the datasets. We do not adopt Email because some graphs in the dataset are too
small to show stable results and exclude PPI as it is a multi-label dataset.
5The results of PGNN are slightly different compared to the paper because we adopt a more practical and
common setting that negative samples in the data are not known apriori but randomly sampled in each epoch.
7
Under review as a conference paper at ICLR 2021
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
We use a softmax layer on the learned node representations as the classifier and adopt accuracy,
i.e., how many percentages of nodes are correctly classified, as the evaluation criteria. We omit the
results of SMP-Identity for this task since the node representations in SMP-Identity have a fixed
dimensionality that does not match the number of classes.
Table 3: The results of node classification tasks measured by accuracy (%). The best results and the
second-best results for each dataset, respectively, are in bold and underlined.
Model	CommUnities	CS	Physics	Cora	CiteSeer	PUbMed
SGC	7.1±2.1	67.2±12.8	92.3±1.6	76.9±0.2	63.6±0.0	74.2±0.1
GCN	7.5±1.2	91.1±0.7	93.1±0.8	81.4±0.5	71.3±0.5	79.3±0.4
GAT	5.0±0.0	90.5±0.5	93.1±0.4	82.9±0.5	71.2±0.6	77.9±0.5
PGNN	5.2±0.5	77.6±7.6	OUt of memory	59.2±1.5	55.7±0.9	OUt of memory
SMP-Linear	99.9±0.3	91.5±0.8	93.1±0.8	80.9±0.8	68.2±1.0	76.5±0.8
The results are shown in Table 3. From the table, we observe that SMP reports nearly perfect results
on Communities. Since the node labels are generated by graph structures on Communities and there
are no node features, the model needs to be proximity-aware to handle it well. P-GNN, which shows
promising results in the link prediction task, also fails miserably here.
On the other five graphs, SMP reports highly competitive performance. These graphs are commonly-
used benchmarks for GNNs. P-GNN, which completely ignores permutation-equivariance, performs
poorly as expected. In contrast, SMP can manage to recover the permutation-equivariant GNNs
and avoid being misled, as proven in Remark 1. In fact, SMP even shows better results than its
counterpart, SGC, indicating that preserving proximities is also helpful for these datasets.
5.4	Ablation Studies
We conduct ablation studies by comparing different SMP variants, including SMP-Identity, SMP-
Linear, and the additional three variants as follows:
•	SMP-MLP: We set FoUtPUt(∙) as a fully-connected network with 1 hidden layer.
•	SMP-Linear-GCNfeat: we set FGNN0 (A, F; W0) in Eq. (10) to be a GCN (Kipf & Welling,
2017), i.e., induce non-linearity in message passing for features. FoUtPUt(∙) is still linear.
•	SMP-Linear-GCNboth: we set both FGNN (A, E; W) and FGNN0 (A, F; W0) to be a GCN (Kipf
& Welling, 2017), i.e., indUce non-linearity in message passing for both featUres and stochastic
representations. FoUtPUt(∙) is linear.
We show the resUlts for link prediction tasks in Table 4. The resUlts for node classification and
pairwise node classification, which imply similar conclUsions, are provided in Table 10 and Table 11
in Appendix B.5. We make the following observations.
•	In general, SMP-Linear shows good-enoUgh performance, achieving the best or second-best re-
sUlts on six datasets and highly competitive on the other (CommUnities). SMP-Identity, which
does not have parameters in the oUtpUt fUnction, performs slightly worse. The resUlts demon-
strate the importance of adopting a learnable linear layer in the oUtpUt fUnction, which is con-
sistent with Remark 1. SMP-MLP does not lift the performance in general, showing that adding
extra complexities in FoUtPUt(∙) brings no gain in those datasets.
•	SMP-Linear-GCNfeat reports the best resUlts on CommUnities, PPI, and PPA, indicating that
adding extra non-linearities in propagating node featUres are helpfUl for some graphs.
•	SMP-Linear-GCNboth reports the best resUlts on Gird with a considerable margin. Recall that
Grid has no node featUres. The resUlts indicate that indUcing non-linearities can help the stochas-
tic representations captUre more proximities, which is more helpfUl for featUreless graphs.
5.5	Efficiency Comparison
To compare the efficiency of different methods qUantitatively, we report the rUnning time of different
methods in Table 5. The resUlts are averaged over 3,000 epochs on an NVIDIA TESLA M40 GPU
8
Under review as a conference paper at ICLR 2021
Table 4: The ablation study of different SMP variants for the link prediction task. Datasets except
PPA are measured by AUC (%) and PPA is measured by Hits@100. The best results and the second-
best results for each dataset are in bold and underlined, respectively.
Model	Grid	Communities	Email	CS	Physics	PPI	PPA
SMP-Identity	55.1±4.8	98.0±0.7	72.9±5.1	96.5±0.1	96.5±0.1	81.0±0.2	0.2018±0.0148
SMP-Linear	73.6±6.2	97.7±0.5	75.7±5.0	96.7±0.1	96.1±0.1	81.9±0.3	0.3582±0.0070
SMP-MLP	72.1±4.3	97.8±0.6	62.7±8.1	88.9±0.8	89.2±0.4	80.1±0.3	0.2035±0.0038
SMP-Linear-GCNfeat	72.8±4.2	98.0±0.4	74.2±3.9	92.9±0.6	94.3±0.2	82.3±1.0	0.4090±0.0087
SMP-Linear-GCNboth	80.5±3.9	97.3±0.7	73.4±5.5	89.8±2.0	91.7±0.2	79.7±0.3	0.2125±0.0232
Table 5: The average running time (in milliseconds) for each epoch (including both training and
testing), on link prediction task.
Model	Grid	Communities	Email	CS	Physics	PPI
SGC	25	28	58	210	651	704
GCN	25	35	75	214	612	784
GAT	36	43	140	258	801	919
PGNN	2L∑	84	206	19,340	Out of Memory	6,521
SMP-Identity	26	37	96	284	751	840
SMP-Linear	28	26	84	212	616	832
SMP-MLP	23	28	83	237	614	831
SMP-Linear-GCNfeat	23	29	90	231	636	855
SMP-Linear-GCNboth	34	40	95	228	626	895
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
with 12 GB of memory. The results show that SMP is computationally efficient, i.e., only marginally
slower than SGC and comparable to GCN. P-GNN is at least an order of magnitude slower except
for the extremely small graphs such as Grid, Communities, or Email with no more than a thousand
nodes. In addition, the expensive memory cost makes P-GNN unable to work on large-scale graphs.
5.6	More experimental results
Besides the aforementioned experiments, we also conduct experiments on the following tasks: graph
reconstruction (Appendix B.1), pairwise node classification (Appendix B.2), and comparing with
one-hot IDs (Appendix B.3). Please refer to the Appendix for experimental results and correspond-
ing analyses.
6 Conclusion
In this paper, we propose SMP, a general and simple GNN to maintain both proximity-awareness
and permutation-equivariance properties. We propose to augment the existing GNNs with stochastic
node representations learned to preserve node proximities. We prove that SMP can enable GNN to
preserve node proximities in theory and is equivalent to a permutation-equivariant GNN with certain
parametrization. Experimental results demonstrate the effectiveness and efficiency of SMP. Ablation
studies show that a linear SMP instantiation works reasonably well on most of the datasets.
References
Dana Angluin. Local and global properties in networks of processors. In Proceedings of the twelfth
annual ACM symposium on Theory ofcomputing,pp. 82-93,1980.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv:1806.01261, 2018.
Stephen P Borgatti. Centrality and network flow. Social networks, 27(1):55-71, 2005.
9
Under review as a conference paper at ICLR 2021
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
Karsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Fifth IEEE
international conference on data mining (ICDM'05), pp. 8-pp.IEEE, 2005.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally
connected networks on graphs. In International Conference on Learning Representations, 2014.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In International Conference on Machine Learning, pp. 942-950, 2018a.
Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via
importance sampling. In International Conference on Learning Representations, 2018b.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257-266, 2019.
Gabriele Corso, LUca Cavalleri, DominiqUe Beaini, Pietro Lio, and Petar VeliCkovic. Principal
neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020.
George DasoUlas, LUdovic Dos Santos, Kevin Scaman, and Aladin VirmaUx. Coloring graph neUral
networks for node disambigUation. arXiv preprint arXiv:1912.06058, 2019.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Matthias Fey, Jan E Lenssen, Christopher Morris, Jonathan Masci, and Nils M Kriege. Deep graph
matching consensus. In International Conference on Learning Representations, 2020.
Thomas Gartner, Peter Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient
alternatives. In Learning theory and kernel machines, pp. 129-143. Springer, 2003.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In International Conference on Machine Learning, pp.
1263-1272, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005., volume 2,
pp. 729-734. IEEE, 2005.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017.
Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, YongDong Zhang, and Meng Wang. Lightgcn:
Simplifying and powering graph convolution network for recommendation. In Proceedings of
the 43rd International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 639-648, 2020.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in neural information processing systems, 2018.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks. In
Advances in Neural Information Processing Systems, pp. 7090-7099, 2019.
10
Under review as a conference paper at ICLR 2021
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. In International Conference on Machine Learning, pp. 2688-
2697, 2018.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proceedings of the 6th International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In International Conference on Learning
Representations, 2019.
Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. On social networks and collaborative
recommendation. In Proceedings of the 32nd international ACM SIGIR conference on Research
and development in information retrieval, pp. 195-202, 2009.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural
networks. In International Conference on Learning Representations, 2016.
Nathan Linial. Locality in distributed graph algorithms. SIAM Journal on computing, 21(1):193-
201, 1992.
Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International Con-
ference on Learning Representations, 2020.
Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. Learning disentangled repre-
sentations for recommendation. In Advances in Neural Information Processing Systems 32, pp.
5712-5723. 2019.
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph
networks. In Advances in Neural Information Processing Systems, pp. 2156-2167, 2019a.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In International Conference on Learning Representations, 2019b.
Alessio Micheli. Neural network for graphs: A contextual constructive approach. IEEE Transactions
on Neural Networks, 20(3):498-511, 2009.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-
5124, 2017.
Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks.
In Proceedings of the AAAI Conference on Artificial Intelligence, 2019.
Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling
for graph representations. In International Conference on Machine Learning, 2019.
Moni Naor and Larry Stockmeyer. What can be computed locally? SIAM Journal on Computing,
24(6):1259-1277, 1995.
Mark Newman. Networks. Oxford university press, 2018.
Antonio Ortega, Pascal Frossard, Jelena Kovacevic, JoSe MF Moura, and Pierre Vandergheynst.
Graph signal processing: Overview, challenges, and applications. Proceedings of the IEEE, 106
(5):808-828, 2018.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710, 2014.
11
Under review as a conference paper at ICLR 2021
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural
networks. arXiv preprint arXiv:2002.03155, 2020.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Kakade Sham and Shakhnarovich Greg. Random projections. CMSC 35900 (Spring 2009) Large
Scale Learning, 2020. URL https://ttic.uchicago.edu/~gregory/Courses/
LargeScaleLearning/lectures/jl.pdf. [Online; accessed 4-September-2020].
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls
of graph neural network evaluation. Relational Representation Learning Workshop, NeurIPS
2018, 2018.
Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(Sep):2539-
2561, 2011.
David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The
emerging field of signal processing on graphs: Extending high-dimensional data analysis to net-
works and other irregular domains. IEEE signal processing magazine, 30(3):83-98, 2013.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th international conference on world
wide web, pp. 1067-1077, 2015.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In Proceedings of the 7th International Conference on Learn-
ing Representations, 2018.
Santosh S Vempala. The random projection method, volume 65. American Mathematical Soc.,
2005.
Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of
the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
1225-1234, 2016.
Duncan J Watts. Networks, dynamics, and the small-world phenomenon. American Journal of
sociology, 105(2):493-527, 1999.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
plifying graph convolutional networks. In International Conference on Machine Learning, pp.
6861-6871, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2018a.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In International
Conference on Machine Learning, pp. 5453-5462, 2018b.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning
with graph embeddings. In Proceedings of the 33rd International Conference on International
Conference on Machine Learning-Volume 48, pp. 40-48, 2016.
Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International
Conference on Machine Learning, pp. 7134-7143, 2019.
Ziwei Zhang, Peng Cui, Xiao Wang, Jian Pei, Xuanrong Yao, and Wenwu Zhu. Arbitrary-order
proximity preserved network embedding. In Proceedings of the 24th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 2778-2786, 2018.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer tissue
networks. Bioinformatics, 33(14):i190-i198, 2017.
12
Under review as a conference paper at ICLR 2021
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
A	Theorems and Proofs
A.1 Theorem 1
Here we formulate and prove Theorem 1.
Theorem 1.	For any walk-based proximity function S (∙), a permutation-equivariant GNN cannot
preserve S(∙), except the trivial solution that all node pairs have the same proximity, i.e., Si,j =
c, ∀i, j, where c is a constant.
Proof. We prove the theorem by contradiction. Assume there exists a non-trivial S(∙) which a
permutation-equivariant GNN can preserve. Consider any graph G = (V, E, F) and denote N =
|V|. We can create G0 = (V0, E0, F0) with |V0| = 2N so that:
尸	ifi ≤ N,j ≤ N	「％ ： if i ≤ N
Ei0,j =	Ei-N,j-N	ifi>N,j>N,	F0i,: =	Fii,-: N:	ifi>N.	(12)
0	else	,
Basically, we generate two “copies” of the original graph, one indexing from 1 to N, and the other
indexing from N + 1 to 2N. By assumption, there exists a permutation-equivariant GNN which can
preserve S(∙) in G0 and We denote the node representations as HO(L) = FGNN(A0, F0; Wg，). It is
easy to see that node vi0 and vi0+N in G0 form an automorphic node pair. Using Corollary 1, their
representations will be identical in any permutation-equivariant GNN, i.e.,
H0i(,:L) = H0i(+LN),:, ∀i ≤ N.
(13)
Also, note that there exists no walk from the two copies, i.e. {v0 s vj} = {vj -→ vi} = 0, ∀i ≤
N, j > N. As a result, for ∀i ≤ N, j ≤ N, ∀ > 0, we have:
|Si,j - S(0)| ≤ Si,j-FdeH0i(,:L),H0j(,L:)+S(0)-FdeH0i(,:L),H0j(,L:)
= Si,j - Fde H0i(,:L), H0j(,L:)	+ Si,j+N - Fde H0i(,:L), H0j(+LN),:	<2.
(14)
We can prove the same for ∀i > N, j > N. The equation naturally holds if i ≤ N, j > N or
i > N,j ≤ N since {vi Svj) = 0. Combining the results, we have ∀e > 0, ∀i,j, ∣Si,j — S (0)∣ <
2. Since can be arbitrarily small, the equation shows that all node pairs have the same proximity
C = S(0), which leads to a contraction and finishes our proof.	□
Notice that in our proof, G0 can be constructed for any graph, so rather than designing one specific
counter-example, we have shown that there always exists an infinite number of counter-examples by
constructing automorphisms in the graph.
Some may find that our counter-examples in the above proof will lead to multiple connected com-
ponents. Next, we give an alternative proof maintaining one connected component (assuming the
original graph is connected) under the assumption that the walk-based proximity is of finite length.
Proof. Similar to the previous proof, we assume there exists a non-trivial S(∙) which a permutation-
equivariant GNN can preserve. Besides, we assume the length of S(∙) is upper bounded by lmax,
where lmax is any finite number, i.e., ∀i, j,
Si,j = S ({vi	vj}) = S ({vi	vj |len(vi	vj) ≤ lmax}) .	(15)
13
Under review as a conference paper at ICLR 2021
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
Then, for a connected graph G = (V, E, F), we create G0 = (V0, E0, F0) similar to Eq. (12). Specif-
ically, denoting N = N + lmax, we let G0 have 3N nodes so that:
Eij	ifi,j≤N		
1	if N ≤ i,j≤ N+1,∣j-i∣ = 1	‘F"	if i ≤ N
Ei-N ,j-N		 if N < i,j ≤ N + N	0	if N <i ≤ N
1		 . . if N + N ≤ i,j ≤ 2N+1,∣j -i| = 1 W , Fi : = ,	,: if 2N < i,j ≤ 2N + N	Fi-N,:		 if N < i ≤ N + N
Ei-2N ,j-2N		0		 if N + N < i ≤ 2N
1	if 2N + N ≤ i,j,∣j-i∣ = 1	Fi-2N,:		 if 2N <i ≤ 2N + N
1	if i = 3N ,j = 1 or j = 3N ,i = 1	、0	if 2N + N < i
0	else		
(16)
Intuitively, we create three “copies” of G and three “bridges” to connect the copies and thus make
G0 also connected. It is also easy to see that nodes Vi, v]N, and 七]+2贯 all form automorphic node
pairs and thus we have:
Hi⑷=Hi+N,：= Hi+)N,:, ∀i ≤ N.	(17)
Next, we can see that the nodes in G0 are divided into six parts (three copies and three bridges),
Which We denote as VI = {v1,…，VN}, V2 = {vN+1 ,…，VN}, V3 = {vN+ι,…，VN+n }, v4 =
{vN+N+1,…，v2N }, V5 = {v2N+1,…，v2N+N }, and V6 = {v2N+N +1,…，v3N }. Since V2, V4, V6
are bridges With length lmax, any Walk crosses these bridges Will have a length large than lmax. For
example, let us focus on Vi ∈ Vj, i.e., i ≤ N. If VjiS in V3, V4, or Vg (i.e., N < j ≤ 2N + N), any
Walk Vi Vj Will either pass the bridge V20 or V60 and thus has a length larger than lmax. As a result,
We have:
Sij= S ({Vi S Vj })= S ({Vi S Vj ∣len(Vi S Vj ) ≤ lmax}) = S (0).	(18)
If Vj ∈ Vj or Vj ∈ V2, i.e., j ≤ N, we can use the fact that Vj and Vj+N forms an automorphic node
pair similar to Eq. (14), i.e., ∀ > 0, We have
|Si,j - S(0)| ≤ Si,j - Fde H0i(,:L), H0j(,L:)	+ S (0) - Fde H0i(,:L), H0j(,L:)
=Sij- Fde jHi⑷，Hj(Lb + S』N- Fde (Hi,L), Hj(LN,：) |< 2≡.
(19)
Similarly, if Vj ∈ V6, i.e., 2N + N < j,we can use the fact that Vj and Vj-N forms an automorphic
node pair to prove the same inequality. Thus, We prove that if i ≤ N， ∀ > 0， ∀j, |Si,j - S(0)| < 2.
The same proof strategy can be applied to i > N. Since can be arbitrarily small, the results show
that all node pairs have the same proximity S (0), which leads to a contraction and finishes our
proof.	口
A.2 Theorem 2
Here we formulate and prove Theorem 2. Note that some notations and definitions are introduced in
Appendix A.1.
Theorem 2.	For the walk-based proximity S = AK (AK )t, SMP can preserve the proximity with
high probability if the dimensionality of the stochastic matrix is sufficiently large, i.e., ∀ > 0， ∀δ >
0, there ∃d0 so that any d > d0:
P(ISij-Fde (Hi,：，Hj,J| < E) > 1 - δ,	QO)
where H are the node representation obtained from SMP in Eq. (11). The result holds for any
stochastic matrix and thus is regardless of whether E is fixed or resampled during each epoch.
Proof. Our proof is mostly based on the standard random projection theory. Firstly, since we have
proven in Theorem 1 that the permutation-equivariant representations cannot preserve any walk-
based proximity, here we prove that we can preserve the proximity only using E, which can be
14
Under review as a conference paper at ICLR 2021
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
easily achieved by ignoring H(L) in FoUtpUt(旧,H(L)]), e.g., if We set FoUtpUt as a linear function, the
model can learn to set the corresponding weights for H(L) as all-zeros.
We set the decoder fUnction as a normalized inner prodUct:
Fde (Hi,:, Hj,:) = dHi,:HT	(21)
Then, denoting ai = AK and recalling E = AK E, we have:
lSi,j -Fde (Hi,:, Hj,:)| = IaiaT -工 Ei,:ET」= IaiaT - ai 工 EETaT |.	(22)
Since E is a GaUssian random matrix, from the Johnson-LindenstraUss lemma (Vempala, 2005) (in
the inner prodUct preservation forUm, e.g., see Corollary 2.1 and its proof in (Sham & Greg, 2020)),
∀0 < e0 < 2, we have:
1	0	(02-03)d
P (Iaiaj- ai dEE aj 1 ≤ 2(Ilaik + kaj Il)) > 1 - 4e	4	.	(23)
By setting d = max：kaik , we have C ' (Ilaik + kaj Il) and:
(maxi k ai k	maxi 11 a√ k )
P (∣Si,j -Fde (Hi,:, Hj,:)1 <d) > 1 - 4e--------------------4-------------
which leads to the theorem by solving and setting d0 as follows:
4e-
(maχikaik	maχikaik )d0
4
δ ⇒ d = 4log δ (maxi kaik)3
0	e2 maxi ∣∣ai∣ 一 e3
(24)
(25)
□
A.3 Theorem 3
Here we formUlate and prove Theorem 3. Note that some notations and definitions are introdUced in
Appendix A.1.
Theorem 3.	For any length-L walk-based proximity, i.e.,
Si,j = S ({vi	vj}) = S ({vi vj Ilen(vi	vj) ≤ L}) ,
where len (∙) is the length of a walk, there exists an SMP variant in Eq. (10) with FGNN (A, E; W)
containing L layers (including the input layer) to preserve that proximity if the following conditions
hold: (1) The stochastic matrix E contains unique signals for different nodes, i.e. Ei,: 6= Ej,:, ∀i 6=
j. (2) The message-passing and updating functions in learning E are bijective. (3) The decoder
function Fde(∙) also takes E as inputs and is universal approximation.
(l)
Proof. Similar as Theorem 2, we only Utilize E dUring oUr proof. We Use ei , 0 ≤ l < L to
denote the node representations in the lth layer of FGNN (A, E; W), i.e., ei(0) = Ei,: and ei(L-1) =
Ei,:. OUr proof strategy is to show that the stochastic node representations can remember all the
information aboUt the walks.
Firstly, as the message-passing and Updating fUnction are bijective by assUmption, we can recover
from the node representations in each layer all their neighborhood representations in the previoUs
layer. Specifically, there exist F(l)(∙), 1 ≤ l < L such that:
F(l) ei(l) = hei(l-1),ne(jl-1),j∈Nioi6.	(26)
For notation conveniences, we split the fUnction into two parts, one for the node itself and the other
for its neighbors:
F(l)
neighbor
Fs(ell)f ei(l)	= ei(l-1)
ei(l)	= nej(l-1), j ∈ Ni .
(27)
6To let F(I) (∙) output a set with arbitrary lengths, we can adopt sequence-based models such an LSTM.
15
Under review as a conference paper at ICLR 2021
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
For the first function, ifwe successively apply such functions from the lth layer to the input layer, we
can recover the input features of the GNN, i.e., E. Since the stochastic matrix E contains a unique
signal for different nodes, we can decode the node ID from ei(0), i.e., there exists Fs(e0lf) ei(0); E = i.
For brevity, we denote applying such l + 1 functions to get the node ID as
Fs(e0lf:l)	ei(l)	= Fs(e0lf) Fs(e1lf)	... Fs(ell)f ei(l)	;E =i.	(28)
For the second function, we can apply Fn(eli-gh1b)or to the decoded vector set so that we can recover their
neighborhood representations in the (l - 2)th layer, etc.
Next, we show that for e(jl-1), there exists a length-l walk vi vj = (va1 , va2 , ..., val), where
va1 = vi, val = vj if and only if Fs(e0lf:l-1) ej(l-1) = al = j and there exists e(l-2) , ..., e(0) such
that:
e(l-2) ∈ F(l-1)	e(l-1)	F (0:l-2) e(l-2) =a
e	∈ Fneighbor	ej	,	Fself	e	=	al-1,
e(l-3) ∈ F(l-2)	e(l-2)	F (0:l-3) e(l-3) = a
e	∈ Fneighbor	e	,	Fself	e	=	al-2 ,	(29)
e(0) ∈ Fn(e1i)ghbor e(1) , Fs(e0lf:0) e(0) = a1 = i.
This result is easily verified as:
(va1 , va2 , ..., val ) is a walk ⇔ Eai ,aj = Eaj ,ai = 1 ⇔ ai ∈ Nai+1 , ∀1 ≤ i < l
⇔ ∃e(i-1) ∈ Fn(eii)ghbor e(i), Fs(e0lf:i-1) e(i-1) =ai,∀1≤i<l.
(30)
Note that all the information is encoded in E, i.e., we can decode {vi vj |len(vi vj) ≤ L} from
ejL-1) by successively applying Fef (∙), Fne)ghbor 3∙ We can also apply FS(S0:LT) to eiL-1) to get
the start node ID i. Putting it together, we have:
F e(jL-1), ei(L-1) = {vi	vj |len(vi	vj) ≤ L} ,	(31)
where F(∙) is composed of Fef (∙), 0 ≤ l < L and Fne)ghbor (∙), 1 ≤ l < L. Applying the proximity
function S(∙), we have:
SFe(jL-1),ei(L-1)	=Si,j.	(32)
We finish the proofby setting the real decoder function Fde(∙) to arbitrarily approximate this desired
function S (F (∙, ∙)) under the universal approximation assumption.	□
B Additional Experimental Results
B.1	Graph Reconstruction
To verify that our proposed SMP can indeed preserve node proximities, we conduct experiments of
graph reconstruction (Wang et al., 2016), i.e., using the node representations learned by GNNs to
reconstruct the edges of the graph. Graph reconstruction corresponds to the first-order proximity
between nodes, i.e., whether two nodes directly have a connection, which is the most straight-
forward node proximity (Tang et al., 2015). Specifically, following Section 5.2, we adopt the inner
product classifier Sigmoid(HiT Hj ) and use AUC as the evaluation metric. To control the impact
of node features (i.e., since many graphs exhibit assortative mixing, even models only using node
features can reconstruct the edges to a certain extent), we do not use node features for all the models.
We report the results in Table 6. The results show that SMP greatly outperforms permutation-
equivariant GNNs such as GCN and GAT in graph reconstruction, clearly demonstrating that SMP
can better preserve node proximities. PGNN shows highly competitive results as SMP. However,
similar to other tasks, the intensive memory usage makes PGNN unable to handle medium-scale
graphs such as Physics and PubMed.
16
Under review as a conference paper at ICLR 2021
Table 6: The results of graph reconstruction measured in AUC (%). The best and the second-best
results for each dataset, respectively, are in bold and underlined. OOM represents out of memory.
Model	Grid	Communities	Email	CS	Physics	PPI	Cora	CiteSeer	PubMed
SGC	74.8±0.4	65.4±1.6	71.6±0.3	66.7±0.1	66.2±0.0	76.3±0.2	56.7±9.7	58.5±0.1	71.9±0.1
GCN	73.0±0.3	63.7±1.2	72.5±0.4	75.5±0.4	76.8±0.4	79.2±0.4	68.2±3.9	69.8±8.0	77.2±2.1
GAT	59.6±1.2	52.9±1.1	56.9±1.9	57.0±1.4	59.1±0.7	61.1±1.9	57.8±1.0	63.2±1.5	58.8±0.8
PGNN	99.4±0.1	97.7±0.1	85.6±0.8	97.2±0.6	OOM	85.2±0.6	98.1±0.6	99.7±0.1	OOM
SMP-Identity	99.2±0.1	97.5±0.1	80.0±0.3	77.1±2.3	73.7±0.3	79.5±0.2	89.7±5.7	97.1±0.8	77.0±0.1
SMP-Linear	99.1±0.1	97.8±0.1	86.7±0.2	96.3±0.2	95.5±0.2	85.5±0.1	96.3±0.1	98.2±0.1	95.8±0.2
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
B.2	Pairwise Node Classification
Besides standard node classification experiments reported in Section 5.3, we follow (You et al.,
2019) and experiment on pairwise node classification, i.e., predicting whether two nodes have the
same label. Compared with standard node classification, pairwise node classification focuses more
on the relations between nodes and thus requires the model to be proximity-aware to perform well.
Similar to link prediction, we split the positive samples (i.e., node pairs with the same label) into an
80%-10%-10% training-validation-testing set with an equal number of randomly sampled negative
pairs. For large graphs, since the possible positive samples are intractable (i.e. O(N 2)), we use a
random subset. Since we also need node labels as the ground-truth, we only conduct pairwise node
classification on datasets when node labels are available. We also exclude the results of PPI since
the dataset is multi-label and cannot be used in a pairwise setting (You et al., 2019). Similar to
Section 5.2, we adopt a simple inner product classifier and use AUC as the evaluation metric.
The results are shown in Table 7. We observe consistent results as link prediction in Section 5.2, i.e.,
SMP reports the best results on four datasets and the second-best results on the other three datasets.
These results again verify that SMP can effectively preserve and utilize node proximities when
needed while retaining comparable performance when the tasks are more permutation-equivariant
like, e.g., on CS and Physics.
Table 7: The results of pairwise node classification tasks measured in AUC (%). The best results
and the second-best results for each dataset, respectively, are in bold and underlined.
Model	Communities	Email	CS	Physics	Cora	CiteSeer	PubMed
SGC	67.4±2.4	56.3±5.4	99.8±0.0	99.6±0.0	99.2±0.3	95.5±0.7	92.3±0.3
GCN	64.9±2.3	55.0±5.7	96.8±0.7	99.7±0.1	97.7±0.6	92.9±1.2	94.8±0.4
GAT	52.5±1.3	47.7±2.7	95.2±0.6	96.3±0.2	91.6±0.7	73.6±2.7	87.1±0.2
PGNN	98.6±0.5	63.3±5.5	90.0±0.5	Out of memory	85.5±1.2	49.8±1.8	Out of memory
SMP-Identity	98.8±0.5	56.9±4.1	99.7±0.0	99.6±0.0	99.2±0.2	95.2±1.1	91.9±0.3
SMP-Linear	98.8±0.5	74.5±4.1	99.8±0.0	99.6±0.0	99.3±0.3	95.3±0.4	93.4±0.2
B.3	Comparison with Using IDs
We further compare SMP with augmenting GNNs using a one-hot encoding of node IDs, i.e., the
identity matrix. Intuitively, since the IDs of nodes are unique, such a method does not suffer from the
automorphism problem and should also enable GNNs to preserve node proximities. However, theo-
retically speaking, using such a one-hot encoding has two major problems. Firstly, the dimensional-
ity of the identity matrix is N × N, and thus the number of parameters in the first message-passing
layer is also on the order of O(N). Therefore, the method will inevitably be computationally ex-
pensive and may not be scalable to large-scale graphs. A large number of parameters will also more
likely lead to the overfitting problem. Secondly, the node IDs are not transferable across different
graphs, i.e., the node v1 in one graph and the node v1 in another graph do not necessarily share a
similar meaning. But as the parameters in the message-passings depend on the node IDs (since they
are input features), such a mechanism cannot handle inductive tasks well.7
7One may question whether SMP is transferable across different graphs since the stochastic features are
independently drawn. Empirically, we find that SMP reports reasonably well results on inductive datasets such
17
Under review as a conference paper at ICLR 2021
Table 8: The results of comparing SMP with using one-hot IDs in GCNs. OOM represents out of
memory. — represents the task is unavailable.
Task	Model	Grid	Communities	Email	CS	Physics	PPI	Cora	CiteSeer	PubMed
Link	GCNonehot	91.5±2.1	98.3±0.7	71.2±3.5	93.1±1.3	OOM	78.6±0.3	86.8±1.5	81.7±1.1	89.4±0.5
Prediction	SMP-Linear	73.6±6.2	97.7±0.5	75.7±5.0	96.7±0.1	96.1±0.1	81.9±0.3	92.7±0.7	92.6±1.0	95.4±0.2
Pairwise Node	GCNonehot	—	98.9±0.5	67.3±5.6	97.6±0.2	OOM	—	98.2±0.3	94.4±1.2	98.9±0.1
Classification	SMP-Linear	—	98.8±0.5	74.5±4.1	99.8±0.0	99.6±0.0	—	99.3±0.3	95.3±0.4	93.4±0.2
Node	GCNonehot	—	99.6±1.0	—	86.9±1.5	OOM	—	77.6±1.1	57.7±5.8	74.9±0.6
Classification	SMP-Linear	—	99.9±0.3	—	91.5±0.8	93.1±0.8	—	80.9±0.8	68.2±1.0	76.5±0.8
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
We also empirically compare such a method with SMP and report the results in Table 8. The results
show that SMP-Linear outperforms GCNonehot in most cases. Besides, GCNonehot fails to handle
Physics, which is only a medium-scale graph, due to the heavy memory usage. One surprising result
is that GCNonehot outperforms SMP-Linear on Grid, the simulated graph where nodes are placed on
a 20 × 20 grid. A plausible reason is that since the edges in Grid follow a specific rule, using a
one-hot encoding gives GCNonehot enough flexibility to learn and remember the rules, and the model
does not overfit because the graph has a rather small scale.
B.4	Additional Link Prediction Results
We further report the results of link prediction on three GNN benchmarks: Cora, CiteSeer, and
PubMed. The results are shown in Table 9. The results show similar trends as other datasets pre-
sented in Section 5.2, i.e., SMP reports comparable results as other permutation-equivariant GNNs
while PGNN fails to handle the task well.
Table 9: The results of the link prediction task measured in AUC (%). The best results and the
second-best results for each dataset, respectively, are in bold and underlined.
Model	Cora	CiteSeer	PubMed
SGC	93.6±0.6	94.7±0.8	95.8±0.2
GCN	90.6±1.0	78.2±1.7	92.4±0.9
GAT	88.5±1.2	87.8±1.0	89.2±0.8
PGNN	75.4±2.3	70.6±1.1	Out of memory
SMP-Identity	93.0±0.6	92.9±0.5	94.5±0.3
SMP-Linear	92.7±0.7	92.6±1.0	95.4±0.2
SMP-MLP	82.8±0.9	80.7±1.1	88.0±0.6
SMP-Linear-GCNfeat	86.7±1.4	81.1±1.4	90.5±0.6
SMP-Linear-GCNboth	80.1±2.5	80.0±2.0	81.1±2.0
B.5	Additional Ablation Studies
We report the ablation study results for the node classification task and pairwise node classification
task in Table 10 and Table 11, respectively. The results again show that SMP-Linear generally
achieves good-enough results on the majority of the datasets and adding non-linearities does not
necessarily lift the performance of SMP.
We also compare whether the stochastic signals E are fixed or not during different training epochs
for our proposed SMP. For brevity, we only report the results for the link prediction task in Table 12.
The results show that fixing E usually leads to better results on transductive datasets (recall that
datasets except Email and PPI are transductive) and resampling E leads to better results on inductive
datasets in general. The results are consistent with our analysis in Section 4.1.
as Email and PPI. One plausible reason is that since the proximities of nodes are preserved even the random
features per se are different (see Theorem 2), all subsequent parameters based on proximities can be transferred.
18
Under review as a conference paper at ICLR 2021
Table 10: The ablation study of different SMP variants for the node classification task. The best
results and the second-best results are in bold and underlined, respectively.
Model	Communities	CS	Physics	Cora	CiteSeer	PubMed
SMP-Linear	99.9±0.3	91.5±0.8	93.1±0.8	80.9±0.8	68.2±1.0	76.5±0.8
SMP-MLP	100.0±0.2	90.1±0.5	92.3±0.8	79.3±0.8	67.0±1.5	76.8±0.9
SMP-Linear-GCNfeat	100.0±0.0	89.8±0.7	92.9±0.8	78.9±1.2	67.8±0.6	77.3±0.6
SMP-Linear-GCNboth	100.0±0.2	77.4±4.2	87.1±3.5	69.2±2.5	49.8±3.1	68.1±4.1
Table 11: The ablation study of different SMP variants for the pairwise node classification task. The
best results and the second-best results are in bold and underlined, respectively.
Model	Communities	Email	CS	Physics	Cora	CiteSeer	PubMed
SMP-Identity	98.8±0.5	56.9±4.1	99.7±0.0	99.6±0.0	99.2±0.2	95.2±1.1	91.9±0.3
SMP-Linear	98.8±0.5	74.5±4.1	99.8±0.0	99.6±0.0	99.3±0.3	95.3±0.4	93.4±0.2
SMP-MLP	98.7±0.3	65.4±6.3	94.3±0.6	97.6±0.4	90.3±3.0	67.7±13.7	93.4±0.4
SMP-Linear-GCNfeat	99.0±0.4	60.2±9.3	95.6±0.7	98.3±0.7	96.1±0.7	88.8±1.6	94.8±0.2
SMP-Linear-GCNboth	98.8±0.4	61.6±6.0	95.2±0.7	97.8±0.8	94.3±1.9	83.5±3.9	94.1±0.7
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
B.6	Comparison of Permutation-equivariant GNNs for link prediction
To investigate the performance of linear and non-linear variants of permutation-equivariant GNNs
for the link prediction task, we additionally report both the training accuracies and the testing accu-
racies of SGC, GCN, and GAT in Table 13. Notice that to ensure a fair comparison, we do not adopt
the early stopping strategy here so that different models have the same number of training epochs
(otherwise, if a model tends to overfit, the early stopping strategy will terminate the training process
when the number of training epochs is small and result in a spurious underfitting phenomena).
The results show that non-linear variants of GNNs (GCN and GAT) are more likely to overfit, i.e.,
the margins between the training accuracies and the testing accuracies are usually larger, than the
linear variant SGC. Besides, though possessing extra model expressiveness, non-linear GNNs are
also difficult to train, i.e., the training accuracies of GCN and GAT are not necessarily higher than
SGC. The results are consistent with the literature Wu et al. (2019); He et al. (2020).
C	Experimental Details for Reproducibility
C.1 Datasets
•	Grid (You et al., 2019): A simulated 2D grid graph with size 20 × 20 and no node feature.
•	Communities (You et al., 2019): A simulated caveman graph (Watts, 1999) composed of 20
communities with each community containing 20 nodes. The graph is perturbed by rewiring 1%
edges randomly. It has no node feature and the label of each node indicates which community
the node belongs to.
•	Email8 (You et al., 2019): Seven real-world email communication graphs. Each graph has six
communities and each node has an integer label indicating the community the node belongs to.
•	Coauthor Networks9 (Shchur et al., 2018): Two networks from Microsoft academic graph in
CS and Physics with their nodes representing authors and edges representing co-authorships
between authors. The node features are embeddings of the paper keywords of the authors.
•	PPI8 (Hamilton et al., 2017): 24 protein-protein interaction networks. Each node has a 50-
dimensional feature vector.
•	PPA10 (Hu et al., 2020): A network representing biological associations between proteins from
58 different species. The node features are one-hot vectors of the species that the proteins are
taken from.
8https://github.com/JiaxuanYou/P-GNN/tree/master/data
9https://github.com/shchur/gnn-benchmark/tree/master/data/npz/
10https://snap.stanford.edu/ogb/data/linkproppred/ppassoc.zip
19
Under review as a conference paper at ICLR 2021
Table 12: The results of comparing whether the stochastic signals E are fixed or not during different
training epochs for the link prediction task. The better of the two results are in bold.
Model	E	Grid	Communities	CS	Physics	Email	PPI
SMP-Identity	Fixed	55.1±4.8	98.0±0.7	96.5±0.1	96.5±0.1	75.9±3.9	80.4±0.4
	Not Fixed	55.2±4.1	97.6±0.7	96.4±0.1	96.5±0.1	72.9±5.1	81.0±0.2
SMP-Linear	Fixed	73.6±6.2	97.7±0.5	96.7±0.1	96.1±0.1	71.3±3.9	71.5±0.7
	Not Fixed	64.4±2.9	97.4±0.1	96.2±0.1	96.1±0.1	75.7±5.0	81.9±0.3
Table 13: The results of SGC, GCN, and GAT for the link prediction task. Both the training accura-
cies and the testing accuracies are reported.
Method	Results	Grid	Communities	Email	CS	Physics	PPI	Cora	CiteSeer	PubMed
SGC	Train	52.4±0.5	50.4±0.5	68.4±1.0	98.3±0.0	97.7±0.0	84.9±0.4	99.5±0.0	99.9±0.0	98.9±0.0
	Test	51.6±2.9	49.2±1.6	67.0±9.3	96.5±0.1	96.5±0.1	78.8±0.7	92.5±0.7	94.0±0.5	95.6±0.3
GCN	Train	51.3±1.0	50.0±0.6	68.9±4.0	95.3±0.3	95.1±0.3	76.1±0.6	96.5±1.0	77.3±1.3	93.9±1.6
	Test	54.6±4.3	49.2±1.1	66.0±3.1	93.2±0.3	93.8±0.3	74.9±0.6	89.2±0.3	76.1±2.5	90.6±1.2
GAT	Train	47.5±1.4	49.6±0.3	52.2±3.6	95.7±0.9	96.4±0.5	82.3±0.3	97.2±0.5	99.3±0.1	98.3±0.1
	Test	50.8±6.0	50.8±2.1	47.6±4.4	91.0±1.1	93.9±0.3	78.5±0.4	75.7±1.7	81.0±0.7	83.7±1.3
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
•	Cora, CiteSeer, PubMed11 (Yang et al., 2016): Three citation graphs where nodes correspond
to papers and edges correspond to citations between papers. The node features are bag-of-words
and the node labels are the ground truth topics of the papers.
We summarize the statistics of datasets in Table 14.
C.2 Hyper-parameters
We use the following hyper-parameters:
•	All datasets except PPA: we uniformly set the number of layers for all the methods as 2, i.e., 2
message-passing steps, and set the dimensionality of hidden layers as 32, i.e., H(l) ∈ RN ×32,
for all 1 ≤ l ≤ L (for GAT, we use 4 heads with each head containing 8 units). We use Adam
optimizer with an initial learning rate of 0.01 and decay the learning rate by 0.1 at epoch 200.
The weight decay is 5e-4. We train the model for 1,000 epochs and evaluate the model every 5
epochs. We adopt an early-stopping strategy by reporting the testing performance at the epoch
which achieves the best validation performance. For SMP, the dimensionality of the stochastic
matrix is d = 32. For P-GNN, we use the P-GNN-F version, which uses the truncated 2-hop
shortest path distance instead of the exact shortest distance.
•	PPA: as suggested in the original paper (Hu et al., 2020), we set the number of GNN layers
as 3 with each layer containing 256 hidden units and add a three-layer MLP after taking the
Hadamard product between pair-wise node embeddings as the predictor, i.e., MLP(Hi Hj).
We use Adam optimizer with an initial learning rate of 0.01. We set the number of epochs for
training as 40, evaluate the results on validation sets every epoch, and report the testing results
using the model with the best validation performance. We also found that the dataset had issues
with exploding gradients and adopt a gradient clipping strategy by limiting the maximum p2-
norm of gradients as 1.0. The dimensionality of the stochastic matrix in SMP is d = 64.
C.3 Hardware and S oftware Configurations
All experiments are conducted on a server with the following configurations.
•	Operating System: Ubuntu 18.04.1 LTS
•	CPU: Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz
•	GPU: NVIDIA TESLA M40 with 12GB of memory
11https://github.com/kimiyoung/planetoid/tree/master/data
20
Under review as a conference paper at ICLR 2021
Table 14: The statistics of the datasets. For datasets with more than one graph, #Nodes and #Edges
are summed over all the graphs and the experiments are conducted in an inductive setting.
Dataset	#GraPhS	#Nodes	#Edges	#Features	#Classes
Grid	1	400	760	-	-
Communities	1	400	3,800	-	20
Email	7	1,005	25,571	-	42
CS	1	18,333	81,894	6,805	15
Physics	1	34,493	247,962	8,415	5
PPI	24	56,944	818,716	50	-
PPA	1	576,289	30,326,273	58	-
Cora	1	2,708	5,429	1,433	7
CiteSeer	1	3,327	4,732	3,703	6
PubMed	1	19,717	44,338	500	3
730	• Software: Python 3.6.8, PyTorch 1.4.0, PyTorch Geometric 1.4.3, NumPy 1.18.1, Cuda 10.1
21