Under review as a conference paper at ICLR 2021
Learning to Observe with	Reinforcement
Learning
Anonymous authors
Paper under double-blind review
Ab stract
We consider a decision making problem where an autonomous agent decides on
which actions to take based on the observations it collects from the environment.
We are interested in revealing the information structure of the observation space
illustrating which type of observations are the most important (such as position
versus velocity) and the dependence of this on the state of agent (such as at
the bottom versus top of a hill). We approach this problem by associating a
cost with collecting observations which increases with the accuracy. We adopt a
reinforcement learning (RL) framework where the RL agent learns to adjust the
accuracy of the observations alongside learning to perform the original task. We
consider both the scenario where the accuracy can be adjusted continuously and
also the scenario where the agent has to choose between given preset levels, such
as taking a sample perfectly or not taking a sample at all. In contrast to the existing
work that mostly focuses on sample efficiency during training, our focus is on the
behaviour during the actual task. Our results illustrate that the RL agent can learn
to use the observation space efficiently and obtain satisfactory performance in the
original task while collecting effectively smaller amount of data. By uncovering the
relative usefulness of different types of observations and trade-offs within, these
results also provide insights for further design of active data acquisition schemes.
1	Introduction
Autonomous decision making relies on collecting data, i.e. observations, from the environment where
the actions are decided based on the observations. We are interested in revealing the information
structure of the observation space illustrating which type of observations are the most important
(such as position versus velocity). Revealing this structure is challenging since the usefulness of the
information that an observation can bring is a priori unknown and depends on the environment as
well as the current knowledge state of the decision-maker, for instance, whether the agent is at the
bottom versus the top of a hill and how sure the agent is about its position. Hence, we’re interested
in questions such as “Instead of collecting all available observations, is it possible to skip some
observations and obtain satisfactory performance?”, “Which observation components (such as the
position or the velocity) are the most useful when the object is far away from (or close to) the target
state?”. The primary aim of this work is to reveal this information structure of the observation space
within a systematic framework.
We approach this problem by associating a cost with collecting observations which increases with the
accuracy. The agent can choose the accuracy level of its observations. Since cost increases with the
accuracy, we expect that the agent will choose to collect only the observations which are most likely
to be informative and worth the cost. We adopt a reinforcement learning (RL) framework where the
RL agent learns to adjust the accuracy of the observations alongside learning to perform the original
task. We consider both the scenario where the accuracy can be adjusted continuously and also the
scenario where the agent has to choose between given preset levels, such as taking a sample perfectly
or not taking a sample at all. In contrast to the existing work that mostly focuses on sample efficiency
during training, our focus is on the behaviour during the actual task. Our results illustrate that the RL
agent can learn to use the observation space efficiently and obtain satisfactory performance in the
original task while collecting effectively smaller amount of data.
1
Under review as a conference paper at ICLR 2021
2	Related Work
A related setting is active learning (Settles, 2010; Donmez et al., 2010) where an agent decides which
queries to perform, i.e., which samples to take, during training. For instance, in an active learning
set-up, an agent learning to classify images can decide which images from a large dataset it would like
to have labels for in order to have improved classification performance. In a standard active learning
approach (Settles, 2010; Donmez et al., 2010) as well as its extensions in RL (Lopes et al., 2009),
the main aim is to reduce the size of the training set, hence the agent tries to determine informative
queries during training so that the performance during the test phase is optimal. In the test phase, the
agent cannot ask any questions; instead, it will answer questions, for instance, it will be given images
to label. In contrast, in our setting the agent continues to perform queries during the test phase, since
it still needs to collect observations during the test phase, for instance as in the case of collecting
camera images for an autonomous driving application. From this perspective, one of our main aims is
to reduce the number of queries the agent performs during this actual operation as opposed to number
of queries in its training phase.
Another related line of work consists of the RL approaches that facilitate efficient exploration of state
space, such as curiosity-driven RL and intrinsic motivation (Pathak et al., 2017; Bellemare et al.,
2016; Mohamed & Rezende, 2015; Still & Precup, 2012) or active-inference based methods utilizing
free-energy (UeltzhOffer, 2018; SchWobel et al., 2018); and the works that focus on operation with
limited data using a model (Chua et al., 2018; Deisenroth & Rasmussen, 2011; Henaff et al., 2018;
Gal et al., 2016). In these works, the focus is either finding informative samples (Pathak et al., 2017)
or using a limited number of samples/trials as much as possible by making use of a forward dynamics
model (Boedecker et al., 2014; Chua et al., 2018; Deisenroth & Rasmussen, 2011; Henaff et al.,
2018; Gal et al., 2016) during the agent’s training. In contrast to these approaches, we would like
to decrease the effective size of the data or the number of samples taken during the test phase, i.e.
operation of the agent after the training phase is over.
Representation learning for control and RL constitutes another line of related work (Watter et al., 2015;
Hafner et al., 2019; Banijamali et al., 2018). In these works, the transformation of the observation
space to a low-dimensional space is investigated so that action selection can be performed using
this low-dimensional space. Similar to these works, our framework can be also interpreted as a
transformation of the original observation space where an effectively low-dimensional space is sought
after. Instead of allowing a general class of transformations on the observations, here we consider a
constrained setting so that only specific operations are allowed, for instance, we allow dropping some
of the samples but we do not allow collecting observations and then applying arbitrary transformations
on them.
Our work associates a cost with obtaining observations. Cost of data acquisition in the context of
Markov decision processes (MDPs) has been considered in a number of works, both as a direct
cost on the observations (Hansen, 1997; Zubek & Dietterich, 2000; 2002) or as an indirect cost of
information sharing in multiple agent settings (Melo & Veloso, 2009; De Hauwere et al., 2010).
Another related line of work is performed under the umbrella of configurable MDPs (Metelli et al.,
2018; Silva et al., 2019) where the agent can modify the dynamics of the environment. Although in
our setting, it is the accuracy of the observations rather than the dynamics of the environment that
the agent can modify, in some settings our work can be also interpreted as a configurable MDP. We
further discuss this point in Section 4.2.
3	Proposed Framework and The Solution Approach
3.1	Preliminaries
Consider a Markov decision process given by hS, A, P, R, Ps0 , γi where S is the state space, A is
the set of actions, P : S × A × S → R denotes the transition probabilities, R : S × A → R denotes
the bounded reward function, Ps0 : S → R denotes the probability distribution over the initial state
and γ ∈ (0, 1] is the discount factor.
The agent, i.e. the decision maker, observes the state of the system st at time t and decides on its
action at based on its policy π(s, a). The policy mapping of the agent π(s, a) : S × A → [0, 1] is
possibly stochastic and gives the probability of taking the action a at the state s. After the agent
2
Under review as a conference paper at ICLR 2021
implements the action at, it receives a reward r(st, at) and the environment moves to the next state
st+1 which is governed by P and depends on at and st . The aim of the RL agent is to learn an
optimal policy mapping π(s, a) so that the expected return, i.e. expected cumulative discounted
reward, J(π) = Eat〜∏,st〜P [Pt Ytr(st, at)] is maximized.
3.2	Partial Observability
Although most RL algorithms are typically expressed in terms of MDPs, in typical real-life appli-
cations the states are not directly observable, i.e., the observations only provide partial, possibly
inaccurate information. For instance, consider a vehicle which uses the noisy images with limited
angle-of-view obtained from cameras mounted on the vehicle for autonomous-driving decisions. In
such scenarios, the data used by the agent to make decisions is not a direct representation of the
state of the world. Hence, we consider a partially observable Markov decision process (POMDP)
where the above MDP is augmented by O and Po where O represents the set of observations and
Po : S → O represents the observation probabilities. Accordingly, the policy mapping is now
expressed as π(o, a) : O × A → [0, 1].
The observation vector at time t is given by ot = [ot1; . . . ; otn] ∈ Rn, where n is the dimension of the
observation vector. The observations are governed by
0t 〜Po(θt∣St; βt)	(1)
wherepo(ot|st; βt) denotes the conditional probability distribution function (pdf) of ot given st and
is parametrized by the accuracy vector
βt = [βi;... ； βn] ∈ Rn	⑵
The parameter βti ≥ 0 represents the average accuracy of the observation component i at time step t,
i.e. oit. For instance, say we have two observations, position o1 and velocity o2 . Then, βt1 denotes the
accuracy of the position and βt2 denotes the accuracy of the velocity. As βti increases, the accuracy of
the observation oit decreases. Given st and βt, the observations are statistically independent, i.e. we
have the factorization
Po(Ot |st； βt) = Y Poi (oi|st； βi)	⑶
i=1,...,n
wherePoi(Oit|st; βti) denotes the conditional pdf of Oit given st and βti.
Note that βti determines the average accuracy, i.e. the accuracy in the statistical sense. We provide an
example below:
Example: Consider the common Gaussian additive noise model with
Oit = sit + vti,	i = 1, . . ., n, (4)
where st = [st1; . . . ; stn] ∈ Rn is the state vector and vt = [vt1; . . . ; vtn] ∈ Rn is the Gaussian noise
vector with N(0, diag(σ2i)). Here, vt and vt0 are statistically independent (stat. ind.) for all t 6= t0
vt
and also vt and st0 are stat. ind. for all t, t0 . Under this observation model, a reasonable choice
for βti is βti = σ2i. Hence, we parametrize Pio(.) as Pio(Oit|sit; βti) = N(sit, βti = σ2i). Note that
vt	o	o	vt
the parametrization in terms of βti can be done in multiple ways, for instance, one may also adopt
βti = σvti .
3.3	Decision maker chooses the accuracy of the observations
The agent can choose βti , hence βti is a decision variable. Observations have a cost which increases
with increasing accuracy, i.e. the cost increases with decreasing βti.
•	In Scenario A, the agent can vary βti on a continuous scale, i.e. βti ∈ [0, ∞].
•	In Scenario B, the agent chooses between i) collecting all the observations with a fixed
level of accuracy or ii) not getting any of them at all. This setting corresponds to the case
with βt = βt1, βt ∈ {βF, ∞}, where 1 ∈ Rn denotes the vector of ones. Here Bf ≥ 0
represents a fixed accuracy level. Note that βF can be zero, corresponding to the case
Ot = st.
3
Under review as a conference paper at ICLR 2021
Remark 3.1 Our proposed setting can be interpreted as a constrained representation learning
problem for RL. In particular, consider the problem of learning the best mapping h(.) with
Zt = h(0t)	(5)
from the high-dimensional original observations bt to some new possibly low-dimensional variables
Zt so that control can be performed reliably on Zt instead of bt. Such settings have been utilized in
various influential work, see for instance E2C approach of Watter et al. (2015).
The proposed approach can be also formulated in a representation framework. In particular, we
interpret the possibly noisy observations ot as the effectively low-dimensional representation Zt used
in (5). Hence, consider the mapping bh(.)
ot = bh(obt),	(6)
where ot and obt denote the noisy and the original measurements, respectively. Compared to (5),
the family of the mappings allowed in (6) is constrained, i.e. one can only adjust the accuracy
parameter instead of using arbitrary transformations from obt to ot . Here, ot is effectively low-
dimensional compared to obt because i) noise decreases the dynamic range and allows effectively
higher compression rates for the data (Scenario A); or ii) the total number of observations acquired is
smaller (Scenario B). Note that not all transformations from st to ot can be written using (6) as an
intermediate step. From this perspective, the formulation in (1) can be said to be more general than
(6).
3.3.1	Motivation
The primary motivation behind the proposed framework is to reveal the inherent nature of the
observation space in terms of usefulness of information the observations provide with respect to the
task at hand. The secondary motivation is to provide a RL framework for solving decision making
problems when the observations have a cost associated with them.
In regard to the first task, we note the following: To reveal this information structure, we associate
an artificial cost with the observations that increase with the accuracy. Hence, only the observation
components (or the observation vectors) which are mostly likely to be informative and worth the cost
will be collected. This decision heavily depends on the state that the agent believes itself to be in. For
instance, in the case of balancing an object at an unstable state (such as pendulum in OpenAi Gym
(Brockman et al., 2016)), we intuitively expect that the agent does not need accurate measurements
when it is far away from the target state. Hence, we’re interested in questions such as “Is it possible
to skip some observations and obtain satisfactory performance?”, “Which observation components
(such as the position or the velocity) are most useful when the object is far away from (or close to)
the target state?”, “How are these results affected by the possible discrepancy between the true state
the agent is in and the one that it believes it to be in due to noisy or skipped observations?”. The
proposed framework reveals this information structure within a systematic setting.
In regard to the second task, we note that there are many practical problems where there is a cost
associated with acquiring observations (Hansen, 1997; Zubek & Dietterich, 2000; 2002), for instance
consider the expensive medical tests (i.e. observations) that have to performed to diagnose a certain
disease (Zubek & Dietterich, 2002) and wireless communications where there is a cost associated with
channel usage (i.e. the right to use a communication channel) and a power cost that increases with
the reliability of communications (Goldsmith, 2005; Cover & Thomas, 1991), see also Section A.1.
The proposed framework can be used to find efficient observation strategies in such problems and to
quantify the possible performance degradation due to the observation cost.
Examples: The proposed scenarios A and B also correspond to practical data acquisition schemes.
We now give some examples: An example for Scenario A is the case where the observations are
obtained using different sensors on the device where the accuracy of each sensor can be individually
adjusted. Another example is the case where the sensors are distributed over the environment and
the readings of the sensors has to be relayed to central decision unit using individual compression
of each observation type and wireless communications. Here, the compression and the wireless
communication introduces an accuracy-cost trade-off where the agent can choose to operate at
different points of. Please see Section A.1 for an example illustrating the accuracy-cost trade-off in
4
Under review as a conference paper at ICLR 2021
wireless communications. An example for Scenario B is the remote control of a device, such as a
drone, where all sensor readings of the device are compressed together and then sent to a decision
unit. Since all readings are compressed and transmitted together, a decision of whether to transmit
the whole observation vector or not has to be made, for instance due the limited power or wireless
channel occupancy constraints.
3.4 Reward Shaping
Reward shaping is a popular approach to direct RL agents towards a desired goal. Here, we want the
agent not only move towards the original goal (which is encouraged by the original reward r), we
also want it to learn to control βt . Hence, we propose reward shaping in the following form:
r = f(rt,βt)	(7)
where r is the original reward, rt is the new modified reward and f (rt, βt) is a monotonically
non-decreasing function of rt and βti, ∀i. Hence, the agent not only tries to maximize the average
of the original reward but it also tries to maximize the “inaccuracy” of the measurements. This can
be equivalently interpreted as minimizing the cost due to accurate measurements. In the case where
there is a direct cost function ci(.) that increases with the accuracy of the observation oi (see, for
instance, the example in Section A.1 where transmission power can be interpreted as the direct cost),
the following additive form can be used
n
rt = Tt- λX ci(βi),	(8)
i=1
where ci (βti) is a non-increasing function of βti and λ ≥ 0 is a weighting parameter. Hence, the
agent’s aim is to maximize the original reward as well as minimize the cost of the observations.
4	Experiments
4.1	Setting
Observation Models: We consider the following environments from the OpenAI Gym (Brockman
et al., 2016): MountainCarContinuous-v0, Pendulum-v0, CartPole-v1. In this section, we illustrate
how the modified environment with noisy observations is obtained for MountainCarContinuous-v0.
The details and the parameter values for the other environments can be found in the Appendix A.2.
We also consider a version of MountainCarContinuous-v0 with observations of the vertical position,
which is presented in Section A.4.
We first explain Scenario A, and then Scenario B. The original observations of the mountain car
environment are the position Xt and the velocity Xt. In our framework, the agent has access to noisy
versions of these original observations
Xt = Xt + Qx × ∆χt(βt),	(9a)
Xt = Xt + Qx X ∆Xt(β2),	(9b)
where Δx(β1)〜U(-β1, β1), ∆Xt(β2)〜U(-β2,β2) andU(-β, β) denotes the uniform distri-
bution over [-β, β]. The noise variables are stat. ind., in particular ∆Xt(β1) and ∆Xt(β2) are stat.
ind. from each other and also stat. ind. over time. Here, Qx and Qx determine the ranges of the
noise levels and they are set as the 0.1 times of the full range of the corresponding observation, i.e.,
Qx = 0.18 and Qx = 0.014.
Our agent chooses βti ∈ [0, 1] in addition to the original action of the environment, i.e. the force
at that would be exerted on the car. The original reward of the environment per step is given by
rt = -0.1 × at2. The reward is shaped using an additive model
rt=rt+KA × (n X βi),	(IO)
where n = 2 and κA > 0 is chosen as 5 × 10-6. The original environment has also a termination
reward which the agent gets when the car passes the target position at 0.45, which is also provided to
our agent upon successful termination.
5
Under review as a conference paper at ICLR 2021
Table 1: Comparison of the average returns
Environment	Original	A	B
MountainCarContinuous -v0	94	94	94
Pendulum-v0	-152	-158	-170
CartPole-v1	494	482	472
In Scenario B, at each time instant we either have no observation or we obtain the original observation
vector, i.e. Xt = Xt and Xt = Xt. These cases correspond to βt = ∞ and β = 0, respectively. The
reward function is given as rt = r + KB X g(βt) where KB = 0.5; and g(βt) = -1 for βt = 0, and
0 otherwise. In the implementation, We have mapped ∞ to 1, i.e. the decision variable is βt ∈ {0,1},
hence βt = 1 corresponds to not obtaining a sample in Scenario B.
RL algorithm: We adopt a deep RL setting, combining reinforcement learning with deep learn-
ing using the policy-based approach Trust Region Policy Optimization (TRPO) (Schulman et al.,
2015; Hill et al., 2018). The parameters are kept constant for all experiments and are provided in
Appendix A.3. For Scenario A, at each time step, noisy observations obtained at that time step are
fed to the algorithm as the observations. For Scenario B, the last acquired observation is fed to the
algorithm as the observation at that time step.
Plots: Unless otherwise stated, all results are reported as averages (such as average cumulative
rewards and average βti) using 1000 episodes. For the plots, observation space is mapped to a grid
with uniform intervals. Averages are taken with respect to the number of visits to each given range
of the observation state. For example, for Scenario A the average of βi when Xt ∈ [-0.1, +0.1] is
shown as one average value at the center 0. For Scenario B, we report the sample skip frequency, i.e.
the number of times the agent decided not to acquire a new observation when the last observed state of
the agent falls into a given interval, such as the average sample skip frequency for X ∈ [-0.1, +0.1]
is reported as one value at 0. In all 2-D plots, the color pink indicates there was no visit to that
observation state.
4.2	Overview
We benchmark our results against the performance of the agent that use the original observations, and
trained using the same RL algorithm. The resulting average cumulative rewards in terms of rt are
presented in Table 1. We present the reward corresponding only to the original task so that we can
evaluate the success of the agent in this task. These results illustrate that the agent can learn to adjust
the accuracy level and still obtain successful performance. For the Mountain car environment, all
agents have the same average return and for the others, the agents working with the noisy/skipped
observations have a slightly weaker performance but still achieve the task of bringing/keeping the
pendulum/pole in a vertical position in a reasonable number of time steps.
At first sight, it may be surprising that the agent can learn to perform these tasks satisfactorily even if
we have not injected any memory to our algorithm, for instance when we only use the current noisy
observations for Scenario A. On the other hand, note that in these environments the observations
are either noisy versions of hidden states which govern the dynamics or they are closely related to
them. From the point of the agent that treats the noisy observations as state this can be interpreted as
a configurable MDP (Metelli et al., 2018; Silva et al., 2019) where the agent controls the noise of the
dynamics. Hence, the task of the agent can be interpreted as adjusting the noise level in the dynamics
which does not necessarily require usage of memory in the decision maker.
We now focus on the data collection strategies chosen by the agent for the mountain car and pendulum
environments. The results for the other environments are provided in the appendix.
4.3	Mountain Car
The chosen noise levels and the sample skip frequencies for the mountain car environment are
presented in Figure 1-2. Note that in Figure 1c, we present the sample skip frequency with respect to
the velocity and the position on the same plot, where the legend also gives the corresponding X-axis
label. In the mountain car environment, the car starts randomly around position -0.5 and it has to
6
Under review as a conference paper at ICLR 2021
0 5 0 5
5 4*3
■ ■ ■ ■
O O O O
->9π0N
-1.25 -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50
Car Position
(a) Scenario A, noise levels Vs Xt
-0.08 -0.06 -0.04 -0.02 0.00 0.02 0.04 0.06 0.08
CarVeIocity
∕ι 、 C	∙	∙	U
(b) Scenario A, noise levels Vs Xt
CarVeIocity
-0.02 -0.01 0.00 0.01 0.02 O.O3 0.04 0.05
6 42。 £ 6
5 5 5 5 4 4
■ ■ ■ ■ ■ ■
O O O O O O
x□unbaH d&s-dEs
-1.0	-0.8	-0.6	-0.4	-0.2	0.0	0.2	0.4
Car Position
(c) Scenario B
Figure 1:	Mountain car, noise levels or sample skip frequency versus one observation type
⅞o-ω>^rou
Noise Level (Car Position)
0.08
0.06
0.04
0.02
0.00
-0.02
-0.04
-0.06
-0.08
-1.25 -1.∞ -0.75 -0.50 -0.25 0.00 0.25 0.50
Car Position
0.6
■0.0
-0.02
-0.04
-0.06
-0.08
至uo">」"u
(b) Scenario A, velocity noise
Sample Skip Frequency
⅞o-ω>^rou
0.8
0.7
0.6
■0.5
0.4
0.3
0.2
0.1
0.0
(c) Scenario B
(a) Scenario A, position noise
Figure 2:	Mountain car, noise levels or skip frequencies over the whole observation space
first go in the reverse direction (corresponding to a negative velocity) to climb the hill located around
position -1.25 in order to gain momentum and climb to hill at the right (corresponding to a positive
velocity) and reach the target location 0.45 which is at the top of this hill. The results reflect some of
the trade-offs in this strategy:
Figure 1a shows that most noisy observations in position and velocity (Scenario A) are preferred
around -0.5 (where the car position is initialized), and the most accurate samples are taken when the
car is around position -1.2. This is the position where the car has to make sure that it has reached
to the top of the left hill so that it has enough momentum to climb the right hill. In the case of the
dependence of the noise level on the velocity, Figure 1b shows that accurate samples are preferred
when the velocity has high positive values. We note that this is not the only viable observation strategy
and there are multiple observation strategies that give approximately the same average return in the
original task. These can be explored using different Q and κ values in our framework.
Figure 1c shows that approximately half of the samples are dropped in Scenario B regardless of the
observation state, suggesting a high inherent sampling rate in the environment. This difference in
the behaviour with the noisy and skipped observations illustrates the fundamental difference in these
frameworks. In the case of noisy observations, the agent has to discover that the observations are
uncertain and counteract this uncertainty. On the other hand, when taking perfect observations are
possible, as in the case of Scenario B, the agent can internalize the exact environment dynamics
(since mountain car environment has no inherent noise in its observations) and determine its exact
state using the previous observed state and its action.
Comparing Figure 2a-2b with Figure 2c, we observe that in the case of noisy observations a larger
part of observation space is visited, which is partly due the fact that the plots are drawn according
to the observations acquired by the agent and not the true states. Note that this does not affect the
performance in the original task, as illustrated in Table 1.
7
Under review as a conference paper at ICLR 2021
876543210
■ ■ ■ ■ ■ ■ ■ ■ ■
ððððððððð
->9πON
/ 、C	. A .	λ
(a) Scenario A, noise levels vs θ
ι-∙- See. A, Angle Noise
[τ- See. A, Angular Velocity Noise
-a,>9πa,-ON
-7.5	-5.0	-2.5	0.0	2.5	5.0	7.5
Angular Velocity
(b) Scenario A, noise levels vs θ
U 5 O 5 O 5
4 3 3 2 2 1
■ ■ ■ ■ ■ ■
U O O O O O
AUUanbaId d&sdEs
(c) Scenario B
*
Figure 3:	Pendulum, noise levels or sample skip frequency versus one observation type
Noise Level (Angle)
(a) Scenario A, position noise
Noise Level (AnguIarVeIocity)
X46o-α,> ∙l"-n6uv
-3	-2	-1	6 i 2	3
Angle
(b) Scenario A, velocity noise
-0.0
Sample Skip Frequency
X46o-a,> ∙l"-n6uv
-3	-2	-1	0	1	2	3
Angle
-0.6
■0.5
0.4
I-0.3
-0.2
-0.1
—Lo.o
(c) Scenario B
Figure 4:	Pendulum, noise levels or skip frequencies over the whole observation space
4.4 Pendulum
The results for the pendulum are presented in Figure 3-4. Here, the task is to keep the pendulum at
a vertical position, corresponding to an angle of 0. Figure 3a and Figure 4a show that observations
with low position (i.e. angle) noise (Scenario A) are preferred when the pendulum is close to the
vertical position and has relatively small angular velocity. On the other hand, when the samples can
be completely skipped (Scenario B), the agent skips a large ratio of the samples in this region, as
shown in Figure 3c and Figure 4c. Note that the agent spends most of the episode in this target region
in the vertical position. Here, the agent prefers noiseless samples since a noisy sample may cause the
control policy to choose a wild movement which might destabilize the pendulum. On the other hand,
the agent may safely skip some of the samples at the upright position as the last sample is very close
to current one because the angular velocity is typically low.
5	Discussion and Conclusions
We have proposed a framework for revealing the information structure of the observation space in a
systematic manner. We have adopted a reinforcement learning approach which utilizes a cost function
which increases with the accuracy of the observations. Our results uncover the relative usefulness of
different types of observations and the trade-offs within; and provide insights for further design of
active data acquisition schemes for autonomous decision making. Further discussion of our results
and some research directions are as follows:
•	Our results illustrate that settings with the inaccurate observations and skipped observations
should be treated differently since the type of uncertainty that the agent has to counteract in
these settings are inherently different.
•	Strategies for processing of the noisy/skipped observations should be investigated. Questions
such as the following arise: “ Should all the processing be off-loaded to the RL agent or
8
Under review as a conference paper at ICLR 2021
should the pre-processing of observations be performed, similar to Kalman filtering in the
case of linear control under linear state space models (Ljung, 1999)?”, “How does the answer
to the former question depend on the RL approach, the environment and the observation
models?”
•	Our results suggest that inherent sampling rate of some of the standard RL environments may
be higher than needed (for instance, see the Mountain Car environment where on average
one can skip one out of every two samples without affecting the performance), indicating
yet-another reason why some of these environments are seen as unchallenging for most of
the state-of-art RL algorithms.
•	We have provided a quantification of the sensitivity of the agent’s performance to
noisy/skipped observations at different observation regions illustrating that this sensitivity
can be quite different based on the observation region. Utilizing this information for sup-
porting robust designs as well as preparing adversarial examples is an interesting line of
future research.
9
Under review as a conference paper at ICLR 2021
References
Ershad Banijamali, Rui Shu, Mohammad Ghavamzadeh, Hung Hai Bui, and Ali Ghodsi. Robust
locally-linear controllable embedding. Inter. Conf. on Artificial Intelligence and Statistics, AISTATS,
84:1751-1759, 2018.
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.
Unifying count-based exploration and intrinsic motivation. Advances in Neural Information
Processing Systems, pp. 1471-1479, 2016.
J. Boedecker, J. T. SPringenberg, J. Wulfing, and M. Riedmiller. Approximate real-time optimal
control based on sparse Gaussian process models. In 2014 IEEE Symp. on Adaptive Dynamic
Program. and Reinforcement Learning (ADPRL), 2014.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Adv. in Neural Information
Processing Systems 31, pp. 4754-4765. 2018.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley, 1991.
Yann-Michael De Hauwere, Peter Vrancx, and Ann Nowe. Learning multi-agent state space rep-
resentations. In Proc.of the 9th Inter. Conf. on Autonomous Agents and Multiagent Systems, pp.
715-722, 2010.
Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: A Model-Based and Data-Efficient
Approach to Policy Search. Proc. of the International Conference on Machine Learning, 2011.
Pinar Donmez, Jaime G. Carbonell, and Jeff G. Schneider. A probabilistic framework to learn from
multiple annotators with time-varying accuracy. In Proc. of the SIAM International Conference on
Data Mining, pp. 826-837, 2010.
Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving PILCO with Bayesian neural
network dynamics models. In Data-Efficient Machine Learning workshop, ICML, 2016.
Andrea Goldsmith. Wireless Communications. Cambridge University Press, 2005.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. Proceedings of Machine Learning
Research, 97:2555-2565, 2019.
E. A. Hansen. Markov decision processes with observation costs. Technical Report 97-01, University
of Massachusetts at Amherst, 1997.
Mikael Henaff, William F. Whitney, and Yann LeCun. Model-based planning with discrete and
continuous actions, 2018.
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/
hill-a/stable-baselines, 2018.
Lennart Ljung. System Identification. Prentice-Hall, 1999.
Manuel Lopes, Francisco S. Melo, and Luis Montesano. Active learning for reward estimation in
inverse reinforcement learning. In European Conference on Machine Learning and Knowledge
Discovery in Databases, ECML, pp. 31-46, 2009.
Francisco S. Melo and Manuela Veloso. Learning of coordination: Exploiting sparse interactions in
multiagent systems. In Proceedings of The 8th Inter. Conf. on Autonomous Agents and Multiagent
Systems, pp. 773-780, 2009.
10
Under review as a conference paper at ICLR 2021
Alberto Maria Metelli, Mirco Mutti, and Marcello Restelli. Configurable Markov decision processes.
Proc. of Machine Learning Research (PMLR),pp. 3491-3500, 2018.
Shakir Mohamed and Danilo J. Rezende. Variational information maximisation for intrinsically
motivated reinforcement learning. pp. 2125-2133, 2015.
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proc. of the International Conference on Machine Learning, pp.
2778-2787, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region
policy optimization. In Proc. of the 32nd International Conference on Machine Learning, pp.
1889-1897, 2015.
Sarah SchWobeL Stefan KiebeL and Dimitrije Markovic. Active Inference, Belief Propagation, and
the Bethe Approximation. Neural Computation, 30(9):2530-2567, September 2018.
Burr Settles. From theories to queries: Active learning in practice. In Active Learning and Experi-
mental Design workshop - AISTATS, 2010.
Rui Silva, Gabriele Farina, Francisco S. Melo, and Manuela Veloso. A theoretical and algorithmic
analysis of configurable mdps. Proc. of the Inter. Conf. on Automated Planning and Scheduling,
29:455-463, Jul. 2019.
Susanne Still and Doina Precup. An information-theoretic approach to curiosity-driven reinforcement
learning. Theory of Bioscience, 131(3):139-148, 2012.
Kai Ueltzhoffer. Deep Active Inference. Biological Cybernetics,112(6):547-573, DeCember 2018.
Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, and Martin A. Riedmiller. Embed
to control: A locally linear latent dynamics model for control from raW images. In Advances in
Neural Information Processing Systems, pp. 2746-2754, 2015.
Valentina Bayer Zubek and Thomas G. Dietterich. A POMDP approximation algorithm that antic-
ipates the need to observe. In PRICAI 6th Pacific Rim Inter. Conf. on Artificial Intelligence, pp.
521-532, 2000.
Valentina Bayer Zubek and Thomas G. Dietterich. Pruning improves heuristic search for cost-sensitive
learning. In Proc. of the 32nd Inter. Conf. on Machine Learning, pp. 19-26, 2002.
11
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Example: Wireless Communications
We now provide a motivating example to illustrate how observations can have a cost that is increasing
with the accuracy and the decision maker can choose this accuracy level.
A standard model for single terminal wireless communications is the additive white Gaussian noise
(AWGN) channel (Goldsmith, 2005; Cover & Thomas, 1991)
yt = xt + vt	(11)
where xt represents the channel input (i.e. message at the transmitter ) at time t, yt represents the
corresponding channel output (i.e. the observation at the receiver) and the white Gaussian random
process vt represents the channel noise. The capacity of this channel, i.e. the maximum number of
information bits that can be sent, is determined by the signal-to-noise ratio (SNR), i.e. the average
power in xt divided by the average power in vt . In particular, the capacity is given by (Goldsmith,
2005; Cover & Thomas, 1991)
P
C = i0g2(i + τx)	(12)
where Px and Pv are the average power levels of xt and vt , respectively. Hence, the capacity increases
with Px. On the other hand, one cannot use a very high value of Px since broadcasting at high power
levels is costly. In particular, PX directly contributes to the actual power required by the transmitter.
Note that PX controls the accuracy of the observations. In particular, by dividing both sides by √PX,
(11) can be equivalently represented as
y = Xt + Vt
(13)
where yt，√= yt, Xt，√= Xt and Vt，√= vt. The average power of Xt is 1 and average
power of Vt is Pv /Pχ. The SNR, and hence, the channel capacity are the same in (11) and (13) and
hence these representations are equivalent for all relevant purposes. In particular, determining PX
directly determines the effective noise level. With Vt Gaussian, we have Vt 〜N(0, Pv). Hence, the
conditional distribution of the observations yt is given by p(yt∣Xt) = N (Xt,Pv/PX) where Pv /Pχ
can be chosen as βt. Hence, as the accuracy of the observations increases (Pv/PX decreases ), the
cost of the observations (PX) increases. In this context, several interesting questions that relates to the
accuracy of the observations and the power cost can be posed, for instance how to distribute a certain
total power budget Ptotal over channels yti = Xit + Vti with different intrinsic power levels Pvi .
This example illustrates the basic premise of our problem setting in a practical scenario; a decision
maker who can adjust the noise levels of the observations which has a cost associated with them. It
also suggests that the constraints on the wireless communications constitute a general and potential
hindrance in remote control applications. Consider a device that makes the observations and takes
actions but gets its commands (i.e. decisions about which actions to take) from another decision
unit, such as the control of a robot or a drone by a remotely run RL algorithm which is controlling a
large number of such units. Here, it is beneficial to consider policies that can work with inaccurate
observations since sending accurate measurements are costly from a power perspective, which will be
particularly important for a device with a limited battery, such as a drone flying at a remote location.
Similarly, if the wireless communication channel cannot be used at all times, for instance, due to
the limited bandwidth available, RL methods that can utilize the limited communication resources
efficiently and optimize performance under such conditions are needed.
A.2 Environment Parameters
In this section, we provide the parameters for all the environments in the experiments that are used
directly from OpenAI Gym. We also consider a vertical position version of MountainCarContinuous-
v0, which is explained in Section A.4.
Consider a generic environment with the observation variables oit, where oit denotes the ith observation
variable at time t. The limited-accuracy observations 0； are obtained using
0t = ot + Qi × ∆ot(βi)
(14)
12
Under review as a conference paper at ICLR 2021
Table 2: Environment parameters, reward weighting for different scenarios
Environment	κA, κB
MountainCarContinuous-v0	5 × 10-6, 0.5
Pendulum-v0	1, 0.2
CartPole-v 1	0.2, 0.04
Table 3: Hyperparameters of the TRPO algorithm
Parameter	Value
Gradient dampening factor	2.35E-05
Weight for the entropy loss	0.01118
Gamma	0.98
GAE factor	0.9
Kullback-Leibler loss threshold	0.000193
No. of timesteps to run per batch	1024
No. iters. for learning for value func	10
Stepsize of value func.	0.00428
where ∆oi 〜U(-βt, βt). We choose Q1 = 0.1 and Q2 = 0.2 for the PendUlUm-v0, Qi = 0.2 for
the CartPole-v1, and Qi = 0.1 for the MountainCarContinuous-v0. The ordering of the observations
is the same with the ones provided in OpenAI Gym (Brockman et al., 2016). For instance, for
MoUntainCarContinUoUs-v0, position and velocity correspond to o1 and o2, respectively. Note that
indices start with i = 0 in OpenAI Gym whereas here we start with i = 1.
The reward fUnction Under Scenario A is given by
rt=Tt+KA × (n X βi),	(15)
where r is the original reward and KA > 0. For Scenario B, it is given by Tt = r + KB X g(βt)
where g(βt) = -1 for βt = 0, and 0 otherwise. The associated K values for different environments
are presented in Table 2.
The scaling factor Q’s for the noise levels and K valUes for the reward fUnction are determined
empirically by first fixing Q (as a percentage of the fUll range of the associated observation) and
searching for K valUes that provide satisfactory performance in the original task. Note that the rest
of the valUes are determined by the specifications of the environments in OpenAI Gym. The resUlts
depend on the valUes of Q and K. For instance, Using larger K pUts a larger weight on the reward dUe
to noise. Hence, the agent prioritizes the reward dUe to noise instead of the reward from the original
environment and, for large enoUgh K valUes, the agent cannot learn to perform the original task.
A.3 TRPO parameters
The same TRPO parameters are Used in all experiments. These are provided in Table 3.
A.4 Mountain Car with Observations of the Vertical Position
To have a better Understanding of the effect of partial observability, we have investigated the following
modification on MoUntainCarContinUoUs-v0: Instead of the horizontal position, the agent Uses the
vertical position as the observation. Hence, the observations are given by
yt = yt + Qy X ∆yt(β1),	(16a)
Xt = Xt + Qx X ∆Xt(β2),	(16b)
where the vertical position yt ∈ [0.1, 1] is given by yt = 0.45 sin(3xt) + 0.55 (Brockman et al.,
2016) and ∆yt(β1)〜U(-β1,β1) and ∆Xt(β2)〜U(-β2,β2). Note that due to sin(∙) function,
13
Under review as a conference paper at ICLR 2021
-a,>9ηa,-ON
0.1
0：0	0.2 OA
Car Vertical Position
(a) Scenario A, noise levels vs yt
-α,>9η ωw-oz
0.25-
-θ'.∞ -θ'θβ -0.04 -0.02 0.00 0.02 0.04 0.06 0.08
CarVeIocity
∕ι 、C	-	-	U
(b) Scenario A, noise levels vs Xt
Figure 5:	Mountain car with vertical position observation, noise levels versus one observation type
Noise Level (Car Velocity)
-0.02
-0.04
-0.06
-0.08
Noise Level (CarVerticaI Position)
>⅛uo-ω>^rou
(a) vertical position noise
-0.02 -
-0.04-
-0.06
-0.08
0.2	0.4	0.6	0.8	1.0
Car Vertical Position
(b) velocity noise
Figure 6:	Mountain car with vertical position observation, noise levels over the observation space
for most of the yt values in the range [0.1, 1], there are two possible horizontal position (xt) values.
Hence, this environment constitutes a POMDP even without any observation noise. Similar to our
experiments with the original environment, Qy and Qx are set as the 0.1 times of the full range of
the corresponding observation, i.e., Qx = 0.09 and Qx = 0.014. As before, the reward is calculated
with (10) with κA = 5 × 10-6.
The average return due to the original task is 93, hence the agent again learns to perform the original
task successfully, see Table 1 for comparison. The chosen noise levels are presented in Figure 5-6.
Comparing these results with Figure 1-2 where the agent takes the horizontal position observation,
we observe that the general trend of the velocity noise with respect to the velocity are the same in
both settings, i.e. decreasing as the agent moves from the negative velocities to positive velocities.
Comparing Figure 5 with Figure 1, we observe that lower relative noise levels are preferred for the
setting with the vertical location observations.
A.5 Additional Results -Cart Pole
We now provide the results for the cart pole environment in Figure 7-10, which were not included in
the main text due to page limitations. For the sake of brevity, the noise levels over observations pairs
is only provided for the position noise levels whereas averages are provided for all observation types.
14
Under review as a conference paper at ICLR 2021
0.20
0.15
5 0 5
3 3 2
■ ■ ■
Ooo
->9πON
-2	-10	1	2
Cart Position
-a,>9πa,-ON
0.25
0.20
0.15
-2-16 i 2
Cart Velocity
0.6-
0.5-
0.4-
0.3-
-a,>9πa,-ON
0.2 ∙
0.1
-0.2	-0.1	0Λ
Pole Angle
0.2
(a)	Scenario A, noise vs position
(b)	Scenario A, noise vs velocity
(c)	Scenario A, noise vs angle
Figure 7:	Cart pole, noise levels versus one observation type
-a,>9πa,-ON
0.0
-1.5	-1.0	-0.5	65 IQ L5
Pole Angular Velocity
Cart velocity
0.30
AUUanbaH d2s-dE"s
AUUanbaH d&sα,-dE"s
(a) Scenario A, noise vs angular ve-(b) Scenario B, sampling frequency (c) Scenario B, sampling frequency
locity	vs position and velocity	vs angle and angular frequency
Figure 8:	Cart pole, noise levels or sample skip frequency versus one observation type
-6uv-0d-6uv-0d
Noise Level (Cart Position)
Noise Level (Cart Position)
1.5-
1.0-
0.5-
0.0-
-0.5-
-I-O-
>u-uo-ω> ,l-n6uva,-0d
0.8
0.6
0.4
0.2
■0.0
1.0
Noise Level (Cart Position)
⅛-uo-ω> trou
-2	-10	1	2
Cart Position
Noise Level (Cart Position)
0.6
■0.0
至Oyg ∙l-n6uv-Od
至Oyg ∙l-n6uv-Od
Figure 9:	Cart Pole, Scenario A, noise levels for the position over the pairs of the observation variables
15
Under review as a conference paper at ICLR 2021
Sample Skip Frequency
Pole Angle	Pole Angle
0.20
0.15
0.10
0.05
0.00
-0.05
-0.10
-0.15
-0.20
Sample Skip Frequency
&PO-g ∙l-n6uv-Od
Sample Skip Frequency
0.7
0.6
-0.5
0.4
0.3
0.2
0.1
>⅛uo-ω> trou
Sample Skip Frequency
0.7
0.6
■0.5
0.4
l-lo.o
0.8
X4g-a,> ∙l-n6ua-0d
Sample Skip Frequency
工
-2	-10	1	2
CarVeIocity
X4g-α,> ∙l-n6ua-0d
Sample Skip Frequency
-0.20 -0.15 -0.10 -0.05 0.00 0.05 0.10 0.15 0.20
Pole Angle
0.8
0.7
・0.6
■0.5
0.4
0.3
-0.2
0.1
0.0
9

Figure 10:	Cart Pole, Scenario B, sample skip frequencies over the pairs of the observation variables
16