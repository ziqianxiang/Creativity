Under review as a conference paper at ICLR 2021
A New Accelerated Gradient Method Inspired by Continuous-
Time Perspective
Anonymous authors
Paper under double-blind review
Abstract
Nesterov’s accelerated method are widely used in problems with machine learning
background including deep learning. To give more insight about the acceleration
phenomenon, an ordinary differential equation was obtained from Nesterov’s accel-
erated method by taking step sizes approaching zero, and the relationship between
Nesterov’s method and the differential equation is still of research interest. In this
work, we give the precise order of the iterations of Nesterov’s accelerated method
converging to the solution of derived differential equation as step sizes go to zero.
We then present a new accelerated method with higher order. The new method
is more stable than ordinary method for large step size and converges faster. We
further apply the new method to matrix completion problem and show its better
performance through numerical experiments.
1 Introduction
Optimization is a core component of statistic and machine learning problems. Recently, gradient-
based algorithms are widely used in such optimization problems due to its simplicity and efficiency
for large-scale situations. For solving convex optimization problem
min F (x),
x∈Rd
where F (x) is convex and sufficiently smooth, a classical first-order method is gradient descent.
We assume that f (x) = VF(x) satisfies L-Lipschitz condition, that is, there exists constant L such
that
kf (x) - f(y)k ≤ Lkx - yk,	∀x, y.
Under these conditions, gradient descent achieves a convergence rate of O(n-1), i.e., kF(xn) -
F(x^)Il decreases to zero at a rate of O(n- 1), where xn denotes the nth iteration and x^ denotes
the minimum point of F(x) in Rd.
Nesterov’s accelerated method (Nesterov, 1983) is a more efficient first-order algorithm than gra-
dient descent, of which we will use the following form: starting with x0 = x1 ,
yn = xn +
n-3
(xn
n
- xn-1),
xn+1 = yn - sf(yn)
(1.1)
for n ≥ 1. It is shown that under abovementioned conditions, Nesterov’s accelerated method
converges at a rate of O(n-2).
Accelerated gradient method has been successful in training deep and recurrent neural networks
(Sutskever et al., 2013) and is widely used in problems with machine learning background to avoid
1
Under review as a conference paper at ICLR 2021
sophisticated second-order methods (Cotter et al., 2011; Hu et al., 2009; Ji & Ye, 2009). To provide
more theorical understanding, an important research topic of Nesterov’s accelerated method is
to find an explanation of the acceleration. On this topic, Nesterov’s method was studied via a
continuous-time perspective (Su et al., 2014). They considered a curve x(t), introduced the ansatz
Xn ≈ x(ny∕s) and substituted it to (1.1). Letting S → 0, they obtained the following differential
equation.
3
x + qx + f (x) = 0.	(1.2)
The differential equation was used as a tool for analyzing and generalizing Nesterov’s scheme.
Furthermore, this idea has been studied from different directions. A class of accelerated methods
have been generated in continuous-time (Wibisono et al., 2016). ODE (1.2) can also be discretized
directly using Runge-Kutta method to achieve acceleration (Zhang et al., 2018).
Although many results have been achieved, the process of obtaining the differential equation (1.2)
has not been rigorous, and the method is still time-consuming for large-scale problems. In this
work, we give the precise order of the iterations of Nesterov’s accelerated method converging to
solution of the differential equation (1.2) with initial conditions
x(0) = xo, x(0) = 0	(1.3)
as step size S goes to zero. Inspired from this perspective, we present a new accelerated method
to make this convergence faster. As we expected, iterations of the new method are closer to the
solution x(t) of differential equation (1.2) than original Nesterov’s method. Moreover, we find the
new method is more stable than original Nesterov’s method when step size is large.
Based on abovementioned observations, we try to take advantage of the new method in more
practical problems. We apply the new method to matrix completion problem. We combine the
new method with proximal operator (Parikh & Boyd, 2014) into a new algorithm, which we call
modified FISTA. We find that the new method performs better than FISTA (Beck & Teboulle,
2009) and acclerated proximal gradient method (Parikh & Boyd, 2014) because it can work with
larger step sizes.
This paper is organized as follows. In section 2, we prove that iterations of Nesterov’s accelerated
method converge to solution of the differential equation (1.2). In section 3, we present a new
method to make the convergence faster and show its better stablity through two simple examples.
In section 4, we apply the new method to matrix completion problem.
2 A strict analysis of the relation between Nesterov’s method and its
continuous-time limit
We refer to x(t) as the solution of differential equation (1.2) with initial conditions (1.3). Existance
and uniqueness of such solutions have been proved (Su et al., 2014). In this section, We give the
order of the iterations of Nesterov’s accelerated method converging to x(t) as step sizes go to zero.
For convenience, we substitute the first equation in Nesterov’s method (1.1) to the second one to
get
n-3	n-3
xn +1 = xn +	(xn - xn-1 ) - S ∙ f I xn +	(xn - xn-1 ) I ∙
nn
We write S = h2 and rewrite the above recurrence relation as
n-3	2	n-3
xn +1 = xn +	(xn - xn-1) - h ∙ f I xn +	(xn - xn-1 ) I ♦
(2.1)
n
n
2
Under review as a conference paper at ICLR 2021
Inspired by the ansatz Xn ≈ x(nyJ^S) (Su et al., 2014), We consider the convergence between xu and
x(nh). More precisely, we show that for fixed time t, xn converges to x(t) as h goes to zero, where
n = h.
2.1 Truncation error
Firstly, we consider the following ‘truncation error’:
L[x(t); h] = x(t + h) — " t 3'x(t) + -~~~~x(t — h)+
h2 f (X(t) + --t^- (x(t) — x(t — h))).
(2.2)
(2.2) is obtained from (2.1) by replacing xn+1, xn, xn-1 with x(t+h), x(t), x(t—h) and substituting
the relation n = h. Our first result is the order of truncation error L[x(t); h].
Theorem 1. Assume f satisfies L-Lipschitz condition, and solution x(t) of the derived differential
equation (1.2) has a continuous third derivative. For fixed time t, the truncation error (2.2) satisfies
L[x(t); h] = O(h3).
Theorem 1 shows the size of error caused by a single iteration when the starting point is just on
x(t). Then we have to add up these errors to prove the convergence proporty we need.
2.2 Convergence theorem
We now come to the convergence theorem. In this theorem, we give the precise order of the iterations
of Nesterov’s method converging to solution of the derived differential equation.
Theorem 2.	Under conditions in Theorem 1, for fixed time t, xt/h converges to x(t) as h goes to
zero at a rate of O(hln h) if x0 = x(0) and xι = x(h).
Theorem 2 coincides with derivation of ODE (1.2) (Su et al., 2014).
3 New accelerated method
3.1	Derivation of the new method and analysis of truncation error
Inspired from the continuous-time perspective and our proof of the convergence from iterations of
Nesterov’s method to its continuous-time limit, we present a new method to make this convergence
faster. Precisely, the new method has a higher truncation order.
We need one more step in our scheme than in Nesterov’s method to achieve higher truncation order
in the following analysis, so we consider a recurrence relation with form
X (山 + β + γ2) xn+2--f (xn + n- (xn - xn-l)
=1	n	n2	n
(3.1)
where {α}, {β} and {γ} are to be determined.
3
Under review as a conference paper at ICLR 2021
Now we expand x(t - h) to first order. Calculation shows that
f (x(t) + -~t^-(x(t) — x(t — h))) = — hx(3)(t) — (ɪ + ]) x(2)(t)
+ (∣2— t) x (1)( t)+O (h 2).
Substitute this expansion to truncation error
4
L[x(t); h] =X
i=1
x(t + (2 — i)h)
+ h2f
+ --t^- (x (t) — x (t —
and choose parameters appropriately to eliminate low-order terms, we get the following recurrence
relation	10n2 +9n+6	4n2 +3	2n— 1 xn + 1 =			xn			xn _1 T	xn —2 + 1	4n2 +8n	2n2 + 4n n 1	4n + 8	3 n	2n — 3	n — 3	(3.2) —2n+4Sf ∖~^xn — ^rxn―1>
Here we rewrite this scheme as Algorithm 1.
Algorithm 1 The new method (3.2)
Input: step size s
Initial value: X2 = X1 = X0 .
(k - 1)th iteration (k ≥ 2). Compute
Yk =	10k2 +9k+6	4k2 +3	2k— 1 4 k 2 + 8 k	k — 2 k2 +4 k Xk- 1 + 4k÷^8 Xk-22
	2k — 3	k — 3
Zk =	"V^	"V^ -k- Xk	k- Xk- 1 2
Xk+1	=X - (yk - 2⅛ 以 Zk)).
For truncation order of this new method, we have the following theorem. The abovementioned
procedure is presented in Appendix A.4 detailedly, as proof of Theorem 3.
Theorem 3.	If f has continuous second order derivative, the first and second derivative are bounded,
and x(t) has continuous fourth derivative, then for fixed t, truncation error of (3.2) satisfies
L[x(tn); h] = O(h4).
The convergence of the new method and x(t) can be proved similar to Theorem 2.
3.2	Advantage of the new method
Since the new method has a truncation error of higher order than original Neaterov’s method,
the iterations of the new method converge to the differential equation (1.2) when those of original
4
Under review as a conference paper at ICLR 2021
Nesterov’s method diverge. In another word, the new method is more stable for large step size. We
present two numerical results in Figure 1 to confirm it.
Quadratic. F(x) = xTAx is a strongly convex function, in which x ∈ R2 and A is a 2 × 2 matrix.
n
Linear regression. F(x) =	(yi — wiT x)2 , where n is the number of samples and (wi, yi) is the
i=1
ith sample.
10-1o0
„8
-t⅛l⅛-
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
(a) min F (x) = xTAx
■"tfs
0 5 0
'∙,ww
-∙⅛l⅛.-
10-5
0	50	100	150	200	250	300	350 400 450	500
n
n
(b) min F(x) = P (yi - wiTx)2
i=1
Figure 1: Iterations of original Nesterov’s method (Nesterov) and the new method (New method)
for quodratic and linear regression objective function. Y-axis represents the gap ∣F(xn) — F(x^)|.
In Figure 1(a), step size s = 0.03705; in Figure 1(b), step size s = 0.00565.
In these examples, iterations of the new method converge to the minimum point, while those of
original Nesterov’s method diverge, which confirms that the new method is more stable for large
step size.
3.3	Absolute stability of Nesterov’s method and the new method
In this subsection, we explain the better stability of the new method with absolute stability theory.
Firstly, recall the scheme of our new method
10n2 +9n+6
Xn + 1 =-------X，
n+1	4 n2 +8 n
4n2 + 3	2n — 1
--------Xn -1 +-------Xn ―〕
2 n2 + 4n n 1	4n + 8 n 2
—-^sf 2n―^x X.
2n + 4	n
n—3
Xn-1
n
We use linear approximation
f Xn +
^F∖Xn +
≈V 2 F ∙ Xn +
and the characteristic equation of this finite scheme is approximately
10n2 + 9n + 6	2n2 — 3n	4n2 + 3	n2 — 3n	2n — 1
λ3 —	—ɪ—	— s ∙ V2 F .—R---- λ2 + k≡----------s W2 F .—R------ λ----------= 0.
4n2 + 8n	2n2 + 4n	2n2 + 4n	2n2 + 4n	4n + 8
For large n, we can ignore the high order terms and the characteristic equation becomes
λ3 — £ — s ∙ V2F) λ2 +(2 — 2 ∙ V2F) λ — 2=0.
5
Under review as a conference paper at ICLR 2021
According to the absolute stability theory, the numerical stability of Nesterov’s scheme with respect
to accumulated roundoff error is equivalent to this: all the roots of the characteristic equation lie
in the unit circle (Leader, 2004). Noticing that the left hand of the equation can be factorized to
(X 一 2) (λ2 -(2 — s ∙ V2F)λ + 1),
the largest modulu of the roots is 1 When 0 ≤ S ∙ VVF ≤ 4, and the absolutely stable region of the
new method is S ∙ V2F ∈ [0,4].
When S∙V2F lies in the absoletely stable region, the related theory guarantees that the error caused
by every iteration will not be magnified as the iteration number increases. To make the analysis
more precise, we should consider the difference of the scheme between iterations caused by different
n. We define the transfer matrix
，(10n2 +9n +6 _	vy2 π 2n2 — 3n、	_ ( 4n2+3 14n2+8n	S ∙ V F * 2n2+4n J	-12n2+4n 0	- S ∙ V2 F ∙ n2 -3n )	2n-1 ʌ s v 1	2n2+4n )	4n+8 ∣ 00 10
and Qn = PnPn-1 …Pi. Error analysis shows that if the largest modulu of eigenvalues of Qn
goes to zero, then error caused by iterations will be eliminated as the iteration number increases.
Figure 2 presents the largest module of eigenvalues of Qn for different values of S ∙ V2 F. From the
experiment we can see that the above condition is satisfied.
Figure 2: The largest modulu of eigenvalues of Qn, where S ∙ V2F is chosen to be 1, 2 and 3.
We then apply the same method to Nesterov’s method discussed in (Su et al., 2014) and conclude
that the absolutely stable region of Nesterov,s method is [0, 4].
According to the ab ove analysis, the absolutely stable region of the new method is four times as
large as Nesterov’s method, so the new method is more stable, and we can choose larger step sizes
to achieve faster convergence.
4	Application to matrix completion problem: modified FISTA
Our theory and numerical results show that the new method is more stable than original Nestrov’s
method. So we can choose larger step size for new method and convergence to the optimal solution
6
Under review as a conference paper at ICLR 2021
can be faster, compared with original Nesterov’s method. In this section we apply the new method
to matrix completion problem. We present a new algorithm which can be viewed as a modification
of the well-konwn fast iterative shrinkage-thresholding algorithm (FISTA) (Beck & Teboulle, 2009).
The performance of modified FISTA can also confirm the advantage of the new method.
For matrix completion problem there exists a ‘true’ low rank matrix M . We are given some entries
of M and asked to fill missing entries. There have been various algorithms to solve such problem
(Candes & Recht, 2009; Keshavan et al., 2010). Besides, it is proposed that matrix completion can
be transformed to the following unconstrained optimization problem (Mazumder et al., 2010)
min F (X) = 2 ∣∣X.bs — Mobs ∣∣1 2 + 川 X ∣∣*∙	(4.1)
Notice that F(X) is composed of a smooth term and a non-smooth term, so gradient-based algo-
rithms cannot be used directly. Proximal gradient algorithms (Parikh & Boyd, 2014) are widely
used in such composite optimization problems, and fast iterative shrinkage-thresholding algorithm
(FISTA) is a successful algorithm. Moreover, FISTA has been extended to matrix completion
case (Ji & Ye, 2009). For convenience, We set G(X) = 11lXobs — Mobsk2, H(X) = λ∣∣X∣∣^, and
g (X ) = PG( X).
The idea of FISTA builds on Nesterov’s method. We also apply acclerated proximal gradient
method (Parikh & Boyd, 2014) for our numerical experiment, Which is composed of Nesterov’s
method and proximal gradient descent. These tWo algorithms are presented in Appendix A.5. We
find the performances of them are similar in our experiments.
Our contribution is the third method (Algorithm 2), the neW method (3.2) combined With proximal
operator, Which We call modified FISTA.
Algorithm 2 Modified FISTA
Input: step size s
Initial value: X2 = X1 = X0 ∈ M100 .
(k - 1)th iteration (k ≥ 2). Compute
10k2 +9k+6	4k2 +3	2k — 1
4 k2 + 8 k	k — 2 k2 + 4 k XkT + 4 k + 8 Xk-2
Zk = 2k—3 Xk — " Xk-1,
kk
1	. ∫1 2 k + 4
Xk+ι=argjmin∣2 ∙ k
X—
2⅛ g(Zk)) 12+ λkX kj.
Notice that the minimizing problems in interations of above three algorithms can be solved directly
by singular value decomposition (Cai & Candes, 2010). We take experiments on a simulated data
set. We use fixed step sizes in the above three algorithms, and the performances are presented in
Figure 3. We find empirically that for all methods, convergence is faster When step size is larger, so
We choose the largest step sizes for all methods to compare their fastest convergence speed. Through
experiments, We find the largest step size that makes modified FISTA convergent is 4.1 (accurate
to one decimal place), While those for the first tWo algorithms are both 1.3. We also compare
performances of the three methods With step sizes reduced from the largest in equal proportion.
We find that When step sizes are chosen to be the largest or reduced from the largest in equal
proportion (80%, 50%, 10%), convergence of modified FISTA is faster than the other tWo methods.
7
Under review as a conference paper at ICLR 2021
(a) origilal
(b) reduced to 80%
(c) reduced to 50%
(d) reduced to 10%
Figure 3: Iterations of FISTA, accelerated proximal gradient descent (Nesterov) and modified
FISTA (Modified FISTA) for matrix completion objective function. Y-axis represents the gap
∣F(Xn) — F(x")|. In Figure 3(a), step size is 1.3 for FISTA and accelerated proximal gradient
descent, and 4.1 for modified FISTA. In the other three figures, step sizes are reduced from 1.3 and
4.1 in the proportion marked below the figures.
We also combine the three methods with backtracking (Beck & Teboulle, 2009) to choose step sizes
automatically. We present modified FISTA with backtracking as Algorithm 3, and the other two
algorithms are similar.
Performances of the three algorithms with backtracking on abovementioned data set are presented
in Figure 4. Convergence of modified FISTA is faster than the other two methods. Moreover, we
find that the final step size of modified FISTA is larger.
5	Discussion
In this paper we prove that iterations of Nesterov’s accelerated method converge to solution of
the derived differential equation as step sizes go to zero. We present a new accelerated method
to make this convergence faster. We use numerical results to show that the new method is more
stable, especially for large step sizes, and explan it using the order of truncation error. We then
apply the new method to matrix completion problem and present a new algorithm which we call
modified FISTA. Numerical experiments show that modified FISTA performs better than existing
algorithms based on Nesterov’s acceleration because it can work with larger step sizes. We will also
combine our new method with stochastic gradient-based algorithms and apply the new method to
deep networks in the future.
8
Under review as a conference paper at ICLR 2021
Algorithm 3 Modified FISTA with backtracking
Input: some β < 1
Initial value. X2 = X1 = X0 ∈ M100 , step size s2 .
(k - 1)th iteration (k ≥ 2).
10k2 + 9k + 6	4k2 + 3	2k - 1
k =	4k2 + 8k k - 2k2 + 4k XkT + 4k + 8 Xk-2
2 k - 3	k - 3
Zk =  ； Xk-------；-Xk-1.
kk
Find the smallest positive integer ik+1 such that with s = βik+1 sk
F(f) < F(Yk) + DX - Yk,g(Zk)E + 1 ∙ 2k+4kf - Ykk2,
2 ks
where
f =argmin∕1 ∙ ?4
X 2 ks
X-
ks
2 k + 4
g (Zk D U + λkχ ∣∣* I.

Set sk+1 = βik+1 sk and compute
Xk+1 = Xf.
n
Figure 4: Iterations of FISTA, accelerated proximal gradient descent (Nesterov) and modified
FISTA (Modified FISTA) for matrix completion objective function. Y-axis represents the gap
∣F(Xn) - F(x^) |. Step sizes are chosen by backtracking.
Our work shows that for an accelerated gradient method, the rate at which it converges to the
derived differential equation is possibly related to its property as an optimization algorithm. We
think this work suggests that more consideration should be given to the corresponding differential
equations when studying optimization algorithms.
9
Under review as a conference paper at ICLR 2021
References
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM Journal on Imaging Sciences, 2(1):183-202, 2009.
Jianfeng Cai and Emmanuel J. Candes. A singular value thresholding algorithm for matrix Com-
pletion. SIAM Journal on Optimization, 20(4):1956-1982, 2010.
Emmanuel J. Candes and Benjamin Recht. Exact matrix completion via convex optimization.
Foundations of Computational Mathematics, 9(6):717-772, 2009.
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. Better mini-batch algorithms
via accelerated gradient methods. Advances in Neural Information Processing Systems 24, pp.
1647-1655, 2011.
John M. Holte. Discrete gronwall lemma and applications. MAA-NCS meeting at the University
of North Dakota, 2009.
Chonghai Hu, Weike Pan, and James T. Kwok. Accelerated gradient methods for stochastic op-
timization and online learning. Advances in Neural Information Processing Systems 22, pp.
781-789, 2009.
Shuiwang Ji and Jiepeng Ye. Exact matrix completion via convex optimization. Proceedings of the
26th International Conference on Machine Learning, pp. 457-464, 2009.
Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few
entries. IEEE Transactions on Information Theory, 56(6):2980-2998, 2010.
Jeffery J. Leader. Numerical analysis and scientific computation. 2004.
Rahul Mazumder, Trevor Hastie, and Rob ert Tibshirani. Spectral regularization algorithms for
learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287-2322,
2010.
Yurii E. Nesterov. A method for solving the convex programming problem with convergence rate
O(k-2). Soviet Mathematics Doklady, 27(2):372-376, 1983.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization,
1(3):127-239, 2014.
Weijie Su, Stephen Boyd, and Emmanuel J. Candes. A differential equation for modeling nesterov’s
accelerated gradient method: theory and insights. Advances in Neural Information Processing
Systems 27, pp. 2510-2518, 2014.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initial-
ization and momentum in deep learning. Proceedings of the 30th International Conference on
Machine Learning, pp. 1139-1147, 2013.
Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated
methods in optimization. Proceedings of the National Academy of Sciences, 113(47):E7351-
E7358, 2016.
Jingzhao Zhang, Aryan Mokhtari, Suvrit Sra, and Ali Jadbabaie. Direct runge-kutta discretization
achieves acceleration. Advances in Neural Information Processing Systems 31, pp. 3900-3909,
2018.
10
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Theorem 1
Theorem 1. Assume f satisfies L-Lipschitz condition, and solution x(t) of the derived differential
equation (1.2) has a continuous third derivative. For fixed time t, the truncation error (2.2) satisfies
L[x(t); h] = O(h3).	(A.1)
t — 3h
f (X(t) +——t- (X(t) — X(t —
t—
X (t)+ —
Proof. Notice that
x(t - h) = x(t) + O(h).
Substitute this equation to the last term of L[x(t); h] to get
3 h
Since f satisfies L-Lipschitz condition, we know
f (N(t) + t t3h (N(t) - N(t - h))) = f (x(t)) + O(h)
3
=—x( t) — -x( t) + O (h).
To get the second equality, we substitute the differential equation (1.2). Then we expend the first
and third terms of L[x(t); h] to third order to get
h2
x (t + h) = x (t) + hx (1)( t) + / x (2)( t) + O (h3),
h2
x (t — h) = x (t) — hx (1)( t) + ~2 x (2)( t) + O (h3).
Substitute these three equations to (2.2), we have
L[X(t); h] = O(h3).	□
Remark 1. (A.1) can also be written as
|L[x(t); h]| ≤ M1h3,
where M1 depends on sups≤t |x(1) (s)| and sups≤t |x(3) (s)|.
Remark 2. Theorem 1 deals with the problem for fixed time t. To finish the proof of the convergence,
we have to consider the situation that tn = nh, where n ≥ 1 is fixed.
We set a fixed time t0 and assume that tn = nh < t0. Since x(t) has a continuous third derivative,
x(t) and its first to third derivative are bounded in [0, t0]. We replace time t in the above proof by
tn and expend the terms of (2.2). Now the term
3h3 2
—2-X (2)( tn )
2tn
obtained from the expansion of x(tn-1) cannot be viewed as O(h3), but there exists M2 > 0 such
that
—呢x⑵
2 tn
h2
(tn ) ≤ M2 一 ∙
n
As a consequence, we have
where M1 and M2 rely on t0 .
h2
1L[x(tn); h] | ≤ M 1 h3 + M2 一,
n
(A.2)
11
Under review as a conference paper at ICLR 2021
A.2 Two lemmas for Theorem 2
For the proof of Theorem 2, we need the following two lemmas.
Lemma 1. (Holte, 2009) For constant α, β > 0 and positive sequence {ηn}n≥0 satisfying
n-1
ηn ≤ β + α	ηi , ∀n > 0,
i=0
the following inequality holds
ηn ≤ eαn(β + αη0).
The above lemma is a classic result and refered to as discrete Gronwall inequality.
Lemma 2. We define matrices Cn and Dn,l as
2 2 n _ 1	_ n — 2
Cn = (n+r	— 0+1
Dn,l = CnCn-I …Cn-I +1,
where n ≥ 0 and 0 < l ≤ n + 1. In addition, we set Dn,0 = I2 . Then there exist positive constants
M, M3 such that for all n, the following two inequalities hold, where the matrix norm is 2-norm.
sup kDn,l k ≤ Mn,
0≤l≤n+1
Dn,n+1 ≤ M3.	(A.3)
Proof. Since
00 ,
we notice that when n ≥ 2,
1-21-2
1-21-2
=
,n
n
D
,
00
11
=
-
-
,n
Dn,
having nothing to do with the value of n. So it is obvious that there exists M3 to make (A.3) holds
and M4 > 0 such that for all n < 2 or n ≥ 2, l > n - 2 or l = 0,
kDn,l k ≤ M4n.
(A.4)
Then we consider the condition when n ≥ 2, 0 < l ≤ n - 2. Notice that
For convenience, we write
Cn =	11
01	10
n+∙) (l
10
Assume we have alreagy got
Dn,l
P = 11
10
P 10
an,l	-1
bn,l	P
satisfying
0 < an,l ≤ l, 0 < bn,l ≤ 1,
12
Under review as a conference paper at ICLR 2021
then since
Dn,l+1 = Dn,lCn-l,
and 0 ≤ n-l+2 < 1, Dn,ι +1 has the same form
Dn,l+1 = P 0
an,l+1 P-1
bn,l+1
satisfying
0 < an,l+1 ≤ l + 1,
Then for fixed n, induce from l = 1, we get
0<bn,l ≤ 1.
satisfying
Dn,l = PDen,lP-1 , P
1	an,l	-1
0	bn,l
0 < an,l ≤ l ≤ n, 0 < bn,l ≤ 1,
for all n ≥ 2, 0 < l ≤ n - 2. Then we can estimate kDn,l k. Notice that
(A.5)
De DeT	1 + a2n,l
Dn,lDn,l = an,lbn,l
an,lbn,l
a2n,l
The eigenvalues of this matrix are
1+a2n,l+b2n,l±	(1 + a2n,l + b2n,l)2 —4b4
λ 1，2 =------------------2--------------------∙
Combining this representation with (A.5), we get the estimation
kDe n,ιk = ρmaxτiλ1ir∣λ2i} ≤ q1+a n,ι+bn,ι ≤ n+2∙
So there exists M5 > 0, such that for all n ≥ 2, 0 < l ≤ n — 2, inequality
kDn,l k ≤ M5n
holds. Combining (A.4) with (A.6), we finish the proof.
(A.6)
□
A.3 Proof of Theorem 2
Theorem 2. Under conditions in Theorem 1, for fixed time t, Xt/h converges to X(t) as h goes to
zero at a rate of O(hln h) if x0 = x(0) and x 1 = x(h).
Proof. In this proof, we first calculate the error caused by a single iteration, which can be divided
into an accumulation term and a truncation term. Then we use the estimation given by Theorem
1 and apply discrete Gronwall inequality to prove the convergence.
Recall the recurrence relation
xn+1
n—3	2	n—3
xn +	(xn — xn-1 ) — h ∙ f I xn +	(xn — xn-1 )
and the definition of truncation error
N (tn +1)= N (tn ) + n^3 ( N (tn ) — N (tn-1)) — h 2 f (X (tn ) + n^^3 ( X (tn ) — X (tn-1)) ) + L [ X (tn )； h ],
n
n
n
n
13
Under review as a conference paper at ICLR 2021
where tn = nh.
Subtract the above two equations, and introduce overall error
en
x(tn)
xn,
we have
2n - 3
en +1 =	en
n
which can also be written as
en-1 — h2bn-1 + L[x(tn); h],
	
n — 3
n
en+2 —
2n — 1
n+ι en+1 +
n—2
----en
n + 1 n
—h2bn + L[x(tn+1); h],
where
2n — 1	n — 2	2n — 1	n — 2
bn =f (η+rXn+1 - η+ιxn)- f [η+rX(tn+1)- η+ιx(tn)>
We will also use the notation
(A.7)
(A.8)
bn
2n-1	n-2
en+2 - Kr en +1 + 币 en
h
Then we rewrite (A.7) into a form that is convenient for recurrence. We set
En
en+1
en
/2 n-1
Cn = n n+r
n—2
n+1
0
Bn
卜7bn
Then (A.7) can be written as
By recursive method, we have
En+1 = CnEn + Bn .
n
En = Cn-1 ∙∙∙ C o E o + £ Cn-I …CnT +1 Bn—i.
l=1
With the notations introduced in Lemma 2, this equation can be written as
n
En = Dn-1,nE0 +	Dn-1,l-1Bn-l.
l=1
(A.9)
Now we need to estimate kBn k. Since f satisfies L-Lipschitz condition, from (A.8) we have
2η 1	η 2
Ibnl ≤ L —|en +11 + -- ∣en∣] ≤ L(2|en+11 + |en|) ≤ 3LIlEnl∣.
η+1	η+1
and
IBnI ≤ 3h2LIEn I + L[X(tn+1); h].	(A.10)
Take norm on both sides of (A.9) and substitute (A.10) and conclusion of Lemma 2, we have the
following estimation
n-1
IEnk ≤ M3kEok + M(- - 1) £ (3h2L∣Eι∣ + L[x(ti+1); h])
l=0
n-1	n-1
≤ M3 kE0k +3Mηh2L	kElk +Mη	L[X(tl+1);h].
l=0	l=0
(A.11)
14
Under review as a conference paper at ICLR 2021
Now we deal with truncation errors. Recall (A.2) in remark of Theorem 1
h2
1L [x (tl); h ] | ≤ M1h + M2 ~γ∙
Take sum to obtain
n-1	n-1
EL[X(tl+1); h] I ≤ nM 1 h3 + M2h2E τ-.
l=0	l=0 +
Notice the classic inequality
(A.12)
n1
» ≤ ln n + Me,
i
i=1
Where Me refers to a positive constant. Substitute it to (A.12), We have
n-1
X |L[x(tl+1); h]| ≤ nM1h3 + M2h2(lnn + Me).
l=0
Substitute this inequality to (A.11), we get a control of kEn k
n-1
kEnk ≤ M3kE0k + 3Mnh2L X kElk +MM1n2h3 + MM2Menh2 + MM2nh2lnn
l=0
Using discrete Gronwall inequality, we have
kEnk ≤ e3Mn2h2L (M3kE0k + MM1n2h3 + MM2Menh2 +MM2nh2lnn+3Mnh2LkE0k .
Then for fixed t, We choose n = 去 to get
kEt/hk ≤ e3Mt2L ((M3 + 3MthL)∣∣E0k + (MM112 + MM2Met)h + MM2thln h) .
Notice that
lim h ln ɪ = 0,
h→0	h
so if E0 = 0, then the vector form of overall error Et/h satisfies
lim kEt/hk =0.
h→0
□
A.4 Proof of Theorem 3
Theorem 3. If f has continuous second order derivative, the first and second derivative are bounded,
and x(t) has continuous fourth derivative, then for fixed t, truncation error of (3.2) satisfies
L[x(t); h] = O(h4).
Proof. Recall the proof of Throrem 1. NoW We expand x(t - h) to first order
x(t - h) = x(t) + hx(1)(t) + O(h2).
15
Under review as a conference paper at ICLR 2021
Then we have
f x(t) +
t - 3h
(x(t) - x(t - h))
=f (X(t) +(1 - 3h) (hx(1)(t) + O(h2))
t
=f x(t) + hx(1) (t) + O(h2)
=f x(t) + hx(1) (t) + O(h2).
We now expand f :
f (X(t) + -~t^-(x(t) — x(t — h))) = f (x(t)) + hx(1)(t)f⑴(x(t)) + O(h2).
To do this, we need f has continuous second derivative and the second derivative is bounded. Take
derivetive on both sides of differential equation
3
X + ~^x + f (x) = 0)
we have
33
(f (x (t))) = —x ⑶(t) - -x ⑵(t) + t2 x ⑴(t).
So
f	x(t) + t	— 3h ~t—(X (t) —	x(t — h))) = — hx(3)(t) — (ɪ + ]) x(2)(t)	
			+ (∣2— t)x (1)( t)+O(h 2).	
Expand x(t + h),	x(t — h),	x(t — 2h) to the third order, we have		
β	1 h [a 1 + ~	,Y1h2、 + 厂,	x(t+ h) =	β , β 1 h Y1 h 2 ) f1 + ~τ + 丁J h2	h3 x (t) + hx (1)( t) +	x (2)( t) +	x (3)( t) + O (h 4)	,
β J 3 h Γ3 + -	Y Y3h2、 + 厂 ,	x(t — h) =	β3 h Y3 h2 Γ3 + ~τ + 丁J h2	h3 x (t) — hx (1)( t) + 工 x (2)( t)——6 x (3)( t) + O (h 4)	,
β , β 4h , Y 4h 2、 a44 + 7 + 丁		x(t — 2h) =	β4 h Y4 h2 <α 4 + ~r + 丁)	
			4h3 x (t) — 2 hx (1)( t) + 2 h 2 x (2) (t)	—	x (3)( t) + O (h 4)	
(A.13)
Substitute these three equations and (A.13) to truncation error of recurrence relation (3.1)
4
L[x(t); h] =X
i=1
,βih ,
0i + T +
x(t + (2 — i)h)
+ h2f
(X (t) + --1~~ (x (t) — x (t —
16
Under review as a conference paper at ICLR 2021
then simple calculation shows that terms with order less than four will be eliminated if we choose
coefficients according to the following equations
	2	'β 1 =	9b - - - k	'Y1 =	m1
α1			2		3m1 + m2 + 3
α2	-5	β 2 二	-6 + 3k	Y 2 二	二	 2
α3	4,	β3	3, =--3k	γ3	m2
α4	-1		2	、Y4 二	m1 - m2 + 3
		、β4 =	k		-	2
where k, m1, m2 can be chosen randomly. Notice that coefficients of recurrence relation (3.2)
satisfy above equations.	□
A.5 Algorithms
Algorithm 4 FISTA
Input: step size s
Initial value: Y1 = X0 ∈ M100 , t1 = 1.
kth iteration (k ≥ 1). Compute
XLary 占阳 T Y-S (Y))『+ 川 X H*
tk+1
Yk+1
Xk + tk-1 (Xk - Xk-1).
tk+1
Algorithm 5 Accelerated proximal gradient method
Input: step size s
Initial value: X1 = X0 ∈ M100 .
kth iteration (k ≥ 1). Compute
k-3
Yk = Xk-1 +----(-(Xk-1 - Xk-2),
k
Xk +1 = arg min fɪ IIX - (Yk -Sg (Yk)) H2 + * X H*
X 2s
A.6 Details about Numerical Experiments in Section 4
Here we produce some details for our numerical experiments in Section 4.
Our experiments are taken on a simulated data set. Firstly, we generated the ‘true’ low rank
matrix M . To do this, we generate a random matrix M0 . Entries of M0 are independent and
uniformly distributed on (0, 20). Then we compute the singular value decomposition of M0, that
is, M0 = UΣV T. After that, we set M = UΣ0V T, where Σ0 is a diagonal matrix with only three
nonzero diagonal elements. It is not difficult to prove that M has rank 3.
17
Under review as a conference paper at ICLR 2021
Secondly, we generate the observation set. For every row of M , we choose randomly ten entrys to
be observed. As a consequence, 10% entries are observed in total.
After data generation step, we apply the abovementioned algorithms (accelerated proximal gradient
method, FISTA and our modified FISTA) with fixed step sizes and backtracking to this data set.
The parameter of the loss function (4.1) is λ = 0.005. For initial point, we simply choose the zero
matrix (every entry equals to zero). For backtracking, we set the initial step size as 10 and the
decay factor β = 0.1.
18