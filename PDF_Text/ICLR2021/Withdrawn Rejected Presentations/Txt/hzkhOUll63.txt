Under review as a conference paper at ICLR 2021
Stability analysis of SGD through the
NORMALIZED LOSS FUNCTION
Anonymous authors
Paper under double-blind review
Ab stract
We prove new generalization bounds for stochastic gradient descent for both the
convex and non-convex case. Our analysis is based on the stability framework.
We analyze stability with respect to the normalized version of the loss function
used for training. This leads to investigating a form of angle-wise stability instead
of euclidean stability in weights. For neural networks, the measure of distance
we consider is invariant to rescaling the weights of each layer. Furthermore, we
exploit the notion of on-average stability in order to obtain a data-dependent quan-
tity in the bound. This data dependent quantity is seen to be more favorable when
training with larger learning rates in our numerical experiments. This might help
to shed some light on why larger learning rates can lead to better generalization in
some practical scenarios.
1	Introduction
In the last few years, deep learning has succeeded in establishing state of the art performances in a
wide variety of tasks in fields like computer vision, natural language processing and bioinformatics
(LeCun et al., 2015). Understanding when and how these networks generalize better is important
to keep improving their performance. Many works starting mainly from Neyshabur et al. (2015),
Zhang et al. (2017) and Keskar et al. (2017) hint to a rich interplay between regularization and the
optimization process of learning the weights of the network. The idea is that a form of inductive
bias can be realized implicitly by the optimization algorithm. The most popular algorithm to train
neural networks is stochastic gradient descent (SGD). It is therefore of great interest to study the
generalization properties of this algorithm. An approach that is particularly well suited to investigate
learning algorithms directly is the framework of stability (Bousquet & Elisseeff, 2002), (Elisseeff
et al., 2005). It is argued in Nagarajan & Kolter (2019) that generalization bounds based on uniform
convergence might be condemned to be essentially vacuous for deep networks. Stability bounds
offer a possible alternative by trying to bound directly the generalization error of the output of the
algorithm. The seminal work of Hardt et al. (2016) exploits this framework to study SGD for both the
convex and non-convex case. The main intuitive idea is to look at how much changing one example
in the training set can generate a different trajectory when running SGD. If the two trajectories must
remain close to each other then the algorithm has better stability.
This raises the question of how to best measure the distance between two classifiers. Our work
investigate a measure of distance respecting invariances in ReLu networks (and linear classifiers)
instead of the usual euclidean distance. The measure of distance we consider is directly related to
analyzing stability with respect to the normalized loss function instead of the standard loss function
used for training. In the convex case, we prove an upper bound on uniform stability with respect
to the normalized loss function which can then be used to prove a high probability bound on the
test error of the output of SGD. In the non-convex case, we propose an analysis directly targeted
toward ReLu neural networks. We prove an upper bound on the on-average stability with respect
to the normalized loss function which can then be used to give a generalization bound on the test
error. One nice advantage coming with our approach is that we do not need to assume that the loss
function is bounded. Indeed, even if the loss function used for training is unbounded, the normalized
loss is necessarily bounded.
Our main result for neural networks involves a data-dependent quantity that we estimate during
training in our numerical experiments. The quantity is the sum over each layer of the ratio between
1
Under review as a conference paper at ICLR 2021
the norm of the gradient for this layer and the norm of the parameters for the layer. We observe that
increasing the learning rate can lead to a trajectory keeping this quantity smaller during training.
Therefore, larger learning rates can lead to a better “actual” stability than what a worst case anal-
ysis from uniform stability would indicate. There are two ways to get our data-dependent quantity
smaller during training. The first one is by facilitating convergence (having smaller norm for the
gradients). The second one is by increasing the weights of the network. If the weights are larger,
the same magnitude for an update in weight space results in a smaller change in angle. In our
experiments, larger learning rates are favorable in both regards.
2	Related work
Normalized loss functions have been considered before (Poggio et al., 2019), (Liao et al., 2018). In,
Liao et al. (2018) test error is seen to be well correlated with the normalized loss. This observation
is one motivation for our study. We might expect generalization bounds on the test error to be
better by using the normalized surrogate loss in the analysis. (Poggio et al., 2019) writes down a
generalization bound based on Rademacher complexity, but motivated by the possible limitations of
uniform convergence for deep learning (Nagarajan & Kolter, 2019), we take the stability approach
instead.
Generalization of SGD has been investigated before in a large body of literature. Soudry et al. (2018)
showed that gradient descent converges to the max-margin solution for logistic regression and Lyu
& Li (2019) provides and extension to deep non-linear homogeneous networks. Nacson et al. (2019)
gives similar results for stochastic gradient descent. From the point of view of stability, starting from
Hardt et al. (2016) without being exhaustive, a few representative examples are Liu et al. (2017),
London (2017), Yuan et al. (2019), Kuzborskij & Lampert (2018).
Since the work of Zhang et al. (2017) showing that currently used deep neural networks are so much
overparameterized that they can easily fit random labels, taking properties of the data distribution
into account seems necessary to understand generalization of deep networks. In the context of
stability, this means moving from uniform stability to on-average stability. This is the main concern
of the work of Kuzborskij & Lampert (2018). They develop data-dependent stability bounds for
SGD by extending over the work of Hardt et al. (2016). Their results have a dependence on the risk
of the initialization point and the curvature of the initialization. They have to assume a bound on the
noise of the stochastic gradient. We do not make this assumption in our work. Furthermore, instead
of having the bounds involve properties of the initialization (which can be useful to investigate
transfer learning), we maintain instead in our bound for neural networks the properties after the
“burn-in” period and therefore closer to the final output since we are interested in the effect of the
learning rate on the trajectory. This is motivated by the empirical work of Jastrzebski et al. (2020)
arguing that in the early phase of training, the learning rate and batch size determine the properties
of the trajectory after a “break-even point”.
It has been observed in the early work of Keskar et al. (2017) that training with larger batch sizes
can lead to a deterioration in test accuracy. The simplest strategy to reduce (at least partially) the
gap with small batch training is to increase the learning rate (He et al., 2019), (Smith & Le, 2018),
(Hoffer et al., 2017), (Goyal et al., 2017). We choose this scenario to investigate empirically the
relevance of our stability bound for SGD on neural networks. Remark that the results in Hardt
et al. (2016) are more favorable to smaller learning rates. It seems therefore important in order to
get theory closer to practice to understand better in what sense larger learning rates can improve
stability.
3	Preliminaries
Let l(w, z) be a non-negative loss function. Furthermore, let A be a randomized algorithm and
denote by A(S) the output of A when trained on training set S = {zι, •…,Zn}〜Dn. The true risk
for a classifier w is given as
LD (w) := Ez〜Dl(w,z)
and the empirical risk is given by
2
Under review as a conference paper at ICLR 2021
LS(W) := 1 Pi=1 I(W, Zi).
When considering the 0 - 1 loss of classifier w, we will write L0D-1(w). Furthermore, we will add
a superscript α when the normalized losses lα are under consideration (these will be defined more
clearly in the subsequent sections respectively for the convex case and the non-convex case). Our
main interest is to ensure small test error and so we want to bound L0D-1(W). The usual approach is
to minimize a surrogate loss upper bounding the 0 - 1 loss. In this paper, we consider the algorithm
stochastic gradient descent with different batch sizes to minimize the empirical surrogate loss. The
update rule of this algorithm for learning rates λt and a subset Bt ⊂ S of size B is given by
Wt+1 = Wt- λtB1 X E(Wt,Zj).	(1)
zj∈Bt
We assume sampling uniformly with replacement in order to form each batch of training exam-
ples. In order to investigate generalization of this algorithm, we consider the framework of stability
(Bousquet & Elisseeff, 2002).
We now give the definitions for uniform stability and on-average stability (random pointwise hy-
pothesis stability in Elisseeff et al. (2005)) for randomized algorithms (see also Hardt et al. (2016)
and Kuzborskij & Lampert (2018)). The definitions can be formulated with respect to any loss func-
tion but since we will study stability with respect to the lα losses, we write the definitions in the
context of this special case.
Definition 1 The algorithm A is said to be uαni-uniformly stable if for all i ∈ {1, . . . , n}
sup E[∣lα(A(S),z) — lα(A(S(i)),z)ll ≤ *	(2)
S,zi0 ,z
Here, the expectation is taken over the randomness of A. The notation S(i) means that we replace
the ith example of S with Zi0.
Definition 2 The algorithm A is said to be aαv -on-average stable if for all i ∈ {1, . . . , n}
E ∣lα(A(S),z) — lα(A(S(i)),z)l ≤ 曝.	⑶
Here, the expectation is taken over S 〜Dn, Z 〜D and the randomness of A. The notation S(i)
means that we replace the ith example of S with Z.
In Hardt et al. (2016), uniform stability with respect to the same loss the algorithm is executed on
is considered. This is a natural choice, however if we are interested in the 0 - 1 loss, different
set of parameters W, W0 can represent equivalent classifiers (that is, predict the same label for any
input). This is the case for logistic regression since any rescaling of the parameters yields the same
classifier (but they can have different training losses). This is also the case for Relu neural networks
where we can rescale each layer without affecting the classifier. This is why we consider stability
with respect to normalized losses instead. Remark that we are still considering SGD executed on
the original loss l (we do not change the algorithm A). The intuitive idea is to measure stability
in terms of angles (more precisely, we consider distances between normalized vectors) instead of
standard euclidean distances (see Figure 1). The proof in Hardt et al. (2016) consists in bounding
E||Wt - Wt0 ||, where Wt represents the weights at iteration t when training on S and Wt0 represents
the weights at iteration t when training on the modified training set S(i). We will instead bound
0
E|| ∣∣WWt∣∣ - ∣μ0^ || (or E[d(f, g)] for an appropriate measure of distance d between neural networks
f and g).
Throughout the paper, || ∙ || will denote the euclidean norm for vectors and the FrobeniUs norm
for matrices. The proofs are given in appendix A for the convex case and in appendix B for the
non-convex case.
3
Under review as a conference paper at ICLR 2021
Figure 1: For the same magnitude of step taken (same ball radius), a larger norm of parameters leads
to a smaller change in angle.
4 Convex case
Consider a linear classifier parameterized by either a vector of weights (binary case) or a matrix of
weights (multi-class case) that we denote by w in both cases. The normalized losses are defined by
w
l	(W, Z)= l(α ||W||, Z) ,	(4)
for α > 0.
In order to state the main result of this section, we need two common assumptions: L-Lipschitzness
of l as a function of w and β-smoothness.
Definition 3 The function l(w, Z) is L-Lipschitz for all Z in the domain (with respect to w) if for
all w, w0, Z,
|l(w, Z) - l(w0, Z)| ≤ L||w - w0||.	(5)
Definition 4 The function l(w, Z) is β-smooth if for all w, w0, Z,
||Vl(w,Z) — Vl(w0,z)|| ≤ β∣∣w - w01|.	(6)
We are now ready to state the main result of this section.
Theorem 1 Assume that l(w, Z) is convex, β-smooth and L-Lipschitz for all Z. Furthermore,
assume that the initial point w° satisfies ||wo || ≥ K for some K such that K = K — L PT-o1 λi > 0
for a Sequence oflearning rates λi ≤ 2∕β. SGD is then run with batch size B on lossfunction l(w, z)
for T steps with the learning rates λt starting from w0. Denote by uαni the uniform stability of this
algorithm with respect to lα. Then,
α -	2L2B
Cuni ≤ α一∕~
nK
T-1
Xλi.
i=0
(7)
What is the difference between our bound and the bound in Hardt et al. (2016) (see theorem 6 in
Appendix A) ? Our bound says that it is not enough to use a small learning rate and a small number
of epochs to guarantee good stability (with respect to the normalized loss). We also need to take
into account the norm of the parameters (here the norm of the initialization) to make sure that the
“effective” learning rate is small. As a side note, we also incorporated the batch size into the bound
which is not present in Hardt et al. (2016) (only B = 1 is considered).
From this result, it is possible to now obtain a high probability bound for the test error. The bound is
over draws of training sets S but not over the randomness of A. 1 So, we actually have the expected
test error over the randomness of A in the bound. This is reminiscent of PAC-Bayes bounds where
here the posterior distribution would be induced from the randomness of the algorithm A.
1It is possible to obtain a bound holding over the randomness of A by exploiting the framework of Elisseeff
et al. (2005). However, the term involving ρ in their theorem 15 do not converge to 0 when the size of the
training set grows to infinity.
4
Under review as a conference paper at ICLR 2021
Theorem 2 Fix α > 0. Let Mα := sup{l(w, z) s.t. ||w|| ≤ α, ||x|| ≤ R}. Then, for any n > 1 and
δ ∈ (0, 1), the following hold with probability greater or equal to 1 - δ over draws of training sets
S:	________
EaLD-1(A(S)) ≤ EALa(A(S)) + /ni + (2n%i + Mo)∖∕ln^∙	(8)
2n
Proof: The proof is an application of McDiarmid’s concentration bound. Remark that we do not
need the training loss to be bounded since we consider the normalized loss which is bounded. The
proof follows the same line as theorem 12 in Bousquet & Elisseeff (2002) and we do not replicate it
here. Remark that we need to use that uniform stability implies generalization in expectation which
is proven for example in theorem 2.2 from Hardt et al. (2016).
Furthermore, We can make the bound hold uniformly for all a's using standard techniques.
Theorem 3 Let C > 0. Assume that lα(w, z) is a convex function of α for all w, z and that uαni
is a non-decreasing function of α. Then, for any n > 1 and δ ∈ (0, 1), the following hold with
probability greater or equal to1 - δ over draws of training sets S:
EaLD-1(A(S)) ≤ inf EAm max (La/2(A(SX,LS(A(S))) + 琮n + (2n%i +
a∈(0,C] I
M ) j2 lW√2(2 + log2 C - log2 a)) + In(I/δ)}
In the next section, We investigate the non-convex case (actually We target directly neural netWorks).
We exploit on-average stability to obtain a data-dependent quantity in the bound. Remark that it is
also argued in Kuzborskij & Lampert (2018) that the Worst case analysis of uniform stability might
not be appropriate for deep learning. Finally, observe that if We use only one layer We get a linear
classifier and so the results from the next section also applies to logistic regression for example.
5 Non-convex case
We noW consider ReLu neural netWorks (although We could easily extend to other Lipschitz non-
linearity) in the setup of multiclass classification. Write f (x) = Wι(σ(∙ ∙ ∙ W2(σ(W1x)))), where X
is an input to the neural netWork, Wi denotes the Weight matrix at layer i and σ denotes the ReLu
function. Consider a non-negative loss function l(s, y) that receives a score vector s = f(x) and a
label y as inputs. We require the loss function to be L-Lipschitz for all y as a function of s. That is,
for all s, s0 , y,
|l(s, y) - l(s0, y)| ≤ L||s - s0||.	(9)
For example, we can use the cross-entropy loss (softmax function with negative log likelihood). In
this case, it is simple to show by bounding the norm of the gradient of l(s, y) with respect to s that
we can use L = √2. Remark that this is slightly different from the Lipschitz assumption of the
previous section (given with respect to the weights w).
Lemma 1 Assume that ||x|| ≤ R. For 1 ≤ j ≤ l, write Sj = Wj(σ(…W2(σ(W1x)))) and
Sj = W0(σ(…W2(σ(WlX))W We have
ιι
l∣sι-slll≤ R X IIWi-WiII Y (Wi0| ifj>i) .	(10)
i
i=1	j=1,j6=i
The previous lemma motivates a measure of “distance” between neural networks.
Definition 5 For neural networks f and g, where the weight matrices of f are given by Wi ∙∙∙ Wι
and the weight matrices of g are g^ven by W0 •…W0, define
ι	Wi	W0
d(f,g) := X||E-时|.	(II)
5
Under review as a conference paper at ICLR 2021
Remark that this distance function is invariant to rescaling the weights of any layer. This is a de-
sirable property since in a ReLu network such a reparametrization leaves the class predicted by the
classifier unchanged for any input to the network.
Let αι,…αι be positive real numbers. We define the lα1,…al losses as
la1, αl (f, z) := l(αl ||wl|| (σ(∙ ^ • α2 ||W2|| (σ(α1 ||W1|| X))))，y)，	(12)
where z = (x, y) and f is the neural network with weight matrix at layer i given by Wi . That is,
we project the weight matrices to give the norm αi to layer i and then we evaluate the loss l on this
“normalized” network. For simplicity, we will only consider the case where all the αi ’s are equal
to say α and we will write lα (f， z). From our definitions and lemma 1, we have that for all z and
neural networks f and g,
∣la(f,z)- la(g,z)∣≤ LRald(f,g).	(13)
In order to bound stability with respect to lα , we will have to ensure that the two trajectories cannot
diverge to much in terms of d(f， g).
We will make two main assumptions to prove our main result. The first one is a modification of the
concept of β-smooth functions for neural networks.
Definition 6 Consider the gradient of the loss function with respect to the parameters W for some
training example z. The vector containing only the partial derivatives for the weights of layer j will
be denoted by V(j)l(W, z). We define {βj}j=ι-layerwise smoothness as thefollowingproperty: For
all j, z, W = (Wι,…，Wl) and W0 = (W1,…，Wl0),
||Vj)l(W,z)-Vj)l(W0，z)|| ≤ βj∣∣Wj∙ - W*.	(14)
We also let β := max{βj}. Remark that β is upper bounding the spectral norm of the bloc diagonal
approximation of the Hessian.
Next, we introduce an assumption meaning informally that with high probability, the norm of pa-
rameters of each layer is eventually non-decreasing.
Definition 7 Let ≥ 0. The growing norms assumption (for this value of ) is defined as the
following: The distribution D satisfies the property that there exists to ∈ {0，1,…，Bn} such that
the probability over draws of training sets from Dn and over the randomness of A of generating a
non-decreasing sequence ||Wj,t||tT=t0 for all j is greater or equal to 1 - .
In Figure 2, we can see that on Cifar10 and Mnist this assumption makes sense (more details on
experiments in section 6).
Figure 2: Norm of parameters for 3 different layers when training a convolutional network on Ci-
far10 and a fully connected network on Mnist with SGD.
We are now ready to state the main theorem of this section.
Theorem 4 Let ≥ 0 and assume that the distribution D satisfies the growing norms assumption
for this value of e. The notation to ∈ {1, 2,…，BB } will now mean implicitly also that to is large
6
Under review as a conference paper at ICLR 2021
enough so that the sequence of norms is non-decreasing. Suppose that the loss function l(s, y) is
L-Lipschitz for all y, non-negative and that lα (f, z) is bounded above by Mα. Furthermore, assume
{βj}lj=ι-layerwise smoothness and that ||x|| ≤ R. Finally, let B denote the batch size, λt ≤ C the
learning rates and T the number of iterations SGD is being run. Then,
aαv ≤	inf
toE{1,2,…,Bn }
2BLRal
(n - B)β
cβ T-1
X Zt + Mα( — + e)
n
t=t0
(15)
where Zt := Pj=IEA,S,z min：WLBI(W),t∣∣} and β = max{βj }.
Exploiting theorem 12 in Elisseeff et al. (2005), we can get a probabilistic bound on the test error
(holding over the randomness in the training sets and the randomness in the algorithm).
Theorem 5 Fix α > 0. Then, for any n > 1 and δ ∈ (0, 1), the following hold with probability
greater or equal to 1 - δ over draws of training sets S and the randomness of the algorithm A:
LD-I(A(S)) ≤ La(A(S)) + S(1)2Mα + 1；nM5v.	(16)
6	Experiments
6.1	LEARNING RATES AND Zt
In this section we conduct some experiments on the datasets Cifar10 and Mnist. We consider the
scenario where we try to reduce the performance gap between small batch and large batch training
by increasing the learning rate. We will give some evidence suggesting that the quantity Zt can be
of interest to assess generalization in this case.
Remark that the bound on stability for neural networks hold for learning rates satisfying λt ≤ c/t.
This is not a very practical schedule for deep learning and so we use a global learning rate being de-
cayed one time by a factor of 10 instead in our experiments. We use no weight decay or momentum
to stay closer to our theoretical analysis of SGD. Remark that in principle, the learning rate could be
as large as we want during the inital burn-in period (before t0) without hurting stability. However,
this burn-in period must be inside the first epoch in the theoretical result we presented. Since in
practice we train for many epochs, it is not clear if such a small burn-in period is long enough to be
significant in current practice. We still think that the quantity Zt is relevant to investigate empirically.
We approximate its value on a training set S with the quantity Zt(S ):= Pj=I ∣" ∣^LBt (Wt)∣∣. Instead
of plotting the value for each iteration, we average Zt(S) for each epoch. This leads to smoother
curves.
We use a 5-layers convolutional network consisting in 2 convolutional layers with maxpooling and
then 3 fully connected layers with cross-entropy loss on Cifar10. We use also the cross-entropy
loss on Mnist but the neural network is a 6-layers fully connected network. In both cases, we use
batch-normalization to facilitate training. All the results in the figures are obtained when using a
batch size of 2048. We started by training with a smaller batch size of 256 and then tried to reduce
the gap in performance between large batch and small batch training by increasing the learning rate.
For example, on Cifar10, we obtain a test accuracy of 86.23% when using a batch size of 256 and
a learning rate of 0.5. When increasing the batch size to 2048 (and maintaining the learning rate
to 0.5), the test accuracy dropped to 85.14%. This happened even if the training loss is reaching
approximately the same value in both cases (0.0123 for batch size 256 and 0.0167 for batch size
2048). We then increased the learning rate to 1.0 and then to 1.5 reaching 85.63% in both cases (not
completely solving the gap but reducing it). A similar phenomenon happens for Mnist. Here, with
batch size 256 we get 98.57% (lr = 0.05) of test accuracy and for batch size 2048, we get 97.52%
(Ir = 0.05), 98.00% (Ir = 0.1) and 98.39% (Ir = 0.5). We plotted the values of Zt(S) during
training in figure 3. We can see that it is better during all training when increasing the learning rate.
To compare with the analysis from Hardt et al. (2016), the quantity Zt would be replace with a global
Lipschitz constant which would not be affected by the actual trajectory of the algorithm. Therefore,
7
Under review as a conference paper at ICLR 2021
in comparison to our bound, the bound in Hardt et al. (2016) would be much more favorable to
smaller learning rates. In other words, the worst case analysis of uniform convergence would require
much smaller learning rates to be used than our result to guarantee good stability. The quantity ζt
can be improved by accelerating convergence because of the numerator (norm of the gradients) but
also by increasing the denominator (norm of the parameters). A larger learning rate can help in
both these regards (see figure 4 and figure 5). Remark also that considering only the norm of the
gradients without the norm of the parameters would lead to a less favorable quantity compared to
considering both the norm of the gradients and the norm of the parameters. A standard analysis of
stability (without the normalized loss) similar to Kuzborskij & Lampert (2018) would not benefit
from the norm of the paramters.
(a) Cifar10
Figure 3: ζt(S) when training a convolutional network on Cifar10 and a fully connected network on
Mnist.
(b) Mnist
(a) Cifar10
(b) Mnist
Figure 4: Norm of the gradient when training a convolutional network on Cifar10 and a fully con-
nected network on Mnist.
8
Under review as a conference paper at ICLR 2021
(a) Cifar10
Figure 5: Norm of the parameters (layer 3) when training a convolutional network on Cifar10 and a
fully connected network on Mnist.
(b) Mnist
6.2	Generalization bound and test error
We show in this section the usefulness of considering the normalized loss for bounding the test error.
We evaluate the bound in theorem 5 and compare it to an analogous version for the unnormalized
loss. For this analogous version, we replace the upper bound M on the loss function by the largest
loss achieved during training. Furthermore, the quantity av is upper bounded by the Lipschitz
constant times the euclidean distance between the weights of the networks. The Lipschitz constant
is replaced by the largest norm of gradient obtained during training. For the normalized loss (our
actual theorem 5), we upper bound aαv by LRαlEd(f, g) (see equation 13). We plot the test error,
the upper bound for the normalized case with α = 1.0 and the upper bound for the unnormalized
case in figure 6.
Figure 6: The bound obtained from the euclidean distance is much worst than the bound obtained
from our normalized distance. The network is a 6-layer fully connected network and the training set
is MNIST.
7	Conclusion
We investigated the stability (uniform and on-average) of SGD with respect to the normalized loss
functions. This leads naturally to consider a more meaningful measure of distance between clas-
sifiers. Our experimental results show that stability might not be as bad as expected when using
larger learning rates in training deep neural networks. We hope that our analysis will be a helpful
step in understanding generalization in deep learning. Future work could investigate the on-average
stability with respect to lα losses for different optimization algorithms.
9
Under review as a conference paper at ICLR 2021
References
Olivier BoUsqUet and Andre Elisseeff. Stability and generalization. J. Mach. Learn. Res., 2:499-526,
2002.
Andre Elisseeff, TheodoroS Evgeniou, and Massimiliano PontiL Stability of randomized learning
algorithms. J. Mach. Learn. Res., 6:55-79, 2005.
Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, LukaSz Wesolowski, Aapo Kyrola,
Andrew TUlloch, Yangqing Jia, and Kaiming He. AccUrate, large minibatch SGD: training im-
agenet in 1 hour. CoRR, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.
02677.
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In Proceedings of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and
Conference Proceedings, pp. 1225-1234, 2016. URL http://proceedings.mlr.press/
v48/hardt16.html.
Fengxiang He, Tongliang Liu, and Dacheng Tao. Control batch size and learning rate to generalize
well: Theoretical and empirical evidence. In Advances in Neural Information Processing Systems
32, pp. 1143-1152. Curran Associates, Inc., 2019.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pp. 1731-1741, 2017.
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
Cho*, and Krzysztof Geras*. The break-even point on optimization trajectories of deep neural
networks. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=r1g87C4KwB.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017. URL https://openreview.net/forum?
id=H1oyRlYgg.
Ilja Kuzborskij and Christoph H. Lampert. Data-dependent stability of stochastic gradient de-
scent. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings ofMachine
Learning Research, pp. 2820-2829. PMLR, 2018.
Yann LeCun, Yoshua Bengio, and Geoffrey E. Hinton. Deep learning. Nature, 521(7553):436-444,
2015. doi: 10.1038/nature14539. URL https://doi.org/10.1038/nature14539.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso A. Poggio. A surprising
linear relationship predicts test performance in deep networks. CoRR, abs/1807.09659, 2018.
URL http://arxiv.org/abs/1807.09659.
Tongliang Liu, Gabor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning
Research, pp. 2159-2167. PMLR, 2017.
Ben London. A pac-bayesian analysis of randomized learning with application to stochastic gradi-
ent descent. In Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp.
2931-2940, 2017.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
CoRR, abs/1906.05890, 2019. URL http://arxiv.org/abs/1906.05890.
10
Under review as a conference paper at ICLR 2021
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan,
volume 89 of Proceedings ofMachine Learning Research, pp. 3051-3059. PMLR, 2019.
Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain general-
ization in deep learning. In Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pp. 11611-11622, 2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proceedings ofThe 28th Conference on Learning Theory, COLT 2015, Paris, France,
July 3-6, 2015, pp. 1376-1401, 2015. URL http://proceedings.mlr.press/v40/
Neyshabur15.html.
Tomaso A. Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks:
Approximation, optimization and generalization. CoRR, abs/1908.09375, 2019. URL http:
//arxiv.org/abs/1908.09375.
Samuel L. Smith and Quoc V. Le. A bayesian perspective on generalization and stochastic gradient
descent. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. URL https:
//openreview.net/forum?id=BJij4yg0Z.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, and Nathan Srebro. The implicit bias of gradient
descent on separable data. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018.
URL https://openreview.net/forum?id=r1q7n9gAb.
Zhuoning Yuan, Yan Yan, Rong Jin, and Tianbao Yang. Stagewise training accelerates conver-
gence of testing error over SGD. In Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 De-
cember 2019, Vancouver, BC, Canada, pp. 2604-2614, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings,
2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
A Appendix: Proofs for the convex case
We start by proving a few lemmas.
Lemma 2 Let v, w ∈ Rn and 0 < c ≤ min{||v||, ||w||}. Then,
|| -V_______w_ || ≤ ||v-w||
11∣∣V∣∣	||w|| || ≤ C
Proof: The proof follows from basic linear algebra manipulations. We give it here for completeness
since it is important in what follows. We need to show that
v v w v w ∖ h^ {v-w,v-w)
h 百-TM ,百-TMi ≤ C .
After some manipulations, one can see that this is equivalent to show that
l∣v∣l2 + l∣w∣l2 - 2c2 + 2(c2 -∣∣v∣∣∣HI)h向，t⅛i ≥ °∙
From CaUchy-SchWarz inequality, h而J, TWJ〉≤ 1. Since c2 - ∣∣v∣∣∣∣w∣∣ ≤ 0, the proof will be
completed by showing that
||v||2 + ||w||2 -2c2+2(c2 - ||v||||w||) ≥0.
11
Under review as a conference paper at ICLR 2021
But this is true since
||v||2 + ||w||2 - 2||v||||w|| = (||v|| - ||w||)2.
Lemma 3 Assume that the update rule G is η-expansive. Furthermore, assume that ||G(v)|| and
I I ∕~X /	∖ I I	I	, 1	T >	r∏1
||G(w)|| are larger than Kη. Then,
|| G(W)_________G(V) || ≤ ||v-w||
|| IIG(W)II - W>|| ≤ ~ΠΓ~-
Proof: For v and w satisfying the assumptions, we have
|| G(V)_________G(W) || ≤ ||G(V)-G(W)|| ≤ ||v-w||
||TRvyn- i∣g(w)∣i || ≤ K	≤	.
Lemma 4 Assume that the update rule is σ-bounded, that is ||G(w) - w|| ≤ σ for all w. Further-
more, assume that ||G(v)|| ≥ K and ||G(w)|| ≥ K. Then,
11 G(W)_________G(V) Il ||v-w|| I ɔɪ
|| IIG(W)II - B川|| ≤ ~ΠΓ+ +2K∙
Lemma 5 Assume that the initial point w0 satisfies ||w0 || ≥ K and that SGD is run with batch size
B and a sequence of learning rates λt on an L-Lipschitz loss function l(w, z) for all z. Then, for
all t ≥ 1
||wt|| ≥ K - L Pit=-01 λi.
Proof:
1B
||wM = ||wt-i - λt-ιB£▽%(Wt-ι,zj)||
B j=1
1B
≥	||wt-1|| - λt-ιB|| E Vl(wt-ι, zj)||
B j=1
≥	||wt-1|| - λt-1L
≥ ||wt-2|| - λt-2L - λt-1L
≥ …
t-1
≥	||w0|| - LXλi
i=0
t-1
≥ K - L X λi.
i=0
For ease of comparison, we give the statement of theorem 3.8 in Hardt et al. (2016).
Theorem 6 (Theorem 3.8 in Hardt et al. (2016)) Assume that the lossfunction f(∙; z) is β-smooth,
convex and L-LiPSchitzfor every Z. Suppose that we run SGD with step sizes αt ≤ 2∕β for T steps.
Then, SGD satisfies uniform stability with
2L2 T
≤ W 3 αt.
uni
We are now ready to prove proposition 1.
Proof of theorem 1: The proof is similar to Hardt et al. (2016). Let wt denotes the output of A
after t steps on training set S and wt0 be the output of A after t steps on training set S(i) for some
i ∈ {1, ∙∙∙ n}. From convexity, the update rule is 1-expansive (See lemma 3.7 from Hardt et al.
(2016)). Furthermore, it follows directly form our lemma 5 that the assumptions in both lemma 3
and lemma 4 are satisfied (we can use σ = Lλt at iteration t in lemma 4). Since the probability of
12
Under review as a conference paper at ICLR 2021
picking the example i in a mini-batch of size B is smaller than B (sampling with replacement), we
have
E
wt+ι
llM+ι∣l
^i^ ∣∣]
lM+ιlllll
< B(EIIwt-Wtll + 2LXt^ + (1 - B)EIIwt-Wtll
1 / ll zιl	2BLλΛ
=k^ (EIIwt- w0 H+ —n—)
Remark that this is true since Ellwt-wol1 ≤ Ellwt-w0l1 + 2彼.From the result in Hardt et al. (2016)
(with a slight change to allow any batch size B), we have
Ellwt- wtIl ≤ 噜 Pt-1 λi.
Therefore,
E [ll冲/ - 3+⅛Ill ≤ 2BL Pt ∩λi
11 11w⅛+i11 llwt十ιll[ 一 nK j=0 i
The result then follows from the inequality
ll(α代,z) — l(α产币,z)l ≤ La∖∖——产币II.
i ∖ ∣∣w∣∣ , )	∖ llw0ll,” —	11 ∣∣w∣∣	llw0ll”
We finally prove theorem 3.
Proof of theorem 3: To simplify the text, write e(α, δ) := EALa(A(S)) + Catab + (2九嗡/ +
Ma) JIny). For i ≥ 1, let a = 2(1-i)C and δi =枭.For any fixed i, we have
PS{EaLD-1(A(S)) >e(0i,δi)} <δi.
Therefore,
PS(∀i, EALD-I(A(S)) ≤ e(αi,δi)}
=1 — PS{∃i, EALD-I(A(S)) > e(ai, δi)}
∞
≥	1 — XPS(EaLD-1(A(S)) >e(αi,δi)}
i=1
∞
≥ 1 — X δi ≥ 1 — δ.
i=1
The last inequality follows from
∞
X δi
i=1
δ G 1 _ δπ2 X
%工 i2 = 2^6^ ≤ .
i=1
We want to show that the set
{S : ∀i, EALD-I(A(S)) ≤ e(αi,δi)}
is contained in the set
{S : ∀α ∈ (0,C], EALgI (A(S ))≤_________________________
Ea max (Lg2(A(S)), La(A(S))) + 嗡应 + 出嚼说 + Ma)S/g C-log2 的计侬1 ⑷}.
13
Under review as a conference paper at ICLR 2021
Let S be such that ∀i, EALDI(A(S)) ≤ e(ai, δi). Let α ∈ (0, C]. Then, there exists i such that
αi ≤ α ≤ 2以.We have
EALDI(A(S))	≤ EALSi(A(S)) + e^ab + (2n⅛‰ + Mai )/n^
≤ EALSi(A(S)) + 嗫ab + (2natab + Ma)Jln2^
≤ EALSi (A(S)) + Vtab + (2n端ab + Mα)j2ln(√2(2 + lθg2 c2- lθg2 O)) t⅛≡
The second inequality is true since both EOtab and Ma are non-decreasing functions of α and ai ≤ α.
The last inequality is true since £ =容 ≤ 2(2+log2 C-log2 α^. Finally, the proof is concluded by
using the convexity of La(A(S)) with respect to α. Indeed, since aa ≤ αi ≤ α, we must have
Lai(A(S)) ≤ max(LO/2(A(S)), La(A(S))).
B Appendix： Proofs for the NON-CONVEX CASE
Proof of lemma 1: The proof is done by induction on the number of layers l. Suppose the result is
true for l — 1 layers. Then we have,
IHII
≤
≤
≤
llWlσ(Sl-1) — Wσ(Sl-I)11
llWl σ(sl-1) — W∕σ(sl-1) — W∕(σ(sl-1) — σ(SI-I))ll
llWσ(sl-1) — WMSI-I)ll + llWAσ(S0-1) — σ(SI-I))ll
llWi — W∕llllσ(Si-1)ll + llWι0llllσ(Sl-1) — σ(Si-1)ll
llWι — W∕llllSi-1ll + llW/llllSl-1 —Si-1ll
≤
l-1
RllWI — Wι0ll ∏llWjll + llW∕llllSl-1 — Si-1ll
i=j
≤
l-1	l-1「	l-1
RllWi — W/ll ∏ llWjll + llW/llRX llWi — W/ll	∏
j=1	i=1 L	j = 1,j=i
llWill if j<i∖
llWiiIif j>i J
l ɪ-	l / ..	..
R X nW,-Wil	∏	(WiII	ifj>
i
i=1	j = 1,j=i '
We exploited the fact that the ReLu non-linearity cannot increase the norm of a vector and also that it
is 1-Lipschitz. The proof is finally concluded by observing that for one layer we have, 11 s； — S11 I ≤
R I I W1 — W0ll.
Definition 8 Let us introduce some notations. Let δ(j)(S, z) := I I Wj,t — W,tll and ∆tj)(S, z):=
EA[δ(j)(S, z) l∀k, δ(k)(S, z) = 0]. Here, Wj,t is obtained when training with S for t iterations
and Wjt is obtained when training with S⑴ for t iterations. The condition inside the expectation
is that after to iterations, the two networks are still exactly the same. Since we are interested in
distances after normalization, we COnSider J(j)(S, z) := I I IWj,t∣∣ 一 ∣∣W%∣ Il and ∆(j)(S, z):=
Ea[W(S, z) l∀k, δ(k)(S, z) = 0]. We will further need J(j)(S, z):=屋;但⑶ and ∆ (j)(S, z):=
t	t I t t0	t	(	Tj- (j)	t
EA图)(S, Z) l∀k,殿)(&Z) =0], where KtJ)(S,z) ：= min{ l l Wj,tl l , l l W;,小 }.
Before proving theorem 4, we establish a lemma. Remark that the structure of the proof of the
following lemma and of theorem 4 is similar to the corresponding results in Hardt et al. (2016) and
in Kuzborskij & Lampert (2018).
14
Under review as a conference paper at ICLR 2021
Lemma 6 Let ≥ 0 and assume that the distribution D satisfies the growing norms assumption for
this value of 匕 The notation to ∈ {0,1,2,…，Bn } will now mean implicitly also that to is large
enough so that the sequence of norms is non-decreasing. Suppose that the loss function l(s, y) is
L-Lipschitz for all y, non-negative and that lα (f, z) is bounded above by Mα. Also, assume that
||x|| ≤ R. Furthermore, let B denote the batch size and T the number of iterations SGD is being
run. Then, for any to ∈ {0,1,2,..., Bn }, the on-average stability satisfies
l
嗫 ≤ LRaI XEs,z EA碌)(S,z) ∣∀k, δ(k)(S,z) =0] + Mα(Bt0 + e).
j=1	n
Proof: Since lα (f, z) is bounded above by Mα and non-negative, we have
∣lα(f,z) - lα(g,z)∣ ≤ Mα.
Therefore, by adding a term Mα to the bound, we can then only consider the case where the
growing norm assumption is satisfied. We won’t write that the expectation is then conditional on
that assumption to lighten the statements. By a similar line of reasoning, we can further condition
inside the expectation with the property that after to iterations, the two networks are still exactly
the same. To make this clearer, write the quantity ∣lα(f, Z) - lɑ(g, z)| as the sum of ∣lα(f, Z)-
lα(g,z)∣I{∀k, δ(k) (S,z) = 0} and ∣lα(f,z) - lα(g,z)∣I{∃k : δ(k)(S,z) = 0}. We bound the first
term by using the fact that
l
∣lα(f,z) - lα(g, Z) | ≤ LRaI d(f, g) = LRal X 器 )(S, z).
j=1
For the second term, we use that lα(f, Z) is bounded above by Mα and non-negative to write again
∣lα(f,z) - lα(g,z)∣ ≤ Mα.
The result then follows from the fact that the probability of picking example i in to iterations is
Smallerthan BO.
Proof of theorem 4: From lemma 2, We always have W) (S, Z) ≤ $(j)(S, z). Therefore, from the
previous lemma,
l
Cav ≤ LRaI ^X ES,z A j) (S, z) + Mα(-^~ + E).
j=1	n
First remark that under our assumption,
Here, Bt denotes the batch of samples at iteration t when training on S and Bt0 denotes the batch
of samples at iteration t when training on S(i). When Bt = Bt0, we will use {βj}lj=1-layerwise
smoothness to bound ∣∣Vj)LBt(Wt) - V(j)LB∕(Wj0)∣∣. Otherwise, we use simply the triangular
inequality. Let p(B, n) be the probability of picking the example i in a mini-batch of size B (this is
smaller than Bn). For t ≥ to, we have
15
Under review as a conference paper at ICLR 2021
∆(+)ι(S,z)
≤ (1 - p(B,n))(1 + βj λt)∆^ (j)(S,z) + p(B,n) (∆(j)(S,z) +
λ E rWj)LBt(Wt)II + l∣v(j)LB0(W)II])
t A	Kj(S⑶	Kyj (S,z)	/
Define ∆j) := Es,z∆j)(S, z) and Zj= EA,s,z RKj)BS(Wt)||
over S and z on both side of the previous inequality, we get
for any t. Taking the expectation
∆“ ≤ (1 — p(B,n))(1 + βjλt)∆(j) + p(B,n)(∆(j) + 2λtZ(j)).
This is true since EA,s,z ""j(L)：；(Wt)11 = EA,s,z ||二(LBt(Wt )||. Rearranging terms and using
Kt (S,z)	Kt (S,z)
1 + x ≤ exp(x), we get
∆tj+ι ≤ [1 + (1 — p(B, n))βjλt]∆(j) + 2p(B, n)λtZ(j)
≤ exp((1 — p(B,n))βj λt )∆j) + 2p(B, n)λtZ(j).
Developing the recursion, we get
≤
T-1	T-1
X 2p(B, n)λtζt(j) Y exp
t=t0	k=t+1
(1 — p(B, n))
Therefore,
≤
≤
≤
≤
≤
≤
≤
2B T-1	T-1 1
—X λtζ(j) exP ( (1 - P(B,n))cβj X k)
t=t0	k=t+1
T-1
—X λtZ(j) exp ((1 — p(B,n))cβj log()
n t=t0
2BcT-1 ζ(j)	T— 1 (1-p(B,n))cβj
t
t=t0
2Bc T-1 ζ(j)	T — 1 (1-p(B,n))cβ
t=t0
t
T-1
―c	max {Z(j)}(T - 1)(1-p(B,n))cβ X
n t0≤t≤T-1 t
0	t=t0
(1-p(B,n))cβ-1
2Bc
—二-------~~τττ;	max
nc(1 — p(B, n))β t0 ≤t≤T -1
T — 1	(1-p(B,n))cβ
t0 — 1
2B
T—1 cβ
(n — B )β t0 — 1
max {ζt(j)}
t0 ≤t≤T -1 t
aαv ≤	inf
to∈{1,2,…,B}
2BLRal
(n - B)β
(*)cβXt0≤m≤T-iM} + M* + e)
j=1
To complete the proof, we will use that maxt0≤t≤T-1{ζt(j)} ≤ PtT=-t1 ζt(j) and reverse the sum
order. With the definition ζt := Plj=1 ζt(j), we then have
aαv ≤	inf
一t0∈{1,2,…,B }
2BLRal
(n - BIe
T- 1 cβT-1	Bt0
(K)	X ζt+Ma —+e)
t=t0
16