Under review as a conference paper at ICLR 2021
Least Probable Disagreement Region for
Active Learning
Anonymous authors
Paper under double-blind review
Ab stract
Active learning strategy to query unlabeled samples nearer the estimated decision
boundary at each step has been known to be effective when the distance from the
sample data to the decision boundary can be explicitly evaluated; however, in nu-
merous cases in machine learning, especially when it involves deep learning, con-
ventional distance such as the `p from sample to decision boundary is not readily
measurable. This paper defines a theoretical distance of unlabeled sample to the
decision boundary as the least probable disagreement region (LPDR) containing
the unlabeled sample, and it discusses how this theoretical distance can be empir-
ically evaluated with a lower order of time complexity. Monte Carlo sampling of
the hypothesis is performed in approximating the theoretically defined distance.
Experimental results on various datasets show that the proposed algorithm con-
sistently outperforms all other high performing uncertainty based active learning
algorithms and leads to state-of-the-art active learning performance on CIFAR10,
CIFAR100, Tiny ImageNet and Food101 datasets. Only the proposed algorithm
outperforms random sampling on CIFAR100 dataset using K-CNN while all other
algorithms fail to do so.
1	Introduction
Active learning (Cohn et al., 1996) is a subfield of machine learning to attain data efficiency with
fewer labeled training data when it is allowed to choose the training data from which to learn. For
many real-world learning problems, large collections of unlabeled samples is assumed available, and
based on a certain query strategy, the label of the most informative data is iteratively queried to an
oracle to be used in retraining the model (Bouneffouf et al., 2014; Roy & McCallum, 2001; Sener
& Savarese, 2017b; Settles et al., 2008; Sinha et al., 2019; Sener & Savarese, 2017a; Pinsler et al.,
2019; Shi & Yu, 2019; Gudovskiy et al., 2020). Active learning attempts to achieve high accuracy
using as few labeled samples as possible (Settles, 2009).
Of the possible query strategies, uncertainty-based sampling (Culotta & McCallum, 2005; Scheffer
et al., 2001; Mussmann & Liang, 2018), which enhances the current model by labeling unlabeled
samples that are difficult for the model to predict, is a simple strategy commonly used in pool-based
active learning (Lewis & Gale, 1994). Nevertheless, many existing uncertainty-based algorithms
have their own limitations. Entropy (Shannon, 1948) based uncertainty sampling can query unla-
beled samples near the decision boundary for binary classification, but it does not perform well in
multiclass classification as entropy does not equate well with the distance to a complex decision
boundary (Joshi et al., 2009). Another approach based on MC-dropout sampling (Gal et al., 2017)
which uses a mutual information based BALD (Houlsby et al., 2011) as an uncertainty measure
identifies unlabeled samples that are individually informative. This approach, however, is not neces-
sarily informative when it is jointly considered with other samples for label acquisition. To address
this problem, BatchBALD (Kirsch et al., 2019) is introduced. However, BatchBALD computes,
theoretically, all possible joint mutual information of batch, and is infeasible for large query size.
The ensemble method (Beluch et al., 2018), one of the query by committee (QBC) algorithm (Se-
ung et al., 1992), has been shown to perform well in many cases. The fundamental premise behind
the QBC is minimizing the version space (Mitchell, 1982), which is the set of hypotheses that are
consistent with labeled samples. However, the ensemble method requires high computation load
because all networks that make up the ensemble must be trained.
1
Under review as a conference paper at ICLR 2021
This paper defines a theoretical distance referred to as the least probable disagreement region
(LPDR) from sample to the estimated decision boundary, and in each step of active learning, labels
of unlabeled samples nearest to the decision boundary in terms of LPDR are obtained to be used
for retraining the classifier to improve accuracy of the estimated decision boundary. It is generally
understood that labels to samples near the decision boundary are the most informative as the samples
are uncertain. Indeed in Balcan et al. (2007), selecting unlabeled samples with the smallest margin
to the linear decision boundary and thereby minimal certainty attains exponential improvement over
random sampling in terms of sample complexity. In deep learning, it is difficult to identify samples
nearest to the decision boundary as sample distance to decision boundary is difficult to evaluate. An
adversarial approach (Ducoffe & Precioso, 2018) to approximate the sample distance to decision
boundary has been studied but this method does not show preservation of the order of the sample
distance and requires considerable computation in obtaining the distance.
2	Distance: Least Probable Disagreement Region (LPDR)
This paper proposes an algorithm for selecting unlabeled data that are close to the decision boundary
which can not be explicitly defined in many of cases.
Let X , Y , H and D be the instance space, the label space, the set of hypotheses h : x → y and the
joint distribution over (x, y) ∈ X × Y. The distance between two hypotheses h and h is defined
as the probability of the disagreement region for h and h. This distance was originally defined in
Hanneke et al. (2014) and Hsu (2010):
Z ʌ	-人,	、	.	. Z	、r
ρ(h,h) ：= PD[h(X) = h(X)].
(1)
EF ・	1 n	T	1	1 ∙ .	Ir / /F F	/F∙f 一 C/F	1	1	1
This paper defines the sample distance d of x to the hypothesis h ∈ H based on ρ as the least
probable disagreement region (LPDR) that contains x:
7/	1 ∖	∙ C	/ Γ 7 \
d(x, h) := inf	ρ(h, h)
h∈H(x,h)
(2)
where H(x, h) = {h ∈ H : h(x) 6= h(x)}.
Figure 1 shows an example of LPDR. Let’s define
H = {hθ : hθ (x) = I[x > θ]} on input x sam-
pled from uniform distribution D = U [0, 1] where
IH is an indicator function. Suppose X = x0 and
h = ha ∈ H when a < x0. Here, H(x0, ha) con-
sists of all hypotheses whose prediction on x0 is in
disagreement with ha (x0) = 1, i.e., H(x0, ha) =
{hb ∈ H : hb(x0) = 0} = {hb ∈ H : b > x0}.
Then, the LPDR between x0 and ha , d(x0 , ha ) =
x0 - a as the infimum of the distance between ha
and hb ∈ H(x0, ha) is ρ(ha, hx0) = x0 - a.
Here, the sample distribution D is unknown, and
H(x, h) may be uncountably infinite. Therefore, a
systematic and empirical method for evaluating the distance is required. One might the procedure
below: Sample hypotheses sets H0 = {h0 : ρ(h, h0) ≤ ρ0} in terms of ρ0, and perform grid search
to determine the smallest ρ0 such that there exists h0 ∈ H0 satisfying h(x) 6= h0 (x) for a given x.
Sampling the hypotheses within the ball can be performed by sampling the corresponding param-
eters with the assumption that the expected hypothesis distance is monotonically increasing for the
expected distance between the corresponding parameters (see Assumption 1). This scheme is based
on performing grid search on ρ0 and is therefore computationally inefficient. However, unlabeled
samples can be ordered according to d without grid search with the assumption that there exists a H0
such that variation ratio V(x) = 1 - f(X)∕∣H0∣ and d(x, h) have strong negative correlation where
fm(x) = maxc Ph0∈H0 I[h0 (x) = c] (see Assumption 2).
丸(理-Q)
ha hx°	hi
I----11—pi
OQvO	1
Figure 1: An example of LPDR between a
sample x = x0 and a hypothesis h = ha
in binary classification using the hθ (x) =
I[x > θ] on input X 〜U [0,1].
2
Under review as a conference paper at ICLR 2021

Figure 2: Empirical validation of Assumption 1. Left figure: Relationship between approximated
hypothesis distance and σ at step t = 0. Hypothesis distance is almost linearly proportional to
log(σ) in the ascension. Right figure: Relationship between variation ratio and σ (MNIST). Sample
distance to the decision boundary can be expressed as σ at which the variation ratio is not zero for
the first time (white arrow). The unlabeled samples are ordered in terms of LPDR.
MNIST (corr.=-0.94 vs log(σ)=-5.0)
2 4 6 8
0.0.0.0.
- - - -
> pu(ŋP u<υ<υMl<l)q .」」0u
-IO -8	-6	-4	-2	O	2	O 25	50	75 IOO 125	150	175	200
∣og(σ)	rank of V
Figure 3: Empirical validation of Assumption 2. Left figure: Spearman’s rank correlation coefficient
between LPDR and the variation ratio in terms of σ showing that there exists a σ such that LPDR
and the variation ratio have a strong rank correlation. Right figure: An example of strong negative
correlation between both ranks when log(σ) = -5.0. Samples with increasing LPDR or variation
ratio are ranked from high to low.
Assumption 1. The expected distance between h and randomly sampled h is monotonically in-
creasing in the expected distance between the corresponding W and W, i.e., E[∣∣W - wι ∣∣ | W] ≤
E[∣w — W2∣ | w] implies that E[ρ(h,hι) | h] ≤ E[ρ(h,h2) | h] where W, Wι and W2 are the
parameters pertaining to h, h1 and h2 respectively.
Assumption 2. There exists a hypothesis set H0 sampled around h having the property that large
variation ratio for a given sample data implies small sample distance to h with high probability, i.e.,
there exists H0 such that V (x1) ≥ V (x2) implies that d(x1, h) ≤ d(x2, h) with high probability.
3	Empirical Studies of LPDR
3.1	Hypotheses and Parameters In Deep Networks: Assumption 1
The distance between two hypotheses can be approximated by vectors of the predicted labels on
random samples by the hypotheses:
m
ρ(h, h) ≈ Pe(h, h) = — XI [h(x(i)) = h(x(i))]	(3)
m i=1
where x(i) is the ith sample for i ∈ [m]. The h is sampled by sampling model parameter W 〜
N(W, Iσ2) where W is the model parameter of h, and the expectation of distances between W and
W depends on σ. The Pe is obtained by the average of 100 times for a fixed σ. The left-hand side
of Figure 2 shows the relationship between Pe and σ on various datasets and deep networks. The
Pe increases almost monotonically as σ increases. This implies that the order is preserved between
3
Under review as a conference paper at ICLR 2021
the σ and ρe . Furthermore, the ρe is almost linearly proportional to log(σ) in the ascension of the
graph, i.e., σ 8 eβρe for some β > 0. The right-hand side ofFigure 2 shows V with respect to σ for
each unlabeled sample on MNIST. The sample distance to the decision boundary can be expressed
as σ at which the variation ratio is not zero for the first time (white arrow), where the indices of
unlabeled samples in y-axis are ordered by LPDR. The variation ratio increases as the σ increases,
and it is expected that the data point with short distance has the large variation ratio compared to the
data point with long distance on a certain range of σ .
3.2	LPDR vs Variation Ratio : Assumption 2
The left-hand side of Figure 3 shows Spearman’s rank correlation coefficient (Spearman, 1904)
between LPDR and the variation ratio with respect to σ . The correlation is calculated using only
unlabeled samples whose variation ratio is not 0. The strong rank correlation is verified when the σ
has the appropriate value. Too larger value of σ generates hypotheses too far away from h, which
is not helpful to measure the distance. The right-hand side of Figure 3 shows an example of σ
(log(σ) = -5.0) which makes LPDR and the variation ratio have a strong negative correlation on
MNIST, that is, the data point with larger variation is closer to the decision boundary. Results for
various datasets and networks are presented in Appendix C.
The time complexity is discussed to validate the efficiency of using variation ratio. Let m, N
and nσ be the unlabeled sample size, |H0| and the number of grid for σ respectively. Ordering
unlabeled samples in terms of LPDR by grid search with respect to σ requires the time complexity
of m × N × nσ (see the right-hand side of Figure 2). However, using variation ratio for ordering
unlabeled samples reduces the time complexity to m × N. In the case ofnσ = cN for some c > 0,
then the time complexity can be reduced from O(mN2) to O(mN).
4	Algorithm for LPDR
4.1	Framework
Let Lt and Ut be the labeled and unlabeled samples at step t. At step t, LPDR trains model
parameters Wt using labeled samples Lt, and constructs H0 by sampling the model parameters
Wn 〜N(W^t, Iσ2) for n ∈ [N]. Then, LPDR queries the top q unlabeled samples having highest
variation ratio from the pool data Pt ⊂ Ut of size m.
4.2	Construction of Sampled Hypothesis Set
It is important to set an appropriate σ when constructing H0 as variation ratios goes to 0 with de-
creasing σ (see the right-hand side of Figure 2) and the rank correlation goes to zero with increasing
σ (see the left-hand side of Figure 3). Theoretically, let’s consider the binary classification with
logistic regression where the predicted label is defined as y = sgn(xTW) and supx∈X kxk∞ < ∞.
Then the following theorem holds and the proof is described in Appendix A.
Theorem 1. Suppose that Wn0 for n = 1, . . . , N are generated with the variance of σ2. For all x,
the followings hold: 1) As N → ∞, 1 - fm(x)/N goes to 0 in probability as σ2 goes to 0, 2) As
N → ∞, 1 - fm(x)/N goes to 1/2 in probability for binary classification using logistic regression
as σ2 goes to ∞.
The implication of Theorem 1 is that when σ is too small or too large, it would be difficult to compare
the sample distances of unlabeled samples. In this active learning task, at least q most informative
unlabeled samples must be identified. To meet this condition, it is reasonable to set ρ0n denoted in the
Algorithm 1 as ρ* = q/m, which is not very small and is less than 1/2 in general, for N hypotheses.
This can be attained by updating σnb as σnb+1=σnbe-β(ρn-ρ*) where β > 0 (see Appendix D).
The Figure 10 in Appendix E shows the final test accuracy with respect to target ρ0n on MNIST
dataset. The LPDR performs best when the target Pn is roughly ρ*. In addition, the range of target
ρ0n , associated with the best performance, is wide; thus, LPDR is relatively robust against target ρ0n .
Furthermore, LPDR is robust against hyperparameters β, N and sampling layers (see Appendix F).
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Least Probable Disagree Region (LPDR)
Input:
L0 , U0 : Initial labeled and unlabeled samples
m, q : Size of pool data and number of queries
σ02 : Initial variance for sampling
ρ* : Target hypothesis distance (= q/m)
Procedure:
1:	for step t = 0, 1, 2, . . . , T - 1
2:	Train parameters Wt With Lt, then evaluate its empirical error εt on Lt
3:	σt → σ10
4:	for n = 1, 2, . . . , N
5:	Sample parameters Wn 〜 N(w^t, Iσ02) for h0n
6:	Compute Yn = e-(εn-εt)+ where εnn is empirical error of Wn on Lt
7:	Compute ρ0n = ρe (ht, h0n)
8:	Update σnl+ι = σ∖e-β(ρn-ρ*) where β > 0
9:	end for
10:	σN0 +1 → σt+1
11:	Compute Vw(x(i)) = 1 - fWi/ Pn=I Yn where 朗=max。Pn=I YnI [hn(x(i)) = c]
12:	Get I * = argmaxι⊂ιPt ,m=q £记工 Vw (x* (i)) where IPt = {j : x(j) ∈ Pt ⊆ Ut}
13:	Update Lt+ι = Lt ∪ {(x⑶，y⑶)}i∈I. and Ut+ι = Ut \ {x(i)}i∈I.
14:	end for
Meanwhile, the efficiency of querying samples in the disagreement region of the version space is
well known both theoretically (Hanneke et al., 2014) and empirically (Beluch et al., 2018). When
the trained hypothesis ht is in the version space, the sampled hypotheses hns are in the version space
with high probability, but there are cases where they are outside the version space (see Appendix G).
Thus, LPDR gives weight Yn on the prediction of sampled hypothesis h0n where Yn = e-(εn-εt)+
is a function of εt = errc= (ht) and εn = errc= (hn). Here, (•)+ is max{0, ∙} and errc(h) is the
empirical error of h on L. Then, LPDR uses weighted variation ratio Vw as a function of the
weighted frequency of the modal class fw as defined below:
(i)
Vw(x(i)) = 1 - ≡Νw—	(4)
n=1 Yn
where fw(i) = maxc PnN=1 YnI [h0n(x(i)) = c] and x(i) ∈ Pt ⊆ Ut.
If H0 is a subset of the version space in realizable case, the sample complexity of LPDR follows
Hanneke’s theorem (Hanneke et al., 2014). Let Λ be the sample complexity defined as the smallest
integer t such that for all t0 ≥ t, err(ht0) ≤ where err(h) := PD[h(X) 6= Y ] with probability at
least 1 - δ. Then, LPDR achieves a sample complexity Λ such that, for D in the realizable case,
∀, δ ∈ (0, 1),
Λ(e,δ, D) . ξ ∙(D log ξ + log (log^)) ∙ log；
where D and ξ are the VC-dimension of H and the disagree coefficient with respect to H and D.
When ξ = O(1), in terms of ；, the number of labeled samples required by LPDR is just
O(log(1∕e) ∙ loglog(1∕e)), while the number of labeled samples by a passive learning is Ω(1∕e).
Therefore, in this case, LPDR provides an exponential improvement over passive learning in sample
complexity (Hsu, 2010).
5 Experiments
This section discusses experimental results on 8 benchmark datasets: MNIST (LeCun et al., 1998),
CIFAR10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2019), EMNIST (Cohen et al., 2017),
CIFAR100 (Krizhevsky et al., 2009), Tiny ImageNet (subset of the ILSVRC dataset containing
5
Under review as a conference paper at ICLR 2021
Table 1: Experimental settings for comparing the performance on various datasets are summarized.
Epochs is the maximum number of training epochs. Data size denotes the sizes of datasets for
training / validation / test. Acquisition size denotes the number of samples for the initial model +
number of samples acquired in each step (from the number of samples in the pool data) → Maximum
number of samples acquired during training.
Dataset	Model	Epochs	Data size train / val / test	Acquisition size		
MNIST	S-CNN	50	55,000/5,000/10,000	20	+20 (2K)	→ 1,020
CIFAR10	K-CNN	150	45,000/5,000/10,000	200	+100 (2K)	→ 10,200
SVHN	K-CNN	150	68,257 / 5,000 / 26,032	200	+100 (2K)	→ 10,200
EMNIST	K-CNN	100	75,000/5,000/10,000	235	+150 (2K)	→ 15,235
CIFAR100	K-CNN	150	45,000/5,000/10,000	2,000	+500 (5K)	→ 27,000
CIFAR100	WRN-16-8	100	45,000/5,000/10,000	5,000	+2,000 (10K)	→ 25,000
Tiny ImageNet	WRN-16-8	200	90,000/10,000/10,000	10,000	+5,000 (20K)	→ 50,000
Food101	WRN-16-8	200	60,600/15,150/25,250	6,000	+3,000 (15K)	→ 30,000
HAM10000	WRN-16-8	100	7,015/ 1,500/1,500	500	+300 (3K)	→ 3,500
200 categories rather than the usual 1000 categories; Russakovsky et al., 2015), Food101 (Bossard
et al., 2014) and HAM10000 (Tschandl et al., 2018) datasets. For fair comparison with other ac-
tive learning algorithms, simple two layered CNN, referred to as ‘S-CNN’ (Chollet et al., 2015) is
used for MNIST and four layered CNN, referred to as ‘K-CNN’ (Chollet et al., 2015) is used for
CIFAR10, SVHN, EMNIST and CIFAR100. Additionally, Wide-ResNet (WRN-16-8; Zagoruyko
& Komodakis, 2016) is used for CIFAR100, Tiny ImageNet, Food101 and HAM10000.
Figure 4-7 show magnified plots of test accuracy to accentuate the difference in performance among
different methods: initial labeled sample sizes are not shown in the figures. Figures that include
initial labeled sample size are presented in Appendix H.
5.1	Experimental Settings
Experimental settings regarding total number of epochs, data size and acquisition size are summa-
rized in Table 1, and other details concerning the model, optimizer, batch size, learning rate and
hyperparameters are presented in Appendix B.
5.2	RESULTS FOR MNIST, CIFAR10, SVHN AND EMNIST
A number of experiments are conducted to compare performance of LPDR with other high perform-
ing uncertainty based active learning algorithms on 8 datasets. Figure 4 shows the test accuracy
with respect to the number of labeled samples on MNIST, CIFAR10, SVHN and EMNIST datasets.
Each algorithm is denoted such as ‘LPDR’: the proposed algorithm, ‘Random’: random sampling,
‘Entropy’: entropy based uncertainty sampling, ‘MC-BALD’: MC dropout sampling using BALD,
‘MC-VarR’: MC dropout sampling using variation ratio (Ducoffe & Precioso, 2015) and ‘ENS-
VarR’: ensemble method. Overall, LPDR either performs best or comparable with all other algo-
rithms. Its performance is consistent regardless of the benchmark datasets. In the early step, LPDR
significantly outperforms all other algorithms on MNIST and CIFAR10 datasets. Of all the algo-
rithms compared, Entropy performed the worst. MC-BALD performed well only on SVHN dataset:
it seems that the performance of BALD is highly dependent on the dataset. With the query size set
to 1, LPDR outperforms BatchBALD on MNIST dataset (see Appendix I). Although MC-VarR and
ENS-VarR are based on different sampling methods, both perform similarily-both outperforming all
others on EMNIST dataset, while showing a significant drop in performance compared to LPDR
on SVHN and CIFAR10 datasets. It is observed that the performances of other algorithms have
a relatively strong data dependency compared to LPDR. On CIFAR10 dataset, the performances of
MC-VarR and ENS-VarR are no better than that of Random, and Entropy and MC-BALD have lower
performance than Random. These results can be attributed to the low network capacity compared to
the data complexity. This issue will be discussed in next section.
6
Under review as a conference paper at ICLR 2021
Figure 4: The performance comparison of LPDR with the uncertainty based active learning algo-
rithms on MNIST, CIFAR10, SVHN and EMNIST datasets (Random: random sampling, Entropy:
entropy based uncertainty sampling, MC-BALD: MC dropout sampling with BALD, MC-VarR: MC
dropout sampling with variation ratio, ENS-VarR: ensemble network with variation ratio). Over-
all, LPDR consistently either performs best or comparable with all other algorithms regardless of
dataset. The performance of all algorithms except LPDR tend to be data dependent.
Figure 5: Performance comparison with respect to the network capacity on CIFAR100 dataset. The
performances of all algorithms except LPDR are much worse than that of Random when using K-
CNN, which has a relatively smaller network capacity than that of WRN-16-8. LPDR is able to
perform consistently better than Random regardless of the network capacity.
5.3	Results for CIFAR 1 00 with K-CNN and Wide-ResNet
In order to compare the performance of the algorithms with respect to the network capacity, exper-
iments are conducted using networks of different capacity but on the same dataset. Figure 5 shows
the results of test accuracy with respect to the number of labeled samples on CIFAR100 dataset with
K-CNN and WRN-16-8. The left-hand figure is the results of using K-CNN, which has a relatively
smaller network capacity than that of WRN-16-8. With the exception of LPDR, the performances
of all algorithms are much worse than that of Random. The right-hand figure is the result of using
WRN-16-8, which has a relatively larger network capacity. In contrast to the results for K-CNN,
most algorithms outperform Random. With a large network capacity, the performance gap between
LPDR and the other algorithms is reduced, but LPDR still outperforms others. LPDR is able to
perform consistently better than Random regardless of the network capacity, and it seems to be
particularly effective with low capacity networks.
7
Under review as a conference paper at ICLR 2021
Tiny ImageNet (Co=IOOOO)
FoodlOl(ro=6OOO)
----Entropy
15Q00	20000	25000	30000	35000	40000	45000	50000	12000 14000 16000 18000 20000 22000 24000 26000 23000 30000
# of labeled samples	# of labeled samples
Figure 6: The performance comparison on Tiny ImageNet and Food101 datasets with WRN-16-8.
LPDR outperforms all other algorithms in more difficult tasks.

5.4	Results for Tiny ImageNet and Food101
Experiments on a more difficult task are conducted. Figure 6 shows test accuracy with respect to
the number of labeled samples on Tiny ImageNet and Food101 datasets with WRN-16-8. Tiny
ImageNet and Food101 are considered to be more difficult than CIFAR100. Even on more difficult
tasks, LPDR outperforms all other algorithms.
5.5	Results for HAM 1 0000
Additional experiments are conducted to compare
the performance of the algorithms on imbalanced
HAM10000 dataset with WRN-16-8. Figure 7
shows the results of the test accuracy with respect to
the number of labeled samples. The LPDR outper-
forms all other algorithms compared. Figure 15 in
Appendix J shows the results of AUC with respect
to the number of labeled samples. The LPDR per-
forms comparable with all other algorithms.
To sum up the comparing algorithms across all ex-
perimental settings and repetitions, rank and Dolan-
More curves are presented in Appendix K. The
LPDR consistently achieves top rank for all steps
and significantly outperforms the other algorithms in
all experimental settings.
Entropy
Hamioooo (ro=5θθ)
# of labeled samples
Figure 7: The performance comparison on
HAM10000 dataset with WRN-16-8. LPDR
outperforms all other algorithms on imbal-
anced dataset.
6	Related Work
Other than uncertainty-based sampling framework (Culotta & McCallum, 2005; Scheffer et al.,
2001; Mussmann & Liang, 2018; Lewis & Gale, 1994; Gal et al., 2017; Kirsch et al., 2019; Beluch
et al., 2018) for active learning, decision-theoretic framework based methods such as expected model
change (Settles et al., 2008) have certain relevance to the proposed LPDR as unlabeled samples
nearer the decision boundary which LPDR is attempting to identify have larger gradients leading
larger model change. Recently, adversarial approaches are proposed to discriminate labeled and
unlabeled samples (Gissin & Shalev-Shwartz, 2019; Sinha et al., 2019; Zhang et al., 2020), and
after performing adversarial learning, any unlabeled samples that is most confidently predicted as
unlabeled is queried and used to retrain the network. Here adversarial learning is used to indirectly
identify sample near the decision boundary.
7	Conclusion
This paper defines a theoretical distance of unlabeled sample to the decision boundary referred to
as the least probable disagreement region (LPDR) containing the unlabeled sample for active learn-
8
Under review as a conference paper at ICLR 2021
ing. LPDR can be evaluated empirically with low computational load by making two assumptions
regarding parameters of the hypothesis space, variation ratio and the LPDR. The two assumptions
are empirically verified.
Experimental results on various datasets show that LPDR consistently outperforms all other high
performing uncertainty based active learning algorithms and leads to state-of-the-art active learning
performance on CIFAR10, CIFAR100, Tiny ImageNet, and Food101 datasets. In addition, LPDR is
able to perform consistently better than random sampling regardless of the network capacity while
all other algorithms compared fail to do so.
LPDR is simple enough to be applied to various classification tasks with deep networks: the im-
plementation requires only sampling a subset of parameters (parameters in the last FC layer of the
deep network). Additionally, LPDR is capable of quick and reliable performance in a variety of dif-
ferent settings with only a computational load that is not much higher than that of other uncertainty
sampling methods. In conclusion, LPDR is an effective uncertainty based sampling algorithm in
pool-based active learning.
References
Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In
Intemational Conference on CompUtationaI Learning Theory, pp. 35-50. Springer, 2007.
William H Beluch, Tim GeneWein, Andreas NUrnberger, and Jan M Kohler. The power of ensembles
for active learning in image classification. In Proceedings of the IEEE Conference on ComPUter
Vision and Pattern Recognition, pp. 9368-9377, 2018.
LUkas Bossard, MatthieU GUillaUmin, and LUc Van Gool. Food-101 - mining discriminative com-
ponents with random forests. In EuroPean Conference on ComPUter Vision, 2014.
Djallel Bouneffouf, Romain Laroche, Tanguy Urvoy, Raphael Feraud, and Robin Allesiardo. Con-
textual bandit for active learning: Active thompson sampling. In International Conference on
NeUraI Information ProceSsing, pp. 40542. Springer, 2014.
Francois Chollet et al. Keras. https://keras.io, 2015.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist
to handwritten letters. In 2017 InternationaI Joint Conference on NeUral NetWorkS (IJCNN), pp.
2921-2926. IEEE, 2017.
David A Cohn, Zoubin Ghahramani, and Michael I Jordan. Active learning with statistical models.
JoUrnal of artificial intelligence research, 4:129-145, 1996.
Aron Culotta and Andrew McCallum. Reducing labeling effort for structured prediction tasks. In
AAAI, volume 5,pp. 746-751, 2005.
Elizabeth D Dolan and Jorge J More. Benchmarking optimization software with performance pro-
files. MathematicaI programming, 91(2):201-213, 2002.
Melanie Ducoffe and Frederic Precioso. Qbdc: query by dropout committee for training deep su-
pervised architecture. arXiv PrePrint arXiv:1511.06412, 2015.
Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin
based approach. arXiv PrePrint arXiv:1802.09841, 2018.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
In ProceedingS of the 34th InternationaI Conference on Machine Learning-Volume 70, pp. 1183-
1192. JMLR. org, 2017.
Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv PrePrint
arXiv:1907.06347, 2019.
Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, and Sotaro Tsukizawa. Deep active
learning for biased datasets via fisher kernel self-supervision. In ProceedingS of the IEEE/CVF
Conference on ComPUter ViSion and Pattern Recognition, pp. 9041-9049, 2020.
9
Under review as a conference paper at ICLR 2021
Steve Hanneke et al. Theory of disagreement-based active learning. FoUndations and Trends® in
Machine Learning, 7(2-3):131-309, 2014.
Neil Houlsby, Ferenc Huszar, ZoUbin Ghahramani, and Mate Lengyel. Bayesian active learning for
classification and preference learning. arXiv Preprint arXiv:1112.5745, 2011.
Daniel Joseph Hsu. AIgorithmS for active Iearning. PhD thesis, UC San Diego, 2010.
Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image
classification. In 2009 IEEE ConferenCe on CompUter ViSion and Pattern Recognition, pp. 2372-
2379. IEEE, 2009.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acqui-
sition for deep bayesian active learning. In AdVanCeS in Neural InfOrmatiOn Processing Systems,
pp. 7024-7035, 2019.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. https:
//www.cs.toronto.edu/~kriz/cifar.html, 2009.
Yann LeCun, Corinna Cortes, and Christopher JC Burges. The mnist database of handwritten digits.
http://yann.lecun.com/exdb/mnist, 1998.
David D Lewis and William A Gale. A sequential algorithm for training text classifiers. In SIGIR’94,
pp. 3-12. Springer, 1994.
Tom M Mitchell. Generalization as search. ArtifiCiaI intelligence, 18(2):203-226, 1982.
Stephen Mussmann and Percy S Liang. Uncertainty sampling is preconditioned stochastic gradient
descent on zero-one loss. In AdVanCeS in NeUralInformation PrOCeSSing Systems, pp. 6955-6964,
2018.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and A Ng. The street view
house numbers (svhn) dataset, 2019.
Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hernandez-Lobato. Bayesian
batch active learning as sparse subset approximation. In AdVanceS in NeUraI InfOrmatiOn
PrOCeSSing Systems, pp. 6359-6370, 2019.
Nicholas Roy and Andrew McCallum. Toward optimal active learning through monte carlo estima-
tion of error reduction. ICML, Williamstown, pp. 441^48, 2001.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of COmPUter ViSion, 115(3):211-252, 2015.
Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for in-
formation extraction. In International SymPOSiUm on Intelligent Data Analysis, pp. 309-318.
Springer, 2001.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv PrePrint arXiv:1708.00489, 2017a.
Ozan Sener and Silvio Savarese. A geometric approach to active learning for convolutional neural
networks. arXiv PrePrint arXiv, 1708:1, 2017b.
Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.
Burr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. In AdVanceS in
neural information PrOCeSSing systems, pp. 1289-1296, 2008.
H Sebastian Seung, Manfred Opper, and Haim Sompolinsky. Query by committee. In Proceedings
of the fifth annual WOrkShOP on Computational Iearning theory, pp. 287-294, 1992.
10
Under review as a conference paper at ICLR 2021
Claude E Shannon. A mathematical theory of communication. Bell System technical journal, 27(3):
379-423,1948.
Weishi Shi and Qi Yu. Integrating bayesian and discriminative sparse kernel machines for multi-
class active learning. In Advances in Neural Information Processing Systems, pp. 2285-2294,
2019.
Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In
ProCeedings of the IEEE International ConferenCe on ComPUter Vision, pp. 5972-5981, 2019.
Charles Spearman. “General Intelligence” objectively determined and measured. AmeriCan JOUrnal
OfPsyChology, 15:201-293, 1904.
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham10000 dataset, a large collection
of multi-source dermatoscopic images of common pigmented skin lesions. SCientifiC data, 5:
180161, 2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv PrePrint
arXiv:1605.07146, 2016.
Beichen Zhang, Liang Li, Shijie Yang, Shuhui Wang, Zheng-Jun Zha, and Qingming Huang. State-
relabeling adversarial active learning. In PrOCeedings of the IEEE/CVF COnferenCe on COmPUter
VisiOn and Pattern Recognition, pp. 8756-8765, 2020.
11
Under review as a conference paper at ICLR 2021
Appendix
A Proof of Theorem 1
Assume that ∣∣Wtk = 1 without the loss of generality, and ∣∣xk = 0 to avoid the null case. The
predicted label of X by Wn disagrees with that by Wt if Sgn (XTWn) = Sgn (XTwt), here, sgn(0)=
1. Note that
XTWn = XTWt + σx%n where 以=(Zni,..., Zn∣w∣)τ,
and Znks are independent random variables from N (0, 1). The event of {Sgn XTWn0 6=
sgn (XTWt)} is equal to that of Ei ∪ E? where E2 = {σXTen ≥ 0, XTWt < 0} and E? = {σXTen <
0, XTWt ≥ 0}. Thus, the proof has two folds: the cases of 1) Ei and 2) &?.
In the first fold,
P[Ei] = P [σX%n ≥ IXTWt|] = P [σ∣∣X∣∣Z ≥ ∣XTWt∣] =1 - Φ (a(：Wt))
where Z 〜 N(0,1), Φ is the cumulative distribution function of the normal distribution, and
a(x, Wt) = |xTWt∣∕∣∣X∣∣. Note that σX%n ~N(0,σ2∣Xk2). Consequently, P[Ei] < 1/2 due
to a (x, Wt) > 0. Hence, the following
f(x)	N 1
ɪ = X R Ihh t(X) = hn(X)i
n=i
goes to value greater than 1/2 in probability as N → ∞ because Var(fm(x)/N ) → 0 as N → ∞.
Therefore, as N → ∞, ∀X, the variation ratio is
f(x)	N 1
1 - M = 1 - X NI [ht(X) = hn(X)] → 1 - Φ
n=i
in probability. This is due to that fm(x) is the frequency of mode class with probability tending to 1
as N → ∞. By the smoothness of Φ,
f(x)
1----m-----→ 1 - Φ(∞) = 0 as σ2 → 0
and
(x)
1----m-----→ 1 - Φ(0) = - as σ2 → ∞.
a (x, Wt)
σ
Next, in the second fold,
P [E2] = P [σXTen <-IXTWt∣] = P [σ∣X∣Z < -IXTWt|] = Φ (-a(X, Wt)
Consequently, P[E2] < 1/2. Hence the following
f(x)	N 1
ɪ = X N Ihh t(X) = hn(X)i
n=i
goes to the value greater than 1/2 in probability as N → ∞ because Var(fm(x)/N ) → 0 as N → ∞.
Therefore, as N → ∞, ∀X, the variation ratio is
1 - fmχ) = 1 -X N Ihht(X) = hn(X)i→ Φ (〉) = 1- Φ (「)
n=i
in probability. This is due to that fm(x) is the frequency of mode class with probability tending to 1
as N → ∞. By the smoothness of Φ,
f(x)
1----m-----→ 1 - Φ(∞) = 0 as σ2 → 0
12
Under review as a conference paper at ICLR 2021
and
(x)
1-----m---→ 1 — Φ(0) = - as σ2 T ∞.
N	2 2	2
This completes the proof.

B	Experimental Settings
B.1	Datasets
MNIST (LeCun et al., 1998) is a dataset of handwritten digits which has a training set of 60, 000
samples and a test set of 10, 000 samples in 10 classes. Each sample is a black and white image and
28 × 28 in size.
CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) are labeled subsets of the 80 million tiny images
dataset which have a training set of 50, 000 samples and a test set of 10, 000 samples in 10 and 100
classes respectively. Each sample is a color image and 32 × 32 in size.
SVHN (Netzer et al., 2019) is a real-world digit image dataset which has a training set of 73, 257
samples and a test set of 26, 032 samples in 10 classes. Each sample is a color image and 32 × 32 in
size.
EMNIST (Cohen et al., 2017) is a dataset of handwritten character digits which has a training set
of 80, 000 samples and a test set of 10, 000 samples in 47 classes. Each sample is a black and white
image and 28 × 28 in size.
Tiny ImageNet is a subset of the ILSVRC (Russakovsky et al., 2015) dataset which has 100, 000
samples in 200 classes. Each sample is a color image and 64 × 64 in size. In experiments, Tiny
ImageNet is split into two parts: a training set of 90, 000 samples and a test set of 10, 000 samples.
Food101 (Bossard et al., 2014) is a fine grained dataset which has a training set of 75, 750 samples
and a test set of 25, 250 samples in 101 classes. Each sample is a color image and resized to 75 × 75.
HAM10000 (Tschandl et al., 2018) is a imbalanced dataset which has 10, 015 samples in 7 classes.
Each sample is a color image and resized to 75 × 75. In experiments, HAM10000 is split into two
parts: a training set of 8, 515 samples and a test set of 1, 500 samples.
All datasets are used without any preprocessing of images.
B.2	Settings
S-CNN, which is the Keras MNIST CNN implementation (Chollet et al., 2015), consists of [3 × 3 ×
32 conv - 3 × 3 × 64 conv - 2 × 2 maxpool - dropout (0.25) - 128 dense - dropout (0.5) - #class dense
- softmax] layers. K-CNN, which is the Keras CIFAR CNN implementation (Chollet et al., 2015),
consists of [two 3 × 3 × 32 conv - 2 × 2 maxpool - dropout (0.25) - two 3 × 3 × 64 conv - 2 × 2
maxpool - dropout (0.25) - 512 dense - dropout (0.5) - #class dense - softmax] layers. WRN-16-8
is a wide residual network that has 16 convolutional layers and a widening factor 8 (Zagoruyko &
Komodakis, 2016). The optimizer, initial learning rate, learning rate schedule and batch size for
each experimental setting are described in Table 2. He normal initialization is used for all models.
All experiments are run for a fixed number of acquisition steps until a certain amount of training
data is labeled. Results are averaged over 5 repetitions. For all datasets, the initial labeled samples
for each repetition are randomly sampled according to the distribution of the training set. For MC
dropout we use 100 forward passes, and ensemble consists of 5 networks of identical architecture but
different random initialization and random batches. For LPDR, we set σ0 = 0.01, β = 1, N = 100
and parameter sampling is applied to the last dense layer of each network.
13
Under review as a conference paper at ICLR 2021
Table 2: Settings for Training
Dataset	Model	Optimizer	Learning Rate	Learning Rate Schedule ×decay [epoch schedule]	Batch Size
MNIST	S-CNN	Adam	0.001	-	32
CIFAR10	K-CNN	Adam	0.0001	-	64
SVHN	K-CNN	Adam	0.0001	-	64
EMNIST	K-CNN	Adam	0.0001	-	64
CIFAR100	K-CNN	Adam	0.0001	-	64
CIFAR100	WRN-16-8	Nesterov	0.05	×0.2 [60, 80]	128
Tiny ImageNet	WRN-16-8	Nesterov	0.1	×0.2 [60, 120, 160]	128
Food101	WRN-16-8	Nesterov	0.1	×0.2 [60, 120, 160]	128
HAM10000	WRN-16-8	Nesterov	0.05	×0.2 [60, 80]	64
C Rank Correlation between LPDR and Variation Ratio
Figure 8 shows an example of negative Spearman’s rank correlation between LPDR and the variation
ratio for each experimental setting. Samples with increasing LPDR or variation ratio are ranked from
high to low. The σ is selected to satisfy Pn = ρ* = q/m at initial step.
CIFAR10 (corr.=-0.93 vs log(σ)=-4.5)
SVHN (corr.=-0.93 vs log(σ) =-4.7)
rank of V
Cifarioo (KCNN)(COrT.=-0.94 VS IOg(C)=-4.7)
rank of V
Cifarioo (WRN)(COrr.=-0.93 VS IOg(C)=-3.2)
rank of V
FoodlOl (corr. =-0.93 vs log(σ)=-4.2)
rank of V
rank of V
HAM10000 (corr. =-0.93 vs log(σ)=-2.3)
rank of V
Figure 8: An example of negative Spearman’s rank correlation between LPDR and the variation
ratio for each experimental setting.
D	REGULATING ρ0n BY THE VARIANCE OF SAMPLING
The left-hand side of Figure 9 shows the ρ0n with respect to the active learning progress. For all
experiments, LPDR reliably guides the ρ0n to be ρ* = q/m (MNIST: 0.01, CIFAR10: 0.05, SVHN:
0.05, EMNIST: 0.075, CIFAR100 (KCNN): 0.1, CIFAR100 (WRN): 0.2, Tiny ImageNet: 0.25,
Food101: 0.2 and HAM10000: 0.1) after the initial few steps. The right-hand side of Figure 9 shows
14
Under review as a conference paper at ICLR 2021
log(σ) with respect to the active learning progress. For all experiments, the variance of sampling
increases as the labeling proceeds. This is because larger variance is required to make the Pn = ρ*
since unlabeled samples move away from the learned decision boundary from labeled samples due
to an increase in network confidence as the number of labeled samples increases.
---EMNlST	-- Tiny ImageNet
—Cifarioo (KCNn) …“ Foodioi
—Cifarioo (wrni	hamioooo
Figure 9: The P0n and σ with respect to the labeling proceeds for all experimental settings. LPDR
reliably guides the P0n to be the target value by increasing the variance of sampling as the number of
labeled samples increases.
E FINAL TEST ACCURACY VS TARGET ρ0n
The Figure 10 shows the final test accuracy with respect to target P0n on MNIST dataset. The results
show that at around ρ*(= 0.02), it performs the best for q = 20 and m = 1000. In addition, the
range of target ρn associated with the best performance, is wide (0.01 〜0.1); thus, LPDR is robust
against the target P0n in the wide range.
P,n=q∕m (=0.02)
Figure 10: The final accuracy with respect to the target ρ0n on MNIST dataset. LPDR performs best
in a wide range of the target ρ0n .
F Robustness of LPDR against Hyperparameters
LPDR has four hyperparameters: 1) the initial variance of sampling σ0 ; 2) the positive hyperparam-
eter for regulating the variance of sampling β; 3) the number of sampled hypotheses N, and 4) the
layer index of the network to which sampling is applied. The σ0 has no significant effect on the per-
formance of LPDR since σ is adaptively regulated based on the ρ0n while sampling the sampled hy-
pothesis. Thus, σ0 is not examined in detail. Figure 11 shows the performance comparison with re-
spect to the hyperparameters of LPDR on MNIST and CIFAR10 datasets. The left figures show that
there is no significant difference in the performance of LPDR for various β ∈ {0.1, 1, 10} on both
datasets. The robustness of LPDR against β is based on the sufficient buffer for regulating σ since the
range of target ρ0n associated with the best performance is wide. The middle figures show that there
is no significant difference in the performance of LPDR for various N ∈ {5, 10, 20, 50, 100, 200}
on both datasets. The robustness of LPDR against N is based on the sufficient discrimination in the
variation ratio for identifying q most informative unlabeled samples with a small number of sampled
15
Under review as a conference paper at ICLR 2021
Figure 11: The performance comparison with respect to the hyperparameters of LPDR on MNIST
and CIFAR10 datasets. LPDR is robust against β and N , and has no significant performance differ-
ence whether the sampling is applied to the parameters of last layer or all layers.
hypotheses by setting ρ* = q/m. The right figures show that there is no significant difference in the
performance of LPDR for the sampling to the parameters of last layer and to the parameters of all
layers of the networks on both datasets.
MNlST	CIFAR10	SVHN
:tpt ×
・ T-HT ×
•. ⅛×
# of labeled samples
EMNIST
# of labeled samples
Cifarioo (kcnn)
# of labeled samples
Cifarioo (wrn)
〈求)」o」」0 6ucro匕
5000 7000 9000 11000 13000 15000 17000 19000 21000 23000
# of labeled samples
Hamioooo
T±UNT
・ T-HT
TnT
TnT
TnT
• TBl
TAT
TAHT ×
r^ul X
I ∙^⅛1
# of labeled samples
Tiny ImageNet
# of labeled samples
FoodlOl
I-TTHX
E」0 6uc∙s-q
• I TXMy
:
• : τ^∩?
• τ^⅛
• ∙∙T6*
• 1l⅛
.，占
-∙∙TΞ*
∙±
# of labeled samples	# of labeled samples	# of labeled samples
Figure 12: The empirical errors of the learned and the sampled hypotheses with respect to the
acquisition step for all experimental settings. It is observed that the empirical error of the learned
hypothesis or the sampled hypothesis is not zero.
• ∙
G	Empirical Errors of Learned and Sampled Hypotheses
Figure 12 shows the empirical error of the learned and the sampled hypotheses with respect to the
acquisition step for all experimental settings. In many cases, the empirical error of the learned
hypothesis becomes zero, thus it is placed in the version space, while the sampled hypothesis is
often placed outside the version space, e.g., in SVHN dataset. Even in the cases of EMNIST and
CIFAR100 with K-CNN datasets, as the number of labeled samples increases, the empirical error of
the learned and the sampled hypothesis increases. To address this situation, LPDR incorporates the
16
Under review as a conference paper at ICLR 2021
Figure 13:	The test accuracy with respect to the number of labeled samples from initial to final step
for all experimental settings.
weighted hypotheses based on the prediction error difference between the learned and the sampled
hypotheses, and it works well empirically.
H	Plots for Test Accuracy
Figure 13	shows the test accuracy with respect to the number of labeled samples from initial to final
step for all experimental settings.
I LPDR VS MC-BATCHBALD
Figure 14	shows the performance comparison between LPDR and MC-BALD on MNIST dataset
using S-CNN when the query size is 1 or 20. LPDR significantly outperforms MC-BatchBALD on
MNIST dataset when q = 1 such that MC-BatchBALD is completely identical to MC-BALD. LPDR
is also expected to outperform MC-BatchBALD even when q > 1: LPDR with q > 1 performs better
than MC-BALD with q = 1 that MC-BatchBALD with q > 1 does not exceed (Kirsch et al., 2019).
Figure 14: The comparison of performance between LPDR and MC-BALD on MNIST dataset
where the query size is 1 or 20. The performance of BatchBALD with q > 1 does not exceed that
of MC-BALD (q = 1) and LPDR (q = 20) outperforms MC-BALD (q = 1).
17
Under review as a conference paper at ICLR 2021
J AUC of HAM 1 0000 dataset
On imbalanced dataset, the performance comparison is performed not only for accuracy but also
for AUC. Figure 15 shows the results of AUC with respect to the number of labeled samples on
HAM10000 dataset. LPDR performs comparable with Entropy or ENS-VarR performing better
than other algorithms.
Figure 15: The comparison of AUC on HAM10000 dataset. LPDR performs comparable with the
best performing algorithms.
K Rank and Dolan-More curves
Figure 16: The rank and Dolan-More curves of the algorithms across all experimental settings and
repetitions. The left figure shows rank curve which is the mean of ranks on all datasets at each step.
LPDR consistently is top-ranked for all steps. The right figure shows the each algorithm’s Dolan-
More curves which present the fraction of problems in which the algorithm has the performance gap
from the best competitor. LPDR maintains the highest value for all τ .
Table 3: The mean (± standard deviation) of performance gap from the best competitor for all
steps of each algorithm on each dataset. LPDR significantly outperforms the other algorithms on all
datasets.
	MNIST	CIFAR10	SVHN	EMNIST	CIFAR100	CIFAR100-W	T. ImageNet	Food101	HAM10000
LPDR	0.17±0.04	0.22±0.07	0.15±0.02	0.13±0.04	0.23±0.10	0.50±0.15	0.42±0.18	0.13±0.09	0.29±0.11
Random	3.45±0.46	1.56±0.07	3.08±0.16	2.80±0.08	0.62±0.17	1.67±0.46	1.41±0.49	1.74±0.48	2.52±0.39
Entropy	1.20±0.27	2.28±0.19	1.92±0.15	0.95±0.13	3.19±0.34	1.19±0.28	1.82±0.36	2.17±0.03	0.98±0.40
MC-BALD	1.91±0.48	2.25±0.16	0.45±0.04	0.78±0.18	1.80±0.35	1.17±0.10	1.00±0.22	1.53±0.09	2.04±0.29
MC-VarR	0.86±0.30	1.76±0.46	1.06±0.11	0.21±0.09	2.20±0.44	0.92±0.27	1.06±0.37	1.72±0.01	1.92±0.30
ENS-VarR	0.65±0.34	1.74±0.18	0.97±0.06	0.22±0.04	2.13±0.28	1.57±0.34	1.85±0.26	1.31±0.31	0.81±0.29
Rank curves and Dolan-More curves are used to compare the performance of the algorithms across
all experimental settings and repetitions. Figure 16 shows the rank and Dolan-More curves for all
18
Under review as a conference paper at ICLR 2021
algorithms considered in the experiment. The rank curve of each algorithm in the left-hand figure
represents the mean of ranks on all datasets at each steps of active learning. LPDR consistently is
top-ranked for all steps.
The right-hand figure shows Dolan-More curves defined as follows (Dolan & More, 2002). Let accp
be the final test accuracy of the a algorithm on thep problem. After defining the performance gap as
∆p = maxχ (accχ) - accp, We can define Dolan-More curve Ra (∙) as a function of the performance
gap factor τ :
Ra(T) = #(P :5 ≤ T)
np
where np is the total number of evaluations for the problem p. Thus, Ra(T) is the ratio of problems
with performance gap between algorithm a and the best performing competitor not more than T .
Note that Ra (0) is the ratio of problems on which algorithm a performs the best. LPDR has the
highest value Ra(0) = 43.3%, and LPDR maintains the highest Ra(T) for all T.
Table 3 presents the mean and the standard deviation of performance gap from the best competi-
tor for all steps of each algorithm on each dataset. Consistent with all the results so far, LPDR
significantly outperforms the other algorithms in all experimental settings.
19