Under review as a conference paper at ICLR 2021
Offline Policy Selection under Uncertainty
Anonymous authors
Paper under double-blind review
Ab stract
The presence of uncertainty in policy evaluation significantly complicates the pro-
cess of policy ranking and selection in real-world settings. We formally consider
offline policy selection as learning preferences over a set of policy prospects given
a fixed experience dataset. While one can select or rank policies based on point
estimates of their policy values or high-confidence intervals, access to the full
distribution over one’s belief of the policy value enables more flexible selection
algorithms under a wider range of downstream evaluation metrics. We propose
BayesDICE for estimating this belief distribution in terms of posteriors of distri-
bution correction ratios derived from stochastic constraints (as opposed to explicit
likelihood, which is not available). Empirically, BayesDICE is highly competitive
to existing state-of-the-art approaches in confidence interval estimation. More im-
portantly, we show how the belief distribution estimated by BayesDICE may be
used to rank policies with respect to any arbitrary downstream policy selection
metric, and we empirically demonstrate that this selection procedure significantly
outperforms existing approaches, such as ranking policies according to mean or
high-confidence lower bound value estimates.
1	Introduction
Off-policy evaluation (OPE) (Precup et al., 2000) in the context of reinforcement learning (RL) is
often motivated as a way to mitigate risk in practical applications where deploying a policy might
incur significant cost or safety concerns (Thomas et al., 2015a). Indeed, by providing methods to
estimate the value of a target policy solely from a static offline dataset of logged experience in the
environment, OPE can help practitioners determine whether a target policy is or is not safe and
worthwhile to deploy. Still, in many practical applications the ability to accurately estimate the
online value of a specific policy is less of a concern than the ability to select or rank a set of policies
(one of which may be the currently deployed policy). This problem, related to but subtly different
from OPE, is offline policy selection (Doroudi et al., 2017; Paine et al., 2020; Kuzborskij et al.,
2020), and it often arises in practice. For example, in recommendation systems, a practitioner may
have a large number of policies trained offline using various hyperparameters, while cost and safety
constraints only allow a few of those policies to be deployed as live experiments. Which policies
should be chosen to form the small subset that will be evaluated online?
This and similar questions are closely related to OPE, and indeed, the original motivations for OPE
were arguably with offline policy selection in mind (Precup et al., 2000; Jiang, 2017), the idea being
that one can use estimates of the value of a set of policies to rank and then select from this set.
Accordingly, there is a rich literature of approaches for computing point estimates of the value of
the policy (Dudik et al., 2011; BottoU et j, 2013; Jiang & Li, 2015; Thomas & BrUnskilL 2016;
Nachum et al., 2019; Zhang et al., 2020; Uehara & Jiang, 2020; Kallus & Uehara, 2020; Yang
et al., 2020). BecaUse the offline dataset is finite and collected Under a logging policy that may be
different from the target policy, prior OPE methods also estimate high-confidence lower and Upper
boUnds on a target policy’s valUe (Thomas et al., 2015a; KUzborskij et al., 2020; BottoU et al., 2013;
Hanna et al., 2016; Feng et al., 2020; Dai et al., 2020; Kostrikov & NachUm, 2020). These existing
approaches may be readily applied to oUr recommendation systems example, by Using either mean
or lower-confidence boUnd estimates on each candidate policy to rank the set and picking the top
few to deploy online.
However, this naive approach ignores crucial differences between the problem setting of OPE and
the downstream evalUation criteria a practitioner prioritizes. For example, when choosing a few
policies out of a large number of available policies, a recommendation systems practitioner may
1
Under review as a conference paper at ICLR 2021
have a number of objectives in mind: The practitioner may strive to ensure that the policy with
the overall highest groundtruth value is within the small subset of selected policies (akin to top-k
precision). Or, in scenarios where the practitioner is sensitive to large differences in achieved value,
a more relevant downstream metric may be the difference between the largest groundtruth value
within the k selected policies compared to the groundtruth of the best possible policy overall (akin
to top-k regret). With these or other potential offline policy selection metrics, it is far from obvious
that ranking according to OPE estimates is ideal (Doroudi et al., 2017).
The diversity of potential downstream metrics in offline policy selection presents a challenge to
any algorithm that yields a point estimate for each policy. Any one approach to computing point
estimates will necessarily be sub-optimal for some policy selection criteria. To circumvent this
challenge, we propose to compute a belief distribution over groundtruth values for each policy.
Specifically, with the posteriors for the distribution over value for each policy calculated, one can
use a straightforward procedure that takes estimation uncertainty into account to rank the policy
candidates according to arbitrarily complicated downstream metrics. While this belief distribution
approach to offline policy selection is attractive, it also presents its own challenge: how should one
estimate a distribution over a policy’s value in the pure offline setting?
In this work, we propose Bayesian Distribution Correction Estimation (BayesDICE) for off-policy
estimation of a belief distribution over a policy’s value. BayesDICE works by estimating posteri-
ors over correction ratios for each state-action pair (correcting for the distribution shift between the
off-policy data and the target policy’s on-policy distribution). A belief distribution of the policy’s
value may then be estimated by averaging these correction distributions over the offline dataset,
weighted by rewards. In this way, BayesDICE builds on top of the state-of-the-art DICE point esti-
mators (Nachum et al., 2019; Zhang et al., 2020; Yang et al., 2020), while uniquely leveraging pos-
terior regularization to satisfy chance constraints in a Markov decision process (MDP). As a prelim-
inary experiment, we show that BayesDICE is highly competitive to existing frequentist approaches
when applied to confidence interval estimation. More importantly, we demonstrate BayesDICE’s
application in offline policy selection under different utility measures on a variety of discrete and
continuous RL tasks. Among other findings, our policy selection experiments suggest that, while the
conventional wisdom focuses on using lower bound estimates to select policies (due to safety con-
cerns) (Kuzborskij et al., 2020), policy ranking based on the lower bound estimates does not always
lead to lower (top-k) regret. Furthermore, when other metrics of policy selection are considered,
such as top-k precision, being able to sample from the posterior enables significantly better policy
selection than only having access to the mean or confidence bounds of the estimated policy values.
2	Preliminaries
We consider an infinite-horizon Markov decision process (MDP) (Puterman, 1994) denoted as M =
hS, A, R, T, μo,γi, which consists of a state space, an action space, a deterministic reward function,1 2
a transition probability function, an initial state distribution, and a discount factor γ ∈ (0, 1]. In this
setting, a policy ∏(at∣st) interacts with the environment starting at so 〜μo and receives a scalar
reward r = R(st, at) as the environment transitions into a new state st+ι 〜T(st,at) at each
timestep t. The value of a policy is defined as
ρ(π) := (1 - γ)Es0,at,st [Pt∞=0 γtrt] .	(1)
2.1	Offline policy selection
We formalize the offline policy selection problem as providing a ranking O ∈ Perm([1, N])
over a set of candidate policies {πi}iN=1 given only a fixed dataset D = {x(j) :=
(s0j), Sj),a⑶,r⑶,SOj))}n=ι where SY)〜μo, (s(j),a(j))〜dD are samples of an unknown
distribution dD, r(j) = R(Sj),a(j)), and Slj)〜T(s(j),a(j)).2 One approach to the offline policy
selection problem is to first characterize the value of each policy (Eq. 1, also known as the normal-
ized per-step reward) via OPE under some utility function u(π) that leverages a point estimate (or
1For simplicity, we restrict our analysis to deterministic rewards, and extending our methods to stochastic
reward scenarios is straightforward.
2This tuple-based representation of the dataset is for notational and theoretical convenience, following Dai
et al. (2020); Kostrikov & Nachum (2020), among others. In practice, the dataset is usually presented as finite-
length trajectories {(s(0j) , a(0j), r0(j) , s(1j), . . . )}jm=1, and this can be processed into a dataset of finite samples
from μo and from dD X R X T. For mathematical simplicity, We assume that the dataset is sampled i.i.d. This
2
Under review as a conference paper at ICLR 2021
lower bound) of the policy value; i.e.,
O — ArgSortDescending({u(∏i)}N=ι).
2.2	Selection evaluation
A proposed ranking O will eventually be evaluated according to how well its policy ordering aligns
with the policies’ groundtruth values. In this section, we elaborate on potential forms of this evalu-
ation score.
To this end, let Us denote the groundtruth distribution of returns of policy ∏ by Z (∙∣∏i). In other
words, Z(∙∣∏i) is a distribution over R such that
∞
Z 〜Z(∙∣∏i)	≡	Z :=(1 - Y) EYt	∙ R(st,at)	；	so 〜μo, at	〜∏i(st), st+1	〜T(st,at)	. (2)
t=0
Note that Ez(∙∣∏i) [z] = ρ(∏i).
As part of the offline policy selection problem, we are given a ranking score S that is a function of
a proposed ranking O and groundtruth policy statistics { Z (∙∣ ∏i)} N=ι. The ranking score S can take
on many forms and is application specific; e.g.,
•	top-k precision: This is an ordinal ranking score. The ranking score considers the top k policies
in terms of groundtruth means ρ(πi ) and returns the proportion of these which appear in the top
k spots of O.
•	top-k accuracy: Another ordinal ranking score, this score considers the top-k policies in sorted
order in terms of groundtruth means ρ(πi) and returns the proportion of these which appear in the
same ordinal location in O.
•	top-k correlation: Another ordinal ranking score, this represents the Pearson correlation coeffi-
cient between the ranking of top-k policies in sorted order in terms of groundtruth means ρ(πi)
and the truly best top-k policies.
•	top-k regret: This is a cardinal ranking score. This score respresents the difference in groundtruth
means ρ(∏i) between the overall best policy 一 i.e., maxi ρ(∏i) — and the best policy among the
top-k ranked policies 一 i.e., maxi∈[i,k] ρ(∏o[k]).
•	Beyond expected return: One may define the above ranking scores in terms of statistics of
Z(∙∣∏i) other than the groundtruth means ρ(∏i). For example, in safety-critical applications, one
may be concerned with the variance of the policy return. Accordingly, one may define CVaR
analogues to top-k precision and regret.
For simplicity, we will restrict our attention to ranking scores which only depend on the average
return of ∏i. To this end, We will use Pi as shorthand for ρ(∏i) and assume that the ranking score S
is a function of O and {pi}N=ι.
2.3	Ranking score simulation from the posterior
It is not clear whether ranking according to vanilla OPE (either mean or confidence based) is ideal
for any of the ranking scores above, including, for example, top-1 regret in the presence of uncer-
tainty. However, if one has access to an approximate belief distribution over the policy’s values,
there is a simple sampling-based approach that can be used to find a near-optimal ranking (opti-
mality depending on how accurate the belief distribution is) with respect to an arbitrary specified
downstream ranking score, and we elaborate on this procedure here.
First, note that if we have access to the true groundtruth policy values {pi}N=ι, and the ranking score
function S, we can calculate the score value of any ranking O and find the ranking O* that optimizes
this score. However, we are limited to a finite offline dataset and the full return distributions are
unknown. In this offline setting, we propose to instead compute a belief distribution q({pi}N=ι), and
then we can optimize over the expected ranking score Eq [S(O, {pi}N=ι)] as shown in Algorithm 1.
This algorithm simulates realizations of the groundtruth values 伉}，by sampling from the belief
distribution q({pi}N=ι), and in this way estimates the expected realized ranking score S over all
is a common assumption in the OPE literature (Uehara & Jiang, 2020) and may be relaxed in some cases by
assuming a fast mixing time (Nachum et al., 2019).
3
Under review as a conference paper at ICLR 2021
possible rankings O . As we will show empirically, matching the selection process (the S used
in Algorithm 1) to the downstream ranking score naturally leads to improved performance. The
question now becomes how to effectively learn a belief distribution over {pi}N=1.
Figure 1: The belief distributions of ρ1 and p2 de-
pend on the uncertainty induced from the finite of-
fline data (D and D0). A user might prefer π2 only
if p(p2 < ρ1) < ξ (a choice of S).OPE based
on mean point estimates would select π2 in either
case as ρ2 has the greater mean. Sampling from
the posterior belief in OfflineSelect allows
simulation of any ranking score under S, aligning
policy selection with the user’s choice ofS.
Algorithm 1 OfflineSelect
Inputs Posteriors q({pi}N=1), ranking score S
Initialize O *; L*	> Track best Score
for O in Perm([1, ..., N]) do
L=0
for j = 1 to n do
SamPle {ρ(j)}N=I 〜qHPi}N=I)
>	Sum up SamPle Scores
L = L + S({P(j) }N=ι,O)
end for
ifL < L* then
>	UPdate best ranking/Score
L* = L; O* = O
end if
end for
return O* , L*
3	BAYESDICE
To learn a belief distribution over {pi}N=1, we pursue a Bayesian approach to infer an approx-
imate posterior distribution given prior beliefs. While model-based Bayesian approaches exist
(e.g., (Deisenroth & Rasmussen, 2011) and variants (Parmas et al., 2018)), they typically suffer
from compounding error, so a model-free approach is preferable. However, Bayesian inference is
challenging in this model-free scenario because the likelihood function is not easy to compute, as it
is defined over infinite horizon returns.
Therefore, we first investigate several approaches to representing policy value, before identifying a
novel posterior estimator that is computationally attractive and can support a broad range of ranking
scores for downstream tasks.
3.1	Policy Ranking Score Representation
In practice, the downstream task of ranking or selecting policy candidates might require more than
the value expectation, but also other properties of the policy value distribution. To ensure that the
necessary distribution properties are computable, we first consider the class of ranking scores we
would like to support:
•	Offline: Since we focus on ranking policies given only offline data, the ranking score should not
depend on on-policy samples.
•	Flexible: Since the downstream task may utilize different ranking scores, the representation of
the policy value should be sufficient to support their efficient computation.
With these considerations in mind, we review ways to represent the value of a policy π. Define
Qn (s,a) = E [£∞=0 YtR(st,at)∣so = s,ao = a] and
dπ (s, a) = (1 - γ) Pt∞=0γtdtπ (s, a) , with
d∏ (s,a) = P (st = s,at = a∣so 〜μo, Vi < t,，i 〜∏ (∙∣Si) ,Si+ι 〜T (∙∣Si ,ai)),
which are the state-action values and stationary visitations of π. These satisfy the recursions
Qn(s,a) = R(s,a) + Y ∙ PπQn(s,a), where PπQ(s,a) := Es/〜T(s,a)e〜∏r) [Q(s0,a0)];	⑶
dπ(s,a) = (1 — γ)μ0(s)π(a∣s) + Y ∙ Pf dπ(s,a), where Pnd(s,a) := π(a∣s) Pgg T(s|s, a)d(s, a). (4)
From these identities, the policy value can be expressed in two equivalent ways:
P(π) = (I- Y) ∙ Eao〜π(so) [Qπ (SO ,a0)]	⑸
S0 〜“0
=E(s,a)〜d∏ [r(s,a)].	⑹
4
Under review as a conference paper at ICLR 2021
Current OPE methods are generally based on one of the representations (1), (5) or (6). For example,
importance sampling (IS) estimators (PrecUP et al., 2000; Murphy et al., 2001; Dudlk et al., 2011)
are based on (1); LSTDQ (Lagoudakis & Parr, 2003) is a representative algorithm for fitting Qπ and
thus based on (5); the recent DICE algorithms (Nachum & Dai, 2020; Yang et al., 2020) estimate
the stationary density ratio Z (s, a) := d dDa so that P (π) = EdD [Z ∙ r], and are thus based on (6).
Among the three strategies, the third is the most promising in our scenario. First, IS suffers from
an exponential growth in variance (Liu et al., 2018) and further requires knowledge of the behavior
policy. In contrast, the functions Qπ and dπ are duals (Nachum & Dai, 2020; Yang et al., 2020),
and share common behavior-agnostic and minimax properties (Uehara & Jiang, 2020), However,
estimation of Qπ assumes a ranking score with a linear dependence on R (s, a), and therefore,
even if we estimate Qπ accurately, it is still impossible to evaluate ranking scores that involve
(1 - Y) E [P∞=o Ytσ(rt)] such that σ(∙) : R → R is a nonlinear function (unless one learns a
different Q function for each possible ranking score, which may be computationally expensive). By
contrast, ranking scores with such nonlinear components can be easily computed from the stationary
density ratio as EdD [Z ∙ σ (r)]. Given these considerations, the estimator via stationary density ratio
satisfies both requirements: it enjoys statistical advantages in the offline setting and is flexible for
downstream ranking score calculation. Therefore, we focus on a Bayesian estimator for Zπ next.
3.2	Stationary Ratio Posterior Estimation
Recall that to apply a simple Bayesian approach to infer the posterior of Zπ, one requires a log-
likelihood function, but such a quantity is not readily calculable in our scenario from the given
data. Therefore, we develop an alternative, computationally tractable approach by considering an
optimization view of Bayesian inference under a chance constraint, which allows us to derive the
posterior over a set of stochastic equations.
Let f (∙) denote a non-negative convex function with f (0) achieving the minimum 0, e.g., f (x)=
x>x. Also let ∆d (s,a) := (1 — Y)μ0(s)π(a∣s) + Y ∙ P∏d(s, a) — d (s,a). Starting with (5) we
reduce the |S | |A| many constraints for the stationary distribution of π to a single feature-vector-
based constraint for Z:
∆d (s, a) = 0,	∀(s, a) ∈ S × A ⇒ hφ, ∆di = 0
⇒ f (hφ,∆di) = 0 ⇒ max β> hφ, ∆di- fXe) = 0
β∈Hφ
(7)
(8)
⇒
max EdD [Z (s, a) ∙ β> (γφ(s0, a0) - Φ (s, a))] +(1- Y) Eμo∏ [β>φ] - f * (β) = 0,(9)
β∈Hφ
where Hφ denotes the bounded Hilbert space with the feature mappings φ, dD denotes the distribu-
tion generating the empirical experience, and we have used Fenchel duality in the middle step. The
function φ (∙, ∙) : S X A → Rm is a feature mapping, with m possibly infinite. Then the condition
hφ, ∆di = 0 can be understood as matching the two distributions (1 - Y)μo(s)π(a∣s)+ Y ∙P∏ d(s, a)
and d (s, a) in terms of their embeddings (Smola et al., 2007), which is a generalization of the ap-
proximation methods in (De Farias & Van Roy, 2003; Lakshminarayanan et al., 2017). In particular,
when |S| |A| is finite and we set φ(s, a) = δs,a, where δs,a ∈ {0, 1}|S||A| is an indicator vector
with a single 1 at position (s, a) and 0 otherwise, we are matching the distributions pointwise. The
feature map φ (s, a) can also be set to general reproducing kernel k ((s, a), ∙) ∈ R∞. As long as the
kernel k (∙, ∙) is characteristic, the embeddings will match if and only if the distributions are identical
almost surely (Sriperumbudur et al., 2011).
Given that the experience was collected by some other means, i.e., D 〜dD, the constraint for Z
in (7) might not hold exactly. Therefore, we consider a feasible set Z ∈ {Z : ` (Z, D) 6 } where
' (Z, D) := maxβ∈Hφ ED [Z (s, a) ∙ β> (yΦ(s0, a0) - φ (s, a)) - f * (β)] +(1 - y) Eμ0∏ [β>φ].
(10)
Note that ` (Z) > 0 since Hφ is symmetric. We expect the posterior of Z, q (Z), to concentrate most
of its mass on this set and balance the prior. Formally, this means
min KL(q||p) - λξ, s.t. Pq (` (Z) 6 ) > ξ,	(11)
q
where the chance constraint considers the probability of the feasibility of Z under the posterior. This
formulation can be equivalently rewritten as
min KL (q||p) - λPq (` (Z) 6 )	(12)
q
5
Under review as a conference paper at ICLR 2021
Then, by applying Markov,s inequality, i.e., Pq (' (Z) 6 e) = 1 - Pq (' (Z) > E) > 1 - Eq [：(z)], We
can obtain an upper bound on (12) as
mqin KL⑷p) + λEq['(Z，D)]
(13)
mζn qmax) KL(q|1p)+: Eq(Z)q(eiz)hED [Z Ga) ∙"…a0)- φ (s，吟—f *(e)]
+ (I- Y) Eμo∏ [β>φ] i ,
(14)
Where the equality folloWs by interchangeability (Shapiro et al., 2014; Dai et al., 2017). We amortize
the optimization for β w.r.t. each Z to a distribution q (β∣Z) to reduce the computational effort.
Due to the space limitation, we postpone the discussion about the important properties of Bayes-
DICE, including the parametrization of the posteriors, the variants of BayesDICE for undiscounted
MDP and alternatives of the log-likelihoods, and the connections to the vanilla Bayesian stochastic
processes, to Appendix A. Please refer the details there.
Finally, note that with the posterior approximation for Zi, denoting the estimate for candidate policy
i, we can draw posterior samples of Pi by drawing a sample Zi 〜 q(Zi) and computing Pi =
* P(S a r)∈D Zi(s, a)r. ThiS defines a posterior distribution over Pi and we further assume that the
distributions are independent for each policy, so q({pi}N=1) = Qi q(pi). This defines the necessary
inputs for OfflineSelect to determine a ranking of the candidate policies.
4	Related work
We categorize the relevant related work into three categories: offline policy selection, off-policy
evaluation, and Bayesian inference for policy evaluation.
Offline policy selection The decision making problem we formalize as offline policy selection
is a member of a set of problems in RL referred to as model selection. Previously, this term has
been used to refer to state abstraction selection (Jiang, 2017; Jiang et al., 2015) as well as learning
algorithm and feature selection (Foster et al., 2019; Pacchiano et al., 2020). More relevant to our
proposed notion of policy selection are a number of previous works which use model selection to
refer to the problem of choosing a near-optimal Q-function from a set of candidate approximation
functions (Fard & Pineau, 2010; Farahmand & Szepesvari, 2011; Irpan et al., 2019; Xie & Jiang,
2020). In this case, the evaluation metric is typically defined as the L∞ norm of difference of Q
versus the state-action value function of the optimal policy Q*. While one can relate this evaluation
metric to the sub-optimality (i.e., regret) of the policy induced by the Q-function, we argue that
our proposed policy selection problem is both more general - since we allow for the use of policy
evaluation metrics other than sub-optimality - and more practically relevant - since in many practical
applications, the policy may not be expressible as the argmax of a Q-function. Lastly, the offline
policy selection problem we describe is arguably a formalization of the problem approached in Paine
et al. (2020) and referred to as hyperparameter selection. In contrast to this previous work, we not
only formalize the decision problem, but also propose a method to directly optimize the policy
selection evaluation metric. Offline policy selection has also been studied by Doroudi et al. (2017),
which considers what properties a point estimator should have in order for it to yield good rankings
in terms of a notion of ranking score referred to as fairness.
Off-policy evaluation Off-policy evaluation (OPE) is a highly active area of research. While
the original motivation for OPE was in the pursuit of policy selection (Precup et al., 2000; Jiang,
2017), the field has historically almost exclusively focused on the related but distinct problem of
estimating the online value (accumulated rewards) ofa single target policy. In addition to a plethora
of techniques for providing point estimates of this groundtruth value (Dudik et al., 2011; Bottou
et al., 2013; Jiang & Li, 2015; Thomas & Brunskill, 2016; Kallus & Uehara, 2020; Nachum et al.,
2019; Zhang et al., 2020; Yang et al., 2020), there is also a growing body of literature that uses
frequentist principles to derive high-confidence lower bounds for the value ofa policy (Bottou et al.,
2013; Thomas et al., 2015b; Hanna et al., 2016; Kuzborskij et al., 2020; Feng et al., 2020; Dai et al.,
2020; Kostrikov & Nachum, 2020). As our results demonstrate, ranking or selecting policies based
on either their estimated mean or lower confidence bounds can at times be sub-optimal, depending
on the evaluation criteria.
6
Under review as a conference paper at ICLR 2021
Bayesian inference for policy evaluation Our proposed method for policy selection relies on
Bayesian principles to estimate a posterior distribution over the groundtruth policy value. While
many Bayesian-inspired methods have been proposed for policy optimization (Deisenroth & Ras-
mussen, 2011; Parmas et al., 2018), especially in the context of exploration (Houthooft et al., 2016;
Dearden et al., 2013; Kolter & Ng, 2009), relatively few have been proposed for policy evaluation.
In one instance, Fard & Pineau (2010) derive PAC-Bayesian bounds on estimates of the Bellman
error of a candidate Q-value function. In contrast to this work, we use our BayesDICE algorithm to
estimate a distribution over target policy value, and this distribution allows us to directly optimize
arbitrary downstream policy selection metrics.
5	Experiments
We empirically evaluate the performance of BayesDICE on confidence interval estimation (which
can be used for policy selection) and offline policy selection under linear and neural network pos-
terior parametrizations on tabular - Bandit, Taxi (Dietterich, 1998), FrozenLake (Brockman et al.,
2016) - and continuous-control - Reacher (Brockman et al., 2016) - tasks. As We show below,
BayesDICE outperforms existing methods for confidence interval estimation, producing accurate
coverage while maintaining tight interval width, suggesting that BayesDICE achieves accurate pos-
terior estimation, being robust to approximation errors and potentially misaligned Bayesian priors
in practice. Moreover, in offline policy selection settings, matching the selection algorithm (Algo-
rithm 1) to the ranking score (enabled by the estimating the posterior) shows clear advantages over
ranking based on point estimates or confidence intervals on a variety of ranking scores. See Ap-
pendix C for additional results and implementation details.
5.1	Confidence interval estimation
Before applying BayesDICE to policy selection, we evaluate the BayesDICE approximate posterior
by computing the accuracy of the confidence intervals it produces. We compare BayesDICE against
a known set of confidence interval estimators based on concentration inequalities. To compute these
baselines, we first use weighted (i.e., self-normalized) per-step importance sampling (Thomas &
Brunskill, 2016) to compute a policy value estimate for each logged trajectory. These trajectories
provide a finite sample of value estimates. We use self-normalized importance sampling since it has
been found to yield better empirical results in MDPs despite being biased (Liu et al., 2018; Nachum
et al., 2019); for Bandit results without self-normalization, see Figure 5 in Appendix C. We then use
empirical Bernstein’s inequality (Thomas et al., 2015b), bias-corrected bootstrap (Thomas et al.,
2015a), and Student’s t-test to derive lower and upper high-confidence bounds on these estimates.
We further consider Bayesian Deep Q-Networks (BDQN) (Azizzadenesheli et al., 2018) with an
average empirical reward prior in the function approximation setting, which applies Bayesian linear
regression to the last layer of a deep Q-network to learn a distribution of Q-values. Both BayesDICE
and BDQN output a distribution of parameters, from which we conduct Monte Carlo sampling and
use the resulting samples to compute a confidence interval at a given confidence level.
We plot the empirical coverage and interval width at different confidence levels in Figure 2. To
compute the empirical interval coverage, we conduct 200 trials with randomly sampled datasets.
The interval coverage is the proportion of the 200 intervals that contains the true value of the target
policy. The interval log-width is the median of the log width of the 200 intervals. As shown in
Figure 2, BayesDICE’s coverage closely follows the intended coverage (black dotted line), while
maintaining narrow interval width across all tasks considered. This suggests that BayesDICE’s pos-
terior estimation is highly accurate, being robust to approximation errors and potentially misaligned
Bayesian priors in practice.
5.2	Policy selection
Next, we demonstrate the benefit of matching the policy selection criteria to the ranking score
in offline policy selection. Our evaluation is based on a variety of cardinal and ordinal ranking
scores defined in Section 2.2. We begin by considering the use of Algorithm 1 with BayesDICE-
approximated posteriors. By keeping the BayesDICE posterior fixed, we focus our evaluation on the
performance of Algorithm 1. We plot the groundtruth performance of this procedure applied to Ban-
dit and Reacher in Figure 3. These figures compare using different S to rank the policies according
to Algorithm 1 across different downstream ranking scores S. We find that aligning the criteria S
used in Algorithm 1 with the downstream ranking score S is empirically the best approach (S = S).
7
Under review as a conference paper at ICLR 2021
# samples = 50
3°∙eJ3AO∙"ΛJ0≡Ifi-ΛVa,-"ΛJaUI
1
-3
▽ BayesDICE (ours) ☆ Bootstrapping Q Student t
O BDQN	♦ Bernstein	-----Expected coverage
Bandit
Frozenlake
Taxi
Confidence interval (1 — α)
ReaCher
# trajectories = 25


Figure 2: Confidence interval estimation on Bandit, FrozenLake, Taxi, and Reacher. The y-axis
shows the empirical coverage and median log-interval width across 200 trials. BayesDICE exhibits
near true coverage while maintaining narrow interval width, suggesting an accurate posterior ap-
proximation.
In contrast, using point estimates such as Mean or Mean ± Std can yield much worse down-
stream performance. We also see that in the Bandit setting, where we can analytically compute the
Bayes-optimal ranking, using aligned ranking scores in conjunction with BayesDICE-approximated
posteriors achieves near-optimal performance.
W = MeBlI	Mean-Std	MeBn+ Std	Bayes-OPtimal -≠- S ^B^ S
Bandit	Reacher
Figure 3: Policy selection using top-k ranking scores compared to mean/confidence ranking ap-
proaches on two-armed Bandit and Reacher. In these experiments, we fix the posterior to the one
approximated by BayesDICE and evaluate different S used in Algorithm 1 to compute a policy
ranking. We find that using S = S (i.e., aligning the ranking score in posterior simulation with the
groundtruth evaluation) results in better performance than simple point estimates. Interestingly, the
lower-bound point estimate almost always performs worse than the mean or the upper bound.
Having established BayesDICE’s ability to compute accurate posterior distributions as well as the
benefit of appropriately aligning the ranking score used in Algorithm 1, we compare BayesDICE
to state-of-the-art OPE methods in policy selection. In these experiments, we use Algorithm 1
with posteriors approximated by BayesDICE and S = S . We compare the use of BayesDICE in this
way to ranking via point estimates of DualDICE (Nachum et al., 2019) and other confidence-interval
estimation methods introduced in Section 5.1. We present results in Figure 4, in terms of top-k regret
and correlation on bandit and reacher across different sample sizes and behavior data. BayesDICE
outperforms other methods on both tasks. See additional ranking results in Appendix C.
Student-t . DualDICE -⅛- BayesDICE
Reacher
Estimator	-★- Bootstrap Bemsteln
Bandit
0	100	200 -	25	50	100	200	25	50	100	200	10 2S SO 100
*Tπ∣js	*1¾js	≠Tr⅛s	≠Tr⅞js
Figure 4: Policy selection evaluation under correlation and regret at top-k in two-armed Bandit (left)
and Reacher (right) compared to other methods using point estimate (DualDICE) or high-confidence
lower bounds. Please see Appendix C for more results with respect to other downstream metrics.
RBgret-T⅛p3, α-0.6
Coirelatlon TbP3. «=0.1
10	25	50	100
*1⅞js
6	Conclusion
In this paper, we formally defined the offline policy selection problem, and proposed BayesDICE
to first estimate posterior distributions of policy values before using a simulation-based procedure
8
Under review as a conference paper at ICLR 2021
to compute an optimal policy ranking. Empirically, BayesDICE not only provides accurate belief
distribution estimation, but also shows excellent performance in policy selection tasks.
References
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA),
pp. 1-9. IEEE, 2018.
Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton Uni-
versity Press, 2009.
Leon Bottou, Jonas Peters, Joaquin Quinonero-Candela, Denis X. Charles, D. Max Chickering, Elon
Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and learning
systems: The example of computational advertising. Journal of Machine Learning Research, 14
(65):3207-3260, 2013. URL http://jmlr.org/papers/v14/bottou13a.html.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.
Bo Dai, Niao He, Hanjun Dai, and Le Song. Provable bayesian inference via particle mirror descent.
In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pp.
985-994, 2016.
Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional distributions
via dual embeddings. In Artificial Intelligence and Statistics, pp. 1458-1467, 2017.
Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schuurmans. Coindice:
Off-policy confidence interval estimation. In Advances in Neural Information Processing Systems,
2020.
Daniela Pucci De Farias and Benjamin Van Roy. The linear programming approach to approximate
dynamic programming. Operations research, 51(6):850-865, 2003.
Richard Dearden, Nir Friedman, and David Andre. Model-based bayesian exploration. arXiv
preprint arXiv:1301.6690, 2013.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472, 2011.
Thomas G Dietterich. The maxq method for hierarchical reinforcement learning. In ICML, vol-
ume 98, pp. 118-126. Citeseer, 1998.
Shayan Doroudi, Philip S Thomas, and Emma Brunskill. Importance sampling for fair policy selec-
tion. Grantee Submission, 2017.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv
preprint arXiv:1103.4601, 2011.
Yaakov Engel, Shie Mannor, and Ron Meir. Bayes meets bellman: The gaussian process approach
to temporal difference learning. In Proceedings of the 20th International Conference on Machine
Learning (ICML-03), pp. 154-161, 2003.
Amir-massoud Farahmand and Csaba Szepesvari. Model selection in reinforcement learning. Ma-
chine learning, 85(3):299-332, 2011.
Mahdi M Fard and Joelle Pineau. Pac-bayesian model selection for reinforcement learning. In
Advances in Neural Information Processing Systems, pp. 1624-1632, 2010.
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu. Accountable off-policy evaluation with
kernel bellman statistics. arXiv preprint arXiv:2008.06668, 2020.
9
Under review as a conference paper at ICLR 2021
Dylan J Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits.
In Advances in Neural Information Processing Systems, pp. 14741-14752, 2019.
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar. Bayesian reinforcement
learning: A survey. arXiv preprint arXiv:1609.04436, 2016.
Josiah P Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals
for off-policy evaluation. arXiv preprint arXiv:1606.06126, 2016.
Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:
Variational information maximizing exploration. In Advances in Neural Information Processing
Systems, pp. 1109-1117, 2016.
Alexander Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, and Sergey
Levine. Off-policy evaluation via off-policy classification. In Advances in Neural Information
Processing Systems, pp. 5437-5448, 2019.
Nan Jiang. A Theory of Model Selection in Reinforcement Learning. PhD thesis, 2017.
Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning.
arXiv preprint arXiv:1511.03722, 2015.
Nan Jiang, Alex Kulesza, and Satinder Singh. Abstraction selection in model-based reinforcement
learning. In International Conference on Machine Learning, pp. 179-188, 2015.
Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evalu-
ation in markov decision processes. Journal of Machine Learning Research, 21(167):1-63, 2020.
J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In Proceedings of
the 26th annual international conference on machine learning, pp. 513-520, 2009.
Ilya Kostrikov and Ofir Nachum. Statistical bootstrapping for uncertainty estimation in off-policy
evaluation, 2020.
Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesvari. Confident off-policy
evaluation and selection through self-normalized importance weighting. arXiv preprint
arXiv:2006.10460, 2020.
Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning
research, 4(Dec):1107-1149, 2003.
Chandrashekar Lakshminarayanan, Shalabh Bhatnagar, and Csaba Szepesvari. A linearly relaxed
approximate linear program for markov decision processes. CoRR, abs/1704.02544, 2017.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
S. Murphy, M. van der Laan, and J. Robins. Marginal mean models for dynamic regimes. Journal
of American Statistical Association, 96(456):1410-1423, 2001.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint
arXiv:2001.01866, 2020.
Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic estimation
of discounted stationary distribution corrections. In Advances in Neural Information Processing
Systems, pp. 2315-2325, 2019.
Arkadi Nemirovski and Alexander Shapiro. Convex approximations of chance constrained pro-
grams. SIAM Journal on Optimization, 17(4):969-996, 2007.
Brendan ODonoghue, Ian Osband, Remi Munos, and Volodymyr Mnih. The uncertainty bellman
equation and exploration. In International Conference on Machine Learning, pp. 3836-3845,
2018.
10
Under review as a conference paper at ICLR 2021
Ian Osband, Benjamin Van Roy, Daniel J Russo, and Zheng Wen. Deep exploration via randomized
value functions. Journal of Machine Learning Research, 20(124):1-62, 2019.
Aldo Pacchiano, My Phan, Yasin Abbasi-Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and
Csaba Szepesvari. Model selection in contextual stochastic bandit problems. arXiv preprint
arXiv:2003.01704, 2020.
Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander
Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement
learning. arXiv preprint arXiv:2007.09055, 2020.
Paavo Parmas, Carl Edward Rasmussen, Jan Peters, and Kenji Doya. Pipps: Flexible model-based
policy search robust to the curse of chaos. In International Conference on Machine Learning, pp.
4065-4074. PMLR, 2018.
Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy
evaluation. In Proceedings of the 17th International Conference on Machine Learning, pp. 759-
766, 2000.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., 1994.
Alexander Shapiro, Darinka Dentcheva, and Andrzej RUszczynski. Lectures on stochastic Program-
ming: modeling and theory. SIAM, 2014.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A hilbert space embedding for
distributions. In International Conference on Algorithmic Learning Theory, pp. 13-31. Springer,
2007.
Bharath K Sriperumbudur, Kenji Fukumizu, and Gert RG Lanckriet. Universality, characteristic
kernels and rkhs embedding of measures. Journal of Machine Learning Research, 12(7), 2011.
Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement
learning. In International Conference on Machine Learning, pp. 2139-2148, 2016.
Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy
improvement. In International Conference on Machine Learning, pp. 2380-2388, 2015a.
Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-
policy evaluation. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015b.
Masatoshi Uehara and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation.
2020.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability, 2020.
Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via
the regularized lagrangian. In Advances in Neural Information Processing Systems, 2020.
Arnold Zellner. Optimal Information Processing and Bayes’s Theorem. The American Statistician,
42(4), November 1988.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. GenDICE: Generalized offline estimation
of stationary values. In International Conference on Learning RePresentations, 2020.
Jun Zhu, Ning Chen, and Eric P. Xing. Bayesian inference with posterior regularization and ap-
plications to infinite latent svms. J. Mach. Learn. Res., 15(1):1799-1847, January 2014. ISSN
1532-4435.
11
Appendix
A More Discussions on BayesDICE
In this section, we provide more details about BayesDICE.
Remark (parametrization of q (Z) and q (β∣Z)): We parametrize both q (Z) (and the result-
ing q (β∣Z)) as Gaussians with the mean and variance approximated by a multi-layer perceptron
(MLP), i.e.: Z = MLPw (s, a) + σ*oξ, ξ 〜N(0,1). W and w0 denote the parameters of the MLP.
Remark (connection to Bayesian inference for stochastic processes): Recall the posterior can
be viewed as the solution to an optimization (Zellner, 1988; Zhu et al., 2014; Dai et al., 2016),
q (Z|D) = argmin hq (Z), logp (Z, D)i +KL(q(Z) ||p(Z))
q∈P
The (13) is equivalent to define the log-likelihood proportion to ` (Z, D), which is a stochastic pro-
cess, including Gaussianprocess (GP) by setting f * (β) = 1 β>β. Specifically, plug f (β) = 1 β>β
back into (13), We have β* = ED [Z (s, a) ∙ (γφ(s0, a0) - φ (s, a))] + (1 - Y) Eμo∏ [φ], resulting the
optimization
min KL (q||p) +
q
:EqEμo∏ED [ζ (si, aι)> k ((si,
a1, s01, a01) , (s2, a2
s02, a02)) Z (s2, a2) , (15)
with the kernel k (x1, x2) := (γφ(s01, a01) - φ (s1, a1))> (γ φ(s02, a02) - φ (s2, a2)) +
(I- γ)2φ(s0,a0)>φ(s0,a0)	+ 2(1 -	γ)φ(sι,a0)>	(γφ(s2,a2)	-φ(s2,a2)),	which is a
GP. Obviously, with different choices of f * (∙), the BayesDICE framework is far beyond GP.
Although the GP has been applied for RL (Engel et al., 2003; Ghavamzadeh et al., 2016; Aziz-
zadenesheli et al., 2018), they all focus on prior on value function; while BayesDICE considers
general stochastic processes likelihood, including GP, for the stationary ratio modeling, which as
we justified is more flexible for different selection criteria in downstream tasks.
Remark (auxilary constraints and undiscounted MDP): As Yang et al. (2020) suggested, the
non-negative and normalization constraints are important for optimization. We exploit positive neu-
ron to ensure the non-negativity of the mean of the q (Z). For the normalization, we consider the
2

chance constraints P ((ED (Z) - 1)6 eij > ξι. By applying the same technique, it leads to
extra term ∣1 Eq [maXα∈R a ∙ ED [Z - lf∣ in (13).
With the normalization condition introduced, the proposed BayesDICE is ready for undiscounted
MDP by simply setting γ = 1 in (13) together with the above extra term for normalization.
Remark (variants of log-likelihood): We apply the Markov’s inequality to (12) for the upper
bound (13). In fact, the optimization with chance constraint has rich literature (Ben-Tal et al., 2009),
where plenty of surrogates can be derived with different safe approximation. For example, if the q is
simple, one can directly calculate the CDF for the probability Pq (` (Z) 6 e); or one can also exploit
different probability inequalities to derive other surrogates, e.g., condition value-at-risk, i.e.,
min KL(q∣∣p)+ λ inf t + LEq [' (Z) — t]	,	(16)
and Bernstein approximation (Nemirovski & Shapiro, 2007). These surrogates lead to better ap-
proximation to the chance probability Pq (` (Z ) 6 e) with the extra cost in optimization.
B	BayesDICE for Exploration vs. Exploitation Tradeoff
In main text, we mainly consider exploiting BayesDICE for estimating various ranking scores for
both discounted MDP and undiscounted MDP. In fact, with the posterior of the stationary ratio
computed, we can also apply it for better balance between exploration vs. exploitation for policy
optimization.
12
Under review as a conference paper at ICLR 2021
Instead of selecting from a set of policy candidates, the policy optimization is considering all fea-
sible policies and selecting optimistically. Specifically, the feasibility of the stationary state-action
distribution can be characterized as
X d (s, a) = (1 - Y) μo + P*d (S),	Vs ∈ S,	(17)
a
where P*d (s) := Eq & T (s|s, a) d (s, a). Apply the feature mapping for distribution matching, We
obtain the constraint for Z ∙ π with Z (s, a):=4黑：)as
maχ	β>EdD	E	(Z (s, a) π (a|s)) φ (s)	- Y (Z (s,	a)	π	(a∣s)) φ (s0)	+(1 - Y) E“0	[β>φ]-f *	(β) = 0.
β∈Hφ
a
(18)
Then, we have the posteriors for all valid policies should satisfies
λPq (' (Z∙∏, D) 6 e) > ξ,	(19)
with ' (Z∙∏,D) := maxβ∈H β>ED Pa (Z (s,a) π (a∣s)) φ (s) - γ (Z (s,a) π (a∣s)) φ (s0)] +
(1 - γ) Eμo [β>φ] - f * (β). Meanwhile, we will select one posterior from among these poste-
riors of all valid policies optimistically, i.e.,
maχ、	Eq [U (τ, r, D)] + λιξ - λKL (q (Z) q (∏) ||p (Z, ∏))	(20)
q(ζ)q(π)
s.t.	Pq (' (Z∙π, D) 6 e) > ξ	(21)
where Eq [U (τ, r, D)] denotes the optimistic policy score to capture the upper bound of the policy
value estimation. For example, the most widely used one is
Eq [U (τ, r, D)] =	EqED [τ	∙ r]	+ λuEq	^ED	[τ ∙ r] - EqED	[τ	∙	r]),
where the second term is the empirical variance and usually known as one kind of “exploration
bonus”.
Then the whole algorithm is iterating between solving (20) and use the obtain policy collecting data
into D in (20).
This Exploration-BayesDICE follows the same philosophy of Osband et al. (2019); ODonoghue
et al. (2018) where the variance of posterior of the policy value is taken into account for exploration.
However, there are several significant differences: i), the first and most different is the modeling
object, Osband et al. (2019); ODonoghue et al. (2018) is updating with Q-function, while we are
handling the dual representation; ii), BayesDICE is compatible with arbitary nonlinear function
approximator, while Osband et al. (2019); ODonoghue et al. (2018) considers tabular or linear func-
tions; iii), BayesDICE is considering infinite-horizon MDP, while Osband et al. (2019); ODonoghue
et al. (2018) considers fixed finite-horizon case. Therefore, the exploration with BayesDICE pave
the path for principle and practical exploration-vs-exploitation algorithm. The regret bound is out of
the scope of this paper, and we leave for future work.
C	Experiment details and additional results
C.1 Environments and policies.
Bandit. We create a Bernoulli two-armed bandit with binary rewards where α controls the propor-
tion of optimal arm (α = 0 and α = 1 means never and always choosing the optimal arm respec-
tively). Our selection experiments are based on 5 target policies with α = [0.75, 0.8, 0.85, 0.9, 0.95].
Reacher. We modify the Reacher task to be infinite horizon, and sample trajectories of length 100
in the behavior data. To obtain different behavior and target policies, We first train a deterministic
policy from OpenAI Gym (Brockman et al., 2016) until convergence, and define various policies by
converting the optimal policy into a Gaussian policy with optimal mean with standard deviation 0.4-
0.3α. Our selection experiments are based on 5 target policies with α = [0.75, 0.8, 0.85, 0.9, 0.95].
13
Under review as a conference paper at ICLR 2021
C.2 Details of neural network implementation
We parametrize the distribution correction ratio as a Gaussian using a deep neural network for the
continuous control task. Specifically, we use feed-forward networks with two hidden-layers of 64
neurons each and ReLU as the activation function. The networks are trained using the Adam opti-
mizer (β1 = 0.99, β2 = 0.999) with batch size 2048.
C.3 Additional experimental results
▼ BayesDICE (ours)	♦ UN-Bernstein --------Expected coverage
☆ UN-Bootstrapping	⅛ UN-Student t
Bandit (no normalization)
# samples = 50	# samples = 100	# samples = 200
qJPIAV⅛o = eλibui
Figure 5: Confidence interval estimation on Bandit where the baselines are unnormalized (UN).
▼ BayesDICE (ours)	♦ Marginal Bernstein --------Expected coverage
★	Marginal Bootstrapping	⅛ Marginal Student t
Frozenlake (marginal)	Taxi (marginal)
# trajectories = 50	# trajectories = 100	# trajectories = 20	# trajectories = 50
WPIM⅛OIlEAJə-uɪ
Figure 6: Confidence interval estimation with baselines computed from marginalized importance
sampling.
14
Under review as a conference paper at ICLR 2021
Score -⅜- Mean -⅛- Mean - Std
True Top3 ~H~ ACeZeOlTeIation/Precision/RegTet Top3
Score = Acc. Top3	Score = Correlation Top3	Score = Precisaon Top3	Score = Regret Top3
25	50	100	200	25	50 IQO 200	25	50	100	200	25	50	100	200
# Tra∣is	# Tr⅛s	# T同S	# Tr⅛s
Score ' Mean -⅛- Mean - Std ------------------- True Top4	-■ Acc∕CorτeIation∕Precision∕Regτet Top4
Figure 7: Additional k values for top-k ranking on bandit. Ranking results based on Algorithm 1
(blue lines) always perform better than using mean or high-confidence lower bound.
S∞re Mean -A- Mean - Std -⅛- Acc∕CorreIationZPrecision∕Regτet Top2
Figure 8: Additional k values for top-k ranking on reacher and additional scores (precision and
regret). Ranking results based on Algorithm 1 (blue lines) generally perform much better than using
mean or high-confidence lower bound for top-k accuracy and correlation. Precision and regret are
similar between posterior samples and the mean/confidence bound based ranking.
15
Under review as a conference paper at ICLR 2021
Bandit
EStimator Bootstrap Bemstein Student-t DualDICE BayesDICE
Score = Regret Top2
Score = Regret Top3
0.020
0.015
0.010
0.005
0.020
0.015
0.010
0.005
0.015
0.010
0.005
Score = Regret Top4
# Trajs
Figure 9:	Improved regret using BayesDICE across all trajectory lengths, behavior data, and top-k
values considered for the bandit task.
16
Under review as a conference paper at ICLR 2021
Reacher
Estimator Bootstrap Bemstein
Student-t -φ- DualDICE BayesDICE
Figure 10:	Improved correlation using BayesDICE across all trajectory lengths, behavior data, and
top-k values considered for the reacher task.
17