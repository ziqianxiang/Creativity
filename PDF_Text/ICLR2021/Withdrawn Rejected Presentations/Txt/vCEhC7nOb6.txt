Under review as a conference paper at ICLR 2021
Inductive Bias of Gradient Descent for Expo-
nentially Weight Normalized Smooth Homo-
geneous Neural Nets
Anonymous authors
Paper under double-blind review
Ab stract
We analyze the inductive bias of gradient descent for weight normalized smooth
homogeneous neural nets, when trained on exponential or cross-entropy loss. Our
analysis focuses on exponential weight normalization (EWN), which encourages
weight updates along the radial direction. This paper shows that the gradient flow
path with EWN is equivalent to gradient flow on standard networks with an adap-
tive learning rate, and hence causes the weights to be updated in a way that prefers
asymptotic relative sparsity. These results can be extended to hold for gradient de-
scent via an appropriate adaptive learning rate. The asymptotic convergence rate
of the loss in this setting is given by Θ(跳匕；古尸),and is independent of the depth of
the network. We contrast these results with the inductive bias of standard weight
normalization (SWN) and unnormalized architectures, and demonstrate their im-
plications on synthetic data sets. Experimental results on simple data sets and
architectures support our claim on sparse EWN solutions, even with SGD. This
demonstrates its potential applications in learning prunable neural networks.
1	Introduction
The prevailing hypothesis for explaining the generalization ability of deep neural nets, despite their
ability to fit even random labels (Zhang et al., 2017), is that the optimisation/training algorithms such
as gradient descent have a ‘bias’ towards ‘simple’ solutions. This property is often called inductive
bias, and has been an active research area over the past few years.
It has been shown that gradient descent does indeed seem to prefer ‘simpler’ solutions over more
‘complex’ solutions, where the notion of complexity is often problem/architecture specific. The
predominant line of work typically shows that gradient descent prefers a least norm solution in some
variant of the L2 -norm. This is satisfying, as gradient descent over the parameters abides by the
rules of L2 geometry, i.e. the weight vector moves along direction of steepest descent, with length
measured using the Euclidean norm. However, there is nothing special about the Euclidean norm in
the parameter space, and hence several other notions of ‘length’ and ‘steepness’ are equally valid. In
recent years, several alternative parameterizations of the weight vector, such as Batch normalization
and Weight normalization, have seen immense success and these do not seem to respect L2 geometry
in the ‘weight space’. We pose the question of inductive bias of gradient descent for some of these
parameterizations, and demonstrate interesting inductive biases. In particular, it can still be argued
that gradient descent with these reparameterizations prefers simpler solutions, but the notion of
complexity is different.
1.1	Contributions
The three main contributions of the paper are as follows.
•	We establish that the gradient flow path with exponential weight normalization is equal
to the gradient flow path of an unnormalized network using an adaptive neuron depen-
dent learning rate. This provides a crisp description of the difference between exponential
weight normalized networks and unnormalized networks.
1
Under review as a conference paper at ICLR 2021
•	We establish the inductive bias of gradient descent on standard weight normalized and
exponentially weight normalized networks and show that exponential weight normalization
is likely to lead to asymptotic sparsity in weights.
•	We provide tight asymptotic convergence rates for exponentially weight normalized net-
works.
2	Related Work
2.1	Inductive Bias
Soudry et al. (2018) showed that gradient descent(GD) on the logistic loss with linearly separable
data converges to the L2 maximum margin solution for almost all datasets. These results were ex-
tended to loss functions with super-polynomial tails in Nacson et al. (2019b). Nacson et al. (2019c)
extended these results to hold for stochastic gradient descent(SGD) and Gunasekar et al. (2018a)
extended the results for other optimization geometries. Ji & Telgarsky (2019b) provided tight con-
vergence bounds in terms of dataset size as well as training time. Ji & Telgarsky (2019a) provide
similar results when the data is not linearly separable.
Ji & Telgarsky (2019c) showed that for deep linear nets, under certain conditions on the initial-
ization, for almost all linearly separable datasets, the network, in function space, converges to the
maximum margin solution. Gunasekar et al. (2018b) established that for linear convolutional nets,
under certain assumptions regarding convergence of gradients etc, the function converges to a KKT
point of the maximum margin problem in fourier space. Nacson et al. (2019a) shows that for smooth
homogeneous nets, the network converges to a KKT point of the maximum margin problem in pa-
rameter space. Lyu & Li (2020) established these results with weaker assumptions and also provide
asymptotic convergence rates for the loss. Chizat & Bach (2020) explore the inductive bias for a
2-layer infinitely wide ReLU neural net in function space and show that the function learnt is a
max-margin classifier for variation norm.
2.2	Normalization
Salimans & Kingma (2016) introduced weight normalization and demonstrated that it replicates the
convergence speedup of BatchNorm. Similarly, other normalization techniques have been proposed
as well(Ba et al., 2016)(Qiao et al., 2020)(Li et al., 2019), but only a few have been theoretically
explored. Santurkar et al. (2018) demonstrated that batch normalization makes the loss surface
smoother and L2 normalization in batchnorm can even be replaced by L1 and L∞ normalizations.
Kohler et al. (2019) showed that for GD, batchnorm speeds up convergence in the case of GLM
by splitting the optimization problem into learning the direction and the norm. Cai et al. (2019)
analyzed GD on BN for squared loss and showed that it converges for a wide range of lr. Bjorck
et al. (2018) showed that the primary reason BN allows networks to achieve higher accuracy is
by enabling higher learning rates. Arora et al. (2019) showed that in case of GD or SGD with
batchnorm, lr for scale-invariant parameters does not affect the convergence rate towards stationary
points. Du et al. (2018) showed that for GD over one-hidden-layer weight normalized CNN, with a
constant probability over initialization, iterates converge to global minima. Qiao et al. (2019) com-
pared different normalization techniques from the perspective of whether they lead to points, where
neurons are consistently deactivated. Wu et al. (2019) established the inductive bias of gradient flow
with weight normalization for overparameterized least squares and showed that for a wider range
of initializations as compared to normal parameterization, it converges to the minimum L2 norm
solution. Dukler et al. (2020) analyzed weight normalization for multilayer ReLU net in the infinite
width regime and showed that it may speedup convergence. Some other papers(Luo et al., 2019;
Roburin et al., 2020) also provide other perspectives to think about normalization techniques.
3	Problem Setup
We use a standard view of neural networks as a collection of nodes/neurons grouped by layers.
Each node u is associated with a weight vector wu , that represents the incoming weight vector
for that node. In case of CNNs, weights can be shared across different nodes. w represents all
2
Under review as a conference paper at ICLR 2021
-5	0	5	10	15	-5	0	5	10	0.0	2.5	5.0	7.5	10.0
(a) EWN	(b) SWN	(c) Unnorm
Figure 1: L2 neighborhoods with = 0.5 radius in parameter space for different parameterizations.
For EWN in the left (resp. SWN in the middle) the parameter [γ, v] (the parameter [α, v] resp.) is
restricted to a 3-d ball of radius and the values that the 2-d weight vector w takes is illustrated for
6 different centers.
the parameters of the network arranged in form of a vector. The dataset is represented in terms of
(xi , yi ) pairs and m represents the number of points in the dataset. The function represented by the
neural network is denoted by Φ(w,.). The loss for a single data point Xi is given by '(yi, Φ(w, Xi))
and the loss vector is represented by `. The overall loss is represented by L(w) and is given by
L(w) = Pm=I '(yi, Φ(w, Xi)). We sometimes abbreviate L(w(t)) as L when the context is clear.
In standard weight normalisation (SWN), each weight vector Wu is reparameterized as Yu kVuj. This
was proposed by Salimans & Kingma (2016), as a substitute for Batch Normalization and has been
practically used in multiple papers such as Sokolic et al. (2017), Dauphin et al. (2017), Kim et al.
(2018) and Hieber et al. (2018). The corresponding update equations for gradient descent are given
by
γu(t +1) = Yu⑴-η⑴vu(t) "wuL
kvu(t)k
Vu(t + 1) = Vu⑴一η⑴ γu(t) (I -
kvu(t)k
Vu(I)Vu(t)>	▽ L
kvu(t)k2	JVwU
(1)
(2)
In exponential weight normalisation (EWN), each weight vector wu is reparameterized as eau j^.
This was mentioned in Salimans & Kingma (2016), but to the best of our knowledge, has not been
widely used. The corresponding update equations for gradient descent with learning rate η(t) are
given by
αu(t + 1)= αu(t) - η(t)eɑu㈤ Vu(TVwUL
kVu(t)k
eαu(t)	Vu(t)Vu(t)> ∖
Vu(t + 1) = Vu(t) - η(t)F∑≡ (I -	kVu(t)k2 ) VwUL
(3)
(4)
The update equations for gradient flow are the continuous counterparts for the same. In case of
gradient flow, for both SWN and EWN, we assume kVu(0)k = 1, to simplify the update equations.
4 Inductive Bias of Weight Normalization
In this section, we state our main results for weight normalized smooth homogeneous models on
exponential loss('(yi, Φ(w, Xi) = e-yiφ(W,Xi)). The results for cross-entropy loss and proofs have
been deferred to the appendix due to space constraints. First, we state the main proposition that
helps in establishing these results for EWN.
Theorem 1. The gradient flow path with learning rate η(t) for EWN and SWN are given as follows:
EWN: dwu≡ = -η(t)kwu(t)k2VwuL	(5)
SWN: dwu≡ = -η(t)(kwu(t)k2VwUL + (1 -kwu(t2k2) (wu(t)>VwυL)wu(t))	(6)
dt	U	kwu(t)k2	U
3
Under review as a conference paper at ICLR 2021
Thus, the gradient flow path of EWN can be replicated by an adaptive learning rate given by
η(t)kwu(t)k2 on unnormalized network(Unnorm). These parameterizations also induce different
neighborhoods in the parameter space, that have been shown in Figure 1.
4.1	Assumptions
The assumptions in the paper can be broadly divided into loss function/architecture based assump-
tions and trajectory based assumptions. The loss functions/architecture based assumptions are
shared across both gradient flow and gradient descent.
Loss function/Architecture based assumptions
1	'(yi, Φ(w, Xi)) = e-yiφ(W,Xi)
2	Φ(., x) is a C2 function, for a fixed x
3	Φ(λw, X) = λLΦ(w, X), for some λ > 0 and L > 0
Gradient flow. For gradient flow, we make the following trajectory based assumptions
(	AD limt→∞ L(W(U)=,	(A3) limt→∞ k'(W⅛∣)⅛ := e
(	A2) limt→∞ kw(t)∣∣ := W	(A4) Let P = mini yiΦ(W, Xi). Then ρ > 0.
The first assumption is typically satisfied in scenarios where a positively homogeneous network
achieves 100% training accuracy. This is not a completely unreasonable assumption, given recent
papers demonstrating neural networks with sufficient overparameterization can fit even random la-
bels(Zhang et al. (2017), Jacot et al. (2018)), and is a standard assumption made when the purpose
is to find the inductive bias. The second assumption states that the network converges in direction
and this has been recently shown in Ji & Telgarsky (2020) to hold for gradient flow on homoge-
neous neural nets without normalization under some regularity assumptions. The third and fourth
assumptions are required to show convergence of the gradients in direction. The fourth assumption
is indeed true for SWN as shown in Lyu & Li (2020) 1.
Gradient Descent. For gradient descent, we also require the learning rate η(t) to not grow too fast.
(	A5) limt→∞ η(t)∣∣Wu(t)∣NwuL(w(t))k = 0 for all U in the network
Proposition 1. Under assumptions (A1)-(A4), limt→∞ η(t)∣∣Wu(t)∣Nwu L(w(t))k = 0 holds for
every U in the network with η(t) = O(L), where c < L
This proposition establishes that the assumption (A5) is mild and holds for constant η(t), that is
generally used in practice.
While some of these assumptions are non-standard we believe they do generally hold, and demon-
strate the viability of these assumptions in a toy experiment which we call Lin-Sep. In this ex-
periment a 2-layered EWN neural network, with 8 neurons in the hidden layer and a ReLU-squared
activation function, is trained on a linearly separable dataset. The learning rate schedule used was
O(L⅛7) and the network was trained till a loss of e-300. The corresponding graphs for EWN are
shown in Figure 2. Similar results for SWN have been deferred to Figure 8 in the appendix.
4.2	Effect of Normalisation on Weight and Gradient Norms
This section contains the main theorems and the difference between EWN and SWN that makes
EWN asymptotically relatively sparse as compared to SWN. First, we will state a common proposi-
tion for both SWN and EWN.
Proposition 2. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent,
for both SWN and EWN, the following hold:
—7 w L(w (t))	m`m
(i)	limt→∞ gwL(W(t))k = μ工i=ι 'iyiVwΦ(w, Xi) = g, where μ > 0.
1Homogeneous networks in the w space are also homogeneous in the γ, v space. Therefore results regard-
ing convergence rates and monotonic margin hold from Lyu & Li (2020). However, the results for convergence
to a KKT point of the max margin problem do not hold. For details, refer Appendix K.
4
Under review as a conference paper at ICLR 2021
(b) Evolution of kwu k
(c) Weight vector convergence
(d) Loss vector convergence
Figure 2: Verification of assumptions for EWN in Lin-Sep experiment: (a) shows the dataset.
In (b), it can be seen that only weights 5,7 and 8 keep on growing in norm. So, only for these,
IlwUIl > 0. (C) shows the components of the unit vector ^^^, only for the weights 5, 7 and 8 as
they keep evolving with time. Eventually their contribution to the unit vector become constant. (d)
shows the components of the loss vector and they also become constant eventually. (e) shows the
normalized parameter margin converging to a value greater than 0.
(e) Normalized parameter margin
wW) W p∙f wu —— 1 iτγη ,	WU (，)∩γ∣r∣ cτ —— IilT)+ _▽WU C(w(*)) ThPn 'W —— Xpf f/lf SCHIP
(ii)	Let	Wu	= limt→∞ kw(t)k	ana	gU	=	limt→∞	Il^wL(w(t))k ♦	T 几en, Wu =八gU for some
λ≥0
The first and second part state that under the given assumptions, for both SWN and EWN, gradients
converge in direction and the weights that contribute to the final direction of w, converge in opposite
direction of the gradients. Now, we provide the main theorem that distinguishes SWN and EWN.
Theorem 2. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent, the
following hold
(i) for EWN kw k > 0 kw k > 0	⇒ lim	kwu ⑴ MWWU L(W⑴)k
(i) JorEWN，kWuk > 0, kwv k > 0 =⇒ limt→∞ ∣∣wv (t)∣∣gwvL(w(t))k (ii)
∣wUkIeUk _ ι
∣w V k∣ev k = 1
(ii)	fηr SWN	km k	> o km k > o ⇒	iirn	kwU⑴IIWWvL(WI))Il _	kwUIllievk	— i
(ii)	forSWN,	kwuk	> 0, kwvk > 0	=⇒	limt→∞	IwUtJIgwU L(w(t))I =	fwvTOUl	= 1
Thus, asymptotically, for EWN, kwu (t)k
kι(t)
IU L(w(t))I
while for SWN, kwu (t)k
k2(t)kVwUL(w(t))k, where kι(t) and k2(t) are independent of the neuron u. We demonstrate
this property of EWN on the Lin-Sep experiment in Figure 3. The results for SWN have been
deferred to Figure 9 in the Appendix.
Now, we provide a corollary for the case of multilayer linear nets.
Corollary 1. Consider a weight normalized(SWN or EWN) multilayer linear net, represented by
y = WnWn_1...W1x. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient
descent, if the dataset is sampled from a continuous distribution w.r.t Rd, then, with probability
1, θ = W1>W2> Wn> converges in direction to the maximum margin separator for all linearly
separable datasets.
5
Under review as a conference paper at ICLR 2021
.6.8Q
Ool
- - -
心 PUB M3≥> M/q φ⊂~ωoυ
m3≥> PUe S M/q φ⊂~ωoυ
「 「-1
-1- 6。一
1.4	1.6	1.8	2.0
∣og IlWUll
=⅛*Δ= Oo-
Oo
i-
一⅛-≥一 6°~
(c)	(d)	(e)
Figure 3: Demonstration of Results for EWN in Lin-Sep experiment: (a) demonstrates part 1
of Proposition 2, where ge is approximated by using w from the last point of the trajectory. Clearly,
▽wu L stops oscillating and converges to e. (b) demonstrates part 2 of Proposition 2 and shows
that for weight vectors 5,7 and 8, Wu(t) converges in opposite direction of VwuL(w(t)). (c), (d)
and (e) demonstrate Theorem 2 for EWN, where for weight vectors 5,7 and 8. The three graphs are
plotted at loss values of e-200, e-250 and e-300 respectively. At each loss value, for the 3 weights,
log kwu k + log kVwu Lk is approximately same.
4.3 Sparsity Inductive Bias for Exponential Weight Normalisation
The inverse relation between kwu(t)k and kVwu L(w(t))k in the EWN trajectory results in an
interesting inductive bias that favours movement along sparse directions.
Proposition 3. Let assumptions (A1)-(A5) be satisfied. Consider two nodes u and v in the network
such that kevk ≥ keuk > 0 and ∣∣Wu(t)k,∣∣Wv(t)k → ∞. Let kgu∣ be denoted by c. Let e,δ be
such that 0 < < c and 0 < δ < 2π. Then, the following holds:
1.	There exists a time t1, such that for all t > t1 both SWN and EWN trajectories have the
following properties:
(a)	Hu L(w(t))k ∈ [c_f C +f]
(a)	kVwvL(w(t))k ∈ [c-e, c + e]
(b)	( Wu ㈤ ∖τ ( Twu Lgtt) ∖ ≥ cos(δ)
(b)	kwu(t)k	kVwu L(w(t))k	≥cos(δ)
(C) ( Wv(t) ∖> ( -VwvL(w(t)) A ≥ cos(δ)
(C)	kwv (t)k	kVwv L(w(t))k ≥ cos(δ).
2∙ forSWN, limt→∞ 除溜=C
3. for EWN, if at some time t2 > t1 ,
(C)	Wu(t2)	>	1	V	Hrn kwu(t)k	—	X
(a)	Wv (t2)	>	(c-e)cos(δ)	=⇒	limt→∞ ∣∣Wv (t)k	=	∞
b	wu(t2)	/	Cos(δ)	=^>	Iirn	llwu(t)k _ 0
(b)	Wv (t2)	<	^+^	=⇒	limt→∞ ∣Wv (t)k = 0
6
Under review as a conference paper at ICLR 2021
(a) Network Architecture
(b) Weight trajectories
Figure 4: (a) Network architecture for the Simple-Traj experiment . (b) Trajectories of the two
weights for EWN and Unnorm, starting from 5 different initialization points.
• class -1
• class 1
(b) EWN
(c) Unnorm
(a) Dataset
Figure 5:	(a) Training data for the XOR experiment. (b, c) Norm of the incoming neuron weights for
the EWN and unnormalized architectures.
The above proposition shows that the limit property of the weights in Theorem 2, makes non-sparse
w an unstable convergent direction for EWN. But that is not the case for SWN. We demonstrate the
relative sparsity between EWN, SWN and Unnorm through two toy experiments - Simple-Traj
and XOR.
In the Simple-Traj experiment, we have a single data point at (2, 1), that is labelled positive and
train a network with linear activations. The architecture is shown in Figure 4a, where weights in
blue and red are frozen to values 1 and 0 respectively. Thus, there are effectively only two scalar
parameters- w1 and w2 . The network is trained till a loss value of e-50 starting from 5 different
initialization points. The weight trajectories in Figure 4b shows that EWN prefers to converge either
along the x or y axis, and hence has an asymptotic relative sparsity property.
In the XOR experiment, we train a 2-layer ReLU network, with 20 hidden neurons on XOR
dataset(shown in Figure 5a). The second layer is fixed to the values 1 or -1 randomly. For at-
taining 100% accuracy on this dataset with this architecture, at least 4 hidden units are needed. As
can be seen in Figure 5, EWN asymptotically uses exactly 4 neurons out of 20, while Unnorm uses
all the 20 neurons. The results for SWN have been deferred to Figure 10 in the appendix.
5	Convergence Rates
Under Assumption (A2), W can be represented as W = g(t)W + r(t), where limt→∞ kg(t)k = 0.
Let d : N → R, given by d(t) = Ptτ =0 η(τ) denote total step size.
7
Under review as a conference paper at ICLR 2021
Q
O	2000	4000
Steps
(a) EWN
Q
O	2000	4000
Steps
(b) SWN
2000
Steps
4000
(c) Unnorm
Figure 6:	Variation of convergence rate of train loss with number of layers for multilayer linear nets
The asymptotic convergence rate of loss for SWN and Unnorm have already been established in Lyu
& Li (2020) as Θ (~~1)1_ 2 ) ∙ For EWN, the corresponding theorem is provided below
Theore^rt 3 For E^WN under Assumntions (A1)-(A5) and lim	kr(t+1) Mt)k ——0 the followinɑ
eor em	. o	^ol	,	urz∙c^e∕	ʃɪssu(mL^7tι∙^ytvs (□	/∖∕/	ar匕	ɪɪɪt—^(^^)	g(t	∣ι)	g(t)	, te f^ovv^ovvi∙fv^^
hold
1.	∣∣w(t)k asymptotically growsat Θ ((log(d(t))L1)
2.	L(w(t)) asymptotically goes down at the rate of Θ (d(t)(klg d(t))2).
For multilayer linear nets, the variation of convergence rate with number of layers for a linearly sep-
arable dataset is illustrated in Figure 6. All of these networks were explicitly initialized to represent
the same point in function space. It can be seen that EWN, SWN and unnormalized networks all
converge faster with more layers, but the effect is much less pronounced for EWN.
6	MNIST Pruning Experiments
As EWN leads to asymptotically sparse solutions, it is likely that a sufficiently trained EWN net-
work would be comparatively robust to pruning. In this section, we compare the pruning efficacy
of EWN, SWN and Unnorm on a 2-layer ReLU network trained on the MNIST dataset. In case of
EWN and SWN, only the first layer is weight normalized as only this layer needs to be pruned. The
pruning criterion used is the difference between the initial and final weight norm, i.e, the weights
that grow the least in norm are pruned first. The corresponding pruning graphs at different loss
values are shown in Figure 7. It can be seen that when the loss levels are sufficiently low, the EWN
network becomes better adapted for pruning, significantly outperforming SWN and the unnormal-
ized network in terms of test accuracy for a given level of pruning. The variation of norm of the
weight vectors with gradient descent steps for neurons in the first layer has been deferred to Figure
11 in the appendix.
7	Conclusion
In this paper, we analyze the inductive bias of weight normalization for smooth homogeneous neural
nets and show that exponential weight normalization is likely to lead to asymptotically sparse solu-
tions and has a faster convergence rate than unnormalized or standard weight normalized networks.
References
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rkxQ-nA9FX.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
8
Under review as a conference paper at ICLR 2021
Percentage of neurons pruned	Percentage of neurons pruned
(a) L = e-10
(b) L = e-300
Figure 7: Variation of test accuracy vs percentage of neurons pruned in first layer at different loss
values for MNIST experiment
Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding
batch normalization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
7694-7705. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7996-understanding-batch-normalization.pdf.
Yongqiang Cai, Qianxiao Li, and Zuowei Shen. A quantitative analysis of the effect of batch normal-
ization on gradient descent. volume 97 of Proceedings of Machine Learning Research, pp. 882-
890, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.
mlr.press/v97/cai19a.html.
Lenalc Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. volume 125 of Proceedings of Machine Learning Research, pp.
1305-1338. PMLR, 09-12 Jul 2020. URL http://proceedings.mlr.press/v125/
chizat20a.html.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. volume 70 of Proceedings of Machine Learning Research, pp. 933-
941, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL http:
//proceedings.mlr.press/v70/dauphin17a.html.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer CNN: Don’t be afraid of spurious local minima. volume 80 of Proceedings of
Machine Learning Research, pp. 1339-1348, Stockholmsmassan, Stockholm Sweden, 10-15 Jul
2018. PMLR. URL http://proceedings.mlr.press/v80/du18b.html.
Yonatan Dukler, Quanquan Gu, and Guido Montufar. Optimization theory for relu neural networks
trained with normalization layers. In International Conference on Machine Learning, 2020.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. volume 80 of Proceedings of Machine Learning Research, pp.
1832-1841, Stockholmsmassan, Stockholm Sweden, 10-15 Jul 2018a. PMLR. URL http:
//proceedings.mlr.press/v80/gunasekar18a.html.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 9461-9471. Curran Associates, Inc., 2018b. URL http://papers.nips.cc/paper/
8156-implicit-bias-of-gradient-descent-on-linear- convolutional-networks.
pdf.
9
Under review as a conference paper at ICLR 2021
Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, and
Matt Post. Sockeye: A toolkit for neural machine translation, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 8571-8580. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8076- neural- tangent- kernel- convergence- and- generalization- in- neural- networks.
pdf.
Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. volume 99
of Proceedings of Machine Learning Research, pp. 1772-1798, Phoenix, USA, 25-28 Jun 2019a.
PMLR. URL http://proceedings.mlr.press/v99/ji19a.html.
Ziwei Ji and Matus Telgarsky. A refined primal-dual analysis of the implicit bias, 2019b.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations, 2019c. URL https://openreview.
net/forum?id=HJflg30qKX.
Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. arXiv
preprint arXiv:2006.06657, 2020.
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 31, pp. 1564-1574. Curran Asso-
ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
96ea64f3a1aa2fd00c72faacf0cb8ac9- Paper.pdf.
Jonas Kohler, Hadi Daneshmand, Aurelien Lucchi, Thomas Hofmann, Ming Zhou, and Klaus
Neymeyr. Exponential convergence rates for batch normalization: The power of length-direction
decoupling in non-convex optimization. volume 89 of Proceedings of Machine Learning Re-
search, pp. 806-815. PMLR, 16-18 Apr 2019. URL http://proceedings.mlr.press/
v89/kohler19a.html.
Boyi Li, Felix Wu, Kilian Q Weinberger, and Serge Belongie. Positional normalization. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d’Alche Buc, E. Fox, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 32, pp. 1622-1634. Curran Associates, Inc., 2019. URL
http://papers.nips.cc/paper/8440- positional- normalization.pdf.
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Towards understanding regularization
in batch normalization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HJlLKjR9FQ.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural net-
works. In International Conference on Learning Representations, 2020. URL https://
openreview.net/forum?id=SJeLIgBKPS.
M. Muresan. A Concrete Approach to Classical Analysis. CMS Books in Mathematics. Springer
New York, 2015. ISBN 9780387789330. URL https://books.google.co.in/books?
id=N8rBgtIu_qgC.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lex-
icographic and depth-sensitive margins in homogeneous and non-homogeneous deep models.
volume 97 of Proceedings of Machine Learning Research, pp. 4683-4692, Long Beach, Cali-
fornia, USA, 09-15 Jun 2019a. PMLR. URL http://proceedings.mlr.press/v97/
nacson19a.html.
Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. volume 89 of
Proceedings of Machine Learning Research, pp. 3420-3428. PMLR, 16-18 Apr 2019b. URL
http://proceedings.mlr.press/v89/nacson19b.html.
10
Under review as a conference paper at ICLR 2021
Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. volume 89 of Proceedings of Machine Learn-
ingResearch,pp. 3051-3059. PMLR, 16-18 Apr2019c. URL http://proceedings.mlr.
press/v89/nacson19a.html.
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Rethinking normalization and
elimination singularity in neural networks. arXiv preprint arXiv:1911.09738, 2019.
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-batch training with
batch-channel normalization and weight standardization, 2020.
Simon Roburin, Yann de Mont-Marin, Andrei Bursuc, RenaUd Marlet, Patrick Perez, and MathieU
Aubry. Spherical perspective on learning with batch norm. arXiv preprint arXiv:2006.13382,
2020.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to
accelerate training of deep neural networks. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29,
pp. 901-909. Curran Associates, Inc., 2016. URL http://papers.nips.cc/paper/
6114- weight- normalization- a- simple- reparameterization- to- accelerate- training- of- de
pdf.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch
normalization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 2483-2493. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7515-how-does-batch-normalization-help-optimization.pdf.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel R. D. Rodrigues. Robust large margin
deep neural networks. IEEE Transactions on Signal Processing, 65(16):4265-4280, Aug 2017.
ISSN 1941-0476. doi: 10.1109/tsp.2017.2708039. URL http://dx.doi.org/10.1109/
TSP.2017.2708039.
Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on sep-
arable data. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=r1q7n9gAb.
Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya Gunasekar,
Rachel Ward, and Qiang Liu. Implicit regularization of normalization methods. arXiv preprint
arXiv:1911.07956, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.
A Proof of Theorem 1
Theorem. The gradient flow path with learning rate η(t) for EWN and SWN are given as follows:
EWN: dwu(t) = -η(t)∣Wu(t)k2VwuL
SWN dwu(t) = -η(t)(kwu(t)k2VwuL + (1 -kw*k2) (wu(t)>VwuL)wu(t))
dt	u	kWu(t)k2	u
The proof for the two parts will be provided in different subsections, where the corresponding part
will be restated for ease of the reader.
11
Under review as a conference paper at ICLR 2021
A.1 Exponential Weight Normalization
Theorem. The gradient flow path with learning rate η(t) for EWN is given by:
dw≡ = -η⑴kwu⑴k2VwuL
Proof. In case of EWN, weights are reparameterized as Wu = eɑu	. Then
kvu k
VαuL=eαu
vu>Vwu L
kvu
Vvu L
αu
--(I -
kvuk (
vuv
k
>
u
kvuk2
)VwuL
Now, in case of gradient flow with learning rate η(t), we can say
vu(t)>Vwu L
llvu(t)k
vu(t)vu(t)> )V L
kvu(t)k2)Vwu
dɑur = -η(t)Vαυ L = -η(t)eau ㈤
dvu(t)	eαu(t)
~7Γ = -n(t)VvuL = -η(t) 而加(I-
Now, using these equations, we can say
dkvu⑴ H2 — K (t)> d dvu(t八—0
dt = u( )	[ dt )=
Thus, kvu(t)k does not change with time. As we assumed kvu(0)k to be 1, therefore for any t,
kvu(t)k = 1. Using this simplification, we can write
dwu(t)	d(eαu(t)vu(t))
dt	dt
= eαu(t)(-η(t)eαu(t)(I - vu(t)vu(t)>)VwuL) - η(t)e2αu(t)(vu(t)>VwuL)vu(t)
= -η(t)e2αu(t)VwuL
Thus, the gradient flow path with exponential weight normalization can be replicated by an adaptive
learning rate given by η(t)kwu(t)k2.
□
A.2 S tandard Weight Normalization
Theorem. The gradient flow path with learning rate η(t) for SWN is given by:
¥ =-η⑴(kwu⑴k2VwuL + (1-⅛ b WuaVwu L)Wu⑻
Proof. In case of SWN, weights are reparameterized as wu = Yu ^^. Then
VγuL
vu> Vwu L
Vvu L = / (I-
kvuk
.VuvU )V L
kvuk2)Vwu
Now, in case of gradient flow with learning rate η(t), we can say
vu(t)>VwuL
dγut(t) = -η ⑻VauL = -η(t)
kvu(t)k
Vu(t)Vu(t)> )V	L
kvu(t)k2 )Vwu
dvu⑴=—r,(t)V	L = —n(t) Yu⑴(I -
dt =	η(t)vvuL =	η(t) kvu(t)k (I
12
Under review as a conference paper at ICLR 2021
Now, similar to EWN, kvu(t)k does not change with time. Using the fact that kvu(t)k = 1 for all
t, we can say
dwu(t) = d(γu(t)vu(t))
dt	dt
=Yu⑴(-η⑴Yu⑴(I- Vu⑴Vu⑴T)VwuL) - η⑴(Vu⑴>VwuL)Vu⑴
=-η(t)(Yu(t)2VwuL + (1 - Yu(t)2)(vu(t)>VwuL)vu(t)
=-η⑴(Yu⑴2VwuL + (1 - Yu(? ) (Wu⑴>vwuL)Wu⑴
Yu (t)
Replacing γu(t) by ∣∣wu(t)k gives the required expression.	□
B Proof of Proposition 1
Proposition. Under assumptions (A1)-(A4), limt→∞ η (t)∣wu (t)∣Vwu L(w(t))∣ = 0 holds for
every U in the network with η(t) = O(L), where c < L
Proof. Under assumption (A1) and (A2), w can be represented as w = g(t)we + r(t), where
limt→∞ kr(t)k = 0. Now, for exponential loss,
m
-Vw L(w(t)) = X 'i (w(t))(yiVw Φ(w(t),Xi))
i=1
`i = e-yiφ(W(t),Xi) = e-g(t"yiφ(w + g(t),Xi)
VwΦ(w(t), Xi)) = g(t)L-1VwΦ(W + T, Xi))
g(t)
From assumption (A4), we know Φ(we, xi) ≥ ρ for all i. Now, using Euler’s homogeneous theorem,
we can say
we>VwΦ(we, xi) = LΦ(we,xi)
Thus, ∣Vw Φ(we, xi)∣ > 0 for all i. Now, using the equations above and assumption (A3), we can
say
lim ___________网皿师______________________= kι
t→∞ e-PMt)Lg(t)L-1k Pm=ι 'iyiVwΦ(W, xi)∣
where kι is some constant. Now, if η(t) = O( £(仅))尸),where c < 1, then using the fact that L
goes down at the rate of e-ρg(t)L and ∣w∣ goes up at the rate of g(t), we can say
lim η(t)∣w(t)∣∣VwL(w(t))∣ ≤ lim
t→∞	t→∞
k2∣w(t)kkVw L(w(t))k
L(w(t))c
lim
t→∞
k1k2e-Mt)Lg(t)L-1k Pm=ι IeiyVΦ(W, xi)k∣w(t)k
L(W(t))c
0
□
C Proof of Proposition 2
Proposition. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent,
for both SWN and EWN, the following hold:
∕∙∖ 1 ∙	—7 w L(w (t))	m`m f) χ^^7 √κ (~	7	、 c
(i)	limt→∞ gwL(w(t))k = μ 工i=ι 'iyiVwΦ(w, Xi) = g, Where μ > 0.
ww) w w "W —— 1i^m+ Wu(*)and <y —— 1i^m*	_^wu C(w(*)) Thpn ^w —— XPf fcY SCHlP
(ii)	Let Wu = limt→∞ kw(t)k ana gu = limt→∞ k^wL(w(t))k . T几en, Wu =八gu for Some
λ≥0
13
Under review as a conference paper at ICLR 2021
The proof for different cases will be split into different subsections and corresponding proposition
will be stated there for ease of the reader. The proof will depend on the Stolz Cesaro theorem(stated
in Appendix J),Integral Form of Stolz-Cesaro Theorem(stated and proved in Appendix J) and fol-
lowing lemmas that have been proved in Appendix I.
Lemma 1. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent, for
both SWN and EWN, we u> geu ≥ 0 for all nodes u in the network.
Lemma 2. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent,
for both SWN and EWN, there exists atleast one node u in the network satisfying kwe u k > 0 and
kegu k > 0.
Lemma 3. Consider two unit vectors a and b satisfying a>b ≥ 0 and a>b < 1. Then, there exists
a small enough > 0, such that for any unit vector c satisfying c>a ≥ cos() and any unit vector d
satisfying d>b ≥ cos(), b> (I - cc>)d ≥ .
Lemma 4. Consider sequence a satisfying the following properties
1.	ak > 0
∞
2.	k=0 ak = ∞
3.	limk→∞ ak = 0
ThenP 连0 j2=∞
Lemma 5. Consider two sequences a and b satisfying the following properties
1.	ak > 0, Pk=0 ak = ∞ and limk→∞ ak = 0
2.	bo > 0, b is increasing and bk+ι ≤ bk + (fk)2
Then P∞=o 葭=∞.
Lemma 6. Consider two sequences a and b satisfying the following properties
1.	ak > 0 and k∞=0 ak = ∞
2.	bk > 0 and Pk∞=0 bk = ∞
3.	k∞=0 (ak - bk ) converges to a finite value
4.	limk→∞ ɑk exists
bk
Then limk→∞ W = 1.
C.1 Exponential Weight Normalization
In this section, we will use eαu(t) and kwu (t)k interchangeably.
C.1.1 Gradient Flow
Proposition. Under assumptions (A1)-(A4) for gradient flow, for EWN, the following hold:
∕∙∖ 1 ∙	一7 w L(w (t))	m^~`m f) v^7 √κ(~	7	、 c
(i)	limt→∞ gwL(W(t))k = μ工i=ι 'iyiVwΦ(w, Xi) = g, where μ > 0.
wW)	W W	wτ	——1 iτγη ,	WU (t)	∩T1∕^∣ o, ——IllT)+ _^wU C(w(*))	ThP0	"W ——XPf	fcY	ti∩TΠP
(ii)	Let Wu =	limt→∞ kw(t)k	anα	gu	=	limt→∞	∣∣^wL(w(t))k * T几en，Wu =八gU	Jor some
λ≥0
Update Equations:
dαu(t)
dt
-η(t)eαU(t)(vu(t)>VwUL(W(t)))
(7)
14
Under review as a conference paper at ICLR 2021
dvu(t)
dt
-η ⑴ eαu(t)(I - Vnt)Vu⑴T)VwuL(W(U)
(8)
(i)	limt→∞	—7wL(w(t))	= μ	Pm -∣	'iyi VwΦ(W,	Xi)	= g,	where μ	> 0
v t t→∞	∣∣VwL(w(t))k i=	i=ι=i	i沙i w ∖	, i/ ð,	产
Proof. Using the fact that ρ > 0 and Euler’s homogeneous theorem, we can say
We>VwΦ(We, xi) = LΦ(We, xi) > 0
Thus, kVw Φ(We, xi)k > 0 for all i.
Let w(t) = g(t)W + r(t), where limt→∞ k^ = 0. Now, by Taylor’s Theorem, we can say
k=1
VwΦ(W(t), xi) = VwΦ(g(t)We, xi) +	V2Φ(g(t)We + kr(t), xi)r(t)dk
k=0
g(t)L-1vwφ(W, Xi) + g(t)LT Z二2+ kr(t) ,xi)*k
Now, V2Φ(W + kr(t), Xi) can be bounded by a constant and limt→∞ kg(t)k = 0. Thus,
limt→∞ Rk=1 V2Φ(W + k黑,Xi)%dk = 0. Thus, we can say, if ∣∣VwΦ(W,Xi)k > 0 for Some
i, then
VwΦ(W(t), Xi)
lim ---小L 7——=VwΦ(w,Xi)	(9)
t→∞	g(t)L-1
Now,
m
-Vw L(W(t)) = X 'i (W(t))(yiVw Φ(W(t),Xi))
i=1
Now, Let S = {i : yiΦ(W,Xi) = min7∙ yjΦ(W,Xj)}. Let C denote min7-∈s yjΦ(W, Xj) 一 ρ.
Consider a ∈ S and b ∈/ S, then
lim 'b(W(t)) = lim e-g(t)L(φ(w + 儒"。)-φ(w + 需,加))
t→∞ `a (W(t)	t→∞
Now, as the minimum difference is G therefore limt→∞ ,(瑟)) = 0. Thus, ∀j / S, 'j = 0. Now,
using Equation (9) and the expression for -VwL(W(t)) from before, we can say
1∙	-VWL(w(t))	∖^e v7 吊/〜	、
ιim ∣B AV 小∖∣∣ = μ 工'iyikwφ(W,xi)
t→∞ ∣VwL(W(t))∣	i S
i∈
where μ = k Pi∈s ei(1iΦ(w,Xi))k .
□
(ii)	∣Weu ∣ > 0 =⇒ Weu = λegu for some λ > 0
Proof. Consider a node u having ∣Weu ∣ > 0. The proof will be split into two parts depending on
∣geu∣ >0or∣geu∣ =0.
Case 1:	∣geu ∣ > 0
Let the angle between Wu and gu be denoted by ∆. Using Lemma 1, we can say ∆ ≤ ∏2. We will
prove the statement by contradiction, so let’s assume ∆ > 0.
Now, we know, 一 口得WuL(w(t))k converges to gu and vu(t) converges in direction of Wu. Taking
ge
dot product with k∣uj on both sides of Equation (8) and using Lemma 3, we can say there exists a
time t1 and a small enough C, such that for any t > t1 ,
(置)> dvu^ ≥ η(t)eαu(t)∣VwuL(W(t))ke	(10)
∣geu ∣	dt	u
15
Under review as a conference paper at ICLR 2021
Now, using the fact that αu → ∞ and Equation (7), we can say
∞
t1
η(t)eau ㈤ kVwu L(w(t))kdt
∞
Integrating Equation (10) on both the sides from t1 to ∞, we get
（氤）

Wu
- vu(t1)) ≥ ∞
This is not possible as vectors on LHS have bounded norm. This contradicts. Hence ∆ = 0.
Case 2:	kgeu k = 0
We are going to show that it is not possible to have kwe u k > 0 and kgeu k = 0. Using Lemma 2, we
can say there exists atleast one node v satisfying kwe v k > 0 and kegv k > 0. Now, using Equation
(7), we can say
kwu(t)k ≤
Zt
k=0
η(k)kwu(k)k2kVwu L(w(k))kdk
From Case 1, we know, for any > 0, that there exists a time t1, such that for t > t1,
(Wv (t)、> ( —Rwv L(W(D)、
Ikwv (t)k ) U∣Vwv L(w(t))k )
≥ cos(). Now, using Equation (7), we can say
kwv(t)k ≥ kwv(t1)k + cos()
Zt
k=t1
η(k)kwv(k)k2kVWv L(w(k))kdk
Thus, we can say, for t > t1,
kwu(t)k ≤________Rk=on(k)kwu(k)k2kVwuL(W(B)kdk_________
kwv⑴k _ kwv(tι)k + cos(e) RLI η(k)kwv(k)k2kVwvL(w(k))kdk
Now, as kwe u k > 0 and kwev k > 0, therefore both the integrals diverge. Also, the integrands
converge in ratio to 0 as kgeu k = 0 and kgev k > 0. Thus, taking limit t → ∞ on both the sides and
using the Integral form of Stolz-Cesaro theorem, we can say
lim
t→∞
kwu⑴k
kwv ⑴k
≤0
However, this is not possible as kweu k > 0 and kwev k > 0. This contradicts. Therefore, such a case
is not possible.	口
C.1.2 Gradient Descent
Proposition. Under assumptions (A1)-(A5) for gradient descent, for EWN, the following hold:
-11 1 i τγη	VwC(w(t)) —— p/	∖ 'Pm e. ∖/ ①(^∖∖τ^ Y -)——G w∕?TIP	// ' 0
(i)	limt-∞ ∣VwL(w(t)) k = μ	2-^i=1 CiyiVWφ( w , Xi) = g, where	μ > 0.
/万)T Uf	)	一	Iim Wu ⑴	CM» g	— Iim	-^wu L(W⑴)	RIOS 击	一	√½r
(ii)	Let	wu	=	limt→∞ kW(t)k	and gu	= limt→∞ ∣vwL(W(t))k .	Then，wu	=	λgu for some
λ≥0
Update Equations:
αu(t + 1)= αu(t) - η(t)eɑu ㈤ ^⑴丁二 L(w(t))	(11)
kvu(t)k
vu(t + 1) = VU⑴-η⑴ɪɪ(I - VUt)Vu(t) )VWuL(W⑴)	(I2)
kvu(t)k	kvu(t)k2
(i)	limt→∞ -7wL(W(t)) = μ Pm -∣ 'iyi VwΦ(w, Xi) = g, where μ > 0
t→∞ kVw L(W(t))k	i=1 i i W , i ,
Proof. Follows exactly as shown for gradient flow in Appendix C.1.1.	口
16
Under review as a conference paper at ICLR 2021
(ii)	kwe u k > 0 =⇒ we u = λegu for some λ > 0
Proof. Consider a node u having kwe u k > 0. The proof will be split into two parts depending on
kgeuk > 0 or kgeuk =0.
Case 1:	kgeu k > 0.
Let the angle between WU and gu be denoted by ∆. Using Lemma 1, We can say ∆ ≤ 2. We will
prove the statement by contradiction, so let’s assume ∆ > 0.
Now, we know, - ∕wuL(W(t))∣∣ converges to gu and Vu(t) converges in direction of Wu. Taking
ge
dot product with kguj on both sides of Equation (12) and using Lemma 3, we can say there exists a
time t1 and a small enough , such that for any t > t1,
Vu(t + 1)>gu
km
Vu(t)>eu
国口
≥
eαu (t)
+ e(η(t) kv^"VwuL(w(t))k)
However, in this case, kvu(t)k doesn’t stay constant and thus increase in dot product doesn’t directly
correspond to an increase in angle. Now, using Equation (12), we can say
eαu(t)
kvu(t + 1)k2 ≤ kvu(t)k2 + (η(t) ∣r-7tw∣∣VwuL(w(t))∣∣)2	(13)
kvu(t)k
Using the above two equations, we can say, for time t > t1 ,
Vu(t + 1)>eu ≥ vu(gUkgu + e(n⑴ keCKk kVwuL(W(t))k)
kvu(t+1)kkeuk ^ qkvu(t)∣∣2+(η(t)壮 Wwu L(w(t 而
Unrolling the equation above, we get
vu(t+1)>gu ≥ vu(teUkgu+Pk==Ia5(k 层/ kvwuL(w(k))k)	(14)
kvu(t+1训euk - q∣∣vu(t1)∣∣2+PM (η(k) IeaU⅛ ∣∣VwuL(w(k 川)2
Now, as αu(t) → ∞, therefore, using Equation (11), we can say
k=∞
X η(k)eαu(k)kVwuL(w(k))k = ∞
k=t1
Now, using this identity, along with the Assumption (A5), Equation (13) and Lemma 5, we can say
∞
X η(k)
k=t1
eαu (k)
kVwuL(w(k))k
∞
Using this along with Equation (14) and Lemma 4, we can say
lim
t→∞
vu(t + 1)>gu
IWu(t +1)唯 Uk
≥∞
However, this is not possible as the vectors on LHS have bounded norm. This contradicts. Thus
∆=0.
Case 2: kgeu k = 0
We are going to show that its not possible to have kweu k > 0 and kgeu k = 0. By Lemma 3, we know
there exists atleast one node s satisfying kwesk > 0 and kges k > 0. Now from Equation (11), we can
say
αu (t)
k=t-1
αu(0) - X η(k)eαu(k)
k=0
vu(k)>Vwu L(w(k))
kvuw
αs(t) = αs(0)-kX-1 η(k)eɑs(k) Vs(TVwk^㈤)
17
Under review as a conference paper at ICLR 2021
Thus,
αu(t)-αs(t)
(au(0)-αs(0))+kX-1(η(k)eɑs(k) ^^^(^^("))
k=0	vs
Vu(k)>Vwu L(w(k))
FUW
(15)
Now, we know, αu(t) and αs (t) → ∞. Also, as kweuk > 0 and kwesk > 0, therefore αu - αs
converges. Therefore the RHS of Equation (15) converges as well. However, RHS is the dif-
ference of two diverging series. Also, as vs(t) and Vws L(w(t)) eventually get aligned and
Wwu L(W)k
limt→∞ gws L(w(t))∣∣ = 0，so，We can Say
lim
t→∞
Pau (t) Vu (t)>^wu L(W(^)
e	"(t)k
Pas (t) Vs (t)>^ws L(W(D)
e	FTW
0
HoWever， this contradicts Lemma 6， as the ratio must be converging to 1 if the limit exists. Therefore
this case is not possible.	□
C.2 Standard Weight Normalization
C.2.1 Gradient flow
Proposition. Under assumptions (A1)-(A4) for gradient flow, for SWN, the following hold:
(i) ∣im	_▽WL(W(t)) ——p ∖ Pm `.7v.V7 φ(-W X.)——W where >> > 0
∖i∕ limt→∞ ∣∣▽wL(w(t))k = μ 2-^i=1 Ciyi V wφ(w, Xi) = g, where μ > 0.
/万)T uf ) 一 Iim	Wu(t) CM» g — Iim	-^wu L(W(t))	击 一	√½r
∖ii∕ Let wu - limt→∞ ∣W(t)∣ and gυ = limt→∞ 1▽wL(W(t))∣ . Then，wu = λgυ for some
λ≥0
Update Equations:
dYu(t) =	(t) vu(t)>vWu L(w(t))
~^~ = -η() KW
dvu(t) _ ，八 Yu(t) /r	vu(t)vu(t)>e ZVm八
=	= n(t) hr Wll(I	IIyr ∕√ΛI∣2 )VWu L(W(t))
dt	kvu(t)k	kvu(t)k2 u
(16)
(17)
(i) limt→∞	—▽wL(W(t))	= μ Pm -∣ 'iyi VwΦ(w, Xi)	= g,	where μ >	0
v t t→∞	∣∣vwL(w(t))∣ 产 Z^i=I i沙i W ∖	, i ð,	产彳
Proof. Follows exactly as shown for gradient flow in Appendix C.1.1.	□
(ii) kweu k > 0 =⇒ weu = λegu for some λ > 0
Proof. Consider a node u having kweuk > 0. In this case， γu(t) can either tend to ∞ or -∞. We
will consider the case γu (t) → ∞. The other case can be handled similarly. The proof will be split
into two parts depending on kgeuk > 0 or keguk = 0.
Case 1: kgeuk > 0
Let the angle between WU and gu be denoted by ∆. Using Lemma 1, we can say ∆ ≤ ∏2. We will
prove the statement by contradiction， so let’s assume ∆ > 0.
Now, we know, 一 ∣vwuL(W(t))∣ converges to gu and v〃(t) converges in direction of Wu. Taking
ge
dot product with ∣gu∣ on both sides of Equation (17) and using Lemma 3, we can say there exists a
time t1 and a small enough , such that for anyt> t1,
(gu ∖> dvu(t)
k≡7
≥ η(t)γu(t)kVWuL(w(t))k
(18)
18
Under review as a conference paper at ICLR 2021
Now, using the fact that γu → ∞ and Equation (16), we can say
Γ η(t)∣∣VwuL(w(t))kdt = ∞
t1
Integrating Equation (18) on both the sides from t1 to ∞, we get
geu
kegu k
>w
( kWUk - vu(t1)) ≥∞
This is not possible as vectors on LHS have bounded norm. This contradicts. Hence ∆ = 0.
Case 2:	kgeu k = 0
We are going to show that it is not possible to have kwe u k > 0 and kgeu k = 0. Using Lemma 2, we
can say there exists atleast one node s satisfying kwesk > 0 and kgesk > 0. Now, using Equation
(16), we can say
kwu(t)k ≤ Z t η(k)kVwuL(w(k))kdk
From Case 1, we know, for any > 0, that there exists a time t1, such that for t > t1,
w ws(t) ʌ	— -▽Ws L(W⑴)∖、CCe√/∖ X[n∖s7 ncinσ Pnnatinn 门 G ∖x∕α can cav
I kws(t)k )	I !!▽ws L(w(t))k J ≥ Cos⑹.Now, using Equation (16), We can say
kws(t)k≥kws(t1)k+cos()Z t η(k)kVWsL(w(k))kdk
k=t1
Thus, we can say, for t > t1,
kwυ(t)k ≤__Rk=oη(k)kvwυL(w(k))kdk_
kwsw - kWV^IΓ^XT⅛VWLW⅛k
Now, as kwe uk > 0 and kwe sk > 0, therefore both the integrals diverge. Also, the integrands
converge in ratio to 0 as kgeuk = 0 and kgevk > 0. Thus, taking limit t → ∞ on both the sides and
using the Integral form of Stolz-Cesaro theorem, we can say
lim
t→∞
kwu(t)k
llws(t)k
≤0
However, this is not possible as lweul > 0 and lwes l > 0. This contradicts. Therefore, such a case
is not possible.
C.2.2 Gradient Descent
Proposition. Under assumptions (A1)-(A5) for gradient descent, for SWN, the following hold:
(i)	lim	_▽WL(W(t)) ——p ∖ Pm e.qvvV7 φ(-W X.) ——W where >> > 0
∖i∕ Iimt→∞ ∣∣▽wL(w(t))k = μ 2-^i=ι CiyivWφ(w, Xi) = g, whef e μ > 0.
ww) W W wτ —— 1i^m+	Wu(*)and <y —— 1i^m*	_▽Wu C(w(*)) Thpn ^w —— XPf feY SCHlP
(iif Let WU = Umt-∞ kw(t)k ana gu = Umt→∞ !▽wL(w(t))k . T几en, WU =八gu Jor Some
λ≥0
Update Equations:
vu(t)>VWu L(W(t))
γu(t + 1) = γu(t)- η(t)-----^UW----------	()
vu(t + 1) = vu(t) - η(t) llγu(t)ll (I - VUS)Vu(t) )vwuL(w(t))	(20)
lvu(t)l	lvu(t)l2	u
(i)	limt→∞ ∣jwL(W(t)) = μ Pm 1 'i yiVw Φ(W, Xi) = g, where μ > 0
v t t→∞ !▽wL(w(t))k	μ ^i=1 i沙i w V , i/ ð,	μ
Proof. Follows exactly as shown for gradient flow in Appendix C.1.1.
□
19
Under review as a conference paper at ICLR 2021
(ii)	kwe u k > 0 =⇒ we u = λegu for some λ > 0
Proof. Consider a node u having kweuk > 0. In this case, γu(t) can either tend to ∞ or -∞. We
will consider the case γu(t) → ∞. The other case can be handled similarly. The proof will be split
into two parts depending on kgeu k > 0 or kegu k = 0.
Case 1:	kgeu k > 0
Let the angle between WU and eu be denoted by ∆. Using Lemma 1, We can say ∆ ≤ ∏. We will
prove the statement by contradiction, so let’s assume ∆ > 0.
Now, we know, 一 ∕wuL(W(t))∣∣ converges to eu and Vu(t) converges in direction of Wu. Taking
ge
dot product with kguɪ on both sides of Equation (20) and using Lemma 3, we can say there exists a
time t1 and a small enough , such that for any t > t1,
vu(t + I)>eu . vu(t)>eu ,	/ W Yu⑴ ∣B ZVmWw
keuk	≥πmr+伞⑴ Fu≡kVwuL(W(t))k)
However, in this case, kvu(t)k doesn’t stay constant and thus increase in dot product doesn’t directly
correspond to an increase in angle. Now, using Equation (20), we can say
kvu(t + 1)k2 ≤ kvu(t)k2 + (η(t)kVwuL(w(t))k)2
kvu(t)k
Using the above two equations, we can say, for time t > t1 ,
(21)
vu(t + i)>eu
/ .	S I I I I ʌ- I I k
Vu(t)>gu
kguk
+ e(η(t) kV⅛ kVwu L(w(t))k)
kvu(t +1)kkeuk - √kvu(t)k2 + (η(t) kVu⅛ kVwu L(w(t)W
Unrolling the equation above, we get
vu(t + 1)> eu 、
kvu(t + i)kkeuk ≥
v⅜4 + Pk=tι e(η(k)欣瑞kvwuL(w(k))k)
√kvu(tι)k2 + ∑k=tι (η(k) kV⅛ kVwu L(w(k))k)2
(22)
Now, as γu(t) → ∞, therefore, using Equation (20), we can say
k=∞
X η(k)kVwuL(W(k))k = ∞
k=t1
Now, using this identity, along with the assumption (A5), Equation (21) and Lemma 5, we can say
XXJk) K(klkvwu L(W(k))k = ∞
Using this along with Equation (22) and Lemma 4, we can say
1.	vu(t +1)>eu
lim ∏——7-——…川〜u ≥ ∞
t-∞ llvu(t + I)kkguk 一
However, this is not possible as the vectors on LHS have bounded norm. This contradicts. Thus
∆=0.
Case 2:	keeu k = 0
We are going to show that its not possible to have kWeuk > 0 and keeuk = 0. By Lemma 3, we know
there exists atleast one node s satisfying kWesk > 0 and keesk > 0. Now from Equation (19), we can
say
k=t-1	vu(k)>VwuL(W(k))
γu(t)=γu(0)- k=0n(k)―—
20
Under review as a conference paper at ICLR 2021
γs(t)=γs(0)-kχ-1 η(k) ¾Lkw≡
Now, γs (t) either diverges to ∞ or -∞. In both the cases, it is a strictly monotonic sequence for
large enough t. Also
lim YKt + I)-Yu⑴=0
t→∞ Ys(t + 1) - Ys(t)
as kgeuk = 0,kgesk > 0 and from Case 1, Ws(t) and -VwsL(w(t)) eventually get aligned. Thus,
using Stolz-Cesaro theorem, we can say
lim
t→∞
Yu⑴
Ys(t)
0
However, this is not possible as kwe uk > 0 and kwe s k > 0. This contradicts. Therefore, this is not
possible.	□
D Proof of Theorem 2
Theorem. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent, the
following hold
kw kwwu k'Wl(t	I∣-U7 Il、八	kwu Il、0	Hrn	kwu(t)kWwuL(W(t))k	_ i
(i)	forEWN,	kwuk >	0,	∣∣Wv H > 0	=⇒	limt→∞	∣∣wv (t)∣∣gwvL(w(t))k	= 1
kww kwwu SWN I∣-U7 Il、八 kWu Il、0	lim	kwu(t)kwwvL(W(t))k — i
(ii)	forswN, kWuH > 0, kwvH > 0 =⇒ limt→∞ ∣Wv(t)k∣Vwu L(w(t))k = 1
Proof for different cases will be split into different subsections and the corresponding case will be
restated there for ease of the reader.
D.1 Exponential Weight Normalization
D.1.1 Gradient Flow
Theorem. Under assumptions (A1)-(A4) for gradient flow, the following holds for EWN:
/门	kwu Ii、n kwu Ii、n______、	km	kwu(t)kkVwuL(W(t))k	_ ι
⑴	HWuH	>	0，HWvH	> 0 =⇒	limt→∞	∣wv (t)k∣Vwv L(w(t))k	= 1
Proof. Consider u and v such that HweuH > 0 and Hwev H > 0. Using Proposition 2, we can say
Hegu H > 0 and Hgev H > 0. Also, from Proposition 2, we can say, for both u and v, weights and
gradients converge in opposite directions. Hence there exists a time t2, such that for any t > t2,
• -Vw L(W(t)) and Wu(t) make an angle or lesser with each other
• -Vwv L(W(t)) and Wv (t) make an angle or lesser with each other
Then, using Equation (7), we can say for any time t > t2,
HWu(t)H ≥ HWu(t2)H + cos()
Zt
kt2
η(k)Hwu(k)H2HVw L(W(k))Hdk
HWu(t)H ≤ HWu(t2)H +
Zt
kt2
η(k)Hwu(k)H2HVw L(W(k))Hdk
HWv(t)H ≥ HWv(t2)H + cos()
Zt
kt2
η(k)Hwv(k)H2HVwv L(W(k))Hdk
HWv(t)H ≤ HWv(t2)H +
Zt
kt2
η(k)Hwv(k)H2HVwv L(W(k))Hdk
21
Under review as a conference paper at ICLR 2021
Using the above equations, we can say, for time t > t2,
kwu(t)k ≥ kwu(t2)k + cos(e) Rk=t2 η(k)kwu(k)k2kVwuL(w(k))kdk
kwv⑴k —	kwv(t2)k + Rk=t2 η(k)kwv(k)k2kVwvL(w(k))kdk
kwu(t)k ≤	kwu(t2)k + RL2 η(k)kwu(k)k2kVwuL(w(k))kdk
kwv⑴k _ kwv(t2)k + cos(e) Rk=t2 η(k)kwv(k)k2kVwvL(w(k))kdk
We know that both integrals diverge as ∣∣wu(t)k and ∣∣wv(t)k → ∞, limt→∞ kWu(t)k and
Wwu(t)Lk
gwv (t)Lk
limt→∞
exist. Taking limit t → ∞ on both the equations and using the Integral form
of Stolz-Cesaro theorem, we get
cos9kwu⑴k2kvwu(t)L(W⑻k ≤ lim ∣K≡ ≤ lim kwu⑴k2kvwu(t)L(W⑻k
t-∞	∣Wv(t)k2∣Vwv(t)L(w(t))k	≤ t→∞ ∣Wv(t)k ≤ t→∞ cos(e)∣Wv(t)k2∣Vwv(t)L(w(t))k
As we know these limits exist and this holds for any > 0, therefore
lim ∣⅛≡ = lim "⑴k2kVwu(t)L(W⑻k
t-∞ ∣Wv(t)k	t-∞ ∣Wv (t)k2∣Vwv(t)L(w(t))k
Simplifying it further, we get
lim kWu(t)kkVWu(t)Lk = 1
t→∞ ∣Wv(t)k∣Vwv(t)LlI =
□
D.1.2 Gradient Descent
Theorem. Under assumptions (A1)-(A5) for gradient descent, the following holds for EWN:
/门	kwu Ii、n	kwu	Ii、n____、 km	kwu(t)kk^wuL(W(t))k	_ ι
⑴	kwuk	> 0，kwvk	> 0 =⇒ limt→∞	∣∣Wv (t)kgwv L(w(t))k	= 1
Proof. Consider two nodes u and s such that ∣We u ∣ > 0 and ∣We s ∣ > 0. From Proposition2, we
know ∣geu ∣ > 0, ∣ges ∣ > 0 and for both u and s, weights and gradients eventually get aligned
opposite to each other. Using Equation (11), we can say
αu (t)
t-1
αu(0) - X η(k)eαu(k)
k=1
Vu(k)>Vwu L(w(k))
FUW
αs(t)
t-1
αs(0) - X η(k)eαs(k)
k=1
Vs(k)>Vws L(w(k))
FsW
Thus,
α ⑴-ɑ ⑴=(a ⑼-α (O))+ kχt-∖ ㈤eɑs(k) vs(k)>VWs(k)L(W㈤) -㈤eαu(k) vu (kLVWuL(W(k)) )
αu(t) as(t) = (αu(0) αs(0))+ / / (n(&)e	∣∣v (k) k	〃(凡)e	k v (k)k	)
(23)
Now, we know, αu(t) and αs(t) → ∞. Also, as ∣weu∣ > 0 and ∣wes∣ > 0, therefore αu(t) - αs(t)
converges. Therefore the RHS of Equation (23) converges as well. However, RHS is the difference
of two diverging series. Also, limt→∞
can say
Qαu(t) vu(t)> Vwu L(Wd))
e	kvu(t)k	—
(t) Vs(t)>VWs L(W(G)
e	FTW
exists. Therefore, using Lemma 6, we
eau(t)kVwu L(w(t))k
t→∞ eas(t)kVwsL(w(t))k
1
□
22
Under review as a conference paper at ICLR 2021
D.2 S tandard Weight Normalization
D.2.1 Gradient Flow
Theorem. Under assumptions (A1)-(A4) for gradient flow, the following holds for SWN:
(i)	km k > o k-^τ k > o ⇒ iirn	kwu(t)kkvwvL(W(t))k _ ι
⑴ kwuk > 0, kwvk > 0 =⇒ l1mt→∞ kwv (t)kkVwu L(w(t))k = 1
Proof. Consider u and v such that kweuk > 0 and kwev k > 0. Using Proposition 2, we can say
kegu k > 0 and kgev k > 0. Also, from Proposition 2, we can say, for both u and v, weights and
gradients converge in opposite directions.
Consider a time t2, such that for any t > t2,
•	-VwuL(w(t)) and w£t) atmost make an angle E with each other
•	一VWvL(w(t)) and Wv(t) atmost make an angle E with each other
Then, using Equation (16), we can say for any time t > t2,
kwu(t)k ≥ kwu(t2)k +cos
(E)Zt
k=t2
η(k)kVwuL(w(k))kdk
kwu(t)k ≤
kwu(t2)k +Z
k=t2
η(k)kVwuL(w(k))kdk
kwv (t)k ≥ kwv(t2)k +cos
(E)Zt
k=t2
η(k)kVwv L(w(k))kdk
kwv(t)k ≤
kwv(t2)k + Z
k=t2
η(k)kVwv L(w(k))kdk
Using the above equations, we can say, for time t > t2,
kwu(t)k ≥ kwu(t2)k + COS(E) Rkt=t2 η(k)kVwu L(w(k))kdk
kwv⑴k —	kwv(t2)k + RL2 η(k)kVwvL(w(k))kdk
kwu(t)k ≤	kwu(t2)k + Rkt=t2η(k)kVwuL(w(k))kdk
kwv ⑴ k — kwv ⑸ k + COs(E) Rkt=t2 η㈤kVwvL(W㈤)kdk
We know that both integrals diverge as kwu(t)k and kwv(t)k → ∞, l1mt→∞ 1处(；)〔 and
l1mt→∞ kKuMLlW(;))]] exist. Taking limit t → ∞ on both the equations and using the Integral
form of Stolzv-Cesaro theorem, we get
COS(E)kVWu(t)L(w(t))k ≤	kwu(t)k ≤	kVWu (t)L(w(t))k
t→∞	kVwv(t)L(w(t))k	≤ t→∞ kwv(t)k - t-∞ COS(E)kVwv(t)L(w(t))k
As we know these limits exist and this holds for any E > 0, therefore
lιm kwu⑴kkVwv(t)Lk = ι
t→∞ kwv(t)kkVwu(t)Lk
□
23
Under review as a conference paper at ICLR 2021
D.2.2 Gradient Descent
Theorem. Under assumptions (A1)-(A5) for gradient descent, the following holds for SWN:
(i)	km k > o k-^τ k > o ⇒ iirn	kwu(t)kkvwvL(W(t))k _ ι
⑴ kwuk > 0, kwvk > 0 =⇒ l1mt→∞ kwv (t)kkVwu L(w(t))k = 1
Proof. Consider u and v such that kweu k > 0 and kwe s k > 0. Using Proposition 2, we can say
kegu k > 0 and kges k > 0. Also, from Proposition 2, we can say, for both u and s, weights and
gradients converge in opposite directions. Now from Equation (19), we can say
小 G∖	k=-1 〃、Vu(k)>VWuL(W(k))
Yu⑴=Yue)- k=0η⑻一FUW—
Ys(t)=Ys(0) - kXX1 η(k)Vs(\：WkL；W(k))
Now, Ys (t) either diverges to ∞ or -∞. In both the cases, it is a strictly monotonic sequence for
large enough t. Also l1mt→∞ Yu(t+1)-γu(t) exists. Therefore, using Stolz-Cesaro Theorem, We can
say	s s
「	kwu(t)kkVws(t)Lk =ι
t→∞ kws(t)kkVwu(t)Lk =
□
E Proof of Proposition 3
Proposition. Let assumptions (A1)-(A5) be satisfied. Consider two nodes u and v in the network
such that kevk ≥ keuk > 0 and ∣∣wu(t)k, ∣∣wv(t)k → ∞. Let 餐q be denoted by c. Let e, δ be
such that 0 < < c and 0 < δ < 2π. Then, the following holds:
1.	There exists a time t1, such that for all t > t1 both SWN and EWN trajectories have the
following properties:
(C)	gwuL(w(t))k ∈	广 + f]
(a)	kVwvL(w(t))k ∈ [c - e, c + e]
(b)	( Wu㈤)> (TwuLgtt) ∖ ≥ cos(δ)
(b)	IkWu(t)“	Ik Vwu L(w(t))k J ≥cos(δ)
(C)	( Wv(t) ∖> (-VwvLwtt) ∖ ≥ cos(δ)
(C)	IkWv (t)k ) IkVwvL(W(t))k J ≥ cos(δ).
2.	forSWN, limt→∞ 粽解=C
3.	for EWN, if at some time t2 > t1 ,
(n)	Wu(t2)	>	1	―	lim kWu(t)k	— M
(a)	Wv (t2)	>	(c-e)cos(δ)	=⇒	l1mt→∞ kWv (t)k	= ∞
b	wu(t2)	r	cos(δ)	=^>	lirn	kwu(t)k _ 0
(D)	Wv (t2)	<	=⇒	l1mt→∞ kWv (t)k = 0
The proof of different cases Will be split into multiple subsections and corresponding proposition
Will be stated there for ease of the reader.
E.1 S tandard Weight Normalization
E.1.1 Gradient Flow
Proposition. Let assumptions (A1)-(A4) be satisfied. Consider two nodes u and v in the network
such that kevk ≥ keuk > 0 and ∣∣wu(t)k, ∣∣wv(t)k → ∞. Let 盖q be denoted by c. Let e, δ be
suCh that 0 < < C and 0 < δ < 2π. Then, the following holds:
24
Under review as a conference paper at ICLR 2021
1. There exists a time t1, such that for all t > t1, SWN trajectory has the following properties:
(a)
(b)
(c)
∣Vwu L(w(t))k
gwv L(w(t))k
∈ [c - , c + ]
(wu(t) A > ( —Nw L(w(t)) A
(∣wu(t)" UlNwu L(w(t))”
(Wv(t) A> (―Nwv L(w(t)) A
(∣Wv (t)k ) ^∣Vwv L(w(t))∣J
≥ Cos(δ)
≥ Cos(δ).
2. limt→∞
kwu (t)k
l∣wv (t)l
c
Proof. The proof of part 1a, i.e, INwU L(W(t))∣∣ ∈ [c 一 e, c + e] follows from the definition of limit
as kNwuL(W(t))k tends to C	v
as ∣NwvL(w(t))∣ tendsto c.
Nnm Ea BiIl mcwα tn fhα nrnnf Cf	Ihia ( wu(t) AT ( -NwuL(w(t)) A、CCqf4 Tha
now, We will move to Ihe proof of PaLt 1b, i.e., ι ∣w (t)∣ j I ∣N L(w(t))∣ j ≥ cos(δ). The
assumptions in this Proposition differ slightly from Proposition 2 and thus the proof is slightly more
involved as we also need to show that Wu(t) converges in direction. The proof will be given for
Yu → ∞. The one for Yu → 一∞ can be handled similarly.
As Iggu I > 0, therefore Vwu L(W(t)) converges in direction. Therefore, for every T satisfying 0 <
w w	wu fkαrα	w timw 7^c	Cllck that fr`ʌ't` +	w.. + c	∣ VWUC^W(*))	ʌ	g	eu ʌ '"⅛. CCu，丁、"M^c∖x: T
τ <	2π, Ihele exists a IiIIle t3,	SUCh IhaI for t	> t3,	1	∣n L(w(t))∣	J	(	Ie 11 y ≥ Cos(T).	now, LeI	S
assume that Wu (t) does not converge in the direction of ggu. Then, there must exist a T satisfying
0 < τ < 2π, such that for this T, there exists a time t4 > t3 satisfying vu(t4)> (kgeUJ) = cos(∆),
where ∆ > T .
Now, we are
that vu(t5)>
ge
With 氤 on
M-M
~geg
to show that for any κ satisfying τ < κ < ∆, there exists a time t5 > t4 such
> cos(κ). Let’s say for a given κ, no such t5 exists. Then, taking dot product
both sides of Equation (17), we can say
(gu ∖	dVu(t)
η(t)Yu(t)kVwu L(w(t))k
>
vu
/-Vwu L(w(t))λ
l∣NwuL(w(t))∣”
Now, as
(TwuL(w(t)) A ≥ cos(丁)and
U∣VwuL(w(t))k ) ≥ Cos(T) and
M-M
~geg
≤ cos(κ), we can say
(gu A> dVu(t)
UOJ ~^r
≥ n(t)Yu⑴INwuL(W⑴)k(Cos(T) - cos(K))
(24)
Now, using the fact that γu → ∞ and using Equation (16), we can say
∞
I	η⑴INwuL(W⑴)kdt = ∞
t=t4
Using this fact and integrating the Equation (24) on both the sides from t4 to ∞, we get a contradic-
tion as vectors on LHS have a finite norm while RHS tends to ∞. Thus, for every κ between T and
∆, there must exist a t5, such that Vu(t5)> (∣^∣UJ) > cos(κ).
cos(β), then
to show for all t ≥ t5, Vu(t)> (keuɪ) > Cos(κ). Now, consider any β such
Using similar argument as in Equation (24), we can say, if for any t6 > t5 ,
(gu ∖	dVu(t6)
≥ η(t6)Yu(t6)∣Nwu L(w(t6))k(cos(τ) -Cos(β))
(25)
This means that the dot product between
and vu(t) goes up, whenever
<
(t)
Vu
A
~geg
cos(τ). Therefore, its not possible that vu(t)>
≤ Cos(κ) for any t > t5 . As κ can be
25
Under review as a conference paper at ICLR 2021
arbitrarily chosen between τ and ∆, and the argument holds for any > 0, wu (t) converges in the
direction of geu
Thfl m*ccf Cf nqrf Ip ip ( Wv(t)、 ( Vwv L(W(t))、、CCql ʌ ʌ Cqn hɑ ChC*xm ；n fhα cqtπp *x∕mr qc
The proof of Part ιc, i.e, ι “w (t)k ι I "▽ L(w(t))k ) ≥ cos(δ) CaIl be ShoWn In the SaIme Way as
1b.	v
The proof of part 2, i.e, limt→∞ kwu(t)k = C can be shown in the same way as Theorem 2 for SWN
gradient flow from Appendix D.2.1.	□
E.1.2 Gradient Descent
Proposition. Let assumptions (A1)-(A5) be satisfied. Consider two nodes u and v in the network
such that kevk ≥ keuk > 0 and ∣∣Wu(t)k, ∣∣Wv(t)k → ∞. Let ^^ be denoted by c. Let e, δ be
such that 0 < < c and 0 < δ < 2π. Then, the following holds:
1. There exists a time t1, such that for all t > t1, SWN trajectory has the following properties:
(a)
kVwu L(w(t))k
kVwv L(w(t))k
∈ [c - , c + ]
(b)
(Wu(t)、> ( —Vwu L(W(D) ∖
U∣Wu(t)kJ Ik Vwu L(w(t))kJ
≥ cos(δ)
(c)
(Wv (t)、> ( —Vwv L(W(t))、
IkWv (t)k ) IkVwvL(W(t))kJ
≥ cos(δ).
2. limt→∞
kWu (t)k
kWv (t)k
c
Proof. The proof of part 1a, i.e, k∣Vwu L(W(t))k ∈ 卜-e，c + e] follows from the definition of limit
as kνwuL(W(t))k tends to C	v
as kVwv L(W(t))k tendsto c.
^M^c*x∕ *x∕α Xiirill InrTVa fc fhα m*ccf Cf nqrf Ih i p ( Wu(t)、	( VwuL(W(t))、、CCqlX、Tha qq
Now, we will move to the proof of part 1b, i.e, ι ∣w (t)k )	I kV L(w(t))k ) ≥ cos(δ). The as-
sumptions in this Proposition differ slightly from Proposition 2 and thus the proof is slightly more
involved as we also need to show that Wu(t) converges in direction. The proof will be given for
γu → ∞. The one for γu → -∞ can be handled similarly.
As keeuk > 0, therefore YWuL(w(t)) converges in direction. Therefore, for every T satisfying 0 <
T -	O√τ7- fkαrα	- tim- 7^c Cllck that fr`ʌ't` + '->k -^	- Vwu C(1^^(*))	ʌ	e gu ʌ '"⅛.	CCu，丁、"M^c∖x: T
τ <	2∏, there exists	a tume t3, SUCh that for t > t3,	1 k^^ L(W(t)) k	J	I ∣∣g k J ≥	Cos(T).	_lnow, _Let	S
assume that Wu (t) does not converge in the direction of eeu. Then, there must exist a τ satisfying
0 < τ < 2π, such that for this T, there exists a time t4 > t3 satisfying Vu(t4)> (^guj) = cos(∆),
where ∆ > T .
Now, we are going to show that for any κ satisfying T < κ < ∆, there exists a time t5 > t4 such
that (kVu(t5)k) (fgu!) > Cos(κ). Let,s say for a given κ, no such t5 exists. Then, taking dot
gg
product with ɪg^ on both sides of Equation (20), we can say
vu(t + i)>eu _vu(t)>eu
―而一=keuk +
η(t) τ^u≡ INwu L(w(t))k
kvu(t)k
eu ∖> (I _ Vu(t)Vu(t)> ) ( -VWu L(W(U) ʌ
keuk； (	kVu(t)k2 ) UVWuL(W(t))k J
Now, as
/-Vwu L(w(t))λ
UlVwu L(w(t))kJ
≥ Cos(T) and
M-M
~ggg
(vu(t)
(FUW
≤ Cos(κ), we can say
Vu (t + 1) eeu	Vu (t) eeu	γu (t)
-m∣~ ≥	keuk	+ (Cos(T) - cos(K))(n(t)P^IkVWuL(W(t))k)	(26)
26
Under review as a conference paper at ICLR 2021
Now, using arguments similar to the proof of Proposition 2 for SWN gradient descent in Appendix
C.2.2, we can show that the above statement leads to a contradiction and thus there must exist a t5
such that (∣∣vu(t5)∣∣^ (Te气)> Cos(κ).
kVu(t5)k	kgeu k
Now, we are going to show that there exists at6 > t5, such that for all t > t6,
cos(κ). Consider a β such that τ < β < κ. Now, if at any time t,(产(；八
then, similar to Equation (26), we can say
Vu(t)
kvu(t)k,
> (-⅛.
Ikgu
< cos(β),
vu (t + 1) geu	vu (t) geu	γu (t)
-----田]--------≥ U +(COS(T) - cos(β))(η(t)ll , ,ll ∣∣VwuL(W⑴)k)
kgeu k	kegu k	kvu (t)k
Using the upper bound on ∣vu(t + 1)∣ from Equation (21), we can say
vu(t + 1)>3 ≥ vu(gu>eu + (Cos(T)-Cos(e))(n⑴⅛⅜kvwuL(W⑴)k)
kvu(t + 1)kkeuk ^	q∣vυ(t)k2 + (η(t)⅛⅛∣VwuL(w(t而
(27)
Let η(t) kYuC ∣∣VwuL(w(t))∣ be denoted by χ(t). Then, the above equation can be rewritten as
vu(t +1)>gu	≥ vu(t)>eu	kvu(t)k	+ (Cos(T)- cos(β))	X⑻
kvu(t +1)kkguk ≥ kvu(t)k∣guk p∣vu(t)k2 +χ(t)2 +(	( )	(β)) p∣vu(t)k2 + χ(t)2
Now, we are going to show that for a small enough χ(t), RHS is greater than k~Vu(t))k igu ∣∣
vu(t)>eu_______∣vu(t)k
kvu(t)kkeuk p∣vu(t)k2 + χ(t)2
+ (Cos(T ) - Cos(β ))
χ(t)
√kvu(t)k2 + X(t)2
vu(t)>gu
kvu(t)k∣guk
(Cos(T) - cos(β)) ,	X(t)	> Aθ⅜(1 - ，	kvu(t)k	)
Pkvu(t)k2 + X(t)2	kvu(t)kkguk'	P∣vu(t)k2 + χ(t)2 J
(Cos(T) - Cos(β)) >
vu(t)>eu ( Vzkvu(t)k2 + X(t)2 - kvu(t)k
kvu(t)k∣guk (	X(t)
Clearly as χ(t) → 0, the RHS tends to 0, therefore the equation is satisfied. Thus for a small enough
χ(t), RHS of Equation (27) is greater than 匕喝假 ∣∣. AS IIVu(t)∣ keeps on increasing and by
Assumption (A5), limt→∞ η(t)γu(t)∣Vwu L(W(t))∣ = 0, we can say there exists a time t7, such
(-]ɔo(- 4γ'>-∙' CJn∖7 v vu	vu	Vu (t) eu	(YeaC IlT-I ∖x:kɑTla∖70r v	Vu (t)、	g gu ɪ ^∙, CCaf 仅、
thatforany t >	t7,	kvu(t)kkeuk	goesUP whenever (kvu(t)k)	(IO) <Cos(e).
Also, by using Equation (20) and Assumption (A5), we can say, that there exists a time t8, such
that for t > t8, (M⅛Γ (高)> Cos(e)	=⇒	(kVu(t+1)kΓ (高)> Cos(K), as
the RHS of Equation (20) goes to 0 norm in limit. Now, define t6 > max(t7 , t8) such that
(kvu(t6)k)(阖)
> Cos(κ) (must exist from previous arguments). Then, as the dot product
always goes up when between Cos(β) and Cos(κ), and can’t go in a single step from being greater
than Cos(β) to less than Cos(κ), therefore, for every t > h,(却；)) (k∣^) > Cos(κ).
Now as the above argument holds for any κ between T and ∆, and for any T > 0, we can say that
Wu(t) converges in direction ofgeu.
The proof of part 1c, i.e, (IIWv(t)∣∣) ( -7wvZL(W(t))) ≥ Cos(δ), follows exactly the same steps as
k	k	∖kwv(t)k)	"VwvL(W(t)川，一 ` 八	J	r
part 1b.
The proof of part 2, i.e, limt→∞ kwu(t)k = C can be shown in the same way as the proof of Theorem
2 for SWN gradient descent from Appendix D.2.2.	口
27
Under review as a conference paper at ICLR 2021
E.2 Exponential Weight Normalization
E.2. 1 Gradient Flow
Proposition. Let assumptions (A1)-(A4) be satisfied. Consider two nodes u and v in the network
such that kgvk ≥ ∣∣guk > 0 and ∣∣Wu(t)∣∣,∣∣Wv(t)∣∣ → ∞. Let ∣∣Uk be denoted by c. Let e, δ be
such that 0 < < c and 0 < δ < 2π. Then, the following holds:
1.	There exists a time t1, such that for all t > t1, EWN trajectory has the following properties:
(C) gwu L(w(t))k ∈	广+f]
(a)	gwvL(w(t))∣ ∈ [c - e, c+q
(b)	( wu(t)
WlkWU(t)k
(c)	I Wv⑶
(C) Ikwv (t)k
> (-▽wu L(w(t))
Ik Vwu L(w(t))k
> (-Vwv L(w(t))
IkVwvL(w(t))k
≥ cos(δ)
≥ cos(δ).
2.	If at some time t2 > t1 ,
(a)
(b)
kwu(t2)k
kwv (t2)k
kwu(t2)k
kwv (t2)k
(c-e) cοs(δ)
=⇒ limt→∞
kwu⑴k
∣w
cos(δ) _ ι∙
< c+6-------⇒ limt→∞
kwu⑴k
kwv (t)k
=0
>
1
∞
PrCCf The nmnf Cf rnsrt IQ i ∣ kVwu L(W(t))k
proof. The Ploof of PaIt ιa, i.e, ∣V L(w(t))k
as kvwuL(W(t))k tends to C	V
as gwvL(w(t))k tendsto c.
∈ [c - , c + ] follows from the definition of limit
^M^c*x∕ *x∕α Xiirill	InrTVa fc fhα m*crYf	Cf nqrf Ih i p ( WU (t)、	(	^wu C(W(*))、、CCqlX、	Tha
Now, we will	IlIoVe to the proof	of PaIt ib, i.e., 1 ∣w (t)k J	〈 kV L(w(t))k	)	≥ cos(δ).	The
assumPtions in this PloPosition diffel slightly flom PloPosition 2 and thus the Ploof is slightly mole
involved as we also need to show that Wu (t) convelges in dilection.
As ∣egu ∣ > 0, thelefole Vwu L(W(t)) convelges in dilection. Thelefole, fol evely τ satisfying 0 <
T ( C)F there pviq!^q IS time +C qiip!i f]ɔuf firn* 十、+C ( ^wu '(w("))、
/ ‹ 2π, IheIe exists a IiIUe t3, SuCh Ihat fol t > t3, 1 ∣v L(W(t)) k J
gu
gu
≥ cos(τ). Now, Let’s
assume that Wu(t) does not convelge in the dilection of geu. Then, thele must exist a τ satisfying
0 < τ < 2π, Such that for this T, there exists a time t4 > t3 satisfying Vu(t4)> (∣∣uj) = cos(∆),
whele ∆ > τ.
Now, we are
that vu (t5)>
ge
with 尚 on
to show that for any κ satisfying τ < κ < ∆, there exists a time t5 > t4 such
> cos(κ). Let’s say for a given κ, no such t5 exists. Then, taking dot product
both sides of Equation (8), we can say
(gu )	dVu(t)
η(t)eαu(t)kVwuL(w(t))k
(-VwuL(w(t))∖
UVwu L(w(t))∣”
Now, as
(-▽wu L(w(t))∖
U∣Vwu L(w(t))k/
≥ cos(τ) and
>
vu
≤ cos(κ), we can say
(gu ∖	dvu(t)
≥ η(t)eαu(t) ∣∣VwuL(w(t))k(cοs(τ) - cοs(κ))
(28)
Now, using the fact that αu → ∞ and using Equation (7), we can say
∞
η(t)eαu(t)kVwuL(w(t))kdt=∞
t=t4
Using this fact and integrating the Equation (28) on both the sides from t4 to ∞, we get a contradic-
tion as vectors on LHS have a finite norm while RHS tends to ∞. Thus, for every κ between τ and
∆, there must exist a t5, such that Vu(t5)> (IlUJ) > Cos(κ).
28
Under review as a conference paper at ICLR 2021
Now, we
that τ <
vu(t6)>
are going
cos(β), then
to show for all t ≥ t5, Vu(t)> (keuɪ) > Cos(κ). Now, consider any β such
Using similar argument as in Equation (28), we can say, if for any t6 > t5 ,
(IB)/ ≥ 他…6)kVwUL(W^k (Cos(T) - cos(β))	(29)
This means that the dot product between
(kg^) and Vu(t) goes up, whenever
<
(t)
Vu
~geg
cos(τ). Therefore, its not possible that vu(t)>
≤ cos(κ) for any t >
arbitrarily chosen between τ and ∆, and the argument holds for any > 0, wu (t)
direction of geu
t5 . As κ can be
converges in the
ThQ nrcrYf Cf nαrf Ip ip ( Wv(t)、 ( ^wv L(W(t))、、CCq( X、fc"∏c∖x∕c QYCICf1∖7 fhɑ CelTnQ CfQnC CIC
The proof of PaLt 1c, i.e, ∣ ∣∣w (t)k ) I "▽ L(w(t))k J ≥ cos(δ) fono^vs exactly the saɪme steps as
part 1b.
ʌrɑɪiɪr kww	TnrYVQ fc flɔp nrccf cf nαrf ɔk i k kWU (t2) k、_1__ ___、	kWU (t) k — γvλ
NOW, wewillmove to theproofofpart2a, i.e, kwv近)口 >(一^^⑷=⇒ limt→∞ "(t)∣∣ = ∞
Using Equation (7),
dkwu⑴k
dt
deau(t)
dt
-η⑴ kwu⑴ k2 (Vu ⑴>Vwu L(W(U)
(30)
Using the equation above and part 1 of the Proposition, we can say for t > t1 ,
d kwu(t)k IlW t-y Il dkwu(t)k IlW t-y Il dkwv(t)k
kwv(t)k = kwν (t 川 dt	-kwu(t 川 dt
dt	IlWv (t)k2
≥ η(t)kWu(t)k(kwu(t)kkVwuL(w(t))k cos(δ)-kwv(t)kkVwvL(w(t))k)	(31)
Iwv(t)I
d kwU(t)k	|, (t)k
In this case, using Equation (31), we can see kwvjt)” > 0 at t2. Thus, ∣wu(t)∣∣ always remains
greater than (c-e)1ros(δ) and keeps on increasing. Let's denote ∣∣wu(t2)∣∣ by ∆. Then we can say
d lwu(t)k	1
-wv^kl ≥ ∆(cos(δ) - -_v)η(t)kwu(t)kkVwuL(w(t))k
dt	∆(c - )	U
As αu → ∞, therefore using Equation (7), we can say t∞ η(t)Iwu(t)IIVwU L(w(t))Idt → ∞.
Thus, integrating both the sides of the equation above from t2 to ∞, we get
∞ d kwu(t)k
“k dt ≥∞
Jt2	dt	-
Thns Iim.	∣wu(t) ∣ ——f-γ->
ThUS limt→∞ ∣∣wv (t)∣ = ∞.
NrZX7 ∖X7Q ∖xR11 mc3 tc	巾 α	nrccf cf nɑrt Oh ； ∣	||wu(t2 ) ∣ /	cos(δ)	_κ ]∖ rn	llwu(t)∣	一 ∩
now, we will move to	the	proof of part 2b, i.e,	∣w %)∣ <	c+e	=⇒ iimt→∞	∣w (t)∣	= 0.
Using Equation (30) and part 1 of the Proposition, we can say for t > t1,
d kWu(t)k	IlW t-y Il d∣wu(t)∣	Il W (八||第wv(t)k
∣wv(t)∣ = kwv (t 川	dt -kwu(t 川	dt
dt	kwv (t)k2
≤ η(t)ŋtk(kwu(t)kkVwuL(w(t))k-kwv(t)kkVwvL(w(t))k cos(δ))	(32)
kwv(t)k
d kwu(t)k	W (t)∣
In this case, using Equation (32), we can see kwvjt* < 0 at t2. Thus, ∣wu(t)∣∣ always remains
smaller than coS(δ) and keeps on decreasing. Now, lets say limt→∞ ∣wu(t) > 0. This means that
c+e	∣wv (t)∣
29
Under review as a conference paper at ICLR 2021
k Wu (t) k、ʌ fcr CrYmQ	ʌ、(ɔ AkC 1pf,c rlɑnefɑ	k Wu (t2) k 卜\： R Thdn ∖x∕α	CeITl	c,λ∖t
11 w (t) k	>	zδ, for SoIme	zδ	> 0. Also, let S denote	“w (t )k by β. Then We	CaIl	say
d kwu⑴k
-ɪɪ ≤ -∆(cos(δ) -β(c + e))ηkwv(t)kkVwvL(w(t))k
As αv → ∞, therefore using Equation (7), we can say f∞ η∣∣Wv(t)k IlVWvL(w(t))∣∣dt → ∞.
Thus, integrating both the sides of the equation above from t2 to ∞, we get
r∞ d kwu⑴k
dkWv (t)k dt ≤-∞
Jt2	dt -
"I 'lɔic ic TlCf neɑɑ7hlɑ CIC	k Wu (t) k	1 ɛ "lc∖x∕αr heiinrlɑrl h∖7 (ɔ rΓ,lιιιc	11τn	k Wu (t) k — (ɔ	I I
ThiS is not possible as	k ^W (t) k	is lower bounded by 0. ThUS	limit_→^∞	k ^W (t) k — o.	1_1
E.2.2 Gradient Descent
Proposition. Let assumptions (A1)-(A5) be satisfied. Consider two nodes u and v in the network
such that ∣evk ≥ keuk > 0 and ∣∣Wu(t)k, ∣Wv(t)k → ∞. Let kguk be denoted by c. Let e, δ be
such that 0 < < c and 0 < δ < 2π. Then, the following holds:
1. There exists a time t1, such that for all t > t1, EWN trajectory has the following properties:
(a)
(b)
(c)
kVwu L(w(t))k
gwv L(w(t))k
∈ [c - , c + ]
(Wu (t) λ > ( —Vwu L(W(t))、
(kwu(t)kj IkVwu L(w(t))∣”
(Wv (t)、> ( —Vwv L(W(t))、
(kwv(t)k )	(kVwvL(w(t))∣”
≥ cos(δ)
≥ cos(δ).
2. If at some time t2 > t1 ,
(a)
(b)
wu(t2)
Wv (t2)
Wu(t2)
Wv (t2)
>	1	___⇒ Iirn	kwu(t)k
>	(c-e)cos(δ) K limt→∞ kwv (t)k
/	cos(δ)	_	ι∙rn	kwu(t)k	_ ∩
<	-τ+τ	—⇒	limt→∞	kwv(t)k	— 0
PrCCf The nmnf Cf rnsrt IQ i k kνwu L(W(t))k
proof. The proof of part 1a, i.e, kν L(w(t))k
as kνwuL(W(t))k tends to C	v
as kVwv L(w(t))k tends to c.
∈ [c - , c + ] follows from the definition of limit
^M^c*x∕ wιu	Xiirill	InrTVa	fc fhα m*ccf Cf nqrf Ih i p (	Wu(t) 、	- VWuL(W(t))、、CCqlX、Tha qq
[Now, We	Will	ImOVe	to the proof of part 1b, i.e, ∣	kW (t)k J	〈 kV L(W(t))k J ≥ cos(δ).	The as-
sumptions in this Proposition differ slightly from Proposition 2 and thus the proof is slightly more
involved as we also need to show that Wu(t) converges in direction.
As keguk > 0, therefore Vwu L(W(t)) converges in direction. Therefore, for every τ satisfying 0 <
w w wu fhαrα	w timw 7^c	Cllck that fr`ʌ't` +	w..	wiu	w VWuC^CW(*))	ʌ	g gu	ʌ '"⅛. CCu，丁、"M^c∖x: T
T < 2π, there exists	a tiιme t3,	SUCh that for t	>	t3,	1 kv L(w(t)) k	)	I kg k	1 ≥ cos(τ). Now, Let	S
assume that Wu(t) does not converge in the direction of geu. Then, there must exist a τ satisfying
0 < τ < 2π, such that for this T, there exists a time t4 > t3 satisfying Vu(t4)> (k∣uj) — cos(∆),
where ∆ > τ.
Now, we are going to show that for any κ satisfying T < κ < ∆, there exists a time t5 > t4 such
that (kVu(t5)k) (keu!) > cos(κ). Let,s say for a given κ, no such t5 exists. Then, taking dot
ge
product with keuk on both sides of Equation (12), we can say
vu(t + i)>eu _vu(t)>eu
―南——keuk 十
αu (t)
η(t) EW kVwu L(w(t))k
kvu(t)k
gU ∖> (I _ Vu(t)Vu(t)> ) ( -VWu L(w(t)) ∖
keuk； (	kvu(t)k2 ) IkVWuL(W(t))k J
30
Under review as a conference paper at ICLR 2021
Now, as
M-M
~geg
(TwuL(w(t)) A ≥ cos(丁)and
UlVwuL(w(t))k ) ≥ Cos(T) αnd
vu(t) A
FUWJ
≤ cos(κ), we can say
vu(t + 1)>egu	vu(t)>geu	eαu(t)
-----田］	≥ U + (CoS(T) - cos(κ))(η(t)ll , ,ll ∣∣VwuL(W⑴)k)	(33)
kgeu k----------------------------------------------------------------------------kegu k	kvu (t)k
Now, using arguments similar to the proof of Proposition 2 for EWN gradient descent in Appendix
C.1.2, we can show that the above statement leads to a contradiction and thus there must exist a t5
SUchthat (⅛⅛), (高A > Cos(K).
Now, We are going to show that there exists at& > t5, such that for all t > t6, (∣Vu(t)∣)	(1^) >
cos(κ). Consider a β such that τ < β < κ. Now, if at any time t, (∣Vu(t)∣)	(∣∣^) < cos(β),
then, similar to Equation (33), we can say
vu(t + 1)>geu	vu(t)>geu	eαu(t)
keuk	≥ ɪr + (Cos(TmtRWkvwuL(w(t))k)
Using the upper bound on ∣vu(t + 1)∣ from Equation (13), we can say
Vu(t + ι)>eu ≥ vu(⅛>eu + (Cos(T) - cos(β))(η(t)kS⅞kvwuL(w(t))k)
kvu(t + 1)kkeuk ^	q∣Vu(t)k2 + (η(t)<⅛⅛∣VwuL(w(t而
Let η(t) 借；；；IlVWuL(w(t))k be denoted by χ(t). Then, the above equation can be rewritten as
vu(t +1)>gu ≥ vu(t)>gu	kvu(t)k	+ (	(T) -	(β))	X(t)
∣Vu(t + ι)kkeuk ≥ ∣Vu(t)kkeuk p∣vu(t)k2 +χ(t)2 +(	( )	(β))P∣Vu(t)k2 + χ(t)2
Now, we are going to show that for a small enough χ(t), RHS is greater than k~Vu(t))∣ 祗 ∣
vu(t)>eu_________kvu(t)k	+ (( ) _	(β))	X(t)	> vu(t)>eu
∣Vu(t)kkguk P∣Vu(t)k2 +χ(t)2 +(	(T)-	(β)) P∣Vu(t)k2 + χ(t)2 > ∣Vu(t)kkeuk
χ(t)	vu (t)>geu	∣vu (t)∣
p ~	' Pkvu(t)k2 + K	FUWkn — Pkvu(t)k2 + 两)
(Cos(T) - Cos(β)) >
vu(t)>eu ( Vzkvu(t)k2 + χ(t)2 - kvu(t)k
kvu(t)kkeuk(	χ(t)
Clearly as χ(t) → 0, the RHS tends to 0, therefore the equation is satisfied. Thus for a small enough
χ(t), RHS of Equation (34) is greater than ∣Vu(t)IIkeuk. AS ∣∣Vυ(t)∣ keeps on increasing and by
Assumption (A5), limt→∞ η(t)γu(t)∣VWu L(W(t))∣ = 0, we can say there exists a time t7, such
that f∕^Λ't' CITl∖7 v	vu	vu Vu (t) gu	(YeaC IlT-I ∖x:kaTla∖7ar	( Vu (t)、	( geu、CCa (仅、
that for any t	>	t7,	kvu(t)k∣gu∣∣	goes UP whenever	(JlVu(t)k )	(IO)	< CoS(e).
Also, by using Equation (12) and Assumption (A5), we can say, that there exists a time t8, such
that for t > t8,(宿喘k)> (高)> Cos(e)	=⇒	(kVu(t+1)k)> (⅛) > Cos(K), as
the RHS of Equation (12) goes to 0 norm in limit. Now, define t6 > max(t7 , t8) such that
v vu(t6)、> ( eu ʌ
IkVu(t6)k )	IkCT )
> Cos(K) (must exist from previous arguments). Then, as the dot product
always goes up when between Cos(β) and Cos(K), and can’t go in a single step from being greater
than Cos(β) to less than Cos(κ), therefore, for every t > t6, (∣Vu(t)J ( k∣⅝) > Cos(K).
Now as the above argument holds for any K between T and ∆, and for any T > 0, we can say that
Wu(t) converges in direction ofgeu.
31
Under review as a conference paper at ICLR 2021
The proof of part 1c, i.e, (∣∣wv(t)∣∣)	( -ywv<L(w(t))) ≥ cos(δ) follows exactly the same steps as
k	k	k kwv(t)k )	∖kVwvL(w(t)川，一 ` ，	J	r
part 1b.
ʌrɑɪiɪr kww TnrYVQ fc flɔp nrccf cf nαrf kk i k k Wu (t2) k、_1_ ____、 _Hm	k Wu (t) k — γvλ
Now，We will move to the Proof of Part 2a, i.e, kwv (t2)k > (c-e)cos(δ) =⇒ limt→∞ kwv(t)k = ∞
Using Equation (11) and part 1 of the Proposition, we can say
kwu(t2 + I)k ≥ kwu(t2)k + n(t2)COS(S)kwu(t2)k2kVwu L(W(t2))k
I Iwv (t2 + I)k —	I Iwv (t2 )k + η(t2)kwv (t2)k2kVwv L(W也力|
=kwu(t2)k ( 1 + cos(S)n(t2)kwu也)∣kvwu L(W也))k ∖
=kwv (t2)k I 1 + η(t2)kwv (t2)kkVwv L(w(t2))k	J
≥ kwu(t2)k
—kwv (t2)k
Thus, kwu(t)k keeps on increasing for t > t2. It can either diverge to infinity or converge to a finite
value. If it converges to a finite value, then by Stolz Cesaro theorem,
lim kwu(t)k = lim kwu(t)k2kVwuL(w(t))k
t-∞ kwv(t)k =t→∞ kwv(t)k2kVwvL(w(t))k
However, this is not possible as kwu(t)k > 1 for every t > t2. Thus, kwu(t)k diverges to infinity.
kwv (t)k	c	kwv (t)k
NrZX7 ∖X7Q ∖xR11 mc3	tc	巾 α nrccf cf nɑrt Oh ； k	kwu(t2 ) k	/	cos(δ) _κ	]∖ rn	kwu(t)k	一 ∩
Now, we will move	to	the proof of Part 2b, i.e,	kwv(t2)k	<	c+e =⇒	limt→∞	kwv(t)k	= 0
Using Equation (11) and part 1 of the Proposition, we can say
kwu(t2 + 1)k ≤	kwu(t2)k + ηK2)kwMt2)k2kνwu L(W(t2))k
kwv (t2 + 1)k ^ kwv (t2)k + η(t2)cos(δ)kwv (t2)k2kvwv L(w(t2))k
=kwu(t2)k (	1 + η(t2)kwu(t2)kkVwuL(w(t2))k	A
kwv (t2)k V+ η(t2)cos(δ)kwv (t2)kkVwv L(w(t2))kJ
≤ kwu(t2)k
^ kwv (t2)k
Thus, kwu(t)k keeps on decreasing for t > t2. As it is always greater than zero, it must converge.
Therefore, by Stolz Cesaro Theorem,
lim kwu(t)k = lim kwu(t)k2kVwuL(w(t))k
t-∞ kwv(t)k =t→∞ kwv(t)k2kVwvL(w(t))k
per kkwu (t) k ɪ tlɔi0 Celn CTll∖7	hα CeItiQ^fikw ∖χ7kατι 11 τn	kkwu (t) k	_ (ɔ	I I
For kW~(t) k	< c, this can only	De satisfied When limt—>∞	kW~(t) k	— 0.	ι_ι
F Proof of Corollary 1
Corollary. Consider a weight normalized(SWN or EWN) multilayer linear net, represented by
y = WnWn-1...W1x. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradi-
ent descent, if the dataset is sampled from a continuous distribution w.r.t Rd, then, with probability
1, θ = W1> W2> ...Wn> converges in direction to the maximum margin separator for all linearly
separable datasets.
Proof. Consider a linear net given by f = WnWn-1...W1x, where f is a scalar as we are consider-
ing a binary classification problem with exponential loss. Then
d L(w(t))
∂W1(t)
m
-X 'i(t)yi(Wn(t)Wn-1(t)…W2(t))> x>
i=0
(35)
32
Under review as a conference paper at ICLR 2021
Now, atleast one of the neurons in every layer would have a non-zero component on we , otherwise
Φ(we , xi) = 0 for all i, that implies ρ = 0.
Let u be one of the nodes of W1 that has a non-zero component in we u and let wu be the kth row of
the matrix W1. Now, from Proposition 2, we know weu = λgeu. Denoting the component ofwe along
matrix Wk by Wfk and using Equation (35), we get, for some λk > 0,
m
WU = λk ((fnfn-1..…f2)>)[k] X Kyix
i=1
>	th
where ((WnWn-1 W2)>)[k] represents the kth component of the product column vector. Let S
represent the set of rows in W1 that have a non-zero component in we . Also, let θ denote the final
convergent direction of θ. Then, for some μ > 0, we can say
m
e = μfT fT.…WfT = μ X λj (((fn fn-1..…f2)>)jj ])2 X KyiXi
j∈S	i=1
Thus, θ satisfies the KKT conditions of the maximum margin problem and if data is sampled from a
continuous distribution w.r.t Rd, then with probability 1(Soudry et al., 2018), would converge to the
maximum margin separator.	□
G Proof of Theorem 3
Theorem. For EWN, under Assumptions (A1)-(A5) and limt→∞
hold
kr(t+1)-r(t)k
g(t+1)-g(t)
0, the following
1.	∣∣w(t)k asymptotically grows at Θ ((log(d(t))L1)
2.	L(w(t)) asymptotically goes down at the rate of Θ (d(t)(klg d(t))2).
First, we will establish rates for gradient flow and then go to the case of gradient descent.
G.1 Gradient Flow
Although the asymptotic convergence rates for smooth homogeneous neural nets have been estab-
lished in Lyu & Li (2020), the proof technique becomes easier to understand for smooth homoge-
neous nets, without weight normalization.
G.1.1 Unnormalized Network
kdr(t) k
Theorem. For Unnorm, under Assumptions (AI)-(A4) for gradient flow and limt→∞ ：%)= 0,
the following hold
1.	∣w(t)∣ asymptotically grows at Θ
2.	L(w(t)) asymptotically goes down at the rate ofΘ
1
t(log t)2-2
Proof. Consider W = g(t)W + r(t), where limt→∞ kg(t)k = 0 and r(t)>W = 0. Now, we make
k端k
g0 (t)
an additional assumption that limt→∞
0. This basically avoids any oscillations in r(t) for
large t, where it can have a higher derivative, but the value may be bounded. Now, we know
ddt = X e-yiφ(W⑴,χi)y"wΦ(w(t), Xi)
i=1
33
Under review as a conference paper at ICLR 2021
Now, We know ∣∣ dW(t) ∣∣ = 0 for any finite t, otherwise W won't change and L can't converge to 0.
Thus, for all t, we can say
k喇k
k pm=ι e-yiφ(w㈤,Xi)y"wΦ(w, Xi)k
Taking limit t → ∞ on both the sides, we get
1
k ddt) k
lim
t→∞
k pm=ι e-yiφ(W⑴,Xi)y"wΦ(w,Xi)k
(36)
1
Now, we know
k dWF k = kg0(t)W + 等 k
dt	dt
m	m	L	r(t)	r(t)
k X e-yiφ(w⑴,Xi)y"wΦ(w, Xi)k = k Xe-yigφ(w+g(t),xi)(yig(t)L-1 VWΦ(W +	, Xi)k
i 1	i 1	g(t)
Let S = {i : Φ(we, Xi) = minj Φ(we, Xj)}. Then as ρ > 0, we can say
lim
t→∞
Il Pm Q-yig(^Lφ(w + r(t),Xi) z√QL-1V7 d)(击-LMt) 丫 [11
k 2^i=ι e	虱)(yig∖t)	vwφ(w + m,xi)k
e-ρg⑴Lg(t)L-1k Pi∈s'iyiVwΦ(w, Xi)k
k
where k is some constant. Also, by the assumption
lim
t→∞
k陪k
g0(t)
1
Substituting the above two equations in Equation (36), we get
UmL	，.、r , 一 ...   ~	,…	...=T
t→∞ e-pg(t) g(t)L-1k Pi∈seyNwΦ(w, Xi)k	k
Now, as loss goes down at the rate of e-ρg(t)L , multiplying the numerator and denominator by
ρLg(t)L-1 and denoting h(t) = ρg(t)L, we get
ρ1-2 h0(t)	_ 1
Iim --------------2----------------------=—
t→∞ Le-h⑴h(t)2-2 k Pi∈s 'iyiVwΦ(w, Xi)k k
Thus, asymptotically, h(t) grows at Θ(log(t) + (2 — 2) log log t) and thus loss goes down at
Θ( —).	□
t(log t)2-L
G.1.2 Exponential Weight Normalization
k dMt) k
Theorem. ForEWN, under Assumptions (A1)-(A4) for gradient flow and limt→∞ U % J = 0, the
g (t)
following hold
1.	∣∣w(t)k asymptotically grows at Θ ((log(t)L1)
2.	L(w(t)) asymptotically goes down at the rate of Θ (^匕]古尸).
Proof. Consider W = g(t)w + r(t), where limt→∞ kr(t)k = 0 and r(t)>w = 0. Now, we make an
kdHt) k
additional assumption that limt→∞ %、= 0.
g (t)
In this case,
—
dLdW(t)) = X e-yiφ(w(t),χi)yiVwΦ(w(t), Xi)
w	i=1
34
Under review as a conference paper at ICLR 2021
HoWever, in this case, for a node u,
dwu(t) _ IIB 八∣2 dL(W⑴)
dt = k u( )k	dwu
Consider a vector a(t) of equal dimension as W, and its components corresponding to a node u is
given by a〃(t) = -∣∣WU∣2 dLdW(t)). NoW as we know W converges in direction to W, therefore,
using the update equation above, We can say
1.	k dd詈 k
lim -----ɪ-----
(→∞ g(t)2ka(t)k
1
Using the update equation for - dL(W(It), We can say
lim
t→∞ e
k dL⅞≡ k
-Pg(t)L g(t)L-1
k1
(37)
Where k1 is some constant. NoW, using the expression for a(t), We can say
lim T‰ = k
(→∞ e-ρg(()L g(t)L-1
Where k is some constant. Using the equations above, We can say
1.	g0 (t)	1
lim	=
(→∞ e-ρg(t)Lg(t)L+1	k
NoW, as loss goes doWn at the rate of e-ρg(()L, multiplying the numerator and denominator by
ρLg(t)L-1 and denoting h(t) = ρg(t)L, We get
I	ρ2h0(t)	1
lim ———-----------=—
(→∞ e-h(t)h(t)2 k
Thus, asymptotically, h(t) groWs at Θ(log(t) + 2 log log t) and thus loss goes doWn at the rate of
θ( ((log ()2 ).
□
G.2 Gradient Descent
Theorem. For Exponential Weight Normalization, under Assumptions (A1)-(A5) and
1	iτγη ,	k r (^+ɪ) r(*)k —— ∩ th P fellCM)i^Π O h∩/∕/
lim(→∞ g((+l)-g(() = 0, t几e Jollowing oObtl
1.	∣∣w(t)k asymptotically grows at Θ ((log(d(t))L1)
2.	L(w(t)) asymptotically goes down at the rate of Θ (d(t)(iog d(t))2 Y
Proof. Consider W = g(t)W + r(t), where limt→∞ kg(()k = 0 and r(t)>W = 0. Now, we make
aKK；HCneIl ClCClIτnτι1^;CnC that 11τn	kr(t+I) r(t)k ___ (ɔ
additional assumptions that iim(—>∞ -g(( ∣ι) g(t')— - 0.
Consider a node u in the network that has kWeuk > 0. The update equations for vu(t) and αu(t) are
given by
αu(t + 1)= αu(t) - η(t)eau ⑴ vu'；WtL(W⑻
eαu(()	v (t)v (t)>
vu(t + 1) = vu(t) - η(t)(I - u(7)Λ(∣2 )VwuL(W(t))
kvu(t)k	kvu(t)k2	u
Now, we will first estimate ∣∣eɑu(t+1) kVu((∣1)k - eɑu(t) kVu(()k ∣∣.	Let δu(t) denote
vu	vu
η(t)eαu(t)kVwu L(w(I))∣ and eu(t) denote the angle between vu(t) and Ewu L(w(I)). We know
35
Under review as a conference paper at ICLR 2021
limt→∞ δu (t) = 0 and limt→∞ u (t) = 0. Now, rewriting update equations in terms of these
symbols, we get
eαu(t+1) = eαu (t) eδu (t) cos(u(t))
vu(t + I)= vu(t) + δu(t) Sin(Eu(U) JWUL(W(t))vu(t)⊥∣
IIvWu L(W(t))vu(t)⊥k
where -VWuL(w(t))vu(t)⊥ denotes the component of -VWuL(w(t)) perpendicular to vu(t).
Now, using these equations we can say
eαu (t+1) vu(t +1) __ gau(t) vu(t) = eαu(t)(gδu (t) cos(eu (t)) kvu(t)k	- 1) vu(t)
kvu(t +1)k	kvu(t)k =	(	kvu(t +1)k	) kvu(t)k
(eɑu(t+1)δu(t)sin(eu(t))A -VwuL(w(t))vu(t)⊥
+ k kvu(t)kkvu(t + 1)k ) IVwuL(w(t))vu(t)⊥k
(38)
Now as limt→∞ ∣∣[%+)[§ = 1, therefore We can say
^δu(t')cos(eu(t')') kvu(t)k _ 1
lim e__________________kvu(t+1)k - 1 = 1
t→∞	δu(t)cos(eu(t))
Now, as kvu (t)k keeps on increasing during the gradient descent trajectory, therefore we can say
kv (t)k∣∣V (t+i)k ≤ k，where k > 0 is some constant. Now dividing both sides of Equation (38) by
eαu(t)δu(t) cos(Eu(t)) and analyzing the coefficient of the second term on RHS, we get
eδu(t) cos(eu(t)) sin(eu(t))
t→∞ ∣∣vu(t)kkvu(t + 1)k Cos(Eu(t)) W
Taking norm on both sides of Equation (38), using Pythagoras theorem and the limits established
above, we can say
k 尸 αu(t+1) vu(t+1) —尸 αu(t) vu(t) k
∣i ke	kvu(t+1)k e	kvu(t)kk = 1
t→∞	eαu(t)δu(t)	=
Now, we also know
keau(t+1) Vu(t+I) _ eαu(t) Vu⑴ k
lim Y----------kvu(t+1)k ；、	kvu(t)kk = ∣Wuk
t→∞	g(t + 1) - g(t)
Now, using equations above and Equation (37), we can say
lim g(t +1) - g(t)	= C
t→∞ η(t)e-ρg(t)L g(t)L+1
where c is some constant. This determines the asymptotic rate of g(t). To get a better closed form,
define define a map d : N → R, given by d(t) = Ptτ=0 η(τ) and a real analytic function f (t)
satisfying f (d(t)) = g(t) for all t ∈ N and limt→∞ "?%(，；)) = 0. Substituting this f in the
equation above, we can say
f0(d(t))
-Pf(d(t))L f (d(t))L+1
lim
t→∞ e
c
Thus f (d(t)) grows at Θ(log(d(t))L). Now, to get convergence rate for loss, multiply and divide
the equation by ρf (d(t))L-1 and denoting h(d(t)) = ρf(d(t))L, we get
一 ，一，..----------=c
h(d(t)) h(d(t))2
lim
t→∞ e-
Thus, h(d(t)) grows at the Θ(log(d(t)) + 2 log log d(t)). Now as g(t) = f (d(t)) and ρg(t)L =
h(d(t)), therefore g(t) asymptotically grows at Θ(log d(t)L) and loss goes down asymptotically at
θ( d(t) log d(t)2).	□
36
Under review as a conference paper at ICLR 2021
H Cross-Entropy Loss
In this section, we will provide the corresponding assumptions and theorems, along with their proofs,
for cross-entropy loss.
H. 1 Notations
Let k denote the total number of classes. As Φ(w, xi) is a multidimensional function for multi-class
classification, let’s denote the jth component of the output by Φj (w, xi). Also, denote the margin
for jth class corresponding to ith data point(j 6= yi) by ρi,j, i.e, ρi,j = Φyi (we , xi) - Φj (we , xi).
Margin for a data point i is defined as ρi = minj6=yi ρi,j . The margin for the entire network is
defined as ρ = mini ρi. Also, define a matrix M(w) of dimensions (m, k), that is given by
M(w)[i,j] = e-(Φyi(w,xi)-Φj(w,xi)) yyii 6=jj
Also, for a matrix A, vec(A) represents the matrix vectorized column-wise.
H.2 Assumptions
The assumptions can be broadly divided into loss function/architecture based assumptions and tra-
jectory based assumptions. The loss functions/architecture based assumptions are shared across both
gradient flow and gradient descent.
Loss function/Architecture based assumptions
1	'(yi, Φ(w, Xi)) = log(l + Pj=yi e-(φyi (w,xi …j (W,Xi)))
2	Φ(., x) is a C2 function, for a fixed x
3	Φ(λw, x) = λLΦ(w, x), for some λ > 0 and L > 0
Gradient flow. For gradient flow, we make the following trajectory based assumptions
(A1) limt→∞	L(w(t))	= 0	(A3)iim+	Vec(M(Wm))	∙= Vecf)
小	(A3)	limt-∞	∣∣vec(M(w(t)))k	:= Vec(M )
(A2) limt→∞ kW(t)k := W	(A4) ρ > 0.
All the assumptions above are exactly the same as for exponential loss, except for (A3). Using
assumption (A1), we can say
iim	e-(Φyi(W(t),xi)-Φj(W(t),xi)) = 0
t→∞
j6=yi
iog(1 + P	e-(Φyi (W(t),xi)-Φj (W(t),xi)))
Iim ------------j=yi _——：—―-----；~———―-------------- = 1
t→∞	P行y e-(φyi(w(t),Xi)-φj(w(t),Xi))	=
(39)
(40)
Thus, we can say, for large enough t, `i ≈ Pj6=y e-(Φyi (W(t),Xi)-Φj (W(t),Xi)). Thus, assumption
(A3) basically states that, not just the loss vector converges in direction, but its components cor-
responding to various classes also converge in direction. This is required to show that gradients
converge in direction in case of multi-class classification.
Gradient Descent. For gradient descent, we also require the learning rate η(t) to not grow too fast.
(A5) limt→∞ η(t)∣∣Wu(t)∣NwuL(w(t))k = 0 for all U in the network
Proposition 4. Under assumptions (A1)-(A4), limt→∞ η(t)∣∣Wu(t)∣Nwu L(w(t))k = 0 holds for
every u in the network with η(t) = O(L), where c < L
Proof. The gradient of the loss function is given by
m
VwL = X TZP-e-(Φy.(w,Xi)-Φj(w,Xi)) X e-(φyi(W,"鹏(w’Xi))(VW%即 Xi)-VW%代, Xi))
i=1 1 +	j 6=yi e	i	j 6=yi
(41)
37
Under review as a conference paper at ICLR 2021
Under Assumption (A1), w(t) can be represented as w(t) = g(t)W + r(t), where limt→∞ kg(t)k =
0 and r(t)>we = 0. Using the equation above, we can say
Iirn kVwL(w(t))k k
lim ∕*∖l , 、 = = — k
t→∞ e-ρg(t)L g(t)L-1
As the order remains the same as in the proof for exponential loss, the proof follows from Appendix
B.	□
This proposition establishes that the Assumption (A5) is mild and holds for constant η(t), that is
generally used in practice.
H.3 Effect of Normalisation on Weight and Gradient Norms
This section contains the main theorems and the difference between EWN and SWN that makes
EWN asymptotically relatively sparse as compared to SWN. First, we will state a common proposi-
tion for both SWN and EWN.
Proposition 5. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent,
for both SWN and EWN, the following hold:
--⅛	1 ∙	—VwL(w(t))	X^~`m	X^"`	Ayr / ∙	∙∖∕V7 √κ	( ■~	、 x~~ι √κ / ■~	、、	~
(i)	limt→∞	kVwL(w(t))k	— μ ∑i=1	∑j=yi M(i,j)(Vwφyi(w, Xi)	- Vwφj (W, Xi))	— g,
where μ > 0.
(ii)	∣∣wUk > 0 —⇒ WU — λgu for some λ > 0
⑴ limt→∞ k^Ww(W'(())k = μ Pi=I Pj=yi M[i,j](Vwφyi (W, Xi)- Vwφj(W, Xi)) = g, Where
μ > 0
Proof. using Assumption (A1), w(t) can be represented as w(t) — g(t)wg + r(t), where
limt→∞ kg-(∣)k = 0. Then,
e—(φyi(w,xi)—φj(w,Xi)) — e—g(t)L ((φyi(W + g(t) ,Xi) — φj(w + g(t),Xi))
r(t)	r(t)
VwΦj(w, Xi) — VwΦyi(w, Xi) — g(t)L 1(VwΦj(W + -^, Xi) — VwΦyi(W + --^, Xi))
g(t)	g (t)
Now, using ρ > 0 and Euler’s homogeneity theorem, we can say
wg > (VwΦj (w, Xi) — VwΦyi (w, Xi)) — L((Φyi (w, Xi) — Φj (w, Xi)) > 0
Thus, kVwΦj (w, Xi) — Vw Φyi (w, Xi)k > 0 for all i,j. Using these facts, Equation (39), Equation
(40) and Equation (41), we can say
lim IlTyw^ WI — μ	M[i,j](VwΦ7r(W, Xi) -VwΦj(W, Xi))
t→∞ kVwL(w(t))k	μ乙乙 1"八 w 八… w 八，i”
w	i=1 j6=yi
□
(ii) ∣∣WUk > 0 =⇒ Wu — λgu for some λ > 0
Proof. The proof follows from Appendix C.
□
The first and second part state that under the given assumptions, for both SWN and EWN, gradients
converge in direction and the weights that contribute to the final direction of W, converge in opposite
direction of the gradients. Now, we provide the main theorem that distinguishes SWN and EWN.
Theorem 4. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent, the
following hold
kw frtr FWN Il缶 Il、八 kwu Il、0	'、 Hm	kWu(t)kWwu L(W(t))k 一 1
⑴ JorEWN, kwuk > 0, kwv k > 0 =⇒ limt→∞ kwv (t)kkVwv L(w(t))k = 1
38
Under review as a conference paper at ICLR 2021
kw kwwu SWN I∣-U7 Il、八 kwu Il、0	Iim	kwu⑴ IIWwv L(w(t))k _ 1
(ii)	forSWN, kwuk > 0, IIWvk > 0 =⇒ limt→∞ kwv(t)kgwu L(w(t))k = 1
Proof. The proof follows from Appendix D.
□
H.4 Sparsity Inductive Bias for Exponential Weight Normalisation
The following proposition shows why EWN is likely to converge to relatively sparse points.
Proposition 6. Let assumptions (A1)-(A5) be satisfied. Consider two nodes u and v in the network
such that kev k > 0, keuk > 0, ∣Wu(t)k → ∞ and ∣∣wv(t)k → ∞. Let 1|/ be denoted by c. Let
, δ be such that 0 < < c and 0 < δ < 2π. Then, the following holds:
1.	There exists a time t1, such that for all t > t1 both SWN and EWN trajectories have the
following properties:
(n]	gwu L(w(t))k ∈ r r+∏
(a)	gwvL(w(t))k ∈ [c - e, c + e]
(b)	( Wu㈤ ∖> ( TwuL(Wo) ) ≥ cos(δ)
(b)	^∣wu(t)k )	^∣VwuL(w(t))k ) ≥cos(δ)
(C) ( Wv ⑴)> (-VwvL(W ㈤))≥ cos(δ)
(C) IkWv (t)∣ )	IkVwvL(w(t))∣U ≥ cos(δ).
2.	forSWN, limt→∞ 粽解=C
3.	for EWN, if at some time t2 > t1,
(n)	WA2)	`	1	―	lirn kwu(t)k	—
(a)	wv (t2)	>	(c-e)cos(δ)	=⇒	limt→∞ ∣wv (t)k	= ∞
b	Wu(t2)	j	Cos(δ)	I：	llWu(t)k _ 0
(b)	Wv(t2)	<	-τ+τ	=⇒	limt→∞ ∣wv(t)k = 0
The above proposition shows that the limit property of the weights in Theorem 4 makes non-sparse
W an unstable convergent direction for EWN, while that is not the case for SWN.
Proof. The proof follows from Appendix E.
□
H.5 Convergence rates
Theorem 5. For EWN, under Assumptions (A1)-(A5) and limt→∞
hold
kr(t+1)-r(t)k
g(t+1)-g(t)
0, the following
1.	∣∣w∣ asymptotically grows at Θ ((log(d(t)) L)
2.	L(w(t)) asymptotically goes down at the rate of Θ (d(t)(Iogd(Ty)2
Proof. The proof follows Appendix G.2, the only difference is in the gradient update. Let w be
represented as W = g(t)W + r(t), where limt→∞ kg(t)k = 0. Using Equation (41), We can say
lim 鼻Wfk = k
t→∞ e-ρg(t)L g(t)L-1
As the order remains the same as in the proof for exponential loss, the proof follows from Appendix
G.2.	□
I Lemma Proofs
Lemma 1. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent, for
both SWN and EWN, We u> eeu ≥ 0for all nodes u in the network.
39
Under review as a conference paper at ICLR 2021
Proof. We will show the proof just for exponential parameterization and gradient descent, but other
cases can be handled similarly.
We only need to consider nodes having kweuk > 0 and kgeuk > 0, as for other nodes weu>geu = 0.
Consider a node u having kweu k > 0 and kgeu k > 0. Let’s say weu>geu < 0. This means, there exists
a time tι, such that for any t > tι, Wu(t)>(-VwuL(w(t))) < 0. Then using Equation (11), We
can say that, for t > t1, αu(t + 1) < αu(t). But, this contradicts the assumption that kwuk → ∞.
Thus, W>eu ≥ 0.	□
Lemma 2. Under assumptions (A1)-(A4) for gradient flow and (A1)-(A5) for gradient descent,
for both SWN and EWN, there exists atleast one node u in the network satisfying kweu k > 0 and
kegu k > 0.
Proof. Under the assumption that ρ > 0 and using Euler’s homogeneous theorem, using Proposition
2, We can say
mm
W>e = μXQeyW>VwΦ(W, Xi) = LμXKyiΦ(W, Xi) > 0
i=1	i=1
Thus, there must be atleast one node s satisfying kWes k > 0 and kges k > 0. Similarly, it can be
shown for cross-entropy loss as well.	□
Lemma 3. Consider two unit vectors a and b satisfying a>b ≥ 0 and a>b < 1. Then, there exists
a small enough > 0, such that for any unit vector c satisfying c>a ≥ cos() and any unit vector d
satisfying d>b ≥ cos(), b> (I - cc> )d ≥ .
Proof. First we will try to find bounds on b>c and c>d.
b>c = b>(a + c - a)
= b>a + b>(c - a)
c> d = (a + c - a)> (b + d - b)
= a>b + a>(d - b) + b>(c - a) + (c - a)>(d - b)
Now, using the fact that c>a ≥ Cos(E) and d>b ≥ Cos(E), we can say ∣∣c 一 a∣∣ ≤，2 - 2cos(c)
and ∣∣d 一 b∣ ≤，2 - 2cos(e). Using these bounds and the equation above, we can say
b>c ≤ a>b + p2 — 2 Cos(E)
c>d ≤ a>b + 2p2 — 2Cos(E) + (2 - 2cos(e))
Using these, we can say
b>(I — cc>)d ≥ Cos(E) — (a>b + p2 — 2cos(E))(a>b + 2p2 — 2cos(e) + (2 — 2cos(e)))
Now, we need to show that there exists an e > 0 such that Cos(E) — (a>b + ,2 - 2Cos(E))(a>b +
2,2 - 2cos(e) + (2 — 2cos(e))) > e. At E = 0, LHS takes the value 1 — (a>b)2, while RHS takes
the value 0. Thus, by continuity with respect to E of the functions involved, we can say that there
exists an e > 0 for which the condition is satisfied.	□
Lemma 4. Consider sequence a satisfying the following properties
1. ak > 0
∞
2.	k=0 ak = ∞
3.	limk→∞ ak = 0
Then P∞=0 qp⅛ = ∞
40
Under review as a conference paper at ICLR 2021
Proof. If Pk∞=0 a2k is bounded, then the statement is obvious. Let’s consider the case when Pk∞=0 a2k
diverges. As limk→∞ ak = 0, therefore there must be an index k1, such that for k ≥ k1, ak ≤ .
Now, as ak ≤ , therefore a2k ≤ ak . Now, as Pk∞=0 a2k diverges, therefore, there must be an index
k2 > k1, such that for any k > k2, Pjk=k aj2 ≥ Pjk=1-01 aj2. Now, for k > k2, we can say
≥
1
√2
k
X
j=k1
a
Plj=k1 al
As P∞=0 ak diverges, therefore P∞=0 / ak diverges as well.
k=0	k=0	jk=0 aj2
□
Lemma 5. Consider two sequences a and b satisfying the following properties
1.	ak > 0, Pk=0 ak = ∞ and limk→∞ ak = 0
2.	b0 > 0, b is increasing and b1+ι ≤ bk + (ak)2
Then Σ∞=0 ak = ∞.
Proof. As we know b is increasing and b2k+1 ≤ bk + (ak)2, Weget
bk ≤ t
k-1	u
b2+X( aj )2 ≤ t
k-1
b2 + b2 X aj
0 j=0
Using this, we can say
k
X a. ≥ X	a
j=0 % — j=0 Jb0 + b0 Pj-
k
≥ X , a —
j=0 /0 + b10 Pk-1 α2
k
Now, if Pk∞=0 a2k does not diverge to infinity, then b remains bounded using the bound above and
then its trivial to establish that P∞=0 ⅞k diverges. In case, P∞=° 碌 diverges to infinity, then there
must be an index k1 such that for any k > k1, we can say Pjk=-01 aj2 ≥ b40. So, for k > k1, we can
say
k
k
XX aj ≥ X
b0	aj
Now, as we have assumed a tends to zero, so there must be an index k2 such that for any k > k2,
ak ≤ . Also, as we have assumed Pj∞=0 aj2 diverges, therefore there must be an index k3 > k2,
such that for k> k3, Pjk=k2 aj2
we can say for k > k3,
≥
Pjk=2 0 aj2. Using these things and that if aj ≤ , then aj2 ≤ aj,
k
X
j=k3
a
bj
k
X
j=k3
b0	aj	b	b0	U
-2	qPk-ι	≥ 2√it 工 aj
l=k3 al	j=k3
≥
Now, as P∞=0 ak diverges, thus P∞=0 ⅞k diverges as well.
□
41
Under review as a conference paper at ICLR 2021
Lemma 6. Consider two sequences a and b satisfying the following properties
1.	ak > 0 and k∞=0 ak = ∞
2.	bk > 0 and Pk∞=0 bk = ∞
3.	k∞=0 (ak - bk) converges to a finite value
4.	limk→∞ ak exists
Then limk→∞ 置=1.
Proof. Let's say limk→∞ ⅞k = c > 1. The other case can be handled similarly. Choose an e > 0
bk
such that c - > 1. Then, there exists an index k1, such that for k > k1, we can say
C 一 E ≤ ak ≤ C + E
bk
Using this, we can say, for k > k1,
bk (C - E - 1) ≤ ak - bk ≤ bk (C + E - 1)
Summing the equation above from k1 to ∞ and recognizing that Pk∞=0 b diverges, we get
P∞=k1 (αk — bk) = ∞. This contradicts. Therefore limk→∞ ^k = 1.	口
J Integral Form of Stolz-Cesaro Theorem
We first state the Stolz-Cesaro Theorem.
Theorem. (Muresan, 2015) Assume that {α}k∞=1 and {b}k∞=1 are two sequences of real numbers
such that {b}∞=ι is strictly monotonic and diverging. Additionally, if limk→∞ b：+：-；： = L exists,
then limk→∞ ；： exists and is equal to L.
Now, we state and prove the Integral Form of Stolz-Cesaro Theorem.
Theorem. Consider two functions f(t) and g(t) greater than zero satisfying Rab f (t)dt < ∞ and
Rabg(t)dt < ∞for every finite α, b. For any time t, its known that Rt∞ f (t)dt = ∞ and Rt∞ g(t)dt =
∞. If limt→∞ ft) exist and is equal to L, then limt→∞ Rt g(t)dt exists for any C and is equal to L.
Proof. Case 1: L = 0 or ∞:
We will prove for L = ∞. The case for 0 can be handled similarly. For any M > 0, there must exist
a time tι > c, such that f(∣) > M, for t > tι. Thus We can say for t > tι,
Ztf(t)dt>Zt1
cc
f (t)dt + M
t g(t)dt
t1
Adding M Rct1 g(t)dt on both the sides, We get
Z f (t)dt + M Z	g(t)dt > Z f (t)dt + M Z g(t)dt
c	cc	c
Dividing both sides by Rct g(t)dt and taking limsup t → ∞(using also the fact that Rab f (t)dt < ∞
and Rab g(t)dt < ∞ for every finite α, b), We get
Rt f (t)dt
limsup JCt L > M
t→∞	c g(t)dt
Similarly the equation holds for liminf as Well. Thus, both liminf and limsup are greater than M for
any M. Hence limt→∞ Rt g(t)dt = ∞.
42
Under review as a conference paper at ICLR 2021
Case2: L is finite
In this case, there must exist some time tι > c, such that L - e < fg(t) < L + e.Thus, We can say
for t > t1 ,
t1f(t)dt+(L-) t g(t)dt ≤	tf(t)dt≤	t1f(t)dt+(L+) t g(t)dt
c	t1	c	c	t1
Taking the left inequality, adding (L - ) Rct1 g(t)dt on both the sides, dividing both the sides by
Rct g(t)dt and taking lim inft→∞, We get
Rt f (t)dt
L - E ≤ liminf JCt J
t→∞ ct g(t)dt
Similarly, taking the right inequality, adding (L + E) Rct1 g(t)dt on both the sides, dividing both the
sides by Rct g(t)dt and taking lim supt→∞, We get
Rt f (t)dt
lim sup -Ct-------- ≤ L + E
t→∞	c g(t)dt
Using the tWo inequalities, We get, for any E > 0,
Rt f (t)dt	R t f (t)dt
limsup -Ct---------liminf 与----------≤ 2e
t→∞ Rc g(t)dt	t→∞ Rc g(t)dt
Thus, limt→∞ Rt ；(；)；； exists and is equal to L.
□
K Standard Weight Normalization is not Locally Lipschitz in its
PARAMETERS
In this section, We Will denote w by θ so as to be consistent With the notaion in Lyu & Li (2020).
SWN(in its parameters γ and v) is also a homogeneous netWork. Therefore, results from Lyu & Li
(2020) should directly apply to the case of SWN as Well. HoWever, a crucial point to be noted is that
it is not even locally Lipschitz around kvu k = 0. Therefore, the assumptions from Lyu & Li (2020)
do not hold.
HoWever, during gradient descent or gradient floW, if started from a finite kvu k > 0, for all u, then
during the entire trajectory, kvu k cannot go doWn. Therefore, the netWork is still locally Lipschitz
along the trajectory it takes. Examining the proofs from Lyu & Li (2020), its clear that the proof
regarding monotonicity of margin and convergence rates are just dependent on the path that gradient
descent/floW takes and thus the proofs hold.
However, the result regarding the limit points of 高 do not hold. One of the crucial theorems the
proof relies on is stated beloW
Theorem. Let {xk ∈ Rd: k ∈ N} be a sequence of feasible points of an optimization problem (P),
{Ek > 0 : k ∈ N} and {δk > 0 : k ∈ N} be two sequences. xk is an (Ek, δk)-KKT point for every k
and Ek → 0, δk → 0. If xk → x as k → ∞ and MFCQ holds at x, then x is a KKT point of (P)
The above statement requires MFCQ to be satisfied at x, that was shown in Lyu & Li (2020) as-
suming local lipschitzness/smoothness at x. However, in this case, for gradient flow, as kvu k does
not grow, while ∣γu∣ → ∞, therefore the convergent point of 扁 will always have the component
corresponding to vu as 0. Thus, the network is not locally lipschitz at x and the proof that MFCQ
holds is violated. Similarly, for gradient descent as well, it can’t be said that vu has a non-zero
component in 岛.Thus, the proof does not hold.
L Experiment Details
In all the experiments, techniques for handling numerical underflow were used as described in Lyu
& Li (2020). However, the learning rate they used was of O(L1), but in our case, we generally
modify it to be O(表),where c < 1.
43
Under review as a conference paper at ICLR 2021
L.1 Lin-Sep
The learning rate used was Lk(⅛7, so that it speeds UP at the beginning of training, but slows down
as loss approaches e-300. The constant k(t) was initialized at 0.01, and was increased by a factor
of 1.1 every time loss went down and decreased by a factor of 1.1 every time loss went up after a
gradient step. Its value was capped at 0.01 for EWN and SWN.
L.2 Simple-Traj
The learning rate used was Lk^, so that it speeds up at the beginning of training, but slows down as
loss approaches e-50. The constant k(t) was initialized at 0.01, and was increased by a factor of 1.1
every time loss went down and decreased by a factor of 1.1 every time loss went up after a gradient
step. Its value was capped at 0.1 for EWN and Unnorm.
L.3 XOR
The learning rate used was Lk(⅛, so that it speeds up at the beginning of training, but slows down as
loss approaches e-50. The constant k(t) was initialized at 0.01, and was increased by a factor of 1.1
every time loss went down and decreased by a factor of 1.1 every time loss went up after a gradient
step. Its value was capped at 0.01 for EWN and 0.1 for other cases.
L.4 Convergence rate experiment
For all SWN, EWN and Unnorm, the learning rate was constant η = 0.01 and they were trained for
5000 steps. All the networks were explicitly initialized to the same point in function space.
L.5 MNIST Pruning Experiment
The learning rate used was 噌.The constant k(t) was initialized at 0.01, and was increased by a
factor of 1.1 every time loss went down and decreased by a factor of 1.1 every time loss went up
after an epoch. Its value was capped at 0.01 for all the cases.
M Results for Standard Weight Normalization
N MNIST Pruning Experiments
44
Under review as a conference paper at ICLR 2021
(a) Dataset
Q
5000	10000
Steps
go.5
0.0
O	5000	IOOOO
Steps
(b) Evolution of kwu k
5000
IOOOO
Steps
(c) Weight vector convergence (d) Loss vector convergence (e) Normalized parameter margin
Figure 8:	Verification of assumptions for SWN in Lin-Sep experiment: (a) shows the dataset.
In (b), it can be seen that only weights 5, 6 and 8 keep on growing in norm. So, only for these,
IlwUIl > 0. (C) shows the components of the unit vector ^^^, only for the weights 5, 6 and 8 as
they keep evolving with time. Eventually their contribution to the unit vector become constant. (d)
shows the components of the loss vector and they also become constant eventually. (e) shows the
normalized parameter margin converging to a value greater than 0.
45
Under review as a conference paper at ICLR 2021
Q≡≥> PUe S M/q φ⊂~ωou
5000
Steps
(b)
10000
6 pue Q3≥> M/q φ⊂~ωou
-1.0	-------------------------------------- -1.0
0	5000	10000	0
Steps
(a)
1.7	1.8	1.9	2.0
∣og IlWUll
(e)
=-l>- 6°-
1.7	1.8	1.9
∣og IlWUll
(d)
-5- 6°
-⅛>- 6°
1.6
1.7
∣og IMUIl
(c)
Figure 9:	Demonstration of Results for SWN in Lin-Sep experiment: (a) demonstrates part 1
of Proposition 2, where ge is approximated by using w from the last point of the trajectory. Clearly,
▽wu L stops oscillating and converges to e. (b) demonstrates part 2 of Proposition 2 and shows
that for weight vectors 5,7 and 8, Wu(t) converges in opposite direction of VwuL(w(t)). (c), (d)
and (e) demonstrate Theorem 2 for SWN, where for weight vectors 5,7 and 8. The three graphs are
plotted at loss values of e-200, e-250 and e-300 respectively. At each loss value, for the 3 weights,
log kVwu Lk - log kwu k is approximately same.
46
Under review as a conference paper at ICLR 2021
-10	1
(a) Dataset
• class -1
• class 1
(b) EWN
O	500	IOOO	1500
Steps	Steps
(c) Unnorm	(d) SWN
O
Figure 10:	(a) shows the XOR dataset. (b), (c) and (d) demonstrate that EWN weights grow sparsely
when compared to Unnorm and SWN
47
Under review as a conference paper at ICLR 2021
E」ON
(a) EWN
Figure 11:	Norm of the weight vector
when trained on MNIST.
IOOO 2000	3000	4000	5000
Steps
(c) Unnorm
vs gradient descent steps, for various nodes in the first layer,
48