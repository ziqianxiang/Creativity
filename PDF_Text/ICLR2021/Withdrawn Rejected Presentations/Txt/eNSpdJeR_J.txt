Under review as a conference paper at ICLR 2021
Deep Learning with Data Privacy via Residual
Perturbation
Anonymous authors
Paper under double-blind review
Ab stract
Protecting data privacy in deep learning (DL) is at its urgency. Several celebrated
privacy notions have been established and used for privacy-preserving DL. However,
many of the existing mechanisms achieve data privacy at the cost of significant
utility degradation. In this paper, we propose a stochastic differential equation
principled residual perturbation for privacy-preserving DL, which injects Gaussian
noise into each residual mapping of ResNets. Theoretically, we prove that residual
perturbation guarantees differential privacy (DP) and reduces the generalization
gap for DL. Empirically, we show that residual perturbation outperforms the state-
of-the-art DP stochastic gradient descent (DPSGD) in both membership privacy
protection and maintaining the DL models’ utility. For instance, in the process of
training ResNet8 for the IDC dataset classification, residual perturbation obtains
an accuracy of 85.7% and protects the perfect membership privacy; in contrast,
DPSGD achieves an accuracy of 82.8% and protects worse membership privacy.
1	Introduction
Many high-capacity deep nets (DNs) are trained with private data, including medical images and
financial transaction data (Yuen et al., 2011; Feng et al., 2017; Liu et al., 2017). DNs usually
overfit and can memorize the private training data, which makes training DNs exposed to data
privacy leakage (Fredrikson et al., 2015a; Shokri et al., 2017; Salem et al., 2018; Yeom et al., 2018;
Sablayrolles et al., 2018). Given a pre-trained DN, the membership inference attack can determine if
an instance is in the training set based on DN’s response (Fredrikson et al., 2014; Shokri et al., 2017;
Salem et al., 2018); the model extraction attack can learn a surrogate model that matches the target
model, given the adversary only black-box access to the target model (Tramer et al., 2016; Gong &
Liu, 2018); the model inversion attack can infer certain features of a given input from the output of a
target model (Fredrikson et al., 2015b; Al-Rubaie & Chang, 2016); the attribute inference attack can
deanonymize the anonymized training data (Gong & Liu, 2016; Zheng et al., 2018).
Machine learning (ML) with data privacy is crucial in many applications (Lindell & Pinkas, 2000;
Barreno et al., 2006; Hesamifard et al., 2018; Bae et al., 2019). Several algorithms have been
developed to reduce privacy leakage include differential privacy (DP) (Dwork et al., 2006), federated
learning (FL) (McMahan et al., 2016; Konecny et al., 2016), and k-anonymity (Sweeney, 2002;
El Emam & Dankar, 2008). Objective, output, and gradient perturbations are among the most used
approaches for ML with DP guarantees at the cost of significant utility degradation (Chaudhuri et al.,
2011; Bassily et al., 2014; Shokri & Shmatikov, 2015; Abadi et al., 2016b; Bagdasaryan et al., 2019).
FL trains centralized ML models, through gradient exchange, with the training data being distributed
at the edge devices. However, the gradient exchange can still leak the privacy (Zhu et al., 2019; Wang
et al., 2019c). Most of the existing privacy is achieved at a tremendous sacrifice of utility. Moreover,
training ML models using the state-of-the-art DP stochastic gradient descent (DPSGD) leads to
tremendous computational cost due to the requirement of computing and clipping the per-sample
gradient (Abadi et al., 2016a). It remains a great interest to develop new privacy-preserving ML
algorithms without excessive computational overhead or degrading the utility of the ML models.
1.1	Our Contribution
In this paper, we propose residual perturbation for privacy-preserving deep learning (DL) with DP
guarantees. At the core of residual perturbation is injecting Gaussian noise to each residual mapping
1
Under review as a conference paper at ICLR 2021
of ResNet (He et al., 2016), and the residual perturbation is theoretically principled by the stochastic
differential equation (SDE) theory. The major advantages of residual perturbation are threefold:
•	It can protect the membership privacy of the training data almost perfectly and often without
sacrificing ResNets’ utility. Furthermore, it can even improve ResNets’ classification accuracy.
•	It has fewer hyperparameters to tune than the benchmark DPSGD. Also, it is more computationally
efficient than DPSGD, which requires to compute the per-sample gradient.
•	It can be easily implemented by a few lines of code in modern DL libraries.
1.2	Related Work
Improving the utility of ML models with DP guarantees is an important task. PATE (Papernot et al.,
2017; 2018) uses semi-supervised learning together with model transfer between the “student” and
“teacher” models to enhance utility. Several variants of the DP notions have also been proposed
to improve the privacy budget and some times can also improve the resulting model’s utility at a
given DP budget (Abadi et al., 2016b; Mironov, 2017; Wang et al., 2018; Dong et al., 2019). Some
post-processing techniques have also been developed to improve the utility of ML models with
negligible computational overhead (Wang et al., 2019a; Liang et al., 2020). From the SDE viewpoint,
(Li et al., 2019; Wang et al., 2015) showed that several stochastic gradient Monte Carlo samplers
could reach state-of-the-art performance in terms of both privacy and utility in Bayesian learning.
Gaussian noise injection in residual learning has been used to improve the robustness of ResNets
(Rakin et al., 2018; Wang et al., 2019b; Liu et al., 2019). In this paper, we inject Gaussian noise to
each residual mapping to achieve data privacy instead of adversarial robustness.
1.3	Organization
We organize this paper as follows: In Section 2, we introduce the residual perturbation for privacy-
preserving DL. In Section 3, we present the generalization and DP guarantees for residual perturbation.
In Section 4, we numerically verify the efficiency of the residual perturbation in protecting data
privacy without degrading the underlying models’ utility. We end with some concluding remarks.
Technical proofs and some more experimental details and results are provided in the appendix.
1.4	Notations
We denote scalars by lower or upper case letters; vectors/ matrices by lower/upper case bold face
letters. For a vector X = (xι,…，Xd) ∈ Rd, We usekxk2 = (Pid=1 |xi|2)1/2 to denote its `2 norm.
For a matrix A, We use kAk2 to denote its induced norm by the vector `2 norm. We denote the
standard Gaussian in Rd as N(0, I) With I ∈ Rd×d being the identity matrix. The set of (positive)
real numbers is denoted as (R+) R. We use B(0, R) to denote the ball centered at 0 With radius R.
2	Algorithms
2.1	Deep Residual Learning and Its Continuous Analogue
Given the training set SN := {xi, yi}iN=1, With {xi, yi} ⊂ Rd × R being a data-label pair. For a
given xi the forWard propagation of a ResNet With M residual mappings can be Written as
xl+1 = xl + F(xl, Wl), for l = 0, 1,…，M — 1, with x0 = Xi； yi = f(xM),	(1)
where F(∙, Wl) is the nonlinear mapping of the lth residual mapping parameterized by Wl; f is the
output activation function, and yi is the predicted label for Xi. The heuristic continuum limit of (1) is
dx(t) = F(x(t), W(t))dt, x(0) = X, where t is the time variable.	(2)
The ordinary differential equation (ODE) (2) can be revertible, and thus the ResNet counterpart might
be exposed to data privacy leakage. For instance, we use the ICLR logo (Fig. 1 (a)) as the initial
data X in (2). Then we simulate the forward propagation of ResNet by solving (2) from t = 0 to
t = 1 using the forward Euler solver with a time step size ∆t = 0.01 and a given velocity field
F (X(t), W(t)) (see Appendix E for the details of F (X(t), W(t))), which maps the original image to
its features (Fig. 1 (b)). To recover the original image, we start from the feature and use the backward
Euler iteration, i.e., X(t) = X(t + ∆t) — ∆tF(X(t + ∆t),t + ∆t), to evolve X(t) from t = 1 to t = 0
with X(1) = x(1) being the features obtained in the forward propagation. We plot the recovered
image from features in Fig. 1 (c), and the original image can be almost perfectly recovered.
2
Under review as a conference paper at ICLR 2021
勒 S .3 ⅜∣CUR 「，」哪E
(a) x(0) (ICLRlogo)	(b) x(1) (ODE)	(C) X(0) (ODE)	(d) x(1) (SDE)	(e) X(0) (SDE)
Figure 1: Illustrations of the forward and backward propagation of the training data using 2D ODE
(2) and SDE (3) models. (a) the original image; (b) & (d) the features of the original image generated
by the forward propagation using ODE and SDE, respeCtively; (C) & (e) the reCovered images by
reverse-engineering the features shown in (b) & (d), respeCtively. We see that it is easy to break the
privaCy of the ODE model, but harder for SDE.
2.2	Residual Perturbation and its SDE Analogue
In this part, we propose two SDE models to reduCe the reversibility of (2), and the Corresponding
residual perturbations analogue Can proteCt the data privaCy in DL.
Strategy I. For the first strategy, we Consider the following SDE model:
dx(t) = F (x(t), W(t))dt + γdB(t), γ > 0,	(3)
where B(t) is the standard Brownian motion. We simulate the forward propagation and reverse-
engineering the input from the output by solving the SDE model (3) with γ = 1 using the same
F(x(t), W(t)) and initial data x. We use the following forward (4) and backward (5) EUler-
Maruyama disCretizations (Higham, 2001) of (3),
x(t + ∆t) = x(t) + ∆tF(x(t), W(t)) + YN(0, √∆t I),	(4)
X(t) = X(t + ∆t) — ∆tF(X(t + ∆t), W(t + ∆t)) + YN(O, √∆t I),	(5)
for the forward and backward propagation, respectively. Figure 1 (d) and (e) show the results of the
forward and backward propagation by SDE, respectively, and these results show that it is much harder
to reverse the features obtained by SDE evolution. The SDE model informs us to inject Gaussian
noise, in both training and test phases, to each residual mapping of ResNet to protect data privacy,
which results in
xi+1 = Xi + F(Xi, Wi) + γni, where ni 〜N(0, I)1.	(6)
Strategy II. For the second strategy, we consider using the multiplicative noise instead of the
additive noise used in (3) 2 and (6), and the corresponding SDE can be written as
dX(t) = F (X(t), W(t))dt + YX(t) dB(t), Y > 0,	(7)
where denotes the Hadamard product. Similarly, we can use the forward and backward Euler-
Maruyama discretizations of (7) to propagate the image in Fig. 1 (a), and we provide these results in
Appendix D.1. The corresponding residual perturbation is
xi+1 = xi + F(Xi Wi) + YXi Θ ni, where ni 〜N(0, I),	(8)
again, the noise YXi	ni is injected to each residual mapping in both training and test phases.
We will provide theoretical guarantees for these two residual perturbation schemes, i.e., (6) and (8),
in Section 3, and numerically verify their efficacy in Section 4.
2.3	Utility Enhancement via Model Ensemble
Wang et al. (2019b) showed that an ensemble of noise injected ResNets can improve models’ utility.
In this paper, we will also study the model ensemble for utility enhancement. We inherit notations
from (Wang et al., 2019b), e.g., we denote an ensemble of two noise injected ResNet8 as En2ResNet8.
3	Main Theory
In this section, we will provide theoretical guarantees for the above two residual perturbations.
1Liu et al. (2019); Wang et al. (2019b) used this noise injection to improve robustness of ResNets.
2Liu et al. (2019) injected multiplicative noise to neural networks to improve their robustness.
3
Under review as a conference paper at ICLR 2021
3.1	Differential Privacy Guarantee for Strategy I
We consider the following function class for ResNets with residual perturbation:
Fi := {f (x) = WTxM∣xi+1 = Xi + φ (Uixi) + γni, i = 0,…，M — 1,	(9)
x0 = input data + πn, n and ni 〜N(0, I), W ∈ Rd, Ui ∈ Rd×d},
where x0 ∈ Rd is the noisy input 3 , Ui is the weight matrix in the ith residual mapping and w ∈ Rd
is the weights of the last layer. γ, π > 0 are hyperparameters. φ = BN(ψ) with BN being the batch
normalization and ψ being a L-Lipschitz and monotonically increasing activation function (e.g.,
ReLU). We first recap on the definition of differential privacy below.
Definition 1 ((, δ)-DP). (Dwork et al., 2006)A randomized mechanism M : SN → R satisfies
(, δ )-DP if for any two datasets S, S0 ∈ S N that differ by one element, and any output subset O ⊆ R,
itholdsthat P[M(S) ∈ O] ≤ ee ∙ P[M(S0) ∈ O] + δ, where δ ∈ (0,1) and e > 0.
We have the following DP guarantee for Strategy I, and we provide its proof in Appendix A.
Theorem 1. Assume the input to ResNet lies in B(0, R) and the output of every residual mapping
is normal distributed and bounded by G, in `2 norm, in expectation. Given the total number of
iterations T used for training ResNet. For any e > 0 and δ, λ ∈ (0, 1), the parameters Ui andW in the
ResNet with residual perturbation satisfies ((λ∕i + (1 — λ))e, δ)-DP and ((λ∕M + (1 — λ))e, δ)-
DP, respectively, provided that π > R,(2Tba)∕(Nλe) and Y > G，(2Tba)∕(Nλe), where
α = log(1∕δ)/ ((1 — λ)e) + 1, M, N and b are the number ofresidual mappings, training data, and
batch size, respectively. In particular, when Y > G,(2Tba)∕(NMλe) the whole model obtained by
injecting noise according to strategy I satisfies (e, δ)-DP.
3.2	Theoretical Guarantees for Strategy II
Privacy. To analyze the residual perturbation (8), we consider the following function class:
F2 ：= {f (x) = WTxM + πxMn| xi+1 = xi + φ (Uixi) + Yxi Θ ni),	(10)
i = 0,…，M — 1, ni 〜N(0, I), kw∣∣2 ≤ a}
where a > 0 is a constant; we denote the entry of xi that has the largest absolute value as ximax ,
and xi is defined as (Sgn(Xj) maχ(∣χj |, η))j=1. Due to batch normalization, We assume φ can be
bounded by a positive constant B. The other notations are defined similar to that in (9).
Consider training F2 by using tWo different datasets S and S0, and We denote the resulting models as:
f (x|S):= WTxM + πxMnM; xi+1 = xi + φ (UIxi) + γxi Θ ni,i = 0,…，M — 1. (11)
f (x|S ) := wTxM + πxMnM; xi+i = xi + φ (U2xi) + γxi Θ ni, i = 0,…，M — 1. (12)
Theorem 2. For f(x|S) and f(x|S0) that are defined in (11) and (12), respectively. Let λ ∈
(0,1), δ ∈ (0,1), and e > 0, if γ > (B∕η)p(2ɑM)∕(λe) and π > ap(2ɑM) ∕λe, where a =
log(1∕δ)/ ((1 — λ)e) + 1, then P[f (x|S) ∈ O] ≤ ee ∙ P[f (x|S0] ∈ O] + δ forany input x and any
subset O in the output space.
We provide the proof of Theorem 2 in Appendix B. Theorem 2 guarantees the privacy of the training
data given only black-box access to the model, i.e., the model Will output the prediction for any input
Without granting adversaries access to the model itself. In particular, We cannot infer Whether the
model is trained on S or S0 no matter hoW We query the model in a black-box fashion. We leave the
theoretical DP-guarantee for for Strategy II as a future Work.
Generalization Gap. Many Works have shoWn that overfitting in training ML models leads to
privacy leakage (Salem et al., 2018), and reducing overfitting can mitigate data privacy leakage (Shokri
et al., 2017; Yeom et al., 2018; Sablayrolles et al., 2018; Salem et al., 2018; Wu et al., 2019b). In
this part, We Will shoW that the residual perturbation (8) can reduce overfitting via computing the
Rademacher complexity. For simplicity, We consider binary classification problems. Suppose
SN ={xi, yi}iN=1 is draWn from X × Y ⊂ Rd × {-1, +1} With X and Y being the input data and label
3We add noise to the input for DP guarantee
4
Under review as a conference paper at ICLR 2021
spaces, respectively. Assume D is the underlying distribution of X × Y , which is unknown. Let
H ⊂ V be the hypothesis class of the ML model. We first recap on the definition of Rademacher
complexity.
Definition 2. (Barlett & Mendelson, 2002) Let H : X → R be the space of real-valued functions
on the space X. For a given sample S = {xι, x2, ∙一，XN } of size N, the empirical Rademacher
complexity of H is defined as
1N
RS(H) ：= -χτEσ [sup T σih(xi)],
N h∈H i=1
where σι,σ?,…,on are i.i.d. Rademacher random variables with P(σi = 1) = P(σi = —1) = 2.
Rademacher complexity is a tool to bound the generalization gap (Barlett & Mendelson, 2002). The
smaller the generalization gap is, the less overfitting the model is. For ∀xi ∈ Rd and constant c ≥ 0,
we consider the following two function classes:
F := {f (x, w) = wxp(T)|dx(t) = Ux(t)dt, x(0) = xi; w ∈ R1×d, U ∈ Rd×d}
with kwk2, kUk2 ≤ c},
G := {f (x, w) = E (wxp(T)) |dx(t) = Ux(t)dt + γx(t)	dB(t), x(0) = xi; w ∈ R1×d
with U ∈ Rd×d, kwk2, kUk2 ≤c},
where 0 < p < 1 takes the value such that xp is well defined on the whole Rd . γ > 0 is a
hyperparameter and U is a circulant matrix that corresponding to the convolution layer in DNs. B(t)
being the 1D Brownian motion. The function class F represents the continuous analogue of ResNet
without inner nonlinear activation functions, and G denotes F with the residual perturbation (8).
Theorem 3. Given the training set SN = {xi, yi}iN=1. We have RSN (G) < RSN (F).
We provide the proof of Theorem 3 in Appendix C, where we will also provide quantitative lower and
upper bounds of the above Rademacher complexities. Theorem 3 shows that residual perturbation (8)
can reduce the generalization error. We will numerically verify this generalization error reduction for
ResNet with residual perturbation in Section 4.
4 Experiments
In this section, we will numerically verify that 1) can residual perturbation protect data privacy; in
particular, membership privacy? 2) can the ensemble of ResNets with residual perturbation improve
the classification accuracy? 3) are skip connections crucial in residual perturbation for DL with data
privacy? 4) what is the advantage of the residual perturbation over the DPSGD? We focus on Strategy
I in this section, and we provide the results of Strategy II in Appendix D.
4.1	Preliminaries
Datasets. We consider both CIFAR10/CIFAR100 (Krizhevsky et al., 2009) and the Invasive Ductal
Carcinoma (IDC) datasets. Both CIFAR10 and CIFAR100 contain 60K 32 × 32 color images with
50K and 10K of them used for training and test, respectively. The IDC dataset is a breast cancer-
related benchmark dataset, which contains 277,524 patches of the size 50 × 50 with 198,738 labeled
negative (0) and 78,786 labeled positive (1). Figure 2 depicts a few patches from the IDC dataset. For
the IDC dataset, we follow Wu et al. (2019a) and split the whole dataset into training, validation, and
test set. The training set consists of 10,788 positive patches and 29,164 negative patches, and the test
set contains 11,595 positive patches and 31,825 negative patches. The remaining patches are used as
the validation set. For each dataset, we split its training set into Dshadow and Dtarget with the same
size. Furthermore, we split Dshadow into two halves with the same size and denote them as Dsthraaidnow
and Dsohuatdow , and split Dtarget by half into Dttararginet and Dtoaurtget . The purpose of this splitting of the
training set is for the membership inference attack, which will be discussed below.
Membership inference attack. To verify the efficiency of residual perturbation for protecting data
privacy, we consider the membership inference attack (Salem et al., 2018) in all the experiments
below. The membership attack proceeds as follows: 1) train the shadow model by using Dsthraaidnow ; 2)
apply the trained shadow model to predict all data points in Dshadow and obtain the corresponding
5
Under review as a conference paper at ICLR 2021
60.s.侬.6¾
Oooo
poLISaI £
Negative	Positive	Negative	Positive
Figure 2: Visualization of a few selected images from the IDC dataset.
4 6 9 Q
4.4.4.5
5 5 5 5
precision
o.o κ≡l
o.o o.o
0.0	0.0
0.0	0.0
0.0	0.2	0.3
gamma
60.s.侬.6¾
Oooo
poLISaI £
.64 3 Q
6.5 37.
9 9 9 8
recall
0.0	12.9
0.0	0.0
0.0	0.0
0.0	0.0
0.0	0.2	0.3
gamma
train and test accuracy AUC under different noises
100.0
0.563
©或 84.7	86.7	85.7 C
0.0	0.2	0.3	0.0	0.2	0.3
gamma	gamma
Figure 3: Performance of residual perturbation for En5ResNet8 with different noise coefficients
(γ) and membership inference attack thresholds on the IDC dataset. Residual perturbation can
significantly improve membership privacy and reduce the generalization gap. γ = 0 corresponding to
the baseline ResNet8. (Unit: %)
classification probabilities of belonging to each class. Then we take the top three classification
probabilities (or two in the case of binary classification) to form the feature vector for each data point.
A feature vector is tagged as 1 if the corresponding data point is in Dsthraaidnow , and 0 otherwise. Then
we train the attack model by leveraging all the labeled feature vectors; 3) train the target model by
using Dttararginet and obtain feature vector for each point in Dtarget . Finally, we leverage the attack
model to decide whether a data point is in Dttararginet .
Experimental settings. We consider En5ResNet8 (ensemble of 5 ResNet8 with residual perturba-
tion) and the standard ResNet8 as the target and shadow models. We use a multilayer perceptron
with a hidden layer of 64 nodes, followed by a softmax output function as the attack model, which is
adapted from (Salem et al., 2018). We apply the same settings as that used in (He et al., 2016) to
train the target and shadow models on the CIFAR10 and CIFAR100. For training models on the IDC
dataset, we run 100 epochs of SGD with the same setting as before except that we decay the learning
rate by 4 at the 20th, 40th, 60th, and 80th epoch, respectively. Moreover, we run 50 epochs of Adam
(Kingma & Ba, 2014) with a learning rate of 0.1 to train the attack model. For both IDC and CIFAR
datasets, we set π as half of γ in Strategy I, which simplifies hyperparameters calibration, and based
on our experiment it gives a good trade-off between privacy and accuracy.
Performance evaluations. We consider both classification accuracy and capability for protecting
membership privacy. The attack model is a binary classifier, which is to decide if a data point is in
the training set of the target model. For any x ∈ Dtarget , we apply the attack model to predict its
probability (p) of belonging to the training set of the target model. Given any fixed threshold t if
p ≥ t, we classify x as a member of the training set (positive sample), and if p < t, we conclude that
x is not in the training set (negative sample); so we can obtain different attack results with different
thresholds. Furthermore, we can plot the ROC curve(see details in subsection 4.5) of the attack model
and use the area under the ROC curve (AUC) as an evaluation of the membership inference attack.
The target model protects perfect membership privacy if the AUC is 0.5 (attack model performs
random guess), and the higher AUC is, the less private the target model is. Moreover, we use the
precision (the fraction of records inferred as members are indeed members of the training set) and
recall (the fraction of training set that is correctly inferred as members of the training set by the attack
model) to measure ResNets’ capability for protecting membership privacy.
4.2	Experiments on the IDC Dataset
In this subsection, we numerically verify that the residual perturbation in protecting data privacy while
retaining the classification accuracy on the IDC dataset. We select the En5ResNet8 as a benchmark
architecture, which has ResNet8 as its baseline architecture (the details of the neural architectures are
provided in Appendix F). As shown in Figure 3, we set four different thresholds to obtain different
attack results with three different noise coefficients (γ) when γ = 0 means the standard ResNet8
without residual perturbation. We also depict the ROC curve for this experiment in Figure 7 (c).
6
Under review as a conference paper at ICLR 2021
Table 1: Residual perturbation vs. DPSGD in training ResNet8 and EnResNet8 for the IDC
classification. Ensemble of ResNet8 with residual perturbation has higher test accuracy and protects
better membership privacy (smaller AUC).
ResNet8 (DPSGD)	EnIReSNet8	En5ResNet8
AUC	Training Acc	Test Acc	AUC	Training Acc	Test Acc	AUC	Training Acc	Test Acc
0.503	0.831	—	0.828	0.496	0.864 一	0.852	0.497	0.869 —	0.857
decision
train and test accurac
AUC under different noises
a,66侬.66
Oooo
poLISaI £
0.0	0.0	0.0
0.0	0.0	0.0
0.0 0.5 0.75 1.0
gamma
.6066..60.66
Oooo
poLISaI £
gamma
λM 66.9 69.3 67.5 60.6
0.0 0.5 0.75 1.0
1∞.0 96.5 86.7
0.757 0.644
gamma
0.0 0.5 0.75 1.0
gamma
Figure 4:	Performance of En5ResNet8 with residual perturbation using different noise coefficients
(Y) and membership inference attack threshold on CIFAR10. Residual perturbation can not only
enhance the membership privacy, but also improve the classification accuracy. Y = 0 corresponding
to the baseline ResNet8 without residual perturbation or model ensemble. (Unit: %)
60⅛6.6066∙
Oooo
poLISaIW
gamma
train and test accurac
λc⅛ 30.5 35.6 30.2 25.6
0.0 0.5 0.75 1.0
gamma
AUC under different noises
0.903
0.581 0.521 0.515
gamma
0.0 0.5 0.75 1.0
gamma
Figure 5:	Performance of En5ResNet8 with residual perturbation using different noise coefficients
(Y) and membership inference attack threshold on CIFAR100. Again, residual perturbation can
not only enhance the membership privacy, but also improve the classification accuracy. Y = 0
corresponding to the baseline ResNet8 without residual perturbation or model ensemble. (Unit: %)
En5ResNet is remarkably better in protecting the membership privacy, and as Y increases the model
becomes more resistant to the membership attack. Also, as the noise coefficient increases, the gap
between training and test accuracies becomes smaller, which resonates with Theorem 3. For instance,
when Y = 0.2 the AUC for the attack model is 0.495 and 0.563, respectively, for En5ResNetS and
ResNet8; the classification accuracy of En5ResNet8 and ResNet8 are 0.867 and 0.847, respectively.
4.2.1 RESIDUAL PERTURBATION VS. DPSGD
In this part, we compare the residual perturbation with the benchmark Tensorflow DPSGD module
(McMahan et al., 2018), and we calibrate the hyperparameters, including the initial learning rate (0.1)
which decays by a factor of 4 after every 20 epochs, noise multiplier (1.1), clipping threshold (1.0),
micro-batches (128), and epochs (100) 4 such that the resulting model gives the optimal trade-off
between membership privacy and classification accuracy. DPSGD is significantly more expensive
due to the requirement of computing the per-sample gradient. We compare the standard ResNet8
trained by DPSGD with En1ResNet8 and En5ResNet8 with residual perturbation (Y = 0.3). Table 1
lists the AUC of the attack model and training and test accuracies of the target model; we see that
residual perturbation can improve accuracy and protect better membership privacy.
4.3 Experiments on the CIFAR10/CIFAR100 Datasets
We further test the residual perturbation for ResNet8 and En5ResNet8 on the CIFAR10/CIFAR100
dataset. Figure 4 plots the performance of En5ResNet8 on the CIFAR10 dataset under the above four
different measures. Again, the ensemble of ResNets with residual perturbation is remarkably less
vulnerable to the membership inference attack; for instance, the AUC of the attack model for ResNet8
and En5ResNet8 (y = 0.75) is 0.757 and 0.573, respectively. Also, the classification accuracy of
En5ResNet8 (67.5%) is higher than that of ResNet8 (66.9%) for CIFAR10 classification. Figure 5
depicts the results of En5ResNet8 for CIFAR100 classification. These results confirm again that
residual perturbation can protect membership privacy and improve classification accuracy.
4https://github.com/tensorflow/PriVaCy/tree/master/tutorials
7
Under review as a conference paper at ICLR 2021
A 3 6
EnU θ-qluθsuφ
Ic
U
A
0.757	0.671	0.634
0.876	I 0.692	0.610
0.906	∣ 0.644	0.573
,O
75
S
gamma
A 3 6
EnU θ-qluθsuφ
train accuracy
100.0	99.1	96.1
100.0	99.1	94.2
100.0	96.5	K!3Π
0.0	0.5	0.75
gamma
EnU φ-qEΘSUφ
test accuracy		
66.9	70.2	67.5
I 70.8	70.3	I 68.7
67.9	69.3	I 67.5
0.0	0.5	0.75
gamma
5
Figure 6:	The performance of residual perturbation with different noise coefficients (γ) and different
number of models in the ensemble. The optimal privacy-utility tradeoff lies in the choice of these
two options. (Unit: %)
(a) CIFARi0(En5ResNet8)	(b) CIFARi00(En5ResNet8)	(c) IDC (En5ResNet8)
Figure 7:	ROC curves for different datasets. (ga: noise coefficient γ)
4.3.1	Effects of the Number of Models in the Ensemble
In this part, we consider the effects of the number of residual perturbed ResNets in the ensemble.
Figure 6 illustrates the performance of EnResNet8 for CIFAR10 classification measured in the AUC,
and training and test accuracy. These results show that tuning the noise coefficient and the number of
models in the ensemble is crucial to optimize the trade-off between accuracy and privacy.
4.4	On the Importance of Skip Connections
Residual perturbations theoretically relies on the irreversibility of the SDEs (3) and (7), and this
ansatz lies in the skip connections in the ResNet. We test both standard ResNet and the modified
ResNet without skip connections. For CIFAR10 classification, under the same noise coefficient
(γ = 0.75), the test accuracy is 0.675 for the En5ResNet8 (with skip connection); while the test
accuracy is 0.653 for the En5ResNet8 (without skip connection). Skip connections makes EnResNet
more resistant to noise injection which is indeed crucial for the success of residual perturbation for
protecting data privacy.
4.5	ROC Curves for Experiments on Different Datasets
The receiver operating characteristic (ROC) curve can be used to illustrate the classification ability of
a binary classifier. ROC curve is obtained by plotting the true positive rate against the false positive
rate at different thresholds. The true positive rate, also known as recall, is the fraction of the positive
set (all the positive samples) that is correctly inferred as a positive sample by the binary classifier. The
false positive rate can be calculated by 1-specificity, where specificity is the fraction of the negative
set (all the negative samples) that is correctly inferred as a negative sample by the binary classifier. In
our case, the attack model is a binary classifier. Data points in the training set of the target model are
tagged as positive samples, and data points out of the training set of the target model are tagged as
negative samples. Then we plot ROC curves for different datasets (as shown in Figure 7). These ROC
curves show that if γ is sufficiently large, the attack model’s prediction will be nearly a random guess.
4.6	Remark on the Privacy Budget
In the experiments above, we set the constants G and R to 30 for Strategy I. For classifying IDC with
ResNet8, the DP budget for Strategy I is ( = 1.1e5, δ = 1e - 5) and the DP-budget for DPSGD
is ( = 15.79, δ = 1e - 5). For classifying CIFAR10 with ResNet8, the DP budget for Strategy
I is ( = 3e5, δ = 1e - 5) and the DP-budget for DPSGD is ( = 22.33, δ = 1e - 5). Note that
theorem 1 offers a quite loose DP budget compared to DPSGD. There are several difficulties we need
to overcome to get tight DP bounds for Strategy I. Compared to DPSGD, it is significantly harder.
In particular, 1) the loss function of the nose injected ResNets is highly nonlinear and very complex
with respect to the weights, also the noise term appears in the loss function due to the noise injected
in each residual mapping. These together make the tight estimate very difficult. 2) In our proof, we
leveraged the framework of subsampled Renyi-DP (Wang et al., 2018) to find a feasible range of
8
Under review as a conference paper at ICLR 2021
noise variance parameter, and then convert to DP to get the value of γ for a given DP budget. This
procedure will significantly reduce the accuracy of the estimated γ. We leave the tight DP guarantee
as future work. In particular, how to reduce the accuracy of estimating due to the conversion between
Renyi-DP and DP.
5	Concluding Remarks
In this paper, we proposed residual perturbations, whose theoretical foundation lies in the theory of
stochastic differential equations, to protect data privacy for deep learning. Theoretically, we prove
that the residual perturbation can reduce the generalization gap with differential privacy guarantees.
Numerically, we have shown that residual perturbations are effective for protecting membership
privacy on some benchmark datasets. In particular, on the IDC benchmark, residual perturbations
protect better membership privacy than state-of-the-art differentially private stochastic gradient
descent and achieve remarkably better classification accuracy.
9
Under review as a conference paper at ICLR 2021
References
Invasive ductal carcinoma (idc) histology image dataset. URL http://www.andrewjanowczyk.com/
use-case-6-invasive-ductal-carcinoma-idc-segmentation/.
Martin Abadi, Andy Chu, Ian Goodfellow, H. McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep Learning with Differential Privacy. In 23rd ACM Conference on Computer and Communica-
tions Security (CCS 2016), 2016a.
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016b.
Mohammad Al-Rubaie and J Morris Chang. Reconstruction attacks against mobile-based continuous
authentication systems in the cloud. IEEE Transactions on Information Forensics and Security, 11
(12):2648-2663, 2016.
Ho Bae, Dahuin Jung, and Sungroh Yoon. Anomigan: Generative adversarial networks for anonymiz-
ing private medical data. arXiv preprint arXiv:1901.11313, 2019.
Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. Differential privacy has disparate
impact on model accuracy. In Advances in Neural Information Processing Systems, pp. 15453-
15462, 2019.
Peter L Barlett and Sahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Jounal of Machine Learning Research, 2002.
Marco Barreno, Blaine Nelson, Russell Sears, Anthony D Joseph, and J Doug Tygar. Can machine
learning be secure? In Proceedings of the 2006 ACM Symposium on Information, computer and
communications security, pp. 16-25, 2006.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473. IEEE, 2014.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk
minimization. Journal of Machine Learning Research, 12(Mar):1069-1109, 2011.
Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. arXiv preprint
arXiv:1905.02383, 2019.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.
Khaled El Emam and Fida Kamal Dankar. Protecting privacy using k-anonymity. Journal of the
American Medical Informatics Association, 15(5):627-637, 2008.
Wei Feng, Zheng Yan, Hengrun Zhang, Kai Zeng, Yu Xiao, and Y. Hou. A Survey on Security,
Privacy and Trust in Mobile Crowdsourcing. IEEE Internet of Things Journal, 2017.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model Inversion Attacks that Exploit Confi-
dence Information and Basic Countermeasures. In 22nd ACM SIGSAC Conference on Computer
and Communications Security (CCS 2015), 2015a.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence
information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on
Computer and Communications Security, pp. 1322-1333, 2015b.
Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart.
Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd
{USENIX} Security Symposium ({USENIX} Security 14), pp. 17-32, 2014.
Neil Zhenqiang Gong and Bin Liu. You are who you know and how you behave: Attribute inference
attacks via users’ social friends and behaviors. In 25th {USENIX} Security Symposium ({USENIX}
Security 16), pp. 979-995, 2016.
10
Under review as a conference paper at ICLR 2021
Neil Zhenqiang Gong and Bin Liu. Attribute inference attacks in online social networks. ACM
Transactions on Privacy and Security (TOPS), 21(1):1-30, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Ehsan Hesamifard, Hassan Takabi, Mehdi Ghasemi, and Rebecca N Wright. Privacy-preserving
machine learning as a service. Proceedings on Privacy Enhancing Technologies, 2018(3):123-142,
2018.
Desmond J Higham. An algorithmic introduction to numerical simulation of stochastic differential
equations. SIAM review, 43(3):525-546, 2001.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Fima C Klebaner. Introduction to stochastic calculus with applications. Springer, 2005.
Jakub Konecny, H Brendan McMahan, Felix X Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv
preprint arXiv:1610.05492, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Bai Li, Changyou Chen, Hao Liu, and Lawrence Carin. On connecting stochastic gradient mcmc and
differential privacy. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 557-566. PMLR, 2019.
Zhicong Liang, Bao Wang, Quanquan Gu, Stanely Osher, and Yuan Yao. Exploring private federated
learning with laplacian smoothing. arXiv preprint arXiv:2005.00218, 2020.
Yehuda Lindell and Benny Pinkas. Privacy preserving data mining. In Annual International
Cryptology Conference, pp. 36-54. Springer, 2000.
Xuanqing Liu, Tesi Xiao, SiSi, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde: Stabilizing
neural ode networks with stochastic noise. arXiv preprint arXiv:1906.02355, 2019.
Yun Liu, Krishna Gadepalli, Mohammad Norouzi, George E. Dahl, and Martin C. Stumpe. Detecting
Cancer Metastases on Gigapixel Pathology Images. arXiv:1703.02442, 2017.
Ledoux M and Talagrand. Probability in Banach spaces: Isoperimetry and processes. Springer, 2002.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov, Nicolas Papernot,
and Peter Kairouz. A general approach to adding differential privacy to iterative training procedures.
arXiv preprint arXiv:1812.06210, 2018.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations
Symposium (CSF), pp. 263-275. IEEE, 2017.
Nicolas Papernot, Martin Abadi, Lfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semisupervised
Knowledge Transfer for Deep Learning from Private Training Data. In 5th International Conference
on Learning Representation (ICLR 2017), 2017.
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, and Lfar Erlingsson. Scalable
Private Learning with PATE. In International Conference on Learning Representations (ICLR
2018), 2018.
Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Parametric noise injection: Trainable random-
ness to improve deep neural network robustness against adversarial attack. arXiv preprint
arXiv:1811.09310, 2018.
11
Under review as a conference paper at ICLR 2021
Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, and Herve Jegou. D\'ej\a vu: an
empirical evaluation of the memorization properties of convnets. arXiv preprint arXiv:1809.06396,
2018.
Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, and Michael Backes.
Ml-leaks: Model and data independent membership inference attacks and defenses on machine
learning models. arXiv preprint arXiv:1806.01246, 2018.
Reza Shokri and Vitaly Shmatikov. Privacy-Preserving Deep Learning. In 22nd ACM SIGSAC
Conference on Computer and Communications Security (CCS 2015), 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks
against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pp.
3-18.IEEE, 2017.
Latanya Sweeney. k-anonymity: A model for protecting privacy. International Journal of Uncertainty,
Fuzziness and Knowledge-Based Systems,10(05):557-570, 2002.
Florian Tramer, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine
learning models via prediction apis. In 25th {USENIX} Security Symposium ({USENIX} Security
16), pp. 601-618, 2016.
Bao Wang, Quanquan Gu, March Boedihardjo, Farzin Barekat, and Stanley J Osher. Dp-lssgd:
A stochastic optimization method to lift the utility in privacy-preserving erm. arXiv preprint
arXiv:1906.12056, 2019a.
Bao Wang, Zuoqiang Shi, and Stanley Osher. Resnets ensemble via the feynman-kac formalism to
improve natural and robust accuracies. In Advances in Neural Information Processing Systems, pp.
1655-1665, 2019b.
Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for free: Posterior sampling and
stochastic gradient monte carlo. In International Conference on Machine Learning, pp. 2493-2502,
2015.
Yu Xiang Wang, Borja Balle, and Shiva Kasiviswanathan. Subsampled R\’enyi Differential Privacy
and Analytical Moments Accountant. arXiv preprint arXiv:1808.00087, 2018.
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi. Beyond inferring
class representatives: User-level privacy leakage from federated learning. In IEEE INFOCOM
2019-IEEE Conference on Computer Communications, pp. 2512-2520. IEEE, 2019c.
Bingzhe Wu, Chaochao Chen, Shiwan Zhao, Cen Chen, Yuan Yao, Guangyu Sun, Li Wang, Xiaolu
Zhang, and Jun Zhou. Characterizing membership privacy in stochastic gradient langevin dynamics.
arXiv preprint arXiv:1910.02249, 2019a.
Bingzhe Wu, Shiwan Zhao, Guangyu Sun, Xiaolu Zhang, Zhong Su, Caihong Zeng, and Zhihong
Liu. P3sgd: Patient privacy preserving sgd for regularizing deep cnns in pathological image
classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2099-2108, 2019b.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning:
Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations
Symposium (CSF), pp. 268-282. IEEE, 2018.
Yuen, Man-Ching, King, Irwin, Leung, and Kwong-Sak. A Survey of Crowdsourcing Systems. In
Proceedings of the IEEE international conference on social computing (Socialcom 2011), 2011.
Xu Zheng, Zhipeng Cai, and Yingshu Li. Data linkage in smart internet of things systems: A
consideration from a privacy perspective. IEEE Communications Magazine, 56(9):55-61, 2018.
Ligeng Zhu, Zhijian Liu, and Song Han. Deep leakage from gradients. In Advances in Neural
Information Processing Systems, pp. 14747-14756, 2019.
12
Under review as a conference paper at ICLR 2021
The appendices are structured as follows. In Sections A, B, and C, we prove Theorems 1, 2, and
3, respectively. In Section D, we provide numerical results for the Strategy II. More experimental
details are provided in Section E. Finally, in Section F, we summarize the architecture of deep neural
networks used in our experiments.
Part
Appendices
Table of Contents
A Proof of Theorem 1	13
A.1 Renyi Differential Privacy........................................ 13
A.2 Proof of Theorem 1 ............................................... 14
B Proof of Theorem 2	15
C Proof of Theorem 3	15
C.1 Some Lemmas ...................................................... 15
C.2 The proof of Theorem 3 ........................................... 17
D Experiments on Strategy II	18
D.1	Forward and Backward Propagation Using (7) ...................... 18
D.2	Experiments on the IDC Dataset .................................. 18
D.3	Experiments on the CIFAR10/CIFAR100 Datasets .................... 18
D.4	ROC Curves for the Experiments on Different Datasets ............ 19
E More Experimental Details	19
F Architectures of the Used DNs	20
A	Proof of Theorem 1
A.1 RENYI Differential Privacy
We will use the notion of Renyi differential privacy (RDP) to prove the differential privacy (DP)
guarantees for the proposed residual perturbations. First, let’s review the definition and several results
of the Renyi differential privacy (Mironov, 2017).
Definition 3. (Mironov, 2017) (Renyi divergence) For any two probability distributions P and Q
defined over the distribution D, the Renyi divergence of order a > 1 is
Da(P ||Q) = ɪlog Ex 〜Q (PlQf；
α-1
Definition 4. (Mironov, 2017) ((α, )-RDP) A randomized mechanism M : D → R is said to have
e-Renyi differential privacy of order a or (α, e) -RDPfor short, if for any adjacent S, S ∈ D that
differ by only one entry, it holds that
Da(M(S)||M(Sj ≤ ε.
Lemma 1. (Mironov, 2017) Let f : D → R1 be (α, e1)-RDP and g : R1 × D → R be (α, e)-RDP,
then the mechanism defined as (X, Y), where X 〜f (D) and Y 〜g(X, D), satisfies (α, eι + e2)-
RDP.
Lemma 2. (Mironov, 2017) (From RDP to (e, δ)-DP) If f is an (α, e)-RDP mechanism, then it also
satisfies (e 一 (log δ)∕(α 一 1), δ)-differential privacy for any 0 < δ < L
13
Under review as a conference paper at ICLR 2021
Lemma 3. (Mironov, 2017) (Post-processing lemma) Let M : D → R be a randomized algorithm
that is (α, C)-RDP and let f : R → R be an arbitrary randomized mapping. Then f (M(∙)) : D →
R0 is (α, )-RDP
A.2 Proof of Theorem 1
In this subsection, we will give a proof of Theorem 1, i.e., DP-guarantee for the Strategy I.
Proof. We will prove Theorem 1 by mathematical induction. Consider two adjacent datasets
S = {xι,…,XN-ι, XN}, S 0 = {xι,∙∙∙, XN-ι, XN} that differ by one entry. For the first
residual mapping, it is easy to check that when Y ≥ RYlahp We have Da(XNl∣XN) =
a∣∣XN - XNk2∕(2γ2) < εp. For the remaining residual mappings, we denote the response of
the ith residual mapping, for any two input data XN and XN, as XN, XN, respectively. Based on our
assumpution, we have
xN + φ(UiXN) 〜N(μN,i, σN,i)
XiN + Φ(uiXN)〜N(μN,i,σN,i),
where μN,i and μN,i are both bounded by the constant G. If Da(XNl∣XN) ≤ ep/i, according the
post-processing lemma (Lemma 3), we have
Da(XN + φ(UixN川XN + Φ(UiXN)) = ak"N： - μN,ik2 ≤ ep/i,
2σN,i
so when Y > ,2aG2∕ep, we further have
Da(XiN + Φ(UiXN) + γn∣∣XN + Φ(UiXN) + Yn)=。2 -+的”2 ≤ i+1，
which implies that Da(XN+1 ||X。)< i+pT∙ On the other hand, note that
V5+1 'Iχi+1 = l(XN-1 + φ (Ui+1Xi+1)) φ (Ui+1Xi+1)(Xi+1)T
V5+1 'Iχi+1 = l (XN-1 + φ (Ui+1Xi+1)) φ (Ui+1Xi+1)(Xi+1)T.
Leveraging the post-processing lemma (Lemma 3) again, we get
Da(VUi+1 '∖χiN+1 ||VUi+1 '∖^i+1 ) < i+l.
Let Bt be the index set with |Bt | = b, and we update Ui as following:
Ut+1∣S = Ut - ab X Vut+ι '(Xj+1, Ut+1)
b j ∈Bt
Ut+1∣S0 = Ut - ab X Vut+1 '(Xj+1, Ut+1),
b j ∈Bt
where Uit+1 is the weights updated after the tth training iterations. When N ∈/ Bt, it’s obviously
that Da(Uit++11∖S∖∖Uit++11∖S0) = 0; when N ∈ Bt, the equations which we use to update Ui can be
rewritten as
Ut+1∣S = Ut - a(1∕b)	X	Vui+1 '(Xj+1, Ut+1) - a(1∕b)Vui+ι'(x-1, Ut+1)
j∈Bt-{N}
Ut+1∣S0 = Ut - a(1∕b)	X	Vut+ι'(Xj+1, Ut+1) - a(1∕b)Vut+ι'(Xi+1, Ut+1).
j∈Bt-{N}
According to the post-processing lemma (Lemma 3), we have Da(Uit++11∖S∖∖Uit++11∖S0) ≤ ep∕(i + 1).
Because there are only (Tb)∕N steps where we use the information of XN and XN. Replace ep
by (Tbep)∕N and use composition theorem we can get after T steps the output UiT+1 satisfies
(a, ep∕(i + 1))-RDP and w satisfies (a, ep ∕M)-RDP. By Lemma 2, we can easily establish the
DP-guarantee for Strategy I, as stated in Theorem 1.	□
14
Under review as a conference paper at ICLR 2021
B Proof of Theorem 2
In this section, we will provide a proof for Theorem 2.
Proof. Let φ = BN (ψ), where BN is batch normalization operation and ψ is an activation function.
Because of the property of batch normalization, we assume that φ can be bounded by a positive
constant B . To show that the model (9) guarantees training data privacy given only black-box access
to the model. Consider training the model (9) with two different datasets S and S0 , and we denote the
resulting model as f (∙∣S) and f (∙∣ S0) ,respectively. In the following, We prove that with appropriate
choices of Y and π, Dα(f (x∣S)∣∣f (x|S )) < e? for any input x.
First, we consider the convolution layers, and let Conv(x)i be the ith entry of the vectorized Conv(x).
Then we have Conv(x)i = Xi + φ(Ux)i + Yxin^ For any two different training datasets, we denote
Conv(X)i|S = xi + φ(Uιx)i + γXini 〜N(Xi + φ(Uιx)i, γ2X2),
and
Conv(X)i|S = Xi + φ(U2X)i + YXini 〜N(Xi + φ(U2X)i, γ2X2).
Therefore, if γ > (B∕η)'(2αM"4), we have
Da (N (Xi + Φ (UιX)i, γ2X2) ||N (Xi + Φ (U2X)i, γ2X2))
2
2
≤
2Y2 η2
α(φ(U1X)i - φ(U2X)i)2 ≤ 4αL2B2kX∣∣
-2Y2X2
2αB2
≤ -ɪɪ ≤ ep ∕M.
Y 2 η2
Furthermore, Y > (F∕η)'(2ɑM”(e、) guarantees (α, ep/M)-RDP for every convolution layer. For
the last fully connected layer, if π > a,(2αM) /ep, we have
Da (N (WTX,∏2X2) ||N (WTX,∏2X2))
α (WTX - WTX)2	4aa2∣∣X∣∣2
=2∏⅛≤ 2∏2∣∣X∣∣2
≤ 2αa2 ≤ ep/M,
π2
i.e., π > a,(2αM) /ep guarantees that the fully connected layer to be (α, ep/M)-RDP. According
to Lemma 1, we have (α, ep)-RDP guarantee for the ResNet of M residual mappings if Y >
(Lb/n) P(2adM)/4) and π > a,(2aM) /ep. Let λ ∈ (0,1), for any given (ep, δ) pair, if
ep ≤ λe and -(log δ"(α - 1), δ) ≤ (1 - λ)e, then we have get the (e, δ)-DP guarantee for the
ResNet with M residual mapping using the residual perturbation Strategy II.	□
C Proof of Theorem 3
In this section, we will proof that the residual perturbation (8) can reduce the generalization error via
computing the Rademacher complexity. Let us first recap on some related lemmas on the stochastic
differential equation and the Rademacher complexity.
C.1 Some Lemmas
Let ` : V × Y → [0, B] be the loss function. Here we assume ` is bounded and B is a positive
constant. In addition, we denote the function class 'h = {(x, y) → '(h(X), y) : h ∈ H, (x, y) ∈
X × Y }. The goal of the learning problem is to find h ∈ H such that the population risk R(h) =
E(χ,y)∈D ['(h(x), y)] is minimized. The gap between population risk and empirical risk RSN (h)=
(1/N) PN=I '(h(xi), yi) is known as the generalization error. We have the following lemma and
theorem to connect the population and empirical risks via Rademacher complexity.
15
Under review as a conference paper at ICLR 2021
Lemma 4. (Ledoux-Talagrand inequality) (M & Talagrand, 2002)Let H be a bounded real valued
function space and let φ : R → R be a Lipschitz with constant L and φ(0) = 0. Then we have
1 Eσ
n
n
sup	σiφ (h (xi))
h∈H i=1
≤ LEσ
n
n
sup	σih (xi)
h∈H i=1
Lemma 5. (Barlett & Mendelson, 2002)Let SN = {(x1,y1),…，(xn, yN)} be samples chosen
i.i.d. according to the distribution D. If the loss function ` is bounded by B > 0. Then for any
δ ∈ (0, 1), with probability at least 1 - δ, the following holds for all h ∈ H,
R (h) ≤ RSN (h) + 2BRsn ('h) + 3BP(log(2∕δ)∕(2N).
In addition, according to the Ledoux-Talagrand inequality and assume loss function is L-lipschitz,
we have
RSN ('h) ≤ LRSN (H).
So the population risk can be bounded by the empirical risk and Rademacher complexity of the
function class H. Because we can’t minimize the population risk directly, we can minimize it
indirectly by minimizing the empirical risk and Rademacher complexity of the function class H.
Next, we will further discuss Rademacher complexity of the function class H. We first introduce
several lemmas below.
Lemma 6. (Klebaner, 2005) For any given matrix U, the solution to the equation
dx(t) = Ux(t)dt
(x(0) = X
has the following expression
x(t) = exp(Ut)X,
Also, we can write the solution to the following equation
) = Uy(t)dt + γy(t)dB(t)
=X
as
y(t) = exp(Ut - 2γ2tI + γB(t)I)X.
Obviously, we have E[y(t)] = X(t).
Lemma 7. A matrix C ∈ Rd×d is circulant, if there exists real number aι,…,a& such that
	a1	a2	. . .	ad
C =	ad	aι	'.-''- ...	...	...a2 . a2	. .	ad	a1
For any circulant matrix, we have the following eigen-decomposition
where
ΨHCΨ	diag(λ1 , .	. . , λd),
/ 1	1	..	1 ʌ
√dΨ =	1	m1	. .	.md-1
... 1	...	.. md-1 m1	. .	.	... . mdd--11
and miS are the roots ofunity and λi = aι + a2m% + •…+ adm：—1
16
Under review as a conference paper at ICLR 2021
C.2 The proof of Theorem 3
Proof. By the definition of Rademacher complexity (Def. 2), we have
N
RSN (F) = (1/N) Eσ sup X σiwxip(T)
f∈F i=1
N
(c/N) Eσ	sup k Xσixip (T) k2
kUk2≤c i=1
Let ui = Ψxip and denote the jth element of ui as ui,j . Then by lemma 7, we have
RSN (F) = (c/N) Eσ sup	Xσiσjhxip(T),xjp(T)i
kUk2≤c	i,j
N
(c/N) Eσ sup (kΨH X σi (Ui,1 exp(λ1Tp), ∙ ∙ ∙ ,ui,d exp(λdTp))τ∣∣2)
lλil≤c	i=l
dN
(c/N) Eσ sup {	σiui,j exp (λjTp)
lλil≤c j=1 Li=1	.
dN
}1/2 = (c/N) exp(cT p)Eσ {X Xσiui,j
j=1 i=1
2
}1/2
NN
(c/N)exp(cTp) Eσk X σiuik2 = (c/N) exp (cTp) Eσ∣∣Ψ X σiXp∣2
i=1	i=1
N
(c/N) exp(cTp) Eσk X σixpk2
i=1
Note that E (wxip(T)) = wE (xip (T)) and according to Lemma 6, similar to proof for the function
class F we have
RSN (G) = (c/N) Eσ sup	XσiσjhExip(T),Exjp(T)i
kUk2≤c	i,j
N
=(c/N)Eσ SuP (∣∣Ψh Xσi(ui,ι exp(λιTp - p(1 — p)Y2T/2),
∣λi∣≤c	M
∙∙∙ ,Ui,d exp(λdTp — p(1 — p)γ 2T/2))T ∣∣2)
(c/N) Eσ sup {	σiui,j exp (λjTp — p(1 — P)YT/2 }1/2
∣λi∣≤ J J
c j=1
i=1
(c/N) exp(cTp - p(1 - p)Y2T/2)E。{£ E σi Uij
j=1
N
i=1
2
}1/2
(c/N)exp (CTp — p(1 — p)γ2T/2)Eσ k £二四。2
i=1
N
(c/N) exp (CTp — p(1 — p)γ2T/2)Eσ∣∣Ψ X σixp∣2
i=1
N
(c/N)exp (CTp — p(1 — p)γ2T/2)Eσk XσiXp∣∣2 < RSN (F)
i=1
Therefore, we have completed the proof for the fact that the ensemble of Gaussian noise injected
ResNets can reduce generalization error compared to the standard ResNet.	口
d
N
d
N
2
17
Under review as a conference paper at ICLR 2021
D Experiments on Strategy II
D. 1 Forward and Backward Propagation Using (7)
Figure 8 plots the forward and backward propagation of the ICLR logo using the SDE model (7).
Again, we cannot simply use the backward Euler-Maruyama discretization to reverse the features
generated by the propagating through the forward Euler-Maruyama discretization.
(a) x(1) (SDE)	(b) X(0) (SDE )
Figure 8:	Illustrations of the forward and backward propagation of the training data using the SDE
model (7). (a) is the features of the original image generated by the forward propagation using SDE;
(b) is the recovered images by reverse-engineering the features shown in (a).
D.2 Experiments on the IDC Dataset
In this subsection, we consider the performance of the second residual perturbation in protecting
membership privacy while retaining the classification accuracy on the IDC dataset. We use the same
ResNet models as that used for the first residual perturbation. We list the results in Fig. 9, these
results confirm that the residual perturbation (8) can effectively protect data privacy and maintain or
even improve the classification accuracy. In addition, we depict the ROC curve for this experiment in
Figure 12 (c). We note that, as the noise coefficient increases, the gap between training and testing
accuracies narrows, which is consistent with Theorem 3.
60∙s.⅛›∙g∙
Oooo
P-OqSaJW
precision
I o.o
I 0.0
I o.o o.o
0.0	0.0
0.0	0.5	1.0
noise coefficient
recall
60∙s⅛2∙a
Oooo
P-OqSaJW
0.0	0.5	1.0
noise coefficient
train and test accurac
0.0	0.5	1.0
noise coefficient
AUC under different noises
0.563
0.493 0.500
0.0	0.5	1.0
noise coefficient
Figure 9:	Performance of residual perturbation (8) for En5ResNet8 with different noise coefficients
(γ) and membership inference attack thresholds on the IDC dataset. Residual perturbation can
significantly improve membership privacy and reduce the generalization gap. γ = 0 corresponding to
the baseline ResNet8. (Unit: %)
D.2.1 Residual Perturbation vs. Differentially Private Stochastic Gradient
Descent
We have shown that the first residual perturbation (6) outperforms the DPSGD in protecting mem-
bership privacy and improving classification accuracy. In this part, we further show that the second
residual perturbation (8) also outperforms the benchmark DPSGD with the above settings. Table 2
lists the AUC of the attack model and training & test accuracy of the target model; we see that
the second residual perturbation can also improve the classification accuracy and protecting better
membership privacy.
D.3 Experiments on the CIFAR 1 0/CIFAR 1 00 Datasets
In this subsection, we will test the second residual perturbation (8) on the CIFAR10/CIFAR100
datasets with the same model using the same settings as before. Figure 10 plots the performance of
En5ResNet8 on the CIFAR10 dataset. These results show that the ensemble of ResNets with residual
18
Under review as a conference paper at ICLR 2021
Table 2: Residual perturbation (8) vs. DPSGD in training ResNet8 for the IDC dataset classification.
Ensemble of ResNet8 with residual perturbation is more accurate for classification (higher test acc)
and protects better membership privacy (smaller AUC).
ResNet8 (DPSGD)	EnIReSNet8	En5ResNet8
AUC	Training Acc	Test Acc	AUC	Training Acc	Test Acc	AUC	Training Acc	Test Acc
0.503	0.831	0.828	0.509	0.880	0.868	0.500	0.895	0.872
perturbation (8) is significantly more robust to the membership inference attack. For instance, the
AUC of the attack model for ResNet8 and En5ResNet8 (Y = 2.0) is 0.757 and 0.526, respectively.
Also, the classification accuracy of En5ResNet8 (Y = 2.0) is higher than that of ResNet8, and
their accuracy is 71.2% and 66.9% for CIFAR10 classification. Figure 11 shows the results of
En5ResNet8 for CIFAR100 classification. These results confirm that residual perturbation (8) can
protect membership privacy and improve classification accuracy once more.
P-OqSEl
PreCiSjOn
υ	o.o o.o
70.1 67.6
网E!:泪 0.0 0.0 0.0
0.0 0.5 1.0 1.5 2.0
noise ∞efficient
ɪeeall
25.6 0.0 0.0
0.0 0.0 0.0
0.0 0.0 0.0
0.0 0.5 1.0 1.5 2.0
noise coefficient
P-OqS①」£
train and test accurac
tC'C
0 66.9 72.7 74.9 72.8 71.2
0.0 0.5 1.0 1.5 2.0
noise coefficient
AUG under different noises
100.0100.0 91.7 85.7
0.757 0.747
0.566 0.536 0-526
0.0 0.5 1.0 1.5 2.0
noise coefficient
Figure 10:	Performance of En5ResNet8 with residual perturbation (8) using different noise co-
efficients (Y) and membership inference attack threshold on CIFAR10. Residual perturbation (8)
can not only enhance the membership privacy, but also improve the classification accuracy. Y = 0
corresponding to the baseline ResNet8 without residual perturbation or model ensemble. (Unit: %)
a66,侬66∙
Oooo
poLIS 8」£
0.0 0.5 1.0 1.5 2.0
noise ∞efficient
P-OqSEl
Figure 11: Performance of En5ResNet8 with residual perturbation (8) using different noise coeffi-
cients (Y) and membership inference attack threshold on CIFAR100. Again, residual perturbation (8)
can not only enhance the membership privacy, but also improve the classification accuracy. Y = 0
corresponding to the baseline ResNet8 without residual perturbation or model ensemble. (Unit: %)
D.4 ROC Curves for THE Experiments on Different Datasets
Figure 12 plots the ROC curves for the experiments on the different datasets with different models
using the second residual perturbation strategy. These ROC curves again show that if Y is sufficiently
large, the attack model,s prediction will be nearly a random guess.
0 8 6 4 2 0
1.SS0.SS
ω? ω>-≡sod ən上
∏c=0.0
∏c=0.5
∏c=1.0
0 8 6 4 2 0
1.0.s0.SS
ω? ω>-≡sod ən上
----πc=0.0
nc=0.5
nc=1.0
∏c=0.0
∏c=0.5
πc=1.0
086420
1.0.s0.SS
ω? ω>-≡sod ən上
(a) CIFAR10(En5ResNet8)	(b) CIFAR100(En5ResNet8)	(c) IDC (En5ResNet8)
Figure 12: ROC curves for different datasets. (nc: noise coefficient)
E More Experimental Details
We give the detailed construction of the velocity field F(x(t), W(t)) in (3) and (7) that used to
generate Figs. 1 and 8 in Algorithm 1.
19
Under review as a conference paper at ICLR 2021
Algorithm 1 The expression of F(x(t), W(t))
Input: image=x(t); rows, cols, channels = image.shape.
Output: F (x(t), W(t))=dirtyimage.
for k in range(channels) do
for i in range(rows): do
for j in range(cols): do
offset(j) =int([j + 50.0cos(2πi∕180)]%cols);
offset(i) =int([i + 50.0sin(2πi∕180)]%row);
dirtyimage[i, j, k] = image[(i + offset(j))%rows, (j + offset(i))%cols, k];
return dirtyimage
F	Architectures of the Used DNs
Figure 13 shows the architectures of ResNets used in this paper, and we plot basic blocks in Fig. 14.
3*3 Conv, 16
Basic Blockl
Basic Block2
Basic Block3
BN+ReLU
avgpool
PreactResNetS
Figure 13: Architectures of the ResNet8 used in our experiments.
20
Under review as a conference paper at ICLR 2021
(a) Basic Block1
(b) Basic Block2
(c) Basic Block3
Figure 14: Architectures of the basic building block of ResNets studied in this paper.
21