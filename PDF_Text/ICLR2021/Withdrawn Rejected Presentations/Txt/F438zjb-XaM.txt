Under review as a conference paper at ICLR 2021
Crowdsourced Phrase-Based Tokenization for
Low-Resourced Neural Machine Translation: The
case of Fon Language
Anonymous authors
Paper under double-blind review
Ab stract
Building effective neural machine translation (NMT) models for very low-resourced and
morphologically rich African indigenous languages is an open challenge. Besides the
issue of finding available resources for them, a lot of work is put into preprocessing and
tokenization. Recent studies have shown that standard tokenization methods do not always
adequately deal with the grammatical, diacritical, and tonal properties of some African
languages. That, coupled with the extremely low availability of training samples, hinders
the production of reliable NMT models. In this paper, using Fon language as a case
study, we revisit standard tokenization methods and introduce Word-Expressions-Based
(WEB) tokenization, a human-involved super-words tokenization strategy to create a better
representative vocabulary for training. Furthermore, we compare our tokenization strategy
to others on the Fon-French and French-Fon translation tasks.
1 Introduction
We would like to start by sharing with you this Fon sentence: « meta meta WE zinwð hen wa aligbo me ».
How would you tokenize this? What happens if we implement the standard method of splitting the sentence
into its word elements (either using the space delimiter or using subword units)?
mɛta mɛta Wg zinw6 hɛn wa aligbo mɛ
mɛta	mɛta	Wg zinw6	hɛn	Wa aligbo	mɛ
Well We did that and discovered that a translation (to French) model, trained on sentences
split this way, gave a literal translation of «chaque singe est entr6 dans la vie avec sa tete, son destin
(English: each monkey entered the stage of life with its head, its destiny)» for the above Fon sentence. But
we are not talking about a monkey here ®. It is a metaphor and so some of the words should be taken
collectively as phrases. Using a phrase-based tokenizer, we got the following:
mɛta mɛta Wg zinw6 hɛn wa aligbo mɛ
mɛta	mɛta W£ zinw6	hɛn wa	aligbo mɛ
1
Under review as a conference paper at ICLR 2021
A native speaker looking at some of these grouped phrases will quickly point out that some of the grouped
phrases are wrong. Probably the phrase-based model could not effectively learn the phrases due to the low data
it was trained on? Also, we got a translation of «singe chaque vient au monde dans vie avec tete et destin
(English: monkey each comes into world in life with head and fate)» . However, this translation is still not
correct. The expression actually means «Every human being is born with chances» . Another interpretation
would be that we must be open to changes, and constantly be learning to take advantages of each situation in
life ©.
One illustrative example, which we encourage the reader to try, is to go to Google Translate and try translating
«it costs an arm and a leg» to your language (native language or a language you understand). For the 20
languages we tried, all the translation results were wrong: literal and not fully conveying the true (some
would say phrasal) expression or meaning. The expression «it costs an arm and a leg», just means «it is
expensive》. Now imagine a language with a sentence structure largely made up of such expressions - that is
Fon敏
Tokenization is generally viewed as a solved problem. Yet, in practice, we often encounter difficulties
in using standard tokenizers for NMT tasks, as shown above with Fon. This may be because of special
tokenization needs for particular domains (like medicine (He & Kayaalp, 2006; Cruz Diaz & Mana L6pez,
2015)), or languages. Fon, one of the five classes of the Gbe language clusters (Aja, Ewe, Fon, Gen, and
Phla-Phera according to (Capo, 2010)), is spoken by approximately 1.7 million people located in southwestern
Nigeria, Benin, Togo, and southeastern Ghana. There exists approximately 53 different dialects of Fon spoken
throughout Benin. Fon has complex grammar and syntax, is very tonal and diacritics are highly influential
(Dossou & Emezue, 2020). Despite being spoken by 1.7 million speakers, Joshi et al. (2020) have categorized
Fon as «left behind» or «understudied» in NLP. This poses a challenge when using standard tokenization
methods.
Given that most Fon sentences (and by extension most African languages) are like the sentence example given
above (or the combination of such expressions), there is a need to re-visit tokenization of such languages.
In this paper, using Fon in our experiment, we examine standard tokenization methods, and introduce the
Word-Expressions-Based (WEB) tokenization. Furthermore, we test our tokenization strategy on the Fon-
French and French-Fon translation tasks. Our main contributions are the dataset, our analysis and the proposal
of WEB for extremely low-resourced African languages (ALRLs). The dataset, models and codes will be
open-sourced on our Github page.
2 Background and Motivation
Modern NMT models usually require large amount of parallel data in order to effectively learn the representa-
tions of morphologically rich source and target languages. While proposed solutions, such as transfer-learning
from a high-resource language (HRL) to the low-resource language (LRL) (Gu et al., 2018; Renduchintala
et al., 2018; Karakanta et al., 2018), and using monolingual data (Sennrich et al., 2016a; Zhang & Zong,
2016; Burlot & Yvon, 2018; Hoang et al., 2018), have proved effective, they are still not able to produce
better translation results for most ALRLs. Standard tokenization methods, like Subword Units (SU) (Sennrich
et al., 2015), inspired by the byte-pair-encoding (BPE) (Gage, 1994), have greatly improved current NMT
systems. However, studies have shown that BPE does not always boost performance of NMT systems for
analytical languages(Abbott & Martinus, 2018). Ngo et al. (2019) show that when morphological differences
exist between source and target languages, SU does not significantly improve results. Therefore, there is a
great need to revisit NMT with a focus on low-resourced, morphologically complex languages like Fon. This
may involve taking a look at how to adapt standard NMT strategies to these languages.
2
Under review as a conference paper at ICLR 2021
3	Tokenization Strategies and their Challenges for Fon
In this section, we briefly discuss the standard tokenization strategies employed in NMT, as well as challenges
faced while applying them to Fon.
Word-Based tokenization (WB) consists of splitting sentences into words, according to a delimiter. We'll
show the limits of this method using this Fon expression: «un Og ganji» . «un» on its own is an interjection,
to express an emotion of surprise or astonishment. But «un tfo» already means " am", "I am at" or "
have" depending on the context in which it is used. The whole expression, «un Og ganji» , could mean "I
am fine" or "I am okay".
Phrase-Based tokenization (PhB) encodes phrases (group of words) as atomic units, instead of words.
As a result, models trained on PhB have the ability to learn and interpret language-specific phrases (noun,
verbal and prepositional phrases), making it better than WB for Fon language. However, due to the low-
resourcedness of the language and the randomness of PhB alignments, some extracted pairs are not always
contextually faultless. For example, the computer alignment gave respectively [zen, une (a, an, one)] and
[azɔn, la (the)] , instead of [zen, une marmite (a pot)] and [azɔn, la maladie (the disease)].
Encoding with SU has made great headway in NMT, especially due to its ability to effectively encode
rare out-of-vocabulary words (Sennrich et al., 2016b). MaChgCek et al. (2018), in analyzing the word
segmentation for NMT, reported that the common property of BPE and SU relies on the distribution of
character sequences, but disregards any morphological properties of the languages in question. Apart from
rule-based tokenization, there are machine learning approaches to tokenization as well, which unfortunately
require a substantial amount of training samples (both original and tokenized versions of the same texts)
(Riley, 1989; Mikheev, 2000; Jurish & Wurzner, 2013). To the best of our knowledge, there is no known
language-specific tokenization proposed for ALRLs, although there have been a number of works on adapting
NMT specifically to them (like (Orife et al., 2020; van Biljon et al., 2020; Vaswani et al., 2017), to mention
but a few).
4	Word-Expressions-Based tokenization (WEB)
WEB involves aligning and extracting meaningful expressions based on linguistic components of Fon
(phonemes, morphemes, lexemes, syntax, and context). This requires the assistance of Fon-French native
speakers. Some examples of good alignments are
nanc6 -→ maman (mum)
ku<fo jigb6zan -→ joyeux anniversaire (Happy Birthday)
X---------------{----------------}
nanc6 ViVE -→ maman Cherie (dear mum)
'-----{z------}
tfoji(fi(fe (fo wutu Ce a nɔnvi Ce
、
z
It is important to note that WEB is not a human-in-the-loop process, because it doesn’t require human
intervention to run. The human intervention occurs while cleaning and preprocessing the dataset. Although
not perfect yet, we describe our algorithm as a recursive search algorithm, which looks and finds the most
3
Under review as a conference paper at ICLR 2021
optimal combination of words and expressions which will produce a better translation for a source sentence.
The following algorithm was designed to encode and decode sentences using the established vocabularies:
1.	Run through the vocabulary and output a list L of all possible word combinations of words and expressions
appearing in the sentence S.
2.	Important principle in Fon: higher word orders = more precise and meaningful expressions. Using this principle,
for each element (word or expression), w ∈ L,
(a)	Check if there exists a higher word order, v ∈ L, such that w ( v.
(b)	If 2a is true, discard w, else keep w.
3.	The output is a list L of optimal expressions that from the initial L, making UP the initial sentence S.
4.	Add <start> and <end> taggers respectively at the beginning and the end of every element W (word or expression)
^
∈ L.
5.	Encode every W (word or expression) ∈ L
We argue that WEB scales well because it does not require any linguistic annotations but knowledge and
intuitions from bilinguals, meaning, we can crowdsource those phrases. We want to state clearly, in order to
avoid any confusion, that WEB is another version of PhB, involving human evaluation. For our study, it took
a group of 8 people, all bilinguals speaking Fon and French, and 30 days to align and extract meaningful
sentences manually. No preliminary trainings have been done with the annotators, given the fact that they
are in majority linguists and natives of the Fon language. This made the step of sentences splitting into
expressions, more natural, reliable and faster.
5	The Fon-French Dataset: Data Collection, Cleaning and expansion
PROCESSES
As our goal is to create a reliable translation system to be used by the modern Fon-speaking community, we
set out to gather more data on daily conversations domain for this study. Thanks to many collaborations with
Fon-French bilinguals, journalists and linguists, we gathered daily citations, proverbs and sentences with their
French translations. After the collection’s stage, we obtained a dataset of 8074 pairs of Fon-French sentences.
The cleaning process, which involved the Fon-French bilinguals, mainly consisted of analyzing the contextual
meanings of the Fon sentences, and checking the quality of the French translations. In many cases, where the
French translations were really bad, we made significant corrections.
Another major observation was the presence of many long and complex sentences. That’s where the
idea of expanding the dataset came from: we proceeded to split, when possible, Fon sentences into short,
independent, and meaningful expressions (expression of 1-6 words), and accordingly add their respective
French translations. At the end of these processes, we obtained our final dataset of 25,383 pairs of Fon-French
sentences. The experiments, described in this paper, were conducted using the final dataset.
We strongly believe that involving the Fon-French bilinguals into the cleaning process, greatly improved
the quality of the dataset. In fact, many initial translation errors were disregarded by standard, rule-based
tokenization (like WB, PhB and SU) and cleaning techniques1. However, with the help of the intuitive
or natural language knowledge of the Fon-French bilinguals, bunch of those errors have been fixed. This
highlights, the importance of having native speakers of ALRLs to clean and review the dataset, during the
initial stages of its compilation.
1Using Python Regex and String packages (https://docs.python.org/3/library/re.html) together with NLTK preprocess-
ing library (https://www.nltk.org/)
4
Under review as a conference paper at ICLR 2021
6	Methodology, Results and Conclusion
In this section, we describe the implementation of WB, PhB, SU, WEB and we compare the results of our
NMT model trained on them for our analysis.
6.1	Creation of vocabularies for WB, PhB, SU and WEB
For WB, we split the sentences according to the standard ’space’ delimiter, using the TensorFlow-Keras text
tokenizer2, getting a vocabulary of 7,845 and 8,756 Fon and French tokens (words) respectively.
For PhB, we use the IBM1 model from nltk.translate.api module3 to align and extract all possible pairs of
sentences. Our main observation was that, some pairs generated were either not meaningful or not maching,
but we didn’t try to rearrange them in order to see how well the generated pairs, without human intervention,
would affect the translation quality. In so doing, we got a vocabulary of 10,576 and 11,724 Fon and French
tokens respectively (word and expressions).
To implement SU , we used TensorFlow’s SubwordTextEncoder4 and built a vocabulary of 7,952 and 8,116
Fon and French tokens (words and subwords) respectively.
To implement WEB, we considered unique expressions as atomic units. Using the steps highlighted for WEB
in section 4, we encoded those atomic units and obtained a vocabulary of 18,759 and 19,785 Fon and French
tokens (word and expressions) used for the model training.
6.2	Dataset splitting, model’ s architecture and training.
From the dataset, we carefully selected 155 long and complex sentences i.e. sentences made of 5 or more
expressions, as test data; sentences that we believe, would test the model’s ability to correctly translate higher
word order expressions in Fon. 10% of the training data, was set aside for validation.
For training, we used an encoder-decoder-based architecture (Sutskever et al., 2014), made up of 128-
dimensional gated rectified units (GRUs) recurrent layers(Cho et al., 2014), with a word embedding layer of
dimension 256 and a 10-dimensional attention model (Bahdanau et al., 2015).
We trained with a batch size of 100, learning rate of 0.001 and 500 epochs, using validation loss to track model
performance. The training took all the 500 epochs, with the loss reducing from one epoch to another. We
would like to emphasize that up only at 500 epochs, with the given hyperparameters, we obtained significant
and meaningful translations.
All training processes took 14 days on a 16GB Tesla K80 GPU. We evaluated our NMT models performances
using BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), CharacTER (TER) (Wang et al.,
2016), and GLEU (Wu et al., 2016) metrics.
6.3	Results and Conclusion:
Table 1 and Table 2 show that our baseline model performs better with PhB, and best with WEB, in terms
of metric and translation quality. It is important to note that while BLEU scores of PhB and WEB, reduced
on the Fr→Fon task, BLEU scores of WB and SU improved on it. We speculate that this might be because
WB and SU enhanced the model’s understanding of French expressions over Fon, confirming the findings of
(Abbott & Martinus, 2018). Ngo et al. (2019). This corroborates our argument that in order to help NMT
2https://www.tensorflow.org
3https://www.nltk.org/api/nltk.translate.html
4http://www.tensorflow.org
5
Under review as a conference paper at ICLR 2021
Translation	Tokenization	BLEU ↑	METEOR ↑	TER ；	GLEU↑
Fon → Fr	WB	6.8	12.2	86.2	9
Fon → Fr	SU	7.6	13.6	87.4	10
Fon → Fr	PhB	38.9	53.7	43.9	42
Fon → Fr	WEB	66.6	77.77	24.2	67
Fr → Fon	WB	15.65	-	-	8
Fr → Fon	SU	25.68	-	-	9
Fr → Fon	PhB	38.74	-	-	25.74
Fr → Fon	WEB	49.37	-	-	43
Table 1: Experimental results of our model trained on WB, SU, PhB and WEB.
systems to translate ALRLs better, it is paramount to create adequate tokenization processes that can better
represent and encode their structure and morphology.
This is a pilot project and there is headroom to be explored with improving WEB. We are also working on
combining WEB with SU, to get the best of both worlds. To promote research and reproducibility in this
direction, the dataset and model will be made publicly available on Github after the review. Simultaneously,
we are working on releasing platforms for the translation service to be used. We believe that it would be a
good way to gather more data and keep constantly improving the model’s performance.
6
Under review as a conference paper at ICLR 2021
	Sentences: Fon , French and English Translations
Source Tokenization output Target	a CfojiCfiCfe Cfo WUtU ce a nɔnvi ce a CfojiCfiCfe Cfo wutu Ce a nɔnvi Ce '	{z			{z	} est-ce que tu me fais confiance mon frere? (my brother, do you trust in me?)
WB	confiance mon oncle (trust my uncle)
PhB	tu me fais confiance? (do you trust me?)
SU	aies la foi (have faith)
WEB	mon frere, est-ce que tu me fais confiance? (my brother do you trust in me?)
Source Tokenization output Target	Cfee man yon numi a, na bɔ yi doto hwe Cfee man yon numi a , na bɔ yi doto hwe J	-V—	J J	-V—	J j,irai a l,hopital vu que je ne me sens PaS bien(Since I am not feeling well, I will go to hospital)
WB	etre malade et se rendre a l,hopital (to be sick and to go to hospi- tal)
PhB	je me rends a l’hopital parce que je ne me sens pas bien ]col- oreng(I am going to hospital because I am not feeling well)
SU	rends a l’hopital, je suis malade (Go to hospital, I am sick)
WEB	je me rendrai a l’hopital vu que je ne me sens pas bien (I will go to hospital since I am not feeling well)
Table 2: Model translations with WB, PhB, SU and WEB
7
Under review as a conference paper at ICLR 2021
References
Jade Z. Abbott and Laura Martinus. Towards neural machine translation for african languages. CoRR,
abs/1811.05467, 2018. URL http://arxiv.org/abs/1811.05467.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to
align and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.
URL http://arxiv.org/abs/1409.0473.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correla-
tion with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Mea-
SuresforMachine Translation and/or Summarization, pp. 65-72, Ann Arbor, Michigan, June 2005. Associ-
ation for Computational Linguistics. URL https://www.aclweb.org/anthology/W05-0909.
Franck BurlOt and FrangOis Yvon. Using monolingual data in neural machine translation: a systematic study.
In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 144-155, Brussels,
Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6315. URL
https://www.aclweb.org/anthology/W18-6315.
Hounkpati B.C. Capo. A Comparative Phonology of Gbe. De Gruyter Mouton, Berlin, Boston, 2010. ISBN
978-3-11-087053-4.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulgehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation.
CoRR, abs/1406.1078, 2014. URL http://arxiv.org/abs/1406.1078.
Noa P. Cruz Diaz and Manuel Mana L6pez. An analysis of biomedical tokenization: Problems and strategies.
In Proceedings of the Sixth International Workshop on Health Text Mining and Information Analysis, pp.
40-49, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/
W15-2605. URL https://www.aclweb.org/anthology/W15-2605.
Bonaventure F. P. Dossou and Chris C. Emezue. Ffr v1.0: Fon-french neural machine translation. In
Proceedings of the AfricanNLP Workshop, International Conference on Learning Representations,
arXiv:arXiv:2003.12111, 2020.
Philip Gage. A new algorithm for data compression. C Users J., 12(2):23-38, February 1994. ISSN
0898-9788.
Jiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K. Li. Universal neural machine translation for extremely
low resource languages. In Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),
pp. 344-354, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:
10.18653/v1/N18- 1032. URL https://www.aclweb.org/anthology/N18-1032.
Ying He and Mehmet Kayaalp. A Comparison of 13 Tokenizers on MEDLINE. 12 2006. doi: 10.13140/2.1.
1133.1206.
Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. Iterative back-translation for
neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and
Generation, pp. 18-24, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:
10.18653/v1/W18-2703. URL https://www.aclweb.org/anthology/W18-2703.
Pratik M. Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of
linguistic diversity and inclusion in the nlp world. ArXiv, abs/2004.09095, 2020.
8
Under review as a conference paper at ICLR 2021
Bryan JUrish and Kay-Michael Wurzner. Word and sentence tokenization with hidden markov models. J.
Lang. Technol. Comput Linguistics, 28:61-83, 2013.
Alina Karakanta, Jon Dehdari, and Josef Genabith. NeUral machine translation for low-resoUrce langUages
withoUt parallel corpora. Machine Translation, 32(1-2):167-189, JUne 2018. ISSN 0922-6567. doi:
10.1007/s10590-017-9203-5. URL https://doi.org/10.1007/s10590-017-9203-5.
Dominik Machacek, Jonas Vidra, and Ondrej Bojar. Morphological and language-agnostic word segmentation
for NMT. CoRR, abs/1806.05482, 2018. URL http://arxiv.org/abs/1806.05482.
Andrei Mikheev. Tagging sentence boundaries. In Proceedings of the 1st North American Chapter of
the Association for Computational Linguistics Conference, NAACL 2000, pp. 264-271, USA, 2000.
Association for Computational Linguistics.
Thi-Vinh Ngo, Thanh-Le Ha, Phuong-Thai Nguyen, and Le-Minh Nguyen. Overcoming the rare word problem
for low-resource language pairs in neural machine translation. In Proceedings of the 6th Workshop on Asian
Translation, pp. 207-214, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-5228. URL https://www.aclweb.org/anthology/D19-5228.
Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel Whitenack, Kathleen Siminyu, Laura Martinus,
Jamiil Toure Ali, Jade Abbott, Vukosi Marivate, Salomon Kabongo, Musie Meressa, Espoir Murhabazi,
Orevaoghene Ahia, Elan van Biljon, Arshath Ramkilowan, Adewale Akinfaderin, Alp Oktem, Wole Akin,
Ghollah Kioko, Kevin Degila, Herman Kamper, Bonaventure Dossou, Chris Emezue, Kelechi Ogueji, and
Abdallah Bashir. Masakhane - machine translation for africa, 2020.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational
Linguistics, pp. 311-318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational
Linguistics. doi: 10.3115/1073083.1073135. URL https://www.aclweb.org/anthology/
P02-1040.
Adithya Renduchintala, Pamela Shapiro, Kevin Duh, and Philipp Koehn. Character-aware decoder for neural
machine translation. CoRR, abs/1809.02223, 2018. URL http://arxiv.org/abs/1809.02223.
Michael D. Riley. Some applications of tree-based modelling to speech and language. In Proceedings
of the Workshop on Speech and Natural Language, HLT ’89, pp. 339-352, USA, 1989. Association
for Computational Linguistics. ISBN 1558601120. doi: 10.3115/1075434.1075492. URL https:
//doi.org/10.3115/1075434.1075492.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword
units. CoRR, abs/1508.07909, 2015. URL http://arxiv.org/abs/1508.07909.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with mono-
lingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 86-96, Berlin, Germany, August 2016a. Association for Computational Lin-
guistics. doi: 10.18653/v1/P16-1009. URL https://www.aclweb.org/anthology/P16-1009.
Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword
units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 1715-1725, Berlin, Germany, August 2016b. Association for Computational Linguis-
tics. doi: 10.18653/v1/P16-1162. URL https://www.aclweb.org/anthology/P16- 1162.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. CoRR,
abs/1409.3215, 2014. URL http://arxiv.org/abs/1409.3215.
9
Under review as a conference paper at ICLR 2021
Elan van Biljon, Arnu Pretorius, and Julia Kreutzer. On optimal transformer depth for low-resource
language translation. In Proceedings of the International Conference on Learning Representations,
https://arxiv.org/abs/2004.04418, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http:
//arxiv.org/abs/1706.03762.
Weiyue Wang, Jan-Thorsten Peter, Hendrik Rosendahl, and Hermann Ney. CharacTer: Translation edit rate
on character level. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared
Task Papers, pp. 505-510, Berlin, Germany, August 2θ16. Association for Computational Linguistics. doi:
10.18653/v1/W16-2342. URL https://www.aclweb.org/anthology/W16-2342.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim
Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing
Liu, Eukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto KazaWa, Keith Stevens, George
Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,
Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s neural machine translation system: Bridging
the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.
org/abs/1609.08144.
Jiajun Zhang and Chengqing Zong. Exploiting source-side monolingual data in neural machine translation.
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 1535,
Austin, Texas, nov 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1160. URL
https://www.aclweb.org/anthology/D16-1160.
10