Under review as a conference paper at ICLR 2021
Online Testing of Subgroup Treatment Ef-
fects Based on Value Difference
Anonymous authors
Paper under double-blind review
Ab stract
Online A/B testing plays a critical role in high-tech industry to guide product de-
velopment and accelerate innovation. It performs a null hypothesis statistical test
to determine which variant is better. However, a typical A/B test presents two
problems: (i) a fixed-horizon framework inflates the false positive errors under
continuous monitoring; (ii) the homogeneous effects assumption fails to identify
a subgroup with a beneficial treatment effect. In this paper, we propose a sequen-
tial test for subgroup treatment effects based on value difference, named SUBTLE,
to address these two problems simultaneously. The SUBTLE allows the experi-
menters to “peek” the results during the experiment without harming the statistical
guarantees. It assumes heterogeneous treatment effects and aims to test if some
subgroup of the population will benefit from the investigative treatment. If the
testing result indicates the existence of such subgroup, a subgroup will be iden-
tified using a readily available estimated optimal treatment rule. We examine the
empirical performance of our proposed test on both simulations and a real data
set. The results show that the SUBTLE has high detection power with controlled
type I error at any time, is more robust to noise covariates, and can achieve early
stopping compared with the corresponding fixed-horizon test.
1	Introduction
Online A/B testing, as a kind of randomized control experiments, is widely used in high-tech indus-
try to assess the value of ideas in a scientific manner (Kohavi et al., 2009). It randomly exposes users
to one of the two variants: control (A), the currently-used version, or treatment (B), a new version
being evaluated, and collects the metric of interest, such as conversion rate, revenue, etc. Then,
a null hypothesis statistical test is performed to evaluate whether there is a statistically significant
difference between the two variants on the metric of interest. This scientific design helps to control
for the external variations and thus establish the causality between the variants and the outcome.
However, the current A/B testing has its limitations in terms of framework and model assumptions.
First of all, most A/B tests employ a fixed-horizon framework, whose validity requires that the
sample size should be fixed and determined before the experiment starts. However, experimenters,
driven by a fast-paced product evolution in practice, often “peek” the experiment and hope to find
the significance as quickly as possible to avoid large (i) time cost: an A/B test may take prohibitively
long time to collect the determined size of samples; and (ii) opportunity cost: the users who have
been assigned to a suboptimal variant will be stuck in a bad experience for a long time (Ju et al.,
2019). The behaviors of continuously monitoring and concluding the experiment prematurely will
be favorably biased towards getting significant results and lead to very high false positive proba-
bilities, well in excess of the nominal significance level α (Goodson, 2014; Simmons et al., 2011).
Another limitation of A/B tests is that they assume homogeneous treatment effects among the popu-
lation and mainly focus on testing the average treatment effect. However, it is common that treatment
effects vary across sub-populations. Testing the subgroup treatment effects will help decision mak-
ers distinguish the sub-population that may benefit from a particular treatment from those who may
not, and thereby guide companies’ marketing strategies in promoting new products.
The first problem can be addressed by applying the sequential testing framework. Sequential testing,
contrast to the classic fixed-horizon test, is a statistical testing procedure that continuously checks
for significance at every new sample and stops the test as soon as a significant result is detected,
1
Under review as a conference paper at ICLR 2021
while controlling the type I error at any time. It generally gives a significant decrease in the required
sample size compared to the fixed-horizon test with the same type I error and type II error control,
and thus is able to end an experiment much earlier. This field was first introduced by Wald (1945),
who proposed sequential probability ratio test (SPRT) for simple hypotheses using likelihood ratio
as the test statistics, and then was extended to composite hypotheses by many following literature
(Schwarz, 1962; Armitage et al., 1969; Cox, 1963; Robbins, 1970; Lai, 1988). A thorough review is
given in Lai (2001). However, the advantage of sequential testing in online A/B testing has not been
recognized until recently Johari et al. (2015) brought the mSPRT, a variant of SPRT to A/B tests.
The second problem shows a demand for a test on subgroup treatment effects. Although sequential
testing is rapidly developing in online A/B test, few work focuses on subgroup treatment effect
testing. Yu et al. (2020) proposed a sequential score test (SST) based on score statistics under a
generalized linear model, which aims to test if there is difference between treatment and control
groups among any subjects. However, this test is based on a restrictive parametric assumption on
treatment-covariates interaction and can’t be used to test the subgroup treatment effects.
In this paper, we consider a flexible model, and propose a sequential test for SUBgroup Treatment
effects based on vaLuE difference (SUBTLE), which aims to test if some group of the population
would benefit from the investigative treatment. Our method does not require to specify any para-
metric form of covariate-specific treatment effects. If the null hypothesis is rejected, a beneficial
subgroup can be easily obtained based on the estimated optimal treatment rule.
The remainder of this paper is structured as follows. In Section 2, we review the idea of the mSPRT
and SST, and discuss how they are related to our test. Then in Section 3, we introduce our proposed
method SUBTLE and provide the theoretical guarantee for its validity. We conduct simulations
in Section 4 and real data experiments in Section 5 to demonstrate the validity, detection power,
robustness and efficiency of our proposed test. Finally, in Section 6, we conclude the paper and
present future directions.
2	Related work
2.1	Mixture sequential probability ratio test
The mixture sequential probability ratio test (mSPRT) (Robbins, 1970) supposes that the indepen-
dent and identically distributed (i.i.d.) random variables Y1,Y2, ∙…have a probability density func-
tion fθ(x) induced by parameter θ, and aims to test
H0 : θ = θ0 v.s. H1 : θ 6= θ0.	(1)
Its test statistics Λπn at sample size n is a mixture of likelihood ratios as below:
Λ
π
n
π(θ)dθ,
(2)
with a mixture density ∏(∙) over the parameter space Θ. The mSPRT stops the sampling at the stage
N = inf{n ≥ 1 : Λ∏ ≥ 1∕α}
(3)
and rejects the null hypothesis H0 in favor of H1 . If no such time exists, it continues the sampling
indefinitely and accept the H0. Since the likelihood ratio under H0 is a nonnegative martingale with
initial value equal to 1, and so is the mixture of such likelihood ratios Λπn , the type I error of mSPRT
can be proved to be always controlled at α by an application of Markov’s inequality and optional
stopping theorem: Ph° (Λn ≥ α-1) ≤ *-个n] = EH1-F ] = α. Besides, mSPRT is a test of power
one (Robbins & Siegmund, 1974), which means that any small deviation from θ0 can be detected
as long as waiting long enough. It is also shown that mSPRT is almost optimal for data from an
exponential family of distributions, with respect to the expected stopping time (Pollak, 1978).
The mSPRT was brought to A/B test by Johari et al. (2015; 2017), who assume that the observa-
tions in control (A = 0) and treatment (A = 1) groups arrive in pairs (Yi(O),γ(I)), i = 1,2, ∙∙∙.
They restricted their data model to the two most common cases in practice: normal distribution
2
Under review as a conference paper at ICLR 2021
and Bernoulli distribution, with μA and μB denoting the mean for control and treatment group,
respectively. They test the hypothesis as below
Ho : θ := μB — μA = 0 v.s. Hi : θ = 0,	(4)
by directly applying mSPRT to the distribution of the differences Zi = Yi(1) - Yi(0) (normal),
or the joint distribution of data pairs (匕⑼，Yi(I)) (Bernoulli), i = 1, 2,…. After making some
approximations to the likelihood ratio and choosing a normal mixture density ∏(θ)〜 N(0,τ2), the
test statistic Λπn is able to have a closed form for both normal and Bernoulli observations.
However, the mSPRT does not work well on testing heterogeneous treatment effects due to the com-
plexity of likelihood induced by individual covariates. Specifically, a conjugate prior ∏(∙) for the
likelihood ratio may not exist anymore so that the computation for the test statistic is challenging.
The unknown baseline covariates effect also increases the difficulty in constructing and approximat-
ing the likelihood ratios (Yu et al., 2020).
2.2	Sequential score test
The sequential score test (SST) (Yu et al., 2020) assumes a generalized linear model with a link
function g(∙) for the outcome Y
g(E[Y|A, X]) = μτX + (θτX)A,	(5)
where A and X denote the binary treatment indicator and user covariates vector, and tests the multi-
dimensional treatment-covariates interaction effect:
H0 : θ = 0 vs. H1 : θ 6= 0,	(6)
while accounting for the linear baseline covariates effect μτX. For the test statistics Λ∏, instead
of using a mixture likelihood ratios as mSPRT, SST employed a mixture asymptotic probability
ratios of a score statistics. Since the probability ratio has the same martingale structure as the
likelihood ratio, the type I error can still be controlled with the same decision rule as mSPRT (3). The
asymptotic normality of the score statistics also guarantees a closed form of Λπn with a multivariate
normal mixture density ∏(∙). However, the considered parametric model (5) can only be used to test
if there are linear covariate-treatment interaction effects, and may fail to detect the existence of a
subgroup with enhanced treatment effects. In addition, the subgroup estimated based on the index
θT Xi may be biased if the assumed linear model (5) is misspecified. Therefore in this paper, we
propose a subgroup treatment effect test, which is able to test the existence of a beneficial subgroup
and does not require to specify the form of treatment effects.
3	Subgroup treatment effects test based on value difference
3.1	Problem setup
Suppose We have i.i.d. data Oi = {Yi, Ai, Xi} , i = 1, 2, •…，where Yi, Ai, Xi respectively denote
the observed outcome, binary treatment indicator, and p-dimensional user covariates vector. Here,
we consider a flexible generalized linear model:
g(E[Yi∣Ai, Xi]) = μ(Xi) + θ(Xi)Ai,	(7)
where baseline covariates effect μ(∙) and treatment-covariates interaction effect θ(∙) are completely
unspecified functions, and g(∙) is a prespecified link function. For example, We use the identity link
g(μ) = μ for normal response and the logit link g(μ) = log j-μμ for binary data.
Assuming Y is coded such that larger values indicate a better outcome, we consider the following
test of subgroup treatment effects:
H0 :	∀x ∈ X,	θ(x)	≤ 0 vs.	H1	:	∃X0	⊂ X such that	θ(x) > 0 for all x ∈	X0,	(8)
where X0 is the beneficial subgroup with P(X ∈ X0) > 0. Note that the above subgroup test is
very different from the covariate-treatment interaction test considered in (6) and is much more chal-
lenging due to several aspects. First, both μ(∙) and θ(∙) are nonparametric and need to be estimated.
3
Under review as a conference paper at ICLR 2021
Second, the considered hypotheses are moment inequalities which are nonstandard. Third, it allows
the nonregular setting, i.e. P{θ(X) = 0} > 0, which makes associated inference difficult. Here, we
propose a test based on value difference between the optimal treatment rule and a fixed treatment
rule.
Let V(d) = E(Y*(a),x){Y*(d(X))} denote a value function for a treatment decision rule, where
Y *(d(X)) is the potential outcome if treatment were allocated according to the fixed treatment deci-
sion rule d(X), which maps the information in X to treatment {0, 1}. Consider the value difference
∆ = V(dopt) - V(0) between the optimal treatment rule dopt = 1 {θ(X) > 0} and the treatment rule
that assigns control to everyone d = 0, where 1{∙} is an indicator function. If the null hypothesis
is true, no one would benefit from the treatment and the optimal treatment rule assigns everyone to
control, and therefore the value difference is zero. However, if the alternative hypothesis is true,
some people would have higher outcomes being assigned to treatment and thus the value difference
is positive. In this way the testing hypotheses (8) can be equivalently transformed into the following
pair:
H0 : ∆ = 0 vs. H1 : ∆ > 0.	(9)
We make the following standard causal inference assumptions: (i) consistency, which states that
the observed outcome is equal to the potential outcome under the actual treatment received, i.e.
Y = Y *(1)I (A = 1) + Y * (0)I (A = 0); (ii) no unmeasured confounders, i.e. Y *(a) ⊥⊥ A|X,
which means the potential outcome is independent of treatment given covariates; (iii) positivity, i.e.
P(A = a|X = x) > 0 for a = 0, 1 and all x ∈ X such that P(X = x) > 0. Under these
assumptions, it can be shown that
V(d) =EX{E[Y|A= d(X), X]}.
3.2	Algorithm and implementation
We take the augmented inverse probability weighted (AIPW) estimator (Robins et al., 1994; Zhang
et al., 2012) for the value function of a given treatment rule d:
VAIPW(d) = 1 X [ Y⅛^ -(— l) ∙ EMAi = d, Xi]]
n i=1	pAi (Xi)	pAi (Xi)
where PA(X) = A * p(X) + (1 - A) * (1 - P(X)) and p(X) = P(A = 1|X) is the propensity
score. This estimator is unbiased, i.e., E(Y,A,X) [VAIPW(d)] = V (d). Moreover, the most important
property of AIPW estimator is the double robustness, that is, the estimator remains consistent if
either the estimator of E[Y |A = d, X] or the estimator of the propensity score P(X) is consistent,
which gives much flexibility. Then the value difference ∆ is unbiased estimated by
ng∙ ∩ V ʃ 1 {Ai = 1(θ(Xi) > 0)}* Y (1 {Ai = 1(θ(Xi) > 0)}八
DO μ,θ,p) := t -PAi(Xi)— * K - (-PAi(Xi)-----------------------------1J
* g-1 (μ(Xi) + θ(Xi)1(θ(Xi) > 0)) ] - ʃ 1 (Ai =0) * Yi - (I(Ai =0) - 1)* g-1 (μ(Xi))]
1 - P(Xi)	1 - P(Xi)
(10)
where g-1(∙) is the inverse of the link function. That is,
e(y,a,x)[D(Oi； μ, θ,p)] = δ∙
Since μ(∙), θ(∙) andp(∙) are usually unknown, We let data come in batches and estimate them based
on previous batches of data. Algorithm 1 shows our complete testing procedures.
In step (ii) of Algorithm 1, we estimate μ(∙) and θ(∙) by respectively building a random for-
est on control observations and on treatment observations in previous batches. The propensity
score p(∙) is estimated by computing the proportion of treatment observations (A = 1) in previ-
ous batches. In step (iv) we estimate σk with σk = ʌ/sk, where Sk is the sample variance of
D(Oi； μk-1,θk-1 ,Pk-ι), NOi ∈ Ck-1. Note that Rk (14) is a multiplier of an asymptotic unbi-
ased estimator for ∆, which is defined as below:
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Subgroup treatment effects sequential test based on value difference
1.	Initialize k = 0, Λkπ = 0. Choose a significance level 0 < α < 1, a batch size m, an initial
batch size l, and a failure time M .
2.	Sample l observations to formulate initial batch C0 .
while True do
(i)	k=k+1;
(ii)	Let Ck-ι = ∪k-1Cj. Estimate μ(∙), θ(∙) andp(∙) based on data in Ck-ι to get μk-ι,
θk-ι andpk-1；
(iii)	Sample another m observations to formulate batch Ck. For each Oi ∈ Ck, calculate
D(Oi； μk-1,θk-1,pk-1 ).Let
=,一 1	1 L 一 —	八	.
Dk(Ck； Ck-l) = 一 D2 D(Oi； μk-1,θk-1,Pk-l)
m
Oi∈Ck
(13)
(iv)	Estimate the conditional standard deviation σk = Sd (Dk(Ck;Ck-1)∣Ck-1) based on
data in Ck-ι and denote it as &卜;
(v)	Calculate
1	1 上 I =. 一 ,
Rk = √ ∑>-* 1Dj(Cj; Cj-ι)	(14)
k j=1
and
Λ∏ = Z ψ⅛j≠2 ∏(∆)d∆,	(15)
where ψ(μ,σ2) (∙) denotes the probability density function of a normal distribution with
mean μ and variance σ2;
if Λ∏ > 1∕a or k X m + l > M then
I break;
end
end
if Λ∏ > 1/a then
I Reject H0. Estimate θ(∙) using all the data UP to now and identify a subgroup 1{θ(X) > 0};
else
I Accept Ho;
end
In section 3.3 we will show that Rk has an asymptotic normal distribution with same variance but
different means under null and local alternatives, so that our test statistics Λkπ (15) is a mixture
asymptotic probability ratios of Rk. Since the value difference is always non-negative, we choose
√2⅛ ∙ exp{一图
a truncated normal π(∆)
• 1(∆ > 0) as the mixture density, where T2
is estimated based on historical data. The simulation result in Appendix A.2.1 shows considerable
robustness in choosing τ2. Our test statistic now has a closed form:
(_________)1/2 × exp ʃ ((T E= σ-1∙ Rk)∖ 1
Ik + (T E= σ-1)2∫	12 h(τ ∙Pk= σ-1)2 + ki
× [1 -F(0)],
(12)
where F(•) is the cumulative distribution function of a normal distribution with mean
√k∙pk=ι σ-1∙Rk …A	-_____ kτ2
(Pk=I ^-1)2 + k and variance τ2(Ek=ι ^-1)2 + k .
If the null hypothesis is rejected, we can employ random forests to estimate θ(∙) based on all the
data up to the time that the experiment ends. Then the optimal treatment rule θ(x) naturally gives
the beneficial subgroup X0 = {x : θ(x) > 0}.
5
Under review as a conference paper at ICLR 2021
3.3 Validity
In this section, we will show that our proposed test SUBTLE is able to control type I error at any
time, that is, Ph0 (Λ∏ > 1∕ɑ) < α for any k ∈ N. As We discussed in Section 2.1, if We can
show that the ratio term in Λkπ (15) has a martingale structure under H0 , it follows easily that the
type I error is alWays controlled at α. Theorem 3.1 gives the respective asymptotic distributions
of Rk under null and local alternative, Which demonstrates that the test statistics Λkπ is a mixture
asymptotic probability ratios weighted by ∏ (∙). Proposition 1 shows that this asymptotic probability
ratio is a martingale When the sample size is large enough. Combining these tWo results With the
demonstration in Section 2.1, we can conclude that the type I error of SUBTLE is always controlled
at α.
We assume the following conditions hold:
•	(C1) k diverges to infinity as sample size n diverges to infinity.
•	(C2) Lindeburg-like condition:
1k
1 X E
k j=1
..ɪ ʃ _ —-	、.
IDj (Cj； Cj-I)I
σj
Cj-I
op(1)
for all > 0.
2
•	(c3) k Pk=1 σ2 → L
j
•	(C4)	1 Pk=I σ-1 (EDj(Cj；Cj-I)ICj-1] - E[Dj(Cj; jtι,μ,θ,p)∣Cj-ι])
op(k-1/2).
•	(C5) kPj=Iσ-1 (EDj(Cj； j-1，μ,θ,p)∣Cj-ι] - △) = Op(k-1/2).
Theorem 3.1 For △ j defined in(11), under conditions (C1)-(C5),
√=	(X σ-1 j	(△ j	-	△)	→	N(0, 1)	as k →	∞,	(16)
where →d represents convergence in distribution. In particular, as k → ∞, Rk --d→ N(0, 1) under
H0
null hypothesis △ = 0, while Rk 一 √1j (Pk=I σ-1) △ -→ N(0,1) under local alternative △=
√τ^, where δ > 0 is fixed.
ψ(0, 1)(Rk)
Proposition 1 Let λk
(Rk)
, and Fk denote a filtration that contains all the
historical information in the first (k + 1) batches Cj. Then under null hypothesis Ho : △ = 0,
E[λj+ι∣Fj] is approximately equal to λj ∙ exp{θp(1)}.
The proofs of above results are given in the Appendix A.1.
4	Simulated experiments
In this section, we evaluate the test SUBTLE on three metrics: type I error, power and sample
size. We first compare SUBTLE with SST in terms of type I error and power under five models in
Section 4.1. Then in Section 4.2, we present the impact of noise covariates on their powers. Finally
in Section 4.3, we compare the stopping time of SUBTLE to the required sample size of a fixed-
horizon value difference test. The significance level α = 0.05, initial batch size l = 300, failure
time M = 2300 and variance of mixture distribution τ2 = 1 are fixed for all simulation settings.
6
Under review as a conference paper at ICLR 2021
4.1	Type I error & power
We consider five data generation models in the form of (7) with logistic link g(∙). Data are generated
in batches with batch size m = 20 and are randomly assigned to two groups with fixed propensity
score p(X) = 0.5. Each experiment is repeated 1000 times to estimate the type I error and power.
For the first four models, we choose
•	Five covariates: Xi iid Ber(0.5), X2 iid Unif [-1,1], X3,X4,X5 iid N(0,1)
•	Two baseline effect: μι(X) = -2 - Xi + Xj, μ2(X) = -1.3 + Xi + 0.5X2 - Xj
•	Two treatment-covariates interaction effect: θι(X) = C ∙ 1{Xi + 2X3 > 0}, θ2(X)
C ∙1{X2 > 0 or X5 < -0.5}.
Table 1: The first four models
Model	Input covariates	μ(X)	θ(X)
I	x1, X3	μι (X)	θi(X)
II	Xi , Xj , X3 , X4, X5	μ2 (X)	θ2(X)
III	Xi , Xj, X3 , X4, X5	μι (X)	θ2(X)
IV	Xi, X2, X3, X4, X5	〃2 (X)	θi(X)
Table 1 displays which covariates, μ(X) and θ(X)
consider the following high-dimensional setting:
Xriid N(0.2r - 0.6,1), r = 1, 2, 3,4, 5
Xriid N(0.2r - 1.6, 2), r = 6, 7, 8, 9,10
Xriid Unif [-0.5r + 5,0.5r - 5], r = 11,12,13
μ(X) = -0.8 + Xi8 + 0.5Xi2 - Xj
are employed in each model. For model V, we
X14 iid Unif [-0.5,1.5]
X15 iid Unif [-1.5,0.5]
Xr	iid Ber(0.2r - 3.1), r = 16,17,18,19, 20
θ(X) = C ∙ 1{(Xi4 > -0.1) & (X20 = 1)},
where C varies among {-1, 0, 0.6, 0.8, 1} indicating the intensity of the value difference. When
C = -1 and 0, the null hypothesis is true and the type I error is estimated, while when C = 0.6, 0.8, 1,
the alternative is true and the power is estimated.
Table 2 shows that the SUBTLE is able to control type I error and achieve competing detection
power, especially under high-dimensional setting (Model V); however, SST couldn’t control type I
error especially when C = -1. This can be explained by two things: (i) the linearity of model (5) is
violated; (ii) SST is testing if there is difference between treatment and control groups among any
subjects, instead of the existence of a beneficial subgroup. Specifically, SST is testing if the least
false parameter θ*, to which the MLE of θ under model misspecification converges, is zero or not.
We also perform experiments with batch size m = 40, and the results (shown in Appendix A.2.1)
do not have much difference.
Table 2: Estimated type I error or power for SUBTLE and SST with batch size 20
Model	I		II		In		IV		V	
C	SUBTLE	SST	SUBTLE	SST	SUBTLE	SST	SUBTLE	SST	SUBTLE	SST
-1	0.009	0.695	0.002	0.589	0.003	0.224	0.004	0.411	0.002	0.008
0	0.015	0.134	0.010	0.023	0.006	0.095	0.010	0.023	0.006	0.038
0.6	0.323	0.564	0.491	0.513	0.269	0.389	0.424	0.425	0.559	0.170
0.8	0.623	0.845	0.878	0.900	0.719	0.723	0.822	0.824	0.925	0.390
1	0.911	0.974	0.988	0.996	0.952	0.943	0.985	0.982	0.997	0.742
4.2	Noise covariates
Itis common in practice that a large number of covariates are incorporated in the experiment whereas
the actual outcome only depends on a few of them. Some covariates do not have any effect on
7
Under review as a conference paper at ICLR 2021
the response, like X4 in Model II, III, IV, and we call them noise covariates. In the following
simulation, we explore the impact of noise covariates to the detection power. We choose Model I
with c = 0.8 as the base model, and at each time add three noise covariates which are respectively
from normal N (0, 1), uniform Unif [-1, 1], and Bernoulli Ber(0.5) distributions. The batch size
is set to m = 40 for computation efficiency. Figure 1 shows that SST has continuously decreasing
powers as the number of noise covariates increases, while the power of SUBTLE is more robust to
the noise covariates
Figure 1: Estimated power v.s. the number
of noise covariates
Figure 2: Histogram of stopping time for
1000 replicates of experiments under Model
V with c=1
4.3	Stopping time
A key feature for sequential test is that it has an expected smaller sample size than fixed-horizon test.
For comparison, we consider a fixed-horizon version of SUBTLE, which leverages the Theorem 3.1
and rejects the null hypothesis H0 : ∆ = 0 when Rk > Zα for some predetermined k, where Zα de-
notes the (1 - α) quantile of standard normal distribution. We assume σT = limk→∞ 1 Pk=I ^-1,
then the required number of batches k can be calculated as k = σ (Za +Z1-poWer) , and thus the re-
quired sample size is n = k * m + l. The true value difference ∆ can be directly estimated from
data generated under true model and tWo treatment rules, While σ2 is estimated by the sample vari-
ance of ∆k0 (11) times k0 for some fixed large k0 . Here, We choose Model V With c = 1 and batch
size m = 20. The stopping sample size of our sequential SUBTLE over 1000 replicates are shoWn
in Figure 2, and the dashed vertical line indicates the required sample size for the fixed-horizon
SUBTLE With the same poWer 0.997 (seen from Table 2) under the same setting. We can find that
most of the time our sequential SUBTLE arrives the decision early than the fixed-horizon version,
but occasionally it can take longer. The distribution of the stopping time for sequential SUBTLE is
right-skeWed, Which is line With the findings in Johari et al. (2015) and Ju et al. (2019).
5	Real data experiments
We use Yahoo real data to examine the performance of our SUBTLE, Which contains user click
events on articles over 10 days. Each event has a timestamps, a unique article id (variants), a binary
click indicator (response), and four independent user features (covariates). We choose tWo articles
(id=109520 and 109510) With the highest click through rates as control and treatment, respectively.
We set the significance level α = 0.05, initial batch size and batch size l = m = 200, and the failure
time M = 50000.
To demonstrate the false positive control of our method, We conduct A/A test and permutation test.
For A/A test, We only use data on article 109510 and randomly generate fake treatment indicator.
Our method accepts the null hypothesis. For permutation test, We use combined data from article
109510 and 109520, and permute their response 1000 times While leaving treatment indicator and
covariates unchanged. The estimated false positive rate is beloW the significance level.
Then We test if there is any subgroup of users Who Would have higher click rate on article 109510.
In this experiment, SUBTLE rejects the null hypothesis With sample size n = 12400. We identify
the beneficial subgroup 1{θ(X) > 0} by estimating θ(X) With random forest on the first 12400
8
Under review as a conference paper at ICLR 2021
observations. To get a structured optimal treatment rule, we then build a classification tree on the
same 12400 samples with random forest estimator 1{θ(X) > 0} as true labels. The resulting
decision tree (shown in Appendix A.2.2) suggests that the users in the subgroup defined by {X3 <
0.7094 or (X3 ≥ 0.7094, X1 ≥ 0.0318 and X4 < 0.0003)} benefit from treatment.
We then use the 50000 samples after the first 12400 samples as test data set, and then compute
the difference of click through rates between article 109510 and 109520 on the test data (overall
treatment effect), and the same difference in the subgroup of the test data (subgroup treatment effect).
We found that the subgroup treatment effect 0.009 is larger than the overall treatment effect 0.006,
which shows that the identified subgroup has enhanced treatment effects than the overall population.
We further compute the inverse probability weighted (IPW) estimator 1 pn=1 I(Ai=d(Xi))*Yi using
the test data for the values of two treatment rules: d1(X) = 0 that assigns everyone to control and
the optimal treatment rule d2(X) = 1{θ(X) > 0} estimated by random forest. Their IPW estimates
are respectively 0.043 and 0.049, which suggests that the estimated optimal treatment rule is better
than the fixed rule that assigns all users to the control group. This implies there exists a subgroup of
the population that does benefit from the article 109510.
6	Conclusion
In this paper, we propose SUBTLE, which is able to sequentially test if some subgroup of the
population will benefit from the investigative treatment. If the null hypothesis is rejected, a beneficial
subgroup can be easily identified based on the estimated optimal treatment rule. The validity of the
test has been proved by both theoretical and simulation results. The experiments also show that
SUBTLE has high detection power especially under high-dimensional setting, is robust to noise
covariates, and allows quick inference most of time compared with fixed-horizon test.
Same as mSPRT and SST, the rejection condition of SUBTLE may never be reached under some
cases, especially when the true effect size is negligible. Thus, a failure time is needed to terminate
the test externally and accept the null hypothesis if we ever reach it. How to choose a failure time to
trade off between waiting time and power need to be studied in the future. Another future direction
is the application of our test under adaptive allocation, where users will have higher probabilities of
being assigned to a beneficial variant based on previous observations. However, the validity may not
be guaranteed anymore under adaptive allocation and more theoretical investigations are needed.
References
Peter Armitage, CK McPherson, and BC Rowe. Repeated significance tests on accumulating data.
Journal of the Royal Statistical Society: Series A (General),132(2):235-244, 1969.
David Roxbee Cox. Large sample sequential tests for composite hypotheses. Sankhya： The Indian
Journal of Statistics, Series A, pp. 5-12, 1963.
Martin Goodson. Most winning a/b test results are illusory. Whitepaper, Qubit, Jan, 2014.
Ramesh Johari, Leo Pekelis, and David J Walsh. Always valid inference: Bringing sequential anal-
ysis to a/b testing. arXiv preprint arXiv:1512.04922, 2015.
Ramesh Johari, Pete Koomen, Leonid Pekelis, and David Walsh. Peeking at a/b tests: Why it mat-
ters, and what to do about it. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 1517-1525. ACM, 2017.
Nianqiao Ju, Diane Hu, Adam Henderson, and Liangjie Hong. A sequential test for selecting the
better variant: Online a/b testing, adaptive allocation, and continuous monitoring. In Proceedings
of the Twelfth ACM International Conference on Web Search and Data Mining, pp. 492-500.
ACM, 2019.
Ron Kohavi, Roger Longbotham, Dan Sommerfield, and Randal M Henne. Controlled experiments
on the web: survey and practical guide. Data mining and knowledge discovery, 18(1):140-181,
2009.
9
Under review as a conference paper at ICLR 2021
Tze Leung Lai. Nearly optimal sequential tests of composite hypotheses. The Annals of Statistics,
pp. 856-886,1988.
Tze Leung Lai. Sequential analysis: some classical problems and new challenges. Statistica Sinica,
pp. 303-351, 2001.
Moshe Pollak. Optimality and almost optimality of mixture stopping rules. The Annals of Statistics,
pp. 910-916, 1978.
H Robbins and D Siegmund. The expected sample size of some tests of power one. The Annals of
Statistics, pp. 415-436, 1974.
Herbert Robbins. Statistical methods related to the law of the iterated logarithm. The Annals of
Mathematical Statistics, 41(5):1397-1409, 1970.
James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when
some regressors are not always observed. Journal of the American statistical Association, 89
(427):846-866, 1994.
Gideon Schwarz. Asymptotic shapes of bayes sequential testing regions. The Annals of mathemati-
cal statistics, pp. 224-236, 1962.
Joseph P Simmons, Leif D Nelson, and Uri Simonsohn. False-positive psychology: Undisclosed
flexibility in data collection and analysis allows presenting anything as significant. Psychological
science, 22(11):1359-1366, 2011.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 113(523):1228-1242, 2018.
Abraham Wald. Sequential tests of statistical hypotheses. The annals of mathematical statistics, 16
(2):117-186, 1945.
Miao Yu, Wenbin Lu, and Rui Song. A new framework for online testing of heterogeneous treatment
effect. In Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence. AAAI,
2020.
Baqun Zhang, Anastasios A Tsiatis, Eric B Laber, and Marie Davidian. A robust method for esti-
mating optimal treatment regimes. Biometrics, 68(4):1010-1018, 2012.
A Appendix
A.1 Proofs
A.1.1 Proof of Theorem 3.1
Among the conditions for Theorem 3.1, (C1) holds by nature. We suppose that (C2) and (C3) hold.
(C4) and (C5) depends on the convergence rate of estimators of μ, θ,p. Wager & Athey (2018)
showed that under certain constraints on the subsampling rate, random forest predictions converge
at the rate ns-1/2,
where s is chosen to satisfy some conditions. We assume that under this rate,
(C4) and (C5) also hold.
Let Fj, 0 ≤ j ≤ k, denote a filtration generated by observations in first (j+1) batches Cj = ∪j=oCr,
and Dj (Cj; d；%,μ,θ,p) denote an AIPW estimator for ∆ with only optimal decision rule estimated
by previous batches:
.	St	1 L G {Ai = 1(θj-ι(Xi) > 0)}	八{Ai = 1(θj-1(Xi) > 0)}
Dj (Cj; dj-l, μ, θ,p) := ~ X ∖ -------------------------- * Yi - I --------------------------1
j-1	m Oi∈Cj	pAi(Xi)	pAi(Xi)
* g-1 (μ(Xi) + θ(Xi)1(θj-ι(Xi) >
，1(Ai= 0) * Y- (1(Ai = 0)
b - P(Xi) i υ -P(Xi)
- 1 ) * g 1 (〃(Xi))卜.
(17)
10
Under review as a conference paper at ICLR 2021
Then
1自-1卜 k-△)	(18)
1 k -	-
=k ∑σ-1 (D j (Cj ； Cj-I)-△)	(19)
j=1
1 k
=k Xσ-1 ((Dj(Cj;Cj-ι) - E[Dj(Cj; d-1, μ,θ,p)∣Cj-ι]) + (E[Dj(Cj; *, μ,θ,p)∣Cj-ι]-
j=1
(20)
1
=k Xσ-1 (Dj(Cj;Cj—1) - E[Dj(Cj;挈tι,μ,θ,p)∣Cj-ι]) + θp(k-1/2)	(21)
j=1
1	・ 一、 .….
=k £&丁 (Dj(Cj; Cj-I)- E[Dj(Cj; Cj-I)IFτ]) + 心'\	(22)
j=1
Above (21) follows by condition (C5) and (22) follows by condition (C4). For j = 1,2, ∙∙∙ ,k, let
Mk = √1k
Dj (Cj; Cj-I) - E[Dj (Cj; Cj-I)IFj-1]
σj
(23)
It is obvious that for each k, Mk,j , 1 ≤ j ≤ k, is a martingale with respect to the filtration Fj .
2
In particular, for all j ≥ 1, E[Mk,j∣Fj-ι] = 0 and Pj=I E[M2,j∣Fj--ι] = 1 Pi=I j → 1 as
k -→ ∞ by (C3). The conditional Lindeberg condition holds in (C2), so the martingale central limit
theory for triangular arrays gives
k
X Mk,j -→d N(0, 1).	(24)
j=1
Plugging it back into (22), we can get
√k (XXσ-1j (△ k - △) → N(0,1).	(25)
A.1.2 Proof of Proposition 1
We first simplify the formula of λk to:
λk
ψ √k (Pk=I ^1)δ, 1
(Rk)
ψ(0, 1) (Rk)
f ι k
eχp] √k X σ-1∙ △ ∙
1k
Rk - 2k (X σ-1)2 ∙ △
j=1
exp {k X σ-1∙ △ ∙ XX (σ-1D j) - 2k (X σ-1)2 ∙ ∆2∖,
j=1	j=1
j=1
where We denote Dj(Cj; Cj-ι) with Dj for simplicity. Let
A.	P"σ-1Dj
△k := jΓ
(26)
(27)
(28)
(29)
11
Under review as a conference paper at ICLR 2021
and remember that Theorem 3.1 gives
(£ σLkL △) → 阳。，1)，
(30)
where σj is estimated from the first j batches Cj_1, j = 1, 2,…，k. Since the true value difference
∆ is not very large in practice, we assume local alternative ∆ = Op(k-1/2) here as in Theorem 3.1.
Then,
Eh0 [λk+ι∖Fk]
(31)
{1 k+1	k+1	Ik+1	I
exP E Σ制「δ ∙ >σ-1Dj) - 2wl)(∑σ-D2 ∙ MFk)	(32)
{1	k+1	k+1	1	k+1	])
11
EHo[ E Σ σ- ∙ δ ∙ >σ-D j)- 2(⅛∏)(∑ σ-) ∙ δ Fk ]ʃ
(33)
{1 k+1	/ k	∖	1	k+1
r∏Eσ-1∙δ∙ ∣E(σ-1Dj) + σ-+1∙EHdDk+1∖Fk]∣ - 27k+ιy(∑σ-1)2 ∙δ2
+ 1j=1	v=1	0	')	( + ) j=1
(34)
{1 k+1	k	1	k+1	)
k+1 E 小 δ ∙E(小j)- 2(k⅛(∑ σ-1)2 ∙ U	(35)
j=1	j=1	'	/ j=1	)
{[ k	k	k k	k+1	1 k ∖	k
1 ʌ -1 A v^/ ʌ -1 n ∖ 1 1	1 ʌ -1	1	入一1 1 A v^/ ʌ -1 n ∖
k∑σj ∙δ∙∑√σj Dj)+ k+1 工σj -k∑σj	∙δ∙∑√σj Dj)
j=1	j=1	∖	j=1	j=1	)	j=1
1 (E -1、2 λ2	(	1	(E -1、2	1 (E 一 1、2、∕v2l	Cc
-2k(Nσj ) ∙ δ - (Wly(N% ) - 2k(Nσj ) ∣ δ ʃ	(36)
{( k	k+1	k ∖	k	(	k + 1	k	∖	)
1	-1	1	-1	-1	1	-12	1	-12	2
(E∑σj - ⅛∑σj ∣ ∙δ ∙ ∑(σj I)j)- (2(l+iy(∑σj ) - 2k(Aσj ) ∣ δ ʃ
(37)
{/ 1 k+1	1 k	∖	k	/	1	k+1	Ik	∖	)
(k⅛ Eσ-1-1 Eσ-1)∙ δ ∙ Eσ-1∙ δ k - (2⅛)(∑σ-1)2 -焉(E σ-1)2∣ δ2
∖	j=1	j=1	)	j=1	∖	j=1	j=1	)	)
(38)
λk∙exp <
k+1	k
E σ-1-j E σ-1∣
j=1	j=1	)
________ - /
{^^^^^^^^^^^^^^^^^^^^^^^
Op(k-1)
k
Eσ-1∆k ×∆
j=1
`---7----'
Op(k1∕2) by (30)
(2⅛7
k+1	k
(E σ71)2 - 2k(E σ-1)
j=1	j=1
^^^^"^^^{^^^^^^^^^^^^^
OP(I)
2∣ ×δ2 >
}
(39)
（六
×
—
λk ∙ exp{op(1)}
(40)
12
Under review as a conference paper at ICLR 2021
Table 3: Estimated type I error or power for SUBTLE and SST with batch size 40
Model	I		II		In		IV		V	
c	SUBTLE	SST	SUBTLE	SST	SUBTLE	SST	SUBTLE	SST	SUBTLE	SST
-1	0.003	0.662	0.000	0.588	0.000	0.219	0.000	0.368	0.002	0.006
0	0.012	0.126	0.002	0.023	0.003	0.077	0.002	0.023	0.006	0.034
0.6	0.297	0.552	0.465	0.549	0.326	0.397	0.414	0.451	0.585	0.216
0.8	0.633	0.837	0.868	0.896	0.680	0.703	0.826	0.816	0.931	0.373
1	0.901	0.969	0.993	0.995	0.947	0.933	0.985	0.978	0.999	0.715
Table 4: Estimated type I error and power for SUBTLE and SST with varying mixture density
variance
τ2	0.0001	0.001	0.01	0.1	1	10
Type I error	0.003	0.024	0.021	0.016	0.005	0.002
Power	0.887	0.976	0.956	0.932	0.892	0.825
A.2 Additional results
A.2.1 Hyperparameters
There are three hyperparameters in our algorithm: batch size m, variance of mixture density τ2, and
failure time M. We did not tune these hyperparameters in our experiments, but used the same value
for SST and SUBTLE. In the following, we will expound the effects of these hyperparameters on
the performance of our tests and provide additional simulation results.
Apart from batch size 20 in Section 4.1, we also conduct experiments with batch size 40 under the
same setting. The results are shown in Table 3. It seems that there is considerable robustness in
choosing batch size.
In theory the choice of mixture density variance τ2 will not have any effect on the type I error
control. Johari et al. (2015) proved that an optimal τ2 in terms of stopping time is the prior variance
times a correction for truncating. It is the reason that we suggest using historical data to estimate
the variance of value difference ∆. Besides, we conduct simulations with varying τ2 . The data is
generated from Model I in Table 1 with c = 0 or c = 1. When c = 0 we estimate the type I error,
while when c = 1 we estimate the power. The results in Table 4 show that the type I error is always
controlled below significance level 0.05 and the power has considerable robustness to the choice of
τ2.
As we mentioned in future work, how to choose the optimal failure time M is still a problem. The
larger the failure time, the higher power we have to detect the difference since we collect more
samples. However, large failure time also means long waiting time and high opportunity cost. Thus,
there is a trade off between waiting time and power.
A.2.2 Optimal treatment rule for Yahoo data
Figure 3 gives the decision tree of the estimated optimal treatment rule. Each left branch contains
the subpopulation whose covariates satisfy the conditions on its parent node. The classification 0/1
on each leaf node indicates the optimal treatment rule for corresponding subpopulation, and the two
values separated by slash gives the number of users who ”truly” (estimated by random forest) benefit
from control and treatment.
13
Under review as a conference paper at ICLR 2021
Figure 3: The decision tree of optimal treatment rule for Yahoo data
14