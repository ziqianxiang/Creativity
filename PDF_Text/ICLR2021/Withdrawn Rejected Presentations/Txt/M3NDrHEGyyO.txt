Under review as a conference paper at ICLR 2021
Accelerating Safe Reinforcement Learning
with Constraint-mismatched Policies
Anonymous authors
Paper under double-blind review
Ab stract
We consider the problem of reinforcement learning when provided with (1) a
baseline control policy and (2) a set of constraints that the controlled system
must satisfy. The baseline policy can arise from a teacher agent, demonstration
data or even a heuristic while the constraints might encode safety, fairness or
other application-specific requirements. Importantly, the baseline policy may be
sub-optimal for the task at hand, and is not guaranteed to satisfy the specified
constraints. The key challenge therefore lies in effectively leveraging the baseline
policy for faster learning, while still ensuring that the constraints are minimally
violated. To reconcile these potentially competing aspects, we propose an iterative
policy optimization algorithm that alternates between maximizing expected return
on the task, minimizing distance to the baseline policy, and projecting the policy
onto the constraint-satisfying set. We analyze the convergence of our algorithm
theoretically and provide a finite-time guarantee. In our empirical experiments on
five different control tasks, our algorithm consistently outperforms several state-of-
the-art methods, achieving 10 times fewer constraint violations and 40% higher
reward on average.1
1	Introduction
Deep reinforcement learning (RL) has achieved superior performance in several domains such as
games (Mnih et al., 2013; Silver et al., 2016) and robotic control (Levine et al., 2016; Rajeswaran
et al., 2017). However, in these complex applications, learning policies from scratch often requires
tremendous amounts of time and computation power. To alleviate this issue, one would like to
leverage a baseline policy available from a teacher or a previous task. However, the baseline policy
may be sub-optimal for the new application and is not guaranteed to produce actions that satisfy given
constraints on safety, fairness, or other costs. For instance, when you drive an unfamiliar vehicle,
you do so cautiously to ensure safety, while at the same time you adapt your driving technique to
the vehicle characteristics to improve your ‘driving reward’. In effect, you (as the agent) gradually
adapt a baseline policy (i.e., prior driving skill) to avoid violating the constraints (e.g., safety) while
improving your driving reward (e.g., travel time, fuel efficiency).
This problem is challenging because directly leveraging the baseline policy, as in DAGGER (Ross
et al., 2011) or GAIL (Ho & Ermon, 2016), may result in policies that violate the constraints since
the baseline is not guaranteed to satisfy them. To ensure constraint satisfaction, prior work either
adds a hyper-parameter weighted copy of the imitation learning (IL) objective (i.e., imitating the
baseline policy) to the RL objective (Rajeswaran et al., 2017; Gao et al., 2018; Hester et al., 2018), or
pre-trains a policy with the baseline policy and then fine-tunes it through RL (Mulling et al., 2013;
Chernova & Thomaz, 2014). Both these approaches incur the cost of weight tuning to satisfy the cost
constraint and do not ensure constraint satisfaction during training.
In this work, to learn from the baseline policy while satisfying constraints, we propose an iterative
algorithm that performs policy updates in three stages. The first step updates the policy to maximize
expected reward using trust region policy optimization (e.g., TRPO (Schulman et al., 2015)). This
can, however, result in a new intermediate policy that is too far from the baseline policy and one that
may not satisfy the constraints. The second step performs a projection in policy space to control
1Code is available at: https://sites.google.com/view/spacealgo.
1
Under review as a conference paper at ICLR 2021
Trust region
Ck+
2	⅞.3.πk
k+3	CθSt
π .然+1
Region around ~ constraint
行	set
★ ]
Reward '
optimal
(b)
∖ ;
Cost
constraint
set
(c)
(a)
Figure 1: (a) Update procedures for SPACE. Step 1 (green) improves the reward in the trust region. Step 2
(blue) projects the policy onto a region around the baseline policy πB. Step 3 (red) projects the policy onto
the constraint set. (b) Illustrating when πB is outside the constraint set. (c) Illustrating when πB is inside the
constraint set. The highest reward is achieved at the yellow star.
the distance between the current policy and the baseline policy. This distance is updated each
episode depending on the reward improvement and constraint satisfaction, allowing the learning
algorithm to explore without being overly restricted by the (potentially constraint-violating) baseline
policy (Rajeswaran et al., 2017). This also enables the baseline policy to influence the learning
without the computational burden of learning a cost function for the baseline policy (Kwon et al.,
2020). The third step ensures constraint satisfaction at every iteration by performing a projection
onto the set of policies that satisfy the given constraints. This ensures recovery from infeasible (i.e.,
constraint-violating) states (e.g., due to approximation errors), and eliminates the need for tuning
weights for auxiliary cost objective functions (Tessler et al., 2018). We call our algorithm Safe Policy
Adaptation with Constrained Exploration (SPACE).
This paper’s contributions are two-fold. We first analyze our proposed SPACE algorithm and
provide a finite-time guarantee for its convergence. We also provide an analysis of controlling the
distance between the learned policy at iteration k and the baseline policy to ensure both feasibility
of the optimization problem and safe exploration by the learning agent. Second, we empirically
compare SPACE with state-of-the-art algorithms on five different control tasks, including two Mujoco
environments with safety constraints from Achiam et al. (2017), two challenging traffic management
tasks with fairness constraints from Vinitsky et al. (2018), and one human demonstration driving task
with safety constraints from Brockman et al. (2016). In all tasks, SPACE outperforms the state-of-the
art safe RL algorithm, projection-based constrained policy optimization (PCPO) in Yang et al. (2020),
averaging 40% more reward with 10 times fewer cost constraint violations. This shows that SPACE
leverages the baseline policy to achieve better learning efficiency while satisfying the cost constraint.
2 Related Work
Policy optimization with constraints. Learning constraint-satisfying policies has been explored in
the context of safe RL (Garcia & Fernandez, 2015), (Hasanbeig et al., 2020; Junges et al., 2016;
Jansen et al., 2020). Prior work either uses a conditional-gradient type of approach (Achiam et al.,
2017), adds a weighted copy of the cost objective in the reward function (Tessler et al., 2018; Chow
et al., 2019; Fujimoto et al., 2019; Stooke et al., 2020), adds a safety layer to the policy (Dalal
et al., 2018) (Avni et al., 2019), or concerns about the chanced constraints (Fu & Prashanth L,
2018; Zheng & Ratliff, 2020). Perhaps the closest work to ours is Projection-based Constrained
Policy Optimization (PCPO) (Yang et al., 2020), which also uses projections in policy space to
ensure constraint satisfaction. However, PCPO does not have the capability to safely exploit prior
information (through a baseline policy). The lack of using prior policies in PCPO makes it sample-
inefficient. In addition, our SPACE algorithm’s update dynamically sets distances between policies
while PCPO does not. This update is important to effectively and safely learn from the baseline policy.
Furthermore, we provide a safety guarantee to ensure the feasibility of the optimization problem
while PCPO does not. Merely adding an IL objective in the reward objective of PCPO cannot learn
efficiently, as shown in experiments. This analysis allows us to advance towards the practical use of
RL in real applications, which PCPO and other algorithms have never done before.
Policy optimization with the initial safe set. Wachi & Sui (2020); Sui et al. (2015); Turchetta et al.
(2016) assume that the initial safe set is given, and the agent explores the environment and verifies the
safety function from this initial safe set. There is no baseline policy here. In contrast, our assumption
is to give a baseline policy to the agent. Both assumptions are reasonable as they provide an initial
understanding of the environment.
2
Under review as a conference paper at ICLR 2021
Algorithm	Properties	Approach
DAGGER (Ross et al., 2011)	⑤ f√图图	Optimize IL objective
DAPG (Rajeswaran et al., 2017)	& §国图	Optimize RL and IL objectives jointly
NAC (Gao et al., 2018)	⑤ § 0	Pre-train with IL and find-tune with RL
IC-GAIL (Wu et al., 2019)	⑥ § 0	Generative adversarial IL
CPO (Achiam et al., 2017)	© △	Safe RL solved by a line search
RCPO (Tessler et al., 2018)	①△	Safe RL solved by a Lagrangian approach
PCPO (Yang et al., 2020)		Safe RL solved by a projection
PID-RL (Stooke et al., 2020)		Safe RL solved by PID control
SPACE (ours)	© AQ @	Safe RL aided by baseline policies with analysis
m maximize reward, A constraint-cost satisfaction, 0 on-policy imitation, § off-policy imitation,		
图图 w/ expert/optimal demonstrations,图 w/ suboptimal demonstration		
Table 1: Comparison to prior work.		
Leveraging baseline policies for RL. Prior work has used baseline policies to provide initial infor-
mation to RL algorithms to reduce or avoid undesirable situations. This is done by either: initializing
the policy with the baseline policy (Driessens & Dzeroski, 2004; Smart & Kaelbling, 2000; KoPPejan
& Whiteson, 2011; Abbeel et al., 2010; Gao et al., 2018; Le et al., 2019; Vecerik et al., 2017; Jaques
et al., 2019); or providing a teacher,s advice to the agent (Garcia & Fernandez, 2012; QuintIa Vidal
et al., 2013; Abel et al., 2017; Zhang et al., 2019). However, such works often assume that the
baseline policy is constraint-satisfying (Sun et al., 2018; Balakrishna et al., 2019). In contrast, our
SPACE algorithm safely leverages the baseline policy without requiring it to satisfy the specified
constraints.
Pathak et al. (2015); Bartocci et al. (2011) also modify the existing known models (policies) based
on new conditions in the context of the formal methods. In contrast, we solve this problem using
projections in the policy space.
Learning from logged demonstration data. To effectively learn from demonstration data given
by the baseline policy, Wu et al. (2019); Brown et al. (2019); Kwon et al. (2020) assess the demon-
stration data by either: predicting their cost in the new task using generative adversarial networks
(GANs) (Goodfellow et al., 2014); or directly learning the cost function of the demonstration data.
However, these approaches require a large number of training samples from the new task. In addition,
the learned cost function is not guaranteed to recover the true one. This may result in driving the
agent to undesirable situations. In contrast, SPACE controls the distance between the learned and
baseline policies to ensure reward improvement and constraint satisfaction.
Table 1 catalogs related work on safe RL and IL. We can differentiate these methods on several
key axes: (1) whether they optimize reward and/or ensure cost satisfaction, (2) whether they use
on-policy methods (i.e., query the baseline policy) or off-policy methods (i.e., learn from a batch of
demonstration data), (3) whether the baseline policy or demonstration data is optimal for the agent’s
own objective, and (4) the technical approach of the work.
3 Problem Formulation
We frame our problem as a constrained Markov Decision Process (CMDP) (Altman, 1999), defined
as a tuple < S , A, T, R, C > . Here S is the set of states, A is the set of actions, and T specifies the
conditional probability T(s0|s, a) that the next state is s0 given the current state s and action a. In
addition, R : S × A → R is a reward function, and C : S × A → R is a constraint cost function.
The reward function encodes the benefit of using action a in state s, while the cost function encodes
the corresponding constraint violation penalty.
A policy is a map from states to probability distributions on A. It specifies that in state s the selected
action is drawn from the distribution π(s). The state then transits from s to s0 according to the state
transition distribution T (s0|s, a). In doing so, a reward R(s, a) is received and a constraint cost
C(s, a) is incurred, as outlined above.
3
Under review as a conference paper at ICLR 2021
Let Y ∈ (0,1) denote a discount factor, and T denote the trajectory T = (so, a0, si, ∙∙∙) induced by a
policy π. Normally, we seek a policy π that maximizes a cumulative discounted reward
Jr(∏)= Eτ~∏[P∞=o YtR(St,at)],	(1)
while keeping the cumulative discounted cost below hC
JC(∏) = Eτ~∏ [P∞=0 YtC(st,at)] ≤ he.	⑵
Here we consider an additional objective. We are provided with a baseline policy πB and at each state
s we measure the divergence between π(s) and πB (s). For example, this could be the KL-divergence
D(s) =. DKL(π(s)kπB(s)). We then seek a policy that maximizes Eq. (1), satisfies Eq. (2), and
ensures the discounted divergence between the learned and baseline policies is below hD :
Jd(∏) = Eτ~∏ [P∞=0 YtD(st)] ≤ hD.	(3)
We do not assume that the baseline policy satisfies the cost constraint. Hence we allow hD to be
adjusted during the learning of π to allow for reward improvement and constraint satisfaction.
Let μt(∙∣∏) denote the state distribution at time t under policy ∏. The discounted state distribution
induced by π is defined to be dπ(s) = (1 - Y) P∞=0 γtμt(s∣π). Now bring in the reward advantage
function (Kakade & Langford, 2002) defined by
∞	AπR(s,a)=. QπR(s,a)-VRπ(s),
where V∏ (s) = ET~∏ [£∞ YtR(st, at)∣so = s] is the expected reward from state S under policy
π, and QR(s, a) = ET~∏ [P∞=o YtR(St, at)∣so = s,ao = a] is the expected reward from state S and
initial action a, and thereafter following policy π. These definitions allow us to express the reward
performance of one policy π0 in terms of another π:
JRg- jr(∏) = 1-γEs~d∏0,a~∏0[AR(s,a)].
Similarly, we can define AπD(s, a), QπD (s, a) and VDπ (s) for the divergence cost, and AπC (s, a),
QπC (s, a) and VCπ (s) for the constraint cost.
4 Safe Policy Adaptation with Constrained Exploration (SPACE)
We now describe the proposed iterative three step algorithm illustrated in Fig. 1. In what follows, πk
denotes the learned policy after iteration k, and M denotes a distance measure between policies. For
example, M might be the 2-norm of the difference of policy parameters or some average over the
states of the KL-divergence of the action policy distributions.
Step 1. We perform one step of trust region policy optimization (Schulman et al., 2015). This
maximizes the reward advantage function AπR(s, a) over a KL-divergence neighborhood of πk:
∏k+3 = arg max E ∏k	[AR (s, a)] s.t. ES ~d∏k [DκL(∏(s)k∏k (s))] ≤ δ. (4)
∏	s~d ,a~π	s S 」
Step 2. We project ∏k+1 onto a region around ∏b controlled by hD to minimize M:
∏k+3 = arg min M(π,πk+3) s.t. Jd (πk) + 六E k	[ADk (s)] ≤ hD.	(5)
π	γ s~d , a~π
Step 3. We project πk+1 onto the set of policies satisfying the cost constraint to minimize M :
∏k+i = arg min M(∏,∏k+ 1) s.t. Je (πk) + 六E k [ACCk (s,a)] ≤ he.	(6)
π	γ s~d , a~π
Control hkD in Step 2. We select h0D to be small and gradually increase hkD at each iteration to
expand the region around πB . Specifically, we make hkD+1 > hkD if:
(a)	Je(πk) > Je (πk-1): this increase is to ensure a nonempty intersection between the region
around πB and the cost constraint set (feasibility). See Fig. 1(b).
(b)	JR(πk) < JR(πk-1): this increase gives the next policy more freedom to improve the
reward and the cost constraint performance (exploration). See Fig. 1(c).
4
Under review as a conference paper at ICLR 2021
It remains to determine how to set the new value of hkD+1. Let U1 denote the set of policies satisfying
the cost constraint, and U2k denote the set of policies in the region around πB controlled by hkD . Then
we have the following Lemma.
Lemma4.1 (UpdatinghD). Ifatstepk+1: h/ ≥ O((JC(πk)-hc)2) + hD,thenUι∩Uk+1 = 0
(feasibility) and Uk+1 ∩ ∂Ui = 0 (exploration).
Proof. See the supplementary material.	□
Lemma 4.1 ensures that the boundaries of the region around πB determined by hD and the set of
policies satisfying the cost constraint intersect. Note that hD will become large enough to guarantee
feasibility during training. This allows the learning algorithm to explore policies within the cost
constraint set while still learning from the baseline policy.
5 A Theoretical Analysis of SPACE
We will implement a policy as a neural network with fixed architecture parameterized by θ ∈
Rn. We then learn a policy from the achievable set {∏(∙∣θ): θ ∈ Rn} by iteratively learn-
ing θ. Let θk and ∏k = ∏(∙∣θk) denote the parameter value and the corresponding policy
at step k. In this setting, it is impractical to solve for the policy updates in Eq. (4), (5) and
(6). Hence we approximate the reward function and constraints with first order Taylor expan-
sions, and KL-divergence with a second order Taylor expansion. We will need the follow-
ing derivatives:⑴ gk = VeE 尸	[AR(s,a)], (2) ak = VeE ∏k	[ADk(s)], (3)
S〜d , a〜π	S〜d , a〜π
ck = VeES〜k a〜π[ACCk(s,a)], and (4) Fk = VθES〜犷[DκL(∏(s)∣∣∏k(s))] . EaCh of these
derivatives are taken w.r.t. the neural network parameter and evaluated at θk. We also define
bk = JD (πk) - hkD, and dk = JC (πk) - hC Let uk = J。=浅—1^, and L = I for the 2-norm
projection and L = Fk for the KL-divergence projection.
We first approximate Eq. (4), Eq. (5) and Eq. (6) using the following approximations:
Step 1. Approximating Eq. (4) yields
θk+1 = argmax gkT(θ - θk) s.t. 1(θ - θk)TFk(θ - θk) ≤ δ.	(7)
e2
Step 2 and Step 3. Approximating Eq. (5) and (6), similarly yields
θk+ 2 = argmin 1(θ - θk+ 3 )TL(θ - θk+3) s.t. akT(θ - θk) + bk ≤ 0,	(8)
e2
θk+1 = argmin 1(θ - θk+ 3 )TL(θ - θk+2) s.t. ckT(θ - θk) + dk ≤ 0,	(9)
e2
where L = I for the 2-norm projection and L = F k for the KL-divergence projection. We solve
these problems using using convex programming (see the supplementary material for the derivation),
then for each policy update, we have
θk+i = θk + UkFk-1gk - maχ(o, UkakTFkTgk + b R-'k
ak L-1ak	(10)
UkckTFk-1gk +dk	1 k
—max(0,---------------) L 1ck.
ckTL-1ck
Algorithm 1 shows the corresponding pseudocode.
Convergence analysis. We consider the following simplified problem to provide a finite-time
guarantee of SPACE:
min f(θ),	(11)
e∈C1 ∩C2
where f : Rn → R is a twice continuously differentiable function at every point in a open set
X ⊆ Rn, and C1 ⊆ X and C2 ⊆ X are compact convex sets with C1 ∩ C2 6= 0. The function f is the
5
Under review as a conference paper at ICLR 2021
negative reward function of our CMDP, and the two constraint sets represent the cost constraint set
and the region around πB .
For a vector x, let kxk denote the Euclidean norm. For a matrix M let kM k denote the induced
matrix 2-norm, and σi(M) denote the i-th largest singular value of M.
Assumption 1. We assume:
(1.1)	The gradient Vf is L-LiPschitz continuous over a open set X.
(1.2)	For some constant G, ∣∣Vf(θ)k ≤ G.
(1.3)	For some constant H, diam(C1) ≤ H and diam(C2) ≤ H.
Assumptions (1.1) and (1.2) ensure that the gradient can not change too rapidly and the norm of the
gradient can not be too large. Assumption (1.3) implies that for every iteration, the diameter of the
region around πB is bounded above by H .
We will need a concept of an -first order stationary point (Mokhtari et al., 2018). For > 0, we say
that θ* ∈ Ci ∩ C2 an e-first order stationary point (e-FOSP) of Problem (11) under KL-divergence
projection if
Vf(θ*)T(θ — θ*) ≥ -e, ∀θ ∈C1 ∩C2.	(12)
Similarly, under the 2-norm projection, θ* ∈ Ci ∩ C2 an e-FOSP of (11) if
Vf (θ*)TF*(θ — θ*) ≥ -e, ∀θ ∈C1 ∩ C2,	(13)
where F * = Vθ Es〜d∏* [Dkl(∏(s)∣∣∏*(s))] . Notice that SPACE converges to distinct stationary
points under the two possible projections (see the supplementary material). With these assumptions,
we have the following Theorem.
Theorem 5.1 (Finite-Time Guarantee of SPACE). Under the KL-divergence projection, there
exists a sequence {ηk} such that SPACE converges to an e-FOSP in at most O(e-2) iterations.
Moreover, at step k + 1
f(θk+i) ≤ f(θk)-	L1(F J.	(14)
ηk
Similarly, under the 2-norm projection, there exists a sequence {ηk} such that SPACE converges to
an e-FOSP in at most O(e-2) iterations. Moreover, at step k + 1
f(θk+1) ≤ f(θk) - 2(Gσι(FL-2i) + H)2 .	(15)
Proof. The proof and the sequence {ηk } are given in the supplementary material.	口
We now make several observations
for Theorem 5.1.
(1)	The smaller H is, the greater the
decrease in the objective. This ob-
servation supports the idea of start-
ing with a small value for hD and
increasing it only when needed.
(2)	Under the KL-divergence projec-
tion, the effect of σi (Fk) is negligi-
ble. This is because in this case ηk
Algorithm 1 SPACE
Initialize a policy ∏0 = ∏(∙∣θ0) and a trajectory buffer B
for k = 0,1, 2,•…do
Run ∏k = ∏(∙∣θk) and store trajectories in B
Compute g, a, c, F, b and d using B
Obtain θk+i using the update in Eq. (10)
if JC(πk) > JC(πk-i) or JR(πk) < JR(πk-i) then
Update hkD+i using Lemma 4.1
Empty B
is determined by the KL-divergence between two consecutive updated policies (see the supplementary
material). This implies that ηk is proportional to σi(Fk). Hence σi(Fk) does not play a major role
in decreasing the objective value.
(3) Under the 2-norm projection, the smaller σi(Fk- ) (i.e., larger σn(Fk)) is, the greater the
decrease in the objective. This is because a large σn(Fk) means a large curvature of f in all
directions. This implies that the 2-norm distance between the pre-projection and post-projection
points is small, leading to a small deviation from the reward improvement direction after doing
projections (see the supplementary material for a visualization).
6
Under review as a conference paper at ICLR 2021
(a) Gather (b) Circle (c) Grid (d) Bottleneck (e) Car-racing (f) Demo.
Figure 2: (a) Gather task: the agent is rewarded for gathering green apples, but is constrained to collect a limited
number of red apples (Achiam et al., 2017). (b) Circle task: the agent is rewarded for moving in a specified wide
circle, but is constrained to stay within a safe region smaller than the radius of the circle (Achiam et al., 2017).
(c) Grid task: the agent controls the traffic lights in a grid road network and is rewarded for high throughput,
but is constrained to let lights stay red for at most 7 consecutive seconds (Vinitsky et al., 2018). (d) Bottleneck
task: the agent controls a set of autonomous vehicles (shown in red) in a traffic merge situation and is rewarded
for achieving high throughput, but constrained to ensure that human-driven vehicles (shown in white) have low
speed for no more than 10 seconds (Vinitsky et al., 2018). (e) Car-racing task: the agent controls an autonomous
vehicle on a race track and is rewarded for driving through as many tiles as possible, but is constrained to use the
brakes at most 5 times to encourage a smooth ride (Brockman et al., 2016). (f) A human player plays car-racing
with demonstration data logged.
6 Experiments
Tasks. We compare the proposed algorithm with existing approaches on five control tasks: three
tasks with safety constraints ((a), (b) and (e) in Fig. 2), and two tasks with fairness constraints ((c)
and (d) in Fig. 2). These tasks are briefly described in the caption of Fig. 2. We chose the traffic
management tasks since a good control policy can benefit millions of drivers. In addition, we chose
the car-racing task since a good algorithm should safely learn from baseline human policies. For all
the algorithms, we use neural networks to represent Gaussian policies. We use the KL-divergence
projection in the Mujoco and car-racing tasks, and the 2-norm projection in the traffic management
task since it achieves better performance. We use a grid-search to select for the hyper-parameters.
See Appendix for more experimental details.
Baseline algorithms. We compare SPACE with five baseline methods.
(1)	Fixed-point Constrained Policy Optimization (f-CPO). In f-CPO, the weight λ is fixed followed
by a CPO update (Achiam et al., 2017). The f-CPO policy update solves:
θk+1 = argmax(gk + λak)T(θ - θk) s.t.ɪ(θ - θk)TFk(θ - θk) ≤ δ, CkT(θ - θk) + dk ≤ 0.
θ
f-CPO adds the divergence objective in the reward function. The lack of weight-adjustment makes it
susceptible to suboptimal πB .
(2)	Fixed-point PCPO (f-PCPO). In f-PCPO, the weight λ is fixed followed by a PCPO update (Yang
et al., 2020). The f-PCPO policy update solves:
θk+ 1 = arg max (gk + λak)T(θ - θk) s.t. ɪ (θ 一 θk )TFk (θ 一 θk) ≤ δ,
θ
θk+1 = arg min 2 (θ - θk+ 1 )TL(θ - θk+ 2) s.t. ckT(θ - θk) + dk ≤ 0.
θ
f-PCPO also adds the divergence objective in the reward function, but followed by a PCPO update
(3) Dynamic-point Constrained Policy Optimization (d-CPO). The d-CPO update solves f-CPO
problem with a stateful λk+1 = (λk)β, where 0 < β < 1. This is inspired by Rajeswaran et al.
(2017), in which they have the same weight-scheduling method to adjust λk.
(4)	Dynamic-point PCPO (d-PCPO). The d-PCPO update solves f-PCPO problem with a stateful
λk+1 = (λk)β, where 0 < β < 1. d-PCPO also uses the same method to learn from πB as in d-CPO.
For all the experiments and the algorithms, the weight is fixed and it is set to 1.
Baseline policies πB. To test whether SPACE can safely and efficiently leverage the baseline policy,
we used three variants of the baseline policies: (1) suboptimal πBcost with JC(πBcost) ≈ 0, (2)
suboptimal πBreward with JC(πBreward) > hC, and (3) πBnear with JC(πBnear) ≈ hC (i.e., the baseline
policy has the same cost constraint as the agent). Note that these πB have different degrees of
constraint satisfaction. This is to examine (1) whether SPACE can achieve better learning efficiency
given the constraint-satisfying πB (i.e., πBcost and πBnear), and (2) whether SPACE can safely learn
from constraint-violating πB (i.e., πBreward). In addition, in the car-racing task we pre-train a πB
using an off-policy algorithm (DDPG (Lillicrap et al., 2016)), which directly learns from human
demonstration data (Fig. 2(f)). This suboptimal human baseline policy is denoted by πBhuman . This is
7
Under review as a conference paper at ICLR 2021
Bottleneck	Car-racing	Grid
0	50 100 150 200 250 300 350 400
# of policy updates
0	50	100 150 200 250 300
# of policy updates
6 4 2 0 8 6 4
6 6 6 6 5 5 5
・us!a
[**s*4vJ⅜-ιt
.......
0	25 50 75 100 125 150 175 200
# of policy updates
# of policy updates
# of policy updates
0 50 100 150 200 250 300 350 400
# of policy updates
-θ- SPACE (Ours) -Q-^ f-CPO -θ- f- PCPO -⅛- d-CPO -θ- d - PCPO F- PCPO (Yang et al.)
0	50 100 150 200 250 300
# of policy updates
Figure 3: The discounted reward, and the undiscounted constraint cost, the undiscounted divergence cost over
policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded area is the standard
deviation over 5 runs. The baseline policies in the grid and bottleneck tasks are πBnear , and the baseline policy in
the car-racing task is πBhuman . The black dashed line is the cost constraint threshold hC . Overall, we observe
that SPACE is the only algorithm that satisfies the constraints while achieving superior reward performance in
all cases. Although πBhuman has substantially low reward, SPACE still can learn to improve the reward. (We
only show the results in these tasks as representative cases since these tasks are more challenging. Please read
Appendix for more results. Best viewed in color.)
# of policy updates

he ■ ■ ■ Jr(ttb)
to simulate that πB comes from a heuristic or human demonstration data, which are far from optimal.
The detailed implementation of updating hD can be found in Appendix E.1.
Overall performance. The learning curves of the discounted reward (JR(π)), the undiscounted
constraint cost (JC(π)), and the undiscounted divergence cost (JD(π)) over policy updates are shown
for all tested algorithms and tasks in Fig. 3. We use πBnear in bottleneck and grid tasks, and πBhuman in
car-racing task. Note that πBhuman is highly suboptimal to the agent (i.e., JR(πBhuman) is small). The
value of the reward is only around 5 as shown in the plot. It does not solve the task at hand. Overall,
we observe that (1) SPACE achieves at least 2 times faster cost constraint satisfaction in all cases
even learning from πBhuman. (2) SPACE achieves at least 10% more reward in the bottleneck and
car-racing tasks compared to the best baseline, and (3) SPACE is the only algorithm that satisfies the
cost constraints in all cases. In contrast, PCPO agent cannot robustly satisfy the cost constraints. This
is because using the baseline policy can provide additional information about the safety constraints.
For example, when using πBcost that is only optimized for the safety constraints specified by the
environment, we can observe the agent trained with SPACE satisfies the constraints more. This shows
that using the baseline policy could also help to improve the cost performance.
For example, in the car-racing task we observe that JD(π) in SPACE decreases at the initial iteration,
but increases in the end. This implies that the learned policy is guided by the baseline policy πBhuman
in the beginning, but use less supervision in the end. In addition, in the grid task we observe that the
final reward of SPACE is lower than the baseline algorithm. This is because that SPACE converges to
a policy in the cost constraint set, whereas the baseline algorithms do not find constraint-satisfying
policies. These observations show that SPACE can robustly ensure constraint satisfaction while
achieving fast learning aided by the potentially suboptimal baseline policy.
f-CPO and f-PCPO. f-CPO and f-PCPO fail to improve the reward and have more cost violations.
Most likely this is due to persistent supervision from the baseline policy which need not satisfy the
cost constraints nor have high reward. For example, in the car-racing task we observe that the value
8
Under review as a conference paper at ICLR 2021
of the divergence cost decreases throughout the training. This implies that the learned policy overly
evolves to the sub-optimal baseline policy and hence degrades the reward performance.
d-CPO and d-PCPO. d-CPO and d-PCPO improve the reward slowly and have more cost violations.
They do not use projection to quickly learn from πB . For example, in the car-racing task JD (π)
in d-CPO and d-PCPO are high compared to SPACE throughout the training. This suggests that
simply adding an IL objective in the reward objective is susceptible to a suboptimal πB . Hence these
approaches require a good selection of the initial value of λk and β.
Suboptimal πBcost and πBreward.	The learning curves of JC (π) and the JR(π) over pol-
icy updates are shown for the point gather task in Fig. 4. We use two suboptimal base-
line policies: πBcost and πBreward, and hC is set to 0.5. They do
We observe that SPACE robustly sat-
isfies the cost constraints in all cases
even when learning from πBreward .
In addition, we observe that learn-
ing guided by πBreward achieves faster
reward learning efficiency at the
initial iteration. This is because
JR (πBreward) > JR(πBcost) as seen in
the reward plot. Furthermore, we ob-
serve that learning guided by πBcost
achieves faster reward learning effi-
ciency at the later iteration. This
is because that by starting in the in-
not solve the task at hand.
200 400 600 800 1000 1200
# of policy updates
0	200 400 600 800 1000 1200
# of policy updates
-θ- SPACE With 脂OSt 今(SPACE wjth 喑Ward 一 一 生
◎(喀St) 头 Jd嘈Ward) .Q1 j^πcost)(喑Ward)
Figure 4: Learning from SuboPtimaI ∏b . The undiscounted Con-
straint cost and the discounted reward over policy updates for the
point gather task. The solid line is the mean and the shaded area
terior of the cost constraint set (i.e.,
JC (πBcost) ≈ 0 ≤ hC), the agent can
safely exploit the baseline policy. The
results show that SPACE enables fast
is the standard deviation over 5 runs. The black dashed line is the
cost constraint threshold hC. We observe that SPACE satisfies the
cost constraints even when learning from the suboptimal πB . (Best
viewed in color.)
convergence to a constraint-satisfying policy, even if πB does not meet the constraints itself or does
not optimize the reward.
7 Conclusion
We address the problem of learning constraint-satisfying policies given a baseline policy from either
a teacher agent or demonstration data. The proposed algorithm effectively learns from the potentially
suboptimal baseline policy without violating the constraints. SPACE achieves superior reward and
cost performance compared with state-of-the-art approaches (i.e., PCPO). We further analyze the
convergence of SPACE and provide an effective approach to controlling the distance between the
learned and baseline policies. Future work will consider learning system dynamics to account for the
uncertainty of the environment and hence enable safe learning in real applications.
References
Pieter Abbeel, Adam Coates, and Andrew Y. Ng. Autonomous helicopter aerobatics through
apprentiCeShiP learning. The International Journal of Robotics Research, 29(13):1608-1639, 2010.
David Abel, John Salvatier, Andreas Stuhlmiiller, and OWain Evans. Agent-agnostic human-in-the-
loop reinforCement learning. arXiv PrePrint arXiv:1701.04079, 2017.
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the International Conference on Machine Learning, pp. 22-31, 2017.
Eitan Altman. Constrained Markov decision Processes, volume 7. CRC Press, 1999.
Guy Avni, Roderick Bloem, Krishnendu Chatterjee, Thomas A. Henzinger, Bettina Konighofer, and
Stefan Pranger. Run-time optimization for learned controllers through quantitative games. In
International Conference on ComPuter Aided Verification, pp. 630-649. Springer, 2019.
9
Under review as a conference paper at ICLR 2021
Ashwin Balakrishna, Brijen Thananjeyan, Jonathan Lee, Arsh Zahed, Felix Li, Joseph E. Gonzalez,
and Ken Goldberg. On-policy robot imitation learning from a converging supervisor. In Proceedings
of the Conference on Robot Learning, 2019.
Ezio Bartocci, Radu Grosu, Panagiotis Katsaros, CR Ramakrishnan, and Scott A. Smolka. Model
repair for probabilistic systems. In International Conference on Tools and Algorithms for the
Construction andAnalysis of Systems, pp. 326-340. Springer, 2011.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Daniel S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond
suboptimal demonstrations via inverse reinforcement learning from observations. In Proceedings
of the International Conference on Machine Learning, pp. 783-792, 2019.
Gong Chen and Marc Teboulle. Convergence analysis of a proximal-like minimization algorithm
using bregman functions. SIAM Journal on Optimization, 3(3):538-543, 1993.
Sonia Chernova and Andrea L. Thomaz. Robot learning from human teachers. Synthesis Lectures on
Artificial Intelligence and Machine Learning, 8(3):1-121, 2014.
Yinlam Chow, Ofir Nachum, Aleksandra Faust, Mohammad Ghavamzadeh, and Edgar Duenez-
Guzman. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
arXiv:1901.10031, 2019.
Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval
Tassa. Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757, 2018.
KUrt Driessens and Saso Dzeroski. Integrating guidance into relational reinforcement learning.
Machine Learning, 57(3):271-304, 2004.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In Proceedings of the International Conference on
Machine Learning, pp. 1329-1338, 2016.
Michael Fu and A. Prashanth L. Risk-sensitive reinforcement learning: A constrained optimization
viewpoint. arXiv preprint arXiv:1810.09126, 2018.
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch
deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.
Yang Gao, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning from
imperfect demonstrations. In Proceedings of the International Conference on Machine Learning,
2018.
Javier Garcia and Fernando Fernandez. Safe exploration of state and action spaces in reinforcement
learning. Journal of Artificial Intelligence Research, 45:515-564, 2012.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
Journal of Machine Learning Research, 16(1):1437-1480, 2015.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Cautious reinforcement
learning with logical constraints. arXiv preprint arXiv:2002.12156, 2020.
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In
Proceedings of the AAAI Conference on Artificial Intelligence, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
Information Processing Systems, pp. 4565-4573, 2016.
10
Under review as a conference paper at ICLR 2021
Nils Jansen, Bettina KOnighofer, Sebastian Junges, Alex Serban, and Roderick Bloem. Safe Re-
inforcement Learning Using Probabilistic Shields (Invited Paper). In Igor Konnov and Laura
KovacS (eds.), 31st International Conference on Concurrency Theory (CONCUR 2020), vol-
ume 171 of Leibniz International Proceedings in Informatics (LIPIcs), pp. 3:1-3:16, DagStUhL
Germany, 2020. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik. ISBN 978-3-95977-160-
3. doi: 10.4230/LIPIcs.CONCUR.2020.3. URL https://drops.dagstuhl.de/opus/
volltexte/2020/12815.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah
Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of
implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
Sebastian Junges, Nils Jansen, Christian Dehnert, Ufuk Topcu, and Joost-Pieter Katoen. Safety-
constrained reinforcement learning for mdps. In International Conference on Tools and Algorithms
for the Construction and Analysis of Systems, pp. 130-146. Springer, 2016.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
Proceedings of the International Conference on Machine Learning, pp. 267-274, 2002.
Rogier Koppejan and Shimon Whiteson. Neuroevolutionary reinforcement learning for generalized
control of simulated helicopters. Evolutionary intelligence, 4(4):219-241, 2011.
Minae Kwon, Erdem Biyik, Aditi Talati, Karan Bhasin, Dylan P. Losey, and Dorsa Sadigh. When
humans aren’t optimal: Robots that collaborate with risk-aware humans. In Proceedings of
ACM/IEEE International Conference on Human-Robot Interaction, 2020.
Hoang M. Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In
Proceedings of the International Conference on Machine Learning, pp. 3703-3712, 2019.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep
visuomotor policies. The Journal of Machine Learning Research, 17(1):1334-1373, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
Proceedings of the International Conference on Learning Representations, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained
optimization. In Advances in Neural Information Processing Systems, pp. 3629-3639, 2018.
Katharina MUlling, Jens Kober, Oliver Kroemer, and Jan Peters. Learning to select and generalize
striking movements in robot table tennis. The International Journal of Robotics Research, 32(3):
263-279, 2013.
Shashank Pathak, Enka Abraham, Nils Jansen, Armando Tacchella, and Joost-Pieter Katoen. A
greedy approach for the efficient repair of stochastic models. In NASA Formal Methods Symposium,
pp. 295-309. Springer, 2015.
Pablo Quintla Vidal, Roberto Iglesias Rodriguez, Miguel Angel Rodrlguez Gonzalez, and Carlos
Vazquez Regueiro. Learning on real robots from experience and simple user feedback. 2013.
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. In Proceedings of Robotics: Science and Systems, 2017.
Steiphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the International Conference on Artificial
Intelligence and Statistics, pp. 627-635, 2011.
11
Under review as a conference paper at ICLR 2021
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the International Conference on Machine Learning, pp.
1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. In Proceedings of the International
Conference on Learning Representations, 2016.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
William D. Smart and Leslie Pack Kaelbling. Practical reinforcement learning in continuous spaces.
In Proceedings of the International Conference on Machine Learning, pp. 903-910. Citeseer, 2000.
Adam Stooke, Joshua Achiam, and Pieter Abbeel. Responsive safety in reinforcement learning by
pid lagrangian methods. arXiv preprint arXiv:2007.03964, 2020.
Yanan Sui, Alkis Gotovos, Joel Burdick, and Andreas Krause. Safe exploration for optimization with
gaussian processes. In International Conference on Machine Learning, pp. 997-1005. PMLR,
2015.
Wen Sun, Geoffrey J. Gordon, Byron Boots, and Andrew J. Bagnell. Dual policy iteration. In
Advances in Neural Information Processing Systems, pp. 7059-7069, 2018.
Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
Proceedings of the International Conference on Learning Representations, 2018.
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause. Safe exploration in finite markov decision
processes with gaussian processes. In Advances in Neural Information Processing Systems, pp.
4312-4320, 2016.
Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess,
Thomas Rothorl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep
reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817,
2017.
Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu, Fangyu
Wu, Richard Liaw, Eric Liang, and Alexandre M. Bayen. Benchmarks for reinforcement learning
in mixed-autonomy traffic. In Proceedings of Conference on Robot Learning, pp. 399-409, 2018.
Akifumi Wachi and Yanan Sui. Safe reinforcement learning in constrained markov decision processes.
arXiv preprint arXiv:2008.06626, 2020.
Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama.
Imitation learning from imperfect demonstration. In Proceedings of the International Conference
on Machine Learning, pp. 6818-6827, 2019.
Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J. Ramadge. Projection-based
constrained policy optimization. In Proceedings of the International Conference on Learning
Representations, 2020.
Ruohan Zhang, Faraz Torabi, Lin Guan, Dana H. Ballard, and Peter Stone. Leveraging human guid-
ance for deep reinforcement learning tasks. In Proceedings of the International Joint Conference
on Artificial Intelligence, pp. 6339-6346, 2019.
Liyuan Zheng and Lillian J. Ratliff. Constrained upper confidence reinforcement learning. arXiv
preprint arXiv:2001.09377, 2020.
12
Under review as a conference paper at ICLR 2021
Supplementary Material for
Accelerating Safe Reinforcement Learning
with Constraint-mismatched Policies
Outline. Supplementary material is outlined as follows. Section A discusses the impact of
the proposed algorithm. Section B details the proof of updating hD in Lemma 4.1. Section
C describes the proof of analytical solution to SPACE in Eq. (10). Section D gives the proof
of finite-time guarantee of SPACE in Theorem 5.1 and discuss the difference between the
KL-divergence and 2-norm projections. Section E assembles the additional experiment results
to provide a detailed examination of the proposed algorithm compared to the baselines. These include:
•	evaluation of the discounted reward versus the cumulative undiscounted constraint cost to
demonstrate that SPACE achieves better reward performance with fewer cost constraint
violations,
•	evaluation of performance of SPACE guided by baseline policies with different JC (πB ) to
demonstrate that SPACE safely learns from the baseline policies which need not satisfy the
cost constraint,
•	ablation studies of using a fixed hD in SPACE to demonstrate the importance of using the
dynamic hD to improve the reward and cost performance,
•	comparison of SPACE and other annealing approaches to demonstrate that SPACE exploits
the baseline policy effectively,
•	comparison of SPACE under the KL-divergence and the 2-norm projections to demonstrate
that they converge to different stationary points,
•	evaluation of using different initial values of h0D to demonstrate that the selection of the
initial value does not affect the performance of SPACE drastically.
Section E also details the environment parameters, the architectures of policies, computational cost,
infrastructure for computation and the instructions for executing the code. Section F provides a
procedure for getting a baseline human policy. Finally, we fill the Machine Learning Reproducibility
Checklist in Section G.
A Impact of SPACE
Many autonomous systems such as self-driving cars and autonomous robots are complex. In order to
deal with this complexity, researchers are increasingly using reinforcement learning in conjunction
with imitation learning for designing control policies. The more we can learn from a previous
policy (e.g., human demonstration, previous applications), the fewer resources (e.g., time, energy,
engineering effort, cost) we need to learn a new policy. The proposed algorithm could be applied
in many fields where learning a policy can take advantage of prior applications while providing
assurances for the consideration of fairness, safety, or other costs. For example, in a dialogue system
where an agent is intended to converse with a human, the agent should safely learn from human
preferences while avoiding producing biased or offensive responses. In addition, in the self-driving
car domain where an agent learns a driving policy, the agent should safely learn from human drivers
while avoiding a crash. Moreover, in the personalized robotic assistant setting where an agent learns
from human demonstration, the agent should carefully imitate humans without damaging itself or
causing harm to nearby humans. These examples highlight the potential impact of the proposed
algorithm for accelerating safe reinforcement learning by adapting prior knowledge. This can open
the door to advances in lifelong learning and adaptation of agents to different contexts.
One deficiency of the proposed algorithm is that the agent still experiments with cost constraint
violation when learning control policies. This is because that any learning-based system needs to
experiment with various actions to find a constraint-satisfying policy. Even though the agent does not
violate the safety constraints during the learning phase, any change or perturbation of the environment
that was not envisioned at the time of programming or training may lead to a catastrophic failure
during run-time. These systems cannot guarantee that sensor inputs will not induce undesirable
13
Under review as a conference paper at ICLR 2021
Region
Region
around 冗 r>C ʌ.;;
★赤毡,(
Reward	后//；
optimal '、一 '
Yround πβ
产 bo^udary
Cost
ι constraint
∖ set
∖⅛
πbov.daryi
Reward ；
∖
optimal ∖
以πB
'J1D
\心+1	/Cost
'、、 //Constraint
set
(a)	(b)
Figure 5: (a) Illustrating when πB is outside the cost constraint set. (b) Illustrating when πB is inside
the cost constraint set. πboundary is the policy with JC (πboundary) = hC. We aim to bound hkD+1
(i.e., the KL-divergence between πboundary and πB) by using hkD .
consequences, nor can the systems adapt and support safety in situations in which new objectives are
created. This creates huge concerns in safety-critical applications such as self-driving vehicles and
personalized chatbot system.
This raises several questions: What human-agent communication is needed to bring humans in the
loop to increase safety guarantees for the autonomous system? How can trust and safety constraints
be incorporated into the planning and control processes? How can one effectively identify unsafe
plans of the baseline policy? We believe this paper will encourage future work to develop rigorous
design and analysis tools for continual safety assurance in conjunction with using baseline policies
from previous applications.
B PROOF OF UPDATING hD IN LEMMA 4.1
Proof. Based on Theorem 1 in Achiam et al. (2017), for any two policies π and π0 we have
JC(∏0) - JC(∏) ≥ 占Es〜d： [AC(s,a) - 1γ-πγ JlDkl(∏0(s)∣∣∏(s))]
⇒ ：YC 2 Es~dπ h∖ /1 DKL(R(S)IIn(S))i ≥ -jc (π0) + jC (π) +	Es~dπ [AC (s,a)i
(1 - Y)	2	1 - Y a 〜∏0
⇒ (^CPEs〜d∏ [ʌ/ɪDκL(∏0(s)∣∣∏(s))i ≥-JC(∏0) + JC(∏)
⇒ (12γγCr J=" [Dkl(∏0(s川∏(s))] ≥-Jc(∏0) + JC(∏)
[n 0 0	i、(1 - Y)4(-JC(nO) + jC(n))2
⇒ Es 〜dπ DKL (n (S)IIn(S)) ≥ ------------------02---------.	(16)
2Y2πC
The fourth inequality follows from Jensen's inequality. We then define 夕(n(s))	=
Pi n(a(i)IS) log n(a(i)IS). By Three-point Lemma (Chen & Teboulle, 1993), for any three policies
n, n0, and n we have
Es〜d∏ [dKL(M(S)IIn(S))] = Es〜d∏ [DKL(M(S)||n(S))] + Es〜d∏ IDKL(n(S)IIn(S))]
-Es〜d∏ [(▽夕(∏(s)) - ▽夕(π(s)))T(∏0(s) - ∏(s))] .	(17)
Let nboundary denote a policy satisfying JC (nboundary) = hC (i.e., nboundary is in the boundary of
the set of the policies which satisfy the cost constraint JC (n) ≤ he). Let n0 = ∏boundary, ∏ = ∏b
14
Under review as a conference paper at ICLR 2021
and π = πk in Eq. (16) and Eq. (17) (this is illustrated in Fig. 5). Then we have
Es~d∏k DKL(πboundary (S)IInB (S))] - Es~d∏k [DKL(∏k(s)II∏B (SH
=Es~d"k DKL (πboundary (S) ||n" (S))]
-Es~d∏k [(W1(πB (S))- ^ψ(πk (S)))T (boundary (S)- πk (S))]
≥ (1 - Y )4(-JC (KbOundary ) + JC (nk ))2
-	2γ2 / 2
-Es~d∏k [(Vg(∏B (s)) - V^(nk (S)))T (∏boundary (S)- ∏k ⑶)]
=(1- Y)4(-hc + JC(πk))2
—	2γ2eC0 2
-Es~d∏k [(W1(∏B (s)) - Vφ(nk (s)))t (TrbOundary (S)- ∏k ⑶)]
=O((- he + JC(∏k))2),	(18)
where JC (πboundary ) = hC .
For the first case in Fig. 5(a), We would like to have U ∩ Uk+1 = 0 (feasibility). For the second case
in Fig. 5(b), we would like to have Uk+1 ∩ ∂U = 0 (exploration). These implies that the policy in
step k + 1 is πboundary which satisfies U1 ∩ U2k+1 6= 0 and U2k+1 ∩ ∂U1 6= 0.
Now let hD+1 = Es~d∏k
Then Eq. 18 implies
DKL(πboundary (S)IIπB (S))
and hD = Es~d∏k [DKL(∏k(s)∣∣∏b(s))]∙
hkD+1 ≥O ((- h。+ JC (∏k))2) + hD.
□
Lemma 4.1 theoretically ensures hD is large enough to guarantee feasibility and exploration of the
agent. Note that we do not provide guarantees for finding an optimal policy. This requires additional
assumptions on the objective function (e.g., convexity).
In addition, the goal of this paper is to understand and analyze how to effectively exploit a baseline
policy in constrained RL. Without such an analysis, we are not confident in deploying SPACE in real
applications. Furthermore, the question of safely using baseline policies has a practical potential. It is
less studied by prior work (Achiam et al., 2017; Chow et al., 2019; Tessler et al., 2018; Yang et al.,
2020).
C Proof of Analytical Solution to SPACE in Eq. (10)
We first approximate the three stages in SPACE using the following approximations.
Step 1. Approximating Eq. (4) yields
θk+1 = argmax gkT(θ - θk) s.t. 1(θ - θk)tFk(θ - θk) ≤ δ.	(19)
θ2
Step 2 and Step 3. Approximating Eq. (5) and (6), similarly yields
θk+ 2 = argmin 1(θ - θk+ 3 )tL(θ - θk+3) s.t. akT(θ - θk) + bk ≤ 0,	(20)
θ2
θk+1 = argmin 1(θ - θk+ 3 )tL(θ - θk+2) s.t. ckT(θ - θk) + dk ≤ 0,	(21)
θ2
where L = I for the 2-norm projection and L = Fk for the KL-divergence projection.
15
Under review as a conference paper at ICLR 2021
Proof. For the first problem in Eq. (19), since F k is the Fisher Information matrix, it is positive
semi-definite. Hence it is a convex program with quadratic inequality constraints. If the primal
problem has a feasible point, then Slater,s condition is satisfied and strong duality holds. Let θ*
and λ* denote the solutions to the primal and dual problems, respectively. In addition, the primal
objective function is continuously differentiable. Hence the Karush-Kuhn-Tucker (KKT) conditions
are necessary and sufficient for the optimality of θ* and λ*. We now form the Lagrangian:
L(θ, λ) = -gkT(θ - θk) + λ( 1(θ - θk)TFk(θ - θk) - δ).
And we have the following KKT conditions:
-gk + λ*Fkθ* - λ*Fkθk	=0	Vθ L(θ*,λ*) = 0	(22)
1(θ* - θk)TFk(θ* - θk) - δ	=0	VλL(θ*,λ*)=0	(23)
1(θ* - θk)TFk(θ* - θk) - δ	≤0	primal constraints	(24)
λ*	≥0	dual constraints	(25)
λ* (1(θ* - θk)tfk(θ* - θk) - δ)	=0	complementary slackness	(26)
By Eq. (22), we have θ* = θk + + Fk-1gk. And by plugging Eq. (22) into Eq. (23), we have
λ* = ʌIgTFk ιgk. Hence we have a solution
2δ
θk+1 = θ* = θk + J T 2δ-	Fk-1gk,	(27)
gk Fk gk
which also satisfies Eq. (24), Eq. (25), and Eq. (26).
For the second problem in Eq. (20), we follow the same procedure for the first problem to form the
Lagrangian:
L(θ,λ) = 1(θ - θk+ 3)TL(θ - θk+1) + λ(akT(θ - θk) + bk).
And we have the following KKT conditions:
Lθ* - Lθk+ 3 + λ*ak =	0	VθL(θ*,λ*) = 0	(28)
akT(θ* - θk)	+ bk =	0	VλL(θ*,λ*)=0	(29)
akT(θ* - θk)	+ bk ≤	0	primal constraints	(30)
λ* ≥	0	dual constraints	(3l)
λ*(akT(θ* - θk) + bk) = 0 complementary slackness	(32)
By Eq. (28), we have θ* = θk + λ*L-1ak. And by plugging Eq. (28) into Eq. (29) and Eq. (31), we
have λ* = max(0, a-%[--θj+b ). Hence we have a solution
θk+ 3 = θ* = θk+ 3 - max(0, ak Wk；3 - θkT+ bk )L-1ak,	(33)
ak L-1ak
which also satisfies Eq. (30) and Eq. (32).
For the third problem in Eq. (21), instead of doing the projection on πk+3 which is the intermediate
policy obtained in the second step, we project the policy ∏k+3 onto the cost constraint. This allows
us to compute the projection without too much computational cost. We follow the same procedure
for the first and second problems to form the Lagrangian:
L(θ, λ) = 1(θ - θk+3)tL(θ - θk+1) + λ(ckT(θ - θk) + dk).
16
Under review as a conference paper at ICLR 2021
And we have the following KKT conditions:
Lθ* - Lθk+3 +	λ*ck	=	0	VθL(θ*,λ*)=0	(34)
ckT(θ* — θk)	+ dk	=	0	VλL(θ* ,λ*)=0	(35)
ckT(θ* — θk)	+ dk	≤	0	primal constraints	(36)
λ*	≥	0	dual constraints	(37)
λ*(ckT(θ* — θk) + dk ) = 0 complementary slackness	(38)
By Eq. (34), We have θ* = θk + λ*L-1ck. And by plugging Eq. (34) into Eq. (35) and Eq. (37), We
CkT (θk+1 -θk )+dk、
have λ* = max(0, ——(CkL-Ick)+-). Hence we have a solution
θk+1 = θ* = θk+ 3 — max(0, Ck (θk+ι - θkT+ dk )L-1ck.	(39)
ck L-1ck
Hence by combining Eq. (27), Eq. (33) and Eq. (39), We have
θk+1
θk + ʌI kτ J-Ik FkTgk-max(0,
gkF k gk
— max(0,
JgkT FLgk akτ F k-1gk+ bk	k
)	ɛa
ak L-1ak
⅛⅜ CkT F k-1gk+dk	k
ckτL-Ick	)L C .
□
D	Proof of Finite-Time Guarantee of SPACE in Theorem 5.1
We noW describe the reason for choosing tWo variants of -FOSP under tWo possible projections. Let
ηRk denote the step size for the reWard, ηDk denote the step size for the divergence cost, and ηCk denote
the step size for the constraint cost. Without loss of generality, under the KL-divergence projection,
at step k + 1 SPACE does
θk+1 = θk + ηR F k-1gk — ηD F k-1 ak — ηC F k-1ck.
Similarly, under the 2-norm projection, at step k + 1 SPACE does
θk+1 = θk + ηRk Fkgk — ηDk ak — ηCkCk.
With this definition, We have the folloWing Lemma.
Lemma D.1 (Stationary Points for SPACE). Under the KL-divergence projection, SPACE con-
verges to a stationary point θ* satisfying
ηR g* = ηD a + ηC c*∙
Under the 2-norm projection, SPACE converges to a stationary point θ* satisfying
ηR g* = F*(ηD a* + ηC c*).
Proof. Under the KL-divergence projection, by using the definition of a stationary point We have
θ* = θ* +ηR* F*-1g* — ηD* F*-1a* — ηC* F*-1C*
⇒ ηR*F*-1g* = ηD* F*-1a* +ηC*F*-1C*
⇒ ηR* g* = ηD* a* + ηC* C* .
Under the 2-norm projection, by using the definition of a stationary point We have
θ* = θ* +ηR* F*-1g* — ηD* a* — ηC* C*
⇒ ηR* F *-1g* =ηD*a*+ηC*C*
⇒ ηR*g* =F*(ηD*a* + ηC* C*).
□
17
Under review as a conference paper at ICLR 2021
Hence Lemma D.1 motivates the need for defining two variants of FOSP.
Before proving Theorem 5.1, we need the following Lemma. Define PCL(θ) =. arg min kθ - θ0k2L =
θ0∈C
arg min (θ - θ0)TL(θ - θ0), and L = Fk under the KL-divergence projection, and L = I under
θ0∈C
the 2-norm projection.
Lemma D.2 (Contraction of Projection (Yang et al., 2020)). For any θ, θ* = PL(θ) ifand only
if (θ - θ*)TL(θ0 - θ*) ≤ 0,∀θ0 ∈ C.
Proof. (⇒) Let θ* = PL(θ) for a given θ ∈ C, θ0 ∈ C be such that θ0 = θ*, and α ∈ (0,1). Then
we have
kθ - θ*kL ≤ kθ - (θ*+ α(θ0 - θ*))∣∣L
=kθ - θ*kL + α2kθ0 - θ*∣∣L - 2α(θ - θ*)TL(θ0 - θ*)
⇒ (θ - θ*)τL(θ0 - θ*) ≤ α∣∣θ0 - θ*∣∣L.	(40)
Since the right hand side of Eq. (40) can be made arbitrarily small for a given α, we have
(θ - θ*)TL(θ0 - θ*) ≤ 0,∀θ0 ∈ C.
(U) Let θ* ∈ C be such that (θ - θ*)TL(θ0 - θ*) ≤ 0, ∀θ0 ∈ C. We show that θ* must be the
optimal solution. Let θ0 ∈ C and θ0 = θ*. Then we have
kθ - θ0kL -kθ - θ*kL = kθ - θ* + θ* - θ0kL -kθ - θ*kL
=l∣θ - θ*kL + kθ0 - θ*kL - 2(θ - θ*)τ L(θ0 - θ*) - ∣θ - θ*∣∣L
>0
⇒kθ -θ0kL > kθ -θ*kL∙
Hence, θ* is the optimal solution to the optimization problem, and θ* = PL(θ).	口
We now prove Theorem 5.1. Without loss of generality, on each learning episode SPACE updates the
reward followed by the alternation of two projections onto the constraint sets (region around πB and
the cost constraint set):
θk+1 = θk - ηkFTVf(θk), θk+3 =	Pc2(θk+3),	θk+1	=	Pc1 (θk+2),if θk ∈	C2,
θk+1 = θk - ηkFTVf (θk), θk+3 =	Pc1 (θk+3),	θk+1	=	Pc2 (θk+2), if θk ∈	Ci,
where ηk is the step size at step k.
Proof. SPACE under the KL-divergence projection converges to an -FOSP. Based on Lemma
D.2 under the KL-divergence projection, and setting θ = θk - ηkFk 1 Vf (θk), θ* = θk+ 2 and
θ0 = θk, we have
(θk - θk+3)tfk(θk - ηkFk-1vf(θk) - θk+3) ≤ 0
⇒ Vf(θk)t(θk+3 - θk) ≤ -4(θk+3 - θk)tFk(θk+3 - θk).	(41)
ηk
Based on the L-Lipschitz continuity of gradients and Eq. (41), we have
f (θk+2) ≤ f(θk) + Vf (θk)t(θk+3 - θk) + L∣∣θk+3 - θkk2
≤ f(θk) - -1(θk+2 - θk)tfk(θk+2 - θk) + Lkθk+3 - θkk2
ηk	2
=f(θk) - L∣∣θk+3 - θkk2 - Vf(θk+3)t(θk+1 - θk+2) - L∣θk+1 - θk+3 k2,
(42)
18
Under review as a conference paper at ICLR 2021
where the equality follows by setting δ (i.e., the size of the trust region) such that
ηk
(θk+3 - θk)tFk (θk+2 - θk)
Lkθk+2 - θk∣∣2 + V/(θk+2)t(θk+1 - θk+3) + L∣∣θk+1 - θk+31∣2.
Again, based on Lemma D.2, for θ ∈ C2 we have
(θk - ηkFk-1Vf (θk) - θk+2 )Fk(θ - θk+3) ≤ 0
⇒ (-ηkFk-1Vf (θk))tFk(θ - θk+3) ≤ -(θk - θk+ 2)tFk(θ - θk+ 2)
⇒ Vf (θk)t(θ - θk+ 3) ≥ -1(θk - θk+ 2 )tFk(θ - θk+2)
ηk
⇒ Vf (θk)tθ ≥ Vf (θk)tθk+ 3 + -1(θk - θk+ 3 )tFk (θ - θk+ 2)
ηk
⇒ f (θk)τ(θ - θk) ≥ Vf (θk)τ(θk+ 2 - θk) + ɪ(θk - θk+3)tFk(θ - θk+2)
ηk
≥ -∣Vf(θk)k∣θk+2 - θkIl - ɪ∣∣θk+2 - θkk∣Fkk∣θ - θk+3Il
ηk
≥-(G + Dση(F)∣θk+2 - θk∣,	(43)
where in the last two inequalities we use the property of the norm. Before reaching an E-FOSP, Eq.
(43) implies that
-E ≥ min Vf (θk)τ(θ - θk) ≥ -(G + D"尸k )∣θk+ 2 - θk∣∣
⇒ Iθk+ 2 - θk k≥ CL (Fk) .	(44)
G + n
Based on Eq. (42) and Eq. (44), we have
f (θk+3) ≤ f (θk) - L∣∣θk+3 - θkIl2 -Vf (θk+2)τ(θk+1 - θk+2) - L∣∣θk+1 - θk+212
≤ f(θk)-
Le2
2(G + Dσ普)2
-Vf(θk+3)τ(θk+1 - θk+ 2) - L∣θk+1 - θk+31∣2.
(45)
Based on the L-Lipschitz continuity of gradients, for the projection to the constraint set C1 we have
f (θk+1) ≤ f (θk+ 2) + Vf (θk+ 2)τ(θk+1 - θk+ 2) + L∣θk+1 - θk+3112.	(46)
Combining Eq. (45) with Eq. (46), we have
…	Le2
fWk+1) ≤ f ⑻)-2(G + 弋产))2 .	(47)
Hence it takes O(e-2) iterations to reach an E-FOSp
SPACE under the 2-norm projection converges to an E-FOSP. Based on Lemma D.2 under the
2-norm projection, and setting θ = θk - ηkFk 1Vf (θk), θ* = θk+ 2 and θ0 = θk, we have
(θk - θk+ 3 )τ(θk - ηkFk-1Vf (θk) - θk+ 3) ≤ 0
⇒(Fk-1Vf (θk))τ(θk+3 - θk) ≤ -3(θk+ 2 - θk)τ(θk+ 2 - θk).	(48)
19
Under review as a conference paper at ICLR 2021
Based on the L-Lipschitz continuity of gradients and Eq. (48), we have
f (θk+2) ≤ f (θk) + Vf (θk)τ(θk+3 - θk) + L∣∣θk+3 - θkIl2
≤ f(θk) + (Fk-1Vf(θk))τ(θk+2 - θk) + Q + L∣θk+3 - θk∣∣2
≤ f (θk) - -1(θk+2 - θk)τ(θk+2 - θk) + Q + L∣∣θk+3 - θkIl2
ηk	2
=f (θk) - L∣θk+3 - θk∣2 - Vf (θk+3)τ(θk+1 - θk+2) - L∣θk+1 - θk+3∣2,
(49)
where Q := Vf (θk)τ(θk+ 3 — θk) — (Fk 1Vf (θk))τ(θk+ 2 — θk), which represents the difference
between the gradient and the nature gradient, and the equality follows by setting δ (i.e., the size of
the trust region) such that
ηk
∣∣θk+2 - θkk2
L∣θk+3 - θk∣2 + Q + Vf (θk+2)T(θk+1 - θk+2) + L∣∣θk+1 - θk+31∣2'
Again, based on Lemma D.2, for θ ∈ C2 we have
(θk - ηkFk-1Vf (θk) - θk+ 2)(θ - θk+ 3) ≤ 0
(-ηkFk-1Vf (θk))t(θ - θk+2) ≤ -(θk - θk+ 3 )t(θ - θk+ 2)
Vf (θk)τFk-1(θ - θk+ 2) ≥ -1(θk - θk+ 2 )t(θ - θk+ 2)
ηk
Vf (θk)τFk-1θ ≥ Vf (θk)τFk-1θk+ 3 + -1(θk - θk+2)τ(θ - θk+ 3)
ηk
Vf (θk)τFk-1(θ - θk) ≥ Vf (θk)τFk-1(θk+ 3 - θk) + -1(θk - θk+ 3 )τ(θ - θk+ 2)
ηk
≥ -∣Vf(θk)k∣Fk-1k∣θk+2 - θkIl - ɪ∣∣θk+3 - θkk∣θ - θk+3Il
ηk
≥ -(Gσι(Fk-1) + D)∣∣θk+ 3 - θk∣,
(50)
where in the last two inequalities we use the property of the norm. Before reaching an e-FOSP, Eq.
(50) implies that
-e ≥ min Vf (θk)τFk-1(θ - θk) ≥ -(Gσ1(Fk-1) + D)∣θk+ 3 - θk∣∣
⇒ ∣∣θk+ 2 - θkIl ≥
(Gσι(Fk-1) + DI
(51)
Based on Eq. (49) and Eq. (51), we have
f(θk+3) ≤ f(θk) - L∣θk+3 - θk∣2 -Vf(θk+2)t(θk+1 - θk+2) - L∣θk+1 - θk+2∣2
≤ f(θk)-
Le2
2(Gσι(Fk-1) + D )2
-Vf (θk+3)t(θk+1 - θk+2) - L∣θk+1 - θk+31∣2.
(52)
⇒
⇒
⇒
⇒
€
Based on the L-Lipschitz continuity of gradients, for the projection to the constraint set C1 we have
f (θk+1) ≤ f (θk+ 2) + Vf (θk+ 2)τ(θk+1 - θk+ 2) + L∣θk+1 - θk+31∣2.	(53)
Combining Eq. (52) with Eq. (53), we have
…	Le2
fLI) ≤ f (θk)- 2(Gσ1(Fk-1) + 条)2 .	(54)
Hence it takes O(e-2) iterations to reach an e-FOSP.	□
20
Under review as a conference paper at ICLR 2021
(a) SPACE under the KL-divergence projection
(b) SPACE under the 2-norm projection
Figure 6: Update procedures for SPACE under the KL and 2-norm projections with two possible
Fisher information matrices. A lower objective value is achieved at the darker green area. Red
and orange ellipses are Fks with two different spectra of singular values. Red and orange dots are
resulting updated points under these two spectra of Fks. (a) A red ellipse has a smaller σ1(Fk) and
an orange ellipse has a larger σ1 (Fk). Both ellipses have the same σn(Fk). The two resulting θk+2
are similar. (b) A red ellipse has a larger σn (Fk) and an orange ellipse has a smaller σn (Fk). Both
ellipses have the same σ1(Fk). θk+ 2 with a larger σn(Fk) (red dot) has greater decrease of the
objective value.
Comments on Assumption 1.3. In the paper, we assume that both the diameters of the cost
constraint set (C1 ) and the region around πB (C2) are bounded above by H. This implies that given
a small value for hD , the convergence speed is determined by how large the constraint set is. This
allows us to do an analysis for the algorithm. In practice, we agree that this assumption is too strong
and leave it as a future work for improvement.
Interpretation on Theorem 5.1. We now provide a visualization in Fig. 6 under two possible
projections. For each projection, we consider two possible Fisher information matrices. Please read
the caption for more detail. In Fig. 6(a) we observe that since the reward improvement and projection
steps use the KL-divergence, the resulting two update points with different σ1 (Fk) are similar. In
addition, under the 2-norm projection, the larger σn (Fk) is, the greater the decrease in the objective.
This is because that a large σn (Fk) implies a large curvature of f in all directions. Intuitively, this
makes the learning algorithm confident about where to update the policy to decrease the objective
value greatly. Geometrically, a large σn (Fk) makes the 2-norm distance between the pre-projection
and post-projection points small, leading to a small deviation from the reward improvement direction.
This is illustrated in Fig. 6(b). We observe that since Fk determines the curvature of f and the 2-norm
projection is used, the updated point with a larger σn (Fk) (red dot) achieves more improvement of
the objective value. These observations imply that the spectrum of the Fisher information matrix does
not play a major role in SPACE under the KL-divergence projection, whereas it affects the decrease
of objective value in SPACE under the 2-norm projection. Hence we choose either KL-divergence or
2-norm projections depending on the tasks to achieve better performance.
E Additional Experiment Results
E.1 Implementation Details
Mujoco Task (Achiam et al., 2017). In the point circle and ant circle tasks, the reward and cost
functions are
R(s) =
vT[-x2;x1]
1+|k[x1;x2]k-d|,
21
Under review as a conference paper at ICLR 2021
Figure 7: The environment of the circle task (adapted from Achiam et al. (2017)). The agent receives
the maximum reward while staying in the safe area by following the red dashed line path.
and
C(S) = 1 [|x1 | > Xlim],
where x1 and x2 are the coordinates in the plane, v is the velocity of the agent, and d, xlim are
environmental parameters that specify the safe area. The agent is rewarded for moving fast in a wide
circle with radius of d, but is constrained to stay within a safe region smaller than the radius of the
circle in x1-coordinate xlim ≤ d. For the point agent, we use d = 5 and xlim = 2.5; for the ant agent,
we use d = 5 and xlim = 1. The environment is illustrated in Fig. 7.
In the point gather task, the agent receives a reward of +10 for gathering green apples, and a cost of
1 for gathering red apples. Two green apples and eight red apples are placed in the environment at
the beginning. In the ant gather task, the agent receives a reward of +10 for gathering green apples,
and a cost of 1 for gathering red apples. The agent also gets a reward of -10 for falling down to
encourage smooth moving. Eight green apples and eight red apples are placed in the environment at
the beginning.
For the point and ant agents, the state space consists of the positions, orientations, velocities, and the
external forces applied to the torso and joint angles. The action space is the force applied to joints.
Traffic Management Task (Vinitsky et al., 2018). In the grid task, the state space, action space,
reward function, and cost function are illustrated as follows.
(1)	States: Speed, distance to the intersection, and edge number of each vehicle. The edges of the
grid are uniquely numbered so the travel direction can be inferred. For the traffic lights, we return 0
and 1 corresponding to green or red for each light, a number between [0, tswitch] indicating how long
until a switch can occur, and 0 and 1 indicating if the light is currently yellow. Finally, we return the
average density and velocity of each edge.
(2)	Actions: A list of numbers a = [-1, 1]n where n is the number of traffic lights. If ai > 0 for
traffic light i it switches, otherwise no action is taken.
(3)	Reward: The objective of the agent is to achieve high speeds. The reward function is
R(s) =
maχ(Vtarget - ∣∣ vtarget - v k, 0)
vtarget
where vtarget is an arbitrary large velocity used to encourage high speeds and v ∈ Rk is the velocities
of k vehicles in the network.
(4)	Cost: The objective of the agent is to let lights stay red for at most 7 consecutive seconds. The
cost function is
n
C(S) = X l[ti,red > 7],
where ti,red is the consecutive time that the light i is in red.
22
Under review as a conference paper at ICLR 2021
In the bottleneck task, the state space, action space, reward function, and cost function are illustrated
as follows.
(1)	States: The states include: the mean positions and velocities of human drivers for each lane for
each edge segment, the mean positions and velocities of the autonomous vehicles on each segment,
and the outflow of the system in vehicles per/hour over the last 5 seconds.
(2)	Actions: For a given edge-segment and a given lane, the action shifts the maximum speed of all
the autonomous vehicles in the segment from their current value. By shifting the max-speed to higher
or lower values, the system indirectly controls the velocity of the autonomous vehicles.
(3)	Reward: The objective of the agent is to maximize the outflow of the whole traffic. The reward
function is
R(St)= X	L,
i — t—卫 ∆t∙niane∙500
i = t ∆t
where nexit(i) is the number of vehicles that exit the system at time-step i, and nlane is the number
of lanes.
(4)	Cost: The objective of the agent is to let the velocities of human drivers have lowspeed for no
more than 10 seconds. The cost function is
nhuman
C(S)= X l[ti,iow > 10],
i—1
where nhuman is the number of human drivers, and ti,low is the consecutive time that the velocity of
human driver i is less than 5 m/s. For more information, please refer to Vinitsky et al. (2018).
Car-racing Task. In the car-racing task, the state space, action space, reward function, and the cost
function are illustrated as follows.
(1)	States: It is a high-dimensional space where the state is a 96 × 96 × 3 tensor of raw pixels. Each
pixel is in the range of [0, 255].
(2)	Actions: The agent has 12 actions in total: a ∈ A = {(asteer, agas, abrake)|asteer ∈
{-1, 0, 1}, agas ∈ {0, 1}, abrake ∈ {0, 0.2}}, where asteer is the steering angle, agas is the amount
of gas applied, and abrake is the amount of brake applied.
(3)	Reward: In each episode, we randomly generate the track. The episode is terminated if the agent
reaches the maximal step or traverse over 95% of the track. The track is discretized into 281 tiles.
The agent receives a reward of 嘴0 for each tile visited. To encourage driving efficiency, the agent
receives a penalty of -1 per-time step.
(4)	Cost: The cost is to constrain the accumulated number of brakes to encourage a smooth ride.
Architectures and Parameters. For the gather and circle tasks we test two distinct agents: a
point-mass (S ⊆ R9, A ⊆ R2), and an ant robot (S ⊆ R32, A ⊆ R8). The agent in the grid task is
S ⊆ R156, A ⊆ R4, and the agent in the bottleneck task is S ⊆ R141 , A ⊆ R20. Finally, the agent in
the car-racing task is S ⊆ R96×96×3 , A ⊆ R3 .
For the simulations in the gather and circle tasks, we use a neural network with two hidden layers
of size (64, 32) to represent Gaussian policies. And we use the KL-divergence projection. For
the simulations in the grid and bottleneck tasks, we use a neural network with two hidden layers
of size (16, 16) and (50, 25) to represent Gaussian policies, respectively. And we use the 2-norm
projection. For the simulation in the car-racing task, we use a convolutional neural network with two
convolutional operators of size 24 and 12 followed by a dense layer of size (32, 16) to represent a
Gaussian policy. And we use the KL-divergence projection. The choice of the projections depends on
the task itself, we report the best performance among two projections. We use tanh as an activation
function for all the neural network policies. In the experiments, since the step size is small, we reuse
the Fisher information matrix of the reward improvement step in the KL-divergence projection step
to reduce the computational cost.
23
Under review as a conference paper at ICLR 2021
Parameter	PC	PG	AC	AG	Gr	BN	CR
Reward dis. factor γ	0.995	0.995	0.995	0.995	0.999	0.999	0.990
Constraint cost dis. factor γC	1.0	1.0	1.0	1.0	1.0	1.0	1.0
Divergence cost dis. factor γD	1.0	1.0	1.0	1.0	1.0	1.0	1.0
step size δ	10-4	10-4	10-4	10-4	10-4	10-4	5 × 10-4
λGRAE	0.95	0.95	0.95	0.95	0.97	0.97	0.95
λCRGAE	1.0	1.0	0.5	0.5	0.5	1.0	1.0
λGDAE	0.95	0.95	0.95	0.95	0.90	0.90	0.95
Batch size	50,000	50,000	100,000	100,000	10,000	25,000	10,000
Rollout length	50	15	500	500	400	500	1000
Constraint cost threshold hC	5	0.5	5	0.2	0	0	5
Divergence cost threshold h0D	5	3	5	3	10	10	5
Number of policy updates	1,000	1,200	2,500	1,500	200	300	600
Table 2: Parameters used in all tasks. (PC: point circle, PG: point gather, AC: ant circle, AG: ant
gather, Gr: grid, BN: bottleneck, and CR: car-racing tasks)
	PCPO		SPACE (Ours)		f-PCPO		f-CPO		d-PCPO		d-CPO	
	M/C	Time	M/C	Time	M/C	Time	M/C	Time	M/C	Time	M/C	Time
PG	B	22.14	B	25.2	B	31.9	B	25.5	B	32.8	B	32.6
PC	B	35.1	B	51.2	B	48.4	B	49.4	B	55.5	B	55.9
AG	B	386.9	B	110.5	C	268.6	C	235.1	B	138.2	B	187.5
AC	B	148.9	B	94.0	C	222.6	C	214.6	B	177.4	B	151.2
Gr	A	105.3	A	91.4	A	88.2	A	58.7	A	116.8	A	115.3
BN	A	257.7	A	181.1	A	162.9	A	161.6	A	259.3	A	275.6
CR	C	993.5	C	971.6	C	1078.3	C	940.1	C	1000.4	C	981.0
Table 3: Real-time in seconds for one policy update for all tested algorithms and tasks. (PC: point
circle, PG: point gather, AC: ant circle, AG: ant gather, Gr: grid, BN: bottleneck, and CR: car-racing
tasks)
We use GAE-λ approach (Schulman et al., 2016) to estimate AπR(s, a), AπC (s, a), and AπD(s). For
the simulations in the gather, circle, and car-racing tasks, we use neural network baselines with the
same architecture and activation functions as the policy networks. For the simulations in the grid and
bottleneck tasks, we use linear baselines. The hyperparameters of all algorithms and all tasks are in
Table 2.
We conduct the experiments on three separate machines: machine A has an Intel Core i7-4770HQ
CPU, machine B has an Intel Core i7-6850K CPU, and machine C has an Intel Xeon X5675 CPU.
We report real-time (i.e., wall-clock time) in seconds for one policy update for all tested algorithms
and tasks in Table 3. We observe that SPACE has the same computational time as the other baselines.
For the most intensive task, i.e., the car-racing task, the memory usage is 6.28GB. The experiments
are implemented in rllab (Duan et al., 2016), a tool for developing RL algorithms. We provide the
link to the code: https://sites.google.com/view/spacealgo.
Comments on the rationale behind when to increase hD. The update method of hD is empir-
ically designed to ensure that the value of the cost does not increase (i.e., JC(πk) ≤ JC(πk-1))
and the reward keeps improving (i.e., JR(πk) ≥ JR(πk-1)) after learning from πB. Lemma 4.1
theoretically ensures hD is large enough to guarantee feasibility and exploration of the agent.
Implementation of Updating hkD. Lemma 4.1 shows that hkD+1 should be increased at least by
O((JC(∏k) - he)2) + hD if JC(∏k) > JC(πk-1) or JR(πk) < JR(πk-1) at step k. We now
provide the practical implementation. For each policy update we check the above conditions. If one of
the conditions satisfies, we increase hD+1 by setting the constant to 10, i.e., 10 ∙ (JC (∏k) - he)2 + h⅛.
In practice, we find that the performance of SPACE is not affected by the selection of the constant.
Note that we could still compute the exact value of hkD+1 as shown in the proof of Lemma 4.1.
However, this incurs the computational cost.
24
Under review as a conference paper at ICLR 2021
(a) Bottleneck	(b) Car-racing	(c) Grid
-θ- SPACE (Ours) -⅛ f-CPO -θ- f-PCPO -⅛- d-CPO -θ- d - PCPO pepo (Yang et al.)
Figure 8: The discounted reward vs. the cumulative undiscounted constraint cost over policy updates
for the tested algorithms and tasks. The solid line is the mean over 5 runs. SPACE achieves the same
reward performance with fewer cost constraint violations in all cases. (Best viewed in color.)
Comments on learning from multiple baseline policies πB. In our setting, we use one πB. This
allows us to do theoretical analysis. One possible idea for learning from multiple πB is to compute
the distance to each πB . Then, select the one with the minimum distance to do the update. This
ensures that the update for the reward in the first step is less affected by πB . And the analysis we did
can be extended. We leave it as future work for developing this.
Comments on refining the PCPO agent’s policy (Yang et al., 2020) directly. Fine-tuning the
pre-trained policy directly might result in lower reward and cost violations. This is because that the
pre-trained policy has a low entropy and it does not explore. We empirically observe that the agent
pre-trained with the baseline policy yields less reward in the new task (i.e., different cost constraint
thresholds hC) as illustrated in Section E.2. In contrast, the SPACE agent simultaneously learns from
the baseline policy while ensuring the policy entropy is high enough to explore the environment.
Comments on the feasibility of getting safe baseline policies. In many real-world applications
such as drones, we can obtain baseline policies modeled from the first principle physics, or pre-
train baseline policies in the constrained and safe environment, or use rule-based baseline policies.
Importantly, We do not assume the baseline has to be a “safe policy” - it can be a heuristic that ignores
safety constraints. This is one of the main motivations for our algorithm: to utilize priors from the
baseline Which may be unsafe, but guarantee the safety of the neWly learned algorithm according to
the provided constraints.
Instructions for Reproducibility. We noW provide the instructions for reproducing the results.
First install the libraries for python3 such as numpy, scipy. To run the Mujoco experiments, get
the licence from https://www.roboti.us/license.html. To run the traffic management
experiments, install FLOW simulator from https://flow.readthedocs.io/en/latest/.
To run the car-racing experiments, install OpenAI Gym from https://github.com/openai/
gym. Our implementation is based on the environment from Achiam et al. (2017), please doWn-
load the code from https://github.com/jachiam/cpo. The code is based on rllab (Duan
et al., 2016), install the relevant packages such as theano (http://deeplearning.net/
software/theano/). Then, doWnload SPACE code from https://sites.google.com/
view/spaceneurips and place the codes on the designated folder instructed by Readme.txt on
the main folder. Finally, go to the example folder and execute the code using python command.
E.2 Experiment Results
Baseline policies. We pre-train the baseline policies using a safe RL algorithm. Here We also
consider three types of baseline policies: (1) suboptimal πBcost With JC (πBcost) ≈ 0, (2) suboptimal
πBreward With JC (πBreward) > hC , and (3) πBnear With JC (πBnear) ≈ hC Note that these πB have
different degrees of constraint satisfaction.
The Discounted Reward vs. the Cumulative Undiscounted Constraint Cost (see Fig. 8). To
shoW that SPACE achieves higher reWard under the same cost constraint violations (i.e., learning a
25
Under review as a conference paper at ICLR 2021
constraint-satisfying policy without violating the cost constraint a lot), we examine the discounted
reward versus the cumulative undiscounted constraint cost. The learning curves of the discounted
reward versus the cumulative undiscounted constraint cost are shown for all tested algorithms and
tasks in Fig. 8. We observe that in these tasks under the same value of the reward, SPACE outperforms
the baselines significantly with fewer cost constraint violations. For example, in the car-racing task
SPACE achieves 3 times fewer cost constraint violations at the reward value of40 compared to the best
baseline - PCPO. This implies that SPACE effectively leverages the baseline policy while ensuring
the constraint satisfaction. In contrast, without the supervision of the baseline policy, PCPO requires
much more constraint violations to achieve the same reward performance as SPACE. In addition,
although the fixed-point and the dynamic-point approaches use the supervision of the baseline policy,
the lack of the projection step makes them less efficient in learning a constraint-satisfying policy.
Comparison of Baseline Policies (see Fig. 9). To examine whether SPACE can safely learn from
the baseline policy which need not satisfy the cost constraint, we consider two baseline policies:
πBcost and πBreward . The learning curves of the undiscounted constraint cost, the discounted reward,
and the undiscounted divergence cost with two possible baselines over policy updates are shown for
all tested algorithms and tasks in Fig. 9. We observe that in the point gather and point circle tasks,
the initial values of the cost are larger than hC (i.e., JC (π0) > hC). Using πBcost allows the learning
algorithm to quickly satisfy the cost without doing the extensive projection onto the cost constraint
set. For example, in the point circle task we observe that learning guided by πBcost quickly satisfies
the cost constraint. In addition, we observe that in the ant gather and ant circle tasks, the initial values
of the cost are smaller than hC (i.e., JC(π0) < hC). Intuitively, we would expect that using πBreward
allows the agent to quickly improve the reward since the agent already satisfies the cost constraint in
the beginning. In the ant gather task we observe that SPACE guided by πBreward does improve the
reward more quickly at around 200 iteration. However, we observe that the agent guided by the both
baseline policies achieve the same final reward performance in the ant gather and ant circle tasks. The
reason is that using dynamic hD allows the agent to stay away from the baseline policy. This makes
the baseline policy less influential in the end. As a result, the reward improvement mostly comes
from the reward improvement step of SPACE if the agent starts in the interior of the cost constraint
set (i.e., JC (π0) ≤ hC).
Fixed hD (see Fig. 10). To understand the effect of using dynamic hkD when learning from a
sub-optimal baseline policy, we compare the performance of SPACE with and without adjusting hD .
The learning curves of the undiscounted constraint cost, the discounted reward, and the undiscounted
divergence cost over policy updates are shown for all tested algorithms and tasks in Fig. 10. We
observe that SPACE with fixed hD converges to less reward. For example, in the ant circle task
SPACE with the dynamic hD achieves 2.3 times more reward. The value of the divergence cost in
the ant circle task shows that staying away from the baseline policy achieves more reward. This
implies that the baseline policy in the ant circle task is highly sub-optimal to the agent. In addition,
we observe that in some tasks the dynamic hD does not have much effect on the reward performance.
For example, in the point gather task SPACE achieves the same reward performance. The values
of the divergence cost in the point gather task decrease throughout the training. These observations
imply that the update scheme of hD is critical for some tasks.
Comparison of SPACE vs. d-CPO, d-PCPO and the Pre-training Approach (see Fig. 11). To
show that SPACE is effective in using the supervision of the baseline policy, we compare the
performance of SPACE to the dynamic-point and the pre-training approaches. In the pre-training
approach, the agent first performs the trust region update with the objective function being the
divergence cost. Once the agent has the same reward performance as the baseline policy (i.e.,
JR(πk) ≈ JR(πB) for some k), the agent performs the trust region update with the objective function
being the reward function. The learning curves of the undiscounted constraint cost, the discounted
reward, and the undiscounted divergence cost over policy updates are shown for all tested algorithms
and tasks in Fig. 11. We observe that SPACE achieves better reward performance compared to the
pre-training approach in all tasks. For example, in the point circle, ant gather and ant circle tasks the
pre-training approach seldom improves the reward but all satisfies the cost constraint. This implies
that the baseline policies in these tasks are highly sub-optimal in terms of reward performance. In
contrast, SPACE prevents the agent from converging to a poor policy.
26
Under review as a conference paper at ICLR 2021
200 400 600 800 1000 1200
# of policy updates
Ξ 12
11。
2 8
2 6
Q 4
200 400 600 800 1000 1200
# of policy updates
2 O
≡ UMPUn
200 400 600 800 1000 1200
# of policy updates
Ooooo
4 3 2 1
Asou llqe>ASUOU ・us-pun
(a) Point gather
(b) Point circle
ASOUlU-e>lsuou ・us-PUn
# of policy updates
# of policy updates
Φuu86>e>P -Uω≡c3
# of policy updates
1TT
500 1000 1500 2000 2500
-Θ--∈>-Θ--∈?-θ-θ-θ-θ-€›∈>
I t
(c) Ant gather
O0T
110
10-10-
p>eMφ> -wωδ
500 1000 1500 2000
# of policy updates
# of policy updates

(d) Ant circle
-θ- SPACE With ∏^st SPACE With ∏^ard - - hc -Q- Jc^θεt) 兴■ Jc(唁Ward) •㊀ l 加暗St)・兴 4(喑Ward)
Figure 9: The undiscounted constraint cost, the discounted reward, and the undiscounted divergence
cost over policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded
area is the standard deviation over 5 runs. SPACE ensures cost constraint satisfaction guided by the
baseline policy which need not satisfy the cost constraint. (Best viewed in color.)
In addition, we observe that in the point gather task the pre-training approach achieves the same
reward performance as the baseline policy, whereas SPACE has a better reward performance compared
to the baseline policy. The pre-training approach does not keep improving the reward after learning
from the baseline policy. This is because that after pre-training with the baseline policy, the entropy
of the learned policy is small. This prevents the agent from trying new actions which may lead to
better reward performance. This implies that pre-training approach may hinder the exploration of the
learning agent on the new environment. Furthermore, in the car-racing task we observe that using
pre-training approach achieves the same reward performance as SPACE but improves reward slowly,
27
Under review as a conference paper at ICLR 2021
9 8 7 6 5
6 S 60.S
ISouAU-E:ISUoU u-!2PUn
					
单					
					
					
					
					
16
14
Ξ12
e
M io
ω
»- 8
2 6
S 4
200 400 600 800 1000 1200
# of policy updates
5 0 5 0 5 0
2 2 11
ISouAU-SASUoU-US=5Un
4SoUΦUU8PO>P ・u.!2PUn
# of policy updates
♦sou 占 SUoU-Us≡5un
# of policy updates
4SoU 8UU8PO>P ・u.!2PUn
# of policy updates
30
20
10
0
4sou 4=B4SUOU -Uω⅞c3
# of policy updates
(C) Ant gather
# of policy updates
IS8 Au-EASUOU-UMPUn
# of policy updates
(e) Car-racing
-θ- SPACE SPACE with fixed hD---------hc ■■■ Jr(πb)
Figure 10: The undiscounted constraint cost, the discounted reward, and the undiscounted divergence
cost over policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded
area is the standard deviation over 5 runs. SPACE with the dynamic hD achieves higher reward. (Best
viewed in color.)
and the pre-training approach has more cost constraint violations than SPACE. This implies that
jointly using reinforcement learning and the supervision of the baseline policy achieve better reward
and cost performance.
0	500	1000 1500 2000 2500
# of policy updates
Oooooo
Ooooo
W 8 6 4 2
4SoU 8uuopo>P ・u.!2PUn
28
Under review as a conference paper at ICLR 2021
For d-CPO and d-PCPO, in the point and ant tasks we observe that both approaches have comparable
or silently better reward and cost performance compared to SPACE. However, in the car-racing task
we observe that d-CPO cannot improve the reward due to a slow update procedure for satisfying the
cost constraint, whereas d-PCPO has a better reward performance. These observations imply that the
projection steps in SPACE allow the learning agent to effectively and robustly learn from the baseline
policy.
Comparison of SPACE under the KL-divergence and the 2-norm Projections (see Fig. 12).
Theorem 5.1 shows that under the KL-divergence and 2-norm projections, SPACE converges to
different stationary points. To demonstrate the difference between these two projections, Fig. 12
shows the learning curves of the undiscounted constraint cost, the discounted reward, and the
undiscounted divergence cost over policy updates for all tested algorithms and tasks. In the Mujoco
tasks, we observe that SPACE under the KL-divergence projection achieves higher reward. For
instance, in the point gather task the final reward is 25% higher under the same cost constraint
satisfaction. In contrast, in the traffic management tasks, we observe that SPACE under the 2-norm
projection achieves better cost constraint satisfaction. For instance, in the grid task SPACE under
the 2-norm projection achieves a lower reward but more cost constraint satisfaction. In addition, in
the bottleneck task SPACE under the 2-norm projection achieves more reward and cost constraint
satisfaction. These observations imply that SPACE converges to different stationary points under two
possible projections depending on tasks.
Initial h0D (see Fig. 13). To understand the effect of the initial value of h0D, we test SPACE with
three different initial values: h0D = 1, h0D = 5, and h0D = 25 in the ant circle and car-racing tasks.
The learning curves of the undiscounted constraint cost, the discounted reward, and the undiscounted
divergence cost over policy updates are shown for all tested algorithms and tasks in Fig. 13. In both
tasks, we observe that the initial value of h0D does not affect the reward and the cost performance
significantly (i.e., the mean of learning curves lies in roughly the same standard deviation over the
initialization). In addition, the value of the divergence cost over three h0D are similar throughout the
training. These observations imply that the update scheme of hkD in SPACE is robust to the choice of
the initial value of h0D .
However, in the car-racing task we observe that the learning curves of using a smaller h0D tend to have
higher variances. For example, the standard deviation of h0D = 1 in the reward plot is 6 times larger
than the one with h0D = 25. This implies that SPACE may have reward performance degradation
when using a smaller initial value of h0D . One possible reason is that when the distance between the
learned and baseline policies is large, using a small value of h0D results in an inaccurate projection
(i.e., due to approximation errors). This causes the policy to follow a zigzag path. We leave the
improvement of this in future work.
F	Human Policies
We now describe the procedure for collecting human demonstration data in the car-racing task. A
player uses the right key, left key, up key and down key to control the direction, acceleration, and
brake of the car. The human demonstration data contain the display of the game (i.e., the observed
state), the actions, and the reward. We collect 20 minutes of demonstration data. A human player is
instructed to stay in the lane but does not know the cost constraint. This allows us to test whether
SPACE can safely learn from the baseline policy which need not satisfy the cost constraints. We then
use an off-policy algorithm (DDPG) trained on the demonstration data to get the baseline human
policy. Since the learned baseline human policy does not interact with the environment, its reward
performance cannot be better than the human performance. Fig. 14 shows the procedure.
Implementation Details of DDPG. We use DDPG as our off-policy algorithm. We use a convolu-
tional neural network with two convolutional operators of size 24 and 12 followed by a dense layer of
size (32, 16) to represent a Gaussian policy. A Q function shares the same architecture of the policy.
The learning rates of the policy and Q function are set to 10-4 and 10-3, respectively.
29
Under review as a conference paper at ICLR 2021
G	The Machine Learning Reproducib ility Checklist (Version 1.2,
MAR.27 2019)
For all models and algorithms presented, indicate if you include2:
• A clear description of the mathematical setting, algorithm, and/or model:
- Yes, please see the problem formulation in Section 3, the update procedure for SPACE
in Section 5, and the architecture of the policy in Section E.1.
• An analysis of the complexity (time, space, sample size) of any algorithm:
- Yes, please see the implementation details in Section E.1.
• A link to a downloadable source code, with specification of all dependencies, including
external libraries:
- Yes, please see the implementation details in Section E.1.
For any theoretical claim, check if you include:
• A statement of the result:
- Yes, please see Section 4 and Section 5.
• A clear explanation of any assumptions:
- Yes, please see Section 4 and Section 5.
• A complete proof of the claim:
- Yes, please see Section B, Section C, and Section D.
For all figures and tables that present empirical results, indicate if you include:
• A complete description of the data collection process, including sample size:
- Yes, please see Section E.1 for the implementation details.
• A link to a downloadable version of the dataset or simulation environment:
- Yes, please see Section E.1 for the simulation environment.
• An explanation of any data that were excluded, description of any pre-processing step:
- It’s not applicable. This is because that data comes from simulated environments.
• An explanation of how samples were allocated for training / validation / testing:
- It’s not applicable. The complete trajectories (i.e., data) is used for training. There is
no validation set. Testing is performed in the form of online learning approaches.
• The range of hyper-parameters considered, method to select the best hyper-parameter
configuration, and specification of all hyper-parameters used to generate results:
- Yes, we randomly select five random seeds, and please see Section E.1 for the imple-
mentation details.
• The exact number of evaluation runs:
- Yes, please see Section E.1 for the implementation details.
• A description of how experiments were run:
- Yes, please see Section E.1 for the implementation details.
• A clear definition of the specific measure or statistics used to report results:
- Yes, please see Section 6.
• Clearly defined error bars:
- Yes, please see Section 6.
2Here	is a link to the list:	https://www.cs.mcgill.ca/~jpineau/
ReproducibilityChecklist.pdf.
30
Under review as a conference paper at ICLR 2021
• A description of results with central tendency (e.g., mean) variation (e.g., stddev):
- Yes, please see Section 6.
• A description of the computing infrastructure used:
- Yes, please see Section E.1 for the implementation details.
31
Under review as a conference paper at ICLR 2021
ISo ;um±suou・u.!2pun
(a) Point gather
ISo ;um±su。。-usz5Un
0	200	400	600	800
# of policy updates
(b) Point circle
# of policy updates
3 2 10
0.SSS
ISoUAum上 suou・u.!2PUn
# of policy updates
(c) Ant gather
200 400 600 800100012001400
# of policy updates
(d) Ant circle
ISo :umtsuou-USz5Un
# of policy updates
(e) Car-racing
Ooooooo
Ooooooo
9 8 7 6 5 4 3
4sou euuφpφ>p UMPUn
# of policy updates
(ɔ SPACE (Ours) Q d — CPO l⅛l d — PCPO (ɔ Pretraining 一 - 6C * * * Jr(□b)
Figure 11:	The undiscounted constraint cost, the discounted reward, and the undiscounted divergence
cost over policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded
area is the standard deviation over 5 runs. SPACE outperforms d-CPO, d-PCPO and the pre-training
approach in terms of the efficiency of the reward improvement and cost constraint satisfaction. (Best
viewed in color.)
32
Under review as a conference paper at ICLR 2021
(a) Point gather
(b) Point circle
# of policy updates	# of policy updates	# of policy updates
(c) Grid
(d) Bottleneck
100
80
60
40
20
SPACE, 2 - norm - proj. SPACE, KL - proj. 一 ■ hC ■ ■ ■ JRmB)
Figure 12:	The undiscounted constraint cost, the discounted reward, and the undiscounted divergence
cost over policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded
area is the standard deviation over 5 runs. SPACE converges to differently stationary points under
two possible projections. (Best viewed in color.)
33
Under review as a conference paper at ICLR 2021
4Jsou4Juffl上 Suou ∙us≡u∩
Iso:um±su。。・u.!2PUn
0 5 0 5 0 5
67.5,2.67.
2 1111
900
(b) Car-racing
-θ- SPACE,
用=5 * SPACE. * = 25
100 200 300 400 500 600
# of policy updates
Figure 13: The undiscounted constraint cost, the discounted reward, and the undiscounted divergence
cost over policy updates for the tested algorithms and tasks. The solid line is the mean and the shaded
area is the standard deviation over 5 runs. We observe that the initial value of h0D does not affect the
reward and the cost performance significantly. (Best viewed in color.)
θuuφ6,lφ>:6∙us≡u∩
# of policy updates
(a) Ant circle
# of policy updates
φucφ^φ>≡ ∙us≡u∩
0	500	1000 1500 2000 2500
# of policy updates
800
700
600
500
400
300
0	100 200 300 400 500 600
# of policy updates

he * * * JrEb)
Off^policy algorithms
(DDPG)
SPACE
Figure 14: Procedure for getting a baseline human policy. We ask a human to play the car-racing
game. He/She does not know the cost constraint. The trajectories (i.e., display of the game, the action,
and the reward) are then stored. A human policy is obtained by using an off-policy algorithm (DDPG)
trained on the trajectories.
34