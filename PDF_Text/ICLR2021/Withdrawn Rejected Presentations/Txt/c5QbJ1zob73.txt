Under review as a conference paper at ICLR 2021
Understanding Self-supervised Learning with
Dual Deep Networks
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel theoretical framework to understand self-supervised learning
methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL).
First, we prove that in each SGD update of SimCLR, the weights at each layer
are updated by a covariance operator that specifically amplifies initial random
selectivities that vary across data samples but survive averages over data augmen-
tations. We show this leads to the emergence of hierarchical features, if the input
data are generated from a hierarchical latent tree model. With the same frame-
work, we also show analytically that in BYOL, the combination of BatchNorm
and a predictor network creates an implicit contrastive term, acting as an approx-
imate covariance operator. Additionally, for linear architectures we derive exact
solutions for BYOL that provide conceptual insights into how BYOL can learn
useful non-collapsed representations without any contrastive terms that separate
negative pairs. Extensive ablation studies justify our theoretical findings.
1	Introduction
While self-supervised learning (SSL) has achieved great empirical success across multiple domains,
including computer vision (He et al., 2020; Goyal et al., 2019; Chen et al., 2020a; Grill et al., 2020;
Misra and Maaten, 2020; Caron et al., 2020), natural language processing (Devlin et al., 2018),
and speech recognition (Wu et al., 2020; Baevski and Mohamed, 2020; Baevski et al., 2019), its
theoretical understanding remains elusive, especially when multi-layer nonlinear deep networks are
involved (Bahri et al., 2020). Unlike supervised learning (SL) that deals with labeled data, SSL
learns meaningful structures from randomly initialized networks without human-provided labels.
In this paper, we propose a systematic theoretical analysis of SSL with deep ReLU networks. Our
analysis imposes no parametric assumptions on the input data distribution and is applicable to state-
of-the-art SSL methods that typically involve two parallel (or dual) deep ReLU networks during
training (e.g., SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), etc). We do so by developing
an analogy between SSL and a theoretical framework for analyzing supervised learning, namely the
student-teacher setting (Tian, 2020; Allen-Zhu and Li, 2020; Lampinen and Ganguli, 2018; Saad
and Solla, 1996), which also employs a pair of dual networks. Our results indicate that SimCLR
weight updates at every layer are amplified by a fundamental positive semi definite (PSD) covariance
operator that only captures feature variability across data points that survive averages over data
augmentation procedures designed in practice to scramble semantically unimportant features (e.g.
random image crops, blurring or color distortions (Falcon and Cho, 2020; Kolesnikov et al., 2019;
Misra and Maaten, 2020; Purushwalkam and Gupta, 2020)). This covariance operator provides a
principled framework to study how SimCLR amplifies initial random selectivity to obtain distinctive
features that vary across samples after surviving averages over data-augmentations.
Based on the covariance operator, we further show that (1) in a two-layer setting, a top-level covari-
ance operator helps accelerate the learning of low-level features, and (2) when the data are generated
by a hierarchical latent tree model, training deep ReLU networks leads to an emergence of the latent
variables in its intermediate layers. We also analyze how BYOL might work without negative pairs.
First we show analytically that an interplay between the zero-mean operation in BatchNorm and the
extra predictor in the online network creates an implicit contrastive term, consistent with empirical
observations in the recent blog (Fetterman and Albrecht, 2020). Note this analysis does not rule
out the possibility that BYOL could work with other normalization techniques that don’t introduce
contrastive terms, as shown recently (Richemond et al., 2020a). To address this, we also derive exact
solutions to BYOL in linear networks without any normalization, providing insight into how BYOL
can learn without contrastive terms induced either by negative pairs or by BatchNorm. Finally, we
also discover that reinitializing the predictor every few epochs doesn’t hurt BYOL performance,
thereby questioning the hypothesis of an optimal predictor in (Grill et al., 2020).
1
Under review as a conference paper at ICLR 2021
Figure 1: (a) Overview of the two SSL algorithms we study in this paper: SimCLR (W1 = W2 = W, no
predictor, NCE Loss) and BYOL (Wι has an extra predictor, W2 is a moving average), (b) Detailed Notation.
Layerl -1
味i nodes
Layerl
ni nodes
To the best of our knowledge, We are the first to provide a systematic theoretical analysis of modern
SSL methods with deep ReLU networks that elucidates how both data and data augmentation, drive
the learning of internal representations across multiple layers.
Related Work. Besides SimCLR and BYOL, we briefly mention other concurrent SSL frame-
works for vision. MoCo (He et al., 2020; Chen et al., 2020b) keeps a large bank of past representa-
tions in a queue as the slow-progressing target to train from. DeepCluster (Caron et al., 2018) and
SwAV (Caron et al., 2020) learn the representations by iteratively or implicitly clustering on the cur-
rent representations and improving representations using the cluster label. (Alwassel et al., 2019)
applies similar ideas to multi-modality tasks. In contrast, the literature on the analysis of SSL with
dual deep ReLU networks is sparse. (Arora et al., 2019) proposes an interesting analysis of how
contrastive learning aids downstream classification tasks, given assumptions about data generation.
However, it does not explicitly analyze the learning of representations in deep networks.
2	Overall framework
Notation. Consider an L-layer ReLU network obeying fl = ψ(fl) and fl = Wl fl-1 for l =
1,... L. Here fl and fl are nl dimensional pre-activation and activation vectors in layer l, with
f0 = X being the input and fL = fL the output (no ReLU at the top layer). Wl ∈ Rnl×nl-1
are the weight matrices, and ψ(u) := max(u, 0) is the element-wise ReLU nonlinearity. We let
W := {Wl}lL=1 be all network weights. We also denote the gradient of any loss function with respect
to fl by gl ∈ Rnl , and the derivative of the output fL with respect to an earlier pre-activation fl by
the Jacobian matrix Jl(x; W) ∈ RnL×nl, as both play key roles in backpropagation (Fig. 1(b)).
An analogy between self-supervised and supervised learning: the dual network scenario.
Many recent successful approaches to self-supervised learning (SSL), including SimCLR (Chen
et al., 2020a), BYOL (Grill et al., 2020) and MoCo (He et al., 2020), employ a dual “Siamese-
like” pair (Koch et al., 2015) of such networks (Fig. 1(b)). Each network has its own set of
weights W1 and W2, receives respective inputs x1 and x2 and generates outputs f1,L(x1; W1)
and f2,L (x2; W2). The pair of inputs {x1 , x2} can be either positive or negative, depending on
how they are sampled. For a positive pair, a single data point x is drawn from the data distribution
p(∙), and then two augmented views x1 and x2 are drawn from a conditional augmentation distri-
bution Paug(∙∣x). Possible image augmentations include random crops, blurs or color distortions,
that ideally preserve semantic content useful for downstream tasks. In contrast, for a negative pair,
two different data points x, x0 〜p(∙) are sampled, and then each are augmented independently to
generate x1 〜 Paug(∙∣x) and x2 〜 Paug(∙∣x0). For SimCLR, the dual networks have tied weights
with W1 = W2, and a loss function is chosen to encourage the representation of positive (negative)
pairs to become similar (dissimilar). In BYOL, only positive pairs are used, and the first network
W1 , called the online network, is trained to match the output of the second network W2 (the target),
using an additional layer named predictor. The target network ideally provides training targets that
can improve the online network’s representation and does not contribute a gradient. The improved
online network is gradually incorporated into the target network, yielding a bootstrapping procedure.
Our fundamental goal is to analyze the mechanisms governing how SSL methods like SimCLR
and BYOL lead to the emergence of meaningful intermediate features, starting from random initial-
izations, and how these features depend on the data distribution P(x) and augmentation procedure
Paug(∙∣x). Interestingly, the analysis of supervised learning (SL) often employs a similar dual net-
work scenario, called teacher-student setting (Tian, 2020; Allen-Zhu and Li, 2020; Lampinen and
Ganguli, 2018; Saad and Solla, 1996), where W2 are the ground truth weights of a fixed teacher
network, which generates outputs in response to random inputs. These input-output pairs constitute
training data for the first network, which is a student network. Only the student network’s weights
2
Under review as a conference paper at ICLR 2021
W1 are trained to match the target outputs provided by the teacher. This yields an interesting mathe-
matical parallel between SL, in which the teacher is fixed and only the student evolves, and SSL, in
which both the teacher and student evolve with potentially different dynamics. This mathematical
parallel opens the door to using techniques from SL (e.g., (Tian, 2020)) to analyze SSL.
Gradient of `2 loss for dual deep ReLU networks. As seen above, the (dis)similarity of repre-
sentations between a pair of dual networks plays a key role in both SSL and SL. We thus consider
minimizing a simple measure of dissimilarity, the squared '2 distance r := 1 kfι,L - f2,L∣∣2 be-
tween the final outputs f1,L and f2,L of two multi-layer ReLU networks with weights W1 and W2
and inputs x1 and x2. Without loss of generality, we only analyze the gradient w.r.t W1. For each
layer l, we first define the connection Kl (x), a quantity that connects the bottom-up feature vector
fl-1 with the top-down Jacobian Jl, which both contribute to the gradient at weight layer l.
Definition 1 (The connection Kl(x)). The connection Kι(x; W) := f1-1(x; W) 0 Jl|(x;W) ∈
Rnl nl-1 ×nL. Here 0 is the Kronecker product.
Theorem 1 (Squared `2 Gradient for dual deep ReLU networks). The gradient gWl of r w.r.t. Wl ∈
Rnl ×nl-1 fora single input pair {x1, x2} is (here K1,l := Kl(x1; W1) and K2,l := Kl(x2; W2)):
gWι = vec(∂r∕∂Wι,l) = Kι,l [κ[lvec(Wι,l) - Kl,lvec(W2,l)] .	(1)
We used vectorized notation for the gradient gWl and weights Wl to emphasize certain the-
oretical properties of SSL learning below. The equivalent matrix form is ∂r∕∂W1,l =
J1|,l [J1,lW1,lf1,l-1 - J2,lW2,lf2,l-1] f1T,l-1. See Appendix for proofs of all theorems in main text.
3 Analysis of SimCLR
As discussed above, SimCLR (Chen et al., 2020a) employs both positive and negative input pairs,
anda symmetric network structure with W1 = W2 = W. Let {x1, x+} be a positive input pair from
x, and let {x1, xk-} for k = 1, . . . , H be H negative pairs. These input pairs induce corresponding
squared '2 distances in output space, r+ := 2∣∣fι,L - f+,L∣∣2,and » := 2∣∣fι,L - fk-,L∣∣2∙
We consider three different contrastive losses, (1) the simple contrastive loss Lsimp := r+ - r-,
⑵(Soft) Triplet loss LTri := Tlog(1 + e(r+-r-+r0)/T) (here ro ≥ 0 is the margin). Note that
limτ→0 Ltτri = max(r+ - r- + r0, 0) (Schroff et al., 2015), (3) InfoNCE loss (Oord et al., 2018):
e-r+ /τ
Lnce(r+, r1-, r2-, . . . , rH - ) : = - log ~,	XɔH	~,-	(2)
e-r+片 + XH=Qe-rk-/T
Note that when ∣∣u∣2 = ∣∣v∣2 = 1, We have -2∣∣u - v∣2 = Sim(U) v) - 1 where sim(u, V)=
kuulQl?, and Eqn. 2 reduces to what the original SimCLR uses (the term e-1/T cancels out).
For simplicity, we move the analysis of the final layer '2 normalization to Appendix A.2. In Ap-
pendix F.6 of BYOL Grill et al. (2020) v3, it shows that even without '2 normalization, the algorithm
still works despite numerical instabilities. In this case, the goal of our analysis is to show that useful
weight components grow exponentially in the gradient updates.
One property of these loss functions is important for our analysis:
Theorem 2 (Common Property of Contrastive Losses). For loss functions L ∈ {Lsimp, Ltτri, Lτnce},
We have ∂L > O, ∂dL- < 0for 1 ≤ k ≤ H and ∂dL+ + PH=I ∂L =0∙
With Theorem 1 and Theorem 2, we now present our first main contribution. The gradient in Sim-
CLR is governed by a positive semi-definite (PSD) covariance operator at any layer l:
Theorem 3 (Covariance Operator for Lsimp). With large batch limit, Wl ’s update under Lsimp is:
Wl(t+ 1) = Wl(t) +α∆Wl(t), where vec(∆Wl(t)) = OPlsimp(W)vec(Wl(t)).	(3)
Where OPsimp(W) := Vx 〜p(,)[Kl(x; W)] ∈ Rnιnι-1×nlnι-1 is the covariance operator for Lsimp,
Kl(x; W) := Exo〜paug(∙∣x) [Kl(x0; W)] is the expected connection under the augmentation distri-
bution, conditional on the datapoint x and α is the learning rate∙
Theorem 4 (Covariance Operator for Ltτri and Lτnce (H = 1, single negative pair)). Let r :=
2 IIfL(X)-fL(x0)∣2 ∙ The covariance operator OPl(W) = 2 Vx,xo 〜p(,)[Kl(x) -Kl(x0 )]+corr,
where Corr = O(Ex,xo 〜p(∙) [Jr(X, X0)trVx00 〜paug(∙∣x)[fL(X〃)]]) ∙ For LTri, ξ(r) = i+----?；)]
— r∕τ
(and limτ→0 ξ(r) = I(r ≤ ro))∙ For LTce，ξ(r) = T i+e-r/丁 ∙ For LsimP，ξ(r) ≡ 1 and Corr = O∙
3
Under review as a conference paper at ICLR 2021
(a)
Figure 2: Overview of Sec. 4. (a) To analyze the functionality of the covariance operator V砧 [KKι(zo)
(Eqn. 3), We assume that Nature generates the data from a certain generative model with latent variable zo and
z0, while data augmentation takes x(zo,z0), changes z0 but keeps zo intact. (b) Sec. 4.1: one layer one neuron
example. (c) Sec. 4.2: two-layer case where V[Kι] and V[K2] interplay. (d) Sec. 4.3: Hierarchical Latent Tree
Models and deep ReLU networks trained with SimCLR. A latent variable Zμ, and its corresponding nodes Nμ
in multi-layer ReLU side, covers a subset of input x, resembling local receptive fields in ConvNet.
Above, we use Covξ[X, Y] := E [ξ(X, Y)(X - EX])(Y - E [Y])|] and Vξ[X] := Covξ[X, X]
(Cov[X, Y] means ξ(∙) ≡ 1). The CoVarianCe operator OPl(W) is a time-varying PSD matrix over
the entire training procedure. Therefore, all its eigenvalues are non-negative and at any time t, Wl
is most amplified along its largest eigenmodes. Intuitively, OPl(W) ignores different views of the
same sample X by averaging over the augmentation distribution to compute Kl(x), and then com-
putes the expected covariance of this augmentation averaged connection with respect to the data
distribution p(x). Thus, at all layers, any variability in the connection across different data points,
that survives augmentation averages, leads to weight amplification. This amplification of weights
by the PSD data covariance of an augmentation averaged connection constitutes a fundamental de-
scription of SimCLR learning dynamics for arbitrary data and augmentation distributions.
4 How the covariance operator drives the emergence of features
To concretely illustrate how the fundamental covariance operator derived in Theorem 3-4 drives fea-
ture emergence in SimCLR, we setup the following paradigm for analysis. The input x = x(z0, z0)
is assumed to be generated by two groups of latent variables, class/sample-specific latents z0 and
nuisance latents z0 . We assume data augmentation only changes z0 while preserving z0 (Fig. 2(a)).
For brevity we use Theorem 3 (LsimP), then OP = Vz0 [Kl(zo)] since z0 is averaged out in Kl(zo).
In this setting, we first show that a linear neuron performs PCA within an augmentation preserved
subspace. We then consider how nonlinear neurons with local receptive fields (RFs) can learn to
detect simple objects. Finally, we extend our analysis to deep ReLU networks exposed to data gener-
ated by a hierarchical latent tree model (HLTM), proving that, with sufficient over-parameterization,
there exist lucky nodes at initialization whose activation is correlated with latent variables underly-
ing the data, and that SimCLR amplifies these initial lucky representations during learning.
4.1	Self-supervised learning and the single neuron: illustrative examples
A single linear neuron performs PCA in a preserved subspace. For a single linear neuron
(L = 1, nL = 1), the connection in definition 1 is simply K1(x) = x. Now imagine the input space
x can be decomposed into the direct sum of a semantically relevant subspace, and its orthogonal
complement, which corresponds to a subspace of nuisance features. Furthermore, suppose the aug-
mentation distribution PaUg (∙∣x) is obtained by multiplying X bya random Gaussian matrix that acts
only in the nuisance subspace, thereby identically preserving the semantic subspace. Then the aug-
mentation averaged connection K1 (x) = Qsx where Qs is a projection operator onto the semantic
subspace. In essence, only the projection of data onto the semantic subspace survives augmentation
averaging, as the nuisance subspace is scrambled. Then OP = Vx[Kι(x)] = QsVχ[x]Qsl. Thus
the covariance of the data distribution, projected onto the semantic subspace, governs the growth
of the weight vector W1 , demonstrating SimCLR on a single linear neuron performs PCA within a
semantic subspace preserved by data augmentation.
A single linear neuron cannot detect localized objects. We now consider a generative model
in which data vectors can be thought of as images of objects of the form X(z0, z0) where z0 is an
important latent semantic variable denoting object identity, while z0 is an unimportant latent variable
denoting nuisance features, like object pose or location. The augmentation procedure scrambles
pose/position while preserving object identity. Consider a simple concrete example (Fig. 3(a)):
X(z0, z0) =	ez0 + e(z0+1) mod d	z0 =1	(4)
ez0 + e(z0+2) mod d z0 = 2,
Here 0 ≤ z0 ≤ d - 1 denotes d discrete translational object positions on a periodic ring and z0 ∈
{1, 2} denotes two possible objects 11 and 101. The distribution is uniform both over objects
4
Under review as a conference paper at ICLR 2021
Figure 3: (a) Two 1D objects under translation: (a1) Two different objects 11 (zo = 1) and 101 (zo = 2)
located at different locations specified by z0. (a2) The frequency table for a neuron With local receptive field of
size 2. (b) In two-layer case (Fig. 2(c)), V[Kι] and V[K2] interplay in two-cluster data distribution.
and positions: p(z0,z0) = 9. Augmentation shifts the object to a uniformly random position
via PaUg(Z0∣z0) = 1/d. For a single linear neuron K1(x) = x, and the augmentation-averaged
connection is K1 (z0) = W L and is actually independent of object identity z° (both objects activate
two pixels at any location). Thus OP1 = Vz0 [K 1(z0)] = 0 and no learning happens.
A local receptive field (RF) does not help. In the same generative model, now consider a linear neu-
ron with a local RF of width 2. Within the RF only four patterns can arise: 00, 01, 10, 11. Taking
the expectation over z0 given zo (Fig. 3(a2)) yields K 1(z0=l) = d [x11 + x01 + x10 + (d - 3)xoo]
and K 1(z0=2) = ɪ [2x0ι + 2xιo + (d - 4)xoo]. Here, x11 ∈ R2 denotes pattern 11. This yields
OP1 = Vzo [K 1(z0)] = 4d2uu|	where U ：= x11 + xoo - xoι - xιo,	(5)
and OP1 ∈ R2×2 since the RF has width 2. Note that the signed sum of the four pattern vectors in U
actually cancel, so that U = 0, implying OPi = 0 and no learning happens. Interestingly, although
the conditional distribution of the 4 input patterns depends on the object identity zo (Fig. 3(a2)), a
linear neuron cannot learn to discriminate the objects.
A nonlinear neuron with local RF can learn to detect object selective features. With a ReLU
neuron with weight vector w, from Def. 1, the connection is now Ki (x, W) = ψ0(wlx)x. Suppose
w(t) happens to be selective for a single pattern xp (where p ∈ {00, 01, 10, 11}), i.e., w(t)|xp >
0 and w(t)lXpθ < 0 for p0 = p. The augmentation averaged connection is then Kι(zQ) 8 Xp where
the proportionality constant depends on object identity zo and can be read off (Fig. 3(a2)). Since this
averaged connection varies with object identity zo for all p, the covariance operator OP1 is nonzero
and is given by Vz0 [K 1 (zo)] = cpxpxp where the constant Cp > 0 depends on the selective pattern
p and can be computed from Fig. 3(a2). By Theorem 3, the dot product xp|w(t) grows over time:
x∣w(t + 1) = x| (I2×2 + αcpXpX∣) w(t) =(1 + αcp∣∣xpk2) x∣Wj(t) > x∣Wj(t) > 0. (6)
Thus the learning dynamics amplifies the initial selectivity to the object selective feature vector xp
in a way that cannot be done with a linear neuron. Note this argument also holds with bias terms
and initial selectivity for more than one pattern. Moreover, with a local RF, the probability of weak
initial selectivity to local object sensitive features is high, and we may expect amplification of such
weak selectivity in real neural network training, as observed in other settings (Williams et al., 2018).
4.2	A Two -layer Case with Multiple Hidden Neurons
Now consider a two-layer network (L = 2). The hidden layer has n1 ReLU neurons while the
output has n2 (Fig. 2(c)). In this case, the augmentation-averaged connection Ki(zo) at the lower
layer l = 1 can be written as (d = no is the input dimension):
Ki(zo) = [w2,1ul(zo), w2,2ul(zo),..., w2,n1 混](zo)]| ∈ Rn1d×n2	⑺
where w1,j ∈ Rd and w2,j ∈ Rn2 are weight vectors into and out of hidden neuron j (Fig. 2(c)),
and Uj (zo) := Ez0|z0 x(zo, z0)I(w1|,j x(zo, z0) ≥ 0) ∈ Rd is the augmentation average of only
those inputs that activate hidden neuron j . While the gradient dynamics in SimCLR under Lsimp
has a close form (Eqn. 65), itis hard to see what happens. Instead, we consider an intuitive sub-case:
Theorem 5 (Dynamics of two-layer (W2 diagonal)). If n1 = n2 and W2 = diag(w2,1, . . . , w2,n1 ):
w 2,j = (w|,j Aj wi,j )w2,j, W 1,j = w2,j Aj wi,j, where Aj := Vzo [uj (zo)].	⑻
Note that for ReLU neurons, Aj changes with w1,j , while for linear neurons, Aj would be con-
stant, since gating ψ0(w∣jx) ≡ 1. It is easy to see that dw2j∕dt = d∣∣wι,j∣∣2/dt and thus
w22,j = kW1,j k22 + c where c is some time-independent constant. Since Aj is always PSD,
W1|,j Aj W1,j ≥ 0 and |w2,j | generically increases, which in turn accelerates the dynamics of W1,j,
which is most amplified at any given time, along the largest eigenvector of Aj . This dynamics ex-
hibits top-down modulation whereby the top-layer weights accelerate the training of the lower layer.
5
Under review as a conference paper at ICLR 2021
Symbol	Definition	Size	Description
Nl , Zl Nμ, Nμh			The set of all nodes and all latent variables at layer l. NOdeS COrreSPOnding to Iatent VariabIe Zμ. Nμh are ChiIdren Under Nμ.
Pμν Vj(ZM), Vj fμ, fNch 	 μ	WVK) Ez [fj |ZM], [vj (ZM)] [fj]j∈Nμ, [fk]keNch	2×2 scalar, 2 ∣Nμ∣,∣Nμhι	The top-down transition probability from zμ to ZV. Expected activation f given zμ (zμ,s descendants are marginalized). Activations for all nodes j ∈ Nμ and for the children of Nμ
Pμν ρ0 sk aμ	2P(zν=1∣Zμ=1) — 1 P(z0 = 1) - P(zo = 0) 2(vk⑴-Vk(0)) [ρμν(k)sk ]k∈N ch μ	scalar in [—1,1] scalar scalar ∣Nμhl	Polarity of the transitional probability. Polarity of probability of root latent zo. Discrepancy of node k w.r.t its latent variable z“(k). Child selectivity vector.
Table 1: Notation for Sec. 4.3 (binary symmetric HLTM).
Previous works (Allen-Zhu and Li, 2020) also mention a similar concept in supervised learning,
called “backward feature correction.” Here we demonstrate rigorously that a similar behavior can
occur in SSL under gradient descent in the 2-layer case when the top layer W2 is diagonal.
As an example, Consideramixture of Gaussians: X 〜2 I(z0 = 1)N (wj,σ2) + 2 I(z0 = 2)N (wg ,σ2)
and let ∆w* := Wi — w2, then in the linear case, Aj 〜 ∆w*∆w*1 and wɪ,j converges to ±∆w*
(Fig. 3(b)). In the nonlinear case with multiple Gaussians, if one of the Gaussians sits at the origin
(e.g., background noise), then dependent on initialization, Aj evolves into wkiwki| for some center
k, and w1,j → wki. Note this dynamics is insensitive to specific parametric forms of the input data.
4.3	Deep ReLU SSL training with Hierarchical Latent Tree Models (HLTM)
We next study how multi-layer ReLU networks learn from data generated by an HLTM, in which
visible leaf variables are sampled via a hierarchical branching diffusion process through a sequence
of latent variables starting from a root variable z0 (Fig. 2(d, left)). The HLTM represents a mathemat-
ical abstraction of the hierarchical structure real-world objects, which consist of spatially localized
parts and subparts, all of which can lie in different configurations or occluded states. See Appendix
D.2 for a detailed description and motivation for the HLTM. Simpler versions of the HLTM have been
used to mathematically model how both infants and linear neural networks learn hierarchical struc-
ture (Saxe et al., 2019). We examine when a multi-layer ReLU network with spatially local RFs can
learn the latent generative variables when exposed only to the visible leaf variables (Fig. 2(d, right)).
We define symbols in TbL 1. At layer l, We have categorical latent variables {zμ}, where μ ∈ Zi
indexes different latent variables. Each zμ can take discrete values. The topmost latent variable is
z0. Following the tree structure, for μ ∈ Zi and ν1, ν2 ∈ Zi-1, conditional independence holds:
P(zνι, Zν2 ∣zμ) = P(zνι ∣Zμ)P(zν2 ∣Zμ). The final sample X is just the collection of all visible leaf
variables (Fig. 2(d)), and thus depends on all latent variables. Corresponding to the hierarchical tree
model, each neural network node j ∈ N maps to a unique μ = μ(j) ∈ Zi. Let Nμ be all nodes that
map to μ. For j ∈ Nμ, its activation fj only depends on the value of zμ and its descendant latent
variables, through input x. Define Vj(zμ) := Ez [fj∣Zμ] as the expected activation w.r.t Zμ. Given a
sample x, data augmentation involves resampling all zμ (which are z0 in Fig. 2), fixing the root z0.
Symmetric Binary HLTM. Here we consider a symmetric binary case: each zμ ∈ {0,1} and for
μ ∈ Zl, V ∈ Zi -1, P(ZV = 1 | zμ = I) = P(ZV = 0 | zμ =O) = (1 + ρμν) /2, where the polarity
Pμν ∈ [一1,1] measures how informative Zμ is. If ρμν = ±1 then there is no stochasticity in the
top-down generation process; ρμν = O means no information in the downstream latents and the
posterior of Z0 given the observation x can only be uniform. See Appendix for more general cases.
Now we compute covariance operator OPμ = Vzo[Kμ(zo)] at different layers, where Kμ(zo)
Ez0
[fNμh ® J||zo]
. Here we mainly check the term Ez0
[fNμh |zo]
and assume Jμ is constant.
Theorem 6 (Activation covariance in binary HLTM). Vz0[Ez0 [fNCh |z0] ] = θμa*aμ. Here a* :=
[Pμν(k)Sk ]k∈Nμh and 0μ ：= ρ2μ(1 — P0). If maXαβ ∣Pαβ | < Lthen limL→+∞ ρoμ → O for μ ∈ ZI.
Theorem 6 suggests when ρ0* and ∣∣aμk are large, the covariance OPμ = θμa*aμ 0 J| Jμ is large
and training is faster. For deep HLTM and deep networks, at lower layers, ρoμ → O and Poμ is
uniform due to mixing of the Markov Chain, making OP* small. Thus training in SSL is faster at
the top layers where the covariance operators have large magnitude. On the other hand, large ∣∣aμ∣
implies sk := (vk(1) — vk(O))/2 is large, or the expected activation vk(ZV) is selective for different
values of ZV for V ∈ ch(μ). Interestingly, this can be achieved by over-parameterization (∣Nμ∣ > 1):
6
Under review as a conference paper at ICLR 2021
Theorem 7 (Lucky nodes in deep ReLU networks regarding to binary HLTM at initializa-
tion). Suppose each element of the weights Wl between layer l + 1 and l are initialized with
[-σw，3/|Nn σw ,3∕∣NCh∣]
Uniform
.There exists σ2 so that V[fk ∣Zν] ≤ σ2 for any k ∈ Ni.
For any μ ∈ Zi+i, if ∣Nμ∣ = O(exp(c)), then with high probability, there exists at least one node
j ∈ Nμ so that their pre-activation gap Vj (1) 一 Vj (0) = 2w|aμ > 0 and the activations satisfy:
卜2⑴一	Vf(O) I	≥ 3σW	4|NH X	IVk(I)- vk (O)|2	(++6p p“ν	- 1)	一 σ2	.	⑼
μ k∈Nch	、	/
I-	μ	」
Intuitively, this means that with large polarity ρμν (strong top-down signals), randomly initialized
over-parameterized ReLU networks yield selective neurons, if the lower layer also contains selective
ones. For example, when σι = 0, C = 9, if ρμν ≥ 63.3% then there is a gap between expected
activations Vj (1) and Vj (0), and the gap is larger when the selectivity in the lower layer l is higher.
Note that at the lowest layer, {Vk} are themselves observable leaf latent variables and are selective
by definition. So a bottom-up mathematical induction of latent selectivity will unfold.
If We further assume JJ Jμ = I, then after the gradient update, for the “lucky" node j We have:
aμwj (t + 1) = aμ [I + α0μaμaμ Wj ⑴=(I + αoμkaμk2)aμwj ⑴ > %wj ⑴ > 0
which means that the pre-activation gap Vj (1) 一 Vj (0) = 2w∣aμ grows over time and the latent
variable zμ is learned (instantiated as fj) during training, even if the network is never supervised
with its true value. While here we analyze the simplest case (JlJμ = I), in practice Jμ changes
over time. Similar to Sec. 4.2, once the top layer starts to have large weights, the magnitude of Jμ
for lower layer becomes larger and training is accelerated.
We implement the HLTM and confirm, as predicted by our theory, that the intermediate layers of
deep ReLU networks do indeed learn the latent variables of the HLTM (see Tbl. 2 below).
5 Analysis of ingredients underlying BYOL learning
In BYOL, the two networks are no-longer identical and, interestingly, only positive pairs are used
for training. The first network with weights W1 = W := {Wbase, Wpred} is an online network
that is trained to predict the output of the second target network with weights W2 = W0, using a
learnable predictor Wpred to map the online to target outputs (Fig. 1(a) and Fig. 1 in Grill et al.
(2020)). In contrast, the target network has W0 := {Wb0ase}, where Wb0 ase is an exponential moving
average (EMA) of Wbase: Wbase(t +1)= YemaWbase(t) + (1 — Yema)Wbase(t). Since BYOL only
uses positive pairs, we consider the following loss function:
r ：=1 kfL(xi； W) — fL0 (x+； W0)k2	(10)
where the input data are positive pairs: xi,x+ 〜 Paug(∙∣x) and X 〜 p(∙). The two outputs,
fL(x1； W) and fL0 (x+； W0), are from the online and the target networks, respectively. Note that
L0 < L due to the presence of an extra predictor on the side of online network (W).
With neither EMA (Yema = 0) nor the predictor, W0 = W and the BYOL update without BN is
Vec (∆Wι)Sym = —Ex [Vχ0〜Paug(∙∣χ) [Ki(x0)]] Vec(Wi)	(11)
(see App. E.1 for proof). This update only promotes variance minimization in the representations of
different augmented views of the same data samples and therefore would yield model collapse.
We now consider the effects played by the extra predictor and BatchNorm (BN) (Ioffe and Szegedy,
2015) in BYOL. Our interest in BatchNorm is motivated by a recent blogpost (Fetterman and Al-
brecht, 2020). We will see that combining both could yield a sufficient condition to create an implicit
contrastive term that could help BYOL learn. As pointed out recently by Richemond et al. (2020a),
BYOL can still work using other normalization techniques that do not rely on cross batch statistics
(e.g., GroupNorm (Wu and He, 2018), Weight Standardization (Qiao et al., 2019) and careful ini-
tialization of affine transform of activations). In Appendix F we derive exact solutions to BYOL
for linear architectures without any normalization, to provide conceptual insights into how BYOL
can still learn without contrastive terms, at least in the linear setting. Here we focus on BatchNorm,
leaving an analysis of other normalization techniques in nonlinear BYOL settings for future work.
When adding predictor, Theorem 1 can still be applied by adding identity layers on top of the target
network W0 so that the online and the target networks have the same depth. Theorem 5 in (Tian,
7
Under review as a conference paper at ICLR 2021
2018) demonstrates this version of BN shifts the downward gradients so their mini-batch mean is 0:
≈i i 1	i i
gl := gl - |B| T gl = gl - gl
i∈B
(12)
Here gi is the i-th sample in a batch and gι is the batch average (same for fι). BackProPagating
through this BN (vs. just subtracting the mean only in the forward pass1), leads to a correction term:
Theorem 8. If (1) the network is linear from layer l to the topmost and (2) the downward gradient
gl undergoes Eqn. 12, then with large batch limits, the correction of the update is2 (for brevity,
dependency on W is omitted, while dependency on W 0 is made explicit):
vec(δWlBN) = Eχ [Kι(x)] {Eχ [K|(x) Vec(Wl) - Eχ [K|(x; W0)] vec(W∕)}	(13)
1 .1	1	■	1 .	1	.	^Λ~γTλ	Λ TTT- . ΓTT7-RN TT -	7~∙	1 1	1
and the corrected weight update is ∆Wl := ∆Wl + δWlBN. Using Eqn. 11, we have:
vec(∆]ι) = vec(∆Wι)sym - Vx [Kι(x)] vec(Wι) + CoVχ [Kι(x'),Kι(x; W )] vec(Wι)	(14)
Corollary 1 (SimCLR). For SimCLR with contrastive losses Lsimp, Ltri and Lnce, δWlBN = 0.
5.1	THE CASE WITHOUT EMA (γema = 0 AND THUS Wbase = Wb0 ase)
In BYOL when the Predictor is Present, W0 6= W and BN is Present, from the analysis above we
know that δWlBN 6= 0, which Provides an imPlicit contrastive term. Note that W0 6= W means there
is a Predictor, the online network uses EMA, or both. We first discuss the case without EMA.
From Theorem 8, if we further consider a single linear Predictor, then the following holds. Here
Kl,base(x) ：= Kl(x; Wbase) and zero-mean expected connection Kl(x) := Kl(x) — Ex [Kl(x)].
Corollary 2 (The role of a Predictor in BYOL). If Wpred = {Wpred } is linear and no EMA, then
vec(∆Wl) = vec(∆Wl)sym + ExhKl,base(X)Wpred(I — WPred)K[base(2)] Vec(Wl)∙ Ifthere is
]
no stop gradient, then vec(∆Wl) = 2vec(∆Wl)sym — Vx [Kl,base(x)(I - Wpred)1] Vec(Wl).
The Predictor. To see why Wpred plays a critical role, we check some special case. If
Wpred = βInL×nL (Wpred has to be a squared matrix), then Vec(∆Wl) = Vec(∆Wl)sym + β(1 —
β)Vx [Kl,base(x)] vec(Wl). If 0 < β < 1, then β(1 — β) > 0 and the covariance operator appears.
In this regime, BYOL works like SimCLR, except that it also minimizes variance across different
augmented views of the same data sample through Vec(∆Wl)sym (Eqn. 11), the first term in Eqn. 14.
Indeed, the recent blogpost (Fetterman and Albrecht, 2020) as well as our own experiments (Tbl. 3)
suggests that standard BYOL without BN fails. In addition, we also initialize the predictor with
small positive weights (See Appendix G.4), and reinitialize the predictor weights once in a while
(Tbl. 5), and BYOL still works, consistent with our theoretical prediction.
Stop Gradient. In BYOL, the target network W0 serves as a target to be learned from, but does not
contribute gradients to the current weight W . Without EMA, we might wonder whether the target
network should also contribute the gradient or not. Corollary 2 shows that this won’t work: no
matter what Wpred is, the update always contains a (weighted) negative covariance operator.
5.2	Dynamics of Exponential Moving Average (EMA)
On the other hand, the EMA part might play a different role. Consider the following linear dynamic
system, which is a simplified version of Eqn. 14 (we omit Vec(∆Wl)sym and Wl0 = Wl,ema):
w(t+ 1) — w(t) = ∆w(t) = α [—w(t) + (1 — λ)wema(t)]	(15)
Theorem 9 (EMA dynamics in Eqn. 15). w(t) H (1 + κ)t. Here we define K := 2(η +
α) (pl + 4αηλ∕(η + α)2 — 1)and η := 1 — Yema. Moreover, if λ ≥ 0, then K ≤ λ∕(1∕α + 1∕η).
From the analysis above, when β is small, We see that the coefficient before Wl (〜β2) is typically
smaller than that before Wl,ema (〜β). This means λ > 0. In this case, κ > 0 and w(t) grows
exponentially and learning happens. Compared to no EMA case (γema = 0 or η = 1), with EMA,
We have η < 1 and K becomes smaller. Then the growth is less aggressive and training stabilizes.
1In PyTorch, the former is x-x.mean() and the latter is x-x.mean().detach().
2A formal treatment requires Jacobian J to incorporate BatchNorm’s contribution and is left for future work.
8
Under review as a conference paper at ICLR 2021
Table 2: Normalized Correlation between the topmost latent variables in binary HLTM and topmost nodes in
deep ReLU networks (L = 5) go up when training with SimCLR with NCE loss. We see higher correlations at
both initialization and end of training, with more over-parameterization (Left:
ρμν
〜Uniform[0.7,1]
〜Uniform[θ.8, l]
〜Uniform[θ.9, l]
Initial
0.51
0.65
0.81
1 epoch
0.69
0.76
0.85
20 epochs
0.76
0.79
0.86
ρμν
〜Uniform[O7,1]
〜Uniform[θ.8, l]
〜Uniform[θ.9, l]
Nμl =
Initial
0.60
0.73
0.87
2, Right: ∣Nμ∣ = 5).
1 epoch
0.72
0.80
0.90
20 epochs
0.88
0.87
0.95
Table 3: Top-1 STL performance with different combination of predictor (P), EMA and BatchNorm using
BYOL. EMA means γema = 0.996. Batch size is 128 and all experiments run on 5 seeds and 100 epochs.
I - I EMA I BN I EMA, BN ∣	∣ P ∣ P,EMA ∣	P,BN	∣P,EMA,BN∣
138.7 ± 0.6139.3 ± 0.9133.0 ± 0.3∣32.8 ± 0.5	39.5 ± 3.1144.4 ± 3.2163.6 ± 1.061 78.1 ± 0.3
Table 4: Top-1 STL performance using different BatchNorm components in the predictor and the projector
of BYOL (Yema = 0.996, 100 epochs). There is no affine part. ''μ'' = zero-mean normalization only, ''μ,σ''
=BN without affine, “'μ, σ才” =normalization with mean and Std but only backpropagating through mean. All
variants with detached zero-mean normalization (in red) yield similar poor performance as no normalization.
-	μ	σ	μ,σ	μ才	σ才	μ",σ	μ,σ才	μH, σ"
43.9 ± 4.2	64.8 ± 0.6	72.2 ± 0.9	78.1 ± 0.3	44.2 ± 7.0	54.2 ± 0.6	48.3 ± 2.7	76.3 ± 0.4	47.0 ± 8.1
Table 5: Top-1 performance of BYOL using reinitialization of the predictor every T epochs.
	Original BYOL	ReInit T = 5	ReInit T = 10	ReInit T = 20
STL-10(100 epochs) ImageNet (60 epochs)	781 60.9	786 61.9	79.1 62.4	79.0 62.4
6 Experiments
We test our theoretical findings through experiments on STL-10 (Coates et al., 2011) and Ima-
geNet (Deng et al., 2009). We use a simplified linear evaluation protocol: the linear classifier is
trained on frozen representations computed without data augmentation. This reuses pre-computed
representations and accelerates evaluation by 10x. See Sec. G.3 for detailed setup.
Hierarchical Latent Tree Model (HLTM). We implement HLTM and check whether the intermediate
layers of deep ReLU networks learn the corresponding latent variables at the same layer. The degree
of learning is measured by the normalized correlations between the ground truth latent variable
zμ and its best corresponding node j ∈ Nμ. Tbl. 2 indicates this measure increases with over-
parameterization and learning, consistent with our analysis (Sec. 4.3). More experiments in Sec. G.2.
Factors underlying BYOL performance. To test our theory, we perform an ablation study of
BYOL on STL-10 by modifying three key components: predictor, EMA and BN. Tbl. 3 shows
that BN and predictor are important and EMA further improves the performance. First, without a
predictor, neither BN nor EMA give good performance. A predictor without BN still doesn’t work.
A predictor with BN starts to show good performance (63.6%) and further adding EMA leads to the
best performance (78.1%). This is consistent with our theoretical findings in Sec. 5, in which we
show that using a predictor with BN yields δWlBN 6= 0 and leads to an implicit contrastive term.
To further test our understanding of the role played by BN, we fractionate BN into several
sub-components: subtract by batch mean (mean-norm), divide by batch standard deviation
(std-norm) and affine, and do ablation studies (Tbl. 4). Surprisingly, removing affine
yields slightly better performance on STL-10 (from 78.1% to 78.7%). We also find that variants of
mean-norm performs reasonably, while variants of detached mean-norm has similar poor perfor-
mance as no normalization, supporting that centralizing backpropagated gradient leads to implicit
contrastive terms (Sec. 5). Note that std-norm also helps, which we leave for future analysis.
We also check whether the online network requires an “optimal predictor” as suggested by recent
version (v3) of BYOL. For this, we reinitialize the predictor (ReInit) every T epochs and compare
the final performance under linear evaluation protocol. Interestingly, as shown in Tbl. 5, ReInit
actually improves the performance a bit, compared to the original BYOL that keeps training the same
predictor, which should therefore be closer to optimal. Moreover, if we shrink the initial weight
range of the predictor to make Covx [KKι(x), Kι(x; W0)] (third term in Eqn. 14) more dominant,
and reduce the learning rate, the performance further improves (See Tbl. 10 in Appendix G.4),
thereby corroborating our analysis.
9
Under review as a conference paper at ICLR 2021
References
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 9729-9738, 2020.
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. Scaling and benchmarking self-
supervised visual representation learning. In Proceedings of the IEEE International Conference
on Computer Vision, pages 6391-6400, 2019.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representa-
tions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 6707-6717, 2020.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Anne Wu, Changhan Wang, Juan Pino, and Jiatao Gu. Self-supervised representations improve
end-to-end speech translation. arXiv preprint arXiv:2006.12124, 2020.
Alexei Baevski and Abdelrahman Mohamed. Effectiveness of self-supervised pre-training for asr. In
ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 7694-7698. IEEE, 2020.
Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of
discrete speech representations. arXiv preprint arXiv:1910.05453, 2019.
Yasaman Bahri, Jonathan Kadmon, Jeffrey Pennington, Sam S Schoenholz, Jascha Sohl-Dickstein,
and Surya Ganguli. Statistical mechanics of deep learning. Annual Review of Condensed Matter
Physics, March 2020.
Yuandong Tian. Student specialization in deep relu networks with finite width and input dimension.
ICML, 2020.
Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020.
Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and trans-
fer learning in deep linear networks. In International Conference on Learning Representations
(ICLR), 2018.
David Saad and Sara A Solla. Dynamics of on-line gradient descent learning for multilayer neural
networks. In Advances in neural information processing systems, pages 302-308, 1996.
William Falcon and Kyunghyun Cho. A framework for contrastive self-supervised learning and
designing a new approach, 2020.
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual represen-
tation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recog-
nition, pages 1920-1929, 2019.
Senthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning: In-
variances, augmentations and dataset biases. arXiv preprint arXiv:2007.13916, 2020.
10
Under review as a conference paper at ICLR 2021
Abe Fetterman and Josh Albrecht. Understanding self-supervised and contrastive learning with
”bootstrap your own latent” (byol), 2020. URL https://untitled-ai.github.io/
understanding-self-supervised-contrastive-learning.html#fn:ssup.
Pierre H. Richemond, Jean-Bastien Grill, Florent Altche, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works
even without batch statistics. arXiv, 2020a.
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsu-
pervised learning of visual features. In Proceedings of the European Conference on Computer
Vision (ECCV), pages 132-149, 2018.
Humam Alwassel, Dhruv Mahajan, Lorenzo Torresani, Bernard Ghanem, and Du Tran. Self-
supervised learning by cross-modal audio-video clustering. arXiv preprint arXiv:1911.12667,
2019.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi.
A theoretical analysis of contrastive unsupervised representation learning. February 2019.
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot
image recognition. In ICML deep learning workshop, volume 2. Lille, 2015.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 815-823, 2015.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Alex H Williams, Tony Hyun Kim, Forea Wang, Saurabh Vyas, Stephen I Ryu, Krishna V Shenoy,
Mark Schnitzer, Tamara G Kolda, and Surya Ganguli. Unsupervised discovery of demixed,
Low-Dimensional neural dynamics across multiple timescales through tensor component anal-
ysis. Neuron, 98(6):1099-1115.e8, June 2018.
Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., May 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. ICML, 2015.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
computer vision (ECCV), pages 3-19, 2018.
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv
preprint arXiv:1903.10520, 2019.
Yuandong Tian. A theoretical framework for deep locally connected relu network. arXiv preprint
arXiv:1809.10829, 2018.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pages 215-223, 2011.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
JG Liao and Arthur Berg. Sharpening jensen’s inequality. The American Statistician, 2018.
Pierre H Richemond, Jean-Bastien Grill, Florent Altche, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. BYOL works
even without batch statistics. October 2020b.
A Saxe, J McClelland, and S Ganguli. Exact solutions to the nonlinear dynamics of learning in deep
linear neural networks. In International Conference on Learning Representations (ICLR), 2014.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Netw., 2(1):53-58, January 1989.
11
Under review as a conference paper at ICLR 2021
A Background and Basic Setting (Section 2)
A.1 Lemmas
Definition 2 (reversibility). A layer l is reversible if there is a Gl(x; W) ∈ Rnl ×nl-1 so that
fl(x; W) = Gl (x; W)fl-1 (x; W) and gl-1 = Gl|(x; W)Qlgl for some constant PSD matrix
Rnl ×nl 3 Ql 0. A network is reversible if all layers are.
Note that many different kinds of layers have this reversible property, including linear layers (MLP
and Conv) and (leaky) ReLU nonlinearity. For multi-layer ReLU network, for each layer l, we have:
Gl(x;W) = Dl(x;W)Wl,	Ql ≡ Inl×nl	(16)
where Dl ∈ Rnl ×nl is a binary diagonal matrix that encodes the gating of each neuron at layer l.
The gating Dl (x; W) depends on the current input x and current weight W.
In addition to ReLU, other activation function also satisfies this condition, including linear,
LeakyReLU and monomial activations. For example, for power activation ψ(x) = xp where p > 1,
we have (where fl is the pre-activation at layer l):
Gι(x; W) =diagp-1(fι)Wι,	Ql ≡ pIm×m	(17)
Remark. Note that the reversibility is not the same as invertible. Specifically, reversibility only
requires the transfer function of a backpropagation gradient is a transpose of the forward function.
Lemma 1 (Recursive Gradient Update (Extension to Lemma 1 in (Tian, 2020)). Let (pseudo)-
Jacobian matrix JL(X) = InL×nL，and recursively define Jl-ι(x) := Jl(X)√QlGl(x) ∈
Rnl ×n-1. Here √Ql is the constant PSD matrix so that √Ql√Ql = Ql 占 0.
If (1) the network is reversible (Def. 2) and (2) QQi commutes With Jl(XI)lJl(xι) and
Jl (X1 )|Jl (X2), then minimizing the `2 objective
r(Wι) := 2kfL(xι; Wι) - fL(X2； W2)k2	(18)
with respect to weight matrix Wl at layer l yields the following gradient at layer l:
gl = JlT(Xi； Wι) Jl(xi； W1)fl(x1; Wι) - Jl(x2; W2) fl(X2； W2)]	(19)
Proof. We prove by induction. Note that our definition of Wl is the transpose of Wl defined in (Tian,
2020). Also our gl(X) is the gradient before nonlinearity, while (Tian, 2020) uses the same symbol
for the gradient after nonlinearity.
For notation brievity, we let fl(X1) := fl(X1; Wl) and Gl(X1) := Gl(X1; Wl). Similar for X2 and
W2.
When l = L,by the property of '2-loss, We know that gL = ∕l(xi; Wi) — fL(X2; W2), by setting
JL(X1) = JL(X2) = I, the condition holds. Now suppose for layer l, we have:
gl = JlT(XI) Jl(xi)fl (xi) - Jl (x2)fl(x2)]	(20)
Then:
gl-1 = GlT(X1)Qlgl	(21)
=GT(XI)QlJlT(Xi) Jl(Xi)fl(Xi) - Jl(X2)fl(X2)]	(22)
=GT(XI)pQJlτ(Xι) ∙ ∣^Jl(X1)pQ?fl(Xi) - Jl(X2)pQ^fl(X2)i	(23)
'--------{z-------} L	」
Jl|-1 (x1 )
=Jl-I(X1) Jl(XI) VzQlGl(Xi) fl-1(X1) - Jl(X2) VZQGl(X2) fl-1(X2)	(24)
`--------{--------}	`--------{--------}
Jl-1(x1)	Jl-1(x2)
= Jl-I(X1) [Jl-i(X1)fl-i(X1) — Jl-I(X2)fl-i(X2)]	(25)
12
Under review as a conference paper at ICLR 2021
Note that for multi-layered ReLU network, Gl(x) = Dl (x)Wl, Ql = I for each ReLU+Linear
layer, if We set xι = x2 = x, Wi = W, W2 = W * (teacher weights), then We go back to the
original Lemma 1 in (Tian, 2020).	□
Remark on ResNet. Note that the same structure holds for blocks of ResNet With ReLU activation.
An alternative form of Lemma 1. Note that We can alternatively group linear Weight With its
immediate doWnWard nonlinearity and Lemma 1 still holds. In this case, We Will have:
gι = J|(xi) [jl(xi)fl(xi) - J1(x2)f1(X2)i	(26)
where Jl (x) is the (PseUdo-)Jacobian: Jl(X) := ∂fL∣∂f (i.e., with respect to the pre-actιvatιon
fl), as defined in the notation paragraph of Sec. 2, and gl is the back-propagated gradient after the
nonlinearity. This will be Used in the following Lemma 2. For other reversible layers (e.g., Eqn. 17),
the relationship between the pseUdo-Jacobian and the real one can differ by a constant (e.g., some
power of √p).
A.2	`2 -NORMALIZATION IN THE TOPMOST LAYER
For '2-normalized layer fl := fl-1∕kfl-1∣∣2, we have Gl := 1/|Ifl-1∣∣2 ∙ 1“乂阳 and due to the
following identity (here y := y∕∣∣y∣∣2):
∣^ = ɪ (i - y yl)	(27)
∂y	IyI2
Therefore we have ∂fl∕∂fl-ι = (Im×m - flfll)Gl and we could set Ql := I - flf∣, which is a
projection matrix and thus PSD. Furthermore, since the normalization layer is at the topmost, Jl = I
and Ql trivially commutes with Jl Jl.
The only issue is that Ql is not a constant matrix and can change over training. Therefore Lemma 1
doesn’t apply exactly to such a layer but can be regarded as an approximate way to model.
A.3 Theorem 1
Now we prove Theorem 1 in a more general setting where the network is reversible (note that deep
ReLU networks are included and its Ql is a simple identity matrix):
Lemma 2 (Squared `2 Gradient for dual deep reversible networks). The gradient gWl of the squared
loss r with respect to Wl ∈ Rnl ×nl-1 for a single input pair {x1, x2} is:
gWl = vec (∂r∕∂W1,l) = K1,l hK1l,lvec(W1,l) - K2l,lvec(W2,l)i .	(28)
Here Kl(x; W):= fl-ι(x; W) 0 Jll(x; W), Ki,l := Kl(xi； Wi) and K2,l := Kl(x2; W2)∙
Proof. We consider more general case where the two towers have different parameters, namely W1
and W2 . Applying Lemma 1 for the branch with input xi at the linear layer l, and we have (See
Eqn. 26):
gi,l = J∣,l[Ji,lW1,lf1,l-1 - J2,lW2,lf2,l-1]	(29)
where fi,l-i := fl-i(xi; Wi) is the activation of layer l - 1 just below the linear layer at tower 1
(similar for other symbols), and gi,l is the back-propagated gradient after the nonlinearity.
In this case, the gradient (and the weight update, according to gradient descent) of the weight Wl
between layer l and layer l - 1 is:
∂r
∂W1l = g1,lfll-1	(30)
= Jil,lJi,lWi,lfi,l-ifil,l-i -Jil,lJ2,lW2,lf2,l-ifil,l-i	(31)
Using vec(AX B) = (Bl 0 A)vec(X) (where 0 is the Kronecker product), we have:
Vec (∂Wr7) = (f1,lτfl,l-1 0 J|,lJ1,l) vec(W1,l) - (f1,l-1fl,l-i 0 J|,lJ2,l) vec(W2,l)
,	(32)
13
Under review as a conference paper at ICLR 2021
Let
Kl(x; W):= fι-ι(x; W) 0 J|(x; W) ∈ Rnlnl-1×nL
(33)
Note that Kl(x; W) is a function of the current weight W, which includes weights at all layers. By
the mixed-product property of Kronecker product (A 0 B)(C 0 D) = AC 0 BD, we have:
∂r
Vec (∂WU
Kl (x1)Kl(x1)|vec(W1,l) - Kl(x1)Kl (x2)|vec(W2,l)
(34)
= Kl(X1) [Kl(X1)|vec(W1,l) — Kl(X2)|vec(W2,l)]
where Kl(X1) = Kl(X1; W1) and Kl(X2) = Kl(X2; W2).
In SimCLR case, we have W1 = W2 = W so
Vec (∂W[) = Kl(XI) [Kl(xι) — Kl(x2)]1 Vec(Wl)
(35)
(36)
□
B Analysis of SimCLR using Teacher- S tudent Setting (Section 3)
B.1 Theorem 2
Proof. For Lsimp and Ltri the derivation is obvious. For Lnce , we have:
∂L
∂r+
∂L _
∂rk-
and obviously we have:
-r+∕τ + PH=1 e-rk0-∕τ
e-rk- /τ
e-r+4 + PH=I e-rk0-4
∂L XX ∂L
dr+ + k=1 drk--
>0
< 0, k = 1, . . . , H
(37)
(38)
(39)
1 1 - e
1
—
τ
□
B.2	The Covariance Operator under different loss functions
Lemma 3. For a loss function L that satisfies Theorem 2, with a batch of size one with samples
X := {xι, x+, xι-, X2-,..., XH-}, where xι, x+ 〜Paug(∙∣x) are augmentation from the same
Sample X, and Xk-〜Paug(∙∣xk) are augmentations from independent samples Xk
have:
p(∙). We
H
vec(gWl ) = Kl(x1)
k=1
∂L
∂rk-
• (Kι(x+) — Kl(Xk-))1
X
vec(Wl)
(40)
〜
Proof. First we have:
vec(gWl)
∂L ∂L ∂r+ XX ∂L ∂rk-
∂Wl = ∂r+ ∂wl + ^ ∂rk- ∂wl
k=1
(41)
Then we compute each terms. Using Theorem 1, we know that:
∂r+
∂wi
∂rk-
^∂WΓ
Kl(x1)(Kl (x1) - Kl (x+))|vec(Wl)
Kl(x1)(Kl(x1) — Kl(xk-))|vec(Wl),	k = 1, . . . ,n
(42)
(43)
Since Eqn. 39 holds, Kl(x1)Kl|(x1) will be cancelled out and we have:
H
vec(gWl) = Kl(x1)
k=1
∂L
k (Kl(X+)- Kl(XkJ)T
vec(Wl )
(44)
□
14
Under review as a conference paper at ICLR 2021
B.3	Theorem 3
Proof. For LsimP ：= r+ - r-, We have H = 1 and ∂L ≡ -1. Therefore using Lemma 3, We have:
vec(gWl) = -Kl(x1) [Kl|(x+) - Kl|(x-)] vec(Wl)	(45)
Taking large batch limits, we know that E [Kι(xι)K^∣(x+)] = Ex [Kι(x)K|(x)] since xι, x+ 〜
PaUg(∙∣x) are all augmented data points from a common sample x. On the other hand,
E [Kι(xι)Kll(xk-)] = Ex [Kι(x)] Ex [K|(x)] since xι 〜PaUg(∙∣x) and Xk-〜Paug(∙∣xk)
are generated from independent samples x and x0k and independent data augmentation. Therefore,
vec(gWι) = -{Ex [Kι(x)K|(x)] - Ex [Kι(x)] Ex [K|(x)]卜ec(Wι)	(46)
=-Vx [K1 (x)] vec(Wι)	(47)
The conclusion follows since gradient descent is used and ∆Wι = -gwι.	□
B.4	Theorem 4
Proof. When ∂L∕∂rk- is no longer constant, we consider its expansion with respect to un-
augmented data point X0 = {x, x01, . . . , x0k}. Here X = {x1, x+, x1-, . . . , xH-} is one
data sample that includes both positive and negative pairs. Note that xι, x+ 〜PaUg(∙∣x) and
Xk-〜Paug(∙∣xk) for 1 ≤ k ≤ H.
∂L
∂rk-
_ ∂L
一drk-
X
+
X0
(48)
where E is a bounded quantity for Ltri (|e| ≤ 1) and Lnce (|e| ≤ 2∕τ).
We consider H = 1 where there is only a single negative pair (and r-). In this case X0 = {X, X0}.
Letr := 2kfL(χ)- fL(x0)k2andξ(x,x0) := -∂fL-IX0.
Note that for Ltri, it is not differentiable, so we could use its soft version: Ltτri(r+, r-) = τ log(1 +
e(r+-r-+r0"τ). It is easy to SeethatlimT→o LTri(r+,r-) → max(r+ - r- + r。, 0).
For the two losses:
• For LtTri, we have
ξ(X, X0) = ξ(r)
e-r/T
e-r0/τ + e-r/T
• For Lnce, we have
ξ(x, x0)= ξ(r) = 1
e-r/T
1 + e-r/T
(49)
(50)
Note that for LtTri we have limT →0 ξ(r) = I(r ≤ r0). for Lnce, since it is differentiable, by Taylor
expansion we have E = O(kX1 - Xk2, kX+ - Xk2, kX- - X0k2), which will be used later.
The constant term ξ with respect to data augmentation. In the following, we first consider the
term ξ, which only depends on un-augmented data points X0 . From Lemma 3, we now have a term
in the gradient:
gl(X) := -Kl(X1) [Kll(X+) - Kll(X-)] ξ(X, X0)vec(Wl)	(51)
Under the large batch limit, taking expectation with respect to data augmentation PaUg and notice
that all augmentations are done independently, given un-augmented data X and X0, we have:
gι(x, x0) := Epaug [gι(X)] = -Kl(X) [Kιl(x) - Kιl(x0)] ξ(x, x0)VeC(Wl)	(52)
Symmetrically, if we swap X and x0 since both are sampled from the same distribution p(∙), we
have:
gl(x0, x) = -Kl(x0) [Kll(x0) - Kll(x)] ξ(x0, x)vec(Wl)	(53)
15
Under review as a conference paper at ICLR 2021
since ξ(X0, X) only depends on the squared `2 distance r (Eqn. 49 and Eqn. 50), we have ξ(X0, X) =
ξ(X, X0) = ξ(r) and thus:
gι(x, x0) + gι(x0, x) = - [Kι(x)Ki|(x) - Kl(X)Ki|(x0) + Kι(x0)Ki|(x0) - KI(XO)Ki|(x)] ξ(r)vec(Wι)
-ξ(r)(KKι(X)- KI(Xy)(Kl(X)- KKι(x0))lvec(Wι)
(54)
Therefore, we have:
Eχ,χ0〜P [gι(X, X0)] = -2Eχ,χ0〜P [ξ(r)(Kι(X) - Kι(X0))(KI(X)- Kι(x0))t] Vec(Wι(55)
2VX,χo〜P [Kι(X) - Kι(X0)] Vec(WI)
(56)
Bound the error. For Lnce, let F := - ∂L, then we can compute their partial derivatives:
∂F	∂F
=-F(1∕τ -	F),	厂=F(1∕τ -	F)
∂r+	∂r-
Note that |F (1∕τ - F)| ≤ 1∕τ2 is always bounded. From Taylor expansion, we have:
(57)
=-
IFI	(r+-*) - IFI	(r- - r-)
dr： l{r+,r-}	+ ∂r- l{r+,r-}
(58)
for derivatives evaluated at some point {r+, r-} at the line connecting (x, x, x0) and (xι, x+, x-).
r+ and r- are squared '2 distances evaluated at (x, x, x0), therefore, r： ≡ 0 and r- = 2∣∣f (x)-
f(X0)k22 (note that here we just use f := fL for brevity).
Therefore, we have r+ - r+0
r - r0 =
=2 kf(χι)- f (x+)k2 and
[f(X) - f(X0)]| [(f(X1) - f(X)) - (f(X-) - f(X0))]
2 k(f (χι) - f (χ))-(f (χ-) - f (χ0))k2
(59)
Therefore, we have:
∂∂F(r+- r+)
≤ T2 ∙ 2 / kf (Xi) - f (X+)k2paug(Xl∣X)Paug(X+∣X)dXldX+
j12 trVaug[f |x]
τ2
(60)
where trVaug[f [x] ：= trVχo〜Paug(∙∣x)[f (x0)] is a scalar. Similarly, using Lemma 4, we have the
following (using ∣∣a∣2 + IlbIl2 ≥ 1 l∣a - b∣∣2). Here co := maxχ ∣∣f (x) - Ex，〜Paug(∙∣x) [f (x0)] ∣∣2:
EPaugIIdF (r- - r-)
(61)
≤	T2 {kf(x) - f(x0)k ( JtrVaug[f|x] + JtrVaug[f|X0] + 2co) +trVaug[f |x] +trVaug[f |x0]
Let MK := maxx ∣Kι (X)∣ so finally we have:
lEx,x0,aug [eKι(XI)(KI(X+) - KJ(X-))] |
≤ Ex,x0,aug [|eKI(XI)(KI(X+ ) - KIT(X-))|]
2M2
≤ J
2Eχ,χ0〜p(∙) kf(x) - f(x0)k √trVaug[f |x] + co
+ 3trEx [Vaug[f|X]](62)
+
—
Note that if there is no augmentation (i.e., paug(X1 |X) = δ(X1 - X)), then c0 = 0, Vaug[f|X] ≡ 0
and the error (Eqn. 62) is also zero. A small range of augmentation yields tight bound.
For LTri, the derivation is similar. The only difference is that We have 1∕τ rather than 1∕τ2 in
Eqn. 62. Note that this didn’t change the order of the bound since ξ(r) (and thus the covariance
operator) has one less 1∕τ as well. We could also see that for hard loss Ltri, since T → 0 this bound
will be very loose. We leave a more tight bound as the future work.
□
16
Under review as a conference paper at ICLR 2021
Remarks for H > 1. Note that for H > 1, Lnce has multiple negative pairs and ∂L∕∂rk- =
e-rrk-/r/z(X)where Z(X) ：= e-r+/T + PH=I e-rk-/T. While the nominator e-rk-片 still only
depends on the distance between x1 and xk- (which is good), the normalization constant Z(X)
depends on H + 1 distance pairs simultaneously. This leads to
∂L	e-kx-x0kk22/T
ξk = 口-----1	=------≡H-------μ-----下厂	(63)
∂rk- X0	1 +	kH=1 e-kx-x0kk22/T
which causes issues with the symmetry trick (Eqn. 54), because the denominator involves many
negative pairs at the same time.
However, if we think given one pair of distinct data point (x, x0), the normalized constant Z aver-
aged over data augmentation is approximately constant due to homogeneity of the dataset and data
augmentation, then Eqn. 54 can still be applied and similar conclusion follows.
C The dynamics of two-layer ReLU network and the interplays
of covariance operators between nearby layers (Section 4.2)
C.1 Theorem 5
Proof. For convenience, We define the centralized version of Uj(z0): Uj(zo) = Uj(zo)-
Ezo [uj (zo)] ∈ Rd and the matrices Ajk := Covz0[uj (zo), Uk (zo)] = Ez0 [U j(zo)U |(zo)] ∈ Rd×d.
Here both j and k run from 1 to n1 .
At layer l = 1 the covariance operator is Vz0 [R 1(z0)] = [w∣ jw2,kAjk] ∈ Rn1d×n1d.
On the other hand, if we check the second layer l = 2, we could compute K∣(zo) ∈ Rn1n2×n2.
Note that for input j of the second layer, we can compute its expectation with respect to z |z0 as
w1|,jUj(z0). On the other hand, since the last layer doesn’t have ReLU nonlinearity, the Jacobian
J∣ = In2×n2 which is independent of the input. So we have:
-w|,iUi(zo)-
K2(Z0)=	w1,∣u∣ (ZO)㊈ In2×n2 ∈ Rn1n2×n2	(64)
...
w1|,n1Un1 (z0)
So at layer l = 2 we can compute the covariance operator Vz0[K 2(z0)] = [w]j AjkWι,k 匠儿乂∕ ∈
Rn1n2×n1n2. Here [w1|,jAjkw1,k] ∈ Rn1×n1.
Using these two covariance operators, we are able to write down the weight update in SimCLR
setting with simple contrastive loss (here Qj := Pk Ajkw1,kw2|,k ∈ Rd×n2):
W ι,j = Qj w∣,j,	W 2,j = Qjwι,j	(65)
The dynamics of Eqn. 65 can be quite general and hard to solve. In the following, we talk about
some special cases.
Diagonal W2. We consider the case where W2 is a diagonal and square matrix, so n1 = n2 and
W2 = diag(w2,1, w2,2, . . . , w2,n1 ) and remains such a structure throughout the training. Note that
this also means there is no bias term for all output nodes.
In this case, we	could simplify	Eqn. 65	due	to the fact that now W2|,k(t)W2,j(t)	=	0	for	j	6=	k at
any time step	t	(again all biases	are zero	in the top-layer, otherwise the orthogonal condition do not
hold):
w1,j	=	w2,j Ajj w1,j	(66)
W 2,j	=	(w|,j Ajj w1,j )w∣,j	(67)
Note that if we multiply W1,j to Eqn. 66 and multiply w2,j to Eqn. 67, we arrive at:
w22,j (W1|,jAjjW1,j)	(68)
(W1|,jAjjW1,j)w22,j	(69)
1	dkw1,j k∣
2	dt
1	dw∣,j
2	dt
17
Under review as a conference paper at ICLR 2021
Therefore, dkw1,j k22/dt = dw22,j /dt and thus kw1,j k22 = w22,j + c with some time-independent
constant c.
□
D Hierarchical Latent Tree Models (Section 4.3)
D.1 Lemmas
Lemma 4 (Variance Squashing). Suppose afunction φ : R → R is L-Lipschitz continuous: ∣φ(x)一
φ(y)∣ ≤ L|x — y|, thenfor X 〜p(∙), we have:
Vp[φ(x)] ≤ L2Vp [x]	(70)
Proof. Suppose χ,y 〜p(∙) are independent samples and μφ := E [φ(χ)]. Note that V[φ(χ)] can be
written as the following:
E∣jφ(X)- φ(y)l2] = 1 E∣j(φ(X)- μφ) - (φ(y) - 〃0)|2]
=E □φ(X)- μΦl2] + E [lφ(y) - μΦl2] - 2E[(O(X)- μφ)(φ(y) - μφ)]
= 2Vp[φ(X)]	(71)
Therefore we have:
1	L2
vp[o(x)] = 2E [|。(X)- φ(y)l2] ≤ ɪE [|X - y|2] = L2Vp[x]	(72)
□
Lemma 5 (Sharpened Jensen’s inequality (Liao and Berg, 2018)). If function φ is twice differen-
tiable, and X 〜p(∙), then we have:
1 V[x] inf φ00 ≤ E [Φ(x)] - φ(E [x]) ≤ 1 V[x] SUp φ00	(73)
Lemma 6 (Sharpened Jensen’s inequality for ReLU activation). For ReLU activation ψ(X) :=
max(X, 0) and X 〜p(∙), we have:
0 ≤ E [Ψ(x)] - Ψ(E [x]) ≤ /VP[x]	(74)
Proof. Since ψ is a convex function, by Jensen’s inequality we have E [ψ(X)] - ψ(E [X]) ≥ 0. For
the other side, let μ := Ep [x] and We have (note that for ReLU, Ψ(x) - ψ(μ) ≤ |x - μ∣):
叫以叫-ψ(E [x]) = /(ψ(X)- @(〃))p(X)dX			(75)
≤ / ∣x — μ∣p(∕)dX			(76)
≤ U ∣X - μ∣2p(X)dX)(	Z p(X)d	1/2 X	(77)
yVP[X]	(78)
where the last inequality is due to Cauchy-Schwarz.	□
D.2 Motivation and description of a general HLTM
Here we describe a general Hierarchical Latent Tree Model (HLTM) of data, and the structure of a
multilayer neural network that learns from this data. The structure of the HLTM is motivated by the
hierarchical structure of our world in which objects may consist of parts, which in turn may consist
of subparts. Moreover the parts and subparts may be in different configurations in relation to each
18
Under review as a conference paper at ICLR 2021
Symbol	Definition	Size	Description
Zl Nl Nμ Nμh mμ	UV∈ch(μ) NV		The set of all latent variables at layer l of the generative model. The set of all neurons at layer l of the neural network. The set of neurons that corresponds to Zμ. The set of neurons that corresponds to children of latent Zμ. Number of possible categorical values taken by Zμ ∈ {0,..., mμ — 1}.
~oμ,τμ Pμν ρμν P0 vj(zμ) vj fμ, fNch μ v0k, V0,Nμh sk aμ	[P(zv lzμ)] 2P(Zν=1∣Zμ=l) - 1 diag[P(zo)] Ez [fj∣Zμ] [vj (Zμ )] [fj ] j∈Nμ , [fk]k∈Nμh [Ez [fk |z0]], [v0k]k∈NCh μ μ 2 (Vk ⑴-Vk (0)) [pμν(k)sk ]keN ch μ	mμ mμ × mV scalar in [-1, 1] m0 × m0 scalar mμ ∣Nμ∣,∣Nμhl mo, mo X ∣Nμh∣ scalar ∣Nμh∣	All-one and all-zero vectors. The top-down transition probability from Zμ to ZV. Polarity of the transitional probability in the binary case. The diagonal matrix of probability of zo taking different values. Expectation of activation fj given Zμ (zμ's descendants are marginalized). Vector form of Vj (zμ). Activations for all nodes j ∈ Nμ and for the children of Nμ Expected activation conditioned on zo Discrepancy of node k w.r.t its latent variable Zv(k). Child selectivity vector in the binary case.
Table 6: Extended notation in HLTM.
(b)
3
IOOQIJVL
∣OO ∙^∙O^O
Deep ReLU
networks
Ooo Ooo
■ Oo
(C)	E因…
I E[Λ∙∣⅞ = 1]
aμ = [Pμι∕(fc)5fc]fc∈ΛζSh
Pμ^ι Pμv2 PW3
QdΓ∙PP]埠
×
Figure 4: (a) The Hierarchical Latent Tree Model (HLTM). (b) Correspondence between latent
variables and nodes in the intermediate layer of neural networks. (c) Definition of Vj, Sj and a* in
Table. 6.
19
Under review as a conference paper at ICLR 2021
other in any given instantiation of the object, or any given subpart could be occluded in any given
view of an object.
The HLTM is a very simple toy model that represents a highly abstract mathematical version of this
much more realistic scenario. It consists of a tree structured generative model of data (see Fig. 2(d)
and Fig. 4). Simpler versions of this generative model have been used to mathematically model how
both infants and linear neural networks learn hierarchically structured data (Saxe et al., 2019). At
the top of the tree (i.e. level L), a single categorical latent variable z0 takes one of m0 possible
integer values values in {0 . . . , m0 - 1}, with a prior distribution P(z0). One can roughly think
of the value of z0 as denoting the identity of one of m0 possible objects. At level L - 1 there
is a set of latent variables Zl-i. This set is indexed by μ and each latent variable zμ is itself a
categorical variable that takes one of m* values in {0,..., m* - 1}. Roughly We can think of
each latent variable zμ as corresponding to a part, and the different values of zμ reflect different
configurations or occlusion states of that part. The conditional probability distributions P(zμ ∣zo),or
mo by mμ transition probability matrices, can roughly be thought of as collectively reflecting the
distribution over the presence or absence, as Well as configurational and occlusional states of each
part μ, conditioned on object identity z°. This process can continue onwards to describe subparts of
parts, etc...
D.3 A two-layer example to demonstrate notation
For simplicity, here we demonstrate the notation in a two-layer generative model, and two-layer
network with L = 2. Thus the top, middle, and leaf levels of the generative model are labelled
by l = 2,1,0, respectively, with corresponding latents zo, zμ and ZV, and the corresponding input,
hidden, and final layers of the neural network are labelled by l = 0, 1, 2 respectively.
As shown in Fig. 4, at the leaf level l = 0 there are a set of visible variables Z0 . This set is indexed
by ν and each visible variable zν is itself a categorical variable that takes one of mν values in the
set {0 . . . , mν - 1}. Roughly we can think of each zν as a pixel, or more generally, some visible
feature. For simplicity, we assume that each part Zμ at level 1 affects a distinct subset of pixels or
visible features zν. In essence, we assume each visible variable zν is a child ofa unique level 1 latent
variable Zμ in the generative tree process (Fig. 2(d)). In the rough analogy to objects and parts, in
this abstraction, each part μ controls the appearance of a subset of spatially localized nearby pixels
or visual features V that are all children of part μ. Conversely, each such local cluster of pixels or
feature values is influenced by the state of a single part. The conditional probability distribution
P(zν∣Zμ), or mμ by m” transition probability matrix, then describes the distribution over pixel or
visual feature values ZV of each child pixel V, conditioned on state Zμ of the parent part μ.
We next consider the two-layer ReLU network that learns from data generated from the two-layer
HLTM (right hand side of Fig. 2(d)). The neural network has a set of input neurons that are in one
to one correspondence with the pixels or visible variables ZV that arise at the leaves of the HLTM,
where l = 0. For any given object zo at layer l = 2, and its associated parts states Zμ at layer l = 1,
and visible feature values ZV at layer l = 0, the input neurons of the neural network receive only the
visible feature values ZV as real analog inputs. Thus the neural network does not have direct access
to the latent variables zo and Zμ that generate these visible variables.
Over-parameterization. While in the pixel level, there is a one to one correspondence between
the children V of a subpart μ and the pixel, in the hidden layer, more than one neuron could cor-
respond to Zμ (i.e., pool from pixels whose values are influenced by part μ), which is a form of
over-parameterization. We thus let Nμ denote the set of such hidden neurons, and we let Nl denote
the set of all neurons in layer l of the network. Thus Nμ is a subset of M. We further let Nch denote
the subset of neurons in layer 0 that provide input to the hidden layer neurons in Nμ. Thus Nch is a
subset of N0. Each neuron in the subset Nc is in one to one correspondence with the children (i.e.,
some Zν) of latent variable Zμ in the generative tree (see Fig. 2(d)).
In applying SSL in this setup, each object is specified by a set of values for zo (object identity),
Zμ (configurational and occlusional states of parts), and ZV (pixel values). Given any such object
and its realization of configurational and occlusional states of parts and the resulting pixel values,
we assume that the process of data augmentation corresponds to resampling Zμ and ZV from the
conditional distributions P(z*|z0) and P(z“|z*), while fixing object identity z°. This augmentation
process then roughly corresponds to being able to sample the same object under different parts
configurations and views.
20
Under review as a conference paper at ICLR 2021
The key question of interest that we wish to address is, under this generative model of data and model
of data augmentation, what do the hidden units of the neural network learn? In particular, can they
invert the generative model to convert pixels values at neural layer l = 0 into hidden representations
at neural layer l = 1 that reflect the existence of parts, with their associated states z*? More
precisely, can the network learn hidden units whose activation across all data points correlates well
with the values a latent variable Zμ takes across all data points? We address this question next, first
introducing further simplifying technical assumptions on the generative model and further notation.
D.4 A general structure of conditional distributions in the HLTM
For convenience, We define the following symbols for k ∈ Nj (note that ∣Nμh | = Nμh is the
number of the children of the node set N/
Vμk	：=	Ez [fk ∣Zμ] = Pμν(k)Vk ∈ Rmμ	(79)
Vμ,Nμh	：=	[vμk ]k∈Nμh	(80)
Vj	=	[Ez[f∙|z“ii = Vμ,NμhWj ∈ Rmμ	(81)
As an extension of binary symmetric HLTM, we make an assumption for the transitional probability:
Assumption 1. For μ ∈ Zi and V ∈ Z1-1, the transitional probability matrix Pμν ：= [P(zν∣Zμ)]
has decomposition PUV =4 1μ1∣ + Cμν where Cμν 1ν = 0μ and iμ Cμν = 0ν∙
Note that Cμν 1 = 0 is obvious due to the property of conditional probability. The real condition is
1μCμν = 0ν. If mμ = mν, then Pμν is a square matrix and Assumption 1 is equivalent to Pμν is
double-stochastic. Assumption 1 makes computation of Pμν easy for any zμ and ZV.
Lemma 7 (Transition Probability). If Assumption 1 holds, then for μ ∈ Zi, V ∈ Z1-1 and α ∈
Zl-2, we have:
Pμα = Pμν PVa = m- 1μ 1α + Cμν CVa
(82)
In general,for any μ ∈ Nli and α ∈ N^ With lι > l2, we have:
Pμα = --1μia +
ma
Cξζ
μ,…,ξ,ζ,…,a
(83)
Proof. Using Assumption 1, we have
Pμa = Pμν PVa
=(ɪ 1μ1l + CμV
mV
(84)
(85)
1V 1|a + CVa
since 1|1” = m”, CμV 1v = 0v and 1|C“a = 0a, the conclusion follows.
□
Remark. In the symmetric binary HLTM mentioned in the main text, all C*“ can be parameterized
as (here q ：= [-1, 1]|):
CL C"" S"v ) = 1 I-PVV -PVVi= 1 ρ"qql	(86)
This is because 1|C*“ = 02 and C*“I2 = 02 provides 4 linear constraints (1 redundant), leaving
1 free parameter, which is the polarity p*“ ∈ [-1,1] of latent variable ZV given its parent Zμ.
Moreover, since q|q = 2, the parameterization is close under multiplication:
C(PμV )C(PVa) = 4 qq∣qq∣ρμV pVa = $ qq∣ρμV pVa = C (ρμV pVa)	(87)
21
Under review as a conference paper at ICLR 2021
D.5 Theorem 6
Proof. First note that for each node k ∈ N(Ch:
V0k ：= [Ez [fk∣zo]] = XEz [fk∣Zν] P(zν∣zo) = P0νVk	(88)
zν
=	(---10 1| + COV) Vk	(89)
mν
=	----101| Vk + COV Vk	(90)
mν	ν
Note that 1O1V|Vk is a constant regarding to change of zO. So we could remove it when computing
covariance operator. On the other hand, for a categorical distribution
(P(zO = 0), u(0)),	(P(zO = 1), u(1)),	. . . ,	(P(zO = mO - 1), u(mO - 1))
With PO := diag[P(zO)], the mean is Ez0 [u] = 1|POu and its covariance can be written as (here
1 = 1O):
Vz0 [u] = (u - 11|POu)|PO(u - 11|POu) = u|(PO - PO11|PO)u	(91)
Note that each column of VO,N ch is VOk. Setting u = VOk and we have:
Vz0[Ez [fNμh 同]=VO1NCh (Po- Pj11lPo)¾Nμh	(92)
Note that Ez° [vok] = m^ 1∣Vk + Ez° [CovVk], since 1iPq1 = 1. With some computation, We could
see Covz0 [VOk, VOk0] = Covz0[COV(k)Vk, COV(k0)Vk0].
The equation above can be applied for any cardinality of latent variables. In the binary symmetric
case, We have (note here We define ρO := P(zO = 1) - P(zO = 0), and q := [-1, 1]|):
Po - Po11lPo = 1(1 - Po)qql	(93)
Note that in the binary symmetric case, according to remarks in Lemma 7, all Cμν = 11 Pμν qq| and
We could compute COVVk:
L	1	i Cov Vk = 2 poν qqlVk	=ρ0ν 2(Vk (I) - Vk (O))q = ρ0ν Sk q	(94)
where according to Eqn. 87, we have: ρoV	:=	ραβ	(95)
and the covariance between node k and k0	o,...,α,β,...,V can be computed as:	
Covz0 [Vok, Vok0]	=	Covz0 [CoV(k)Vk, CoV(k0)Vk0]	(96)
=	ρOν(k)ρOν(k0)sksk0 ɪqlqqlq(1 - IPI)	(97)
=	ρoV(k)ρoV(k0)sksk0 (1 - ρo2)	(98)
=	pOμPμν(k)Pμν(k0)sk SkO(I - P2)	(99)
The last equality is due to the fact that due to tree structure, the path from zO to all child nodes in
NCh must pass zμ.
Therefore We can compute the covariance operator:
Vzo[Ez [fNμh同]=ρ0μ(1 -ρ2)aμaμ	(100)
When L → +∞, We have:
ρOV :=	ραβ → 0	(101)
O,...,α,β,...,V
and thus the covariance becomes zero as well.	□
22
Under review as a conference paper at ICLR 2021
D.6 Theorem 7
Proof. According to our setting, for each node k ∈ Nμ, there exists a unique latent variable ZV With
ν = ν(k) that corresponds to it. In the following we omit its dependency on k for brevity.
Since We are dealing With binary case, We define the folloWing for convenience:
Vk+	:	= Vk (1)	(102)
Vk-	:	= Vk (0)	(103)
Vk	:二	=I(V+ + Vl) = 2 (Ez [fk ∖zν = 1]+ Ez [fk∖zν = 0])	(104)
vNCh	:二 μ	=[Vk]k∈NCh ∈ RIN力 μ	(105)
Sk :	=I(V+ - V-) = 2(Ez [fk ∖zν = 1] - Ez [fk∖zν = 0])	(106)
sNCh	: μ	=[sk]k∈NCh ∈ RlNμh 1 μ	(107)
We also define the sensitivity of node k to be λk := |(vk+)2 - (vk-)2|. Intuitively, a large λk means
that the node k is sensitive for changes of latent variable zν. If λk = 0, then the node k is invariant
to latent variable zν .
5 T C .	∙ 1	7	/»	1 ∙ .	.	. ∙	Jl	..1.	. F 1
We first consider pre-activation fj :=	k wjkfk and its expectation With respect to latent variable
z:
v+ := Ezh讣μ = 1b	V- := Ezh讣μ = θ]	(108)
Note that for each node k ∈ N ch We have:
μ
vμk = Vk + ρμν Sk, vμk = Vk - ρμν Sk	(109)
Let aμ := [ak]k∈NCh= [ρμνSk]k∈NCh and
μ μ
UNCh	:=	Vμ+NCh=	hE [fk lzμ	=	1]	i	= VNμh	+ aμ	(IIO)
uNμh	:=	Vμ,Nμh	:=	hE [fklzμ	=	0]	i	= VNμh	- aμ	(III)
Then we have v+ = WjuNCh and V- = WjUNCh.
∙j	μ μ	∙j	μ μ
Note that Wj is a random variable with each entry Wjk 〜 Uniform -σw J∣N3h^, σwj母h1].
It is easy to verify that E [wjk] = 0 and V[wjk] = σW/∖Nμch∖. Therefore, for two dimensional
vector Vj = [v+, V-]τ, we can compute its first and second order moments: Ew [Vj] = 0 and
22
Vw [Vj] = INwhTVμ,NμhV；NCh = INwhT [uNCh, UNCh]τ[uNCh, UNCh].
μ	μ	μ	μ	μ	μ	μ
Define the positive and negative set (note that ak := ρμνSk):
A+ =	{k	:	ak	≥ 0},	A-	=	{k	:	ak	< 0}	(112)
Without loss of generality, assume that Pk∈A a2k ≥ Pk∈A a2k. In the following, we show there
exists j with λj is greater than some positive threshold. Otherwise the proof is symmetric and we
can show λj is lower than some negative threshold.
When ∖Nμch ∖ is large, by Central Limit Theorem, V can be regarded as zero-mean 2D Gaussian
distribution and we have for some c > 0:
P
v+ ≥
√Nchl
kuNμh k
1 - erf(√C∕2)
2
(113)
Moreover, if al 6= 0, then the following probability is also not small :
∣∣uNCh k and V- < 0
(114)
23
Under review as a conference paper at ICLR 2021
Therefore, when ∣Nμ | = O(exp(c)), with high probability, there exists Wjso that
v+ = w|uNch ≥
μ
kuNμh ιι,
v- = WjuNch < 0
μ
Since VNCh ≥ 0 (all fk are after ReLU and non-negative), this leads to:
μ
v+ ≥ X∖ kuNμh k ≥ ⅛⅛ SsX^ ≥ σw S见扁 X PμνSk
ʌ/lNμh |	ʌ/lNμh| V k∈A+	μ μ k∈Nμh
By Jensen’s inequality, we have (note that ψ(x) := max(x, 0) is the ReLU activation):
v+ = Ez [fj lzμ = 1]= Ez hψ(Jj)lzμ = 1i
≥ ψ (Ezh讣μ = 1D = ψ(v+) ≥ σw SwHXPS
y μ k∈Nμh
(115)
(116)
(117)
(118)
On the other hand, we also want to compute V- := Ez [fj∣zμ = 0] using sharpened Jensen,s in-
equality (Lemma 6). For this we need to compute the conditional covariance Vz [fj ∣zμ]:
Vz [fj ∣Zμ] = X WjkVz [fk∣Zμ] ≤ 需 X Vz[fk∣Zμ]
k	μ | k
=|N Cw | X (EzV ∣zμ [V[fk lzV]]+ VzV ∣zμ [Ez [fklzV]D
≤ 3σW 卜2 + |Nch | X VzV lzμ [Ez [fk lzV]])
(119)
(120)
(121)
Note that 2 is due to conditional independence: fk as the computed activation, only depends on
latent variable zν and its descendants. Given zμ , all zν and their respective descendants are inde-
pendent of each other and so does fk. 3 is due to the fact that each wjk are sampled from uniform
distribution and |wjk| ≤ σw J∣N⅛hψ.
Here VzV ∣ zμ [Ez [fk | zν]] = sk(1 - PJJV) Can be computed analytically. It is the variance of a binomial
distribution: with probability 1 (1 + ρμν) we get v[ otherwise get v-. Therefore, we finally have:
Vzfjlzμ] ≤ 3σW 卜 2 + ∣NCh∣ X Sk(1 -
(122)
As a side note, using Lemma 4, since ReLU function ψ has Lipschitz constant ≤ 1 (empirically it is
smaller), we know that:
Vz[fj∣zμ] ≤ 3σw 卜2 + ∣NCh∣ X sk(1 -
(123)
Finally using Lemma 6 and V- < 0, We have:
Vj-
Ez [fj lzμ = 0] = Ez hψ(fj )lzμ = 0i
ψ (Ez [讣μ = O]) +，露国际=0]
qVzfjM= 0]
σw y3σ2+∣NChi X sk(1 - ρμν)
(124)
(125)
(126)
(127)
≤
≤
24
Under review as a conference paper at ICLR 2021
Combining Eqn. 118 and Eqn. 127, we have a bound for λj :
λj =	(V+ )2 -	(v-)2	≥	3σW	" ∣N∣h∣	X sk (C-6^^ρμν -	1)	- σl	(128)
□
E The Analysis of BYOL in Sec. 5
E.1 Derivation of BYOL gradient
Note that for BYOL, we have:
∂r
Vec (∂W∣
Kl(x1;W) [Kl|(x1;W)vec(Wl) -Kl|(x2;W0)vec(Wl0)]
(129)
under large batchsize, we have (note that we omit W for any term that depends on W, but make
dependence of W0 explicit in the math expression):
Vec (∂W = = Ex 〜P(∙) [Ex，〜Paug(∙∣x) [Kl (x')KI(X0)] Vec(WI ) - KI(X)K|(x； WO)Vec(W0)]
For brevity, We write Ex [∙] := Ex〜p(,)[∙] and Ex，[∙] := Ex，〜paug(∙∣x) [∙]. Similar for V. And the
equation above can be written as:
vec (黑)=Ex {Vxo [Kι(χ0)]} vec(Wι)	(130)
∂Wl
+ Ex {Kι(x) [K∣(x)vec(Wι) - K∣(x; W0)vec(W∕)]}	(131)
In terms of weight update by gradient descent, since ∆Wl = - ∂∂r-, We have:
vec(∆Wι) = -Ex {Vxo [Kι(χ0)]} vec(Wι)	(132)
-Ex {Kι(x)[K|(x)vec(Wι) - KI(x; W0)vec(W∕)]}	(133)
If we consider the special case W = W0, then the last two terms cancelled out, yielding:
vec(∆Wι)sym = -Ex {Vx0 [Kι(x0)]} vec(Wι)	(134)
And the general update (Eqn. 136) can be written as:
vec (∆Wι) = vec(∆Wι)sym	(135)
-Ex {Kι(x)[Kιl(x)vec(Wι) - Kl(x; W0)vec(W∕)]}	(136)
E.2 Theorem 8
g∂r
Proof. When BN is present, Eqn. 129 needs to be corrected with an additional term, ∂∂w :
δWιBN, where δWιBN is defined as follows:
1
δWιBN := B X Digιfi-1
∣B ∣ i∈B
∂r
∂W
(137)
From the proof of Theorem 1 (see Eqn. 26), we know that for each sample i ∈ B (note that by
definition, the back-propagated gradient after nonlinearity g； equals to Dqigi where g； is the back-
propagated gradient before nonlinearity):
Dιigιi = JιiI[JιiWιfιi-1 - Jιi(W0)Wι0fιi-1(W0)]	(138)
Since the network is linear from layer l to the topmost layer L, we have Di = Dι. Since the only
input dependent part in Jιi is the gating function between the current layer l and the topmost layer
L, for linear network the gating is always 1 and thus Jι = Jl and is independent of input data. We
25
Under review as a conference paper at ICLR 2021
now have (note that we omit W for any terms that are dependent on W, but will write W0 explicitly
for terms that are depend on W0):
δWlBN
IBl X Diglfi-I = -Dl glfl-1
|B| i∈B
J：[JlWlfl-I- Jl(W0)Wl0fl-ι(W0)]fl-ι
Therefore we have:
vec(δWlBN) = (fl-1 0 JlT) [(fl-ι 0 JlT)Vec(Wl)- (fl-ι(W0) 0 JlT(W0))Vec(Wl')]
Note that by assumption, since Jl doesn't depend on the input data, We have
fl-1 0 JlT = EB [fl-ι] 0 JlT = EB [fl-1 0 Jl|]
(139)
(140)
(141)
(142)
Taking large batchsize limits and notice that the batch B could contain any augmented data generated
from independent samples fromp(∙), we have:
Vec(δWlBN)	=	Ex,x0	[Kl (x0)] Ex,x0	[KlT(x0)]Vec(Wl)	(143)
-	Ex,x0	[Kl (x0)] Ex,x0	[KlT(x0;W0)]Vec(Wl0)	(144)
An important thing is that the expectation is taking over X 〜P(X) and x0 〜PaUg(∙∣x). Intuitively,
this is because fl-ι and gl are averages over the entire batch, which has both intra-sample and
inter-sample variation.
With augment-mean connection Kl(x) we could write:
vec(δWlBN)	=	Ex	[Kl(x)]	Ex	[KlT(x)]	Vec(Wl)- Eχ	[Kl(x)]	Eχ	[KlT(x; W0)] Vec(Wl0)
=ExiKl (x)] {Eχ [KlT(x)] vec(Wl) - Eχ [KlT(x; W0)] vec(Wl0)}	(145)
Plug in δWl,BN into Eqn. 136 and we have corrected gradient for BYOL:
vec(∂Wl)	= vec (∂Wl) -vec(δWlBN)	(146)
=Ex [Vχ0〜Paug(∙∣χ) [Kl(x0)]] vec(Wl)+ Vx [Kl(x)] vec(Wl)	(147)
-Covx [Kl(x),Kl(x; W0)] vec(Wl0)	(148)
And the weight update ∆]Wl = ∆Wl + δWlBN is:
vec (∆Wl)	= -Ex [Vx，〜paug(∙∣x) [Kl(x0)]] vec(Wl) - Vx [Kl(x)] vec(Wl)	(149)
+ Covx [Kl(x),Kl(x; W0)] vec(Wl0)	(150)
Using Eqn. 134, we have:
vec ∆]Wl) = vec(∆Wl) + vec(δWlBN)	(151)
= vec(∆Wl)sym	(152)
-Vx [Kl(x)] vec(Wl) + Covx [Kl(x), Kl(x; W0)] vec(Wl0)	(153)
□
E.3 Corollary 1
Proof. In this case, both the target and online networks use the same weight and there is no predictor.
This means W0 = W. Therefore, in Eqn. 145, all Wl = Wl0 and δWlBN = 0.
Note that for SimCLR, the loss function contains both positive pair squared distance r+ and negative
pair squared distance r-. The argument above shows that δWlBN = 0 for positive pair distance r+.
For negative pair distance r-, with the same logic in Theorem. 8, we will see δWlBN takes the same
form as Eqn. 145 and thus is zero as well.	□
Remarks. Note that BatchNorm does not matter in terms of gradient update, modulo its benefit
during optimization. This is justified in the recent blogpost (Fetterman and Albrecht, 2020).
26
Under review as a conference paper at ICLR 2021
E.4 Corollary 2
Proof. By our condition, we consider the case that the extra predictor is a linear layer: Wpred =
{Wpred}. Note that Wpred ∈ RnL ×nL is a squared matrix, otherwise we cannot compute the loss
function between the output fL0 from the online network with the output fL from the target network.
In this case, for connection Kl(x) in the common part of the network (in Wbase), we have:
Kl(X) = fl-1(x) ® JI(X) = fl-1(x) ® JIbase(X)Wpred	(154)
=(fl-1(x) 乳 JIbase(X))WIred	(155)
Here Jl,base(X) is the Jacobian from the current layer l to the layer right before the extra predictor.
The last equality is due to the fact that fl-ι is a vector. Therefore, for augment-mean Kl (x), since
Wpred doesn’t depend on the input data distribution, we have:
Kl (X) = Kl,base(X)WpIred	(156)
where Kl,base(x) ：= Kl(x; Wbase). To make things concise, let Kl(x) := Kl(x) - Ex [Kl(x)].
Obviously We have Kl(x) = Kl,base(X)Wpred. And the covariance operator becomes:
Vx[Kl(X;W )] = ExhK l,base(X)WpredWPredKl|base(X)i	(157)
Now let ∆Wl be the last two terms in Eqn. 14:
vec([) = -Vx [Kl(X; W)] Vec(Wl) + Covx [Kl(x; W), Kl(x; W0)] Vec(Wl0)	(158)
Since there is no EMA, Wbase = Wb0 ase and we have:
vec([) = {-Vx[Kl(χ; W)] +Covx [Kl(x; W ),Kl(x; W0)]}vec(Wl)	(159)
=Ex [Kl,base(x)WPred(I - WPred)K∣base(x)] vec(Wl)	(160)
Therefore, the final expression ofvec(∆Wl) is the following:
vec(∆]Wl) = vec(∆Wl) + vec(δWlBN)
={-Ex [Vx，〜paug(∙∣x) [Kl (X0)]] + Ex [Kl,base(x)WPred(I - WPred)K，'^⑺]} vec(Wl )
If there is no stop gradient on the target network side, and we receive gradient from both the on-
--------------------------------------------------------------------------------
line and the target network, then for any common layer l, the weight update vec(∆Wl ) becomes
symmetric (note that this can be derived by swapping W0 with W and add the two terms together):
--*~`—■
vec(∆]Wl) = 2vec(∆Wl)sym	(161)
-(Vx[Kl(x; W)] + Vx [Kl(x; W0)]) vec(Wl)	(162)
+ Covx [Kl(x; W),Kl(x; W0)] vec(Wl)	(163)
+ Covx [Kl(x; W0),Kl(x; W)] vec(Wl)	(164)
which gives:
vec(] ) = 2vec(∆Wl)sym - Ex [Kl,base(x)(I - Wpred)1 (I - WPred) KlIbase(x)] Vec(Wl )
=2vec(∆Wl)sym - Vx [Kl,base(x)(I - Wpred)1] Vec(Wl)	(165)
□
27
Under review as a conference paper at ICLR 2021
E.5 Theorem 9
Proof. Consider the following discrete dynamics of a weight vector w(t):
w(t+ 1) - w(t) = α [-w(t) + (1 - λ)wema(t)]	(166)
where α is the learning rate, wema(t + 1) = γemawema(t) + (1 - γema)w(t) is the exponential
moving average of w(t). For convenience, we use η := 1 - γema.
Since it is a recurrence equation, we apply z-transform on the temporal domain, where w(z) :=
Z[w(t)] = Pt+=∞0 w(t)z-t. This leads to:
Z(W(Z) — W(0)) = W(Z) — α(W(Z) — Z[Wema(t)](1 — λ))
Note that for Wema(t) we have:
(167)
z(wema(z) - wema(0)) = (1 - η)wema(z) + ηw(z)	(168)
IfWe set Wema(0) = 0, i.e., the target netWork is all zero at the beginning, then it gives Wema(Z) =
z-7+η w(z). Plugging it back to Eqn. 167 and we have:
Z(W(Z) — w(0)) = W(Z) — αw(z)(1---^— (1 — λ)
(169)
And then we could solve w(z):
w(z)
z(z - 1 + η)
(z - 1)2 + (η + α)(z - 1) + αηλ
w(0)
(170)
Note that the denominator has two roots z1 and z2 :
Z1,2 = 1 - 2 (η + α ± P(η + ɑ)2 - 4αηλ)
(171)
and w(z) can be written as
W(Z) = (ZZ-ZZ-)1W(O)
(172)
Without loss of generality, let z1 < z2. The larger root z2 > 1 when λ < 0, so the zero (z = 1 -η
γema) in the nominator won’t cancel out the pole at z2. And we have:
z
(z - z1)(z - z2)
Z (Z - ZI) - (Z - Z2)
Z2 - Zl (z - Zl)(Z - Z2)
(173)
z2 - z1	z - z2 z - z1
11	1
(174)
z2 - z1	1 - z2z-1	1 - z1z-1
(175)
where 1/(1 - z2z-1) corresponds to a power series z2t in the temporal domain. Therefore, we could
see w(t) has exponential growth due to z2 > 1.
Now let us check how z2 changes over η, i.e., how the parameter γema := 1 - η of EMA affects the
learning process. We have:
η+α	4αηλ
z2	+p(M1 + EA -1
(176)
Use the fact that (1 + x)1/2 ≤ 1 + 2X for X ≥ 0, We have:
η + α 4αηλ
z2 - 1 ≤  4— 2 ：----T2^ 二
4	(η + α)2
λ
ɪrɪ
αη
(177)
Compared to no EMA case (i.e., γema = 0 orη = 1), with a γema < 1 but close to 1 (or equivalently,
η is close to 0), the upper bound of Z2 becomes smaller but still greater than 1, and the exponential
growth is less aggressive, which stabilizes the training. Note that if Yema = 1 (or η = 0), then
Wema(t) ≡ Wema(O) = 0 and learning also doesn,t happen.	□
z
1
1
—
—
28
Under review as a conference paper at ICLR 2021
F Exact solutions to BYOL with linear architectures without
BatchNorm
An interesting property of BYOL is that it finds useful non-collapsed solutions for the online net-
work and target network, despite the fact that it does not employ contrastive terms to separate the
representations of negative pairs. While BatchNorm can implicitly introduce contrastive terms in
BYOL, as discussed in the main paper, recent work (Richemond et al., 2020b) has shown that other
normalization methods which do not introduce contrastive terms, nevertheless enable BYOL to work
well. We therefore analyze BYOL in a simple linear setting to obtain insight into why it does not
lead to collapsed solutions, even without BatchNorm. We first derive exact fixed point solutions to
BYOL learning dynamics in this setting, and discuss their stability. We then discuss specific models
for data distributions and augmentation procedures, and show how the fixed point solutions of BYOL
learning dynamics depend on both data and augmentation distributions. We then discuss how our
theory reveals a fundamental role for the predictor in avoiding collapse in BYOL solutions. Finally,
we derive a highly reduced three dimensional description of BYOL learning dynamics that pro-
vide considerable insights into dynamical mechanisms enabling BYOL to avoid collapsed solutions
without negative pairs to force apart representations of different objects.
F.1 The fixed point structure of BYOL learning dynamics.
We consider a single linear layer online network with weights W1 ∈ Rn1 ×n0 and a single layer
target network with weights Θ ∈ Rn1 ×n0 . Additionally, the online network has a predictor layer
with weights W2 ∈ Rn1 ×n1 , that maps the output of the online network to the output space of
the target network. BYOL only uses positive pairs in which a single data point x is drawn from
the data distribution p(∙), and then two augmented views xι and x2 are drawn from a conditional
augmentation distribution Paug(∙∣x). The loss function driving the learning dynamics of the online
weights W1 and predictor weights W2 given a single positive pair {x1, x2} and a given target
network Θ is then given by
L= kW2W1x1 -Θx2k22.	(178)
In contrast, the dynamics of the target network weights Θ follows that of the online weights W1
through an exponential moving average. In the limit of large batch sizes and slow learning rates, the
combined learning dynamics is then well approximated by the continuous time ordinary differential
equations (see e.g. Saxe et al. (2014) for analogous equations in the setting of supervised learning):
where
and
To dW = [Θ∑d - W2W1∑s] WT	(179)
Tp dW = WT [Θ∑d - W2W1∑s]	(180)
dΘ τt~7Γ = -θ + wι, dt	(181)
Σs ≡ Ex1 [x1x1T]	(182)
Σd ≡ Eχ1,x2 [xιxT] = Ex〜p(∙) [K(x)K(x)τ],	(183)
KK(X) ≡ ExI 〜paug(∙∣x) [xi] .	(184)
Here Σs is the correlation matrix ofa single augmented view x1 of the data x, while Σd is the corre-
lation matrix between two augmented views of the same data point, or equivalently, the correlation
matrix of the augmentation averaged vector KK(x). Additionally, we have retained the possibility
of having three different learning rates for the online, predictor, and target networks, represented by
the time constants τo, τp, and τt respectively.
Because of the linearity of the networks, the final outcome of learning depends on the data and
augmentation procedures only through the two correlation matrices Σs and Σd. Examining equa-
tion 179-equation 181, we find sufficient conditions for a fixed point given by W2W1Σs = ΘΣd
and W1 = Θ. Inserting the second equation into the first and right multiplying both sides by [Σs]-1
29
Under review as a conference paper at ICLR 2021
(assuming Σs is invertible), yields a manifold of fixed point solutions in W1 and W2 satisfying the
nonlinear equation
W2W1 = W1Σd[Σs]-1.	(185)
This constitutes a set of n1 × n2 nonlinear equations in (n1 × n2) + (n2 × n2) unknowns, yielding
generically a nonlinear manifold of solutions in W1 and W2 of dimensionality n2 × n2 corresponding
to the number of predictor parameters. For concreteness, we will assume that n2 ≤ n1, so that the
online and target networks perform dimensionality reduction. Then a special class of solutions to
equation 185 can be obtained by assuming the n2 rows of W1 correspond to n2 left-eigenvectors
of Σd [Σs]-1 and W2 is a diagonal matrix with the corresponding eigenvalues. This special class of
solutions can then be generalized by a transformation W2 → SW2 S-1 and W1 → SW1 where S
is any invertible n2 by n2 matrix. Indeed this transformation is a symmetry of equation 185, which
defines the solution manifold.
In addition to these families of solutions, the collapsed solution W1 = W2 = Θ = 0 also exists,
and a natural question is, why doesn’t BYOL generically converge to this collapsed solution? This
question can be addressed by analyzing the stability of both the collapsed solution and the families
of solutions presented above. The basic calculation involves computing the Jacobian of the vector
field defining the dynamics of equation 179 through equation 181. A fixed point solution is stable
if and only if all eigenvalues of the Jacobian evaluated at a fixed point solution are negative. Using
methods similar to that of Baldi and Hornik (1989), which carried out a similar stability analysis
for learning dynamics in two weight layer linear networks in the supervised setting, it is possible to
show that all of the above fixed point solutions are unstable except for those derived from the special
solutions where the n2 rows of W1 correspond to the top n2 principal eigenmodes of Σd[Σs]-1.
Thus this analysis sketch provides conceptual insights into why BYOL, at least in this simple setting,
learns nontrivial, and potentially useful representations with only positive examples, and does not
converge to the naive collapsed solution. Basically, the collapsed solution, as well as other subdomi-
nant solutions, are unstable, while solutions corresponding to the principal eigenmodes of Σd[Σs]-1
are stable. Thus, from generic initial conditions, one would expect that the row space of the online
network would converge to the span of the top n2 principal eigenmodes of Σd[Σs]-1.
F.2 Illustrative models for data and data augmentation
While the above section suggests that BYOL converges to the top eigenmodes of Σd[Σs]-1, here
we make this result more concrete by giving illustrative examples of data distributions and data
augmentation procedures, and the resulting properties of Σd[Σs]-1.
Multiplicative scrambling. Consider for example a multiplicative subspace scrambling model,
used in the illustration of SIMCLR in Sec. 4.1. In this model, data augmentation scrambles a
subspace by multiplying by a random Gaussian matrix, while identically preserving the orthogonal
complement of the subspace. In applications, the scrambled subspace could correspond to a space of
nuisance features, while the preserved subspace could correspond to semantically important features.
More precisely, we consider a random scrambling operator A which only scrambles data vectors
x within a fixed k dimensional subspace spanned by the orthonormal columns of the n0 × k ma-
trix U. Within this subspace, data vectors are scrambled by a random Gaussian k × k matrix B.
Thus A takes the form A = Pc + UBUT where Pc = I - UUT is a projection operator onto
the n0 - k dimensional conserved, semantically important, subspace orthogonal to the span of the
columns of U, and the elements of B are i.i.d. zero mean unit variance Gaussian random variables
so that E [BijBki] = δikδji. Under this simple model, the augmentation average K(x) in equa-
tion 184 becomes K(X) = Pcx. Thus, intuitively, under multiplicative subspace scrambling, the
only aspect of a data vector that survives averaging over augmentations is the projection of this
data vector onto the preserved subspace. Then the correlation matrix of two different augmented
views is Σd = PcΣxPc while the correlation matrix of two identical views is Σs = Σx where
Σx ≡ Ex〜p(∙) [xxτ] is the correlation matrix of the data distribution. Thus BYOL learns the prin-
cipal eigenmodes of Σd[Σs]-1 = PcΣxPc[Σx]-1. In the special case in which Pc commutes with
Σx, we have the simple result that Σd [Σs]-1 = Pc, which is completely independent of the data
correlation matrix Σx . Thus in this simple setting BYOL learns the subspace of features that are
identically conserved under data augmentation, independent of how much data variance there is in
the different dimensions of this conserved subspace.
30
Under review as a conference paper at ICLR 2021
It is interesting to compare to SimCLR in the same setting, which learns the principal eigenmodes
of PcΣxPc as described in Sec. 4.1. Thus SimCLR also projects to the conserved subspace, but is
further influenced by the correlation matrix of the data within this subspace. In actual applications,
which performs better will depend on whether or not features of high variance within the conserved
subspace are important for downstream tasks; SimCLR (BYOL) should perform better if conserved
features of high variance are (are not) important.
Additive scrambling. We also consider, as an illustrative example, data augmentation procedures
which simply add Gaussian noise with a prescribed noise covariance matrix Σn . Under this model,
we have Σs = Σx + Σn while Σd = Σx . Thus in this setting, BYOL learns principal eigenmodes of
Σd[Σs]-1 = Σx[Σx + Σn]-1. Thus intuitively, dimensions with larger noise variance are attenuated
in learned BYOL representations. On the otherhand, correlations in the data that are not attenuated
by noise are preferentially learned, but the degree to which they are learned is not strongly influenced
by the magnitude of the data correlation (i.e. consider dimensions that lie along small eigenvalues
of Σn).
F.3 The importance of the predictor in BYOL.
Here we note that our theory explains why the predictor plays a crucial role in BYOL learning in
this simple setting, as is observed empirically in more complex settings. To see this, we can model
the removal of the predictor by simply setting W2 = I in all the above equations. The fixed point
solutions then obey W1 = W1Σd[Σs]-1. This will only have nontrivial, non-collapsed solutions
if Σd [Σs]-1 has eigenvectors with eigenvalue 1. Rows of W1 consisting of linear combinations of
these eigenvectors will then constitute solutions.
This constraint of eigenvalue 1 yields a much more restrictive condition on data distributions and
augmentation procedures for BYOL to have non-collapsed solutions. It can however be satisfied
in multiplicative scrambling if an eigenvector of the data matrix Σx lies in the column space of
the projection operator Pc (in which case it is an eigenvector of eigenvalue 1 of Σd[Σs]-1 =
PcΣxPc[Σx]-1. This condition cannot however be generically satisfied for additive scrambling
case, in which generically all the eigenvalues of Σd[Σs]-1 = Σx [Σx + Σn]-1 are less than 1. In
this case, without a predictor, it can be checked that the collapsed solution W1 = Θ = 0 is stable.
In contrast, with a predictor, the collapsed solution can be checked to be unstable, and therefore it
will not be found from generic initial conditions.
Thus overall, in this simple setting, our theory provides conceptual insight into how the introduc-
tion of a predictor is crucial for creating new non-collapsed solutions for BYOL, whose existence
destabilizes the collapsed solutions.
F.4 Reduction of BYOL learning dynamics to low dimensions
The full learning dynamics in equation 179 to equation 181 constitutes a set of high dimensional
nonlinear ODEs which are difficult to solve from arbitrary initial conditions. However, there is a
special class of decoupled initial conditions which permits additional insight. Consider the special
case in which Σs and Σd commute, and so are simultaneously diagonalizable and share a common
set of eigenvectors, which we denote by uα ∈ Rn0. Consider also a special set of initial conditions
where each row of W1 and the corresponding row of Θ are both proportional to one of the eigen-
modes uα , with scalar proportionality constants w1α and θα respectively, and W2 is diagonal, with
the corresponding diagonal element given by w2α . Then it is straightforward to see that under the
dynamics in equation 179 to equation 181, that the structure of this initial condition will remain the
same, with only the scalars w1α , θα and w2α changing over time. Moreover, the scalars decouple
across the different indices α, and the dynamics are driven by the eigenvalues λsα and λdα of Σs and
Σd respectively. Inserting this special class of initial conditions into the dynamics in equation 179
to equation 181, and dropping the α index, we find the dynamics of the triplet of scalars is given by
dw2 τo Y	[θλd - w2w1λs] w1	(186)
dw1 Tp T	w2 [θλd - w2w1λs]	(187)
dθ τt dt =	-θ + w1 .	(188)
31
Under review as a conference paper at ICLR 2021
Tied predictor
No predictor
Fixed target
Figure 5: A visualization of BYOL dynamics in low dimensions. Left: Black arrows denote the vector field
of the flow in the w1 and w2 of plane online and predictor weights in Eqns. 186 and 187 when the target
network weight θ is fixed to 1. For all 3 panels, λs = 1, λd = 1/2, and τo = τp = τt = 1, and all
vectors are normalized to unit length to indicate direction of flow alone. The red curve shows the hyperoblic
manifold of stable fixed points w2w1 = θλdλs-1, while the red point at the origin is an unstable fixed point.
For a fixed target network, the online and predictor weights will cooperatively amplify each other to escape the
collapsed solution at the origin. Middle: A visualization of the full low dimensional BYOL dynamics in Eqns
186-188 when the online and predictor weights are tied so that w1 = w2 = w. The green curve shows the
nullcline θ = W corresponding to dθ = 0 and the blue curve shows part of the nullcline 端 =0 corresponding
to w2 = θλdλs-1. The intersection of these two nullclines yields two fixed points (red dots): an unstable
collapsed solution at the origin w = θ = 0, and a stable nontrivial solution with θ = w and w = λd λs-1 .
Right: A visualization of dynamics in Eqns 186-188 when the the predictor is removed, so that w2 is fixed to 1.
The resulting two dimensional flow field on w = w1 and θ is shown (black arrows). The green curve shows the
nullcline W = θ corresponding to d = 0, while the blue curve shows the nullcline W = θλd λ-1. The slope
of this nullcline is λsλd-1 > 1. The resulting nullcline structure yields a single fixed point at the origin which
is stable. Thus there only exists a collapsed solution. In the special case where λsλd-1 = 1, the two nullclines
coincide, yielding a one dimensional manifold of solutions.
Alternatively, this low dimensional dynamics can be obtained from equation 179 to equation 181
not only by considering a special class of decoupled initial conditions, but also by considering the
special case where every matrix is simply a 1 by 1 matrix, making the scalar replacements W1 → w1,
W2 → w2, Θ → θ, Σs → λs, and Σd → λd.
The fixed point conditions of this dynamics are given by θ = w1 and w2w1 = θλdλs-1. Thus the
collapsed point w1 = w2 = θ = 0 is a solution. Additionally w2 = λdλs-1 and w1 = θ taking
any value is also a family of non-collapsed solutions. We can understand the three dimensional
dynamics intuitively as follows when τt τo and τo = τp . In this case, the target network evolves
very slowly compared to the online network, as is done in practice, and for simplicity we use the
same learning rate for the predictor as we do for the online network. In this situation, we can treat θ
as approximately constant on the fast time scale of τo on which the online and predictor weights w1
and w2 evolve. Then the joint dynamics in equation 186 and equation 187 obeys gradient descent
on the error function
E = λ (θλdλ-1 — W2Wι)2.	(189)
Iso-contours of constant error are hyperbolas in the w1 by w2 plane, and for fixed θ, the origin w1 =
w2 = 0 is a saddle point, yielding an unstable fixed point (see Fig. 5 (left)). From generic initial
conditions, w1 and w2 will then cooperatively amplify each other to rapidly escape the collapsed
solution at the origin, and approach the zero error hyperbolic contour w2w1 = θλdλs-1 where θ is
close to its initial value. Then the slower target network θ will adjust, slowly moving this contour
until θ = w1 . The more rapid dynamics of w1 and w2 will hug the moving contour w2w1 = θλdλs-1
as θ slowly adjusts. In this fashion, the joint fast dynamics of w1 and w2, combined with the slow
dynamics of θ, lead to a nonzero fixed point for all 3 values, despite the existence of a collapsed
fixed point at the origin. Moreover, the larger the ratio λdλs-1, which is determined by the data, the
larger the final values of both w1 and w2 will tend to be.
We can obtain further insight by noting that the submanifold w1 = w2 , in which the online and
predictor weights are tied, constitutes an invariant submanifold of the dynamics in Eqns. 186 to
188; if w1 = w2 at any instant of time, then this condition holds for all future time. Therefore we
32
Under review as a conference paper at ICLR 2021
Figure 6: The Hierarchical Latent Tree ModeI(HLTM) used in our experiments (Sec. G.2 and Sec. 6).
can both analyze and visualize the dynamics on this two dimensional invariant submanifold, with
coordinates W = wι = w2 and θ (Fig. 5 (middle)). This analysis clearly shows an unstable collapsed
solution at the origin, with W = θ = 0, and a stable non-collapsed solution at W = θ = λdλ-1.
We note again, that the generic existence of these non-collapsed solutions in Fig. 5 depends critically
on the presence of a predictor with adjustable weights W2. Removing the predictor corresponds to
forcing W2 = 1, and non-collapsed solutions cannot exist unless λd = λs , as demonstrated in Fig. 5
(right). Thus, remarkably, in BYOL in this simple setting, the introduction of a predictor network
plays a crucial role, even though it neither adds to the expressive capacity of the online network,
nor improves its ability to match the target network. Instead, it plays a crucial role by dramatically
modifying the learning dynamics (compare e.g. Fig 5 middle and right panels), thereby enabling
convergence to noncollapsed solutions through a dynamical mechanism whereby the online and
predictor network cooperatively amplify each others’ weights to escape collapsed solutions ( Fig. 5
(left)).
Overall, this analysis of BYOL learning dynamics provides considerable insight into the dynami-
cal mechanisms enabling BYOL to avoid collapsed solutions, without negative pairs to force apart
representations, in what is likely to be the simplest nontrivial setting.
G	Additional Experiments
G. 1 Experiments on Two -layer network
We conduct experiments to verify our theoretical reasoning in Sec. 4.2. We follow the setting in
Theorem 5, i.e., two-layer ReLU network (L = 2) that contains the same number of hidden nodes
as the number of output nodes (n1 = n2). The top-layer weight W2 is a diagonal matrix with no
bias (note that “no bias” is important here, otherwise we won’t have w2|,j w2,k = 0 for j 6= k).
We use Lτnce (τ = 0.05 and H = 1) and Lsimp and use SGD optimizer with learning rate of 0.01.
All experiments run with 5000 minibatches with batchsize 128. We test cases with and without `2
normalization at the output layer. For each setting, i.e., (loss, normalization), we run 30 random
seeds. The data are generated by a mixture of 10 Gaussian distributions (with uniform prior on each
mixture), with mean μk 〜 N(0, I) and a covariance matrix ∑k = 0.1I. We set the first cluster to
be zero-mean.
Without `2 normalization. Fig. 7 shows the weight growth of the top (W2) and bottom (W1) layer.
As predicted by Theorem 5, the weight quickly grows to infinity. Note that the y-axis is log scale
so exponential growth is shown as linear. From the figure, it is clear that with Lsimp, their growth is
super exponential due to the inter-plays between the top and the bottom layers. On the other hand,
with Lnce, the growth slows down due to the fact that its associated covariance operator has a weight
which decays exponentially when the representations of two distinct samples become far apart.
With `2 normalization. With the normalization, the weights will not grow substantially and we
focus ourselves more on the meaning of each intermediate nodes after training. From the theoretical
reasoning in Sec. 4.2, in the ReLU case, we should see each node gradually moves towards (or
specializes to) one cluster after training. Fig. 8 shows that a node j that is only active for 1 or 2
cluster centers (out of 10) have much higher |W2,j | than some other node that is active on many
cluster centers. This shows that those “specialized” nodes has undergone learning and their fan-out
weight magnitude |W2,j | becomes (or remains) high, according to the dynamics in Theorem 5.
33
Under review as a conference paper at ICLR 2021
Number of minibatches
WithoutZ2 normalization, the growth of ∣l∕Vι∣
Number of minibatches
IOOO 2000	3000	4000	5000
Number of minibatches
Figure 7: Top row: Without `2 normalization, training with SimCLR and Lsimp leads to fast growth
of the weight magnitude over time (each curve is one training curve out of 30 trials with different
random seeds). Furthermore, this growth is super exponential due to the interplay between top
and bottom layers, as suggested by the dynamics in Eqn. 8. Note that the y-axis is in log scale.
Bottom row: Without `2 normalization, Lnce has more stable weight magnitude over training since
its covariance operator is weighted (See Theorem 4).
0.4-
0.2-
0.0-
-0.2-
-0.4 -
-0.6-
-0.8-
Without ∕2 normalization, the growth of ∣l∕l∕2∣
0
1000	2000	3000	4000	5000
Number of minibatches
Relationship between specialization of node j and its ∣w2j∣ (Lnce)
#Cluster centers that activate the node
Relationship between specialization of node j and its ∣w2j∣ (Qmpie)
≠C∣uster centers that activate the node
Figure 8: When |w2,j| is high, the corresponding node j is highly selective to one specific cluster of
the data generative models. On the other hand, those node j with low selectivity has very small w2,j
and does not contribute substantially to the output of the network. Training with Lτnce (Left Plot)
seems to yield stronger selectivity than with Lsimp (Right Plot).
34
Under review as a conference paper at ICLR 2021
Figure 9: Ablation of how the Frobenius norm of the covariance operator OP changes over training,
under different factors: depth L, sample range of ρ*ν (ρμν 〜Uniform[delta」ower, 1]) and over-
parameterization ∣Nμ∣. Top row: covariance operator of immediate left latent variable of the root
node z0; Bottom row: covariance operator of immediate right child of the root node z0.
G.2 HLTM
We also check how the norm of the covariance operator (OP) changes during training in differ-
ent situations. For this set of experiments, we construct a complete binary tree of depth L. The
class/sample-specific latent z0 is at the root, while other nuisance latent variables are labeled with
a binary encoding (e.g., μ = “s010” for a Zμ that is the left-right-left child of the root zo). Please
check Fig. 6 for details.
Again we use SimCLR with the Lτnce loss (τ = 0.1 and H = 1) to train the model. `2 normal-
ization is used in the output layer. The results are shown in Fig. 9 and Fig. 10. We could see
that norm of the covariance operator indeed go up, showing that it gets amplified during train-
ing. We perform a grid search of depth = [3,4,5], delta」Ower = [0.7,0.8,0.9] (and the po-
larity p*v 〜 Uniform[delta」ower, 1] at each layer) and over-parameterization parameter hid =
∣Nμ∣ = [2,5,10]. For each experiment configuration, We run 30 random seeds.
G.3 BYOL experiments Setup
For all STL-10 task, We use ResNet18 as Wbase. The extra predictor is tWo layer. It takes 128
dimensional input, has a hidden layer of size 512, and its output is also 128 dimensional. We
use ReLU in-betWeen the tWo layers. When We add BN to the predictor, We add it before ReLU
activation. When We say there is no BN in Tbl. 3, We remove BN in both predictor and projector
layer (but not in the encoder). Same as BYOL paper (Grill et al., 2020), symmetric loss function is
used With `2 normalization in the topmost layer.
We use simple SGD optimizer. The learning rate is 0.03 (unless otherWise stated), momentum is 0.9
and Weight decay is 0.0004. The training batchsize is 128. The ImageNet experiment runs on 32
GPUs With a batchsize 4096.
G.4 Additional BYOL experiments
Based on our theoretical analysis, We try training the predictor in different Ways and check Whether
it still Works.
From the analysis in Sec. 5, We knoW that the reason Why BYOL Works is due to the dominance
of CoVχ[Kι(x), Kι(x; W)] and its resemblance of the covariance operator Vx[Kι(x)], which is a
PSD matrix.
35
Under review as a conference paper at ICLR 2021
do*-。E3N
0.5
0.4
a.
° 0.3
ħ 0.2
z
0.1
0.0
0	10	20	30	40	50
Iterations
0.08------depth=5
0.06-
0.04-
0.02-
0.00 -y-------------
do*-。E^oz
0
0
0
0
0	10	20	30	40	50
ItamHCnU
0	10	20	30	40	50
0	10	20	30	40	50
Itamticnu
dɔjo Eħz
0.100
0.075-
0.050 -
0.025
0.150
0	10	20	30	40	50
0	10	20	30	40	50
ItamMCnU
Figure 10: Ablation of how the Frobenius norm of the covariance operator OP changes over training.
Same setting as Fig. 9 but focus on lower level. Note that since we have used `2 normalization at
the topmost layer, the growth of the covariance operator is likely not due to the growth of the weight
magnitudes, but due to more discriminative representations of the input features fμ with respect to
different z0 . Top row: covariance operator of left-right-left latent variable from the root node z0 ;
Bottom row: covariance operator right-left-right-left latent variable from the root node z0 .
The dominance should be stronger if the predictor has smaller weights than normally initialized
using Xavier/Kaiming initialization. Also, CoVχ[Kι(x),Kι(x; W0)] should behave more like a
PSD matrix, if the predictor’s weights are all small positive numbers and no BN is used.
The following table justifies our theoretical findings. In particular, Tbl. 10 shows better performance
in STL-10 with smaller learning rate and smaller sample range of the predictor weights.
Table 7: Training one-layer predictor with positive initial weights and no EMA (γema = 0). All
experiments run for 3 seeds.
Sample range of predictor weight	[0, 0.01]	[0,0.02]	[0, 0.05]
With BN in predictor	62.78 ± 1.40	62.94 ± 1.03	62.31 ± 1.80
Without BN in predictor	71.95 ± 0.27	72.06 ± 0.44	71.91 ± 0.59
Table 8: Training one-layer predictor with positive initial weights with EMA (γema = 0.996) and
predictor resetting every T = 10 epochs. All experiments run for 3 seeds. Note that Xavier range is
Uniform[-0.15, 0.15] and our initialization range is much smaller than that.
SamPle range of Predictor Weight 11	[0,0.003]
With BN in predictor
Without BN in predictor
65.61 ± 1.34
74.39 ± 0.67
[0, 0.005]
70.56 ± 0.57
74.52 ± 0.63
[0, 0.007]
70.87 ± 1.51
74.80 ± 0.57
36
Under review as a conference paper at ICLR 2021
Table 9: Same as Tbl. 8 but with different weight range. All experiments run for 3 seeds.
Sample range of predictor weight	[0, 0.01]	[0,0.02]	[0, 0.05]
With BN in Predictor	68.98 ± 2.34	66.56 ± 1.70	68.41 ± 1.19
Without BN in predictor	74.66 ± 0.81	73.60 ± 0.32	74.34 ± 0.77
Table 10: Top-1 Performance on STL-10 with a two-layer predictor with BN and EMA
(γema = 0.996). Learning rate is smaller (0.02) and predictor weight sampled from
Uniform[-range, range]. Note that for this, Xavier range is Uniform[-0.097, 0.097] and our
range is smaller.
Weight range	0.01	0.02	0.03	0.05
T=3	79.48 ± 0.40	79.70 ± 0.47	79.66 ± 0.37	78.63 ± 0.10
T = 5	78.97 ± 0.62	79.63 ± 0.23	79.65 ± 0.37	79.01 ± 0.27
T = 10	79.25 ± 0.20	79.63 ± 0.22	79.58 ± 0.25	79.18 ± 0.22
T = 20	79.15 ± 0.66	79.91 ± 0.10	79.78 ± 0.05	79.54 ± 0.25
37