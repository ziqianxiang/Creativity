Under review as a conference paper at ICLR 2021
fast estimation for privacy and utility in dif-
FERENTIALLY PRIVATE MACHINE LEARNING
Anonymous authors
Paper under double-blind review
Ab stract
Recently, differential privacy has been widely studied in machine learning due to
its formal privacy guarantees for data analysis. As one of the most important pa-
rameters of differential privacy, controls the crucial tradeoff between the strength
of the privacy guarantee and the utility of model. Therefore, the choice of has a
great influence on the performance of differentially private learning models. But
so far, there is still no rigorous method for choosing . In this paper, we deduce
the influence of on utility private learning models through strict mathematical
derivation, and propose a novel approximate approach for estimating the utility of
any value. We show that our approximate approach has a fairly small error and
can be used to estimate the optimal according to the expected utility of users. Ex-
perimental results demonstrate high estimation accuracy and broad applicability
of our approximate approach.
1	introduction
In recent years, more and more researches have exposed potential privacy risks in large-scale ma-
chine learning tasks Bassily et al. (2014); Fredrikson et al. (2014; 2015); Shokri et al. (2017); Yeom
et al. (2018). Therefore, with the broad deployment of machine learning applications and machine
learning as a service, the privacy concerns are becoming more and more serious. To address this
problem, many recent studies turn to combining machine learning algorithms with the framework of
differential privacy Dwork et al. (2014), which guarantees privacy by adding random noise to each
of the model parameters. The amount of added noise will directly affect the privacy guarantee and
utility of learned model. The more added noise, the stronger the privacy guarantee, and the more
serious the degradation of utility. Mathematically, this trade-off is tunable by a parameter .
The value of is essential for differentially private learning models. Too small may lose too much
utility that results in a useless model, and too large may provide meaningless privacy guarantee.
Thus, how to choose is a difficult task for users. The crux of this problem is that users are offered
too little insight into how this should be done. For most users, the only way is to try the value of
one by one to seek for the optimal that provides satisfied privacy-utility trade-off. Note that each
attempt requires to train a model to see if this is available, this will cause a lot of meaningless
consumption of resources and time, and usually, it is very difficult for users to find the optimal that
meets their expectations in limited attempts.
One way to address this issue is to make users understand their privacy requirements, so that the
corresponding value of can be directly calculated according to the formula of differential privacy.
Some solutions have been put forward based on this point Lee & Clifton (2011); Hsu et al. (2014);
Naldi & Dacquisto (2015); Kohli & Laskowski (2018). However, these solutions often rely on
strong assumptions, such as users are able to perceive and judge the effects of , which is often
hard to achieve in reality. Especially in DP-ML, where the privacy is a quite abstract and complex
concept.
In this paper, we focus on solving the problem from a new angle, which takes advantage of users’
familiarity with utility. For most users, the concept of utility is more intuitive than privacy. The
requirement of utility can be easily expressed by many indexes, such as accuracy, precision, loss,
error, etc. Therefore, choosing according to the expected utility is more likely to gain better
recognition and acceptance in practice. Based on this point, we put forward our approach, which
1
Under review as a conference paper at ICLR 2021
can estimate the optimal value according to users’ expected utility, or, conversely, estimate the
utility of model for a given value of .
The contributions of this paper can be summarized as follows:
•	We comprehensively analyze the influence of on utility in differentially private machine
learning. We propose a practical approximate approach for utility estimation, which has a
fairly small error on estimation results.
•	We show how to use our approximate approach to estimate the optimal according to the
expected utility. We also offer an approximate method for directly obtaining the private
model after estimation.
•	Experimental results show that the estimation results of our approximate approach is con-
siderable close to the actual measured results and the error is basically in line with our
expectations.
2	related works
The question of how to choose has always existed since differential privacy was first proposed
in Dwork (2006). However, so far, the relevant research is still very limited, most of which mainly
focus on the explainability of privacy. Lee & Clifton (2011) analyzes the necessity of privacy guar-
antee from the perspective of adversary. Naldi & Dacquisto (2015) conducted two new parameters
to provide a more precise picture of the level of differential privacy achieved. Hsu et al. (2014)
provides some analysis of privacy considerations through the balance between data consumers and
data providers. Kohli & Laskowski (2018) uses an information system to incorporate user’s privacy
preferences by having them vote on the privacy parameters they would like to have. Different from
the above researches, we start from a completely new perspective, which allows users to choose the
value of according to their expectations on utility.
Within the research community of DP-ML, the influence of on utility is also a hot research topic.
Especially in recent years, many novel methods Abadi et al. (2016); Geumlek et al. (2017); Zhang
et al. (2017); Yu et al. (2019) can significantly reduce the added noise for achieving the same level
of differential privacy, which can reduce the effect of noise on utility. However, the utility bound,
or error bound, proposed in previous literature can only be used as reference in related researches,
which can hardly provide helpful instruction for users in practice.
The closest research to our work is Ligett et al. (2017), in which they proposed a meta-method
to find the empirically strongest privacy level that meets the accuracy constraint. In their method,
a very private hypothesis is initially computed, and then the noise is gradually subtracted until a
sufficient level of accuracy is achieved. However, this method requires many times of attempts (or
noise subtraction), and just as we mentioned in introduction, this will cause a lot of consumption
of resources and time. Our method can find the empirically strongest privacy level that meets the
accuracy constraint through a few training times, which is more efficient and faster.
3	preliminary
In the rest of this paper, We use ∣∣∙∣∣ to denote the L2 norm. All vectors will typically be written in
boldface. We will use I to denote the identity matrix. Before delving into the details of our approach,
we will first recall some basic concepts and expressions of differential privacy and differentially
private learning.
3.1	differential privacy
Differential privacy is a rigorous mathematical framework for privacy guarantee. It has recently
received a significant amount of research attention for its robustness to known attacks. The typical
definition of differential privacy is given in Dwork et al. (2014), as follows:
Definition 1 (-differential privacy). Given a randomized function M : D → R with domain D and
range R, we say M is -differential privacy if for all pairs of neighboring inputs D, D0 differing by
2
Under review as a conference paper at ICLR 2021
one record, and for any subset of outputs S ⊆ R, we have:
Pr[M(D) ∈ S] ≤ e × Pr[M(D0) ∈ S].
A common way to achieve differential privacy is to add some randomized noise to the output, where
the noise is proportional to sensitivity of function M : maxD,D0 kM(D) - M(D0)k.
3.2	differentially private learning
For machine learning algorithms, we consider the regularized empirical risk minimization, which is
to learn a classifier from labeled examples. Let X denotes the input space and Y denotes the output
space, given a dataset D = {z1, z2, . . . , zn} where zi = (xi, yi) ∈ X × Y, the objective is to get a
model θ from the following unconstrained optimization problem:
λ
θ = argmminL(θ, D) + qI∣Θ∣I2,	(1)
where L(θ, D) = n PZi '(θ, Zi) is the empirical loss and '(θ, Zi) is the loss function for Zi. The
term of λ ∣∣θ∣∣2 is the regularize] that prevents over-fitting.
To solve the privacy issues in machine learning, Chaudhuri et al. (2011) proposed an effective
mechanism for learning algorithm to achieve -differential privacy, called objective perturbation.
Formally, the objective perturbation mechanism would be expressed as:
θe = argmin L(θ,D) + I∣Θ∣I2 + 1 bT θ +1 δ∕∣θ∣∣2,	⑵
θ	2	n 2
where be is random noise with density
f(b) = 1e-β*bl1,	⑶
α
where the parameter α is a normalizing constant. It has been proven that 2 achieves -
differential privacy if L(θ, D) is 1-strongly convex and twice-differentiable, with ∣∣VθL(θ, D)|| ≤
1,∣∣VθL(θ, D)|| ≤ C in θ, andβe,δe are defined as follows Chaudhuri et al. (2011):
(1 e,	if 0 <e ≤ log(1 + n2λ + ncλ2),
]1(= _ 1 lop,(1 +	2c	+ C )	iff > ]op∙(1 + 2c + C )
(2 E 2 log(1+	nλ	+	n2λ2 ),	if e > log(1 + nλ +	n2λ2 ),
(n(e 4c-1)-λ,	if 0 <e ≤ iog(ι+nc+n⅛2),
10,	ife > ιog(ι+nc+n⅛).
Our analysis in this paper is mainly based on the objective perturbation mechanism in the above
form.
βe
δe
4 approximate approach
As we discussed at the beginning of the paper, choosing an appropriate value of E is essential for
DP-ML. However, without any instructions, it is also a very difficult job, especially for large-scale
learning tasks. Repeated training not only brings huge consumption, but also gets little feedback:
looking for the next test point will be as clueless as the previous one, just knowing that it should be
larger or smaller.
Our goal is to offer users an efficient way to choose the optimal E for their private learning algorithms.
Specifically, we aim to let users know the utility of an arbitrary E, or the optimal E achieves the
expected utility in a very convenient way and a very short time, so that they can choose the optimal
E more efficiently and purposefully. To this end, our first step will be understanding the influence
of E on the model utility in the differentially private learning. In order to facilitate calculation and
analysis, we use the empirical loss to represent the model utility.
3
Under review as a conference paper at ICLR 2021
4.1	approximate analysis
Formally, we denote θ0 and θ as the true minimizers of objective function that achieves 0- and
-differential privacy, respectively. Then, we denote L(θ0 , D) - L(θ, D) as the utility difference
between θ0 and θ . According to the Taylor expression approximation at , we have
L(θj,D)- L(θe, D) ≈	(JD) (e0-e).	(5)
∂
Apparently, m addition to the change of e, the utility change also denpends on d D. Theorem 1
gives Us a further analysis of dL(θ1D).
Theorem 1. Assume that L(θ, D) is twice-differentiable and strictly convex in θ. Let W =
* L∂θ ①D + λI + δe I, b：=紫,61=蒙.Then, we have
”(θ[ D) = - (vθL(θe, D))T We-1 (nbe + δeθe).	(6)
Proof. Due to the fact that the derivative of the objective function 2 at θe is 0, We have that
_ , ʌ .
∂L(θe, D)
-∂θ-
1
+ λθe + -be + δe θe = 0.
(7)
Take the derivative of 7 With respect to , We have:
-C	, ʌ	_ ʌ
∂2L(θe, D) ∂θe
∂θ2~∂I
+ λ等 + 1 be + δe 等 + δeθe = 0.
∂ n	∂
Re-arranging the terms, We get
Thus We obtain
∂ ∂2L(θe, D)
I	∂θ2-
-1 be — δ0θe∙
ne e
+ λI + δeI
(8)
Finally, We have
_ , ʌ .
∂L(θe, D)
∂^
T
∂L(θe, D))	∂θe
∂θ~∂
— ReL(θe, D))T WeT
Which concludes the proof.
□
The assumptions in Theorem 1 can be satisfied on common used loss functions, such as logistic
regression, Huber SVM and quadratic loss. In certain cases, some of these assumptions can be
Weakened. We Will discuss this case in Section 4.4.
4.2	approximate calculation
Plug 6 into 5 , We have
L(θe0, D) — L(θe, D) ≈ — (vθ L(θe,D))T WeT (b be + δ[ θe) (「6 ∙	(9)
Although Equation 9 seems very complicated, many of the terms are familiar to us, such as
VeL(θe, D), d Ldθ①D1 and θe. A lot of existing methods can be used to calculate them in the
process of model training. Next, We Will discuss the calculation of the only neW term2, b0e .
1Also knoWn as the Hessian matrix.
2 δ and δ0 can be directly calculated by Equation 4
4
Under review as a conference paper at ICLR 2021
The difficulty of calculating b0 is that the function outputting b is not explicitly given. Thus,
we need to first form a function related to E that outputs b. Let b = (b1, . . . , bd), we have
b0 = (b01, . . . , b0d). Since b is random noise with density in 3, the probability distribution function
of each bi could be given as follows:
=
b)
F(
-+
1-2 1-2
ɪ) + ɪ eβ≡b
αβJ+ αβe e ，
ɪ) _ ɪe-艮b
αβ Jaee e ,
if b ≤ 0,
if b > 0.
(10)
Note that limχ→∞ F (b) = 1,we have Oe- = 2 ,thus We can get rid of α and re-write the probability
distribution function in the following manner:
F(b)
ifb ≤ 0,
ifb > 0.
(11)
Take the inverse of F (b), we have
ι /、	f ɪ ln(2c),	if 0 < C ≤ 1,
F	(C) =「」n(2-2c), if 2 < c<1,	(12)
Note that F-1(c) is a function that takes C 〜U(0,1) as input and outputs random variables follows
F (C), itis the funCtion that satisfies our requirement. Thus, we Can CalCulate eaCh b0i by the following
formula:
b，= ∂F-ι(Ci)=--圭 in(2ci)dβ6,	if 0 < ci ≤ 2,	(13)
i ∂e	ɪ -12 ln(2 - 2ci)dβ∈, if 2 < ci < 1,
where each Ci 〜 U(0,1).
Equation 9 provides an approximate approach for us to calculate the change of model’s utility when
varying the value of , which offers users a solution for choosing the optimal . The details of how
this could be done will be discussed in Section 5.
4.3	error analysis
Given the complete Taylor expression of Equation 5 as follows
^τ
LMe-LM,D = -⅛~) (J-e) +	(14)
where r(e0 - E)= d L⅞,D) (e0 - e)2 is the Taylor remainder, e ∈ [e, e0], we can see that our error
mainly comes from omitting the Taylor remainder. Thus, to derive the error bound of Equation 9,
we require the Taylor remainder to be bounded, specifically, we require d L∂⅛,D) to be bounded.
∂b0
Theorem 2. Assume that L(θ, D) is third-differentiable and strictly convex in θ. Let b- =	,
δ~ =*,H^^ =岸Ld(^6 and T§^ =	D). Then, we have
∂2L(θg, D)
∂e2
n 喔+δ ©J W-1HawT1 (n 喔+δ @
-(VθL(f)e, D))T W-1(TΘ3 + δ红)W-1 1b喔 +
-(vθLM, D))T W-1 ɑb0 + δgθ)
+ δ[ (Vθ L(θ,, D))T W-1W-1 (J 喔 + δ] θj .
(15)
Proof. See Appendix A.1.
□
By bounding each term in 15, we can obtain our error bound as follows:
5
Under review as a conference paper at ICLR 2021
Theorem 3.	Assume that L(θ, D) is third-differentiable and strictly convex in θ. The δ is defined
as 4. Ifthere exist constants c1,c2,c3,c4 so that for ∀x ∈ D, θ ∈ C, ∣∣θ∣∣ ≤ ci, ∣∣Vθ L(θ, D)|| ≤
c2, ∣∣VθL(θ, D)|| ≤ c3, ∣VθL(θ, D)ll ≤ c4. Then, for any 0 <e* < 1 we have
| Lapprox (仇，，D)- Lactua1(仇，,D) | = O
(J)2
n2e4
(16)


where Lapprox (θ0, D) is the approximation loss at 0 estimated by the loss at , Lactual (θ0, D) is
true loss at 0 from actual measurement, n is the number of training samples.
Proof. See Appendix A.2.
□
From Equation 16, we can see that our error bound is proportional to the difference between and
0. That is, the closer the measuring to the estimation target 0, the higher the estimation accuracy.
In addition, our error bound is also inversely proportional to n and measuring , which indicates that
more training samples and large measuring values would help to produce more accurate estimates.
For example, ifwe set n = 10000 and use = 0.1 as measuring point to estimate the loss at = 0.2,
then the error is only about 10-6, which is quite small as the range of loss is 0 to 1.
However, sometimes the targets 0 we want to estimate may be quite small. In this case, we can
divide the estimate targets into groups with similar 0 values and use the mean value of each group
as the measuring , which may minimize the estimation error.
4.4 assumption violation
Equation 5 relies on several assumptions that may be violated in practice. The first would be non-
convex or non-convergent objectives. In this case, the obtained parameters θ may not be the global
minimum, thus Equation 7 may not hold. To address this issue, we can form a convex quadratic
approximation of the loss around θ, i.e.,
L(θ,D) ≈ L(θe,D) + Vθ L(θe, D)(θe - θ) + (θe- θ)τ Vθ L(θe, D)(θe - θ).
The second violation would be non-differentiable losses, in which neither VθL(θ, D) nor
VθL@, D) exists. To address this issue, We can approximate the loss function by a different one,
which is doubly differentiable.
Remark 1. The above analysis shows that our approximate approach can be extended to the non
convex, non-convergent and non-differentiable situations. However, in this paper, we mainly focus
on verifying the effectiveness of our approach. Therefore, we only consider the normal case in our
evaluations. We leave the important analysis of our approximate approach for non-convex, non-
convergent and non-differentiable cases to our future work.
5 CHOOSING WITH OUR APPROXIMATION APPROACH
With the approximation approach and its error, We can achieve our goal: to offer users an efficient
Way to choose the optimal . The simplest Way is to select a suitable initial point , train a model
With , calculate all coefficients in 9, and then use 9 to estimate the utility of other points. It should
be noted that When selecting initial point, it is better to refer to the considerations mentioned in
Section 4.3.
Users can also choose by setting a baseline of utility. To this end, We need to re-arrange the terms
in 9 as folloWs:
,ʌ , ʌ .
L(θeo, D)- L(θe, D)
(17)
Similarly, users only need to train the model once at an initial point to calculate all coefficients in 9,
and then the optimal can be obtained according to the users’ base line of utility. The baseline of
utility could be given in various forms, for example, the user can give the difference from the utility
of initial point, such as setting the baseline to be 10% higher.
6
Under review as a conference paper at ICLR 2021
Note that we only need to train the algorithm on the full dataset only once, our optimization effect
is considerable, especially in large-scale learning tasks. Once the optimal 0 is estimated, the user
can use it to train the final model directly. Of course, if the user has very strict requirements on the
accuracy, he/she can also use our approximation results as instructions for choosing the test during
repeated training, which can save a lot of training times to help him/her find the optimal faster.
Sometimes, it may be inconvenient for users to re-train the model, for example, the training data
may have been lost, or the computing resource authorization may have been expired. To solve the
problems of such users, we also provide a model estimation method as follows:
Theorem 4.	Assume that L(θ, D) is twice-differentiable and strictly convex in θ. Let W =
d L∂θ2 ,D) + λI + δe I, b[=嗓,δ[=嗓.Then, we have
θ ≈ θe - WeT (1 b； + δ[ θj.	(18)
Apparently, Equation 18 is derived from the following formula:
∂θe
θe0 - θe ≈ -^-(C - E),
∂
which is the Taylor expression approximation of θe at e. However, We still suggest that users use the
estimated C to re-train the model if conditions permit, since the superposition of estimation methods
may bring about large errors.
6 evaluations
6.1	evaluation setup
In this section, we will empirically analyze the performance of our approximation approach. We
implement the output perturbation mechanism of Chaudhuri et al. (2011) based on the open source
released by Iyengar et al. (2019), using Laplace distribution for noise sampling. Our evaluation
considers the loss functions for two commonly used models: logistic regression (LR) and Huber
SVM (SVM), which are defined as follows:
'lr (Z) = log(1 + e-z),
0,	ifz >1+h,
'svm (Z) = ∖ 41h (1 + h - Z)I2, if |1 - z| ≤ h,
[1 — z,	ifz < 1 — h,
(19)
both of which are twice-differentiable. The Adult Dua & Graff (2017), Kddcup99 Stamper and
Gisette Guyon et al. (2004) datasets are used in our experiments, each of which is randomly par-
titioned into 80% training samples and 20% testing samples. We use stochastic gradient descent
algorithm to minimize 1 and set the iterations and learning rate to be 100 and 0.01. Due to the ran-
dom noise addition, all the experiments are repeated ten times and the average results are reported.
We tune the hyperparameter λ by training a non-private model on each dataset and find the optimal
value tobe λ = 10-6. The constant c in Equation 4 is set to be 0.25 for both loss functions according
to Chaudhuri et al. (2011).
6.2	evaluation results and analysis
The first experiment is to verify our approximation approach. In this experiment, we fix the sample
number n = 10000. The measuring points are chosen as C = 0.1, C = 0.25, C = 0.75 and the target
points vary from C = 0.05 to C = 1.0. The experimental results on Adult and Kddcup99 are shown
in Figure 1, and the experimental results on Gisette are shown in Appendix A.3 due to the limitation
of space. The orange line, which represents actual empirical loss of each C is called the actual line,
and the other lines, which represent the estimate of loss, are called fitting line. Each fitting line is
drawn by the estimation results of the loss of each C, which are called the target points, based on the
actual measurement under a specific C, which is called the measuring point. It can be seen that our
7
Under review as a conference paper at ICLR 2021
Target Point (ɛ)
(a) Adult-LR
(b) Adult-SVM
(c) Kddcup99-LR
(d) Kddcup99-SVM
Figure 1:	Performance of our approximation approach on datasets of Adult and Kddcup99 with
logistic regression (LR) loss and Huber SVM (SVM) loss.
Table 1: Average error of estimated loss for each measuring point.
C	0.10	0.25	0.30	0.35	0.45	0.75	0.85
Adlut-LR	0.00823	0.00003	0.00033	0.00004	0.00293	0.00649	0.01480
Adlut-SVM	0.02052	0.00445	0.00367	0.00385	0.00298	0.00770	0.00840
KddCup-LR	0.02393	0.00763	0.00845	0.00960	0.00283	0.01186	0.01200
KddCup-SVM	0.00866	0.00851	0.01052	0.01143	0.00848	0.00112	0.00038
Gisette-LR	0.01482	0.01931	0.00302	0.00810	0.00394	0.04637	0.03693
Gisette-SVM	0.00571	0.00567	0.01023	0.00550	0.00823	0.01745	0.02911
approximation results basically fit the empirical loss for each . In addition, we can see that each
fitting line has a different slope. The larger the value of C used in the actual measurement, the greater
the slope of the fitting line. This is consistent with our previous analysis. Specially, each fitting line
will cross the actual line at its measured C. In addition, it can be seen that each fitting line is straight.
The reason for this is that after training at the measuring C, the coefficients in 9 are determined,
which makes a linear relationship between the value of target C and its estimated empirical loss. We
will explore the non-linear relationship between the two in our future work.
In order to further analyze the impact of the selection of measuring point on the estimation results,
we calculate the average error of estimated loss when each C is used as the measuring point and
represent the results in Table 1. The experimental results are abridged, and the rest can be found in
Appendix A.3. For each experiment, the result in bold represents the measuring C with minimum
error. Although the C with minimum error of each results are slightly different, C in the middle tends
to have lower average error. It is their sum of squares to other points is smaller. Thus, when using
our approximate approach for choosing optimal C, we suggest to begin with C in the middle.
Finally, we evaluate the effect of sample number on estimation error. Due to the limited sample
number of Gisette, we only use Adult and Kddcup99 in this experiment. We vary the sample number
from 1000 to 20000 and fix C for each evaluation to be the one that achieves minimum error in
Table 1. The results are shown in Figure 2. Obviously, with the increase of sample number, the
estimation error decreases, which is consistent with our previous analysis.
0.0	0.2	0.4	0.6	0.8
Target Point (ɛ)
(a) Adult-LR
0.0	0.2	0.4	0.6	0.8
Target Point (ɛ)
(d) KddCuP99-SVM
0.0	0.2	0.4	0.6	0.8
Target Point (ε)
(b) Adult-SVM
0.0	0.2	0.4	0.6	0.8
Target Point (£)
(c) Kddcup99-LR
Figure 2:	AffeCt of samPle number on estimation error. The measuring Point for eaCh evaluation,
from left to right, is set to be C = 0.25, C = 0.45, C = 0.45, C = 0.85, respeCtively, whiCh aChieves
the minimum error in Table 1.
8
Under review as a conference paper at ICLR 2021
7 conclusion
In this paper, we focus on solving the problem of choosing optimal in DP-ML. We start by analyz-
ing the influence of on the utility of learned model. Then we put forward our approximate approach
for estimating the utility difference between the private models trained with any two values. We
show how to use our approximate approach to solve the problem we mentioned at the beginning.
We conduct several experiments to verify our analysis. Experimental results demonstrate the good
estimation accuracy and broad applicability of our approximate approach.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318. ACM, 2016.
Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
Computer Science, pp. 464-473. IEEE, 2014.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical
risk minimization. Journal of Machine Learning Research, 12(Mar):1069-1109, 2011.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Cynthia Dwork. Differential privacy. In Proceedings of the 33rd international conference on Au-
tomata, Languages and Programming - Volume Part II, 2006.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and TrendsR in Theoretical Computer Science, 9(3-4):211-407, 2014.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322-1333. ACM, 2015.
Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart.
Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd
{USENIX} Security Symposium ({USENIX} Security 14), pp. 17-32, 2014.
Joseph Geumlek, Shuang Song, and Kamalika Chaudhuri. Renyi differential privacy mechanisms
for posterior sampling. In Advances in Neural Information Processing Systems, pp. 5289-5298,
2017.
Isabelle Guyon, S R Gunn, Asa Benhur, and Gideon Dror. Result analysis of the nips 2003 feature
selection challenge. pp. 545-552, 2004.
Justin Hsu, Marco Gaboardi, Andreas Haeberlen, Sanjeev Khanna, Arjun Narayan, Benjamin C
Pierce, and Aaron Roth. Differential privacy: An economic method for choosing epsilon. pp.
398-410, 2014.
Roger Iyengar, Joseph P Near, Dawn Song, Om Thakkar, Abhradeep Thakurta, and Lun Wang.
Towards practical differentially private convex optimization. pp. 299-316, 2019.
N. Kohli and P. Laskowski. Epsilon voting: Mechanism design for parameter selection in differential
privacy. In 2018 IEEE Symposium on Privacy-Aware Computing (PAC), pp. 19-30, 2018.
Jaewoo Lee and Chris Clifton. How much is enough? choosing for differential privacy. pp.
325-340, 2011.
Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, and Zhiwei Steven Wu. Accuracy first:
Selecting a differential privacy level for accuracy-constrained erm. In Proceedings of the 31st
International Conference on Neural Information Processing Systems, NIPS’17, pp. 2563-2573,
Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
9
Under review as a conference paper at ICLR 2021
Maurizio Naldi and Giuseppe Dacquisto. Differential privacy: An estimation theory-based method
for choosing epsilon. arXiv: Cryptography and Security, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3-18. IEEE, 2017.
Niculescu-Mizil A. Ritter S. Gordon G.J. & Koedinger K.R. Stamper, J. Challenge data set from
kdd cup 2010 educational data mining challenge.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learn-
ing: Analyzing the connection to overfitting. pp. 268-282, 2018.
Lei Yu, Ling Liu, Calton Pu, Mehmet Emre Gursoy, and Stacey Truex. Differentially private model
publishing for deep learning. arXiv preprint arXiv:1904.02200, 2019.
Jiaqi Zhang, Kai Zheng, Wenlong Mou, and Liwei Wang. Efficient private erm for smooth objec-
tives. arXiv preprint arXiv:1703.09947, 2017.
10