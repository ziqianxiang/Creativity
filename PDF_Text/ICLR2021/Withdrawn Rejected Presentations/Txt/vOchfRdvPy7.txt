Under review as a conference paper at ICLR 2021
To be Robust or to be Fair: Towards Fairnes s
in Adversarial Training
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial training algorithms have been proved to be reliable to improve ma-
chine learning models’ robustness against adversarial examples. However, we find
that adversarial training algorithms tend to introduce severe disparity of accuracy
and robustness between different groups of data. For instance, PGD adversarially
trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l∞-
8 adversarial accuracy1 on the class ”automobile” but only 59% and 17% on class
”cat”. This phenomenon happens in balanced datasets and does not exist in natu-
rally trained models when only using clean samples. In this work, we theoretically
show that this phenomenon can generally happen under adversarial training algo-
rithms which minimize DNN models’ robust errors. Motivated by these findings,
we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness
problem when doing adversarial defenses and experimental results validate the
effectiveness of FRL.
1	Introduction
The existence of adversarial examples (Goodfellow et al., 2014; Szegedy et al., 2013) causes huge
concerns when applying deep neural networks on safety-critical tasks, such as autonomous driv-
ing vehicles and face identification (Morgulis et al., 2019; Sharif et al., 2016). These adversarial
examples are artificially crafted samples which do not change the semantic meaning of the natu-
ral samples, but can misguide the model to give wrong predictions. As countermeasures against
the attack from adversarial examples, adversarial training algorithms aim to train classifier that can
classify the input samples correctly even when they are adversarially perturbed. Namely, they opti-
mize the model to have minimum adversarial risk of that a sample can be perturbed to be wrongly
classified:
min E
fx
max L(f (x + δ), y)
These adversarial training methods (Kurakin et al., 2016; Madry et al., 2017; Zhang et al., 2019b)
have been shown to be one type of the most effective and reliable ways to improve the model robust-
ness against adversarial attacks. Although promising to improve model’s robustness, recent studies
show side-effects of adversarial training: it usually degrades model’s clean accuracy (Tsipras et al.,
2018).
In our work, we find a new intriguing property about adversarial training algorithms: they usually
result in a large disparity of accuracy and robustness between different classes. As a preliminary
study in Section 2, we apply natural training and PGD adversarial training (Madry et al., 2017) on
the CIFAR10 dataset (Krizhevsky et al., 2009) using a ResNet18 (He et al., 2016) architecture. For
a naturally trained model, the model performance in each class is similar. However, in the adver-
sarially trained model, there is a severe performance discrepancy (both accuracy and robustness) of
the model for data in different classes. For example, the model has high clean accuracy and robust
accuracy (93% and 67% successful rate, separately) on the samples from the class “car”, but much
poorer performance on those “cat” images (59% and 17% successful rate). More preliminary re-
sults in Section 2 further show the similar “unfair” phenomenon from other datasets and models.
Meanwhile, we find that this fairness issue does not appear in natural models which are trained on
clean data. This fact demonstrates that adversarial training algorithms can indeed unequally help to
improve model robustness for different data groups and unequally degrade their clean accuracy.
1The model’s accuracy on the input samples that have been adversarially perturbed.
1
Under review as a conference paper at ICLR 2021
In this work we first define this problem as the unfairness problem of adversarial training algo-
rithms. If this phenomenon happens in real-world applications, it can raise huge concerns about
safety or even social ethics. Imagine that an adversarially trained traffic sign recognizer has overall
high robustness, but it is very inaccurate and vulnerable to perturbations for some specific signs such
as stop signs. The safety of this autonomous driving car is still not guaranteed. In such case, the
safety of this recognizer depends on the worst class performance. Therefore, in addition to achiev-
ing overall performance, it is also essential to achieve fair accuracy and robustness among different
classes, which can guarantee the worst performance. Meanwhile, this problem may also lead to the
issues from social ethics perspectives, which are similar to traditional ML fairness problems (Buo-
lamwini & Gebru, 2018). For example, a robustly trained face identification system might provide
different qualitative levels of service safety for different ethnic communities.
In this paper, we first explore the potential reason which may cause this unfair accuracy / unfair
robustness problem. In particular, we aim to answer the question - “Will adversarial training algo-
rithms naturally cause unfairness problems, such as the disparity of clean accuracy and adversarial
robustness between different classes?” To answer this question, we first propose a conceptual ex-
ample under a mixture of two spherical Gaussian distributions which resembles to the previous
work (Tsipras et al., 2018) but with different variances. In this setting, we hypothesize that ad-
versarial training tends to only use robust features for model prediction, whose dimension is much
lower than the non-robust feature space. In the lower dimensional space, an optimal linear model is
more sensitive to the inherent data distributional difference and be biased when making predictions.
Motivated by these empirical and theoretical findings, we then propose a Fair Robust Learning (FRL)
framework to mitigate this unfairness issue, which is inspired from the traditional debiasing strategy
to solve a series of cost-sensitive classification problems but we make specific effort to achieve the
fairness goal in adversarial setting. Our main contributions can be summarized as following: (a) We
discover the phenomenon of “unfairness” problem of adversarial training algorithms and implement
empirical studies to present this problem can be general; (b) We build a conceptual example to
theoretically investigate the main reasons that cause this unfairness problem; and (c) We propose a
Fair Robust Learning (FRL) framework to mitigate the unfairness issue in adversarial setting.
2	Preliminary Studies
CIFAR10 In this section, we present our preliminary studies to show that adversarial train-
ing algorithms usually present the unfairness issues, which are related to the strong disparity
of clean accuracy and robustness among different classes. We implement algorithms including
PGD adversarial training (Madry et al., 2017) and TRADES (Zhang et al., 2019b) on the CI-
FAR10 dataset (Krizhevsky et al., 2009). In CIFAR10, we both naturally and adversarially train
ResNet18 (He & Garcia, 2009) models. In Figure 1, we present list the the model’s accuracy and
robustness performance (under PGD attack by intensity 4/255 and 8/255) for each individual class.
Figure 1: Clean and adversarial accuracy in each class of CIFAR10 dataset, from a naturally trained
ResNet model (left), PGD-adversarially trained model (middle) and TRADES (right), against ad-
versarial examples under l∞-norm by 8/255. The trained models’ robustness are evaluated by un-
targeted PGD attack under l∞-norm constrained by 8/255 and 4/255.
From the Figure 1, We can observe that - for the naturally trained models, every class has similar
clean accuracy (around 90 ± 5%) and adversarial accuracy (close to 0%) under the PGD attack.
It suggests that naturally trained models do not have strong disparity of both clean and robustness
performance among classes. HoWever, for adversarially trained models (under PGD Adv. Training
or TRADES), the disparity phenomenon becomes severe. For example, a PGD-adversarially trained
model has 59.1% clean accuracy and 17.4% adversarial accuracy for the samples in the class “cat”,
2
Under review as a conference paper at ICLR 2021
which are much lower than the model’s overall performance. This phenomenon demonstrates that
adversarial training algorithms cannot provide the same help for the robustness for the samples in
class “cat” as other classes, and unfairly degrades too much clean accuracy for “cat”. We list our
empirical studies under more model architectures in Table 3 and more datasets (GTRSB (Stallkamp
et al., 2011)) in Appendix A.2, where we can find the similar observations.
GTSRB We also investigate the fairness is-
sue in German Traffic Sign Recognition Bench-
mark (GTRSB) (Stallkamp et al., 2011). It con-
sists of 43 classes of images from different traf-
fic signs, with image sizes 32 × 32 × 3. In
this dataset we also both naturally and adver-
sarially train a 3-Layer CNN classifier. We list
the model’s performance and sort the classes
in the order of decreasing clean accuracy and
adv. accuracy. From the Figure 2, we can see
that for the naturally trained model (left), most
classes have high accuracy which is over 90%,
but for adversarial training, some classes’ accu-
racy drops by a large margin. Meanwhile, ad-
versarial training also unequally improves the
Figure 2: Class-wise Clean & Adversarial Accu-
racy on GTSRB of Naturally Trained Model (left)
and Adversarially Trained Model (right).
model’s robustness against PGD attacks given that some classes have very low adversarial accuracy.
In this dataset, both natural model and robust model have clear distinguished adversairal accuracy
(robustness) among classes.
3	Theoretical Analysis based on A Conceptual Example
From our preliminary studies, we always observe that adversarially trained models have huge per-
formance disparity (clean and adversarial accuracy) between different groups. In this section, we
try to understand the unfairness problem via theoretical analysis based on a binary classification
problem on a mixture-Gaussian distribution, which is similar to (Tsipras et al., 2018). We first state
the necessary notions in this paper.
Notations. In the following, we use f to denote the classification model which is a mapping f :
X → Y from input data space X and output labels Y . Generally, naturally training will find the
optimal f to minimize the overall clean error Rnat(f) = Pr.(f (x) 6= y); and adversarially training
will minimize the overall robust error RrOb(f) = Pr.(∃δ, ∣∣δ∣∣ ≤ e, s.t.f (X + δ) = y). Specifically in
the following binary classification problem, Y = {-1, +1} and each class’s clean error and robust
error are denoted as the conditional probabilities: Rnat(f, -1) = Pr.(f (x) = +1|y = -1), and
Rrob(f, -1) = Pr.(∃δ, ∣∣δ∣∣ ≤ 3 Stf(X + δ) = +1∣y = -1), respectively.
3.1	A B inary Classification Task
Our study is motivated by (Tsipras et al., 2018) which uncovers one key behavior of adversarial
training: it excludes high-dimensional non-robust features (which are vulnerable to attack) and only
preserves lower-dimensional robust features for prediction. Thus, in our case, we assume our con-
ceptual dataset has the data-label pairs (X, y) sampled from a distribution D follows:
dim = m dim = d	2
y u-r {-1, +1}, θ = (G,E)- YN(-,θσ+⅛) ify = -1	⑴
where N (θ, σ+2 1I) is a normal distribution with mean vector θ and covariance matrix σ+2 1 I and same
for class “-1”. Following the work (Tsipras et al., 2018), we suppose that the feature space consists
of two kinds of features: (a) robust features with center γ and dimension m; and (b) non-robust
features with center η and dimension d. We assume η <	< γ, so an adversarial perturbation δ
with intensity ∣∣δ∣∣∞ ≤ e can manipulate a non-robust feature to have a different sign in expectation,
but δ cannot attack a robust feature. Usually, the non-robust features’ dimension d is far higher than
the robust features’ dimension d, i.e., (m << d).
In our case, we assume that the 2 classes have a key difference between their variances: σ+1 :
σ-1 = K : 1, where K > 1. In this theoretical example, our main hypothesis is that: the variance
difference between 2 classes will not lead to strong disparity of model performance for a naturally
trained model whose prediction is based on a high dimensional feature space. However, the variance
difference can cause large performance gap (both accuracy and robustness) for adversarially trained
3
Under review as a conference paper at ICLR 2021
models which are based on low-dimensional robust features. To illustrate this fact, we will explicitly
calculate the 2 classes’ clean and robust errors in the proposed distribution for both clean models
and robust models.
3.2	Optimal Linear Model to Minimize Clean Error
We first calculate one linear model for this data to minimize the total clean errors. Specifically, we
consider a linear classifier f with its optimal parameters w* and b*:
f* (x) = sign(hw*, xi +b*)
where w* , b* = arg min Pr.(f (x) 6= y)	(2)
w,b
where w is features’ weight vector and b is the model intersection. In later parts, we use f to
represent f* for convenience. We call the optimized model naturally trained model because it
minimizes model’s clean error to get overall high clean accuracy. Typically, a naturally trained
model will use both robust features and non-robust features for inference but its prediction outcome
majorly depends on non-robust features. Next, we show the exact form of the errors for each class
in Theorem 1 and the proof is provided in Appendix B.1.
Theorem 1 Optimal linear classifier which minimizes overall clean error in D will have class-
conditional clean errors:
Znat(f,-1))
ʌ
z
、
Rnnat(f, -1) = Pr.{N(0,1) ≤ a - √K ∙ A2 + q(Κ)},
Rnat(f, +1) = Pr.{N(0,1) ≤ -K ∙ A + PA2 + q(K)},
(3)
(4)
|
}
where A = σ(κ2-i)，mγ2 + dη2 and q(K)
同≤ C :
{^^^^^^^^^^^^^
Znat(f,+1))
2Kθg(KK). It has the robust error under attack
Zrob(f,-1))
Rrob(f, -1) = Pr.{N (0, 1) ≤ Znat(f, -1) +
人.
mγ + dη 0
Rrob(f, +1) = Pr.{N (0, 1) ≤ Znat(f, +1) +
mγ 2 + dη2
mγ + dη
(5)
^mγ2 + dη2 Kσ
}.
(6)
σ
{
I
}
{^^^^*∖/^^^^^^
Zrob(f,+1))
Note that the term A (consisting of feature dimension d, m and center γ , η) represents how expres-
sive the information from D that model f can use for prediction. Thus, when the term A is large,
the model has close clean errors between the 2 classes, namely Rnat(f, -1) ≈ Rnat(f, +1). It is
because the q(K) term in their z-scores2 can be ignored when A is large. On the other hand for
their robust errors, typically We assume the adversarial attack by ∣∣δ∣∣ ≤ ∈o can bring in major threat
to mislead natural model f, which will result both Zrob(f, -1) and Zrob(f, +1) to be large positive
numbers, so that the robust errors of both classes Rrob(f, -1) and Rrob(f, +1) are also large.
3.3 Optimal Linear Model to Minimize Robust Error
During adversarial training, the desired linear classifier should minimize the total robust error, which
minimizes the probability that there exists a perturbation δ constrained by budge ∣∣δ∣∣∞ ≤ E that can
let the model make mistake. Formally, we describe a linear classifier after adversarial training as:
fadv (x) = sign(hwadv, xi + badv)
where w*dv, b*dv = arg min Pr.(∃δ, ∣∣δ∣∣ ≤ e, s.t. fadv(x + δ) = y).	⑺
w,b
2At here we use Znat(f, -1) and Znat(f, +1) to denote the z-score for the standard normal distribution
corresponding to Rnat(f, -1) and Rnat(f, +1). Similarly, we use Zrob(f, -1) and Zrob (f, +1) to denote the
z-score of the standard normal distribution of Rrob(f, -1) and Rrob(f, +1).
4
Under review as a conference paper at ICLR 2021
Similarly We use fadv to denote f/v for convenience. During adversarial training, the linear model
will only preserve the weights on robust features and exclude all non-robust features as proved in
Lemma 2 in the Appendix. We Will also call the adversarially trained model in D a robust model,
because its features cannot be easily manipulated by perturbations under ∣∣δ∣∣ ≤ U We show the
robust model’s clean and robust errors in Theorem 2 and the proof is provided in Appendix B.2.
Theorem 2 Optimal linear classifier which minimizes overall robust error in D will have class-
conditional clean errors:
Rnnat(fadv, -1) = Pr{N(0,1) ≤ B - PK ∙ B2 + q(K)-西e}	(8)
σ
Rnat(fadv, +I) = Pr{N(O, I) ≤ -K ∙ B + pB2 + q(K) - ^-6}	(9)
Kσ
where B = σ(κ2-i)√m ∙ (γ - C), q(K) = 2Kg(K), and robust errors under attack ∣∣δ∣∣ ≤ ∈o:
R rob (fadv, -1) = Pr {N (0, 1) ≤ Znat(fadv, -1) + √m' }	(IO)
σ
R rob (fadv, +1) = Pr {N (0, 1) ≤ Znat(fadv, +1) + √m~F7~ }.	(II)
Kσ
Recall that we assume the dimension of non-robust features is much higher than that of robust
features (d >>m) and γ, η and C have similar scale: C = Θ(γ) and η = Θ(γ). Therefore, in Eq. (8)
and Eq. (9), for model fadv's clean errors, the decisive term B is at scale θ((d/m)- 1 ∙ A), where A
is the term in Eq.(3). In Corollary 1 (proved in Appendix B.3), we show that it is this relationship
between A and B that will finally bring in the “unfair” issue to both model accuracy and robustness
performance.
Corollary 1 Adversarially Trained Model on D will have larger clean error disparity between the
2 classes, compared to a Naturally Trained model.
We calculate 2 classes’ clean error difference: (Rnat(f, +1) - Rnat(f, -1)) for natural model f; and
(Rnat(fadv, +1) -Rnat(fadv, -1)) for adversarially trained model fadv. Since both terms are positive,
we show their ratio (detailed proof in Appendix):
Ω _ Rnat(fadv, +1) - Rnat(fadv, -1)〉(K2 - 1) Iog(K城。(_d_)1
=	Rnat(f, +1) -Rnat(f,-1)	≥	2Θ(γ2)	一(嬴) .
(12)
In Eq. (12), we can tell once d/m is large, the ratio Ω is also large (e.g. > 1) and the adversarial
training is showed to enlarge the 2 classes, clean error disparity. The ratio Ω in Eq. (12) uncovers
the main factor which may cause the “unfair” phenomenon in adversarially trained models: because
the robust models make prediction in a feature space with dimension m which is much lower than d
(dimension for natural models), their clean accuracy disparity between classes can be more sensitive
to the classes’ distributional difference K. In this way the robust model presents strong disparity of
clean accuracy.
Furthermore, for the robust errors (i.e., Eqs. (10) and (42)) in adversariallly trained model, an ad-
versarial attack under intensity C0 will make the test error increase only by a small margin compared
to the model,s clean error in Eqs. (8) and (9). It is because the their z-scores difference is de-
cided by √m which is small and much lower than d. Because of the marginal difference between
Rnat(fadv, -1) and Rrob(fadv, -1) (also for class “+1”), the model,s fairness condition on robust er-
rors will align to the clean errors (in Eq. (12)). Empirical results on real datasets also support this
assumption (Figure 1). As a conclusion, adversarial training can bring in unfairness issues on both
clean and robust performance.
4	Fair Robust Learning (FRL)
Faced with the unfairness problem of adversarial training shown in Section 2 and 3, we desire to
devise a Fair Robust Learning (FRL) strategy, in order to train robust models that have balanced
accuracy and robustness performance for each class. Formally, we aim to train a classifier f to have
minimal overall robust error (Rrob(f)); while stressing f to satisfy a series of fairness constraints:
minimize Rrob (f)
s.t.	Rnat(f, i) - Rnat(f) ≤ τ1 and Rrob(f,i) - Rrob(f) ≤ τ2 for each i ∈ Y
(13)
5
Under review as a conference paper at ICLR 2021
where τ1 and τ2 are small and positive values. The constraints in Eq. (13) restrict the model’s
error for each class i (both clean error Rnat(f, i) and robust error Rrob(f, i)) should not exceed the
average level (Rnat(f) and Rrob(f)) by a large margin. Therefore, the model will not have specific
weak points under the risk of wrong prediction or adversarial attacking. Next, we will discuss the
detailed components of our Fair Robust Learning (FRL) algorithm to solve the problem in Eq. (13).
4.1	Traditional Model Debiasing Method: A Reductions Approach
In order to solve the fair robust training problem in Eq. (13), we follow the main pipeline from
traditional machine learning debiasing works such as (Agarwal et al., 2018), which reduces the
problem in Eq. (13) into a series of Cost-sensitive classification problems and continuously penalizes
the terms which violate the fairness constraints. We begin by introducing Lagrange multipliers
φ = (φinat, φriob) (non-negative) for each constraint in Eq. (13) and form the Lagrangian:
L(f, φ)
Y
Rrob(f)+Xφinat(Rnat(f,i)-Rnat(f)-
i=1
Y
τ1 ) + X φriob(Rrob(f, i) - Rrob(f) - τ2)
i=1
(14)
It equals to solving the max-min game between f and φ as:
max min
φnat,φrob≥0 f
L(f,φ).
(15)
Typically, given a fixed φ, if the current model f violates some constraints in Eq. (13) (for ex-
ample Rnat(f, i) - Rnat (f) - τ1 > 0), we first solve the outer-maximization problem in Eq. (15)
by increasing its corresponding multiplier φinat. As a result, we upweight the training weight (or
cost) for the clean loss Rnat (f, i) of all samples in the class i. Then, the algorithm will solve the
inner-minimization given new φ to optimize the model f, the error of Rnat(f, i) is therefore heavily
penalized and the model will give more priority to the correct prediction for the class i. In this
way the model gives more priority to mitigate violated terms in Eq. (14). During this process, the
model f and Lagrangian multiplier φ will be alternatively updated to achieve the equilibrium until
we finally reach an optimal model that satisfies the fairness constraints. Based on this traditional
debiasing strategy, next we will discuss the main difference of our task in the adversarial setting
from this traditional approach.
4.2	Debias Clean Error and B oundary Error S eparately
One thing to note is that in the Eq (13), the robust error is always strongly related to clean er-
rors (Zhang et al., 2019b; Tsipras et al., 2018) (see Eq. 16). Thus, during the debiasing process
above, we could have twisted the influence on some class i’s clean and robust errors Rnat(f, i) and
Rrob(f, i). It means that when we upweight the cost for Rrob(f, i) as introduced in Eq. (15), we also
implicitly upweight the cost for Rnat(f, i). Thus, we will not get a precise update for φ. To solve
this issue, we can separate the robust error into the sum of clean error and boundary error inspired
by (Zhang et al., 2019b) as:
Rrob(f,i) = Pr.{∃δ, s.t. f(x+δ) 6=y|y=i}
=Pr.{f (x) = y|y = i} + Pr.{∃δ, f (x + δ) ∙ f (x) ≤ 0|y = i}	(16)
= Rnat(f, i) + Rbdy (f, i)
where Rbdy(f, i) = Pr.{∃δ, f (X + δ) ∙ f (x) ≤ 0|y = i} represents the probability that a sample from
class i lies close to the decision boundary and can be attacked. By separating the clean error and
boundary error during adversarial training, we are able to independently debias the unfairness from
both clean error and boundary error. Formally, we have the training objective as:
minimize Rnat(f) + Rbdy (f)
f	(17)
s.t. Rnat(f,i) - Rnat(f) ≤ τ1 and Rbdy(f,i) - Rbdy(f) ≤ τ2 for each i ∈ Y
We introduce Lagrangian multipliers φ = (φinat, φibdy) and solve the max-min game for Eq. (17) simi-
lar to Eq. (15). Note that if the constraints in Eq. (17) are satisfied, the fairness quality for robust error
(Rrob(f, i) = Rnat(f, i) + Rbdy(f, i)) of each class can also be guaranteed by τ1 + τ2. In practice,
we will use surrogate loss functions (such as cross entropy) L(f (x), y) and max L(f (x), f(x0)) to
l∣δ∣∣≤e
optimize the clean and boundary errors as suggested by (Zhang et al., 2019b).
6
Under review as a conference paper at ICLR 2021
4.3 Cost-Sensitive Classification for Clean Errors vs B oundary Errors
During the debiasing training process in Eq. (17), if one class i’s clean error violates the fairness
inequality: Rnat(f, i) -Rnat(f) -τ1 > 0, upweighting the cost for Rnat(f, i) can help penalize large
Rnat(f, i) and mitigate the unfairness issue as suggested by (Agarwal et al., 2018). Note that we refer
to this strategy as “Reweight”. However, if the boundary error for class i: Rbdy(f, i) - Rbdy (f) -
τ2 > 0, we claim that only upweighting its cost (or Reweight) could not succeed to fulfill the
cost-sensitive classification goal in adversarial setting. Our empirical studies in Section 5 show that
upweighting the boundary error for some class i cannot effectively reduce model’s boundary error
specifically for class i but can bring in side-effects to degrade class i’s clean accuracy. It is evident
from (Ding et al., 2018) that increasing the margin during adversarial training can effectively
improve model’s robustness against attacks under current intensity . Therefore, we hypothesize that
enlarging the adversarial margin when generating adversarial examples during training specifically
for the class i can improve this class’s robustness and reduce the large boundary error Rbdy(f, i). In
this work, we refer to this strategy as “Re-margin”. Empirical study in Table 2 and Figure 3 validates
its effectiveness.
We present the main components and process of Fair Robust Learning (FRL) in Algorithm 1. Note
that BEST(f, φ, ) denotes the adversarial training process under adjusted hyper-parameter φ,
mentioned in Eq. (14) and we test the performance under the validation set which is denoted as
EVAL(f, ∙). In Algorithm 1, in each iteration We first test our initialized or pretrained model f on
the validation set to check whether it violates the unfairness constraints (step 5). Then we update
Lagrangian multiplier φnat to reWeight the clean loss for each class. We propose three strategies
to update hyper-parameter to balance the boundary loss including ReWeight (option 1), Remargin
(option 2) and ReWeight+Regmargin (option 3). We folloW one of the options (step 7) for boundary
loss. Finally We adversarially train the model under the updated setting by φ, .
Algorithm 1 The Fair Robust Learning (FRL) Algorithm
1:	Input: Fairness constraints specified by τι > 0 and τ2 > 0, test time attacking radius e° and
hyper-param update rate α1 , α2
2:	Output: Fairly robust neural netWork f
3:	Randomly initialize netWork f or initialize netWork With pre-trained configuration
Set φinat = 0, φibdy = 0 and φ = (φnat, φbdy), adv. training radius i = 0 for each i ∈ Y
4:	repeat
5:	Rnat(i, f), Rbdy(i, f) = EVAL(f, 0)	B Evaluate f for each class
6:	φnat	=	φnat	+ αι	∙	(Rnat(i,	f) -	丁。	B Update multiplier φ∏at
7:	Φbdy =	Φbdy + α	∙	(Rbdy(i,	f)	-	T2)	B Option 1. Update multiplier	φbdy
7:	or Φbdy	=	Φbdy; G = ei ∙ exp(α2 ∙	(Rbdy(i,f)	- τ2))	B Option 2. Remargin
7:	or Φbdy	=	Φbdy + α ∙ (Rbdy(i, f)	- T)：
Ei = ∈i ∙ exp(α2(Rbdy(i, f) - τ2))	B Option 3. Reweight + Remargin
8:	f J BEST(f, φ, E)	B Adv. training under hyper-param φ, E from current f
9:	Until Model f SatiSfieS all COnStraintS
5	Experiment
In this section, we will present the experimental results to validate the effectiveness of the proposed
framework (FRL) for building fairly robust DNN models. We implement and compare our proposed
three strategies (i.e., Reweight, Remargin and Reweight+Remargin) on real-world data and discuss
their possible different consequences. We also discuss the main difference of the manner of our
proposed three potential debiasing strategies.
Experimental Setup & Baselines We conduct our experiments on CIFAR10 (Krizhevsky et al.,
2009). For CIFAR10, we present our main results under the model architecture PreAct Residual
Network (He et al., 2016). As comparison sets to show our method can improve fairness, we also
present the original performance from two popular adversarial training algorithms (Madry et al.,
2017; Zhang et al., 2019b). Meanwhile we add a baseline debiasing method which are inherited
from (Agarwal et al., 2018) (we directly apply it to reweight the cost of adversarial examples during
7
Under review as a conference paper at ICLR 2021
adversarial training) as a representative to show traditional debiasing methods might not be eas-
ily applied to solve unfairness issues in adversarial setting. For CIFAR10 dataset, we mainly test
our method’s performance on a ResNet18 (He & Garcia, 2009)) model, and we apply the PGD at-
tack (Madry et al., 2017) algorithm to generate adversarial examples under l∞-norm under 8/255.
For each of the debiasing algorithm, we set the fairness constraints τ1 and τ2 be 5% and 7% respec-
tively, for clean and boundary errors. Please refer to this link3 for our empirical implementations.
Debiasing Performance (CIFAR10) We first check whether our proposed FRL framework can
help resolve the unfairness issue in adversarial training. Refer to our goal in Eq. (13) to achieve the
fairness constraints to get both balanced clean and robustness performance. We report the trained
model’s average clean error rate, boundary error rate and robust error rate (defined in Eq.(16)), as
well as the worst intra-class clean, boundary and robust error rates (Table 1). Thus, for an optimal
equally robust model, we hope each of these worst intra-class errors is not too large compared to
the average errors. Meanwhile, it is also necessary that one debiasing strategy should not have too
much sacrifice on the model’s overall clean and robustness performance.
Table 1: Average & worst-class clean error, boundary error and robust error for various algorithms.
	Avg. Clean	Worst Clean	Avg. Bdy.	Worst Bdy.	Avg. Rob.	Worst Rob.
PGD Adv. Training	17.3 二	40.9	=	39.4	54.4	56.9	82.6
TRADES(β = 1)	14.4	27.9	43.6	62.6	58.0	83.6
TRADES(β = 2)	16.9	34.9	39.1	56.6	55.5	82.2
Baseline Reweight	192	28.3	392	53.7	582	80.1
FRL(Reweight)	17.0	22.5	41.6	51.2	58.6	73.3
FRL(Remargin)	16.9	24.9	41.6	50.6	58.5	76.3
FRL(Reweight+Remargin)	18.4	24.7	40.3	47.4	58.7	70.2
In Table 1 for CIFAR10 dataset, we present the performance of all three versions of our proposed
FRL framework. Compared to the baselines, all the FRL algorithms reduce the worst intra-class
clean, boundary and robust error under different degrees. FRL (Reweight) can get the best debiasing
performance to achieve minimal “worst-class” clean error, but it cannot debias boundary loss well.
The method (Reweight + Remargin) can be the most effective way to debias boundary error disparity
and robust error disparity.Our added baseline method (Baseline Reweight) (Agarwal et al., 2018)
only have minor help for clean performance fairness but cannot debias boundary error or robust
error. For each debiasing method of FRL, compared to vanilla PGD and TRADES, the total average
performance is only degraded by a slight margin (1 〜2% for clean error and 1 〜3% for robust
error), thus the debiasing method will not sacrifice too much total performance.
Debiasing Performance (GTSRB) We also apply our FRL debiasing method to GTSRB (Stal-
lkamp et al., 2011), which consists of images from 43 various traffic signs. Since this dataset is
highly unbalanced (some classes only have around 200 training images), we only keep 17 classes
when performing adversarial training. For this dataset, we apply a 4-layer CNN classifier and test
the robustness using PGD attack under l∞-norm by 12/255. In Table 2, we list the average and
worst-class clean, robust and boundary errors for different training algorithms. The results show
that the FRL method (Reweight, Reweight+Remargin) can effectively help to improve the model’s
worst-class clean and robust performance. Furthermore, since the original dataset is imbalanced,
the debiasing method brings in side-effect to improve the model’s overall performance, with lower
average clean error and robust error.
Table 2: Average & worst-class clean, boundary and robust error for various algorithms in GTSRB.
	Avg. Clean	Worst Clean	Avg. Bdy.	Worst Bdy.	Avg. Rob.	Worst Rob.
PGD Adv. Training	1.2 二	9.1	=	18.3	42.3	20.6	49.5
TRADES(β = 1)	1.6	7.3	21.5	50.4	23.1	59.7
TRADES(β = 2)	2.1	10.3	17.5	40.3	19.6	46.3
Baseline Reweight	08	64	19.4	42.4	20∏	44.0
FRL(Reweight)	0.7	3.5	18.3	39.4	19.0	40.0
FRL(Remargin)	2.4	10.7	18.0	44.3	18.4	44.3
FRL(Reweight+Remargin)	0.8	4.0	16.6	34.2	17.5	38.2
3https://drive.google.com/open?id=1IjE11VpV5CGXx633Nef3McgVOoRGMK9x
8
Under review as a conference paper at ICLR 2021
Compare Different Debiasing
Strategies. In Figure 3, we
take a closer look at the be-
havior of different proposed de-
biasing strategies mentioned in
Section 4.3 and test whether
they can succeed in solving the
constrained training problem in
Eq.(17). We present the model’s
maximum violation (e.g. v(i) =
Figure 3: Debiasing Manner for 3 FRL Options in CIFAR10.
Rnat(f, i) - Rnat(f, i) - τ1) among all classes for each training epoch that hot-started from a pre-
trained (adversarially) ResNet model. If v(i) ≤ 0 for each class in Y, each fairness constraint is
satisfied. From the figure we can tell that FRL (Reweight) method cannot adequately balance the
boundary error, and it always presents a trade-off relation with the clean error constraints. Introduc-
ing the Remargin method will facilitate to achieve fairness for boundary errors. More details (e.g.
average and worst clean / robustness in training) are in Fig. 5 in Appendix A.2.
6	Related Works
Adversarial Attacks and Adversarial Training. The existence of adversarial attacks (Goodfel-
low et al., 2014; Szegedy et al., 2013; Carlini & Wagner, 2017) causes huge concerns when people
adopt machine learning models in various application domains (Xu et al., 2019; Jin et al., 2020).
As countermeasures against adversarial examples, adversarial training (robust optimization) algo-
rithms (Goodfellow et al., 2014; Madry et al., 2017; Zhang et al., 2019b; Shafahi et al., 2019; Zhang
et al., 2019a) are formulated as a min-max problem that directly minimize the model’s risk on the ad-
versarial samples such that the machine learning model is robust under the adversarial attacks. They
are shown to be one of the most reliable strategies to improve model safety. Another mainstream
of defense methods are certified defense, which aims to provide provably robust DNNs under lp
norm bound (Wong & Kolter, 2018; Cohen et al., 2019) and guarantee the robustness. In this work,
we focus on studying the potential risk of the defense algorithms from a new scope of the fairness
concerns.
Fairness in Machine Learning & Imbalanced Dataset. Fairness issues recently draw much atten-
tion from the community of machine learning. These issues can generally divided into two catego-
rizations: (1) prediction outcome disparity: the models tend to have some unreasonable preference
of prediction for some specific demographic groups (Zafar et al., 2017); and (2) prediction quality
disparity: the models tend to have much lower performance on some specific groups than oth-
ers (Buolamwini & Gebru, 2018). Please refer to the works (Barocas et al.; Mehrabi et al., 2019)
for a more comprehensive and detailed summary of fairness study in machine learning. The reasons
that cause these discrimination problems might come from data distribution or the learning algo-
rithms. Unlike existing works, this work is the first to study the unfairness issue in the adversarial
setting. We argue that robustly trained models are likely to have different accuracy and robustness
quality among different classes, and it may be introduced by both data distribution and the adversar-
ial training algorithm. We also mention imbalanced data learning problem (He & Garcia, 2009; Lin
et al., 2017) as one related topic of our work. Since in our work, (e.g. Figure 1), we show that the
prediction performance differences are indeed existing between different classes. This phenomenon
is also well-studied in imbalanced data problems or long-tail distribution learning problems (Wang
et al., 2017) where some classes have much fewer training samples than others. However, in our
case, we show that this unfairness problem can also happen in the balanced data, so it desires new
scopes and methods for a further study.
7	Conclusion
In this work we first theoretically and empirically uncover one side-effect of adversarial training
algorithms: it can cause serious disparity for both clean accuracy and adversarial robustness between
different classes of the data. As a first attempt to resolve unfairness issues from adversarial training,
we propose the Fair Robust Learning (FRL) framework to mitigate this issue.
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudk John Langford, and Hanna Wallach. A reduc-
tions approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.
Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning.
9
Under review as a conference paper at ICLR 2021
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
Cial gender classification. In Conference onfairness, accountability and transparency, pp. 77-91,
2018.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized
smoothing. arXiv preprint arXiv:1902.02918, 2019.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin adversar-
ial (mma) training: Direct input space margin maximization through adversarial training. arXiv
preprint arXiv:1812.02637, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge
and data engineering, 21(9):1263-1284, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Wei Jin, Yaxin Li, Han Xu, Yiqi Wang, and Jiliang Tang. Adversarial attacks and defenses on
graphs: A review and empirical study. arXiv preprint arXiv:2003.00653, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
TsUng-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense
object detection. In Proceedings of the IEEE international conference on computer vision, pp.
2980-2988, 2017.
Aleksander Madry, Aleksandar Makelov, LUdwig Schmidt, Dimitris Tsipras, and Adrian VladU.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Ninareh Mehrabi, Fred Morstatter, NripsUta Saxena, Kristina Lerman, and Aram Galstyan. A sUrvey
on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.
Nir MorgUlis, Alexander Kreines, Shachar Mendelowitz, and YUval Weisglass. Fooling a real car
with adversarial traffic signs. arXiv preprint arXiv:1907.00374, 2019.
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng XU, John Dickerson, Christoph
StUder, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In
Advances in Neural Information Processing Systems, pp. 3358-3369, 2019.
Mahmood Sharif, SrUti BhagavatUla, LUjo BaUer, and Michael K Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 acm
sigsac conference on computer and communications security, pp. 1528-1540, 2016.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Traffic Sign
Recognition Benchmark: A mUlti-class classification competition. In IEEE International Joint
Conference on Neural Networks, pp. 1453-1460, 2011.
Christian Szegedy, Wojciech Zaremba, Ilya SUtskever, Joan BrUna, DUmitrU Erhan, Ian Goodfellow,
and Rob FergUs. IntrigUing properties of neUral networks. arXiv preprint arXiv:1312.6199, 2013.
Dimitris Tsipras, Shibani SantUrkar, Logan Engstrom, Alexander TUrner, and Aleksander Madry.
RobUstness may be at odds with accUracy. arXiv preprint arXiv:1805.12152, 2018.
YU-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Advances in
Neural Information Processing Systems, pp. 7029-7039, 2017.
10
Under review as a conference paper at ICLR 2021
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Han Xu, Yao Ma, Haochen Liu, Debayan Deb, Hui Liu, Jiliang Tang, and Anil Jain. Adversarial
attacks and defenses in images, graphs and text: A review. arXiv preprint arXiv:1909.08072,
2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171-
1180, 2017.
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate
once: Painless adversarial training using maximal principle. arXiv preprint arXiv:1905.00877, 2
(3), 2019a.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I
Jordan. Theoretically principled trade-off between robustness and accuracy. arXiv preprint
arXiv:1901.08573, 2019b.
11
Under review as a conference paper at ICLR 2021
A Appendix.
A.1 Overall Performance on More Models
Table 3: Adversarial training algorithms on CIFAR10 dataset (ResNet18 (above) and
ReNet34(below)). We report the average clean accuracy and adversarial accuracy (under PGD attack
by 8/255), as well as the worst / best clean accuracy and adv. accuracy among all classes.
	Avg. Clean.	Avg. Adv.	Worst Clean	Best Clean	Worst Adv.	Best Adv.
Natural Training	93.1	0.0	87.5	=	97.7	0.0	0.0
PGD Adv. Training	82.7	43.1	59.1	93.6	17.4	64.9
TRADES	83.1	44.0	65.1	93.4	17.8	67.0
Natural Training	95.1	0.0	88.1	=	98.2	0.0	0.0
PGD Adv. Training	86.6	46.3	72.3	96.4	19.8	72.2
TRADES	85.5	56.3	67.0	95.2	27.5	79.5
A.2 The Phenomenon of Unfair Robustness on GTSRB
GTSRB We also show the similar unfairness
phenomenon in German Traffic Sign Recog-
nition Benchmark (GTRSB) (Stallkamp et al.,
2011), which consist of 43 classes of images
from different traffic signs, with image sizes
32 × 32 × 3. In this dataset we also both nat-
urally and adversarially train a 3-Layer CNN
classifier. We list the model’s performance and
sort the classes in the order of decreasing clean
accuracy and adv. accuracy. From the Figure
4, we also see the natural training has similar
clean accuracy between different classes, but
adversarial training will enlarge their gap and
Figure 4: Unfairness in GTSRB
result the clean performance has a heavier tail property. In this dataset, both natural model and ro-
bust model have clear distinguished adversairal accuracy (robustness). However, adversarial training
even hardly provide any robustness improvement for some classes.
	Avg. Clean.	Avg. Adv.	Worst Clean	Best Clean	Worst Adv.	Best Adv.
Natural Training	99.5	=	18.9	91.7	=	100.0	0.0	二	72.5
PGD Adv. Training	94.5	44.4	50.0	100.0	1.3	90.0
TRADES	91.2	47.2	35.3	100.0	3.3	92.0
Table 4: Adversarial training algorithms on GTSRB dataset (on a 3-layer CNN model). We report
the average clean accuracy and adversarial accuracy (under PGD attack by 12/255), as well as the
worst / best clean accuracy and adv. accuracy among all classes.
B Proof of theorems
B.1 Proof of Theorem 1
Before proving Theorem 1, we first establish the optimal linear classifier in natural training through
Lemma 1, which facilitates us to prove the clean error and robust error after natural training.
Lemma 1 (Optimal linear classifier in natural training) For the data following the distribution
in Eq. (1), the naturally trained linear classifier f(x) = sign(wTx + b) has the optimal weight that
satisfy： w1 = w2 = ∙ ∙ ∙ = Wm and wm+1 = wm+2 = ∙ ∙ ∙ = wm+b. Moreover, their ratio satisfy
w1 : wm+1 = γ : η and b : w1 = w0 : 1 where
w0
=A2 K2 + 1
=A K2 - 1
A2 K ((K2 - 1)2
2σ2 log K
+ A2 (K2 - 1).
—
12
Under review as a conference paper at ICLR 2021
(c) Violation
(a) Adv. Acc
(b) Clean Acc
(d) Adv. Acc
(e) Clean Acc
(f) Violation
(g) Adv. Acc
(h) Clean Acc
(i) Violation
Figure 5: Debiasing training process
Proof 1 (Proof of Lemma 1) Wefirst prove wι = w2 = •一=Wm and Wm+ι = Wm+2 = •一=
wm+b by contradiction. We define G1 = {1, 2, . . . , m} and G2 = {m + 1, m + 2, . . . , m + d}. We
make the following assumption: for the optimal w and b, we assume there exist wi < wj for i 6= j
and i, j ∈ G1 . Then we obtain the following clean error for two classes with this classifier w
R(f, -1) = Pr{	X	wkNk	+ b + wiN(-γ,σ-2 1)	+ wjN (-γ, σ-2 1) >	0}	(18)
k6=i,k6=j
R(f, +1) = Pr{	X	wkNk	+ b + wiN(+γ,σ+2 1)	+ wjN (+γ, σ+2 1) <	0}	(19)
k6=i,k6=j
If we use wj to replace wi, we obtain new errors
R(f,-1)= Pr{	X	WkNk	+ b +	WjN(-γ,σ-ι)+ WjN(-γ, σ-ι)	> 0}	(20)
k6=i,k6=j
R(f, +1) = Pr{	X	WkNk	+ b +	WjN(+γ,σ+ι) + WjN(+γ,σ+ι)	< 0}.	(21)
k6=i,k6=j
It implies R(f,-1) + R(f, +1) < R(f,-1) + R(f, +1).
Therefore, it contradicts with the assumption we make and we conclude for an optimal linear clas-
Sifier in natural training, it must satisfies Wi = W2 = •一 =Wm. The same argument applies to
i,j ∈ G2 and we conclude Wm+i = Wm+2 = •一=Wm+d similarly.
13
Under review as a conference paper at ICLR 2021
Next, we calculate the ratio between wɪ, wm+ι.
R(f) = Pr {f (x) = y]	(22)
=Pr{f (x) = 1|y = -1} ∙ Pr{y = -1} + Pr{f (x) = -1|y = 1} ∙ Pr{y = +1}
Y Pr{wτX + b > 0|y = —1} + Pr{wτX + b < 0|y = —1}
=Pr{ X WiNi-Y σ-ι) + X WjN(-η, σ2ι) + b > 0|y = -1}
i∈Gι	j∈G2
+ Pr{£wiN(+Y,%)+£ WjN(+η,σ+ι)+ b < 0|y = +1}
i∈Gι	j∈G2
Pr{N(0,1) <
b — mwιγ — dwm+ι η
JmW2 + dwm+1σ
X--------------
^z''∙^"^~
Z(T)
} + Pr{N(0,1) < -b - mwιγ - dwm+ιη
m mw1 +dwm+ι σK
X--------------------/
^^{^^™
Z(+1)
.}
(23)
,
We derive the optimal w1 and wm+1 by taking dRf = 0 and ∂W(Jl = 0:
∂R(f)
∂wι
iexp ( - 1(Z(-1))2)∙ " +
√2π	V 2	∂	∂wι
1	1	1/7/ ι ∖∖2' ∂Z(+1)
萍exp (- 2 (Z(+1)) )∙ -1WΓ
0
∂R(f)=
∂Wm+ι
These imply
1	1	1 s 八 6 ∂Z (-1)
萍exp (- 2(Z(T)) )∙ aWm+1
+ ^exp (- 1(Z(+1))2)∙筌生
√2π	'2	∂ ∂wm+ι
∂Z (-1)/∂Z (+1)
∂W1	∂W1
∂Z (-1)/∂Z (+1)
∂Wm+1 ∂Wm+1
0
and therefore
-YdWm+ι — bwι + dηw1wm+1
-YdWm+1 + bw1 + dηw1Wm+1
Simplifying it gives wɪ : Wm+1 = η : γ.
-ηmw12 - bw2 + mYw1 wm+1
-ηmw2 + bwm+ι + mγw1Wm+1'
(24)
Then, we calculate the ratio between wɪ and b. Based on the relation between wɪ and wm+ι, we let
wι = YW and wm+ι = ηw for some constant w. Substitute wɪ and wm+ι into Eq. (23), we have
R(f,-1)= Pr {N (0,1)
mγ2w + dη2w
Pr{N (0, 1)
σw,mY2 + dη2
m mγ2 + dη2
"Ξ },
(25)
(26)
R(f, +1)= Pr {N (0,1)
-b
m mγ2 + dη2
σwK,wγ2 + dη2
Κσ
}.
(27)
For simplicity, we denote A = 'mγ2 + dη2 and assume Wo
∂[R(∕,-1) + R(∕, + 1)]
W. We will find optimal b by letting
∂wq
0. In detail, it is
<
<
<
b
—
—
σ
+
}
---1-= exp(-1(w0 — —)2)--------1  exp(-1(-w0--------ʌ)2) = 0	(28)
Aσ√2π	2 Aσ σ	KAσ√2π	2 KAσ Kσ
which gives 1 (^ɪ + K⅛)2 + 2(器一A)2) = 2 log K and therefore we obtain
_ a2 K2 + 1 M S4 ɪ 2σ2 log K
W0 = A K2-I - AKy (K2 - 1)2 + A2(K2 - 1).
14
Under review as a conference paper at ICLR 2021
Proof 2 (Proof of Theorem 1) Substitute the optimal linear model {w, b} in Eq. (23), we have
clean errors
where
Rnat(f,-1) = Pr.{N (0, 1) ≤Znat(f,-1)},
Rnat(f, +1) = Pr.{N (0, 1) ≤ Znat(f, +1)},
Znat(f,-1)}=
Znat(f,+1)}=
2A
(K2 - 1)σ
4^^4A2	2log K
-Ky (κ 2—1)2+ (K2 -1),
-2KA
(K2 - 1)σ
4	4A2
(K (K2 - 1)2σ2
l 2log K
+ (K2 - 1).
(29)
(30)
(31)
(32)
+
Substitute the optimal linear model {w, b} into robust error in Eq. (35) and (36), we have robust
errors:
Rrob(f, -1)= Pr.{N(0,1) ≤ Znat(f, -1) + 7 j	,空},	(33)
mγ 2 + dη2 σ
Rrob(f, +1)= Pr{N(0,1) ≤ Znat(f, +1) + 丁 + ^ ,黑}.	(34)
mγ2 + d2 Kσ
B.2 Proof of Theorem 2
Before proving Theorem 2, we first establish the optimal linear classifier in adversarial training
through Lemma 2, which facilitates us to prove the clean error and robust error after adversarial
training.
Lemma 2 (Optimal linear model in adversarial training) For the data distribution assumed in
Eq. (1), the adversarially trained linear classifier f(x) = sign(wT x + b) has the optimal weight
that satisfy: wι = w2 = •…=Wm, wm+ι = Wm+2 = •…=wm+b = 0 and b : wι = w0 : 1 where
2	、2 K2 + 1	/	、2n I―4—7	2σ2 log K一
w0 = m3-e) K2-1 - m(γ-e) KX (K2-Ip + -e)2(K 2- 1).
Proof 3 (Proof of Lemma 2)
Rrob(f,-1) = Pr.{∃ kδk ≤,f(x+δ) >0|y= -1}
= Pr.{ max f(x + δ) > 0 | y = -1}
kδk≤
m	m+d
= Pr.{ max XWi(N(-γ, σ-2 1) + δi) + X Wi(N(-η, σ-2 1) + δi) + b > 0}.
kδk≤ i=1	i=m+1
(35)
Similarly, we have
Rrob(f,+1) = Pr.{∃ kδk ≤,f(x+δ) <0|y=+1}
= Pr.{ max f(x + δ) > |y = +1}
kδk≤
Pr.{ max
kδk≤
m+d
Wi(N(γ,σ+2 1) + δi) + X Wi (N(+η, σ+2 1) + δi) + b < 0}.
i=m+1
(36)
15
Under review as a conference paper at ICLR 2021
To prove the theorem by contradiction, we first assume that for the optimal w, there exist some
wi > 0 where i ∈ G2 = {m + 1, m + 2, . . . , m + d}. Then
Rrob(f, -1)
Pr.{
j6=i
max wj(N(θj, σ-2 1) + δj) + b+ max wi(N (-η, σ-2 1) + δi) > 0}. (37)
∣δj∣≤e	∣δi∣≤e
{z
A
{z
B
|
}
Because wi > 0, B is maximized when δ = . We obtain
Rrob(f, -1) = Pr.{A + wiN(-η + , σ-2 1) > 0} ≥ Pr.{A > 0}	(38)
So wi = 0 gives a better robust error. We can also assume wi < 0 and use similar contradiction
to prove wi = 0. Similar argument holds for Rrob(f, +1). Therefore, we arrive at the conclusion
wi = 0 for all m + 1 ≤ i ≤ m + d. The calculation of w0 is similar to the proof of Lemma 1 and
we omit the proof here.
Proof 4 (Proof of Theorem 2) Substitute the optimal linear model in Lemma 2 into Eq. (23), we
obtain the clean error on the adversarially trained model:
Rnat(f, -1) = Pr.{N (0, 1) <
2√m(γ - E)
(K2 - 1)σ
K I 4m(γ — E)2	2log K,
V (K2 - 1)2σ2 + K2 - 1 },
_ - /
{z
Znat(fadv,-1)
(39)
Rnat(f, +1) = Pr{N(0,1) < - -M(Y - ')
(K2 - 1)σ
K 4m(Y - e)2
+ V (K2 - 1)2σ2
2log K
+ K2 -1).
(40)
Znat (fadv ,+1)
Substitute the optimal linear model in Lemma 2 into Eq. (35) and (36), we obtain the robust error
on the adversarially trained model:
R rob (fadv, -1) = Pr{N (O, I) ≤ Znat(fadv, -1) + √m, },	(41)
σ
R rob (fadv, +1) = Pr {N (0, 1) ≤ Znat(fadv, +1) + √m~F7~ }∙	(42)
Kσ
B.3 Proof of Corollary 1
Proof 5 (Proof of Corollary 1) We try to show R0(f, +1) - R0(f, -1) > R(f, +1) - R(f, -1).
Z0(+1) R0(f, +1) - R0(f, -1) =	g(x)dx Z0(-1) Z(+1) R(f, +1) - R(f, -1) =	g(x)dx = Z(-1)	= g(E0) (Z0(+1) - Z0(-1))	(43) l{zjl	{	' CA g(E) (Z(+1) - Z(-1))	(44) l{zjlZ DB
where Z0(-1) ≤ E0 ≤ Z0(+1) and Z (-1) ≤ E ≤ Z (+1) according to Mean Value Theorem. Here
g(∙) is the density function of standard normal distribution.
Next, we try to show: RR(f，+1)-R(f 二)) = BAC > 1. Remind that
_ 2√m + dΘ(γ)	/ 4mΘ(γ2)	∣ 2log K
(- ) = σ(K2 - 1) y (K2 - 1)2σ2 + K2 - 1
= //(Y) (√m + d - K pm + d + q(K))
(K2 - 1)σ
H (√m + d - K pm + d + q(K)),
(45)
(46)
(47)
16
Under review as a conference paper at ICLR 2021
where q(K )=，”之斗)。g K
and similarly we have
Z(+1) z (-K,m + d + ʌ/m + d + q(K)),	(48)
Z0(-1) z (√m - KPm + q(K)),	(49)
Z0(+1) z (-K√m + Pm + q(K)).	(50)
Therefore, we derive
A _ Z0(+1) - Z0(-1)
——:—：--------:—：—
B Z(+1) - Z(-1)
(-K√m + Pm + q(K)) - (√m - KPm + q(K))
Z
(-K√m + VZm + d + q(K)) - (√m - KVZm + d + q(K))
_	，m + q(K) - √m
ʌ/m + d + q(K) — ʌ/m + d
ʌ/m + d + q(K) + ʌ/m + d
√m + q(K) + √m
= Θ(( -)2)
m
and
C g(C
一=——
D g(e)
where Z0(-1) ≤ e' ≤ Z0(+1) and Z(-1) ≤ C ≤ Z(+1)
Finally,
AB≥ q(K) ∙ Θ(( -) 2)
CD	m
(51)
(52)
(53)
(54)
(55)
(56)
(57)
17