Under review as a conference paper at ICLR 2021
Lipschitz-Bounded Equilibrium Networks
Anonymous authors
Paper under double-blind review
Ab stract
This paper introduces new parameterizations of equilibrium neural networks, i.e.
networks defined by implicit equations. This model class includes standard mul-
tilayer and residual networks as special cases. The new parameterization admits
a Lipschitz bound during training via unconstrained optimization: no projections
or barrier functions are required. Lipschitz bounds are a common proxy for ro-
bustness and appear in many generalization bounds. Furthermore, compared to
previous works we show well-posedness (existence of solutions) under less re-
strictive conditions on the network weights and more natural assumptions on the
activation functions: that they are monotone and slope restricted. These results
are proved by establishing novel connections with convex optimization, operator
splitting on non-Euclidean spaces, and contracting neural ODEs. In image clas-
sification experiments we show that the Lipschitz bounds are very accurate and
improve robustness to adversarial attacks.
1	Introduction
Deep neural network models have revolutionized the field of machine learning: their accuracy on
practical tasks such as image classification and their scalability have led to an enormous volume of
research on different model structures and their properties (LeCun et al., 2015). In particular, deep
residual networks with skip connections He et al. (2016) have had a major impact, and neural ODEs
have been proposed as an analog with “implicit depth” (Chen et al., 2018). Recently, a new structure
has gained interest: equilibrium networks (Bai et al., 2019; Winston & Kolter, 2020), a.k.a. implicit
deep learning models (El Ghaoui et al., 2019), in which model outputs are defined by implicit
equations incorporating neural networks. This model class is very flexible: it is easy to show that
includes many previous structures as special cases, including standard multi-layer networks, residual
networks, and (in a certain sense) neural ODEs.
However model flexibility in machine learning is always in tension with model regularity or robust-
ness. While deep learning models have exhibited impressive generalisation performance in many
contexts it has also been observed that they can be very brittle, especially when targeted with ad-
versarial attacks (Szegedy et al., 2014). In response to this, there has been a major research effort
to understand and certify robustness properties of deep neural networks, e.g. Raghunathan et al.
(2018a); Tjeng et al. (2018); Liu et al. (2019); Cohen et al. (2019) and many others. Global Lip-
schitz bounds (a.k.a. incremental gain bounds) provide a somewhat crude but nevertheless highly
useful proxy for robustness (Tsuzuku et al., 2018; Fazlyab et al., 2019), and appear in several anal-
yses of generalization (e.g. (Bartlett et al., 2017; Zhou & Schoellig, 2019)).
Inspired by both of these lines of research, in this paper we propose new parameterizations of equi-
librium networks with guaranteed Lipschitz bounds. We build directly on the monotone operator
framework of Winston & Kolter (2020) and the work of Fazlyab et al. (2019) on Lipschitz bounds.
The main contribution of our paper is the ability to enforce tight bounds on the Lipschitz constant
of an equilibrium network during training with essentially no extra computational effort. In addi-
tion, we prove existence of solutions with less restrictive conditions on the weight matrix and more
natural assumptions on the activation functions via novel connections to convex optimization and
contracting dynamical systems. Finally, we show via small-scale image classification experiments
that the proposed parameterizations can provide significant improvement in robustness to adversar-
ial attacks with little degradation in nominal accuracy. Furthermore, we observe small gaps between
certified Lipschitz upper bounds and observed lower bounds computed via adversarial attack.
1
Under review as a conference paper at ICLR 2021
2	Related work
Equilibrium networks, Implicit Deep Models, and Well-Posedness. As mentioned above, it
has been recently shown that many existing network architectures can be incorporated into a flexible
model set called an equilibrium network (Bai et al., 2019; Winston & Kolter, 2020) or implicit deep
model (El Ghaoui et al., 2019). In this unified model set, the network predictions are made not by
forward computation of sequential hidden layers, but by finding a solution to an implicit equation
involving a single layer of all hidden units. One major question for this type of networks is its well-
posedness, i.e. the existence and uniqueness of a solution to the implicit equation for all possible
inputs. El Ghaoui et al. (2019) proposed a computationally verifiable but conservative condition on
the spectral norm of hidden unit weight. In Winston & Kolter (2020), a less conservative condition
was developed based on monotone operator theory. Similar monotonicity constraints were previ-
ously used to ensure well-posedness of a different class of implicit models in the context of nonlin-
ear system identification (Tobenkin et al., 2017, Theorem 1). On the question of well-posedness, our
contribution is a more flexible model set and more natural assumptions on the activation functions:
that they are monotone and slope-restricted.
Neural Network Robustness and Lipschitz Bounds. The Lipschitz constant of a function mea-
sures the worst-case sensitivity of the function, i.e. the maximum “amplification” of difference in
inputs to differences in outputs. The key features of a good Lipschitz bounded learning approach in-
clude a tight estimation for Lipschitz constant and a computationally tractable training method with
bounds enforced. For deep networks, Tsuzuku et al. (2018) proposed a computationally efficient
but conservative approach since its Lipschitz constant estimation method is based on composition of
estimates for different layers, while Anil et al. (2019) proposed a combination of a novel activation
function and weight constraints. For equilibrium networks, El Ghaoui et al. (2019) proposed an esti-
mation of Lipschitz bounds via input-to-state (ISS) stability analysis. Fazlyab et al. (2019) estimates
for deep networks based on incremental quadratic constraints and semidefinite programming (SDP)
were shown to give state-of-the-art results, however this was limited to analysis of an already-trained
network. The SDP test was incorporated into training via the alternating direction method of multi-
pliers (ADMM) in Pauli et al. (2020), however due to the complexity of the SDP the training times
recorded were almost 50 times longer than for unconstrained networks. Our approach uses a similar
condition to Fazlyab et al. (2019) applied to equilibrium networks, however we introduce a novel
direct parameterization method that enables learning robust models via unconstrained optimization,
removing the need for computationally-expensive projections or barrier terms.
3	Problem Formulation and Preliminaries
3.1	Problem statement
We consider the weight-tied network in which x ∈ Rd denotes the input, and z ∈ Rn denotes the
hidden units, y ∈ Rp denotes the output, given by the following implicit equation
z = σ(Wz + Ux + bz), y = Woz + by	(1)
where W ∈ Rn×n, U ∈ Rn×d, and Wo ∈ Rp×n are the hidden unit, input, and output weights,
respectively, bz ∈ Rn and by ∈ Rp are bias terms. The implicit framework includes most current
neural network architectures (e.g. deep and residual networks) as special cases. To streamline the
presentation we assume that σ : R → R is a single nonlinearity applied elementwise, although our
results also apply in the case that each channel has a different activation function, nonlinear or linear.
Equation (1) is called an equilibrium network since its solutions are equilibrium points of the differ-
ence equation zk+1 = σ(Wzk + Ux + bz) or the ODE Z(t) = —z(t) + σ(Wz(t) + Ux + bz). Our
goal is to learn equilibrium networks (1) possessing the following two properties:
•	Well-posedness: For every input x and bias bz, equation 1 admits a unique solution z.
•	γ-Lipschitz: It has a finite Lipschitz bound of γ, i.e., for any input-output pairs (x1, y1),
(x2, y2) we have ky1 — y2k2 ≤ γkx1 — x2k2.
2
Under review as a conference paper at ICLR 2021
3.2	Preliminaries
Monotone operator theory. The theory of monotone operators on Euclidean space (see the survey
Ryu & Boyd (2016)) has been extensively applied in the development of equilibrium networks
(Winston & Kolter, 2020). In this paper, we will use the monotone operator theory on non-Euclidean
spaces (Bauschke et al., 2011), in particular, we are interested in a finite-dimensional Hilbert space
H, which we identify with Rn equipped with a weighted inner product hx, yiQ := y>Qx where
Q 0. The main benefit is that we can construct a more expressive equilibrium network set. A
brief summary or relevant theory can be found in Appendix C.1; here we give some definitions
that are frequently used throughout the paper. An operator is a set-valued or single-valued function
defined by a subset of the space A ⊆ H × H. A function f : H → R ∪ {∞} is proper if f(x) < ∞
for at least one x. The subdifferential and proximal operators of a proper function f are defined as
∂f (x) := {g ∈ H | f(y) ≥ f(x) + hy - x, giQ, ∀y ∈ H},
ProXa(X) := {z ∈ H | Z = arg min Jku — XkQ + αf (u)}
u2
u
respectively, where IIXkQ :=，(x, XiQ is the induced norm. For n = 1, we only consider the case
of Q = 1. An operator A is monotone if hu —v, X — yiQ ≥ 0 and strongly monotone with parameter
m if hu — v, X — yiQ ≥ mkX — yk2Q for all (X, u), (y, v) ∈ A. The operator splitting problem is that
of finding a zero of a sum of two operators A and B, i.e. find an X such that 0 ∈ (A + B)(X).
Dynamical systems theory. In this paper, we will also treat the solutions of (1) as equilibrium
points of certain dynamical systems Z(t) = f (z(t)). Then, the Well-PoSedneSS and robustness
properties of (1) can be guaranteed by corresponding properties of the dynamical system’s solution
set. A central focus in robust and nonlinear control theory for more than 50 years - and largely
unified by the modern theory of integral quadratic constraints (Megretski & Rantzer, 1997) - has
been on systems which are interconnections of linear mappings and “simple” nonlinearities, i.e.
those easily bounded in some sense by quadratic functions. Fortuitously, this characteristic is shared
with deep, recurrent, and equilibrium neural networks, a connection that we use heavily in this
paper and has previously been exploited by Fazlyab et al. (2019); El Ghaoui et al. (2019); Revay
et al. (2020) and others. A particular property we are interested in is called contraction (Lohmiller
& Slotine, 1998), i.e., any pair of solutions z1(t) and z2(t) exponentially converge to each other:
kzι(t) — Z2(t)k ≤ αkzι(0) — Z2(0)ke-βt
for all t > 0 and some α, β > 0. Contraction can be established by finding a Riemannian metric
with respect to which nearby trajectories converge, which is a differential analog of a Lyapunov
function. A nice property of a contracting dynamical system is that if it is time-invariant, a unique
equilibrium exists and it possesses a certain level of robustness. Moreover, contraction can also be
linked to monotone operators, i.e. a system is contracting w.r.t. to a constant (state-independent)
metric Q if and only if the operator —f is strongly monotone w.r.t. Q-weighted inner product. We
collect some directly relevant results from systems theory in Appendix C.2. 4
4 Main Results
This section contains the main theoretical results of the paper: conditions implying well-posedness
and Lipschitz-boundedness of equilibrium networks, and direct (unconstrained) parameterizations
such that these conditions are automatically satisfied.
Assumption 1. The activation function σ is monotone and slope-restricted in [0, 1], i.e.,
0 ≤ σ3-σ ⑹ ≤ 1, ∀x,y ∈ R,x = y.
X—y
(2)
Remark 1. We will show below (Proposition 1 in Section 4.2) that Assumption 1 is equivalent to
the assumption on σ in Winston & Kolter (2020), i.e. that σ(∙) = ProXf (∙) for some proper convex
function f. However, the above assumption is arguably more natural, since it is easily verified for
standard activation functions. Note also that if different channels have different activation functions,
then we simply require that they all satisfy (2).
3
Under review as a conference paper at ICLR 2021
The following conditions are central to our results on well-posedness and Lipschitz bounds:
Condition 1.	There exists a Λ ∈ D+, with D+ denoting diagonal positive-definite matrices, such
that W satisfies
2Λ - ΛW - WTΛ	0.	(3)
Condition 2.	Given a prescribed Lipschitz bound γ > 0, there exists Λ ∈ D+ such that W, Wo , U
satisfy
2Λ — AW — WTA — 1WTWo — 1ΛUUTA A 0.	(4)
γγ
Remark 2. Note that Condition 2 implies Condition 1 since 1∕γ(WTWo + AUUTA)占 0. As a
partial converse, if Condition 1 holds, then for any Wo , U there exist a sufficiently large γ such that
Condition 2 is satisfied.
The main theoretical results of this paper are the following:
Theorem 1.	If Assumption 1 and Condition 1 hold, then the equilibrium network (1) is well-posed,
i.e. for all x and bz, equation (1) admits a unique solution z. Moreover, it has a finite Lipschitz
bound from x to y.
Theorem 2.	If Assumption 1 and Condition 2 hold, then the equilibrium network (1) is well-posed
and has a Lipschitz bound of γ.
As a consequence, we call (1) a Lipschitz bounded equilibrium network (LBEN) if its weights satisfy
either (3) or (4). The full proofs appear in Appendices E.1 and E.2, but here we sketch some of the
main ideas. We can represent (1) as an algebraic interconnection between linear and nonlinear parts:
v = Wz + Ux + bz, z = σ(v),	y = Woz + by.	(5)
It can be shown that for any pair of solutions to the nonlinear part za = σ(va), zb = σ(vb), if we
define ∆v = va - vb and ∆z = za - zb then Assumption 1 implies the following:
h∆v-∆z,∆ziΛ≥0.	(6)
for any A ∈ D+. This and Condition 1 can be used to prove global stability of a unique equilibrium
of the differential equation V = -v + Wσ(v) + Ux + bz, which proves there is a unique solution to
(1) for any inputs. Next, straightforward manipulations of Condition 2 show that any pairs of inputs
xa , xb and outputs ya , yb satisfy the following, where ∆x = xa - xb and ∆y = ya - yb :
Y k∆χk2 - γ∣∣∆y k2 ≥ 2h∆v - ∆z, ∆z iΛ ≥ 0,
where the inequality comes (6). This implies the Lipschitz bound k∆yk2 ≤ γk∆xk2.
Remark 3. In Fazlyab et al. (2019) it was claimed that (6) holds with a richer (more powerful)
class of multipliers A which were previously introduced for robust stability analysis of systems with
repeated nonlinearities (Chu & Glover, 1999; D’Amato et al., 2001; Kulkarni & Safonov, 2002).
However this is not true: a counterexample was given in Pauli et al. (2020), and here we provide a
brief explanation: even if the nonlinearities σ(vi) are repeated when considered as functions of vi,
their increments ∆zi = σ(vi + ∆vi) - σ(vi) are not repeated when considered as functions of ∆vi,
since they depend on the particular vi which generally differs between units.
Example 1. We illustrate the extra flexibility of Condition 1 compared to the condition of Winston
& Kolter (2020) by a toy example. Consider W ∈ R2×2 and take a slice near W = 0 of the form
W = 0	W12	, for which we have: 2I	- W - WT =	2	-W12	.	(7)
0	W22	-W12	2 - 2W22
By Sylvester’s criterion, this matrix is positive-definite if and only if W22 < 1 and det(2I - W -
WT) = 4(1 - W22) - W122 > 0, which defines a parabolic region in the W12, W22 plane.
Applying our condition (3), without loss of generality take A = diag(1, α) with α > 0 and we have
2A - AW - WTA
2
-W12
-W12
2α - 2αW22
4
Under review as a conference paper at ICLR 2021
Figure 1:	Valid coefficient ranges for Example 1.
Gray region: the condition from Winston & Kolter (2020) is fea-
sible: 2I - W - WT	0.
White region (including gray region): our well-posedness con-
dition is feasible: ∃Λ ∈ D+ : 2Λ - ΛW - WTΛ	0.
Black region: neither condition feasible.
The positivity test is now W22 < 1 and 4α(1 - W22) - W122 > 0. For each W12 there is sufficiently
large α such that the second condition is satisfied, since the first implies 1 - W22 > 0. Hence the
only constraint on W is that W22 < 1, which yields a much larger region in the W12 , W22 plane
(see Figure 1). Interestingly, in this simple example with ReLU activation, the condition W22 < 1 is
also a necessary condition for well-posedness (El Ghaoui et al., 2019, Theorem 2.8).
4.1	Direct Parameterization for Unconstrained Optimization
Training a network that satisfies Condition 1 or 2 can be formulated as an optimization problem with
convex constraints. In fact, Condition 1 is a linear matrix inequality (LMI) in the variables Λ and
ΛW, from which W can be determined uniquely. Similarly, via Schur complement, Condition 2 is
an LMI in the variables Λ, ΛW, ΛU, Wo , and γ, from which all network weights can be determined.
In a certain theoretical sense LMI constraints are tractable - NesteroV & NemiroVskii (1994) proved
they are polynomial-time solvable - however for even for moderate-scale networks (e.g. ≤ 100
actiVations) the associated barrier terms or projections become a major computational bottleneck.
In this paper, we propose direct parameterization that allows learning via unconstrained optimization
problem, i.e. all network parameters are transformations of free (unconstrained) matrix variables, in
such a way that LMI constraints (3) or (4) are automatically satisfied.
For well-posedness, i.e. Condition (1), we parameterize via the following free variables: V ∈ Rn×n,
d ∈ Rn, and skew-symmetric1 matrix S = -ST ∈ Rn×n, from which the hidden unit weight is
W =I-Ψ(VTV +I+S),	(8)
where Ψ = diag ed and > 0 is some small constant to ensure strict positive-definiteness. Then
it follows from straightforward manipulations that Condition 1 holds with Λ = Ψ-1 if and only if
W can be written as (8). When Ψ = I, this reduces to the parameterization of Winston & Kolter
(2020).
Similarly, for a specific Lipschitz bound, i.e. Condition 2, we add to the parameterization the free
input and output weights U and Wo , and arbitrary γ > 0. We can construct
W = I — Ψ ( ɪWTWo + ^Ψ-1UUTΨ-1 + VTV + eI + S) ,	(9)
2γ o	2γ
for which (4) is automatically satisfied. Again, it can easily be verified that this construction is
necessary and sufficient, i.e. any W satisfying (4) can be constructed via (9).
4.2	Monotone Operator Perspective
In this section, we will show that finding the solution to LBEN (1) is equivalent to solving a well-
posed operator splitting problem, and hence a unique solution exists. First, we need the following
observation on the activation function σ .
Proposition 1. Assumption 1 holds if and only if there exists a convex proper function f : R →
R ∪ {∞} such that σ(∙) = ProXf (∙).
1 Note that S can be parameterized via its upper or lower triangular components, or via S = N - NT with
N free, which can be more straightforward if W is defined implicitly via linear operators, e.g. convolutions.
5
Under review as a conference paper at ICLR 2021
The proof of Proposition 1 with a construction of f appears in Appendix E.3, along with a list of
f for popular σ . It is well-known in monotone operator theory (Ryu & Boyd, 2016) that for any
convex closed proper function f, the proximal operator proxf1 (x) is monotone and non-expansive
(i.e. slope-restricted in [0, 1]). Proposition 1 is a converse result for scalar functions.
Remark 4. To our knowledge Proposition 1 is novel, however for several popular activation func-
tions the corresponding functions f were computed in Li et al. (2019) (see also Table 3 in Ap-
pendix E.4). Compared with Li et al. (2019), our work gives a necessary and sufficient conditions.
Now we connect LBEN (1) to an operator splitting problem.
Proposition 2. Finding a solution of LBEN (1) is equivalent to solving the well-posed operator
splitting problem 0 ∈ (A + B)(z) with the operators
A(z) = (I-W)(z)-(Ux+bz),	B=∂f	(10)
where f(z) :=	in=1 λif(zi) with λi as the ith diagonal element of Λ.
The proof appears in Appendix E.4 and Theorem 1 follows directly since the above operator splitting
problem has a unique solution for any x, bz .
Computing an equilibrium. There exist various of operator splitting algorithms to compute the
solution of LBEN (1), e.g., ADMM (Boyd et al., 2011) and Peaceman-Rachford splitting (Kellogg,
1969). Winston & Kolter (2020) found that Peaceman-Rachford splitting converges very rapidly
when properly tuned, and our experience agrees with this.
Gradient backpropagation. As shown in (Winston & Kolter, 2020, Section 3.5), the gradients of
the loss function '(∙) can be represented by
∂'	∂'	-1 τ∂(Wz? + Ux + bz)
∂0= ∂Z7(I-JW) J 画
(11)
where z? denotes the solution of (1), (∙) denotes some learnable parameters in the parameterization
(8) or (9), and J ∈ Dσ(W z? + Ux + bz) with Dσ as the Clarke generalized Jacobian of σ. Since σ
is piecewise differentiable, then the set Dσ(W z? + Ux + bz) is a singleton almost everywhere. The
following proposition reveals that (11) is well-defined, see proof in Appendix E.5.
Proposition 3. The matrix I - JW is invertible for all z?, x and bz.
4.3	Connections to Convex Optimization
Since LBEN (1) is equivalent to an operator splitting problem, an interesting question is whether it
can further be connected to a convex optimization problem. Here we construct an equivalent convex
problem for the LBEN whose parameterization satisfies S = 0.
Proposition 4. If the direct parameterization (either (8) or (9)) of an LBEN satisfies S = 0, then
for all x and bz, the solution of (1) is the minimizer of the following strongly convex optimization
problem:
min ^1(I - W)z - Ux - bz,z) + f(z).	(12)
The proof is in Appendix E.6. Furthermore, for an important subclass of LBEN where σ is ReLU,
it has an equivalent convex quadratic programming (QP) formulation.
Proposition 5. Consider an LBEN (1) with ReLU activation. For all x and bz, the solution of (1) is
the minimizer of the following strongly convex QP problem:
min	^z>Hz + p>z s.t. Z ≥ 0, (I - W)z ≥ Ux + bz	(13)
z2
where H = 2Λ - ΛW - W>Λ andp = -Λ(U x + bz).
Note that the QP (13) also works for the case where S is non-zero. The proof (see Appendix E.7) is
built on the “key insights” of ReLU activation from Raghunathan et al. (2018b). This allows one to
compute the solution of LBEN (1) using the many free or commercial QP solvers.
6
Under review as a conference paper at ICLR 2021
4.4	Contracting Neural ODEs
In this section, we will prove the existence of a solution to (1) from a different perspective: by
showing it is the equilibrium of a contracting dynamical system (a “neural ODE”). We first add a
smooth state v(t) ∈ Rn to avoid the algebraic loop in (5). This idea has long been recognized
as helpful for well-posedness questions (Zames, 1964). We define the dynamics of v(t) by the
following ODE:
V(t) = —v(t) + Wz(t) + Ux + bz, z(t) = σ(v(t)).	(14)
The well-posedness of (1) is equivalent to the existence and uniqueness of an equilibrium of (14) for
all x and bz , which is established by the following proposition.
Proposition 6. If Assumption 1 and Condition 1 hold, then the neural ODE (14) is contracting w.r.t.
some constant metric P	0.
The proof is in Appendix E.8. Moreover, the metric P can be found via semidefinite programming.
The above proposition also proves that the nonlinear operator — f with f (v) = —v + Wσ(v) + Ux +
bz, zeros of which define solutions of LBEN (1), is actually monotone w.r.t. the P -weighted inner
product, which gives a first-order cutting-plane oracle for the zero location v? such that f(v?) = 0.
I.e. given a test point vt 6= v?, it proves that v? is in the half-space defined by hv? — vt, f (vt)iP > 0.
This may offer alternative ways to solve LBEN (1), e.g. via Nemirovski (2004); Nesterov (2007).
4.5	Feedforward Networks as a Special Case
Consider a multi-layer feedforward network of the form
zι = Uox + bo, Z'+ι = σ(W'Z' + b`), ' = 1,...,L — 1, y = WLzL + bL,	(15)
which can be rewritten as an equilibrium network (1) as shown in Appendix A The above equilibrium
network is obviously well-posed as a unique solution exists. The following proposition shows that
(44) is also an LBEN.
Proposition 7. The LBEN parameterization (8) contains all feedforward networks.
In Winston & Kolter (2020), a set of well-posed equilibrium network, called monotone operator
equilibrium network (MON), is introduced via the following parameterization
W = (1 —m)I—A>A+B> —B	(16)
where m > 0 is a hyper-parameter, A, B are learnable matrices. The MON parameterization can be
understood as a special case of LBEN with a fixing Ψ = I .
Proposition 8. The MON parameterization (16) does not contain all feedforward networks, and if
m ≥ 1 it does not contain any feedforward networks.
From the proof (see Appendix E.11). The set of feedforward networks in MON shrinks as the hyper-
parameter m increases. Most experiments in Winston & Kolter (2020) use m = 1, which excludes
all feedforward networks.
In the feedforward case, our Lipschitz bound condition (4) is equivalent to the state-of-art bound
estimation method in Fazlyab et al. (2019). The major benefit of our direct parameterization (9) is
that it allows such bounds to be imposed during training without any additional computational cost.
The details are given in Appendix D.
5 Experiments
In this section we test our approach on the MNIST and CIFAR-10 image classification problems.
Our numerical experiments focus on model robustness, the trade-off between model performance
and the Lipschitz constant, and the tightness of the Lipschitz bound. We compare the proposed
LBEN to unconstrained equilibrium networks, monotone operator equilibrium network (MON) of
Winston & Kolter (2020), and fully connected networks trained using Lipschitz margin training
(LMT) (Tsuzuku et al., 2018). When studying model robustness to adversarial attacks, we use the
L2 Fast Gradient Method, implemented as part of the Foolbox toolbox (Rauber et al., 2020). All
7
Under review as a conference paper at ICLR 2021
14
12
× LBEN 7 = 0
× LBEN 7 = 0
× LBEN 7 = 0
× LBEN 7 = 5
同
4
8-
0
10
Srl
10°
O
□κ O
IO1
Lipschitz
X LBEN 7 < ∞
OLMTC=I
OLMTC=Ioo
OLMTC = 250
OLMTC=I(X)O
□ MON
□ Unconstrained
IO2
IO3
(b) Test error with adversarial perturbation versus
size of adversarial perturbation. Lower is better.
(a) Nominal test error vs Lipschitz constant esti-
mates: markers indicate observed lower bounds for
all methods, vertical lines indicate certified upper
bounds for LBEN
8
X
X
X
O
O
iɔrr
□
Figure 2:	Image classification results on MNIST character recognition data set.
models are trained on a either a standard desktop computer with an NVIDIA GeForce RTX 2080
graphics card or using a google cloud instance with a Nvidia Tesla V100 graphics card. Details
of the models and training procedure can be found in Appendix F, all code will be made available
online but links are omitted due to the double-blind review process.
5.1	MNIST Experiments with Fully-Connected Networks
In Figure 2a the test error versus the observed Lipschitz constant, computed via adversarial attack for
each of the models trained. We can see clearly that the parameter γ in LBEN offers a trade-off be-
tween test error and Lipschitz constant. Comparing the LBENγ=5 with both MON and LBENγ<∞,
we also note a slight regularizing effect in the lower test error.
By comparison, LMT (Tsuzuku et al., 2018) with c as a tunable regularization parameter displays a
qualitatively similar trade-off, but underperforms LBEN in terms of both test error and robustness.
If we examine the unconstrained equilibrium model, we observe a Lipschitz constant more than an
order of magnitude higher, i.e. this model has regions of extremely high sensitivity, without gaining
any accuracy in terms of test error.
For the LBEN models, the lower and upper bounds on the Lipschitz constant are very close: the
markers are very close to their corresponding lines in Figure 2a, see also the table of numerical
results in Appendix A in which the approximation accuracy is in many cases around 90%.
Next we tested robustness of classification accuracy to adversarial attacks of various sizes, the results
are shown in Figure 2b and summarized in Table 1. We can clearly see that decreasing γ (i.e. stronger
regularization) in the LBEN models results in a far more gradual degradation of performance as
perturbation size increases, with only a mild impact on nominal (zero perturbation) test error.
Next, we examined the impact of our parameterization on computational complexity compared to
other equilibrium models. The test and training errors versus number of epochs are plotted in Figure
5, and we can see that all models converge similarly, and also take roughly the same amount of time
per epoch. This is a clear contrast to the results of Pauli et al. (2020) in which imposing Lipschitz
constraints resulted in fifty-fold increase in training time. Interestingly, we can also see in Figure 5
the effect of regularisation for LBEN with γ = 5: higher training error but lower test error. We have
observed several cases where the unconstrained equilibrium model became unstable during training,
LBEN never exhibits this problem.
Finally, we examined the quality of the Lipschitz bounds as a function of network size, comparing
the upper and lower bounds on fully connected networks with width 20 to 1000. The results are
shown in Figure 6. It can be observed that network size only has a mild effect on the quality of the
Lipschitz bounds, which decrease slightly as width is increased by a factor of 50.
8
Under review as a conference paper at ICLR 2021
5 4 3 2 1 O
3 3 3 3 3 3
JOJJH⅛θl
U
Iu
CqdJolJR 户səT
908070605040
0	0.5
1	1.5	2	2.5	3
h perturbation
—LBEN T = 1.0
-LBEN 7 = 5.0
LBEN 7 = 50.0
一LBEN 7 < ∞
Feedforward
30
IO0	IO1
Lipschitz (lower bound)
(a)	Nominal test error vs observed lower bound on
Lipschitz constant.
(b)	Test error with adversarial perturbation versus
size of adversarial perturbation. Lower is better.
Figure 3:	Image classification results on CIFAR-10 data set.
5.2 CIFAR- 1 0 Experiments With Convolutional Networks
The previous example looked at simple fully connected networks, however, our approach can also be
applied to structured layers such as convolutions. Here, we perform several experiments exploring
the use of convolutional layers on the CIFAR-10 dataset. To study the improved expressibility we
will compare the LBEN to the LBEN with its metric set to the identity, denoted LBEN Λ=I . Note
that the model set LBEN Λ=I,γ<∞ corresponds to the MON. Additional model details can be found
in Appendix F.2.
In Figure 3a, we have plotted the test performance versus the observed Lipschitz constant for the
LBEN and LBEN Λ=I for varying Lipschitz bound γ = 1, 2, 3, 5, 50, along with the LBENγ <∞ ,
MON, and feed-forward convolutional networks with 40, 81, 160, and 200 channels. Again, we
see that the Lipschitz bound has a regularizing effect, trading off between nominal fit and robust-
ness. Additionally, we see that the LBEN provides both better performance and robustness than
the traditional feed-forward convolutional networks of similar sizes, highlighting the benefit of the
equilibrium network structure.
Comparing LBEN and LBENΛ=I, we can see that the metric gives higher quality models for LBEN
with specified γ, but it is slightly worse for LBEN γ < ∞ compared to MON. This is likely due
to the extra expressiveness of the model leading to some overfitting. This can also be seen in the
training curves in Figure 7.
Figure 3b shows the test error versus the size of adversarial perturbation for the lBEN and 162
channel feed-forward convolutional network. We observe that the LBEN provides a much more
gradual loss in performance than the feed-forward network, with γ = 5 offering an excellent mix of
nominal performance and robustness. The feed-forward networks of different sizes exhibited similar
results, however only one is plotted in Figure 3b for clarity.
6 Conclusions
In this paper, we have shown that the flexible framework of equilibrium networks can be made
robust via a simple and direct parameterization which results in guaranteed Lipschitz bounds. These
results can also be directly applied (as a special case) to standard multilayer and residual deep neural
networks, and also provide a direct parameterization of nonlinear ODEs satisfying strong stability
and robustness properties.
Extension to equilibrium network structures more general than (1) is an interesting area for future
research. Our results can be extended to more general multivariable “activations” if they can be
described accurately via monotonicity properties or integral quadratic constraints. One particular
example where this is possible is where the “activation” computes the arg min of a quadratic pro-
gram of the sort that appears in constrained model predictive control (Heath & Wills, 2007).
9
Under review as a conference paper at ICLR 2021
References
Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In Inter-
national Conference on Machine Learning,pp. 291-301. PMLR, 2019.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems, pp. 690-701, 2019.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Heinz H Bauschke, Patrick L Combettes, et al. Convex analysis and monotone operator theory in
Hilbert spaces, volume 408. Springer, 2011.
Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the
alternating direction method of multipliers. Now Publishers Inc, 2011.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018.
Yun-Chung Chu and Keith Glover. Bounds of the induced norm and model reduction errors for
systems with repeated scalar nonlinearities. IEEE Transactions on Automatic Control, 44(3):
471-483, 1999.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310-1320, 2019.
Fernando J D' Amato, Mario A Rotea, AV Megretski, and UT Jonsson. New results for analysis of
systems with repeated nonlinearities. Automatica, 37(5):739-747, 2001.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Y. Tsai. Implicit deep
learning. arXiv:1908.06315, 2019.
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George Pappas. Efficient
and accurate estimation of lipschitz constants for deep neural networks. In Advances in Neural
Information Processing Systems, pp. 11427-11438, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
WP Heath and AG Wills. Zames-falb multipliers for quadratic programming. IEEE Transactions
on Automatic Control, 10(52):1948-1951, 2007.
Joao P Hespanha. Linear Systems Theory. Princeton university press, 2018.
R Bruce Kellogg. A nonlinear alternating direction method. Mathematics of Computation, 23(105):
23-27, 1969.
Hassan K Khalil. Nonlinear systems. Prentice-Hall, 2002.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic gradient descent. In ICLR:
International Conference on Learning Representations, 2015.
Vishwesh V Kulkarni and Michael G Safonov. All multipliers for repeated monotone nonlinearities.
IEEE Transactions on Automatic Control, 47(7):1209-1212, 2002.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Jia Li, Cong Fang, and Zhouchen Lin. Lifted proximal operator machines. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 33, pp. 4181-4188, 2019.
Changliu Liu, Tomer Arnon, Christopher Lazarus, Clark Barrett, and Mykel J Kochenderfer. Algo-
rithms for verifying deep neural networks. arXiv preprint arXiv:1903.06758, 2019.
10
Under review as a conference paper at ICLR 2021
Winfried Lohmiller and Jean-Jacques E Slotine. On contraction analysis for non-linear systems.
Automatica, 34(6):683-696,1998.
Alexandre Megretski and Anders Rantzer. System analysis via integral quadratic constraints. IEEE
Transactions on Automatic Control, 42(6):819-830, 1997.
Arkadi Nemirovski. Prox-Method with Rate of Convergence O(1/t) for Variational Inequalities with
Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle Point Problems.
SIAM Journal on Optimization, 15(1):229-251, 2004.
Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related
problems. Mathematical Programming, 109(2):319-344, 2007.
Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex program-
ming. SIAM, 1994.
Patricia Pauli, Anne Koch, Julian Berberich, and Frank AllgoWer. Training robust neural networks
using Lipschitz bounds. arXiv preprint arXiv:2005.02929, 2020.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. In International Conference on Learning Representations, 2018a.
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems, pp.
10877-10887, 2018b.
Anders Rantzer. On the Kalman-Yakubovich-Popov lemma. Systems & Control Letters, 28(1):7-10,
1996.
Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast
adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensor-
flow, and jax. Journal of Open Source Software, 5(53):2607, 2020.
Max Revay, Ruigang Wang, and Ian R Manchester. A convex parameterization of robust recurrent
neural networks. IEEE Control Systems Letters, 2020. doi: 10.1109/LCSYS.2020.3038221.
Ernest K Ryu and Stephen Boyd. Primer on monotone operator methods. Appl. Comput. Math, 15
(1):3-43, 2016.
John W Simpson-Porco and Francesco Bullo. Contraction theory on riemannian manifolds. Systems
& Control Letters, 65:74-80, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR: International Conference on
Learning Representations, 2014.
Vincent Tjeng, Kai Y Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
integer programming. In International Conference on Learning Representations, 2018.
Mark M Tobenkin, Ian R Manchester, and Alexandre Megretski. Convex parameterizations and
fidelity bounds for nonlinear identification and reduced-order modelling. IEEE Transactions on
Automatic Control, 62(7):3679-3686, 2017.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifi-
cation of perturbation invariance for deep neural networks. In Advances in neural information
processing systems, pp. 6541-6550, 2018.
Ezra Winston and J. Zico Kolter. Monotone operator equilibrium networks. arXiv:2006.08591,
2020.
G. Zames. Realizability Condition for Nonlinear Feedback Systems. IEEE Transactions on Circuit
Theory, 11(2):186-194, 1964.
SiQi Zhou and Angela P Schoellig. An analysis of the expressiveness of deep neural network
architectures based on their lipschitz constants. arXiv preprint arXiv:1912.11511, 2019.
11
Under review as a conference paper at ICLR 2021
A	Experimental Results on MNIST Character Recognition
This appendix contains tables of results on MNIST and CIFAR-10 data sets.
Legend:
•	Err: Test error (%),
•	kak2: `2 norm of adversarial attack.
•	γup : certified upper bound on Lipschitz constant (for models that provide one).
•	γlow : observed lower bound on Lipschitz constant via adversarial attack.
•	Y approx: approximation ratio of Lipschitz constant as percentage = 100 X ( Yow ).
γup
Models:
•	LBEN: the proposed Lipschitz bounded equilibrium network..
•	MON: the monotone operator equilibrium network of Winston & Kolter (2020).
•	UNC: an unconstrained equilibrium network, i.e. W directly parameterized.
•	LMT: Lipschitz Margin Training model as in Tsuzuku et al. (2018).
•	Lip-NN: The Lipschitz Neural Network model of Pauli et al. (2020). Note these figures
are as reported in (Pauli et al., 2020), all other figures are calculated by the authors of the
present paper.
Model	Err: ka∣∣2 = 0	Err: ka∣∣2 ≤ 5	Err: ka∣∣2 ≤ 10	YUp	YIoW	γ approx
LBENγ<∞	2.03	=	56.0	=	82	=	-	9.8	-
LBENY=5	181	46.4	95.4		2.912	58.2%
LBENY=I	236	194	855	-1-	0.865	86.5%
LBENY=0.8	259	174	80.1	""0.8-	0.715	89.4%
LBENY=0.4	4.44	16.1	650	0~4A~	0.372	-93%-
LBENY=0.2	741	144	426	Z02Ξ	0.184	92%
MON	2.04	558	88.6	-	7.75	-
UNC	2.08	48:75	77.9	-	239.0	-
LMTc=I	23	594	88.1	-	17.5	-
LMTc=100	34	654	920	-	7.66	-
LMTc=250	692	618	98.4	-	6.92	-
LMTc=I。。。	12:23	78.4	—	98.9 —	-	3.10	-
LiP-NN	3.55	—	-	-	~874~	-	-
Table 1: Results from MNIST experiments.
12
4
2
3
7
7
7

0

4

7
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
4
4
2

2
2
2
3
2
7
4
q
O

4
U
7
日
3
3
2
7
2
夕
夕
7
7
7
7
q
4




4



O


£
£



3

4
4

4

7
7
7
7

3
7
7
a
夕

4
£

4


曳


Z
7
Z
Z
夕
Z
7
7
7
7一
7
2
a
a
a
a
a
a
0
13

N
」7
夕一
* 一
夕
a

0
0
Figure 4: Random selection of MNIST adversarial examples from Figure 2b. Top to bottom is increasing perturbation size. Left to right are different examples.
Under review as a conference paper at ICLR 2021
IOT
(女)Jang UlBJj.
IO—
0	5	10	15	20	25	30	35	40
Epochs
—MON
—Unconstrained
—LBEN 7 < ∞
—LBEN 7 = 5
武)≡uħ3酉
Figure 5: Left: Training set error versus epochs. Right: Test set error versus epochs. Note that
the left and right plots are on different scales. The time per epoch for the MON, unconstrained,
LBENγ<∞ and LBENY=5 networks are 14.4, 16.1, 14.9 and 14.8 seconds per epoch respectively.
Width
Figure 6: Approximation accuracy of the Lipschitz bound versus the network width of LBEN from
the MNIST example. The certified upper bound is YUP and the observed lower bound is γι0w.
14
Under review as a conference paper at ICLR 2021
B Experimental Results on CIFAR-10 dataset
Model	Err: ∣∣a∣∣2 = 0	Err： IlaII2 ≤ 0.5	Err： Ilall2 ≤ 1.0	Yup	Ylow	γ approx
LBENγ<∞	31.1	=	96.1 ɪɪ	100 ɪɪ	-	31.1	-
LBENY=50	284	755	954	-30-	2.89	-57%-
LBENγ=5	29.9	658	855		1.39	27.8%
LBENY=3	313	64.2	835		1.14	38.0%
LBENY=2	379	625	80.5	~^Γ~	0.92	46.0%
LBENY=I —	36.2	618	788	~1~~	0.60	60.0%
FFW=40	33:07	915	998	-	6.06	--%-
FFW=81	326	93.3	100	-	8.42	--%-
FFW=162	325	950	100	-	11.3	--%-
FFW=200	32.6	—	94.5	100	-	12.4	-%
Table 2: Results from CIFAR experiments. FF refers to the feed-forward convolutional network.
6∪
50
40
∙IO.LIabjouɪLnGjɪ
20
10
0
0	5	10	15
Epochs
20	25
Figure 7: LBEN and MON training error versus epochs on CIFAR-10 dataset. The red curves have
the metric set so that Λ = I whereas the blue curves optimize over the metric. The line styles
correspond to different gain bounds. Note that both MON and LBENγ<∞ achieve zero training
error.
C Preliminaries
C.1 Monotone Operators with Non-Euclidean Inner Products
We present some basic properties of monotone operators on a finite-dimensional Hilbert space H,
which we identify with Rn equipped with a weighted inner product hx, yiQ = y>Qx with Q 0.
For n = 1, we only consider the case of Q = 1. The induced norm ∣∣x∣∣q is defined as，(x, x〉q. A
relation or operator is a set-valued or single-valued map defined by a subset of the space A ⊆ H×H;
we use the notation A(x) = {y | (x, y) ∈ A}. If A(x) is a singleton, we called A a function.
Some commonly used operators include: the linear operator A(x) = {(x, Ax) | x ∈ H}; the
operator sum A + B = {(x, y + z) | (x, y) ∈ A, (x, z) ∈ B}; the inverse operator A-1 =
{(y, x) | (x, y) ∈ A}; and the subdifferential operator ∂f = {(x, ∂f(x))} with x = dom f and
∂f (x) = {g ∈ H | f(y) ≥ f(x) + hy - x, giQ, ∀y ∈ H}. An operator A has Lipschitz constant L
if for any (x, u), (y, v) ∈ A
ku - vkQ ≤ Lkx - ykQ.	(17)
15
Under review as a conference paper at ICLR 2021
An operator A is non-expansive if L = 1 and contractive if L < 1. An operator A is monotone if
hu - v, x - yiQ ≥ 0, ∀(x, u), (y, v) ∈ A.	(18)
It is strongly monotone with parameter m if
hu - v,x - yiQ ≥ mkx - yk2Q, ∀(x, u), (y, v) ∈ A.	(19)
A monotone operator A is maximal monotone if no other monotone operator strictly contains it,
which is a property required for the convergence of most fixed point iterations. Specifically, an
affine operator A(x) = Wx + b is (maximal) monotone if and only if QW + W>Q 0 and
strongly monotone if QW + W>Q mI. A subdifferential ∂f is maximal monotone if and only
if f is a convex closed proper function.
The resolvent and Cayley operators for an operator A are denoted RA and CA and respectively
defined as
RA = (I + αA)-1, CA =2RA -I	(20)
for any α > 0. When A(x) = Wx + b, then
RA(x) = (I+αW)-1(x-αb)	(21)
and when A = ∂f for some CCP function f, then the resolvent is given by a proximal operator
RA(X) = ProXa(X) := arg min j∣x - ZkQ + αf(z).	(22)
f	z2	Q
The resolvent and Cayley operators are non-expansive for any maximal monotone A, and are con-
tractive for strongly monotone A. Operator splitting methods consider finding a zero in a sum of
operators (assumed here to be maximal monotone), i.e., find z such that 0 ∈ (A + B)(z). For ex-
ample, the convex optimization problem in (12) can be formulated as an operator splitting problem
with A(z) = (I - W)z - b and B = ∂f. Proposition 2 shows that A is strongly monotone and
Lipschitz with some parameters of m and L. Here we give some popular operator splitting methods
for this problem as follows.
•	Forward-backward splitting: zk+1 = RB (zk - αA(zk)), i.e.,
uk = ((1 -α)I+αW)zk+αb
zk+1 = ProXfα (uk)
•	Peaceman-Rachford splitting: uk+1 = CACB(uk), zk = RB(uk), i.e.,
uk+1/2 = 2zk - uk,
zk+1/2 = (I + α(I - W))-1(uk+1/2 + αb),
uk+1 = 2Xk+1/2 - uk+1/2,
zk+1 = ProXfα (uk+1).
(23)
(24)
•	Douglas-Rachford splitting (or ADMM): uk+1 = 1/2(I + CACB)(uk), zk = RB(uk),
i.e.,
uk+1/2 = 2zk - uk,
zk+1/2 = (I + α(I - W))-1(uk+1/2 + αb),
uk+1 = 2Xk+1/2 - uk+1/2,	(25)
zk+1 = ProXfα (uk+1).
A sufficient condition for forward-backward splitting to converge is α < 2m/L2 . The Peacemance-
Rachford and Douglas-Rachford methods converge for any α > 0, although the convergence speed
will often vary substantially based upon α.
16
Under review as a conference paper at ICLR 2021
C.2 Dynamical System Theory
In this section, we present some concepts and results of dynamical system theory that are used in
this paper. We consider a nonlinear system of the form
z(t) = f (z(t))	(26)
where z (t) ∈ Rn is the state, and the function f is assumed to be Lipschitz continuous. By Picard’s
existence theorem we have a unique a solution for any initial condition. The above system is time-
invariant since f is not explicitly depends on t. System (26) is called linear time-invariant (LTI)
system if f(z ) = Az + b for some matrix A ∈ Rn×n and b ∈ Rn. The point z? ∈ Rn is call an
equilibrium of (26) if f(z?) = 0.
The central concern in dynamical system theory is stability. While there are many different stability
notions (Khalil, 2002), here we mainly focus on two of them: exponential stability and contraction
w.r.t a constant metric Q 0. System (26) is said to be locally exponentially stable at the equilib-
rium z? w.r.t. to the metric Q if there exist some positive constants α, β, δ such that for any initial
condition z(0) ∈ Bδ(z?) := {z | kz - z?kQ < δ}, the following condition holds:
I∣z(t) - z?k ≤ α∣∣z(0) - z*∣∣Qe-叫 ∀t > 0.	(27)
And it is said to be globally exponentially stable if the above condition also holds for any δ > 0.
The exponentially stability can be verified via Lyapunov’s second method, i.e., finding a Lyapunov
function V = ∣∣z∣p with P * 0 such that V(t) ≤ -2βV(t) along the solutions, i.e.,
(z - z?)>P f (z) + f(z )>P(z - z?) + 2β(z - z?)>P(z - z?) ≤ 0.	(28)
System (26) is said to be contracting w.r.t. the metric Q if there exist some positive constants α, β
such that for any pair of solutions z1(t) and z2(t), we have
Iz1(t)-z2(t)IQ ≤ αIz1(0) - z2(0)IQe-βt, ∀t>0.	(29)
Note that contraction is a much stronger notion than global exponential stability as Condition (27)
can be implied by Condition (29) by setting z1 = z and z2 = z?. However, unlike the Lyapunov
analysis, contraction analysis can be done via simple local analysis which does not require any
prior-knowledge about the equilibrium z?. Specifically, contraction can be established by the local
exponential stability of the associated differential system defined by
.
∆ Z = Df (z)∆z
where ∆z (t) is the infinitesimal variation between z(t) and its neighborhood solutions, and Df
is Clarke generalized Jacobian. The condition for (26) to be contracting can be represented as a
state-dependent Linear Matrix Inequality (LMI) as follows
PDf (z) + Df(z)>P + 2βP Y 0	(30)
for some P * 0 and all z ∈ Rn . For an LTI system, exponential stability and contraction are
equivalent and the stability condition can be s if A is Hurwitz stable (i.e. all eigenvalues of A have
strictly negative real part).
For most applications, the dynamic system usually involves an external input x(t) ∈ Rm and an
output y(t) ∈ Rp, whose state-space representation takes the form of
z(t) = f (z(t),x(t)), y(t) = h(z(t),x(t)).	(31)
Here we measure the robustness of the above system under input perturbation by incremental L2 -
gain. That is, system (31) has an incremental L2 -gain bound of Y if for any pair of inputs x 1 (∙) ,χ2 (∙)
with R0T Ix1(t) - x2(t)I22dt < ∞ for all T > 0, and any initial conditions z1(0) and z2(0), the
solutions of (31) exists and satisfy
ZTIy1(t)-y2(t)I22
0
dt ≤ γ2
ZT
0
Ix1(t) - x2(t)I22 dt + κ(z1(0), z(0))
(32)
for some function κ(z1, z2) ≥ 0 with κ(z, z) = 0. Note that γ can be viewed as a Lipschitz
bound of all the mappings defined by (31) with some initial condition from the input signal χ(∙) to
17
Under review as a conference paper at ICLR 2021
y(∙). For any two constant inputs xι, x2, let zι, z2 and yι, y2 be the corresponding equilibrium and
steady-state output, respectively. From (32) we have
ky1 - y2k22 ≤ kx1 - x2k22 + κ(z1, z2)/T,
which implies a Lipschitz bound of γ as T → ∞.
A particular class of nonlinear systems that have strong connections to various neural networks is
the so-called Lure system, which takes the form of
Z(t)= Az(t) + Bφ(Cz(t))	(33)
where A, B, C are constant matrices with proper size, and φ is a static nonlinearity with sector
bounded of [α, β]: for all solution (v, w) with w = φ(v)
(w - αv)> (βv - w) ≥ 0	(34)
or equivalently wv	Π wv ≥ 0 with
Π	2αβI	(α + β)I	35
Π = (α + β)I	-2I	.	(35)
This implies that the origin is an equilibrium since φ(0) = 0. The above system can be viewed as a
feedback interconnection of a linear system
ʃ Z(t) = Az(t) + Bw(t)
v(t) = Cz(t)
(36)
and a nonlinear memoryless component w(t) = φ(v(t)). The above linear system can also be
described by a transfer function G(s) with s ∈ C. We refer to Hespanha (2018) for details about
frequency-domain concepts and results of linear systems. The frequency-domain representation for
the sector bounded condition (34) can be written as
V(jω) ]* π ΓV(jω)
W(jω)	1 W(jω)
≥0
∀ω ∈ R
(37)
where V(jω) and W(jω) are Fourier transforms of V and w, respectively, (∙)* denotes the com-
plex conjugate. Then, the closed-loop stability of the feedback interconnection can be verified by
the Integral Quadratic Constraint (IQC) theorem (Megretski & Rantzer, 1997). Although the IQC
framework allows for more general dynamic multipliers, here we only focus on the simple constant
multiplier defined in (35).
Theorem 3.	Let G be stable andφ be a static nonlinearity with sector bound of [α, β]. The feedback
interconnection of G and φ is stable if here exists > 0 such that
G(IM ∏ G(Iω) W -ei, ∀ω ∈ R.	(38)
The Kalman-Yakubovich-Popov (KYP) lemma (Rantzer, 1996) can be applied to demonstrate the
equivalence of Condition 3 in Theorem 3 to an LMI condition. The result is stated as follows.
Theorem 4.	There exists a e > 0 such that (38) holds if and only if there exists a matrix P
such that
A>P +PA PB C>
B>P	0	+	0
Y 0.
C
0
0
I
0
I
Π
D LBEN Parameterization for Feedforward Networks
Given an equilibrium network (1) with weights U, W , and Wo, we can estimate its Lipschitz bound
γ by solving the following SDP with (n + 1) decision variables:
2Λ - ΛW - W>Λ -ΛU Wo>
min	γ s.t.	-U>Λ	γI	0	0.
γ>0, Λ∈D+	Wo	0	γI
(39)
18
Under review as a conference paper at ICLR 2021
Note that the above LMI constraint is equivalent to (4) via Schur complement. A tight upper bound
is then obtained by minimizing γ. When a deep neural network (a special case of equilibrium
network) is considered, the above SDP yields the same bound estimation as LipSDP-Neuron in
Fazlyab et al. (2019) since both formulations involve minimizing the gain bound γ subject to an
equivalent constraint (41).
Training a feedforward network with a prescribed Lipschitz bound is a challenge problem due to
the LMI constraint (39) as well as the sparse structure of W. Following the similar idea of direct
parameterization, we will construct a parameterization built on (9) to represent the following weight
W
一 0
卬1
.
.
.
0
(40)
0
WL-1	0
We first look at a simple case where W is a dense strictly lower triangular matrix. Given a square
matrix H, its LDU partition is defined as H = [H]D + [H]L + [H]U where [H]D is a diagonal
matrix, [H]L ([H]U) is a strictly lower(upper) triangular matrix. Given any hyper-parameter γ > 0,
the parameterization contains the following free variables: V ∈ Rn×n , Wo ∈ Rp×n , and U ∈ Rn×d .
Let S = [H]l - [H]>, Ψ = [H]D1 and U = ΨU where H = V > V + eI + (W> Wo + U U >)∕2γ.
Then, the LBEN parameterization (9) yields
W = I - Ψ 1 ɪWTWo + 71Ψ-1UUtΨ-1 + VtV + eI + S) = -2[H]01 [H]l,
2γ o 2γ
which is a dense lower triangular matrix. To impose the sparse pattern like (40), we need
ΓΛι	H>
Hl卜2
H
H
H>
..
..
HL-2	Λl-i	H]-
HL-1	ΛL
where Λi belongs to D+ with 1 ≤ i ≤ L, and Hj has the same dimension as Wj for1 ≤ j ≤ L -1.
To make V> V have the same band structure as H, we further parameterize V as follows
「Γι	1
Φ1V1 Γ2
V =
..
..
ΦL-1VL-1 ΓL
where Γi, Φj ∈ D+ and Vj> Vj = I. The unitary matrix Vj can be parameterized by Vj = eSj where
Sj> = -Sj . The diagonal blocks of V>V are Γi2 + Φi2 with ΦL = 0 while the lower off-diagonal
blocks are Γj+1Φj Vj with 1	≤ j ≤ L -1. Similar techniques can be applied to the parameterization
_ . ^
of Wo and Ub.
E	Proofs
E.1 Proof of Theorem 1
We presents two proofs for the well-posedness of equilibrium network (1). All these proofs are
based on the following lemma.
Lemma 1 (Simpson-Porco & Bullo (2014)). Fora time-invariant contracting dynamical system, all
its solutions converge to a unique equilibrium.
(Monotone operator perspective): This proof is mainly based on Proposition 2, which states that the
solution of (1) is also a zero of the operator splitting problem 0 ∈ (A + B)(z), where the operators
19
Under review as a conference paper at ICLR 2021
A and B are given in (10). Condition 1 implies that the operator A is strongly monotone while
Assumption 1 implies that the operator B is maximal monotone. Furthermore, the Clay operator
CA is contractive and CB is non-expansive. Thus, applying Peaceman-Rachford algorithm to 0 ∈
(A + B)(z) yields a contracting discrete-time system (24) since CACB is a contractive operator.
Since (24) is time-invariant, it yields a unique solution z for any x and bz .
(Neural ODE perspective): This proof is built on Proposition 6, which states that the neural ODE
(14) is a contracting continuous-time dynamical system under the Assumption 1 and Condition 1.
For any fixed input x and bz, system (14) is also time-invariant and hence its solution converges to a
unique equilibrium, which is also the solution of (1).
We now prove the Lipschitz boundedness of a well-posed equilibrium network. Condition 1 implies
that there exists a constant > 0 such that
2Λ - ΛW - WTΛ	I.
For any δ ∈ (0, ) and weights Wo , U, we can find a sufficiently large but finite γ such that
1(WTWo + AUU>Λ) W (e - δ)I.
γ
Then, Condition 2 holds for A and γ since
2A - AW - WTA - 1(WTWo + AUU>A)占 δI A 0.
γo
From Theorem 2, γ is a Lipschitz bound for the well-posed equilibrium network (1).
E.2 Proof of Theorem 2
Rearranging Eq. (4) yields
2A - AW - WTA A 1(WTWo + AUUTA)占 0.
γ
The well-posedness of the equilibrium network (1) follows by Theorem 1. To obtain the Lipschitz
bound, we first apply Schur complement to (4):
^2A - AW - W>A - YW>Wo -AU1
-U>A γ	γI A 0.
Left-multiplying ∆z> ∆x> and right-multiplying ∆z> ∆x>> gives
2∆>A∆z - 2∆>AW∆z- 1∆>W> Wo∆z - 2∆>AU∆χ + Y∣∣∆χ∣∣2 ≥ 0.
z	z	γzo	z
Since (5) implies ∆v = W∆z + U∆x and ∆y = Wo∆z , the above inequality is equivalent to
Yk∆χk2 - 1 k∆yk2 ≥ 2∆>A∆z - 2∆zA∆v = 2h∆v - ∆z, ∆ziΛ.	(41)
γz
Then, the Lipschitz bound of γ for the equilibrium network (1) follows by (6).
E.3 Proof of Proposition 1
(if): It is well-known that if f is convex closed proper function, then prox1f is monotone and non-
expansive, i.e., it is slope-restricted in [0, 1]. Here f is not necessary to be closed as dom f (i.e.
the range of σ) could be open interval (zl, zr) or half-open interval (zl, zr] or [zl, zr). This can be
ʌ
resolved by defining f as the restriction of f on the closed interval [Zι, ^J and then make Zl → Zl
and Zr -→ Zr.
(only if): Assumption 1 implies that σ is a non-decreasing and piece-wise differentiable function on
R. Then, the range of σ is an interval, denoted by Z . We will construct the derivative function f0
on Z first and then integrate it to obtain f. Let {Zj ∈ Z}j∈Z be the sequence containing all points
such that either σ0(x-) = 0 or σ0(x+) = 0 for all x ∈ σ-1(Zj). Note that σ-1(Z) is a singleton for
20
Under review as a conference paper at ICLR 2021
Activation	σ(x)	Convex f (Z)	dom f
ReLU	max(x, 0)	0	[0, ∞)
LeakyReLU	max(x, 0.01x)	99 min(Z, 0)2	R
Tanh	tanh(x)	2 [ln(1 一 z2) + Zln (1+f) 一 z2]	(-1,1)
Sigmoid	1/(1 + e-x)	Z ln z +(1 — z) ln(1 — z) — z2	(0,1)
Arctan	arctan(x)	一 ln(| cos z|) — z2-	(—1,1)
Softplus	ln(1 + ex)	一Li2(ez) — iπz — z2∕2	(0, ∞)
Table 3: A list of common activation functions σ(x) and associated convex proper f (z) whose
proximal operator is σ(x). For z ∈/ dom f, we have f(z) = ∞. In the case of Softplus activation,
Lis (z) is the polylogarithm function.
all z ∈ (zj, zj+1), whereas σ-1(zj) is a closed interval of the forms (-∞, xr], [xl, xr] or [xl, ∞).
Then, we define f0 as follows
{min[σ-1 (z)] — z, if Z = zj and min σ-1(z) > -∞,
max[σ-1(z)] — z, if Z = Zj and minσ-1(z) = -∞,
σ-1(z) — z, otherwise.
Without loss of generality, we assume that 0 ∈ Z and σ-1(0) is well-defined. We define the function
f as follows
f(Z) = R0zf0(ζ)dζ+C ifZ∈Z,
∞	otherwise,
where C is an arbitrary constant. Note that f is a convex function as f0 is a piecewise differentiable
function on Z and for those points where x = σ-1(Z) is well-defined, f0 is differentiable with
f00(z) = 1∕σ0(x) — 1 ≥ 0 as σ0(x) ∈ (0,1]. Finally, the definition of f0 implies that 0 ∈ Z 一
σ-1(Z) + ∂f(Z), which implies that Z = σ(x) is the unique minimizer of 1/2(Z — x)2 + f (Z).
Furthermore, since σ is well-defined, we can conclude that f is bounded from below. We also
provide a list of f for common activation functions in Table 3. A similar list can also be found in Li
et al. (2019).
E.4 Proof of Proposition 2
Similar to Winston & Kolter (2020), we first show that the solution of (1), if it exists, is an fixed
point of the forward-backward iteration (23) with α = 1:
Zk+1 = RB(Zk — αAZk) = proxf1(Zk — α(I — W)Zk + α(Ux + bz)) = σ(W Zk + Ux + bz).
The last equality follows by
-argminz1 2 (zι 一 x。2 + f(zj
σ(x)=	.
_argminzn 2(Zn - Xn)2 + f (zn)
1n
arg min J∣z — xkΛ +	λif(Zi) = ProXI(X)
z 2	i=1
Note that the necessary condition for σ(∙) to be diagonal is that the weight A is positive diagonal.
Now we prove the well-posedness of LBEN by showing that the operator splitting problem 0 ∈
(A+ B)(Z) has a unique solution for any x and bz. Both Condition 1 and 2 implies that the operator
A is strongly monotone and its Cayley operator CA is contractive. Then, the Peaceman-Rachford
iteration (24) is contracting and hence it converges to a unique fixed point.
21
Under review as a conference paper at ICLR 2021
E.5 Proof of Proposition 3
The matrix J is diagonal with elements in [0,1]. Decompose A = Π( J + μI) for some small μ > 0,
i.e. Π = A(J + μI)-1, which is diagonal and positive-definite. By denoting H = Π(I 一 W) +
(I - W)TΠ we obtain the following inequality from (3):
Π J(I 一 W) + (I - W)T JΠ + μH 占 eI,
which can be rearranged as
Π(I - JW) + (I 一 JW)TΠ 占 eI + 2Π(I - J) 一 μH.
Since 2Π(I 一 J)占 0, We can choose a sufficiently small μ such that
Π(I 一 JW) + (I 一 JW)tΠ a 0,
which further implies that I 一 JW is strongly monotone w.r.t. Π-weighted inner product, and is
therefore invertible.
E.6 Proof of Proposition 4
First, we show that (12) is strongly convex. Since f(z) is a conic combination of convex functions
f(zi), we only need to show that the quadratic term is strongly convex, i.e.,
V2J = A(I 一 W) + (I — W)>A a 0
where
J(Z) =(2(I - W)z - Ux - bz,z)
which follows by either Condition 1 or (2). Moreover, since S = 0 for the direction parameterization
of W, we have A(I 一 W) = (I 一 W)>A and hence ∂J = A. Then, finding the global minimizer
of the strongly convex optimization problem (12) is equivalent to finding a zero for the operator
splitting problem 0 ∈ ∂(J + f)(z) = (A + B)(z).
E.7 Proof of Proposition 5
The proof is based on the “key insights” of ReLU activation from Raghunathan et al. (2018b).
That is, a ReLU constraint z = max(x, 0) is equivalent to the following three linear and quadratic
constraints between z and x: (i) z(z 一 x) = 0, (ii) z ≥ x, and (iii) z ≥ 0. From this observation an
equilibrium network (1) can be equivalently expressed as the following constraints (I) z>(z—q) = 0,
(II) z ≥ q, and (III) z ≥ 0, where q = Wz + Ux + b. Note that (II) and (III) can be rewritten as the
linear constraints in the QP problem (13) while (I) is equivalent to J(z) = 0 with
J(z) := z>A(z 一 q) = 2z>Hz + p>z
for any A ∈ D+. It is obvious that J(z) ≥ 0 for all z satisfying (II) and (III), and hence the solution
of (1) is a global minimizer of the QP problem (13). If A satisfies either Condition 1 or 2, then H
is positive-definite and(13) is a strongly convex QP problem. Thus, its global minimizer is unique,
which is also the solution of LBEN (1).
E.8 Proof of Proposition 6
From (14) the dynamics of ∆v and ∆z can be formulated as a feedback interconnection of a linear
.
system ∆v = 一∆v + W∆z and a static nonlinearity ∆z = σ(va) 一 σ(vb). The linear system
can be represented by a transfer function is G(s) = 1/(s + 1)W. The nonlinear component can
be rewritten as ∆z = Φ(va, vb)∆v where Φ as a diagonal matrix with each Φii ∈ [0, 1]. For the
nonlinear component Φ, its input and output signals satisfies the quadratic constraint (6). For the
linear system G, we have the following lemma.
Lemma 2. If Condition 1 holds, then for all ω ∈ {R ∪ ∞}
*
G(jω)
I
0A
A — 2A
G(jω)
I
Y 0.
(42)
22
Under review as a conference paper at ICLR 2021
The KYP Lemma (Theorem 4) states that (42) is equivalent to the existence of a P = P> such that
-2P	PW
WTP	0
0Λ
Λ -2Λ
Y 0.
+
It is clear from the upper-left block that P 0. The above inequality also implies
2h-∆v + W∆z, ∆viP ≤ h∆z-∆v,∆ziΛ-(k∆zk22+k∆vk22) ≤ -(k∆zk22+k∆vk22)
for some > 0. The contraction property of the neural ODE (14 follows since
dk∆vkP = 2h-∆v + W∆z, ∆vip ≤ -e(k∆zk2 + k∆vk2) ≤ -2βk∆vkP
for some sufficiently small β > 0. As a byproduct of the above inequality, we will show that the
operator -f with with f (v) = -v + W σ(v) + Ux + bz is strictly monotone w.r.t. the P -weighted
inner product since
h-f(va) + f(vb), va - vbiP = h∆v -W∆z,∆viP ≥βk∆vk2P.
E.9 Proof of Lemma 2
Note that (42) is equivalent to
2Λ — Go(jω)ΛW — Go(-jω)WTA 占 μI	(43)
where Go(jω) = 1+1^. For Some ω ∈ (R ∪ ∞) let g = <G0(jω) = <Go(-jω), where < denotes
real part. It is easy to verify that g = 1 /(ω2 + 1) ∈ [0,1]. From (3) We have
2gA - gAW - gWTA	gI
for some > 0. Rearranging the above inequality yields
2A - gAW - gWTA	gI + (1 - g)2A
Now, since g ∈ [0, 1] the right-hand-side is a convex combination of two positive definite matrices:
eI and 2A, therefore (43) holds for some μ > 0 and all ω ∈ (R ∪ ∞).
E.10 Proof of Proposition 7
It is straightforward to verify that an equilibrium network with the following weights is identical to
the feedforward network (15):
z1
Z2
.
.
.
zL
一 0
W1
.
.
.
0
0
ΓUo1
0
.
.
.
0
0 WL] . (44)
W
U
To construct an LBEN parameterization in the form (8) for W, we first need the following lemma.
Lemma 3. Condition 1 holds for any strictly lower triangular W.
Proof. We prove it by showing that for any δ > 0, there exists a A ∈ D such that
H(An,Wn) :=An(I-Wn)+(I-Wn)>An	22-nδI.	(45)
where An, Wn are the upper left n × n elements of A, W, respectively. For n = 1, λ1 > δ is
sufficient since W1 = 0. Assuming that (45) holds for An and Wn , then we have
H(An+1,Wn+1) -21-nδI
H(An,Wn) - 21-nδI
-wn+1An
-An wn>+1
2(λn+1 - 2-nδ)
(46)
where An+1 = diag(An, λn+1) and Wn+1
[ Wn 0 ]
wn+1
0 . By applying Schur complement to
(46), Inequality (45) holds for the case of n + 1 if λn+ι > 2-nδ + 2n-2∣AnWn+ι∣2∕δ.	□
Based on the above lemma, we can construct a V such that V >V = 1/2[A(I -W)+(I -W)>A]-eI
where e = 21-nδ. By choosing Ψ = A-1 and S = (AW - W>A)/2, the LBEN parameterization
(8) recovers the exact W. Thus, LBEN contains all feedforward networks (44).
We note that “skip connections” as in a residual network can easily be added to the above structure
via additional non-zero blocks in the lower-left part of the weight W.
23
Under review as a conference paper at ICLR 2021
E.11 Proof of Proposition 8
From the MON parameterization (16) we have
H(m, W) := 2(1 -m)I-W -W> = 2A>A	0.
Let Wm be the set of non-zero and strictly lower triangular W such that H(m, W)	0. Note
that Wm1 ⊂ Wm2 if m1 > m2. Because H(m1, W)	0 implies H(m2, W) = H(m1, W) +
2(m1 - m2)I	0 for all m2 < m1 . Proposition 8 follows if limm→0 Wm does not contain
all strictly lower triangular W . Since W is a strictly lower triangular, H(0, W) is a semidefinite
matrix whose diagnoal elements are 2. As the norm of W increases, H(0, W) becomes indefinite.
Taking the feedforward network (44) with L = 2 as an example, the set of W0 is characterized by
W1 W1> 4I since
H(0,W)= -2WI 1 -2WI1>	0.
Now We show that Wm = 0 for all m ≥ 1. Since the diagnoal elements of H(m, W) are non-
positive when m ≥ 1, the matrix H(m, W) is not semi-definite for any strictly lower triangular W.
F	Training Details
F.1 MNIST EXAMPLE
This section contains the model structures and the details of the training procedure used for the
MNIST examples. All models are trained using the ADAM optimizer Kingma & Ba (2015) with an
initial learning rate of 1 × 103. All models are trained for 40 Epochs, and the learning rate is reduced
by a factor of 10 every 10 epochs.
The models in the MNIST example are all fully connected models with 80 hidden neurons and ReLU
activations. For the equilibrium models, the forward and backward passes models are performed
using the Peaceman-Rachford iteration scheme with = 1 and a tolerance of 1 × 10-2. When
evaluating the models, we decrease the tolerance of the spitting method to 1 × 10-4. We use the
same α tuning procedure as Winston & Kolter (2020). All models were trained using the same initial
point. Note that for LBEN, this requires initializing the metric Λ = I .
The feed-forward models trained using Lipschitz margin training were trained using the original
author’s code which can be found at https://github.com/ytsmiling/lmt.
F.2 CIFAR- 1 0 Example
This section contains the model structures and the details of the training procedure used for the
CIFAR-10 examples. All models are trained using the ADAM optimizer Kingma & Ba (2015) with
an initial learning rate of 1 × 103 . The models were trained for 25 epochs and the learning rate
was reduced by a factor of 10 after 15 epochs. Each model contains a single convolutional layer, an
average pooling layer with kernel size 2, and a linear output layer.
The convolutional LBEN has 81 channels and is parametrized as discussed below. The MON sim-
ilarly has 81 channels. Unless otherwise stated, the feed-forward convolutional network has 162
channels which gives it approximately the same number of parameters as the LBEN.
The MON was evaluated using the Peaceman-Rachford Iteration scheme.
Convolutional LBEN
Following the approach of Winston & Kolter (2020), we parametrize U and V in equation 9 via
convolutions. The skew symmetric matrix is constructed by taking the skew symmetric part of a
convolution S, so that S = 1 (S — S>). Similar, to Winston & Kolter (2020), we also find that using
a weight normalized parametrization improves performance. Specifically, we use the following
parametrization: V = √α∣V∣, S = β ∣∣∣, U = √η 者 and Wo = √ξ得.
24
Under review as a conference paper at ICLR 2021
In Winston & Kolter (2020) Peaceman-Rachford is used and the operator I - W can be quickly
inverted using the fast Fourier transform. This situation is more complicated in our case as the term
Wo>utWout cannot be represented as a strict convolution and this is not diagonalized by the Fourier
matrix,. Instead, we apply Forward-Backward Splitting algorithm shown in equation 23 which does
not require a matrix inversion.
We have observed that the rate of convergence of the Forward-Backward splitting algorithm is highly
dependent on the monotonicity parameter m. In particular, for the convolutional models, we found
there was a strong trade-off between the ease of solve for the equilibrium versus the model express-
ibility and the accuracy of the Lipschitz bound.
25