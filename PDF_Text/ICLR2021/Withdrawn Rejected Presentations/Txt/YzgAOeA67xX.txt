Under review as a conference paper at ICLR 2021
Stable Weight Decay Regularization
Anonymous authors
Paper under double-blind review
Ab stract
Weight decay is a popular regularization technique for training of deep neural net-
works. Modern deep learning libraries mainly use L2 regularization as the default
implementation of weight decay. Loshchilov & Hutter (2018) demonstrated that L2
regularization is not identical to weight decay for adaptive gradient methods, such
as Adaptive Momentum Estimation (Adam), and proposed Adam with Decoupled
Weight Decay (AdamW). However, we found that the popular implementations of
weight decay, including L2 regularization and decoupled weight decay, in modern
deep learning libraries usually damage performance. First, the L2 regularization is
unstable weight decay for all optimizers that use Momentum, such as stochastic
gradient descent (SGD). Second, decoupled weight decay is highly unstable for all
adaptive gradient methods. We further propose the Stable Weight Decay (SWD)
method to fix the unstable weight decay problem from a dynamical perspective.
The proposed SWD method makes significant improvements over L2 regularization
and decoupled weight decay in our experiments. Simply fixing weight decay in
Adam by SWD, with no extra hyperparameter, can outperform complex Adam
variants, which have more hyperparameters.
1	Introduction
Weight decay is a popular and even necessary regularization technique for training deep neural
networks that generalize well (Krogh & Hertz, 1992). People commonly use L2 regularization as
“weight decay” for training of deep neural networks and interpret it as a Gaussian prior over the model
weights. This is true for vanilla SGD. However, Loshchilov & Hutter (2018) revealed that, when the
learning rate is adaptive, the commonly used L2 regularization is not identical to the vanilla weight
decay proposed by Hanson & Pratt (1989):
θt = (1 - λ0)θt-1 - ηgt,	(1)
where λ0 is the weight decay hyperparameter, θt is the model parameters at t-th step , η is the learning
rate, and gt is the gradient of the minibatch loss function L(θ) at θt-1. Zhang et al. (2018) revealed
three different roles of weight decay. However, the quantitative measures for weight decay are still
missing.
Adaptive gradient methods that use Adaptive Learning Rate, such as AdaGrad (Duchi et al., 2011),
RMSprop (Hinton et al., 2012), Adadelta (Zeiler, 2012) and Adam (Kingma & Ba, 2015), are a
class of most popular methods to accelerate training of deep neural networks. Loshchilov & Hutter
(2018) reported that, in adaptive gradient methods, the correct implementation of weight decay
should be applied to the weights directly and decoupled from the gradients. We show the different
implementations of decoupled weight decay and L2 regularization for Adam in Algorithm 3.
It has been widely observed that adaptive gradient methods usually do not generalize as well as SGD
(Wilson et al., 2017). A few Adam variants tried to fix the hidden problems in adaptive gradient
methods, including AdamW Loshchilov & Hutter (2018), AMSGrad (Reddi et al., 2019) and Yogi
(Zaheer et al., 2018). A recent line of research, such as AdaBound (Luo et al., 2019), Padam (Chen &
Gu, 2018), and RAdam (Liu et al., 2019), believes controlling the adaptivity of learning rates may
improve generalization. This line of research usually introduces extra hyperparameters to control the
adaptivity, which requires more efforts in tuning hyperparameters.
Although popular optimizers have achieved great empirical success for training of deep neural
networks, we discover that nonoptimal weight decay implementations have been widely used in
1
Under review as a conference paper at ICLR 2021
Table 1: Test performance comparison of optimizers. We report the mean and the standard deviations
(as the subscripts) of the optimal test errors computed over three runs of each experiment. AdamS
generalizes better than popular adaptive gradient methods significantly and often compares favorably
with the baseline optimizer SGD.
Dataset	Model	SGD	AdamS	Adam	AMSGRAD	AdamW	AdaBound	Padam	Yogi	RAdam
CIFAR-10	ResNet18	5.01o.03	4.910.04	6.960.02	6.16o.i8	5.08o.o7	5.65o.o8	5.12o.o4	5.87o.12	6.01o.1o
	VGG16	6.42o.o2	6.090.11	7.310.25	7.140.14	6.59o.13	6.76o.12	6.15o.o6	6.90o.22	6.56o.o4
CIFAR-100	DenseNet121	19.81o.33	20.520.26	25.110.15	24.430.09	21.55o.14	22.69o.15	21.10o.23	22.15o.36	22.27o.22
	GoogLeNet	21.21o.29	21.050.18	26.120.33	25.53o.i7	21.29o.17	23.18o.31	21.82o.17	24.24o.16	22.23o.15
modern deep learning libraries, including PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al.,
2016), MXNet (Chen et al., 2015), and Chainer (Tokui et al., 2019). The nonoptimal implementations
can lead to the unstable weight decay problem during training, which may harm performance seriously.
We try to answer when the unstable weight decay problem happens and how to mitigate it. While
Loshchilov & Hutter (2018) discovered that weight decay should be decoupled from the gradients, we
further discovered that weight decay should be coupled with the effective learning rate. We organize
our main findings as follows.
1.	The effect of weight decay can be interpreted as iteration-wisely rescaling the loss landscape
and the learning rate at the same time. We formulate the weight decay rate based on the rescaling
ratio per unit stepsize. We call it unstable weight decay if the weight decay rate is not constant during
training, seen in Definition 1. Our empirical analysis suggests that the unstable weight decay problem
may undesirably damage performance of popular optimizers.
2.	L2 regularization is unstable weight decay in all optimizers that use Momentum. Most
popular optimizers in modern deep learning libraries use Momentum and L2 regularization at the
same time, including SGD (with Momentum). Unfortunately, L2 regularization often damages
performance in the presence of Momentum.
3.	Decoupled weight decay is unstable weight decay in adaptive gradient methods. All adaptive
gradient methods used L2 regularization as weight decay until Loshchilov & Hutter (2018) proposed
decoupled weight decay. However, decoupled weight decay only solves part of the unstable weight
decay problem. Decoupled weight decay is still unstable in the presence of Adaptive Learning Rate.
4.	Always make weight decay rates stable. We proposed the stable weight decay (SWD) method
which applies a bias correction factor on decoupled weight decay to make weight decay more stable
during training. SWD makes significant improvements over L2 regularization and decouple weight
decay in our experiments. We display the test performance in Table 1. The Adam with SWD (AdamS)
is displayed in Algorithm 4.
2 A Dynamical Perspective on Weight Decay
In this section, we study how weight decay affects learning dynamics and how to quantitatively
measure the effect of weight decay from a viewpoint of learning dynamics. We also reveal the hidden
weight decay problem in SGD with Momentum.
A dynamical perspective on weight decay. We first present a new theoretical tool for understanding
the effect of weight decay. The vanilla weight decay described by Hanson & Pratt (1989) is given by
Equation 1. A more popular implementation for vanilla SGD in modern deep learning libraries is
given by
θt = (I - ηλ)θt-ι - ηdLMT)
∂θ
(2)
where we denote the training loss of one minibatch as L(θ) and weight decay should be coupled with
the learning rate.
We define new coordinates wt ≡ θt(1 - ηλ)-t, which is an iteration-dependent rescaled system of
θ. In the system of w, we may define the loss function of w as Lw(wt) ≡ L((1 - ηλ)twt) = L(θt).
2
Under review as a conference paper at ICLR 2021
Then, we may rewrite Equation 2 as
Wt = wt-ι - (1 - ηλ)-tηdLTT) = WtT — (1 - ηλ)-2t+1ηdL JwtT).	(3)
∂θ	∂w
Equation 3 describes learning dynamics in the rescaled coordinates of W. The equivalence of Equation
2 and Equation 3 suggests that learning dynamics with weight decay in the original coordinates ofθ
is equivalent to learning dynamics without weight decay in the rescaled coordinates of W. Thus, the
effect of weight decay can be interpreted as flattening the loss landscape ofθby a factor of (1 - ηλ)
per iteration and increase the learning rate by a factor of (1 - ηλ)-2 per iteration.
The weight decay rate and the total weight decay effect. In dynamics of SGD, the learning rate
η may be interpreted as unit stepsize or unit dynamical time. The dynamical time of deep learning
dynamics at t-th iteration can be defined as the cumulative sum of stepsizes: τ = tη. Thus, for each
time step as dτ = η, it is (1 - λ) that decides the rescaling rate of the dynamical system W per unit
dynamical time. We define the weight decay rate as the rescaling rate of the dynamical system W per
unit dynamical time:
R = 1 - λ,	(4)
which indicates the rescaling rate of the coordinates W per unit stepsize or unit dynamical time. We
define the total weight decay effect after t iterations as
ρt = (1 - ηλ)t,	(5)
which indicates the total flattened ratio of the coordinates W after t iterations. We naturally have
ρt = Rτ in the continuous-time limit or small η limit. Based on the weight decay rate, we define the
key concept, stable weight decay, as Definition 1.
Definition 1 (Stable Weight Decay). The weight decay is stable if the weight decay rate is constant
during training. Otherwise, we define it unstable weight decay.
Suppose there exists an optimal weight decay rate for training of a deep network. Then, we should
fine-tune λto have a proper weight decay rate and keep the the weight decay rate approximately equal
to the optimal weight decay rate during training. Thus, we conjecture that stable weight decay should
be a desirable property for deep learning. In this paper, we conducted comprehensive empirical
analysis to verify the advantage of stable weight decay.
Vanilla weight decay is unstable weight decay in the presence of learning rate schedulers. We
first take vanilla SGD as the studied example, where no Momentum is involved. It is easy to see
that vanilla SGD with L2 regularization λ∣∣θk2 is also given by Equation 2. Suppose the learning
rate is fixed in the whole training procedure. Then Equation 2 will be identical to Equation 1 if we
simply choose λ0 = ηλ. However, learning rate decay is quite important for training of deep neural
networks. So Equation 2 is not identical to Equation 1 in practice.
Which implementation is better? Based on the measure of the weight decay rate, we propose
Proposition 1. We leave all proofs in Appendix A.
Proposition 1. Suppose learning dynamics is governed by Equation 1 and the learning rate at t-th
step is ηt. Then, the weight decay rate at t-th step is given by R = 1 一 λ0.
Corollary 2.0.1. Suppose all conditions of Proposition 1 hold. Then, Equation-1-based weight decay
is unstable weight decay in the presence of learning rate schedulers.
Based on Definition 1 and Proposition 1, we naturally obtain Corollary 2.0.1. We argue that Equation-
2-based weight decay is better than Equation-1-based weight decay, because Equation 2 can make
the weight decay rate stable. In Figure 1, we empirically verified that the popular Equation-2-
based weight decay indeed outperforms the vanilla implementation in Equation 1. Fortunately, the
Equation-2-based implementation has been widely accepted by modern deep learning libraries.
L2 regularization is unstable weight decay in the presence of Momentum. Our previous analysis
revealed that L2 regularization is stable weight decay for vanilla SGD. It is also commonly believed
that L2 regularization is an optimal weight decay implementation when Adaptive Learning Rate is
not involved. Almost all deep learning libraries directly use L2 regularization as the default weight
decay implementation. However, we discover that L2 regularization is not identical to weight decay
and often harms performance slightly when Momentum is involved.
3
Under review as a conference paper at ICLR 2021
Figure 1: We compared Equation-1-based weight
decay and Equation-2-based weight decay by
training ResNet18 on CIFAR-10 via vanilla SGD.
We divided the learning rate by 10 at the epoch of
80 and 160, which is a popular learning rate sched-
uler. No matter how we choose λ for Equation-
1-based weight decay, Equation-2-based weight
decay shows better test performance. It demon-
strates that the form -ηλθ is a better weight de-
cay implementation than -λθ in the presence of
learning rate schedulers.
Epochs
0.06
10-7
0.07
αα
5E
ιo-β io-5 ιo-4 ιo-3 ιo-2
Weight Decay
IOT
Figure 2: We compare the generalization of
Vanilla SGD, SGD, SGDW, and SGDS under var-
ious weight decay hyperparameters by training
VGG16 on CIFAR-10. The optimal performance
of SGDS/SGDW is better than the widely used
Vanilla SGD and SGD. For Vanilla SGD, SGD,
and SGDS, we may choose λL2 = λS = 0.0005
to maintain the optimal performance. But we have
to re-tune λW = 0.005 for SGDW. Hyperparame-
ter Setting: β1 = 0 for Vanilla SGD; β1 = 0.9 for
SGD, SGDW, and SGDS. We repeat each simula-
tion for three runs. A similar experimental result
for ResNet18 is presented in Appendix C.
SGDS
seH 6-Ee31
IOT	IO-4	IQ-3
Weight Decay
5.985
5.820
5.655
5.490
5.325
5.160
4.995
4.830
4.665
4.500
seH 6-Ee31
SGD
10-β	10-5 IO-4 IO-3
Weight Decay
5.985
5.820
5.655
5.490
5.325
5.160
4.995
4.830
4.665
4.500
Figure 3: The test errors of ResNet18 on CIFAR-10. SGDS has a deeper blue basin near dark points
(≤ 4.83%). The optimal choices of η and λ are very close for SGDS and SGD.
We take Heavy Ball Method (HBM) (Zavriev & Kostyuk, 1993), which only uses fixed momentum
inertia and dampening coefficients, as the studied example in the presence of Momentum, as HBM-
style Momentum methods are widely used by many popular optimizers. SGD implemented in PyTorch
(Paszke et al., 2019) is actually HBM with default hyperparameters. SGD in TensorFlow (Abadi
et al., 2016) follows a slightly different style of Momentum, which uses the learning rate inside the
updating rule of Momentum, seen in Appendix E. In this paper, we use the popular implementation
of SGD in PyTorch, displayed in Algorithm 1, as the base implementation, as it follows the style of
HBM. Note that our theoretical results can be very easily generalized to the SGD implementation in
TensorFlow. We write HBM with L2 regularization as
mt = β1mt-1 + β3(gt + λL2θt-1)
θt = θt-1 - ηmt,
(6)
where β1 and β3 are the hyperparameters. In the common setting of SGD with Momentum, β1 = 0.9
and β3 = 1. We propose Proposition 2 which indicates that L2 regularization is unstable in the
presence of Momentum.
Proposition 2. Suppose learning dynamics is governed by SGD with L2 regularization and β1 > 0.
Then, the followings hold:
(1)	L2 regularization is not identical to weight decay;
(2)	the weight decay rate of L2 regularization is not constant during training.
4
Under review as a conference paper at ICLR 2021
Fixing weight decay in Momentum. For obtaining a stable weight decay rate, we propose the SWD
correction for HBM as
(
mt = β1mt-1 + β3gt
θt = [ι - β3(1ββ1) ηλ] θt-ι - ηmt,
(7)
where 的匕："η is the effective learning rate in HBM, as mt ≈ e3(--^) E[gt]. As (1-βt) converges
to 1 soon, We use the simplified ι-β⅛- η as the bias correction in practice, where a relatively large
weight decay rate in the first dozen iterations can work like a model parameter initialization strategy.
It is easy to see that the bias correction factor ι-β3^ for weight decay is exactly the difference between
our stable weight decay and decoupled weight decay suggested by Loshchilov & Hutter (2018). The
pseudocode of SGD with SWD (SGDS) is displayed in Algorithm 2. It may also be called HBM with
SWD (HBMS).
Algorithm 1: SGD/SGDW (HBM/HBMW) gt = VL(θt-ι) + λθt-i; mt = β1mt-1 + β3gt ; θt = θt-ι - ηmt - ηλθt-1	Algorithm 2: SGDS (HBMS) gt = VL(Ot-I); mt = β1mt-1 + β3gt; θt = θt-i - ηmt - I--1 ηλθt-1;
Now, we have three kinds of weight decay hyperparameters. We use λL2, λW, and λS denote the L2
regularization hyperparameter, the decoupled weight decay hyperparameter, and the stable weight
decay hyperparameter, respectively.
Empirical Analysis on SGDS. We empirically verified that the optimal performance of
SGDS/SGDW is often better than the widely used vanilla SGD and SGD, seen in Figures 2 and 10.
We leave more empirical results of SGDS in Appendix C, such as Table 2. Particularly, we report the
test performance of SGDS and SGD for ResNet18 on CIFAR-10 under various learning rates and
weight decay in Figure 3. SGDS has a deeper blue basin than SGD.
It is not well-known that SGDW often outperforms SGD slightly. We believe it is mainly because
people rarely know re-tuning λw based on ι-β⅛- is necessary for maintaining good performance
when people switch from SGD to SGDW. As the effective learning rate of SGD is ι-β3^η, the weight
decay rate of SGDW is actually R = 1-e1 λw rather than λw. If we use different settings of βι
and β3 in decoupled weight decay, we will undesirably change the weight decay rate R unless we
re-tune λW . However, people usually directly let λW = λL2 for SGDW in practice. Figure 2 shows
that, the optimal λL2 and λS in SGD ( with β1 = 0.9 and β3 = 1) are both 0.0005, while the
optimal λW is 0.005 instead. The optimal λL2 and λS are almost same, while the optimal λW is
quite different. Thus, the advantage of SGDS over SGDW can save us from re-tuning the weight
decay hyperparameter.
3 Fixing Weight Decay in Adaptive Gradient Methods
In this section, we study weight decay in dynamics of adaptive gradient methods. Loshchilov & Hutter
(2018) first pointed that, when the learning rate is adaptive, the commonly used L2 regularization is
not identical to weight decay.
Algorithm 3: Adam/AdamW	
gt =	二 VL(θt-ι)+ λθt-ι;
mt	= β1mt-1 + (1 - β1)gt;
vt =	β2vt-1 + (1 - β2)gt2;
m t	mt — < 1-βt ;
Vt =	-Vt ∙ 	r- 1-βt ;
θt =	二 θt-1 - √η +e mt - ηλθt-1;
Algorithm 4: AdamS
gt =	VL(θt-1);
mt	= β1mt-1 + (1 - β1)gt;
vt =	β2Vt-1 + (1 - β2)gt2;
m t	mt 	 < 1-βt ;
Vt 二	-	Vt ∙ 	r- 1-βt ;
Vt =	二 mean(Vt);
θt =	二 Θt-1 - -7^~mt - + λθt-i; t 1	Vt+e t	√^7 t 1;
5
Under review as a conference paper at ICLR 2021
Decoupled weight decay is unstable weight decay in adaptive gradient methods. In the following
analysis, we ignore the effect of Momentum and focus on the effect of Adaptive Learning Rate. Thus,
AdamW dynamics can be written as
θt = (I - ηλ)θt-ι - ηv-1 dL*τ),	⑻
∂θ
where vt is the exponential moving average of the squared gradients in Algorithm 3 and the power
-1
notation of a vector means the element-wise power of the vector. We interpret ηvt 2 as the effective
learning rate of adaptive gradient methods. We clearly see that decoupled weight decay uses the
vanilla learning rate rather than the effective learning rate. Thus, the weight decay rate is anisotropic
in AdamW. We propose Proposition 3.
Proposition 3. Suppose learning dynamics is governed by AdamW. Then, the weight decay rate RW
is given by
RW = 1 — λv 2,	(9)
where 1 is the all-ones vector. Thus, decoupled weight decay is unstable weight decay in adaptive
gradient methods.
We also note that a good property of decoupled weight decay is that the total weight decay effect
after t iterations is still ρt = (1 - ηλ)t. In summary, the advantage of AdamW is that the total weight
effect ρ after training is isotropic and does not depend on v , while the disadvantage of AdamW is that
v makes the weight decay rate anisotropic and unstable during training.
Fixing weight decay in adaptive gradient methods. It is easy to fix weight decay by using the
following bias-corrected weight decay:
θt = (I - ηv-1 λ)θt-ι - ηv-1 dL(θtT),	(IO)
∂θ
where the weight decay rate R = (1 - λ). Without losing generality, we first assume the dimension-
ality of θ is one. The bias correction factor Vt 2 make the weight decay rate ideally stable during
training in one-dimensional dynamics.
However, if we still use the element-wise bias correction factor Vt 2 in multi-dimensional dynamics,
the total weight decay effect ρ will become highly different along different dimensions: ρt =
(1 - λ)Pk=1 ηvk 2. The anisotropic total weight decay effect is undesirable. Unfortunately, there is
no simple solution to achieving both an ideally stable weight decay rate and an isotropic total weight
decay effect at the same time for adaptive gradient methods. This may be an internal fault of all
optimizers that use Adaptive Learning Rate.
Although there is no ideal stable weight decay for Adam, it is possible to apply an isotropic bias
correction term to implement relatively more stable weight decay. We propose the updating rule in
presence of Adaptive Learning Rate and SWD as:
θt = (I - ηv-22λ)θt-ι - ηv-1 dL(θtτ),	(II)
∂θ
where Vt is the mean of all elements of the vector vt. This is exactly the updating rule of AdamS,
-1	-1
displayed in Algorithm 4. Here ηvt 2 is for approximating the effective learning rate. As Vt 2 is
isotropic, the total weight decay effect is still same along different dimensions. The weight decay
rate RS of AdamS dynamics is given by
_1	1
RS = 1 - λv- 2 Vt.	(12)
Finally, we propose Propositions 4 and 5 for quantitatively measuring the magnitude of the weight
decay rate of AdamS and AdamW. As L2 norm is the most popular measure for the magnitude of
vectors, we measure the stability of AdamW and AdamS in terms of L2 norms of the coefficients of
λ in the corresponding weight decay rate.
Proposition 4. For the weight decay rate of AdamW in Equation 9, the L2 norm of the vector
coefficient of λ is equal to the sum of all elements in Vt, which is highly varying during training.
6
Under review as a conference paper at ICLR 2021
U叫P
0.10-
0.07
---AdamS
——AdamW
---Adam
JCUJW-səj.
0.34
0.34
0.24
0	25	50	75	100 125 150 175 200
Epochs
(a) VGG16on CIFAR-10
---AdamS
——AdamW
---Adam
JCUJW-səj.
0.2B
0.26
0.24
0.22
0	25	50	75 100 125 150 175 200
Epochs
IOO 125 150 175 200	O 25
Epochs
(b) ResNet34 on CIFAR-100
75 100 125 150 175 200
Epochs
(c) DenseNet121 on CIFAR-100
---AdamS
---AdamW
---Adam
0	25	50	75 100 125 150 175 200	0	25	50	75 100 125 150 175 200
Epochs	印 OChS
JCUJW W8F



Figure 4: The learning curves of AdamS, AdamW, and Adam on CIFAR-10 and CIFAR-100. Top
Row: Test curves. Bottom Row: Training curves. AdamS shows significantly better generalization
than AdamW and Adam.
Proposition 5. For the weight decay rate of AdamS in Equation 12, the L2 norm of the vector
coefficient of λ is equal to the space dimensionality, which is constant during training.
Based on Propositions 4 and 5, we argue that SWD is more stable than decoupled weight decay in
adaptive gradient methods in the sense of the magnitude.
4 Empirical Analysis and Discussion
In this section, we empirically study how the SWD-corrected optimizers are compared with conven-
tional optimizers. We choose Adam as the base optimizer, and train popular deep models, including
ResNet18/ResNet34 (He et al., 2016), VGG16 (Simonyan & Zisserman, 2014), DenseNet121 (Huang
et al., 2017), GoogLeNet (Szegedy et al., 2015), and Long Short-Term Memory (LSTM) (Hochreiter
& Schmidhuber, 1997), on CIFAR-10/CIFAR-100 (Krizhevsky et al., 2009) and Penn TreeBank
(Marcus et al., 1993). The implementation details can be found in Appendix B.
Empirical results. Figure 4 shows the learning curves of AdamS, AdamW, and Adam on several
benchmarks. In our experiments, AdamS always leads to lower test errors, and sometimes leads to
lower training losses. However, Figure 5 shows that, even if with similar or higher training losses,
AdamS still generalizes significantly better than AdamW, Adam, and popular Adam variants. Figure
6 displays the learning curves of all adaptive gradient methods. The test performance of other models
can be found in Table 1. Simply fixing weight decay in Adam by SWD even outperforms recent Adam
variants. Figure 7 further demonstrates that AdamS consistently outperforms Adam and AdamW
under various weight decay hyperparameters. According to Figure 7, we also notice that the optimal
decoupled weight decay hyperparameter in AdamW can be very different from L2 regularization
and stable weight decay, but the corresponding optimal weight decay rates of three weight decay
implementations are often similar. Finally, AdamS also outperforms AdamW and Adam significantly
on the Language Modeling experiment, seen in Figure 8. We leave the experiments with cosine
annealing schedulers and warm restarts (Loshchilov & Hutter, 2016) in Appendix F, which also
support that AdamS yields superior test performance.
Popular Adam variants often generalize worse than SGD. Some papers (Luo et al., 2019; Chen
& Gu, 2018) argued that their proposed Adam variants may generalize as well as SGD. However,
we found that this argument is contracted with our comparative experimental results, such as Table
7
Under review as a conference paper at ICLR 2021
Figure 5: The scatter plot of
training losses and test errors
during final 40 epochs of train-
ing ResNet34 on CIFAR-100.
Even with similar or higher train-
ing losses, AdamS still general-
izes better than other Adam vari-
ants. We leave the scatter plot
on CIFAR-10 in Appendix D.
∙IOJi-səj.
---AdamS
---AdamW
---Adam
—AMSGrad
AdaBound
---Padam
YogI
RAdam
0	25	50	75 100 125 150 175 200
Epochs
Figure 6: The learning curves
of all adaptive gradient methods
by training ResNet34 on CIFAR-
100. AdamS outperforms other
Adam variants. The test perfor-
mance of other models can be
found in Table 1.
xl-×3-e9dslBJ.
25	50	75 100 125 150 175 200
Epochs
(a)	Test Perplexity
Jou"'-səɪ
Figure 7: The test errors of
VGG16 on CIFAR-10 with var-
ious weight decay rates. The
displayed weight decay value
of AdamW has been rescaled
by the bias correction factor ≈
0.001. A similar experimental
result for ResNet34 is presented
in Appendix D.
Ooooooo
6 4 2 O B 6 4
×3-9d 6u-u-eu
20
0	25	50	75 100 125 150 175 200
Epochs
(b)	Training Perplexity
Figure 8:	Language Modeling. The learning curves of AdamS, AdamW, and Adam for LSTM on
Penn TreeBank. AdamS has better test performance than AdamW and Adam for LSTM. The optimal
test perplexity of AdamS, AdamW, and Adam are 69.90, 72.88, and 77.01, respectively. Note that
the lower perplexity is better.
1.	In our empirical analysis, these advanced Adam variants may narrow but not completely close
the generalization gap between adaptive gradient methods and SGD. SGD with a fair weight decay
hyperparameter as the baseline performance usually generalizes better than recent adaptive gradient
methods. The main problem may lie in weight decay. SGD with weight decay λ = 0.0001, a common
setting in related papers, is often not a good baseline, as λ = 0.0005 often shows better generalization
on CIFAR-10 and CIFAR-100, seen in Figures 2, 10 and 7. We also conduct comparative experiments
with λ = 0.0001, seen in Table 3 of Appendix D. Under the setting λ = 0.0001, while some existing
Adam variants may outperform SGD sometimes due to the lower baseline performance of SGD,
AdamS shows superior test performance. For example, for ResNet18 on CIFAR-10, the test error of
AdamS is lower than SGD by nearly one point and no other Adam variant may compare with AdamS.
Two metrics for weight decay. The main effect of weight decay for deep learning dynamics can
be reflected by these two metrics: the weight decay rate R and the total weight decay effect ρ. We
summary two principles about weight decay critically related to performance. First, we need to make
the weight decay rate as stable as possible during training. Second, we need choose the optimal
weight decay hyperparameter to maintain the optimal total weight decay effect. These two principles
lead to two linear scaling rules.
Two linear scaling rules. We report two kinds of linear scaling rules of weight decay and learning
rates for selecting the weight decay hyperparameter λ. First, as Proposition 1 and Figure 1 indicate,
the learning rate scheduler should be applied to both the learning rate and weight decay in the form
of -ηtλθt-1 during training. Second, the initial learning rate and the weight decay hyperparameter λ
8
Under review as a conference paper at ICLR 2021
AdamS
IO-2
10-3
IO-4
IO-5 .	.	.	.
IO-6	10-5	10-4	IO-3
Weight Decay
seu 6u-u∙le9η
seu 6u 一
Adam
6.48
6.26
6.04
5.82
5.60
5.38
5.16
4.94
4.72
4.50
10-5	10-4 IO-3
Weight Decay
Figure 9:	The test errors of ResNet18 on CIFAR-10. AdamS has a much deeper and wider basin near
dark points (≤ 4.9%). The optimal test error of AdamS, AdamW, and Adam are 4.52%, 4.90%, and
5.49%, respectively. The displayed weight decay value of AdamW has been rescaled by the bias
correction factor ≈ 0.001.
are linearly coupled for the total weight decay effect. The expression of the total weight decay effect
in Equation 5 mainly depends on the weight decay hyperparameter λ, the learning rate η, and the
number of iterations t. Usually, people do not fine tune the number of iterations t. Given a fixed t and
the optimal weight decay rate R, the total weight decay effect ρ approximately depends on ηλ. The
second linear scaling rule indicates that, in the procedure of fine-tuning hyperparameters, we should
try larger λ for smaller initial learning rates to maintain the optimal total weight decay effect. We
observe that the blue basin of AdamS and AdamW in Figure 9 both approximately reflect the linear
scaling rule, which was also reported by Van Laarhoven (2017); Loshchilov & Hutter (2018); Li et al.
(2020). We point that its theoretical mechanism closely relates to the total weight decay effect.
A novel interpretation of weight decay. The effect of weight decay can be interpreted as iteration-
wisely flattening the loss landscape and increasing the learning rate at the same time. Thus, it is not
surprising that increasing weight decay and increasing learning rate have some similar influence on
optimization performance (Van Laarhoven, 2017; Zhang et al., 2018; Hoffer et al., 2018). However,
simply increasing learning rates cannot replace weight decay in practice. For weight decay, increasing
the learning rate by a factor of (1 - ηλ)-2 is iteration-wise during training, and flattening the loss
landscape by a factor of (1 - ηλ) per iteration must happen at the same time. This interpretation of
weight decay actually controls the adaptivity of learning rates in learning dynamics, as the total weight
decay effect ρ is isotropic along different dimensions. This may explain why stable weight decay can
effectively fix the ill-conditioned flat minima selection of adaptive gradient methods revealed by Xie
et al. (2020b) from a diffusion perspective (Xie et al., 2020a).
5 Conclusion
While Loshchilov & Hutter (2018) discovered that weight decay should be decoupled from the
gradients, we further discovered that weight decay should be coupled with the effective learning
rate. We proposed the SWD method which applies the bias correction on decoupled weight decay
to make the weight decay rate more stable during training. The empirical results demonstrate that
SWD makes significant improvements over L2 regularization and decoupled weight decay (when
they are different). Particularly, we observe that the advantage of existing Adam variants is very
weak compared with the performance improvement by simply fixing weight decay. The standard
Adam with SWD, with no extra hyperparameter, usually outperforms complex Adam variants, such
as Padam and AdaBound, which have more hyperparameters. Although our analysis mainly focused
on Adam, SWD can be easily combined with other Adam variants, too. The proposed principle and
method of stable weight decay expands our understanding towards the role of weight decay in deep
learning. It will be interesting to further investigate the theory for the weight decay rate in future.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
9
Under review as a conference paper at ICLR 2021
machine learning. In 12th {USENIX} symposium on operating systems design and implementation
({OSDI} 16),pp. 265-283, 2016.
Jinghui Chen and Quanquan Gu. Closing the generalization gap of adaptive gradient methods in
training deep neural networks. arXiv preprint arXiv:1806.06763, 2018.
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,
Chiyuan Zhang, and Zheng Zhang. Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems. arXiv preprint arXiv:1512.01274, 2015.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(Jul):2121-2159, 2011.
StePhen Jose Hanson and Lorien Y Pratt. Comparing biases for minimal network construction with
back-propagation. In Advances in neural information processing systems, pp. 177-185, 1989.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a overview of mini-batch gradient descent. 2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In Advances in Neural Information Processing Systems,
pp. 2160-2170, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 3rd International
Conference on Learning Representations, ICLR 2015, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in
neural information processing systems, pp. 950-957, 1992.
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33, 2020.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2019.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. 7th International Conference on Learning Representations, ICLR 2019,
2019.
Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.
10
Under review as a conference paper at ICLR 2021
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Advances in neural information processing systems, pp.
8026-8037, 2019.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. 6th
International Conference on Learning Representations, ICLR 2018, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Seiya Tokui, Ryosuke Okuta, Takuya Akiba, Yusuke Niitani, Toru Ogawa, Shunta Saito, Shuji Suzuki,
Kota Uenishi, Brian Vogel, and Hiroyuki Yamazaki Vincent. Chainer: A deep learning framework
for accelerating the research cycle. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pp. 2002-2011, 2019.
Twan Van Laarhoven. L2 regularization versus batch and weight normalization. arXiv preprint
arXiv:1706.05350, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for minima selection: Stochastic
gradient descent escapes from sharp minima exponentially fast. arXiv preprint arXiv:2002.03495,
2020a.
Zeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama. Adai: Separating the
effects of adaptive learning rate and momentum inertia. arXiv preprint arXiv:2006.15815, 2020b.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-9803,
2018.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329, 2014.
SK Zavriev and FV Kostyuk. Heavy-ball method in nonconvex optimization problems. Computational
Mathematics and Modeling, 4(4):336-341, 1993.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. In International Conference on Learning Representations, 2018.
A	Proofs
A. 1 Proof of Proposition 1
Proof. In the presence of learning rate schedulers, we have the updating rule of Equation-1-based
weight decay as
θt = (1 - λ0)θt-1 - ηtgt.	(13)
According to the definition of the weight decay rate, We have R = 1 - λ0 at t-th step.
Obviously, the weight decay rate will change with learning rate schedulers.
The proof is now complete.	口
11
Under review as a conference paper at ICLR 2021
A.2 Proof of Proposition 2
Proof. We first show how L2 regularization and weight decay are different in the presence of
Momentum.
We may rewrite HBM with L2 regularization as
mt = β1mt-1 + β3gt
θt	θt = θt-1 - ηmt - ηλL2β3 Qk = I βk 1θt-k = θt-1 - ηmt - ηλL2β3 1-βl hθt-1i,
(14)
where we denote the exponential moving average of past θ	ashθt-1i=1-ββt Qk=ι βk 1θt-k.
Although hθt-1i is an approximated value of θt-1, L2 regularization use hθt-1i instead of the
standard θt-1 in weight decay introduces undesirable noise into learning dynamics of SGD with L2
regularization.
Ashθt-1i 6= θt-1, obviously, we may not recover weight decay in Equation 2 from L2 regularization.
This naturally means that L2 is not identical to weight decay.
The weight decay rate of SGD with L2 is given by
R = 1 - λL2β31—βt Mt-IiΘt-1,	(15)
where 1 is the all-ones vector and θt--11 is the element-wise inverse ofθt-1.
The difference between θt-1 and hθt-1i also indicates that the weight decay rate cannot be constant
during training.
The proof is now complete.
□
A.3 Proof of Proposition 3
Proof. According to the definition of the weight decay rate, the weight decay rate of AdamW
dynamics is
RW = 1 - λv 2,	(16)
where 1 is the all-ones vector.
As V-2 is adaptive, RW is not constant during training.
The proof is now complete.	□
A.4 Proof of Propositions 4 and 5
Proof. By Equation 9 and Equation 12, we have the weight decay rate of AdamW as
RW = 1 - λv 2,	(17)
and the weight decay rate of AdamS as
_1	1
RS = 1 - λv- 2 V.	(18)
We note that λ in Equation 12 (AdamS) has a vector coefficient as Vt - V, while λ in Equation 9
(AdamW) has a vector coefficient as Qt.
_ 1	1
It is easy to compute the L2 norms of two vector coefficients”® 2 V ∣∣2 equals the dimensionality
1
n in AdamS dynamics and Ilvt2 k 2 equals the sum of all elements in Vt in AdamW dynamics.
It means λ in AdamS has a magnitude-constant coefficient, while λ in AdamW has a magnitude-
varying coefficient.
The proof is now complete.	□
12
Under review as a conference paper at ICLR 2021
Table 2: Test performance comparison of Adai, AdaiS, SGD, and SGDS. Stable/Decoupled Weight
Decay often outperform L2 regularization for optimizers involving in momentum. We report the
mean and the standard deviations (as the subscripts) of the optimal test errors computed over three
runs of each experiment.
Dataset	Model	AdaiS	Adai	SGDS	SGD
CIFAR- 1 0	ResNet18	4.59o.i6	4.74o.14	4.69o.o9	5.01o.o3
	VGG16	5.81o.07	6.00o.o9	6.280.07	6.42o.o2
CIFAR- 1 00	DenseNet 1 2 1	19.44o.2i	19.59o.38	19.61o.26	19.81o.33
	GoogLeNet	20.50o.25	20.55o.32	20.68o.o3	21.21o.29
B Experimental Details
B.1	Image Classification
Data Preprocessing: We perform the common per-pixel zero-mean unit-variance normalization,
horizontal random flip, and 32 × 32 random crops after padding with 4 pixels on each side.
Hyperparameter Settings: We select the optimal learning rate for each experiment from
{0.001, 0.01, 0.1, 1, 10} for non-adaptive gradient methods and use the default learning rate for
adaptive gradient methods. In the experiments on CIFAF-10 and CIFAR-100: η = 1 for Adai;
η = 0.1 for AdaiS; η = 0.1 for SGD and SGDS; η = 0.001 for AdamS, Adam, AMSGrad, AdamW,
and AdaBound; η = 0.01 for Padam. For the learning rate schedule, the learning rate is divided by
10 at the epoch of {80, 160} for CIFAR-10 and {100, 150} for CIFAR-100, respectively. The batch
size is set to 128 for both CIFAR-10 and CIFAR-100.
The strength of L2 regularization and SWD is default to 0.0005 as the baseline. Considering the
linear scaling rule, We choose λw = λL2. Thus, the weight decay of AdamW uses λw = 0.5 for
CIFAR-10 and CIFAR-100. As for SWD in AdaiS, we choose λS = 0.005 for achieve a similar
total weight decay effect as SGD. The basic principle of choosing weight decay strength is to let all
optimizers have either a similar weight decay rate R or a similar total weight decay effect ρ to the
baseline L2 regularization.
We set the momentum hyperparameter β1 = 0.9 for SGD and SGDS. As for other optimizer
hyperparameters, we apply the default hyperparameter settings directly.
We leave the empirical results with the weight decay setting λ = 0.0001 in Appendix D.
The source code of AdamS and SGDS in Supplementary Materials is available to the public.
B.2	Language Modeling
We use a classical language model, Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber,
1997) with 2 layers, 512 embedding dimensions, and 512 hidden dimensions, which has 14 million
model parameters and is similar to the “medium LSTM” in Zaremba et al. (2014). Note that our
baseline performance is better than the reported baseline performance in Zaremba et al. (2014). The
benchmark task is the word-level Penn TreeBank (Marcus et al., 1993). We empirically compared
AdamS, AdamW, and Adam under the common and same conditions.
Hyperparameter Settings. Batch Size: B = 20. BPTT Size: bptt = 35. Weight Decay: λL2 =
λS = 0.00005, and λW = 0.05. Learning Rate: η = 0.001. The dropout probability is set to 0.5.
We clipped gradient norm to 1.
13
Under review as a conference paper at ICLR 2021
0.12
0.10
0.11
.Iallw"gɪ
0.05
0.04
06
io-7
10-6 IO-5 10-4	10-3 IO-2 lθ-ɪ
Weight Decay
Figure 10: We compare the generalization of Vanilla SGD, SGD, SGDW, and SGDS with various
weight decay hyperparameters by training ResNet18 on CIFAR-10. The optimal weight decay rates
are near 0.0005 for all three weight implementations. The optimal performance of SGDS/SGDW
is better than Vanilla SGD and SGD. For Vanilla SGD, SGD, and SGDS, we may safely choose
λL2 = λS = 0.0005. But we have to re-tune λW = 0.005 for SGDW. Hyperparameter Setting:
β1 = 0 for Vanilla SGD; β1 = 0.9 for SGD, SGDW, and SGDS.
Jou"'-səɪ
0.080
0.075
0.070
0.065
0.060
0.055
0.050
0.045
IOO 125 150 175 200
Epochs
Jαu",-j8x
0.085
0.060
0.100
0.095
0.075
0.070
0	2 5	50	75 100 125 150 175 200
Epochs
JQJ"a-φ,i
0	25	50	75 100 125 150 175 200
印 OChS
75
(a)	ResNet18 on CIFAR-10
(b)	VGG16 on CIFAR-10
(c)	GoogLeNet on CIFAR-100
Figure 11:	Generalization analysis on SGDS and SGD with L2 regularization. Hyperparameter
Setting: λS = λL2 = 0.0005 and β1 = 0.9.
JOJJ"'-səɪ
0	25	50	75 100 125 150 175 200
Epochs
(a)	ResNet18 on CIFAR-10
JOJJ"'-səɪ
0	25	50	75 100 125 150 175 200
Epochs
(b)	VGG16on CIFAR-10
JQJJW-səl
0	25	50	75 100 125 150 175 200
印 OChS
(c)	GoogLeNeton CIFAR-100
Figure 12:	Generalization analysis on AdaiS and Adai with L2 regularization. Hyperparameter
Setting: λS = λL2 = 0.0005.
14
Under review as a conference paper at ICLR 2021
0	25	50	75 100 125 15C 175 200
Epochs
(a) ResNet18
Adams
——AdimW
—Adam
——AMSGrad
AdaBownd
---Padam
Yogi
RAdam
-OJa⅛φh
0	25	50	75 100 125 150 175 2∞
Epochs
(b) VGG16
-OJa⅛φh
0.34
0.32
0.30
0.24
0.22
——MvrS
——AdimW
---Adam
——AMSGrad
AdaBowixi
——Padam
Yogl
RAdam
0	25	50	75 100 125 15C 175 200
Epochs
(c) DenseNet121
034
032
030
028
024
022
0	25	50	75 100 125 150 175 200
Epochs
(d) GoogLeNet
Ss9CU-U-∙u
0.04	0.05	0.06	0.07	0.08	0.09	0.10
Test Errors
Figure 13: The learning curves of adaptive gradient methods.
Ss9CU-U-∙u
10~3
0.20	0.22
0.24	0.26	0.2β	0.30
Test Errors
Figure 14:	Even if with similar or higher training losses, AdamS still generalizes better than AdamW
and Adam. The scatter plot of training losses and test errors during final 50 epochs of training VGG16
on CIFAR-10 and DenseNet121 on CIFAR-100.
C Supplementary Figures and Results of SGD with Momentum and
Adai
We compare SGDS, SGDW, vanilla SGD and SGD under various weight decay hyperparameters by
training ResNet18 on CIFAR-10. We observe that the optimal performance of SGDS/SGDW is better
than vanilla SGD and SGD in Figure 10. The advantage of SGDS over SGDW is that we do not need
to fine-tune the weight decay hyperparameters based on 1-β3^.
We report the learning curves of SGD and SGDS in Figure 11. SGDS compares favorably with SGD.
Based on a diffusion theoretical framework (Xie et al., 2020a), Xie et al. (2020b) proposed Adaptive
Inertia Estimation (Adai) that uses adaptive momentum inertia instead of Adaptive Learning Rate to
help training. Adaptive inertia can be regarded as an inertia-adaptive variant of HBM. The previous
analysis on HBM can be easily generalized to Adai. We display Adai with SWD (AdaiS) in Algorithm
6. We report the learning curves of Adai and AdaiS in Figure 12, which may verify the generalization
advantage of AdaiS over Adai.
D Supplementary Figures and Results of Adaptive Gradient
Methods
We report the learning curves of all adaptive gradient methods in Figure 13. They shows that vanilla
Adam with SWD can outperform other complex variants of Adam.
Figure 14 displays the scatter plot of training losses and test errors during final 40 epochs of training
DenseNet121 on CIFAR-100.
Figure 15 displays the test performance of AdamS, AdamW, and Adam under various weight decay
hyperparameters of ResNet34 on CIFAR-100.
We train ResNet18 on CIFAR-10 for 900 epochs to explore the performance limit of AdamS, AdamW,
and Adam in Figure 16.
15
Under review as a conference paper at ICLR 2021
JalJ",-səɪ
Figure 15:	We compare the generalization of Adam, AdamW, and AdamS with various weight decay
rates by training ResNet34 on CIFAR-100. The displayed weight decay of AdamW in the figure
has been rescaled by the bias correction factor ≈ 0.001. The optimal test performance of AdamS is
significantly better than AdamW and Adam.
JCUJW bh
0.04
0 100 200 300 400 500 600 700 SOO 900
Epochs
Figure 16:	We train ResNet18 on CIFAR-10 for 900 epochs to explore the performance limit of
AdamS, AdamW, and Adam. The learning rate is divided by 10 at the epoch of 300 and 600. AdamS
achieves the most optimal test error, 4.70%.
Table 3 displays the test performance with λ = 0.0001, which is a common weight decay setting in
related papers.
Finally, we present the learning curves for language modeling experiments in Figure 8. As we expect,
AdamS outperforms AdamW and Adam again.
E Additional Algorithms
Algorithm 5 is the TensorFlow implementation for SGD.
Algorithm 6 is the implementation of Adai, AdaiW, and AdaiS. As 1-β3^ is always 1 in Adai, AdaiW
is identical to AdaiS.
16
Under review as a conference paper at ICLR 2021
Table 3: Test performance comparison of optimizers with λL2 = λS = 0.0001 and λW = 0.1.
AdamS still show better test performance than popular adaptive gradient methods and SGD.
DATASET	MODEL	SGD	ADAMS	ADAM	AMSGRAD	ADAMW	ADABOUND	PADAM	YOGI	RADAM
CIFAR-1 0	RESNET18	5.58	4.69	6.08	5.72	5.33	6.87	5.83	5.43	5.81
	VGG16	6.92	6.16	7.04	6.68	6.45	7.33	6.74	6.69	6.73
CIFAR-1 00	DENSENET12 1	20.98	21.35	24.39	22.80	22.23	24.23	22.26	22.40	22.40
	GOOGLENET	21.89	21.60	24.60	24.05	21.71	25.03	26.69	22.56	22.35
We note that the implementation of AMSGrad in Algorithm 7 is the popular implementation in
PyTorch. We use the PyTorch implementation in our paper, as it is widely used in practice.
Algorithm 5: SGD in TensorFlow
gt = VL(θt-1) + λθt-i;
mt = β1mt-1 - ηgt;
θt = θt-1 + mt;
Algorithm 6: Adai /AdaiS=AdaiW
gt = VL(θt-1) +λθt-1;
vt = β2vt-1 + (1 - β2)gt2;
V — Vt ∙
Vt = i-βt ；
Vt = mean (V t);
βιt = (I - β0Vt).Cliρ(0, 1 - e);
mt = β1tmt-1 + (1 - β1t)gt;
m,——_____mt____■
mt= 1-Qk=1 βιk
Ot = θt-ι 一 ηmt 一 ηλθt-ι;
Algorithm 7: AMSGrad/AMSGradW		Algorithm 8: AMSGradS	
gt =	VL(θt-ι)+ λθt-i	gt =	VL(θt-ι);
mt =	β1mt-1 + (1 一 β1)gt;	mt =	β1mt-1 + (1 一 β1)gt;
Vt =	β2Vt-1 + (1 一 β2)gt2;	Vt =	β2Vt-1 + (1 一 β2)gt2;
m t =	mt. - 	 ι-βt;	m t =	mt - - 	 1-βt;
Vmax	= max(Vt, Vmax);	Vmax	= max(Vt, Vmax);
Vt =	Vmax ∙ 1-βt ;	Vt =	Vmax • 1-βt ;
θt =	θt-i 一 √η+ mt 一 ηλθt-ι;	Vt =	mean (Vt);
			 ∩ 		θt-ι — √η-mt 一 √‰λθt-i; t 1	^+ +e t	√vt t 1
		θt =	
F Experiments with Cosine Annealing Schedulers and Warm
Restarts
In this section, we conducted comparative experiments on AdamS, AdamW, and Adam in the presence
of cosine annealing schedulers and warm restarts proposed by Loshchilov & Hutter (2016). We set
the learning rate scheduler with a recommended setting of Loshchilov & Hutter (2016): T0 = 14
and Tmul = 2. The number of total epochs is 210. Thus, we trained each deep network for four
runs of warm restarts, where the four runs have 14, 28, 56, and 112 epochs, respectively. Other
hyperparameters and details are displayed in Appendix B.
Our experimental results in Figures 17 and 18 suggest that AdamS consistently outperforms AdamW
and Adam in the presence of cosine annealing schedulers and warm restarts. It demonstrates that,
with various learning rate schedulers, the advantage of SWD may generally hold.
17
Under review as a conference paper at ICLR 2021
j01iuj-s。1-
0.05
0	25	50	75 100 125 150 175 200	0	25
Epochs
(a)	ResNet18
50	75 100 125 150 175 200
Epochs
(b)	VGG16
SSon 6u-u-eJJ.
10-β
0	25	50	75 100 125 150 175 200
Epochs
Figure 17: The learning curves of ResNet18 and VGG16 on CIFAR-10 with cosine annealing and
warm restart schedulers. The weight decay hyperparameter: λL2 = λS = 0.0005 and λW = 0.5.
Top Row: Test curves. Bottom Row: Training curves. AdamS yields significantly lower test errors
and training losses than AdamW and Adam.
0.04-1.................................
10^6	10^5	10^4 IOT IOT
Weight Decay
(b) VGG16
0.04
Io-6	10^5	10^4 IOT
Weight Decay
(a) ResNet18

Figure 18: The test errors of ResNet18 and VGG16 on CIFAR-10 under various weight decay
with cosine annealing and warm restart schedulers. AdamS yields significantly better optimal test
performance than AdamW and Adam.
18