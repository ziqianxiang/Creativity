Under review as a conference paper at ICLR 2021
Incremental Learning on Growing Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Graphs have attracted numerous attention in varied areas and are dynamic in many
scenarios. Among dynamic graphs, growing graphs with frequently expanding
vertex and edge sets are typical and widely existed, e.g. the rapidly growing so-
cial networks. Confronting such growing data, existing methods on either static or
dynamic graphs take the entire graph as a whole and may suffer from high compu-
tation cost and memory usage due to the continual growth of graphs. To tackle this
problem, we introduce incremental graph learning (IGL), a general framework to
formulate the learning on growing graphs in an incremental manner, where tra-
ditional graph learning method could be deployed as a basic model. We first an-
alyze the problems of directly finetuning on the incremental part of graph, and
theoretically discuss the unbiased and edge-preserved conditions of IGL. In our
method, when the graph grows with new-coming data, we select or generate ver-
tices and edges within restricted sizes from the previous graph to update current
model together with the new data. Here, two strategies, i.e. sample-based and
cluster-based, are proposed for learning with restricted time and space complex-
ity. We conduct experiments on the node classification and link prediction tasks
of multiple datasets. Experimental results and comparisons show that our method
achieves satisfying performance with high efficiency on growing graphs.
1	Introduction
Graph represents complicated relations of data in non-Euclidean domain with topological structure.
Recently, graph neural networks have attracted considerable efforts and been successfully applied
to various fields (Zhang et al., 2020). Most existing research concentrate on static graphs, while in
real-world applications, graphs are generally dynamic in nature. The dynamics of graph include the
addition and removal of vertices and edges, and the change of vertex attributes and edge weights
(Harary & Gupta, 1997). Recent works for dynamic graph learning (Pareja et al., 2020; Manessi
et al., 2020; Xu et al., 2020) consume dynamic structures using recurrent architectures and capture
temporal patterns. However, there are less efforts concentrating on the growing graph, a typical and
widely existed dynamic graphs, with continually added vertices and edges, e.g. social networks and
citation networks. It is a challenging problem to frequently updating models for adapting the growth
of graph with high efficiency, which has not been fully concerned in the literature.
Given a growing graph, we consider the incremental parts as subgraphs of the entire graph, with
edges correspondingly split into intra-edges inside subgraphs and inter-edges among subgraphs, as
shown in Figure 1 (a). For such data, the manner of incremental learning is an efficient strategy,
where we expect to update model based on new subgraphs and consistently perform on the entire
graph observed so far. Compared with incremental learning on data without inter-sample connec-
tions (e.g. images), incremental learning on growing graphs shows the differences that: (i) the
previous vertices and edges must be stored in the database throughout the time, and may be related
with new subgraphs, (ii) using previous data for updating with new data is essential, considering the
utilization of inter-edges among subgraphs. To perform on the entire observed graph at each time,
directly learning on the whole set is a feasible solution but time-consuming due to the continual
growth of the graph. In contrast, simply learning on the new subgraph requires low complexity in
time and space. But such a learning strategy introduces bias to the later-coming subgraphs and drops
the inter-edges among subgraphs, which seriously corrupt the effectiveness of graph learning.
Present work. We present incremental graph learning (IGL) to formulate the incremental learning
on growing graphs and explore efficient learning strategy, as shown in Figure 1 (b). Given a growing
1
Under review as a conference paper at ICLR 2021
(a)
(b)
Figure 1: (a) A growing graph with sequentially added subgraphs. (b) Framework of IGL. At each
time, we train on the new subgraph together with selected or generated vertices and edges from the
old graph within restricted size, and aim to perform on the entire observed graph.
graph {Gt} in training, we generate a graph GtL for updating current model, where the model is mod-
ulized as a backbone and can be implemented by different graph learning methods. Then the model
is expected to perform on the entire observed graph in test. To alleviate the problems of directly
learning on the new subgraphs, we select or generate vertices and edges within restricted sizes from
the old graph, and combine them with current subgraph as GtL . Intuitively, GtL should be unbiased
to the entire graph and preserve enough inter-edges. We then theoretically analyze the unbiased and
edge-preserved conditions for such process, and propose sample-based and cluster-based strategies
to generate GtL under specified memory constraints. Experiments of node classification and link
prediction on multiple datasets are conducted for evaluations.
Comparison with related works. We review related works and present their main differences with
IGL in motivation, target and method designing. (a) Incremental learning continually extends its
learned knowledge based on new data, where the incremental data could be new samples, categories,
domains, etc. Related works on data with independent samples (e.g. images), mainly focus on
incremental categories (Rebuffi et al., 2017) or incremental domains (Li & Hoiem, 2017) to solve
the problem of catastrophic forgetting (McCloskey & Cohen, 1989). Sample-incremental learning
is mostly concerned in traditional machine learning methods and could be naturally conducted by
neural networks with mini-batch training. Though IGL is a sample-incremental learning, the inter-
edges among vertices from different time, i.e. independent samples, make it a challenging problem
for existing methods. (b) Inductive graph learning could be generalized to graph structures different
from the learned one under the same distribution (Hamilton et al., 2017; Zeng et al., 2019), which
is suitable for dynamic graphs to some extent. However, it has to consume the entire graph at each
time to capture the increasing patterns on growing graphs, and could be adapted as backbones in our
IGL framework. (c) Mini-batch based graph learning learns on a large graph by cutting batches of
subgraphs from it with mini-batch training (Chiang et al., 2019). Though also sequentially learning
on subgraphs, the entire graph here keeps accessible for selecting subgraphs that could be trained
multiple times. While in IGL, the split and permutation of subgraphs are specifically given as input.
Contributions. (i) We present IGL, a general framework to address the problem of efficiently
learning on growing graphs in an incremental manner. (ii) We theoretically analyze problems of
baseline solutions, and propose sample-based and cluster-based strategies to generate graphs for
learning within restricted size. (iii) We conduct experiments on multiple datasets in the IGL scenario,
and experimental results show satisfying performance and high efficiency of proposed methods.
2	Problem Definition
Incremental graph learning (IGL) is defined on a growing graph G1 (V1 , E1), G2(V2, E2), ...,
Gt(Vt, Et), ..., where Gt(Vt, Et) indicates the graph at time t with the vertex set Vt and edge set
Et. Edge (vi, vj) in Et represents an undirected connection between vi and vj in Vt. The growth of
graph suggests ∀i < j, Vi ⊆ Vj , Ei ⊆ Ej . The set of new vertices and edges at time t are noted as
Vtnew = Vt - Vt-1, Etnew = Et - Et-1. And Etnew could be split into two subsets: (i) intra-edges
Etintra among vertices in Vtnew, (ii) inter-edges Etinter between vertices in Vtnew and Vt-1.
Considering any element-level graph learning task T, e.g. node classification, with Θ as parameters
of learning model, we define fτ(G, Θ) for measuring Θ,s performance on G. Then the optimization
2
Under review as a conference paper at ICLR 2021
Algorithm 1 Framework of incremental graph learning.
Input： G1(V1, Ei), G2(V2, E),…,Gt(Vt, Et),…// the growing graph
Input: Vmax, Emax	// restrictions to additional size of learned graph
Initialize learning parameters as Θ0
for t = 1, 2, ... do
Vnew JVt- Vt-1, Enew JEt- Et-1
Etintra J {(%, Vj) ∈ Etnew | ”，Vj ∈ Vnew}, Etnter J Enew - Etintra
Generate additional vertex set ∆VL and edge set ∆EL, s.t. ∣∆VL∣ ≤ Vmax, ∣∆EL∣ ≤ Emax
VtL J Vtnew ∪ ∆VtL , EtL J Etintra ∪ ∆EtL
Update parameters Θt J GraphLearning(GtL(VtL, EtL), Θt-1)
end for
target of IGL at time t is Θt = arg maxΘ fT (Gt, Θ). To make our framework capable of being
generalized to different graph learning tasks and adapting existing methods, we suppose to generate
a graph GtL (VtL, EtL) for optimizing Θ with a static graph learning method. The method could be
regarded as a backbone of the framework. A straightforward solution, named as joint training, is
to directly optimize on the entire graph, i.e. GtL = Gt . Due to the growth of graph, joint training
suffers from increasing computation cost and memory usage. Another lightweight choice, finetun-
ing, is to learn on the new subgraph, i.e. GtL = Gtnew(Vtnew, Etintra) and drop the inter edges Etinter,
but dropping edges may seriously corrupt the effectiveness of learning (Dai et al., 2018). To utilize
Etinter with efficient computation, we allow to assign additional memory to expand the vertex and
edge sets of Gtnew within restricted sizes. Based on the above ideas, the framework of IGL is for-
mulated in Algorithm 1. It is noted that we modulize the backbone learning method as the function
GraphLearning(G, Θ), which learns on G with parameters initialized as Θ for the specified task.
We conclude properties of a qualified IGL method as follows: (i) It takes a sequence of grow-
ing graph as input. (ii) During the training stage at each time, the entire graph observed so far is
freely available only in the preprocessing time. Memory size and computation complexity for learn-
ing should be restricted within specified boundaries, or increase relatively slow compared with the
growth of graph. (iii) It performs effectively on the entire graph observed so far in test.
3	Method
3.1	Theoretical Motivations
Suppose the GraphLearning() process in IGL is performed by the graph neural networks consist-
ing of aggregators and updaters (Zhou et al., 2018). Since finetuning learns on each new subgraph
s.t. GtL = Gtnew , it intuitively introduces the following problems: (i) Subgraph bias. Since the
propagation of graph learning is conducted inside each subgraph, the parameters for learning aggre-
gating results tend to the bias of subgraphs rather than the entire graph. (ii) Inter-edges missing.
The inter connections Etinter among different subgraphs are completely dropped during the training,
which blocks the aggregation process through edges and disturbs the effectiveness of graph learning.
To address these problems, following conditions should be considered for GtL (VtL, EtL).
Unbiased Estimation To alleviate the bias of subgraph, the aggregation results of vertices in VtL
for learning should be unbiased estimations of them in the entire graph:
∀v ∈ Vt, E (agg (v, Nt (V) ∩VL) | v ∈ VL) = agg (v,Nt (V)),	⑴
where Nt(V) = {u ∈ Vt | (u, V) ∈ Et}, and agg(V, N) aggregates embeddings from vertices in
N to v, details of which may vary in different methods. For convenience of further analysis, we
consider the mean aggregator (Hamilton et al., 2017) within one layer, and assume that all related
edges to VtL are kept in EtL. Then we show that uniform sampling in Vt satisfies the condition (1):
Theorem 4.1 Suppose agg(v,N) = ∑u∈n∣Ny, andEL = {(u, v) ∈ Et | u, V ∈ VL}. If ∀uι, u2 ∈
Vt, P(u1 ∈ VtL) = P(u2 ∈ VtL), then for any possible size |VtL|, equation (1) holds.
Under specified memory limits, however, keeping equal probability of sampling in Vt will inevitably
reduce the utilization of later-coming subgraphs Gtnew to be O(1/t), and restricts the graph learning
efforts. Thus, we relax the above condition to be soft-unbiased that holds all the Vtnew in VtL and
conducts uniform sampling in Vt-1 for the additional vertex set ∆VtL = Vt - VtL.
3
Under review as a conference paper at ICLR 2021
Graph
Learning
Current
Model
Current
Model
①≥i:①」
①>es
Graph
Learning
CtT / ~ ~、、 Cluster-based Strategy
①>es
Figure 2: Pipeline of our proposed methods for IGL, where we can choose the sample-based or
cluster-based strategy, generating a graph for learning to update current model at each time.
Theorem 4.2 If ∀u ∈ Vtnew, u ∈ VtL, and ∀u1 , u2 ∈ Vt-1, P (u1 ∈ VtL) = P (u2 ∈ VtL), then for
any possible size |VL|, ∀v ∈ Vt, E (agg (v, Nt (V) ∩ VL) | v ∈ VL) equals
E ( IJNt(V)LI) agg (V, Nt (V)) - E ( R-11(V) -VLI) agg(v, M-I(V)).	(2)
Nt(v) ∩VL∣	INt (v) ∩VL∣	' 〃
Let λι = E ( NNt(VVLI), λ = -E (IN-Ivv∩VVLl), Wecouldget λι+λ2 = 1 and the expectation
of aggregation result as λ1agg (V, Nt (V)) +λ2agg (V, Nt-1 (V)), a weighted average of the full ag-
gregation in Gt and Gt-1. Since the initial parameter Θt-1 has been updated for agg (V, Nt-1 (V))
and λ1 ≥ 1, λ2 ≤ 0, the expectation of current aggregation provides an unbiased optimizing direc-
tion toWards target agg (V, Nt (V)). And λ1 → 1, λ2 → 0 When Vmax increases. Thus, We present
an soft-unbiased estimation With full utilization of neW subgraphs.
Edge Preservation Since the missing of inter-edges may seriously affect the graph learning re-
sults, We aims at preserving more edges of Etinter in ∆EtL , Which could be formulated as
maxI∆EtL∩EtinterI, s.t. I∆EtLI ≤ Emax.	(3)
∆EtL
The edge-preservation could be required as a definite optimization problem in (3), or sampling With
priority to vertices with higher degrees so that P(u ∈ VL) H ∣{(u, v) ∈ Etinter I V ∈ Vnew }∣,
Which naturally shoWs conflict With the unbiased condition in practice.
Our proposed methods basically follow the unbiased and edge-preserved conditions. We first present
the sample-based strategy (3.2) that selectively satisfies part of the conditions, and the following
cluster-based strategy (3.3) presents a cluster-graph that mediately satisfies both of them.
3.2	Sample-based S trategy
We consider sampling a subset ∆VtL from Vt-1 within size of Vmax, and preserving all the related
edges, i.e. Emax = (IVtnewI + Vmax)2 - IEtintra I by default.
∆VtL = SampleV ertices(Gt-1, Vmax), ∆EtL = {(u, v) ∈ Etinter Iu∈ ∆VtL,v∈Vtnew}, (4)
where the sampling function SampleV ertices() has been widely studied in the literature for se-
lecting a representative subgraph from the original graph (Hu & Lau, 2013). Considering the above
required conditions, we explore following pragmatic methods for appropriate sampling.
Random Selection uniformly selects Vmax vertices from Vt-1, which absolutely follows the unbi-
ased condition, but ignores to preserve enough edges for learning, especially in sparse graphs.
Random Jump is a traversal-based sampling (Leskovec & Faloutsos, 2006) and we adapt it in the
following steps: Starting with any vertex in Vtnew, we either randomly walk to a neighboring vertex
in Vt-1 with probability p and select it, or randomly jump to a vertex in Vtnew with probability
(1 - p). Repeat to fill the sampled set. Zhao et al. (2019) show that the probability of sampling a
vertex tends to be proportional to its degree, which partly meets the edge-preserved condition.
Degree-based Selection is inspired directly from the edge-preserved condition to sample with pri-
ority to vertices related to more inter-edges. Let Dt(u) = I{(u, v) ∈ Et}I be degree of u, we define
Dnew(U) = Dt(UD-(Dt-I(U), ∀u ∈ Vt-ι as new degree of vertices to reflect their closeness to the
new subgraph through inter-edges. Then we select top-Vmax vertices in Vt-1 by their new degrees.
4
Under review as a conference paper at ICLR 2021
The above methods consider only part of required conditions. It can be proved that, ignoring the
ideal case when all the vertices in Vt-1 connect with same number of vertices in Vtnew, such sam-
pling in (4) satisfies the two required conditions iff all the vertices are sampled, i.e. joint training.
Theorem 4.3 Suppose ∃u1, u2 ∈ Vt-1, s.t. Dt(u1) - Dt-1 (u1) 6= Dt(u2) - Dt-1(u2). Then the
unbiased and edge-preserved conditions hold iff ∀u ∈ Vt-1, P (u ∈ ∆VtL) = 1, i.e. VtL = Vt.
3.3	Cluster-based Strategy
Theorem 4.3 suggests that the sample-based strategy could not require both unbiased estimation and
edge preservation. In this section, we relax the basic assumption in sample-based strategy that GtL
should be a subgraph of Gt , and construct a cluster-graph to mediately satisfies both conditions.
Suppose vertices in Vt-1 are arranged into K cluster sets {Cit-1}iK=1 with centers {cit-1}iK=1, where
CtT = |Ji| ∑v∈c⅛-ι v. Then the cluster-graph is defined as
f ∆VL = {c1-1,...,cK-1}
∆EtL ={(cit-1, v) | v ∈ Vtnew, ∃ u ∈ Cit-1, (u, v) ∈ Etinter} ∪	,	(5)
〔	{(ct-1, cj-1) | ∃ Ui ∈ Ct-1, U2 ∈ Ct-1, (uι, U2)∈ Et-1}
which suggests that the cluster centers are added as new cluster-vertices, and the edges connecting
to any vertex in Vt-1 are directly transferred to the corresponding cluster-vertex. It is noted that the
additional edge sets in equation (5) represent Etinter and Et-1, respectively.
In the cluster-graph, Etinterare approximately preserved by connecting cluster-vertices. For the un-
biased condition, here Vt-1 are not directly included in VtL. Let i(U) = k if U ∈ Ckt-1, we could
replace P(U ∈ VtL) for calculating the expectation of aggregation by ⅛ P(Ct-I) ∈ VtL )
1
F,
with an error proportional to the tightness of clustering. Thus, the cluster-vertices tend to uniform
sampling from Vt-1 when the clusters are within the same size, i.e. balanced clustering.
Due to the continual growth of graph, direct clustering on the entire graph is time-consuming. For
an approximate but efficient clustering with balanced size, we first conduct clustering on the new
vertices Vnew for cluster sets {∆Ct}K=I and corresponding centers {^t}K=ι. Bipartite matching
algorithm is applied to optimize a bijective marching function M(∙) : {1,...,K} → {1,...,K} for
the objective: minm(,)∑3ι∣∣ck 1 - Cim(k) k22 , which assigns new clusters to closer old clusters.
Then we merge the clusters as Ckt = Ckt-1 ∪ ∆Cmt (k) and update the center ctk correspondingly.
We adapt two methods for the implementation of balanced clustering: (i) random grouping that
uniformly assigns vertices into groups as a baseline method, (ii) the constrained k-means cluster-
ing algorithm (Bradley et al., 2000) with the restricted minimum cluster size b|Vtnew |/Kc.
3.4	Information Theory Analysis
We present a theoretical analysis to evaluate the effectiveness of methods from the perspective of
information theory. The old graph is regarded as unknown variables and we compare the quantities
of information provided by different methods. Formally, old vertices are simplified as n random
variables X = {xi}in=1 in {0, 1}. We use the information entropy in Boltzmann formula that S =
kp logΩ, where kB is a constant and Ω is the number of microstates (i.e. possible states of all
variables). Specifically, the entropy in finetuning and joint training are kB log 2n and kB log 1 = 0.
We regard the proposed methods as r constraints e1, ..., er to X, and the entropy as S(e1, ..., er), so
the quantities of information ∆S = S0 - S(e1, ..., er) and we compare ∆S as follows.
Theorem 4.4 Let f(i) denotes the equation xi = v, and g(i, j) denotes the equation Σjk=ixk = m,
where v, m is given by the real distribution of X. Let l1, l2, ..., lr be the size of clusters, where
Σir=1li = n and Lk = Σik=1li. Then the following inequalities holds:
Sc = S(g(1, n),g(n + 1, 2n),...,g((r-1n + 1,n)) ≤ S(f(1),f(2),...,f(r)),
r r r	r	(6)
E(Sc) ≤ E(S(g(1, L1), g(L1 + 1, L2),..., g(Lr-1 + 1, n))).
Theorem 4.4 shows that the cluster-based strategy with balanced clustering provides larger ∆S, i.e.
more information quantities. Detailed discussions and proofs are in Appendix E.4.
5
Under review as a conference paper at ICLR 2021
Table 1: Average accuracy (%) of node classification in IGL. The value T under each dataset
denotes the temporal length of growing graph. Top-3 results are marked in bold red, blue and black.
Method	Cora (T = 10)	Citeseer (T = 20)	Pubmed (T = 20)	Flickr (T = 50)	Reddit (T = 100)
finetuning (lower bound)	39.69 ± 2.19	46.94 ± 1.82	52.61 ± 4.22	56.26 ± 0.56	57.42 ± 3.07
Bounds joint training (upper bound)	58.85 ± 0.76	56.56 ± 0.98	67.18 ± 0.61	63.45 ± 0.38	89.30 ± 0.27
EWC (Kirkpatrick et al., 2017)	41.23 ± 1.51	46.65 ± 1.67	58.00 ± 3.27	56.30 ± 1.85	73.30 ± 2.23
Compared LwF (Li & Hoiem, 2017)	42.97 ± 2.35	46.33 ± 1.36	55.44 ± 4.57	56.83 ± 1.38	56.27 ± 1.44
ILMthd	iCaRL (Rebuffi et al., 2017)	50.11 ± 2.03	53.93 ± 1.32	57.73 ± 0.97	57.58 ± 1.23	83.52 ± 0.57
IL Methods TEM (Chaudhry et al., 2019b)	48.22 ± 1.46	52.30 ± 1.40	58.41 ± 1.85	55.74 ± 0.78	74.12 ± 0.60
A-GEM (Chaudhry et al., 2019a)	50.25 ± 1.54	52.52 ± 1.58	62.44 ± 1.68	54.55 ± 2.24	79.57 ± 0.67
Ours	sample-random	49.96 ± 1.43	52.42 ± 1.79	61.73 ± 1.53	56.77 ± 0.59	79.40 ± 0.18
(sample)	sample-random jump	50.27 ± 1.67	52.23 ± 1.63	61.27 ± 1.74	56.53 ± 0.85	80.70 ± 1.00
sample-new degree	54.90 ± 1.17	53.81 ± 1.65	62.75 ± 1.21	58.17 ± 0.32	78.25 ± 1.24
Ours	cluster-random	56.22 ± 1.10	53.99 ± 1.59	65.91 ± 1.02	60.78 ± 0.35	86.03 ± 0.09
(cluster-)	cluster-cons.KMeans	56.66 ± 0.91	54.50 ± 1.76	65.64 ± 1.18	60.70 ± 0.44	86.10 ± 0.09
4 Experiments
4.1	Experimental Setup
Datasets. We evaluate the proposed methods on: (i) the citation networks Cora, Citeseer and
Pubmed (Sen et al., 2008) with documents as vertices and citation relations as edges, (ii) the online
image network Flickr (Zeng et al., 2019) with image vertices connected with common properties,
(iii) the social network Reddit (Hamilton et al., 2017) with posts vertices connected with common
users’ comments. Since IGL is required to perform on any input of growing graphs, we randomly
split and permute the vertices into T groups to simulate the growth of graph.
Tasks. Two common graph learning tasks in element-level are conducted: (i) node classification in
the semi-supervised manner that predicts categories of nodes with only a small set of nodes labeled
for learning (Yang et al., 2016), (ii) link prediction that predicts the existence of connection between
two vertices based on their learned embeddings (Kipf & Welling, 2016).
Compared methods. The results of finetuning and joint training are provided as approximate lower
and upper bounds for reference. Though existing incremental learning (IL) methods mainly focus
on incremental category or domain scenarios, and are not completely suitable for IGL, we adapt rep-
resentative methods in recent years for a comprehensive comparison, including the regularization-
based methods EWC (Kirkpatrick et al., 2017) and LwF (Li & Hoiem, 2017) without using previous
data, and the replay-based methods iCaRL (Rebuffi et al., 2017), TEM (Chaudhry et al., 2019b)
and A-GEM (Chaudhry et al., 2019a) that stores samples from previous data. In comparison, our
proposed sample and cluster based strategies are noted starting with “sample-” and “cluster-”.
Details of the datasets, configurations and compared methods are presented in the appendices.
4.2	Results of Incremental Graph Learning
Node Classification. For the backbone model in GraphLearning() for node classification, we
adapt GCN (Kipf & Welling, 2017) for smaller graphs Cora and Citeseer, and GraphSAGE (Hamil-
ton et al., 2017) for the rest. The total time-steps of growing graph T is specified for each dataset.
For balanced category learning, we set the constraint Vmax = M × number of categories, and con-
duct the sampling and clustering inside each category, and M = 3 in our main experiments. The
influence of the above settings are further studied in Section 4.3, with details in Appendix A.
Table 1 lists the average of classification accuracy on the entire observed graph throughout the time.
Compared with the finetuning, the IL methods in comparison achieve improved but not stable perfor-
mance, since they are not specifically designed for graphs. Our proposed methods achieve satisfying
improvements, where the cluster-based strategy outperforms the sample-based, and steps closer to
the upper bound. Such performance is consistent with theoretical analysis on required conditions,
and verifies that mediately satisfying both the unbiased and edge-preserved condition performs bet-
ter. Additional experiments further explain to such performance that the cluster-based strategy gen-
6
Under review as a conference paper at ICLR 2021
Table 2: Average AUC (%) of link prediction in IGL. The ‘-’ denotes that the results are unavail-
able due to out of memory on large graphs using joint training.
Method	Cora (T = 10)	Citeseer (T = 20)	Pubmed (T = 20)	Flickr (T = 50)	Reddit (T = 100)
finetuning (lower bound) Bounds	66.37 ± 0.47	60.41 ± 0.28	73.12 ± 1.02	57.58 ± 0.04	85.97 ± 2.39
joint training (upper bound)	89.22 ± 0.08	93.55 ± 0.12	79.92 ± 0.05	-	-
EWC (Kirkpatrick et al., 2017)	68.63 ± 1.63	64.37 ± 2.83	76.25 ± 2.80	61.55 ± 3.95	85.02 ± 3.73
Compared LwF (Li & Hoiem, 2017)	71.04 ± 0.82	63.73 ± 1.31	78.08 ± 0.81	57.05 ± 0.62	90.00 ± 1.99
ILMthd	iCaRL (Rebuffi et al., 2017)	72.03 ± 0.63	61.68 ± 0.39	75.86 ± 1.26	57.19 ± 0.22	83.15 ± 3.82
IL Methods TEM (Chaudhry et al., 2019b)	68.30 ± 0.63	62.48 ± 0.39	75.46 ± 1.20	57.88 ± 0.22	86.45 ± 1.16
A-GEM (Chaudhry et al., 2019a)	67.72 ± 0.86	64.32 ± 0.44	76.29 ± 0.97	56.66 ± 0.40	82.34 ± 5.98
Ours	sample-random	68.12 ± 1.20	66.78 ± 0.46	74.74 ± 1.12	57.60 ± 0.24	88.45 ± 1.82
(sample)	sample-random jump	66.93 ± 2.87	66.43 ± 3.06	75.54 ± 1.46	58.13 ± 0.24	87.57 ± 1.76
sample-new degree	70.33 ± 0.31	65.62 ± 1.84	76.08 ± 0.89	57.93 ± 0.18	87.96 ± 2.32
Ours	cluster-random	71.13 ± 0.44	71.45 ± 0.35	79.10 ± 0.41	60.16 ± 0.43	88.48 ± 0.61
(cluster-)	cluster-cons.KMeans	73.60 ± 0.36	68.27 ± 0.39	81.92 ± 0.55	63.51 ± 0.13	90.58 ± 0.61
erates more distinguishable feature distributions (Section 4.4) and preserves more adequate edges
under specified memory restrictions (Section 4.5) for efficient graph learning.
Link Prediction. The basic settings are the same as node classification. We follow Kipf & Welling
(2016) using the reconstruction loss for training and calculate the Area Under Curve (AUC) as
evaluation metric. The memory constraint is set as Vmax = Mall without considering categories,
where Mall = 20 for the citation networks, and Mall = 100 for Flickr and Reddit. The edge sets
Etintra and Etinter are split into training, validation and test sets with the proportion of 8 : 1 : 1.
Details of sampling negative edges for training and other configurations are in Appendix B.
We report the average AUC in Table 2. Similar comparison of results are observed, except that
the cluster-based strategy outperforms joint training on Pubmed, which is attributed to the newly
generated edges incident to the cluster-vertices. It is noted that the clustering here is conducted
among all vertices rather than inside each category, and raises stricter requirement to the tightness
of clustering method. Thus, the difference between random grouping and constrained k-means in
the cluster-based strategy gets larger compared with that in the node classification.
4.3	Ablation Studies
We conduct ablation studies on the basic configurations with node classification to observe the in-
fluence of these factors, and the results are shown in Figure 3: (a) Total time-steps T . With the
increase of T for the same size of graph, finetuning dramatically decreases, while our proposed
methods drop relatively slow like joint training, showing their robustness to the temporal length of
growing graphs. (b) Memory constraint M . The proposed methods show uptrend to joint training
with larger available memory size, and maintain consistent comparison as the main experiments.
(c) Backbone models. We conduct experiments on the same dataset using different backbones,
including GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018), GIN (Xu et al., 2018) and
GraphSAGE (Hamilton et al., 2017). The results suggest that though theoretically analyzed under
mean aggregator and tested with basic backbones, our proposed methods own the ability of adapting
different static graph learning methods to the task of IGL.
(¾1) .00< P① 6更①><
(a)
(b)
Figure 3: Results of ablation studies in node classification. (a) Results of Reddit with different
length of growing graph sequence (in log scale). (b) Results of Reddit under different memory
constraint to size of the graph for learning. (c) Results of Pubmed using different backbone models.
GCN
GAT GIN GraPhSAGE
Backbone model
(c)
7
Under review as a conference paper at ICLR 2021
finetuning sample-random sample-new degree cluster-random ClUSter-cons.KMeans
Figure 4: Graph feature visualization using t-SNE in node classification of Cora at time 5 and 10.
Training vertices are drawn in red, where the cluster vertices are stroked in black.
4.4	Visualization Analysis
We visualize the learned features of vertices to analyze the effectiveness of proposed methods. In
Figure 4, we apply t-SNE (Maaten & Hinton, 2008) to features of observed vertices in node classi-
fication of Cora, and color them with the label information. Features of our proposed methods show
more discernible distribution of categories. To visualize the supervisions, we color the training ver-
tices in red, with the cluster vertices stroked in black. In the cluster-based strategy, the training
vertices turn to be located in those sharp places that are more representative to distinguish different
categories, which could enhance the effectiveness of learning. In comparison, the distribution of
training vertices are more stochastic in finetuning and the sample-based strategy.
4.5	Time and Space Complexity
We record the consumed time of learning, including the preprocessing and training time, to estimate
the computation cost, and count the number of vertices and edges of GtL at each time as the space
complexity. In comparison of proposed methods, the average results of node classification on Reddit
are listed in Table 3, together with the classification accuracy for reference. Results show that it takes
extremely high complexity for joint training to achieve its great performance, while our proposed
methods show comparable performance and meanwhile maintain much lower complexity. And it is
noted that the cluster-based strategy succeeds to preserve more inter-edges under memory restriction,
which demonstrates that the edge-preserved condition is better satisfied for efficient learning.
Table 3: Average time and space complexity of node classification on Reddit.
Method	finetuning	joint training	sample- random	sample- random jump	sample- new degree	cluster- random	cluster- cons.KMeans
Time (s)	25.63	346.37	26.24	26.39	31.77	27.85	28.27
#Nodes (K)	2.33	117.61	2.57	2.57	2.57	2.57	2.57
#Edges (K)	11.51	39,282	14.17	21.18	12.89	43.53	43.38
Acc (%)	57.42	89.30	79.40	80.70	78.25	86.03	86.10
5 Conclusion
In this paper, we study the problem of efficient learning on growing graphs in an incremental man-
ner, and formulate a general framework named incremental graph learning (IGL). We theoretically
analyze the unbiased and edge-preserved conditions for the IGL problem and correspondingly pro-
pose sample-based and cluster-based strategies, which generate a graph within a restricted size for
updating the model to achieve efficient learning at each time. Experimental results of node classifica-
tion and link prediction tasks on growing graphs show that the proposed methods achieve satisfying
performance with high efficiency. Future works may further step into our modulized graph learning
process and adapt IGL to growing graphs with real-world timestamps.
8
Under review as a conference paper at ICLR 2021
References
Paul S Bradley, Kristin P Bennett, and Ayhan Demiriz. Constrained k-means clustering. Microsoft
Research, Redmond, 20(0):0, 2000.
Francisco M Castro, Manuel J Marln-Jimenez, Nicolas GUiL Cordelia Schmid, and Karteek Alahari.
End-to-end incremental learning. In Proceedings of the European conference on computer vision
(ECCV),pp. 233-248, 2018.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with A-GEM. In International Conference on Learning Representations, 2019a.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486, 2019b.
Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-GCN: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
257-266, 2019.
Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on
graph structured data. In Proceedings of the 35th International Conference on Machine Learning,
PMLR, volume 80, 2018.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,
S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December
2017, Long Beach, CA, USA, pp. 1024-1034, 2017.
Frank Harary and Gopal Gupta. Dynamic graph models. Mathematical and Computer Modelling,
25(7):79-87, 1997.
Pili Hu and Wing Cheong Lau. A survey and taxonomy of graph sampling. arXiv preprint
arXiv:1308.5865, 2013.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings, 2017.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery and data mining, pp. 631-636, 2006.
Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
and machine intelligence, 40(12):2935-2947, 2017.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in neural information processing systems, pp. 6467-6476, 2017.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Franco Manessi, Alessandro Rozza, and Mario Manzo. Dynamic graph convolutional networks.
Pattern Recognition, 97:107000, 2020.
9
Under review as a conference paper at ICLR 2021
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology oflearning and motivation, volume 24, pp. 109-165.
Elsevier, 1989.
Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi,
Tim Kaler, Tao B Schardl, and Charles E Leiserson. EvolveGCN: Evolving graph convolutional
networks for dynamic graphs. In AAAI, pp. 5363-5370, 2020.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL:
Incremental classifier and representation learning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 2001-2010, 2017.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings,
2018.
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.
Large scale incremental learning. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 374-382, 2019.
Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive represen-
tation learning on temporal graphs. arXiv preprint arXiv:2002.07962, 2020.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2018.
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In International conference on machine learning, pp. 40-48. PMLR, 2016.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
saint: Graph sampling based inductive learning method. In International Conference on Learning
Representations, 2019.
Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. IEEE Transactions
on Knowledge and Data Engineering, 2020.
Junzhou Zhao, Pinghui Wang, John CS Lui, Don Towsley, and Xiaohong Guan. Sampling online
social networks by random walk with indirect jumps. Data Mining and Knowledge Discovery, 33
(1):24-57, 2019.
Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
arXiv:1812.08434, 2018.
A More Details of Node Classification
A. 1 Configurations
We set the walking probability p = 0.85 in the random jump method. For the backbone model
GraphSAGE, we set the minibatch size as 1000 and the neighborhood sample sizes as 25, 10, which
are the same as the original paper. During the training process, we use the Adam optimizer with
learning rate of 0.01 and weight decay of 0.0005. At each time, we run 100 epochs for training
and validation and load the model with best validation accuracy for test. We run 50 times on Cora,
Citerseer and Pubmed to report the average results with standard derivations, and 5 times on Flickr
and Reddit. The experiments are conducted on one GPU of GeForce GTX 1080Ti.
10
Under review as a conference paper at ICLR 2021
A.2 Pseudo-Code
Given the specified memory constraint Vmax = M × C (number of categories), we apply the
sampling and clustering inside each category with the constraint M . Since the labeling information
is required, the above process is conducted on the training and validation set respectively. The
pseudocode in Algorithm 2 shows details of generating the learned graph in node classification.
Algorithm 2 Process of generating learned graph in node classification.
Input: Vtnew , Etintra , Gt (Vt , Et), Vmax , Emax
Input: Category information CateCory(∙) for the label vertices
Output: The learned graph GtL (VtL = VtL-train ∪ VtL-val ∪ VtL-test, EtL)
ew = Vtnew-train ∪ Vtnew-val ∪ Vtnew-test // get split of new vertices
1 = Vt - Vtnew = Vtt-ra1in ∪ Vtv-al1 ∪ Vtt-es1t // get split of old vertices
nVtVtLVtLE
— train《_ Vnew —train VL-val《_ Vnew—val VL-test《_ Vnew-test
《_ Eintra
for c = 1, 2, ..., C do
VC—1 一{v ∈ Vt-1 | Category(V = c}
Generate Vc-rainVC-val from Vc-I with specified method under constraints of VmCax, Emcax
VL-train《_ VL-train ∪ Vc-train VL-val《_ VL-val ∪ Vc-val
end for
VL4_ VL-train ∪ VL-val ∪ VL-test
EL ~ EL ∪ {(u, V) ∈ Et | u ∈tVL - Vnew, V ∈ Vnew}
return Gtlearn (Vtlearn, Etlearn)
A.3 Backbone Network Architectures
We list the architectures and parameters of the backbone networks in main experiment.
GCNConv(fin, fout) denotes the convolution defined in GCN with the input and output channels
as fin and fout, and SAGE_Conv(fin, fout) denotes the same in GraPhSAGE.
	Cora	Citeseer	Pubmed	Flickr	Reddit
Layer1	Dropout(p=0.5) GCN_Conv(1433, 16) ReLU()	Dropout(p=0.5) GCN-Conv(3707, 64) ReLU()	SAGE-Conv(500, 64) ReLU()	SAGE-Conv(500, 256) ReLU()	SAGE-Conv(602,128) ReLU()
Layer2	Dropout(p=0.5) GCN-Conv(16, 7) SoftMax()	Dropout(p=0.5) GCN-Conv(64, 6) SoftMax()	Dropout(p=0.5) SAGE-Conv(64, 3) SoftMax()	Dropout(p=0.5) SAGE-Conv(256, 2) SoftMax()	Dropout(p=0.5) SAGE-Conv(128,41) SoftMax()
The architectures of backbones for Pubmed in ablation study are as follows.
	GCN	GAT	GIN
Layer1	Dropout(p=0.5) GCN_Conv(1433, 64) ReLU()	Dropout(p=0.6) GATCOnV(1433, 64) ELU()	Dropout(p=0.5) GIN-Conv(1433, 64) ReLU()
Layer2	Dropout(p=0.5) GCN.Conv(64, 3) SoftMax()	Dropout(p=0.6) GATCOnV(64, 3) SoftMax()	Dropout(p=0.5) GINConv(64, 3) SoftMax()
A.4 Classification Accuracy throughout the Time
We show the classification accuracy on the entire observed graph at each time. It is observed that in
the starting time of growing graph with larger T and the whole time of growing graph with smaller
T , the classification accuracy shows uptrend of all methods. This is because in such stage, the
training samples are not enough for models to capture patterns of each categories. While in the later
time, results of finetuning show downtrend or unstable performance due to the forgetting of learned
patterns. In contrast, results of joint training could maintain stable performance for it repeats to train
on all the previous data. For our proposed methods, sample-based strategy solves the problem of
11
Under review as a conference paper at ICLR 2021
forgetting in the later period of time, while leave larger distances to joint training, compared with
cluster-based strategy.
-------------------------------------------joint training -----------finetuning
sample-random -------------------------------------------------------sample-random jump - sample-new degree -cluster-random
ClUSter-ConS.KMeans
(％) Aoe-n84
U。一0SSO
ΦPON
Figure 5: Classification accuracy on the entire observed graph at each time.
Flickr*
—1~~1~~1~~1~~1~~1~~1~~1~~1	20
10	20	30	40	50
Time
Reddit
100
90
80
70
60
50
40
30
0
10 20 30 4 0 50 60 70 80 90 100
Time
B	More Details of Link Prediction
B.1	Configurations
For training with the reconstruction loss on large graphs, we sample 1,000 positive and negative
edges respectively for computing the loss in each iteration. Settings of backbone models are the
same as node classification. During the training process, we use the Adam optimizer with learning
rate of 0.01 and weight decay of 0.0005. At each time, we run 200 epochs for training and validation
and load the model with best validation accuracy for test. We run 50 times on Cora, Citerseer
and Pubmed datasets to get the average results, and 5 times on Flickr and Reddit datasets. The
experiments are conducted on one GPU of GeForce GTX 1080Ti.
B.2	Negative Edge Sampling
The reconstruction loss requires sampled negative edges. For training on small graphs, the negative
edges are directly sampled from those unconnected vertex pairs in each iteration. However, such
sampling process is time-consuming on large graphs, thus we generate a set of negative edges in
advance and randomly sample required number of edges from it. The algorithm for generating
negative edge set is in Algorithm 3.
Algorithm 3 Negative edge sampling on large graphs.
Input: the positive edges EPos and corresponding vertices V
initialize Eneg J φ
for (v1,v2) in Epos do
randomly select v20 from V
Eneg J Eneg ∪ {(v1,v20)}
end for
return Eneg
It is noted that we do not check if the sampled edge belongs to the positive set. Because the checking
process is also time-consuming and the possibility of sampling a positive edge generally is quite low
on large graphs. Since the cluster-based strategy provides new edges connecting clusters, we also
apply negative sampling for those edges with above algorithm. For both small and large graphs,
we generate negative cluster edges during the preprocessing and merge them with normal negative
edges in training. Specially, we find that it shows better performance for the Pubmed dataset to
remove the negative cluster edges in training, and we report the results in the main experiments.
B.3	Network Architectures
We list the architectures and parameters of the backbone networks in main experiment. The back-
bone generates embeddings of vertices that are used to estimate the existence probability of edges.
12
Under review as a conference paper at ICLR 2021
	Cora	Citeseer	Pubmed	Flickr	Reddit
Layer1	GCN-Conv(1433, 64) ReLU()	GCN-Conv(3707,128) ReLU()	SAGE_Conv(500,128) ReLU()	SAGE_Conv(500,512) ReLU()	SAGE.Conv(602, 256) ReLU()
Layer2	GCN-Conv(128, 64)	GCN-Conv(128,64)	Dropout(p=0.5) SAGE-Conv(128,64)	Dropout(p=0.5) SAGE-Conv(512,256)	Dropout(p=0.5) SAGE-Conv(128,64)
B.4	Prediction AUC throughout the time
We show the AUC on the entire observed graph at each time, similar to the node classification.
However, different trends are shown in the results. All the methods could reach a relatively high
performance compared to their average results, since the task of link prediction might be easier
in smaller subgraphs than the node classification. On the Pubmed where cluster-based strategy
outperforms joint training, we observe that it reaches higher in the later period of time, when the
generated cluster-graph could include more edges that are not in the original graph.
------------------------------------------------joint training ----------finetuning
-------------------------------------------------------------------------sample-random -sample-random jump — sample-new degree — cluster-random - ClUSter-ConS.KMeans
Cora
(％) 0∩<
u。。一pe」d
XUn
CiteSeer
100
90
80
70
60
50
40
30 1~1~1~1~1~1~1~1~1~1~1
0 2 4 6 8 10 12 14 16 18 20
Time
100
90
80
70
PUbmed
10 20 30 40 50 60 70 80 90 100
100
90
80
70
60
60 1~~1~1~~1~~1~1~~1~~1~1~~1~~1	40
0 2 4 6 8 10 12 14 16 18 20	0
Time
Figure 6:	AUC on the entire observed graph at each time.
C Datasets
C.1 Statistics
Table 4 shows statistics of the benchmark datasets used in the experiments. Besides the basic in-
formation including graph size, feature dimensions and labels, we list the split of training, val-
idation and test sets. For the Cora, Citeseer and Pubmed, we follow the standard fixed split in
GCN (Kipf & Welling, 2017). For the Flickr and Reddit, to follow the similar split fashion, we
uniformly sample 200 and 100 vertices in each category for training, and sample fixed number
of vertices as the validation and test sets. Specifically, we generate a two-category version of
Flickr, noted as ”Flickr*”, in experiments of node classification. Since most vertices in Flickr
are in label of category 4 or 6, when split into groups, the evaluation metric turns to be bet-
ter on models overfitting to these two categories, which influences the comparison of methods.
Thus, we remove vertices belong to other categories and related edges, and use the rested graph
as ”two-category version” of Flickr. For the link prediction, we use the original Flickr dataset
without category information. All the dataset are collected by the library of PyTorch Geometric
(https://github.com/rusty1s/pytorch_geometric).
C.2 Changes of Degrees in Growing Graph
To know details of graph changes during the growth, we further show the average degree of the
subgraphs and entire graph in Figure 7. For the subgraphs, since it is randomly cut off from the
graph, the average degree fluctuates throughout all tasks within in low range, which brings difficulty
for learning directly on the subgraphs. The average degree of the entire graph continually grows,
which is attributed to the appearance of inter-subgraph edges. And missing of such inter-edges
influence on the graph learning results.
13
Under review as a conference paper at ICLR 2021
Table 4: Dataset statistics and basic settings.
Cora Citeseer Pubmed Flickr* Flickr Reddit
# Nodes	2,708	3,327	19,717	60,736	89,250	232965
# Edges	5,429	4,732	44,338	409,564	899,756	11,606,919
# Features	1,433	3,707	500	500	500	602
# Classes	7	6	3	2	7	41
# Training Nodes	140	120	60	400	-	4,100
# Validation Nodes	500	500	500	2,000	-	12,300
# Test Nodes	1,000	1,000	1,000	30,000	-	50,000
qde」6qns u!de」°e-luw
Time	Time
Time	Time
1.5 ―'——'——'——'——'——'——'——'——'——'
0 10 20 30 40 50 60 70 80 90 100
Time
300
250
200
150
100
50
0
Time
φφslφα φφslφα ΦOTra^φ><⅛
Figure 7:	The average degree of subgraph and entire graph throughout tasks in benchmarks.
D Details of Compared Methods
Since existing incremental learning methods are designed for incremental categories or domains, we
adapt representative methods to our IGL scenario for comparison. In this section, we present brief
introductions to these methods and describe how we adapt them for incremental graph learning.
LwF (Li & Hoiem, 2017) presents a distillation loss that forces the updated model to generate
similar outputs with the previous model using KL-Divergence. Before the trainiang at each time, it
copies the parameters of model learned on previous data, and uses the model as a “teacher-model”
to restrict the update of current model.
EWC (Kirkpatrick et al., 2017) presents an regularization loss term to penalize the update of param-
eters based on their importance to previous tasks. The importance of parameters are estimated by
the Fisher information matrix.
iCaRL (Rebuffi et al., 2017) firstly addresses the problem of incremental class learning on neural
networks. It is a rehearsal-based method that store previous samples, named as exemplars, with
restricted memory constraints. We adapt its way to selecting exemplars into our sample-based strat-
egy, which calculate the mean value of each category and select samples closer to the centers. Such
strategy of sampling exemplars is also accepted in the following methods, e.g. EEIL (Castro et al.,
2018) and BiC (Wu et al., 2019).
TEM (Chaudhry et al., 2019b) explores the different ways of updating the exemplars set within
specified memory, including reservoir, ringbuffer, k-Means and MoF. Since k-Means and MoF are
based on similar ideas to the iCaRL, we adapt the ringbuffer strategy that follows FIFO rule to
preserve the last several samples of each category.
14
Under review as a conference paper at ICLR 2021
A-GEM follows and further improves GEM (Lopez-Paz & Ranzato, 2017) to restrict optimization of
parameters based on the direction of gradients, where a reference gradient is computed by a random
sampled subset of previous samples.
Among the compared methods, LwF and EWC require hyper-parameters to control the weight of
regularization loss terms, which are determined by grid search in IGL settings.
E Proofs
E.1 Proof to Theorem 4.1
Let |Vt| = n, |Nt (v)| = m, |VtL| = k, we denote L as the number of vertices in Nt (v) that are
included in VtL, i.e. L = |Nt(v) ∩ VtL| and 0 ≤ L ≤ min(m, k). Then
E (agg (v, Nt (V) ∩ VL) | v ∈ VL)
=E (E (agg(v,Nt (v) ∩VL) | v ∈ VtL,L = l))	⑺
=∑mn(m,k)p(L = I) * E (agg (v, Nt (v) ∩VL) | v ∈VL,L = l),
Let S(Nt(v), l) includes all the possible combination sets with length of l from Nt (v), i.e.
S(Nt(v), l) = {S ⊆ Nt(v) ∧ |S| = l}. Since∀u1,u2 ∈ Vt,P(u1 ∈ VtL) = P(u2 ∈ VtL),
we get
E (agg (v, Nt (v) ∩VL) | v ∈VL,L = l)
1 Σu∈S u
ςs∈s(Nt(V)Q Cmjy * ~Γ~
l * c(m,l产u∈Nt(V)C(m - 1,1 - 1)u
(8)
ςu∈M(v)u
m
agg (v, Nt (v))
where C(∙, ∙) denotes the combination. Substitute (8) for (7), We get
E (agg (v, Nt (v) ∩ VL) | v ∈ VL)
= Σlm=i0n(m,k)P (L = l) * agg (v, Nt (v))
= agg (v, Nt (v))
E.2 Proof to Theorem 4.2
We follow the basic notations in the above proof. When v ∈ VtL
agg (v, Nt (v) ∩VL)
=L ςu∈M(v)∩VL U
=L ςu∈M-i(v)∩VL U + L ,UE(Nt(V)-Nt-I(V))CVL U
(9)
(10)
15
Under review as a conference paper at ICLR 2021
Since ∀u1, u2 ∈ Vt-ι,P(u1 ∈ VL) = P(u2 ∈ VL), similar to Theorem 4.1, we let |N-1| = m0
and l be the number of sampled vertices from old data, then
E (Lςu∈M-i (v)∩vl U 1 v ∈ Vt J
=E(E (Lςu∈Nji(v)∩vlU 1 v ∈ VL, L = l ÷ IM(V) -Nt-ι(V)I))
=ςi=0W)P(L = l ÷ IM(V)- M-I(V))1) * (ςS∈S(Nt-ι(v),l)八 * 夕U『U)
C (, l)	L
=∑W'k)p(L = l÷ IM(V)-M-I(V))1) * (L*∖,l)∑u∈Ntτ(V)CIm - 1,l- 1)u)
=∑mnm,,k)p(L = l ÷ IM(V)-M-1(V))∣) * ! * ∑u∈Nt-1(V)U
L	m0
=E(l ÷ IM(V)-M-1(V))I )a"(V,MT(V))
=E( %-；(V);并)agg(V,M-I(V))
` INt(V) ∩vli ' yyt t 1 "
(11)
Since ∀u ∈ VneW,P(u ∈ VL) = 1, we get (M(V) -M-I(V)) ∩ VL = M(V) - M-I(V), then
E (L ∑U∈(N(v)-N-1(v))∩VL U
E (L ∑U∈Nt(v)-Nt-1(V) U
E (L∑u∈n(v)u) - E (L∑u∈n-i(v)u
(12)
E
IM(V)I
INt(V) ∩VL∣
agg(VM(V)) - E (∣NNv)1∩V)?L∣) a"(V,MT(V))
Substitute (11) and (12) for (10), we get
E (agg(V, Nt (v) ∩ VL))
=E( INt-I(V) ∩B" )agg(v, M-I(V)) ÷ E (	IN(V)I	) agg(v,M(V))
∖ ∣Nt(v) ∩VL∣ y L t 1' 〃	V∣Nt(v) ∩ VL∣7	∖ 八〃
-E (∣N⅛) a≡(v,Nt-I(V))
=E ( JNt(V)I 八)agg(v, Nt (v)) - E( IM-I(V) -VLI )agg(v, M-I(V))
k∣M(v) ∩ VL∣; gg( , t( ))	(Wt(V) ∩vLI) gg( , t 1( ))
Theorem 4.2 proved.
We then PrOVe that λι ÷ λ2 = 1, where λι = E ( INNV∩Vli ) , λ2 = -E ( IN-IVV∩VVL1).
IM(V)I	) _ E (IM-1 (V)-VLI)
IM(V) ∩VL∣;	【INt (V) ∩VL∣ )
IM(V)∣-∣Nt-1 (V)-VLI)
INt (V) ∩VL∣	)
∣Nt-1(v) -VLI ÷ IM(V)- M-I(V)-VLI ÷ IM(V) ∩ VLI - ∣Nt-1 (V) - VLI
INt (V) ∩VL∣
IM(V)-M-1(V)-VLI ÷ IM(V) ∩VL∣)
INt (V) ∩VL∣	)
λ1 ÷ λ2 = E
=E
=E
=E
(14)
16
Under review as a conference paper at ICLR 2021
Since Nt(v) - Nt-1(v) ⊆ VtL, we get |Nt(v) - Nt-1 (v) - VtL| = 0, then
λ1 + λ2 = E
(∣Nt(v) ∩VL∣ A
UNt (v) ∩VL"
(15)
1
E.3 Proof to Theorem 4.3
(i)	When ∀u ∈ Vt-1, P(u ∈ ∆VtL) = 1, we get ∆VtL = Vt-1 and VtL = Vt. Then obviously
•	∀u,v ∈ Vt-1,P(u ∈ VtL) = P(v ∈ VtL) = 1.
•	EL = Et, ∣∆EL ∩ EinterI = ∣Einter | = maX∆EL ∣∆EL ∩ Einter |.
(ii)	When the unbiased condition and edge-preserved condition hold, we assume that the max-
imum target is restricted by the specified size Vmax and Emax. Thus, we consider P (u ∈
VL) H ∣{(u, V) ∈ Etnter ∣ V ∈ Vnew }∣ = Dt(u) - Dt-ι(u). Since ∃uι,u2 ∈ Vt-ι, s.t.
Dt(u1) - Dt-1(u1) 6= Dt(u2) - Dt-1(u2), we get P(u1 ∈ VtL) 6= P(u2 ∈ VtL), which conflicts
with the unbiased condition. So the assumption does not hold.
E.4 Proof to Theorem 4.4
Theorem 4.4 represents sampling as a single observation equation f (∙), and clustering as a group
observation equation g(∙, ∙). Thus, the theorem suggests that cluster-based strategy provides more
information quantity than sample-based strategy, and a balanced clustering works better in cluster-
based strategy.
To prove the first inequality in Theorem 1, we firstly prove the following lemma that knowing the
average value of the whole set provides more information than knowing value of some element.
Lemma 1. Given n elements x1, x2, ..., xn (n ≥ 1) with possible value 0 or 1. Follow the definition
of f (∙) and g(∙, ∙), ∀i ∈ [1,n],
S(g(1,n))≤S(f(i)).	(16)
Proof. We calculate the value of entropy based on the definition that S = kB logΩ, where Ω is the
number of mircostates. We omit the constant kB for simplicity. Then for the left of the inequality
knowing definite value of one element, Ω = 2n-1 and S(f (i)) = log 2n-1. For the right of the
inequality, suppose g(1,n) : ∑n=ιXj = v, then Ω = C(n,v)= 皿：-@> where C(∙, ∙) denotes
the combination. It is known that C(n, V) ≤ C(n, [nJ). Using the inequality conducted from the
Stirling’s approximation that
√2πnn+1 e-n ≤ n! ≤ e nn+1 e-n,	(17)
we get
n!
C(n,V) ≤ C(n,bn/2J) = [n∕2j!dn∕2]!
e nn+1e-n
≤ (√2∏bn∕2j bn/2」+1 e-bn∕2c )(√2∏dn∕2] dn∕2e + 2e-dn∕2e)	(18)
enn+1	__ e 2n+1 _	2e 1
≤ ---———-t = -----=-------- 2
- 2π(2)n+1	2π√n	π√n
For n ≥ 3, ∏√n < 1, then C(n, V) < 2n-1. When n = 1, 2, it is easy to know from calculation that
C(n, v) ≤ 2n-1. Thus ∀n ≥ 1,
S(g(1,n))=logC(n,V) ≤log2n-1=S(f(i))	(19)
Now we prove the first inequality in Theorem 1 that
S(g(1,r ),g( r + 1, 2n ),...,g((r-r1n + 1,n)) ≤ S (f (1),f (2),...,f (r))	(20)
17
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
SPHtthe n elements With IelIgth Of ^ ^∩d denote the IIUmber Of microstates Of each group as
Q-aL QUO7 L …"(T — 1-Where 空-he equation Withthis group∙ TheIl based on Lemma
L We get
≈(g(M + L (Z 一1 )'s) ≤⅞L∙Z = O∙-:(TI1) (21)
TheIl We SUm the above inequalities from OtO(T — 1) as
? ; n/ 、n A 2n∕ 、ll)A //
s(g( Lg+LL:、g( T ÷ IW))
T——1
LogαF≤og (%I) (22)
Γo
Log2iusl)J(2L∙∙∙J)
The above PrOOf ShoWS that the CIUStebased Strategy offering average VaIUe PrOVideS more info
matIIthatthe SamPIe—based StrategyOfferg single element ValU 尸 WhiCh is Stated by the first
inequality in Theorem L
We restate the SeCOlld inequalityTheOrem
1、1、1/ 、n i 2n / 、ll)` /∕/
(s(g(L ——L g( — + L ——)：：7 g(---------------------+ Iw))) ≤ S3
TTT T (Z5)
IE(S(g( LLIL g( LI + L 172)：：7 g( L:1 + L) L
Where Ho is the expectation Of entropy∙ GiVeIl S elements 电广：「七好"the POSSibIe VaIUe s U Σ尸 1M
are from O to ny with C(F S) COIlditiOIlS OUt Ofthe total 2 如 COIlditiOIlTheIlthe expectation Of Q
knowing the average VaIUe is
E(Q (g( L 呈 U S) H 24Σr0 C?S)2 (24)
2时
We define the discrete functioι一七(TZ) H 2——如 ∑UoC(T^ s)By COmPUtiIlg the expectation Of Qi H
L :; from r groups in (23We get
= IOghF(g(x + L (J+I)'s) (25)
Γo
T——1
= IOg ɪɪɪ) uτgL
Γo
and SimiIarIy
IE(S(g(LLlLg(Ll + LL2L∙∙∙7g(L11 + Lm)) U ∑lk>g) (26)
TheIl if the function - Og七(TZ) is a COIIVeXfUIlCtiOI‰ using the COIlVeXity inequality 4, V O- ΣAu
Lg∑la&)≤l>二 og*and Set >u〉& uwe COUld get
ogp(小)uogp(ΣΓ2)IAMr-ɪs (27)
TheIlog≤ ∑lgand the inequality (23) hdBy CaICUtq(n) ugm H
一Og 2 ^fnand the 2nd Order difference >q(72) ≥ 0》thus -Og七(TZ) is COIlVeX and the proved
inequality hd
F MoRE VISUALlZATIoNS
Figure 8 ShoWS the ViSUaHZation results Of all methods in 44 We further ViSUaliZe the IIOde Classi
Cation results at each timeFigureShoWilIg the COrreCtlyand WrOIIgIy classified IlOdeSiIl green
and blue respectively，Red IIOdeSiIldiCate the CUrrenttraing IIOdeS and We do IIOt draw the CIUSter
IIOdeSiIl OUr CIUStebased Stmtegy∙ NOdeS have IIOt appeared UlltiI IIOW are in gray TheIl two facts
COUId be ObSerVedeach row Of the ViSUaliZatIL
18
Under review as a conference paper at ICLR 2021
•	Some newly appeared nodes are correctly classified without connecting to the training
nodes, which suggests the inherited knowledge from previously trained model.
•	Some blue nodes in previous task turn to be green in following tasks, which suggests that
new vertices and edges may rectify the error in previous learning process.
When comparing among columns, we show the effectiveness of proposed methods together with
baselines. Our methods achieve comparable performance with the joint training method with less
cost of computation and memory.
Figure 8: Feature visualization using t-SNE of all methods training node classification on Cora.
19
Under review as a conference paper at ICLR 2021
	training nodes
	correct nodes
	wrong nodes
	unappeared nodes
finetuning
joint
training
sample-
random
sample-
random jump
sample-
new degree
cluster-
random
cluster-
cons.KMeans
Figure 9: Visualization of node classification results on Cora. We show the results at time 2, 4,
6, 8, 10 in one row for each method. For the appeared nodes, we draw the training nodes in red, and
draw nodes classified into correct/wrong category in green/blue. Then unappeared nodes until now
are in gray.
20