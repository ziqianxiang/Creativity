Under review as a conference paper at ICLR 2021
Parametric Copula-GP model for analyzing
MULTIDIMENSIONAL NEURONAL AND BEHAVIORAL
RELATIONSHIPS
Anonymous authors
Paper under double-blind review
Ab stract
One of the main challenges in current systems neuroscience is the analysis of
high-dimensional neuronal and behavioral data that are characterized by differ-
ent statistics and timescales of the recorded variables. We propose a parametric
copula model which separates the statistics of the individual variables from their
dependence structure, and escapes the curse of dimensionality by using vine cop-
ula constructions. We use a Bayesian framework with Gaussian Process (GP)
priors over copula parameters, conditioned on a continuous task-related variable.
We improve the flexibility of this method by 1) using non-parametric conditional
(rather than unconditional) marginals; 2) linearly mixing copula elements with
qualitatively different tail dependencies. We validate the model on synthetic data
and compare its performance in estimating mutual information against the com-
monly used non-parametric algorithms. Our model provides accurate information
estimates when the dependencies in the data match the parametric copulas used
in our framework. Moreover, when the exact density estimation with a parametric
model is not possible, our Copula-GP model is still able to provide reasonable
information estimates, close to the ground truth and comparable to those obtained
with a neural network estimator. Finally, we apply our framework to real neuronal
and behavioral recordings obtained in awake mice. We demonstrate the ability
of our framework to 1) produce accurate and interpretable bivariate models for
the analysis of inter-neuronal noise correlations or behavioral modulations; 2) ex-
pand to more than 100 dimensions and measure information content in the whole-
population statistics. These results demonstrate that the Copula-GP framework
is particularly useful for the analysis of complex multidimensional relationships
between neuronal, sensory and behavioral data.
1	Introduction
Recent advances in imaging and recording techniques have enabled monitoring the activity of hun-
dreds to several thousands of neurons simultaneously (Jun et al., 2017; Helmchen, 2009; Dombeck
et al., 2007). These recordings can be made in awake animals engaged in specifically designed tasks
or natural behavior (Stringer et al., 2019; Pakan et al., 2018a;b), which further augments these al-
ready large datasets with a variety of behavioral variables. These complex high dimensional datasets
necessitate the development of novel analytical approaches (Saxena & Cunningham, 2019; Steven-
son & Kording, 2011; Staude et al., 2010) to address two central questions of systems and behavioral
neuroscience: how do populations of neurons encode information? And how does this neuronal ac-
tivity correspond to the observed behavior? In machine learning terms, both of these questions
translate into understanding the high-dimensional multivariate dependencies between the recorded
variables (Kohn et al., 2016; Shimazaki et al., 2012; Ince et al., 2010; Shamir & Sompolinsky, 2004).
There are two major methods suitable for recording the activity of large populations of neurons
from behaving animals: the multi-electrode probes (Jun et al., 2017), and calcium imaging meth-
ods (Grienberger et al., 2015; Helmchen, 2009; Dombeck et al., 2007) that use changes in intracel-
lular calcium concentration as a proxy for neuronal spiking activity at a lower temporal precision.
While neuronal spiking occurs on a temporal scale of milliseconds, the behavior spans the timescales
from milliseconds to hours and even days (Mathis et al., 2018). As a result, the recorded neuronal
1
Under review as a conference paper at ICLR 2021
and behavioral variables may operate at different timescales and exhibit different statistics, which
further complicates the statistical analysis of these datasets.
The natural approach to modeling statistical dependencies between the variables with drastically
different statistics is based on copulas, which separate marginal (i.e. single variable) statistics from
the dependence structure (Joe, 2014). For this reason, copula models are particularly effective for
mutual information estimation (Jenison & Reale, 2004; Calsaverini & Vicente, 2009b), which quan-
tifies how much knowing one variable reduces the uncertainty about another variable (Quiroga &
Panzeri, 2009). Copula models can also escape the ‘curse of dimensionality’ by factorising the
multi-dimensional dependence into pair-copula constructions called vines (Aas et al., 2009; Czado,
2010).
Copula models have been successfully applied to spiking activity (Onken et al., 2009; Hu et al.,
2015; Shahbaba et al., 2014; Berkes et al., 2009), 2-photon calcium recordings (Safaai, 2019) and
multi-modal neuronal datasets (Onken & Panzeri, 2016). However, these models assumed that the
dependence between variables was static, whereas in neuronal recordings it may be dynamic or
modulated by behavioral context (Doiron et al., 2016; Shimazaki et al., 2012). Therefore, it might
be helpful to explicitly model the continuous time- or context-dependent changes in the relationships
between variables, which reflect changes in an underlying computation.
Here, we extend a copula-based approach by adding explicit conditional dependence to the param-
eters of the copula model, approximating these latent dependencies with Gaussian Processes (GP).
It was previously shown that such a combination of parametric copula models with GP priors out-
performs static copula models (Lopez-Paz et al., 2013) and even dynamic copula models on many
real-world datasets, including weather forecasts, geological data or stock market data (Hernandez-
Lobato et al., 2013). Yet, this method has never been applied to neuronal recordings before.
In this work, we increase the complexity of both marginal and copula models in order to adequately
describe the complex dependencies commonly observed in neuronal data. In particular, we use
conditional marginal models to account for changes of the single neuron statistics and mixtures of
parametric copula models to account for changes in tail dependencies. We also improve the scalabil-
ity of the method by using stochastic variational inference. We develop model selection algorithms,
based on the fully-Bayesian Watanabe-Akaike information criterion (WAIC). Finally and most im-
portantly, we demonstrate that our model is suitable for estimating mutual information. It performs
especially well when the parametric model can closely approximate the target distribution. When it
is not the case, our copula mixture model demonstrates sufficient flexibility and provides close in-
formation estimates, comparable to the best state-of-the-art non-parametric information estimators.
The goal of this paper is to propose and validate the statistical Copula-GP method, and illustrate that
it combines multiple desirable properties for neuroscience applications: interpretability of paramet-
ric copula models, accuracy in density and information estimation and scalability to large datasets.
We first introduce the copula mixture models and propose model selection algorithms (Sec. 2). We
then validate our model on synthetic data and compare its performance against other commonly used
information estimators (Sec. 3). Next, we demonstrate the utility of the method on real neuronal and
behavioral data (Sec. 4). We show that our Copula-GP method can produce bivariate models that
emphasize the qualitative changes in tail dependencies and estimate mutual information that ex-
poses the structure of the task without providing any explicit cues to the model. Finally, we measure
information content in the whole dataset with 5 behavioral variables and more than 100 neurons.
2	Parametric copula mixtures with Gaussian process priors
Our model is based on copulas: multivariate distributions with uniform marginals. Sklar’s theo-
rem (Sklar, 1959) states that any multivariate joint distribution can be written in terms of univariate
marginal distribution functions p(yi) and a unique copula which characterizes the dependence struc-
ture: p(yι,...,yN) = c(F1(y1)... FN (yN)) XQN=I Pyi). Here, Fi(∙) are the marginal cumulative
distribution functions (CDF). Thus, for each i, Fi(yi) is uniformly distributed on [0,1].
For high dimensional datasets (dim y), maximum likelihood estimation for copula parameters may
become computationally challenging. The two-stage inference for margins (IFM) training scheme is
typically used in this case (Joe, 2005). First, univariate marginals are estimated and used to map the
data onto a multidimensional unit cube. Second, the parameters of the copula model are inferred.
2
Under review as a conference paper at ICLR 2021
Rotation-equivariant copulas	x 4 rotations	= copula
Figure 1: Copula families used in the mixture models in our framework. The percentage in the
upper-left corner shows how often each of the families was selected to be used in a copula mixture
for pairwise relationships in the real neuronal data from Pakan et al. (2018a) (see Sec. 4).
Conditional copulas Following the approach by Hernandez-Lobato et al. (2013), We are using
Gaussian Processes (GP) to model the conditional dependencies of copula parameters:
N
p(y|x) = c F1(y1|x), . . . , FN (yN |x)x × Yp(yi|x) .	(1)
i=1
In the most general case, the marginal PDFs p(yi|x) and CDFs Fi(yi|x) and the copula c(. . . |x) it-
self can all be conditioned on x. In our framework, x is assumed to be one-dimensional. A Gaussian
Process is ideally suited for copula parametrization, as it provides an estimate of the uncertainty in
model parameters, which we utilize in our model selection process (Sec. 2.1).
Conditional marginals Previous works on conditional copulas (Hernandez-Lobato et al., 2013)
relied on the assumption that marginal distributions remain constant. This assumption might not
hold for some real-life datasets, including neuronal recordings. Thus, we propose to use conditional
marginal distributions instead. Note, that the interpretation of the copula model itself would depend
on the assumptions made for the marginal distributions (see Appx. C.1 for further discussion).
In order to estimate marginal CDFs F(yi|x), we use the non-parametric fastKDE (O’Brien et al.,
2016) algorithm, which allows for direct estimation of the conditional distributions. These CDFs are
then used to map the data onto a unit hypercube using the probability integral transform: F (yi |x) →
Ui 〜u[o,i], such that Ui is uniformly distributed for any x.
Bivariate copula families We use 4 copula families as the building blocks for our copula models:
Gaussian, Frank, Clayton and Gumbel copulas (Fig. 1). All of these families have a single parameter,
corresponding to the rank correlation (Table 1). We also use rotated variants (90。，180。，270。)of
Clayton and Gumbel copulas in order to express upper tail dependencies and negative correlation.
Table 1: Bivariate copula families and their GPLink functions
Copula	Domain	GPLinkf): R →	dom(cj )
Independence	一	一	
Gaussian	[-1,1]	Erf(f/1.4)	
Frank	(-∞, ∞)	0.3 ∙ f + sign(f) ∙	(0.3 ∙ f)2
Clayton	[0,∞)	Exp(0.3 ∙ f)	
Gumbel	[1,∞)	1 + Exp(0.3 ∙ f)	
Since we are primarily focused on the analysis of neuronal data, we have first visualized the de-
pendencies in calcium signal recordings after a probability integral transform, yielding empirical
conditional copulas. As a distinct feature in neuronal datasets, we observed changes in tail depen-
dencies with regard to the conditioning variable. Since none of the aforementioned families alone
could describe such conditional dependency, we combined multiple copulas into a linear mixture
model: c (u|x) = PjM=1 φj (x)cj (u; θj(x)), where M is the number of elements, φj (x) is the con-
centration of the j th copula in a mixture, cj is the pdf of the j th copula, and θj is its parameter.
3
Under review as a conference paper at ICLR 2021
Each of the copula families includes the Independence copula as a special case. To resolve this over-
completeness, we add the Independence copula as a separate model with zero parameters (Table 1).
For independent variables yind , the Independence model will be preferred over the other models in
our model selection algorithm (Sec. 2.1), since it has the smallest number of parameters.
Gaussian Process priors We parametrize each copula in the mixture model with an independent
latent GP: f 〜N(μ X 1, Kλ(x,x)). For each copula family, We constructed GPLink functions
(Table 1) that map the GP variable onto the copula parameter domain: θj = GPlinkcj (fj), R →
dom(cj). Next, We also use GP to parametrize concentrations φj (x), Which are defined on a simplex
(Pφ=1):
j-1	M - m - 1
φj =(I - tj) Y tm,	tm = φ (fm + φ 1( —M----m一
m=1
tM = 0,
where Φ is a CDF of a standard normal distribution and fm ~ N(“m × 1, Ke (x, x)). We use the
RBF kernel Kλ (x, x) With bandWidth parameter λ. Therefore, the Whole mixture model With M
copula elements is parameterized by [2M - 1] independent GPs and requires [2M - 1] hyperparam-
, r ʌ i c- λ ι rVι	c- f
eters: {λ}M for θ and {λ}M-1 for φ.
Approximate Inference Since our model has latent variables with GP priors and intractable poste-
rior distribution, direct maximum likelihood Type-II estimation is not possible and an approximate
inference is needed. Such an inference problem with copula models has previously been solved
with the expectation propagation algorithm (Hernandez-Lobato et al. (2013), see comparison in
Appx. B.4), which was not suitable for large scale data. Recently, a number of scalable approximate
inference methods were developed, including stochastic variational inference (SVI) (Titsias, 2009;
Cheng & Boots, 2017), scalable expectation propagation (SEP) (Hernandez-Lobato & Hernandez-
Lobato, 2016), and MCMC based algorithms (Hensman et al., 2015), as well as a scalable exact
GP (Wang et al., 2019). We chose to use SVI due to availability of the well-established GPU-
accelerated libraries: PyTorch (Paszke et al., 2017) and GPyTorch (Gardner et al., 2018).
2.1	Bayesian Model selection
We use the Watanabe-Akaike information criterion (WAIC, Watanabe (2013)) for model selection.
WAIC is a fully Bayesian approach to estimating the Akaike information criterion (AIC) (see Eq. 31
in the original paper by Watanabe (2013)). The main advantage of the method is that it avoids the
empirical estimation of the effective number of parameters, which is often used for approximation of
the out-of-sample bias. It starts with the estimation of the log pointwise posterior predictive density
(lppd) (Gelman et al., 2014):
NS	N
lppd = Xlog (SXp(yi∣θs)) ,	PWAIC = X VS=I(logp(yi∣θs)),
where {θs }S is a draw from a posterior distribution, which must be large enough to represent the
posterior. Next, the pWAIC approximates the bias correction, where VsS=1 represents sample vari-
ance. Therefore, the bias-corrected estimate of the log pointwise posterior predictive density is given
by： elppdWAIC = lppd — PWAIC = -N ∙ WAICoriginal.
In the model selection process, we aim to choose the model with the lowest WAIC. Since our copula
probability densities are continuous, their values can exceed 1 and the resulting WAIC is typically
negative. Zero WAIC corresponds to the Independence model (pdf = 1 on the whole unit square).
Since the total number of combinations of 10 copula elements (Fig. 1, considering rotations) is large,
exhaustive search for the optimal model is not feasible. In our framework, we propose two model
algorithms for constructing close-to-optimal copula mixtures: greedy and heuristic (see Appx. A.4
for details). The greedy algorithm is universal and can be used with any other copula families
without adjustment, while the heuristic algorithm is fine-tuned to the specific copula families used
in this paper (Fig. 1). Both model selection algorithms were able to select the correct 1- and 2-
component model on simulated data and at least find a close approximation (within WAICtol =
0.005) for more complex models (see validation of model selection in Appx. B).
4
Under review as a conference paper at ICLR 2021
2.2	Entropy and mutual information
Our framework provides tools for efficient sampling from the conditional distribution and for cal-
culating the probability density p(y|x). Therefore, for each x=t the entropy H(y|x=t) can be
estimated using Monte Carlo (MC) integration: H(y|x=t) = - Ep(y|x=t) log p(y|x=t). The prob-
ability p(y|x=t) factorizes into the conditional copula density and marginal densities (1), hence the
entropy also factorizes (Jenison & Reale, 2004) as H(y|x=t) = P H(yi|x=t) + Hc(ux|x=t),
where ux = F(y|x). The conditional entropy can be integrated as H(y|x) = PiN=1 H(yi|x) +
Hc(ux|x=t)p(t)dt, separating the entropy of the marginals {yi}N from the copula entropy.
Now, I(x, y) = I(x, G(y)) if G(y) is 1) a homeomorphism, 2) independent of x (Kraskov et al.,
2004). If marginal statistics are independent of x, then the probability integral transform u = F(y)
satisfies both requirements, and I(x, y) = I(x, u). Then, in order to calculate the mutual infor-
mation I(x, u) := H(u) - H (u|x), we must also rewrite it using only the conditional distribution
p(u|x), which is modelled with our Copula-GP model. This can be done as follows:
I(x, u) = H(u) -	H (u|x = t)p(t)dt
E log p(u|x) - E log E p(u|x).
p(u,x)	p(u)	p(x)
(2)
The last term in (2) involves nested integration, which is computationally difficult and does not scale
well with N = dim u. Therefore, we propose an alternative way of estimating I(x, y), which avoids
double integration and allows us to use the marginals conditioned on x (ux = F(y|x)), providing
a better estimate of H(y|x). We can use two separate copula models, one for estimating p(y) and
calculating H(y), and another one for estimating p(y|x) and calculating H(y|x):
Hc(u1x, . . . , uxN |s = t)p(t)dt,	(3)
N
I(x, y) =	I(x, yi) + Hc(u1, . . . ,uN) -
i=1
where both entropy terms are estimated with MC integration. Here we only integrate over the unit
cube [0, 1]N and then dom x, whereas (2) required integration over [0, 1]N × dom x.
The performance of both (2) and (3) critically depends on the approximation of the dependence
structure, i.e. how well the parametric copula approximates the true copula probability density. If
the joint distribution p(y1 . . . yN ) has a complex dependence structure, as we will see in synthetic
examples, then the mixture of parametric copulas may provide a poor approximation of p(y) and
overestimate Hc(u1, . . . , uN), thereby overestimating I(x, y). The direct integration (2), on the
other hand, typically underestimates the I(x, y) due to imperfect approximation of p(y|x), and
under assumption that the marginals can be considered independent of x.
We further refer to the direct integration approach (2) as “Copula-GP integrated” and to the alterna-
tive approach (3) as “Copula-GP estimated” and assess both of them on synthetic and real data.
2.3	Copula vine constructions
High-dimensional copulas can be constructed from bivariate copulas by organizing them into hierar-
chical structures called copula vines (Aas et al., 2009). In this paper, we focus on the canonical vine
or C-vine, which factorizes the high-dimensional copula probability density function as follows:
N
c(u) =	c1i (u1 , ui)	×
i=2
NN
cij|{k}k<i (F(ui|{uk}k<i), F(uj|{uk}k<i))
i=2 j=i+1
(4)
where {k}k<i = 1, . . . , i - 1 and F (.|.) is a conditional CDF. Note, that all of the copulas in (4)
can also be conditioned on x via Copula-GP model. We choose the first variable u1 to be the one
with the highest rank correlation with the rest (sum of absolute values of pairwise Kendall’s τ),
and condition all variables on the first one. We repeat the procedure until no variable is left (see
Appx. A.5). It was shown by Czado et al. (2012) that this ordering facilitates C-vine modeling.
Code availability Code will be made available on GitHub upon paper acceptance.
5
Under review as a conference paper at ICLR 2021
3 Validation on artificial data
We compare our method with the other commonly used non-parametric algorithms for mutual infor-
mation estimation: KraskoV-StogbaUer-Grassberger (KSG, Kraskov et al. (2004)), Bias-ImProved-
KSG by Gao et al. (BI-KSG, Gao et al. (2016)) and the Mutual Information Neural Estimator
(MINE, Belghazi et al. (2018)). In this section, we Use relatively low dimensional data (≤10D), for
which we can still directly calcUlate the trUe mUtUal information.
Corr=-0,1/1.0,
B StUdenT with Corr=0.7, df=2∕150
A Gaussian with Corr=-0,1/1.
C Gauss
P(U)
N
yi = yi + (jQyp1N
p(u |x=0)	P(U |x=1)
——True ——CoPUla-GP integr.——CoPUla-GP estim,——BI-KSG ——KSG ——MINE100 ——MINE200 ——MINE500
#VariabIeS (dim y)
#VariabIeS (dim y)
#VariabIeS (dim y)
FigUre 2: Conditional entroPy H (y|x) and mUtUal information I(x, y) measUred by different meth-
ods on synthetic data. UPPer row shows the dePendency strUctUres p(u) and conditional dePendency
strUctUres at the beginning and the end of the domx = [0, 1]. A MUltivariate GaUssian samPles.
B MUltivariate StUdent T samPles. C MUltivariate GaUssian samPles y (same as A), morPhed into
another distribution p(y) with a tail dependence, while I(x, y) = I(x, y). Gray intervals show
either standard error of mean (SE, 5 repetitions), or ,(SE)2 + (MCtol )2 for integrated variables.
Note, that MINE estimates depend on the choice of hyper-parameters (e.g. number of hidden units).
First, if there is no model mismatch, we expect our information estimates to be unbiased due to
the MC estimator being unbiased (Luengo et al., 2020). We confirm this fact using the dataset
sampled from a multivariate Gaussian distribution, with cov(yi, yj) = ρ + (1 - ρ) δij, where δij
is Kronecker’s delta and ρ = -0.1 + 1.1x, x ∈ [0, 1]. Our algorithm selects a Gaussian copula
on these data, which perfectly matches the true distribution. The same applies to any linear mixture
of copulas from Tab. 1 as well (see Appx. B.3), and is covered by the automated tests on simulated
data (see Appx. B.1). Therefore, Copula-GP measures both entropy and mutual information exactly
(within integration tolerance, see Fig. 2A). The performance of the non-parametric methods on this
dataset is lower. It was shown before that KSG and MINE both severely underestimate the MI for
high-dimensional Gaussians with high correlation (e.g. see Fig. 1 in Belghazi et al. (2018)). The
Copula-GP model (integrated) provides accurate estimates for highly correlated (up to ρ = 0.999,
at least up to 20D) Gaussian distributions (see Appx. B.2).
Next, we test the Copula-GP performance on the Student T distribution, which can only be approx-
imated by our copula mixtures, but would not exactly match any of the parametric copula families
from Fig. 1. We keep the correlation coefficient ρ fixed at 0.7, and only change the number of de-
grees of freedom exponentially: df = exp(5x) + 1, x ∈ [0, 1]. This makes the dataset particularly
challenging, as all of the mutual information I(x, y) is encoded in tail dependencies of p(y|x). The
true H(y|x) of the Student T distribution was calculated analytically (see Eq. A.12 in Calsaverini &
Vicente (2009a)) and I(x, y) was integrated numerically according to (2) given the true p(y|x).
Fig. 2B shows that most of the methods underestimate I(x, y). Copula-GP (integrated) and MINE
(with 100 hidden units) provide the closest estimates. The training curve for MINE with more hidden
units (200,500) showed signs of overfitting (abrupt changes in loss at certain permutations) and the
resulting estimate was higher than the true I(x, y) at higher dimensions. It was shown before that
MINE provides inaccurate and inconsistent results on datasets with low I(x, y) (Song & Ermon,
6
Under review as a conference paper at ICLR 2021
2019). We also demonstrate I(x, y) estimation with a combination of two copula models for H(y)
and H (y|x): “Copula-GP estimated” (see Eq. 3). In lower dimensions, it captures less information
than “Copula-GP integrated”, but starts overestimating the true MI at higher dimensions, when the
inaccuracy of the density estimation for p(y) builds up. This shows the limitation of the “estimated”
method, which can either underestimate or overestimate the correct value due to parametric model
mismatch, whereas “integrated” method consistently underestimates the correct value. We conclude
that Copula-GP and MINE demonstrate similar performance in this example, while KSG-based
methods significantly underestimate I(x, y) in higher dimensions.
Finally, we created another artificial dataset that is not related to any of the copula models used in
our framework (Table 1). We achieved that by applying a homeomorphic transformation F(y) to
a multivariate Gaussian distribution. Since the transformation is independent of the conditioning
variable, it does not change the I(x, y) = I(x, F(y)) (Kraskov et al., 2004). Therefore, we possess
the true I(x, y), which is the same as for the first example in Fig. 2A. Note, however, that there is
no ground truth for the conditional entropy in this example, since H(y) 6= H(F(y)). We transform
the Gaussian copula samples y ∈ u[N0,1] from the first example as yei = yi + (QjN=1 yj)1/N and
again transform the marginals using the empirical probability integral transform u = F(ye). Both
conditional p(u|x) and unconditional p(u) densities here do not match any of the parametric copulas
from Table 1. As a result, “Copula-GP estimated” overestimated the correct value, while “Copula-
GP integrated” underestimated it similarly to the MINE estimator with 100 hidden units.
Fig. 2 demonstrates that the performance of the parametric Copula-GP model critically depends
on the match between the true probability density and the best mixture of parametric copula ele-
ments. When the parametric distribution matches the true distribution (e.g. Fig. 2A or Fig. A5), our
Copula-GP framework provides unbiased estimates and predictably outperforms all non-parametric
methods. Nonetheless, even when the exact reconstruction of the density is not possible (Figs. 2B-
C), the mixtures of the copula models are still able to model the changes in tail dependencies, at least
qualitatively. In those challenging examples, our method performs similarly to the neural-network
based method (MINE) and still outperforms KSG-like methods.
4 Validation on real data
A
B
y
777“S7 77、J
* 一
C
D
o ι
Neuron 3
140—160
0	1
Neuron 3
X
∣lNeuron 3
J Neuron 60
I Neuron 63
∣Licking rate
Figure 3: Applications of the Copula-GP framework to neuronal and behavioral data from the visual
cortex of awake mice. A Schematic of the experimental task (Pakan et al., 2018a; Henschke et al.,
2020) in virtual reality (VR); B Example traces from ten example trials: x is a position in VR, y
is a vector of neuronal recordings (blue) and behavioral variables (red); C-D Density plots for: the
noise correlation (C) and the behavioral modulation (D) examples; E-G Conditional entropy for
the bivariate examples (E-F) and the population-wide statistics (G); H Comparison of Copula-GP
methods (“estimated” and “integrated”) vs. non-parametric MI estimators on subsets of variables.
∕zrrrWr


We investigate the dependencies observed in neuronal and behavioral data and showcase possible
applications of the Copula-GP framework. We used two-photon calcium imaging data of neuronal
7
Under review as a conference paper at ICLR 2021
population activity in the primary visual cortex of mice engaged in a visuospatial navigation task in
virtual reality (data from Henschke et al. (2020)). Briefly, the mice learned to run through a virtual
corridor with vertical gratings on the walls (Fig. 3A, 0-120 cm) until they reached a reward zone
(Fig. 3A, 120-140 cm), where they could get a reward by licking a reward spout. We condition our
Copula-GP model on the position in the virtual environment x and studied the joint distribution of
the behavioral (yι... y5) and neuronal (y6 ... y109) variables (dimy=109). Fig. 3B shows a part
of the dataset (trials 25-35 out of 130). The traces here demonstrate changes in the position x of
the mouse as well as the activity of 3 selected neurons and the licking rate. These variables have
different patterns of activity depending on x and different signal-to-noise ratios. Both differences
are reflected in marginal statistics, which are shown on the right with the density plots of equal area.
Constructing interpretable bivariate models We first studied bivariate relationships between
neurons. In order to do this, we transformed the raw signals (shown in Fig. 3B) with a probability
integral transform u = F(y). We observed strong non-trivial changes in the dependence structure
c(u|x) subject to the position in the virtual reality x and related visual information (Fig. 3C). Such
stimulus-related changes in the joint variability of two neuronal signals are commonly described as
noise correlations. The Copula-GP model provides a more detailed description of the joint proba-
bility that goes beyond linear correlation analysis. In this example, the dependence structure is best
characterized by a combination of Gaussian and Clayton copula (rotated by 90。). The density plots
Fig. 3C demonstrate the match between the true density (outlines) and the copula model density
(blue shades) for each part of the task. We measure the accuracy of the density estimation with the
proportion of variance explained R2, which shows how much of the variance of the variable y2 can
be predicted given the variable yι (see Eq. A1 in Appx. A.1). The average R2 for all yι is provided
in the upper right corner of the density plots.
Next, we show that our model can be applied not only to the neuronal data, but also to any of the
behavioral variables. Fig. 3D shows the dependence structure between one of the neurons and the
licking rate. The best selected mixture model here is Frank + Clayton 0。+ Gumbel 270°, which
again provides an accurate estimate of the conditional dependence between the variables. Such
copula mixture models allow us to analyze the contribution of the tail dependencies to the mutual
information, and backtrack the outliers in order to relate them to certain trials or behaviors (see
Appx. C.2). Therefore, Figs. 3C-D demonstrate that our Copula-GP model provides both an accurate
fit for the probability distribution and an interpretable visualization of the dependence structure.
Figs. 3E-F show the absolute value of the conditional entropy |H(ux|x)|, which is equivalent to
the mutual information between two variables I(u1x , u2x). For both examples, the MI peaks in the
reward zone. The bivariate Copula-GP models were agnostic of the reward mechanism in this task,
yet they revealed the position of the reward zone as an anomaly in the mutual information.
Measuring information content in a large neuronal population Finally, we constructed a C-
vine describing the distribution between all neuronal and behavioral variables (dim ux = 109)
and measured the conditional entropy H(ux|x) for all variables in the dataset {u1x...u1x09}. The
conditional entropy in Fig. 3G peaks in the reward zone (similarly to Figs. 3E-F) and also at the
beginning of the trial, where the velocity of the animal varied the most on the trial-to-trial basis.
While constructing the C-vine, we ordered the variables according to their pairwise rank correlations
(see Sec. 2.3). We considered subsets of the first N variables and measured the MI with the position
for each subset. We compared the performance of our Copula-GP method on these subsets of ux
vs. KSG and MINE. Fig. 3H shows that all 3 methods provide similar results on subsets of up to
10 variables, yet in higher dimensions both MINE and KSG show smaller I(x, {uix<N}) compared
to our Copula-GP method, which agrees with the results obtained on the synthetic data (Fig. 2).
The true values of I(x, {uix<N}) are unknown, yet we expect the integrated Copula-GP (solid line)
to underestimate the true value due to parametric model mismatch. The Copula-GP “estimated”
(dashed line) can either under- or over-estimate the information (see eq. 3), but here it almost per-
fectly matches the “integrated” result, which suggests that the model was able to tightly approximate
both p(ux|x) and p(ux), and, as a result (eq. 3), I(x, {uix<N}). These results demonstrate superior
performance of our Copula-GP model on high-dimensional neuronal data.
8
Under review as a conference paper at ICLR 2021
5	Discussion
We have developed a Copula-GP framework for modeling conditional multivariate joint distribu-
tions. The method is based on linear mixtures of parametric copulas, which provide flexibility for
estimating complex dependencies but still have interpretable parameters. We approximate condi-
tional dependencies of the model parameters with Gaussian Processes which allow us to implement
a Bayesian model selection procedure. The selected models combine the accuracy in density es-
timation with the interpretability of parametric copula models. We demonstrated the performance
of our framework in mutual information estimation on synthetically generated data. Despite the
general limitations of the parametric models, our framework performs either significantly better
(when the true dependence matches a parametric model), or at least as good as the state-of-the-art
non-parametric information estimators. Unlike black-box machine-learning methods for entropy es-
timation, our Copula-GP model uses interpretable elements (pair copulas) and latent GP parameters,
which can be isolated, visualized, and analyzed (Appx. C). The framework is also well suited for
describing the dependencies observed in neuronal and behavioral data. We demonstrated that the
model scales well at least up to 109 variables and 21k samples, while theoretically, the parameter
inference scales as O(n ∙ m2), where n is a number of samples and m is the (effective) number
of variables (see Appx. A.6). Alternative popular non-copula methods that could be applied to our
real data, include GPFA or GLMs on the deconvolved spikes. Contrary to these approaches, copula
methods allow us to model the dependencies between elements with utterly different statistics (e.g.
licks vs. velocity). In addition, Copula-GP explicitly represents the dependencies as a function of
position, revealing insightful information about the task structure. To the best of our knowledge,
there are currently no other methods with this combination of features. The possible applications
of Copula-GP include, but are not limited to, studying the neuronal population statistics, noise cor-
relations, behavioral or contextual modulations. In summary, we demonstrated that the Copula-GP
approach can make stochastic relationships explicit and generate accurate models of dependencies
between neuronal responses, sensory stimuli, and behavior. Future work will focus on implement-
ing model selection for the vine structure and improving the scalability of the mutual information
estimation algorithm.
References
Kjersti Aas, Claudia Czado, Arnoldo Frigessi, and Henrik Bakken. Pair-copula constructions of multiple de-
pendence. Insurance: Mathematics and economics, 44(2):182-198, 2009.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,
and R Devon Hjelm. MINE: Mutual Information Neural Estimation, 2018.
Pietro Berkes, Frank Wood, and Jonathan W Pillow. Characterizing neural dependencies with copula models.
In Advances in neural information processing systems, pp. 129-136, 2009.
R. S. Calsaverini and R. Vicente. An information-theoretic approach to statistical dependence: Copula infor-
mation. EPL (Europhysics Letters), 88(6):68003, Dec 2009a. ISSN 1286-4854. doi: 10.1209/0295-5075/
88/68003. URL http://dx.doi.org/10.1209/0295-5075/88/68003.
Rafael S Calsaverini and Renato Vicente. An information-theoretic approach to statistical dependence: Copula
information. EPL (Europhysics Letters), 88(6):68003, 2009b.
Ching-An Cheng and Byron Boots. Variational inference for gaussian process models with linear complexity.
In Advances in Neural Information Processing Systems, pp. 5184-5194, 2017.
Claudia Czado. Pair-copula constructions of multivariate copulas. In Copula theory and its applications, pp.
93-109. Springer, 2010.
Claudia Czado, Ulf Schepsmeier, and Aleksey Min. Maximum likelihood estimation of mixed c-vines with
application to exchange rates. Statistical Modelling, 12(3):229-255, 2012.
Brent Doiron, Ashok LitWin-Kumar, Robert Rosenbaum, Gabriel K Ocker, and Kresimir Josic. The mechanics
of state-dependent neural correlations. Nature neuroscience, 19(3):383, 2016.
Daniel A Dombeck, Anton N Khabbaz, Forrest Collman, Thomas L Adelman, and David W Tank. Imaging
large-scale neural activity with cellular resolution in awake, mobile mice. Neuron, 56(1):43-57, 2007.
9
Under review as a conference paper at ICLR 2021
Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying Fixed k-Nearest Neighbor Information
Estimators, 2016.
Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch: Black-
box matrix-matrix Gaussian process inference with GPU acceleration. In Advances in Neural Information
Processing Systems, pp. 7576-7586, 2018.
Andrew Gelman, Jessica Hwang, and Aki Vehtari. Understanding predictive information criteria for bayesian
models. Statistics and computing, 24(6):997-1016, 2014.
Christine Grienberger, Xiaowei Chen, and Arthur Konnerth. Dendritic function in vivo. Trends in neuro-
sciences, 38(1):45-54, 2015.
Fritjof Helmchen. Two-photon functional imaging of neuronal activity. CRC Press, 2009.
Julia U Henschke, Evelyn Dylda, Danai Katsanevaki, Nathalie Dupuy, Stephen P Currie, Theoklitos
Amvrosiadis, Janelle MP Pakan, and Nathalie L Rochefort. Reward association enhances stimulus-specific
representations in primary visual cortex. Current Biology, 2020.
James Hensman, Alexander G Matthews, Maurizio Filippone, and Zoubin Ghahramani. MCMC for varia-
tionally sparse gaussian processes. In Advances in Neural Information Processing Systems, pp. 1648-1656,
2015.
Daniel Hemandez-Lobato and Jose Miguel Hernandez-Lobato. Scalable gaussian process classification via
expectation propagation. In Artificial Intelligence and Statistics, pp. 168-176, 2016.
Jose Miguel Hernandez-Lobato, James R Lloyd, and Daniel Hernandez-Lobato. Gaussian process con-
ditional copulas with applications to financial time series. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems
26, pp. 1736-1744. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
5084- gaussian- process- conditional- copulas- with- applications- to- financial- time- series.
pdf.
Meng Hu, Kelsey L Clark, Xiajing Gong, Behrad Noudoost, Mingyao Li, Tirin Moore, and Hualou Liang.
Copula regression analysis of simultaneously recorded frontal eye field and inferotemporal spiking activity
during object-based working memory. Journal of Neuroscience, 35(23):8745-8757, 2015.
Robin AA Ince, Riccardo Senatore, Ehsan Arabzadeh, Fernando Montani, Mathew E Diamond, and Stefano
Panzeri. Information-theoretic methods for studying population codes. Neural Networks, 23(6):713-727,
2010.
Rick L. Jenison and Richard A. Reale. The shape of neural dependence. Neural Computation, 16
(4):665-672, 2004. doi: 10.1162/089976604322860659. URL https://doi.org/10.1162/
089976604322860659.
Harry Joe. Asymptotic efficiency of the two-stage estimation method for copula-based models. Jour-
nal of Multivariate Analysis, 94(2):401 - 419, 2005. ISSN 0047-259X. doi: https://doi.org/10.
1016/j.jmva.2004.06.003. URL http://www.sciencedirect.com/science/article/pii/
S0047259X04001289.
Harry Joe. Dependence modeling with copulas. CRC press, 2014.
James J Jun, Nicholas A Steinmetz, Joshua H Siegle, Daniel J Denman, Marius Bauza, Brian Barbarits, Al-
bert K Lee, Costas A Anastassiou, Alexandru Andrei, Cagatay Aydin, et al. Fully integrated silicon probes
for high-density recording of neural activity. Nature, 551(7679):232-236, 2017.
Adam Kohn, Ruben Coen-Cagli, Ingmar Kanitscheider, and Alexandre Pouget. Correlations and neuronal
population information. Annual review of neuroscience, 39:237-256, 2016.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information. Phys. Rev. E,
69:066138, Jun 2004. doi: 10.1103/PhysRevE.69.066138. URL https://link.aps.org/doi/10.
1103/PhysRevE.69.066138.
David Lopez-Paz, Jose Miguel Hernandez-Lobato, and Ghahramani Zoubin. Gaussian process vine copulas for
multivariate dependence. In International Conference on Machine Learning, pp. 10-18, 2013.
David Luengo, Luca Martino, Monica Bugallo, Victor Elvira, and Simo Sarkka. A survey of monte carlo
methods for parameter estimation. EURASIP Journal on Advances in Signal Processing, 2020(1):1-62,
2020.
10
Under review as a conference paper at ICLR 2021
Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga Abe, Venkatesh N Murthy, Mackenzie Weygandt
Mathis, and Matthias Bethge. DeepLabCut: markerless pose estimation of user-defined body parts with deep
learning. Nature neuroscience, 21(9):1281-1289, 2018.
Arno Onken and Stefano Panzeri. Mixed vine copulas as joint models of spike counts and local field potentials.
In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS’16,
pp. 1333-1341, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.
Arno Onken, Steffen Grunewalder, Matthias HJ Munk, and Klaus Obermayer. Analyzing short-term noise
dependencies of spike-counts in macaque prefrontal cortex using copulas and the flashlight transformation.
PLoS computational biology, 5(11), 2009.
Travis A. O’Brien, Karthik Kashinath, Nicholas R. Cavanaugh, William D. Collins, and John P. O’Brien. A
fast and objective multidimensional kernel density estimation method: fastKDE. Computational Statistics
& Data Analysis, 101:148 - 160, 2016. ISSN 0167-9473. doi: https://doi.org/10.1016/j.csda.2016.02.014.
URL http://www.sciencedirect.com/science/article/pii/S0167947316300408.
Janelle MP Pakan, Stephen P Currie, Lukas Fischer, and Nathalie L Rochefort. The impact of visual cues,
reward, and motor feedback on the representation of behaviorally relevant spatial locations in primary visual
cortex. Cell reports, 24(10):2521-2528, 2018a.
Janelle MP Pakan, Valerio Francioni, and Nathalie L Rochefort. Action and learning shape the activity of
neuronal circuits in the visual cortex. Current opinion in neurobiology, 52:88-97, 2018b.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,
Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In Advances in
Neural Information Processing Systems, 2017.
Rodrigo Quian Quiroga and Stefano Panzeri. Extracting information from neuronal populations: information
theory and decoding approaches. Nature Reviews Neuroscience, 10(3):173-185, 2009.
Wang A Panzeri S Harvey CD Safaai, H. Characterizing information processing of parietal cortex
projections using vine copulas. In Bernstein Conference 2019. American Physical Society, 2019.
doi: doi:10.12751/nncn.bc2019.0067. URL https://abstracts.g-node.org/abstracts/
f80ac63f-88fc-4203-9c2b-a279bb9e201a.
Shreya Saxena and John P. Cunningham. Towards the neural population doctrine, 2019. ISSN 18736882.
Babak Shahbaba, Bo Zhou, Shiwei Lan, Hernando Ombao, David Moorman, and Sam Behseta. A semipara-
metric bayesian model for detecting synchrony among multiple neurons. Neural Computation, 26(9):2025-
2051,2014. doi: 10.1162∕NECO∖_a\_00631. URL https://doi.org/10.1162/NECO_a_00631.
PMID: 24922500.
Maoz Shamir and Haim Sompolinsky. Nonlinear population codes. Neural computation, 16(6):1105-1136,
2004.
Hideaki Shimazaki, Shun-ichi Amari, Emery N Brown, and Sonja Grun. State-space analysis of time-varying
higher-order spike correlation for multiple neural spike train data. PLoS computational biology, 8(3), 2012.
Abe Sklar. Fonctions de reprtition an dimensions et leursmarges. Publ. Inst. Statis. Univ. Paris, 8:229-231,
1959.
Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators,
2019.
Benjamin Staude, Stefan Rotter, and Sonja Grun. Cubic: cumulant based inference of higher-order correlations
in massively parallel spike trains. Journal of computational neuroscience, 29(1-2):327-350, 2010.
Ian H. Stevenson and Konrad P. Kording. How advances in neural recording affect data analysis. Nature
Neuroscience, 14(2):139-142, feb 2011. ISSN 1097-6256. doi: 10.1038/nn.2731. URL http://www.
nature.com/articles/nn.2731.
Carsen Stringer, Marius Pachitariu, Nicholas Steinmetz, Charu Bai Reddy, Matteo Carandini, and Kenneth D
Harris. Spontaneous behaviors drive multidimensional, brainwide activity. Science, 364(6437):eaav7893,
2019.
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In Artificial Intelli-
gence and Statistics, pp. 567-574, 2009.
11
Under review as a conference paper at ICLR 2021
Ke Wang, Geoff Pleiss, Jacob Gardner, Stephen Tyree, Kilian Q Weinberger, and Andrew Gordon Wilson.
Exact gaussian processes on a million data points. In Advances in Neural Information Processing Systems,
pp.14648-14659, 2019.
Sumio Watanabe. A widely applicable Bayesian information criterion. Journal of Machine Learning Research,
14(Mar):867-897, 2013.
A Appendix Methods
A. 1	Goodness-of-fit
We measure the accuracy of the density estimation with the proportion of variance explained R2 . We com-
pare the empirical conditional CDF ecdf (u2 |u1 = y) vs. estimated conditional CDF ccdf (u2 |u1 = y) and
calculate:
R2(y)=1-X
u
u2
ecdf (u2 |u1 = y) - ccdf (u2 |u1 = y) 2
ecdf(u2 ∣uι = y) — U
(A1)
where R2 (y) quantifies the portion of the total variance of u2 that our copula model can explain given u1 = y,
and u2 = F(y2) = 0.5. The sum was calculated for u2 = 0.05n, n = 0 ... 20.
Next, we select all of the samples from a certain interval of the task (x ∈ [x1 , x2]) matching one of those shown
in Figure 3 in the paper. We split these samples u1 ∈ [0, 1] into 20 equally sized bins: {Ii}20 . For each bin Ii,
we calculate (A1). We evaluate ccdf (u2 |u1 = yi) ≈ ccdf (u2 |u1 ∈ Ii) using a copula model from the center
of mass of the considered interval of x: Xμ = mean(x) for samples X ∈ [xι ,x2]. We use the average measure:
R2 =	E	R2 (mean(uι ∈ Ii)),
p(u1∈Ii)
(A2)
to characterize the goodness of fit for a bivariate copula model. Since u1 is uniformly distributed on [0, 1], the
probabilities for each bin p(uι ∈ Ii) are equal to 1/20, and the resulting measure R2 is just an average R2
from all bins. The results were largely insensitive to the number of bins (e.g. 20 vs. 100).
A.2 Variational inference
Since our model has latent variables with GP priors and intractable posterior distribution, the direct maximum
likelihood Type-II estimation is not possible and an approximate inference is needed. We used stochastic
variational inference (SVI) with a single evidence lower bound (Hensman et al., 2015):
N
LELBO =	E	logp(yi|fi) — KL[q(u)||p(u)],
i=1 q(fi)
(A3)
implemented as VariationalELBO in GPyTorch (Gardner et al., 2018). Here N is the number of data
samples, u are the inducing points, q(u) is the variational distribution and q(f) = p(f |u)q (u)du.
Following the Wilson and Nickisch (2015) approach (KISS-GP), we then constrain the inducing points to a
regular grid, which applies a deterministic relationship between f and u. As a result, we only need to infer
the variational distribution q(u), but not the positions of u. The number of grid points is one of the model
hyper-parameters: grid_size.
Equation A3 enables joint optimization of the GP hyper-parameters (constant mean μ and two kernel parame-
ters: scale and bandwidth) and parameters of the variational distribution q (mean and covariance at the inducing
points: U 〜N(μu × 1, ∑u) ) (Hensman et al., 2015). We have empirically discovered by studying the conver-
gence on synthetic data, that the best results are achieved when the learning rate for the GP hyper-parameters
(base_lr) is much greater than the learning rate for the variational distribution parameters (Var_lr, see
Table A1).
Priors For both the neuronal and the synthetic data, We use a standard normal prior p(u)〜N(0, I) for
a variational distribution. Note, that the parametrization for mixture models was chosen such that the afore-
mentioned choice of the variational distribution prior with zero mean corresponds to a priori equal mixing
coefficients φj = 1/M for j = 1 . . . M . In our experiments with the simulated and real neuronal data, we
observed that the GP hyper-parameter optimisation problem often had 2 minima (which is a common situation,
see Figure 5.5 on page 116 in Williams and Rasmussen (2006)). One of those corresponds to a short kernel
lengthscale (λ) and low noise (minf σ2), which we interpret as overfitting. To prevent overfitting, we used
λ 〜N(0.5, 0.2) prior on RBF kernel lengthscale parameter that allows the optimizer to approach the minima
from the region of higher λ, ending up in the minimum with a larger lengthscale.
12
Under review as a conference paper at ICLR 2021
Optimization We use the Adam optimizer With two learning rates for GP hyper-parameters (base_lr) and
variational distribution parameters (var_lr). We monitor the loss (averaged over 50 steps) and its changes in
the last 50 steps: ∆ loss = mean(loss[-100:-50]) - mean(loss[-50:]). If the change be-
comes smaller than check_waic, then we evaluate the model WAIC and check if it is lower than -WAICt0ι.
If it is higher, we consider that either the variables are independent, or the model does not match the data. Either
way, this indicates that further optimisation is counterproductive. If the WAIC < -WAICtol, we proceed with
the optimisation until the change of loss in 50 steps ∆loss becomes smaller than loss_tol (see Table A1).
Effective learning rates for different families The coefficients in the GPLink functions for different
copula families are also a part of model hyper-parameters. The choice of these coefficients affects the gradients
of the log probability function. Since GPLink functions are nonlinear, they affect the gradients in various
parameter ranges to a different extent. This results in variable convergence rates depending on the true copula
parameters.
To address the problem of setting up these hyper-parameters, we have created the tests on synthetic data with
different copula parameters. Using these tests, we manually adjusted these hyper-parameters such that the GP
parameter inference converged in around 1000-2000 iterations for every copula family and parameter range.
We have also multiplied the GP values corresponding to the mixture coefficients by 0.5, to effectively slow
down the learning of the mixture coefficients φ compared to the copula coefficients θ, which also facilitates the
convergence.
Hyper-parameter selection The hyper-parameters of our model (Table A1) were manually tuned, often
considering the trade off between model accuracy and evaluation time. A more systematic hyper-parameter
search might yield improved results and better determine the limits of model accuracy.
Table A1: Hyper-parameters of the bivariate Copula-GP model
Hyper-parameter	Value	Description
base_lr	10-2	Learning rate for GP parameters
var-lr	10-3	Learning rate for variational distribution
grid_size	128	Number of inducing points for KISS-GP
waic-tol	0.005	Tolerance for WAIC estimation
loss-tol	10-4	Loss tolerance that indicates the convergence
Check_Waic	0.005	Loss tolerance when we check WAIC
... and GPLink parameters listed in Table 1.		
A.3 Bayesian model selection
In model selection, we are aiming to construct a model with the lowest possible WAIC. Since our copula prob-
ability densities are continuous, their values can exceed 1 and the resulting WAIC is typically negative. Zero
WAIC corresponds to the Independence model (pdf = 1 on the whole unit square). We also set up a tolerance
(WAICtol = 0.005), and models with WAIC ∈ [-WAICtol , WAICtol] are considered indistinguishable
from the independence model.
Since the total number of combinations of 10 copula elements (Fig.1) is large, exhaustive search for the optimal
model is not feasible. In our framework, we propose two model algorithms for constructing close-to-optimal
copula mixtures: greedy and heuristic.
A.4 Model selection algorithms
The greedy algorithm (Algorithm 1) starts by comparing WAIC of all possible single-copula models (from
Table 1, in all rotations) and selecting the model with the lowest WAIC. After that, we add one more copula
(from another family or in another rotation) to the first selected copula, and prepend the element that yields the
lowest WAIC of the mixture. We repeat the process until the WAIC stops decreasing. After the best model
is selected, we remove the inessential elements using the reduce(.) function. This function removes those
elements which have an average concentration of < 10% everywhere on x ∈ [0, 1]. This step is added to
improve the interpretability of the models and computation time for entropy estimation (at a small accuracy
cost) and can, in principle, be omitted.
The greedy algorithm can be improved by adding model reduction after each attempt to add an element. In
this case, the number of elements can increase and decrease multiple times during the model selection process,
which also must be terminated if the algorithm returns to the previously observed solution. Even though it
13
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
Algorithm 1: Greedy algorithm for copula mixture selection
M,Moid J [],[];
Sc J [Independence, Gauss, Frank, 4 X Clayton, 4 X Gumbel];
// 4× includes all rotations
// while every update of the model yields a new best
whileWAIC(M) ≤ WAIC(Mold) and size(Sc) > 0 do
Mold J M ;
select c from Sc such that WAIC(prepend(c, M)) is minimal;
M J prepend(c, M) ;
remove c from Sc;
end
Mbest J reduce(Mold);
return Mbest;
Algorithm 2: Heuristic algorithm for copula mixture selection
G J [Gauss];
if WAIC(G) > -Waijtol then
I return [Independence];
end
MCl J [Independence, Gauss, 4 X Clayton];
MGu J [Independence, Gauss, 4 X Gumbel];
Mbest,Mworst J (MCl,MGu) sorted by WAIC;
if WAIC(G) < WAIC(Mbest) then
I return G;
end
for i J 3 . . . size(Mbest ) do
M J Mbest with i-th element replaced by Mworst [i];
Mbest J M if WAIC(M) < WAIC(Mbest);
end
Mbest J reduce(Mbest);
if Gauss ∈ Mbest then
M J Mbest with Gauss replaced by Frank;
Mbest J M if WAIC(M) < WAIC(Mbest);
end
// Gauss often gets confused with pairs of e.g.
Clayton 0° + Gumbel 0°
if size(Mbest) > 1 then
for i J 1 . . . (size(Mbest) - 1) do
for j J (i + 1) . . . size(Mbest) do
M J Mbest with i-th andj-th elements removed;
M J prepend(Gauss, M);
if WAIC(M) < WAIC(Mbest) then
Mbest J M ;
break;
end
end
end
end
Mbest J reduce(Mbest);
return Mbest;
14
Under review as a conference paper at ICLR 2021
complicates the algorithm, it reduces the maximal execution time (observed on the real neuronal data) from
〜90 minutes down to 〜40 minutes.
The heuristic algorithm focuses on the tail dependencies (Algorithm 2). First, we try a single Gaussian copula.
If variables are not independent, we next compare 2 combinations of 6 elements, which are organized as follows:
an Independence copula together with a Gaussian copula and either 4 Clayton or 4 Gumbel copulas in all
4 rotations (0°, 90°, 180°, 270°). We select the combination with the lowest WAIC. After that, We take the
remaining Clayton/Gumbel copulas one by one and attempt to switch the copula type (Clayton to Gumbel or
vise versa). If this switching decreases the WAIC, we keep a better copula type for that rotation and proceed
to the next element.
Here we make the assumption, that because Clayton and Gumbel copulas have most of the probability density
concentrated in one corner of the unit square (the heavy tail), we can choose the best model for each of the 4
corners independently. When the best combination of Clayton/Gumbel copulas is selected, we can (optionally)
reduce the model.
We have not yet used a Frank copula in a heuristic algorithm. We attempt to substitute the Gaussian copula with
a Frank copula (if it is still a part of the reduced mixture, see lines 16-19 in Alg. 2). Sometimes, a Gaussian
copula can be mistakenly modeled as a Clayton & Gumbel or two Gumbel copulas. So, as a final step (lines
20-31, Alg. 2), we select all pairwise combinations of the remaining elements, and attempt to substitute each
of the pairs with a Gaussian copula, selecting the model with the lowest WAIC. Despite a large number of
steps in this algorithm, the selection process takes only up to 25 minutes (in case all elements in all rotations
are required).
The procedure was designed after observing the model selection process on a variety of synthetic and real
neuronal datasets.
A.5 Vine copulas
Vine models provide a way to factorize the high-dimensional copula probability density into a hierarchical set
of bivariate copulas (Aas et al., 2009). There are many possible decompositions based on different assumptions
about conditional independence of specific elements in a model, which can be classified using graphical models
called regular vines (Bedford and Cooke, 2001; 2002). A regular vine can be represented using a hierarchical
set of trees, where each node corresponds to a conditional distribution function (e.g. F (u2 |u1)) and each edge
corresponds to a bivariate copula (e.g. c(u2, u3|u1)). The copula models from the lower trees are used to obtain
new conditional distributions (new nodes) with additional conditional dependencies for the higher trees, e.g. a
ccdf of a copula c(u2, u3|u1) and a marginal conditional distribution F (u2 |u1) from the 1st tree provide a
new conditional distribution F(u3|u1, u2) for a 2nd tree. Therefore, bivariate copula parameters are estimated
sequentially, starting from the lowest tree and moving up the hierarchy. The total number of edges in all trees
(= the number of bivariate copula models) for an m-dimensional regular vine equals m(m - 1)/2.
The regular vines often assume that the conditional copulas c(ui, uj |{uk}) themselves are independent of their
conditioning variables {uk }, but depend on the them indirectly through the conditional distribution functions
(nodes) (Acar et al., 2012). This is known as the simplifying assumption for vine copulas (Haff et al., 2010),
which, if applicable, allows to escape the curse of dimensionality in high-dimensional copula construction.
In this study, we focus on the canonical vine or C-vine, which has a unique node in each tree, connected to all of
the edges in that tree. For illustration, see, for example, Figure 2 in Aas et al. (2009). The C-vine was shown to
be a good choice for neuronal datasets (Onken & Panzeri, 2016), as they often include some proxy of neuronal
population activity as an outstanding variable, strongly correlated with the rest. This variable provides a natural
choice for the first conditioning variable in the lowest tree. In the neuronal datasets from Henschke et al. (2020),
this outstanding variable is the global fluorescence signal in the imaged field of view (global neuropil).
To construct a C-vine for describing the neuronal and behavioral data from Henschke et al. (2020), we used a
heuristic element ordering based on the sum of absolute values of Kendall’s τ of a given element with all of
the other elements. It was shown by Czado et al. (2012) that this ordering facilitates C-vine modeling. For all
of the animals and most of the recordings (14 out of 16), including the one used in Figure 3, the first variable
after such ordering was the global neuropil activity. This again confirms, that a C-vine with the global neuropil
activity as a first variable is an appropriate model for the dependencies in neuronal datasets.
A.6 Algorithmic complexity
In this section, we discuss the algorithmic complexity of the parameter inference for a C-vine copula model.
The parameter inference for each of the bivariate Copula-GP models scales as O(n), where n is the number of
samples, since we use a scalable kernel interpolation KISS-GP (Wilson and Nickisch, 2015). As we mentioned
in Sec. A.5, a full m-dimensional C-vine model requires m(m - 1)/2 bivariate copulas, trained sequentially.
15
Under review as a conference paper at ICLR 2021
As a result, the O(n) GP parameter inference has to be repeated m(m — 1)/2 times, which yields O(n ∙ m2)
complexity.
In practice, the computational cost (in terms of time) of the parameter inference for each bivariate model varies
from tens of seconds to tens of minutes. The heuristic model selection is designed in such a way, that it discards
independent variables in just around 20 seconds (line 3 in Alg. 2). As a result, most of the models are quickly
skipped and further considered as Independence models, and their contribution to the total computational cost
can be neglected. When the model is evaluated, the Independence components are also efficiently ‘skipped’
during sampling, as ppcf function is not called for them. The Independence models also add zero to C-vine
log probability, so they are also ‘skipped’ during log probability calculation. They also reduce the total memory
storage, as no GP parameters, which predominate the memory requirements, are stored for these models.
In a conditional C-vine trained on a real neuronal dataset with 109 variables, 5253 out of 5886 (89%) bivariate
models were Independence, which leaves only 633 non-Independence models.
In practice, this means that the algorithmic complexity of the model is much better than the naive theoretical
prediction O(n ∙ m2), based on the structure of the graphical model. Suppose that the actual number of the
non-Independence models NnI in a vine model is much smaller than m(m — 1)/2 and can be characterized
by an effective number of dimensions meff 〜NNnι. In this case, instead of the O(m2) scaling with the
number of variables, the complexity highly depends on the sparsity of the dependencies in the graphical model
and scales with as O(n ∙ NnI)〜 O(n ∙ m2ff).
Therefore, the our method is especially efficient on the datasets with a low effective dimensionality meff, such
as the neuronal data. The number of variables m itself has little effect on the computational cost and memory
storage.
B More validation on synthetic data
Computing infrastructure We developed our framework and ran the majority of our experiments (de-
scribed both in the paper and Supplemental Material) on an Ubuntu 18.04 LTS machine with 2 x Intel(R)
Xeon(R) Gold 6142 CPU @ 2.60GHz and 1x GeForce RTX 2080 + 1 x GeForce RTX 2080 Ti GPUs. For
training C-vine models, we used another Scientific Linux 7.6 machine with 1 x Intel(R) Xeon(R) Silver 4114
CPU @ 2.20GHz and 8 x GeForce RTX 2080 Ti GPUs.
Code availability Code will be made available on GitHub upon paper acceptance.
B.1	Model selection for bivariate copulas
Synthetic data We generate artificial data by sampling from a copula mixture, parametrized in two different
ways:
1.	mixing concentrations of all copulas were constant and equal to 1/N (N = number of copulas), but
copula parameters θ were parametrized by the phase-shifted sinus functions:
θi = Aisin 卜mN + 2πx) + Bi, X ∈ [0,1]	(A4)
where i is the index of the copula in a mixture, m = 1. For Clayton and Gumbel copulas, the absolute
value of the sinus was used. The amplitudes Ai were chosen to cover most of the range of parameters,
except for extremely low or high θs for which all copula families become indistinguishable (from
independence or deterministic dependence, respectively).
2.	copula parameters θ were constant, but mixing concentrations φ were parametrized by the phase-
shifted sinus functions (same as Eq. A4, with Ai = Bi = 1/N and m = 2). Such parametrization
ensures that the sum of all mixing concentrations remains equal to one (PiN=1 φ = 1). Yet, each φ
turns to zero somewhere along this trajectory, allowing us to discriminate the models and infer the
correct mixture.
Identifiability tests We tested the ability of the model selection algorithms to select the correct mixture
of copula models, the same as the one from which the data was generated. We generated 5000 samples with
equally spaced unique inputs on [0,1].
Both model selection algorithms were able to correctly select all of the 1-component and most of the 2-
component models on simulated data. For simulated data with larger numbers of components (or 2 very similar
components), the WAIC of the selected model was either lower (which is possible given a limited number of
samples) or close to the WAIC of the correct parametric model. In other words, the difference between the
16
Under review as a conference paper at ICLR 2021
WAIC of the correct model and of the best selected model never exceeded the WACtest_t°i = 0.05, which
We set UP as a criteria for passing the test: ∆WAIC < WAICtest_t°i. Since all the tests were passed SuCCess-
fully, we conclude that both algorithms are capable of finding optimal or close-to-optimal solutions for copula
mixtures.
A more detailed report on the model identifiability tests Tables A3-A7 below illustrate the search
for the best model. The copula model names in these tables are shortened to the first two letters, e.g. Gumbel
becomes ‘Gu’, Frank becomes ‘Fr’. The information in these Tables provides some intuition on the model
selection process and the range of WAICs for the correct or incorrect models. The final selected models are
shown in bold.
Table A3 demonstrates that both greedy and heuristic algorithms can identify the correct single copula model.
Some key intermediate models (M in Alg. 1-2) with their WAICs are listed in the table, along with the total
duration of simulations (T, in minutes) on RTX 2080Ti for both algorithms.
Table A4 shows the identification of the mixtures with 2 components, where the copula parameters θ were con-
stant (independent of x) and mixing concentrations φ were parameterized by the phase-shifted sinus functions
(Eq. A4). All of these models were correctly identified with both algorithms. The mixtures with 2 components,
where the copula parameters θ varied harmonically (as in Eq. A4) but the mixing concentrations φ were con-
stant, were harder to identify. Table A5 shows that a few times, each of the algorithms selected a model that
was better than the true model (WAICbest - WAICtrue < 0). The greedy algorithm made one mistake, yet
the model it selected was very close to optimal. Such misidentification happens due to the limited number of
samples in a given synthetic dataset.
Tables A6-A7 show the model selection for 3 component models. Again, as in Tables A4-A5, either θ or φ was
constant. Here, the model selection algorithms could rarely identify the correct model (due to overcompleteness
of the mixture models), but always selected the one that was very close to optimal: WAICbest - WAICtrue
WAICtest_tol.
Note, that WAICtest_t°i is different from waic_tol. We have set WaicJtol for comparison against Inde-
pendent model to such a small value (10x smaller than WAICtest_t。。because we want to avoid making false
assumptions about conditional independences in the model. Also note, that the WAIC of the true model de-
pends on the particular synthetic dataset generated in each test. Therefore, the final WAIC in the left and in the
right columns of Tables A3-A7 can be slightly different (yet, right within WAICtest_t°i).
B.2	Accuracy of entropy estimation
GaUSSian entropy
GaUSSian entropy
Figure A4: Accuracy of the entropy estimation for multivariate Gaussian distributions. A Entropy of
the 20-dimensional multivariate Gaussian distribution for different correlation coefficients ρ. B Es-
timation error for the entropy shown in A. C Entropy of the multivariate Gaussian distributions with
ρ = 0.99 and varying dimensionality. D Estimation error for the entropy shown in C.
In this section, we consider a fixed copula mixture model with known parameters θ and test the reliability of
the entropy estimation with Monte Carlo (MC) integration. We test the accuracy of the entropy estimation on
a multivariate Gaussian distribution, with cov(yi , yj ) = ρ + (1 - ρ) δij , where δij is Kronecker’s delta and
ρ ∈ [0, 0.999]. Given a known Gaussian copula, we estimate the entropy with MC integration and compare it
to the analytically calculated true value. We set up a tolerance to ∆H = 0.01(dim y). As a result, for every
correlation ρ (Fig. A4A-B) and every number of dimensions dim y (Fig. A4C-D), the Copula-GP provides
an accurate result, within the error margin. In Figure A4, BI-KSG estimates (Gao et al., 2016) obtained on
the dataset with 10k samples are shown for comparison. This experiment 1) validates the MC integration;
2) validates the numerical stability of the probability density function of the Gaussian copula up to a specified
maximal ρ = 0.999 (for ρ > 0.999 the model is indistinguishable from the deterministic dependence u1 = u2).
17
Under review as a conference paper at ICLR 2021
B.3	Validation on a non-gaussian copula
Copula-GP is guaranteed to produce unbiased entropy estimates when the true dependency matches the para-
metric model (i.e. is Gaussian, Clayton, Frank or Gumbel, or any linear mixture of those), assuming that the
implementation of log-likelihood and sampling is correct, which is ensured by the tests and correctly estimated
parameters.
Figure 2A tested the entropy and mutual information estimates on the Gaussian copula. Here, we follow the
same procedure and generate samples from a C-Vine, in which each bivariate element is a Clayton copula. We
varied the parameter θ of the Clayton copula linearly from 0 to 2.
The observed result is qualitatively identical to the validation on a Gaussian copula model (see Fig. 2A):
Copula-GP measures both entropy and mutual information exactly (within integration tolerance, see Fig. A5),
while the performance of the non-parametric methods on this dataset is lower.
---CoPula-GP integrated
——BI-KSG
---KSG
——MINE100
——MINE500
---True
Figure A5: Conditional entropy H(y|x) (left panel) and mutual information I(x, y) (right panel)
measured by different methods on synthetic data, generated from Clayton copula. The performance
of various estimators is qualitatively similar to Fig. 2A with the Gaussian copula.
B.4	Performance on a UCI benchmark dataset
Here We compare our method With the similar method from LoPez-Paz et al. (2013) and Hernandez-Lobato
et al. (2013). Apart from differences in implementation (SVI vs. EP, (un)/conditional marginals), our Copula-
GP method has tWo conceptual improvements 1) We use non-parametric conditional marginals, instead of
unconditional; 2) We use linear mixtures of copula elements With qualitatively different tail dependencies.
Using the UCI shuttle dataset (Frank et al., 2011), We compare the performance of the Lopez-Paz model vs.
Copula-GP model Which Was either alloWed to use only one copula element, or a mixture of any number of
elements (selected With the model selection method, see Appx. A.4). We used vine copulas truncated at 3 or 5
trees, ran them on the training set and evaluated the log-likelihood on the test set (higher is better).
As a result, Copula-GP restricted to the use of only one copula element produced similar results to Lopez-Paz
et al. (2013), While our mixture model considerably outperformed both of these single element models.
Table A2: Average test log-likelihood for Copula-GP vs. Lopez-Paz et al. (2013)
Trees	Lopez-Paz et al.	Single copula	Mixture
3	3.645±0.427	3.759±0.307	4.326±0.170
9	4.755±0.389	4.470±0.215	5.115±0.150
C	Interpretation of Copula-GP models
C.1	Conditional vs. unconditional marginals
In this section, We explain the difference betWeen copula models With conditional vs. unconditional marginals.
Consider a pair of neurons, Which both fire strongly in a certain context (e.g. in the reWard zone of the VR
environment, see Fig. A6A). We simulate the activity of these neurons using a Poisson model (Fig. A6B) and
then convolve the simulated spike count data With the exponential kernel, in order to generate continuous data
similar to the recordings shoWn in Fig. 3. The resulting generated activity is shoWn in Fig. A6C.
Let us then use unconditional marginals to project simulated neuronal recordings onto a unit cube (Fig. A6D,
left). The color of the data point shoWs the position X, and one can see that the orange and red points (X > 0.5)
are concentrated in the upper right corner of the plot. If We plot a histogram for one of the neurons and only
18
Under review as a conference paper at ICLR 2021
A 2.0
1.5
0.5
1.0
0.0
0.0	0.2	0.4	0.6	0.8	1.0
X
20
30
40
B0
10
0
20
80
40	60
X
Unconditional marginals
1.0
0.5
0.0
0.0	0.5	1.0
Y1
500
400
300
200
100
0.0
0.5
1.0
Y0 for X>0.5
E
2.0
Copula parameters for 1 vs 0
(u n c
1.5
---- Clayton 0°
---- Clayton 180
1.0
O
U
Φl 0.5
0.0
0.0	0.2	0.4	0.6	0.8	1.0
Position in VR, [cm]
Mixing param for 1 vs 0
1.00
0.75
0.50
0.25
0.00
----Qayton 0°
---- Clayton 180
0.9-1.0
0.0	0.2	0.4	0.6	0.8	1.0
Position in VR, [cm]
Conditional marginals
1.0
0.5
0.0
0.0	0.5	1.0
Y1
彳
250
200
150
100
50
0.5
1.0
Y0 for X>0.5
0
0.0
0.0-0.6
1.0
0.5
0.0
0.0	0.5	1.0
Y1
0.6-0.7
1.0
0.5
0.0
0.0	0.5	1.0
0.7-0.9
1.0 -
0.5 -
0.0 -
1.0
0.5
0.0
0.0	0.5	1.0
0.0	0.5	1.0
D
G
0
X
F
等
Figure A6: Toy model of two independent neurons with the same conditional firing rate: A average
firing rate depending on X ; B raster plot with spike counts for one of the neurons; C simulated
calcium traces from 10 trials for one of the neurons; D data-points projected onto a unit cube by the
probability integral transform based on unconditional marginals; colored points show joint neural
data-point (Y0,Y1) recorded at X, where X is color-coded (as in A); the histogram on the right
shows conditional marginal distribution p(Y0|X > 0.5); E mixture model fit for the data in D
(with unconditional marginals); F density plots comparing predictions of the mixture model (shades)
with the actual data (black outlines); the fit is poor due to non-uniformity of empirical marginal
distributions; G same as D, but based on conditional marginals, estimated using fastKDE.
orange-red points, which corresponds to p(Yi |X > 0.5), we see that it is far from being uniform (Fig. A6D,
right).
In previous works on conditional copulas (Hemandez-Lobato et al., 2013), the marginals were assumed to be
unconditional. Such statistical copula-based models would ignore the non-uniformity of the empirical copula
marginals, and still attempt to describe the data with some heavy-tailed copula (Fig. A6E). However, uniformity
of copula marginals is one of the main model assumptions, and it does not hold in this case. As a result, the
model fit is poor (Fig. A6F).
Modelling with conditional marginals, as in our Copula-GP model, is radically different. The non-parametric
conditional marginal model (estimated with fastKDE) reflects the fact that both neurons are typically firing
strongly around X ≈ 0.7, and would transform the marginals accordingly (Fig. A6G, left). As a result, all
the model assumptions, including uniformity of copula marginals, will strictly hold (Fig. A6G, right). Addi-
tionally, our copula model would focus solely on the joint trial-to-trial variability (i.e. on noise correlations)
of these neurons, regardless of the absolute values of their firing rates. In this example, the model finds no
noise correlations (best fit for Fig. A6G is Independence), which is the correct result for the simulation of two
independent neurons.
Therefore, conditional marginals make our model more flexible and capable of holding the modelling assump-
tions even on complex neuronal data with highly variable marginal statistics. Also, for the neuronal data in
particular, the model becomes more interpretable, as the conditional copula c(ux|x) focuses only on the joint
trial-to-trial variability of neural activity. Therefore, the statistics corresponding to the network becomes en-
tirely separated from the single unit statistics. This makes our Copula-GP model particularly well suited for
neuroscience applications.
C.2 Model parameters for the bivariate neuronal and behavioral examples
In this section, we provide visualisations for the parameters of the bivariate copula models from Figure 3C-F
and discuss the interpretability of these models.
19
Under review as a conference paper at ICLR 2021
A Inter-neuronal SelSeCefTeiatIOnS
0-60	60-120	120-140
----Gaussian
----Clayton 90°
----Pearson's rho
mω uo∙!n0N
Pqz=ISE-IOUIS
Copula parameters θ
一 0.5
0	50	100	150
Position in VR, [cm]
Mixing parameters φ
1.0
回0.5
0.0
0	50	100	150
Position in VR, [cm]
havioral modulations
0-60	60-120	120-140	140-160
06 norueN
Frank
Clayton 0°
Gumbel 270°
Pearson's rho
Copula parameters Θ
.2 .0 .2
00H
p0E∙!oul6
1.0
Mixing parameters φ
50	100	150
Position in VR, [cm]
0	10	10	10	1
Licks
0.5
0.0
0	50	100	150	0
Position in VR, [cm]
Figure A7: Parameters of the copula mixture models. From left to right: copula probability densities
(same as Fig.3C-D); a list of selected copula elements; copula parameters θ; mixing concentrations
φ. These plots are provided for: A the noise correlation example; B the behavioral modulation
example.
Figure A7 shows the probability density of the joint distribution of two variables and the parameters of a
corresponding Copula-GP mixture model. The plots on the left repeat Fig.3C-D and represent the true density
(outlines) and the copula model density (blue shades) for each part of the task.
In the noise correlation example (Fig. A7A), we observe the tail dependencies between the variables (i.e.
concentration of the probability density in a corner of the unit square) around [0-60] cm and [140-160] cm of
the virtual corridor. There is only one element with a tail dependency in this mixture: Clayton 90° copula. On
the right-most plot in Fig. A7A, we see the mixing concentration for the elements of the mixture model. The
concentration of Clayton 90° copula (orange line) is close to 100% around 20 Cm and 150 cm, which agrees
with our observations from the density plots.
The confidence intervals (±2σ) for the parameters approximated with Gaussian processes are shown with
shaded areas in parameter plots. These intervals provide a measure of uncertainty in model parameters. For
instance, when the concentration of the Gaussian copula in the mixture is close to 0% (x around 20 cm and
150 cm), the confidence intervals for the Gaussian copula parameter (θ, blue shade) in Fig. A7A become very
wide (from almost 0 to 1). Since this copula element is not affecting the mixture for those values of x, its θ
parameter has no effect on the mixture model log probability. Therefore, this parameter is not constrained to
any certain value. In a similar manner, we see that the variables are almost independent between 60 and 120 cm
(see density plots on the left in Fig. A7). Both copula elements can describe this independence. As a result, the
mixing concentrations for both elements have high uncertainty in that interval of x. Yet, Gaussian copula with
a slightly positive correlation is still a bit more likely to describe the data in that interval.
The copula parameter plot in Fig. A7A also shows Pearson’s ρ, which does not change much in this example
and remains close to zero. This illustrates, that the traditional linear noise correlation analysis would ignore
(or downplay) this pair of neurons as the ones with no dependence. This happens because the Pearson’s ρ only
captures the linear correlation and ignores the tail dependencies, whereas our model provides a more detailed
description of the joint bivariate distribution.
In the behavioral modulation example (Fig. A7B), we observe more complicated tail dependencies in the den-
sity plots. The best selected model supports this observation and provides a mixture model with 3 components,
2 of which have various tail dependencies. The Clayton 0° copula (orange) describes the lower tail dependence
observed in the second part of the virtual corridor with gratings (around [60-120] cm, see Fig. 3A for task
structure). This dependence can be verbally interpreted as follows: when there is no licking, the Neuron 60 is
certainly silent; but when the animal is licking, the activity of Neuron 60 is slightly positively correlated with
the licking rate.
These examples illustrate, that by analysing the copula parameters and the mixing concentrations of the Copula-
GP mixture model, one can interpret the changes in the bivariate dependence structure. Just like traditional
tuning curves characterize the response of a single neuron, our mixture model characterizes the ‘tuning’ of
the dependence structure between pairs of variables to a given stimulus or context. Knowing the qualitative
properties of the copula elements that constitute a copula mixture, one can focus on the dominant element of
the copula mixture for every given conditioning variable x and describe the shape of the dependence.
20
Under review as a conference paper at ICLR 2021
C.3 Ablation study
After the model is selected, one can ablate the elements one by one and check the WAIC of these models.
Following this procedure, one can test whether all of the elements are important in the mixture.
We performed the ablation on the models from Fig. A7. All models after ablation had higher WAIC than the
originally selected model. Ablation of the Clayton copula in Fig. A7B increased WAIC the most (from -0.043
to -0.033), while both elements in Fig. A7A were equally important (WAIC after ablation was the same (within
tolerance)).
C.4 Detection of heavy tails
Since the Copula-GP framework is based on a linear mixture model, it is possible to calculate the probability
that a certain data-point was generated from a heavy-tailed component of the distribution (e.g. Clayton). Given
a mixture model with estimated parameters, described by:
K
c (u|x) =	φj (x)cj (u; θj (x)),
j=1
where ui = CDFi(yi|x), x is a conditioning variable and y is a vector of neuronal and/or behavioral record-
ings. Then, for a data-point (x, y), this probability is equal to:
p(y∣clayton,x) ∙ P(Clayton|x)
P(Clayton|x，y) =------------p(y∣χ)-----------
EjWClayton φj (X)Cj (U； θ (X))
Pj∈All φj (X)Cj (u； θj (X)).
By thresholding P(Clayton∣X, y) and P(y∣X), one can select those data-points that constitute the heavy tail of
the distribution, which is described by Clayton copulas in our copula mixture model.
Appendix references
James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational gaussian process classi-
fication. 2015.
Andrew Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian processes (KISS-
GP). In International Conference on Machine Learning, pages 1775-1784, 2015.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning, volume 2.
MIT press Cambridge, MA, 2006.
Tim Bedford and Roger M Cooke. Probability density decomposition for conditionally dependent random
variables modeled by vines. Annals of Mathematics and Artificial intelligence, 32(1-4):245-268, 2001.
Tim Bedford and Roger M Cooke. Vines: A new graphical model for dependent random variables. Annals of
Statistics, pages 1031-1068, 2002.
Elif F Acar, Christian Genest, and Johanna NesLehova. Beyond simplified Pair-CoPula constructions. Journal
of Multivariate Analysis, 110:74-90, 2012.
Ingrid Hobæk Haff, Kjersti Aas, and Arnoldo Frigessi. On the simplified pair-copula construction-simply
useful or too simplistic? Journal of Multivariate Analysis, 101(5):1296-1310, 2010.
David Lopez-Paz, Jose Miguel Hernandez-Lobato, and Ghahramani Zoubin. Gaussian process vine copulas for
multivariate dependence. In International Conference on Machine Learning, pages 10-18, 2013.
Jose Miguel Hernandez-Lobato, James R Lloyd, and Daniel Hernandez-Lobato. Gaussian process con-
ditional copulas with applications to financial time series. In C. J. C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Sys-
tems 26, pages 1736-1744. Curran Associates, Inc., 2013. URL http://papers.nips.cc/paper/
5084- gaussian- process- conditional- copulas- with- applications- to- financial- time- series.
pdf.
Andrew Frank, Arthur Asuncion, et al. UCI machine learning repository, 2010. URL http://archive. ics. uci.
edu/ml, 15:22, 2011.
21
Under review as a conference paper at ICLR 2021
Table A3: The model selection histories for 1-element mixtures
True Model	Greedy			Heuristic		
	Search attempts	WAIC	T		Search attempts	WAIC	T
Ga	^^Ga GaFr Ga	-0.1619 -0.1610 -0.1619	25 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Ga	-0.1513 -0.1499 -0.1498 -0.1513	3m
Fr	Tr FrCl90 FrCl90Gu270 FrCl90Gu270Gu90 Fr	-0.1389 -0.1395 -0.1396 -0.1396 -0.1389	57 m	^^Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Fr	-0.1400 -0.1391 -0.1391 -0.1509	3m
Cl0	^ci0 Cl0Gu0 Cl0Gu0Cl180 Cl0	-0.5225 -0.5226 -0.5225 -0.5224	37 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Cl0	-0.3825 -0.4943 -0.5303 -0.5311	5m
Gu0	^G50 Gu0Cl180 Gu0Cl180Gu180 Gu0	-0.6267 -0.6268 -0.6267 -0.6230	43 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 GaGu0 Gu0	-0.5555 -0.5988 -0.5946 -0.6040 -0.6050	7m
Cl90	Cl90 Cl90Cl270 Cl90	-0.5389 -0.5389 -0.5389	22 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Cl90	-0.3922 -0.5047 -0.5409 -0.5410	5m
Gu90	^GΞ90 Gu90Gu270 Gu90Gu270Cl270 Gu90Gu270Cl270Cl90 Gu90	-0.6137 -0.6144 -0.6145 -0.6144 -0.6137	55 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 GaGu90 Gu90	-0.5501 -0.5893 -0.5831 -0.5887 -0.5950	7m
Cl180	Cl180 Cl180Cl0 Cl180Cl0In Cl180	-0.5566 -0.5582 -0.5582 -0.5565	36m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Cl180	-0.3932 -0.4956 -0.5493 -0.5489	7m
Gu180	^GΞ180 Gu180Cl0 Gu180Cl0Fr Gu180	-0.6131 -0.6164 -0.6163 -0.6131	43 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Gu180	-0.5553 -0.6091 -0.6045 -0.6154	6m
Cl270	Cl270 Cl270Gu270 Cl270	-0.5434 -0.5433 -0.5434	23 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Cl270	-0.3909 -0.5094 -0.5535 -0.5548	5m
Gu270	^GΞ≡ Gu270Cl90 Gu270Cl90In Gu270Cl90InCl180 Gu270	-0.5928 -0.5934 -0.5935 -0.5931 -0.5928	51 m	-Ga InGaGu180Gu270 Gu0Gu90 InGaCl0Cl90Cl180Cl270 Gu270	-0.5763 -0.6277 -0.6179 -0.6300	6m
22
Under review as a conference paper at ICLR 2021
Table A4: The model selection histories for 2-element mixtures with constant θ and variable φ
True	Greedy			Heuristic		
Model	Search attempts	WAIC	T	Search attempts	WAIC	T
Gu90	^^Ga	-0.1877	101m	Ga	-0.1922	11m
Ga	GaGu90	-0.2855		InGaGu180Gu270Gu0Gu90	-0.3070	
	GaGu90 Cl270	-0.2855		InGaCl0Cl90Cl180Cl270	-0.2996	
	GaGu90 Cl270Fr	-0.2856		GaCl0Gu0Gu90	-0.3082	
	GaGu90 Cl270FrGu270	-0.2856		GaCl0Cl180Gu90	-0.3076	
	GaGu90Cl270FrGu270-	-0.2856		GaGu90	-0.3091	
	C严					
	Gu90Ga	-0.2854				
Ga	^^Fr	-0.1635	87 m	Ga	-0.1600	5m
Cl270	FrCl270	-0.2707		InGaGu180Gu270Gu0Gu90	-0.2687	
	FrCl270Ga	-0.2747		InGaCl0Cl90Cl180Cl270	-0.2835	
	FrCl270GaGu180	-0.2782		GaCl270	-0.2845	
	FrCl270GaGu180Cl90	-0.2781				
	GaCl270	-0.2821				
Gu180	^GΞ180	-0.1681	99 m	Ga	-0.1534	8m
Fr	Gu180Fr	-0.2099		InGaGu180Gu270Gu0Gu90	-0.1993	
	Gu180FrCl180	-0.2101		InGaCl0Cl90Cl180Cl270	-0.1977	
	Gu180FrCl180Cl90	-0.2105		InGaGu180	-0.2074	
	Gu180FrCl180Cl90In	-0.2106		FrGu180	-0.2104	
	Gu180FrCl180Cl90In-	-0.2099				
	Gu270					
	FrGu180	-0.2099				
Cl0	^^Fr	-0.1587	92m	Ga	-0.1652	5m
Cl90	FrCl0	-0.2600		InGaGu180Gu270Gu0Gu90	-0.3142	
	FrCl0Cl90	-0.3173		InGaCl0Cl90Cl180Cl270	-0.3430	
	FrCl0Cl90Gu270	-0.3176		Cl0Cl90	-0.3448	
	FrCl0Cl90Gu270In	-0.3176				
	FrCl0Cl90Gu270InCl270	-0.3175				
	Cl90Cl0	-0.3190				
Cl180	^^Fr	-0.2204	103m	Ga	-0.1965	7m
Gu270	FrCl180	-0.3488		InGaGu180Gu270Gu0Gu90	-0.3591	
	FrCl180Gu270	-0.3874		InGaCl0Cl90Cl180Cl270	-0.3688	
	FrCl180Gu270Cl90	-0.3877		GaGu270Cl180	-0.3771	
	FrCl180Gu270Cl90Ga	-0.3878		Gu270Cl180	-0.3772	
	FrCl180Gu270Cl90Ga-	-0.3878				
	Gu90					
	Gu270Cl180	-0.3888				
Table A5: The model selection histories for 2-element mixtures with constant φ and variable θ
True		Greedy			Heuristic		
Model	Search attempts		WAIC	T		Search attempts	WAIC	T
Gu90	^GΞ90		-0.1419	60 m	-Ga	-0.1538	10m
Ga	Gu90Fr		-0.2022		InGaGu180Gu270Gu0Gu90	-0.2320	
	Gu90FrCl270		-0.2024		InGaCl0Cl90Cl180Cl270	-0.2218	
	Gu90FrCl270Ga		-0.2024		GaCl90Gu0 Gu90	-0.2321	
	FrGu90		-0.2021		GaGu90	-0.2326	
	WAICbest - WAICtrue:		-0.0013				
Ga	Gu90		-0.1495	56m	^^Ga	-0.1062	7m
Cl270	Gu90Fr		-0.1894		InGaGu180Gu270Gu0Gu90	-0.1747	
	Gu90FrCl270		-0.1915		InGaCl0Cl90Cl180Cl270	-0.1783	
	Gu90FrCl270In		-0.1902		GaGu0Cl270	-0.1812	
	Cl270FrGu90		-0.1915		GaCl270	-0.1801	
	WAICbest - WAICtrue:		0.0032				
23
Under review as a conference paper at ICLR 2021
True	Greedy	Heuristic
Model	Search attempts	WAIC T	Search attempts	WAIC T
Gu180	Gu180	-0.1600^^58m	-Ga	-0.1331 ^^8m-
Fr	Gu180 Fr	-0.2191	InGaGu180Gu270Gu0Gu90	-0.1944
	Gu180 FrCl270	-0.2195	InGaCl0Cl90Cl180Cl270	-0.1936
	Gu180FrCl270Cl0	-0.2190	GaGu180Cl90Gu0Gu90	-0.1945
	FrGu180	-0.2190	GaGu180	-0.1992
		WAICbest- WAICtrue:	-0.0094
Cl0	Gu180	-0.0253^^62 m	^^Ga	-0.0079^^5m-
Cl90	Gu180 Cl90	-0.2383	InGaGu180Gu270Gu0Gu90	-0.1904
	Gu180 Cl90Cl0	-0.2506	InGaCl0Cl90Cl180Cl270	-0.2330
	Gu180 Cl90Cl0In	-0.2509	Cl0Cl90	-0.2361
	Gu180 Cl90Cl0InFr	-0.2508	
	Cl0Cl90	-0.2586	
Cl180	Gu270	-0.0242^^69 m	-Ga	-0.0083^^6m-
Gu270	Gu270 Cl180	-0.2499	InGaGu180Gu270Gu0Gu90	-0.2277
	Gu270 Cl180Gu180	-0.2517	InGaCl0Cl90Cl180Cl270	-0.2535
	Gu270 Cl180Gu180In	-0.2518	GaCl90Cl180	-0.2549
	Gu270 Cl180Gu180InCl0	-0.2518	
	Gu270 Cl180Gu180InCl0Fr	-0.2518	
	Cl180Gu270	-0.2500	
		WAICbest - WAICtrue:	-0.0098	
Table A6: The model selection histories for 3-element mixtures with constant θ and variable φ
True Model	Greedy			Heuristic		
	Search attempts	WAIC	T		Search attempts	WAIC	T
Ga Cl90 Gu0	^G50 Gu0Cl90 Gu0Cl90Cl0 Gu0Cl90Cl0 Fr Cl90Gu0 WAICbest- WAICtrue:	-0.1399 -0.2494 -0.2519 -0.2518 -0.2494 -0.0036	44 m	^^Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 GaCl90Cl180 WAICbest- WAICtrue:	-0.1252 -0.2481 -0.2565 -0.2564 0.0014	6m
Fr Cl90 Gu0	Tr FrCl90 FrCl90Gu0 FrCl90Gu0Cl180 FrCl90Gu0Cl180In Gu0Cl90Fr	-0.0591 -0.1460 -0.1730 -0.1736 -0.1734 -0.1731	77 m	^^Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 GaCl90Cl180 WAICbest — WAICtrue:	-0.0489 -0.1573 -0.1578 -0.1621 0.0059	6m
Fr Cl180 Gu270	^^Fr FrCl180 FrCl180Gu270 FrCl180Gu270Cl90 FrCl180Gu270Cl90Gu180 FrCl180Gu270Cl90Gu180- Cl0 Gu270Cl180Fr	-0.0741 -0.1513 -0.1707 -0.1708 -0.1711 -0.1710 -0.1703	87 m	^^Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 InGaGu270Cl180Cl270 InGaGu270Cl180Gu90 InGu270Cl180 WAICbest — WAICtrue:	-0.0618 -0.1567 -0.1670 -0.1680 -0.1695 -0.1735 -0.0011	9m
Gu0 Gu180 Cl90	^G50 Gu0Cl90 Gu0Cl90Gu180 Gu0Cl90Gu180Cl180 Gu180Cl90Gu0	-0.1695 -0.3040 -0.3234 -0.3233 -0.3234	47 m	^^Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 GaGu180Cl90Cl180 GaGu180Cl90Gu0 Gu180Cl90Gu0	-0.1477 -0.2986 -0.3033 -0.3054 -0.3111 -0.3113	11m
24
Under review as a conference paper at ICLR 2021
Table A7: The model selection histories for 3-element mixtures with constant φ and variable θ
True Model	Greedy			Heuristic		
	Search attempts	WAIC	T		Search attempts	WAIC	T
Ga Cl90 Gu0	^^Fr FrGu270 FrGu270Gu0 FrGu270Gu0Cl0 FrGu270Gu0Cl0Cl180 FrGu270Gu0Cl0Cl180In Gu0Gu270 WAICbest - WAICtrue:	-0.0177 -0.1284 -0.1407 -0.1423 -0.1435 -0.1432 -0.1451 0.0132	66 m	^^Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 InCl0Gu270Gu0 InCl0Cl90Gu0 InCl0Cl90Cl180 GaCl90Cl180 WAICbest - WAICtrue:	-0.0142 -0.1291 -0.1289 -0.1317 -0.1346 -0.1301 -0.1313 0.0068	13m
Fr Cl90 Gu0	^^Fr FrGu270 FrGu270Gu0 FrGu270Gu0Cl180 FrGu270Gu0Cl180Cl0 FrGu270Gu0Cl180Cl0In Gu0Gu270 WAICbest - WAICtrue：	-0.0265 -0.1290 -0.1445 -0.1450 -0.1466 -0.1468 -0.1451 0.0109	71m	^^Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 InGaGu180Cl90Cl180 InGaGu180Cl90Gu0 InCl90Gu0 WAICbest - WAICtrue:	-0.0192 -0.1411 -0.1429 -0.1474 -0.1472 -0.1477 0.0010	9m
Fr Cl180 Gu270	Tr FrGu270 FrGu270Gu0 FrGu270Gu0Cl180 FrGu270Gu0Cl180Gu180 Gu0Gu270Fr WAICbest- WAICtrue：	-0.0129 -0.1105 -0.1237 -0.1254 -0.1248 -0.1234 0.0094	61 m	-Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 InGu270Cl180 InGu270Gu0 InGu270Gu0 WAICbest - WAICtrue:	-0.0185 -0.1309 -0.1326 -0.1393 -0.1334 -0.1326 0.0088	6m
Gu0 Gu180 Cl90	^G50 Gu0Cl90 Gu0Cl90Cl0 Gu0Cl90Cl0 Ga Gu0Cl90Cl0 GaCl270 Cl0Cl90Gu0 WAICbest - WAICtrue:	-0.0756 -0.2380 -0.2556 -0.2591 -0.2590 -0.2555 0.0026	55 m	-Ga InGaGu180Gu270Gu0Gu90 InGaCl0Cl90Cl180Cl270 GaCl0Gu270Gu0 GaCl0Cl90Gu0 Cl0Cl90Gu0 WAICbest - WAICtrue:	-0.0454 -0.2476 -0.2459 -0.2493 -0.2559 -0.2538 0.0006	7m
25