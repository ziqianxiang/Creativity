Under review as a conference paper at ICLR 2021
Quickly Finding a Benign Region via Heavy
Ball Momentum in Non-Convex Optimization
Anonymous authors
Paper under double-blind review
Ab stract
The Heavy Ball Method (Polyak, 1964), proposed by Polyak over five decades
ago, is a first-order method for optimizing continuous functions. While its stochas-
tic counterpart has proven extremely popular in training deep networks, there are
almost no known functions where deterministic Heavy Ball is provably faster than
the simple and classical gradient descent algorithm in non-convex optimization.
The success of Heavy Ball has thus far eluded theoretical understanding. Our
goal is to address this gap, and in the present work we identify two non-convex
problems where we provably show that the Heavy Ball momentum helps the it-
erate to enter a benign region that contains a global optimal point faster. We
show that Heavy Ball exhibits simple dynamics that clearly reveal the benefit
of using a larger value of momentum parameter for the problems. The first of
these optimization problems is the phase retrieval problem, which has useful ap-
plications in physical science. The second of these optimization problems is the
cubic-regularized minimization, a critical subroutine required by Nesterov-Polyak
cubic-regularized method (Nesterov & Polyak (2006)) to find second-order sta-
tionary points in general smooth non-convex problems.
1	Introduction
Poylak’s Heavy Ball method (Polyak (1964)) has been very popular in modern non-convex optimiza-
tion and deep learning, and the stochastic version (a.k.a. SGD with momentum) has become the de
facto algorithm for training neural nets. Many empirical results show that the algorithm is better
than the standard SGD in deep learning (see e.g. Hoffer et al. (2017); Loshchilov & Hutter (2019);
Wilson et al. (2017); Sutskever et al. (2013)), but there are almost no corresponding mathematical
results that show a benefit relative to the more standard (stochastic) gradient descent. Despite its pop-
ularity, we still have a very poor justification theoretically for its success in non-convex optimization
tasks, and Kidambi et al. (2018) were able to establish a negative result, showing that Heavy Ball
momentum cannot outperform other methods in certain problems. Furthermore, even for convex
problems it appears that strongly convex, smooth, and twice differentiable functions (e.g. strongly
convex quadratic functions) are one of just a handful examples for which a provable speedup over
standard gradient descent can be shown (e.g (Lessard et al., 2016; Goh, 2017; Ghadimi et al., 2015;
Gitman et al., 2019; Loizou & Richtarik, 2017; 2018; Gadat et al., 2016; ScieUr & Pedregosa, 2020;
Sun et al., 2019; Yang et al., 2018a; Can et al., 2019; Liu et al., 2020; Sebbouh et al., 2020; Flam-
marion & Bach, 2015)). There are even some negative results when the function is strongly convex
but not twice differentiable. That is, Heavy Ball momentum might lead to a divergence in convex
optimization (see e.g. (Ghadimi et al., 2015; Lessard et al., 2016)). The algorithm’s apparent success
in modern non-convex optimization has remained quite mysterious.
In this paper, we identify two non-convex optimization problems for which the use of Heavy Ball
method has a provable advantage over vanilla gradient descent. The first problem is phase re-
trieval. It has some useful applications in physical science such as microscopy or astronomy (see e.g.
(Candes et al., 2013), (Fannjiang & Strohmer, 2020), and (Shechtman et al., 2015)). The objective
is
minw∈Rd f (w) ：= 41n Pn=I ((x>W) - y，2
(1)
where Xi ∈ Rd is the design vector and yn = (x>w*)* 2 is the label of sample i. The goal is to
recover w* UP to the sign that is not recoverable (CandeS et al., 2013). Under the Gaussian design
1
Under review as a conference paper at ICLR 2021
setting (i.e. Xi 〜N(0, Id)), it is known that the empirical risk minimizer (1) is w* or -w*, as long
as the number of samples n exceeds the order of the dimension d (see e.g. Bandeira et al. (2014)).
Therefore, solving (1) allows one to recover the desired vector w* ∈ Rd up to the sign. Unfortunately
the problem is non-convex which limits our ability to efficiently find a minimizer. For this problem,
there are many specialized algorithms that aim at achieving a better computational complexity and/or
sample complexity to recover w* modulo the unrecoverable sign (e.g. (Cai et al., 2016; Candes & Li,
2014; CandeS et al., 2015; 2013; Chen & Candes, 2017; Duchi & Ruan, 2018; Ma et al., 2017; 2018;
Netrapalli et al., 2013; Qu et al., 2017; Tu et al., 2016; Wang et al., 2017a;b; Yang et al., 2018b;
Zhang et al., 2017a;b; Zheng & Lafferty, 2015)). Our goal is not about providing a state-of-the-art
algorithm for solving (1). Instead, we treat this problem as a starting point of understanding Heavy
Ball momentum in non-convex optimization and hope for getting some insights on why Heavy Ball
(Algorithm 1 and 2) can be faster than the vanilla gradient descent in non-convex optimization and
deep learning in practice. If we want to understand why Heavy Ball momentum leads to acceleration
for a complicated non-convex problem, we should first understand it in the simplest possible setting.
We provably show that Heavy Ball recovers the desired vector w*, up to a sign flip, given a random
isotropic initialization. Our analysis divides the execution of the algorithm into two stages. In
the first stage, the ratio of the projection of the current iterate wt on w* to the projection of wt
on the perpendicular component keeps growing, which makes the iterate eventually enter a benign
region which is strongly convex, smooth, twice differentiable, and contains a global optimal point.
Therefore, in the second stage, Heavy Ball has a linear convergence rate. Furthermore, up to a value,
a larger value of the momentum parameter has a faster linear convergence than the vanilla gradient
descent in the second stage. Yet, most importantly, we show that Heavy Ball momentum also has
an important role in reducing the number of iterations in the first stage, which is when the iterate
might be in a non-convex region. We show that the higher the momentum parameter β, the fewer the
iterations spent in the first stage (see also Figure 1). Namely, momentum helps the iterate to enter a
benign region faster. Consequently, using a non-zero momentum parameter leads to a speedup over
the standard gradient descent (β = 0). Therefore, our result shows a provable acceleration relative
to the vanilla gradient descent, for computing a global optimal solution in non-convex optimization.
The second of these is solving a class of cubic-regularized problems,
minw f (W) := 1 w>Aw + b>w + P kwk3,	(2)
where the matrix A ∈ Rd×d is symmetric and possibly indefinite. Problem (2) is a sub-routine of the
Nesterov-Polyak cubic-regularized method (Nesterov & Polyak (2006)), which aims to minimize a
non-convex objective F(∙) by iteratively solving
wt+1 = argminw∈Rd{VF(Wt)>(w - Wt) + 1 (W - wt)>V2F(w)(w - Wt) + P∣∣w - wt∣∣3},
(3)
With some additional post-processing, the iterate Wt converges to an (g, h) second order station-
ary point, defined as {W : ∣Vf (W)∣ ≤ g and V2f(W)	-hId} for any small g, h > 0.
However, their algorithm needs to compute a matrix inverse to solve (2), which is computationally
expensive when the dimension is high. A very recent result due to Carmon & Duchi (2019) shows
that vanilla gradient descent approximately finds the global minimum of (2) under mild conditions,
which only needs a Hessian-vector product and can be computed in the same computational com-
plexity as computing gradients (Pearlmutter, 1994), and hence is computationally cheaper than the
matrix inversion of the Hessian. Our result shows that, similar to the case of phase retrieval, the
use of Heavy Ball momentum helps the iterate to enter a benign region of (3) that contains a global
optimal solution faster, compared to vanilla gradient descent. For certain non-convex problems, e.g.
dictionary learning (Sun et al., 2015), matrix completion (Chi et al., 2019), robust PCA (Ge et al.,
2017), and learning a neural network (Ge et al. (2019); Bai & Lee (2020)), where it suffices to find
a second-order stationary point, our result consequently might have application.
To summarize, our theoretical results of the two non-convex problems provably show the benefit
of using Heavy Ball momentum. Compared to the vanilla gradient descent, the use of momentum
helps to accelerate the optimization process. The key to showing the acceleration in getting into
benign regions of these problems is a family of simple dynamics due to Heavy Ball momentum. We
will argue that the simple dynamics are not restricted to the two main problems considered in this
paper. Specifically, the dynamics also naturally arise when solving the problem of top eigenvector
computation (Golub & Loan, 1996) and the problem of saddle points escape (e.g. (Jin et al., 2017;
2
Under review as a conference paper at ICLR 2021
Wang et al., 2020)), which might imply the broad applicability of the dynamics for analyzing Heavy
Ball in non-convex optimization.
Algorithm 1: Heavy Ball method (Polyak, 1964) (Equivalent version 1)	Algorithm 2: Heavy Ball method (Polyak, 1964) (Equivalent version 2)
1: Required: steP size η and momentum	1: Required: steP size η and momentum Parameter
Parameter β ∈ [0, 1].	β ∈ [0, 1].
2: Init: w0 = w-1 ∈ Rd	2: Init: W0 ∈ Rd and m-1 = 0.
3: for t = 0 to T do	3: for t = 0 to T do
4:	UPdate iterate	4:	UPdate momentum mt := βmt-1 + Vf (Wt).
wt+1 = Wt - ηVf (Wt) + β(wt - Wt-l)	5:	UPdate iterate Wt+1 := Wt - ηmt.
5: end for	6: end for
2	More related works
Heavy Ball (HB): HB has two exactly equivalent presentations in the literature (see Algorithm 1
and 2). Given the same initialization, both algorithms generate the same sequence of {wt}. In
Algorithm 2, We note that the momentum mt can be written as mt = P：=。βt-sVf (Ws) and can
be viewed as a weighted sum of gradients. As we described in the opening paragraph, there is little
theory of showing a provable acceleration of the method in non-convex optimization. The only
exception that we are aware of is (Wang et al., 2020). They show that HB momentum can help to
escape saddle points faster and find a second-order stationary point faster for smooth non-convex
optimization. They also observed that stochastic HB solves (1) and that using higher values of the
momentum parameter β leads to faster convergence. However, while their work focused on the
stochastic setting, their main result required some assumptions on the statistical properties of the
sequence of observed gradients; it is not clear whether these would hold in general. In appendix A,
we provide a more detailed literature review of HB. To summarize, current results in the literature
imply that we are still very far from understanding deterministic HB in non-convex optimization, let
alone understanding the success of stochastic HB in deep learning. Hence, this work aims to make
progress on a simple question: can we give a precise advantage argument for the acceleration effect
of Heavy Ball in the deterministic setting?
Phase retrieval: The optimization landscape of problem (1) and its variants has been studied by
(Davis et al., 2018; Soltanolkotabi, 2014; Sun et al., 2016; White et al., 2016), which shows that as
long as the number of samples is sufficiently large, it has no spurious local optima. We note that the
problem can also be viewed as a special case of matrix sensing (e.g. Li et al. (2018); Gunasekar et al.
(2017); Li & Lin (2020); Li et al. (2019); Gidel et al. (2019); You et al. (2020)); in Appendix A,
we provide a brief summary of matrix sensing. For solving phase retrieval, Mannellia et al. (2020)
study gradient flow, while Chen et al. (2018) show that the standard gradient descent with a random
initialization like Gaussian initialization solves (1) and recovers w* UP to the sign. Tan & Vershynin
(2019) show that online gradient descent with a simple random initialization can converge to a
global oPtimal Point in an online setting where fresh samPles are required for each steP. In this
PaPer, we show that Heavy Ball converges even faster than the vanilla gradient descent. Zhou et al.
(2016) ProPose leveraging Nesterov’s momentum to solve Phase retrieval. However, their aPProach
requires delicate and comPutationally exPensive initialization like sPectral initialization so that the
initial Point is already within the neighborhood of a minimizer. Similarly, Xiong et al. (2018; 2020)
show local convergence of Nesterov’s momentem and Heavy Ball momentum for Phase retrieval,
but require the initial Point to be in the neighborhood of an oPtimal Point. Jin et al. (2018) ProPose
an algorithm that uses Nesterov’s momentum together as a subroutine with Perturbation for finding
a second-order stationary Point, which could be aPPlied for solving Phase retrieval. ComPared to
(Zhou et al., 2016; Jin et al., 2018; Xiong et al., 2018; 2020), we consider directly aPPlying gradient
descent with Heavy Ball momentum (i.e. HB method) to the objective function with simple random
initialization, e.g. Gaussian initialization, which is what PeoPle do in Practice and is what we want
to understand. The goals of the works are different. Finally, we note that there are some efforts
in integrating the technique of generative models and Phase retrieval, which could helP the task
of image recovery (e.g. Hand et al. (2018)). Phase retrieval might also be a good entry Point
of understanding some observations in oPtimization and neural net training (e.g. Mannellia et al.
(2020)).
3
Under review as a conference paper at ICLR 2021
(a) Obj. value (1) vs. t	(b) |wtk| vs. t	(c) kwt⊥k vs. t
Figure 1: Performance of HB with different β = {0, 0.3, 0.5, 0.7, 0.9, 1.0 → 0.9} for phase retrieval. (En-
larged figures are available in Appendix H.) Here “1.0 → 0.9” stands for using parameter β = 1.0 in the first
few iterations and then switching to using β = 0.9 after that. In our experiment, for the ease of implementa-
tion,We let the criteria of the switch be 叫 f(WI(Wf)(Wt)
≥ 0.5}, i.e. if the relative change of objective value
compared to the initial value has been increased to 50%. Algorithm 3 and Algorithm 4 in Appendix H describe
the procedures. All the lines are obtained by initializing the iterate at the same point wο 〜N(0, Id/(10000d))
and using the same step size η = 5 × 10-4. Here we set w* = eι and sample Xi 〜N(0, Id) with dimension
d = 10 and number of samples n = 200. We see that the higher the momentum parameter β, the faster the
algorithm enters the linear convergence regime. (a): Objective value (1) vs. iteration t. We see that the higher
the momentum parameter β, the faster the algorithm enters the linear convergence regime. (b): The size of
projection of wt on w* over iterations (i.e. |wtk | vs. t), which is non-decreasing throughout the iterations until
reaching an optimal point (here, kw* k = 1). (c): The size of the perpendicular component over iterations (i.e.
kwt⊥ k vs. t), which is increasing in the beginning and then it is decreasing towards zero after some point. We
see that the slope of the curve corresponding to a larger momentum parameter β is steeper than that ofa smaller
one, which confirms Lemma 1 and Lemma 3.
3	Phase Retrieval
3.1	Preliminaries
Following the works of Candes et al. (2013); Chen et al. (2018), we assume that the design vectors
{xi} (which are known a priori) are from Gaussian distribution Xi 〜N(0, Id). Furthermore, without
loss of generality, we assume that W* = e1 (so that ∣W* ∣ = 1), where e1 is the standard unit vector
whose first element is 1. We also denote Wtk := Wt[1] and Wt,⊥ := [Wt[2], . . . , Wt[d]]> . That is,
Wtk is the projection of the current iterate Wt on W*, while Wt⊥ is the perpendicular component.
Throughout the paper, the subscript t is an index of the iterations while the subscript i is an index of
the samples.
Before describing the main results, we would like to provide a preliminary analysis to show how
momentum helps. Applying gradient descent with Heavy Ball momentum (Algorithm 1) to ob-
jective (1), we see that the iterate is generated according to Wt+1 = Wt — η 1 En=I ((x>Wt)3 —
(xi>Wt)yi xi + β(Wt - Wt-1). On the other hand, the population counterpart (i.e. when the number
of samples n is infinite) of the update rule turns out to be the key to understanding momentum. The
population gradient VF(W) := Ex〜N(0,In)[Vf (w)] is (proof is available in appendix B)
VF (W) = (3∣∣w∣∣2 — l)w — 2(w> w)w*.
(4)
Using the population gradient (4), we have the population update, Wt+1 = Wt - ηVF(Wt) +β(Wt -
Wt-1), which can be decomposed as follows:
wk+ι = (1 + 3η(I- kwtk2))wtl + β(Wk- Wk-I)
w⊥+ι = (1 + η(I- 3kwtk2))w⊥ + β(W⊥ - Wt-I).
(5)
Assume that the random initialization satisfies ∣∣w01∣2《1. From the population recursive system
(5), both the magnitude of the signal component wtk and the perpendicular component wt⊥ grow
exponentially in the first few iterations.
4
Under review as a conference paper at ICLR 2021
Lemma 1. For a positive number θ and the momentum parameter β ∈ [0, 1], if a non-negative
sequence {at} satisfies a0 ≥ a-1 > 0 and that for all t ≤ T,
at+1 ≥ (1 + θ)at + β(at - at-1),	(6)
then {at} satisfies
at+1 ≥ (1 + (1 + ∣ι+θ )θ) at,	⑺
for every t = 1, . . . , T + 1. Similarly, if a non-positive sequence {at} satisfies a0 ≤ a-1 < 0 and
that for all t ≤ T, at+1 ≤ (1 + θ)at + β(at - at-1), then {at} satisfies
at+1 ≤ (1 + (1 + ∣ι+θ )θ) at,	⑻
for every t = 1, . . . , T + 1.
One can view at in Lemma 1 as the projection of the current iterate wt onto a vector of interest. The
lemma says that with a larger value of the momentum parameterβ, the magnitude ofat is increasing
faster. It also implies that if the projection due to vanilla gradient descent satisfies at+1 ≥ (1 + θ)at,
then the magnitude of the projection only grows faster with the use of Heavy Ball momentum. The
dynamics in the lemma are the keys to showing that Heavy Ball momentum accelerates the process
of entering a benign (convex) region. The factor ɪ++^ θ in (7) and (8) represents the contribution due
to the use of momentum, and the contribution is larger with a larger value of momentum parameter
β. Now let us apply Lemma 1 to the recursive system (5) and pretend that the magnitude of kwt k
was a constant for a moment. Denote θt := 3η(1 -Ilwtk2) and Gt= η(1 一 3∣wt∣2) and notice that
θt > θt > 0 when kwtk2 < 3. We can rewrite the recursive system as
wtk+1 = (1 + θt)wtk + β(wtk - wtk-1)
w⊥+1 = (I + θ)w⊥ + β(w⊥ - w⊥-1).
(9)
Since the above system is in the form of (6), the dynamics (7) and (8) in Lemma 1 suggest that the
larger the momentum parameter β, the faster the growth rate of the magnitude of the signal compo-
nent wtk and the perpendicular component wt⊥. Moreover, the magnitude of the signal component wtk
grows faster than that of the perpendicular component wt⊥. Both components will grow until the size
of iterate Ilwtk2 is SUficiently Iarge (i.e. ∣∣wtk2 > 1). After that，the magnitude of the perpendicular
component wt⊥ starts decaying, while |wtk | keeps growing until it approaches 1. Furthermore, we
have that the larger the momentum parameter β, the faster the decay rate of the (magnitude of the)
perpendicular component wt⊥ . In other words, |wtk | converges to 1 and wt⊥ converges to 0 quickly.
Lemma 3 in Appendix C, which is a counterpart of Lemma 1, can be used to explain the faster decay
of the magnitude due to a larger value of the momentum parameter β .
By using the population recursive system (5) and Lemma 1 & 3 as the tool, we obtain a high-level
insight on how momentum helps (see also Figure 1). The momentum helps to drive the iterate to
enter the neighborhood of a w* (or -w*) faster.
3.2	Main results
We denote dist(wt, w*) := min{kwt -w* k2, kwt+w* k2} as the distance between the current iterate
wt and w* , modulo the unrecoverable sign. Note that both ±w* achieve zero testing errors. Further-
more, as long as the number of samples is sufficiently large, i.e. n & dlogd, there exists a constant
H > 0 so that the Hessian satisfies V2f (w)占 HId for all w ∈ IBZ(土w*) With high probability,
where BZ (±w*) represents the balls centered at ±w* with a radius Z (e.g. (Ma et al., 2017)). So in
this paper We consider the case that the local strong convexity holds in the neighborhood BZ (±w*).
We will divide the iterations into two stages. The first stage consists of those iterations that satisfy
0 ≤ t ≤ TZ, where TZ is defined as
TZ= min{t ： Ilwkl 一 1| ≤ 2 and kw⊥k ≤ 2},	(10)
and ζ > 0 is sufficiently small so that it makes wTζ be in the neighborhood of w* or -w* which
is smooth, twice differentiable, strongly convex, see e.g. (Ma et al., 2017; Soltanolkotabi, 2014).
5
Under review as a conference paper at ICLR 2021
Observe that if ||wk| - 1| ≤ 2 and ∣∣w⊥∣∣ ≤ 2, We have that dist(wt,w*) ≤ ∣∣wt - wj ≤
||wtk | - 1| + kwt⊥ k ≤ ζ. The second stage consists of those iterations that satisfy t ≥ Tζ . Given
that dist(wτ<,w*) ≤ Z and that the local strong convexity holds in IBZ(±w*), the iterate Wt would
be in a benign region at the start of this stage, Which alloWs linear convergence to a global optimal
point. That is, we have that dist(wt, w*) ≤ (1 - V)t-TζZ for all t ≥ TZ, where 1 > ν > 0 is some
number. Since the behavior of the momentum method in the second stage can be explained by the
existing results (e.g. Section 3 of Saunders (2018) or Xiong et al. (2020)), the goal is to understand
why momentum helps to drive the iterate into the benign region faster.
To deal with the case that only finite samples are available in practice, we will consider some per-
turbations from the population dynamics (5). In particular, we consider
wt+ι = (1 + 3η(1 - kwtk2) + ηξt)wk + β(wk - Wk-I)
w⅛ι[j] = (1 + η(1 - 3kwtk2) + ηρt,j)w⊥[j] + β(w⊥[j] - w⊥-ι[j]),
(11)
where {ξt} and {ρt,j} for 1 ≤ j ≤ d- 1 are the perturbation terms. The perturbation terms are used
to model the deviation from the population dynamics. In this paper, we assume that there exists a
small number Cn > 0 such that for all iterations t ≤ TZ and all j ∈ [d - 1], max{∣ξt∣, ∣ρt,j ∣} ≤ cn,
where the value cn should decay when the number of samples n is increasing and cn u 0 when there
are sufficiently large number of samples n.
Theorem 1. Suppose that the approximated dynamics (11) holds with max{∣ξt∣, ∣ρt,j∣} ≤ Cn for
all iterations t ≤ TZ and all dimensions j ∈ [d - 1], where cn ≤ Z. Suppose that the local
strong convexity holds in BZ (±w*). Assume that the initial point wo satisfies |w0| & √d 上 & and
∣∣W0∣ < 3. Set the momentum parameter β ∈ [0,1]. Assume that the norm of the momentum ∣mt∣
is bounded for all t ≤ TZ, i.e. ∣mt ∣ ≤ Cm for some constant Cm > 0. If the step size η satisfies
_________1________
36(1+z) max{cm,1}
η≤
, then Heavy Ball (Algorithm 1 & 2) takes at most
log d
η (1 + Cnβ )
number of iterations to enter the benign region BZ(w*) or BZ (-w*), where Cn
—1- ≤ 1
i+n/2 ≤ 1.
Furthermore, for allt ≥ TZ, the distance is shrinking linearly for some values ofη and β < 1. That
is, we have that dist(wt, w*) ≤ (1 — V )t-Tζ Z ,for some number 1 > ν > 0.
The theorem states that the number of iterations required for gradient descent to enter the linear
convergence is reduced by a factor of (1 + Cnβ), which clearly demonstrates that momentum helps
to drive the iterate into a benign region faster. The constant Cn suggests that the smaller the step size
η, the acceleration due to the use of momentum is more evident. The reduction can be about u 1 +β
for a small η. After TZ, Heavy Ball has a locally linear convergence to w* or -w*. Specifically, if
w0 > 0, then we will have that wk > 0 for all t and that the iterate will converge to w*; otherwise,
we will have wtk < 0 for all t and that the iterate will converge to -w*. The proof of Theorem 1 is
in Appendix D.
Remark 1: The initial point is required to satisfy <wo,w*i & √γ 110g& and ∣∣wok < 3. The first
condition can be achieved by generating w0 from a Gaussian distribution or uniformly sampling
from a sphere with high probability (see e.g. Chapter 2 of (Blum et al., 2018)). The second condition
can then be satisfied by scaling the size appropriately. We note that the condition that the norm of
the momentum is bounded is also assumed in (Wang et al., 2020).
Remark 2: Our theorem indicates that in the early stage of the optimization process, the momentum
parameter β can be as large as 1, which is also verified in the experiment (Figure 1). However, to
guarantee convergence after the iterate is in the neighborhood of a global optimal solution, the
parameter β must satisfy β < 1 (Polyak, 1964; Lessard et al., 2016).
Remark 3: The number V of the local linear convergence rate due to Heavy Ball momentum actually
depends on the smoothness constant L and strongly convexity constant μ in the neighborhood of a
global solution, as well as the step size η and the momentum parameter β (see e.g. Section 3 of
(Saunders, 2018) or the original paper (Polyak, 1964)). By setting the step size η = 4/(√L + √μ)2
6
Under review as a conference paper at ICLR 2021
and the momentum parameter β = max{∣1 一 √ηL∣, |1 一 √ημ∣}2, V will depend on the squared
root of the condition number √κ := /L∕μ instead of K := L∕μ, which means that an optimal local
convergence rate is achieved (e.g. (Bubeck, 2014)). In general, up to a certain threshold, a larger
value of β leads to a faster rate than that of standard gradient descent.
4 Cubic-Regularized problem
4.1	Notations
We begin by introducing the notations used in this section. For the symmetric but possibly indefinite
matrix A, we denote its eigenvalue in the increasing order, where any λ(i) might be negative. We de-
note the eigen-decomposition ofA as A := Pid=1 λ(i) (A)vivi>, where each vi ∈ Rd is orthonormal.
We also denote Y := -λ(1)(A), and γ+ = max{γ, 0}, and ∣∣Ak2 := max{∣λ(1)(A)∣, ∣λ(d)(A)∣}.
For any vector w ∈ Rd, we denote w(i) as the projection on the eigenvector of A, w(i) = hw, vii. De-
note w* as a global minimizer of the cubic-regularized problem (2) and denote A* := A + ρ∣w^∣Id.
Previous works of Nesterov & Polyak (2006); Carmon & Duchi (2019) show that the minimizer w*
has a characterization; it satisfies ρ∣w*∣ ≥ Y and Vf (w*) = A*w* + b = 0. Furthermore, the
minimizer w* is unique if ρ∣w* ∣ > γ . In this paper, we assume that the problem has a unique
minimizer so that ρ∣w* ∣ > Y. The gradient of the cubic-regularized problem (2) is
Vf(W) = Aw + b + PkWkW = A*(w 一 w*) — ρ(∣w*∣ — ∣w∣)w.	(12)
By applying the Heavy Ball algorithm (Algorithm 1) to the cubic-regularized problem (2), we see
that it generates the iterates via
Wt+1 = Wt — ηVf (wt) + β(Wt 一 Wt-1) = (Id — ηA 一 ρη∣Wt∣Id)wt 一 ηb + β(wt 一 wt-i).
(13)
4.2	Entering a benign region faster
For the cubic-regularized problem, we define a different notion of a benign region from that for the
phase retrieval. The benign region here is smooth, contains the unique global optimal point W*,
and satisfies a notion of one-point strong convexity (Kleinberg et al., 2018; Li & Yuan, 2017; Safran
et al., 2020),
W ∈ Rd :〈w — w*, Vf(W) ≥ HkW — W* ∣2, where £ > 0.	(14)
We note that the standard strong convexity used in the definition of a benign region for phase retrieval
could imply the one-point strong convexity here, but not vice versa (Hinder et al., 2020).
Previous work of Carmon & Duchi (2019) shows that if the norm of the iterate is sufficiently large,
i.e. ρkWk ≥ Y 一 δ for any sufficiently small δ > 0, the iterate is in the benign region that contains
the global minimizer W* . To see this, by using the gradient expression of Vf (W), we have that
〈w — W*, Vf (w)i =(w — W*)>(A* + 22(kwk — ∣∣w*k)Id)(w — W*)
+ 2(kw*k -kwk)2(kwk + kw*k).
(15)
The first term on the r.h.s. of the equality becomes nonnegative if the matrix A* + ρ (∣∣w∣∣ — ∣∣w*k)Id
becomes PSD. Since A* (—Y + ρkw* k)Id, it means that if ρkwk ≥ Y — ρkw* k — Y , the matrix
becomes PSD (note that by the characterization, ρkw* k — Y > 0). Furthermore, if the size of iterate
ρkwk satisfies ρkwk > Y — ρkw* k — Y , the matrix becomes positive definite and consequently,
We have that A* + 22 (k w k - k w* k) Id * IOId for a number £ > 0. Therefore, (15) becomes
〈w — w*, Vf (w)i ≥ £kw — w*k2,	£ > 0.	(16)
Therefore, the benign region of the cubic regularized problem can be characterized as
B := {w ∈ Rd : PkWk > γ — (ρ∣∣w*k — γ)}.	(17)
What we are going to show is that HB with a larger value of the momentum parameter β enters the
benign region IB faster. We have the following theorem, which shows that the size of the iterate
Pkwt k will grow very fast to exceed any level below Y. Furthermore, the larger the momentum
parameter β, the faster the growth, which shows the advantage of Heavy Ball over vanilla gradient
descent.
7
Under review as a conference paper at ICLR 2021
W(I)
ηb(I)
Theorem 2. Fix any number δ that satisfies δ > 0. Define Tδ := min{t : ρkwt+1 k ≥ γ - δ}.
Suppose that the initialization satisfies w0(1)b(1) ≤ 0. Set the momentum parameter β ∈ [0, 1]. If the
step size η satisfies η ≤ 口4口2+ρ∣∣w*k, then Heavy Ball (Algorithm 1&2) takes at most
Tδ ≤ ----------2----------log (1 + γ+(1+ β∕(1 + 泡))
ηδ(i+ β∕(i + ηδ))	4p|b(1)|
number of iterations required to enter the benign region B.
Note that the case of β = 0 in Theorem 2 reduces to the result of Carmon & Duchi (2019)which
analyzes vanilla gradient descent. The lemma implies that the higher the momentum parameter β,
the faster that the iterate enters the benign region for which a linear convergence is possible; see also
Figure 2 for the empirical results. Specifically, β reduces the number of iterations Tδ by a factor of
(1 + β∕(1 + ηδ)) (ignoring the β in the log factor as its effect is small), which also implies that for a
smaller step size η, the acceleration effect due to the use of momentum is more evident. The factor
can be approximately 1 + β for a small η. Lastly, the condition w0(1)b(1) ≤ 0 in Theorem 2 can be
satisfied by wo = w-ι = -r奇 for any r > 0.
Proof. (sketch; detailed proof is available in Appendix E) The theorem holds trivially when γ ≤
0, so let us assume γ > 0. Recall the notation that w(1) represents the projection of w on the
eigenvector v1 of the least eigenvalue λ(1)(A), i.e. w(1) = hw, v1i. From the update, we have that
+ 1+ 3-(Wfi).	(18)
-ηb(1)
Denote at := -Wb(I) and in the detailed proof We will show that at ≥ 0 for all t ≤ Tδ. We can
rewrite (18) as at+1 = (1 + ηγ - ρηkwt k)at + 1 + β(at - at-1 )≥ (1 + ηδ)at + 1 + β(at - at-1 ),
where the inequality is due to that ρkwtk ≤ γ - δ for t ≤ Tδ. So we can now see that the dynamics
is essentially in the form of (6) except that there is an additional 1 on the r.h.s of the inequality.
Therefore, we can invoke Lemma 1 to show that the higher the momentum, the faster the iterate
enters the benign region. In Appendix E, we consider the presence of 1 on the r.h.s and obtain a
tighter bound than what Lemma 1 can provide.	□
We have shown that the simple dynamic results in entering a benign region that is one-point strongly
convex to w* faster. However, different from the case of phase retrieval, we are not aware of any
prior results of showing that the iterate generated by Heavy Ball keeps staying in a region that
has the property (16) once the iterate is in the region. Carmon & Duchi (2019) show that for the
cubic regularized problem, the iterate generated by vanilla gradient descent stays in the region under
certain conditions, which leads to a linear convergence rate after it enters the benign region. Showing
that the property holds for HB is not in the scope of this paper, but we empirically observe that Heavy
Ball stays in the region. Subfigure (a) on Figure 2 shows that the norm kwt k is monotone increasing
for a wide range of β, which means that the iterate stays in the benign region according to (17).
Assuming that the iterate stays in the region, in Appendix F, we show a locally linear convergence
of HB for which up to a certain threshold of β, the larger the β the better the convergence rate.
5 Discussion and conclusion
Let us conclude by a discussion about the applicability of the simple dynamics to other non-convex
optimization problems. Let A be a positive semi-definite matrix. Consider applying HB to top
eigenvector computations, i.e. solving minw∈Rd[∣wk≤ι -2w>Aw, which is a non-convex optimiza-
tion problem as it is about maximizing a convex function. The update of HB for this objective is
wt+1 = (Id + ηA)wt + β(wt - wt-1 ). By projecting the iterate wt+1 on an eigenvector ui of the
matrix A, we have that
hwt+1,uii = (1 + ηλi )hwt,uii + β(hwt, uii - hwt-1,uii).	(19)
We see that this is in the form of the simple dynamics in Lemma 1 again. So one might be able to
show that the larger the momentum parameter β, the faster the top eigenvector computation. This
8
Under review as a conference paper at ICLR 2021

- - -
-1-2-3
Ooo
-二IM 一 ∖ Iiuou ①ωJ①七
(a) norm ∣∣wt ∣∣ vs. iteration	(b) f (Wt) - f (w*) vs. iteration
Figure 2: Solving (2) with different values of momentum parameter β . The empirical result shows the clear
advantage of Heavy Ball momentum. Subfigure (a) shows that larger momentum parameter β results in a faster
growth rate of ∣∣wt ∣∣, which confirms Lemma 2 and shows that it enters the benign region IB faster with larger β.
Note that here we have that kw* k = 1. It suggests that the norm is non-decreasing during the execution of the
algorithm for a wide range ofβ except very large β. For β = 0.9, the norm starts decreasing only after it arises
above ∣w* ∣. Subfigure (b) show that higher β also accelerates the linear convergence. Now let us switch to
describe the setup of the experiment. We first set step size η = 0.01, dimension d = 4, ρ= ∣w*∣ = ∣A∣2 = 1,
γ = 0.2 and gap = 5 × 10-3. Then we set A = diag([-γ; -γ + gap; a33; a44]), where the entries a33 and a44
are sampled uniformly random in [—γ + gap; ∣A∣2 ]. We draw W = (A + ρ∣w* ∣∣Id )-ξ θ, where θ 〜N (0; Id)
and log2 ξ is uniform on [—1,1]. We set w* = k∣wW*∣∣k W and b = -(A + ρ∣w* ∣∣Id)w*. The procedure makes
W* the global minimizer of problem instance (A, b, ρ). Patterns shown on this figure exhibit for other random
problem instances as well.
connection might be used to show that the dynamics of HB momentum implicitly helps fast saddle
points escape. In Appendix G, we provide further discussion and show some empirical evidence.
We conjecture that if a non-convex optimization problem has an underlying structure like the ones
in this paper, then HB might be able to exploit the structure and hence makes progress faster than
vanilla gradient descent.
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi-
mate local minima faster than gradient descent. STOC, 2017.
Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via first-order oracles. NeurIPS,
2018.
Yu Bai and Jason D. Lee. Beyond linearization: On quadratic and higher-order approximation of
wide neural networks. ICLR, 2020.
Afonso S. Bandeira, Jameson Cahill, Dustin G. Mixon, and Aaron A. Nelson. Saving phase: Injec-
tivity and stability for phase retrieval. Applied and Computational Harmonic Analysis, 2014.
Avrim Blum, John Hopcroft, and Ravindran Kannan. Foundations of data science. Neural compu-
tation, 2018.
Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in
Machine Learning, 2014.
T. Tony Cai, Xiaodong Li, and Zongming Ma. Optimal rates of convergence for noisy sparse phase
retrieval via thresholded wirtinger flow. The Annals of Statistics, 2016.
Bugra Can, Mert GUrbUzbalaban, and Lingjiong Zhu. Accelerated linear convergence of stochastic
momentum methods in wasserstein distances. ICML, 2019.
9
Under review as a conference paper at ICLR 2021
Emmanuel J. Candes and Xiaodong Li. Solving quadratic equations via phaselift when there are
about as many equations as unknowns. Foundations of Computational Mathematics, 2014.
Emmanuel J. Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable
signal recovery from magnitude measurements via convex programming. Communications on
Pure and Applied Mathematics, 2013.
Emmanuel J. Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow:
Theory and algorithms. IEEE Transactions on Information Theory, 2015.
Yair Carmon and John Duchi. Gradient descent finds the cubic-regularized nonconvex newton step.
SIAM Journal on Optimization, 2019.
Yair Carmon, John Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal of Optimization, 2018.
You-Lin Chen and Mladen Kolar. Understanding accelerated stochastic gradient descent via the
growth condition. arXiv:2006.06782, 2020.
Yuxin Chen and Emmanuel J. Candes. Solving random quadratic systems of equations is nearly as
easy as solving linear systems. Communications on Pure and Applied Mathematics, 2017.
Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Gradient descent with random ini-
tialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming,
2018.
Yuejie Chi, Yue M. Lu, and Yuxin Chen. Nonconvex optimization meets low-rank matrix factoriza-
tion: An overview. arXiv:1809.09573, 2019.
Hadi Daneshmand, Jonas Kohler, Aurelien Lucchi, and Thomas Hofmann. Escaping saddles with
stochastic gradients. ICML, 2018.
Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase
retrieval. IMA Journal on Numerical Analysis, 2018.
Jelena Diakonikolas and Michael I. Jordan. Generalized momentum-based methods: A hamiltonian
perspective. arXiv:1906.00436, 2019.
Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas Poczos, and Aarti Singh. Gradient
descent can take exponential time to escape saddle points. NIPS, 2017.
John Duchi and Feng Ruan. Solving (most) ofa set of quadratic equalities: Composite optimization
for robust phase retrieval. Information and Inference, 2018.
Cong Fang, Zhouchen Lin, and Tong Zhang. Sharp analysis for nonconvex sgd escaping from saddle
points. COLT, 2019.
Albert Fannjiang and Thomas Strohmer. The numerics of phase retrieval. Acta Numerica, 2020.
Nicolas Flammarion and Francis Bach. From averaging to acceleration, there is only a step-size.
COLT, 2015.
Sebastien Gadat, Fabien Panloup, and Sofiane Saadane. Stochastic heavy ball. arXiv:1609.04228,
2016.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points — online stochastic
gradient for tensor decomposition. COLT, 2015.
Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A
unified geometric analysis. ICML, 2017.
Rong Ge, Tengyu Ma, and Jason D. Lee. Learning one-hidden-layer neural networks with landscape
design. ICLR, 2019.
10
Under review as a conference paper at ICLR 2021
Euhanna Ghadimi, Hamid Reza Feyzmahdavian, and Mikael Johansson. Global convergence of the
heavy-ball method for convex optimization. ECC, 2015.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. NeurIPS, 2019.
Igor Gitman, Hunter Lang, Pengchuan Zhang, and Lin Xiao. Understanding the role of momentum
in stochastic gradient methods. NeurIPS, 2019.
Gabriel Goh. Why momentum really works. Distill, 2017.
Gene H. Golub and Charles F. Van Loan. Matrix computations. Johns Hopkins University Press,
1996.
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. NIPS, 2017.
Paul Hand, Oscar Leong, and Vladislav Voroninsk. Phase retrieval under a generative prior.
NeurIPS, 2018.
Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing star-convex
functions and beyond. COLT, 2020.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. NIPS, 2017.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, and Michael I. Jordan. How to escape
saddle points efficiently. ICML, 2017.
Chi Jin, Praneeth Netrapalli, and Michael I. Jordan. Accelerated gradient descent escapes saddle
points faster than gradient descent. COLT, 2018.
Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M. Kakade, and Michael I. Jordan. Stochastic gradient
descent escapes saddle points efficiently. arXiv:1902.04811, 2019.
Rahul Kidambi, Praneeth Netrapalli, Prateek Jain, and Sham M. Kakade. On the insufficiency of
existing momentum schemes for stochastic optimization. ICLR, 2018.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local
minima? ICML, 2018.
Walid Krichene, Kenneth F. Caluyay, and Abhishek Halder. Global convergence of second-order
dynamics in two-layer neural networks. arXiv:2006.07867, 2020.
Jason D. Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael I. Jordan, and Ben-
jamin Recht. First-order methods almost always avoid strict saddle-points. Mathematical Pro-
gramming, Series B, 2019.
Laurent Lessard, Benjamin Recht, and Andrew Packard. Analysis and design of optimization algo-
rithms via integral quadratic constraints. SIAM Journal on Optimization, 2016.
Kfir Y. Levy. The power of normalization: Faster evasion of saddle points. arXiv:1611.04831, 2016.
Huan Li and Zhouchen Lin. Provable accelerated gradient method for nonconvex low rank opti-
mization. Machine Learning, 2020.
Yuanxin Li, Cong Ma, Yuxin Chen, and Yuejie Chi. Nonconvex matrix factorization from rank-one
measurements. AISTATS, 2019.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
NeurIPS, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. COLT, 2018.
11
Under review as a conference paper at ICLR 2021
Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. arXiv:2007.07989, 2020.
Nicolas Loizou and Peter Richtarik. Momentum and stochastic momentum for stochastic gradient,
newton, proximal point and subspace descent methods. arXiv:1712.09677, 2017.
Nicolas Loizou and Peter Richtarik. Accelerated gossip via stochastic heavy ball method. Allerton,
2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statis-
tical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and
blind deconvolution. Foundations of Computational Mathematics, 2017.
Junjie Ma, Ji Xu, and Arian Maleki. Optimization-based amp for phase retrieval: The impact of
initialization and l2-regularization. IEEE Transactions on Information Theory, 2018.
Chris J. Maddison, Daniel Paulin, Yee Whye Teh, Brendan O’Donoghue, and Arnaud Doucet.
Hamiltonian descent methods. arXiv:1809.05042, 2018.
Stefano Sarao Mannellia, Giulio Birolib, Chiara Cammarotac, Florent Krzakalab, Pierfrancesco Ur-
bania, and Lenka Zdeborova. Complex dynamics in simple neural networks: Understanding
gradient flow in phase retrieval. arXiv:2006.06997, 2020.
Yurii Nesterov and B.T. Polyak. Cubic regularization of newton method and its global performance.
Math. Program., Ser A 108, 177-205, 2006.
Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimiza-
tion. NIPS, 2013.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 1994.
B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 1964.
Qing Qu, Yuqian Zhang, Yonina C. Eldar, and John Wright. Convolutional phase retrieval via
gradient descent. NIPS, 2017.
Sashank Reddi, Manzil Zaheer, Suvrit Sra, Barnabas Poczos, Francis Bach, Ruslan Salakhutdinov,
and Alex Smola. A generic approach for escaping saddle points. AISTATS, 2018.
Itay Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the
optimization landscape of shallow relu neural networks. arXiv:2006.01005, 2020.
Michael Saunders. Notes on first-order methods for minimizing smooth functions. Lecture note,
2018.
Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum.
ICML, 2020.
Othmane Sebbouh, Robert M. Gower, and Aaron Defazio. On the convergence of the stochastic
heavy ball method. arXiv:2006.07867, 2020.
Yoav Shechtman, Yonina C. Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei Miao, and
Mordechai Segev. Phase retrieval with application to optical imaging: a contemporary overview.
IEEE signal processing magazine, 2015.
Mahdi Soltanolkotabi. Algorithms and theory for clustering and nonconvex quadratic programming.
Stanford University Ph. D. Dissertation, 2014.
Matthew Staib, Sashank J. Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra. Escaping saddle
points with adaptive gradient methods. ICML, 2019.
12
Under review as a conference paper at ICLR 2021
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere.
arXiv:1504.06785, 2015.
Ju Sun, Qing Qu, and John Wright. A geometrical analysis of phase retrieval. International Sympo-
sium on Information Theory, 2016.
Tao Sun, Penghang Yin, Dongsheng Li, Chun Huang, Lei Guan, and Hao Jiang. Non-ergodic
convergence analysis of heavy-ball algorithms. AAAI, 2019.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. ICML, 2013.
Yan Shuo Tan and Roman Vershynin. Online stochastic gradient descent with arbitrary initialization
solves non-smooth, non-convex phase retrieval. arXiv:1910.12837, 2019.
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank
solutions of linear matrix equations via procrustes flow. ICML, 2016.
Gang Wang, Georgios B. Giannakis, and Yonina C. Eldar. Solving systems of random quadratic
equations via truncated amplitude flow. IEEE Transactions on Information Theory, 2017a.
Gang Wang, Georgios B. Giannakis, Yousef Saad, and Yonina C. Eldar. Solving most systems of
random quadratic equations. NIPS, 2017b.
Jun-Kun Wang, Chi-Heng Lin, and Jacob Abernethy. Escaping saddle points faster with stochastic
momentum. ICLR, 2020.
Chris D. White, Sujay Sanghavi, and Rachel Ward. The local convexity of solving systems of
quadratic equations. Results in Mathematics, 2016.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, , and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning. NIPS, 2017.
Huaqing Xiong, Yuejie Chi, Bin Hu, and Wei Zhang. Convergence analysis of accelerated first-order
methods for phase retrieval. MTNS, 2018.
Huaqing Xiong, Yuejie Chi, Bin Hu, and Wei Zhang. Analytical convergence regions of accelerated
gradient descent in nonconvex optimization under regularity condition. Automatica, 2020.
Yi Xu, Jing Rong, and Tianbao Yang. First-order stochastic algorithms for escaping from saddle
points in almost linear time. NeurIPS, 2018.
Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. IJCAI, 2018a.
Zhuoran Yang, Lin Yang, Ethan Fang, Tuo Zhao, Zhaoran Wang, and Matey Neykov. Misspecified
nonconvex statitical optimization for sparse phase retrival. Mathematical Programming, 2018b.
Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant
learning rates for double over-parameterization. arXiv:2006.08857, 2020.
Huishuai Zhang, Yuejie Chi, and Yingbin Liang. Provable non-convex phase retrieval with outliers:
Median truncated wirtinger flow. ICML, 2017a.
Huishuai Zhang, Yi Zhou, Yingbin Liang, and Yuejie Chi. A nonconvex approach for phase retrieval:
Reshaped wirtinger flow and incremental algorithms. JMLR, 2017b.
Qinqing Zheng and John Lafferty. A convergent gradient descent algorithm for rank minimization
and semidefinite programming from random linear measurements. NIPS, 2015.
Yi Zhou, Huishuai Zhang, and Yingbin Liang. Geometrical properties and accelerated gradient
solvers of non-convex phase retrieval. IEEE Allerton Conference on Communication, Control,
and Computing, 2016.
13
Under review as a conference paper at ICLR 2021
A Related works
A. 1 Heavy Ball
We first note that Algorithm 1 and Algorithm 2 generate the same sequence of the iterates {wt}
given the same initialization w0 , the same step size η, and the same momentum parameter β.
Lessard et al. (2016) analyze the Heavy Ball algorithm for strongly convex quadratic functions by
using tools from dynamical systems and prove its accelerated linear rate. Ghadimi et al. (2015)
also show an O(1/T ) ergodic convergence rate for general smooth convex problems, while Sun
et al. (2019) show the last iterate convergence on some classes of convex problems. Nevertheless,
the convergence rate of both results are not better than gradient descent. Maddison et al. (2018)
and Diakonikolas & Jordan (2019) study a class of momentum methods which includes Heavy
Ball by a continuous time analysis. Can et al. (2019) prove an accelerated linear convergence to a
stationary distribution for strongly convex quadratic functions under Wasserstein distance. Gitman
et al. (2019) analyze the stationary distribution of the iterate of a class of momentum methods that
includes SGD with momentum for a quadratic function with noise, as well as studying the condition
of its asymptotic convergence. Loizou & Richtarik (2017) show linear convergence results of the
Heavy Ball method for a broad class of least-squares problems. Loizou & RiChtarik (2018) study
solving a average consensus problem by HB, which could be viewed as a strongly convex quadratic
function. Sebbouh et al. (2020) show a convergence result of stochastic HB under a smooth convex
setting and show that it can outperform SGD under the assumption that the data is interpolated.
Chen & Kolar (2020) study stochastic HB under a growth condition. Yang et al. (2018a) show an
O(1/√T) rate of convergence in expected gradient norm for smooth non-convex problems, but the
rate is not better than SGD. Liu et al. (2020) provide an improved analysis of SGD with momentum.
They show that SGD with momentum can converge as fast as SGD for smooth nonconvex settings in
terms of the expected gradient norm. Krichene et al. (2020) show that in the continuous time regime,
i.e. infinitesimal step size is used, stochastic HB converges to a stationary solution asymptotically
for training a one-hidden-layer network with infinite number of neurons, but the result does not
show a clear advantage compared to standard SGD. Lastly, Wang et al. (2020) show that the Heavy
Ball’s momentum can help to escape saddle points faster and find a second order stationary point
faster for smooth non-convex optimization. However, while their work focused on the stochastic
setting, their main result required two assumptions on the statistical properties of the sequence of
observed gradients; it is not clear whether these would hold in general. Specifically, they make
an assumption called APAG (Almost Positively Aligned with Gradient), i.e. Et[hVf (Wt), mt 一
gti] ≥ 一 1 ∣∣Vf (Wt)II2, where Vf (Wt) is the deterministic gradient, gt is the stochastic gradient, and
mt is the stochastic momentum. They also make an assumption called APCG (Almost Positively
Correlated with Gradient), i.e. EtKVf (Wt),MtmR] ≥ -c0ησmaχ(Mt)∣Vf (wt)∣2, where Mt is a
PSD matrix that is related to a local optimization landscape.
We also note that there are negative results regarding Heavy Ball (see e.g. Lessard et al. (2016);
Ghadimi et al. (2015); Kidambi et al. (2018)).
A.2 Matrix sensing
Problem (1) can also be viewed as a special case of matrix factorization or matrix sensing. To see
this, one can rewrite (1) as minu∈Rd×ι * P2ι 加-hAi, UU>)) , where Ai = xixi ∈ Rd×d and
the dot product represents the matrix trace. Li et al. (2018) show that when the matrices {Ai} satisfy
restricted isometry property (RIP) and U ∈ Rd×d , gradient descent can converge to a global solution
with a close-to-zero random initialization. Yet, if the matrix Ai is in the form of a rank-one matrix
product, the matrix might not satisfy RIP and a modification of the algorithm might be required
(Li et al. (2018)). Li et al. (2019), a different group of authors, show that with a carefully-designed
initialization (e.g. spectral initialization), gradient descent will be in a benign region in the beginning
and will converge to a global optimal point. In our work, we do not assume that xixi> satisfies RIP
neither do we assume a carefully-designed initialization like spectral initialization is available. Li &
Lin (2020) show a local convergence to an optimal solution by Nesterov’s momentum for a matrix
factorization problem; the initial point needs to be in the neighborhood of an optimal solution. In
contrast, we study Polyak’s momentum and are able to establish global convergence to an optimal
solution with a simple random initialization. Gunasekar et al. (2017); Gidel et al. (2019); You et al.
14
Under review as a conference paper at ICLR 2021
(2020) study implicit regularization of gradient descent for the matrix sensing/matrix factorization
problem. The directions are different from ours.
B Population gradient
Lemma 2. Assume that kw≠k = L
Eχ~N(0,id) [(x>w)3x — (x>w*)2(x>w)x] =(3]|w『—1)W — 2(w>w)w：
*.
Proof. In the following, denote
Q := E[(x> w)3x]
R := E[(x>w*)2 (x>w)x].
Now define h(w) := E[(x>w)4]. We have that h(w) = 3∣∣w∣∣4 as x>w ~ N(0, kw∣∣2) (i.e. the
fourth order moment). Then,
Vh(w) = 4E[(x>w)3x] = 4Q.
So Q = 4Vh(w) = 3∣∣wk2w.
For the other term, define g(w) := E[(x>w*)2(x>w)2]. Given that
x>w*1 ~ N ([0],[kw*k2 w>w]),	(20)
x> w	0	w*> w	kwk2
we can write
x>w* ~ θιx>w + Θ2Z,
where Z ~ N(0,1), θι ：= k⅛2, and θ2 ：= ∣∣w*k2 — (Www2),. Then,
g(w) = θ12E[(x>w)4] + 2θ1 θ2E[(x>w)3z] + θ22E[z2(x>w)2]
= 3(w*>w)2 + θ22E[z2]E[(x>w)2]
= 2(w*>w)2 + ∣w∣2 ∣w* ∣2 .
So we have that
Vg(w) = 2E[(x>w*)2(x>w)x] = 2R,
which in turn implies that
R = 2 Vg(w) = 2 (4(w>w)w* + 2∣w*∣w) = 2(w>w)w* + ∣∣w*∣2w.
Combining the above results, we have that
VF (W) = Q — R = 3∣w∣2w — ∣w*k2w — 2(w>w)w* = (3∣w∣2 — 1)w — 2(w> w)w*. (21)
□
C S imple lemmas
Lemma 1: For a positive number θ and the momentum parameter β ∈ [0, 1], if a non-negative
sequence {at} satisfies a0 ≥ a-1 > 0 and that for all t ≤ T,
at+1 ≥ (1 + θ)at + β(at — at-1 ),	(22)
then {at} satisfies
at+1 ≥ (1 + (1 + ∣ι+θ )θ) at,	(23)
for every t = 1, . . . , T + 1. Similarly, if a non-positive sequence {at} satisfies a0 ≤ a-1 < 0 and
that for all t ≤ T, at+1 ≤ (1 + θ)at + β(at — at-1 ), then {at} satisfies
at+1 ≤ (1 + (1 + ∣ι+θ )θ) at,	(24)
for every t = 1, . . . , T + 1.
15
Under review as a conference paper at ICLR 2021
Proof. Let Us first prove the first part of the statement. In the following, We denote C :=	. For
the base case t = 1, we have that
a2 ≥ (1 + θ)a1 + β(a1 - a0) ≥ (1 + θ)a1 + βcθa1,	(25)
where the last inequality holds because aι - ao ≥ θcaι ^⇒ aι ≥ y-^ao, as aι ≥ (1 + θ)a0 =
1-1θCao. Now suppose that it holds at iteration t, at+ι ≥ (1 + (1+ βc)θ)at. Consider iteration t +1,
we have that
at+2 ≥ (1 + θ)at+1 + β(at+1 - at) ≥ (1 + θ)at+1 + θcβat+1,	(26)
where the last inequality holds because at+ι - at ≥ θcat+ι ^⇒ at+ι ≥ ι1^ at as at+ι ≥
(1 + (1+ βc)θ)at ≥ ι-1θcat, given the assumption at t and that C := ι+^.
The second part of the statement can be proved similarly.	□
Lemma 3. For a positive number θ < 1 and the momentum parameter β ∈ [0, 1] that satisfy
(1 + 1-θ)θ < 1, ifa non-negative sequence {bt} satisfies bo ≤ b-ι and thatfor all t ≤ T,
bt+1 ≤(1-θ)bt+β(bt-bt-1),	(27)
then {bt} satisfies
bt+1 ≤ (1 - (1 +11-θ )θ) bt,	(28)
for every t = 1, . . . , T + 1. Similarly, ifa non-positive sequence {bt} satisfies bo ≥ b-1 and that
for all t ≤ T, bt+1 ≥ (1 - θ)bt + β(bt - bt-1), then {bt} satisfies
bt+1 ≥ (1 - (1 +11-θ )θ) bt,	(29)
for every t = 1, . . . , T + 1.
Proof. Let us first prove the first part of the statement. In the following, we denote C := i-^. For
the base case t = 1, we have that
b2 ≤ (1-θ)b1+β(b1-bo) ≤ (1 - θ)b1 - βCθb1,	(30)
where the last inequality holds because
bi - bo ≤ -θcbι ^⇒ bi ≤ J ..bo,	(31)
as bi ≤ (1 - θ)bo = ι+1θcbo due to that C = i-^. Now suppose that it holds at iteration t.
bt+i ≤ (1 - (1 + βC)θ)bt.
Consider iteration t + 1, we have that
bt+2 ≤ (1 - θ)bt+i + β(bt+i -bt) ≤ (1 - θ)bt+i - θCβbt+i,	(32)
where the last inequality holds because
bt+i - bt ≤ -θcbt+i Q⇒ bt+i ≤ J. °. bt	(33)
as bt+i ≤ (1 - (1 + βc)θ)bt ≤ ɪ+^bt, due to the induction at t and that C
The second part of the statement can be proved similarly.
□
16
Under review as a conference paper at ICLR 2021
D Proof of Theorem 1
Recall the recursive system (11).
wt+ι = (1 + 3η(1 - kwtk2) + ηξt)wk + β(Wk- Wt-I)
w⊥+ι[j] = (1 + η(I- 3kwtk2) + ηρt,j)w⊥[j] + β(w⊥[j] - w⊥-ι[j]),
(34)
where {ξt} and {ρt,j } are the perturbation terms. In the analysis, we will show that the sign of Wtk
never changes during the execution of the algorithm. Our analysis divides the iterations of the first
stage to several sub-stages. We assume that the size of the initial point W0 is small so that it begins
in Stage 1.1.
•	(Stage 1.1) considers the duration when ∣∣wtk ≤ y 4 + Cn, which lasts for at most T0
iterations, where T0 is defined in Lemma 4. A by-product of our analysis shows that
|WTk0+1
9 + cn.
•	(Stage 1.2) considers the duration when the perpendicular component ∣Wt⊥ ∣ is decreasing
and eventually falls below Z/2, which consists of all iterations To ≤ t ≤ To + Tb, where
Tb is defined in Lemma 5.
•	(Stage 1.3) considers the duration when |wk | is converging to the interval [1 一 2, 1 + 2], if
it was outside the interval, which consists of all iterations To + Tb ≤ t ≤ To + Tb + Ta ,
where Ta is defined in Lemma 6.
In stage 1.1, both the signal component |Wtk | and ∣Wt⊥ ∣ grow in the beginning (see also Figure 1).
The signal component grows exponentially and consequently it only takes a logarithm number of
iterations To + 1 to reach |WTk +1 | >
y 4 + cn which also means that ∣∣WT0+1∣∣ >
Moreover, the larger the momentum parameter β, the smaller the number ofTo. After ∣Wt∣ passing
the threshold, the iterate enters Stage 1.2.
Lemma 4. (Stage1.1) Denote Ca := τ⅛. There will be at most To :
1+ 2
log(J 4 + cn^WOI)
log (1+(I + Cae)η3 )
iterations
such that Wtk ≤
. Furthermore, for all t in this stage, ∣Wt ∣ ≤
Cn
-3^ .
5
6
InStage1.2, WehavethatkWtk2 ≥ |wk|2 ≥ 4 + Cn > 3 so that the perpendicular component ∣w⊥ ∣
is decaying while the signal component |Wtk | keeps growing before reaching 1 (see also Figure 1). In
particular, the perpendicular component decays exponentially so that at most additional Tb iterations
is needed to fall below 2 (i.e. ∣ w⊥ k ≤ 2), With larger β leading to a smaller number of Tb Notice
that if wk satisfies ||wk|-11 ≤ 22 when ∣∣ w⊥ ∣∣ falls below 2, we immediately have that TZ ≤ To+Tb;
otherwise, the iterate enters the next stage, Stage 1.3.
Lemma 5. (Stage 1.2) Denote Cb :
i⅛ andω := maxt ∣∣w⊥k ≤
1- 3
. There will be at most
2
2.
Tb ≤
log(另)
Iog(I-3(I+Cbe))
iterations such that
≤ |Wtk | and kWt⊥ k ≥
In stage 1.3, we show that |wk | converges towards 1 linearly, given that kw⊥k ≤ 2. Specifically,
after at most additional Ta iterations, we have that ||wk | - 1| ≤ 2 and that larger β reduces the
number of iterations Ta .
Lemma 6. (Stage 1.3) Denote c2,g := 1+； Z, c2,d := ɪ-^, and ω := maxt |wk | ≤ J 10 + 3Cn.
There will be at most Ta := max (曹(1+2⅞+C⅞2, log(i-g⅛+⅛dβπ) iterations such that
||wk| - 1| ≥ 2 and ∣∣w⊥k ≤ 2.
17
Under review as a conference paper at ICLR 2021
By combining the result of Lemma 4, Lemma 5 and Lemma 6, we have that
T ≤ T0 + Tb + Ta =
log( "n)
log (1 + η5 (1 + Caβ))
+	log(2⅛)
+ log(1- η(1 + Cbβ))
+ max
log(一)
V 9 +cn
log(修)
iog(i + η2(1 + cζ,gβ))，log(1 - ηζ(I + cζ,dβ))
<6log(nW+cn)	6log(等)	(2iog(√r=)	2log(ι+ω72))
.+	+ ^∩ax
〜5η(1 + caβ)	η(1 + Cbβ)	'ηZ(1 + 5β)) ηZ(1 + CEe))J
. log d
η η(I + Cne),
(35)
where the last inequality uses that |w0| & √d ；g & due to the random isotropic initialization. The
detailed proof of Lemma 4-6 is available in the following subsections.
After t ≥ Tζ , the iterate enters a benign region that is locally strong convex, smooth, twice differ-
entiable, and contains w* (or -w*) (Ma et al., 2017; White et al., 2016), which allows us to use
the existing result of gradient descent with Heavy Ball momentum for showing its linear conver-
gence. In particular, the result of local landscape (e.g. Ma et al. (2017); White et al. (2016)) and the
known convergence result of gradient descent with Heavy Ball momentum (e.g. Saunders (2018);
Polyak (1964); Lessard et al. (2016); Xiong et al. (2020)) can be used to show that for all t > Tζ ,
dist(wt, w*) ≤ (1 - ν)t-Tζ dist(wTζ, w*) ≤ (1 - ν)t-Tζζ, for some number 1 > ν > 0.
D.1 S TAGE 1.1
Lemma: 4 (Stage 1.1) Denote Ca := τ⅛. There will be at most To :
1+ 2
d log(d4 +cn/|w0 I) e
log (1 +(I + Cae)n3 )
iterations such that ∣∣wtk ≤ ʌ/4 + Cn. Furthermore, we have that |wTo+1∣ >
Proof. Let us first assume that Wtk > 0 and denote at := Wtk . By using that ∣Wt∣ ≤
this stage, we can lower-bound the growth rate of at as
at+ι ≥ (1 + 3η(1 -IlWtl|2) - ηlξt∣)at+ e(at - at-ι)
in
(1 + 3η(1 - 9 + Cn) - Cn) at + β(at - at-1)
5
1 + η3)at+ e(at - at-ι)
5
(1 + (I + Cae)n 3 )at,
(36)
where in the last inequality we use Lemma 1 and that
1
ι+η5.
(37)
Observe that (1 + (1 + Caβ)η3) > 1. Consequently, the sign of wk never change in this stage. So
for Wk ≥ ʌ 4 9 + Cn it takes number of iterations at most
T0 := d
log( 丁)
log (1 + (1 + Caβ)η5)
e≤
2log( 丁)
(1 + Cae )η 3
(38)
≥
≥
≥
Similar, when Wk < 0, We can show that after at most To iterations, Wk falls below - y 99 + Cn
Since ∣∣wt∣ > |wk |, it means that there will be at most To iterations such that ∣wt∣ ≤ 4 4 + Cn □
18
Under review as a conference paper at ICLR 2021
D.2 S TAGE 1.2
Lemma 5: (Stage 1.2) Denote Cb := ι1η and ω := maxt ∣∣w⊥k ≤ y 9 + Cn. There will be at
most Tb := diog(i-g((2ω)cbβ)) e iterations such that J9 + Cn ≤ |wk | and ∣∣w⊥ ∣∣ ≥ 2.
Proof. Let t0 be the last iteration of the previous stage. We have that ∣wt0 ∣ ≥ kwt0 k ≥ 9+ + cn.
Denote at := |wtk |. In this stage, we have that at keeps increasing until ∣wt ∣2 & 1. Moreover,
wtk remains the same sign as the previous stage. Now fix an element j 6= 1 and denote bt :=
∣w⊥ [j] |. From Lemma 7, We know that the magnitude bt := ∣w⊥ [j] | is non-increasing in this stage.
Furthermore, we can show the decay of bt as follows. If wt[j], wt-1[j] > 0,
wt+ι[j] ≤ (1 + η(I- 3kwtk2) + n|Pt,j|)wt[j] + β(Wtj] — wt-ι[j])
4
≤ (1+ η (1 — 3(八 + Cn)) + 3n) Wt[j] + β(wt[j] — Wt-1 [j ])
9
≤ (1 — 3)wt[j] + β(wt[j] — Wt-IjD
≤ (1 — 3(1 + Cbe))Wt[j],
(39)
where in the last inequality we used Lemma 3, as the condition (1 + ɪ-ɪ) 3 < 1 is satisfied, and we
denote that
1
Cb = T-淳.
On the other hand, if Wt[j], Wt-1[j] < 0,
wt+ιj] ≥ (1 + η(1 — 3Iwtk2) + η∣ρt,j∖)wt[j] + β(wt[j] — Wt-IjD
≥ (1 + η(1 ― 3(4 + Cn)) + nCn)Wt[j] + e(Wt j] ― Wt-1 j])
9
≥ (1 — 3)Wtj] + e(Wtj] — Wt-1 j])
≥ (1 — 3(1 + Cbe ))Wt j],
(40)
(41)
where in the last inequality we used Lemma 3, as the condition (1 + ɪ-ɪ) 3 < 1 is satisfied, and we
denote that
Cb :
1
1 — η∕3.
(42)
The inequalities of (39) and (41) allow us to write ∣Wt+ι[j]∣ ≤ (1 — 3(1 + Cbe))∣Wt[j]|. Taking the
square of both sides and summing all dimension j 6= 1, we have that
kW3k2 ≤(1 - 3(1 + Cbe))21尾『.	(43)
Consequently, for ∣Wt⊥ ∣ to fall below ζ∕2, it takes at most
log( E) e ≤ d	log(2⅛)
Iog(I- 3 (1 + Cbe))	Iog(I- 3 (1 + Cbe))
(44)
iterations.
Lastly, Lemma 7 implies that at the time that the magnitude of Wt⊥ [j] starts decreasing, ∣Wt ∣2 ≤
4 + Cn, which in turn implies that ω := maxt ∣∣w⊥ k ≤ J 9 + Cn
□
19
Under review as a conference paper at ICLR 2021
D.3 S TAGE 1.3
Lemma 6: (Stage 1.3) Denote c4,g := ɪ-ɪɪ, c4,d := ι-1ηζ, and ω := maxt |wk | ≤ J晋 + 1 Cn.
There will be at most Ta := max (：：g(；+Z/I/+C9 +cn), iog(il-g(ζ(i+cLβ))) iterations Such that
||wk| - 1| ≥ 2 and kw⊥k ≤ 2.
Proof. Denote t0 the last iteration of the previous stage. We have that t0 ≤ T0 + Tb. Since wtk does
not change the sign in stage 1.1 and 1.2, w.l.o.g, we assume that wtk > 0. Denote at := wtk . We
consider at，in two cases: at，≤ 1 - 2 and at，≥ 1 + 2. If at，≤ 1 - 2, then We have that for all t
in this Stage, kwtk2 ≤ a； + ∣∣w⊥∣∣2 ≤ (1 - 2)2 + (2)2 ≤ 1 - 2,forany sufficiently small Z. So at
grows as follows.
at+ι ≥ (1 + 3η(I - kwtk2) - M&Dat + β(at - at-1)
ζ
≥ (1 + 3η2 - ηcn)at + β(at- at-1)
(a)	ζ
≥ (1 + η2)at + β(at - at-ι)
(b)	ζ
≥ (I + η2(I + cζ,gβ))at,
where (a) is by ζ ≥ cn and (b) is due to that Lemma 1 and that
1
cζ,g = E2.
(45)
(46)
Consequently, it takes at most
log √-ζ/2
d---------Z^+cn—e	(47)
log(1 + η 2(1 + cζ,g β))
number of iterations in this stage for at to rise above 1 - 2 .Onthe other hand, if at，≥ 1 + 2, then
we can lower bound ∣∣wtk2 in this stage as ∣∣wt∣∣2 ≥ a2 ≥ (1 + 2)2. We have that
at+ι ≤ (1 + 3η(1 - kwtk2) + ηlξt∣)at + β(at - at-ι)
≤ (1 - 3ηζ + cn)at + β(at - at-1)
(a)	(48)
≤ 1 - ηζ at + β(at - at-1)
(b)
≤ (1 - ηζ(1 + cZ,dβ))at,
where (a) uses ζ ≥ cn and (b) uses Lemma 3
.	_	1
cζ,d := 1 - ηZ
(49)
That is, at is decreasing towards 1 + Z/2. Denote ω := maxt |wk |. we see that it takes at most
d -lo≡J≤2—e
log(1 - ηZ (1 + cζ,dβ))
number of iterations in this stage for at to fall below 1 + Z/2. Lastly, Lemma 8 implies that at the
time that the magnitude of wk starts decreasing, ∣∣wt∣∣2 ≤ 甘 + 1 Cn, which in turn implies that
ω := maxt |wk | ≤ J晋 + 1 cn
On the other hand, by Lemma 10, the magnitude of the perpendicular component is non-increasing
in this stage, and hence kwt⊥ k keeps staying below Z/2.
Similar analysis holds for wtk < 0, hence we omitted the details.
□
20
Under review as a conference paper at ICLR 2021
D.4 Some supporting lemmas
Lemma 7. Suppose that η satisfies η ≤ -,-----------ʌ1---------. Let to be the first time such that
36(3 +cn) max{cm,1}
I∣wtok2 ≥ 3 + Cn. Then, there exists a time T ≤ to such that w⊥+1[j] ≤ w⊥[j], if w⊥[j] ≥ 0;
Similarly, w⊥+1[j] ≥ w⊥[j], if w⊥ [j ] ≤ 0. Furthermore, we have that ∣∣wto∣∣2 ≤ 4 + Cn.
t⊥-0[1j[]j]
(51)
Proof Recall that wt+ι[j] = (1 + η(I - 3kwtk2) + ηρt,j)w⊥[j] + β(w⊥[j] - wt-ι[j]). W.l.o.g,
let us consider wo⊥ [j] > 0. Assume that wt⊥ [j] ≥ wt⊥-1 [j] for all t ≤ to ; otherwise, there exists a
time τ < to such that wτ⊥ [j] ≤ wτ⊥-1 [j].
Denote λto,j := η(1 - 3kwtok2) + ηρt00,j. Since ∣∣wto∣∣2 ≥ 11 + Cn, Wehavethat λt0,j ≤ 0. Wecan
rewrite the dynamics as
wt⊥0+1 [j] = 1 + λt0 ,j + β -β
wt⊥0 [j]	=	1	0
We have that
k	[w⊥⊥+j[j]] k	≤ k [1 + λt0,j + β -β]	k2	∙k [ww⊥[M k.	(52)
We Will shoW that	wt⊥ +1 [j] ≤	wt⊥-1[j] if wt⊥-1[j] >	0,	Which means that	the magnitude
of w⊥ [j] has stopped increasing. It suffices to shoW that the spectral norm of the matrix
k 1 + λt10,j + β -0β k2 is not greater than 1.
Note that the roots of the characteristic equation of the matrix, z2 - (1 + λt0,j + β)z + β are
(l+λt0 ,j +β)±√ (1+λt0,j +β)2-4β
2	.
If the roots are complex conjugate, then the magnitude of the roots is at most √; consequently the
spectral norm is at most √ ≤ 1. On the other hand, if the roots are real, to show that the larger
root is not larger than 1, it suffices to shoW that VZ(I + λt0,j + β产一4β ≤ 1 - λt0,j - β, which is
guaranteed if λt0,j ≤ 0. To show that the smaller root is not greater than -1, we need to show that
√(1 + λto,j + β)2 - 4β ≤ 3 + λto,j + β, which is guaranteed if λt0,j ≥-1.
By definition, ∣∣wto-ik2 < 1 + Cn. If η ≤ min{ 16kwt；_1g小,ι81m}, then by invoking Lemma 9,
we have that ∣∣wto k2 - ∣∣wto-ιk2 ≤ 9 and consequently, Wehavethatkwt0 k2 ≤ 4 + cn. Thus, by
choosing η satisfies η ≤ 16(1/1+C ” , we have that kwt。∣∣2 ≤ 9 + cn. Using the upper-bound of
kwt0 k2 and the constraint of η, we have that λt0,j ≥ -η( 3 + Cn) ≥ -1.
Similar analysis when wt⊥ [j] is negative; and hence omitted.
□
Lemma 8. Suppose that η satisfies η ≤ —/-------1----------. Let tι be the first time (if exist) such
36(1+ 3Cn) max{cm,1}
that kwtι k2 ≥ 1 + 3Cn. Then, there exists a time T ≤ tι such that wk+ι ≤ wT, if wτk ≥ 0. Similarly,
wT+1 ≥ wT, if wT ≤ 0. Furthermore, we have that kwtι ∣∣2 ≤ 10 + 1 Cn.
Proof. Recall that wt+ι = (1 + 3η(1 - kwt∣2) + ηξt)wk + β(wk - wt-ι). W.l.o.g, let us consider
wok > 0. Assume that wtk ≥ wtk-1 for all t ≤ t1; otherwise, there exists a time T < t1 such that
wTk ≤ wTk-1.
Denote λtι := 3η(1 - ∣∣wtι k2) + ηξt∖. Since kwtι ∣∣2 ≥ 1 + 1 cn We have that λh ≤ 0. We can
rewrite the dynamics as
wtk1+1	[1 + λt1 + β -β]	wtk1
wtk1	1	0	wtk1-1
(53)
21
Under review as a conference paper at ICLR 2021
We have that
k	"wkl+1#	k ≤ k	[1	+ λt1	+ β	-[	k2 ∙k "Wt1	#k.	(54)
wt1	wt1-1
The analysis essentially follows the same lines as Lemma 7. Specifically, to show that wtk +1 ≤
wtk -1, it suffices to ensure that λt1 := 3η(1 - Iwt1 I2) + ηξt1 ∈ [-1, 0]. We have that λt1 ≤ 0 by
the definition of t1. Furthermore, by the definition of t1 - 1, we have that kwtι-112 < 1 + 3cn So
if η ≤ min{36∣∣wt J1kcm, 18cm}, then by invoking Lemma 9, we have that 忖如 ∣2 - IlwtI-1 k2 ≤ 1
and consequently, we have that k wtι k2 ≤ 190 +1 cn. Thus, by choosing η satisfies η ≤
1
36 (1+ 3 Cn) cm
we have that Ilwt1 k2 ≤ 190 + 3cn. Using this upper-bound of kwt1 k2 and the constraint of η, we
have that λ尢 ≥ -η(ɪ + 2cn) ≥ -1. Therefore, We have completed the proof.
□
Lemma 9. Assume that the norm of the momentum is bounded for all t ≤ Tζ, i.e. ImtI ≤ cm, ∀t ≤
TZ. Set the SteP size η satisfies η ≤ min{36口仅「^c , ɪ^^}. Then, we have that
IlwtIl2 -∣Iwt-1∣∣2 ≤ -.
9
Proof. To see this, we will use the alternative presentation Algorithm 2, which shows that wt =
wt-1 - ηmt-1, where the momentum mt-1 stands for the weighted sum of gradients up to (and
including) iteration t - 1, i.e. mt-1
IwtI2 - Iwt-1 I2 as
PS=0 βt-1-sVf (ws). Using the expression, we can expand
IwtI2 - Iwt-1I2 = Iwt-1 - ηmt-1I2 - Iwt-1I2 = -2ηhwt-1,mt-1i + η2Imt-1I2
≤ 2ηkwt-ιkkmt-ιk + η2kmt-ιk2 ≤ 1.	(55)
9
where the last inequality holds ifη ≤ min{ 36∣∣wt-1∣∣cm, ι⅛}∙
Lemma 10. Fix an index j. Set η ≤	∣ ι-1-7——
3	/ - 36(1+1 cn) max{cm,
□
1} and β ≤ 1. Suppose that kwt∣2 ≥ 1 + Cn.
If for a number R > 0, we have that I ww⊥t [j[j]]
I ≤ R, then I wwt⊥+t⊥1[j[j]]
k ≤ R.
Proof. The proof is similar to that of Lemma 7. We have that
I[⅛jj]I≤I]1+λtτ+β -β[I2∙I]⅛⅛	(56)
Denote λt,j := η(1 - 3IwtI2) + ηρt,j. As the proof of Lemma 7, to show that the spectral norm
of the matrix I 1 + λt1,j + β -0β I2 is not greater than one. It suffices to have β ≤ 1 and
that λt,j ∈ [-1,0]. By the assumption, it holds that Ilwtk2 ≥ 3 + cn so We have that λt,j ≤
0. Furthermore, by Lemma 8, if the step size η satisfies η ≤ —彳-1-, We have that
36(1+ 3cn) max{cm,1}
IlwtII2 ≤ 10 + 1 Cn. Therefore, using the upper-bound of the step size and the norm, We can obtain
that λt,j := η(1 - 3IwtI2) + ηρt,j ≥ -1. Hence, we have completed the proof.
□
22
Under review as a conference paper at ICLR 2021
E Proof of Theorem 2
To prove Theorem 2, we will need the following lemma.
Lemma 11. Fix any number δ > 0. Define Tδ := min{t : ρkwt+1 k ≥ γ - δ}. Assume that
η ≤ kAk2+ρ∣w k ∙ Suppose that w01)b(1) ≤ 0. Then, we have that w(1)b⑴ ≤ 0, forall 0 ≤ t ≤ Tδ.
Proof. The lemma holds trivially when γ ≤ 0, so let us assume γ > 0.
Recall the Heavy Ball generates the iterates as
wt(+1)1 = (1 - ηλ(1)(A) - ρηkwtk)wt(1) - ηb(1) + β(wt(1) - wt(-1)1).
We are going to show that for all t,
wt(1)b(1) ≤ 0 and (wt(1) - wt(-1)1)b(1) ≤ -cbw wt(1) b(1),	(57)
for any constant cbw ≥ 0. The initialization guarantees that w0(1)b(1) ≤ 0 and that (w0(1) -
w-(11))b(1) = 0 ≤ -cbww0(1)b(1). Suppose that (57) is true at iteration t. Consider iteration t + 1.
wt(+1)1b(1) = (1 - ηλ(1) (A) - ρηkwtk)wt(1)b(1) - η(b(1))2 + β (wt(1) - wt(-1)1)b(1)
≤ (1 - ηλ(1) (A) - ρηkwt k - β cbw)wt(1) b(1) - η(b(1))2	(58)
≤ 0,
where the first inequality is by induction at iteration t and the second one is true if (1 - ηλ(1)A -
ρη kwt k - βcbw) ≥ 0, which gives a constraints about η,
1 - ηλ(1) (A) - ρηkwtk ≥ cbw.	(59)
Now let us switch to show that (wt(+1)1 - wt(1))b(1) ≤ -cbwwt(+1)1b(1), which is equivalent to showing
that w(+)1b(1) ≤ “I w(1)b⑴.From (58), it suffices to show that
(1- ηλ(1)(A)- ρηkwtk-βcbw)w(I)b(1) -ηW))2 ≤ AW(I)b⑴.(60)
Since wt(1)b(1) ≤ 0, a sufficient condition of the above inequality is
1 - ηλ⑴(A) - Pnkwtk - βcbw - t—1— ≥ 0.
1	+cbw
Now using that ρkwt k ≤ γ - δ and that 1+x ≤ 1 - 2x for x ∈ [0, 1]. It suffices to have that
(61)
1-	nλ(I) (A) - n(γ - δ) TCbW - τ+cbw ≥ 1+nδ - βcbw - 1+2CbW ≥nδ - 2CbW ≥ 0.(62)
By setting CbW = 0, we have that the inequality is satisfied. Substituting CbW = 0 to (59), we have
that
/	1
n ≤ λ⑴(A) + Pkwtk,
Recall that ρkwtk ≤ γ - δ for all t ≤ Tδ . So we have that η ≤----- 1—— . Furthermore,
k Ak2 + (Y-δ ∙ ∙ lγ-δ≥0
by using that ρ∣∣w* k > Y, it suffices to have that n ≤ kAk2+ρkw k . We have completed the proof.
□
Given Lemma 11, we are ready for proving Theorem 2.
Proof. (of Theorem 2) The lemma holds trivially when γ ≤ 0, so let us assume γ > 0. Recall
the notation that w(1) represents the projection of w on the eigenvector v1 of the least eigenvalue
λ(1)(A), i.e. w(1) = hw, v1i. From the update rule, we have that
~w+) = (Id+ nγ - Pnkwtk) -⅞⅛)+ 1+ β(W-η-wtτ)
(63)
23
Under review as a conference paper at ICLR 2021
Denote at := -Wb(I). We can rewrite (63) as
at+1 = (1 + ηγ - ρηkwt k)at + 1 +	β(at	-	at-1)	≥ (1 +	ηδ)at + 1 +	β(at	-	at-1),	(64)
where the inequality is due to that ρkwt k ≤ γ - δ for t ≤ Tδ. Now we are going to show that,
at+ι ≥ (1 + nδ + ι+ηδβ)at + 1. For the above inequality to hold, it suffices to show that at - at-ι ≥
ι++ηδat. That is, at ≥ jδ∕(ι+ηδ) atτ∙ The base case t = 1 holds because aι ≥ (1 + nδ)a0 + 1 ≥
ι-ηδ∕(ι+ηδ)a。，SUPPOSe that at iteration t, we have that at ≥ jδ∕(ι+ηδ)at-ι. Consider t + 1,
we have that at+ι ≥ (1 + nδ)at + 1 + β(at - at-ι) ≥ (1 + nδ)at + 1 ≥ 12/(1+例 at, where
the second to last inequality is because at ≥ 12/(1+桢)at-ι implies at ≥ a—. Therefore, we
have completed the induction. So we have shown that
W(+)1
(1)
=(Id + ηγ - Pnkwtk)-η⅛(iτ + 1 +
β(w(1) -w(1) )	w(1)
———ηb(ij-1 ≥ (1 + nδ + #凝β) -Wb(I) + 1. Recursively expanding the inequality, We have that
(1)	(1)	(1)
-η+17 ≥(1+nδ + ι++ηδβ) -⅛e+1 ≥ (1+nδ+1+‰ β)2 -η-⅛+(1+n+1⅛ e+1
≥ ... ≥ ηδ(i+0)((1 + nδ + l+⅛ β)t - 1).
(65)
Therefore, γ-δ ≥) kwTδk ≥ lw(1)1 (≥) δ+δβb(11+ηδ) ((1+ nδ + 1⅛β/-1) where (a) uses that
for t ≤ Tδ, ρkwtk ≤ γ - δ, and (b) uses (65) and Lemma 11 that wt(1)b(1) ≤ 0. Consequently,
Tδ ≤
log (1+
(Y-δMδ+δβ^1 + ηδD
P∣b⑴ I
iog(i+ηδ+1+⅛β)
(a)	2	lcO (1 + (γ-δ)δ(i+β∕(i+ηδ)))
≤ ηδ(i+β∕(i+ηδ))log11 +	ρ∣b(υ∣	)
≤) ηδ(⅛
log 1 +
y+ (i+β∕(i+ηδ))
4ρ∣b(I)I
(66)
where (a) uses that log(1 + x) ≥ 2 for any X ∈ [0,〜2.51], and (b) uses γδ - δ2 ≤ γ+.
□
F Convergence of HB for the cubic-regularized problem
Theorem 3. Assume that the iterate stays in the benign region that exhibits one-point strong con-
vexity to w*, i.e. B := {w ∈ Rd : Pkwk ≥ Y — δ} for a number δ > 0, once it enters the
benign region. Denote	CConverge	:= 1----------2c0------V,	where	cδ	and	co	are some constants
1-2ηcδ (Pkw* k—γ)
that satisfy 0.5 > cδ > 0 and cc0 > 0. Suppose that there is a number R, such that the size of
the iterate satisfies kwt k ≤ Rfor all t during the execution of the algorithm and that kw* k ≤ R.
Denote L := kAk2 + 2PR. Also, suppose that the momentum parameter β ∈ [0, 0.65] and that
the step size n SaaiSfieS n ≤ min (4(kA* k+26ρR), (50L+2(26)2)(kA* k + PR)(1+k∣A* k + ρR) + 1.3L). See
wo = w-1 = -r kbk for any sufficiently small r > 0. Then, in the benign region it takes at most
t . T := ______________1___________ log( (k Ak2 + 2PR)W )
〜 L ηcδ(ρkw*k-γ)(1+ βcconverge ) g'	2e J
number of iterations to reach an -approximate error, where cc := 4R2 (1 + ηC) with C =
3CO(Pkw*k- Y) + 3(CoCδ(ρkw*k- Y)+ 10max{0, γ}).
Note that the constraint of n ensures that ncδ (Pkw*k - Y) ≤ 1; as a consequence, CConverge ≤
1. Theorem 3 indicates that up to an upper-threshold, a larger value of β reduces the number of
iterations to linearly converge to an -optimal point, and hence leads to a faster convergence.
Let us now make a few remarks. First, we want to emphasize that in the linear convergence rate
regime, a constant factor improvement of the convergence rate (i.e. of T here) means that the slope
of the curve in the log plot of optimization value vs. iteration is steeper. Our experimental result
24
Under review as a conference paper at ICLR 2021
(Figure 2) confirms this. In this figure, we can see that the curve corresponds to a larger momentum
parameter β has a steeper slope than that of the smaller ones. The slope is steeper as β increases,
which justifies the effectiveness of the momentum in the linear convergence regime. Our theoretical
result also indicates that the acceleration due to the use of momentum is more evident for a small
step size η. When η is sufficiently small, the number cconverge are close to 1, which means that the
number of iterations can be reduced approximately by a factor of 1 + β .
Secondly, from the theorem, one will need |b(1) | be non-zero, which can be guaranteed with a high
probability by adding some Gaussian perturbation on b. We refer the readers to Section 4.2 of
Carmon & Duchi (2019) for the technique.
To prove Theorem 3, we will need a series of lemmas which is given in the following subsection.
F.1 Some supporting lemmas
Lemma 12. Denote Sequences dt := P(∣∣w*k — IlWtII)IlWt — w*∣∣2, et := -(Wt — w*)>(A* —
2ηCwA?)(Wt- w*), gt := — ρ(kw*k — kwtk)2(kw*k + ∣∣wtk — 4ηρCwkwtk2), wherecw := 12ββ
and Cw := (1 + Cw) + (1 + Cw)2. Forall t, we have that
t-1
hWt —	W? ,	Wt	— Wt-1i	+	Cw IWt	— Wt-1I2 ≤ η	β t-1-s (ds	+	es	+ gs).
s=1
Proof. We use induction for the proof. The base case t = 0 holds, because W0 = W-1 by ini-
tialization and both sides of the inequality is 0. Let us assume that it holds at iteration t. That is,
hWt —W?, Wt —Wt-1i +CwIWt — Wt-1 I2 ≤ ηPts-=11βt-1-s(ds+es+gs). Consider iteration t + 1.
We want to prove that
t
hWt+1 — W? , Wt+1 — Wti + Cw IWt+1 — Wt I2 ≤ η	βt-s(ds+es+gs).
Denote ∆ :=	Wt+1 —	Wt.	It is equivalent to showing that	h∆,	Wt	+ ∆ —	W? i	+	Cw I∆I2 ≤
ηPts=1βt-s(ds + es +gs), or
hs (Wt) + β(wt — Wt-i),Wt — W* — ηVf (wt) + β(wt — Wt-1)i
t
+ CwI — ηVf (Wt) +β(Wt — Wt-i)I2 ≤ ηXβt-s(ds + es +gs).
(67)
which is in turn equivalent to showing that
—η Wf(Wt),Wt — W*i +η2(i + Cw) ∣Vf(Wt)k2 —2ηβ(i + CwIhVf(Wt),Wt - Wt-I
-----------{z--------}	'--------'、--------------{-----------}
(a)	(b)	(c)
+ βhWt — Wt-i, Wt — W*i + β2 IWt — Wt-i I2 + Cwβ2 IWt — Wt-i I2	(68)
t
≤ η X βt-s(ds + es + gs).
s=i
For term (a), we have that
hWt —W*,Vf(Wt)i
= (Wt — W*)>A* (Wt — W*) + ρ(IWtI — IW* I)(IWt I2 — W*>Wt)
=(Wt- w*)>(a* + p(kWtk - kW*k)Id)(Wt- w*) + P(kW*k - kWtk)2(kWtk + kW*k).
=(Wt- w*)>A*(Wt- w*) + 2(kWtk - kW*k)kWt - w*I2 + P(kW*k - kWtll)2(kWtk + kW*k).
(69)
Notice that A* 占一Y + ρ∣w* ∣Id 占 0Id, as ρ∣w*∣ ≥ γ. On the other hand, for term (b), We get that
IVf(Wt)I2= IA*(Wt—W*)—P(IW*I —IWtI)WtI2
≤ 2(wt — w*)>A*(wt — w*) + 2ρ2(∣∣w*k — ∣∣Wtk)2∣∣Wtk2.
(70)
25
Under review as a conference paper at ICLR 2021
For term (c), we can bound it as
-2ηβ(1 + CW)(V∕(wt),wt - Wt-I) ≤ ll√2η(i + CWRf(Wt)IIll√2β(wt - wt-ι)k
≤ 1 (2η2(1 + Cw)2kVf(Wt)I2 +2β2kwt- Wt-Ik2)	(71)
=η2(1 + CW)21IVf(Wt)Il2 + β2∣∣wt - wt-ιk2∙
Combining the above, we have that
-η(Vf (Wt), Wt - W*〉+ η2(1 + CW)IlVf(Wt)I2 - 2ηβ(1 + CW)Ef(Wt),Wt - Wt-1
+ β(Wt - Wt-i,Wt - W*〉+ β2∣∣Wt - Wt-Ik2 + CWβ2∣∣Wt - Wt-Ik2
≤ -ηhVf(Wt),Wt - W*〉+ η2CW IlVf(Wt)k2 + β(Wt - Wt-1,Wt - W*〉+ β2(2 + CW )∣∣Wt - Wt-Ik2
≤ -η(Wt - W*)> (A* - 2ηcWA*)(Wt - w*)
-ηP(kw*k - kwtll)2(kw*k + ∣∣wtk - 4ηρ≡W∣∣wtk2) + η∣(kw*k - kwtk)kwt - w*k2
+ β(Wt — Wt-1, Wt — W*〉+ β2 (2 + Cw) kwt — Wt-Ik2
'----------------------V---------------------}
= β(Wt-Wt-ι,Wt-W*> + βCw kWt-Wt-1 ∣∣2
t-1	t
≤ η(dt + et +	9t)	+ ηβ^Xβt	1 s(ds	+ es +	9s)	：= η^Xβt	s(ds	+ es	+ 9s).
S=	S=	(72)
where in the second to last inequality we used β(2 + CW)= CW and the last inequality we used the
assumption at iteration t. So we have completed the induction.
□
Lemma 13. Assume that for all t, ∣∣wtk ≤ R for some number R. Following the notations used in
Lemma 12, we denote sequences dt ：= P(∣w*∣ — kwtk)kwt — w*∣2, et := -(Wt — w*)>(A* —
2ηcWA*)(wt - w*), gt ：= -P(∣w*∣ - kwtk)2(kw*k + l∣wt∣∣- 4η减WkWt∣∣2), where CW ：=吕
and Cw ：= (1 + CW) + (1 + CW)2. Let us also denote Ce := (2β2 + 4β + Lβ), L := ∣∣Ak2 + 2ρR,
and Zt ：= PS=0 βt-s+1Vf(ws). If η satisfies: (1) η ≤ 2+e, and (2) η ≤ /A：?, then we have
that for all t,
∣∣Wt+1 - W*k2
≤ (1 - η(1 + β)[ρ∣wtk - (Y - PkWk~γ)])∣wt- w*∣2
+ η2(wt-i - w*)>(2ceA2 + 2β2LId + 2c：ρ2R2Id)(Wt-I- W*) + η2(ce - 2cw)∣∣zt-21∣2 (73)
t-2
+ 2ηβ2 ^X βt 2 s(ds + es + gs).
S=0
Proof. Recall that the update of heavy ball is
Wt+1 = Wt - ηVf (wt) + β(wt - Wt-1).
(74)
26
Under review as a conference paper at ICLR 2021
So the distance term can be decomposed as
∣∣wt+ι - w*∣∣2 =Ilwt - ηV∕(wt) + β(wt - wt-ι) - w*∣∣2
= kwt - w*∣2 - 2ηhwt — w*, Vf (Wty) + η2 ∣∣Vf(wt)∣∣2
+ β2∣∣Wt - Wt-11∣2 - 2ηβ(wt - Wt-I)TVf(Wt) + 2β(wt - w*)>(wt - Wt-I)
= ∣wt - w*∣2 - 2η(wt - w*, Vf (Wty) + η2 IlVf(Wt)∣∣2
+ β2∣∣wt - wt-i∣∣2 - 2ηβ{wt - w*, Vf (wt)) - 2ηβ(w* - Wt-L Vf (Wt))
+ 2β(wt - w*)T(Wt - Wt-1)
= IlWt- w*∣2 - 2η(1 + β) hwt - w*, Vf (wt)) +η2 ∣∣Vf(Wt)II2
`-------7--------'	`----V-----'
(a)	(b)
+ (β2 + 2β)∣Wt - Wt-Ik2 - 2ηβ(w* - Wt-1, Vf(Wt)) + 2β(wt-i - w*)τ(Wt - wt-i).
(75)
For term (a),(wt - w*, Vf (wt)), by using that Vf (w) = A*(w - w*) - ρ(∣∣w*∣ - IlWk)w, we can
bound it as
(wt - w*, Vf (wt))
=(wt - w*)τA*(wt - w*) + P(IlWtIl - llwj)(llwtll2 - w>Wt)
(Wt- w*)τ(A* + P(IWtk- ∣∣w*∣I)Id)(Wt- W*) + P(∣∣w*∣∣ - ∣∣wtIl)2(IWtk + |IW*I).
(76)
On the other hand, for term (b), ∣∣Vf (Wt) ∣∣2, we get that
IlVf(Wt)Il2 = ∣∣A*(wt - w*) - ρ(∣∣w*∣∣-∣∣wt∣∣)wt∣∣2
≤ 2(wt - w*)τA*(wt - w*) + 2ρ2(Iw*I - IwtI)2IwtI2.
By combining (75),(76), and (77), we can bound the distance term as
∣∣Wt+i - W* I2 = ∣∣Wt - W* I2 - 2η(1 + β)(wt - w*, Vf (Wt)) + η2∣∣Vf(wt)∣∣
+ (β2 + 2β)∣wt - Wt-1∣2 - 2ηβ(w* - wt-i, Vf (Wt)) + 2β(wt-i - w*)T(Wt - wt-i)
≤ Iwt - w* I2
(77)
-2η(1 + β)((wt - w*)τ(A* + P(IWtk- ∣∣w*II)Id)(Wt- w*))	(78)
-ρη(1 + β)(llw* ITWtIl)2 (IWtI + Ilw*k)
+ η2(2(wt - w*)τA*(wt - w*) + 2ρ2(∣w*∣ - ∣wt∣)2∣wt∣2)
+ (β2 + 2β)∣wt - wt-i∣2 - 2ηβ(w* - wt-i, Vf (Wt)) + 2β(wt-i - w*)T(Wt - wt-i).
Now let us bound the terms on the last line of (78). For the second to the last term, it is equal to
-2ηβhw* - wt-i, Vf(Wt))	(79)
=-2ηβ(w* - wt-i, Vf (wt-i)) - 2ηβ(w* - wt-i, Vf(Wt)- Vf(Wt-1)),
On the other hand, the last term of (78) is equal to
2β(wt-i - w*)τ(wt - wt-i) = -2βη(wt-i - w*, Vf (Wt-I)) + 2β2(wt-i - w*,wt-i - wt-2).
(80)
By Lemma 12, for all t, we have that
t-2
hwt-i - w*, wt-i - Wt-2〉≤ -CW∣wt-i - wt-2∣2 + ηβt 2 S(ds + es + gs).	(81)
s=i
Therefore, by combining (79), (80), and (81), we have that
(β2 + 2β)∣wt - wt-i∣2 - 2ηβ(w* - wt-i, Vf (Wt)) + 2β(wt-i - w*)T(Wt - wt-i)
≤ (β2 + 2β)∣wt - wt-i∣2 - 2ηβhwt-i - w*, Vf(Wt-I)- Vf(Wt)) - 2cwβ2∣wt-i - wt-212
t-2
+ 2ηβ2 ^Xβt 2 s(ds + es + gs).
S=i
(82)
27
Under review as a conference paper at ICLR 2021
Combining (78) and (82) leads to the following,
∣∣wt+ι - w*∣∣2 ≤ Ilwt - w*∣∣2 - 2η(i + β)((wt - w*)>(4 + 2(lIwtIl-Ilw*∣∣)∕d)(wt - w*))
—ρη(i + β) (IlwJ-Ilwtll)2(Ilwt Il + ∣∣w*∣I)
+ η2(2(wt - w*)>A*(wt - w*) + 2ρ2(Iw*I - Iwt∣∣)2∣IwtIl2)
+ (β2 + 2β)!wt - wt-ι∣∣2 - 2ηβhwt-ι - w*, ▽/(wt-1)- v/(wt)i.
t-2
-2cwβ2∣∣wt-1 - wt-2∣∣2 + 2ηβ2 Xβt-2-s(ds + es + gs).
s = 1
(83)
Let US now bound the terms on the second to the last line (83) above,
(β2 + 2β)!wt - wt-i∣∣2 - 2ηβhwt-1 - w*, vf (wt-I)- Vf(Wt)〉.	(84)
First, note that ∣∣V2f (w)∣∣ ≤ ∣∣A∣∣2 +2ρ∣∣w∣∣. So we know that f is L := Mg + 2ρR smooth on
{w : IIwIl ≤ R}. Second, denote
t
Zt= X βt-s+1Vf (ws),	(85)
s = 0
we can bound Iwt - wt-1 I as
t-1	t-2
Iwt- wt-1I = I - ηXβt-1-sVf (ws)I ≤ η∣∣Vf (wt-1)I + η∣∣ Xβt-1-sVf (ws)∣∣
s=0	s=0
：=η|Vf(wt-1)| + η|zt-2|,
t-1	t-2
Iwt - wt-1I2 = I-η X βt-1-sVf (ws)I2 ≤ 2η2IVf(wt-1)I2 +2η2∣∣ X βt-1-sVf (ws)∣∣2
s=0	s=0
= 2η2∣∣Vf(wt-1)I2 +2η2Izt-2I2,
(86)
Using the results, we can bound (84) as
(β2 + 2β)Iwt - wt-1I2 - 2ηβhwt-1 - w*, Vf (wt-1) - Vf(wt)i
(86)
≤ 2η2(β2 + 2β)(IVf(wt-1)I2 + Izt-2I2) + 2η2βL∣∣wt-1 - w*I(IVf(w- )I + I%-2I)
≤ 2η2(β2 + 2β)(IVf(wt-1)I2 + Izt-2I2)
+ η2βL(Iwt-I- w*∣∣2 + |lVf (wt-1)∣∣2) + η2βL(Iwt-I- w*∣∣2 + Izt-21∣2)
=2η2(β2 + 2β)(IVf(wt-1)I2 + Izt-2I2) +2η2βLIwt-1 - w*I2 + η2βL∣∣Vf(wt-1)I2
+ η2βLIzt-2I2.
(87)
Note that
IVf(wt-1)I2 = IA*(wt-1 - w*) - ρ(Iw*I - Iwt-1I)wt-112
≤ 2(wt-1 - wJ>A*(wt-1 - wj + 2p2(∣∣w*i -∣∣wt-1 II) ∣∣wt-1∣∣2.
Denote
Ce := (2β2 +4β + Lβ).	(89)
By (87), (88), we have that
(β2 +2β)∣wt - wt-1∣2 - 2ηβhwt-1 - w*, Vf (wt-1) - Vf (wt)i.
≤ 2η2ce(wt-1 - w*)>A2(wt-1 - w*) + 2η2cep2(∣w*∣ - ∣wt-1∣)2∣wt-1∣2 + 2η2β2L∣wt-1 - w*∣2
+ η2cβ lzt-2l2.
(90)
28
Under review as a conference paper at ICLR 2021
Let US summarize the results so far, by (83) and (90), we have that
∣∣wt+ι - w*∣∣2
≤ Ilwt - w*ll2 - 2η(i + β)((wt - w*)>(A* + 2(||wtll - l∣w*k)∕d)(wt - w*))
-Pη(i + β)(IIwJ- IwtIl)2(IlwtIl + ∣∣w*∣I)
+ η2(2(wt - w*)>A*(wt - w*) + 2ρ2(∣∣w*∣∣ - Iwt∣∣)2∣IwtIl2)
+ 2η2Cβ(wt-1 - w*)>A2(wt-i - w*) + 2η2Cβρ2(∣∣w*∣∣ - ∣∣wt-i∣∣)2∣∣wt-i∣∣2 + 2η2β2LIwt-I - w*∣∣2
t-2
+ η2(cβ - 2cw)Ilzt-21∣2 + 2ηβ2 ^Xβt 2 s(ds + es + gS),
s = 1
(91)
where we also used that wt-ι - wt-2 = -βzt-2. We can rewrite the inequality above further as
llwt+ι - w*I2
≤ (wt - w*)>(Id - 2η(1 + β)A*(Id - 1 +βA*) 一 ηρ(1 + β)(Iwt∣l - llw*I)Id)(wt - w*)
-ηρ(I + βXllw*Il - IlwtIl)2 (IIwtII(I —7+ β") + llw*II) +2η2cβ”(IlwJ - ∣∣wt-1∣∣)[∣wt-1∣∣2
、-----------------------------V-----------------------------}
≤0
+ η2(wt-i - wj>(2cβA* + 2β2LId)(wt-1 - wj
t-2
+ η2(cβ - 2cw)Ilzt-21∣2 + 2ηβ2 ^Xβt 2 s(ds + es + gS),
s=1
(92)
where we used that -ηρ(1 + β)(∣∣w*∣∣
satisfies
-忖川飞的口- ⅛k) + Iw* I)
≤ 0 for any η that
L 1 + β
η ≤折
(93)
as IlwtII ≤ R for all t. Let us simplify the inequality (92) further by writing it as
∣∣wt+ι - w*∣∣2
≤ (wt - w*)>(ld - 2η(1 + β)A*(Id - 1 + βA*) - ηρ(1 + β)(∣∣wt∣∣ - ∣∣w*∣∣)Id)(wt - w*)
+ η2(wt-i - w*)>(2cβA2 + 2β2LId + 2cβρ2R2Id)(wt-i - w*)+ η2(cβ - 2cw)∣∣zt-2∣∣2
t-2
+ 2ηβ2 ^Xβt 2 s(ds + es + 9s).
s=1
≤) (1 - η(1+mMwtii- (Y- ρ!w2_γ )])nwt- w*ιι2
+ η2(wt-i - w*)>(2cβA2 + 2β2LId + 2cβρ2R2Id)(wt-i - w*)+ η2(cβ - 2cw)∣∣zt-2∣∣2
t-2
+ 2ηβ2 X βt 2 s(ds + es + gs),
s=i
(94)
where (a) is because that (∣∣w*Il-Ilwt-ill)2Ilwt-1∣∣2 ≤ ∣∣w* - wt-1∣∣2∣∣wt-1∣∣2 ≤ ∣∣w* - wt-ι∣∣2 R2
as IlwtII ≤ R for all t, while (b) is by another constraint of η,
r 1 + β
η -4∣∣a*∣∣2 ,
so that 2ηA*(Id — i+βA*)占 3ηA* 占 3η(-Y + ρ∣∣w*I)Id∙
(95)
□
29
Under review as a conference paper at ICLR 2021
Lemma 14. Fix some numbers co, ci > 0. Denote ωζ-2 := P：=0 β. Following the notations and
assumptions used in Lemma 12 and Lemma 13. If η satisfies: (1) η ≤ 孙 八4 俨 +CββL+2cgρ2R2, (2)
η ≤ H(V ,*1β M A U, (3) η ≤ "2 ∕1β R,(4) η ≤ 4Tjr-R, and (5) η ≤「j"；?+：：,2fi,21 for
β(2cw +Lβωt-2 )kA* k	ρβ2Lωt-2R	4ρcwR	Lβ2ωt-2(kA^ k2+ρ2R2)
all t, then we have that for all t,
llwt+i — w*ll2 ≤ (1 一 η(i+ β)[ρkwtk — (Y — ρkw*'k~γ)])∣* — w*∣∣2 + ηβc0∣*-1 — w*Il2
t-2
+ 2ηβcι (kA*Il + ρR) Xβt 2 skws — w*∣∣2
s=0
t-2
—2ηβ2 X βt 2 S(WS- w*)> (A* — ((|w* Il-Il WtlI)Id)(WS- w*).
s = 1
Proof. From Lemma 13, we have that
I∣wt+1 — w*k2
≤ (1 一 η(I + β)[ρHwt∣l — (Y — PkWkγ)])l∣wt — w*ll2
+ η2(wt -1 — w*)>(2cβ A* + 2β2LId + 2cβ ρ2 R2 Id)(Wt-1 — w*) + η2(cβ — 2cw )∣Izt-2 Il2
t-2
+ 2ηβ2 ^Xβt 2 s(ds+ es+ gs),
S=1
≤ (1 — η(I + β) [ρllwt∣l — (Y — ——*2—ɪ)]) IIWt — w*ll2 + ηβco∣∣wt-ι — w*∣∣2 + η2(cβ — 2cw)∣∣zt-2∣∣2
t-2
+ 2ηβ2 ^Xβt 2 s(ds + es + gs),
S=1
≤ (1 — η(1 + β)[ρ∣lwtIl — (Y —川吗~Y)])Hwt — w*ll2 + ηβco∣∣wt-ι — w*∣∣2
t-2
+ 2η2(cβ — 2cw)β2ωf-2 ^X βt 2 S(WS — w*)>(A* + ρ2R2Id)(WS — w*)
S=o
t-2
+ 2ηβ2 ^Xβt 2 S(dS+ eS+ gS),
s=1
(96)
where (a) is by a constraint of η so that η(2ceIA*I2 + 2β2L + 2cβρ2R2) ≤ coβ, and (b) is due to
the following, denote ωf := Pt=0 β, we have that
t	t Qt-S
IztI2 := IXβt-S+1v/(Ws)I2 = β2(ωβ)2IX，Vf(WS)II2
S=0	s = 0 ωt
t	t-S
≤ β2(ωf)2(X JIVf(Ws)I2)	(97)
s=0 ωt
t
≤ 2β2ωf (Xβt-s(ws — w*)τ(A2 + ρ2R2Id)(ws — w*)).
s=0
where the first inequality of (97) is due to Jensen,s inequality and the second inequality of (97) is
due to the following upper-bound of the gradient norm, IVf (WS) 12 = IA*(ws — w*) — ρ(Iw*I —
∣∣WsI)wsI2 ≤ 2(ws — w*)τA*(ws — w*) + 2ρ2(Iw*I — IwsI)2IwsI2 ≤ 2(ws — w*)τA2(ws —
30
Under review as a conference paper at ICLR 2021
w*) + 2ρ2 kw* - Ws k2R2. Bythe definitions of ds, es,gs, we further have that
kwt+1 - w*k2 ≤(1 - η(1 + β)[ρ∣wM - (Y - ρkw*2k_γ)])kws - w*k2 + ηβc0kwt-1 - w*k2
t-2
-2ηβ2 Xβt 2 S(WS- w*厂(A* -左(IlW*11 - IlWSIl)Id)(WS- w*)
s = 1
t-2
+ 2ηβ2 Xβt-2-s(Ws - W*)>(η(2Cw + (Ce - 2cw)ω3)A*)(Ws - w*)
s = 1
t-2
+ 2η2ρ2R2β2ωt-2(cβ - 2cw) Xβt-2-s∣Ws - w*∣2
s = 1
t-2
-ηβ 2P X βt-2-s(kw*k - kwsk)2(kw*k + IlWSk- 4ηρcw IlWSk2)
s=1
+ η2(ce - 2cw)2βtωt-2(ia*i2 + ρ2R2)llw0 - w*II2
≤(1 - η(i+ ⑶同助口 - (Y - p∣w*2∣^γ)])Hwt - w*II2 + ηβc0∣∣wt-1 - w*∣∣2
t-2
-2ηβ2 X βt 2 S(WS- w*)> (a* -号(IIw* Il -IlWSlI)Id)(WS- w*)
s = 1
t-2
+ 2ηβc1 Xβt-2-s(wS - W*)>(A* + PRId)(WS - w*)
s = 1
t-2
-ηβ2ρX- m11)2(11511 + IlWSk- 4ηρcwIlWSk2)
s=1
+ η2(ce - 2cw)2β%β-2(kA*I∣2 + ρ2R2)IIwo - w*k2,
(98)
where the last inequality is due to (1): 2η2β2(2Cw + (Ce - 2cw)ωβ-2)A* W 2η2β2(2Cw +
Lβωβ.2)A2 W 2ηβc1A*, as Ce - 2cw ≤ Lβ and that η ≤	: β , and (2) that
e(2cw +Leωt-2 ) A* k
2η2P2R2β%3(Ce- 2cw) ≤ 2η2ρ2R2Lβ3ωβ-2 ≤ 2ηρβc1R, as η ≤。乎：1 R ∙
Pe Lωt-2R
To continue, let us bound the last two terms on (98). For the second to the last term, by using
that kwtk ≤ R for all t and that η ≤ " R, we have that (∣∣w*∣∣ + IlWSk _ 4ηρCwIlWSk2) ≥
IW* I. Therefore, we have that the second to last term on (98) is non-positive, namely,
-ηβ2ρ PS=1 βt-2-S(kw*k - ∣∣Ws∣∣)2 (kw*k + ∣∣Ws∣∣ -4ηρCw ∣∣Ws∣∣2) ≤ 0. For the last term on (98),
by USingthat η ≤ LB2："A%ρ22r2), we have that η2(ce - 2cw)2βtω3(kA* 112+ρ2R2)kwo-
Le ωt-2(kA*k +。R )
w*k2 ≤ η22Lβt+1 ωe-2(kA*k2 + ρ2R2)∣∣w0 - w*∣2 ≤ 2ηβt-1c1 (kA*k + ρR)∣∣w0 - w*∣2.
Combining the above results, we have that
kwt+ι 一 w*k2 ≤ (I 一 η(l + β)[ρ∣∣wt∣∣ — (Y — ρkw*2∣γ)])kwt — w*k2 + ηβc0kwt-i — w*k2
t-2
+ 2ηβc1 (kA* Il + ρr) X βt 2 s kws - w* ∣∣2
S=0
t-2
-2ηβ2 X βt 2 S(WS- w*)> (a* -( (kw* k - kws k)Id) (WS- w*)∙
s = 1
(99)
31
□
Under review as a conference paper at ICLR 2021
The following lemma will be used for getting the iteration complexity from Lemma 14.
Lemma 15. For a non-negative SeqUenCe {yt}t>o, suppose that it satisfies
t-2
yt+ι ≤ pyt + qyt-ι + rβyt-2 + χβXβt-2-s% + Zet-T +1, forall t ≥ τ, (100)
for non-negative numbers p, q,r,x,z ≥ 0 and β ∈ [0,1), where we denote
t
yt := X βt-sys∙
s=0
Fix some numbers φ ∈ (0,1) and ψ > 0. Define θ = p+Vzp 2+4(q+°). Suppose that
β≤
and β ≤
P + p2p+ + 4(q + φ)
2
then we have that for all t ≥ τ,
y ≤(P+，p2 + 4(q+φ) y-τc B
where cτ,β is an upper bound that satisfies,for any t ≤ T,
yt + θyt-ι + ^yt-2 + βψz ≤ cτ,β,
(101)
(102)
(103)
2
r + φ + X
φ
1
ψ
Proof. For a non-negative sequence {yt}t>0, suppose that it satisfies
t-2
yt+ι ≤ Pyt + qyt-ι + rβyt-2 + χβ X βt-2-sys + zβt-τ+1, for all t ≥ τ, (104)
for non-negative numbers p, q, r, x, Z and β ∈ [0,1), where we denote
t
yt ：= X βt-sys∙
s=0
(105)
yt+1 + θyt + φyt-ι + ψzβt-τ+2
(a)	t-2
≤ (p + θ)yt + qyt-1 + rβyt-2 + Φyt-1 + xβ£βt-2-sys + (ψβ + 1)zβt-τ +1
s=τ
t-2
=(p + θ)yt + (q + Φ)yt-1 + (rβ + φβ )yt-2 + xβ X βt-2-sys + (ψβ + 1)zβt-τ +1
s=τ
≤(p+θ)(yt + ⅜+τyt-1)+β(r+φ+x)yt-2 + (ψβ+1)zβ t-τ+1
(c)
≤ (p + θ)(yt + θyt-i) + β(r + φ + x)yt-2 + (ψβ + 1)zβt-τ +1
(d)
≤ (p + θ)(yt + θyt-i + φyt-2 + ψzβt-τ+1) ≤ (p + θ)2(yt-i + θyt-2 + φyt-3 + ψzβt-τ-1) ≤ ∙ ∙.
:=(P + θ)t-τ +1Cτ,β ∙
(106)
where (a) is due to the dynamics (100), (b) is because yt-ι = yt-1 + βyt-2, (c) is by,
q+Φ ≤ θ,
p+θ
(107)
and (d) is by
β ≤ r⅞+空X andβ ≤P+θ - ψ
(108)
32
Under review as a conference paper at ICLR 2021
Note that (107) holds if,
-p + vzp2 +4(q + φ)
2
(109)
θ ≥
Let US choose the minimal θ = -p+ 山；+4"+。). So we have P + θ = KVZp 2^4(q+φ), which
completes the proof.	□
Lemma 16. Assume that for all t, ∣∣wt∣∣ ≤
in Lemma 14 so that ci ≤
______C0______
40(∣∣A*∣∣+ρR) .
R for some number R. Fix the numbers co ,ci > 0
Suppose that the SteP size η satisfies (1) η ≤
2cβ ∣∣A*k2 + cββL+2cβρ2R，⑵ η ≤	十盛；β-2)∣∣A*∣∣, (3) η ≤ 演最予,(4) η ≤ 4⅛R, (5)
η ≤ wj："PRP2R2),⑹ η ≤ ⅛r，and(7) η ≤ 4⅛⅛ forallt，where Ce := (2β2+4β+Lβ)，
L ：= ∣∣A∣∣2 +2PR，cw := i2ββ，cw := (1 + Cw ) + (1 + cw )2 and ωt_2 := pS=0 β. Furthermore, sup-
pose that the momentum parameter β satisfies β ≤ min{ ɪɪ (1 — η(1 + β)δ), 1 — η(1 + β)δ — 0.1).
Fix numbers δ, δ0 so that ɪ(ρ∣∣w*∣∣ — Y) > δ > 0 and 馈 ≥ δ0 > 0. Assume that ∣∣wt ∣∣ is non-
decreasing. If Pkwt Il ≥ Y — ɪ (PIlw*∣∣ — γ) + δ and PIlwtll ≥ Y — δ0 for some t ≥ t*，then we have
that
Ilwt - w*k2 ≤ (1 - η(1 + β)δ +
2ηβco	`t-t*
1 - η(1+ β)δ)	cβ,t∙-
where Cβ,t* is a number that satisfies for any t ≤ t*， ∣∣wt — w*∣∣2 + [-券？* Ilwt-I - w*∣∣2 +
ηβcoPS=Oβt-2-sIlws -w*Il2+maχ{0,β20ηPs=IIβt*-1-s(γ2-PIlwsII)Ilws-w*H2} ≤ cβ,t*
Proof. From Lemma 14, We have that
llwt+i - w*∣∣2 ≤ (1 - η(1 + β)[ρ∣∣wt∣∣ - (Y - ρHw*2∣^γ)])Hwt - w*ll2 + ηβco∣∣wt-ι - w*Il2
t-2
+ 2ηβcι(IlAII* + PR)^Xβt 2 SIlwS-w*∣∣2
s=0
t-2
-2ηβ2 Eβt 2 S(WS- w*)> (A* -左(llw*Il-IlwsIl)Id)(WS- w*).
s = 1
Using that for all t ≥ t*, P∣∣wt ∣∣ ≥ Y - 2 (Pllw*∣∣ - γ) + δ, We have that
llwt+1 - w*I2 ≤ (1 - η(1 + β)δ)∣∣wt - w*∣∣2 + ηβco∣∣wt-i — w*∣∣2
t-2
+ 2ηβcι(∣∣Ak* + PR) ^Xβt 2 SkwS- w*∣∣2
s=o
t-2
-2ηβ2 ^Xβt 2 s(ws - w*)>(A* - (llw*ll- Ilws∣∣)Id)(ws - w*)
s=1
=(1 - η(1 + β)δ)Iwt - w*I2 + ηβco∣∣wt-1 - w*∣∣2
t-2
+ 2ηβcι(∣∣Ak* + PR) ^Xβt 2 SkwS- w*∣∣2
s=o
(110)
—
—
t 2
2ηβ2 E βt -2- s(ws - w*)>(A* - P(Iw*I - IwsI)ld)(ws - w*)
S=t*
t* -ɪ
2ηβ2 E βt-2-s(ws - w*)>(A* - P(Iw*I - IwsI)ld)(ws - w*).
s = 1
We bound the second to last term of (110) as follows. Note that for S ≥ t*, we have that PkwSk ≥
Y - δ0 by the assumption that kwtk is non-decreasing. Therefore, once Pkwtk exceeds any level
33
Under review as a conference paper at ICLR 2021
blow Y — δ, it will not fall below Y — δ0. So we have that
A* — P (kw*k - IlwSk)Id 占 A* — Pkw* kId + Y"2- Id
占(—γ+ρkw*ι∣)Id - Pkw*kId + y-2— Id
(111)
(C)
占
where (a) uses that PkwSk ≥ Y 一 δ for some number δ0 > 0, (b) uses the fact that A* 占(—+
ρkw*k)Id, and (c) uses the fact that ρkw*k ≥ Y∙ Using the result, We can bound the second to the
last term of (110) as
t-2
一 2ηβ2 X βt 2 s(ws — w*)> (A* — 2 (∣∣w*k — IIwSk)Id) (ws — w*)
S t *
t-2
≤ ηβ2δ0 X βt-2-skws — w* k2.
s = t*
(112)
Now let us switch to the last term of (110). Using the fact that A* 占(—γ + ρkw*k)Id, and that
ρkw*k ≥ Y by the characterization of the optimizer ∣∣w*k, We have that
a* — P (kw*k - kws∣∣)Id 占 P (kw*k + IlwSk) Id —Y 占(PkwSk - Y )Id.	(I⑶
So we can bound the last term as
—
t* — 1
2ηβ2 X βt-2-s(wS — w*)>(A* — P(kw*k — kwS∣l)Id)(wS — w*)
s=1	(114)
t*-1
≤ 2ηβt-t*+1 X βt*-1-s(wS — w*)>(2 — PIlwSk)Id(wS — w*) ：= 2ηβt-t*+1Dt*,
s = 1
where we denote Dt* := PS=II βt*-1-s(wS — w*)>(2 — 2kwS∣l)Id(wS — w*). Combining (110),
(112), (114), we have that
kwt+1 — w*k2 ≤ (1 — η(i + β)δ)kwt — w*k2 + ηβc0kwt-1 — w*k2
t-2
+ 2ηβC1(kAk* + PR) Xβt 2 SkwS- w*∣∣2
s=0
t-2
+ ηβ2δ0 X βt-2-1∣wS — w*k2 + 2ηβt-t*+1Dt*.
S^ t*
(115)
Now we are ready to use Lemma 15. Set p, q, r, x, Z in Lemma 15 as follows.
•	P = 1 — η(1 + β )δ
•	q = ηβc0
•	r = 2η(kA*k + pR)q
•	X = ηβδ0
•	Z = 2ηDt* l{Dt* ≥ 0}.
•	φ = q = ηβc0
•	ψ = 10.
34
Under review as a conference paper at ICLR 2021
we have that
Ilwt -
W ∣∣2 ‹(1 - η(1 + β) + ,(1 - η(1 + 万州)2 + 4(2ηβcp)广“
≤ (1 - η(1 + β)δ +
2ηβc0	、t-t*
1 - η(1+ β)δ)
(116)
1	J . a+a√ 1+4b∕α2 / a⅛a(l+ 2b)	, b 0	1	τ .< .	.∙ r	2 、
where we use that------------------ ≤ -------L— = a + b for number a, b that satisfy 4b∕a2 ≥
-1. Now let US check the conditions of Lemma 15 to see if β ≤
P十 √p2 十4(q +φ) φ
r+φ+x
and β
p+ Vzp +4⅛±φ! - ψ. For the first condition, β(r + φ + x) ≤ p+ Vzp +4(q±φφ is equivalent
to β(2η(∣A*∣ + PR)CI + ηβco + ηβδz) ≤ p+ Vzp +4(q±φ!ηβc0, which can be satisfied by
c1 ≤ 40「U AC0」CR and δ0 ≤ 20, leading to an upper bound of β, which is β ≤ |?p+Vzp +4(q+φ.
40( k JA* k + P^X)	2 0	112
Using the expression of p, q, φ, and ψ, it suffices to have that β ≤ ɪɪ (1 - η(1 + β)δ). On the other
hand, for the second condition, β ≤ p+Vzp +4(q±φ2 - _1, by using the expression of p, q, φ, and ψ,
it suffices to have that β ≤ 1 - η(1 + β)δ - 0.1.
≤
□
F.2 PROOF OF THEOREM 3
Proof. Let δ = c3 (PIlW*∣ - Y) for some number c^
some number C0 > 0. By Lemma 16, we have that
< 0.5. Denote CTge := 1 - τ¾ for
IlWt - w* I2 ≤ (1 - η(1 + β)δ +
2ηβC0δ
1 - η(1 + β)δ
t——t *
cβ,t*
≤ (1 - ηδ(1+ βc*verge))t-t*cβ,t*
(117)
where cβ,t* is a number that satisfies for any t ≤ t*, ∣wt - w*∣2 + -森曙” IlWt-I — w*∣2 +
ηβc0 PS=0 βt-2-s∣∣ws- w*∣∣2 +max{0,β20η Ps=LI βt*-1-s(γ - A IlWSll)IlWS- w*∣∣2} ≤ cβ,t*.
Note that we can obtain a trivial upper-bound cβ,t for any t as follows. Using that Iwt -w*∣∣2 ≤ 4R2
and that c0 — C0δ and that δ — c (PIlW* ∣∣ - Y), we can upper-bound the term as
cβ,t ≤ 4r2(1 + 1-：(；； ?力|[；-Y)+ ηβc0cδ (Hg. γ) Et + β10η max{0,γ}H)
≤ 4R2(1 + ηβCβ)
:= cCβ .
(118)
where we define Cβ
4R2(1 + ηβCCβ).
[2：：!川::-：)、+ τ-Lβ(C0cδ(PkW*i - γ) + 10max{0,γ}) and cβ :
1-n(1+β)cδ (PkW*k-γ)	β
Now denote CCOnverge := 1 —
2cq
1-2ncδ (PkW* ∣∣-γ)
. We have that
"	、"Ja) IAS + 2ρR	,l2
f(wt*+t) - f(w*) ≤	2	l∣wt*+t - w*ll
≤ iai2 + 2PRCβ exp ( - ηcδ(ρ∣∣w*∣∣ - Y)(1 + βcconverge)t)
(119)
where (a) uses the (IlAIl2 + 2pR)-smoothness of function f (∙) in the region of {w : Iw∣∣ ≤ R}, and
(b) uses (117), (118) and that δ := c^ (PIlW* I - Y). So we see that the number of iterations in the
linear convergence regime is at most
t ≤T:
1
ηcδ(PIlW*I - Y)(1+ βcconverge)
log( (IAl∣2 + 2PR)Cβ )
(120)
35
Under review as a conference paper at ICLR 2021
Lastly, let us check if the step size η satisfies the constraints of Lemma 16. Recall the notations that
Cw := 1-ββ, ɑw := (1 + Cw) + (1 + Cw)2, Cβ ：= (2β2 +4β + Le), and L := kA∣∣2 + 2ρR. Lemma 16
has the following constraints，(1) η ≤ 2cβ kA*k2 + cββL+2cβ ρ2R2 , Q) η ≤ β(2Cw +Lβ>1-β))kA* k，
⑶ η ≤ ρβ2LRc∕(1-β), (4) η ≤ 4ρδw R, (5) η ≤ Lβ2 (kA^k2 + P2R2)∕(1-β), (6) η ≤ 2+β, and ⑺
η ≤ 4 J+：2. For the constraints of (1), using co = C0δ and that δ = Cδ (Pkw*k - Y), it can be
rewritten as
≤__________2οcδ (Pkw*k - Y)________
η ≤ 2(2β + 4 + L)(kA*k2 + P2R2) +2βL
(121)
For the constraints of (2), using c1 ≤ 40(kA0δ+ρR) and that δ = Cδ (Pkw* k - Y), it can be rewritten
as	*
C0cδ (Pkwj- Y)
η ≤ 40β(kA*k2 + PRkA*k)(2≡w + Lβ∕(1 - β)).
(122)
The constraints of (3) can be written as, using ci ≤4oqgf+pR) and that δ = Cδ (Pkw* k - Y),
C0cδ (Pkw*k- Y)
η ≤ 40PR(kA*k + PR)β2L∕(1 - β).
The constraints of (5) translates into, using c1 ≤ 40(kA0δ+ρR) and that δ = Cδ (Pkw* k - Y),
⅞0cδ (Pkw*k - Y)
η ≤ Le2(kA*k2 + P2R2)∕(1- β).
Considering all the above constraints, it suffices to let η satisfies
1
(
η ≤ min
cοcδ (Pkw*k - Y)
4(kA*k + CwPR)，Cβ(kA*k + pR)(1 + kA*k + PR) + 2βL ，
(123)
(124)
(125)
where Cβ := max(4e + 8 + 2L, 40-βL + 80βCw, 4Cw).
Note that the constraint of η satisfies ηcδ Pkw* k - Y ≤ 1 Cδ ≤ 1. Using this inequality, we
can simplify the constraint regarding the parameter e in Lemma 16, which leads to e ∈ [0, 0.65].
Consequently, we can simplify and upper bound the constants cCw , Cβ and Cβ, which leads to the
theorem statement.
Thus, we have completed the proof.
□
G More discussions
Recall the discussion in the main text, we showed that the iterate wt+1 generated by HB satisfies
hwt+1,uii = (1 + ηλi)hwt, uii + e(hwt, uii - hwt-1,uii),	(126)
which is in the form of dynamics shown in Lemma 1. Hence, one might be able to show that with
the use of the momentum, the growth rate of the projection on the eigenvector ui (i.e. |hwt+1, uii|)
is faster as the momentum parameter e increases. Furthermore, the top eigenvector projection
∣hwt+ι,uιi∣ is the one that grows at the fastest rate. As the result, after normalization (i.e. IwTJ
), the normalized solution will converge to the top eigenvector after a few iterations T .
However, we know that power iteration or Lanczos method are the standard, specialized, state-of-
the-art algorithms for computing the top eigenvector. It is true that HB is outperformed by these
methods. But in the next subsection, we will show an implication of the acceleration result, com-
pared to vanilla gradient descent, of top eigenvector computations.
36
Under review as a conference paper at ICLR 2021
(a) η = 1 × 10-2	(b) η = 5 × 10-3	(c) η = 1 × 10-3	(d) η = 5 × 10-4
Figure 3: Distance to the top leading eigenvector vs. iteration when applying HB for solving minw 2 w> Aw.
The acceleration effect due to the use of momentum is more evident for small η . Here we construct the matrix
A = BB> ∈ R10×10 with each entry ofB ∈ R10×10 sampled fromN(0, 1).
G.1 Implication: escape saddle points faster
Recent years there is a growing trend in designing algorithms to quickly find a second order sta-
tionary point in non-convex optimization (e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-
Zhu & Li (2018); Xu et al. (2018); Ge et al. (2015); Levy (2016); Fang et al. (2019); Jin et al.
(2017; 2018; 2019); Daneshmand et al. (2018); Staib et al. (2019); Wang et al. (2020)). The com-
mon assumptions are that the gradient is L-Lipschitz: ∣∣Vf (x) — Vf (y)k ≤ Lkx — y∣∣ and that
the Hessian is ρ-Lipschitz: ∣∣V2f (x) — V2f (y)∣ ≤ Pkx — y∣∣, while some related works make
some additional assumptions. All the related works agree that if the current iterate wt0 is in the
region of strict saddle points, defined as that the gradient is small (i.e. ∣Vf (wt0)∣ ≤ g ) but
the least eigenvalue of the Hessian is strictly negative (i.e. λmin(V2f(wt0 ))	—hId), then
the eigenvector corresponding the least eigenvalue λmin(V2f(wt0 )) is the escape direction. To
elaborate, by ρ-Lipschitzness of the Hessian, f(wt0+t) — f (wt0 ) ≤ hVf (wt0), wt0+t — wt0i +
5(wt0 +t — Wto)> V2f (wto)(wt0 +t — Wto)+ 3 ∣∣Wto +t — Wto k3. Soif Wto+t — Wto is in the direction
2
I	、	/
exhibit negative curvature
of the bottom eigenvector of V2 f(Wto), then 2(Wto +t — Wto )> V2 f(Wto)(Wto+t — Wto) ≤ — c0Ch
for some c0 > 0. Together with the fact that the gradient is small when in the region of saddle
points and the use of a sufficiently small step size can guarantee that the function value decreases
sufficiently (i.e.f(Wto+t) — f(Wto) ≤ —cCh for some c > 0). Therefore, many related works design
fast algorithms by leveraging the problem structure to quickly compute the bottom eigenvector of
the Hessian (see e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al.
(2018)).
An interesting question is as follows “If the Heavy Ball algorithm is used directly to solve a non-
convex optimization problem, can it escape possible saddle points faster than gradient descent?”
Before answering the question, let us first conduct an experiment to see if the Heavy Ball algorithm
can accelerate the process of escaping saddle points. Specifically, we consider a problem that was
consider by Staib et al. (2019); Reddi et al. (2018); Wang et al. (2020) for the challenge of escaping
saddle points. The problem is
minw f(W) := 1 Pn=I (1W>HW + x>W + MIIIO)	(127)
with H ：=0 —0 1 . where xi 〜 N(0, diag([0.1, 0.001])) and the small variance in the second
component will provide smaller component of gradient in the escape direction. At the origin, we
have that the gradient is small but that the Hessian exhibits a negative curvature. For this problem,
Wang et al. (2020) observe that SGD with momentum escapes the saddle points faster but they make
strong assumptions in their analysis. We instead consider the Heavy Ball algorithm (i.e. Algo-
rithm 1, the deterministic version of SGD with momentum). Figure 4 shows the result and we see
that the higher the momentum parameter β , the faster the process of escaping the saddle points.
We are going to argue that the observation can be explained by our theoretical result that the Heavy
Ball algorithm computes the top eigenvector faster than gradient descent. Let us denote h(W) :=
1 Pn=I (x>W + ∣∣w∣∣10). We can rewrite the objective (127) as f (w) := 1 w>Hw + h(W). Then,
37
Under review as a conference paper at ICLR 2021
the Heavy Ball algorithm generates the iterate according to
1-η 0
wt+1 =	0	1 + η Wt + β(Wt - Wt-I) - ηVhCwt).
U + + 10_|	(128)
、	-V-	J
:二A
By setting η ≤ 1, we have that the top eigenvector of A is u1 = e2 , which is the es-
cape direction. Now observe the similarity (if one ignores the term ηVh(Wt)) between the up-
date (128) and 19. It suggests that a similar analysis could be used for explaining the es-
cape process. In Appendix G.2, we provide a detailed analysis of the observation. How-
ever, problem (127) has a fixed Hessian and is a synthetic objective function. For general
smooth non-convex optimization, can we have a similar explanation? Consider applying the
Heavy Ball algorithm to minw f (W) and suppose that at time t0, the iterate is in the re-
gion of strict saddle points. We have that Wt+1+t0 - Wt0 = Id - ηV2f (Wt0 ) Wt+t0 -
wt0) + β((wt+t0 — wt0 ) — (wt+t0-1 — wt0 )) + η( V2f(Wt0 )(wt+t0 — wt0 ) -Vf(Wt+t0J ).
|	7	^^^^^^^^^^^^^^eJ
By setting η ≤ L With L being the smoothness constant of the
problem, we have that the top eigenvector Id - ηV2f(Wt0 ) is
the eigenvector that corresponds to the smallest eigenvalue of the
Hessian V2f(Wt0), Which is an escape direction. Therefore, if the
deviation term can be controlled, the dynamics of the Heavy Ball
algorithm can be vieWed as implicitly and approximately comput-
ing the eigenvector, and hence We should expect that higher val-
ues of momentum parameter accelerate the process. To control
V2 f (Wt0)(Wt+t0 -Wt0)-Vf (Wt+t0), one might Want to exploit the
ρ-Lipschitzness assumption of the Hessian and might need further
mild assumptions, as Du et al. (2017) provide examples shoWing
that gradient descent can take exponential time T = Ω(exp(d)) to
escape saddles points. Previous Work of Wang et al. (2020) makes
strong assumptions to avoid the result of exponential time to escape.
deviation
Figure 4: Solving (127) (With
n = 10) by the Heavy Ball al-
gorithm With different β .
On the other hand, Lee et al. (2019) shoW that first order methods can escape strict saddle points
almost surely. So We conjecture that under additional mild conditions, if gradient descent can escape
in polynomial time, then using Heavy Ball momentum (β 6= 0) Will accelerate the process of escape.
We leave it as a future Work.
G.2 Escaping saddle points of (127)
Recall the objective is
minw f (w) := n P= (ɪW>Hw + x>W + ∣∣w∣∣10)	(129)
With
H := 10 -00.1 .	(130)
where Xi 〜 N(0, diag([0.1, 0.001])) and the small variance in the second component will provide
smaller component of gradient in the escape direction. At the origin, We have that the gradient is
small but that the Hessian exhibits a negative curvature. Let US denote h(W) := 1 Enn=I (x>W +
k Wk 10). We can rewrite the objective as f (w) := ɪW>Hw + h(W). Then, the Heavy Ball algorithm
generates the iterate according to
Wt+1 = Wt - ηHWt + β(Wt - Wt-1) - ηVh(Wt)
= 1	-0	η 1	+00.1η	Wt	+	β(Wt -	Wt-1)	-	ηVh(Wt)	(131)
:= AWt + β(Wt - Wt-1) - ηVh(Wt),
where the matrix A is defined as A :=
1-η
0
0
1 + 0.1η
By setting η ≤ 1, we have that the
top eigen-vector of A is u1 = e2 = 01
, which is the escape direction. In the following, let us
38
Under review as a conference paper at ICLR 2021
denote u2	:=	e1	=	10	and denote X = 1 Pi=I xi.	Since	kwkpp	:= (Pd=I	∣Wi∣p) and that
d∂wkp- := p∣Wj∣p-1 d∂WWjl = p∣Wj∣p-2Wj, wehavethat
Vh(w) = x + 10abs(w)8 ◦ w,	(132)
where ◦ denotes element-wise product and abs(∙) denotes the absolution value of its argument in the
element-wise way.
By initialization w0 = w-1 = 0, we have that (w0> u1) = (w->1u1) = (w0>u2) = (w->1u2) = 0 and
the dynamics
wt>+1u2 = (1 - η)wt>u2 + β(wt>u2 - wt>-1u2 ) - ηu2> Vh(wt)
w>+1U1 = (1 + 10)w>uι + β(w)uι — Wt-ιUι) 一 ηu>Vh(wt),
while we also have the initial condition that
w>u2 = —ηu>Vh(wo) = —ηu>Vh( 0 ) = —ηu>x = —ηx[i]
w>uι = —ηu>Vh(wo) = —ηu>Vh( 0 ) = —ηu>x = —ηx[2].
That is,
w] = —ηx.
Using (132), we can rewrite (133) as
Wt+1U2 = (1 — η)w>U2 + β(w>U2 — Wt-iU2)— ηx[1] — 10η(Wt+1U2 )9
Wt+1U1 = (1 + 10)w> U1 + β(w> U1 — Wt-1U1) — ηx[2] — 10η(wt+ι U1)9.
Note that we have that
Vf (w) := Hw + Vh(w) = Hw + x + 10abs(w)8 ◦ W
1 + 10|w[1]|8	0
=	0	-0.1 + 10∣w[2]∣8 W + x
So the stationary points of the objective function satisfy
(1 + 10∣w[1]∣8)w[1] + x[1] = 0
(—0.1 + 10∣w[2]∣8)w[2] + x[2] = 0
To check if the stationary point is a local minimum, we can use the expression of the Hessian
V2f(w) := H + V2h(w)
1 + 90|w[1]|8	0
=	0	—0.1 + 90|w[2]|8 .
(133)
(134)
(135)
(136)
(137)
(138)
(139)
Therefore, the Hessian at a stationary point is positive semi-definite as long as ∣w[2]∣8 ≥ 焉,which
can be guaranteed by some realizations of x[2] according to (138).
Now let us illustrate why higher momentum β leads to the faster convergence in a high level way,
From (138), we see that one can specify the stationary point by determining x[1] and x[2]. Fur-
thermore, from (139), once the iterate Wt satisfies ∣wt[2]∣8 > 焉,the iterate enters the locally
strongly convex and smooth region, for which the local convergence of the Heavy Ball algorithm
is known (Ghadimi et al. (2015)). W.l.o.g, let us assume that x[2] is negative. From (i34), we
have that w>u1 > 0 when x[2] is negative. Moreover, from (136), if, before the iterate satisfies
∣wt[2]∣8 > gio , we have that 10(w>uι) - 10(w>uι)9 — x[2] > 0, then the contribution of the
projection on the escape direction (i.e. wt>+1u1) due to the momentum term β(wt> u1 — wt>-1u1)
is positive, which also implies that the larger β, the larger the contribution and hence the faster the
growing rate of ∣wt[2]∣. Now let us check the condition, 10 (w>u1) — 10(w>ui)9 — x[2] > 0 before
∣wt[2]∣8 > 焉.A sufficient condition is that (10 —击)(w7u1) — x[2] > 0, which we immediately
see that it is true given that x[2] is negative and that w>u1 > 0 and that the magnitude of w>u1 is
increasing. A similar reasoning can be conducted on the other coordinate wt+1[1] := wt>+1u2.
39
Under review as a conference paper at ICLR 2021
H Empirical results
H.1 Phase retrieval
*MΦPOE ① n」l ① £ 0一 ①。UEωp
Figure 5: Performance of gradient descent with Heavy Ball momentum with different values of
β = {0, 0.3, 0.5, 0.7, 0.9, 1.0 → 0.9} for solving the phase retrieval problem (1). The case of
β = 0 corresponds to the standard gradient descent. Left: we plot the convergence to the true
model w*, defined as min(kwt - w*k, kwt + w*k), as the global sign of the objective equation 1 is
unrecoverable. Right: we plot the objective value (1) vs. iteration t.
O 1OOO 2000	3000	4000	5000
iteration
0	1000	2000	3000	4000	5000
iteration
(a) |wtk | vs. t	(b) kwt⊥ k vs. t
Figure 6: Performance of HB with different β = {0, 0.3, 0.5, 0.7, 0.9, 1.0 → 0.9} for phase retrieval. (a):
The size of projection of Wt on w* over iterations (i.e. |wk | vs. t), which is non-decreasing throughout the
iterations until reaching an optimal point (here, ∣∣w* ∣∣ = 1). (b): The size of the perpendicular component over
iterations (i.e. kwt⊥ k vs. t), which is increasing in the beginning and then it is decreasing towards zero after
some point. We see that the slope of the curve corresponding to a larger momentum parameter β is steeper than
that of a smaller one, which confirms Lemma 1 and Lemma 3.
All the lines are obtained by initializing the iterate at the same point w0 ~ N(0,Td/(10000d))
and using the same step size η = 5 × 10-4. Here We set w* = e1 and sample Xi ~ N(0, Td)
with dimension d = 10 and number of samples n = 200. We see that the higher the momentum
parameter β , the faster the algorithm enters the linear convergence regime. For the line represented
by HB+(β = 1.0 → 0.9), it means switching to the use of β = 0.9 from β = 1 after some
iterations. Below, Algorithm 3 and Algorithm 4, we show two equivalent presentations of this
40
Under review as a conference paper at ICLR 2021
practice. In our experiment, for the ease of implementation, we let the criteria of the switch be
1{ f(Wf(Wf(Wt) ≥ 0.5}, i.e. if the relative change of objective value compared to the initial value
has been increased to 50%.
Algorithm 3: Switching β = 1 to β ‹ 1
1:	Required: step size η and momentum parameter β ∈ [0, 1).
d
2:	Init: u = v = w0 ∈ Rd and β = 1.
3:	for t = 0 to T do
4:	Update iterate wt+ι = Wt — ηVf (Wt) + β(u — v).
5:	Update auxiliary iterate u = v
6:	Update auxiliary iterate v = Wt .
7:	If { Criteria is met }
^ _
8:	β	=	β.
9:	u	=	v = Wt+1	#	(reset	momentum)
10:	end
11:	end for
Algorithm 4: Switching β = 1 to β < 1.
1:	Required: step size η and momentum parameter β ∈ [0, 1).
2:	Init: wo ∈ Rd, m-ι = 0d, β = 1.
3:	for t = 0 to T do
4:	Update momentum mt := βmt-1 + Vf (Wt).
5:	Update iterate Wt+1 := Wt — ηmt .
6:	If { Criteria is met }
^ _
7:	β = β.
8:	mt = 0.	# (reset momentum)
9:	end
10:	end for
H.2 Cubic-regularized problem
Figure 2 shows empirical results of solving the cubic-regularized problem by Heavy Ball with dif-
ferent values of momentum parameter β. Subfigure (a) shows that larger momentum parameter β
results in a faster growth rate of kwt k, which confirms Lemma 2 and shows that it enters the benign
region IB faster with larger β. Note that here We have that k w* k = 1. It suggests that the norm is
non-decreasing during the execution of the algorithm for a wide range ofβ except very large β For
β = 0.9, the norm starts decreasing only after it arises above kw* k. Subfigure (b) show that higher
β also accelerates the linear convergence, as one can see that the slope of a line that corresponds
to a higher β is steeper than that of the lower one (e.g. compared to β = 0), which verifies Theo-
rem 3. We also observe a very interesting phenomenon: when β is set to a very large value (e.g. 0.9
here), the pattern is intrinsically different from the smaller ones. The convergence is not monotone
and its behavior (bump and overshoots when decreasing); furthermore, the norm of kwt k generated
by the high β is larger than kw* k of the minimizer at some time during the execution of the algo-
rithm, which is different from the behavior due to using smaller values of β (i.e. non-decreasing of
the norm until the convergence). Our theoretical results cannot explain the behavior of such high
β, as such value of β exceeds the upper-threshold required by the theorem. An investigation and
understanding of the observation might be needed in the future.
Now let us switch to describe the setup of the experiment. We first set step size η = 0.01, dimension
d = 4, ρ = kw*k = kAk2 = 1, γ = 0.2 and gap = 5 × 10-3. Then we set A = diag([-γ; -γ +
gap; a33; a44]), where the entries a33 and a44 are sampled uniformly random in [-γ + gap; kAk2].
We draw W = (A + ρ∣∣w*kId)-ξθ, where θ 〜N(0; Id) and log? ξ is uniform on [-1, l]. We
set w* = kkww∣kW and b = -(A + ρ∣∣w* kId)w*. The procedure makes w* the global minimizer of
problem instance (A, b, ρ). Patterns shown on this figure exhibit for other random problem instances
as well.
41