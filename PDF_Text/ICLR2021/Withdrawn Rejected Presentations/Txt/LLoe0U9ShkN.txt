Under review as a conference paper at ICLR 2021
Global inducing point variational posteriors
for Bayesian neural networks and deep Gaus-
SIAN PROCESSES
Anonymous authors
Paper under double-blind review
Ab stract
We derive the optimal approximate posterior over the top-layer weights in a
Bayesian neural network for regression, and show that it exhibits strong depen-
dencies on the lower-layer weights. We adapt this result to develop a correlated
approximate posterior over the weights at all layers in a Bayesian neural network.
We extend this approach to deep Gaussian processes, unifying inference in the two
model classes. Our approximate posterior uses learned “global” inducing points,
which are defined only at the input layer and propagated through the network to ob-
tain inducing inputs at subsequent layers. By contrast, standard, “local”, inducing
point methods from the deep Gaussian process literature optimise a separate set
of inducing inputs at every layer, and thus do not model correlations across layers.
Our method gives state-of-the-art performance for a variational Bayesian method,
without data augmentation or tempering, on CIFAR-10 of 86.7%.
1	Introduction
Deep models, formed by stacking together many simple layers, give rise to extremely powerful
machine learning algorithms, from deep neural networks (DNNs) to deep Gaussian processes (DGPs)
(Damianou & Lawrence, 2013). One approach to reason about uncertainty in these models is to use
variational inference (VI) (Jordan et al., 1999). VI in Bayesian neural networks (BNNs) requires the
user to specify a family of approximate posteriors over the weights, with the classical approach being
independent Gaussian distributions over each individual weight (Hinton & Van Camp, 1993; Graves,
2011; Blundell et al., 2015). Later work has considered more complex approximate posteriors, for
instance using a Matrix-Normal distribution as the approximate posterior for a full weight-matrix
(Louizos & Welling, 2016; Ritter et al., 2018). By contrast, DGPs use an approximate posterior
defined over functions — the standard approach is to specify the inputs and outputs at a finite number
of “inducing” points (Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017).
Critically, these classical BNN and DGP approaches define approximate posteriors over functions that
are independent across layers. An approximate posterior that factorises across layers is problematic,
because what matters for a deep model is the overall input-output transformation for the full model,
not the input-output transformation for individual layers. This raises the question of what family
of approximate posteriors should be used to capture correlations across layers. One approach for
BNNs would be to introduce a flexible “hypernetwork”, used to generate the weights (Krueger
et al., 2017; Pawlowski et al., 2017). However, this approach is likely to be suboptimal as it
does not sufficiently exploit the rich structure in the underlying neural network. For guidance, we
consider the optimal approximate posterior over the top-layer units in a deep network for regression.
Remarkably, the optimal approximate posterior for the last-layer weights given the earlier weights
can be obtained in closed form without choosing a restrictive family of distributions. In particular,
the optimal approximate posterior is given by propagating the training inputs through lower layers to
compute the top-layer representation, then using Bayesian linear regression to map from the top-layer
representation to the outputs.
Inspired by this result, we use Bayesian linear regression to define a generic family of approximate
posteriors for BNNs. In particular, we introduce learned “pseudo-data” at every layer, and compute
the posterior over the weights by performing linear regression from the inputs (propagated from
1
Under review as a conference paper at ICLR 2021
lower layers) onto the pseudo-data. We reduce the burden of working with many training inputs by
summarising the posterior using a small number of “inducing” points. We find that these approximate
posteriors give excellent performance in the non-tempered, no-data-augmentation regime, with
performance on datasets such as CIFAR-10 reaching 86.7%, comparable to SGMCMC (Wenzel et al.,
2020). Our approach can be extended to DGPs, and we explore connections to the inducing point GP
literature, showing that inference in the two classes of models can be unified.
2	Methods
We consider neural networks with lower-layer weights {W'}L=ι, W' ∈ RN'-1×N', and top-layer
weights, Wl+i ∈ RNL ×NL+1, where the activity, F', at layer ' is given by,
Fi = XWι,	F' = φ (F'-ι) W' for ' ∈{2,...,L} ,	(1)
where φ(∙) is an elementwise nonlinearity. The outputs, Y ∈ RP×NL+1, depend on the top-level
activity, FL , and the output weights, WL+1, according to a likelihood, P (Y|WL+1, FL ). In the
following derivations, we will focus on ` > 1; corresponding expressions for the input layer can be
obtained by replacing φ(F0) with the inputs, X ∈ RP×N0. The prior over weights is independent
across layers and output units (see Sec. 2.3 for the form of S'),
P (W') = QN= 1P (wλ),	P (wλ) = N (w!∣0, NLTS'),	⑵
where wλ' is a column of W', i.e. all the input weights to unit λ in layer `. To fit the parameters of
the approximate posterior, Q {W}'L=+11 , we maximise the evidence lower bound (ELBO),
L = Eq({w}L=1i) [logP (Y,{W}L⅛1X) - IogQ ({W}jL+ι1)] .	(3)
To build intuition about how to parameterise Q {W}'L=+11 , we consider the optimal
Q (Wl+i∣{W'}L=i) for any given Q ({W'}L=J. We begin by simplifying the ELBO by in-
corporating terms that do not depend on WL+1 into a constant, c,
L = Eq({W'}"∣I) [logP (Y, Wl+i I X,{W'}Nι) — IogQ (Wl+i∣{W'}'=i) + c] .	(4)
Rearranging these terms, we find that all WL+1 dependence can be written in terms of the KL
divergence between the approximate posterior of interest and the true posterior,
L = EQ({W'}L=1)[ lOgP (YI X, {W'}'=i)
— DKL (Q (Wl+i∣{W'}L=i) II P (Wl+iI Y, X, {W'}L=ι)) + c]. (5)
Thus, the optimal approximate posterior is,
Q (Wl+iI{W'}L=i) = P (Wl+iI Y, X, {W'}L=I) (X P (Y]Wl+i, FL) P (Wl+i) , (6)
and where the final proportionality comes by applying Bayes theorem and exploiting the model’s
conditional independencies. For regression, the likelihood is Gaussian,
P (YIWl+1 , FL) = QNL+1 N (yλ; φ (FL) wL+1, Λ-+i),	⑺
where yλ is the value of a single output channel for all training inputs, and ΛL+1 is a precision
matrix. Thus, the posterior is given in closed form by Bayesian linear regression (Rasmussen &
Williams, 2006).
2.1	Defining the full approximate posterior with global inducing points and
pseudo-data
We adapt the optimal scheme above to give a scalable approximate posterior over the weights at all
layers. To avoid propagating all training inputs through the network, which is intractable for large
datasets, we instead propagate M global inducing locations, U0 ,
U1 = U0W1,	U' = φ (U'-1) W'	for ` = 2, . . . , L + 1.	(8)
2
Under review as a conference paper at ICLR 2021
logP (YJX, {W}L=n+XIlOgQTW⅛
(12)
Next, the optimal posterior requires outputs, Y . However, no outputs are available at inducing
locations for the output layer, let alone for intermediate layers. We thus introduce learned variational
parameters to mimic the form of the optimal posterior. In particular, we use the product of the prior
over weights and a “pseudo-likelihood”, N (vɪ; uɪ, Λ-1) ,representing noisy “pseudo-observations”
of the outputs of the linear layer at the inducing locations, uɪ = φ (U'-ι) wɪ. Substituting uɪ into
the pseudo-likelihood the approximate posterior becomes,
Q (W'∣{W'o}'-1ι) « QN= ιN (vλ； Φ (U'-1) wλ, Λ-1) P (wλ),
Q (W'∣{W'o}'-1ι) = QN= ιN (wλ∣∑WΦ (U'-1)T Λ'vλ, ∑w),
∑w =(Nf-iS-1 + φ (U'-ι)T Λ'φ (U'-ι))T.	(9)
where v： and Λ' are variational parameters. Therefore, our full approximate posterior factorises as
L+1
Q ({W'}L+ι* 1) = Y Q (W'∣{W'o }'-1).	(10)
'=1
Thus, the full ELBO can be written,
L = Eq({w}L=+1i) [logP(Y|X, {W}L+11) +logP ({W'}L+ι1) - IogQ ({W'}L+ι1)]	(11)
=eq({w}L=+i1)
The forms of the ELBO and approximate posterior suggest a sequential procedure to evaluate and
subsequently optimise it: we alternate between sampling the weights using Eq. (9) and propagating
the data and inducing locations using Eq. (8) (see Alg. 1). In summary, the parameters of the
approximate posterior are the global inducing inputs, U0 , and the pseudo-data and precisions at
all layers, {V', Λ'}L+ι1. As each factor Q (W'∣{W0};二)is a Gaussian, these parameters
can be optimised using standard reparameterised variational inference (Kingma & Welling, 2013;
Rezende et al., 2014) in combination with the Adam optimiser (Kingma & Ba, 2014) (Appendix A).
Importantly, by placing inducing inputs on the training data (i.e. Uo = X), and setting vʌ = y： this
approximate posterior matches the optimal top-layer posterior (Eq. 6). Finally, we note that while
this posterior is conditionally Gaussian, the full posterior over all {W'}L+ι1 is non-Gaussian, and is
thus potentially more flexible than a full-covariance Gaussian over all weights.
Algorithm 1: Global inducing points for neural networks
Parameters: inducing inputs, Uo, inducing outputs and precisions, {V', Λ'}j=ι, at all layers.
Neural network inputs: (e.g. MNIST digits) F0
Neural network outputs: (e.g. classification logits) FL+1
L + ∞ J 0
for ` ∈ {1, . . . , L + 1} do
Compute the mean and covariance over the weights at this layer
∑w =(N'-iS-1 + φ (U'-ι)τ Λ'φ (U'-i)) -1
m` = ∑w Φ(U'-ι)τ Λ'V'
Sample the weights and compute the ELBO
W' 〜N (M', ∑W) = Q (W'∣{W'o}'-1ι)
LJL + logP(W') - log N (W'∣M', ∑W)
Propagate the inputs and inducing points using the sampled weights,
U' = φ (U'-ι) W'
_ f` = φ (F'-ι) W'
LJL + logP(Y∣FL+ι)
3
Under review as a conference paper at ICLR 2021
2.2	Efficient convolutional Bayesian linear regression
The previous sections were valid for a fully connected network. The extension to convolutional
networks is straightforward in principle: we transform the convolution into a matrix multiplica-
tion by treating each patch as a separate input feature-vector, flattening the spatial and channel
dimensions together into a single vector. Thus, the feature-vectors have length in_channels
X kernel_Width X kernel_height, and the matrix U' contains Patches_per_image
X minibatch patches. Likewise, We now have inducing outputs, v：, at each location in all the
inducing images, so this again has length patches_per_image X minibatch. After explicitly
extracting the patches, we can straightforwardly apply standard Bayesian linear regression.
However, explicitly extracting image patches is very memory intensive in a DNN. If we consider a
standard convolution with a 3 X 3 convolutional kernel, then there is a 3 X 3 patch centred at each
pixel in the input image, meaning a factor of 9 increase in memory consumption. Instead, we note that
computing the matrices required for linear regression, φ (U'-ι)T Λ'φ (U'-ι) and φ (U')T Λ'V',
does not require explicit extraction of image-patches. Instead, these matrices can be computed by
taking the autocorrelation of the image/feature map, i.e. a convolution operation where we treat the
image/feature map, as both the inputs and the weights (Appendix B for details).
2.3	Priors
We consider four priors in this work, which we refer to using the class names in the BNN library
published with this paper. We are careful to ensure that all parameters in the model have a prior and
approximate posterior, which is necessary to ensure that ELBOs are comparable across models.
First, we consider a Gaussian prior with fixed scale, NealPrior, so named because it is necessary to
obtain meaningful results when considering infinite networks (Neal, 1996),
S' = I,	(13)
though it bears strong similarities to the “He” initialisation (He et al., 2015). NealPrior is defined
so as to ensure that the activations retain a sensible scale as they propagate through the network.
We compare this to the standard N (0, 1) (StandardPrior), which causes the activations to increase
exponentially as they propagate through network layers (see Eq. 2):
S' = N'-1 I.	(14)
Next, we consider ScalePrior, which defines a prior and approximate posterior over the scale,
S' =	s' I	P	(s`)	= Gamma (s`; 2, 2)	Q	(s`)	= Gamma (s`; 2	+ ɑ`, 2 + β')	(15)
where here we parameterise the Gamma distribution with the shape and rate parameters, and α'
and β' are non-negative learned parameters of the approximate posterior over s' . Finally, we
consider SpatialIWPrior, which allows for spatial correlations in the weights. In particular, we take
the covariance to be the Kronecker product of an identity matrix over channel dimensions, and a
Wishart-distributed matrix, L'-1, over the spatial dimensions,
S' = I 0 L—1
P (L') = InverseWishart (L'; (N'-1 + 1) I	, N'-1 + 1)
Q (L') = InverseWishart (L'; (N'-1 +1)I+Ψ,N'-1 +1+ν)	(16)
where the non-negative real number, ν, and the positive definite matrix, Ψ, are learned parameters of
the approximate posterior (see Appendix C).
3	Extension to DGPs
It is a remarkable but underappreciated fact that BNNs are special cases of DGPs, with a particular
choice of kernel (Louizos & Welling, 2016; Aitchison, 2019). Combining Eqs. (1) and (2),
P(F'∣F'-ι) = QN= ιN (fλ∣0, K (F'-ι)), K (F'-ι) = Nφ (F'-ι) S'φ (F'-ι)T . (17)
4
Under review as a conference paper at ICLR 2021
factorised
Figure 1: Predictive distributions on the toy dataset. Shaded regions represent one standard deviation.
Hamiltonian
Monte Carlo
Here, we generalise our approximate posterior to the DGP case and link to the DGP literature. In a
DGP there are no weights; instead We work directly with inducing outputs {U'}L=ι1,
P(U'∣U'-ι) = QN= ιN (uλ∣0, K (U'-ι)),	(18)
Note that here, we take the “global” inducing approach of using the inducing outputs from the
previous layer, U'-ι as the inducing inputs for the next layer. In this case, we need only learn the
original inducing inputs, U0 . This contrasts with the standard “local” inducing formulation, (as in
Salimbeni & Deisenroth, 2017), which learns separate inducing inputs at every layer, Z'-ι, giving
P (U'∣Z'-ι) = QN= ιN (uλ∣0, K (Z'-ι)).
As usual in DGPs (Salimbeni & Deisenroth, 2017), the approximate posterior over u` induces an
approximate posterior on f` through the prior correlations. However, it is important to remember
that underneath the tractable distributions in Eqs. (17) and (18), there is an infinite dimensional
GP-distributed function, f`, such that f` = F'(F'-ι). Standard local inducing point methods
specify a factorised approximate posterior over F' by specifying the function,s inducing outputs,
U' = F' (Z'-ι), at a finite number of inducing input locations, Z'-ι. Importantly, the approximate
posterior over a function, F', depends only on Z'-1, and U'. Thus, standard, local inducing, DGP
approaches (e.g. Salimbeni & Deisenroth, 2017), give a layerwise-independent approximate posterior
over {F'}'L=+11, as they treat the inducing inputs, {Z'-1}'L=+11, as fixed, learned parameters and use a
layerwise-independent approximate posterior over {U'}'L=+11 (Appendix F).
Next, we need to choose the approximate posterior on {U'}'L=+11. However, if our goal is to intro-
duce dependence across layers, it seems inappropriate to use the standard layerwise-independent
approximate posterior over {U'}'L=+11. Indeed, in Appendix F, we show that such a posterior implies
functions in non-adjacent layers (e.g. F' and F'+2) are marginally independent, even with global
inducing points.
To obtain more appropriate approximate posteriors, we derive the optimal top-layer posterior for
DGPs, which involves GP regression from activations propagated from lower layers onto the output
data (Appendix E). Inspired by the form of the optimal posterior we again define an approximate
posterior by taking the product of the prior and a “pseudo-likelihood”,
Q (U'∣U'-ι) « QN= 1 Q (uλ) N (vλ∣uλ, A-1)
Q(U'∣U'-ι) = QN= ιN (uλ∣∑UA'vλ, ∑U),	∑U = (KT (U'-ι) + A')-1,	(19)
where vλ' and A'-1 are learned parameters, and in our global inducing method, the inducing inputs,
U'-1, are propagated from lower layers (Eq. 18). Importantly, setting the inducing inputs to the
training data and vλL+1 = yλ , the approximate posterior captures the optimal top-layer posterior for
regression (Appendix E). Under this approximate posterior, dependencies in U' naturally arise across
all layers, and hence there are dependencies between functions F' at all layers (Appendix F).
5
Under review as a conference paper at ICLR 2021
Figure 2: ELBO for different approximate posteriors as we change network depth/width on a dataset
generated using a linear Gaussian model. The rand → gi line lies behind the global inducing line in
width = 50 and width = 250.
In summary, we propose an approximate posterior over inducing outputs that takes the form
L+1
Q ({U'}L+ι1) = ∏ Q (U'∣U'-ι).	(20)
l=1
As before, the parameters of this approximate posterior are the global inducing inputs, U0, and the
pseudo-data and precisions at all layers, { v`, Λ'}L+ι1. Our ELBO, which We derive in D, takes the
form
L = eq({F',u'}L=I1∣χ,u0) logP(YIFL+1) + X log Q (u[u∖l) .	(21)
We provide a full description of our method as applied to DGPs in Appendix D. In concluding our
discussion of our proposed method, we note that we provide an analysis of the asymptotic complexity
in Appendix L.
4	Results
We describe our experiments and results to assess the performance of global inducing points (‘gi’)
against local inducing points (‘li’) and the fully factorised (‘fac’) approximation family. We addi-
tionally consider models where we use one method up to the last layer and another for the last layer,
which may have computational advantages; we denote such models ‘method1 → method2’.
4.1	Uncertainty in 1D regression
We demonstrate the use of local and global inducing point methods in a toy 1-D regression problem,
comparing it with fully factorised VI and Hamiltonian Monte Carlo (HMC; (Neal et al., 2011)).
Following Herndndez-Lobato & Adams (2015), we generate 40 input-output pairs (x, y) with the
inputs x sampled i.i.d. from U ([-4, -2] ∪ [2, 4]) and the outputs generated by y = x3 + , where
e 〜N(0, 32). We then normalised the inputs and outputs. Note that we have introduced a 'gap’
in the inputs, following recent work (Foong et al., 2019b; Yao et al., 2019; Foong et al., 2019a)
that identifies the ability to express ‘in-between’ uncertainty as an important quality of approximate
inference algorithms. We evaluated the inference algorithms using fully-connected BNNs with 2
hidden layers of 50 ReLU hidden units, using the NealPrior. For the inducing point methods, we used
100 inducing points per layer.
The predictive distributions for the toy experiment can be seen in Fig. 1. We observe that of the
variational methods, the global inducing method produces predictive distributions closest to HMC,
with good uncertainty in the gap. Meanwhile, factorised and local inducing fit the training data, but
do not produce reasonable error bars: this demonstrates an important limitation of methods lacking
correlation structure between layers.
6
Under review as a conference paper at ICLR 2021
DGP-2
DGP-5
DGP-2
DGP-5
BNN
Scale
BNN
Scale
BNN
N (0,1)
BNN
N (0,1)
boston
N=506,D=13
-0.29
—∙-
-2.76
-2.30
-1.55
factorised
global inducing
Figure 3: Average test log likelihoods for BNNs on the UCI datasets (in nats). Error bars represent
one standard error. Shading represents different priors. We connect the factorised models with the fac
→ gi models with a thin grey line as an aid for easier comparison. Further to the right is better.
yacht
N=308,D=6
-1.05
-1.44
-0.20
DSVI
We provide additional experiments looking at the effect of the number of inducing points in Ap-
pendix H, and experiments looking at compositional uncertainty (Ustyuzhaninov et al., 2020) in both
BNNs and DGPs for 1D regression in Appendix I.
4.2	Depth-dependence in deep linear networks
The lack of correlations between layers might be expected to become more problematic in deeper
networks. To isolate the effect of depth on different approximate posteriors, we considered deep linear
networks trained on data generated from a toy linear model: 5 input features were mapped to 1 output
feature, where the 1000 training and 100 test inputs are drawn IID from a standard Gaussian, and the
true outputs are drawn using a weight-vector drawn IID from a Gaussian with variance 1/5, and with
noise variance of 0.1. We could evaluate the model evidence under the true data generating process
which forms an upper bound (in expectation) on the model evidence and ELBO for all models.
We found that the ELBO for methods that factorise across layers — factorised and local inducing —
drops rapidly as networks get deeper and wider (Fig. 2). This is undesirable behaviour, as we know
that wide, deep networks are necessary for good performance on difficult machine learning tasks. In
contrast, we found that methods with global inducing points at the last layer decay much more slowly
with depth, and perform better as networks get wider. Remarkably, global-inducing points gave good
performance even with lower-layer weights drawn at random from the prior, which is not possible for
any method that factorises across layers. We believe that fac → gi performed poorly at width = 250
due to optimisation issues as rand → gi performs better yet is a special case of fac → gi.
4.3	Regression benchmark: UCI
We benchmark our methods on the UCI datasets in Hemgndez-Lobato & Adams (2015), popular
benchmark regression datasets for BNNs and DGPs. Following the standard approach (Gal &
Ghahramani, 2015), each dataset uses 20 train-test ‘splits’ (except for protein with 5 splits) and the
inputs and outputs are normalised to have zero mean and unit standard deviation. We focus on the
five smallest datasets, as we expect Bayesian methods to be most relevant in small-data settings (see
App. J andM for all datasets). We consider two-layer fully-connected ReLU networks, using fully
factorised and global inducing approximating families, as well as two- and five-layer DGPs with
7
Under review as a conference paper at ICLR 2021
Table 1: CIFAR-10 classification accuracy. The first block shows our results using SpatialIWPrior,
with ScalePrior in brackets. The next block shows comparable past results, from GPs and BNNs.
The final block show non-comparable (sampling-based) methods. Dashes indicate that the figures
were either not reported, are not applicable. The time is reported per epoch with ScalePrior and for
MNIST, rather than CIFAR-10 because of a known performance bug in the convolutions required
in Sec. 2.2 with 32 × 32 (and above) images https://github.com/pytorch/pytorch/
issues/35603.
	test log like.	accuracy (%)	ELBO	time
factorised	-0.58 (-0.66)	80.27 (77.65)	-1.06 (-1.12)	19s
local inducing	-0.62 (-0.60)	78.96 (79.46)	-0.84 (-0.88)	33 s
fac → gi	-0.49 (-0.56)	83.33 (81.72)	-0.91 (-0.96)	25 s
global inducing	-0.40 (-0.43)	86.70 (85.73)	-0.68 (-0.75)	65 s
Shi et al. (2019)	—	80.30%	—	
Li et al. (2019)	—	81.40%	—	
Shridhar et al. (2019)	—	73%	—	
Wenzel et al. (2020)	-0.35	88.50%	—	
doubly-stochastic variational inference (DSVI) (Salimbeni & Deisenroth, 2017) and global inducing.
For the BNNs, we consider the standard N(0, 1) prior and ScalePrior.
We display ELBOs and average test log likelihoods for the un-normalised data in Fig. 3, where
the dots and error bars represent the means and standard errors over the test splits, respectively.
We observe that global inducing obtains better ELBOs than factorised and DSVI in every case,
indicating that it does indeed approximate the true posterior better (since the ELBO is the marginal
likelihood minus the KL to the posterior). While this is the case for the ELBOs, this does not always
translate to a better test log likelihood due to model misspecification, as we see that occasionally
DSVI outperforms global inducing by a very small margin. The very poor results for factorised on
ScalePrior indicate that it has difficulty learning useful prior hyperparameters for prediction, which is
due to the looseness of its bound to the marginal likelihood. We provide experimental details, as well
as additional results with additional architectures, priors, datasets, and RMSEs, in Appendices J and
M, for BNNs and DGPs, respectively.
4.4	Convolutional benchmark: CIFAR- 1 0
For CIFAR-10, we considered a ResNet-inspired model consisting of conv2d-relu-block-avgpool2-
block-avgpool2-block-avgpool-linear, where the ResNet blocks consisted of a shortcut connection in
parallel with conv2d-relu-conv2d-relu, using 32 channels in all layers. In all our experiments, we
used no data augmentation and 500 inducing points. Our training scheme (see App. O) ensured that
our results did not reflect a ‘cold posterior’ (Wenzel et al., 2020). Our results are shown in Table 1.
We achieved remarkable performance of 86.7% predictive accuracy, with global inducing points used
for all layers, and with a spatial inverse Wishart prior on the weights. These results compare very
favourably with comparable Bayesian approaches, i.e. those without data augmentation or posterior
sharpening: past work with deep GPs obtained 80.3% (Shi et al., 2019), and work using infinite-width
neural networks to define a GP obtained 81.4% accuracy (Li et al., 2019). Remarkably, with only 500
inducing points we are approaching the accuracy of sampling-based methods (Wenzel et al., 2020),
which are in principle able to more closely approximate the true posterior. Furthermore, we see that
global inducing performs the best in terms of ELBO (per datapoint) by a wide margin, demonstrating
that it gets far closer to the true posterior than the other methods. We provide additional results on
uncertainty calibration in Appendix K.
5	Related work
Louizos & Welling (2016) attempted to use pseudo-data along with matrix variate Gaussians to
form an approximate posterior for BNNs; however, they restricted their analysis to BNNs, and it is
not clear how their method can be applied to DGPs. Their approach factorises across layers, thus
8
Under review as a conference paper at ICLR 2021
missing the important layerwise correlations that we obtain. Moreover, they encountered an important
limitation: the BNN prior implies that u` is low-rank and it is difficult to design an approximate
posterior capturing this constraint. As such, they were forced to use M < n` inducing points, which
is particularly problematic in the convolutional, global-inducing case where there are many patches
(points) in each inducing image input.
Note that some work on BNNs reports better performance on datasets such as CIFAR-10. However,
to the best of our knowledge, no variational Bayesian method outperforms ours without modifying
the BNN model or some form of posterior tempering (Wenzel et al., 2020), where the KL term in the
ELBO is down-weighted relative to the likelihood (Zhang et al., 2017; Bae et al., 2018; Osawa et al.,
2019; Ashukha et al., 2020), which often increases the test accuracy. However, tempering clouds the
Bayesian perspective, as the KL to the posterior is no longer minimised and the resulting objective
is no longer a lower bound on the marginal likelihood. By contrast, we use the untempered ELBO,
thereby retaining the full Bayesian perspective. Dusenberry et al. (2020) report better performance on
CIFAR-10 without tempering, but only perform variational inference over a rank-1 perturbation to the
weights, and maximise over all the other parameters. Our approach retains a full-rank parameterisation
of the weight matrices.
Ustyuzhaninov et al. (2020) attempted to introduce dependence across layers in a deep GP by coupling
inducing inputs to pseudo-outputs, which they term “inducing points as inducing locations”. However,
as described above, the choice of approximate posterior over u` is also critical. They used the
standard approximate posterior that is independent across layers, meaning that while functions in
adjacent layers were marginally dependent, the functions for non-adjacent layers were independent
(Appendix F). By contrast, our approximate posteriors have marginal dependencies across u` and
functions at all layers, and are capable of capturing the optimal top-layer posterior.
6	Conclusions
We derived optimal top-layer variational approximate posteriors for BNNs and deep GPs, and used
them to develop generic, scalable approximate posteriors. These posteriors make use of global
inducing points, which are learned only at the bottom layer and are propagated through the network.
This leads to extremely flexible posteriors, which even allow the lower-layer weights to be drawn from
the prior. We showed that these global inducing variational posteriors lead to improved performance
with better ELBOs, and state-of-the-art performance for variational BNNs on CIFAR-10.
References
Laurence Aitchison. Why bigger is not always better: on finite and infinite neural networks. arXiv
preprint arXiv:1910.08013, 2019.
Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain
uncertainty estimation and ensembling in deep learning. arXiv preprint arXiv:2002.06470, 2020.
Juhan Bae, Guodong Zhang, and Roger Grosse. Eigenvalue corrected noisy natural gradient. arXiv
preprint arXiv:1811.12565, 2018.
Maurice Stevenson Bartlett. On the theory of statistical regression. Proceedings of the Royal Society
OfEdinburgh, 53:260-283,1933.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
David R Burt, Carl Edward Rasmussen, and Mark van der Wilk. Convergence of sparse variational
inference in gaussian processes regression. Journal of Machine Learning Research, 21:1-63, 2020.
Djalil	ChafaL	Bartlett decomposition and other factorizations, Oct
2015.	URL http://djalil.chafai.net/blog/2015/10/20/
bartlett- decomposition- and- other- factorizations/.
Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In Artificial Intelligence and
Statistics, pp. 207-215, 2013.
9
Under review as a conference paper at ICLR 2021
Michael W Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-an Ma, Jasper Snoek, Katherine Heller,
Balaji Lakshminarayanan, and Dustin Tran. Efficient and scalable bayesian neural nets with rank-1
factors. arXiv preprint arXiv:2005.07186, 2020.
Vincent Dutordoir, Mark van der Wilk, Artem Artemev, Marcin Tomczak, and James Hensman. Trans-
lation insensitivity for deep convolutional Gaussian processes. arXiv preprint arXiv:1902.05888,
2019.
Sebastian Farquhar, Lewis Smith, and Yarin Gal. Try depth instead of weight correlations: Mean-field
is a less restrictive assumption for deeper networks. arXiv preprint arXiv:2002.03704, 2020.
Andrew YK Foong, David R Burt, Yingzhen Li, and Richard E Turner. Pathologies of factorised Gaus-
sian and MC dropout posteriors in Bayesian neural networks. arXiv preprint arXiv:1909.00719,
2019a.
Andrew YK Foong, Yingzhen Li, Jose Miguel Hernandez-Lobato, and Richard E Turner. ,in-between'
uncertainty in Bayesian neural networks. arXiv preprint arXiv:1906.11537, 2019b.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. arXiv preprint arXiv:1506.02142, 2015.
Alex Graves. Practical variational inference for neural networks. In Advances in neural information
processing systems,pp. 2348-2356, 2011.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1321-1330. JMLR. org, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on ImageNet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Jose Miguel Hernandez-Lobato and Ryan Adams. Probabilistic backpropagation for scalable learning
of Bayesian neural networks. In International Conference on Machine Learning, pp. 1861-1869,
2015.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the sixth annual conference on Computational
learning theory, pp. 5-13, 1993.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization
trick. In Advances in neural information processing systems, pp. 2575-2583, 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009.
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron
Courville. Bayesian hypernetworks. arXiv preprint arXiv:1710.04759, 2017.
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon S Du, Wei Hu, Ruslan Salakhutdinov, and Sanjeev
Arora. Enhanced convolutional neural tangent kernels. arXiv preprint arXiv:1911.00809, 2019.
Christos Louizos and Max Welling. Structured and efficient variational deep learning with matrix
Gaussian posteriors. In International Conference on Machine Learning, pp. 1708-1716, 2016.
10
Under review as a conference paper at ICLR 2021
Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated prob-
abilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence,
2015.
Radford M Neal. Priors for infinite networks. In Bayesian Learningfor Neural Networks, pp. 29-53.
Springer, 1996.
Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov chain Monte
Carlo, 2(11):2, 2011.
Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh Jain, Runa Eschenhagen,
Richard E Turner, and Rio Yokota. Practical deep learning with Bayesian principles. In Advances
in Neural Information Processing Systems, pp. 4289-4301, 2019.
Nick Pawlowski, Andrew Brock, Matthew CH Lee, Martin Rajchl, and Ben Glocker. Implicit weight
uncertainty in neural networks. arXiv preprint arXiv:1711.01297, 2017.
Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan
Kaufmann, 1988.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning.
ISBN-13 978-0-262-18253-9, 2006.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable Laplace approximation for neural
networks. In 6th International Conference on Learning Representations, ICLR 2018-Conference
Track Proceedings, volume 6. International Conference on Representation Learning, 2018.
Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep Gaussian
processes. In Advances in Neural Information Processing Systems, pp. 4588-4599, 2017.
Jiaxin Shi, Michalis K Titsias, and Andriy Mnih. Sparse orthogonal variational inference for Gaussian
processes. arXiv preprint arXiv:1910.10596, 2019.
Kumar Shridhar, Felix Laumann, and Marcus Liwicki. A comprehensive guide to Bayesian convolu-
tional neural network with variational inference. arXiv preprint arXiv:1901.02731, 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Brian Trippe and Richard Turner. Overpruning in variational bayesian neural networks. arXiv preprint
arXiv:1801.06230, 2018.
Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser, Erik Bodin, Neill DF Campbell, and Carl Hen-
rik Ek. Compositional uncertainty in deep Gaussian processes. UAI, 2020.
Florian Wenzel, Kevin Roth, Bastiaan S Veeling, Jakub Swiatkowski, Linh Tran, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. How good is the Bayes
posterior in deep neural networks really? arXiv preprint arXiv:2002.02405, 2020.
Jiayu Yao, Weiwei Pan, Soumya Ghosh, and Finale Doshi-Velez. Quality of uncertainty quantification
for Bayesian neural network inference. arXiv preprint arXiv:1906.09686, 2019.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as
variational inference. arXiv preprint arXiv:1712.02390, 2017.
11
Under review as a conference paper at ICLR 2021
A Reparameterised variational inference
In variational inference, the ELBO objective takes the form,
P (w)
SEQφ(w)卜OgP(Y1X，w) + logQφ⅛
(22)
where w is a vector containing all of the elements of the weight matrices in the full network,
{W'}L+11, and φ = (Uo, {V', Λ'}L+1) are the parameters of the approximate posterior. This
objective is difficult to differentiate wrt φ, because φ parameterises the distribution over which the
expectation is taken. Following Kingma & Welling (2013) and Rezende et al. (2014), we sample
from a simple, fixed distribution (e.g. IID Gaussian), and transform them to give samples from Q (w),
W(0 Φ)〜Qφ (W).
Thus, the ELBO can be written,
L(Φ) = EJlOg P (Y|X, w(e; φ)) + log ：(* 吗、
Qφ (W(; φ))
(23)
(24)
As the distribution over which the expectation is taken is now independent of φ, we can form unbiased
estimates of the gradient of L(φ) by drawing one or a few samples of .
B Efficient convolutional linear regression
Working in one dimension for simplicity, the standard form for a convolution in deep learning is
Yiu,c
=	Xi(u+δ)c0Wδc0,c.
c0 δ
(25)
where X is the input image/feature-map, Y is the output feature-map, W is the convolutional weights,
i indexes images, c and c0 index channels, u indexes the location within the image, and δ indexes the
location within the convolutional patch. Later, we will swap the identity of the “patch location” and
the “image location” and to facilitate this, we define them both to be centred on zero,
U ∈ {-(W - 1)/2,..., (W - 1)/2}	δ ∈ {-(S - 1)/2,..., (S - 1)/2}	(26)
where W is the size of an image and S is the size of a patch, such that, for example for a size 3 kernel,
δ ∈ {-1, 0, 1}.
To use the expressions for the fully-connected case, we can form a new input, X0 by cutting out each
image patch,
Xi0u,δc = Xi(u+δ)c,	(27)
leading to
Xi0u,δc0Wδc0,c.
δc0
(28)
Note that we have used commas to group pairs of indices (iu and δc0) that may be combined into
a single index (e.g. using a reshape operation). Indeed, combining i and u into a single index and
combining δ and c0 into a single index, this expression can be viewed as standard matrix multiplication,
Y = X0W.	(29)
This means that we can directly apply the approximate posterior we derived for the fully-connected
case in Eq. (9) to the convolutional case. To allow for this, we take
X0 = Λfφ (U'-ι),	Y = a1/2V'.	(30)
While we can explicitly compute X0 by extracting image patches, this imposes a very large memory
cost (a factor of 9 for a 3 × 3 kernel, with stride 1, because there are roughly as many patches as pixels
in the image, and a 3 × 3 patch requires 9 times the storage of a pixel. To implement convolutional
linear regression with a more manageable memory cost, we instead compute the matrices required
12
Under review as a conference paper at ICLR 2021
for linear regression directly as convolutions of the input feature-maps, X with themselves, and as
convolutions of the X with the output feature maps, Y , which we describe here.
For linear regression (Eq. 9), we first need to compute,
φ( (UeT)T A'v')δc,c' =(XTY) δcQ = X XkCYiUC
Rewriting this in terms of X (i.e. without explicitly cutting out image patches), we obtain,
X0TYδcc0=XXi(U+δ)cYiU,c0.
, iU
(31)
(32)
This can be directly viewed as the convolution of X and Y , where we treat Y as the “convolutional
weights”, u as the location within the now very large (size W) “convolutional patch”, and δ as the
location in the resulting output. Once we realise that the computation is a spatial convolution, it is
possible to fit it into standard convolution functions provided by deep-learning frameworks (albeit
with some rearrangement of the tensors).
Next, we need to compute,
Φ (U'-ι)T Λeφ (U'-)) = (MT X0) J K= X X0u,δc X0u° c∙
,	iU
(33)
Again, rewriting this in terms of X (i.e. without explicitly cutting out image patches), we obtain,
X0TX0 δc,δ0c0 = Xiu Xi(u+δ)cXi(u+δ0)c0.
(34)
To treat this as a convolution, we first need exact translational invariance, which can be achieved
by using circular boundary conditions. Note that circular boundary conditions are not typically
used in neural networks for images, and we therefore only use circular boundary conditions to
define the approximate posterior over weights. The variational framework does not restrict us
to also using circular boundary conditions within our feedforward network, and as such, we use
standard zero-padding. With exact translational invariance, we can write this expression directly as a
convolution,
X T X0	0 0 = X XiucXi(u+δ0 -δ)c0
,	iu
(35)
where
(δ0-δ)∈{-(S-1),...,(S-1)}	(36)
i.e. for a size 3 kernel, (δ0 - δ) ∈ {-2, -1, 0, 1, 2}, where we treat Xiuc as the “convolutional
weights”, u as the location within the “convolutional patch”, and δ0 - δ as the location in the resulting
output “feature-map”.
Finally, note that this form offers considerable benefits in terms of memory consumption. In particular,
the output matrices are usually quite small — the number of channels is typically 32 or 64, and the
number of locations within a patch is typically 9, giving a very manageable total size that is typically
smaller than 1000 × 1000.
C Wishart distributions with real-valued degrees of freedom
The classical description of the Wishart distribution,
Σ 〜Wishart (LV),	(37)
where Σ is a P × P matrix, states that P ≥ ν is an integer and we can generate Σ by taking the
product of matrices, X ∈ Rν×P, generated IID from a standard Gaussian,
Σ = XTX,	Xij 〜N (0,1).	(38)
However, for the purposes of defining learnable approximate posteriors, we need to be able to sample
and evaluate the probability density when ν is positive real.
13
Under review as a conference paper at ICLR 2021
To do this, consider the alternative, much more efficient means of sampling from a Wishart distribution,
using the Bartlett decomposition (Bartlett, 1933). The Bartlett decomposition gives the probability
density for the Cholesky of a Wishart sample. In particular,
P (Tjj)= Gamma (Tj; ν4+1,1),
P (Tj<k ) = N (Tjk ; 0, 1) .
(39)
(40)
(41)
Here, Tjj is usually considered to be sampled from a χ2 distribution, but we have generalised this
slightly using the equivalent Gamma distribution to allow for real-valued ν. Following Chafai (2015),
We need to change variables to Tjj rather than Tj2j,
P(Tj) = P (Tj)
∖dTj∖
IdTjT
Gamma (Tj ； V-F, 2) 2Tj,
(Tj)(T+1)/2-1 LTj/ 2T..
2(ν-j+1)∕2Γ ( ν-j + 1 )	jj,
TT LTj
2(ν-j-1)∕2Γ (ν-j+1).
(42)
(43)
(44)
(45)
Thus, the probability density for T under the Bartlett sampling operation is
P(T)=
j
<^_
Tjj-je-Tj	Y	ɪ e-Tjk∕2
9ν-2-1Γ ( ν-j+1)	-ɪr I √2∏
2	2 Γ ( -2----k k∈{j+1,…,m}
------- ----------}、-----------------------}
on-diagonals	off-diagonals
(46)
(47)
To convert this to a distribution on Σ, we need the volume element for the transformation from T to
Σ,
dΣ
m
2m Y Tjmj-j+1dT,
j=1
(48)
which can be obtained directly by computing the log-determinant of the Jacobian for the transforma-
tion from T to Σ, or by taking the ratio of Eq. (46) and the usual Wishart probability density (with
integral ν). Thus,
m
2m Y Tjmj-j+1
j=1
-1
π-
j2
Tjνj-m-1e-Tj2j∕2
ν-2+1 Γ (ν-2+1
π
∈{j+1,...,m}
1
1—e
√2∏
-Tj2k∕2
(49)
(50)
14
Under review as a conference paper at ICLR 2021
Breaking this down into separate components and performing straightforward algebraic manipula-
tions,
π
j
1
2(ν-j -1)/2
γ γ τν-m— 1 = ∣τ∣ ν-m—1 = ∣∑∣(ν-m—1)/2
j
P (Σ) = Y e-Tj2j∕2	Y	e-Tj2k∕2
j	k∈{j+1,...,m}
= e-PjkTj2k∕2 = e- Tr(Σ)∕2,
kjΠ,m} √2= (Π J Mn
2(m-j-I)/2
2(ν-j+1)∕2
∏ 2(j-1)/2
2(v-m)/2
2-mν ∕2
(51)
(52)
(53)
(54)
(55)
(56)
π
1
1
π
j
1
where the second-to-last line was obtained by noting that j - 1 covers the same range of integers as
m - j - 1 under the product. Finally, using the definition of the multivariate Gamma function,
∏Γ(ν-2+1)	∏	√∏ = πm(m-1)/4nΓ(ν-2+1) =Γm (ν).
j	k∈{j+1,...,m}	j
We thereby re-obtain the probability density for the standard Wishart distribution,
IVIl(V-m-1)/2	Tr(∑”2
P (Σ) = J-J------------------.
J	2mνWm ( (V )
(57)
(58)
D Full description of our method for deep Gaussian process
Here we give the full derivation for our doubly-stochastic variational deep GPs, following Salimbeni
& Deisenroth (2017). A deep Gaussian process (DGP; (Damianou & Lawrence, 2013; Salimbeni
& Deisenroth, 2017)) defines a prior over function values, f` ∈ RP×N', where ' is the layer, P is
the number of input points, and n` is the “width” of this layer, by stacking L +1 layers of standard
Gaussian processes (we use L + 1 layers instead of the typical L layers to retain consistency with
how we define BNNs):
P (Y, {F'}L+11∣X) = P(Y∣Fl) QL=11 P(F'∣F'-1).	(59)
Here, the input is F0 = X, and the output is Y (which could be continuous values for regression,
or class labels for classification), and the distribution over f` ∈ RP×N' factorises into independent
multivariate Gaussian distributions over each function,
P (F'∣F'-1) = QN= 1 P (fλ∣F'-1) = QN= 1N (fλ∣0, K (f`-1)),	(60)
where f： is the λth column of f`, giving the activation of all datapoints for the λth feature, and K (∙)
is a function that computes the kernel-matrix from the features in the previous layer.
To define a variational approximate posterior, we augment {F'}L+11 with inducing points consisting
of the function values {U'}L+11,
P (Y, {F', U'}L+11∣X, Uo) = P (Y∣Fl+1) QL+11 P (f`, U'∣F'-1, U'-1)	(61)
and because f` and U' are the function outputs corresponding to different inputs (F'_1 and U'_1),
they form a joint multivariate Gaussian distribution, analogous to Eq. (60),
P (F', u'∣f'-1, u'-1) = 口N O ∣0, K ((U'-1))).
(62)
15
Under review as a conference paper at ICLR 2021
Following Salimbeni & Deisenroth (2017) we form an approximate posterior by conditioning the
function values, f` on the inducing outputs, u`,
Q ({F', U'}L+ι1∣X, Uo) = QL+i1 Q (f`, U'∣F'-1, U'-ι)
=QL+i1 P (F'∣U', U'-1, F'-ι) Q (U'∣U'-ι),	(63)
where Q (U'∣U'-ι) is given by Eq. (19), i.e.
Q(U'∣U'-ι) = QN= ιN (uλ∣∑UΛ'vλ, ∑U),	∑U = (KT(U'-ι) + Λ')-1,	(64)
and P (F'|U', U'-1, F'-1) is given by standard manipulations of Eq. (62). Importantly, the prior
can be factorised in an analogous fashion,
P (F', U'|F'-1, U'-1) = P (F'|U', U'-1, F'-1) P (U'|U'-1).	(65)
The full ELBO can be written as
L = Eq.f u il+i1xu)[logP (YNF'，U'}L=j∣x, 7 1.	(66)
QdF',u'}'=i|x,Uo) [ m Q ({F', u`}L+11∣X, Uo) _	∖ '
Substituting Eqs. (61), (63) and (65) into Eq. (66), the P (F'|U', U'-1, F'-1) terms cancel and we
obtain,
L = eq({F',U'}L=i1∣X,Uo)
L+1
log P (Y|FL+1) + X log
'=1
P(U'∣U'-ι)
Q(U'∣U'-ι)
(67)
Analogous to the BNN case, we evaluate the ELBO by alternatively sampling the inducing function
values U' given U'-1 and propagating the data by sampling from P (F'|U', U'-1, F'-1) (see
Alg. 2). As in Salimbeni & Deisenroth (2017), since all likelihoods we use factorise across datapoints,
we only need to sample from the marginals of the latter distribution to sample the propagated function
values. As in the BNN case, the parameters of the approximate posterior are the global inducing
inputs, U0, and the pseudo-data and precisions at all layers, {V', Λ'}'L=1. We can similarly use
standard reparameterised variational inference (Kingma & Welling, 2013; Rezende et al., 2014) to
optimise these variational parameters, as Q (U'|U'-1) is Gaussian. We use Adam (Kingma & Ba,
2014) to optimise the parameters.
Algorithm 2: Global inducing points for deep Gaussian processes
Parameters: inducing inputs, U0, inducing outputs and precisions, {V', Λ'}'L=1, at all layers.
DGP inputs: (e.g. MNIST digits) F0
DGP outputs: (e.g. classification logits) FL+1
LJ 0
for ` ∈ {1, . . . , L + 1} do
Compute the mean and covariance over the inducing outputs at this layer
∑u = (KT (U'-ι) + Λ')-1
M' = Σ'uΛ'V'
Sample the inducing outputs and compute the ELBO
U' 〜N(M', ∑U) = Q(U'∣U'-ι)
LJL + IogP (U' | U'-1) - log N (U'∣M', ∑U)
Propagate the inputs using the sampled inducing outputs,
_ F' 〜P(F'U', U'-ι, F'-ι)
LJL + logP(Y∣FL+ι)
E Motivating the approximate posterior for deep GPs
Our original motivation for the approximate posterior was for the BNN case, which we then extended
to deep GPs. Here, we show how the same approximate posterior can be motivated from a deep GP
16
Under review as a conference paper at ICLR 2021
perspective. As with the BNN case, we first derive the form of the optimal approximate posterior for
the last layer, in the regression case. Without inducing points, the ELBO becomes
-P (Y,{F'}L+1)-
L = Ec 门TIL+1∖ log	'/I '匕1，,	(68)
Q(WJI) [ m Q ({F'}L=11) _ ,	一
where We have defined a generic variational posterior Q ({F'}L+11). Since We are interested in the
form of Q (FL+1 ∣{F'}L=I), we rearrange the ELBO so that all terms that do not depend on FL+1
are absorbed into a constant, c:
L = Eq({f.缜i) [logP (Y, Fl+i∖{F'}L=i) — logQ(FL+ι)+ c] .	(69)
Some straightforward rearrangements lead to a similar form to before,
L = eq({F'}L=1)[ lθgp (γ∣{F' }L=l)
-DKL (Q (Fl+i∣{F'}L=i) II P (Fl+i∣ Y, {F'}L=i)) + c], (70)
from which we see that the optimal conditional posterior is given by Q (Fl+i∣{F'}L=J =
Q (FL+1IFL) = P (FL+1 I Y, FL), which has a closed form for regression: it is simply the standard
GP posterior given by training data Y at inputs FL. In particular, for likelihood
P(Y∣Fl+i, Λl+i) = QNL+1 N (yL+1 ∖ff+1, Λ-+J ,	(71)
where ΛL+1 is the precision,
Q (Fl+i∣Fl) = QNL+1 N (fL+1∖∑L+ιΛL+ιyL+1, ∑L+ι),	(72)
ΣfL+1 = (K-1 (FL) +ΛL+1)-1.	(73)
Finally, as is usual in GPs (Rasmussen & Williams, 2006), the predictive distribution for test points
can be obtained by conditioning using this approximate posterior.
F	Comparing our deep GP approximate posterior to previous work
The standard approach to inference in deep GPs (e.g. Salimbeni & Deisenroth, 2017) involves local
inducing points and an approximate posterior over {U'}L=II that is factorised across layers,
Q ({U'}L+ι1) = QL=I1 QN= iN (uλ; mλ, ∑λ) ∙	(74)
This approximate posterior induces an approximate posterior over the underlying infinite-dimensional
functions f` at each layer, which are implicitly used to propagate the data through the network
via f` = F'(F'-ι). We show a graphical model summarising the standard approach in Fig. A1a.
While, as Salimbeni & Deisenroth (2017) point out, the function values {F'}31 are correlated,
the functions {F'}L=+1 themselves are independent across layers. We note that for BNNs, this is
equivalent to having a posterior over weights that factorises across layers.
One approach to introduce dependencies across layers for the functions would be to introduce the
notion of global inducing points, propagating the initial U0 through the model. In fact, as we note
in the Related Work section, Ustyuzhaninov et al. (2020) independently proposed this approach to
introducing dependencies, using a toy problem to motivate the approach. However, they kept the form
of the approximate posterior the same as the standard approach (Eq. 18); we show the corresponding
graphical model in Fig. A1b. The graphical model shows that as adjacent functions F' and F'+ι
share the parent node U', they are in fact dependent. However, non-adjacent functions do not share
any parent nodes, and so are independent; this can be seen by considering the d-separation criterion
(Pearl, 1988) for F'-ι and F'+ι, which have parents (U'-2, U'-ι) and (U', U'+ι) respectively.
Our approach, by contrast, determines the form of the approximate posterior over u` by performing
Bayesian regression using U'-1 as input to that layer’s GP, where the output data is V'. This results
in a posterior that depends on the previous layer, Q (U'IU'-1). We show the corresponding graphical
model in Fig. A1c. From this graphical model it is straightforward to see that our approach results in
a posterior over functions that are correlated across all layers.
17
Under review as a conference paper at ICLR 2021
(a)	Graphical model illustrating the approach of Salimbeni & Deisenroth (2017). The inducing inputs,
{U'-ι}L=+ι1 are treated as learned parameters and are therefore omitted from the model.
(b)	Graphical model illustrating the approach of Ustyuzhaninov et al. (2020). The inducing inputs are given by
the inducing outputs at the previous layer.
(c) Graphical model illustrating our approach.
Figure A1: Comparison of the graphical models for three approaches to inference in deep GPs:
Salimbeni & Deisenroth (2017), Ustyuzhaninov et al. (2020), and ours.
G Parameter s caling for ADAM
The standard optimiser for variational BNNs is ADAM (Kingma & Ba, 2014), which we also use.
Considering similar RMSprop updates for simplicity (Tieleman & Hinton, 2012),
∆w = η-I g
PE[g2]
(75)
where the expectation over g2 is approximated using a moving-average of past gradients. Thus,
absolute parameter changes are going to be of order η. This is fine if all the parameters have roughly
18
Under review as a conference paper at ICLR 2021
2 points	10 points
ELBO: -2.244	ELBO: 2.055
A
20 points
ELBO:2,125
40 points
ELBO:2.179
Figure A2: Predictive distributions on the toy dataset as the number of inducing points changes.
the same order of magnitude, but becomes a serious problem if some of the parameters are very large
and others are very small. For instance, if a parameter is around 10-4 and η = 10-4, then a single
ADAM step can easily double the parameter estimate, or change it from positive to negative. In
contrast, if a parameter is around 1, then ADAM, with η-4 can make proportionally much smaller
changes to this parameter, (around 0.01%). Thus, we need to ensure that all of our parameters have
the same scale, especially as we mix methods, such as combining factorised and global inducing
points. We thus design all our new approximate posteriors (i.e. the inducing inputs and outputs) such
that the parameters have a scale of around 1. The key issue is that the mean weights in factorised
methods tend to be quite small — they have scale around 1 / √fan-in. To resolve this issue, we store
scaled weights, and we divide these stored, scaled mean parameters by the fan-in as part of the
forward pass,
scaled weights
weights =-------,	——
fan-in
(76)
This scaling forces us to use larger learning rates than are typically used.
H Exploring the effect of the number of inducing points
In this section, we briefly consider the effect of changing the number of inducing points, M , used in
global inducing. We reconsider the toy problem from Sec. 4.1, and plot predictive posteriors obtained
with global inducing as the number of inducing points increases from 2 to 40 (noting that in Fig. 1
we used 100 inducing points).
We plot the results of our experiment in Fig. A2. While two inducing points are clearly not sufficient,
we observe that there is remarkably very little difference between the predictive posteriors for 10 or
more inducing points. This observation is reflected in the ELBOs per datapoint (listed above each
plot), which show that adding more points beyond 10 gains very little in terms of closeness to the
true posterior.
However, we note that this is a very simple dataset: it consists of only two clusters of close points
with a very clear trend. Therefore, we would expect that for more complex datasets more inducing
points would be necessary. We leave a full investigation of how many inducing points are required
to obtain a suitable approximate posterior, such as that found in Burt et al. (2020) for sparse GP
regression, to future work.
I	Understanding compositional uncertainty
In this section, we take inspiration from the experiments of Ustyuzhaninov et al. (2020) which
investigate the compositional uncertainty obtained by different approximate posteriors for DGPs. They
noted that methods which factorise over layers have a tendency to cause the posterior distribution for
each layer to collapse to a (nearly) deterministic function, resulting in worse uncertainty quantification
within layers and worse ELBOs. In contrast, they found that allowing the approximate posterior to
have correlations between layers allows those layers to capture more uncertainty, resulting in better
19
Under review as a conference paper at ICLR 2021
-1	0	1
X
-10	1	-10	1
x	x
Figure A3: Posterior distributions for 2-layer DGPs with local inducing and global inducing. The
first two columns show the predictive distributions for each layer taken individually, while the last
column shows the predictive distribution of the output y.
Table 2: ELBOs and variances of the intermediate functions for a BNN fit to the toy data of Fig. 1.
	ELBO	V[Fι]	V[F2]	V[F3]
factorised	-4.585	0.0728	0.4765	0.1926
local inducing	-5.469	0.0763	0.4473	0.0643
global inducing	2.236	0.4820	0.4877	1.0820
ELBOs and therefore a closer approximation to the true posterior. They argue that this then allows
the model to better discover compositional structure in the data.
We first consider a toy problem consisting of 100 datapoints generated by sampling from a two-layer
DGP of width one, with squared-exponential kernels in each layer. We then fit two two-layer DGPs
to this data - one using local inducing, the other using global inducing. The results of this experiment
can be seen in Fig. A3, which show the final fit, along with the learned posteriors over intermediate
functions F1 and F2 . These results mirror those observed by Ustyuzhaninov et al. (2020) on a similar
experiment: local inducing, which factorises over layers, collapses to a nearly deterministic posterior
over the intermediate functions, whereas global inducing provides a much broader distribution over
functions for the two layers. Therefore, global inducing leads to a wider range of plausible functions
that could explain the data via composition, which can be important in understanding the data. We
observe that this behaviour directly leads to better uncertainty quantification for the out-of-distribution
region, as well as better ELBOs.
To illustrate a similar phenomenon in BNNs, we reconsider the toy problem of Sec. 4.1. As it is
not meaningful to consider neural networks with only one hidden unit per layer, instead of plotting
intermediate functions we instead look at the mean variance of the functions at random input points,
following roughly the experiment Dutordoir et al. (2019) in Table 1. For each layer, we consider the
quantity
一 1 n`	一
Ex h ∑V[fλ (x)]，	(77)
- N'λ=1	_
20
Under review as a conference paper at ICLR 2021
boston	concrete energy	kin8nm	naval	power	protein	Wine	yacht
N=506,D=13	N=1030,D=8	N=768,D=8	N=8192, D=8 N=11934,D=16 N=9568, D=4	N=45730,D=9 N=1599,D=11	N=308,D=6
ω
factorised ∙ local inducing ∙ global inducing ∙ fac → gi
Figure A4: ELBOs per datapoint and average test log likelihoods for BNNs on UCI datasets.
where the expectation is over random input points, which we sample from a standard normal. We
expect that for methods which introduce correlations across layers, this quantity will be higher, as
there will be a wider range of intermediate functions that could plausibly explain the data. We confirm
this in Table 2, which indicates that global inducing leads to many more compositions of functions
being considered as plausible explanations of the data. This is additionally reflected in the ELBO,
which is far better for global inducing than the other, factorised methods. However, we note that the
variances are far closer than we might otherwise expect for the second layer. We hypothesise that this
is due to the pruning effects described in Trippe & Turner (2018), where a layer has many weights
that are close to the prior that are then pruned out by the following layer by having the outgoing
weights collapse to zero. In fact, we note that the variances in the last layer are small for the factorised
methods, which supports this hypothesis. By contrast, global inducing leads to high variances across
all layers.
We believe that understanding the role of compositional uncertainty in variational inference for
deep Bayesian models can lead to important conclusions about both the models being used and the
compositional structure underlying the data being modelled, and is therefore an important direction
for future work to consider.
J UCI results with Bayesian neural networks
For this Appendix, We consider all of the UCI datasets from Herngndez-Lobato & Adams (2015),
along with four approximation families: factorised (i.e. mean-field), local inducing, global inducing,
and fac→gi, Which may offer some computational advantages to global inducing. We also considered
three priors: the standard N (0, 1) prior, NealPrior, and ScalePrior. The test LLs and ELBOs for
BNNs applied to UCI datasets are given in Fig. A4. Note that the ELBOs for the global inducing
methods (both global inducing and fac→gi) are almost alWays better than those for baseline methods,
often by a very large margin. HoWever, as noted earlier, this does not necessarily correspond to better
test log likelihoods due to model misspecification: there is not a straightforWard relationship betWeen
the ELBO and the predictive performance, and so it is possible to obtain better test log likelihoods
With Worse inference. We present all the results, including for the test RMSEs, in tabulated form in
Appendix P.
21
Under review as a conference paper at ICLR 2021
factorised	local inducing	fac → gi
Figure A5: Calibration curves for CIFAR-10
J.1 Experimental details
The architecture we considered for all BNN UCI experiments were fully-connected ReLU networks
with 2 hidden layers of 50 hidden units each. We performed a grid search to select the learning
rate and minibatch size. For the fully factorised approximation, we selected the learning rate from
{3e-4, 1e-3, 3e-3, 1e-2} and the minibatch size from {32, 100, 500}, optimising for 25000 gradient
steps; for the other methods we selected the learning rate from {3e-3, 1e-2} and fixed the minibatch
size to 10000 (as in Salimbeni & Deisenroth (2017)), optimising for 10000 gradient steps. For all
methods we selected the hyperparameters that gave the best ELBO. We trained the models using
10 samples from the approximate posterior, while using 100 for evaluation. For the inducing point
methods, we used the selected batch size for the number of inducing points per layer. For all methods,
we initialised the log noise variance at -3, but use the scaling trick in Appendix G to accelerate
convergence, scaling by a factor of 10. Note that for the fully factorised method we used the local
reparameterisation trick (Kingma et al., 2015); however, for fac → gi we cannot do so because the
inducing point methods require that covariances be propagated through the network correctly. For
the inducing point methods, we additionally use output channel-specific precisions, Λl,λ, which
effectively allows the network to prune unnecessary neurons if that benefits the ELBO. However, we
only parameterise the diagonal of these precision matrices to save on computational and memory
cost.
K	Uncertainty calibration for CIFAR- 1 0
To assess how well our methods capture uncertainty, we consider calibration. Calibration is assessed
by comparing the model’s probabilistic assessment of its confidence with its accuracy — the pro-
portion of the time that it is actually correct. For instance, gathering model predictions with some
confidence (e.g. softmax probabilities in the range 0.9 to 0.95), and looking at the accuracy of these
predictions, we would expect the model to be correct with probability 0.925; a higher or lower value
would represent miscalibration.
We begin by plotting calibration curves in Fig. A5, obtained by binning the predictions in 20 equal bins
and assessing the mean accuracy of the binned predictions. For well-calibrated models, we expect the
line to lie on the diagonal. A line above the diagonal indicates the model is underconfident (the model
is performing better than it expects), whereas a line below the diagonal indicates it is overconfident (it
is performing worse than it expects). While it is difficult to draw strong conclusions from these plots,
it appears generally that factorised is poorly calibrated for both priors, that SpatialIWPrior generally
improves calibration over ScalePrior, and that local inducing with SpatialIWPrior performs very well.
To come to more quantitative conclusions, we use expected calibration error (ECE; (Naeini et al.,
2015; Guo et al., 2017)), which measures the expected absolute difference between the model’s
confidence and accuracy. Confirming the results from the plots (Fig. A5), we find that using the more
sophisticated SpatialIWPrior gave considerable improvements in calibration. While, as expected, we
find that our most accurate prior, SpatialIWPrior, in combination with global inducing points did very
well (ECE of 0.021), the model with the best ECE is actually local inducing with SpatialIWPrior,
albeit by a very small margin. We leave investigation of exactly why this is to future work. Finally,
note our final ECE value of 0.021 is a considerable improvement over those for uncalibrated models
22
Under review as a conference paper at ICLR 2021
Table 3: Expected calibration error for CIFAR-10
	factorised	local inducing	fac → gi	global inducing
ScalePrior	0.053	0.040	0.049	0.038
SpatialIWPrior	0.045	0.018	0.036	0.021
in Guo et al. (2017) (Table 1), which are in the region of 0.03-0.045 (although considerably better
calibration can be achieved by post-hoc scaling of the model’s confidence).
L Asymptotic complexity
In the deep GP case, the complexity for global inducing is exactly that of standard inducing point
Gaussian processes, i.e. O(M3 + PM2) where M is the number of inducing points, and P can be
taken to be the number of training inputs, or the size of a minibatch, as appropriate. The first term,
M3, comes from computing and sampling the posterior over u` based on the inducing points (e.g.
inverting the covariance). The second term, and PM2 comes from computing the implied distribution
over f`.
In the fully-connected BNN case, we have three terms, O(N3 + MN2 + PN2). The first term, N3,
where N corresponds to the width of the network, arises from taking the inverse of the covariance
matrix in Eq. (9), but is also the complexity e.g. for propagating the inducing points from layer to the
next (Eq. 8). The second term, MN2, comes from computing that covariance in Eq. (9), by taking the
product of input features with themselves. The third term P N2 comes from multiplying the training
inputs/minibatch by the sampled inputs (Eq. 1).
M UCI results with deep Gaussian processes
In this appendix, We again consider all of the UCI datasets from Herndndez-Lobato & Adams (2015)
for DGPs with depths ranging from two to five layers. We compare DSVI (Salimbeni & Deisenroth,
2017), local inducing, and global inducing. While local inducing uses the same inducing-point
architecture as Salimbeni & Deisenroth (2017), the actual implementation and parameterisation is
very different and in addition they used a complex architecture involving skip connections that We
may not have matched exactly. As such, We do expect to see differences betWeen local inducing and
Salimbeni & Deisenroth (2017).
We shoW our results in Fig. A6. Here, the results are not as clear-cut as in the BNN case. For the
smaller datasets (i.e. boston, concrete, energy, Wine, yacht), global inducing generally outperforms
both local inducing and DSVI, as noted in the main text, especially When considering the ELBOs.
We do hoWever observe that for poWer, protein, and one model for kin8nm, the local approaches
sometimes outperform global inducing, even for the ELBOs. We believe this is due to the fact that
relatively feW inducing points Were used (100), in combination With the fact that global inducing has
far feWer variational parameters than the local approaches. This may make optimisation harder in the
global inducing case, especially for larger datasets Where the model uncertainty does not matter as
much, as the posterior concentration Will be stronger. Importantly, hoWever, our results on CIFAR-10
indicate that these issues do not arise in very large-scale, high-dimensional datasets, Which are of
most interest for future Work.
We provide tabulated results, including for RMSEs, in Appendix P.
M.1 Experimental details
Here, We matched the experimental setup in Salimbeni & Deisenroth (2017) as closely as possible.
In particular, We used 100 inducing points, and full-covariance observation noise. HoWever, our
parameterisation is still someWhat different from theirs, in part because our approximate posterior is
defined in terms of noisy function-values, While their approximate posterior Was defined in terms of
the function-values themselves.
23
Under review as a conference paper at ICLR 2021
-2.53 -2.33 -3.15 -3.10-0.87 -0.69 1.34	1.38 6.80	7.90 -2.78 -2.72-2.81 -2.71-0.97 -0.94 -0.48 -0.20
I ∙ DSVI ∙ local inducing ∙ global inducing
Figure A6: ELBOs per datapoint and average test log likelihoods for DGPs on UCI datasets. The
numbers indicate the depths of the models.
As	the	original	results	in Salimbeni	&	Deisenroth (2017)	used differ-
ent	UCI	splits,	and did not provide	the ELBO,	we reran	their code
https://github.com/ICL-SML/Doubly-Stochastic-DGP (changing the number
of epochs and noise variance to reflect the values in the paper), which gave very similar log
likelihoods to those in the paper.
N MNIST 500
For MNIST, we considered a LeNet-inspired model consisting of two conv2d-relu-maxpool blocks,
followed by conv2d-relu-linear, where the convolutions all have 3 × 3 kernels with 64 channels. We
trained all models using a learning rate of 10-3.
When training on very small datasets, such as the first 500 training examples in MNIST, we can see a
variety of pathologies emerge with standard methods. To help build intuition for these pathologies,
we introduce a sanity check for the ELBO. In particular, we could imagine a model that sets the
distribution over all lower-layer parameters equal to the prior, and sets the top-layer parameters so
as to ensure that the predictions are uniform. With 10 classes, this results in an average test log
likelihood of -2.30 ≈ log(1/10), and an ELBO (per datapoint) of approximately -2.30. We found
that many combinations of the approximate posterior/prior converged to ELBOs near this baseline.
Indeed, the only approximate posterior to escape this baseline for ScalePrior and SpatialIWPrior is
global inducing points. This is because ScalePrior and SpatialIWPrior both offer the flexibility to
shrink the prior variance, and hence shrink the weights towards zero, giving uniform predictions, and
potentially zero KL divergence. In contrast, NealPrior and StandardPrior do not offer this flexibility:
you always have to pay something in KL divergence in order to give uniform predictions. We believe
that this is the reason that factorised performs better than expected with NealPrior, despite having an
ELBO that is close to the baseline. Furthermore, it is unclear why local inducing gives very test log
likelihood and performance, despite having an ELBO that is similar to factorised. For StandardPrior,
all the ELBOs are far lower than the baseline, and far lower than for any other priors. Despite this,
factorised and fac → gi in combination with StandardPrior appear to transiently perform better in
terms of predictive accuracy than any other method. These results should sound a note of caution
24
Under review as a conference paper at ICLR 2021
OgK poo-"-'6o- Eltiauou qo」dtis
NealPnor	StandardPrior	ScalePnor	SpatialIWPnor
0.0 -
-2.5 -
-5.0 -
-7.5 -
0 -
factorised
local inducing
fac T gi
global inducing
1.0 -
0.8 -
0.6 -
0	20	40 0	20	40 0	20	40 0	20	40
thousand epochs thousand epochs thousand epochs thousand epochs
Figure A7: The ELBO, test log likelihoods and classification accuracy with different priors and
approximate posteriors on a reduced MNIST dataset consisting of only the first 500 training examples.
whenever we try to use factorised approximate posteriors with fixed prior covariances (e.g. Blundell
et al., 2015; Farquhar et al., 2020). We leave a full investigation of these effects for future work.
O Additional experimental details
All the methods were implemented in PyTorch. We ran the toy experiments on CPU, with the UCI
experiments being run on a mixture of CPU and GPU. The remaining experiments - linear, CIFAR-10,
and MNIST 500 - were run on various GPUs. For CIFAR-10, the most intensive of our experiments,
we trained the models on one NVIDIA Tesla P100-PCIE-16GB. We optimised using ADAM (Kingma
& Ba, 2014) throughout.
Factorised We initialise the posterior weight means to follow the Neal scaling (i.e. drawn from
NealPrior); however, We use the scaling described in Appendix G to accelerate training. We initialise
the weight variances to 1e-3/y∕N'-ι for each layer.
Inducing point methods For global inducing, we initialise the inducing inputs, U0, and pseudo-
data for the last layer, VL+1, using the first batch of data, except for the toy experiment, where we
initialise using samples from N (0, 1) (since we used more inducing points than datapoints). For
the remaining layers, we initialise the pseudo-data by sampling from N (0, 1). We initialise the log
precision to -4, except for the last layer, where we initialise it to 0. We additionally use a scaling
factor of 3 as described in Appendix G. For local inducing, the initialisation is largely the same,
except we initialise the pseudo-data for every layer by sampling from N (0, 1). We additionally
sample the inducing inputs for every layer from N (0, 1).
Toy experiment For each variational method, we optimise the ELBO over 5000 epochs, using full
batches for the gradient descent. We use a learning rate of 1e-2. We fix the noise variance at its
25
Under review as a conference paper at ICLR 2021
true value, to help assess the differences between each method more clearly. We use 10 samples
from the variational posterior for training, using 100 for testing. For HMC, we use 10000 samples
to burn in, and 10000 samples for evaluation, which we subsequently thin by a factor of 10. We
initialise the samples from a standard normal distribution, and use 20 leapfrog steps for each sample.
We hand-tune the leapfrog step sizes to be 0.0007 and 0.003 for the burn-in and sampling phases,
respectively.
Deep linear network We use 10 inducing points for the inducing point methods. We use 1 sample
from the approximate posterior for training and 10 for testing, training for 40 periods of 1000 gradient
steps, using full batches for each step, with a learning rate of 1e-2.
UCI experiments The splits that we used for the UCI datasets can be found at https://github.
com/yaringal/DropoutUncertaintyExps.
CIFAR-10 The CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.
html; (Krizhevsky et al., 2009)) is a 10-class dataset comprising RGB, 32 × 32 images. It is
divided in two sets: a training set of 50,000 examples, and a validation set of 10,000 examples.
For the purposes of this paper, we use the validation set as our test set and refer to it as such, as is
commonly done in the literature. We use a batch size of 500, with one sample from the approximate
posterior for training and 10 for testing. For pre-processing, we normalise the data using the training
dataset’s mean and standard deviation. Finally, we train for 1000 epochs with a learning rate of 1e - 2
(see App. G for an explanation of why our learning rate is higher than might be expected), and we use
a tempering scheme for the first 100 epochs, slowly increasing the influence of the KL divergence
to the prior by multiplying it by a factor that increases from 0 to 1. In our scheme, we increase the
factor in a step-wise manner, meaning that for the first ten epochs it is 0, then 0.1 for the next ten, 0.2
for the following ten, and so on. Importantly, we still have 900 epochs of training where the standard,
untempered ELBO is used, meaning that our results reflect that ELBO. Finally, we note that we share
the precisions Λ' within layers instead of using a separate precision for each output channel as was
done in the UCI case. This saves memory and computational cost although possibly at the expense of
predictive performance.
MNIST 500 The MNIST dataset (http://yann.lecun.com/exdb/mnist/) is a dataset
of grayscale handwritten digits, each 28 × 28 pixels, with 10 classes. It comprises 60,000 training
images and 10,000 test images. For the MNIST 500 experiments, we trained using the first 500
images from the training dataset and discarded the rest. We normalised the images using the full
training dataset’s statistics.
Deep GPs As mentioned, we largely follow the approach of Salimbeni & Deisenroth (2017) for
hyperparameters. For global inducing, we initialise the inducing inputs to the first batch of training
inputs, and we initialise the pseudo-outputs for the last layer to the respective training outputs. For the
remaining layers, we initialise the pseudo-outputs by sampling from a standard normal distribution.
We initialise the precision matrix to be diagonal with log precision zero for the output layer, and log
precision -4 for the remaining layers. For local inducing, we initialise inducing inputs and pseudo
data by sampling from a standard normal for every layer, and initialise the precision matrices to be
diagonal with log precision zero.
26
Under review as a conference paper at ICLR 2021
P Tables of UCI Results
Table 4: Average test log likelihoods in nats for BNNs on UCI datasets (errors are ± 1 standard error)
	factorised	local inducing	global inducing	factorised → global
boston - N (0, 1)	-2.74 ± 0.03	-2.80 ± 0.04	-2.59 ± 0.03	-2.60 ± 0.02
NealPrior	-2.76 ± 0.04	-2.71 ± 0.04	-2.55 ± 0.05	-2.63 ± 0.05
ScalePrior	-3.63 ± 0.03	-2.73 ± 0.03	-2.50 ± 0.04	-2.61 ± 0.04
concrete - N (0, 1)	-3.17 ± 0.02	-3.28 ± 0.01	-3.08 ± 0.01	-3.14 ± 0.01
NealPrior	-3.21 ± 0.01	-3.29 ± 0.01	-3.14 ± 0.01	-3.15 ± 0.02
ScalePrior	-3.89 ± 0.09	-3.30 ± 0.01	-3.12 ± 0.01	-3.15 ± 0.01
energy - N (0, 1)	-0.76 ± 0.02	-1.75 ± 0.01	-0.73 ± 0.03	-0.68 ± 0.03
NealPrior	-0.79 ± 0.02	-2.06 ± 0.09	-0.72 ± 0.02	-0.70 ± 0.02
ScalePrior	-2.55 ± 0.01	-2.42 ± 0.02	-0.73 ± 0.02	-0.74 ± 0.02
kin8nm - N (0, 1)	1.24 ± 0.01	1.06 ± 0.01	1.22 ± 0.01	1.28 ± 0.01
NealPrior	1.26 ± 0.01	1.06 ± 0.01	1.18 ± 0.01	1.29 ± 0.01
ScalePrior	1.23 ± 0.01	1.10 ± 0.01	1.19 ± 0.01	1.29 ± 0.00
naval - N (0, 1)	7.25 ± 0.04	6.06 ± 0.10	7.29 ± 0.04	7.23 ± 0.02
NealPrior	7.37 ± 0.03	4.28 ± 0.37	6.97 ± 0.04	7.45 ± 0.02
ScalePrior	6.99 ± 0.03	2.80 ± 0.00	6.63 ± 0.04	7.50 ± 0.02
power - N(0, 1)	-2.81 ± 0.01	-2.82 ± 0.01	-2.80 ± 0.01	-2.79 ± 0.01
NealPrior	-2.81 ± 0.01	-2.84 ± 0.01	-2.82 ± 0.01	-2.81 ± 0.01
ScalePrior	-2.82 ± 0.01	-2.84 ± 0.01	-2.82 ± 0.01	-2.81 ± 0.01
protein - N (0, 1)	-2.83 ± 0.00	-2.93 ± 0.00	-2.84 ± 0.00	-2.81 ± 0.00
NealPrior	-2.86 ± 0.00	-2.92 ± 0.01	-2.87 ± 0.00	-2.82 ± 0.00
ScalePrior	-2.86 ± 0.00	-2.91 ± 0.00	-2.85 ± 0.00	-2.80 ± 0.00
wine - N(0, 1)	-0.98 ± 0.01	-0.99 ± 0.01	-0.96 ± 0.01	-0.96 ± 0.01
NealPrior	-0.99 ± 0.01	-0.98 ± 0.01	-0.97 ± 0.01	-0.96 ± 0.01
ScalePrior	-1.22 ± 0.01	-0.99 ± 0.01	-0.96 ± 0.01	-0.96 ± 0.01
yacht - N (0, 1)	-1.41 ± 0.05	-2.39 ± 0.05	-0.68 ± 0.03	-1.12 ± 0.02
NealPrior	-1.58 ± 0.04	-1.84 ± 0.05	-0.81 ± 0.03	-1.04 ± 0.01
ScalePrior	-4.12 ± 0.03	-2.71 ± 0.22	-0.79 ± 0.02	-0.97 ± 0.05
27
Under review as a conference paper at ICLR 2021
Table 5: Test RMSEs for BNNs on UCI datasets (errors are ± 1 standard error)
	factorised	local inducing	global inducing	factorised → global
boston - N (0, 1)	3.60 ± 0.21	3.85 ± 0.26	3.13 ± 0.20	3.14 ± 0.20
NealPrior	3.64 ± 0.24	3.55 ± 0.23	3.14 ± 0.18	3.33 ± 0.21
ScalePrior	9.03 ± 0.26	3.57 ± 0.20	2.97 ± 0.19	3.27 ± 0.20
concrete - N (0, 1)	5.73 ± 0.11	6.34 ± 0.11	5.39 ± 0.09	5.55 ± 0.10
NealPrior	5.96 ± 0.11	6.35 ± 0.12	5.64 ± 0.10	5.70 ± 0.11
ScalePrior	12.66 ± 1.04	6.48 ± 0.12	5.56 ± 0.10	5.68 ± 0.09
energy - N (0, 1)	0.51 ± 0.01	1.35 ± 0.02	0.50 ± 0.01	0.47 ± 0.02
NealPrior	0.51 ± 0.01	1.95 ± 0.14	0.49 ± 0.01	0.47 ± 0.01
ScalePrior	3.02 ± 0.05	2.67 ± 0.06	0.50 ± 0.01	0.49 ± 0.01
kin8nm - N (0, 1)	0.07 ± 0.00	0.08 ± 0.00	0.07 ± 0.00	0.07 ± 0.00
NealPrior	0.07 ± 0.00	0.08 ± 0.00	0.07 ± 0.00	0.07 ± 0.00
ScalePrior	0.07 ± 0.00	0.08 ± 0.00	0.07 ± 0.00	0.07 ± 0.00
naval - N (0, 1)	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
NealPrior	0.00 ± 0.00	0.01 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
ScalePrior	0.00 ± 0.00	0.01 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
power - N (0, 1)	4.00 ± 0.03	4.06 ± 0.03	3.96 ± 0.04	3.93 ± 0.04
NealPrior	4.03 ± 0.04	4.13 ± 0.03	4.06 ± 0.03	4.00 ± 0.04
ScalePrior	4.06 ± 0.04	4.12 ± 0.04	4.05 ± 0.04	4.01 ± 0.04
protein - N (0, 1)	4.12 ± 0.02	4.54 ± 0.02	4.14 ± 0.02	4.04 ± 0.01
NealPrior	4.21 ± 0.01	4.50 ± 0.03	4.27 ± 0.02	4.06 ± 0.01
ScalePrior	4.22 ± 0.02	4.46 ± 0.01	4.19 ± 0.02	4.00 ± 0.02
wine - N(0, 1)	0.65 ± 0.01	0.65 ± 0.01	0.63 ± 0.01	0.63 ± 0.01
NealPrior	0.66 ± 0.01	0.65 ± 0.01	0.64 ± 0.01	0.64 ± 0.01
ScalePrior	0.82 ± 0.01	0.65 ± 0.01	0.64 ± 0.01	0.64 ± 0.01
yacht - N (0, 1)	0.98 ± 0.07	2.35 ± 0.13	0.56 ± 0.04	0.50 ± 0.04
NealPrior	1.15 ± 0.07	1.37 ± 0.11	0.57 ± 0.04	0.63 ± 0.05
ScalePrior	14.55 ± 0.59	5.75 ± 1.34	0.56 ± 0.04	0.63 ± 0.04
28
Under review as a conference paper at ICLR 2021
Table 6: ELBOs per datapoint in nats for BNNs on UCI datasets (errors are ± 1 standard error)
	factorised	local inducing	global inducing	factorised → global
boston - N(0, 1)	-1.55 ± 0.00	-1.54 ± 0.00	-1.02 ± 0.01	-1.03 ± 0.00
NealPrior	-1.03 ± 0.00	-0.99 ± 0.00	-0.63 ± 0.00	-0.70 ± 0.00
ScalePrior	-1.54 ± 0.00	-0.96 ± 0.00	-0.59 ± 0.00	-0.70 ± 0.00
concrete - N(0, 1)	-1.10 ± 0.00	-1.08 ± 0.00	-0.71 ± 0.00	-0.78 ± 0.00
NealPrior	-0.88 ± 0.00	-0.87 ± 0.00	-0.59 ± 0.00	-0.65 ± 0.00
ScalePrior	-1.45 ± 0.01	-0.88 ± 0.00	-0.57 ± 0.00	-0.63 ± 0.00
energy - N(0, 1)	-0.13 ± 0.02	-0.53 ± 0.01	0.72 ± 0.00	0.59 ± 0.00
NealPrior	0.21 ± 0.00	-0.33 ± 0.04	0.95 ± 0.00	0.79 ± 0.01
ScalePrior	-1.12 ± 0.00	-0.47 ± 0.01	0.96 ± 0.01	0.80 ± 0.01
kin8nm - N(0, 1)	-0.38 ± 0.00	-0.43 ± 0.00	-0.26 ± 0.00	-0.31 ± 0.00
NealPrior	-0.35 ± 0.00	-0.43 ± 0.00	-0.31 ± 0.00	-0.28 ± 0.00
ScalePrior	-0.51 ± 0.00	-0.39 ± 0.01	-0.29 ± 0.00	-0.27 ± 0.00
naval - N(0, 1)	1.68 ± 0.01	1.65 ± 0.09	2.89 ± 0.04	2.11 ± 0.02
NealPrior	2.02 ± 0.02	-0.09 ± 0.34	2.53 ± 0.04	2.62 ± 0.02
ScalePrior	1.91 ± 0.03	-1.42 ± 0.00	2.30 ± 0.03	2.71 ± 0.02
power - N(0, 1)	-0.08 ± 0.00	-0.06 ± 0.00	-0.02 ± 0.00	-0.03 ± 0.00
NealPrior	-0.05 ± 0.00	-0.05 ± 0.00	-0.01 ± 0.00	-0.01 ± 0.00
ScalePrior	-0.13 ± 0.00	-0.05 ± 0.00	-0.01 ± 0.00	-0.01 ± 0.00
protein - N(0, 1)	-1.09 ± 0.00	-1.14 ± 0.00	-1.06 ± 0.01	-1.09 ± 0.00
NealPrior	-1.11 ± 0.00	-1.13 ± 0.00	-1.09 ± 0.00	-1.09 ± 0.00
ScalePrior	-1.13 ± 0.00	-1.12 ± 0.00	-1.07 ± 0.00	-1.07 ± 0.00
wine - N(0, 1)	-1.48 ± 0.00	-1.47 ± 0.00	-1.36 ± 0.00	-1.36 ± 0.00
NealPrior	-1.31 ± 0.00	-1.30 ± 0.00	-1.22 ± 0.00	-1.23 ± 0.00
ScalePrior	-1.46 ± 0.00	-1.29 ± 0.00	-1.22 ± 0.00	-1.23 ± 0.00
yacht - N(0, 1)	-1.04 ± 0.02	-1.30 ± 0.02	0.08 ± 0.01	-0.23 ± 0.01
NealPrior	-0.46 ± 0.02	-0.39 ± 0.01	0.74 ± 0.01	0.31 ± 0.01
ScalePrior	-1.61 ± 0.00	-0.77 ± 0.10	0.79 ± 0.01	0.30 ± 0.01
29
Under review as a conference paper at ICLR 2021
Table 7: Average test log likelihoods for our rerun of Salimbeni & Deisenroth (2017), and our
implementations of local and global inducing points for deep GPs of various depths.
{dataset} - {depth}	DSVI	local inducing	global inducing
boston - 2	-2.50 ± 0.05	-2.48 ± 0.05	-2.35 ± 0.05
3	-2.50 ± 0.05	-2.49 ± 0.05	-2.36 ± 0.05
4	-2.51 ± 0.05	-2.49 ± 0.05	-2.35 ± 0.05
5	-2.52 ± 0.05	-2.47 ± 0.05	-2.35 ± 0.05
concrete - 2	-3.14 ± 0.01	-3.13 ± 0.02	-3.11 ± 0.02
3	-3.15 ± 0.01	-3.13 ± 0.02	-3.11 ± 0.02
4	-3.15 ± 0.01	-3.13 ± 0.02	-3.11 ± 0.02
5	-3.11 ± 0.01	-3.13 ± 0.02	-3.11 ± 0.02
energy - 2	-0.72 ± 0.02	-0.71 ± 0.03	-0.69 ± 0.03
3	-0.78 ± 0.02	-0.71 ± 0.03	-0.70 ± 0.03
4	-0.88 ± 0.02	-0.71 ± 0.03	-0.70 ± 0.03
5	-0.79 ± 0.02	-0.70 ± 0.03	-0.70 ± 0.03
kin8nm - 2	1.34 ± 0.00	1.36 ± 0.00	1.34 ± 0.00
3	1.38 ± 0.01	1.35 ± 0.00	1.34 ± 0.00
4	1.38 ± 0.00	1.35 ± 0.00	1.34 ± 0.00
5	1.36 ± 0.00	1.35 ± 0.00	1.34 ± 0.00
naval - 2	6.55 ± 0.22	7.88 ± 0.03	7.99 ± 0.06
3	6.46 ± 0.23	7.82 ± 0.04	8.02 ± 0.05
4	6.79 ± 0.05	7.83 ± 0.05	8.00 ± 0.03
5	6.70 ± 0.07	7.82 ± 0.04	7.94 ± 0.04
power - 2	-2.78 ± 0.01	-2.74 ± 0.01	-2.78 ± 0.01
3	-2.74 ± 0.01	-2.72 ± 0.01	-2.78 ± 0.01
4	-2.74 ± 0.01	-2.73 ± 0.01	-2.78 ± 0.01
5	-2.74 ± 0.01	-2.72 ± 0.01	-2.78 ± 0.01
protein - 2	-2.80 ± 0.00	-2.81 ± 0.00	-2.82 ± 0.00
3	-2.73 ± 0.00	-2.78 ± 0.02	-2.78 ± 0.00
4	-2.71 ± 0.00	-2.79 ± 0.01	-2.78 ± 0.00
5	-2.70 ± 0.00	-2.82 ± 0.00	-2.78 ± 0.00
wine - 2	-0.95 ± 0.01	-0.96 ± 0.01	-0.96 ± 0.01
3	-0.95 ± 0.01	-0.96 ± 0.01	-0.96 ± 0.01
4	-0.95 ± 0.01	-0.96 ± 0.01	-0.96 ± 0.01
5	-0.95 ± 0.01	-0.96 ± 0.01	-0.96 ± 0.01
yacht - 2	-0.41 ± 0.03	-0.27 ± 0.06	-0.29 ± 0.05
3	-0.46 ± 0.03	-0.23 ± 0.04	-0.25 ± 0.06
4	-0.49 ± 0.03	-0.20 ± 0.05	-0.25 ± 0.05
5	-0.49 ± 0.03	-0.27 ± 0.05	-0.23 ± 0.04
30
Under review as a conference paper at ICLR 2021
Table 8: Test RMSEs for our rerun of Salimbeni & Deisenroth (2017), and our implementations of
local and global inducing points for deep GPs of various depths.
{dataset} - {depth}	DSVI	local inducing	global inducing
boston - 2	2.94 ± 0.17	2.87 ± 0.14	2.63 ± 0.14
3	2.94 ± 0.18	2.89 ± 0.14	2.68 ± 0.14
4	2.98 ± 0.18	2.88 ± 0.15	2.64 ± 0.13
5	2.99 ± 0.19	2.87 ± 0.13	2.63 ± 0.13
concrete - 2	5.73 ± 0.09	5.53 ± 0.09	5.42 ± 0.10
3	5.75 ± 0.09	5.54 ± 0.11	5.46 ± 0.12
4	5.74 ± 0.09	5.53 ± 0.11	5.42 ± 0.10
5	5.49 ± 0.09	5.52 ± 0.12	5.42 ± 0.10
energy - 2	0.49 ± 0.01	0.48 ± 0.01	0.48 ± 0.01
3	0.51 ± 0.01	0.48 ± 0.01	0.48 ± 0.01
4	0.56 ± 0.02	0.49 ± 0.01	0.48 ± 0.01
5	0.51 ± 0.01	0.48 ± 0.01	0.48 ± 0.01
kin8nm - 2	0.06 ± 0.00	0.06 ± 0.00	0.06 ± 0.00
3	0.06 ± 0.00	0.06 ± 0.00	0.06 ± 0.00
4	0.06 ± 0.00	0.06 ± 0.00	0.06 ± 0.00
5	0.06 ± 0.00	0.06 ± 0.00	0.06 ± 0.00
naval - 2	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
3	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
4	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
5	0.00 ± 0.00	0.00 ± 0.00	0.00 ± 0.00
power - 2	3.87 ± 0.04	3.74 ± 0.04	3.87 ± 0.04
3	3.75 ± 0.04	3.67 ± 0.04	3.89 ± 0.04
4	3.75 ± 0.04	3.68 ± 0.04	3.87 ± 0.03
5	3.73 ± 0.04	3.67 ± 0.05	3.87 ± 0.04
protein - 2	3.99 ± 0.02	4.04 ± 0.02	4.08 ± 0.01
3	3.77 ± 0.01	3.96 ± 0.05	3.95 ± 0.01
4	3.70 ± 0.01	3.99 ± 0.05	3.93 ± 0.01
5	3.69 ± 0.01	4.08 ± 0.02	3.93 ± 0.02
wine - 2	0.63 ± 0.01	0.63 ± 0.01	0.63 ± 0.01
3	0.63 ± 0.01	0.63 ± 0.01	0.63 ± 0.01
4	0.63 ± 0.01	0.63 ± 0.01	0.63 ± 0.01
5	0.63 ± 0.01	0.63 ± 0.01	0.63 ± 0.01
yacht - 2	0.42 ± 0.03	0.34 ± 0.03	0.36 ± 0.03
3	0.44 ± 0.03	0.33 ± 0.03	0.35 ± 0.03
4	0.44 ± 0.04	0.33 ± 0.03	0.35 ± 0.03
5	0.45 ± 0.03	0.35 ± 0.03	0.33 ± 0.02
31
Under review as a conference paper at ICLR 2021
Table 9: ELBOs per datapoint for our rerun of Salimbeni & Deisenroth (2017), and our implementa-
tions of local and global inducing points for deep GPs of various depths.
{dataset} - {depth}	DSVI	local inducing	global inducing
boston - 2	-0.48 ± 0.01	-0.37 ± 0.01	-0.29 ± 0.01
3	-0.49 ± 0.01	-0.38 ± 0.01	-0.28 ± 0.01
4	-0.50 ± 0.01	-0.37 ± 0.01	-0.30 ± 0.01
5	-0.52 ± 0.01	-0.37 ± 0.01	-0.29 ± 0.01
concrete - 2	-0.52 ± 0.00	-0.42 ± 0.00	-0.39 ± 0.00
3	-0.53 ± 0.00	-0.42 ± 0.00	-0.40 ± 0.01
4	-0.58 ± 0.01	-0.42 ± 0.00	-0.39 ± 0.00
5	-0.62 ± 0.00	-0.43 ± 0.01	-0.38 ± 0.00
energy - 2	0.97 ± 0.00	1.40 ± 0.02	1.47 ± 0.00
3	0.86 ± 0.01	1.42 ± 0.02	1.48 ± 0.00
4	0.73 ± 0.02	1.40 ± 0.02	1.48 ± 0.00
5	0.82 ± 0.02	1.44 ± 0.01	1.48 ± 0.00
kin8nm - 2	-0.18 ± 0.00	-0.16 ± 0.00	-0.16 ± 0.00
3	-0.18 ± 0.00	-0.16 ± 0.00	-0.16 ± 0.00
4	-0.18 ± 0.00	-0.16 ± 0.00	-0.16 ± 0.00
5	-0.19 ± 0.00	-0.16 ± 0.00	-0.16 ± 0.00
naval - 2	2.02 ± 0.27	3.34 ± 0.04	3.63 ± 0.04
3	1.87 ± 0.31	3.29 ± 0.05	3.57 ± 0.06
4	2.30 ± 0.06	3.25 ± 0.06	3.54 ± 0.06
5	2.17 ± 0.08	3.32 ± 0.04	3.51 ± 0.05
power - 2	0.02 ± 0.00	0.05 ± 0.00	0.03 ± 0.00
3	0.03 ± 0.00	0.05 ± 0.00	0.03 ± 0.00
4	0.03 ± 0.00	0.05 ± 0.00	0.04 ± 0.00
5	0.03 ± 0.00	0.05 ± 0.00	0.03 ± 0.00
protein - 2	-1.06 ± 0.00	-1.07 ± 0.00	-1.06 ± 0.00
3	-1.02 ± 0.00	-1.05 ± 0.01	-1.05 ± 0.00
4	-1.01 ± 0.00	-1.06 ± 0.01	-1.04 ± 0.00
5	-1.01 ± 0.00	-1.08 ± 0.00	-1.04 ± 0.00
wine - 2	-1.19 ± 0.00	-1.18 ± 0.00	-1.17 ± 0.00
3	-1.19 ± 0.00	-1.18 ± 0.00	-1.17 ± 0.00
4	-1.20 ± 0.00	-1.18 ± 0.00	-1.17 ± 0.00
5	-1.20 ± 0.00	-1.18 ± 0.00	-1.17 ± 0.00
yacht - 2	1.16 ± 0.01	1.78 ± 0.01	1.86 ± 0.01
3	1.09 ± 0.01	1.80 ± 0.01	1.91 ± 0.01
4	1.07 ± 0.01	1.81 ± 0.01	1.91 ± 0.01
5	1.07 ± 0.01	1.81 ± 0.01	1.89 ± 0.01
32