Under review as a conference paper at ICLR 2021
Quantifying and	Learning	Disentangled
Representations with Limited Supervision
Anonymous authors
Paper under double-blind review
Ab stract
Learning low-dimensional representations that disentangle the underlying factors
of variation in data has been posited as an important step towards interpretable
machine learning with good generalization. To address the fact that there is no
consensus on what disentanglement entails, Higgins et al. (2018) propose a formal
definition for Linear Symmetry-Based Disentanglement, or LSBD, arguing that
underlying real-world transformations give exploitable structure to data.
Although several works focus on learning LSBD representations, such methods
require supervision on the underlying transformations for the entire dataset, and
cannot deal with unlabeled data. Moreover, none of these works provide a metric
to quantify LSBD.
We propose a metric to quantify LSBD representations that is easy to compute
under certain well-defined assumptions. Furthermore, we present a method that
can leverage unlabeled data, such that LSBD representations can be learned with
limited supervision on transformations. Using our LSBD metric, our results show
that limited supervision is indeed sufficient to learn LSBD representations.
1	Introduction
Disentangled representation learning aims to create low-dimensional representations of data that
separate the underlying explanatory factors of variation in data. These representations provide an
interpretable (Sarhan et al., 2019) and useful tool for various purposes, such as noise removal (Lopez
et al., 2018), continuous learning (Achille et al., 2018), and visual reasoning (van Steenkiste et al.,
2019).
However, there is no consensus about the exact properties that characterize a disentangled represen-
tation. Higgins et al. (2018) provide a formal definition for Symmetry-Based Disentangled (SBD)
and Linearly SBD (LSBD) data representations, building upon the idea that representations should
reflect the underlying structure of the data. In particular, they argue that variability in the data comes
from transformations in the real world from which the data is observed. Having a formal definition
of disentanglement can serve as a paradigm for the evaluation of disentangled representations.
Although several methods have been proposed to learn SBD or LSBD representations, none of them
provide a clear metric for quantifying the level of disentanglement in these representations. Quessard
et al. (2020) introduce a loss term that measures the complexity of the transformations acting on their
learned representations based on the number of parameters needed, but this term does not directly
characterize disentanglement. Caselles-DUPre et al. (2019) only evaluate the performance of their
learned representations when used in a particular downstream task.
Moreover, existing methods require information about the transformation relationships among data
points for the entire training dataset. This information is used to produce models that enforce the
properties of SBD or LSBD representations and can be considered as a form of supervision. For
example, this supervision can consist of the parameters of the transformation that connects a pair of
data points, such as a rotation angle. Obtaining this supervision on the transformations for a dataset
can be an expensive task that requires expert knowledge.
In this work we focus on characterizing and quantifying LSBD and developing a method capable
of obtaining LSBD representations by using a limited amount of supervision on the transformation
properties of a dataset. The main contributions of this paper are:
1
Under review as a conference paper at ICLR 2021
1.	An easy-to-compute metric to quantify LSBD given certain assumptions (see Section 4),
which acts as an upper bound to a more general metric (derived in Appendix D).
2.	A partially supervised method to obtain LSBD representations that, during training, can
also use data without supervision on the transformation relationships.
2	Related work
The concept of disentanglement comes from the intuition that data can be described in terms of a set
of independent explanatory factors that constitute the variability of the data. In probabilistic model-
ing, these factors are interpreted as random independent unobserved latent variables that condition
the data generation process (Kulkarni et al., 2015; Higgins et al., 2016; Chen et al., 2016; 2018).
Nevertheless, there is no consensus about the exact properties that disentangled representations
should have. Evidence of this is the wide range of metrics used to characterize such representa-
tions (Locatello et al., 2018). However, a reasonable expectation is that such representations should
capture and separate the variations and properties of the data.
Recent work has turned the attention to capturing not only the independent explanatory factors of
a dataset, but also the transformations that those factors undergo in data generation. Examples of
methods that attempt to produce representations with similar transformation properties to those of
the explanatory factors of data are (Cohen & Welling, 2015; Worrall et al., 2017; Sosnovik et al.,
2019).
Transformations of the real world determine the variability of data and its structure. These so-
called symmetry transformations have long been studied in Physics (Gross, 1996) and have been
formalized with group theory. Such transformations often affect only a subset of the properties that
describe the real world and leave the rest invariant.
The connection between disentanglement and symmetries has recently been formalized by Higgins
et al. (2018) into the definitions of Symmetry-Based Disentangled (SBD) and Linearly SBD (LSBD)
representations. QUessard et al.(2θ2θ); Caselles-DUPre et al. (2019) propose methods to obtain such
SBD and/or LSBD representations, but their methods require supervision on the transformation
relationships among datapoints for the entire training dataset. Moreover, there is no clear metric for
the level of disentanglement in their methods.
3	Symmetry-Based Disentanglement
Higgins et al. (2018) provide a formal description of disentanglement that connects the symmetry
transformations affecting the real world (from which data is generated) to the internal representa-
tions of a model. The definitions are groUnded in concepts from groUp theory, for a more detailed
description of these concepts please refer to Appendix A.
These definitions assUme the following setting. W is the set of possible world states, with Underlying
symmetry transformations that are described by a group G and its action ∙ : G X W → W on W.
In particUlar, G can be decomposed as the direct prodUct of K groUps G = G1 × . . . × GK . Data
is obtained via an observation function b : W → X that maps world states to observations in
a data space X. A model’s internal representation of data is modeled with the inference function
h : X → Z that maps data to the representation space Z. Together, the observation and the inference
constitute the model’s internal representation of the real world f : W → Z with f(w) = h ◦ b(w).
The definitions for Symmetry-Based Disentangled (SBD) and Linearly SBD (LSBD) representations
formalize the requirement that a model’s internal representation f : W → Z should reflect and
disentangle the transformation properties of the real world. The definition of SBD can be found in
Appendix B. In particular, our work focuses on LSBD representations in which the transformation
properties of the model’s internal representations should be linear. The exact definition is as follows:
Linearly Symmetry-Based Disentangled (LSBD) Representations A model’s internal represen-
tation f : W → Z , where Z is a vector space, is LSBD with respect to the group decomposition
G= G1 × ... × GKif
2
Under review as a conference paper at ICLR 2021
•	there is a decomposition of the representation space Z = Zi ㊉...㊉ ZK into K vector
subspaces,
•	there are group representations for each subgroup in the corresponding vector subspace
ρk : Gk → GL(Zk), k ∈ {1, . . . , K}
•	the group representation ρ : G → GL(Z) for G in Z acts on Z as
P(g) ∙ Z = (Pι(gι) ∙ zi, ...,Pk (gκ) ∙ ZK),	(I)
forg = (g1, . . . , gK) ∈ G and z = (z1, . . . , zK) ∈ Z with gk ∈ Gk and zk ∈ Zk.
•	the map f is equivariant with respect to the actions of G on W and Z, i.e. , for all w ∈ W
and g ∈ G it holds that f(g ∙ W) = ρ(g) ∙ f(w).
Symmetry-based disentanglement with respect to the data space X We do not directly observe
W , only data observations in X, thus it is most practical to evaluate (L)SBD with respect to the
inference map h : X → Z. However, the definitions for SBD and LSBD representations are given
with respect to the world states representation map f : W → Z. If the action of G on W is regular
and b is injective though, a representation f is (L)SBD if the inference h restricted to all possible
observations is also (L)SBD. In this case, we can evaluate the same conditions for (L)SBD only for
the function h. For a more detailed description on these conditions consult Appendix C.
4	Assumptions
In this paper we assume a setting where an underlying group structure and its action on the data
space and representation space are provided as expert knowledge, which allows us to provide a
metric that quantifies Linear Symmetry-Based Disentanglement (LSBD) and a method to achieve it.
Although these assumptions limit the settings in which our disentanglement metric and method can
be used, we believe it is valuable because (1) it provides a good basis for future research into more
general metrics and methods by making clear which assumptions should be relaxed, (2) it gives
good insights into the effect of the level of supervision on transformations (as shown by our results
in Section 7), and (3) there are many practical scenarios where having this type of expert knowledge
is a reasonable assumption.
More concretely, we make the following assumptions about the available information about the data
and its transformations:
1.	Let W be the set of real world states. Let X be the data space, where observations are
obtained. Let Z be the representation space where we wish to model this data; Z is a
vector space with given basis.
2.	We have a Lie group G and a regular action of G on W that describes the underlying sym-
metries ofW. The group decomposes as the direct product of groups G = G1 × . . . × GK.
3.	The observation function b : W → X is injective. Therefore, we can define a group action
for G on the image of b, i.e. ∙ : G X b(W) → b(W) as g∙ X = b(g ∙ b-1(x)) (see Appendix C
for more details). This action is also regular. Practically, this implies that each observation
is a transformed version of another observation, and this transformation is unique.
4.	We have an action ∙ : G × Z → Z that is linear and has a known representation P : G →
GL(Z), i.e. for any g ∈ G we can determine ρ(g).
5.	The representation ρ : G → GL(Z) is linearly disentangled, i.e. there is a known decom-
position Z = Zi ㊉...㊉ Zk such that ρ(g) ∙ Z = (ρι(gι) ∙ zi,..., PK(gκ) ∙ ZK), with
gk ∈ Gk , Zk ∈ Zk for k = 1, . . . , K.
6.	There exists an embedded submanifold ZG ⊆ Z such that the action ofG on Z restricted to
ZG is regular, and ZG is invariant under the action. Only ZG will be used as the codomain
of the inference map, h : X → ZG .
Example: the special orthogonal group SO(2) of 2D rotations Let’s look at an example that
satisfies the assumptions above and can reasonably occur in practice. Consider a windmill that can
be rotated around a fixed vertical axis, with blades that can rotate independently of the windmill’s
orientation. The dataset consists of observations (i.e. 2D images) of this windmill, where the only
3
Under review as a conference paper at ICLR 2021
variability is in the rotations of the windmill and of its blades, all other aspects (e.g. camera angle,
lighting conditions, object position, etc.) are fixed. Thus, the variability in this dataset can be
described as two independent 2D rotations, see Figure 1.
Figure 1: Illustration of an LSBD representation with respect to the decomposition
G = S O(2) × SO(2) for the windmill example, 3D model by (Poly). The action of G on the real
world is decomposed as two independent transformations: the rotation of the windmill with respect
to a vertical axis and the rotation of the blades. The inference function h : X → Z maps images
into Z = R2 ㊉ R2. On Z, the action of G is the independent rotation of each vector subspace. The
action of G on ZG = S 1 × S 1 ⊆ Z is regular.
We can satisfy the assumptions above as follows:
1.	The world states W can be described by the angle of the entire windmill and the angle of
the blades, these angles are independent of each other. The data space X is the space of
images, i.e. arrays of pixel values. A suitable representation space Z will become apparent
soon.
2.	It is well-known that 2D rotations can be described by the special orthogonal group SO(2),
so the transformations underlying this dataset can be described with the group decomposi-
tion G = G1 × G2 = SO(2) × SO(2). The corresponding action is indeed regular; any
two world states are connected with one unique transformation (i.e. pair of rotations).1
3.	If the windmill looks different from each angle, and the blades are always visible, then the
observation function b : W → X is indeed injective.
4.	Each subgroup Gk = SO(2) has a representation ρk : SO(2) → GL(R2) as rotation
matrices for vectors in R2, so We can USe Z = Z1 ㊉ Z2 = R2 ㊉ R2 as the representation
space, and the direct sum P = ρ1 ㊉ ρ2 as the group representation.
5.	Defined in this Way, the representation ρ is linearly disentangled.
6.	The actions of the subgroups Gi on the subspaces Zi are not regular, but they become
regular When restricted to the unit circle or 1-sphere S1 = {z ∈ R2 : kzk = 1}, Which is a
submanifold of Zi = R2 and invariant under the action. Thus, We can use ZG = S1 × S 1
as the codomain of the inference map h, to obtain a regular group action of G on ZG .
1Note that for a windmill with e.g. 4 identical blades, a rotation of 2 radians corresponds to the identity
transformation, and angles can be defined mod 2.
4
Under review as a conference paper at ICLR 2021
5	Quantifying Linear Symmetry-Based Disentanglement
In this section we provide a metric that is easy to compute and characterizes LSBD under the as-
sumptions of Section 4, by quantifying the equivariance of the inference map h with respect to the
group action on X and Z. Moreover, in Appendix D we show that this metric is an upper bound to
a more general LSBD metric that can in theory be evaluated for any encoder.
Characterizing LSBD representations The assumptions from Section 4 ensure that all criteria
of the LSBD definition (see Section 3) are satisfied, except for equivariance of the inference map
h : X → ZG with respect to the action of G on X and Z. Thus, to evaluate whether h is disentan-
gled, We only need to check if h(g ∙ X) = g ∙ h(x) holds for all X ∈ X and g ∈ G.
Since the action ofG on X is regular (see Section 4), the transformations in a dataset X = {Xn }nN=1
(With Xn ∈ X) can be fully described using only N-1 transformations. In particular, itis convenient
to describe all other data points in terms of the first one: X = {gn ∙ χι}N=ι, where gι = e is the
group identity. Note that {gn}nN=2 can be used to describe the transformation betWeen any tWo points
Xi, Xj ∈ X (assuming access to the inverse transformations as well), since Xi = gi ∙ (g-1 ∙ Xj).
Equivariance can therefore be evaluated for X using only N - 1 comparisons, by testing whether
h(gn ∙ xi) = ρ(gn) ∙ h(Xi) for n = 2,...,N .To simplify the notation, we write the data represen-
tations as zn = h(Xn) for n = 1, . . . , N. Some rewriting shows that we can evaluate equivariance
by checking whether
z1 = p(g2 I) ∙ z2 = ... = ρ(gN1) ∙ zN.	(2)
This formulation not only gives an efficient way of checking perfect equivariance (and thus LSBD),
but also allows for an efficient way to quantify the divergence from this ideal situation, as we will
show next.
Quantifying LSBD Equation (2) shows how to check if learned data representations {zn}nN=1 are
LSBD, given the transformations {gn}nN=2, but it also provides a perspective to quantify how well
the representations are disentangled if perfect disentanglement is not achieved. The transformed
representations {ρ(g-1) ∙ Zn}N=ι should ideally all be the same, so their dispersion is a reasonable
measure to quantify equivariance (where no dispersion means perfect equivariance).
An efficient way to measure dispersion is by computing the sample variance. Fora sample {an}nN=1
in Euclidean space, this is computed as N PN=I l∣an - ak, where a =焉 PN=I an is the sample
mean. In our work however, representations are modeled in a submanifold ZG that itself is typically
not Euclidean. But by defining a metric d on this manifold, we can generalize the sample mean
with the FreChet mean a = meanN=ι(an) = argmina∈ZG PN=I d2(an, a), and then compute the
FreChet variance N PN=I d2(an a).
Therefore, we propose the following metric (lower is better) to quantify equivariance, and thus
LSBD (under the assumptions of Section 4), for a set of representations {zn}nN=1 ⊆ ZG with in-
between transformations {gn}nN=2:
C	1 3 2 / —1、	—\
LLSBD = ^N〉: d (P(gn ) ∙ zn, z),
n=1
with z := mNan(P(g—1) ∙ znj,
n=1
(3)
(4)
for some metric d defined on the manifold ZG. In particular, d should be preserved under the group
representation P for this metric to be sensible.
In the example from Section 4 we saw ZG = S1 × S1. In particular, each circle S1 = {z ∈ R2 :
lzl = 1} is a submanifold of Euclidean space. We choose Euclidean distance as the metric d, since
itis well-defined on the manifold and preserved under the group representation P. Moreover, for any
two z, z0 ∈ S1 , the squared distance between them can be efficiently computed as
d2(z,z0) = ∣z — z0∣2 = 2 ∙ (1 — zτ ∙ z0).
(5)
5
Under review as a conference paper at ICLR 2021
To compute the squared distance between points in S1 × S 1 we can simply sum up the squared
distances per submanifold S 1. With this metric, the Frechet mean has a simple solution2 3:
meNan(zn)
n=1
PN=1Zn
Il PN=1 Zn Il
(6)
6	Methodology
We present a method to learn LSBD representations (as defined in Section 3) given some transforma-
tion labels, under the assumptions in Section 4. We emphasize that there are two main components
that lead to LSBD representations:
1.	a suitable topology for the representation space ZG, to be able to model the (linear) action
of the underlying symmetry group correctly, and
2.	some supervision on transformations to ensure equivariance w.r.t. the predefined group
representation.
Our method consists ofan unsupervised and a supervised part, such that we can learn from unlabeled
data as well as from data with additional supervision on transformations. First we formalize what
we mean by transformation labels, then we outline the unsupervised and supervised parts of our
method.
Method dataset: observations with limited transformation labels For our method we assume
access to two disjoint datasets X and Y of observations in the data space X, where we have addi-
tional information on the transformations between elements in Y . More precisely, we assume that
Y consists of L batches of M data points each, where all data points in a batch can be expressed
with a known group element acting on the first member of the batch. Formally, this dataset can be
described as
Y = {{xlm : Xlm = glm ∙ xl1}m=1}l = 1,	(7)
where the first group transformation for each batch corresponds to the identity gl1 = e. The unla-
beled dataset X consists of N - M ∙ L data points with no information about the transformations
among the data points.
Unsupervised learning: VAE with suitable topology To model data representations for the un-
labelled dataset X in a submanifold ZG of Euclidean space, we use a Diffusion Variational Au-
toencoder (∆VAE) (Perez Rey et al., 2020). This is a Variational Autoencoder (Kingma & Welling,
2014; Rezende et al., 2014) that is capable of encoding data into a closed Riemannian latent space.
∆VAEs can use any closed Riemannian manifold embedded in a Euclidean space as a latent space
(or latent manifold), provided that a certain projection function from the Euclidean embedding space
into the latent manifold is known and the scalar curvature of the manifold is available. The ∆VAE
uses a parametric family of posterior approximates obtained from a diffusion process over the latent
manifold. To estimate the intractable terms of the negative ELBO, the reparameterization trick is
implemented via a random walk.
In the case of S 1 as a latent (sub)manifold, we consider the Euclidean space R2 as the embedding
space, and the projection function Π : R2 → S1 normalizes points in the embedding space: Π(z) =
∣Z∣ .3 The scalar curvature of S1 is 0. It is straightforward to extend these properties to a latent space
S1 × S1.
Supervision: enforcing equivariance with transformation labels CaSelleS-DUPre et al. (2019)
proved that (L)SBD representations cannot be inferred from a training set of unlabeled observations,
but that access to the transformations between data points is needed. They therefore use a training
set of observation pairs with a given transformation between them.
2Note that this function is not defined if || PnN=1 zn|| = 0, but this isn’t an issue in practice.
3This projection function is not defined for z = 0, but this value does not occur in practice.
6
Under review as a conference paper at ICLR 2021
However, we posit that only a limited amount of supervision is sufficient, if we have access to a
larger dataset of unlabeled observations as well. Since obtaining supervision on transformations
is typically more expensive than obtaining unsupervised observations, it is desirable to limit the
amount of supervision needed.
Therefore, we augment the unsupervised ∆VAE with a supervised method that makes use of the
transformation-labeled batches from Y. This method makes use of the same principles from Sec-
tion 5, but on a batch level instead of a full dataset. By alternating the unsupervised and supervised
training phases, we have a method that makes use of both unlabeled and transformation-labeled
observations.
Xi
xm
LKL
「mean
LRE
Figure 2: Overview of the computation flow of the supervised training method for a batch
{xm}M=ι = {gm ∙ xι}M=I(Where gι = e).
Figure 2 illustrates the supervised part of our method for a given transformation-labeled batch
{x1,x2,...,XM} = {xι, g2 ∙ χι,...,gM ∙ xι} from Y.4 The supervised part of the method uses
the same encoder and decoder netWorks from the unsupervised part, but an extra processing step
is added betWeen encoding and decoding the full batch, Which encourages equivariance W.r.t. the
given transformation-labels. The procedure of a forWard pass is as folloWs:
1.	Compute latent variables {z1, . . . , zM}, by sampling from the posteriors q(z|xm) (i.e. a
regular forWard pass through the encoder and sampling layer of a ∆VAE).
2.	Compute Z = meanM=I(P(gr-1)
• Zm).
3.	Compute ρ(gm) ∙ Z and the parameters of p(χ∣ρ(gm) • Z) for (i.e. a regular forward pass of
ρ(gm) ∙ Z through the ∆VAE decoder).
We train the full network to optimize the following loss function:
LL = LREl + LKL + Y ∙ Llsbd	(8)
MM	M
=-E log P(Xm|P(gm) • Z)+ £ KL(q(z∣Xm)∣∣p(z))+ Y ∙ E d2(Zm,p(gm) ∙ Z).	(9)
m=1	m=1	m=1
Here LRmEeal + LKL is essentially the regular ELBO used to train a ∆VAE, but instead of a single
sample from q(Z∣χm,) to compute LRE, we used the “corrected” sample ρ(gm) . Z. This encourages
the decoder to follow the required group structure.
7	Experiments & Results
We designed experiments on datasets with known group decomposition to test the following:
1.	Study LSBD representations obtained with our methodology.
4To avoid notational cluttering we omit the index l describing the batch to which each data point belongs.
7
Under review as a conference paper at ICLR 2021

(a) Square Translation	(b) Arrow Rotation	(c) ModelNet40 Airplane
Figure 3: Example images from each of the datasets used. Each image corresponds to an example
data point for a combination of two factors, e.g. color and orientation. The factors change horizon-
tally and vertically and the boundaries of each dataset example are periodic.
2. Evaluate the stability and the behavior of the method using our LSBD metric from Section 5
for a range of numbers of labeled pairs L across several training repetitions.
All datasets contain 64 × 64 pixel images, with a known group decomposition G = SO(2) × SO(2)
describing the underlying transformations. For each subgroup a fixed number of 64 transformations
is selected. Each image is generated from a single initial data point upon which all possible group
actions are applied, resulting in datasets with N = 4096 images. The datasets exemplify differ-
ent group actions of SO(2): periodic translations, in-plane rotations, out-of-plane rotations, and
periodic hue-shifts, see Figure 3. For more details, see Appendix E.
For the supervised datasets we use pairs (i.e. batches of size M = 2) of transformation-labeled data
points. From a dataset of size N , we randomly sample L ≤ N/2 batches of unique data points
without replacement. Each pair is labeled with a description of the transformation between the two
data points in the pair.
We choose M = 2 for our experiments since itis the most limited setting for our supervised method.
Higher values of M would provide stronger supervision, so successful results with M = 2 imply
that good results can also be achieved for higher values of M (but not necessarily vice versa).
In our experiments, we use a fixed weight γ = 100 and vary the number of batches L. We train
the ∆VAE for 300 epochs per experiment using the Adam optimizer, each epoch consists of a full
unsupervised and supervised phase. For each experiment 10 repetitions were trained and evaluated
by computing the LSBD metric across the training dataset. Details of the ∆VAE architecture can be
found in Appendix F.
Square Translation Dataset	Arrow Rotation Dataset	ModelNet40 Airplane Dataset
Number of Labeled Pairs
09 ZoO 寸 OgZ
InT 9 fxl 8 0"l 6
N LnZOcSJLnZ
t—I ɪ-1 ɪ-1 t—I
Number of Labeled Pairs
Number of Labeled Pairs
8寸ON
8寸0Z
8寸OZ
Figure 4: Square Translation, Arrow Rotation and Modelnet40 Airplane datasets’ box plots for the
LSBD metric evaluated over 10 training repetitions for different numbers of labeled pairs L and
γ= 100.
The results obtained for the three datasets are presented in Figure 4, using the LSBD metric from
Section 5 (lower is better). The results show that for all three datasets, only a limited amount of
supervision is needed to obtain LSBD representations consistently over multiple runs. In particular,
8
Under review as a conference paper at ICLR 2021
we observe that by providing transformation labels for disjoint pairs of data points only, our model
can learn a more general description of the underlying transformations that connect the entire dataset.
8	Conclusion & Future Work
In this work, we specified some well-defined assumptions that allow us to define an easy-to-compute
metric to quantify Linearly Symmetry-Based Disentanglement (LSBD), as defined by Higgins et al.
(2018). Moreover, we presented a method to obtain such LSBD representations, making use of
unlabeled observations as well as a limited amount of supervision on transformations.
Using our LSBD metric, we showed experimentally that our method can indeed learn LSBD repre-
sentations, using only a limited amount of supervision on transformations, by making use of unla-
beled observations as well.
Our LSBD metric and method require a number of assumptions, as explained in Section 4. This
limits the applicability of the metric and method, but also provides a clear direction towards a more
general approach; by studying how we can relax these well-formalized assumptions.
Moreover, our metric is in fact an upper bound to a more general metric (see Appendix D), which
is however less straightforward to compute. Characterizing how to compute this metric in various
situations will allow for better comparison with other methods aiming to obtain LSBD representa-
tions.
References
Alessandro Achille, Tom Eccles, Loic Matthey, Christopher P. Burgess, Nick Watters, Alexander
Lerchner, and Irina Higgins. Life-long disentangled representation learning with cross-domain
latent homologies. In Advances in Neural Information Processing Systems, pp. 9873-9883, 2018.
Hugo Caselles-Dupre, Michael Garcia Ortiz, and David Filliat. Symmetry-based disentangled rep-
resentation learning requires interaction with environments. In Advances in Neural Information
Processing Systems, pp. 4606-4615, 2019.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2615-2625, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Taco S. Cohen and Max Welling. Transformation properties of learned visual representations. In
3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Pro-
ceedings, number 3, pp. 1-11, 2015.
Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation,
Stichting Blender Foundation, Amsterdam, 2020. URL http://www.blender.org.
D. J. Gross. The role of symmetry in fundamental physics. Proceedings of the National Academy
of Sciences of the United States of America, 93(25):14256-14259, 1996. ISSN 00278424. doi:
10.1073/pnas.93.25.14256.
Brian C Hall. Lie Groups, Lie Algebras, and Representations, volume 222 of Graduate Texts
in Mathematics. Springer International Publishing, Cham, 2015. ISBN 978-3-319-13466-
6. doi: 10.1007/978-3-319-13467-3. URL http://link.springer.com/10.1007/
978-3-319-13467-3.
Irina Higgins, Loic Matthey, Xavier Glorot, Arka Pal, Benigno Uria, Charles Blundell, Shakir Mo-
hamed, and Alexander Lerchner. Early Visual Concept Learning with Unsupervised Deep Learn-
ing. 2016. URL http://arxiv.org/abs/1606.05579.
9
Under review as a conference paper at ICLR 2021
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. In International Confer-
ence on Learning Representations (ICLR), 2014. ISBN 1312.6114v10. doi: 10.1051/0004-6361/
201527329.
Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and Josh Tenenbaum. Deep convolutional
inverse graphics network. In Advances in neural information processing systems, pp. 2539-2547,
2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard Scholkopf, and Olivier
Bachem. Challenging common assumptions in the unsupervised learning of disentangled repre-
sentations. arXiv preprint arXiv:1811.12359, 2018.
Romain Lopez, Jeffrey Regier, Michael I. Jordan, and Nir Yosef. Information constraints on auto-
encoding variational Bayes. Advances in Neural Information Processing Systems, 2018-Decem
(1):6114-6125, 2018. ISSN 10495258.
Luis A. Perez Rey, Vlado Menkovski, and Jim Portegies. Diffusion variational autoencoders. In
Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-
20, pp. 2704-2710, 7 2020. doi: 10.24963/ijcai.2020/375. URL https://doi.org/10.
24963/ijcai.2020/375.
Poly. Windmill. https://poly.google.com/view/ctIRaIM3zXu. Accessed 25th Octo-
ber 2020.
Robin Quessard, Thomas D. Barrett, and William R. Clements. Learning Group Structure and
Disentangled Representations of Dynamical Environments. pp. 1-10, 2020. URL http://
arxiv.org/abs/2002.06991.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31st International Con-
ference on International Conference on Machine Learning-Volume 32, pp. II-1278, 2014.
Mhd Hasan Sarhan, Abouzar Eslami, Nassir Navab, and Shadi Albarqouni. Learning Interpretable
Disentangled Representations Using Adversarial VAEs. volume 4, pp. 37-44. Springer Interna-
tionalPublishing,2019. ISBN978-3-030-33391-1. doi: 10.1007/978-3-030-33391-1{\_}5.
N. Sedaghat and T. Brox. Unsupervised generation ofa viewpoint annotated car dataset from videos.
In IEEE International Conference on Computer Vision (ICCV), 2015.
Stefano Soatto. Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol
Conversion and the Interplay Between Sensing and Control. 2011. URL http://arxiv.org/
abs/1110.2053.
Ivan Sosnovik, MiChaI Szmaja, and Arnold Smeulders. Scale-Equivariant Steerable Networks. Inter-
national Conference on Learning Representations, ICLR 2020 - Conference Track Proceedings,
pp. 1-14, 2019. URL http://arxiv.org/abs/1910.11093.
Sjoerd van Steenkiste, Francesco Locatello, Jurgen Schmidhuber, and Olivier Bachem. Are disen-
tangled representations helpful for abstract visual reasoning? In Advances in Neural Information
Processing Systems, pp. 14245-14258, 2019.
Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Harmonic
networks: Deep translation and rotation equivariance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5028-5037, 2017.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3D ShapeNets: A Deep Representation for Volumetric Shapes. 2014. ISSN 10636919. doi:
10.1109/CVPR.2015.7298801. URL http://arxiv.org/abs/1406.5670.
10
Under review as a conference paper at ICLR 2021
Appendix
A	Preliminaries: group theory
In this appendix, we summarize some concepts from group theory that are important to understand
the main text of the paper. Group theory provides a useful language to formalize the notion of
symmetry transformations and their effects. For a more elaborate discussion we refer the reader to
the book from Hall (2015) on group theory.
Group A group is a non-empty set G together with a binary operation ◦ : G × G → G that satisfies
three properties:
1.	Associativity: For all f, g, h ∈ G, it holds that f ◦ (g ◦ h) = (f ◦ g) ◦ h.
2.	Identity: There exists a unique element e ∈ G such that for all g ∈ G it holds that e ◦ g =
g ◦ e = g.
3.	Inverse: For all g ∈ G there exists an element g-1 ∈ G such that g-1 ◦ g = g ◦ g-1 = e.
Direct product Let G and G0 be two groups. The direct product, denoted by G × G0 , is the group
with elements (g, g0) ∈ G × G0 with g ∈ G and g0 ∈ G0, and the binary operation ◦ : G × G0 →
G × G0 such that (g, g0) ◦ (h, h0) = (g ◦ h, g0 ◦ h0).
Lie group A Lie group is a group where G is a smooth manifold, this means it can be described in
a local scale with a set of continuous parameters and that one can interpolate continuously between
elements of G.
Group action Let A be a set and G a group. The group action of G on A is a function
GA : G × A → A that has the properties 5
1.	GA (e, x) = x for all a ∈ A
2.	GA(g, (GA (g0, a)) = GA(g ◦ g0, a) for all g, g0 ∈ G and a ∈ A
Regular action The action of G on A is regular if for every pair of elements a, a0 ∈ A there exists
a unique g ∈ G such that g ∙ a = a0.
Group representation A group representation of G in the vector space V is a function ρ : G →
GL(V ) (where GL(V ) is the general linear group on V) such that for all g, g0 ∈ G ρ(g ◦ g0) =
ρ(g) ◦ ρ(g0) and ρ(e) = IV , where IV is the identity matrix.
Direct sum of representations The direct sum of two representations ρ1 : G → GL(V ) in V and
P2 : G → GL(V0) in V0 is a group representation ρι ㊉ ρ2 : G → GL(V ㊉ V0) over the direct sum
V ㊉ V0, defined for V ∈ V and v0 ∈ V0 as:
(PI ㊉ P2)(g) ∙ (V, vO) = (PI(O) ∙ v,P2(g) ∙ v0)	(IO)
B	Symmetry-Based Disentanglement definition
Given the assumptions stated in Section 3, the definition for Symmetry-Based Disentangled (SBD)
representations introduced by (Higgins et al., 2018) is as follows:
Symmetry-Based Disentangled (SBD) Representations Suppose that the group G is decom-
posed as the direct product of K groups, G = G1 × . . . × GK which models the transformations of
the world states via an action ∙ : G X W → W on W. A representation f is a SBD representation
with respect to G = G1 × . . . × GK if
5To avoid notational clutter, We write GA (g,a) = g ∙ a where the set A on which g ∈ G acts can be inferred
from the context.
11
Under review as a conference paper at ICLR 2021
•	there is an action ∙ : G X Z → Z,
•	the map f : W → Z is equivariant with respect to the actions of G on W and Z, i.e.
∀w ∈ W and g ∈ G it holds that f(g ∙ W)= g ∙ f(w), and
•	there is a decomposition Z = Z1 × . . . × ZK such that each Zi is invariant to the action of
all but one subgroup. This means that for Zi ∈ Zi g ∙ Zi = Zi for all g ∈ Gj if j = i and it
is only affected by Gi .
C	Symmetry-Based Disentanglement in the data space
If b is injective, then we can define an action of G on the possible observations b(W) ⊆ X. The
injectivity of b assures unique observations for different world states (in cases where b is not injec-
tive, it is possible to address this through active sensing (Soatto, 2011), so assuming injectivity is
reasonable in practice). If b is injective then restricted to its image it is bijective and we can define
the action ∙ : G × b(W) → b(W) of G on b(W) as g ∙ x = b(g ∙ b-1(x)).
If the action of G on W is regular then b is equivariant with respect to the actions of G on W and
b(W). In such case, if h is LSBD with respect to the decomposition of G for the available data then
f is also LSBD. This means that in such a setting we only need to focus on the disentanglement of
the inference map h, as this will assure the disentanglement of the model’s representation f of the
world.
D General metric
In this appendix, we show how the disentanglement metric LLSBD in the main text (with a specific
choice for the metric d) is an upper bound for a more general LSBD metric MLSBD . The more
general metric can be defined for any encoding map from data space X to latent space Z and a data
probability measure μ on X if μ satisfies the following condition. We assume the data (probability)
measure μ equals the pushforward GX(∙, xi)#V of a probability measure V on G by the function
GX (∙,xι),i.e.
μ(A) = (Gχ(∙, xi)#v)(A)= v ({g ∈ G | Gχ(g, xi) ∈ A})
for Borel subsets A ⊂ X . Here GX denotes the action of G on X and x1 is a base point. The
situation of a discrete dataset {χn}N=ι = {gi ∙ xi}N=I corresponds to the case in which V and μ are
following empirical measures on the group G and data space X respectively
1N	1N
V = NΣδgi	μ := NΣδXi.
We define the more general LSBD metric MLSBD as follows
MLSBD :
inf
ρ∈P(G,Z)
ρ(g)-i
G
• h(g ∙ xi) 一 / ρ(g0)-1 ∙ h(g' ∙ xι)dv(g0)
G
2	dV(g)	(11)
ρ,h,μ
G
where the norm ∣∣ ∙ kρ,h,μ is a Hilbert-space norm depending on the representation ρ, the encoding
map h : X → Z, and the data measure μ. Moreover, P(G, Z) denotes the set of linear representa-
tions of G in Z. From now on, We will denote the norm as ∣∣T∣ *.
To describe the norm ∣∣∙∣* we start with an arbitrary inner product (•, •) on the linear latent space Z.
Assume that P splits in irreducible representations Pi : G → Zi where Z = Zi ㊉•…㊉ Zk for some
k ∈ N. We will define a new inner product(•，•)* on Z as follows. First of all we declare Zi and Zj
to be orthogonal with respect to(•, •)* if i = j. We denote by ∏ the orthogonal projection on Z%.
For v, w ∈ Zi , we set
hv, wi := λi-i
g∈G(P(g)
• v, P(g) • w)dm(g)
(12)
12
Under review as a conference paper at ICLR 2021
where m is the (bi-invariant) Haar measure normalized such that m(G) = 1 and set
λi :
∏i(h(x))k2 dm(g)dμ(x)
(13)
if the integral on the right-hand side is strictly positive and otherwise we set λi := 1. This construc-
tion completely specifies the new inner product, and it has the following properties:
•	the subspaces Zi are mutually orthogonal
•	ρi (g) is orthogonal on Zi for every g ∈ G, in other words ρi maps to the orthogonal group
on Zi . Moreover, ρ maps to the orthogonal group on Z. This follows directly from the
bi-invariance of the Haar measure and the definition of (∙, ∙)*.
•	If πi is the orthogonal projection to Zi , then
kπi
X
(h(x))k"μ(x)
(14)
1
if the integral on the left is strictly positive.
We now explain why LLSBD is an upper bound for MLSBD , in case Z in the main text is endowed
with the norm ∣∣ ∙ k *. Note also that in the example with SO(2) rotations in the main text, the norm
k」* is just the ordinary Euclidean norm.
First of all, because of the infimum in the definition of MLSBD, for any representation ρ considered
in the main text we find
MLSBD
≤	ρ(g)-1
G
• h(g ∙ xι) — /
G
ρ(g0)-1 • h(g0 • x1)dν(g0)
2
dν(g)
*
(15)
G
Finally, it is a property of the mean that for every c ∈ Z the right-hand side is smaller than
ρ(g)-1
G
• h(g • x1) - c2* dν(g).
(16)
Written out for a discrete dataset, letting zn := h(xn), and taking the mean from Equation (6)
C := meNan(Zn) =: Z
n=1
(17)
we find that
N
MLSBD ≤ X ∣∣P(gn)-1 • Zn - z∣∣* = LLSBD.	(18)
n=1
We will now give an alternative expression for the disentanglement metric MLSBD, since it will
more visibly relate to the definition of equivariance. Let ρ ∈ P(G, Z) be a linear representation of
G in Z. By expanding the inner product (or by using usual computation rules for expectations and
variances), we first find that
∣∣ρ(g)-1
G
• h(g • x1) -	ρ(g0)-1 • h(g0 • x1)dν(g0)
G
2
dν(g)
*
G∣∣ρ(g)-1
• h(g • x1)∣∣2* dν(g) - ∣∣∣	ρ(g)-1 • h(g• x1)dν(g)
(19)
G
2
*
2∣g∣g kρ(g)-1 • h(g •x1)
- ρ(g0)-1 • h(g0 • x1)k2*dν(g)dν(g0).
We now use that ρ maps to the orthogonal group for (•, •)*, so that we can write the same expression
as
2 GG kρ(g ◦ gτ)-1
h(((g ◦ g0-1) • g0) • x1) - h(g0 • x1)k*2dν(g)dν(g0)
(20)
This brings us to the alternative characterization of MLSBD as
MLSBD = ρ∈PnG,Z) 2 LL ∖∣P(OgT)Th(((g°g0-1")XI)-h(g0.χI)k*dν⑼dν(gO)QI)
13
Under review as a conference paper at ICLR 2021
In particular, if for every data point X there is a unique group element gχ such that X = gχ ∙ xi, the
disentanglement metric MLSBD can also be written as
ρ∈PnG Z) 1∣a JXkP(g。g-1)-1h((g ◦ g-1) ∙x) - h(x)k2dν(g)dμ(χ)	(22)
in which the equivariance condition appears prominently. The condition becomes even more appar-
ent if ν is in fact the Haar measure itself, in which case the metric equals
ρ∈pnG Z) 2 Ig JX kP(g)-1 ◦ h(g ∙ X) - h(X)II2dm⑼dμ(X)	(23)
Although this expression would also make for a natural choice of metric in general (when ν is not
necessarily the Haar measure), this choice is not directly practical as one does not have access to
data points g ∙ X for arbitrary g.
E Datasets: additional information
Square Translation This dataset consists of a set of images of a black background with a square
of 8 × 8 white pixels. The dataset is generated applying vertical and horizontal translations of the
white square considering periodic boundaries.
Arrow Rotation This dataset consists of a set of images depicting a colored arrow at a given
orientation. The dataset is generated by applying cyclic shifts of its color and in-plane rotations.
The cyclic color shifts were obtained by preselecting a fixed set of 64 colors from a circular hue
axis. The in-plane rotations were obtained by rotating the arrow along an axis perpendicular to the
picture plane over 64 predefined positions.
ModelNet40 Airplane The ModelNet40 Airplane dataset consists of a dataset of renders obtained
using Blender v2.7 Community (2020) from a 3D model of an airplane within the ModelNet40
dataset Wu et al. (2014); Sedaghat & Brox (2015). We created each image by varying two properties:
the airplane’s color and its orientation with respect to the camera. The orientation was changed via
rotation with respect to a vertical axis (out-of-plane rotation). The colors of the model were selected
from a predefined cyclic set of colors similar to the arrow rotation dataset.
F ∆VAE ARCHITECTURE
Table 1: Encoder architecture
Input shape	Layer	Output shape
(64, 64, d)	Conv2D	(64, 64, 64)-
(64, 64, 64)	MaxPool2D	(32, 32, 64)-
(32, 32, 64)	Conv2D	(32, 32, 64)-
(32, 32, 64)	MaxPool2D	(16, 16, 64)-
(16,16, 64)	Conv2D	(16, 16, 64)-
(16,16, 64)	MaxPool2D-	(8, 8, 64)
(8, 8, 64)	Flatten	8*8*64
8*8*64	Dense	^64
64	—	Dense	4+1	—
The architecture of the ∆VAE encoder and decoder are shown in Tables 1 and 2, respectively. For
grayscale images (Square Translation), d = 1, for RBG images, d = 3. All convolutional layers
use a kernel size of 3 × 3 with 64 filters, stride 1, and are followed by a ReLU activation (with the
exception of the final decoder layer, which uses a sigmoid activation to produce pixel values between
0 and 1). MaxPooling layers use have pool size of2 × 2. The number of units in the dense (or fully
connected) layers is given by their output shape. Dense layers are followed by a ReLU activation,
except for the last layer of the encoder. UpSampling layers enlarge the first two dimensions by a
factor of 2, using nearest neighbor interpolation.
14
Under review as a conference paper at ICLR 2021
Table 2: Decoder architecture
Input shape	Layer	Output shape
^4	Dense	^64
"64	Dense	8*8*64
8*8*64	Reshape	(8, 8, 64)
(8, 8, 64)	UPSamPling2D	(16, 16, 64)
(16,16, 64)	Conv2D	(16,16, 64)-
(16,16, 64)	UPSamPling2D	(32, 32, 64)-
(32, 32, 64)	Conv2D	(32, 32, 64)-
(32, 32, 64)	UPSamPling2D	(64, 64, 64)-
(64, 64, 64)—	Conv2D	(64, 64, d) 一
The first 4 dimensions of the encoder output are projected onto the latent manifold S 1 × S 1 to
represent the mean parameter μ of the posterior, the final dimension represents the parameter log t,
for which we limit the values between -10.0 and -5.0 for numerical stability. See Perez Rey et al.
(2020) for more details about these parameters and the sampling process.
15