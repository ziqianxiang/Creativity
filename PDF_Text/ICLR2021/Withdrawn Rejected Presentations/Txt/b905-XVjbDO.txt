Under review as a conference paper at ICLR 2021
Globally Injective ReLU Networks
Anonymous authors
Paper under double-blind review
Ab stract
Injectivity plays an important role in generative models where it enables inference;
in inverse problems and compressed sensing with generative priors it is a precursor
to well posedness. We establish sharp characterizations of injectivity of fully-
connected and convolutional ReLU layers and networks. First, through a layerwise
analysis, we show that an expansivity factor of two is necessary and sufficient
for injectivity by constructing appropriate weight matrices. We show that global
injectivity with iid Gaussian matrices, a commonly used tractable model, requires
larger expansivity between 3.4 and 10.5. We also characterize the stability of
inverting an injective network via worst-case Lipschitz constants of the inverse.
We then use arguments from differential topology to study injectivity of deep
networks and prove that any Lipschitz map can be approximated by an injective
ReLU network. Finally, using an argument based on random projections, we show
that an end-to-end—rather than layerwise—doubling of the dimension suffices
for injectivity. Our results establish a theoretical basis for the study of nonlinear
inverse and inference problems using neural networks.
1	Introduction
Many applications of deep neural networks require inverting them on their range. Given a neural
network N : Z → X , where X is often the Euclidean space Rm and Z is a lower-dimensional space,
the map N-1 : N (Z) → Z is only well-defined when N is injective. The issue of injectivity is
particularly salient in two applications: generative models and (nonlinear) inverse problems.
Generative networks model a complicated distribution pX over X as a pushforward of a simple
distribution pZ through N. Given an x in the range of N, inference requires computing pZ(N-1 (x))
which is well-posed only when N is injective. In the analysis of inverse problems (Arridge et al.,
2019), uniqueness of a solution is a key concern; it is tantamount to injectivity of the forward operator.
Given a forward model that is known to yield uniqueness, a natural question is whether we can design
a neural network that approximates it arbitrarily well while preserving uniqueness. Similarly, in
compressed sensing with a generative prior N and a possibly nonlinear forward operator A injective
on the range of N, we seek a latent code z such that A(N(z)) is close to some measured y = A(x).
This is again only well-posed when N can be inverted on its range (Balestriero et al., 2020). Beyond
these motivations, injectivity is a fundamental mathematical property with numerous implications.
We mention a notable example: certain injective generators can be trained with sample complexity
that is polynomial in the image dimension (Bai et al., 2018).
1.1	Our Results
In this paper we study injectivity of neural networks with ReLU activations. Our contributions can be
divided into layerwise results and multilayer results.
Layerwise results. For a ReLU layer f : Rn → Rm we derive sufficient and necessary conditions
for invertibility on the range. For the first time, we construct deterministic injective ReLU layers with
minimal expansivity m = 2n. We then derive specialized results for convolutional layers which are
given in terms of filter kernels instead of weight matrices. We also prove upper and lower bounds on
minimal expansivity of globally injective layers with iid Gaussian weights. This generalizes certain
existing pointwise results (Theorem 2 and Appendix A.2). We finally derive the worst-case inverse
1
Under review as a conference paper at ICLR 2021
Lipschitz constant for an injective ReLU layer which yields stability estimates in applications to
inverse problems.
Multilayer results. A natural question is whether injective models are sufficiently expressive.
Using techniques from differential topology we prove that injective networks are universal in the
following sense: if a neural network N1 : Z → R2n+1 models the data, Z ⊂ Rn , then we can
approximate N1 by an injective neural network N2 : Z → R2n+1. As N2 is injective, the image set
N2 (Z) is a Lipschitz manifold. We then use an argument based on random projections to show that
an end-to-end expansivity by a factor of ≈ 2 is enough for injectivity in ReLU networks, as opposed
to layerwise 2-expansivity implied by the layerwise analysis.
We conclude with preliminary numerical experiments to show that imposing injectivity improves
inference in GANs while preserving expressivity.
1.2	Why Global Injectivity?
The attribute “global” relates to global injec-
tivity of the map N : Z → Rm on the low-
dimensional latent space Z , but it does not im-
ply global invertibility over Rm , only on the
range N (Z) ⊂ Rm. If we train a GAN gen-
erator to map iid normal latent vectors to real
images from a given distribution, we expect that
any sampled latent vector generates a plausible
image. We thus desire that any N (z) be pro-
duced by a unique latent code z ∈ Z . This is
equivalent to global injectivity, or invertibility
on the range. Our results relate to the growing
literature on using neural generative models for
compressed sensing (Bora et al., 2017). They
parallel the related guarantees for sparse recov-
ery where the role of the low-dimensional latent
space is played by the set of all k-sparse vec-
tors. One then looks for matrices which map
all k-sparse vectors to distinct measurements
(Foucart & Rauhut, 2013). As an example, in
the illustration in Figure 1 images coresponding
[xι,X2,xs]τ = ReLU([wι, w2, w3]τ⅛, ¾]τ)
Figure 1: An illustration of a ReLU layer N :
R2 → R3, x = N (z), that is not globally injec-
tive. Differently colored regions in the z-space are
mapped to regions of the same color in the x-space.
While N is locally injective in the pink, blue and
green wedges in z-space, the orange, brown, and
violet wedges are mapped to coordinate axes. N is
thus not injective on these wedges. This prevents
construction of an inverse in the range of N .
to latent codes in orange, brown, and violet wedges cannot be compressively sensed. Finally, a neural
network is often trained to directly reconstruct an image x from its (compressive) low-dimensional
measurements y = A(x) without introducing any generative models. In this case, whenever A is
Lipschitz, it is immediate that the learned inverse must be injective.
1.3	Related Work
Closest to our work are the papers of Bruna et al. (2013), Hand et al. (2018) and Lei et al. (2019).
Bruna et al. (2013) study injectivity of pooling motivated by the problem of signal recovery from
feature representations. They focus on `p pooling layers; their Proposition 2.2 gives a criterion similar
to the DSS (Definition 1) and bi-Lipschitz bounds for a ReLU layer (similar to our Theorem 3).
Unlike Theorems 1 and 3, their criterion and Lipschitz bound are in some cases not precisely aligned
with injectivity; see Appendix E.1.
Compressed sensing with GAN priors requires inverting the generator on its range (Bora et al., 2017;
Shah & Hegde, 2018; Wu et al., 2019; Mardani et al., 2018; Hand et al., 2018). Lei et al. (2019)
replace the end-to-end inversion by the faster and more accurate layerwise inversion when each layer
is injective. They show that with high probability a ReLU layer with an iid normal weight matrix
can be inverted about a fixed point if the layer expands at least by a factor of 2.1. This result is
related to our Theorem 2 which gives conditions for global injectivity or layers with random matrices.
Hand & Voroninski (2017) show that when the weights of a ReLU network obey a certain weighted
distribution condition, the loss function for the inversion has a strict descent direction everywhere
2
Under review as a conference paper at ICLR 2021
except in a small set. The condition is in particular satisfied by random matrices with expansivity
nj = Ω(n7--ι log nj-ι), where n7- is the output dimension of layer j.
A continuous analogy of our convolutional construction (Definition 3) was considered by Mallat
et al. (2018). They show that ReLU acts as a phase filter and that the layer is bi-Lipschitz and hence
injective when the filters have a diverse phase and form a frame. Discretizing their model gives a
statement related to Corollary 2 and Theorem 4.
Injectivity is automatic in invertible neural networks such as normalizing flows (Grover et al., 2018;
Kingma & Dhariwal, 2018; Grathwohl et al., 2018). Specialized architectures with simple Jacobians
give easy access to the likelihood (Dinh et al., 2014; 2016; Gomez et al., 2017), which facilitates
application to inverse problems (Ardizzone et al., 2018). Inference with GANs can be achieved by
jointly training a generative network and its inverse (Donahue et al., 2016; Dumoulin et al., 2016),
which is well-defined when the generator is injective. Relaxed injective probability flows resemble
GAN generators but are trained via approximate maximum likelihood (Kumar et al., 2020). Injectivity
is promoted by keeping the smallest singular value of the Jacobian away from zero at the training
examples, a necessary but not sufficient condition. In general, Jacobian conditioning improves GAN
performance (Heusel et al., 2017; Odena et al., 2018). Finally, lack of injectivity interferes with
disentanglement (Chen et al., 2016; Lin et al., 2019). In this context, injectivity seems to be a natural
heuristic to increase latent space capacity without increasing its dimension (Brock et al., 2016).
Notation. Given a matrix W ∈ Rm×n we define the notation w ∈ W to mean that w ∈ Rn is a row
vector of W . For a matrix W ∈ Rm×n with rows {wj }jm=1 and x ∈ Rn, we write
S(x,W) := {j ∈ [[m]]: hwj,xi ≥ 0}	(1)
and Sc(x, W) for its complement, with [[m]] = {1, . . . , m}. We let NN(n, m, L, m) be the family
of functions Nθ : Rn → Rm of the form
N(Z) = WL+1φL(WL …φ2(W2φl(WIz + bl) + b2) ∙∙∙ + bL)	⑵
Indices ' = 1,...,L index the network layers, b` ∈ Rn'+1 are the bias vectors, w` ∈ Rn'+1×n'
are the weight matrices with nι = n, nL = m, and φ' are the nonlinear activation func-
tions. We will denote ReLU(x) = max(x, 0). We write n = (n1, n2, . . . nL-1) and
θ = (W1, b1, . . . , WL, bL) for the parameters that determine the function Nθ. We also write
NN (n, m, L) = Sn∈ZL-1 NN (n, m, L, n) andNN (n, m) = SL∈Z NN (n, m, L).
2	Layerwise Injectivity of ReLU Networks
For a one-to-one activation function such as a leaky ReLU or a tanh, it is easy to see that injectivity of
x 7→ Wi x implies the injectivity of the layer. We therefore focus on non-injective ReLU activations.
2.1	Directed Spanning Set
Unlike in the case of a one-to-one activation, when φ(x) = ReLU(x), x 7→ φ(W x) cannot be
injective for all x ∈ Rn if W is a square matrix. Noninjectivity of ReLU(W x) occurs in some simple
situations, for example if Wx is not full rank or if Wx ≤ 0 element-wise for a nonzero x, but these
two conditions on their own are not sufficient to characterize injectivity. Figure 2 gives an example of
a weight matrix that yields a noninjective layer that passes these two tests. In order to facilitate the
full characterization of injectivity of ReLU layers, we define a useful theoretical device:
Definition 1 (Directed Spanning Set). Let W ∈ Rn×m . We say that W has a directed spanning set
(DSS) of Ω ⊂ Rn with respect to a vector X ∈ Rn if there exists a Wr such that each row vector of
Wx is a row vector of W,
hx, Wii ≥ 0 for all Wi ∈ WX,	(3)
and Ω ⊂ SPan(Wr). When omitted Ω is understood to be Rn.
For a particular X ∈ Rn, it is not hard to verify if a given W = {wi}i=ι,...,m has a DSS of Ω. One can
simply let Wr = {wi ∈ Rn : hx, Wii ≥ 0}. Then, W has a DSS for Ω with respect to X if and only
if Wχ is a basis of Ω, which can be checked efficiently. Note that having full rank is necessary for it
3
Under review as a conference paper at ICLR 2021
Figure 2: Illustration of the DSS definition. Left: A configuration of 4 vectors in R2 that do not have
a DSS w.r.t. all x ∈ R2. In this case, the vectors do not generate an injective layer. The set of labels
indicate which wj have positive inner product with vectors in the wedge; there are two wedges with
only one such {wj}; we have ReLU(W x1) = ReLU(Wx2). Center: A configuration where four
vectors have a DSS for all x ∈ R2 . These vectors correspond to a minimally-expansive injective
layer; see Corollary 2. Right: A plot of kReLU(W x)k1 where W is given as in the left figure. Note
that x 7→ ReLU(W x) is linear in every wedge.
to have a DSS for any x ∈ Rn . It is however not sufficient. For example, Idn ∈ Rn×n , the identity
matrix in Rn, is clearly full rank, but it doesn’t have a DSS of Rn w.r.t. x = (-1, 0, . . . , 0) ∈ Rn.
To check whether W has a DSS for all x ∈ Rn, note that W partitions Rn into open wedges Sk,
Rn = Sk Sk, with constant sign patterns. That is, for x1, x2 ∈ Sk, sign(W x1) = sign(Wx2). (See
also proof of Theorem 2 in Appendix A.2.2.) Checking whether W has a DSS for all x is equivalent
to checking that for every wedge there are at least n vectors Wk ⊂ W with positive sign, hw, xi > 0
for x ∈ Sk, and that Wk spans Rn. Since the number of wedges can be exponential in m and n
(Winder, 1966) this suggests an exponential time algorithm. We also note the connection between
DSS and spark in compressed sensing (Foucart & Rauhut, 2013), defined as the size of the smallest
set of linearly dependent vectors in W. If every wedge has n or more positive signs, then full spark
n + 1 is sufficient for W to have a DSS w.r.t all x ∈ Rn. Computing spark is known to be NP-hard
(Tillmann & Pfetsch, 2013); whether one can do better for DSS remains an open question.
2.2	Fully Connected Layer
The notion of a DSS immediately leads to our main result for fully connected layers.
Theorem 1 (Conditions for Injectivity of ReLU(W x)). Let W ∈ Rm×n where n > 1 be a matrix
with row vectors {wj }jm=1, and ReLU(y) = max(y, 0). The function ReLU(W (∙)): Rn → Rm is
injective if and only if W has a DSS w.r.t every x ∈ Rn.
The question of injectivity in the case when b = 0 and when b 6= 0 are very similar and, as Lemma 1
shows, the latter question is equivalent to the former on a restricted weight matrix.
Lemma 1 (Injectivity of ReLU(Wx+b)). Let W ∈ Rm×n and b ∈ Rm. Thefunction ReLU(W(.) +
b): Rn → Rm is injective if and only if ReLU(W ∣b≥o∙) is injective, where W ∣b≥o ∈ Rm×n is row-
wise the same as W where bi ≥ 0, and is a row of zeroes when bi < 0.
Remark 1 (Injectivity of ReLU(W x), Positive x). Without normalization strategies, inputs of all but
the first layer are element-wise non-negative. One can ask whether ReLU(W x) is injective when x
is restricted to be element-wise non-negative. Following the same argument as in Theorem 1, we
find that W must have a DSS w.r.t. x ∈ Rn for every x that is element-wise non-negative. With
normalizations strategies however (for example batch renormalization (Ioffe, 2017)) the full power of
Theorem 1 and Lemma 1 may be necessary. In Appendix B we show that common normalization
strategies such as batch, layer, or group normalization do not interfere with injectivity.
Remark 2 (Global vs Restricted Injectivity). Even when the conditions of Theorem 1 and Lemma 1
are not satisfied the network might be injective in some X ⊂ Rn. Indeed, N(x) = ReLU(Idn ∙) is
in general not injective, but it is injective in X = {x ∈ Rn : xi > 0 for i = 1, . . . , n}, a convex and
open set. Theorem 1 and Lemma 1 are the precise criteria for single-layer injectivity for all x ∈ Rn .
4
Under review as a conference paper at ICLR 2021
The above layerwise results imply a sufficient condition for injectivity of deep neural networks.
Corollary 1 (layerwise Injectivity Implies End-to-End Injectivity). Let N : Rn → Rm be a deep
neural network OfthefOrm (2). Ifeach layer φ'(W' ∙ +b`): Rni → Rni+1 is injective, then so is N.
Note that Corollary 1 is sufficient but not necessary for injectivity of the entire network. Consider
for example an injective layer ReLU(W), and N(x) = ReLU(Idm ∙ ReLU(W(x))), where I is the
identity matrix. Clearly,
ReLU(Idm ∙ ReLU(W(x))) = ReLU(ReLU(W(x))) = ReLU(W(x))	(4)
so N is injective, but it fails the criterion of Corollary 1 (at the ReLU(I∙) layer).
An important implication of Theorem 1 is the following result on minimal expansivity.
Corollary 2 (Minimal Expansivity). For any W ∈ Rm×n, ReLU(W∙) is non-injective if m < 2 ∙ n.
If W ∈ R2n×n satisfies Theorem 1 and Lemma 1, than up to row rearrangement W can be written as
B
W = -DBB	(5)
where B, D ∈ Rn×n and B is a basis and D a diagonal matrix with strictly positive diagonal entries.
n, number of columns
While m < 2n immediately precludes injectivity, Corol-
lary 2 gives a simple recipe for the construction of
minimally-expansive injective layers with m = 2n. To the
best of our knowledge this is the first such result. Further,
we can also use Corollary 2 to build weight matrices for
which m > 2n. For example, the matrix
B
W = -DB ,
M
where B ∈ Rn×n has full rank, D ∈ Rn×n is a positive
diagonal matrix, and M ∈ Rm-2n×n is arbitrary, has
a DSS w.r.t. every x ∈ Rn . This method can be used
in practice to generate injective layers by using standard
spectral regularizers to ensure that B is a basis (Cisse
et al., 2017). In Section 3 we will show that, in fact, an
end-to-end rather than layerwise doubling of dimension Figure 3: A visualization of regions
is sufficient for injectivity.	where asymptotics of I(m, n) in The-
Previous work that sought injectivity uses random-weight orem 2 are valid.
models. Lei et al. (2019) show that a layer is invertible
about a point in the range provided that m ≥ 2.1n and W is iid Gaussian. In Appendix A.3 we show
that m = 2.1n is not enought to guarantee injectivity. In fact, we show that with high probability
an iid Gaussian weight matrix W yields a globally invertible ReLU layer, when W is sufficiently
expansive, and conversely does not satisfy Theorem 1 if it is not expansive by at least a factor of 3.4:
Theorem 2 (Injectivity for Gaussian Weights). Let I(m, n) = P{x 7→ ReLU(W x) is injective}
with the entries ofW iid standard normal and c = m/n fixed. Then as n → ∞,
I(m, n) → 1 if c ' 10.5 and I(m, n) → 0 if c / 3.4.
The parameter regions defined in Theorem 2 are illustrated in Figure 3.
We note that the study of neural networks with random weights is an important recent research
direction (Pennington & Worah, 2017; Benigni & PeCha 2019). Pennington & Worah (2017)
articulate this clearly, arguing that large complex systems, of which deep networks are certainly an
example, can be profitably studied by approximating their constituent parts as random variables.
2.3	Inverse Lipschitz Constant for Networks
Because ∣∣ReLU(Wx) - ReLU(Wy)k ≤ ∣∣W∣∣ ∣∣x - yk, it is clear that ReLU(W∙) is Lipschitz
with constant kW k; whether the inverse is Lipschitz, and if so, with what Lipschitz constant, is less
obvious. We can prove the following result (see Appendix C.1):
5
Under review as a conference paper at ICLR 2021
Theorem 3 (Global Inverse Lipschitz Constant). Let W ∈ Rm×n have a DSS w.r.t. every x ∈ Rn.
There exists a C(W) > 0 such that for any x0, x1 ∈ Rn,
kReLU(Wx0) - ReLU(Wx1)k2 ≥ C(W) kx0 -x1k2	(6)
where C (W) = √2= minχ∈Rn σ(W ∣s(χ,w)), and σ denotes the smallest singular value.
This result immediately yields stability estimates when solving inverse problems by inverting (injec-
tive) generative networks. We note that the (2m)-1/2 factor is essential; the naive “smallest singular
value” estimate is too optimistic.
2.4 Convolutional Layer
N2
Figure 4: A visualization of the indices
in (8) in two dimensions. The blue re-
gion is a kernel C of width O, the pink
region is the zero-padded box of width
P , and D is the offset of the kernel C
in the pink box. The entire signal is the
green box of width N .
Since convolution is a linear operator we could simply ap-
ply Theorem 1 and Lemma 1 to the convolution matrix. It
turns out, however, that there exists a result specialized to
convolution that is much simpler to verify. In this section,
capital letters such as I, J and K denote multi-indexes;
the same holds for their element-wise limits such as N and
M . Nj refers to the jth index in the multi-index N . We
use the shortcuts C ∈ RN for C ∈ RN1 × ∙∙∙ × RNp,
and PN=ICI = PNI=I …PNp=1 cIι,...,Ip, for I =
(I1, . . . , Ip). The symbol 1 can refer to number 1 or a
p-tuple (1,..., 1). Further, the notation I ≤ J means that
Ik ≤ Jk for all k = 1,...,p and likewise for ≥, < and >.1	.
If I ≤ J, then there is at least one k such that Ik > Jk.
Definition 2 (Convolution Operator). Let C ∈ RO . We
say that C is a convolution kernel of width O. Given C and
x ∈ RN, we define the convolution operator C ∈ RN×N
with stride 1 as,
O	O+J
(Cx)J =	CO-I+1xJ+I =	CO+J-I0+1xI0.
I =1	I0 =1+J
(7)
When 1 6≤ K or K 6≤ O we will set CK = 0. We do not specify the boundary condition on x
(zero-padded, periodic, or otherwise), as our results hold generally.
Definition 3 (Convolutional Layer). We say that a matrix W ∈ RM ×N is a convolution operator,
with nQ convolutions, provided that W can be written (up to row rearrangement) in the form
W = [CT CT … CTQ]T where each Ck is a convolution operator for k = 1,..., nQ. A
neural network layer for which W is a convolution operator is called a convolution layer.
Definitions 2 and 3 automatically model the standard multi-channel convolution used in practice. For
2D images of size 512 × 512, nc input channels, and 3 × 3 convolutions, we simply let x ∈ RN with
N = (512, 512, nc) andC ∈ RO with O = (3,3,nc).
To state our main result for convolutions we also need to define the set of zero-padded kernels for
C ∈ RO . Think of a multi-index as a box (or a hyperrectangle). Let P be a multi-index such that O
“fits” in P . Then we define
ZP (C) = {D ∈ RP : d is a shift of C within the box P}.	(8)
Theorem 4 (Sufficient Condition for Injectivity of Convolutional Layer). Suppose that W ∈ RM ×N
is a convolution layer with convolutions {Ck}qk=1, and corresponding kernels {Ck}qk=1. If for any P,
q
W|ZP := [ ZP(Ck)	(9)
k=1
has a DSSfor RP with respect to all X ∈ RP, then ReLU(W∙) satisfies Theorem 1.
6
Under review as a conference paper at ICLR 2021
Theorem 4 applies to multi-channel convolution of width (O, nc) = (O1, . . . , Op, nc) provided that
we choose a (P, nc) such that O ≤ P . This theorem shows that if a convolution operator W has
a DSS w.r.t. vectors with support in the pink region in Figure 4, then it has a DSS w.r.t. vectors
supported on the green region as well.
3 Universality and Expansivity of Deep Injective Networks
We now consider general properties of deep injective networks. Note that a neural network Nθ ∈
NN(n, m) is LiPschitz smooth: there is L0 > 0 such that ∣Nθ(x) - N(y)| ≤ L0|x - y| for all
x, y ∈ Rn . If Nθ : Rn → Rm is also injective, results of basic topology imply that for any bounded
and closed set B ⊂ Rn the maP Nθ : B → Nθ (B) has a continuous inverse Nθ-1 : Nθ (B) → B and
the sets B and its image Nθ(B) are homeomorPhic. Thus, for examPle, if Z is a random variable
suPPorted on the cube [0, 1]n ⊂ Rn, the sPhere Sn-1 ⊂ Rn, or the torus Tn-1 ⊂ Rn, we see that
Nθ(Z) is a random variable suPPorted on a set in Rm that is homeomorPhic to an n-dimensional cube
or an n - 1 dimensional sPhere or a torus, resPectively. This means that injective neural networks
can model random distributions on surfaces with Prescribed toPology. Moreover, if Nθ : Rn → Rm
is a decoder, all objects x ∈ Nθ(Rn) corresPond to the unique code z such that x = Nθ(z).
A fundamental ProPerty of neural networks is that they can uniformly aPProximate any continuous
function f : Z → Rm defined in a bounded set Z ⊂ Rn . For general dimensions n and m, the
injective neural networks do not have an analogous ProPerty. For instance, if f : [-π, π] → R
is the trigonometric function f(x) = sin(x), it is easy to see that there is no injective neural
network (or any other injective function) N : [-∏,∏] → R such that |f(x) - N(x)| < 1.
Consider, however, the following trick: add two dimensions in the outPut vector and consider the
maP F : [-π, π] → R3 given by F (x) = (0, 0, sin(x)) ∈ R3. When f is aPProximated by a
one-dimensional, non-injective ReLU network fθ : [-π, π] → R and α > 0 is small, then the neural
network Nθ(x) = (αφ(x), αφ(-x), fθ(x)) is an injective maP that aPProximates F.
In general, as ReLU networks are Piecewise affine maPs, it follows in the case m = n that if a ReLU
network Nθ : Rn → Rn is injective, it has to be surjective (Scholtes, 1996, Thm. 2.1). This is a
limitation of injective neural networks when m = n. Building on these ideas we show that when the
dimension of the range sPace is sufficiently large, m ≥ 2n + 1, injective neural networks become
sufficiently exPressive to universally model arbitrary continuous maPs.
Theorem 5 (Universal APProximation with Injective Neural Networks). Let f : Rn → Rm be a
continuous function, where m ≥ 2n + 1, and L ≥ 1. Then for any ε > 0 and compact subset
Z ⊂ Rn there exists a neural network Nθ ∈ NN(n, m) of depth L such that Nθ : Rn → Rm is
injective and
|f (x) - Nθ (x)| ≤ ε, for all x ∈ Z.	(10)
Before describing the Proof, we note that this result also holds for leaky ReLU networks with no
modification. The key fact used in the Proof is that the network is a Piecewise affine function.
To Prove this result, we combine the aPProximation results for neural networks (e.g., Pinkus’s density
result for shallow networks (Pinkus, 1999, Theorem 3.1) or Yarotsky’s result for deeP neural networks
(Yarotsky, 2017)), with the LiPschitz-smooth version of the generic Projector technique. This
technique from differential toPology is used for examPle to Prove the easy version of the Whitney’s
embedding theorem (Hirsch, 2012, ChaPter 2, Theorem 3.5). Related random Projection techniques
have been used earlier in machine learning and comPressed sensing (Broomhead & Kirby, 2000;
2001; Baraniuk & Wakin, 2009; Hegde et al., 2008; Iwen & Maggioni, 2013).
The Proof of Theorem 5 is based on aPPlying the above classical results to locally aPProximate the
function f : Rn → Rm by some (Possibly non-injective) ReLU-based neural network Fθ : Rn →
Rm, and augment it by adding additional variables, so that Hθ (x) = (x, Fθ (x)) is an injective maP
Hθ : Rn → Rn+m. The image of this maP, M = Hθ (Rn), is an n-dimensional, LiPschitz-smooth
submanifold of Rn+m. The dimension of m + n is larger than 2n + 1, which imPlies that for a
randomly chosen Projector P1 that maPs Rn+m to a subsPace of dimension m + n - 1, the restriction
of P1 on the submanifold M, that is, P1 : M → P1 (M), is injective. By aPPlying n random
projectors, Pi,..., Pn, we have that N = Pn ◦ Pn-I ◦ •••◦ Pi ◦ H is an injective map whose
image is in a m dimensional linear sPace. These Projectors can be multiPlied together, but are
7
Under review as a conference paper at ICLR 2021
generated sequentially. By choosing the projectors Pj in a suitable way, the obtained neural network
Nθ approximates the map f .
We point out that in the proof of Theorem 5 it is crucial that we first approximate function f by a
neural network and then apply to it a generic projection to make the neural network injective. Indeed,
doing this in the opposite order may fail as an arbitrarily small deformation (in C(Rn)) of an injective
map may produce a map that is non-injective.
Figure 5: An illustration of an injective deep
neural network that avoids expansivity as de-
scribed by Corollary 3. White trapezoids are
expansive weight matrices satisfying Theo-
rem 1, and the blue trapezoids are random
projectors that reduce dimension while pre-
serving injectivity.
The proof of Theorem 5 implies the following corol-
lary on cascaded neural networks where the dimen-
sions of the hidden layers can both increase and de-
crease (see Figure 5):
Corollary 3. Let n, m, dj ∈ Z+, j = 0, 1, . . . , 2k
be such that d0 = n, d2k = m ≥ 2n + 1 and dj ≥
2n + 1 for even indexes j ≥ 2. Let
Fk = Bk ◦ fθk ◦ Bk-1 ◦ fθk-1) ◦•••◦ Bi ◦ fθ1)
where fθ(j) : Rd2j-2 → Rd2j-1 are injective neu-
ral networks and Bj ∈ Rd2j-1 → Rd2j are ran-
dom matrices whose joint distribution is absolutely
continuous with respect to the Lebesgue measure
of Qjk=1(Rd2j ×d2j-1). Then the neural network
Fk : Rn → Rm is injective almost surely.
Observe that in Corollary 3 the weight matrices Bj
and Bj 0 may not be independent. Moreover, the dis-
tribution of the matrices Bj may be supported in an
arbitrarily small set, that is, a small random perturba-
tion of the weight matrices make the function Fk injective. Corollary 3 characterizes deep globally
injective networks, while avoiding the exponential growth in dimension as indicated by Corollary 2.
4	Numerical Experiment: Injective GANs Improve Inference
We devise a simple experiment to demonstrate that 1) one can construct injective networks using
Corollary 2 that 2) perform as well as the corresponding non-injective networks, while 3) being better
at inference problems. One way to do inference with GANs is to train a so-called inference network
Inet : X → Z jointly with the generator G : Z → X so that I(G(z)) ≈ z (Donahue et al., 2016;
Dumoulin et al., 2016); Inet is trained to invert G on its range. This is used to evaluate the likelihood
of an image x in the range of G as pZ(Inet(x)). However, for pZ (Inet(G(z)) = pZ (z) to hold, G
must be injective—otherwise the distribution of Inet (G(z)) will be different from that of z. One
would thus hope that the distribution of Inet (G(z)) will be closer to that of z if we enforce injectivity.
If that is the case and the sample quality is comparable, then we have a practical tool to improve
inference.
We use the DCGAN (Radford et al., 2015) architecture with the same hyperparameters, layer sizes
and normalization strategies for the regular GAN and the injective GAN; see Appendix F. Note
that the original DCGAN uses several ReLU conv layers with an expansivity of 2 per layer. By
Corollary 2 these layers are non-injective. Therefore, the filters of the DCGAN are modified to be of
the form [C; -s2C] in order to build an injective GAN. We train the networks to generate 64 × 64 × 3
images and draw the latent code z ∈ R256 iid from a standard normal distribution. We test on
CelebA (Liu et al., 2015) and FFHQ (Karras et al., 2019) datasets. To get a performance metric,
We fit Gaussian distributions N(μ, Σ) and N(μi∩j, ∑i∩j) to G(Z) and Ginj(z). We then compute the
Wasserstein-2 distance W2 between the distribution of Z 〜N(0, I256)and the two fitted Gaussians
using the closed-form expression for W2 betWeen Gaussians, W22(N(μ1, Σ1),N(μ2, Σ2)) = kμ1 -
μ21∣2 + Tr(∑ι + ∑2 - 2(∑ι∑2)1/2). We summarize the results in Table 1. Despite the restrictions on
the weights of injective generators, their performance on popular GAN metrics-FreChet inception
distance (FID) (Heusel et al., 2017) and inception score (IS) (Salimans et al., 2016)—is comparable
to the standard GAN while inference improves. That the generated samples are indeed comparable to
the standard GAN can be also gleaned from the from the figure in Appendix F.
8
Under review as a conference paper at ICLR 2021
Table 1: Injectivity improves inference without sacrificing performance.
Dataset	Type of G	Inception Score↑	Frechet Inception Distance]	W2(Pz,Pz))
CelebA	Injective	2.24 ± 0.09	39.33 ± 0.41	18.59
	Regular	2.22 ± 0.16	50.56 ± 0.52	33.85
FFHQ	Injective	2.56 ± 0.15	61.22 ± 0.51	9.87
	Regular	2.57 ± 0.16	47.23 ± 0.90	19.63
5	Conclusion
We derived and explored conditions for injectivity of ReLU neural networks. In contrast to prior work
which looks at random weight matrices, our characterizations are deterministic and derived from first
principles. They are also sharp in that they give sufficient and necessary conditions for layerwise
injectivity. Our results apply to any network of the form (2)—they only involve weight matrices
but make no assumptions about the architecture. We included explicit constructions for minimally
expansive networks that are injective; interestingly, this simple criterion already improves inference
in our preliminary experiments. The results on universality of injective neural networks further justify
their use in applications; they also implicitly justify the various Jacobian conditioning strategies
when learning to generate real-world data. Further, injective neural networks are topology-preserving
homeomorphisms which opens applications in computational topology and establishes connections to
tools such as self-organizing maps. Analysis of deep neural networks has an analogue in the analysis
of inverse problems where one studies uniqueness, stability and reconstruction. Uniqueness coincides
with injectivity, quantitative stability with the Lipschitz constant of the inverse, and, following Lei
et al. (2019), from a linear program we get a reconstruction in the range.
Acknowledgments
We are grateful to Daniel Paluka and Charles Clum for pointing out a problem with the bound on the
sum of binomial coefficients used to prove Theorem 2.
9
Under review as a conference paper at ICLR 2021
References
Robert A. Adams. Sobolev spaces. Academic Press [A subsidiary of Harcourt Brace Jovanovich,
Publishers], New York-London, 1975. Pure and Applied Mathematics, Vol. 65.
Lynton Ardizzone, Jakob Kruse, Sebastian Wirkert, Daniel Rahner, Eric W Pellegrini, Ralf S Klessen,
Lena Maier-Hein, Carsten Rother, and Ullrich Kothe. Analyzing inverse problems with invertible
neural networks. arXiv preprint arXiv:1808.04730, 2018.
Simon Arridge, Peter Maass, Ozan Oktem, and Carola-Bibiane Schonlieb. Solving inverse problems
using data-driven models. Acta Numerica, 28:1-174, 2019.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in gans.
arXiv preprint arXiv:1806.10586, 2018.
Randall Balestriero, Sebastien Paris, and Richard Baraniuk. Max-affine spline insights into deep
generative networks. arXiv preprint arXiv:2002.11912, 2020.
Richard G. Baraniuk and Michael B. Wakin. Random projections of smooth manifolds. Found.
Comput. Math., 9(1):51-77, 2009. ISSN 1615-3375. doi: 10.1007/s10208-007-9011-z. URL
https://doi.org/10.1007/s10208-007-9011-z.
Lucas Benigni and Sandrine P6ch6. Eigenvalue distribution of nonlinear models of random matrices.
arXiv preprint arXiv:1904.03090, 2019.
Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative
models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp.
537-546. JMLR. org, 2017.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with
introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
D. S. Broomhead and M. Kirby. A new approach to dimensionality reduction: theory and algorithms.
SIAM J. Appl. Math., 60(6):2114-2142, 2000. ISSN 0036-1399. doi: 10.1137/S0036139998338583.
URL https://doi.org/10.1137/S0036139998338583.
D. S. Broomhead and M. J. Kirby. The whitney reduction network: A method for computing
autoassociative graphs. 13(11), 2001. ISSN 0899-7667. doi: 10.1162/089976601753196049. URL
https://doi.org/10.1162/089976601753196049.
Joan Bruna, Arthur Szlam, and Yann LeCun. Signal recovery from pooling representations. arXiv
preprint arXiv:1311.4025, 2013.
Peter Burgisser and Felipe Cucker. Condition: The geometry ofnumerical algorithms, volume 349.
Springer Science & Business Media, 2013.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.
Stefan Cobzag, Radu Miculescu, and Adriana Nicolae. Lipschitz functions, volume 2241 of
Lecture Notes in Mathematics. Springer, Cham, 2019. ISBN 978-3-030-16488-1; 978-3-
030-16489-8. doi: 10.1007/978-3-030-16489-8. URL https://doi.org/10.1007/
978-3-030-16489-8.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
10
Under review as a conference paper at ICLR 2021
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Jeff Donahue, PhiliPP Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier MastroPietro, Alex Lamb, Martin Arjovsky,
and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016.
Simon Foucart and Holger Rauhut. A Mathematical Introduction to Compressive Sensing. Birkhauser
Basel, 2013. ISBN 0817649476.
Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network:
BackProPagation without storing activations. In Advances in neural information processing systems,
pp. 2214-2224, 2017.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. arXiv preprint
arXiv:1810.01367, 2018.
Aditya Grover, Manik Dhar, and Stefano Ermon. Flow-gan: Combining maximum likelihood
and adversarial learning in generative models. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Paul Hand and Vladislav Voroninski. Global guarantees for enforcing deep generative priors by
empirical risk. arXiv preprint arXiv:1705.07576, 2017.
Paul Hand, Oscar Leong, and Vlad Voroninski. Phase retrieval under a generative prior. In Advances
in Neural Information Processing Systems, pp. 9136-9146, 2018.
Chinmay Hegde, Michael Wakin, and Richard Baraniuk. Random projections for manifold learning.
In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis (eds.), Advances in Neural Information
Processing Systems 20, pp. 641-648. Curran Associates, Inc., 2008. URL http://papers.
nips.cc/paper/3191-random-projections-for-manifold-learning.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural
information processing systems, pp. 6626-6637, 2017.
Morris W Hirsch. Differential topology, volume 33. Springer Science & Business Media, 2012.
Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):
251-257, 1991.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized
models. In Advances in neural information processing systems, pp. 1945-1953, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Mark A. Iwen and Mauro Maggioni. Approximation of points on low-dimensional manifolds via
random linear projections. Inf. Inference, 2(1):1-31, 2013. ISSN 2049-8764. doi: 10.1093/imaiai/
iat001. URL https://doi.org/10.1093/imaiai/iat001.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4401-4410, 2019.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in Neural Information Processing Systems, pp. 10215-10224, 2018.
11
Under review as a conference paper at ICLR 2021
Abhishek Kumar, Ben Poole, and Kevin Murphy. Regularized autoencoders via relaxed injective
probability flow. arXiv preprint arXiv:2002.08927, 2020.
Qi Lei, Ajil Jalal, Inderjit S Dhillon, and Alexandros G Dimakis. Inverting deep generative models,
one layer at a time. In Advances in Neural Information Processing Systems, pp. 13910-13919,
2019.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural networks,
6(6):861-867, 1993.
Zinan Lin, Kiran Koshy Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr: Disentangling
generative adversarial networks with contrastive regularizers. arXiv preprint arXiv:1906.06034,
2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of International Conference on Computer Vision (ICCV), 12 2015.
J. Luukkainen and J. Vaisala. Elements of Lipschitz topology. Ann. Acad. Sci. Fenn. Ser A I
Math., 3(1):85-122, 1977. ISSN 0066-1953. doi: 10.5186/aasfm.1977.0315. URL https:
//doi.org/10.5186/aasfm.1977.0315.
StePhane Mallat, Sixin Zhang, and Gaspar Rochette. Phase harmonic correlations and convolutional
neural networks. arXiv preprint arXiv:1810.12136, 2018.
Morteza Mardani, Enhao Gong, Joseph Y Cheng, Shreyas S Vasanawala, Greg Zaharchuk, Lei Xing,
and John M Pauly. Deep generative adversarial neural networks for compressive sensing mri. IEEE
transactions on medical imaging, 38(1):167-179, 2018.
Pertti Mattila. Geometry of sets and measures in Euclidean spaces, volume 44 of Cambridge Studies
in Advanced Mathematics. Cambridge University Press, Cambridge, 1995. ISBN 0-521-46576-1;
0-521-65595-1. doi: 10.1017/CBO9780511623813. URL https://doi.org/10.1017/
CBO9780511623813. Fractals and rectifiability.
John W. Milnor and James D. Stasheff. Characteristic classes. Princeton University Press, Princeton,
N. J.; University of Tokyo Press, Tokyo, 1974. Annals of Mathematics Studies, No. 76.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Frank Morgan. Geometric measure theory: a beginner’s guide. Academic press, 2016.
Augustus Odena, Jacob Buckman, Catherine Olsson, Tom B Brown, Christopher Olah, Colin Raffel,
and Ian Goodfellow. Is generator conditioning causally related to gan performance? arXiv preprint
arXiv:1802.08768, 2018.
Jeffrey Pennington and Pratik Worah. Nonlinear random matrix theory for deep learning. In Advances
in Neural Information Processing Systems, pp. 2637-2646, 2017.
Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143-195,
1999.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in neural information processing systems, pp.
901-909, 2016.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems, pp.
2234-2242, 2016.
Stefan Scholtes. Homeomorphism conditions for coherently oriented piecewise affine mappings.
Mathematics of operations research, 21(4):955-978, 1996.
12
Under review as a conference paper at ICLR 2021
Viraj Shah and Chinmay Hegde. Solving linear inverse problems using gan priors: An algorithm
with provable guarantees. In 2018 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP),pp. 4609-4613. IEEE, 2018.
Andreas M Tillmann and Marc E Pfetsch. The computational complexity of the restricted isometry
property, the nullspace property, and related concepts in compressed sensing. IEEE Transactions
on Information Theory, 60(2):1248-1259, 2013.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
RO Winder. Partitions of n-space by hyperplanes. SIAM Journal on Applied Mathematics, 14(4):
811-818, 1966.
Yan Wu, Mihaela Rosca, and Timothy Lillicrap. Deep compressed sensing. arXiv preprint
arXiv:1905.06723, 2019.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
13