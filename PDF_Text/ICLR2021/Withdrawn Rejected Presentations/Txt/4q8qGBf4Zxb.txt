Under review as a conference paper at ICLR 2021
Network Architecture Search for Domain
Adaptation
Anonymous authors
Paper under double-blind review
Ab stract
Deep networks have been used to learn transferable representations for domain
adaptation. Existing deep domain adaptation methods systematically employ pop-
ular hand-crafted networks designed specifically for image-classification tasks,
leading to sub-optimal domain adaptation performance. In this paper, we present
Neural Architecture Search for Domain Adaptation (NASDA), a principle frame-
work that leverages differentiable neural architecture search to derive the optimal
network architecture for domain adaptation task. NASDA is designed with two
novel training strategies: neural architecture search with multi-kernel Maximum
Mean Discrepancy to derive the optimal architecture, and adversarial training
between a feature generator and a batch of classifiers to consolidate the feature
generator. We demonstrate experimentally that NASDA leads to state-of-the-art
performance on several domain adaptation benchmarks.
1 Introduction
Supervised machine learning models (Φ) aim to minimize the empirical test error ((Φ(x), y)) by
optimizing Φ on training data (x) and ground truth labels (y), assuming that the training and testing
data are sampled i.i.d from the same distribution. While in practical, the training and testing data
are typically collected from related domains under different distributions, a phenomenon known as
domain shift (or domain discrepancy) (Quionero-Candela et al., 2009). To avoid the cost of annotating
each new test data, Unsupervised Domain Adaptation (UDA) tackles domain shift by transferring the
knowledge learned from a rich-labeled source domain (P (xs, ys)) to the unlabeled target domain
(Q(xt)). Recently unsupervised domain adaptation research has achieved significant progress with
techniques like discrepancy alignment (Long et al., 2017; Tzeng et al., 2014; Ghifary et al., 2014;
Peng & Saenko, 2018; Long et al., 2015; Sun & Saenko, 2016), adversarial alignment (Xu et al.,
2019a; Liu & Tuzel, 2016; Tzeng et al., 2017; Liu et al., 2018a; Ganin & Lempitsky, 2015; Saito
et al., 2018; Long et al., 2018), and reconstruction-based alignment (Yi et al., 2017; Zhu et al., 2017;
Hoffman et al., 2018; Kim et al., 2017). While such models typically learn feature mapping from one
domain (Φ(xs)) to another (Φ(xt)) or derive a joint representation across domains (Φ(xs) 0 Φ(xt)),
the developed models have limited capacities in deriving an optimal neural architecture specific for
domain transfer.
To advance network designs, neural architecture search (NAS) automates the net architecture engi-
neering process by reinforcement supervision (Zoph & Le, 2017) or through neuro-evlolution (Real
et al., 2019a). Conventional NAS models aim to derive neural architecture α along with the net-
work parameters w, by solving a bilevel optimization problem (Anandalingam & Friesz, 1992):
Φα,w = argmina Lval(W*(α), α) s.t. w*(α) = argminwLtrain(w, α),where Ltrain and LvaI in-
dicate the training and validation loss, respectively. While recent works demonstrate competitive
performance on tasks such as image classification (Zoph et al., 2018; Liu et al., 2018c;b; Real et al.,
2019b) and object detection (Zoph & Le, 2017), designs of existing NAS algorithms typically assume
that the training and testing domain are sampled from the same distribution, neglecting the scenario
where two data domains or multiple feature distributions are of interest.
To efficiently devise a neural architecture across different data domains, we propose a novel learning
task called Neural Architecture Search for Domain Adaptation (NASDA). The ultimate goal of
NASDA is to minimize the validation loss of the target domain (Ltval). We postulate that a solution
to NASDA should not only minimize validation loss of the source domain (Lsval), but should also
1
Under review as a conference paper at ICLR 2021
Figure 1: An overview of NASDA: (a) Continuous relaxation of the research space by placing a
mixture of the candidate operations on each edge. (b) Inducing the final architecture by joint opti-
mization of the neural architecture parameters α and network weights w , supervised by minimizing
the validation loss on the source domain and reducing the domain discrepancy. (c)(d) Adversarial
training of the derive feature generator G and classifiers C.
reduce the domain gap between the source and target. To this end, we propose a new NAS learning
schema:
Φα,w = argminαLval(W*(α),α) + disc(Φ*(Xs), Φ*(Xt))	⑴
s.t. w* (α) = argminw Lsrain(w,α)	(2)
where Φ* = Φα,w*(α), and disc(Φ*(xs), Φ*(xt)) denotes the domain discrepancy between the
source and target. Note that in unsupervised domain adaptation, Lttrain and Ltval cannot be computed
directly due to the lack of label in the target domain.
Inspired by the past works in NAS and unsupervised domain adaptation, we propose in this paper an
instantiated NASDA model, which comprises of two training phases, as shown in Figure 1. The first is
the neural architecture searching phase, aiming to derive an optimal neural architecture (α*), following
the learning schema of Equation 1,2. Inspired by Differentiable ARchiTecture Search (DARTS) (Liu
et al., 2019a), we relax the search space to be continuous so that α can be optimized with respect to
Lsval and disc(Φ(xs), Φ(xt)) by gradient descent. Specifically, we enhance the feature transferability
by embedding the hidden representations of the task-specific layers to a reproducing kernel Hilbert
space where the mean embeddings can be explicitly matched by minimizing disc(Φ(xs), Φ(xt)). We
use multi-kernel Maximum Mean Discrepancy (MK-MMD) (Gretton et al., 2007) to evaluate the
domain discrepancy.
The second training phase aims to learn a good feature generator with task-specific loss, based on
the derived α* from the first phase. To establish this goal, we use the derived deep neural network
(Φα* ) as the feature generator (G) and devise an adversarial training process between G and a batch
of classifiers C . The high-level intuition is to first diversify C in the training process, and train G to
generate features such that the diversified C can have similar outputs. The training process is similar
to Maximum Classifier Discrepancy framework (MCD) (Saito et al., 2018) except that we extend
the dual-classifier in MCD to an ensembling of multiple classifiers. Experiments on standard UDA
benchmarks demonstrate the effectiveness of our derived NASDA model in achieving significant
improvements over state-of-the-art methods.
Our contributions of this paper are highlighted as follows:
•	We formulate a novel dual-objective task of Neural Architecture Search for Domain Adap-
tation (NASDA), which optimize neural architecture for unsupervised domain adaptation,
concerning both source performance objective and transfer learning objective.
•	We propose an instantiated NASDA model that comprises two training stages, aiming to
derive optimal architecture parameters α* and feature extractor G, respectively. We are the
first to show the effectiveness of MK-MMD in NAS process specified for domain adaptation.
• Extensive experiments on multiple cross-domain recognition tasks demonstrate that NASDA
achieves significant improvements over traditional unsupervised domain adaptation models
as well as state-of-the-art NAS-based methods.
2
Under review as a conference paper at ICLR 2021
2	Related Work
Deep convolutional neural network has been dominating image recognition task. In recent years,
many handcrafted architectures have been proposed, including VGG (Simonyan & Zisserman, 2014),
ResNet (He et al., 2016), Inception (Szegedy et al., 2015), etc., all of which verifies the importance
of human expertise in network design. Our work bridges domain adaptation and the emerging field of
neural architecture search (NAS), a process of automating architecture engineering technique.
Neural Architecture Search Neural Architecture Search has become the mainstream approach
to discover efficient and powerful network structures (Zoph & Le, 2017; Zoph et al., 2018). The
automatically searched architectures have achieved highly competitive performance in tasks such
as image classification (Liu et al., 2018c;b), object detection (Zoph et al., 2018), and semantic
segmentation (Chen et al., 2018). Reinforce learning based NAS methods (Zoph & Le, 2017; Tan
et al., 2019; Tan & Le, 2019) are usually computational intensive, thus hampering its usage with
limited computational budget. To accelerate the search procedure, many techniques has been proposed
and they mainly follow four directions: (1) estimating the actual performance with lower fidelities.
Such lower fidelities include shorter training times (Zoph et al., 2018; Zela et al., 2018), training on a
subset of the data (Klein et al., 2017), or on lower-resolution images. (2) estimating the performance
based on the learning curve extrapolation. Domhan et al. (2015) propose to extrapolate initial learning
curves and terminate those predicted to perform poorly. (3) initializing the novel architectures based
on other well-trained architectures. Wei et al. (2016) introduce network morphisms to modify an
architecture without changing the network objects, resulting in methods that only require a few GPU
days (Elsken et al., 2017; Cai et al., 2018a; Jin et al., 2019; Cai et al., 2018b). (4) one-shot architecture
search. One-shot NAS treats all architectures as different subgraphs of a supergraph and shares
weights between architectures that have edges of this supergraph in common (Saxena & Verbeek,
2016; Liu et al., 2019b; Bender, 2018). DARTS (Liu et al., 2019a) places a mixture of candidate
operations on each edge of the one-shot model and optimizes the weights of the candidate operations
with a continuous relaxation of the search space. Inspired by DARTS (Liu et al., 2019a), our model
employs differentiable architecture search to derive the optimal feature extractor for unsupervised
domain adaptation.
Domain Adaptation Unsupervised domain adaptation (UDA) aims to transfer the knowledge learned
from one or more labeled source domains to an unlabeled target domain. Various methods have
been proposed, including discrepancy-based UDA approaches (Long et al., 2017; Tzeng et al., 2014;
Ghifary et al., 2014; Peng & Saenko, 2018), adversary-based approaches (Liu & Tuzel, 2016; Tzeng
et al., 2017; Liu et al., 2018a), and reconstruction-based approaches (Yi et al., 2017; Zhu et al.,
2017; Hoffman et al., 2018; Kim et al., 2017). These models are typically designed to tackle single
source to single target adaptation. Compared with single source adaptation, multi-source domain
adaptation (MSDA) assumes that training data are collected from multiple sources. Originating from
the theoretical analysis in (Ben-David et al., 2010; Mansour et al., 2009; Crammer et al., 2008),
MSDA has been applied to many practical applications (Xu et al., 2018; Duan et al., 2012; Peng
et al., 2019). Specifically, Ben-David et al. (2010) introduce an H∆H-divergence between the
weighted combination of source domains and a target domain. These models are developed using the
existing hand-crafted network architecture. This property limits the capacity and versatility of domain
adaptation as the backbones to extract the features are fixed. In contrast, we tackle the UDA from a
different perspective, not yet considered in the UDA literature. We propose a novel dual-objective
model of NASDA, which optimize neural architecture for unsupervised domain adaptation. We are
the first to show the effectiveness of MK-MMD in NAS process which is designed specifically for
domain adaptation.
3	Neural Architecture Search for Domain Adaptation
In unsupervised domain adaptation, we are given a source domain Ds = {(xis, yis)}in=s1 of ns labeled
examples and a target domain Dt = {xtj}jn=t 1 ofnt unlabeled examples. The source domain and target
domain are sampled from joint distributions P(xs, ys) and Q(xt, yt), respectively. The goal of this
paper is to leverage NAS to derive a deep network G : x 7→ y, which is optimal for reducing the shifts
in data distributions across domains, such that the target risk Et (G) = E(χt,yt)〜Q [G (Xt) = yt] is
minimized. We will start by introducing some preliminary background in Section 3.1. We then
describe how to incorporate the MK-MMD into the neural architecture searching framework in
3
Under review as a conference paper at ICLR 2021
Section 3.2. Finally, we introduce the adversarial training between our derived deep network and a
batch of classifiers in Section 3.3. An overview of our model can be seen in Algorithm 1.
3.1	Preliminary: DARTS
In this work, we leverage DARTS (Liu et al., 2019a) as our baseline framework. Our goal is to search
for a robust cell and apply it to a network that is optimal to achieve domain alignment between Ds
and Dt . Following Zoph et al. (2018), we search for a computation cell as the building block of the
final architecture. The final convolutional network for domain adaptation can be stacked from the
learned cell. A cell is defined as a directed acyclic graph (DAG) of L nodes, {xi}iN=1, where each
node x(i) is a latent representation and each directed edge e(i,j) is associated with some operation
o(i,j) that transforms x(i). DARTS (Liu et al., 2019a) assumes that cells contain two input nodes and
a single output node. To make the search space continuous, DARTS relaxes the categorical choice of
a particular operation to a softmax over all possible operations and is thus formulated as:
Mx)=X P Ti:)""O(X)	⑶
o∈O	o0∈O exp(αo0 )
where O denotes the set of candidate operations and i < j so that skip-connect can be applied. An
intermediate node can be represented as xj = Pi<j O(i,j) (xi). The task of architecture search then
reduces to learning a set of continuous variables α = {α(i,j)}. At the end of search, a discrete
architecture can be obtained by replacing each mixed operation θ(i,j) with the most likely operation,
i.e., o*(i,j) = argmaχo∈o αθi,j) and α* = {o*(i,j)}.
3.2	Searching Neural Architecture
Denote by Ltrain and Lval the training loss and validation loss, respectively. Conventional neural
architecture search models aim to derive Φα,w by solving a bilevel optimization problem (Anan-
dalingam & Friesz, 1992): Φα,w = argmi□a Lval(w*(α), α) s.t. w*(α) = argmi□wLtrain(W,α).
While recent work (Zoph et al., 2018; Liu et al., 2018c) have show promising performance on tasks
such as image classification and object detection, the existing models assume that the training data
and testing data are sampled from the same distributions. Our goal is to jointly learn the architecture
α and the weights w within all the mixed operations (e.g. weights of the convolution filters) so that
the derived model Φw* α* can transfer knowledge from Ds to Dt with some simple domain adapation
,
guidence. Initialized by Equation 1, we leverage multi-kernel Maximum Mean Discrepancy (Gretton
et al., 2007) to evaluate disc(Φ*(xs), Φ*(xt).
MK-MMD Denote by Hk be the Reproducing Kernel Hilbert Space (RKHS) endowed with a
characteristic kernel k. The mean embedding of distribution P in Hk is a unique element μk (P)
such that Ex〜Pf (x) = hf (x), μk (P)〉" for all f ∈ Hk. The MK-MMD dk (P, Q) between
probability distributions P and Q is defined as the RKHS distance between the mean embeddings of
P and Q. The squared formulation of MK-MMD is defined as
dk (P, Q) , IlEP [Φα (xs)] - EQ [Φα (xt)] ∣∣Hk .	(4)
In this paper, we consider the case of combining Gaussian kernels with injective functions fΦ,
where k(x, x0) = exp(-kfΦ(x) - fΦ(x)0k2). Inspired by Long et al. (2015), the characteristic
kernel associated with the feature map Φ, k (xs, xt) = hΦ (xs) , Φ (xt)i, is defined as the convex
combination of n positive semidefinite kernels {ku },
)
k
K
nn
βuku :	βu = 1,βu > 0,∀u
u=1	u=1
(5)
where the constraints on {βu} are imposed to guarantee that the k is characteristic. In practice we
use finite samples from distributions to estimate MMD distance. Given Xs = {x1, •…，xm}〜P
and Xt = {x1,…，xm}〜Q, one estimator of dk (P, Q) is
1	21
d2(P,Q) = Tmy Ek(X i,X i) - Tmy Ek(Xi,xj) + Tmy Ek(X j,X j)∙	⑹
2 i6=i0	2 i6=j	2 j6=j 0
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Neural Architecture Search for Domain Adaptation
Phase I: Searching Neural Architecture
1:	Create a mixed operation o(i,j) parametrized by α(i,j) for each edge (i, j)
2:	while not converged do
3：	UPdate architecture a by ∂dα Lval (W - ξ ∂dw Lrain(W, α), α) + λ 聂(dk &(XS), φ(Xt)))
4:	Update weights W by descending 岛LSrain (w, α)
5： end while
6： Derive the final architecture based on the learned α*.
Phase II: Adversarial Training for Domain Adaptation
1:	Stack feature generator G based on a*, initialize classifiers C
2:	while not converged do
3:	Step one: Train G and C with Ls(xs, ys) = -E(χs,ys)〜Ds PK=I 1[k=ys] logp(ys∣xs)
4:	Step two: Fix G, train C with loss: LS (XS, yS) - Ladv(Xt)(Eq. 13)
5:	Step three: Fix C, train G with loss: Ladv(xt)
6:	end while
The merit of multi-kernel MMD lies in its differentiability such that it can be easily incorporated into
the deep network. However, the computation of the 或(P, Q) incurs a complexity of O(m2), which
is undesirable in the differentiable architecture search framework. In this paper, we use the unbiased
estimation of MK-MMD (Gretton et al., 2012) which can be computed with linear complexity.
NAS for Domain Adaptation Denote by LtSrain and LSval the training loss and validation loss on the
source domain, respectively. Both losses are affected by the architecture α as well as by the weights W
in the network. The goal for NASDA is to find a* that minimizes the validation loss Cval(W*, α*) on
the target domain, where the weights w* associated with the architecture are obtained by minimizing
the training loss W* = argminw LtSrain (W, α*). Due to the lack of labels in the target domain, it is
prohibitive to compute Ltval directly, hampering the assumption of previous gradient-based NAS
algorithms (Liu et al., 2019a; Chen et al., 2019). Instead, we derive α* by minimizing the validation
loss LSval (W*, α*) on the source domain plus the domain discrepancy, disc(Φ(xS), Φ(xt)), as shown
in Equation 1.
Inspired by the gradient-based hyperparameter optimization (Franceschi et al., 2018; Pedregosa, 2016;
Maclaurin et al., 2015), we set the architecture parameters α as a special type of hyperparameter.
This implies a bilevel optimization problem (Anandalingam & Friesz, 1992) with α as the upper-level
variable and W as the lower-level variable. In practice, we utilize the MK-MMD to evaluate the
domain discrepancy. The optimization can be summarized as follows:
φα,w = argminα(Lval(W*(α),α) + λ碌(φ(xs ), φ(Xt)))	⑺
s.t. W* (α) = argminw LtSrain(W, α)	(8)
where λ is the trade-off hyperparameter between the source validation loss and the MK-MMD loss.
Approximate Architecture Search Equation 7,8 imply that directly optimizing the architecture
gradient is prohibitive due to the expensive inner optimization. Inspired by DARTS (Liu et al.,
2019a), we approximate W* (α) by adapting W using only a single training step, without solving the
optimization in Equation 8 by training until convergence. This idea has been adopted and proven to
be effective in meta-learning for model transfer (Finn et al., 2017), gradient-based hyperparameter
tuning (Luketina et al., 2016) and unrolled generative adversarial networks. We therefore propose a
simple approximation scheme as follows:
∂α (Lval(W*(α),α) + λ碌(Φ(xs), Φ(xt)))
∂∂	∂
≈∂αLval (W - ξ∂WLtrain(W, a), α) + 入赤("k (φ(x ), φ(X ))；	⑼
where W -ξ∂ww LSr ain (W, α) denotes weight for one-step forward model and ξ is the learning rate
for a step of inner optimization. Note Equation 9 reduces to VaLval (w, α) if W is already a local
optimum for the inner optimization and thus VWLtrain(w, α) = 0.
5
Under review as a conference paper at ICLR 2021
dil_conv_5x5
Skip_Connect	JTlz
口k-21 l2sep_conv 3x3^^^=^
∖ sep_conv_3x3j ------∖
VζskipZconnect	3 一
p^^u_Conv_3x3t^J
\sep_conv_3x3^/
max_pool_3x3	_
dil_conv_5x
skip connect
sep_conv_3x3
I c_{k-1} |3aVg_pooi_3x3
max_pool_3x3
max_pool_3x3
I c_{k-2} |^avg_pool_3x3
(a) Normal cells (left) and Reduce cells (right) for STL→CIFAR10
(b) Normal cells (upper) and Reduce cells (lower) for MNIST→USPS
Method	Params (M)	Search Cost (GPU days)
NASNet	3.3	1,800
AmoebaNet	3.2	3,150
PNAS	3.2	225
DARTS	3.3	1.5
SNAS	2.8	1.5
PDARTS	3.4	0.3
NASDA	2.7	0.3
(c) Network architecture statis-
tics comparison
Figure 2: (a) Neural architecture for STL→CIFAR10 task. (b) Neural architecture for MNIST→USPS
results. (c) Comparison between our NASDA model and state-of-the-art NAS models.
The second term of Equation 9 can be computed directly with some forward and backward passes.
For the first term, applying chain rule to the approximate architecture gradient yields
∂ s 0	∂2	s	∂ s 0
∂α Lval (w,α) -ξ(∂ɑ∂W Ltrain (W, Q) ∂W0Lval (W, Q))	(IO)
where w0 = W-ξ悬Ltrain (w, α). The expression above contains an expensive matrix-vector product
in its second term. We leverage the central difference approximation to reduce the computation
complexity. Specifically, let η be a small scalar and w± = W ± η∂WLval(w0, α). Then:
d 2 CS	d ss 0 0 ∖	∂a Ltrain(W+, Q) - ∂α Ltrain(W , Q)
∂Q∂W L"n(W，Q) ∂W0 S(W，Q) ≈--------------------------2η----------------- (II)
Evaluating the central difference only requires two forward passes for the weights and two backward
passes for Q, reducing the complexity from quadratic to linear.
3.3 Adversarial Training for Domain Adaptation
By neural architecture searching from Section 3.2, we have derived the optimal cell structure (q*)
for domain adaptation. We then stack the cells to derive our feature generator G. In this section, we
describe how do we consolidate G by an adversarial training of G and the classifiers C . Assume C
includes N independent classifiers {C(i)}iN=1 and denote pi(y|x) as the K -way propabilistic outputs
of C(i), where K is the category number.
The high-level intuition is to consolidate the feature generator G such that it can make the diversified
C generate similar outputs. To this end, our training process include three steps: (1) train G and C on
DS to obtain task-specific features, (2) fix G and train C to make {C(i)}iN=1 have diversified output,
(3) fix C and train G to minimize the output discrepancy between C. Related techniques have been
used in Saito et al. (2018); Kumar et al. (2018).
First, we train both G and C to classify the source samples correctly with cross-entropy loss. This
step is crucial as it enables G and C to extract the task-specific features. The training objective is
min LS (xS , yS ) and the loss function is defined as follows:
K
Ls (XS, ys ) = -E(Xs ,ys)〜Ds X 1[k=ys] logp(yS|xS)	(12)
k=1
6
Under review as a conference paper at ICLR 2021
In the second step, we are aiming to diversify C. To establish this goal, we fix G and train C to
increase the discrepancy of C’s output. To avoid mode collapse (e.g. C(1) outputs all zeros and C(2)
output all ones), we add Ls(xs, ys) as a regularizer in the training process. The high-level intuition
is that we do not expect C to forget the information learned in the first step in the training process.
The training objective is min Ls(xs, ys) - Ladv(xt), where the adversarial loss is defined as:
N-1 N
Ladv(Xt) = Ext〜Dt X X k(pi(y∣xt)-Pj(y∣xt)kι	(13)
i=1 j=i+1
In the last step, we are trying to consolidate the feature generator G by training G to extract
generalizable representations such that the discrepancy of C’s output is minimized. To achieve this
goal, we fix the diversified classifiers C and train G with the adversarial loss (defined in Equation 13).
The training objective is min Ladv(Xt).
4 Experiments
We compare the proposed NASDA model with many state-
of-the-art UDA baselines on multiple benchmarks. In the
main paper, we only report major results; more details
are provided in the supplementary material. All of our
experiments are implemented in the PyTorch platform.
In the architecture search phase, we use λ=1 for all the
searching experiments. We leverage the ReLU-Conv-BN
order for convolutional operations, and each separable
convolution is always applied twice. Our search space
O includes the following operations: 3 × 3 and 5 × 5
separable convolutions, 3 × 3 and 5 × 5 dilated separa-
ble convolutions, 3 × 3 max pooling, identity, and zero.
Our convolutional cell consists of N = 7 nodes. Cells
located at the 3 and 3 of the total depth of the network
are reduction cells. The architecture encoding therefore is
Method	STL → CIFAR10
DANN	56.9
MCD	69.2
DWT	71.2
SE	74.2
G2A	72.8
VADA	73.5
DIRT-T	75.3
NASNet+Phase II	67.3
AmoebaNet+Phase II	67.0
DARTS+Phase II	68.8
PDARTS+Phase II	66.0
NASDA	76.8
Table 1: Accuracy (%) STL→CIFAR-10
(αnormal , αreduce ), where αnormal is shared by all the normal cells and αreduce is shared by all the
reduction cells.
4.1	Setup
Digits We investigate three digits datasets: MNIST, USPS, and Street View House Numbers (SVHN).
We adopt the evaluation protocol of CyCADA (Hoffman et al., 2018) with three transfer tasks: USPS
to MNIST (U → M), MNIST to USPS (M → U), and SVHN to MNIST (S → M). We train our
model using the training sets: MNIST (60,000), USPS (7,291), standard SVHN train (73,257).
STL→CIFAR10 Both CIFAR10 (Krizhevsky et al., 2009) and STL (Coates et al., 2011) are both
10-class image datasets. These two datasets contain nine overlapping classes. We remove the ‘frog’
class in CIFAR10 and the ‘monkey’ class in STL datasets as they have no equivalent in the other
dataset, resulting in a 9-class problem. The STL images were down-scaled to 32×32 resolution to
match that of CIFAR10.
SYN SIGNS→GTSRB We evaluated the adaptation from synthetic traffic sign dataset called SYN
SIGNS (Moiseev et al., 2013) to real-world sign dataset called GTSRB (Stallkamp et al., 2011).
These datasets contain 43 classes.
We compare our NASDA model with state-of-the-art DA methods: Deep Adaptation Network
(DAN) (Long et al., 2015), Domain Adversarial Neural Network (DANN) (Ganin & Lempitsky,
2015), Domain Separation Network (DSN) (Bousmalis et al., 2016), Coupled Generative Adver-
sarial Networks (CoGAN) (Liu & Tuzel, 2016), Maximum Classifier Discrepancy (MCD) (Saito
et al., 2018), Generate to Adapt (G2A) (Sankaranarayanan et al., 2018), Stochastic Neighborhood
Embedding (d-SNE) (Xu et al., 2019b), Associative Domain Adaptation (ASSOC) (Haeusser et al.,
2017).
7
Under review as a conference paper at ICLR 2021
Method	M→U	U→M	S→M	Avg(digits)	Method	SYN SIGNS → GTSRB
DAN	81.1	-	71.1	76.1	DAN	91.1
DANN	85.1	73.0	71.1	76.4	DANN	88.7
DSN	91.3*	-	82.7	-	DSN	93.1
CoGAN	91.2*	89.1	-	-	CORAL	86.9
MCD	94.2	94.1	96.2	94.8	MCD	94.4
G2A	95.0	90.8	92.4	92.7	ASSOC	82.8
SBADA-GAN	95.3	97.6	76.1	89.7	SRDA-RAN	93.6
d-SNE	99.0	98.7	96.5	98.1	DADRL	94.6
NASDA	98.0	98.7	98.6	984	NasdA	96.7
Table 2: Accuracy (%) on Digits and Traffic Signs for unsupervised domain adaptation.
4.2	Empirical Results
Neural Architecture Search Results We show the neural architecture search results in Figure 2. We
can observe that our model contains more “avg_pool” and “5x5_conv” layer than other NAS model.
This will make our model more generic as the average pooling method smooths out the image and
hence the model is not congested with sharp features and domain-specific features. We also show
that our NASDA model contains less parameters and takes less time to converge compared with
state-of-the-art NAS architectures. Another interesting finding is that our NASDA contains more
sequential connections in both Normal and Reduce cells when trained on MNIST→USPS.
Unsupervised Domain Adaptation Results The UDA results for Digits and SYN SIGNS→GTSRB
are reported in Table 2, with results of baselines directly reported from the original papers if the
protocol is the same (numbers with * indicates training on partial data). The NASDA model
achieves a 98.4% average accuracy for Digits dataset, outperforming other baselines. For SYN
SIGNS→GTSRB task, our model gets comparable results with state-of-the-art baselines. The results
demonstrate the effectiveness of our NASDA model on small images.
The UDA results on the STL→CIFAR10 recognition task are reported in Table 1. Our model achieves
a performance of 76.8%, outperforming all the baselines. To compare our search neural architecture
with previous NAS models, we replace the neural architecture in G with other NAS models. Other
training settings in the second phase are identical to our model. As such, we derive NASNet+Phase
II (Zoph et al., 2018), AmoebaNet+Phase II (Shah et al., 2018) , DARTS+Phase II (Liu et al.,
2019a), and PDARTS+Phase II (Chen et al., 2019) models. The results in Table 1 demonstrate that
our model outperform other NAS based model by a large margin, which shows the effectiveness of
our model in unsupervised domain adaptation. Specifically, we set PDARTS+Phase II as an ablation
study to demonstrate the effectiveness of our task-specific design in learning domain-adaption aware
features.
Analysis To dive deeper into the training process of our NASDA
model, we plot the T-SNE embedding of the weights of C in
USPS→MNIST in Figure 3. This is achieved by recording
the weights of all the classifiers at each epoch. The black dot
indicates epoch zero, which is the common starting point. The
color from light to dark corresponds to the epoch number from
small to large. The T-SNE plots clearly show that the classifiers
are diverged from each other, demonstrating the effectiveness of
the second step of our NASDA training described in Section 3.2.
5 Conclusion
Figure 3: T-SNE for 4 classifiers
In this paper, we first formulate a novel dual-objective task of Neural Architecture Search for Domain
Adaptation (NASDA) to invigorate the design of transfer-aware network architectures. Towards
tackling the NASDA task, we have proposed a novel learning framework that leverages MK-MMD
to guide the neural architecture search process. Instead of aligning the features from existing
handcrafted backbones, our model directly searches for the optimal neural architecture specific for
domain adaptation. Furthermore, we have introduced the ways to consolidate the feature generator,
which is stacked from the searched architecture, in order to boost the UDA performance. Extensive
empirical evaluations on UDA benchmarks have demonstrated the efficacy of the proposed model
against several state-of-the-art domain adaptation algorithms.
8
Under review as a conference paper at ICLR 2021
References
G Anandalingam and Terry L Friesz. Hierarchical optimization: An introduction. Annals of
Operations Research, 34(1):1-11, 1992.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.
Gabriel Bender. Understanding and simplifying one-shot architecture search. International Confer-
ence on Machine Learning, 2018.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in neural information processing systems, pp. 343-351,
2016.
Han Cai, Tianyao Chen, Weinan Zhang, Yong Yu, and Jun Wang. Efficient architecture search by
network transformation. In Thirty-Second AAAI conference on artificial intelligence, 2018a.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation
for efficient architecture search. International Conference on Machine Learning, 2018b.
Liang-Chieh Chen, Maxwell Collins, Yukun Zhu, George Papandreou, Barret Zoph, Florian Schroff,
Hartwig Adam, and Jon Shlens. Searching for efficient multi-scale architectures for dense image
prediction. In Advances in neural information processing systems, pp. 8699-8710, 2018.
Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging
the depth gap between search and evaluation. In Proceedings of the IEEE International Conference
on Computer Vision, pp. 1294-1303, 2019.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pp. 215-223, 2011.
Koby Crammer, Michael Kearns, and Jennifer Wortman. Learning from multiple sources. Journal of
Machine Learning Research, 9(Aug):1757-1774, 2008.
Tobias Domhan, Jost Tobias Springenberg, and Frank Hutter. Speeding up automatic hyperparameter
optimization of deep neural networks by extrapolation of learning curves. In Twenty-Fourth
International Joint Conference on Artificial Intelligence, 2015.
Lixin Duan, Dong Xu, and Shih-Fu Chang. Exploiting web images for event recognition in con-
sumer videos: A multiple source domain adaptation approach. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on, pp. 1338-1345. IEEE, 2012.
Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search for
convolutional neural networks. NIPS Workshop on Meta-Learning, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1126-1135, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL
http://proceedings.mlr.press/v70/finn17a.html.
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimilano Pontil. Bilevel
programming for hyperparameter optimization and meta-learning. ICML, 2018.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Ma-
chine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 1180-1189,
Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/
ganin15.html.
Muhammad Ghifary, W Bastiaan Kleijn, and Mengjie Zhang. Domain adaptive neural networks for
object recognition. In Pacific Rim international conference on artificial intelligence, pp. 898-904.
Springer, 2014.
9
Under review as a conference paper at ICLR 2021
Arthur Gretton, Karsten M Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alex J Smola. A
kernel method for the two-sample-problem. In Advances in neural information processing systems,
pp. 513-520, 2007.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012.
P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. Associative domain adaptation. In 2017
IEEE International Conference on Computer Vision (ICCV), pp. 2784-2792, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and
Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In Jennifer Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 1989-1998, Stockholmsmassan,
Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr.press/
v80/hoffman18a.html.
Haifeng Jin, Qingquan Song, and Xia Hu. Auto-keras: An efficient neural architecture search system.
In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining, pp. 1946-1956, 2019.
Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover
cross-domain relations with generative adversarial networks. In Doina Precup and Yee Whye
Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70
of Proceedings of Machine Learning Research, pp. 1857-1865, International Convention Centre,
Sydney, Australia, 06-11 Aug 2017. PMLR. URL http://proceedings.mlr.press/
v70/kim17a.html.
Aaron Klein, Stefan Falkner, Simon Bartels, Philipp Hennig, and Frank Hutter. Fast Bayesian
Optimization of Machine Learning Hyperparameters on Large Datasets. In Aarti Singh and
Jerry Zhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence
and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 528-536, Fort
Lauderdale, FL, USA, 20-22 Apr 2017. PMLR. URL http://proceedings.mlr.press/
v54/klein17a.html.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Abhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris,
Bill Freeman, and Gregory Wornell. Co-regularized alignment for unsupervised domain
adaptation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9345-
9356. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8146- co- regularized- alignment- for- unsupervised- domain- adaptation.
pdf.
Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, and Yu-Chiang Frank Wang. A unified feature
disentangler for multi-domain image translation and manipulation. CoRR, abs/1809.01361, 2018a.
URL http://arxiv.org/abs/1809.01361.
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Proceedings
of the European Conference on Computer Vision (ECCV), pp. 19-34, 2018b.
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hierar-
chical representations for efficient architecture search. ICLR, 2018c.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. Interna-
tional Conference on Learning Representations, 2019a.
10
Under review as a conference paper at ICLR 2021
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. Interna-
tional Conference on Learning Representations, 2019b.
Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In Advances in neural
information processing systems, pp. 469-477, 2016.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features
with deep adaptation networks. In Francis Bach and David Blei (eds.), Proceedings of the 32nd
International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning
Research, pp. 97-105, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.
mlr.press/v37/long15.html.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Deep transfer learning with
joint adaptation networks. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pp. 2208-2217, 2017. URL
http://proceedings.mlr.press/v70/long17a.html.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based tuning
of continuous regularization hyperparameters. In International conference on machine learning,
pp. 2952-2960, 2016.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization
through reversible learning. In International Conference on Machine Learning, pp. 2113-2122,
2015.
Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh, and A R. Domain adaptation with multiple
sources. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou (eds.), Advances in Neural
Information Processing Systems 21, pp. 1041-1048. Curran Associates, Inc., 2009.
Boris Moiseev, Artem Konev, Alexander Chigorin, and Anton Konushin. Evaluation of traffic sign
recognition methods trained on synthetically generated data. In Jacques Blanc-Talon, Andrzej
Kasinski, Wilfried Philips, Dan Popescu, and Paul Scheunders (eds.), Advanced Concepts for
Intelligent Vision Systems, pp. 576-583, Cham, 2013. Springer International Publishing. ISBN
978-3-319-02895-8.
Fabian Pedregosa. Hyperparameter optimization with approximate gradient. 2016.
Xingchao Peng and Kate Saenko. Synthetic to real adaptation with generative correlation alignment
networks. In 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018,
Lake Tahoe, NV, USA, March 12-15, 2018, pp. 1982-1991, 2018. doi: 10.1109/WACV.2018.00219.
URL https://doi.org/10.1109/WACV.2018.00219.
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1406-1415, 2019.
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset
Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051, 9780262170055.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Aging evolution for image classifier
architecture search. In AAAI 2019, 2019a.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
volume 33, pp. 4780-4789, 2019b.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier
discrepancy for unsupervised domain adaptation. In The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), June 2018.
11
Under review as a conference paper at ICLR 2021
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt:
Aligning domains using generative adversarial networks. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition,pp. 8503-8512, 2018.
Shreyas Saxena and Jakob Verbeek. Convolutional neural fabrics. In Advances in Neural Information
Processing Systems, pp. 4053-4061, 2016.
Syed Asif Raza Shah, Wenji Wu, Qiming Lu, Liang Zhang, Sajith Sasidharan, Phil DeMar, Chin
Guok, John Macauley, Eric Pouyoul, Jin Kim, et al. Amoebanet: An sdn-enabled network service
for big data science. Journal of Network and Computer Applications, 119:70-82, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. CoRR, abs/1409.1556, 2014.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The German Traffic Sign
Recognition Benchmark: A multi-class classification competition. In IEEE International Joint
Conference on Neural Networks, pp. 1453-1460, 2011.
Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation.
CoRR, abs/1607.01719, 2016. URL http://arxiv.org/abs/1607.01719.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural
networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 6105-6114, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http:
//proceedings.mlr.press/v97/tan19a.html.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2820-2828, 2019.
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion:
Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2017.
Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In International
Conference on Machine Learning, pp. 564-572, 2016.
Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang.
Adversarial domain adaptation with domain mixup. arXiv preprint arXiv:1912.01805, 2019a.
Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, and Liang Lin. Deep cocktail network:
Multi-source unsupervised domain adaptation with category shift. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 3964-3973, 2018.
Xiang Xu, Xiong Zhou, Ragav Venkatesan, Gurumurthy Swaminathan, and Orchid Majumder. d-
sne: Domain adaptation using stochastic neighborhood embedding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2497-2506, 2019b.
Zili Yi, Hao (Richard) Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning
for image-to-image translation. In ICCV, pp. 2868-2876, 2017.
Arber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning:
Efficient joint neural architecture and hyperparameter search. ICML 2018 Workshop on AutoML
(AutoML 2018), 2018.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Computer Vision (ICCV), 2017 IEEE International
Conference on, 2017.
12
Under review as a conference paper at ICLR 2021
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. ICLR, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 8697-8710, 2018.
13