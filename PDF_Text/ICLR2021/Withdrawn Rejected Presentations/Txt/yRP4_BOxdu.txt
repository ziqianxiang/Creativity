Under review as a conference paper at ICLR 2021
Joint Learning of Full-structure Noise in
Hierarchical Bayesian Regression Models
Anonymous authors
Paper under double-blind review
Ab stract
We consider hierarchical Bayesian (type-II maximum likelihood) models for ob-
servations with latent variables for source and noise, where both hyperparameters
need to be estimated jointly from data. This problem has application in many
domains in imaging including biomagnetic inverse problems. Crucial factors in-
fluencing accuracy of source estimation are not only the noise level but also its
correlation structure, but existing approaches have not addressed estimation of
noise covariance matrices with full structure. Here, we consider the reconstruc-
tion of brain activity from electroencephalography (EEG). This inverse problem
can be formulated as a linear regression with independent Gaussian scale mixture
priors for both the source and noise components. As a departure from classical
sparse Bayesan learning (SBL) models where across-sensor observations are as-
sumed to be independent and identically distributed, we consider Gaussian noise
with full covariance structure. Using Riemannian geometry, we derive an efficient
algorithm for updating both source and noise covariance along the manifold of
positive definite matrices. Using the majorization-maximization framework, we
demonstrate that our algorithm has guaranteed and fast convergence. We validate
the algorithm both in simulations and with real data. Our results demonstrate that
the novel framework significantly improves upon state-of-the-art techniques in the
real-world scenario where the noise is indeed non-diagonal and fully-structured.
1 Introduction
Having precise knowledge of the noise distribution is a fundamental requirement for obtaining accu-
rate solutions in many regression problems (Bungert et al., 2020). In many applications however, itis
impossible to separately estimate this noise distribution, as distinct ”noise-only” (baseline) measure-
ments are not feasible. An alternative, therefore, is to design estimators that jointly optimize over the
regression coefficients as well as over parameters of the noise distribution. This has been pursued
both in a (penalized) maximum-likelihood settings (here referred to as Type-I approaches) (Petersen
& Jung, 2020; Bertrand et al., 2019; Massias et al., 2018) as well as in hierarchical Bayesian settings
(referred to as Type-II) (Wipf & Rao, 2007; Zhang & Rao, 2011; Hashemi et al., 2020; Cai et al.,
2020a). Most contributions in the literature are, however, limited to the estimation of only a diago-
nal noise covariance (i.e., independent between different measurements) (Daye et al., 2012; Van de
Geer et al., 2013; Dalalyan et al., 2013; Lederer & Muller, 2015). Considering a diagonal noise
covariance is a limiting assumption in practice as the noise interference in many realistic scenarios
are highly correlated across measurements; and thus, have non-trivial off-diagonal elements.
This paper develops an efficient optimization algorithm for jointly estimating the posterior of regres-
sion parameters as well as the noise distribution. More specifically, we consider linear regression
with Gaussian scale mixture priors on the parameters and a full-structure multivariate Gaussian
noise. We cast the problem as a hierarchical Bayesian (type-II maximum-likelihood) regression
problem, in which the variance hyperparameters and the noise covariance matrix are optimized by
maximizing the Bayesian evidence of the model. Using Riemannian geometry, we derive an effi-
cient algorithm for jointly estimating the source and noise covariances along the manifold of positive
definite (P.D.) matrices.
To highlight the benefits of our proposed method in practical scenarios, we consider the problem
of electromagnetic brain source imaging (BSI). The goal of BSI is to reconstruct brain activity
1
Under review as a conference paper at ICLR 2021
from magneto- or electroencephalography (M/EEG), which can be formulated as a sparse Bayesian
learning (SBL) problem. Specifically, it can be cast as a linear Bayesian regression model with
independent Gaussian scale mixture priors on the parameters and noise. As a departure from the
classical SBL approaches, here we specifically consider Gaussian noise with full covariance struc-
ture. Prominent source of correlated noise in this context are, for example, eye blinks, heart beats,
muscular artifacts and line noise. Other realistic examples for the need for such full-structure noise
can be found in the areas of array processing (Li & Nehorai, 2010) or direction of arrival (DOA)
estimation (Chen et al., 2008). Algorithms that can accurately estimate noise with full covariance
structure are expected to achieve more accurate regression models and predictions in this setting.
2 Type-II Bayesian Regression
We consider the linear model Y = LX + E, in which a forward or design matrix, L ∈ RM ×N, is
mapped to the measurements, Y, by a set of coefficients or source components, X. Depending on the
setting, the problem of estimating X given L and Y is called an inverse problem in physics, a multi-
task regression problem in machine learning, or a multiple measurement vector (MMV) recovery
problem in signal processing (Cotter et al., 2005). Adopting a signal processing terminology, the
measurement matrix Y ∈ RM ×T captures the activity of M sensors at T time instants, y(t) ∈
RM×1, t = 1, . . . , T, while the source matrix, X ∈ RN×T, consists of the unknown activity of N
sources at the same time instants, x(t) ∈ RN×1, t = 1, . . . , T. The matrix E = [e(1), . . . , e(T)] ∈
RM ×T represents T time instances of zero-mean Gaussian noise with full covariance Λ, e(t) ∈
RM×1 〜N(0, Λ),t = 1,...,T, which is assumed to be independent of the source activations.
In this paper, we focus on M/EEG based brain source imaging (BSI) but the proposed algorithm can
be used in general regression settings, in particular for sparse signal recovery (CandeS et al., 2006;
Donoho, 2006) with a wide range of applications (Malioutov et al., 2005). The goal of BSI is to
infer the underlying brain activity X from the EEG/MEG measurement Y given a known forward
operator, called lead field matrix L. As the number of sensors is typically much smaller than the
number of locations of potential brain sources, this inverse problem is highly ill-posed. This prob-
lem is addressed by imposing prior distributions on the model parameters and adopting a Bayesian
treatment. This can be performed either through Maximum-a-Posteriori (MAP) estimation (Type-I
Bayesian learning) (Pascual-Marqui et al., 1994; Gorodnitsky et al., 1995; Haufe et al., 2008; Gram-
fort et al., 2012; CaStano-CandamiI et al., 2015) or, when the model has unknown hyperparameters,
through Type-II Maximum-Likelihood estimation (Type-II Bayesian learning) (Mika et al., 2000;
Tipping, 2001; Wipf & Nagarajan, 2009; Seeger & Wipf, 2010; Wu et al., 2016).
In this paper, we focus on Type-II Bayesian learning, which assumes a family of prior distributions
p(X| Θ) parameterized by a set of hyperparameters Θ. These hyper-parameters can be learned from
the data along with the model parameters using a hierarchical Bayesian approach (Tipping, 2001;
Wipf & Rao, 2004) through the maximum-likelihood principle:
ΘII := arg maxp(Y∣Θ)
Θ
arg max
Θ
/p(Y∣X, Θ)p(X∣Θ)dX .
(1)
Here we assume a zero-mean Gaussian prior with full covariance Γ for the underlying source dis-
tribution, x(t) ∈ RN×1 〜 N(0, Γ),t = 1,...,T. Just as most other approaches, Type-II Bayesian
learning makes the simplifying assumption of statistical independence between time samples. This
leads to the following expression for the distribution of the sources and measurements:
TT
p(x∣γ) = Y P(χ(t)∣r) = γ Ν(0,γ)
t=1	t=1
TT
p(y∣x) = Yp(y(t)∣χ(t)) = YN(Lχ(t), Λ).
t=1	t=1
(2)
(3)
The parameters of the Type-II model, Θ, are the unknown source and noise covariances, i.e., Θ =
{Γ, Λ}. The unknown parameters Γ and Λ are optimized based on the current estimates of the
source and noise covariances in an alternating iterative process. Given initial estimates of Γ and Λ,
2
Under review as a conference paper at ICLR 2021
the posterior distribution of the sources is a Gaussian of the form (Sekihara & Nagarajan, 2015)
T
p(X∣Y, Γ) = Y N(μχ(t), Σχ) , where	(4)
t=1
μχ(t) = ΓL>(Σy)-1y(t)	(5)
Σx = Γ - ΓL>(Σy)-1LΓ	(6)
Σy = Λ + LΓL> .	(7)
The estimated posterior parameters μχ(t) and Σχ are then in turn used to update Γ and Λ as the
minimizers of the negative log of the marginal likelihoodp(Y∣Γ, Λ), which is given by (WiPf et al.,
2010):
1T
LIIcΓ, Λ) = - logp(Y∣Γ, Λ) = log∣Σy | + T ]Ty(t)>Σy 1y(t)
t=1
1T
=log∣Λ + LΓL>∣ + - Ey(t)> (Λ + LΓL>)-1 y(t) ,	(8)
t=1
where | ∙ | denotes the determinant of a matrix. This process is repeated until convergence. Given the
final solution of the hyperparameters ΘII = {ΓII, ΛII}, the posterior source distribution is obtained
by plugging these estimates into equations 3 to 6.
3 Proposed Method: Full-structure Noise (FUN) Learning
Here we propose a novel and efficient algorithm, full-structure noise (FUN) learning, which is able
to learn the full covariance structure of the noise jointly within the Bayesian Type-II regression
framework. We first formulate the algorithm in its most general form, in which both the noise
distribution and the prior have full covariance structure. Later, we make the simplifying assumption
of independent source priors, leading to the pruning of the majority of sources. This effect, which
has also been referred to as automatic relevance determination (ARD) or sparse Bayesian learning
(SBL) is beneficial in our application of interest, namely the reconstruction of parsimonious sets of
brain sources underlying experimental EEG measurements.
Note that the Type-II cost function in equation 8 is non-convex and thus non-trivial to optimize.
A number of iterative algorithms such as majorization-minimization (MM) (Sun et al., 2017) have
been proposed to address this challenge. Following the MM scheme, we first construct convex
surrogate functions that majorizes LII cΓ, Λ) in each iteration of the optimization algorithm. Then,
we show the minimization equivalence between the constructed majoring functions and equation 8.
This result is presented in the following theorem:
Theorem 1.	Let Λk and Σky be fixed values obtained in the ck)-th iteration of the optimization
algorithm minimizing LIIcΓ, Λ). Then, optimizing the non-convex type-II ML cost function in equa-
tion 8, LIIcΓ, Λ), with respect to Γ is equivalent to optimizing the following convex function, which
majorizes equation 8:
Lconvce(Γ, Λk) = tr((Ck)-1 Γ) + tr(MSΓ-1) ,	(9)
where CkS and MkS are defined as:
1	-1	1 T
Ck ：=(L> (∑y)— L)	, Mk ：= T XXk(t)xk(t)> .	(10)
t=1
Similarly, optimizing LII cΓ, Λ) with respect to Λ is equivalent to optimizing the following convex
majorizing function:
Lnonse(Γk, Λ) = tr((CN)-1 Λ) + tr(MNΛ-1) ,	(11)
where CkN and MkN are defined as:
1T
CN := (∑y) , MN := T E(y(t) - LXk(t))(y(t) - LXk(t))> .	(12)
t=1
3
Under review as a conference paper at ICLR 2021
Proof. The proof is presented in Appendix A.	□
We continue by considering the optimization of the cost functions Lscoounrvce(Γ, Λk) andLcnoonisve(Γk, Λ)
with respect to Γ and Λ, respectively. Note that in case of source covariances with full structure,
the solution of Lscoounrvce (Γ, Λk ) with respect to Γ lies in the (N2 - N)/2 Riemannian manifold of
positive definite (P.D.) matrices. This consideration enables us to invoke efficient methods from
Riemannian geometry (see Petersen et al., 2006; Berger, 2012; Jost & Jost, 2008), which ensures
that the solution at each step of the optimization is contained within the lower-dimensional solu-
tion space. Specifically, in order to optimize for the source covariance, the algorithm calculates the
geometric mean between the previously obtained statistical model source covariance, CkS, and the
source-space sample covariance matrix, MkS, in each iteration. Analogously, to update the noise co-
variance estimate, the algorithm calculates the geometric mean between the model noise covariance,
CkN, and the empirical sensor-space residuals, MkN . The update rules obtained from this algorithm
are presented in the following theorem:
Theorem 2.	The cost functions Lscoounrvce(Γ, Λk) and Lcnoonisve (Γk , Λ) are both strictly geodesically
convex with respect to the P.D. manifold, and their optimal solution with respect to Γ and Λ, respec-
tively, can be attained according to the two following update rules:
1
Γk+1 ~ (CS)2 ((CS)-1/2MS(CS)T/2) 2 (CS)2 ,	(13)
Λk+1 一(CN)2 ((CN)T/2MN(CN)T/2) ι (CN)2.	(i4)
Proof. A detailed proof can be found in Appendix B.	□
Convergence of the resulting algorithm is shown in the following theorem.
Theorem 3.	Optimizing the non-convex type-II ML cost function in equation 8, LII(Γ, Λ) with
alternating update rules for Γ and Λ in equation 13 and equation 14 leads to an MM algorithm
with guaranteed convergence guarantees.
Proof. A detailed proof can be found in Appendix C.	□
While Theorems 1-3 reflect a general joint learning algorithm, the assumption of sources with full
covariance structure is often relaxed in practice. The next section will shed light on this important
simplification by making a formal connection to SBL algorithms.
3.1 Sparse Bayesian Learning with Full Noise Modeling
In brain source imaging, the assumption of full source covariance is often relaxed. Even if, tech-
nically, most parts of the brain are active at all times, and the concurrent activations of different
brain regions can never be assumed to be fully uncorrelated, there are many experimental settings
in which it is reasonable to assume only a small set of independent brain sources. Such sparse
solutions are physiologically plausible in task-based analyses, where only a fraction of the brain’s
macroscopic structures is expected to be consistently engaged. A common strategy in this case is to
model independent sources through a diagonal covariance matrix. In the Type-II Bayesian learning
framework, this simplification interestingly leads to sparsity of the resulting source distributions,
as, at the optimum, many of the estimated source variances are zero. This mechanism is known as
sparse Bayesian learning and is closely related to the more general concept of automatic relevance
determination. Here, we adopt the SBL assumption for the sources, leading to Γ-updates previously
described in the BSI literature under the name Champagne (Wipf & Nagarajan, 2009). As a novelty
and main focus of this paper, we here equip the SBL framework with the capability to jointly learn
full noise covariances through the geometric mean based update rule in equation 14. In the SBL
framework, the N modeled brain sources are assumed to follow independent univariate Gaussian
distributions with zero mean and distinct unknown variances γn xn(t)〜 N(0, Yn), n = 1,...,N.
In the SBL solution, the majority of variances is zero, thus effectively inducing spatial sparsity of
the corresponding source activities. For FUN learning, we also impose a diagonal structure on the
source covariance matrix, Γ = diag(γ), where γ = [γ1, . . . , γN]>. By constraining Γ in equation 9
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Full-structure noise (FUN) learning
Input: The lead field matrix L ∈ RM ×N and the measurement vectors
y(t) ∈RM×1,t= 1,...,T.
Result: The estimated prior source variances [γ1, . . . , γN]>, noise covariance Λ, the posterior
mean μχ(t) and covariance Σχ of the sources.
1	Set a random initial value for Λ as well as γ = [γ1, . . . , γN]>, and construct Γ = diag(γ).
2	Calculate the statistical covariance Σy = Λ + LΓL> .
Repeat
3	Calculate the posterior mean as μχ(t) = ΓL>(Σy )-1 y(t).
4	Calculate CkS and MSk based on equation 10, and update γn for n = 1, . . . , N based on
equation 15.
5	Calculate CkN and MkN based on equation 12, and update Λ based on equation 14.
Until stopping condition is satisfied;
6	Calculate the posterior covariance as Σx = Γ - ΓL>(Σy)-1LΓ.
to the set of diagonal matrices, W, we can show that the update rule equation 13 for the source
variances simplifies to the following form:
T PW(Xn (t))2
for n = 1, . . . , N ,
(15)
where Ln denotes the n-th column of the lead field matrix. Interestingly, equation 15 is identical to
the update rule of the Champagne algorithm. A detailed derivation of equation 15 can be found in
Appendix D.
Summarizing, the FUN learning approach, just like Champagne and other SBL algorithms, assumes
independent Gaussian sources with individual variances (thus, diagonal source covariances), which
are updated through equation equation 15. Departing from the classical SBL setting, which as-
sumes the noise distribution to be known, FUN models noise with full covariance structure, which
is updated using equation 14. Algorithm 1 summarizes the used update rules.
Note that various recent Type-II noise learning schemes for diagonal noise covariance matrices
(Hashemi et al., 2020; Cai et al., 2020a) that are rooted in the concept of SBL can be also derived
as special cases of FUN learning assuming diagonal source and noise covariances, i.e., Γ, Λ ∈ W.
Specifically imposing diagonal structure on the noise covariance matrix for the FUN algorithm, Λ,
results in identical noise variance update rules as derived in Cai et al. (2020a) for heteroscedastic,
and in Hashemi et al. (2020) for homoscedastic noise. We explicitly demonstrate this connection
in Appendix E. Here, we note that heteroscedasticity refers to the common phenomenon that mea-
surements are contaminated with non-uniform noise levels across channels, while homoscedasticity
only accounts for uniform noise levels.
4	Numerical Simulations and Real Data Analysis
Source, Noise and Forward Model: We simulated a sparse set of N0 = 5 active brain sources
that were placed at random positions on the cortex. To simulate the electrical neural activity
of these sources, T = 200 identically and independently distributed (i.i.d) points were sampled
from a Gaussian distribution, yielding sparse source activation vectors X(t). The resulting source
distribution, represented as X = [X(1), . . . , X(T)], was projected to the EEG sensors through
application of lead field matrix as the forward operator: Ysignal = LX. The lead field ma-
trix, L ∈ R58×2004, was generated using the New York Head model (Huang et al., 2016) tak-
ing into account the realistic anatomy and electrical tissue conductivities of an average human
head. Further details regarding forward modeling is provided in Appendix F. Gaussian additive
noise was randomly sampled from a zero-mean normal distribution with full covariance matrix Λ:
e(t) ∈ RM×1 〜N(0, Λ),t = 1,...,T. This setting is further referred to as full-structure noise.
Note that we also generated noise with diagonal covariance matrix, referred to as heteroscedas-
tic noise, in order to investigate the effect of model violation on reconstruction performance. The
5
Under review as a conference paper at ICLR 2021
noise matrix E = [e(1), . . . , e(T)] ∈ RM×T was normalized by it Frobenius norm and added
to the signal matrix Ysignal as follows： Y = Ysignal + ((I-a)kYsignalkF/仪|国松) E, where a de-
termines the signal-to-noise ratio (SNR) in sensor space. Precisely, SNR is obtained as follows:
SNR = 20logιo (α∕ι-α). In the subsequently described experiments the following values of α were
used: α={0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.65, 0.7, 0.8}, which correspond to the following SNRs:
SNR={-12, -7.4, -5.4, -3.5, -1.7, 0, 1.7, 3.5, 5.4, 7.4, 12} (dB). MATLAB codes for producing the
results in the simulation study are uploaded here.
Evaluation Metrics and Simulation Set-up: We applied the full-structure noise learning approach
on the synthetic datasets described above to recover the locations and time courses of the active
brain sources. In addition to our proposed approach, two further Type-II Bayesian learning schemes,
namely Champagne with homo- and heteroscedastic noise learning (Hashemi et al., 2020; Cai et al.,
2020a), were also included as benchmarks with respect to source reconstruction performance and
noise covariance estimation accuracy. Source reconstruction performance was evaluated accord-
ing to the earth mover’s distance (EMD) (Rubner et al., 2000)), the error in the reconstruction of
the source time courses, the average Euclidean distance (EUCL) (in mm) between each simulated
source and the best (in terms of absolute correlations) matching reconstructed source, and finally
F1-measure score (Chinchor & Sundheim, 1993). A detailed definition of evaluation metrics is
provided in Appendix F. To evaluate the accuracy of the noise covariance matrix estimation, the fol-
lowing two metrics were calculated: the Pearson correlation between the original and reconstructed
noise covariance matrices, Λ and Λ, denoted by Λsim , and the normalized mean squared error
(NMSE) between Λ and Λ, defined as NMSE = ∣∣Λ - Λ∣∣F∕∣∣Λ∣∣F. Note that NMSE measures
the reconstruction of the true scale of the noise covariance matrix, while Λsim is scale-invariant
and hence only quantifies the overall structural similarity between simulated and estimated noise
covariance matrices. Each simulation was carried out 100 times using different instances of X and
E, and the mean and standard error of the mean (SEM) of each performance measure across rep-
etitions was calculated. Convergence of the optimization programs for each run was defined if the
relative change of the Frobenius-norm of the reconstructed sources between subsequent iterations
was less than 10-8. A maximum of 1000 iterations was carried out if no convergence was reached
beforehand.
Figure 1 shows two simulated datasets with five active sources in presence of full-structure noise
(upper panel) as well as heteroscedastic noise (lower panel) at 0 (dB) SNR. Topographic maps
depict the locations of the ground-truth active brain sources (first column) along with the source
reconstruction result of three noise learning schemes assuming noise with homoscedastic (second
column), heteroscedastic (third column), and full (fourth column) structure. For each algorithm, the
estimated noise covariance matrix is also plotted above the topographic map. Source reconstruction
performance was measured in terms of EMD and time course correlation (Corr), and is summarized
in the table next to each panel. Besides, the accuracy of the noise covariance matrix reconstruction
was measured on terms of Λsim and NMSE. Results are included in the same table. Figure 1 (upper
panel) allows for a direct comparison of the estimated noise covariance matrices obtained from the
three different noise learning schemes. It can be seen that FUN learning can better capture the overall
structure of ground truth full-structure noise as evidenced by lower NMSE and similarity errors
compared to the heteroscedastic and homoscedastic algorithm variants that are only able to recover
a diagonal matrix while enforcing the off-diagonal elements to zero. This behaviour results in higher
spatial and temporal accuracy (lower EMD and time course error) for FUN learning compared to
competing algorithms assuming diagonal noise covariance. This advantage is also visible in the
topographic maps. The lower-panel of Figure 1 presents analogous results for the setting where the
noise covariance is generated according to a heteroscedastic model. Note that the superior spatial
and temporal reconstruction performance of the heteroscedastic noise learning algorithm compared
to the full-structure scheme is expected here because the simulated ground truth noise is indeed
heteroscedastic. The full-structure noise learning approach, however, provides fairly reasonable
performance in terms of EMD, time course correlation (corr), and Λsim, although it is designed
to estimate a full-structure noise covariance matrix. The convergence behaviour of all three noise
learning variants is also illustrated in Figure 1. Note that the full-structure noise learning approach
eventually reaches lower negative log-likelihood values in both scenarios, namely full-structure and
heteroscedastic noise.
Figure 2 shows the EMD, the time course reconstruction error, the EUCL and the F1 measure score
incurred by three different noise learning approaches assuming homoscedastic (red), heteroscedastic
6
Under review as a conference paper at ICLR 2021
100
Full Structure
Ground Truth
Homoscedastic
Heteroscedastic
-Homoscedastic
Heteroscedastic
■■Full Structure
>00ω90N SdBW U-B怎
ə-oN
0.145 0.887	0.418 0.8223
0.072 0.941	0,926 0,1713
Heteroscedastic
Full Structure
Ground Truth
Homoscedastic
Figure 1: Two examples of the simulated data with five active sources in presence of full-structure
noise (upper panel) as well as heteroscedastic noise (lower panel) at 0 (dB) SNR. Topographic maps
depict the locations of the ground-truth active brain sources along with the source reconstruction
results of three noise learning schemes. For each algorithm, the estimated noise covariance matrix is
also plotted above the topographic maps. The source reconstruction performance of these examples
in terms of EMD and time course correlation (Corr) is summarized in the associated table next to
each panel. We also report the accuracy with which the ground-truth noise covariance was estimated
in terms of the Λsim and NMSE. The convergence behaviour of all three noise estimation approaches
is also shown.
Full Structure
HeteroScedaStic
-Homoscedastic
Heteroscedastic
-Full Structure
Number of iterations
吸灌
Homoscedastic	0.230	0.884	0.851	0.280
Heteroscedastic	0.072	0.925	0.994	0.012
Full Structure	0.150	0.886	0.845	0.425
IOHOmoSCedaStiC-B-Heteroscedastic∙6Full Structure]
-5	0	5	10	-5	0	5	10	-5	0	5	10	-5	0	5	10
SNR(dB)	SNR(dB)	SNR(dB)	SNR(dB)
Figure 2: Source reconstruction performance (mean ± SEM) of the three different noise learning
schemes for data generated by a realistic lead field matrix. Generated sensor signals were super-
imposed by either full-structure or heteroscedastic noise covering a wide range of SNRs. Perfor-
mance was measured in terms of the earth mover’s distance (EMD), time-course correlation error,
F1-measure and Euclidean distance (EUCL) in (mm) between each simulated source and the recon-
structed source with highest maximum absolute correlation.
7
Under review as a conference paper at ICLR 2021
Figure 3: Auditory evoked field (AEF) localization results versus number of trials from one rep-
resentative subject using FUN learning algorithm. All reconstructions show focal sources at the
expected locations in the left (L: top panel) and right (R: bottom panel) auditory cortex. As a result,
the limited number of trials does not influence the reconstruction results of FUN learning algorithm.
(green) and full-structure (blue) noise covariances for a range of 10 SNR values. The upper panel
represents the evaluation metrics for the setting where the noise covariance is full-structure model,
while the lower-panel depicts the same metric for simulated noise with heteroscedastic diagonal co-
variance. Concerning the first setting, FUN learning consistently outperforms its homoscedastic and
heteroscedastic counterparts according to all evaluation metrics in particular in low-SNR settings.
Consequently, as the SNR decreases, the gap between FUN learning and the two other variants
increases. Conversely, heteroscedastic noise learning shows an improvement over FUN learning
according to all evaluation metrics when the simulated noise is indeed heteroscedastic. However,
note that the magnitude of this improvement is not as large as observed for the setting where the
noise covariance is generated according to a full-structure model and then is estimated using the
FUN approach.
Analysis of Auditory Evoked Fields (AEF): Figure 3 shows the reconstructed sources of the Au-
ditory Evoked Fields (AEF) versus number of trials from a single representative subject using FUN
learning algorithm. Further details on this dataset can be found in Appendix G. We tested the re-
construction performance of FUN learning with the number of trials limited to 1, 2, 12, 63 and 120.
Each reconstruction was performed 30 times with the specific trials themselves chosen as a random
subset of all available trials. As the subplots for different trials demonstrate, FUN learning algorithm
is able to correctly localize bilateral auditory activity to Heschel’s gyrus, which is the characteristic
location of the primary auditory cortex, under a few trials or even a single trial.
5	Discussion
This paper focused on sparse regression within the hierarchical Bayesian regression framework and
its application in EEG/MEG brain source imaging. To this end we developed an algorithm, which
is, however, suitable for a much wider range of applications. What is more, the same concepts used
here for full-structure noise learning could be employed in other contexts where hyperparameters
like kernel widths in Gaussian process regression (Wu et al., 2019) or dictionary elements in the dic-
tionary learning problem (Dikmen & Fevotte, 2012) are to be inferred. Besides, using FUN learning
algorithm may also prove useful for practical scenarios in which model residuals are expected to be
correlated, e.g., probabilistic canonical correlation analysis (CCA) (Bach & Jordan, 2005), spectral
independent component analysis (ICA) (Ablin et al., 2020), wireless communication (Prasad et al.,
2015; Gerstoft et al., 2016; Haghighatshoar & Caire, 2017; Khalilsarai et al., 2020), robust portfo-
lio optimization in finance (Feng et al., 2016), graph learning (Kumar et al., 2020), thermal field
reconstruction (Flinth & Hashemi, 2018), and brain functional imaging (Wei et al., 2020).
8
Under review as a conference paper at ICLR 2021
Noise learning has also attracted attention in functional magnetic resonance imaging (fMRI) (Cai
et al., 2016; Shvartsman et al., 2018; Cai et al., 2019b; 2020b; Wei et al., 2020), where various
models like matrix-normal (MN), factor analysis (FA), and Gaussian-process (GP) regression have
been proposed. The majority of the noise learning algorithms in the fMRI literature rely on the
EM framework, which is quite slow in practice and has convergence guarantees only under certain
strong conditions. In contrast to these existing approaches, our proposed framework not only applies
to the models considered in these papers, but also benefits from theoretically proven convergence
guarantees. To be more specific, we showed in this paper that FUN learning is an instance of the
wider class of majorization-minimization (MM) framework, for which provable fast convergence is
guaranteed. It is worth emphasizing our contribution within the MM optimization context as well.
In many MM implementations, surrogate functions are minimized using an iterative approach. Our
proposed algorithm, however, obtains a closed-form solution for the surrogate function in each step,
which further advances its efficiency.
In the context of BSI, Engemann & Gramfort (2015) proposed a method for selecting a single regu-
larization parameter based on cross-validation and maximum-likelihood estimation, while Huizenga
et al. (2002); De Munck et al. (2002); Bijma et al. (2003); De Munck et al. (2004); Ahn & Jun
(2011); Jun et al. (2006) and Plis et al. (2006) assume more complex spatiotemporal noise covari-
ance structures. A common limitation of these works is, however, that the noise level is not estimated
as part of the source reconstruction problem on task-related data but from separate noise recordings.
Our proposed algorithm substantially differs in this respect, as it learns the noise covariance jointly
with the brain source distribution. Note that The idea of joint estimation of brain source activity and
noise covariance has been previously proposed for Type-I learning methods in (Massias et al., 2018;
Bertrand et al., 2019). In contrast to these Type-I methods, FUN is a Type-II method, which learns
the prior source distribution as part of the model fitting. Type-II methods have been reported to
yield consistently superior results than Type-I methods (Owen et al., 2012; Cai et al., 2019a; 2020a;
Hashemi et al., 2020). Our numerical results show that the same hold also for FUN learning, which
performs on par or better than existing variants from the Type-II family (including conventional
Champagne) in this study. We plan to provide a formal comparison of the performance of noise
learning within Type-I and Type-II estimation in our future work.
While being broadly applicable, our approach is also limited by a number of factors. Although
Gaussian noise distributions are commonly justified, it would be desirable to also include more
robust (e.g., heavy-tailed) non-Gaussian noise distributions in our framework. Another limitation
is that the superior performance of the full-structure noise learning technique comes at the ex-
pense of higher computational complexity compared to the variants assuming homoscedastic or
heteroscedastic strucutre. Besides, signals in real-world scenarios often lie in a lower-dimensional
space compared to the original high-dimensional ambient space due to the particular correlations
that inherently exist in the structure of the data. Therefore, imposing physiologically plausible con-
straints on the noise model, e.g., low-rank or Toeplitz structure, not only provides side information
that can be leveraged for the reconstruction but also reduces the computational cost in two ways:
a) by reducing the number of parameters and b) by taking advantage of efficient implementations
using circular embeddings and the fast Fourier transform (Babu, 2016). Exploring efficient ways
to incorporate these structural assumptions within a Riemannian framework is another direction of
future work.
6	Conclusion
This paper proposes an efficient optimization algorithm for jointly estimating Gaussian regression
parameter distributions as well as Gaussian noise distributions with full covariance structure within
a hierarchical Bayesian framework. Using the Riemannian geometry of positive definite matrices,
we derived an efficient algorithm for jointly estimating source and noise covariances. The benefits
of our proposed framework were evaluated within an extensive set of experiments in the context of
electromagnetic brain source imaging inverse problem and showed significant improvement upon
state-of-the-art techniques in the realistic scenario where the noise has full covariance structure. The
performance of our method is assessed through a real data analysis for the auditory evoked field
(AEF) dataset.
9
Under review as a conference paper at ICLR 2021
References
Pierre Ablin, Jean-Francois Cardoso, and Alexandre Gramfort. Spectral independent component
analysis with noise modeling for M/EEG source separation. arXiv preprint arXiv:2008.09693,
2020.
Minkyu Ahn and Sung C Jun. MEG/EEG Spatiotemporal Noise Covariance estimation in the Least
Squares Sense. In The Asia-Pacific Signal and Information Processing Association (APSIPA),
2011.
Prabhu Babu. MELT—maximum-likelihood estimation of low-rank Toeplitz covariance matrix.
IEEESignalProcessing Letters, 23(11):1587-1591, 2016.
Francis R Bach and Michael I Jordan. A probabilistic interpretation of canonical correlation analysis.
2005.
Sylvain Baillet, John C Mosher, and Richard M Leahy. Electromagnetic brain mapping. IEEE
Signal Processing Magazine, 18(6):14-30, 2001.
Yousra Bekhti, Felix Lucka, Joseph Salmon, and Alexandre Gramfort. A hierarchical Bayesian per-
spective on majorization-minimization for non-convex sparse regression: application to M/EEG
source imaging. Inverse Problems, 34(8):085010, 2018.
Aharon Ben-Tal. On generalized means and generalized convex functions. Journal of Optimization
Theory and Applications, 21(1):1-13, 1977.
Marcel Berger. A panoramic view of Riemannian geometry. Springer Science & Business Media,
2012.
Quentin Bertrand, Mathurin Massias, Alexandre Gramfort, and Joseph Salmon. Handling correlated
and repeated measurements with the smoothed multivariate square-root Lasso. In Advances in
Neural Information Processing Systems, pp. 3959-3970, 2019.
Rajendra Bhatia. Positive definite matrices, volume 24. Princeton University Press, 2009.
Fetsje Bijma, Jan C De Munck, Hilde M Huizenga, and Rob M Heethaar. A mathematical approach
to the temporal stationarity of background noise in MEG/EEG measurements. NeuroImage, 20
(1):233-243, 2003.
Silvere Bonnabel and Rodolphe Sepulchre. Riemannian metric and geometric mean for positive
semidefinite matrices of fixed rank. SIAM Journal on Matrix Analysis and Applications, 31(3):
1055-1070, 2009.
Leon Bungert, Martin Burger, Yury Korolev, and Carola-Bibiane Schoenlieb. Variational regular-
isation for inverse problems with imperfect forward operators and general noise models. arXiv
preprint arXiv:2005.14131, 2020.
Chang Cai, Mithun Diwakar, Dan Chen, Kensuke Sekihara, and Srikantan S Nagarajan. Robust
empirical Bayesian reconstruction of distributed sources for electromagnetic brain imaging. IEEE
Transactions on Medical Imaging, 39(3):567-577, 2019a.
Chang Cai, Ali Hashemi, Mithun Diwakar, Stefan Haufe, Kensuke Sekihara, and Srikantan S Na-
garajan. Robust estimation of noise for electromagnetic brain imaging with the Champagne algo-
rithm. NeuroImage, 225:117411, 2020a.
Ming Bo Cai, Nicolas W Schuck, Jonathan W Pillow, and Yael Niv. Representational structure
or task structure? Bias in neural representational similarity analysis and a Bayesian method for
reducing bias. PLoS computational biology, 15(5):e1006299, 2019b.
Ming Bo Cai, Michael Shvartsman, Anqi Wu, Hejia Zhang, and Xia Zhu. Incorporating structured
assumptions with probabilistic graphical models in fMRI data analysis. Neuropsychologia, pp.
107500, 2020b.
10
Under review as a conference paper at ICLR 2021
Mingbo Cai, Nicolas W Schuck, Jonathan W Pillow, and Yael Niv. A Bayesian method for reducing
bias in neural representational similarity analysis. In Advances in Neural Information Processing
Systems,pp. 4951-4959, 2016.
Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete
and inaccurate measurements. Communications on pure and applied mathematics, 59(8):1207-
1223, 2006.
Sebastian Castafio-Candamil, Johannes Hohne, Juan-David Martlnez-Vargas, Xing-Wei An, Ger-
man Castellanos-Dominguez, and Stefan Haufe. Solving the EEG inverse problem based on
space-time-frequency structured sparsity constraints. NeuroImage, 118:598-612, 2015.
Chiao En Chen, Flavio Lorenzelli, Ralph E Hudson, and Kung Yao. Stochastic maximum-likelihood
DOA estimation in the presence of unknown nonuniform noise. IEEE Transactions on Signal
Processing, 56(7):3038-3044, 2008.
Nancy Chinchor and Beth M Sundheim. Muc-5 evaluation metrics. In Fifth Message Understanding
Conference (MUC-5), 1993.
Shane F Cotter, Bhaskar D Rao, Kjersti Engan, and Kenneth Kreutz-Delgado. Sparse solutions
to linear inverse problems with multiple measurement vectors. IEEE Transactions on Signal
Processing, 53(7):2477-2488, 2005.
Sarang S Dalal, JM Zumer, V Agrawal, KE Hild, Kensuke Sekihara, and SS Nagarajan. NUTMEG:
a neuromagnetic source reconstruction toolbox. Neurology & Clinical Neurophysiology: NCN,
2004:52, 2004.
Sarang S Dalal, Johanna M Zumer, Adrian G Guggisberg, Michael Trumpis, Daniel DE Wong,
Kensuke Sekihara, and Srikantan S Nagarajan. MEG/EEG source reconstruction, statistical eval-
uation, and visualization with NUTMEG. Computational Intelligence and Neuroscience, 2011,
2011.
Arnak Dalalyan, Mohamed Hebiri, Katia Meziani, and Joseph Salmon. Learning heteroscedastic
models by convex programming under group sparsity. In International Conference on Machine
Learning, pp. 379-387, 2013.
Jason V Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S Dhillon. Information-theoretic
metric learning. In Proceedings of the 24th International Conference on Machine Learning, pp.
209-216, 2007.
Z John Daye, Jinbo Chen, and Hongzhe Li. High-dimensional heteroscedastic regression with an
application to eQTL data analysis. Biometrics, 68(1):316-326, 2012.
Jan Casper De Munck, Hilde M Huizenga, Lourens J Waldorp, and RA Heethaar. Estimating sta-
tionary dipoles from MEG/EEG data contaminated with spatially and temporally correlated back-
ground noise. IEEE Transactions on Signal Processing, 50(7):1565-1572, 2002.
Jan Casper De Munck, Fetsje Bijma, Pawel Gaura, Cezary Andrzej Sieluzycki, Maria IneS Branco,
and Rob M Heethaar. A maximum-likelihood estimator for trial-to-trial variations in noisy
MEG/EEG data sets. IEEE Transactions on Biomedical Engineering, 51(12):2123-2128, 2004.
Onur Dikmen and CedriC Fevotte. Maximum marginal likelihood estimation for nonnegative dic-
tionary learning in the Gamma-Poisson model. IEEE Transactions on Signal Processing, 60(10):
5163-5175, 2012.
David L Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289-
1306, 2006.
Denis A Engemann and Alexandre Gramfort. Automated model selection in covariance estimation
and spatial whitening of MEG and EEG signals. NeuroImage, 108:328-342, 2015.
Dylan Fagot, Herwig Wendt, CedriC Fevotte, and Paris Smaragdis. Majorization-minimization algo-
rithms for convolutive NMF with the beta-divergence. In ICASSP 2019-2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 8202-8206. IEEE, 2019.
11
Under review as a conference paper at ICLR 2021
Yiyong Feng, Daniel P Palomar, et al. A signal processing perspective on financial engineering.
Foundations and Trends® in Signal Processing, 9(1-2):1-231, 2016.
Axel Flinth and Ali Hashemi. Approximate recovery of initial point-like and instantaneous sources
from coarsely sampled thermal fields via infinite-dimensional compressed sensing. In 2018 26th
European Signal Processing Conference (EUSIPCO), pp. 1720-1724. IEEE, 2018.
Peter Gerstoft, ChristoPh F Mecklenbrauker, Angeliki Xenaki, and Santosh Nannuru. Multisnapshot
sparse Bayesian learning for DOA. IEEE Signal Processing Letters, 23(10):1469-1473, 2016.
Irina F Gorodnitsky, John S George, and Bhaskar D Rao. Neuromagnetic source imaging with
FOCUSS: a recursive weighted minimum norm algorithm. Electroencephalography and Clinical
Neurophysiology, 95(4):231-251, 1995.
Alexandre Gramfort, Matthieu Kowalski, and Matti Hamalainen. Mixed-norm estimates for the
M/EEG inverse problem using accelerated gradient methods. Physics in Medicine and Biology,
57(7):1937, 2012.
Saeid Haghighatshoar and Giuseppe Caire. Massive MIMO channel subspace estimation from low-
dimensional projections. IEEE Transactions on Signal Processing, 65(2):303-318, 2017.
Matti Hamalainen, Riitta Hari, Risto J Ilmoniemi, Jukka Knuutila, and Olli V Lounasmaa. Magne-
toencephalography—theory, instrumentation, and applications to noninvasive studies of the work-
ing human brain. Reviews of modern Physics, 65(2):413, 1993.
Ali Hashemi and Stefan Haufe. Improving EEG source localization through spatio-temporal sparse
Bayesian learning. In 2018 26th European Signal Processing Conference (EUSIPCO), pp. 1935-
1939. IEEE, 2018.
Ali Hashemi, Chang Cai, Gitta Kutyniok, Klaus-Robert Muller, Srikantan Nagarajan, and Stefan
Haufe. Unification of sparse Bayesian learning algorithms for electromagnetic brain imaging
with the majorization minimization framework. bioRxiv, 2020.
Stefan Haufe and Arne Ewald. A simulation framework for benchmarking EEG-based brain con-
nectivity estimation methodologies. Brain topography, pp. 1-18, 2016.
Stefan Haufe, Vadim V Nikulin, Andreas Ziehe, Klaus-Robert Muller, and Guido Nolte. Combining
sparsity and rotational invariance in EEG/MEG source reconstruction. NeuroImage, 42(2):726-
738, 2008.
Yu Huang, Lucas C Parra, and Stefan Haufe. The New York head — a precise standardized volume
conductor model for EEG source localization and tES targeting. NeuroImage, 140:150-162, 2016.
Hilde M Huizenga, Jan C De Munck, Lourens J Waldorp, and Raoul PPP Grasman. Spatiotemporal
EEG/MEG source analysis based on a parametric noise covariance model. IEEE Transactions on
Biomedical Engineering, 49(6):533-539, 2002.
David R Hunter and Kenneth Lange. A tutorial on MM algorithms. The American Statistician, 58
(1):30-37, 2004.
Matthew W Jacobson and Jeffrey A Fessler. An expanded theoretical treatment of iteration-
dependent majorize-minimize algorithms. IEEE Transactions on Image Processing, 16(10):2411-
2422, 2007.
Jurgen Jost and JeUrgen Jost. Riemannian geometry and geometric analysis, volume 42005.
Springer, 2008.
Sung C Jun, Sergey M Plis, Doug M Ranken, and David M Schmidt. Spatiotemporal noise covari-
ance estimation from limited empirical magnetoencephalographic data. Physics in Medicine &
Biology, 51(21):5549, 2006.
Mahdi Barzegar Khalilsarai, Tianyu Yang, Saeid Haghighatshoar, and Giuseppe Caire. Structured
channel covariance estimation from limited samples in Massive MIMO. In ICC 2020-2020 IEEE
International Conference on Communications (ICC), pp. 1-7. IEEE, 2020.
12
Under review as a conference paper at ICLR 2021
SandeeP Kumar, Jiaxi Ying, Jose Vinicius de Miranda Cardoso, and Daniel P Palomar A unified
framework for structured graph learning via spectral constraints. Journal of Machine Learning
Research, 21(22):1-60, 2020.
Johannes Lederer and Christian L Muller. Don’t fall for tuning Parameters: tuning-free variable se-
lection in high dimensions with the TREX. In Proceedings of the Twenty-Ninth AAAI Conference
on Artificial Intelligence, PP. 2729-2735, 2015.
Tao Li and Arye Nehorai. Maximum likelihood direction finding in sPatially colored noise fields
using sParse sensor arrays. IEEE Transactions on Signal Processing, 59(3):1048-1062, 2010.
Leo Liberti. On a class of nonconvex Problems where all local minima are global. Publications de
IfInstitutMathemathique,76(90):101-109, 2004.
Thomas LiPP and StePhen Boyd. Variations and extension of the convex-concave Procedure. Opti-
mization and Engineering, 17(2):263-287, 2016.
Dmitry Malioutov, Mujdat Cetin, and Alan S Willsky. A sparse signal reconstruction perspective
for source localization with sensor arrays. IEEE Transactions on Signal Processing, 53(8):3010-
3022, 2005.
Mudassir Masood, Ali Ghrayeb, Prabhu Babu, Issa Khalil, and Mazen Hasna. A minorization-
maximization algorithm for an-based MIMOE secrecy rate maximization. In 2016 IEEE Global
Conference on Signal and Information Processing (GlobalSIP), pp. 975-980. IEEE, 2016.
Mathurin Massias, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon. Generalized concomi-
tant multi-task lasso for sparse multimodal regression. In International Conference on Artificial
Intelligence and Statistics, pp. 998-1007, 2018.
Sebastian Mika, Gunnar Ratsch, and Klaus-Robert Muller. A mathematical programming approach
to the kernel fisher algorithm. Advances in neural information processing systems, 13:591-597,
2000.
Eric Mjolsness and Charles Garrett. Algebraic transformations of objective functions. Neural Net-
works, 3(6):651-669, 1990.
Maher Moakher. A differential geometric approach to the geometric mean of symmetric positive-
definite matrices. SIAM Journal on Matrix Analysis and Applications, 26(3):735-747, 2005.
Julia P Owen, David P Wipf, Hagai T Attias, Kensuke Sekihara, and Srikantan S Nagarajan. Per-
formance evaluation of the Champagne source reconstruction algorithm on simulated and real
M/EEG data. Neuroimage, 60(1):305-323, 2012.
Diethard Ernst Pallaschke and Stefan Rolewicz. Foundations of mathematical optimization: convex
analysis without linearity, volume 388. Springer Science & Business Media, 2013.
Athanase Papadopoulos. Metric spaces, convexity and nonpositive curvature, volume 6. European
Mathematical Society, 2005.
Roberto D Pascual-Marqui, Christoph M Michel, and Dietrich Lehmann. Low resolution electro-
magnetic tomography: a new method for localizing electrical activity in the brain. International
Journal of psychophysiology, 18(1):49-65, 1994.
Hendrik Bernd Petersen and Peter Jung. Robust instance-optimal recovery of sparse signals at
unknown noise levels. arXiv preprint arXiv:2008.08385, 2020.
Peter Petersen, S Axler, and KA Ribet. Riemannian geometry, volume 171. Springer, 2006.
Sergey M. Plis, David M. Schmidt, Sung C. Jun, and Doug M. Ranken. A generalized spatiotemporal
covariance model for stationary background in analysis of MEG data. In 2006 International
Conference of the IEEE Engineering in Medicine and Biology Society, pp. 3680-3683. IEEE,
2006.
13
Under review as a conference paper at ICLR 2021
Ranjitha Prasad, Chandra R Murthy, and Bhaskar D Rao. Joint channel estimation and data detection
in MIMO-OFDM systems: A sparse Bayesian learning approach. IEEE Transactions on Signal
Processing, 63(20):5369-5382, 2015.
Tamas Rapcsak. Geodesic convexity in nonlinear optimization. Journal of Optimization Theory and
Applications, 69(1):169-183, 1991.
Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A unified convergence analysis of block
successive minimization methods for nonsmooth optimization. SIAM Journal on Optimization,
23(2):1126-1153, 2013.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for
image retrieval. International Journal of Computer Vision, 40(2):99-121, 2000.
Matthias W Seeger and David P Wipf. Variational Bayesian inference techniques. IEEE Signal
Processing Magazine, 27(6):81-91, 2010.
Kensuke Sekihara and Srikantan S Nagarajan. Electromagnetic brain imaging: a Bayesian perspec-
tive. Springer, 2015.
Michael Shvartsman, Narayanan Sundaram, Mikio Aoi, Adam Charles, Theodore Willke, and
Jonathan Cohen. Matrix-normal models for fMRI analysis. In International Conference on Arti-
ficial Intelligence and Statistics, pp. 1914-1923. PMLR, 2018.
Suvrit Sra and Reshad Hosseini. Conic geometric optimization on the manifold of positive definite
matrices. SIAM Journal on Optimization, 25(1):713-739, 2015.
Suvrit Sra and Reshad Hosseini. Geometric optimization in machine learning. In Algorithmic
Advances in Riemannian Geometry and Applications, pp. 73-91. Springer, 2016.
Ying Sun, Prabhu Babu, and Daniel P Palomar. Majorization-minimization algorithms in signal
processing, communications, and machine learning. IEEE Transactions on Signal Processing, 65
(3):794-816, 2017.
Michael E Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine
Learning Research, 1(Jun):211-244, 2001.
Sara Van de Geer, Johannes Lederer, et al. The Lasso, correlated design, and improved oracle in-
equalities. In From Probability to Statistics and Back: High-Dimensional Models and Processes-
A Festschrift in Honor of Jon A. Wellner, pp. 303-316. Institute of Mathematical Statistics, 2013.
CJ van Rijsbergen. Information retrieval. 1979.
Nisheeth K Vishnoi. Geodesic convex optimization: Differentiation on manifolds, geodesics, and
convexity. arXiv preprint arXiv:1806.06373, 2018.
Huilin Wei, Amirhossein Jafarian, Peter Zeidman, Vladimir Litvak, Adeel Razi, Dewen Hu, and
Karl J Friston. Bayesian fusion and multimodal DCM for EEG and fMRI. NeuroImage, 211:
116595, 2020.
Ami Wiesel, Teng Zhang, et al. Structured robust covariance estimation. Foundations and Trends®
in Signal Processing, 8(3):127-216, 2015.
David Wipf and Srikantan Nagarajan. A unified Bayesian framework for MEG/EEG source imaging.
NeuroImage, 44(3):947-966, 2009.
David P Wipf and Bhaskar D Rao. Sparse Bayesian learning for basis selection. IEEE Transactions
on Signal Processing, 52(8):2153-2164, 2004.
David P Wipf and Bhaskar D Rao. An empirical Bayesian strategy for solving the simultaneous
sparse approximation problem. IEEE Transactions on Signal Processing, 55(7):3704-3716, 2007.
David P Wipf, Julia P Owen, Hagai T Attias, Kensuke Sekihara, and Srikantan S Nagarajan. Robust
Bayesian estimation of the location, orientation, and time course of multiple correlated neural
sources using MEG. NeuroImage, 49(1):641-655, 2010.
14
Under review as a conference paper at ICLR 2021
Anqi Wu, Oluwasanmi Koyejo, and Jonathan W Pillow. Dependent relevance determination for
smooth and structured sparse regression. J. Mach. Learn. Res., 20(89):1-43, 2019.
Tong Tong Wu, Kenneth Lange, et al. The MM alternative to EM. Statistical Science, 25(4):492-
505, 2010.
Wei Wu, Srikantan Nagarajan, and Zhe Chen. Bayesian machine learning: EEG\MEG signal pro-
cessing measurements. IEEE Signal Processing Magazine, 33(1):14-36, 2016.
Alan L Yuille and Anand Rangarajan. The concave-convex procedure. Neural computation, 15(4):
915-936, 2003.
Pourya Zadeh, Reshad Hosseini, and Suvrit Sra. Geometric mean metric learning. In International
Conference on Machine Learning, pp. 2464-2471, 2016.
Zhilin Zhang and Bhaskar D Rao. Sparse signal recovery with temporally correlated source vectors
using sparse Bayesian learning. IEEE Journal of Selected Topics in Signal Processing, 5(5):
912-926, 2011.
A Proof of Theorem 1
Proof. We start the proof by recalling equation 8:
1T
LncΓ, Λ) = - logp(Y∣Γ, Λ) = log∣Σy | + T ^y(t)>Σ-1y(t) .	(16)
t=1
The upper bound on the log ∣Σy∣ term can be directly inferred from the concavity of the log-
determinant function and its first-order Taylor expansion around the value from the previous iter-
ation, Σky, which provides the following inequality (Sun et al., 2017, Example 2):
log z ∣≤ log i^yi+tr [3y)T(%-空)i
= log^yi+tr 1(空)-1 引-tr [(a)T Σ] .	(17)
Note that the first and last terms in equation 17 do not depend on Γ; hence, they can be ignored in
the optimization procedure. Now, we decompose Σy into two terms, each of which only contains
either the noise or source covariances:
tr [(a) — 1 %] = tr [(空)-1 (A + lγl>)]="[(现尸 Ai + 红[(现尸 lγl>] . (18)
In next step, We decompose the second term in equation 8, T1 PT=I y(t)>Σy 1y(t), into two terms,
each of which is a function of either only the noise or only the source covariances. To this end, we
exploit the following relationship between sensor and source space covariances:
1T	1T	1T
T Ey(t)>∑y 1y(t) = T EXk(t)>Γ-1xk⑴ + T E(y(t) - Lxk(t))>Λ-1(y(t) - LXk(t)).
t=1	t=1	t=1
(19)
By combining equation 18 and equation 19, rearranging the terms, and ignoring all terms that do not
depend on Γ, we have:
1T
LnCr) ≤ tr [(∑y)-1 LΓL>] + T X xk(t)>Γ-1Xkct) + const
t=1
=tr((Ck)-1 Γ) + tr(MSSΓ-1) + const = Lconvce(Γ, Λk) + const,
(20)
where CSS =(L> (∑y)-1 L) and Mk = TT PT=IXk(t)xk(t)>. Note that constant values in
equation 20 do not depend on r; hence, they can be ignored in the optimization procedure. This
15
Under review as a conference paper at ICLR 2021
proves the equivalence of equation 8 and equation 9 when the optimization is performed with respect
to Γ.
The equivalence of equation 8 and equation 11 can be shown analogously, with the difference that
we only focus on noise-related terms in equation 18 and equation 19:
1T
Ln(Λ) ≤ tr [(∑y)-1 Λ] + T X(y(t) - LXk(t))>Λ-1(y(t) - LXk(t)) + const
t=1
=tr((CN)-1 Λ) + tr(MN Λ-1) + const = Lnonse(Γk, Λ + const ,	(21)
where CN = ∑y, and MN = T PT=ι(y(t) - LXk(t))(y(t) - Lxk(t))>. Constant values in
equation 21 do not depend on Λ; hence, they can again be ignored in the optimization procedure.
Summarizing, we have shown that optimizing equation 8 is equivalent to optimizing Lcnoonisve(Γk, Λ)
and LsoUVce(Γ, Λk), which concludes the proof.	□
B Proof of Theorem 2
Before presenting the proof, the subsequent definitions and propositions are required:
Definition 4 (Geodesic path). Let M be a Riemannian manifold, i.e., a differentiable manifold
whose tangent space is endowed with an inner product that defines local Euclidean structures. Then,
a geodesic between two points on M, denoted by p0, p1 ∈ M, is defined as the shortest connecting
path between those two points along the manifold, ζl(p0, p1) ∈ M for l ∈ [0, 1], where l = 0 and
l = 1 defines the starting and end points of the path, respectively.
In the current context, ζl (p0, p1) defines a geodesic curve on the positive definite (P.D.) mani-
fold joining two P.D. matrices, P0 , P1 > 0. The specific pairs of matrices we will deal with are
{CkS,MkS}and{CkN,MkN}.
Definition 5 (Geodesic on the P.D. manifold). Geodesics on the manifold of P.D. matrices can be
shown to form a cone within the embedding space. We denote this manifold by S++. Assume two
P.D. matrices P0, P1 ∈ S++. Then, for l ∈ [0, 1], the geodesic curve joining P0 to P1 is defined as
(Bhatia, 2009, Chapter. 6):
ξι(Po,PI) = (P0)2 ((P0)T2P1(P0)T2)l(P0)1	l ∈ [0,1] .	(22)
Note that P0 and P1 are obtained as the starting and end points of the geodesic path by choosing
l = 0 and l = 1, respectively. The midpoint of the geodesic, obtained by setting l = 11, is called the
geometric mean. Note that, according to Definition 5, the following equality holds :
ξι(Γo,Γ1)-1 = ((ΓoV2 ((Γo)T2Γι(Γ0)T2)l(ΓoV2)
=((Γ0)T2 ((ΓoV2(Γι)T(ΓoV2)l(Γ0)T2) = ξι(Γ-1,Γ-1) .	(23)
Definition 6 (Geodesic convexity). Let p0 and p1 be two arbitrary points on a subset A of a
Riemannian manifold M. Then a real-valued function f with domain A ⊂ M with f : A → R is
called geodesic convex (g-convex) if the following relation holds:
f (ζl(p0,p1)) ≤ lf(p0) + (1 - l)f(p1) ,	(24)
where l ∈ [0, 1] and ζ(p0, p1) denotes the geodesic path connecting two points p0 and p1 as defined
in 4. Thus, in analogy to classical convexity, the function f is g-convex if every geodesic ζ(p0, p1)
of M between p0, p1 ∈ A, lies in the g-convex set A. Note that the set A ⊂ M is called g-convex,
if any geodesics joining an arbitrary pair of points lies completely in A.
Remark 7. Note that g-convexity is a generalization of classical (linear) convexity to non-Euclidean
(non-linear) geometry and metric spaces. Therefore, it is straightforward to show that all convex
functions in Euclidean geometry are also g-convex, where the geodesics between pairs of matrices
are simply line segments:
ζl(p0,p1) = lp0 + (1 - l)p1 .	(25)
16
Under review as a conference paper at ICLR 2021
For the sake of brevity, we omit a detailed theoretical introduction of g-convexity, and only borrow
a result from Zadeh et al. (2016); Sra & Hosseini (2015). Interested readers are referred to Wiesel
et al. (2015, Chapter 1) for a gentle introduction to this topic, and Papadopoulos (2005, Chapter. 2)
Rapcsak (1991); Ben-Tal (1977); Liberti (2004); Pallaschke & Rolewicz (2013); Bonnabel & Sepul-
chre (2009); Moakher (2005); Sra & Hosseini (2016); Vishnoi (2018) for more in-depth technical
details.
Now we are ready to state the proof, which parallels the one provided in Zadeh et al. (2016, Theorem.
3).
Proof. We only show the proof for Lscoounrvce(Γ, Λk). The proof for Lcnoonisve (Γk, Λ) can be presented
analogously; and therefore, is omitted here for brevity. We proceed in two steps. First, we limit
our attention to P.D. manifolds and express equation 24 in terms of geodesic paths and functions
that lie on this particular space. We then show that Lscoounrvce(Γ, Λk) is strictly g-convex on this
specific domain. In the second step, we then derive the updates rules proposed in equation 13 and
equation 14.
B.1	Part I: Proving g-convexity of the Majorizing Cost Functions
We consider geodesics along the P.D. manifold by setting ζl (p0, p1) to ξl (Γ0, Γ1) as presented in
Definition 5, and define f(.) to be f(Γ) = tr(CkSΓ) + tr(MkSΓ-1), representing the cost function
Lscoounrvce(Γ,Λk).
We now show that f (Γ) is strictly g-convex on this specific domain. For continuous functions as
considered in this paper, fulfilling equation 24 for f(Γ) and ξl(Γ0, Γ1) with l = 1/2 is sufficient to
prove strict g-convexity:
tr (Ckξ1∕2(Γ0,巧))+ tr (Mkξ1∕2(Γ0,巧)-1)
< Itr(CSr0) + 1tr (MkΓ0-1)
+ %r(CSΓι) + 2tr (MkΓι-1) .	(26)
Given CkS ∈ S++, i.e., CkS > 0 and the operator inequality (Bhatia, 2009, Chapter. 4)
ξι∕2(ro,rι) Y 2γo + 2γi ,	(27)
we have:
tr (CSξ1∕2(Γ0, Γι)) < 2tr (CkΓo) + ∣tr (CkΓ1) ,	(28)
which is derived by multiplying both sides of equation 27 with CkS followed by taking the trace on
both sides.
Similarly, we can write the operator inequality for {Γ0-1, Γ1-1} using equation 23 as:
ξ1∕2(Γ0, Γ1)-1 = ξ1∕2(Γ-1, Γ-1) Y 1Γ-1 + 2Γ-1 ,	(29)
Multiplying both sides of equation 29 by MkS ∈ S++, and applying the trace operator on both sides
leads to:
tr (Mkξ1∕2(Γ0, Γ1)-1) < 2tr (MkΓ0-1) + Itr(MSrIT) .	(30)
Summing up equation 28 and equation 30 proves equation 26 and concludes the first part of the
proof.
B.2 Part II: Detailed Derivation of the Update Rules in Equations 13 and 14
We now present the second part of the proof by deriving the update rules in equations 13 and 14.
Since the cost function Lscoounrvce(r, Λk) is strictly g-convex, its optimal solution in the k-th iteration
17
Under review as a conference paper at ICLR 2021
is unique. More concretely, the optimum can be analytically derived by taking the derivative of
equation 9 and setting the result to zero as follows:
VLconvce(Γ, Λk) = (Ck)T- Γ-1MkΓ-1 = 0 ,	(31)
which results in
Γ (CS)T Γ = MS .	(32)
This solution is known as the Riccati equation, and is the geometric mean between CkS and MkS
(Davis et al., 2007; Bonnabel & Sepulchre, 2009):
1
Γk+1 = (CS)2 ((CS)-1/2Mk(CS)-1/2)2 (Ck)1.
The update rule for the full noise covariance matrix can be derived analogously:
Λk+1 = (CN)1 ((CN)T/2MN(CN)T/2)2 (CN)1.
Remark 8. Note that the obtained update rules are closed-form solutions for the surrogate cost
functions, equations 9 and 11, which stands in contrast to conventional majorization minimization
algorithms (see section C in the appendix), which require iterative procedures in each step of the
optimization.
Deriving the update rules in equation 13 and equation 14 concludes the second part of the proof of
Theorem 2.	□
C Proof of Theorem 3
In the following, we provide proof for Theorem 3 by showing that alternating update rules for Γ and
Λ in equation 13 and equation 14 are guaranteed to converge to a local minimum of the Bayesian
Type-II likelihood equation 8. In particular, we will prove that FUN learning is an instance of
the general class of majorization-minimization (MM) algorithms, for which this property follows
by construction. To this end, we first briefly review theoretical concepts behind the majorization-
minimization (MM) algorithmic framework (Hunter & Lange, 2004; Razaviyayn et al., 2013; Ja-
cobson & Fessler, 2007; Wu et al., 2010).
C.1 Required Conditions for Majorization-Minimization algorithms
MM encompasses a family of iterative algorithms for optimizing general non-linear cost functions.
The main idea behind MM is to replace the original cost function in each iteration by an upper bound,
also known as majorizing function, whose minimum is easy to find. The MM class covers a broad
range of common optimization algorithms such as convex-concave procedures (CCCP) and proximal
methods (Sun et al., 2017, Section IV), (Mjolsness & Garrett, 1990; Yuille & Rangarajan, 2003; Lipp
& Boyd, 2016). Such algorithms have been applied in various domains such as brain source imaging
(Hashemi & Haufe, 2018; Bekhti et al., 2018; Cai et al., 2020a; Hashemi et al., 2020), wireless
communication systems with massive MIMO technology (Masood et al., 2016; Haghighatshoar &
Caire, 2017; Khalilsarai et al., 2020), and non-negative matrix factorization (Fagot et al., 2019).
Interested readers are referred to Sun et al. (2017) for an extensive list of applications on MM.
The problem of minimizing a continuous function f(u) within a closed convex setU ⊂ Rn:
min f(u) subject to u ∈ U ,	(33)
within the MM framwork can be summarized as follows. First, construct a continuous surrogate
function g(u|uk) that majorizes, or upper-bounds, the original function f(u) and coincides with
f(u) at a given point uk:
[A1]	g(uk|uk) = f(uk)	∀uk ∈U
[A2]	g(u|uk) ≥f(u)	∀u,uk ∈U .
18
Under review as a conference paper at ICLR 2021
Second, starting from an initial value u0 , generate a sequence of feasible points
u1, u2, . . . , uk, uk+1 as solutions of a series of successive simple optimization problems, where
[A3]	uk+1 := arg min g(u|uk) .
u∈U
If a surrogate function fulfills conditions [A1]-[A3], then the value of the cost function f decreases
in each iteration: f (uk+1) ≤ f(uk). For the smooth functions considered in this paper, we further
require that the derivatives of the original and surrogate functions coincide at uk :
[A4]	Vg(uk ∣uk) = Vf (uk)	∀ Uk ∈U .
We can then formulate the following theorem:
Theorem 9. Assume that an MM algorithm fulfills conditions [A1]-[A4]. Then, ^very limit point
of the sequence of minimizers generated in [A3], is a stationary point of the original optimization
problem in equation 33.
Proof. A detailed proof is provided in Razaviyayn et al. (2013, Theorem 1).	□
C.2 Detail Derivation of the Proof of Theorem 3
We now show that FUN learning is an instance of majorization-minimization as defined above,
which fulfills Theorem 9.
Proof. We need to prove that conditions [A1]-[A4] are fulfilled for FUN learning. To this end, We
recall the upper bound on log ∣Σy∣ in equation 17, which fulfills condition [A2] since it majorizes
log ∣Σy | as a result of the concavity of the log-determinant function and its first-order Taylor ex-
pansion around Σky. Besides, it automatically satisfies conditions [A1] and [A4] by construction,
because the majorizing function in equation 17 is obtained through a Taylor expansion around Σky .
Concretely, [A1] is satisfied because the equality in equation 17 holds for Σy = Σky. Similarly, [A4]
is satisfied because the gradient of log ∣Σy | at point ∑y, (∑y) 1 defines the linear Taylor approxi-
mation log ∣∑y I + tr [(∑y) 1 (Σy 一 ∑y)]. Thus, both gradients coincide in ∑y by construction.
Now, we prove that [A3] can be satisfied by showing that Lscoounrvce(Γ, Λk) reaches its global min-
imum in each MM iteration. This is guaranteed if Lscoounrvce(Γ, Λk) can be shown to be convex or
g-convex with respect to Γ. To this end, we first require the subsequent proposition:
Proposition 10. Any local minimum ofa g-convex function over a g-convex set is a global minimum.
Proof. A detailed proof is presented in Rapcsak (1991, Theorem 2.1).	□
Given the proof presented in appendix B.1, we can conclude that equation 20is g-convex; hence, any
local minimum of Lscoounrvce (Γ, Λk) is a global minimum according to Proposition 10. This proves that
condition [A3] is fulfilled and completes the proof that the optimization of equation 8 with respect
to Γ using the convex surrogate cost function equation 9 leads to an MM algorithm. For the sake
of brevity, we omit the proof for the optimization with respect to Λ based on the convex surrogate
function in equation 11, Lnonve(Γk, Λ), as it can be presented, analogously.	□
D	Derivation of Champagne as a Special Case of FUN Learning
We start the derivation of update rule equation 15 by constraining Γ to the set of diagonal matrices
W: Γ = diag(γ), where γ = [γ1, . . . , γN]>. We continue by rewriting the constrained optimization
with respect to the source covariance matrix,
Γk+1 = arg min tr(CkSΓ) + tr(MkSΓ-1) ,	(34)
Γ∈W, Λ=Λk
19
Under review as a conference paper at ICLR 2021
as follows:
γk+1
arg min diag [(Ck)-1] Y + diag [Mk] YT
Y, Λ=Λk	S_____t_______{__sz_______________}
LdiUgce(YlYk)
(35)
whereY-1 = [γ1-1, . . . , γN-1]> is defined as the element-wise inversion ofY. The optimization with
respect to the scalar source variances is then carried out by taking the derivative of equation 35 with
respect to γn, for n = 1, . . . , N, and setting it to zero,
W ([(CS)-1 i γn + [Mk]γ-1)
=h(Ck)-lin,n - (⅛ 皿八
= 0 for n = , . . . , N ,
where Ln denotes the n-th column of the lead field matrix. This yields the following update rule
γn+=
[MS n,n	=	T P=I(Xn (t))2
t [(Ck)-1 in,n_ t L>3y )-1 Ln
for n = , . . . , N ,
which is identical to the update rule of Champagne (Wipf & Nagarajan, 2009).
E Derivation of Champagne with Heteroscedastic Noise Learning
as a S pecial Case of FUN Learning
Similar to Appendix D, we start by constraining Λ to the set of diagonal matrices W: Λ = diag(λ),
where λ = [λ1, . . . , λM]>. We continue by reformulating the constrained optimization with respect
to the noise covariance matrix,
Λk+1 = arg min tr(CkNΛ) + tr(MkNΛ-1) ,
Λ∈W, Γ=Γk
(36)
as follows:
λk+1 = arg min diag [(CN)-Ii λ + diag [mN] λ-1 ,	(37)
λ, Γ=Γk	、 L	J /
Ldiag (λ∣λk)
noise
where λ-1 = [λ1-1, . . . , λ-M1]> is defined as the element-wise inversion of λ. The optimization
with respect to the scalar noise variances then proceeds by taking the derivative of equation 37 with
respect to λm, for m = , . . . , M, and setting it to zero,
∂λm (h(CN) 1iλm+ [MN] λm1)
=[(CN)-1i	-，[MN]
m,m (λm)
0 for m = , . . . , M .
This yields the following update rule:
1 -
∏Μ⅛]	T P=1(y(t) - LXk(t))(y(t) - LXk(t))>
N m,m
t [(CN)-1i = t	[(∑y)-1i
m,m	m,m
for m = , . . . , M ,
(38)
which is identical to the update rule of the Champagne with heteroscedastic noise learning as pre-
sented in Cai et al. (2020a).
20
Under review as a conference paper at ICLR 2021
-O-Homoscedastic-≡-Heteroscedastic-O- Full Structure
əs-oN ΘJnlon-s=nLI-
əs-oN O-lsepəosojələH
Figure 4: Accuracy of the noise
covariance matrix reconstruction in-
curred by three different noise learn-
ing approaches assuming homoscedas-
tic (red), heteroscedastic (green) and
full-structure (blue) noise covariances.
The ground-truth noise covariance ma-
trix is either full-structure (upper row)
or heteroscedastic diagonal (lower row).
Performance is assessed in terms of
the Pearson correlation between the en-
tries of the original and reconstructed
noise covariance matrices, Λ and Λ, de-
noted by Λsim (left column). Shown
is the similarity error 1 - Λsim . Fur-
ther, the normalized mean squared error
(NMSE) between A and A, defined as
NMSE = ||A-A||F∕∣∣A∣∣F is reported
(right column).
F Pseudo-EEG Signal Generation
Our simulation setting is an adoption of the EEG inverse problem, where brain activity is to be
reconstructed from simulated pseudo-EEG data (Haufe & Ewald, 2016).
Forward Modeling: Populations of pyramidal neurons in the cortical gray matter are known to be
the main drivers of the EEG signal (Hamalainen et al., 1993; Baillet et al., 2001). Here, we use a
realistic volume conductor model of the human head to model the linear relationship between pri-
mary electrical source currents generated within these populations and the resulting scalp surface
potentials captured by EEG electrodes. The lead field matrix, L ∈ R58×2004 , was generated using
the New York Head model (Huang et al., 2016) taking into account the realistic anatomy and elec-
trical tissue conductivities of an average human head. In this model, 2004 dipolar current sources
were placed evenly on the cortical surface and 58 sensors were considered. The lead field matrix,
L ∈ R58×2004 was computed using the finite element method. Note that the orientation of all source
currents was fixed to be perpendicular to the cortical surface, so that only scalar source amplitudes
needed to be estimated.
Evaluation Metrics: Source reconstruction performance was evaluated according to the following
metrics. First, the earth mover’s distance (EMD) (Rubner et al., 2000; Haufe et al., 2008)) was
used to quantify the spatial localization accuracy. The EMD measures the cost needed to transform
two probability distributions defined on the same metric domain (in this case, distributions of the
true and estimated sources defined in 3D Euclidean brain space) into each other. EMD scores were
normalized to [0, 1]. Second, the error in the reconstruction of the source time courses was measured.
To this end, Pearson correlation between all pairs of simulated and reconstructed (i.e., those with
non-zero activations) source time courses was assessed as the mean of the absolute correlations
obtained for each source, after optimally matching simulated and reconstructed sources based on
maximal absolute correlation. We also report another metric for evaluating the localization error
as the average Euclidean distance (EUCL) (in mm) between each simulated source and the best (in
terms of absolute correlations) matching reconstructed source. For assessing the recovery of the true
support, we also compute F1-measure scores (Chinchor & Sundheim, 1993; van Rijsbergen, 1979):
F1 = 2×TP∕P+TP+FP, where P denotes the number of true active sources, while TP and FP are the
numbers of true and false positive predictions. Note that perfect support recovery, i.e., F1 = 1, is
only achieved when there is a perfect correspondence between ground-truth and estimated support.
21
Under review as a conference paper at ICLR 2021
To evaluate the accuracy of the noise covariance matrix estimation, the following two metrics were
calculated: the Pearson correlation between the original and reconstructed noise covariance ma-
trices, Λ and Λ, denoted by Λsim, and the normalized mean squared error (NMSE) between Λ
and A, defined as: NMSE = ∣∣Λ - Λ∣∣F∕∣∣Λ∣∣F. Similarity error was then defined as one minus
the Pearson correlation: 1 - Λsim . Note that NMSE measures the reconstruction of the true scale
of the noise covariance matrix, while Λsim is scale-invariant and hence only quantifies the overall
structural similarity between simulated and estimated noise covariance matrices.
Evaluating the accuracy of the noise covariance matrix estimation: Figure 4 depicts the accuracy
with which the covariance matrix is reconstructed by three different noise learning approaches as-
suming noise with homoscedastic (red), heteroscedastic (green) and full (blue) structure. The ground
truth noise covariance matrix either had full (upper row) or heteroscedastic (lower row) structure.
Performance was measured in terms of similarity error and NMSE. Similar to the trend observed
in Figure 2, full-structure noise learning leads to better noise covariance estimation accuracy (lower
NMSE and similariy error) for the full-structure noise model, while superior reconstruction perfor-
mance is achieved for heteroscedastic noise learning when true noise covariance is heteroscedastic.
G	Further Details on Auditory Evoked Fields (AEF) Dataset
The MEG data used in this article were acquired in the Biomagnetic Imaging Laboratory at the
University of California San Francisco (UCSF) with a CTF Omega 2000 whole-head MEG system
from VSM MedTech (Coquitlam, BC, Canada) with 1200 Hz sampling rate. The lead field for each
subject was calculated with NUTMEG (Dalal et al., 2004) using a single-sphere head model (two
spherical orientation lead fields) and an 8 mm voxel grid. Each column was normalized to have a
norm of unity. The neural responses of one subject to an Auditory Evoked Fields (AEF) stimulus
were localized. The AEF response was elicited with single 600 ms duration tones (1 kHz) presented
binaurally. 120 trials were collected for AEF dataset. The data were first digitally filtered from 1 to
70 Hz to remove artifacts and DC offset, time-aligned to the stimulus, and then averaged across the
following number of trials:{1,2,12, 63,120}. The pre-stimulus window was selected to be 100 ms
to 5 ms and the post-stimulus time window was selected to be 60 ms to 180 ms, where 0 ms is the
onset of the tone (Wipf et al., 2010; Dalal et al., 2011; Owen et al., 2012; Cai et al., 2019a).
22