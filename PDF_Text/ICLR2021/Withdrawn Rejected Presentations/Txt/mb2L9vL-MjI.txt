Under review as a conference paper at ICLR 2021
The Quenching-Activation Behavior of the
Gradient Descent Dynamics for Two-layer
Neural Network Models
Anonymous authors
Paper under double-blind review
Ab stract
A numerical and phenomenological study of the gradient descent (GD) algorithm
for training two-layer neural network models is carried out for different parameter
regimes. It is found that there are two distinctive phases in the GD dynamics in
the under-parameterized regime: An early phase in which the GD dynamics fol-
lows closely that of the corresponding random feature model, followed by a late
phase in which the neurons are divided into two groups: a group of a few (maybe
none) “activated” neurons that dominate the dynamics and a group of “quenched”
neurons that support the continued activation and deactivation process. In par-
ticular, when the target function can be accurately approximated by a relatively
small number of neurons, this quenching-activation process biases GD to picking
sparse solutions. This neural network-like behavior is continued into the mildly
over-parameterized regime, in which it undergoes a transition to a random feature-
like behavior where the inner-layer parameters are effectively frozen during the
training process. The quenching process seems to provide a clear mechanism for
“implicit regularization”. This is qualitatively different from the GD dynamics
associated with the “mean-field” scaling where all neurons participate equally.
1	Introduction
In the past few years, much effort has been devoted to the understanding of the theoretical foun-
dation behind the spectacular success of neural network (NN)-based machine learning. The main
theoretical questions concern the training process and the generalization property of solutions found.
For two-layer neural network (2LNN) and deep residual neural network models, it has been proved
that solutions with “good” generalization performance do exist. Specifically, it has been shown that
for the appropriate classes of target functions, the generalization error associated with the global
minimizers of some properly regularized 2LNN and deep residual neural networks obey Monte
Carlo-like estimates: O(1∕m) for the approximation error and O(1∕√n) for the estimation error,
where m and n are the number of parameters and the size of the training set, respectively (Barron,
1994; Bach, 2017; E et al., 2019b;a). The fact that these estimates do not suffer from the curse of di-
mensionality (CoD) is one of the fundamental reasons behind the success of neural network models
in high dimensions.
An important open question is: Do standard optimization algorithms used in practice find good solu-
tions? NN-based models often work in the over-parameterized regime where the models can easily
fit all the training data, and some of solutions may give rise to large test errors (Wu et al., 2017).
However, it has been observed in practice that small test error can often be achieved with appropriate
choice of the hyper-parameters, even without the need of explicit regularization (Neyshabur et al.,
2014; Zhang et al., 2017). This means that there are some “implicit regularization” mechanisms at
work for the optimization algorithm with the particular choices of hyper-parameters.
A rather complete picture has been established for highly over-parameterized NN models. Unfor-
tunately the overall result is somewhat disappointing: While one can prove that GD converges to
a global minimizer of the empirical risk (Du et al., 2019b;a), the generalization properties of this
global minimizer is no better than that of an associated random feature model (RFM) (Jacot et al.,
2018; E et al., 2020; Arora et al., 2019). In fact, E et al. (2020) and Arora et al. (2019) proved that
the entire GD paths for the NN model and the associated RFM stay uniformly close for all time.
1
Under review as a conference paper at ICLR 2021
A natural question is then: Can there be implicit regularization when the network is less over-
parameterized? What would be the mechanism of the implicit regularization? More generally,
what is the qualitative behavior of the GD dynamics in different regimes including the under-
parameterized regimes? In this paper, we provide a systematic investigation for two-layer neural
networks by well-designed experiments. Our objective is to get some insight from this kind of ex-
perimental studies, which we hope will be helpful for subsequent theoretical work. Specifically, our
findings are summarized as follows.
•	It is observed that when the network is less over-parameterized, the GD dynamics exhibit
two phases. During the first phase, GD follows closely that of the corresponding RFM.
Afterwards, GD enters the second phase where neurons form two groups (the first group
might be empty): a group of activated neurons and a group of quenched neurons. Depend-
ing on the target functions, the quenched neurons can exhibit continued quenching and
sparse activation processes. In particular, if the target function can be well approximated
by a small number of neurons, GD is biased to picking sparse solutions.
•	Based on these observations, we then investigate how the extent of over-parameterization
affects the generalization properties of GD solutions. We find that the test error shows a
sharp transition within the mildly over-parameterized regime. This transition suggests that
implicit regularization is quite sensitive to the change of the network width.
•	Lastly, we study 2LNNs under mean-field scaling (Chizat & Bach, 2018; Mei et al., 2018;
Rotskoff & Vanden-Eijnden, 2018; Sirignano & Spiliopoulos, 2020), i.e. with an extra 1/m
factor added to the expression of the function, where m denotes the number of neurons.
We observe that in this case all the neurons contribute pretty much equally and the test
performance is much more robust to the change of network width.
2	Preliminaries
2.1	Two-layer neural networks
Under conventional scaling, a two-layer neural network model is given by:
m
fm(x; a, B) =	ajσ(bjTx) = aT σ(Bx),
j=1
(1)
where a ∈ Rm, B = (b1, b2, . . . , bm)T ∈ Rm×d and σ(t) = max(0, t) is the ReLU activation
function. Later we will consider the mean-field scaling where the expression above is replaced by
1m	1
fm(x; a, B) = 一 Eajσ(bTx) = 一aTσ(Bx),
m j=1	m
(2)
but we will focus on the conventional scaling unless indicated otherwise. As a comparison, the
random feature model is given by fm (x; a, B0), where only the coefficient a can be varied; B0 is
randomly sampled and is fixed during training.
Let S = {(xi, yi = f*(xi))}n=ι denote the training set. f * is the target function. We assume that
{xi} are drawn independently from π0, the uniform distribution over Sd-1 := {x ∈ Rd : kxk = 1}.
The empirical risk and the population risk are defined by TRn(a, B) = ɪ Pn=ι(fm(xi; a, B)-
f *(xi))2 and R(a, B)= Ex〜∏°[(fm(x; a, B)- f *(x))2], respectively.
Following the study of the function space for two-layer neural networks (E et al., 2019c; Bach,
2017), We will focus on target functions of the form f *(x) = Eb〜∏* [a*(b)σ(bτx)] with π* being
a probability distribution over Sd-1. The population risk can be written as:
m
R(a, B) = Ex[(^X αjσ(bj ∙ x) — Eb〜∏* [a*(b)σ(b* ∙ x)])2]
j=1
mm
=E aj aj2 k(bjι, %) - 2£ ajEb 〜∏* [a* (b)k(bj, b)]
j1,j2 =1	j=1
+ Eb〜∏*Ebo〜∏* [a*(b)a*(b0)k(b, b0)],	(3)
2
Under review as a conference paper at ICLR 2021
where k(b, b0) = ∣∣bkkb0k (sinθ + (π 一 θ)cosθ) with θ = arccos(hb, b0))(Cho & Saul, 2009).
Since our primary interest is to gain some insight about the behavior of the training dynamics in
different parameter and scaling regimes, we choose to work with an idealized setting with simple
synthetic target functions instead of real data sets. Real data sets have extra complicating factors that
prevent us from focusing on particular aspects of the training process. In fact most of our efforts will
be devoted to target functions of finite neurons or that can be accurately approximated by a relatively
small number of neurons, similar to the widely used teacher-student setting (Saad & Solla, 1995;
Tian, 2017; Safran & Shamir, 2018; Goldt et al., 2019). However, we will also discuss one example
for which this is not the case.
2.2	GD dynamics for two-layer neural networks
We consider the most popular initialization (LeCun et al., 2012; He et al., 2015),
aj(0)〜N(0,β1 2),	bj(0)〜N(0,I∕d).	(4)
For the original LeCUn initialization (LeCUn et al., 2012), β = 1∕√m. However, We have found
consistently that for 2LNNs the behavior of the GD dynamics is qualitatively very close to the case
when β = 0. We refer to Appendix A for some numerical results along this line. For simplicity we
will focus on the case when β = 0.
Du et al. (2019b) proved that GD converges exponentially fast to a global minimizer in the highly
over-parameterized regime. Subsequently it was shown in (E et al., 2020) that these GD solutions
are uniformly close to that of the associated RFM with B0 = B(0) as the features:
Theorem 1 (Informal). Let a(t) denote the GD solution of associated RFM. For any δ ∈ (0,1), if
m ≥ poly(n, log(1∕δ)), β = 0, with probability 1 一 δ we have
SUp	|fm(x; α(t), B(t)) - fm(x; d(t), B°)∣≤ poly(n, log(1∕δ)).	(5)
x∈Sd-1,t∈[0,∞)	m
The key observations for this case is the time scale separation: aj- (t)〜 O(∣bj∣) = O(1) and
*
bj(t)〜O(∣aj |) = O (poly(n)∕m). In the highly over-parameterized regime, the dynamics of bj is
effectively frozen. GD for the 2LNN degenerates to the GD for the corresponding RFM.
There are three important large parameters: m, n, d. They are the number of neurons, the number
of training samples, the input dimension, respectively. There are two obvious extreme situations
that are of interest. One is when m n. This was described above in Theorem 1 and is relatively
well understood. The other is when n m, which will be investigated by considering n = ∞.
The regime when m 〜n or m 〜n/(d + 1) are also of interest since these are regimes where the
“resonance” (or the closely related “double descent”) phenomena might occur, as we learned from
previous work on RFM (Advani & Saxe, 2017; Belkin et al., 2019; Ma et al., 2020).
3	GD dynamics for the case with infinite samples
As a starting point, we first investigate the GD dynamics for the population risk, i.e. n = ∞. We
will see later that the phenomena revealed here are indicative of the neural network-like (NN-like)
behavior for GD dynamics.
3.1	Single neuron target function: Neuron activation and deactivation
First we look at the case where the target function is a single neuron:
f；(X)= σ(b* ∙x),
where b； = eι. The results are presented in Figure 1. One can observe several interesting features
about the GD dynamics.
(1) Initially, the GD dynamics for the 2LNN is close to that of the corresponding RFM. (2) The GD
dynamics for the 2LNN and RFM depart from each other around the time when the loss function for
the RFM starts to saturate. (3)The converged solution for the 2LNN is very sparse. In fact only one
neuron contributes significantly to the model in this experiment.
3
Under review as a conference paper at ICLR 2021
(a)
(b)
Figure 1: The dynamic behavior of learning the single neuron target function. The results of RFM are shown
as a comparison. Here, m = 200, d = 100 and the learning rate is 0.001. (a) The dynamics of the population
risk. (b) The a coefficients of the converged solution. (c) The dynamics of the magnitude of each neuron, where
the magnitude is defined as akbk. (d) The projection to the first two coordinates of b. The green and orange
ones correspond to the initialization and GD solution, respectively.
1
0
0
0
0
0
8
6
2
0
言。4
15000
T=20000
T=0
bj[0]
(d)
Number of iterations
(c)
0	5000
We also see that there are two phases in the GD dynamics. In the first phase, the dynamics follows
closely the GD dynamics for the RFM. In the second phase, the outer layer coefficients a are small
except for one neuron. In the transition from the first to the second phase, except for one neuron, all
the other neurons are “quenched” in the sense that their outer layer coefficients a keep decreasing.
Thus, the dynamics of their inner layer weights b become very slow. Consequently, as shown in
Figure 1d, only one neuron is “specialized" to the teacher b* (Saad & Solla, 1995), and the inner
layer weights of most neurons have hardly changed. The same behavior was observed with other
realizations of the initial data, except that the number of “activated” neurons can be different. But in
all cases, we always observe few “activated” neurons in the second phase.
3.1.1	Circle neuron target function: Multi-step phenomenon
Next We consider a more sophisticated target function:再(x) = Eb〜∏2 [σ(bτx)], where ∏ is the
uniform distribution over the unit circle Γ = {b ∈ Rd : b12 + b22 = 1 and bi = 0 ∀i > 2}.

%

IO0
10^*
0
T = 2 × 105
T = 106
T = 3 × 106
20
40
60
Index of neurons
80
100
(c)
Figure 2: The dynamic behavior of learning the circle neuron target function. Here m = 100, d = 100 and
the learning rate is 0.005. (a) The dynamics of the population risk. (b) The dynamics of the magnitude of each
neuron; the inset is the zoom-in of the first 30000 iterations. (c) The a coefficients of the solutions selected to
represent the three steps.
A typical dynamic behavior of the population risk is shown in Figure 2a. We see that there are still
two phases. But in the second phase, the population risk decreases in a “step-like” fashion. To see
what happens, we plot the dynamics of the a coefficient of each neuron in Figure 2b. We see that
after the first phase, most of the neurons start to die out slowly (see the inset of Figure 2b). As the GD
dynamics proceeds, a few new neurons are activated from the “quenched neurons”. This activation
process can be very slow, and the loss function is almost constant before activation actually happens.
The activation process is relatively fast and causes a fast decay of the loss function. Figure 2c shows
three representative solutions for the three steps. We see that from the first and second step, two
more neurons are activated. From the second to the third step, one more neuron pops out.
3.1.2	Finite neuron target functions
The observation that GD dynamics picks out sparse solutions and the associated quenching-
activation behavior happens in a more general setting: learning finite neurons. Consider the target
function f3(x) = Pjm=I a^σ(b* ∙ x).
4
Under review as a conference paper at ICLR 2021
We are interested in the case: m >
m*. The single-neuron target func-
tion is a special case with m* = 1.
Figure 3 shows the dynamic behav-
ior for m = 50, m* = 40. In this
case m is the same order of m*, but
we still see the GD dynamics tends to
find solutions with the number of ac-
tive neurons close to m*. The learn-
ing process is qualitatively similar to
the case of circle neuron target func-
tion except that the activation process
proceeds in a more continuous fash-
ion and therefore the step-like behav-
0.10
0.05
旦 0.00
-0.05
-0.10
Index of neuron
0.10
0.05
旦 0.00
-0.05
-0.10
Figure 3: The dynamic behavior of learning a finite-neuron target
function. Here, a* = 1/m, {bj } are uniformly drawn from SdT
with d = 100 and the learning rate is 0.001. Left: The magnitude
of each neuron of the convergent solution. Right: The dynamics
of the magnitude of each neuron.
ior is less pronounced. This trend becomes clearer if m is much larger than m* . See Appendix B
for more experiments.
3.1.3	S urface neuron target function
100
10-1
10-2
10-3
0.02
0.01
---NN
——RF
000
2.0
×107
1.5
Number of iterations
Figure 4: The dynamic behavior of learning the surface neuron
target function. Here m = 100, d = 100 and learning rate η =
0.005. Left: The dynamics of the population risk; Right: The
dynamics of the magnitude for each neuron. The inset is the zoom-
in of the first 500, 000 iterations.
0.5
1.0
1.5
Number of iterations
2.0
×107
The target functions studied above
are all functions that can be accu-
rately approximated by a small set of
neurons. Next we consider an exam-
ple in the opposite direction: the “sur-
face neuron target function”
启(X) = Eb^∏3 [σ(bτx)]
where π3 is the uniform distribu-
tion over Ω = {b : Pd=I b22 =
1 and bi = 0, ∀i > d/2}. This func-
tion is represented by a very large set
of neurons, each of which contributes
an equal small amount. Shown in
Figure 4 are the numerical results.
One can see that in this case all the neurons are quenched and there are no activated neurons. The
activation process is replaced by smooth changes of all the neurons.
3.2	Understanding the second phases by using effective dynamics
In the second phase, the neurons can be divided into two groups (the first group might be empty):
I1 and I2 , where I1 , I2 are the set of indices of active and quenched neurons, respectively. We have
aj = O(1) forj ∈ I1 and aj = o(1) forj ∈ I2. Denote by fIi (x； t) = j∈Ii aj(t)σ(bjT(t)x) with
i = 1, 2 the functions represented by the active and quenched neurons, respectively.
The dynamics of neurons for the two groups are very different. For j ∈ I1 , both aj and bj changes
significantly. However, for j ∈ I2, aj(t) evolves much faster than bj (t), since aj 〜 O(kbjk) =
*
O(1), bj 〜 O(|aj|)《 1. As a result {aj}j∈i2 is effectively slaved to the optimal solutions with *
{bj }j ∈I2 held fixed, which is given by
argmin{aj}j∈I2 Ex[(X aj σ(bjT (t)x) +fI1(x；t) — f*(x))2].
j∈I2
In this way, one obtains an effective dynamics governing the second phase: For active neurons, i.e.
j ∈ I1,
aj(t) = -Eχ[(f (x; t) - f*(x))σ(bT(t)x)]
bj(t) = -Eχ[(f (x； t) — f *(x))aj(t)σ0(bT(t)x)x];
for quenched neurons, i.e. j ∈ I2 ,
aj(t) = aj* (t)
bj(t) = -Eχ[(f (x； t) — f *(x))aj(t)σ0(bT(t)x)x],
(6)
(7)
(8)
5
Under review as a conference paper at ICLR 2021
where {a*(t)}j∈i2 is the solution of (6), and f (x; t) = Pj aj(t)σ(bT(t)x) is the functions repre-
sented by the solutions of the effective dynamics at the time t.
We numerically test how accurate the effective dynamics is for two target functions: single neuron
and surface neuron. For surface neuron, I1 is empty. Shown in Figure 5 are the results. We see that
indeed the effective dynamics is able to capture the original dynamics very well, even at relatively
long-time scales (see Figure 13 in the appendix).
t=100
0	10	20	30	40	50
0.04
0.02
0.00
0.04
0.02
0.03
0.02
0.01
t=20
t=4000
a(ff)(t)	ʌʌʌʌ/ʌ
t=499990
0	10	20	30	40	50
Figure 5:	Comparison between a(t) and aeff (t) at different times, where a(t), aeff (t) are the solutions of the
original and effective dynamics, respectively. Left: The single neuron target function fɪ. Right: The surface
neuron target function f4.
4 GD dynamics for the case of finite training samples
We now turn to the more realistic situation where the size of the training set is finite.
4.1	The dynamic behavior
We refer the reader to Appendix D for some numerical results of the highly over-parameterized
regime as well as the under-parameterized regime, i.e. m < n/(d + 1). We find that in the under-
parameterized regime, the dynamic behavior of GD is qualitatively similar to the case n = ∞.
We call the regime between the highly over-parameterized and under-parameterized regimes
the mildly over-parameterized regime. The key question of interest is how the mildly over-
parameterized regime bridges the highly over-parameterized regime and the under-parameterized
regime. We have already seen that these two regimes differ in several aspects. One is that in
the highly over-parameterized regime, the inner layer coefficients b barely change. In the under-
parameterized regime, a small number of neurons experience large changes for their inner-layer
coefficients.
First, we investigate the GD dynamics for two interesting scalings: m = cn/(d + 1) and m = Cn
where c and C are constants. Shown in Figures 6 are two examples for m = 3n/(d + 1) and
m = 0.75n with n = 200, d = 19 respectively. More examples with the same scalings but different
values of n can be found in Appendix D.
5000	10000	1 5000	20000	2 5000
Number of iterations
0.25
0.20
0.15
0.10
0.05
0.00
-0.05
-0.10
-0.15
0	10000	20000	30000	40000
Number of iterations
(a) m = 3n/(d + 1)	(b) m = 0.75n
Figure 6:	The dynamic behavior of learning single-neuron target function for two mildly over-parameterized
cases. Here n = 200, d = 19 and learning rate is 0.001. For each case, Left: The dynamics of the training and
test losses (results from the corresponding random feature model is also plotted for comparison); Right: The
dynamics of the magnitude of each neuron.
One can see that the behavior shown in Figures 6a resembles the ones shown for infinite samples,
whereas the behavior shown in Figures 6b does not. In the first case, the test accuracy improves
substantially after the GD dynamics departs from that of the RFM, and there is a notable presence
of the activation phenomena. In the second case, the test error saturates soon after the GD dynamics
6
Under review as a conference paper at ICLR 2021
departs from that of the RFM, and there are no clear presence of the activation phenomena. We will
call the first case “NN-like” and the second case “RF-like”.
4.2	Generalization error and the path norm
Test errors
2.6
2.4
,S 2.2
g>
2.0
1.8
∣ogι0(m)
Figure 7: Heatmap of test errors of GD so-
lutions with varying m’s and n’s. The target
function is the single-neuron target function
f1 with d = 40. GD is stopped when the
training loss is smaller than 10-8. The two
dashed lines corresponds to m = n/(d + 1)
and m = n, respectively.
Next we examine the generalization error of the GD so-
lution. Shown in Figure 7 are the test errors of GD so-
lutions with varying m’s and n’s. The target function
is the single neuron target function fl. We see that the
test errors exhibit a sharp transition somewhere between
m = n/(d + 1) to m = n. This is consistent with the
observation in Figure 6 that when m 〜 n/(d + 1), GD
shows a NN-like behavior while m 〜n, GD shows a RF-
like behavior. Moreover, the transition seems to become
sharper with increasing values of n and m.
In the following, we take a closer look at how the in-
crease of m affects the test performance by consider-
ing the circle neuron target function. The results for
the single-neuron target function are similar, and can
be found in Appendix E. For 2LNNs, the generalization
gap can be controlled by the path norm of the parame-
ters (Neyshabur et al., 2015; E et al., 2019b), defined by
kθkP = Pjm=1 |aj |kbj k2 . Therefore we will compute the
path norm of the parameters selected by GD.
Figure 8a examines the test error and the path norm as m
changes. One can see that as m becomes larger, the test
error of the NN model eventually becomes close to that of the RFM. One thing to notice is that
these changes seem to behave smoothly across the points: m = n/(d + 1) and m = n, where the
NN and the RFM change from an under-parameterized situation to an over-parameterized situation,
respectively. We do not observe the peak of test errors around m = n/(d + 1) as suggested in
(Belkin et al., 2019). We suspect that the peak observed in (Belkin et al., 2019) is the result of the
special training method used there. Interestingly, Figure 8 and Figure 7 show that there does exist a
peak around m = n, the same place for the RFM. This should be the result of the close proximity
of the GD dynamics for 2LNN and RFM during the first phase. The latter performs extremely badly
when m = n due to resonance as shown in (Ma et al., 2020). Thus it is not surprising to see that NN
also performs the worst around m = n.
In addition, the path norm seems to serve as a good indicator of the generalization performance. For
example, one can see a dramatic increase of the path norm from m = n/(d + 1) to m = n, and the
path norm peaks around m = n.
m=2n∕(d+1)
o.o
-0.5
(a)
1	2	3
Iogio(Width)
Figure 8: Test performance of GD solutions for the circle neuron f2. (a) The path norm and test error as a
function of m. Here n = 200, d = 20 and the learning rate is 0.001. GD is stopped after the training error is
smaller than 10-6. (b) The test error as a function of n for three different input dimensions.
Iogio(Width)
2.0	2.5	3.0	2.0
logɪo(n)
(b)
m=1.5π
-1.0
(」R」a I&S6O 一
2.5	3.0
logɪo(n)
To further explore the difference between the two scalings m 〜n/(d + 1) and m 〜n, we show in
Figure 8b the test error as a function of the size of training set. The result suggests that under the
scaling m 〜n, the test error may suffer from curse of dimensionality (CoD), while for the scaling
7
Under review as a conference paper at ICLR 2021
m ~ n/(d + 1), it does not seem to be the case. Let the test error decrease as O(1∕nɑ(d^). CoD
means that α(d) decreases (to 0) as d → ∞. Thus, it is difficult to learn the target function when d
is large if the model suffers from CoD.
5 GD dynamics with mean-field scaling
For simplicity, we call the GD dynamics of the scaled 2LNN (2) GD-MF and the GD dynamics of
the unscaled 2LNN (1) GD-conventional. Under the mean-field scaling, aj and bj are of the same
*
order. Thus aj and b7- are also of the same order, which means that there is no time-scale separation.
First let us look at the case when n
∞. Figure 9 shows the dynamic behavior
of GD-MF for single neuron target func-
tion fɪ. Different from GD-conventional
shown in Figure 1, we see that almost all
the neurons move significantly and con-
tribute to the model.
Figure 10 shows the test error for the case
with finite sample. We see that the test er-
ror of GD-MF varies more smoothly with
the increase of network width. There is al-
most no clear deterioration of the perfor-
mance even when we increase the network
width to the highly over-parameterized
regime. This is clearly different from the
case for GD-conventional. Moreover, we
observe that the GD-MF solutions do not
seem to suffer from CoD at the regime m
Figure 8b).
Figure 9: GD-MF dynamics for learning the single neuron
target function fɪ. Here m = 50,d = 100,n = ∞ and
learning rate is 0.001. Left: The magnitude of each neu-
ron for the converged solution. Right: The projection to the
first two coordinates of b for each neuron. The green ones
correspond to the random initialization; the orange ones cor-
respond to the solutions found by GD-MF.
~ n, which is not the case for GD-conventional (see
Test errors
(s0I6O-
1.8
2.0	2.5	3.0	3.5	4.0	4.5
logɪo(m)
(a) Single neuron f.
-1.0
MF
Conventional
θtθ ls2≡MOI
2.5	3.0
logιo(n)
1	2	3	2.0
logιo(m)
(b) Circle neuron f2.
Figure 10:	(a) The heatmap of test errors of GD-MF solutions for learning the single neuron fɪ. (b) The test
performance of learning the circle neuron f2. The left shows the test error as a function of m. The right shows
the test error as a function of n for the scaling m = 1.5n.
6 Conclusion
The experimental results shown in this paper suggest the following: (1) Under the conventional scal-
ing, both the training process and the generalization performance are quite sensitive to the network
parameters. With the mean-field scaling, both are more stable, though not always better. (2) In the
NN-like regime under conventional scaling, the quenching process provides the mechanism of im-
plicit regularization, since it does not allow the the path norm to grow out of control. Consequently
the generalization gap is controlled.
Obviously, many questions remain open. For example, how do the results discussed in this paper
manifest for practical datasets and deep neural networks? Can we substantiate some of the findings
by rigorous results? These are all questions that are left for future work.
8
Under review as a conference paper at ICLR 2021
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8139-8148, 2019.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of Ma-
chine Learning Research, 18(19):1-53, 2017.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
Learning, 14(1):115-133, 1994.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learning practice and the classical bias-variance trade-off. Proceedings of the National Academy
of Sciences, 116(32):15849-15854, 2019.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems, pp. 3036-3046, 2018.
Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural
information processing systems, pp. 342-350, 2009.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019a.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019b.
Weinan E, Chao Ma, and Qingcan Wang. A priori estimates of the population risk for residual
networks. arXiv preprint arXiv:1903.02154, 2019a.
Weinan E, Chao Ma, and Lei Wu. A priori estimates of the population risk for two-layer neural
networks. Communications in Mathematical Sciences, 17(5):1407-1425, 2019b.
Weinan E, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for neural
network models. arXiv preprint arXiv:1906.08039, 2019c.
Weinan E, Chao Ma, and Lei Wu. A comparative analysis of optimization and generalization prop-
erties of two-layer neural network and random feature models under gradient descent dynamics.
Science China Mathematics, pp. 1-24, 2020.
Sebastian Goldt, MadhU Advani, Andrew M Saxe, Florent Krzakala, and Lenka Zdeborova. Dy-
namics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
In Advances in Neural Information Processing Systems, pp. 6981-6991, 2019.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Delving deep into rectifiers: SUrpassing
hUman-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neUral networks. In Advances in neural information processing systems, pp. 8580-
8589, 2018.
Yann A LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In
Neural networks: Tricks of the trade, pp. 9-48. Springer, 2012.
Chao Ma, Lei Wu, and Weinan E. The slow deterioration of the generalization error of the random
feature mode. In Mathematical and Scientific Machine Learning Conference, 2020.
9
Under review as a conference paper at ICLR 2021
Song Mei, A Montanari, and P Nguyen. A mean field view of the landscape of two-layers neural
networks. In Proceedings of the National Academy of Sciences, volume 1l5, pp. E7665-E7671,
2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence
and asymptotic error scaling of neural networks. In Advances in neural information processing
systems, pp. 7146-7155, 2018.
David Saad and Sara A Solla. On-line learning in soft committee machines. Physical Review E, 52
(4):4225, 1995.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural net-
works. In International Conference on Machine Learning, pp. 4433-4441, 2018.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central
limit theorem. Stochastic Processes and their Applications, 130(3):1820-1852, 2020.
Yuandong Tian. An analytical formula of population gradient for two-layered ReLU network and its
applications in convergence and critical point analysis. In International Conference on Machine
Learning, pp. 3404-3413, 2017.
Lei Wu, Zhanxing Zhu, and Weinan E. Towards understanding generalization of deep learning:
Perspective of loss landscapes. arXiv preprint arXiv:1706.10239, 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep-
resentations, 2017.
10
Under review as a conference paper at ICLR 2021
A	The influence of the initial magnitude of the outer
COEFFICIENTS
First We consider the case β = 1/√m. Figure 11 shows the numerical results with the same setting
as in Section 3.1. We see that the dynamical behavior is qualitatively the same as the case of β = 0.
For β = 1∕mγ with Y ≥ 1/2, the time-scale separation between the inner and outer layers always
hold initially as long asm is large enough. In these cases we see basically the same kind of behavior
as shown in the main text. To simplify the experiments and the presentation, we set β = 0, i.e.
γ = ∞, in which case we do not need to take very large values ofm.
Number of iterations	Index of neurons
(a)	(b)
0	1000	2000	3000	4000	5000	6000	7000
Number of iterations
0	1000	2000	3000	4000	5000	6000	7000
Number of iterations
.	(C)	(d)
Figure 11:	The dynamic behavior of GD with β = 1/√m. Here the target function is the single neuron fɪ.
m = 200, d = 100 and learning rate is 0.001. We also plot the results of RFM as comparison. (a) The dynamic
behavior of the population risk; (b) The magnitude of each neuron for the converged solution. (c) The dynamics
，*
of the a coefficient of each neuron. (d) The dynamics of {bj [0]}, the projection of bj to b = eɪ.
B Additional results for the finite neuron target function
Figure 12	shows additional results for learning finite neurons when m is much larger than m*. We
see that the behavior is basically the same as Figure 3 except that more neurons concentrate around
0.
Figure 12: Learning finite neurons with a* = 1/m, bj 〜∏o. Here m = 100, m* = 40 and learning rate is
0.001. Left: The magnitude of each neuron of the final solution; Right: The dynamics of the magnitude for
each neuron.
11
Under review as a conference paper at ICLR 2021
C Additional results for the effective dynamics
In Figure 13, we plot the difference between the effective dynamics and original dynamics as a
function of number of iterations. We see that the two dynamics are pretty close in the second phase
even in a long time scale.
m = 50
—∣∣a(t)- aeff(t)M∣a(t)∣∣-
∣∣B(t)- Beff(t)∣M∣∣B(t)∣∣F
0	200000	400000
m = 50
10-11一 IB(t)-aeff(tWHa(t)H
Figure 13:	The difference between the effective dynamics and the original dynamics. Left: The single neuron
target function f With m = 50; Right: The surface neuron target function f4 With m = 50.
D Additional experimental results for GD dynamics with finite
SAMPLES
D. 1 The under-parameterized regime
We first look at the regime Where the netWork is under-parameterized, i.e. m < n/(d+ 1). A typical
result for the single neuron target function is shoWn in Figure 14. We see that overall the qualitative
behavior of GD is similar to that of the case When n = ∞ studied in Section 3. There are still tWo
phases, and the convergent solution is sparse. Figure 14b shoWs that the quenching process becomes
sloWer compared to the case With infinite data.
(b)
m=9, n=200 d=20
-0.2
0	1	2	3	4	5	6	7	8
Index of neurons
(C)
Figure 14:	GD dynamics for learning the single neuron target function in the under-parameterized regime.
Here m = 9, d = 20, n = 200 and the learning rate η = 0.005. (a) Time history of training and test errors. (b)
Time history of the magnitude for each neuron. (c) The magnitude of each neuron of the convergent solution.
Figure 15	shoWs the result for the circle neuron target function. We see again that there are still
tWo phases. But the multi-step phenomenon becomes less pronounced compared to the infinite
data case. From Figure 15b, the neurons can still roughly be divided into tWo groups: active and
quenched neurons. Also We still observe some activation during the second phase, although this
process is much more smooth compared to the case With infinite data.
D.2 The mildly over-parameterized regime
Here We provide more experimental results under the two scalings m 〜n/(d + 1) and m 〜n. The
results are shoWn in Figure 16 and 17. We see that the dynamic behavior is similar to the ones in
Figure 6.
D.3 The highly over-parameterized regime
We first look at the simple situation Where the netWork is highly over-parameterized. Figure 18
shoWs some typical results in this regime. Clearly in this regime, the GD dynamics for the NN
model stay uniformly close to that of the RFM for all time, as Was proved in E et al. (2020). We
12
Under review as a conference paper at ICLR 2021
(a)
Figure 15: GD dynamics for the circle neuron target function in the under-parameterized case. Here m
50, n = 500, d = 5. (a) The dynamics of training and test loss. (b) The dynamics of the outer coefficients.
1.0
0.8
0.6
0.4
0.2
0.0
-0.2
m=60, n=400 d=20
Index of neurons
(a)	(b)	(c)
Figure 16: The dynamic behavior of the GD solutions for m = 3n/(d + 1). Here m = 60, n = 400, d = 19
and learning rate η = 0.001.
100
10-1
10-2
S —.
10-3
O
—I
10-4
10-5
10-6
0	20000 40000 60000 80000 IOOOOO 120000 140000 160000
Number of iterations
m=300, n=400 d=20
. nn
0.1
-0.1
(b)
(c)
200
Index of neurons
20000 40000 60000 80000 IOOOOO 120000 140000 160000
Number of iterations
Figure 17: The dynamic behavior of the GD solutions for m = 0.75n. Here m = 300, n = 400, d = 19 and
learning rate η = 0.001.
0.0
(a)
see that the training error goes to 0 exponentially fast, but the testing error quickly saturates. Note
that Theorem 1 suggests that the network width should satisfy m & n2λn-4 ln(n2δ-1), but in this
experiment m = 0.5n2 is enough.
Figure 18: Comparison between the GD solutions for 2LNN and RFM for the single neuron target function
fι. The learning rate 0.001 and m = 2000,n = 200, d = 20. Left: The time history of the training and test
error. Right: The magnitude of the converged solutions.
13
Under review as a conference paper at ICLR 2021
E	Additional results on the test performance
Figure 19 shows how the test error is affected by the number of samples n and the number of neurons
m for the single neuron target function.
uot3 IS9)0160-
1	2	3
Iogio(Width)
⑶
1	2	3
logɪo(width)
m=2n∕(d+1)
103
Numberofsamples
d=10
d=40
d=100
-ouɑ)jsəl
ιo^1
m=1.5n
103
Numberofsamples
(b)
Figure 19: Test performance of GD solutions for single neuron fɪ. (a) The path norm and test error as a
function of m. Here n = 200, d = 20 and the learning rate is 0.001. GD is stopped after the training error is
smaller than 10-8. (b) The test errors as a function ofn for three different input dimensions.
14