Under review as a conference paper at ICLR 2021
Gradient penalty from a maximum margin per-
SPECTIVE
Anonymous authors
Paper under double-blind review
Ab stract
A popular heuristic for improved performance in Generative adversarial networks
(GANs) is to use some form of gradient penalty on the discriminator. This gradient
penalty was originally motivated by a Wasserstein distance formulation. However,
the use of gradient penalty in other GAN formulations is not well motivated. We
present a unifying framework of expected margin maximization and show that a
wide range of gradient-penalized GANs (e.g., Wasserstein, Standard, Least-Squares,
and Hinge GANs) can be derived from this framework. Our results imply that
employing gradient penalties induces a large-margin classifier (thus, a large-margin
discriminator in GANs). We describe how expected margin maximization helps
reduce vanishing gradients at fake (generated) samples, a known problem in GANs.
From this framework, we derive a new L∞ gradient norm penalty with Hinge loss
which generally produces equally good (or better) generated output in GANs than
L2-norm penalties (based on the Frechet Inception Distance).
1	Introduction
Generative adversarial networks (GANs) (Goodfellow et al., 2014) are a very successful class of
generative models. Their most common formulation involves a game played between two competing
neural networks, the discriminator D and the generator G. D is a classifier trained to distinguish
real from fake examples, while G is trained to generate fake examples that will confuse D into
recognizing them as real. When the discriminator’s objective is maximized, it yields the value of a
specific divergence (i.e., a distance between probability distributions) between the distributions of real
and fake examples. The generator then aims to minimize that divergence (although this interpretation
is not perfect; see Jolicoeur-Martineau (2018a)).
Importantly, many GANs apply some form of gradient norm penalty to the discriminator (Gulrajani
et al., 2017; Fedus et al., 2017b; Mescheder et al., 2018; Karras et al., 2019). Gradient norm penalty
has been widely adopted by the GAN community as a useful heuristic to improve the stability of GANs
and the quality of the generated outputs. This penalty was originally motivated by a Wasserstein
distance formulation in Gulrajani et al. (2017). However, its use in other GAN formulations is
not well motivated. Given its success, one might wonder how one could derive an arbitrary GAN
formulation with a gradient penalty?
In this paper, we derive a framework which shows that gradient penalty arises in GANs from using a
maximum margin classifier as discriminator. We then use this framework to better understand GANs
and devise better gradient penalties.
The main contributions of this paper are:
1.	A unifying framework of expected margin maximization and showing that gradient-penalized
versions of most discriminator/classifier loss functions (Wasserstein, Cross-entropy, Least-
Squares, Hinge-Loss) can be derived from this framework.
2.	A new method derived from our framework, a L∞ gradient norm penalty with Hinge
function. We hypothesize and show that this method works well in GAN.
3.	We describe how margin maximization (and thereby gradient penalties) helps reduce vanish-
ing gradients at fake (generated) samples, a known problem in many GANs.
1
Under review as a conference paper at ICLR 2021
4.	We derive the margins of Relativistic paired and average GANs (Jolicoeur-Martineau, 2018b;
2019).
The paper is organized as follows. In Section 2, we show how gradient penalty arises from the
Wasserstein distance in the GAN literature. In Section 3, we explain the concept behind maximum-
margin classifiers (MMCs) and how they lead to some form of gradient penalty. In Section 4, we
present our generalized framework of maximum-margin classification and experimentally validate
it. In Section 5, we discuss of the implications of this framework on GANs and hypothesize that
L1 -norm margins may lead to more robust classifiers. Finally, in Section 6, we provide experiments
to test the different GANs resulting from our framework. Note that due to space constraints, we
relegated the derivations of the margins of Relativistic GANs to Appendix C.
2	Gradient Penalty from the GAN Literature
2.1	Notation
We focus on binary classifiers. Let f be the classifier and (x, y)〜D the distribution (of a dataset
D) with n data samples x and labels y. As per SVM literature, y = 1 when x is sampled from
class 1 and y = -1 when X is sampled from class 2. Furthermore, We denote xi = x|(y = 1)〜P
and X2 = x|(y = -1) 〜Q as the data samples from class 1 and class 2 respectively (with
distributions P and Q). When discussing GANs, xi 〜P (class 1) refer to real data samples and
X2 〜Q (class 2) refer to fake data samples (produced by the generator). The L∞-norm is defined as:
||x || ∞ = max(|xi|, |x2|, . . . , |xk|).
The critic (C) is the discriminator (D) before applying any activation function (i.e., D(x) = a(C(x)),
where a is the activation function). For consistency with existing literature, we will generally refer to
the critic rather than the discriminator.
2.2	GANS
GANs can be formulated in the following way:
max Exi〜P [fι(C(xι))]+ Ez〜Z [f2(C(G(z)))],	(1)
C：XtR
min Ez〜Z [f3(C(G(z)))],	⑵
G ： Z →X
where fi, f2 , f3 : R → R, P is the distribution of real data with support X, Z is a multivariate
normal distribution with support Z = R, C(x) is the critic evaluated at x, G(z) is the generator
evaluated at z, and G(Z)〜Q, where Q is the distribution of fake data.
Many variants exist; to name a few: Standard GAN (SGAN) (Goodfellow et al., 2014) corresponds
to fi(z) = log(sigmoid(z)), f2(z) = log(sigmoid(-z)), and f3(z) = -fi(z). Least-Squares
GAN (LSGAN) (Mao et al., 2017) corresponds to fi(z) = -(1 - z)2, f2(z) = -(1 + z)2,
and f3(z) = -fi(z). HingeGAN (Lim & Ye, 2017) corresponds to fi (z) = -max(0, 1 - z),
f2(z) = -max(0, 1 + z), and f3(z) = -z.
2.3	Integral Probability Metric Based GANs
An important class of statistical divergences (distances between probability distributions) are Integral
probability metrics (IPMS)(Muller, 1997). IPMs are defined in the following way:
IPMF(P||Q) = sup Ex]〜p[C(χι)] - Ex2〜Q[C(χ2)],
C∈F
where F is a class of real-valued functions.
A widely used IPM is the Wasserstein’s distance (Wi), which focuses on the class of all 1-Lipschitz
functions. This corresponds to the set of functions C such that "xjX-C(x2)≤ 1 for all X1,X2, where
d(xi, x2) is a metric. Wi also has a primal form which can be written in the following way:
Wi (P, Q)
inf
π∈Π(P,Q)
/
M×M
d(xi, x2) dπ(xi, x2),
2
Under review as a conference paper at ICLR 2021
where Π(P, Q) is the set of all distributions with marginals P and Q and we call π a coupling.
The Wasserstein distance has been highly popular in GANs due to the fact that it provides good
gradient for the generator in GANs which allows more stable training.
IPM-based GANs (Arjovsky et al., 2017; Gulrajani et al., 2017) attempt to solve the following
problem
min max Eχ?〜p[C(xι)] - Ez〜z[C(G(z))].
G C∈F
2.4	Gradient Penalty as a way to estimate the Wasserstein Distance
To estimate the Wasserstein distance using its dual form (as a IPM), one need to enforce the 1-
Lipschitz property on the critic. Gulrajani et al. (2017) showed that one could impose a gradient
penalty, rather than clamping the weights as originally done (Arjovsky et al., 2017), and that this
led to better GANs. More specifically, they showed that if the optimal critic f * (x) is differentiable
everywhere and that X = αxι + (1 - α)x2 for 0 ≤ α ≤ 1, we have that ||VC*(X)∣∣2 = 1 almost
everywhere for all pair (x1, x2) which comes from the optimal coupling π*.
Sampling from the optimal coupling is difficult so they suggested to softly penalize
E式∣∣VχC(X)∣∣2 — 1)2, where X = αxι + (1 — α)x2, α 〜U(0,1), xι 〜P, and x2 〜Q. They
called this approach Wasserstein GAN with gradient-penalty (WGAN-GP). However, note that this
approach does not necessarily estimate the Wasserstein distance since we are not sampling from π*
and f* does not need to be differentiable everywhere (Petzka et al., 2017).
Of importance, gradient norm penalties of the form Ex(||VxD(X)||2 - δ)2, for some δ ∈ R are very
popular in GANs. Remember that D(X) = a(C(X)); in the case of IPM-based-GANs, we have that
D(X) = C(X). It has been shown that the GP-1 penalty (δ = 1), as in WGAN-GP, also improves
the performance of non-IPM-based GANs (Fedus et al., 2017a). Another successful variant is GP-0
(δ = 0 and x 〜P) (Mescheder et al., 2018; Karras et al., 2019). Although there are explanations to
why gradient penalties may be helpful (Mescheder et al., 2018; Kodali et al., 2017; Gulrajani et al.,
2017), the theory is still lacking.
3	Maximum-Margin Classifiers
In this section, we define the concepts behind maximum-margin classifiers (MMCs) and show how it
leads to a gradient penalty.
3.1	Decision B oundary and Margin
The decision boundary of a classifier is defined as the set of points x0 such that f(x0) = 0.
The margin is either defined as i) the minimum distance between a sample and the boundary, or ii)
the minimum distance between the closest sample to the boundary and the boundary. The former thus
corresponds to the margin of a sample and the latter corresponds to the margin of a dataset . In order
to disambiguate the two cases, we refer to the former as the margin and the latter as the minimum
margin.
3.2	Geometric Margin and Gradient Penalty
The first step towards obtaining a MMC is to define the Lp -norm margin:
γ(x) = min ||x0 - x||p s.t. f(x0) = 0	(3)
x0
With a linear classifier (i.e., f(x) = wT x), there is a close form solution. However, the formulation
of the Lp -norm margin equation 3 has no closed form for arbitrary non-linear classifiers. A way to
derive an approximation of the margin is to use Taylor’s approximation before solving the problem
3
Under review as a conference paper at ICLR 2021
(as done by Matyasko & Chau (2017) and Elsayed et al. (2018)):
γp (x) = min ||r||p s.t. f (x + r) = 0
r
≈ min ||r||p s.t. f (x) + Nxf(X)Tr = 0
r
_	lf(χ)l
=||Vxf(X)l∣q,
where ∣∣∙∣∣q isthe dual norm (Boyd & Vandenberghe, 2004) of ∣∣∙∣∣p. By Holder,s inequality (Holder,
1889; Rogers, 1888), we have that 1/p + 1/q = 1. This means that ifp = 2, we still get q = 2; if
p = ∞, we get q = 1; ifp = 1, we get q = ∞.
The goal of MMCs is to maximize a margin, but also to obtain a classifier. To do so, we simply
replace α(X) = |f (X)| by αe(X, y) = yf(X). We call αe the functional margin. After replacement, we
obtain the geometric margin:
γe(X, y)
yf(χ)
IlVxf(X)||q
Ifp = 2 and f(X) is linear, this leads to the same geometric margin used in Support Vector-Machines
(SVMs). Matyasko & Chau (2017) used this result to generalize Soft-SVMs to arbitrary classifiers by
simply penalizing the Lp-norm of the gradient rather than penalizing the Lp -norm of the model’s
weights (as done in SVMs). Meanwhile, Elsayed et al. (2018) used this result in a multi-class setting
and maximized the geometric margin directly.
4	Generalized framework of Maximum-margin clas sification
4.1	Framework
Here we show how to generalize the idea behind maximizing the geometric margin into arbitrary
loss functions with a gradient penalty. Directly maximizing the geometric margin is an ill-posed
problem. The numerator and denominator are dependent on one another; increasing the functional
margin also increases the norm of the gradient (and vice-versa). Thereby, there are infinite solutions
which maximize the geometric margin. For this reason, the common approach (as in SVM literature;
see Cortes & Vapnik (1995)) is to: i) constrain the numerator and minimize the denominator, or ii)
constrain the denominator and maximize the numerator.
Approach i) consists of minimizing the denominator and constraining the numerator using the
following formulation:
min||Vxf(X)||p s.t. yf(X) ≥ 1∀(X,y) ∈ D	(4)
The main limitation of this approach is that it only works when the data are separable. However, if
we take the opposite approach of maximizing a function of yf (X) and constraining the denominator
||f (X)||2, we can still solve the problem with non-separable data. This corresponds to approach ii):
maxE(x,y)〜D [yf(x)] s.t. ∣∣Vxf(x)∣∣q ≤ 1 or ∣∣Vxf(X)||q = 1.	(5)
The constraint chosen can be enforced by either i) using a KKT multiplier (Kuhn & Tucker, 1951;
Karush, 1939) or ii) approximately imposing it with a soft-penalty. Furthermore, one can use any
margin-based loss function rather than directly maximize yf (X). Thus, we can generalize this idea
by using the following formulation:
minE(x,y)〜D [L(yf(x)) + λg(∣∣Vxf(χ)∣∣q)].	(6)
where L, g : R → R and λ is a scalar penalty term. There are many potential choices of L and g
which we can use.
If L is chosen to be the hinge function (i.e., L(z) = maX(0, 1 - z)), we ignore samples far from
the boundary (as in Hard-Margin SVMs). For general choices of L, every sample may influence
4
Under review as a conference paper at ICLR 2021
the solution. The identity function L(z) = z, cross entropy with sigmoid activation L(z) =
- log(sigmoid(z))) and least-squares L(z) = (1 - z)2 are also valid choices.
A standard choice of g is g(z) = (z2 - 1). This corresponds to constraining ∣∣Vχf (χ)∣∣2 = 1
or ∣∣Vχf(χ)∣∣2 ≤ 1 for all X (by KKT conditions). As an alternative, We can also consider soft
constraints of the form g(z) = (z - 1)2 or g(z) = max(0, z - 1). The first function enforces a soft
equality constraint so that z ≈ 1 While the second function enforces a soft inequality constraint so
that z ≤ 1. Soft constraints are useful if the goal is not to obtain the maximum margin solution but to
obtain a solution that leads to a large-enough margin.
Of importance, MMCs can be seen as a generalization of Support Vector Machines (SVMs). When
p = 2 and f is linear (f (x) = wT x), equation 4 corresponds exactly to Hard-Margin SVMs and
equation 6 With L(z) = max(0, 1 - z) and g(z) = (z2 - 1) corresponds exactly to Soft-Margin
SVMs (Cortes & Vapnik, 1995).
4.2	Experimental evidence of large margin from gradient penalties
We ran experiments to empirically shoW that gradient-penalized classifiers (trained to optimize
equation 6) maximize the expected margin. We used the sWiss-roll dataset (Marsland, 2015) to obtain
tWo classes (one is the sWiss-roll and one is the sWiss-roll scaled by 1.5). The results are shoWn in
Table 1 (Details of the experiments are in Appendix A).
Table 1: Expected Lp Margin for different types of gradient penalties (or none). Classifier Was trained
on the sWiss-roll dataset With a cross-entropy loss function.
Type of gradient penalty	Expected Lp Margin		
	p=2	p=1	p=∞
No gradient penalty	.27	.25	.24
g(Z) = (Z - 1)2			
L2 gradient penalty (L2 margin)	.62	.75	.64
L∞ gradient penalty (L1 margin)	.43	.58	.33
L1 gradient penalty (L∞ margin)	.69	.85	.60
g(Z) = max(0, Z - 1)			
L2 gradient penalty (L2 margin)	.43	.53	.37
L∞ gradient penalty (L1 margin)	.41	.56	.31
L1 gradient penalty (L∞ margin)	.43	.44	.42
We observe that We obtain much larger expected margins (generally 2 to 3 times bigger) When using
a gradient penalty; this is true for all types of gradient penalties.
5	Implications of the Maximum Margin Framework on GANs
5.1	GANs can be derived from the MMC Framework
Although not immediately clear given the different notations, let f(x) = C(x) and We have:
E(x,y)~D [L(yf (X))] = Exi~P[L(C(XI))]+ Ez~Z[L(-C(G(Z)))].
Thus, the objective functions of the discriminator/critic in many penalized GANs are equivalent to
the ones from MMCs based on equation 6. We also have that L(z) = log(sigmoid(z)) corresponds
to SGAN, L(z) = (1 - z)2 corresponds to LSGAN, and L(z) = maX(0, 1 - z) corresponds to
HingeGAN. When g(z) = (z - 1)2, We also have that L(z) = z corresponds to WGAN-GP. Thus,
most Lp -norm gradient penalized GANs imply that the discriminator approximately maximize an
expected Lq -norm margin.
5
Under review as a conference paper at ICLR 2021
5.2	Why do Maximum-Margin Classifiers make good GAN
Discriminators/Critics ?
To show that maximizing an expected margin leads to better GANs, we prove the following statements:
1.	classifier maximizes an expected margin q⇒ classifier has a fixed LiPschitz constant
2.	MMC with a fixed Lipschitz constant =⇒ better gradients at fake samples
3.	better gradients at fake samples =⇒ stable GAN training.
5.2.1	Equivalence between gradient norm constraints and Lipschitz functions
As stated in Section 2.4, the WGAN-GP approach of softly enforcing ∣∣Vχf(X)∣∣2 ≈ 1 at all
interpolations between real and fake samples does not ensure that we estimate the Wasserstein
distance (W1). On the other hand, we show here that enforcing ||Vxf(x)||2 ≤ 1 is sufficient in order
to estimate W1.
Assuming d(x1, x2) is a Lp-norm, p ≥ 2 and f(x) is differentiable, we have that:
∣Vf(x)llp ≤ K ^⇒ f is K-Lipschitz on Lp.
See appendix for the proof. Adler & Lunz (2018) showed a similar result on dual norms.
This suggests that, in order to work on the set of Lipschitz functions, we should enforce that
||Vx f (x)|| ≤ 1 for all x. This can be done, through equation 6, by choosing g(z) = (z2 - 1) or,
in approximation (using a soft-constraint), by choosing g(z) = max(0, z - 1). Petzka et al. (2017)
suggested using a similar function (the square hinge) in order to only penalize gradient norms above
1.
If we let L(z) = z and g(z) = max(0, z - 1), we have an IPM over all Lipschitz functions. thus,
we effectively approximate W1 . This means that W1 can be found through maximizing a geometric
margin. Meanwhile, WGAN-GP only leads to a lower bound on W1 .
Importantly, most successful GANs (Brock et al., 2018; Karras et al., 2019; 2017) either enforce
the 1-Lipschitz property using Spectral normalization (Miyato et al., 2018) or use some form of
gradient norm penalty (Gulrajani et al., 2017; Mescheder et al., 2018). Since 1-Lipschitz is equivalent
to enforcing a gradient norm constraint (as shown above), we have that most successful GANs
effectively train a discriminator/critic to maximize a geometric margin.
The above shows that training an MMC based on equation (6) is equivalent to training a classifier
with a fixed Lipschitz constant.
5.2.2	MMC leads to better gradient at fake samples
Consider a simple two-dimensional example where x = (x(1), x(2)). Let real data (class 1) be
uniformly distributed on the line between (1, -1) and (1, 1). Let fake data (class 2) be uniformly
distributed on the line between (-1, -1) and (-1, 1). This is represented by Figure 1a. Clearly, the
maximum-margin boundary is the line x(1) = 0 and any classifier should learn to ignore x(2).
Consider a non-linear classifier of the form f(x) = sigmoid(w1x(1) + w0) (See Figure 1b). To
ensure we obtain an MMC, we need to enforce ||Vxf(x)|| ≤ K.
The best classifier with Lipschitz constant K = 1 is obtained by choosing w1 = 4. The maximum-
margin boundary is at x(1) = 0 (which we get by taking w0 = 0; blue curve in Figure 1b); for this
choice, we have that f(xr) = .02 and f(xf) = .98 for real (xr) and fake (xf) samples respectively.
Meanwhile, if We take a slightly worse margin with boundary at x(i)= 4 (equivalent to choosing
w0 = -1; red curve in Figure 1b), we have that f(xr) = .01 and f(xf) = .95 for real and fake
samples respectively. Thus, both solutions almost perfectly classify the samples. However, the
optimal margin has gradient .07, while the worse margin has gradient .03 at fake samples; this is
why maximizing a margin lead to similar signal for real and fake samples. Furthermore, if we had
enforced a bigger Lipschitz constant (K = 2), the best classifier would have been obtained with
w1 = 8 (green curve in Figure 1b); this would have caused vanishing gradients at fake samples unless
we had scaled up the learning up. Thus, for a fixed or decreasing learning rate, it is important to fix
K (and ideally to a small value) in order for the gradient signal to be strong at fake samples.
6
Under review as a conference paper at ICLR 2021
x(2)
-1
-2 -
---Fake samples
---Real samples
—Min-Margin boundary
…∙ Max-Margin boundary
-2	-1	0	1	2
x(1)
(b)
(a)
2 -
1
0 -
T
Figure 1: a) Two-dimensional GAN example with different choices ofboundaries, b) Vf (x(i)) at
different values of x(1) for the two-dimensional example assuming a sigmoid function.
In summary, the maximum-margin discriminator provides a stronger signal at fake samples by
preventing a sharp change in the discriminator (i.e., small gradient near real/fake data and large
gradient between real and fake data) and centering the classifier so that the gradients at real and
fake samples are similar. This further suggests that imposing a gradient penalty in the interpolation
between real and fake data (as done in WGAN-GP) is most sensible to ensure that the gradient norm
remains small between real and fake data.
5.2.3 Better gradients at fake samples implies stable GAN training
In GANs, the dynamics of the game depends in great part on Vxff(xf) where xf’s are samples
from the fake, or generated, distribution. This is because the generator only learns through the
discriminator/critic and it uses Vxff(xf) in order to improve its objective function. Thus, for stable
training with a fixed or decreasing learning rate, ||Vxf f (xf)|| should not be too small.
The above means that, in order to get stable GAN training, we need to ensure that we obtain a
solution with a stable non-zero gradient around fake samples. Thus, it is preferable to solve the
penalized formulation from equation equation 6 and choose a large penalty term λ in order to obtain
a small-gradient solution.
5.3 Are certain Margins better than others ?
It is well known that Lp-norms (with p ≥ 1) are more sensitive to outliers as p increases which is why
many robust methods minimize the L1-norm (Bloomfield & Steiger, 1983). Furthermore, minimizing
the L1 -norm loss results in a median estimator (Bloomfield & Steiger, 1983). This suggests that
penalizing the L2 gradient norm penalty (p = 2) may not lead to the most robust classifier. We
hypothesize that L∞ gradient norm penalties may improve robustness in comparison to L2 gradient
norm penalties since they correspond to maximizing L1 -norm margin. In Section 6, we provide
experimental evidence in support of our hypothesis.
6	Experiments
Following our analysis and discussion in the previous sections, we hypothesized that L1 margins,
corresponding to a L∞ gradient norm penalty, would perform better than L2 margins (L2 gradient
norm penalty). As far as we know, researchers have not yet tried using a L∞ gradient norm penalty in
7
Under review as a conference paper at ICLR 2021
GANs. In addition, We showed that it would be more sensible to penalize violations of ||Vf (x) ||q ≤ 1
rather than ||Vf (x)||q ≈ 1.
To test these hypotheses, we ran experiments on CIFAR-10 (a dataset of 60k images from 10
categories) (Krizhevsky et al., 2009) using HingeGAN (L(z) = max(0, 1 - z)) and WGAN (L(z) =
z) loss functions with L1, L2, L∞ gradient norm penalties. We enforce either ||Vf (x)||q ≈ 1 using
Least Squares (LS) (g(z) = (z - 1)2) or ||Vf (x)||q ≤ 1 using Hinge (g(z) = max(0, z - 1)).
We used the standard hyperparameters: a learning rate (lr) of .0002, a batch size of 32, and the
ADAM optimizer (Kingma & Ba, 2014) with parameters (α1, α2) = (.50, .999) We used a DCGAN
architecture (Radford et al., 2015). As per convention, we report the Frechet Inception Distance
(FID) (Heusel et al., 2017); lower values correspond to better generated outputs (higher quality and
diversity). As per convention, all 50k images from the training part of the dataset were used for
training and to calculate the FID. We ran all experiments using seed 1 and with gradient penalty
λ = 20. Details on the architectures are in the Appendix. All models were trained using a single
GPU. Code is available on xxxxx. The results are shown in Table 2.
Table 2: Frechet Inception Distance (FID) after 100k generator iterations on CIFAR-10.
g(IVχf(x))∣∣q )	WGAN	HingeGAN
(IVxf(X))IIi-I)2	99.7	88.9
max(0, ∣[Vχf(x))∣∣ι — 1)	65.6	77.3
~(IIVxf(X))∣∣2 — 1)2~	37.6	32.8
max(0, ∣[Vχf(x))∣∣2 — 1)	37.8	33.9
~(IVxf (x))II∞ — 1)2-	33.4	33.6
max(0, IIVxf(x))II∞ — 1)	36	27.1
Due to space constraint, we only show the previously stated experiments in Table 2. However, we
also ran additional experiments on CIFAR-10 with 1) Relativistic paired and average HingeGAN,
2) β = (0, .90), 3) the standard CNN architecture from Miyato et al. (2018). Furthermore, we ran
experiments on CAT (Zhang et al., 2008) with 1) Standard CNN (in 32x32), and 2) DCGAN (in
64x64). These experiments correspond to Table 3, 4, 5, 6, and 7 from the appendix.
In all sets of experiments, we generally observed that we obtain smaller FIDs by using: i) a larger q
(as theorized), ii) the Hinge penalty to enforce an inequality gradient norm constraint (in both WGAN
and HingeGAN), and iii) HingeGAN instead of WGAN.
7	Conclusion
This work provides a framework in which to derive MMCs that results in very effective GAN loss
functions. In the future, this could be used to derive new gradient norm penalties which further
improve the performance of GANs. Rather than trying to devise better ways of enforcing 1-Lipschitz,
researchers may instead want to focus on constructing better MMCs (possibly by devising better
margins).
This research shows a strong link between GANs with gradient penalties, Wasserstein’s distance, and
SVMs. Maximizing the minimum L2-norm geometric margin, as done in SVMs, has been shown
to lower bounds on the VC dimension which implies lower generalization error (Vapnik & Vapnik,
1998; Mount, 2015). This paper may help researchers bridge the gap needed to derive PAC bounds
on Wasserstein’s distance and GANs/IPMs with gradient penalty. Furthermore, it may be of interest
to theoreticians whether certain margins lead to lower bounds on the VC dimension.
References
Adler, J. and Lunz, S. Banach wasserstein gan. In Advances in Neural Information Processing
Systems,pp. 6754-6763, 2018.
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In Interna-
tional Conference on Machine Learning, pp. 214-223, 2017.
8
Under review as a conference paper at ICLR 2021
Bloomfield, P. and Steiger, W. L. Least absolute deviations: theory, applications, and algorithms.
Springer, 1983.
Boyd, S. and Vandenberghe, L. Convex optimization. Cambridge university press, 2004.
Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image
synthesis. arXiv preprint arXiv:1809.11096, 2018.
Cortes, C. and Vapnik, V. Support-vector networks. Machine learning, 20(3):273-297,1995.
Elsayed, G., Krishnan, D., Mobahi, H., Regan, K., and Bengio, S. Large margin deep networks for
classification. In Advances in neural information processing systems, pp. 842-852, 2018.
Fedus, W., Rosca, M., Lakshminarayanan, B., Dai, A. M., Mohamed, S., and Goodfellow, I. Many
paths to equilibrium: Gans do not need to decrease adivergence at every step. arXiv preprint
arXiv:1710.08446, 2017a.
Fedus, W., Rosca, M., Lakshminarayanan, B., Dai, A. M., Mohamed, S., and Goodfellow, I. Many
paths to equilibrium: Gans do not need to decrease a divergence at every step. arXiv preprint
arXiv:1710.08446, 2017b.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and
Bengio, Y. Generative adversarial nets. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence,
N. D., and Weinberger, K. Q. (eds.), Advances in Neural Information Processing Systems 27,
pp. 2672-2680. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5423-generative-adversarial-nets.pdf.
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training
of wasserstein gans. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish-
wanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30,
pp. 5767-5777. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7159-improved-training-of-wasserstein-gans.pdf.
Hestenes, M. R. Multiplier and gradient methods. Journal of optimization theory and applications, 4
(5):303-320, 1969.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information
Processing Systems, pp. 6626-6637, 2017.
Holder, O. via an averaging clause. Messagesfrom the Society ofSciences and the Georg-Augusts-
Universitatzu Gottingen,1889:3847, 1889.
Jolicoeur-Martineau, A. Gans beyond divergence minimization. arXiv preprint arXiv:xxxx, 2018a.
Jolicoeur-Martineau, A. The relativistic discriminator: a key element missing from standard gan.
arXiv preprint arXiv:1807.00734, 2018b.
Jolicoeur-Martineau, A. On relativistic f -divergences. arXiv preprint arXiv:1901.02474, 2019.
Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of gans for improved quality,
stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Karras, T., Laine, S., and Aila, T. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4401-4410, 2019.
Karush, W. Minima of functions of several variables with inequalities as side constraints. M. Sc.
Dissertation. Dept. of Mathematics, Univ. of Chicago, 1939.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Kodali, N., Abernethy, J., Hays, J., and Kira, Z. On convergence and stability of gans. arXiv preprint
arXiv:1705.07215, 2017.
9
Under review as a conference paper at ICLR 2021
Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical
report, Citeseer, 2009.
Kuhn, H. W. and Tucker, A. W. Nonlinear programming, in (j. neyman, ed.) proceedings of the
second berkeley symposium on mathematical statistics and probability, 1951.
Lim, J. H. and Ye, J. C. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.
Mao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., and Smolley, S. P. Least squares generative adversarial
networks. In 2017 IEEE International Conference on Computer Vision (ICCV),pp. 2813-2821.
IEEE, 2017.
Marsland, S. Machine learning: an algorithmic perspective. CRC press, 2015.
Matyasko, A. and Chau, L.-P. Margin maximization for robust classification using deep learning. In
2017 International Joint Conference on Neural Networks (IJCNN), pp. 300-307. IEEE, 2017.
Mescheder, L., Geiger, A., and Nowozin, S. Which training methods for gans do actually converge?
arXiv preprint arXiv:1801.04406, 2018.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for generative
adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Mount, J. How sure are you that large margin implies low vc dimension? Win-Vector Blog, 2015.
Muller, A. Integral probability metrics and their generating classes of functions. Advances in Applied
Probability, 29(2):429-443, 1997.
Petzka, H., Fischer, A., and Lukovnicov, D. On the regularization of wasserstein gans. arXiv preprint
arXiv:1709.08894, 2017.
Radford, A., Metz, L., and Chintala, S. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Rogers, L. J. An extension of a certain theorem in inequalities. Messenger of Math., 17:145-150,
1888.
Rudin, W. Functional analysis, mcgrawhill. Inc, New York, 1991.
Vapnik, V. and Vapnik, V. Statistical learning theory. Wiley, 1998.
Zhang, W., Sun, J., and Tang, X. Cat head detection-how to effectively exploit shape and texture
features. In European Conference on Computer Vision, pp. 802-816. Springer, 2008.
10