Under review as a conference paper at ICLR 2021
Continuous Transfer Learning
Abstract
Transfer learning has been successfully applied across many high-impact applica-
tions. However, most existing work focuses on the static transfer learning setting,
and very little is devoted to modeling the time evolving target domain, such as the
online reviews for movies. To bridge this gap, in this paper, we focus on the con-
tinuous transfer learning setting with a time evolving target domain. One major
challenge associated with continuous transfer learning is the time evolving relat-
edness of the source domain and the current target domain as the target domain
evolves over time. To address this challenge, we first derive a generic generaliza-
tion error bound on the current target domain with flexible domain discrepancy
measures. Furthermore, a novel label-informed C-divergence is proposed to mea-
sure the shift of joint data distributions (over input features and output labels)
across domains. It could be utilized to instantiate a tighter error upper bound
in the continuous transfer learning setting, thus motivating us to develop an ad-
versarial Variational Auto-encoder algorithm named CONTE by minimizing the
C-divergence based error upper bound. Extensive experiments on various data sets
demonstrate the effectiveness of our CONTE algorithm.
1 Introduction
Transfer learning has achieved significant suc-
cess across multiple high-impact application
domains (Pan & Yang, 2009). Compared to
conventional machine learning methods assum-
ing both training and test data have the same
data distribution, transfer learning allows US to
learn the target domain with limited label infor-
mation by leveraging a related source domain
with abundant label information (Ying et al.,
2018). However, in many real applications, the
target domain is constantly evolving over time.
Source domain 京	网
Negative transfer ----------------
Target domain IBTI	vT1	Qτj	…	%	> 也
Figure 1: Illustration of continuous transfer learn-
ing. It learns a predictive function in DTt us-
ing knowledge from both source domain DS and
historical target domain DTi (i = 1, ∙∙∙ ,t - 1).
Directly transferring from the source domain DS
to the target domain DTt might lead to negative
transfer with undesirable predictive performance.
For example, the online movie reviews are changing over the years: some famous movies were not
well received by the mainstream audience when they were first released, but became famous only
years later (e.g., Citizen Cane, Fight Club, and The Shawshank Redemption); whereas the online
book reviews typically do not have this type of dynamics. It is challenging to transfer knowledge
from the static source domain (e.g., the book reviews) to the time evolving target domain (e.g., the
movie reviews). Therefore, in this paper, we study the transfer learning setting with a static source
domain and a continuously time evolving target domain (see Figure 1), which has not attracted much
attention from the research community and yet is commonly seen across many real applications. The
unique challenge for continuous transfer learning lies in the time evolving nature of the task relat-
edness between the static source domain and the time evolving target domain. Although the change
in the target data distribution in consecutive time stamps might be small, over time, the cumulative
change in the target domain might even lead to negative transfer (Rosenstein et al., 2005).
Existing theoretical analysis on transfer learning (Ben-David et al., 2010; Mansour et al., 2009)
showed that the target error is typically bounded by the source error, the domain discrepancy of
marginal data distributions and the difference of labeling functions. However, it has been ob-
served (Zhao et al., 2019; Wu et al., 2019) that marginal feature distribution alignment might not
guarantee the minimization of the target error in real world scenarios. This indicates that in the
context of continuous transfer learning, marginal feature distribution alignment would lead to the
sub-optimal solution (or even negative transfer) with undesirable predictive performance when di-
rectly transferring from DS to the target domain DTt at the tth time stamp. This paper aims to bridge
the gap in terms of both the theoretical analysis and the empirical solutions for the target domain
with a time evolving distribution, which lead to a novel continuous transfer learning algorithm as
1
Under review as a conference paper at ICLR 2021
well as the characterization of negative transfer. The main contributions of this paper are summa-
rized as follows: (1) We derive a generic error bound for continuous transfer learning setting with
flexible domain divergence measures; (2) We propose a label-informed domain discrepancy measure
(C-divergence) with its empirical estimate, which instantiates a tighter error bound for continuous
transfer learning setting; (3) Based on the proposed C-divergence, we design a novel adversarial
Variational Auto-encoder algorithm (CONTE) for continuous transfer learning; (4) Extensive exper-
imental results on various data sets verify the effectiveness of the proposed CONTE algorithm.
The rest of the paper is organized as follows. Section 2 introduces the notation and our problem
definition. We derive a generic error bound for continuous transfer learning setting in Section 3.
Then we propose a novel C-divergence in Section 4, followed by a instantiated error bound and a
novel continuous transfer learning algorithm in Section 5. The experimental results are provided in
Section 6. We summarize the related work in Section 7, and conclude the paper in Section 8.
2	Preliminaries
In this section, we introduce the notation and problem definition of continuous transfer learning.
2.1	Notation
We use X and Y to denote the input space and label space. Let DS and DT denote the source and
target domains with data distribution pS (x, y) and pT (x, y) over X×Y, respectively. Let H be a
hypothesis class on X , where a hypothesis is a function h : X→Y. The notation is summarized in
Table 3 in the appendices.
2.2	Problem Definition
Transfer learning (Pan & Yang, 2009) refers to the knowledge transfer from source domain to target
domain such that the prediction performance on the target domain could be significantly improved
as compared to learning from the target domain alone. However, in some applications, the target
domain is changing over time, hence the time evolving relatedness between the source and target
domains. This motivates us to consider the transfer learning setting with the time evolving target
domain, which is much less studied as compared to the static transfer learning setting. We formally
define the continuous transfer learning problem as follows.
Definition 2.1. (Continuous Transfer Learning) Given a source domain DS (available at time stamp
j =1) and a time evolving target domain {DTj }jn=1 with time stamp j, continuous transfer learning
aims to improve the prediction function for target domain DTt+1 using the knowledge from source
domain DS and the historical target domain DTj (j = 1, ∙∙∙ ,t).
Notice that the source domain DS can be considered a special initial domain for the time-evolving
target domain. Therefore, for notation simplicity, we will use DT0 to represent the source domain
in this paper. It assumes that there are mT0 labeled source examples drawn independently from a
source domain DT0 and mTj labeled target examples drawn independently from a target domain
DTj at time stamp j .
3	AGeneric Error Bound
Given a static source domain and a time evolving target domain, continuous transfer learning aims
to improve the target predictive function over DTt+1 using the source domain and historical target
domain. We begin by considering the binary classification setting, i.e., Y = {0, 1}. The source error
of a hypothesis h can be defined as follows: cTq(h) = E(χ,y)〜PTo (x,y) [L(h(x),y)] where L(∙, ∙)
is the loss function. Its empirical estimate using source labeled examples is denoted as /(h).
Similarly, We define the target error eq (h) and the empirical estimate of the target error ^τj. (h) over
the target distribution pTj (x, y) at time stamp j. A natural domain discrepancy measure over joint
distributions on X × Y between features and class labels can be defined as follows:
di (Dτo, Dt ) = SUp IPrDT0 [Q] - PrDT [Q]∣	(1)
Q∈Q 0
where Q is the set of measurable subsets under pτ0 (x, y) and pτ (x, y)1. Then, the error bound of
continuous transfer learning is given by the following theorem.
Theorem 3.1. Assume the loss function L is bounded with 0 ≤L≤M. Given a source domain
Dτ0 and historical target domain {Dτi}it=1, for h ∈H, the target domain error eτt+1 on Dt+1 is
1 Note that it is slightly different from L1 or variation divergence in (Ben-David et al., 2010) with only
marginal distribution of features involved.
2
Under review as a conference paper at ICLR 2021
bounded as follows.
1	( t	t
eTt+1 (h) ≤ - fμt-jeTj (h) + ME μt-jdi (DTj, DTt+1)
μ ∖j=0	j=0
where μ ≥ 0 is the domain decay rate2 indicating the importance of source or historical target
domain over Dτ+γ, and 口 = Pj=o 厂’.
Remark. In particular, we have the following arguments. (1) It is not tractable to accurately es-
timate d1 from finite examples in real scenarios (Ben-David et al., 2010); (2) This error bound
could be much tighter when considering other advanced domain discrepancy measures, e.g., A-
distance (Ben-David et al., 2007), discrepancy distance (Mansour et al., 2009), etc. (3) There are
two special cases: when μ = 0, the error bound of Dτ+1γ would be simply determined by the latest
historical target data DTt, and if μ goes to infinity, Dτt+1 is just determined by the source data
Dτo because intuitively the coefficient μt-j/μ of historical target domain data DTj (j = 1,…，t)
converges to zero.
Corollary 3.2. With the assumption in Theorem 3.1 and assume that the loss function L is symmetric
(i.e., L(y1,y2)=L(y2,y1) for y1,y2 ∈Y) and obeys the triangle inequality, Then
(1)	if A-distance (Ben-David et al., 2007) is adopted to measure the distribution shift, i.e., dH∆H
suPh,h0∈H IPrDTO [h(x) = h0(x)] - PrDτ[h(x) = h0(x)]∣, we have:
eTt+ι (h) ≤ - (X μt-jeTj (h) + M X μt-j (dH∆H(DTj , DTt+1 ) +
μ V=0	j=0	∖
where λj = minh∈H e% (h) + eTt+ι (h).
(2)	if discrepancy distance (Mansour et al., 2009) is adopted to measure the distribution shift, i.e.,
ddisc(DTo, DT)=maXh,h0∈H ∣Edt0 [L(h(x),h0(x))] - EDT [L(h(x),h0(x))] ∣, we have:
eTt+ι (h) ≤ - (X μt-jeTj(h) + X μt-j (ddisC(DTj, DTt+1) + %))
μ ∖j=0	j=0
where Ωj = Ed5 [L(h；(x),y)] + Ed3[L(h；(x),hJ+i(x))] + Ed3[L(hJ+ι(x),y)], and
hj = argminh∈H e4 (h) for j =0,…，t,t + 1.
The aforementioned domain discrepancy measures mainly focus on the marginal distribution over
input features and have inspired a line of practical transfer learning algorithms (Ganin et al., 2016;
Chen et al., 2019). However, recent work (Wu et al., 2019; Zhao et al., 2019) observed that the
minimization of marginal distributions cannot guarantee the success of transfer learning in real sce-
narios. We propose to address this problem by incorporating the label information in the domain
discrepancy measure (see next section).
4 Label-informed Domain Discrepancy
In this section, we introduce a novel label-informed domain discrepancy measure between the source
domain DT0 and target domain DT , its empirical estimate, and a transfer signature based on this
measure to identify potential negative transfer. The use of this discrepancy measure in continuous
transfer learning will be discussed in the next section.
4.1	C-DIVERGENCE
For a hypothesis h ∈H, we denote I(h) to be the subset of X such that x ∈ I(h) ⇔ h(x) = 1. In
order to estimate the label-informed domain discrepancy from finite samples in practice, instead of
Eq. (1), we propose the following C-divergence between DT0 and DT , taking into consideration the
joint distribution over features and class labels:
de(Dto, Dt) = sup ∣PrDτ0 [{I(h),y = 1}∪{I(h),y = 0}]-PrDT[{I(h),y = i}∪{I(h),y = 0}]∣
h∈H	0
— ⑵
where I(h) is the complement of I(h).
2In this case, We assume μ0 = 1 for any μ ≥ 0.
3
Under review as a conference paper at ICLR 2021
We show that some existing domain discrepancy methods (e.g., Ben-David et al. (2007)) can be seen
as special cases of this definition by using the following relaxed covariate shift assumption.
Definition 4.1. (Relaxed Covariate Shift Assumption) The source and target domains satisfy the
relaxed covariate shift assumption if for any h ∈H,
PrDT0 [y	|	I(h)]	= PrDT [y	|	I(h)]	=	Pr[y	|	I(h)]	(3)
Notice that it would be equivalent to covariance shift assumption (Shimodaira, 2000; Johansson
et al., 2019) when I(h) consists of only one example for all h ∈H(see Lemma A.6 for details).
Lemma 4.2. With the relaxed covariate shift assumption, for any h ∈H, we have:
-PDDt [I(h)]) ∙ Sh + PDDt [y = 1] - PDDt0 [y =1] |
hUH I PrDDτo [I(h)]
dC (DT0 , DT)
where Sh = Pr[y = 1|I(h)] - Pr[y = 0|I(h)].
Remark. FDom Lemma 4.2, we can see that in the special case wheDe Sh is a constant foD all h ∈H
and PDDT [y = 1] = PDDT0 [y = 1], the pDoposed C-diveDgence is Deduced to the A-distance (Ben-
David et al., 2007) defined on the maDginal distDibution of featuDes. MoDe geneDally speaking, C-
diveDgence can be consideDed as a weighted veDsion of the A-distance wheDe the hypothesis whose
chaDacteDistic function has a laDgeD class-sepaDability (i.e., |Sh|) Deceives a higheD weight. Intu-
itively, compaDed to A-distance, C-diveDgence would pay less attention to class-insepaDable Degions
in the input featuDe space, which pDovide iDDelevant infoDmation foD leaDning the pDediction function
in the taDget domain.
Moreover, the following theorem states that in conventional transfer learning scenario with a static
source domain and a static target domain, the target error is bounded in terms of C-divergence across
domains and the expected source error.
Theorem 4.3. Assume that loss function L is bounded, i.e., theDe exists a constant M>0 such that
0 ≤L≤M. FoD a hypothesis h ∈H, we have the following bound:
eτ(h) ≤ eτo (h) + M ∙ de(Dt0 , DT)
4.2 Empirical Estimate of C-DIVERGENCE
In practice, it is difficult to calculate the proposed C-divergence based on Eq. (2) as it uses the
true underlying distributions. Therefore, we propose the following empirical estimate of the C-
divergence between DT0 and DT as follows. Assuming that the hypothesis class H is symmetric
(i.e., 1 - h ∈Hif h ∈H), the empirical C-divergence is:
11
de(Dτo,Dt) = 1-min 1--- E	I[(x,y) ∈ Dτ°]+--E	I[(x,y) ∈ DT]1(4)
h∈H mT	mT
0 (x,y):h(x)6=y	(x,y):h(x)=y
where DT0 and DT denote the source and target domains with finite samples, respectively. I[a] is
the binary indicator function which is 1 if a is true, and 0 otherwise.
The following lemma provides the upper bound of the true C-divergence using its empirical estimate.
Lemma 4.4. FoD any δ ∈ (0, 1), with pDobability at least 1 - δ oveD mT0 labeled souDce examples
BT0 and mT labeled taDget examples BT, we have:
de (Dτo, DT) ≤ de (DTO, DT) + (<Bt0 (LH) + <Bt(Lh)) +3 f ^^ɪɪ +
wheDe <B (LH)(B ∈ {BT0, BT}) denotes the RademacheD complexity (MansouD et al., 2009) oveD B
and LH = {(x, y) → I[h(x) = y] : h ∈ H} is a class of functions mapping Z = X X Y to {0, 1}.
4.3 Negative Transfer Characterization
Informally, negative transfer is considered as the situation where transferring knowledge from the
source domain has a negative impact on the target learner (Wang et al., 2019): eT (A(DT0 , DT)) >
eτ(A(0, DT)) where A is the learning algorithm. eτ is the target error induced by algorithm A. 0
implies that it only considers the target data set for target learner. In this paper, we define a transfer
signature to measure the transferability from source domain to target domain as follows.
(5)
TS(DT||DT0))= inf (eT(A(DT0,DT))-eT(A(0,DT)))
A∈G
where G is the set of all learning algorithms. We state that source domain knowledge is not trans-
ferable over target domain when TS(DT||DT0)) > 0. Specially, since A(DT0 , DT) learns an
optimal classifier using both source and target data, We can define eτ(A(DTO,DT)) = eτ(hα)
4
Under review as a conference paper at ICLR 2021
where hα = argminh∈H(A) αeτ(h) + (1 - α)eτ0 (h) and H(A) is the hypothesis space induced
by A. When we only consider the target domain with α = 1, eτ(A(0, DT)) = eτ(hr) where
hT = arg minh∈H(A) eτ(h). Then We have the following theorem regarding the transfer signature.
Theorem 4.5. Assuming the loss function L is bounded with 0 ≤L≤M, we have
eτ(hα) ≤ eτ(hT) + 2(1 - α)Mdc(Dt0 , DT)
Furthermore,
TS(DT||DT0)) ≤2(1-α)MdC(DT0,DT)
Remark. We have the following observations: (1) Larger C-divergence between domains is often
associated with a higher transfer signature, which indicates that negative transfer can be charac-
terized using the proposed C-divergence; (2) Empirically, the larger amount of labeled target data
could increase the value of α, resulting in the learned classifier relying more on the target data,
which is consistent with the observation in (Wang et al., 2019). One extreme case is where α =1,
implying we have adequate labeled target examples for standard supervised learning on the target
domain without transferring knowledge from the source domain.
5 Proposed Algorithm
In this section, we derive the continuous error bound based on our proposed C-divergence, followed
by a novel continuous transfer learning algorithm (CONTE) by minimizing the error upper bound.
Notice that in the context of continuous transfer learning, we also use the proposed C-divergence
between the target domain at adjacent time stamps to measure the change in distribution over time.
5.1	Continuous Error Bound with Empirical C-DIVERGENCE
The following theorem states that for a bounded loss function L, the target error in continuous trans-
fer learning can be bounded in terms of the empirical classification error within source and histori-
cal target domains, the empirical C-divergence across domains as well as the empirical Rademacher
complexity of the class of functions LH = {(x, y) → I[h(x)=y]:h ∈H}.
Theorem 5.1. (Continuous Error Bound) Assume the loss function L is bounded with 0 ≤L≤M.
Given a source domain DT0 and historical target domain {DTi}it=1, for h ∈Hand δ ∈ (0, 1), with
probability at least 1 - δ, the target domain error eTt+1 on DTt+1 is bounded as follows.
eτt+ι (h) ≤ ɪ (XX μt-j ^Tj (h)+ M XX μt-jdc (Dj DDTt+ι) + M Λ∣
μ j=0	j=0	)
where Λ = Pt- 0 (< BT (LH) + < BT (LH )+3,署ɪ +3,产工 + J M2log 4 ).
j =0 BTj H BTt+1 H	2mTj	2mTt+1	2mTj
Remark. Compared to continuous error bounds in Corollary 3.2 using existing domain diver-
gence measures (Ben-David et al. (2007); Mansour et al. (2009)), our bound consists of only data-
dependent terms (e.g., empirical source error and C-divergence), whereas previous error bounds are
determined by the error terms involving the intractable labeling function or optimal target hypothe-
sis (see Corollary 3.2).
5.2	CONTE Algorithm
For continuous transfer learning, we leverage both the source domain and historical target domain
data to learn the predictive function for the current time stamp. To this end, we propose to minimize
the error bound in Theorem 5.1 for learning the predictive function on DTt+1 . Furthermore, we
aim to learn a domain-invariant and time-invariant latent feature space such that the C-divergence
across domains and across time stamps could be minimized. Therefore, we present an adversarial
Variational Auto-encoder (VAE) algorithm with the following overall objective function:
J(T0,T1,T2,…，Tt+ι) = ^X μt-j (Lclc (Tj,Tt+ι) + dc(D4 ,方丁叶1) + λLELBo (Tj,Tt+ι))	(6)
j=0
where Lclc(Tj,Tt+1) represents the classification error over the labeled examples from DTj and
DTt+1, dC (DTj , DTt+1) is the empirical estimate of C-divergence across domain. Thus the first two
terms of Eq. (6) are associated with ^T∙ (h)+dc (DTj, D^τ+1λ,) in the error bound of Theorem5.1 The
third term LELBO(Tj, Tt+1) is the variational bound in the VAE framework (see Figure 4) when
learning the latent feature space and λ > 0 is a hyper-parameter. In this case, we have μ ∈ [0,1]
because we assume that the data distribution of a time-evolving target domain shifts smoothly over
time. Then we instantiate the terms of Eq. (6) as follows.
5
Under review as a conference paper at ICLR 2021
Inspired by semi-supervised VAE (Kingma et al., 2014), we propose to learn the feature space by
maximizing the following likelihood across domains.
logpθ(x, y) = KL(q°(z∣x, y)∣∣pθ(z|x, y)) + Eq.(z∣x,y)[logPθ(x, y, z) - logqφ(z∣x, y)]
where φ and θ are the learnable parameters in the encoder and decoder respectively, and z is the
latent feature representation of the input example (x, y). KL(∙∣∣∙) is KUllback-Leibler divergence.
The evidence lower bound (ELBO), a lower bound on this log-likelihood, can be written as follows.
Eθ,φ(x,y) = Eqφ(z∣χ,y) [logpθ(x,y∣z)] +KL(qφ(z∣x,y)l∣p(z))	(7)
where Eθ,φ(x, y) ≤ logpθ(x, y). Similarly, we have the following ELBO to maximize the log-
likelihood of pθ (x) when the label is not available:
Uθ,φ(x) = Ey (qφ(y∣x) ∙ Eθ,φ(x, y) - Eqφ(y∣χ) [logq°(y∣x)D	(8)
wherepθ(x, y, z) = pθ(x|y, z)pθ(y∣z)p(z) with prior Gaussian distributionp(z) = N(0, I). There-
fore, the variational bound LELBO(Tj, Tt+1) is given below.
LELBO(Tj,Tt+1)=-XmTj+mTt+1Eθ,φ(xi,yi)-XuTt+1Uθ,φ(xi,yi)	(9)
i=1	i=1
where uTt+1 is the number of unlabeled training examples from DTt+1 . Besides, the classification
error Lclc(Tj,Tt+1) can be expressed as follows.
Lclc(Tj,Tt+ι) = XmTj +mτt+1 L (yi,qφ(∙∣xi))	(10)
i=1
where qd(∙) is the discriminative classifier formed by the distribution qφ(y∣x) in Eq. (8), and L(∙, ∙)
is the cross-entropy loss function in our experiments. To estimate the C-divergence, we first define
h to be a two-dimensional characteristic function with h(x,y)=1⇔ h(x)=y ⇔{h(x)=1,y =
1} ∨ {h(x)=0,y =0} for h ∈H. Then the empirical C-divergence in Eq. (4) can be rewritten as
follows.
dc(Dτj, DTt+ι) = 1-min∣'	X	I[(x,y) ∈ DTj]+^—	X	I[(x,y) ∈ DTt+』
h m mTj	-	mTt+ι	-	1
(x,y):h(x,y)=0	(x,y):h(x,y) = 1
Note that the latent feature representation Z learned by qφ(z∣x, y) could capture the label-informed
information of an example (x, y). Thus, the hypothesis h can be considered as the composition of
a feature extraction qd and a domain classifier Fj, i.e, h(x,y) = Fj(qφ(z∣x, y)). Formally, the
empirical estimate of C-divergence is given below.
11
dC(DTj,DTt+ι) = 1- ιψn I —	E	I[z	∈ DTj]	+	;;^ —	E	I[z	∈ DTt+/(11)
Fj m	m
j	Tj z:Fj (z)=0	Tt+1 z:Fj (z)=1
The benefits of CONTE are twofold: first, it learns the latent feature space using both input x and
output y; second, it minimizes a tighter error upper bound based on C-divergence in Theorem 5.1.
This framework can also be interpreted as a minimax game: the VAE learns a domain-invariant and
time-invariant latent feature space, whereas the domain classifier Fj aims to distinguish the exam-
ples from different domains and different time stamps. In this paper, we adopt the gradient reversal
layer (Ganin et al., 2016) when updating the parameters of domain classifier Fj, and thus CONTE
can be optimized by back-propagation in an end-to-end manner (see Algorithm 1 in appendices).
However, we observe that (1) it is difficult to estimate the C-divergence with only limited labeled
target examples from DTt+1 ; (2) when learning the latent features z, combining the data x (e.g.,
one image) and class-label y directly might lead to over-emphasizing the data itself due to its high
dimensionality compared to y. To address these problems, we propose the following Pseudo-label
Inference, i.e., We infer the pseudo labels of unlabeled examples using the classifier qφ(y∣x) for each
training epoch. Using labeled source and target examples as well as unlabeled target examples with
inferred pseudo labels, the C-divergence could be estimated in a balanced setting. Furthermore, to
enforce the compatibility between features x and label y, we adopt a pre-encoder step to learn a
dense representation for the input x, and then learn the label-informed latent features z.
6	Experimental Results
Synthetic Data: We generate a synthetic data set in which each domain has 1000 posi-
tive examples and 1000 negative examples randomly generated from Gaussian distributions
N([1.5cosθ, 1.5Sinθ]T,0.5 ∙ I2×2) andN([1.5cos(-θ), 1.5sin(-θ)]T, 0.5 ∙ I2×2), respectively.
We let θ = 0 for the source domain (denoted as S1), and θ =浮(i = 1, ∙∙∙ ,t) for the time evolving
target domain with t = 8 time stamps (denoted as T1, •一，T8).
Image Data: We consider the following two tasks: digital classification (MNIST, SVHN) and image
classification (Office-31 with three domains: Amazon, DSLR and Webcam; and Office-Home with
6
Under review as a conference paper at ICLR 2021
four domains: Art, Product, Clipart and Real World). Since standard domains are static in these data
sets, we will simulate the time-evolving distribution shift on the target domain by adding noise (e.g.,
random salt&pepper noise, adversarial noise, rotation). Take SVHN→MNIST as an example, we
will use SVHN as the static source domain, and MNIST as the target domain at the first time stamp.
By adding adversarial noise to the MNIST images, we obtain a time-evolving target domain (denoted
as T1,…，T11in Table 1). For Office-31 and Office-Home, We add the random salt&pepper noise
and rotation to generate the evolving target domain. More details can be found in the appendices.
Baselines: The baseline methods are as folloWs. (1) SourceOnly: training With only source data;
(2) TargetERM: empirical risk minimization (ERM) on only target domain; (3) DAN (Long et al.,
2015), CORAL (Sun & Saenko, 2016), DANN (Ganin et al., 2016), ADDA (Tzeng et al., 2017),
WDGRL (Shen et al., 2018), DIFA (Volpi et al., 2018) and MDD (Zhang et al., 2019): training With
feature distribution alignment. (4) CONTE: training With label-informed distribution alignment on
the evolving target domain while μ ∈ {0,0.2,0.4,0.6,0.8,1}. (5) CONTE∞: a one-time transfer
learning variant of CONTE that directly transfers from source domain to current target domain. We
fix λ = 0.1, and all the methods use the same neural network architecture for feature extraction.
6.1	Evaluation of C-divergence
We compare the proposed C-divergence with
conventional domain discrepancy measure A-
distance (Ben-David et al., 2007) on a synthetic
data set with an evolving target domain. We as-
sume that the hypothesis space H consists of
linear classifiers in the feature space. Figure 2
shows the domain discrepancy and target clas-
sification accuracy for each pair of source and
target domains. We have the following obser-
Figure 2: Comparison of domain discrepancy and
target accuracy
vations. (1) The classification accuracy on the target domain significantly decreases from target
domain T1 to T8. One explanation is that the joint distribution p(x, y) on the time evolving target
domain gradually shifted. (2) The A-distance increases from S1→T1 to S1→T4, and then decreases
from S1→T4 to S1→T8. That is because it only estimates the difference of the marginal feature dis-
tribution p(x) between the source and target domains. (3) The C-divergence keeps increasing from
S1→T1 to S1→T8, which indicates the decreasing task relatedness between the source and the tar-
get domains. Therefore, compared with A-distance3, the proposed C-divergence better characterizes
the transferability from the source to the target domains.
6.2	Evaluation of Error Bound
When there is only one time stamp involved
in the target domain, Theorem 5.1 is reduced
to the standard error bound in the conventional
static transfer learning setting. We empiri-
cally compare this reduced error bound with the
existing Rademacher complexity based error
bound in (Mansour et al., 2009) (see Theorem
A.4 in appendices for being self-contained).
Figure 3: Comparison of error bounds
We use the 0-1 loss function as L and assume that the hypothesis space H consists of linear classi-
fiers in the feature space. Figure 3 shows the estimated error bounds and target error with the time
evolving target domain (i.e., S1→T1, •一，S1→T8 in a new synthetic data set with a slower time
evolving target domain to ensure that the baseline bound is meaningful most of the time) where we
choose h = hT。. It demonstrates that our C-divergence based error bound is much tighter than the
baseline. Notice that when transferring source domain S1 to target domain T8, our error bound is
largely determined by the C -divergence, whereas the baseline is determined by the difference be-
tween the optimal source and target hypothesizes. Furthermore, given any hypothesis h ∈H, we
may not be able to estimate the baseline bound when the optimal hypothesis is not available.
6.3	Evaluation of Continuous Transfer Learning
Tables 1 and 2 provide the continuous transfer learning results on digital and office-31 data sets
where the classification accuracy on target domain is reported (the best results are highlighted in
bold). It is observed that (1) the classification accuracy using SourceOnly algorithm significantly
3The results for other existing discrepancy measures follow a similar pattern and thus omitted for brevity
7
Under review as a conference paper at ICLR 2021
decreases on the evolving target domain due to the shift of joint data distribution p(x, y) on target domain; (2) the performance of static baseline algorithms is largely affected by the distribution							
shift in the evolving target domain, and even					worse	than TargetERM in some cases (e.g., on T6-	
T11 from SVHN to evolving MNIST); (3) CONTE significantly outperforms CONTE∞ as well as other competitors on target domain by a large margin (i.e., up to 30% improvement on the last time stamp of target domain) because it effectively leverages the historical target domain information to smoothly re-align the target distribution when the change of target domain distribution in consecutive							
time stamps is small.							
Table 1:	Transfer learning accuracy from SVHN (source) to time evolving MNIST (target)						
Target Domain T1		T2	T3	T4	T5	T6	T7	T8	T9	T10	T11
SourceOnly	0.6998	0.6738	0.6336	0.5692	0.4747	0.4110 0.3087 0.2220 0.1481 0.0828	0.0764
TargetERM	0.7451	0.6997	0.6618	0.6314	0.6368	0.6359 0.6695 0.7133 0.7214 0.7450	0.7512
CORAL	0.8349	0.8410	0.7633	0.7063	0.6496	0.5900 0.5031 0.5101 0.4337 0.4156	0.4502
DANN	0.8666	0.8356	0.8018	0.7529	0.7309	0.6641 0.6614 0.5618 0.5204 0.5082	0.4594
ADDA	0.8667	0.8487	0.7982	0.7187	0.6804	0.5397 0.4366 0.3473 0.2636 0.1659	0.1259
WDGRL	0.8990	0.8602	0.8247	0.8222	0.7452	0.6877 0.6481 0.5896 0.5145 0.4952	0.5196
DIFA	0.9164	0.8993	0.8713	0.8273	0.7935	0.6661 0.5956 0.4381 0.3479 0.2448	0.1332
CONTE∞	0.9747	0.9552	0.9514	0.9279	0.8801	0.8833 0.8691 0.6979 0.7030 0.7415	0.7316
CONTE	0.9747	0.9740	0.9803	0.9864	0.9908	0.9940 0.9950 0.9965 0.9970 0.9967	0.9975
Table 2: Transfer learning accuracy on Office-31							
		Amazon → Webcam				Webcam → DSLR	
	T1	T2	T3	T4	-^TT^	_ _ T1	T2	T3	T4	T5
SourceOnly	0.7490	0.2255	0.2282	0.1275	0.1503	0.9651 0.4309 0.3329 0.1611	0.2027
TargetERM	0.5584	0.3933	0.4215	0.3396	0.3732	0.4966 0.4201 0.4188 0.3248	0.4067
DAN	0.8537	0.5007	0.4993	0.3638	0.4470	0.9772 0.7302 0.6161 0.4765	0.5302
CORAL	0.8711	0.5235	0.4819	0.3195	0.4054	0.9812 0.7289 0.6671 0.4846	0.5221
DANN	0.8389	0.4993	0.4121	0.3973	0.3382	0.9651 0.7356 0.6416 0.4510	0.5490
MDD	0.8940	0.6738	0.5490	0.5141	0.4295	0.9724 0.8738 0.7315 0.5047	0.5289
CONTE∞	0.9154	0.6376	0.5758	0.4591	0.4846	0.9785 0.8591 0.7289 0.4926	0.5557
CONTE	0.9154	0.8134	0.8081	0.7611	0.7826	0.9785	0.9235	0.9208	0.8886	0.9154
7	Related Work
Transfer Learning: Transfer learning (Ying et al., 2018; Jang et al., 2019) improves the perfor-
mance of a learning algorithm on the target domain by using the knowledge from the source domain.
It is theoretically proven that the target error is well bounded (Ben-David et al., 2010; Mansour et al.,
2009), followed by a line of practical algorithms (Shen et al., 2018; Long et al., 2017; 2018; Saito
et al., 2018; Chen et al., 2019) with covariate shift assumption. However, it is observed that this as-
sumption does not always hold in real-world scenarios (Rosenstein et al., 2005; Wang et al., 2019).
Multi-source Domain Adaptation: Multi-source domain adaptation improves the target prediction
function from multiple source domains (Zhao et al., 2018; Hoffman et al., 2018; Wen et al., 2020).
It is similar to our problem setting as source and historical target domains can be considered as
multiple “source” domains when modeling the target domain at the current time stamp. However,
only limited labeled target examples are provided in our problem setting, whereas multi-source
domain adaptation requires that all source domains have adequate labeled examples.
Continual Learning: Continual lifelong learning (Parisi et al., 2019; Rusu et al., 2016; Hoffman
et al., 2014; Bobu et al., 2018) involves the sequential learning tasks with the goal of learning a
predictive function on the new task using knowledge from historical tasks. Most of them focused on
mitigating catastrophic forgetting when learning new tasks from only one evolving domain, whereas
our work studied the transferability between a source domain and a time evolving target domain.
8	Conclusion
In this paper, we study continuous transfer learning with a time evolving target domain, which has
not been widely studied and yet is commonly seen in many real applications. We start by deriving
a generic error bound of continuous transfer learning with flexible domain discrepancy measures.
Then we propose a novel label-informed C-divergence to measure the domain discrepancy incorpo-
rating the label information, and study its application in continuous transfer learning, which leads to
an improved error bound. Based on this bound, we further propose a generic adversarial Variational
Auto-encoder algorithm named CONTE for continuous transfer learning. Extensive experiments on
both synthetic and real data sets demonstrate the effectiveness of our CONTE algorithm.
8
Under review as a conference paper at ICLR 2021
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in Neural Information Processing Systems, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine Learning, 2010.
Andreea Bobu, Eric Tzeng, Judy Hoffman, and Trevor Darrell. Adapting to continuously shifting
domains. In International Conference on Learning Representations Workshop, 2018.
Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminabil-
ity: Batch spectral penalization for adversarial domain adaptation. In International Conference
on Machine Learning, 2019.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Judy Hoffman, Trevor Darrell, and Kate Saenko. Continuous manifold based adaptation for evolv-
ing visual domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2014.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. In Advances in Neural Information Processing Systems, 2018.
Yunhun Jang, Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. Learning what and where to transfer.
In International Conference on Machine Learning, 2019.
Fredrik D Johansson, Rajesh Ranganath, and David Sontag. Support and invertibility in domain-
invariant representations. In International Conference on Artificial Intelligence and Statistics,
2019.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems,
2014.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features
with deep adaptation networks. In International Conference on Machine Learning, 2015.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In International Conference on Machine Learning, 2017.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge
and Data Engineering, 2009.
German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 2019.
Michael T Rosenstein, Zvika Marx, Leslie Pack Kaelbling, and Thomas G Dietterich. To transfer or
not to transfer. In NIPS 2005 Workshop on Transfer Learning, 2005.
9
Under review as a conference paper at ICLR 2021
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
arXiv:1606.04671, 2016.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2018.
Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In Thirty-Second AAAI Conference on Artificial Intelligence,
2018.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning and Inference, 2000.
Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
European conference on computer vision, pp. 443-450. Springer, 2016.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2017.
Riccardo Volpi, Pietro Morerio, Silvio Savarese, and Vittorio Murino. Adversarial feature augmen-
tation for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2018.
ZirUi Wang, Zihang Dai, Barnabas Poczos, and Jaime Carbonell. Characterizing and avoiding nega-
tive transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2019.
JUnfeng Wen, RUssell Greiner, and Dale SchUUrmans. Domain aggregation networks for mUlti-
soUrce domain adaptation. In International Conference on Machine Learning, pp. 10927-10937,
2020.
Yifan WU, Ezra Winston, Divyansh KaUshik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribUtion alignment. In International Conference on Machine Learn-
ing, 2019.
Wei Ying, YU Zhang, JUnzhoU HUang, and Qiang Yang. Transfer learning via learning to transfer.
In International Conference on Machine Learning, 2018.
YUchen Zhang, Tianle LiU, Mingsheng Long, and Michael I Jordan. Bridging theory and algorithm
for domain adaptation. In International Conference on Machine Learning, 2019.
Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J
Gordon. Adversarial mUltiple soUrce domain adaptation. In Advances in neural information
processing systems, pp. 8559-8570, 2018.
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. In International Conference on Machine Learning, 2019.
10