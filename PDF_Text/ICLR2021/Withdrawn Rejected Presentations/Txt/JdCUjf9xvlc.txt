Under review as a conference paper at ICLR 2021
Fourier Representations for Black-Box
Optimization over Categorical Variables
Anonymous authors
Paper under double-blind review
Ab stract
Optimization of real-world black-box functions defined over purely categorical
variables is an active area of research. In particular, optimization and design
of biological sequences with specific functional or structural properties have a
profound impact in medicine, materials science, and biotechnology. Standalone
acquisition methods, such as simulated annealing (SA) and Monte Carlo tree search
(MCTS), are typically used for such optimization problems. In order to improve
the performance and sample efficiency of such acquisition methods, we propose
to use existing acquisition methods in conjunction with a surrogate model for the
black-box evaluations over purely categorical variables. To this end, we present
two different representations, a group-theoretic Fourier expansion and an abridged
one-hot encoded Boolean Fourier expansion. To learn such models, characters
of each representation are considered as experts and their respective coefficients
are updated via an exponential weight update rule each time the black box is
evaluated. Numerical experiments over synthetic benchmarks as well as real-world
RNA sequence optimization and design problems demonstrate the representational
power of the proposed methods, which achieve competitive or superior performance
compared to state-of-the-art counterparts, while improving the computational cost
and/or sample efficiency substantially.
1 Introduction
A plethora of practical optimization problems involve black-box functions, with no simple analytical
closed forms, that can be evaluated at any arbitrary point in the domain. Optimization of such
black-box functions poses a unique challenge due to restrictions on the number of possible function
evaluations, as evaluating functions of real-world complex processes is often expensive and time
consuming. Efficient algorithms for global optimization of expensive black-box functions take past
queries into account in order to select the next query to the black-box function more intelligently.
While black-box optimization of real-world functions defined over integer, continuous, and mixed
variables has been studied extensively in the literature, limited work has addressed incorporation of
purely categorical type input variables.
Categorical type variables are particularly challenging when compared to integer or continuous
variables, as they do not have a natural ordering. However, many real-world functions are defined
over categorical variables. One such problem, which is of wide interest, is the design of optimal
chemical or biological (protein, RNA, and DNA) molecule sequences, which are constructed using a
vocabulary of fixed size, e.g. 4 for DNA/RNA. Designing optimal molecular sequences with improved
or novel structures and/or functionalities is of paramount importance in material science, drug and
vaccine design, synthetic biology and many other applications (see Dixon et al. (2010); Ng et al.
(2019); Hoshika et al. (2019); Yamagami et al. (2019)). Design of optimal sequences is a difficult
black-box optimization problem over a combinatorially large search space (Stephens et al. (2015)),
in which function evaluations often rely on either wet-lab experiments, physics-inspired simulators,
or knowledge-based computational algorithms, which are slow and expensive in practice. Another
problem of interest is the constrained design problem, e.g. find a sequence given a specific structure
(or property), which is inverse of the well-known folding problem discussed in Dill & MacCallum
(2012). This problem is complex due to the strict structural constraints imposed on the sequence. In
fact one of the ways to represent such a complex structural constraint is to constrain the next choice
1
Under review as a conference paper at ICLR 2021
sequentially based on the sequence elements that have been chosen a priori. Therefore, we divide the
black box optimization problem into two settings, depending on the constraint set: (i) Generic Black
Box Optimization (BBO) problem referring to the unconstrained case and (ii) Design Problem that
refers to the case with complex sequential constraints.
Let xt be the t-th sequence evaluated by the black box function f. The key question in both settings is
the following: Given prior queries x1, x2 . . . xt and their evaluations f(x1) . . . f(xt), how to choose
the next query xt+1? This acquisition must be devised so that over a finite budget of black-box
evaluations, one is closest to the minimizer in an expected sense over the acquisition randomness.
In the literature, for design problems with sequential constraints, MCTS (Monte Carlo Tree Search)
based acquisitions are often used with real function evaluations f (xt). In the generic BBO problems
in the unconstrained scenario, Simulated Annealing (SA) based techniques are typically used as
acquisition functions. A key missing ingredient in the categorical domain is a surrogate model for the
black-box evaluations that can interpolate between such evaluations and use cost-free approximate
evaluations from the surrogate model internally (in acquisition functions) in order to reduce the need
for frequently accessing real evaluations. This leads to improved sample efficiency in acquisition
functions. Due to the lack of efficient interpolators in the categorical domains, existing acquisition
functions suffer under a finite budget constraint, due to reliance on only real black-box evaluations.
Contributions: We address the above problem in our work. Our main contributions are as follows:
1. We present two representations for modeling real-valued combinatorial functions over categorical
variables, which we then use in order to learn a surrogate model for the generic BBO problem
and the design problem. The surrogate model is updated via a hedge algorithm where the basis
functions in our representations act as experts. The latter update happens once for every real
black-box evaluation. To the best of our knowledge, the representations and/or their use in
black-box optimization of functions over categorical variables are novel to this work.
2.	In the BBO problem, the proposed method uses a version of simulated annealing that utilizes
the current surrogate model for many internal cost-free evaluations before producing the next
black-box query.
3.	In the design problem, the proposed method uses a version of MCTS in conjunction with
the current surrogate model as reward function of the terminal states during intermediate tree
traversals/backups in order to improve the sample efficiency of the search algorithm.
4.	Numerical results, over synthetic benchmarks as well as real-world biological (RNA) sequence
optimization and design problems demonstrate the competitive or superior performance of the
proposed methods over state-of-the-art counterparts, while substantially reducing the computation
time and sample efficiency, respectively.
2	Related Work
Hutter et al. (2011) suggests a surrogate model based on random forests to address optimization
problems over categorical variables. The proposed SMAC algorithm uses a randomized local
search under the expected improvement acquisition criterion in order to obtain candidate points
for black-box evaluations. Bergstra et al. (2011) suggests a tree-structured Parzen estimator (TPE)
for approximating the surrogate model, and maximizes the expected improvement criterion to find
candidate points for evaluation. For optimization problems over Boolean variables, multilinear
polynomials Ricardo Baptista (2018); Dadkhahi et al. (2020) and Walsh functions Lepretre et al.
(2019) have been used in the literature.
Bayesian Optimization (BO) is a commonly used approach for optimization of black-box functions
(Shahriari et al. (2015)). However, limited work has addressed incorporation of categorical variables
in BO. Early attempts based on converting the black-box optimization problem over categorical
variables to that of continuous variables have not been very successful (Gdmez-Bombarelli et al.
(2018); Golovin et al. (2017); GarridO-MerChgn & Herndndez-Lobato (2020)). A few BO algorithms
have been specifically designed for black-box functions over combinatorial domains. In particular,
the BOCS algorithm Ricardo Baptista (2018), primarily devised for Boolean functions, employs a
sparse monomial representation to model the interactions among different variables, and uses a sparse
Bayesian linear regression method to learn the model coefficients. The COMBO algorithm of Oh
et al. (2019) uses Graph Fourier Transform (GFT) over a combinatorial graph, constructed via graph
cartesian product of variable subgraphs, to gauge the smoothness of the black-box function. However,
2
Under review as a conference paper at ICLR 2021
both BOCS and COMBO are hindered by associated high computational complexities, which grow
polynomially with both the number of variables and the number of function evaluations.
More recently, a computationally efficient black-box optimization algorithm (COMEX) (Dadkhahi
et al. (2020)) was introduced to address the computational impediments of its Bayesian counterparts.
COMEX adopts a Boolean Fourier representation as its surrogate model, which is updated via an
exponential weight update rule. Nevertheless, COMEX is limited to functions over the Boolean
hypercube. We generalize COMEX to handle functions over categorical variables by proposing
two representations for modeling functions over categorical variables: an abridged one-hot encoded
Boolean Fourier representation and Fourier representation on finite Abelian groups. The utilization
of the latter representation as a surrogate model in combinatorial optimization algorithms is novel to
this work. Factorizations based on one-hot encoding has been previously (albeit briefly) suggested in
Ricardo Baptista (2018) to enable black-box optimization algorithms designed for Boolean variables
to address problems over categorical variables. Different from Ricardo Baptista (2018), we show that
we can significantly reduce the number of additional variables introduced upon one-hot encoding,
and that such a reduced representation is in fact complete and unique.
For design problems, we focus on the RNA sequence design problem (RNA inverse folding). The
goal is to find an RNA sequence consistent with a given secondary structure, as the functional state
of the RNA molecule is determined by the latter structure (Hofacker et al. (1994)). Earlier RNA
design methods explore the search space by trial and error and use classic cost function minimization
approaches such as adaptive random walk (Hofacker (2003)), probabilistic sampling (Zadeh et al.
(2011)), and genetic algorithms (Taneda (2015)). Recent efforts employ more advanced machine
learning methods such as different Monte Carlo Tree Search (MCTS) algorithms, e.g. MCTS-RNA
(Yang et al. (2017)) or Nested MCTS (Portela (2018)), and reinforcement learning that either performs
a local search as in Eastman et al. (2018) or learns complete candidate solutions from scratch (Runge
et al. (2018)). In all these approaches, the assumption is that the algorithm has access to a large
number of function evaluations, whereas we are interested in sample efficiency of each algorithm.
As an alternative to parameter free search methods (such as SA), Swersky et al. (2020) suggests to
use a parameterized policy to generate candidates that maximize the acquisition function in Bayesian
optimization over discrete search spaces. Our MCTS acquisition method is similar in concept to
Swersky et al. (2020) in the sense that the tabular value functions are constructed and maintained
over different time steps. However, we are maintaining value functions rather than a policy network.
3	Black-Box Optimization over Categorical Variables
Problem Setting: Given the combinatorial categorical domain X = [k]n and a constraint set C ⊆ X,
with n variables each of cardinality k, the objective is to find
x* = arg min f (x)	(1)
x∈C
where f : X 7→ R is a real-valued combinatorial function. We assume that f is a black-box function,
which is computationally expensive to evaluate. As such, we are interested in finding x* in as few
evaluations as possible. We consider two variations of the problem depending on how the constraint
set C is specified.
Generic BBO Problem: In this case, the constraint set C = X . For example, RNA sequence
optimization problem that searches for an RNA sequence with a specific property optimized lies
within this category. A score for every RNA sequence, reflecting the property we wish to optimize, is
evaluated by a black box function.
Design Problem: The constraint set is complex and is only sequentially specified. For every
sequence of x1x2 . . . xi consisting of i characters from the alphabet [k], the choice of the next
character xi+1 ∈ C(x1x2 . . . xi) ⊆ [k] is specified by a constraint set function C(x1 . . . xi). The
RNA inverse folding problem in Runge et al. (2018) falls into this category, where the constraints on
the RNA sequence are determined by the sequential choices one makes during the sequence design.
The goal is to find the sequence that is optimal with respect to a pre-specified structure that also obeys
complex sequential constraints.
Our Techniques: In order to address this problem, we adopt a surrogate model-acquisition function
based learning framework, where an estimate for the black-box function f (i.e. the surrogate model)
3
Under review as a conference paper at ICLR 2021
is updated sequentially via black-box function evaluations observed until time step t. The selection
of candidate points for black-box function evaluation is carried out via an acquisition function, which
uses the surrogate model f as an inexpensive proxy (to make many internal calls) for the black-box
function and produces the next candidate point to be evaluated. The sequence proceeds as follows:
Surrogate model updated on (xt, f(xt)) → Acquisition function makes (many)
calls to Surrogate model to propose xt+1 → Surrogate model updated on (xt+1, f(xt+1))
In the sequel, we propose two representations that can be used as surrogate models for black-box
combinatorial functions over categorical variables. These representations serve as direct generaliza-
tions of the Boolean surrogate model based on Fourier expansion proposed in Dadkhahi et al. (2020)
in the sense that they reduce to the Fourier representation for real-valued Boolean functions when
the cardinality of the categorical variables is two. In addition, both approaches can be modified to
address the more general case where different variables are of different cardinalities. However, for
ease of exposition, we assume that all the variables are of the same cardinality. Finally, we introduce
two popular acquisition function to be used in conjunction with the proposed surrogate models in
order to propose new queries for subsequent black-box function evaluations.
3.1	Representations for the Surrogate Model
We present two representations for combinatorial functions f : [k]n → R and an algorithm to update
from the black-box evaluations. The representations use the Fourier basis in two different ways.
Abridged One-Hot Encoded Boolean Fourier Representation: The one-hot encoding of each
variable xi : i ∈ [n] can be expressed as a (k - 1)-tuple (xi1, xi2, . . . , xi(k-1)), where xij ∈ {-1, 1}
are Boolean variables with the constraint that at most one such variable can be equal to -1 for any
given xi ∈ [k].
We consider the following representation for the combinatorial function f :
n
fα (x) = Σ Σ Σ	αI,JψI,J(x)
(2)
m=0 I∈([n]) J∈[k-1]lIl
where [k - 1]|I| denotes ∣I∣-fold cartesian product of the set [k - 1] = {1,2,...,k - 1},(宁)
designates the set of m-subsets of the set [n], and the monomials ψI,J (x) can be written as
ψI,J(x) =	xij
{(i,j):i=I',j=J','E[III]}
(3)
A second order approximation (i.e. at m = 2) of the representation in (2) can be expanded in the
following way:
fα (x) = α0 + Σ Σ αi'Xi' + Σ Σ	αijpqxipxjq .	(4)
i∈[n] '∈[k-1]	(i,j)∈([2]) (P,q)∈[k-1]2
Example. For n = 2 variables x1 and x2, each of which with cardinality k = 3, we have the
one-hot encoding of (x11, x12) and (x21, x22) respectively. From Equation (4), the one-hot encoding
factorization for this example can be written as
f(x) = α0 + α1x11 + α2x12 +α3x21 + α4x22 + α5x11x21 +α6x11x22 + α7x12x21 + α8x12x22.
Note that the representation in Equation (2) has far less terms than a vanilla one-hot encoding with
all the combinations of one-hot variables included (as suggested in Ricardo Baptista (2018)). The
reason for this reduction is two-fold: (i) (k - 1) Boolean variables model each categorical variable
of cardinality k, and more importantly (ii) each monomial term has at most one Boolean variable xij
from its corresponding parent categorical variable xi . The following theorem states that this reduced
representation is in fact unique and complete.
Theorem 3.1. The representation in Equation (2) is complete and unique for any real-valued
combinatorial function.
4
Under review as a conference paper at ICLR 2021
Proof. See Appendix.	□
Fourier Representation on Finite Abelian Groups: We define a cyclic group structure Z/kiZ over
the elements of each categorical variable xi (i ∈ [n]), where ki is the cardinality of the latter variable.
From the fundamental theorem of abelian groups Terras (1999), there exists an abelian group G
which is isomorphic to the direct sum (a.k.a direct product) of the cyclic groups Z/kiZ corresponding
to the n categorical variables:
G = Z/kiZ ㊉ Z/k2Z ㊉...㊉ Z∕knZ	(5)
where the latter group consists of all vectors (a1,a2,...,an) such that a% ∈ Z∕k%Z and = denotes
group isomorphism. We assume that ki = k (∀i ∈ [n]) for simplicity, but the following representation
could be easily generalized to the case of arbitrary cardinalities for different variables.
The Fourier representation of any complex-valued function f(x) on the finite abelian group G is
given by Terras (1999)
f(x) = X αI ψI (x)	(6)
I∈[k]n
where αI are (in general complex) Fourier coefficients, [k]n is the n-fold cartesian product of the set
[k] and ψI(x) are complex exponentials 1 (k-th roots of unity) given by
ψI (x) = exp(2πj hx,Ii/k).
Note that the latter complex exponentials are the characters of the representation, and reduce to
the monomials (i.e. in {-1, 1}) when the cardinality of each variable is two. A second order
approximation of the representation in (6) can be written as:
fα (x) = α0 + Σ Σ α%' exp(2πjxi'∕k) + Σ Σ	αijpq exp(2πj(xip+xj q)/k). (7)
i∈[n] '∈[k-1]	(i,j)∈([2]) (p,q)∈[k-1]2
For a real-valued function fα(x) (which is of interest here), the representation in (6) reduces to
fα(x) = <	αI ψI (x) =	αr,I ψr,I (x) - Σ αi,I ψi,I (x)	(8)
I∈[k]n	I∈[k]n	I∈[k]n
where
ψr,ι (x) = cos(2πhx,1i∕k) and ψi,ι (x) = sin(2πhx,1i∕k)	(9)
αr,I = <{αI} and αi,I = ={αI}	(10)
We note that the number of characters utilized in this representation is almost twice as many as that
of monomials used in the previous representation.
Surrogate Model Learning: We adopt the learning algorithm of combinatorial optimization with
expert advice Dadkhahi et al. (2020) in the following way. We consider the monomials ψI,J (x) in
(3) and the characters ψ',ι(x) in (10) as experts. For each surrogate model, We maintain a pool of
such experts, the coefficients of which are refreshed sequentially via an exponential weight update
rule. We refer to the proposed algorithm as Expert-Based Categorical Optimization (ECO) and the
two versions of the algorithm with the two proposed surrogate models are called ECO-F (based on
the One-Hot Encoded Boolean Fourier Representation) and ECO-G (based on Fourier Representation
on Finite Abelian Groups), respectively. A summary of the algorithm is given in the Appendix.
3.2 Acquisition Functions
In this subsection, we discuss how two popular acquisition functions, namely Simulated Annealing
(SA) and Monte Carlo Tree Search (MCTS), work with a surrogate model and use cost-free evaluations
of the surrogate model to select the next query for the black box evaluation. In the literature, SA has
been used for the generic BBO problems whereas MCTS has been used for the design problems.
1 Note that in the general case of different cardinalities for different variables, I ∈ [k1] × [k2] × . . . × [kn]
where × denotes the cartesian product and the exponent denominator in the complex exponential character is
replaced by k = LCM(k1, k2, . . . , kn).
5
Under review as a conference paper at ICLR 2021
CA	A	♦	ɪl	C	∙	∙ . ∙	∕'	. ∙	∙	1	∙	1	.	♦♦♦*/、/
SA as Acquisition Function: Our acquisition function is devised so as to minimize fα (x), the
current estimate for the surrogate model. A simple strategy to minimize fα (x) is to iteratively switch
fbα given the values
each variable into the value that minimizes
of all the remaining variables, until
no more changes occur after a sweep through all the variables xi (∀i ∈ [n]). A strategy to escape
local minima in this context Pincus (1970) is to allow for occasional increases in fα by generating
a Markov Chain (MC) sample sequence (for x), whose stationary distribution is proportional to
exp(-fα(X)/s), where S is gradually reduced to zero. This optimization strategy was first applied by
Kirkpatrick et al. (1983) in their Simulated Annealing algorithm to solve combinatorial optimization
problems. We use the Gibbs sampler Geman & Geman (1984) to generate such a MC by sampling
from the full-conditional distribution of the stationary distribution, which in our case is given by the
SoftmaX distribution over {-fα(xi = ', χ-i)∕s}'∈[k], for each variable Xi conditional on the values
of the remaining variables x-i. By decreasing s from a high value to a low one, we allow the MC to
first search at a coarse level avoiding being trapped in local minima.
Algorithm 1 presents our simulated annealing (SA) version for categorical domains, where s(t) is an
annealing schedule, which is a non-increasing function of t. We use the annealing schedule suggested
in Spears (1993), which follows an exponential decay with parameter ' given by s(t) = exp(-'t∕n).
In each step of the algorithm, we pick a variable xi (i ∈ [n]) uniformly at random, evaluate the
surrogate model (possibly in parallel) k times, once for each categorical value ` ∈ [k] for the chosen
variable xi given the current values x-i for the remaining variables. We then update xi with the
sampled value in [k] from the corresponding softmax distribution.
MCTS as Acquisition Function: We formulate the design problem as an undiscounted Markov
decision process (S, A, T, R). Each state s ∈ S corresponds to a partial or full sequence of
categorical variables of lengths in [0, n]. The process in each episode starts with an empty sequence
s0, the initial state. Actions are selected from the set of permissible additions to the current state
(sequence) st at each time step t, At = A(st) ⊂ A. The transition function T is deterministic,
and defines the sequence obtained from the juxtaposition of the current state st with the action at,
i.e. st+1 = T (st, at) = st ◦ at. The transitions leading to incomplete sequences yield a reward
of zero. Complete sequences are considered as terminal states, from which no further transitions
(juxtapositions) can be made. Once the sequence is complete (i.e. at a terminal state), the reward
is obtained from the current surrogate reward model fbα . Thus, the reward function is defined as
R(st, at, st+1) = -fbα(st+1) if st+1 is terminal, and zero otherwise.
MCTS is a popular search algorithm used for design problems. MCTS is a rollout algorithm
which keeps track of the value estimates obtained via Monte Carlo simulations in order to pro-
gressively make better selections. The UCT selection criteria, see Kocsis & SZePeSVdri (2006),
is typically used as tree policy, where action at at state St in the tree search is selected via:
πτ(St) = arg maXa∈A(st) Q(st, a) + C∙∖∕ln N(st)∕Ν(st,a), where T is the search tree, C is the explo-
ration parameter, Q(S, a) is the state-action value estimate, and N(S) and N(S, a) are the visit counts
for the parent state node and the candidate state-action edge, respectively. For the selection of actions
RS
in states outside the tree search, a random default policy is used: π	(St ) = unif(At ).
,	tt
A summary of the proposed algorithm is given in Algorithm 2. Starting with an empty sequence S0 at
the root of the tree, we follow the tree policy until a leaf node of the search tree is reached (selection
step). At this point, we append the state corresponding to the leaf node to the tree and initialize a
value function estimate for its children (extension step). From the reached leaf node we follow the
default policy until a terminal state is reached. At this point, we plug the sequence corresponding to
this terminal state into the surrogate reward model -fα and observe the reward r. This reward is
backed up from the leaf node to the root of the tree in order to update the value estimates Q(S, a) via
Monte Carlo (i.e. using the average reward) for all visited (S, a) pairs along the path. This process is
repeated until a stopping criterion (typically a max number of playouts) is met, at which point the
sequence Sbest with maximum reward rbest is returned as the output of the algorithm.
6
Under review as a conference paper at ICLR 2021
Algorithm 1 SA for Categorical Variables with
Surrogate Model
1:	Inputs: surrogate model fbα, annealing
schedule s(t), categorical domain X
2:	Initialize x ∈ X
3:	t = 0
4:	repeat
5:	i〜 unif([n])
6：	Xi∣X-i	〜
Softmax({-fɑt (Xi=',χ-G∕s(t)}'∈[k])
7:	t — t + 1
8:	until Stopping Criteria
9:	return x
Algorithm 2 MCTS with Surrogate Reward
1:	Inputs: surrogate model fα , search tree T
2:	Initialize sbest = {}, rbest = -∞
3:	repeat
4:	Sleaf J SeIection(πT)
5:	TJT∪{sleaf}
6:	st J Simulation(πRS, sleaf)
7:	r J -fbα (st )
8:	Backup(sleaf, r)
9:	if r > rbest then
10:	rbest J r and sbest J st
11:	end if
12:	until Stopping Criteria
13:	return sbest
Computational Complexity: The computational complexity per time step associated with learning
the surrogate model, for both representations introduced in 3.1, is in O(d) = O(km-1nm), and
is thus linear in the number of experts d. Moreover, the complexity of the simulated annealing
algorithm (Algorithm 1) is in O(kkm-1nm-1n) = O(kd), assuming that the number of iterations in
each SA run is in O(n). As a result, the overall complexity of the algorithm is in O(kd). Finally,
the computational complexity of each playout in Algorithm 2 is in (O)(kn), leading to an overall
complexity of (O)(kd), assuming (O)(d/n) playouts per time step.
4	Experiments and Results
In this section, we measure the performance of the proposed representations, when used as sur-
rogate/reward model in conjunction with search algorithms (SA and MCTS) in BBO and design
problems. The learning rate used in exponential weight updates is selected via the anytime learning
rate schedule suggested in Dadkhahi et al. (2020) and Gerchinovitz & Yu (2011) (see Appendix).
The maximum degree of interactions used in our surrogate models is set to two for all the problems;
increasing the max order improved the results only marginally. The sparsity parameter λ in exponen-
tial weight updates is set to 1 in all the experiments following the same choice made in Dadkhahi
et al. (2020). Experimentally, the learning algorithm is fairly insensitive to the variations in the latter
parameter. In each experiment, we report the results averaged over multiple runs (20 runs in BBO
experiments and 10 runs in design experiments) ± one standard error of the mean. The experiments
were run on machines with CPU cores from the Intel Xeon E5-2600 v3 family.
BBO Experiments: We compare the performance of our ECO algorithms in conjunction with SA
with two baselines, random search (RS) and simulated annealing (SA), as well as a state-of-the-art
Bayesian combinatorial optimization algorithm (COMBO) Oh et al. (2019). In particular, we consider
two synthetic benchmarks (Latin square problem and pest control problem) and a real-word sequence
design problem in biology: RNA sequence optimization. In addition to the performance of the
algorithms in terms of the best value of f(x) observed until a given time step t, we measure the
average computation time per time step of our algorithm versus that of COMBO. The decay parameter
used in the annealing schedule of SA is set to ` = 3 in all the experiments. In addition, the number of
SA iterations in our surrogate models is set to T = 3 × n. Intuitively, each of these parameters creates
an exploration-exploitation trade-off. The smaller (larger) the value of ` or T , the more exploratory
(exploitative) is the behavior of SA. The selected values seem to create a reasonable balance; tuning
these parameters may improve the performance of the acquisition function.
Synthetic Benchmarks: We consider two synthetic problems: Latin square problem Colbourn &
Dinitz (2006), a commonly used combinatorial optimization benchmark, and the pest control problem
considered in Oh et al. (2019) (see Appendix for the latter results). In both problems, we have n = 25
categorical variables, each of cardinality k = 5. A Latin square of order k is a k × k matrix of
elements xij ∈ [k], such that each number appears in each row and column exactly once. When
k = 5, the problem of finding a Latin square has 161, 280 solutions in a space of dimensionality
525. We formulate the problem of finding a Latin square of order k as a black-box optimization by
7
Under review as a conference paper at ICLR 2021
---RS ----- SA ---- COMBO ---- ECO-F ---- ECO-G
---RS ------- MCTS ----- LEARNA ----- ECO-F ----- ECO-G
0	100	200	300	400	500
Time Step
Figure 1: RNA BBO Problem with n = 30 Figure 2: Eterna100 puzzle #41 (n = 35)
imposing an additive penalty of one for any repetition of numbers in any row or column. Hence,
function evaluations are in the range [0, 2k(k - 1)], and a function evaluation of zero corresponds to
a Latin square of order k. We consider a noisy version of this problem, where an additive Gaussian
noise with zero mean and standard deviation of 0.1 is added to function evaluations observed by each
algorithm. Both ECO-F and ECO-G outperform the baselines with a considerable margin. In addition,
ECO-G outperforms COMBO until time step t = 190. At larger time steps, COMBO outperforms
the other algorithms, however, this performance comes at the price of a far larger computation time.
ECO-F and ECO-G offer a speed-up over COMBO by a factor of roughly 100 and 50, respectively.
RNA Sequence Optimization Problem: Consider an RNA sequence as a string A = a1 . . . an of n
letters (nucleotides) over the alphabet Σ = {A, U, G, C}. A pair of complementary nucleotides ai
and aj, where (i < j), can interact with each other and form a base pair (denoted by (i, j)), A-U,
C-G and G-U being the energetically stable pairs. Thus, the secondary structure, i.e. the minimum
free energy structure, of an RNA can be represented by an ensemble of pairing bases. A number of
RNA folding algorithms Lorenz et al. (2011); Markham & Zuker (2008) use a thermodynamic model
(e.g. Zuker & Stiegler (1981)) and dynamic programming to estimate MFE of a sequence. However,
the O(n3) time complexity of these algorithms prohibits their use for evaluating substantial numbers
of RNA sequences Gould et al. (2014) and exhaustively searching the space to identify the global
free energy minimum, as the number of sequences grows exponentially as 4n .
We formulate the RNA sequence optimization problem as follows: For a sequence of length n, find
the RNA sequence which folds into a secondary structure with the lowest MFE. In our experiments,
we initially set n = 30 and k = 4. We then use the popular RNAfold package Lorenz et al. (2011) to
evaluate the MFE for a given sequence. The goal is to find the lowest MFE sequence by calling the
MFE evaluator minimum number of times. As depicted in Figure 1, both ECO-F and particularly
ECO-G outperform the baselines as well as COMBO by a considerable margin.
RNA Design Experiments: The problem is to find a primary RNA sequence φ which folds into
a target structure w, given a folding algorithm F . Such target structures can be represented as
a sequence of dots (for unpaired bases) and brackets (for paired bases). In our algorithm, the
action sets are defined as follows. For unpaired sites At = {A, G, C, U} and for paired sites
At = {GC, CG, AU, UA}. At the beginning of each run of our algorithm (ECO-F and ECO-G in
conjunction with MCTS acquisition), we draw a random permutation for the order of locations to be
selected in each level of the tree. The reward value offered by the environment (i.e. the black-box
function) at any time step t corresponds to the normalized Hamming distance between the target
structure ω and the structure yt = F(xt) of the sequence xt found by each algorithm, i.e. dH (w, yt).
We compare the performance of our algorithms against RS as a baseline, where random search is
carried out over the given structure (i.e. default policy πRS) rather than over unstructured random
sequences. We also include two state-of-the-art algorithms in our experiments: MCTS-RNA of Yang
et al. (2017) and LEARNA of Runge et al. (2019). MCTS-RNA has an exploration parameter, which
we tune in advance (per sequence). LEARNA has a set of 14 hyper-parameters tuned a priori using
training data and is provided by the authors of Runge et al. (2019). Note that the latter training phase
(for LEARNA) as well as the former exploration parameter tuning (for MCTS-RNA) is offered to
the respective algorithms as an advantage, whereas for our algorithm we use a global set of heuristic
choices for the two hyper-parameters, rather than attempting to tune the two hyper-parameters. In
particular, we set the exploration parameter c to 0.5 and the number of MCTS playouts at each time
8
Under review as a conference paper at ICLR 2021
step to 30 × h, where h is the height of the tree (i.e. number of dots and bracket pairs). The latter
heuristic choice is made since the bigger the tree, the more playouts are needed to explore the space.
We point out that the entire design pipeline in state-of-the-art algorithms typically also includes a
local improvement step (as a post-processing step), which is either a rule-based search (e.g. in Yang
et al. (2017)) or an exhaustive search (e.g. in Runge et al. (2019)) over the mismatched sites. We
do not include the local improvement step in our experiments, since we are interested in measuring
sample efficiency of different algorithms. In other words, the question is the following: given a fixed
and finite evaluation budget, which algorithm is able to get closer to the target structure.
For our experiments, we focus on three puzzles from the Eterna-100 dataset Anderson-Lee et al.
(2016). Two of the selected puzzles (#15 and #41 of lengths 30 and 35, resp.), despite their fairly
small lengths, are challenging for many algorithms (see Anderson-Lee et al. (2016)). In both puzzles,
our algorithms ECO-F and ECO-G (with MCTS acquisition) are able to significantly improve the
performance of MCTS when limited number of black-box evaluations is available. All algorithms
outperformed RS as expected. Within the given 500 evaluation budget, ECO-G, and especially
ECO-F, are superior to LEARNA by a substantial margin (see Appendix). In puzzle number 41
(Figure 2), again both ECO-G and ECO-F significantly outperform LEARNA, over the given number
of evaluations. Interestingly, ECO-F is able to outperform LEARNA throughout the evaluation
process, and in average finds a far better final solution than LEARNA. See Appendix for the third
puzzle.
5	Conclusions and Future Work
In summary, we propose novel Fourier representations as surrogate models for black box optimiza-
tion over categorical variables and show performance improvements over existing baselines when
combined with state-of-the-art acquisition methods.
Considering the performance variability of the two surrogate model representations introduced in this
paper across different problems, an important research avenue would be to incorporate an ensemble
of surrogate models rather than a single one. Such an ensemble model would then update and explore
both models simultaneously and draw samples from either individual models or a combination of
both at any given time step. It would be interesting to see if such an ensemble model would in fact be
able to outperform both individual models over different combinatorial problems.
Our ECO algorithm incorporates an online estimator learnt via Hedge, rather than a Bayesian posterior
mean function which is commonly used in conjunction with Thompson Sampling (TS) and UCB
for uncertainty quantification in Bayesian optimization algorithms. The Hedge algorithm has strong
adversarial guarantees (see Dadkhahi et al. (2020) for theoretical results in the Boolean case), which
can be shown to carry over to our setting as well. More precisely, given any additional black box
evaluation, it is guaranteed to move closer to the true black-box model. However, the exploration
bonus used in the acquisition process via SA, used in our algorithm, can be shown to be domain
independent, i.e. sampled i.i.d from the same distribution regardless of the query point. The terms
that account for uncertainty in both TS and UCB are domain dependent and depend on the query
point. As such, domain dependent uncertainty incorporation would be an interesting next step, which
is left for future work.
References
Jeff Anderson-Lee, Eli Fisker, Vineet Kosaraju, Michelle Wu, Justin Kong, Jeehyung Lee, Minjae Lee,
Mathew Zada, Adrien Treuille, and Rhiju Das. Principles for predicting rna secondary structure
design difficulty. Journal ofMolecular Biology, 428(5, Part A):748 - 757, 2016. Challenges in
RNA Structural Modeling and Design.
Frances H Arnold. Design by directed evolution. Accounts of chemical research, 31(3):125-131,
1998.
James Bergstra, Remi Bardenet, Yoshua Bengio, and Balazs Kegl. Algorithms for hyper-parameter op-
timization. In Proceedings of the 24th International Conference on Neural Information Processing
Systems, NIPS’11, pp. 2546-2554, 2011.
9
Under review as a conference paper at ICLR 2021
Nicholas Bogard, Johannes Linder, Alexander B Rosenberg, and Georg Seelig. A deep neural network
for predicting and engineering alternative polyadenylation. Cell, 178(1):91-106, 2019.
David H Brookes, Hahnbeom Park, and Jennifer Listgarten. Conditioning by adaptive sampling for
robust design. arXiv preprint arXiv:1901.10060, 2019.
J Ross Buchan and Ian Stansfield. Halting a cellular production line: responses to ribosomal pausing
during translation. Biology of the Cell, 99(9):475-487, 2007.
Charles J. Colbourn and Jeffrey H. Dinitz. Handbook of Combinatorial Designs, Second Edition
(Discrete Mathematics and Its Applications). Chapman & Hall/CRC, 2006.
Hamid Dadkhahi, Karthikeyan Shanmugam, Jesus Rios, Payel Das, Samuel Hoffman, Troy David
Loeffler, and Subramanian Sankaranarayanan. Combinatorial black-box optimization with expert
advice. In KDD, 2020.
Matthew Davis, Selena M Sagan, John P Pezacki, David J Evans, and Peter Simmonds. Bioinformatic
and physical characterizations of genome-scale ordered rna structure in mammalian rna viruses.
Journal of virology, 82(23):11824-11836, 2008.
Ken A Dill and Justin L MacCallum. The protein-folding problem, 50 years on. science, 338(6110):
1042-1046, 2012.
Neil Dixon, John N Duncan, Torsten Geerlings, Mark S Dunstan, John EG McCarthy, David Leys,
and Jason Micklefield. Reengineering orthogonally selective riboswitches. Proceedings of the
National Academy of Sciences, 107(7):2830-2835, 2010.
Peter Eastman, Jade Shi, Bharath Ramsundar, and Vijay S Pande. Solving the rna design problem
with reinforcement learning. PLoS computational biology, 14(6):e1006176, 2018.
Eduardo C GarridO-MerChdn and Daniel Herndndez-Lobato. Dealing with categorical and integer-
valued variables in bayesian optimization with gaussian processes. Neurocomputing, 380:20-35,
2020.
Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Transactions on pattern analysis and machine intelligence, 6:721-741,
1984.
Sebastien Gerchinovitz and Jia Yuan Yu. Adaptive and optimal online linear regression on '1-balls.
In Algorithmic Learning Theory, pp. 99-113. Springer Berlin Heidelberg, 2011.
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Sculley.
Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD
international conference on knowledge discovery and data mining, pp. 1487-1495, 2017.
Rafael Gdmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos6 Miguel Herndndez-Lobato,
Benjamin Sdnchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Aldn Aspuru-Guzik. Automatic chemical design using a data-driven continuous
representation of molecules. ACS central science, 4(2):268-276, 2018.
Nathan Gould, Oliver Hendy, and Dimitris Papamichail. Computational tools and algorithms for
designing customized synthetic genes. Frontiers in bioengineering and biotechnology, 2:41, 2014.
Anvita Gupta and James Zou. Feedback gan for dna optimizes protein functions. Nature Machine
Intelligence, 1(2):105-111, 2019.
Ivo L Hofacker. Vienna rna secondary structure server. Nucleic acids research, 31(13):3429-3431,
2003.
Ivo L Hofacker, Walter Fontana, Peter F Stadler, L Sebastian Bonhoeffer, Manfred Tacker, and
Peter Schuster. Fast folding and comparison of rna secondary structures. Monatshefte fur
Chemie/Chemical Monthly, 125(2):167-188, 1994.
John H Holland and Judith S Reitman. Cognitive systems based on adaptive algorithms. In Pattern-
directed inference systems, pp. 313-329. Elsevier, 1978.
10
Under review as a conference paper at ICLR 2021
Shuichi Hoshika, Nicole A Leal, Myong-Jung Kim, Myong-Sang Kim, Nilesh B Karalkar, Hyo-Joong
Kim, Alison M Bates, Norman E Watkins, Holly A SantaLucia, Adam J Meyer, et al. Hachimoji
dna and rna: A genetic system with eight building blocks. Science, 363(6429):884-887, 2019.
Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for
general algorithm configuration. In Proceedings of the 5th International Conference on Learning
and Intelligent Optimization, LION’05, pp. 507-523, 2011.
J. Kennedy and R. Eberhart. Particle swarm optimization. In Proceedings of ICNN’95 - International
Conference on Neural Networks, volume 4, pp. 1942-1948 vol.4, 1995.
Nathan Killoran, Leo J Lee, Andrew Delong, David Duvenaud, and Brendan J Frey. Generating and
designing dna with deep generative models. arXiv preprint arXiv:1712.06148, 2017.
S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi. Optimization by simulated annealing. Science, 220
(4598):671-680, 1983.
Levente Kocsis and Csaba Szepesvdri. Bandit based monte-carlo planning. In Proceedings of the
17th European Conference on Machine Learning, ECML’06, pp. 282-293. Springer-Verlag, 2006.
Florian Lepretre, SebaStien Verel, Cyril Fonlupt, and Virginie Marion. Walsh functions as surrogate
model for pseudo-boolean optimization problems. In The Genetic and Evolutionary Computation
Conference (GECCO 2019), Proceedings of the Genetic and Evolutionary Computation Conference,
pp. 303-311. ACM, 2019.
Hui Li, Taek Lee, Thomas Dziubla, Fengmei Pi, Sijin Guo, Jing Xu, Chan Li, Farzin Haque, Xing-Jie
Liang, and Peixuan Guo. Rna as a stable polymer to build controllable and defined nanostructures
for material and biomedical applications. Nano today, 10(5):631-655, 2015.
Johannes Linder and Georg Seelig. Fast differentiable DNA and protein sequence optimization for
molecular design. Available at: https://arxiv.org/pdf/2005.11275, 2020.
Ge Liu, Haoyang Zeng, Jonas Mueller, Brandon Carter, Ziheng Wang, Jonas Schilz, Geraldine Horny,
Michael E Birnbaum, Stefan Ewert, and David K Gifford. Antibody complementarity determining
region design using high-capacity machine learning. Bioinformatics, 36(7):2126-2133, 2020.
Ronny Lorenz, Stephan H. Bernhart, Christian Honer ZU Siederdissen, Hakim Tafer, Christoph
Flamm, Peter F. Stadler, and Ivo L. Hofacker. Viennarna package 2.0. Algorithms for Molecular
Biology, 6(1):26, 2011.
Nicholas R Markham and Michael Zuker. Unafold. In Bioinformatics, pp. 3-31. Springer, 2008.
Andrew H Ng, Taylor H Nguyen, Mariana Gdmez-Schiavon, Galen Dods, Robert A Langan, Scott E
Boyken, Jennifer A Samson, Lucas M Waldburger, John E Dueber, David Baker, et al. Modular and
tunable biological feedback control using a de novo protein switch. Nature, 572(7768):265-269,
2019.
Changyong Oh, Jakub Tomczak, Efstratios Gavves, and Max Welling. Combinatorial bayesian
optimization using the graph cartesian product. In Advances in Neural Information Processing
Systems 32, pp. 2910-2920. Curran Associates, Inc., 2019.
Martin Pincus. Letter to the editor—a monte carlo method for the approximate solution of certain
types of constrained optimization problems. Operations Research, 18(6):1225-1228, 1970.
Fernando Portela. An unexpectedly effective monte carlo technique for the rna inverse folding
problem. BioRxiv, pp. 345587, 2018.
Matthias Poloczek Ricardo Baptista. Bayesian optimization of combinatorial structures. In ICML,
2018.
Frederic Runge, Danny Stoll, Stefan Falkner, and Frank Hutter. Learning to design rna. arXiv preprint
arXiv:1812.11951, 2018.
Frederic Runge, Danny Stoll, Stefan Falkner, and Frank Hutter. Learning to design RNA. In
International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2021
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):
148-175, 2015.
William M. Spears. Simulated annealing for hard satisfiability problems. In DIMACS Workshop:
Cliques, Coloring, and Satisfiability, pp. 533-557, 1993.
Zachary D Stephens, Skylar Y Lee, Faraz Faghri, Roy H Campbell, Chengxiang Zhai, Miles J Efron,
Ravishankar Iyer, Michael C Schatz, Saurabh Sinha, and Gene E Robinson. Big data: astronomical
or genomical? PLoS biology, 13(7):e1002195, 2015.
Kevin Swersky, Yulia Rubanova, David Dohan, and Kevin Murphy. Amortized bayesian optimization
over discrete spaces. In Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
(UAI), volume 124, pp. 769-778, 2020.
Akito Taneda. Multi-objective optimization for rna design with multiple target secondary structures.
BMC bioinformatics, 16(1):280, 2015.
Audrey Terras. Fourier Analysis on Finite Groups and Applications. London Mathematical Society
Student Texts. Cambridge University Press, 1999.
Edoardo Trotta. On the normalization of the minimum free energy of rnas by sequence length. PloS
one, 9(11), 2014.
Ryota Yamagami, Mohammad Kayedkhordeh, David H Mathews, and Philip C Bevilacqua. Design
of highly active double-pseudoknotted ribozymes: a combined computational and experimental
study. Nucleic acids research, 47(1):29-42, 2019.
Xiufeng Yang, Kazuki Yoshizoe, Akito Taneda, and Koji Tsuda. Rna inverse folding using monte
carlo tree search. BMC Bioinformatics, 8:468, 2017.
Joseph N Zadeh, Conrad D Steenberg, Justin S Bois, Brian R Wolfe, Marshall B Pierce, Asif R
Khan, Robert M Dirks, and Niles A Pierce. Nupack: Analysis and design of nucleic acid systems.
Journal of computational chemistry, 32(1):170-173, 2011.
Michael Zuker and Patrick Stiegler. Optimal computer folding of large rna sequences using thermo-
dynamics and auxiliary information. Nucleic acids research, 9(1):133-148, 1981.
12
Under review as a conference paper at ICLR 2021
A	Proof of Theorem 3.1
We first assume that the one-hot variables xij ∈ {0, 1}. Plugging different choices of x ∈ X = [k]n
into Equation (2) leads to a system of linear equations with kn unknowns (coefficients αI,J) and
kn equations. We can express this system in matrix form as the product of the matrix of monomials
(where each column j corresponds to a monomial ψj , and each element (i, j) corresponds to the
evaluation of the monomial ψj at the i-th choice for x) and the vector of unknown coefficients, which
is set equal to the function values at the corresponding choices for x. We claim that there exists a
permutation of the rows and columns of the matrix of monomials such that the latter becomes unit
lower triangular, and is thereby full rank. As a result, the representation in (2) for xij ∈ {0, 1} is
complete and unique.
To formally show that this permutation for the monomials’ matrix exists, we use a construction by
induction over the number of variables n included in the representation. We denote the monomials’
matrix over ' variables with Φ', and define the one-hot variables Xij (j ∈ [k - 1]) as the descendants
of the parent categorical variable xi (∀i ∈ [n]). It is easy to see that such a construction for the base
case of only one variable exits, as the monomials’ matrix is a k × k matrix. In this case, we use
the following permutation of the monomials (columns): (1, x11, x12, . . . , x1(k-1)). We also use the
following permutation of the x1 values in rows: (k, 1, . . . , k - 1). As a result, in this matrix only the
elements of the first column and the main diagonal are non-zero and equal to one, and thus the matrix
Φ1 is unit lower triangular.
Assuming that the induction hypothesis holds for n variables, we show that it also holds for n + 1
variables. Starting from a unit lower triangular matrix Φn, we can construct the matrix Φn+1
as follows. Note that all the kn columns of Φn correspond to the monomials composed of the
descendants of the first n variables {xij : ∀i ∈ [n] and ∀j ∈ [k - 1]}, whereas each of the additional
kn+1 - kn columns introduced in Φn+1 involves exactly one term from {x(n+1)j : ∀j ∈ [k - 1]}
(possibly also containing factors from the previous n variables). We can express kn+1 - kn using the
following binomial expansion:
kn+1 -kn = (k - 1) nX+1	n (k - 1)m-1.	(11)
m=1
In words, the additional kn+1 - kn columns can be considered as a collection of mn-1 (k - 1)m-1
m-th order monomials (m ∈ [n + 1]), each of which includes one out of the k - 1 descendant
variables x(n+1)j (j ∈ [k - 1]) together with m - 1 variables from the (descendants) of the previous
n variables. Each of the latter m - 1 variables can take values in [k - 1], whereas the remaining
n - m + 1 variables are set to k.
Starting with m = 1, we have (k - 1) first order terms (x(n+1)1, x(n+1)2, . . . , x(n+1)(k-1)) which
we assign to columns (kn+ 1, . . . , kn+ (k- 1)). The x values associated with rows (kn+ 1, . . . , kn+
(k - 1)) are constructed by assuming that (i) xn+1 takes values from 1 to k - 1 (in order), while (ii)
all the remaining xj (j ∈ [n]) variables are set to k. As a consequence of (i), all the elements on the
main diagonal are ones; also, as a consequence of (ii), all the higher degree monomials involving
xn+1 (which occupy the elements after the diagonal ones) are equal to zeros. Thus the augmented
matrix, until this point, remains unit lower triangular.
We then consider the second degree terms m = 2, where we have n(k - 1) terms (choice of one out
of the other n variables, each taking values from [k - 1]) for each of the k - 1 choices for the variable
xn+1. Starting with second order monomials involving x1 and xn+1, we again assume that all the
remaining variables xi (i 6∈ {1, n + 1}) are equal2 to k. For any choice of (x1, xn+1) ∈ [k - 1]2,
we add a new column corresponding to the monomial xjx(n+i)' as well as a new row in which
xi = j and xn+1 = `, whereas the remaining variables are set to k. As a result, we have that: (i)
the diagonal element in the new row/column is equal to one, and (ii) all the elements in the future3
columns are zeros, since any combination of one of the descendants of xn+1 with the remaining
2Note that this assumption is necessary in order to ensure that monomials in future columns for the same row
are evaluated to zero; choices of x where this assumption is not valid is addressed in next rows.
3 Note that the elements in the previous columns in the same row corresponding to monomials involving the
selected m - 1 variables are non-zeros as well.
13
Under review as a conference paper at ICLR 2021
variables (any variable except x1 and xn+1) is zero. We continue this construction strategy for the
remaining variables until all the second degree terms involving xn+1 and one out of the remaining
n - 1 variables is exhausted. We then repeat the same idea for terms with orders up to n + 1, as
defined by the binomial expansion in (11). As a result of this construction strategy, the monomial
matrix Φn+1 is unit lower triangular.
Now, we use this result to show the completeness and uniqueness of the representation with one-hot
variables in {-1, 1} in the following way.
Completeness: We showed that the representation (2) over {0, 1} is complete, i.e. we can express
any function using the representation in (2), where we have at most one descendant term from the
same parent in each monomial. Now, we replace each xij (from {0, 1}) in the latter representation
with (1-x0ij)/2, where x0ij ∈ {1, -1}. The new representation can also be expressed via the expansion
(2) since no two descendants from the same parent variable are being multiplied with each another.
Uniqueness: Assume that the uniqueness condition is not satisfied. Then we have two distinct
polynomial representations f1 and f2 that have the same value for every x. However, since f1 and
f2 are distinct polynomials, f1(x) - f2(x) is a polynomial p(x) which is non-zero in at least one
input x*. This implies that fι(x*) - f2(x*) is also non-zero, which is a contradiction, and the proof
is complete.
Remark 1. The group-theoretic Fourier representation defined in Equation (6) is unique and com-
plete.
Proof. Let χ = [k]n be the categorical domain. Let the true function be f. For generality, let us
consider a complex valued function f : χ → C where C is the field of complex numbers. The
basis functions are ψI (x). Now, one can view a function as a [k]n-length vector, one entry each for
evaluating the function at every point in the domain χ. We denote the vector for function f, thus
obtained, by fχ ∈ Ckn. Similarly, denote the vector for evaluations of the basis function ψI by
ψIχ ∈ Ckn. Let A be a matrix created by stacking all vectors corresponding to basis vectors in the
columns. Then, the Fourier representation can be written as fχ = Aα where α is the vector of Fourier
coefficients in our group-theoretic representation. Now, due to the use of complex exponentials, one
can show that Px∈[k]n ψI(x)ψI0 (x) = 0 if I 6= I0. Therefore, the columns of the matrix A are
orthogonal. Hence, A is a full rank matrix. Therefore, our representation is merely representing a
vector in another full rank orthogonal basis. Hence, it is unique and complete.	□
B Description of Algorithms
Surrogate Model Learning Algorithm: Let n and k denote the number of variables and the cardi-
nality of each variable, respectively. The surrogate models used in ECO-F and ECO-G correspond to
approximations of the representations given in (2) and (8), respectively, where each approximation
is obtained by restricting the maximum order of interactions among variables to m. We consider
each term in the latter surrogate models, i.e. monomials ψI,J from (3) in ECO-F and characters
ψβ,I (β ∈ {r, i}) from (10) in ECO-G, as an expert, denoted by ψi (i ∈ [d]). The number of such
experts in ECO-F is d = Pim=0 ni (k - 1)i which coincides with the dimensionality of the space kn
when m = n, whereas the number of experts in ECO-G is equal to d = 2 Pim=0 ni (k - 1)i - 1. The
coefficient of each expert ψi is designated by αi . Since the exponential weights, utilized to update
the coefficients αi, are non-negative, we maintain two non-negative coefficients αi+ and αi-, which
yield αi = αi+ - αi-.
We initialize all the coefficients with a uniform prior, i.e. αiγ = 1/2d (∀i ∈ [d] andγ ∈ {-, +}). In
each time step t, we draw a sample xt via Algorithm 1 with respect to our current estimate for the
surrogate model fbα . The latter sample is
then plugged into the black-box function to obtain the
evaluation f(xt). This leads to a mixture loss `t as the difference between the evaluations obtained
by our surrogate model and the black-box function for query xt . Using this mixture loss, we compute
the individual loss `it for each expert ψi . Finally, we update each coefficient in the model via an
exponential weight obtained according to its incurred individual loss. We repeat this process until
stopping criteria are met.
14
Under review as a conference paper at ICLR 2021
Algorithm 3 Expert Categorical Optimization
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
Inputs: sparsity λ, max model order m
t J 0, ∀γ ∈ {-, +} ∀i ∈ [d] : αt,γ J 21d
repeat
^
Xt 〜fat via Algorithm 1 or Algorithm 2
Observe f(xt)
tt
fαt (x) J i∈[d] αit,+ - αit,- ψi(x)
't+1 J fat (Xt) — f (Xt)
for i ∈ [d] and γ ∈ {-, +} do
't+1 J 2 λ't+1 ψi(xt)
αt+γ1 J αt,γ eχp(- γηt 't+1)
t+1
i,γ
t+1
i,γ
end for
Σμ∈{-, + } Σj∈[d]
t+1
j,μ
α
J λ ∙
α
α
13:	t Jt+1
14:	until Stopping Criteria
15:	return b* = argmin{χi: ∀i∈[t]} f(xi)
Number of Experts: The number of terms in vanilla one-hot encoded Fourier representation is 2kn,
whereas our abridged representation reduces this number to kn matching the space dimensionality,
thereby making the algorithm computationally tractable and efficient. When a max degree of m is
used in the approximate representation, the number of terms in the abridged representation is equal to
d = Pim=0 ni (k - 1)i. The corresponding number in a vanilla one-hot encoded representation is
equal to Pim=0 nik . Finally, the numbers of terms in the full and order-m group-theoretic Fourier
expansions are equal to 2kn - 1 and d = 2 Pim=0 ni (k - 1)i - 1, respectively.
MCTS Algorithm: For a complete version of Algorithm 2, see Algorithm 4.
Learning Rate: The anytime learning rate (at time step t) used in Algorithm 3 is given by Gerchi-
novitz & Yu (2011); Dadkhahi et al. (2020):
ηt = min
ln(2 d)]
vt-1 /
(12)
where C δ {i2(√ 一 1)∕(exp(1) — 2) and
et
=∆ inf 2k : 2k ≥ max
k∈Z
s∈[t]
,ma XJzYs-zμ∕
γ,μ∈{-,+}
vt
=∆ X X	αjγ,s	zjγ,s
s∈[t]	j∈[d]
γ∈{-,+}
E	αμ,szμ,s
k∈[d]
μ∈{-,+}
—
2
C Continued Related Work
A variety of discrete search algorithms and meta-heuristics have been studied in the literature
for combinatorial optimization over categorical variables. Such algorithms, including Genetic
Algorithms Holland & Reitman (1978), Simulated Annealing Spears (1993), and Particle Swarms
Kennedy & Eberhart (1995), are generally inefficient in finding the global minima. In the context
of biological sequence optimization, the most popular method is directed evolution Arnold (1998),
which explores the space by only making small mutations to existing sequences. In the context of
sequence optimization, a recent promising approach consists of fitting a neural network model to
predict the black box function and then applying gradient ascent on the latter model Killoran et al.
15
Under review as a conference paper at ICLR 2021
Algorithm 4 MCTS with Surrogate Reward Model
1: 2: 3: 4: 5: 6: 7: 8: 9:	Inputs: surrogate reward model fbα, exploration parameter c, search tree T sbest = {}, rbest = -∞ repeat Initialize episode t = 0, st = [] while st ∈ T do		 at — ∏T(St) = argmaXa∈At Q(st, a) + C√lnN(st)∕N(st,a) St+1 - T(st, at) = St ◦ at t V- t + 1 end while
10:	sleaf = st
11: 12:	TV T∪{st} ∀a ∈ At+1 : N(st, a) = 0, Q(st, a) = 0
13:	repeat
14:	at V πRS(st)
15:	st+1 V T(st, at) = st ◦ at
16:	tV t+1
17:	until st is terminal
18:	r V -fbα (st )
19:	s V sleaf
20:	repeat
21:	N(s, a) V N(s, a) + 1
22:	Q(s, a) - Q(s, a) + Nsa)(T - Q(s, a))
23:	s V parent(s); a V visited action on s
24:	until s is the root node
25:	if r > rbest then
26:	rbest V r and sbest V st
27:	end if
28:	until Stopping Criteria
29:	return sbest
16
Under review as a conference paper at ICLR 2021
(2017); Bogard et al. (2019); Liu et al. (2020). This approach allows for a continuous relaxation of the
discrete search space making possible step-wise local improvements to the whole sequence at once
based on a gradient direction. However, these methods have been shown to suffer from vanishing
gradients Linder & Seelig (2020). Further, the projected sequences in the continuous relaxation
space may not be recognized by the predictors, leading to poor convergence. Generative model-based
optimization approaches aim to learn distributions whose expectation coincides with evaluations
of the black box and try to maximize such expectation Gupta & Zou (2019); Brookes et al. (2019).
However, such approaches require a pre-trained generative model for optimization.
D BBO Experiments
Latin Square Problem: A latin square of order k is a k × k matrix of elements xij ∈ [k], such that
each number appears in each row and column exactly once. When k = 5, the problem of finding a
latin square has 161, 280 solutions in a space of dimensionality 525. We formulate the problem of
finding a latin square of order k as a black-box function by imposing an additive penalty of one for
any repetition of numbers in any row or column. As a result, function evaluations are in the range
[0, 2k(k - 1)], and a function evaluation of zero corresponds to a latin square of order k. We consider
a noisy version of this problem, where an additive Gaussian noise with zero mean and standard
deviation of σ = 0.1 is added to function evaluations observed by each algorithm.
Figure 3 demonstrates the performance of different algorithms, in terms of the best function value
found until time t, over 500 time steps. Both ECO-F and ECO-G outperform the baselines with a
considerable margin. In addition, both ECO-G and ECO-F match COMBO’s performance closely
until time step t = 190. At larger time steps, COMBO outperforms the other algorithms, however,
this performance comes at the price of a far larger computation time. As demonstrated in Table
1, ECO-F and ECO-G offer a speed-up over COMBO by a factor of approximately 100 and 50,
respectively.
10.0
7.5
——RS ——SA	——ɔ COMBO ——ECO-F ——ECO-G
17.5
15.0
12.5
5.0
2.5
0.0
0	100
200	300
Time Step
400	500
Figure 3: Best function evaluation seen so far for the Latin Square problem.
Pest Control Problem: In the pest control problem, given n stations and k - 1 pesticide types, the
idea is to maintain the spread of pest (with minimum cost), which is propagating throughout the
stations in an interactive and probabilistic fashion. The k-th category for each variable corresponds
to the choice of no pesticide at all. Controlling the spread of the pest is carried out via the choice
of the right type of pesticide subject to a penalty proportional to its associated cost. A closed form
definition of this problem is given in Oh et al. (2019).
The results for different algorithms are shown in Figure 4. Despite the fact that COMBO is able to
find the minimum in fewer time steps (in ≈ 200 steps) than ECO-F (in ≈ 360 steps) on average,
17
Under review as a conference paper at ICLR 2021
ECO-F outperforms COMBO during initial time steps (until t ≈ 180). SA performs competitively,
but eventually is unable to find the optimal solution to this problem over the designated 500 steps.
The poor performance of ECO-G can be explained by the interactive nature of the problem, where
early mistakes are punished inordinately. Early mistakes made by ECO-G can also be attributed to
the large number of experts (with noisy coefficients) in its model, which in turn promotes an early
exploratory behavior.
---RS ----- SA ----- COMBO ---- ECO-F ---- ECO-G
0	100	200	300	400	500
Time Step
Figure 4: Best function evaluation seen so far for the pest control problem.
RNA Sequence Optimization Problem: Structured RNA molecules play a critical role in many
biological applications, ranging from control of gene expression to protein translation. The native
secondary structure ofa RNA molecule is usually the minimum free energy (MFE) structure. Consider
an RNA sequence as a string A = a1 . . . an of n letters (nucleotides) over the alphabet Σ =
{A, U, G, C}. A pair of complementary nucleotides ai and aj, where (i < j), can interact with each
other and form a base pair (denoted by (i, j)), A-U, C-G and G-U being the energetically stable pairs.
Thus, the secondary structure of an RNA can be represented by an ensemble of pairing bases.
Finding the most stable RNA sequences has immediate applications in material and biomedical
applications Li et al. (2015). Studies show that by controlling the structure and free energy of a
RNA molecule, one may modulate its translation rate and half-life in a cell Buchan & Stansfield
(2007); Davis et al. (2008), which is important in the context of viral RNA. A number of RNA folding
algorithms Lorenz et al. (2011); Markham & Zuker (2008) use a thermodynamic model (e.g. Zuker
& Stiegler (1981)) and dynamic programming to estimate MFE of a sequence. However, the O(n3)
time complexity of these algorithms prohibits their use for evaluating substantial numbers of RNA
sequences Gould et al. (2014) and exhaustively searching the space to identify the global free energy
minimum, as the number of sequences grows exponentially as 4n .
Here, we formulate the RNA sequence optimization problem as follows: For a sequence of length
n, find the RNA sequence that will fold into the secondary structure with the lowest minimum free
energy. In our experiments, we initially set n = 30 and k = 4. We then use the popular RNAfold
package Lorenz et al. (2011) to evaluate the MFE for a given sequence. The goal is to find the
lowest MFE sequence by calling the MFE evaluator minimum number of times. The performance of
different algorithms is depicted in Figure 1, where both ECO-F and particularly ECO-G outperform
the baselines as well as COMBO by a considerable margin.
Energy-optimized RNA Structures: Sample RNA sequences obtained via ECO-G after 4000 time
steps for n = 30 and n = 60 are shown in Figures 6 and 7, respectively. The resulting energy-
optimized sequences (as obtained using RNAfold service) have high (> 90%) GC content that
makes the strongest positive contribution to lowering MFE Trotta (2014), as pairings between
G and C have three hydrogen bonds and are more stable compared to A and U pairings, which
18
Under review as a conference paper at ICLR 2021
VXH JoJ8cα
Figure 5: Best function evaluation seen so far for the RNA sequence optimization problem with n = 30.
Table 1: Average computation time per step (in Seconds) over different problems and algorithms.
Dataset	n	k	COMBO	ECO-F	ECO-G
Latin S quare	25	5	170.4	1.5	3.6
Pest Control	25	5	151.0	1.4	3.3
Sequence Prediction	30	4	253.8	2.0	5.7
have only two. The final single-strand RNA sequence folds into a GC-paired double helix and
a 4 nucleotide long hairpin loop in the middle, which is a tetraloop reported in literature (http:
//www.rna.icmb.utexas.edu/SIM/4C/energetics_new/). Figures 10 and 11 show
two sample structures of the ECO-G optimized sequences for n = 31, again showing the same trend.
For odd values of n, there is presence of a loop with an odd number of residues or a single unpaired
base at the end, but there is still a GC-rich double helix. In contrast, the structures generated by the
under-performing algorithms do show presence of unpaired bases and are less in GC content, leading
to high energy structures (e.g. Figures 8 and 9 are obtained via SA after 4000 steps for n = 30 and
60, respectively).
E Design Experiments
For our experiments, we focus on three puzzles from the Eterna-100 dataset Anderson-Lee et al.
(2016). Two of the selected sequences (puzzles 15 and 41 of lengths 30 and 35, resp.), despite their
fairly small lengths, are challenging for many algorithms (see Anderson-Lee et al. (2016)). In both
puzzles, our MCTS variants (ECO-F and ECO-G) are able to significantly improve the performance
of MCTS when limited number of true rewards are available. All algorithms outperformed RS as
expected. Within the given 500 evaluation budget, both ECO-G, and especially ECO-F, are superior to
LEARNA by a substantial margin (see Figure 12). In puzzle number 41 (Figure 13), again both ECO-
G and ECO-F significantly outperform LEARNA, over the given number of evaluations. Interestingly,
ECO-F is able to outperform LEARNA throughout the evaluation process, and in average finds a far
better final solution than LEARNA.
The final sequence is puzzle #70 of length 184. The results of different algorithms over the latter
puzzle is shown in Figure 14. As we can see from this figure, MCTS-RNA and LEARNA perform
very similarly over the given 500 evaluation budget. ECO-F is able to outperform the remaining
algorithm throughout the evaluation steps. Initially, ECO-G has a similar performance to those of
19
Under review as a conference paper at ICLR 2021
Figure 6: RNA Structure via ECO-G for n = 30 Figure 7: RNA Structure via ECO-G for n = 60
MCTS-RNA and LEARNA, but offers an improved performance over the latter two just after 400
steps.
F	Choice of The Acquisition Method
Throughout the experiments, we designated SA and MCTS as acquisition methods for generic BBO
and design problems, respectively. The latter choice was made in accordance with the literature,
where SA is typically used as a baseline and method of choice for the generic BBO problem, whereas
MCTS has been commonly used for the design problem. For instance, SA has been considered as a
baseline and/or acquisition method in Dadkhahi et al. (2020), Ricardo Baptista (2018), and Oh et al.
(2019) (albeit with a different algorithm than ours). On the other hand, MCTS (i.e. RNA-MCTS as
well as its variations) is perhaps the most popular RNA design technique in the literature. Here, we
point out that both SA and MCTS can be used for both generic BBO and design problems. In this
section, we compare the performance of different acquisition methods in each problem.
First, we consider the generic BBO problem of RNA sequence optimization with n = 30 (considered
in Section 4). Figure 15 demonstrates the performance of ECO-F and ECO-G when SA or MCTS are
used as acquisition methods. As we can see from this figure, the SA-as-acquisition-method variants
perform slightly better than MCTS-as-acquisition-method counterparts over 500 steps. In particular,
although the performance gap is initially moderately large, over time this performance gap becomes
smaller.
Next, we consider two design problems considered in Section 4: puzzles #15 and #41. Note that,
when using SA as the acquisition method, we apply the softmax operator (in Algorithm 1) over the
set of {GC, CG, AU, UA} if the corresponding variable is part of a paired base. As we can see from
Figure 16, for puzzle #15, ECO-F with MCTS outperforms the remaining algorithms. The rest
of the algorithms have very similar performances, with ECO-F (MCTS) marginally surpassing the
SA variants. As we can see from Figure 17, for puzzle #41, the MCTS variant of ECO-F slightly
20
Under review as a conference paper at ICLR 2021
Figure 8: RNA Structure via SA for n = 30 Figure 9: RNA Structure via SA for n = 60
Figure 10: RNA Structure via ECO-G for n = 31 Figure 11: RNA Structure via ECO-G for n = 31
outperforms its SA variant, whereas the MCTS variant of ECO-G maintains a bigger gap with its SA
variant.
21
Under review as a conference paper at ICLR 2021
——RS ——MCTS ——≈ LEARNA ——ECO-F ——ECO-G
(XH JO ⊂=≥
18
16
100
200	300
Time Step
400	500
Figure 12: Best function evaluation for RNA Design of puzzle #15 with n = 30.
——RS ——MCTS	—ɔ LEARNA ——ECO-F ——ECO-G
5
2
2
5 0 5 0
亿15.12,10.
(XHM—o Ci
7.5
5.0
0	100	200	300	400	500
Time Step
4 2 0 8 6 4
2
0
如
Figure 13: Best function evaluation for RNA Design of puzzle #41 with n = 35.
G Order of the S urro gate Model
As mentioned in Section 4, we used m = 2 as the maximum order of the representations in all
the experiments. In this section, we focus on the generic BBO problem of RNA optimization and
investigate the impact of the model order on the performance of the proposed ECO algorithms. In
particular, we compare the performance of the algorithm at m = 3 with that of m = 2. As we can see
from Figure 18, at smaller evaluation budgets, the order 2 models moderately outperform the order 3
counterparts in both ECO-F and ECO-G. As we increase the number of samples, this performance
gap becomes smaller. At the 500 evaluation budget, ECO-G3 outperforms ECO-G2 by a small margin
of 0.1. At the same evaluation budget, ECO-F3 is slightly inferior to ECO-F2 by a margin of 0.2.
Considering the convergence behavior of the curves at order 3 versus those of order 2, we expect the
former models to eventually outperform the latter models at higher number of evaluations. However,
since in BBO problems sample efficiency is typically of main concern, it would make sense to use
low-order approximations. We point out that a similar observation was made in Ricardo Baptista
(2018) for the Boolean case, where higher order models suffer from a slower start due to the higher
22
Under review as a conference paper at ICLR 2021
——RS ——MCTS ——≈ LEARNA ——ECO-F ——ECO-G
80
70
O	1OO	200	300	400	500
Time Step
Figure 14: Best function evaluation for RNA Design of puzzle #70 with n = 184.
——ECO-G (SA) —— ECO-F (SA) —— ECO-G (MCTS) ——ECO-F (MCTS)
-30
0 5 0 5
112 2
- - - -
(Xb Jo UG
O	100
200	300
Time Step
400	500
Figure 15: Comparison of different acquisition methods for the generic BBO problem of RNA sequence
optimization with n = 30.
dimensionality of the parameter space. From our experiments in categorical problems, this behavior
seems to be even more pronounced due to the higher dimensionality of the categorical domains.
A summary of the computation times for ECO algorithms at different model orders is given in table
2. Since the complexity of ECO is linear in the number of experts, which exponentially grows with
the model order m, we observe an increase in the computational complexity of ECO-F3 (ECO-G3)
versus that of ECO-F2 (ECO-G2) by a factor of 9.7 (16.3).
Table 2: Average computation time per step (in Seconds) at different model orders.
ECO-F2 ECO-F3 ECO-G2 ECO-G3
2.0	19.4	5.7	93.1
23
Under review as a conference paper at ICLR 2021
——ECO-G (SA) ——ECO-F (SA) ——ECO-G (MCTS) ——ECO-F (MCTS)
(XH JO UG
Figure 16: Comparison of different acquisition methods for design puzzle #15 with n = 30.
——ECO-G (SA) ——ECO-F (SA) ——ECO-G (MCTS) ——ECO-F (MCTS)
22.5
20.0
-17.5
X
W 15。
1 12.5
10.0
7.5
5.0
Figure 17: Comparison of different acquisition methods for design puzzle #41 with n = 35.
24
Under review as a conference paper at ICLR 2021
CXH JO防。CQ
——EC0-F2 ——ECO-G2 ——ECO-F3 ——ECO-G3
O	1OO	200	300	400	500
Time Step
Figure 18: Impact of model order on the performance of ECO-F/G in the RNA optimization problem with
n = 30.
25