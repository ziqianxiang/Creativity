Under review as a conference paper at ICLR 2021
On the Geometry of Deep Bayesian Active
Learning
Anonymous authors
Paper under double-blind review
Ab stract
We present geometric Bayesian active learning by disagreements (GBALD), a
framework that performs BALD on its geometric interpretation interacting with a
deep learning model. There are two main components in GBALD: initial acquisi-
tions based on core-set construction and model uncertainty estimation with those
initial acquisitions. Our key innovation is to construct the core-set on an ellipsoid,
not typical sphere, preventing its updates towards the boundary regions of the dis-
tributions. Main improvements over BALD are twofold: relieving sensitivity to
uninformative prior and reducing redundant information of model uncertainty. To
guarantee the improvements, our generalization analysis proves that, compared to
typical Bayesian spherical interpretation, geodesic search with ellipsoid can derive
a tighter lower error bound and achieve higher probability to obtain a nearly zero
error. Experiments on acquisitions with several scenarios demonstrate that, yield-
ing slight perturbations to noisy and repeated samples, GBALD further achieves
significant accuracy improvements than BALD, BatchBALD and other baselines.
1	Introduction
Lack of training labels restricts the performance of deep neural networks (DNNs), though prices of
GPU resources were falling fast. Recently, leveraging the abundance of unlabeled data has become
a potential solution to relieve this bottleneck whereby expert knowledge is involved to annotate
those unlabeled data. In such setting, the deep learning community introduced active learning (AL)
(Gal et al., 2017) that, maximizing the model uncertainty (Ashukha et al., 2019; Lakshminarayanan
et al., 2017) to acquire a set of highly informative or representative unlabeled data, and solicit
experts’ annotations. During this AL process, the learning model tries to achieve a desired accuracy
using minimal data labeling. Recent shift of model uncertainty in many fields, such as Bayesian
neural networks (Blundell et al., 2015), Monte-Carlo (MC) dropout (Gal & Ghahramani, 2016), and
Bayesian core-set construction (Sener & Savarese, 2018), shows that, new scenarios arise from deep
Bayesian AL (Pinsler et al., 2019; Kirsch et al., 2019).
Bayesian AL (Golovin et al., 2010; Jedoui et al., 2019) presents an expressive probabilistic interpre-
tation on model uncertainty (Gal & Ghahramani, 2016). Theoretically, for a simple regression model
such as linear, logistic, and probit, AL can derive their closed-forms on updating one sparse subset
that maximally reduces the uncertainty of the posteriors over the regression parameters (Pinsler et al.,
2019). However, for a DNN model, optimizing massive training parameters is not easily tractable. It
is thus that Bayesian approximation provides alternatives including importance sampling (Doucet
et al., 2000) and Frank-Wolfe optimization (Vavasis, 1992). With importance sampling, a typical
approach is to express the information gain in terms of the predictive entropy over the model, and it
is called Bayesian active learning by disagreements (BALD) (Houlsby et al., 2011).
BALD has two interpretations: model uncertainty estimation and core-set construction. To estimate
the model uncertainty, a greedy strategy is applied to select those data that maximize the parameter
disagreements between the current training model and its subsequent updates as (Gal et al., 2017).
However, naively interacting with BALD using uninformative prior (Strachan & Van Dijk, 2003)(Price
& Manson, 2002), which can be created to reflect a balance among outcomes when no information
is available, leads to unstable biased acquisitions (Gao et al., 2020), e.g. insufficient prior labels.
Moreover, the similarity or consistency of those acquisitions to the previous acquired samples, brings
redundant information to the model and decelerates its training.
Core-set construction (Campbell & Broderick, 2018) avoids the greedy interaction to the model by
capturing characteristics of the data distributions. By modeling the complete data posterior over the
1
Under review as a conference paper at ICLR 2021
Model uncertainty estimation
Input
BALD
C o∣f set construction
Ellipsoid
GBALD
Figure 1: Illustration of two-stage GBALD framework.
BALD has two types of interpretation: model uncer-
tainty estimation and core-set construction where the
deeper the color of the core-set element, the higher the
representation; GBALD integrates them into a uniform
framework. Stage 1 : core-set construction is with
an ellipsoid, not typical sphere, representing the origi-
nal distribution to initialize the input features of DNN.
Stage 2 : model uncertainty estimation with those ini-
tial acquisitions then derives highly informative and
representative samples for DNN.
distributions of parameters, BALD can be deemed as a core-set construction process on a sphere
(Kirsch et al., 2019), which seamlessly solicits a compact subset to approximate the input data
distribution, and efficiently mitigates the sensitivity to uninformative prior and redundant information.
From the view of geometry, updates of
core-set construction is usually optimized
with sphere geodesic as (Nie et al., 2013;
Wang et al., 2019). Once the core-set
is obtained, deep AL immediately seeks
annotations from experts and starts the
training. However, data points located at
the boundary regions of the distribution,
usually win uniform distribution, cannot
be highly-representative candidates for the
core-set. Therefore, constructing the core-
set on a sphere may not be the optimal
choice for deep AL.
This paper presents a novel AL framework,
namely Geometric BALD (GBALD),
over the geometric interpretation of BALD
that, interpreting BALD with core-set con-
struction on an ellipsoid, initializes an
effective representation to drive a DNN
model. The goal is to seek for significant
accuracy improvements against an uninformative prior and redundant information. Figure 1 describes
this two-stage framework. In the first stage, geometric core-set construction on an ellipsoid initializes
effective acquisitions to start a DNN model regardless of the uninformative prior. Taking the core-set
as the input features, the next stage ranks the batch acquisitions of model uncertainty according to
their geometric representativeness, and then solicits some highly-representative examples from the
batch. With the representation constraints, the ranked acquisitions reduce the probability of sampling
nearby samples of the previous acquisitions, preventing redundant acquisitions. To guarantee the
improvement, our generalization analysis shows that, the lower bound of generalization errors of AL
with the ellipsoid is proven to be tighter than that of AL with the sphere. Achieving a nearly zero
generalization error by AL with ellipsoid is also proven to have higher probability. Contributions of
this paper can be summarized from Geometric, Algorithmic, and Theoretical perspectives.
•	Geometrically, our key innovation is to construct the core-set on an ellipsoid, not typical
sphere, preventing its updates towards the boundary regions of the distributions.
•	In term of algorithm design, in our work, from a Bayesian perspective, we propose a
two-stage framework that sequentially introduces the core-set representation and model
uncertainty, strengthening their performance “independently”. Moreover, different to the
typical BALD optimizations, we present geometric solvers to construct core-set and estimate
model uncertainty, which result in a different view for Bayesian active learning.
•	Theoretically, to guarantee those improvements, our generalization analysis proves that,
compared to typical Bayesian spherical interpretation, geodesic search with ellipsoid can
derive a tighter lower error bound and achieve higher probability to obtain a nearly zero
error. See Appendix B.
The rest of this paper is organized as follows. In Section 2, we first review the related work. Secondly,
we elaborate BALD and GBALD in Sections 3 and 4, respectively. Experimental results are presented
in Section 5. Finally, we conclude this paper in Section 6.
2	Related work
Model uncertainty. In deep learning community, AL (Cohn et al., 1994) was introduced to improve
the training of a DNN model by annotating unlabeled data, where the data which maximize the
model uncertainty (Lakshminarayanan et al., 2017) are the primary acquisitions. For example, in
ensemble deep learning (Ashukha et al., 2019), out-of-domain uncertainty estimation selects those
data which do not follow the same distribution as the input training data; in-domain uncertainty
draws the data from the original input distribution, producing reliable probability estimates. Gal &
2
Under review as a conference paper at ICLR 2021
Ghahramani (2016) use MC dropout to estimate predictive uncertainty for approximating a Bayesian
convolutional neural network. Lakshminarayanan et al. (2017) estimate predictive uncertainty using
a proper scoring rule as the training criteria to fed a DNN.
Bayesian AL. Taking a Bayesian perspective (Golovin et al., 2010), AL can be deemed as minimizing
the Bayesian posterior risk with multiple label acquisitions over the input unlabeled data. A potential
informative approach is to reduce the uncertainty about the parameters using Shannon’s entropy (Tang
et al., 2002). This can be interpreted as seeking the acquisitions for which the Bayesian parameters
under the posterior disagree about the outcome the most, so this acquisition algorithm is referred to
as Bayesian active learning by disagreement (BALD) (Houlsby et al., 2011).
Deep AL. Recently, deep Bayesian AL attracted our eyes. Gal et al. (2017) proposed to cooperate
BALD with a DNN to improve the training. The unlabeled data which maximize the model uncertainty
provide positive feedback. However, it needs to repeatedly update the model until the acquisition
budget is exhausted. To improve the acquisition efficiency, batch sampling with BALD is applied as
(Kirsch et al., 2019; Pinsler et al., 2019). In BatchBALD, Kirsch et al. (2019) developed a tractable
approximation to the mutual information of one batch of unlabeled data and current model parameters.
However, those uncertainty evaluations of Bayesian AL whether in single or batch acquisitions all
take greedy strategies, which lead to computationally infeasible, or excursive parameter estimations.
For deep Bayesian AL, being short of interactions to DNN can not maximally drive their model
performance as (Pinsler et al., 2019; Sener & Savarese, 2018), etc.
3	BALD
BALD has two different interpretations: model uncertainty estimation and core-set construction. We
simply introduce them in this section.
3.1	Model uncertainty estimation
We consider a discriminative model p(y∣x, θ) parameterized by θ that maps x ∈ X into an output
distribution over a set of y ∈ Y . Given an initial labeled (training) set D0 ∈ X × Y , the Bayesian
inference over this parameterized model is to estimate the posterior p(θ∣D0), i.e. estimate θ by
repeatedly updating D0 . AL adopts this setting from a Bayesian view.
With AL, the learner can choose unlabeled data from Du = {xi}jN=1 ∈ X, to observe the outputs of the
current model, maximizing the uncertainty of the model parameters. Houlsby et al. (2011) proposed
a greedy strategy termed BALD to update D° by estimating a desired data x* that maximizes the
decrease in expected posterior entropy:
x* = arg maxH[θ∣Do] - Ey~p(y∣χ,D0) [H[θ∣x,y, Do]],	(1)
x∈Du
where the labeled and unlabeled sets are updated by D0 = D0 ∪ {x*, y*}, Du = Du/x*, and y*
denotes the output of x* . In deep AL, y * can be annotated as a label from experts and θ yields a
DNN model.
3.2 Core-set construction
Let P(θ∣Do) be updated by its log posterior logP(θ∣Do, x*), y* ∈ {yi}iN=1, assume the outputs are
conditional independent of the inputs, i.e. P(y* ∣x*, Do) = ∫θ P(y*∣x*, θ)P(θ∣Do)dθ, then we have
the complete data log posterior following (Pinsler et al., 2019):
Ey*[logp(θ∣Do,x*,y*)] = Ey*[logp(θ∣Do) + logp(y*∣x*,θ) - logp(y*∣x*,Do)]
=logp(θ∣Do) + Ey*[logp(y*∣χ*,θ) + H[y*∣χ*, Do]]
=logp(θ∣Do) + ∑ ∣Eyi[logp(yi∣xi,θ) + H[yi∣xi,Do]]).
(2)
The key idea of core-set construction is to approximate the log posterior of Eq. (2) by a subset
of Du ⊆ Du such that: Eγu [logp(θ∣Do, Du, Yu)] ≈ EYu [logp(θ∣Do,Du,YU)], where Yu and Yu
denote the predictive labels of Du and Du respectively by the Bayesian discriminative model, that is,
P(YuDu,Do) = ∫θP(Yu∣Du,θ)p(θ∣Do)dθ, andP(YuDu,Do)=八P(YuDu,θ)p(θ∣D°)dθ. Here
Du can be indicated by a core-set (Pinsler et al., 2019) that highly represents Du. Optimization tricks
such as Frank-Wolfe optimization (Vavasis, 1992) then can be adopted to solve this problem.
3
Under review as a conference paper at ICLR 2021
Motivations. Eqs. (1) and (2) provide the Bayesian rules of BALD over model uncertainty and
core-set construction respectively, which further attract the attention of the deep learning commu-
nity. However, the two interpretations of BALD are limited by: 1) redundant information and 2)
uninformative prior, where one major reason which causes these two issues is the poor initialization
on the prior, i.e. p(D0∣θ). For example, unbalanced label initialization on Do usually leads to an
uninformative prior, which further conducts the acquisitions of AL to select those unlabeled data
from one or some fixed classes; highly-biased results with (Gao et al., 2020) redundant information
are inevitable. Therefore, these two limitations affect each other.
4	GBALD
GBALD consists of two components: ini-
tial acquisitions based on core-set construc-
tion and model uncertainty estimation with
those initial acquisitions.
4.1	Geometric
INTERPRETATION OF CORE-SET
(a) Sphere geodesic
Figure 2: Optimizing BALD with sphere and ellip-
soid geodesics. Ellipsoid geodesic rescales the sphere
geodesic to prevent the updates of the core-set towards
the boundary regions of the sphere where the charac-
teristics of the distribution cannot be captured. Black
points denote the feasible updates of the red points.
Dash lines denote the geodesics.
(b) Ellipsoid geodesic
Modeling the complete data posterior over
the parameter distribution can relieve the
above two limitations of BALD. Typically,
finding the acquisitions of AL is equivalent
to approximating a core-set centered with
spherical embeddings (Sener & Savarese,
2018). Let Wi be the sampling weight of
Xi, I Wi 10 ≤ N, the core-set construction is to optimize:
N	N N	r
min	∑ Eya[logp(yi∣Xi,θ) + H[%∣Xi, Do]] - ∑ WiEyi [logp(%∣Xi,θ) + H[y∕xi, Do]]
Wi=IL	i=1
2
(3)
s----------------V---------------' S-----------------V-----------------'
L	L(W)
where L and L(W) denote the full and expected (weighted) log-likelihoods, respectively (Campbell
& Broderick, 2018; 2019). Specifically, ∑N1 H[yi∣Xi,Do] = - ∑yip(yi∣Xi,D0)log(p(yi∣4,Do),
wherep(yi∣Xi,Do) = fθp(yi∣Xi, θ)p(θ∣Do)dθ. Note ∣∣ ∙ ∣∣ denotes the '2 norm.
The approximation ofEq. (3) implicitly requires that the complete data log posterior of Eq. (2) w.r.t.
L must be close to an expected posterior w.r.t. L(W) such that approximating a sparse subset for
the original inputs by sphere geodesic search is feasible (see Figure 2(a)). Generally, solving this
optimization is intractable due to cardinality constraint (Pinsler et al., 2019). Campbell & Broderick
(2019) proposed to relax the constraint in Frank-Wolfe optimization, in which mapping X is usually
performed in a Hilbert space (HS) with a bounded inner product operation. In this solution, the sphere
embedded in the HS replaces the cardinality constraint with a polynomial constraint. However, the
initialization on Do affects the iterative approximation to DU at the beginning of the geodesic search.
Moreover, the posterior of p(θ∣Do) is uninformative, if the initialized Do is empty or not correct.
Therefore, the typical Bayesian core-set construction of BALD cannot ideally fit an uninformative
prior. The another geometric interpretation of core-set construction, such as k-centers (Sener &
Savarese, 2018), is not restricted to this setting. We thus follow the construction of k-centers to find
the core-set.
k-centers. Sener & Savarese (2018) proposed a core-set representation approach for active deep
learning based on k-centers. This approach can be adopted in core-set construction of BALD without
the help of the discriminative model. Therefore, the uninformative prior has no further influence on
the core-set. Typically, the k-centers approach uses a greedy strategy to search the data χ whose
nearest distance to elements of Do is the maximal:
x = arg max min ∣∣Xi - & ∣∣,	(4)
Xi∈Du ci∈D0
then Do is updated by Do ∪ {x,可}, DU is updated by Du∖≡, where y denotes the output of χ. This
max-min operation usually performs k times to construct the centers.
From the view of geometry, k-centers can be deemed as the core-set construction via spherical
geodesic search (Badoiu et al., 2002; Har-Peled & Mazumdar, 2004). Specifically, the max-min
4
Under review as a conference paper at ICLR 2021
optimization guides D0 to be updated into one data, which draws the longest line segment from xi, ∀i
across the sphere center. The iterative update on χ is then along its unique diameter through the
sphere center. However, this greedy optimization has large probability that yields the core-set to fall
into boundary regions of the sphere, which cannot capture the characteristics of the distribution.
4.2 Initial acquisitions based on core-set construction
We present a novel greedy search which rescales the geodesic of a sphere into an ellipsoid following
Eq. (4), in which the iterative update on the geodesic search is rescaled (see Figure 2(b)). We follow
the importance sampling strategy to begin the search.
Initial prior on geometry. Initializing p(Do∣θ) is performed with a group of internal spheres
centered with Dj , ∀j, subjected to Dj ∈ Do, in which the geodesic between Do and the unlabeled
data is over those spheres. Since Do is known, specification of θ plays the key role for initializing
p(Do ∣θ). Given a radius Ro for any observed internal sphere, p(yi∣xi, θ) is firstly defined by
1,	∃j, llχi- DjIl ≤ Ro,
p(yi∣xi,θ) = ∙
max-
R0
Ilxi-DjIl ∙
∀j,llxi - Dill > R0,
(5)
thereby θ yields the parameter Ro . When the data is enclosed with a ball, the probability of Eq. (5) is
1. The data near the ball, is given a probability of max{ Ux RD F } constrained by min ∣∣ xi -DjI,∀j,
i.e. the probability is assigned by the nearest ball to xi , which is centered with Dj . From Eq. (3),
the information entropy of yi Z {y1,y2,...,yN} over xi Z {x1,x2,…,xN} can be expressed as the
integral regarding p(yi ∣xi, θ):
∑H(yi∣xi,Do) = - ∑ ∫ p(yi∣xi, θ)p(θ∣Do)dθlog( ∫ p(yi∣xi, θ)p(θ∣Do))dθ,	(6)
which can be approximated by - ∑iN=1 p(yi ∣xi, θ)log(p(yi ∣xi, θ)) following the details of Eq. (3). In
short, this indicates an approximation to the entropy over the entire outputs on Du that assumes the
prior p(Do∣θ) w.r.t. p(yi∣xi, θ) is already known from Eq. (5).
Max-min optimization. Recalling the max-min optimization trick of k-centers in the core-set
construction of (Sener & Savarese, 2018), the minimizer of Eq. (3) can be divided into two parts:
minx* L and maxw L(w), where Do is updated by acquiring x*. However, updates of Do decide the
minimizer of L with regard to the internal spheres centered with Di, ∀i. Therefore, minimizing L
should be constrained by an unbiased full likelihood over X to alleviate the potential biases from the
initialization of Do . Let Lo denote the unbiased full likelihood over X that particularly stipulates
Do as the k-means centers written as U of X which jointly draw the input distribution. We define
Lo = ∣∑iN=1 Eyi [logp(yi∣xi, θ) + H[yi∣xi,U]]∣ to regulate L, that is
min ∣Lo - L∣∣2, s.t. Do = Do ∪ {x",y*}, Du = Du∖x*.
x*
The other sub optimizer is maxw L(w). We present a greedy strategy following Eq. (1):
N
(7)
1m≤ia≤xN mwiin i∑=1wiEyi[logp(yi∣xi,θ)+H[yi∣xi,Do]]
N
(8)
∑ wilogp(yi∣xi, θ) - ∑ wip(yi ∣xi, θ)logp(yi∣xi, θ),
i=1
i=1
which can be further written as: ∑iN=1 wilogp(yi∣xi, θ)(1 - logp(yi ∣xi, θ)). Let wi = 1, ∀i for
unbiased estimation of the likelihood L(w), Eq. (8) can be simplified as
max min logp(yi ∣xi, θ),
xi∈Du Dj ∈D0
(9)
where p(yi∣xi, θ) follows Eq. (5). Combining Eqs. (7) and (9), the optimization of Eq. (3) is then
transformed as
N
x* = arg max min ∣Lo - L∣∣2 + logp(yj-∣xj-,θ) ∙,
Xj ∈Du Dj ∈D0
(10)
where Do is updated by acquiring x*, i.e. Do = Do ∪ {x*,y*}.
Geodesic line. For a metric geometry M , a geodesic line is a curve γ which projects its in-
terval I to M : I → M, maintaining everywhere locally a distance minimizer (Lou et al.,
2020). Given a constant ν > 0 such that for any a, b ∈ I there exists a geodesic distance
5
Under review as a conference paper at ICLR 2021
d(Y(Ia),Ye)) = ∫a √gγ(t)(γ'(t),Y'(t))dt, where γ'(t) denotes the geodesic curvature, and g
denotes the metric tensor over M. Here, We define γ'(t) = 0, then gγ(t)(0,0) = 1 such that
d(γ(a),γ(b)) can be generalized as a segment of a straight line: d(γ(a),γ(b)) = IIa - b∣∣.
Ellipsoid geodesic distance. For any observation points p, q ∈ M, if the spherical geodesic distance
is defined as d(Y(p), Y(q)) = Ip - qI. The affine projection obtains its ellipsoid interpretation:
d(Y(p), Y(q)) = Iη(p - q)I, where η denotes the affine factor subjected to 0 < η < 1.
Optimizing with ellipsoid geodesic search. The max-min optimization of Eq. (10) is performed on
an ellipsoid geometry to prevent the updates of core-set towards the boundary regions, where ellipsoid
geodesic line scales the original update on the sphere. Assume Xi is the previous acquisition and x* is
the next desired acquisition, the ellipsoid geodesic rescales the position of x* as x* = Xi + η(χ* - Xi).
Then, we update this position of xe* to its nearest neighbor xj in the unlabeled data pool, i.e.
arg minxj∈Du Ixj - xe* I, also can be written as
arg min IlXj - [xi + η(x* - Xi)]∣∣.
xj∈Du
(11)
To study the advantage of ellipsoid geodesic search, Appendix B presents our generalization analysis.
4.3 Model uncertainty estimation with core-set
GBALD starts the model uncertainty estimation with those initial core-set acquisitions, in which it
introduces a ranking scheme to derive both informative and representative acquisitions.
Single acquisition. We follow (Gal et al., 2017) and use MC dropout to perform Bayesian inference
on the model of the neural network. It then leads to ranking the informative acquisitions with batch
sequences is with high efficiency. We first present the ranking criterion by rewriting Eq. (1) as batch
returns:
{χ1, χ2 ,…,χb } = arg max	H[θ∣D0] - Eyi：b~p(yi：b|Xi：b,Do) [H[θ∣xLb,yLb, D0]],	(12)
{^1,^2,...,Xb}⊆Du
where Xi：b = {X1,X2,…，Xb}, yi：b = {y1,y2, ...,yb}, yi denotes the output of Xi. The informative
acquisition x* is then selected from the ranked batch acquisitions Xi：b due to the highest representation
for the unlabeled data:
x* =	argmax ∙ max p(yi∣x*,θ) ：=	*R0	∙,	(13)
xj∈{x^,x2,…,x" ∙ Dj∈D0	llxi - Dj 11
where t denotes the index of the final acquisition, subjected to 1 ≤ t ≤ b. This also adopts the max-min
optimization of k-centers in Eq.(4), i.e. x* = arg maXχ*∈{χ3χ3…词} minDj∈d0 ∣∣x* - Dj ∣∣.
Batch acquisitions. The greedy strategy of Eq. (13) can be written as a batch acquisitions by setting
its output as a batch set, i.e.
{x*ι ,..∙,x*b'} =	argmaχ	p(y*i：tb,lx*i：tb, ,θ),
x⅛1 :tb' ⊆{xl ,x2 ,...,xb }
(14)
Where x*i：tb' = {X*1,…,χ*b,}, y*i：tb, = {y*,…,y*b'}, y* denotes the output of χ*, 1 ≤ i ≤ b', and
1 ≤ b′ ≤ b. This setting can be used to accelerate the acquisitions ofAL in a large dataset. Appendix
A presents the two-stage GBALD algorithm.
5 Experiments
In experiments, we start by showing how BALD degenerates its performance with uninformative
prior and redundant information, and show that how our proposed GBALD relieves theses limitations.
Our experiments discuss three questions: 1) is GBALD using core-set of Eq. (11) competitive with
uninformative prior? 2) can GBALD using ranking of Eq. (14) improve informative acquisitions
of model uncertainty? and 3) can GBALD outperform state-of-the-art acquisition approaches?
Following the experiment settings of (Gal et al., 2017; Kirsch et al., 2019), we use MC dropout to
implement the Bayesian approximation of DNNs. Three benchmark datasets are selected: MNIST,
SVHN and CIFAR10. More experiments are presented in Appendix C.
5.1	Uninformative priors
As discussed in the introduction, BALD is sensitive to an uninformative prior, i.e. p(D0 ∣θ). We thus
initialize D0 from a fixed class of the tested dataset to observe its acquisition performance. Figure 3
presents the prediction accuracies of BALD with an acquisition budget of 130 over the training
6
Under review as a conference paper at ICLR 2021
set of MNIST, in which we randomly select 20 samples from the digit ‘0’ and ‘1’ to initialize D0,
respectively. The classification model of AL follows a convolutional neural network with one block
of [convolution, dropout, max-pooling, relu], with 32, 3x3 convolution filters, 5x5 max pooling, and
0.5 dropout rate. In the AL loops, we use 2,000 MC dropout samples from the unlabeled data pool to
fit the training of the network following (Kirsch et al., 2019).
The results show BALD can
slowly accelerate the training
model due to biased initial ac-
quisitions, which cannot uni-
formly cover all the label cat-
egories. Moreover, the unin-
formative prior guides BALD
to unstable acquisition results.
As the shown in Figure 3(b),
BALD with Bathsize = 10
shows better performance than
that of Batchsize =1; while
BALD in Figure 3(a) keeps sta-
(a) Digit ‘0’	(b) Digit ‘1’
Figure 3: Acquisitions with uninformative priors from digit ‘0’ and
‘1’.
ble performance. This is because the initial labeled data does not cover all classes and BALD with
Batchsize =1 may further be misled to select those samples from one or a few fixed classes at the first
acquisitions. However, Batchsize >1 may result in a random acquisition process that possibly covers
more diverse labels at its first acquisitions. Another excursive result of BALD is that the increasing
batch size cannot degenerate its acquisition performance in Figure 3(b). Specifically, Batchsize
=10 A Batchsize =1 A Batchsize =20,40 A Batchsize =30, where '>, denotes 'better' performance;
Batchsize = 20 achieves similar results of Batchsize =40. This undermines the acquisition policy of
BALD: its performance would degenerate when the batch size increases, and sometimes worse
than random sampling. This also is the reason why we utilize a core-set to start BALD in our
framework.
Different to BALD, the core-set construction of GBALD using Eq. (11) provides a complete label
matching against all classes. Therefore, it outperforms BALD with the batch sizes of 1, 10, 20, 30,
and 40. As the shown learning curves in Figure 3, GBALD with a batch size of 1 and sequence size
of 10 (i.e. breakpoints of acquired size are 10, 20, ..., 130) achieves significantly higher accuracies
than BALD using different batch sizes since BALD misguides the network updating using poor prior.
5.2	Improved informative acquisitions
Repeated or similar acquisitions
delay the acceleration of the
model training of BALD. Fol-
lowing the experiment settings
of Section 5.1, we compare the
best performance of BALD with
a batch size of 1 and GBALD
with different batch size parame-
ters. Following Eq. (14), we set
b = {3,5,7} and b'=1, respec-
tively, that means, we output the
highest representative data from
a batch of highly-informative
(a) Digit ‘0’	(b) Digit ‘1’
Figure 4: GBALD outperforms BALD using ranked informative
acquisitions which cooperate with representation constraints.
acquisitions. Different settings on b and b′ are used to observe the parameter perturbations of
GBALD.
Training by the same parameterized CNN model as Section 5.1, Figure 4 presents the acquisition
performance of parameterized BALD and GBALD. As the learning curves shown, BALD cannot
accelerate the model as fast as GBALD due to the repeated information over the acquisitions. For
GBALD, it ranks the batch acquisitions of the highly-informative samples and selects the highest
representative ones. By employing this special ranking strategy, GBALD can reduce the probability
of sampling nearby data of the previous acquisitions. It is thus GBALD significantly outperforms
BALD, even if we progressively increase the ranked batch size b.
7
Under review as a conference paper at ICLR 2021
(a) MNIST
(b) SVHN
Figure 5: Active acquisitions on MNIST, SVHN, and CIFAR10 datasets.
(c) CIFAR10
Table 1: Mean±std of the test accuracies of the breakpoints of the learning curves on MNIST, SVHN,
and CIFAR-10.
Datasets
MNIST
SVHN
CIFAR-10
Algorithms
Var	BALD Entropy	k-medoids	k-centers GBALD
0.8419± 0.1721 0.8645±0.1909 0.8498±0.2098 0.8785±0.1433 0.8052±0.1838 0.9106±0.1296
0.8535±0.1098 0.8510±0.1160 0.8294±0.1415 0.8498±0.1294 0.7909±0.1235 0.8885±0.1054
0.7122±0.1034 0.6760±0.1023 0.6536±0.1038 0.71837±0.1245 0.5890±0.1758 0.7440±0.1087
5.3	Active acquisitions
GBALD using Eqs. (11) and (14) has been demonstrated to achieve successful improvements over
BALD. We thus combine these two components into a uniform framework. Figure 5 reports the AL
accuracies using different acquisition algorithms on the three image datasets. The selected baselines
follow (Gal et al., 2017) including 1) maximizing the variation ratios (Var), 2) BALD, 3) maximizing
the entropy (Entropy), 4) k-medoids, and one greedy 5) k-centers approach (Sener & Savarese,
2018). The network architecture is a three-layer MLP with three blocks of [convolution, dropout,
max-pooling, relu], with 32, 64, and 128 3x3 convolution filters, 5x5 max pooling, and 0.5 dropout
rate. In the AL loops, the MC dropout still randomly samples 2,000 data from the unlabeled data pool
to approximate the training of the network architecture following (Kirsch et al., 2019). The initial
labeled data of MNIST, SVHN and CIFAR-10 are 20, 1000, 1000 random samples from their full
training sets. Details of baselines are presented in Appendix C.1.
The batch size of the compared baselines is 100, where GBALD ranks 300 acquisitions to select
100 data for the training, i.e. b = 300, b′ = 100. As the learning curves shown in Figure 5, 1)
k-centers algorithm performs more poorly than other compared baselines because the representative
optimization with sphere geodesic usually falls into the selection of boundary data; 2) Var, Entropy
and BALD algorithms cannot accelerate the network model rapidly due to highly-skewed acquisitions
towards few fixed classes at its first acquisitions (start states); 3) k-medoids approach does not interact
with the neural network model while directly imports the clustering centers into its training set; 4)
The accuracies of the acquisitions of GBALD achieve better performance at the beginning than the
Var, Entropy and BALD approaches which fed the training set of the network model via acquisition
loops. In short, the network is improved faster after drawing the distribution characteristics of
the input dataset with sufficient labels. GBALD thus consists of the representative and informative
acquisitions in its uniform framework. Advantages of these two acquisition paradigms are integrated
and present higher accuracies than any single paradigm.
Table 1 reports the mean±std val-
ues of the test accuracies of the
breakpoints of the learning curves
in Figure 5, where breakpoints of
MNIST are {0, 10, 20, 30,..., 600},
breakpoints of SVHN are
{0, 100, 200,..., 10000}, and
breakpoints of CIFAR10 are
{0, 100, 200,..., 20000}. We then
calculate their average accuracies
and std values over these acquisition
Table 2: Number of acquisitions on MNIST, SVHN and
CIFAR10 until 70%, 80%, and 90% accuracies are reached.
Algorithms
Var
BALD
Entropy
k-modoids
k-centers
GBALD
Accuracies
70%	80%
140/1,700/5,700
110/1,700 /8,800
110/1,900/11,200
70/1,700/5,900
110/2,000/10,100
50/1,400/4,800
150/2,200/›20,000
120 /2,300/>20,000
150/2,400/>20,000
90/2,200/16,000
150/3,800/>20,000
70/1,900/12,200
90%
210/>10,000/>6,100
190/7,100 / >20,000
200/8,600/>20,000
170/6,200 />20,000
280/>10,000/>20,000
170/3,900/>20,000
points. As the shown in Table 1, all std values around 0.1, yielding a norm value. Usually, an average
8
Under review as a conference paper at ICLR 2021
accuracy on a same acquisition size with different random seeds of DNNs, will result a small std
value. Our mean accuracy spans across the whole learning curve.
The results show that 1) GBALD achieves the highest average accuracies; k-medoids is ranked
the second amongst the compared baselines; 2) k-centers has ranked the worst accuracies amongst
these approaches; 3) the others, which iteratively update the training model are ranked at the middle
including BALD, Var and Entropy algorithms. Table 2 shows the acquisition numbers of achieving
the accuracies of 70%, 80%, and 90% on the three datasets. The three numbers of each cell are the
acquisition numbers over MNIST, SVHN, and CIFAR10, respectively. The results show that GBALD
can use fewer acquisitions to achieve a desired accuracy than the other algorithms.
5.4	GBALD VS. BATCHBALD
Figure 6: ComParisons of BALD, Batch-
BALD, and GBALD of active acquisi-
tions on MNIST With bath settings.
Batch active deep learning was recently proposed to accelerate the training of a DNN model. In
recent literature, BatchBALD (Kirsch et al., 2019) extended BALD with a batch acquisition setting
to converge the network using fewer iteration loops. Different to BALD, BathBALD introduces
diversity to avoid repeated or similar output acquisitions.
How to set the batch size of the acquisitions attracted our eyes before starting the experiments.
It involves with whether our experiment settings are fair and reasonable. From a theoretical
view, the larger the batch size, the worse the batch acquisitions will be. Experiments results of
(Kirsch et al., 2019) also demonstrated this phenomenon. We thus set different batch sizes to
run BatchBALD. Figure 6 reports the comparison results of BALD, BatchBALD, and our pro-
posed GBALD following the experiment settings of Section 5.3. As the shown in this figure,
BatchBALD degenerates the test accuracies if We Progres-
sively increase the bath sizes, where BatchBALD with a
batch size of 10 keePs similar learning curves as BALD.
This means BatchBALD actually can accelerate BALD
With a similar acquisition result if the batch size is not
large. That means, if the batch size is betWeen 2 to 10,
BatchBALD Will degenerate into BALD and maintains
highly-consistent results.
Also because of this, BatchBALD has the same sensitivity
to the uninformative Prior. For our GBALD, the core-set
solicits sufficient data Which ProPerly matches the inPut
distribution (W.r.t. acquired data set size ≤ 100), Providing
PoWerful inPut features to start the DNN model (W.r.t.
acquired data set size > 100). Table 3 then Presents the mean±std of breakPoints ({0, 10, 20, ..., 600})
of active acquisitions on MNIST With batch settings. The statistical results shoW GBALD has much
higher mean accuracy than BatchBALD With different bath sizes. Therefore, evaluating the model
uncertainty of DNN using highly-rePresentative core-set samPles can imProve the Performance of the
neural netWork.
6 Conclusion
We have introduced a novel Bayesian AL frame-
Work, GBALD, from the geometric PersPective,
Which seamlessly incorPorates rePresentative
(core-set) and informative (model uncertainty
estimation) acquisitions to accelerate the train-
ing of a DNN model. Our GBALD yields sig-
nificant imProvements over BALD, flexibly re-
solving the limitations of an uninformative Prior
and redundant information by oPtimizing the
acquisition on an elliPsoid. Generalization anal-
ysis has asserted that training With elliPsoid has
Table 3: Mean±std of BALD, BatchBALD, and
GBALD of active acquisitions on MNIST With
batch settings.
Algorithms	Batch sizes	Accuracies
BALD	1	0.8654±0.0354
BatchBALD	10	0.8645±0.0365
BatchBALD	40	0.8273±0.0545
BatchBALD	100	0.7902±0.0951
GBALD	3	0.9106±0.1296
tighter loWer error bound and higher Probability to achieve a zero error than training With a tyPical
sPhere. ComPared to the rePresentative or informative acquisition algorithms, exPeriments shoW that
our GBALD sPends much feWer acquisitions to accelerate the accuracy. Moreover, it keePs slighter
accuracy reduction than other baselines against rePeated and noisy acquisitions.
9
Under review as a conference paper at ICLR 2021
References
Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. In International Conference on
Learning Representations, 2019.
Arsenii Ashukha, Alexander Lyzhov, Dmitry Molchanov, and Dmitry Vetrov. Pitfalls of in-domain
uncertainty estimation and ensembling in deep learning. In International Conference on Learning
Representations, 2019.
Mihai Badoiu, Sariel Har-Peled, and Piotr Indyk. Approximate clustering via core-sets. In Proceed-
ings of the thiry-fourth annual ACM Symposium on Theory of computing, pp. 250-257, 2002.
Shai Ben-David and Ulrike Von Luxburg. Relating clustering stability to properties of cluster
boundaries. In 21st Annual Conference on Learning Theory (COLT 2008), pp. 379-390. Omnipress,
2008.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613-1622, 2015.
Trevor Campbell and Tamara Broderick. Bayesian coreset construction via greedy iterative geodesic
ascent. In International Conference on Machine Learning, pp. 698-706, 2018.
Trevor Campbell and Tamara Broderick. Automated scalable bayesian inference via hilbert coresets.
The Journal of Machine Learning Research, 20(1):551-588, 2019.
David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. Machine
learning, 15(2):201-221, 1994.
Arnaud Doucet, Simon Godsill, and Christophe Andrieu. On sequential monte carlo sampling
methods for bayesian filtering. Statistics and computing, 10(3):197-208, 2000.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050-1059,
2016.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1183-1192.
JMLR. org, 2017.
Mingfei Gao, Zizhao Zhang, Guo Yu, Sercan O Arik, Larry S Davis, and Tomas Pfister. Consistency-
based semi-supervised active learning: Towards minimizing labeling cost. ECCV, 2020.
Daniel Golovin, Andreas Krause, and Debajyoti Ray. Near-optimal bayesian active learning with
noisy observations. In Advances in Neural Information Processing Systems, pp. 766-774, 2010.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Sariel Har-Peled and Soham Mazumdar. On coresets for k-means and k-median clustering. In
Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pp. 291-300,
2004.
Neil Houlsby, Ferenc Huszðr, ZoUbin Ghahramani, and Mgte Lengyel. Bayesian active learning for
classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.
Khaled Jedoui, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Deep bayesian active learning for
multiple correct outputs. arXiv preprint arXiv:1912.01119, 2019.
Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch
acquisition for deep bayesian active learning. In Advances in Neural Information Processing
Systems, pp. 7024-7035, 2019.
10
Under review as a conference paper at ICLR 2021
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in neural information processing
systems,pp. 6402-6413, 2017.
Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser-Nam Lim, and Christopher De Sa.
Differentiating through the frechet mean. ICML, 2020.
Feiping Nie, Hua Wang, Heng Huang, and Chris Ding. Early active learning via robust representation
and structured sparsity. In Twenty-Third International Joint Conference on Artificial Intelligence,
2013.
Michael Osborne, Roman Garnett, Zoubin Ghahramani, David K Duvenaud, Stephen J Roberts, and
Carl E Rasmussen. Active learning of model evidence using bayesian quadrature. In Advances in
neural information processing systems, pp. 46-54, 2012.
Hae-Sang Park and Chi-Hyuck Jun. A simple and fast algorithm for k-medoids clustering. Expert
systems with applications, 36(2):3336-3341, 2009.
Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hernandez-Lobato. Bayesian batch
active learning as sparse subset approximation. In Advances in Neural Information Processing
Systems, pp. 6356-6367, 2019.
Harold J Price and Allison R Manson. Uninformative priors for bayes’ theorem. In AIP Conference
Proceedings, volume 617, pp. 379-391, 2002.
Nicholas Roy and Andrew McCallum. Toward optimal active learning through monte carlo estimation
of error reduction. ICML, Williamstown, pp. 441-448, 2001.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL
https://openreview.net/forum?id=H1aIuk-RW.
Rodney W Strachan and Herman K Van Dijk. Bayesian model selection with an uninformative prior.
Oxford Bulletin of Economics and Statistics, 65:863-876, 2003.
Min Tang, Xiaoqiang Luo, and Salim Roukos. Active learning for statistical natural language parsing.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pp.
120-127. Association for Computational Linguistics, 2002.
Stephen A Vavasis. Approximation algorithms for indefinite quadratic programming. Mathematical
Programming, 57(1-3):279-311, 1992.
Zengmao Wang, Bo Du, Weiping Tu, Lefei Zhang, and Dacheng Tao. Incorporating distribution
matching into uncertainty for multiple kernel active learning. IEEE Transactions on Knowledge
and Data Engineering, 2019.
11
Under review as a conference paper at ICLR 2021
A. Two-stage GBALD Algorithm
The two-stage GBALD algorithm is described as follows: 1) construct core-set on ellipsoid (Lines
3 to 13), and 2) estimate model uncertainty with a deep learning model (Lines 14 to 21). Core-set
construction is derived from the max-min optimization of Eq. (10), then updated with ellipsoid
geodesic w.r.t. Eq. (11), where θ yields a geometric probability model w.r.t. Eq. (5). Importing
the core-set into D0 derives the deep learning model to return b informative acquisitions one time,
where θ yields a deep learning model. Ranking those samples, we select b′ samples with the highest
representations as the batch outputs. The iterations of batch acquisitions stop until its budget is
exhaust. The final update on D0 is our acquisition set of AL.
Details of the hyperparameter settings are presented at Appendix C.6.
Algorithm 1: Two-stage GBALD Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Input: Data set X, core-set size NM, batch returns b, batch output b′, iteration budget A.
Initialization: α ― 0, core-set M - 0.
Stage 1 begins:
Initialize θ to yield a geometric probability model w.r.t. Eq. (5).
Perform k-means to initialize U to D0 .
Core-set construction begins by acquiring x↑,
for i ― 1,2,..., NM do
*
Xi 一 arg maxXi∈Du
min0i∈Do - ∣∣Lo - Lll + logp(yi∣Xi,θ) ∙
where
Lo — ∣∑Nι Eyi [logp(yi∣χi,θ)+ H[yi∣xi, U ]]∣.
Ellipsoid geodesic line scales x*: x* — arg mi□χ?.∈du
Update xi* into core-set M: M — xi* ∪ M.
Update N — N - 1.
lxj - [xi + η(x* -xi)]l.
end
Import core-set to update D°: Do — M ∪ U', where U' updates each element of U into their
nearest samples in X.
Stage 2 begins:
Initialize θ to yield a deep learning model.
while α < A do
Return b informative deep learning acquisitions in one budget:
{x*,x*,…,x*} — arg maXχ∈Du H[θ∣Do] - Ey~p(y∣χ,D0)[H[θ∣x, y, Do]].
Rank b′ informative acquisitions with the highest geometric representativeness:
{xtι ,…，xtb' } — arg maxx 养{x；q,…网} p(yi Ixi ,θ).
Update {x* ,...,x*'} into Do： Do — Do ∪ {x*ι ,...,x*'}.
α — α+ 1.
end
Output: final update on Do .
B. Generalization errors of geodesic search with sphere and
ELLIPSOID
Optimizing with ellipsoid geodesic linearly rescales the spherical search on tighter geometric object.
The inherent motivation is that the ellipsoid can prevent the updates of core-set towards the boundary
regions of the sphere. This section presents generalization error analysis from geometry, which
provides feasible error guarantees for geodesic search with ellipsoid, following the perceptron
analysis. Proofs are presented in Appendix D.
12
Under review as a conference paper at ICLR 2021
B.1	Error bounds of geodesic search with sphere
Given a perceptron function h ：= wιxι + w2x2 + w3, the classification task is over two classes A
and B embedded in a three-dimensional geometry. Let SA and SB be the spheres that tightly cover
A and B, respectively, where SA is with a center ca and radius Ra, and SB is with a center cb and
radius Rb . Under this setting, generalization analysis is presented.
Theorem 1. Given a linear perceptron function h = w1x1 + w2x2 + w3 that classifies A and B,
and a sampling budget k, with representation sampling over SA and SB, the minimum distances to
the boundaries of SA and SB of that representation data are defined as da and db, respectively. Let
err(h, k) be the classification error rate with respect to h and k, ɪ = arcsinRRda, we then have an
inequality of error:
4Ra - (2Ra + tk )(Ra - tk)2
4R3+4R3
4R3 - (2Rb + tk)(Rb - tk)2
4R3+4Rα
, < err(h,k) <
1
k
where tk
- μk + √4k — ^R + N — μ — √⅛ — nR，μk = (ɪ — φ CoS π )nR3 -
4∏R3 t'
3k , tk
4∏Ra
3k ∙
R+3J—μk+√rμs—πR
π27Rb, and μk = (ɪ —1 CoSsn )πR3 —
	
μ
	
B.2	Error bounds of geodesic search with ellipsoid
Given class A and B are tightly covered by ellipsoid Ea and Eb in a three-dimensional geometry.
Let Ra1 be the polar radius of Ea, {Ra2, Ra3} be the equatorial radii of Ea, Rb1 be the polar radius
of Eb, and {Rb2, Rb3} be the equatorial radii of Eb, the generalization analysis is ready to present
following these settings.
Theorem 2. Given a linear perceptron function h = w1x1 + w2x2 + w3 that classifies A and B,
and a sampling budget k, with representation sampling over Ea and Eb, the minimum distance to
the boundaries of Ea and Eb of that representation data are defined as da and db, respectively. Let
err(h, k) be the classification error rate with respect to h and k, ɪ = arcsin RR-da, we then have an
inequality of error:
4 ∏i Rai - (2Rai + λk)(Raι - λk)2
4 ∏i Rai +4 ∏i Rbi
4 ∏i Rbi — (2Rbi +》k)(Rbi— λk)2
4 ∏i Rbi + 4 ∏i Rai
, < err(h, k) <
1
k
where i = 1, 2, 3, λk
— σk + √线 - ∏!⅛ +	{/— σk	— √1⅛∑	∏3Rι	σk	= (2k-4	—
2π V 4π2	27π3	V 2π V 4π2	27π3 , k 1 3k
笠)∏ ∏i Rai- 4π⅛Ri, λk = R1 +
σk =(曝4 - 甯)∏ ∏i Rbi - 4π⅛Ri
B.3	Probability bounds of achieving a zero generalization error
Let Pr[err(h, k) = 0]Sphere and Pr[err(h, k) = 0]Ellipsoid be the probabilities of achieving a zero
generalization error of geodesic search with sphere and ellipsoid, respectively, we present their
inequality relationship.
Theorem 3. Based on γ-tube theory (Ben-David & Von Luxburg, 2008) of clustering stability, the
probability of achieving a zero generalization error of geodesic search with ellipsoid can be defined
as the volume ratio of VolSThbee) ∙ Then, we have:
Pr[err(h, k) = 0]Sphere =
1 —度
R3 ,
a
13
Under review as a conference paper at ICLR 2021
where tk keeps consistent with Theorem 1.
Theorem 4. Based on γ-tube theory (Ben-David & Von Luxburg, 2008) of clustering stability, the
probability of achieving a zero generalization error of geodesic search with ellipsoid can be defined
as the volume ratio of VVoETpbed) ∙ Then, we have：
Pr[err(h, k) = 0]ElliPsoid = 1 -:吗。R
Ra1 Ra2 Ra3
where λki
冬+V一舞+√∏mi
+
π3Ra3i
-27∏3i， and σki
2k-4
3k
πRai ʌ _ D R 4	_ 4πRb1 Rb2 Rb3
2φ )πRaι Ra2 Ra3	3k :
i = 1, 2, 3.
B.4	Main theoretical results of geodesic search with ellipsoid
Tighter lower error bound
Let lower[err(h, k)]Sphere and lower[err(h, k)]Ellipsoid be the lower bounds of the generalization
errors of geodesic search with sphere and ellipsoid, respectively. With Ra1 < Ra, compare Theorem
1 and Theorem 2, we have the following proposition.
Proposition 1. Given a linear perceptron function h = w1x1 + w2x2 + w3 that classifies A and
B, and a sampling budget k. With representation sampling over Sa and Sb, let err(h, k) be the
classification error rate with respect to h and k, the lower bounds of geodesic search with sphere and
ellipsoid satisfy: lower[err(h, k)]Ellipsoid < lower[err(h, k)]Sphere.
Higher probability of achieving a zero error
Let Pr[err(h, k) = 0]Sphere and Pr[err(h, k) = 0]Ellipsoid be the probabilities of achieving a zero
generalization error of geodesic search with sphere and ellipsoid, respectively. Their relationship is
presented in Proposition 2.
Proposition 2. Given a linear perceptron function h = w1x1 + w2x2 + w3 that classifies A and
B, and a sampling budget k. With representation sampling over Ea and Eb, let err(h, k) be the
classification error rate with respect to h and k, the probabilities of geodesic search with sphere and
ellipsoid satisfy: Pr[err(h, k) = 0]Ellipsoid > Pr[err(h, k) = 0]Sphere.
With these theoretical results, geometric interpretation of geodesic search over ellipsoid is more
effective than sphere due to tighter lower error bound and higher probability to achieve zero error.
Theorems 6 and 7 of Appendix D then present a high-dimensional generalization for the above
theoretical results in terms of the volume functions of sphere and ellipsoid.
C. Experiments
C.1 Baselines
To evaluate the performance of GBALD, several typical baselines from the latest deep AL literatures
are selected.
• Bayesian active learning by disagreement (BALD) (Houlsby et al., 2011). It has been
introduced in Section 3.
• Maximize Variation Ratio (Var) (Gal et al., 2017). The algorithm chooses the unlabeled data
that maximizes its variation ratio of the probability:
x* = arg max {1 - max Pr(y∣, x, Do))}.
x∈Du	y∈Y
(15)
• Maximize Entropy (Entropy) (Gal et al., 2017). The algorithm chooses the unlabeled data
that maximizes the predictive entropy:
x* = arg max{ - ∑ Pr(y∣x, D0))log(Pr(y∣x, D0))}.
x∈Du	y∈Y
(16)
14
Under review as a conference paper at ICLR 2021
Table 4: Mean±std of active acquisitions on SVHN with 5,000 and 10,000 repeated samples.
Algorithms	Accuracies 0 repeated samples 5,000 repeated samples	10,000 repeated samples
Var BALD GBALD	-0.8535±0.1098	0.8478±0.1074	0.8281±0.1082 0.8510±0.1160	0.8119±0.1216	0.7689±0.1288 0.8885±0.1054	0.8694±0.1032	0.8630±0.1002
• k-modoids (Park & Jun, 2009). A classical unsupervised algorithm that represents the input
distribution with k clustering centers:
*	*	*、
{x1,x2,...,xfc} = arg min { ∑ ∑ Ilxi - z∕∣},	(17)
z1,z2,...,zk i=1 zi∈Xk
where Xk	denotes the k-th subcluster centered with zi, and zi ∈ X , ∀i..
• Greedy k-centers (k-centers) (Sener & Savarese, 2018). A geometric core-set interpretation
on sphere. See Eq. (4).
• BatchBALD (Kirsch et al., 2019). A batch extension of BALD which incorporates the
diversity, not maximal entropy as BALD, to rank the acquisitions:
{xt1 , ..., xtb } = arg max H(yt1 ,  , ytb ) - Ep(θ∣D0) [H(yt1 ,  ytb ∣θ)],
xt1 ,...,xtb
(18)
where H(yt1 ,  , ytb) denote the expected entropy over all possible labels from
yt1	to	ytb	such that	H(yt1, ..., ytb)	=	Ep(yt1,	...ytb)[-logp(yt1,	...ytb],	and
Ep(θ∣D0) [H(yt1 ,  ytb ∣θ)] is estimated by MC sampling (Roy & McCallum, 2001) (Osborne
et al., 2012) a subset from X which approximates the parameter distributions of θ.
The parameter settings of Eq. (5) are R0 = 2.0e + 3 and η =0.9. Accuracy of each acquired dataset
size of the experiments are averaged over 3 runs.
C.2 Active acquisitions with repeated samples
Repeatedly collecting samples in the establishment of a database is very common. Those repeated
samples may be continuously evaluated as the primary acquisitions of AL due to the lack of one or
more kinds of class labels. Meanwhile, this situation may lead the evaluation of the model uncertainty
to fall into repeated acquisitions. To respond this collecting situation, we compare the acquisition
performance of BALD, Var, and GBALD using 5,000 and 10,000 repeated samples from the first
5,000 and 10,000 unlabeled data of SVHN, respectively. In addition, the unsupervised algorithms
which do not interact with the network architecture, such as k-medoids and k-centers, have been
shown that they cannot accelerate the training in terms of the experiment results of Section 5.3. Thus,
we are no longer studying their performance. The network architecture still follows the settings of
the MLP as Section 5.3.
The acquisition results over the repeated SVHN datasets are presented in Figure 7. The batch sizes
of the compared baselines are 100, where GBALD ranks 300 acquisitions to select 100 data for
the training, i.e. b = 300, b′ = 100. The mean±std values of these baselines of the breakpoints (i.e.
{0, 100, 200, ..., 10000}) are reported in Table 4. Results demonstrate that GBALD shows slighter
perturbations on repeated samples than Var and BALD because it draws the core-set from the input
distribution as the initial acquisition, leading small probability to sample from one or more fixed
class. In GBALD, the informative acquisitions constrained with geometric representations further
scatter the acquisitions spread in different classes. However, Var and BALD algorithms have no
particular schemes against the repeated acquisitions. The maximizer on the model uncertainty may
be repeatedly produced by those repeated samples. In additional, the unsupervised algorithms such
as k-medoids and k-centers don not have these limitations, but cannot accelerate the training since
there has no interactions with the network architecture.
15
Under review as a conference paper at ICLR 2021
(a) Var
(b) BALD
Figure 7: Active acquisitions on SVHN with 5,000 and 10,000 repeated samples.
(c) GBALD
(a) Var
(b) BALD
Figure 8: Active noisy acquisitions on SVHN with 5,000 and 10,000 noisy labels.
(c) GBALD
C.3 Active acquisitions with noisy samples
Noisy labels (Golovin et al., 2010) (Han et al., 2018) are inevitable due to human errors in data
annotation. Training on noisy labels, the neural network model will degenerate its inherent properties.
To assess the perturbations of the above acquisition algorithms against noisy labels, we organize
the following experiment scenarios: we select the first 5,000 and 10,000 samples respectively from
the unlabeled data pool of the MNIST dataset and reset their labels by shifting {‘0’,‘1’,...,‘8’} to
{‘1’,‘2’,...,‘9’}, respectively. The network architecture follows MLP of Section 5.3. The selected
baselines are Var and BALD.
Figure 7 presents the acquisition results of those baseline with noisy labels. The batch sizes of
the compared baselines are 100, where GBALD ranks 300 acquisitions to select 100 data for the
training, i.e. b = 300, b′ = 100. Table 5 presents the mean±std values of the breakpoints (i.e.
{0, 100, 200, ..., 10000}) over learning curves of Figure 8. The results further show that GBALD has
smaller noisy perturbations than other baselines. For Var and BALD, model uncertainty leads high
probabilities to sample those noisy data due to their greatly updating on the model.
C.5 Acceleration of accuracy
Accelerations of accuracy i.e. the first-orders of breakpoints of the learning curve, describe the
efficiency of the active acquisition loops. Different to the accuracy curves, the acceleration curve
reflects how active acquisitions help the convergence of the interacting DNN model.
16
Under review as a conference paper at ICLR 2021
Table 5: Mean±std of active noisy acquisitions on SVHN with 5,000 and 10,000 noises.
Algorithms	0 noises	Accuracies 5,000 noises	10,000 noises
Var	0.8535±0.1098	0.7980±0.1203	0.7702±0.1238
BALD	0.8510±0.1160	0.8205±0.1185	0.7849±0.1239
GBALD	0.8885±0.1054	0.8622±0.0991	0.8301±0.0916
We thus firstly present the acceleration curves of different baselines on MNIST, SVHN, and CIFAR10
datasets following the experiments of Section 5.3. The acceleration curves of active acquisitions
are drawn in Figure 9. Observing those acceleration curves of different algorithms clearly finds
that, GBALD always keeps higher accelerations of accuracy than the other baselines against the
three benchmark datasets. This revels the reason of why GBALD can derive more informative and
representative data to maximally update the DNN model.
The acceleration curves of active acquisitions with repeated samples of Appendix C.2 are presented
in Figure 10. As the shown in this figure, GBALD presents slighter perturbations to the number of
repeated samples than that of Var and BALD due to its effective ranking scheme on optimizing model
uncertainty of DNN. The acceleration curves of active noisy acquisitions of Section Appendix C.3
are drawn in Figure 11. Compared to Figure 7, it presents more intuitive descriptions for the noisy
perturbations to different baselines. With horizontal comparisons to acceleration curves of Var and
BALD, our proposed GBALD has smaller noisy perturbations due to 1) the powerful core-set which
properly captures the input distribution, and 2) highly representative and informative acquisitions of
model uncertainty.
10 6 4 2 0
Aoe.Inooe J。
-0.02
0	100	200	300	400	500	600
Acquired dataset size
(a) MNIST
-Var,Batchsize=1
—BALD,Batchsize=1
—Entropy,Batchsize=1
—k-medoids
-k-centers
-GBALD,Batchsize=3
42186420
""qmmmm
Aoe.Inooe J。
-VarBatchsiZe=100
— BALD,BatchsiZe=100
一Entropy,Batchsize=100
—k-medoids
-k-centers
-GBALD,Batchsize=300
一Var,Batchsize=100
-BALD,Batchsize=100
-Entropy,Batchsize=100
-k-medoids
-k-centers
-GBALD,Batchsize=300
-0.02 --l---l----l---l---l----l---l---l----
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Acquired dataset size
(b) SVHN
-0.02 1------1--------1--------1---------
0	0.5	1	1.5	2
Acquired dataset size	χ104
(c) CIFAR10
0 6 4 2 0
Aoe.Inooe J。

Figure 9:	Accelerations of accuracy of different baselines on MNIST, SVHN, and CIFAR10 datasets.
2.5 ×10-3
2 x10-3
AOe.InOOe J。UO-e」@-@OO<
0	2000	4000	6000	8000	10000
Acquired dataset size
(a) Var
2 x10-3
一BALD,Batchsize=100,0 Receptions
一BALD,Batchsize=100,5000 Receptions
一BALD,Batchsize=100, 100000 Receptions
,51"5ο-05
AOe.InOOe J。UOe」@-@OO<
0	2000	4000	6000	8000	10000
Acquired dataset size
(b) BALD
AOe.InOOe J。UO=e」@-@OO<
2000	4000	6000	8000	10000
Acquired dataset size
(c) GBALD
Figure 10:	Accelerations of accuracy of active acquisitions on SVHN with 5,000 and 10,000 repeated
samples.
17
Under review as a conference paper at ICLR 2021
—Var,Batchsize=100,0 Noises
一Var,Batchsize=100,5000 Noises
一Var,Batchsize=100, 100000 Noises
0 5 0 5 0
Aoe-Jnoo< J。uo-e」@-@oo<
-51---------1--------1---------1---------1---------
0	2000	4000	6000	8000	10000
Acquired dataset size
(a) Var
,51"5o-"5
Aoe-Jnoo<o uoe」@-@oo<
-1
0	2000	4000	6000	8000	10000
Acquired dataset size
(b) BALD
Aoe.Inooe J。
—GBALDBatChSiZe=300, 0 Noises
—GBALDBatChSiZe=300,5000 Noises
—GBALDBatChSiZe=300, 100000 Noises
-1 -----1------1------1------1------
0	2000	4000	6000	8000	10000
Acquired dataset size
(C) GBALD
Figure 11:	ACCelerations of aCCuraCy of aCtive noisy aCquisitions on SVHN with 5,000 and 10,000
noisy labels.
Table 6: Relationship of aCCuraCies and sizes of Core-set on SVHN.
Size of core-set	Start accuracy	Accuracies Ultimate accuracy	Mean±std accuracy
NM = 1,000	08790	0.9344	0.9134±0.0169
NM = 2,000	0.8898	0.9212	0.9151±0.0148
NM = 3,000	0.8848	0.9364	0.9173±0.0138
NM = 4,000	0.8811	0.9271	0.9146±0.0165
NM = 5,000	0.8959	0.9342	0.9197±0.0117
C.6 Hyperparameter Settings
What is the proper time to start aCtive aCquisitions using Eq. (14) in GBALD framework? Does the
ratio of Core-set and model unCertainty aCquisitions affeCt the performanCe of GBALD?
We discuss the key hyperparameter of GBALD here: core-set size NM. Table 6 presents the
relationship of aCCuraCies and the sizes of Core-set, where the start aCCuraCy denotes the test aCCuraCy
over the initial Core-set, and the ultimate aCCuraCy denotes the test aCCuraCy over up to Q = 20, 000
training data. Let b = 1000, b′ = 500 in GBALD, NM be the number of the Core-set size, the iteration
budget A of GBALD then can be defined as A = (Q - NM )/b'. For example, if the number of the
initial core-set labels are set as NM = 1,000, We have A = (Q - NM)W ≈ 38; ifNM = 2,000, then
A = (Q -NM)/b' ≈ 36.
From Table 6, GBALD algorithm keep stable accuracies over the start, ultimate, and mean±std
accuracies When there inputs more than 1,000 core-set labels. Therefore, draWing sufficient core-set
labels using Eq. (10) to start the model uncertainty of Eq. (14) can maximize the performance of our
GBALD frameWork.
Hyperparameter settings on batch returns b and bath outputs b′. Experiments of Sections 5.1
and 5.2 used different b and b′ to observe the parameter perturbations. No matter What the settings of
b′ and b are, GBALD still outperforms BALD. For single acquisition of GBALD, We suggest b = 3
and b′ = 1. For bath acquisitions, the settings on b′ and b are user-defined according the time cost and
hardWare resources.
Hyperparameter setting on iteration budget A. Given the acquisition budget Q, let b′ be the
number of the output returns at each loop, NM be the number of the core-set size, the iteration budget
A of GBLAD then can be defined as A = (Q - NM )/b′.
Other hyperparameter settings. Eq. (5) has one parameter R0 Which describes the geometry prior
from probability. The default radius of the intern balls R0 is used to legalize the prior and has no
further influences on Eq. (10). It is set as R0 = 2.0e + 3 for those three image datasets. Ellipsoid
18
Under review as a conference paper at ICLR 2021
geodesic is adjusted by η which controls how far of the updates of core-set to the boundaries of the
distributions. It is set as η = 0.9 in this paper.
C.7 TWO-SIDED t-TEST
We present two-sided (two-tailed) t-test for the learning curves of Figure 5. Different to the mean±
std of Table 1, t-test can enlarge the significant difference of those baselines. In typical t-test, the two
groups of observations usually require a degree of freedom smaller than 30. However, the numbers of
breakpoints of MNIST, SVHN, and CIFAR10 are 61, 101, and 201, respectively, thereby holding
a degree of freedom of 60, 100, 200, respectively. It is thus we introduce t-test score to directly
compare the significant difference of pairwise baselines.
t-test score between any pair group of breakpoints are defined as follows. Let B1 = {α1 , α2, ..., αn}
and B2 = {β1, β2, ..., βn}, there exists t-score of
μU
t - score = √n-,
σ___________________
Where μ = n ∑n=ι(αi- Bi), and σ = ∖Jn-ι ∑n=ι(αi- βi- μ)2.
In two-sided t-test, B1 beats B2 on breakpoints αi and βi satisfying a condition of t - score > ν;
B2 beats B1 on breakpoints αi and βi satisfying a condition of t - score < -ν, Where ν denotes the
hypothesized criterion With a given confidence risk. FolloWing (Ash et al., 2019), We add a penalty
of e to each pair of breakpoints, which further enlarges their differences in the aggregated penalty
matrix, Where e denotes the number of B1 beats B2 on all breakpoints. All penalty values finally
calculate their L1 expressions.
Figure 12 presents the penalty matrix over learning curves of Figure 5. Column-wise values at
the bottom of each matrix show the overall performance of the compared baselines. As the shown
results, GBALD has significant performances than that of the other baselines over the three datasets.
Especially for SVHN, it has superior performance.
⑶ MNIST
Var BALD Entropy k-medoids k-centers GBALD
Var
BALD
Entropy
k-medoids
k-centers
GBALD
0.7556	2.3351	0.7023 7.4051	0.9958	77.2318
(b) SVHN
Figure 12:	A pairwise penalty matrix over active acquisitions on MNIST, SVHN, and CIFAR10.
Column-wise values at the bottom of each matrix show the overall performance of the compared
baselines (larger value has more significant superior performance).
D. Proofs
We firstly present the generalization errors of k = 3 ofAL with sphere as a case study. The assumption
of the generalization analysis is described in Figure 13.
19
Under review as a conference paper at ICLR 2021
Figure 13: Assumption of the generalization analysis. The ball above h denotes Sb, the ball below h
denotes Sa , and Rb < Ra . Spherical cap1 of the half-sphere of Sa is denoted as D.
D.1 CASE STUDY OF GENERALIZATION ANALYSIS OF err(h, 3) OF GEODESIC SEARCH WITH
SPHERE
Theorem 5. Given a linear perceptron function h = w1x1 + w2x2 + w3 that classifies A and B,
and a sampling budget k. With representation sampling over Sa and Sb, the minimum distance to
the boundaries of SA and SB of that representation data are defined as da and db, respectively. Let
err(h, k) be the classification error rate with respect to h and k, π = arcsin Raτ~da, we have an
ψ	Ra
inequality of error:
min
(2Ra +1)(Ra-1)2 (2Rb +1')(Rb-1')2
-4Ra + 4R3-, -4R3 + 4Ra-
• < err(h, 3) < 0.3334,
t' = R2 +
where t =
πRa+∖Z-T-√4∣z-π7Ia=,μ=(2- 1 cosπ)nRa- 4πRb
3J- 2μπ + √μ2 - π27Rb + N-受-√∏2 - πRb, and μ = (2 - 1 CoS π )πR3
InR3
9

Proof. Given the unseen acquisitions of {q。，qb, q*}, where q。∈ A, qb ∈ B, and q* ∈ A or q* ∈ B
is uncertain. However, the position of q* largely decides h. Therefore, the proof studies the error
bounds highly related to q* in terms of two cases: Ra ≥ Rb and Ra < Rb .
1) If Ra ≥ Rb, q* ∈ A. Estimating the position of q* starts from the analysis on qa . Given the volume
function Vol(∙) over the 3-D geometry, We know: Vol(A) = 4∏Ra and Vol(B) = 4∏R3. Given k = 3
over Sa and Sb, we define the minimum distance of qa to the boundary of A as da . Let Sa be cut
off by a cross section h′, where C be the cut and D be the spherical cap1 of the half-sphere (see
Figure 12) that satisfy
VOl(C) = 3πRa - VOl(D) = 4π(Ra9+ R),	(19)
and the volume of D is	____________
r2∏√Ra-(Ra-da)2 arcsinRRd
Vol(D) = ∏( √Ra -(Ra -da) )2(Ra- da) + /	-X^∏~ ∏R dx
0	2π
-	-	arcsinRa-d	-	________________ 小人、
=∏(Ra - (Ra - da)2)(Ra- da) +	X KRa(NR -(Ra- da )2 )	(20)
2π
= ∏(Ra - (Ra - da)2)(Ra- da) + ∏RaarcsinRaD da √Ra -(Ra- da)2.
Ra
1https://en.wikipedia.org/wiki/Spherical_cap
20
Under review as a conference paper at ICLR 2021
Let π = arcsin Ra-da, Eq.(20) can be written as
ψ	Ra
Vol(D) = π(R2 -(Ra- da)2)(Ra- da) +-----a cos — .	QI)
夕 夕
Introducing Eq. (21) to Eq. (19), we have
(S----cos - )πR3 - π(Ra -(Ra- da)2)(Ra- da) =
3 φ φ
4∏(R3 + R3)
9
Let t = Ra - da, Eq. (22) can be rewritten as
∏t3 - ∏R21 + (2 - 1cosπ)∏R3 - 4πR3 = 0.
a '9 g g a 9
To simplify Eq. (23), let μ = (2 - 1 cosIn )πRa - 4πRb, Eq. (23) then can be written as
πt3 - ∏Rat + μ = 0.
(22)
(23)
(24)
The positive solution of t can be
t=	.Rl	3 + N	-2Π+∖		π3R3 a	3 + N	-比-y		π3R3 a
	3			4 4π2	-27π3		2π	'	4 4π2	-27π3
voi(D)=2 ∏Ra - 4π(Ra9+ R3)
=2 πRa - 9 πR3 > 0.
(25)
Based on Eq. (19), we know
(26)
Thus, 3∕2Rb < Ra. We next prove q* ∈ A. Based on Eq. (26), we know
∏R < 1 ∏R.	(27)
Then, the following inequalities hold: 1)2πR3 < ∏Ra, 2) 2πR < 3∏Ra, and 3) 2πR + 1 πR <
3πRa + 1 πR3. Finally, We have
∏R < 1 (∏Ra + ∏R3).	(28)
Therefore, Vol(B) < 3(Vol(A) + Vol(B)). We thus know: 1) q& ∈ A and it is with a minimum
distance da to the boundary of Sa, 2) qb ∈ B, and 3) q* ∈ A. Therefore, class B can be deemed as
having a very high probability to achieve a nearly zero generalization error and the position of q*
largely decides the upper bound of the generalization error of h.
In Sa that covers class A, the nearly optimal error region can be bounded as the spherical cap of Sa
with a volume constraint of Vol(A) - Vol(C). We thence have an inequality of
VoI(A)- VoI(C) < err
Vol(A) + Vol(B)
We next calculate the volume of the spherical cap:
(h, 3) < 1.
πx2d y
(29)
Vol(A) - Vol(C) =
Ra -da
∫-Ra
3 πRa- πda(Ra- -3a).
(30)
Eq. (29) then is rewritten as
3πRa - π (3Ra- da)da
3 πRa+4 πR3
Then, we have the error bound of
4R3a - (3Ra - da)d2a
< err(h, 3) < 0.3334,
4Ra + 4R3
< err(h, 3) < 0.3334.
(31)
(32)
21
Under review as a conference paper at ICLR 2021
Introducing da = Ra - t, Eq. (32) is written as
4R* 3 - (2Ra + t)(Ra -t)2
a	4Ra + 4R3	< err(h, 3)< 0.3334.	(33)
2) With another assumption of Ra < Rb, We follow the same proof skills of Ra ≥ Rb and know
4R3-般⅞b)d2 < e”(h, 3)< 0.3334, ie where db = Rb-1' and t' = R + N-舞 + √∏二要 +
√√- 2∏ - √4∏∏22 - π27Rb, and μ = ( 2 - 1 cos π )πR33 - 4πRa.
We thus conclude that min1 4Ra-(4Ra+4RRa-t)2,"陪陋麒飞乎—)2 ∙ < err(h, 3) < 0.3334.	□
4R +4R	4R +4R
D.2 Proof of Theorem 1
We next present the generalization errors against an agnostic sampling budget k following the above
proof technique.
Proof. The proof studies two cases: Ra ≥ Rb and Ra < Rb. 1) If Ra ≥ Rb, we estimate the optimal
position of q& that satisfies q& ∈ A. Given the volume function Vol(∙) over the 3-D geometry, we
know: Vol(A)= 萼Ra and Vol(B)= 萼R3. Assume q& be the nearest representative data to the
boundary of S a, qb be the nearest representative data to the boundary of Sb and q* be the nearest
representative data to h either in Sa or Sb . Given the minimum distance of qa to the boundary of A
as da. Let Sa be cut off by a cross section h′, where C be the cut and D be the spherical cap of the
half-sphere that satisfy
VOl(C) = 2πR - VOl(D) = 4π(Ra； Rb),	(34)
3 a	3k
and the volume of D is	___________
,--------------„	r2π√Ra-(Ra-da)2 arcsinRR-da	„
Vol(D) = ∏( √Ra -(Ra- da)2 )2(Ra- da) + /	—X-3∏- TR dx
0	2π
a	a	arcsin Ra-da	C -------------------
=∏(R - (Ra - da)2)(Ra- da) + —^~3∏~ ^欣曲 y/R -(Ra- da )2 )
2π
=∏(Ra - (Ra - da)2)(Ra- da) + ∏Ra arcsin Ra D da √R -(Ra- da)2 ∙
Ra
Let - = arcsinRa-da, Eq. (36) can be written as
φ	Ra
Vol(D) = ∏(R2 - (Ra - da)2)(Ra- da) +---a cos — .
夕 夕
Introducing Eq. (36) to Eq. (34), we have
(2 - 1cosπ )∏R3 -MR - (Ra - da)2)(Ra - da) = 4^^ j Rb).
3 φ φ	3k
(35)
(36)
(37)
Let tk = Ra - da, we know
πt3 - πR21 + (2kr^ -1 cosπ )πR3 - --Rb- = 0.	(38)
3k	φ φ	3k
To simplify Eq. (38), let μk = (23-4 - 1 cos-)∏R33 - 4nR, Eq. (38) then can be written as
πt3 - πRt + μk = 0.	(39)
The positive solution of tk can be
R2	3 ΓμΓ^J^Γ^3R3 +{ μk / μk	∏3Ra	(40)
=+	∖-∖	+	.
3	2π	4π2	27π3	2π	4π2	27π3
22
Under review as a conference paper at ICLR 2021
Based on Eq. (35), we know
Vol(D) =
2πR3 - 4∏(Ra + R)
3 a	3k
=2k-4∏R - ɪ∏R > 0.
3k a 3k	b
Thus, 3/k-2 Rb < Ra. We next prove q* ∈ A. According to Eq. (41), We know
πR3 < -2^^ πR3.
(41)
(42)
Then, the following inequalities hold: 1)总∏R < ∏Rl, 2)(k-2)k∏R < 1 ∏Rl, and 3)(k-2)k∏R +
k⅛2 πR3 <1 πRa+ k⅛鬃 πR3. Finaiiy, we have:
πRb3
1	3 k2 - 2k - 2	3
<	i + Tk-^T 叫
=Zn(Ra + R) - TL^^^η∑π
k a	(k - 2)k
<	Zn(Ra+R3).
ka
(43)
Therefore, Vol(B) < 1 (Vol(A) + Vol(B)). We thus know: 1) q& ∈ A and it is with a minimum
distance da to the boundary of Sa, 2) qb ∈ B, and 3) q* ∈ A. Therefore, class B can be deemed as
having a very high probability to achieve a zero generalization error and the position of q* largely
decides the upper bound of the generalization error of h.
In Sa that covers class A, the nearly optimal error region can be bounded as Vol(A) - Vol(C). We
then have the inequality of
Vol(A) - Vol(C)	1
Vol(A)+ Vol(B) < err(h,k) < k
Based on the volume equation of the spherical cap in Eq. (30), we have
3nRa - 3 (3Ra - da)d
3 nRa + 4 nR3
Then, we have the error bound of
4Ra - (3Ra - da)da
-4R3 + 4R3-
Introducing da = Ra - tk, Eq. (46) is written as
—< err(h, k) <
k
< err(h, k) < ɪ.
4Ra -(2Ra +	(Ra-针 <	< 1 .
4R3 + 4Ra	v , < k
(44)
(45)
(46)
(47)
2) With another assumption of Ra < Rb, we follow the same proof skills of Ra ≥ Rb and know
4R3-R3R4Rab)d2 < err(h, k) < 1, where db = Rb - t'k and t'k = r2 + √-μk + √μ2 - π27r3 +
3 Λμk -√μkL	_ ∏3r3 and	〃，=	(2k-4	_ 1 Cosπ )nR3 _	4∏Ra
V 2π V 4π2	27π3 , and	μk =	( 3k W CoS W )nRb 3k .
We thus conclude that min, 4Ra-(2RR+：4RRa-tk)2, 4R3(2R点RRLtk)2 . < err(h, k) < ɪ.	□
4Ra +4Rb	4Rb +4Ra	k
D.3 Proof of Theorem 2
Proof. Given class A and B are tightly covered by ellipsoid Ea and Eb in a three-dimensional
geometry. Let Ra1 be the polar radius of Ea, {Ra2, Ra3 } be the equatorial radii of Ea, Rb1 be
polar radius of Eb, and {Rb2 , Rb3 } be the equatorial radii of Eb. Based on Eq. (10), we know
Rai < Ra, Rbi < Rb, ∀i, where Ra and Rb are the radii of the spheres over the class A and B,
respectively. We follow the same proof technique of Theorem 1 to present the generalization errors
of AL with ellipsoid.
23
Under review as a conference paper at ICLR 2021
The proof studies two cases: Ra、≥ Rb and Ra、< R^. 1) If Ra、≥ Ry, q* ∈ A. Given the
volume function Vol(∙) over the 3-D geometry, We know: Vol(A) = 4∏Ra、Ra? Ra3 and Vol(B)=
43∏ Rbi Rb2 Rb3. Given the minimum distance of q& to the boundary of A as da. Let Ea by cut off by
a cross section h′, where C be the cut and D be the ellipsoid cap of the half-ellipsoid that satisfy
Vol(C) = 2πRaR2Ra - Vol(D) = 4n(Rai Ra2 Ra3 + RbI Rb2Rbb3,	(48)
3 a a a	3k
and the volume of D is approximated as
Ra、 -da
I--------------- q	GnRa2 Rab arcsin T-	q
Vol(D) ≈ ∏(NRaI-(RaI- da)2)2(Raι - da) + /	——a、∏Ra1 dx
=π(Ra1
- (Ra、 - da)2)(Ra、
Ra、 -da
arcsin —、-- C
-da) + ——0 Ra nR2、(∏Ra2 Rab )
(49)
=π(Ra1
-(Ra1 - da)2)(RaI - da) + $πRaI Ra2 Rab arcsin —OR-a
Let ɪ = arcsin RR -da, Eq. (49) can be written as
a、	π2
Vol(D) = ∏(%-(Ra∖- da)2)(Ra、- da) + %Ra、Ra2 Rab .	(50)
Introducing Eq. (50) to Eq. (48), we have
(2 - πRΓ )nRa、Ra2 Rab- n(RaI-(RaI
- da)2)(Ra、
- da ) =
4n(Ra、Ra2 Rab + Rb、Rb2 Rbb )
3k
(51)
Let λk = Ra、 - da, we know
∏λk - ∏R2λk + (2k-4 -，)nRa、Ra2Rab - 4nRbeb2 Rbb = 0.	(52)
3k	2φ	3k
To simplify Eq. (52), let σk = (2k-4 - π-Ra^-)nRa、Ra2 Rab- 4VRbIRb Rbb, Eq. (52) then can be
written as
πλk - πRaI λk + σk = 0.	(53)
The positive solution of λk can be
λk =殳 + {I- σ + √<HW + x I- σ-√<HW.	(54)
3	N 2π V 4π2	27π3	N 2π V 4π2	27π3
The remaining proof process follows Eq. (40) to Eq. (46) of Theorem 1. We thus conclude that
4 ∏i Rai - (2RaI + λk )(RaI - λk)2
4 ∏i Rai +4 ∏i Rbi
4 ∏i R%-(2Rb、+ Xk)(Rb、- λk)2
4 ∏i Rbi + 4 ∏i Rai
Y err(h, k) <
1
k
(55)
where i = 1,2,3, λk
学+
4∏⅛ - ~27∏Γ, and σk = ( ɪ -
2RbL )nRb、Rb2 Rbb - 4KRaIRI2 Rab ∙ In a simple way, Ra、Ra2 Rab and Rb、Rb2 Rbb can be written as
∏i Rai and ∏i Rbi, i=1, 2, 3, respectively.
□
D.4 Proof of Theorem 3
Proof. In clustering stability, γ-tube structure that surrounds the cluster boundary largely decides the
performance of a learning algorithm. Definition of γ-tube is as follows.
Definition 1. γ-tube Tubeγ (f) is a set of points distributed in the boundary of the cluster.
TUbeY(f) ：= {x ∈ X∣'(x, B(f)) ≤ γ},	(56)
where X is a noise-free cluster with n samples, B(f) ：= {x ∈ X,f is discontinuous at x }, f is a
clustering function, and '(∙, ∙) denotes the distance function.
24
Under review as a conference paper at ICLR 2021
Following this conclusion, representation data can achieve the optimal generalization error if they are
spread over the tube structure. Let γ = da, the probability of achieving a nearly zero generalization
error can be expressed as the volume ration of γ-tube and Sa :
Vol(Tubeγ)
Pr[err(h,k) = 0] Sphere = Vol(S)
3πR3 - 3 (Ra- da)3
3∏R3
(57)
=1 - ιk
R3 ,
a
where tk keeps consistent with Eq. (40). With the initial sampling from the tube structure of class A,
the subsequent acquisitions of AL would be updated from the tube structure of class B . If the initial
sampling comes from the tube structure of B, the next acquisition must be updated from the tube
structure of A. With the updated acquisitions spread over the tube structures of both classes, h is easy
to achieve a nearly zero error. Then Theorem 3 is as stated.	□
D.5 Proof of Theorem 4
Proof. Following the proof technique of Theorem 3, volume of the tube is redefined as Vol(Tubeγ)
4∏Raι Ra2 Ra3. Then, we know
Vol(Tubeγ)
Pr[err(h,k) = 0] Ellipsoid = —-
Vol(Ea)
3πRaι Ra2 Ra3 - 3 (RaI- da)(Ra2 - da) (Ra3 - da)
π3R3ai	2k-4
^27∏^，and Oki = (kΓ -
4∏Raι Ra2 Ra3
λk1 λk2 λk3
=1-------------,
Ra1Ra2Ra3
where λki =	- σπ+√ 4∏2 - ∏R3+ √- σπ -
nRai )στ R R R _ 4πRbl Rb2 Rb3 i = 1 2 3
2φ ) π 1 Lai 2La2 1 l∙a3	3k ,= ',乙,O.
Theorem 4 then is as stated.
(58)
□
D.6 Proof of Proposition 1
Proof. Let Cubea tightly covers Sa with a side length of 2R0, and Cubea tightly covers the cut C,
following theorem 1, we know
Then, we know
err(h,k) > Vol(A)- Vol(C) > Cubea- Cubea
' ,'	Vol(A)	Cubea
err(h,k) > πR3『Rd
πR3a
da
=1 - R.
(59)
(60)
Meanwhile, let Cubee tightly covers Ea with a side length of 2Ra1 , Cubee ′ tightly covers C ,
following theorem 1, we know
Then, we know
err(h,k) > Vol(A)- Vol(C) > Cubee- Cubee
' ,'	Vol(A)	Cubee
err(h, k) >
∏Raι Ra2 Ra3 - ∏daRa2 Ra3
πRa1Ra2Ra3
da
1 - 瓦?.
(61)
(62)
Since Ra 1 < Ra, we know 1 - da- > 1 - da-. It is thus the lower bound of AL with ellipsoid is tighter
1	Ra	Ra1
than AL with sphere. Then, Proposition 1 holds.	□
25
Under review as a conference paper at ICLR 2021
D.7 Proof of Proposition 2
Proof. Following the proofs of Theorem 3:
Pr[err(h, k) = 0]SPhere = 1 -去
R3a
=ι - (Ra - da )3
=1I1-Ra )3
(63)
Following the proofs Theorem 4:
Pr[err(h, k) = 0]EUiPsoid = 1 - λk1 13λk3
R3
a1
(Ra1 - da )(Ra2 - da )(Ra3 - da )
Ra
(64)
Based on Proposition 1, 1 - Rda > 1 - Ra, therefore Pr[err(h,k) = 0]SPhere < Pr[err(h, k)=
0] ElliPsoid. Then, Proposition 2 is as stated.	□
D.8 LOWER DIMENSIONAL GENERALIZATION OF THE d-DIMENSIONAL GEOMETRY
With the above theoretical results, we next present a connection between 3-D geometry and d-
dimensional geometry. The major technique is to prove that the volume of the 3-D geometry is a
lower dimensional generalization of the d-dimensional geometry. It then can make all proof process
from Theorems 1 to 5 hold in d-dimensional geometry.
Theorem 6.	Let Vold be the volume of a d-dimensional geometry, Vol3(Sa) is a low-dimensional
generalization of Vold(Sa).
Proof. Given Sa over class A is defined with x21 + x22 + x32 = Ra2 . Let x21 + x22 = ra2 be its 2-D
generalization of Sa, assume that x2 be a variable parameter in this 2-D generalization formula, the
volume” (2-D volume is the area of the geometry object) of it can be expressed as
Vol2(Sa) = ∫ ra2 ra2
-ra
- x22 dx2 .
(65)
Let H be an angle variable that satisfies x2 = rasin(∂), We know dx2 = Tacos(ff)d0. Then, Eq. (65)
is rewritten as
∕^^ ∏∕2
Vol2(Sa) = ∫	2ra2cos2(H)dH
-π∕2
π∕2
= ∫	4ra2cos2(H)dH.
(66)
For a 3-D geometry, for the variable x3, it is over a cross-section which is a 2-dimensional ball
(circle), where the radius of the ball can be expressed as racos(H), s.t. H ∈ [0, π]. Particularly, let
Vol3(Sa) be the volume of Sa with 3 dimensions, the volume of this 3-dimensional sphere then can
be written as
π∕2
Vol3(Sa)=∫0
2Vol2(racos(H))ra(cos(H))dH.
(67)
With Eq. (67), volume of a d-dimensional geometry can be expressed as the integral over the
(d-1)-dimensional cross-section of Sa
Volm(Sa) = ∫	2Volm-1 (racos(H))ra(cos(H))dH,	(68)
where Volm-1 denotes the volume of (m-1)-dimensional generalization geometry of Sa.
26
Under review as a conference paper at ICLR 2021
Based on Eq. (68), we know Vol3 can be written as
rπ/2	,
V0l3(Sa) = /	2Vol2(Racos(H))ra(cos" ))d"
Introducing Eq. (66) into Eq. (69), we have
rπ∕2
V0l3(Sa) = /	2Vol2(Racos(夕))ra(cos(夕))d夕
π∕2 π∕2
=∫	∫	8(racos(夕)2cos2(夕)ra(cos(H))d夕 dH
42
=3 πRa s.t. Ra = ra.
(69)
(70)
Therefore, the generalization analysis results of the 3-D geometry still can hold in the high dimensional
geometry. Then,	口
Theorem 7.	Let Vold be the volume of a d-dimensional geometry, Vol3(Ea) is a low-dimensional
generalization of Vold(Ea).
Proof. The integral of Eq. (67) also can be adopted into the volume of Vol3(Ea) by transforming
the area i.e. V0l2(Sa) into V0l2(Ea). Then, Eq.(68) follows this transform.	□
27