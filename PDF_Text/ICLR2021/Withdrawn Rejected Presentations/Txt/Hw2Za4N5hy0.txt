Under review as a conference paper at ICLR 2021
Federated Learning with Decoupled
Probabilistic-Weighted Gradient Aggregation
Anonymous authors
Paper under double-blind review
Ab stract
In the federated learning paradigm, multiple mobile clients train local models in-
dependently based on datasets generated by edge devices, and the server aggre-
gates parameters/gradients from local models to form a global model. However,
existing model aggregation approaches suffer from high bias on both data dis-
tribution and parameter distribution for non-IID datasets, which result in severe
accuracy drop for increasing number of heterogeneous clients. In this paper, we
proposed a novel decoupled probabilistic-weighted gradient aggregation approach
called FeDEC for federated learning. The key idea is to optimize gradient param-
eters and statistical parameters in a decoupled way, and aggregate the parameters
from local models with probabilistic weights to deal with the heterogeneity of
clients. Since the overall dataset is unaccessible by the central server, we intro-
duce a variational inference method to derive the optimal probabilistic weights to
minimize statistical bias. We further prove the convergence bound of the proposed
approach. Extensive experiments using mainstream convolutional neural network
models based on three federated datasets show that FeDEC significantly outper-
forms the state-of-the-arts in terms of model accuracy and training efficiency.
1	Introduction
Federated learning (FL) has emerged as a novel distributed machine learning paradigm that allows
a global machine learning model to be trained by multiple mobile clients collaboratively. In such
paradigm, mobile clients train local models based on datasets generated by edge devices such as
sensors and smartphones, and the server is responsible to aggregate parameters/gradients from local
models to form a global model without transferring data to a central server. Federated learning has
been drawn much attention in mobile-edge computing (Konecny et al. (2016); SUn et al. (2017))
with its advantages in preserving data privacy (Zhu & Jin (2020); Jiang et al. (2019); Keller et al.
(2018)) and enhancing communication efficiency (Shamir et al. (2014); Smith et al. (2018); Zhang
et al. (2013); McMahan et al. (2017); Wang et al. (2020)).
Gradient aggregation is the key technology of federated learning, which typically involves the fol-
lowing three steps repeated periodically during training process: (1) the involved clients train the
same type of models with their local data independently; (2) when the server sends aggregation
signal to the clients, the clients transmit their parameters or gradients to the server; (3) when server
receives all parameters or gradients, it applies an aggregation methods to the received parameters
or gradients to form the global model. The standard aggregation method FedAvg (McMahan et al.
(2017)) and its variants such as FedProx (Li et al. (2020a)), Zeno (Xie et al. (2019)) and q-FedSGD
(Li et al. (2020b)) applied the synchronous parameter averaging method to the entire model in-
discriminately. Agnostic federated learning (AFL) (Mohri et al. (2019)) defined an agnostic and
risk-averse objective to optimize a mixture of the client distributions. FedMA (Wang et al. (2020))
constructed the shared global model in a layer-wise manner by matching and averaging hidden
elements with similar feature extraction signatures. The recurrent neural network (RNN) based
aggregator (Ji et al. (2019)) learned an aggregation method to make it resilient to Byzantine attack.
Despite the efforts that have been made, applying the existing parameter aggregation methods for
large number of heterogeneous clients in federated learning still suffers from performance issues.
It was reported in (Zhao et al. (2018)) that the accuracy of a convolutional neural network (CNN)
model trained by FedAvg reduces by up to 55% for highly skewed non-IID dataset. The work of
1
Under review as a conference paper at ICLR 2021
(Wang et al. (2020)) showed that the accuracy of FadAvg (McMahan et al. (2017)) and FedProx
(Li et al. (2020a)) dropped from 61% to under 50% when the client number increases from 5 to 20
under heterogeneous data partition. A possible reason to explain the performance drops in federated
learning could be the different levels of bias caused by inappropriate gradient aggregation, on which
we make the following observations.
Data Bias: In the federated learning setting, local datasets are only accessible by the owner and
they are typically non-IID. Conventional approaches aggregate gradients uniformly from the clients,
which could cause great bias to the real data distribution. Fig. 1 shows the distribution of the real
dataset and the distributions of uniformly taking samples from different number of clients in the
CIFAR-10 dataset (Krizhevsky (2009)). It is observed that there are great differences between the
real data and the sampled distributions. The more clients involved, the more difference occurs.
0123456789
Classes
Amplitude
Figure 1: The differences between real data Figure 2: The cumulative distribution func-
and sampled datasets (CIFAR-10).	tion of means and variances in BN layers of
different FL models (ResNet18@CIFAR-10).
Parameter Bias: A CNN model typically contains two different types of parameters: the gradient
parameters from the convolutional (Conv) layers and full connected (FC) layers; and the statis-
tical parameters such as mean and variance from the batch normalization (BN) layers. Existing
approaches such as FedAvg average the entire model parameters indiscriminately using distributed
stochastic gradient descent (SGD), which will lead to bias on the means and variances in BN layer.
Fig. 2 shows the means and variances in BN layer distribution of a centrally-trained CNN model
and that of FedAvg-trained models with different number of clients on non-IID local datasets. It
is observed that the more clients involved, the larger deviation between the central model and the
federated learning models.
Our contributions: In the context of federated learning, the problems of data bias and parameter
bias have not been carefully addressed in the literature. In this paper, we propose a novel gradient
aggregation approach called FeDEC. The main contribution of our work are summarized as follows.
(1) We propose the key idea of optimizing gradient aggregation with a decoupled probabilistic-
weighted method. To the best of our knowledge, we make the first attempt to aggregate gradient
parameters and statistical parameters separatively, and adopt a probabilistic mixture model to resolve
the problem of aggregation bias for federated learning with heterogeneous clients. (2) We propose
a variational inference method to derive the optimal probabilistic weights for gradient aggregation,
and prove the convergence bound of the proposed approach. (3) We conduct extensive experiments
using five mainstream CNN models based on three federated datasets under non-IID conditions. It
is shown that FeDEC significantly outperforms the state-of-the-arts in terms of model accuracy and
training efficiency.
2	Related Work
We summarize the related work as two categories: parameter/gradient aggregation for distributed
learning and federated learning.
Distributed Learning: In distributed learning, the most famous parameter aggregation paradigm
is the Parameter Server Framework (Li et al. (2014)). In this framework, multiple servers maintain
a partition of the globally shared parameters and communicate with each other to replicate and
migrate parameters, while the clients compute gradients locally with a portion of the training data,
and communicate with the server for model update. Parameter server paradigm had motivated the
development of numerous distributed optimization methods (Boyd et al. (2011); Dean et al. (2012);
Dekel et al. (2012); Richtdrik & Takgc (2016); Zhang et al. (2015)). Several works focused on
2
Under review as a conference paper at ICLR 2021
improving the communication-efficiency for distributed learning (Shamir et al. (2014); Smith et al.
(2018); Zhang et al. (2013)). To address the issue of model robustness, Zeno (Xie et al. (2019)) was
proposed to make distributed machine learning tolerant to an arbitrary number of faulty workers. The
RNN based aggregator (Ji et al. (2019)) adopted a meta-learning approach that utilizes a recurrent
neural network (RNN) in the parameter server to learn to aggregate the gradients from the workers,
and designed a coordinatewise preprocessing and postprocessing method to improve its robustness.
Federated Learning: Federated learning (Konecny et al. (2015)) is an emerging edge distributed
machine learning paradigm that aims to build machine-learning models based on datasets distribut-
ing across multiple clients. One of the standard parameter aggregation methods is FedAvg (McMa-
han et al. (2017)), which combines local stochastic gradient descent (SGD) on each client with a
server that performs parameter averaging. The lazily aggregated gradient (Lag) method (Chen et al.
(2018)) allowed clients running multiple epochs before model aggregation to reduce communication
cost. For heterogeneous datasets, FedProx (Li et al. (2020a)) modified FedAvg by adding a hetero-
geneity bound on datasets and devices to tackle heterogeneity. The FedMA (Wang et al. (2020))
method, derived from AFL Mohri et al. (2019) and PFNM (Yurochkin et al. (2019)), demonstrated
that permutations of layers can affect the gradient aggregation results, and proposed a layer-wise
gradient aggregation method to solve the problem. For fair resources allocation, the q-FedSGD (Li
et al. (2020b)) method encouraged a more uniform accuracy distribution across devices in federated
networks.
However, all the methods did not differentiate gradient parameters and statistical parameters and
aggregated the entire model in a coupled manner. In this paper, we make the first attempt to decou-
ple the aggregation of gradient parameters and statistical parameters with probabilistic weights to
optimize the global model to achieve fast convergence and high accuracy in non-IID conditions.
3	FeDEC: a Decoupled Gradient Aggregation Method
3.1	Objective of Federated Learning with Non-IID Data
Consider a federated learning scenario with K clients that train their local CNN models indepen-
dently based on local datasets x1, x2, . . . , xK and report their gradients and model parameters to a
central server. The objective of the server is to form an aggregate global CNN model to minimize
the loss function over the total datasets x = {x1, x2, . . . , xK}. Conventional federated learning
tends to optimize the following loss function:
K |x |
mwin L(W, x) ：= E ~ΓτLkk((Wk，Xk)，	⑴
k=1 |x|
where W is the parameters of the global model, Wk (k = 1, 2,…，K) is the parameters of the
k-th local model; L(∙) and Lk (∙) indicate the loss functions for global model and local models
accordingly. The above objective assumes training samples uniformly distributed among the clients,
so that the aggregated loss can be represented by the sum of percentage-weighted of the local losses.
As discussed in section 1, conventional federated learning has two drawbacks. Firstly, local datasets
are collected by mobile devices used by particular users, which are typically non-IID. Training sam-
ples on each client may be drawn from a different distribution, therefore the data points available
locally could be bias from the overall distribution. Secondly, since a neural network model is typ-
ically consists of convolutional (Conv) layers and full-connected (FC) layers that are formed by
gradient parameters, and batch normalization (BN) layers that are formed by statistical parameters
such as mean and variance, aggregating them without distinction will cause severe deviation of the
global model parameters.
To address the above issues, we propose a decoupled probabilistic-weighted approach for federated
learning that focuses on optimizing the following loss function:
K
min L({WtN N, Wmt ean,Wvtar},x) :=	πkLk({WNt-N1,k,Wmt-e1a,nk,Wtv-ar1,k},xk),	(2)
*
k = 1
where * indicates NN, mean and Var; WNN, Wtm,ean and W> are the parameters of Conv and
FC layers of the global model after t-th aggregation epoch; WNt-N1,k, Wmt-e1a,nk and Wvt-ar1,k are the
k-th local model been trained several local epoch based t - 1-th global model; πk(k = 1, . . . , K) is
3
Under review as a conference paper at ICLR 2021
the probability that a sample is drawn from the distribution of the k-th client, i.e., πk ∈ [0, 1] (k =
1, . . . ,K) and PkK=1 πk = 1.
The above formulation objects to minimize the expected loss over K clients with non-IID datasets.
Next, we will introduce a decoupled method called FeDEC to optimize the parameters of different
types of layers separatively, and derived the probability weights πk for parameter aggregation.
3.2 Decoupled Probabilistic-Weighted Gradient Aggregation Method
In this section, we proposed a decoupled method to derive the global model with respect to WNt N
(parameters of Conv and FC layers) and Wmt ean, Wvt ar (statistical parameters of BN layers).
3.2.1	Gradient Aggregation for Conv and FC Layers
Since the parameters of Conv and FC layers are neural network weights which are updated by
distributed gradient descent method (Nesterov (1983)), they are appropriate to be aggregated
with a similar approach that adapts conventional federated average for non-IID datasets. Let
gk = WNN： — WNN,k (k = 1,..., K), where NN* indicates NN parameters after full lo-
cal training. be the gradient of the k-th client in the t-th training epoch. After receiving the gradients
from K clients, the central server update the parameters Kof global model as follows.
WNt N = WtN-N1 - βXπktgkt,	(3)
k=1
where β is the learning rate for parameter update, πkt (k = 1, . . . , K) are the probabilistic weights
with PkK= πkt = 1 that are derived in section 3.2.3.
3.2.2	Parameter Aggregation for Means and Variances in BN
Different from the Conv and FC layers, the BN layers mainly contain statistical parameters such as
mean and variance. Conventional federated learning aggregates BN layers and other layers without
distinction, which could lead to high bias of means and variances in BN layer of the global model.
Thus we propose a different way to aggregate means and variances in BN layer as follows.
In t-th training epoch, the means and varianKces Wmt ean, Wvt ar in BN layer, which are updated by:
W
mean
πktWmt,kean,
k=

1K
W Var = lxl-κ ∑( IX k ∖- 1) ∏k W V4r,
|x| - K k=
(4)
(5)
where Wmt,kean and Wvt,akr indicate the means and variances in BN layers of the k-th client in epoch
t; πkt (k = 1, . . . , K) are probabilistic weights with PkK=1 πkt = 1 that are derived in section 3.2.3.
In the above equations, we update the mean with the weighted average of local models, and up-
date the variance with the weighted pooled variance (Killeen (2005)), which can give an unbias
estimation of parameters of the whole dataset under non-IID conditions (see Appendix A.2).
3.2.3	Derivation of Probabilistic Weights
We adopt a mixture probabilistic model to describe non-IID datasets in federated learning. Without
loss of generality, in the t-th training epoch, we assume the mini-batch samples of each client fol-
lows a Gaussian distribution Nk(μk,σk) (k = 1,..., K), where μk,σk are the mean and standard
deviation of the distribution that vary among clients. We omit the upper script t for simplicity there-
after. The whole samples can be described as a Gaussian Mixture Model (GMM) with the following
probability function:	K
p (x ∖λ) = f∏kP (x k ∖μk ,σk),	(6)
k=1
where λ = {n：, μk ,σk ∖ k = 1, 2,…，K} are the parameters of the GMM model1.
1Noted that the proposed variational inference method can be applied to other non-Gaussian distributions
with slight modification.
4
Under review as a conference paper at ICLR 2021
In federated learning, the local data samples are accessed by particular client and the central server
can only observe the statistics of local dataset such as mean and standard variance. Without know-
ing the overall samples, conventional expectation-maximization (EM) algorithm (Dempster et al.
(1977)) cannot be applied to derive λ. Alternatively, we introduce a variational inference method to
estimate the parameters of λ.
Specifically, we construct a variational Bayesian generative model to
generate data that are close to the reported statistics of local models
as possible, and use the generated data to estimate the GMM model
parameters. The plate notions of the generative model are shown in
Fig. 3. The notations are explained as follows.
•	stk = {Wtm,kean , Wvt,akr} is the observed statistics from the feature
maps of k-th client.
•	Zk = {Zk i∖(i = 1,2, ∙∙∙ ,C)} is a vector of latent variables with
length C, where ztk,i ∈ [0, 1], PiC=1 ztk,i = 1, and C is the number of
classes for a classification task. Ztk can be viewed as a data distribu-
tion that represents the probability of a sample in client k belonging
to the classes.
Figure 3: The variational
Bayesian generative model
using plate notations.
•	θ = {θk} are generative model parameters, and φ = {φk} are
variational parameters.
The solid lines in Fig. 3 denote the generative model pθk (Ztk)pθk (stk∖Ztk), and the dashed lines denote
the variational approximation qφk (Ztk ∖stk) to the intractable posterior pθk (Ztk ∖stk) We approximate
pθk (Ztk∖stk) with qφk (Ztk∖stk) by minimizing their divergence:
φk, θk = arg min divergence (qφk (Zk ∖S Sk) ∖∖ Pθk (Zk ∖S Sk)),
θk,φk	(7)
C	(7)
s.t.	ZSk i = 1.
i=1 k,i
To derive the optimal value of the parameters φk and θk, we compute the marginal likelihood ofSSk:
log P(S tk) = DKL (qφk(z tk ∖S k) ∖∖ Pθk(z tk ∖S k)) + E qφk (z k∣ s k)log Pk(如 k k) .	⑻
In Eq. 8, the first term is the KL-divergence (Joyce (2011)) of the approximate distribution and the
posterior distribution; the second term is called the ELBO (Evidence Lower BOund) on the marginal
likelihood of dataset in the k-th client.
Since logp(SSk)is non-negative, the minimization problem of Eq. 7 can be converted to maximize
the ELBO. To solve the problem, we change the form of ELBO as:
E qφk (zk∣s k)
^log Pθk (ZkS k) -
.g qφk(Zk∖Sk)一
E qφk (z k∣ s k)
|
ι	p(zk) -
oqφk (Zk ∖ S k) 
+ Eqφk (ztk |stk ) [log pθk (Sk ∖Zk )] .
{z
Encoder
}|
{z
Decoder
(9)
The above form is a variational encoder-decoder structure: the model qφk (ZSk∖SSk) can be viewed
as a probabilistic encoder that given an observed statistics SSk it produces a distribution over the
possible values of the latent variables ZSk; The model pθk (SSk∖ZSk) can be refered to as a probabilistic
decoder that reconstructs the value of SSk based on the code ZSk. According to the theory of variational
inference (Kingma & Welling (2014)), the problem in Eq. 9 can be solved with stochastic gradient
descent (SGD) method using a fully-connected neural network to optimize the mean squared error
loss function.
}
With the derived optimal parameters φk, θk., We can extract the latent variables Ztk that is interpreted
as the sample distribution of client-k. Therefore Z k Can be used to infer the parameters (∏k ,μk ,σk)
of k-th component of the GMM model. Specifically, the probabilistic weights πk can be represented
by	C	K C
πk={X jj} 4X X jj}.
5
Under review as a conference paper at ICLR 2021
4	Convergence Analysis
In this section, we will show that the convergence of the proposed FeDEC algorithm is theoreti-
cally guaranteed. We use the following assumptions and lemmas, and the convergence guarantee is
provided in Theorem 1.
Assumption 1 (Unbiased Gradient): We assume that the stochastic gradients git is an unbiased
estimator of the true gradient Pf (Wt), i.e., E[gt] = Pf (Wt), where f (∙) is any convex objective
function and wit is its variables.
Assumption 2 (Gradient Convex Set): We assume that gradient set G is a convex set, where all
gradients g1, g2, . . . , gK are in G, and any g = PiK=1 λigi (∀λi > 0 and PiK=1 λi = 1) is in G.
Lemma 1 (L-Lipschitz Continuity): For a function f (∙) is Lipschitz continuous if there exists a
positive real constant L such that, for all real x1 and x2:
|f(x1) - f(x2)| ≤ L|x1 - x2|.
Lemma 2 (Jensen’s Inequality): Iff(W) is a convex function on W, and E[f (W)] and f (E[W])
are finite, then:
E[f (W)] ≥ f (E[W])).
Definition 1 (Projection Operation): Assume W* is an intermediate result of optimization, we
define a project operator ∩w (w*) to project W* to the domain W, which is computed by:
(W*) = arg min ||W - W*||.
w∈W
W
Definition 2 (Diameter of Domain): Given a function f (W), where W ∈ W, and W is f’s domain
of definition. The diameter ofW is denoted by Γ: for every W1, W2 ∈ W: ||W1 - W2|| ≤ Γ.
Theorem 1 (Guaranteed Convergence Rate): Ifa convex function f(W) is L-Lipschitz continuous
function, then ||Pf (W)|| ≤ L. Let Γ be the diameter of domain. Applying equations (3)(4)(5) for
gradients aggregation, we have the following convergence rate for the proposed FeDEC algorithm:
Γ2 β
f(W )-mWf(W) ≤°(2βT + 2l ))	(11)
where WT is the average result of W for total training epoch T, β is the learning rate in equation-(3),
and T is the total training epoch. Ifwe let β = L√√T, the convergence rate is ° (√1T).
Proof skeleton: We provide a simple description of the proof skeleton of Theorem 1 with the follow-
ing steps. (1) Since f (∙) is a convex function, we have f (wt) — f (w) ≤ Pf (wt)(wt — w). (2)With
assumption 1 and 2, we have f(Wt) - f(W) ≤ 2β(Ilwt - WIl2 - Ilw*+1 - WIl2) + 2UPf(Wt)Il2,
where W*t+1 is the intermediate result of f(W) in update time t+ 1. (3) With lemma 1 and definition
1, by projecting w*+1 to Wt +1, We have f (wt) — f (w) ≤ *(Il Wt — W Il2 — Il Wt+1 — W Il2) + 22 L2.
(4) Summing from t = 1 to T and with definition 1 and 2, we have PtT=1 f(Wt) — Tf(W) ≤
2βΓ2 + 2L2T. (5) According to lemma 2, we have f (WT) — f (w) ≤ 2βT + βL2. (6) Taking
β = Γ/(L∖fT), we can obtain the convergence rate in the theorem. The detailed proof of Theorem
1 and explanations are provided in Appendix A.1.
According to Theorem 1, the FeDEC parameter aggregation algorithm is guaranteed to converge,
and the convergence rate can be as fast as general stochastic gradient decent which only related to the
training epoch T with an associated constants. The constant is related to the optimization problem
parameters such as lipschitz constant L, and diameter of domain Γ.
5	Performance Evaluation
In this section, we evaluate the performance of the proposed FeDEC method for federated learning.
6
Under review as a conference paper at ICLR 2021
5.1	Experimental Setup
Implementation. We implement the proposed FeDEC parameter aggregation approach and the
considered baselines in PyTorch (Paszke et al. (2019)). We train the models in a simulated federated
learning environment consisting of one server and a set of mobile clients with wireless network
connections. Unless explicitly specified, the default number of clients is 20, and the learning rate
β = 0.01. We conduct experiments on a GPU-equipped personal computer (CPU: Inter Core i7-
8700 3.2GHz, GPU: Nvidia GeForce RTX 2070, Memory: 32GB DDR4 2666MHz, and OS: 64-bit
Ubuntu 16.04).
Models and datasets. We conduct experiments based on 5 mainstream neural network models:
ResNet18 (He et al. (2016)), LeNet (Lecun et al. (1998)), DenseNet121 (Huang et al. (2017)), Mo-
bileNetV2 (Sandler et al. (2018)), and a 4-layer CNN (every CNN layer is followed by a BN layer).
The detailed structure of the CNN models are provided in Appendix A.3.
We use three real world datasets: MNIST (LeCun et al. (2010)), Fashion-MNIST (Xiao et al.
(2017)), and CIFAR-10 (Krizhevsky (2009)). MNIST is a dataset for hand written digits classi-
fication with 60000 samples and each example is a 28 × 28 greyscale image. Fashion-MNIST is
a dataset intended to replace the original MNIST for benchmarking machine learning algorithms.
CIFAR-10 is a larger dataset with 10 categories. Each category has 5000 training images and 1000
validation images of size 32 × 32. For each dataset, we use 80% of the data for training and amal-
gamate the remaining data into a global test set.
η ∈ [0, 1], ifx ∈ classj,
N (0.5, 1), otherwise.
We form non-IID local datasets as follows. Assume there are C classes of samples in a dataset.
Each client draw samples form the dataset with probability pr(x)
It means that the client draw samples from a particular class j with a fixed probability η, and from
other classes based on standard Gaussian distribution. The larger η is, the more likely the client’s
samples concentrate on a particular class, and the more heterogeneous the local datasets are.
5.2	Performance Comparison
We compare the performance of FeDEC with 5 state-of-the-art methods: FedAvg (McMahan et al.
(2017)), RNN based aggregator (Ji et al. (2019)), FedProx (Li et al. (2020a)), q-FedSGD (Li et al.
(2020b)), and FedMA (Wang et al. (2020)). The results are analyzed as follows.
Figure 4: Convergence of dif- Figure 5: Training accuracy of Figure 6: KL-divergence of dif-
ferent algorithms (ResNet18 on different algorithms (ResNet18 ferent algorithms.
CIFAR-10).	on CIFAR-10).
Convergence: In this experiment we study the convergence of all baselines and our algorithm by
showing the total communication epochs versus train loss. Fig. 4 shows the result of ResNet18 on
CIFAR-10. It is shown that the loss of all algorithms tend to be stable after a number of epoches.
Clearly FeDEC has the lowest loss among all algorithms, which means that FeDEC converges faster
that of baselines. The results of more CNN models on different datasets are shown in Appendix A.4.
Training Efficiency: In this experiment we study the test accuracy versus time during training of a
CNN model with federated learning. Fig.5 shown the results of training ResNet18 on CIFAR-10. It
is shown that FeDEC reaches 0.8 accuracy after 18 minutes, while FedMA, FedProx, and FedAvg
take 36 to 63 minutes to reach the same accuracy. FeDEC approaches 0.9 accuracy after 54 minutes,
while the accuracy of other algorithms are below 0.85. The results of more CNN models on different
datasets are shown in Appendix A.5. It suggests that FeDEC trains much faster than the baseline
algorithms and it can reach high accuracy in a short time period.
7
Under review as a conference paper at ICLR 2021
Parameter Bias: In this experiment we study the parameter bias of federated learning algorithms.
Fig. 6 compares the KL-divergence between the means and variances in BN of global models
aggregated by different algorithms and the central model. It is shown that FedAvg, FedProx, and q-
FedSGD have exceptional high parameter bias, while FeDEC has significantly lower KL-divergence
compared to the baselines for different CNN models on different datasets.
Global Model Accuracy: In this experiment, we compare the global model accuracy of different
federated parameter aggregation algorithms after training to converge. We repeat the experiment
for 20 rounds and show the average results in Table 1. As shown in the table, the central method
yields the highest accuracy. In comparison of different federated learning methods, FeDEC sig-
nificantly outperforms the other algorithms in global model accuracy. It performs better than the
state-of-the-art method FedMA with 2.87%, 3.17%, 2.58%, and 3.09% accuracy improvement in
ResNet18, DenseNet121, MobileNetV2, and 4-L CNN respectively for CIFAR-10, 1.09% improve-
ment in LeNet for F-MNIST, and 0.33% improvement in LeNet for MNIST accordingly. FeDEC
achieves the highest accuracy among all baselines, and it performs very close to the centralized
method, whose accuracy drop is less than 3% in all cases.
Table 1: Average test accuracy on non-IID datasets. The “Central” method trains the CNN model in the cen-
tral server with global dataset. The “FeDEC(w/o)” method means using the proposed probabilistic-weighted
aggregation methodwithout distinguishing N N and mean, var. The “FeDEC” method represents the pro-
posed decoupled probabilistic-weighted aggregation approach.
Model@ Dataset	Central	FedAvg	RNN	FedProx	q-FedSGD	FedMA	FeDEC(W/o)	FeDEC
ResNet18@CIFAR-10	92.33%	83.29%	-	83.47%	81.08%	87.44%	88.30% =	90.31%
DenseNet121@CIFAR-10	93.24%	82.36%	-	85.03%	82.43%	88.12%	89.25%	91.29%
MobileNetV2@CIFAR-10	92.51%	83.11%	-	80.68%	79.82%	86.59%	87.93%	89.17%
4-L CNN@CIFAR-10	85.53%	79.33%	-	80.02%	77.77%	81.76%	83.11%	84.85%
LeNet@F-MNIST	90.42%	87.41%	82.32%	88.33%	86.21%	89.02%	89.52%	90.11%
LeNet@MNIST	98.95%	97.32%	97.06%	97.55%	95.88%	98.16%	98.37%	98.49%
BiLSTM@Sent140	81.47%	72.14%	-	71.08%	68.44%	73.81%	77.51%	77.51%
Hyperparameter Analysis: We further analyze the influence
of two hyperparameters in federated learning: the number of
clients involved and the heterogeneity of local datasets.
Fig. 7 compares the test accuracy of the global model for dif-
ferent number of involved clients. According to the figure, the
performance of FeDEC is stable. When the number of mobile
clients increases from 5 to 20, the test accuracy slightly de-
creases from 0.909 to 0.893. Other baseline algorithms yield
significant performance drop. FeDEC achieves the highest test
accuracy among all federated learning algorithms in all cases,
and it performs very close to the central model.
In the experiment, the heterogeneity of local datasets is repre-
sented by η, the probability that a client tends to sample from a
particular class. The more η approaches to 1, the more hetero-
geneous the local datasets are. Fig. 8 shows the test accuracy
under different level of heterogeneity. As η increases, the test
accuracy of all models decreases. FeDEC yields the highest
test accuracy among all algorithms, and its performance drops
much slower than the baselines. It verifies the effectiveness of
the proposed probabilistic-weighted gradient aggregation ap-
proach under non-IID conditions.
ferent number of clients (ResNet18
on CIFAR-10).
Figure 8: Test accuracy with
different level of heterogeneity
(ResNet18 on CIFAR-10).
6 Conclusion
Gradient aggregation played an important role in federated learning to form a global model. To
address the problem of data and parameter bias in federated learning for non-IID dataset, we pro-
posed a novel probabilistic parameter aggregation method called FeDEC that decoupled gradient
parameters and statistical parameters to aggregate them separatively. The probabilistic weights were
optimized with variational inference, and the proposed method was proved to be convergence guar-
anteed. Extensive experiments showed that FeDEC significantly outperforms the state-of-the-arts
on a variety of performance metrics.
8
Under review as a conference paper at ICLR 2021
References
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends in Machine
Learning, 3(1):1122, 2011.
Tianyi Chen, Georgios Giannakis, Tao Sun, and Wotao Yin. Lag: Lazily aggregated gradient for
communication-efficient distributed learning. Advances in Neural Information Processing Sys-
tems (NIPSU8),pp. 5050-5060, 2018.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y. Ng. Large scale
distributed deep networks. In Advances in Neural Information Processing Systems (NIPS’12), pp.
1223-1231. 2012.
O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using
mini-batches. Journal of Machine Learning Research, 13:165-202, 2012.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via
the em algorithm. JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, 39(1):1-38,
1977.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR’16), pp. 770-778,
2016.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely con-
nected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR’17), pp. 2261-2269, 2017.
Jinlong Ji, Xuhui Chen, Qianlong Wang, Lixing Yu, and Pan Li. Learning to learn gradient aggre-
gation by gradient descent. In Proceedings of the Twenty-Eighth International Joint Conference
on Artificial Intelligence (IJCAI’19), pp. 2614-2620, 2019.
Linshan Jiang, Rui Tan, Xin Lou, and Guosheng Lin. On lightweight privacy-preserving collabo-
rative learning for internet-of-things objects. In Proceedings of the International Conference on
Internet of Things Design and Implementation (IoTDI’19), pp. 70-81, 2019.
James M. Joyce. Kullback-Leibler Divergence, pp. 720-722. 2011.
Marcel Keller, Valerio Pastro, and Dragos Rotaru. Overdrive: Making SPDZ great again. In Ad-
vances in Cryptology (EUROCRYPT’18), volume 10822, pp. 158-189, 2018.
Peter R. Killeen. An alternative to null-hypothesis significance tests. Psychological science, 16(5):
345-53, 2005.
Diederik Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR’14), 2014.
Jakub Konecny, H. Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed
optimization beyond the datacenter. NIPS Optimization for Machine Learning Workshop 2015,
pp. pp.5, 2015.
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtðrik. Federated optimiza-
tion: Distributed machine learning for on-device intelligence. ArXiv, abs/1610.02527, 2016.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann Lecun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. In Proceedings of the IEEE, pp. 2278-2324, 1998.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
9
Under review as a conference paper at ICLR 2021
Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski,
James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the
parameter server. In 11th USENIX Symposium on Operating Systems Design and Implementation
(OSDr14),pp. 583-598, 2014.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and
Systems (MLSys’20), pp. 429-450. 2020a.
Tian Li, Maziar Sanjabi, and Virginia Smith. Fair resource allocation in federated learning. In
International Conference on Learning Representations (ICLR’20), 2020b.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Areas.
Communication-efficient learning of deep networks from decentralized data. Proceedings of the
20th International Conference on Artificial Intelligence and Statistics (AISTATS’17), 54:1273-
1282, 2017.
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Pro-
ceedings of the 36th International Conference on Machine Learning (ICML’19), volume 97, pp.
4615-4625, 2019.
Y. Nesterov. A method of solving a convex programming problem with convergence rate o( £).
Dokl. Akad. Nauk SSSR, 269:543-547, 1983.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, and Zachary DeVito. Pytorch: An imperative style, high-performance deep learning library.
In Advances in Neural Information Processing Systems (NeurIPS’19), pp. 8024-8035, 2019.
Peter RiChtðrik and Martin Takdc. Distributed coordinate descent method for learning with big data.
Journal of Machine Learning Research, 17(1):1-25, 2016.
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen. Mobilenetv2: Inverted residuals and
linear bottlenecks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR’18),
pp. 4510-4520, 2018.
Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using
an approximate newton-type method. In Proceedings of the 31st International Conference on
Machine Learning (ICML’14), volume 32, pp. 1000-1008, 2014.
V. Smith, S. Forte, C. Ma, M. Takac, M. I. Jordan, and M. Jaggi. Cocoa: A general framework
for communication-efficient distributed optimization. Journal of Machine Learning Research, 18
(230):1-47, 2018.
Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, and Tie-Yan Liu. Ensemble-compression:
A new method for parallel training of deep neural networks. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases (ECML-KDD’17), pp. 187-202, 2017.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. In International Conference on Learning Represen-
tations (ICLR’20), 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. 2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with
suspicion-based fault-tolerance. In Proceedings of the 36th International Conference on Machine
Learning (ICML’19), volume 97, pp. 6893-6901, 2019.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Pro-
ceedings of the 36th International Conference on Machine Learning (ICML’19), volume 97, pp.
7252-7261, 2019.
10
Under review as a conference paper at ICLR 2021
Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In
Advances in Neural Information Processing Systems (NIPS'15), pp. 685-693. 2015.
Y. Zhang, J. C. Duchi, , and M. J. Wainwright. Communication-efficient algorithms for statistical
opti- mization. Journal of Machine Learning Research, 14:33213363, 2013.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and V. Chandra. Federated learning
with non-iid data. ArXiv, abs/1806.00582, 2018.
H. Zhu and Y. Jin. Multi-objective evolutionary federated learning. IEEE Transactions on Neural
Networks and Learning Systems, 31(4):1310-1322, 2020.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Convergence Guarantee (Theorem 1 in Section 4)
We provide the detailed proof of Theorem 1 in Section 4. We first restate the necessary equations
and the theorem.
In section 3, we propose the usage of the following equations for gradient decent update algorithm:
K
WNt N = WtN-N1 - β X πkt gkt ,
k=1
where β is the learning rate for parameter update. And for batch normalization update:
K
Wt = X πt Wt,k
mean	k me
k=1
(3)
(4)
(5)
t
var
K
E(Il X k II _1)∏k W 忠.
k=1
1
W
We restate the theorem in section 4 in the following:
Assumption 1 (Unbiased Gradient): We assume that the stochastic gradients git is an unbiased
estimator of the true gradient Pf (Wt), i.e., E[gt] = Pf (WIt), where f (∙) is any convex objective
function and wit is its variables.
Assumption 2 (Gradient Convex Set): We assume that gradient set G is a convex set, where all
gradients g1, g2, . . . , gK are in G, and any g = PiK=1 λigi (∀λi > 0 and PiK=1 λi = 1) is in G.
Lemma 1 (L-Lipschitz Continuity): For a function f (∙) is Lipschitz continuous if there exists a
positive real constant L such that, for all real x1 and x2:
If(x1) _ f(x2)I ≤ LIx1 _ x2I.
Lemma 2 (Jensen’s Inequality): Iff(W) is a convex function on W, and E[f (W)] and f (E[W])
are finite, then:
E[f(W)] ≥ f(E[W])).
Definition 1 (Projection Operation): Assume W* is an intermediate result of optimization, we
define a project operator ∩w (w*) to project W* to the domain W, which is computed by:
(W*) = arg min IIW _ W*II.
w∈W
W
Definition 2 (Diameter of Domain): Given a function f (W), where W ∈ W, and W is f’s domain
of definition. The diameter ofW is denoted by Γ: for every W1, W2 ∈ W: IIW1 _ W2II ≤ Γ.
Theorem 1 (Guaranteed Convergence Rate): Ifa convex function f(W) is L-Lipschitz continuous
function, then IIPf (W)II ≤ L. Let Γ be the diameter of domain. Applying equations (3)(4)(5) for
gradients aggregation, we have the following convergence rate for the proposed FeDEC algorithm:
Γ2	β
f (W ) -mW f (w) ≤ O(2βT + 2L ),	(11)
where WT is the average result of W for total training epoch T, β is the learning rate in equation-(3),
and T is the total training epoch. Ifwe let β = L√√T, the convergence rate is O ( √1t ).
Proof: To simplify the analysis, we consider fixed learning rate β. The proof includes the following
steps:
(1)	According to the definition of convex function,
f (Wt) _ f(W) ≤ Pf (Wt)(Wt _W).
12
Under review as a conference paper at ICLR 2021
(2)	We define G(w) = ▽/T(Wt)(wt — w), and gt = PK=I ∏kgtk. The intermediate result of f (w)
in update time t + 1 is denoted by W；+1. With assumption 1 and assumption 2, we have:
G(w)
	
1((wt )2 — Wt W — W FIWt + W F1W)
21β ((wt )2 — wt w + w2 — (w F1)2 + 2ww F1 — w2 + (wt )2 — 2wt W:+1 + (w tt +1)2)
2φ(11
2β(11
2β(11
Wt — W Il2 — Il W F1 — W Il2 + Il Wt — W ：+11∣2)
Wt — W Il2 — IM+1 — W Il2) + 2 ∣∣Vf (wt) Il2
Wt — Wll 2 — lM+1 — Wll 2) + 2ll g tH 2.
So we have:
f(wt) — f(w) ≤ ɪ(IlWt — WIl2
—ll W *+1 — Wll 2) + β ll gt|| 2.
(3)	We project w；+1 to Wt+1. With definition 1 and using non-expandable property of projection
operation of convex set, we have:
GI(W) ≤ 2β(IIWt — Wll2 — llwt+1 - Wll2) + 2llgt||2
Due to L-Lipschitz Continuity (Lemma 1), we have:
G(w) ≤ 2β(llwt — Wll2 — llwt+1 — Wll2)+ 2L2
So we have:
f(wt) 一 f(w) ≤ 2β(llwt - Wll2 — llwt+1 — Wll2) + 2L2
(4)	According to definition 2, summing up all W from t = 1 to T, we have:
T
X f (wt) — Tf (w) ≤
t=1
≤
≤
2β (IIWI — W ll 2 — ll w t+1 — W ll 2) + 2 L 2 T
2βl w1—W ll 2+2L 2 T
-1Γ2 + βL2 T.
2 β	2
(5)	According to Jensen,s Inequality (Lemma 2), we have:
1T
f (W t)-f (w) = f (T EW t)-f (w)
1T
≤ TEf(Wt) —f(W)
t=1
Γ2 β 2
≤ 2βT + 2 l -
We can get the result:
Γ2	β
f (WT) — min f (w) ≤ O(--+ -L2),
八 ) w∈nn八 ) ≤	(2βT + 2	),
13
Under review as a conference paper at ICLR 2021
(6)	Taking β = Γ/(Lm), the right part of the above equation becomes
2βT + 2 L2 = rLTT + 2L√TL L = √
Therefore we can obtain the simplified expression of the convergence bound O(方).
14
Under review as a conference paper at ICLR 2021
A.2 Explanation of Unbias Parameter Aggregation in Section 3.2.2
We compute the expectation of the aggregated parameters Wmt ean and Wvt ar in Section 3.2.2 as
follows.
K
E[Wtmean]	= E	πkWmt,kean
k=1
K
=	πkE Wmt,kean
k=1
E[Wvtar]
E
1K
-∣X∣∖-κ ∑( IIX k II-1) ∏k W Ivar
k=1
∣∣v∣1 E E	Ii χ k Ii-1) ∏k w Var
||X|| - K
1K
PPK ɪl( II x k∣∣- 1) ∏k E [W t,kr]
According to the above equations, if the parameters of the local models Wmt,kean and Wvt,akr are
unbias, then the aggregated model parameters are unbias as well.
15
Under review as a conference paper at ICLR 2021
A.3 S tructure of the Neural Network Models in Section 5
Here we report the detailed model structure used in the experiments. We use LeNet shown in Table
2 and the 4-layer CNN model shown in Table 3. We adopt a slim ResNet18 as shown in Table 4,
where “Conv2d” is convolution layer, “BatchNorm2d” is batch normalization layer, and “Linear” is
fully-connected layer. We can observe that every convolution layer is followed by a batch normal-
ization (BN) layer. For all models, we use ReLU layer after every Conv2d layer. The structure of
DenseNet1212 and MobileNetV23 can be found in GitHub.
For language model, we consider the sentiment analysis task on tweets from Sentiment140 with
2-layer BiLSTM. The BiLSTM binary classifier containing 256 hidden units with pretrained 100-
dimentional GloVe embedding. Each twitter account corresponds to a device.
Table 2: Detailed information of the LeNet architecture.
Parameter	Shape	Layer hyper-parameter
Conv2d	1 × 16 × 5 × 5	stride=1, padding=2
BatchNorm2d	16×2	—
MaxPool2d	-	stride=2, kernel size=2
Conv2d	16×16×5×5	stride=1, padding=2
BatchNorm2d	16×2	—
Conv2d	16×32× 5×5	stride=1, padding=2
BatchNorm2d	32 × 2	-
MaxPool2d	—	stride=2, kernel size=2
Conv2d	32 × 32 × 5 × 5	stride=1, padding=2
BatchNorm2d	32 × 2	-
Linear	1568 × 512	bias=True
Linear	512 × 128	bias=True
Linear	128 × 10	bias=True
Table 3: Detailed information of the 4-L CNN architecture.
Parameter	Shape	Layer hyper-parameter
Conv2d	3 × 32 × 5 × 5	—
BatchNorm2d	32 × 2	-
Conv2d	32 × 32 × 5 × 5	-
BatchNorm2d	32 × 2	—
MaxPool2d	-	stride=1, kernel size=2
Conv2d	32 × 64 × 3 × 3	-
BatchNorm2d	64 × 2	—
Conv2d	64 × 64 × 5 × 5	-
BatchNorm2d	64 × 2	-
MaxPool2d	—	stride=1, kernel size=2
Linear	1024 × 128	bias=True
Linear	128 × 10	bias=True
2https://github.com/kuangliu/pytorch-cifar/blob/master/models/densenet.py
3https://github.com/kuangliu/pytorch-cifar/blob/master/models/mobilenetv2.py
16
Under review as a conference paper at ICLR 2021
Table 4: Detailed information of the ResNet18 architecture.
Parameter	Shape	Layer hyper-parameter
Conv2d	3 × 64 × 3 × 3	stride=1, padding=1
BatchNorm2d	64 × 2	-
Conv2d	64 × 64 × 3 × 3	stride=1, padding=1
BatchNorm2d	64 × 2	
Conv2d	64 × 64 × 3 × 3	stride=1, padding=1
BatchNorm2d	64 × 2	-
Conv2d	64 × 64 × 3 × 3	stride=1, padding=1
BatchNorm2d	64 × 2	-
Conv2d	64 × 64 × 3 × 3	stride=1, padding=1
BatchNorm2d	64 × 2	
Conv2d	64× 128×3× 3	stride=2, padding=1
BatchNorm2d	128 × 2	-
Conv2d	128 × 128 × 3 × 3	stride=1, padding=1
BatchNorm2d	128 × 2	-
Conv2d	128× 128×3× 3	stride=1, padding=1
BatchNorm2d	128 × 2	
Conv2d	128× 128×3× 3	stride=1, padding=1
BatchNorm2d	128 × 2	-
Conv2d	128 × 256 × 3 × 3	stride=2, padding=1
BatchNorm2d	256 × 2	-
Conv2d	256 × 256 × 3 × 3	stride=1, padding=1
BatchNorm2d	256 × 2	
Conv2d	256 × 256 × 3 × 3	stride=1, padding=1
BatchNorm2d	256 × 2	-
Conv2d	256 × 256 × 3 × 3	stride=1, padding=1
BatchNorm2d	256 × 2	-
Conv2d	256× 512×3× 3	stride=2, padding=1
BatchNorm2d	512 × 2	
Conv2d	512×512×3×3	stride=1, padding=1
BatchNorm2d	512× 2	-
Conv2d	512 × 512 × 3 × 3	stride=1, padding=1
BatchNorm2d	512× 2	-
Conv2d	512×512×3×3	stride=1, padding=1
BatchNorm2d	512 × 2	
Linear	512 × 10	bias = True
17
Under review as a conference paper at ICLR 2021
A.4 Convergence of Federated Learning Algorithms for Different Models on
Different Datasets
(a) CIFAR-10 (ResNet18).
(b) CIFAR-10 (DenseNet121).
(c) CIFAR-10 (MobileNetV2).
(a) Fashion-MNIST (LeNet).
Figure 10: Convergence of different algorithms for LeNet on FMNIST and MNIST.
Figure 9: Convergence of different algorithms for different CNN models on CIFAR-10.
(b) MNIST (LeNet).
18
Under review as a conference paper at ICLR 2021
A.5 Training Efficiency of Federated Learning Algorithms for Different
Models on Different Datasets
(a) CIFAR-10 (ResNet18).
(b) CIFAR-10 (DenseNet121).
(c) CIFAR-10 (MobileNetV2).
Figure 11: Training accuracy of different algorithms for different CNN models on CIFAR-10.
0.9 τ
⅛ 0.8
o 0.7-
8 0.6-
i-
0.5 ɪ
0 4 8 12 16 20 24 28 32 36
Time (min)
(a) Fashion-MNIST (LeNet).
Time (min)
(b) MNIST (LeNet).
Figure 12: Training accuracy of different algorithms for LeNet on FMNIST and MNIST.
19
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 1: FeDEC Aggregation
Server
for t= 1 to T do
Transmit Wt to all clients.
Receive gt, μ, ^ from all clients.
Inference ∏k with 9 based on μ, ^ from Wtmeςιr,-, WVar.
Update WtN-N1 with 3 to get WNt N .
Aggregate Wtmean with 4 and Wvt ar with 5.
_ Combine WNN, Wtnean and WIVar to new model Wt.
Client
for t= 1 to T do
Receive server model Wt .
Train local model Wt based on local dataset and get local gradients gt .
Transmit gt and μ, ^ to server.
Stop training until received server model Wt.
①6£u①UJ①d
0.0
4 3 2
SSSO
True & Inferenced Distribution
Figure 13: Variational inferenced approximate distribution of two clients of CIFAR-10 dataset ex-
periment.
20