Under review as a conference paper at ICLR 2021
Complex neural networks have no spurious
LOCAL MINIMA
Anonymous authors
Paper under double-blind review
Ab stract
Most non-linear neural networks are known to have poor local minima (Yun et al.,
2019) and it is shown that training a neural network is NP-hard (Blum & Rivest,
1988). A line of work has studied the global optimality of neural networks in
various settings but unfortunately all previous networks without spurious local
minima are linear networks or networks with unrealistic assumptions. In this work
we demonstrate for the first time that a non-linear neural network can have no poor
local minima under no assumptions.
Recently, a number of papers considered complex-valued neural networks
(CVNNs) in various settings and suggest that CVNNs have competitive or even
preferable behaviour compared to real-valued networks. Unfortunately, there is
currently no theoretical analysis on the optimization of complex-valued networks,
given that complex functions usually have a disparate optimization landscape.
This is the first work towards analysing the optimization landscape of CVNNs.
We prove a surprising result that no spurious local minima exist for one hidden
layer complex-valued neural networks with quadratic activation. Since CVNNs
can have real-valued datasets and there are no assumptions, our results are ap-
plicable to practical networks. Along the way, we develop a novel set of tools
and techniques for analyzing the optimization of CVNNs, which may be useful
in other contexts. Lastly, we prove spurious local minima exist for CVNNs with
non-analytic CReLU activation.
1	Introduction
Neural networks have seen great success empirically, inspiring a lot of work theoretically, but most
of them are real-valued networks. Thus, exploring complex-valued neural networks(CVNNs) is an
interesting potential way to find better architectures. A number of papers considered CVNNs and
suggested that CVNNs have richer representational capacity, faster learning ability, better general-
ization, and noise-robust memory mechanisms compared to real-valued networks (Trabelsi et al.,
2018; Hirose & Yoshida, 2012; Arjovsky et al., 2016; Danihelka et al., 2016; Wisdom et al., 2016).
This naturally leads one to inquire about the optimization landscape of CVNNs, in comparison to
that of real-valued neural networks
The minimum modulus principle (MMP) is a fundamental result in complex analysis which can be
roughly viewed as a statement that “analytic functions have no spurious local minima” with respect
to the modulus. Below is a graph that compares a same function, x sin(x), on different fields R
and C. Since complex numbers are not ordered, the y-axis on the right graph is the square of the
modulus of the range. We can see from Figure 1 that the left graph has many local minima while the
right graph only has global minima. Although most CVNNs are not analytic, the MMP still provides
an insight for us that there might be a potential for CVNNs to have a better optimization landscape
due to the properties of complex numbers. Note that fully-connected multi-layer perceptrons with
analytic activation functions are analytic with respect to either the input or each weight matrix.
Therefore we conjecture that all CVNNs with analytic activations have a superior landscape. It
should be noted that generally the loss function of a CVNN is not analytic because the loss is
calculated by summing modulus. Non-analytic functions not only lose the MMP but also are non-
differentiable. Thus, analyzing the optimization landscape of CVNNs is not trivial and can have no
connection with the MMP. The main technique we apply in this paper is called Wirtinger calculus
that we will describe later.
1
Under review as a conference paper at ICLR 2021
Figure 1: Left: x sin(x) vs. x where x ∈ R. Right: ||z sin(z)||2 vs. z where z ∈ C. We plot the left
one in three dimensions for consistency.
1.1	Related work
1.1.1	Complex-valued neural networks
The study of complex-valued neural networks can be dated back to the last century (Benvenuto &
Piazza, 1992; Little et al., 1990; Georgiou & Koutsougeras, 1992; Nitta, 2002) in the signal pro-
cessing community. For some well-known deep learning architecture like CNN, RNN, and GAN,
there are complex-valued counterparts (Arjovsky et al., 2016; Danihelka et al., 2016; Wisdom et al.,
2016; Minin, 2012; Goodfellow et al., 2014; Guberman, 2016; Wolter & Yao, 2018; Dedmari et al.,
2018; Sun et al., 2019). More recently, CVNNs started to gain attention in deep learning commu-
nity. Trabelsi et al. (2018) proposed an extensive framework for complex-valued neural networks
and demonstrated that they have a competitive performance compared to real-valued networks.
Complex-valued RNNs were shown to have richer representational capacity, faster learning abil-
ity, and better memory mechanisms (Arjovsky et al., 2016; Danihelka et al., 2016; Wisdom et al.,
2016). Hirose & Yoshida (2012) showed that CVNNs have better generalization characteristics. A
connection between CVNNs and privacy protection was also explored (Xiang et al., 2020). In ad-
dition, a large number of papers have studied the application of CVNNs to fields such as quantum,
medicine, geoscience, audio, image, NLP, signal processing, and more (Grant et al., 2018; Dedmari
et al., 2018; Jingkun Gao & Li, 2019; Choi et al., 2019; Tay et al., 2018; Gaudet & Maida, 2018;
Pande et al., 2008). We can see from the above that CVNNs are not just a theoretical curiosity, and
have seen wide application in real-world problems.
1.1.2	Optimization landscape of neural networks
Since the loss function of neural networks is non-convex, analyzing the optimization landscape
of the loss is always hard. Given that poor local minima exist in common neural networks like
over-parametrized ReLU networks (Yun et al., 2019), people try to prove poor local minima do not
exist in other settings. Linear neural networks, for example, were proved to have no spurious local
minima although having a non-convex loss (Baldi & Hornik., 1989; Baldi & Lu, 2012; Kawaguchi,
2016). More recently, there are results of “no spurious local minima” on more networks such as
shallow quadratic networks and shallow ReLU networks (Wu et al., 2018; Soltanolkotabi et al.,
2019; Ghorbani et al., 2019). Unfortunately, all prior works make unrealistic assumptions of one
form or another, which we summarize and describe in Table 1. The assumptions we list in the table
below indicate unrealistic ones. For the results on linear networks, it is assumed that the covariance
of the training data is full rank, but this is not unrealistic.
Assumption A. A1p-m and A5u-m in (Kawaguchi, 2016).
Assumption B. Two hidden units. Weight vectors are unit-normed and orthogonal.
Assumption C. The weight vector v ∈ Rk connecting the hidden layer and the output node must
contain at least d positive entries and d negative entries where k ≥ 2d.
2
Under review as a conference paper at ICLR 2021
Reference	Model	Linearity	Assumptions
(Baldi & Hornik., 1989)	Shallow linear networks	Linear	None
(Baldi & Lu, 2012)	Shallow complex linear networks	Linear	None
(Kawaguchi, 2016)	Deep linear networks	Linear	None
(Kawaguchi, 2016)	Deep ReLU networks	Non-linear	Asm. A
(Wu et al., 2018)	Shallow ReLU networks	Non-linear	Asm. B
(Soltanolkotabi et al., 2019)	Shallow quadratic networks	Non-linear	Asm. C
(Ghorbani et al., 2019)	Shallow quadratic networks	Non-linear	Asm. D
Ours	Shallow complex quadratic networks	Non-linear	None
Table 1: Neural Networks Without Spurious Local Minima
Assumption D. Over-parametrization and feature vectors being Gaussian.
A5u-m is unrealistic as suggested in (Kawaguchi, 2016; Choromanska et al., 2015), where they
assumed independence between hidden nodes. Assumption B and C are restrictive, and are unlikely
to arise in real-world settings. Assumption C set the second weight vector to have at least d positive
weights and at least d negative weights. There is no way to guarantee such setting during and after
training, especially with algorithm like backpropagation. As for Assumption D, itis unlikely that the
feature vectors will be Gaussian in real-world problems. We can see all previous analysis on non-
linear neural networks have extremely unrealistic assumptions. Another work done in Kawaguchi
& Kaelbling (2020) and Liang et al. (2018) suggested that adding one neuron can distinguish all
spurious local minima, but local minima still exist with infinity norm. Our work has no assumption
as in all the above. It is, therefore, the first non-linear neural network that has no poor local minima
under no unrealistic assumption. Linearity is important because linear models can only fit data that
are linearly separable. However, with enough parameters non-linear neural nets can approximate
any function by universal approximation theorem (Cybenko.; Barron., 1993).
1.2	Our contribution
We prove that one hidden layer CVNNs with quadratic activation have no spurious local minima.
The proof is built on (Soltanolkotabi et al., 2019), and it turns out that their assumptions can be
avoided in the complex-valued setting. Theorem 1 states our result formally.
Theorem 1.	Assume the dataset is {xi , yi} for i = 1, 2, . . . , n with xi ∈ Cd and yi ∈ C. The
training model we consider is one hidden layer complex-valued neural networks with quadratic
activation. It is in the form of
x 7→ vT ψ(Wx)
with ψ(z) = ψ((z1, . . . , zd)) = (z12, . . . , zd2), W ∈ Ck×d, v ∈ Ck with vi 6= 0, and k ≥ d. Then
the training loss as a function of the weights W
1n
l( w ) = 2n∑ k yi-v ψ( WX i) k2
n i=1
1n
=ʒ- J2(yi - V T Ψ( WX i))*(yi — V T Ψ( WX i))
2n
i=1
has no spurious local minima, i.e. all local minima are global.
We require only the mild assumptions that v has non-zero entries and k ≥ d. If an entry of zero
is needed for achieving global minimum, we can make the corresponding row of W to be zero and
have the same loss as being a global minimum. For the same reason, note that for any non-zero
weights pair (W, ev) that achieves the global minimum can be rescaled to have the same loss with
any v we want. Thus, a global optimum with respect to W is also a global optimum with respect to
(W, ev). k ≥ d implies the number of hidden nodes is larger than the number of input nodes, which
is very common in over-parametrized deep learning era.
3
Under review as a conference paper at ICLR 2021
To prove Theorem 1 we make use of the semi-definite property of the Wirtinger hessian matrix. We
will show that a local minimum W must satisfy the expression in Lemma 3 which makes it to be a
global minimum. The first half of the proof is similar to the proof of Theorem 2.1 in (Soltanolkotabi
et al., 2019). We derive a nice and simple expression of the hessian multiplied by an arbitrary
direction U. Wirtinger calculus is used to extend their steps to the complex case. The second half
of the proof turns out to be very different because we avoided their main assumption that v having
at least d positive entries and at least d negative entries, which is unrealistic. By using the property
of complex numbers, we can show that a local minimum W must satisfy the expression in Lemma 3
by contradiction. A full proof can be found in Section 3.
We also explore the optimization landscape of CReLU activated CVNNs. See the definition of
CReLU in Appendix A.3.
Theorem 2.	Assume the dataset is {xi, yi} for i = 1, 2, . . . , n with xi ∈ Rd and yi ∈ R. The train-
ing model we consider is one hidden layer complex-valued neural networks with CReLU activation.
It is in the form of
x 7→ vT CReLU(Wx)
with W ∈ Ck×d, and v ∈ Ck. Suppose the dataset is real-valued, xi ’s are distinct, the hidden layer
has a width of at least 2, and cannot be fitted linearly, then the loss function
1n
l(w)=厂 Σ k yi - yTCReLu(Wx) k2
2n
i=1
has infinitely many spurious local minima.
Unlike quadratic functions, CReLU is not analytic. Therefore, CVNNs activated by CReLU having
poor local minima is not beyond our expectation. The proof is built on (Yun et al., 2019) with some
modifications to fit in the complex case. Firstly we construct a point that is as good as the global
minimum of a linear model. Then we prove that point is a local minimum. The rest of the proof is
the same as in (Yun et al., 2019). They showed that there exists a strictly better point to prove that
the local minimum is spurious. The proof is provided in Appendix A.3 due to space constraints.
2	Preliminaries
In this section we provide some introduction to complex analysis and Wirtinger calculus. See more
definitions and lemmas in the appendix.
2.1	Notations and useful identities
Let R denote the real field and C denote the complex field. Since both of them are fields, they share
many common properties. Notice that R ⊆ C and Rm×n ⊆ Cm×n . Let z = z1 + iz2 ∈ C, we use
||z|| =，z2 + z2 ∈ R to denote its modulus and z* = zɪ - iz2 to denote its conjugate. R(Z) and
I(z) are used to denote the real and imaginary part of a complex vector z∈ Cn. For M∈ Cm×n,
MT and M* are transpose and conjugate transpose of M, MC denotes the matrix whose entries are
conjugates of entries in M, Null(M) := {v | Mv = 0} denotes the null space, and vec(M) denotes
the vectorization of M. For z ∈ C and M ∈ Cm×n, we have z* = zC and M* = (MC)T.
2.2	Complex functions
A complex function f : C 7→ C is given by f(z) = u(z) + iv(z). We can also think of f as f :
R2 7→ R2 where f(x, y) = (u(x, y), v(x, y)). A complex-valued multivariate function f : Cn 7→ C
is given by f(z) = u(z) + iv(z) where u(z), v(z) ∈ R. The function we will consider most in this
paper is a real-valued function with matrix input f : Cm×n 7→ R.
A complex function is analytic if it is differentiable at every point and the point’s neighbourhood in
the domain. An analytic function must satisfy the Cauchy Riemann equations (CRE). See Appendix
A.1.1 for the definitions of differentiable, analytic, CRE, and more. We mention that a non-constant
real-valued complex function does not satisfy the CRE and is not analytic.
4
Under review as a conference paper at ICLR 2021
Recall that the loss function in Theorem 1 is
L(W) = 21n XX k yi-vTΨ(Wχi) k2
i=1
which is not analytic. However, yi - vT ψ(Wχi) is analytic for each i, which indicates their deriva-
tives are well defined.
2.3	Wirtinger calculus
Since L(W) is not differentiable in the traditional sense, we require a new way of calculating the
complex gradient. Many non-differentiable complex functions are in fact differentiable in the real
sense if we treat Cn as R2n. L(W) is one of them. Wirtinger calculus, also known as the CR-
Calculus, is a neat way of deriving the derivatives. For a differentiable complex function, Wirtinger
derivatives are the same as the traditional derivatives. By using Wirtinger calculus we not only have
defined derivatives that reflect a function’s gradient, but also have meaningful results on the first and
second derivatives. Critical points, positive semi-definite Hessian, and Taylor expansions all have
their counterparts in Wirtinger calculus. We provide few important exposition of Wirtinger calculus
here. More explanations are provided in the appendix and a systematic introduction can be found in
(Kreutz-Delgado, 2005) and (Bouboulis, 2010).
Consider the complex-valued function f : Cn 7→ C, f(z) = u(χ, y) + iv(χ, y). The Wirtinger
derivative and the conjugate Wirtinger derivative are defined to be
where
∂f :=1 (f + i∂f∖ = 1 (匹 _ ∂v∖
∂zj ' 2 Idxj Zdyj) 2 Idxj ∂yj)
Note that the Wirtinger derivative is well defined as long as the real functions u and v are differ-
entiable with respect to χ and y. In our case, the loss function L(W) has well-defined Wirtinger
derivative.
See the definition of conjugate-complex derivative and conjugate-complex differentiable in the ap-
pendix. We now have the following lemma which follows directly from the definitions.
Lemma 1. If f is complex differentiable, then its Wirtinger derivative is the same as the normal
derivative, while the conjugate Wirtinger derivative is equal to zero.
dff = f0, ∂ZC = 0.
Similarly, if f is conjugate-complex differentiable, then its conjugate Wirtinger derivative is equal to
the normal conjugate-complex derivative, while the Wirtinger derivative is equal to zero.
* = f：，f = 0.
dzC	dz
Wirtinger derivative share many properties as normal derivatives like linearity, product rule, and
chain rule. We have more explanations in the appendix.
Lastly we provide expressions for Wirtinger gradient, Wirtinger Hessian, and the second order Tay-
lor’s expansion formula.
~ ,.
▽ f (z)
5
Under review as a conference paper at ICLR 2021
f(z + h) = f(z) + (Vf(z))* ∙ O + 1(h*, hτ) ∙ V2f(z) ∙ O + o(∣∣h∣∣2)
A point z is called a critical point of f if and only if Vf (z) = 0. Since the loss function we will
be analyzing is real-valued, as in the standard setting, if W is a local minimum of L(W), then the
Wirtinger’s Hessian of L(W) is positive semi-definite. The formal statement is provided in Section
3 and the proof can be found in the appendix.
3	CVNNs have better optimization landscape
We prove Theorem 1 in this section step by step. The framework and techniques used here can
provide insights for future work in analysing the optimization landscape of complex networks.
3.1	Derivative calculations
Firstly we observe that L(W) is a function which maps complex input to real output, i.e. L(W) :
Ck×d 7→ R, and it is not differentiable because conjugate functions do not satisfy the CRE (Cauchy-
Riemann Equation). Now let L(W) = * P2ι Li (W)*Li (W) where Li(W) : Ck×d → C given
by Li (W) = yi - vTψ(Wxi). Observe that Li (W) is complex differentiable in the traditional
sense and thus Li(W) has well-defined first and second derivatives. For a fixed i, we let Gi(W) =
Li(W)*Li(W). We now show how to calculate the derivatives of Li (W) and Gi(W). The derivatives
of L(W) follow easily by linearity.
3.1.1	DERIVATIVE CALCULATIONS OF Li (W)
Now we have
Li (W) = vTψ(Wxi) - yi
which is complex differentiable. The first derivative with respect to the q-th row of W is denoted by
Vwq Li (W) and we have
Vwq Li (W) = vqψ0 (hwq, xii)xi
VWLi(W) = Dvψ0 (Wxi)xiT
where Dv = diag(v1 , . . . , vk). And the second derivatives
∂2	00
∂W2Li(W) = VpΨ (hwp,Xii)XiXT
∂2
7;——Li(W) = 0
∂wpwq
for p 6= q.
3.1.2	DERIVATIVE CALCULATIONS OF Gi(W)
By the product rule of Wirtinger calculus, we have
VWGi(W) =VW(Li*Li)(W) =VWLi*(W)Li(W)+VWLi(W)Li*(W) = VWLi(W)Li*(W).
VWLi*(W)Li(W) = 0 since Li* is conjugate-complex differentiable. Similarly,
VWCGi(W) = VWC(Li*Li)(W) = VWC Li* (W)Li (W)+VWC Li (W)Li* (W) = VWCLi*(W)Li(W).
Based on that we have the second derivatives
V2WGi(W) = V2WLi(W)Li* (W)
V2WC Gi (W) = V2WC Li* (W)Li(W)
VwpVwpCGi(W) = vp*vpψ0(hwp,Xii)*ψ0(hwp,Xii)XiCXiT
Vwq VwpC Gi (W) = vp*vqψ0(hwp, Xii)*ψ0(hwq, Xii)XiCXiT
VwpC Vwp Gi (W) = vp*vpψ0 (hwp, Xii)*ψ0 (hwp, Xii)XiXi*
VwqCVwpGi(W) = vpvq*ψ0 (hwp, Xii)ψ0 (hwq, Xii)*XiXi*.
Notice that VWVWC Gi (W) and VWCVWGi(W) are kd × kd matrices.
6
Under review as a conference paper at ICLR 2021
3.2 Technical lemmas
We provide some important lemmas before proving the theorem. The proofs can be found in the
appendix.


Lemma 2. If W is a local minimum of L(W),
where
Ve2L(We) =	VW VWC L(W)
V2WL(We)
for all h ∈ Ckd.
0 ≤ (h*, hτ) ∙V2L(W) ∙
∈R
V2WC L(We)
VWC VW L(We)
Proof. See Appendix.
Lemma 3. (Extension of Lemma 6.1 in (Soltanolkotabi et al., 2019) to the complex case.) Any
point We ∈ Ck×d obeying
1- X(XTWTdiag(V)WXi- yi)*χiXT = 1- X Li (W)XiXT = 0
2n	2n
i=1	i=1
is a global optimum of the loss function
1n
L(W) = ʒ- £ Il XTWTdiag(v)WXi - y ||2
2n
i=1
1n
=诟£ Il yi- vTψ(wχi) Il2.
n i=1
Proof. See Appendix.
3.3 Proof of Theorem 1
Let W ∈ Ck×d be a local minimum. Let U ∈ Ck×d be an arbitrary direction and h = vec(U). We
define
H = ∣(h*, hT) ∙ V2L(W) •
=2(Vec(U)*, Vec(U)T) (VaWC(WW)
V2WCL(W)	vec(U)
VWCVWL(W)	Vec(U)C
By linearity,
H _ 1(Vec(U)* Vec(U)T) (2n Pn=ι VWVWCGi(W)
H =2 (Vec(U) , Vec(U)) ( 2n Pn=1 VWGi(W)
2n P乙 VWCGi(W) M Vec(U八
2n pn=ι VWCVWGi(W)J IVec(U)。广
For each term we haVe
(Vec(U; Vec(U)T) (VWVvWC(¾(W)
V2WCGi(W)	Vec(U)
VWC VWGi(W)	Vec(U)C
2R(Vec(U)T VWGi (W)Vec(U) + Vec(U)*VwVwc Gi(W)Vec(U)).
Now we consider two cases. Case 1 is for rank(DvW) = d and case 2 is for rank(DvW) < d.
For the first case, since k ≥ d and rank(DvW) = d, DvW has a left inVerse K ∈ Cd×k such
that KDVW = I. Notice that by VWL(W) = 0 and VWL(W) = 2n Pi= ι VWLi(W)L弘W)=
DVW(1 pi=ι C(W)XiXT), we haVe
1n
DvW( n ]Tl： (W)XiXT)=0.
n i=1
7
Under review as a conference paper at ICLR 2021
Multiplying both sides by K we get
1n
-ELr(W)XiXT = 0,
n i=1
which concludes the proof by Lemma 3.
For the second case, we can let U = abT with a ∈ Ck and Dva ∈ Null(WT). b ∈ Cd is an arbitrary
vector. We now show that Vec(U)r NWNWC Gi(W)Vec(U) = 0. Recall that
VwC Gi(W) = VwC Lr(W)Li(W) = vpψ0(hwp, Xi i)rxC Li(W)
NwpNwpCGi(W) = vprvpψ0(hwp,Xii)rψ0(hwp,Xii)XiCXiT =k vpψ0 (hwp, Xii) k2 XiCXiT
Vwq VwpC Gi (W) = vprvqψ0(hwp,Xii)rψ0(hwq,Xii)XiCXiT.
Therefore we can treat VWVWC Gi (W) as a k × k matrix with each entry being a d × d matrix. Now
by some algebra we haVe
Vec(U)rVWVWC Gi(W)Vec(U) =k XiT WT Dv UXi k2
where
XiTWTDvUXi = XiTWTDvabTXi = 0.
Now by linearity and Lemma 2 we haVe
1
2n
H
n
R(Vec(U)TXV2WGi(W)Vec(U))
i=1
≥0
where
nn
X Vec(U)TV2WGi (W)Vec(U) = X Vec(U)TV2WLi(W)Lir (W)Vec(U)
i=1	i=1
n
= 2 X Lir (W)(XiTUTDvUXi)
i=1
= 2(aTDva)bT X Lir (W)XiXiT b.
We argue that we can assume (aTDva) 6= 0 here. The reason is the following. Since a 6= 0, there
is an entry ai 6= 0. Suppose aTDva = 0, we can multiply vi by 4 and multiply the the i0th row
of W by 1. Now aτDvnew a = 3via22 = 0. Note that the two weight matrices W and Wnew have
the same null space. By lemma 2 the old matrix W together with v is a global minimum if and
only if the new matrix Wnew together with vnew is a global minimum, because their corresponding
M = WT diag(v)W is the same. Therefore, proving Wnew is a global minimum of Li is equivalent
to proving W is a global minimum. Thus, we can assume (aTDva) 6= 0 without loss of generality.
Let 2(aTDva) = aι + "a? ∈ C and bτ (Pn=I Lr(W)XiXT) b = bi + ib ∈ C. We now prove
Pn=ILr(W)XiXT = 0 by contradiction. Since H = *R((aι + a2i) ∙ (bi + b2i)) = *(aibi —
a2b2) ≥ 0, we prove if Pin=1 Lir(W)XiXiT 6= 0 then H < 0 for some (b1, b2). Since for a fixed
pair (ai, a2) we can make aibi — a2b2 a negative number simply by setting the signs of (bi, b2)
according to the signs of (ai, a2). For example, if ai > 0 and a2 < 0 then aibi — a2b2 < 0 for
(bi, b2) that bi < 0 and b2 < 0. Now let
n
M = X Lir (W)XiXiT 6= 0,M ∈ Cd×d.
i=i
Let Mi,j denotes the entry on the i’th row and the j’th column of M.
We prove that we can have any sign on bi and b2 , which implies M must be zero. Suppose there
exists a i ∈ [d] such that Mi,i 6= 0, then we let b = (0, . . . , βi, . . . , 0)T where βi can be any
complex number we want. Therefore, bi + ib2 = Mi,i ∙ βi can have any sign. Suppose Mi,i = 0
for all i ∈ [d] and Mi,j 6= 0 for some (i, j), then we let b = (0, . . . , βi, . . . , 0, . . . , βj, . . . , 0)T.
Now bi + ib2 = 2Mi,j ∙ βi ∙ βj which can have any sign.
Therefore, W is a global minimum by Lemma 3.
□
8
Under review as a conference paper at ICLR 2021
4 Discussion and future work
We studied the optimization of complex-valued neural networks for the first time. We proved spu-
rious local minima do not exist for a CVNN with analytic activation. The properties of complex
numbers endow CVNNs with better optimization landscape compared to real-valued networks. Our
result can serve as a strong reason for using complex networks. We showed that spurious local min-
ima exist for a CVNN with non-analytic activation. Thus, a promising future research direction will
be investigating the optimization landscape of CVNNs with analytic activations, for example, tanh.
It will also be interesting to extend our result to deep networks. Therefore, complex-valued neural
networks have the potential to be deep non-linear neural networks with practical activations having
no spurious local minima. Such result would be invaluable in modern deep learning era because
existence of poor local minima is one of the most intractable problems in deep learning due to the
non-convexity in optimization of neural networks.
References
Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks.
2016.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Pierre Baldi and Zhiqin Lu. Complex-valued autoencoders. Neural Networks, 33:136-147, 2012.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930-945, 1993.
N. Benvenuto and F. Piazza. On the complex backpropagation algorithm. IEEE Transactions on
Signal Processing, 40(4):967-969, 1992.
Avrim Blum and Ronald L. Rivest. Training a 3-node neural network is NP-complete. In Pro-
ceedings of the First Annual Workshop on Computational Learning Theory, COLT ’88, pp. 9-18,
1988.
Pantelis Bouboulis. Wirtinger’s calculus in general hilbert spaces. 2010.
Hyeong-Seok Choi, Janghyun Kim, Jaesung Huh, Adrian Kim, Jung-Woo Ha, and Kyogu Lee.
Phase-aware speech enhancement with deep complex U-net. In International Conference on
Learning Representations, 2019.
Anna Choromanska, Yann LeCun, and Gerard Ben Arous. Open problem: The landscape of the loss
surfaces of multilayer networks. volume 40 of Proceedings of Machine Learning Research, pp.
1756-1760, 2015.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals, and Systems (MCSS), (4):303-314.
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves. Associative long
short-term memory. Proceedings of Machine Learning Research, pp. 1986-1994, 2016.
Muneer Ahmad Dedmari, Sailesh Conjeti, Santiago Estrada, Phillip Ehses, Tony Stocker, and Martin
Reuter. Complex fully convolutional neural networks for MR image reconstruction. In Machine
Learning for Medical Image Reconstruction, pp. 30-38, 2018.
Chase Gaudet and Anthony Maida. Deep quaternion networks. In 2018 International Joint Confer-
ence on Neural Networks (IJCNN), pp. 1-8, 2018.
G. M. Georgiou and C. Koutsougeras. Complex domain backpropagation. IEEE Transactions on
Circuits and Systems II: Analog and Digital Signal Processing, 39(5):330-334, 1992.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural network. In Advances in Neural Information Processing Systems 32,
pp. 9111-9121. 2019.
9
Under review as a conference paper at ICLR 2021
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor-
mation Processing Systems 27,pp. 2672-2680. 2014.
E. Grant, Marcello Benedetti, Shuxiang Cao, A. Hallam, J. Lockhart, V. Stojevic, Andrew G. Green,
and S. Severini. Hierarchical quantum classifiers. npj Quantum Information, 4:1-8, 2018.
Nitzan Guberman. On complex valued convolutional neural networks. ArXiv, abs/1602.09046,
2016.
Akira Hirose and Shotaro Yoshida. Generalization characteristics of complex-valued feedforward
neural networks in relation to signal coherence. IEEE Transactions on Neural Networks and
Learning Systems, 23(4):541-551, 2012.
Yuliang Qin Hongqiang Wang Jingkun Gao, Bin Deng and Xiang Li. Enhanced radar imaging using
a complex-valued convolutional neural network. IEEE Geoscience and Remote Sensing Letters,
16(1):35-39, 2019.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in Neural Information
Processing Systems 29, pp. 586-594. 2016.
Kenji Kawaguchi and Leslie Kaelbling. Elimination of all bad local minima in deep learning. volume
108 of Proceedings of Machine Learning Research, pp. 853-863, 2020.
Ken Kreutz-Delgado. The complex gradient operator and the CR-calculus. 2005.
Shiyu Liang, Ruoyu Sun, Jason D. Lee, and R. Srikant. Adding one neuron can eliminate all bad
local minima. In Advances in Neural Information Processing Systems 31, pp. 4350-4360. 2018.
Gordon R. Little, Steven C. Gustafson, and Robert A. Senn. Generalization of the backpropagation
neural network learning algorithm to permit complex weights. Applied Optics, 29(11):1591-1592,
1990.
Alexey Minin. Complex valued recurrent neural network: From architecture to training. Journal of
Signal and Information Processing, 03:192-197, 2012.
Tohru Nitta. On the critical points of the complex-valued neural network. In Proceedings of the 9th
International Conference on Neural Information Processing, 2002. ICONIP ’02., volume 3, pp.
1099-1103 vol.3, 2002.
Anupama Pande, Ashok Kumar Thakur, and Swapnoneel Roy. Complex-valued neural network
in signal processing: A study on the effectiveness of complex valued generalized mean neuron
model. International Journal of Electrical and Computer Engineering, 2(1):39 - 44, 2008.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimiza-
tion landscape of over-parameterized shallow neural networks. IEEE Trans. Inf. Theor., 65(2):
742-769, 2019.
Qigong Sun, Xiufang Li, Lingling Li, Xu Liu, Fang Liu, and Licheng Jiao. Semi-supervised
complex-valued GAN for polarimetric SAR image classification. In IGARSS 2019 - 2019 IEEE
International Geoscience and Remote Sensing Symposium, pp. 3245-3248, 2019.
Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. Hermitian co-attention networks for text matching in
asymmetrical domains. In Proceedings of the Twenty-Seventh International Joint Conference on
Artificial Intelligence, IJCAI-18, pp. 4425-4431, 2018.
Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep Subramanian, Joao Fe-
lipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua Bengio, and Christopher J Pal. Deep
complex networks. In International Conference on Learning Representations, 2018.
Scott Wisdom, Thomas Powers, John R. Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity
unitary recurrent neural networks. In Proceedings of the 30th International Conference on Neural
Information Processing Systems, pp. 4887-4895, 2016.
10
Under review as a conference paper at ICLR 2021
Moritz Wolter and Angela Yao. Complex gated recurrent neural networks. In Advances in Neural
Information Processing Systems 31 ,pp. 10536-10546. 2018.
Chenwei Wu, Jiajun Luo, and Jason D. Lee. No spurious local minima in a two hidden unit ReLU
network, 2018.
Liyao Xiang, Hao Zhang, Haotian Ma, Yifan Zhang, Jie Ren, and Quanshi Zhang. Interpretable
complex-valued neural networks for privacy protection. In International Conference on Learning
Representations, 2020.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad
local minima in neural networks. In International Conference on Learning Representations, 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Supplements of Section 2
A.1.1 More on complex analysis
We provide some basic definitions of univariate complex functions. The generalization of multivari-
ate functions is the same as in the real case.
Let f : C 7→ C given by f (z) = u(z) + iv(z) where z = x + iy.
Definition 1. Suppose that f is defined on some open neighbourhood of z0. Then, the derivative of
f at z0 is given by
f0(z0)
lim
∆z→0
f(z0 + ∆z) - f(zo)
∆
where ∆z = ∆x + i∆y, provided this limit exists. Such an f is said to be differentiable at z0 .
Definition 2 (Analytic functions). A complex function f (z ) is called analytic at the point z0 if it is
differentiable at z0 and in a neighbourhood of z0 .
Some examples of analytic functions include all polynomials, trigonometric functions, and expo-
nential functions.
Definition 3 (Cauchy-Riemann equations). If f0 (z) exists, the partials ofu and v exist at (x, y) and
satisfy the Cauchy-Riemann equations
∂u	∂v	∂u	∂v
∂χ (χ,y) = ∂y (x,y) and ∂y (x,y) = - ∂x (x,y).
Theorem 3 (Necessary conditions for differentiability). Suppose that f is differentiable at z. Then
the Cauchy-Riemann equations hold at Z and f 0 (z) = dU (x,y)+ i Iv (x,y) = Iy (x,y) - i du (x,y).
Theorem 4 (Sufficient conditions for differentiability). Suppose f(z) is defined throughout some
open neighbourhood U of the point zo = x0 + iyo, and suppose that ∣χ, ∣y, ∣χ, ∣v exist everywhere
in U. Then, if ∣χ, ∣y,翁,∂vy are continuous at (xo,yo) and satisfy the Cauchy-Riemann equations
at (xo,yo),then f is differentiable at zo and f 0(z)=瑞(x,y) + i∣χ(x,y) = ∣y (x,y) - i1(x,y).
Let z* denotes the conjugate of Z and |z| denotes the modulus of z. We list some properties of
complex numbers. For all z, y ∈ C, we have the following.
|z*| = |z|
zz* = |z|2
z*
Z = 2-2 if z = 0
|z|2
z + z*	z - z*
R(Z) =	2	and I(Z) = -2i-
|zy| = |z||y|
|zn| = |z|n
A.1.2 More on Wirtinger calculus
We provide more details on Wirtinger calculus that we use in proving theorem 1.
Let f : Cn 7→ C given by f(z) = u(z) + iv(z) where z = x + iy.
Definition 4. Conjugate Cauchy Riemann conditions (CCRC).
∂u
∂x
∂u
and —
∂y
∂v
∂ X
12
Under review as a conference paper at ICLR 2021
Proposition 1. If f is differentiable in the real sense at (x, y) and the CCRC hold, then f is
conjugate-complex differentiable.
Proposition 2. If f is conjugate-complex differentiable at z then u and v are differentiable in the
real sense and they satisfy the conjugate Cauchy Riemann conditions.
Proposition 3. If f is differentiable in the real sense, then
f and
∂zC
∂fC
∂z .
Proposition 4 (Linearity). If f, g are differentiable in the real sense and α, β ∈ C, then
∂(af + βg)	∂f =α—	+e也
∂ Z	∂z	, ∂Z
∂ (αf + βg)=	∂f	+ β生
∂zc	α∂ ZC	+ β∂ ZC
Proposition 5 (Product Rule). If f, g are differentiable in the real sense, then
f) = ∂f + ∂gf
g,
∂z	∂z	∂z
f) = f。+®f
∂ZC = ∂ZC g + ∂zc f
Proposition 6 (Chain Rule). If f,gare differentiable in the real sense, then
∂(f ◦ g)
∂z
f ⑹ dgz+∂ZC(f) ⅛^
d(∂zCg)=f ⑷ ∂Zc+∂fC(f) dgc.
A.2 Omitted proofs
A.2.1 Proof of Lemma 2
We first prove that it is a real value. By linearity it is sufficient to show
(h*, hτ) ∙V2Gi(W) ∙ (m ∈ R.
Let h = vec(U) be an arbitrary direction. Since
(V2WGi(W))C = V2WCGi(W)
and
(VWVWCGi(W))C = VWC VWGi (W),
we have
(Vec(U)T VWC VWGi(W)Vec(U)C )c = Vec(U)*VwVw° Gi(W)Vec(U)
and
(Vec(U)T VW Gi(W)Vec(U))C = Vec(U)*VW0 Gi(W)Vec(UC).
Thus,
(Vec(U)* Vec(U)T JVWVWCGi(W)	VWCGi(W) ʌ V Vec(U八
(Vec(U) , Vec(U) )	V2WGi(W)	VWC VWGi (W)	Vec(U)C
= (Vec(U)TV2WGi (W)Vec(U) + Vec(U)TVWCVWGi(W)Vec(UC)
+ Vec(U)*VwVwc Gi(W)Vec(U) + Vec(U)*VWc Gi(W)Vec(UC))
=2R(Vec(U)TVWGi(W)Vec(U) + Vec(U)*VWVWCGi(W)Vec(U)) ∈ R
13
Under review as a conference paper at ICLR 2021
Now suppose W is a local minimum, the second order expansion at W is
LCW + U) = L(W) + VLCW) ∙ (h) +1(h,h*) ∙V2L(W) ∙e)+ o(∣∣h∣∣2).
Since W is a local minimum, the gradient is zero. When ||h|| is small enough,
1(h, h*) ∙ V2L(W) ∙ (hɪɔ = L(W + U)- L(W) ≥ 0.
□
A.2.2 Proof of Lemma 3
Let M = WTdiag(v)W. Then loss function becomes L(M) = ɪ pn=1 ∣∣ XTMxi - yi k2 . By some
algebra, we write
1n
L(M) = 2n EXM*xT*xTMxi- 2R(yixMxi)+ k y k2).
n i=1
Notice that x*M*xT*xTMxi =∣ xTMxi ∣∣2. From the expression We can see L(M) is convex in M
because R(y*xTMxi) and xTMxi are linear with respect to M.
NoW by Wirtinger calculus and the convexity,
1n
Me being a global minimum of L(M) ⇔ 2n E(XTMxi - yi)*xixT = 0.
n i=1
The rest follows the proof of Lemma 6.1 in (Soltanolkotabi et al., 2019).
□
A.3 EXISTENCE OF SPURIOUS LOCAL MINIMA IN CVNNS WITH CRELU
We prove spurious local minima exist for shallow CVNNs with CReLU. We define CReLU first.
Definition 5 (CReLU). ∀z ∈ C,
CReLU(z) = ReLU(R(z)) + iReLU(I(z))
where
ReLU(x) = max(0, x)
for x ∈ R. Thus ∀z ∈ Cn, CReLU(z) means (CReLU(z1), . . . , CReLU(zn)).
As in (Yun et al., 2019), we make the activation in the proof to be more general.
Definition 6. ∀z ∈ C,
,. ʌ , ... ʌ , ..
h(z) = h(R(z)) + ih(I(z))
where
h(x) = max(0, s+x) + min(0, s-x)
for x ∈ R. We let s+ > 0, s- ≥ 0 and s+ 6= s-. Note that CReLU is a member of this class.
We now prove Theorem 2. The proof is built on (Yun et al., 2019). The first step is to construct a local
minimum that is as good as the linear solution, which is essentially the same as in (Yun et al., 2019)
with some modifications. Let X ∈ Rd×n denotes the input data where each column corresponds to a
single data point. Without loss of generality, we define the augmented X and W to be [XT 1n]T and
[W b], and the loss function to be L(Wι, W2) = 2n k W2h(W1X+b1)+b2IT-y kF. Note that the
expression of loss function here is more general than the expression we stated in Theorem 2. Now
we define a linear least square loss '(W) = 2n ∣∣ WX 一 y kF and its minimizer W ∈ C1×(d+1). By
T
Wirtinger's derivative we have V'(W) = (WX ― y)*X = 0. Let y = WX where y = W[xT 1]T.
14
Under review as a conference paper at ICLR 2021
Let η ∈ C satisfies that R(η) = min{-1, 2miniR(y∕} and I(η) = min{-1,2mi&I(yji)}. Let
[M][d] denotes the first d components of M and [M]d+1 denotes the last component. As in (Yun
et al., 2019) we define parameters
^	-	- r	-
[W][d]
0(k-1)×d
,bi = α
[W]d+i - η
-η1d-1
t ∙ .	1	1 1 1.1	z-* /ɪɪr G ɪɪr ∙a^' ∖	1 11 —	11 9	/) / ɪ ɪ τ ∖ TTT	. 1 . ∙ . ∙
and it can be checked that L(Wi, bi, W2, b2)=吉 ∣∣ y - y kF = '(W). We now prove that it is a
local minimum of L. We slightly permute the parameters and prove their risk is always larger. Let
the permuted parameters be Wi + i, bi + δi, W2 + 2, and b2 + δ2. Note that
(WX - y)*XT = (y - y)*[XT 1„] = 0,
which means (y - y)*XT = 0 and (y - y)*1n = 0. We mention that for a matrix M ∈ Ck×n its
Frobenius norm can be written as
kn
k M kF= XX |mij |2 = trace(M*M).
i=i j=i
Therefore, for small enough i and δi such that (Wi + i)xi + bi + δi > 0 for all i,
.,ʌ ʌ ʌ ʌ _ 、
L(Wi + ei, bi + δι, W2 + % ^2 + δ2)
=ɪ k y -y + s+(W2e1 + W 1e2 + e1e2)x + s+(W2δι + e2b1 + β2δι)iT kF
2n	n
=ɪ k y - y kF +ɪ k s+(W2e1 + W 1e2 + e1e2)x + s+(W2δ1 + e2b1 + e2δι)iT kF
2n	2n	n
.,ʌ ʌ ʌ ʌ ,
≥L(W 1, bi, W 2, b2).
Since the inequality holds for any α > 0, we showed that there are infinitely many local minima.
The rest of the proof follows the step 2 of the proof of Theorem 1 in (Yun et al., 2019). While
our networks are complex-valued, we assumed the dataset are real-valued in Theorem 2. Besides,
since Step 2 is to construct a point strictly better than the local minimum constructed before, the
rest part of the proof is exactly the same as in there. There is no need to extend their proof to the
complex-valued case. For readers who are not familiar with their proof, we provide a sketch proof.
They defined a set J := {j ∈ [m - 1]| Pi≤j(yyi - yi) 6= 0, yyj < yyj+i} based on the y labels of the
dataset. There are two cases, J = 0 and J = 0, whose main ideas are essentially the same.
For the case of J 6= 0, by careful choices of weights Wi, bi, W2, b2, and parameters β, γ,
they showed the loss of the new weights are strictly better than that of the old weights. Let
yyi = [W][dx]xi + [W]dx+i and L0(W) denotes the loss of the previous local minimum. It was
shown that
.,~ ~ ~ ~
L(W i, bi, W 2, b2)
s+F γ -@)2 +1 X (yi + 任产 γ - y,)2
s++s-	2 i>j0	s++s-
2 IXe
S+≡ γ+O(γ 2)
which concludes the previous weight is spurious local minimum.
15
Under review as a conference paper at ICLR 2021
Similarly, for the case of J = 0, by careful choices of weights Wι, bi, W2, b2, and parameters α,
β, γ, it was shown that
.,~ ~ ~ ~
L(W 1, bi, W 2, b2)
1	, _ T	S+ — S-	、2	1 z _ T	S+ — S-	、
=工工(y - au	Xi--:p-Y -	yi)	+ 2 ʌ (y	-	au	Xi	+ --—Y - Vi)
2	i≤jι	s+ + S-	2 i>jl	s+ + s-
1	m
2	%(Vi - aurXi - Vi)2 +
i=1
E (Vi - auτXi - Vi) - £ (Vi - auτXi - Vi)
i>jι	i≤jι
Sτ≡ γ+o(y2)
U + O(02) + X (Vi - Vi) - X(Vi - Vi) s+ - S- Y + o(αγ)+ O(γ2)
匕	i≤j	s+ + s-
which concludes the previous weight is spurious local minimum.
□
16