Under review as a conference paper at ICLR 2021
H-divergence: A Decision-Theoretic Probabil-
ity Discrepancy Measure
Anonymous authors
Paper under double-blind review
Ab stract
Measuring the discrepancy between two probability distributions is a fundamental
problem in machine learning and statistics. Based on ideas from decision theory,
we investigate a new class of discrepancies that are based on the optimal deci-
sion loss. Two probability distributions are different if the optimal decision loss is
higher on the mixture distribution than on each individual distribution. We show
that this generalizes popular notions of discrepancy measurements such as the
Jensen Shannon divergence and the maximum mean discrepancy. We apply our
approach to two-sample tests, which evaluates whether two sets of samples come
from the same distribution. On various benchmark and real datasets, we demon-
strate that tests based on our generalized notion of discrepancy is able to achieve
superior test power. We also apply our approach to sample quality evaluation as
an alternative to the FID score, and to understanding the effects of climate change
on different social and economic activities.
1	Introduction
Quantifying the difference between two probability distributions is a fundamental problem in ma-
chine learning. Modelers choose different types of discrepancies, or probability divergences, to
encode their prior knowledge, i.e. which aspects should be considered to evaluate the difference,
and how they should be weighted. The divergences used in machine learning typically fall into two
categories, integral probability metrics (IPMs, Muller (1997)), and f -divergences (Csiszar, 1964).
IPMs, such as the Wasserstein distance, maximum mean discrepancy (MMD), are based on the idea
that if two distributions are identical, any function should have the same expectation under both
distributions. IPM is defined as the maximum difference in expectation for a set of functions. IPMs
are used to define training objectives for generative models (Arjovsky et al., 2017), perform in-
dependence tests (Doran et al., 2014), robust optimization (Esfahani & Kuhn, 2018) among many
other applications. On the other hand, f -divergences, such as the KL divergence and the Jensen
Shannon divergence, and are based on the idea that if two distributions are identical, they assign the
same likelihood to every point, so the ratio of the likelihood always equals one. One can define a
distance based on the how the likelihood ratio differs from one. KL divergence underlies some of
the most commonly used training objectives for both supervised and unsupervised machine learning
algorithms, such as minimizing the cross entropy loss.
We propose a third category of divergences called H -divergences that overlaps with but does not
equate the set of integral probability metrics or the set f -divergences. Our distance is based on
a generalization (DeGroot et al., 1962) of Shannon entropy and the quadratic entropy (Burbea &
Rao, 1982). Instead of measuring the best average code length of any encoding scheme (Shannon
entropy), the generalized entropy can choose any loss function (rather than code length) and set of
actions (rather than encoding schemes), and is defined as the best expected loss among the set of
actions. In particular, given two distribution p and q, we compare the generalized entropy of the
mixture distribution (p + q)/2 and the generalized entropy of p and q individually. Intuitively, if
p and q are different, it is more difficult to minimize expected loss under the mixture distribution
(p + q)/2, and hence the mixture distribution should have higher generalized entropy; ifp and q are
identical, then the mixture distribution is identical to p or q, and hence should have the same gen-
eralized entropy. We define the divergence based on the difference between entropy of the mixture
distribution and the entropy of individual distributions.
1
Under review as a conference paper at ICLR 2021
Figure 1: Relationship between H-divergence (this paper) and existing divergences. The Jensen
Shannon divergence is an f -divergence but not an IPM; the MMD is an IPM but not an f -divergence;
both are H-divergences. There are also H-divergences that are not f -divergences or IPMs.
Our distance strictly generalizes the maximum mean discrepancy and the Jensen Shannon diver-
gence. We illustrate this via the Venn diagram in Figure 1. This generalization allows us to choose
special losses and actions spaces to leverage inductive biases and machine learning models from
different problem domains. For example, if we choose the generalized entropy as the maximum
log likelihood of deep generative models, we are able to recover a distance that works well for
distributions over high dimensional images.
To demonstrate the empirical utility of our proposed divergence, we use it for the task of two sample
test, where the goal is to identify whether two sets of samples come from the same distribution
or not. A test based on a probability discrepancy declares two sets of samples different if their
discrepancy exceed some threshold. We use H-divergences based on generalized entropy defined
by the log likelihood of off-the-shelf generative models. Compared to state-of-the-art tests based on
e.g. MMD with deep kernels (Liu et al., 2020), tests based on the H-divergence achieve better test
power on a large set of benchmark datasets.
As another application, we use H-divergence for sample quality evaluation, where the goal is to
compare a set of samples (e.g. generated images from a GAN) with ground truth samples (e.g. real
images). We show that H-divergences generally monotonically increase with the amount of corrup-
tion added to the samples (which should lead to worse sample quality), even in certain situations
where the FID score (Heusel et al., 2017) is not monotonically increasing.
Finally we show that H-Divergence can be used to understand whether distribution change affect
decision making. As an illustrative example, we study whether climate change affect decision mak-
ing in agriculture and energy production. Traditional divergences (such as KL) let policy makers
measure if the climate has changed; H-Divergence can provide additional information on whether
the change is relevant to decision making for different social and economic activities.
2	Background
2.1	Probability Distances
Let X denote a finite set or a finite dimensional vector space, and P(X) denote the set of prob-
ability distributions on X that have a density. We consider the problem of defining a probability
divergence between any two distributions in P(X ), where a probability divergence is any function
D : P(X) × P(X) → R that satisfies D(pkq) ≥ 0, D(pkp) = 0, ∀p, q ∈ P(X) (Note that in
general a divergence does not require D(pkq) > 0 ∀p 6= q).
Integral Probability Metrics Let F denote some set of functions X → R. The integral probabil-
ity metrics is defined as
IPMF(pkq)= sup|Ep[f(X)]-Eq[f(X)]|
f∈F
Several important divergences belong to integral probability metrics. Examples include the Wasser-
stein distance, where F is the set of 1-Lipschitz functions; the total variation distance, where F is
the set of functions X → [-1, 1]. The maximum mean discrepancy (MMD) (Rao, 1982; Burbea &
Rao, 1984; Gretton et al., 2012) chooses a kernel function k : X × X → R+ and is defined by
MMD(pkq) =Ep,pk(X,Y)+Eq,qk(X,Y)-2Ep,qk(X,Y)
2
Under review as a conference paper at ICLR 2021
MMD is an IPM where F is the unit norm functions in the RKHS associated with the kernel k.
f -Divergences Choose any convex continuous function f : R+ → R such that f (1) = 0, the
f -Divergence is defined as (assuming densities exist) Df (pkq) = Eq [f (p(X)/q (X))]. Examples of
f -Divergences include the KL divergence, where f : t 7→ t log t and the Jensen Shannon divergence,
where f : t → (t + 1) log ( t++ι 1 +1 log t.
Scoring Rule Distance (GrunWald et al., 2004; Gneiting & Raftery, 2007) Another large class of
probability distances are defined by proper scoring rules. A function S : P(X ) × P(X ) → R is
called a proper scoring rule if ∀p, q ∈ P(X) we have S(p, q) ≥ S(p, p). Intuitively it is any function
that is small when two distributions are identical and large when two distributions differ. Given a
scoring rule S we can define a distance by DS(pkq) = S(p, q) - S(p, p).
2.2	H-Entropy
For any action space A and any loss function ` : X × A → R, the H-entropy (DeGroot et al., 1962;
DeGroot, 2005; Grunwald et al., 2004) is defined as H'(p) = infa∈A Ep['(X, a)].
In words, H-entropy is the Bayes optimal loss of a decision maker who must select some action a
not for a particular x, but for an expectation over p(x).
H-entropy generalizes several important notions of uncertainty. Examples include: Shannon En-
tropy, where A as the set of probabilities P(X), and '(x, a) = - log a(x); Variance where A = X,
and '(x, a) = ∣∣x - a∣∣2; Predictive V-entropy, where A ⊂ P(X) is some subset of distributions,
and '(x, a) = — log a(x) (Xu et al., 2020).
The most important property that we will use is that the H entropy is concave.
Lemma 1. (DeGrOOt et al., 1962) For any choice of' : X ×A → R, h` is a concave function.
This Lemma can be proved by observing that inf is a concave function, i.e., it is always better to
pick an optimal action for p and q separately rather than a single one for both.
H'(αp +(1 — α)q) = inf (αEp['(X, a)] + (1 — α)Eq['(X, a)])
≥ a inf Ep['(X, a)] + (1 — α) inf Eq['(X, a)] = ɑH'(p) + (1 — α)H'(q)
a
a
This Lemma reflects why h` can be thought of as a measurement of entropy or uncertainty. If
the distribution is more uncertain (e.g. mixture of p and q rather than p and q separately) then the
optimal action always suffers a higher loss.
3	Definition and Theoretical Properties
3.1	H-Jensen Shannon Divergence
As a warm up, we first present a special case of our definition.
Definition 1 (H-Jensen Shannon divergence).
DJS (p,q) = H' (号)—2 (H'(p) + H'(q))
(1)
The above is a divergence between P and q because H-entropy is concave, so DJS is always non-
negative. In particular, if we choose H' as the Shannon entropy, Definition 1 recovers the usual
Jensen Shannon divergence. Other special choices of entropy can also recover definitions in (Burbea
& Rao, 1982). In addition, we can define a divergence for any convex combination αp + (1 — α)q
where α ∈ (0, 1) but for this paper we only consider α = 1/2.
3
Under review as a conference paper at ICLR 2021
3.2	General H-divergence
In addition to the H-Jensen Shannon divergence, there are other functions based on the H-entropy
that satisfy the requirements of a divergence. For example, the following quantity
DMm = H (审)-min(H'(p), H'(q))
(2)
is also a valid divergence (this will be proved later as a special case of Lemma 2). We can define a
general set of divergences that includes the above two divergences with the following definition:
Definition 2 (H-divergence). For two distributionsp,q on X, choose any continuous function φ :
R2 → R such that φ(θ, λ) > 0 whenever θ+ λ > 0 and φ(θ, λ) = 0 whenever θ+ λ = 0, define
Dφ(pkq) = o(h` (一)
-He(P),H
Intuitively H (p++q) - HXP) and H (p++q) - He(q) measure how much more difficult it is to
minimize loss on the mixture distribution (p + q)/2 than onp andq respectively. φ is a general class
of function that could convert these differences into a divergence, while satisfying the desirable
properties in the next section.
The H-divergence generalizes all the previous definitions, as shown by the following proposition.
Therefore any property of H-divergence is inherited by e.g. H-Jensen Shannon divergence.
Proposition 1. Choose φ(θ, λ) = θ++λ then Dφ(p, q) is the H-Jensen Shannon divergence in Eq.(1).
Choose φ(θ, λ) = max(θ, λ) then Dφ(p, q) is the H-Min divergence in Eq.(2).
3.3	Properties of the H-divergence
We first verify that Dφ is indeed a probability divergence. In particular, Lemma 2 show that Dφ
satisfies the requirements for a probability divergence. For the proof see Appendix A.
Lemma 2. For any choice of' andfor any choice of φ that satisfy Definition 2, Dφ is non-negative
and d' (p, q) = 0 whenever P = q.
One important property of the H-divergence is that two distributions have non-zero divergence if and
only if they have different optimal actions, i.e. the optimal solutions for their respective H-entropy
are different. This is shown in the following proposition (proof in Appendix A).
Proposition 2. arg inf。Ep['(X, a)] ∩ arg inf。Eq ['(X, a)] = 0 ifand only if Dφ(Pkq) > 0.
Intuitively, Dφ only takes into account any difference between distributions that lead to different
choice of optimal actions. This property allow us to incorporate prior knowledge about the problem.
By choosing the A and ` we can specify which differences between distributions lead to different
optimal actions, and which differences do not. The corresponding Dφ will only be non-zero when
two distributions differ in way we would like to capture.
For example, we can choose A as a set of generative models (e.g. VAES) and '(x, a) is the negative
log likelihood ofx under generative model a. If under two distributions we end up learning the same
generative model that maximizes log likelihood, the H-divergence between them is zero.
3.4	Relationship to MMD
An important special case of the H-divergence is the set of Maximum Mean Discrepency (MMD)
distances, as shown by the following theorem
Theorem 1.	The set of H-Jensen Shannon Divergences is strictly larger than the set of MMD dis-
tances.
4
Under review as a conference paper at ICLR 2021
To prove this theorem for each choice of kernel k : X × X → R+ we construct some A and `
(details in Appendix A) such that
H' (中)=1 Ep[k(X,X)] + 1 Eq[k(X,X)] - 4Ep,p[k(X, Y)] - 1 Eq,q[k(X, Y)] - 2Ep,q[k(X,Y)]
22	2	4	4	2
H'(p) = Ep[k(X,X)] - Ep,p[k(X,Y)] H'(q)= Eq[k(X,X )] - Eq,q [k(X,Y)]
Simple algebra show that applying Definition 1 recovers the MMD distance with kernel k.
3.5 Estimation and Convergence
In many downstream tasks, we would like to estimate the H-divergence from data. Specifically
We are provided with a set of i.i.d. samples Pm = (xi, ∙∙∙ , xm) drawn from distribution P and
^m = (χ1,…，xmm) drawn from distribution q, and would like to obtain an estimate of Dφ (Pkq)
based on the samples. In this section we propose an empirical estimator for the H-divergence and
show that it has nice convergence properties.
Let Dφ(Pm ∣∣^m) be the empirical (random) estimator for Dφ(Pkq) defined by
1m
Φ I inf mE'(xi0, a)
i=1
1m	1m
一 inf	inf	— inf
am	am	a
i=1	i=1	i=1
where x0i0 = xibi +x0i(1 - bi) is a sample from the mixture distribution (P + q)/2 for bi uniformly
sampled from {0, 1}.
Before presenting the convergence results, we first must define several assumptions that make con-
vergence possible. In particular, we are going to assume that the loss function ` is C-bounded, i.e.
there exists some C such that 0 ≤ '(x, a) ≤ C, ∀a, x. In addition, we assume that φ is 1-Lipschitz
under the ∞-norm, i.e. ∣φ(θ + dθ,λ + dλ) — φ(θ, λ)∣ ≤ max(dθ, dλ), ∀θ, λ, dθ, dλ ∈ R . If φ
is not 1-Lipschitz we can often rescale φ to make it 1-Lipschitz. Finally, define the Radamacher
complexity of ` as
Rm(I) = EXi~p,ea ~Uniform({ —1,1})
1m
sup - Tei'(Xi, a)
a∈A m i=1
Based on these assumptions we can bound the difference between Dφ(Pmkqm) and Dφ(Pkq).
Theorem 2.	If' is C-bounded, and φ is 1 -Lipschitz under the ∞-norm,for any choice Ofdistribution
P, q ∈ P(X) and t > 0 we have
2
1. PrD φ(Pmk^m) ≥ t] ≤ 4e-2C2 * 4 if P = q.
2. Pr [∣DΦ(P^mkqm) — Dφ(Pkq) I ≥ 4max(Rm('), Rm(I)) + t] ≤ 4e-2C2
For proof see Appendix A. What is most interesting about Theorem 2 is that when P = q, the con-
vergence of Dφ(Pmk^m) does not depend on the Radamacher complexity of '. This is particularly
useful for two sample test, where we would like to decide if P = q based on finite samples. If P = q
the empirical estimate Dφ(Pmkqm) will quickly converge to 0 for relatively small sample size m.
4 Experiment: Two Sample Test
4.1 Two Sample Test
For the task of two sample test, we would like to decide if two sets of samples are drawn from the
same distribution. Specifically, given two sets of samples Pm := (xi, •一，xm,) it.P and ‰ :=
(x；, ∙∙∙ , χmj) i%∙ q we would like to decide if P = q. Typically a two sample test algorithm estimates
some divergence D(Pmkqm) and outputs P = q if the divergence exceeds some threshold.
5
Under review as a conference paper at ICLR 2021
Figure 2: Average test power (Left) and the average type I error (Right) on the Blob dataset for
different sample sizes. For our method (dashed line), H-Div (10) and H-Div (15) use a mixture of
Gaussian distribution with 10 and 15 mixture components respectively (details in Section 4.3). Our
method has significantly better test power (left plot).
There are two types of errors: Type I error happens when p = q but the algorithm incorrectly
outputs p 6= q. The probability that an algorithm makes a type I error is called the significance level.
Type II error happens when p 6= q but the algorithm incorrectly outputs p = q . The probability an
algorithm does NOT make a type II error is called the test power (higher is better). Note that both
the significance level and the test power are defined relative to the distribution p and q.
We follow the typical setup where we would like to guarantee the significance level while empirically
measuring the test power. In particular, the significance level can be guaranteed with a permutation
test ( nst et al, 200 ). In a permutation test, in addition to the original set of samples Pm and qm,
We also uniformly randomly swap elements between Pm and qm, and sample multiple randomly
swapped datasets (Pm,qmm), (Pm,qml), ∙ ∙ ∙. The testing algorithm outputs p = q if D(PmIlqm) is
in the top α-quantile among {D(P1n), D(Pm|确),∙ ∙ ∙ }.1 Permutation test guarantees the sig-
nificance level (i.e. low Type I error) because if P = q then swapping elements between Pm and
qm should not change its distribution, so each pair (Pm, qm), (Pm, (^mm), ∙ ∙ ∙ should have the same
distribution. Therefore, D(Pm ∣∣qm) happens to be in the top α-quantile with at most α probability.
4.2	Experiment Setup
Dataset We follow Liu et al. (2020) and consider four datasets: Blob (Liu et al., 2020), HDGM
(Liu et al., 2020), HIGGS (Adam-Bourdarios et al., 2014) and MNIST (LeCun & Cortes, 2010).
Baselines We compare our proposed approach with six other divergences. All methods are based
on the permutation test explained in Section 4.
•	MMD-D & MMD-O: MMD-D (Liu et al., 2020) measures the MMD distance with a deep kernel,
while MMD-O (Gretton et al., 2012) measures the MMD distance with a Gaussian kernel.
•	ME & SCF: Mean embedding (ME) and smoothed characteristic functions (SCF) (Chwialkowski
et al., 2015; Jitkrittum et al., 2016) are distances based on the difference in Gaussian kernel mean
embedding at a set of optimized points, or a set of optimized frequencies.
•	C2STS-S & C2ST-L: (Lopez-Paz & Oquab, 2017; Cheng & Cloninger, 2019) use a classifier’s
accuracy distinguishing between the two distributions to measure the probability discrepancy.
4.3	Implementation Details
In the evaluation benchmark proposed in (Liu et al., 2020) there are two sets of samples, a training
set used to tune hyper-parameters, and a validation set used to for evaluating the test performance.
The training set and the validation set have the same number of samples. We also leverage the
training set, but we only have one choice of hyper-parameter, which is the function φ. In particular,
we choose φ(θ, λ) = (θs+λs) 1/s for s > 1 (which includes the H-Jensen Shannon divergence when
s = 1 and the H-Min divergence when s = ∞. We use the training set to find the best parameter
s. For fair comparison each baseline method also use the training set to tune hyper-parameters as
implemented in (Liu et al., 2020).
1
1If a tie is possible, then always break ties by sorting D(Pm ∣∣qm) in front. In this case the significance level
is at most α (and could be less).
6
Under review as a conference paper at ICLR 2021
Figure 3: Average test power and type I error on HDGM dataset. Left: results with the same sample
size (4000) and different data dimensions. Right: results with the same sample dimension (10) and
different sample sizes. Our method (H-Div, dashed line) achieve better test power for almost every
setup. In particular, when the data dimension is small, all tests have high test power. Our method
scales better with data dimension and shows an advantage for higher dimensional problems.
For all the experiments, we always choose A to be a set of distributions, and l (x, a) as the negative
log likelihood of l under distribution a. For the Blob and HDGM dataset, we choose A as the set of
mixture of Gaussian distributions. The number of mixture component is intentionally different from
the ground truth dataset to show that our approach performs well under mild mis-specification. We
also provide results for Blob choosing Parzen density estimtor as A in Appendix B.1. For the Higgs
dataset we use a Parzen density estimator, and for MNIST we use a variational autoencoder (Kingma
& Welling, 2013).
Each permutation test uses 100 permutations, and we run each test 100 times to compute the test
power (i.e. the percent of times it correctly outputs p 6= q). Finally we plot and report the perfor-
mance standard deviation by repeating the entire experiment 10 times.
4.4	Experiment Results
The average test powers are reported in Figure 2, Figure 3, Table 1 and Table 2. Our approach
achieves superior test power across the board. Notably on the Higgs dataset we achieve the same
test power with 2-5x fewer samples than the second best test, and on the MNIST dataset we can
achieve perfect test power even on the smallest sample size evaluated in (Liu et al., 2020). Our
method also produces consistent test power in comparison with the baselines as we observe much
lower variance of the test power between different random draws of the dataset.
Following (Liu et al., 2020) we also evaluate the test power as the dimension of the problem increases
(Figure 3). Our test power decreases gracefully as the dimension of the problem increases.
N	I ME	SCF	C2ST-S	C2ST-L	MMD-O	MMD-D	H-Div
1000	O.12O±0.007	0.095±0.007	0.082±0.015	0.097±0.014	0.132±0.005	0.113±0.013	0.240±0.020
2000	0.165±0.019	0.130±0.019	0.183±0.026	0.232±0.032	0.291±0.017	0.304±0.012	0.380±0.040
3000	0.197±o.012	0.142±0.025	0.257±0.049	0.399±0.058	0.376±0.022	0.403±0.050	0.685±0.015
5000	0.410±0.041	0.261±0.044	0.592±0.037	0.447±0.045	0.659±0.018	0.699±0.047	1.000±0.000
8000	0.691 ±0.067	0.467±0.038	0.892±0.029	0.878±0.020	0.923±0.013	0.952±0.024	1.000±0.000
10000	0.786±0.041	0.603±0.066	0.974±0.007	0.985±0.005	1.000±0.000	1.000±0.000	1.000±0.000
Avg.	I 0.395	0.283	0.497	0.506	0.564	0.579	0.847
Table 1: Average test power ± standard error for N samples over the HIGGS dataset.
N	I ME	SCF	C2ST-S	C2ST-L	MMD-O	MMD-D	H-Div
200	0.414±0.050	0.107±0.018	0.193±0.037	0.234±0.031	0.188±0.010	0.555±0.044	1.000±0.000
400	0.921±0.032	0.152±0.021	0.646±0.039	0.706±0.047	0.363±0.017	0.996±0.004	1.000±0.000
600	1.000±0.000	0.294±0.008	1.000±0.000	0.977±0.012	0.619±0.021	1.000±0.000	1.000±0.000
800	1.000±0.000	0.317±0.017	1.000±0.000	1.000±0.000	0.797±0.015	1.000±0.000	1.000±0.000
1000	1.000±0.000	0.346±0.019	1.000±0.000	1.000±0.000	0.894±0.016	1.000±0.000	1.000±0.000
Avg.	I 0.867	0.243	0.768	0.783	0.572	0.910	1.000
Table 2: Average test power ± standard error for N samples over the MNIST dataset.
7
Under review as a conference paper at ICLR 2021
Figure 4: Example plots of H-divergence across different geographical locations for losses ` related
to agriculture (left) and energy production (right). Darker color indicates larger H-divergence. Com-
pared to divergences such as KL, H-divergence measures changes relevant to different social and
economic activities (by selecting appropriate loss functions `). For example, even though climate
change significantly impact the high latitude or high altitude areas, this change has less relevance to
agriculture (because few agriculture activities are possible in these areas).
5	Experiment: Measuring Climate Change
This experiment aims to show that if a domain expert designs a loss function ` : X ×A → R, they can
obtain valuable insight from the associated H-JS divergence DJS. AS an illustrative example We use
climate data and study how climate change affects decision making. The H-divergence allows policy
makers to quantitatively measure aspects of climate change that are relevant to decision making in
different areas such as agriculture or renewable energy production.2
Setup We use the NOAA database which contains daily weather from thousands of weather sta-
tions at different geographical locations. For each location, we summarize the weather sequence
of each year into a few summary statistics (the average temperature, rainfall, etc). In other words,
x ∈ X represents the yearly weather summary. Let p denote the empirical distribution of yearly
weather for the years 1981-1999, and q denote the empirical distribution of yearly weather for the
years 2000-2019. (p + q)/2 can be interpreted as the yearly weather empirical distribution for the
entire period 1981-2019. Further details of these experiments are in Appendix C.2.
Example: Agriculture It is known that climate changes affect crop suitability (Lobell et al.,
2008). Let A denote the set of possible crops to plant such as wheat/barley/rice, and '(x, a) de-
note the loss of planting crop a if the yearly weather is x. We estimate the function ` by matching
geographical locations in the FAO crop yield dataset (FAOSTAT et al., 2006) to weather stations in
the NOAA database, and learn a function to predict crop yield from weather data with kernel ridge
regression. For each geographical location We can compute the H-divergence DJS for the estimated
` (plotted in Figure 4 left).
The H-divergence has a natural interpretation: a geographical location could either (1) plant the
same crop for the entire period 1981-2019 that is optimal for the local climate (i.e. choose
a* = arg mina∈A E(p+q)/2 ['(X, a)]); (2) plant the optimal crops for 1981-1999 and for 2000-2019
respectively. H divergence measures the additional loss of option (1) compared to option (2). In
other Words, it is the excess loss of not adapting crop type to climate change.
Example: Energy production Changes in Weather also affect electricity generation, since cli-
mate change could affect the amount of Wind/solar energy available. Let A denote the number
of wind/solar/fossil fuel power plants built, and '(x,a) denote the loss (negative utility) when
the Weather is x. We obtain the function ` by empirical estimation formulas for energy produc-
tion (Npower, 2012). The H-divergence for this loss function is shown in Figure 4 (right). Intuitively
the H divergence measures the excess loss of using the same energy generation infrastructure for the
entire time period vs. using different infrastructure that adapts to climate change.
6	Experiment: Evaluating Sample Quality
We also qualitatively verify that H-Divergence defines a distance between distributions that are often
consistent with human intuition. For example, this can be used to evaluate generative models, where
2Designing loss functions ` that capture the effect of climate on human activities is a well studied topic in
economics and environmental science, and beyond the scope of this work. Because of the simplifications we’ve
made, our results should be taken as an illustrative example of how domain experts might use H-divergence.
8
Under review as a conference paper at ICLR 2021
the gold standard evaluation of the distance between generated images and real images are usually
with human judgement (Zhou et al., 2019). Human evaluation is expensive, so several surrogate
measurements are commonly used, such as the Frechet Inception Distance (FID) (Heusel et al.,
2017) or the inception score.
For this task, we choose A as the set of Gaussian mixture distributions on the inception feature space
and l(x, a) as the negative log likelihood of x under distribution a. To evaluate the performance, we
use the same setup as (Heusel et al., 2017), where we add corruption (such as Gaussian noise) from
(Hendrycks & Dietterich, 2019) to a set of samples. Intuitively, adding more corruption degrades the
sample quality, so a good measurement of sample quality should assign a lower quality score (higher
divergence). The results are plotted in Figure 5. The remaining plots of other perturbations are in
Appendix B.2. Both FID and H-divergence are generally monotonically increasing as we increase
the amount of corruption. Our method is slightly better on some perturbations (such as “snow”),
where the FID fails to be monotonically increasing, but our method is still monotonic.
Figure 5: The divergence between corrupted image and original image measured by H-divergence
vs. FID. For better comparison we normalize each distance to between [0, 1] by a linear transforma-
tion. For “speckle” and “impulse” corruption, both divergences are monotonically increasing with
more corruption. For “snow” corruption H-divergence is monotonic while FID is not. Other types
of corruptions are provided in Appendix B.
7	Discussion and Open Questions
Explaining improved test power We postulate that the test power improvements come from
leveraging progress in generative model research: for each type of data (e.g. bio, image, text) there
has been decades of research finding suitable generative models; we use commonly used generative
models (in modern literature) for each data type (e.g. KDE for low dimensional physics/bio data,
VAE for simple images). Further study is needed to further explain the observed experiment result.
Computational efficiency If ` is not a convex function then evaluating the H-divergence
can be computationally difficult. In particular, gradient descent does not guarantee finding
arg inf a Ep ['(X, a)]. This Can be a short-coming of H-divergence, and practitioners should in-
terpret the empirical H-divergence estimation with caution. The practical remedy we use in our
paper (when ` is non-convex) is to use the same number of gradient update steps for evaluating
h` (p++q), H'(p), H'(q). Additional techniques to address non-convex optimization (such as Stein
Variational Gradient Descent, restarts, beam search, etc) are interesting future work.
For H-divergence there is a trade-off between faster computation and accurate estimation of H-
divergence (which should lead to improved test power). Our implementation takes about 3 hours
to run on a single GPU, while the second best baseline (MMD-D) takes about 30 minutes. In
time sensitive applications, studying the trade-off between computation time and test power is an
interesting question.
Statistical efficiency In Theorem 2 H-divergence can be efficiently estimated (i.e. independent of
the Radamacher complexity of `) when p = q. However, MMD can be efficiently estimated even
when p 6= q (Gretton et al., 2012). Itis an open question whether this is true for other special classes
of H-divergences.
9
Under review as a conference paper at ICLR 2021
References
Claire Adam-Bourdarios, Glen Cowan, Cecile Germain, Isabelle Guyon, Balazs KegL and David
Rousseau. The higgs boson machine learning challenge. In Proceedings of the 2014 International
Conference on High-Energy Physics and Machine Learning - Volume 42, HEPML'14,pp.19-55.
JMLR.org, 2014.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Jacob Burbea and C Radhakrishna Rao. Entropy differential metric, distance and divergence mea-
sures in probability spaces: A unified approach. Journal ofMultivariate Analysis, 12(4):575-596,
1982.
Jacob Burbea and C Radhakrishna Rao. Differential metrics in probability spaces. Probab. Math.
Stat, 3:241-258, 1984.
X. Cheng and A. Cloninger. Classification logit two-sample testing by neural networks. ArXiv,
abs/1909.11298, 2019.
Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testing with analytic representations of probability measures. In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems
28, pp. 1981-1989. Curran Associates, Inc., 2015.
Imre Csiszar. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der er-
godizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85-108, 1964.
Morris H DeGroot. Optimal statistical decisions, volume 82. John Wiley & Sons, 2005.
Morris H DeGroot et al. Uncertainty, information, and sequential experiments. The Annals of
Mathematical Statistics, 33(2):404-419, 1962.
Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Scholkopf. A permutation-based kernel
conditional independence test. In UAI, pp. 132-141, 2014.
Michael D Ernst et al. Permutation methods: a basis for exact inference. Statistical Science, 19(4):
676-685, 2004.
Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1-2):115-166, 2018.
FAO FAOSTAT et al. Fao statistical databases. Rome: Food and Agriculture Organization of the
United Nations, 2006.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American statistical Association, 102(477):359-378, 2007.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723-773, 2012.
Peter D Grunwald, A Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy
and robust bayesian decision theory. the Annals of Statistics, 32(4):1367-1433, 2004.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626-6637, 2017.
Wittawat Jitkrittum, Zoltan Szabo, Kacper P Chwialkowski, and Arthur Gretton. Interpretable dis-
tribution features with maximum testing power. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
181-189. Curran Associates, Inc., 2016.
10
Under review as a conference paper at ICLR 2021
Diederik P Kingma and Max Welling. Auto-Encoding variational bayes. arXiv preprint
arXiv:1312.6114v10, December 2013.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, A. Gretton, and D. Sutherland. Learning deep
kernels for non-parametric two-sample tests. ArXiv, abs/2002.09116, 2020.
David B Lobell, Marshall B Burke, Claudia Tebaldi, Michael D Mastrandrea, Walter P Falcon, and
Rosamond L Naylor. Prioritizing climate change adaptation needs for food security in 2030.
Science, 319(5863):607-610, 2008.
David Lopez-Paz and M. Oquab. Revisiting classifier two-sample tests. arXiv: Machine Learning,
2017.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in
Applied Probability, pp. 429-443, 1997.
Npower. Wind turbine power calculations. Mechanical and Electrical Engineering Power Industry,
The Royal Academy of Engineering, 2012.
C Radhakrishna Rao. Diversity and dissimilarity coefficients: a unified approach. Theoretical
population biology, 21(1):24-43, 1982.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable
information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.
Sharon Zhou, Mitchell Gordon, Ranjay Krishna, Austin Narcomey, Li F Fei-Fei, and Michael Bern-
stein. Hype: A benchmark for human eye perceptual evaluation of generative models. In Advances
in Neural Information Processing Systems, pp. 3449-3461, 2019.
11
Under review as a conference paper at ICLR 2021
A Proofs
Lemma 2. For any choice of' andfor any choice of φ that satisfy Definition 2, Dφ is non-negative
and d' (p, q) = 0 whenever P = q.
Proof of Lemma 2. For any choice of p, q by the convexity of the H-entropy we have
H p+p) - H'(p) ≥ 2(H'(q) - HAP))
H
-HXq) ≥ 2(He(P)- H(q))
Therefore we have
p + q
+ h` ”
≥0
By the requirement on φ We know that Dφ(pkq) ≥ 0.
□
Proposition 2. arg inf a Ep['(X, a)] ∩ arg inf a Eq ['(X, a)] = 0 ifand only if Dφ (Pkq) > 0.
Proofof Proposition 2. Denote Ap = arg inf a Ep['(X,a)] and A* = arg inf a Eq ['(X,a)]. Also
compute
h`(中)=inf Ep+q ['(X,a)]=inf (1 Ep['(X,a)] + 1 Eq['(X, a)f)	⑶
2	a 2	a2	2
If Ap ∩ A* = 0, for any action α0 such that Ep['(X, a0)] = HXP), we must have a0 ∈ Ap so
a/ ∈ Aq and Eq['(X, α0)] > H'(q). Similar if we choose a" such that 2Eq['(X, α00)] = H'(q) we
have similarly have Ep['(X, α00)] > HXP). In other words, for any choice of action a ∈ A either
Ep[l(X, α)] > h`(p) or Eq[l(X, a)] > H'(q). Therefore
inf (5Ep['(X,a)] + 5Eq['(X,a)]) > 5H'(P) + 5HKq)	(4)
a2	2	2	2
Combining Eq.(3) and Eq.(4) we have
By Definition 2 this would imply (for any choice of φ that satisfies the requirements in Definition 2
Dφ(Pkq) > 0.
To prove the converse simply obverse that if A*p ∩ Aq* 6= φ, let a* ∈ Ap* ∩ Aq* we have a* =
arg infa∈∕ Ep+q [l(X, α)]. This implies that
2H' (?) - H'(q) - h`(p) = 2E中[l(X, α*)] - Eq[l(X, a*)] - Ep[l(X, a*)] =0
By Definition 2 we can conclude that Dφ(Pkq) = 0.	□
Theorem 1.	The set of H-Jensen Shannon Divergences is strictly larger than the set of MMD dis-
tances.
Proof of Theorem 1. Let k(x, y) be some kernel on an input space X, and let H be the RKHS
induced by the kernel. Define φ(x,y) = ∣∣k(x, ∙) 一 k(y, ∙)kH. TheMMD distance is defined by
MMD(P, q) = Ep,pk(X, Y) + Eq,qk(X, Y) - 2Ep,qk(X, Y)
12
Under review as a conference paper at ICLR 2021
We can rewrite this in the following form:
MMD(p, q) = Ep,q Φ(X,Y )- 2 Ep,pΦ(X,Y )- 2 Eq,q Φ(X,Y)
We also observe an algebraic relationship for any function f(X, Y)
E p+q , p+q f(X,Y )= 1 Ep,pf (X,Y ) + 4 Eq,q f(X,Y ) + 1 Ep,q f(X,Y)
Based on the above we can derive
MMD(P, q) = Ep,q Hk(X, ∙) - k(Y, ∙)I∣H - 2 Ep,pkk(X, ∙) - k(Y, ∙)I∣H - 2 Eq,q Hk(X,，) - k(Y, ∙)∣∣H
=2E p+q, p+q Ilk(X, ,)- k(Y, ∙)∣∣H - Ep,p∣ Ik(X, ∙)- k(Y, ∙)∣∣H — Eq,q Ilk(X, ∙) — k(Y ∙)∣∣H
=4Ep+q∣∣k(X, ∙) — Ep+qk(Y, ∙)∣∣H — 2Ep∣∣k(X, ∙) — Epk(Y, ∙)∣∣H — 2Eq∣∣k(X, ∙) — Eqk(Y, ∙)∣∣H
=4 inf Ep+qIk(X, ∙) — akH — 2 inf EpIIk(X, ∙) — aIlH — 2 inf EqIlk(X, ∙) — a∣lH∙
a∈H 2	a∈H	a∈H
Therefore we can define a loss ` : X × H → R where
'(x, a) = 4∣∣k(x, ∙) — a∣∣H
Under the new notation we have
MMD(p, q) = inf Ep+ql(X, a) — 1 (inf Epl(X, a) + inf Eql(X, a))
a 2	2 a	a
=h` (p+i) — 1(H'(p) + H'(q)) = DJS(Pkq)
Conversely we want to show that not every H-Jensen Shannon divergence is a MMD. For example,
take h` to be the Shannon entropy, then the corresponding DJS is the Jensen-Shannon divergence,
which is not a MMD. This is because the JS divergence is a type of f -divergence, and the only
f -divergence that is also an IPM is total variation distance. Therefore, the set of H-Jensen Shannon
Divergences is strictly larger than the set of MMDs.	□
Theorem 2.	If' is C-bounded, and φ is 1 -Lipschitz under the ∞-norm,for any choice Ofdistribution
P, q ∈ P(X) and t > 0 we have
2
1. PrD夕(PmIlqm) ≥ t] ≤ 4e-2C2 if P = q.
2. Pr [∣DΦ(p^mIqm) — Dφ(pIq)∣ ≥ 4max(Rmm('), Rmm('))+ t] ≤ 4e-2C2
ProofofTheorem 2. LetPm be a sequence of n samples (xi, •一,Xm) drawn fromp, and ^mj a se-
quence of n samples (x；, ∙∙∙ ,xmm) drawn from q. Let rm the sub-sampling mixture (x；0, ∙∙∙ ,xmm)
defined in Section 3.5 (i.e. x0i0 = xibi + x0i(1 — bi) where bi is uniformly sampled from {0, 1}).
We also overload the notation h` by defining HXPm,) = infa∈∕ mm Pm=I l(xi, a), and define
H'(^m),H'(rm) similarly.	他
Before proving this theorem we need the following Lemmas
Lemma 3. Under the assumptions of Theorem 2
2
Pr[H'(Pm) — E[H'(Pm)] ≥ t] ≤ e-b
Lemma 4. Under the assumptions of Theorem 2
Pr[∣H'(P) — H'(Pm)∣≥ 2Rm(') + t] ≤ e-2C2m
13
Under review as a conference paper at ICLR 2021
To prove the first statement of the Theorem, when p = q we can denote μ = E[H'(pm)]=
E[H'(qm)] = E[H'(rm)] and we have
Pr [dφ(PmIlqm) ≥ t]
=Pr[φ(H'(rm) - h`(pm), Hgg) - H'(qm)) ≥ t]	Def 2
≤ Pr [H'(rm) - H(Pm) ≥ t] + Pr [H'(rm) - H'(qm) ≥ t]	Def 2, Union bound
≤ Pr[H'(pm) - μ ≥ t∕2]+2Pr[H'(rm) - μ ≥ t/2] + Pr [H£(qm) - μ ≥ t/2]	Union bound
_ t2
≤ 4e 2c2∕m	Lemma 3
To prove the second statement of the Theorem, we observe that
∖Dφ(pm⅛m)- Dφ(pkq)∖
=I φ (HMrm ) - Hg(Pm) , He(Rm)- H'(qm)) - φ (H' (P_20_) - Hg(P) , Hg (P_2_Q_) - Hg(Q))I
≤ max ( ∣ Hg(r m) — Hg(Pm) — Hg (0 ； Q ) + Hg(P) ∣ , ∣ Hg(rm) — Hg(Qm) — Hg (0 ； Q ) + Hg(q) ∣ )
≤ max ( I Hg (rm) - Hg ( 0 2 Q ) | + ∖ Hg(Pm) - Hg(P)I , | Hg (r m) - Hg ( ?-2-^- ) | + ∖Hg(qm,) - Hg (q) ∖
Def2
φ 1-Lip
Jensen
Therefore, the event ∖I)φ(pm∣∣qm) — D'(Pkq)I ≥ 4max(Rm(2), Rm(Qy) + t happens only if at
least one of the following events happen
Hewm) - He (审 j | ≥ Rm(Q) + Rm(Q) +1/2 ≥ 2Rm+q"2(Q)+1/2	R convex
IH'(pm) - He(P) ∖ ≥ 2Rm(Q) + t∕2
IHe(qm) - He (q)∖ ≥ 2Rm(Q) + t∕2
_ t2m
Based on Lemma 4 each of these events only happen with probability at most e 2c2. Therefore we
can conclude by union bound that
2
Pr[∣θφ(p∣∣q)- Dφ(p∣∣q)∣ ≥4max(Rm(Q), Rm(Q))+ t] ≤ 4e-即
Finally we prove the two Lemmas used in the theorem. Lemma 4 is a standard result in the
Radamacher complexity literature. For a proof, see e.g. Section 26.1 in (Shalev-Shwartz & Ben-
David, 2014). Lemma 3 can also be proved by standard techniques. We provide the proof here.
ProofofLemma 3. Consider two sets of samples xi,…，叼，…，Xm and *,…，xj,…，Xm
where Xi = Xi for every index i = 1,…，m except index j.
inf — SX Q(xi, a) — inf — SX Q(χi, a) ≤ sup ɪ SX Q(xi, a)———SX Q(χi, a)
a m	a m	a m	m
i	i	i	i
ɪ sup IQ(Xj, a) — Q(Xj, a) ∣
C
≤ —
m
Then we can conclude by Mcdiarmid inequality that
Pr
inf ɪ X Q(Xi,a) - E
am
i
inW X Q(Xi ,a)
i
≥t
2t2
≤ e C2/m
e
2t2 m
^2^
□
□
14
Under review as a conference paper at ICLR 2021
B Additional Experimental Results
B.1 Blob dataset with Parzen density estimator
→- MMD-O →- C2ST-L →- C2ST-S →- ME
0.10
Figure 6: The same plot as Figure 2, but with Parzen density estimator and different hyper-
parameters (with 30 random runs rather than 10). The conclusion is identical to Figure 2.
B.2 Complete Results for Evaluating Sample Quality experiment
Seventy ot Corruption
Figure 7: Additional plots that extend Figure 5.
15
Under review as a conference paper at ICLR 2021
C Additional Results
C.1 Connection to Optimal Transport
We first show that H-divergence can also have a transportation interpretation. For all the intuitive in-
terpretations we avoid technical difficulty by assuming X is a finite set, even though all the formulas
are applicable when X has infinite cardinality.
Setup Choose A = X, and let l(x, a) be a symmetric function (l(x, a) = l(a, x)) that denotes
the cost of transporting a unit of goods from x to a. When we say that a unit of goods is located
according to p, we mean that there is 1 unit of goods dispersed in X locations, where p(x) is the
amount of goods at location x.
Optimal Transport Distance The optimal transport distance is defined by
O'(p,q)=	inf	ErXY [l(X,Y)]
rXY,rX=pX,rY=qY
Intuitively the optimal transport distance measures the following cost: initially the goods are located
according to p, we would like to move them to locate according to q; O(p, q) denote the minimum
cost to accomplish this transportation task.
H-Divergence as Optimal Storage We first consider the intuitive interpretation to the H-entropy
H'(p) = inf Ep['(X, a)]	a* = arg inf Ep['(X, a)]
a	a∈X
Suppose we want to move goods located according to p to a storage location (for example, we want
to collect all the mail in a city to a package center), then a* is the optimal location to build the storage
location, and H-entropy measures the minimum cost to transport all goods to the storage location.
Similarly 2H' (p++q) measures the minimum cost to transport both goods located according to P and
goods located according to q to the same storage location. The H-divergence
2DJS(Pkq) := 2H' (p+q) - (H'(q) + H'(p))
measures the reduction of transportation cost with two storage locations (one for p and one for q)
rather than a single storage location (for both P and q).
The H-Divergence is related to the optimal transport distance by the following inequality.
Proposition 3. If ' satisfies the triangle inequality ∀x, y, z ∈ X, l(x, y) + l(y, z) ≥ l(x, z) then
DJS(Pkq) ≤ 1 O(p,q)
Proof of Proposition 3. Let aq* = arg inf Eq [l(X, a)] then we have
2H'审
inf(Ep['(X,a)]+Eq['(X,a)]) ≤Ep['(X,aq*)]+Eq['(X,aq*)]
a
≤ rXY,rX=inpXf,rY=qY ErXY ['(X, aq*)] +Eq['(X,aq*)]
≤	inf	ErXY ['(X, Y) + '(Y, aq*)] + Eq['(X, aq*)]
rXY,rX =pX ,rY =qY
=O'(P, q) +2Hxq)
Intuitively, to move goods located according to P and goods located according to q to some storage
location, one option is to first transport all goods from P to q (so that the goods at location x will be
2q(x)), then move the goods located according to 2q to the optimal storage location. Similarly we
have
2H' (审)≤ O(q,p) + 2H'(p)
which combined we have
2DJS(Pkq) = 2H' (?) - (H'(q) + H'(p)) ≤ O(p, q)
□
16
Under review as a conference paper at ICLR 2021
C.2 Climate Change Experiment Details
Setup Details In this experiment, we extract the statistics of yearly weather for each year from
1981-2019. We use the NOAA dataset, which contains daily weather data from thousands of weather
stations across the globe. For each year we compute the following summary statistics: average yearly
temperature, average yearly humidity, average yearly wind speed and average number of rainy days
in an year. For example x1990 is a 4 dimensional vector where each dimension correspond to one of
the summary statistics above.
LetP denote the uniform distribution over {χi98i,…，χ1999} and q denote the uniform distribution
over {x2000,…，χ2019}. For example Ep ['(X, a)] denote the expected loss of action a for a random
year sampled from 1981-1999. Note that for many decision problems, it is possible to make yearly
decisions (e.g. decide the best crop to plant for each year). However, because we want to measure
the difference between two time periods 1981-1999 vs. 2000-2019, we choose the action space A
to be a single crop selection that will be used for the entire time period (rather than a different crop
selection for each year). Similarly for energy production we choose the action space A to be the
proportion of different energy production methods that will be used for the entire time period.
Crop yield We obtain the crop yield loss function '(x, a) with the following procedure
1.	We obtain the crop yield dataset from (FAOSTAT et al., 2006), each entry we extract is the
following tuple: (country code, year, crop type, yield per hectare (kg/ha))
2.	We associate each country code with the central coordinate (i.e. the average latitude and
longitude) of the country. For each central coordinate we find the nearest weather station
in the NOAA database. We use data for the nearest weather station as the weather data for
the country.
3.	Based on step 2 for each (country code, year) pair we can associate a weather statistics (i.e.
the 4 dimensional vector described in Setup Details). We update each entry in step 1 to be
(weather statistics, crop type, yield per hectare).
4.	Based on the data entries we obtain in step 3 we train a Kernel Ridge regression model to
learn the function '(x, a) where x is the weather statistics, a is the crop type, and '(x, a) is
learned to predict the yield (normalized by market price) of the weather x for crop type a.
Energy production We consider three types of energy production methods: solar, wind and tra-
ditional (such as fossil fuel). Solar energy and wind energy both depend heavily on weather, while
traditional energy does not. In particular, we use empirical formulas for solar and wind energy
calculation:
solar H number of sunny days * daylight hour
Wind H Wind velocity3
17