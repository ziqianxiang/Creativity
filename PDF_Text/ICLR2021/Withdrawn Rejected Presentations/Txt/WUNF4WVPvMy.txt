Under review as a conference paper at ICLR 2021
Acceleration in Hyperbolic and Spherical
Spaces
Anonymous authors
Paper under double-blind review
Ab stract
We further research on the acceleration phenomenon on Riemannian manifolds
by introducing the first global first-order method that achieves the same rates as
accelerated gradient descent in the Euclidean space for the optimization of smooth
and geodesically convex (g-convex) or strongly g-convex functions defined on the
hyperbolic space or a subset of the sphere, up to constants and log factors. To
the best of our knowledge, this is the first method that is proved to achieve these
rates globally on functions defined on a Riemannian manifold M other than the
Euclidean space. Additionally, for any Riemannian manifold of bounded sectional
curvature, we provide reductions from optimization methods for smooth and g-
convex functions to methods for smooth and strongly g-convex functions and vice
versa.
1	Introduction
Acceleration in convex optimization is a phenomenon that has drawn lots of attention and has yielded
many important results, since the renowned Accelerated Gradient Descent (AGD) method of Nesterov
(1983). Having been proved successful for deep learning Sutskever et al. (2013), among other fields,
there have been recent efforts to better understand this phenomenon Allen Zhu & Orecchia (2017);
Diakonikolas & Orecchia (2019); Su et al. (2016); Wibisono et al. (2016). These have yielded
numerous new results going beyond convexity or the standard oracle model, in a wide variety of
settings Allen-Zhu (2017; 2018a;b); Allen Zhu & Orecchia (2015); Allen Zhu et al. (2016); Allen-Zhu
et al. (2017); Carmon et al. (2017); Cohen et al. (2018); CUtkosky & Sarl6s (2019); Diakonikolas &
Jordan (2019); Diakonikolas & Orecchia (2018); Gasnikov et al. (2019); Wang et al. (2016). This
sUrge of research that applies tools of convex optimization to models going beyond convexity has
been frUitfUl. One of these models is the setting of geodesically convex Riemannian optimization. In
this setting, the fUnction to optimize is geodesically convex (g-convex), i.e. convex restricted to any
geodesic (cf. Definition 1.1).
Riemannian optimization, g-convex and non-g-convex alike, is an extensive area of research. In recent
years there have been nUmeroUs efforts towards obtaining Riemannian optimization algorithms that
share analogoUs properties to the more broadly stUdied EUclidean first-order methods: deterministic
de Carvalho Bento et al. (2017); Wei et al. (2016); Zhang & Sra (2016), stochastic Hosseini &
Sra (2017); KhUzani & Li (2017); TripUraneni et al. (2018), variance-redUced Sato et al. (2017;
2019); Zhang et al. (2016), adaptive Kasai et al. (2019), saddle-point-escaping Criscitiello & BoUmal
(2019); SUn et al. (2019); Zhang et al. (2018); ZhoU et al. (2019); Criscitiello & BoUmal (2020),
and projection-free methods Weber & Sra (2017; 2019), among others. UnsUrprisingly, Riemannian
optimization has foUnd many applications in machine learning, inclUding low-rank matrix completion
Cambier & Absil (2016); Heidel & SchUlz (2018); Mishra & SepUlchre (2014); Tan et al. (2014);
Vandereycken (2013), dictionary learning Cherian & Sra (2017); SUn et al. (2017), optimization Under
orthogonality constraints Edelman et al. (1998), with applications to RecUrrent NeUral Networks
Lezcano-Casado (2019); Lezcano-Casado & Martinez-RUbio (2019), robust covariance estimation
in GaUssian distribUtions Wiesel (2012), GaUssian mixtUre models Hosseini & Sra (2015), operator
scaling Allen-Zhu et al. (2018), and sparse principal component analysis Genicot et al. (2015); Huang
& Wei (2019b); Jolliffe et al. (2003).
1
Under review as a conference paper at ICLR 2021
However, the acceleration phenomenon, largely celebrated in the Euclidean space, is still not under-
stood in Riemannian manifolds, although there has been some progress on this topic recently (cf.
Related work). This poses the following question, which is the central subject of this paper:
Can a Riemannian first-order method enjoy the same rates as AGD in the Euclidean space?
In this work, we provide an answer in the affirmative for functions defined on hyperbolic and spherical
spaces, up to constants depending on the curvature and the initial distance to an optimum, and up to
log factors. In particular, the main results of this work are the following.
Main Results:
•	Full acceleration. We design algorithms that provably achieve the same rates of convergence
as AGD in the Euclidean space, up to constants and log factors. More precisely, we obtain
the rates Oe(L∕√ε) and O*(,L∕μlog(μ∕ε)) when optimizing L-Smooth functions that
are, respectively, g-convex and μ-strongly g-convex, defined on the hyperbolic space or a
subset of the sphere. The notation Oe(∙) and O* (∙) omits log(L∕ε) and log(L∕μ) factors,
respectively, and constants. Previous approaches only showed local results Zhang & Sra
(2018) or obtained results with rates in between the ones obtainable by Riemannian Gradient
Descent (RGD) and AGD Ahn & Sra (2020). Moreover, these previous works only apply to
functions that are smooth and strongly g-convex and not to smooth functions that are only
g-convex. As a proxy, we design an accelerated algorithm under a condition between of
convexity and quasar-convexity in the constrained setting, which is of independent interest.
•	Reductions. We present two reductions for any Riemannian manifold of bounded sectional
curvature. Given an optimization method for smooth and g-convex functions they provide a
method for optimizing smooth and strongly g-convex functions, and vice versa. This allows
to focus on designing methods for one set of assumptions only.
It is often the case that methods and key geometric inequalities that apply to manifolds with bounded
sectional curvatures are obtained from the ones existing for the spaces of constant extremal sectional
curvature Grove et al. (1997); Zhang & Sra (2016; 2018). Consequently, our contribution is relevant
not only because we establish an algorithm achieving global acceleration on functions defined on
a manifold other than the Euclidean space, but also because understanding the constant sectional
curvature case is an important step towards understanding the more general case of obtaining
algorithms that optimize g-convex functions, strongly or not, defined on manifolds of bounded
sectional curvature.
Our main technique for designing the accelerated method consists of mapping the function domain
to a subset B of the Euclidean space via a geodesic map: a transformation that maps geodesics to
geodesics. Given the gradient of a point x ∈ M, which defines a lower bound on the function that is
linear over the tangent space of x, we find a lower bound of the function that is linear over B, despite
the map being non-conformal, deforming distances, and breaking convexity. This allows to aggregate
the lower bounds easily. We believe that effective lower bound aggregation is key to achieving
Riemannian acceleration and optimality. Using this strategy, we are able to provide an algorithm
along the lines of the one in Diakonikolas & Orecchia (2018) to define a continuous method that we
discretize using an approximate implementation of the implicit Euler method, obtaining a method
achieving the same rates as the Euclidean AGD, up to constants and log factors. Our reductions take
into account the deformations produced by the geometry to generalize existing Euclidean reductions
Allen Zhu & Hazan (2016); Allen Zhu & Orecchia (2017).
Basic Geometric Definitions. We recall basic definitions of Riemannian geometry that we use in
this work. For a thorough introduction we refer to Petersen et al. (2006). A Riemannian manifold
(M, g) is a real smooth manifold M equipped with a metric g, which is a smoothly varying inner
product. For x ∈ M and any two vectors v, w ∈ TxM in the tangent space of M, the inner product
hv, Wix is g(v, w). For V ∈ TχM, the norm is defined as usual ∣∣vkχ def，(v,v〉x. Typically, X is
known given v or w, so we will just write hv, wi or kvk if x is clear from context. A geodesic is a
curve γ : [0, 1] → M of unit speed that is locally distance minimizing. A uniquely geodesic space is
a space such that for every two points there is one and only one geodesic that joins them. In such a
case the exponential map Expx : TxM → M and inverse exponential map Expx-1 : M → TxM
are well defined for every pair of points, and are as follows. Given x, y ∈ M, v ∈ TxM, and a
2
Under review as a conference paper at ICLR 2021
geodesic γ of length kvk such that γ(0) = x, γ(1) = y, γ0(0) = v/kvk, we have that Expx(v) = y
and Exp-1(y) = v. Note, however, that Expχ(∙) might not be defined for each V ∈ TXM. We
denote by d(x, y) the distance between x and y. Its value is the same as k Expx-1(y)k. Given a
2-dimensional subspace V ⊆ TxM, the sectional curvature at x with respect to V is defined as the
Gauss curvature of the manifold Expx(V ) at x.
Notation. Let M be a manifold and let B ⊆ Rd . We denote by h : M → B a geodesic map
Kreyszig (1991), which is a diffeomorphism such that the image and the inverse image of a geodesic
is a geodesic. Usually, given an initial point x0 of our algorithm, we will have h(x0) = 0. Given a
point X ∈ M We use the notation X = h(x) and vice versa, any point in B will use a tilde. Given
two points x, y ∈ M and a vector v ∈ TxM in the tangent space of x, we use the formal notation
hv, y - Xi def -hv,x - yi defhv, Exp-1(y)). Given a vector V ∈ TχM, we call V ∈ Rd the vector of
the same norm such that {x + λv∣λ ∈ R+, x + λv ∈ B} = {h(Expχ(λv))∣λ ∈ I ⊆ R+}, for some
interval I. Likewise, given X and a vector V ∈ Rd, we define V ∈ TχM. Let x* be any minimizer
of F : M → R. We denote by R ≥ d(χo, x*) a bound on the distance between_x* and the initial
point X0. Note that this implies that x* ∈ Expχ° (B(0, R)), for the closed ball B(0, R) ⊆ TχοM.
Consequently, we will work with the manifold that is a subset of a d-dimensional complete and
simply connected manifold of constant sectional curvature K , namely a subset of the hyperbolic
space or sphere Petersen et al. (2006), defined as Expχ0 (B(0, R)), with the inherited metric. Denote
by H this manifold in the former case and S in the latter, and note that we are not making explicit
the dependence on d, R and K . We want to work with the standard choice of uniquely geodesic
manifolds Ahn & Sra (2020); Liu et al. (2017); Zhang & Sra (2016; 2018). Therefore, in the case that
the manifold is S, we restrict ourselves to R < n/2√K, so S is contained in an open hemisphere.
The big O notations O(∙) and O*(∙) omit log(L∕ε) and log(L∕μ) factors, respectively, and constant
factors depending on R and K.
We define now the main properties that will be assumed on the function F to be minimized.
Definition 1.1 (Geodesic Convexity and Smoothness). Let F : M → Rbe a differentiable function
defined on a Riemannian manifold (M, g). Given L ≥ μ > 0, we say that F is L-smooth, and
respectively μ-strongly g-convex, if for any two points x,y ∈ M, F satisfies
F(y) ≤ F(x) + EF(x),y -xi + 2d(x,y)2, resp. F(y) ≥ F(x) + EF(x),y - Xi + 2d(x,y)2.
We say F is g-convex if the second inequality above, i.e. μ-strong g-convexity, is satisfied with μ = 0.
Note that we have used the formal notation above for the subtraction of points in the inner product.
Comparison with Related Work. There are a number of works that study the problem of first-
order acceleration in Riemannian manifolds of bounded sectional curvature. The first study is Liu
et al. (2017). In this work, the authors develop an accelerated method with the same rates as AGD
for both g-convex and strongly g-convex functions, provided that at each step a given nonlinear
equation can be solved. No algorithm for solving this equation has been found and, in principle,
it could be intractable or infeasible. In Alimisis et al. (2019) a continuous method analogous to
the continuous approach to accelerated methods is presented, but it is not known if there exists an
accelerated discretization of it. In Alimisis et al. (2020), an algorithm presented is claimed to enjoy
an accelerated rate of convergence, but fails to provide convergence when the function value gets
below a potentially large constant that depends on the manifold and smoothness constant. In Huang
& Wei (2019a) an accelerated algorithm is presented but relying on strong geometric inequalities that
are not proved to be satisfied. Zhang & Sra (2018) obtain a local algorithm that optimizes L-smooth
and μ-strongly g-convex functions achieving the same rates as AGd in the Euclidean space, up to
constants. That is, the initial point needs to start close to the optimum, O((μ∕L)3/4) close, to be
precise. Their approach consists of adapting Nesterov’s estimate sequence technique by keeping
a quadratic on TxtM that induces on M a regularized lower bound on F(x*) via ExPx=(∙). They
aggregate the information yielded by the gradient to it, and use a geometric lemma to find a quadratic
in Txt+1 M whose induced function lower bounds the other one. Ahn & Sra (2020) generalize the
previous algorithm and, by using similar ideas for the lower bound, they adapt it to work globally,
obtaining strictly better rates than RGD, recovering the local acceleration of the previous paper, but
not achieving global rates comparable to the ones of AGD. In fact, they prove that their algorithm
eventually decreases the function value at a rate close to AGD but this can take as many iterations
as the ones needed by RGD to minimize the function. In our work, we take a step back and focus
3
Under review as a conference paper at ICLR 2021
on the constant sectional curvature case to provide a global algorithm that achieves the same rates
as AGD, up to constants and log factors. It is common to characterize the properties of spaces of
bounded sectional curvature by using the ones of the spaces of constant extremal sectional curvature
Grove et al. (1997); Zhang & Sra (2016; 2018), which makes the study of the constant sectional
curvature case critical to the development of full accelerated algorithms in the general bounded
sectional curvature case. Additionally, our work studies g-convexity besides strong g-convexity.
Another related work is the approximate duality gap technique Diakonikolas & Orecchia (2019),
which presents a unified view of the analysis of first-order methods for the optimization of convex
functions defined in the Euclidean space. It defines a continuous duality gap and by enforcing
a natural invariant, it obtains accelerated continuous dynamics and their discretizations for most
classical first-order methods. A derived work Diakonikolas & Orecchia (2018) obtains acceleration in
a fundamentally different way from previous acceleration approaches, namely using an approximate
implicit Euler method for the discretization of the acceleration dynamics. The convergence analysis
of Theorem 2.4 is inspired by these two works. We will see in the sequel that, for our manifolds of
interest, g-convexity is related to a model known in the literature as quasar-convexity or weak-quasi-
convexity Guminov & Gasnikov (2017); Hinder et al. (2019); Nesterov et al. (2018).
2	Algorithm
We study the minimization problem minx∈M F (x) with a gradient oracle, for a smooth function
F : M → R that is g-convex or strongly g-convex. In this section, M refers to a manifold that can
be H or S, i.e. the subset of the hyperbolic space or sphere Expχ0 (B(0, R)), for an initial point x0.
For simplicity, we do not use subdifferentials so we assume F : M → R is a differentiable function
that is defined over the manifold of constant sectional curvature M0 d=ef Expx0 (B (0, R0)), for an
R0 > R, and we avoid writing F : M0 → R. We defer the proofs of the lemmas and theorems in
this and following sections to the supplementary material. We assume without loss of generality
that the sectional curvature of M is K ∈ {1, -1}, since for any other value of K and any function
F : M → R defined on such a manifold, we can reparametrize F by a rescaling, so it is defined
over a manifold of constant sectional curvature K ∈ {1, -1}. The parameters L, μ and R are
rescaled accordingly as a function of K, cf. Remark C.1. We denote the special cosine by CK(∙),
which is cos(∙) if K = 1 and cosh(∙) if K = —1. We define X = h(M) ⊆ B ⊆ Rd. We use
classical geodesic maps for the manifolds that we consider: the Gnomonic projection for S and the
Beltrami-Klein projection for H Greenberg (1993). They map an open hemisphere and the hyperbolic
space of curvature K ∈ {1, -1} to B = Rd and B = B (0, 1) ⊆ Rd, respectively. We will derive our
results from the following characterization Greenberg (1993). Let X, y ∈ B be two points. Recall that
We denote X = h-1(X), y = h-1(y) ∈ M. Then We have that d(x, y), the distance between X and y
with the metric of M, satisfies
CK (d(X, y))
1 + K hx,yi
√1 + Kkxk2 ∙ √1 + Kkyk2
(1)
Observe that the expression is symmetric with respect to rotations. In particular, the symmetry implies
X is a closed ball of radius R, with CK (R) = (1 + KJ?2)-1/2.
Consider a point X ∈ M and the lower bound provided by the g-convexity assumption when
computing VF(x). Dropping the μ term in case of strong g-convexity, this bound is linear over
TxM. We would like our algorithm to aggregate effectively the lower bounds it computes during the
course of the optimization. The deformations of the geometry make it a difficult task, despite the
fact that we have a simple description of each individual lower bound. We deal with this problem in
the following way: our approach is to obtain a lower bound that is looser by a constant depending
on R, and that is linear over B . In this way the aggregation becomes easier. Then, we are able
to combine this lower bound with decreasing upper bounds in the fashion some other accelerated
methods work in the Euclidean space Allen Zhu & Orecchia (2017); Diakonikolas & Orecchia (2018;
2019); Nesterov (1983). Alternatively, we can see the approach in this work as the constrained
non-convex optimization problem of minimizing the function f : X → R, X → F(h-1(X)):
minimize f (X),	for X ∈ X.
In the rest of the section, we will focus on the g-convex case. For simplicity, instead of solving the
strongly g-convex case directly in an analogous way by finding a lower bound that is quadratic over
B, we rely on the reductions of Section 3 to obtain the accelerated algorithm in this case.
4
Under review as a conference paper at ICLR 2021
The following two lemmas show that finding the aforementioned linear lower bound is possible, and
is defined as a function of Vf (X). We first gauge the deformations caused by the geodesic map h.
Distances are deformed, the map h is not conformal and, in spite of it being a geodesic map, the
image of the geodesic Expχ(λVF(x)) is not mapped into the image of the geodesic X + λVf (x),
i.e. the direction of the gradient changes. We are able to find the linear lower bound after bounding
these deformations.
Lemma 2.1. Let x,y ∈ M be two different points, and in part b) different from xo. Let a be the
angle ∠XoXy, formed by the vectors Xo 一 X and y 一 X. Let a be the corresponding angle between
the vectors ExP-1(xo) and Exp-1 (y). Assume without loss of generality that X ∈ span{2ι} and
Vf (X) ∈ span{e1,≡2} for the canonical orthonormal basis {ei}d=ι. Let ei ∈ TxM be the unit
vector such that h maps the image ofthe geodesic Expx(λei) to the image ofthe geodesic X + λei,
for i = 1, . . . , d, and λ, λ ≥ 0. Then, the following holds.
a)	Distance deformation:
KCK(R) ≤ K kfcl ≤ K.
b)	Angle deformation:
「、 一1 I^^1 + Kl∣X∣l2-
Sm(Q) = Sm(Q)J	∙ 2	,
V 1 + K∣∣x∣∣2 Sin (Q)
CoS(Q)=CoS(Q)∕τ+K||X；2 Sin2(α).
c)	Gradient deformation:
VF(x) = (1 + K∣∣X∣∣2)Vf(X)IeI + P + Kl∣X∣∣2 Vf(X)2e2	and	ei ⊥ ej for i = j.
And if V ∈ TxM is a vector normal to VF(x), then V is normal to Vf (x).
The following uses the deformations described in the previous lemma to obtain the linear lower
bound on the function, given a gradient at a point X. Note that Lemma 2.1.c implies that We have
hVf (X),y 一 Xi = 0 if and only ifhVF(χ), y 一 Xi = 0. In the proof we lower bound, generally,
linear functions defined on TxM by linear functions in the Euclidean space B. This generality alloWs
to obtain a result with constants that only depends on R.
Lemma 2.2. Let F : M → R be a differentiable function and let f = F ◦ h-1. Then, there are
constants γn, YP ∈ (0,1] depending on R such thatfor all x, y ∈ M satisfying hVf (X), y — X)= 0
we have:
≤ EF(χ),y - Xi ≤ ɪ
YP ≤ hVf(χ),y — Xi ≤ Yn.
In particular, if F is g-convex we have:
f(X) + ɪhVf(χ),y — Xi ≤ f(y)	if hVf(X),y — x) ≤ 0,
Yn
f(X) + YphVf(X),y — Xi ≤ f(y)	if hVf(X),y — x)≥ 0.
(2)
(3)
The two inequalities in (3) show the linear lower bound. Only the first one is needed to bound
f (X*) = F(χ*). The first inequality applied to y = X* defines a model known in the literature
as quasar-convexity or weak-quasi-convexity Guminov & Gasnikov (2017); Hinder et al. (2019);
Nesterov et al. (2018), for which accelerated algorithms exist in the unconstrained case, provided
smoothness is also satisfied. However, to the best of our knowledge, there is no known algorithm
for solving the constrained case in an accelerated way. The condition in (3) is, trivially, a relaxation
of convexity that is stronger than quasar-convexity. We will make use of (3) in order to obtain
acceleration in the constrained setting. This is of independent interest. Recall that we need the
constraint to guarantee bounded deformation due to the geometry. We also require smoothness of f .
The following lemma shows that f is as smooth as F up to a constant depending on R.
Lemma 2.3. Let F : M → R be an L-smooth function and f = F ◦ h-1. Assume there is a point
X* ∈ M such that VF(X*) = 0. Then f is O(L)-smooth.
5
Under review as a conference paper at ICLR 2021
Using the approximate duality gap technique Diakonikolas & Orecchia (2019) we obtain accelerated
continuous dynamics, for the optimization of the function f . Then we adapt AXGD to obtain an
accelerated discretization. AXGD Diakonikolas & Orecchia (2018) is a method that is based on
implicit Euler discretization of continuous accelerated dynamics and is fundamentally different
from AGD and techniques as Linear Coupling Allen Zhu & Orecchia (2017) or Nesterov’s estimate
sequence Nesterov (1983). The latter techniques use a balancing gradient step at each iteration
and our use of a looser lower bound complicates guaranteeing keeping the gradient step within the
constraints. We state the accelerated theorem and provide a sketch of the proof in Section 2.1.
Theorem 2.4. Let Q ⊆ Rd be a convex set of diameter 2R. Let f : Q → R be an L-smooth function
satisfying (3) with constants γn, YP ∈ (0,1]. Assume there is a point X* ∈ Q such that Vf (X*) = 0.
Then, we can obtain an ε-mιnιmιzer of f using O( JL∕(γi2γpε)) queries to the gradient oracle of f.
Finally, we have Riemannian acceleration as a direct consequence of Theorem 2.4, Lemma 2.2 and
Lemma 2.3.
Theorem 2.5 (g-Convex Acceleration). Let F : M → R be an L-smooth and g-convex function
and assume there is a point x* ∈ M satisfying VF(x*) = 0. Algorithm 1 computes a point xt ∈ M
satisfying F(Xt) 一 F(x*) ≤ ε using O(,L∕ε) queries to the gradient oracle.
We observe that if there is a geodesic map mapping a manifold into a convex subset of the Euclidean
space then the manifold must necessarily have constant sectional curvature, cf. Beltrami’s Theorem
Busemann & Phadke (1984); Kreyszig (1991). This precludes a straightforward generalization from
our method to the case of non-constant bounded sectional curvature.
Algorithm 1 Accelerated g-Convex Minimization
Input: Smooth and g-convex function F : M → R, for M = H or M = S.
Initial point X0; Constants L, γP, γn. Geodesic map h satisfying (1) and h(X0) = 0.
BoUnd on the distance to a minimum R ≥ d(xq, x*). AccUracy ε and number of iterations t
1:	X def h(Expx0 (B (0,R))) ⊆B;	f def F ◦ h-1	and ψ(X) =f ɪ ∣∣X∣∣2
2:	Zo — Vψ(Xo);	Ao - 0
3:	for i from 0 to t 一 1 do
4:	ai+ι J (i + I)YnYP/2L
5:	Ai+1 - Ai + ai+1
6:	λ J BinaryLineSearch(xi, z%, f, X, ai+ι, Ai, ε, L, γn YP) (cf. Algorithm 2 in Appendix A)
7:	Xi J (1 一 KXi + λVψ*(Zi)
〜 . ________________ ., 、
8:	Zi J Zi-Iai+1/Tn))Vfai )
9:	Xi+1 J (1 - λ)Xi + λVψ*(Zi)	[Vψ*(p) = argmi%∈χ{∣∣Z ―p∣∣} = Πχ(p)]
10:	zZi+1 J zZi 一 (ai+1/Yn)Vf (XZi+1)
11:	end for
12:	return Xt .
2.1 S ketch of the proof of Theorem 2.4.
Inspired by the approximate duality gap technique Diakonikolas & Orecchia (2019), let αt be an
increasing function of time t, and denote At = Rtt da「= Rtt αT dτ. We define a continuous method
that keeps a solution XZt, along with a differentiable upper bound Ut on f(Xt) and a lower bound Lt
on f(XZ*). In our case f is differentiable so we can just take Ut = f(Xt). The lower bound comes
from
f(XZ*) ≥
Rtt0 f(Xτ)dατ	Rtt0 YnhVf(XT),X* - XTidɑτ
A +	A
(4)
after applying some desirable modifications, like regularization with a 1-strongly convex function
ψ and removing the unknown XZ* by taking a minimum over X . Note (4) comes from averaging (3)
for yZ = XZ*. Then, if we define the gap Gt = Ut - Lt and design a method that forces αtGt to be
non-increasing, We can deduce f (Xt) — f (x*) ≤ Gt ≤ αt0Gt0/at. By forcing ddt(αtGt) = 0, We
naturally obtain the following continuous dynamics, where zt is a mirror point and ψ * is the Fenchel
6
Under review as a conference paper at ICLR 2021
dual of ψ, cf. Definition A.2.
Zt =-----αNf (Xt); Xt = ——αt —" O—〜上; % = Vψ(Xt0),xt0 ∈ X	(5)
γn	γn	αt	0	0	0
We note that except for the constant γn , these dynamics match the accelerated dynamics used in the
optimization of convex functions Diakonikolas & Orecchia (2019; 2018); Krichene et al. (2015).
The AXGD algorithm Diakonikolas & Orecchia (2018), designed for the accelerated optimization
of convex functions, discretizes the latter dynamics following an approximate implementation of
implicit Euler discretization. This has the advantage of not needing a gradient step per iteration to
compensate for some positive discretization error. Note that in our case we must use (3) instead of
convexity for a discretization. We are able to obtain the following discretization coming from an
approximate implicit Euler discretization:
(Xi=	Ai^i+i⅜i∕Yn Xi	+	AiMa=Kn Vψ*(z∕	Zi=Zi-	ae Nf (Xi)	⑹
xi	xi+1	=	Ai^i+α+l∕Yn Xi	+	AiMai+n/Yn Vψ*(ζi);	zi+1	=	Zi	- 亲 Vf(Xi+1)
where Yi ∈ [γp, 1∕γn] is a parameter, Xo ∈ X is an arbitrary point, Zo = Vψ(Xo) and now at is a
discrete measure and αt is a weighted sum of Dirac delta functions αt = P∞=1 aiδ(t 一 (to + i 一 1)).
Compare (6) with the discretization in AXGD Diakonikolas & Orecchia (2018) that is equal to
our discretization but with no Yn or ^i. Or equivalently with Yi = 1∕γn and with no Yn for the
mirror descent updates of ζi and ZZi+1. However, not having convexity, in order to have per-iteration
discretization error less than ε∕Aτ, we require Yi to be such that Xi+1 satisfies
f(Xi+1) — f(Xi) ≤ YihVf(Xi+1),Xi+1 — Xii + ε,	(7)
where ε is chosen so that the accumulated discretization error is < ε∕2, after having performed
the steps necessary to obtain an ε∕2 minimizer. We would like to use (3) to find such a Yi but we
need to take into account that we only know XZi+1 a posteriori. Indeed, using (3) we conclude that
setting Yi to 1∕Yn or YP then we either satisfy (7) or there is a point Yi ∈ (Yp, 1∕Yn) for which
hVf (Xi+1), Xi+1 — Xii = 0, which satisfies the equation for ε = 0. Then, using smoothness of f,
existence of x* (that satisfies Vf (x*) = 0), and boundedness of X we can guarantee that a binary
search finds a point satisfying (7) in O(log(Li∕Ynε)) iterations. Each iteration of the binary search
requires to run (6), that is, one step of the discretization. Computing the final discretization error, we
obtain acceleration after choosing appropriate learning rates ai . Algorithm 1 contains the pseudocode
of this algorithm along with the reduction of the problem from minimizing F to minimizing f . We
chose ψ(X) def 2∣∣Xk2 as our strongly convex regularizer.
3	Reductions
The construction of reductions proves to be very useful in order to facilitate the design of algorithms in
different settings. Moreover, reductions are a helpful tool to infer new lower bounds without extra ad
hoc analysis. We present two reductions. We will see in Corollary 3.2 and Example 3.4 that one can
obtain full accelerated methods to minimize smooth and strongly g-convex functions from methods
for smooth and g-convex functions and vice versa. These are generalizations of some reductions
designed to work in the Euclidean space Allen Zhu & Hazan (2016); Allen Zhu & Orecchia (2017).
The reduction to strongly g-convex functions takes into account the effect of the deformation of the
space on the strong convexity of the function Fy (X) = d(X, y)2∕2, for X, y ∈ M. The reduction to
g-convexity requires the rate of the algorithm that applies to g-convex functions to be proportional to
the distance between the initial point and the optimum d(Xo, X*). The proofs of the statements in this
section can be found in the supplementary material. We will use Timens(∙) and Time(∙) to denote
the time algorithms Ans and A below require, respectively, to perform the tasks we define below.
Theorem 3.1. Let M be a Riemannian manifold, let F : M → R be an L-smooth and μ-strongly
g-convex function, and let X* be its minimizer. Let Xo be a starting point such that d(Xo, X*) ≤ R.
Suppose we have an algorithm Ans to minimize F, such that in time T = Timens (L, μ, R) it produces
a point XT satisfying F(XT) — F(x*) ≤ μ ∙ d(xo, x* )2∕4. Then we can compute an ε-minimizer of
F in time O(Timens(L, μ, R) log(R2μ∕ε)).
Theorem 3.1 implies that if we forget about the strong g-convexity of a function and we treat it as it
is just g-convex we can run in stages an algorithm designed for optimizing g-convex functions. The
7
Under review as a conference paper at ICLR 2021
fact that the function is strongly g-convex is only used between stages, as the following corollary
shows by making use of Algorithm 1.
Corollary 3.2. We can compute an ε-minimizer of an L-smooth and μ-strongly g-convex function
F : M → R in O*( ∖jL∕μ log(μ∕ε)) queries to the gradient oracle, where M = S or M = H.
We note that in the strongly convex case, by decreasing the function value by a factor we can guarantee
We decrease the distance to x* by another factor, so We can periodically recenter the geodesic map to
reduce the constants produced by the deformations of the geometry, see the proof of Corollary 3.2.
Finally, We shoW the reverse reduction.
Theorem 3.3. Let M be a Riemannian manifold of bounded sectional curvature, let F : M → R be
an L-smooth and g-convex function, and assume there is a point x* ∈ M such that VF(x*) = 0.
Let x0 be a starting point such that d(x0, x*) ≤ R and let ∆ satisfy F(x0) - F(x*) ≤ ∆. Assume
we have an algorithm A that given an L-smooth and μ-strongly g-convexfunction F : M →R, with
minimizer in Expχ0 (B(0, R)), and any initial point Xo ∈ M produces a point X ∈ Expχ0 (B(0, R))
in time T = Time(L,μ, M,R) satisfying F(x) 一 mmχ∈M F(x) ≤ (F(xo) 一 mιnχ∈M F(x))∕4.
Let T = dlog2(∆∕ε)∕2e + 1. Then, we can compute an ε-minimizer in time PtT=-01 Time(L +
2-t∆KR-∕R2, 2-t∆KR+∕R2, M, R), where KR+ andKR- are constants that depend on R and the
bounds on the sectional curvature of M.
Example 3.4. Applying reduction Theorem 3.3 to the algorithm in Corollary 3.2 We can optimize
L-Smooth and g-convex functions defined on H or S with a gradient oracle complexity of O(L∕√ε).
Note that this reduction cannot be applied to the locally accelerated algorithm in (Zhang & Sra, 2018),
that We discussed in the related Work section. The reduction runs in stages by adding decreasing
μi-strongly convex regularizers until we reach μ% = O(ε). The local assumption required by the
algorithm in (Zhang & Sra, 2018) on the closeness to the minimum cannot be guaranteed. In (Ahn &
Sra, 2020), the authors give an unconstrained global algorithm whose rates are strictly better than
RGD. The reduction could be applied to a constrained version of this algorithm to obtain a method
for smooth and g-convex functions defined on manifolds of bounded sectional curvature and whose
rates are strictly better than RGD.
4	Conclusion
In this work we proposed a first-order method with the same rates as AGD, for the optimization of
smooth and g-convex or strongly g-convex functions defined on a manifold other than the Euclidean
space, up to constants and log factors. We focused on the hyperbolic and spherical spaces, that have
constant sectional curvature. The study of geometric properties for the constant sectional curvature
case can be usually employed to conclude that a space of bounded sectional curvature satisfies a
property that is in between the ones for the cases of constant extremal sectional curvature. Several
previous algorithms have been developed for the optimization in Riemannian manifolds of bounded
sectional curvature by utilizing this philosophy, for instance Ahn & Sra (2020); Ferreira et al. (2019);
Wang et al. (2015); Zhang & Sra (2016; 2018). In future work, we will attempt to use the techniques
and insights developed in this work to give an algorithm with the same rates as AGD for manifolds of
bounded sectional curvature.
The key technique of our algorithm is the effective lower bound aggregation. Indeed, lower bound
aggregation is the main hurdle to obtain accelerated first-order methods defined on Riemannian
manifolds. Whereas the process of obtaining effective decreasing upper bounds on the function works
similarly as in the Euclidean space—the same approach of locally minimizing the upper bound given
by the smoothness assumption is used—obtaining adequate lower bounds proves to be a difficult
task. We usually want a simple lower bound such that it, or a regularized version of it, can be easily
optimized globally. We also want that the lower bound combines the knowledge that the g-convexity
or g-strong convexity provides for all the queried points, commonly an average. These Riemannian
convexity assumptions provide simple lower bounds, namely linear or quadratic, but each with respect
to each of the tangent spaces of the queried points only. The deformations of the space complicate
the aggregation of the lower bounds. Our work deals with this problem by finding appropriate lower
bounds via the use of a geodesic map and takes into account the deformations incurred to derive
a fully accelerated algorithm. We also needed to deal with other technical problems. Firstly, we
8
Under review as a conference paper at ICLR 2021
needed a lower bound on the whole function and not only on F (x*), for which We had to construct
two different linear lower bounds, obtaining a relaxation of convexity. Secondly, we had to use an
implicit discretization of an accelerated continuous dynamics, since at least the vanilla application of
usual approaches like Linear Coupling Allen Zhu & Orecchia (2017) or Nesterov’s estimate sequence
Nesterov (1983), that can be seen as a forward Euler discretization of the accelerated dynamics
combined with a balancing gradient step Diakonikolas & Orecchia (2019), did not work in our
constrained case. We interpret that the difficulty arises from trying to keep the gradient step inside
the constraints while being able to compensate for a lower bound that is looser by a constant factor.
9
Under review as a conference paper at ICLR 2021
References
Kwangjun Ahn and Suvrit Sra. From Nesterov’s estimate sequence to riemannian acceleration. arXiv
preprint arXiv:2001.08876, 2020. URL https:// arxiv.org/ abs/ 2001.08876.
FoiVos Alimisis, Antonio Orvieto, Gary Becigneul, and Aurelien Lucchi. A continuous-time PersPec-
tive for modeling acceleration in riemannian optimization. arXiv preprint arXiv:1910.10782, 2019.
URL https:// arxiv.org/ abs/ 1910.10782.
Foivos Alimisis, Antonio Orvieto, Gary Becigneul, and Aurelien Lucchi. Practical accelerated
oPtimization on riemannian manifolds. arXiv preprint arXiv:2002.04144, 2020. URL https:
// arxiv.org/ abs/ 2002.04144.
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. J. Mach.
Learn. Res.,18:221:1-221:51, 2017. URL http://jmlr.org/papers/v18/16-410.html.
Zeyuan Allen-Zhu. Natasha 2: Faster non-convex oPtimization than SGD. In Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems
2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada, pp. 2680-2691, 2018a. URL
http://papers.nips.cc/paper/7533-natasha-2-faster-non-convex-optimization-than-sgd.
Zeyuan Allen-Zhu. Katyusha X: practical momentum method for stochastic sum-of-nonconvex
optimization. In Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmdssan, Stockholm, Sweden, July 10-15, 2018, pp. 179-185, 2018b. URL
http:// proceedings.mlr.press/ v80/ allen- zhu18a.html.
Zeyuan Allen Zhu and Elad Hazan. Optimal black-box reductions between optimiza-
tion objectives.	In Advances in Neural Information Processing Systems 29: An-
nual Conference on Neural Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 1606-1614, 2016. URL http:// papers.nips.cc/ paper/
6364- optimal- black- box- reductions- between- optimization- objectives.
Zeyuan Allen Zhu and Lorenzo Orecchia. Nearly-linear time positive LP solver with faster con-
vergence rate. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory
of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015, pp. 229-236, 2015. doi:
10.1145/2746539.2746573. URL https:// doi.org/ 10.1145/ 2746539.2746573.
Zeyuan Allen Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient
and mirror descent. In 8th Innovations in Theoretical Computer Science Conference, ITCS 2017,
January 9-11, 2017, Berkeley, CA, USA, pp. 3:1-3:22, 2017. doi: 10.4230/LIPIcs.ITCS.2017.3.
URL https:// doi.org/ 10.4230/ LIPIcs.ITCS.2017.3.
Zeyuan Allen Zhu, Zheng Qu, Peter RichEik, and Yang Yuan. Even faster accelerated coordinate
descent using non-uniform sampling. In Proceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1110-1119, 2016.
URL http:// proceedings.mlr.press/ v48/ allen- zhuc16.html.
Zeyuan Allen-Zhu, Yuanzhi Li, Rafael Mendes de Oliveira, and Avi Wigderson. Much faster
algorithms for matrix scaling. In 58th IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017, pp. 890-901, 2017. doi: 10.1109/
FOCS.2017.87. URL https://doi.org/10.1109/FOCS.2017.87.
Zeyuan Allen-Zhu, Ankit Garg, Yuanzhi Li, Rafael Oliveira, and Avi Wigderson. Operator scaling via
geodesically convex optimization, invariant theory and polynomial identity testing. In Proceedings
of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pp. 172-181, 2018.
Herbert Busemann and Bhalchandra Phadke. A general version of Beltrami’s theorem in the large.
Pacific Journal of Mathematics, 115(2):299-315, 1984.
Leopold Cambier and Pierre-Antoine Absil. Robust low-rank matrix completion by riemannian
optimization. SIAM J. Scientific Computing, 38(5), 2016. doi: 10.1137/15M1025153. URL
https:// doi.org/ 10.1137/ 15M1025153.
10
Under review as a conference paper at ICLR 2021
Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. "Convex until proven guilty":
Dimension-free acceleration of gradient descent on non-convex functions. In Proceedings of the
34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017, pp. 654-663, 2017. URL http:〃PrOCeedingS.mlrpress∕v70/CarmOn17a.html.
Anoop Cherian and Suvrit Sra. Riemannian dictionary learning and sparse coding for positive
definite matrices. IEEE Trans. Neural NetwOrks Learn. Syst., 28(12):2859-2871, 2017. doi:
10.1109/TNNLS.2016.2601307. URL https://dOi.Org/10.1109/TNNLS.2016.2601307.
Michael Cohen, Jelena Diakonikolas, and Lorenzo Orecchia. On acceleration with noise-corrupted
gradients. In PrOCeedings Of the 35th InternatiOnal COnferenCe On MaChine Learning, ICML
2018, Stockholmsmdssan, Stockholm, Sweden, July 10-15, 2018, pp. 1018-1027, 2018. URL
http:// prOCeedings.mlr.press/ v80/ COhen18a.html.
Chris Criscitiello and Nicolas Boumal. Efficiently escaping saddle points on manifolds. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 5985-5995, 2019.
URL http:// papers.nips.cc/ paper/ 8832- efficiently- escaping- saddle- points- on- manifolds.
Chris Criscitiello and Nicolas Boumal. An accelerated first-order method for non-convex optimization
on manifolds. arXiv preprint arXiv:2008.02252, 2020.
Ashok Cutkosky and Tamas Sarl6s. Matrix-free preconditioning in online learning. In Proceedings of
the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, pp. 1455-1464, 2019. URL http:// proceedings.mlr.press/ v97/ cutkosky19b.html.
Glaydston de Carvalho Bento, Orizon P. Ferreira, and Jefferson G. Melo. Iteration-complexity of
gradient, subgradient and proximal point methods on riemannian manifolds. J. Optim. Theory
Appl., 173(2):548-562, 2017. doi: 10.1007/s10957-017-1093-4. URL https://doi.org/10.1007/
s10957-017-1093-4.
Jelena Diakonikolas and Michael I. Jordan. Generalized momentum-based methods: A hamiltonian
perspective. CoRR, abs/1906.00436, 2019. URL http://arxiv.org/abs/1906.00436.
Jelena Diakonikolas and Lorenzo Orecchia. Accelerated extra-gradient descent: A novel accelerated
first-order method. In 9th Innovations in Theoretical Computer Science Conference, ITCS 2018,
January 11-14, 2018, Cambridge, MA, USA, pp. 23:1-23:19, 2018. doi: 10.4230/LIPIcs.ITCS.
2018.23. URL https:// doi.org/ 10.4230/ LIPIcs.ITCS.2018.23.
Jelena Diakonikolas and Lorenzo Orecchia. The approximate duality gap technique: A unified
theory of first-order methods. SIAM Journal on Optimization, 29(1):660-689, 2019. doi: 10.1137/
18M1172314. URL https:// doi.org/ 10.1137/ 18M1172314.
Alan Edelman, Tomas A. Arias, and Steven Thomas Smith. The geometry of algorithms with
orthogonality constraints. SIAM J. Matrix Analysis Applications, 20(2):303-353, 1998. doi:
10.1137/S0895479895290954. URL https:// doi.org/ 10.1137/ S0895479895290954.
OP Ferreira, MS Louzeiro, and LF Prudente. Gradient method for optimization on riemannian
manifolds with lower bounded curvature. SIAM Journal on Optimization, 29(4):2517-2541, 2019.
Alexander Gasnikov, Pavel E. Dvurechensky, Eduard A. Gorbunov, Evgeniya A. Vorontsova, Daniil
Selikhanovych, CeSar A. Uribe, Bo Jiang, Haoyue Wang, Shuzhong Zhang, SebaStien Bubeck,
Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. Near optimal methods for minimizing
convex functions with lipschitz $p$-th derivatives. In Conference on Learning Theory, COLT 2019,
25-28 June 2019, Phoenix, AZ, USA, pp. 1392-1393, 2019. URL http:// proceedings.mlr.press/
v99/ gasnikov19b.html.
Matthieu Genicot, Wen Huang, and Nickolay T. Trendafilov. Weakly correlated sparse components
with nearly orthonormal loadings. In Geometric Science of Information - Second International
Conference, GSI 2015, Palaiseau, France, October 28-30, 2015, Proceedings, pp. 484-490, 2015.
doi: 10.1007/978-3-319-25040-3\_52. URL https://doi.org/10.1007/978-3-319-25040-3_52.
Marvin J Greenberg. Euclidean and non-Euclidean geometries: Development and history. Macmillan,
1993.
11
Under review as a conference paper at ICLR 2021
Karsten Grove, Peter Petersen, and Silvio Levy. Comparison geometry, volume 30. Cambridge
University Press, 1997.
Sergey Guminov and Alexander Gasnikov. Accelerated methods for alpha-weakly-quasi-convex
problems. arXiv preprint arXiv:1710.00797, 2017. URL https:// arxiv.org/ abs/ 1710.00797.
Gennadij Heidel and Volker Schulz. A riemannian trust-region method for low-rank tensor completion.
Numerical Lin. Alg. with Applic., 25(6), 2018. doi: 10.1002/nla.2175. URL https://doi.org/10.
1002/nla.2175.
Oliver Hinder, Aaron Sidford, and Nimit Sharad Sohoni. Near-optimal methods for minimizing
star-convex functions and beyond. CoRR, abs/1906.11985, 2019. URL http:// arxiv.org/ abs/ 1906.
11985.
Reshad Hosseini and Suvrit Sra. Matrix manifold optimization for gaussian mixtures. In Advances in
Neural Information Processing Systems 28: Annual Conference on Neural Information Processing
Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 910-918, 2015. URL
http:// papers.nips.cc/ paper/ 5812- matrix- manifold- optimization- for- gaussian- mixtures.
Reshad Hosseini and Suvrit Sra. An alternative to EM for gaussian mixture models: Batch and
stochastic riemannian optimization. CoRR, abs/1706.03267, 2017. URL http:// arxiv.org/ abs/ 1706.
03267.
Wen Huang and Ke Wei. Extending FISTA to riemannian optimization for sparse pca. arXiv preprint
arXiv:1909.05485, 2019a. URL https://arxiv.org/abs/1909.05485.
Wen Huang and Ke Wei. Riemannian proximal gradient methods. arXiv preprint arXiv:1909.06065,
2019b. URL https:// arxiv.org/ abs/ 1909.06065.
Ian T Jolliffe, Nickolay T Trendafilov, and Mudassir Uddin. A modified principal component
technique based on the lasso. Journal of computational and Graphical Statistics, 12(3):531-547,
2003.
Jurgen Jost and Jeurgen Jost. Riemannian geometry and geometric analysis, volume 42005. Springer,
5 edition, 2008.
Hiroyuki Kasai, Pratik Jawanpuria, and Bamdev Mishra. Riemannian adaptive stochastic gradient
algorithms on matrix manifolds. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 3262-3271, 2019. URL
http:// proceedings.mlr.press/ v97/ kasai19a.html.
Masoud Badiei Khuzani and Na Li. Stochastic primal-dual method on riemannian manifolds of
bounded sectional curvature. In 16th IEEE International Conference on Machine Learning and
Applications, ICMLA 2017, Cancun, Mexico, December 18-21, 2017, pp. 133-140, 2017. doi:
10.1109/ICMLA.2017.0-167. URL https://doi.org/10.1109/ICMLA.2017.0-167.
E. Kreyszig. Differential Geometry. Differential Geometry. Dover Publications, 1991. ISBN
9780486667218. URL https://books.google.es/books?id=P73DrhE9F0QC.
Walid Krichene, Alexandre M. Bayen, and Peter L. Bartlett. Accelerated mirror descent
in continuous and discrete time. In Advances in Neural Information Processing Systems
28: Annual Conference on Neural Information Processing Systems 2015, December 7-12,
2015, Montreal, Quebec, Canada, pp. 2845-2853, 2015. URL http:// papers.nips.cc/ paper/
5843-accelerated-mirror-descent-in-continuous-and-discrete-time.
Mario Lezcano-Casado. Trivializations for gradient-based optimization on manifolds.
In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pp. 9154-9164, 2019. URL http:// papers.nips.cc/ paper/
9115- trivializations- for- gradient- based- optimization- on- manifolds.
Mario Lezcano-Casado. Curvature-dependant global convergence rates for optimization on manifolds
of bounded geometry. arXiv preprint arXiv:2008.02517, 2020.
12
Under review as a conference paper at ICLR 2021
Mario Lezcano-Casado and David Martinez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. In Proceedings of the 36th Interna-
tional Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
USA, pp. 3794-3803, 2019. URL http://Proceedings.mlrpress∕v97∕lezcano-casado19a.html.
Yuanyuan Liu, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. Accelerated first-order
methods for geodesically convex optimization on riemannian manifolds. In Isabelle Guyon, Ulrike
von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp.
4868-4877, 2017.
Bamdev Mishra and Rodolphe Sepulchre. R3MC: A riemannian three-factor algorithm for low-rank
matrix completion. In 53rd IEEE Conference on Decision and Control, CDC 2014, Los Angeles,
CA, USA, December 15-17, 2014, pp. 1137-1142, 2014. doi: 10.1109/CDC.2014.7039534. URL
https:// doi.org/ 10.1109/ CDC.2014.7039534.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2).
In Soviet Mathematics Doklady, volume 27, pp. 372-376, 1983.
Yurii Nesterov, Alexander Gasnikov, Sergey Guminov, and Pavel Dvurechensky. Primal-dual
accelerated gradient descent with line search for convex and nonconvex optimization problems.
arXiv preprint arXiv:1809.05895, 2018. URL https:// arxiv.org/ abs/ 1809.05895.
Peter Petersen, S Axler, and KA Ribet. Riemannian geometry, volume 171. Springer, 2006.
Hiroyuki Sato, Hiroyuki Kasai, and Bamdev Mishra. Riemannian stochastic variance reduced
gradient. CoRR, abs/1702.05594, 2017. URL http:// arxiv.org/ abs/ 1702.05594.
Hiroyuki Sato, Hiroyuki Kasai, and Bamdev Mishra. Riemannian stochastic variance reduced gradient
algorithm with retraction and vector transport. SIAM Journal on Optimization, 29(2):1444-1472,
2019. doi: 10.1137/17M1116787. URL https://doi.org/10.1137/17M1116787.
Weijie Su, StePhen P Boyd, and Emmanuel J. Candes. A differential equation for modeling nesterov's
accelerated gradient method: Theory and insights. J. Mach. Learn. Res., 17:153:1-153:43, 2016.
URL http:// jmlr.org/ papers/ v17/ 15- 084.html.
Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere II: recovery by
riemannian trust-region method. IEEE Trans. Inf. Theory, 63(2):885-914, 2017. doi: 10.1109/TIT.
2016.2632149. URL https:// doi.org/ 10.1109/ TIT.2016.2632149.
Yue Sun, Nicolas Flammarion, and Maryam Fazel. Escaping from saddle points on rieman-
nian manifolds. In Advances in Neural Information Processing Systems 32: Annual Con-
ference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December
2019, Vancouver, BC, Canada, pp. 7274-7284, 2019. URL http:// papers.nips.cc/ paper/
8948-escaping-from-saddle-points-on-riemannian-manifolds.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of
initialization and momentum in deep learning. In Proceedings of the 30th International Conference
on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1139-1147, 2013.
URL http:// proceedings.mlr.press/ v28/ sutskever13.html.
Mingkui Tan, Ivor W. Tsang, Li Wang, Bart Vandereycken, and Sinno Jialin Pan. Riemannian
pursuit for big matrix recovery. In Proceedings of the 31th International Conference on Machine
Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 1539-1547, 2014. URL http:
// proceedings.mlr.press/ v32/ tan14.html.
Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael I. Jordan. Averaging stochastic
gradient descent on riemannian manifolds. CoRR, abs/1802.09128, 2018. URL http:// arxiv.org/
abs/1802.09128.
Bart Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal on
Optimization, 23(2):1214-1236, 2013. doi: 10.1137/110845768. URL https://doi.org/10.1137/
110845768.
13
Under review as a conference paper at ICLR 2021
Di Wang, Satish Rao, and Michael W. Mahoney. Unified acceleration method for packing and covering
problems via diameter reduction. In 43rd International Colloquium on Automata, Languages,
and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy, pp. 50:1-50:13, 2016. doi:
10.4230/LIPIcs.ICALP.2016.50. URL https://doi.org/10.4230/LIPIcs.ICALP.2016.50.
X. M. Wang, C. Li, and J. C. Yao. Subgradient projection algorithms for convex feasibility on
riemannian manifolds with lower bounded curvatures. J. Optim. Theory Appl., 164(1):202-217,
2015. doi: 10.1007/s10957-014-0568-9. URL https://doi.org/10.1007/s10957-014-0568-9.
Melanie Weber and Suvrit Sra. Frank-wolfe methods for geodesically convex optimization with
application to the matrix geometric mean. CoRR, abs/1710.10770, 2017. URL http:// arxiv.org/
abs/1710.10770.
Melanie Weber and Suvrit Sra. Nonconvex stochastic optimization on manifolds via riemannian
frank-wolfe methods. CoRR, abs/1910.04194, 2019. URL http://arxiv.org/abs/1910.04194.
Ke Wei, Jian-Feng Cai, Tony F Chan, and Shingyu Leung. Guarantees of riemannian optimization
for low rank matrix completion. arXiv preprint arXiv:1603.06610, 2016. URL https:// arxiv.org/
abs/1603.06610.
Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. A variational perspective on accelerated
methods in optimization. CoRR, abs/1603.04245, 2016. URL http:// arxiv.org/ abs/ 1603.04245.
Ami Wiesel. Geodesic convexity and covariance estimation. IEEE Trans. Signal Process., 60(12):
6182-6189, 2012. doi: 10.1109/TSP.2012.2218241. URL https:// doi.org/ 10.1109/ TSP.2012.
2218241.
Hongyi Zhang and Suvrit Sra. First-order methods for geodesically convex optimization. In
Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26,
2016, pp. 1617-1638, 2016. URL http:// proceedings.mlr.press/ v49/ zhang16b.html.
Hongyi Zhang and Suvrit Sra. An estimate sequence for geodesically convex optimization. In
Sebastien Bubeck, Vianney PercheL and Philippe RigOllet (eds.), Conference On Learning Theory,
COLT 2018, Stockholm, Sweden, 6-9 July 2018, volume 75 of Proceedings of Machine Learning
Research, pp. 1703-1723. PMLR, 2018. URL http:// proceedings.mlr.press/ v75/ zhang18a.html.
HOngyi Zhang, Sashank J. Reddi, and Suvrit Sra. Riemannian SVRG: fast stOchastic Op-
timizatiOn On riemannian manifOlds. In Advances in Neural Information Processing Sys-
tems 29: Annual Conference on Neural Information Processing Systems 2016, December
5-10, 2016, Barcelona, Spain, pp. 4592-4600, 2016. URL http:// papers.nips.cc/ paper/
6515-riemannian-svrg-fast-stochastic-optimization-on-riemannian-manifolds.
JingzhaO Zhang, HOngyi Zhang, and Suvrit Sra. R-SPIDER: A fast riemannian stOchastic OptimizatiOn
algOrithm with curvature independent rate. CoRR, abs/1811.04194, 2018. URL http:// arxiv.org/
abs/1811.04194.
Pan ZhOu, XiaO-TOng Yuan, and Jiashi Feng. Faster first-Order methOds fOr stOchastic nOn-cOnvex
OptimizatiOn On riemannian manifOlds. In The 22nd International Conference on Artificial Intelli-
gence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pp. 138-147, 2019.
URL http:// proceedings.mlr.press/ v89/ zhou19a.html.
14