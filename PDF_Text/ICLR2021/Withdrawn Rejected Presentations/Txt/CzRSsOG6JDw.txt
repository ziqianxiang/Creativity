Under review as a conference paper at ICLR 2021
The impacts of known and unknown demon-
STRATOR IRRATIONALITY ON REWARD INFERENCE
Anonymous authors
Paper under double-blind review
Ab stract
Algorithms inferring rewards from human behavior typically assume that people
are (approximately) rational. In reality, people exhibit a wide array of irrationalities.
Motivated by understanding the benefits of modeling these irrationalities, we
analyze the effects that demonstrator irrationality has on reward inference. We
propose operationalizing several forms of irrationality in the language of MDPs,
by altering the Bellman optimality equation, and use this framework to study how
these alterations affect inference.
We find that incorrectly assuming noisy-rationality for an irrational demonstrator
can lead to remarkably poor reward inference accuracy, even in situations where
inference with the correct model leads to good inference. This suggests a need
to either model irrationalities or find reward inference algorithms that are more
robust to misspecification of the demonstrator model. Surprisingly, we find that if
we give the learner access to the correct model of the demonstrator’s irrationality,
these irrationalities can actually help reward inference. In other words, if we could
choose between a world where humans were perfectly rational and the current
world where humans have systematic biases, the current world might counter-
intuitively be preferable for reward inference. We reproduce this effect in several
domains. While this finding is mainly conceptual, it is perhaps actionable as well:
we might ask human demonstrators for myopic demonstrations instead of optimal
ones, as they are more informative for the learner and might be easier for a human
to generate.
1 Introduction
Motivated by difficulty in reward specification (Lehman et al., 2018), inverse reinforcement learning
(IRL) methods estimate a reward function from human demonstrations (Ng et al., 2000; Abbeel
and Ng, 2004; Kalman, 1964; Jameson and Kreindler, 1973; Mombaur et al., 2010). The central
assumption behind these methods is that human behavior is rational, i.e., optimal with respect to their
reward (cumulative, in expectation). Unfortunately, decades of research in behavioral economics and
cognitive science (Chipman, 2014) have unearthed a deluge of irrationalities, i.e., of ways in which
people deviate from optimal decision making: hyperbolic discounting, scope insensitivity, illusion of
control, decision noise, loss aversion, to name a few.
While as a community we are starting to account for some possible irrationalities plaguing demon-
strations in different ways (Ziebart et al., 2008; 2010; Singh et al., 2017; Reddy et al., 2018; Evans
et al., 2016; Shah et al., 2019), we understand relatively little about what effect irrationalities have
on the difficulty of inferring the reward. In this work, we seek a systematic analysis of this effect.
Do irrationalities make it harder to infer the reward? Is it the case that the more irrational someone
is, the harder it is to infer the reward? Do we need to account for the specific irrationality type
during learning, or can we get away with the standard noisy-rationality model? The answers to these
questions are important in deciding how to move forward with reward inference. If irrationality,
even when well modelled, still makes reward inference very difficult, then we will need alternate
ways to specify behaviors. If well-modelled irrationality leads to decent reward inference but we
run into problems when we just make a noisy-rational assumption, that suggests we need to start
accounting for irrationality more explicitly, or at least seek assumptions or models that are robust
to many different types of biases people might present. Finally, if the noisy-rational model leads
1
Under review as a conference paper at ICLR 2021
Long Planning Horizon
Low θspeed	High θspeed
Low θspeed	High θspeed
Figure 1: We found that irrationality does not always hinder reward inference (section 3.2) - it is
in many cases actually helpful. Here, We depict rational and myopic (short-sighted) behavior in a
merging environment (section 4.2) for two different rewards, with higher and lower weights on going
fast. The rational car (white) exhibits similar behavior under both reward functions, while the myopic
car (blue) overtakes on the shoulder when the reward function places a high weight on speed. This
makes it easier to differentiate what its reward is.
to decent inference even when the demonstrator is irrational, then we need not dedicate significant
resources to addressing irrationality.
One challenge with conducting such an analysis is that there are many irrationalities in the psychology
and behavioral economics literature, with varying degrees of mathematical formalization versus
empirical description. To structure the space for our analysis, we operationalize irrationalities
in the language of MDPs by systematically enumerating possible deviations from the Bellman
equation - imperfect maximization, deviations from the true transition function, etc. This gives US a
formal framework in which we can simulate irrational behavior, run reward inference, and study its
performance. Armed with this formalism, we then explore the various impacts of irrationality on
reward learning in three families of environments: small random MDPs, a more legible gridworld
MDP, and an autonomous driving domain drawn from the robotics literature (Sadigh et al., 2016).
Irrationality can help, rather than hinder reward inference - if it is modelled correctly. We
first explore the impacts of demonstrator irrationality when the irrationality is known to the reward
inference algorithm. Surprisingly, we find that certain irrationalities actually improve the quality of
reward inference - that is, they make reward easier to learn. Importantly, this is not compared to
assuming the wrong model of the human: our finding is that humans who exhibit (correctly modelled)
irrationality are more informative than humans who exhibit (correctly modelled) rationality! This
is consistent in all three domains. We explain this theoretically from the perspective of the mutual
information between the demonstrator behavior and the reward parameters, proving that some
irrationalities are arbitrarily more informative than rational behavior.
Unmodelled irrationality leads to remarkably poor reward inference. It might seem that we
can’t immediately benefit from the knowledge that irrationalities help inference unless we have a
comprehensive understanding of human decision-making, and so we should just stick to the status
quo of modeling people as rational. However, we find that modeling irrational demonstrators as
noisily-rational can lead to worse outcomes than not performing inference at all and just using the
prior (section 5). Encouragingly, we also find evidence that even just modeling the demonstrator’s
irrationality approximately allows a learner to outperform modeling the demonstrator as noisily-
rational (section E).
Overall, we contribute 1) a theoretical and empirical analysis of the effects of different irrationalities
on reward inference, 2) a way to systematically formalize and cover the space of irrationalities in
order to conduct such an analysis, and 3) evidence for the importance and benefit of accounting for
irrationality irrationality during inference.
Our results suggest that modeling people as noisily rational leads to poor reward inference, and that it
is important to model the irrationalities of human demonstrators. Our good news is that if we manage
to do that well, we might be better off even compared to a counterfactual world in which people are
actually rational! Of course, modeling irrationality is a long term endeavour. Our near-term good
news is two fold: first, irrationalities can be an ally for teaching. For example, we could ask human
demonstrators to act more myopically to better communicate their reward to the learners. Second, we
need not get the biases perfectly correct to do better than assuming noisy-rationality. Instead, using
slightly more realistic models of human irrationality could lead to better inference.
2
Under review as a conference paper at ICLR 2021
Myopic Discounting
Myopic Horizon
Boltzmann-Rationality	Prospect Bias Hyperbolic Discounting
Vi+1 (S) = max X Ps,a(s0) (rθ(s,a, s0) + YV(SO))
a
s0∈S ∖	∖
OPtimISm/Pessimism	Extremal BiaS
Illusion of Control
Figure 2: In section 2.3, we modify the components of the Bellman update to cover different types of
irrationalities: changing the max into a softmax to capture noise (Boltzmann), changing the transition
function to capture optimism/pessimism or the illusion of control, changing the reward values to
capture the nonlinear perception of gains and losses (Prospect), changing the average reward over time
into a maximum (Extremal), and changing the discounting to capture more myopic decision-making.
2 Framework： Biases AS deviations from THE BELLMAN update
2.1	Exploring biases through simulation
While ideally we would recruit human subjects with different irrationalities and measure how well
we can learn rewards, this is prohibitive because we do not get to dictate someone’s irrationality type:
people exhibit a mix of them, some yet to be discovered. Further, measuring the accuracy of inference
from observing real humans is complicated by the fact that we do not have ground truth access to
the human’s reward function. For instance, suppose we asked subjects to produce a set of (behavior,
reward function) pairs. We could then try to predict the reward functions from the behaviors. But
how did we, the experimenters, infer the reward functions from the people? if we are wrong in our
assumptions about which irrationalities are affecting their behavior and/or explicit reports of rewards,
we would remain deluded about the subjects’ true intentions and preferences.
To address these issues, we simulate demonstrator behavior subject to different irrationalities, run
reward inference, and measure the performance against the ground truth, i.e., the accuracy of a
Bayesian posterior on the reward parameter given the (simulated) demonstrator’s inputs.
2.2	Background and formalism
Consider an Uncertain-Reward MDP (URMDP) (Bagnell et al., 2001; Regan and Boutilier, 2011;
Desai, 2017) M = (S, A, {Ps,a}, γ, Θ, p, r), consisting of finite state and action sets S and A,
distributions over states {Ps,a} representing the result of taking action a in state s, discount rate
γ ∈ [0, 1), a (finite) set of reward parameters Θ, a prior distribution p ∈ ∆(S × Θ) over starting states
and reward parameters, and a parameterized state-action reward function r : Θ × S × A × S → R,
where rθ (s, a, s0) represents the reward received.
We assume that the human demonstrator’s policy π satisfies π = d(θ), where d is an (environment-
specific) planner d : Θ → Π that returns a (possibly stochastic) policy given a particular reward
parameter θ. The rational demonstrator uses a planner dRational that, given a reward parameter θ,
returns a policy that maximizes its expected value. on the other hand, we say that an demonstrator
is irrational if its planner returns policies with lower expected value than the optimal policy, for at
least one θ ∈ Θ.
2.3	Types and degrees of irrationality
There are many possible irrationalities that people exhibit (Chipman, 2014), far more than what we
could study in one paper. To provide good coverage of this space, we start from the Bellman update,
and systematically manipulate its terms and operators to produce a variety of different irrationalities
that deviate from the optimal MDP policy in complementary ways. For instance, operating on the
discount factor can model myopic behavior, while operating on the transition function can model
optimism or the illusion of control. We parametrize each irrationality so that we can manipulate its
“intensity" or deviation from rationality. Figure 2 summarizes our approach, which we detail below.
3
Under review as a conference paper at ICLR 2021
2.3.1	Rational demonstrator
In our setup, the rational demonstrator does value iteration using the Bellman update from Fig. 2.
Our models change this update to produce different types of non-rational behavior.
2.3.2	Modifying the max operator: B oltzmann
Boltzmann-rationality modifies the maximum over actions maxa with a Boltzmann operator with a
parameter β :
Vi+1(s) = Boltzβa X Ps,a(s0) (rθ (s, a, s0) +γVi(s0)) ,
s0∈S
where Boltzβ (x) = Pi xi eβxi /Pi eβxi (Ziebart et al., 2010; Asadi and Littman, 2017). This is
the most popular stochastic model used in reward inference(Ziebart et al., 2010; Asadi and Littman,
2017; Fisac et al., 2017). After computing the value function, the Boltzmann-rational planner dBoltz
returns a policy where the probability of an action is proportional to the exponential of the Q-value
of the action:
π(a∣s) H eβQθ(s,a).
The constant β is called the rationality constant, because as β → ∞, the human choices approach
perfect rationality (optimality), whereas β = 0 produces uniformly random choices.
2.3.3 Modifying the transition function
Our next set of irrationalities manipulate the transition function away from reality.
Illusion of Control. People often overestimate their ability to control random events (Thompson,
1999). To model this, we consider demonstrators that use the Bellman update:
Vi+1(s) = max X Psna(s0) (rθ(s,a,s0) + γVi(s0))
a,
s0∈S
where Psn,a(s0) H (Ps,a(s0))n. As n → ∞, the demonstrator acts as if it exists in a deterministic
environment. As n → 0, the demonstrator acts as if it had an equal chance of transitioning to every
possible successor state.
Optimism/Pessimism. Many people systematically overestimate or underestimate their chance
experiencing of positive over negative events (Sharot et al., 2007). We model this using demonstrators
that modify the probability they get outcomes based on the value of those outcomes:
Vi+1(s) = max X Psωa(s0) (rθ(s,a,s0) + γVi(s0))
a,
s0∈S
where Psω,a(s0) H Ps,a(s0)eω(rθ(s,a,s0)+γVi(s)). ω controls how pessimistic or optimistic the demon-
strator is. As ω → +∞ (respectively, ω → -∞), the demonstrator becomes increasingly certain that
good (bad) transitions will happen. As ω → 0, the demonstrator approaches the rational demonstrator.
2.3.4 Modifying the reward: Prospect Bias
Next, we consider demonstrators that use the modified Bellman update:
Vi+1(s) = max X Ps,a(s0) (f (rθ (s, a, s0)) + γVi(s0))
a s0∈S
where f : R → R is some scalar function. This is equivalent to solving the MDP with reward f ◦ rθ,
and allows us to model human behavior such as loss aversion and scope insensitivity.
Prospect Bias. Kahneman and Tversky (2013) inspires us to consider a particular family of fs:
Γlog(1 + |r|) r > 0
fc(r) = 0	r = 0
(-Clog(1 + |r|) r < 0
c controls how loss averse the demonstrator is. As c → ∞, the demonstrator primarily focuses on
avoiding negative rewards. As c → 0, the demonstrator focuses on maximizing positive rewards.
4
Under review as a conference paper at ICLR 2021
2.3.5 Modifying the sum between reward and future value: Extremal
Extremal. People seem to exhibit duration neglect, sometimes only caring about the maximum
intensity of an experience (Do et al., 2008). We model this using Bellman update:
Vi+1(S) = max	Ps,a(S0) max (rθ(S,a, S0), (1 - α)rθ (S, a, S0) + αVi(S0))
a s0∈S
These demonstrators maximize the expected maximum reward along a trajectory, instead of the
expected sum of rewards. As α → 1, the demonstrator maximizes the expected maximum reward
received along the full trajectory. As α → 0, the demonstrator becomes greedy and maximizes the
immediate reward.
2.3.6 Modifying the discounting
Myopic Discount. In practice, people are sometimes myopic, only considering near-term rewards.
One way to model this is to decrease gamma in the Bellman update. At Y = Y*, the discount rate
specified by the environment, this is the rational demonstrator. As γ → 0, the demonstrator becomes
greedy and only acts to maximize immediate reward.
Myopic Value Iteration. As another way to model human myopia, we consider a demonstrator
that performs only h steps of Bellman updates. That is, this demonstrator cares equally about rewards
for a horizon h, and discount to 0 reward after that. As h → ∞, this demonstrator becomes rational.
If h = 1, this demonstrator only cares about the immediate reward.
Hyperbolic Discounting. Human also exhibit hyperbolic discounting, with a high discount rate
for the immediate future and a low discount rate for the far future (Grune-Yanoff, 2015). Alexander
and Brown (2010) formulate this as the following Bellman update:
0 rθ(s, a, s0) + Vi(s0)
Vi+1(S) = max E Ps,a (S )	(s0)s
s0∈S
k modulates how much the demonstrator prefers rewards now versus the future. As k → 0, this
demonstrator becomes a rational demonstrator without discounting.
3 Exploring the effects of known biases on reward inference
Armed with our framework for characterizing irrationality, we test its implications for reward
inference. We start by investigating the effects in random MDPs.
3.1	Experimental Design: Exact Inference in Random MDPs.
Independent Variables. We manipulate the type of the planner, and vary the degree parameters
for each. We use different environments, sample different ground truth reward parameters, and test
different trajectory lengths for the demonstrated behavior (T = 3, 15, and 30 state-actions pairs).
Dependent Measures. To separate the inference difficulty caused by suboptimal inference from
the difficulty caused by demonstrator irrationality, we perform the exact Bayesian update on the
trajectory ξ (Ramachandran and Amir, 2007), which gives us the posterior on θ given ξ, P(θ∣ξ)=
R PPξξθθpP)θ0). Our primary metric is the expected log loss of this posterior:
LogLoSS(θ∣ξ) = Eθ*,ξ 〜d(θ*) [- log P (θ*∣ξ)].
A low log loss implies that We are assigning a high likelihood to the true θ*. Note that in this case,
the log loss is equal to the entropy of the posterior H(θ∣ξ) = H(θ) - I(θ; ξ).
For each environment and irrationality type, we calculate the performance of reward inference on
trajectories of a fixed length T. To sample a trajectory of length T from a demonstrator, we fix θ*
and start state s. Then, we generate rollouts starting from state s until T state, action pairs have been
sampled from ∏ = d(θ*). We repeat this procedure 10 times for each start state.
5
Under review as a conference paper at ICLR 2021
Boltzmann
Figure 3: The log loss (lower = better) of the posterior as a function of the parameter we vary for
each irrationality type, on the random MDP environments. For the irrationalities that interpolate to
the rational planner, we denote the value that is closest to rational using a dashed vertical line. Every
irrationality except Prospect Bias all have parameter settings that outperform the rational planner. The
error bars show the standard error of the mean, calculated by 1000 bootstraps across environments.
Simulation Environment. We used MDPs with 10 states and 2 actions, where each (s, a)-pair has
2 random, nonzero transition-probabilities. We used γ = 0.99 and start trajectories in every state
without reward. In these θ is a vector of length 3, where θi is the reward received from transitions out
of state i (and all other rewards are 0). We discretized each θi with 4 values, leading to ∣Θ∣ = 64. We
generated 20 such random MDPs.
3.2	Results
Fig. 3 plots the log loss for each irrationality for random MDPs. The degree affects reward inference,
with many settings naturally resulting in worse inference, especially at the extremes. However,
what is surprising is that every type except Prospect Bias has at least one degree (parameter) setting
that results in better inference with enough data: we see that most irrationality types can be more
informative than rational behavior. The more data we have, the more drastic the difference (T=30
results in both better inference and larger difference relative to rational).
We put this to the test with a repeated-measures ANOVA with planner type as the independent variable,
using the data from the best parameter setting from each type and T=30, and environment as a random
effect. We find a significant main effect for planner type (F (8, 806372) = 6102.93, p < 0.001). A
post-hoc analysis with Tukey HSD corrections confirmed that every irrationality except Prospect
improved inference compared to the fully rational planner (p < .001). For T = 30, Optimism with
ω = 3.16 performed best, followed by Illusion of Control with n = 0.1 and Boltzmann with β = 10.
4	Does this effect generalize?
In the domain of random MDPs, we found that not only does irrationality affect reward inference,
certain irrationalities can actually result in better inference. In this section, we probe the generality of
this finding empirically, and explain it theoretically.
4.1	Gridworld
Random MDPs lack structure, so we first test a toy environment that adds natural structure based on
OpenAI Gym’s ‘Frozen-Lake-v0’ (Brockman et al., 2016) (Fig. 5 in the appendix). Fig. 6 in the
appendix shows the results in the gridworld: they are eerily (and reassuringly) similar to the random
MDPs. This suggests that irrationalities can indeed help inference across differently structured MDPs.
By inspecting the policies, we see that the irrational demonstrators were able to outperform the
6
Under review as a conference paper at ICLR 2021
Table 1: Autonomous Driving Results: Merging
Bayesian IRL	CIOC
Horizon	Log Loss	Information Gain	Cosine Similarity	Normalized Return
3	0.690	0.696	0.999	0.939
5	0.824	0.562	0.940	0.981
7	1.383	0.004	0.350	0.856
rational demonstrator by disambiguating between θs that the rational demonstrator could not. To
visualize this, we show examples of how the policy of several irrational demonstrators differ on the
gridworld when the rational demonstrator’s policies are identical in Fig. 7, Fig. 8, and Fig. 9
(included in the appendix).
4.2	Autonomous Driving
Even if known irrationality helps reward inference both empirically and theoretically in small MDPs,
the question still remains whether this result will matter in practice. Next, we investigate the effect
of demonstratsor irrationality on reward inference in a continuous state and action self-driving car
domain (Sadigh et al., 2016). Switching to this real-world domain means we can no longer plan
exactly, so we use model predictive control. It also means we can no longer run exact inference, so
we approximate Bayesian IRL through samples, and also test continuous IOC (CIOC, Levine and
Koltun (2012)) which recovers an approximate MLE. We measure the cosine similarity between the
estimate and θ*, as well as the normalized reward of the trajectory when optimizing the recovered
estimate. We use the merging task from Fig. 1 (more details in the appendix).
We report our results in Table 1. As the columns relating to Bayesian IRL show, decreasing the MPC
planning horizon significantly decreases the log loss of the Bayesian IRL posterior. As with the
gridworld results in section 4.1, the reason for this is that the shorter planning horizons exhibit more
diverse behavior (as a function of the reward). Of the 4 reward settings we used, MPC with horizon 3
and 5 produced two different qualitative behaviors, whereas all rewards led to qualitatively similar
behavior with horizon 7 (depicted in Fig. 1). When the weight on target speed was large enough,
MPC with horizon 3 and 5 would produce a trajectory that overtakes the front car by going off the
road. In all other cases, the demonstrator would merge between the two other cars.
We found similar results for CIOC: decreasing the MPC planning horizon increases the cosine
similarity between the true and CIOC-recovered rewards. The return of optimizing the CIOC-
recovered rewards is also higher when the demonstrator’s planning horizon is 3 or 5 than 7.
Overall, yet again we find that irrationality (in this case specifically myopia) improves reward
inference by producing more diverse behavior as a function of the reward.
4.3	Theoretical Analysis
We now investigate this phenomenon theoretically. We show that not only can rational behavior be
arbitrarily less informative than irrational behavior, but also that this applies to Boltzmann-rationality.
Informativeness as mutual information. The mutual information I(θ; d(θ)) = H(θ) - H(θ∣d(θ))
between the policy and the reward parameters allows us to quantify the informativeness of a planner.
As the conditional entropy H(θ∣d(θ)) = E [- logp(θ∣d(θ)] is equal to the log loss of the posterior
under the true model, the mutual information upper bounds how much better (in terms of log
loss) any inference procedure can do, relative to the prior. A known insight is that planners that
optimize for maximizing the discounted sum of rewards are not the same as those that optimize
for being informative (Dragan et al., 2013; Hadfield-Menell et al., 2016): arg maxd Eθ[Vθd(θ)] 6=
arg maxd I(θ; d(θ)). While this insight has been tested empirically, we begin with a theoretical lens
for understanding it.
Irrationalities exist that are arbitrarily better for inference than rationality. We first consider
deterministic planners: planners that return deterministic policies. We show that there are cases
where the rational behavior is not informative at all, whereas some (irrational) deterministic planner
achieves the theoretical upper bound on informativeness. (Proof in appendix.)
7
Under review as a conference paper at ICLR 2021
Figure 4: A comparison of reward inference using a correct model of the irrationality type, versus
always using a Boltzmann-rational model (β = 10), on the random MDPs (left) and the car envi-
ronment (right). The impairment due to model misspecification greatly outweighs the variation in
inference performance caused by various irrationalities. The error bars show the standard error of the
mean, calculated by the bootstrap across environments.
Proposition 1. There exists a family of URMDPs with state spaces of any size, such there exists a
deterministic planner dirrationalsatisfying I(θ, dirrational(θ)) = log ∣Θ∣ and I(θ, dRational(θ)) = 0∙
Boltzmann-rationality is (arbitrarily) more informative than full rationality. Of course, the
upper bound is attained by some irrational planner. It demonstrates that an demonstrator can perform
better than rational when specifically optimizing for informativeness. But this is an artificial, contrived
kind of irrationality. In fact, prior work that maximized informativeness did so by solving a more
difficult problem than rationality (Dragan et al., 2013; Hadfield-Menell et al., 2016). Here, we provide
evidence that Boltzmann-rationality, a standard model of stochastic choice (discussed in section 2.3),
outperforms full rationality for reward inference. (Proof in appendix.)
Proposition 2. There exists a family ofone-state two-action MDPs, with arbitrarily large ∣Θ∣ such
that I(θ,dBoltz(θ)) = log Θ and l(θ,dRatiOnal(θ)) = 0∙
5	Effects of misspecification on reward inference
We see that irrationalities sometimes hinder, but sometimes help reward inference. So far, the learner
had access to the type (and degree of irrationality) during inference. Next, we ask how important it is
to know this. Can we not bother with irrationality, make a default assumption, and run inference?
Assuming noisy rationality can lead to very poor inference. Fig. 4 suggests that the answer is
no. We start by comparing inference with the true model on random MDPs versus with assuming
the standard Boltzmann model as a default. The results are quite striking: not knowing the correct
irrationality harms inference tremendously.1 We then confirm that misspecification greatly harms
inference in the autonomous driving environment. This emphasizes the importance of understanding
irrationality when doing reward inference going forward.
Approximate models of irrationality might be enough. This finding is rather daunting, as perfect
models of irrationality are very challenging to develop. But do they need to be perfect? Our final
analysis suggests that the answer is no as well. In Fig. 11 of the appendix, we report the log loss
of inference with the correct type, but under misspecification of the parameter. Encouragingly, we
find that in many cases, merely getting the type of the demonstrator irrationality correct is actually
sufficient to lead to much better inference than assuming Boltzmann rationality. Further, we also find
evidence that the learner does not need to get the type exactly right either: as shown in Fig. 12, if the
learner accounts for the demonstrator’s myopia, but gets the type of the myopia wrong, this still leads
to significantly better inference than assuming Boltzmann rationality.
1(Shah et al., 2019) proposed a way to model irrationality and analyzed its benefit over assuming Boltzmann;
the benefit was very limited, which they attributed due to their deep learning model’s brittleness compared to
exact planning. Here, we compare to perfect modeling to analyze the headroom that modeling has.
8
Under review as a conference paper at ICLR 2021
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, page 1. ACM,
2004. URL: http://people.eecs.berkeley.edu/~russell/classes/cs294/
s11/readings/Abbeel+Ng:2004.pdf.
William H Alexander and Joshua W Brown. Hyperbolically discounted temporal difference learning.
Neural computation, 22(6):1511-1527, 2010.
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 243-252.
JMLR. org, 2017.
J Andrew Bagnell, Andrew Y Ng, and Jeff G Schneider. Solving uncertain markov decision processes.
2001.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym.(2016). arXiv preprint arXiv:1606.01540, 2016.
Susan E. F. Chipman. The Oxford Handbook of Cognitive Science. Oxford University Press, 11 2014.
ISBN 9780199842193.
Antoine Cully, Jeff Clune, Danesh Tarapore, and Jean-Baptiste Mouret. Robots that can adapt like
animals. Nature, 521(7553):503-507, 2015.
Nishant Desai. Uncertain reward-transition mdps for negotiable reinforcement learning. Technical
report, UC Berkeley, Berkeley, California, USA, 2017.
Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular
neural network policies for multi-task and multi-robot transfer. In 2017 IEEE International
Conference on Robotics and Automation (ICRA), pages 2169-2176. IEEE, 2017.
Amy M Do, Alexander V Rupert, and George Wolford. Evaluations of pleasurable experiences: The
peak-end rule. Psychonomic Bulletin & Review, 15(1):96-98, 2008.
Anca D Dragan, Kenton CT Lee, and Siddhartha S Srinivasa. Legibility and predictability of robot
motion. In Proceedings of the 8th ACM/IEEE international conference on Human-robot interaction,
pages 301-308. IEEE Press, 2013.
OWain Evans, Andreas Stuhlmuller, and Noah Goodman. Learning the Prefer-
ences of ignorant, inconsistent agents. In Thirtieth AAAI Conference on Artifi-
cial Intelligence, 2016. URL: https://pdfs.semanticscholar.org/31bf/
e42e77a572bd83c0529e0f03bc3dc8af52c2.pdf.
Jaime F Fisac, Monica A Gates, Jessica B Hamrick, Chang Liu, Dylan Hadfield-Menell, Malayandi
PalaniaPPan, Dhruv Malik, S Shankar Sastry, Thomas L Griffiths, and Anca D Dragan. Pragmatic-
Pedagogic value alignment. arXiv preprint arXiv:1707.06354, 2017. URL: https://arxiv.
org/pdf/1707.06354.pdf.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust reWards With adverserial inverse re-
inforcement learning. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=rkHywl-A-.
Till Grune-Yanoff. Models of temPoral discounting 1937-2000: An interdisciPlinary exchange
betWeen economics and Psychology. Science in context, 28(4):675-713, 2015.
Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. CooPerative inverse
reinforcement learning. In Advances in neural information processing systems, Pages 3909-3917,
2016.
Antony Jameson and Eliezer Kreindler. Inverse Problem of linear oPtimal control. SIAM Journal
on Control, 11(1):1-19, 1973. URL: https://epubs.siam.org/doi/pdf/10.1137/
0311001.
9
Under review as a conference paper at ICLR 2021
Mingxuan Jing, Xiaojian Ma, Wenbing Huang, Fuchun Sun, and Huaping Liu. Task transfer by
preference-based cost learning. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pages 2471-2478, 2019.
Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In
Handbook of the fundamentals of financial decision making: Part I, pages 99-127. World Scientific,
2013.
Rudolf Emil Kalman. When is a linear control system optimal? Journal of Basic Engi-
neering, 86(1):51-60, 1964. URL: https://asmedigitalcollection.asme.org/
fluidsengineering/article-abstract/86/1/51/392203.
Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J
Bentley, Samuel Bernard, Guillaume Beslon, David M Bryson, et al. The surprising creativity of
digital evolution: A collection of anecdotes from the evolutionary computation and artificial life
research communities. arXiv preprint arXiv:1803.03453, 2018.
Sergey Levine and Vladlen Koltun. Continuous inverse optimal control with locally optimal examples.
arXiv preprint arXiv:1206.4617, 2012.
Katja Mombaur, Anh Truong, and Jean-Paul Laumond. From human to humanoid locomotion—an
inverse optimal control approach. Autonomous robots, 28(3):369-383, 2010. URL: https:
//link.springer.com/content/pdf/10.1007/s10514-009-9170-7.pdf.
Andrew YNg, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1,
page 2, 2000. URL: http://ai.stanford.edu/~ang/papers/icml00-irl.pdf.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In IJCAI, volume 7,
pages 2586-2591, 2007.
Sid Reddy, Anca Dragan, and Sergey Levine. Where do you think you’re going?: Inferring beliefs
about dynamics from behavior. In Advances in Neural Information Processing Systems, pages
1454-1465, 2018.
Kevin Regan and Craig Boutilier. Robust online optimization of reward-uncertain mdps. In Twenty-
Second International Joint Conference on Artificial Intelligence, 2011.
Dorsa Sadigh, Shankar Sastry, Sanjit A Seshia, and Anca D Dragan. Planning for autonomous cars
that leverage effects on human actions. In Robotics: Science and Systems, volume 2. Ann Arbor,
MI, USA, 2016.
Dorsa Sadigh, Anca D. Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based
learning of reward functions. In Robotics: Science and Systems (RSS), 2017.
Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca D Dragan. On the feasibility of learning, rather
than assuming, human biases for reward inference. arXiv preprint arXiv:1906.09624, 2019. URL:
https://arxiv.org/pdf/1906.09624.pdf.
Tali Sharot, Alison M Riccardi, Candace M Raio, and Elizabeth A Phelps. Neural mechanisms
mediating optimism bias. Nature, 450(7166):102-105, 2007.
Sumeet Singh, Jonathan Lacotte, Anirudha Majumdar, and Marco Pavone. Risk-sensitive inverse
reinforcement learning via semi-and non-parametric methods. arXiv preprint arXiv:1711.10055,
2017.
Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey.
Journal of Machine Learning Research, 10(Jul):1633-1685, 2009.
Suzanne C Thompson. Illusions of control: How we overestimate our personal influence. Current
Directions in Psychological Science, 8(6):187-190, 1999.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.
Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of
maximum causal entropy. 2010.
10
Under review as a conference paper at ICLR 2021
Appendices
A Details for Gridworld
In this section, we investigate the effects of irrationality on inference in an MDP based on OpenAI
Gym’s ‘Frozen-Lake-v0’ (Brockman et al., 2016). This a small 5x5 gridworld (Fig. 5), consisting of
three types of cells: ice, holes, and rewards. The demonstrator can start in any ice cell. At each ice
cell, the demonstrator can move in one of the four cardinal directions. With probability 0.8, they will
go in that direction. With probability 0.2, they will instead go in one of the two adjacent directions.
Holes and rewards are terminal states, and return the demonstrator back to their start state. They
receive a penalty of -10 for falling into a hole and θi ∈ [0,4] (∣Θ∣ = 25) for entering into the ith
reward cell.
Fig. 6 shows the analogue of the results in section 3.2 for this gridworld: they are reassuringly
similar. We use the same experimental design as the random MDPs in section 3.1.
To visualize why inference quality is improved, we show examples of how the policy of several
irrational demonstrators differ on the gridworld when the rational demonstrator’s policies are identical
in Fig. 7, Fig. 8, and Fig. 9.
Finally, Fig. 10 example of why using the wrong model for reward inference leads to bad inference.
In it, the reward inference algorithm assumes that the demonstrator in Boltzmann when it is actually
Myopic. The Boltzmann rational agent would take this trajectory only if the reward at the bottom
was not much less than the reward at the top. The myopic agent with n ≤ 4, however, only "sees"
the reward at the bottom. Consequently, inferring the preferences of the myopic agent as if it were
Boltzmann leads to poor performance in this case.
B Details for Autonomous Driving Environment
Even if known irrationality helps reward inference both empirically and theoretically in small MDPs,
the question still remains whether this result will matter in practice. As a result, we investigate the
effect of demonstrator irrationality on reward inference in the self-driving car domain (Sadigh et al.,
2016).
Simulation environment. As in previous work in the car domain (Sadigh et al., 2016; 2017), we
model the dynamics of cars using a point-mass model. The state of each car is a 4-dimensional vector
s = [x y h v], where x, y are the coordinates of the car, h is the heading, and v is the speed. The
control input for the car is a two dimensional vector a = [u1 u2], where u1 is the steering input and
Figure 5: The gridworld used in section 4.1.
11
Under review as a conference paper at ICLR 2021
Boltzmann
ιo1
IO3
IO1
IO1
10-,
10-,
IO0
n
IO0
C
-IO1 -IO0 0 IO0 IO1
ω
Extremal
Myopic h
Hyperbolic
Myopic 7
Figure 6: The analog of Fig. 3 for the gridworld. Error bars are the standard error of the mean. The
findings are surprisingly similar as with the random MDPs. Note the more limited x-axis ranges we
used in this experiment.
the states shown: the rational policy is to go away from the hole regardless of θ, but an optimistic
demonstrator takes the chance and goes for the larger reward - up in the first case, down in the second.
u2 is the acceleration. We also include a friction coefficient α. The dynamics model of the vehicle is:
*
[X y hi)] = [v ∙ cosh v ∙ Sinh v ∙ uι u — α ∙ v].
For ease of simulation, we discretized the simulation along the time dimension using the following
dynamics:
∆st = [∆xt ∆yt ∆ht ∆vt] = [it ∙ cos h ∙ dt Vt ∙ sin h ∙ dt Vt ∙ uι ∙ dt (u2 — α ∙ Vt) ∙ dt],
where ∆st = st+1 — St and Vt = Vt + 0.5 ∙ u2 ∙ dt. In our experiments, we used dt = 0.1s.
Reward functions in this environment are assumed to be a linear combination of features:
rθ(s, a) = θ>f (s, a), f : S × A → Rn
Due to environment complexity, we can’t solve for optimal trajectories in our environment directly.
Instead, we suppose that the planner of is performing Model Predictive Control (MPC) at every
12
Under review as a conference paper at ICLR 2021
while the rational expert always detours around the hole and attempts to reach the larger reward,
Figure 9: Boltzmann (β = 100) produces different policies for θ* = (1,1) vs. θ* = (4,4): When
∣∣θ∣∣ is larger, the policy becomes closer to that of the rational demonstrator, as the differences in
Q-values becomes larger.
rational agent Would take this trajectory only if the reWard at the bottom Was not much less than the
reWard at the top. The myopic agent With n ≤ 4, hoWever, only "sees" the reWard at the bottom.
Consequently, inferring the preferences of the myopic agent as if it Were Boltzmann leads to poor
performance in this case.
13
Under review as a conference paper at ICLR 2021
iteration - that is, it will plan a finite horizon sequence of actions to maximize its reward, execute
the first action in the sequence, then replan. As a basic model of irrationality, we consider shorter
planning horizons. We assume that the reward inference procedure knows the planninng horizon
exactly.
Bayesian IRL in the car environment First, we consider the analogue of the results in section 3.2
in the car environment: what is the log-loss of the posterior on θ, given different planning horizons?
Since the demonstrations used in this work are generated via trajectory optimization (and not an
inherently stochastic process), using Bayesian IRL requires us to specify a “fake" observation model
P(ξ∣θ).
We use the following distribution, normal in feature space, for P(ξ∣θ):
D"∣Q∖	(E[f(ξ)]-E[f(ξ%])T ∑-*1(E[f(ξ)]-E[f(ξ%])
P(ξ∣θ) H e	ξθ	,
where E[f (ξ^] is the expectation of the features of the optimal trajectory ξ^ for r.
Maximum likelihood IRL in cars In practice, full Bayesian inference is completely intractable for
complicated domains such as cars. Instead, the state of the art for reward inference are approximate,
maximum-likelihood estimate (MLE) based methods.To study the effects of irrationality on MLE-
based inference methods, we also perform Levine and Koltun’s Continuous IOC with Locally Optimal
Examples (CIOC) (Levine and Koltun, 2012), which uses a Boltzmann model of the demonstrator
with a second order Laplace approximation of the normalizing constant. Since our demonstrators
are actually locally optimal, we rectify the bias induced by the Boltzmann model by using a large
β = 104. To rectify the issue of local optimization in CIOC, we initialize the optimization procedure
with the true reward weights.
As CIOC returns an (approximate) MLE estimate for the reward parameter θ, we cannot use the
ʌ
log-loss metric. Instead, we evaluate the reward functions by the cosine similarity of θ* and θ, as
ʌ
well as the normalized return of the trajectory when optimizing the recovered θ. (We normalize the
returns so that the optimal trajectory has return 1 and the trajectory that goes forward at constant
speed 0.)
Driving Scenario: Merging Our experiments were performed in a simple merging environment (Fig.
1). In it, the demonstrator wants to merge into the right lane while trying to maintain its 1.2 forward
speed. In addition to the demonstrator car, the right lane contains two constant velocity cars, traveling
at 0.8 speed. The features of this environment are composed of a squared penalty for deviating from
1.2 forward speed, features for the squared distances to the medians of each of the lanes, a feature for
the minimum squared distance to any of the medians of the lanes, and a smooth collision feature.
For the Bayesian IRL scenario, we considered four different reward functions on this domain,
consisting of varying the weight on the penalty for deviating from the target speed. All the other
weights are unchanged. In particular, we considered θspeed ∈ {0.5, 1, 2, 4}. In the CIOC scenario,
we used θspeed = 1. We also consider 3 different planning horizons: h = 3, h = 5, and h = 7. This
means we had 12 conditions for Bayesian IRL and 3 for CIOC.
C	Proofs for section 4.3
First, note that for deterministic planners, there that there are |A||S| such policies, and thus we have
I(θ; d(θ)) ≤ H (d(θ)) ≤ |S| log |A| (as the entropy of a discrete random variable X is bounded
above by the logarithm of the size of its support |X|). Similarly, we also have that I(θ; d(θ)) ≤
H(θ) ≤ log ∣Θ∣. For deterministic planners, we thus have
I(θ; d(θ)) ≤ min(log ∣Θ∣, |S| log |A|),
We will now prove a stronger version of proposition 3.
Proposition 3. There exists a family of URMDPs with state and action spaces of any size, such that
the rational planner provides no information, and there exists a deterministic planner that provides
minQog ∣Θ∣, |S | log |A|) bits worth of information.
14
Under review as a conference paper at ICLR 2021
Proof. Consider a set of environments where Θ = {1, 2, ..., |A||S|}, the prior p is uniform over Θ
(and thus H(θ) = log ∣θ∣), and where
rθ(s,a,s0)= θ0
a = a*
a = a*
for some a*. Then the unique optimal policy has π(s) = a* for every θ (as θ > 0). This implies that
I(θ, dRational (θ)) ≤ H(dRational(θ)) = 0.
However, the planner d0 : θ 7→ π(θ), with some fixed some ordering {π(i)}i∈{1,2,...,|A||S|} of the
possible policies, satisfies H(θ∣d0(θ)) = 0 and thus I(θ; d0(θ)) = H(θ) = log ∣Θ∣ = |S| log |A|. □
By choosing A such that |A| ≥ ∣θ∣1/|S|, We get the Proposition 3 in the text.
Proposition 4. There exists a family ofone-state two-action MDPs, with arbitrarily large ∣Θ∣ such
that I(θ,dBoltz (θ)) = log ∣Θ∣ and I(θ,dRational(θ)) = 0∙
Proof. Suppose first action aι has reward θ, where θ ∈ {1, 2,…,一∣θ∣}, and the second action a2
has reward 0. Then the rational planner dRational will always return a policy that always takes the first
action, while dBoltz (θ) is the policy
eβ(θ+V (s1))	1
πθ (a1|sI)= eβ(θ+v (sι)) + eβv (si) = 1 + e-βθ
π (a2|sI) = ι+e>
This mapping is injective, and so H(θ∣⅛oltz(θ)) = 0 and thus I(θ; ⅛oltz(θ)) = H(θ) = log ∣Θ∣. □
D More theory
Why recover the reward parameter? Even if irrationalities can help with inference, a natural
question why we wish to infer the reward. If the actual goal is acting optimally, a rational demonstrator
would be great because we can just imitate them. In addition to the fact that real people are not
perfectly rational, the reason we focus on inference is that imitation is not enough in some cases, e.g.
if we need to behave in new environments (Taylor and Stone, 2009; Devin et al., 2017; Jing et al.,
2019), or transfer the policy to a robot with different dynamics (Cully et al., 2015; Fu et al., 2018;
Reddy et al., 2018).
In fact, there are cases where being robust to changes in dynamics requires identifying the reward
parameters. Indeed, there exist such cases where it is impossible to identify the parameters from
rational demonstrators, but possible from Boltzmann demonstrators:
Proposition 5. There exists an URMDP M and a set of new transition probabilities
{{Ps(i()}}i∈{i,2,...,∣θ∣} such that log ∣Θ∣ bits Ofinormation are needed to compute the optimal policy
under all the transition probabilities; and where I(θ, dRational (θ)) = 0, whereas I(θ, dBoltz(θ)) =
log ∣Θ∣.
Proof. We construct a 2 state, 2 action URMDP. Label the two states s1 and s2 . Let Θ =
{1,2,..., ∣θ∣}, the prior be uniform over Θ, and let r(s, a, si) = θ for all s, a and r(s, aι, s2) = 0,
r(s, a2, s2) = -1 for all s.
Under the original transition probabilities {Ps,a}, a1 leads deterministically to s1 while a2 deter-
ministically leads to s2. Then, the unique optimal policy is π(s) = a* for every θ (as θ > 0). This
implies that I(θ, dRational(θ)) = 0. On the other hand, dBoltz is an injective map (as in Proposition 2),
and thus I(θ; ⅛oltz(θ)) = log ∣Θ∣.
Now, we construct {Ps(,ia)} such that a1 is optimal for θ ≤ i - 1, and a2 is optimal for θ ≥ i. First,
let a1 deterministically lead to a2, thus, leading to reward 0. Next, let a2 lead to s1 with probability
2i+1. Therefore:
0	2	2i - 1
Es0〜Ps%[rθ(S,a2,s )] = 2i+1 θ - 2i+ι
15
Under review as a conference paper at ICLR 2021
Then for θ ≥ i, we have E§0〜P⑸[rθ(s, a2, s0)] ≥ 2+ > 0 and for θ ≤ i - 1, We have
Es0^P(ia2 [rθ (s, a2, s0)] ≤ - 2i+1 < 0.
By construction, in order to compute the optimal policy for every new transition probabilities {Ps(,ia) },
We must know the value of θ, and thus need log ∣Θ | bits of information.	□
E Additional Misspecification Results
How well do we have to know the planners’ parameters to outperform Boltzmann-rationality?
A natural question to ask after the results of section 5 is how exactly we need to know the bias before
we can outperform the Boltzmann-rational assumption. To explore this, we compare inference with
the true planner model, but with (possibly) the wrong parameter in Fig. 11. In order to deal with
the fact that deterministic planners can assign probability 0 to a trajectory, we smoothed the policies
by applying a softmax to the planners’ Q-values with β = 10.0. Surprisingly, we find that for many
demonstrator irrationalities, a wide variety of assumed parameter settings still allowed the learner to
outperform inference under the Boltzmann-rational assumption. In fact, for several irrationalities, the
learner is always better off with the correct type, regardless of what planner parameter they assume.
This suggests that we don’t need to model human irrationalities perfectly to improve on existing
reward inference.
How well do we have to know the rationality type to outperform Boltzmann-rationality?
Another natural question is whether we need to know the exact type of the planner. For example,
while many pieces of evidence point toward people having some form of myopia in decision making,
the exact form of the myopia may not be clear. To investigate this quesiton, we compare the learner’s
performance when the demonstrator assumes the wrong form of Myopia in Fig. 12. As before,
we smoothed the policies by applying a softmax to the planners’ Q-values with β = 10.0. We find
that even though the learner gets the type of the irrationality wrong, the learner still does better
than assuming Boltzmann-rationality. Again, this suggests that we don’t need to model human
irrationalities perfectly to improve on existing reward inference.
16
Under review as a conference paper at ICLR 2021
Boltzmann
Optimism	Extremal
QQOo
8 6 4 2
ssoou
Illusion
0.25	0.50	0.75	1.00
ɑ
Hyperbolic
5 0 5 0
2 2 11
SSoqou
100
k
102
---- Learner assumes Boltzmann -------- Learner assumes Illusion -------- Learner assumes Optimism -------- Learner assumes Extremal
----------------------------------------------------------------------------------------------------------- Learner assumes Prospect - Learner assumes Myopic Y - Learner assumes Myopic h - Learner assumes Hyperbolic
Figure 11: The log loss (lower = better) of various models under parameter misspecification. Each
x-axis shows the parameter that the learner assumes. The orange line represents the performance
when the learner makes the faulty assumption that the demonstrator is Boltzmann-rational. In many
cases, the learners perform better than by assuming Boltzmann-rational just by getting the type of the
planner correct, even if they don’t get the exact parameter correct. The error bars show the standard
error of the mean, calculated by the bootstrap across environments.
---- Learner assumes Boltzmann ----- Learner assumes Myopic Y ------ Learner assumes Myopic h
Figure 12: The log loss (lower = better) of two myopic demonstrators under type misspecification.
On the left, the demonstrator performs myopic value iteration (Myopic h), but the learner assumes
the demonstrator has a myopic discount rate γ (Myopic γ). On the right, the demonstrator has a
myopic discount rate γ but the learner assumes myopic value iteration. However, in both cases, this
leads to better inference than assuming Boltzmann-rationality. The error bars show the standard error
of the mean, calculated by the bootstrap across environments.
17