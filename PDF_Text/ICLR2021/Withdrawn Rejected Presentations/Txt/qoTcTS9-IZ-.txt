Under review as a conference paper at ICLR 2021
Dynamically Stable Infinite-Width Limits of
Neural Classifiers
Anonymous authors
Paper under double-blind review
Ab stract
Recent research has been focused on two different approaches to studying neural
networks training in the limit of infinite width (1) a mean-field (MF) and (2) a
constant neural tangent kernel (NTK) approximations. These two approaches have
different scaling of hyperparameters with the width of a network layer and as a
result, different infinite-width limit models. Restricting ourselves to single hidden
layer nets with zero-mean initialization trained for binary classification with SGD,
we propose a general framework to study how the limit behavior of neural models
depends on the scaling of hyperparameters with network width. Our framework
allows us to derive scaling for existing MF and NTK limits, as well as an uncount-
able number of other scalings that lead to a dynamically stable limit behavior of
corresponding models. However, only a finite number of distinct limit models
are induced by these scalings. Each distinct limit model corresponds to a unique
combination of such properties as boundedness of logits and tangent kernels at
initialization or stationarity of tangent kernels. Existing MF and NTK limit models,
as well as one novel limit model, satisfy most of the properties demonstrated by
finite-width models. We also propose a novel initialization-corrected mean-field
limit that satisfies all properties noted above, and its corresponding model is a
simple modification for a finite-width model.
1	Introduction
For a couple of decades neural networks have proved to be useful in a variety of applications. However,
their theoretical understanding is still lacking. Several recent works have tried to simplify the object of
study by approximating a training dynamics of a finite-width neural network with its limit counterpart
in the limit of a large number of hidden units; we refer it as an "infinite-width" limit. The exact type
of the limit training dynamics depends on how hyperparameters of the training dynamics scale with
width. In particular, two different types of limit models have been already extensively discussed in
the literature: an NTK model (Jacot et al., 2018) and a mean-field limit model (Mei et al., 2018; 2019;
Rotskoff & Vanden-Eijnden, 2019; Sirignano & Spiliopoulos, 2020; Chizat & Bach, 2018; Yarotsky,
2018). A recent work (Golikov, 2020) attempted to provide a link between these two different types
of limit models by building a framework for choosing a scaling of hyperparameters that lead to a
"well-defined" limit model. Our work is the next step in this direction. We study infinite-width limits
for networks with a single hidden layer trained to minimize cross-entropy loss with gradient descent.
Our contributions are following.
1.	We develop a framework for reasoning about scaling of hyperparameters, which allows one
to infer scaling parameters that allow for a dynamically stable model evolution in the limit
of infinite width. This framework allows us to derive both mean-field and NTK limits that
have been extensively studied in the literature, as well as the "intermediate limit" introduced
in Golikov (2020).
2.	Our framework demonstrates that there are only 13 distinct stable model evolution equations
in the limit of infinite width that can be induced by scaling hyperparameters of a finite-width
model. Each distinct limit model corresponds to a region (two-, one-, or zero-dimensional)
of a green band of the Figure 1, left.
3.	We consider a list of properties that are statisfied by the evolution of finite-width models,
but not generally are for its infinite-width limits. We demonstrate that mean-field and NTK
1
Under review as a conference paper at ICLR 2021
sym-default
finite logits
at initialization
finite tangent kernels
at initialization
-(XH-04-Γ- 60- Sqe U 笛① E
NTK
logits and kernels
are of the same order
at initialization
evolving
kernels
training step, k+1
=1024r batch size
IO0	IO1	IO2
training step, k+1
1	qσ
Figure 1: A diagram on the left specifies several properties demonstrated by finite-width mod-
els. As plots on the right demonstrate, our novel IC-MF limit model satisfy all of these prop-
erties, while MF and NTK limit models, as well as sym-default limit model presented in the
paper violate some of them. Left: A band of scaling exponents (qσ, q) that lead to dynamically
stable model evolutions in the limit of infinite width, as well as dashed lines of special properties that
corresponding limits satisfy. Three colored points correspond to limit models that satisfy most of
these properties. Right: Training dynamics of three models that correspond to color points on the left
plot, as well as of initialization-corrected mean-field model (IC-MF), which does not correspond to
any point of the left plot. These models are results of scaling of a reference model of width d = 27
(black line) up to width d = 216 (colored lines). Solid lines correspond to the test set, while dashed
lines are for the train set. Note that we have added a small vertical displacement to all curves in order
to make them visually distinguishable. See Appendix F for details.
limit models, as well as "sym-default" limit model which was not discussed in the literature
previously, are special in the sense that they satisfy most of these properties among all limit
models induced by hyperparameter scalings. We propose a model modification that allows
for all of these properties in the limit of infinite width and call the corresponding limit
"initialization-corrected mean-field limit (IC-MF)".
4.	We discuss the ability of limit models to approximate the training dynamics of finite-width
ones. We show that our proposed IC-MF limiting model is the best among all other possible
limit models.
While our present analysis is restricted to networks with a single hidden layer, we discuss a high-level
plan for generalizing it to deep nets, as well as an expected outcome of this research program, in
App. H.
2	Training a one hidden layer net with SGD
Here we consider training a one hidden layer net fd with d hidden units with SGD. We assume
the hyperparameters, namely, initialization variances and learning rates, are scaled as power-laws
of d. Each scaling induces a limit model f∞ = limd→∞ fd. We present a notion of dynamical
stability, which states that the change of logits after a single gradient step is comparable to logits
themselves. We derive a necessary condition for dynamical stability in terms of the power-law
exponents of hyperparameters. We then present a list of conditions that divide the class of scalings
into 13 subclasses; each subclass corresponds to a unique distinct limit model.
Consider a one hidden layer network:
d
f(x;a,W) = aTφ(WTx) = X arφ(wrTx),	(1)
r=1
where x ∈ Rdx, W = [w1, . . . , wd] ∈ Rdx×d, and a = [a1, . . . , ad]T ∈ Rd. We assume a
nonlinearity to be real analytic and asymptotically linear: φ(z) = Θz→∞ (z). Such a nonlinearity
2
Under review as a conference paper at ICLR 2021
can be, e.g. "leaky softplus": φ(z) = ln(1 + ez) - α ln(1 + e-z) for α > 0. This is a technical
assumption introduced to simplify proofs. Note that we have used traditional leaky ReLUs in our
experiments: see App. F for details. We assume the loss function '(y, Z) to be the standard binary
cross-entropy loss: '(y, Z) = ln(1 + e-yz), where labels y ∈ {-1,1}. The data distribution loss is
defined as L(a, W) = Eχ,y~D'(y, f (x; a, W)). We assume that the data distribution D does not
depend on width d.
Weights are initialized with isotropic gaussians with zero means: WrO)〜 N(0,σw2I), a(r0) ~
N(0, σa2) ∀r = 1 . . . d. The evolution of weights is driven by the stochastic gradient descent (SGD):
∆θ(k) = θ(k+1) - θ(k) = -ηθ d'(y(k),f (Xθk);a,w)), (χθk),yθk))〜d,
(2)
where θ is either a or W . We assume that gradients for a and W are estimated using independent
data samples (x(ak), ya(k)) and (x(wk), yw(k)). While this assumption is indeed non-standard, we note
that corresponding stochastic gradients still give unbiased estimates for true gradients. Moreover,
we have used either full-batch GD or standard mini-batch SGD in our experiments: see App. F for
details. Define:
a(rk)
σa
Then the dynamics transforms to:
(k)
Wrk)=K, ηa=σ2,	ηw=σ2.
(3)
∂'(yθk),f(xθk) ； σ°a, σw W))
∆θ卜=ηθ
_ ʌ
∂θr
while scaled initial conditions become: ^r0) ~ N (0,1), W r0) ~ N (0, I) ∀r = 1 ...d.
By expanding gradients, we get the following:
△ark) = -ηaσavfd)'(Xak),yak))小μWrk),TXak)),	^* ~ n(o, i),
△w4 = -ηwσaσwvfd)'(χW≈),yWk))ɑrk)φ0(...)χW≈),	WrO) ~N(0,I),
(4)
(5)
(6)
Vfd)”,y)= ⅛2∣	)=1+e XPf：2(XM , fdk)(X)= % XX M φ(σw W rk),TX).
z=fd (x)	1 + exp(fd (x)y)	r=1
Without loss of generality assume σw = 1 (we can rescale inputs X otherwise). We shall omit a
subscript of σa from now on. Assume hyperparameters that drive the dynamics obey power-law
dependence on d:
σ(d) =	σ*	×	(d∕d*)qσ,	ηa(d) = ηa	X	(d")°a,	ηw(d)	= ηw	X 口心丸.	⑺
Given this, a network of width d* has hyperparameters σ* and η*∨w. Here and then we write "a ∨ w"
meaning "a or w".
This assumption is quite natural: for He initialization (He et al., 2015) commonly used in practice
σ 8 d-1/2, while we keep learning rates in the original parameterization constant while changing
width by default: η°∨w = const, which implies ^ α d and 力仅 8 d0. On the other hand, NTK
scaling (Jacot et al., 2018; Lee et al., 2019) requires scaled learning rates to be constants: ηa∨w H d0.
Scaling exponents (qσ, ⅞a, ‰) together with proportionality factors (d*,σ*,η*, nW) define a limit
model f∞(k)(X) = limd→∞ fd(k)(X). We call a model "dynamically stable in the limit of large width"
if it satisfies the following condition which we state formally in Appendix A:
Condition 1 (informal versionof Condition 4 in Appendix A). Let △fd(k) (X) = fd(k+1)(X)-fd(k)(X).
△fd(k)
∃kbaiance ∈ N : Nk ≥ kbaiance -k-γ— Staysfinitefor large d.
balance
fd
3
Under review as a conference paper at ICLR 2021
Roughly speaking, this condition states that the change of logits after a single step is comparable to
logits themselves. This means that the model learns.
Note that this condition is weaker than the one used in Golikov (2020), because it allows logits to
vanish or diverge with width. Such situations are fine, because only logit signs matter for the binary
classification.
For simplicity assume q& = Gw = q. We prove the following in Appendix B.1:
Proposition 1. Suppose q& = qw = q and D is a continuous distribution. Then Condition 1 requires
qσ + qG ∈ [-1/2, 0] to hold.
This statement gives a necessary condition for growth rates of σ and η to lead to a well-defined limit
model evolution. This condition corresponds to a band in (qσ, qq)-plane: see Figure 1, left. We refer it
as a "band of dynamical stability".
Each point of this band corresponds to a dynamically stable limit model evolution. We present several
conditions that separate the dynamical stability band into regions. We then show that each region
corresponds to a single limit model evolution.
We start with defining tangent kernels. Since φ is smooth, we have:
∆fdkk(χ) = fdk+11(χ) - fdk(χ) = XX dfd^χ)∣	△呼)+o琮∨w→o(η 花 w+ηw,2)=
r=ι	dθr ∣θr=θrk)
=-ηavfd)'(χafc),yafc)) Kakd(x,Xak))-ηwvfd)'(xw≈),y川 Kwkd(x,xw≈))+ o(η粉w + ηw,2),
(8)
where we have defined kernels:
d
Kakd(x, x0) = (d∕d*)°aσ2 X Φ(W行Tx)φ(W汐Tx0),	(9)
r=1
d
Kwd(X, x0) = (d∕d*)qwσ2 X ∣^rk)∣2φ0(Wrk),τx)φ0(W步①XO)XTx0.	(10)
r=1
Here we deviate from the traditional definition of tangent kernels (e.g. from Jacot et al. (2018)) in
embedding learning rate growth factors into kernels. This is done for avoiding 0 × ∞ ambiguity
when f^a∨w grows with d while σ vanishes so that “a learning rate times a kernel" stays finite. This is
the case for the mean-field scaling: ι^a∨w α d, while σ 8 d-1.
While for the NTK scaling kernels stop evolving with k in the limit of large d, this is not the case
generally. Indeed, for the mean-field scaling mentioned above we have:
d
Kakd(x, x0) = σ*,2(d∕d*)T X φ(wrk),τx)φ(wrk),τx0).	(11)
r=1
Similarly to the NTK case, the kernel above converges due to the Law of Large Numbers, however in
contrast to the NTK case the weights evolve in the limit: Wfk) 9 W£0). This is due to the fact that
weight increments are proportional to ηwσ which is H d0 for the mean-field scaling but H d-1/2 for
the NTK one. For this reason, similarly to model increments △fd(k) we define kernel increments:
△Kat)w,d (x, x0)=Ka∨w,d (x, x0) - Ka∨)w,d(x, x0).	(12)
Condition 2 (informal version of Condition 5 in Appendix A). Following conditions separate the
band of dynamical stability (Figure 1, left):
1.	fd(0) stays finite for large d.
2.	Ka(0∨)w,d stays finite for large d.
3.	Ka(0∨)w,d∕fd(0) stays finite for large d.
4
Under review as a conference paper at ICLR 2021
4	NKW∨w,d stays finite for large d.
We prove the following in Appendix B.2:
Proposition 2 (Separating conditions). Given Condition 1, Condition 2 reads as, point by point:
1.	A limit model at initialization is finite: qσ + 1/2 = 0.
2.	Tangent kernels at initialization are finite: 2qσ + q+1 = 0.
3.	Tangent kernels and a limit model are of the same order at initialization: qσ + ¢+1/2 = 0.
4.	Tangent kernels start to evolve: qσ + q = 0.
We have also checked this Proposition numerically for limit models discussed below: see Figure 1,
right. Each condition corresponds to a straight line in the (q»,办plane: see Figure 1, left. These four
lines divide the well-definiteness band into 13 regions: three are two-dimensional, seven are one-
dimensional, and three are zero-dimensional. In Appendix C we show that each region corresponds
to a single distinct limit model evolution; we also list corresponding evolution equations. Note that a
segment (a one-dimensional region) that corresponds to the Condition 2-2 exactly coincides with a
family of "intermediate scalings" introduced in Golikov (2020).
3 Capturing the behavior of finite-width nets
A possible use-case for a limit model is being a proxy for a given finite-width net, useful for theoretical
considerations. For example, a number of theoretical properties, including convergence to a global
minimum and generalization, are already proven for nets near the NTK limit: see Arora et al. (2019b).
Note that a typical finite-width model satisfies all four statements of Condition 2 (if we exclude
the word "limit" from them). Indeed, neural nets are typically initialized with He initialization (He
et al., 2015) that guarantees finite fd(0) even for large width d. Since learning rates of finite nets
are finite, the tangent kernels are finite as well. Nevertheless, a neural tangent kernel of a typical
finite-width network evolves significantly: Arora et al. (2019a) have shown that freezing NTK of
practical convolutional nets sufficiently reduces their generalization ability; Woodworth et al. (2019)
also noticed that evolution of NTK is sufficient for good performance.
Consequently, if we want a limit model to capture the dynamics of a finite-width net, we have to
satisfy all four statements of Condition 2. However, as one can see from Figure 1, we cannot satisfy
all of them simultaneously. We say that one limit model captures the behavior of a finite-width one
better than the other, if all statements of Conditions 2 satisfied by the latter are satisfied by the former
too. If we say in this case that "the former dominates the latter" then one can easily notice that there
are only three "non-dominated" limit models which we discuss in the upcoming section. After that,
we introduce a model modification that allows for a limit satisfying all four statements.
3.1	"Non-dominated" limit models: MF, NTK and "sym-default"
Obviously, the three "non-dominated" limit models are exactly three zero-dimensional regions
(points) in Figure 1, left. First suppose statements 1, 2 and 3 hold, hence tangent kernels are constant
throughout training (see Figure 1, right). A corresponding point q0 = -1/2, q = 0 reads as
σ 8 d-1/2 and η = const, which is the case considered in the seminal paper on NTK (JaCOt et al.,
2018). The limit dynamics is then given as (see App. C.1.1 and App. C for the general derivation,
and eqs. (71-75) for a complete system of evolution equations):
fntk+1) (X)=ftk,∞ (χ)-ηR2'(χ"yak)) κao∞ (χ, Xak))-ηw Vfktk '(您,曲)κw0,∞(χ, χ(wħ,
ftk,∞(x) ~N(0,σ*,2σ⑼,2(x)),	(13)
where (xi∨w, ya∨w)〜 D and limit tangent kernels Ka(0∨)w,∞ and standard deviations at the initializa-
tion σ(0) (x) can be calculated along the same lines as in Lee et al. (2019).
5
Under review as a conference paper at ICLR 2021
Next, suppose statements 2 and 4 hold. In this case K∞(k) does not coincide with K∞(0) (see Figure
1, right), hence the dynamics analogous to (13) is not closed. However, the limit dynamics can be
expressed as an evolution of a weight-space measure (see Rotskoff & Vanden-Eijnden (2019); Chizat
& Bach (2018) for a similar dynamics for the gradient flow, App. C.2.1 and App. C for the general
derivation, and eqs. (94-96) for a complete system of evolution equations):
μ∞+1) = μ∞) +div(μ∞)∆θmf)),	μ∞) = N(0,Iι+dχ),	(14)
(X) = σ* J aφ(WTx) μ∞)(d^,dW),
(k)
where the vector field ∆θmf is defined as follows:
∆θ;(f)(a, W) = -vf? '(χ 步北八 φ(w T Xak))
x(k))x(k),T]T
xw xw	,
(15)
(16)
where we write "[u, v]" meaning a concatenation of two row vectors u and v. Here we have qσ = -1,
q= 1, hence σ 8 d-1 and η 8 d; this hyperparameter scaling were used in Rotskoff & Vanden-
Eijnden (2019); Chizat & Bach (2018). Note that since a measure at the initialization μ∞) has a
zero mean, a limit model vanishes at the initialization fm(0f,)∞ = 0 (see Figure 1, right) thus violating
statements 1 and 3 of Condition 2.
Finally, consider a point for which statements 1 and 4 hold: qσ = -1/2, ¢=1/2. This situation is
very similar to what we call "default" scaling. Consider He initialization (He et al., 2015), typically
used in practice: o& H d-1/2 and σw H dχ1/2. Assume learning rates (in original parameterization)
are not modified with width: ηo = const and ηw = const. This implies ^ H d and n仅 H 1, or
Ga = 1 and ‰ = 0. We refer the scaling qσ = -1/2, q& = 1 and ‰ = 0 as "default", and the
scaling qσ = -1/2, q = 1/2 as "sym-default". A limit model evolution for the sym-default scaling
looks as follows (see App. C.2.2 for an equivalent formulation and App. C for the general derivation,
and eqs. (97-101) for a complete system of evolution equations):
μ∞+1) = μ∞ + div(μ∞)∆θ(km-def), μ∞) = N(o,iι+dχ),	(17)
fs(0ι-def,∞(x) ~N(0,σ*,2σ(0),2 (x)),	z(k)-def,∞ (X)=Iyaφ(wT x) μ∞)(da,dw) > 0 , (18)
(k)
where the vector field ∆θsym-def is defined similarly to the MF case (16):
△喘-def(a, W) = -[vfkLef `(Xa) ,yk))φ(W T Xak)), Vfkm-def `(Xwk,∙yWk~)aφ (W T Xwio)X 俨τ]τ,
VfkLef'(x, y) = -y[yzskι-def,∞(X) <0] for k ≥ 1∙	(19)
As we show in Appendix D, the default scaling leads to an almost similar limit dynam-
ics as the sym-default scaling: eqs. (114-117) for a complete system of the corresponding
evolution equations. The quantity zs(ykm)-def,∞ should be perceived as a sign of fs(ykm)-def,∞ =
σ* limd→∞ ddσ++1 R ^φ(WTx) μdk)(da, dW)). The reason why we have to switch from logits
to their signs is that the limit model diverges for k ≥ 1: limd→∞ fd(k) (X) = ∞. Nevertheless the
gradient of the cross-entropy loss is well-defined even for infinite logits: it just degenerates into the
gradient of a hinge-type loss: limf→+∞×z d'ff)= -y[yz < 0]. For this reason, we redefine the
loss gradient for k ≥ 1 in terms of logit signs: eq. (19). Note that besides of the fact that logits
diverge in the limit of large width, the measure in the parameter space μdk) stays well-defined.
3.2	Initialization-corrected mean-field (IC-MF) limit
Here we propose a dynamics that satisfy all four statements of Condition 2. We then show how to
modify the network training for the finite width in order to ensure that in the limit of the infinite width
its training dynamics converge to the proposed limit one. Consider the following:
μ∞+1) = μ∞ + div(μ∞)∆θ(ckJf),	μ∞) = N (0,Iι+dχ),	(20)
6
Under review as a conference paper at ICLR 2021
fiCRf,∞(X) = σ* / aφ(w TX) μ∞k(da, d 旬 + ftk∣∞(X),
where fn(t0k),∞ is defined similarly to above:
fntκ(X)〜N(O,σ*,2σ⑼,2(X)),
(21)
(22)
the vector field ∆θi(ckm)f is defined analogously to the mean-field case:
∆θ(Rf(a,W) = -[vf2`(Xak),wk)φ(wTXak)),vf2。思3曲⑼Tx&x俨T]T,(23)
See App. E and eqs. (131-134) for a complete system of evolution equations. The only difference
between this dynamics and the mean-field dynamics is a bias term fn(t0k),∞ in the definition of logits.
This bias term does not depend on k and stays finite for large d in contrast to fm(0f,)∞ which vanishes
for large d; it ensures Condition 2-1 to hold. As for Condition 2-4, tangent kernels evolve with k
(k)
SimPly because the measure μ∞ evolves With k similarly to the mean-field case (see Figure 1, right).
Indeed,
Kwk∞ (X0, χ)=σ*,2d* 八""(k),T …(k),Tχ0) μ∞)(da,dw),	(24)
and the limit of Ka(k,d) is Written in a similar Way. Kernels at initialization Ka(0∨)w,∞ are finite due to
the LaW of Large Numbers (Condition 2-2); this, and the finiteness of fn(t0k) ensures Condition 2-3.
As We shoW in APPendix E the dynamics (20) is a limit for the GD dynamics of the folloWing model
with learning rates ηa∨w = η*∨w(d/d*)1:
dd
ficmf,d(x; a,W) = σ*(d∕d*)-1 X arφ(wTX)+ σ*((d∕d*)-"-(d∕d*)T) X ^*φ(Wr0)，Tx).
r=1	r=1
*	(25)
Note that ficmf,d* (x) = σ* Pd=ι ^rφ(WTx): we have not altered the model definition at d = d*.
3.3	Experiments
Consider a network of width d* initialized with a standard deviation σ* and trained with learning
rates η*∨w. We call this model a "reference". Consider a family of models indexed by a width d
with hyPerParameters sPecified by the Power-law scaling (7). We train a reference network of width
d* = 128 for the binary classification with a cross-entroPy loss on the CIFAR2 dataset (a subset of
first two classes of CIFAR10). We track the divergence of a limit network from the reference one
using the following quantity: EX〜DtestDlogits(f∞k)(x) || fdk)(x)), where
Diogits(ξ || ξ*) = KL(N(Eξ, Varξ) || N(Eξ*, Varξ*)).	(26)
We have also tried other divergence measures; see APPendix I.
Results are shown in Figure 2. The NTK limit tracks the reference network well only for the first
20 training stePs; a similar observation has been already made by Lee et al. (2019). At the same
time, the mean-field limit starts with a high divergence (since the initial limit model is zero in this
case), however, after the 80-th steP, it becomes smaller than that of the NTK limit. This can be
the imPlication of non-stationary kernels. As for the default case, divergence of logits results in a
blow-uP of the KL-divergence.
The best overall case is the ProPosed IC-MF limit, which retains the small KL-divergence related to
the reference model throughout the training Process. CaPturing the behavior of finite-width nets is
also Possible by introducing finite-width corrections for the NTK (Dyer & Gur-Ari, 2019; Huang &
Yau, 2019). However, this gives us an infinite sequence of equations, which is intractable. We have
to cut this sequence; this gives us an aPProximate dynamics, which is still comPlicated. In contrast,
our IC-MF limit is a simPle modification of the MF limit, and at the same time, a good Proxy for
finite-width networks.
7
Under review as a conference paper at ICLR 2021
Figure 2: Initialization-corrected mean-field (IC-MF) limit captures the behavior of a given
finite-width network best among other limit models. We plot a KL-divergence of logits of different
infinite-width limits of a fixed finite-width reference model relative to logits of this reference model.
Setup: we train a one hidden layer network with SGD on CIFAR2 dataset; see Appendix F for details.
KL-divergences are estimated using gaussian fits with 10 samples.
4	Related work
A pioneering work of Jacot et al. (2018) have shown that a gradient descent training of a neural net
can be viewed as a kernel gradient descent in the space of predictors. The corresponding kernel is
called a neural tangent kernel (NTK). Generally, NTK is random and non-stationary, however Jacot
et al. (2018) have shown that in the limit of infinite width it becomes constant given a network is
parameterized appropriately. In this case the evolution of the model is determined by this constant
kernel; see eq. (13). The training regime when NTK is hardly varying is coined as "lazy training",
as opposed to the "rich" training regime, when NTK evolves significantly (Woodworth et al., 2019).
Chizat et al. (2019) noted that the training becomes lazy for a finite width if one scales the output of
the network appropriately. While being theoretically appealing, "laziness" assumption turns out to
have a number of limitations in explaining the success of deep learning (Arora et al., 2019a; Ghorbani
et al., 2019).
Another line of works considers the evolution of weights as an evolution of a weight-space mea-
sure, similar to eq. (14) (Mei et al., 2018; 2019; Sirignano & Spiliopoulos, 2020; Chizat & Bach,
2018; Rotskoff & Vanden-Eijnden, 2019; Yarotsky, 2018). This weight-space measure becomes
deterministic in the limit of infinite width, given the network is parameterized appropriately; the
corresponding limit dynamics is called "mean-field". Note that the parameterization required here for
the convergence to a limit dynamics differs from the one used in the NTK literature.
Our framework for reasoning about scaling of hyperparameters is similar in spirit to the one used
in Golikov (2020). However, there are several crucial differences. First, we do not consider weight
increments, as well as a model decomposition, and do not try to estimate exponents of the former and
for terms of the latter, which arguebly complicates the work of Golikov (2020). Instead, we present
derivations in terms of the limit behavior of logits and kernels which appears to be simpler and clearer.
Second, our criterion of "dynamical stability" of scaling is weaker compared to the one of Golikov
(2020) and more suitable for classification problems, since it allows for diverging or vanishing
logits, as long as they give meaningful classification responses. In particular, our dynamical stability
condition covers practically important "default" limit for which learning rates are kept constant while
width grow up to infinity. Note that "intermediate limits" investigated in Golikov (2020) exactly
correspond to limit models which satisfy Condition 2-2. Moreover, both "sym-default" and IC-MF
limit models we propose in the present work have not been discussed previously; we present limit
evolution equations for both of them (see Appendix C). Finally, our analysis suggests that there are
only 13 distinct limit models that can be induced by power-law scaling of hyperparameters.
5	Conclusions
The current work follows a direction started in Golikov (2020): we study how one should scale
hyperparameters of a neural network with a single hidden layer in order to converge to a "dynamically
8
Under review as a conference paper at ICLR 2021
stable" limit training dynamics. A weaker dynamical stability condition leads us to a richer class
of possible limit models as compared to Golikov (2020). In particular, the class of limit models we
consider includes a "default" limit model that corresponds to a network with infinitely large number
of nodes and finite learning rates in the original parameterization. This "default" limit model does not
satisfy a "well-definiteness" condition of Golikov (2020).
Moreover, we show that the class of limit models that can be achieved by scaling hyperparameters of
finite-width nets is finite. The space of hyperparameter scalings is divided by regions with certain
conditions on the training dynamics, and each region corresponds to a single limit model. All of
these conditions are satisfied by finite-width networks, but cannot be satisfied by limit models all
simultaneously. We propose a modification of a finite-width model; the limit of this modification
corresponds to a limit model that satisfy all of the conditions mentioned above and tracks the dynamics
of a "reference" finite-width net better than other limit models.
References
Dyego Araujo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep neural
networks. arXiv preprint arXiv:1906.00193, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems,pp. 8139-8148, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584, 2019b.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. In Advances in neural information processing systems, pp. 3036-
3046, 2018.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2933-2943, 2019.
Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. arXiv preprint
arXiv:1909.11304, 2019.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural network. In Advances in Neural Information Processing Systems, pp.
9108-9118, 2019.
Eugene A Golikov. Towards a general theory of infinite-width limits of neural classifiers. arXiv
preprint arXiv:2003.05884, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy.
arXiv preprint arXiv:1909.08156, 2019.
Arthur Jacot, Franck Gabriel, and CIement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pp.
8571-8580, 2018.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570-8581,
2019.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
9
Under review as a conference paper at ICLR 2021
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. In Conference on Learning Theory, pp.
2388-2464, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Grant M Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of neural networks: an
interacting particle system approach. stat, 1050:30, 2019.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of deep neural networks. arXiv
preprint arXiv:1903.04440, 2019.
Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of
large numbers. SIAM Journal on Applied Mathematics, 80(2):725-752, 2020.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
Dmitry Yarotsky. Collective evolution of weights in wide neural networks. arXiv preprint
arXiv:1810.03974, 2018.
A	Formal conditions for Section 2
Here we present formal definitions for notions that appear in Section 2; they are required for
mathematical rigor. First, recall the definition of tangent kernels:
d
Kakd(x, x0) = (d∕d*)°aσ2 X φ(Wrk),Tx)φ(Wrk),Tx0),	(27)
r=1
d
Kwkd(x, x0) = (d∕d*)0wσ2 X ∣^rk)∣2φ0(Wrk),τx)φ0(W尹①x0)xτx0.	(28)
r=1
The kernels are used to express a model increment:
∆fdkk(x) = fdk+I)(X)-fdk)(x) = X fx4 . ∆%k) + Oηa∨w→0彷粉 W + ηw,2) =
r=1	∂θr ∣θr = θ(k)
=-navfd)'(Xak),yak))Kakd(x, Xak))- ηw Vfd)”卯,沙伊)Kwkd (x, Xw))+。(湖 w+&,
(29)
Define the linear part of the model increment with respect to learning rate proportionality factors:
f∨w (x) =dfk) (X)	=-vd `(X(OL ,婢w)Ka%,d(x, XakL).	(30)
d√aVw	η*=0
ηw=0
We use this quantity to rewrite the model increment:
∆fdk)(x) = ηa∆fdka),0(x)+ηw ∆fdkw,(x)+o(ηaηw+福 2).	(3i)
Let us consider kernel definitions (27) and (28) again. Their increments are given by:
d
∆Kakd(x, x0) = -ηw(d∕d*产σ3 X (φ(W卜江x)φ0(λWrk),τx0) + φ0(W卜江x)φ(Wrk),τx0)) X
r=1
X Vfd)'(xw≈),y华)ark)φ0(Wrk),τxw≈))(x + XO)Txw≈) + Onw→o(ηw,2d3α+4qσ+1),	(32)
d→∞
10
Under review as a conference paper at ICLR 2021
d
∆κWkd(x, x0) = -ηW(d/d*产σ3 X Iark) I2 (φ0(w步,Tx)φ00(wrk),TX0)+
r=1
+Φ00(w rk),T χ)Φ0(w rk),T X0))XT X0 ×
X ▽：?'(XMywk))^*φ0(M初TXa)(X + XO)TX华 + Onw→o(ηW%3'+4qσ+1)-
d→∞
d
-η*(d∕d*)2qσ3 X 2ark')φ0(W衿TX)φ0(W衿T文)×
r=1
X Vfk)'(XaRyak))φ(wrk),TXak))+。/-。㈤，％+…1).(33)
d→∞
Similarly to what was done for model increments, we define linear parts of the kernel increments
with respect to learning rate proportionality factors:
d δKakd(X, X)
dηw
nw=0
d
-(d∕d*产σ3 X Q(Wrk江Xw(Wrk),Tx0) + φ<Wrkk江X)φ(Wrkk江x0)) X
r=1
X Vfd)'(xw≈),ywk))^rk)φ0(WIrIe)Txwk))(x + x0)txwk),
(34)
∆κwkw,0d(x, X0)= d δk¾x, X =
nw =0
d
=—(d∕d*产σ3 X ∣^rk)∣2 Q(Wrkk江x)φ00(Wrk),tx0) + φ"(Wrkk江x)φ0(WP江x0))XTXX
X Vfd)'(xMywk))^)。0^rk),TXwk))(X + x0)tXW),	(35)
∆KWka),d(X，X0)= dXO)=
ηι∣a	入* C
na=0
d
=—(d∕d*产σ3 X 2^rk)φ0(Wrk),Tx)φ0(Wrk),TXyVfd'(xa 册，族(巾衿TXak)). (36)
r=1
Note that ∆Kaa^(x, x0) = 0 since ^r-terms are absent in the definition of Kad, eq. (27).
Define PT,d = Pyxwxak-□疝…六『八〜D2k-1 {yfdk)(x) < 0} — the probability of
giving a wrong answer on the step k. Let kterm,d ∈ N ∪ {+∞} be a maximal k such that ∀k0 < k
p(ekr0r),d > 0. Generally, kterm,d depends on hyperparameters, as well as on the data distribution D.
Scaling exponents (qσ, ⅞a, ‰) together with proportionality factors (d*,σ*,η*, nW) define a limit
model f∞(k)(X) = limd→∞ fd(k)(X). We call a model "dynamically stable in the limit of large width"
if it satisfies the following condition:
Condition 3. ∃kbalance ∈ N : ∀k ∈ [kbalance, kterm,∞) ∩ N ya(k)f∞(k)(X(ak)) < 0 and
yWk)f∞k)(Xwk)) < 0 imply ∆fdka∨0w (x) = Θd→∞(fdkbalanCe)(X)) X-a.e.定也,X(Ia∨W)-a.s.
This condition puts a constraint on exponents (qσ, ⅞a, ‰); this constraint generally depends on
the train data distribution D and on proportionality factors d*, σ*, and na∨w. In order to obtain a
data-independent hyperparameter-independent constraint, we need the condition above to hold for
any value of kterm,∞ and any values of d*, σ*, and 殖Vw∙ Without loss of generality We can assume
kterm,∞ to be infinite, which gives the following condition:
11
Under review as a conference paper at ICLR 2021
Condition 4 (a formal version of Condition 1). Given kterm,∞ = +∞, ∃kbaiance ∈ N : ∀σ* > 0
∀ηα∨w > 0 ∀k ≥ kbaiance 期a f^Mk) < 0 and yWk f∞k (xW八 < 0 imply ∆fd,a∨w (x)=
Θd→∞(fdkbalanCe)(X)) X-a.e. (ya∨w, xf∨W)-a.s.
Condition 5 (a formal version of Condition 2). Following conditions separate the band of dynamical
stability (Figure 1, left):
1.	A limit model at initialization is finite: fd(0) (X) = Θd→∞ (1) X-a.e.
2.	Tangent kernels at initialization are finite: Kd(0,a)∨w (X, X0 ) = Θd→∞ (1) (X, X0)-a.e.
3.	Tangent kernels and a limit model are of the same order at initialization: Kd(0,a)∨w(X, X0) =
Θd
→∞(fd(0)(X)) (X,X0)-a.e.
4.	Tangent kernels start to evolve: ∆Kd(0,w),a0 ∨w (X, X0) = Θd→∞(Kd(0,w) (X, X0)) (X, X0)-a.e. and
∆Kd(0,a),w0(X,X0) = Θd→∞(Kd(0,a)(X,X0)) (X,X0)-a.e.
B	Proofs of propositions
We restate all necessary definitions here. We assume the non-linearity φ to be real analytic and
asymptotically linear: φ(z) = Θz→∞(z). We assume the loss function '(y, Z) to be the standard
binary cross-entropy loss: '(y, Z) = ln(1 + e-yz), where labels y ∈ {-1,1}.
The training dynamics is given as:
∆^rk) = -ηασvfd)'(χak),yak))。(俞卜，TXak)),	^rok 〜n(o, i),	⑶7
△wrk) = -ηwσVfk)'(x*,yWk)) ^rk)φ0(wrk),TXwk))X争，Wr0)〜N(0,I) YT ∈ 园，(38)
…=5Lfdk,(x) = πfyσ X 革 φ(w 尸 X)，
where (X^w ,yα∨w)〜D for D being the data distribution.
We assume hyperparameters to be scaled with width as power-laws:
σ(d) = σ*	× (d∕d*)qσ,	ηa(d)	=	ηa	X	(d")°a,	ηw⑹=ηw	X (d∕d*)°w.
B.1	Proof of Proposition 1
Define:
qθk) = inf{q ： θ(k) = Od→∞(dq)}, q∆θ = inf{q ： ∆θ(k) = Od→∞(dq)},	(39)
where θ should be substituted with a or w. We define inf(0) = +∞. We introduce similar definitions
for other quantities:
qfk)(X) = inf{q ： fdk)(X) = Od→∞(dq)},	q软X,y) = inf{q ： Vfd)'(X,y) = Od→∞(dq)},
(40)
q∆f(X) = inf{q : ∆fdk)(X) = Od→∞(dq)},	q∆fo∨ (X) = inf{q: ∆fdka)∨w(X) = Od→∞(dq)}.
(41)
Lemma 1. Assume D is a continuous distribution. Then following hold:
1.	∀k ≥ 0 ∀X,yq(k')(X,y) ≤ 0, while [yf∞k)(X) < 0] implies q∙(k)(X,y) = 0.
2.	qa0∨w = 0, qf0)(X) = qσ + 1 X-a.e.
12
Under review as a conference paper at ICLR 2021
3.	∀k ≥ 0 q∆a/ ∆w = qa∨w + qσ + qW∕)a + q∙(k) (X(k),y(k)) (Xa∨)w ,ya∨)w) -a.s.
4.	∀k ≥ 0 q∆fa∨w (x) = 2qσ + 1 + GaVw +2qWk)a + q^ (Xθk∨w ,J0∨w ) X-a.e. (Xαk∨w , yak∨w )-a.s.
5.	∀k ≥ 0 qσ + qw + q(kk ≤ 0 implies that for sufficiently small 比 and ηw q(kf (x)=
max(q∆(kf)0 (X), q∆(kf)0 (X)) X-a.e. (X(ak), ya(k), X(wk), yw(k))-a.s.
6.	∀k	≥	0 qα∨+1)	=	max(qa∨)w,q(k()∕∆w) (x(k),ykk))-a.s.,	qfk+1)(x)=
max(qf(k)(X), q∆(kf)(X)) X-a.e. (X(ak), ya(k), X(wk), yw(k))-a.s.
Proof. (1) follows from the fact that ∂'(y, z)∕∂z is bounded ∀y, while ∣∂'(y, z)∕∂z∣ ∈ [1/2,1]
when yz < 0.
^r0) 〜N (0,1) which is not zero and does not depend on d, hence q(0 = 0; similar holds for w.
For x = 0 we have fj0)(x) = σ Pd=ι ^r0)φ(W"Tx) = Θd→∞(d"+qσ) due to the Central Limit
Theorem. Hence (2) holds.
Since D is a.c. wrt Lebesgue measure on R1+dx, and φ is real analytic and non-zero,
φ(Wrk),TXaVw) = 0 and φ0(WCXaVw) is well-defined (XaVw,yθ∨w)-a.s. This implies that
q∆α∕∆w = qa∨w + qσ + qwk)a+q(k) (XaVw, yvw)(XaVw, yavw)-a.s., which is exactly (3).
Consider ∆fd(,ka),0:
△fdk)(X) = -Vfk)`(Xak)加为 Kad(x Xak))=
d
=-Vfk)`(Xak),yak)) (d∕d*)qaσ2 xφ(wrk),Tx)φ(wrk),TXak)).⑹
r=1
For the same reason as discussed above φ(Wrk),TXak)) = 0 (Xak), yak))-a.s., and φ(W，，江x) = 0
X-a.e. Since the summands are distributed identically and are generally non-zero, the sum introduces
a factor of d. Indeed, an expectation of the sum is a sum of expectations by a linearity property.
Moreover, the absolute value of the k-th moment of the sum of d indentically distributed terms does
not exceed dk times the k-th moment of each summ(akn) d. Hence the sum itself scales as d. Since
φ is asymptotically linear, each φ-term scales as dqw . Collecting all terms together, we obtain
q∆fo (x) = 2qσ + 1 + Ga + 2q∙(k) + qg'(Xk ,yS) x-a.e. (Xak),yak) )-a.s. Following the same steps
for △fw(k),0, we get (4).
Let us overview △fd(k)(X) in detail:
△fd(k)(X)
d ∞1
∂jfd(X)
r=1
j=ιj! ∂Wi1 ...∂wirr Wr=Wrk)
∆wrk),iι... ∆wrk),j+
r
∂jfd(X)
+
∞
X !______:_____L
j = 1 j! ∂^r ∂Wr ... ∂Wrj
∆^r∆w(k),i2 ...∆w(k),ij
Wr=W rk)	r	r
ar=ark
d∞
=X X j!(-1)jηwσj+1(vfd)'(xw≈),ywk)))j×
r=1 j=1
X (ark))j+ι(Φ0(wrk),τ Xwk))) jΦj)(wrk),T X)(Xwk),T xj+
∞
+ X j7(-1)jηaηwTσj+1vf?'(Xak),yak))(Vfd)'(xw≈),ywk)))jT×
j=1 j !
× (^rk))j-ιΦ('Wrk),TXak))(Φ0(wrk),TXwiɔ))j-1ΦjT)(Wrk),TX)(Xwi^,tχ)j-11.	(43)
13
Under review as a conference paper at ICLR 2021
Assumption	q°	+	Gw	+	q(kk	≤ 0 implies	nWσj+1 (&尹)j+1	=	Od→∞(ηwσ2(ark))2) and
nanW-1σj+1 (ark))j-1 = Od→∞(naσ2).
Since q(k)(x,y) ≤ 0 ∀x,y due to (1), (▽：：)'(Xwk),yWk)))j = Od→∞gfd'(xWk,yWky) and
d；d '(Xa ,yM(dfd '(xWk ,yWk ))j-1 = Od→∞(vfd)'(xak),yak))).
Since	φ(z)	=	Θz→∞(z),	φ0(W护TXwC))	=	Od→∞(1)	and	(φ0(W汐TXwC)))j	=
Od→∞(Φ0(Wrkk,TXwJ))) for j ≥ L
Hence for small enough na and nw the first term of each sum which corresponds to j = 1 dominates
all others, even in the limit of infinite d:
△/dkg = - x (nw σ2vfd)'(X(k),ywk))(^rk))2Φ0(Aw rk),T 戏	(w rk),T X)Xwi0,T x+
r=1
+ %σ22vfd '(Xk ,y*φ(w 衿 T Xak))φ(wrk),τ X)+
+ oηa∨w→0 (θd→∞ ((naVfd)'(Xak),yak)) + ^Vfd)'区卯^))谭))2) σ2)))=
=nw ∆fdkw,(X)+nt∆fdf,(X)+
+ oηa∨w→0 (θd→∞ ((naVfd)'(Xak),yak)) + nwVfd)'区卯^))谭))2) σ2d)).	(44)
Note that two summands depend on (X(wk), yw(k)) and (X(ak), ya(k)) respectively, which do not depend
on each other. Hence q∆(kf) (X) = max(q∆(kf)0 (X), q∆(kf)0 (X)) X-a.e. (X(ak∨)w,ya(k∨)w)-a.s., which is (5).
Note that the o-term does not alter the exponent. Indeed,
(nαVfd)'(Xak),yak))+ nwVfd'(X<Mywi)(&尹)2) σ2d =
=Od→∞ (d1+2qσ + max(ga + q箸(Xak),yak)),gw + q^!(χWk),yWk)) + 2qak)))=
=Od→∞ (d1+2qσ +maχ(qa+qi^' (xak) ,yak) ) + 2qWk) ,qw +q(k) (XwC) ,yWk) )+2qak)))=
= Od→∞ dmax(q∆(kf)a0 (x),q∆(kf)w0 (x)) . (45)
One before the last equality holds, because qw(k) ≥ 0 due to (2) and (6), while the last equality holds
due to (4).
By definition we have 8心+1) = &2 + ∆^rk). Since the second term depends on (Xak),yak)), while
the first term does not, we get ggk+1) = max(qak),q∆ki)). Similar holds for W丁 and fdk)(x) X-a.e.,
which gives (6).	□
Lemma 2. Assume D is a continuous distribution, kterm = +∞ and qGa = qGw = qG. Then
1.	Ifqσ + qG ≤ 0 then ∀k ≥ 0 qa(k∨)w = 0 (X(a:k-1), ya(:k-1), X(w:k-1), yw(:k-1))-a.s.
2.	If	qσ	+	qG	> 0 then ∀k ≥ 0	qa(k∨)w	=	k(qσ	+	qG)	with positive probability wrt
(:k-1) (:k-1)	(:k-1)	(:k-1)
(Xa	, ya	, Xw	, yw	).
Proof. Here and in subsequent proofs we will write "almost surely" meaning "almost surely wrt
(X(a:k), ya(:k), X(w:k), yw(:k))" for appropriate k; we apply a similar shortening for "with positive probabil-
ity wrt (X(a:k), ya(:k), X(w:k), yw(:k))".
If qσ + qG ≤ 0 then statements 1, 2, 3 and 6 of Lemma 1 imply ∀k ≥ 0 qa(k∨)w = 0 a.s.
Assume qσ + qG > 0. We will prove that ∀k ≥ 0 qa(k∨)w = max(0, k(qσ + qG)) with positive probability
by induction. Induction base is given by Lemma 1-2.
14
Under review as a conference paper at ICLR 2021
Combining the induction assumption and Lemma 1-3 We get q(k)∕∆w = (k + 1)(qσ + q) +
q 裁 X(Ry(W)) with positive probability wrt (x2k-1) ,yfk-1), XwkT),yWk-1)) (x1X ,yg% )-a.s.
Since kterm = +∞ > k, ya(k∨)w fd(k) (x(ak∨)w) < 0 With positive probability Wrt (x(ak), ya(k), x(wk), yw(k)),
and Lemma 1-1 implies that q∆a/δw = (k + 1)(qσ + O) with positive probability wrt
(:k) (:k)	(:k) (:k)
(Xa , ya , Xw , yw ).
Finally, Lemma 1-6 concludes the proof of the induction step.	□
Lemma 3. Assume D is a continuous distribution, kterm,∞ = +∞, Ga = Ow = q and Oσ + O ≤ 0*
Then ∀k ≥ 0
1* ya(k∨)wf∞(k)(X(ak∨)w)	<	0 implies q∆(kf)0	(X)	=	2qσ + 1 + qO X-a*e*
fa∨w
(X(:k-1)	(:k-1) X(:k-1)	(:k-1))-as
(Xa , ya	, Xw , yw	)-a*s*
2*	ya(k)f∞(k)(X(ak))	< 0 and	yw(k)f∞(k)(X(wk))	< 0 imply	q∆(kf)(X) =	2qσ	+ 1 +	qO	X-a*e*
(XakT),yak-1), XwkT),ywkτ))-a.s. for sufficiently small τ^a and ηw.
Proof. By Lemma 2 ∀k ≥ 0 q(k∨w = 0 a.s. Since ya∨)w f∞∞^(Xak∨w) < 0, q(k) = 0 due to Lemma
1-1. Given this, Lemma 1-4 implies ∀k ≥ 0 q∆(kf)0	(X) = 2qσ + 1 + qO X-a.e. a.s. Hence by virtue of
∆fa∨w
Lemma 1-5 ∀k ≥ 0 q∆f (x) = 2qσ + 1 + q x-a.e. a.s. for sufficiently small ^ and ηw.	□
Proposition 3. Suppose qOa = qOw = qO and D is a continuous distribution. Then Condition 4 requires
qσ + qO ∈ [-1/2, 0] to hold.
Proof. By Lemma 2 if qσ + qO > 0 then qa(k∨)w = k(qσ + qO) with positive probability. At the
same time by virtue of Lemma 1-1 kterm,∞ = +∞ implies q(k = 0 with positive probability.
Given this, Lemma 1-4 implies q∆(k)0 (X) = qσ + 1 + (2k + 1)(qσ + qO) X-a.e. with positive
∆fa∨w
probability. This means that the last quantity cannot be almost surely equal to qf(kbalance)(X) for
any kbalance independent on k. Since ∆fd(,ka)∨,0w(X) = Θd→∞(fd(kbalance)(X)) requires q∆(kf)0 (X) =
qf(kbalance)(X), we conclude that Condition 4 cannot be satisfied ifqσ + qO > 0.
Hence qσ + qO ≤ 0. Then by Lemma 3 ∀k ≥ 0 ya(k)f∞(k)(X(ak)) < 0 and yw(k)f∞(k)(X(wk)) < 0 imply
q∆f (x) = 2qσ + 1 + q x-a.e. (Xak-11 ,ytak-11, XwkT),ywk-1))-a.s. for sufficiently small 比 and
ηw. We will show that Condition 4 requires qσ + q ∈ [-1/2,0] to hold already for these sufficiently
small ^ and ηw.
Suppose ya(k)f∞(k)(x(ak)) < 0 and yw(k)f∞(k)(x(wk)) < 0. Given this, points 1 and 6 of Lemma 1 imply
Pkbaiance ≥ 1 qfbalan=e(x) = max(qf0)(x), 2qσ + 1 + q) = max(qσ + 1, 2qσ + 1 + q) x-a.e. a.s.
Hence q∆f/y (x) = qfkbalanCe)(X) x-a.e. a.s. if and only if qσ + 2 ≤ 2qσ + 1 + q, which is qσ + q ≥
-1/2; we can take kbalance = 1 without loss of generality. Having q∆(kf)0	(x) = qf(kbalance)(x) is
∆fa∨w	f
necessary to have ∆fd(,ka)∨,0w (x) = Θd→∞ (fd(kbalanCe)(x)).
Summing all together, Condition 4 requires qσ + q ∈ [-1/2,0] to hold.	□
B.2 Proof of Proposition 2
Proposition 4. Let Condition 4 holds; then
1.	fd(0)(x) = Θd→∞(1) x-a.e. is equivalent to qσ + 1/2 = 0.
2.	Kd(0,a)∨w (x, x0) = Θd→∞(1) (x, x0)-a.e. is equivalent to 2qσ + qq + 1 = 0.
15
Under review as a conference paper at ICLR 2021
3.	Kda∨w(x, x0) = Θd→∞(fd0)(X)) (x,x0)-a.e. is equivalent to q° + q+1∕2 = 0.
4.	∆Kd(0,w),a0∨w(x,x0)	=	Θd→∞(Kd(0,w) (x,x0)) (x, x0)-a.e.	and ∆Kd(0,a),w0 (x, x0)
Θd→∞(K( )(x, x0)) (x, x0)-a.e. is equivalent to qσ + q = 0.
Proof. Statement (1) directly follows from Lemma 1-2:
d
fd0)(X)= σ X ar°)φ(W r0),T x) = Θd→∞(dqσ+1/2)	(46)
r=1
(X)-a.e. due to the Central Limit Theorem.
Statement (2) follows from the definition of kernels and the Law of Large Numbers:
d
Kf0d(x, X0) = (d∕d*)qσ2 X φ(wr0)，Tx)φ(wr0),Tx0) = Θd→∞(dq+2qσ+1)	(47)
r=1
(x, x0)-a.e.; the same logic holds for the other kernel: K0d(x, x0) = Θd→∞(dq+2qσ+1) (x, x0)-a.e.
Combining derivations of the two previous statements, we get the statement (3). Now we proceed
to the last statement. Consider again the kernel Ka(0,d); a linear part of this increment with respect to
proportionality factors of learning rates is given by, see eq. (34):
∆Ka(0w),,d0(x,x0)
∂ ∆κaod(x, a
dηw
ηw=0
d
=-(d∕d*产σ3 X (φ(M0),Tx)φ0(η^^r0),Tx0) + φ0(η^^r0),Tx)φ(M0),Tx0)) X
r=1
X Wd)'(Xw0),yS))d)"(Wr0),TXwI))(X + XO)TXwI),	(48)
Hence ∆Kι0W,d = Θd→∞(Ka0d) is equivalent to qσ + q = 0. Considering the second kernel K0d
and its increment is equivalent to the same condition.	口
C The number of distinct limit models is finite
It is easy to see that due to the Proposition 4 Condition 5 divides the well-definiteness band into 13
regions. We now show that when proportionality factors σ* and η*∨w are fixed, choosing a limit
model evolution is equivalent to picking a single region from these 13.
Indeed, for any width d a model evolution can be written as follows:
∆fdk)(x) = -ηw Vfd)'(xw≈),y的)(Kwkd(X, x(k))+
+o¾∨w→o(ηw ∆κwkid(x, Xa)+ηa∆κwα,d (x, XwC))))-
d→∞
-KVfd)'(xak),yak)) (κfkd(x, Xak))+Onw →ο(ηw ∆κaw),,d (x, Xak)))).
d→∞
fdk+1)(x) = fdk) (X) + Afd^X),	Vfd)'(x,y) =	：k),
1 + exp(fd (x)y)
d
fd0)(x)=σ*dqσ X^ro)Φ(ʌwro),Tx),	(aro),wro)) ~N(o,iι+dχ).
r=1
Now we introduce normalized kernels:
Kakd(x, x0) = (d∕d*)τ-°-2qσ Kakd(x, x0)=σ*,2d* X φ(wrk),τ x)φ(w rk),T *,
(49)
(50)
(51)
(52)
16
Under review as a conference paper at ICLR 2021
宜Wkd(x,x') = (d/d*)T-A2qσKwkd(x,x0) =σ*,2Cd X ∣*∣2φ,(W衿Tχ)φ0(W(MTX)XTx0.
r=1
(53)
Note that after normalization kernels stay finite in the limit of large width due to the Law of Large
Numbers. Similarly, we normalize logits, as well as kernel and logit increments:
∆K Rd' = (d∕d*)T-α-2qσ∆K(7d',	(54)
Mdk) = (d∕d*)T-q-2qσ∆fdk) ,	fdk ) = Id∕d* )T-q-2qσ fdk ).	(55)
We then rewrite the model evolution as:
∆fdk( (x) = -ηw Vfk) '(xwk) ,ywk) KKwkd(x, XW))+
+ Ona∨w→0(ηw∆Kwkid(x, Xa)+naK<W2：d (x, Xa)))-
d→∞
-ηM?'(Xak),yak))(κakd(x,Xak))+Onw→cι(ηw∆κawd(x,Xak)))).	(56)
d→∞
fk+1)(x) = fk)(x) + ∆fk)(x) Vk ≥ 0,	(57)
d
fd0)(X)=σ*(d∕d*)-1-°f X&，)。(秘0)，Tx), (^ro),〜M0∕+dx),
r=1
fdk)(x) = (d∕d*)1+α+2qσ fdk) (X),
_______-y________
1 + exp(fdk) (x)y)
Vk ≥ 0.
(58)
(59)
C.1 Constant normalized kernels case
Kernek Ka∨)w,d are either constants (hence ∆Kf)d' T 0 as d → ∞) or evolve with k in the limit of
large d. First assume they are constants; in this case qσ + q < 0 due to Proposition 4-4, and
∆fdk)(x) = -ηwVk)'(xw=),yM (KwOd(X,Xw=)) + %vw-o(1))-
d→∞
—
ηaVk)'(xak),yak)) (KaOd(x, Xak))+Onw-。⑴).
d→∞
(60)
Since normalized kernels Ka∨)w d converge to non-zero limit kernels Ka∨)w,∞, we can rewrite the
formula above as:
∆fdk)(x) = -ηw Vk)'(xw!),y的)(KwO∞(x, Xw0) + Od→∞(1))-
-ηaVd'(Xa 增"Ka∞(x, Xak))+od→∞(i)). (61)
fd0) (x)= σ*(d∕d*)T∕2-q-qσ (N (0,σ ⑼,2(x)) + Od→∞(1)),	(62)
where σ(0) (x) can be calculated in the same manner as in Lee et al. (2019). As required by Proposition
3 1∕2 + q + qσ ≥ 0, hence f0)(x) = Od→∞(1). This implies the following:
vf∞'(X,y) = dl→∞ Vd)'(X，y)=
-y[N(0,σ(0),2(x))y < 0]	for	1∕2 +	qσ	> 0;
1+exp(σ*d*,1∕2N (0,σ(0),2(x))y)	for	1∕2 +	qσ	= 0;
-y∕2	for	1∕2 +	qσ	< 0.
(63)
lim
d→∞
-y
1 + exp((d∕d*)1+α+2qb fd0)(x)y)
17
Under review as a conference paper at ICLR 2021
On the other hand, ∆f0)(x) = Θ^→∞(1) with positive probability over (Xae∨w,y(vw). Hence
fO) = Od→∞(∆fdO)) and f1) = f0) + ∆fO) = Θd→∞(1). For the same reason, fk+1)=
f(k) + ∆f(fc) = Θd→∞(1) Vk ≥ 0.
This implies the following:
Vk ≥ 0
V”(x,y)
dl→∞ Vfd+1)'(x，y)
d→∞ 1 + exp((d∕d*)1+α+2qb fk+I)(X)y)
-y[limd
→∞fdk+1) (χ)y< 0]
__________-y__________
1+exp(limd
—y∞> 乃k+I)(X)y)
-y∕2
for 1 + q + 2qσ > 0;
for 1 + q + 2qσ =0;	(64)
for 1 + q + 2qσ < 0.
If we define f∞k' (x) = limd→∞ fdk)(x), We get the following limit dynamics:
△恋(χ) = -ClV?'(xWM)KW0,∞(x, x^) - nW?'(xlMyMK∞(x, Xak)),
(65)
Ka0∞ (x, x‘)=σ*,2d*E W 〜N (o,idx)φ(w t x)。(W t xi,	(66)
Kw,∞(x, x0) = σ*,2d*E(^,W)〜N(0,iι+dχ)∣02φ'(WTx)φ0(WTXO)XTx0,	(67)
fk+1)(x) =	fk)(x)+4fk)(X)	f ⑼(x)	= (σ*d*"2N (0,σ(O),2(X))	for 1/2	+ q + qσ	=0;
f∞	( ) =	f∞ ( )+ f∞ ( ),	f∞ ()	=10	for 1/2	+ q + qσ	>	0;
(68)
(-y[N(0,σ(O),2(x))y < 0]	for	1/2 +	qσ	> 0;
vf∞ '(x,y) =	∖ 1+exp(σ*d*,1/2N (0,σ(O) ,2(x))y)	fθr	1/2 +	qσ∙	= O,
I-y∕2	for	1/2 +	qσ	< 0;
(69)
Vfy) '(x,y)
-y[∕(k+1)(x)y < 0]
_______-y______
1+exp(f∞k+I)(X)y)
-y/2
for 1 + q + 2qσ > 0;
for 1 + q + 2qσ =0;
for 1 + q + 2qσ < 0;
Vk ≥ 0.
(70)
This dynamics is defined by proportionality factors σ*, d*, τ^*vw, and signs of three exponents:
1/2 + qσ, 1 + q + 2qσ and 1/2 + q + qσ. Since we assume proportionality factors to be fixed,
choosing signs of exponents is equivalent to choosing a limit model. Note that these exponents
exactly correspond to those mentioned in Proposition 4, points 1, 2 and 3. One can easily notice from
Figure 1 (left) that given qσ + q < 0, there are 8 distinct sign configurations.
Note also that since we are interested in binary classification problems, only the sign of logits matters.
Since f，，= (d/d*)1+，+2+ "), signs of f，，and of W) are the same for all d. Hence Vx, y
limd→∞ sign(fdk)(x)) = limd→∞ sign(∕dk)(x)) = Sign(J⅛k,(x)).
C.1.1 NTK LIMIT MODEL
We state here a special case of the NTK scaling (qσ = -1/2, q = 0, see Jacot et al. (2018)) explicitly.
Since in this case 1 + q + 2qσ = 0, we can omit tildas everywhere. This results in the following limit
dynamics:
∆f∞k)(x) = -nwvf∞∞'(x4,y年)κw0,∞(x, XM- n*vf∞∞'^^环说北(x,Xak)),⑺)
Kg%(x, x0) = σ*,2d*EW〜N(0,idx)Φ(WTx)φ(WTx0),	(72)
Kw,∞(x, x') = σ*,2d*E(^,W)〜N(0,iι+dx)∣02φ'(WTx)φ'(WTXO)XTx0,	(73)
18
Under review as a conference paper at ICLR 2021
f∞k+I)(X) = f∞∞)(x)+∆f∞)(x),	f∞0)(x) = b*d*,1/2N (0,σ⑼,2(x)),	(74)
Vf∞ '(χ,y)
_______-y________
1 + exp(f∞k)(x)y)
∀k ≥ 0.
(75)
C.2 Non-stationary normalized kernels case
Suppose now q» + q = 0. In this case ∆κd0W,,a∨w (x, x0) = Θd→∞ (Kd(0,w) (x, x0)) (x, x0)-a.e. and
∆Kd(0,a),w0 (x, x0) = Θd→∞ (Kd(0,a)(x, x0)) (x, x0)-a.e. by virtue of the Proposition 4-4. Hence kernels
evolve in the limit of large width (at least, for sufficiently small η*∨w).
If we follow the lines of the previous section, we will get a limit dynamics which is not closed:
△恋(X) = -ηw Vf∞ '(x^,y俨)(K*(x, Xwk))+
+ Oηa∨w →0(ηw δk般 ∞(x, Xwk))+ηa∆KWk),,∞(x, x&))-
-ηavf∞'(xak),yak))(kak∞∞(x, Xak))+Onw→0(ηwδkaw),,∞o(x,Xak)))),须
f∞k+1)(x)= f∞∞)(x) + f>(x),爬)(x) = 0,	(77)
(-y[N(0,σ⑼,2(x))y < 0]	for 1/2 + qσ	>	0;
vf∞ '(x,y)={ 1+eχp(σ*d*,"N(0,σ(0),2(x))y)	for 1/2 + qσ	=	0;	(78)
1―y/2	for 1/2 + qσ	<	0;
(―y[f∞k+1)(x)y < 0] for 1 + qσ > 0;
vfk+1)'(x,y)=	1+exp(fk+l)(x)y)	1+ 口。=0; Yk ≥ 0∙	(79)
1―y/2	for 1 + qσ < 0;
The reason for this is non-stationarity of kernels. As a workaround we consider a measure in the
weight space:
1d
μd) = d E δark)乳 δw rk).	(80)
r=1
Recall the stochastic gradient descent dynamics:
△ark) = -ηaσ*vfd)'(xak),yak)) φ(wrk),TXak)), ^r0) 〜m。,力	⑻)
△wrk) = -ηwσ*vfd)'(xWk),ywk)) ark%(wrk),TXMXw)	Wr°) 〜N©几)	岱幻
Here We have replaced ι^a∨wσ with η*∨wσ*, because q» + ⅞ = 0. Similar to Rotskoff & Vanden-
Eijnden (2019); Chizat & Bach (2018), this dynamics can be expressed in terms of the measure
defined above:
1d
μdk+1) = μdk) +div(μdk)∆θdk)),	μd0) = d X S@ro), θ∖0) 〜N(0∕+dχ) ∀r ∈ [d], (83)
∆θdk)(^, W)=
=[ηaσ*vfd)'(xak),yak))φ(WTXak)), ηwσ*vfd)'(xMyWk))^φ0(WTX华)x年,T]T,	(84)
fdk)(x) = o*(d/d*)1+qb / aφ(WTx) μdk)(d^,dW),	(85)
19
Under review as a conference paper at ICLR 2021
Vfd)'(χ,y)
-------‰------ ∀k ≥ 0.
1 + exp(fd( )(x)y)
We rewrite the last equation in terms of fi(k(x) = (d∕d*)-1-qσfjk)(x):
f(k)(x) = σ*d* / ^φ(w^Tx) μ(dk (da, dw^),
-y
1 + exp((d∕d*)1+qσ fdk)(x)y)
∀k ≥ 0.
This dynamics is closed. Taking the limit d → ∞ yields:
μ(∞k+1) = μ(∞k) + div(μ(∞k)∆θ∞(k)), μ(∞0)=N(0,I1+dx),
(86)
(87)
(88)
(89)
∆θ∞)(^,旬=
=-喘σ*Vf∞'(Xak),yak))0(WTXak)), nWσ*vf∞'^x^,y^)aφ∖wTXa)X华T]T,(90)
f∞k)(x) = σ*d* / ^φ(w^Tx) μ∞k(da, dw^),
(91)
vf∞ '(x,y)
-y[N (0, σ(0),2 (x))y < 0]
-y
1+exp(σ*d*L2N (0,σ(O),2(x))y)
-y∕2
for1∕2+qσ > 0;
for 1∕2 + qσ = 0;
for 1∕2 + qσ < 0;
(92)
~/
vf∞+1)'(x,y)
-y[f
(k+1)
∞
-y
(x)y < 0]
1+exp(f∞k+1)(x)y)
-y∕2
for 1 + qσ > 0;
for 1 + qσ = 0;	∀k ≥ 0.
for 1 + qσ < 0;
(93)
Since proportionality factors σ*, d*, and n*∨w are assumed to be fixed, choosing qσ is sufficient to
define the dynamics. Signs of exponents 1∕2 + qσ and 1 + qσ give 5 distinct limit dynamics. Together
with 8 limit dynamics for constant normalized kernels case, this gives 13 distinct limit dynamics,
each corresponding to a region in the band of a dynamical stability (Figure 1, left).
As was noted earlier, only the sign of logits matters, and our f(k) preserve the sign for any d: ∀x
limd→∞ sign(fdk)(x)) = limd→∞ sign(f(k)(x)) = sign(f∞k)(x)).
C.2.1 MF limit model
We state here a special case of the mean-field scaling (qσ = -1, q = 1, see Rotskoff & Vanden-
Eijnden (2019) or Chizat & Bach (2018)) explicitly. Similar to NTK case, since 1 + qσ = 0 we can
omit tildas. This results in the following limit dynamics:
μ(∞k+1) = μ(∞k) + div(μ(∞k)∆θ∞(k)),	μ(∞0)=N(0,I1+dx),	(94)
∆θ∞)(^,旬=
=-喘σ*vf∞'(xak),yak))φ(WTXak)), nwσ*vf∞'(xW≈),yWk))^φ0(WTxW≈))x俨t]t,(95)
f∞k)(x) = σ*d* / ^φ(w^Tx) μ∞)(da,dw^),	vf∞'(x,y)
_______-y________
1 + exp(f∞k) (x)y)
∀k ≥ 0. (96)
20
Under review as a conference paper at ICLR 2021
C.2.2 Sym-default limit model
Another special case which deserves explicit formulation is what we have called a "sym-default"
limit model. The corresponding scaling is: q° = -1/2, q = 1/2. The resulting limit dynamics is the
following:
μ∞+1) = μ∞) +div(μ∞)∆θ(k)),	μ∞ = N(0,lι+dχ),	(97)
∆θ∞)(^, W)=
=-喘σ*vf∞'(Xak),yak))0(WTXak)), ηwσ*vf∞'(xMyWk))^φ0(WTX卯)χ伊t]t, (98)
f∞k(x) = σ*d* / ^φ(WTx) μ∞k(da, dW),	(99)
vf∞ '(χ,y)
_____________-y______________
1 + exp(σ*d*,V2N(0, σ⑼,2(x))y)
(100)
Vf∞+1)'(x, y) = -y[f∞k+1)(x)y < 0] ∀k ≥ 0.	(101)
D Default scaling
Consider the special case of the default scaling: q» = -1/2, Ga = 1, Gw = 0. Then corresponding
dynamics can be written as follows:
∆ark) = -nab*(d/d*)1/2vfd)'(Xak)加为。(应尹,TXak)), ^* 〜“(。,力	(io2)
△WW = -ηwσ*(d**)T2Vfd)'(xw≈),ywk)) ark)φ0(WW，丁XWM, W乎〜Nald乂),
(103)
d
fdk)(x) = b*(d/d*)T/2 Xark)φ(Wrk),Tx), vfd)'(x,y) =------------y	∀k ≥0.
r=1	r	fd	1 + exp(fd(k) (x)y)
(104)
As one can see, increments of output layer weights ∆a(kk diverge with d. We introduce their
normalized versions: ∆a(kk = (d/d*)-1/2∆a(kk. Similarly, we normalize output layer weights
themselves: Grk) = (d/d*)-1/2^). Then the dynamics transforms to:
△&rk)= -η*σ*vfd)'(xak),yak))。(俞步①Xak)),	&，)〜Na ⑷涉尸)，(io5)
△Wrk) = -ηwσ*vf7'(xw≈),ywk)) Grk)φ0(W护Txw≈))xw≈),	W* 〜”(0,几)，(106)
d
fdk)(x) = σ* X 相(农产x), Vfd)'(x,y) =-------------------y	∀k ≥ 0.	(107)
d	r=1	fd	1 + exp(fd(k) (x)y)
Similar to Appendix C.2, we have to introduce a weight-space measure in order to take a limit of
d→ ∞:
1d
μd) = dΣSδark) 乳 δw rk).	(108)
In terms of the measure the dynamics is expressed then as follows:
μdk+1) = μdk) + div(μdk)∆θdk)),	(io9)
21
Under review as a conference paper at ICLR 2021
1d
μd0)	=力 ∑2δa(0)㊈	δw (O),	arO)	~N(0, (d/d*)-1),	W rO)	~N(0,Idχ)	∀r	∈	[d],	(110)
d	ar	wr
∆θdk)(5, W)=
=-[ηaσ*vf?'(Xak),yak))0(WTXak)), ηwσ*Vfd)'(x^,yWk))5φ0(WTXa)X华T]T, (111)
fdk)(x) = σ*d / aφ(WvTx) μ^ (da, dw^),
vfd)'(χ,y)
_______-y________
1 + exp(fdk) (x)y)
∀k ≥ 0. (112)
We rewrite the last equation in terms of fad(k)(X) = d-1fd(k)(X):
fdk)(x) = σ* / aφ(WTx) μdk)(d^,
dW),
Vfd)'(x,y)
________-y_______
1 + exp(dfdkk (x)y)
∀k ≥ 0. (113)
A limit dynamics then takes the following form:
μ∞+1) = μ∞ +div(μ∞)∆θdk)), μ∞) = δ 因N(0,idχ)	(114)
∆θ∞)(a, W)=
=-[ηaσ*vf∞'(xak),yak))φ(WTXak)), nWσ*vf∞'(xWk),yWk))αφ0(WTXwk))X俨t]t,	(115)
vf∞ '(x,y)
_____________-y______________
1 + exp(σ*d*,V2N(0, σ(O),2(x))y)
(116)
f(k)(x)= σ* ∕αφ(WT x) μ∞)(da,dW),	vf∞+1)'(x,y) = -y[fk+1)(x)y < 0] ∀k ≥ 0.
(117)
As one can notice, the only difference between this limit dynamics and the limit dynamics of
sym-default scaling (Appendix C.2.2) is the initial measure.
We now check the Condition 5. First of all, by the Central Limit Theorem, fd(O)(x) = Θd→∞(1),
hence the first point of Condition 5 holds. As for kernels, we have:
d
Kakd(x, x0) = σ*,2 X φ(Wrk),Tx)φ(Wrk),Tx0),	(118)
r=1
d
KWkd(x, x0) = σ*,2(d∕d*)T X ∣^rk)∣2φ0(ΛWrk),Tx)φ0(Wrk),Tx0)xTx0.	(119)
r=1
We see that while KW(O,)d converges to a constant due to the Law of Large Numbers, Ka(O,d) diverges as
d → ∞. This violates the second statement of Condition 5, and the third as well, since f∞(O) is finite.
Consider now kernel increments:
d
△Kawid (x, XO)=—σ*,3wd*)-" x (φ(w (IkkaX)φ0(w rk),T x0)+
r=1
+Φ0(w rk),T x)Φ(w rk),T x0)) ×
X Vfd)'(xMyWk))^rk)φ0(Wrk),TXwk))(X + XO)TXM (120)
22
Under review as a conference paper at ICLR 2021
d
△KWM(x, x0) = -σ*,3(d∕d*)T2 x ∣^rk)∣2 (φ0(Λw rk),T χ)φ00 (Wrk),T x0)+
r=1
+ φ00(W rk),T x)φ0(W (Ik)T x0))XT x0×
X W?'(XMywk))^*φ0(W尸Xwk))(X + x0)TXM (121)
d
△KW?,d(X, x0)= —σ*,3(d∕d*)τ/2 x 2^rk)φ0(w rk),T x)φ0(w rk),T x∖×
r=1
× vfd)'(Xak),yak))φ(wrk),TXak)).(⑵)
For k = 0 terms inside sums of each increment have zero expectations. Hence the Central Limit
Theorem can be used here. We get: △Ka(0w),,d0 = Θd→∞(1), △Kw(0w),,0d = Θd→∞(d-1), △Kw(0a),,d0 =
Θd→∞(1). Since Ka(0,d) = Θd→∞(d), Kw(0,)d = Θd→∞ (1), the last statement of Condition 5 is
violated as well.
E	Initialization-corrected mean-field (IC-MF) limit
Here we consider the same training dynamics as for the mean-field scaling (see Appendix C.2), but
with a modified model definition:
∆^rk) = -ηaσ*vfd)'(xak),yak))φ(wrk),TXak)),	^* 〜m。, i),	(⑵)
△wrk) = -ηwσ*vfd'(xW),yW ^rk)φ0(Wrk),TXa)Xa, W4 〜N(Q,Q.	(124)
dd
fdk)(x) =σ*(d∕d*)-1 X ^rk)φ(W衿Tx) + σ*(d∕d*)T/2 X &，)0(应/Tx),	(125)
r=1	r=1
V(k)'(x,y) = ----‰------ ∀k ≥ 0.	(126)
fd	1 + exp(fdk)(x)y)一
Similar to the mean-field case (Appendix C.2), we rewrite the dynamics above in terms of the
weight-space measure:
d
μdk+1) = μdk) +div(μdk)∆θdk)), μd0) = d X δθ(0), θr0)~N(0,Iι+dχ) ∀r ∈ [d], (127)
r=1
∆θdk)(^, W)=
=-[ηaσ*Vf?'(Xak),yak)W(WTXak)), ηwσ*vfd''(Xwk),yWk))^φ0(WTXwk))Xwk),t]t,	(128)
fdk) (x) = σ*d* J ^φ(WTx) μdk)(d^, dW)
+ σ*(dd*)1/2 J ^φ(WTx) μd0)(dα, dW),
(129)
Vfd)'(X,y)=i + exp-dk)(x)y) ∀k ≥ 0.
(130)
Note that here fd(k) stays finite in the limit of d → ∞ for any k ≥ 0. Hence taking the limit d → ∞
yields:
μ∞+1) = μ∞ +div(μ∞)∆θ∞)), μ∞∞ = N(0,Iι+dχ),	(131)
∆θ∞)(^, W)=
=-[ηaσ*vf∞'(xak),yak))Φ(WTXak)), ηwσ*vf∞'(x^,yWk))^φ0(WTXwk))X俨t]t,	(132)
23
Under review as a conference paper at ICLR 2021
f∞k)(x) = σ*d* / ^φ(WTx) μ∞)(d^, dW) + σ*d*,"N(0, σ⑼,2(x)),	(133)
vf∞ '(X,y)
_______-y________
1 + exp(f∞k)(x)y)
∀k ≥ 0.
(134)
F Experimental details
We perform our experiments on a feed-forward fully-connected network with a single hidden layer
with no biases. We learn our network as a binary classifier on a subset of the CIFAR2 dataset (which
is a dataset of first two classes of CIFAR101) of size 1024. We report results using a test set from the
same dataset of size 2000. We do not do a hyperparameter search, for this reason, we do not use a
validation set.
We train our network for 2000 training steps to minimize the binary cross-entropy loss. We use a
full-batch GD as an optimization algorithm. We repeat our experiments for 10 random seeds and
report mean and deviations in plots for logits and kernels (e.g. Figure 1, left). For plots of the
KL-divergence, we use logits from these 10 random seeds to fit a single gaussian. Where necessary,
We estimate data expectations (e.g. EX〜D|f (x)|) using 10 samples from the test dataset.
We experiment with other setups (i.e. using a mini-batch gradient estimation instead of exact one, a
larger train dataset, a multi-class classification) in Appendix G. All experiments Were conducted on a
single NVIDIA GeForce GTX 1080 Ti GPU using the PyTorch frameWork (Paszke et al., 2017). Our
code is available online: hsuppressed for anonymityi.
Although our analysis assumes initializing variables With samples from a gaussian, nothing changes if
We sample σξ instead, Where ξ can be any symmetric random variable With a distribution independent
on hyperparameters.
In our experiments, we took a network of width d* = 27 = 128 with leaky ReLU activation and
apply the Kaiming He uniform initialization (He et al., 2015) to its layers; We call this netWork a
reference network. According to the Kaiming He initialization strategy, initial weights have a zero
mean and a standard deviation σ* α (d*)-1/2 for the output layer, while the standard deviation of
the input layer does not depend on the reference width d*. For this network we take learning rates in
the original parameterization ηa* = ηw* = 0.02. After that, we scale its initial weights and learning
rates with width d according to a scaling at hand:
σ=σ
ηa∨w = ηa∨w (d )qa∨w
Note that we have assumed σw = 1. By definition, ι^a∨w = ηa∨w/σ2∨w; this implies:
2
ηa=η* (σ*)
= ηa*
ηw = ηw*
G	Experiments for other setups
Although plots provided in the main body represent the full-batch GD on a subset of CIFAR2, we
have experimented with other setups as well. For instance, we have varied the batch size and the
size of the train dataset. Results are shown in Figures 3-7. Here differences are marginal and not
qualitative.
We have also experimented with multi-class classification: see Figure 8. Here we have trained our
network on the full CIFAR10 dataset with SGD with batches of size 100. As we see on left plot, the
IC-MF limit model has the lowest KL-divergence relative to the reference model, however, in terms
of the test accuracy, all the limit models are similar.
1CIFAR10 can be downloaded at https://www.cs.toronto.edu/~kriz/cifar.html
24
Under review as a conference paper at ICLR 2021
Figure 3: Test accuracy of different limit models, as well as of the reference model. Setup: We train a
one hidden layer network on subsets of the CIFAR2 dataset of different sizes with SGD with varying
batch sizes.
H	Generalization to deep nets proposal
While our present analysis is devoted to networks with a single hidden layer, we discuss possible
generalizations to deep nets here.
Consider a network with H hidden layers. For simplicity, assume that widths of all hidden layers are
equal to d. We thus have to consider H + 1 learning rates 祉H, one for each layer, and similarly H + 1
initialization variances σ0: H. Without loss of generality, We may assume the input layer variance to
be equal to 1 (we can rescale inputs otherwise). This gives 2H + 1 hyperparameters in total.
Similarly to What We did for H = 1, We assume that each hyperparameter obeys a poWer-laW With
respect to Width. Let us refer the set of the poWer-laW exponents as a "scaling". Again, We Want to
reason about What the scaling should be in order to converge to a dynamically stable limit model: see
Condition 1. Moreover, We Want to derive conditions that separate the domain of "dynamically stable"
scalings, such that each region corresponds to a distinct unique dynamically stable limit model: see
Condition 2.
Having that much hyperparameters seems burdening, and this prohibits us to draW a nice tWo-
dimensional scaling plane as We did for H = 1: see Figure 1. For this reason, one have to reduce the
dimensionality of a scaling.
First, it is tempting to consider a homogeneous activation function: a leaky ReLU. This introduces
a symmetry in the Weight space that guarantees that dynamics depends only on the product of
initialization variances: σH × . . . × σ0; let us refer this product as σ. This approach Was previously
used by Golikov (2020), hoWever We have to note that non-smoothness of the activation function
introduces certain mathematical obstacles. Nevertheless, one may consider sacrificing mathematical
rigor in favor of reducing the number of hyperparameters from 2H + 1 to H + 1.
The next simplification should affect learning rate scaling exponents. Similar to What We have done
for a shallow net, we may assume all learning rate exponents to be equal: qo = ... = qH = q.
25
Under review as a conference paper at ICLR 2021
Figure 4: Mean kernel diagonals E X〜D(ηaKa,d(x, x) + nWKw,d(x, x)) of different limit models, as
well as of the reference model. Setup: We train a one hidden layer network on subsets of the CIFAR2
dataset of different sizes with SGD with varying batch sizes. Data expectations are estimated with 10
test data samples.
The NTK limit, which generalizes naturally to deep nets, requires qo = ... = GH = 0, and hence
conforms the assumption above. However, a possible generalization of the mean-field limit requires
qo = qh = 1, while qι = ... = Gh-ι = 2; see Sirignano & Spiliopoulos (2019); Araujo et al.
(2019); Golikov (2020). This aspect suggests the following alternatives:
1.	Consider qGo = qGH = qG, while qG1 = . . . = qGH-1 = qGhid; this results in a three-dimensional
space of scalings: (qσ, qG, qGhid).
2.	Consider qGo = qGH = qG, while qG1 = . . . = qGH-1 = 2qG; this results in a two-dimensional
space of scalings that covers both of the NTK and the mean-field scalings.
The former class of scalings is richer, but if it does not contain any interesting limit models that are
present in the second class, it can be more expository to tighten the class to the latter. By "interesting"
we mean limit models that are "non-dominated" in a similar sense as we have specified in Section 3.
In order to define which limit models are better than others in approximating finite-width nets ("non-
dominated"), we have to derive conditions that separate the domain of dynamically stable scalings
into regions of distinct unique corresponding limit models, similar to Condition 2. We hypothesize
that these conditions are similar to the shallow case: (1) a limit model at initialization is finite, (2)
kernels at initialization are finite, (3) a limit model and kernels are of the same order, (4) kernels
evolve at initialization. Since we have decided to consider separate learning rate scalings for hidden
layers and for input and output layers, we expect that the above-proposed conditions should consider
two distinct families of kernels respectively: hidden kernels and input plus output kernels.
It will be very interesting to check if all of the dynamically stable limit models are specified either by
an evolution in a model space driven by constant kernel, orby an evolution ofa weight-space measure,
as was the case for H = 1; see Appendix C. Investigating a non-dominated limit model, different
from both the NTK and the mean-field models, should be a valuable outcome of the proposed research
26
Under review as a conference paper at ICLR 2021
train SiZe = 1024, batch CiZe = 1024
=X)J-d .⅛oSqE ueəiu
10°	101	102
training step, k+1
2 × 101 3 × 1(⅛× 101 6 × 101 IO2
training step, k+16
-(X≤G4J∙- 60- SqeUe ① UJ
2× 1013× 1(⅛× IO1 6 × IO1 10
training step, k+16
train size = 1024, batch size = 1024 train SiZe = 1024, batch SiZe = 256
Figure 5: Mean absolute logits E X〜D ∖f (x)∖ of different limit models, as well as of the reference
model. Setup: We train a one hidden layer network on subsets of the CIFAR2 dataset of different
sizes with SGD with varying batch sizes. Data expectations are estimated with 10 test data samples.
-
10
-(XX*/£§
-(X HF山
Figure 6: Mean absolute logits relative to kernel diagonals E X〜D∖fd(x)∕(^^2Ka,d(x, x) +
nWKw,d(x, x))∖ of different limit models, as well as of the reference model. Setup: We train a
one hidden layer network on subsets of the CIFAR2 dataset of different sizes with SGD with varying
batch sizes. Data expectations are estimated with 10 test data samples.
=XX⅛(x≤d
10
program; it will be even more valuable if this limit model will not be covered by both mean-field and
constant kernel formalisms.
27
Under review as a conference paper at ICLR 2021
IO1
Q<υ-l,⅛E=)-l>l
Figure 7: KL-divergence of different limit models relative to a reference model. Setup: We train a
one hidden layer network on subsets of the CIFAR2 dataset of different sizes with SGD with varying
batch sizes.
train SiZe = 8192, batch SiZe = 512
IO2
training step, k+4
----default
----sym-default

Figure 8: Left: KL-divergence of different limit models relative to a reference model. Right:
Accuracies on the test set of different limit models as well as of the reference model. Setup: We train
a one hidden layer network on the full CIFAR10 dataset with SGD with batches of size 100.
We also have to note that according to Golikov (2020), the mean-field limit vanishes for H > 2. This
fact suggests that the analysis for deep nets should be held for H = 2 and for H > 2 separately.
I	Measuring divergence between a limit model and a reference
ONE
We track the divergence of a limiting network from a reference one, which can be done in two ways.
The first one is tracking divergence directly between logits: EX〜DtestDIogitS(f∞∞'(x) || fdk)(x)) for
some divergence measure D(∙ || ∙). The second one is tracking divergence between probabilities:
EX〜DtestDProb(σ(f∞k (x)) || σ(fdk)(x))), where we have overloaded the notation by denoting the
standard sigmoid as σ: σ(x) = (1 + exp(-x))-1.
We choose a KL-divergence for the first case. However, measuring a KL-divergence between logits is
hardly possible, since we do not have an access to the distribution of f (k)(x) as a random variable
depending on initialization. For this reason, we fit a gaussian to its samples:
DIogits(ξ || ξ*) = KL(N(Eξ, Var ξ) || N(Eξ, Var ξ*)).	(135)
This case is depicted in Figure 9, left.
28
Under review as a conference paper at ICLR 2021
IO0	IO1	IO2	IO3	IO0	IO1	IO2	IO3
training step, k+1	training step, k+1
Figure 9: Left: we plot a KL-divergence of logits of different infinite-width limits of a fixed finite-
width reference model relative to logits of this reference model. KL-divergences are estimated using
gaussian fits with 10 samples. Right: same, for probabilities instead of logits. KL-divergences are
estimated using beta-distribution fits with 10 samples. Setup: we train a one hidden layer network
with SGD on CIFAR2 dataset; see Appendix F for details.
As for the second case, we may want to measure a KL-divergence between distributions on probabili-
ties. Again, this is not possible, because the true distribution of σ(f∞(k) (x)) is not known; for this
reason, we decide to first fit a beta distribution, and then measure the divergence:
Dprob(ξ || ξ*) = KL(Beta(amie(ξ),βmie(ξ)) || Beta(amie(ξ*),βm∕e(ξ*))),	(136)
where αmle(ξ) and βmle(ξ) are maximum-likelihood estimations of hyperparameters of a beta:
αmie(ξ)= Eξ (E¾-Eξ) - 1),	βmie(ξ) = (1-Eξ) (^^-^ - l) ∙	(137)
Var ξ	Var ξ
This case is plotted in Figure 9, right.
Both cases can be generalized to work with multi-class classification. In the first case, we can simply
fit a gaussian with a diagonal covariance matrix; this was done in Figure 8, left. In the second case,
we can fit a Dirichlet random variable using a maximum-likelihood estimation as before.
29