Under review as a conference paper at ICLR 2021
On the Dynamic Regret of Online Multiple
Mirror Descent
Anonymous authors
Paper under double-blind review
Ab stract
We study the problem of online convex optimization, where a learner makes se-
quential decisions to minimize an accumulation of strongly convex costs over time.
The quality of decisions is given in terms of the dynamic regret, which measures
the performance of the learner relative to a sequence of dynamic minimizers. Prior
works on gradient descent and mirror descent have shown that the dynamic regret
can be upper bounded using the path length, which depend on the differences be-
tween successive minimizers, and an upper bound using the squared path length has
also been shown when multiple gradient queries are allowed per round. However,
they all require the cost functions to be Lipschitz continuous, which imposes a
strong requirement especially when the cost functions are also strongly convex.
In this work, we consider Online Multiple Mirror Descent (OMMD), which is
based on mirror descent but uses multiple mirror descent steps per online round.
Without requiring the cost functions to be Lipschitz continuous, we derive two
upper bounds on the dynamic regret based on the path length and squared path
length. We further derive a third upper bound that relies on the gradient of cost
functions, which can be much smaller than the path length or squared path length,
especially when the cost functions are smooth but fluctuate over time. Thus, we
show that the dynamic regret of OMMD scales linearly with the minimum among
the path length, squared path length, and sum squared gradients. Our experimental
results further show substantial improvement on the dynamic regret compared with
existing alternatives.
1	Introduction
Online optimization refers to the design of sequential decisions where system parameters and cost
functions vary with time. It has applications to various classes of problems, such as object tracking
(Shahrampour & Jadbabaie, 2017), networking (Shi et al., 2018), cloud computing (Lin et al., 2012),
and classification (Crammer et al., 2006). Itis also an important tool in the development of algorithms
for reinforcement learning (Yuan & Lamperski, 2017) and deep learning (Mnih et al., 2015).
In this work, we consider online convex optimization, which can be formulated as a discrete-time
sequential learning process as follows. At each round t, the learner first makes a decision xt ∈ X,
where X is a convex set representing the solution space. The learner then receives a convex cost
function ft(x) : X → R and suffers the corresponding cost of ft(xt) associated with the submitted
decision. The goal of the online learner is to minimize the total accrued cost over a finite number
of rounds, denoted by T . For performance evaluation, prior studies on online learning often focus
on the static regret, defined as the difference between the learner’s accumulated cost and that of an
optimal fixed offline decision, which is made in hindsight with knowledge of ft(∙) for all t:
TT
RegsT = Xft(xt) - minXft(x).
x∈X
t=1	t=1
A successful online algorithm closes the gap between the online decisions and the offline counterpart
when normalized by T , i.e., sustaining sublinear static regret in T . In the literature, there are various
online algorithms (Zinkevich, 2003; Cesa-Bianchi & Lugosi, 2006; Hazan et al., 2006; Duchi et al.,
2010; Shalev-Shwartz, 2012) that guarantee a sublinear bound on the static regret.
1
Under review as a conference paper at ICLR 2021
However, algorithms that guarantee performance close to that of a static decision may still perform
poorly in dynamic settings. Consequently, the static regret fails to accurately reflect the quality of
decisions in many practical scenarios. Therefore, the dynamic regret has become a popular metric in
recent works (Besbes et al., 2015; Mokhtari et al., 2016; Yang et al., 2016; Zhang et al., 2017), which
allows a dynamic sequence of comparison targets and is defined by
TT
RegT = X ft(xt)- X ft(xt),
where χ↑ = argminχ∈χ ft(x) is a minimizer of the cost at round t.
It is well-known that the online optimization problem may be intractable in a dynamic setting, due to
arbitrary fluctuation in the cost functions. Hence, achieving a sublinear bound on the dynamic regret
may be impossible. However, it is possible to upper bound the dynamic regret in terms of certain
regularity measures. One of the measures to represent regularity is the path length, defined by
T
CT = X kx-xLιk,	(1)
t=2
which illustrates the accumulative variation in the minimizer sequence. For instance, the dynamic
regret of online gradient descent for convex cost functions can be bounded by O(√T(1 + CT))
(Zinkevich, 2003). 1 For strongly convex functions, the dynamic regret of online gradient descent
can be reduced to O(CT) (Mokhtari et al., 2016). When the cost functions are smooth and strongly
convex, by allowing the learner to make multiple queries to the gradient of the cost functions, the
regret bound can be further improved to O(min(CT, ST)), where ST represents the squared path
length, defined by
T
ST = X k嬉-xLιk2,	⑵
t=2
which can be smaller than the path length when the distance between successive minimizers is small.
All the aforementioned studies require the cost functions to be Lipschitz continuous. However, there
are many commonly used cost functions, e.g., the quadratic function that do not meet the Lipschitz
condition. In addition, the above works rely on measuring distances using Euclidean norms, which
hinders the projection step in gradient descent update for some constraint sets, e.g., probability
simplex (Duchi, 2018).
Besides gradient descent, mirror descent is another well-known technique of online convex optimiza-
tion (Hall & Willett, 2015; Jadbabaie et al., 2015). Mirror descent uses the Bregman divergence,
which generalizes the Euclidean norm used in the projection step of gradient descent, thus acquiring
expanded applicability to a broader range of problems. In addition, the Bregman divergence is only
mildly dependent on the dimension of decision variables (Beck & Teboulle, 2003; Nemirovsky &
Yudin, 1983), so that mirror descent is optimal among first-order methods when the decision variables
have high dimensions (Duchi et al., 2010). In this work we focus on the mirror descent approach.
In previous works on online mirror descent, the learner queries the gradient of each cost function
only once, and performs one step of mirror descent to update its decision (Hall & Willett, 2015;
Shahrampour & Jadbabaie, 2017). In this case, the dynamic regret has an upper bound of order
O(√T(1 + CT)), which is the same as that of online gradient descent in (Zinkevich, 2003). In this
work, we investigate whether it is possible to improve the dynamic regret when the learner performs
multiple mirror descent steps in each online round, while relaxing the Lipschitz continuity condition
on the cost functions.
To this end, we analyze the performance of the Online Multiple Mirror Descent (OMMD) algorithm,
which uses multiple steps of mirror descent per online round. When the cost functions are smooth
and strongly convex, we show that the upper bound on the dynamic regret can be reduced from
1A more general definition of the dynamic regret was introduced in (Zinkevich, 2003), which allows
comparison against an arbitrary sequence {ut}tT=1. We note that the regret bounds developed in (Zinkevich,
2003) also hold for the specific case of Ut = x。
2
Under review as a conference paper at ICLR 2021
O(√T(1 + CT)) to O(min(Cτ, ST, GT)), where GT represent the sum squared gradients, i.e.,
T
Gt = X kVft(xt)k2,	(3)
t=1
where k. k * denotes the dual norm. The sum squared gradients GT can be smaller than both the path
length and squared path length, especially when the cost functions fluctuate drastically over time. In
contrast to the aforementioned works, our analysis does not require the cost functions to be Lipschitz
continuous. Furthermore, our numerical experiments suggest substantially reduced dynamic regret
compared with the best known alternatives, including single-step dynamic mirror descent (Hall &
Willett, 2015), online multiple gradient descent (Zhang et al., 2017), and online gradient descent
Zinkevich (2003).
2	Online Multiple Mirror Descent
In this section, we describe OMMD and discuss how the learner can improve the dynamic regret by
performing multiple mirror descent steps per round. Before delving into the details, we proceed by
stating several definitions and standard assumptions.
2.1	Preliminaries
Definition 1: The Bregman divergence with respect to the regularization function r(∙) is defined as
Dr(x, y) = r(x) - r(y) - hVr(y), X - y).
The Bregman divergence is a general distance-measuring function, which contains the Euclidean
norm and the Kullback-Leibler divergence as two special cases.
Using the Bregman divergence, a generalized definition of strong convexity is given in (Shalev-
Shwartz & Singer, 2007).
Definition 2: A convex function f (∙) is λ-strongly convex with respect to a convex and differentiable
function r(∙) if
f (y) + hVf (y),x - y) + λDr (x,y) ≤ f(x), ∀x,y ∈ X.
Following many prior studies on mirror descent, we assume that the cost functions are λ-strongly
convex, where the above generalized strong convexity definition is used. We further assume that
the cost functions are L-smooth, and the regularization function r(∙) is Lr-smooth and 1-strongly
convex with respect to some norm (refer to App. A for definitions). We note that these are standard
assumptions commonly used in the literature after the group of studies began by (Hazan et al., 2006;
Shalev-Shwartz & Singer, 2007), to provide stronger regret bounds by constraining the curvature of
cost functions.
We further make a standard assumption that the Bregman divergence is Lipschitz continuous as
follows:
|Dr(x, z) - Dr(y, z)| ≤ Kkx - yk, ∀x, y, z ∈ X,
where K is a positive constant. We note that this condition is much milder than the condition of
Lipschitz continuous cost functions required in (Zhang et al., 2017; Mokhtari et al., 2016; Hall &
Willett, 2015). There is a notable weakness in such bounds. Since the sequence of cost functions
are revealed to the learner, the learner has no control over it. If these cost functions happen to not
meet the Lipschitz condition, earlier analyses that require this condition become inapplicable. In this
work, we do not require the cost functions to be Lipschitz continuous. Instead, we move the Lipschitz
continuity condition from the cost functions to the Bregman divergence to broaden the application of
our work. The main benefit of this is that the regularization function and the corresponding Bregman
divergence is within the control of the learner. The learner can carefully design this regularization
function to satisfy the Lipschitz continuity of the associated Bregman divergence with a small factor.
For example, in the particular case of the KL divergence, which is obtained by the choosing negative
entropy as the regularization function, on the set X = {x| Pd=ι Xi = 1; Xi ≥ D}, the constant K is
of O(log D). Other examples of many widely used Bregman divergences that satisfy this condition
are given in (Bauschke & Borwein, 2001).
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Online Multiple Mirror Descent
Input: Arbitrary initialization of xι ∈ X; step size a; time horizon T.
Output: Sequence of decisions {xt : 1 ≤ t ≤ T}.
1:	for t = 1, 2, . . . , T do
2:	submit Xt ∈ X and receive ft(∙)
3:	set yt1 = xt
4:	for i = 1, 2, . . . , M do
5:	yi+1 = argminy∈x {hvft(yt),yi + 1 Dr (y,yt)}
6:	end for
7:	set xt+1 = ytM+1
8:	end for
2.2	Online Convex Optimization with OMMD
We consider online optimization over a finite number of rounds, denoted by T. At the beginning of
every round t, the learner submits a decision represented by xt , which is taken from a convex and
compact set X. Then, an adversary selects a function ft(∙) and the learner suffers the corresponding
cost ft(xt). The learner then updates its decision in the next round. With standard mirror descent,
this is given by
xt+ι = argmin{hVft(xt),xi + 1 Dr (x,xt)}	(4)
where α isa fixed step size, and Dr (∙, ∙) isthe Bregman divergence corresponding to the regularization
function r(∙). The update in equation 4 suggests that the learner aims to stay close to the current
decision xt as measured by the Bregman divergence, while taking a step in a direction close to the
negative gradient to reduce the current cost at round t.
OMMD uses mirror descent in its core as the optimization workhorse. However, in contrast to
classical online optimization methods, where the learner queries the gradient of each cost function
only once, OMMD is designed to take advantage of the curvature of cost functions by allowing the
learner to make multiple queries to the gradient in each round. This is especially important when
the successive cost functions have similar curvatures. In particular, in order to track xJ+i the learner
needs to access the gradient of the cost function, i.e., Vft+i (∙). Unfortunately, this information is not
available until the end of round t + 1. However, if the successive functions have similar curvatures,
the gradient of ft(∙) is a reasonably accurate estimate for the gradient of ft+ι(∙). In this case, every
time that the learner queries the gradient of ft(∙), it finds a point that is likely to be closer to the
minimizer of ft+ι(∙). Hence, it may benefit the learner to perform multiple mirror descent steps in
each round.
Thus, the learner generates a series of decisions, represented by yt1, yt2, . . . , ytM+1, via the following
updates:
y1 = xt, yi+1 = argmin{hVft(yi),yi+ 1 Dr(y,yi)}, i = 1,2,...,M.	(5)
y∈X	α
Then, by setting xt+1 = ytM+1, the learner proceeds to the next round, and the procedure continues.
Note that M is independent of T .
Applying multiple steps of mirror descent can reveal more information about the sequence of
minimizers. It can reduce the dynamic regret, but only if the series of decisions in equation 5 helps
decrease the distance to the minimizer xtJ+1. Therefore, quantifying the benefit of OMMD over
standard mirror descent requires careful analysis on the impact of the fluctuation of ft(∙) over time.
To this end, we provide an analysis to bound the dynamic regret of OMMD in the next section.
3	Theoretical Results
The following lemma paves the way for the proposed analysis on the dynamic regret of OMMD. It
bounds the distance of the learner’s future decision from the current optimal solution, after a single
step of mirror descent.
4
Under review as a conference paper at ICLR 2021
Lemma 1 Assume that ft(∙) is λ-strongly convex with respect to a differentiablefUnction r(∙), and
is L-smooth. Single-step mirror descent with a fixed SteP Size α ≤ L guarantees thefollowing:
Dr (x；,xt+1) ≤ βDr (xt,xt),
where XJ= is the unique minimizer of ft(∙), andβ
2αλ
1+αλ.
Lemma 1 is proved in App. B in the supplementary material.
Remark 1. Lemma 1 states that a mirror descent step reduces the distance (measured by the Bregman
divergence) of the learner’s decisions to the current minimizer. This generalizes the results in
(Mokhtari et al., 2016; Zhang et al., 2017), where similar bounds were derived for online gradient
descent when the distance was measured in Euclidean norms. In particular, those results correspond
to the special choice of r(x) = kxk22, which reduces the Bregman divergence to Euclidean distance,
i.e., Dr(x,y) = kx - yk2.
Lemma 1 indicates that the distance between the next decision xt+1 and the minimizer xt= is strictly
smaller than the distance between the current decision xt and the minimizer at round t. This implies
that if the minimizers of the functions ft(∙) and ft+ι(∙), which are X= and x=+1 respectively, are
not far from each other, applying mirror descent multiple times enables the online learner to more
accurately track the sequence of optimal solutions Xt= .
The succeeding theorems provide three separate upper bounds on the dynamic regret of OMMD,
based on path length CT (as defined in equation 1), squared path length ST (as defined in equation 2),
and sum squared gradients (as defined in equation 3).
Theorem 2 Assume that r(∙) is Lr -smooth and 1 -Strongly convex with respect to some norm k ∙ k and
that the costfunctions are L-smooth and λ-strongly convex with respect to r(∙). Let Xt be the sequence
ofdecisions generated by OMMD with a fixed step size ɪ < α ≤ L and M ≥ d (ɪ + 20χ) log Lre
mirror descent steps per round. The dynamic regret satisfies the following bound:
T
X ft (Xt) - ft (Xt= ) ≤
t=1
(CT+	kX1= -X1k).
where β is the shrinking factor derived in Lemma 1, and K is the Lipschitz constant associated with
Dr (∙, ∙).
The proof of Theorem 2 is given in App. C in the supplementary material.
Remark 2. It has been shown in (Hall & Willett, 2015) that single-step mirror descent guarantees an
upper bound of O(√T(1 + CT)) on the dynamic regret for convex cost functions. With that bound,
a sublinear path length is not sufficient to guarantee sublinear dynamic regret. In contrast, Theorem 2
implies that OMMD reduces the upper bound to O(CT) when the cost functions are strongly convex
and smooth, which implies that a sublinear path length is sufficient to yield sublinear dynamic regret.
Remark 3. The range of M where the bound in Theorem 2 holds is usually wide. For example, it is
M ≥ 3 and M ≥ 5 for the two experiments shown in Section 4.
Theorem 3 Under the same convexity and smoothness conditions stated in Theorem 2, let Xt
be the sequence of decisions generated by OMMD with a fixed step size α ≤ L and M ≥
d(2 + 20λ) log2Lre mirror descent steps per round. For any arbitrary positive COnStant θ, the
dynamic regret is upper bounded by
X ft(Xt)- ft(χt) ≤ X kvft2&k= +
t=1	t=1
LLr + θ A (S + kxl - x1k2
1 — 2Lr βM I(T +	2
Theorem 3 is proved in App. D in the supplementary material.
Since the gradient at Xt= is zero if Xt= is in the relative interior of the feasibility set X, i.e.,
kvft(Xt=)k = 0, the above theorem can be simplified to the following corollary.
Corollary 4 If Xt= belongs to the relative interior of the feasibility set X for all t, the dynamic regret
bound in Theorem 3 is of order O(ST).
5
Under review as a conference paper at ICLR 2021
When the cost functions drift slowly, the distances between successive minimizers are small. Hence,
the squared path length ST, which relies on the square of those distances, can be significantly smaller
than the path length CT . In this case, Theorem 3 and Corollary 4 can provide a tighter regret bound
than Theorem 2.
Theorem 5 Under the same convexity and smoothness conditions stated in Theorem 2, let xt be the
sequence OfdecisiOns generated by OMMD with a fixed step size a > 击∙ ThefOllOwing bound holds
on the dynamic regret:
T	α2λ
X ft(Xt) - ft (W) ≤ 4αλ - 2 GT.
t=1
The proof of Theorem 5 is given in App. E in the supplementary material.
Remark 4. Interestingly, Theorem 5 implies that sublinear dynamic regret can be achieved when the
gradient of the cost functions shrink over time. For instance, if kVft(χ)k* = O(1∕tγ) for some
γ > 0, Theorem 5 guarantees O(T 1-2γ) dynamic regret. This is especially important when the cost
functions decrease while the minimizers fluctuate. In this scenario, the path length CT and squared
path length ST may grow linearly, whereas diminishing gradients ensure sublinear GT .
Theorem 2, Corollary 4, and Theorem 5, respectively, state that the dynamic regret of OMMD is
upper bounded linearly by path length CT , squared path length ST , and sum squared gradients GT .
This immediately leads to the following result.
Corollary 6 Under the same cOnvexity and smOOthness cOnditiOns stated in TheOrem 2, the dynamic
regret Of OMMD with suitably chOsen α and M has an upper bOund Of O(min(CT, ST, GT)).
Remark 5. We note that (Mokhtari et al., 2016) and (Zhang et al., 2017) provide upper bounds
of O(CT) and O(min(CT, ST)), respectively, on the dynamic regret of online gradient descent
with single and multiple gradient queries, while (Hall & Willett, 2015) presents an upper bound of
O(√Τ(1 + CT)) on the dynamic regret of online single-step mirror descent. Corollary 6 shows that
OMMD can improve the dynamic regret bound to O(min(CT, ST, GT)). Furthermore, in contrast to
these studies, our analysis does not require the cost functions to be Lipschitz continuous.
The quantities Ct, ST, and GT represent distinct aspects ofan online learning problem and are not
generally comparable. The following example demonstrates the benefit of having multiple upper
bounds and taking their minimum. Consider a sequence of quadratic programming problems of the
form ft(x) = kAtx - btk2 over the d-dimensional probability simplex. Assume that for any t ≥ 1,
we have the parameter sequence of
A = J diag( tp1ι, 0,0,..., 0), if t is odd
t	d diag(0, tp1ι, 0,..., 0), if t is even,
and b = [£,焉,...,£]0, wherep1 andp2 are positive constants such that p2 ≤ pi. In this setting,
we observe that CT = O(T) and GT = O(T 1-p1-p2). Thus, GT can be considerably smaller than
CT . On the other hand, it is also possible that CT is smaller than GT in other cases. For example,
let At = diag(1/2, 0, . . . , 0) on odd rounds and At = diag(0, -1/2, . . . , -1/2) , and bt be the unity
vector for all t. In this case, we observe that CT = O(1), while the sum gradient scales linearly with
time, i.e., GT = O(T). Thus, neither CT nor GT alone can provide a small regret bound for all cases.
Similar examples can be found in comparison between ST and GT but are omitted for brevity.
4	Experiments
We investigate the performance of OMMD via numerical experiments in two different learning
scenarios (with further experiments presented in App. G in the supplementary material). First, we
consider a ridge regression problem on the CIFAR-10 dataset (Krizhevsky, 2009). Then, we study a
case of online convex optimization where the difference between successive minimizers diminishes
as time progresses. We compare OMMD with the following alternatives: Online Gradient Descent
(OGD) (Zinkevich, 2003), Online Multiple Gradient Descent (OMGD) (Zhang et al., 2017), and
Dynamic Mirror Descent (DMD) (Hall & Willett, 2015).
6
Under review as a conference paper at ICLR 2021
In the first experiment, we consider multi-class classification with ridge regression. In this task, the
learner observes a sequence of labeled examples (ω, z), where ω ∈ Rd, and the label z, denoting the
class of the data example, is drawn from a discrete space Z = {1, 2, . . . , c}. We use the CIFAR-10
image dataset, which contains 5 × 104 data samples. Each data sample ω is a color image of size
32 × 32 pixel that can be represented by a 3072-dimensional vector, i.e., d = 3072. Data samples
correspond to color images of objects, including airplanes, cars, birds, cats, deer, dogs, frogs, horses,
ships, and trucks. Hence, there are c = 10 different classes. For ridge regression, the cost function
associated with batch of data samples at round t, i.e., (ω1,t, z1,t), . . . , (ωb,t, zb,t), is given by
f(x, (ωt,zt)) = kωtT x - ztk22,
where X is the optimization variable, which is constrained by the set X = {X : X ∈ Rd+ , k
X k1 = 1}, and (ωt , zt) compactly represents the batch of data samples at round t, i.e., ωt =
[ω1,t, ω2,t, . . . , ωb,t]T and zt = [z1,t, z2,t, . . . , zb,t]T. The goal of the learner is to classify streaming
images online by tracking the unknown optimal parameter χ↑. We use the negative entropy regular-
ization function, i.e., r(X) = Pjd=1 Xj log(Xj), which is strongly convex with respect to the l1-norm.
Then, the mirror descent update in equation 4 leads to the following update:
i =	yi,j eχpJaVft(Yij))
Pd=I yi,j eχp(-αv ft(yi,j))，
(6)
where xt,j and Vft(Xt)j denote the j-th component of Xt and Vft(Xt) respectively. The proof of
the above closed-form update is given in App. F in the supplementary material. In our experiment,
we set batch size to 20 data samples per online round, and set α = 0.1.
In Fig. 1, we compare the performance of OMMD with DMD, OGD, and OMGD in terms of the
dynamic regret. We see that the methods based on mirror descent perform better than those based on
gradient descent as generally expected. Furthermore, OMMD with M = 10 can reduce the dynamic
regret up to 30% in comparison with DMD. The dynamic regret associated with all algorithms grow
linearly with the number of rounds. This is because the sequence of minimizers χ↑ depend on batches
of samples that are independent over time, so that they do not converge. We note that this is common
for online optimization in dynamic settings where steady fluctuation in the environment results in
linear dynamic regret.
Next, we study the performance of OMMD in solving a sequence of quadratic programming problems
of the form ft(X1, X2) = ρkX1 - at k2 + kX2 - btk2, where ρ is a positive constant, at and bt are
time-variant vectors, and the decision variable are X1 ∈ Rd1, X2 ∈ Rd2, such that d1 + d2 = d. In
our experiment, we set P = 10, di = 500, and d = 1000. We assume that b is time-invariant and for
all rounds t we have bt = 2, while at satisfies the recursive formula at+1 = at + 1∕√t with initial
value a1 = -1.5. We further set the step size α = 0.03. We use the same regularization function and
constraint set as in the previous experiment.
From Fig. 2, we observe that the performance advantage of OMMD is even more pronounced. As
time progresses and the difference between the successive cost functions becomes less significant,
the difference between the minimizers decreases. In this case, OMMD can significantly improve
the performance of online optimization by reducing the gap between the learner’s decisions and
the minimizers sequence. In particular, compared with DMD, OMMD with M = 10 reduces the
dynamic regret up to 80% after 2500 rounds.
5	Related Works
The problem of online convex optimization has been extensively studied in the literature since the
seminal work of (Zinkevich, 2003). Most prior works study various online algorithms that guarantee
a sublinear bound on the static regret (Zinkevich, 2003; Cesa-Bianchi & Lugosi, 2006; Hazan et al.,
2006; Duchi et al., 2010; Shalev-Shwartz, 2012). Here we review the most relevant works with a
focus on the dynamic regret.
5.1	Dynamic Regret of Online Gradient Descent
Dynamic regret was first introduced in (Zinkevich, 2003) for the analysis of online gradient descent,
where an O(√TCt) bound on the dynamic regret was derived for convex functions. When the learner
has knowledge of the path length beforehand, the dynamic regret can be upper bounded by O(√TCT)
7
Under review as a conference paper at ICLR 2021
Ie4
7 6 5 4 3
期63^ U-IUeUXo
500
1000	1500	2000	2500
Round
⅞9j6sm U-IUeUXo
Figure 2: Dynamic regret of slowly drifting cost
functions.
Figure 1: Dynamic regret comparison on
CIFAR-10 dataset.
(Yang et al., 2016). For strongly convex cost functions, the upper bound on the dynamic regret can be
reduced to O(CT) (Mokhtari et al., 2016). The above works make only a single query to the gradient
of the cost functions in every round. By allowing the learner to make multiple gradient queries per
online round, the regret bound can be improved to O(min(CT , ST )) when the cost functions are
smooth and strongly convex (Zhang et al., 2017). The analysis in all aforementioned studies requires
the cost functions to be Lipschitz continuous. However, many commonly used cost functions do not
satisfy this condition over an unbounded feasible set, e.g., the quadratic function, and even when
the feasible set is bounded the Lipschitz factor can be excessively large, especially when the cost
functions are strongly convex. Therefore, Lipschitz continuity of the cost functions is not assumed in
our analysis. Instead, we move this condition from the cost functions to the Bregman divergence,
which the learner can control and design. In addition, the above works rely on measuring distances
using Euclidean norms, while the updates with Euclidean distance are challenging for some constraint
sets, e.g., probability simplex (Duchi, 2018). It is known that gradient descent does not perform as
well as mirror descent, especially when the input dimension is high (Nemirovsky & Yudin, 1983;
Beck & Teboulle, 2003).
5.2 Dynamic Regret of Online Mirror Descent
The dynamic regret of online single-step mirror descent was studied in (Hall & Willett, 2015), where
an upper bound of O(√T(1 + CT)) was derived for convex cost functions. To take advantage of
smoothness in cost functions, an adaptive algorithm based on optimistic mirror descent (Rakhlin &
Sridharan, 2013) was proposed in (Jadbabaie et al., 2015), which contains two steps of mirror descent
per online round. However, different from our work, in that variant the learner is allowed to make only
a single query about the gradient. The algorithm further requires some prior prediction of the gradient
in each round, which is used in the second mirror descent step. The dynamic regret bound was given
in terms of a combination of the path length CT , deviation between the predictions and the actual
gradients DT, and functional variation FT = PtT=1 maxx∈X |ft(x) - ft-1(x)|. 2 Unfortunately, to
achieve this bound, the algorithm requires the design of a time-varying step size that depends on
the optimal solution in the previous step, which prevents direct numerical comparison with OMMD.
Therefore, in Section 4 we have experimented only with the method of (Hall & Willett, 2015).
All aforementioned works make only a single query to the gradient of the cost functions in every
online round. In contrast, in this work, we allow the learner to make multiple gradient queries per
round. The learner then uses this information to update its decision via multiple steps of mirror
descent. In this way, we show the dynamic regret can be upper bounded linearly by the minimum
among the path length, squared path length, and sum squared gradients. Furthermore, as opposed to
the aforementioned works, our analysis does not require the cost functions to be Lipschitz continuous.
Finally, there is also recent work in the literature on distributed online mirror descent (Shahrampour &
Jadbabaie, 2017). As expected, it is more challenging to achieve performance guarantee in distributed
optimization. We focus on centralized online convex optimization in this work.
2We note that the regret bounds derived in (Jadbabaie et al., 2015) is under the same definition of (Zinkevich,
2003).
8
Under review as a conference paper at ICLR 2021
6	Conclusion
We have studied online convex optimization in dynamic settings. By applying the mirror descent
step multiple times in each round, We show that the upper bound on the dynamic regret can be
reduced significantly from O(√T(1 + CT)) to O(min(Gr, ST, GT)), when the cost functions are
strongly convex and smooth. In contrast to prior studies (Hall & Willett, 2015; Zhang et al., 2017;
Mokhtari et al., 2016), our analysis does not require the cost functions to be Lipschitz continuous.
Numerical experiments with the CIFAR-10 dataset, and sequential quadratic programming, and
additional examples show substantial improvement on the dynamic regret compared with existing
alternatives.
9
Under review as a conference paper at ICLR 2021
References
Heinz H Bauschke and Jonathan M Borwein. Joint and separate convexity of the bregman distance.
Studies in Computational Mathematics, 8:22-36, 2001.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167-175, 2003.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Non-stationary stochastic optimization. Operations
Research, 63(5):1227-1244, 2015.
Nicolo Cesa-Bianchi and Ggbor Lugosi. Prediction, Learning, and Games. Cambridge university
press, 2006.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Machine Learning Research, 7:551-585, 2006.
John C Duchi. Introductory lectures on stochastic optimization. The mathematics of data, 25:99,
2018.
John C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari. Composite objective mirror
descent. In Proceedings of the Conference on Learning Theory, 2010.
Eric C Hall and Rebecca M Willett. Online convex optimization in dynamic environments. IEEE
Journal of Selected Topics in Signal Processing, 9(4):647-662, 2015.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for
stochastic strongly-convex optimization. Journal of Machine Learning Research, 15(1):2489-
2512, 2014.
Elad Hazan, Adam Tauman Kalai, Satyen Kale, and Amit Agarwal. Logarithmic regret algorithms
for online convex optimization. In Proceedings of the Conference on Learning Theory, 2006.
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan. Online optimiza-
tion:Competing with dynamic comparators. In Proceedings of the International Conference on
Artificial Intelligence and Statistics, 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL http:
//www.cs.toronto.edu/~kriz/learning- features- 2009- TR.pdf.
Minghong Lin, Adam Wierman, Lachlan LH Andrew, and Eno Thereska. Dynamic right-sizing for
power-proportional data centers. IEEE/ACM Transactions on Networking, 21(5):1378-1391, 2012.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. Online optimization in
dynamic environments: Improved regret rates for strongly convex problems. In Proceedings of the
IEEE Conference on Decision and Control, 2016.
Arkadii Nemirovsky and David Yudin. Problem Complexity and Method Efficiency in Optimization.
Wiley, 1983.
Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences.
In Proceedings of the International Conference on Advances in Neural Information Processing
Systems, 2013.
Shahin Shahrampour and Ali Jadbabaie. Distributed online optimization in dynamic environments
using mirror descent. IEEE Transactions on Automatic Control, 63(3):714-725, 2017.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in
Machine Learning, 4(2):107-194, 2012.
10
Under review as a conference paper at ICLR 2021
Shai Shalev-Shwartz and Yoram Singer. Logarithmic Regret Algorithms for Strongly Convex Repeated
Games. The Hebrew University, 2007.
Ming Shi, Xiaojun Lin, Sonia Fahmy, and Dong-Hoon Shin. Competitive online convex optimization
with switching costs and ramp constraints. In Proceedings of the IEEE Conference on Computer
Communications, 2018.
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi. Tracking slowly moving clairvoyant: Optimal
dynamic regret of online learning with true and noisy gradient. In Proceedings of International
Conference on Machine Learning, 2016.
Jianjun Yuan and Andrew Lamperski. Online control basis selection by a regularized actor critic
algorithm. In Proceedings of the IEEE American Control Conference, 2017.
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou. Improved dynamic regret for
non-degenerate functions. In Proceedings of the International Conference on Advances in Neural
Information Processing Systems, 2017.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the International Conference on Machine Learning, 2003.
11
Under review as a conference paper at ICLR 2021
A	Additional Definitions
Definition 3: A function f (∙) is L-Smooth, if there exists a positive constant L such that
f(y) ≤ f(x) + hVf(x),y - Xi + L2ky - x∣∣2, ∀x,y ∈ X.
Definition 4: A convex function f (∙) is λ-strongly convex with respect to some norm ∣∣ ∙ ∣∣, if there
exists a positive constant λ such that
f(y) + hVf(y),x - yi + λ∣∣X — y∣2 ≤ f(χ), ∀χ,y ∈ X.
Definition 5: A function f (∙) is Lipschitz continuous with factor G if for all X and y in X, the
following holds:
|f (X) - f (y)| ≤ G∣X - y∣, ∀X,y ∈ X.
B Proof of Lemma 1
Consider single-step mirror descent update as follows:
x = argmin Jft(X0) + hVft(x0),y - x0i + 1 Dr(y,x0)) .	(7)
y∈X	α
Strong convexity of the above minimization objective implies
ft(x0) + hVft(x0 ),x 一 X0i + 1 Dr (x,X0) ≤	(8)
α
ft(x0) + hVft(χ0),y — X0i + 1 Dr(y,χ0) — 1 Dr(y,χ), ∀y ∈ X.
αα
Furthermore, from the smoothness condition, we have
ft(x) ≤ ft(x0) + hVft(χ),χ — x0i + L ||x — χ0k2.	(9)
Substituting equation 9 into equation 8, and setting y = x↑, we obtain
ft(X) — 3∣IX — x0k2 +-Dr (x, x0) ≤	(IO)
2α
ft(X0) + hVft(X0),Xt — X0 i + 1 Dr (X；,X0) — 1 Dr (x；,X).
αα
Since α ≤ +, and regularization function r(∙) is 1-strongly convex, we have
1 Dr(x,x0) ≥ LDr(x,x0) ≥ gIX — X0∣∣2.	(11)
α2
Next, we exploit the strong convexity of the cost function, i.e.,
ft(X0) +〈Vft(X0),X； — X0i ≤ ft(Xt) — λDr(x^,x0).	(12)
Combining equation 10, equation 11, and equation 12, we obtain
ft(X)≤ ft (x；) - λDr(Xt,X0) + 1 Dr(Xt ,X0) — 1 Dr(X；,X).	(13)
αα
Next, we use the result of (Hazan & Kale, 2014), which states that for evey λ-strongly convex function
ft(.), the following bound holds:
ft(x) — ft(Xt) ≥ λDr(x；,x),	(14)
where Xt； = argminx∈Xft(X). Combining the above with equation 13, we obtain
Dr (Xt；, X) ≤ βDr (Xt；, X0),	(15)
where β = 1 - ι+αɑ.

12
Under review as a conference paper at ICLR 2021
C Proof of Theorem 2
C.1 Key Lemmas
The following two lemmas pave the way for our regret analysis leading to Theorem 2. Lemma 7
presents an alternative form for the mirror descent update.
Lemma 7 Suppose there exists zt+ι that satisfies Vr(^z1)+ι) = Vr(xt) 一 aVft(xt), for some
strongly convex function r(∙), and step size a. Then, thefollowing updates are equivalent
xt+1 = argmin Dr(x, zt+1),	(16)
x∈X
xt+ι = argmin {〈Vft(xt),xi+ 1 Dr(x,xt)} .	(17)
Proof. We begin by expanding equation 16 as follows:
xt+1 = argmin{r(x) 一 r(zt+1) 一 hVr(zt+1), x 一 zt+1i}
x∈X
= argmin{r(x) 一 hVr(zt+1), xi}
x∈X
= argmin{r(x) 一 hVr(xt) 一 αVft (xt), xi}
x∈X
= argmin{αhVft(xt),xi + r(x) 一 r(xt) 一 hVr(xt), x 一 xti}
x∈X
=argmin{hVft(xt),xi + 1 Dr (x,xt)}.	(18)
Thus, the update in equation 16 is equivalent to equation 17.
Lemma 8 Under the same convexity and smoothness condition stated in Theorem 2, let xt be the
sequence of decisions generated by OMMD. Then, the following bound holds:
kxt+1 - xt k ≤ PLr βM kxt - xt k,	(19)
where Lr is the smoothnessfactor ofthe regularization function r(∙), and β is the shrinkingfactor
obtained in Lemma 1.
Proof. Using the result of Lemma 1, OMMD with M mirror descent steps guarantees
Dr (x↑,Xt+1) ≤ βM Dr (Xt,Xt).	(20)
Since the regularization function r(∙) is 1-strongly convex, We have
kxt_2xt+lk ≤ r(W) — r(xt+1) 一 hVr(Xt+1),x； — xt+1i.	QI)
Next, we exploit the smoothness condition of the regularization function r(∙), i.e.,
r(xji) — r(xt) — hVr(xt),xJi - Xti ≤ L ∣∣xji - Xtk2.	(22)
By combining the above with equation 20, and equation 21, and using the definition of Bregman
divergence, we obtain
kxt+ι-x和I2 ≤ LrβM∣xt-x和∣2.	(23)
Taking the square root on both sides of equation 23 completes the proof.
13
Under review as a conference paper at ICLR 2021
C.2 Proof of the Theorem
Now, we are ready to present the proof of Theorem 2. In this proof, we will use the following
properties of Bregman divergence.
(a)	By direct substitution, the following equality holds for any x, y, z ∈ X,
(▽r(Z)- Vr(y),x - y)= Dr(x,y) - Dr(x,z) + Dr(y,z).	(24)
(b)	If x = argminx0∈X Dr(x0, z), i.e., x is the Bregman projection of z into the set X, then for any
arbitrary point y ∈ X , we have
Dr(y, z) ≥ Dr (y, x) + Dr (x, z).	(25)
To bound the dynamic regret, We begin by using the strong convexity of the cost function ft(∙), i.e.,
ft(xt) - ft(x；) ≤ (Vft(xt),xt - x；)一 λDr(x；, Xt)
≤
≤
≤
≤
≤
1	(Vr(xt) - Vr(zt+ι),Xt - x；) - λDr(x；,xt)
α
α (Dr(Xt , Xt) - Dr(Xt ,zt+1) + Dr(Xt, zt+1)) - λDr(Xt , Xt)
α (Dr (x；,Xt) - Dr (X；,Xt+l) - Dr (Xt+1,Zt+1 ) + Dr (Xt,Zt+l)) - λDr (x；,Xt)
(----λ) Dr (x； , Xt) + (Dr (Xt, zt+1) - Dr (xt+1, zt+1))
α
α
(1 - λ)(ft(Xt) -ft(x；)) + 1 (Dr(Xt,Zt+ι) - Dr(Xt+1,Zt+l)) ,	(26)
Where in the second line We have used the alternative mirror descent update stated in Lemma 7, i.e.,
Vft(xt) = (1∕α)(Vr(xt) - Vr(zt+ι)). To obtain the third line, we have utilized the Bregman
divergence property in equation 24. We have used the Bregman projection property in equation 25 in
the fourth line. By omitting some negative terms, and using equation 14, we obtain the right-hand
side of equation 26.
Thus, if a > 2λ, we have
ft (Xt) - ft (Xt； ) ≤
2αλ -1 (Dr(xt,zt+1) - Dr (xt+1,zt+1
(a)
≤
2αλK-ι kxt+1-xtk
≤ 2αλ - 1 (kxt+1 - X；k + kXt - X；k)
(b) λK	,____
≤ E(1 + pLrβM)kxt-x；k,
(27)
where we have used the Lipschitz continuity of Bregman divergence to obtain inequality (a), and we
have applied Lemma 8 to obtain inequality (b). Summing equation 27 over time, we have
,N............ λK ，广工..................
RegT = Eft(xt) - ft(X；) ≤ 2αλ-1 (1 + √LrβM) E kxt - x；k.	(28)
Now, we proceed to bound PtT=1 kxt - xt； k as follows:
TT
X kxt -xt；k = kx1 -x1；k + X kxt -xt；k
t=1	t=2
T
≤ kx1 -x；1k + X kxt -xt；-1k + kxt；-1 - xt；k
t=2
(a)	二/______ 工
≤ kx1 - xlk +E VZLreMkxt-1 - x；-1k +E kx； - x；-1k,	(29)
t=2	t=2
14
Under review as a conference paper at ICLR 2021
where We used the result of Lemma 8 to obtain inequality (a). If M ≥ d (2 + 21λ) log L", We have
βM
2αλ M	-2Mαλ	1
1 -τ+αλ)	≤exp(τ+ατ) <l,
(30)
Which implies LrβM < 1. Therefore, by combining equation 29 and equation 30, We have
T
XIlxt-xtk ≤
t=1
kxi-x；k + P= kW-W-ik
ι - POM	ι - PLeM
(31)
Finally, substituting equation 31 into equation 28 completes the proof.

D Proof of Theorem 3
In order to bound the dynamic regret, We begin by the smoothness condition of the cost function
ft(.), i.e.,
ft(Xt)- ft(χ⅛) ≤ "ft(W),xt - xti + Lkxt - Wk2
≤ l∣vft(Xt)∣*kxt - Wk + 刍∣xt - Wk2.	(32)
Next, We use the fact
IVft(Xt)ItkXt - Xtk ≤ W" + θkxt -XT,	(33)
for any arbitrary positive constant θ > 0. Thus, We have
t t t f/*、VkVft(Xt) k2 I(L + 0)kXt - Xtk2
ft(Xt) - ft(Xt) ≤ —2θ— +------2-----.	(34)
Summing equation 34 over time, We obtain
R d — X ʃ / 、 ʃ / tʌ X kVft (Xt) kt	L + θ X II	t∣∣2
RegT = / ft(Xt) - ft(Xt) ≤ ɪʧ	2θ	1	2- ʌ/ kXt - Xt k .	(35)
NoW, We proceed by bounding PtT=1 kXt - Xtt k2 as folloWs:
TT
X kXt -Xttk2 = kX1 -Xt1k2 +X kXt - Xtt-1 + Xtt-1 -Xttk2
T
≤ kXi - Xlk2 + X (2kXt - Xt-Ik2 + 2kXt-i - Xtk2)
t=2
TT
≤ kX1-Xt1k2+2βMLrXkXt-Xtt-1k2+2XkXtt-1-Xttk2.	(36)
We note that if M ≥d( 11 + 击)log 2L", then 2βMLr < 1. Therefore, from equation 36 we can
obtain
T	t2	T
kX _ Xtk2 ≤ kX1	x1 k	I__2	kXt _ x* k2	(37)
々kXt - xt k ≤ 1 - 2βMLr + 1 - 2βMLr 2kxt - xt-1k .	(37)
Substituting equation 37 into equation 35 completes the proof.
15
Under review as a conference paper at ICLR 2021
E Proof of Theorem 5
The proof of Theorem 5 initially follows the first half of the proof of Theorem 2, which is repeated
here for completeness.
To analyze the dynamic regret, We first use the strong convexity of the cost function ft(∙), i.e.,
ft(xt) - ft(xt) ≤ Bftlxt),xt- x；)一 λDr(x；, Xt)
≤
≤
≤
≤
-BY(Xt)- Vr(zt+ι),xt - x；i - λDr(x；, Xt)
α
-(Dr (X；,Xt)- Dr (X；, Zt+l) + Dr (xt,Zt+ι)) — λDr (x；,Xt)
α (Dr (x；, xt) - Dr (x；, xt+1) - Dr(Xt+1,zt+1 ) + Dr(Xt, zt+l)) - λDr (x； , xt)
(----λ) Dr(Xjt , xt) +	(Dr(Xt, zt+1) - Dr(Xt+l,zt+l))
α
ft (Xt) - ft (Xt； )
α
1 - λ
α
≤
λ
+ α (Dr (xt,zt+l) - Dr (xt+1, zt+1)) ,	(38)
where in the second line we have used the alternative mirror descent update stated in Lemma 7, i.e.,
▽ft(Xt) = (1∕α)(Vr(xt) — Vr(zt+ι)). To obtain the third line, we have utilized the Bregman
divergence property in equation 24. We have used the Bregman projection property in equation 25 in
the fourth line. By omitting some negative terms, and using equation 14, we obtain the right-hand
side of equation 38.
Therefore, if α > 2λ, we have
ft(xt) - ft(Xt) ≤ 2αλ - 1 (Dr(Xt, zt+1) - Dr (xt+1, zt+1)) .
Now we continue to bound Dr(xt, zt+1). By the definition of Bregman divergence, we have
Dr(xt,zt+1) + Dr(zt+1, xt) = hVr(xt) - Vr(zt+1), xt - zt+1i
= hαVft (xt), xt - zt+1i
≤ kαVft(xt)∣∣*IlXt- zt+ιk
j a2∣Bf，m∣2 I kxt - zt+1k2
≤ -2kVft(Xt)IL +-----------------------------------2-----∙
The strong convexity of the regularization function implies
kx一J+1" ≤ r(zt+ι) - r(xt) - hVr(xt),zt+ι 一 Xti = Dr(zt+ι,xt)∙
Combining the above with equation 40, we obtain
Dr (xt,zt+l) ≤ '2 ||Vft(xt)k；.
By substituting equation 42 into equation 40, and summing over time, we have
T	2λ
RegT = Xft(xt) - ft(x；) ≤	τkVft(xt)k2∙
4αλ -2
t=1
(39)
(40)
(41)
(42)
(43)

F Closed-form Update for Mirror Descent
In this section, we derive the close-form mirror descent update in equation 6.
16
Under review as a conference paper at ICLR 2021
Let r(y) = Pjd=1 yj log(yj) be the negative entropy. Then, we have
d
Dr(y,yti) = X yjlog(yj) - yti,jlog(yti,j) - (log(yti,j) + 1)(yj -yti,j)
j=1
d
=X yj- log( yj) + h1,y - yii = DκL(y,yii),
(44)
where yti,j denotes the j-th component of the decision vector yti, and DKL (y, yti) represents the KL
divergence between y and yti .
Now consider the update in equation 5, which can be written as follows:
1d
minimizey∈χ Iyft(yit),y) +	yj log(Jyj-)
α j=1	yti,j
subject to h1, yi = 1, y ≥ 0.
The Lagrangian of the above problem is given by
d1	y
L(y,λ,γ) = hyfMyi),yi + ɪ^ [α"j log(yj)+ λyj - Yj"/一λ,
(45)
(46)
where λ ∈ R and γ ∈ Rd+ are Lagrange multipliers corresponding to the constraints. Next, we take
derivative with respect to y to obtain
L-L(y, λ, γ) = yft(yi)j + — log(yj) +------IogMj) + λ - γj.
∂yj	α	α α
Setting the above to zero results in the following closed-form update:
i+1 =	yi,j exp(-α*ft(yt,j)
如	Pd=I yi,j eχp(-αVft(yi,j))
(47)
(48)

G	Additional Experiments
In this section, we present additional experiments to study the performance of OMMD. In the first
experiment, we use the MNIST dataset. In the second experiment, we consider a switching problem
where the cost function switches between two quadratic functions after a specific number of rounds.
First, we consider the well-known MNIST digits dataset, where every data sample ω is an image of
size 28 × 28 pixel that can be represented by a 784-dimensional vector, i.e., d = 784. Each sample
corresponds to one of the digits in {0, 1, . . . , 9}, and thus, there are c = 10 different classes. The
goal of the learner is to classify streaming digit images in an online fashion.
We consider a robust regression problem, where the cost function for the batch of data samples at
time t is given by
f(x, (ωt, zt)) = kωt x - zt k1 ,
where x is the optimization variable, belonging to the constraint set is X = {x : x ∈ Rn+, k x k1= 1}.
We use the negative entropy regularization function, i.e., r(x) = Pid=1 xi log(xi), which is strongly
convex with respect to the l1-norm. We set the step size α = 0.1 and use a batch size of 20 data
examples per round.
From Fig. 3, we again observe that OMMD consistently outperforms the other alternatives. In
particular, compared with DMD, applying M = 10 steps of mirror descent can reduce the dynamic
regret up to 20%. We also see that the dynamic regret grows linearly with the number of rounds,
17
Under review as a conference paper at ICLR 2021
-j3J6au U-IUeUAa
IOO 2QQ 300	400	500	600 7QQ 800
Round
-64
⅞9j6smIUeUXo
---OGD
---OMGD, M = 2
---OMGD, M = 5
“…'OMGD, M = 10

DMD
——OMMD, M = 2
——OMMD, M = 5
……OMMD, M = 10
0	50	100	150	200	250	30Q
Round
Figure 3: Dynamic regret comparison on Figure 4: Dynamic regret comparison for
MNIST dataset.	switching cost.
which is a natural consequence of steady fluctuation in the sequence of dynamic minimizers Xj= as
explained before.
Next, we consider the case where the cost function switches between two functions. Both functions
are in the quadratic form ft(x) = kAtx - btk22, where At ∈ Rd×d, and bt ∈ Rd. In particular, we
assume that the parameter At is chosen among
A(I) = diag(ɪ, ɪ,..., ɪ, 0,0,..., 0), and	A(2) = diag(0,0,..., 0, ɪ, ɪ,..., ɪ),
t	tPl t tpι,	, tpι，、，/	，/	t	、'二 'J tPl , tpι,	, tPl "
|-------{-------}	d2	di |-----------{--------}
d1	d2
such that di + d2 = d, and bt =［焉,...,焉］0. Therefore, at each round the cost function is either
ft(1)(x) = kA(t1)x - btk22 or ft(2) (x) = kAt(2)x - bt k22 . We assume that the cost function switches
between fj(1)(∙) and 铲(∙) every T rounds. In our experiment, We set di = 10, d = 1000, pi = 0.9,
and p2 = 0.1. We further set the switching period τ = 10, and parameter α = 0.02. The dynamic
regret roughly reflects the accumulated mismatch error over time.
In Fig. 4, we compare the performance of OMMD with that of other alternatives in terms of the
dynamic regret. OMMD with M = 10 nearly halves the dynamic regret of DMD after 300 rounds.
Furthermore, the benefit of applying multiple steps of mirror descent can be significant even for
smaller values of M .
18