Under review as a conference paper at ICLR 2021
Safety Verification of Model Based
Reinforcement Learning Controllers
Anonymous authors
Paper under double-blind review
Ab stract
Model-based reinforcement learning (RL) has emerged as a promising tool for
developing controllers for real world systems (e.g., robotics, autonomous driving,
etc.). However, real systems often have constraints imposed on their state space
which must be satisfied to ensure the safety of the system and its environment.
Developing a verification tool for RL algorithms is challenging because the non-
linear structure of neural networks impedes analytical verification of such models
or controllers. To this end, we present a novel safety verification framework for
model-based RL controllers using reachable set analysis. The proposed frame-
work can efficiently handle models and controllers which are represented using
neural networks. Additionally, if a controller fails to satisfy the safety constraints
in general, the proposed framework can also be used to identify the subset of initial
states from which the controller can be safely executed.
1 Introduction
One of the primary reasons for the growing application of reinforcement learning (RL) algorithms
in developing optimal controllers is that RL does not assume a priori knowledge of the system dy-
namics. Model-based RL explicitly learns a model of the system dynamics, from observed samples
of state transitions. This learnt model is used along with a planning algorithm to develop optimal
controllers for different tasks. Thus, any uncertainties in the system, including environment noise,
friction, air-drag etc., can also be captured by the modeled dynamics.
However, the performance of the controller is directly related to how accurately the learnt model
represents the true system dynamics. Due to the discrepancy between the learnt model and the
true model, the developed controller can behave unexpectedly when deployed on the real physical
system, e.g., land robots, UAVs, etc. (Benbrahim & Franklin, 1997; Endo et al., 2008; Morimoto
& Doya, 2001). This unexpected behavior may result in the violation of constraints imposed on the
system, thereby violating its safety requirements (Moldovan & Abbeel, 2012). Thus, it is necessary
to have a framework which can ensure that the controller will satisfy the safety constraints before
it is deployed on a real system. This raises the primary question of interest: Given a set of safety
constraints imposed on the state space, how do we determine whether a given controller is safe or
not?
In the literature, there have been several works that focus on the problem of ensuring safety. Most
of these works incorporate safety constraints in the learning phase to train a controller (policy) to
satisfy certain desired specifications or constraints. However, to achieve this goal, some works make
strict assumptions on the complete or accurate knowledge of the system dynamics (Zheng & Ratliff,
2020; Hasanbeig et al., 2020) which can be difficult to obtain. Further, to incorporate safety during
learning, some works approximate the original problem to represent safety constraints in a tractable
form (Fu et al., 2018; Avni et al., 2019), which reduces the performance of the final trained controller
(FU et al., 2018; Eriksson & Dimitrakakis, 2019; Junges et al., 2016; Konighofer et al., 2020). On
the other hand, some of the works aim at finding a safe controller, under the assumption of a known
baseline safe policy (Hans et al., 2008; Garcia & Fernandez, 2012; Berkenkamp et al., 2017; Thomas
et al., 2015; Laroche et al., 2019; Zheng & Ratliff, 2020), or several known safe policies (Perkins
& Barto, 2002). However, such safe policies may not be readily available in general. Alternatively,
Akametalu et al. (2014) used reachability analysis to develop safe model-based controllers, under the
1
Under review as a conference paper at ICLR 2021
assumption that the system dynamics can be modeled using Gaussian processes, i.e., an assumption
which is violated by most modern RL methods that make use of neural networks (NN) instead.
While there have been several works proposed to develop safe controllers, some of the assumptions
made in these works may not be possible to realize in practice. In recent years, this limitation has
drawn attention towards developing verification frameworks for RL controllers, which is the focus of
this paper. The safety verification algorithm proposed in this work is a standalone framework which
makes no assumptions on how the model-based RL controller is trained. It works independently of
the training phase to identify the safe initial conditions for any given policy. One advantage of using
a standalone verification framework is that we can deploy potentially unsafe policies on real systems,
without further training, by restricting their initial conditions to only the safe states. Since verifying
safety of an NN based RL controller is also related to verifying the safety of the underlying NN
model (Xiang et al., 2018b; Tran et al., 2019b; Xiang et al., 2018a; Tran et al., 2019a), we provide
an additional review for these methods in Appendix A.1.
Contributions: In this work, we focus on the problem of determining whether a given controller
is safe or not, with respect to satisfying constraints imposed on the state space. To do so, we pro-
pose a novel safety verification algorithm for model-based RL controllers using forward reachable
tube analysis that can handle NN based learnt dynamics and controllers, while also being robust
against modeling error. The problem of determining the reachable tube is framed as an optimal
control problem using the Hamilton Jacobi (HJ) partial differential equation (PDE), whose solution
is computed using the level set method. The advantage of using the level set method is the fact that
it can represent sets with non-convex boundaries, thereby avoiding approximation errors that most
existing methods suffer from. Additionally, if a controller is deemed unsafe, we take a step further
to identify if there are any starting conditions from which the given controller can be safely exe-
cuted. To achieve this, a backward reachable tube is computed for the learnt model and, to the best
of our knowledge, this is the first work which computes the backward reachable tube over an NN.
Finally, empirical results are presented on two domains inspired by real-world applications where
safety verification is critical.
2	Problem Setting
Let S ⊂ Rn denote the set of states and A ⊂ Rm denote the set of feasible actions for the RL agent.
Let S0 ⊂ S denote the set of bounded initial states and ξ := {(st, at)}tT=0 represent a trajectory
generated over a finite time T , as a sequence of state and action tuples, where subscript t denotes
the instantaneous time. Additionaly, let s(∙) and a(∙) represent a sequence of states and actions,
respectively. The state constraints imposed on the system are represented as unsafe regions using
bounded sets Cs = ∪ip=1C(si), where C(si) ⊂ S, ∀i ∈ {1, 2, . . . p}. The true system dynamics is
given by a non-linear function f : S X A → Rn such that, S = f (s, a), and is unknown to the agent.
A model-based RL algorithm is used to find an optimal controller π : S → A, to reach a set
of target states T ⊂ S within some finite time T , while avoiding constraints Cs . An NN model,
fθ : S × A → Rn parameterized by weights θ, is trained to learn the true, but unknown, system
dynamics from the observed state transition data tuples D = {(st, at, ∆st+1)(i)}iN=1. However, due
to sampling bias, the learnt model fθ may not be accurate. We assume that it is possible to estimate
a bounded set D ⊂ Rn such that, at any state S ∈ S, augmenting the learnt dynamics fθ with some
d ∈ D results in a closer approximation of the true system dynamics at that particular state. Using
this notation, we now define the problem of safety verification ofa given controller π(S).
Problem 1 (Safety verification): Given a set of initial states S0, determine if ∀S0 ∈ S0, all the
trajectories ξ executed under π(S) and following the system dynamics f, satisfy the constraints Cs
or not.
The solution to Problem 1 will only provide a binary yes orno answer to whether π(S) is safe or not
with respect to S0. In the case where the policy is unsafe, a stronger result is the identification of safe
initial states Ssafe ⊂ S0 from which π(S) executes trajectories which always satisfy the constraints
Cs. This problem is stated below.
2
Under review as a conference paper at ICLR 2021
Problem 2 (Safe initial states): Given π(s), find Ssafe, such that, any trajectory ξ executed under
π(s) and following the system dynamics f, starting from any s0 ∈ Ssafe, satisfies the constraints
3	Safety Verification
To address Problems 1 and 2, we use reachability analysis. Reachability analysis is an exhaustive
verification technique which tracks the evolution of the system states over a finite time, known as
the reachable tube, from a given set of initial states S0 (Maler, 2008). If the evolution is tracked
starting from S0, then it is called the forward reachable tube and is denoted as FR(T). Analogously,
if the evolution is tracked starting from Cs to S0, then it is called the backward reachable tube and
is denoted as BR(T).
In the following sections, we will formulate a reachable tube problem for NN-based models and
controllers, and then propose a verification framework that (a) can determine whether or not a given
policy π is safe, and (b) can compute Ssaf e ifπ is unsafe. To do so, there are two main questions that
need to be answered. First, since the true system dynamics f is unknown, how can we determine a
conservative bound on the modeling error, to augment the learnt model fθ and better model the true
system dynamics when evaluating a controller π? Second, how do we formulate the forward and
backward reachable tube problems over the NN modeled system dynamics?
3.1	Model-Based Reinforcement Learning
In this section, We focus on the necessary requirements for the modeled dynamics fθ and discuss the
estimation of the modeling error set D. A summary of the model-based RL framework is presented
in Appendix A.2. Recall that the learnt model fθ is represented using an NN and predicts the change
in states ∆St+ι ∈ Rn. To learn f, an observed data set D = {(st, at, ∆st+ι)(i)}N=ι is first split
into a training data set Dt and a validation data set Dv . A supervised learning technique is then used
to train fθ over Dt by minimizing the prediction error E = N PN=1 l∣∆st+ι - ∆s(+ι∣∣2, where
∆St+ι is the change predicted by the learnt model, ∆st+ι is the true observed change in state and
Nt = |Dt|. With this notation, we now formalize the following necessary assumption for this work.
This assumption is required to ensure boundedness during analysis and is easily satisfied by NNs
that use common activation functions like tanh or sigmoid (Usama & Chang, 2018).
Assumption 1 fθ is Lipschitz continuous and∀j ∈ {1,…，n}, ∆Sj ∈ [—c, c], where |c| < ∞.
Modeling error: As mentioned earlier, the accuracy of the learnt model fθ depends on the quality
of data and the NN being used, thereby resulting in some modeling error d in the prediction of
the next state. Estimating modeling errors is an active area of research and is required for several
existing works on safe RL (Akametalu et al., 2014; Gillula & Tomlin, 2012), and is complementary
to our goal. Since the primary contribution of this work is the development of a reachable tube
formulation for model-based controllers that use NNs, we rely on existing techniques (Moldovan
et al., 2015) to estimate a conservative modeling error bound. We leverage the error estimates
dj = ∆st+)ι - ∆s(+)ι ,of fθ, for the transition tuples in the validation set Dv to construct the upper
confidence bound	d+	=	[d1,	d2, . .	. dn]	and lower confidence bound	d-	=	[-d1,	-d2, . . . -	dn]
for d, for each state dimension. Let a high-confidence bounded error set D be defined such that
D = {d : ∀i ∈ {1, ..., n}, di- < di < di+}. We then use D to represent the augmented learnt
system dynamics as
fθr := fθ (s, a) + d, d ∈ D.	(1)
3.2	Reachable Tube Formulation
For an exhaustive verification technique on a continuous state and action space, it is infeasible to
sample trajectories from every point in the given initial state and further, to verify whether all these
trajectories satisfy the safety constraints. Therefore, reachable sets are usually (approximately) rep-
resented as convex polyhedrons and their evolution is tracked by pushing the boundaries of this
polyhedron according to the system dynamics. However, as convex polyhedrons can lead to large
3
Under review as a conference paper at ICLR 2021
Figure 1: Flowchart of the proposed safety verification algorithm. The rectangle on the left repre-
sents the flow for computing the forward reachable tube (FRT), which can only state if π(s) is safe
or unsafe for S0 . The rectangle on the right presents the flow for computing the backward reachable
tube (BRT), which is invoked if π(s) is unsafe. The BRT analysis can compute the subset of safe
initial states Ssaf e for π(s), if such a set exists.
approximation errors, we leverage the level set method (Mitchell et al., 2005) to compute the bound-
aries of the reachable tube for NN-based models and controllers at every time instant. With the
level set method, even non-convex boundaries of the reachable tube can be represented, thereby en-
suring an accurate computation of the reachable tube (Mitchell et al., 2005). Since the non-linear,
non-convex structure of NNs is not suitable for analytical verification techniques, the reachability
analysis gives an efficient, simulation-based approach to analyze the safety of the controller. There-
fore, in this subsection, we formulate the reachable tube for a NN modeled system dynamics fθ , but
first we formally define the forward reachable tube and backward reachable tube for a policy π(s).
Forward reachable tube (FRT): It is the set of all states that can be reached from an initial set S0 ,
when the trajectories ξ are executed under policy π(s) and system dynamics f(r) (s, a, d). The FRT
is computed over a finite length of time T and is formally defined as
FR(T) := {s : ∀d ∈ D, s(∙) satisfies S = f(r)(s, a, d), where a = π(s), st0 ∈ S0,tf = T}, (2)
where t0 and tf denote the initial and final time of the trajectory ξ, respectively.
Backward reachable tube (BRT): It is the set of all states which can reach a given bounded tar-
get set T ⊂ Rn, when the trajectories ξ are executed under policy π(S) and system dynamics
fθ" (s, a, d). The BRT is also computed for a finite length of time, with the trajectories starting at
time t0 = -T and ending at time tf = 0. It is denoted as
BR(-T) := {s0 : ∀d ∈ D, s(∙) satisfies S = f(r)(s, a, d), where a = π(s)
with st0 = s-T ; stf ∈ T, tf ∈ [-T, 0]}.
(3)
The key difference between the FRT and BRT is that, for the former, the initial set of states are
known, whereas for the latter, the final set of states are known.
4
Under review as a conference paper at ICLR 2021
Outline: The flowchart of the safety verification framework proposed in this work is presented
in Fig. 1. Given a model-based policy π(s), the set of initial states S0 and the set of constrained
states Cs, the first step is to estimate the bounded set of modeling error D, as discussed in Section
3.1. Using fr), the FRT is constructed from the initial set So and it contains all the states reachable
by π(s) over a finite time T. Thus, if the FRT contains any state from the unsafe region Cs, π(s)
is deemed unsafe. Therefore, the solution to Problem 1, is determined by analyzing the set of
intersection of the FRT with Cs as
π =	safe	if	FR(T ) ∩ Cs	=	0,
π	unsafe	if	FR(T ) ∩ Cs	6=	0.
(4)
If π(s) is classified as safe for the entire set S0, then no further analysis is required. However, if
π(s) is classified as unsafe, we proceed to compute the subset Ssafe ⊂ S0 of initial states for which
π(s) generates safe trajectories. Ssafe is the solution to Problem 2 and allows an unsafe policy to be
deployed on a real system, with restrictions on the starting states. To this end, the BRT is computed
from the unsafe region Cs , to determine the set of trajectories (and states) which terminate in Cs .
The intersection of the BRT with S0 determines the set of unsafe initial states Sunsafe. To determine
Ssafe, we utilize the following properties, (a) Ssafe ∪ Sunsafe = S0, and (b) Ssafe ∩ Sunsafe = 0,
and compute
Ssafe = Sunsafe ∩ S0 ∙
(5)
If Ssafe = 0, then We have identified the safe initial states for ∏(s), otherwise, it is concluded that
there are no initial states in S0 from which π(s) can generate safe trajectories.
Mathematical Formulation: This section presents the mathematical formulation to compute the
BRT. The FRT can be computed with a slight modification to the BRT formulation and this is dis-
cussed in the end of this section.
Recall, for the BRT problem, there exists a target set T ⊂ Rn which the agent has to reach in
finite time, i.e., the condition on the final state is given as stf ∈ T. Conventionally, for the BRT
formulation, the final time tf = 0 and the starting time t0 = -T, where 0 < T < ∞. When
evaluating a policy π(s), the controller input is computed by the given policy as a = π(s). However,
following the system dynamics fr in (1), the modeling error d is now included in the system as
an adversarial input, whose value at each state is determined so as to maximize the controller’s
cost function. We use the HJ PDE to formulate the effect of the modeling error on the system for
computing the BRT, but first we briefly review the formulation of the HJ PDE with an NN modeled
system dynamics fθ in the following.
For an optimal controller, we first define the cost function which the controller has to minimize. Let
C(st, at) denote the running cost of the agent, which is dependent on the state and action taken at
time t ∈ [-T, 0]. Let g(stf) denote the cost at the final state stf. Then, the goal of the optimal
controller is to find a series of optimal actions such that
min	C(sτ, aτ)dτ + g(stf)	(6)
subject to S = fθ(s, a), Stf ∈ T,
where a「∈ A and fθ is the NN modeled system dynamics. The above optimization problem is
solved using the dynamic programming approach (Smith & Smith, 1991), which is based on the
Principle of Optimality (Troutman, 2012). Let V (St, t) denote the value function ofa state S at time
t ∈ [-T, 0], such that
V (St, t) = min
aτ G)
Zt0C(
Sτ, aτ)dτ + g(Stf)
min
aτG)
t+δ
C(Sτ, aτ)dτ + V(St+δ,t + δ)
(7)
t
where δ > 0. V (St, t) is a quantitative measure of being at a state S, described in terms of the cost
required to reach the goal state from S. Then, using the Taylor series expansion, V (St+δ, t + δ) is
approximated around V (St, t) in (7) to derive the HJ PDE as
dV
dtj- + man W ∙ fθ(s, a) + C(s, a)J = 0,
V(Stf,tf) = g(Stf),
(8)
5
Under review as a conference paper at ICLR 2021
where VV ∈ Rn is the spatial derivative of V. Additionally, the time index has been dropped above
and the dynamics constraint in (6) has been included in the PDE. Equation (8) is a terminal value
PDE, and by solving (8), we can compute the value of a state V(st, t) at any time t.
We now discuss how the formulation in (8) can be modified to obtain the BRT. It is noted that
along with computing the value function, the formulation in (8) also computes the optimal action a.
However, in Problems 1 and 2, the optimal policy π(s) is already provided. Therefore, the constraint
a = π(s) should be included in problem (6), thereby avoiding the need of minimizing over actions
a ∈ A in (8). Additionally, as discussed in Section 3.1, the NN modeled system dynamics fθ may
not be a good approximation of the true system dynamics f. Instead, the augmented learnt system
dynamics fr) in(1) is used in place of fθ in (8), since it better models the true dynamics at a given
state. However, by including f(r) in (8), the modeling error d is now included in the formulation.
The modeling error d ∈ D is treated as an adversarial input which is trying to drive the system
away from it’s goal state by taking a value which maximizes the cost function at each state. Thus,
to account for this adversarial input, the formulation in (8) is now maximized over d.
Lastly, the BRT problem is posed for a set of states and not an individual state. Hence, an efficient
representation of the target set is required to propagate an entire set of trajectories at a time, as
opposed to propagating individual trajectories.
Assumption 2 The target set T ⊂ Rn is closed and can be represented as the zero sublevel set of a
bounded and Lipschitz continuous function l : Rn → R, such that, T = {s : l(s) ≤ 0}.
The above assumption defines a function l to check whether a state lies inside or outside the target
set. If T is represented using a regular, well-defined geometric shape (like a sphere, rectangle,
cylinder, etc.), then deriving the level set function l(s) is straight forward, whereas, an irregularly
shaped T can be represented as a union of several well-defined geometric shapes to derive l(s).
For the BRT problem, the goal is to determine all the states which can reach T within a finite time.
The path taken by the controller is irrelevant and only the value of the final state is used to determine
if any state s ∈ BR (-T). From Assumption 2, the terminal condition stf ∈ T can be restated as
l(stf) ≤ 0. Thus, to prevent the system from reaching T, the adversarial input d tries to maximize
l(stf), thereby pushing stf as far away from T as possible. Therefore, the cost function in (6) is
modified to J = l(stf). Additionally, any state which can reach T within a finite time interval T is
included in the BRT. Therefore, if any trajectory reaches T at some tf < 0, it shouldn’t be allowed
to leave the set. Keeping this in mind, the BRT optimization problem can be posed as
max min l(st )
d(∙) V∈[-t,0]	fJ
subject to: S = f(r)(s, a, d), a = π(s), l(stf) ≤ 0,
(9)
where the inner minimization over time prevents the trajectory from leaving the target set. Then, the
value function for the above problem is defined as
VR(st, t) := max l(s(tf)).
d(∙)
(10)
Comparing this with (7), it is observed that the value of a state s is no longer dependent on the
running cost C(s, a). This doesn’t imply that the generated trajectories are not optimal w.r.t. action
a, because the running cost is equivalent to the negative reward function, for which π(s) is already
optimized. Instead, VR solely depends on whether the final state of the trajectory lies within the
target set or not, i.e., whether or not stf ∈ T. Thus, the value function VR for any state s is equal
to l(stf), where stf is the final state of the trajectory originating at s. Then, the HJ PDE for the
problem in (9) is stated as
dVR + min{0, H*(s, VVR(St, t), t)} = 0,
dt
VR(Stf,tf)=l(Stf),
where H * = max (VVR ∙ f(r)(s, ∏(s), d)),
(11)
where H * represents the optimal Hamiltonian. Since We are computing the BRT,
min{0, H* (S, VVR(St, t), t)} in the PDE above ensures that the tube grows only in the backward
6
Under review as a conference paper at ICLR 2021
Figure 2: (Left) The environment for the safe land navigation problem. (Middle) The FRTs for both
the augmented learnt model dynamics and true system dynamics classify the controller as unsafe.
(Right) The BRT computed from obstacles 1 and 2 for the given controller. Ssaf e computed by the
proposed BRT algorithm is compared with the ground truth (G.T.) data for the safe initial states,
marked in green.
direction, thereby preventing a trajectory which has reached T from leaving. In Problem (11), the
optimal action has been substituted by π(s) and the augmented learnt dynamics is used instead of
fθ. The Hamiltonian optimization problem can be further simplified to derive an analytical solution
for the modeling error. By substituting the augmented dynamics from (1), the optimization problem
can be re-written as
，	个，、	,
H* = VVR ∙ fθ(s, a)+ max VVR ∙ d.	(12)
d
Expanding VVR = [p1,p2,... ,pn]T ∈ Rn, the vector product VVR ∙ d = p1 d1 + p2d2 +... + Pndn.
Therefore, to maximize VVR ∙ d, the disturbance control is chosen as
di	ifpi >0
di =	, ∀i = 1, . . . n.
-di ifpi < 0
With this analytical solution, the final PDE representing the BRT is stated as
dVR + min{0, H * (s, VVR (St ,t),t)} = 0,
VR(stf,tf)=l(stf),
where H = VVR ∙ fθ(s, α)+(∣Pι∣dι + ∣P2∣d2 + …+ ∣Pn∣dn).
(13)
(14)
The value function VR(st, t) in (14) represents the evolution of the target level set function back-
wards in time. By finding the solution to VR in the above PDE, the level set function is determined at
any time instant t ∈ [-T, 0], thereby determining the BRT. From the result of Theorem 2 in Mitchell
et al. (2005), it is proved that the solution of VR in (14) at any time t gives the zero sublevel set for
the BRT. Thus,
Br(-T) = {s : VR(St,t) ≤ 0, t ∈ [-T, 0]}.	(15)
The solution to VR can be computed numerically by using existing solvers for the level set method.
A brief note on the implementation of the algorithm is included in subsection A.3 in the Appendix.
There are a few things to note about the formulation in (14). First, Equation (14) assumes that T is a
desired goal state. However, the formulation can be modified ifT is an unsafe set, in which case, the
adversarial modeling error tries to minimize the Hamiltonian. Similarly, the input d can represent
any other disturbance in the system, either adversarial or cooperative. Second, to compute the FRT,
the formulation in (14) is modified from a final value PDE to an initial value PDE.
4	Experiments
In our experiments, we aim to answer the following questions: (a) Can safety verification be done
for an NN-based π and fθ using FRT?, and (b) Can Ssafe be identified using BRT if π is deemed un-
safe? To answer these two questions, we demonstrate results on the following domains inspired by
7
Under review as a conference paper at ICLR 2021
real world safety-critical problems, where RL controllers developed using a learnt model can be ap-
pealing, as they can adapt to transition dynamics involving friction, air-drag, wind, etc., which might
be hard to explicitly model otherwise. It is noted that since the proposed verification framework is
developed for control-oriented tasks for physical systems, the state representation of such systems
comprises of position, velocity and orientation data. Therefore, the state dimensions of such class
of problems are typically not as large as the popular image-based OpenAI or Deepmind domains.
Instead, the results are demonstrated on experimental domains which are similar to the ones in prior
works on safety verification of NN controllers for physical systems (Xiang et al., 2018b; Xiang &
Johnson, 2018; Akintunde et al., 2018; 2019).
Safe land navigation: Navigation of a ground robot in an indoor environment is a common ap-
plication which requires the satisfaction of safety constraints by π to avoid collision with obstacles.
For this setting, we simulate a ground robot which has continuous states and actions. The initial
configuration of the domain is shown in Fig. 2. The set of initial and goal states are represented by
circles and the obstacles with rectangles.
Safe aerial navigation: This domain simulates a navigation problem in an urban environment
for an unmanned aerial vehicle (UAV). Constraints are incorporated while training π to ensure that
collision is avoided with potential obstacles in its path. States and actions are both continuous and
the initial configuration of the domain is shown in Fig 3. The set of initial and goal states are
represented using cuboids, and the obstacles with cylinders.
Analysis: To address the questions with respect to the above mentioned domains, we first train a
NN based fθ to estimate the dynamics using sampled transitions. This fθ is then also used to learn
a NN based controller π which is trained with a cost function designed to mitigate collisions. For
brevity, only the representative results for this π are discussed here; implementation details and more
experimental results are available in Appendix A.2, A.3, A.4 and A.5.
To address the first question, the FRT is computed for both the domains over the augmented learnt
dynamics f(r) as shown in Fig. 2 and Fig. 3. Additionally, for land navigation We also compute the
FRT over the true system dynamics f, which serves as a way to validate the safety verification result
of π from the proposed framework. It is observed that for both the domains, FRTs deem the given
policy π as unsafe, since the FRTs intersect with one of the obstacles. Even when π is learnt using a
cost function designed to avoid collisions, the proposed safety verification framework successfully
brings out the limitations of π, which may have resulted due to the use of function approximations,
ill-specified hyper-parameters, convergence to a local optimum, etc.
For the second question, the BRT is computed from both the obstacles for the given controller π,
as shown in Fig. 2 and Fig. 3. To estimate the accuracy of the BRT computation, we compare the
computed Sunsaf e and Ssaf e sets with the ground truth (G.T.) data generated using random samples
of possible trajectories. It is observed that the BRT from obstacle 1 does not intersect with S0,
implying that all trajectories are safe w.r.t. obstacle 1. However, the BRT from obstacle 2 intersects
with S0 and identifies the subset of initial states which are unsafe. The set of unsafe initial states
computed by the BRT algorithm may not be exact, as is seen in Fig. 3 where the BRT computation
over approximates Sunsafe . Such an information can be critical to safely deploy even an unsafe
controller just by restricting its starting conditions.
5	Conclusion
In this paper, we have presented a novel framework using forward and backward reachable tubes
for safety verification and determination of the subset of initial states for which a given model-
based RL controller always satisfies the state constraints. The main contribution of this work is
the formulation of the reachability problem for a neural network modeled system dynamics and the
use of level set method to compute an exact reachable tube by solving the Hamilton-Jacobi partial
differential equation, for the reinforcement learning framework, thereby minimizing approximation
errors that other existing reachability methods suffer. Additionally, the proposed framework can
identify the set of safe initial sets for a given policy, thereby determining the initial conditions for
which even a sub-optimal, unsafe policy satisfies the safety constraints.
8
Under review as a conference paper at ICLR 2021
Figure 3: (Left) The safe aerial navigation domain. (Middle) The FRT is computed for the aug-
mented learnt dynamics. For clarity in 3D, FRT with true dynamics is not plotted; instead the
approximation quality can be better visualized in BRT. (Right) Comparison of Ssaf e computed by
the proposed BRT algorithm with the ground truth data of the safe initial states, marked in green
(and unsafe states marked in red), it is observed that the BRT over approximates Sunsaf e .
While the results from the proposed framework are promising, there is still room for improvement.
One of the drawbacks of using the level set method is the fact that it scales poorly with the increasing
dimension of the state space. Recent progress in addressing the scalability issue includes decompo-
sition of system dynamics into subsystems which can later be coupled via common states or controls
(Bansal et al., 2017; Margellos & Lygeros, 2011; Chen et al., 2018). Additionally, the application
of reachability analysis in developing safe learning based controllers is also a promising direction
(Akametalu et al., 2014; Fisac et al., 2018).
References
Anayo K Akametalu, Jaime F Fisac, Jeremy H Gillula, Shahab Kaynama, Melanie N Zeilinger,
and Claire J Tomlin. Reachability-based safe learning with gaussian processes. In 53rd IEEE
Conference on Decision and Control, pp. 1424-1431. IEEE, 2014.
Michael Akintunde, Alessio Lomuscio, Lalit Maganti, and Edoardo Pirovano. Reachability analysis
for neural agent-environment systems. In KR, pp. 184-193, 2018.
Michael E Akintunde, Andreea Kevorchian, Alessio Lomuscio, and Edoardo Pirovano. Verifica-
tion of rnn-based neural agent-environment systems. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pp. 6006-6013, 2019.
Guy Avni, Roderick Bloem, KriShnendu Chatterjee, Thomas A Henzinger, Bettina KOnighofer, and
Stefan Pranger. Run-time optimization for learned controllers through quantitative games. In
International Conference on Computer Aided Verification, pp. 630-649. Springer, 2019.
Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: A brief
overview and recent advances. In 2017 IEEE 56th Annual Conference on Decision and Control
(CDC), pp. 2242-2253. IEEE, 2017.
Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya Nori, and An-
tonio Criminisi. Measuring neural net robustness with constraints. In Advances in neural infor-
mation processing systems, pp. 2613-2621, 2016.
Hamid Benbrahim and Judy A Franklin. Biped dynamic walking using reinforcement learning.
Robotics and Autonomous Systems, 22(3-4):283-302, 1997.
Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In Advances in neural information processing
systems, pp. 908-918, 2017.
9
Under review as a conference paper at ICLR 2021
Mo Chen, Sylvia L Herbert, Mahesh S Vashishtha, Somil Bansal, and Claire J Tomlin. Decomposi-
tion of reachable sets and tubes for a class of nonlinear systems. IEEE Transactions on Automatic
Control, 63(11):3675-3688, 2018.
Michael G Crandall and P-L Lions. Two approximations of solutions of hamilton-jacobi equations.
Mathematics of computation, 43(167):1-19, 1984.
Souradeep Dutta, Susmit Jha, Sriram Sankaranarayanan, and Ashish Tiwari. Learning and verifi-
cation of feedback control systems using feedforward neural networks. IFAC-PapersOnLine, 51
(16):151-156, 2018.
Souradeep Dutta, Xin Chen, and Sriram Sankaranarayanan. Reachability analysis for neural feed-
back systems using regressive polynomial rule inference. In Proceedings of the 22nd ACM Inter-
national Conference on Hybrid Systems: Computation and Control, pp. 157-168, 2019.
Gen Endo, Jun Morimoto, Takamitsu Matsubara, Jun Nakanishi, and Gordon Cheng. Learning cpg-
based biped locomotion with a policy gradient method: Application to a humanoid robot. The
International Journal of Robotics Research, 27(2):213-228, 2008.
Hannes Eriksson and Christos Dimitrakakis. Epistemic risk-sensitive reinforcement learning. arXiv
preprint arXiv:1906.06273, 2019.
Jaime F Fisac, Anayo K Akametalu, Melanie N Zeilinger, Shahab Kaynama, Jeremy Gillula, and
Claire J Tomlin. A general safety framework for learning-based control in uncertain robotic
systems. IEEE Transactions on Automatic Control, 64(7):2737-2752, 2018.
Michael Fu et al. Risk-sensitive reinforcement learning: A constrained optimization viewpoint.
arXiv preprint arXiv:1810.09126, 2018.
Javier Garcia and Fernando Fernandez. Safe exploration of state and action spaces in reinforcement
learning. Journal of Artificial Intelligence Research, 45:515-564, 2012.
Jeremy H Gillula and Claire J Tomlin. Guaranteed safe online learning via reachability: track-
ing a ground target using a quadrotor. In 2012 IEEE International Conference on Robotics and
Automation, pp. 2723-2730. IEEE, 2012.
Alexander Hans, Daniel SChneegaβ, Anton Maximilian Schafer, and Steffen Udluft. Safe explo-
ration for reinforcement learning. In ESANN, pp. 143-148, 2008.
Mohammadhosein Hasanbeig, Alessandro Abate, and Daniel Kroening. Cautious reinforcement
learning with logical constraints. arXiv preprint arXiv:2002.12156, 2020.
Chao Huang, Jiameng Fan, Wenchao Li, Xin Chen, and Qi Zhu. Reachnn: Reachability analysis of
neural-network controlled systems. ACM Transactions on Embedded Computing Systems (TECS),
18(5s):1-22, 2019.
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural
networks. In International Conference on Computer Aided Verification, pp. 3-29. Springer, 2017.
Radoslav Ivanov, James Weimer, Rajeev Alur, George J Pappas, and Insup Lee. Verisig: veri-
fying safety properties of hybrid systems with neural network controllers. In Proceedings of the
22nd ACM International Conference on Hybrid Systems: Computation and Control, pp. 169-178,
2019.
Sebastian Junges, Nils Jansen, Christian Dehnert, Ufuk Topcu, and Joost-Pieter Katoen. Safety-
constrained reinforcement learning for mdps. In International Conference on Tools and Algo-
rithms for the Construction and Analysis of Systems, pp. 130-146. Springer, 2016.
Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An
efficient smt solver for verifying deep neural networks. In International Conference on Computer
Aided Verification, pp. 97-117. Springer, 2017.
Yafim Kazak, Clark Barrett, Guy Katz, and Michael Schapira. Verifying deep-rl-driven systems. In
Proceedings of the 2019 Workshop on Network Meets AI & ML, pp. 83-89, 2019.
10
Under review as a conference paper at ICLR 2021
Bettina Konighofer, Roderick Bloem, Sebastian Junges, Nils Jansen, and Alex Serban. Safe re-
inforcement learning using probabilistic shields. In International Conference on Concurrency
Theory: 31st CONCUR 2020: Vienna, Austria (Virtual Conference). Schloss Dagstuhl-Leibniz-
Zentrum fur Informatik GmbH, Dagstuhl Publishing, 2020.
Marta Z Kwiatkowska. Safety verification for deep neural networks with provable guarantees. 2019.
Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with
baseline bootstrapping. In International Conference on Machine Learning, pp. 3652-3661.
PMLR, 2019.
Oded Maler. Computing reachable sets: An introduction. Tech. rep. French National Center of
Scientific Research, 2008.
Kostas Margellos and John Lygeros. Hamilton-jacobi formulation for reach-avoid differential
games. IEEE Transactions on Automatic Control, 56(8):1849-1861, 2011.
Ian M Mitchell. A toolbox of level set methods. UBC Department of Computer Science Technical
Report TR-2007-11, 2007.
Ian M Mitchell, Alexandre M Bayen, and Claire J Tomlin. A time-dependent hamilton-jacobi formu-
lation of reachable sets for continuous dynamic games. IEEE Transactions on automatic control,
50(7):947-957, 2005.
Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. arXiv
preprint arXiv:1205.4810, 2012.
Teodor Mihai Moldovan, Sergey Levine, Michael I Jordan, and Pieter Abbeel. Optimism-driven
exploration for nonlinear systems. In 2015 IEEE International Conference on Robotics and Au-
tomation (ICRA), pp. 3239-3246. IEEE, 2015.
Jun Morimoto and Kenji Doya. Acquisition of stand-up behavior by a real robot using hierarchical
reinforcement learning. Robotics and Autonomous Systems, 36(1):37-51, 2001.
Stanley Osher and James A Sethian. Fronts propagating with curvature-dependent speed: algorithms
based on hamilton-jacobi formulations. Journal of computational physics, 79(1):12-49, 1988.
Stanley Osher, Ronald Fedkiw, and K Piechor. Level set methods and dynamic implicit surfaces.
Appl. Mech. Rev., 57(3):B15-B15, 2004.
Theodore J Perkins and Andrew G Barto. Lyapunov design for safe reinforcement learning. Journal
of Machine Learning Research, 3(Dec):803-832, 2002.
Luca Pulina and Armando Tacchella. Challenging smt solvers to verify neural networks. Ai Com-
munications, 25(2):117-135, 2012.
James Albert Sethian. Level set methods and fast marching methods: evolving interfaces in compu-
tational geometry, fluid mechanics, computer vision, and materials science, volume 3. Cambridge
university press, 1999.
David K Smith and David K Smith. Dynamic programming: a practical introduction. Ellis Horwood
New York, 1991.
Xiaowu Sun, Haitham Khedr, and Yasser Shoukry. Formal verification of neural network controlled
autonomous systems. In Proceedings of the 22nd ACM International Conference on Hybrid Sys-
tems: Computation and Control, pp. 147-156, 2019.
Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy
improvement. In International Conference on Machine Learning, pp. 2380-2388, 2015.
toolbox. helperoc. https://github.com/HJReachability/helperOC, May 2019.
Hoang-Dung Tran, Feiyang Cai, Manzanas Lopez Diego, Patrick Musau, Taylor T Johnson, and
Xenofon Koutsoukos. Safety verification of cyber-physical systems with reinforcement learning
control. ACM Transactions on Embedded Computing Systems (TECS), 18(5s):1-22, 2019a.
11
Under review as a conference paper at ICLR 2021
Hoang-Dung Tran, Patrick Musau, Diego Manzanas Lopez, Xiaodong Yang, Luan Viet Nguyen,
Weiming Xiang, and Taylor T Johnson. Parallelizable reachability analysis algorithms for feed-
forward neural networks. In 2019 IEEE/ACM 7th International Conference on Formal Methods
in Software Engineering (FormaIiSE),pp. 51-60. IEEE, 2019b.
John L Troutman. Variational calculus and optimal control: optimization with elementary convexity.
Springer Science & Business Media, 2012.
Muhammad Usama and Dong Eui Chang. Towards robust neural networks with lipschitz continuity.
In International Workshop on Digital Watermarking, pp. 373-389. Springer, 2018.
Weiming Xiang and Taylor T Johnson. Reachability analysis and safety verification for neural
network control systems. arXiv preprint arXiv:1805.09944, 2018.
Weiming Xiang, Hoang-Dung Tran, and Taylor T Johnson. Output reachable set estimation and
verification for multilayer neural networks. IEEE transactions on neural networks and learning
systems, 29(11):5777-5783, 2018a.
Weiming Xiang, Hoang-Dung Tran, Joel A Rosenfeld, and Taylor T Johnson. Reachable set estima-
tion and safety verification for piecewise linear systems with neural network controllers. In 2018
Annual American Control Conference (ACC), pp. 1574-1579. IEEE, 2018b.
Guoqing Yang, Guangyi Qian, Pan Lv, and Hong Li. Efficient verification of control systems with
neural network controllers. In Proceedings of the 3rd International Conference on Vision, Image
and Signal Processing, pp. 1-7, 2019.
Liyuan Zheng and Lillian J Ratliff. Constrained upper confidence reinforcement learning. arXiv
preprint arXiv:2001.09377, 2020.
12