Under review as a conference paper at ICLR 2021
Learned ISTA with Error-based Thresholding
for Adaptive Sparse Coding
Anonymous authors
Paper under double-blind review
Ab stract
The learned iterative shrinkage thresholding algorithm (LISTA) introduces deep
unfolding models with learnable thresholds in the shrinkage function for sparse
coding. Drawing on some theoretical insights, we advocate an error-based thresh-
olding (EBT) mechanism for LISTA, which leverages a function of the layer-wise
reconstruction error to suggest an appropriate threshold value for each observation
on each layer. We show that the EBT mechanism well-disentangles the learnable
parameters in the shrinkage functions from the reconstruction errors, making them
more adaptive to the various observations. With rigorous theoretical analyses, we
show that the proposed EBT can lead to faster convergence on the basis of LISTA
and its variants, in addition to its higher adaptivity. Extensive experimental results
confirm our theoretical analyses and verify the effectiveness of our methods.
1	Introduction
Sparse coding is widely used in many machine learning applications (Xu et al., 2012; Dabov et al.,
2007; Yang et al., 2010; Ikehata et al., 2012), and its core problem is to deduce the high-dimensional
sparse code from the obtained low-dimensional observation, for example, under the assumption of
y = Axs + ε, where y ∈ Rm is the observation corrupted by the inevitable noise ε ∈ Rm, xs ∈ Rn
is the sparse code to be estimated, and A ∈ Rm×n is an over-complete dictionary matrix. To recover
xs purely from y is called sparse linear inverse problem (SLIP). The main challenge for solving
SLIP is its ill-posed nature because of the over-complete modeling, i.e., m < n. A possible solution
to SLIP can be obtained via solving a LASSO problem using the l1 regularization:
min ky - Axk2 + λkxk1.	(1)
x
Possible solutions for Eq. (1) are iterative shrinking thresholding algorithm (ISTA) (Daubechies
et al., 2004) and its variants, e.g., fast ISTA (FISTA) (Beck & Teboulle, 2009). Despite their sim-
plicity, these traditional optimization algorithm suffer from slow convergence speed in large scale
problems. Therefore, Gregor & LeCun (2010) proposed the learned ISTA (LISTA) which was a deep
neural network (DNN) whose architecture followed the iterative process of ISTA. The thresholding
mechanism was modified into shrinkage functions in the DNNs together with learnable thresholds.
LISTA achieved superior performance in sparse coding, and many theoretical analyses have been
proposed to modify LISTA to further improve its performance (Chen et al., 2018; Liu et al., 2019;
Zhou et al., 2018; Ablin et al., 2019; Wu et al., 2020).
Yet, LISTA and many other deep networks based on it suffer from two issues. (a) Though the
thresholds of the shrinkage functions in LISTA were learnable, their values were shared among
all training samples and thus lack adaptability to the variety of training samples and robustness
to outliers. According to prior work (Chen et al., 2018; Liu et al., 2019), the thresholds should
be proportional to the upper bound of the norm of the current estimation error to guarantee fast
convergence in LISTA. However, outliers with drastically higher estimation errors will affect the
thresholds more, making the learned thresholds less suitable to other (training) samples. (b) For
the same reason, it may also lead to poor generalization to test data with different distribution (or
sparsity (Chen et al., 2018)) from the training data. For instance, in practice, we may only be given
some synthetic sparse codes but not the real ones for training, and current LISTA models may fail
to generalize under such circumstances.
In this paper, we propose an error-based thresholding (EBT) mechanism to address the aforemen-
tioned issues of LISTA-based models to improve their performance. Drawing on theoretical insights,
1
Under review as a conference paper at ICLR 2021
EBT introduces a function of the evolving estimation error to provide each threshold in the shrink-
age functions. It has no extra parameter to learn compared with original LISTA-based models yet
shows significantly better performance. The main contributions of our paper are listed as follows:
•	The EBT mechanism can be readily incorporated into popular sparse coding DNNs (e.g.,
LISTA (Gregor & LeCun, 2010) and LISTA with support selection (Chen et al., 2018)) to
speed up the convergence with no extra parameters.
•	We give a rigorous analysis to prove that the estimation error of EBT-LISTA (i.e., a com-
bination of our EBT and LISTA) and EBT-LISTA with support selection (i.e., a combina-
tion of our EBT and LISTA with support selection) is theoretically lower than the original
LISTA (Gregor & LeCun, 2010) and LISTA with support selection (Chen et al., 2018),
respectively. In addition, the introduced parameters in our EBT are well-disentangled from
the reconstruction errors and need only to be correlated with the dictionary matrix to ensure
convergence. These results guarantee the superiority of our EBT in theory.
•	We demonstrate the effectiveness of our EBT in the original LISTA and several of its vari-
ants in simulation experiments. We also show that it can be applied to practical applications
(e.g., photometric stereo analysis) and achieve superior performance as well.
The organization of this paper is structured as follows. In Section 2, we will review some preliminary
knowledge of our study. In Section 3, we will introduce a basic form of our EBT and several of its
improved versions. Section 4 provides a theoretical study of the convergence of EBT-LISTA. Exper-
imental results in Section 5 valid the effectiveness of our method in practice. Section 6 summarizes
this paper.
2	Background and Preliminary Knowledge
As mentioned in Section 1, ISTA is an iterative algorithm for solving LASSO in Eq. (1). Its update
rule is: x(0) = 0 and
x(t+1) = shλ∕γ((I - ATA∕γ)χ(t) + ATy∕γ), ∀t ≥ 0,	⑵
where Shb(X) = Sign(X)(|x| - b)+ is a shrinkage function with a threshold b ≥ 0 and (•)+ =
max{0, ∙}, Y is a positive constant scalar greater than or equal to the maximal eigenvalue of the
symmetric matrix ATA. LISTA kept the update rule of ISTA but learned parameters via end-to-end
training. Its inference process can be formulated as X(0) = 0 and
X(t+1) = Shb(t) (W (t)X(t) + U(t)y), t = 0, . . . , d,	(3)
where Θ = {W(t), U(t), b(t)}t=0,...,d is a set of learnable parameters, and, specifically, b(t) is the
layer-wise threshold which is learnable but shared among all samples. LISTA achieved lower recon-
struction error between its output and the ground-truth Xs compared with ISTA, and it is proved to
convergence linearly (Chen et al., 2018) with W(t) = I - U(t)A holds for any layer t. Thus, Eq. (3)
can be written as.
X(t+1) = Shb(t) ((I - U (t) A)X(t) + U (t)y), t = 0, . . . , d.	(4)
Chen et al. (2018) further proposed support selection for LISTA, which introduced Shp(b(t),p) (X)
whose elements are defined as
(sign(xi)(∣Xi| - b), if |xi| > b,i ∈ Sp
(Shp(b(t),p)(X))i = Xi,	if |Xi| > b, i ∈ Sp ,	(5)
(0,	otherwise
to substitute the original shrinking function Shb(t) (X), where Sp is the set of the index of the largest
p% elements (in absolute value) in vector X. Formally, the update rule of LISTA with support
selection is formulated as X(0) = 0 and
X(t+1) = Shp(b(t),p(t))((I- U A)X(t) + U (t)y), t = 0, . . .,d,	(6)
where p(t) is a hyper-parameter and it increases from early layers to later layers. LISTA with support
selection can achieve faster convergence compared with LISTA (Chen et al., 2018).
2
Under review as a conference paper at ICLR 2021
Theoretical studies (Chen et al., 2018; Liu et al., 2019) also demonstrate that the threshold of LISTA
and its variants should satisfy the equality
b(t) 一 μ(A) sup ∣∣x(t) - Xskp	(7)
xs∈S
to ensure fast convergence in the noiseless case (i.e., ε = 0), where S is the training set and μ(A) is
the general mutual coherence coefficient of the dictionary matrix A. Note that μ(A) is a crucial term
in this paper, here we formally give its definition together with the definition of W(A) as follows.
Definition 1 For A ∈ Rm×n, its generalized coherence coefficient is defined as μ(A) =
inf W ∈Rn×m,Wi,a,i=ι maxi=j ∣(Wi,: A:,j )|, and we say W ∈ W (A) if maxi=j (Wi,：A：,j) = μ(A).
3 Methods
In LISTA and its variants, the threshold b(t) is commonly treated as a learnable parameter. As
demonstrated in Eq. (7), b(t) should be proportional to the upper bound of the estimation error of the
t-th layer in the noiseless case to ensure fast convergence. Thus, some outliers or extreme training
samples largely influence the value of b(t), making the obtained threshold not fit the majority of the
data. To be specific, We know that the suggested value of b(t) is b(t) = μ(A)supi=0,ι,...,n∣x(t) 一
xsi kp for kxk1a training set {xsi}i=0,1,...,n, and normal training of LISTA leads toitin theory (Chen
et al., 2018). Yet, if a new training sample xsn+1 with higher reconstruction error is introduced, the
expected b(t) shall change to μ(A)kχηt++ι 一 Xsn+1 kp, which is probably undesirable for the other
samples. Similar problems occur if there exists a large variety in the value of reconstruction errors.
In order to solve this problem, we propose to disentangle the reconstruction error term from the
learnable part of the threshold and introduce adaptive thresholds for LISTA and related networks.
We attempt to rewrite the threshold at the t-th layer as something like
b㈤=P㈤ kx⑴一 Xskp,
(8)
where P(t) is a layer-specific learnable parameter. However, the ground-truth Xs is actually unknown
for the inference process in SLIP. Therefore, we need to find an alternative formulation. Notice that
in the noiseless case, it holds that AX(t) 一 y = A(X(t) 一 Xs), thus we further rewrite Eq. (8) into
b⑴=P㈤kQ(Ax(t)-y)kp,
(9)
where Q ∈ Rn×m is a compensation matrix introduced to let Eq. (9) approximate Eq. (8) better, i.e.,
a matrix that makes QA approaches the identity matrix is more desired. However, note that although
QAis a low-rank matrix and can never be an identity matrix, we can encourage its diagonal elements
to be 1 and the non-diagonal elements to be nearly zero. This can be directly achieved by letting
Q ∈ W(A), where W(A) is defined in Definition 1. According to some prior works (Liu et al.,
2019; Wu et al., 2020; Chen et al., 2018), we also know that U(t) ∈ W(A) to guarantee linear
convergence, thus U(t) can probably be a reasonable option for the matrix Q in our method, making
it layer-specific as well. Therefore, our EBT-LISTA is formulated as X(0) = 0 and
X(t+1) = shb(t)((I 一 U(t)A)X(t) + U(t)y), t = 0,...,d,
b(t) =P(t)kU(t)(AX(t) 一 y)kp.
(10)
Note that only P(t) and U(t) are learnable parameters in the above formulation, thus our EBT-LISTA
actually introduces no extra parameters compared with the original LISTA. The architecture of
LISTA and EBT-LISTA are shown in Figure 1. We can also apply our EBT mechanism on LISTA
with support selection (Chen et al., 2018). It is straightforward to keep the support set selection
operation and replace the fixed threshold with our EBT, and such a combination can be similarly
formulated as X(0) = 0 and
X(t+1) = shp(b(t),p(t))((I 一 U (t) A)X(t) + U (t)y),	t = 0, . . . , d,
b(t) =P(t)kU(t)(AX(t) 一 y)kp.
(11)
3
Under review as a conference paper at ICLR 2021
(a) LISTA
(b) EBT-LISTA
Figure 1: The t-th layer of LISTA and EBT-LISTA.
Our former analysis is based on the noiseless case. For noise case, there is A(x(t) - xs) = Ax(t) -
y + ε. Since the noise is generally unknown in practical problems, we may add an extra learnable
parameter on the threshold to compensate the noise, i.e.,
b(t) =ρ(t)kU(t)(Ax(t) -y)kp+α(t),	(12)
where α(t) is the learnable parameter for the observation noise.
4 Theoretical Analysis
In this section, we provide convergence analyses for LISTA and LISTA with support selection.
We focus on the noiseless case and the main results are obtained under a mild assumption of the
ground-truth sparse code. To be more specific, we assume that the ground-truth sparse vector xs is
sampled from the distribution γ(B, s), i.e., the number of its nonzero elements follows a uniform
distribution U (0, s) and the magnitude of its nonzero elements follows a arbitrary distribution in
[-B, B]. Compared with the assumptions made in some recent work (Chen et al., 2018; Liu et al.,
2019; Wu et al., 2020), i.e., X(B, s) = {xs |kxs k0 ≤ s, kxs k∞ ≤ B}, our assumption here provides
a more detailed yet also stricter description of the distribution of xs, especially for the sparsity of
xs . In fact, it can be easily derived that our assumption can also be rewritten in the form of some
prior assumptions. In particular, for ∀xs ∈ γ(B, s), there exist xs ∈ X(B, s) := {xs|kxs k0 ≤
s, kxs k∞ ≤ B} (Wu et al., 2020). We will mention the condition that s is sufficiently small for
error-based thresholding, which means μ(A)s ≪ 1 specifically.
4.1	Error-based Thresholding on LISTA
Let us first discuss the convergence of LISTA and how our EBT improves LISTA in accelerating
convergence. Proof of all our theoretical results can be found in Appendix A.2. To get started, we
would like to recall the theoretical guarantee of the convergence of LISTA, which was given in the
work of Chen et al. (2018)’s.
Lemma 1 (Chen et al., 2018). For LISTA formulated in Eq. (4), ifxs is sampled from γ(B, s) and s
is small such that μ(A)(2s-1) < L with U ⑺ ∈ W (A),assume that b(t = μ(A)supχs ∣∣x⑺—Xs ∣∣ι
is achieved to guarantee the “no false positive” property (i.e. supp(x(t)) ⊂ supp(xs)), then the
estimation x(t) at the t-th layer of LISTA satisfies
∣x(t) — xs∣2 ≤ sB exp (c0t),
where co = log((2s — 1)μ(A)) < 0.
The above lemma shows that the reconstruction error of LISTA decreases at a rate of c0 and the
reconstruction error bound is also related to s and B. With Lemma 1, we further show in the
following theorem that the convergence of EBT-LISTA is similarly bounded from above.
Theorem 1 (Convergence of EBT-LISTA) For EBT-LISTA formulated in Eq. (10), for p = 1, if
xs is sampled from γ(B, s) and s is sufficiently small, with U(t) ∈ W (A), assume ρ(t) =
μ(A)
1 -μ(A)s
is achieved to guarantee the “no false positive” property, then the estimation x(t) at the t-th layer
satisfies
∣x(t) — xs∣2 ≤ q0 exp (c1t),
where qo < SB and ci < co hold with the probability of 1 — μ(A)s.
4
Under review as a conference paper at ICLR 2021
From Theorem 1 we know that EBT-LISTA converges similarly with a rate of c1, which is probably
faster than that of the original LISTA. In addition, we show that its reconstruction error at each layer
with an index t is also probably lower than that of the original LISTA, with qo < sB. Since μ(A)
is generally small in practical and S is assumed to be sufficiently small such that μ(A)s《 1, the
probability of achieving the superiority is very high in theory. We will further show in experiments
that even if s is not that small (i.e., the sparsity is not that high), our EBT can still achieve favorable
improvement on the basis of LISTA. Moreover, unlike the desired threshold in the original LISTA
(i.e., μ(A) SuPxskx(t) — Xs ∣∣ι) which depends on specific training samples, the desired threshold in
EBT-LISTA is disentangled with the reconstruction error.
4.2	Error-based Threshold on LISTA with support selection
Effective prior work from Chen et al. (2018) proposed to improve the performance of LISTA with
the operation of support set selection, and it helps to achieve lower error bound when compared with
the original LISTA. We first give a detailed discussion on the convergence of LISTA combined with
support selection before we delve deeper into the theoretical study of how combining our EBT with
them further improves the performance.
Lemma 2 (Convergence of LISTA with support selection) For LISTA with support selection as
formulated in Eq. (6), if xs is sampled from γ(B, s) and s is sufficiently small, with U(t) ∈ W (A),
assume that b(t) = μ(A) SuPxSkx(t) 一 xskι is achieved andp(t) is sufficiently large, there actually
exist two convergence phases.
In the first phase, i.e., t ≤ t0, the t-th layer estimation x(t) satisfies
kx(t) 一 xsk2 ≤ sB exP (c2t),
where co ≤ log((2s — 1)μ(A)). In the second phase, i.e., t > to, the estimation x(t) satisfies
kx㈤-Xs∣2 ≤ Ckx(I) -xs∣2,
where C ≤ sμ(A).
Lemma 2 shows that when powered with support selection, LISTA shows two different convergence
phases. The earlier phase is generally slower and the later phase is faster. On the basis of the
theoretical results in Lemma 2, we further show in the following theorem that the convergence of
EBT-LISTA with support selection is similarly bounded and also has two phases.
Theorem 2 (Convergence of EBT-LISTA with support selection) For EBT-LISTA with support
selection andp = 1, if xs is sampled from γ(B, s) and s is sufficiently small, with U(t) ∈ W (A),
assume ρ(t) = ι-μ((A)s is achieved andp(t) is sufficiently large, there exist two convergence phases.
In the first phase, i.e., t ≤ t1, the t-th layer estimation x(t) satisfies
kx(t) 一 xsk2 ≤ q1exP(c3t),
where c3 < c2, qι < SB and tι < to hold with a probability of 1 一 μ(A)s. In the second phase, i.e.,
t > t1, the estimation x(t) satisfies
kx(t)— xs∣2 ≤ Ckx(I) 一 xs∣2,
where C ≤ sμ(A).
The above theorem shows that further incorporated with EBT, the model shall also show two dif-
ferent phases of convergences. In addition, it processes the same rate of convergence in the second
phase, comparing with the results of the original LISTA with support selection in Lemma 2, while in
the first phase, our EBT leads to faster convergence and faster enter the second phase, which shows
the effectiveness of our EBT in LISTA with support selection in theory.
5 Experiments
We conduct extensive experiments on both synthetic data and real data to validate our theorem and
testify the effectiveness of our methods. The network architectures and training strategies in our ex-
periments most follow those of prior works (Chen et al., 2018; Wu et al., 2020). To be more specific,
5
Under review as a conference paper at ICLR 2021
all the compared networks have d = 16 layers and their learnable parameters W(t), U(t), ρ(t)(or b(t)
without out EBT) are not shared between layers. The training batch size is 64, and we use the pop-
ular Adam optimizer (Kingma & Ba, 2014) for training with its default hyper-parameters β1 = 0.9,
β2 = 0.999. The training is performed from layer to layer in a progressive way, i.e., if the validation
loss of the current layer does not decrease for 4000 iterations, the training will move to the next
layer. When training each layer, the learning rate is first initialized to be 0.0005 and it will then
decrease to 0.0001 and finally decrease to 0.00001, if the validation loss does not decrease for 4000
iterations. Specifically, in our proposed methods, we impose the constraint between W (t) and U(t)
and make sure it holds that W (t) = I - U(t)A, ∀t, i.e., the coupled constraints are introduced for
all the evaluated models. For models empowered with support selection, we will append “SS” to
their name for clarity, i.e., LISTA with support selection is renamed “LISTA-SS” in the following
discussions. The value of ρ(t) is initialized to be 0.02.
5.1	Simulation Experiments
Basic settings. In simulation experiments, we setm = 250, n = 500, and we generate the dictionary
matrix A by using the standard Gaussian distribution. The indices of the non-zero entries in xs are
determined by a Bernoulli distribution letting its sparsity (i.e., the probability of any of its entry
be zero) be pb , while the magnitudes of the non-zero entries are also sampled from the standard
Gaussian distribution. The noise ε is sampled from a Gaussian distribution where the standard
deviation is determined by the noise level. With y = Axs +ε, we can randomly synthesize in-stream
xs and get a corresponding set of observations for training. Similarly, we also synthesize two sets
for validation and test, respectively, each contains 1000 samples. The sparse coding performance of
different models is evaluated by the normalized mean squared error (NMSE) in decibels (dB):
NMSE(X,xs ) = 10 ιog10 (kx-k2rk2
(13)
Disentanglement. First we would like to compare the learned parameters (i.e., b(t) and p(t)) for
thresholds in both LISTA and our EBT-LISTA. Figure 2(a) shows how the learned parameters (in
a logarithmic coordinate) vary with the index of layers in LISTA and EBT-LISTA. Note that the
mean values are removed to align the range of the parameters of different models on the same y-
axis. It can be seen that the obtained values for the parameter in EBT-LISTA do not change much
from lower layers to higher layers, while the reconstruction errors in fact decrease. By contrast, the
obtained threshold values in LISTA vary a lot across layers. Such results imply that the optimal
thresholds in EBT-LISTA are indeed independent to (or say disentangled from) the reconstruction
error, which confirms the theoretical result in Theorem 1. Similar observation can also be made
on LISTA-SS (i.e., LISTA with support selection) and our EBT-LISTA-SS (i.e., EBT-LISTA with
support selection), as shown in Figure 2(b).
(a) LISTA	(b) LISTA-SS
Figure 2:	Disentanglement of the reconstruction error and learnable parameters in our EBT. z(t)
here indicates ρ(t) and b(t) for networks with or without EBT, respectively.
We analyze the obtained threshold values in .EBT-LISTA and EBT-LISTA-SS, i.e., b(t) =
ρ(t)kU(t) (Ax(t) - y)kp (with p = 2), and we compare them with the thresholds values obtained
in LISTA and LISTA-SS. Note that the threshold values in our EBT-based models differ from sam-
ple to sample, we show the results in Figure 3. It can be seen that the learned thresholds in our
EBT-based methods and the original LISTA and LISTA-SS are similar, which indicates that the in-
troduced EBT mechanism does not modify the training dynamics of the original methods, and our
EBT works by disentangling the reconstruction error and learnable parameters.
6
Under review as a conference paper at ICLR 2021
LISTA
EBT-LISTA
4	6	8101214
Index of layers
-10 —'-	--------：---
0	2	4	6	8	IC
Index of layers
(a) LISTA	(b) LISTA-SS
Figure 3:	Thresholds obtained from different methods across layers.
.
Validation of Theorem 2. Figure 4(a) shows how the NMSE of EBT-LISTA-SS varies with the
index of layers. Besides the l1 norm (i.e., b(t) = ρ(t) kU(t)(Ax(t) - y)k1) concerned in the theorem,
we also test EBT-LISTA-SS with the l2 norm (i.e., letting b(t) = ρ(t) kU (t) (Ax(t) - y)k2). It can
be seen that, with both the l1 and l2 norms, EBT-LISTA-SS leads to consistently faster convergence
than LISTA. Also, itis clear that there exist two convergence phases for EBT-LISTA-SS and LISTA-
SS, and the later phase is indeed faster. With faster convergence, EBT-LISTA-SS finally achieves
superior performance. The experiment is performed in the noiseless case with pb = 0.95. Similar
observations can be made on the basis of other variants of LISTA (e.g., ALISTA, see Figure 4(b)).
O 2	4	6	8 IO 12 14 16
Index of layers
LISTA-SS
EBT-LISTA-SS-∕ι
EbT-LISTA-SS-Z2
—∣;BT-Λ1.ISTΛ-∕1
—ebt-λlistλ-∕2
0	2	4	6
Index of layers
(a) LISTA-SS	(b) ALISTA
Figure 4:	Validation of Theorem 2: there exist two convergence phases and our EBT accelerates the
convergence of LISTA-SS, in particular in the first phase.
.
Adaptivity to unknown sparsity. As have been mentioned, in some practical scenarios, there may
exist a gap between the training and test data distribution, orwe may not know the distribution of real
test data and will have to train on synthesized data based on the guess of the test distribution. Under
such circumstances, itis of importance to consider the adaptivity/generalization of the sparse coding
model (trained on a specific data distribution or with a specific sparsity) to test data sampled from
different distributions with different sparsity. We conduct an experiment to test in such a scenario,
in which we let the test sparsity be different from the training sparsity. Figure 5 shows the results
in three different settings. The black curves represent the optimal model when LISTA is trained on
exactly the same sparsity as that of the test data. It can be seen that our EBT has huge advantages in
such a practical scenario where the adaptivity to un-trained sparsity is required, and the performance
gain of LISTA is larger when the distribution shift between training and test is larger (cf. purple line
and yellow line in Figure 5(a) and 5(b)).
(mp)msnN
(a) pb changes from 0.8 to 0.99
-0	2	4	6	8	10 12 14 16
Index of layers
(b) pb changes from 0.8 to 0.9
u0 2	4	6	8 10 12 14 16
Index of layers
(c) pb changes from 0.9 to 0.8
Figure 5: NMSE of different methods when the test sparsity is different from the training sparsity.
We use “Origin” to indicate the optimal scenario where the LISTA models are trained on exactly the
same sparsity as that of the test data.
Comparison with competitors. Here we compare EBT-LISTA-SS, EBT-LISTA, and EBT-ALISTA
with other methods comprehensively. In addition to LISTA-SS, LISTA, and ALISTA, we also com-
pare with learned AMP (LAMP) (Borgerding et al., 2017) here. The performance of different net-
7
Under review as a conference paper at ICLR 2021
works under different noise levels are shown in Figure 6. It can be seen that when combined with
LISTA and its variants, our EBT achieves better or similar performance. Figure 6(a) demonstrates
that the combination of ALISTA and EBT performs the best in the noiseless case (i.e., SNR=∞), yet
it is inferior to the other networks when noise presents. The figures also show that the performance
of our EBT is more promising in the noiseless or low noise cases (i.e., SNR=∞ and SNR=40dB),
while in a very noisy scenarios it provides little help.
Index of layers
Oooooo
1 2 3 4 5 6
------
Mp)msnN
LlSlA-SS
ALISTA
EBT-LISTA
1 BT-I.ISTA-SS
ERT-AI ISTA
(a) SNR=∞	(b) SNR=40dB	(c) SNR=20dB
Figure 6:	NMSE of different sparse coding methods under different noise levels. It can be seen that
our EBT performs favorably well under SNR=∞ and SNR=40dB.
We further test different networks under different sparsity and different condition numbers. Note that
for the methods with support selection (i.e., EBT-ALISTA, EBT-LISTA-SS, ALISTA, and LISTA-
SS), p and pmax are set as 0.6 and 6.5 when pb = 0.95, and are set as 1.2 and 13.0 when pb = 0.9.
Figure 7 demonstrates some of the results in different settings, while more results can be found in
Appendix A.1. In all settings, we can see that our EBT leads to significantly faster convergence. In
addition, the superiority of our EBT-based models is more significant with a larger pb for which the
assumption of a sufficiently small s is more likely to hold (comparing Figure 7(a), 9(a), and 9(b)).
We also tried training with pb = 0.99, yet we found that some classical models failed to converge in
such a setting so the results are now shown here.
0	2	4	6	8 10 12 14 16
Index of layers
(a) pb=0.95
O 2	4	6	8 10 12 14 16
Index of layers
(b) condition number=3
0	2	4	6	8 10 12 14 16
Index of layers
(c) condition number=100
Figure 7:	NMSE of different sparse coding methods in different settings where different sparsity and
different condition numbers are considered. When we vary the condition numbers, we fix pb=0.9.
5.2 Photometric Stereo Analysis
We also consider a practical sparse coding task: photometric stereo analysis (Ikehata et al., 2012).
The task solves the problem of estimating the normal direction of a Lambertian surface, given q
observations under different light directions. It can be formulated as
o = ρLn + e,	(14)
where o ∈ Rq is the observation, n ∈ R3 represents the normal direction of the Lambertian surface
which is to be estimated, L ∈ Rq ×3 represents the normalized light directions, e is the noise vector,
and ρ is the diffuse albedo scalar. Although the normal vector n is unconstrained in Eq. (14), the
noise vector e is found to be generally sparse (Wu et al., 2010; Ikehata et al., 2012). Therefore, we
may estimate the noise e first. We introduce the orthogonal complement of L, denoted by Lt, to
rewrite Eq. (14) as
Lto=ρLtLn+Lte=Lte.	(15)
On the basis of the above equation, the estimation of e is basically a sparse coding problem in the
noiseless case, where Lt is the dictionary matrix A, e is the sparse code xs to be estimated in the
8
Under review as a conference paper at ICLR 2021
reformulated problem, and Lto is the observation y. Once We have achieved a reasonable estimation
of e, we can further obtain n by using the equation n = Lt(o - e).
In this experiment, We mainly folloW the settings in Xin et al. 2016’s and Wu et al. 2020’s Work. We
use the same bunny picture for evaluation and L is also randomly selected from the hemispherical
surface. We set the number of observations q to be 15, 25, and 35 and let the training sparsity ofe be
pt = 0.8. The final performance is evaluated by calculating the average angle betWeen the estimated
normal vector and the ground-truth normal vector (in degree). Since the distribution of the noise is
generally unknoWn in practice, the adaptivity is of great importance for this task. We use tWo test
settings for evaluating different models, in Which the sparsity of the noise in test data (i.e., pe) is set
as 0.8 and 0.9. We compare EBT-LISTA-SS, LISTA-SS, and tWo conventional methods including
using the least squares (codenamed: ls) and least 1-norm (codenamed: l1) in Table 1. Also, We
build the reconstruction 3D error maps for LISTA-SS and EBT-LISTA-SS, shoWn in Figure 8 in the
appendix. The results shoW that EBT-LISTA-SS outperforms all the competitors in all the concerned
settings, note that the advantage is remarkable When pe = 0.9, Which means our EBT-based netWork
has better adaptivity and can be more effective in this practical tasks.
Table 1: Mean error (in degree) With different number of observations and different test sparsity
pe	q	ls	l1	LISTA-SS	EBT-LISTA-SS
	15	3.41	0.678	5.50 × 10-2	4.09 × 10-2
0.8	25	3.05	0.408	7.48 × 10-3	3.17 × 10-3
	35	2.78	0.336	1.89 × 10-3	5.95 × 10-4
	15	1.94	0.232	6.67 × 10-3	2.57 × 10-3
0.9	25	0.145	2.03	1.33 × 10-3	1.64 × 10-4
	35	1.61	0.088	2.93 X 10-4	4.91 × 10-5
6 Conclusion
In this paper, We have studied the thresholds in the shrinkage functions of LISTA. We have proposed
a novel EBT mechanism that Well-disentangles the learnable parameter in the shrinkage function
on each layer of LISTA from its layer-Wise reconstruction error. We have proved theoretically that,
in combination With LISTA and its variants, our EBT mechanism leads to faster convergence and
achieves superior final sparse coding performance. Also, We have shoWn that the EBT mechanisms
endoW deep unfolding models higher adaptivity to different observations With a variety of sparsity.
Our experiments on both synesthetic data and real data have testified the effectiveness of our EBT,
especially When the distribution of the test data is different from that of the train data. We hope to
extend our EBT mechanism to more complex tasks in future Work.
References
Pierre Ablin, Thomas Moreau, Mathurin Massias, and Alexandre Gramfort. Learning step sizes
for unfolded sparse coding. In Advances in Neural Information Processing Systems, pp. 13100—
13110, 2019.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM journal on imaging sciences, 2(1):183-202, 2009.
Mark Borgerding, Philip Schniter, and Sundeep Rangan. Amp-inspired deep netWorks for sparse
linear inverse problems. IEEE Transactions on Signal Processing, 65(16):4293-4308, 2017.
Xiaohan Chen, Jialin Liu, Zhangyang Wang, and Wotao Yin. Theoretical linear convergence of un-
folded ista and its practical Weights and thresholds. In Advances in Neural Information Processing
Systems, pp. 9061-9071, 2018.
K Dabov, A Foi, V Katkovnik, and K Egiazarian. Image denoising by sparse 3-d transform-domain
collaborative filtering. IEEE Transactions on Image processing, 16(8):2080-2095, 2007.
9
Under review as a conference paper at ICLR 2021
Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint. Communications on Pure and Applied
Mathematics: A Journal Issued by the Courant Institute OfMathematical Sciences, 57(11):1413-
1457, 2004.
Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the
27th International Conference on International Conference on Machine Learning, pp. 399-406.
Omnipress, 2010.
Satoshi Ikehata, David Wipf, Yasuyuki Matsushita, and Kiyoharu Aizawa. Robust photometric stere-
o using sparse regression. In 2012 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 318-325. IEEE, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jialin Liu, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Alista: Analytic weights are as good as
learned weights in lista. In International Conference on Learning Representations (ICLR), 2019.
Kailun Wu, Yiwen Guo, Ziang Li, and Changshui Zhang. Sparse coding with gated learned ista. In
Proceedings of the International Conference on Learning Representations, 2020.
Lun Wu, Arvind Ganesh, Boxin Shi, Yasuyuki Matsushita, Yongtian Wang, and Yi Ma. Robust pho-
tometric stereo via low-rank matrix completion and recovery. In Asian Conference on Computer
Vision, pp. 703-717. Springer, 2010.
Bo Xin, Yizhou Wang, Wen Gao, David Wipf, and Baoyuan Wang. Maximal sparsity with deep
networks? In Advances in Neural Information Processing Systems, pp. 4340-4348, 2016.
Xu Xu, Xiaohan Wei, and Zhongfu Ye. Doa estimation based on sparse signal recovery utilizing
weighted l1-norm penalty. IEEE signal processing letters, 19(3):155-158, 2012.
Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma. Image super-resolution via sparse
representation. IEEE transactions on image processing, 19(11):2861-2873, 2010.
Joey Tianyi Zhou, Kai Di, Jiawei Du, Xi Peng, Hao Yang, Sinno Jialin Pan, Ivor W Tsang, Yong Liu,
Zheng Qin, and Rick Siow Mong Goh. Sc2net: Sparse lstms for sparse coding. In Thirty-Second
AAAI Conference on Artificial Intelligence, 2018.
10
Under review as a conference paper at ICLR 2021
A Appendix
(a) pb=0.8, LISTA-SS: ζ=0.055	(b) pb=0.8, EBT-LISTA-SS: ζ=0.041
(c) pb=0.9, LISTA-SS: ζ=0.0067	(d) pb=0.9, EBT-LISTA-SS: ζ=0.0026
Figure 8: Reconstruction 3D error maps of different methods in different settings. ζ here is the mean
estimation error in degree. Note that the maximal error is 0.1 and 0.03 in theory when Pb = 0.8 and
Pb = 0.9, respectively.
A.1 Additional Simulation Results
Comparison with competitors. More experiment results than those in Figure 7 are given here.
Figure 9 shows the results of our methods with EBT (i.e. EBT-LISTA, EBT-LISTA-SS, and EBT-
ALISTA) and other competitors in settings including Pb=0.9, Pb=0.8 (i.e. the sparsity is 0.9 and
0.8), and when the condition number is set as 3 (with Pb = 0.9). For those concerned methods with
support selection (EBT-ALISTA, EBT-LISTA-CPSS, ALISTA, and LISTA-CPSS), P and Pmax are
set as 1.2(1.5) and 13(16.25) when Pb = 0.9(0.8). From Figure 9, we can find that our EBT leads to
better performance as shown in the results in the main paper.
16
(a) pb=0.9
O 2	4	6	8 IO 12
Index of layers
(b) pb=0.8
0	2	4	6	8 10 12 14 16
Index of layers
Figure 9: NMSE of different sparse coding methods in different settings where different sparsity
and different condition numbers are considered.
0	2	4	6	8 10 12 14 16
Index of layers
(c) condition number=30
Random sparsity. We also consider the scenario where the sparsity of the data follows a certain
distribution. We test With two distributions of sparsity (i.e., Pb): Pb 〜U(0.9,1) (uniform distribu-
tion) and Pb 〜 N(0.95, 0.025) ∈ [0.9,1] (truncated normal distribution). The comparison results
in such settings are shown in Figure 10. Our EBT lead to huge advantages in all the methods and
setting, indicating that the conclusion in Theorem 1 can be extended to broader distributions of the
data sparsity.
11
Under review as a conference paper at ICLR 2021
Ooooooooo
12345678
Ξ Z - Ξ -
(HP)工SnN
LlSlA-SS
ALISTA
EB1-LIS1-A
EBT-LISTA-SS
EBT-ALISTA
Index of layers
Index of layers
(a) Uniform distribution
(b) Truncated normal distribution
—LISTA
LISTA-SS
*---ALLSTA-
-EBT-LISIA
j EBT-LISTΛ-SS
j EBT-ΛLISTΛ
Figure 10: NMSE of different sparse coding methods when the sparsity of the data follows a certain
distribution.
EBT mechanism on (F)ISTA. We further conduct our proposed EBT mechanism on standard ISTA
and FISTA. Note that ISTA and FISTA are nonlineared convergence. We set the scalar γ (in Eq. 2)
as a constant and the regularization coefficients λ (in Eq. 1 and Eq. 2) as 0.1 and 0.2. In our
proposed methods, We set λ∣∣Aχ(t) - y∣∣1 /γ as the thresholds, ComParaed With λ∕γ in (F)ISTA.
From the experiment results shown in Figure 11, we can find that our EBT mechanism leads to faster
ConvergenCe. HoWever the fast ConvergenCe have negative influenCe on the final performanCes. That
is beCause the ConvergenCe speed of ISTA and FISTA are sub-lineared, While our EBT meChanism
is proposed for linear ConvergenCe methods. Therefore, EBT-(F)ISTA might ConvergenCe too fast to
reaCh a better performanCe.
A.2 Proof of Theorems
We first give some important notations before We delve into the proofs. The support set is defined as
the index set of non-zero values of x and it is Written as supp(x). We define S as the support set of
the veCtor xs, and We further let |S| denote the element number ofS. We denote (x)i as i-th element
of the veCtor x and denote (M)ij as the element from the i-th roW and j-th Column of the matrix M.
A.2. 1 Proof of Theorem 1
Assume i ∈/ S, i.e., (xs)i = 0. If (x(t+1))i 6= 0 , note that y = Axs, there is,
b(t)=ρ(t)∣U(t)(Ax(t)-y)∣1< |(x(t+1))i|
< |[(I-U(t)A)x(t) +U(t)Axs]i|
=|[(I-U(t)A)(x(t)-xs)+xs]i|
≤∣[(I- U⑴A)(X⑴-Xs)]i∣ + ∣(xs)i|
=|[(I-U(t)A)(x(t)-xs)]i|
=|X(I-U(t)A)ij(X(t)-Xs)j|	(16)
j
≤ X l(I - U⑴A)ij(x(t)-xs)j|
j
≤ X μ(A)I(X⑴-Xs )j |
j
≤ μ(A)∣x(t) - xs∣ι.
From above, we also have [(I — U(t)A)(X⑴—xs)]i ≤ μ(A)∣x(t) — xs∣1, therefore we have ∣∣(I —
U⑴A)(X⑴一Xs)∣ι ≤ ∣S∣μ(A)∣∣X⑴一Xs∣∣ι. SinceIU⑴A(X⑴一Xs)∣∣ι = Il(X⑴-Xs) - (I -
U(t)A)(X(t) - Xs)∣1, we have
(1 -|S∣μ(A))∣∣X⑴一Xskι ≤ ∣∣U⑴A(X⑴-Xs)IlI ≤ (1 + ∣S∣μ(A))∣∣X⑴一Xs||「	(17)
12
Under review as a conference paper at ICLR 2021
10	20	30	40	50	60	70	80	90 100
Iterations
0	10	20	30	40	50	60	70	80	90 100
Iterations
(a) λ=0.1	(b) λ=0.2
Figure 11: NMSE of different sparse coding methods where different regularization coefficients λ
are considered.
Since Q⑴ 一 μ(A)	≥ —μ(A)— there is
Since P = 1-μ(A)s ≥ 1-μ(A)∣S|，there is
P⑴IlU⑴A(X⑴一Xs)∣∣ι ≥ μ(A)∣∣χ(t) - Xskι.	(18)
Eq. (16) and (18) are conflicted, which means (x(t+1))i 一 0 if (xs)i 一 0 (i.e. supp(x(t+1)) ⊂ S),
which means our EBT-LISTA is also ”no false positive”.
From Eq. (10), we have
x(t+1) — xs = shb(t)((I — U(t)A)x(t) + U⑴y) — Xs
一 (I - U(t)A)X(t) + U(t)AXs - Xs - b(t)	h(X(t+1))	(19)
一 (I-U(t)A)(X(t) - Xs) - b(t)	h(X(t+1)),
where h(X) 一 1 if X > 0, h(X) 一 -1 if X < 0 and h(X) ∈ [-1, 1] if X 一 0. For the i-th element of
X(t+1) - Xs, we have
|(X(t+1) - Xs)i| 一 |(I-U(t)A)(X(t)-Xs)i-b(t)h(Xi(t+1))|	(20)
≤ ∣(I - U⑴A)(X⑴一xs)i| + |b(t)].
Since supp(X(t+1)) ⊂ S, we have IX(t+1) - Xs I1 一 P|iS (X(t+1) - Xs)i|. Thus we have
S
kX(t+1) - Xski ≤ X(I(I- U(t)A)(X(t) - Xs)i| + |b(t)|)
i
SS
一 X(IX(I- U⑴A)ij(X(t)-Xs)jI + 1b(t)I)
ij
SS
≤XXI(I-U(t)A)ij(X(t)-Xs)jI+ISIIb(t)I	(21)
≤ (|S I- 1)μ(A)kX(t) - Xs ki + |S|p(t) kU(t) A(X(t) - Xs )ki
≤ (IS I- 1)μ(A)kX(t) - Xs ki + 1μ-A(A)S kU ⑴ A(X(t)-Xs )kι∙
≤ (IS I + IS 11 + μ(A)S - 1)μ(A)k(X(t) - Xs )ki.
The final step holds because ISI ≤ s and Eq.(17) hold. The l2 error bound of t-th output of EBT-
LISTA can be calculated as
kX(t) -Xsk2 ≤ kX(t) - Xski
≤ ((IS I + IS 11 + μ(A)S - 1)μ(A))tk(X(O)-Xs )ki	(22)
≤ q0 exp(cit),
13
Under review as a conference paper at ICLR 2021
where qo = ∣∣Xs ∣∣ι and ci = log((∣S| + |S| 1-μ(A)S 一 1)μ(A)). Compare ci with co, we have
|S |
exp(co) ― exp(ci) = 2μ(A)(s -  ----rɪ) > 0
1 — μ(A)s
hold when |S| < s(1 — μ(A)s). Under this circumstance, We have
qo = ∣∣Xs11 ≤ |S|B < s(1 — μ(A)s)B ≤ sB.
(23)
(24)
Note that xs is sampled from γ(B, s), Eq. (23) and (24) hold with the probability with of 1 - η,
where
s-|S|
η =------
s
=μ(A)s.
(25)
A.2.2 Proof of Lemma 2
For LISTA with support selection formulated as Eq.(6), there is
x(t+i) — xs = shp(b(t) ,p(t)) ((I — U (t) A)x(t) + U (t)y) — xs
= (I — U(t)A)x(t) + U(t)Axs —xs — b(t)	g(x(t+i))	(26)
(I — U(t)A)(x(t) —xs) — b(t)	g(x(t+i)),
where
{0,i ∈ Sp,xi = 0
1, i ∈/ Sp,xi > 0
—1,i ∈Sp,Xi < 0.
[—1, 1], xi = 0
For the i-th element of x(t+i) — xs, we have
|(x(t+i) —xs)i| = |(I—U(t)A)(x(t) — xs)i — b(t)	g(xi(t+i))|
≤ |(I—U(t)A)(x(t) — xs)i| + |b(t)	g(xi(t+i))|.
(27)
(28)
Since b(t) = μ(A) SuPxSkx(t) — Xs∣ι, same as the standardLISTA, LISTA with support selection
is also ”no false positive”. Therefore, supp(x(t+i)) ⊂ S and ∣x(t+i) — xs ∣i = PiS (x(t+i) — xs)i.
Similar to Eq.(21), we have
S
kχ(t+1) — xski ≤ X(∣(I — U(t)A)(x(t) — xs)i∣ + |b(t) ©g(x(t+I))D
i
SS
= X(| X(I — U(t)A)ij(x(t) — xs)j| + |b(t) © g(xi(t+i))|)
i j	(29)
SS	S
≤	XX I(I - U(t)A)ij(x(t) - Xs)jI + X |b(t) ©g(χ(t+1))l
i j 6=i	i
S
≤	(∣S∣ - 1)μ(A)kχ(t) - Xskl + X |b(t) © g(χit+1))∣.
i
From Eq.(27), we have |g(xi(t+i))| ≤ 1 and g(xi(t+i)) = 0 only if i ∈ Sp and xi(t+i) 6= 0. We let
St+i denote the number of non-zero entries in x(t+i). Also, Pt+i denotes the number of the largest
p(t+i)% elements (in absolute value) in x(t+i). Therefore the number of zero entries in g(x(t+i)) is
min(St+i, Pt+i). Then Eq.(29) can be calculated as
S
kx(t+I)-XskI ≤ (∣S∣- 1)μ(A)kχ(t) - Xski + X |b(t) © g(xit+1))∣
i
≤ (∣S∣ - 1)μ(A)kx(t) - Xski + (|S| - min(St+ι,Pt+ι))∣b(t)∣
= (ISI -I)μ(A)kx(t) - Xski + (ISI- min(St+i,pt+i'))μ(A) SuP ∣∣x(t) - xs Il i.
xs
(30)
14
Under review as a conference paper at ICLR 2021
Then, we take the supremum of Eq.(30), there is
sup ∣∣x(t+1) - Xski ≤ (2s - 1 — min(St+1,Pt+1))μ(A)sup ∣∣x(t) - Xs∣∣ι∙	(31)
xs	xs
Note that ∣xs ∣1 ≤ sB and assume k = argmint(St, Pt), the l2 upper bound of t-th output can be
calculated as
∣x(t) - xs∣2 ≤ ∣x(t) - xs∣1 ≤ sup ∣x(t) - xs∣1
xs
≤ ( Y(2s — 1 — min(Si,Pi))μ(A) j sup ∣∣x(0) — Xs∣ι
i=1	xs
≤ ((2s — 1 — min(Sk,Pk))μ(A))tsB
≤ sB exp(c2t),
(32)
where c2 = log((2s — 1 — min(Sk, Pk))μ(A)). Apparently, wehave c2 ≤ c0 =log((2s — 1)μ(A)).
From Eq.(32), we have ∣x(t) — xs ∣1 ≤ sB exp(c2t), which means l1 error bound can approaches
to 0. Thus, there exists a t*, when t > t*, ∣x(t) — Xs∣ι ≤ mini∈s(xs)i. Note that |xi(t) — (xs )i | ≤
∣x(t) — xs∣1. If i ∈ S, i.e., (xs)i 6= 0, there exists xi(t) 6= 0, which means S ⊂ supp(x(t)).
Recall the ”no false positive” property, i.e., supp(x(t)) ⊂ S, we can conclude that supp(x(t)) = S.
Recall Pt increases layerwise and p(t) is sufficiently large, there exists t0 statisfies Pt0 > s, we let
to = max(t*,t0), if t > to, there exists Pt ≥ |S| and SuPP(X(t)) = S. Under this circumstance, if
i ∈ S, we have xi(t) 6= 0 and i ∈ Spt, which means every element in S will be selected as support.
Therefore, we have
Xi(t+1) — (Xs)i = shp(b(t),p(t))(((I — U (t) A)X(t) + U(t)AXs)i) — (Xs)i
= ((I — U(t)A)X(t) + U(t)AXs)i — (Xs)i	(33)
= ((I — U(t)A)(X(t) — Xs))i.
We let XS ∈ R|S| denote the vector that keeps the elements with indices of X in S and remove the
others. Similarly, We let M (S, S) ∈ RlSl×lSl denote the submatrix of matrix M which keeps the
row and column if the index belongs to S . Then, we have
∣X(t+1) — Xs∣2 = ∣(X(t+1) — Xs)S∣2
= ∣((I — U(t)A)(X(t) — Xs))S∣2
=k(I — U (t)A)(S, S )(x(t) — Xs)S ∣2	(34)
≤k(I - U (t)A)(S, S )∣2k(x(t) - xs)s k2
= C∣(X(t) — Xs)∣2,
where C = ∣∣(I — U(t)A)(S, S)12. Furtherwehave C ≤ ∣∣(I — U(t)A)(S, S)∣f ≤ P∣S∣2μ(A)2 ≤
sμ(A).
A.2.3 Proof of Theorem 2
For the EBT-LISTA with support selection formulated in Eq. (11), similar to Eq. (26) and (28), we
have
|(X(t+1) —Xs)i|=|(I—U(t)A)(X(t) — Xs)i — b(t)	g(Xi(t+1))|	(35)
≤ |(I—U(t)A)(X(t) — Xs)i| + |b(t)	g(Xi(t+1))|,
where b(t) = ρ(t)∣∣U(t)(Ax(t) — y)∣ι =］彳/)§ ∣∣U(t)A(x(t) — Xs)∣ι. Same as the origin EBT-
LISTA, EBT-LISTA with support selection is also”no false positive” and Eq. (17) hold either. There-
fore ∣∣U(t)A(x(t) — Xs)∣ι ≤ (1 + |S∣μ(A))∣x(t) — Xs∣ι ≤ (1 + sμ(A))∣x(t) — Xs∣ι. Similar to
15
Under review as a conference paper at ICLR 2021
Eq. (29) and (30), there is
S
kx(t+1) - xsk1 = X |(x(t+1) - xs)i|
i
S
≤ X(I(I- U㈤A)(X㈤-χs)i∣ + 步㈤ Θ g(χ(t+1))∣)
i
SS
X(|X(I-U(t)A)ij(x(t) - xs)j | + |b(t) Θ g(xi(t+1))|)
ij
SS	S
≤ XX I(I - U(t)A)ij(X⑴-Xs)jI + X |b(t) Θ g(x(t+1))∣
i j 6=i	i
≤ (|S| - 1)μ(A)kx㈤-Xski + (∣s∣- min(St+ι,Pt+ι))∣b(t)∣
≤ (∣S∣- 1)μ(A)kx(t) - Xski + (∣S∣- min(St+ι,Pt+ι))μ(A)1 + "^ k(x(t) - Xs)ki
1 — μ(A)s
≤ "	2"、ISI- 1 + μ(A)Smin(St+i,Pt+i) - 1)〃(A)k(X(t)-Xs)ki.
1 — μ(A)s	1 — μ(A)s
(36)
Similar to Eq. (32), the l2 error bound can be calculated as
kX(t) -Xsk2 ≤ kX(t) -Xski
≤ Ytl [(1	2mʌ ISI- 1 + μ(A)Smin(Si,Pi) - 1)μ(A)1 k(X⑼-Xs)ki
g[1 — μ(A)S	1 - μ(A)S	」
≤ (1 Y1 ʌ ISI- 1+ μ(A)Smin(Sk,Pk) - 1)μ(A)] t kXski
1 — μ(A)S	1 — μ(A)S	_|
≤ qi exp(c3t),
(37)
where qi = IEki and c3 = log(( i-∕a)s ⑸一1+μ(A)smin(Sk,Pk) - 1)μ(A)).
Compare c3 with c2 , we have
exp(c2) - exp(c3) = 2μ(A)(S -
≥ 2μ(A)(S -
IS I +
1 — μ(A)S
m⅛ASmin(Sk ,Pk))
IS I
1 — μ(A)S
)>0
(38)
hold when IS I < s(1 - μ(A)S). Under this circumstance, We have
qi = ∣Xski ≤ IS IB < s(1 - μ(A)S)B ≤ sB.
Note that Xs
where
is sampled from γ(B, S), Eq. (38) and (39) hold with the probability with of 1
η
S-ISI
S
(39)
- η,
(40)
μ(A)S.
Similar to LISTA with support selection, there exists a t**, when t > t**, ∣X(t) - Xski ≤
mini∈S(Xs)i. Therefore, supp(X(t)) = S. Recall that c3 < c2 holds with the probability of
1 - η, we have t** < t* hold with the probability of 1 - η. When we use the same settings for p(t)
as LISTA-SS, we have same t0 satisfying Pt0 > S. Let ti = max(t**, t0), we have ti ≤ t0 with the
probability of 1 - η. When t > ti, we have Xi(t) 6= 0 and i ∈ Spt. Same as Eq. (33) and (34), there
is
kX(t+i) - Xsk2 ≤ Ck(X(t) - Xs)k2,	(41)
where C = k(I - U(t)A)(S, S)k2 ≤ k(I - U(t)A)(S, S)∣f ≤ P6I2μ(A)2 ≤ s〃(A).
16