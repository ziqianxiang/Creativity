Under review as a conference paper at ICLR 2021
Sparse Recovery via Bootstrapping: Collabo-
rative or Independent?
Anonymous authors
Paper under double-blind review
Abstract
Sparse regression problems have traditionally been solved using all available mea-	1
surements simultaneously. However, this approach fails in challenging scenarios	2
such as when the noise level is high or there are missing data / adversarial samples.	3
We propose JOBS (Joint-Sparse Optimization via Bootstrap Samples) - a collab-	4
orative sparse-regression framework on bootstrapped samples from the pool of	5
available measurements via a joint-sparse constraint to ensure support consistency.	6
In comparison to traditional bagging which solves sub-problems in an independent	7
fashion across bootstrapped samples, JOBS achieves state-of-the-art performance	8
with the added advantage of having a sparser solution while requiring a lower	9
number of observation samples.	10
Analysis of theoretical performance limits is employed to determine critical optimal	11
parameters: the number of bootstrap samples K and the number of elements L in	12
each bootstrap sample. Theoretical results indicate a better bound than Bagging	13
(i.e. higher probability of achieving the same or better performance). Simulation	14
results are used to validate this parameter selection. JOBS is robust to adversarial	15
samples that fool the baseline method, as shown by better generalization in an	16
image reconstruction task where the adversary has similar occlusions or alignment	17
as the test sample. Furthermore, JOBS also improves discriminative performance	18
in a facial recognition task in a sparse-representation-based classification setting.	19
1 Introduction	20
In compressed sensing (CS) and sparse regression, a classic linear inverse solution via least squares 21
plus a sparsity-promoting penalty term has been extensively studied. Sparse regression is important 22
for feature selection, reducing over-fitting, and representation learning. and there are rich variants that 23
solve important problems such as dictionary learning (Duarte-Carvajalino & Sapiro, 2009), matrix 24
completion (CandeS & Recht, 2009), Robust Principle Component Analysis (CandeS et al., 2011), 25
matrix factorization (Lee & Seung, 2001), and sparse neural networks (Alvarez & Salzmann, 2016). 26
Mathematically speaking, let A ∈ Rm×n be the sensing matrix, x ∈ Rn contains the sparse codes	27
with very few non-zero entries, z is a noise vector with low bounded energy, and y ∈ Rm be the	28
measurement vector, commonly generated by a linear model with measurement noise: y = Ax + z .	29
The `1 norm minimization is the most common strategy, also known as LASSO (Tibshirani, 1996) or	30
Basis Pursuit denoising (Chen et al., 2001).	31
The performance of `1 minimization has been thoroughly studied in the CS literature (Cohen et al., 32
2009; Candes, 2008; Candes et al., 2006; Donoho, 2006; Candess & Romberg, 2007), including the 33
correctness and robustness based on the Null Space Property (NSP) (Cohen et al., 2009) and the 34
Restricted Isometry Property (RIP) (Candes, 2008; Candes et al., 2006) and mild sufficient conditions 35
on random matrices with sufficient sample complexity to obtain bounded reconstruction error with 36
high probability (Candes, 2008).	37
Even though the baseline `1 min., works pretty well in many applications, Unfortunately, its perfor-	38
mance suffers in challenging, high noise cases. Moreover it has trouble with partially missing and/or	39
severely corrupted samples. Additionally, it is not robust against adversarial samples (illustrated in 40
Figure 1) which are similar to the test case but are from a different class. Adversarial samples can	41
be caused by lack of variation in the training data, and algorithms that can overcome these samples	42
exhibit better generalization.	43
1
Under review as a conference paper at ICLR 2021
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
Bagging, a classic method for regression and classification tasks, has shown its robustness in high
noise cases (Breiman, 1996). In this paper, we use Bagging to refer to employing Bagging procedure
in sparse recovery. To obtain the Bagging solution, the same objective function is solved multiple
times independently from bootstrap (Efron, 1979) samples (uniformly sampled at random with
replacement) and then multiple predictions are averaged. Applying the Bagging method in sparse
regression has been shown to reduce estimation error when the sparsity level s is high for a specific
sparsity pattern (Breiman, 1996).
However, individually solved predictors are not guaranteed to have the same support, and in the worst
case, their average Can be quite dense - its support size growing UP to a multiple of the number of
estimates. Bolasso was proposed to alleviate this problem (Bach, 2008a) by estimating the support
from the intersection of all bootstrapped estimators. However, this strategy is very aggressive and
during large noise cases, the supports of the estimators may not align and it recovers an extremely
sparse solution.
In this paper, we propose to collaboratively enforce the row sparsity constraint among all predictors
using the `1,2 norm to resolve the support inconsistency issue in Bagging and avoid the overly
aggressive Bolasso type of scheme. We name this algorithm JOBS (Joint-sparse Optimization from
Bootstrap Samples). The proposed method involves two key parameters: the bootstrap sample size L
of random sampling with replacement from the original m measurements and the K number of those
bootstrap vectors. JOBS improves the robustness of sparse recovery in challenging scenarios such as
high noise, limited measurements, and in the presence of adversarial samples. A short summary of
comparing JOBS to classical methods is in Table 1.
Methods	'1 min.	I Bagging	JOBS
Robustness against large noise;	baseline	better	better
against adversarial samples	No	No	Yes
Sparsity	medium	I dense ∣	sparse
Optimal L	=m	I small I	smaller
Factor to `1 bound,	1	pL/m < 1	pL/m < 1
with probability	1	I - eO(K∕L)	I - eO(K∕L2)
Table 1: Comparison of different methods of sparse recov-
ery. The factor in the last row is the term associated with
measurement noise power kz k2 .
Figure 1: Adversarial sam-
ple is from a different class
and is more similar to the
test than dictionary atoms
from the same class.
NOTATIONS: Let A denote the original sensing matrix of size m × n. Let y represent the mea-
surement vector. Let I1 , I2,...,IK be bootstrap samples, each containing L elements. For each
bootstrapped sample Ij , the corresponding bootstrapped sensing matrix A[Ij] and bootstrapped
measurements vector y[ij] are generated, where the operation (∙)[I] takes the rows of a matrix/ vector
supported on I. xj is a feasible estimator for the j-th bootstrap sample. Concatenating K estimators
x1, x2,...,xK, we obtain the sparse-code matrix X of size n × K. The row sparsity norm that we
impose in the optimization is defined as the sum of the `2 norm of each row of this matrix: for X,
kXk1,2=P(kx[1]Tk2,kx[2]Tk2,...,kx[n]Tk2).
The proposed method: JOBS consists of three steps. First, we generate K bootstrap samples:
{I1 , I2 ,..,IK }, each containing L indices. The bootstrapped data contains K pairs of sensing
matrices measurements: {y[I1], A[I1]}, {y[I2], A[I2]} , {y[IK], A[IK]}. Second, we solve the
collaborative recovery on those sets. For parameter λL,K > 0 that balances the least squares fit and
the joint sparsity penalty based on the choice of (L, K), the joint sparse optimization is:
K
Xc =minλL,KkXk1,2 +0.5X ky[Ij] - A[Ij]xjk22.	(1)
j=1
2
Under review as a conference paper at ICLR 2021
The proposed form in J1λ2 is a special case of block (group) sparse recovery (Berg & Friedlander,
2008) and there are numerous optimization methods for solving them such as (Boyd et al., 2011;
Baron et al., 2009; Heckel & Bolcskei, 2012; Sun et al., 2009; Bach, 2008b; Berg & Friedlander,
2008; Wright et al., 2009b; Deng et al., 2011). Finally, the JOBS solution is obtained by averaging
the columns of the solution from (1):
1K
JOBS: XJ = —X Xj.	(2)
2	Theoretical Results
2.1	Correctness of JOBS via Block Null S pace Property (BNSP)
Block Null Space Property (BNSP), characterizes the exact recovery condition of our algorithm as a
Necessary and sufficient condition of noiseless program (Gao et al., 2015). we established BNSP
for JOBS and since it established characterizes the existence and uniqueness of the true noiseless
JOBS solution, and then we prove the correctness of JOBS-noiseless defined in (14). Since the final
estimate the average of the solution, the latter part of Theorem 2 implies that the JOBS solution is
also optimal xJ = x?. The detailed proof is shown in Appendix 8.
Definition 1 (BNSP for JOBS) A set of bootstrapped sensing matrices {A[I1], A[I2],..., A[IK]}
satisfies BNSP of order s if ∀ (v1 , v2,..., vK) ∈ Null(A[I1]) × Null(A[I2])... ×
Null(A[IK])\{(0, 0,..., 0)}, such that for all S : S⊂{1, 2,..., n}, card(S) ≤ s, kV [S]k1,2 <
kV[Sc]k1,2.
Theorem 2 (Correctness of JOBS) The noiseless JOBS program successfully recovers all the
s-row sparse solution if and only if {A[I1], A[I2],...,A[IK]} satisfies BNSP of the order of s
described in Definition 1. The solution is of the form X? = (x?, x?,...,x?), where x? is the unique
true sparse solution. Then, the JOBS solution xJ, which is the average over columns of X?, is x?.
2.2	Block Restricted Isometry Propertity (BRIP) of JOBS
Let the JOBS block diagonal matrix AJ = block_diag(A[I1], A[I2],...,A[IK]), where block_diag
denotes the operator that stacks matrices as a block diagonal matrices, and B = {B1 , B2,...,Bn} is
the block partition of all indices of vectorized matrix X ∈ Rn×K that correspond to the row sparsity
pattern. Let 6§|万 denote row sparse Block Restrict Isometry Property (BRIP) constant of order S over
a given block partition B and δs denote the standard RIP constant of order s. We have the following
proposition for JOBS by using the induced vector norm form of eigenvalue function.
Proposition 3 (BRIP for JOBS) For all s ≤ n, s ∈ Z+,
δs∣B(AJ) =	max *δs(A[Ij]).	⑶
j=1,2,...,K
It is not surprising at all that the BRIP of JOBS depends on the worst case among all K bootstrapped
matrices since a smaller RIP constant indicates better recovery ability. The proof of this proposition
is elaborated in Appendix 10.
2.3	Noisy Recovery for JOBS
Next, we analyze the error bound for JOBS using BNSP and BRIP in the noisy case. Note that our
theorems are based on deterministic sensing matrix, measurements and noise vectors: A, y, z and
the randomness in our framework is introduced by the bootstrap sampling process.
From previous analysis, we have established that if the BRIP constant of order 2s is less than √2 - 1,
it implies that {A[I1], A[I2],..., A[IK]} satisfies BNSP of order s. Then, Theorem 2 establishes
that the optimal solution to J12 the noiseless version of joint sparse optimization is the s-row sparse
signal X? with every column being x? . Similar to the bound in Theorem 2 in (Eldar & Mishali,
2009) ,the reconstruction error is determined by the s-block sparse approximation error and the
noise level. The Hoeffding’s tail bound is used to obtain the worst case performance for JOBS. The
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
3
Under review as a conference paper at ICLR 2021
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
following theorem states the performance bound for JOBS when the ground truth signal x? is exactly
s-sparse.
The relationship to the upper bound of RIP constant is discussed in Section 2.5. In the more general
case, when the sparsity level of x? possibly exceeds s, we use the we derived the following error
bounded associated with measurements error and s- sparse approximation error.
The error bound in Theorem 5 relates to s-sparse approximation error as well as the noise level,
which is similar to `1 minimization and block sparse recovery bounds. JOBS also introduces a
relaxation error bounded by kek2, which is the distance of the true vector to its top s- sparse
approximation.
Theorem 4 (JOBS: error bound for kx? k0 = s ) Let y = Ax? + z, kzk2 < ∞. If there exists a
constant related to parameters (L, K) SUCh that, δ2s∣B(AJ) ≤ 6l,k < √2 一 1 and the true solution
is exactly s-sparse, then for any τ>0, JOBS solution xJ satisfies
p{ kxJ - x?k2 ≤ Cι(δL,κ)(弁kzk2 + T)} ≥ 1 - exp -KT4,	(4)
where Cι(∙) is the same non-decreasing functions of δ as in Theorem 1.3 in (Candes, 2008), which is
reminded in Preliminary results session in Appendix 6.
Theorem 5 (JOBS: error bound for the general case) Let y = Ax? + z, kz k2 < ∞. If there
exists a constant related to parameters (L, K) such that, δ2s∣∏(AJ) ≤ Jl,k < √2 — 1, thenforany
T>0, JOBS solution xJ satisfies
L	—2K T4	一
P{kxJ - x?k2 ≤ Co(δL,κ)s-1/2keki + Cι(δL,κ)(ʌ/-∣∣z∣∣2 + T)} ≥ 1 - exp -^.	(5)
where Ci (∙) is the same non-decreasing function of δ as in in Theorem 1.3 in (Candes, 2008); e is the
s-sparse approximation error: e = x? - x0 with x0 containing the largest s components of the true
solution x?; and ∣∣ Ak∞,ι = maxi=1,2,…,m(∣∣a[i]T ||i) denotes the largest '1 -norm ofall rows of A.
We use Theorem 5 to explain the case when the number of measurements is low compared to the true
sparsity level s. The trade-offs for a good choice of the bootstrap sample size - and the number of
bootstrap samples K are discussed in Section 2.5.
2.4	Noisy Recovery for Bagging in Sparse Recovery
We now give the error bounds for employing the Bagging scheme in sparse recovery problems, in
which the final estimate is the average over multiple estimates solved individually and independently
from bootstrap samples.
Theorem 6 (Bagging: Error bound for ∣x? ∣0 = s ) Let y = Ax? + z, ∣z ∣2 < ∞. If there exists
a constant related to parameters (-, K) such that, for all j ∈{1,2,...,K}, δ2s (A[Ij]) ≤ δL,K <
√2 - 1, where A [Ij- ] is the bootstrapped matrix. and let XB be the solution of Bagging, then, for
any T>0, xB satisfies
p{kxB - X?k2 ≤ Ci(δLκ)(∏kz∣2 + T)} ≥ 1 - exp -K；,	(6)
where Cι(∙) is the same non-decreasing function of δ as in Theorem 1.3 in (Candes, 2008).
Theorem 7 (Bagging: Error bound for the general case) Let y = Ax? + z, ∣z∣2 < ∞. If there
exists a constant related to parameters (-, K) such that, for all j ∈{1,2,...,K}, δ2s (A[Ij]) ≤
δL,κ < √2 — 1, and then,for any T > 0, the Bagging solution xB satisfies
p{kxB - X?k2 ≤Co(δL,κ)s-1/2keki + Ci(δL,κ)(∏kz∣2 + T)} ≥ 1 - exp -K4. (7)
where Co(∙), Cι(∙) are the same non-decreasingfunctions of δ as in Theorem 1.3 in (Candes, 2008),
and b0 = (Co(δ)CιT(δ)sT∕2ke∣∣ι + √L∣∣z∣∣∞)2.
4
Under review as a conference paper at ICLR 2021
Theorem 7 gives the performance bound for Bagging in general signal recovery without the s-sparse 157
assumption, and it reduces to Theorem 6 when the s-sparse approximation error is zero, i.e., 158
kek1 =0. Both Theorem 6 and 7 above show that increasing the number of estimates K improves 159
the result by increasing the lower bound of the certainty for the same performance level.	160
JOBS vs Bagging bounds: The RIP condition for Bagging is the same as the RIP condition for 161
JOBS, under the assumption that all bootstrapped matrices A[Ij]s are well-behaved for the worst 162
case analysis. When kx? k0 = s, kek =0, the bound in Bagging is worse than JOBS since the 163
certainty for algorithm is at least 1 - exp —, compared to the error bound 1 - exp /聚 in 164
JOBS. When kek > 0, we can derive (the right hand side) r.h.s. of Bagging (7) < the r.h.s. of Bagging 165
in s- sparse (6) < the r.h.s. of JOBS (5). With an L2 term instead of L in the denominator, the 166
bound is tighter for JOBS given the same L and K. This comparison shows that JOBS has a better 167
theoretical worst-case performance bound for an s-sparse signal; recovery of a nearly s-sparse signal 168
follows similar behavior.	169
2.5	Key Parameters (L, K) Selection from Theoretical Analysis	170
Concerning the sampling ratio L/m, two competing factors influence the optimal choice. In general, 171
the BRIP constant decreases with increasing L; thus, more measurements leads to better recovery. 172
Additionally, increasing L also results in smaller δ and Cι(δ). However, larger L can also increase 173
noise, evidenced by the second factor associated with the noise power term, PL/m. Thus, a moderate 174
choice of the L/m ratio is best. Experimental results show best performance at L/m ≈ 0.4. As m 175
grows larger and problem becomes easier, the optimal L/m also increases.	176
As for the number of estimates K, increasing K weakly increases the BRIP constant, but not by a 177
significant margin. In the sparse regression simulation, we find that increasing K in general does not 178
degrade performances. Increasing K mainly reduces uncertainty in (5), which decays exponentially 179
with K. The certainty can be written as p(K)=1- exp{-αK}, for some α>0. The growth 180
rate of dp(K)/dK is non-negative and decreasing with K. In short, although increasing K will in 181
general improve the results, the improvement margin decreases as K gets larger. We validate this 182
phenomenon in our simulation.	183
3	Experimental Results
184
Three experiments are done to investigate the property of JOBS: (i) on classic sparse reconstruction 185
task on synthetic data, (ii) image reconstruction with presence of adversarial examples on real dataset 186
(iii) standard image classification task on real dataset.	187
3.1	Sparse reconstruction from compressed measurements	188
In this section, we perform sparse recovery on a generic synthetic dataset to study the performance 189
of the proposed algorithm. In our experiment, all entries of A ∈ Rm×n are i.i.d. samples from 190
the standard normal distribution N(0, 1). The signal dimension n = 200, and various numbers of 191
measurements from 50 to 150. The ground truth signals x? has its sparsity level set to s = 50. The 192
location of each non-zeros entry is selected uniformly at random whereas its magnitude is sampled 193
from the standard Gaussian distribution. For the noise processes z, all entries are sampled i.i.d. from 194
N (0, σ2), with variance σ2 = 10-SNR/10kAxk22, where SNR represents the Signal-to-Noise Ratio. 195
In our experiment, we study three different noise levels: when SNR =0, 1 and 2 dB. The same solver 196
to solve sparse regression in all comparison methods: JOBS, Bagging, Bolasso, `1 minimization. 197
Details is in Appendix 11.	198
We explore how two key parameters - the number of estimates K and the bootstrapping ratio L/m 199
一 affect sparse regression results. In our experiment, we vary K = 30, 50,100 while setting the 200
bootstrap ratio L/m from 0.1 to 1 with an increment of 0.1. We report the average recovered Signal 201
to Noise Ratio (SNR) as the error measure to evaluate the recovery performance: SNR(xb, x?)= 202
-10 log10 kxb - x?k22/kx? k22 (dB) averaged over 20 independent trials. For all algorithms, we vary 203
the balancing parameter λL,K at different values from .01 to 200 and then select the optimal value 204
that gives the maximum averaged SNR over all trials at each (L, K).
205
5
Under review as a conference paper at ICLR 2021
m = 75	m = IOO	Phase diagrams
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
Figure 2: Recovery performance curves for JOBS and Bagging (with various L,K) versus `1 min-
imization. Left 1-4: The number of measurements are m = 75, 100 from left to right. Left 5-6:
Phase diagrams of JOBS, Bagging. Noise level is set to SNR = 0 dB.
Performance of JOBS, `1 min., Bagging is illustrated in Figure 2 with different total numbers
of measurements m = 50, 150. Note, for each condition with a particular choice of (L, K), the
information available to JOBS, Bagging and BolaSSo algorithms is identical and '1 -minimization
always has access to all m measurements. When the number of measurements m is limited, JOBS
outperforms `1 minimization significantly. As m increases, the margin decreases. When the number
of measurements is low (the sparsity level s = 50 and m is only 50 - 150, which is between 1s - 3s),
and with very small bootstrap sampling ratio L/m (L/m is only 0.3 - 0.5) JOBS and Bagging are
quite robust and outperform all other algorithms using the same parameters (L, K). In addition,
although JOBS and Bagging are similar in terms of the best performance limit, which are within 3%
in our overall experiments. Bagging requires higher L/m ratios (typically ≥ 0.6) to achieve peak
performance than JOBS. This is explored more in the next paragragh.
JOBS vs Bagging
•	JOBS Optimal Sampling Ratio is Consistently Smaller than That of Bagging. Both JOBS and
Bagging outperform the classical `1 minimization algorithm in the challenging case when the total
number of measurements m is low. The peak performance of JOBS and Bagging are comparable
(within 3%). Table 2 shows the optimal ratios for JOBS algorithm and for Bagging with the number
of measurements m from 50 - 150 and various SNR ratios SNR =0, 1, 2 dB. The optimal bootstrap
sampling ratio for JOBS is smaller than that for Bagging. With the same K as Bagging, JOBS
achieves optimal performance with a much smaller vector size L compared to Bagging.
A smaller bootstrap size leads leads to a reduction of the algorithm complexity. With the
ADMM implementation, the theoretical complexity levels for both Bagging and JOBS algorithms are
the same for the same (L, K): O(n2(L + n)K)+TO(n2K), where T is number of iterations. This
result Since the optimal L is smaller, JOBS yields a smaller complexity than Bagging.
	SNR = 0		SNR= 1		SNR = 2	
m	JOBS	Bag.	JOBS	Bag.	JOBS	Bag.
^O=	0.5	0.6	0.6	0.8	0.5	0.8
75	0.4	0.9	0.4	1	0.4	0.7
100	0.3	0.7	0.3	1	0.4	1
150	0.4	1	0.5	1	0.5	1
m	Bootstrapping-based methods		
	JOBS	Bagging	Bolasso
-30-	. 89 ± 3%	91 ± 2%	0.03 ± 0.1%
75	78 ± 4%	82 ± 5%	0.20 ± 0.4%
100	71 ± 4%	91 ± 2%	0.25 ± 0.3%
150	47 ± 6%	87 ± 5%	3.6 ± 1%
Table 2: The Empirical Optimal Sampling Ratios Table 3: The averaged sparsity ratios of re-
L/m with Limited Measurements m. K = 100. covered signals. The numerical threshold for
being non-zero is 10-2. SNR =0dB.
•	JOBS solutions are consistently sparser than Bagging solutions. We check the sparsity of the
reconstructed signals through the numeric sparsity ratio: the sparsity ratio for a reconstructed vector xb
is the ratio elements with whose magnitude higher than the threshold (τ>0) over all elements. From
Table 3, JOBS generally produces sparser solutions than Bagging. It verifies our motivation to have
more precise control over the sparsity level in JOBS algorithm than individually solved predictors
such as Bagging, which are not guaranteed to have the same support on bootstrapped solutions.
6
Under review as a conference paper at ICLR 2021
3.2 Image Reconstruction: JOBS is robust against adversarial samples
This experiment verifies the robustness
of JOBS in the presence of adversarial
samples. A adversarial sample defi-
nition is illustrated as in Fig. 1. We
evaluate two cases: Case A - Adver-
sarial sample from occlusion: Test
image, to be recovered, is of a woman
with scarf. The dictionary does not in-
clude any images with a scarf from the
same class, but it does include an ad-
versarial sample from a different class:
i.e. a man wearing a scarf. Case B
- Adversarial sample from misalign-
ment: Test image is of a woman. All
dictionary atoms from the same class
are rotated to various degrees. How-
ever, there is a picture of a man with
the same alignment as the test image.
In both cases, we use a common face
recognition dataset: the cropped AR
dataset (Martinez & Kak, 2001), con-
taining pre-aligned images taken in
various controlled conditions. The di-
mensions of all images are 165 × 120
pixels. We took images from two peo-
ple: one woman (with label W -001)
and one man (with label M -001) as
our dictionary and test signal to be
reconstructed in both cases. We use
simplified notation W# to indicate la-
bel # from W -001 and M # for pic-
tures of the man. For each person, the
same label corresponds to the same
controlled condition.
Bolasso-0.7 solutions:
Adversarial
SamPle (a)
Ground
Truth (b)
dictionary atoms examples
from the
dictionary atoms examples
from the men
sparse
21.74dB
17.79dB
PSNRto (b,)
PSNRtP (a)
PSNRto (b,)
PSNRto (a)
21.01 dB
15.82dB
Figure 3: Reconstruction with adversarial sample with sim-
ilar occlusion as test. Top: adversarial sample (a) and dic-
tionary examples. Mid-Bot.: The test image (b) and recon-
structions. `1 favors (a). Bagging solutions contain energy
from (c), far from ground truth. Bolasso has visible com-
ponents from (a). JOBS successfully avoids (a) as sparsity
regularization increases (yellow box). Ground truth PSNRs
calculated with respect to b0 , the top 100 rows of (b).
(b,) upper face
20.47dB
20.85dB
16.16dB
29.65 dB
20.47dB
17.63dB
19.11dB
16.32dB
dense
sparse
Case A: The dictionary contains W1 - W10 from the woman and M1 - M11 from the man. The
test image to be reconstructed is of the woman wearing the scarf: W11. M11 serves as an adversarial
sample that may fool recovery methods due to a scarf occlusion very similar to that in the test image.
Parameters: The reconstruction is performed directly in the vectored image domain. As a variation
for JOBS and Bagging, instead of picking random bootstrap samples, we pick 200 random 12 × 12
patches, to take advantage of the local robust features. We adopt a soft version of Bolasso: Bolasso-S
to reduce the chance of zero solutions. The estimated support contains locations present in at least S
replications, rather than requiring them to be in all K (Bach, 2008a). We take S = 0.7.
Bagging reconstruction is taken from the exact same set of measurements as JOBS. The sparsity
regularization parameters for (dense, sparse) solutions for `1 are (100, 5000); for Bagging are
(100, 2500); for Bolasso-0.7 are (0.01, 1.2); and for JOBS are (103, 104), respectively.
Performance from all four methods is shown in Figure 3. `1 minimization is fooled by the adversarial
example and selects it at any sparsity level. Although Bagging does not suffer as much from the
adversarial sample, its dense solution contains strong artifacts from an image with glasses as shown in
Figure 3(c). Bolasso, like `1 min., is strongly influenced by the adversarial sample. In contrast, JOBS
avoided the adversarial sample well: the component of the adversarial example in JOBS solution
reduces with increasing sparsity regularization, and the scarf eventually becomes invisible.
Case B: The test is W7. The dictionary contains W1 - W6, each rotated 5° incrementally Counter-
clockwise, as well as an adversarial sample W7. The test and adversarial images have the same
alignment whereas all other dictionary atoms are misaligned.
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
7
Under review as a conference paper at ICLR 2021
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
Parameters: We pick 1200 random patches of dimension 3 × 3. The sparsity regularization
parameters for `1, Bagging, Bolasso, and JOBS are 105, 600, 200 and 1500, respectively.
The dictionary atoms from the same class; W Adversarial
sample (a)
(from right to left, each rotates 5
counter-clockwise)
onstruc-
tions
Dictionary
Bolasso-0.7
Performance of four algorithms is il-
lustrated in Figure 4. Here we use
two metrics: the ratio of reconstructed
signal from atoms from women dictio-
nary over all locations and the PSNR
to adversarial sample (a). According
to both measures, the order of robust-
ness to (a) from weak to strong is `1
min., Bolasso, Bagging and JOBS. Al-
though Bagging has a large compo-
nent from the correct class (75%), it
is too dense and the reconstructed pic-
ture is blurry due to different align-
ment conditions in the dictionary.
3.3 Image Classification
To confirm that improvements in re-
gression directly lead to improve-
ments in classification, we performed
Figure 4: Reconstruction with adversarial sample with the
same alignment condition as the test. Top: Adversarial sam-
ple (a) and examples in dictionary. Bot.: Reconstructions:
`1 returns similarly to (a). Bagging solutions are too dense
and therefore blurred. Bolasso contains large energy from
(a). JOBS performs the best: a clear image with more than
90% from the correct class.
classification experiments on the same
cropped AR dataset. We first use ran-
dom projection (Gaussian matrix) for
dimension reduction to generate m =
50 random features as measurements.
Then we solve the sparse regression
problem using all four algorithms all
within a Sparse Representation-based
Classification (SRC) framework proposed by Wright in (Wright et al., 2009a) to predict class label.
As shown in Table 4, classification based on sparse representations generated by JOBS shows a
consistent improvement of 3% in classification accuracy over the baseline `1 minimizatioon. As with
regression, the optimal bootstrapping ratio for JOBS at only 0.5 is lower than Bagging. The JOBS
solution is also much sparser than Bagging’s (threshold for being non-zero is 10-6), similar to '1.
Table 4: Classification accuracy, optimal parameters, and sparsity comparison with various methods.
Bagging is NOT directly used on classification but on bagged sparse code. Dataset is Cropped AR
(m = 50), with training ratio 0.92.
Metrics	Baseline '1 min.	Bootstrapping-based methods JOBS	Bagging	Bolasso
Accuracy Optimal (L/m, K) Sparsity Ratio	0.855 (1,1) 3.6% (±0.5%)	0.880	0.855	0.790 (0.5,30)	(1,50)	(0.9,30) 2.7%(±0.5%) 27% (±3%)	0.56% (±0.3%)
4 Conclusion
We propose a collaborative signal recovery framework named JOBS, motivated from powerful
bootstrapping ideas in machine learning. JOBS improves the robustness of sparse recovery in
challenging scenarios of noisy environments and/or limited measurements, and with the presence
of adversarial samples. Below are highlights: (i) JOBS is particularly powerful when the number
of measurements m is limited, outperforming `1 min. by a large margin. (ii) JOBS achieves
desirable performances with relatively low bootstrap ratio L/m than Bagging and small number of
bootstrapped observation vectors K. (iii) The optimal sampling ratio for collaborative JOBS is lower
than that of independent Bagging while achieving similar results, resulting in a lower computation
complexity. (iv) JOBS solutions are generally more sparse than Bagging,s — a desirable property in
sparse recovery. (v) JOBS is robust against adversarial samples.
8
Under review as a conference paper at ICLR 2021
References	334
The birthday problem. http://www.math.uah.edu/stat/urn/Birthday.html. Cre- 335
ative Commons License.	336
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. In 337
Advances in Neural Information Processing Systems,pp. 2270-2278,	2016.	338
F. R Bach. Bolasso: model consistent lasso estimation through the bootstrap. In Proceedings of the 339
25th Int. Conf. on Machine learning (ICML), pp. 33-40. ACM, 2008a.	340
F. R Bach. Consistency of the group lasso and multiple kernel learning. The J. of Machine Learning 341
Research, 9:1179-1225, 2008b.	342
R. Baraniuk, M. Davenport, R. DeVore, and M. Wakin. A simple proof of the restricted isometry 343
property for random matrices. Constructive Approx., 28(3):253-263, 2008.	344
D.	Baron, M. F Duarte, M. B Wakin, S. Sarvotham, and R. G Baraniuk. Distributed compressive 345
sensing. arXiv preprint arXiv:0901.3403, 2009.	346
E.	Berg and M. P Friedlander. Probing the Pareto frontier for basis pursuit solutions. SIAM J. on 347
Scientific Computing, 31(2):890-912, 2008. doi: 10.1137/080714488. URL http://link. 348
aip.org/link/?SCE/31/890.	349
S.	Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical 350
learning via the alternating direction method of multipliers. Foundations and Trends in Machine 351
Learning, 3(1):1-122, 2011.	352
L. Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.	353
P L Buhlmann. Bagging, SUbagging and bragging for improving some prediction algorithms. In 354
Research Report, volume 113. Seminar fur Statistik, ETH Zurich, Switzerland, 2003.	355
P. L Buhlmann and B. YU. Explaining bagging. In Research Report, volUme 92. Seminar fur Statistik, 356
ETH Zurich, Switzerland, 2000.	357
E. J Candes. The restricted isometry property and its implications for compressed sensing. Comptes 358
Rendus Mathematique, 346(9):589-592, 2008.	359
E. J Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from 360
highly incomplete frequency information. IEEE Trans. on Info. theory, 52(2):489-509, 2006.	361
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foun- 362
dations of Computational mathematics, 9(6):717, 2009.	363
Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? 364
Journal of the ACM (JACM), 58(3):1-37, 2011.	365
E. Candess and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse prob., 23(3): 366
969, 2007.	367
S. Chen, D. L Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM review, 43 368
(1):129-159, 2001.	369
A.	Cohen, W. Dahmen, and R. DeVore. Compressed sensing and best k-term approximation. Journal 370
of the American mathematical society, 22(1):211-231, 2009.	371
W. Deng, W. Yin, and Y. Zhang. Group sparse optimization by alternating direction method. In Rice 372
CAAM Report TR11-06, pp. 88580R. International Society for Optics and Photonics, 2011.	373
D. L Donoho. Compressed sensing. IEEE Trans. on Info. theory, 52(4):1289-1306, 2006.	374
Julio Martin Duarte-Carvajalino and Guillermo Sapiro. Learning to sense sparse signals: Simul- 375
taneous sensing matrix and sparsifying dictionary optimization. IEEE Transactions on Image 376
Processing, 18(7):1395-1408, 2009.	377
9
Under review as a conference paper at ICLR 2021
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
B.	Efron. Bootstrap methods: another look at the jackknife. TheAnnalsof Stat.,7(1):1-26, 1979.
Y. C Eldar and M. Mishali. Robust recovery of signals from a structured union of subspaces. IEEE
Trans. on Info. Theory, 55(11):5302-5316, 2009.
Y. Gao, J. Peng, and Y. Zhao. On the null space property of 'q-minimization for in compressed
sensing. J. of Function Spaces, 2015, 2015.
R. Heckel and H. Bolcskei. Joint sparsity with different measurement matrices. In Proc. of 50th
Annual Allerton Conf. on Communication, Control, and Computing, (ALLERTON), pp. 698-702.
IEEE, 2012.
W. Hoeffding. Probability inequalities for sums of bounded random variables. J. of the American
Statistical Association, 58(301):13-30, 1963.
Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances
in neural information processing systems, pp. 556-562, 2001.
A. M Martinez and A. C Kak. PCA versus LDA. IEEE Trans. on Pattern Anal. Mach. Intelligence,
23(2):228-233, 2001.
A. F Mendelson, M. A Zuluaga, B. F Hutton, and S. Ourselin. What is the distribution of the number
of unique original items in a bootstrap sample? arXiv, 2016.
L. Sun, J. Liu, J. Chen, and J. Ye. Efficient recovery of jointly sparse vectors. In Advances in neural
information processing systems (NeurIPs), pp. 1812-1820, 2009.
R. Tibshirani. Regression shrinkage and selection via the Lasso. J. of the Royal Stat. Society. Series
B, pp. 267-288, 1996.
I.	Weiss. Limiting distributions in some occupancy problems. The Annals of Mathematical Statistics,
29(3):878-884, 1958.
J.	Wright, A. Y Yang, A. Ganesh, S. S Sastry, and Y. Ma. Robust face recognition via sparse
representation. IEEE Trans. on Pattern Anal. Mach. Intelligence, 31(2):210-227, 2009a.
S. J Wright, R. D Nowak, and M. AT Figueiredo. Sparse reconstruction by separable approximation.
IEEE Trans. on Sig. Proc., 57(7):2479-2493, 2009b.
10