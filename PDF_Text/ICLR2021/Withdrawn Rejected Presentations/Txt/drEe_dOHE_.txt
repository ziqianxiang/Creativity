Under review as a conference paper at ICLR 2021
InvertGAN: Reducing mode collapse with
multi-dimensional Gaussian Inversion
Anonymous authors
Paper under double-blind review
Ab stract
Generative adversarial networks have shown their ability in capturing high-
dimensional complex distributions and generating realistic data samples e.g. im-
ages. However, existing models still have difficulty in handling multi-modal out-
puts, which are often susceptible to mode collapse in the sense that the generator
can only map latent variables to a part of modes of the target distribution. In this
paper, we analyze the typical cases of mode collapse and define the concept of
mode completeness in the view of probability measure. We further prove that the
inverse mapping can play an effective role to mitigate mode collapse. Under this
framework, we further adopt the multi-dimensional Gaussian loss instead of one-
dimensional one that has been widely used in existing work, to generate diverse
images. Our experiments on synthetic data as well as real-word images show the
superiority of our model. Source code will be released with the final paper.
1	Introduction
For generative models, the real-word data is often assumed to sample from an unknown and im-
plicit distribution. Deep generative models often try to construct a mapping from a known distri-
bution e.g. Gaussian distribution to the implicit target1. Recent efforts mainly focus on Variational
Auto-Encoders (VAEs) (Kingma & Welling, 2014), Generative Flow models (Rezende & Mohamed,
2015) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).
However, many problems still exist for generative models. VAEs embed a given data sample to a hid-
den Gaussian distribution as representation, which may not be essentially effective for high-quality
data generation. Flow models can obtain a one-to-one mapping, at the cost of high computational
overhead. GAN can generate high-resolution images, but it often suffers from mode collapse, which
tends to only focus on a subset of modes while excludes other parts of the target distribution (Liu
et al., 2020), leading to a poor diversity of generated samples.
This paper focuses on solving the mode collapse for GAN. Recent studies address mode collapse
mainly in two ways: i) modifying the model learning to achieve a better convergence (Gulrajani
et al., 2017; Metz et al., 2017); ii) encouraging the models to learn diverse modes with vari-
ance (Elfeki et al., 2019; Meulemeester et al., 2020) or mapping back to learn the representation
together (Srivastava et al., 2017; Donahue et al., 2017). The former mainly focuses on the study of
convergence between the generated distribution and the target one. While the later tries to encourage
the generator to sample fake data with diversity similar to real data.
In this paper, we departure from the popular view centering around the relation between the gener-
ated distribution and the real one, but pay more attention to the relation between the source distri-
bution and the target distribution. Specifically, we design an inverse mapping based method to tame
mode collapse. Different from previous auto-encoding works with the assumption of one dimen-
sional Gaussian distribution, we regard the inverse of real data with a multi-dimensional Gaussian
sampling view and the resulting method is termed as InvertGAN. Our main contributions include:
1)	We carefully analyze the typical cases of mode collapse, i.e mode missing, mode imbalance, and
a mixture of the former two cases. Based on this mapping perspective, we define mode completeness
in Definition 1, that is, the generative mapping T shall meet the condition of Eq. 1. Then based on
1 Such a mapping is formally termed by generative mapping in this paper.
1
Under review as a conference paper at ICLR 2021
this definition, in Proposition 1 we prove that the inverse of a real dataset as the target can be viewed
as independent samples of the source distribution.
2)	Based on our analysis, we propose InvertGAN to address mode collapse. It assumes a multi-
dimensional Gaussian distribution based on the inverse of real data and promotes the multi-
dimensional distribution to be close to standard Gaussian distribution, which emphasizes the in-
dependence between different dimensions for source distribution. Fig. 3 shows that the adoption of
multi-dimensional Gaussian, that is, the use of the covariance matrix instead of variance, is better
than the traditional single-dimensional method.
3)	On synthetic data with multiple modes, we empirically show that InvertGAN outperforms peer
methods by different metrics including covered mode number, Quality, reverse KL divergence. On
real world images, our simple technique also performs competitively by Inception Score, FID espe-
cially for those with high resolution (STL-10) and a large number of categories (CIFAR-100).
2	Related Work
Since its debut (Goodfellow et al., 2014) as an effective generative model, many subsequent works
have been proposed to improve the stability and quality of generation. However, GANs still suffer
from unstable training, and mode collapse has been one of the most common issues for GANs
training. Recent approaches tackle mode collapse mainly in the following different ways:
Improving the training behavior. The Unrolled GAN Metz et al. (2017) presents a surrogate ob-
jective to train the generator along with the unrolled optimization of the discriminator, which shows
improvements in terms of training stability and reduction of mode-collapse. As an improvement
to Wasserstein GANs (Arjovsky et al., 2017), WGAN-GP (Gulrajani et al., 2017) devises a gradi-
ent penalty whose effectiveness has shown in the realistic generation system (Karras et al., 2018).
(Mescheder et al., 2018) gives the ODE view and proves that zero gradient penalty can improve the
convergence for the generation.
Enforcing to capture diverse modes. To generate data with the same diversity as the real data is,
many methods are proposed to solve mode collapse for the GAN model. (Elfeki et al., 2019) uses
the theory of determinantal point processes and gives a penalty in the layer of the discriminator to
enforce the generated data having similar covariance of real data. The approach in (Meulemeester
et al., 2020) uses Bure metric instead and discusses more training details. The VEEGAN (Srivastava
et al., 2017) design an inverse of the generator and encourage the discriminator to distinguish the
joint distribution for real and generated one, which is similar to the work (Donahue et al., 2017).
Multiple generators and discriminators. One direct way of reducing mode-collapse is involving
more than one generators to achieve wider coverage for the true distribution. In (Liu & Tuzel, 2016),
two coupled generator networks are trained with parameter sharing to jointly learn the true distribu-
tion. The multi-agent based system MAD-GAN (Ghosh et al., 2018) involves multiple generators
along with one discriminator. The system implicitly encourages each generator to learn their own
mode. On the other hand, multiple discriminators are used in (Durugkar et al., 2017) as ensemble.
Similarly two additional discriminators are trained to improve the diversity (Nguyen et al., 2017).
We note these works are orthogonal to our contribution and mostly could be fulfilled in conjunction
with ours to further improve the training stability.
3	Inverse Mapping based Probabilistic View to Mode collapse
Despite their success, GANs still suffer the mode collapse issue, especially for complicated distri-
bution. This section presents our inverse mapping based techniques to address this challenge.
3.1	Case Analysis for Mode Collapse
We use Fig. 1 to illustrate typical cases of mode collapse from the perspective of mapping between
two probability measures: the left blue line is the known source probability measure α defined in the
domain A, while the semicircle on the right is the target β in the domain B. The goal of a generative
model is to establish the mapping T from the source distribution to the target one i.e. real data.
2
Under review as a conference paper at ICLR 2021
Figure 1: Mode collapse in the view of mapping: the left is the source distribution and the right is
target. (a) mode missing in grey; (b) mode imbalance as the green part is less mapped from source.
i)	Mode missing. Not all the modes can be completely generated i.e. mapped from the source dis-
tribution. As shown in Fig. 1(a), almost half part of the probability measure β can not be generated
from α, which we call mode missing. In this case, the generative mapping T is not a surjection (i.e.
T (A) 6= B), which is the main reason for mode missing.
ii)	Mode imbalance. As shown in Fig. 1(b), most of the target distribution can be covered yet there
exists an imbalance that some part is densely mapped (red) while the rest (green) is more sparse.
iii)	Mixture of the two. Mode missing and imbalance are mixed in Fig. 1(c), which will be ad-
dressed separately by our techniques, though the former can be treated as a special case of the latter.
Definition 1 (mode completeness for mapping) For any measurable set C ⊂ B from the target
domain, the generative mapping T is defined to be mode complete w.r.t. probability measure from α
to β, if T satisfies:
β(C) = α(z ∈ A : T(Z) ∈C})	(1)
which is often written as the push-forward operator (Peyre & Cuturi, 2018) β = T#a.
Equivalently, the above proposition can be rewritten from a sampling perspective. Given the inde-
pendent samples Z = {zi}iN=1 and X = {xi}iN=1 where zi is sampled from α and xi is sampled
from β . Here N is a large enough number and then we have:
card(X ∩ C) = card({z ∈ Z : T(z) ∈ C})	(2)
where card(∙) is the cardinal number of the set. Obviously, there exist infinite solutions for the
mapping T and the core of the mode collapse problem is that T# α does not coincide with the
probability measure β of the real data. Therefore, when training T to find the hidden probability
measure β, it must be noted that the real data {xi} is independently sampled from β.
Besides, Eq. 1 has met the condition that T is a surjection. Given a non-empty set C ⊂ B where
β(C) 6= 0, if α({x ∈ A : T(z) ∈ C}) = 0 which means C is the set that can not be mapped from A
, we can get β(C) = 0. It contradicts the precondition β(C) 6= 0. For mode imbalance in Fig. 1(b),
the subset C and {x ∈ A : T(z) ∈ C} share the same probability, which prevents the imbalance for
the number of independent samples in the corresponding domain.
3.2	An Inverse Method for Mode Collapse
Inverse methods have been applied (Srivastava et al., 2017) in GANs to improve the generation and
translation. Here we will show that the inverse method can be used to address mode missing and
imbalance. Previous works (Kingma & Welling, 2014; Srivastava et al., 2017) mainly
We turn to designing an inverse mapping T-1 to address mode missing, which can fulfill surjection
of mapping T. More specifically, for any target sample x, the constrain T-1(x) ∈ A can be enforced
for training T and its inverse T-1, which ensures the existence of the corresponding z for T(z) = x.
Proposition 1 (inverse constraints for target samples) If the generative mapping T is mode com-
plete from probability measure α to β and its inverse T-1 exists, and if {xi}in=1 are n independent
SamPlesfrom β, then {Z = TT(Xi) }n=ι can be viewed as n independent SamPiesfrom β.
3
Under review as a conference paper at ICLR 2021
Figure 2: The InvertGAN: generator G maps random samples from source standard MD Gaussian
to target ones and F inverts the target sample back to a source sample obeying MD Gassuian.
Proof If the mapping T is mode complete according to Definition 1, then Eq. 1 is satisfied. Given
the inverse T-1, Eq. 1 can be transformed into:
β(C) = α{z ∈ A : Z =
T-1(xi),xi ∈ C}
(3)

So by setting C = {x1}, {x2}, . . . , {xn} respectively, we can find that the corresponding probabili-
ties for {xj and 伍} are equal. It means that {T T(Xi) : Xi ∈ C} can be regarded as independent
samples from α.
Therefore, given the real data {xi }in=1 which is often viewed as independent samples from some
unknown distribution, we only need to train the inverse mapping so that {T-1(xi)} is independently
sampled from the probability measure α, and we can get diverse samples by independent sampling. 4
4 The Proposed Model
The original GAN model consists of a discriminator D : Rd → R and a generator G : Rl → Rd ,
which are typically embodied by deep neural networks. Given the empirical distribution Px, D(x)
as used to distinguish generator samples from true data samples, while G(z) is the mapping from
Gaussian sample z to a point in the data space Rd . The discriminator and generator are optimized
by solving the minimax problem, by alternating the two phases of training:
mGnmDaχ V(G,D) = Ex 〜P, [log(D(X))] + Ez 〜P, [lOg(I- D(G(Z)))]
(4)
The first term gives the expectation of probability that x comes from real data distribution Px and
the second involves an input distribution Pz , which is embodied by a standard multi-dimensional
Gaussian distribution N (0, Il) in this paper. Here l is the dimension ofz. Later in this paper we will
elaborate the reason why a multi-dimensional Gaussian is used which differs from existing works
using a single-dimensional Gaussian.
4.1 Inverse Mapping for Mode Missing
Recall that in Sec. 3.2, we have shown that designing the inverse mapping can be used to reduce
mode collapse. We adopt the neural network F as the inverse of the generator. To achieve F = G-1
and solve the mode missing problem, we design the follow loss:
Lcons(G,F) = Ez〜Pzkz - F(G(Z))k2 + Ex〜Pxkx - G(F(x))∣∣2
l	— y—	J ∖	—	}
to achieve inverse mapping	to avoid mode missing
(5)
The first term promotes F be the inverse of G, which uses the reconstruction penalty as an expecta-
tion of the cost of autoencoding noise vectors (Srivastava et al., 2017) and the second term promotes
that F (x) ∈ Rl, which makes Z exist in Rl. Then for every real data point x, We can find the
corresponding Z in Rl which satisfies G(Z) = x.
4
Under review as a conference paper at ICLR 2021
4.2 MD Gaussian loss for Mode concentration
As mentioned in Sec. 3.2, given the real data {xi} sampled from Pχ, {z = F(xi)} follows the
distribution Pz, which can be viewed as i.i.d samples from Pz . So to solve mode imbalance, the
inverse mapping F should make {Z} more like being sampled from standard Gaussian Pz.
We propose to use multi-dimensional Gaussian (MD Gaussian) as the source distribution instead of
the widely used 1D Gaussian (Srivastava et al., 2017). The reasons are two folds:
1)	The source 1D Gaussian used in previous works can be regarded as a standard MD Gaussian
whose dimensions are independent to each other (in the sense that each sampling from a 1D Gaussian
can be regarded as sampled from a standard MD Gaussian along a certain dimension). However, the
inverse samples {Z} are always dependent if We use 1D Gaussian; 2) There is a correlation among
the data points, and using covariance instead of variance can better capture this correlation with
enhanced model capacity.
So we assume Zi = T(Xi) follow the MD Gaussian Pz with its mean μz and variance Σ5, then our
goal is to make Pz approximate to Zi = T(Xi) by training, so that the real data can be considered
as conversion of samples from Pz and the mode will not collapse. To get the approximation, we can
design the Gaussian loss LGau for the inverse F with the following distances or divergence:
1)	Wasserstein distance. The Wasserstein distance has been widely used to evaluate the distance
between two distributions. Given two MD Gaussains Pz and Pzz, the 2-Wasserstein distance is:
W2 (Pz, Pz) = √kμzk2 + trace(Σz + I1 - 2Σz/2)	(6)
We can see that W2 (Pz, Pz) = 0 if and only if μz = 0 and Σz = Il.
2)	KL divergence. It is an important divergence to measure the difference between two distributions.
Given MD Gaussians Pz and Pzz, the KL divergence KL(Pz, Pzz) can be specified as:
KL(Pz,Pχ) = 1 {log(det(Σz)) - l + trace(Σ-1) + μz>Σz-1μz}	(7)
3)	p-norm distance. In essence, the Wasserstein distance and KL divergence use the trace of matrix,
which may pay more attention to the difference of its eigenvalues. Here we define a simple distance
between MD Gaussian distance calledp-norm distance. Given two Gaussians Ya = N(μo, ∑a) and
Yb = N(μb, ∑b), we specify thep-norm distance Dp(γa, Yb) as:
Dp(Ya,Yb) = kμa - μbkp + k∑a - ∑bkp	(8)
where ∣∣ ∙ ∣∣p is thep-norm for the vector. For matrix, we set k∑kp = (Pi Pj Σj)1. We can prove
that LP is a distance because ∣∣ ∙ ∣∣p is a norm and satisfies the the triangle inequality as given by:
DP(Pz,Pz) = ∣∣μzkp + k∑z - Ilkp	(9)
4)	The Z-discriminator. It discriminates whether the generated image is a sample of real distri-
bution Px . Similarly, we can also use the discriminator to distinguish the difference between real
Gaussian samples and generated ones. By inputting as many samples sampled from Pz as possible,
together with the corresponding inverse samples {Z} from the re京 data, we optimize Dz and F by
adversarial learning to push {z} closer to the sampling results of Pz. Then we can get our final loss
as:
minmaxV(G,D) + Lcons(G, F) +LGau(F)	(10)
G,F D
where LGau (F) can be specified based on different distances or divergence (the method with
Z-discriminator should train one more discriminator. We will discuss its final loss in Appendix).
1D Gaussian Loss. To show the superiority ofMD Gaussian loss, here we also test the 1D Gaussian
loss which is given as follows, where σz is the standard deviation of {Z}.
W2(Pz, Pz) = Pkμzk2 + kσz- 1k2	(11)
Fig. 3 and Fig. 5 show the superiority of MD Gaussian loss which will be detailed in experiments.
5
Under review as a conference paper at ICLR 2021
Inverse Gaussian Z
Real Gaussian Z
Generation Results
N4933>
Sso*lu"ssne9 ~noqzM
Sso-Ju"ssne9 <∏
Sso-Ju"ssne9 QΣ
Figure 3: Results visualization for Ring, by comparing VEEGAN and InvertGAN under different
Gaussian losses as detailed in Sec. 4.2 after training 24K mini-batches. Both 1D and MD Wasser-
stein distance are used. The first column is the inverse {Zi} of real data {xi}, which has the same
number of points for each color (i.e. mode). The red box in the third row represents that 1D Gaussian
loss may cause some modes to gather and affect the balance. The second column refers to random
sampling based on standard Gaussian Pz, which maps to different modes. The third column is the
random generation result, and the fourth column is the generation percentage of each mode.
Percentage of Generated Modes
■ Mode 1
Mode 2
■ Mode 3
■ Mode 4
■ Mode 5
■ Mode 6
■ Mode 7
I Mode 8
Bad Generation
■ Mnd* 1
■ Mode 4
■ Mode 5
■ Mode 6
■ Mode 7
■ Mode 1
Mode 2
■ Mode 3
■ Mode 4
■ Mode 5
■ Mode 6
■ Mode 7
u Mode 8
Bad Generation
■ Mode 1
■ Mode4
■ ModeS
■ Mode 6
■ Mode 7
，Mode 8
Bad Generation
■	Mode 3
Bad Generation
■	Mode 3
5	Experiments
The experiments cover both synthetic dataset and real-world images. To highlight the advantages
and disadvantages of the model in design, we adopt a simpler network architecture for both synthetic
and real world experiments to more directly evaluate the contributions of our techniques.
5.1	Experiments on Synthetic Datasets
Mode collapse can be accurately measured on synthetic data, as the real distribution is known. In
line with (Metz et al., 2017), we simulate two synthetic datasets. The batch size is set to 128.
Ring: a mixture of eight 2D Gaussians with their mean {(2 cos (in/4), 2 cos (i∏∕4))}8=ι and the
standard deviation 0.001. 12.5K samples are simulated from each Gaussian distribution i.e. 100K
samples in total. 50K samples from Pz are used to generate x for testing.
Grid: a mixture of 25 2D isotropic Gaussians with mean {(2i, 2j)}i2,j=-2 and standard deviation
0.0025. 4K samples are simulated from each Gaussian (i.e. 100K samples in total). 100K samples
from Pz are used to generate target samples {X} for testing.
6
Under review as a conference paper at ICLR 2021
10	20	30	40	50
K Iterations
(b) Reverse KL divergence
(a) Generation quality
Figure 4: Comparison of different Gaussian distribution loss on the Ring synthetic data.
SSol UBωsne0Cn
SSolU.≡ssnBO
Figure 5: Generations of grid data given poor initialization. Compared with 1D Gaussian loss, MD
Gaussian loss can overcome the mode collapse after enough training steps (i.e. batch iteration). The
comparison on Ring is given in appendix which similarly shows the advantages of our methods.
For the network architecture, (Metz et al., 2017) suggest that the activation function tanh can improve
training. Different from (Metz et al., 2017; Elfeki et al., 2019), we are refrained from this trick to
more directly verify the role of our techniques. So in the synthetic experiment, we use four linear
layers with ReLu activation function for testing.
For metrics, following (Metz et al., 2017; Elfeki et al., 2019), we use the numbers of modes covered,
generation quality2 and reverse KL divergence. Since in the experiment, each mode shares the same
number of real samples, it can be used to calculate the reverse KL divergence between the generated
distribution and the real one (Nguyen et al., 2017): D KL(model∖∖data) = Pm=I Pi log Ipm.
Note the reverse KL divergence is not strictly defined because Pim=1 pi < 1 (i.e. there exist poor
generated points). So the reverse KL divergence allows to be negative as shown in Fig. 4(b).
Superiority of MD Gaussian loss. As shown in Fig. 3, MD Gaussian loss gives an good result
on synthetic data. Compared with methods without using Gaussian loss and VEEGAN, InvertGAN
with Gaussian loss does not view the data in isolation, but considers them as a whole for the entire
batch. It is clear that InvertGAN with Gaussian loss performs well as shown in Fig. 3 that every
mode can be recovered and the inverse of real samples are close to the center of standard Gaussian.
Compared with 1D Gaussian, MD Gaussian loss makes the inverse of real data more close to real
Gaussian samples. As shown in the third row in Fig. 3, 1D Gaussian loss can easily make it imbal-
ance and MD Gaussian loss overcomes this limitation. Besides, it is well known that the training of
2The definition is (Meulemeester et al., 2020): if the generated data point is within 3 times standard deviation
of the Gaussian mean, consider it a good generated point and the resulting ratio is used as the generation quality.
7
Under review as a conference paper at ICLR 2021
Table 1: Comparison results on the synthetic data: Ring and Grid (best in bold).
Models	Mode Score↑	2D-Ring Quality% ↑	-RKLZ-	2D-Grid		
				Mode↑	Quality% ↑	RKLZ
GAN	3.6 ± 0.5	98.8 ± 0.6	0.92 ± 0.11-	18.4 ± 1.6	98.0 ± 0.4	0.75 ± 0.25
BiGAN	6.8 ± 1.0	38.6 ± 9.5	0.43 ± 0.18	24.2 ± 1.2	83.4 ± 2.9	0.26 ± 0.20
Unrolled GAN	6.4 ± 2.2	98.6 ± 0.5	0.42 ± 0.53	8.2 ± 1.7	98.7 ± 0.6	1.27 ± 0.17
VEEGAN	5.4 ± 1.2	38.8 ± 16.7	0.40 ± 0.10	20.0 ± 2.6	85.0 ± 5.9	0.41 ± 0.10
InvertGAN (ours)	8.0 ± 0.0	99.0 ± 0.2	0.17 ± 0.06	25.0 ± 0.0	98.0 ± 0.4	0.26 ± 0.12
Table 2: Comparison results on real-world datasets: CIFAR-10 and CIFAR-100 (best in bold).
Models	IS↑	CIFAR10 FIDJ-	MS↑	CIFAR100			STL-10		
				IS↑	FIDI	MS↑	IS↑	FIDI	MS↑
GAN	1.88	262.91	1.86	-5.03	74.41	5.09	2.28	245.21	2.29
BiGAN	3.30	184.37	3.27	4.41	75.27	4.49	1.22	251.21	1.22
Unrolled GAN	4.72	74.05	4.67	6.15	79.14	6.14	4.78	142.16	4.62
VEEGAN	3.71	120.62	3.71	4.80	75.86	4.85	1.45	298.95	1.46
InvertGAN (ours)	4.68	75.80	4.67	5.33	74.26	5.30	5.16	139.10	5.12
GANs is sometimes sensitive to bad initialization which leads to mode missing as shown in Fig .5.
However, MD Gaussian loss performs more robustly.
Comparing different MD Gaussian loss. Fig. 4 shows the results of InvertGAN with different MD
Gaussian losses. Fig.4(a) shows the quality of generation results. It is clear that all the methods are
approaching 100%. However, in Fig. 4(b), we can find the difference and p-norm loss gets a best
result. Thus we use p-norm loss as the MD Gaussian loss to compare with other methods in Table 1.
Comparing different methods. Our InvertGAN is compared with vanilla GAN (Goodfellow et al.,
2014), BiGAN (Donahue et al., 2017), Unrolled GAN (Metz et al., 2017) and VEEGAN (Srivastava
et al., 2017) on Ring and Grid synthetic datasets. It can be found that our InvertGAN obtains the
best and stable performance as shown in Table 1 with significant improvement.
5.2	Experiments on Real-world Dataset
The experiments are performed on three datasets including CIFAR-10, CIFAR-100, and STL-10.
Compared with CIFAR-10, images in STL-10 are of higher resolution. All compared models are
trained by 100K steps (i.e. number of batches whose size is 64). The image quality is assessed
according to the Inception Score (Salimans et al., 2016), Frechet Inception Distance (FID) (HeUSel
et al., 2017) and Mode Score (Che et al., 2017), whose computing are all mainly based on the In-
ception network Szegedy et al. (2016). The results are calculated based on 10K generated images
for CIFAR-10 and CIFAR-100, and based on 5K generated images for STL-10. The detailed infor-
mation for network architectures for the real-world images is given in the appendix.
The comparison is given in Table 2 which suggests InvertGAN performs competitively, especially
on the challenging STL-10 and CIFAR-100. Here we use the p-norm distance as MD Gaussian loss.
Most notably, our InvertGAN has never encountered training failure e.g. gradient explosion which
all the other compared methods struggled during the training in our experiment.
More visual results and comparison are given in the appendix.
6	Conclusion and Outlook
In this paper, we have analyzed the typical cases of mode collapse and define the concept of mode
completeness from the mapping perspective between two probability measures. Our devised inverse
mapping, as well as the multi-dimensional Gaussian loss show their effectiveness to address the
mode collapse issue, on both challenging synthetic dataset and real-world images.
The future work will give more in-depth studies on the role of different loss designs as well as
combination of other techniques to further reduce mode collapse.
8
Under review as a conference paper at ICLR 2021
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In ICML,pp. 214-223, 2017.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. In ICLR, 2017.
JeffDonahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017.
I.	Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. In ICLR, 2017.
Mohamed Elfeki, Camille Couprie, Morgane Riviere, and Mohamed Elhoseiny. Gdpp: Learning
diverse generations using determinantal point processes. In ICML, 2019.
A. Ghosh, V. Kulharia, V. Namboodiri, P. H. Torr, and P. K. Dokania. Multi-agent diverse generative
adversarial networks. In CVPR, 2018.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial nets. In NIPS, 2014.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of
wasserstein gans. In NIPS, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.
T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality,
stability, and variation. In ICLR, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In NIPS, 2016.
Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image gen-
eration via self-conditioned gans. In CVPR, 2020.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In ICML, 2018.
Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. In ICLR, 2017.
Hannes Meulemeester, Joachim Schreurs, Michael Fanuel, Bart Moor, and Johan Suykens. The
bures metric for taming mode collapse in generative adversarial networks. In CVPR, 2020.
T. Nguyen, T. Le, H. Vu, and D. Phung. Dual discriminator generative adversarial nets. In NIPS,
2017.
Gabriel Peyre and Marco Cuturi. Computational optimal transport. 2018.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
ICML, 2015.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems,
2016.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael Gutmann, and Charles Sutton. Veegan:
Reducing mode collapse in gans using implicit variational learning. In NIPS, 2017.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR, 2016.
9
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 FINAL LOSS FOR INTRODUCING A DISCRIMINATOR Dz
We introduce a discriminator to distinguish whether it is from standard Gaussian distribution. We
can get the final loss as
minmaxV(G,D)+Lcons(G,F)+LGau(F,Dz)	(12)
G,F D,Dz
where LGau (F, Dz) can be defined as
Ez〜Pz [log(Dz(z))] + Ex〜Px [log(1 - Dz(F(x)))]	(13)
Through alternating training, we can get the optimal G, F and D, Dz .
A.2 Network Architectures
Table 3: Network Architecture for Inverse F.
Layer	Output size			Kernel
Conv2d	32	X	32	3 × 3, 64
ResnetBlock	32	×	32	3 × 3, 64 3 × 3, 64
AvgPool2d	16	×	16	3 × 3, stride 2
ResnetBlock	16	×	16	3 × 3, 64 3 × 3, 128 1 × 1, 128
AvgPool2d	8	×	8	3 × 3, stride 2
ResnetBlock	8	×	8	3 × 3, 128 3 × 3, 256 1 × 1, 256
AvgPool2d	4	×	4	3 × 3, stride 2
ResnetBlock	4	×	4	3 × 3, 256 3 × 3, 512 1 × 1, 512
Linear	20			
Table 4: Network Architectures for Generator G.		
Layer	Output size	Kernel
Linear	8192 一	
ResnetBlock	4×4	3 X 3, 256 3 X 3, 256 1 X 1, 256
Upsample	8 × 8 一	scale factor = 2.0
ResnetBlock	8×8	3 X 3, 128 3 X 3, 128 1 X 1, 128
Upsample	16 × 16 -	scale factor = 2.0
ResnetBlock	16×16	3 X 3, 64 3 X 3, 64 1 X 1, 64
Upsample	32 × 32 -	scale factor = 2.0
ResnetBlock	32 × 32	3 X 3, 64 3 X 3, 64
Conv2d	32 X 32 一	
10
Under review as a conference paper at ICLR 2021
A.3 More results for Synthetic Data
A.3.1 Ring data
The generation results for Ring compared with other methods are:
STEP IK	STEP3K	STEP9K	STEP 27K	STEP 81K	STEP 300K
Figure 6: Comparison among different methods for Ring .
11
Under review as a conference paper at ICLR 2021
A.3.2 Grid data
The generation results for Grid compared with other methods are:
STEP IK	STEP 3K	STEP9K STEP 27K
Zs Pqo∙lu∩
N493w>
一二
I—
∖ij
z<0eω>⊂-
Figure 7: Comparison among different methods for Grid .
12
Under review as a conference paper at ICLR 2021
A.4 Qualitative Results for Real Data
A.4. 1 Cifar 1 0 Qualitative Results
(b) BiGAN
(a) Original GAN
— 11 ∙ 1
■黑蜀，明g+
苑金扁国朝一■■
(d) VEEGAN
-
9
0□cυu□≡f一•畔
□: EirU
AESB. .3H逸
(c) Unrolled GAN
昌□i
≡r岳
M∙∙F
(1≡口
□昌
(e) InvertGAN
Figure 8: Comparison among different methods for cifar10 .
13
Under review as a conference paper at ICLR 2021
A.4.2 Cifar 1 00 Qualitative Results
(a) Original GAN
护物重湾型
(b) BiGAN
(c) Unrolled GAN
(d) VEEGAN
(e) InvertGAN
Figure 9: Comparison among different methods for cifar100 .
■r1
S ≡BH
:与IgK
14
Under review as a conference paper at ICLR 2021
A.4.3 STL-10 Qualitative Results
(b) BiGAN
(a) Original GAN
Figure 10: Comparison among different methods for STL-10 .
15