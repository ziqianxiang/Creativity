Under review as a conference paper at ICLR 2021
HYPE-C: Evaluating Image Completion Mod-
els Through S tandardized Crowdsourcing
Anonymous authors
Paper under double-blind review
Ab stract
A significant obstacle to the development of new image completion models is the
lack of a standardized evaluation metric that reflects human judgement. Recent
work has proposed the use of human evaluation for image synthesis models, allow-
ing for a reliable method to evaluate the visual quality of generated images. How-
ever, there does not yet exist a standardized human evaluation protocol for image
completion. In this work, we propose such a protocol. We also provide experimen-
tal results of our evaluation method applied to many of the current state-of-the-art
generative image models and compare these results to various automated metrics.
Our evaluation yields a number of interesting findings. Notably, GAN-based im-
age completion models are outperformed by autoregressive approaches.
1	Introduction
Image completion is a form of image generation in which a set of missing pixels within an image
are filled in by a generative model conditioned on the visible pixels. It is is an important problem in
machine learning and has a wide range of real-world applications, including photo repair, content-
aware filling, and denoising. Image completion also acts as a useful proxy task to measure the
progress of generative image models in general, as it is much easier to detect problems such as
mode collapse or memorization in image completion models than in image synthesis models.
Over the past few years, an increasing number of image completion models based on GAN and
autoregressive architectures have been released. But how do we best evaluate and compare these
models? This question is of critical importance for measuring current progress and informing future
research. Despite the development of a plethora of new models, there does not yet exist a standard-
ized evaluation method appropriate for the task of image completion.
Reconstruction error metrics-such as lι error, l2 error and peak Signal-to-noise ratio (PSNR)-are
commonly used, but are a poor method to evaluate such models. There may be many different valid
ways to complete an image other than the original content, and these metrics only judge how well a
model recreates the missing portion of an image in its original state.
Image synthesis models are often compared using automatic metrics based on deep networks fea-
tures, such as the Frechet Inception Distance (Heusel et al., 2017) and the Inception Score (Salimans
et al., 2016). Such techniques have been adapted to provide alternative metrics applicable to image
completion (Zhang et al., 2018), but the use of deep networks to evaluate models is flawed in a num-
ber of ways. Not only can deep networks diverge considerably from human perception, as evidenced
by their failures on adversarial examples (Nguyen et al., 2015), but the evaluation results can vary
substantially depending on the architecture of the evaluation model and the dataset on which it was
trained (Barratt & Sharma, 2018).
For these reasons it has been well recognized that there does not yet exist a satisfactory automatic
procedure for generative model evaluation-particularly in the case of image completion-and that
the gold standard remains human assessment. While it is common to use crowdsourcing platforms
such as Amazon Mechanical Turk to evaluate models, these evaluations are usually ad-hoc and do
not allow for a direct comparison of different models. Recent work has attempted to standardize
human evaluation for image synthesis models and provide an easy to use benchmark. In particular,
the HYPE protocol (Zhou et al., 2019) accepts a set of fully synthetic images, randomly mixes them
with real images taken from the models training set, and asks humans on crowdsouring platforms
1
Under review as a conference paper at ICLR 2021
to rate each image as real or fake. The HYPE score of a model measures how often the generated
images are rated as real by humans.
However, HYPE is not well suited to evaluating image completion models. As it does not provide
standardized image masks to define the regions to complete, it is not possible to guarantee a fair and
consistent score when applied to image completion. Additionally, due to the fact that HYPE does
not define a standardized test set on which to perform completions and selects comparison images
randomly from the training set, it is not possible to directly compare models on a per-image basis.
Our two main contributions are as follows. First, we develop a modified version of HYPE, called
“HYPE-Completion (HYPE-C)”, that resolves these issues and makes it possible to apply HYPE to
image completion models. 1 Second, we use HYPE-C to benchmark a number image completion
models, well-known GAN architectures which we modify to perform image completion, and au-
toregressive models which are able to perform image completion out-of-the-box, in order to create
a baseline for future work. We evaluate the models at different image resolutions on commonly
used datasets, including FFHQ (Karras et al., 2019), CUB (Wah et al., 2011), LSUN-Bedrooms (Yu
et al., 2015), and Stanford Cars (Krause et al., 2013). On each dataset, all models are evaluated on
the same test split that is disjoint from the training images.
Additionally, our HYPE-C evaluation reveals a number of interesting findings. First, state-of-the-
art GAN-based image synthesis models modified to perform image completion often outperform
models designed for image completion from the ground up. Second, GAN-based image completion
methods are substantially outperformed by autoregressive image completion. Third, high quality
image completion remains difficult for all of our evaluated methods across datasets, with the only
exception of faces at lower resolutions. Finally, that a model trained to predict human evaluations
directly can outperform previous automatic evaluation methods, but is still not a reasonable approx-
imation of human judgement.
2	Related Work
2.1	Per-Pixel Perceptual Metrics
Image completion models are often evaluated using simple perceptual or reconstruction error met-
rics. These include metrics such as l1 or l2 error, as well as peak signal-to-noise ratio (PSNR)
and the structural similarity index measure (SSIM). However, while these metrics are well suited to
evaluating the quality of lossy compression algorithms or similar tasks, they are unable to provide a
reasonable assessment of image completion quality as they simply measure the difference between
the original and partially synthetic images at the scale of individual pixels. Under these metrics, an
image completion model may receive a poor score despite being able to consistently create realis-
tic and believable images, if the completed regions diverge significantly from the original image in
pixel space.
2.2	Automatic Evaluation Metrics for Generative Models
Many methods for evaluating generative image models have been proposed, particularly for uncon-
ditional GANs (Borji, 2019). As GANs are prone to overfitting and mode collapse (Arora et al.,
2017; Arora & Zhang, 2017), it is important to not only evaluate the visual fidelity of generated
images, but also their diversity. The most commonly used automatic evaluation metrics are the In-
ception Score (Salimans et al., 2016) and the Frechet Inception Distance (FID) (HeUsel et al., 2017).
The Inception Score uses an Inception Net model pre-trained on ImageNet to label a large number
of generated samples, and measUres the KL divergence between the conditional label distribUtion
p(y | x) and the marginal distribUtion p(y) = p(y | x = G(z))dz. Generated images with a high
visUal fidelity shoUld be more easily labeled and therefore have a conditional label distribUtion with
low entropy, while the marginal distribUtion shoUld have a high entropy when the images are diverse.
Therefore, the Inception Score exp(Eχ[KL(p(y | χ)∣∣p(y))]) simultaneously evaluates both image
1The sUpporting code Used to laUnch HYPE-C evalUations throUgh Amazon Mechanical TUrk will be re-
leased publicly upon publication.
2
Under review as a conference paper at ICLR 2021
quality and diversity. However, the Inception Score may produce deceptive results when applied to
models trained on a dataset other than ImageNet (Salimans et al., 2016; Barratt & Sharma, 2018).
The FID uses the same pre-trained Inception Net model, but does not utilize the label distributions.
Instead, it embeds a set of real images and a set of generated images into the latent feature space
of the Inception Net. Then, modelling the distributions of the embeddings of the real and generated
images as multivariate Gaussians N(μR, Σr) and N(μG, Σr), the FID is the Frechet distance be-
tween them. FID is sensitive to both the visual quality of generated images as well as their diversity.
However, it makes the strong assumption that the feature vectors follow a Gaussian distribution.
Additionally, as in the case of the Inception Score, the FID may not provide accurate results when
applied to models trained on datasets other than ImageNet.
Techniques similar to FID have been used for purposes other than GAN evaluation as well. Loss
functions computed from the features of a pre-trained network are a key component of neural style
transfer (Johnson et al., 2016; Jing et al., 2019) and are frequently used for super-resolution (Johnson
et al., 2016; Ledig et al., 2017; Wang et al., 2018). Zhang et al. (2018) recently proposed adapting
such techniques to create a perceptual distance metric, which outperforms classical metrics such as
PSNR and SSIM in correlation with human evaluations. Although these techniques have produced
impressive results in their respective domains, they are still reliant on pre-trained networks, meaning
that they may produce vastly different results depending upon the specific network architecture used
and the dataset on which the network was trained, and can only act as a rough approximation of how
human beings perceive images.
2.3	Human Evaluation Metrics
Direct human evaluation through crowdsourcing platforms such as Amazon Mechanical Turk pro-
vides an attractive alternative, as it avoids many of the pitfalls of automatic metrics. The recently
produced HYPE (Human eYe Perceptual Evaluation) benchmark (Zhou et al., 2019) stan-
dardized the process of performing human evaluations for generative image models, allowing for
direct comparison. The first HYPE evaluation method, HYPEtime, tasks human evaluators from
Amazon Mechanical Turk with classifying a random sequence of real and fake images. Images are
shown for a varying length of time, and the generative model is scored based on the average min-
imum length of time required for the human evaluators to reliably discern real from fake images.
The second evaluation method, HYPE∞, again shows evaluators a random sequence of real and fake
images, but does not limit the amount of time that each image is displayed, and scores models based
on the percentage of images incorrectly classified by the evaluators (both real and fake).
3	Evaluation Method
Although HYPE provides an effective standardized method to perform human evaluations of image
synthesis models, it is not directly applicable to image completion. In order to adapt HYPE to the
task of image completion, we must define a standardized set of test images, as well as standardized
mask or set of masks to apply to the test images denoting which regions to complete.
For our modified HYPE evaluation protocol, we first select 100 random test images and 100 random
comparison images from the dataset. We then mask the bottom half of the test images and use
the respective image completion model to replace the masked portion, while leaving the comparison
images unchanged. All 200 images are then shown in random order to each of the human evaluators,
who classify each image as real or synthetic, and are given an unlimited amount of time to view each
image. The model’s HYPE-C score is simply the percentage of images (both real and completed)
that are misclassified by the human evaluators. In the scenario that the model produces perfect
completed images, the human evaluators are no longer able to distinguish the difference between
real and partially synthetic images, reaching a HYPE-C score of 50% or more. In our experiments
we use 15 human evaluators for each model evaluation. The primary difference between HYPE-C
and the original HYPE is the use of a standardized test set and image mask.
Note that while masking the bottom half of each image does not necessarily replicate a real-world
use case, it allows us to evaluate the widest range of model architectures, and provides a useful
baseline.
3
Under review as a conference paper at ICLR 2021
Quality Control: To ensure high-quality results, we follow the approach used in HYPE of requiring
evaluators to pass a qualification exam. Potential evaluators are shown a random sequence of50 real
and 50 synthetic images, and must correctly classify at least 65 of the given images. Additionally, to
encourage workers to remain fully focused on their given tasks, we pay evaluators a bonus of $0.02
for every correctly classified image.
4	Experiments
4.1	Datasets
We trained models on the Flickr-Faces-HQ (FFHQ) dataset (Karras et al., 2019), the Caltech-UCSD
Birds-200-2011 (CUB) dataset (Wah et al., 2011), the Stanford Cars dataset (Krause et al., 2013),
the LSUN-Bedroom dataset (Yu et al., 2015), and the LSUN-Cat dataset (Yu et al., 2015) at 32x32,
64x4, and 128x128 resolution.
Train and Test Split: From each dataset, we randomly selected 100 test images and 100 comparison
images from the dataset’s predefined test-set if available. For datasets that did not have a predefined
test-set, we generated a new train-validation-test split, with 100 test images and 100 comparison
images set aside for HYPE-C evaluation. The same train-validation-test splits were used for the
evaluation of all models at all resolutions.
4.2	Evaluated Models
Inpainting Method	Model	Feasible Resolutions
Latent Space Search	StyleGAN (Karras et al., 2019) ProGAN (Karras et al., 2018) WGAN-GP (Gulrajani et al., 2017)	32x32, 64x64, 128x128 32x32, 64x64, 128x128 32x32, 64x64, 128x128
Conditional GAN	Conditional StyleGAN (Karras et al., 2019) Conditional ProGAN (Karras et al., 2018) Conditional WGAN-GP (Gulrajani et al., 2017) DeepFill (Yu et al., 2020)	32x32, 64x64, 128x128 32x32, 64x64, 128x128 32x32, 64x64, 128x128 32x32, 64x64, 128x128
Autoregressive Generation	PixelCNN++ (Salimans et al., 2019) PixelSNAIL (Chen et al., 2018) Pixel Constrained CNN (Dupont & Suresha, 2018)	32x32, 64x64 32x32, 64x64 32x32, 64x64
Table 1: Model Architectures
In Table 1 we list all of the tested model architectures. We evaluated a combination of both GAN-
based image completion methods and autoregressive methods. The evaluated methods can be di-
vided into three categories:
Latent Space Search: We adapt several unconditional GAN architectures to image completion
using the latent space search technique proposed by Yeh et al. (2018). Given any trained GAN
model, we search the latent space for an optimal latent vector, one that maximizes the discriminator
score of the generated image, while minimizing the difference between the generated and the input
image.
Conditional GAN Generation: We also adapt unconditional GAN architecture to perform image
completion by modifying the architecture directly. We swap the latent inputs to the generator with
features conditioned on the upper-half of the image, while making minimal changes to other parts
of the model. More specifically, we adopt an encoder-decoder architecture as the generator, where
the encoder consists of multiple convolutional layers, and the decoder is the one used in the original
generator. Skip connections are made to propagate the high-resolution details from the encoder to
the decoder. In terms of training objectives, in addition to the adversarial term, the generator is also
trained with an image reconstruction term that minimizes the difference between the inpainted and
the original image (Wang et al., 2020). Additionally, we test the DeepFill architecture (Yu et al.,
2020), which is designed for image completion.
4
Under review as a conference paper at ICLR 2021
Autoregressive Generation: Our evaluation method can in general be directly applied to autore-
gressive generative models. As PixelCNN++ (Salimans et al., 2019) and PixelSNAIL (Chen et al.,
2018) generate images pixel-by-pixel in raster scan order, it is not necessary to modify the model ar-
chitecture to generate the bottom half of test images. Additionally, Pixel Constrained CNN (Dupont
& Suresha, 2018) is an autoregressive model specifically designed for image completion and requires
no modification for our evaluation.
4.3	Evaluation Results
Model	FFHQ	Stanford Cars
StyleGAN (Karras et al., 2019)	0.119	0.126
ProGAN (Karras et al., 2018)	0.303	0.159
WGAN-GP (Gulrajani et al., 2017)	0.095	0.164
Conditional StyleGAN (Karras et al., 2019)	0.453	0.160
Conditional ProGAN (Karras et al., 2018)	0.440	0.111
Conditional WGAN-GP (GUkajani et al., 2017)	0.156	0.135
DeepFill (Yu et al., 2020)	0.259	0.080
PixelCNN++ (Salimans et al., 2019)	0.486	0.306
PixelSNAIL (Chen et al., 2018)	0.458	0.293
Pixel Constrained CNN (Dupont & SUreSha, 2018)	0.200	0.166
Table 2: HYPE-C scores of different models on FFHQ and Stanford Cars at 32x32 resolution.
In order to identify and filter out models that are not capable of higher resolution image generation,
we first performed an initial set of evaluations of all model architectures on the FFHQ and Stanford
Cars datasets at 32x32 resolution, the results of which can be seen in Table 2. Autoregressive meth-
ods in particular are often unable to cope with high resolution image generation. On the other hand,
autoregression and conditional GAN models achieve the best results. For a qualitative comparison
of images completed by the selected models, see Figure 1.
Based on these initial results, we selected four top-performing model architectures (ProGAN, Con-
StyleGAN, Con-ProGAN, and PixelCNN++), including at least one model from each category of
image inpainting methods. We then evaluated these models on the remaining datasets and resolutions
(Table 3). Figure 2, 3 show qualitative examples. More are provided in appendix.
Resolution	Dataset	ProGAN	Con-StyleGAN	Con-ProGAN	PixelCNN++
	FFHQ	0.303	0.453	0.4403	0.486
	Stanford Cars	0.159	0.160	0.111	0.306
32x32	CUB	0.331	0.399	0.377	0.376
	LSUN-Bedroom	0.387	0.310	0.415	0.465
	LSUN-Cat	0.353	0.403	0.407	0.461
	FFHQ	0.123	0.307	0.374	0.369
	Stanford Cars	0.088	0.086	0.138	0.239
64x64	CUB	0.122	0.156	0.303	0.337
	LSUN-Bedroom	0.245	0.303	0.352	0.283
	LSUN-Cat	0.129	0.368	0.317	0.374
	FFHQ	0.077	0.218	0.298	x
	Stanford Cars	0.047	0.084	0.050	x
128x128	CUB	0.106	0.146	0.167	x
	LSUN-Bedroom	0.087	0.214	0.269	x
	LSUN-Cat	0.109	0.252	0.227	x
Table 3: Full results for ProGAN, Con-StyleGAN, Con-ProGAN, and PixelCNN++.
5
Under review as a conference paper at ICLR 2021
There are a few notable aspects of these results. First, our modified GAN architectures are able
to outperform models designed for image completion. Second, PixelCNN++ outperforms all of
the other models on every dataset at both 32x32 and 64x64 resolution. Third, even at these low
resolutions, the evaluated models perform poorly on all datasets, with the exception of FFHQ at
32x32 resolution. The current state-of-the-art unconditional GAN models are capable of generating
realistic images at a resolution of 1024x1024 or higher (Karras et al., 2019; 2018), yet perform
poorly when they are tasked with completing half of a low resolution image.
Figure 1: Qualitative comparison between different methods at 32x32 resolution. More results can
be found in the appendix.
4.4	Comparison with Automatic Metrics
We compare FID scores with HYPE-C for each of our evaluated models in Figure 4 in order to
determine how well corresponds to human perception. To evaluate an unconditional GAN model,
one would usually generate a large set of synthetic images and then compute the FID between that
set and the model’s training set. Since we are evaluating image completion models, we complete a
subset of the training set and compute the FID between it and the full unmodified training set. We
complete the same subset of images when evaluating all models at all resolutions.
6
Under review as a conference paper at ICLR 2021
Since we are performing image completions, we are limited to generating a set of samples of equal
size to the training set with which to compute activation statistic. This is particularly problematic
with smaller datasets such as Stanford Cars and CUB, where the training set may have only a few
thousand images. Due to the increased computational cost of image completion compared to image
generation, we also limit the size of our samples to at most 10,000 images per dataset.
Figure 4: FID vs HYPE-C Scores
■ ProGAN
• Con-StyIeGAN
♦ Con-ProGAN
▲ PixeICNN+ +
I--1 32X32
[--1 64x64
[ΞΞ] 128×128
■■ FFHQ
Stanford Cars
CUB
LSUN-Bedroom
LSUN-Cat
It is clear from the figure that the evaluations of model performance using FID and HYPE-C do
not coincide. This is not entirely unexpected, as HYPEtime and HYPE∞ scores were shown to
be uncorrelated with FID by Zhour et al. (Zhou et al., 2019). However, FID measures both image
quality and diversity, while HYPEtime and HYPE∞ only measure image quality, making it somewhat
of an unfair comparison. Since HYPE-C evaluates models based on their ability to complete a set of
images representative of the target distribution, it is sensitive to image diversity, and we can much
more directly compare our results with FID.
We use Spearman rank correlation coefficients to determine how strongly FID correlates with
HYPE-C scores across all models at each resolution. We also measure the correlation between
the human evaluations on individual images and four different perceptual metrics - perceptual dis-
tance (Zhang et al., 2018), peak signal-to-noise ratio (PSNR), the structural similarity index (SSIM),
and mean squared error (MSE). The results can be found in Table 4. A value of ρ = ±1 indicates a
perfect positive or negative correlation, while a value of ρ = 0 indicates no correlation. We see that
FID is nearly completely uncorrelated with HYPE-C, while the perceptual metrics achieve no more
than a weak correlation with human evaluations.
Metric	32x32		64x64		128x128	
	ρ	P	ρ	p	ρ	p
FID	-0.003	0.991	-0.052	0.850	-0.028	0.931
Perceptual Distance	-0.342	0.00	-0.322	0.00	-0.306	0.00
PSNR	0.133	0.00	0.196	0.00	0.240	0.00
SSIM	0.278	0.00	0.373	0.00	0.326	0.00
MSE	-0.133	0.00	-0.196	0.00	-0.240	0.00
Table 4: Spearman Rank Correlation Coefficients of human evaluations and automatic metrics
7
Under review as a conference paper at ICLR 2021
4.5 Human Score Predictor
We attempt to create an automatic evaluation metric in a way similar to the perceptual distance met-
ric (Zhang et al., 2018). We created training, validation, and test sets using the full set of completed
image from our earlier experiments labeled by their individual average human HYPE-C score. We
then trained multiple model architectures to predict the human ratings of the completed images.
We tested a variety of core CNN architectures, including Inception Net, AlexNet, ResNet, and VGG.
For each architecture, we replaced the final classification layer with a small fully connected network.
To prevent over-training, we used early stopping triggered by a stalled validation score. We tested
one version of each model where the core CNN weights are held constant, and one where the core
CNN is fine-tuned. The results can be found in Table 5 and Table 6.
Model	l1 Error	ρ	p
Inception Net V3	0.145	0.764	0.00
AlexNet	0.186	0.637	0.00
ResNet-18	0.158	0.728	0.00
ResNet-34	0.143	0.766	0.00
ResNet-50	0.141	0.778	0.00
ResNet-101	0.136	0.784	0.00
ResNet-152	0.132	0.795	0.00
VGG-11	0.183	0.642	0.00
VGG-13	0.183	0.631	0.00
VGG-16	0.179	0.648	0.00
VGG-19	0.182	0.640	0.00
Table 5: Score prediction using pre-trained
networks as feature extractors.
Model	l1 Error	ρ	p
Inception Net V3	0.142	0.773	0.00
AlexNet	0.162	0.704	0.00
ResNet-18	0.139	0.783	0.00
ResNet-34	0.139	0.772	0.00
ResNet-50	0.140	0.771	0.00
ResNet-101	0.141	0.774	0.00
ResNet-152	0.138	0.777	0.00
VGG-11	0.143	0.764	0.00
VGG-13	0.160	0.710	0.00
VGG-16	0.207	0.523	0.00
VGG-19	0.215	0.517	0.00
Table 6: Score prediction with fine-tuning of
feature extraction networks.
While these results are a substantial improvement over the automatic metrics we have previously
discussed, our score predictor models are not necessarily sufficient to act as a replacement for eval-
uation by real humans. They will also be vulnerable to the same issues as other metrics based
on neural network features, namely that they may produce deceiving results when applied to other
datasets, and may have certain ”blindspots” with respect to specific images i.e. adversarial examples.
5 Conclusion
In this work we introduced HYPE-C, a modified form of HYPE capable of evaluating image comple-
tion models. We provided both qualitative and quantitative experimental results of HYPE-C applied
to a variety of image completion models, forming a baseline for comparison. We showed that well-
known GAN-based image synthesis models modified to perform image completion can outperform
more complex methods in our setting and that autoregressive models can often outperform GANs
in terms of human evaluation at low resolutions. Using our evaluation method, we were able to
perform an analysis of the efficacy of automatic evaluation metrics, and show that they only weakly
correlate with human evaluations. Finally, we evaluated the use of features extracted from a variety
of pre-trained networks as a means to create a proxy for human evaluation.
8
Under review as a conference paper at ICLR 2021
References
Sanjeev Arora and Yi Zhang. Do GANs actually learn the distribution? An empirical study. pp.
1-11, 2017. URL http://arxiv.org/abs/1706.08224.
Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium
in generative adversarial nets (GANs). 34th International Conference on Machine Learning,
ICML 2017, 1:322-349, 2017.
Shane Barratt and Rishi Sharma. A Note on the Inception Score. 2018. URL http://arxiv.
org/abs/1801.01973.
Ali Borji. Pros and cons of GAN evaluation measures. Computer Vision and Image Understanding,
179:41-65, 2019. ISSN 1090235X. doi: 10.1016/j.cviu.2018.10.009.
Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved au-
toregressive generative model. 35th International Conference on Machine Learning, ICML 2018,
2:1364-1372, 2018. URL https://github.com/neocxi/pixelsnail-public.
http://arxiv.org/abs/1712.09763.
Emilien Dupont and Suhas Suresha. Probabilistic Semantic Inpainting with Pixel Constrained
CNNs. 89, 2018. URL http://arxiv.org/abs/1810.03728.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Im-
proved training of wasserstein GANs. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus,
S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems, vol-
ume 2017-Decem, pp. 5768-5778. Curran Associates, Inc., 2017. URL http://papers.
nips.cc/paper/7159-improved-training-of-wasserstein-gans.pdf.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in
Neural Information Processing Systems, 2017-Decem(Nips):6627-6638, 2017. ISSN 10495258.
Yongcheng Jing, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, and Mingli Song. Neural style
transfer: A review. IEEE transactions on visualization and computer graphics, 2019.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for
improved quality, stability, and variation. 6th International Conference on Learning Representa-
tions, ICLR 2018 - Conference Track Proceedings, pp. 1-26, 2018.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarial networks. Proceedings of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, 2019-June:4396-4405, 2019. ISSN 10636919. doi: 10.1109/CVPR.
2019.00453.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3D Object Representations for Fine-
Grained Categorization. In 4th International IEEE Workshop on 3D Representation and Recog-
nition (3dRR-13), Sydney, Australia, 2013.
Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, AIejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic sin-
gle image super-resolution using a generative adversarial network. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 4681-4690, 2017.
Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confi-
dence predictions for unrecognizable images. Proceedings of the IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition, 07-12-June-2015:427-436, 2015. ISSN
10636919. doi: 10.1109/CVPR.2015.7298640.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
9
Under review as a conference paper at ICLR 2021
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving the
PixelCnn with discretized logistic mixture likelihood and other modifications. 5th International
Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 2019.
URL http://arxiv.org/abs/1701.05517.
C Wah, S Branson, P Welinder, P Perona, and S Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical report, 2011.
Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen
Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings
ofthe European Conference on Computer Vision (ECCV),pp. 0-0, 2018.
Yi Wang, Ying-Cong Chen, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Attentive normalization for
conditional image generation. arXiv preprint arXiv:2004.03828, 2020.
Raymond A. Yeh, Teck Yian Lim, Chen Chen, Alexander G. Schwing, Mark Hasegawa-Johnson,
and Minhn Do. Image Restoration with Deep Generative Models. ICASSP, IEEE International
Conference on Acoustics, Speech and Signal Processing - Proceedings, 2018-April:6772-6776,
2018. ISSN 15206149. doi: 10.1109/ICASSP.2018.8462317.
Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of
a Large-scale Image Dataset using Deep Learning with Humans in the Loop. arXiv preprint
arXiv:1506.03365, 2015.
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-Form Image
Inpainting With Gated Convolution. 2019 IEEE/CVF International Conference on Computer
Vision (ICCV), pp. 4470-4479, 2020. doi: 10.1109/iccv.2019.00457.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 586-595, 2018.
Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Durim Morina, and
Michael S. Bernstein. Hype: Human eye perceptual evaluation of generative models. Deep
Generative Models for Highly Structured Data, DGS@ICLR 2019 Workshop, (NeurIPS), 2019.
10
Under review as a conference paper at ICLR 2021
A Qualitative and Automatic Metric Results
A.1 FFHQ
A.1.1 32x32
Figure 5: Ground truth for FFHQ at 32x32 resolution.
11
Under review as a conference paper at ICLR 2021
Figure 6: FFHQ 32x32 results for StyleGAN.
12
Under review as a conference paper at ICLR 2021
■cacn则∙∙p□
Human: 0.667
Perceptual: 1.666
MSE: 1710.503
PSNR: 15.800
SSIM: 0.687
Human: 0.867
Perceptual: 1.512
MSE: 1301.806
PSNR: 16.985
SSIM: 0.747
Human: 0.333
Perceptual: 1.967
MSE: 597.076
PSNR: 20.371
SSIM: 0.704
Human： 0.067
Perceptual: 2.128
MSE: 2607.054
PSNR: 13.969
SSIM: 0.695
Human: 1.000
Perceptual: 1.266
MSE: 863.396
PSNR: 18.769
SSIM: 0.807
Human: 0.200
Perceptual: 4.757
MSE: 5953.506
PSNR: 10.383
SSIM: 0.579
Human: 0.000
Perceptual: 4.068
MSE: 2415.062
PSNR: 14.302
SSIM: 0.549
Human: 0.133
Perceptual: 1.975
MSE: 1998.947
PSNR: 15.123
SSIM: 0.673
Human: 0.133
Perceptual: 2.024
MSE: 1122.705
PSNR: 17.628
SSIM: 0.624
Human: 0.400
Perceptual: 3.266
MSE: 5963.152
PSNR: 10.376
SSIM: 0 675
Figure 7: FFHQ 32x32 results for ProGAN.
13
Under review as a conference paper at ICLR 2021
Figure 8: FFHQ 32x32 results for WGAN-GP.
14
Under review as a conference paper at ICLR 2021
■ ■■■IE Uli目由足
Human: 0.467
Perceptual: 2.147
MSE: 1096.743
PSNR: 17.730
SSIM: 0.755
Human: 0.867
Perceptual: 1.716
MSE: 1265.258
PSNR: 17.109
SSIM: 0.779
Human: 0.533
Perceptual: 2.088
MSE: 558.400
PSNR: 20.661
SSIM: 0.756
Human： 0.800
Perceptual: 1.373
MSE: 719.069
PSNR: 19.563
SSIM: 0.805
Human: 0.467
Perceptual: 2.186
MSE: 893.892
PSNR: 18.618
SSIM: 0.680
Human: 0.600
Perceptual: 1.184
MSE: 411.978
PSNR: 21.982
SSIM: 0.873
Human: 0.400
Perceptual: 1.677
MSE: 1175.150
PSNR: 17.430
SSIM: 0.730
Human: 0.933
Perceptual: 1.476
MSE: 1373.757
PSNR: 16.752
SSIM: 0.759
Human: 0.267
Perceptual: 1.987
MSE: 1848.679
PSNR: 15.462
SSIM: 0.649
Human: 0.733
Perceptual: 1.096
MSE: 531.784
PSNR: 20.873
SSIM: 0.873
Human: 0.533
Perceptual: 1.704
MSE: 734.640
PSNR: 19.470
aisn
Human: 0.800
Perceptual: 1.619
MSE: 1340.085
PSNR: 16.859
Human: 0.867
Perceptual: 1.603
MSE: 2508.763
PSNR: 14.136
Human: 0.667
Perceptual: 1.636
MSE: 1482.483
PSNR: 16.421

Human: 0.800
Perceptual: 0.818
MSE: 296.909
Human: 1.000
Perceptual: 1.603
MSE: 1189.305
PSNR: 17.378
Human: 0.600
Perceptual: 1.534
MSE: 1352.379
PSNR: 16.820
Human: 0.600
Perceptual: 1.703
MSE: 1046.409
PSNR: 17.934
EI■>■■电
Human: 0.733
Perceptual: 0.832
MSE: 738.866
PSNR: 19.445
SSIM: 0.909
Human: 0.867
Perceptual: 1.405
MSE: 446.514
PSNR: 21.632
SSIM: 0.778
Human: 0.733
Perceptual: 1.756
MSE: 917.824
PSNR: 18.503
SSIM: 0.768
Human： 0.933
Perceptual: 1.210
MSE: 1279.149
PSNR: 17.062
SSIM: 0.811
Human: 0.867
Perceptual: 1.126
MSE: 847.202
PSNR: 18.851
SSIM: 0.869
Human: 0.733
Perceptual: 2.728
MSE: 834.258
PSNR: 18.918
SSIM: 0.792
Human: 0.867
Perceptual: 2.191
MSE: 3167.540
PSNR: 13.124
SSIM: 0.666
Human: 0.933
Perceptual: 1.267
MSE: 557.635
PSNR: 20.667
SSIM: 0.779
Human: 0.667
Perceptual: 1.645
MSE: 1132.379
PSNR: 17.591
SSIM: 0.688
Human: 0.933
Perceptual: 1.726
MSE: 2006.048
PSNR: 15.107
SSIM: 0 748
口令俺汀hr要嫡毋
Human: 0.800
Perceptual: 1.509
MSE: 1111.564
PSNR: 17.671
SSIM: 0.759
Human: 0.400
Perceptual: 2.230
MSE: 1046.869
PSNR: 17.932
SSIM: 0.762
Human: 0.733
Perceptual: 1.767
MSE: 1133.052
PSNR: 17.588
SSIM: 0.800
Human： 0.867
Perceptual: 1.025
MSE: 572.907
PSNR: 20.550
SSIM: 0.854
Human: 0.867
Perceptual: 1.763
MSE: 715.070
PSNR: 19.587
SSIM: 0.849
Human: 0.533
Perceptual: 2.363
MSE: 4030.616
PSNR: 12.077
SSIM: 0.653
Human: 0.800
Perceptual: 1.871
MSE: 1347.502
PSNRi 16.836
SSIM: 0.675
Human: 0.667
Perceptual: 1.970
MSE: 2033.063
PSNR: 15.049
SSIM: 0.606
Human: 0.400
Perceptual: 2.807
MSE: 422.252
PSNR: 21.875
SSIM: 0.788
Human: 0.867
Perceptual: 1.435
MSE: 1546.365
PSNR: 16.238
SSIM: 0 684
Figure 9: FFHQ 32x32 results for Conditional StyleGAN.
15
Under review as a conference paper at ICLR 2021
Figure 10: FFHQ 32x32 results for Conditional ProGAN.
16
Under review as a conference paper at ICLR 2021

Figure 11: FFHQ 32x32 results for Conditional WGAN-GP.
17
Under review as a conference paper at ICLR 2021
Figure 12: FFHQ 32x32 results for DeepFill.

18
Under review as a conference paper at ICLR 2021
Human: 0.600
Perceptual: 1.459
MSE: 2144.115
PSNR: 14.818
SSIM: 0.732
Human: 0.533
Perceptual: 1.869
MSE: 1664.424
PSNR: 15.918
SSIM: 0.765


Human: 0.800
Perceptual: 1.574
MSE: 2192.251
PSNR: 14.722
SSIM: 0.725
Human: 0.867
Perceptual: 2.210
MSE: 1847.872
PSNR: 15.464
SSIM: 0.644
Human: 0.533
Perceptual: 2.104
MSE: 2901.015
PSNR: 13.505
SSIM: 0.659
Human： 0.800
Perceptual: 1.051
MSE: 803.645
PSNR: 19.080
SSIM: 0.877
Human: 0.933
Perceptual: 1.042
MSE: 2465.783
PSNR: 14.211
SSIM: 0.803
Human: 0.667
Perceptual: 1.521
MSE: 2767.053
PSNR: 13.711
SSIM: 0.723
Human: 0.933
Perceptual: 1.009
MSE: 1249.650
PSNR: 17.163
SSIM: 0.808
Human: 0.733
Perceptual: 1.407
MSE: 1276.447
PSNR: 17.071
SSIM: 0.769
息日
Human: 0.800
Perceptual: 1.672
MSE: 1234.278
PSNR: 17.217
SSIM: 0.702
Human: 0.733
Perceptual: 2.090
MSE: 1338.182
PSNR: 16.866
SSIM: 0 809
软。野鼠■同■
Human: 0.800
Perceptual: 0.940
MSE: 497.494
PSNR: 21.163
SSIM: 0.849
Human: 0.667
Perceptual: 1.347
MSE: 1236.587
PSNR: 17.209
SSIM: 0.800
Human: 0.333
Perceptual: 2.569
MSE: 2583.505
PSNR: 14.009
SSIM: 0.698
Human: 0.733
Perceptual: 1.128
MSE: 354.666
PSNR: 22.633
SSIM: 0.806
Human： 0.800
Perceptual: 0.939
MSE: 862.736
PSNR: 18.772
SSIM: 0.790
Human: 0.533
Perceptual: 1.612
MSE: 2785.404
PSNR: 13.682
SSIM: 0.658
Human: 0.800
Perceptual: 1.416
MSE: 894.332
PSNR: 18.616
SSIM: 0.740
Human: 0.400
Perceptual: 2.268
MSE: 2405.061
PSNR: 14.320
SSIM: 0.742
Human: 0.667
Perceptual: 1.867
MSE: 2085.538
PSNR: 14.939
SSlM: 0.734
Human: 0.867
Perceptual: 1.459
MSE: 1354.192
PSNR: 16.814
SSIM: 0.749
Human: 0.667
Perceptual: 1.534
MSE: 1090.963
PSNR: 17.753
SSIM: 0 735
Human: 0.733
Perceptual: 1.573
MSE: 1805.711
PSNR: 15.564
SSIM: 0 727
Figure 13: FFHQ 32x32 results for PixelCNN++.
19
Under review as a conference paper at ICLR 2021
Figure 14: FFHQ 32x32 results for PixelSNAIL.
20
Under review as a conference paper at ICLR 2021
Figure 15: FFHQ 32x32 results for Pixel Constrained CNN.
21
Under review as a conference paper at ICLR 2021
A.1.2 64x64
Figure 16: Ground truth for FFHQ at 64x64 resolution.
22
Under review as a conference paper at ICLR 2021
Figure 17: FFHQ 64x64 results for ProGAN.
23
Under review as a conference paper at ICLR 2021
Figure 18: FFHQ 64x64 results for Conditional StyleGAN.
24
Under review as a conference paper at ICLR 2021
Figure 19: FFHQ 64x64 results for Conditional ProGAN.
25
Under review as a conference paper at ICLR 2021
Figure 20: FFHQ 64x64 results for PixelCNN++.
26
Under review as a conference paper at ICLR 2021
A.1.3 128x128
■	ElQSH
Figure 21: Ground truth for FFHQ at 128x128 resolution.
27
Under review as a conference paper at ICLR 2021
Figure 22: FFHQ 128x128 results for ProGAN.
28
Under review as a conference paper at ICLR 2021
Figure 23: FFHQ 128x128 results for Conditional StyleGAN.
29
Under review as a conference paper at ICLR 2021
Figure 24: FFHQ 128x128 results for Conditional ProGAN.
30
Under review as a conference paper at ICLR 2021
A.2 Stanford Cars
A.2.1 32x32
Figure 25: Ground truth for Stanford Cars at 32x32 resolution.
31
Under review as a conference paper at ICLR 2021
Figure 26: Stanford Cars 32x32 results for StyleGAN.
32
Under review as a conference paper at ICLR 2021
Figure 27: Stanford Cars 32x32 results for ProGAN.
33
Under review as a conference paper at ICLR 2021
Figure 28: Stanford Cars 32x32 results for WGAN-GP.
34
Under review as a conference paper at ICLR 2021
Figure 29: Stanford Cars 32x32 results for Conditional StyleGAN.
35
Under review as a conference paper at ICLR 2021
Figure 30: Stanford Cars 32x32 results for Conditional ProGAN.
36
Under review as a conference paper at ICLR 2021
Figure 31: Stanford Cars 32x32 results for Conditional WGAN-GP.
37
Under review as a conference paper at ICLR 2021
Figure 32: Stanford Cars 32x32 results for DeepFill.
38
Under review as a conference paper at ICLR 2021
Figure 33: Stanford Cars 32x32 results for PixelCNN++.
39
Under review as a conference paper at ICLR 2021
Figure 34: Stanford Cars 32x32 results for PixelSNAIL.
40
Under review as a conference paper at ICLR 2021
Figure 35: Stanford Cars 32x32 results for Pixel Constrained CNN.
41
Under review as a conference paper at ICLR 2021
A.2.2 64x64
Figure 36: Ground truth for Stanford Cars at 64x64 resolution.
42
Under review as a conference paper at ICLR 2021
Figure 37: Stanford Cars 64x64 results for ProGAN.
43
Under review as a conference paper at ICLR 2021
Figure 38: Stanford Cars 64x64 results for Conditional StyleGAN.
44
Under review as a conference paper at ICLR 2021
Figure 39: Stanford Cars 64x64 results for Conditional ProGAN.
45
Under review as a conference paper at ICLR 2021
Figure 40: Stanford Cars 64x64 results for PixelCNN++.
46
Under review as a conference paper at ICLR 2021
A.2.3 128x128
Figure 41: Ground truth for Stanford Cars at 128x128 resolution.
47
Under review as a conference paper at ICLR 2021
Figure 42: Stanford Cars 128x128 results for ProGAN.
48
Under review as a conference paper at ICLR 2021
Figure 43: Stanford Cars 128x128 results for Conditional StyleGAN.
49
Under review as a conference paper at ICLR 2021
Figure 44: Stanford Cars 128x128 results for Conditional ProGAN.
50
Under review as a conference paper at ICLR 2021
A.3 CUB
A.3.1 32x32
Figure 45: Ground truth for CUB at 32x32 resolution.
51
Under review as a conference paper at ICLR 2021
Figure 46: CUB 32x32 results for ProGAN.
52
Under review as a conference paper at ICLR 2021
Human: 0.600
Perceptual: 3.176
MSE: 1513.161
PSNR: 16.332
Human: 0.333
Perceptual: 2.388
MSE: 879.240
PSNR: 18.690
Human: 0.533
Perceptual: 3.598
MSE: 423.347
PSNR: 21.864
SSIM: 0.520
Human: 0.533
Perceptual: 4.375
MSE: 369.321
PSNR: 22.457
SSIM: 0.663
Human: 0.733
Perceptual: 3.935
MSE: 445.933
PSNR: 21.638
SSIM: 0.614
Human: 0.600
Perceptual: 3.500
MSE: 1227.960
PSNR: 17.239
SSIM: 0.600
Human: 0.533
Perceptual: 3.800
MSE: 2451.114
PSNR: 14.237
SSIM: 0.587
Human: 0.800
Perceptual: 3.450
MSE: 1869.688
PSNR: 15.413
SSIM: 0.548
Human: 0.800
Perceptual: 3.997
MSE: 1155.792
PSNR: 17.502
SSIM: 0.482
Human: 0.867
Perceptual: 2.797
MSE: 655.252
PSNR: 19.967
SSIM: 0 582
Human: 0.533
Perceptual: 3.670
MSE: 3204.803
PSNR: 13.073
Human: 0.400
Perceptual: 3.648
MSE: 754.849
PSNR: 19.352
SSIM: 0.544
Human: 0.800
Perceptual: 1.910
MSE: 699.292
PSNR: 19.684
SSIM: 0.560
Human: 0.267
Perceptual: 3.503
MSE: 3154.498
PSNR: 13.142
SSIM: 0.519
Human: 0.267
Perceptual: 4.292
MSE: 2355.168
PSNR: 14.411
Human: 0.200
Perceptual: 2.794
MSE: 254.614
PSNR: 24.072
Human: 0.933
Perceptual: 3.669
MSE: 1214.293
PSNR: 17.288
Human: 0.667
Perceptual: 2.915
MSE: 645.047
PSNR: 20.035
SSIM: 0.683
Human: 0.533
Perceptual: 4.435
MSE: 671.348
PSNR: 19.861
Human: 0.667
Perceptual: 3.280
MSE: 661.618
PSNR: 19.925
Human: 0.600
Perceptual: 2.984
MSE: 1045.201
PSNR: 17.939
SSIM: 0.654
Human: 0.400
Perceptual: 3.701
MSE: 291.361
PSNR: 23.486
SSIM: 0.695
Human: 0.467
Perceptual: 2.661
MSE: 1533.926
PSNR: 16.273
SSIM: 0.619
Human: 0.400
Perceptual: 3.071
MSE: 474.328
PSNR: 21.370
SSIM: 0.554
Human: 0.933
Perceptual: 3.816
MSE: 1691.158
PSNR: 15.849
SSIM: 0.531
Human: 0.733
Perceptual: 3.473
MSE: 2181.699
PSNR: 14.743
SSIM: 0.521
Human: 0.733
Perceptual: 2.565
MSE: 1251.469
PSNR: 17.157
SSIM: 0.506
Human: 0.867
Perceptual: 4.520
MSE: 2418.410
PSNR: 14.296
SSIM: 0.579
Human: 0.133
Perceptual: 2.576
MSE: 743.800
PSNR: 19.416
SSIM: 0.598
Human: 0.467
Perceptual: 4.277
MSE: 1626.444
PSNR: 16.018
SSIM: 0 598

Human: 0.733
Perceptual: 3.398
MSE: 1075.113
PSNR: 17.816
SSIM: 0.564
Human: 0.533
Perceptual: 3.892
MSE: 2951.509
PSNR: 13.430
SSIM: 0.557
Human: 0.733
Perceptual: 3.234
MSE: 697.734
PSNR: 19.694
SSIM: 0.634
Human: 0.800
Perceptual: 3.857
MSE: 1030.827
PSNR: 17.999
SSIM: 0.547
Human: 0.600
Perceptual: 2.513
MSE: 1009.829
PSNR: 18.088
SSIM: 0.572
Human: 0.267
Perceptual: 4.185
MSE: 1745.314
PSNR: 15.712
SSIM: 0.548
Human: 0.733
Perceptual: 4.487
MSE: 835.862
PSNR: 18.909
SSIM: 0.650
Human: 0.667
Perceptual: 4.899
MSE: 2664.807
PSNR: 13.874
Human: 0.800
Perceptual: 3.371
MSE: 891.118
PSNR: 18.631
SSIM: 0.651
Figure 47: CUB 32x32 results for Conditional StyleGAN.
Human: 0.667
Perceptual: 3.389
MSE: 3011.754
PSNR: 13.343
SSIM: 0.530
53
Under review as a conference paper at ICLR 2021
Figure 48: CUB 32x32 results for Conditional ProGAN.
54
Under review as a conference paper at ICLR 2021
Figure 49: CUB 32x32 results for PixelCNN++.
55
Under review as a conference paper at ICLR 2021
A.3.2 64x64
Figure 50: Ground truth for CUB at 64x64 resolution.
56
Under review as a conference paper at ICLR 2021
Figure 51: CUB 64x64 results for ProGAN.
57
Under review as a conference paper at ICLR 2021
Figure 52: CUB 64x64 results for Conditional StyleGAN.
58
Under review as a conference paper at ICLR 2021
Human: 0.267
Perceptual: 4.397
MSE: 1995.694
PSNR: 15.130
Human: 0.333
Perceptual: 3.486
MSE: 3148.522
PSNR: 13.150
Human: 0.467
Perceptual: 3.770
MSE: 1638.360
PSNR: 15.987
Human： 0.667
Perceptual: 3.760
MSE: 1707.672
PSNR: 15.807
Human: 0.133
Perceptual: 3.539
MSE: 2010.491
PSNR: 15.098
Human: 0.533
Perceptual: 3.539
MSE: 2299.781
PSNR: 14.514
Human: 0.333
Perceptual: 3.699
MSE: 2490.684
PSNR: 14.168
Human: 0.400
Perceptual: 3.809
MSE: 1654.517
PSNR: 15.944
Figure 53: CUB 64x64 results for Conditional ProGAN.
59
Under review as a conference paper at ICLR 2021
Figure 54: CUB 64x64 results for PixelCNN++.
60
Under review as a conference paper at ICLR 2021
A.3.3 128x128
Figure 55: Ground truth for CUB at 128x128 resolution.
61
Under review as a conference paper at ICLR 2021
Figure 56: CUB 128x128 results for ProGAN.
62
Under review as a conference paper at ICLR 2021
Figure 57: CUB 128x128 results for Conditional StyleGAN.
63
Under review as a conference paper at ICLR 2021
Figure 58: CUB 128x128 results for Conditional ProGAN.
64
Under review as a conference paper at ICLR 2021
A.4 LSUN-Bedroom
A.4.1 32x32
Figure 59: Ground truth for LSUN-Bedroom at 32x32 resolution.
65
Under review as a conference paper at ICLR 2021
Figure 60: LSUN-Bedroom 32x32 results for ProGAN.
66
Under review as a conference paper at ICLR 2021
中湮■园■■■，事安
Human: 0.000
Perceptual: 4.036
MSE: 3165.975
PSNR: 13.126
SSIM: 0.601
Human: 0.467
Perceptual: 2.765
MSE: 1178.018
PSNR: 17.419
SSIM: 0.691
Human: 0.333
Perceptual: 3.029
MSE: 607.062
PSNR: 20.298
SSIM: 0.637
Human: 0.800
Perceptual: 2.869
MSE: 4047.454
PSNR: 12.059
SSIM: 0.605
Human: 0.067
Perceptual: 2.326
MSE: 640.847
PSNR: 20.063
SSIM: 0.702
Human: 0.400
Perceptual: 2.533
MSE: 872.507
PSNR: 18.723
SSIM: 0.731
Human: 0.267
Perceptual: 3.240
MSE: 2837.471
PSNR: 13.601
SSIM: 0.589
Human: 0.867
Perceptual: 3.075
MSE: 2209.110
PSNR: 14.689
SSIM: 0.473
Human: 0.133
Perceptual: 3.102
MSE: 981.050
PSNR: 18.214
SSIM: 0.588
Human: 0.400
Perceptual: 3.587
MSE: 1595.653
PSNR: 16.101
SSIM: 0 642
Human: 0.267
Perceptual: 2.990
MSE: 720.568
PSNR: 19.554
SSIM: 0.614
Human: 0.467
Perceptual: 3.140
MSE: 1856.878
PSNR: 15.443
SSIM: 0.617
Human: 0.733
Perceptual: 2.459
MSE: 1450.735
PSNR: 16.515
SSIM: 0.662
Human: 0.267
Perceptual: 3.219
MSE: 3583.461
PSNR: 12.588
SSIM: 0.563
Human: 0.400
Perceptual: 2.785
MSE: 758.549
PSNR: 19.331
SSIM: 0.661
Human: 0.867
Perceptual: 3.851
MSE: 2434.394
PSNR: 14.267
SSIM: 0.585
Human: 0.600
Perceptual: 3.046
MSE: 1416.516
PSNR: 16.619
SSIM: 0.614
Human: 0.800
Perceptual: 2.324
MSE: 1470.237
PSNR: 16.457
SSIM: 0.769
Human: 0.400
Perceptual: 3.098
MSE: 3237.966
PSNR: 13.028
SSIM: 0.653
F
Human: 0.600
Perceptual: 2.945
MSE: 656.294
PSNR: 19.960
SSIM: 0.757
Human: 0.467
Perceptual: 3.142
MSE: 1405.130
PSNR: 16.654
SSIM: 0.598
Human: 0.400
Perceptual: 3.123
MSE: 2095.773
PSNR: 14.917
SSIM: 0 644

Human: 0.533
Perceptual: 3.091
MSE: 1951.109
PSNR: 15.228
SSIM: 0.611
Human: 0.333
Perceptual: 3.407
MSE: 2592.493
PSNR: 13.994
SSIM: 0.634
Human: 0.733
Perceptual: 3.870
MSE: 3393.866
PSNR: 12.824
SSIM: 0.636
Human: 0.533
Perceptual: 3.253
MSE: 1072.207
PSNR: 17.828
SSIM: 0.633
Human: 0.400
Perceptual: 3.449
MSE: 819.993
PSNR: 18.993
SSIM: 0.661
Human: 0.200
Perceptual: 2.676
MSE: 770.675
PSNR: 19.262
SSIM: 0.676
Human: 0.533
Perceptual: 2.910
MSE: 1541.252
PSNR: 16.252
SSIM: 0.617
Human: 0.733
Perceptual: 3.353
MSE: 2236.003
PSNR: 14.636
SSIM: 0 724
Figure 61: LSUN-Bedroom 32x32 results for Conditional StyleGAN.
67
Under review as a conference paper at ICLR 2021
Figure 62: LSUN-Bedroom 32x32 results for Conditional ProGAN.
68
Under review as a conference paper at ICLR 2021
Figure 63: LSUN-Bedroom 32x32 results for PixelCNN++.
69
Under review as a conference paper at ICLR 2021
A.4.2 64x64
Figure 64: Ground truth for LSUN-Bedroom at 64x64 resolution.
70
Under review as a conference paper at ICLR 2021
Figure 65: LSUN-Bedroom 64x64 results for ProGAN.
71
Under review as a conference paper at ICLR 2021
Figure 66: LSUN-Bedroom 64x64 results for Conditional StyleGAN.
72
Under review as a conference paper at ICLR 2021
Figure 67: LSUN-Bedroom 64x64 results for Conditional ProGAN.
73
Under review as a conference paper at ICLR 2021
Figure 68: LSUN-Bedroom 64x64 results for PixelCNN++.
74
Under review as a conference paper at ICLR 2021
A.4.3 128x128
Figure 69: Ground truth for LSUN-Bedroom at 128x128 resolution.
75
Under review as a conference paper at ICLR 2021
Figure 70: LSUN-Bedroom 128x128 results for ProGAN.
76
Under review as a conference paper at ICLR 2021
Figure 71: LSUN-Bedroom 128x128 results for Conditional StyleGAN.
77
Under review as a conference paper at ICLR 2021
Figure 72: LSUN-Bedroom 128x128 results for Conditional ProGAN.
78
Under review as a conference paper at ICLR 2021
A.5 LSUN-Cat
A.5.1 32x32
Figure 73: Ground truth for LSUN-Cat at 32x32 resolution.
79
Under review as a conference paper at ICLR 2021
Figure 74: LSUN-Cat 32x32 results for ProGAN.
80
Under review as a conference paper at ICLR 2021
Figure 75: LSUN-Cat 32x32 results for Conditional StyleGAN.
81
Under review as a conference paper at ICLR 2021
Figure 76: LSUN-Cat 32x32 results for Conditional ProGAN.
82
Under review as a conference paper at ICLR 2021

Human: 0.467
Perceptual: 2.546
MSE: 3188.136
PSNR: 13.095
SSIM: 0.591
Human: 1.000
Perceptual: 2.848
MSE: 2222.461
PSNR: 14.662
SSIM: 0.623
Human: 0.400
Perceptual: 2.727
MSE: 1741.525
PSNR: 15.722
SSIM: 0.616
Human： 0.800
Perceptual: 2.386
MSE: 974.013
PSNR: 18.245
SSIM: 0.601
Human: 0.600
Perceptual: 3.533
MSE: 4490.450
PSNR: 11.608
SSIM: 0.576
Human: 0.333
Perceptual: 2.735
MSE: 1408.275
PSNR: 16.644
SSIM: 0.685
Human: 0.467
Perceptual: 4.044
MSE: 2605.378
PSNR: 13.972
SSIM: 0.559
Human: 0.600
Perceptual: 2.132
MSE: 3090.734
PSNR: 13.230
SSIM: 0.683
Human: 0.933
Perceptual: 2.882
MSE: 1467.240
PSNR: 16.466
SSIM: 0.730
Human: 0.200
Perceptual: 3.675
MSE: 1984.715
PSNR: 15.154
SSIM: 0 647
Figure 77: LSUN-Cat 32x32 results for PixelCNN++.
83
Under review as a conference paper at ICLR 2021
A.5.2 64x64
Figure 78: Ground truth for LSUN-Cat at 64x64 resolution.
84
Under review as a conference paper at ICLR 2021
τιιman: o.(iικ:
Figure 79: LSUN-Cat 64x64 results for ProGAN.
85
Under review as a conference paper at ICLR 2021
倒区盟鹫0∙iS SIdil
Human: 0.200
Perceptual: 3.052
MSE: 1622.166
PSNR: 16.030
SSIM: 0.628
Human: 0.333
Perceptual: 3.710
MSE: 1361.071
PSNR: 16.792
SSIM: 0.615
Human: 0.133
Perceptual: 2.624
MSE: 1373.049
PSNR: 16.754
SSIM: 0.654
Human： 0.200
Perceptual: 3.351
MSE: 2139.238
PSNR: 14.828
SSIM: 0.559
Human: 0.467
Perceptual: 3.909
MSE: 4035.924
PSNR: 12.071
SSIM: 0.568
Human: 0.467
Perceptual: 3.554
MSE: 1575.998
PSNR: 16.155
SSIM: 0.609
Human: 0.533
Perceptual: 3.446
MSE: 2634.178
PSNR: 13.924
SSIM: 0.563
Human: 0.467
Perceptual: 4.411
MSE: 2100.526
PSNR: 14.908
SSIM: 0.597
Human: 0.800
Perceptual: 3.820
MSE: 1794.610
PSNR: 15.591
SSIM: 0.551
Human: 0.733
Perceptual: 2.872
MSE: 712.825
PSNR: 19.601
SSIM: 0 704

Human: 0.667
Perceptual: 3.234
MSE: 1956.786
PSNR: 15.215
SSIM: 0.594
Human: 0.600
Perceptual: 3.087
MSE: 918.366
PSNR: 18.501
SSIM: 0.683
Human: 0.200
Perceptual: 3.700
MSE: 1457.538
PSNR: 16.495
SSIM: 0.610
Human： 0.533
Perceptual: 3.746
MSE: 561.240
PSNR: 20.639
SSIM: 0.609
Human: 0.133
Perceptual: 3.443
MSE: 3453.649
PSNR: 12.748
SSIM: 0.684
Human: 0.533
Perceptual: 3.359
MSE: 1570.307
PSNR: 16.171
SSIM: 0.653
Human: 0.067
Perceptual: 3.062
MSE: 1700.119
PSNR: 15.826
SSIM: 0.561
Human: 0.067
Perceptual: 3.071
MSE: 5450.305
PSNR: 10.767
SSIM: 0.605
Human: 0.467
Perceptual: 3.374
MSE: 721.940
PSNR: 19.546
SSIM: 0.667
Human: 0.533
Perceptual: 3.785
MSE: 1492.141
PSNR: 16.393
SSIM: 0 608
Figure 80: LSUN-Cat 64x64 results for Conditional StyleGAN.
86
Under review as a conference paper at ICLR 2021

Human: 0.000
Perceptual: 3.705
MSE: 3273.981
PSNR: 12.980
SSIM: 0.528
Human: 0.200
Perceptual: 4.181
MSE: 2900.728
PSNR: 13.506
SSIM: 0.530
Human: 0.200
Perceptual: 4.094
MSE: 3347.125
PSNR: 12.884
SSIM: 0.527
Human: 0.667
Perceptual: 3.105
MSE: 2440.390
PSNR: 14.256
SSIM: 0.751
Human: 0.533
Perceptual: 3.426
MSE: 1773.272
PSNR: 15.643
SSIM: 0.624
Human: 0.400
Perceptual: 4.089
MSE: 2341.834
PSNR: 14.435
SSIM: 0.522
Human: 0.267
Perceptual: 2.836
MSE: 633.089
PSNR: 20.116
SSIM: 0.648
Human: 0.467
Perceptual: 4.605
MSE: 3877.499
PSNR: 12.245
SSIM: 0.541
Human: 0.333
Perceptual: 3.754
MSE: 2155.031
PSNR: 14.796
SSIM: 0.546
Human: 0.200
Perceptual: 3.024
MSE: 1051.082
PSNR: 17.914
SSIM: 0 672
巴■物事&后造一由n
Human: 0.200
Perceptual: 2.674
MSE: 1309.422
PSNR: 16.960
SSIM: 0.625
Human: 0.400
Perceptual: 3.261
MSE: 763.696
PSNR: 19.302
SSIM: 0.678
Human: 0.000
Perceptual: 2.258
MSE: 1102.571
PSNR: 17.707
SSIM: 0.685
Human: 0.333
Perceptual: 3.624
MSE: 3046.104
PSNR: 13.293
SSIM: 0.535
Human: 0.333
Perceptual: 3.889
MSE: 3606.985
PSNR: 12.559
SSIM: 0.574
Human: 0.800
Perceptual: 4.138
MSE: 2512.339
PSNR: 14.130
SSIM: 0.583
Human: 0.467
Perceptual: 3.742
MSE: 2750.998
PSNR: 13.736
SSIM: 0.559
Human: 0.333
Perceptual: 4.186
MSE: 3753.101
PSNR: 12.387
SSIM: 0.599
Human: 0.733
Perceptual: 3.921
MSE: 5317.597
PSNR: 10.874
SSIM: 0.550
Human: 0.467
Perceptual: 2.799
MSE: 370.354
PSNR: 22.445
SSIM: 0 740
或d∙∙*∙E∙∙o
Human: 0.467
Perceptual: 3.938
MSE: 3309.259
PSNR: 12.933
SSIM: 0.559
Human: 1.000
Perceptual: 3.407
MSE: 1976.350
PSNR: 15.172
SSIM: 0.653
Human: 0.267
Perceptual: 3.757
MSE: 2577.335
PSNR: 14.019
SSIM: 0.554
Human： 0.533
Perceptual: 3.875
MSE: 934.756
PSNR: 18.424
SSIM: 0.620
Human: 0.267
Perceptual: 3.278
MSE: 3446.486
PSNR: 12.757
SSIM: 0.686
Human: 0.333
Perceptual: 3.274
MSE: 1269.629
PSNR: 17.094
SSIM: 0.634
Human: 0.400
Perceptual: 3.837
MSE: 4930.205
PSNR: 11.202
SSIM: 0.587
Human: 0.267
Perceptual: 2.772
MSE: 4736.691
PSNR: 11.376
SSIM: 0.589
Human: 0.133
Perceptual: 3.297
MSE: 2345.804
PSNR: 14.428
SSIM: 0.579
Human: 0.467
Perceptual: 4.153
MSE: 3245.777
PSNR: 13.018
SSIM: 0 601
Figure 81: LSUN-Cat 64x64 results for Conditional ProGAN.
87
Under review as a conference paper at ICLR 2021
^B・国网・丛高・
国金•心瓷』麓7惠署
Human: 0.733
Perceptual: 3.011
MSE: 2897.173
PSNR: 13.511
Human: 0.333
Perceptual: 3.068
MSE: 2413.944
PSNR: 14.304
Human: 0.667
Perceptual: 3.388
MSE: 3517.866
PSNR: 12.668
Human: 0.200
Perceptual: 3.359
MSE: 3428.060
PSNR: 12.780
Human: 0.600
Perceptual: 3.942
MSE: 2908.058
PSNR: 13.495
Human: 0.600
Perceptual: 3.913
MSE: 3070.654
PSNR: 13.258
Human： 0.800
Perceptual: 3.309
MSE: 1846.276
PSNR: 15.468
Human: 0.533
Perceptual: 4.316
MSE: 4624.318
PSNR: 11.480
Human: 0.667
Perceptual: 3.407
MSE: 2448.234
PSNR: 14.242
Human: 0.533
Perceptual: 2.942
MSE: 1227.054
PSNR: 17.242
Human: 0.467
Perceptual: 3.936
MSE: 2846.705
PSNR: 13.587
Human: 0.533
Perceptual: 3.529
MSE: 5287.009
PSNR: 10.899
Human: 0.800
Perceptual: 3.329
MSE: 1173.855
PSNR: 17.435
Human： 0.667
Perceptual: 3.808
MSE: 1371.260
PSNR: 16.760
Human: 0.867
Perceptual: 3.513
MSE: 1574.824
PSNR: 16.158
Human: 0.600
Perceptual: 3.224
MSE: 3016.540
PSNR: 13.336
Human: 0.600
Perceptual: 3.503
MSE: 3004.857
PSNR: 13.353
Human: 0.533
Perceptual: 2.243
MSE: 1436.297
PSNR: 16.558
Human: 0.867
Perceptual: 4.183
MSE: 2917.009
PSNRl 13.481
Human: 0.600
Perceptual: 3.793
MSE: 2678.404
PSNR: 13.852
BiKlkHl
Human: 0.867
Perceptual: 3.348
MSE: 1362.479
PSNR: 16.788
Human: 0.800
Perceptual: 3.992
MSE: 2750.395
PSNR: 13.737
SSIM: 0.601
Human: 0.133
Perceptual: 2.673
MSE: 1496.803
PSNR: 16.379
Human: 0.667
Perceptual: 4.089
MSE: 2945.564
PSNR: 13.439
Human: 0.800
Perceptual: 2.697
MSE: 1454.921
PSNR: 16.502

Human: 0.333
Perceptual: 3.789
MSE: 3896.799
PSNR: 12.224
Human: 0.000
Perceptual: 3.731
MSE: 3191.954
PSNR: 13.090

Figure 82: LSUN-Cat 64x64 results for PixelCNN++.
88
Under review as a conference paper at ICLR 2021
A.5.3 128x128
Figure 83: Ground truth for LSUN-Cat at 128x128 resolution.
89
Under review as a conference paper at ICLR 2021
τιιman: (i.ooι:
Figure 84: LSUN-Cat 128x128 results for ProGAN.
90
Under review as a conference paper at ICLR 2021
Figure 85: LSUN-Cat 128x128 results for Conditional StyleGAN.
91
Under review as a conference paper at ICLR 2021
Figure 86: LSUN-Cat 128x128 results for Conditional ProGAN.
92