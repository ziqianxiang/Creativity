Under review as a conference paper at ICLR 2021
Out-of-Distribution Generalization
with Maximal Invariant Predictor
Anonymous authors
Paper under double-blind review
Ab stract
Out-of-Distribution (OOD) generalization is a problem of seeking the predictor
function whose performance in the worst environment is optimal. This paper makes
both theoretical and algorithmic contributions to OOD problem. We consider a set
of all invariant features conditioned to which the target variable and the environ-
ment variable becomes independent, and theoretically prove that one can seek an
OOD optimal predictor by looking for the mutual-information maximizing feature
amongst the invariant features. We establish this result as Maximal Invariant Pre-
dictor condition. Our theoretical work is closely related to approaches like Invariant
Risk Minimization and Invariant Rationalization. We also derive from our theory
the Inter Gradient Alignment(IGA) algorithm that uses a parametrization trick to
conduct feature searching and predictor training at once. We develop an extension
of the Colored-MNIST that can more accurately represent the pathological OOD
situation than the original version, and demonstrate the superiority of IGA over
previous methods on both the original and the extended version of Colored-MNIST.
1	Introduction
In general, most machine learning algorithms today make an inherent assumption that all members of
all datasets in concern are independently and identically sampled from the same distribution (IID).
Unfortunately, this assumption is not always valid (Bengio et al., 2020). In reality, train-dataset
and test-dataset can be coming from different distributions, on which the input-output relations are
different because of the presence of environmental factors such as those related to the way the data
were collected and where the data were obtained (Shen et al., 2018; Storkey, 2009). This is why most
machine learning algorithms often fail when challenged to make a prediction on the dataset sampled
from “yet-unseen” distribution (“out of distribution (OOD)” dataset) (Arjovsky et al., 2019).
To address this problem, we need to consider the set of distributions that can be produced by
all possible environmental factors, and look for the model whose performance on the worst-case
environment is optimal; that is, we want to find a solution of
arg min max Cost(f |)
f ∈A
(1)
where A is the set of all possible values for the environmental factor, and Cost(f |) is the cost of
the model f in the presence of the environmental factor . The problem (1) is often referred to as
OOD generalization problem (Buhlmann, 2018; Arjovsky et al., 2019). We would say that f is
OOD-optimal if it solves (1). Unfortunately, as mentioned in Arjovsky et al. (2019), the problem (1)
often cannot be directly solved because we cannot observe datasets in all environments of A. This
problem is both interesting and difficult because not all distributions can be produced by the members
of A; indeed, if any distribution can be produced by some ∈ A, then max∈A Cost(f |) can be
equally maximal for any choice of f because the distribution that is most adversarial to f would also
be realizable by some in A.
Several studies in the past tackled this OOD problem from both theoretical and algorithmic direction,
with the aforementioned work of Arjovsky et al. (2019) being one of them. While different in
approach, studies like Arjovsky et al. (2019) and Peters et al. (2016); Buhlmann (2018); Subbaswamy
et al. (2019) all generally advocate that, in order to solve (1), one shall exploit some invariance that
is shared across the observed set of environments, such as causal relations or a feature that is equally
1
Under review as a conference paper at ICLR 2021
important in all environments. These approaches are different from distributional robustness-type
methods (Ben-Tal et al., 2013; Hu et al., 2018; Najafi et al., 2019) that aim to optimize the worst-
performance of the model over a compact set of environments embedded in some metric space. While
distributional robustness is a powerful family of methods, it may not necessarily be the best option
when We can,t foresee how far the new environment e would be from the training environments.
The invariance-based OOD studies mentioned in the previous paragraph are, however, case-specific
results in the sense that they prove the OOD optimality of their methods on situations in which the
relationships amongst the observable variables can be described by some particular graphical model
or a certain form of linear models. Therefore, the successes of these studies make us wonder the
following two fundamental questions in a more general setting : “When does the invariant feature
help us in solving OOD problem?” “What type of invariant feature is useful in OOD problem?” To the
best of our knowledge, there has not been a study that has succeeded in answering the first question
in the scope beyond a ‘case study’. There is still much room left for further investigation. As for
the second question, Ferenc (2019) recently investigated an information theoretic generalization of
Arjovsky et al. (2019), and suggested an objective
arg max	I(Y, Φ(X))	(2)
P (Y ∣Φ(X),e) = P (Y ∣Φ(X))∀e
where Y is the target variable, X is the input variable, is the environment factor and Φ is searched
over a general space of nonlinear measurable functions. This objective claims that one needs not only
some invariant feature, but an invariant feature that maximizes the mutual information with the target
variable. Chang et al. (2020) uses a similar approach in their particular graphical model setting.
In this study, we will discuss when we can use an invariant feature to solve OOD problem as well as
when we can solve (1) using (2). More particularly, we use the theory of basic probability and the
classic decomposition result used in Darmois (1953); Peters et al. (2012); Achille & Soatto (2017) to
investigate when we can use the invariance of form P(Y∣Φ, E) = P(Y∣Φ) to solve OOD problem.
We also name the solution of (2) as Maximal Invariant Predictor(MIP), and fill the gap between the
OOD objective (1) and the objective (2). To find a MIP, we derive Inter-Gradient-Alignment(IGA)
algorithm directly from (2), which uses a parametrization trick to conduct invariant-feature searching
and predictor training simultaneously. We demonstrate the efficacy of our algorithm on Colored
MNIST(C-MNIST, Arjovsky et al. (2019)) and Extended Colored-MNIST(EC-MNIST), a slight
generalization of the original that can be used to more accurately construct the pathological OOD
situation described in Arjovsky et al. (2019). We summarize our contributions below.
1.	We prove a sufficient condition required for an invariant Φ whose corresponding E[Y∣Φ(X)]
is OOD optimal, and justify the use of Maximal Invariant Predictor(MIP) for to solve OOD
problem.
2.	From MIP we derive Inter-Gradient-Alignment(IGA), a novel OOD algorithm.
3.	We present Extended Colored MNIST, a generalization of the original Colored MNIST
that can be used to represent an important pathological case that cannot be realized by the
original.
4.	We demonstrate the efficacy of IGA on Colored MNIST and Extended Colored MNIST.
The paper is structured as follows. We first describe the notations + problem setting in section 2. We
present our theoretical results in section 3, and describe IGA in section 4. Finally, we demonstrate
the efficacy of our algorithm on C-MNIST and EC-MNIST in section 5.
2	Notations and Problem Setting
In this section, we introduce the set of notations we use throughout the paper, and describe our
theoretical claims in more formal language. We use r.v to abbreviate random variable.
Notation We follow the rules of notations used in a standard probability text like Durrett (2019).
For any set of r.vs M = {Mι, M2,.., } on the probability triple (Ω, F, P), we say Z ∈ σ(M) or
Z is measurable with respect to σ(M) whenever Z can be written as a measurable function of
Ms. We may use Z and Z(M) interchangeably in this case1. We say A ⊥ B when A and B are 1
1For those not familiar with this notation, please identify σ(M) as {Z; Z = s(M) for some function s} in
the main part of this manuscript.
2
Under review as a conference paper at ICLR 2021
independent. Let us also use a lower case letter to denote a realization of the corresponding r.v (e.g.,
m is a realization of M, and is a realization of E). We always use E to represent the expectation
with respect to the probability distribution P. Inside E, variables in upper case are the only variables
that are integrated with respect to P. Also, for any probability distribution Q on F, we use its lower
case letter q to denote its density.
Formal Problem Statement Let X,Y ,E respectively represent the input r.v, the target r.v, and the
environmental r.v. At the time of the training, only X, Y are observable, and the observations are
grouped by the realizations of corresponding E . We will measure the performance of the predictor
f (X ) by some Bregman divergence loss D, which generalizes popular losses like KL divergence
and Mean Square Error (Banerjee et al., 2003). That is, for all ∈ supp(E), we compute the loss
on the environment by L(f) := E[D(f (X), Y )|]. OOD problem is to seek the minimizer of the
Out-of-Distribution (OOD) loss, given by
arg min max L(f),	(3)
f	∈supp(E)
We say that f is OOD optimal if it is a solution of (3). That being said, we also assume the following
for the underlying distributions.
Invariance assumption We assume that
I : = {Φ ∈ σ(X); Y ⊥ E∣Φ} = {Φ ∈ σ(X); I (Y; E∣Φ) = 0}	(4)
is non-empty. We refer to I as the set of invariant features from now on. In the graphical model
represented in Figure 1, for example, I is the set of all random variables that can be generated by
some subset of {X1, X3, X4} that contains X1. This fact can be verified with d-separation theorem
(Bishop, 2007). However, in our study, we do not develop our theory from an explicit graphical
models like the one we just explained. Unlike in Peters et al. (2016); Subbaswamy et al. (2019);
Chang et al. (2020), we also do not necessarily assume that some subset of X ’s coordinates can
represent something meaningful in the system (e.g., node representation in DAG). Instead, we study in
the setting in which the data is generated from some abstract distribution P(X, Y, E). Our invariance
assumption is not too far-fetched in this general setting as well. For example, consider the problem of
animal-image recognition. If Y, X , E are respectively the animal label, the image, and the person
who takes the picture, an animal’s biological feature captured in the image can be a member of I.
Indeed, such Φ is not unique all the time as well; for instance, face of animal and posture of animal
can both be in I.
Figure 1: A graphical model with nonempty I (4). The target variable is Y and the covariate is
X := [X1,X2,X3,X4].
3	Theory
3.1	Related Theoretical Works
In order to highlight the contributions of our work, we would like to mention several theoretical studies
of OOD optimal predictors that are related to our work. Arjovsky et al. (2019) is one of the pioneers
that proposed a construction of an OOD optimal solution from an invariant feature. In their work, they
theoretically investigated a specific case in which Φ is linear in X and g(Φ(X)) = E[Y∣Φ(X), e]
for an invariant Φ is also linear in Φ for all , and proved the condition under which their algorithm
can obtain an OOD optimal solution in their situation. Chang et al. (2020) also showed that, when
the underlining graphical model is of a specific form, they can find an OOD optimal solution by
seeking arg maxY ⊥E |MX I(Y; M X) amongst all binary mask M for which Y ⊥ E|M X.
Rojas-Carulla et al. (2018) claims that one can find an OOD optimal solution in the form E[Y|MX]
for a binary mask M. Rojas-Carulla et al. (2018), however, makes an unstated assumption that for any
e, there exists another environment e0 such that P(Y, X|e0) = P(M X, Y|e)P (Mc X|e) with
Mc being the complementary mask of M . While the cases studied in these works are all important
3
Under review as a conference paper at ICLR 2021
on their own, many of their assumptions do not hold in general. The proofs of the OOD optimality in
many of these works above also assume that the observed coordinate decomposition of X is aligned
in a such a special way that the invariant feature can be expressed using a subset of X’s coordinates
or a linear function of X .
In this paper, we aim to make a more general statement than these predecessors. First, we will
describe when we can use the invariant feature in I (4) to find a solution to (3). Second, we will
describe when we can use MIP (2) to solve (3), thereby giving support to the idea in Ferenc (2019). In
the rest of this section, we provide supports to these two claims using the theories of basic probability
and the result of Darmois (1953) which claims that, for any pair of random variables X, Y , we
can find a noise function NY ⊥ X such that Y = f(X, NY ) when the cumulative distribution is
sufficiently regular.
3.2	Maximal Invariant Predictor
In this section we provide brief definitions and theoretical results. For their details, please see
Appendix B. Let us assume that the invariant set I in (4) is non-empty, and suppose Φ ∈ I. Then
applying the aforementioned result of Darmois (1953) to the r.v pair (Φ, E), we can say that there
exists Eψ ⊥ Φ such that σ(Φ, E) = σ(Φ, Eψ). Applying the same argument again to (Eψ , E), we can
say that there is Eφ ⊥ Eψ such that σ(E, Eψ) = σ(Eφ, Eψ). For any Φ ∈ I, we can thus decompose
into the pair (φ, ψ). In summary, we have the following;
•	Eψ ∈ σ(E), a part of E that is independent of Φ
•	Eφ = Eψc ; that is, Eφ ⊥ Eψ and E ∈ σ(Eφ , Eψ).
Put in still other words, this is a decomposition of E into the component that can affect Φ and
the component that cannot. Also, let us apply Darmois (1953) to (X, Φ) to obtain Ψ ⊥ Φ with
X ∈ σ(Φ, Ψ).We emphasize that we are assuming a setup that is at least as general as the works of
(Darmois, 1953; Peters et al., 2012; Achille & Soatto, 2017) that use this decomposition result. For a
more intuitive scenario, consider another image recognition problem of predicting the animal label Y
from the image X in the set of natural images collected with a loose directive of “take any pictures
of a given list of animals”. Suppose that Φ is the physical appearance of the animal. Then the part
of the environmental factor that is able to influence Φ can be the biological state of the animal (Eφ)
(e.g., physical state, genetic feature). The part of the environmental factor that can not influence Φ
can be, for example, the preference of the photographer (Eψ). Clearly, Eψ ⊥ Φ. Meanwhile, Ψ, the
complement of Φ, can be the background of the animal. Since the photographer does not have much
choice for the individual animal to encounter, Ψ is independent of Φ if he/she is not so picky.
The following is our first claim about the case in which an invariant feature can be used to find an
OOD optimal solution.
Proposition 3.1. Let Φ ∈ I. Also, put f *(X) = E[Y∣Φ] and suppose that, for all e© there exists
some eψ such that2 X ⊥ Y∣(Φ, e©, eψ). Then f * = arg minf maxe∈supp(E)Le(f).
For the proof, see Appendix B. Because the proposition 3.1 is pivotal in our theory, we would like to
give a name to the condition used therein;
Definition 3.2. We say Φ ∈ I satisfies a controllability condition iffor all e©, there exists at least
one corresponding eψ such that X ⊥ Y|e©, eψ.
In other words, this is a condition in which there is always a way to twig a Φ-irrelevant part of
the environment to sever the relation between X and Y using Φ and the environment. From the
proposition 3.1, we can also say the following.
Corollary 3.3. Ifthere exists one eψ for which Ψ ⊥ Y∣eψ, then E[Y∣Φ] is OOD optimal.
Figure 8 in Appendix B is an example of a system in which the corollary 3.3 can hold. The corollary
3.3 may be helpful in determining when the OOD optimality statement in Rojas-Carulla et al. (2018)
is valid. In their proof, Rojas-Carulla et al. (2018) claims their solution to be OOD-optimal under an
2As we describe in Appendix B, we can increase the number cases in which this holds if we choose Eψ to be
larger in the sense of sigma algebra.
4
Under review as a conference paper at ICLR 2021
unstated assumption that, for any e, there exists some e0 such that P (Y, X ∣e0) = P (Φ, Y |e)P (Ψ∣e).
We can guarantee the validity of this assumption if Ψ ⊥ E and if Φ satisfies the condition of the
corollary 3.3. It also turns out that, if and Φ can sever the relation between Y and X, then this Φ is
optimal in the environment .
Proposition 3.4. Suppose Φ ∈ I and suppose that E satisfies X ⊥ Y∣Φ,匕 Thenfor this particular G
Φ = arg maxZ∈σ(X) I(Y; Z|).
In other words, the controllability condition for a feature Φ also guarantees that we can twig the
Φ-irrelevant part of E to produce E in which Φ is the best feature in the environment E. The situation
considered here is not too unrealistic. In the case of the animal-recognition example we mentioned
above, we can choose EEψ to be a person who wants to take a picture of every animal in front of an
all-green background for the Chroma-key purpose. With such EE, the biological feature of an animal
would always be the best feature.
At this point yet, we still have not made an explicit connection between (1) and (2). The following
lemma would help us to fill the gap.
∙-v
Lemma 3.5. Suppose Φ ∈ I and suppose that for some e* , Φ ⊥ Y |e* Then there exists no Φ ∈ I
∙-v	∙-v
with Φ ∈ σ(Φ) such that I(Y; Φ) < I(Y; Φ).
The lemma 3.5 states that, unless Φ is a maximal element of I, there is no hope in finding an
environment in which E[Y∣Φ] is OOD optimal. This motivates to find Φ ∈ I satisfying
Φ* ∈ arg max I (Y; Φ) = arg max I (Y; Φ).	(5)
Φ∈I	I (Y [E∣Φ)=0
The equation (5) is indeed the very Maximal Invariant Predictor(MIP) condition mentioned in
the introduction. When we restrict the search space I to its linear subset {Φ ∈ I; Φ = M
X for some binary mask M }, we retrieve the objective in Chang et al. (2020). We can justify this
objective more formally when appropriate conditions are met. Note that, in cases like Figure 1, all
invariant features are generated by a common invariant feature vector [X1, X3, X4]. This type of
case may also arise when the data is generated by an undirected graphical model that is a perfect map
(see Appedix C). In these cases, MIP is unique and the proposition 3.5 would imply that Φ cannot
be OOD optimal unless it achieves maxΦ∈I I(Y; Φ). If we use these facts, we can also claim the
following:
Theorem 3.6. Suppose that I can be generated by one invariant subset of r.vs, and that there exists
at least one Φ ∈ I that satisfies the controllability condition. Then E[Y∣Φ*] is OOD optimal if Φ* is
MIP.
4	Inter-environmental Gradient Alignment Algorithm
To seek a solution to the MIP objective (5), we propose Inter Gradient Alignment(IGA) algorithm,
which optimizes the following objective;
arg min E[Le(θ)] + λ trace(Var(VθLE(θ))).	(6)
θ
where L is the loss of the θ-parameterized prediction model computed on the environment E. We
are using the upper case E above to indicate that E is the random variable with respect to which the
expectation and the variance are taken. We call this Inter Gradient Alignment algorithm, because it
encourages the gradient computed at each environment to align with those of other environments.
Note that (6) does not require us to separately compute the invariant feature Φ. Table 2 is a summary of
ʌ
our algorithm. We use E to denote the empirical expectation. Let Gtrain = {E1, E2, ...Ek} ⊂ supp(E)
denote a size= k set of environments from which the user can collect the dataset. IGA assumes
that the user is given a set of datasets {D; E ∈ Gtrain}, with each D being a set of samples from
p(y, x|E). Because we assume that neither the identities nor the effects of E are disclosed to the user
in any way whatsoever, each E is only observable as an integer index of datasets.
4.1	Derivation of IGA
We derive (6) from the MIP objective (5). To evaluate I (Y ;Φ) and I (Y; E∣Φ), we need to be able to
evaluate both p(y∣φ, E) andp(y∣φ) for arbitrary realizations y, φ and e. However, the complexity of
5
Under review as a conference paper at ICLR 2021
Algorithm 11nter-environmental Gradient Alignment Algorithm
Input： qθ(y∣x),{D/ e ∈ Gtrain}
Return: qθ
1:	for each iteration do
2： for €i in Gtrain do
ʌ
3：	compute Lei (θ) = E[log qθ(Y|X)|司
4：	compute ^θL6i (θ)
5： end for
6:	COmpUte E[LE] := ∣Gtrain∣ Eei∈Gtra,n * Lei ⑻
7:	COmpUte trace(VM JLE (J))) := Pei∈Gtrain ULei ⑹-VθE[LE] ||2
8： update θ by the gradient descent using the eq (6)
9： end for
Qθt(y∖x,e1) &
Qθt(m ʤ)
τraje Trajectory of θ during the training
▽e&i, the gradient of the loss for task ει
▽e&2 , the gradient of the loss for task ε2
Ve-Ce3 , the gradient of the loss for task ε3
Vβ⅛[^ε], the gradient over all tasks
Qθτ(y∖χ)

Figure 2: IGA algorithm
Figure 3: The relation of the θ-
updates in IGA to the gradient com-
puted at each task.
p(y∣φ, e) is usually unknown. The relationship between p(y∣φ, e) is p(y∣φ) is unknown as well, and it
depends on the choice of h : X → Φ. We therefore use a specific parametrization to represent these
distributions. Note that because Φ ∈ σ(X) (so that Φ can be written as a function of X), p(y∣φ) can
be written as q(y|x) for some h-related distribution q. Representing q with a θ-parametrized family
of functions f (∙∣θ),we may write p(y∣φ, e) and p(y∣φ) as
p(y∣φ) = qθ (y|x) = f (y∣χ; θ - αVθ e[Le (θ)])	(7)
p(y∣φ,e) = qθ(y∣x,e) = f(y∣x; θ -。口Le(θ)).	(8)
The advantage of this parametrization is multi-fold. First, when we evaluate these approximations
empirically, the e in our approximations only appears as an index in the empirical expectation. We
do not have to use the real identity of e in the evaluation of the approximation. Second, if the
model is regular enough, we can expect E[qθ(y|x, E)] = qθ(y|x) for small enough a. Finally, by its
design, qθ(y|x, e) is closer to p(y∣x, e) than qθ(y|x) is at all time, which is required by the property
of Kullback Leibler Divergence. See Appendix C.1 for more discussion about our parametrization.
Now, I(Y; Φ) in (6) can be evaluated with E[Le(θ)]. The regularization term I(Y; E∣Φ) can be
approximated as
I (Y; E∣Φ) = E[dκL(qθ (Y |X, E )kqθ (Y |X))]
=E[Le(Θ — αVθE[Le(θ)]) — Le(θ — αVθLe(θ))] = α trace(Var(VθLe(θ))).	(9)
Thus, qθ(y|x, e) is close to q§(y|x) if a is small. Figure 3 is an illustration each VLe relative to the
actual direction of the update. For more detailed computation, see Appendix C.2.
4.2 Two phase learning vs one phase learning
Previous deep learning OOD algorithms like Arjovsky et al. (2019) and Chang et al. (2020) aim to
learn an OOD optimal predictor in two phases: (i) the phase of learning a invariant feature Φ ∈ σ(X)
via a function h : X → Φ and (ii) the phase of learning a predictor g : Φ → Y. When the loss is of
Bregman divergence type, the optimal g* for any given h is of the form E [Y ∣Φ] or P (Y ∣Φ), and g*
itself depends on the choice of h; we shall therefore write gh for g*. Because gh and h are dependent
on one another, allowing a large model space for either g* or h would make the training difficult.
The algorithm of Arjovsky et al. (2019) took the approach of using a small model space for gh* and
large model space for h, and assumed gh* to be always linear in black box Φ. Meanwhile, because
the complexity of gh* may vary with h, it may not be possible to find gh* if the model space is too
small. Chang et al. (2020) took an approach of assuming a possibly large model space for gh* , and
instead sought h from those that can be expressed as M X with a binary mask random variable
M . However, if the model space of gh* is large, optimizing gh* with respect to function h can be a
daunting task. In fact, Chang et al. (2020) is giving up the computation of the gradient of gh* with
respect to the parameter of h. As we discuss in Appendix E.2, it was empirically difficult to train
these inter-related functions in two separate phases when the model space was large. IGA is different
from previous approaches in that it implicitly trains g and h in one phase.
6
Under review as a conference paper at ICLR 2021
5	Experiment
We evaluated the efficacy of our method on Colored-MNIST and Extended Colored-MNIST. For more
detail of the setting and the implementation, please see Appendix D.
5.1	Experimental setup
Extended Colored-MNIST (EC-MNIST) Because our extension generalizes the original Colored-
MNIST (C-MNIST) in Arjovsky et al. (2019), we describe our Extended Colored-MNIST (EC-MNIST)
first. Unlike the original C-MNIST, the invariant predictor set I (4) in our extended version contains
more than one variable. Our extended version is particularly different from the original one in that it
can describe a case in which the domain invariant feature (i.e, features Φ that is independent to E)
alone is not necessarily sufficient for the optimal environment-agnostic prediction. Each instance of
our EC-MNIST dataset is constructed as follows;
1.	Set xch2 to 1 with probability ch2 . Set it to 0 with probability 1 - ch2 .
2.	Generate a binary label yobs from y with the following rule: yobs = 0 if y ∈ {0 〜4} and
y°bs = 1 otherwise. If Xch2 = k, construct yobs by flipping y°bs with probability pk(k ∈ {0,1}).
3.	Put yobs = Xcho, and construct Xcho from Xcho by flipping Xcho with probability 金九。.
4.	Construct xobs as xfig × [xch0, (1 - xch0), xch2]. As an RGB image, this will come out as an
image in which the red scale is turned on and the green scale is turned off if xch0 = 1, and
other-way around if xch0 = 0. Blue scale is turned on only if xch2 = 1.
Figure 5 is the graphical model for the generation of EC-MNIST. In the experiment on this dataset,
only (Yobs, Xobs) are assumed observable. At training times, the machine learner will be given a set
of datasets Dtrain = {D; ∈ Rtrain} in which D is a set of observations gathered when E = . In
the test time, the learner will be challenged to make an inference of Yobs from Xobs in the presence
of an unknown environment e*. If e* is far away from any members of Rtrain, a model overfitted to
Dtrain Can fail catastrophically on e*.
For our EC-MNIST, Yobs can be predicted at maximal probability of ch2 max{p0, 1 - p0} + (1 -
ech2) max{p1, 1 - p1}. In this problem, Xfig is a E independent factor. At the same time, Xch2
is a member of I, and Xfig together with Xch2 can create a better predictor than Xfig alone. In
fact, the oracle prediction by Xfig alone can attain an average value of at most max{ch2p0 + (1 -
ch2)p1, ch2(1 -p0) + (1 - ch2)(1 - p1)}, which is lower than the that of the [Xfig, Xch2] oracle.
This follows from Fatou’s lemma (Folland, 2013). In EC-MNIST, Xfig is the maximal random
variable that is independent of environmental factors. The oracle with Xfig alone coincides with the
upper bound of Adversarial Domain Adaptation(ADA) (Li et al., 2018; Zhang et al., 2018) which
looks for a feature Φ with which it is difficult to identify which environment the X is coming from.
Because ADA essentially looks for a feature Φ that is independent to E , ADA cannot provide an
optimal solution in this case. IRM (Arjovsky et al., 2019) also discusses such a case in their work.
Colored MNIST (C-MNIST): The original C-MNIST in Arjovsky et al. (2019) is a special case
of our EC-MNIST in which the distribution of xch2 does not vary with . Figure 4 is the graphical
model of C-MNIST. Notice that if one uses Xfig alone, one can make the predictions at the optimal
accuracy of max(1 - p, p). Thus, ADA may attain the optimal solution in C-MNIST. In this sense,
our EC-MNIST is more suited for the investigation of pathological cases in OOD study.
5.2	Result
We compared our algorithm against Invariant Risk Minimization (IRM)(Arjovsky et al., 2019),
Empirical Risk Minimization (ERM), and the aforementioned oracle(s) in terms of OOD accuracy,
which is the minimum accuracy over all environments. For the comparison against other two-phase
training method akin to Chang et al. (2020), see Appendix E.2.
Colored-MNIST The left column in table 6 compares the results of the algorithms in terms of the
OOD accuracy (3). As we see in the table, our method outperforms both ERM and IRM. Figure
7(a)(b) plots the OOD accuracy against the regularization parameter. In general, larger regularization
parameter promotes the performance. The OOD accuracy plateaus around λ ~ 104.
7
Under review as a conference paper at ICLR 2021
Figure 4: The graphical
model of C-MNIST
Figure 5: The graphical
model of EC-MNIST
Model	OOD accuracy	
	C-MNIST	EC-MNIST
Oracle	0.75	0.75
Xfig	0.75	0.25
ERM	0.172±.029	0.176±.029
IRM	0.592±.011	0.430±.080
Ours	0.620±.015	0.594±.013
IRMf	0.593±.044	
(a) IRM on C-MNIST
(b) Ours on C-MNIST
Figure 7: The plot of regularization parameter λ against the accuracies on C-MNIST (p = 0.25) and
on EC-MNIST (p0 = 0.25, p1 = 0.75).
Figure 6: Numerical performance of OOD
algorithms. Xfig designates the figure-only
oracle (i.e. the upper bound of ADA). f is the
OOD accuracy of IRM reported in Ahuja et al.
(2020). For the result of IRM in Arjovsky
et al. (2019), please see Appendix E.1.
(c) IRM on EC-MNIST
(d) Ours on EC-MNIST
Extended Colored-MNIST The right column in the table 6 compares the results of the algorithms
in terms of the OOD performance (3). Again in this set of experiments, we perform better than
ERM and IRM. We also perform better than the Xfig oracle. Because Xch2 is necessary in order to
outperform the Xfig oracle (see section 5.1), our result suggests that we are actually using the feature
Xch2 in making the prediction. Figure 7(c)(d) plots the OOD accuracy against the regularization
parameter. Again, a larger regularization parameter generally promotes the OOD performance.
6	Other Related Works
As mentioned in the introduction, many studies in the past advocated the importance of invariance in
OOD problem, and they differ in the form of invariance they propose. Because the feature that is
easy to learn differs with the chosen model-architecture and the environment, some human-imposed
measure often seems necessary to encourage the learner to focus on the environment-agnostic relations
(Geirhos et al., 2020). The aforementioned ADA (Li et al., 2018; Zhang et al., 2018) is practically a
method to look for good -independent feature. We may say that such a method is looking for a good
feature that is invariant of the choice of domain, or equivalently the choice of to which the input
X is conditioned to. Covariate shift (Ben-David et al., 2007; Johansson et al., 2019; Shimodaira,
2000) works in the setting that P(Y|X, e) is invariant with respect to e. As discussed in Buhlmann
(2018), DAG causal relation is another powerful representation of invariance with respect to the
environment, and much research has been done to better utilize the causal relations for OOD problem
(Peters et al., 2016; Subbaswamy et al., 2019; Rojas-Carulla et al., 2018; Buhlmann, 2018; Chang
et al., 2020). Most of these works look for a part of causal relations that remain invariant with respect
to environments. At the same time, identifying causal relations are difficult in practice (Buhlmann,
2018), and these theories are not always readily applicable. We may say that recent studies of IRM
and its variants (Arjovsky et al., 2019; Ahuja et al., 2020) are, in part, and an effort to investigate
the invariance of form E[Y∣Φ,e] = E[Y∣Φ] where Φ is a black-box non-linear feature. Chang
et al. (2020) seeks an invariant feature of form P (Y |M X,e) = P(Y|M X) with M being a
binary mask variable. Our study is an approach that claims the usefulness of the invariance of form
P(Y∣Φ, e) = P(Y∣Φ) with black-box Φ, and we used an information-theoretic mean tojustify the
usage of such invariant feature in OOD problem.
8
Under review as a conference paper at ICLR 2021
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep
representations. arXiv preprint arXiv:1706.01350, 2017.
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk
minimization games. 2020.
Martin Arjovsky, Leon Bottou,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Arindam Banerjee, Xin Guo, and Hui Wang. On the optimality of conditional expectation as a
bregman predictor, 2003.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In Advances in neural information processing Systems, pp. 137-144, 2007.
A. Ben-Tal, D. den Hertog, A.M.B. De Waegenaere, B. Melenberg, and G. Rennen. Robust solutions
of optimization problems affected by uncertain probabilities. Management Science, 59(2):341-357,
2013. ISSN 0025-1909. Appeared earlier as CentER Discussion Paper 2011-061.
Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Nan Rosemary Ke, Sebastien Lachapelle, Olexa
Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle
causal mechanisms. In International Conference on Learning Representations, 2020.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and
Statistics). 2007.
Peter Buhlmann. Invariance, causality and robustness. arXiv preprint arXiv:1812.08233, 2018.
Enrique Castillo, Jose M Gutierrez, and Ali S Hadi. Expert systems and probabilistic network models.
Springer Science & Business Media, 2012.
Shiyu Chang, Yang Zhang, Mo Yu, and Tommi S Jaakkola. Invariant rationalization. arXiv preprint
arXiv:2003.09772, 2020.
Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
G. Darmois. Analyse generale des liaisons stochastiques: etude PartiCuIiere de l,analyse factorielle
line´ aire. Revue de l’Institut International de Statistique / Review of the International Statistical
Institute, 21(1/2):2-8, 1953. ISSN 03731138.
Rick Durrett. Probability: Theory and Examples. Thomson, 2019.
Huszar Ferenc. Invariant risk minimization: An information theoretic view. 2019. URL https：
//www.inference.vc/invariant-risk-minimization/.
Gerald B Folland. Real analysis: modern techniques and their applications. John Wiley & Sons,
2013.
Maxime Gasse. Probabilistic Graphical Model Structure Learning: Application to Multi-Label
Classification. PhD thesis, 2017.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias
improves accuracy and robustness. In International Conference on Learning Representations,
2019.
Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. arXiv preprint
arXiv:2004.07780, 2020.
9
Under review as a conference paper at ICLR 2021
Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised
learning give robust classifiers? In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research,pp. 2029-2037, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bUgs, they are featUres. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d AlChe-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information
Processing Systems 32, pp. 125-136. CUrran Associates, Inc., 2019.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Fredrik D. Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-
invariant representations. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), Proceedings
of Machine Learning Research, volume 89 of Proceedings of Machine Learning Research, pp.
527-536. PMLR, 16-18 Apr 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adver-
sarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5400-5409, 2018.
Peter Lucas, JoSe A Gamez, and Antonio Salmeron Cerdan. Advances in Probabilistic Graphical
Models, volume 213. Springer, 2007.
Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial
perturbations in learning from incomplete data. In Advances in Neural Information Processing
Systems 32, pp. 5542-5552. Curran Associates, Inc., 2019.
Judea Pearl et al. Causal inference in statistics: An overview. Statistics surveys, 3:96-146, 2009.
J. Peters, P. BUhlmann, and N. Meinshausen. Causal inference using invariant prediction: identifica-
tion and confidence intervals. Journal of the Royal Statistical Society, Series B (with discussion),
78(5):947-1012, 2016.
Jonas Peters, Joris Mooij, Dominik Janzing, and Bernhard Scholkopf. Identifiability of causal graphs
using functional models. arXiv preprint arXiv:1202.3757, 2012.
Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant models for
causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.
Soroosh Shafieezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distribu-
tionally robust logistic regression. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 1576-1584. Curran
Associates, Inc., 2015.
Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, and Peixuan Chen. Causally regularized learning
with agnostic data selection bias. In Proceedings of the 26th ACM international conference on
Multimedia, pp. 411-419, 2018.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of Statistical Planning and Inference, 90(2):227-244, October 2000.
Amos Storkey. When training and test sets are different: characterizing learning transfer. Dataset
shift in machine learning, pp. 3-28, 2009.
Adarsh Subbaswamy, Peter Schulam, and Suchi Saria. Preventing failures due to dataset shift:
Learning predictive models that transport. In Kamalika Chaudhuri and Masashi Sugiyama (eds.),
Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning
Research, pp. 3118-3127. PMLR, 16-18 Apr 2019.
Yexun Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. Domain-invariant adversarial learning for
unsupervised domain adaption. arXiv preprint arXiv:1811.12751, 2018.
10
Under review as a conference paper at ICLR 2021
Appendix
A	Why invariant features ?
We give more explanation about our motivation in solving the OOD problem. As mentioned in the
introduction, OOD problem arises when one has to deal with sets of datasets coming from possibly
different distributions. For example, when we are to conduct image-recognition from a dataset of
animal pictures collected by a group of photographers, we might have to consider the fact that some
photographer might favor a picture of a given animal in a certain posture at a certain set of locations at
a certain time of the day, and some do not. If camel happens to be always found in desert in the set of
pictures taken by Jerry the photographer, an animal classifier trained on his dataset may characterize
camel as a horse-like figure in desert. Meanwhile, it might be the case that Tom the photographer
prefers to take a picture of any animal at a zoo. Indeed, the animal classifier trained on Jerry’s dataset
will most likely fail to recognize Tom’s picture of camel as camel, because a picture of a camel in a
cage does not agree with the description of the camel learned by the classifier.
Such behavior shall not be considered as a bug, because a horse-like figure in desert is definitely
a characterizing description of camel on the distribution that underlies the set of natural images
taken by Jerry. This type of problem can arise in a more subtle way. Some studies report that in
practice, the machine learner may associate the label with even more subtle features like a texture or
a pixel-pattern that cannot be discerned by humans (Ilyas et al., 2019; Geirhos et al., 2019).
Then how shall we formalize what is “reasonable” and what is not? If the system of concern is
governed by a certain causal relation, the exterior factor can only affect the input-output distribution
in a certain way, and some parts of the relations in the system will remain unchanged (Pearl et al.,
2009; BUhlmann, 2018). We can borrow from this philosophy; We can say that the exterior factor is
affecting the distribution in a “reasonable” manner if there is always some part of the input-output
relation that remains invariant under its influence. We can then work with the assumption that some
part of the input-output relation is shared in common by all distributions to be considered. For the
aforementioned camel example, camels’essential biological feature in the picture is not affected by
the person who takes the picture. This is an example of the approach based on invariance. By finding
the part of the system that stays invariant under the effect of environmental factor, one may be able to
find a solution to the OOD problem. Our study explains when this is actually possible.
We do not necessarily say that this is the only way to go in all situations. For example, if environmental
factor and its effects are known in advance, we may take a completely different strategy. In the animal
picture recognition problem we mentioned above, if we know from the beginning that ”photographer”
is the environmental factor and that they affect the ”background” part of the image, one may be able
to improve the generalization ability of the trained model by allowing it to leverage the similarity
relations among the photographers. In such cases, methods of distributional robustness might be
particularly helpful (Ben-Tal et al., 2013; Hu et al., 2018; Najafi et al., 2019; Shafieezadeh Abadeh
et al., 2015).
But in many cases, figuring out what is affecting the dataset is a difficult problem on its own, and it is
often not known how similar the new photographer is to the set of photographers we have seen. This
is the motivation behind the invariance-based approach to the OOD problem.
B Proof of the Invariance Results
In this section we provide the proofs for the theoretical results we presented in the section 3 of the main
manuscript. Let E, X, Y be random variables defined with probability triple (Ω, F, P). Throughout,
we will follow the notation rules we introduced in the main part of the manuscript. Following
the convention in probability, we use σ(Z) to denote a sigma algebra generated by X, and write
W ∈ σ(Z) whenever W is measurable with respect to Z. We will generally use upper case letters to
denote random variables, and use lower case letters to denote the corresponding realizations. Unless
otherwise noted, we will use E to represent the expectation with respect to P . We will also use ⊥ to
represent independency relation, and use A ⊥ B|C to convey that P(A, B|C) = P(A|C)P(B|C).
11
Under review as a conference paper at ICLR 2021
Now, let Φ be a random variable that is mesurable with respect to σ(X), and let us define the
following elements that can be derived from Φ:
1.	Fψ = {E ∈ σ(E); E ⊥ Φ}) is a set of random variables representable as a function of E
that are independent from variable Φ
2.	Eψ ∈ Fψ. If it exists, we define this to be a random variable that is maximal in the sense
that there is no other Z ∈ Fψ with σ(Eψ) ( σ(Z).
3.	Eφ = Eψ； that is, Eφ ⊥ Eψ and σ(Eφ, Eφ) = σ(E).
As for the existence of the last decomposition, we appeal to the result of Darmois (1953), which
claims that , for any joint distribution of (X, Y ) we can find a function f and a noise NY ⊥ X, such
that Y = f(X, NY ). Therefore, for every ∈ supp(E), let us WLOG suppose a map r for which
r(φ, ψ) = , and use (φ, ψ) and interchangeably. Let us also define the invariant setI by
I = {Φ ∈ σ(X); (Y∣Φ, E) = (Y∣Φ), Y ⊥ Φ}	(10)
and that this set is non-empty. Then the following lemma will be useful for the main result 3.1, which
provides a set of conditions under which the Φ ∈ I satisfies the OOD optimality:
Lemma B.1. If Φ ∈ I, then Eψ ⊥ (Φ, Y).
Proof. By definition, Y ⊥ E∣Φ so in particular, Y ⊥ Eψ ∣Φ and Y ⊥ Eφ∣Φ. Moreover, by definition,
Eψ ⊥ Φ. Thus P(Eψ |Y, Φ) = P(Eψ ∣Φ) = P(Eψ) and Eψ ⊥ (Φ, Y), as desired.	口
We are now ready to present the proposition 3.1. In an equation that uses multiple E notation, will
use a notation like EZ to help seeing that the inside the symbol is an integration about Z .
Proposition B.2. Let g be a strictly convex, differentiable function and let D be the corresponding
Bregman Loss function. Let Φ ∈ σ(I), and let wφ be the measurable function such that Φ = wφ(X).
Also, put f *(X) = E[Y∣Φ] = g*(wφ(X)) and suppose that,forall e@ there exists eψ such that
X ⊥ Yl(Φ,M)	(11)
Then
f * = argmin sup E[D(f(X), Y )|e]	(12)
f	∈supp(E)
Proof. We are going to leverage the fact that, if G is a sub sigma algebra of F to which Y is
measurable, then Banerjee et al. (2003)
arg min E[D(Z, Y)] = E[Y|G].	(13)
Z∈G
We are also going to use the fact that the Bregman loss function
D(a, b) = g(a) - g(b) -〈a - b Vg(b)i
is convex about its first coordinate, a. Now, in order to show the claim, we need to show that
sup E[D(f (X), Y)|] ≥ sup E[D(f* (X), Y)|] for all measurable f. To show this, for any fixed
we need to be able to find one 0 such that
E[D(f(X),Y)H] ≥ E[(D(f*(X),Y)4	(14)
Suppose a fixed and suppose that = r(φ, ψ) for the appropriate invertible map r, and let 0 be
such that e0 = r(tφ, eψ) with eψ that satisfies the condition in the claim. Now, the Bregman Loss
function D(x, y) is convex with respect to x. With Jensen’s inequality and repeated application of
Tower rule (Durrett, 2019),
EX,Y [D(f(X), Y)|0] = EΦ,Y [EX[D(f(X), Y)|0, Φ, Y]]	(15)
≥ Eφ,y[D(Eχ[f (X)∣Φ, Y, J], Y)K]]	(16)
=Eφ,y[(D(Ex [f (X )∣Φ,e'],Y )k0]]	(17)
= EΦ,Y [D(h(Φ, 0), Y)|0]	(18)
12
Under review as a conference paper at ICLR 2021
where We set h(Φ, e0) = E[f (X) ∣Φ, e0] and used Jensen's inequality in the second line. Here, the
random variable that is integrated in the last expression is Φ and Y only, and e$, eψ, <≡ψ are all
constant. Moreover, by the lemma B.1, (Y, Φ)kφ, eψ = (Y, Φ)kφ, eψ. Therefore, using the property
of eψ,
Eφ,γ[D(h(Φ, C, Y)kφ, eψ] = Eφ,γ[D(h(Φ, e@, eψ), Y)|2 eψ]	(19)
=Eφ,y[D(h(Φ, S eψ), Y)∣s cψ]	(20)
≥ Eφ,yD(EY[Y∣Φ, S eψ], Y)∣eφ, eψ]	(21)
=Eφ,y [D(E[Y ∣Φ],Y )kφj ]	(22)
=Eφ,y [D(f *(X ),Y )kφ,cψ ]	(23)
Above, the inequality in the third line follows just from the optimality of the conditional expecation
about the Bregman divergence, and the equality in the fourth line follows from the fact that Φ ∈ I.
All together we have
E[D(f(X),Y)旬 ≥ [(D(f*(X),Y)同.	(24)
□
Indeed, the condition in the claim 3.1 does not hold forjust any arbitrary Φ ∈ I. In order for g*(Φ)
to be optimal in the sense of 3.1, Φ has to be special in some sense.
Proposition B.3. Suppose Φ ∈ I and suppose that E satisfies X ⊥ Y ∣Φ,匕 Thenfor this particular
,
Φ = arg maxI(Y; Z |E).
Z∈σ(X)
Proof. Suppose that there exists B ∈ σ(X) with I (Y; B, Φ∣e) > I (Y; Φ∣e). Notice then that
I (Y ； B, Φ∣e)= I (Y ； Φ∣e) + I (Y ； B∣Φ, E)	(25)
and it follows that I (Y; B∣Φ,e) > 0. Since I (Y; X ∣Φ,e) ≥ I (Y; B∣Φ, e) this implies that Y ⊥
X∣Φ, E and in particular, E does not satisfy X ⊥ Y∣Φ, e. Thus, in order for X ⊥ Y∣Φ, E to hold for e,
such B cannot exist. In other words, Φ must be optimal among all Z ∈ σ(X) on any environment
satisfying (3).	□
The necessary condition for (3) can be also more succinctly stated as follows.
Proposition B.4. Suppose Φ ∈ I and suppose that for some e* , Φ ⊥ Y |e* Then there exists no
∙-v	∙-v	∙-v
Φ ∈ I with Φ ∈ σ(Φ) such that I(Y; Φ) < I(Y; Φ).
∙-v
Proof. Let Φ be as stated in the assumption. Then since g(X) ⊥ Y∣Φ, e*, it particularly follows that
∙-v
Φ(X) ⊥ Y|e*. Thus we have
∙-v	∙-v
P (Y ∣Φ) = P (Y ∣Φ ,e*)	(26)
∙-v
=P (Y ∣Φ, Φ,e*)	(27)
=P (Y ∣Φ,e*)	(28)
=P (Y ∣Φ)	(29)
Where the first and the last equalities follow from the invariance property, the second equality
follows from the tower rule of conditional expectation, and the third equality follows from the
∙-v
assumption about E*. Note that both Φ and Φ are functions about X. Also, note that the mutual
∙-v
information depends only on conditional distribution. It follows that I(Y; Φ) = I(Y; Φ), and the
claim follows.	□
Theorem B.5. Suppose that there exists at least one Φ for which there is a corresponding Eψ for
every Eφ such that X ⊥ Y∣Φ, Eφ, Eψ. Suppose also that I is generated by one Φo. In this case
E[Y∣Φ*] is OOD optimal if
Φ* = arg max I(Y; Φ).	(30)
Φ∈I
13
Under review as a conference paper at ICLR 2021
∙-v
Proof. If Φ ∈ I is such that for every φ there is a corresponding ψ , Φ is automatically OOD
optimal, and it needs to be maximal by the proposition B.4. Now, if I is generated by one Φ0,
then Φo is the unique maximal variable in the sense that it is equivalent to Φ* UP to its sigma
∙-v	∙-v	∙-v
field. Because Φ ∈ σ(Φ0), σ(Φ0) = σ(Φ) necessarily by the maximality of Φ as well. We
∙-v
thus have σ(Φ0) = σ(Φ) = σ(Φ*). Thus, for Φo there exists eψ satisfying 3 for every e$, and
E[Y∣Φo] = E[Y∣Φ*] is OOD optimal.	□
As one explicit examPle, that I is generated by one variable can haPPen when the underlying
distribution P is UG-faithful, that is, P is faithful to the independence relations for which there exists
some undirected graph G that induces a perfect map (Gasse, 2017; Lucas et al., 2007; Castillo et al.,
2012). More succinctly stated, we have the following corollary.
Corollary B.6. Suppose that there exists at least one Φ for which there is a corresponding ψ for
every e@ such that X ⊥ Y∣Φ, e0, eψ, and suppose that P is UG-faithful to some undirected graph.
Then MIP implies OOD optimality.
Proof. If P is UG-faithful to some undirected graph, we know that strong union law of the conditional
independence applies. That is, if X ⊥ Y∣Φι, E and X ⊥ Y∣Φ2, E, then X ⊥ Y∣Φι, Φ2, E as well.
Let Φ* be a MIpIf there exists another Φ ∈ I such that Φ ∈ σ(φ*), then Φ0 = [Φ, Φ*] has larger
mutual information than Φ*. Also, by the strong union property, Φo ∈ I. This contradicts the
maximality of Φ*, so I = σ(Φ*) necessarily, and the claim follows by the application of the theorem
B.	□
This corollary might be applicable to some DAG cases as well. For more detailed relation between
DAG and undirected graph, see Bishop (2007). We shall also mention that, when P is UG-faithful
to some undirected graph, the MIP is everything that excludes E and Y . As we emphasize over and
over, we are considering the case in which the identity of E as well as the correlation amongst the
covariates and variates, so even if it is known that P is faithful to some undirected graph, finding MIP
might be better strategy in such case.
The following is another sufficient condition for the OOD optimality that can hold if we can make a
slightly stronger assumption about E .
Corollary B.7. Suppose that E admits an independence decomposition (Eφ, Eψ) such that Eψ = Eφc
and Eφ ⊥ Ψ. Ifthere exists one 联ψ for which Ψ ⊥ Y归ψ, then E[Y∣Φ] is OOD optimal.
Proof. Since Φ ⊥ (Ψ, Eψ), we have (Φ ⊥ Ψ) ∣Eψ because
P (Φ∣Ψ, Eψ ) = P (Φ) = P (Φ∣Eψ)	(31)
Likewise, by the assumption we have Eφ ⊥ (Ψ, Eψ), so we have (Eφ ⊥ Ψ)∣Eψ. Now, we also have
the assumption that Ψ ⊥ Y归ψ. Altogether we have (Ψ ⊥ (Y, Eφ, Φ))归ψ. This tells us that
P (Ψ∣Y, Eφ, Φ, eψ) = P (Ψ∣6ψ )= P (Ψ∣Y, 6ψ)	(32)
This is to say that Ψ ⊥ Y∣Φ, Eφ, (≡ψ, and the claim follows by the application of 3.1.	□
Wa also want to make a comment regarding the choice of Eψ .
Remark B.8. Making Eψ maximal in the assumption (3) in general cases the difficulty of satisfying
∙-v
(3). To see this, suppose that Eψ is not maximal, and that there exists Eψ with the corresponding
∙-v	∙-v
complement Eφ for which σ(Eψ) ( Eψ. Now, again in the way of Darmois (1953), let Eψ0 ⊥ Eψ be
such that σ(Eψ, Eψ) = σ(Eψ). This way we can let the triplet (Eφ, Eψ, Eψ) to represent e, and let the
pair (Eφ, Eψ) serve as a Eφ. Whenever one can say that, for all e© there exists eψ satisfying (3), I
can also say as well that, for all e© there exists eψ = (eψ, eψ) for which (3) holds. This is clear by
construction. However, the reverse is not true in general. Even if for e© there exists eψ for which (3)
holds, it is not necessarily true that, for all e© there exists eψ satisfying (3). To see this, suppose that,
for a given e©, there is a unique (eψ, eψ) such that (3) holds. In this case, for any e© = (e©, eψ) with
eψ = eψ, there is no eψ for which (3) holds.
14
Under review as a conference paper at ICLR 2021
Figure 8: (Left) A rough schematic view of the condition assumed in the corollary B.7, interpreted in
the language of graphical model. (Right) When there is a eψ that breaks the edge from Y to Ψ, the
conditional expectation E [Y ∣Φ] will be OOD optimal.
C More details ab out Method
C.1 Difficulties in finding the MIP
At a first glance, the concept we presented in the last section seems to require two steps: (i) the
search for the solution Φ* of (30) and (ii) the computation of E[Y∣Φ*], which is a function of
Φ*. However, finding Φ* is difficult on its own because I(Y; Φ) requires P(Y∣Φ)(density) for its
evaluation. Without reasonably accurate knowledge about the density of X and Y , it is hard to
compute (30), let alone the condition for Φ's invariance.
Classic constrained optimization methods often use regularization terms to enforce the constraint. Let
us use Q to approximate P, and let us parametrize Q by ξ and Φ by η. We can interpret our problem
(30) as the optimization of
argmin E[dκL[p(YX)kp(YΦ)]]
ξ,η	(33)
+ E[dκL[p(YΦ)|除(YΦ)]]+ λIq(Y; E∣Φη)
about both Φ and q. The last regularization term encourages Φ to be in I, and the second term
encourages q to be close to p. The first term encourages Φ to be closely correlated with Y . Now,
Iq(Y, E∣Φη) can be approximated by
E[dκL(qξ(Y∣Φη, E)kqξ(Y∣Φη))]	(34)
and it also follows that
E[dκL[p(Y IX )kqξ (Y ∣Φη)]]
=E[dκL[p(Y IX )kp(Y ∣Φη)]]	(35)
+ E[dκL[p(Y∣Φη )kqξ (Y ∣Φη)]].
With the understanding that the pair of the parameters (ξ, η) represents a function, let us write
dKL[p(YIX)kqξ(YIΦη)] as Le(ξ, η). We can think of (33) as the minimizer of the following
equation about ξ, η;
E[LE(ξ,η)]+λE[dKL(qξ(YIΦη,E)kqξ(YIΦη))].	(36)
However, we encounter another problem yet again. In general, the form of q(yIφ) as a function of
φ depends on the choice of φ 3. This problem is subtle but important. For instance, if qξ(yIφη) is
modeled as f(y, x, ξ, η), then f(y, x, ξ, η0) for η0 6= η is generally not equal to qξ(yIφη0). Rather, it
will most likely equal qξ (y∣φη') for completely other qξ. To see this, note that qξ (y∣Φηo) is solution
about the optimization of arg minr dKL[qξ(yIφη)kr(yIφη0)] about the density r, and it is clear that
the optimal r depends on η . Thus, the choice of η and the choice of ξ are not independent, and the
simultaneous update of ξ and η can be nonsensical.
While aiming for different invariant feature, IRM (Arjovsky et al., 2019) too encountered a similar
problem and endeavored to skirt this problem by assuming that E[Y IΦ] is linear with respect to Φ,
and modeled E[Y IΦ] = h1, Φi by requiring Φ to absorb all linear transformation. However, it is
unlikely that E[Y IΦ] takes the same form throughout the algorithm in search of a good Φ. The same
things can be said to P(Y ∈ AIΦ) for any A, which is, in essence, E[1A(Y)IZ].
3Recall that φ is a symbol representing the realization of Φ
15
Under review as a conference paper at ICLR 2021
C.2 Penalty derivation
In this section, we provide the derivation of the penalty term we omitted in the section 4.2. For the
notations, please take a look at the main manuscript.
I (Y ； E∣Φ) = Iq (Y; E∣Φ)= E[dκL(qθ (Y|X, E )kqθ (Y |X))]	(37)
=E[log qθ(Y|X, E) — log qθ(Y|X)]	(38)
=E[Le(Θ — αVθE[Le(θ)]) - Le(θ - αVθLe(θ))]	(39)
=α(E[VθLE(θ)TVθLE(θ)] - E[VθLe(θ)]TE[VθLe(θ)])	(40)
= α trace(Var(Vθ LE (θ))	(41)
Also, in our implementation, we took advantage of the smallness of α to approximate q(y|x; θ) in the
first term with f (y|x; θ) instead of f(y|x; θ - αVθEe [Le (θ)]). In fact, faithfully computing the first
term with f (y|x; θ - αVθ E[LE (θ)]) did not make much difference in the training process.
D Implementation Detail
In this section we describe the details of the experiment design along with the architectures of the
models we used. In order to present a self-contained material, we first restate the experimental setting
we already described in the main manuscript.
D.1 Colored MNIST
Colored MNIST is an experiment that was used in (Arjovsky et al., 2019). The goal of the task in
Colored MNIST is to predict the label of a given digit in the presence of varying exterior factor, E .
The left panel of the figure 10 is a Bayesian Network representation of this experiment. Each member
of the Colored MNIST dataset is constructed from an image-label pair (x, y) in MNIST, as follows.
1.	Assign a binary label yobs from y with the following rule: yobs = 0 if y ∈ {0 〜4} and
yobs = 1 otherwise.
2.	Flip yobs with a fixed probability P to produce ybs.
3.	Let xfig be the binary image corresponding to y.
4.	Put yobs = Xchi, and construct Xchi from Xch by flipping Xchi with probability e.
5.	Construct xods = xfig × [xch0, (1 -xch0), 0].(that is, red if xch1 = 1 and green if xch1 = 0.)
Indeed, Xobs has exactly same information as the pair (Xfig, Xchi).
In this experiment, only (Yobs, Xobs) are assumed observable. At training times, the machine learner
will be given a set of datasets Dtrain = {De ; e ∈ Rtrain} in which De is a set of observations
gathered when E = e. We set |Rtrain | = 2, and choose |De| = 25000. More particularly, for the ei
we chose the flip-rate(p) to be 0.1, and chose p = 0.2 for the e2. Each image was resized to 14 × 14
resolution.
For the test evaluation, we randomly sampled 10 instances ofp uniformly from the range [0, 1] to
construct Rtest , and approximated the OOD accuracy by computing the worst performance over all
Rtest . We used 5 seeds to produce each numerical result. For the model, we used 4 Layers MLP with
2500 units per each layer and elu activation(Clevert et al., 2015), and did not use bias term in the last
sigmoid activation. We used batch normalization (BN)(Ioffe & Szegedy, 2015) for each layer, and
optimized the model using Adam(Kingma & Ba, 2014) with alpha = 0.0015, beta1=0.0, beta2=0.9
over 500 iterations. In general, less number of iterations yielded better results when |Rtrain | was
small (less overfitting).
D.2 Extended Colored MNIST
As described in the main manuscript, Extended Colored MNIST is a modified version of colored
MNIST, in which the dataset was constructed using the following procedure. The right panel of the
figure 10 is a Bayesian Network representation of this experiment.
16
Under review as a conference paper at ICLR 2021
1.	Set xch2 to 1 with probability ech2 . Set it to 0 with probability 1 - ech2 .
2.	Construct yobs in the same way as in Colored MNIST. If e^? = k, construct yobs by flipping
yobs with probability pk(k ∈ {θ, 1}.)
3.	Put yobs = Xcho, and construct Xcho from Xch by flipping Xchi with probability echo.
4.	Construct xobs as xfig × [xch0, (1 - xch0), xch2]. As an RGB image, this will come out as
an image in which the red scale is turned on and the green scale is turned off if Xch0 = 1,
and otherway around if Xch0 = 0. Blue scale is turned-on only if Xch2 = 1.
In this experiment, we set |Rtrain| = 5, and choose |De| = 10000, and resized each image in the
dataset to 14 × 14 resolution. To produce e ∈ Rtrain, we selected ech0 randomly from the range
[0.1, 0.2], and selected ech2 randomly from the range [0.3, 0.4].
Mean while, we set |Rtest| = 9. To produce n-th member of Rtest, we set ech0 = 0.1 and we
selected ech2 randomly from the range [0.0, 1.0]. We chose p0 = 0.25, p1 = 0.75 for both Rtest and
Rtrain.
We used 5 seeds to produce each numerical result. For the model, we used 4 Layers MLP with
2500 units per each layer, and did not use bias term in the last sigmoid activation. We used batch
normalization for each layer, and optimized the model using Adam with alpha = 0.0005, beta1=0.0,
beta2=0.9 over 2000 iterations. The performance-values of IRM in the Table 3 of the main article are
the results produced by the model that achieved the best average train accuracy among all models
trained with λ > 104 . The averages were computed over 5 seeds.
OC^^S3~)Λ^^is 10Λ IX)
0Λ S3 ，》 ，.，lfrφ IU e∙ IS ，》 ，3 M»X» "3
0Λ S3 ，》 ，.，lfrφ IU e∙ IS ，》 ，3 M»X» "3
(b) Example images of Ex-
tended Colored MNIST. The
first two channels and the
third channel are perturbed
by the different mechanism.
See the main manuscript for
the way of the construction.
(a) Example images of Col-
ored MNIST. Only two chan-
nels are used for all images,
and the colors are flipped
randomly by the exterior
factor.
Figure 9:	Example of Colored MNIST and Extended Colored MNIST
17
Under review as a conference paper at ICLR 2021
Figure 10:	The graphical model of Colored MNIST(left) and Extended Colored MNIST(right)
E Additional Result
E.1 More experimental results on C-MNIST and EC-MNIST
The result of Extended Colored Mnist with p0 = 0.25, p1 = 0.65 for both Rtest and Rtrain. Our
algorithm outperforms the Invariant Risk Minimization(IRM)(Arjovsky et al., 2019) in this case as
well in Figure 11.
----orade
----train a∞
----ood acc
10*	10*	10* ιos 10β
lambda
(a) IRM
----orade
-----train acc
----ood acc
100	101	10*	10* ιo4 ιos 10β
lambda
(b) IGA
Figure 11: Result on Extended Colored MNIST (p0 = 0.25, p1 = 0.65. )

In general, IRM does not work well with standard gradient descent when we implement MLP without
Batch Normalization (Figure 12).
Figure 12: Result of IRM on Colored MNIST with MLP without BN.
18
Under review as a conference paper at ICLR 2021
We shall note that the original implementation of the IRM published in Github (https://github.
com/facebookresearch/InvariantRiskMinimization) uses a very specific schedule
for the regularization parameter λ, and it makes λ to jump to a very large value at a very specific
timing that is difficult to identify from the training set. The following figures are the result of their
original algorithm on Colored MNIST and Extended Colored MNIST implemented with various
jump-timings of λ. For Colored MNIST, the original IRM works for specific choices of the jump
timing(200 〜300). For Extended Colored MNIST, the original algorithm does not work too well
for any choice of the jump timings. Meanwhile, IRM works relatively well on Colored MNIST
consistently if we apply batch normalization, and it works well even without “jumping” the λ. For
the tables we present in the main manuscript, we reported the result of IRM implemented with batch
normalization, which consistently yielded better results than the original implementation.

/	-----oracle
02	------ train acc
-----ocd acc
90
1β0	200 MO 400 S»
Jump Iteration
rejnooe
(a) MLP
04
----oracle
02 ------ train acc
-----ood acc
90
1β0	200 MO 400 S»
Jump Iteration
(b) MLP with BN
Figure 13: The plot of jump timing against the accuracies on Colored MNIST
-----oracle
02 ------- train acc
-----OOda 8
90
1βO 200 MO 400 S»
Jump Iteration
(a) MLP
----oracle
02 -------- train acc
-----OOda 8
90
1βO 200 MO 400 S»
Jump Iteration
(b) MLP with BN
Figure 14: The plot of jump timing of IRM against the accuracies on Extended Colored MNIST.
-----oracle
-----train acc
-----ood acc
100
orace
----train acc
----ood acc
(a) MLP
(b) MLP with BN
Figure 15: The plot of jump timing of IRM against the accuracies on Extended Colored MNIST.
E.2 Ablation study for two phase training with nonlinear predictor
IRM is a type of two-phase training 4 that uses linear predictor g : Φ → Y . Meanwhile, IR (Chang
et al., 2020) uses another variant of two-phase training in which non-linear g : Φ → Y is considered.
If C(f|) is the loss of the function f : X → Y on the environment , (Chang et al., 2020) optimize
the following objective:
arg min max
g,h "e
{e[C(g ◦ h|E)] + λa(E[C(g ◦ h|E)] - E[C(g> ◦ h|E)])}
(42)
where a(t) is a convex function that is monotonically increasing in t when t < 0, and strictly
monotonically increasing in t when t ≥ 0, g* = arg min C (f |e) for each e, and λ is the regularization
19
Under review as a conference paper at ICLR 2021
parameter. We use such monotonic function because in true expectation with respect to X and Y ,
C(g ◦ h∣∣ e) > C(gh ◦ h∣∣ e) by the optimiality of g[. We conducted this optimzation on both C-MNIST
and EC-MNIST, and studied the relation between λ and the final accuracy as well as the value of the
regularization term. Unlike in the original that searches h of the form M ◦ X with binary mask M,
we searched h over the space of black-box function represented by MLP.
We used MLP for both g and h. For g, we used MLP with 4 layers containing 1500 nodes each and
activation function elu and did not use bias term in the last sigmoid activation. For h, we used MLP
with 4 layers containing 1500 nodes each and activation function elu and did not use bias term in
the last sigmoid activation. As is done in both Arjovsky et al. (2019) and Chang et al. (2020), we
optimized both models in parallel without propagating the loss of gh to h. For both C-MNIST and
EC-MNIST, we evaluated the model performance in the same way as in the IRM experiments.
As we see in the plots below, even when the environment agnostic loss C(g ◦ h|E) is very close to
environment aware loss C(g* ◦ h|E), the the performance on the training environments does not
generalize to all environments. This tendency was observed irrespective of the presence of Batch
normalization. This is possibly because true g* is not estimated well in the training process because
of the inter-dependency between g and h.
oracle
train acc
OOd acc
lambda	lambda
(a) accuracy (b) The final nll of environment
agnostic predictor and environ-
ment aware predictor
Figure 16: The two phase training results for Colored MNIST with MLP encoder and MLP predictor
(a) accuracy (b) The final nll of environment
agnostic predictor and environ-
ment aware predictor

Figure 17:	The two phase training results for Colored MNIST with MLP + BN encoder and MLP +
BN predictor
----oracle
----train acc
----ood acc
OW)
02$
=020
«.1C
0.10
10, IO1 10*	10, IO4 10,	10,
1θ'	1β,	104	√	10,
IarntMta
(a) accuracy (b) The final nll of environment
agnostic predictor and environ-
ment aware predictor
Figure 18:	The two phase training results for Extended Colored MNIST (p0 = 0.25, p1 = 0.65) with
MLP + BN encoder and MLP + BN predictor
20
Under review as a conference paper at ICLR 2021
(a)	accuracy
(b)	The final nll of environment
agnostic predictor and environ-
ment aware predictor
Figure 19: The two phase training results for Extended Colored MNIST (p0 = 0.25, p1 = 0.75) with
MLP + BN encoder and MLP + BN predictor
δ∙BJn8e
-----train acc
-----ood acc
------------------------------ nil of env-aware model
2Λ
---- nl∣0fenv-ag∏08tlc model
20
1θ'	10,	1θ'	1β,	10* vf 10,
IamtMta
1θ'	10,	1θ'	1β,	10* vf 10,
IamtMta
(a) accuracy (b) The final nll of environment
agnostic predictor and environ-
ment aware predictor
Figure 20:	The two phase training results for Extended Colored MNIST (p0 = 0.25, p1 = 0.65) with
MLP encoder and MLP predictor
0.7
04
-----oracle
-----train acc	W 0λ
-----ood acc
04
nil of env-aware model
nil of env-ag∏08tlc model
IamtMta
IamtMta
(a) accuracy (b) The final nll of environment
agnostic predictor and environ-
ment aware predictor
Figure 21:	The two phase training results for Extended Colored MNIST (p0 = 0.25, p1 = 0.75) with
MLP encoder and MLP predictor
21