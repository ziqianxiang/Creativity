Under review as a conference paper at ICLR 2021
Spherical Motion Dynamics: Learning Dynam-
ics of Neural Network with Normalization,
Weight Decay, and SGD
Anonymous authors
Paper under double-blind review
Ab stract
In this work, we comprehensively reveal the learning dynamics of neural network
with normalization, weight decay (WD), and SGD (with momentum), named as
Spherical Motion Dynamics (SMD). Most related works study SMD by focusing
on “effective learning rate” in “equilibrium” condition, i.e. assuming the con-
vergence of weight norm. However, their discussions on why this equilibrium
condition can be reached in SMD is either absent or less convincing. Our work in-
vestigates SMD by directly exploring the cause of equilibrium condition. Specif-
ically, 1) we introduce the assumptions that can lead to equilibrium condition in
SMD, and prove that weight norm can approach its theoretical value in a linear
rate regime with given assumptions; 2) we propose “angular update” as a substi-
tute for effective learning rate to measure the evolving of neural network in SMD,
and prove angular update can also approach to its theoretical value in a linear rate
regime; 3) we verify our assumptions and theoretical results on various computer
vision tasks including ImageNet and MSCOCO with standard settings. Experi-
ment results show our theoretical findings agree well with empirical observations.
1 Introduction and Background
Normalization techniques (e.g. Batch Normalization (Ioffe & Szegedy, 2015) or its variants) are
one of the most commonly adopted techniques for training deep neural networks (DNN). A typical
normalization can be formulated as following: consider a single unit in a neural network, the input
is X, the weight of linear layer is w (bias is included in w), then its output is
/V	c、 /Xw - μ(Xw)	c∖
y(X; w; Y; ⑶=g( σ(wX) Y + ⑶,
(1)
where g is a nonlinear activation function like ReLU or sigmoid, μ, σ are mean and standard de-
viation computed across specific dimension of Xw (like Batch Normalization (Ioffe & Szegedy,
2015), Layer Normalization Ba et al. (2016), Group Normalization (Wu & He, 2018), etc.). β, γ
are learnable parameters to remedy for the limited range of normalized feature map. Aside from
normalizing feature map, Salimans & Kingma (2016) normalizes weight by l2 norm instead:
w
y(x; w; γ; β) = g(X ∣jw∣iɪ γ + β),
(2)
where || ∙ ∣∣2 denotes l2 norm of a vector.
Characterizing evolving of networks during training. Though formulated in different manners,
all normalization techniques mentioned above share an interesting property: the weight w affiliated
with a normalized unit is scale-invariant: ∀α ∈ R+, y(X; αW; γ, β) = y(X; w; γ, β). Due to the
scale-invariant property of weight, the Euclidean distance defined in weight space completely fails
to measure the evolving of DNN during learning process. As a result, original definition of learning
rate η cannot sufficiently represent the update efficiency of normalized DNN.
To deal with such issue, van Laarhoven (2017); Hoffer et al. (2018); Zhang et al. (2019) propose “ef-
fective learning rate” as a substitute for learning rate to measure the update efficiency of normalized
1
Under review as a conference paper at ICLR 2021
neural network with stochastic gradient descent (SGD), defined as
η
Tkff = Hwl.
(3)
Joint effects of normalization and weight decay. van Laarhoven (2017) explores the joint effect
of normalization and weight decay (WD), and obtains the magnitudes of weight by assuming the
convergence of weight, i.e. if wt = wt+1, the weight norm can be approximated as ||wt||2 =
O(Pη∕λ), where λ is WD coefficient. Combining with Eq.(3), We have Teff = √ηλ. A more
intuitive demonstration about relationship between normalization and weight decay is presented in
Chiley et al. (2019) (see Figure 1): due to the fact that the gradient of scale invariant weight ∂L∕∂w
(L is the loss function of normalized network without WD part) is always perpendicular to weight
w, one can infer that gradient component ∂L∕∂w always tends to increase the weight norm, while
the gradient component provided by WD always tends to reduce weight norm. Thus if weight norm
remains unchanged, or “equilibrium has been reached”1, one can obtain
Wt - wt+1 _ p2-λ	∂L∕∂w
∣∣wt∣∣2	=Vn E∣∣∂L∕∂w∣∣2.
(4)
Eq.(4) implies the magnitude of update is scale-invariant of gradients, and effective learning rate
should be √2nλ. Li & Arora (2020) manages to estimate the magnitude of update in SGDM, their
result is presented in limit and accumulation manner: if both limT -→∞ RT = 1∕T PtT=0 ∣∣wt∣∣ and
limT -→∞ DT = 1∕T PtT=0 ∣∣wt - wt+1∣∣ exist, then we have
iim Dt = -2nλ.
T-→∞ RT	1 + α
(5)
Though not rigorously, one can easily speculate from Eq.(5) the magnitude of update in SGDM
cases should be ,2nλ∕(1 + α) in equilibrium condition. But proof of Eq.(5) requires more strong
assumptions: not only convergence of weight norm, but also convergence of update norm kwt+1 -
wt k2 (both in accumulation manner).
As discussed above, all previous qualitative
results about “effective learning rate” (van
Laarhoven, 2017; Chiley et al., 2019; Li &
Arora, 2020) highly rely on equilibrium condi-
tion, but none of them explores why such equi-
librium condition can be achieved. Only van
Laarhoven (2017) simply interprets the occur-
rence of equilibrium as a natural result of con-
vergence of optimization, i.e. when optimiza-
tion is close to be finished, wt = wt+1, result-
ing in equilibrium condition. However, this in-
terpretation has an apparent contradiction: ac-
cording to Eq.(4) and (5), when equilibrium
condition is reached, the magnitude of update is
constant, only determined by hyper-parameters,
which means optimization process has not con-
verged yet. Li & Arora (2020) also notices the
non-convergence of SGD with BN and WD, so
they do not discuss reasonableness of assump-
tions adopted by Eq.(5). In a word, previous re-
sults about “effective learning rate” in equilib-
Figure 1: Illustration of optimization behavior
with BN and WD. Angular update ∆t represents
the angle between the updated weight Wt and its
former value Wt+1.
rium condition can only provide vague insights,
they are difficult to be connected with empirical observation.
In this work, we comprehensively reveal the learning dynamics of normalized neural network using
stochastic gradient descent without/with momentum (SGD/SGDM) and weight decay, named as
Spherical Motion Dynamics (SMD). Our investigation aims to answer the following question:
1”Weight norm remains unchanged” means ||wt||2 ≈ ||wt+1 ||2, Chiley et al. (2019) calls this condition as
“equilibrium”, which will also be used in the following context of this paper. Note equilibrium condition is not
mathematically rigorous, we only use it for intuitive analysis.
2
Under review as a conference paper at ICLR 2021
Why and how equilibrium condition can be reached in Spherical Motion Dynamics?
Specifically, our contributions are
•	We introduce the assumptions that can lead to equilibrium condition in SMD, and justify
their reasonableness by sufficient experiments. We also prove under given assumptions,
equilibrium condition can be reached as weight norm approach to its theoretical value in
a linear rate regime in SMD. Our assumptions show the equilibrium condition can occur
long before the finish of the whole optimization;
•	We define a novel index, angular update, to measure the change of normalized neural
network in a single iteration, and derive its theoretical value under equilibrium condition
in SMD. We also prove angular update can approach to its theoretical value in a linear
regime along with behavior of weight norm. Our results imply that the update efficiency of
SGD/SGDM on normalized neural network only depends on pre-defined hyper-parameters
in both SGD and SGDM case;
•	We verify our theorems on different computer vision tasks (including one of most challeng-
ing datasets ImageNet (Russakovsky et al., 2015) and MSCOCO (Lin et al., 2014)) with
various networks structures and normalization techniques. Experiments show the theoreti-
cal value of angular update and weight norm agree well with empirical observation.
Recently, a parallel work (Li et al., 2020b) is published to analyze the equilibrium condition of
normalized neural network with weight decay in SGD case. They model the learning dynamic
of normalized network as a SDE, and propose the concept “intrinsic learning rate” (λe = λη) to
measure the converge rate of weight norm and effective learning rate. Their main result, in which
converging time to equilibrium in SGD case is O(1∕(λη)), is consistent with our theory. The major
difference between Li et al. (2020b) and our work is that all results from Li et al. (2020b) are
qualitatively results or conjectures limited on SGD case. While our work derives the quantitative
results on both SGD/SGDM cases, our analysis allows us to precisely predict the value of weight
norm/angular update in large scale data experiments, and clarify the difference of approaching rate
between SGD and SGDM case. Besides, Li et al. (2020b) conducts empirical study to discuss the
connection between the equilibrium state and accuracy of trained model. Our work mainly focuses
on the equilibrium condition itself, connection between equilibrium condition and performance of
trained model is beyond our discussion in this paper. The experiments we present in main text are
only for verification of our theory.
Our theory on equilibrium condition in Spherical Motion Dynamics implies equilibrium condition
mostly relies on the update rules of SGD/SGDM with WD, and scale-invariant property. The cause
of equilibrium condition is independent of decrease of loss or trajectory of optimization, but equi-
librium condition turns out to significantly affect update efficiency of normalized network by con-
trolling the relative update (Eq.(4)). We believe equilibrium condition is one of the key reason why
learning dynamic of normalized neural network is not consistent with traditional optimization the-
ory (Li et al., 2020b). We think it has great potential to study the leaning dynamic of normalized
network or develop novel efficient learning strategy under the view of Spherical Motion Dynamics.
2	Related Work
Normalization techniques Batch normalization(BN (Ioffe & Szegedy, 2015)) is proposed to deal
with gradient vanishing/explosion, and accelerate the training of DNN. Rapidly, BN has been widely
used in almost all kinds of deep learning tasks. Aside from BN, more types of normalization tech-
niques have been proposed to remedy the defects of BN (Ioffe, 2017; Wu & He, 2018; Chiley et al.,
2019; Yan et al., 2020) or to achieve better performance (Ba et al., 2016; Ulyanov et al., 2016; Sal-
imans & Kingma, 2016; Shao et al., 2019; Singh & Krishnan, 2020). Though extremely effective,
the mechanism of BN still remains as a mystery. Existing works attempt to analyze the function of
BN: Ioffe & Szegedy (2015) claims BN can reduce the Internal Covariance Shift (ICS) of DNN;
Santurkar et al. (2018) argue that the effectiveness of BN is not related to ICS, but the smoothness
of normalized network; Luo et al. (2019) shows BN can be viewed as an implicit regularization
technique; Cai et al. (2019) proves that with BN orthogonal least square problem can converge at
linear rate; Dukler et al. (2020) proves weight normalization can speed up training in a two-layer
ReLU network.
3
Under review as a conference paper at ICLR 2021
Weight decay Weight decay (WD) is well-known as l2 regularization, or ridge regression, in statis-
tics. WD is also found to be extreme effective when applied in deep learning tasks. Krizhevsky &
Geoffrey (2009) shows WD sometimes can even improve training accuracy not just generalization
performance; Zhang et al. (2019) show WD can regularize the input-output Jacobian norm and re-
duce the effective damping coefficient; Li et al. (2020a) discusses the disharmony between WD
and weight normalization. A more recent work Lewkowycz & Gur-Ari (2020) empirically finds the
number of SGD steps T until a model achieves maximum performance satisfies T 8 焉,where
λ, η are weight decay factor and learning rate respectively, they interpret this phenomenon under
the view of Neural Tangent Kernel (Jacot et al., 2018), showing that weight decay can accelerate
the training process. Notice their result has no connection with equilibrium condition discussed in
this work. Our results shows the cause of equilibrium condition can be reached long before neural
network can get its highest performance.
Effective learning rate Due to the scale invariant property caused by normalization, researchers
start to study the behavior of effective learning rate. van Laarhoven (2017); Chiley et al. (2019)
estimate the magnitude of effective learning rate under equilibrium assumptions in SGD case; Hoffer
et al. (2018) quantify effective learning rate without equilibrium assumptions, so their results are
much weaker; Arora et al. (2019) proves that without WD, normalized DNN can still converge
with fixed/decaying learning rate in GD/SGD cases respectively; Zhang et al. (2019) shows WD
can increase effective learning rate; Li & Arora (2020) proves standard multi-stage learning rate
schedule with BN and WD is equivalent to an exponential increasing learning rate schedule without
WD. As a proposition, Li & Arora (2020) quantifies the magnitude of effective learning rate in
SGDM case. But none of them have ever discussed why equilibrium condition can be reached. A
recent work Li et al. (2020b) studies the convergence of effective learning rate by SDE, proving
that the convergence time is of O(1∕(λη)), where λ, η are weight decay factor and learning rate
respectively. Their result can only provide intuitive understanding, and is limited on SGD case.
3	Preliminary on S pherical Motion Dynamics
First of all, we review the property of scale invariant weight, and depict Spherical Motion Dynamics
(SMD) in SGD case. Notice except definitions, all intuitive statements or derivations in this section
mostly comes from previous literature, they are not mathematically rigorous. We summarize them
to provide background of our topic and preliminary knowledge for readers.
Lemma 1. If w is scale-invariant with respect to L(w) , then for all k > 0, we have:
∂L
∂w
i
w=wt
∂ Li
∂w lw=kwt
0
L ∂L∣
k ∂w iw=wt
(6)
(7)
Proof can be seen in Appendix B.1. Lemma 1 is also discussed in Hoffer et al. (2018); van Laarhoven
(2017); Li & Arora (2020). Eq.(7) implies gradient norm is influenced by weight norm, but weight
norm does not affect the output of DNN, thus we define unit gradient to eliminate the effect of
weight norm.
Definition 1 (Unit Gradient). If Wt = 0, W = w∕∣∣w∣∣2, the Unitgradientof ∂L/∂w∣w=wt is
∂L∕∂w∣w=wt.
Setting k as 1/||Wt||2 inEq.(7), we have
∂L∣	=，∙ ∂L∣	.
∂w ∣w=wt	∣∣Wt∣∣ ∂w Iw=Wt
A typical SGD update rule without WD is
∂L i
wt+1 = wt - η∂wlw=wt，
if ||Wt||2 = ||Wt+1||2, dividing both side of Eq.(9) by ||Wt||2, we have
Wt+1 = Wt -
η	∂L
∣∣wt∣∣2 ∂W ∣w=wt
∂L i
wt-%ff∙ ∂W Iw=Wt
(8)
(9)
(10)
4
Under review as a conference paper at ICLR 2021
Eq.(10) shows effective learning rate can be viewed as learning rate of SGD on unit sphere
Sp-1 (Hoffer et al., 2018). But effective learning rate still cannot properly represent the magni-
tude of update, since unit gradient norm is unknown. Therefore we propose the angular update
defined below.
Definition 2 (Angular Update). Assuming wt is a scale-invariant weight from a neural network at
iteration t, then the angular update ∆t is defined as
∆t = ](wt, wt+1) = arccos
hwt, Wt+ιi ∖
lwM ∙ ||wt+i||7 ,
(11)
where ](∙, ∙) denotes the angle between two vectors,《,•〉denotes the inner product.
According to Eq.(6), ∂L∕∂w is perpendicular to weight w. Therefore, if angular update ∆t is small
enough, it can be approximated by first order Taylor series expansion of tan ∆t, then we can see its
connection between effective learning rate and unit gradient norm
∆t = tan(∆t) = ɪ ∙∣∣ILl	∣∣2= ηeff∙ ILl	.	(12)
||wt||	∂w IW=Wt	∂w IW=Wt
Another deduction from Eq.(6) is that weight norm always increases because
ILl
I∣wt+ι∣l2 = l*||2 + (η∣∣dL∣w=wj2)2 > ∣wM2∙	(13)
From Eq.(7) we can infer that increasing weight norm can lead to smaller gradient norm if unit
gradient norm is unchanged. Zhang et al. (2019) states the potential risk that GD/SGD without WD
but BN will converge to a stationary point not by reducing loss but by reducing effective learning
rate due to increasing weight norm. Arora et al. (2019) proves that full gradient descent can avoid
the risk and converge to a stationary point defined in Sp-1, but their results still require sophisticated
learning rate decay schedule in SGD case. Besides, practical implementation suggests training DNN
without WD always suffers from poor generalization (Zhang et al., 2019; Bengio & LeCun, 2007;
Lewkowycz & Gur-Ari, 2020).
Now considering the update rule of SGD with WD:
ILl
wt+1 = Wt - n(『	+ λwt).
Iw W=Wt
We can approximate the update of weight norm by
η2	IL l
llwt+lll2 ≈ ||wt||2 - ληllwtll2+ 2∣∣wi」IwL=Wtt||2.
(14)
(15)
The derivation of Eq.(15) is presented in Appendix A.1. Eq.(15) implies WD can provide di-
rection to reduce weight norm, hence Chiley et al. (2019); Zhang et al. (2019) points out the
possibility that weight norm can be steady, but do not explain this clearly. Here we demon-
strate the mechanism deeper (see Figure 1): if unit gradient norm remains unchanged, note “cen-
2
tripetal force” (一λη∣∣wt∣∣2) is proportional to weight norm, while “centrifugal force” (卯In ∣∣3 ∙
2||Wt||2
|| ∂∣L I	||2) is inversely proportional to cubic weight norm. As a result, the dynamics of weight
W W=Wt
norm is like a spherical motion in physics: overly large weight norm makes centripetal force larger
than centrifugal force, leading to decreasing weight norm; while too small weight norm makes cen-
tripetal force smaller than centrifugal force, resulting in the increase of weight norm. Intuitively,
equilibrium condition tends to be reached if the number of iterations is sufficiently large.
Notice that the core assumption mentioned above is “unit gradient norm is unchanged”. In fact this
assumption can solve the contradiction we present in Section 1: convergence ofwt leads to nonzero
∣∣wt+ι - wt∣∣2∕∣∣wt∣∣2. Our analysis implies the convergence of weight norm is not equivalent
to convergence of weight, steady unit gradient norm can also make weight norm converge, steady
unit gradient norm does not rely on the fact that optimization has reached an optimum solution.
But a problem about unit gradient assumption rises: unit gradient norm cannot remain unchanged
during training in practice, it is not a reasonable assumption. In next section, we formulate this
assumption in a reasonable manner, and rigorously prove the existence of equilibrium condition in
SGD case. Discussion about SGD cannot be easily extended to SGDM case, because momentum
is not always perpendicular to weight as unit gradient. But we can still prove the existence of
equilibrium condition in SGDM case under modified assumptions.
5
Under review as a conference paper at ICLR 2021
4	Main Results
First of all, we prove the existence of equilibrium condition in SGD case and provide the approaching
rate of weight norm.
Theorem 1.	(Equilibrium condition in SGD) Assume the loss function is L(X; w) with scale-
invariant weight W, denote gt = ∂dL IX= WJ gt = gt ∙ ∣∣w∕∣2. Considering the update rule of SGD
with weight decay,
Wt+1 = Wt - η ∙ (gt + λwt)	(16)
where λ, η ∈ (0, 1). If the following assumptions hold: 1) λη << 1; 2) ∃L, V, ∈ R+,
E[∣∣gt∣l2∣Wt] = L, E[(l∣gt∣l2 - L)2∣Wt] ≤ V； 3) ∃l ∈ R+, I∣gt∣l2 > l ,l > 2[ 1¾ F L Then
∃B > 0, w* = PLη∕(2λ), we have
E[∣∣wτ||2 - (w*)2]2 ≤ (1 - 4λη)TB + Vη2.	(17)
Remark 1. The theoretical value of weight norm w* in Theorem 1 is consistent with the derivation
of weight norm in equilibrium in van Laarhoven (2017), though van Laarhoven (2017) assumes the
equilibrium condition has been reached in advance, hence van Laarhoven (2017) cannot provide
the approaching rate and scale of bias/variance. The vanishing term ((1 - 4λη)TB) in Eq.(17) is
consistent with the convergence time O(1∕(λη)) presented in Li et al. (2020b).
Proof of Theorem 1 can be seen in Appendix B.2. It shows the square of weight norm can approach
to its theoretical value in a linear rate regime (when vanishing term is larger than noisy term in
Eq.(17)), and its variance is bounded by Vn-, which is empirically small. Now We discuss the
reasonableness of assumptions in Theorem 1. Assumption 1 is consistent with commonly used
settings; assumptions 2 and 3 imply the E||gt ||2 remains unchanged within several iterations, and its
lower bound cannot be far from its expectation.
We need to clarify assumption 2 further: we do not require the E|®t ||2 must be constant across the
whole training process, but only remains unchanged locally. On one hand, small learning rate(η) can
guarantee E|®t ||2 changes slowly; on the other hand, when E||gt ||2 changes, it means that square of
weight norm will approach to its new theoretical value as Theorem 1 describes. Experiment result in
Figure 2 strongly justifies our analysis. We also conduct extensive experiments to verify our claim
further, please refer to Appendix C.1.
Now we extend Theorem 1 to SGDM case. SGDM is more complex than SGD since momentum is
not always perpendicular to the weight, therefore we need to modify assumptions.
Theorem 2.	(Equilibrium condition in SGDM) Considering the update rule of SGDM (heavy ball
method (Polyak, 1964)):
vt = αvt-1 + gt + λWt	(18)
Wt+1 = Wt - ηvt	(19)
where λ,η,a ∈ (0,1). If following assumptions hold: 4) λη << 1, λη < (1 — √ɑ)2;
5) Define ht = ∣∣gt∣∣2 + 2α(vt-i,gt, ht = ht ∙ ∣∣Wt∣∣2. ∃L,V, ∈ R+, E[htWt] = L,
E[(ht -	L)2∣wt]	≤ V; 6)∃l	∈	R+,	ht	> l, l > 2[(i-a)3(i+0)-8λn(i-α) f L	then	∃B	>	0,
w* = PLη∕(λ(1 — α)(2 — λη∕(1 + α))), we have
E[∣∣wt∣∣2 - (w*)2]2 ≤ 3B ∙ (1 -产)t + 叫，：4。；+。4),	(20)
1 - α	l(1 - α)4
Remark 2. So far, no other work rigorously prove equilibrium condition can be reached in SGDM
cases. Even the most relevant work (Li et al., 2020b) only provides their conjecture on approaching
rate of weight norm in SGDM, they speculate that the time of approaching to equilibrium should be
O(1∕(λη)), same order as approaching time in SGD case, their conjecture cannot provide further
insight. While our results (vanishing terms in Eq.(17), (20) respectively) can clearly reflect the
difference: the approaching rate of SGDM should be 1∕(1 - α) times faster than rate of SGD with
same ηλ. α is usually set as 0.9, so SGDM can reach equilibrium condition much faster than SGD.
6
Under review as a conference paper at ICLR 2021
Proof can be seen in Appendix B.3. Like assumption 1, assumption 4 is also satisfied for commonly
used hyper-parameter settings. Besides, λη < (1 - √ɑ)2 is also mentioned in Li & Arora (2020)
for other purpose; Assumption 5 shows not unit gradient gradient norm ||gt ||2 but an adjusted value
ht dominates the expectation and variance of the weight norm square. We empirically find the
expectation of hvt-ι, gti is very close to 0, therefore the behavior of ht is similar to that of ∣∣gt∣∣2
(see (d) in Figure 2), making square of weight norm approaching its theoretical value in SGDM
case. We leave theoretical analysis on ht as future work. As for assumption 6, commonly used
settings (ηλ << 1) can make 2](-仪尸.+*—8λη(i-α)]2 as an very small lower bound for l/L.
The experiments on justification of assumptions 4,5,6 can be seen in Figure 2 and appendix C.1.
Comparing with Eq.(17) and Eq.(20), it implies that with same η, λ, SGDM can reach equilibrium
condition much faster than SGD, but may have a larger variance, our experiments also verify that(see
(b), (e) in Figure 2).
Since we have proven weight norm will approach to its theoretical value in SMD in a linear rate
regime, we can derive the theoretical value of angular update ∆t and its variance.
Theorem 3.	(Theoretical value of Angular Update) In SGD case, if assumptions in theorem 1
holds, then ∃C > 0, we have
E(∆τ - √2λη)2 < (1 - 4ηλ)TC + V	(21)
Ll
In SGDM case, if assumptions in theorem 2 holds, ∃C > 0, we have
IRVA / 2λη ¥ /4ηλ YTC I(I- α2)(1 + 4α2 + α4)v	OrI
Egt- V τ+α) <(I- 1-a) C+--------------------4La------------	(22)
Remark 3. Theoretical value of angular update in Theorem 3 is consistent with Eq.(4, 5) from
Chiley et al. (2019); Li & Arora (2020) respectively. Notice the variance term in Eq.(21,22) is of
O(V /Ll), it is too large comparing with its empirical value, we leave it as a future work to improve
the bound of variance term. Though connection between performance of neural network and equi-
librium condition is beyond the main discussion of this paper, the findings of a parallel work (Li
et al., 2020b) can inspire us the possible advantage of momentum: Li et al. (2020b) interprets that
smaller λη can get higher performance when training DNN, but the approaching rate (O (λη)) is
slow; larger λη hasfaster approaching rate but it can only get bad performance. Inspired by Li et al.
(2020b), we suspect that ∆t H √λη is highly correlated to the performance ofDNN: smaller angu-
lar update in equilibrium condition can lead to better performance of DNN. According to theorem
3, with same λη, momentum method hasfaster approaching rate(λη∕(1 — α)) and SmaUer angular
update (,2λη∕(1 + α)) than pure SGD. This means momentum method can simultaneously accel-
erate training process and improve the final performance, comparing pure SGD method. We leave
this conjecture as a future work.
Proof of theorem 3 is shown in Appendix B.4. According to theorem 3, the theoretical value of
angular update and its approaching rate almost only depends on hyper-parameters: weight decay
factor λ, learning rate η, (and momentum factor α). It implies update efficiency of scale-invariant
weights within a single step is totally controlled by predefined hyper-parameters in equilibrium
condition, regardless other attributes of the weights (shape, size, position in network structure, or
effects from other weights). That’s the key reasons why we propose Angular Update (Eq.(11))
to replace effective learning rate (Eq.(3)): effective learning rate can only reflect the influence of
weight norm, it cannot reveal how unit gradient norm affect the relative update (see Eq.(10)), while
angular update in equilibrium condition implies that both weight norm and unit gradient norm do
not affect the scale of update within a single iteration, only hyper-parameters (learning rate, weight
decay factor, momentum factor) matter. Our experiment results strongly prove our claim (see Figure
3(a), 3(b), 3(d), 3(e)).
5	Experiments
In this section, we show that our theorems can agree well with empirical observations on Ima-
geNet (Russakovsky et al., 2015) and MSCOCO (Lin et al., 2014). We conduct experiments in two
cases. In the first case we train neural network with fixed learning rate to verify our theorems in
SGD and SGDM, respectively; in the second case we investigate SMD with more commonly used
settings, multi-stage learning rate schedule.
7
Under review as a conference paper at ICLR 2021
5.1	Fixed Learning Rate
With fixed learning rate, we train Resnet50 (He et al., 2016) with SGD/SGDM on ImageNet. Learn-
ing rate is fixed as η = 0.2; WD factor is λ = 10-4; with SGDM, the momentum factor is α = 0.9.
Figure 2 presents the unit gradient norm square, weight norm, and angular update of the weights
from layer.2.0.conv2 of Resnet50 in SGD and SGDM cases, respectively. It can be inferred
from Figure 2 the behavior of ||gt ||2, ht and hyper-parameter settings satisfy our assumptions in
Theorem 1 and 2, therefore theoretical value of weight norm and angular update agree with empir-
ical value very well. We also can observe SGDM can achieve equilibrium more quickly than SGD.
According to Eq.(17),(20), the underlying reason might be with same learning rate η and WD factor
λ, approaching rate of SGDM (1 - i—α) is smaller than that of SGD (1 - λη).
(a) Unit gradient norm in SGD
(b) Weight norm in SGD
(c) Angular update in SGD
(f) Angular update in SGDM
(d) Unit gradient norm square and (e) Weight norm in SGDM
r .	....
h in SGDM
Figure 2: Performance of layer.2.0.conv2 from Resnet50 in SGD and SGDM, respectively. In (a),
(d), semitransparent line represents the raw value of ||gt||2 or ht, while solid line represents the
averaged value within consecutive 200 iterations to estimate the expectation of ||gt||2 or ht; In (b),
(e), blue solid lines shows the raw value of weight norm ||wt ||2 , while dashed line represent the
theoretical value of weight norm computed in Theorem 1, 2 respectively. The expectations E∣∣g∣∣2
and Eh are estimated as solid lines (a) and (d) respectively; In (c), (f), red lines represent raw value
of angular update during training, dashed lines represent the theoretical value of angular update
computed by λ∕2λp and ,2λη/ (1 + α) respectively.
5.2	Multi-stage Learning Rate Schedule
Now we turn to study the behavior of angular update with SGDM and multi-stage leanring rate
schedule on Imagenet (Russakovsky et al., 2015) and MSCOCO (Lin et al., 2014). In ImageNet
classification task, we still adopt Resnet50 as baseline for it is a widely recognized network struc-
ture.The training settings rigorously follow Goyal et al. (2017): learning rate is initialized as 0.1,
and divided by 10 at 30, 60, 80-th epoch; the WD factor is 10-4; the momentum factor is 0.9. In
MSCOCO experiment, we conduct experiments on Mask-RCNN (He et al., 2017) benchmark using
a Feature Pyramid Network (FPN) (Lin et al., 2017), ResNet50 backbone and SyncBN (Peng et al.,
2018) following the 4x setting in He et al. (2019): total number of iteration is 360, 000, learning rate
is initialized as 0.02, and divided by 10 at iteration 300, 000, 340, 000; WD coefficient is 10-4.
There appears to be some mismatch between theorems and empirical observations in (a), (b) of
Figure 3: angular update in the last two stages is smaller than its theoretical value. This phenomenon
can be well interpreted by our theory: according to Theorem 1, 2, when equilibrium condition is
reached, theoretical value of weight norm satisfies ||wt112 æ 4∕λ, therefore when learning rate is
8
Under review as a conference paper at ICLR 2021
(a) Angular update in Imagenet
(d) Angular update in MSCOCO
(c) Weight norm in Imagenet
(e) Angular update in MSCOCO (f) Weight norm in MSCOCO
(rescaled)
Figure 3: In (a),(b),(d),(e), solid lines with different colors represent raw value of angular update
of weights from all convolutional layer in the model; In (a), (d), training setting rigorously follows
Goyal et al. (2017); He et al. (2019) respectively; In (b), (e), weight norm is dividied by √10 as
long as learning rate is divided by 10; In (c), (f), weight norm is computed on layer.1.0.conv2 in
Resnet50 backbone. Blue line represent original settings, orange line represent rescaled settings.
divided by k , equilibrium condition is broken, theoretical value of weight norm in new equilibrium
condition should get 弋 1/k smaller. BUt new equilibrium condition cannot be reached immediately
(see (c), (f) in Figure 3), following corollary gives the least number of iterations to reach the new
equilibrium condition.
Corollary 3.1. In SGD case with learning rate η, WD coefficient λ, if learning rate is divided by k,
and unit gradient norm remains unchanged, then at least d[log(k)]/(4λη)] iterations are required
to reach the new equilibrium condition; In SGDM case with momentum coefficient α, then at least
d[log(k)(1 - α)]/(4λη)e iterations are required to reach the new equilibrium condition.
Corollary 3.1 also implies SGD/SGDM with smaller learning rate requires more iterations to reach
new equilibrium condition. Hence, in second learning rate stage in Imagenet experiments, angular
update can reach its new theoretical value within 15 epochs, but during last two learning rate stages
of Imagenet/MSCOCO experiments, SGDM cannot completely reach new equilibrium by the end
of training procedure. As a result, we observe empirical value of angular update seems smaller than
its theoretical value. Based on our theorem, we can bridge the gap by skipping the intermediate
process from old equilibrium to new one. Specifically, when learning rate is divided by k, norm
of scale-invariant weight is also divided by 4∕k,, SGDM can reach in new equilibrium immediately.
Experiments((b),(e) in Figure 3) show this simple strategy can make angular update always approx-
imately equal to its theoretical value across the whole training process though learning rate changes.
6	Conclusion
In this paper, we comprehensively reveal the learning dynamics of DNN with normalization,
WD, and SGD/SGDM, named as Spherical Motion Dynamics (SMD). Different from most related
works (van Laarhoven, 2017; Hoffer et al., 2018; Chiley et al., 2019; Li & Arora, 2020), we directly
explore the cause of equilibrium. Specifically, we introduce the assumptions that can lead to equi-
librium condition, and show these assumptions are easily satisfied by practical implementation of
DNN; Under given assumptions, we prove equilibrium condition can be reached at linear rate, far
before optimization has converged. Most importantly, we show our theorem is widely valid, they
can be verified on one of most challenging computer vision tasks, beyond synthetic datasets. We
believe our theorems on SMD can bridge the gap between current theoretical progress and practical
usage on deep learning techniques.
9
Under review as a conference paper at ICLR 2021
References
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rkxQ-nA9FX.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007.
Yongqiang Cai, Qianxiao Li, and Zuowei Shen. A quantitative analysis of the effect of batch nor-
malization on gradient descent. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, Long Beach, California, USA, 09-15 JUn 2019. PMLR. URL
http://proceedings.mlr.press/v97/cai19a.html.
Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia Samaniego de la FUente,
Vishal SUbbiah, and Michael James. Online normalization for training neUral networks. In Ad-
vances in Neural Information Processing Systems 32, pp. 8433-8443. CUrran Associates, Inc.,
2019.
Yonatan Dukler, Quanquan Gu, and Guido MontUfar. Optimization theory for relu neural networks
trained with normalization layers. In International conference on machine learning, 2020.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings ofthe
IEEE international conference on computer vision, pp. 2961-2969, 2017.
Kaiming He, Ross Girshick, and Piotr Dollar. Rethinking imagenet pre-training. In Proceedings of
the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 2160-2170. Curran Associates, Inc., 2018.
Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized
models. In Advances in neural information processing systems, pp. 1945-1953, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, pp. 448-456, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 8571-8580. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8076- neural- tangent- kernel- convergence- and- generalization- in- neural- networks.
pdf.
Alex Krizhevsky and Hinton Geoffrey. Learning multiple layers of features from tiny images. 2009.
Aitor Lewkowycz and GUy Gur-Ari. On the training dynamics of deep networks with l_2 regular-
ization. Advances in Neural Information Processing Systems, 33, 2020.
10
Under review as a conference paper at ICLR 2021
Xiang Li, Chen Shuo, and Yang Jian. Understanding the disharmony between weight normalization
family and weight decay. In AAAI, 2020a.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. In Interna-
tional Conference on Learning Representations, 2020. URL https://openreview.net/
forum?id=rJg8TeSFDH.
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional
optimization analyses: The intrinsic learning rate. Advances in Neural Information Processing
Systems, 33, 2020b.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C LaWrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
TsUng-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
Feature pyramid netWorks for object detection. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 2117-2125, 2017.
Ping LUo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. ToWards Understanding regUlarization
in batch normalization. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=HJlLKjR9FQ.
Ningning Ma, XiangyU Zhang, Hai-Tao Zheng, and Jian SUn. ShUfflenet v2: Practical gUidelines
for efficient cnn architectUre design. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 116-131, 2018.
Chao Peng, Tete Xiao, Zeming Li, YUning Jiang, XiangyU Zhang, Kai Jia, Gang YU, and Jian SUn.
Megdet: A large mini-batch object detector. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6181-6189, 2018.
Boris T Polyak. Some methods of speeding Up the convergence of iteration methods. USSR Com-
putational Mathematics and Mathematical Physics, 4(5):1-17, 1964.
Olga RUssakovsky, Jia Deng, Hao SU, Jonathan KraUse, Sanjeev Satheesh, Sean Ma, Zhiheng
HUang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visUal
recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
Tim Salimans and DUrk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neUral netWorks. In Advances in Neural Information Processing Systems, pp.
901-909, 2016.
Mark Sandler, AndreW HoWard, Menglong ZhU, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residUals and linear bottlenecks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 4510-4520, 2018.
Shibani SantUrkar, Dimitris Tsipras, AndreW Ilyas, and Aleksander Madry. HoW Does Batch Nor-
malization Help Optimization? arXiv e-prints, art. arXiv:1805.11604, May 2018.
Wenqi Shao, Tianjian Meng, JingyU Li, RUimao Zhang, YUdian Li, Xiaogang Wang, and Ping LUo.
Ssn: Learning sparse sWitchable normalization via sparsestmax. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 443-451, 2019.
SaUrabh Singh and Shankar Krishnan. Filter response normalization layer: Eliminating batch de-
pendence in the training of deep neUral netWorks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), JUne 2020.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
TWan van Laarhoven. L2 regUlarization versUs batch and Weight normalization. In Advances in
Neural Information Processing Systems. 2017.
11
Under review as a conference paper at ICLR 2021
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun. Towards stabi-
lizing batch statistics in backward propagation of batch normalization. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
SkgGjRVKDS.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. In International Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=B1lz-3Rct7.
12
Under review as a conference paper at ICLR 2021
A Missing derivations
A.1 Approximation of Weight norm Update
Now considering the update rule of SGD with WD:
∂L I
wt+1 = Wt - η(∂W∣ _	+ λwt).	(23)
Since 怒 I	is perpendicular to wt, we have
IW = Wt
C	∂L I	C
l∣wt+ι∣∣2 = ∣∣wt-η( ∂L∣w=w + λwt)∣∣2	(24)
=(I- λη)2∣∣wt∣∣i+ η2∣H- i	∣∣i∙	(25)
∂w IW=Wt
Therefore
∣∣wt+1∣∣2 - ∣∣wt∣∣2 = J(I - λη)2∣∣wt∣∣i + η2∣∣IL	∣∣∣^-∣∣wt∣∣2	(26)
V	∂w IW=Wt
(I- λη)2∣∣wt∣∣2 + η2∣∣IL1	∣∣2 - ∣∣wt∣∣2
=	IW=W-------------.	(27)
J(1 - λη)2∣∣wt∣∣2 + η2∣∣ 恁 ∣	∣∣2 + ∣∣wt∣∣2
W=Wt
Now assume η, λ is extremely small, so that (1-λ疗 ∣∣wt∣∣2+η2∣∣ W LWt∣∣2	- ∣∣wt∣∣22	≈	-2λη∣∣wt∣∣2 + η2∣∣ IL 1		∣∣2 (28) w=wt	
J(I - λη)2∣∣wt∣∣2+ η2∣∣IL	∣∣2 + ∣∣wt∣∣2 ≈ V	Iw IW=Wt Therefore, we have η2	IL I	C ∣∣wt+1∣∣2-∣∣wt∣∣2	≈ -λη∣∣wt∣∣2+9∣∣	I, ∣∣-ξ- i	∣∣2	二 2∣∣wt∣∣2 Iw IW=Wt where 怒1	is the unit gradient defined in Definition 1. IW=Wt			2∣∣wt∣∣2. =-λη∣∣wt∣∣2 +	2 η。∣∣ 2∣∣wt∣∣31∣	IL Iw	(29) ∣∣22 w=wt (30)
B PROOF OF THEOREMS
Remark 4. In thefollowing context, we will use thefollowing conclusion multiple times: ∀δ, ε ∈ R,
if ∣δ∣《1, ∣ε∣《1, then we have:
(1 + δ)2 ≈ 1 + 2δ, √1 + δ ≈ 1 + —,	------ ≈ 1 — δ, (1 + δ)(1 + ε) ≈ 1 + δ + ε. (31)
2	1 + δ
B.1	PROOF OF LEMMA 1
Proof. Given wŋ ∈ Rp∖{0}, since ∀k > 0, L(w0) = L(kwo), then we have
IL(w) ∣ IW	I w=W0	IL(kw) I Iw W	IL(w) =	二	 =W0	IW	I	∙ k I w = kwo		(32)
IL(kw) I Ik	I W=Wo	=hiL(W) I Iw	i w=	∖	1 Z w0 0 = k	/IL(W) I	∖ Y~∂W- i w=W0 , W0”	0	(33)
					□
13
Under review as a conference paper at ICLR 2021
B.2	Proof OF Theorem 1
Lemma 2. Ifthe SeqUenCe {xt}∞=1 satisfies
Xt ≥ αxt-ι +-----
Xt-1
(34)
.where xι > 0, L > 0
Then, we have
xt ≥
(35)
Proof. If Xt ≥ Jɪ-ɑ, since PLlQ - α) ≥ 2√0L then we have
L I L	L	Γ~L
xt+1 ≥ αxt + X ≥ W E + pL∕(1 - α) =VT-
which means ∀k > t,xk ≥ ,L∕(1 - α). If Xt < /L∕(1 - α), then we have
-xt+1 ≤ (α -
7L(Xt- O)	-Xt) <α(↑∕τ-La E
(37)
Therefore, by deduction method we have if XT < /L∕(1 - α), then ∀t ∈ [1,T - 1], we have
In summary, we have
XT <
0 < Xt < Xt+1
<
(38)
(39)
(40)
□
ProofofTheorem 1. sincehwt, 9t) = 0, then we have:
ι∣wt+ι∣ιl =(1 - ηλ)2∣∣wt ill+ 叱！ ∣j∣η	(41)
llwt ∣∣2
Denote Xt as ∣∣wt∣∣2,	Lt	as ∣∣gt∣∣2	and omit	O((ηλ)2) part. Then	Lt	> l,	E[Lt∣xt]	=	L,
Var(LtiXt) = E[(Lt - L)2∣xt] < V. Eq.(41) is equivalent to
xt+1 =(I- 2λη)xt+号-
(42)
According to Lemma 2, we have
xt > y^η - (1 - 2λη)t-1∣xo- J^n∣.
V 2λ	V 2λ
(43)
Eq.(43) implies when t > 1 + "g"T’必喝；)--；黑x0-"/(41)I) ,we have Xt > ^^
Now, denote x* as the JLn, then we have
E[(xt+1 - x*)2∣xt] = E[((1 - 2λη -")(xt - x*) + L-L)2∣xt]
xt x	xt
=(1 - 2λη - L-)2(xt - x*)2 + E[(Lt-L)2岛旧4
xtx	xt
(44)
14
Under review as a conference paper at ICLR 2021
If t > T(α, λ, η,l, xo) = 1+ log((√2τ)"η∕O4λ)-2*x0-Mln/(4λ^, Eq.(43) implies
1 — 2λη 一 ∖ ∙2λ ∙ 2λη < ι — 2λη — Ln .	(45)
V l	x*xt
Combining with assumption 3 in Theorem 1, We have 1 - 2λn - 2L∙- ∙ 2λη > 0, which means
0 < 1 - 2λη -3.	(46)
x xt
Combining with Eq.(44), (43), (46), ift > T(α, λ, η, l, x0), we have
E[(xt+1 - x*)2∣xt] < (1 - 2λη)2(xt - x*)2 + 4Vη λ.	(47)
Considering the expectation with respect to the distribution of xt, we have
E(xt+1 - x*)2 < (1 - 2λη)2E(xt - x*)2 + 4Vη3λ.	(48)
Approximate (1 - 2λη)2 = 1 - 4λη, and iterate Eq.(48) for t - T(α, λ, η, l, x0) times, we have
E(Xt- x*)2 < (1 - 4λη)t-T(α,λ,n,l,x0)E(xτg,λ,n,ι,xo) - x*)2 + *.	(49)
T(α, λ, η, l, x0) is finite, and only depends on α, λ, η, l, x0. Hence we can set B = max{(1 -
4λη)-tE(xt - x*)2|t = 0, 1, 2, ..., T(α, λ, η, l, x0)}, note B is finite by
L2	V +L2
Ex2+1 = E[(1-2λη)2x2+2(1-2λη)Lt + L] < (1-4λη)Ex2 +2(1-2λη)L+ min(x2,lη/(2λ)) ∙
t	0,	(50)
therefore, ∀t > 0, we have
E(Xt - x*)2 < (1 - 4λη)tB + 邛.
(51)
□
B.3 Proof of Theorem 2
Lemma 3. Assume a,β,ε ∈ (0,1), where β << 1. Denote diag(1 — ɪ2—α, α, α2 + τ2α2β as A,
k = ((ɪɪp, - (1-200)2, (ɪ-θp )τ, e = (1,1,1)T. Ifε < 3 [ 1-βɑ2 - 1—α ] ,then ∀d ∈ Rp, WehaVe
ll(Λ - εβ(1 - α)2keτ)d∣∣2 < (1 - Tɪ)∣∣d∣∣2	(52)
1-α
Proof. Omit O(β2) part, we have
||(A-εβ(1-α)2keτ )d∣∣2 = (1- -)d2+α2d2+α4(1+-)d3-2εβ(dι+d2+⅛)(dι-2α2d2 +α4d3)
2	1-α 1	2	1-α 3
(53)
(d1+ d2 + d3)(d1 - 2α2d2 + α4d3)
= [d1 + (1 - 2α2)d2]2 + [d1 + (1 + a')"3]2 - (1/2 + 2ɑ4)d2 - 1+α8 + (α4 - 2α2)d2d3
1	α4 α8 α4	5
≥ - (2	+ 2a	+ ^2^)d2	- (ɪ	+ ^2	2a	+ 2)d3
≥ - 3d2 - 2d2.
(54)
15
Under review as a conference paper at ICLR 2021
Then we have
∣∣(Λ — εβ(1 — a)2keτ)d∣∣2 ≤ (1----)d2 + (02 + 3βε)d2 + (α4 +
1 — α
4βɑ4
1 — α
+ 5βε 渥(55)
Since ε < 3 [ 1-02 - τ⅛], We have
Hence, we have
02 + 3βε <
4 4βα4	5βε
04 + E+〒<
4β
1		------
1	— α
4β
1 — a
(56)
(57)
∣∣(Λ - εβ(1 - α)2keτ)d||2 <
(1 -「)∣∣d∣∣2
1 — α
(58)
□
Proof of Theorem 2.
The update rule is
Wt+1 = Wt - ηvt
=Wt - η(αvt-i + 77gt7τ + λwt)
HwtH
=Wt - η(αwt-1Wt + TTgtTT + λwt)
η	HwtH
=(1 - ηλ + α)wt - αwt-i - gtη.
(59)
Then we have
∣∣Wt+ι∣∣2 = (1 - ηλ + α)2∣∣Wt∣∣i - 20(1 + α - ηλ)(Wt, Wt-ii + 02∣Wt-i∣∣2
+ ∣∣gt∣∣2η2 + 2(aWt-i, gt ηi
=(1 - ηλ + α)2∣∣Wt∣∣2 - 20(1 + α - ηλ)(Wt, Wt-Ii + 02∣Wt-i∣∣2
+ ∣∣gt∣∣2η2 + 2<0(Wt + ηvt-i), gtηi	(60)
=(1 - ηλ + α)2∣∣Wt∣∣2 - 20(1 + α - ηλ)(Wt, Wt-ii + 02∣∣Wt-i∣∣2
2
htη2
+ ∣W! .
〈Wt+1, Wti = (1 + α - λη)∣∣Wt∣∣2 - 0(Wt, Wt-ii,
Let Xt, A, e denote:
Xt
at
bt
Ct
∣∣Wt ∣∣22
(Wt, Wt-1 i
∣∣Wt-i∣∣22
(1 + a - λη)2
1 + a — λη
1
0),
0
—20(1 + 0 — λη)
—a
0
02
0
0
respectively. The Eq.(60), (61) is formulated as a iterative map:
Xt+1
AXt +
2
htη2
------e
eτ Xt
(61)
(62)
(63)
(64)
(65)
(66)
A
e
16
Under review as a conference paper at ICLR 2021
When λη < (1 - √ɑ)2, the eigen value of A are all real number:
λ1
λ
13
(1 + α — λη)2 + (1 + α — λη)，(1 + ɑ — λη)2 — 4α
--------------------------------------------α
2
α,
(1 + α - λη)2 - (1 + α - λη) p(1 + α — λη)2 — 4α
2
1 - 12=ηα + o(λ2η2), (67)
(68)
-α=02+(⅛λη+o(λ2η29)
and they satisfy
0 < λ3 < λ2 = α < λι < 1.	(70)
Therefore, A can be formulated as
S TAS = Λ,	(71)
where Λ is a diagonal matrix whose diagonal elements are the eigen value of A; the column vector
of S is the eigen vectors of A, note the fromulation of S, Λ are not unique. Specifically, we set
Λ, S as
(λι	0	0 ∖
Λ =	I 0 λ2	0 I ,
∖ 0	0	λ3 )
/	1	1
1 _	I 1+α-λn	1+α-λn
S =	I	α+λι	α+λ2
∖	λl	λ2
1
1+α-λn
ɑʌʌɜ
ʌɪ
(72)
(73)
Moreover, the inverse of S exists, and can be explicitly expressed as
S-1
/	(α+λι )λι
(λι-α)(λι-λ3)
2a2
—
(λ1-α)(α-λ3)
(α + λ3)λ3
∖	(α-λ3 )(λι-λ3)
2λι (α+λι )(α+λ3)
--:----:—:---:—:-----r-
(λ1-λ3)(λ1-α)(1+α-β)
2α(α+λ1)(α+λ3)
(λ1-α)(α-λ3)(1+ɑ-β)
2λ3(α+λ3)(α+λ1)
--:----:—:---:—:-----r-
(λ1-λ3)(α-λ3)(1+α-β)
λ1λ3(α+λ1)
(λ1-α)(λ1-λ3)
2αλ1λ3
(λ1-α)(α-λ3)
(α+λ3)λ1λ3
(α-λ3)(λLλ3)
(74)
let Yt = STXt, combining with Eq.(66), we have
Yt+1 = ΛYt +
(STte)τYt S-".
(75)
Combining with Eq.(73) and Eq.(74), and set Yt = (at, bt, Ct)t, we rewrite Eq.(75) as
at+1 二	T 9	/	. ʌ ∖ ʌ .ʌ	htη	(α + λ1)λ1	(Μ、 At + bt + Ct	(λ1 - α)(λ1 -尢)，
7 bt+1	二	一CC	C tη2	2a2	E At + bt + Ct (λ1 - α)(α - λ3)'
Ct+1	二	.λ .ɪ +	htη2	(α + 尢)尢	(78) at + bt + Ct (α - λ3)(λ1 -尢)∙
TL T . I I I I 9	~	T	~ TL T	,1 CIl ♦	♦	. ∙	F	. 1	. ∙ 1 ∙ 1
Note ∣∣wt∣∣2 = Ct + bt + Ct. Now we prove the following inequations by mathematical induction
bt	<	0,	(79)
Ct	>	0,	(80)
/	ʌ ∖7 (α - λ1)bt	>	(λ1 -13)Ct,	(81)
Ct + bt + Ct	>	0,	(82)
Ct+1 + bt+1 + c t+1,	>	2 λ1(at + bt + Ct) + τ	-	-,	(83)
		at + bt + Ct	
17
Under review as a conference paper at ICLR 2021
Since the start point Xi = (ɑi, ai, ai)T (ai > 0), the start point Yi = S-1X1. Combining with
Eq.(74), we have
〜
cb1
2α2λη
(λ1 - Q)(Q - λ3) 1,
(84)
ci
λ3(λ3 + α)(1 - α + λη)
(λ3 — α)(λ1 — λ3)(l + α
.~
1 - α - λη
—λη)v 1 — α + λη
一λ1)a1,
(85)
F < ∙ <	1	/	∖'7-∕∖ ʌ ∖ ~	1 ʌ ∙ 1	~,7,~	TCFz rΓ Fk	-	∕∖
by which we have (α - λ1)b1 > (λ1 - λ3)c1. Besides c1 + b1 + C1 = eτSH = eτX1 = α1 > 0.
Suppose for t = T, Eq. (79), (80), (81), (82) hold, combining with Eq.(77), (78), we can derive
bτ +1 < 0, aτ+1 > 0, so Eq.(79), (80) hold for t = T + 1; Besides, we have
(α — λ1)bτ +1 = α(α — λ1) bτ +
htη2
~	I	1.	I	≈
ατ + bτ + CT
> λ3(λ1 - λ3))cT +
htη2
2α2
(Q - λ3)
(Q + λ3)λ3
(86)
----H---- --:----
aτ + bτ + CT	(α —尢)
=(ʌi - λ3)Cτ +1,
thus Eq.(81) holds for t = T + 1. Sum Eq.(76), Eq.(77), Eq.(78), due to Eq.(81) we have
GT +1 + bτ +1 + ct +1 = λ∖dτ + αbτ + λ3cτ +
> λ1(cτ + bτ + CT) +
h tη2
CT + bτ + CT
2
ht η2
(87)
~ I 7 I ≈
ατ + bτ + CT
〜
Eq.(83) holds for t = T + 1, combining with the fact that CT + bτ + CT > 0, we have CT +1 +
bτ +1 + CT +1 > 0.
According to Lemma 2, we can estimate the lower bound of at + bt + Ct: when t > 1 +
log((√2-1)√ln∕(4λ))-log(∣∣∣wo∣∣2-√‰7^∣)
log(1-2λη)	,
〜IC「、 lη 〜，∕lη(1 - Q)
Ct + bt + ct ≥ 2 2(1 - λ1)〜V	4λ
(88)
Now we can analyze the expectation of distance(l2 norm) between Yt = (Ct, bt, Ct)T and the fixed
point Y * = (α*, b*, C*)τ which satisfies
A	~ . ^Γ . ~	⅛	~ ⅛
Assume Xt = at + bt + Ct, x* = α*
=λy* + (STe；TY* ST&
+ b* + c* > 0, then we have:
(89)
Then we have
匕+1- Y* = (A - XX*keτ)(匕-Y*) +
(Ct- L)η2 k
k
(90)
E[∣∣Yt+i - Y*I∣2∣K] = ll(Λ
where k = (k1,k2,k3)τ = S-1e. In the following context, we will omit the O(λ2η2) part since
λη《1. k1, k2,k3 can be approximated as
k1	=n 1、2 + O(e), (1 - Q)2	(91)
k2	2Q =-+ O(e),	(92)
k3	Q2 =E+ O(e).	(93)
-	LkeT)(Yt - Y*)∣∣2 + 也W≡∣∣k∣∣2 xtx	x	(94)
—
〜
〜
〜
〜
〜
Y *
Xt
18
Under review as a conference paper at ICLR 2021
一 一一 _	___ .	一	一.	T .
The fixed point	of Eq.(89) is	computed as	x = a +	b*	+ c*
known that if t > 1 +
have
log(1-2λη)
di。-。)?-德)'andwehave
xt > J1"；-。',, therefore we
Lη2
XtX*
E[(ht — L)2∣匕]4
-----X------η
xt
∕2L2
V τ ∙ 2λn,
4Vn3λ
l(1 — α).
According to assumption 3, we can prove
BL< —[可-二]，
combining with Lemma 3, we have
E[∣∣γt+1 — γ *∣∣2∣ 匕]< (1 — 3 )∣∣ 匕—γ *∣∣2 + 4Vη3λ∣∣ IfcF,
1 — α	l(1 — α)
which implies
E∣∣γt+1 — Y *∣∣2 < (1 —产)t-τ e∣∣Yt — Y *∣∣2 + V*；4。2；。4),
1 — α	l(1 —。)4
(95)
(96)
(97)
(98)
(99)
where T = [1 + log((√2T)"/(松(--2^，。||2-ʤ/(41)!)]. Therefore, similar to the proof of
theorem 1, ∃B > 0,
E∣∣Yt+L Y * ∣ ∣2 <(1 — !⅛)tB +
Vη2(1 + 4ɑ2 + α4)
l(1 -。)4
(100)
Recall 11 Wt ∣ ∣ 2 = eτYt, therefore
E[∣ ∣ Wt ∣ ∣ 2 — (w*)2]2 ≤ 3E 11 匕+1 - Y * 11 2 < 3(1 —
4λη t ŋ	3Vη2(1+4α2 + α4)
1 — α + l(1 —。)4
(101)
Remark 5. By Eq.(101), the variance of ∣ ∣ Wt ∣ ∣ 2 is bounded by 3Vη i,：+4。^+。), which is not small
enough. But we somehow can reduce it: according to Eq.(96), if Xt is close to its theoretical value,
then EKht-L) X] η4 < 4Vη 尹一。),hence variance of ∣ ∣ Wt ∣ ∣ 2 can be bounded by ^%)；：-。)+。).
B.4 PROOF OF THEOREM 3
Proof. In SGD case, we have
hwt+1, Wt = (1 — λη) ∣ ∣ Wt ∣ ∣ 2,
(102)
then we have
cos2 ∆t
hWt+i,Wti2	=(I _ 2λ ) ∣∣ Wt ∣∣ 2
∣∣ Wt ∣∣ 2 ∙∣∣ Wt+1 ∣∣ 2 =( —	η) k!^.
(103)
<
<
□
According to the definition of ∆t, ∆t ≥ 0, and ∆t is very close to 0, hence we have
1 — cos2 ∆t	(104)
∕1	∕1	由、2 ∣ ∣Wt ∣∣ 2 V1 —(I-λη) i^t+≡	(105)
J1-11-λη)2 B V	χt+ι	(106)
19
Under review as a conference paper at ICLR 2021
where χt, xt+i denotes ∖∖wt||2, ∣∣wt+ι||2 respectively as in Eq. (42). Assume t is sufficiently large
so that xt, xt+i are close to x* = ʌ/Ln, the first order of Taylor series expansion of Eq.(106) at
xt = xt+i = x* is
∆t=v2λη∣+(Iaλη)	∙	x*	∙[(Xt+ι	- * -	(Xt	- x*)].	(107)
Reorganizing Eq.(107), and applying Cauchy Inequality, we have	
|∆t -，办切1 =	-~~nJ，∙ (x*)∣ ∙ [(xt+i - x*) - (Xt - x*)]l	(108)
1	2λ	C	C ≤ 8λη ∙ Ln ∙ 2[(xt- X ) + (xt+i- X )]	(109)
= U l [(xt - x*)1 + (Xt+1 - x*)l] 2Lηl	(110)
Combining with Eq.(51) and Eq.(108), we have	
EAt- p2^]1 = H l [E(xt - x*)1 + E(Xt+1 - x*)l] ≤ (1 - 4λη)t ∙ C + --, 2Lηl	Ll	(111)
where C = UL∙ B, B is defined in Eq.(51).	
In SGDM case, the angular update can be computed by	
∆t = sin(∆t)	(112)
= v 1 - cosl ∆t	(113)
=h	hwt, Wt+iil =V - ||wt||l •∣∣wt+1|||	(114)
=s」,	(115)
where (at,bt,Ct) = (||wt|||, (wt, wt+i), ||wt+i|||). According to the proof of theorem 2, when t
is sufficiently large, (at, bt, Ct) will be close to the fixed point of Eq.(66), (a*, b*,c*), where a* =
C* =
at (at
(ʌ// Ln 入n、,b* = 1+α-λa*. Then the first order Taylor series expansion of Eq.(115)
Y A(1 —a)(2— ι+α)	1+α
= a* , bt = b* , Ct = C* ) is
∆t
I 2λη	√1 + α
V 1 + α + 2√2λη
λη )|rat a a*	2(bt — b*) + Ct — c*
1 + α	a*	b*	c*
(116)
(117)
Now substituting (at, bt, Ct) with (at, bt, ct) defined in Eq.(75, 76,77, 78), Eq.(107) is rewritten as
+ √√+α ∙ (1 — ¥)l3[(1-^(Ct — c*) + O(λη)]	(118)
2y2Λη	1 + Q a*	αl
一 一.r. 一,、. 一 一 一 _	______ _	„ ,,	一
where (c*, b*, c*) is the fixed point of Eq.(89). Omit O(λη), we have
∆t -广
段∙ (1 -
λη )4λ(1 - 0)(2 - 1⅛)[(1 - q)4
1 + α	Lη	α4
≤
(c - C*)1 ] (119)
(1 - al)(1 - a)4
4Lηlα4
||匕-Y*|||,
(120)
20
Under review as a conference paper at ICLR 2021
where Yt, Y * is defined in Eq.(75, 89) respectively. According to Eq.(100), the mean square error
E∣∆t — 12+ 12+ηα ∣2 can be bounded by
EA-不12 ≤ (I-力：—α)4EIM - Y*112	(121)
≤ (1 -	4λη 1-α	)tC+	(1 -	α2)(1 - α)4 V η2(1 + 4α2+ α4) • 4Lη2α4	1(1 — a)4	(122)
= (1 -	4λη 1-α	)tC+	V(1	—a2)(1 + 4α2 + α4) 4L1α4	,	(123)
□
(1-0⅜-α尸∙ B, B is defined in Eq.(101).
where C
B.5 Proof of Corollary 3.1
Proof of Corollary 3.1. In SGD case, and Eq.(42), we have
I∣wt+I∣l2 > (1 - 2λη)∣∣wtII2,	(124)
which means
I∣wt+TII2 > (1 - 2λη)TI∣wt∣l2.	(125)
On the other hand, We know that when η is divided by k, ||wt ||2 should be divided by √k to reach
the new equilibrium condition, therefore we have
llwt+T||2 — 1、/1	、T
IIwtII2 = √k >(1 - η) .	(I )
Since λη	1, log(1 - 2λη) ≈ -2λη, thus
T > log(k)
4λη .
(127)
In SGDM case, by Eq.(87), we have
Similar to SGD case, we have
"wt+1"2 >(I- ι2-ηɑ X*"2,
log(k)(1 - a)
4λη
(128)
(129)
□
21
Under review as a conference paper at ICLR 2021
C Experiments
C.1 Experiments on Sythetic Data
In this section we apply experiments on sythetic data to prove our claim in section 4: theorem 1
only requires expected square norm of unit gradient is locally steady, the expectations do not need
to remain unchanged across the whole training process. The proof of theorem implies square norm
of weight is determined by the following iterative map:
xt+ι = (I - 2λη)xt +—t~^~ι	(13O)
xt
where λ, η ∈ (0, 1), Lt denotes the square of unit gradient norm. Hence we simulate xt with
different type of {Lt}t∞=1. Results in Figure 4 shows as long as the local variance of square norm
of unit gradient is not too much, and expectation of Lt changes smoothly, weight norm can quickly
converge to its theoretical value base on expectation of square norm of unit gradient.
We also simulate SGDM case by following iteration map
Xt+1 = AXt + -L⅛ ∙ e,	(131)
Xt[0]
where A, Xt, e is defined as Eq.(62), (63), (64). Simulation results is shown in Figure 5.
C.2 Complementary in Multi-Stage Learning Rate Schedule
In this section we present complementary results in Multi-Stage Learning Rate Schedule experiment.
The plots of weight norm (empirical and predicted values) in multi-learning rate stage is shown in
Figure 6. We also present the test performance of resent50/maskrcnn on Imagenet/MSCOCO with
multi-stage learning rate schedule mentioned in Section 5.2. We only provide complementary results
for reference only. We do not intend to prove the advantages or disadvantages of rescaling
strategy here, it is beyond the discussion of this paper.
	Top 1 AccUracy(%)
Standard Rescale	76:25 76.27
Table 1: Performance of Resnet50 on Imagenet with multi-stage learning rate scheduler.
	AP bbox	bbox AP50	-AP bbox-	AP mask-	AP mask	AP mask
Standard	39.25	58.88	42.95	35.16^^	56.09	37.48
Rescale	38.38	57.98	42.31	34.49	55.27	36.71
Table 2: Performance of Resnet50 on Imagenet with multi-stage learning rate scheduler in Section.
C.3 Rethinking Linear Scaling Principle in Spherical Motion Dynamics
In this section, we will discuss the effect of Linear Scaling Principle (LSP) under the view of SMD.
Linear Scaling Principle is proposed by Goyal et al. (2017) to tune the learning rate η with batch
size B by η 8 B. The intuition of LSP is if weights do not change too much within k iterations,
then k iterations of SGD with learning rate η and minibatch size B (Eq.(132)) can be approximated
by a single iteration of SGD with learning rate kη and minibatch size kB (Eq.(133):
wt+k
wt+1
Wt-ηX(B X ∂L∣	+ λwt+j)，
j<k B x∈Bj ∂w wt+j ,x
Wt - kη(-1 XX dL∣	+ λwt).
kBj<kx∈Bj∂w∣wt,x
(132)
(133)
22
Under review as a conference paper at ICLR 2021
Goyal et al. (2017) shows that combining with gradual warmup, LSP can enlarge the batch size up
to 8192(256 × 32) without severe degradation on ImageNet experiments.
LSP has been proven extremely effective in a wide range of applications. However, from the perspec-
tive of SMD, the angular update mostly relies on the pre-defined hyper-parameters, and it is hardly
affected by batch size. To clarify the connection between LSP and SMD, we explore the learning
dynamics of DNN with different batch size by conducting extensive experiments with ResNet50
on ImageNet, the training settings rigorously follow Goyal et al. (2017): momentum coefficient is
α = 10-4; WD coefficient is λ = 10-4; Batch size is denoted by B; learning rate is initialized as
2B6 ∙ 0.1; Total training epoch is 90 epoch, and learning rate is divided by 10 at 30, 60, 80 epoch
respectively.
The results of experiments(Figure 7, 8) suggests that the assumption of LSP does not always hold
in practice because of three reasons: first, the approximate equivalence between a single iteration
in large batch setting, and multiple iterations in small batch setting can only hold in pure SGD
formulation, but momentum method is far more commonly used; Second, according Theorem 2,
the enlargement ratio of angular update is only determined by the increase factor of learning rate.
Figure 7 shows in practice, the accumulated angular update ](wt, wt+k) in small batch batch setting
is much larger than angular update ](wt, wt+1) of a single iteration in larger batch setting when
using Linear Scaling Principle; Third, even in pure SGD cases, the enlargement of angular update
still relies on the increase of learning rate, and has no obvious connection to the enlargement of
gradient’s norm when equilibrium condition is reached (see Figure 8).
In conclusion, though LSP usually works well in practical applications, SMD suggests we can find
more sophisticated and reasonable schemes to tune the learning rate when batch size increases.
C.4 Spherical Motion Dynamics with Different Network Structures
We also verify our theory on other commonly used network structures (MobileNet-V2 (Sandler
et al., 2018), ShuffleNet-V2+ (Ma et al., 2018)) with standard training settings. The results is shown
in Figure 9.
23
Under review as a conference paper at ICLR 2021
Figure 4: Simulation of SGD (Eq.(130)), η = 0.1, λ = 0.001, xo = 10, et 〜U(-3, 3). Orange
lines represent the square of unit gradient norm; blue solid lines represent simulated value of weight
norm square; black dashed lines represent theoretical value of weight norm square
24
Under review as a conference paper at ICLR 2021
Figure 5: Simulation ofSGDM (Eq.(131)), η = 0.1, λ = 0.001, xo = 10, Q 〜U(-3,3), α = 0.9.
Orange lines represent the square of unit gradient norm; blue solid lines represent simulated value
of weight norm square; black dashed lines represent theoretical value of weight norm square
25
Under review as a conference paper at ICLR 2021
22
(a) ||g||2 and h in standard settings	(b) ||g||2 and h in rescaled settings
(c) Weight norm in Imagenet with theoretical value
TΓ-<∙	X- I I ~ I I 9 T	F	C	∙	1 , l'	1	t zʌ	C .	T~1	, LC 1	11	Z ∖ /1 ∖	,
Figure 6: ||贝 ||2, ht and norm of weight from IayerL0.conv2 in ReSnet50 backbone. (a),(b) present
∣∣g∣∣2, h in multi-state learning rate schedule experiments discussed in Section 5.2. semitranspar-
ent line represents the raw value of ∣∣gt∣∣2 and ht, solid line represents the averaged value within
consecutive 200 iterations to estimate the expectations E∣∣gt∣∣2 and Eht. (c) presents the empirical
value of weight norm and its theoretical value in standard and rescaled cases. The theoretical value
is computed by estimated expectations Eht in (a), (b) respectively.
26
Under review as a conference paper at ICLR 2021
B = 256, 4(wt,IVtt4)
B=1024, A(wt,Wt + ι)
40	60	80
(a)	B = 256 versus B = 1024
(b)	B = 256 versus B = 4096
Figure 7: Angular update of weights from layer1.0.conv2 in ResNet50. The blue lines represent
the angular update of weights within a single iteration in when batch settings is B = 1024(4096);
The red lines represent the accumulated angular update within 4(16) iterations in smaller batch
setting(B = 256).
(a) ι∣g(1024)Il
(a) ||g(256)||
Figure 8: Enlargement ratio of gradients’ norm of weights from layer1.0.conv2 when batch size
increases . ||g(k) || represents the gradient’s norm computed using k samples(not average).
(b) l∣g(4096)ll
(U) l∣g(256)ll
27
Under review as a conference paper at ICLR 2021
(a) Angle Update of MobileNet-V2
(b) Angular Update of ShuffleNet-V2+
Figure 9: The angular update ∆t of MobileNet-V2 (Sandler et al., 2018) and ShuffleNet-V2+ (Ma
et al., 2018). The solid lines with different colors represent all scale-invariant weights from the
model; The dash black line represents the theoretical value of angular update, which is computed by
J 12λα. Learning rate η is initialized as 0.5, and divided by 10 at epoch 30, 60, 80 respectively; WD
coefficient λ is 4 × 10-5; Momentum parameter α is set as 0.9.
28