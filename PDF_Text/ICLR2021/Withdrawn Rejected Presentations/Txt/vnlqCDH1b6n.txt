Under review as a conference paper at ICLR 2021
Learning disentangled representations with
the Wasserstein Autoencoder
Anonymous authors
Paper under double-blind review
Ab stract
Disentangled representation learning has undoubtedly benefited from objective
function surgery. However, a delicate balancing act of tuning is still required
in order to trade off reconstruction fidelity versus disentanglement. Building on
previous successes of penalizing the total correlation in the latent variables, we
propose TCWAE (Total Correlation Wasserstein Autoencoder). Working in the
WAE paradigm naturally enables the separation of the total-correlation term, thus
providing disentanglement control over the learned representation, while offering
more flexibility in the choice of reconstruction cost. We propose two variants
using different KL estimators and perform extensive quantitative comparisons on
data sets with known generative factors, showing competitive results relative to
state-of-the-art techniques. We further study the trade off between disentanglement
and reconstruction on more-difficult data sets with unknown generative factors,
where the flexibility of the WAE paradigm in the reconstruction term improves
reconstructions.
1	Introduction
Learning representations of data is at the heart of deep learning; the ability to interpret those
representations empowers practitioners to improve the performance and robustness of their models
(Bengio et al., 2013; van Steenkiste et al., 2019). In the case where the data is underpinned by
independent latent generative factors, a good representation should encode information about the data
in a semantically meaningful manner with statistically independent latent variables encoding for each
factor. Bengio et al. (2013) define a disentangled representation as having the property that a change
in one dimension corresponds to a change in one factor of variation, while being relatively invariant to
changes in other factors. While many attempts to formalize this concept have been proposed (Higgins
et al., 2018; Eastwood & Williams, 2018; Do & Tran, 2019), finding a principled and reproducible
approach to assess disentanglement is still an open problem (Locatello et al., 2019).
Recent successful unsupervised learning methods have shown how simply modifying the ELBO
objective, either re-weighting the latent regularization terms or directly regularizing the statistical
dependencies in the latent, can be effective in learning disentangled representation. Higgins et al.
(2017) and Burgess et al. (2018) control the information bottleneck capacity of Variational Autoen-
coders (VAEs, (Kingma & Welling, 2014; Rezende et al., 2014)) by heavily penalizing the latent
regularization term. Chen et al. (2018) perform ELBO surgery to isolate the terms at the origin of
disentanglement in β-VAE, improving the reconstruction-disentanglement trade off. Esmaeili et al.
(2018) further improve the reconstruction capacity of β-TCVAE by introducing structural depen-
dencies both between groups of variables and between variables within each group. Alternatively,
directly regularizing the aggregated posterior to the prior with density-free divergences (Zhao et al.,
2019) or moments matching (Kumar et al., 2018), or simply penalizing a high Total Correlation (TC,
(Watanabe, 1960)) in the latent (Kim & Mnih, 2018) has shown good disentanglement performances.
In fact, information theory has been a fertile ground to tackle representation learning. Achille & Soatto
(2018) re-interpret VAEs from an Information Bottleneck view (Tishby et al., 1999), re-phrasing it
as a trade off between sufficiency and minimality of the representation, regularizing a pseudo TC
between the aggregated posterior and the true conditional posterior. Similarly, Gao et al. (2019) use
the principle of total Correlation Explanation (CorEX) (Ver Steeg & Galstyan, 2014) and maximize
the mutual information between the observation and a subset of anchor latent points. Maximizing the
1
Under review as a conference paper at ICLR 2021
mutual information (MI) between the observation and the latent has been broadly used (van den Oord
et al., 2018; Hjelm et al., 2019; Bachman et al., 2019; Tschannen et al., 2020), showing encouraging
results in representation learning. However, Tschannen et al. (2020) argued that MI maximization
alone cannot explain the disentanglement performances of these methods.
Building on the Optimal Transport (OT) problem (Villani, 2008), Tolstikhin et al. (2018) introduced
the Wasserstein Autoencoder (WAE), an alternative to VAE for learning generative models. Similarly
to VAE, WAE maps the data into a (low-dimensional) latent space while regularizing the averaged
encoding distribution. This is in contrast with VAEs where the posterior is regularized at each data
point, and allows the encoding distribution to capture significant information about the data while still
matching the prior when averaged over the whole data set. Interestingly, by directly regularizing the
aggregated posterior, WAE hints at more explicit control on the way the information is encoded, and
thus better disentanglement. The reconstruction term of the WAE allows for any cost function on the
observation space, opening the door to better suited reconstruction terms, for example when working
with continuous RGB data sets where the Euclidean distance or any metric on the observation space
can result in more accurate reconstructions of the data.
In this work, following the success of regularizing the TC in disentanglement, we propose to use the
Kullback-Leibler (KL) divergence as the latent regularization function in the WAE. We introduce
the Total Correlation WAE (TCWAE) with an explicit dependency on the TC of the aggregated
posterior. Using two different estimators for the KL terms, we perform extensive comparison with
succesful methods on a number of data sets. Our results show that TCWAEs achieve competitive
disentanglement performances while improving modelling performance by allowing flexibility in the
choice of reconstruction cost.
2	Importance of Total correlation in disentanglement
2.1	Total correlation
The TC of a random vector Z ∈ Z under P is defined by
dZ
TC(Z),XHpd(Zd)-Hp(Z)	(1)
d=1
where pd(zd) is the marginal density over only zd and Hp(Z) , -Ep logp(Z) is the Shannon
differential entropy, which encodes the information contained in Z under P . Since
dZ
X Hpd (Zd) ≤ Hp(Z)	(2)
d=1
with equality when the marginals Zd are mutually independent, the TC can be interpreted as the
loss of information when assuming mutual independence of the Zd ; namely, it measures the mutual
dependence of the marginals. Thus, in the context of disentanglement learning, we seek a low TC of
the aggregated posterior, p(z) = X p(z|x) p(x) dx, which forces the model to encode the data into
statistically independent latent codes. High MI between the data and the latent is then obtained when
the posterior, p(z|x), manages to capture relevant information from the data.
2.2 Total correlation in ELBO
We consider latent generative models pθ(x) = JZpθ(x|z) P(Z) dz with prior P(Z) and decoder net-
work, pθ(x∣z), parametrized by θ. VAEs approximate the intractable posterior p(z∣x) by introducing
an encoding distribution (the encoder), qφ(z∣χ), and learning simultaneously θ and φ when optimizing
the variational lower bound, or ELBO, defined in Eq. 3:
LELBO(θ,φ) ,pdEχL(EXJogPθ(X|Z)] - KL(qφ(z∣x) k P(Z))] ≤pClEXJogpθ(X)
ata	φ	ata	(3)
2
Under review as a conference paper at ICLR 2021
Following Hoffman & Johnson (2016), we can decompose the KL term in Eq. 3 as:
1
Nbatch
N
X KL(qφ(z|xn) k P(Z))
n=1
KL q(Z, N) k q(Z)p(N) +KL q(Z)
(4)
(ɪ) index-code MI
(ii) marginal KL
Where p(n) = N, q(z∣n) = q(z∣Xn), q(z,n) = q(z∣n)p(n) and q(z) = PN=I q(z∣n)p(n).①
refers to the index-code mutual information and represents the MI between the data and the latent
under the join distribution q(z, n), and ⑥ to the marginal KL matching the aggregated posterior to
the prior. While discussion on the impact of a high index-code MI on disentanglement learning is
still open, the marginal KL term plays an important role in disentanglement. Indeed, it pushes the
encoder netWork to match the prior When averaged, as opposed to matching the prior for each data
point. Combined With a factorized prior p(z) = Qdpd(zd), as it is often the case, the aggregated
posterior is forced to factorize and align With the axis of the prior. More specifically, the marginal KL
term in Eq. 4 can be decomposed the as sum of a TC term and a dimensionWise-KL term:
KL	q(Z)	k	p(Z)	=TC	q(Z)	+XKL	qd(Zd)	k pd(Zd)	(5)
d=1
Thus maximizing the ELBO implicitly minimizes the TC of the aggregated posterior, enforcing the
aggregated posterior to disentangle as Higgins et al. (2017) and Burgess et al. (2018) observed When
strongly penalizing the KL term in Eq. 3. Chen et al. (2018) leverage the KL decomposition in Eq. 5
by refining the heavy latent penalization to the TC only. HoWever, the index-code MI term in Eq. 4
seems to have little to no role in disentanglement (see ablation study of Chen et al. (2018)), potentially
arming the reconstruction performances (Hoffman & Johnson, 2016).
3 WAE naturally good at disentangling
In this section We introduce the OT problem and the WAE objective, and discuss the compelling
properties of WAEs for representation learning. Mirroring β-TCVAE decomposition, We derive the
TCWAE objective.
3.1 WAE
The Kantorovich formulation of the OT betWeen the true-but-unknoWn data distribution PD and the
model distribution Pθ, for a given cost function c, is defined by:
OTc(PD,Pθ)
inf C	c(x,X) Y(x,X) dxdX
Γ∈P(PD,Pθ) X×X
(6)
Where P(PD, Pθ) is the space of all couplings of PD and Pθ; namely, the space of joint distributions
Γ on X × X Whose densities γ have marginals pD andpθ. Tolstikhin et al. (2018) derive the WAE
objective by restraining this space and relaxing the hard constraint on the marginal using a soft
constraint With a Lagrange multiplier (see Appendix A for more details):
Wd,c(Θ,Φ) , E E E c(x,x) + λD(q(Z) k P(Z))	⑺
PD (χ)qφ (z∣x)pθ(x∣z)	'	)
where D is any divergence function and λ a relaxation parameter. The decoder, pθ(x∣z), and the
encoder, qφ(z∣x), are optimized simultaneously by dropping the closed-form minimization over the
encoder network, with standard stochastic gradient descent methods.
Similarly to the ELBO, objective 7 consists of a reconstruction cost term and a latent regularization
term, preventing the latent codes to drift away from the prior. However, WAE explicitly penalizes
the aggregate posterior. This motivates, following Section 2.2, the use of WAE in disentanglement
learning. Rubenstein et al. (2018) have shown promising disentanglement performances without
modifying the objective 7. Another important difference lies in the functional form of the reconstruc-
tion cost in the reconstruction term. Indeed, WAE allows for more flexibility in the reconstruction
term with any cost function allowed, and in particular, it allows for cost functions better suited to
the data at hand and for the use of deterministic decoder networks (Tolstikhin et al., 2018; Frogner
et al., 2015). This can potentially result in an improved reconstruction-disentanglement trade off as
we empirically find in Sections 4.2 and 4.1.
3
Under review as a conference paper at ICLR 2021
3.2 TCWAE
In this section, for notation simplicity, we drop the explicit dependency of the distributions to their
respective parameters.
Following Section 2.2 and Eq. 5, we chose the divergence function, D, in Eq. 7, to be the KL diver-
gence and assume a factorized prior (e.g. p(z) = N (0dZ , IdZ)), obtaining the same decomposition
than in Eq. 5. Re-weighting each term in Eq. 5 with hyper-parameters β and γ, and plugging into
Eq. 7, we obtain our TCWAE objective:
,E E E
P(Xn)q(z∣Xn ) Lp(XnIZ)
WTC
dZ	dZ
c(xn,Xn)] + βKL (q(Z) k Y qd(Zd)) + Y X KLed(Zd) k Pd(Zd))
d=1	d=1
(8)
Given the positivity of the KL divergence, the TCWAE in Eq. 8 is an upper-bound of the WAE
objective of Eq. 7 with λ = min(β, γ).
Eq. 8 can be directly related to the β-TCVAE objective of Chen et al. (2018):
EE
p(xn)q(z|xn)
dZ
- logp(xn |Z) +βKL q(Z) k Y qd(Zd)
d=1
+αIq q(Z,N);q(Z)p(N)
dZ
+ γ X KL qd(Zd) k pd(Zd)
d=1
(9)
As already mentioned, the main differences are the absence of index-code MI and a different
reconstruction cost function. Setting α = 0 in Eq. 9 makes the two latent regularizations match but
breaks the inequality in Eq. 3. Matching the two reconstruction terms would be possible if we could
find a ground cost function C such that Ep(χn∣z)c(xn, Xn) = - logρ(xn∣Z).
3.3 Estimators
While being grounded and motivated by information theory and earlier works on disentanglement,
using the KL as the latent divergence function, as opposed to other sampled-based divergences
(Tolstikhin et al., 2018; Patrini et al., 2018), presents its own challenges. Indeed, the KL terms are
intractable, and especially, we need estimators to approximate the entropy terms. We propose to use
two estimators, one based on importance weight-sampling Chen et al. (2018), the other on adversarial
estimation using the denisty-ratio trick (Kim & Mnih, 2018).
TCWAE-MWS
Chen et al. (2018) propose to estimate the intractable terms Eq log q(Z) and Eqd log qd(Z) in the
KL terms of Eq. 8 with Minibatch-Weighted Sampling (MWS). Considering a batch of observation
{xι,.. .χNbαtch}, they sample the latent codes Zi 〜q(z∣Xi) and compute:
1	Nbatch	1	Nbatch
q(Z)log q(Z) ≈ Natch Xlog N×Natch X q(zilxj)
i=1	j=1
(10)
This estimator, while being easily computed from samples, is a biased estimator of Eq log q(Z).
Chen et al. (2018) also proposed an unbiased version, the Minibatch-Stratified Sampling (MSS).
However, they found that it did not result in improved performances, and thus, as Chen et al. (2018),
we chose to use the simpler MWS estimator. We call the resulting algorithm the TCWAE-MWS.
Other sampled-based estimators of the entropy or the KL divergence have been proposed (Rubenstein
et al., 2019; Esmaeili et al., 2018). However, we choose the solution of Chen et al. (2018) for 1) its
simplicity and 2) the similarities between the TCWAE and β-TCVAE objectives.
TCWAE-GAN
A different approach, similar in spirit to the WAE-GAN originally proposed by Tolstikhin et al.
(2018), is based on adversarial-training. While Tolstikhin et al. (2018) use the adversarial training
to approximate the JS divergence, Kim & Mnih (2018) use the density-ratio trick and adversarial
4
Under review as a conference paper at ICLR 2021
(a) Rec.	(b) Latent reg.
Figure 1: Reconstruction and latent regularization terms as functions of β for the NoisydSprites data
set. (a): reconstruction error. (b): latent regularization term (MMD for WAE, KL for TCWAE). (c):
reconstruction error against latent regularization. (d): reconstruction error against MMD. Shaded
regions show ± one standard deviation.
(c) Rec. vs latent reg.
(d) Rec. vs MMD
training to estimate the intractable terms in Eq. 8. The the density-ratio trick (Nguyen et al., 2008;
Sugiyama et al., 2011) estimates the KL divergence as:
KL(q(Z) k γZ qd (Zd)) ≈ q(Z) log I-D)Z)	(II)
where D plays the same role than the discriminator in GANs and ouputs an estimate of the probability
that Z is sampled from q(Z) and not from QddZ=1 qd(Zd). Given that we can easily sample from q(Z),
we can use Monte-Carlo sampling to estimate the expectation in Eq. 11. The discriminator Dis
adversarially trained alongside the decoder and encoder networks. We call this adversarial version
the TCWAE-GAN.
4	Experiments
We perform a series of quantitative and qualitative experiments, starting with an ablation study on
the impact of using different latent regularization functions in WAEs followed by a quantitative
comparison of the disentanglement performances of our methods with existing ones on toy data sets
before moving to qualitative assessment of our method on more challenging data sets. Details of
the data sets, the experimental setup as well as the networks architectures are given in Appendix B.
In all the experiments we fix the ground-cost function of the WAE-based methods to be the square
Euclidean distance: c(x, y) = kx - yk2L .
4.1	Quantitative analysis: disentanglement on toy data sets
Ablation study of the latent divergence function We compare the impact of the different latent
regularization functions in WAE-MMD (Tolstikhin et al., 2018), TCWAE-MWS and TCWAE-GAN.
We take β = γ in the TCWAE objectives isolating the impact of the different latent divergence
functions used in the TCWAE and the original WAE. We train the methods with β ∈ {1, 2, 4, 6, 8, 10},
and report the results Figure 1 in the case of the NoisydSprites data set (Locatello et al., 2019). As
expected, the higher the penalization on the latent regularization (high β), the poorer the recon-
structions. We can see that the trade off between reconstruction and latent regularization is more
sensible for TCWAE-GAN, where a relatively modest improvement in latent regularization results in
an important deterioration of reconstruction performances while TCWAE-MWS is less sensible. This
is better illustrated in Figure 1c with a much higher slope for TCWAE-GAN than for TCWAE-MWS.
WAE seems to be relatively little impacted by the latent penalization weight. We note in Figure1b
the bias of the MWS estimator (Chen et al., 2018). Finally, we plot the reconstruction versus the
MMD between the aggregated posterior and the prior for all the models in Figure (1d). Interestingly,
TCWAEs actually achieved a lower MMD (left part of the plot) even if they are not being trained
with that regularization function. However, as expected given that the TCWAE do not optimized the
reconstruction-MMD trade off, the WAE achieved a better reconstruction (bottom part of the plot).
5
Under review as a conference paper at ICLR 2021
Table 1: Reconstruction and disentanglement scores (± one standard deviation).
Method	MSE	MIG	factorVAE	SAP
TCWAE MWS (β = 6)	34.95 ± 0.90	0.323 ± 0.04	0.77 ± 0.01	0.072 ± 0.004
TCWAE GAN (β = 10)	11.39 ± 0.28	0.181 ± 0.01	0.76 ± 0.03	0.074 ± 0.003
β-TCVAE (β = 6)	14.30 ± 2.43	0.235 ± 0.03	0.81 ± 0.03	0.070 ± 0.006
FactorVAE (γ = 10)	8.17 ± 0.86	0.24 ± 0.06	0.78 ± 0.03	0.077 ± 0.011
(a) dSprites
Method	MSE	MIG	factorVAE	SAP
WAE (λ = 2)	982.51 ± .20	0.019 ± .00	0.40 ± .09	0.011 ± .005
TCWAE MWS (β = 2)	998.17 ± 3.82	0.118 ± .08	0.57 ± .07	0.011 ± .005
TCWAE GAN (β = 4)	986.77 ± .48	0.055 ± .03	0.58 ± .04	0.017 ± .005
β-TCVAE (β = 8)	998.67 ± 3.71	0.101 ± .06	0.53 ± .11	0.015 ± .007
FactorVAE (γ = 25)	988.10 ± .81	0.066 ± .03	0.52 ± .07	0.019 ± .008
(b) NoisydSprites
Method	MSE	MIG	factorVAE	SAP
WAE (λ = 6)	24.40 ± .43	0.014 ± .01	0.41 ± .04	0.010 ± .004
TCWAE MWS (β = 2)	39.53 ± .24	0.322 ± .00	0.73 ± .01	0.067 ± .001
TCWAE GAN (β = 8)	33.57 ± .57	0.158 ± .02	0.67 ± .04	0.039 ± .009
β-TCVAE (β = 6)	43.64 ± .28	0.261 ± .11	0.67 ± .14	0.053 ± .020
FactorVAE (γ = 25)	33.23 ± .53	0.256 ± .07	0.69 ± .09	0.066 ± .013
(c) ScreamdSprites
Method	MSE	MIG	factorVAE	SAP
WAE (λ = 2)	3.85 ± .0.03	0.010 ± .000	0.38 ± .02	0.008 ± .004
TCWAE MWS (β = 2)	11.48 ± .26	0.029 ± .003	0.44 ± .03	0.017 ± .002
TCWAE GAN (β = 2)	6.87 ± .10	0.030 ± .007	0.46 ± .02	0.015 ± .001
β-TCVAE (β = 4)	10.34 ± .06	0.030 ± .001	0.46 ± .02	0.016 ± .001
FactorVAE (γ = 100)	8.60 ± .15	0.038 ± .00	0.47 ± .02	0.015 ± .003
(d) smallNORB
Disentanglement performances We compare our methods with β-TCVAE (Chen et al., 2018),
FactorVAE (Kim & Mnih, 2018) and the original WAE-MMD (Tolstikhin et al., 2018) on the dSprites
(Matthey et al., 2017), NoisydSprites (Locatello et al., 2019), ScreamdSprites (Locatello et al., 2019)
and smallNORB (LeCun et al., 2004) data sets whose ground-truth generative-factors are known
and given in Table 3, Appendix B.1. We use three different disentanglement metrics to assess
the disentanglement performances: the Mutual Information Gap (MIG, Chen et al. (2018)), the
factorVAE metric (Kim & Mnih, 2018) and the Separated Attribute Predictability score (SAP, Kumar
et al. (2018)). We follow Locatello et al. (2019) for the implementation of these metrics. We use the
Mean Square Error (MSE) of the reconstructions to assess the reconstruction performances of the
methods. For each model, we use 6 different values for each parameter, resulting in thirty-six different
models for TCWAEs, and six for the remaining methods (see Appendix B.1 for more details).
Mirroring the benchmark methods, we first tune γ in the TCWAEs, regularizing the dimensionwise-
KL, subsequently focusing on the role of the TC term in the disentanglement performances. The
heat maps of the different scores for each method and data set are given Figures 5, 6, 7 and 8 in
Appendix C. As expected, while β controls the trade off between reconstruction and disentanglement,
γ affects the range achievable when tuning β. Especially, for γ > 1, we can see Figures5,6, 7 and 8
that better disentanglement is obtained without much deterioration in reconstruction.
Table 1 reports the results, averaged over 5 random runs, for the four different data sets. For each
method, we report the best β taken to be the one achieving an overall best ranking on the four different
metrics (see Appendix ?? for details). Note that the performances of WAE on the dSprites data set,
both in term of reconstruction and disentanglement where significantly worse and meaningless, thus,
in order to avoid unfair extra tuning of the parameters, we chose not to include them. TCWAEs achieve
competitive performances across all the data sets, with top scores in several metrics. Especially, the
square Euclidean distance seems to improve the trade off and perform better than the cross-entropy
with color images (NoisydSprites, ScreamdSprites) but less so with black and white images (dSprites).
See Appendix C for more results on the different data sets.
6
Under review as a conference paper at ICLR 2021
As a sanity check, we plot Figure 2 the latent traversals of the different methods on the smallNORB
data set. More specifically, we encode one observation and traverse the latent dimensions one at
the time (rows) and reconstruct the resulting latent traversals (columns). Visually, all methods,
with the exception of WAE, learn to disentangle, capturing four different factors in line with the
ground-truth generative factors. Models reconstructions and samples for the different data sets are
given in Appendix C.
WAE
TCWAE-MWS TCWAE-GAN	β-TCVAE
FactorVAE
Figure 2: Latent traversals for each model on smallNORB. The parameters are the same than
the ones reported in Tables 1 and 7. Each row i corresponds to the traversal of the latent zi
while the columns correspond to a step in the that traversal. The rows are order by increasing
KL 1/Ntest Ptestset q(zi|x) k p(zi) and the traversal range is [-2, 2].
Finally, we visualise the reconstruction-disentanglement trade off by plotting the different disen-
tanglement metrics against the MSE in Figure 3. As expected, when the TC regularization weight
is increased, the reconstruction deteriorates while the disentanglement improves up to a certain
point. Then, when too much penalization is put on the TC term, the poor quality of the recon-
structions prevents any disentanglement in the generative factors. Reflecting the results of Table 1,
TCWAE-MWS seems to perform better (top-left corner represents better reconstruction and dis-
entanglement). TCWAE-GAN presents better reconstruction but slightly lower disentanglement
performances (bottom left corner).
Figure 3: Disentanglement versus reconstruction on the ScreamdSprites data set. Annotations at each
point are values of β . Points with low reconstruction error and high scores (top-left corner) represent
better models.
4.2	Qualitative analysis: disentanglement on real-world data sets
We train our methods on 3Dchairs (Aubry et al., 2014) and CelebA (Liu et al., 2015) whose generative
factors are not known and qualitatively find that TCWAEs achieve good disentanglement. Figure 4
shows the latent traversals of four different factors learned by the TCWAEs, while Figures 16 and
18 in Appendix D show the models reconstructions and samples. Visually, TCWAEs manage to
capture different generative factors while retaining good reconstructions and samples. This confirms
our intuition that the flexibility offered in the construction of the reconstruction term, mainly the
possibility to chose the reconstruction cost function and use deterministic decoders, improves the
reconstruction-disentanglement trade off. In order to assess the quality of the reconstructions, we
compute the MSE of the reconstructions and the FID scores (Heusel et al., 2017) of the reconstructions
and samples. Results are reported in Table 2. TCWAEs indeed beat their VAEs counterparts in both
data sets. It is worth noting that, while the performances of FactorVAE in Table 2 seem good, the
7
Under review as a conference paper at ICLR 2021
SAVW—HVMDI
Size
LegS type
> > > > ⅛ ⅛
肃疝a a M M
Orientation
Back rest size
NVqHVMDI
tΓ
M
Glasses/Beard	Smile
Hue
32
lɔp
翳加miiMyTiWi
SMn 由 VMɔl NVqHVMDI
nl∙l5 号
sb∙,ss/
Gender

Figure 4: Latent traverSalS for TCWAE-MWS and TCWAE-GAN. Each line correSpondS to one input
data point while each Subplot correSpondS to one latent factor. We vary evenly the encoded latent
codeS in the interval [-4, 4].
Table 2: MSE and FID ScoreS for the different data SetS. DetailS of the methodology iS given in
Appendix B
B

Method	3D chairS			CelebA		
	MSE	Rec.	SampleS	MSE	Rec.	SampleS
TCWAE-MWS	45.8 ± 4.72	1.227	1.821	147.5 ± 33.58	1.204	1.264
TCWAE-GAN	29.8 ± 3.46	0.518	0.362	129.8 ± 34.45	1.003	0.975
β-TCVAE	43.0 ± 4.85	1.346	1.845	180.8 ± 51.1	1.360	1.411
FactorVAE	42.1 ± 7.58	0.895	0.684	201.4 ± 51.84	1.017	0.982
inSpection of the reconStructionS and SampleS in Appendix D ShowS that FactorVAE in fact Struggle
to generalize and to learn a Smooth latent manifold.
5	Conclusion
Leveraging the Surgery of the KL regularization term of the ELBO objective, we deSign a new
diSentanglement method baSed on the WAE objective whoSe latent divergence function iS taken to be
the KL divergence between the aggregated poSterior and the prior. The WAE framework naturally
enableS the latent regularization to depend explicitly on the TC of the aggregated poSterior, quantity
previouSly aSSociated with diSentanglement. USing two different eStimatorS of the KL termS, we Show
that our methodS achieve competitive diSentanglement on toy data SetS. Moreover, the flexibility in
the choice of the reconStruction coSt function offered by the WAE framework makeS our method
more compelling when working with more challenging data SetS.
8
Under review as a conference paper at ICLR 2021
References
A.	Achille and S. Soatto. Information dropout: Learning optimal representations through noisy
computation. In IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.
M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3D chairs: exemplar part-based
2D-3D alignment using a large dataset of CAD models. In CVPR, 2014.
P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual
information across views. In Advances in Neural Information Processing Systems, 2019.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. In
IEEE Transactions on Pattern Analysis and Machine Intelligence, 2013.
C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Under-
standing disentangling in β-VAE. arXiv:804.03599, 2018.
R. T. K. Chen, X. Li, R. Grosse, and D. Duvenaud. Isolating sources of disentanglement in VAEs. In
Advances in Neural Information Processing Systems, 2018.
K. Do and T. Tran. Theory and evaluation metrics for learning disentangled representations.
arXiv:1908.09961, 2019.
C. Eastwood and C. K. I. Williams. A framework for the quantitative evaluation of disentangled
representations. In International Conference on Learning Representations, 2018.
B.	Esmaeili, H. B. Wu, S. Jain, A. Bozkurt, N. Siddharth, B. Paige, D. H. Brooks, J. Dy, and J.-W.
van de Meent. Structured disentangled representations. In AISTATS, 2018.
C.	Frogner, C. Zhang, H. Mobahi, M. Araya, and T. A. Poggio. Learning with a Wasserstein loss. In
Advances in Neural Information Processing Systems, 2015.
S. Gao, R. Brekelmans, G. Ver Steeg, and A. Galstyan. Auto-encoding total correlation explanation.
In International Conference on Artificial Intelligence and Statistics, 2019.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. GANs trained by a two
time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information
Processing Systems, 2017.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed, and A. Ler-
chner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In
International Conference on Learning Representations, 2017.
I. Higgins, D. Amos, D. Pfau, S. Racaniere, L. Matthey, D. J. Rezende, and A. Lerchner. Towards a
definition of disentangled representations. arXiv:1812.02230, 2018.
R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, and Y. Bengio.
Learning deep representations by mutual information estimation and maximization. In International
Conference on Learning Representations, 2019.
M. D. Hoffman and M. J. Johnson. ELBO surgery: yet another way to carve up the variational
evidence lower bound. In NIPS Workshop on Advances in Approximate Bayesian Inference, 2016.
H. Kim and A. Mnih. Disentangling by factorising. In International Conference on Machine Learning,
2018.
D. P. Kingma and J. Ba. Adam: a method for stochastic optimization. In International Conference on
Learning Representations, 2015.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on
Learning Representations, 2014.
A. Kumar, P Sattigeri, and A Balakrishnan. Variational inference of disentangled latent concepts
from unlabeled observations. In International Conference on Learning Representations, 2018.
9
Under review as a conference paper at ICLR 2021
Y. LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance
to pose and lighting. In IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, 2004.
Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In International
Conference on Computer Vision, 2015.
F.	Locatello, S. Bauer, M. Lucic, G Raetsch, S. Gelly, B. Scholkopf, and O. Bachem. Challenging
common assumptions in the unsupervised learning of disentangled representations. In International
Conference on Machine Learning, 2019.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement
testing Sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
X. Nguyen, M. J. Wainwright, and I. J. Michael. Estimating divergence functionals and the likelihood
ratio by penalized convex risk minimization. In Advances in Neural Information Processing
Systems 20, 2008.
G.	Patrini, M. Carioni, P Forra S. Bhargav, M. Welling, R. Van Den Berg, T. GeneWein, and
F. Nielsen. Sinkhorn autoencoders. arXiv:1810.01118, 2018.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference
in deep generative models. In International Conference on Machine Learning, 2014.
P. Rubenstein, O. Bousquet, J. Djolonga, C. Riquelme, and I. Tolstikhin. Practical and consistent
estimation of f-divergences. In Advances in Neural Information Processing Systems, 2019.
P. K. Rubenstein, B. Schoelkopf, and I. Tolstikhin. Learning disentangled representations With
Wasserstein Auto-Encoders. In ICLR Workshop, 2018.
M.	Sugiyama, T. Suzuki, and T. Kanamori. Density ratio matching under the Bregman divergence: A
unified frameWork of density ratio estimation. In Annals of the Institute of Statistical Mathematics,
2011.
N.	Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. In Annual Allerton
Conference on Communication, Control and Computing, 1999.
I. Tolstikhin, O. Bousquet, S. Gelly, and B. Schoelkopf. Wasserstein Auto-Encoders. In International
Conference on Learning Representations, 2018.
M. Tschannen, J. Djolonga, P. K. Rubenstein, S. Gelly, and M. Lucic. On mutual information
maximization for representation learning. In International Conference on Learning Representations,
2020.
A. van den Oord, Y. Li, and O. Vinyals. Representation learning With contrastive predictive coding.
arXiv:1807.03748, 2018.
S. van Steenkiste, F. Locatello, J. Schmidhuber, and O. Bachem. Are disentangled representations
helpful for abstract visual reasoning? In Advances in Neural Information Processing Systems,
2019.
G. Ver Steeg and A. Galstyan. Discovering structure in high-dimensional data through correlation
explanation. In Advances in Neural Information Processing Systems, 2014.
C. Villani. Optimal Transport: Old and New. Springer Berlin Heidelberg, 2008.
S. Watanabe. Information theoretical analysis of multivariate correlation. In IBM Journal of Research
and Development, 1960.
S. Zhao, J. Song, and S. Ermon. InfoVAE: Balancing learning and inference in variational autoen-
coders. In AAAI Conference on Artificial Intelligence, 2019.
10
Under review as a conference paper at ICLR 2021
A WAE derivation
We recall the Kantorovich formulation of the OT between the true-but-unknown data distribution PD
and the model distribution Pθ, with given cost function c:
OTc(PD , Pθ)
inf C	c(x,X) γ(x,X) dxdX
Γ∈P(PD,Pθ) X×X
(12)
where P(PD, Pθ) is the space of all couplings of PD and Pθ:
P(PD,Pθ)
∣Γ J /Y(x,X) dx = PD(X), J γ(x,X) dx = pθ(x)
(13)
Tolstikhin et al. (2018) first restrain the space of couplings to the joint distributions of the form:
γ(χ, X) = I pθ(X|z) q(z∣χ)PD(χ) dz
Z
(14)
where q(z|x), for x ∈ X, plays the same role as the variational distribution in variational inference.
While the marginal constraint on x (first constraint in Eq. 13) in Eq. 14 is satisfied by construction,
the second marginal constraint (that over x giving Pθ in in Eq. 13) is not guaranteed. A sufficient
condition is to have for all z ∈ Z :
X
X
q(z|x) PD (x) dx = P(z)
(15)
Secondly, Tolstikhin et al. (2018) relax the constraint in Eq. 15 using a soft constraint with a Lagrange
multiplier:
—
Wcc(PD,Pθ)
inf
q(Z|X)
I	c(x, X) γ(x, X) dx dx + λ D(q(Z) ∣∣ P(Z))
X×X
(16)
where D is any divergence function, λ a relaxation parameter, γ is defined in Eq. 14 and q(Z) is the
aggregated posterior as define in Section 2. Finally, they drop the closed-form minimization over the
variational distribution q(z|x), to obtain the WAE objective, as defined in Section 3.1:
Wd,c(θ,φ)，EEE c(x,x) + λ D (q(Z) ∣∣ P(Z))
'	PD(X)qφ(z∣x)pθ (X|z)	、	)
≈ EE E c(x,xn) + λ D (q(Z) k p(Z)]
p(Xn)qφ(z∣Xn)Pθ (Xn∣z)	'	3
(17)
B Implementation details
B.1 Experimental setup
We train and compare our methods on four different data sets, two with known ground-truth generative
factors (see Table 3): dSprites (Matthey et al., 2017) with 737,280 binary, 64 × 64 images and
smallNORB (LeCun et al., 2004) with 48,600 greyscale, 64 × 64 images; and two with unknown
ground-truth generative factors: 3Dchairs (Aubry et al., 2014) with 86,366 RGB, 64 × 64 images and
CelebA (Liu et al., 2015) with 202,599 RGB 64 × 64 images.
Table 3: Ground-truth generative-factors of the dSprites and smallNORB data sets.
data set	Generative factors (number of different values)
dSprites and variations Shape (3), Orientation (40), Position X (32), Position Y (32)
smallNORB	categories (5), lightings (6), elevations (9), azimuths (18)
We use a batch size of 64 in Section 4.2, while in the main experiments of Section 4.1, we take
a batch size of 100. In the ablation study of Section 4.1, we use a bigger batch size of 256 in
order to reduce the impact of the bias of the MWS estimator (Chen et al. (2018) however show that
11
Under review as a conference paper at ICLR 2021
Table 4: Hyper parameters values ranges used in the different Sections.
Method	Section 4.2	Section 4.1
TCWAE-MWS	-{1, 2,4, 6, 8,10}2―	{1, 2, 5, 10, 15, 20}2
TCWAE-GAN	{1,2,4,6,8,10}2	{1, 2, 5, 10, 20, 50}2
β-TCVAE	{1,2,4,6,8,10}	{1, 2, 5, 10, 15, 20}
FactorVAE	{1, 10, 25, 50, 75, 100}	{1, 2, 5, 10, 20, 50}
there is very little impact on the performance of the MWS when using smaller batch size). For all
experiments, we use the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0005, beta1
of 0.9, beta2 of 0.999 and epsilon of 0.0008 and train for 300,000 iterations. For all the data sets
of Section 4.1, we take the latent dimension dZ = 10, while we use dZ = 16 for 3Dchairs and
dZ = 32 for CelebA. We use Gaussian encoders with diagonal covariance matrix in all the models
and deterministic decoder networks when possible (WAE-based methods). We follow Locatello et al.
(2019) for the architectures in all the experiments expect for CelebA where we follow Tolstikhin
et al. (2018) (details of the networks architectures given Section B.2). We use a (positive) mixture of
Inverse MultiQuadratic (IMQ) kernels and the associated reproductive Hilbert space to compute the
MMD when it is needed (WAE and ablation study of Section 4.1).
The different parameter values used for each experiment are given Table 4. γ is chosen such that
the resulting method achieves the best score s, when averaging over all the β values, where the
score is defined as the sum of the ranking on each individual metric: s = rM SE + Pmetric rmetric
where rMSE designed the ranking of the MSE (lower is better) and rmetric, for metric in {MIG,
FactorVAE, SAP}, is the ranking of the disentanglement performances as measured by the given
metric (higher is better). β is then chosen such that the resulting method, with the previously found
γ, achieves the best overall score s defined above. In Section 4.1, we use a validation run to select
the parameters values and report the MSE and FID scores on a test run. MSE are computed on a
test set of size 10,000 with batch size of 1,000, while we follow Heusel et al. (2017) for the FID
implementation: we first compute the activation statistics of the features maps on the full test set
for both the reconstruction, respectively samples, and the true observations. We then compute the
Frechet distance between two Gaussian with the computed statistics.
B.2 Models architectures
The GaUssian encoder networks, q@(z|x) and decoder network, pθ(x|z), are parametrized by neural
networks as follow:
Pθ (x|z)
δfθ (z) if WAE based method,
IN(μθ(z), σθ(Z)) otherwise.
where fθ, μθ, σ%, μφ and σφ are the outputs of convolutional neural networks. All the experiments
use the architectures of Locatello et al. (2019) except for CelebA where we use the architecture
inspired by Tolstikhin et al. (2018). The details for the architectures are given Table 5.
All the discriminator networks, D, are fully connected networks and share the same architecture
given Table 5. The optimisation setup for the discriminator is given Table 6.
12
Under review as a conference paper at ICLR 2021
Table 5: Networks architectures
Encoder	Decoder	Discriminator
Input: 64 × 64× c	Input: dZ	Input: dZ
CONV. 4 × 4 × 32 stride 2 ReLU	FC 256 ReLU	FC 1000 ReLU
CONV. 4 × 4 × 32 stride 2 ReLU	FC 4 × 4 × 64 ReLU	FC 1000 ReLU
CONV. 4 × 4 × 64 stride 2 ReLU	CONV. 4 × 4 × 64 stride 2 ReLU	FC 1000 ReLU
CONV. 4 × 4 × 64 stride 2 ReLU	CONV. 4 × 4 × 32 stride 2 ReLU	FC 1000 ReLU
FC 256 Relu	CONV. 4 × 4 × 32 stride 2 ReLU	FC 1000 ReLU
FC 2 × dZ	CONV. 4 × 4 × c stride 2	FC 1000 ReLU
		FC 2
(a) Locatello et al. (2019) architectures
Encoder	Decoder	Discriminator
Input: 64 × 64 × c	Input: dZ	Input: dZ
CONV. 4 × 4 × 32 stride 2 BN ReLU	FC 8 × 8 × 256 BN ReLU	FC 1000 ReLU
CONV. 4 × 4 × 64 stride 2 BN ReLU	CONV. 4 × 4 × 128 stride 2 BN ReLU	FC 1000 ReLU
CONV. 4 × 4 × 128 stride 2 BN ReLU	CONV. 4 × 4 × 64 stride 2 BN ReLU	FC 1000 ReLU
CONV. 4 × 4 × 256 stride 2 BN ReLU	CONV. 4 × 4 × 32stride 2 BN Relu	FC 1000 ReLU
FC 2 × dZ	CONV. 4 × 4 × c	FC 1000 ReLU
FC 1000 ReLU
FC 2
(b) CelebA networks architectures
Table 6: FactorVAE discriminator setup
Parameter	Value
Learning rate beta 1 beta 2 epsilon	1e-4 (Section 4.1)/ 1e-5 (Section 4.2) 0.5 0.9 1e-08
13
Under review as a conference paper at ICLR 2021
C Quantitative experiments
Hyper parameter tuning
Figure 5: Heat maps for the different scores on dSprites.
Figure 6: Heat maps for the different scores on NoisydSprites.
Figure 7: Heat maps for the different scores on ScreamdSprites.
Figure 8: Heat maps for the different scores on smallNORB.
14
Under review as a conference paper at ICLR 2021
Table 7: γ values for methods for each data set.
Method	dSprites	NoisydSprites	ScreamdSprites	SmallNORB
TCWAE MWS	2	2	1	1
TCWAE GAN	1	1	10	2
DISENTANGLEMENT SCORES vs β
For each method, we plot the distribution (over five random runs) of the different metrics for different
β values.
Figure 9: Violin plots of the different scores versus γ on dSprites.
15
Under review as a conference paper at ICLR 2021
Figure 10: Violin plots of the different scores versus γ on NoisydSprites.
Figure 11: Violin plots of the different scores versus γ on ScreamdSprites.
16
Under review as a conference paper at ICLR 2021
Figure 12: Violin plots of the different scores versus γ on smallNORB.
Reconstructions and samples
17
Under review as a conference paper at ICLR 2021
(a) Reconstructions
(b) Samples
Figure 13: Samples and reconstructions for each model on the NoisydSprites. (a): Reconstructions.
Top-row: input data, from second-to-top to bottom row: WAE, TCWAE-MWS, TCWAE-GAN ,
β-TCVAE, FactorVAE. (b) Samples. From top to bottom row: WAE, TCWAE-MWS, TCWAE-GAN,
β-TCVAE, FactorVAE. Parameters are the ones reported in Tables 1 and 7
18
Under review as a conference paper at ICLR 2021
(a) Reconstructions
(b) Samples
Figure 14: Same than Figure 13 but for ScreamdSprites.
19
Under review as a conference paper at ICLR 2021
(a) Reconstructions
(b) Samples
Figure 15: Same than Figure 13 but for smallNORB.
20
Under review as a conference paper at ICLR 2021
D Qualitative experiments
3Dchairs
SMn 山 VMDI
NVqHVMDI
Reconstructions
卜£4 弟米上
察市 d** 吊 内*， 局
工吊 W 盾 4 吊 ZK ,月
***奏肃T琳/匕自
NM ■胃*
~WL∙H*X
W>∖ι*k∙dwA
吊小■曲， ■尚吊奥、
dG∙w 旨	W，*
WH -卜 K 吊冒曲黑 ，
R H ・卜加用目* 胃 .
、1恃％9« 曾和用，
AxaJ \・卜产・・
*@卜1符臬具加帚菜
KHbk 利* 罢 1方上
， > 科•，、，<、Z*
第4 R”号卜庠尊年，
fff⅛*>∙h⅛⅜⅜⅜
Figure 16:	Reconstructions (left quadrants) and samples (right quadrants) for TCWAE-MWS (top
quadrants) and TCWAE-GAN (bottom quadrants).
21
Under review as a conference paper at ICLR 2021
B HVʌɔL 0
Reconstructions
(OlUC HVAJsOB工
星卜附	WMVR∣*∣b	务
泉卜 > 果渭卜 **∣b册
/义 ,卜 帝/ 。帚 M
JM 代卜布 d∙MZ
普女 +月声疝』吊*安
图女BRaM肃用 '鼻
jg∙t*s^累艮自
J百,“黑、货支艮S
■ »・R，H&K 用、
Samples
Figure 17:	Reconstructions (left quadrants) and samples (right quadrants) for β-TCVAE (top quad-
rants) and FactorVAE (bottom quadrants).
22
Under review as a conference paper at ICLR 2021
CelebA
Reconstructions
Samples
Figure 18: Same as Figure 16 for the CelebA data set.
SMn 山 VMɔɪ
NVO山 VMɔl
23
Under review as a conference paper at ICLR 2021
FactorVAE (γ = 20)	β-TCVAE (β = 10)
Reconstructions
Samples
Figure 19: Same as Figure 18 for β-TCVAE (top quadrants) and FactorVAE (bottom quadrants).
24