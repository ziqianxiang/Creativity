Under review as a conference paper at ICLR 2021
Estimating Treatment Effects via
Orthogonal Regularization
Anonymous authors
Paper under double-blind review
Ab stract
Decision-making often requires accurate estimation of causal effects from obser-
vational data. This is challenging as outcomes of alternative decisions are not
observed and have to be estimated. Previous methods estimate outcomes based
on unconfoundedness but neglect any constraints that unconfoundedness imposes
on the outcomes. In this paper, we propose a novel regularization framework in
which we formalize unconfoundedness as an orthogonality constraint. We provide
theoretical guarantees that this yields an asymptotically normal estimator for the av-
erage causal effect. Compared to other estimators, its asymptotic variance is strictly
smaller. Based on our regularization framework, we develop deep orthogonal
networks for unconfounded treatments (DONUT) which learn outcomes that are
orthogonal to the treatment assignment. Using a variety of benchmark datasets
for causal inference, we demonstrate that DONUT outperforms the state-of-the-art
substantially.
1	Introduction
Estimating the causal effect of an intervention (i. e., treatment effect) is integral for individual
decision making in many domains such as marketing (Brodersen et al., 2015; Hatt & Feuerriegel,
2020), economics (Heckman et al., 1997), and epidemiology (Robins et al., 2000). For instance, in
order to control an epidemic, it is relevant for public decision-makers to estimate the causal effect of
school-closures (intervention) on the infection rate (outcome).
The causal effect of an intervention can be estimated in two ways: randomized control trials (RCTs)
and observational studies. RCTs are widely recognized as the gold standard for estimating causal
effects, yet conducting RCTs is often infeasible (Robins et al., 2000). For instance, randomly
allocating different policy interventions during an epidemic might be unethical and impractical.
Unlike RCTs, observational studies adopt observed data to infer causal effects. For this, covariates
must be collected that contain all confounders (i. e., variables that affect both treatment and outcome).
This is becoming increasingly common due to ease of access to rich data. In this paper, we estimate
the average causal effect of a treatment from observational data.
In order to estimate the causal effect of a treatment, the outcome of an alternative treatment has to
be estimated. However, this is challenging, since we do not know what the outcome would have
been if another treatment had been applied. Existing methods for estimating treatment effects use
the treatment assignment as a feature and train regression models to estimate the outcomes (Funk
et al., 2011; Kallus, 2017b). Methods based on nearest neighbors and matching are adopted to find
similar subjects (Ho et al., 2007; Crump et al., 2008; Kallus, 2017a; 2020). Tree and forest-based
methods (Wager & Athey, 2018) estimate the treatment effect at the leaf node and train many weak
learners to build expressive ensemble models. Gaussian process-based methods provide uncertainty
quantification (Alaa & van der Schaar, 2017; Ray & Szabo, 2019). Weighting-based approaches
re-weight the outcomes using weights based on covariate and treatment data (Kallus, 2018). For
instance, Fong et al. (2018); Yiu & Su (2018) seek weights such that the treatment assignment
is unassociated with the covariates. However, they do not require the treatment assignment to be
unassociated with the potential outcomes. Doubly robust methods combine a model for the outcomes
and a model for the treatment propensity in a manner that is robust to misspecification (Funk et al.,
2011; Benkeser et al., 2017; Chernozhukov et al., 2018). Recently, deep learning has been successful
for this task due to its strong predictive performance and ability to learn representations of the data
1
Under review as a conference paper at ICLR 2021
(e. g., Johansson et al., 2016; Louizos et al., 2017; Shalit et al., 2017; Yao et al., 2018; Yoon et al.,
2018; Shi et al., 2019). To ensure identifiability of the causal effect, state-of-the-art methods for
estimating treatment effects are based on unconfoundedness (i. e., all confounders are measured, and
thus included in the covariates). Hence, unconfoundedness is assumed for identifiability, yet, during
estimation of the model parameters, any implications on the unobserved outcomes that arise from
unconfoundedness have been neglected.
Contribution.1 In this paper, (i) we introduce a regularization framework that exploits unconfound-
edness . To this end, we formalize unconfoundedness as an orthogonality constraint. This constraint
is used during estimation of the model parameters to ensure that the outcomes are orthogonal to
the treatment assignment. We prove sufficient conditions under which this yields an asymptotically
normal estimator for the average causal effect. Compared to other estimators, its asymptotic variance
is strictly smaller. (ii) Based on our regularization framework, we develop deep orthogonal networks
for unconfounded treatments (DONUT) for estimating average causal effects. DONUT leverages
the predictive capabilities of neural networks to learn outcomes that are orthogonal to the treatment
assignment. Using a variety of benchmark datasets for causal inference, we demonstrate that DONUT
outperforms the state-of-the-art substantially.
2	Problem Setup
Our objective is to estimate the average treatment effect (ATE) of a binary treatment from obser-
vational data. For this, we build upon the Neyman-Rubin potential outcomes framework (Rubin,
2005). Consider a population where every subject i is described by the d-dimensional covariates
Xi ∈ Rd. Each subject is assigned a treatment Ti ∈ {0, 1}. The random variable Yi(1) corresponds
to the outcome under treatment, i. e., Ti = 1, whereas Yi(0) corresponds to the outcome under no
treatment, i. e., Ti = 0. These two random variables, Yi(1), Yi (0) ∈ R, are known as the potential
outcomes. Due to the fundamental problem of causal inference, only one of the potential outcomes is
observed, but never both. The observed outcome is denoted by Yi .
Our aim is to estimate the average treatment effect
ψ=E[Y(1)-Y(0)].	(1)
The following standard assumptions are sufficient for identifiability of the causal effect (Imbens &
Rubin, 2015): consistency (i. e., ∀t ∈ {0, 1} : Y = Y (t), if T = t); positivity (i. e., ∀x ∈ Rd : 0 <
P (T = 1 | X = x) < 1); and unconfoundedness. Unconfoundedness assumes that all confounders
are measured and, hence, conditioning on them blocks all backdoor paths. This is equivalent to
assuming that the potential outcomes Y (1) and Y (0) are independent of the assigned treatment T
given the covariates X, i. e.,
Y(1),Y(0) ⊥ T | X.	(2)
Based on this, the ATE is equal to
ψ = E[E[Y | X, T = 1] -E[Y | X, T = 0]].	(3)
Our task is to estimate the function f(x, t) = E[Y | X = x, T = t] for all x ∈ Rd and t ∈ {0, 1}
based on observational data D = {(Xi, Ti, Yi)}in=1.
3 Orthogonal Regularization for Estimating Treatment Effects
The key idea of our regularization framework is to exploit the implications on the outcomes, that result
from unconfoundedness. For this, we formalize unconfoundedness as an orthogonality constraint.
This orthogonality constraint ensures that the outcomes are orthogonal to the treatment assignment.
We later introduce a specific variant of our regularization framework based on neural networks, which
yields DONUT.
1Code available at github.com/anonymous/donut (anonymized for peer-review).
2
Under review as a conference paper at ICLR 2021
3.1	Unconfoundedness as Orthogonality Constraint
Under unconfoundedness, the outcomes are independent of the assigned treatment given the covariates,
i. e.,
Y(1),Y(0) ⊥ T | X.	(4)
Using the inner producthV, W)= 1 PZi Vi Wi, We formalize the following orthogonality Con-
straint:
1n
hY(t) - f (X,t),T - ∏(X)i = — V(Yi(t) - f (Xi,t))(Ti - ∏(Xi)) = 0,	(5)
n i=1
for t ∈ {0, 1}, and where f(x, t) = E[Y | X = x, T = t] and π(x) = E[T | X = x]. The function
π is the propensity score.2 This is a necessary condition for unconfoundedness, since unconfound-
edness requires the covariance between Y(t) and T given X to be zero due to the independence
of the outcomes and the treatment assignment. Hence, since the inner product in (5) is the empiri-
cal covariance between Y(t) and T given X, unconfoundedness requires the (centered) outcomes
to be orthogonal to the (centered) treatment assignment with respect to the above inner product.3
An appropriate method for estimating outcomes based on unconfoundedness should ensure that
the orthogonality constraint holds. Although existing methods are based on unconfoundedness,
the orthogonality of the outcomes to the treatment assignment is ignored during estimation of the
model parameters. As a remedy, we propose a regularization framework that accommodates the
orthogonality constraint in the estimation procedure.
3.2 Proposed Regularization Framework
In our regularization framework, the orthogonality constraint in (5) is included in the estimation
procedure as follows. Let Hf ⊆ [Rd × {0, 1} → R] and Hπ ⊆ [Rd → [0, 1]] be function classes for
f and π, and ∈ R a model parameter. Then, the objective is to find the solution to the optimization
problem
(f …Hnff×H∏ ×R LFL(f，n； D) + λ ωOR(f，n，
; D),
(6)
where LFL is the so-called factual loss (see Section 3.2.1) between the estimated models and observed
(factual) data. The term Ωor is our new orthogonal regularization that enforces the orthogonality
constraint using the model parameter (see Section 3.2.2). The variable λ ∈ R+ is a hyperparameter
controlling the strength of regularization. We describe both components of (6), i. e., factual loss and
orthogonal regularization, in the following.
3.2.1	FACTUAL LOSS LFL
Similar to previous work (e. g., Shi et al., 2019), the outcome model f and propensity score model π
are estimated using the observed data and the factual loss given by
1n
LFLf ,n; D) = n ɪ^(f (Xi, Ti) - Yi)2 + α CrOSSEntropy(π(Xi), Ti),	(7)
where α ∈ R+ is a hyperparameter weighting the terms of the factual loss. We note the following
about the estimation of the outcome model f in (7). In the first term of the factual loss, f is fitted
to the observed outcomes and, thus, no information about the unobserved outcomes is used. As a
consequence, the model learns about the observed outcomes, but not about the unobserved outcomes.
State-of-the-art methods exclusively rely on the factual loss to estimate the outcomes (e. g. Shalit
et al., 2017). This is based on the assumptions that similar subjects have similar outcomes (Yao et al.,
2018) and that for every subject in the data, there is a similar subject in the data with the opposite
treatment. However, this is unlikely in presence of selection bias, i. e., when treatment and control
groups differ systemically. This pitfall is addressed by our orthogonal regularization in the following.
2Note that the propensity score is defined as π(x) = P (T = 1 | X = x) (Rosenbaum & Rubin, 1983),
which is equivalent to E[T | X = x] for binary treatments.
3Note that we use the notion orthogonality with respect to the inner product in (5). This is different from the
notion of Neyman orthogonality in (Nie & Wager, 2017; Chernozhukov et al., 2018), which requires the GateaU
derivative of a debiased score function to vanish at the true parameter. In contrast, our orthogonality constraint
exploits unconfoundedness to ensure that the outcomes are orthogonal (w.r.t. the inner product in (5)) to the
treatment assignment.
3
Under review as a conference paper at ICLR 2021
3.2.2	Orthogonal Regularization Ωor
We now specify the orthogonal regularization term Ωor that ensures that the orthogonality constraint
in (5) is satisfied. We first state Ωor and then explain each of its parts. The orthogonal regularization
term is given by
1n
Ωor(f,∏,e; D) = -∑(Yi*(0) — f (Xi, 0))2,	(8)
n i=1
with
-n
Yi* (0) = Yi - ψ*Ti, ψ* = - Ef(XiJ)- f (Xi, 0)	(9)
n i=1
and
f(Xi, t)	=	f(Xi,	t)	+	(Ti	-	π(Xi)),	fort ∈	{0, -},	(10)
where the pseudo outcome Yi*(0) and the perturbation function f are required to learn outcomes
that are orthogonal to the treatment assignment, as described in the following.
Pseudo outcome. The untreated outcome of a subject can be expressed as Yi (0) = Yi - ψ(Xi) Ti,
where ψ(X) = f(X, -) - f(X, 0) is the true treatment effect at X. Hence, if we had access to the
treatment effect ψ(X), we would also have access to the untreated outcome Yi(0) even if we did not
observe it.4 We use the average treatment effect of the current model fit, i. e., ψ* in (9), as proxy
for ψ(Xi). This creates a pseudo outcome, Yi*(0) = Yi - ψ* Ti. Under sufficient conditions, this
converges to the true outcome (see Section 4).
Perturbation function. in order to ensure that the orthogonality constraint is satisfied, i. e., hY (t) -
f(X, t), T - π(X)i = 0, we use a perturbation function f. Simply adding the inner product as a
regularization term is not sufficient, since this does not guarantee that the orthogonality constraint
holds at a solution of (6). A similar approach is used in targeted minimum loss estimation (e. g.,
van der Laan & Rose, 2011) and, more general, dual problems in optimization (Zalinescu, 2002). We
extend the function f to a perturbation function f using the model parameter as in (10). As a result,
solving the optimization problem in (6) forces the outcome estimates f to satisfy the orthogonality
constraint. Mathematically, this can be seen by taking the partial derivative of (6) and setting it to
zero, i. e.,
∂	-n
0= H (LFL + λ Ωor (f,n,e; D))I	=2-]T(Yi*(0) - f e(Xi, 0))(Ti - ∏(Xi)),	(11)
∂e	le=^	n z—z
=	i=1
and, hence, hY * (0) - f (X, 0), T - π(X)i = 0. it is analytically sufficient to regularize only the
untreated outcome.5 Whenever one of the outcomes is orthogonal to the treatment assignment, the
other outcome is also orthogonal. A proof of this is provided in Appendix A.
Then, the outcomes are estimated using f (x, t) and, therefore, the estimate for the ATE is ψ =
n
n Ei=I f ,(Xi, 1) - f e(Xi, 0), which coincides with ψ*, since the perturbation terms in (10) cancel.
3.3 Deep Orthogonal Networks for Unconfou nded Treatments
our regularization framework works with any outcome model f and propensity score model π that
is estimated through a loss function. The regularization framework ensures that the outcomes are
estimated such that they are orthogonal to the treatment assignment. We introduce a specific variant
of our regularization framework based on feedforward neural networks, which yields deep orthogonal
networks for unconfounded treatments (DONUT). Neural networks present a suitable model class
due to their strong predictive performance. The architecture of DoNUT is set as follows. For the
outcome model f, we use the basic architecture of TARNet (Shalit et al., 2017). TARNet uses a
deep feedforward neural network to produce a representation layer, followed by two 2-layer neural
networks to predict each of the potential outcomes from the shared representation. For the propensity
4This can be seen by distinction of cases: if Ti = 1, then Yi(0) = Yi — ψ(Xi), and if Ti = 0, then
Yi(0) = Yi.
5it is straightforward to extend the orthogonal regularization term to incorporate both outcomes by adding
the same term for the treated outcome Y ⑴，including the pseudo outcome Y * (1) and the perturbation function
f (x, 1) to enforce orthogonality. We demonstrate this in Appendix A.
4
Under review as a conference paper at ICLR 2021
score model π, we use a logistic regression (Rosenbaum & Rubin, 1983). The choice of feedforward
neural networks is particularly strengthened by the theoretical discussion in Section 4.
4 Theoretical Guarantees
We prove sufficient conditions under which our regularization framework yields an asymptotically
normal6 7 estimator ψ for the true ATE ψ. Asymptotic normality is particularly favorable for an
estimator, as such estimators converge to the true ATE at a rate 1∕√n. We further show that,
compared to other estimators, our estimator has strictly smaller asymptotic variance.
Theorem 1. (Asymptotic Normality.) Suppose
1.	The estimator η = (f, π) for the outcome and propensity Scoremodel converges to some
η = (f, ∏) in the sense that ∣∣η - ηk = 0p(1), where either f = f or ∏ = π (or both)
corresponds to the true function.
2.	The treatment effect is homogeneous, i. e., ∀x ∈ Rd : ψ (x) = ψ.
3.	The estimators f and π take values in P-Donsker classes, i. e., Hf, Hn ∈ CLT(P) J
Then ψ is asymptotically normal, i. e.,
2
√n3 — M →N(0，EVr(T | X)])，	(12)
where σ2 is the variance ofthe outcome.	□
The proof is provided in Appendix C. We make the following remarks about the result and its
conditions. Condition 1 requires that either the model for the outcomes f or the model for the
propensity score π (or both) is correctly specified, but not necessarily both. This means that our
estimator is doubly robust, since it is consistent under correct specification of either f or π . In
particular, neural networks converge at a fast enough rate to invoke the doubly robustness condition
(Farrell et al., 2018). Condition 2 can be easily relaxed to any specification of ψ as long as it has
finitely many parameters and given that the appropriate identification criteria hold. This changes the
orthogonal regularization in (8) accordingly; see Appendix D for details. However, the advantage
of Condition 2 is that it explains why the asymptotic variance of ψ is smaller compared to other
estimators. In particular, the difference in asymptotic variance to some estimators (e. g., the inverse
probability weighted estimator) can be sizable if the propensity score is close to zero or one. Often,
this difference is not offset by weaker restrictions imposed by heterogeneous treatment effects (e. g.,
Vansteelandt & Joffe, 2014). Condition 3 captures a large class of functions of estimators for f and π
from which we can choose. An overview of P-Donsker classes is given in Mikosch et al. (1997). In
particular, the class of feedforward neural networks is a P-Donsker class, since Lipschitz parametric
functions are P-Donsker functions, and any Lipschitz transformation of a P-Donsker function is again
a P-Donsker function. For mathematical details, see Appendix E. Together with the convergence rate
for doubly robustness, this provides theoretical justification for the use of neural networks in our
regularization framework, and therefore in DONUT.
Similarity to partially linear regression. We find an interesting similarity between our estimator
ψ and the estimator obtained by partially linear regression (PLR) (see (1.5) in Chernozhukov et al.
(2018)). The analytical expression of ψ yield by our regularization framework in (8) is given in
(31) in Appendix C. This is similar to the estimator obtained by PLR after partialling the effect of
X out from T (see (1.5) in Chernozhukov et al. (2018)). However, the PLR separately estimates
the nuisance functions f and ∏ and then plugs them into the estimator. Our approach does not use
the analytical expression as plug-in estimator, but the estimator arises directly from solving the
6We refer to Appendix B for a brief discussion on asymptotically normal estimators.
7A class F of measurable functions on a probability space (Ω, A, P) is called a P-Donsker class if, for
Gn = √n(Pn - P), the empirical process {G：f : f ∈ F}n≥ι converges weakly to a P-Brownian bridge GP.
Donsker classes include parametric classes, but also many other classes, including infinite-dimensional classes,
e. g., smooth functions and bounded monotone functions. See (Mikosch et al., 1997) for more details.
5
Under review as a conference paper at ICLR 2021
optimization problem in (6) via (8). The difference becomes apparent in the experiments (Section 5),
where we include the PLR as a baseline and find that the performance of DONUT is superior.
Comparison to other estimators. We compare the asymptotic behavior of our estimator ψ to other
regular estimators (e. g., Funk et al. (2011); Nie & Wager (2017)). We find that, under the conditions
of Theorem 1, there does not exist a regular estimator that achieves strictly smaller asymptotic
variance than our estimator. The reason for this is as follows. We proof in Appendix C that our
regularization framework yields an asymptotically normal estimator for the ATE. In particular, we
proof that our estimator is efficient (see Appendix C for the efficient influence function), and therefore
it achieves the efficiency bound. As a consequence, among all regular estimators, our estimator
achieves the smallest asymptotic variance. In Appendix G, we make an example in which we compare
the inverse probability weighted estimator to our estimator. We show that the difference in asymptotic
variance becomes particularly pronounced in presence of selection bias (i. e., when treatment and
control groups differ systemically).
To summarize, we prove sufficient conditions under which our estimator ψ is asymptotically normal.
Hence, our proposed estimator converges to the true ATE ψ at a fast rate. Compared to other regular
estimators, its asymptotic variance is strictly smaller, since we prove that our estimator is efficient.
Since feedforward neural networks converge at a fast enough rate to invoke doubly robustness and
belong to a P-Donsker class, these results hold true when using feedforward neural networks in our
regularization framework, which yields DONUT.
5	Experiments
Our proposed DONUT is evaluated against state-of-the-art baselines, where we find that its ATE
estimation is superior (Section 5.2). Its benefits appear especially for problems subject to selection
bias, which is confirmed as part of a simulation study (Section 5.3). In the latter, the theoretical
guarantees from Section 4 are confirmed.
5.1	Setup
Evaluating methods for estimating causal effects is challenging as we rarely have access to ground
truth causal effects. Established procedures for evaluation of such methods rely on semi-synthetic
data, which reflect the real world. Our experimental setup follows established procedure regarding
datasets, baselines, and performance metrics (e. g., Johansson et al., 2016; Shalit et al., 2017).
Datasets. We evaluate all methods across four benchmark datasets for causal inference: IHDP (e. g.,
Johansson et al., 2016), Twins (e. g., Yoon et al., 2018), ACIC 2018 (e. g., Shi et al., 2019), and Jobs
(e. g., Shalit et al., 2017). The first three datasets are semi-synthetic, while the last originated from a
RCT. Details on IHDP, Twins, ACIC 2018, and Jobs are provided in Appendix H.
Training details. DONUT is trained using the regularization framework in (6), where both the
outcome model and the propensity score model are trained jointly using stochastic gradient descent
with momentum. The hidden layer size is 200 for the representation layers and 100 for the outcome
layers similar to Shalit et al. (2017); Shi et al. (2019). The hyperparameter α in the factual loss (7) is
set to 1 and λ in the orthogonal regularization (8) is determined by hyperparameter optimization over
{10k}2k=-2. For IHDP, we follow established practice (e. g., Shalit et al., 2017) and average over
1,000 realizations of the outcomes with 63/27/10 train/validation/test splits. Following Shalit et al.
(2017); Yoon et al. (2018); Shi et al. (2019), we average over 100 different train/validation/test splits
for Twins and Jobs, and over 10 splits for each dataset for ACIC, all with ratios 56/24/20.
Baselines. We compare DONUT against 17 state-of-the-art methods for estimating treatment effects,
organized in the following groups: (i) Regression methods: Linear regression with treatment as
covariate (OLS/LR-1), separate linear regressors for each treatment (OLS/LR-2), and balancing
linear regression (BLR) (Johansson et al., 2016); (ii) Matching methods: k-nearest neighbor (k-NN)
(Crump et al., 2008); (iii) Tree methods: Bayesian additive regression trees (BART) (Chipman
et al., 2012), random forest (R-Forest) (Breiman, 2001), and causal forest (C-Forest) (Wager &
Athey, 2018); (iv) Gaussian process methods: Causal multi-task Gaussian process (CMGP) (Alaa
& van der Schaar, 2017) and debiased Gaussian process (D-GP) (Ray & Szabo, 2019); (v) Neural
network methods: Balancing neural network (BNN) (Johansson et al., 2016), treatment-agnostic
6
Under review as a conference paper at ICLR 2021
Table 1: Results for estimating average treatment effects on IHDP, Twins, and Jobs. Lower is better.
Results
Datasets (Mean ± Std)
Method	IHDP (ATE)		TWINS (ATE)		JOBS (ATT)	
	In-s.	Out-s.	In-s.	Out-s.	In-s.	Out-s.
OLS/LR-1	.73 ± .04	.94± .06	^^.0038 ± .0025	.0069 ± .0056	.01 ± .00	.08 ± .04
OLS/LR-2	.14± .01	.31 ± .02	.0039 ± .0025	.0070 ± .0059	.01 ± .01	.08 ± .03
BLR	.72 ± .04	.93± .05	.0057 ± .0036	.0334 ± .0092	.01 ± .01	.08 ± .03
k-NN	.14± .01	.79 ± .05	.0028 ± .0021	.0051 ± .0039	.21 ± .01	.13 ± .05
BART	.23 ± .01	.34± .02	.1206 ± .0236	.1265 ± .0234	.02 ± .00	.08 ± .03
R-Forest	.73 ± .05	.96± .06	.0049 ± .0034	.0080 ± .0051	.03 ± .01	.09 ± .04
C-Forest	.18 ± .01	.40 ± .03	.0286 ± .0035	.0335 ± .0083	.03 ± .01	.07 ± .03
D-GP	.14± .32	.17± .47	.0046 ± .0033	.0058 ± .0043	.03 ± .01	.06 ± .05
CMGP	.11 ± .10	.13 ± .12	.0124 ± .0051	.0143 ± .0116	.06 ± .06	.09 ± .07
BNN	.37 ± .03	.42 ± .03	.0056 ± .0032	.0203 ± .0071	.04 ± .01	.09 ± .04
TARNET	.26 ± .01	.28± .01	.0108 ± .0017	.0151 ± .0018	.05 ± .02	.11 ± .04
CFR-WASS	.25 ± .01	.27± .01	.0112 ± .0016	.0284 ± .0032	.04 ± .01	.09 ± .03
GANITE	.43 ± .05	.49 ± .05	.0058 ± .0017	.0089 ± .0075	.01 ± .01	.06 ± .03
AIPWE	.13± .12	.22 ± .28	.0027 ± .0013	.0048 ± .0032	.03 ± .01	.10 ± .08
TMLE	.13± .10	.33± .32	.0034 ± .0020	.0053 ± .0025	.02 ± .01	.06 ± .05
PLR	.63 ± .09	1.32 ± .31	.0133 ± .0255	.0084 ± .0354	.09 ± .04	.10 ± .08
Dragonnet	.14± .01	.20± .05	.0062 ± .0051	.0064 ± .0054	.02 ± .01	.06 ± .05
DONUT	.13 ± .01	.19± .02	.0025 ± .0016	.0033 ± .0026	.01 ± .00	.06 ± .05
representation network (TARNet) (Shalit et al., 2017), counterfactual regression with Wasserstein
distance (CFR-WASS) (Shalit et al., 2017), generative adversarial networks (GANITE) (Yoon et al.,
2018), and Dragonnet (Shi et al., 2019); (vi) Plug-in estimators: Augmented inverse probability
weighted estimator (AIPWE) (Cao et al., 2009), targeted maximum likelihood estimator (TMLE)
(van der Laan & Rubin, 2006), and partially linear regression (PLR) (Chernozhukov et al., 2018) all
of them using the outcome and propensity score model of Dragonnet as nuisance functions.
Performance metrics. Following established procedure, we report the following metrics for each
dataset. For IHDP and ACIC 2018, we use the absolute error in average treatment effect (Johansson
etal.,2016): 以te = |1 Pn=ι(f (xi, 1)-f (xi, 0)) — nn Pn=I (f(xi, 1)-f(xi, 0))|. For Twins, we use
the absolute error in observed average treatment effect (Yoon et al., 2018): EATE = 11 Pn=ι(yi(1)-
yi(0)) -1 Pn=1(yi (1) - yi(0))∣. For Jobs, all treated subjects T were part of the original randomized
sample E , and hence, the true average treatment effect can be computed on the treated by ATT =
|T |-1 Pi∈T yi - |C ∩ E |-1 Pi∈C∩E yi, where C is the control group. Similar to (Shalit et al., 2017),
we then use the error: EATT = IATT - |T|-1 Ei∈t(f(xi, 1) - f(xi, 0))|.
5.2	Results
IHDP, Twins, and Jobs have been used to evaluate many methods for estimating treatment effects. In
Table 1, the results of the experiments on IHDP, Twins, and Jobs are presented. Overall, DONUT
achieves competitive performance across all datasets. On IHDP, the CMGP baseline achieves slightly
lower estimation error (mean: .11 (CMGP) vs. .13 (DONUT)), but at the drawback of an inferior
standard deviation (std.: .10 (CMGP) vs. .01 (DONUT)). However, as shown by previous work, the
small sample size and the limited simulation settings of IHDP make it difficult to draw conclusions
about methods (e. g., Yoon et al., 2018; Shi et al., 2019). In contrast, DONUT achieves superior
performance on both Twins and Jobs, where the number of samples is much larger (Twins: n =11,400;
Jobs: n =3,212). On these datasets, DONUT is state-of-the-art. In particular, we point out the
difference to PLR which uses the analyitcal expression of our estimator similar to Chernozhukov
et al. (2018) as discussed in Section 4, but as a plug-in estimator. For a reasonable comparison, we
use the outcome models of Dragonnet as nuisance functions. We find that DONUT achieves superior
performance.
Among neural network-based methods (i. e., BNN, TARNet, CFR-WASS, GANITE, Drag-
onnet, and DONUT), DONUT performs superior across all datasets. In compari-
son to TARNet, which shares the basic architecture of DONUT (but without orthog-
onal regularization), we achieve substantial reduction in ATE estimation error across
7
Under review as a conference paper at ICLR 2021
all datasets (out-of-sample error reduction: 32.1 % on IHDP, 78.1 % on Twins, and
45.5 % on Jobs). This demonstrates the effectiveness of our regularization framework.
We further evaluate DONUT using ACIC 2018.
This collection of datasets was introduced for eval-
uating neural networks for estimating average treat-
ment effects (Shi et al., 2019). We compare
DONUT against the most competitive, and current
state-of-the-art method, i. e., Dragonnet. In addition,
we compare against TARNet (i. e., DONUT, but
without orthogonal regularization). Table 2 presents
the results of the experiments on ACIC 2018. The
main observation is that DONUT improves estima-
tion relative to TARNet (DONUT without orthogo-
nal regularization) and relative to Dragonnet across
a large collection of datasets. These results further
Table 2: Results for estimating average treatment
effects on ACIC 2018. Comparison between Drag-
onnet (most competitive baseline), TARNet (same
architecture as DONUT, but without orthogonal reg-
ularization) and DONUT. Lower is better.
Results
Method	ACIC (EATE)	
	In-s.	Out-s.
TARNET	4.53 ± 0.72	4.48 ± 0.74
Dragonnet	2.97 ± 1.46	2.99 ± 1.48
DONUT	1.22 ± .33	1.26 ± 0.29
confirm the benefit of including orthogonal regularization in the estimation procedure.
5.3	Simulation Study on Selection Bias
To evaluate the robustness of DONUT with regards to selection bias (i. e., when
treatment and control groups differ substantially), we generate synthetic data
with varying selection bias according to a similar protocol as in Yao et al.
(2018); Yoon et al. (2018). Details on
We compare DONUT against Dragonnet, which
makes use of the AIPWE to estimate the average
treatment effect. This is particular interesting for
the comparison with DONUT due to the results
in Section 4. We showed that, compared to any
other estimator, our estimator obtains strictly
smaller asymptotic variance, especially in pres-
ence of selection bias. Figure 1 presents the
mean and standard deviation of ATE for varying
selection bias. We report two major insights: (i)
Dragonnet is outperformed by DONUT across
different levels of selection bias. As selection
bias increases, the estimation error of DONUT
remains stable. (ii) The standard deviation of
both DONUT and Dragonnet increases as selec-
tion bias increases. However, in line with our
findings that DONUT has the smallest asymp-
totic variance among all regular estimators, the
standard deviation of DONUT remains consis-
the protocol are provided in Appendix I.
Kullback-Leibler divergence
Figure 1: Comparison of DONUT and Dragonnet in
presence of selection bias. Reported is the mean and
standard deviation of ATE on synthetic data as selection
bias is varied (measured by the Kullback-Leibler diver-
gence). For each level, both algorithms are run on 100
realizations of the datasets. Lower is better.
tently smaller than the standard deviation of Dragonnet. In addition, this difference becomes more
pronounced the larger the selection bias. Hence, we observe empirical properties of our estimator
that coincide with the theory derived in Section 4.
6	Conclusion
Understanding causal effects is crucial for reliable decision-making. In this paper, we present a
regularization framework for estimating average causal effects. We formalize unconfoundedness as an
orthogonality constraint that is used to learn outcomes that are orthogonal to the treatment assignment.
We prove theoretical guarantees that our regularization framework yields an asymptotically normal
estimator. Based on this, we develop DONUT, which leverages the predictive capabilities of neural
networks to estimate average causal effects. Experiments on datasets show that, in most cases,
DONUT outperforms the state-of-the-art substantially. This work provides an interesting avenue for
future research on causal inference. We hypothesize that most existing models can be improved by
8
Under review as a conference paper at ICLR 2021
incorporating orthogonal regularization. We leave the derivation of a unifying theory for future work.
A revised version of this work can be found in Hatt & Feuerriegel (2021).
9
Under review as a conference paper at ICLR 2021
References
A. M. Alaa and M. van der Schaar. Bayesian inference of individualized treatment effects using multi-
task Gaussian processes. In Advances in Neural Information Processing Systems, pp. 3424-3432,
2017.
D. Almond, K. Y. Chay, and D. S. Lee. The costs of low birth weight. The Quarterly Journal of
Economics, 120(3):1031-1083, 2005.
D. Benkeser, M. Carone, M. J. van der Laan, and P. B. Gilbert. Doubly robust nonparametric inference
on the average treatment effect. Biometrika, 104(4):863-880, 2017.
L. Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
K. H. Brodersen, F. Gallusser, J. Koehler, N. Remy, and S. L. Scott. Inferring causal impact using
bayesian structural time-series models. Annals of Applied Statistics, 9(1):247-274, 2015.
W. Cao, A. A. Tsiatis, and M. Davidian. Improving efficiency and robustness of the doubly robust
estimator for a population mean with incomplete data. Biometrika, 96(3):723-734, 2009.
V. Chernozhukov, D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins.
Double/debiased machine learning for treatment and structural parameters. The Econometrics
Journal, 21(1):C1-C68, 2018.
H. A. Chipman, E. I. George, and R. E. McCulloch. BART: Bayesian additive regression trees.
Annals of Applied Statistics, 6(1):266-298, 2012.
R. K. Crump, V. J. Hotz, G. W. Imbens, and O. A. Mitnik. Nonparametric tests for treatment effect
heterogeneity. Review of Economics and Statistics, 90(3):389-405, 2008.
V. Dorie. NPCI: Non-parametrics for Causal Inference. https://github.com/vdorie/npci,
2016.
M. H. Farrell, T. Liang, and S. Misra. Deep neural networks for estimation and inference. arXiv
preprint arXiv:1809.09953, 2018. URL http://arxiv.org/abs/1809.09953.
C. Fong, C. Hazlett, K. Imai, et al. Covariate balancing propensity score for a continuous treatment:
Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1):
156-177, 2018.
M.J. Funk, D. Westreich, C. Wiesen, T. Sturmer, M. A. Brookhart, and M. Davidian. Doubly robust
estimation of causal effects. American Journal of Epidemiology, 173(7):761-767, 2011.
Tobias Hatt and Stefan Feuerriegel. Early detection of user exits from clickstream data: A markov
modulated marked point process model. In Proceedings of The Web Conference 2020, pp. 1671-
1681, 2020.
Tobias Hatt and Stefan Feuerriegel. Estimating average treatment effects via orthogonal regularization.
arXiv preprint arXiv:2101.08490, 2021.
J.	J. Heckman, H. Ichimura, and P. E. Todd. Matching as an econometric evaluation estimator:
Evidence from evaluating a job training programme. The Review of Economic Studies, 64(4):
605-654, 1997.
J. L. Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational and
Graphical Statistics, 20(1):217-240, 2011.
D. E. Ho, K. Imai, G. King, and E. A. Stuart. Matching as nonparametric preprocessing for reducing
model dependence in parametric causal inference. Political analysis, 15(3):199-236, 2007.
G. W. Imbens and D. B. Rubin. Causal inference in statistics, social, and biomedical sciences.
Cambridge University Press, 2015.
F. D. Johansson, U. Shalit, and D. Sontag. Learning representations for counterfactual inference. In
33rd International Conference on Machine Learning, pp. 4407-4418, 2016. ISBN 9781510829008.
10
Under review as a conference paper at ICLR 2021
N. Kallus. A framework for optimal matching for causal inference. In Artificial Intelligence and
Statistics,pp. 372-381, 2017a.
N. Kallus. Recursive partitioning for personalization using observational data. In Proceedings of the
34th International Conference on Machine Learning, pp. 1789-1798, 2017b.
N. Kallus. Balanced policy evaluation and learning. In Advances in neural information processing
systems, pp. 8895-8906, 2018.
N. Kallus. Generalized optimal matching methods for causal inference. Journal of Machine Learning
Research, 21(62):1-54, 2020.
K. Kandasamy, A. Krishnamurthy, B. P6czos, L. Wasserman, and J. M. Robins. Nonparametric
von Mises estimators for entropies, divergences and mutual informations. In Advances in Neural
Information Processing Systems, pp. 397-405, 2015.
E. H. Kennedy. Semiparametric theory and empirical processes in causal inference. In Statistical
causal inferences and their applications in public health research, pp. 141-167. 2016.
R. J. LaLonde. Evaluating the econometric evaluations of training programs with experimental data.
The American Economic Review, 76(4):604-620, 1986.
C. Louizos, U. Shalit, J. Mooij, D. Sontag, R. Zemel, and M. Welling. Causal effect inference
with deep latent-variable models. In Advances in Neural Information Processing Systems, pp.
6447-6457, 2017.
M. F. MacDorman and J. O. Atkinso. Infant mortality statistics from the linked birth/infant death. In
Mon Vital Stat Rep, 46 (suppl 2), pp. 1-22, 1998.
T. Mikosch, A. W. van der Vaart, and J. A. Wellner. Weak convergence and empirical processes,
volume 92. Springer Science & Business Media, 1997. ISBN 9781475725476.
X. Nie and S. Wager. Quasi-oracle estimation of heterogeneous treatment effects. arXiv preprint
arXiv:1712.04912, 2017.
K.	Ray and B. Szabo. Debiased bayesian inference for average treatment effects. In Advances in
Neural Information Processing Systems, pp. 11929-11939, 2019.
J. M. Robins, M. A. Herndn, and B. Brumback. Marginal structural models and causal inference in
epidemiology. Epidemiology, 11(5):550-560, 2000.
P. R. Rosenbaum and D. B. Rubin. The central role of the propensity score in observational studies
for causal effects. Biometrika, 70(1):41-55, 1983.
D. B. Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal of the
American Statistical Association, 100(469):322-331, 2005.
U. Shalit, F. D. Johansson, and D. Sontag. Estimating individual treatment effect: Generalization
bounds and algorithms. In 34th International Conference on Machine Learning, pp. 4709-4718,
2017. ISBN 9781510855144.
C. Shi, D. M. Blei, and V. Veitch. Adapting neural networks for the estimation of treatment effects.
In Advances in Neural Information Processing Systems, pp. 1-11, 2019.
Y. Shimoni, C. Yanover, E. Karavani, and Y. Goldschmnidt. Benchmarking framework for
performance-evaluation of causal inference analysis. arXiv preprint arXiv:1802.05046, 2018.
M. J. van der Laan and S. Rose. Targeted learning: causal inference for observational and experi-
mental data. Springer Science & Business Media, 2011.
M. J. van der Laan and D. Rubin. Targeted maximum likelihood learning. The international journal
of biostatistics, 2(1), 2006.
A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.
11
Under review as a conference paper at ICLR 2021
S. Vansteelandt and M. Joffe. Structural nested models and G-estimation: The partially realized
promise. Statistical Science, 29(4):707-731, 2014.
S. Wager and S. Athey. Estimation and inference of heterogeneous treatment effects using random
forests. Journal of the American Statistical Association, 113(523):1228-1242, 2018.
L. Yao, M. Huai, S. Li, J. Gao, Y. Li, and A. Zhang. Representation learning for treatment effect
estimation from observational data. In Advances in Neural Information Processing Systems, pp.
2633-2643, 2018.
S. Yiu and L. Su. Covariate association eliminating weights: a unified weighting framework for
causal effect estimation. Biometrika, 105(3):709-722, 2018.
J. Yoon, J. Jordon, and M. van der Schaar. Ganite: Estimation of individualized treatment effects
using generative adversarial nets. In 6th International Conference on Learning Representations,
pp. 1-22, 2018.
C. Zalinescu. Convex analysis in general vector spaces. World scientific, 2002.
12
Under review as a conference paper at ICLR 2021
A	Sufficiency of Regularizing the Untreated Outcome
We show that it is sufficient to regularize Y (0), since orthogonality of Y (0) implies orthogonality of
Y (1) to T given X. Suppose Y (0) satisfies the orthogonality constraint, i. e.,
1n
hY(0) - f(X, 0), T - π(X )i = -E(Yi(0) — f (Xi, 0))(Ti — ∏(Xi)) = O.	(14)
Then,
-n
hY (I)- f (X, 1), T — ∏(X )i = -E (匕(I)- f (Xi, 1))(Ti — ∏(Xi))	(15)
n i=1
-n
=-V (Yi(1) — E[Yi | Xi, T = 1])⑵ — ∏(Xi))	(16)
n
i=1
1n
=-V (Yi(I) — E[Yi⑴ I Xi])(Ti — ∏(Xi))	(17)
n i=1
1n
=—V2	(Yi	+ ψ (Xi)(I-	Ti)- EIYi	+ ψ (Xi)(I-	Ti)	| Xi])(Ti	— π(Xi ))	(18)
n
i=1
1n
-	V (Yi — Ψ(Xi) Ti-	E[Yi	—	Ψ(Xi)	Ti	I	Xi])(Ti	— ∏(Xi))	(19)
n
i=1
-	n
—	T (Yi(O) — EIYi(0) I Xi])(3 — ∏(Xi))	(20)
n i=1
-	n
—	E (Yi(O) — E[Yi I Xi, T = 0])⑵ — ∏(Xi))	(21)
n i=1
-	n
-	∑(Yi(0) — f (Xi, 0))(Ti — ∏(Xi))	(22)
n
i=1
hY(0)- f(X,0),T — ∏(X)i =0,	(23)
using that Y (0) = Y — ψ(X) T and Y (-) = Y + ψ(X) (- — T) in (18) and (20). Hence, hY (-) —
f (X, -), T — π(X)i is zero and, therefore, Y (-) is orthogonal to T given X. Nevertheless, it is
straightforward to adapt the orthogonal regularizer in (8) to accommodate both outcomes. Consider
the following adapted orthogonal regularizer
nn
ωOR (f,π, €；D) = - X(Yi*(0) — fe (0, Xi))2 + — X(YiP) — f P, Xi))2,	(24)
n i=1	n i=1
with
-n
Yi*(0) = Yi — ψ* Ti,	YiP)= Ti + ψ* (1- Ti), ψ* = - ^f(Xi,1)- f (Xi, 0) (25)
and
f e(Xi, 0) = f (Xi, 0) + e(Ti — ∏(Xi)),	(26)
f (Xi, -) = f(Xi, -) + (Ti — π(xi)),	(27)
where is a additional model parameters. Similar to the result in Section 3.2.2., this yields that
hY * (0) — f (X, 0), T — π(X)i and hY *(-) — f(X,-), T — π(X)i are zero, when the partial
derivative of (24) w. r. t. is zero.
13
Under review as a conference paper at ICLR 2021
B Asymptotically Normal Estimators
In this section, we give a brief discussion on asymptotically normal estimators. We refer to van der
Vaart (1998) for an in-depth discussion. Suppose ψ is an estimator for the true parameter ψ. Then, ψ
is an asymptotically normal estimator if it satisfies
1	1 G	L
ψ -ψ = nEφ(Zi) + OpQ/√n),	(28)
i=1
where φ(Z) is referred to as the influence function at Z = (X, T, Y ) (Kandasamy et al., 2015).
The influence function has mean zero and finite variance (i. e., E[φ(Z)] = 0 and Var(φ(Z)) < ∞).
The existence and uniqueness of the influence function follows by the Riesz representation theorem.
Asymptotic normality is a favorable property of an estimator, since, by the central limit theorem,
√n(^ - ψ) → N(0, Var(φ(Z))).	(29)
Hence, an asymptotically normal estimator is asymptotically normal distributed and unbiased. More-
over, the variance shrinks in proportion to 1/√n as the sample size grows and is given by the variance
of the influence function. As a consequence, the distribution of the estimator ψ converges weakly to
a dirac delta function centered around the true ψ.
C Proof of Theorem 1
In observational studies, the covariates X are often high-dimensional and limited knowledge about
the nuisance functions (i. e., outcome and propensity score functions f and π) is available. In such a
case, it is reasonable to use flexible, data-adaptive methods for estimating the nuisance functions,
e. g., neural networks. However, the complexity of these methods makes asymptotic analysis difficult,
because the estimators used to construct η = (f, ∏) are not described by a single finite-dimensional
parameter. Nevertheless, under some conditions, we can learn about the asymptotics of ψ using tools
from empirical process theory.
First, we need to introduce some notation. Throughout the proof, we will use P{f (Z)} =	f(z)dP
to denote expectations of f(Z) for a random variable Z (treating the function f as fixed). Hence,
P{f (Z)} is random if f is random (e. g., estimated from the sample). In contrast, E[f (Z)] is a fixed
non-random quantity, which averages over randomness in both Z and f and thus will not equal
n
P{f (Z)} except when f = f is fixed and non-random. Moreover, we let Pn = n Ei=I δzi denote
the empirical measure such that sample averages can be written as ɪ Pn=ι f (Zi) = f f (z)dPn =
Pbn (f (Z)). For clarity, we will denote the true nuisance functions and average treatment effect as
η0 = (f0, π0) and ψ0, respectively. We further denote the triplet (Y, T, X) by Z.
For f = fe, ∏, and ψ = ɪ Pi=1 f e(Xi, 1) 一 f e(Xi, 0) from (8),the regularization framework yields
1n
1
hY*(0) - f(X,0),T 一 ∏(X)i = — E(K - ψTi- f(Xi,0))(Ti - ∏(Xi)) = 0,	(30)
n
i=1
by construction. Hence, we receive an analytical expression of the estimator of the average treatment
effect by solving the above for ψ, i. e.,
-t _ S	ʌ
ψ = n pn=ι(匕 - f(Xi, 0))(Ti- ∏(Xi))
=	1 Pn=ITi(Ti- ∏(Xi))
EI	C	.1	J	.	♦	♦	F 7	T1⅞	/	/ Γ7	1
Therefore, the estimator is given by ψ = Pn (m(Z; η)), where
(31)
m(Z; η) =
(Y - f(X, 0))(T - π(X))
P{T(T - π(X))}
(32)
and η = (π, f) denotes the nuisance functions.
We recall the conditions in Theorem 1. Suppose the estimator η = (f, ∏) converges to some η =
(f, ∏) in the sense that kη - ηk = op(1)8, where either f = fo or ∏ = ∏o (or both) corresponds to the
8op(1/rn) employs the usual stochastic order notation so that Xn = op(1/rn) means that rnXn → 0 in
probability.
14
Under review as a conference paper at ICLR 2021
true nuisance function. Thus at least one nuisance estimator needs to converge to the correct function,
but one can be misspecified. Then, P{m(Z; η)} = P{m(Z; no)} = ψo, from the straightforward to
check fact that P{(m(Z; ∏, fo)} = P{m(Z; ∏o, f)} for any ∏ and f. Consider the decomposition
ψ - Ψo = Pn (m(Z； n)) - P{m(Z； n)}
=(Pn - P){m(Z; n)} + P{m(Z; n) - m(Z; n)}.
If the estimators for the nuisance functions n take values in P-Donsker classes, then m(Z; n) also
belongs to a P-Donsker class, since Lipschitz transformations of Donsker functions are again Donsker
functions. The Donsker property together with the continuous mapping theorem yields that (Pn -
P){m(Z; n)} is asymptotically equivalent to (Pn - P){m(Z; n)} up to 0p(1∕√n) error (see Mikosch
et al. (1997) for more details on P-Donsker classes). Therefore,
ψ - ψo = (Pn - P){m(Z； n)} + P{m(Z； n) - m(Z； n)} + 0p(1∕∕n).	(33)
It is left to show that P{m(Z; n) - m(Z; n)} is asymptotically negligible. This term equals
P
)
, , .., ^ , .. , .., , ..
(T - π(X ))(Y - f(X, 0)) (T - ∏0(X ))(Y - fo(X, 0))
---------------------------------------------------------
P{T(T - Π(X))}
P{T(T-π0(X))}
(34)
P π0 (X)
, , .., , . ^ , ..
(1 - Π(X ))(f0(X,1) - f(X, 0))
P{T(T - π(X))}
(1- ∏0(X))(f0(X,1) - fo(X, 0))!)
P{T(T -∏0(X))}	Jf
(35)
—
_P ( (1- ∏0(X ))∏(X )(f0(X, 0) - f(X, 0)))
I	P{T (T - ∏(X))}	J
(36)
P π0 (X)
, , .., , . ^ , ..
(1 - Π(X ))(fo(X,1) - f(X, 0))
P{∏o(X)(1 - π(X))}
(1- ∏0(X))(f0(X,1) - fo(X, 0))!)
P{∏o(X )(1-∏o(X))}	Jf
(37)
_P( (1- ∏0(X))∏(X)(f0(X,0) - f(X,0)))
I	P{∏0(X)(1 - π(X))}	J
(38)
P ( ∏o(X )(1 - π(X))(fo(X, 1) - f(X, 0)))
Pj	P{∏o(X )(1 - π(X))}	∫-ψ0
(39)
_P( (1- ∏0(X))∏(X)(f0(X,0) - f(X,0)))
t	P{∏0 (X )(1 - ∏(X))}	J
(40)
∏ ∏0(X)(1 - π(X))(fo(X, 1) - f(X, 0))	∏0(X)(1 - π(X))(fo(X, 1) - fo(X, 0))
P	-
[	P{∏o(X)(1 - π(X))}	P{∏0(X)(1 - ∏(X))}
_P( (1- ∏0(X))∏(X)(f0(X,0)- f(X,0)))
-t	P{∏o(X)(1- ∏(X))}	∫,
(42)
where we use P{m(Z; n)} = P{m(Z; no)} in (34), iterated expectation in (35), and P{T(T -
Π(X))} = P{∏o(x)(1 - Π(X))} (likewise for ∏o instead of π) in (37). In (39), and similarly in
(341), we use that ψo(X) = ψo and, therefore, fo (X, 1) -fo(X, 0) = ψo. As a result, by simplifying,
the above equals
∏o(X)- Π(X)
P{∏o(X)(1 - π(X))}
, ʌ , ..
(fo(X, 0) - f(X, 0)).
(43)
—
Therefore, by the fact that ∏o and ∏ are bounded away from zero and one, along with the Cauchy-
Schwarz inequality (i. e., P{f g} ≤ kf kkgk), we have that (up to a multiplicative constant)
∣P{m(Z; n) - m(Z; n)}| is bounded above by
...... ʌ, ...
k∏o(X) - ∏(X )kkfo(X, O) - f(X, 0)k.	(44)
Thus, for example if ∏ is based on a correctly specified parametric model (e. g., logistic regression),
so that ∣∣π - ∏ok = 0p(1∕√n), then we only need f to be consistent, i.e., kf - fok = 0p(1), to
make the product term P{m(Z; n) - m(Z; n)} = 0p(1/√n) asymptotically negligible. Then the
doubly robust estimator satisfies ψ - ψo = (Pn - P){m(Z; no)} + 0p(1/√n) and It IS efficient with
15
Under review as a conference paper at ICLR 2021
♦ n	i' . ∙ l / Γ7 I ∖	f Γ7 ∖ I rɪ-il	.1 ∙	.1	.	1 ∙ . ∕' ? τ.
influence function φ(Z; ψ, η) = m(Z; η) - ψ. Thus, this proves the asymptotic normality of ψ. It
♦	.	1	,1,,1	.	♦	i' ?	1
remains to show that the asymptotic variance of ψ equals
σ2
P{Var(T | X)}.
(45)
Note that in the paper, E[f (Z)] is used similarly to P{f (Z)}. We introduced the notion P{f (Z)}
here to make clear that we only integrate over randomness in Z and not the function estimates f. As
seen in Appendix B, the asymptotic variance of an estimator is the variance of its influence functions.
From above, we know that the efficient influence function of ψ is φ(Z; ψ, η) = m(Z; η) - ψ. Thus,
using that P{φ(Z; ψ, η)} = 0 (by definition),
Var(φ(Z; ψ, η)) = P{φ(Z; ψ, η)2}	(46)
= P{(m(Z; η) - ψ)2}	(47)
= P{(m(Z; η)2} - ψ2.	(48)
This, together with
p{m(Z∙ y7)2} =__P{U2}___
{ ( ;η) } = P{Var(T | X)}
(49)
where U = Y - f(X,T)〜N(0,σ2) and P{T(T - ∏(X))} = P{Var(T | X)} proves the
statement.	□
D Orthogonal Regularization under Linear S pecification of the
Treatment Effect
In Theorem 1, condition 2 ensures the homogeneity of the treatment effect, i. e., ψ(X) = ψ. This can
be easily relaxed to any specification of ψ as long as it has finitely many parameters and given that
the appropriate identification criteria hold (for linear specification, this is the non-singularity of the
design matrix). We explain in this section how the orthogonal regularization in (8) changes when
linear specification of the treatment effect is considered, i. e., ψ(x) = θ>x, where θ ∈ Rd. Recall the
orthogonal regularizer from (8)
1n
ωOR (f,π,gD) = -∑(Yi*(0) - fe(Xi, 0))2,	(50)
n i=1
where the perturbation function fe remains unchanged. The pseudo outcome Yi* (0) is now given as
Y；*(0) = Yi -(θ>Xi) Ti,	(51)
where θ is given by the system of linear equations,
1n
θ>Xi = — Vv(Ti, Xi)(f(Xi,1) - f(Xi,0)),	(52)
n
i=1
where v(t, x) is an arbitrary function of the dimension of θ, which ensures that the system of
linear equations possesses a unique solution. For instance, in the case of ψ(x) = θ>x, the choice
v(ti, xi) = xi ti could be made. The procedure goes as follows. First, the above system of linear
equations is solved for θ. Second, the pseudo outcome Yi*(0) = Yi - (θ>Xi)Ti can be computed.
Finally, this is plugged into the regularizer. The proof for asymptotic normality goes similar to the
one under homogeneous treatment effect.
E FEEDFORWARD NEURAL NETWORKS ARE IN P-DONSKER CLASS
We show that feedforward neural networks are in a P-Donsker class. A feedforward neural network
fd is defined by the recursion
f1(x) = W1>x,	fi(x) = Wi>σ(fi-1(x)), i = 2, . . . , d,	(53)
16
Under review as a conference paper at ICLR 2021
for d ∈ N≥1, matrices {Wi}id=1 of appropriate dimensions, activation function σ, which applies
element-wise, and d is the depth of the neural network.
f1 is clearly in a P-Donsker class as it is Lipschitz parametric. The Donsker property is preserved
under Lipschitz transformations. Hence, fi (x), for i = 2, . . . , d, is in a P-Donsker class, if the
activation function is a Lipschitz transformation. This is the case for most common activation
functions. We give two examples in the following.
The ReLU activation function, σReLU(x) = max(0, x), preserves the Donsker property, since a
constant function is clearly in a P-Donsker class and taking the maximum of two P-Donsker functions
is again a P-Donsker function due to the preservation under Lipschitz transformations. The same
follows for the ELU activation function,
σELU(x) =	xα,(ex - 1)
if x > 0
if x ≤ 0
where α ≥ 0, since it is a Lipschitz transformation.
F ASYMPTOTICS OF ψ UNDER HETEROGENEOUS TREATMENT EFFECTS
zʌ	. ∙	.	Cl ♦,	.	1	1	.	. i'i' . T . 1 ∙	?
Our estimator retains a useful interpretation under heterogeneous treatment effects. In this case, ψ
converges to
P{Var(T | X)ψ(X)}
P{Var(T | X)}
(54)
where ψ(X) = f(X, 1) - f (X, 0) is the true treatment effect at X. This can be interpreted
as a weighted average of treatment effects ψ(X). As a consequence, most weight is given to
subpopulations with large Var(T | X), i. e., which are most informative about the treatment effect.
We consider the same notation as in the proof in Appendix C and resume it. From the proof in
Appendix C, we know that
ψ - ψ = (Pn - P){m(Z； η)} + P{m(Z; η) - m(Z； η)} + 0p(1∕√n),	(55)
since the estimator of the nuisance functions η = (f, ∏) belong to a P-Donsker class, and therefore
m(Z; η) belongs to a P-Donsker class. Similar to the proof in Appendix C, it is left to investigate
the term P{m(Z; η) - m(Z; η)}. We show that this term is asymptotically not negligible when the
treatment effect is heterogeneous. Again, by iterated expectations, this term equals
P ( (∏0(X)- ∏(X ))(fo(X,1)- f(X, 0)) )_ P ʃ ∏0(X )(1 - ∏0(X))(fo(X,1) - fo(X, 0)) ]	(56)
1	P{∏0(X )(1 - ∏(X))}	-	∖	P{∏0(X )(1 - ∏0(X))}	J ()
The first term can be bounded from above (up to a multiplicative constant) by the Cauchy-Schwarz
inequality and, therefore, is asymptotically negligible if one of the nuisance functions is correctly
specified. Thus, only the second term remains. Using ψ(X) = f0(X, 1) - f0(X, 0), the second term
can be written as
P{π0(X)(1 - π0(X))ψ(X)}
P{π0(X)(1 - π0(X))}
(57)
Since π0 (X)(1 - π0 (X)) = Var(T | X), this concludes the proof.
□
G Comparison to Inverse Probability Weighted Estimator
We compare our estimator ψ to the inverse probability weighted (IPW) estimator (e. g., Funk et al.,
2011). The IPW estimator is a popular estimator for treatment effects due to its improvement in
efficiency and reduction in bias compared to unweighted estimators.
Under identical conditions as in Theorem 1, the IPW estimator, denoted as ψIPW, is asymptotically
normal with asymptotic variance σ2E[1/Var(T | X)] (e.g., Kennedy, 2016). We can compare
the asymptotic behavior of our estimator ψ and the IPW estimator ψIPW. Both estimators are
asymptotically unbiased, but with different asymptotic variance as the following result shows.
17
Under review as a conference paper at ICLR 2021
C- --Il- -YrT 7	. 1	7 ∙. ∙	∙ Er	1	.1	.	∙	/" ?	∙	.	∙	.1	11
Corollary 1. Under the conditions in Theorem 1, the asymptotic variance of ψ is strictly smaller
than the asymptotic variance of ψIPW.
Proof. The statement follows by Jensen,s inequality.	□
The difference between the asymptotic variances becomes particularly pronounced in presence of
selection bias, i. e., when the treatment group is systematically different from the control group.
Corollary 2. If the propensity score π(x) is close to 0 or 1 for some x ∈ Rd, then the asymptotic
variance of ψIPW can take arbitrary large values.
Proof. For some small > 0, let π(X) ∈ [0, 1]\(, 1 - ). Then, by Bhatia-Davis inequality and
since T is binary, Var(T | X) = ∏(X)(1 - ∏(X)). Hence, Var(T | X) ≤ e, and, therefore,
E[1/Var(T | X)] ≥ 1/e, which proves the statement.	□
H Detailed Description of the Datasets
H.1 IHDP
Hill (Hill, 2011) introduced a semi-synthetic dataset created from the Infant Healthand Development
Program (IHDP). This dataset is based on a randomized experiment that examines the effect of home
visits by specialists on future cognitive scores. The dataset consists of 747 children (t = 1: 139,
t = 0: 608) with 25 covariates. Similar to Shalit et al. (2017), we use 1,000 realizations from setting
A in the NPCI package (Dorie, 2016).
H.2 TWINS
This dataset is made up of all births in the USA between 1989 and 1991 (Almond et al., 2005).
Only twins are considered among these births. Treatment (i. e., T = 1) is defined as being the
heavier twin (and T = 0 as being the lighter twin). The outcome is defined as 1-year mortality.
There are 30 covariates available for each pair of twins that relate to the parents, pregnancy and
birth: marital status; race; residence; number of previous births; pregnancy risk factors; quality of
care during pregnancy; and number of gestation weeks prior to birth. Only twins that weigh less
than 2 kg and have no missing covariates (list-wise deletion) are taken into account. This creates
a complete dataset (without missing data).9 The final cohort consists of 11,400 twin pairs, whose
mortality rate is 17.7 % for lighter twins and 16.1 % for heavier ones. In this setting, we observed
both T = 0 (lighter twin) and T = 1 (heavier twin) for each pair of twins; Therefore, the true
treatment effect in this dataset is known. In order to simulate an observational study, one of the twins
is selectively observed based on information using the covariates (which leads to selection bias) as
follows: T | X 〜Bern(Sigmoid(w>x + n)) where w> 〜U((-0.1,0.1)30×1) and n 〜N(0,0.1).
H.3 JOBS
Jobs studied in LaLonde (1986) consists of randomized data based on the National Supported Work
program and non-randomized observational study data. A (random) subset of randomized data is
used to evaluate the algorithms. The dataset consists of 722 randomized samples (T = 1: 297, T = 0:
425) and 2,490 non-randomized samples (T = 1: 0, T = 0: 2,490), all with 7 covariates.
H.4 ACIC 2018
ACIC 2018 is a collection of semi-synthetic datasets derived from the linked birth and infant death
data (LBIDD) (MacDorman & Atkinso, 1998), and was developed for the 22018 Atlantic Causal
Inference Conference competition (ACIC) (Shimoni et al., 2018). The simulation includes 63 different
data generation processes with a sample size from 1,000 to 50,000. Each dataset is a realization from
a separate distribution, which itself is randomly drawn in accordance with the settings of the data
generation process. Similar to Shi et al. (2019), we randomly pick 3 datasets10 of size either 5k or
9We provide the complete dataset at github.com/anonymous/donut (anonymized for peer-review).
10We provide the list of unique dataset identification numbers at github.com/anonymous/donut
(anonymized for peer-review) for reproducibility.
18
Under review as a conference paper at ICLR 2021
10k for each of the 63 data generating process settings and exclude all datasets with indication of
strong selection bias. This yields a total number of 97 datasets.
I Data Generating Process for S imulation S tudy
For the simulation study, we follow a similar protocol as in Yao et al. (2018); Yoon et al. (2018).
We generate 2,500 untreated samples from N(010×1, 0.5 × ΣΣ>) and 5,000 treated samples from
N(μι, 0.5 X ∑∑>), where Σ 〜U((0,1)10×10). Varying μι yields different levels of selection bias,
which is measured by the Kullback-Leibler divergence. The larger the Kullback-Leibler divergence,
the greater the distributional distance between treatment and control group, and, thus, the larger
the selection bias. The outcome is generated as Y | X = x,T = t 〜(w>x + t + n), where
W 〜U((-1,1)10×1), and n 〜N(0,0.1).
19