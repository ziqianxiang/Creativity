Under review as a conference paper at ICLR 2021
D2P-FED:	Differentially Private Federated
Learning with Efficient Communication
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose the discrete Gaussian based differentially private
federated learning (D2P-FED), a unified scheme to achieve both differential pri-
vacy (DP) and communication efficiency in federated learning (FL). In particular,
compared with the only prior work taking care of both aspects, D2P-FED provides
stronger privacy guarantee, better composability and smaller communication cost.
The key idea is to apply the discrete Gaussian noise to the private data transmis-
sion. We provide complete analysis of the privacy guarantee, communication cost
and convergence rate of D2P-FED. We evaluated D2P-FED on INFIMNIST and
CIFAR10. The results show that D2P-FED outperforms the-state-of-the-art by
4.7% to 13.0% in terms of model accuracy while saving one third of the commu-
nication cost. The results might be surprising at its first glance but is reasonable
because the quantization level k in D2P-FED is independent of q. As long as
q is large enough, the probability that the noise exceeds q is small and thus has
negligible impact on the model accuracy.
1	Introduction
Federated learning (FL) is a popular machine learning paradigm that allows a central server to train
models over decentralized data sources. In federated learning, each client performs training locally
on their data source and only updates the model change to the server, which then updates the global
model based on the aggregated local updates. Since the data stays locally, FL can provide better
privacy protection than traditional centralized learning. However, FL is facing two main challenges:
(1) FL lacks a rigorous privacy guarantee (e.g., differential privacy (DP)) and indeed, it has been
shown to be vulnerable to various inference attacks (Nasr et al., 2019; Pustozerova & Mayer; Xie
et al., 2019); (2) FL incurs considerable communication costs. In many potential applications of FL
such as mobile devices, these two challenges are present simultaneously.
However, privacy and communication-efficiency have mostly been studied independently in the past.
As regards privacy, existing work has applied a gold-standard privacy notion - differential privacy
(DP) - to FL, which ensures that the server could hardly determine the participation of each client by
observing their updates (Geyer et al., 2017). To achieve DP, each client needs to inject noise to their
local updates and as a side effect, the performance of the trained model would inevitably degrade.
To improve model utility, secure multiparty computation (SMC) has been used in tandem with DP
to reduce noise (Jayaraman et al., 2018; Truex et al., 2019). The key idea is to prevent the server
from observing the individual updates, make only the aggregate accessible, and thus transform from
local DP to central DP. However, SMC introduces extra communication overhead to each client.
There has been extensive research on improving communication efficiency of FL while ignoring
the privacy aspect (Tsitsiklis & Luo, 1987; Balcan et al., 2012; Zhang et al., 2013; Arjevani &
Shamir, 2015; Chen et al., 2016). However, these communication reduction methods either have
incompatible implementations with the existing DP mechanisms or would break the DP guarantees
when combined with SMC.
The only existing work that tries to reconcile DP and communication efficiency in FL is
cpSGD (Agarwal et al., 2018). The authors leveraged the Binomial mechanism, which adds Bi-
nomial noise into local updates to ensure differential privacy. The discrete nature of Binomial noise
allows it to be transmitted efficiently. However, cpSGD faces several limitations when applied to
real-world applications. Firstly, with Binomial noise, the output of a learning algorithm would have
different supports on different input datasets; as a result, Binomial noise can only guarantee approx-
1
Under review as a conference paper at ICLR 2021
imate DP where the participation of the client can be completely exposed with nonzero probability.
Also, there lacks a tight composition for DP with Binomial noise and the resulting privacy budget
skyrockets in a multi-round FL protocol. Hence, the Binomial mechanism cannot produce a useful
model with a reasonable privacy budget on complex tasks. Last but not least, the Binomial mecha-
nism involves several mutually constrained hyper-parameters and the privacy formula is extremely
complicated, which makes hyper-parameter tuning a difficult task.
In this paper, we propose the discrete Gaussian based differential private federated learning (D2P-
Fed), an alternative technique to reduce communication costs while maintaining differential privacy
in FL. Our key idea is to leverage the discrete Gaussian mechanism in FL, which adds discrete
Gaussian noise into client updates. We show that the discrete Gaussian mechanism satisfies Renyi
DP which provides better composability. We employ secure aggregation along with the discrete
Gaussian mechanism to lower the noise and exhibit the privacy guarantee for this hybrid privacy
protection approach. To save the communication cost, we integrate the stochastic quantization and
random rotation into the protocol. We then cast FL as a general distributed mean estimation problem
and provide the analysis of the utility for the overall protocol. Our theoretical analysis sheds light on
the superiority of D2P-FED to cpSGD. Our experiments show that D2P-FED can lead to state-of-
the-art performance in terms of managing the trade-off among privacy, utility, and communication.
2	Related Work
It is well studied how to improve the communication cost in traditional distributed learning set-
tings (Tsitsiklis & Luo (1987); Balcan et al. (2012); Zhang et al. (2013); Arjevani & Shamir (2015);
Chen et al. (2016)). However, most of the approaches either require communication between the
workers or are designed for specific learning tasks so they cannot be applied directly to general-
purpose FL. The most relevant work is Suresh et al. (2017) which proposed to use stochastic quan-
tization to save the communication cost and random rotation to lower mean squared error of the
estimated mean. We follow their approach to improve the communication efficiency and model util-
ity of D2P-FED. Nevertheless, our work differs from theirs in that we also study how to ensure DP
for rotated and quantized data transmission and prove a convergence result for the learning algorithm
with both communication cost reduction and privacy protection steps in place.
On the other hand, differentially private FL is undergoing rapid development during the past few
years (Geyer et al. (2017); McMahan et al. (2017); Jayaraman et al. (2018)). However, these meth-
ods mainly focus on improving utility under a small privacy budget and ignore the issue of com-
munication cost. In particular, we adopt a similar hybrid approach to Truex et al. (2019), which
combines SMC with DP for reducing the noise. SMC ensures that the centralized server can only
see the aggregated update but not individual ones from clients and as a result, the noise added by
each client can be reduced by a factor of the number of clients participating in one round. The
difference of our work from theirs is that we inject discrete Gaussian noise to local updates instead
of the continuous Gaussian noise. This allows us to use secure aggregation (Bonawitz et al., 2017)
which is much cheaper than threshold homomorphic encryption used by Truex et al. (2019). We
further study the interaction between discrete Gaussian noise and the secure aggregation as well as
their effects on the learning convergence.
We identify cpSGD (Agarwal et al. (2018)) as the most comparable work to D2P-FED. Just like
D2P-FED, cpSGD aims to improve both the communication cost and the utility under rigorous
privacy guarantee. However, cpSGD suffers from three main defects discussed in Section 1. This
paper proposes to use the discrete Gaussian mechanism to mitigate these issues in cpSGD.
3	Background and Notation
In this section, we provide an overview of FL and DP and establish the notation system. We use
bold lower-case letters (e.g. a,b,c) to denote vectors, and bold upper-case letters (e.g. A, B, C) for
matrices. We denote 1 …n by [n].
FL Overview. In a FL system, there are one server and n clients Ci , i ∈ [n]. The server holds
a global model of dimension d. Each client holds (IID or non-IID) samples drawn from some
unknown distribution D. The goal is to learn the global model w ∈ Rd that minimizes some loss
function L(w, D). To achieve this, the system runs a T -round FL protocol. The server initializes
2
Under review as a conference paper at ICLR 2021
the global model with w0. In round t ∈ [T], the server randomly sub-samples γn clients from [n]
with sub-sampling rate γ and broadcasts the global model wt-1 to the chosen clients. Each chosen
client Ci then runs the local optimizers (e.g. SGD, Adam, and RMSprop), computes the difference
between the locally optimized model wt(i) and the global model wt-1: gt(i) = wt(i) - wt-1, and
uploads gt(i) to the server. The server takes the average of the differences and update the global
model Wt = wt-1 + 1 P g(i).
Communication in FL. The clients in FL are often edge devices, where the upload bandwidth is
fairly limited; therefore, communication efficiency is of uttermost importance to FL. Let π denote
a communication protocol. We denote the per-round communication cost as C(π, g[n] ). To lower
the communication cost, the difference vectors are typically compressed before sent to the server.
The compression would degrade model performance and we measure the performance loss via the
mean squared error. Specifically, letting g denote the actual mean of difference vectors n P= g(i)
and g denote the server,s estimated mean of difference vectors using some protocol such as D2P-
Fed, We could measure the performance loss by E(∏, g[n]) = E[kg - gk2], i.e., the mean squared
error between the estimated and the actual mean. This mean squared error is directly related to the
convergence rate ofFL (AgarWal et al., 2018).
Threat Model & Differential Privacy. We assume that the server is honest-but-curious. Namely,
the server Will folloW the protocol honestly under the laW enforcement or reputation pressure, but
is curious to learn the client-side data from the legitimate client-side messages. In the FL context,
the server Wants to get information about the client-side data by studying the local updates received
Without deviating from the protocol.
The above attack, Widely knoWn as the inference attack (Shokri et al., 2017; Yeom et al., 2018; Nasr
et al., 2019), can be effectively mitigated using a canonical privacy notation namely differential
privacy (DP). Intuitively, DP, in the context of ML, ensures that the trained model is nearly the same
regardless of the participation of any arbitrary client.
Definition 1 ((, δ)-DP).A randomized algorithm f : D → R is (, δ)-differentially private if for
every pair of neighboring datasets D and D0 that differs only by one datapoint, and every possible
(measurable) output set E the following inequality holds: P [f (D) ⊆ E] ≤ eP [f (D0) ⊆ E] + δ.
(, δ)-DP has been used as a privacy notion in most of the existing Works of privacy-preserving FL.
However, in this paper, we consider a generalization of DP, Renyi differential privacy (RDP), which
is strictly stronger than (, δ)-DP for δ > 0 and alloWs tighter analysis for compositing multiple
mechanisms. This second point is particularly appealing, as FL mostly comprises multiple rounds
yet the existing works suffer from skyrocketing privacy budgets for multi-round learning.
Definition 2 ((α, )-RDP). For two probability distributions P and Q with the same support, the
Renyi divergence of order α > 1 is defined by Dα(P kQ) = 0⅛ log Ex 〜q( PQ(I) )α ∙ A randomized
mechanism f : D → R is (α, )-RDP, if for any neighboring datasets D, D0 ∈ D it holds that
Dα(f(D)kf(D0)) ≤ .
The intuition behind RDP is the same as other variants of differential privacy: “Similar inputs should
yield similar output distributions,” and the similarity is measured by the Renyi divergence under
RDP. RDP can also be converted to (, δ)-DP using the following transformation.
Lemma 1 (RDP-DP conversion (Mironov (2017))). If M obeys (α, )-RDP, then M obeys ( +
log(1∕δ)∕(α — 1), δ)-DPfor all 0 < δ < L
RDP enjoys an operationally convenient and quantitatively accurate way of tracking cumulative
privacy loss when compositing multiple mechanisms (Lemma 2) or being combined with subsam-
pling (Wang et al., 2018). As a result, RDP is particularly suitable for the context of ML.
Lemma 2 (Adaptive composition of RDP (Mironov (2017))). If (randomized) mechanism M1 obeys
(α, 1)-RDP, and M2 obeys (α, 2)-RDP, then their composition obeys (α, 1 + 2)-RDP.
4	Discrete Gaussian Mechanism
In this section, we present the discrete Gaussian mechanism and establish its privacy guarantee. We
first introduce discrete Gaussian distribution.
3
Under review as a conference paper at ICLR 2021
Definition 3 (Discrete Gaussian Distribution). Discrete Gaussian is a probability distribution on
a discrete additive subgroup L (for instance, a multiple of Z) parameterized by σ. For a discrete
Gaussian distribution Nl(g) and X ∈ L, the probability mass on X is proportional to LxI/(2σ2.
Discrete Gaussian mechanism works by adding noise drawn from discrete Gaussian distribution.
Canonne et al. (2020) proved concentrated DP for the discrete Gaussian mechanism. However,
there lacks tight privacy amplification and composition theorem for concentrated DP. To address,
we turn to RDP and provide the first RDP analysis for the discrete Gaussian mechanism. The proof
is delayed to Appendix A due to space limitation.
Theorem 1 (RDP for discrete Gaussian mechanism). If f has sensitivity 1 and range(f) ⊆ L, then
the discrete Gaussian mechanism: f(∙) + NL(σ) satisfies (α, α∕(2σ2))-RDP
Under RDP, discrete Gaussian exhibits tight privacy amplification bound under sub-sampling (Wang
et al., 2018). This suits FL well since a subset of clients is sub-sampled to upload the updates in
each round.
Corollary 1 (Privacy amplification for discrete Gaussian mechanism (Wang et al., 2018)). If a
discrete Gaussian mechanism is (α, 弄)-RDP, then augmented with subsampling (without replace-
2
ment), the privacy guarantee is amplified to (1) (α, O(-^r)) in the high privacy regime; or (2)
(α, O(αγ2eσ2)) in the low privacy regime.
Besides, RDP enables discrete Gaussian mechanism to be composed tightly with analytical moments
accountant (Wang et al., 2018), which saves a huge amount of privacy budget in a multi-round FL.
Analytical moments accountant is a data structure that tracks the cumulant generating function of
the composed mechanisms symbolically. Since it has no closed-form solution, we instead introduce
the canonical composition of RDP (Mironov, 2017) below for the ease of discussion in Section 6.3.
Corollary 2 (Composition for discrete Gaussian mechanism (Wang et al., 2018)). If a discrete
Gaussian mechanism is (α, 寿)-RDP then the SeqUentiaI COmPOSitiOn of T such mechanisms yield
(α, 2Tα)-RDP guarantee. Ifwe convert all the RDP guarantees back to (e, δ)-DP the growth of E
under the same δ is asymptotically O(√T).
Note that both privacy amplification and composition are given in an asymptotic form for the clarity
of presentation. For tight bound, we refer the readers to Theorem 27 and Section 3.3 in Wang et al.
(2018).
5	D2P-FED: Algorithm and Privacy Analysis
In this section, we formally present D2P-FED and provide rigorous privacy analysis.
5.1	Algorithm
Algorithm 1 provides the pseudocode for D2P-FED. It follows the general FL pipeline which itera-
tively performs the following steps: (1) the server broadcasts the global model to a subset of clients;
(2) the selected clients train the global model on their local data and upload the resulting model
difference; and (3) the server aggregates the model differences uploaded by the clients and updates
the global model. Grounded on the general FL pipeline, D2P-FED introduces some additional steps
at the client side as follows to improve communication efficiency and privacy.
Stochastic Quantization & Random Rotation (line 11-14). To lower the communication cost,
the clients stochastically quantize the values in the update vectors to some discrete domain. Com-
pared with real number encoding which costs 32 or 64 bits per dimension, the quantized value only
requires log2(k) bits per dimension where k is the level of quantization (see line 12-14 in Algo-
rithm 1 and McMahan et al. (2016) for detailed explanation.). On the other hand, quantization
lowers the fidelity of the update vector and thus leads to some error in estimating the mean of gra-
dients. To lower the estimation error, the clients apply randomized rotation to the updates before
quantization as proposed by McMahan et al. (2016). The details are discussed in Section 6.1.
4
Under review as a conference paper at ICLR 2021
Discrete Gaussian Mechanism (line 15). We apply the discrete Gaussian mechanism to ensure
DP. To determine the noise magnitude to be added, We need to bound the '2-sensitivity (defined
in Section 5.2) of the gradient aggregate. Without quantization and random rotation, one could
clip the individual gradient update and consequently the '2-sensitivity is just the clipping threshold.
HoWever, the inclusion of compression steps makes the analysis of `2 -sensitivity more sophisticated.
We provide the analysis of sensitivity and RDP guarantees for the entire algorithm in Section 5.2.
Each client samples the noise from the discrete Gaussian distribution. In contrast to prior Work
Where each client adds independent noise, We require the clients to share the same random seed,
generate the same noise and add an average share of the noise in each round. This is because the
sum of multiple independent discrete Gaussians is no longer a discrete Gaussian. Note that the
same random seed is also required for communication reduction in secure aggregation so We can
conveniently reuse it here Without introducing more further overhead (see BonaWitz et al. (2017) for
details). The noise magnitude is set to ensure that the aggregate noise from all clients provides the
global DP.
Algorithm 1: D2P-FED Protocol.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
max
Input: Support Lattice: L = g_r ∙ Z, Noise Scale: σ, Rotation Matrix: R, Random Seed: S
Quantization Level: k, φq(x) = 2g_〔 ((2gmaxX + ɪ2ɪ) mod q — ɪɪɪ), q is odd.
for t - [T] do
Server:
I Sub-sample a subset of clients S ⊂ [n], |S| = Yn and broadcast wt-ι and gmax to S
Client:
I foreach client i ∈ S do Send Uj to j, Uij 〜 Unif (Ld),Lq := {x ∈ L, |x| ≤ q-1}
Client:
foreach client i ∈ S do
Train the model wt* (i) With wt as initialization
(i)
gt
w(ti) - wt_1
gripped = g(i)/max(1, kgtDk2 )
g(i)	= R × g(i)
gt,rotated	× gt
,clipped
let b[r] := —gmax + 2r_1 for every r ∈ [0, k)
/* Compute the difference
/* Clip the difference
/* Random
Rotation
/*
Quantize
*/
*/
*/
*/
for j ∈ d, b[r] ≤ gFquantized
[j] ≤ b[r + 1] do
g(iq“antized[j]
b[r + 1]
g(irotated[j] - b[r]
w.p.	b[r + 1] — b[r]
b[r]
gfdp= g(i)uantized + Vn
s
Vi ~
/* Discrete
Gaussian
gt = φq(φq(gt,dp" + Pj=i,j∈s Uij- Pj=ij∈s Uji)
Send g(i) to the server
Server:
/* Mask
gt
wt
γ1n Pi∈s g(i)
Wt-I + gt
/* Aggregate
*/
*/
*/
Secure aggregation. (line 5,16,19) To reduce the noise magnitude for ensuring DP, We hide
clients’ individual updates from the central server and only alloW it to see the aggregated updates via
the technique of secure aggregation. If the individual updates Were available to the central server,
they should also be protected With the same privacy guarantee as the averaged update; in this case,
the required noise scales up With O(γn). On the other hand, if the central server can only access the
aggregated updates, then the required noise is O(1). Hence, secure aggregation of local updates can
lead to significant noise reduction. HoWever, there exists a challenge for integrating secure aggrega-
tion With discrete Gaussian mechanism: discrete Gaussian variables have infinite support, thereby
incompatible With secure aggregation Which operates on finite field. In Section 5.2, We propose to
address this challenge by mapping the noised vector to a cyclic additive group and then applying
secure aggregation and shoW that the RDP guarantees are preserved under the mapping.
5
Under review as a conference paper at ICLR 2021
5.2	Privacy Analysis
Before applying discrete Gaussian mechanism to D2P-FED, we need to figure out how to calibrate
the added noise. In differential privacy, the calibration is guided by sensitivity of the function as
defined below.
Definition 4 ('2-sensitivity). Given afunction f : D → R and two neighboring datasets D and D0,
the '2-sensitivity of f: ∆f is defined as ∆f = max。,。， kf(D)-f(D0)k2.
In DP for deep learning, the traditional way to bound '2-sensitivity is to clip the update vector.
However, quantization will further influence the sensitivity after clipping. We provide the sensitivity
after quantization as below. The proof is delayed to Appendix B due to space limitation.
Theorem 2. Ifwe clip the l2 norm of g to D, and quantize it to k = √d + 1 levels, then the '2
sensitivity of the difference is 4D.
Given Theorem 1 and Theorem 2, we provide the RDP bound for D2P-FED.
Corollary 3 (RDP for D2P-FED). Given the clipping bound D, the noise scale σ, D2P-FEDfollows
(α, 8αD2)-RDP
Remark 1: Comparison with cpSGD It seems unclear how to interpret the above bound when
compared with Theorem 1 in cpSGD (Agarwal et al., 2018). Indeed, the claim that D2P-FED has
a better privacy guarantee than cpSGD can be mainly justified by the following three aspects: (1)
D2P-FED follows RDP which is a strictly stronger privacy notion than cpSGD which is intrisically
limited to (, δ)-DP; (2) D2P-FED enjoys a tighter composition compared to cpSGD. This is of
critical significance in a FL protocol with potentially thousands of rounds; (3) Our experimental
results in Figure 1a also empirically show that D2 P-Fed enjoys a tighter composition than cpSGD.
The total privacy budget for D2P-FED grows much more slowly than cpSGD as training proceeds.
Remark 2: Privacy Effect of Secure Aggregation. Corollary 3 is built on the assumption that
the centralized server only has access to the summed updates but not the individual ones. If the
centralized server has access to individual updates, the noise has to scale up γn times to maintain the
same privacy guarantee which severely hinders the model accuracy. To consolidate the assumption,
we leverage a cryptographic technique, secure aggregation (Bonawitz et al., 2017) which guarantees
that the centralized server can only see the aggregated result. The basic intuition is to mask the
inputs with random values canceling out in pairs. However, since discrete Gaussian has infinite
support, we cannot directly apply random masks to it. To reconcile secure aggregation with discrete
Gaussian, we propose to project the involved values into a quotient group after shifting and then
apply the random masks as shown in line 16 in Algorithm 1. According to the post-processing
theorem of RDP (Mironov (2017)), the result still follows rigorous Renyi differential privacy as
proved in Appendix C. Note that we consider a simplified version of the full secure aggregation
protocol (Bonawitz et al. (2017)) in Algorithm 1 and omit many interesting details such as the
generation of the random masks and how to deal with dropout. We deem this to be enough to clarify
the idea behind the reconciliation. The complete version of secure aggregation can be reconciled
using exactly the same trick.
Theorem 3 (Informal). Distributed discrete Gaussian mechanism with secure aggregation obeys
the same RDP guarantee as vanilla global discrete Gaussian mechanism with the same parameters.
6 Communication Protocol & Utility Analysis
In this section, we present our communication protocol in detail and discuss the communication cost
and the estimation error of D2P-FED with direct comparison to cpSGD. The drastic improvement
of D2 P-Fed mainly comes from the tight composition of discrete Gaussian mechanism compared
to binomial mechanism in cpSGD.
6.1	Communication Protocol
As the first step, we leverage stochastic k-level quantization proposed by McMahan et al. (2016)
to lower the communication cost as described in line 12-14 in Algorithm 1. If we denote vanilla
6
Under review as a conference paper at ICLR 2021
stochastic k-level quantization with πk, then we successfully reduce the per-round communication
cost C(∏k, g[n]) = n ∙ (ddlog2 k] + O(1)).
However, stochastic quantization sacrifices some accuracy for communication efficiency. Con-
Cretely, E(∏k,g[n]) = O(d ∙ n pn=1 ∣∣g(i)k2). Since the dimension of parameters d is tens of
thousands to hundreds of thousands in federated learning, the estimation error of the mean is too
large. Thus to reduce the estimation error, we randomly rotate the difference vector (McMahan
et al., 2016) as the second step. The key intuition is that the MSE of stochastic uniform quanti-
zation is O(d(gmax)2). With random rotation, We can limit gmax to Jlogd w.h.p. so the MSE
will be improved to O(lon-d). Agarwal et al. (2018) also leverages random rotation to reduce MSE.
However, in their setting, random rotation intrinsically harms their privacy guarantee because the
'∞-sensitivity might increase with rotation. A natural advantage of discrete Gaussian is that the
privacy guarantee only depends on '2-sensitivity which is an invariant under rotation. Thus, random
rotation does not harm our privacy guarantee at all. We omit the details here and refer the interested
readers to McMahan et al. (2016). We denote the protocol using k-level quantization and random
rotation with πk(rot). We know that C(πk(rot), g[n]) remains the same while the MSE error is reduced
to E(∏krot),g[n])= O(等∙ n Pn=I kg(i)k2).
6.2	Convergence Rate of D2 P-Fed
In this section, we relate the convergence rate with mean squared error using Corollary 4 and analyze
the mean squared error of mean estimation in D2P-FED. Note that we assume each client executes
one iteration in each round so g equals to the gradient or belongs to the sub-gradients.
Corollary 4 (Ghadimi & Lan (2013)). Let F (w) = L(w, D) for some given distribution D. Let
F(W) be L-smooth and ∀w, ∣VF(w)∣ ≤ P. Let w0 satisfy F(w0) 一 F(w*) ≤ pf. Then after T
rounds	____
Et^(Unif(T))[∣VF(Wt)k2] ≤ 2ρFL +2√2λ√LpF + PB
TT
,where λ2 = maxι≤t≤τ 2E[∣g(wt) — VF (Wt)Il2] + 2max1≤t≤τ Eq [∣g(wt) — g(wt)∣2] ,and
B = maxι≤t≤τ Ilg(Wt) — g(wt)k.
As corollary 4 indicates, for a given gradient bound, the convergence rate approximately grows with
the reciprocal of MSE. Thus, we analyze D2P-FED’s MSE and obtain the following theorem. The
proof is delayed to Appendix D due to space limitation.
Theorem 4. Ifwe choose σ ≥ 1/y∕2π, the mean squared error is
1	4d(gmax)2 1 σ2
E (πk,q,NL(σ2), gn ) ≤ (I- 1 + 3e-2∏2σ2 (1 —φ(nq)))∙ n(k 一 1)2 (4+ Yn ) + (1 一φ(n(q-k-1))) ∙q
where Φ is the cumulative distribution function (CDF) of the standard normal distribution.
Remark 1: Choice of gmax. As indicated by Theorem 4, the dominant term in MSE is pro-
portional to the square of gmax. A natural choice of gmax is the clipping bound D . If we want
to match up with the MSE guarantee in cpSGD: O( ；(]”,), we need to inherit their choice
of gmax = O(D Jlog(d)), and this can be achieved by clipping l∞ norm of the gradient after
random rotation. For instance, according to Lemma 8 in Agarwal et al. (2018), we can choose
~ …	2∖∕log( 2nd )D _ .	.	............. C	.	~….
gmax = -ɪ——匕δ	. In that case, the possibility that the maximum of g exceeds gmax is at most
δ. It follows that the possibility that the '∞-clipping really changes the update is bounded by δ.
Hence, the RHS of the MSE bound in Theorem 4 evolves to (1 —(1 — Φ(nq)) — δ) ∙
4d(g-1)2 (I + Yσn2 ) + (I - φ(n(q - k - 1)) + δ)
• q2, which is on the same order with the original
bound given δ is small.
Remark 2: Comparison with cpSGD. As the MSE bound is on the same order with cpSGD (even
the constants are close!), a natural question is ”what is the advantage of D2P-FED over cpSGD in
7
Under review as a conference paper at ICLR 2021
terms of MSE?” Indeed, the advantage stems from a smaller standard deviation of the noise. Given
a fixed privacy budget, due to the tighter composition of sub-sampled RDP (Wang et al., 2018), each
round of D2P-FED can have more privacy budget and thus smaller noise scale. Plugging a smaller
σ into Theorem 4 will give a better MSE and thus a better convergence rate as cpSGD follows a
similar convergence rate bound. Moreover, according to Figure 1 in Agarwal et al. (2018), even
with the same noise scale, Gaussian noise provides stronger privacy guarantee than Binomial noise.
As discrete Gaussian noise follows the same RDP bound as Gaussian noise, we believe discrete
Gaussian can map the same-scale noise to lower privacy cost.
6.3	Communication Cost of D2P-FED
First, we provide the trivial per-round communication cost which is exactly the same as cpSGD.
Theorem 5. The per-round communication cost of D2 P-FED is
C(πkqtNL(σ2), g[n])=n Yd log(nq+1)+O(I))
.
Now let’s compare the number of rounds in cpSGD and D2P-FED qualitatively. Note that during
the following discussion, we usually omit δ for the ease of clarification and assume that δ is fixed.
For cpSGD, the known tightest bound is by the combination of standard privacy amplification (Balle
et al., 2018) and advanced composition (Dwork et al., 2010). Concretely, if a mechanism costs pri-
vacy budget, then after composed with sub-sampling the privacy budget is reduced to O(γ) where
Y is the sub-sampling rate. If the mechanism is composed sequentially T times, the privacy budget
grows to O(PT log(1∕δ)e). Thus, the total privacy budget of CPSGD is O(YPT log(1∕δ)c). On the
other hand, D2P-FED provides a total privacy budget of O(YvTe), saving a factor of ,log(1∕δ).
Since δ is typically very small, the saving is quite significant. If the privacy budgets for the two
protocols are the same, then D2P-Fed can use noise with O(,log(1∕δ)) smaller scale than cpSGD
in each round. This will lead to a O(vzlog(1∕δ))-time faster convergence. Then for a given gradient
bound, D2P-FED can reach it with O(log(1∕δ))-time fewer rounds and thus save O(log(1∕δ))-time
communication cost.
Both D2P-FED and cpSGD intrinsically require secure aggregation to establish their privacy guar-
antee. Agarwal et al. (2018) did not discuss the issue explicitly. As pointed out in Bonawitz et al.
(2017), once combined with secure aggregation, each field has to expand at least Yn times (Yn is the
number of chosen clients) to prevent overflow of the sum.
7 Evaluation
We would like to answer the following three questions using empirical evaluation: (1) How D2P-
Fed performs in a multi-round federated learning compared to cpSGD under either the same privacy
guarantee or the same communication cost? (2) How different choices of hyper-parameters affect
the performance of D2P-FED? (3) Does D2P-FED work under heterogeneous data distribution?
Due to space limitation, we present our main results for (1) in this section and defer the results for
(2) and (3) to Appendix E.
7.1	Experiment Setup
To answer the above questions, we evaluated D2P-FED and cpSGD on INFIMNIST (Bottou (2007))
and CIFAR10 (Krizhevsky et al. (2009)). We sampled 10M hand-written digits from INFIMNIST
and randomly split the data among 100K clients. In each round, 100 clients are randomly chosen to
upload their difference vectors to train a three-layer MLP. For CIFAR10, we select 10 out of 2000
clients in each round to train a 2-layer convolutional network. All RDP bounds are converted to
(e, δ)-DP for the ease of comparison and the total δ is set to 1e-5 for all experiments.
7.2	Model Accuracy vs. Privacy Budget
To answer the first question, we studied model accuracy under the same privacy budget as shown in
Figure 1a. Compared with cpSGD, D2P-FED achieves 4.7% higher model accuracy on INFIMNIST
8
Under review as a conference paper at ICLR 2021
100
12
少
ycaruccA ledoM
ycaruccA ledo
100
Non-Private, 4 bytes
2	4	6	8	10
Communication Cost (mBytes)
Communication Cost (mBytes)
INFIMNIST	CIFAR10	INFIMNIST	CIFAR10
(a) Privacy Budget vs. Model Accuracy.	(b) Communication Cost vs. Model Accuracy.
Figure 1: D2P-FED vs. cpSGD.
and 13.0% higher model accuracy on CIFAR10 after convergence. As expected, D2P-FED com-
poses far tighter under sub-sampling as the lines are much sharper than those of cpSGD. Conse-
quently, D2P-FED converges at a smaller privacy budget than cpSGD as well. Although in Figure 1a
cpSGD has better accuracy in the high privacy region, it is not necessarily the case but depends on
the scale of the discrete Gaussian noise, as studied in Section E.1. Note that the results for cpSGD in
Figure 1 is different from the results in Figure 2 in the original paper (Agarwal et al., 2018). The rea-
son is that in the original paper, they do not sub-sample clients in each round but assign each client
to exactly one round beforehand to avoid composition which cpSGD cannot handle well. However,
the scheme is far from practical in the real world due to the dynamic nature of the clients.
7.3 Model Accuracy vs. Communication Cost
To answer the second question, we also studied the model accuracy under the same communication
cost. As shown in Figure 1b, D2P-FED consistently achieves better model accuracy under the
same communication cost on both INFIMNIST and CIFAR10. The main reason is that the tight
composition property allows D2P-FED to use smaller per-feature communication cost while still
achieving better accuracy. As a concrete instance, D2P-FED with 50% compression rate can achieve
better accuracy than cpSGD with 25% compression rate. cpSGD with 50% compression rate either
leads to an unacceptable privacy budget or does not converge.
8 Conclusion
In this work, we developed D2P-FED to achieve both differential privacy and communication ef-
ficiency in the context of federated learning. By applying the discrete Gaussian mechanism to the
private data transmission, D2P-FED provides stronger privacy guarantee, better composability and
smaller communication cost than the only prior work, cpSGD, both theoretically and empirically.
References
Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Brendan McMa-
han. cpsgd: Communication-efficient and differentially-private distributed sgd. In Advances in
Neural Information Processing Systems,pp. 7564-7575, 2018.
Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and
optimization. In Advances in neural information processing systems, pp. 1756-1764, 2015.
Maria Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, commu-
nication complexity and privacy. In Conference on Learning Theory, pp. 26-1, 2012.
Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight
analyses via couplings and divergences. In Advances in Neural Information Processing Systems,
pp. 6277-6287, 2018.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
9
Under review as a conference paper at ICLR 2021
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp.1175-1191, 2017.
Leon Bottou. The infinite mnist dataset. 2007.
Clement Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. arXiv preprint arXiv:2004.00010, 2020.
Jiecao Chen, He Sun, David Woodruff, and Qin Zhang. Communication-optimal distributed clus-
tering. In Advances in Neural Information Processing Systems, pp. 3727-3735, 2016.
Cynthia Dwork, Guy N Rothblum, and Salil Vadhan. Boosting and differential privacy. In 2010
IEEE 51st Annual Symposium on Foundations of Computer Science, pp. 51-60. IEEE, 2010.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Distributed learning without
distress: Privacy-preserving empirical risk minimization. In Advances in Neural Information
Processing Systems, pp. 6343-6354, 2018.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. arXiv preprint arXiv:1710.06963, 2017.
Ilya Mironov. Renyi differential privacy. In 2017 IEEE 30th Computer Security Foundations Sym-
posium (CSF), pp. 263-275. IEEE, 2017.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning:
Passive and active white-box inference attacks against centralized and federated learning. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 739-753. IEEE, 2019.
Anastasia Pustozerova and Rudolf Mayer. Information leaks in federated learning.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3-18. IEEE, 2017.
Ananda Theertha Suresh, X Yu Felix, Sanjiv Kumar, and H Brendan McMahan. Distributed mean
estimation with limited communication. In International Conference on Machine Learning, pp.
3329-3337, 2017.
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui Zhang, and
Yi Zhou. A hybrid approach to privacy-preserving federated learning. In Proceedings of the 12th
ACM Workshop on Artificial Intelligence and Security, pp. 1-11, 2019.
John N Tsitsiklis and Zhi-Quan Luo. Communication complexity of convex optimization. Journal
of Complexity, 3(3):231-243, 1987.
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.
Federated learning with matched averaging. arXiv preprint arXiv:2002.06440, 2020.
Yu-Xiang Wang, Borja Balle, and Shiva Kasiviswanathan. Subsampled r\’enyi differential privacy
and analytical moments accountant. arXiv preprint arXiv:1808.00087, 2018.
Wikipedia. Theta function — wikipedia, the free encyclopedia. URL https://en.
wikipedia.org/wiki/Theta_function.
10
Under review as a conference paper at ICLR 2021
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor attacks against feder-
ated learning. In International Conference on Learning Representations, 2019.
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learn-
ing: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations
Symposium (CSF), pp. 268-282. IEEE, 2018.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, and Nghia Hoang. Sta-
tistical model aggregation via parameter matching. In Advances in Neural Information Processing
Systems, pp. 10956-10966, 2019a.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Trong Nghia Hoang,
and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. arXiv
preprint arXiv:1905.12022, 2019b.
Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic
lower bounds for distributed statistical estimation with communication constraints. In Advances
in Neural Information Processing Systems, pp. 2328-2336, 2013.
A	Proof for Theorem 1
We consider the Renyi divergence between two discrete Gaussian distributions differing in the mean
value.
Proof.
Da(NL(0,σ2)kM(μ,σ2))
=) -ɪj-logXP---------~~∖2](,> 2、、eχp(-αx3(2σ2)) ∙ eχp(-(1 - α)(x - μ)2/(2σ2))
α - 1 L ∑Leχp(Tx - 〃)2/(2b2))
=0-1 log PLexp(-(x - μ)2∕(2σ2)) Xexp((-x2 + 2(I - α)μx - (1 - α)μ2M2σ2))
(2)	1
≤ 0-ɪ log{exp((α2 - α)μ2∕(2σ2))}
=αμ2∕(2σ2)
□
(1) Because range(f) ⊆ L, we only consider μ ∈ L. Thus the denominator of Nl(0,。2) and
NL(μ,σ2) cancels out as PLexp(-(x - μ)2∕(2σ2)) is periodic. (2)
PL exP(-(X-μ)2^2σ2))
PL exP(-(x-*)2/(2b2))
√∏户((1-α)πμ,e-π2 )
^01)
≤ 1 where H is the Jacobi theta function (Wikipedia).
B Proof for Theorem 2
Proof. The l2 sensitivity of δ is naturally bounded by 2D. The rotation does not change the sensi-
tivity. The k-level quantization might expand the space as shown in Figure 2. An upper bound on
the radius of the red circle is D + √dɪ-ɪ. When we take k = √d - 1, it reduces to 2D. Thus, the
upper bound on the sensitivity is 4D.	□
C Proof for Theorem 3
We first prove the following lemma.
11
Under review as a conference paper at ICLR 2021
Figure 2: Sensitivity after Quantization.
Lemma 3.
φq(x) = φq( x)
Proof.
X	X 2gmax k - 1	, q - 1
Tφq3 =工 ⅛-Γ((如XX + ɪ
_ 2gmax∕4-1 V J -1
一k - 1((2gmax Tx	2
= φq( x)
mod q -
mod q -
q - 1
2
q - 1
2
)
)


□
Then we prove Theorem 3 as follows.
Proof. Given Lemma 3,
gt=k X g(i)
i∈S
=k X φq(φq(g(idP) +	X	Uij-	X	Uji)
i∈S	j6=i,j∈S	j 6=i,j∈S
=k φq (Xmq (g(idp) +	X	Uij-	X	Uji))
i∈S	j 6=i,j∈S	j6=i,j∈S
=k φq (X g(idp)
i∈S
The expression P∈ g(idp forms a centralized discrete Gaussian mechanism and according to the
post-processing theorem, the same RDP guarantee still holds.	□
D Proof for Theorem 4
Proof Sketch. Before starting the proof, we introduce two lemmas for the proof.
Lemma 4 (Proposition 19 from Canonne et al. (2020)). For all σ ∈ R with σ > 0,
4π2σ2
V[Nz(0,σ2)] ≤ σ2(1- -^-ι) <σ2
πσ-
Moreover, if σ2 ≤ 1 ,then V[Nz(0, σ2)] ≤ 3 ∙ e-212
12
Under review as a conference paper at ICLR 2021
Lemma 5 (Proposition 23 from Canonne et al. (2020)). For all m ∈ Z with m ≥ 1 and all σ ∈ R
with σ > 0,
PX〜Nz(0,σ2)X ≥ m] ≤ PX〜N(0,σ2)X ≥ m - 1]∙
Moreover, if σ ≥ 1/λ∕2π, we have
PX〜Nz(0,σ2)[X ≥ m] ≥
1 + 3e-2∏2σ2 PX〜N(0,σ2)[X ≥ m]
Now we start our proof. MSE can be rewritten in the following format.
dn
—	—	j-,	I V-n. V-n.	—	C
E[kX - Xk2∣]=..卒[(Xi(j) - Xi(j))2]
「	i	TT∏Γ∕ -ð- / .∖ V / .∖∖91	1 ,	∙ 1	T，	ι
For each ? = E[(Xi(j) - Xi(j))2] , We need to consider two cases. If no overflow happens, due to
Lemma 4
2Xmax	4(Xmax)2 1	σ2
?-o ≤ E[(τ-Γ)2(V(Ber(Pi(j))) + V(NL(σ)))] ≤ ⅛-1)T勺 + Yn)
If overflow happens, trivially we have ?o ≤k2 . Thus, we have
? = P[-o] ∙ ?-o + P[o] ∙ ?o ≤ PX〜NL(σ2)[X ≤ q] ∙ ?-o + PX〜NL(σ2)[X ≥ q - k] ∙ ?。	(1)
With Lemma 5, we can bound the probabilities and get the final MSE result in the theorem.
□
E Other Evaluation
E.1 Influence of Noise Scale
Budget
Figure 3: D2P-FED under different σ.
The hyper-parameter of the most vital interest in D2P-FED is the scale of the noise. To understand
the effect of the noise scale, we evaluated D2P-FED on INFIMNIST, with 3 different choices of noise
scale as shown in Figure 3. It is no surprise that the higher the noise scale, the smaller the privacy
budget and the lower the model accuracy. This also illustrates the claim we have in Section 7.2 that
D2P-FED can also have relatively good performance in the high privacy region at the cost of the
model accuracy.
E.2 Influence of Heterogeneous Data Distribution
It is well known that data is sometimes heterogeneously distributed among clients in a federated
learning system. Thus, to better understand D2P-FED’s behavior under heterogeneous data distribu-
tion, we simulated heterogenenous data distribution by distributing the INFIMNIST data according
to the classes and evaluated D2P-FED on these clients. The results are shown in Figure 4 and we
can see that under heterogeneous data distribution the model accuracy drops by more than 10%.
This complies with the previous empirical results and there have been a line of researches focusing
on addressing the issue (Yurochkin et al., 2019a;b; Wang et al., 2020). Although orthogonal to this
paper, we deem it as an interesting open problem how to integrate these works with D2P-FED.
13
Under review as a conference paper at ICLR 2021
0000
0864
1
ycaruccA ledoM
Figure 4: D2P-FED under homogeneous/heterogeneous distribution.
0
3.5
4	4.5	5	5.5
Budget
E.3 INFLUENCE OF GROUP SIZE q
We also ran D2P-FED with multiple choices of discrete group size q. We observe that once the noise
scale σ is fixed, the performance is relatively robust to q.
- D2P-FED (σ = 0.6), 2 bytes
∙→~ D2P-Fed (σ = 0.6), 3 bytes
→- D2P-Fed (σ = 0.6), 4 bytes
— Non-Private, 4 bytes
20	40	60	80
#Round
Figure 5: D2P-FED under different group size q.
14