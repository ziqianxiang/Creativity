Under review as a conference paper at ICLR 2021
Natural Compression for Distributed Deep
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Modern deep learning models are often trained in parallel over a collection of
distributed machines to reduce training time. In such settings, communication
of model updates among machines becomes a significant performance bottleneck
and various lossy update compression techniques have been proposed to alleviate
this problem. In this work, we introduce a new, simple yet theoretically and
practically effective compression technique: natural compression (Cnat). Our
technique is applied individually to all entries of the to-be-compressed update
vector and works by randomized rounding to the nearest (negative or positive)
power of two, which can be computed in a “natural” way by ignoring the mantissa.
We show that compared to no compression, Cnat increases the second moment of
the compressed vector by not more than the tiny factor 9/8, which means that the
effect of Cnat on the convergence speed of popular training algorithms, such as
distributed SGD, is negligible. However, the communications savings enabled by
Cnat are substantial, leading to 3-4× improvement in overall theoretical running
time. For applications requiring more aggressive compression, we generalize Cnat
to natural dithering, which we prove is exponentially better than the common
random dithering technique. Our compression operators can be used on their own
or in combination with existing operators for a more aggressive combined effect,
and offer new state-of-the-art both in theory and practice.
1 Introduction
Modern deep learning models (He et al., 2016) are almost invariably trained in parallel or distributed
environments, which is necessitated by the enormous size of the data sets and dimension and
complexity of the models required to obtain state-of-the-art performance. In our work, the focus is
on the data-parallel paradigm, in which the training data is split across several workers capable of
operating in parallel (Bekkerman et al., 2011; Recht et al., 2011). Formally, we consider optimization
problems of the form
min f (x) :
x∈Rd
n
1 P fi(x),
i=1
(1)
where x ∈ Rd represents the parameters of the model, n is the number of workers, and fi : Rd → R is
a loss function composed of data stored on worker i. Typically, fi is modeled as a function of the form
fi(x) := EZ〜Di [fζ(x)], where Di is the distribution of data stored on worker i, and fζ : Rd → R is
the loss of model x on data point ζ . The distributions D1, . . . , Dn can be different on every node,
which means that the functions f1, . . . , fn may have different minimizers. This framework covers i)
stochastic optimization when either n = 1 or all Di are identical, and ii) empirical risk minimization
when fi(x) can be expressed as a finite average, i.e, m1- Pm=I fj (x) for some fij : Rd → R.
Distributed Learning. Typically, problem (1) is solved by distributed stochastic gradient descent
(SGD) (Robbins & Monro, 1951), which works as follows: Stochastic gradients gi (xk)’s are com-
puted locally and sent to a master node, which performs update aggregation gk = Pi gi (xk).
The aggregated gradient gk is sent back to the workers and each performs a single step of SGD:
χk+1 = Xk - ηkgk, where ηk > 0 is a step size.
A key bottleneck of the above algorithm, and of its many variants (e.g., variants utilizing mini-
batching (Goyal et al., 2017), importance sampling (Horvgth & Richtdrik, 2019), momentum (NeS-
terov, 2013), or variance reduction (Johnson & Zhang, 2013)), is the cost of communication of the
1
Under review as a conference paper at ICLR 2021
typically dense gradient vector gi(xk), and in a parameter-sever implementation with a master node,
also the cost of broadcasting the aggregated gradient gk . These are d dimensional vectors of floats,
with d being very large in modern deep learning. It is well-known (Seide et al., 2014; Alistarh et al.,
2017; Zhang et al., 2017; Lin et al., 2018; Lim et al., 2018) that in many practical applications with
common computing architectures, communication takes much more time than computation, creating
a bottleneck of the entire training system.
Communication Reduction. Several solutions were suggested in the literature as a remedy to this
problem. In one strain of work, the issue is addressed by giving each worker “more work” to do,
which results in a better communication-to-computation ratio. For example, one may use mini-
batching to construct more powerful gradient estimators (Goyal et al., 2017), define local problems
for each worker to be solved by a more advanced local solver (Shamir et al., 2014; Richtdrik &
Takdc, 2016; Reddi et al., 2016), or reduce communication frequency (e.g., by communicating only
once (McDonald et al., 2009; Zinkevich et al., 2010) or once every few iterations (Stich, 2018)).
An orthogonal approach to the above efforts aims to reduce the size of the communicated vectors
instead (Seide et al., 2014; Alistarh et al., 2017; Wen et al., 2017; Wangni et al., 2018; Hubara et al.,
2017) using various lossy (and often randomized) compression mechanisms, commonly known in the
literature as quantization techniques. In their most basic form, these schemes decrease the # bits used
to represent floating point numbers forming the communicated d-dimensional vectors (Gupta et al.,
2015; Na et al., 2017), thus reducing the size of the communicated message by a constant factor.
Another possibility is to apply randomized sparsification masks to the gradients (Suresh et al., 2017;
KOneCny & Richtdrik, 2018; Alistarh et al., 2018; Stich et al., 2018), or to rely on coordinate/block
descent updates-rules, which are sparse by design (Fercoq et al., 2014).
One of the most important considerations in the area of compression operators is the compression-
variance trade-off (KOneCny & Richtdrik, 2018; Alistarh et al., 2017; Horvdth et al., 2019). For
instance, while random dithering approaches attain up to O(d1/2) compression (Seide et al., 2014;
Alistarh et al., 2017; Wen et al., 2017), the most aggressive schemes reach O(d) compression by
sending a constant number of bits per iteration only (Suresh et al., 2017; KOneCny & Richtdrik,
2018; Alistarh et al., 2018; Stich et al., 2018). However, the more compression is applied, the more
information is lost, and the more will the quantized vector differ from the original vector we want
to communicate, increasing its statistical variance. Higher variance implies slower convergence
(Alistarh et al., 2017; Mishchenko et al., 2019), i.e., more communication rounds. So, ultimately,
compression approaches offer a trade-off between the communication cost per iteration and the
number of communication rounds.
Outside of the optimization for machine learning, compression operators are very relevant to optimal
quantization theory and control theory (Elia & Mitter, 2001; Sun & Goyal, 2011; Sun et al., 2012).
Summary of Contributions. The key contributions of this work are following:
•	New compression operators. We construct a new “natural compression” operator (Cnat ; see
Sec. 2) based on a randomized rounding scheme in which each float of the compressed vector is
rounded to a (positive or negative) power of 2. This compression has a provably small variance,
at most 1/8 (see Thm 1), which implies that theoretical convergence results of SGD-type methods
are essentially unaffected (see Thm 6). At the same time, substantial savings are obtained in the
amount of communicated bits per iteration (3.56× less for float32 and 5.82× less for float64). In
addition, we utilize these insights and develop a new random dithering operator—natural dithering
(Dnp,ast ; see Sec. 3)—which is exponentially better than the very popular “standard” random dithering
operator (see Thm 5). We remark that Cnat and the identity operator arise as limits of Dnp,ast and Dspt,as
as s → ∞, respectively. Importantly, our new compression techniques can be combined with existing
compression and sparsification operators for a more dramatic effect as we argued before.
•	State-of-the-art compression. When compared to previous state-of-the-art compressors such as
(any variant of) sparsification and dithering—techniques used in methods such as Deep Gradient
Compression (Lin et al., 2018), QSGD (Alistarh et al., 2017) and TernGrad (Wen et al., 2017)—our
compression operators offer provable and often large improvements in practice, thus leading to new
state of the art. In particular, given a budget on the second momentω+1 (see Eq (3)) of a compression
operator, which is the main factor influencing the increase in the number of communications when
communication compression is applied compared to no compression, our compression operators offer
the largest compression factor, resulting in fewest bits transmitted (see Fig 1).
•	Lightweight & simple low-level implementation. We show that apart from a randomization pro-
cedure (which is inherent in all unbiased compression operators), natural compression is computation-
free. Indeed, natural compression essentially amounts to the trimming of the mantissa and possibly
2
UnderreVieW as a COnferenCe PaPer af ICLR 2021
FigUre I: COmmuniCatiOn (in bits) v∙the SeCOnd moment E + 1 (See Eq (3))
for severState—of—the&rt COmPreSSOrS applied to a gradient Of SiZe d H IOOur
methods (CnaCand 0) are depicted With a SqUare marker For any fixed com—
munication budgeLnatUrdithering OfferS an exponentiimprovement On Standard
ditheringy and When USed in COmPOSitiOn With SParSifiCatiOF it OKerS an Order Of ma
nitude improvement"
FigUre 2: An illustration Of na∙COmPreSSiOn a'
plied to Z U 25 CnaC(2∙5) U 2 with Probab
ity u75" and Cnac (2∙5) U 4 with prob∙
—2 H2ThiS ChOiCe Of ProbabiHtieS en—
SureS that the COmPreSSiOn OPeratOriS UnbiaSeP i∙e:
Enac (Z)-川
increasing fhe exponenf by One∙ ThiSiSfhe Hrsf COmPreSSiOn mechanism Wifh SUCh a Ccnafural:
COmPafib≡fy Wifil binary Aoafing POin二 ypes∙
•	Proofloflconcepf SySfem 三fh in—nefwork aggregation (INA)∙ The recently PrOPOSed
SWifChML (SaPiO ef ar 2019) SySfem alleviafes fhe COmmUniCafionbOaeneck Via in—nefwork
aggregation (s.A) Of gradiems∙∞'nce CUrrem PrOgrammabIe IIefWOrk SWifCheS are OnlyCaPabIe
Of adding imeger∞IleW UPdafe COmPreSSionmefhOdS are IIeeded WhiCh Can SUPPly OUfPUfSin an
insger formaL OUr natural COmPreSSion mechanism is rhe Hrs二ha〔 is PrOVabIy able〔0 OPerare in rhe
SWirChML framework as r COmmUniCareSinregerS Only二he signy PIUS rhe brs forming rhe exponent:
Of a floaL MOreOVeL having bounded (and small) VarianC 尸 ifis COmPafibIe Wifh existing distɪibufed
ITaining methods∙
•	BidireCdOnal COmPreSSiOn for SGD∙ We PrOVide COnVergenCe Theory for disabufed SGD WhiCh
allows for COmPreSSioTI both at the WOrker and masfer side (See AlgOrifhm I)∙ The COmPreSSion
OPerarorS ComParibIe Wirh OUrrheory form a Iarge famiIy (OPerarorS C ∈ JB(E) for Somennire E ≥ O;
See Dennifion 2)∙ ThiS enables Safe experimenfafion Wifh existing and faci≡afes fhe development: Of
new COmPreSSiOn OPerarOrS nne—runed〔0 specific deep Ieaming model archirecrures∙ OUr COnVergenCe
resulf (Thm 1) appHeSfO SmOOfhand IIon—convex functions and OUrrafeS PrediCfHnear SPeed—up
Wifh respec二Ofhe IlUmber Of machines-
•	BefferfOfal COmPleXiE MOSrimPOrrantly. We are rhe Hrs二O ProVe rha二he increase in rhe number
OfiferatiOnS CaUSed by (a CarefU=y designed) COmPreSSiOn is more Than COmPenSafed by fhe SaVingS
in COmmUniCafiOFWhiChleadS S an OVeralI PrOVabIe SPeedUP in ITaining Iime∙ Read Thm 6二he
discussion following rhe theorem and TabIe IfOr more derails。Torhe besr Of OUr knowledge。standard
difher≡∙g (QSGD (AHSfarh ef al: 2017)) is fhe Only PreViOUSly known COmPreSSion fechnique able
S achieve This Wifh OUr distɪibufed SGD Wifh bi—direcfional compression Imporfantly" our IIafUraI
difhering is exponentially better Than Sfandard difher≡∙g3 and hence PrOVideSfOr Sfafe—of—fhe —arf
PerformanCe in ConneCfion Wifh AIgorifhm L
•	EXPerimenfS∙ We ShOW fhaf CnaC SigninCantIyredUCeS fhe ITa≡∙ing Iime COmPared S IIO COmPres—
SiOiL We PrOVide empirical evidence in rhe form SCaIing experimenFshowing rhar Cnax does nor hurC
COnVergenCeWhen fhe number Of WOrkerSiS growing。We also ShOW fhaf POPUlar COmPreSSiOn mefh—
OdS SUCh as random SParS≡cafion and random difher≡∙g are enhanced by COmbination Wifh natural
COmPreSSiOn Or IIafUraI difhering (See APPendiX A) ∙ The COmbined COmPreSSiOn Technique reduces
fhe IlUmber Of COmmUniCafiOn rounds WifhOUf any IlOfiCeabIe impacf On ConVergenCe PrOVid≡∙g fhe
Same qualify SoIUtion∙
2 NATURAL CoMPRESSIoN
We define a IleW (randomized) COmPreSSionfeChniqUe∙ which We CaII nafura一 COWreSs§• This
is fundamenfalIyafUnCtiOnmaPPing f ∈ IR fo a random VariabIe CnaX (E) m IR∙ In CaSe Of VeCfOrS
2 = (21: ∙二 2d) ∈ IRd We apply ifin an elemem—wise fashion: (CnaX (2))2 = CnaX (20) ∙ NafUraI
COmPreSSiOn CnaC PerfOrmS a randomized IOgarifhmiC rounding of if s≡∙puf f ∈ IR∙ GiVen IIOnZerO t.
Iefa ∈ IR be SUCh fhaf 三=2Q (Le; α =og2 三)∙ Then 2P」≤ 三=2Q ≤ 2p- and We round - fo
eifher Sign(E) 2P」“ Or fo Sign(E) 2p一 ∙ When f =pwe Sef CnaX(O) H 0∙ The PrObab≡fies are ChoSen
SOfhaf CnaX3is an UnbiaSed esfimafor Of E i∙e∙3 E - CnaX3- Hf for all f ∙ ForinSfanCe" t H ——2.75
will be rounded S either ——4 Or ——2 (SinCe ——22 ≤ ——2.75 ≤ ——21L and f = O∙75 will be rounded S
either 1/2 or 1 (SinCe 2——1 ≤ O∙75 ≤ 20) ∙ AS a consequences if f is an infeger POWer Of 2=hen CnaX
w≡leave t UnChanged∙ See Fig∙ 2∙
Under review as a conference paper at ICLR 2021
Definition 1 (Natural compression). Natural compression is a random function Cnat : R 7→ R defined
as follows. We set Cnat(0) = 0. If t 6= 0, we let
Cnat (t) :
ʃsign(t) ∙ 2blog2|t|c,
jsign(t) ∙ 2dlog2 |t|e,
with p(t),
with 1 - p(t),
(2)
2dlog2 |t|e -|t|
where probability p(t) :=	21^2 ∣t-1 1
Alternatively, (2) can be written as Cnat(t) = sign(t) ∙ 2blog2|t|c (1 + λ(t)), where λ(t)〜
Bernoulli(1 - p(t)); that is, λ(t) = 1 with prob. 1 - p(t) and λ(t) = 0 with prob. p(t). The
key properties of any (unbiased) compression operator are variance, ease of implementation, and
compression level. We characterize the remarkably low variance of Cnat and describe an (almost)
effortless and natural implementation, and the compression it offers in rest of this section.
Cnat has a negligible variance: ω = 1/8. We identify natural compression as belonging to a large
class of unbiased compression operators with bounded second moment (Jiang & Agrawal, 2018;
Khirirat et al., 2018; HorVgth et al., 2019), defined below.
Definition 2 (Compression operators). A function C : Rd → Rd mapping a deterministic input to a
random vector is called a compression operator (on Rd). We say that C is unbiased and has bounded
second moment (ω ≥ 0) if
E[C(x)] = x,	E kC(x)k2 ≤ (ω+ 1) kxk2	∀x ∈ Rd.	(3)
If C satisfies (3), we will write C ∈ B(ω).
Note that ω = 0 implies C(x) = x almost surely. It is easy to see that the variance ofC(x) ∈ B(ω) is
bounded as: E ∣∣C(x) - x『≤ ω ∣∣xk2. If this holds, we say that “C has variance ω'”. The importance
of B(ω) stems from two observations. First, operators from this class are known to be compatible
with several optimization algorithms (Khirirat et al., 2018; Horvgth et al., 2019). Second, this class
includes most compression operators used in practice (Alistarh et al., 2017; Wen et al., 2017; Wangni
et al., 2018; Mishchenko et al., 2019). In general, the larger ω is, the higher compression level might
be achievable, and the worse impact compression has on the convergence speed.
The main result of this section says that the natural compression operator Cnat has variance 1/8.
Theorem 1.	Cnat ∈ B(1/8).
Consider now a similar unbiased randomized rounding operator to Cnat ; but one that rounds to one of
the nearest integers (as opposed to integer powers of 2). We call it Cint. At first sight, this may seem
like a reasonable alternative to Cnat . However, as we show next, Cint does not have a finite second
moment and is hence incompatible with existing optimization methods.
Theorem 2.	There is no ω ≥ 0 such that Cint ∈ B(ω).
From 32 to 9 bits, with lightning speed. We now explain that performing natural compression of a
real number in a binary floating point format is computationally cheap. In particular, excluding the
randomization step, Cnat amounts to simply dispensing off the mantissa in the binary representation.
The most common computer format for real numbers, binary32 (resp. binary64) of the IEEE 754
standard, represents each number with 32 (resp. 64) bits, where the first bit represents the sign,
8 (resp. 11) bits are used for the exponent, and the remaining 23 (resp. 52) bits are used for the
mantissa. A scalar t ∈ R is represented in the form (s, e7, e6, . . . , e0, m1, m2, . . . , m23), where
7
s, ei, mj ∈ {0, 1} are bits, via the relationship t = (-1)s × 2e-127 × (1 + m), e = P ei2i, m =
i=0
23
mj 2-j , where s is the sign, e is the exponent and m is the mantissa. A binary32 representation
j=1
of t = -2.75 is visualized in Fig 4. In this case, s = 1, e7 = 1, m2 = m3 = 1 and hence
t = (-1)s × 2e-127 × (1+m) = -1 × 2 × (1+2-2 +2-3) = -2.75.
It is clear that 0 ≤ m < 1, and hence 2e-127 ≤ |t| < 2e-126. Moreover, p(t)
2e-126-∣t∣
2eτ27
2 - |t|2127-e = 1 - m. Hence, natural compression oft represented as binary32 is given as follows:
Cnat(t)
ʃ(-1)s X 2e-127,
(-1)s × 2e-126,
with probability 1 - m,
with probability m.
4
Under review as a conference paper at ICLR 2021
Figure 3: Randomized rounding for natural (left)
and standard (right) dithering (s = 3 levels).
1	* ⅛
Z 一 一 τ......................:
11	BBIIIIMMMM-ImiM■■■■■■■■■■■ = -2.75
Figure 4: IEEE 754 single-precision binary
floating-point format: binary32.
Observe that (-1)s × 2e-127 is obtained from t by setting the mantissa m to zero, and keeping both
the sign s and exponent e unchanged. Similarly, (-1)s × 2e-126 is obtained from t by setting the
mantissa m to zero, keeping the sign s, and increasing the exponent by one. Hence, both values can
be computed from t essentially without any computation.
Communication savings. In summary, in case of binary32, the output Cnat(t) of natural compres-
sion is encoded using the 8 bits in the exponent and an extra bit for the sign. This is 3.56× less
communication. In case of binary64, we only need 11 bits for the exponent and 1 bit for the sign, and
this is 5.82× less communication.
Compatibility with other compression techniques We start with a simple but useful observation
about composition of compression operators.
Theorem 3. If C1 ∈ B(ω1) and C2 ∈ B(ω2), then C1 ◦ C2 ∈ B(ω12), where ω12 = ω1ω2 + ω1 + ω2,
and C1 ◦ C2 is the composition defined by (C1 ◦ C2)(x) = C1 (C2(x)).
Combining this result With Thm. 1, We observe that for any C ∈ B(ω), We have Cnat◦C ∈ B(9ω∕8+1∕8).
Since Cnat offers substantial communication savings with only a negligible effect on the variance of
C , a key use for natural compression beyond applying it as the sole compression strategy is to deploy
it With other effective techniques as a final compression mechanism (e.g., With sparsifiers (Stich et al.,
2018)), boosting the performance of the system even further. HoWever, our technique Will be useful
also as a post-compression mechanism for compressions that do not belong to B(ω) (e.g., TopK
sparsifier (Alistarh et al., 2018)). The same comments apply to the natural dithering operator Dnp,ast ,
defined in the next section.
3	Natural Dithering
Motivated by the natural compression introduced in Sec 2, here We propose a neW random dithering
operator Which We call natural dithering. HoWever, it Will be useful to introduce a more general
dithering operator, one generalizing both the natural and the standard dithering operators. For
1 ≤ p ≤ +∞, let kxkp be p-norm: kxkp := (Pi |xi|p)1/p.
Definition 3 (General dithering). The general dithering operator With respect to the p norm and With
s levels 0 =ls < ls-1 < ls-2 < …< l1 <10 = 1, denoted。£滥，,is defined as follows. Let
X ∈ Rd. If X = 0, we let DCe,n,s(x) = 0. If x = 0, we let yi := ∣xi∣∕kx∣∣p for all i ∈ [d]. Assuming
lu+1 ≤ yi ≤ lu for some U ∈ {0,1,...,s - 1}, we let (。£滥%力))勿=C(IlXllP) × Sign(Xi) × ξ (yi),
where C ∈ B(ω) for some ω ≥ 0 and ξ(yi) is a random variable equal to lu with probability yi-lu+；,
and to lu+1 with probability llu--y11. Note that E [ξ(yi)] = yi.
Standard (random) dithering, Dspt,as, (Goodall, 1951; Roberts, 1962) is obtained as a special case of
general dithering (which is also novel) for a linear partition of the unit interval, ls-1 = 1∕s, ls-2 = 2∕s,
. . . , l1 = (s-1)∕s and C equal to the identity operator. Ds2t,as operator was used in QSGD (Alistarh et al.,
2017) and Ds∞ta,1 in Terngrad (Wen et al., 2017). Natural dithering—a novel compression operator
introduced in this paper—arises as a special case of general dithering for C being an identity operator
and a binary geometric partition of the unit interval: ls-1 = 21-s, ls-2 = 22-s, . . . , l1 = 2-1. For
the INA application, we apply C = Cnat to have output always in powers of 2, which would introduce
extra factor of 9∕8 in the second moment. A comparison of the ξ operators for the standard and natural
dithering with s = 3 levels applied to t = 3∕8 can be found in Fig 3. When DgCe,pn,s is used to compress
gradients, each worker communicates the norm (1 float), vector of signs (d bits) and efficient encoding
of the effective levels for each entry i = 1, 2, . . . , d. Note that Dnp,ast is essentially an application of
Cnat to all normalized entries of X, with two differences: i) we can also communicate the compressed
norm IXIp , ii) in Cnat the interval [0, 21-s] is subdivided further, to machine precision, and in this
5
Under review as a conference paper at ICLR 2021
Approach	CWi	No. iterations T 0(ωw) = O((ωw + 1)θ)	Bits per 11ter. Wi → M	Speedup Factor
Baseline New	-identity- Cnat	1 (9∕8)θ	32d 	9d		1 3.2x-3.6x
Sparsification New	Sq Cnat ◦ Sq	(d∕qp 	(9d∕8q)θ		(33 + log2 d)q (10 + log2 d)q	0.6×-6.0× 1.0x-1O7x
Dithering New	s-1 Dsta Dpat	(1 + κd1∕r21-s)θ (9/8 + κd121-s)θ	31 + d(2 + s) 31 + d(2 + log2 s)	1.8x-15.9x 4.1x-16.0x
Table 1: The overall speedup of distributed SGD with compression on nodes via CWi over a Baseline variant without compression. Speed is
measured by multiplying the # communication rounds (i.e., iterations T (ωW )) by the bits sent from worker i to master (Wi 7→ M) per 1
iteration. We neglect M 7→ Wi communication as in practice this is often much faster (see e.g. (Mishchenko et al., 2019), for other cost/speed
model see Appendix D.7). We do not just restrict to this scenario and . We assume binary32 representation. The relative # iterations sufficient
to guarantee ε optimality is T0 (ωW ) := (ωW + 1)θ, where θ ∈ (0, 1] (see Thm 6). Note that in the big n regime the iteration bound
T(ωW) is better due to θ ≈ 0 (however, this is not very practical as n is usually small), while for small n we have θ ≈ 1. For dithering,
r = min{p, 2}, κ
The Speedup Factor
respect to speedup.
=min{1, √d21-s}. The lower bound for the Speedup Factor is obtained for θ = 1, and the upper bound for θ = 0.
T (ωw >#Bits、
T (0)∙32d J
figures were calculated for d
106, q = 0.1d (10% sparsity), p = 2 and optimal choice of s with
sense Dnp,ast can be seen as a limited precision variant ofCnat. As is the case with Cnat, the mantissa is
ignored, and one communicates exponents only. The norm compression is particularly useful on the
master side since multiplication by a naturally compressed norm is just summation of the exponents.
The main result of this section establishes natural dithering as belonging to the class B(ω):
Theorem 4.	Dnat ∈ B(ω), where ω = 1/8 + d1/r21-s min {1, d1/r21-s} , and r = min{p, 2}.
To illustrate the strength of this result, we now compare natural dithering Dnp,ast to standard dithering
Dspt,as and show that natural dithering is exponentially better than standard dithering. In particular,
for the same level of variance, Dnp,ast uses only s levels while Dspt,au uses u = 2s-1 levels. Note also
that the levels used by Dnp,ast form a subset of the levels used by Dspt,as (see Fig 22). We also confirm
this empirically (see Appendix A.4).
Theorem 5.	Fixing s, natural dithering Dnp,ast has O(2s-1/s) times smaller variance than standard
dithering Dpta. Fixing ω, if U = 2s-1 ,then Dsta ∈ B(ω) implies that Dpat ∈ B(9∕8(ω + 1) - 1).
4 Distributed SGD
There are several stochastic gradient-type methods (Robbins & Monro, 1951; Bubeck et al., 2015;
Ghadimi & Lan, 2013; Mishchenko et al., 2019) for solving (1) that are compatible with compression
operators C ∈ B(ω), and hence also with our natural compression (Cnat) and natural dithering
(Dnp,ast ) techniques. However, as none of them support compression at the master node we propose a
distributed SGD algorithm that allows for bidirectional compression (Algorithm 1 in Appendix D.1).
We note that there are two concurrent papers to ours (all appeared online in the same month and
year) proposing the use of bidirectional compression, albeit in conjunction with different underlying
algorithms, such as SGD with error feedback or local updates (Tang et al., 2019; Zheng et al., 2019).
Since we instead focus on vanilla distributed SGD with bidirectional compression, the algorithmic
part of our paper is complementary to theirs. Moreover, our key contribution—the highly efficient
natural compression and dithering compressors—can be used within their algorithms as well, which
expands their impact further.
We assume repeated access to unbiased stochastic gradients gi(xk) with bounded variance σi2 for
every worker i. We also assume node similarity represented by constant ζi2 , and that f is L-smooth
(gradient is L-Lipschitz). Formal definitions as well as detailed explanation of Algorithm 1 can be
found in Appendix D. We denote Z2 = 1 P21 Z2, σ2 = 1 Pn=I σf and
(ωM+I)(Iωw+I) σ
n
+
β = 1 +ωM +
α
(∣m + 1)ωw
n
(4)
where CM ∈ B(ωM) is the compression operator used by the master node, CWi ∈ B(ωWi ) are the
compression operators used by the workers and ωW := maxi∈[n] ωWi .
6
Under review as a conference paper at ICLR 2021
Sso- 6u-us∙l1-
ωωo- 6u-u-eJJ.
1.0
AO.8
∣0.6
«0.4
O 10 20 30 40 50 60 70 80 90 IOOllO
Time(min)
0	40 80 120 160 2QQ 240 28。320
Epochs
I1-5
11.0
O 10 20 30 40 50 60 70 80 90 IOO IlO
TIme(S)
GIat
No Compression
2.94x faster
0 20 40 60 80 100120140160180 200	0 300 600 900 1200 1500 1800 2100
Epochs	Time(S)
0	300 600 900 1200 1500 1800 2100
Time(S)
Figure 5: Train Loss and Test Accuracy of ResNet110 and Alexnet on CIFAR10. Speed-up is
displayed with respect to time to execute fixed number of epochs, 320 and 200, respectively.
Figure 6: Training throughput speedup.
Figure 7: Accumulated transmission size of 1
worker (CIFAR10 on AlexNet).
OQOO
8 6 4 2
X3ejn8e
0.00 0.25 0.50 0.75 1.00
bits 1613
OQOO
8 6 4 2
X3ejn8e
0	20	40	60
epochs
2 1
Ho- 6u-us∙q
0.00 0.25 0.50 0.75 1.00
bits 1613
2 10
Ho- 6u-u-∙q
0	20	40	60
epochs
Figure 8: Train loss and test Aacuracy of VGG11 on CIFAR10. Green line: CWi = Ds2t,a27, CM
identity. Blue line: CWi = Dn2,a8t , CM = Cnat .
Theorem 6.	Let CM ∈ B(ωM), CWi ∈ B(ωWi) and ηk ≡ η ∈ (0, 2∕βL), where α, β are as in (4). If
a is picked uniformly at randomfrom {0,1,…，T 一 1}, then
E [kVf(xa)k2]
≤ 2f'η(2-βLη)τ^+ 2-1∣βLη,	(5)
where x? is an opt. solution of (1). In particular, ifwefix any ε > 0 and choose η = Lg+εβ) and
T ≥ 2L(f(χ0)-f(χ?))(α+eβ)∕ε2 ,then E [∣∣Vf(xa)∣∣2] ≤ ε .
The above theorem has some interesting consequences. First, notice that (5) posits a O(1∕T) conver-
gence of the gradient norm to the value 2-aLLη, which depends linearly on a. In VieW of (4), the more
compression we perform, the larger this value becomes. More interestingly, assume now that the same
compression operator is used at each worker: CW = CWi . Let CW ∈ B(ωW) and CM ∈ B(ωM) be
the compression on master side. Then, T (ωM ,ωw) := 2L(f (x0)-f (x^))ε-2(α+εβ) is its iteration
complexity. In the special case of equal data on all nodes, i.e., ζ = 0, we get α = (ωM +1)(ωW +1)σ2∕n
and β = (ωM +1) (1+ ωW∕n). If no compression is used, then ωW = ωM = 0 and α+ εβ = σ2∕n+ ε.
So, the relative slowdown of Algorithm 1 used with compression compared to Algorithm 1 used
without compression is given by
T(TM,ωW)= (ωM + I) (( Wn)- + (I+ωW∕n"!ασ2∕n+ε) ∈ (ωM + 1, (ωM + 1)(ωW + I)].
The upper bound is achieved for n = 1 (or for any n and ε → 0), and the lower bound is achieved
in the limit as n → ∞. So, the slowdown caused by compression on worker side decreases with
7
Under review as a conference paper at ICLR 2021
900
800
700
600
500
400
300
200
100
4	8
Number of workers
In-Network
Aggregation
No Compression
■ Cnat
Cnat deterministic
_ ATE/s at line rate
一(ring all-reduce)
— ATE/s at line rate (INA)
― ATE/s at line rate (Cnat)
Figure 9: Violin plot of Aggregated Tensor Elements (ATE) per
second. Dashed lines denote the maximum ATE/s under line rate.
Figure 10: Convergence com-
parison for weak scaling.
n. More importantly, the savings in communication due to compression can outweigh the iteration
slowdown, which leads to an overall speedup! See Table 1 for the computation of the overall worker
to master speedup achieved by our compression techniques (also see Appendix D.7 for additional
similar comparisons under different cost/speed models). Notice that, however, standard sparsification
does not necessarily improve the overall running time — it can make it worse. Our methods have the
desirable property of significantly uplifting the minimal speedup comparing to their “non-natural”
version. The minimal speedup is more important as usually the number of nodes n is not very big.
5 Experiments
To showcase properties of our approach in practice, we built a proof-of-concept system and provide
evaluation results. We focus on illustrating convergence behavior, training throughput improvement,
and transmitted data reduction. Experimental setup is presented in Appendix B.
Results. We first elaborate the microbenchmark experiments of aggregated tensor elements (ATE)
per second. We collect time measurements for aggregating 200 tensors with the size of 100MB,
and present violin plots which show the median, min, and max values among workers. Fig 9 shows
the result where we vary the number of workers between 4 and 8. The performance difference
observed for the case of Cnat , along with the similar performance for Cnat deterministic indicate that
the overhead of doing stochastic rounding at the aggregator is a bottleneck.
We then illustrate the convergence behavior by training ResNet110 and AlexNet models on CIFAR10.
Fig 5 shows the train loss and test accuracy over time. We note that natural compression lowers
training time by 〜26% for ResNet110 (l7% more than QSGD for the same setup, see Alistarh
et al. (2017) (Table 1)) and 66% for AlexNet, compared to using no compression, while the accuracy
matches the results in (He et al., 2016) without any loss of final accuracy with the same hyperpa-
rameters setting, while training loss is not affected by compression. In addition, combining Cnat
with other compression operators, we can see no effect in convergence, but significant reduction in
communication, e.g., 16× fewer levels for Dnp,ast w.r.t. Dspt,as; see Fig 8 and for other compressions see
Fig 19 and 20 in Appendix.
Next, we report the speedup measured in average training throughput while training benchmark CNN
models on Imagenet dataset for one epoch. The throughput is calculated as the total number of
images processed divided by the time elapsed. Fig 6 shows the speedup normalized by the training
throughput of the baseline, that is, TensorFlow + Horovod using the NCCL communication library.
We further break down the speedup by showing the relative speedup of In-Network Aggregation,
which performs no compression but reduces the volume of data transferred (shown below). We
also show the effects of deterministic rounding on throughput. Because deterministic rounding
does not compute random numbers, it provides some additional speedups. However, it may affect
convergence. These results represent potential speedups in case the overheads of randomization were
low, for instance, when using simply lookup for pre-computed randomness. We observe that the
communication-intensive models (VGG, AlexNet) benefit more from quantization as compared to the
computation-intensive models (GoogleNet, Inception, ResNet). These observations are consistent
with prior work (Alistarh et al., 2017). To quantify the data reduction benefits of natural compression,
we measure the total volume of data transferred during training. Fig 7 shows that data transferred
grows linearly over time, as expected. Natural compression saves 84% of data, which greatly reduces
communication time. Fig 10 studies weak scaling for training ResNet50 on ImageNet showing
that Cnat in itself does not have a negative effect on weak scaling. Further details and additional
experiments including convergence experiments for Neural Collaborative Filtering (He et al., 2017)
are presented in Appendix A.
8
Under review as a conference paper at ICLR 2021
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp.
440-445. Association for Computational Linguistics, 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-
efficient sgd via gradient quantization and encoding. In Advances in Neural Information Processing
Systems ,pp.1709-1720, 2017.
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit Khirirat, and Cedric
Renggli. The convergence of sparsified gradient methods. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 31, pp. 5977-5987. Curran Associates, Inc., 2018.
Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-sgd: Distributed sgd
with quantization, sparsification and local computations. In Advances in Neural Information
Processing Systems, pp. 14695-14706, 2019.
Ron Bekkerman, Mikhail Bilenko, and John Langford. Scaling up machine learning: Parallel and
distributed approaches. Cambridge University Press, 2011.
Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and Trends®
in Machine Learning, 8(3-4):231-357, 2015.
N. Dryden, T. Moon, S. A. Jacobs, and B. V. Essen. Communication quantization for data-parallel
training of deep neural networks. In 2016 2nd Workshop on Machine Learning in HPC Environ-
ments (MLHPC), pp. 1-8, Nov 2016. doi: 10.1109/MLHPC.2016.004.
Nicola Elia and Sanjoy K Mitter. Stabilization of linear systems with limited information. IEEE
transactions on Automatic Control, 46(9):1384-1400, 2001.
Olivier Fercoq, Zheng Qu, Peter RichEik, and Martin Takdc. Fast distributed coordinate descent for
minimizing non-strongly convex losses. IEEE International Workshop on Machine Learning for
Signal Processing, 2014.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
W. M. Goodall. Television by pulse code modulation. The Bell System Technical Journal, 30(1):
33-49, Jan 1951. ISSN 0005-8580. doi: 10.1002/j.1538-7305.1951.tb01365.x.
Priya Goyal, Piotr Dolldr, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training
ImageNet in 1 hour. CoRR, abs/1706.02677, 2017.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In Proceedings of the 32Nd International Conference on International
Conference on Machine Learning - Volume 37, ICML’15, pp. 1737-1746. JMLR.org, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural
collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web,
WWW ’17, pp. 173-182, 2017.
Samuel Horvdth and Peter Richtdrik. Nonconvex variance reduced optimization with arbitrary
sampling. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 2781-2789, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
9
Under review as a conference paper at ICLR 2021
Samuel Horvdth, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and Peter Richtdrik.
Stochastic distributed learning with gradient quantization and variance reduction. arXiv preprint
arXiv:1904.05115, 2019.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700-4708, 2017.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. The Journal
ofMachine Learning Research,18(1):6869-6898, 2017.
Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse and
quantized communication. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2525-2536.
Curran Associates, Inc., 2018.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Sarit Khirirat, Hamid Reza Feyzmahdavian, and Mikael Johansson. Distributed learning with
compressed gradients. arXiv preprint arXiv:1806.06573, 2018.
D. P. Kingma and J. Ba. ADAM: A Method for Stochastic Optimization. In ICLR, 2015.
Jakub KOnecny and Peter Richtdrik. Randomized distributed mean estimation: accuracy vs communi-
cation. Frontiers in Applied Mathematics and Statistics, 4(62):1-11, 2018.
Hyeontaek Lim, David G Andersen, and Michael Kaminsky. 3lc: Lightweight and effective traffic
compression for distributed machine learning. arXiv preprint arXiv:1802.07389, 2018.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and Bill Dally. Deep gradient compression: Reducing
the communication bandwidth for distributed training. In ICLR 2018 - International Conference
on Learning Representations, 2018.
Ryan McDonald, Mehryar Mohri, Nathan Silberman, Dan Walker, and Gideon S. Mann. Efficient
large-scale distributed training of conditional maximum entropy models. In Y. Bengio, D. Schu-
urmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta (eds.), Advances in Neural Information
Processing Systems 22, pp. 1231-1239. Curran Associates, Inc., 2009.
Konstantin Mishchenko, Eduard Gorbunov, Martin Takdc, and Peter Richtdrik. Distributed learning
with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019.
T. Na, J. H. Ko, J. Kung, and S. Mukhopadhyay. On-chip training of recurrent neural networks with
limited numerical precision. In 2017 International Joint Conference on Neural Networks (IJCNN),
pp. 3716-3723, May 2017. doi: 10.1109/IJCNN.2017.7966324.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems,
pp. 693-701, 2011.
Sashank J. Reddi, Jakub Konecny, Peter Richtdrik, Barnabds P6czos, and Alexander J. Smola. AIDE:
Fast and communication efficient distributed optimization. CoRR, abs/1608.06879, 2016.
Peter Richtdrik and Martin Takdc. Distributed coordinate descent method for learning with big data.
Journal of Machine Learning Research, 17(75):1-25, 2016.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathematical
Statistics, pp. 400-407, 1951.
L. Roberts. Picture coding using pseudo-random noise. IRE Transactions on Information Theory, 8
(2):145-154, February 1962. ISSN 0096-1000. doi: 10.1109/TIT.1962.1057702.
10
Under review as a conference paper at ICLR 2021
Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind
Krishnamurthy, MasoUd Moshref, Dan R. K. Ports, and Peter Richtdrik. Scaling distributed
machine learning with in-network aggregation. CoRR, abs/1903.06701, 2019. URL http:
//arxiv.org/abs/1903.06701.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and
application to data-parallel distributed training of speech dnns. In Interspeech 2014, September
2014.
Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using
an approximate Newton-type method. In Eric P. Xing and Tony Jebara (eds.), Proceedings of
the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine
LearningResearch, pp. 1000-1008, Bejing, China, 2014. PMLR.
Sebastian U. Stich. Local SGD converges fast and communicates little. CoRR, abs/1805.09767, 2018.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 31, pp. 4452-4463. Curran Associates, Inc.,
2018.
John Z Sun and Vivek K Goyal. Scalar quantization for relative error. In 2011 Data Compression
Conference, pp. 293-302. IEEE, 2011.
John Z Sun, Grace I Wang, Vivek K Goyal, and Lav R Varshney. A framework for bayesian optimality
of psychophysical laws. Journal of Mathematical Psychology, 56(6):495-501, 2012.
Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and H. Brendan McMahan. Distributed mean
estimation with limited communication. In Proceedings of the 34th International Conference on
Machine Learning, 2017.
Hanlin Tang, Chen Yu, Xiangru Lian, Tong Zhang, and Ji Liu. DoubleSqueeze: Parallel stochastic
gradient descent with double-pass error-compensated compression. In Kamalika Chaudhuri and
Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 6155-6165, Long Beach,
California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/
tang19d.html.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Advances in Neural Information Processing Systems, pp.
1306-1316, 2018.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural
Information Processing Systems, pp. 1509-1519, 2017.
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang. ZipML: Training linear
models with end-to-end low precision, and a little bit of deep learning. In Doina Precup and
Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 4035-4043, International Convention
Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Shuai Zheng, Ziyue Huang, and James Kwok. Communication-efficient distributed blockwise
momentum sgd with error-feedback. In Advances in Neural Information Processing Systems, pp.
11450-11460, 2019.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J. Smola. Parallelized stochastic gradient
descent. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.),
Advances in Neural Information Processing Systems 23, pp. 2595-2603. Curran Associates, Inc.,
2010.
11
Under review as a conference paper at ICLR 2021
Appendix
For easy navigation through the Paper and the Appendices, we provide a table of contents.
Contents
1	Introduction	1
2	Natural Compression	3
3	Natural Dithering	5
4	Distributed SGD	6
5	Experiments	8
A	Extra Experiments	14
A.1 Convergence Tests on CIFAR	10 .................................. 14
A.1.1	DenseNet Hyperparameters: ........................................... 14
A.1.2	AlexNet Hyperparameters: ............................................ 15
A.1.3	ResNet Hyperparameters: ............................................. 15
A.2 Convergence Tests on ImageNet ............................................... 15
A.3 Convergence Tests for Neural	Collaborative Filtering ........................ 15
A.4 Dnp,ast vs. Dspt,au: Empirical Variance ..................................... 16
A.4.1 Dnp,ast has exponentially better variance ............................ 16
A.4.2 Dnp,ast needs exponentially less levels to achieve the same variance . 17
A.4.3 Dspt,as can outperform Dnp,ast in the big s regime ................... 17
A.4.4 Compressing gradients ................................................ 17
A.5 Different Compression Operators ............................................. 17
B	Experimental setup	20
C	Details and Proofs for Sections 2 and 3	22
C.1 Proof of Theorem 1 .......................................................... 22
C.2 Proof of Theorem 2 .......................................................... 22
C.3 Proof of Theorem 3 .......................................................... 23
C.4 Proof of Theorem 4 .......................................................... 23
C.5 Proof of Theorem 5 .......................................................... 23
C.6 Natural Compression and Dithering Allow for Fast Aggregation ................ 24
D Details and Proofs for Section 4	25
D.1 Algorithm ................................................................... 25
12
Under review as a conference paper at ICLR 2021
D.2 Assumptions and Definitions .................................................. 25
D.3 Description of Algorithm 1 ................................................... 25
D.4 Three Lemmas Needed for the Proof of Theorem 6 ............................... 26
D.5 Proof of Theorem 6 ........................................................... 27
D.6 A Different Stepsize Rule for Theorem 6 ...................................... 28
D.7 SGD with Bidirectional Compression: Four Models .............................. 28
D.7.1	Model 1 .............................................................. 29
D.7.2	Model 2 .............................................................. 29
D.7.3	Model 3 .............................................................. 29
D.7.4	Model 4 .............................................................. 30
D.7.5 Communication strategies used in Tables 1, 3, 5, 6 .................... 30
D.8 Sparsification - Formal Definition ........................................... 31
E Limitations and Extensions	31
13
Under review as a conference paper at ICLR 2021
Figure 11: DenseNet40 (k = 12)
AUeJn8e≡91
O 20 40 60 80 IOO 120 140 160 180 200
Epochs
O 300	600	900
Time(S)
AUeJn8e≡91
O 300	600	900
Time(S)
O IOO 200	300	400	500
Time(S)
O 20 40 60 80 IOO120 140160180 200	O IOO 200	300	400	500
Epochs	Time(S)
Figure 12:	AlexNet (Batch size: 256, 512 and 1024)
A Extra Experiments
A.1 Convergence Tests on CIFAR 10
In order to validate that Cnat does not incur any loss in performance, we trained various DNNs
on the Tensorflow CNN Benchmark1 on the CIFAR 10 dataset with and without Cnat for the same
number of epochs, and compared the test set accuracy, and training loss. As mentioned earlier, the
baseline for comparison is the default NCCL setting. We didn’t tune the hyperparameters. In all of
the experiments, we used Batch Normalization, but no Dropout was used.
Looking into Figures 11, 12 and 13, one can see that Cnat achieves significant speed-up without
incurring any accuracy loss. As expected, the communication intensive AlexNet (62.5 M parameters)
benefits more from the compression than the computation intensive ResNets (< 1.7 M parameters)
and DenseNet40 (1 M parameters).
A.1.1 DenseNet Hyperparameters:
We trained DenseNet40 (k = 12) and followed the same training procedure as described in Huang
et al. (2017). We used a weight decay of 10-4 and the optimizer as vanilla SGD. We trained for a
total of 300 epochs. The initial learning rate was 0.1, which was decreased by a factor of 10 at 150
and 225 epoch.
1https://github.com/tensorflow/benchmarks
14
Under review as a conference paper at ICLR 2021
0 40 80 120 160 200 240 280 320
Epochs
0	600 1200 1800 2400 3000 3600
Time(S)
0	600 1200 1800 2400 3000 3600
Time(S)
I I I I _
3O- 6--eJl
SSO-BWWeJJ.
X3ejn8e≡9J.
O 600 1200 1800 2400 3000 3600 4200
Time(S)
O 40 80 120 160 200 240 280 320	O 600 1200 1800 2400 3000 3600 4200
Epochs	Time(S)
Figure 13:	ResNet (#layers: 20, 44 and 56)
A.1.2 AlexNet Hyperparameters:
For AlexNet, we chose the optimizer as SGD with momentum, with a momentum of 0.9. We trained
on three minibatch sizes: 256, 512 and 1024 for 200 epochs. The learning rate was initially set to be
0.001, which was decreased by a factor of 10 after every 30 epoch.
A.1.3 ResNet Hyperparameters:
All the ResNets followed the training procedure as described in He et al. (2016). We used a weight
decay of 10-4 and the optimizer was chosen to be vanilla SGD. The minibatch size was fixed to be
128 for ResNet 20, and 256 for all the others. We train for a total of 64K iterations. We start with an
initial learning rate of 0.1, and multiply it by 0.1 at 32K and 48K iterations.
A.2 Convergence Tests on ImageNet
To further demonstrate the convergence behavior of Cnat , we run experiments which conform to
the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). We follow publicly available
benchmark2 and apply Cnat on it without modifying any hyperparameter. The model trains for 50
epochs on 8 and 16 workers, with default ResNet50 setup: SGD optimizer with 0.875 momentum,
cosine learning rate schedule with 0.256 initial learning rate and linear warmup during the first 8
epochs. The weight decay is set to 1/32768 and is not applied on Batch Norm trainable parameters.
Furthermore, 0.1 label smoothing is used. As shown in 14, Cnat does not incur any accuracy loss
even if applied on large distributed tasks.
A.3 Convergence Tests for Neural Collaborative Filtering
We also train Neural Collaborative Filtering (NCF) (He et al., 2017) on MovieLens-20M Dataset
using Cnat and compare its convergence to no compression. Neural Collaborative Filtering is a big
recommendation model with ~32 million parameters. We use a publicly available benchmark3 and
apply Cnat on it without modifying any hyperparameter. The model trains for 20 epochs on 8 workers
with ADAM optimizer (Kingma & Ba, 2015) (lr= 4.5 × 10-3, β1 = 0.25, β2 = 0.5), a global
2https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow/
Classification/RN50v1.5
3https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/
Recommendation/NCF
15
Under review as a conference paper at ICLR 2021
",SOn u-ejɪ
----Ciw 8 workers
----CEe 16 workers
-----No Compression 8 workers
----- No Compression 16 workers
Figure 14:	ResNet50 on ImageNet
batch-size of 220, and a dropout ratio of 0.5. No weigt decay is applied. Fig 15 shows that Cnat
performs similar to no compression both in terms of training loss and test hit rate.
-8-6-4
Ooo
SSO-U,j
O	500 IOOO 1500
Steps
l6l4l22,≡
9 9 9 9 8
OI ©se°七工
250 500 750 IOOO 1250 1500 1750
Steps
Figure 15: Neural Collaborative Filtering on MovieLens-20M
A.4 Dp,s VS. Dp,u: EMPIRICAL VARIANCE
nat	sta
In this section, we perform experiments to confirm that Dnp,ast level selection brings not just theoretical
but also practical performance speedup in comparison to Dspt,au . We measure the empirical variance of
Dspt,au and Dnp,ast . For Dnp,ast , we do not compress the norm, so we can compare just variance introduced
by level selection. Our experimental setup is the following. We first generate a random vector x of
size d = 105, with independent entries with Gaussian distribution of zero mean and unit variance (we
tried other distributions, the results were similar, thus we report just this one) and then we measure
normalized empirical variance
ω(x) :
kc (X)-Xk2
kxk2
We provide boxplots, each for 100 randomly generated vectors X using the above procedure. We
perform this forp = 1, p = 2 and p = ∞. We report our findings in Fig 16, Fig 17 and Fig 18. These
experimental results support our theoretical findings.
A.4. 1 Dnp,ast HAS EXPONENTIALLY BETTER VARIANCE
In Fig 16, we compare Dnp,ast and Dspt,au for u = s, i.e., we use the same number of levels for both
compression strategies. In each of the three plots we generated vectors X with a different norm. We
find that natural dithering has dramatically smaller variance, as predicted by Thm 5.
2500
2000
1500-
IOOO
500-
O
⅛?2
⅛I5=I⅛>
Figure 16: Dnp,ast vs. Dspt,au with u = s.
¾5

0 5 0 5 0 5 0
111Z 工 z

16
Under review as a conference paper at ICLR 2021
A.4.2 Dnp,ast NEEDS EXPONENTIALLY LESS LEVELS TO ACHIEVE THE SAME VARIANCE
In Fig 17, we set the number of levels for Dspt,au to u = 2s-1. That is, we give standard dithering an
exponential advantage in terms of the number of levels (which also means that it will need more bits
for communication). We now study the effect of this change on the variance. We observe that the
empirical variance is essentially the same for both, as predicted by Thm 5.
Mus 3ueμe>
0.975-
0.970
0.965-
0.960
1.000
0.995
0.990
0.985

Figure 17: Dnp,ast vs. Dspt,au with u = 2s-1.
A.4.3 Dspt,as CAN OUTPERFORM Dnp,ast IN THE BIG s REGIME
We now remark on the situation when the number of levels s is chosen to be very large (see Fig 18).
While this is not a practical setting as it does not provide sufficient compression, it will serve as
an illustration of a fundamental theoretical difference between Dspt,as and Dnp,ast in the s → ∞ limit
which we want to highlight. Note that while Dspt,as converges to the identity operator as s → ∞,
which enjoys zero variance, Dnp,ast converges to Cnat instead, with variance that can’t reduce below
ω = 1/8. Hence, for large enough s, one would expect, based on our theory, the variance of Dnp,ast to
be around 1/8, while the variance of Dspt,as to be closer to zero. In particular, this means that Dspt,as can,
in a practically meaningless regime, outperform Dnp,ast. In Fig 18 we choose p = ∞ and s = 32 (this
is large). Note that, as expected, the empirical variance of both compression techniques is small, and
that, indeed, Dspt,as outperforms Dnp,ast .
H⅛sci>
Figure 18: When p = ∞ and s is very large, the empirical variance of Dspt,as can be smaller than that
of Dnp,ast . However, in this case, the variance of Dnp,ast is already negligible.
A.4.4 Compressing gradients
We also performed identical to those reported above, but with a different generation technique of the
vectors x. In particular, instead of a synthetic Gaussian generation, we used gradients generated by
our optimization procedure as applied to the problem of training several deep learning models. Our
results were essentially the same as the ones reported above, and hence we do not include them.
A.5 Different Compression Operators
We report additional experiments where we compare our compression operator to previously proposed
ones. These results are based on a Python implementation of our methods running in PyTorch as this
enabled a rapid direct comparisons against the prior methods. We compare against no compression,
random sparsification, and random dithering methods. We compare on MNIST and CIFAR10 datasets.
For MNIST, we use a two-layer fully connected neural network with RELU activation function. For
CIFAR10, we use VGG11 with one fully connected layer as the classifier. We run these experiments
17
Under review as a conference paper at ICLR 2021
with 4 workers and batch size 32 for MNIST and 64 for CIFAR10. The results are averages over 3
runs.
We tune the step size for SGD for a given “non-natural” compression. Then we use the same step
size for the “natural” method. Step sizes and parameters are listed alongside the results.
Figures 19 and 20 illustrate the results. Each row contains four plots that illustrate, left to right, (1)
the test accuracy vs. the volume of data transmitted from workers to master (measured in bits), (2)
the test accuracy over training epochs, (3) the training loss vs. the volume of data transmitted, and (4)
the training loss over training epochs.
One can see that in terms of epochs, we obtain almost the same result in terms of training loss and test
accuracy, sometimes even better. On the other hand, our approach has a huge impact on the number of
bits transmitted from workers to master, which is the main speedup factor together with the speedup
in aggregation if we use In-Network Aggregation (INA). Moreover, with INA we compress updates
also from master to nodes, hence we send also fewer bits. These factors together bring significant
speedup improvements, as illustrated in Fig 6, which strongly suggests similar speed-up in training
time as observed for Cnat, see e.g. Section 5.
18
Under review as a conference paper at ICLR 2021
OQOO
8 6 4 2
X32n8e as
0	2	4
bits IelZ
OQOO
8 6 4 2
X32n8e
0	10	20	30	40
epochs
2 1
ωωO- 6u-u-eJ*
0
2	4
bits IelZ
2 1
ωωO- 6u-u-eJ*
0	10	20	30	40
epochs
(a)	No Additional compression, step size 0.1.
Qoo
6 4 2
AU2n8e as
AU2n8e,sii
ωωO- 6u-u-eJ*j
——S01+ None
--S01 » Cnat + Cnat
ωωO- 6u-u-eJ*j
0	20	40
epochs
0	2	4
bits IelZ
0	20	40
epochs
0	2	4
bits IelZ
(b)	Random sparsification, step size 0.04, sparsity 10%.
Qoo
6 4 2
X32n8e as
Qoo
6 4 2
X32n8e
Ho- 6u-u-eJ*j
—S,o∙1 + None
—SrtU-Cnat+ Cnat
Ho- 6u-u-eJ*j
0	20	40
epochs
0	2	4
bits IelZ
O 20	40
epochs
0	2	4
bits IelZ
(c)	Random sparsification with non-uniform probabilities (Wangni et al., 2018), step size 0.04, sparsity 10%.
Oooo
8 6 4 2
X3ejn8e
OQOO
8 6 4 2
X3ejn8e
2 1
Ho- 6u-u-∙q
2 1
Ho- 6u-u-∙q
20	40
epochs
60
0.00 0.25 0.50 0.75 1.00
bits lel3
0	20	40	60
epochs
0.00 0.25 0.50 0.75 1.00
bits	Ie 13
(d)	Random dithering, step size 0.08, s = 8, u = 27, second norm.
Figure 19:	CIFAR10 with VGG11.
19
Under review as a conference paper at ICLR 2021
.(ɪ .(ɪ J
X32n8e as"*-
9=m94m
AU2n8e as"*
0	1	2	3	4	5
bits	leu
9=9694s
AU2n8e,"*
5	10	15	20	O
epochs
ωωO- 6u-u-eJ*j
2	3	4	5
bits	leu
ωωO- 6u-u-eJ*j
5	10	15	20
epochs
(a)	No Additional compression, step size 0.1.
0.0	0.2
0.4
bits
.(ɪ .(ɪ J
X32n8e,s"*j
5	10	15	20
epochs
Ho- 6u-u-eJ*j
∏	——S01 + None
l∖ ------ S01 β Cnat + Cnat
I I I
Ho- 6u-u-eJ*j
5	10	15	20
epochs
0.0	0.2	0.4	0.6	0.8
bits	leu
(b)	Random sparsification, step size 0.04, sparsity 10%.
0.0	0.2	0.4	0.6	0.8
bits	leu
一 S1°1 + None
——Se T。Cnat+ Cnat
5	10	15	20
epochs
—Sλi + None
一 Srt,∙1∙ Cnat+ Cnat
0.0	0.2	0.4	0.6	0.8
bits	leu
5	10	15	20
epochs
(c)	Random sparsification with non-uniform probabilities (Wangni et al., 2018), step size 0.04, sparsity 10%.
y9etis9
0.00 0.2 5 0.50 0.75 1.00 1.25 1.50
bits	leu
.(ɪ .(ɪ .(ɪ
X3ejn8etis
5	10	15	20
epochs
ωωO- 6u-u-∙q
0.00 0.25 0.50 0.75 1.00 1.25 1.50
bits	leu
I I
ωωO- 6u-u-∙q
5	10	15	20
epochs
(d)	Random dithering, step size 0.01, s = 8, u = 27, second norm.
Figure 20:	MNIST with 2 fully conected layers.
B Experimental setup
Our experiments execute the standard CNN benchmark4. We summarize the hyperparameters setting
in Appendix A.1.2. We further present results for two more variations of our implementation: one
without compression (providing the baseline for In-Network Aggregation (Sapio et al., 2019)), and
the other with deterministic rounding to the nearest power of 2 to emphasize that there exists a
performance overhead of sampling in natural compression.
We implement the natural compression operator within the Gloo communication library5, as a
drop-in replacement for the ring all-reduce routine. Our implementation is in C++. We integrate
our communication library with Horovod and, in turn, with TensorFlow. We follow the same
communication strategy introduced in SwitchML (Sapio et al., 2019), which aggregates the deep
learning model’s gradients using In-Network Aggregation on programmable network switches. We
choose this strategy because natural compression is a good fit for the capabilities of this class of
modern hardware, which only supports basic integer arithmetic, simple logical operations and limited
storage.
A worker applies the natural compression operator to quantize gradient values and sends them to the
aggregator component. As in SwitchML, an aggregator is capable of aggregating a fixed-length array
of gradient values at a time. Thus, the worker sends a stream of network packets, each carrying a
chunk of compressed values. For a given chunk, the aggregator awaits all values from every worker;
4https://github.com/tensorflow/benchmarks
5https://github.com/facebookincubator/gloo
20
Under review as a conference paper at ICLR 2021
IelO
le9
+j 1.0
C
8 0-5
0.0
-50
E=3:E=-u__Ei
-40	-30	-20	-10
exponent value
EDE-XraE
UJnUnU-LU
EnE-XeE
-50	-40	-30	-20	-10
exponent value
0	10
通
Figure 21:	Histogram of exponents of gradients exchanged during the entire training process for
ResNet110 (left) and Alexnet (right). Red lines denote the minimum and maximum exponent values
of all gradients.
then, it restores the compressed values as integers, aggregates them and applies compression to
quantize the aggregated values. Finally, the aggregator multicasts back to the workers a packet of
aggregated values.
For implementation expedience, we prototype the In-Network Aggregation as a server-based program
implemented atop DPDK6 for fast I/O performance. We leave to future work a complete P4 imple-
mentation for programmable switches; however, we note that all operations (bit shifting, masking,
and random bits generation) needed for our compression operator are available on programmable
switches.
Implementation optimization. We carefully optimize our implementation using modern x86 vector
instructions (AVX2) to minimize the overheads in doing compression. To fit the byte length and access
memory more efficiently, we compress a 32-bit floating point numbers to an 8-bit representation,
where 1 bit is for the sign and 7 bits are for the exponent. The aggregator uses 64-bit integers to store
the intermediate results, and We choose to clip the exponents in the range of -50 〜10. As a result,
we only use 6 bits for exponents. The remaining one bit is used to represent zeros. Note that it is
possible to implement 128-bit integers using tWo 64-bit integers, but We found that, in practice, the
exponent values never exceed the range of -50 〜10 (Figure 21).
Despite the optimization effort, we identify non-negligible 10 〜15% overheads in doing random
number generation used in stochastic rounding, Which Was also reported in Hubara et al. (2017).
We include the experimental results of our compression operator without stochastic rounding as
a reference. There could be more efficient ways to deal with stochastic rounding, but we observe
that doing deterministic rounding gives nearly the same training curve in practice meaning that
computational speed up is neutralized by slower convergence due to biased compression operator.
Hardware setup. We run the workers on 8 machines configured with 1 NVIDIA P100 GPU, dual
CPU Intel Xeon E5-2630 v4 at 2.20GHz, and 128 GB of RAM. The machines run Ubuntu (Linux
kernel 4.4.0-122) and CUDA 9.0. Following Sapio et al. (2019), we balance the workers with 8
aggregators (4 aggregators in the case of 4 workers) running on machines configured with dual
Intel Xeon Silver 4108 CPU at 1.80 GHz. Each machine uses a 10 GbE network interface and
has CPU frequency scaling disabled. The chunks of compressed gradients sent by workers are
uniformly distributed across all aggregators. This setup ensures that workers can fully utilize their
network bandwidth and match the performance of a programmable switch. We leave the switch-based
implementation for future work.
6https://www.dpdk.org
21
Under review as a conference paper at ICLR 2021
C Details and Proofs for Sections 2 and 3
C.1 Proof of Theorem 1
By linearity of expectation, the unbiasedness condition and the second moment condition (3) have
the form
E [(C(x))i] = xi ,	∀x ∈ Rd,	∀i ∈ [d]	(6)
and
dd
XE(C(x))i2 ≤ (ω+1)Xxi2,	∀x∈Rd.	(7)
i=1	i=1
Recall that Cnat (t) can be written in the form
Cnat(t) = sign(t) ∙ 2blog2ltic(1 + λ(t)).	(8)
2dlog2 |t|e -|t|
where the last step follows since p(t) =	2biθg2 ∣t-1 |. Hence,
E [Cnat(t)] = E [sign(t) ∙ 2blog2 1t|c (1 + λ(t))] = sign(t) ∙ 2blog21t|c (1 + E [λ(t)])
=sign(t) ∙ 2blog2|t|c (1 + 1 - p(t)) = t,
This establishes unbiasedness (6).
In order to establish (7), it suffices to show that E (Cnat(x))i2 ≤ (ω + 1)xi2 for all xi ∈ R. Since by
definition (Cnat(x))i = Cnat(xi) for all i ∈ [d], it suffices to show that
E (Cnat(t))2 ≤ (ω + 1)t2,	∀t ∈ R.	(9)
If t = 0 or t = sign(t)2α with α being an integer, then Cnat(t) = t, and (9) holds as an identity with
ω = 0, and hence inequality (9) holds for ω = 1/8. Otherwise t = sign(t)2α where a := bαc < α <
dαe = a + 1. With this notation, we can write
E [(Cnat(t))2] = 22a 2a+2- |t| +22(a+1)≡=a2a	=	2a(3∣t∣ - 2a+1).
So,
E [(Cnat(t))2]
t2
2a(3|t| - 2a+1)	/	2a(3∣t∣ - 2a+1)
t2	≤	2a<s<pa+1	t2
sup
1<θ<2
2a(3 ∙ 2aθ - 2a+1)
(2a θ)2
sup
1<θ<2
3θ - 2
~~θ2~
The optimal solution of the last maximization problem is θ = 3, with optimal objective value 9. This
implies that (9) holds with ω = 8.
C.2 Proof of Theorem 2
Let assume that there exists some ω < ∞ for which Cint is the ω quantization. Unbiased rounding to
the nearest integer can be defined in the following way
Cint(xi) :=	bdxxice,
dxi e,
with probability p(xi),
with probability 1 - p(xi),
wherep(xi) = dxie - xi. Let’s take 1-D example, where x ∈ (0, 1), then
E [Cint(x2)] = (1 - x)02 + x12 = x ≤ ωx2,
which implies ω ≥ 1/x, thus taking x → 0+, one obtains ω → ∞, which contradicts the existence
of finite ω.
22
Under review as a conference paper at ICLR 2021
C.3 Proof of Theorem 3
The main building block of the proof is the tower property of mathematical expectation. The tower
property says: If X and Y are random variables, then E [X] = E [E [X | Y]] . Applying it to the
composite compression operator C1 ◦ C2, we get
E [(C1 ◦ C2) (x)] =E[E[C1(C2(x)) | C2(x)]] (=3) E [C2(x)] (=3) x .
For the second moment, we have
E hk(C1 ◦ C2) (x)k2i =EhEhkC1(C2(x))k2 | C2(x)ii
(≤3) (ω2 + 1)E hkC1(x)k2i
≤ (ω1 + 1)(ω2 + 1) kxk2 ,
which concludes the proof.
C.4 Proof of Theorem 4
Unbiasedness of Dnp,ast is a direct consequence of unbiasedness of DgCe,pn,s .
For the second part, we first establish a bound on the second moment of ξ :
≤
(E1 、ς)1-s∖ 9 Ixi I2	1 Ixil	i-sʌ Ixil 91-s
1师≥2 J8kxkp+11Jxkp<2 Jkxkp2
9 bf +1
8同p
Using this bound, we have
E hkDnp,ast(x)k2i	=
(10)
≤
≤
≤
≤
E hkxkpi X Eξ (而)
kxk2p
9同2
83p
d
+X1
i=1
9 同2 +min {2 1一1叫同 ι, 22-2sd 同 p}
9 Ilxk2 + min {d1∕221-s kxkp Ilxk, 22-2sd IIxkpO
9 + d1/ min{p,2}21-s min {l,d1/min{p，2}21-s}) ∣x∣2 ,
≤
where the second inequality follows from min{ai , bi } ≤ min{ ai , bi } and the last two
inequalities follow from the following consequence of Holder,s inequality ∣∣xkp ≤ d1/p-1/2 kxk for
1 ≤ p < 2 and from the fact that kxkp ≤ kxk forp ≥ 2. This concludes the proof.
C.5 Proof of Theorem 5
s	2s-1
The main building block of the proof is useful connection between Dnp,ast and Dspt,a2	, which can be
formally written as
Dnasʒ(X) = Ilxkp ∙ Sign(X) ∙Cnat(ξ(x)) ,	(10)
where (ξ(x))i = ξ(x∕kχkp) with levels 0,1∕2s-1,2∕2s-1, ∙∙∙ , 1. Graphical visualization canbe found
in Fig 22.
23
Under review as a conference paper at ICLR 2021
Figure 22: 1D visualization of the workings of natural dithering Dnp,ast and standard dither-
ing Dspt,au with u = 2s-1, with s = 4. Notice that the numbers standard dithering rounds
to, i.e., 0, 1/8, 2/8, . . . , 7/8, 1, form a superset of the numbers natural dithering rounds to, i.e.,
0, 2-3, 2-2, 2-1, 1. Importantly, while standard dithering uses u = 24-1 = 8 levels (i.e., inter-
vals) to achieve a certain fixed variance, natural dithering only needs s = 4 levels to achieve the
same variance. This is an exponential improvement in compression (see Theorem 5 for the formal
statement).
Equipped with this, we can proceed with
E [k(x√kχkp)∣∣2i	(1=)	Enkxkp ∙ Sign(X) ∙Cnat(ξ(x))
=E [kxkpi ∙ E [kCnat(ξ(x))k2i
Thm. 1	9	2
≤ 8E [∣∣kxkp Sign(X)&X)H
=9EM2s-112 (χ)]
9
≤	(ω + 1),
8
which concludes the proof.
C.6 Natural Compression and Dithering Allow for Fast Aggregation
Besides communication savings, our new compression operators Cnat (natural compression) and Dnp,ast
(natural dithering) bring another advantage, which is ease of aggregation. Firstly, our updates allow
in-network aggregation on a primitive switch, which can speed up training by up to 300% (Sapio
et al., 2=19) itself. Moreover, our updates are so simple that if one uses integer format on the master
side for update aggregation, then our updates have just one non-zero bit, which leads to additional
speed up. For this reason, one needs to operate with at least 64 bits during the aggregation step, which
is the reason why we also do Cnat compression on the master side; and hence we need to transmit just
exponent to workers. Moreover, the translation from floats to integers and back is computation-free
due to structure of our updates. Lastly, for Dnp,ast compression we obtain additional speed up with
respect to standard randomized dithering Dspt,as as our levels are computationally less expensive due to
their natural compatibility with floating points. In addition, for effective communication one needs
to communicate signs, norm and levels as a tuple for both Dnp,ast and Dspt,as, which needs to be then
multiplied back on the master side. For Dnp,ast , this is just the summation of exponents rather than
actual multiplication as is the case for Dspt,as .
D Details and Proofs for Section 4
D.1 Algorithm
24
Under review as a conference paper at ICLR 2021
Algorithm 1 Distributed SGD with bidirectional compression
Input: learning rates {ηk}kT=0 > 0, initial vector x0
for k = 0, 1, . . . T do
Parallel: Worker side
for i = 1, . . . , n do
compute a stochastic gradient gi(xk) (of fi at xk)
compress it ∆ik = CWi (gi (xk))
end for
Master side
aggregate ∆k = Pin=1 ∆ik
compress gk = CM (∆k) and broadcast to each worker
Parallel: Worker side
for i = 1, . . . , n do
xk+1=Xk — η~ gk
end for
end for
D.2 Assumptions and Definitions
Formal definitions of some concepts used in Section follows:
Definition 4. Let fi : Rd → R be fixed function. A stochastic gradient for fi is a random function
gi(x) so that E [gi(χ)] = Vfi(x).
In order to obtain the rate, We introduce additional assumptions on gi(x) and Vfi(x).
Assumption 1 (Bounded Variance). We say the stochastic gradient has variance at most σi2 if
E [∣∣gi(x) — Vfi(X)『]≤ σ2 for all X ∈ Rd. Moreover, let σ2 = ɪ Pn=I σ2.
Assumption 2 (Similarity). We say the variance of gradient among nodes is at most ζi2 if
∣∣Vfi(x) — Vf(x)k2 ≤ ZI for all X ∈ Rd. Moreover, let Z2 = nn Pn=1 Z2.
Moreover, We assume that f is L-smooth (gradient is L-Lipschitz). These are classical assumptions
for non-convex SGD (Ghadimi & Lan, 2013; Jiang & AgraWal, 2018; Mishchenko et al., 2019) and
comparing to some previous Works (Alistarh et al., 2017), our analysis does not require bounded
iterates and bounded the second moment of the stochastic gradient. Assumption 2 is automatically
satisfied With Z2 = 0 if every Worker has access to the Whole dataset. If one does not like Assumption 2
one can use the DIANA algorithm (HOrVgth et al., 2019) as a base algorithm instead of SGD, then
there is no need for this assumption. For simplicity, We decide to pursue just SGD analysis and We
keep Assumption 2.
D.3 Description of Algorithm 1
Let us describe Algorithm 1. First, each Worker computes its oWn stochastic gradient gi(Xk), this
is then compressed using a compression operator CWi (this can be different for every node, for
simplicity, one can assume that they are all the same) and send to the master node. The master node
then aggregates the updates from all the Workers, compress With its oWn operator CM and broadcasts
update back to the Workers, Which update their local copy of the solution parameter X.
Note that the communication of the updates can be also done in all-to-all fashion, Which implicitly
results in CM being the identity operator. Another application, Which is one of the key motivations
of our natural compression and natural dithering operators, is in-network aggregation (Sapio et al.,
2019). In this setup, the master node is a network switch. HoWever, current netWork sWitches can
only perform addition (not even average) of integers.
D.4 Three Lemmas Needed for the Proof of Theorem 6
Before We proceed With the theoretical guarantees for Algorithm 1 in smooth non-convex setting,
We first state three lemmas Which are used to bound the variance of gk as a stochastic estimator of
25
Under review as a conference paper at ICLR 2021
the true gradient Vf (Xk). In this sense compression at the master-node has the effect of injecting
additional variance into the gradient estimator. Unlike in SGD, where stochasticity is used to speed
up computation, here we use it to reduce communication.
Lemma 7 (Tower property + Compression). If C ∈ B(ω) and z is a random vector independent of C,
then
E kC(z) - zk2 ≤ωE kzk2 ; E kC(z)k2 ≤(ω+1)E kzk2 .	(11)
Proof. Recall from the discussion following Definition 2 that the variance of a compression operator
C ∈ B(ω) can be bounded as
E hkC(x) - xk2i ≤ ω kxk2 ,	∀x ∈ Rd.
Using this with z = x, this can be written in the form
E hkC(z)-zk2 | zi ≤ωkzk2,	∀x∈Rd ,	(12)
which we can use in our argument:
E hkC(z) - zk2i	= E hE hkC(z) - zk2 |zii
(≤12) E hω kzk2i
= ωE hkzk2i .
The second inequality can be proved exactly same way.	口
Lemma 8 (Local compression variance). Suppose x is fixed, C ∈ B(ω), and gi(x) is an unbiased
estimator of Vfi(x). Then
E hkC(gi(x)) - Vfi(x)k2i ≤ (ω + 1)σi2 + ω kVfi(x)k2.	(13)
Proof.
E hkC(gi(x)) - Vfi(x)k2i	Def.=4+(3)
(11)
≤
Def. 4+(3)
Assum. 1
≤
E kC(gi(x))-gi(x)k2 +E kgi(x)-Vfi(x)k2
ωE hkgi(x)k2i +Ehkgi(x)-Vfi(x)k2i
(ω + 1)E hkgi(x) - Vfi(x)k2i +ω kVfi(x)k2
(ω + 1)σi2 +ω kVfi(x)k2 .
□
Lemma 9 (Global compression variance). Suppose x is fixed, CWi ∈ B(ωWi) for all i, CM ∈ B(ωM),
and gi(x) is an unbiased estimator of Vfi(x) for all i. Then
2
E
X CWi (gi(x))
≤ α+βkVf(x)k2,
(14)
where ωW = maxi∈[n] ωWi and
α = (ωM + 1)(ωW + 1) σ2 + (ωM + 1)ωW Z2 , β =1 + 3m + (ωM + 1)ωW .	(15)
nn	n
Proof. For added clarity, let us denote
n
∆ = X CWi (gi(x)).
i=1
26
Under review as a conference paper at ICLR 2021
Using this notation, the proof proceeds as follows:
E	—CM (∆)
∣n
2
Def. 4+(3)
E	—cm。)- Vf(X)
∣n
2 + kVf(X)k2
Def. 4+(3)
n12E hkCM ⑷-δ∣∣[
+ E 1∆ -Vf (x)
∣n
2 + kVf(X)k2
(11)
≤
筌E gk[
+ E 1∆ -Vf (x)
∣n
2 + kVf(X)k2
Def. 4+(3)
(ωM + 1)E 1∆ 一 Vf (x)
∣n
2 +(ωM+1) kVf(X)k2
ω +1 n
JMn+- XE [kCWi(gi(X)) - Vfi(X)k2] + (3M + 1) kVf(X)k2
(13)
≤
i=1
(ωM + 1)(ωw + I) q2 +
n
(ωM + 1)ωW 1 XX kVfi(x)k
n
n
i=1
2
Assum. 2
≤
D.5 Proof of Theorem 6
+(ωM+1)kVf(X)k2
(ωM + 1)(ωw + I) q2 +
n
(ωM + 1)ωW 1 XX kVfi(x) -Vf (x)k
n
n
i=1
+(1 + JM + 包+1ωW )kVf(X)k2
(ωM + 1)(ωW + 1) σ2 + (ωM + 1)ωW Z2
n
n
+ (l + JM + (ωM+1ωW) kVf(X)k2
Using L-smoothness of f and then applying Lemma 9, we get
Eff(Xk)] +E [(Vf (Xk),xk+1 - xk)] + LE h∣∣xk+1 - xk∣∣2i
2
□
≤
E ff(Xk)] - ηEhlIVf(Xk)『]+ 2η2E

(14)
≤
Eff(Xk)] - (ηk - Lβη2) E h∣∣vf(χk)∣∣2] + L媪.
Summing these inequalities for k = 0, ..., T - 1, we obtain
T-1
X Qk- 2βnk) E [∣∣Vf(Xk)∣∣2] ≤ f(x0) - f(X*) +
Taking ηk = η and assuming
TLank
-2
2
n<Le，
(16)
one obtains
T-1
E [kVf(χa)k2i ≤ T XE h∣∣Vf(χk)∣∣2i ≤
k=0
It is easy to check that if we choose η
2(f(x0) - f (x?)) + Lan .= δ( T)
Tn (2 - Lβn)	+ 2 - Lβn := (n, ).
T≥
2L(f(x0)-f(x*))(α+eβ)
工⑺+三.)(which Satisfies (16) for every ε > 0), then for any
we have δ(η, T ) ≤ ε, concluding the proof.
27
Under review as a conference paper at ICLR 2021
	Master can aggregate real numbers (e.g., a workstation)	Master can aggregate integers only (e.g., SwitchML (Sapio et al., 2019))
Same communication speed both ways	MODEL 1	MODEL 3
Master communicates infinitely fast	MODEL 2	MODEL 4	
Table 2: Four theoretical models.
D.6 A Different Stepsize Rule for Theorem 6
Looking at Theorem 6, one can see that setting step size

ηk = η
2(f(x0)- f(x?))
LTa
with
T ≥ Lβ2(f(x0)- f(x?))
α
(number of iterations), we have iteration complexity
(r (ωw+1)(ωM+1)
which will be essentially same as doing no compression on master and using CW ◦ CM or CW ◦ CM
on the workers’ side. Our rate generalizes to the rate of Ghadimi & Lan (2013) without compression
and dependency on the compression operator is better comparing to the linear one in Jiang & Agrawal
(2018)7. Moreover, our rate enjoys linear speed-up in the number of workers n, the same as Ghadimi
& Lan (2013). In addition, if one introduces mini-batching on each worker of size b and assuming
each worker has access to the whole data, then σ2 → σ2 /b and ζ2 → 0, which implies
r I(ωw+1)(ωM+1)
IV τn
→O
(ωW + 1)(ωM + 1)
Tbn
and hence one can also obtain linear speed-up in terms of mini-batch size, which matches with Jiang
& Agrawal (2018).
D.7 SGD with Bidirectional Compression: Four Models
It is possible to consider several different regimes for our distributed optimization/training setup,
depending on factors such as:
•	The relative speed of communication (per bit) from workers to the master and from the
master to the workers,
•	The intelligence of the master, i.e., its ability or the lack thereof of the master to perform
aggregation of real numbers (e.g., a switch can only perform integer aggregation),
•	Variability of various resources (speed, memory, etc) among the workers.
For simplicity, we will consider four situations/regimes only, summarized in Table 2.
Direct consequences of Theorem 6: Notice that (5) posits a O(1/T) convergence of the gradient
norm to the value 2aLLLn，which depends linearly on a. In view of (4), the more compression We
perform, the larger this value. More interestingly, assume now that the same compression operator
is used at each worker: CW = CWi . Let CW ∈ B(ωW) and CM ∈ B(ωM) be the compression
on master side. Then, T(ωM, ωw) := 2L(f (x0) - f (x*))ε-2(α + εβ) is its iteration complexity.
In the special case of equal data on all nodes, i.e., Z = 0, we get α = QM+1)(ωw+1)σ2∕n and
β = (ωM + 1)(1+ ωw∕n). If no compression is used, then ωw = ωM = 0 and a + εβ = σ2∕n + ε.
7Jiang & Agrawal (2018) allows compression on the worker side only.
28
Under review as a conference paper at ICLR 2021
So, the relative slowdown of Algorithm 1 used with compression compared to Algorithm 1 used
without compression is given by
T (ωM ,ωw)
T(0,0)
(QW + 1)σ2∕n +(1+ ωw∕n)ε)
0σ / n + ε
(ωM + 1) ∈ (ωM + 1, (ωM + 1)(ωW + 1)] .
(17)
The upper bound is achieved for n = 1 (or for any n and ε → 0), and the lower bound is achieved
in the limit as n → ∞. So, the slowdown caused by compression on worker side decreases with
n. More importantly, the savings in communication due to compression can outweigh the iteration
slowdown, which leads to an overall speedup!
D.7.1 Model 1
First, we start with the comparison, where we assume that transmitting one bit from worker to node
takes the same amount of time as from master to worker.
Compression C ∈ B(ω)	No. iterations T (ω) = O((ω + 1)1+θ)	Bits per iteration Wi 7→ M + M 7→ Wi	Speedup T (0)B(0) T (ω)r(ω)
None	1	2 ∙ 32d	1
Cnat	(8 )1+θ	2 ∙ 9d	2.81x-3.16x
Sq	(d)1+θ	2 ∙ (33 + log2 d)q	0.06x-0.60x
Sq ◦Cnat	(8d)1+θ	2 ∙ (10 + log2 d)q	0.09x-0.98x
Dpt,2"1	(1 + √d21-sκ)W	2 ∙ (32 + d(s + 2))	1.67x-1.78x
DP,s nat	(61 + 8 √d21-sκ)		2 ∙ (8 + d(log2 S + 2))	3.19x-4.10x
Table 3: Our compression techniques can speed up the overall runtime (number of iterations T (ω)
times the bits sent per iteration) of distributed SGD. We assume binary32 floating point representation,
bi-directional compression using C , and the same speed of communication from worker to master
(Wi 7→ M) and back (M 7→ Wi). The relative number of iterations (communications) sufficient
to guarantee ε optimality is T0(ω) := (ω + 1)θ, where θ ∈ (1, 2] (see Theorem 6). Note that big
n regime leads to better iteration bound T (ω) since for big n we have θ ≈ 1, while for small n
We have θ ≈ 2. For dithering, K = min{1, √d21-s}. The 2.81 × speedup for Cnat is obtained for
θ = 1, and the 3.16× speedup for θ = 0. The speedup figures were calculated ford = 106, p = 2
(dithering),optimal choice of s (dithering), and q = 0.1d (sparsification).
D.7.2 Model 2
For the second model, We assume that the master communicates much faster than Workers thus com-
munication from Workers is the bottleneck and We don’t need to compress updates after aggregation,
thus CM is identity operator With ωM = 0. This is the case We mention in the main paper. For
completeness, We provide the same table here.
D.7.3 MODEL 3
Similarly to previous sections, We also do the comparison for methods that might be used for In-
NetWork Aggregation. Note that for INA, it is useful to do compression also from master back to
Workers as the master Works just With integers, hence in order to be compatible With floats, it needs
to use bigger integers format. Moreover, Cnat compression guarantees free translation to floats. For
the third model, We assume We have the same assumptions on communication as for Model 1. As a
baseline, We take SGD With Cnat as this is the most simple analyzable method, Which supports INA.
D.7.4 MODEL 4
Here, We do the same comparison as for Model 3. In contrast, for communication We use the same
assumptions as for Model 2.
29
Under review as a conference paper at ICLR 2021
Approach	CWi	No. iterations T 0(ωw) = O((ωw + 1)θ)	Bits per 1 iter. Wi → M	Speedup Factor
Baseline New	-identity- Cnat	1 (9∕8)θ	32d 	9d		1 3.2x-3.6x
Sparsification New	Sq Cnat ◦ Sq	(d∕qp 	(9d∕8q)θ		(33 + log2 d)q (10 + log2 d)q	0.6×-6.0× 1.0x-1O7x
Dithering New	s-1 Dsta Dpat	(1 + κd1∕r21-s)θ (9/8 + κd121-s)θ	31 + d(2 + s) 31 + d(2 + log2 s)	1.8x-15.9x 4.1x-16.0x
Table 4: The overall speedup of distributed SGD with compression on nodes via CWi over a Baseline
variant without compression. Speed is measured by multiplying the # communication rounds (i.e.,
iterations T(ωW)) by the bits sent from worker to master (Wi 7→ M) per 1 iteration. We neglect
M 7→ Wi communication as in practice this is much faster. We assume binary32 representation. The
relative # iterations sufficient to guarantee ε optimality is T0(ωW) := (ωW + 1)θ, where θ ∈ (0, 1]
(see Theorem 6). Note that in the big n regime the iteration bound T(ωW) is better due to θ ≈ 0
(however, this is not Very practical as n is usually small), while for small n We have θ ≈ 1. For
dithering, r = min{p, 2}, K = min{1, √d21-s}. The lower bound for the Speedup Factor is
obtained for θ = 1, and the upper bound for θ = 0. The Speedup Factor (TTW))32dits) figures were
calculated ford = 106, q = 0.1d, p = 2 and optimal choice of s with respect to speedup.
Approach	C	Slowdown (iters / baseline)	Bits per iter. Wi → M + M → Wi	Speedup factor
Baseline	Cnat	1	2 ∙ 9d	1
Sparsification	Sq〈Cnat	(d∕q)1+θ	一	2 ∙(10 + log? d)q	0.03x-O30x-
Dithering	Dp,$ 2∙ynat	(9/8 + Kd r 21-s)1+θ	2 ∙ (8 + d(2 + log2 s))	1.14x-1.30x
Table 5: Overall speedup (number of iterations T times the bits sent per iteration (Wi 7→ M + M 7→
Wi) of distributed SGD. We assume binary32 floating point representation, bi-directional compression
using the same compression C . The relative number of iterations (communications) sufficient to
guarantee ε optimality is displayed in the third column, where θ ∈ (0, 1] (see Theorem 6). Note
that big n regime leads to smaller slowdown since for big n we have θ ≈ 0, while for small n
we have θ ≈ 1. For dithering, we chose P = 2 and K = min{1, √d21-s}. The speedup factor
figures were calculated ford = 106, p = 2 (dithering),optimal choice of s (dithering), and q = 0.1d
(sparsification).
Approach	CWi	CM	Slowdown (iters / baseline)	Wi → M commun. (bits / iteration)	Speedup factor
Baseline	Cnat	Cnat	1	9d	1
Sparsification	Sq ◦Cnat	Cnat	(d∕q)θ	一	(10 + log2 d)q	0.30x-3.0θx-
Dithering	Dpai	Cnat	(9∕8 + Kd r 21-s)θ	(8 + d(2 + log2 S))	1.3x-4.5x
Table 6: Overall speedup (number of iterations T times the bits sent per iteration (Wi 7→ M) of
distributed SGD. We assume binary32 floating point representation, bi-directional compression using
CWi , CM . The relative number of iterations (communications) sufficient to guarantee ε optimality
is displayed in the third column, where θ ∈ (0, 1] (see Theorem 6). Note that big n regime leads to
smaller slowdown since for big n we have θ ≈ 0, while for small n we have θ ≈ 1. For dithering,
we choseP = 2 and K = min{1, √d21-s}. The speedup factor figures were calculated for d = 106,
p = 2 (dithering),optimal choice of s (dithering), and q = 0.1d (sparsification).
D.7.5 Communication strategies used in Tables 1, 3, 5, 6
No Compression or Cnat . Each worker has to communicate a (possibly dense) d dimensional vector
of scalars, each represented by 32 or 9 bits, respectively.
30
Under review as a conference paper at ICLR 2021
Sparsification Sq with or without Cnat . Each worker has to communicate a sparse vector of q
entries with full 32 or limited 9 bit precision. We assume that q is small, hence one would prefer to
transmit positions of non-zeros, which takes q(log2 (d) + 1) additional bits for each worker.
Dithering (Dspt,as or Dnat). Each worker has to communicate 31(8 - Dnat) bits (sign is always positive,
so does not need to be communicated) for the norm, and log2 (s) + 1 bits for every coordinate for
level encoding (assuming uniform encoding) and 1 bit for the sign.
D.8 Sparsification - Formal Definition
Here we give a formal definition of the sparsification operator Sq used in Tables 1, 3,5,6.
Definition 5 (Random sparsification). Let 1 ≤ q ≤ d be an integer, and let ◦ denote the Hadamard
(element-wise) product. The random sparsification operator Sq : Rd → Rd is defined as follows:
Sq (x) = — ∙ ξ ◦ x,
where ξ	∈ Rd is a random vector chosen uniformly from the collection of all binary vectors
y ∈ {0, 1}d with exactly q nonzero entries (i.e., kyk0 = q}).
The next result describes the variance of Sq :
Theorem 10. Sq ∈ B(d/q - 1).
Notice that in the special case q = d, Sq reduces to the identity operator (i.e., no compression is
applied), and Theorem 10 yields a tight variance estimate: d/d - 1 = 0.
Proof. See e.g. Stich et al. (2018)(Lemma A.1).
□
Let us now compute the variance of the composition Cnat ◦ Sq. Since Cnat ∈ B(1/8) (Theorem 1)
and Sq ∈ B(d/q - 1) (Theorem 10), in view of the our composition result (Theorem 3) we have
CW = Cnat ◦ Sq ∈ B(ωW),
where
1
ωW = 8
(18)
E Limitations and Extensions
Quantization techniques can be divided into two categories: biased (Alistarh et al., 2018; Stich
et al., 2018) and unbiased (Alistarh et al., 2017; Wen et al., 2017; Wangni et al., 2018). While the
focus of this paper was on unbiased quantizations, it is possible to combine our natural quantization
mechanisms in conjunction with biased techniques, such as the TopK sparsifier proposed in Dryden
et al. (2016); Aji & Heafield (2017) and recently analyzed in Alistarh et al. (2018); Stich et al. (2018),
and still obtain convergence guarantees.
31