Under review as a conference paper at ICLR 2021
Decoupling Exploration and Exploitation for
Meta-Reinforcement Learning without Sacri-
FICES
Anonymous authors
Paper under double-blind review
Ab stract
The goal of meta-reinforcement learning (meta-RL) is to build agents that can
quickly learn new tasks by leveraging prior experience on related tasks. Learning
a new task often requires both exploring to gather task-relevant information and
exploiting this information to solve the task. In principle, optimal exploration and
exploitation can be learned end-to-end by simply maximizing task performance.
However, such meta-RL approaches struggle with local optima due to a chicken-
and-egg problem: learning to explore requires good exploitation to gauge the
exploration’s utility, but learning to exploit requires information gathered via
exploration. Optimizing separate objectives for exploration and exploitation can
avoid this problem, but prior meta-RL exploration objectives yield suboptimal
policies that gather information irrelevant to the task. We alleviate both concerns
by constructing an exploitation objective that automatically identifies task-relevant
information and an exploration objective to recover only this information. This
avoids local optima in end-to-end training, without sacrificing optimal exploration.
Empirically, Dream substantially outperforms existing approaches on complex
meta-RL problems, such as sparse-reward 3D visual navigation.1
1	Introduction
A general-purpose agent should be able to perform multiple related tasks across multiple related
environments. Our goal is to develop agents that can perform a variety of tasks in novel environments,
based on previous experience and only a small amount of experience in the new environment. For
example, we may want a robot to cook a meal (a new task) in a new kitchen (the environment)
after it has learned to cook other meals in other kitchens. To adapt to a new kitchen, the robot must
both explore to find the ingredients, and use this information to cook. Existing meta-reinforcement
learning (meta-RL) methods can adapt to new tasks and environments, but, as we identify in this
work, struggle when adaptation requires complex exploration.
In the meta-RL setting, the agent is presented with a set of meta-training problems, each in an
environment (e.g., a kitchen) with some task (e.g., make pizza); at meta-test time, the agent is given a
new, but related environment and task. It is allowed to gather information in a few initial (exploration)
episodes, and its goal is to then maximize returns on all subsequent (exploitation) episodes, using this
information. A common meta-RL approach is to learn to explore and exploit end-to-end by training
a policy and updating exploration behavior based on how well the policy later exploits using the
information discovered from exploration (Duan et al., 2016; Wang et al., 2016a; Stadie et al., 2018;
Zintgraf et al., 2019; Humplik et al., 2019). With enough model capacity, such approaches can express
optimal exploration and exploitation, but they create a chicken-and-egg problem that leads to bad
local optima and poor sample efficiency: Learning to explore requires good exploitation to gauge the
exploration’s utility, but learning to exploit requires information gathered via exploration; therefore,
with only final performance as signal, one cannot be learned without already having learned the other.
For example, a robot chef is only incentivized to explore and find the ingredients if it already knows
how to cook, but the robot can only learn to cook if it can already find the ingredients by exploration.
To avoid the chicken-and-egg problem, we propose to optimize separate objectives for exploration and
exploitation by leveraging the problem ID—an easy-to-provide unique one-hot for each training meta-
1Project web page: https://anonymouspapersubmission.github.io/dream/
1
Under review as a conference paper at ICLR 2021
training task and environment. Such a problem ID can be realistically available in real-world meta-RL
tasks: e.g., in a robot chef factory, each training kitchen (problem) can be easily assigned a unique
ID, and in a recommendation system that provides tailored recommendations to each user, each user
(problem) is typically identified by a unique username. Some prior works (Humplik et al., 2019;
Kamienny et al., 2020) also use these problem IDs, but not in a way that avoids the chicken-and-egg
problem. Others (Rakelly et al., 2019; Zhou et al., 2019b; Gupta et al., 2018; Gurumurthy et al., 2019;
Zhang et al., 2020) also optimize separate objectives, but their exploration objectives learn suboptimal
policies that gather task-irrelevant information (e.g., the color of the walls). Instead, we propose
an exploitation objective that automatically identifies task-relevant information, and an exploration
objective to recover only this information. We learn an exploitation policy without the need for
exploration, by conditioning on a learned representation of the problem ID, which provides task-
relevant information (e.g., by memorizing the locations of the ingredients for each ID / kitchen). We
also apply an information bottleneck to this representation to encourage discarding of any information
not required by the exploitation policy (i.e., task-irrelevant information). Then, we learn an exploration
policy to only discover task-relevant information by training it to produce trajectories containing
the same information as the learned ID representation (Section 4). Crucially, unlike prior work, we
prove that our separate objectives are consistent: optimizing them yields optimal exploration and
exploitation, assuming expressive-enough policy classes and enough meta-training data (Section 5.1).
Overall, we present two core contributions: (i) we articulate and formalize a chicken-and-egg coupling
problem between optimizing exploration and exploitation in meta-RL (Section 4.1); and (ii) we
overcome this with a consistent decoupled approach, called DREAM: Decoupling exploRation and
ExploitAtion in Meta-RL (Section 4.2). Theoretically, in a simple tabular example, we show that
addressing the coupling problem with Dream provably improves sample complexity over existing
end-to-end approaches by a factor exponential in the horizon (Section 5). Empirically, we stress test
Dream’s ability to learn sophisticated exploration strategies on 3 challenging, didactic benchmarks
and a sparse-reward 3D visual navigation benchmark. On these, Dream learns to optimally explore
and exploit, achieving 90% higher returns than existing state-of-the-art approaches (Pearl, E-RL2,
Import, VariBAD), which struggle to learn an effective exploration strategy (Section 6).
2	Related Work
We draw on a long line of work on learning to adapt to related tasks (Schmidhuber, 1987; Thrun &
Pratt, 2012; Naik & Mammone, 1992; Bengio et al., 1991; 1992; Hochreiter et al., 2001; Andrychow-
icz et al., 2016; Santoro et al., 2016). Many meta-RL works focus on adapting efficiently to a new
task from few samples without optimizing the sample collection process, via updating the policy
parameters (Finn et al., 2017; Agarwal et al., 2019; Yang et al., 2019; Houthooft et al., 2018; Men-
donca et al., 2019), learning a model (Nagabandi et al., 2018; Sfmundsson et al., 2018; Hiraoka et al.,
2020), multi-task learning (Fakoor et al., 2019), or leveraging demonstrations (Zhou et al., 2019a). In
contrast, we focus on problems where targeted exploration is critical for few-shot adaptation.
Approaches that specifically explore to obtain the most informative samples fall into two main
categories: end-to-end and decoupled approaches. End-to-end approaches optimize exploration and
exploitation end-to-end by updating exploration behavior from returns achieved by exploitation (Duan
et al., 2016; Wang et al., 2016a; Mishra et al., 2017; Rothfuss et al., 2018; Stadie et al., 2018; Zintgraf
et al., 2019; Humplik et al., 2019; Kamienny et al., 2020; Dorfman & Tamar, 2020). These approaches
can represent the optimal policy (Kaelbling et al., 1998), but they struggle to escape local optima
due to a chicken-and-egg problem between learning to explore and learning to exploit (Section 4.1).
Several of these approaches (Humplik et al., 2019; Kamienny et al., 2020) also leverage the problem
ID during meta-training, but they still learn end-to-end, so the chicken-and-egg problem remains.
Decoupled approaches instead optimize separate exploration and exploitation objectives, via, e.g.,
Thompson-sampling (TS) (Thompson, 1933; Rakelly et al., 2019), obtaining exploration trajectories
predictive of dynamics or rewards (Zhou et al., 2019b; Gurumurthy et al., 2019; Zhang et al., 2020),
or exploration noise (Gupta et al., 2018). While these works do not identify the chicken-and-egg
problem, decoupled approaches coincidentally avoid it. However, existing decoupled approaches,
including those (Rakelly et al., 2019; Zhang et al., 2020) that leverage the problem ID, do not learn
optimal exploration: TS (Rakelly et al., 2019) explores by guessing the task and executing a policy for
that task, and hence cannot represent exploration behaviors that are different from exploitation (Russo
et al., 2017). Predicting the dynamics (Zhou et al., 2019b; Gurumurthy et al., 2019; Zhang et al.,
2020) is inefficient when only a small subset of the dynamics are relevant to solving the task. In
2
Under review as a conference paper at ICLR 2021
Cook soup exploration episode
S	(gather information)
exploitation episode 1 exploitation episode N
(solve the task and maximize returns)
BH ⑤ ©... G)
Cook PiZZa exploration episode
S 4 RM ` T恒( (gather information)
Trial 1
Trial 2
Figure 1: Meta-RL setting: Given a new environment and task, the agent is allowed to first explore and gather
information, and then must use this information to solve the task in subsequent exploitation episodes.
contrast, we propose a separate mutual information objective for exploration, which both avoids the
chicken-and-egg problem and yields optimal exploration when optimized (Section 5). Past work
(Gregor et al., 2016; Houthooft et al., 2016; Eysenbach et al., 2018; Warde-Farley et al., 2018) also
optimize mutual information objectives, but not for meta-RL.
Exploration in general RL. The general RL setting (i.e., learning from scratch) also requires
targeted exploration to gather informative samples that enables learning a policy to solve the problem.
In contrast to exploration algorithms for general RL (Bellemare et al., 2016; Pathak et al., 2017;
Burda et al., 2018; Leibfried et al., 2019), which must visit many novel states to find regions with
high reward, exploration in meta-RL can be even more targeted by leveraging prior experience from
different problems during meta-training. As a result, DREAM can learn new tasks in just two episodes
(Section 6), while learning from scratch can require thousands or even millions of episodes.
3	Preliminaries
Meta-reinforcement learning. The meta-RL setting considers a family of Markov decision processes
(MDPs) (S, A, Rμ, Tμi with states S, actions A, rewards Rμ, and dynamics Tμ, indexed by a one-
hot problem ID μ ∈ M, drawn from a distribution p(μ). Colloquially, We refer to the dynamics as the
environment, the rewards as the task, and the entire MDP as the problem. Borrowing terminology from
Duan et al. (2016), meta-training and meta-testing both consist of repeatedly running trials. Each
trial consists of sampling a problem ID μ 〜p(μ) and running N + 1 episodes on the corresponding
problem. Following prior evaluation settings (Finn et al., 2017; Rakelly et al., 2019; Rothfuss et al.,
2018; Fakoor et al., 2019), we designate the first episode in a trial as an exploration episode consisting
of T steps for gathering information, and define the goal as maximizing the returns in the subsequent
N exploitation episodes (Figure 1). Following Rakelly et al. (2019); Humplik et al. (2019); Kamienny
et al. (2020), the easy-to-provide problem ID is available for meta-training, but not meta-testing trials.
We formally express the goal in terms of an exploration policy πexp used in the exploration episode
and an exploitation policy πtask used in exploitation episodes, but these policies may be the same
or share parameters. Rolling out πexp in the exploration episode produces an exploration trajectory
τexp = (s0, a0, r0, . . . , sT), which contains information discovered via exploration. The exploitation
policy πtask may then condition on τexp and optionally, its history across all exploitation episodes in a
trial, to maximize exploitation episode returns. The goal is therefore to maximize:
J(∏exp, ∏task) = Eμ〜p(μ),τexp〜∏exp [V^(7exp； 〃)] ,	(1)
where Vtask(τexp; μ) is the expected returns of ∏task conditioned on Texp, summed over the N exploita-
tion episodes in a trial with problem ID μ.
End-to-end meta-RL. A common meta-RL approach (Wang et al., 2016a; Duan et al., 2016; Rothfuss
et al., 2018; Zintgraf et al., 2019; Kamienny et al., 2020; Humplik et al., 2019) is to learn to explore
and exploit end-to-end by directly optimizing J in (1), updating both from rewards achieved during
exploitation. These approaches typically learn a single recurrent policy ∏(at | st,τQ for both
exploration and exploitation (i.e., πtask = πexp = π), which takes action at given state st and history
of experiences spanning all episodes in a trial T：t = (s0, a0, r。,..., st-ι, at-ι,rt-ι). Intuitively, this
policy is learned by rolling out a trial, producing an exploration trajectory τexp and, conditioned on
Texp and the exploitation experiences so far, yielding some exploitation episode returns. Then, credit
is assigned to both exploration (producing Texp) and exploitation by backpropagating the exploitation
returns through the recurrent policy. Critically, estimates of the expected exploitation returns in (1)
(e.g., from a single roll-out or value-function approximation) form the learning signal for exploration.
Directly optimizing the objective J this way can learn optimal exploration and exploitation strategies,
but optimization is challenging, which we show in Section 4.1.
3
Under review as a conference paper at ICLR 2021
(b)
(a) Coupled Exploration and Exploitation
(End-to-end Optimization)	∣---------
Learned
FiXed
(shaded)
DREAM: Decoupled ExploRation and ExploitAtion in Meta-RL
Figure 2: (a) Coupling between the exploration policy πexp and exploitation policy πtask. These policies are
illustrated separately for clarity, but may be a single policy. Since the two policies depend on each other (for
gradient signal and the τexp distribution), it is challenging to learn one when the other policy has not learned. (b)
DREAM: πexp and πtask are learned from decoupled objectives by leveraging a simple one-hot problem ID during
meta-training. At meta-test time, the exploitation policy conditions on the exploration trajectory as before.
4	Decoupling Exploration and Exploitation
4.1	The Problem with Coupling Exploration and Exploitation
We begin by showing that end-to-end optimization struggle with local optima due to a chicken-and-
egg problem. Figure 2a illustrates this. Learning πexp relies on gradients passed through πtask. If πtask
cannot effectively solve the task, then these gradients will be uninformative. However, to learn to
efficiently solve the task, πtask needs good exploration data (trajectories τexp) from a good exploration
policy πexp. This results in bad local optima as follows: if our current (suboptimal) πtask obtains low
rewards with a good informative trajectory τgeoxopd, the low reward would cause πexp to learn to not
generate τgeoxopd. This causes πexp to instead generate trajectories τbeaxdp that lack information required to
obtain high reward, further preventing the exploitation policy πtask from learning. Typically, early
in training, both πexp and πtask are suboptimal and hence will likely reach this local optimum. In
Section 5.2, we illustrate how this local optimum can cause sample inefficiency in a simple example.
4.2	Dream: Decoupling Exploration and Exploitation in Meta-Learning
While we can sidestep the local optima of end-to-end training by optimizing separate objectives for
exploration and exploitation, the challenge is to construct objectives that yield the same optimal
solution as the end-to-end approach. We now discuss how we can use the easy-to-provide problem
IDs during meta-training to do so. A good exploration objective should encourage discovering
task-relevant distinguishing attributes of the problem (e.g., ingredient locations), and ignoring task-
irrelevant attributes (e.g., wall color). To create this objective, the key idea behind DREAM is to learn
to extract only the task-relevant information from the problem ID, which encodes all information about
the problem. Then, Dream’s exploration objective seeks to recover this task-relevant information.
Concretely, Dream extracts only the task-relevant information from the problem ID μ via a stochastic
encoder Fψ (Z | μ). To learn this encoder, We train an exploitation policy ∏task to maximize rewards,
conditioned on samples Z from Fψ (Z | μ), while simultaneously applying an information bottleneck
to z to discard information not needed by πtask (i.e., task-irrelevant information). Then, DREAM
learns an exploration policy πexp to produce trajectories with high mutual information with Z . In this
approach, the exploitation policy πtask no longer relies on effective exploration from πexp to learn, and
once Fψ (Z | μ) is learned, the exploration policy also learns independently from ∏task, decoupling
the two optimization processes. During meta-testing, when μ is unavailable, the two policies easily
combine, since the trajectories generated by πexp are optimized to contain the same information as
the encodings Z 〜Fψ (Z | μ) that the exploitation policy πtask trained on (overview in Figure 2b).
Learning the problem ID encodings and exploitation policy. We begin with learning a stochastic
encoder Fψ (Z | μ) parametrized by ψ and exploitation policy ∏^sk parametrized by θ, which
conditions on Z. We learn Fψ jointly with πθtask by optimizing the following objective:
maximize E“〜p(μ),z〜Fψ(z∣μ) V涕Sk(z; μ)
ψ,θ
×------------------------------}
^z'v"^^
Reward
-λ I(W μ),
~{{}
Information bottleneck
(2)
where Vπtas (z; μ) is the expected return of π£Sk on problem μ given and encoding z. The information
bottleneck term encourages discarding any (task-irrelevant) information from Z that does not help
maximize reward. Importantly, both terms are independent of the exploration policy πexp. This
objective is derived from forming the Lagrangian of a constrained optimization problem, where λ-1
is the dual variable, detailed in Appendix E.
4
Under review as a conference paper at ICLR 2021
We minimize the mutual information I(z; μ) by minimizing a variational upper bound on it,
Eμ [Dkl(FΨ (Z | μ)∣∣r(z))], where r is any prior and Z is distributed as pψ (Z) = fμ Fψ (Z | μ)p(μ)dμ.
Learning an exploration policy given problem ID encodings. Once we’ve obtained an encoder
Fψ (z | μ) to extract only the necessary task-relevant information required to optimally solve each task,
we can optimize the exploration policy πexp to produce trajectories that contain this same information
by maximizing their mutual information I(τexp; Z). We slightly abuse notation to use πexp to denote
the probability distribution over the trajectories τexp. Then, the mutual information I(τexp; Z) can be
efficiently maximized by maximizing a variational lower bound (Barber & Agakov, 2003) as follows:
I(τexp; Z)=	H(Z)-	H(Z	|	τexp)	≥ H(z)+	Eμ,z〜Fψ,τexp〜∏exp [log qω (z	|	Texp)]	(3)
T
=H(z) + Eμ,z〜Fψ [log qω (z)] + E”,z〜Fψ,τexp〜∏exp Elog 9ω (Z | Texp) - log qω (z | Texpi ),
t=1
where qω is any distribution parametrized by ω. We maximize the above expression over ω to learn
qω that approximates the true conditional distribution p(Z | Texp), which makes this bound tight. In
addition, We do not have access to the problem μ at test time and hence cannot sample from Fψ (z | μ).
Therefore, qω serves as a decoder that generates the encoding Z from the exploration trajectory Texp.
Recall, our goal is to maximize (3) w.r.t., trajectories Texp from the exploration policy πexp. Only the
third term depends on Texp, so we train πexp on rewards set to be this third term (information gain):
rexp(at, st+1,τexp1; μ) = Ez〜尸33“)[log qω (z | [st+1； at； TeXp/) - log qω (z | TexpJ] - c. (4)
Intuitively, the exploration reward for taking action at and transitioning to state st+1 is high if this
transition encodes more information about the problem (and hence the encoding Z 〜Fψ (z | μ)) than
was already present in the trajectory T:etx-p 1 = (s0, a0, r0, . . . , st-2, at-2, rt-2). We also include a
small penalty c to encourage exploring efficiently in as few timesteps as possible. This reward is
attractive because (i) it is independent from the exploitation policy and hence avoids the local optima
described in Section 4.1, and (ii) it is dense, so it helps with credit assignment. It is also non-Markov,
since it depends on Texp, so we maximize it with a recurrent πφexp(at | st, T:etxp), parametrized by φ.
4.3	A Practical Implementation of Dream
Altogether, Dream learns four separate neural network components, which we detail below.
1.	Encoder Fψ (z | μ): For simplicity, we parametrize the stochastic encoder by learning a determin-
istic encoding fψ(μ) and apply Gaussian noise, i.e., Fψ(z | μ) = N(fψ(μ), ρ2I). We choose a
convenient prior r(z) to be a unit Gaussian with same variance ρ2I, which makes the information
bottleneck take the form of simple '2-regularization ∣∣fψ(μ)k2.
2.	Decoder qω(z | Texp): Similarly, we parametrize the decoder qω(z | Texp) as a Gaussian cen-
tered around a deterministic encoding gω (Texp) with variance ρ2I. Then, qω maximizes
Eμ,z 〜Fψ (z∣μ) [kz - gω (T 刃||2] w∙r∙t∙, ω (Equation 3), and the exploration rewards take the form
rexp(a, s0,Texp; μ) = ∣∣fψ(μ) - gω([texp; a； s0])∣2 - ∣fψ(μ) - gω([texp])∣2 - c (Equation 4).
3.	Exploitation policy ∏task and 4. Exploration policy ∏exp: We learn both policies with double deep
Q-learning (van Hasselt et al., 2016), treating (s, z) as the state for πθtask.
For convenience, we jointly learn all components in an EM-like fashion, where in the exploration
episode, we assume fψ and πθtask are fixed. Appendix A includes all details and a summary (Algo-
rithm 1).
Overall, Dream avoids the chicken-and-egg problem in Section 4.1. Dream learns the exploitation
policy and encoder completely independently from exploration. This enables the encoder to learn
quickly, and once it has learned, it (together with the decoder) forms a learning signal for exploration
separate from the expected exploitation returns, which improves sample efficiency (Section 5.2).
5	Analysis of Dream
5.1	Theoretical Consistency of the Dream Objective
A key property of DREAM is that it is consistent: maximizing our decoupled objective also maximizes
expected returns (Equation 1). This contrasts prior decoupled approaches (Zhou et al., 2019b; Rakelly
5
Under review as a conference paper at ICLR 2021
Sample Complexity
20	40	60	80	100	120
Number of Actions (|A|)
Exploration Q-VaIUes
0	500	1000	1500	2000
Number of Samples
Exploitation Q-values
0	500	1000	1500	2000
Number of Samples
0	500	1000	1500	2000
Number of Samples
0
Figure 3: (a) Sample complexity of learning the optimal exploration policy as the action space |A| grows (1000
ʌ ʌ
seeds). (b) Exploration Q-values Qexp(a). The policy arg maxa Qexp(a) is optimal after the dot. (c) Exploitation
values given optimal trajectory Vtask(T?xp). (d) Returns achieved on a tabular MDP With |A| 二 8 (3 seeds).
et al., 2019; Gupta et al., 2018; Gurumurthy et al., 2019; Zhang et al., 2020), which also decouple
exploration from exploitation, but do not recover the optimal policy even With infinite data. Formally,
Proposition 1. Assume(S, A, Rμ, Tμ〉is ergodic for all problems μ ∈ M. Let V*(μ) be the
maximum expected returns achievable by any exploitation policy with access to the problem ID μ,
i.e., with complete information. Let n?sk, πe^xp, F? and q?(Z | τexp) be the optimizers ofthe DREAM
objective. Then, if the function classes D REAM optimizes over are well-specified, there exists a finite
T such that if the length of the exploration episode is at least T,
task
V ? (z, μ)J = Eμ〜p(μ) [V (μ)] ∙
Optimizing DREAM,s objective achieves the maximal returns V*(μ) even without access to μ during
meta-testing (proof in Appendix C.1). We can remove the ergodicity assumption by increasing the
number of exploration episodes, and Dream performs well on non-ergodic MDPs in our experiments.
5.2	An Example Illustrating the Impact of Coupling on Sample Complexity
With enough capacity, end-to-end approaches can also learn the optimal policy, but can be highly
sample inefficient due to the coupling problem in Section 4.1. We highlight this in a simple tabular
example to remove the effects of function approximation: Each episode is a one-step bandit problem
with action space A. Taking action a? in the exploration episode leads to a trajectory TexP that reveals
the problem ID μ; all other actions a reveal no information and lead to TaXp. The ID μ identifies a
unique action that receives reward 1 during exploitation; all other actions get reward 0. Therefore,
taking a? during exploration is necessary and sufficient to obtain optimal reward 1. We now study the
number of samples required for RL2 (the canonical end-to-end approach) and Dream to learn the
optimal exploration policy with -greedy tabular Q-learning. We precisely describe a more general
setup in Appendix C.2 and prove that Dream learns the optimal exploration policy in Ω(∣A∣H |M|)
times fewer samples than RL2 in this simple setting with horizon H. Figure 3a empirically validates
this result and we provide intuition below.
In the tabular analog of RL2, the exploitation Q-values form targets for the exploration Q-values:
QeXP(a) - Vtask(τaexp) := max。，Qtask(τaexp, a0). We drop the fixed initial state from notation. This
creates the local optimum in Section 4.1. Initially Vtask(T?xp) is low, as the exploitation policy has
not learned to achieve reward, even when given T?xp. This causes Qexp(a?) to be small and therefore
argmax。QeXP(a) = a? (Figure 3b), which then prevents Vtask(T?xp) from learning (Figure 3c) as
TeXP is roughly sampled only once per * episodes. This effect is mitigated only when Qexp(a?)
becomes higher than Qexp(a) for the other uninformative a’s (the dot in Figure 3b-d). Then, learning
both the exploitation and exploration Q-values accelerates, but getting there takes many samples.
ʌ
In Dream, the exploration Q-values regress toward the decoder q: Qexp(a) - log q(μ | Texp(a)).
This decoder learns much faster than Qtask, since it does not depend on the exploitation actions.
Consequently, Dream’s exploration policy quickly becomes optimal (dot in Figure 3b), which
enables quickly learning the exploitation Q-values and achieving high reward (Figures 3c and 3d).
In general, Dream learns in far fewer samples than end-to-end approaches, since in end-to-end
approaches like RL2, exploration is learned from a quantity requiring many samples to accurately
estimate (i.e., the exploitation Q-values in this case). Initially, this quantity is estimated poorly, so the
signal for exploration can erroneously "down weight" good exploration behavior, which causes the
6
Under review as a conference paper at ICLR 2021
chicken-and-egg problem. In contrast, in Dream, the exploration policy learns from the decoder,
which requires far fewer samples to accurately estimate, avoiding the chicken-and-egg problem.
6	Experiments
Many real-world problem distributions (e.g., cooking) require exploration (e.g., locating ingredients)
that is distinct from exploitation (e.g., cooking these ingredients). Therefore, we desire benchmarks
that require distinct exploration and exploitation to stress test aspects of exploration in meta-RL, such
as if methods can: (i) efficiently explore, even in the presence of distractions; (ii) leverage informative
objects (e.g., a map) to aid exploration; (iii) learn exploration and exploitation strategies that general-
ize to unseen problems; (iv) scale to challenging exploration problems with high-dimensional visual
observations. Existing benchmarks (e.g., MetaWorld (Yu et al., 2019) or MuJoCo tasks like HalfChee-
tahVelocity (Finn et al., 2017; Rothfuss et al., 2018)) were not designed to test exploration and are un-
suitable for answering these questions. These benchmarks mainly vary the rewards (e.g., the speed to
run at) across problems, so naively exploring by exhaustively trying different exploitation behaviors
(e.g., running at different speeds) is optimal. They further don’t include visual states, distractors, or in-
formative objects, which test if exploration is efficient. We therefore design new benchmarks meeting
the above criteria, testing (i-iii) with didactic benchmarks, and (iv) with a sparse-reward 3D visual nav-
igation benchmark, based on Kamienny et al. (2020), that combines complex exploration with high-
dimensional visual inputs. To further deepen the exploration challenge, we make our benchmarks
goal-conditioned. This requires exploring to discover information relevant to any potential goal, rather
than just a single task (e.g., locating all ingredients for any meal vs. just the ingredients for pasta).
Comparisons. We compare DREAM with state-of-the-art end-to-end (E-RL2 (Stadie et al., 2018),
VariBAD (Zintgraf et al., 2019), and Import (Kamienny et al., 2020)) and decoupled approaches
(Pearl-UB, an upper bound on the final performance of Pearl (Rakelly et al., 2019)). For Pearl-
UB, we analytically compute the expected rewards achieved by optimal Thompson sampling (TS)
exploration, assuming access to the optimal problem-specific policy and true posterior problem
distribution. Like Dream, Import and Pearl also use the one-hot problem ID, during meta-
training. We also report the optimal returns achievable with no exploration as "No exploration."
Where applicable, all methods use the same architecture. The full architecture and approach details
are in Appendix B.3.
We report the average returns achieved by each approach in trials with one exploration and one
exploitation episode, averaged over 3 seeds with 1-standard deviation error bars (full details in
Appendix B). We periodically evaluate each approach on 100 meta-testing trials, after 2000 meta-
training trials. In all plots, the training timesteps includes all timesteps from both exploitation and
exploration episodes in meta-training trials.
6.1	Didactic Experiments
We first evaluate on the grid worlds shown in Fig-
ures 4a and 4b. The state consists of the agent’s (x, y)-
position, a one-hot indicator of the object at the agent’s po-
sition (none, bus, map, pot, or fridge), a one-hot indicator
of the agent’s inventory (none or an ingredient), and the
goal. The actions are move up, down, left, or right; ride
bus, which, at a bus, teleports the agent to another bus of
the same color; pick up, which, at a fridge, fills the agent’s
inventory with the fridge’s ingredients; and drop, which, at
the pot, empties the agent’s inventory into the pot. Episodes
consist of 20 timesteps and the agent receives -0.1 reward
at each timestep until the goal, described below, is met (de-
tails in Appendix B.1; qualitative results in Appendix B.2).
Figure 4: Didactic grid worlds to stress test
exploration. (a) Navigation. (b) Cooking.
Targeted exploration. We first test if these methods can efficiently explore in the presence of
distractions in two versions of the benchmark in Figure 4a: distracting bus and map. In both, there are
4 possible goals (the 4 green locations). During each episode, a goal is randomly sampled. Reaching
the goal yields +1 reward and ends the episode. The 4 colored buses each lead to near a different
potential green goal location when ridden and in different problems μ, their destinations are set to be
1 of the 4! different permutations. The distracting bus version tests if the agent can ignore distractions
by including unhelpful gray buses, which are never needed to optimally reach any goal. In different
7
Under review as a conference paper at ICLR 2021
Dream	E-RL2	Import VARiBAD PEARL-UB 一一， Optimal 一一， NoeXpIoration
Figure 6: Cooking results: only Dream achieves optimal reward on training problems (left) logged every
10 meta-training trials, and on generalizing to unseen problems (middle). 3D visual navigation results: only
Dream reads the sign and solves the task (right).
problems, the gray buses lead to different permutations of the gray locations. The map version tests if
the agent can leverage objects for exploration by including a map that reveals the destinations of the
colored buses when touched.
Figure 5 shows the results after 1M steps. Dream learns to
optimally explore and thus receives optimal reward in both
versions: In distracting bus, it ignores the unhelpful gray
buses and learns the destinations of all helpful buses by rid-
ing them. In map, it learns to leverage informative objects,
by visiting the map and ending the exploration episode. Dur-
ing exploitation, Dream immediately reaches the goal by
riding the correct colored bus. In contrast, Import and E-
RL2 get stuck in a local optimum, indicative of the coupling
problem (Section 4.1), which achieves the same returns as no
exploration at all. They do not explore the helpful buses or
map and consequently sub-optimally exploit by just walking
to the goal. VariBAD learns slower, likely because it learns
a dynamics model, but eventually matches the sub-optimal
returns of Import and RL2 in ~3M steps (not shown).
Drea DreAM
DReAM (no bottleneck)
E-RL E-RL2
Import
VARiBAD
——PeArL-UB
■ - Optimal
■ - No exploration
Figure 5: Navigation results. Only Dream
optimally explores all buses and the map.
Pearl achieves sub-optimal returns, even with infinite meta-training (see line for Pearl-UB), as
follows. TS explores by sampling a problem ID from its posterior and executing its policy conditioned
on this ID. Since for any given problem (bus configuration) and goal, the optimal problem-specific
policy rides the one bus leading to the goal, TS does not explore optimally (i.e., explore all the buses or
read the map), even with the optimal problem-specific policy and true posterior problem distribution.
Recall that Dream tries to remove extraneous information from the problem ID with an information
bottleneck that minimizes the mutual information I(z; μ) between problem IDs and the encoder
Fψ (Z | μ). In distracting bus, We test the importance of the information bottleneck by ablating it from
Dream. As seen in Figure 5 (left), this ablation (Dream (no bottleneck)) wastes its exploration on
the gray unhelpful buses, since they are part of the problem, and consequently achieves low returns.
Generalization to new problems. We test generalization to unseen problems in a cooking bench-
mark (Figure 4b). The fridges on the right each contain 1 of 4 different (color-coded) ingredients,
determined by the problem ID. The fridges’ contents are unobserved until the agent uses the "pickup"
action at the fridge. Goals (recipes) specify placing 2 correct ingredients in the pot in the right order.
The agent receives positive reward for picking up and placing the correct ingredients, and negative
reward for using the wrong ingredients. We hold out 1 of the 43 = 64 problems from meta-training.
Figure 6 shows the results on training (left) and held-out (middle) problems. Only Dream achieves
near-optimal returns on both. During exploration, it investigates each fridge with the "pick up" action,
and then directly retrieves the correct ingredients during exploitation. E-RL2 gets stuck in a local
optimum, only sometimes exploring the fridges. This achieves 3.8x lower returns, only slightly higher
than no exploration at all. Here, leveraging the problem ID actually hurts Import compared to E-
RL2. Import successfully solves the task, given access to the problem ID, but fails without it. As
before, VariBAD learns slowly and TS (Pearl-UB) cannot learn optimal exploration.
8
Under review as a conference paper at ICLR 2021
6.2	Sparse-Reward 3D Visual Navigation
We conclude with a challenging benchmark testing both
sophisticated exploration and scalability to pixel inputs. We
modify a benchmark from Kamienny et al. (2020) to increase
both the exploration and scalability challenge by including
more objects and a visual sign, illustrated in Figure 7. In
the 3 different problems, the sign on the right says “blue”,
“red” or “green.” The goals specify whether the agent should
collect the key or block. The agent receives +1 reward for
collecting the correct object (color specified by the sign,
shape specified by the goal), -1 reward for the wrong object,
and 0 reward otherwise. The agent begins the episode on Figure 7: 3D Visual Navigation. The agent
the far side of the barrier and must walk around the barrier must read the sign to determine what col-
to visually “read” the sign. The agent’s observations are ored object to go to.
80 × 60 RGB images and its actions are to rotate left or right, move forward, or end the episode.
Dream is the only method that learns to read the sign and achieve reward (Figure 6 right). All end-
to-end approaches get stuck in local optima due to the chicken-and-egg coupling problem, where they
do not learn to read the sign and hence stay away from all the objects, in fear of receiving negative
reward. This achieves close to 0 returns, consistent with the results in Kamienny et al. (2020). As
before, Pearl-UB cannot learn optimal exploration.
7 Conclusion
In summary, this work identifies a chicken-and-egg problem that end-to-end meta-RL approaches
suffer from, where learning good exploitation requires already having learned good exploration and
vice-versa. This creates challenging local optima, since typically neither exploration nor exploitation
is good at the beginning of meta-training. We show that appropriately leveraging simple one-hot
problem IDs allows us to break this cyclic dependency with Dream. Consequently, Dream has
strong empirical performance on meta-RL problems requiring complex exploration, as well as
substantial theoretical sample complexity improvements in the tabular setting. Though prior works
also leverage the problem ID and use decoupled objectives that avoid the chicken-and-egg problem, no
other existing approaches can recover optimal exploration empirically and theoretically like Dream.
References
R.	Agarwal, C. Liang, D. Schuurmans, and M. Norouzi. Learning to generalize from sparse and
underspecified rewards. arXiv preprint arXiv:1902.07198, 2019.
M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and
N. D. Freitas. Learning to learn by gradient descent by gradient descent. In Advances in neural
information processing systems, pp. 3981—3989, 2016.
D. Barber and F. V. Agakov. The IM algorithm: a variational approach to information maximization.
In Advances in neural information processing systems, 2003.
M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, and R. Munos. Unifying count-
based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1471-1479, 2016.
S.	Bengio, Y. Bengio, J. Cloutier, and J. Gecsei. On the optimization of a synaptic learning rule. In
Preprints Conf. Optimality in Artificial and Biological Neural Networks, volume 2, 1992.
Y. Bengio, S. Bengio, and J. Cloutier. Learning a synaptic learning rule. In IJCNN-91-Seattle
International Joint Conference on Neural Networks, volume 2, pp. 969-969, 1991.
Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. arXiv
preprint arXiv:1810.12894, 2018.
M. Chevalier-Boisvert. Gym-Miniworld environment for openai gym. https://github.com/
maximecb/gym-miniworld, 2018.
9
Under review as a conference paper at ICLR 2021
R. Dorfman and A. Tamar. Offline meta reinforcement learning. arXiv preprint arXiv:2008.02598,
2020.
Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2 : Fast reinforcement
learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
B.	Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a
reward function. arXiv preprint arXiv:1802.06070, 2018.
R. Fakoor, P. Chaudhari, S. Soatto, and A. J. Smola. Meta-Q-learning. arXiv preprint
arXiv:1910.00125, 2019.
C.	Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning (ICML), 2017.
K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. arXiv preprint
arXiv:1611.07507, 2016.
A. Gupta, R. Mendonca, Y. Liu, P. Abbeel, and S. Levine. Meta-reinforcement learning of structured
exploration strategies. In Advances in Neural Information Processing Systems (NeurIPS), pp. 5302-
5311, 2018.
S.	Gurumurthy, S. Kumar, and K. Sycara. Mame: Model-agnostic meta-exploration. arXiv preprint
arXiv:1911.04024, 2019.
T.	Hiraoka, T. Imagawa, V. Tangkaratt, T. Osa, T. Onishi, and Y. Tsuruoka. Meta-model-based meta-
policy optimization. arXiv preprint arXiv:2006.02608, 2020.
S. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. In
International Conference on Artificial Neural Networks (ICANN), pp. 87-94, 2001.
R. Houthooft, X. Chen, Y. Duan, J. Schulman, F. D. Turck, and P. Abbeel. Vime: Variational
information maximizing exploration. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 1109-1117, 2016.
R.	Houthooft, Y. Chen, P. Isola, B. Stadie, F. Wolski, O. J. Ho, and P. Abbeel. Evolved policy gradients.
In Advances in Neural Information Processing Systems (NeurIPS), pp. 5400-5409, 2018.
J. Humplik, A. Galashov, L. Hasenclever, P. A. Ortega, Y. W. Teh, and N. Heess. Meta reinforcement
learning as task inference. arXiv preprint arXiv:1905.06424, 2019.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable
stochastic domains. Artificial intelligence, 101(1):99-134, 1998.
P. Kamienny, M. Pirotta, A. Lazaric, T. Lavril, N. Usunier, and L. Denoyer. Learning adaptive
exploration strategies in dynamic environments through informed policy regularization. arXiv
preprint arXiv:2005.02934, 2020.
S.	Kapturowski, G. Ostrovski, J. Quan, R. Munos, and W. Dabney. Recurrent experience replay
in distributed reinforcement learning. In International Conference on Learning Representations
(ICLR), 2019.
F. Leibfried, S. Pascual-Diaz, and J. Grau-Moya. A unified bellman optimality principle combining
reward maximization and empowerment. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 7869-7880, 2019.
E. Z. Liu, M. Hashemi, K. Swersky, P. Ranganathan, and J. Ahn. An imitation learning approach for
cache replacement. arXiv preprint arXiv:2006.16239, 2020a.
E. Z. Liu, R. Keramati, S. Seshadri, K. Guu, P. Pasupat, E. Brunskill, and P. Liang. Learning abstract
models for strategic exploration and fast reward transfer. arXiv preprint arXiv:2007.05896, 2020b.
R. Mendonca, A. Gupta, R. Kralev, P. Abbeel, S. Levine, and C. Finn. Guided meta-policy search. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 9653-9664, 2019.
10
Under review as a conference paper at ICLR 2021
N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel. A simple neural attentive meta-learner. arXiv
preprint arXiv:1707.03141, 2017.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529-533, 2015.
A. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn. Learning to
adapt in dynamic, real-world environments through meta-reinforcement learning. arXiv preprint
arXiv:1803.11347, 2018.
D. K. Naik and R. J. Mammone. Meta-neural networks that learn by learning. In [Proceedings 1992]
IJCNN International Joint Conference on Neural Networks, volume 1, pp. 437-442, 1992.
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga,
and A. Lerer. Automatic differentiation in pytorch, 2017.
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised
prediction. In Computer Vision and Pattern Recognition (CVPR), pp. 16-17, 2017.
K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine. Efficient off-policy meta-reinforcement
learning via probabilistic context variables. arXiv preprint arXiv:1903.08254, 2019.
J. Rothfuss, D. Lee, I. Clavera, T. Asfour, and P. Abbeel. Promp: Proximal meta-policy search. arXiv
preprint arXiv:1810.06784, 2018.
D. Russo, B. V. Roy, A. Kazerouni, I. Osband, and Z. Wen. A tutorial on thompson sampling. arXiv
preprint arXiv:1707.02038, 2017.
S. sæmundsson, K. Hofmann, and M. P. Deisenroth. Meta reinforcement learning with latent variable
gaussian processes. arXiv preprint arXiv:1803.07551, 2018.
A.	Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap. One-shot learning with memory-
augmented neural networks. arXiv preprint arXiv:1605.06065, 2016.
J. Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the
meta-meta-…hook. PhD thesis, Technische Universitat Munchen, 1987.
B.	Stadie, G. Yang, R. Houthooft, P. Chen, Y. Duan, Y. Wu, P. Abbeel, and I. Sutskever. The
importance of sampling inmeta-reinforcement learning. In Advances in Neural Information
Processing Systems (NeurIPS), pp. 9280-9290, 2018.
W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3):285-294, 1933.
S. Thrun and L. Pratt. Learning to learn. Springer Science & Business Media Springer Science &
Business Media, 2012.
L. van der Maaten and G. Hinton. Visualizing data using t-SNE. Journal of machine learning
research, 9(0):2579-2605, 2008.
H. van Hasselt, A. Guez, and D. Silver. Deep reinforcement learning with double Q-learning. In
Association for the Advancement of Artificial Intelligence (AAAI), volume 16, pp. 2094-2100, 2016.
J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran,
and M. Botvinick. Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016a.
Z. Wang, T. Schaul, M. Hessel, H. V. Hasselt, M. Lanctot, and N. D. Freitas. Dueling network
architectures for deep reinforcement learning. In International Conference on Machine Learning
(ICML), 2016b.
D. Warde-Farley, T. V. de Wiele, T. Kulkarni, C. Ionescu, S. Hansen, and V. Mnih. Unsupervised
control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018.
11
Under review as a conference paper at ICLR 2021
Y. Yang, K. Caluwaerts, A. Iscen, J. Tan, and C. Finn. Norml: No-reward meta learning. In
Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems,
pp. 323-331,2019.
T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A benchmark
and evaluation for multi-task and meta reinforcement learning. arXiv preprint arXiv:1910.10897,
2019.
J. Zhang, J. Wang, H. Hu, Y. Chen, C. Fan, and C. Zhang. Learn to effectively explore in context-
based meta-RL. arXiv preprint arXiv:2006.08170, 2020.
A. Zhou, E. Jang, D. Kappler, A. Herzog, M. Khansari, P. Wohlhart, Y. Bai, M. Kalakrishnan,
S. Levine, and C. Finn. Watch, try, learn: Meta-learning from demonstrations and reward. arXiv
preprint arXiv:1906.03352, 2019a.
W. Zhou, L. Pinto, and A. Gupta. Environment probing interaction policies. arXiv preprint
arXiv:1907.11740, 2019b.
L. Zintgraf, K. Shiarlis, M. Igl, S. Schulze, Y. Gal, K. Hofmann, and S. Whiteson. Varibad: A very
good method for bayes-adaptive deep RL via meta-learning. arXiv preprint arXiv:1910.08348,
2019.
12
Under review as a conference paper at ICLR 2021
A Dream Training Details
Algorithm 1 summarizes a practical algorithm for training Dream. We parametrize both the
exploration and exploitation policies as recurrent deep dueling double-Q networks (Wang et al.,
ʌ
2016b; van Hasselt et al., 2016), with exploration Q-values Qexp(s, τexp, a; φ) parametrized by φ
(and target network parameters φ0) and exploitation Q-Values Qtask(s, z, a; θ) parametrized by θ (and
target network parameters θ0). We train on trials with one exploration and one exploitation episode,
but can test on arbitrarily many exploitation episodes, as the exploitation policy acts on each episode
independently (i.e. it does not maintain a hidden state across episodes). Using the choices for Fψ and
qω in Section 4.3, training proceeds as follows.
We first sample a new problem for the trial and roll-out the exploration policy, adding the roll-out to a
replay buffer (lines 7-9). Then, we roll-out the exploitation policy, adding the roll-out to a separate
replay buffer (lines 10-12). We train the exploitation policy on both stochastic encodings of the
problem ID N (fψ (μ), ρ2I) and on encodings of the exploration trajectory gω (T exp).
Next, we sample from the replay buffers and update the parameters. First, we sample
(st, at, st+ι,μ, τexp)-tuples from the exploration replay buffer and perform a normal DDQN update
on the exploration Q-Value parameters φ using rewards computed from the decoder (lines 13-15).
Concretely, we minimize the following standard DDQN loss function w.r.t., the parameters φ, where
the rewards are computed according to Equation 4:
Lexp(φ) = E IlQeXP(St,τexpι, at； φ) - (rexp + YQeXp(St+ι, [丁当；at； st], aDDQN； Φ0)∣∣	,
where rexp = llfψ (μ) - gω (TeXP) l∣2 - llfψ (μ) - gω (Te-I) I∣2 -C
and aDDQN = arg max Qexp(st+ι, [τexp1; at； st]; φ).
a
We perform a similar update with the exploitation Q-Value parameters (lines 16-17). We sample
(s,a, r, s0,μ, Texp)-tuples from the exploitation replay buffer and perform a DDQN update from the
encoding of the problem ID by minimizing the following loss:
LtaSk-id(θ, ψ) = E，task(s, fψ(μ),a; θ) - (r + QtaSk(S0, fψ√(μ),aprob; θ')(],
where。皿=arg max Qtask(s0,gω (Texp), a; θ) and aprob = arg max Qtask(s0,fψ (μ), a; θ).
aa
Finally, from the same exploitation replay buffer samples, we also update the problem ID embedder
to enforce the information bottleneck (line 18) and the decoder to approximate the true conditional
distribution (line 19) by minimizing the following losses respectiVely:
LbOttleneck(Ψ) = Eμ [min(∣∣fψ (μ)k2 ,K)]
and Ldecoder(ω) = ETexp Xlfψ(μ) - gω(Texp)ll2 .
t
Since the magnitude ∣∣fψ (μ)k2 partially determines the scale of the reward, we add a hyperparameter
K and only minimize the magnitude when it is larger than K . Altogether, we minimize the following
loss:
L(φ, θ, ω, ψ) = Lexp (φ) + Ltask-id (θ, ψ) + Lbottleneck(ψ) + Ldecoder (ω).
As is standard with deep Q-learning (Mnih et al., 2015), instead of sampling from the replay buffers
and updating after each episode, we sample and perform all of these updates eVery 4 timesteps. We
periodically update the target networks (lines 20-22).
B Experiment Details
B.1	Problem Details
Distracting bus / map. Riding each of the four colored buses teleports the agent to near one of
the green goal locations in the corners. In different problems, the destinations of the colored buses
13
Under review as a conference paper at ICLR 2021
Algorithm 1 Dream DDQN
1:	Initialize exploitation replay buffer Btask = {} and exploration replay buffer Bexp = {}
2:	Initialize exploitation Q-Value Qtask parameters θ and target network parameters θ0
exp	0
3:	Initialize exploration Q-value Qexp parameters φ and target network parameters φ0
4:	Initialize problem ID embedder fψ parameters ψ and target parameters ψ0
5:	Initialize trajectory embedder gω parameters ω and target parameters ω 0
6:	for trial = 1 to max trials do
7:	Sample problem μ 〜p(μ), defining MDP(S, A, Rμ, Tμ)
8:	Roll-out e-greedy exploration policy Qexp(St, Texp, at; φ), producing trajectory Texp = (so ,a0,..., ST).
9:	Add tuples to the exploration replay buffer BeXP = BeXP ∪ {(st, at, st+ι, μ, Texp)}t.
10:	Compute embedding Z 〜N(fψ(μ), ρ21).
task
11:	Roll-out e-greedy exploitation policy Qtask(st , z, at; θ), producing trajectory (s0 , a0 , r0 , . . .) with
rt = Rμ (st+1 ).
12:	Add tuples to the exploitation replay buffer BtaSk = BtaSk ∪ {(st, at, rt, st+ι, μ, Texp)}t.
13:	Sample batches of (St, at, st+ι, μ, Texp)〜BeXP from exploration replay buffer.
14:	Compute reward rexp = kfψ(μ) - gω(TeXP)k2 — ∣∣fψ(μ) — gω(Te-1)∣∣2 — C(EqUation 4).
15:	Optimize φ with DDQN update with tuple (st, at, rtexp, st+1) with Lexp(φ)
16:	Sample batches of (s, a, r, s0, μ, Texp)〜BtaSk-id from exploitation replay buffer.
17:	Optimize θ and ψ with DDQN update with tuple ((s, μ), a, r, (s0, μ)) with Ltask-id(θ, ψ)
18:	Optimize ψ on LbOttleneck(ψ) = Vψ min(kfψ(μ)∣∣2 , K)
19:	Optimizeω on LdeCOder(ω) = Vω PtIIfψ(μ) — gω(TeXP)k2 (Equation 3)
20:	if trial ≡ 0 (mod target freq) then
21:	Update target parameters φ0 = φ, θ0 = θ , ψ 0 = ψ , ω 0 = ω
22:	end if
23:	end for
agent m map I I unhelpful bus StoP	bus ∣ ∣ goal
Figure 8: Examples of different distracting bus and map problems. (a) An example distracting bus problem.
Though all unhelpful distracting buses are drawn in the same color (gray), the destinations of the gray buses are
fixed within a problem. (b) Another example distracting bus problem. The destinations of the helpful colored
buses are a different permutation (the orange and green buses have swapped locations). This takes on permutation
3 ≡ μ (mod 4!), instead of 1. The unhelpful gray buses are also a different permutation (not shown), taking on
permutation 5 =[51.(c) An example map problem. Touching the map reveals the destinations of the colored
buses, by adding μ to the state observation.
14
Under review as a conference paper at ICLR 2021
change, but the bus positions and their destinations are fixed within each problem. Additionally, in
the distracting bus domain, the problem ID also encodes the destinations of the gray buses, which are
permutations of the four gray locations on the midpoints of the sides. More precisely, the problem ID
μ ∈ {0,1,..., 4! X 4! = 576} encodes both the permutation of the colored helpful bus destinations,
indexed as μ (mod 4!) and the permutation of the gray unhelpful bus destinations as ［纪.We hold
out most of the problem IDs during meta-training (23 × 576 = 552 are held-out for meta-training).
In the map domain, the problem μ is an integer representing which of the 4! permutations of the four
green goal locations the colored buses map to. The states include an extra dimension, which is set to
0 when the agent is not at the map, and is set to this integer value μ when the agent is at the map.
Figure 8 displays three such examples.
Cooking. In different problems, the (color-
coded) fridges contain 1 of 4 different ingre-
dients. The ingredients in each fridge are un-
known until the goes to the fridge and uses the
pickup action. Figure 9 displays three example
problems. The problem ID μ is an integer be-
tween 0 and 43, where μ = 42a + 4b + C indi-
cates that the top right fridge has ingredient a,
the middle fridge has ingredient b and the bot-
tom right fridge has ingredient c.
Figure 9: Three example cooking problems. The con-
tents of the fridges (color-coded) are different in differ-
ent problems.
The goals correspond to a recipe of placing the two correct ingredients in the pot in the right order.
Goals are tuples (a, b), which indicate placing ingredient a in the pot first, followed by ingredient
b. In a given problem, we only sample goals involving the recipes actually present in that problem.
During meta-training, We hold out a randomly selected problem μ = 11.
We use the following reward function Rμ. The agent receives a per timestep penalty of -0.1 reward
and receives +0.25 reward for completing each of the four steps: (i) picking up the first ingredient
specified by the goal; (ii) placing the first ingredient in the pot; (iii) picking up the second ingredient
specified by the goal; and (iv) placing the second ingredient in the pot. To prevent the agent from
gaming the reward function, e.g., by repeatedly picking up the first ingredient, dropping the first
ingredient anywhere but in the pot yields a penalty of -0.25 reward, and similarly for all steps. To
encourage efficient exploration, the agent also receives a penalty of -0.25 reward for picking up the
wrong ingredient.
Cooking without goals. While we evaluate on goal-conditioned benchmarks to deepen the explo-
ration challenge, forcing the agent to discover all the relevant information for any potential goal,
many standard benchmarks (Finn et al., 2017; Yu et al., 2019) don’t involve goals. We therefore in-
clude a variant of the cooking task, where there are no goals. We simply concatenate the goal (recipe)
to the problem ID μ. Additionally, we modify the rewards so that picking up the second ingredient
yields +0.25 and dropping it yields -0.25 reward, so that it is possible to infer the recipe from the
rewards. Finally, to make the problem harder, the agent cannot pick up new ingredients unless its
inventory is empty (by using the drop action), and we also increase the number of ingredients to 7.
The results are in Section B.2.
Sparse-reward 3D visual navigation. We implement this domain in Gym MiniWorld (Chevalier-
Boisvert, 2018), where the agent’s observations are 80 × 60 × 3 RGB arrays. There are three problems
μ = 0 (the sign says “blue”), μ = 1 (the sign says “red”), and μ = 2 (the sign says “green"). There
are two goals, represented as 0 and 1, corresponding to picking up the key and the box, respectively.
The reward function Aμ(s, i) is +1 for picking up the correct colored object (according to μ) and the
correct type of object (according to the goal) and -1 for picking up an object of the incorrect color or
type. Otherwise, the reward is 0. On each episode, the agent begins at a random location on the other
side of the barrier from the sign.
B.2	Additional Results
15
Under review as a conference paper at ICLR 2021
Figure 10: Examples of Dream’s learned exploration behavior. (a) Dream learns the optimal exploration
behavior on the distraction variant: riding 3 of the 4 helpful colored buses, which allows it to infer the
destinations of all colored buses and efficiently reach any goal during exploitation episodes. (a) Without the
information bottleneck, Dream also explores the unhelpful gray buses, since they are part of the problem. This
wastes exploration steps, and leads to lower returns during exploitation episodes. (c) Dream learns the optimal
exploration on the map variant: it goes to read the map revealing all the buses’ destinations, and then ends the
episode, though it unnecessarily rides one of the buses.
Analysis of the learned policies. Please see https://anonymouspapersubmission.
github.io/dream/ for videos and analysis of the exploration and exploitation behavior learned
by Dream and other approaches, which is described in text below.
Distracting bus / map. Figure 10 shows the exploration policy DREAM learns on the distracting bus
and map domains. With the information bottleneck, Dream optimally explores by riding 3 of the 4
colored buses and inferring the destination of the last colored bus (Figure 8a). Without the information
bottleneck, Dream explores the unhelpful gray buses and runs out of time to explore all of the colored
buses, leading to lower reward (Figure 8b). In the map domain, Dream optimally explores by visiting
the map and terminating the exploration episode. In contrast, the other methods (RL2, Import,
VariBAD) rarely visit the colored buses or map during exploration and consequently walk to their
destination during exploitation, which requires more timesteps and therefore receives lower returns.
In Figure 11, we additionally visualize the exploration trajectory encodings gω (τexp) and problem
ID encodings fψ (μ) that DREAM learns in the distracting bus domain by applying t-SNE (Van der
Maaten & Hinton, 2008). We visualize the encodings of all possible problem IDs as dots. They
naturally cluster into 4! = 24 clusters, where the problems within each cluster differ only in the
destinations of the gray distracting buses, and not the colored buses. Problems in the support of the
true posterior p(μ | Texp) are drawn in green, while problems outside the support (e.g., a problem that
specifies that riding the green bus goes to location (0, 1) when it has already been obserVed in τexp
that riding the orange bus goes to location (0, 1)) are drawn in red. We also plot the encoding of the
exploration trajectory τexp so far as a blue cross and the mean of the green clusters as a black square.
We find that the encoding of the exploration trajectory gω(τexp) tracks the mean of the green clusters
until the end of the exploration episode, when only one cluster remains, and the destinations of all the
colored buses has been discoVered. IntuitiVely, this captures uncertainty in what the potential problem
ID may be. More precisely, when the decoder is a Gaussian, placing gω(τexp) at the center of the
encodings of problems in the support exactly minimizes Equation 3.
16
Under review as a conference paper at ICLR 2021
ride blue bus	ride orange bus	ride green bus
• Problem encoding /g(μ) in the support of p(μ ∖ τexp)	* Trajectory encoding gω (TeXP)
• Problem encoding fψ(μ) outside the support ofp(μ ∖ τexp) ■ Mean of the green clusters
agen agent
ðpot ∙
fridge (ingredients)
pickup action
Figure 11: Dream’s learned encodings of the exploration trajectory and problems visualized with t-SNE (van der
Maaten & Hinton, 2008).
Dream	VARiBAD 一 —1 Optimal
E-RL2	PEARL-UB - i No exploration
----Import
Figure 12:	Dream learns the optimal explo-
ration policy, which learns the fridges’ con-
tents by going to each fridge and using the
pickup action.
Figure 13:	Cooking without goals results. Only
Dream learns the optimal policy, achieving ~2x
more reward than the next best approach.
Robustness to imperfections in the problem ID. Recall that the problem ID is a simple and easy-
to-provide unique one-hot for each problem. We test of Dream is robust to imperfections in the
problem ID by assigning each problem in the map benchmark 3 different problem IDs. When a
problem is sampled during meta-training, it is randomly labeled with 1 of these 3 different problem
IDs. We find that this imperfection in the problem ID does not impact Dream’s final performance at
all: it still achieves optimal exploitation returns of 0.8 after 1M time steps of training.
Cooking. Figure 12 shows the exploration policy DREAM learns on the cooking domain, which
visits each of the fridges and investigates the contents with the "pickup" action. In contrast, the
other methods rarely visit the fridges during exploration, and instead determine the locations of the
ingredients during exploitation, which requires more timesteps and therefore receives lower returns.
Cooking without goals. We provide additional results in the case where the cooking domain is
modified to not include goals (see Section B.1). The results are summarized in Figure 13 and show
the same trends as the results in original cooking benchmark. Dream learns to optimally explore
by investigating the fridges, and then also optimally exploits, by directly collecting the relevant
ingredients. The next best approach E-RL2 , only sometimes explores the fridges, again getting stuck
in a local optimum, yielding only slightly higher reward than no exploration at all.
Sparse-reward 3D visual navigation. DREAM optimally explores by walking around the barrier
and reading the sign. See https://anonymouspapersubmission.github.io/dream/
for videos. The other methods do not read the sign at all and therefore cannot solve the problem.
17
Under review as a conference paper at ICLR 2021
B.3	Other Approaches and Architecture Details
In this section, we detail the loss functions that E-RL2, Import, and VariBAD optimize, as well as
the model architectures used in our experiments. Where possible, we use the same model architecture
for all methods: Dream, E-RL2 , Import, and VariBAD. All approaches are implemented in
PyTorch (Paszke et al., 2017), using a DQN implementation adapted from Liu et al. (2020b) and code
adapted from Liu et al. (2020a).
State and problem ID embeddings. All approaches use the same method to embed the state and
problem ID. For these embeddings, we embed each dimension independently with an embedding
matrix of output dimension 32. Then, we concatenate the per-dimension embeddings and apply two
linear layers with output dimensions 256 and 64 respectively, with ReLU activations.
In the 3D visual navigation task, we use a different embedding scheme for the states, as they are
images. We apply 3 CNN layers, each with 32 output layers and stride length 2, and with kernel sizes
of 5, 5, and 4 respectively. We apply ReLU activations between the CNN layers and apply a final
linear layer to the flattened output of the CNN layers, with an output dimension of 128.
All state and problem ID embeddings below use this scheme.
Experience embeddings. E-RL2, IMPORT, VARIBAD and the exploration policy in DREAM also
learn an embedding of the history of prior experiences τexp = (s0, a0, r0, s1, . . .) and current state sT.
To do this, we first separately embed each (st+1, at, rt, dt)-tuple, where dt is an episode termination
flag (true if the episode ends on this experience, and false otherwise), as follows:
•	Embed the state st as e(st), using the state embedding scheme described above.
•	Embed the action at as e(at) with an embedding matrix of output dimension 16. We set
a-1 to be 0.
•	For the standard meta-RL setting and during exploitation episodes, embed the rewards with
a linear layer of output dimension 16. With reward-free exploration in IMRL, the rewards
are not embedded in the exploration policy of Dream, and are embedded as 0 reward for
the other approaches, since the same policy is used during both exploration and exploitation.
We set r-1 to be 0.
•	Embed the episode termination dt as e(dt) with an embedding matrix of output dimension
16. Note that d is true during all episode terminations within a trial for RL2, IMPORT, and
VARIBAD.
Then, we apply a final linear layer with output dimension 64 to the concatenation of the above
[e(st); e(at); e(rt); dt]. Finally, to obtain an embedding of the entire history τexp, we embed each
experience separately as above, and then pass an LSTM with hidden dimension 64 over the experience
embeddings, where the initial hidden and cell states are set to be 0-vectors.
DREAM. For the decoder gω (τ exp = (s0, a0, r0, s1, . . . , sT)), we embed each transition
(st, at, rt, st+1) of the exploration trajectory τexp using the same embedding scheme as above, ex-
cept we also embed the next state st+1. We do not embed the rewards in the reward-free adaptation
version of IMRL. Then, given embeddings for each transition, we embed the entire trajectory by
passing an LSTM with output dimension 128 on top of the transition embeddings, followed by two
linear layers of output dimension 128 and 64 with ReLU activations.
For the exploitation policy Q-Values Q^sk(a | s, z), We either choose Z to be the decoder embedding
of the exploration trajectory gω (Texp) or to be an embedding of the problem ID e® (μ), where we
alWays use the exploration trajectory embedding gω(τexp) at meta-test time. We embed the state With
a learned embedding functions e(s). Then we apply a linear layer of output dimension 64 to the
concatenation of [e(s); z] with a ReLU activation. Finally, we apply two linear layer heads of output
dimension 1 and |A| respectively to form estimates of the value and advantage functions, using the
dueling Q-network parametrization. To obtain Q-values, we add the value function to the advantage
function, subtracting the mean of the advantages.
18
Under review as a conference paper at ICLR 2021
For the exploration policy Q-values Qφp(at | st, TexP), We embed the St and TexP according to the
embedding scheme above. Then, we apply two linear layer heads to obtain value and advantage
estimates as above.
E-RL2. E-RL2 learns a policy π(at | st, T:t) producing actions at given the state st and history T:t.
Like With all approaches, We parametrize this With dueling double Q-netWorks, learning Q-values
Q(st, T:t, at). We embed the current state st and history T:t using the embedding scheme described
above (With episode termination embeddings). Then, We apply tWo final linear layer heads to obtain
value and advantage estimates.
IMPORT IMPORT also learns a recurrent policy π(at | st, z), but conditions on the embedding z,
which is either an embedding of the problem μ or the history T：t. We also parametrize this policy
With dueling double Q-netWorks, learning Q-values Q(st, z, at). We embed the state st as e(st), the
problem μ as eφ(μ) and the history T：t as e&(T：t) using the previously described embedding schemes.
Then we alternate meta-training trials between choosing Z = eφ(μ) and Z = eθ(T：t). We apply a
linear layer of output dimension 64 to the concatenation [e(st); z] with ReLU activations and then
apply two linear layer heads to obtain value and advantage estimates.
Additionally, Import uses the following auxiliary loss function to encourage the history embedding
eθ(T：t) to be close to the problem embedding eφ(μ) (optimized only w.r.t., θ):
LIMPORT(θ) = βE(τ,μ) Ekeθ (T：t) - eφ(μ)k2 ,
t
where T is a trajectory from rolling out the policy on problem μ. Following Kamienny et al. (2020),
we use β = 1 in our final experiments, and found that performance changed very little when we
experimented with other values of β .
VARIBAD. VARIBAD also learns a recurrent policy π(at | Z), but over a belief state z capturing
the history T：t and current state st . We also parametrize this dueling double Q-networks, learning
Q-values Q(st, Z, at).
VARIBAD encodes the belief state with an encoder enc(Z | st, T: t). Our implementation of this
encoder embeds st and T：t using the same experience embedding approach as above, and use the
output as the mean m for a distribution. Then, we set enc(Z | st, T: t) = N (m, ν2I), where
ν2 = 0.00001. We also tried learning the variance instead of fixing it to ν2I by applying a linear
head to the output of the experience embeddings, but found no change in performance, so stuck with
the simpler fixed variance approach. Finally, after sampling Z from the encoder, we also embed the
current state st as e(st) and apply a linear layer of output dimension 64 to the concatenation [e(st); Z].
Then, we apply two linear layer heads to obtain value and advantage estimates.
VariBAD does not update the encoder via gradients through the policy. Instead, VariBAD jointly
trains the encoder with state decoder T(s0 | a, s, Z) and reward decoder TR(s0 | a, s, z), where Z is
sampled from the encoder, as follows. Both decoders embed the action a as e(a) with an embedding
matrix of output dimension 32 and embed the state s as e(s). Then we apply two linear layers with
output dimension 128 to the concatenation [e(s); e(a); Z]. Finally, we apply two linear heads, one for
the state decoder and one for the reward decoder and take the mean-squared error with the true next
state s0 and the true rewards r respectively. In the 3D visual navigation domain, we remove the state
decoder, because the state is too high-dimensional to predict. Note that Zintgraf et al. (2019) found
better results when removing the state decoder in all experiments. We also tried to remove the state
decoder in the grid world experiments, but found better performance when keeping the state decoder.
We also found that VariBAD performed better without the KL loss term, so we excluded that for
our final experiments.
B.4	Hyperparameters
In this section, we detail the hyperparameters used in our experiments. Where possible, we used the
default DQN hyperparameter values from Mnih et al. (2015). and shared the same hyperparameter
values across all methods for fairness. We optimize all methods with the Adam optimizer (?).
Table 1 summarizes the shared hyperparameters used by all methods and we detail any differences in
hyperparameters between the methods below.
19
Under review as a conference paper at ICLR 2021
Hyperparameter
Discount Factor γ
Test-time
Learning Rate
Replay buffer batch size
Target parameters syncing frequency
Update frequency
Grad norm clipping
Value
0.99
0
0.0001
32
5000 updates
4 steps
10
Table 1: Hyperparameters shared across all methods: Dream, RL2, Import, and VariBAD.
All methods use a linear decaying schedule for -greedy exploration. For RL2, IMPORT, and
VARIBAD, we decay from 1 to 0.01 over 500000 steps. For DREAM, we split the decaying between
the exploration and exploitation policies. We decay each policy’s from 1 to 0.01 over 250000 steps.
We train the recurrent policies (Dream’s exploration policy, RL2, Import, and VariBAD) with a
simplified version of the methods in Kapturowski et al. (2019) by storing a replay buffer with up to
16000 sequences of 50 consecutive timesteps. We decrease the maximum size from 16000 to 10000
for the 3D visual navigation experiments in order to fit inside a single NVIDIA GeForce RTX 2080
GPU. For Dream’s exploitation policy, the replay buffer stores up to 100K experiences (60K for 3D
visual navigation).
For DREAM, we additionally use per timestep exploration reward penalty c = 0.01, decoder and
stochastic encoder variance ρ2 = 0.1, and information bottleneck weight λ = 1. Note that this
information bottleneck weight λ could be adapted via dual gradient descent to solve the constrained
optimization problem in Appendix E, but we find that dynamically adjusting λ is not necessary for
good performance. For the MiniWorld experiments, we use c = 0.
C Analysis
C.1 Consistency
We restate the consistency result of Dream (Section 5.1) and prove it below.
Proposition 1. Assume(S, A, Rμ, Tμ〉is ergodic for all problems μ ∈ M. Let V*(μ) be the
maximum expected returns achievable by any exploitation policy with access to the problem ID μ,
i.e., with complete information. Let π%sk, ∏?p, F? and q?(z | Texp) be the optimizers ofthe DREAM
objective. Then, if the function classes D REAM optimizes over are well-specified, there exists a finite
T such that if the length of the exploration episode is at least T,
task
V ? (z; μ)J = Eμ〜p(μ) [V (μ)] ∙
Proof. Recall that n?ask and F?(z | μ) are optimized with an information bottleneck according to
Equation 2 in order to solve the constrained optimization problem in Appendix E. Note that if n?ask is
optimized over an expressive enough function class and λ approaches 0, which is necessary to solve
the constrained optimization problem associated with the information bottleneck (Appendix E), then
∏task achieves the desired expected returns conditioned on the stochastic encoding of the problem
F?(z | μ) (i.e., has complete information):
Eμ~p(μ),z~F⅛(z∣μ) [V ? (z； μR = Eμ~p(μ) [V (μ)] ,
task
where Vπ? (z; μ) is the expected returns of n?ask on problem μ given embedding z. Therefore, it
suffices to show that the distribution over z from the decoder q?(z | τexp) is equal to the distribution
from the encoder F?(z | μ) for all exploration trajectories in the support of ∏exp (Texp | μ)2, for each
problem μ. Then,
Eμ〜p(μ),τexp〜πexp,z〜q?(z|Teχp) [V ? (z; μR = Eμ~p(μ),z~Fλ(z∣μ) [V ? (z; μR
=Eμ〜p(μ) [V*(μ)]
2We slightly abuse notation to use πexp(τexp | μ) to denote the distribution of exploration trajectories Texp
from rolling out πexp on problem μ.
20
Under review as a conference paper at ICLR 2021
as desired. We show that this occurs as follows.
Given stochastic encoder F?(z | μ), exploration policy π?XP maximizes I(Texp; Z) = H(Z) - H(Z |
Texp) (Equation 3) by assumption. Since only H(Z | Texp) depends on n?xp, the exploration policy
outputs trajectories that minimize
H(Z IT P) = Eμ〜p(μ) [Eτexp〜∏exp(τexp〜μ) [Ez〜F*(z∣μ) [- logP(Z IT P)]]]
=Eμ〜p(μ) [Eτexp〜∏exp(τ对〜μ) [H(F?(z | μ),p(Z | T力用,
where P(Z | T exp) is the true conditional distribution and H (F?(Z ∣ μ),p(Z ∣ T exp)) is the cross-
entropy. The cross-entropy is minimized when P(Z ∣ Texp) = F?(z ∣ μ), which can be achieved
with long enough exploration trajectories T if(S, A, Rμ, Tμi is ergodic (by visiting each transition
sufficiently many times). Optimized over an expressive enough function class, q?(Z I Texp) equals
the true conditional distribution P(Z ∣ Texp) at the optimum of Equation 3, which equals F?(z ∣ μ) as
desired.	□
C.2 Tabular Example
We first formally detail a more general form of the simple tabular example in Section 5.2, where
episodes are horizon H rather than 1-step bandit problems. Then we prove sample complexity bounds
for RL2 and DREAM, with -greedy tabular Q-learning with = 1, i.e., uniform random exploration.
Setting. We construct this horizon H setting so that taking a sequence of H actions a? (instead of
a single action as before) in the exploration episode leads to a trajectory T?exp that reveals the problem
μ; all other action sequences a lead to a trajectory TaXP that reveals no information. Similarly, the
problem μ identifies a unique sequence of H actions a* that receives reward 1 during exploitation,
while all other action sequences receive reward 0. Again, taking the action sequence a? during
exploration is therefore necessary and sufficient to obtain optimal reward 1 during exploitation.
We formally construct this setting by considering a family of episodic MDPS (S, A, Rμ,Tμi
parametrized by the problem ID μ ∈ M, where:
•	Each exploitation and exploration episode is horizon H.
•	The action space A consists of A discrete actions {1, 2, . . . , A}.
•	The space of problems M = {1, 2,..., ∣ AIH } and the distribution p(μ) is uniform.
To reveal the problem via the optimal action sequence a? and to allow a* to uniquely receive reward,
we construct the state space and deterministic dynamics as follows.
•	States s ∈ S are (H + 2)-dimensional and the deterministic dynamics are constructed
so the first index represents the current timestep t, the middle H indices represent the
history of actions taken, and the last index reveals the problem ID if a? is taken. The
initial state is the zero vector s0 = 0 and we denote the state at the t-th timestep st as
(t,a0,a1, . . . ,at-1,0, . . . ,0,0).
•	The optimal exploration action sequence a? is set to be taking action 1 for H timesteps.
In problem μ taking action aH-ι = 1 at state SH-ι = (H — 1, ao = 1,..., aH —=
1, 0, 0) (i.e., taking the entire action sequence a?) transitions to the state sH = (H, a0 =
1,..., aH-2 = 1, aH-ι = 1, μ), revealing the problem μ.
•	The action sequence a* identified by the problem μ is set as the problem μ in base ∣A∣:
i.e., aμ is a sequence of H actions (ao, aι,..., &h-i) with PH-I at∣A∣t = μ. In prob-
lem μ with aμ = (a0, aι,..., aH-ι), taking action aH_1 at timestep H - 1 at state
SH-1 = (H - 1, ao, aι,..., aH-2,0,0) (i.e., taking the entire action sequence a*) yields
Rμ(sH-ι, aH-ι) = 1. Reward is 0 everywhere else: i.e., Rμ(s, a) = 0 for all other states
S and actions a.
• With these dynamics, the exploration trajectory Taexp = (So, ao, ro, . . . , SH) is uniquely
identified by the action sequence a and the problem μ if revealed in SH. We therefore write
TaXP = (a, μ) for when a = a? reveals the problem μ, and TaXP = (a, 0), otherwise.
21
Under review as a conference paper at ICLR 2021
Uniform random exploration. In this general setting, we analyze the number of samples required
to learn the optimal exploration policy with RL2 and DREAM via -greedy tabular Q-learning. We
formally analyze the simpler case where = 1, i.e., uniform random exploration, but empirically find
that DREAM only learns faster with smaller , and RL2 only learns slower.
In this particular tabular example with deterministic dynamics that encode the entire action history
and rewards, learning a per timestep Q-value is equivalent to learning a Q-value for the entire
ʌ
trajectory. Hence, we denote exploration Q-values Qexp (a) estimating the returns from taking the
entire sequence of H actions a at the initial state so and exeuction Q-Values QtaSk(Texp, a) estimating
the returns from taking the entire sequence of H actions a at the initial state s0 given the exploration
trajectory τexp. We drop s0 from notation, since it is fixed.
Recall that RL2 learns exploration Q-values Qexp by regressing toward the exploitation Q-values
Qtask. We estimate the exploitation Q-values Qtask(τexp, a) as the sample mean of returns from taking
actions a given the exploration trajectory τexp and estimate the exploration Q-values Qexp(a) as the
sample mean of the targets. More precisely, for action sequences a 6= a? , the resulting exploration
trajectory TaXP is deterministically (a, 0), so We set Qexp(a) = Vtask(τjexp) = maxa，Qtask(τjexp, a0).
For a?, the resulting exploration trajectory TaXP may be any of (a?, μ) for μ ∈ M, so we set Qexp(a?)
as the empirical mean of Vtask(τjexp) of observed τaxp.
Recall that DREAM learns exploration Q-values Qexp by regressing toward the learned decoder
log q(μ | τjexp). We estimate the decoder q(μ | TaXP) as the empirical counts of (μ, TaXP) divided by the
empirical counts of TaXP and similarly estimate the Q-values as the empirical mean of log q(μ | 丁：期).
We denote the exploration Q-values learned after t timesteps as Qexp, and similarly denote the
estimated decoder after t timesteps as ^t.
Given this setup, we are ready to state the formal sample complexity results. Intuitively, learning
the exploitation Q-values for RL2 is slow, because, in problem μ, it involves observing the optimal
exploration trajectory from taking actions a? and then observing the corresponding exploitation
actions a*, which only jointly happens roughly once per |A|2H samples. Since RL2 regresses the
exploration Q-values toward the exploitation Q-values, the exploration Q-values are also slow to learn.
In contrast, learning the decoder q(μ | TaXP) is much faster, as it is independent of the exploitation
actions, and in particular, already learns the correct value from a single sample of a? . We formalize
this intuition in the following proposition, which shows that Dream learns in a factor of at least
|A|H |M| fewer samples than RL2 .
Proposition 2. Let T be the number of samples from uniform random exploration such that the
greedy-exploration policy is guaranteed to be optimal(i.e., arg maxa Qexp (a) = a?)for all t ≥ T. If
Qexp is learned with DREAM, the expected value of T is O(|A|H log |A|H). If Qexp is learned with
RL2, the expected value of T is Ω(∣A∣2h|M| log |A|H).
Proof. For DREAM, QTp(a?) > QTp(a) for all a = a? if log ^t(μ | (a*,μ)) > log ^t(μ | (a, 0))
for all μ and a = a?. For all t ≥ T, QeXP is guaranteed to be optimal, since no sequence of samples
will cause log ^t(μ | (a*,μ)) = 0 ≤ log^t(μ | (a, 0)) for any a = a?. This occurs once we,ve
observed (μ, (a, 0)) for two distinct μ for each a = a? and we,ve observed (μ, (a?, μ)) for at least
one μ. We can compute an upperbound on the expected number of samples required to observe
(μ, TaXP) for two distinct μ for each action sequence a by casting this as a coupon collector problem,
where each pair (μ, TaXP) is a coupon. There are 2|A|H total coupons to collect. This yields that the
expected number of samples is O(|A|H log |A|H).
For RL2, the exploration policy is optimal for all timesteps t ≥ T for some T only if the exploitation
values VTaSk(Texp = (a?, μ)) = 1 for all μ in M. Otherwise, there is a small, but non-zero probability
that VtaSk(Texp = (a, 0)) will be greater at some t > T. For the exploitation values to be optimal
at all optimal exploration trajectories VVaSk(Texp = (a?, μ)) = 1 for all μ ∈ M, we must jointly
observe exploration trajectory Texp = (a?, μ) and corresponding action sequence a* for each problem
μ ∈ M. We can lower bound the expected number of samples required to observe this by casting
this as a coupon collector problem, where each pair (texp = (a?, μ), a*) is a coupon. There are
22
Under review as a conference paper at ICLR 2021
|M| ∙ AIH unique coupons to collect and collecting any coupon only occurs with probability ∣a∣h
in each episode. This yields that the expected number of samples is Ω(∣A∣2H ∙∣M∣∙ log |A|H). □
D	Extended Related Work
Detailed comparison with Import and Humplik et al. (2019) We compare DREAM with IM-
port (Kamienny et al., 2020) and Humplik et al. (2019) in greater detail, since they also leverage
the problem ID. The key difference between Dream and these works is that these works still learn
exploration from returns achieved during exploitation, which leads to the chicken-and-egg problem.
In contrast, Dream learns exploration by maximizing the mutual information between exploration
trajectories and learned z’s containing only task-relevant information (Section 4.2).
More specifically, like Dream, Import also conditions on the problem ID to learn exploitation with-
out the need for exploration, which can accelerate learning exploration, but unlike Dream, Import
still learns exploration from end-to-end signal, which can make learning exploration challenging as
follows. Suppose that optimal exploitation conditioned on the problem ID is already learned, and
exploration is learned from the exploitation returns. Since initially, the exploitation has not learned to
interpret the trajectories produced by the exploration policies, it may still achieve low returns given a
good exploration trajectory, which would erroneously "down weight" the good exploration trajectory
(i.e., the chicken-and-egg problem). Humplik et al. (2019) attempts to learn better representations
by predicting the problem ID as an auxiliary task. This can help learn a better policy, but does not
address the chicken-and-egg coupling problem.
E Information B ottleneck
The objective (Equation 2) for learning the exploitation policy and encoder is derived from minimizing
the mutual information between problem IDs and the encoder’s stochastic outputs, under the constraint
that the exploitation policy achieves maximal returns, i.e.:
minimize
subject to
I (z; μ)
Eμ〜p(μ),z〜F(z∣μ) [VRz； μ)] is maximal.
(5)
We form the Lagrangian with λ-1 as the dual variable, which yields Equation 2. This constrained
optimization problem is only satisfied with the Langrangian is maximized, as the dual variable tends
to infinity: i.e., λ approaches 0.
23