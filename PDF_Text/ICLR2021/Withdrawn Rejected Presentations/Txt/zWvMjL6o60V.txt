Under review as a conference paper at ICLR 2021
Improving Mutual Information based Feature
Selection by Boosting Unique Relevance
Anonymous authors
Paper under double-blind review
Ab stract
Mutual Information (MI) based feature selection makes use of MI to evaluate each
feature and eventually shortlist a relevant feature subset, in order to address issues
associated with high-dimensional datasets. Despite the effectiveness of MI in
feature selection, we have noticed that many state-of-the-art algorithms disregard
the so-called unique relevance (UR) of features, which is a necessary condition
for the optimal feature subset. In fact, in our study of seven state-of-the-art and
classical MIBFS algorithms, we find that all of them underperform as they ignore
UR of features and arrive at a suboptimal selected feature subset which contains
a non-negligible number of redundant features. We point out that the heart of
the problem is that all these MIBFS algorithms follow the criterion of Maximize
Relevance with Minimum Redundancy (MRwMR), which does not explicitly
target UR. This motivates us to augment the existing criterion with the objective
of boosting unique relevance (BUR), leading to a new criterion called MRwMR-
BUR. We conduct extensive experiments with several MIBFS algorithms with and
without incorporating UR. The results indicate that the algorithms that boost UR
consistently outperform their unboosted counterparts in terms of peak accuracy and
number of features required. Furthermore, we propose a classifier based approach
to estimate UR that further improves the performance of MRwMR-BUR based
algorithms.
1 Introduction
High-dimensional datasets tend to contain irrelevant or redundant features, leading to extra computa-
tion, larger storage, and decreased performance (Bengio et al., 2013; Gao et al., 2016; Bermingham
et al., 2015; Hoque et al., 2016). Mutual Information (MI) (Cover & Thomas, 2006) based feature
selection, which is a classifier independent filter method, addresses those issues by selecting a relevant
feature subset. We start this paper by discussing the value of MI based feature selection (MIBFS).
Interpretability: Dimensionality reduction methods consist of two classes: feature extraction and
feature selection. Feature extraction transforms original features into new features with lower
dimensionality (e.g., PCA). This method may perform well in dimensionality reduction, but the
extraction process (e.g., projection) loses the physical meaning of features (Chandrashekar & Sahin,
2014; Sun & Xu, 2014; Nguyen et al., 2014; Gao et al., 2016). In contrast, feature selection preserves
the interpretability by selecting a relevant feature subset. This helps to understand the hidden
relationship between variables and makes techniques such as MIBFS preferred in various domains
(e.g., healthcare) (Kim et al., 2015; Liu et al., 2018; Chandrashekar & Sahin, 2014).
Generalization: Feature selection methods are either classifier dependent or classifier independent
(Guyon & Elisseeff, 2003; Chandrashekar & Sahin, 2014). Examples of the former type include
the wrapper method and the embedded method (e.g., LASSO (Hastie et al., 2015)) which performs
feature selection during the training of a pre-defined classifier. The classifier dependent method
tends to provide good performance as it directly makes use of the interaction between features and
accuracy. However, the selected features are optimized for the pre-defined classifier and may not
perform well for other classifiers. The filter method, which is classifier independent, scores each
feature according to its relevance with the label. As a filter method, MIBFS quantifies relevance
using MI as MI can capture the dependencies between random variables (e.g., feature and label).
Consequently, the feature subset selected by MIBFS is not tied to the bias of the classifier and is
relatively easier to generalize (Bengio et al., 2013; L. et al., 2011; Meyer et al., 2008).
1
Under review as a conference paper at ICLR 2021
Performance: Although MIBFS is an old idea dating back to 1992 (Lewis, 1992), it still can provide
competitive performance in dimensionality reduction (see several recent survey works (Zebari & et al,
2020; Venkatesh & Anuradha, 2019)). We now provide a new perspective using the Information
Bottleneck (Tishby et al., 2000) (IB) to explain the superior performance of MIBFS and suggest why
MI is the right metric for feature selection. IB was proposed to search for the solution that achieves
the largest possible compression, while retaining the essential information about the target and in
(Shwartz-Ziv & Tishby, 2017), IB is used to explain the behavior of neural networks. Specifically,
let X be the input data to the neural network, Y be the corresponding label and X be the hidden
representation of neural networks. Shwartz-Ziv & Tishby (2017) demonstrate that the learning
process in neural networks consists of two phases: (i) empirical error minimization (ERM), where
I(X; Y ) gradually increases to capture relevant information about the label Y. (ii) representation
compression, where I(X, X) decreases and I(X; Y ) remains almost unchanged, which may be
responsible for the absence of overfitting in neural networks.
We note that the goal of MIBFS is to find the minimal feature subset with maximum MI with respect
to the label (Brown et al., 2012). Mathematically, the goal can be written as follows.
S* = arg min f (arg max I (S; Y)),	(1)
S⊆Ω
where f (A, B, •…)=(|A|, |B|, .…),|A| represents the number of features in A and Ω is the set
of all features, S ⊆ Ω is the selected feature subset and S* is the optimal feature subset. In such
a manner, MIBFS naturally converts the representation learning process of neural networks to the
process of feature selection (if We consider S as a type of hidden representation X) and attempts
to obtain an equivalent learning outcome. Specifically, maximizing the I(S; Y) corresponds to the
ERM phase and minimizing the size of S corresponds to the representation compression phase. We
believe this new perspective sheds light on the superior performance of MIBFS in dimensionality
reduction and rationalizes the use of MI for feature selection.
We note that finding the optimal feature subset S* in (1) through exhaustive search is computationally
intractable. Therefore, numerous MIBFS algorithms (Meyer et al., 2008; Yang & Moody, 2000;
Nguyen et al., 2014; Bennasar et al., 2015; Peng et al., 2005) are proposed and attempt to select the
optimal feature subset following the criterion of Maximize Relevance with Minimum Redundancy
(MRwMR) (Peng et al., 2005).
In this paper, we explore a promising feature property, called Unique Relevance (UR), which is the
key to select the optimal feature subset in (1). We note that UR has been defined for a long time and
it is also known as strong relevance (Kohavi & John, 1997). However, only very few works (Liu
et al., 2018; Liu & Motani, 2020) look into it and the use of UR for feature selection remains largely
uninvestigated. We fill in this gap and improve the performance of MIBFS by exploring the utility of
UR. We describe the flow of the remaining paper together with several contributions as follows.
1.	We shortlist seven state-of-the-art (SOTA) and classical MIBFS algorithms and uncover the fact
that all of them ignore UR and end up underperforming, namely they select a non-negligible
number of redundant features, contradicting the objective of minimal feature subset in (1). In fact,
it turns out that the minimal feature subset in (1) must contain all features with UR.
2.	We point out that, the heart of the problem is that existing MIBFS algorithms following the
criterion of MRwMR (Peng et al., 2005), which lacks a mechanism to explicitly identify the UR
of features. This motivates us to augment MRwMR and include the objective of boosting UR,
leading to a new criterion for MIBFS, called MRwMR-BUR.
3.	We estimate UR using the KSG estimator (Kraskov et al., 2004) and conduct experiments with
five representative MIBFS algorithms on six datasets. The results indicate that the algorithms that
boost UR consistently outperform their unboosted counterparts when tested with three classifiers.
4.	We improve MRwMR-BUR by proposing a classifier based approach to estimate UR and our
experimental results indicate that this approach further improves the classification performance of
MRwMR-BUR based algorithms.
2	Background and Definitions
We now formally define the notation used in this paper. We denote the set of all features by
Ω = {Xk, k = 1,…，M}, where M is the number of features. The feature Xk ∈ Ω and the label Y
2
Under review as a conference paper at ICLR 2021
因 Unique Relevance (UR) □ Irrelevant Information (II)
Figure 1: Entropy of feature A and B from the perspective of OR, UR and II. Feature A represents a
feature with UR and OR with respect to the label, Feature B represents a feature with no UR and
some OR with respect to the label.
are both vectors of length N, where N is the number of samples. Let S ⊆ Ω be the set of selected
features and S ⊆ Ω be the set of unselected features, i.e., Ω = {S, S}.
2.1	Information Content: OR, UR, II
The information content presents in a feature can be divided into three parts: Overlapped Relevance,
Unique Relevance and Irrelevant Information (see Feature A in Fig. 1).
Relevant information is quantified using MI with the label. The Unique Relevance (UR) of a feature
Xk is defined as the unique relevant information which is not shared by any other features in Ω.
Mathematically, UR can be calculated as the MI loss when removing that feature from Ω. By the
chain rule for MI (Cover & Thomas, 2006), UR can be written as
UR = I(Ω; Y) - I(Ω∖Xk; Y) = I(Xk; Y∣Ω∖Xk).	(2)
We note that UR is equivalent to strong relevance defined in (Kohavi & John, 1997; Brown et al.,
2012). The Overlapped Relevance (OR) of a feature Xk is the relevant information content of a
feature Xk which is shared (or overlapped) with other features in Ω. By the chain rule for ML OR
can be written as
OR = I(Xk; Y)-(I(Ω; Y) - I(Ω∖(Xk); Y)).	(3)
The definition of OR is equivalent to weak relevance defined in (Kohavi & John, 1997; Brown et al.,
2012). Consider an example of Ω containing two features Xj and Xk, then the OR of feature Xk is
I(Xj; Xk; Y), which is known as multivariate mutual information. A positive value of I(Xj; Xk; Y)
is a sign of redundancy, while a negative value expresses synergy (McGill, 1954).
Irrelevant information (II) can be understood as the noise in the signal. Overfitting to the irrelevant
aspects of the data will confuse the classifier, leading to decreased accuracy (John et al., 1994; Song
et al., 2011). Mathematically, we define II of feature Xk as
II=H(Xk) -I(Xk;Y) =H(Xk|Y).	(4)
We note that a feature Xk can be completely irrelevant with respect to the label Y if I(Xk; Y) = 0.
There is another popular type of decomposition called partial information decomposition (PID)
(Williams & Beer, 2010) which decomposes the total mutual information of a system into three
parts: unique information, redundant information, synergistic information and a follow-up work
(Bertschinger & et al, 2014) attempts to quantify each term based on ideas from decision theory. The
definition of UR is the same as the unique information in (Williams & Beer, 2010), but calculated
differently from (Bertschinger & et al, 2014). Furthermore, OR is equal to the difference of shared
information and synergistic information in (Bertschinger & et al, 2014).
2.2	Estimation of Mutual Information
In this paper, we estimate MI using the KSG estimator (Kraskov et al., 2004) which uses the K
nearest neighbors of points in the dataset to detect structure in the underlying probability distribution.
3
Under review as a conference paper at ICLR 2021
	Gas sensor	Colon	Sonar	Madelon	Leukemia	Isolet
Features	128	2000	60	500	7070	617
Instances	13874	62	208	2600	72	1560
Classes	6	2	2	2	2	26
Data Type	Cont.	Disc.	Cont.	Cont.	Disc.	Cont.
UR (%)	2.34%	4.3%	28.3%	29.6%	34.9%	37.1%
Table 1: Information of experimental datasets showing the fraction of features with UR.
In a recent work (Gao et al., 2018), the KSG estimator is proven to be consistent under some mild
assumptions. We note that the KSG estimator is not applicable when the random variable being
studied is a mixture of continuous and discrete values. For the case with mixed random variables, we
can apply the mixed KSG estimator (Gao & et al, 2017), which demonstrates good performance at
handling mixed variables. We note that the features of all datasets studied in this paper are either
purely discrete (real-valued) or continuous while all labels are purely discrete (real-valued) (see Table
1). Hence, we use the KSG estimator (Kraskov et al., 2004) to compute MI quantities.
3	A New Criterion for MI based Feature Selection
In this section, we first show the crucial role of UR in selecting the optimal feature subset in (1). Next,
we conduct experiments to uncover the fact that all studied MIBFS algorithms are underperforming
and hence, motivating MRwMR-BUR as a new criterion for MIBFS.
3.1	A Necessary Condition for Optimality
Recall the goal of MIBFS in (1) is to find the minimal feature subset with maximum MI with respect
to the label (Brown et al., 2012). Several works (Yu & Liu, 2004; John et al., 1994) have noticed that
UR is a necessary condition for the optimal solution in (1). This can be simply proved as follows.
Proposition 1. The optimal feature subset S* in (1), which has maximum I (S; Y) with minimum
|S |, must contain all features with UR.
Proof. Assume there exists a feature Xk ∈ Ω with non-zero UR. Suppose We have a feature subset
S ⊆ Ω∖Xk which has maximum I(S; Y) with minimum |S|. Since Xk has non-zero UR, we have
I (Ω; Y) > I (Ω∖Xk; Y) by definition. Therefore, I (Ω; Y) > I (S; Y) as I (Ω∖Xk; Y) > I (S; Y)
given S ⊆ Ω∖Xk. But this contradicts the initial assumption that I (S; Y) is maximum.	□
We note that the optimal feature subset S* may also contain features with OR and no UR at certain
situations. For example, consider a feature subset T which contains all features with UR, it is possible
to have another feature Xm 6∈ T which contributes to higher joint MI (i.e., I(Xm, T; Y) > I(T; Y)).
3.2	Motivating the MRwMR-BUR Criterion
In this subsection, we shortlist seven SOTA and classical MIBFS algorithms: Mutual Information
Maximization (MIM) (Lewis, 1992), Joint Mutual Information (JMI) (Yang & Moody, 2000), Joint
Mutual Information Maximization (JMIM) (Bennasar et al., 2015), minimal Redundancy Maximum
Relevance (mRMR) (Peng et al., 2005), greedy search algorithm (GSA) (Brown et al., 2012). Mutual
Information Convex Optimization (MICO) (Sun & et al, 2019), Conditional Mutual Information
based Feature Selection considering Interaction (CMIFSI) (Tang et al., 2019) We simulate their
feature selection process using Sonar dataset (Dua & Graff, 2017) and evaluate the variation of joint
MI I(S; Y) as more features are selected.
In Fig. 2, we classify features as three types: (i) features with nonzero UR, (ii) features with zero UR,
and (iii) redundant features, namely that all redundant features (e.g.,Xredun) can be removed after
saturation of joint MI without decreasing the joint MI (i.e., I (Ssatur; Y) = I(Ssatur\Xredun; Y)).
In Fig. 2 (left), we provide an example of the GSA feature selection process and summarize the
performance of all algorithms in Fig. 2 (right), where redundancy rate represents the percentage of
redundant features in the selected feature subset when the joint MI saturation is firstly reached.
In Fig. 2 (left), we observe that GSA is able to perform well at the beginning and select features
with UR. However, as more features are selected, the joint distribution of the selected feature subset
4
Under review as a conference paper at ICLR 2021
0.254
P
0.252
W
0.25
一
0.248
0	10	20	30	40
Number of Features in Selected Feature Subset S
• Feature with UR
▲ Feature without UR
■ Redundant Feature
algorithm	redundancy rate
MIM	36.9%
JMI	21.6%
JMIM	25.6%
mRMR	27.5%
GSA	14.7%
CMIFSI	19.4%
MICO	24.3%
Figure 2: (left) Feature selection with GSA. (right) Redundancy rates for various algorithms.
becomes more complex, and then GSA tends to select some redundant features. Similar performance
can be observed for other algorithms (see Fig. 2 (right)). We note that, redundant features contribute
nothing, but increase the size of the selected feature subset, undermining the objective of minimal
feature subset requirement in (1). Surprisingly, all of the SOTA and classical algorithms studied are
selecting a non-negligible number of redundant features, which uncovers the fact that all of them are
underperforming. Lastly, we note that similar performance can be observed on another dataset (Gas
Sensor dataset (Alexander et al., 2012)) as well (see A.1 in Appendix).
The heart of the problem is that features with UR are not prioritized during the selection process as all
these algorithms follow the MRwMR criterion which lacks a mechanism to identify UR. Furthermore,
based on our analysis of various datasets (see Table 1), features with UR usually make up a very
small fraction of the total number of features and hence, is difficult to select them without explicitly
targeting them. This motivates us to augment MRwMR and include the objective of boosting unique
relevance (BUR), leading to a new criterion, called MRwMR-BUR. In Section 6, we discuss results
showing that the new criterion can significantly reduce the redundancy rate.
4	Performance Evaluation
In this section, we shortlist five representative MIBFS algorithms and modify them to include the
objective of BUR. We conduct experiments to compare the performance of each algorithm before
and after boosting UR, so as to evaluate the proposed MRwMR-BUR criterion.
4.1	Experiment Setup
To support the proposed MRwMR-BUR criterion, we conduct experiments using six public datasets
(Peng et al., 2005; Dua & Graff, 2017; Little et al., 2007; Alexander et al., 2012) described in Table 1
and compare the performance of MRwMR-BUR to MRwMR via Support Vector Machine (SVM)
(Cortes & Vapnik, 1995), K-Nearest Neighbors (KNN) (Larose & Larose, 2014) and Random Forest
(RF) (Breiman, 2001). We shortlist five representative MRwMR based algorithms: MIM (Lewis,
1992), JMI (Yang & Moody, 2000), JMIM (Bennasar et al., 2015), mRMR (Peng et al., 2005) and
GSA(Brown et al., 2012). Those algorithms are augmented to include the objective of BUR as
Jnew (Xi) = (1 - β) × Jold (Xi) + β × JUR(Xi),	(5)
where Jold is the MRwMR based algorithm (e.g., MIM) and Jnew is corresponding MRwMR-BUR
form and JUR(Xi) returns the UR of feature Xi. We note that this modification is slightly different
for JMI as JMI is not bounded and the score increases as more features are selected. Therefore, we
divide the original form of JMI by the size of the selected feature subset and include BUR as shown
in (5). Moreover, the details for each algorithm with and without BUR are provided in Appendix A.2.
We will denote the algorithm that extends XYZ as XYZ-BUR (e.g., MIM and MIM-BUR).
For each run, the dataset is randomly split into two subsets: training dataset (75%) and testing dataset
(25%). We apply all MRwMR and MRwMR-BUR (with β = 0.1) based algorithms on the same
training dataset to select features and evaluate them using the same testing dataset. To ensure fair
comparison, the parameters of all classifiers are tuned via grid search and we note that all methods
share the same grid search range and step size. Some key parameters are tuned as follows. (i) the
number of neighbors K for KNN is tuned from 3 to 30 with step size of 2. (ii) the regularization
coefficient c for SVM is chosen from {0.01, 0.1, 1, 10}. (iii) the number of trees in the RF is chosen
from {10,15, ..., 50}. Lastly, the source code will be released at the camera ready stage.
5
Under review as a conference paper at ICLR 2021
4.2	Performance Comparison
The results averaged over 20 runs via RF are shown in Fig. 3. For each algorithm, we report two
results: (i) the peak averaged accuracy and its spread over 20 runs using boxplot. (ii) the number
of features required to obtain the peak accuracy. Moreover, to better summarize the performance
difference between the MRwMR based algorithm and its MRwMR-BUR form, we report two sets
of results for each dataset: the peak averaged accuracy improvement and the feature improvement.
Specifically, as shown below the subplot of Fig. 3, from left to right, it is the minimum, maximum
and averaged improvement over five comparisons between the MRwMR based algorithm and its
MRWMR-BUR form (e.g., compare MIM to MIM-BUR, JMI to JMLBUR and so on). A negative
value of the improvement indicates that the peak averaged accuracy of the MRwMR-BUR based
algorithm is loWer than its original form or the number of features required for the MRWMR-BUR
based algorithm is more than its original form.
In terms of MRWMR based algorithms, We observe that MIM tends to provide the Worst performance
in terms of peak averaged accuracy and number of features required (see Figure 3 (a), (b), (c), (f)).
We suspect it is because that MIM assumes features are independent from each other, leading to
degraded performance. For other MRWMR based algorithms, JMI and GSA generally perform better
than other MRWMR based algorithms (see Figure 3 (a), (d), (d), (f)). This finding agrees With (BroWn
et al., 2012; Liu et al., 2018) as GSA is greedy in nature and JMI can increase the complementary
information betWeen features.
Comparing the performance of MRWMR to MRWMR-BUR, We observe that the performance of
most MRWMR based algorithms is improved after including the BUR objective (e.g., compare MIM
to MIM_BUR). For example, the maximum and average accuracy improvement for the Leukemia
dataset is approximately 0.014 and 0.005, respectively. Moreover, feWer number of features (i.e., 9.8
features on average) are required. Similar improvements can be observed for the other datasets as
Well. We note that similar trends can be found using SVM and KNN as Well (see A.3 in Appendix).
5	Estimating UR via a Classifier based Approach
In this section, We first introduce an alternative classifier based approach to estimate UR in Section
5.1. Next, We compare it to performance of estimating UR using the KSG estimator in Section 5.2.
5.1	The Classifier based Approach to Estimate UR
The UR of feature Xk in (2) can be equivalently expressed as
I(Xk； Y∣Ω∖Xk) = H(Y∣Ω∖Xk) - H(Y∣Ω).
(6)
We note that the term H (Y ∣Ω) in (6) is constant for every candidate feature during the feature
selection process. Therefore, boosting UR is equivalent to boosting H(Y∣Ω∖Xk), which is defined as
H (Y ∣Ω∖Xk) = -E{log P (Y ∣Ω∖Xk)}.
(7)
The RHS of (7) is a function of the likelihood P (Y ∣Ω∖Xk), which we can estimate using a classifier
Q with parameter θ. Hence, the estimated UR of feature Xk can be written as
1N
UR ≡ H (Y ∣Ω∖Xk) ≈ NN fQ(yi * * * * &∣(Ω∖xk )i,θ),
N i=1
(8)
where N is the number of training samples. The reason for estimating UR via a classifier is two-fold:
(i) We understand that the estimation of high-dimensional MI is still in the development stage. This
approach provides an alternate way to estimate UR and the estimated UR will approach the true
value as the number of samples N grows given the classifier Q is a consistent estimator (Lehmann
& Casella, 2006). (ii) We note that different classifiers may favour the UR of features differently
due to their working mechanisms and assumptions. For example, the improvement of MRwMR-
BUR over MRwMR is marginal using SVM with linear kernel (see A.3 in Appendix). We suspect
that this is because MI quantifies a non-linear relationship between random variables and this non-
linear relationship may not be of much help to the linear classifier. Estimating UR via a classifier
attempts to adapt the value of UR to different classifiers. Furthermore, we note that estimating UR in
such a manner changes MRwMR-BUR based algorithms to a classifier dependent method, but we
highlight that the selected feature subset still preserves the interpretability and is preferred in some
performance-oriented tasks if the classifier based approach can further improve the performance.
6
Under review as a conference paper at ICLR 2021
	Gas sensor	Colon	Sonar	Madelon	Leukemia	Isolet
MIM-BUR	99.45% (88)	85.21%(23)	80.23%(41)	72.75%(77)	96.64%(42)	86.34%(139)
MIM_BUR_CLF	99.32%(68)	86.41%(19)	80.57% (47)	74.32% (57)	98.64% (38)	87.69%(131)
mRMR_BUR	99.48%(71)	87.09%(7)	81.29%(40)	72.57%(77)	96.63%(63)	86.13%(131)
mRMR_BUR_CLF	99.50% (69)	88.82%(31)	81.53%(37)	73.19%(57)	99.13%(53)	88.50%(135)
JMI-BUR	99.44%(75)	74.93%(39)	81.25%(48)	71.93%(63)	97.24%(75)	87.94%(143)
JMI_BUR_CLF	99.48% (72)	75.76% (36)	81.79% (40)	72.47% (57)	99.33% (64)	87.58%(127)
JMIM_BUR	99.48% (80)	75.92%(19)	80.72%(34)	73.32% (67)	95.88%(61)	85.44%(123)
JMIM_BUR_CLF	99.45%(79)	76.31% (16)	81.13%(46)	73.19%(57)	99.42% (72)	85.72%(127)
GSA-BUR	99.47%(87)	86.49%(13)	81.94%(26)	72.93%(59)	95.74%(58)	87.25%(121)
GSA_BUR_CLF	99.50% (83)	88.25%(21)	81.73%(49)	74.58% (57)	99.21%(65)	88.97%(106)
Table 2: Peak averaged accuracy and corresponding number of features required for MRwMR-BUR
based algorithms (with β = 0.1) using Random Forest. The algorithm with an extension of CLF
indicates that the algorithm uses the classifier based approach to estimate UR.
5.2	Performance Comparison
We apply this new approach to estimate UR and re-implement the MRwMR-BUR based algorithms
in Section 4. We highlight that all estimated URs are first normalized from 0 to 1 using min-max
normalization. The peak averaged accuracy and corresponding number of features required are shown
in Table 2 with β = 0.1. We note that the UR is estimated using the classifier being tested. In Table
2, the KSG estimator is used to estimate UR except for the CLF named algorithms, which use the
classifier based approach. We highlight the best accuracy using shading. In general, we find that the
classifier based approach outperforms the KSG estimator in terms of peak averaged accuracy and
number of features required in most datasets and algorithms studied. For example, the accuracy of
the classifier based approach for mRMR-BUR on the Leukemia dataset is increased by 0.025 while
10 fewer features are required. Furthermore, similar performance trends can be observed using KNN
and SVM as well (see A.3 in Appendix).
6	Reflections
In this section, we present some reflections and suggestions for future work.
Potentially Better MIBFS Algorithms: We highlight that, in this paper, we are not proposing a
new MIBFS algorithm. Instead, we explore a new criterion for the design of MIBFS algorithms
that incorporate UR into the objective. Our experimental results demonstrate that the proposed
MRwMR-BUR criterion has superior performance over the existing MRwMR criterion. We believe
this new insight can inspire better MIBFS algorithms without increasing algorithm complexity. In
fact, the UR only needs to be calculated once and can be reused in subsequent calculations. Hence,
the complexity of the UR augmented algorithms is comparable to the original algorithms.
Redundancy Rates: By not prioritizing UR, existing MIBFS algorithms choose a high fraction of
redundant features and end up underperforming. Our experiments in Section 3.2 demonstrate that
the redundancy rate of GSA is 14.7%. By incorporating UR, the redundancy rate of GSA-BUR is
significantly reduced to 9.09% with β = 0.05 and 6.25% with β = 0.1.
Always Select Features with UR First?: The results in Section 3.1 show that the minimal feature
subset S* in (1) must include all features with UR. However, it may not be a good idea to include
only features with UR at first when the selected feature size is constrained. This is because features
with UR (e.g., Xk) may not contribute to higher relevance in the short term given S ⊆ Ω∖Xk (e.g.,
I(Xk; Y |S) = 0) as removing conditional features may decrease the MI.
Optimal Value of β : The performance of MRwMR-BUR based algorithms relies on the value of β,
which balances the original objective of maximizing relevance with prioritizing UR. From extensive
experiments, we have found that using β = 0.1 to balance UR and relevance. Alternatively, β can be
thought of as a hyper-parameter and tuned via a validation dataset. The theoretical determination of
the optimal value β is clearly worth deep thought. This could help to further minimize redundancy by
prioritizing more important features.
7
Under review as a conference paper at ICLR 2021
I——MIM ——MIM BUR	mRMR	mRMR BUR	JMI	JMLBUR	JMIM	JMIM-BUR	GSA	GSA-BUR
0.90
0.89
0.88
0.87
0.86
0.85
0.84
136 138 139	143 145 147
(a) Isolet Dataset
Accuracy/Feature Improvement: {0.1%, 0.9%, 0.38%}/{-7, 15, 3.6}
119 121 123	127 129 131
(b) Leukemia Dataset
Accuracy/Feature Improvement: {-0.05%, 1.4%, 0.5%}/{-13, 44, 9.8}
(c) Madelon
Accuracy/Feature Improvement: {-0.6%, 1.1%, 0.22%}/{-2, 11, 3.8}
(d) Gas Sensor Dataset
Accuracy/Feature Improvement: {-0.02%, 0.15%, 0.04%}/{-3, 24, 7}
(e) Colon Dataset
Accuracy/Feature Improvement: {1.3%, 2.5%, 1.9%}/{-10, 27, 7.6}
(f) Sonar Dataset
Accuracy/Feature Improvement: {0.5%, 1.7%, 0.8%}/{-8, 15, 4.4}
Figure 3: Performance Comparison between MRwMR based algorithms and corresponding
MRwMR-BUR forms on six public datasets. The vertical and horizontal axes are classification
accuracy and the corresponding number of features required, respectively. The results shown are
averages over 20 trials via Random Forest with β = 0.1. The box corresponds to the lower and upper
quartile. The horizontal line inside the box depicts the peak averaged accuracy and the whiskers
depict the minimum and maximum over the trials. In each subplot, we report two sets of results: peak
averaged accuracy improvement and feature improvement. Specifically, from left to right, it is the
minimum, maximum and averaged improvement over five comparisons between the MRwMR based
algorithm and its MRwMR-BUR form (e.g., compare MIM to MIM_BUR, JMI to JMLBUR and so
on). A negative value of improvement indicates that the peak averaged accuracy of the
MRwMR-BUR algorithm is lower than its original form or the number of features required for the
MRwMR-BUR algorithm is more than its original form. Similar Performance can be observed for
SVM and KNN as well (see A.3 in Appendix).
8
Under review as a conference paper at ICLR 2021
References
Vergara Alexander et al. Chemical gas sensor drift compensation using classifier ensembles, Sensors
and Actuators B. Chemical, 2012.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Mohamed Bennasar, Yulia Hicks, and Rossitza Setchi. Feature selection using joint mutual informa-
tion maximisation. Expert Systems with Applications, 42(22):8520-8532, 2015.
Mairead L. Bermingham et al. Application of high-dimensional feature selection: evaluation for
genomic prediction in man. Scientific reports, 5:103-112, 2015.
Nils Bertschinger and et al. Quantifying unique information. Entropy, 16(4):2161-2183, 2014.
Leo Breiman. Random forests. Machine learning, 45(1):5-32, 2001.
Gavin Brown et al. Conditional likelihood maximisation: a unifying framework for information
theoretic feature selection. Journal of machine learning research, 13(1):27-66, 2012.
Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods. Computers &
Electrical Engineering, 40(1):16-28, 2014.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273-297,
1995.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory, 2nd edition. John Wiley &
Sons, 2006.
Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.
ics.uci.edu/ml.
Shuyang Gao, Greg Ver Steeg, and Aram Galstyan. Variational information maximization for feature
selection. In Proceedings of the 30th Advances in Neural Information Processing Systems, pp.
487-495, 2016.
Weihao Gao and et al. Estimating mutual information for discrete-continuous mixtures. In Proceed-
ings of Advances in neural information processing systems, pp. 5986-5997, 2017.
Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying fixed k-nearest neighbor informa-
tion estimators. IEEE Transactions on Information Theory, 64(8):5629-5661, 2018.
Isabelle Guyon and Andre Elisseeff. An introduction to variable and feature selection. Journal of
machine learning research, 3(Mar):1157-1182, 2003.
Trevor Hastie, Robert Tibshirani, and Martin Wainwright. Statistical Learning with Sparsity: The
Lasso and Generalizations. Chapman & Hall/CRC, 2015.
N. Hoque et al. A fuzzy mutual information-based feature selection method for classification. Fuzzy
Information and Engineering, 8(3):355-384, 2016.
George H. John, Ron Kohavi, and Karl Pfleger. Irrelevant features and the subset selection problem.
In Proceedings of the 8th International Conference on Machine Learning, pp. 121-129, 1994.
Been Kim, Julie A. Shah, and Finale Doshi-Velez. Mind the gap: A generative approach to in-
terpretable feature selection and extraction. In Proceedings of Advances in Neural Information
Processing Systems, pp. 2260-2268, 2015.
Ron Kohavi and George H. John. Wrappers for feature subset selection. Artificial intelligence, 97
(1-2):273-324, 1997.
Alexander Kraskov, Harald Stogbauer, and Peter Grassberger. Estimating mutual information.
Physical Review E, 69(6):066-138, 2004.
9
Under review as a conference paper at ICLR 2021
Ladha L. et al. Feature selection methods and algorithms. International journal on computer science
and engineering, 1(3):1787 - 1797, 2011.
Daniel T. Larose and Chantal D. Larose. Discovering knowledge in data: an introduction to data
mining. John Wiley & Sons, 2014.
Erich L. Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006.
David D. Lewis. Feature selection and feature extraction for text categorization. In Proceedings of
the workshop on Speech and Natural Language, pp. 212-217, 1992.
Max A. Little et al. Exploiting nonlinear recurrence and fractal scaling properties for voice disorder
detection. Biomedical engineering online, 6(1):23, 2007.
Shiyu Liu and Mehul Motani. Exploring unique relevance for mutual information based feature
selection. In 2020 IEEE International Symposium on Information Theory (ISIT), pp. 2747-2752,
2020.
Shiyu Liu, Jia Yao, Chongyu Zhou, and Mehul Motani. SURI: Feature selection based on unique
relevant information for health data. In Proceedings of IEEE International Conference on Bioinfor-
matics and Biomedicine, pp. 687-692, 2018.
W. McGill. Multivariate information transmission. Transactions of the IRE Professional Group on
Information Theory, 4(4):93-111, 1954.
Patrick Emmanuel Meyer, Colas Schretter, and Gianluca Bontempi. Information-theoretic feature
selection in microarray data using variable complementarity. IEEE Journal of Selected Topics in
Signal Processing, 2(3):261-274, 2008.
Xuan Vinh Nguyen et al. Effective global approaches for mutual information based feature selection.
In Proceedings of the 20th international conference on Knowledge discovery and data mining, pp.
512-521, 2014.
Hanchuan Peng, Fuhui Long, and Chris Ding. Feature selection based on mutual information: criteria
of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis
& Machine Intelligence, 27(8):1226-1238, 2005.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.
arXiv preprint arXiv:1703.00810, 2017.
Qinbao Song, Jingjie Ni, and Guangtao Wang. A fast clustering-based feature subset selection
algorithm for high-dimensional data. IEEE transactions on knowledge and data engineering, 25
(1):1-14, 2011.
Lin Sun and Jiucheng Xu. Feature selection using mutual information based uncertainty measures
for tumor classification. Bio-medical materials and engineering, 24(1):763-770, 2014.
Zhenqiang Sun and et al. Mutual information based multi-label feature selection via constrained
convex optimization. Neurocomputing, 329:447-456, 2019.
Xiaochuan Tang, Yuanshun Dai, and Yanping Xiang. Feature selection based on feature interactions
with application to text categorization. Expert Systems with Applications, 120:207-216, 2019.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
B. Venkatesh and J. Anuradha. A review of feature selection and its methods. Cybernetics and
Information Technologies, 19(1):3-26, 2019.
Paul L. Williams and Randall D. Beer. Nonnegative decomposition of multivariate information. arXiv
preprint arXiv:1004.2515, 2010.
10
Under review as a conference paper at ICLR 2021
Howard Hua Yang and John Moody. Data visualization and feature selection: New algorithms for
nongaussian data. In Proceedings of Advances in Neural Information Processing Systems, pp.
687-693, 2000.
Lei Yu and Huan Liu. Efficient feature selection via analysis of relevance and redundancy. Journal of
machine learning research, 5(Oct):1205-1224, 2004.
Rizgar Zebari and et al. A comprehensive review of dimensionality reduction techniques for feature
selection and feature extraction. Journal of Applied Science and Technology Trends, 1(2):56-70,
2020.
11
Under review as a conference paper at ICLR 2021
A Appendix
In A.1, we provide more results of the redundancy experiment conducted in Section 3.2. Next, we
show the scoring function of studied MIBFS algorithms before and after including BUR in A.2.
Lastly, we show the performance comparison between MRwMR and MRwMR-BUR, as well as the
effect of the classifier based approach using SVM and KNN in A.3.
A. 1 Redundancy Experiment on the Gas Sensor Dataset
In Section 3.2, we conduct experiments on the Sonar dataset to demonstrate that all studied MIBFS
algorithms are selecting redundant features, undermining the objective of minimal feature subset in
(1). In this subsection, we conduct the same experiment using the Gas Sensor dataset and summarize
the performance in Table 3.
algorithm	redundancy rate
-MIM JMI JMIM mRMR GSA CMIFSI MICO	23.6% 22.5% 24.44% 18.6% 15.4% 27.7% 20.5%
Table 3: Redundancy Experiment Results on the Gas Sensor Dataset
A.2 Scoring Functions of the Five MRwMR Algorithms with/without BUR
In this subsection, we follow the same notation as in Section 2. Namely, Xi, Y , S, and S are the
feature, the label, the set of selected features, and the set of unselected features, respectively. JUR(Xi)
denotes the unique relevance of feature Xi (i.e., JUR(Xi) = I(Ω; Y) - I(Ω∖(Xi); Y)). The five
representative MRwMR algorithms are: MIM Lewis (1992), JMI Yang & Moody (2000), JMIM
Bennasar et al. (2015), mRMR Peng et al. (2005) and GSABrown et al. (2012). The scoring functions
for these algorithms with and without BUR are shown below.
JMIM (Xi ) =	arg max I(Xi ; Y) ~ Xi ∈s	(9)
JMIM-BUR(Xi)	= arg max (1 - β) × I(Xi; Y) + β × JUR(Xi) ~ Xi∈S	(10)
JJMI(Xi) = arg max	I(Xi,Xj; Y) Xi∈S	Xj ∈S		(11)
JJMI_BUR(Xi):	=argmax (1 - β) × X I(Xi,Xj; Y) × 西+ β × JUR(Xi) Xi∈S	Xj ∈s	|S|	(12)
JmRMR (Xi ) =	二 argmax I (Xi； Y) - ɪ X I (Xi,Xj) Xi∈S	IS | Xj ∈s	(13)
JmRMR卫UR(Xi) = arg max (1 - β) ×(I(Xi； Y)-面 X I(Xi,Xj)) + β X JUR(Xi) Xi∈S	IS | Xj ∈s		(14)
JJMIM (Xi ) =	arg max min I(Xi , Xj ； Y) Xi∈S	Xj ∈s	(15)
jJMIM.BUR ( Xi )	= arg max (1 - β) × ( minI(Xi,Xj；Y)) + β × JUR(Xi) Xi ∈S	Xj ∈s	(16)
JGSA (Xi ) =	arg max I(Xi,S ； Y) 〜 Xi∈S	(17)
JGSA_BUR(Xi)	= arg max (1 - β) × I(Xi,S ； Y) + β × JUR(Xi) 一	(18)
Xi∈S
12
Under review as a conference paper at ICLR 2021
A.3 More Experimental Results using SVM and KNN
In the main paper, we only provide experimental results using random forest. We note that similar
performance trends can be observed using SVM and KNN as well (see SVM in Table 4 and KNN in
Table 5).
	Gas sensor	Colon	Sonar	Madelon	Leukemia	Isolet
MIM	96.76%(93)	81.13%(67)	73.02%(37)	61.52%(36)	96.65%(99)	88.22%(148)
MIM-BUR	96.76%(93)	82.23%(63)	73.75%(39)	61.83%(35)	97.72%(97)	88.73%(121)
MIM_BUR_CLF	97.23% (89)	82.58% (65)	74.28% (48)	65.21% (42)	98.23%(81)	89.48%(109)
mRMR	96.76%(99)	83.11%(40)	75.27%(49)	62.07%(39)	97.72%(77)	89.33%(135)
mRMR_BUR	96.84%(95)	82.97%(42)	75.49% (43)	62.07%(39)	97.72%(77)	89.62%(131)
mRMR_BUR_CLF	96.75%(89)	84.31% (37)	75.23%(48)	62.21% (47)	98.63%(61)	89.45%(127)
JMI	96.84%(89)	73.32%(17)	74.13%(41)	61.25%(31)	96.78%(86)	89.91%(134)
JMI_BUR	96.84%(89)	73.98%(15)	74.25%(37)	62.09% (35)	96.82%(86)	89.47%(144)
JMI_BUR_CLF	97.23%(93)	73.62%(19)	75.32% (49)	61.88%(37)	98.15%(75)	90.08%(119)
JMIM	96.70%(98)	76.85%(19)	73.35%(49)	62.25%(41)	96.82%(93)	88.58%(138)
JMIM_BUR	96.70%(98)	77.58%(23)	73.37%(38)	62.31%(44)	97.28%(94)	88.62%(144)
JMIM_BUR_CLF	97.35%(95)	79.62% (27)	73.62% (37)	62.47% (43)	98.42%(71)	89.21%(133)
GSA	96.48%(95)	73.77%(35)	73.82%(36)	63.51%(39)	96.82%(67)	89.37%(138)
GSA-BUR	96.48%(95)	74.28%(27)	74.13% (31)	63.27%(33)	97.14%(54)	89.75%(131)
GSA_BUR_CLF	96.86% (96)	76.33%(32)	73.95%(46)	63.72% (37)	98.62% (68)	90.02%(137)
Table 4: Peak averaged accuracy and corresponding number of features required for each MRwMR
and MRwMR-BUR based algorithm (β = 0.1) using SVM. For MRwMR-BUR based algorithms, the
algorithm with an extension of CLF indicates that the algorithm uses the classifier based method to
estimate UR. Otherwise, the algorithm estimates UR using the KSG estimator.
	Gas sensor	Colon	Sonar	Madelon	Leukemia	Isolet
MIM	99.10%(87)	84.12%(63)	82.95%(24)	73.38%(57)	95.51%(46)	78.48%(139)
MIM-BUR	99.15%(85)	85.32%(41)	83.44%(23)	74.76%(51)	96.13%(40)	78.85%(135)
MIM_BUR_CLF	99.08%(80)	86.42% (45)	84.82% (45)	77.21% (67)	97.82% (29)	80.02%(131)
mRMR	99.06%(83)	82.57%(45)	85.02%(30)	75.53%(61)	97.72%(54)	81.02%(129)
mRMR.BUR	99.08%(81)	85.02%(32)	85.81% (31)	76.21%(67)	97.70%(53)	82.13%(117)
mRMR_BUR_CLF	99.21% (81)	86.52% (35)	84.97%(30)	76.58% (69)	98.61% (48)	82.75%(121)
JMI	99.16%(72)	70.02%(47)	83.42%(48)	79.15%(50)	95.58%(46)	79.75%(135)
JMI_BUR	99.14%(65)	70.58%(43)	85.02% (45)	80.02% (53)	96.52%(45)	79.48%(137)
JMI_BUR_CLF	99.32% (80)	71.35%(41)	84.39%(48)	79.72%(59)	99.13%(39)	81.05%(125)
JMIM	99.12%(85)	72.17%(85)	84.17%(49)	77.23%(60)	95.81%(34)	79.16%(137)
JMIM_BUR	99.09%(80)	76.22%(63)	83.82%(47)	78.36%(63)	95.83%(34)	79.48%(139)
JMIM_BUR_CLF	98.97%(81)	77.58% (53)	84.23%(41)	79.51% (53)	99.47% (29)	80.27%(131)
GSA	98.98%(58)	85.02%(72)	83.46%(48)	78.15%(53)	95.61%(59)	79.97%(133)
GSA-BUR	99.01%(56)	86.22%(61)	84.62%(40)	78.42%(55)	95.84%(62)	80.25%(121)
GSA_BUR_CLF	99.27% (67)	88.53%(52)	85.51% (45)	78.57% (49)	98.63%(53)	81.23%(117)
Table 5: Peak averaged accuracy and corresponding number of features required for each MRwMR
and MRwMR-BUR based algorithm (β = 0.1) using KNN. For MRwMR-BUR based algorithms, the
algorithm with an extension of CLF indicates that the algorithm uses the classifier based method to
estimate UR. Otherwise, the algorithm estimates UR using the KSG estimator.
13