Under review as a conference paper at ICLR 2021
Nonconvex continual learning
with Episodic Memory
Anonymous authors
Paper under double-blind review
Ab stract
Continual learning aims to prevent catastrophic forgetting while learning a new
task without accessing data of previously learned tasks. The memory for such
learning scenarios build a small subset of the data for previous tasks and is used
in various ways such as quadratic programming and sample selection. Current
memory-based continual learning algorithms are formulated as a constrained op-
timization problem and rephrase constraints as a gradient-based approach. How-
ever, previous works have not provided the theoretical proof on convergence to
previously learned tasks. In this paper, we propose a theoretical convergence
analysis of continual learning based on stochastic gradient descent method. Our
method, nonconvex continual learning (NCCL), can achieve the same convergence
rate when the proposed catastrophic forgetting term is suppressed at each iteration.
We also show that memory-based approaches have an inherent problem of over-
fitting to memory, which degrades the performance on previously learned tasks,
namely catastrophic forgetting. We empirically demonstrate that NCCL success-
fully performs continual learning with episodic memory by scaling learning rates
adaptive to mini-batches on several image classification tasks.
1	Introduction
Learning new tasks without forgetting previously learned tasks is a key aspect of artificial intelli-
gence to be as versatile as humans. Unlike the conventional deep learning that observes tasks from
an i.i.d. distribution, continual learning train sequentially a model on a non-stationary stream of data
(Ring, 1995; Thrun, 1994). The continual learning AI systems struggle with catastrophic forgetting
when the data acess of previously learned tasks is restricted (French & Chater, 2002).
To overcome catastrophic forgetting, continual learning algorithms introduce a memory to store
and replay the previously learned examples (Lopez-Paz & Ranzato, 2017; Aljundi et al., 2019b;
Chaudhry et al., 2019a), penalize neural networks with regularization methods (Kirkpatrick et al.,
2017; Zenke et al., 2017), use Bayesian approaches (Nguyen et al., 2018; Ebrahimi et al., 2020),
and other novel methods (Yoon et al., 2018; Lee et al., 2019). Although Gradient Episodic Memory
(GEM) (Lopez-Paz & Ranzato, 2017) first formulated the continual learning as a constrained op-
timization problem, the theoretical convergence analysis of the performance of previously learned
tasks, which implies a measure of catastrophic forgetting, has not been investigated yet.
Continual learning with episodic memory utilizes a small subset of the data for previous tasks to keep
the model staying in a feasible region corresponding to moderate suboptimal region. GEM-based
approaches use the rephrased constraints, which are inequalities based on the inner product of loss
gradient vectors for previous tasks and a current task. This intuitive reformulation of constrained
optimization does not provide theoretical guarantee to prevent catastrophic forgetting. In addition,
the memory-based approaches have the critical limitation of overfitting to memory. Choosing the
perfect memory for continual learning is an NP-hard problem (Knoblauch et al., 2020), then the
inductive bias by episodic memory is inevitable. This problem also degrades the performance on
previously learned tasks like catastrophic forgetting but has not been discussed quantitatively to
analyze backward transfer (BWT).
In this paper, we address the continual learning with episodic memory as a smooth nonconvex finite-
sum optimization problem. This generic form is well studied to demonstrate the convergence and
complexity of stochastic gradient methods for the nonconvex setting (Zhou & Gu, 2019; Lei et al.,
1
Under review as a conference paper at ICLR 2021
2017; Reddi et al., 2016; Zaheer et al., 2018). Unlike the convex case, the convergence is gener-
ally measured by the expectation of the squared norm of the gradient Ek▽/(χ)k2. The theoretical
complexity is derived from the -accurate solution, which is also known as a stationary point with
EkVf (χ)k2 ≤ e. We formulate the proposed continual learning algorithm as a Stochastic gradient
descent (SGD) based method that updates both previously learned tasks from episodic memory and
the current task simultaneously. By leveraging the update method, we can introduce a theoretical
analysis of continual learning problems.
We highlight our main contributions as follows.
•	We develop convergence analysis for continual learning with episodic memory
•	We show the degradation of backward transfer theoretically and experimentally as prob-
lems of catastrophic forgetting and overfitting to memory.
•	We propose a nonconvex continual learning algorithm that scales learning rates based on
sampled mini-batch.
1.1 Related Work
The literature in continual learning can be divided into episodic learning and task-free learning.
Episodic learning based methods assume that a training model is able to access clear task bound-
aries and stores observed examples in the task-wise episodic memory (Lopez-Paz & Ranzato, 2017;
Chaudhry et al., 2019a). On the other hand, an AI system experiences arbitrarily shifting data
streams, which we are not able to access task boundaries in the real world. Task-free continual
learning studies the general scenario without the task-boundary assumption. Aljundi et al. (2019a)
introduces Memory-aware Synapses (MAS) and applies a learning protocol without waiting until a
task is finished. Furthermore, the following work (Aljundi et al., 2019b) adopt the memory system
of GEM selecting observed examples to store for preventing catastrophic forgetting.
Smooth nonconvex finite-sum optimization problem has been widely employed to derive the theo-
retical complexity of computation for stochastic gradient methods (Ghadimi & Lan, 2013; 2016; Lei
et al., 2017; Zaheer et al., 2018; Reddi et al., 2016). Unlike the convex optimization, the gradient
based algorithms are not expected to converge to the global minimum but are evaluated by measur-
ing the convergence rate to the stationary points in the nonconvex case. The complexity to reach
a stationary point is a key aspect of building a new stochastic gradient method for nonconvex op-
timization. In constrast with general optimization, memory-based continual learning methods have
a limited data pool for previously learned tasks, which causes an overfitting problem to memory.
(Knoblauch et al., 2020) found that optimal continual learning algorithms and building a perfect
memory is equivalent. Furthermore, the authors proved that these two problems are NP-hard. The
theoretical result shows that overfitting to memory is inevitable.
2 Preliminaries
We consider a continual learning problem with episodic memory where a learner can access the
boundary between the previous task and the current task. The continuum of data in (Lopez-Paz &
Ranzato, 2017) is adopted as our task description of continual learning. First, we formulate our goal
as the smooth nonconvex finite-sum optimization problems with two objectives,
1 nf	1 ng
min F(X) = f (x) + g(x) = 一 Efi(X) + — Egj(x)	(1)
x∈Rd	nf	ng
i=1	j=1
where X ∈ Rd is the model parameter, each objective component fi (X), gj (X) is differentiable and
nonconvex, and nf, ng are the numbers of components. We define two different components of the
finite-sum optimization as objectives from a sample i of previously learned tasks fi(X) and a sample
j of the current task gj (X).
Unlike the general stochastic optimization problem, we assume that the initial point X0 in continual
learning is an -accurate solution of f(X) with EkVf (X)k2 ≤ for some 1. By the property
of nonconvex optimization, we know that there might exist multiple local optimal points that satisfy
moderate performance on the previously learned task (Garipov et al., 2018). This implies that the
2
Under review as a conference paper at ICLR 2021
model parameter x stays in the neighborhood of x0 or usually moves from an initial local optimal
point x0 to the other local optimal point at the t-th iteration, xt over T iterations of a successful
continual learning scenario.
The continual learning algorithm with an episodic memory with size m cannot access the whole
dataset of the previously learned tasks with nf samples but use limited samples in the memory when
a learner trains on the current task. This limited access allows us to prevent catastrophic forgetting
partially. However the fixed samples from memory cause a biased gradient and the overfitting prob-
lem. In Section 3, we provide the convergence analysis of the previously learned tasks f (x), which
are vulnerable to catastrophic forgetting.
We denote fi(x) as the component, which indicates the loss of sample i from the previously learned
tasks with the model parameter X and Vfi(x) as its gradient. We use It, Jt as the mini-batch of
samples at iteration t and denote btf, btg as the mini-batch size |It|, |Jt| for brevity throughout the
paper. We also note that gj from the current task holds the above and following assumptions.
To formulate the convergence over iterations, we introduce the Incremental First-order Oracle
(IFO) framework (Ghadimi & Lan, 2013), which is defined as a unit of cost by sampling the pair
(Vfi(x), fi(x)). For example, a stochastic gradient descent algorithm requires the cost as much as
the batch size bt at each step, and the total cost is the sum of batch sizes PtT=1 bt. Let T () be the
minimum number of iterations to guarantee -accurate solutions. Then the average bound of IFO
complexity is less than or equal to PtT=(1) bt .
To analyze the convergence and compute the IFO complexity, we define the loss gap between two
local optimal points ∆f as
f(x0)-0≤int≤fTf(xt),
(2)
which might be much smaller than the loss gap of SGD. Suppose that the losses of all optimal points
have the same values, i.e., f (x*) = f (χ0), then We have ∆f ≤ 0. This implies that ∆f is not a
reason for moving away from a stationary point of f, which we will explain details in Section 3.
We also define σf, σg for f, g, respectively, as the upper bounds on the variance of the stochastic
gradients of a given mini-batch. For brevity, we write only one of them σf,
σf = SUp b1 XkVfi(X)- Vf(X)k2.
x bf
i=1
(3)
Throughout the paper, we assume the L-smoothness.
Assumption 1 fi is L-smooth that there exists a constant L > 0 such that for any X, y ∈ Rd,
kVfi(X) - Vfi(y)k ≤LkX-yk	(4)
where ∣∣∙k denotes the Euclidean norm. Then thefollowing inequality directly holds that
—L2∣x 一 yk2 ≤ fi(x) — fi(y) — hVfi(y),χ 一 yi ≤ LL∣X — yk2.	(5)
In this paper, we consider the framework of continual learning with episodic memory. By the as-
sumption of GEM, we assign each task sample from i.i.d. distribution within its episode to the same
memory budget m. In the learning phase at task k ∈ {1, 2, ∙∙∙ , K}, we sample a batch with size nf
from memories of all task with size [m ∙ (k - 1)].
3 Nonconvex Continual Learning
In this section, we present the convergence analysis of continual learning in the nonconvex setting.
The theoretical result shows why catastrophic forgetting occurs in view of the nonconvex optimiza-
tion problem. As a result, we can propose the Non-Convex Continual Learning (NCCL) algorithm,
where the learning rates for the previously learned tasks and the current tasks are scaled by the value
of the inner product by their gradients for the parameter in Section 3.3.
3
Under review as a conference paper at ICLR 2021
3.1	One Episode Analysis
The key element behind preventing catastrophic forgetting is to use gradient compensation on the
training step of the current task. It can be considered as an additive gradient, in turn, is applied to
the gradient of the current task, although GEM (Lopez-Paz & Ranzato, 2017) uses the quadratic
programming and EWC (Kirkpatrick et al., 2017) introduces the auxiliary loss function. First, we
present the proposed gradient compensation, which uses samples of the episodic memory for a single
new task episode. We define the gradient update
xt+1 = Xt- αHt VfIt(Xt)- βHt VgJt (Xt)	(6)
where αHt , βHt are learning rates scaled by the sampled mini-batches for Ht = It ∪ Jt and
VfHt (Xt), VgHt (Xt) are the estimates of the gradient Vf(Xt), Vg(Xt) respectively. Equation 6
implies that the parameter is updated on the current task g with a gradient compensation on pre-
viously learned tasks f by αHt VfIt (Xt). Our goal is to explain the effect of the gradient update
βHt VgJt (Xt) on the convergence to stationary points of f(X) and observe the properties of the
expectation of each element over It. For iteration t ∈ [1, T] and a constant L, we define the catas-
trophic forgetting term Ct to be the expectation in terms of VgJt (Xt):
β2 L
Ct = E 看 kVgjt (xt)k2- βH hVf(xt), VgJt (xt)i ,	(7)
which we derive in Appendix A. We temporally assume the following to show the convergence
analysis of continual learning.
Assumption 2 Suppose that the episodic memory M contains the entire data points of previously
learned tasks [k - 1] on the k-th episode and replays the mini-batch It ⊂ M. Then VfIt (Xt) is an
unbiased estimate that E[et] = 0 for et = VfIt (Xt) - Vf(Xt).
In the next section, we do not use Assumption 2 and investigate the biasedness of the episodic
memory M that causes the overfitting on memory. Our first main result is the following theorem
that provides the stepwise change of convergence of our algorithm.
Theorem 1 Suppose that LaHt 一 OHt ≤ Y for some γ > 0 and α乩 ≤ L. Under Assumption 1, 2,
we have
EkVf(Xt)k2 ≤ ι-⅛ (a⅛ (E[f (Xt)- f(xt+1)]+Ct)+ ¥4	⑻
We present the proof in Appendix A. Note that the catastrophic forgetting term Ct exists, unlike
the general SGD, and this term increases the IFO complexity. Fortunately, we can tighten the upper
bound of Equation (8) by minimizing Ct . Now we telescope over a single episode for the current
task. Then we obtain the following theorem.
Theorem 2 Let aH = a = √t for some c > 0 and all t ∈ [T] and 1 — L a = -1- > 0 for some A.
Under Theorem 1, we have
min EkVf(Xt)k2 ≤ √A= (C (△/ + X Ct) +
(9)
This theorem can explain the theoretical background of catastrophic forgetting. The cumulative
summation of catastrophic forgetting terms P Ct increases drastically over iterations. This fact
implies that the stationary point X0 can diverge. An immediate consequence of Equation 9 is that
we can consider the amount of catastrophic forgetting as an optimization-viewed factor. Without
the additive catastrophic forgetting term, Theorem 2 is equivalent to the result for SGD with a fixed
learning rate (Ghadimi & Lan, 2013). Similar to SGD, the upper bound of Equation 9 can be made
O(√Aτ(∆f + P Ct)) when We assume that 2Lcσf = O(1).
Conversely, we consider the convergence analysis of g(X) by changing roles for f and g in Theorem
2. In the very beginning of iterations, △g is dominant in Equation 9, and its catastrophic forget-
ting term Ct,g with regard to VfIt (Xt) is relatively small because Xt is the neighborhood of the
4
Under review as a conference paper at ICLR 2021
stationary point. When we consider Assumption 2 and the samples from previously learned tasks
are constantly provided, the norm of gradients kfIt (xt)k is bounded. Therefore, g(x) can reach a
stationary point by the same rate as SGD. However, We cannot access the full dataset of previously
learned tasks because of the setting of continual learning. There exists an extra term that interrupts
the convergence of g(x), which is called the overfitting. We now ignore the extra term to conjecture
that kVgjt (x)k is at least bounded. Then We have the following corollary.
Corollary 1 Let the expected stationary of g(x) be O(√ψ) for a constant δ > 0 and the upper
bound of learning rate for g(x) be β > 0. The cumulative sum of the catastrophic forgetting term
C is O(β2δ√T). Nonconvex continual learning by Equation (6) does not converge as iterating the
algorithmfor the worst case, where min EkVf(Xt)k2 is O(β2δ) for 1《β2δ√T. When β2δ ≤ 方,
we have
mtinEkVf(Xt)∣∣2 = O (=).	(10)
Then, the IFO complexity for achieving an -accurate solution off(x) is O 1/2 .
We would like to emphasize that catastrophic forgetting is inevitable in the worst case scenario be-
cause the stationary of f(X) is not decreasing and the convergence on f(X) cannot be recovered no
matter how long we proceed training. Building a tight bound of C is the key to preventing catas-
trophic forgetting. Note that the general setting to minimize C is scaling down the learning rate β to
β2δ ≤ 1∕√T. Then we have the decreasing C = O(1∕√T). However, this method is slowing down
the convergence of the current task g(X) and not an appropriate way. The other option is to minimize
Ct itself rather than tightening the loose upper bound O(β2δ√T). We discuss how to minimize this
term by scaling two learning rates in Section 3.3. The constrained optimization problem of GEM
provided a useful rephrased constraint but cannot explain and guarantee the catastrophic forgetting
in the nonconvex setting. Our convergence analysis of continual learning is the first quantitative
result of catastrophic forgetting in the manner of nonconvex optimization.
3.2 Overfitting to Episodic Memory
In section 3, we discussed the theoretical convergence analysis of continual learning for smooth non-
convex finite-sum optimization problems. The practical continual learning tasks have the restriction
on full access to the entire data points of previously learned tasks, which is different from Assump-
tion 2. The episodic memory with limited size [M] incurs the bias on Vf(Xt). Suppose that we
sample a mini-batch of previously learned tasks from episodic memory M . Then we can formulate
this bias E[eM] as
E[eM] =EVfIt(Xt)-Vf(Xt) =VfM(Xt)-Vf(Xt).	(11)
This equation shows that the bias depends on the choice of M . In the optimization, the bias drag
the convergence of f(X) to fM (X). This fact is considered as the overfitting to the memory M.
(Knoblauch et al., 2020) prove that selecting a perfect memory is hard. We can conclude that
E[eM] 6= 0. Now we extract the overfitting bias on M from the ignored element in Equation 21
at Appendix A and the catastrophic forgetting term in Equation 7. The bias related term BMt is
added to the upper bound of Equation 9 and reformulates the catastrophic forgetting term to a prac-
tical catastrophic forgetting term Ct as
BMt =γh(Vf(Xt),VfM(Xt)-Vf(Xt)i+βHthVfM(Xt)-Vf(Xt),VgJt(Xt)i	(12)
β2 L
Ct = e[% kVgjt (xt)k2 - βHt hVfit (xt), VgJt (xt)i 卜	(13)
Note that the upper bound ofCt is the same as Ct even ifwe modify it to the version with the limited
memory size scenario. The cumulative sum of BMt over iterations is the amount of disturbance by
overfitting to memory. This inherent defect of a memory-based continual learning framework can
be considered as a generalization gap phenomenon Keskar et al. (2016), and small mini-batch size
can resolve this problem. In Section 4, we demonstrate the effect of different mini-batch sizes to
alleviate the overfitting problem on the memory M .
5
Under review as a conference paper at ICLR 2021
Figure 1: Geometric illustration ofNon-Convex Continual Learning (NCCL). In the continual learn-
ing setting, the model parameter starts from the moderate local optimal point for the previously
learned tasks XP. Over T iterations, we expect to reach the new optimal point XPuc which has a
good performance on both previously learned and current tasks. In t-th iteration, the model param-
eter xt encounters either VgJt,pos3) or Vghneg(χt). These two different cases indicate whether
(fit (Xt), VgJt (xt)〉is positive or not. To prevent χt from escaping the feasible region, i.e., catas-
trophic forgetting, we propose the theoretical condition on learning rates for f and g.
3.3 Scaling Learning rates
The result of convergence analysis provides a simple continual learning framework that only scales
two learning rates in the gradient update of Equation 6. As we proved in the above, we should tighten
the upper bound of Ct to prevent catastrophic forgetting. We propose an adaptive scaling method
for learning rates that can minimize or reduce Ct in the both case of (VfIt (xt), VgJt (Xt)) ≤ 0
and (VfIt (xt), VgJt (χt)) > 0. We note that Equation 13 is a quadratic polynomial of β∏t where
β∩t > 0. First, we can solve the minimum of the polynomial on β% when (VfIt (xt), VgJt (xt)) >
0. By differentiating on β%, we can easily find the minimum Ct and the optimal learning rate β.
β*	= (VfIt(X‘), VgJt(Xt))
βHt = LkVgJt (Xt)Il2
—
(VfIt (Xt), VgJt(Xt))
2L∣∣VgJt (xt )∣∣2
(14)
A direct consequence CIt < 0 implies that the optimal catastrophic forgetting surprisingly helps
f (x) to decrease the upper bound of stationary. For (VfIt (xt), VgJt(xt)) ≤ 0, however, β%
should be negative to achieve the global minimum of Cj, which violates our assumption. Instead,
we propose a surrogate of VgJt (xt),
VJt (Xt) = VgJt (Xt)Y fl, VgJt (Xt))f1 ∙
(15)
The surrogate borrows the gradient VfIt (Xt) to cancel out the negative component of VfIt (Xt) from
VgJt (xt). Now we can reduce the catastrophic forgetting term drastically by boosting learning rate
αHt without correcting VgJt (xt) directly. The remaining non-negative value of Ct is caused by
the magnitude of VgJt (xt) itself. This phenomenon cannot be inevitable when we should learn the
current task for all continual learning framework.
We summarize our results as follows.
0H = /α(1 - hf⅞ff*xt"),	(VfIt(xt), VgJt(xt)) ≤0
t [α,	(VfIt(Xt), VgJt(Xt)) > 0
(16)
_ Jα,	(VfIt(Xt), VgJt(Xt))≤ 0
βHt = ff⅛≡⅛^,	(VfIt (xt), VgJt (xt)) >0
(17)
We derive the details of the result in this section in Appendix B. The existing GEM-based al-
gorithms have only focused on canceling out the negative direction of VfM(xt) from VgJt(xt)
with the highly computation cost for the only case (VfIt (Xt), VgJt (Xt)) ≤ 0. The proposed
6
Under review as a conference paper at ICLR 2021
Algorithm 1 Nonconvex Continual Learning (NCCL)
Input: K task data stream {Dι, •…DK}, initial model χ0, memory {Mk} with each size m
for k = 1 to K do
for t = 0 to T - 1 do
Uniformly sample a mini-batch It ⊂ [m ∙ (k - 1)] with |It| = bf
Uniformly sample a mini-batch Jt ⊂ Dk with |Jt | = bg and store Jt into Mk
Compute learning rates α%,β% with Vfit (xt), VgJt (Xt)
Xt+1 - Xt- aHt VfIt (Xt)- βHt VgJt (Xt)
end for
χ0 J XT-1
end for
method has the advantage over both leveraging Ct to achieve the better convergence for the case
hVfIt (Xt), VgJt (Xt)i > 0 and even reducing the effect of catastrophic forgetting by the term
β2 L
-H-∣∣Vgjt(Xt)Il2 for the casehV"(Xt), VgJt(Xtyi ≤ 0 . Figure 1 illustrates intuitively how
scaling learning rates achieve the convergence to a mutual stationary point XP∪c as We proved the
theoretical complexity in Corollary 1.
4	Experiments
Based on our theoretical analysis of continual learning, we evaluate the proposed NCCL model in
episodic continual learning with 3 benchmark datasets. We run our experiments on a GPU server
with Intel i9-9900K, 64 GB RAM, and 2 NVIDIA Geforce RTX 2080 Ti GPU.
4.1	Experimental Setup
Baselines. We compare NCCL to the following continual learning algorithms. Fine-tune is a basic
baseline that the model trains data naively without any support, such as memory. Elastic Weight
Consolidation (EWC) (Kirkpatrick et al., 2017) uses the regularized loss by Fisher Information.
Reservoir Sampling (Chaudhry et al., 2019b) show that simple experience replay can be a power
continual learning algorithm. It randomly selects a fixed number of examples from the stream of data
tasks, which is similar with GEM and A-GEM. GEM and A-GEM Lopez-Paz & Ranzato (2017);
Chaudhry et al. (2019a) is the original and a variant of Gradient Episodic Learning.
Datasets. We use the following datasets. 1) Kirkpatrick et al. (2017) design Permuted-MNIST,
a MNIST (LeCun et al., 1998) based dataset, where we apply a fixed permutation of pixels to trans-
form a data point to make the input data distribution unrelated. 2) Zenke et al. (2017) introduce
Split-MNIST dataset, which splits MNIST dataset into five tasks. Each task consists of two classes,
for example (1, 7), and has approximately 12K images. 3) Split-CIFAR10 is one of most commonly
used continual learning datasets based on CIFAR10 dataset (Krizhevsky et al., 2009), respectively
(Lee et al., 2020; Rebuffi et al., 2017; Zenke et al., 2017; Lopez-Paz & Ranzato, 2017; Aljundi et al.,
2019b).
Training details. We use fully-connected neural networks with two hidden layers of [100, 100]
with ReLU activation. For CIFAR10 datasets, we use a smaller viersion of ResNet18 from the
setting in GEM. To show the empirical result of our theoretical analysis, we apply vanilla SGD into
all train networks.
Performance measurement. We conduct our experiment on K tasks. We evaluate our exper-
iments by two measures, ACC and BWT. ACC is the average test accuracy of all tasks after the
whole learning is finished. Backward Transfer (BWT) is a measure of for forgetting, which shows
how much learning new tasks has affected the previously learned tasks. When BWT < 0, it implies
that catastrophic forgetting happens. Formally, we define ACC and BWT as
1K	1K
ACC = K EACCk,k, BWT = k EACCk,k - ACC®,®,	(18)
k=1	k=1
where ACCi,j is the accuracy of task i at the end of episode j.
7
Under review as a conference paper at ICLR 2021
Table 1: Comparison on ACC and BWT on Permuted-MNIST, Split-MNIST, Split-CIFAR10 on 5
epochs per task over 5 runs. For A-GEM, we report the result in (Chaudhry et al., 2019a).
Method	Permuted-MNIST		SPlit-MNIST		SPlit-CIFAR10	
	ACC (%)	BWT (%)	ACC (%)	BWT (%)	ACC (%)	BWT (%)
Finue-tune	2.43	12.10	19.31	13.00	18.08	9.22
EWC	68.30	0.29	19.80	4.20	42.40	0.26
Reservoir Sampling	10.01	1.00	43.82	-	44.00	-
GEM	89.50	0.06	92.20	-	61.20	0.06
A-GEM	89.10	0.06	93.10	-	62.30	0.07
GSS	77.30	-	84.80	-	33.56	-
NCCL (ours)	68.52	0.22	63.26	0.33	23.11	0.21
Table 2: Comparison on ACC of Permuted-MNIST with 5 permutation tasks on a single epoch per
task with multiple choices of hyperparmeters over 5 runs. We define m as the memory budget for
each task, bg as a batch size for the current task, bf as a batch size for a single previous task.
m	bg	bf	ACC (%)	m	bg	bf	ACC (%)
500	200	10	77.02	200	200	10	75.82
500	200	20	73.95	200	200	20	74.98
500	200	50	71.78	200	200	50	72.35
4.2	Results
Table 1 and Table 2 show our main experimental results. We explain the property of Split dataset
first. Split dataset divide the whold dataset by the number of tasks, so we get a partial version of
dataset. For example, 5 Split-MNIST, we can consider the number of data points per task as the
number of 0.2 epoch. Then, we can call a single epoch of 5 Split-MNIST as a 5 repeated sets of its
datapoints for a task. We conduct experiments on 20 Permuted-MNIST, 5-Split MNIST, and 5-Split
CIFAR10. We can notice that NCCL does not outperform GEM and A-GEM. We conjecture that the
reason of the lower performance is the differences of optimization techniques for new task. GEM-
based methods apply the quadratic programming algorithm to continual learning, which spends
more iterations to find a better surrogate for the negative direction between the previous task and the
current task, but this procedure requires the very longer computation time which is not effective. We
also expect that the theoretical convergence analysis for GEM surrogates can be achieved in future
work. Compared to other reported methods, the performance of NCCL has a reasonable result. By
these observations, we conclude the followings.
•	Our theoretical convergence of analysis is reasonable for explaining catastrophic forgetting.
•	NCCL has both theoretical and empirical supports.
•	We observe that the small mini-batch size from memory is more effective.
5	Conclusion
In this paper, we have presented the first generic theoretical convergence analysis of continual learn-
ing. Our proof shows that a training model can circumvent catastrophic forgetting by suppressing the
disturbance term on the convergence of previously learned tasks. We also demonstrate theoretically
and empirically that the performance of past tasks by nonconvex continual learning with episodic
memory is degraded by two separate reasons, catastrophic forgetting and overfitting to memory.
To tackle this problem, nonconvex continual learning applies two methods, scaling learning rates
adaptive to mini-batches and sampling mini-batches from the episodic memory. Compared to other
constrained optimization methods, the mechanism of NCCL utilizes both positive and negative di-
rections between two stochastic gradients from the memory and the current task to keep a stable
performance on previous tasks. Finally, it is expected the proposed nonconvex framework if helpful
to analyze the convergence rate of other continual learning algorithms.
8
Under review as a conference paper at ICLR 2021
References
Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Pro-
Ceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 11254-11263,
2019a.
Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection
for online continual learning. In Advances in Neural Information Processing Systems, pp. 11816-
11825, 2019b.
Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient
lifelong learning with A-GEM. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019a.
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and Marc’Aurelio Ranzato. On tiny episodic memories in continual
learning. arXiv preprint arXiv:1902.10486, 2019b.
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach. Uncertainty-guided
continual learning with bayesian neural networks. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Robert M. French and Nick Chater. Using noise to compute error surfaces in connectionist net-
works: A novel means of reducing catastrophic forgetting. Neural Computation, 14(7):1755-
1769, 2002. doi: 10.1162/08997660260028700. URL https://doi.org/10.1162/
08997660260028700.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G
Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett (eds.), Advances in Neural Information Processing Systems 31, pp. 8789-
8798. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8095- loss- surfaces- mode- connectivity- and- fast- ensembling- of- dnns.
pdf.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and
stochastic programming. Mathematical Programming, 156(1-2):59-99, 2016.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom-
ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences,
114(13):3521-3526, 2017.
Jeremias Knoblauch, Hisham Husain, and Tom Diethe. Optimal continual learning has perfect
memory and is np-hard. arXiv preprint arXiv:2006.05188, 2020.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgetting with
unlabeled data in the wild. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 312-321, 2019.
9
Under review as a conference paper at ICLR 2021
Soochan Lee, Junsoo Ha, Dongsu Zhang, and Gunhee Kim. A neural dirichlet process mixture
model for task-free continual learning. In 8th International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
Scsg methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.
David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In
Advances in Neural Information Processing Systems, pp. 6467-6476, 2017.
Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, and Richard E. Turner. Variational continual learn-
ing. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classifier and representation learning. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 2001-2010, 2017.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323, 2016.
Mark B. Ring. Continual learning in reinforcement environments. PhD thesis, University of Texas
at Austin, TX, USA, 1995. URLhttp://d-nb.info/945690320.
Sebastian Thrun. A lifelong learning perspective for mobile robot control. In Intelli-
gent Robots and Systems, Selections of the International Conference on Intelligent Robots
and Systems 1994, IROS 94, Munich, Germany, 12-16 September 1994, pp. 201-214,
1994. doi: 10.1016/b978-044482250-5/50015-3. URL https://doi.org/10.1016/
b978-044482250-5/50015-3.
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive meth-
ods for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-
9803, 2018.
Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
Proceedings of machine learning research, 70:3987, 2017.
Dongruo Zhou and Quanquan Gu. Lower bounds for smooth nonconvex finite-sum optimization.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
7574-7583, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
10
Under review as a conference paper at ICLR 2021
Appendix
A Theoretical Analysis
Proof of Theorem 1 We analyze the convergence of nonconvex continual learning with episodic
memory here. Recall that the gradient update is the following
xt+1 = Xt- αHtPfIt(Xt)- βHt VgJt (Xt)
for all t ∈ {1,2,…，T}. Since We assume that f, g is L-Smooth, We have the following inequality
by applying Equation 5:
f (xt+1) ≤ f (xt) + hVf (Xt), xt+1 - Xti + Ikxt+1 - xtk2
=f(Xt) - hVf (Xfl,α∙Ht VfIt (Xt) + βHt VgJt (Xt)i + L ∣∣αHt VfIt (Xt) + Bht VgJt (Xt)k2
≤f(Xt)-αHthVf(Xt),VfIt(Xt)i -βHthVf(Xt),VgJt(Xt)i
+ L aH t∣VfIt (Xt)k2 + L βH t∣Vgjt (Xt)k2.	(19)
Let et = VfIt (Xt) - Vf(Xt) and define
Gt = LβHt∣Vgjt(Xt)k2 -βHthVf(Xt),VgJt(Xt)i,
for t ≥ 1. We have
f(Xt+1) ≤ f (Xt) - αHt hVf(Xt), VfIt (Xt)i + L ɑHt∣∣VfIt (Xt)Il2 + Ct
≤ f(Xt)- (α% - 2aH)kVf(Xt)k2 - (aHt - LaHt)hVf(Xt),eti + 2αHtketk2 + Ct.
Taking expectations With respect to It on both sides, noting that
Ct = EQt]
, We obtain
"t - LaHt) IVf(Xt)k2 ≤ f(Xt) - f(Xt+1) - (aHt - LaHt)EKVf(Xt),eti] + LaH=||“『+ E[Ct]
≤ f (Xt)- f (Xt+1) + Ct + — aHtket k2 + (LaHt- aHt)EKVf(Xt), eti].
Rearranging the terms and assume that LaH - a& ≤ Y and 1 - 2 a& > 0, We have
kVf (Xt)k2 ≤ ——1^-T (f (Xt) - f (Xt+1) + Ct + (LaHt - aHt)E[hVf (Xt), et〉]) + JLaHM
aHt (1 - -2 aHt )	1 - -2 aHt
≤ —τr1L-T (f (Xt) - f (Xt+1) + Ct + YEKVf(Xt),eti]) + 22aHLketk2.
aHt (1 - 2 aHt )	1 - 2 aHt
(20)
Note that under Assumption 2, E[hVf(Xt), eti] = 0, We conclude
kVf(Xt)k2 ≤ ——1^-T (f (Xt) - f(Xt+1) + Ct) + 22aHLketk2 .	(21)
aHt (I - 2aHt )	1 - 2aHt
Furthermore, the batch size b
Proof of Theorem 2 Suppose that the learning rate aHt is a constant a = c∕√T, for c > 0,
1 - l2 a = Aa > 0. Then, by summing Equation 21 from t = 0 to T - 1, we have
11
Under review as a conference paper at ICLR 2021
min EkVf (xt)k2
1 T-1
≤ T EEkVf(Xt)k2
Lemma 1 Let a constant δ > 0 and an upper bound β > βHt > 0. The sum of the catastrophic
forgetting term over iterations T PToI Ct is O(δ√T). For δ ≤ √T, we have O(1).
Proof The upper bound of the catastrophic forgetting term is
Ct = E	kVgjt (Xt)Il2 - βHt hVf (χt), VgJt (χt)i
β2 L
≤ E [牛kVgjt(χt)k2 + βHtkVf(χt)kkVgjt(χt)k
=O(E[kVgjt(xt)k2]).
Since
kVgJt(xt)k2 ≤ kVg(xt)k2 + kVgJt(xt) - g(xt)k2
2
≤ kVg(χt)k2 + bg
bg
where σg is analogous to Equation 3 and bg is the mini-batch size of g. Then we have
Ct =O(EkVg(xt)k2)
O
where t ∈ [T] and for some δ > 0. Summing over time t, we have
T-1
C = ECt = T ∙ O
t=o
Therefore, We obtain O⑴ when β2δ√T ≤ 1.
Proof of Corollary 1 To formulate the IFO calls, let T()
T() = min {T : min EkVf(xt)k2 ≤ }.
Recall that EkVf(Xt) ∣∣2 =O(P√Ct) by Theorem 2. Then by Lemma 1, we have
minEkVf(Xt)k2=O
β β2δ√T
I √T
O(β2δ).
It implies that min EkVf (xt)k2 is not decreasing when 1 W β2δ √T. Then, xt cannot reach to the
stationary point.
On the other hand, f (x) can be converged to the stationary point when β2δ ≤ √T such that
minEkVf(Xt)k2=O(β2δ)=O
(22)
12
Under review as a conference paper at ICLR 2021
To derive a bound for T (), we note that
Then we have
T()
The IFO call is defined as PtT=(1) bf,t. Therefore, the IFO call is O(1/2).
B Derivation of equations in Section 3
Proof of Equations 15 Let the surrogate NgJt (Xt) as
VJt(Xt)=VgJt (Xt)Y f¾，VgJt (U f¾.	(23)
Then, we have
& = E ]βHL IIVgJt (Xt)k2 - βHt hVfit (Xt), VJt (xt)i
=E [号(“t (Xt)k2 - 2 hVfIk VfItVgJtLi2+巧 f,tVg)- βHthV" V VgJt (Xt)i
=E [号(kVgJt (Xt)k2 -(巧 V〉VgJtl2""2) -βHt (hVfIt(Xt), VgJt (Xt)i-hVft (Xt), VgJt (Xt)i)
2	kVfIt (X )k
f [ βH t L (∣∣v t tx ∣∣2	hVfIt (Xt), VgJt (Xt)i2)]	rw
=e[F IkVgJt(X)k----------IlVfIt (Xt)Il2—力.	(24)
13