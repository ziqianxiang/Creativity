Under review as a conference paper at ICLR 2021
Optimal Designs of Gaussian Processes with
Budgets for Hyperparameter Optimization
Anonymous authors
Paper under double-blind review
Ab stract
The remarkable performance of modern deep learning methods depends critically
on the optimization of their hyperparameters. One major challenge is that eval-
uating a single hyperparameter configuration on large datasets could nowadays
easily exceed hours or days. For efficient sampling and fast evaluation, some
previous works presented effective computing resource allocation schemes and
built a Bayesian surrogate model to sample candidate hyperparameters. However,
the model itself is not related to budgets which are set manually. To deal with this
problem, a new Gaussian Process model involved in budgets is proposed. Further,
for this model, an optimal design is constructed by the equivalence theorem to
replace random search as an initial sampling strategy in the search space. Experi-
ments demonstrate that the new model has the best performance among competing
methods. Moreover, comparisons between different initial designs with the same
model show the advantage of the proposed optimal design.
1	Introduction
In recent years, deep learning systems have reached remarkable performance on several important
tasks and receive more and more attention (Lake et al., 2015; Silver et al., 2016; Wu et al., 2016).
Decades of machine learning (ML) research ranging from learning strategies (Rumelhart et al., 1986;
Bengio et al., 2013) to new architectures (LeCun et al., 1995; He et al., 2016) bring this huge success.
Among these successful machine learning systems, almost all of them contain hyperparameters
such as learning rates, batch sizes, or even model architectures that should be tuned carefully for
performance. Nowadays, ML research has developed a new field, named automated machine learning
(AutoML), which aims to automate the ML procedure by spending machine compute time instead of
human research time. The most basic task in AutoML is to automatically set these hyperparameters
to optimize performance.
The hyperparameter optimization problem can be formulated as:
x* = argmin f (x),
x∈X
where x represents a hyperparameter configuration, X is a given search space of hyperparameters,
and f is the target function.
Among many HPO algorithms for solving the optimization problem (Feurer & Hutter, 2019), Bayesian
Optimization (BO) becomes a popular approach due to its sample efficiency (Snoek et al., 2012;
Thornton et al., 2013; Snoek et al., 2015; Feurer et al., 2015). For a more detailed introduction to BO,
we refer to the excellent tutorials by Brochu et al. (2010); Shahriari et al. (2015).
For improving the BO methods with a given search space, we have the following four aspects.
•	Changing the initial design (Jones et al., 1998; Konen et al., 2011; Brockhoff et al., 2015;
Zhang et al., 2019);
•	Changing the surrogate model (Rasmussen, 2003; Hutter et al., 2011; Bergstra et al., 2011;
Springenberg et al., 2016);
•	Changing the acquisition function (Srinivas et al., 2010; Hennig & Schuler, 2012; Hernandez-
Lobato et al., 2014; Wang & Jegelka, 2017; Ru et al., 2018);
1
Under review as a conference paper at ICLR 2021
•	Using the multi-fidelity methods (Thornton et al., 2013; Karnin et al., 2013; Jamieson &
Talwalkar, 2016; Li et al., 2017; Falkner et al., 2018).
For multi-fidelity methods, it involves high-fidelity data obtained by more computing resources and
low-fidelity data with less resources. For expensive high-fidelity models, however, even performing
the number of simulations needed for fitting a surrogate may be too expensive. Inexpensive but less
accurate low-fidelity models are often also available. Multi-fidelity models combine them in order to
achieve accuracy at a reasonable cost.
In this paper we are concerned with multiple fidelities due to the practical applicability. In this
situation, it is possible to define substantially cheaper versions of the objective function of interest,
and the performance of low-fidelity roughly correlates with the performance of the full objective
function. In the literature, many previous works are mainly based on the Successive Halving (SH)
algorithm (Jamieson & Talwalkar, 2016). SH is proposed to identify the best configuration among
K configurations. It evaluates all hyperparameter configurations, throws the worst half, doubles the
budgets and repeats until one configuration left. This method uses a manually given budget for each
iteration which is not desired in AutoML. Therefore, we propose a more proper surrogate model
involved budgets, the corresponding acquisition function and initial design for this model. The main
contribution of our work is as follows.
•	A more accurate model for multi-fidelity data is presented. We add budgets as a factor into
the model to measure the uncertainty caused by using low-fidelity data.
•	The corresponding acquisition function is proposed. This function guides the next sampling
strategy. Thus, the budget-related function will give the next dual sample, the configuration
to be evaluated and the budgets it needs. This procedure helps to set budgets automatically.
•	The optimal initial design for the model is constructed. This design aims to give a more
accurate estimator of the model with the same number of initial samples. It helps to obtain a
better performance or quick convergence.
•	A theorem for judge whether a design is optimal.
•	Simulation studies illustrate that the proposed new model with the corresponding optimal
design outperforms other competing methods.
2 Gaussian Proces s with Budgets
The validation performance of machine learning algorithms can be modeled as a function f : X → R,
where X is the search space of their hyperparameters. The HPO problem is then defined as finding
x* = argminχ∈χ f (x). In the literature, researchers always assume that the true performance f (x)
cannot be observed directly. Instead, we observe y(x) = f(x) + ε, where ε is a noise and follows
N(0, σn2oise) (Falkner et al., 2018). However, the difference between the observation and the true
performance is not only caused by the noise, but also due to the used budgets. Consequently, it
is natural to consider the model which involves budgets, y(x, b) = f (x) + ε(b), where ε(b)〜
N(0, 1/b). This model makes y(x, b) tend to f(x) as b → ∞. The most popular surrogate model
of f(x) is the Gaussian Process (GP) model. They are flexible, meaning that they can fit a wide
variety of surfaces, from very simple to highly complex. Then, we can use a set of collected data
{y(x1, b1), y(x2, b2), . . . , y(xn, bn)} to predict f (x). For multi-fidelity cases, implementing high-
accuracy training for the complex neural network with huge data can be costly. It is not proper to
simply measure the uncertainty caused by budgets with a normal distribution since it is a major error
term. For this purpose, we add another GP model to fit the error term,
y(x, b) = g(x) + h(x, b),	(1)
where g(x) and h(x, b) are realizations of two mutually independent Gaussian stochastic processes
{G(x), x ∈ X} and {H (x, b), (x, b) ∈ X × (0, ∞)}. Further, we assume that E(G(x)) = f1>(x)β1
and E(H (x, b)) = f2> (b)β2, where β1 and β2 are unknown parameters, f1 and f2 are known
regression functions and limb→∞ f2(b) = 0. This model is also used in Tuo et al. (2014) for
computer experiments. Different from theirs, for HPO problems, the covariance matrices are assumed
2
Under review as a conference paper at ICLR 2021
as follows,
Cov(G(x1), G(x2)) = σ2Kθ1 (x1, x2)
Cov(H(x1, b1), H(x2, b2)) = τ2Kθ2 (x1, x2) exp -
1(bι — b2)∖
2 -h2-b
(2)
where Kθ1 and Kθ2 are kernel functions and {σ, τ, θ1, θ2, h > 0} is a set of parameters for describing
the correlation. Note that the second formulation of the covariance matrix in Tuo et al. (2014) is
according to the simplest GP, the Brownian motion (Durrett, 2019). For computer experiments, when
b increases, var(H (x, b)) needs to decrease to zero monotonically. However, in our cases, we just
need to measure the correlation between two configurations with this RBF kernel. The limiting case
h → 0 is used in Ru et al. (2019) and showed its practicality. For the choice of the kernel function K,
the Gaussian correlation family as the most common kernel is adopted,
Kθi (x1, x2) = exp -	θij (x1j - x2j )2 ,	(3)
j=1
where (∙)j denotes the j-th entry of (∙)i.
For this Gaussian Process with Budget model (GPB), we can use a standard Bayesian optimization
procedure that is a sequential design strategy for finding the best hyperparameter configuration x. In
hyperparameter optimization problems, the validation performance f : X → R of hyperparameters
x ∈ X is our goal of minimization. In most cases, f(x) does not admit an analytic form, which
is approximated by the GPB model. The key difference between ours and other models such as
Gaussian processes, random forests, or tree-structured Parzen estimator approach (Bergstra et al.,
2011) is that we consider the budgets into the model and set the value of budgets automatically in
iterations. Based on the data collected on the fly Dn = {(x1, b1, y1), . . . , (xn, bn, yn)}, we sample
next configuration (xn+1, bn+1) according to the acquisition function. The standard algorithmic
procedure of BO is stated as follows.
1.	Assume an initial surrogate model that is the GPB model in this work and take randomly
initial samples (xi, bi, yi) to estimate the model.
2.	Compute an acquisition function a : (X, B) → R which is the expected improvement (EI)
in this work based on the current model.
3.	Sample a batch of hyperparameter configurations and corresponding budgets based on the
acquisition function.
4.	Evaluate the configurations with corresponding budgets and refit the model.
5.	Repeat Steps 2-4 until the stop condition is met.
In the literature of BO, acquisition functions (Step 2) (Wang & Jegelka, 2017) and batch sampling
methods (Step 3) (GOnzalez et al., 2016) are well studied, but how to choose proper initial samples
for speeding up convergence (Step 1) remains largely open. Jones et al. (1998); Konen et al. (2011);
Brockhoff et al. (2015); Zhang et al. (2019) used model-free initial designs Latin hypercube design
or orthogonal array to improve the performance while we construct an optimal initial design for the
particular GPB model in this work.
3 Optimal Initial Design
In this section we introduce some common criteria first to show the basic effect of optimal designs,
minimizing the variance of estimating the key parameters β1 , β2 . This property can give a more
accurate estimation for the GPB model after initialization to accelerate the convergence of the iteration.
For this particular GPB model, we propose an equivalence theorem of the optimality and derive
algorithms for constructing the optimal design.
3.1	Some Common Criteria
We talk about trace criteria under the normal linear model first, then we will show that these criteria
can also be used in other models by using the Fisher information matrix. See Appendix B for more
3
Under review as a conference paper at ICLR 2021
criteria. Now we consider the following normal linear model,
p
y = y^xjβj + ε, where ε ~ N(0,σ2).	(4)
Trace criterion Trace criterion is chosen when our experiment aims to minimize the total variance
of the least squares estimates β: varβ1 + varβ2 + . . . + varβp, because Cov(β) = σ2 (X>X)-1 and
varβι + Vare2 + … + vrβp = σ2 trace[(X > X )-1].
Φ'-Optimality Now we apply the Φ'-optimality introduced by Kiefer (1974) to the Fisher informa-
tion matrix M. Here Φ'(M) = (trM')1/',' ≥ 0. With different choices of ', various criteria occur.
As mentioned before, the most common examples of optimality criteria are
Φ0(M) = det(M) (D-optimality),
Φ1 (M) = tr(M) (A-optimality),
Φ∞ (M) = the maximum eigenvalue of M (E-optimality).
3.2	Approximate Design
The optimization of discretized variables is more difficult than continuous variables. Hence, we
derive the theory of the optimal approximate designs first. According to the approximate results, we
propose algorithms for constructing the exact optimal design.
Now we review approximate designs defined as discrete probability measures with finite support
points. The support points x1 , . . . , xN of a design ξ indicate the hyperparameter configurations where
observations are taken, and the corresponding weights ω1, . . . , ωN represent the probability weights
at these support points. This approximate design ξ is denoted by {S, ω} where S = {x1, . . . , xN}
and ω = {ω1, . . . , ωN}. See Kiefer (1974) for more details. By direct calculation, we have that
the Fisher information M(Fξ) = Fξ>Φ-1Fξ, where the i-th row of Fξ is (ωif1(xi)>, ωif2(bi)>)
for i = 1, 2, . . . , N and the (i, j)-th entry of Φ is σ2Kθ1 (xi, xj) + τ2Kθ2 (xi, xj) max(bi, bj)-h for
i,j = 1, 2,...,N. We apply the Φ'-optimality to this Fisher information and obtain the optimization
problem as follows,
Definition 1 An approximate design ξ is locally Φ'-optimal if ξ = arg max Φ'(M(Fξ)).
3.3	Theoretical Re sult
Theorem 1 uses the Fr´chet derivative d(x,ξ) = limε→o+ ε-1 {logΦ'[M((1 - ε)ξ + εδχ)]-
log Φ'(M(ξ))}, where δχ is the Dirac measure on a single point X introduced by Silvey (2013) to
derive an equivalence theorem for the GPB model.
Theorem 1 An approximate design ξ is locally Φ'-optimal w.r.t. the objective function Φ'(M(ξ)) if
and only ifthe Fr´chet derivative holds that
d(x, ξ)
tr((塔 ψ-1Fξ )'-1(F> Ψ-1Fξ + FξrΨ-1 F⅛χ))
-2≤0
for all x ∈ X.
The brief proof is given in Appendix A. Theorem 1 helps us tojudge whether a design is Φ'-optimal.
Note that when the fis are linear or quadratic functions, the optimization of d(x, ξ) w.r.t. x is linear
programming or quadratic programming respectively. Hence the maximum of d(x, ξ) can be obtained
easily.
Further, in practice, we cannot use this approximate optimal design to run real experiments. At this
time, Theorem 1 is used as a criterion to find a new point x in the iterated construction algorithm.
This construction will be discussed in the next section in detail.
4
Under review as a conference paper at ICLR 2021
3.4 Algorithms for Constructing Exact Optimal Designs
This section proposes two algorithms for constructing exact optimal designs instead of turning an
optimal approximate design into an exact design such as rounding procedure (Pukelsheim & Rieder,
1992).
According to Theorem 1, if a design ξ is not optimal, we can find a configuration x such that
d(x, ξ) > 0. It reveals that we should move the current design along the direction of δx. This
intuitiveness inspires the first algorithm described in Algorithm 1.
Algorithm 1 Iterated Construction
input Maximum iteration I; Sample size N; Number of factors p.
output The exact Φ'-optimal design X*.
1:	Initialize the design with a Latin hypercube N × p matrix X(0) .
2:	for s = 0, 1, . . . , I do
3:	x = arg max d(x, X(s)).
4:	if d(x, X(s)) ≤ 0 then
5:	break;
6:	end if
7:	X(s+1) = X(S) ∪x∖X(s),where X(S) is the i-th row of X(S) and i = argmaxj Φ'(M(X(S) ∪
x\Xj(s))).
8:	end for
9:	Output the optimal design X* = X(s).
In each iteration, we find a best alternative configuration x according to Theorem 1 to replace
the worst one among N configurations of the current design. Theorem 1 also gives a termination
condition d(x, X(S)) > 0.
Compared with general initialization, random search or grid search, this optimal design guarantees to
have models with minimum variance. However, it may explore the space insufficiently.
For this purpose, we introduce Latin hypercube design (LHD) (McKay et al., 1979). It is a kind
of uniform designs since it guarantees that it is uniform on each one-dimensional projection. Let
A = (aij) be an N ×p Latin hypercube matrix in which each column is a permutation on {1, . . . , N}
and all the columns are obtained independently. An ordinary Latin hypercube design D0 = (dij)
of N runs in p factors is generated through dij = (aij -uij)/N, for i = 1, . . . , N, j = 1, . . . , p,
where the uij are independent random variables following U [0, 1], dij is the value of factor j on
the ith run, and the uij and the aij are mutually independent. When D0 is projected onto any one
dimension, precisely one point falls within one of the N equally spaced intervals of (0, 1] given by
(0, N], (N, N],..., (n-1, 1]. Its uniformity leads to exploring the space in a more balanced manner.
Consequently, we consider a Φ'-optimal LHD, i.e., the optimal design among all LHDs. For the
construction of LHD, threshold algorithm is used because the main step of the construction is
permutation, which can be viewed as integer programming.
Algorithm 2 is a kind of simulated annealing. The number of iterations works as the role of
temperature. These two algorithms have the same terminate conditions, maxx d(x, X(S)) ≤ 0, i.e.,
one contribution of Theorem 1. The other one is to guide the decision of next iterated design reflected
in Line 7 of Algorithm 1 and Line 10 of Algorithm 2.
4 Experimental Results
In this section, we compare different initial designs for the GBP model in synthetic experiments
with several classic optimization functions. For more applications, auto data augment and neural
architecture search (NAS) are applied.
5
Under review as a conference paper at ICLR 2021
Algorithm 2 Threshold Acceptance
input Maximum iteration I; Sample size N; Number of factors p.
output The exact Φ'-optimal LHD X*.
1:	Initialize the design with a Latin hypercube N × p matrix X(0) .
2:	for s = 0, 1, . . . , I do
3:	if maxx d(x, X(s)) ≤ 0 then
4:	X * = X ⑶；
5:	break;
6:	end if
7:	Pick one column randomly.
8:	Exchange its two rows randomly.
9:	Obtain a new LHD X(s+1).
10:	if maxx d(x, X*) - maxx d(x, X (s+1)) > -1/s then
11:	X* = X(s+1)；
12:	end if
13:	end for
14:	Output the optimal design X*.
4.1	Synthetic Experiments
Consider the objective function with effect of budgets:
f (x,h) = S?(2°x)+ 3x3 cos(5χ) + 10(χ — 0.5)2 — 0.6 /2+ ɪ sin(15π(x + 0.1))/5.
1+x	b
In the GPB model, the space of kernel parameters are set to θι,θ2 ∈ [40,1000], τ2∕σ2 ∈ [0.03,0.07].
Using the BO method and the proposed GPB model with different initial designs, we can see the
effect of this step. For all designs, the sample size is set to be six. Let IC denote the design obtained
by Algorithm 1 and TA by Algorithm 2. We compare them with random initial and LHD in two
aspects. The result is given in Table 1. Standard deviation (Std.) of β is to measure the robustness of
model estimation. IC has the best performance as its construction while TA is the second best since
it is constrained in LHD. For integral loss, we can see that optimal LHD obtained by Algorithm 2
outperforms other designs since its uniformity from LHD and optimality from Φ'.
Table 1: The performance of different initial designs with N = 6. The integral loss is defined by the
absolute difference between the true model With different b and the estimated model.
Initial design	Std. of β	integral loss, b=1	integral loss, b=2/3	integral loss, b=0.5
Random	-4.598^^	0.297 ± 0.121 ^^	0.285 ± 0.116	0.417 ± 0.388
IC	-37184^^	^^0.305 ± 0.051 ^^	0.306 ± 0.055	0.297 ± 0.065
TA	3.525	0.256 ± 0.064	0.238 ± 0.052	0.227 ± 0.029
For more objective functions listed in Table 5 in Appendix C, Table 2 shows that the proposed initial
designs have much more robust estimation than Random Search does.
Table 2: The performance of different initial designs with different number of initial points.
	rosenbrock		sixhumpcamp	
Initial points	10	20	10	20
Random	0.4963	074700	0.5769	0.4326
IC	0.3918	03276	0.3789	0.3372
TA	0.3583	0.3570	0.4526	0.2983
4.2	Data Augmentation
Data augmentation (DA) is an effective technique to generate more samples from data by rotating,
inverting or other operations for improving the accuracy of image classifiers. However, most
6
Under review as a conference paper at ICLR 2021
implementations are manually designed with a few exceptions. Cubuk et al. (2018) proposed a simple
procedure called AutoAugment to automatically search for improved data augmentation policies.
Unfortunately, it is very time-consuming, e.g., it takes 5000 GPU hours in searching procedure for
CIFAR100 (Krizhevsky et al., 2009). More recently, Ho et al. (2019) and Lim et al. (2019) designed
more efficient algorithms for this particular task.
In their search space, a policy consists of 5 sub-policies with each sub-policy consisting of two
image operations to be applied in sequence. Additionally, each operation is also associated with
two hyperparameters: (1) the probability of applying the operation, and (2) the magnitude of the
operation. In total, there are 16 operations in the search space. Each operation also comes with a
default range of magnitudes. These settings are described in Cubuk et al. (2018). For this problem,
we need to tune two hyperparameters of each sub-policy and choose the best five sub-policies to form
a policy. This is a natural HPO problem.
We search the data augmentation policy in the image classification tasks of CIFAR-10 and CIFAR-100
and follow the setting in AutoAugment (Cubuk et al., 2018) to search for the best policy on a smaller
data set, which consists of 4, 000 randomly chosen examples, to save time for training child models
during the augmentation search process. For the child model architecture, we use WideResNet-28-10
(28 layers - widening factor of 10) (Zagoruyko & Komodakis, 2016). The augmentation policy is
combined with standard data pre-processing: on one image, we normalize the data in the following
order, use the horizontal flips with 50% probability, zero-padding and random crops, augmentation
policy, and finally Cutout with 16 × 16 pixels (DeVries & Taylor, 2017). Experiments use 4 parallel
workers for 4 iterations with 20, 10, 10, 10 configurations respectively. BO is run with budgets of 50
epochs for each configuration. The GPB model is run with alternative budgets of 1, 10, 20, 30, 40, 50
epochs. For each sub-task, we use a SGD optimizer with a weight decay of 0.0005, momentum of
0.9, learning rate of 0.1. We use the found policies to train final models on CIFAR-10, CIFAR-100
with 200 epochs.
Table 3: The performance of different initial designs and the comparison with the GP model in DA.
Initial design	cifar10 acc	cifar10 budget~~	cifar100 acc	cifar100 budget
Random (GPB)	96.97 ± 0.127	1811.0 ± 143.380	81.07 ± 0.277	1750 ± 4.0
IC (GPB)	97.25 ± 0.076	1821.7 ± 131.556	81.23 ± 0.115	1994.5 ± 79.5
TA (GPB)	97.17 ± 0.134	1755.3 ± 183.603	81.17 ± 0.065	1769 ± 24
Random (GP)	96.84 ± 0.233~	2500	80.52 ± 0.105	2500
Table 3 shows that for the proposed GPB model, we have competing results with much less budgets.
The reason for uncertainty of budgets is that our model samples the value of budgets automatically
while BO always use the maximum budgets for evaluation. For comparing accuracy, IC is the best on
CIFAR-100 and TA is the second best, but the advantage is very weak. It is because for these public
data, the training of neural network is often time-consuming and offsets most impact of the initial
designs.
4.3	Neural Architecture Search
One crucial aspect of the deep learning development is novel neural architectures. Designing
architectures manually is a time-consuming and error-prone process. Because of this, there is a
growing interest in automated neural architecture search (NAS). Elsken et al. (2019) provided an
overview of existing work in this field of research. We use the search space of DARTS (Liu et al., 2019)
as an example to illustrate HPO methods on NAS. Particularly, their goal is to search for a cell as a
basic unit. In each cell, there are N nodes forming a fixed directed acyclic graph (DAG). Each edge of
the DAG represents an operation, such as skip-connection, convolution, max pooling, etc., weighted by
the architecture parameter α. For the search procedure, the training loss and validation loss are denoted
by Ltrain and Lval respectively. Then the architecture parameters are learned with the following
bi-level optimization problem: mi□a Lvai(ω*(α), α), s.t. ω*(α) = argmi□ω Ltrain(ω,α).
Here, α are hyperparameters in the HPO framework. For evaluating α, we need to optimize its
network parameters ω. It is usually time-consuming. We search the neural architectures in the image
classification tasks of CIFAR10 and follow the settings of DARTS (Liu et al., 2019). The architecture
parameter α determines two kinds of basic units: normal cell and reduction cell. We search the
7
Under review as a conference paper at ICLR 2021
network architecture in the image classification tasks of CIFAR-10 and CIFAR-100 on a smaller data
set, which consists of 4, 000 randomly chosen examples, to save time for training child models during
the network architecture search process.
Table 4: The performance of different initial designs and the comparison with the GP model in NAS.
Initial design	cifar10 acc	cifar10 budget	cifar100 acc	cifar100 budget
Random (GPB)	96.56 ± 1.685	1834 ± 119.2	79.625 ± 0.435	1840.5 ± 5.5
IC (GPB)	97.22 ± 0.258	1790.333 ± 98.324	82.655 ± 0.015	1970 ± 26
TA (GPB)	97.67 ± 0.385	1681.523 ± 134.523	82.920 ± 0.150	1793.5 ± 9.5
Random (GP)	96.99 ± 0.307~	2500	81.755 ± 0.195	2500
Experiments use 4 parallel workers for 4 iterations with 20, 10, 10, 10 configurations respectively.
BO is run with budgets of 50 epochs for each configuration. The GPB model is run with alternative
budgets of 1, 10, 20, 30, 40, 50 epochs. For a sampled architecture parameter, we fix it in the training
process of updating model parameters. A SGD optimizer is used with learning rate of 0.025,
momentum of 0.9, weight decay of 0.0003, and a cosine learning decay with an annealing cycle.
We use the found network architecture cell to build a 20-layer network and to train final models
on CIFAR-10, CIFAR-100 with 200 epochs. Table 4 shows the similar results as Table 3 but more
significantly, especially on CIFAR100.
The weakness of DARTS is that it has many skip-connect operations which is not preferred. Zela
et al. (2020); Liang et al. (2019) reduced the number of skip-connect operations by early stopping.
However, this issue disappears in the proposed method naturally which is depicted in Figure 1,
because DARTS changes architecture parameters and network parameters in turn while we do not
change the architecture during the training of network.
Figure 1: The architectures of normal cell (above) and reduction cell (bottom) learned by GPB on
CIFAR10.
5 Conclusions
This work has proposed a new model called GPB to involve budgets in the model. This helps to
automatically set the evaluation resources for each hyperparameter configuration and take the variance
caused by different budgets into account. Further, for this particular model, an optimal design is
constructed for estimating the model more accurately. The construction algorithm is derived by the
equivalence theorem (i.e., Theorem 1) which can also be used to judge whether a design is optimal.
Simulation studies support our theoretical results that optimal initial designs constructed by our
algorithms can improve the model robustness and speed up the convergence. In the end, we apply
the method to two popular machine learning problems, NAS and DA, and have the same conclusion
as synthetic experiments. Note that the optimal designs proposed here rely on the particular model.
Zhang et al. (2019) used orthogonal array and range analysis without assuming a model to make the
initialization efficient. However it is quite simple and not compared to other uniform designs. How to
construct an optimal model-free design is still an open problem that we leave for future work.
8
Under review as a conference paper at ICLR 2021
References
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
James S Bergstra, Remi Bardenet, YoShua Bengio, and BalazS KegL Algorithms for hyper-parameter
optimization. In Advances in neural information processing systems, pp. 2546-2554, 2011.
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.
Dimo Brockhoff, Bernd Bischl, and Tobias Wagner. The impact of initial designs on the performance
of matsumoto on the noiseless bbob-2015 testbed: A preliminary study. In Proceedings of the
Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation,
pp. 1159-1166, 2015.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Rick Durrett. Probability: theory and examples, volume 49. Cambridge university press, 2019.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. Journal
of Machine Learning Research, 20(55):1-21, 2019.
Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimiza-
tion at scale. In International Conference on Machine Learning, pp. 1437-1446, 2018.
Matthias Feurer and Frank Hutter. Hyperparameter optimization. In Automated Machine Learning,
pp. 3-33. Springer, 2019.
Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank
Hutter. Efficient and robust automated machine learning. In Advances in neural information
processing systems, pp. 2962-2970, 2015.
Javier Gonzalez, ZhenWen Dai, Philipp Hennig, and Neil Lawrence. Batch bayesian optimization via
local penalization. In Artificial intelligence and statistics, pp. 648-657, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global optimization.
Journal of Machine Learning Research, 13(Jun):1809-1837, 2012.
Jose Miguel Hernandez-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy
search for efficient global optimization of black-box functions. In Advances in neural information
processing systems, pp. 918-926, 2014.
Daniel Ho, Eric Liang, Ion Stoica, Pieter Abbeel, and Xi Chen. Population based augmentation:
Efficient learning of augmentation policy schedules. arXiv preprint arXiv:1905.05393, 2019.
Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization
for general algorithm configuration. In International conference on learning and intelligent
optimization, pp. 507-523. Springer, 2011.
Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter
optimization. In Artificial Intelligence and Statistics, pp. 240-248, 2016.
Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization of expensive
black-box functions. Journal of Global optimization, 13(4):455-492, 1998.
9
Under review as a conference paper at ICLR 2021
Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed bandits.
In International Conference on Machine Learning, pp.1238-1246, 2013.
Jack Kiefer. General equivalence theory for optimum designs (approximate theory). The annals of
Statistics, pp. 849-879, 1974.
Wolfgang Konen, Patrick Koch, Oliver Flasch, Thomas Bartz-Beielstein, Martina Friese, and Boris
Naujoks. Tuned data mining: a benchmark study on different tuners. In Proceedings of the 13th
annual conference on Genetic and evolutionary computation, pp. 1995-2002, 2011.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The
handbook of brain theory and neural networks, 3361(10):1995, 1995.
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:
A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning
Research, 18(1):6765-6816, 2017.
Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen Zhuang, and
Zhenguo Li. Darts+: Improved differentiable architecture search with early stopping. arXiv
preprint arXiv:1909.06035, 2019.
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. arXiv
preprint arXiv:1905.00397, 2019.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. DARTS: Differentiable architecture search. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=S1eYHoC5FX.
Michael D McKay, Richard J Beckman, and William J Conover. Comparison of three methods for
selecting values of input variables in the analysis of output from a computer code. Technometrics,
21(2):239-245, 1979.
Friedrich Pukelsheim and Sabine Rieder. Efficient rounding of approximate designs. Biometrika, 79
(4):763-770, 1992.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine
Learning, pp. 63-71. Springer, 2003.
Binxin Ru, Michael A Osborne, Mark Mcleod, and Diego Granziol. Fast information-theoretic
bayesian optimisation. In International Conference on Machine Learning, pp. 4384-4392, 2018.
Binxin Ru, Ahsan S Alvi, Vu Nguyen, Michael A Osborne, and Stephen J Roberts. Bayesian
optimisation over multiple continuous and categorical inputs. arXiv preprint arXiv:1906.08878,
2019.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by
back-propagating errors. nature, 323(6088):533-536, 1986.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):
148-175, 2015.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484, 2016.
Samuel Silvey. Optimal design: an introduction to the theory for parameter estimation, volume 1.
Springer Science & Business Media, 2013.
10
Under review as a conference paper at ICLR 2021
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep neural
networks. In International Conference on Machine Learning, pp. 2171-2180, 2015.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization
with robust bayesian neural networks. In Advances in neural information processing systems, pp.
4134-4142, 2016.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimiza-
tion in the bandit setting: no regret and experimental design. In International Conference on
Machine Learning, pp. 1015-1022, 2010.
Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka: Combined
selection and hyperparameter optimization of classification algorithms. In Proceedings of the 19th
ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 847-855,
2013.
Rui Tuo, CF Jeff Wu, and Dan Yu. Surrogate modeling of computer experiments with different mesh
densities. Technometrics, 56(3):372-380, 2014.
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. In
International Conference on Machine Learning, pp. 3627-3635. JMLR. org, 2017.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation sys-
tem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,
2016.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hut-
ter. Understanding and robustifying differentiable architecture search. In International Confer-
ence on Learning Representations, 2020. URL https://openreview.net/forum?id=
H1gDNyrKDS.
Xiang Zhang, Xiaocong Chen, Lina Yao, Chang Ge, and Manqing Dong. Deep neural network
hyperparameter optimization with orthogonal array tuning. In International Conference on Neural
Information Processing, pp. 287-295. Springer, 2019.
Appendices
A	Proof of Theorem 1
The approximate design ξ is a minimizer if and only if the FMchet derivative
d(x,ξ)=E∏mε-1 {logΦ'[M((1 -ε)Fξ + εFδ,)] -logΦ'(M(Fξ))}
is non-positive for any X ∈ X, where δχ denotes a one-point design on x. To get the Frechet
derivative, We can calculate the Gateaux derivative of log(Φ'(M(•))) at ξ in the direction δχ, i.e.,
G(ξ,δχ) = limε→0+ ε-1[logΦ'(M(Fξ + εFδχ)) - logΦ'(M(Fξ))]. Note that logΦ'(M(∙))=
log[tr(M(∙)')]1∕' = '-1 log tr(M(•)'). The key calculation is as follows,
tr(M (Fξ + εFδχ)') = tr(((Fξ + εFδχ )>Ψ-1(Fξ + εFδχ))')
= tr((F> Ψ-1Fξ + εF>Ψ-1Fξ + εF> Ψ-1Fδχ)') + o(ε)
= tr((F>Ψ-1Fξ)' + 'ε(Fξ>Ψ-1Fξ)'-1(F>Ψ-1 Fξ + Fξ>Ψ-1Fδx)) + o(ε)
=tr(M (ξ)' + 'ε(F> Ψ-1Fξ )'-1(F>Ψ-1Fξ + Fξ> Ψ-1Fδχ)) + o(ε).
11
Under review as a conference paper at ICLR 2021
Then, the molecule of the G^teaux derivative is that
log
Φ'(M (ξ + εδχ))
Φ'(M (ξ))
ι	tr(M(ξ + εδχ)')
=lθg	tr(M (ξ)')
tr((F>Ψ
='-1 log 1 + 'ε ——
-1Fξ )'-1(F> Ψ-1Fξ + F7Ψ-1Fδχ))
+ o(ε)
tr((FTψ-1Fξ )`-1 (F> ψ-1Fξ + 塔田-圾。))
ε	tr(F>Ψ-1Fξ)'	+ o(ε).
By the definition of the G^teaux derivative, it follows that
G(ξ,δx)
tr((F>Ψ-1Fξ )'-1(F> Ψ-1Fξ + F>Ψ-1Fδχ))
(5)
The FrWchet derivative can be rewritten as
d(x,ξ)=上mε-1{logΦ'[M(Fξ + ε(Fδχ - Fξ))] - logΦ'(M(Fξ))}.
Replace Fδx by Fδx - Fξ in Equation (5), it is obtained that
d(x, ξ)
tr((F>Ψ-1Fξ )'-1 (F> Ψ-1Fξ + F∕Ψ-1Fδχ))
tr(Fξr Ψ-1Fξ)'
- 2.

B More Optimal Criteria
A-Optimality A-optimality is used when the experiment aims to estimate more than one linear
function of the parameters , e.g., K>β, because Cov(K>β) = σ2K>(X>X)-1K. Then we
minimize σ2trace[K>(X>X)-1K] = σ2trace[(X>X)-1KK>] = σ2[(X>X)-1A] with A =
KK>, i.e., A can be any p × p symmetric non-negative definite matrix.
C-Optimality C-optimality is chosen when estimating one particular linear function of the parame-
ters is of our interest, c> β , this criterion is a special case of A-optimality criterion. It is also called lin-
ear optimality. So we aim to minimize var(c>β) = σ2c>(X>X)-1c = σ2trace[c>(X>X)-1c]=
σ2trace[(X>X)-1cc>].
D-Optimality D-optimality is used to minimize the confidence ellipsoid of the estimate β ,
det Cov(β)=σ2det(X>X)-1=σ2∣X>X∣-1=σ2Πjλ-1, where the λj are eigenvalues of X>X.
E-Optimality E-optimality is used when we are interested in estimating a normalized linear function
of the parameters. It can be considered a special case of C-optimality. So we may want to minimize
max var(c>β) for any c, such that, kc∣∣ = 1. By leaving σ2 out we have max∣∣ck = ι var(c>β)=
maxkck=1 c>(X>X)-1c = max eigenvalue of (X>X)-1.
Non-Linear Models For non-linear models, we can apply the optimality to the Fisher information
matrix of β. The Cramer-Rao bound states that the inverse of the Fisher information IS a lower bound
on the variance of any unbiased estimator of β . Thus, when the model is not linear, we can maximize
the Fisher information in different ways to achieve different optimality.
C More Objective Functions
Table 5 presents the objective functions used in Table 2. The search space is that x1 ∈ [-2, 2],
x2 ∈ [-2, 2], b ∈ {104, 103, 102, 101, 100, 10-1, 10-2, 10-3, 10-4, 10-5}.
12
Under review as a conference paper at ICLR 2021
Table 5: objective functions for simulation experiments.
	objective function
SixhUmPcamP	4	∑4 f (x) = (4 — 2.1x1 + 寸)x2 + xιX2 + (―4 + 4x2)x2 + b ∙ N(0,1)
rosenbrock	f (x) = 100(x； - x2)2 + (xι - 1)2 + b ∙ N(0,1)
13