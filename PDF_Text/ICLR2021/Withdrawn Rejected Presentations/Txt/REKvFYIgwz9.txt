Under review as a conference paper at ICLR 2021
Deep Reinforcement Learning for Optimal
Stopping with Application in Financial
Engineering
Anonymous authors
Paper under double-blind review
Ab stract
Optimal stopping is the problem of deciding the right time at which to take a
particular action in a stochastic system, in order to maximize an expected reward.
It has many applications in areas such as finance, healthcare, and statistics. In this
paper, we employ deep Reinforcement Learning (RL) to learn optimal stopping
policies in two financial engineering applications: namely option pricing, and
optimal option exercise. We present for the first time a comprehensive empirical
evaluation of the quality of optimal stopping policies identified by three state of the
art deep RL algorithms: double deep Q-learning (DDQN), categorical distributional
RL (C51), and Implicit Quantile Networks (IQN). In the case of option pricing, our
findings indicate that in a theoretical Black-Schole environment, IQN successfully
identifies nearly optimal prices. On the other hand, it is slightly outperformed by
C51 when confronted to real stock data movements in a put option exercise problem
that involves assets from the S&P500 index. More importantly, the C51 algorithm
is able to identify an optimal stopping policy that achieves 8% more out-of-sample
returns than the best of four natural benchmark policies. We conclude with a
discussion of our findings which should pave the way for relevant future research.
1	Introduction
We consider the problem of Optimal Stopping (OS) in a stochastic system, which can be described as
follows: the system evolves from one state to another in discrete time steps up to some fixed horizon
T . At each time step the decision maker has the option to stop the process or wait for a later step to
do so. If he decides to stop, then he gets a reward that depends on the current state of the system.
Otherwise, the decision maker does not receive any reward immediately, but can decide to stop at a
future time step.
In spite of its simplicity, the optimal stopping model is of use in many fields of application including
asset selling, gambling, and sequential hypothesis testing. Recently, in (Ajdari et al., 2019), it
is used to determine when to stop the treatment of patients receiving fractionated radiotherapy
treatments. (Liyanage et al., 2019) proposes an OS framework to perform feature selection in the
classification of urban issue requests on civic engagement platforms. (Dai et al., 2019) combines
Bayesian optimization and OS in the design of early-stopping strategies for the training of neural
networks. Its most popular application is however in financial engineering. For example, a Bermudan
option is a financial derivative product with a predetermined maturity deadline that will pay out at
exercise time, chosen among a discrete set of time points, an amount that depends on the value of an
underlying financial asset. Based on no-arbitrage theory, these options are usually priced according
to the expected return achieved by an optimal exercise (i.e. stopping) strategy under an assumed
martingale stochastic process, such as a Geometric Brownian Motion (GBM). Alternatively, the buyer
of such an option will often seek to exercise it at the most profitable moment without exact knowledge
of the stochastic dynamics of the underlying asset.
In stochastic control, one way to solve the optimal stopping problem is by using Approximate
Dynamic Programming (ADP) (Bertsekas & Tsitsiklis, 1996; Puterman, 2014) to approximate the
value function that specifies the best expected reward one can receive starting from a given state.
Once this is achieved, a greedy policy based on the approximate value function is expected to provide
1
Under review as a conference paper at ICLR 2021
good decisions. In this paper, we apply the latest advancements in reinforcement learning applied on
several Atari 2600 games (Mnih et al., 2013; 2015; Bellemare et al., 2017; Dabney et al., 2018) to
address the optimal stopping problem. The original algorithms have been modified to adapt to time
series data and a Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) recurrent
neural network is implemented to model long sequences and integrate history. Following the work
by (Hessel et al., 2018), we also combine three additional techniques, the first of which is Double
Q-learning, first introduced in (van Hasselt, 2010) to address the problem of over-estimation of action
values, and has been subject to improvements in (Hasselt et al., 2016) to attain better performance.
The second is the dueling architecture (Wang et al., 2016) that uses two separate estimators: one
for the state value function and one for the state-dependent action advantage function, which leads
to better policy evaluation in the presence of many similar-valued actions. The third is multi-step
bootstrapping of targets (De Asis et al., 2018) which helps accelerate the propagation of newly
observed rewards to earlier visited states and balances the bias-variance trade-off.
In this work, we perform the first comprehensive empirical study of the use of recent deep reinforce-
ment learning algorithms to solve the problem of optimal stopping with application in option pricing
and optimal exercising. We show, for the first time, that with well-designed modifications to the
original algorithms, deep RL architectures such as Double Deep Q-Network (DDQN) (Mnih et al.,
2015), Categorical Distributional RL (C51) (Bellemare et al., 2017) and Implicit Quantile Networks
(IQN) (Dabney et al., 2018) are able to identify policies that achieve near optimal performance in
terms of pricing and to outperform predictive financial models, such as the binomial tree model, in an
option exercising problem. Furthermore, our experiments demonstrate that: (1) models based on deep
reinforcement learning have a high ability to learn and adapt to stochastic environments with high
volatility and randomness; (2) C51 and IQN algorithms outperform DDQN in terms of performance,
at a cost of more computation time; (3) C51 slightly outperforms IQN when confronted to real stock
data movements, identifying an option exercise policy that achieves 8% more out-of-sample returns
than the best of four natural benchmark policies.
The rest of this paper is organized as follows: In Section 2, we discuss related work to our problem,
then define the problem in Section 3. Section 4 describes the three RL architectures that will be
evaluated. Section 5 then presents two experiments involving financial engineering applications that
compare the performance of RL to natural benchmarks. Finally, in Section 6 we conclude with a
discussion of our findings which should pave the way for relevant future research.
2	Related work
Among the state-of-the-art ADP approaches that solve the optimal stopping problem, one finds the
simulation-regression approach of (See Carriere (1996); Longstaff & Schwartz (2001); Tsitsiklis &
Van Roy (2001)) which uses regression to approximate the optimal continuation value at each state of
the system, and the martingale duality-based approach of (V. Desai et al., 2012). The latter relaxes
the non-anticipativity requirement of the policy by allowing it to use future information, but on the
other hand penalizes any policy that uses such information. Several other approaches have been
derived from these two including Rogers (2002), Brown et al. (2010), and Goldberg & Chen (2018).
However, such numerical methods either suffer from the well-known curse of dimensionality, assume
that the underlying stochastic model is known (or in the very least the states that makes it Markovian),
or require the fine tuning of basis functions.
On the RL side, (Becker et al., 2019) proposes an approach in which multilayer feed-forward neural
networks are used. (Li et al., 2009) also applies the least-squares policy iteration RL method to
the problem of learning exercise policies for American options and shows the good quality of the
exercise policies discovered. Even-Dar et al. (2003) provides a model-based and a model-free
variant of a RL action elimination method, based on learning an upper and a lower estimates of
the value function. They further derive stopping conditions that guarantee that the learned policy
is approximately optimal with high probability. In addition, (Buehler et al., 2019) proposes a RL
framework for hedging a portfolio of derivatives, and (Lu, 2017) proposes a recurrent RL algorithm
with evolutionary strategies for robo-trading of long and short positions. (Jiang et al., 2017) presents
a RL framework for portfolio management using a deterministic policy gradient algorithm. (Ciocan
& Misic, 2018) on the other hand addresses the OS problem by constructing interpretable optimal
stopping policies from data using binary trees. Goel et al. (2017) proposes a model-free policy search
2
Under review as a conference paper at ICLR 2021
method that reuses data for sample efficiency by leveraging problem structure to simultaneously learn
and plan. On the other hand, Yu & Bertsekas (2007) introduces alternative algorithms to Q-learning
for OS, which are based on projected value iteration, linear function approximation, and least squares.
(Becker et al., 2019) propose a value-based reinforcement learning for OS learning from Monte Carlo
samples, with application to derivative pricing. It is the closest paper to our work, and the reader
is referred to it for more details of the problem setup.(Hu, 2019) consider the problem of ranking
response surfaces as image segmentation, using feed-forward neural networks to approximate the
value function. Reformulating the optimal stopping problem as a surface ranking problem, they
apply this scheme to pricing Bermudan options. (Chen et al., 2019) propose a Q-learning based
algorithm for OS with an application to derivative pricing. In their paper, they prove convergence of
the algorithm using ODE analysis, and also observe that it achieves optimal asymptotic variance.
To the best of our knowledge, our paper is the first to apply and compare the performance of Deep
DQN (DDQN) (Mnih et al., 2013; 2015), Categorical Distributional RL (C51) (Bellemare et al.,
2017), and Implicit Quantile Networks (IQN) (Dabney et al., 2018) on optimal stopping problems.
3	Problem definition
Using a similar notation to (Ciocan & Misic, 2018), we have the following definitions:
•	β : the discount factor ∈ [0,1]	•	T	: the horizon of the problem.
•	At : the set of possible actions at time t	•	X	⊆ RL : the set of possible states.
•	Ω ⊆ XT : the set of possible trajectories	•	Π	: the set of possible policies ∏ : X →	A.
In particular, we let At := {continue, stop} for t < T,	and AT := {stop}, and always assume	that
the current time t can be inferred from a state ofX, i.e. t = f(X) iffX is observed at time t.
The stopping time with policy π is defined as: τπ = min{t ∈ [0, . . . , T] : π(Xt) = stop} .
Our goal is to find the optimal policy, that maximizes the average discounted return received over all
trajectories:
∏* = argmax∏∈∏E [βτπg(τ∏,ω)],
where ω is shorthand for a trajectory [X1, . . . , XT] and the expectation is taken based on the
distribution of ω. Furthermore, g(t, ω) refers to the payout received when stopping at time t under
trajectory ω .
Alternatively, one can also define
π*(Xt) = ʃ stop	if E[βtg(t,ω)Xt] ≥ E[βτπ+1 g(τ∏+1,ω)∣Xt]
t continue otherwise
where τπt = min{t0 ∈ [t, . . . , T] : π(Xt0 ) = stop}.
In the case of a Bermudan option pricing or exercising problem, X includes information about the
current value of the financial asset, which can be recovered through some S(X), and g(t, ω) :=
max(0, S(Xt) - K) with strike price K for a call option or g(t, ω) := max(0, K - S(Xt)) for a
put option. In other words, a call option will pay the difference between the value of the asset and the
strike price if it is positive, and the opposite occurs for the put option. When K is set to be S(X0),
the option is said to be at-the-money.
The challenge of our approach lies in the development of an efficient implementation that combines
all the recently proposed improvements for deep RL algorithms. Moreover, the original Double
Q-learning (Mnih et al., 2015) uses plain fully-connected layers and exhibits low performance when
applied on strategic games of long time dependencies. In our implementation, LSTM has been
adopted to learn the representation of states, aggregate partial information from the past, and capture
long-term dependencies in our sequential data.
It is also worth mentioning that OS problems cannot be exactly cast as a regression of the optimal
stopping time, or the classification of X as either “stop” or “continue” given that the trajectories are
unlabeled, and that the consequences of continuing are delayed.
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Double Deep Q-Learning Algorithm
1:	Inputs: time series data x, length of an observation l
2:	Initialize: replay memory D to capacity C
3:	Initialize: action-value function Q with random weights θ
4:	Initialize: target action-value function Q with weights φ = θ
5:	for episode = 1 to M do
6:	Select random starting point k
7:	Initialize: first sequence s1 = xk-l:k k K - xk k T
8:	Initialize: episode buffer B to capacity T
9:	With probability episode is random and a uniformly random day to stop tstop is selected
10:	for t = 1 to T do
11:	if episode is random then
12:	if t < tstop then
13:	Set at = continue
14:	else if t = tstop then
15:	Set at = stop
16:	end if
17:	else
18:	select at = arg maxa Q(st, a; θ)
19:	end if
20:	Execute action at and observe reward rt and new observation xk+t
21:	Set st+1 = xk+t-l:k+t k K - xk+t k T - t
22:	Store transition (st, at, rt, st+1) in B
23:	if (at is “stop” or t = tstop) and K - xk+t-1 > 0 then
24:	Exit for loop
25:	end if
26:	end for
27:	Store episode buffer B in replay memory D
28:	Sample a random mini-batch of buffers B of sequence transitions (sj, aj, rj, sj+1) from D
29:	if episode terminates at step j + 1 then
30:	Set yj = rj
31:	else
32:	0 Set yj = rj + γ maxa0 Q(sj+1, a0; φ)
33:	end if
34:	Perform gradient descent on (yj - Q(sj, aj; θ))2 with respect to network parameters θ
35:	Every C episodes reset Q = Q
36:	end for
4	Architectures
Algorithm 1 shows our implementation of double deep Q-Learning. C51 and IQN versions were
similarly developed by adapting the action-value Q-network to learn the value distribution of returns,
and to additionally approximate its quantile function, respectively. We refer interested readers to
(Bellemare et al., 2017) and (Dabney et al., 2018) for implementation details on these two types of
modifications. Since we are dealing with time series of varying lengths (they end when the agent
stops the process), the core of our model uses a dynamic LSTM layer: specifically, we used three
layers with cells of size 512. Our architecture integrates two neural networks: a primary network to
choose an action given the current state, and a target network that generates the target Q-value for
that action. Adam gradient descent (Kingma & Ba, 2014) was used to optimize our networks using
Huber loss as our temporal difference error.
During each episode of training, we first decide whether the episode will employ a random policy or
not. If it is random, then a random stopping time is chosen. Otherwise, the policy learned so far is
used throughout the episode. The probability of employing a random policy is annealed from 1 to
0.01 over time, and no random action is taken during validation or test. We observed that this approach
improved learning efficiency when compared to -greedy policies since it avoided unnecessary use
of the neural network in trajectories where a random action eventually ends up being taken. Our
4
Under review as a conference paper at ICLR 2021
Table 1: Hyperparameters of the different RL versions.
Task	Algorithm	Hyperparameters
GBM	DDQN	learning-rate=0.0001 + batch-size=128 + C=3000 + UPdate-target-freq=300
	C51	learning-rate=0.0025 + batch-size=64 + C=3000 + update-target-freq=30
	IQN	learning-rate=0.00005 + batch-size=128 + C=3000 + UPdate-target-freq=1000
S&P500	DDQN	learning-rate=0.005 + batch-size=64 + C =10000 + UPdate-target-freq=300
	C51	learning-rate=0.0025 + batch-size=64 + C=3000 + update-target-freq=30
	IQN	learning-rate=0.0025 + batch-size=64 + C=3000 + update-target-freq=100
exploration strategy also quickly provided to the agent a more diversified set of experiences to learn
from, with a stopping time that is uniformly distributed over the horizon compared to -greedy which
had a bias towards stopping early. Given the nature of our application, it would be in theory possible
to dismiss exploration altogether if the whole trajectory (even passed the stopping time) was used in
training. However, we found that learning only from the part of the trajectory that precedes stopping
time together with some annealed exploration improved the quality of final policies. We suspect that
this is due to the fact that the neural network’s predictive power ends up focusing more on relevant
states. We also consider that the trajectory past the stopping time might be unobservable in some
applications, e.g. the secretary problem.
It is worth mentioning that, unlike in the original DDQN algorithm (Mnih et al., 2015), we take into
consideration the sequential nature of data. Hence, when sampling a mini-batch of buffers B, we
provide a sequence of T time steps and a maximum of T* batch-size sequence transitions are used in
every mini-batch, depending on the stopping time of every episode ((T 一 n +1)* batch-size in case
of n-step bootstrapping).
The maximum dueling architecture and multi-step bootstrapping of 7-steps (De Asis et al., 2018)
have been integrated and optimised into our version of DDQN, however, they degraded the results for
IQN and C51 during tests and hence their use was omitted in all versions.
Finally, we added a dropout wrapper around the LSTM and fully connected layers, with a drop
probability of 20%, to reduce the risk of over-fitting during training. Soft-updates (τ = 0.001) of the
target network were implemented and tested, where the network is smoothly and gradually updated in
contrast to hard updates that assign the whole online network to the primary network at each update.
Overall, while hard update appeared more stable and better performing when trained with synthetic
data in Section 5.1, soft-updates were the favoured configuration for real data training (in Section
5.2) for both C51 and IQN.
All the code was implemented in tensorflow 1.14 using, among others, CudnnCompatibleLSTMCell
on a GPU, which is 3-5x faster than normal LSTM implementations, and is platform-independent.
To further accelerate learning, We first anneal rapidly to allow the algorithm to learn from more
meaningful samples, then we decelerate the annealing speed through time. This has proven to be
more efficient during our experiments.
5	Empirical Results
In this section, we assess the performance of three different RL algorithms on two financial engi-
neering problems. In the first one, RL is used to price a Bermudan put option in a context where the
underlying stock dynamics are assumed to be known. This is a case where a unique price exists and
can be computed numerically by employing approximation methods such as binomial tree models.
We are therefore able to compare the performance of RL to a ground truth which will validate the
potential of C51 and IQN at identifying truly optimal policies. The second setting involves an optimal
exercise problem where the underlying stock’s dynamics are unknown and based purely on historical
data. We will show that in this real world setting, state of the art methods like C51 can learn policies
that significantly outperform simple benchmark out-of-sample.
5
Under review as a conference paper at ICLR 2021
In both applications, the state X ∈ X will be defined as a sequence of L = 15 scalar values (history
of prices), concatenated with the amount of remaining time (T - t) to maturity of the option and the
relative position of the stock value compared to strike price βt max(K - S(Xt), 0) - max(S(Xt) -
K, 0)), a feature that either returns the discounted reward that will be received (if strictly positive), or
otherwise returns how far the stock is from the strike price. This makes the real size of states fed to
neural networks L + 2. In order to warm start the LSTM, each episode is started 12 days earlier while
the policy is only implemented from day 1. Finally, we limited the number of epochs of training to 5
to avoid overfitting and also to limit computation time. In the special case of C51, in order to have
comparable training times, C51 was trained on a subset of only 48 trajectories (instead of 160) in
Section 5.1, while in Section 5.2 it was trained for only three epochs.
Our experiments will systematically involve three steps of execution. First, we calibrate the hyper-
parameters of each algorithm using a training (Training ) and validation set (Valid_HP ). Once the
optimal setting is found, we employ a second validation set (Valid_Model ) in order to assess in an
unbiased way which algorithm is best performing and finally test this best performing algorithm on a
reserved set of test data. This process allows us to make claims about the statistical significance of
our results. Performance of the RL policies will be compared to three natural benchmarks: “Rand”
which chooses uniformly at random the exercise among the T alternatives, “First” and “Last” which
exercise respectively on the first τ = 1 and last τ = T days, and a binomial tree model (B.M.) that is
either calibrated on the true stock dynamics (in Section 5.1) or on the available set of L historical
prices (in Section 5.2).
5.1	B ermudan option pricing under Black-Schole setting
Our first experiment consists of a classical Black-Schole option pricing problem. When a financial
asset is assumed to behave according to a Geometric Brownian Motion (GBM) model, it is well known
that, in order to avoid giving rise to arbitrage opportunities, a financial derivative of this asset needs to
be priced according to the optimal expected revenue that can be obtained under the GBM’s so called
risk neutral martingale measure. Here, we focus on the case of pricing an at-the-money Bermudan
put option with daily exercise opportunities (often used as a proxy for pricing American options),
where the daily discretized risk neutral measure takes the form of St = St-ιe(r-号Dt+σVZδ∖ With
r as the risk-free continuously compounded yearly interest rate, σ as the volatility of the asset, ε
as the standard normal distribution, and ∆t as the amount of time elapsed between t - 1 and t.
Specifically, in our experiments, we let S0 = 1, σ = 20%, ∆t = 1/252, r = 5%, and the horizon
T = 38. Similarly as in Li et al. (2009), RL will consider a discount factor of β = e-r∆t which
effectively prices an option that pays e-rt∆t max(0, S(X0) - S(Xt)) at exercise time. Hence, in this
experiment, we train the three RL algorithms on simulated trajectories in order to use the expected
reward from the best trained model as an estimation of the arbitrage-free option price (AFOP). While
such a price can be obtained with high precision much more efficiently using binomial tree models
(B.M.), our aim is to verify whether modern RL algorithms are mature and flexible enough to reach
optimality and retrieve such a price.
In this experiment, the Training set is composed of 160 sampled trajectories of 928 days each, from
which are drawn 135600 episodes used in training. The Valid_HP set (for hyper-parametrization)
consists of 40 independently and identically drawn trajectories of 928 days (24000 episodes), while
the Valid_Model (for algorithm selection) and Test set consist of 200 and 400 i.i.d. trajectories over
928 days respectively (159600 and 319200 episodes). Table 1 summarizes the hyper-parameters.
Table 2 presents the results for this experiment. Looking at the numbers, we observe that both C51 and
IQN achieve high Expected Reward (ER) in training and both steps of validation. While C51 appeared
to be the best performing approach on Valid_HP , we suspect that the selection of hyperparameters
overfitted the Valid_HP set given that 1) it outperformed the theoretically optimal policy generated
by the binomial tree model; 2) the performance degraded when validating on Valid_Model set. Given
its better performance in Valid_Model , IQN was selected for the final out-of-sample test where it
estimates a AFOP of 0.0284 ± 0.0001 compared to a ground truth of 0.0283 ± 0.0001. This confirms
that the resulting IQN exercise policy is statistically equivalent to the theoretical policy.
Overall, we can conclude that, despite the context of high stochasticity of GBMs, RL models such
as IQN are flexible enough to learn optimal exercise policies. This shows the high potential of RL
algorithms to replace conventional approaches in situations where the dynamics of the risk neutral
6
Under review as a conference paper at ICLR 2021
Table 2: Performance of the 3 versions of RL vs Baselines for GBM data.
Data \ Method		DDQN	C51	IQN	Rand	Last	First	B.M.
Training	ER	0.0263	0.0267	0.0268	0.0229	0.0263	0.0160	0.0267
	Time (sec)	0.4666	1.3029	0.6668				
Valid_HP	ER	0.0279	0.0280	0.0275	0.0234	0.0279	0.0167	0.0276
Valid_Model	ER	0.0270	0.0273	0.0275	0.0236	0.0271	0.0163	0.0275
	CI	0.0002	0.0002	0.0001	0.0001	0.0002	0.0001	0.0001
Test	ER/AFOP	0.0282	0.0283	0.0284	0.0243	0.0282	0.0168	0.0283
	CI	0.0001	0.0001	0.0001	0.0001	0.0001	0.0001	0.0001
Best values are marked in bold. Training time is per episode (on a Titan X GPU). CI refers to a 90% confidence
interval. Out-of-sample performance of RL model selected with Valid_Model is underlined.
martingale require a large state space in order to become Markovian, and should be easier to adapt
to situations were the market is incomplete or stock dynamics are unknown. On the other hand,
one needs to be aware of the heavy computational burden imposed by current state-of-the-art RL
algorithms. Beyond requiring substantial training time due to their model-free nature, the selection of
best performing hyper-parameters is still more of an art than a science. In particular, we observed
that regions of best performing hyperparameter values were sensitive to factors such as the number of
trajectories and epochs that were used.
5.2	S&P 500 STOCK DATA
In this section, we consider the optimal exercise problem of a Bermudan option. In particular, we
consider the distribution of T days stock trajectories in which one first draws the stock randomly
from 111 stocks that compose the S&P 500 index, and a random date on the period 2014-03-27
to 2019-12-10. The Training set considers trajectories from a subset of 60 different stocks and
dates from the period 2014-03-27 to 2016-03-29 (733 trading days), Valid_HP considers the same
set of stocks with period 2016-03-29 to 2017-11-10 (183 days). The Valid_Model set considers
51 other stocks over the period 2014-03-27 to 2017-11-10. Finally, the test set is composed of all
111 stocks over a “future” period 2017-11-11 to 2019-12-10 (522 working days). In order for the
policies to treat similarly stocks with different starting price, we focus on the task of maximizing the
Expected Relative Option Payout (EROP) for an at-the-money put option with horizon T = 38: i.e.
g(t, ω) := (1/S(X0)) max(0, S(X0) - S(Xt)). Once again, the performance is compared to Rand,
First, and Last policies, while B.M. captures the optimal policy for a GBM calibrated on the last L
days. The same discount rate of β = e-0.05/252 was used. Finally, we let the reader refer to Table 1
to find the best hyperparameters found using Valid_HP for each algorithm.
Table 3 shows the performance of the 3 RL algorithms against the four benchmarks. We can see that
both C51 and IQN outperform DDQN in the Valid_HP set, this is confirmed in the Valid_Model
set which points to C51 as the best model to recommend for out-of-sample tests, although IQN
holds a tight second place. The Test set demonstrates that the best IQN outperforms significantly
the four benchmarks in terms of Expected Reward (and EROP). Indeed, it achieves on average a
2.91% relative option payout compared to exercising on the last day which achieves 2.17%, and the
binomial tree model approach that achieves 2.53%. The table also presents Expected Option Return
(EOR) which accounts approximately for the return on investment when implementing each policy
assuming that the option is priced based on a GBM risk neutral measure calibrated on the recent
history. Specifically, we see that C51 achieves a 22.0% return on average which is 8% higher than
any of the competing classical benchmark.
We wish to emphasize that, throughout our extensive set of experiments, including unreported
experiments with stocks which dynamics followed a more sophisticated Generalized AutoRegressive
Conditionally Heteroscedastic (GARCH) stock model, we observed that IQN has the ability to rapidly
fit the training data, although this can in some cases lead to overfitting. Also, during our experiments,
we noted that DDQN was 1.2-1.5x faster than IQN and around 2-4x faster than C51 depending on
7
Under review as a conference paper at ICLR 2021
Table 3: Performance of the 3 versions of deep RL vs Baselines for S&P 500 data
Data \ Method	DDQN	C51	IQN	Rand	Last	First	B.M.
Training (60 stocks)	ER	0.0290	0.0326	0.0336	0.0249	0.0281	0.0174	0.0270
Valid_HP (60 stocks) ER	0.0156	0.0126	0.0147	0.0130	0.0118	0.0082	0.0123
Valid_Model	ER	0.0274	0.0289	0.0285	0.0245	0.0272	0.0166	0.0256
(51 other stocks)	CI	0.0003	0.0004	0.0003	0.0003	0.0004	0.0003	0.0004
ER	0.0285	0.0291	0.0281	0.0265	0.0271	0.0175	0.0258
CI	0.0003	0.0004	0.0003	0.0004	0.0004	0.0003	0.0004
EROP	2.85%	2.91%	2.81%	2.65%	2.71%	1.75%	2.58%
Test	CI (all stocks combined	0.03%	0.04%	0.03%	0.04%	0.04%	0.03%	0.04%
on a future period) EOR	17.1%	22.0%	17.6%	8.8%	13.9%	-32.6%	5.3%
CI	1.2%	1.6%	1.3%	1.4%	1.6%	1.2%	1.5%
Best values are marked in bold. CI refers to a 90% confidence interval. Out-of-sample performance of RL
model selected with Valid_Model is underlined.
machine configuration, the type of GPU and available memory. Finally, IQN consumes considerably
more memory than DDQN. These are all characteristics that are worth taking into account when
choosing the right RL approach.
6	Concluding Discussion
Solving the problem of optimal stopping in finance where data is known to have a high degree of
randomness (unpredictable) is both a notoriously challenging and intriguing task. In this paper,
we demonstrated the ability of three variants of deep reinforcement learning algorithms (DDQN,
C51, and IQN) to learn simply from real historical stock price observations complex stopping time
policies in the presence of uncertainty, volatility, and non-stationarities. Despite being more difficult
to employ and requiring a more significant computional investment than traditional off-the-shelf
methods, our experiments present empirical evidence that these deep RL algorithms are flexible
enough to retrieve optimal policies in context where these can be computed exactly (option pricing
under GBM dynamics), and to significantly out-perform off-the-shelf methods when the dynamics
of the underlying stochastic system are both unknown and likely to violate simplifying Markovian
assumptions. In particular, distributional IQN and C51 rise up as the favoured algorithms to employ
in practice, with a strong preference for C51 when computation time is less of an issue.
Further improvements to these three algorithms could certainly be investigated. In particular, in the
option pricing application, one could boost performance by accessing additional side-information
about the financial markets (observations of market indexes or stock market, financial and business
news) or about the underlying asset (e.g. financial statements). There are also several opportunities
for improving these networks. A WaveNet model (Oord et al., 2016) could be used to replace the
slower LSTM. Convolutional layers with filters could also be added to the network. These filters
can then be interpreted as feature maps that hold technical significance (as is often the case in image
processing applications), which could help with the interpretability of the RL policy. Finally, one
could also integrate some learning to search techniques to improve training time and stability.
In closing, it is worth mentioning that our experience of hyper-parameters tuning taught us that it
is demanding and fragile, often requiring us to re-align the search grid the moment that problems
are slightly modified. We also observed in our experiments with real stock data, that it could be
beneficial to avoid shuffling the episodes during training with the effect of improving the out-of-
sample performance in periods that are chronologically close to the last episodes that were trained on.
This idea could potentially be useful in online learning, when the underlying process is non-stationary,
since it implicitly fine-tunes the algorithm according to the most recent data. We believe these
constitute two important directions of future investigation.
8
Under review as a conference paper at ICLR 2021
References
Ali Ajdari, Maximilian Niyazi, Nils Henrik Nicolay, Christian Thieke, Robert Jeraj, and Thomas
Bortfeld. Towards optimal stopping in radiation therapy. Radiotherapy and Oncology, 134:96-100,
2019.
Sebastian Becker, Patrick Cheridito, and Arnulf Jentzen. Deep optimal stopping. Journal of Machine
Learning Research, 20(74):1-25, 2019.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 449-458. JMLR. org, 2017.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic programming, volume 5. Athena Scientific
Belmont, MA, 1996.
David B. Brown, James E. Smith, and Peng Sun. Information relaxations and duality in stochastic
dynamic programs. Operations Research, 58(4-part-1):785-801, 2010.
Hans Buehler, Lukas Gonon, Josef Teichmann, and Ben Wood. Deep hedging. Quantitative Finance,
pp. 1-21, 2019.
Jacques F. Carriere. Valuation of the early-exercise price for derivative securities using simulations
and splines. 1996.
Shuhang Chen, Adithya M Devraj, Ana Busic, and Sean P Meyn. Zap Q-learning for optimal stopping
time problems. arXiv preprint arXiv:1904.11538, 2019.
Dragos Florin Ciocan and Velibor V. Misic. Interpretable optimal stopping. Computing Research
Repository (CoRR), abs/1812.07211, 2018.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. arXiv preprint arXiv:1806.06923, 2018.
Zhongxiang Dai, Haibin Yu, Bryan Kian Hsiang Low, and Patrick Jaillet. Bayesian optimization meets
bayesian optimal stopping. In International Conference on Machine Learning, pp. 1496-1506,
2019.
Kristopher De Asis, J Fernando Hernandez-Garcia, G Zacharias Holland, and Richard S Sutton.
Multi-step reinforcement learning: A unifying algorithm. In Thirty-Second AAAI Conference on
Artificial Intelligence, 2018.
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for
reinforcement learning. In Proceedings of the 20th International Conference on Machine Learning
(ICML-03), pp. 162-169, 2003.
Karan Goel, Christoph Dann, and Emma Brunskill. Sample efficient policy search for optimal
stopping domains. arXiv preprint arXiv:1702.06238, 2017.
David A. Goldberg and Yilun Chen. Beating the curse of dimensionality in options pricing and
optimal stopping. arXiv preprint arXiv:1807.02227, 2018.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double Q-
learning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pp.
2094-2100. AAAI Press, 2016.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Ruimeng Hu. Deep learning for ranking response surfaces with applications to optimal stopping
problems. arXiv preprint arXiv:1901.03478, 2019.
9
Under review as a conference paper at ICLR 2021
Zhengyao Jiang, Dixing Xu, and Jinjun Liang. A deep reinforcement learning framework for the
financial portfolio management problem. arXiv preprint arXiv:1706.10059, 2017.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Yuxi Li, Csaba Szepesvari, and Dale Schuurmans. Learning exercise policies for American options.
In Artificial Intelligence and Statistics, pp. 352-359, 2009.
Yasitha Warahena Liyanage, Daphney-Stavroula Zois, Charalampos Chelmis, and Mengfan Yao.
Automating the classification of urban issue reports: an optimal stopping approach. In ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 3137-3141. IEEE, 2019.
Francis A. Longstaff and Eduardo S. Schwartz. Valuing American options by simulation: A simple
least-squares approach. Review of Financial Studies, pp. 113-147, 2001.
David W Lu. Agent inspired trading using recurrent reinforcement learning and LSTM neural
networks. arXiv preprint arXiv:1707.07338, 2017.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing Atari with deep reinforcement learning. Computing
Research Repository (CoRR), abs/1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, February 2015. ISSN 00280836.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw
audio. arXiv preprint arXiv:1609.03499, 2016.
Martin L Puterman. Markov Decision Processes.: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
L. C. G. Rogers. Monte Carlo valuation of American options. Mathematical Finance, 12(3):271-286,
2002.
John N. Tsitsiklis and Benjamin Van Roy. Regression methods for pricing complex American-style
options. IEEE Transactions on Neural Networks, 12(4):694-703, 2001.
Vijay V. Desai, Vivek F. Farias, and Ciamac C. Moallemi. Pathwise optimization for optimal stopping
problems. Management Science, 58, 12 2012.
Hado van Hasselt. Double Q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor,
R. S. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp.
2613-2621. 2010.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling
network architectures for deep reinforcement learning. In Maria Florina Balcan and Kilian Q.
Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning,
volume 48, pp. 1995-2003, New York, New York, USA, 20-22 Jun 2016. PMLR.
Huizhen Yu and Dimitri P Bertsekas. Q-learning algorithms for optimal stopping based on least
squares. In 2007 European Control Conference (ECC), pp. 2368-2375, 2007.
10