Under review as a conference paper at ICLR 2021
Addressing the Topological Defects of Disen-
TANGLEMENT
Anonymous authors
Paper under double-blind review
Ab stract
A core challenge in Machine Learning is to disentangle natural factors of vari-
ation in data (e.g. object shape vs pose). A popular approach to disentangle-
ment consists in learning to map each of these factors to distinct subspaces of
a model’s latent representation. However, this approach has shown limited em-
pirical success to date. Here, we show that this approach to disentanglement
introduces topological defects (i.e. discontinuities in the encoder) for a broad
family of transformations acting on images —encompassing simple affine trans-
formations such as rotations and translations. Moreover, motivated by classical
results from group representation theory, we propose an alternative, more flex-
ible approach to disentanglement which relies on distributed equivariant oper-
ators, potentially acting on the entire latent space. We theoretically and em-
pirically demonstrate the effectiveness of our approach to disentangle affine
transformations. Our work lays a theoretical foundation for the recent success
of a new generation of models using distributed operators for disentanglement
(see discussion). All code is available at https://anonymous.4open.
*^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^
science/r/5b7e2cbb- 54dc- 4fde- bc2c- 8f75d29fc15a/.
1 Introduction
Learning disentangled representations is arguably key to build robust, fair, and interpretable ML
systems (Bengio et al., 2013; Lake et al., 2017; Locatello et al., 2019a). However, it remains unclear
how to achieve disentanglement in practice. Current approaches aim to map different factors of
variations in the data to distinct subspaces of a latent representation, but have achieved only limited
empirical success (Higgins et al., 2016; Burgess et al., 2018). More work on the theoretical foun-
dations of disentanglement could provide the key to the development of more successful approaches.
In its original formulation, disentanglement consists in isolating statistically independent fac-
tors of variation in data into independent latent dimensions. This perspective has led to a range of
theoretical studies investigating the conditions under which these factors are identifiable (Locatello
et al., 2019b; Shu et al., 2020; Locatello et al., 2020; Hauberg, 2019; Khemakhem et al., 2020).
More recently, Higgins et al. (2018) has proposed an alternative perspective connecting disentangle-
ment to group theory (see Appendix A for a primer on group theory). In this framework, the factors
of variation are different subgroups acting on the dataset, and the goal is to learn representations
where separated subspaces are equivariant to distinct subgroups —a promising formalism since
many transformations found in the physical world are captured by group structures (Noether,
1915). However, the fundamental principles for how to design models capable of learning such
equivariances remain to be discovered (but see Caselles-DUPre et al. (2019)).
Here we attack the problem of disentanglement through the lens of topology (Munkres, 2014). We
show that for a very broad class of transformations acting on images —encomPassing all affine
transformations (e.g. translations, rotations), an encoder that would maP these transformations into
dedicated latent subsPaces would necessarily be discontinuous. With this assurance, we reframe
disentanglement by distinguishing its objective from its traditional imPlementation, resolving
the discontinuities of the encoder. Guided by classical results from grouP rePresentation theory
1
Under review as a conference paper at ICLR 2021
[ɪ]^ ÷[7∣÷^÷[fo] ÷ [ɪ]
b e^∏
■ ■■■■■■
lllllll
lllllll
“,mm
lllllll
C I VAE I	I BetaVAE ∣	∣ CaVAE ∣
il∣L∣∣ 11 IlIUiU 3
Latent Dimension	Latent Dimension	Latent Dimension
S-÷0⅛÷≡E T"举 ∖
IQ	I VAE I	I P-VAE I	I CCI-VAE ∣
0.35] 1  0∙35]	0,35π .
S 0.30 .	0.30 .	0.30 ■■
i^l|llliL_ ≡l∣lk^
Rank	Rank	Rank
HΞq
BH
Original 0。	36°	72°	108°	144o	1800	216°	252°	288°	324°
DDD□Qαα□□□D
original 0β 36o	72β	108o	144o	180o	216o	252β	288o	324β
BaBaaBBDESB
Figure 1: Failure modes of common disentanglement approaches. A. Latent traversal best cap-
turing rotation for a VAE, β-VAE, and CCI-VAE for rotated MNIST restricted to a single digit class
(”4”). B. Same as panel A for all 10 MNIST classes. C. Variance of single latents in response to
image rotation, averaged over many test images. D. Ranked eigenvalues of the latent covariance ma-
trix in response to image rotation, averaged over many test images. E. A supervised disentangling
model successfully reconstructs some digits (top) but fails on other examples (bottom). F. Failure
cases of the supervised model trained on a dataset of 2000 rotated shapes (see also Fig. 8).
(Scott & Serre, 1996), we then theoretically and empirically demonstrate the capacity of a model
equipped with distributed equivariant operators in latent space to disentangle a range of affine
image transformations including translations, rotations and combinations thereof.
2	Empirical Limitations of Traditional Disentanglement
In this section we empirically explore the limitations of traditional disentanglement approaches, in
both unsupervised (variational autoencoder and variants) and supervised settings.
VAE, beta-VAE and CCI-VAE We show that, consistent with results from prior literature, a
variational autoencoder model (VAE) and its variants are successful at disentangling the factors of
variation on a simple dataset. We train a VAE, beta-VAE and CCI-VAE (Kingma & Welling, 2014;
Higgins et al., 2016; Burgess et al., 2018) on a dataset composed of a single class of MNIST digits
(the ”4s”), augmented with 10 evenly spaced rotations (all details of the models and datasets are in
App. B). After training, we qualitatively assess the success of the models to disentangle the rotation
transformation through traditional latent traversals: we feed an image of the test set to the network
and obtain its corresponding latent representation. We then sweep a range of values for each latent
dimension while freezing the other dimensions, obtaining a sequence of image reconstructions for
each of these sweeps. We present in Fig. 1A examples of latent traversals along a single latent
dimension, selected to be visually closest to a rotation (see Fig. 5 for latent traversals along all other
latent dimensions). We find that all these models are mostly successful at the task of disentangling
rotation for this simple dataset, in the sense that a sweep along a single dimension of the latent
maps to diverse orientations of the test image.
We then show that on a slightly richer dataset (MNIST with all digits classes), a VAE model
and its variants fail to disentangle shape from pose. We train all three models studied (VAE,
2
Under review as a conference paper at ICLR 2021
beta-VAE, CCI-VAE) on MNIST augmented with rotation, and find that all these models fail to
disentangle rotation from other shape-related factors of variation (see Fig. 1B for the most visually
compelling sweep and Fig. 6 for sweeps along all latent dimensions). We further quantify the
failure of disentanglement by measuring the variance along each latent in response to a digit
rotation, averaged over many digits (see Fig. 1C and details of analysis in App. E). We find that
the information about the transformation is distributed across the latents, in contradiction with the
conventional notion of disentanglement. One possibility would be that the direction of variance
is confined to a subspace, but that this subspace is not aligned with any single latent. In order to
discard this possibility, we carry a PCA-based analysis on the latent representation (Fig. 1D and
App. E) and we show that the variance in latent representation corresponding to image rotation is
not confined to a low-dimensional subspace.
Supervised Disentanglement We further explore the limitations of traditional disentanglement in
a supervised framework. We train an autoencoder on pairs of input and target digit images (Fig. 1E),
where the target image is a rotated version of the input image with a discrete rotation angle indexed
by an integer value k . The input image is fed into the encoder to produce a latent representation.
This latent representation is then multiplied by a matrix operator ψk, parameterized by the known
transformation parameter k. This matrix operator, which we call the disentangled operator, is
composed of a 2-by-2 diagonal block with a rotation matrix and an identity matrix along the other
dimensions (shown in Fig. 1E). The disentangled operator (i) is consistent with the cyclic structure
of the group of rotations and (ii) only operates on the first two latent dimensions, ensuring all other
dimensions are invariant to the application of the operator. The transformed latent is then decoded
and compared to the target image using an L2 loss (in addition, the untransformed latent is decoded
and compared to the original image for regularization purposes). The only trainable parameters are
the encoder and decoder weights. We use the same architecture for the encoder and decoder of this
model that we use for the VAE models in the previous section. This supervised disentanglement
model partly succeeds in mapping rotation to a single latent on rotated MNIST (Fig. 1E top row).
However, there remains some digits for which disentanglement fails (Fig. 1E bottom row). ⅛⅛
difficult to evaluate the CaPacity of the model to learn to rotate many different images With MNIST,
because MNIST is only comp6sed of 10 ClaSSeS of ShaPeS CorreSPonding to the 10 different digits.
To further expose the limitations of this model, we design a custom dataset composed of 2000
simple shapes in all possible orientations. When trained on this extensive dataset, We find that our
the^model fails to capture rotations on many shapes. Instead, it replaces the shape of the input image
with a mismatched stereotypical shape (Fig. 1F). We reproduce all these results with translation in
the appendix (Fig. 8-12).
In conclusion, we find that common disentanglement methods are limited in their ability to
disentangle pose from shape in a relatively simple dataset, even with strong supervision (see also
Locatello et al. (2019b)). We cannot empirically discard the possibility that a larger model, trained
for longer on even more examples of transformed shapes, could eventually learn to disentangle
pose from shape. However, in the next section we will prove, using arguments from topology, that
under the current definition of disentanglement, an autoencoder cannot possibly learn a perfect
disentangled representation for all poses and shapes. In Sec. 43.4 and Sec. 4, We will show that
*^Z^Z^z^Z^Z^Z^z^Z^Z^Z^Z^
another type of model —inspired by group representation theory— can properly disentangle pose
from shape.
3	Reframing Disentanglement
In this section, we formally prove that traditional disentanglement by a continuous encoder is math-
ematically impossible for a large family of transformations, including all affine transformations.
We then provide a more flexible definition of disentanglement that does not suffer from the same
theoretical issues.
3.1	Mathematical Impossibility of Disentanglement
We first consider a simple example case where disentanglement is impossible. We consider the
space of all images of 3 pixels X = R3, and the transformation acting on this space to be the group
of integer finite translations, assuming periodic boundary conditions of the image in order to satisfy
3
Under review as a conference paper at ICLR 2021
Figure 2: Visual proof of the topological defects of disentanglement. A. Top left: O1 , O2 and O3
are three examples of orbits of 3-pixel-images transformed by translation. Bottom: (left) orbits visu-
alized in image space (points constitute the orbits, continuous lines are for visualization purposes);
(right) orbits in latent space. When projected onto the equivariant subspace ZE (gray dotted lines),
all orbits should collapse onto each other. Yet the orbit of a uniformly black image (red dot) con-
tains a single point and thus cannot be mapped onto the other orbits. B. Discontinuity of fE around
symmetric images. Top: consider an image of an equilateral triangle, with an infinitesimal pertur-
bation on one corner (black dot), undergoing rotation (color changes are for visualisation purposes).
Bottom: (left) after a rotation of 120°, the orbit in image space (here projected onto 3 dimensions
for visualisation) almost loops back on itself; (right) in ZE , each angle of rotation corresponds to a
distinct point in space. Therefore, the encoder fE is discontinuous (as shown by the red arrows).
the group axiom of invertibility (Fig. 2A, see App. A for definitions). Given an image, the set of
images resulting from the application of all possible translations to this image is called the orbit
of this image. We note that the space of images R3 is composed of an infinite set of disjoint or-
bits. Can we find an encoder f which maps every point of image space X to a disentangled space Z?
To conform to the conventional definition of disentanglement (Higgins et al., 2018) (see
App. C for a formal definition), Z should be composed of two subspaces, namely (i) an equivariant
subspace ZE containing all and only the information about the transformation (i.e. location along
the orbit) and (ii) an invariant subspace ZI , invariant to the transformation but containing all other
information about the image (i.e. identity of the orbit). Each orbit should thus lie in a plane parallel
to ZE (otherwise some information about the transformation would leak into ZI), and all orbits
projected onto ZE should map onto each other (otherwise some information about the identity of
the orbit would leak into ZE). We now consider the orbit containing the black image [0,0,0]. Since
all translations of the black image are the black image itself, this orbit contains only one point. And
yet, the image of this orbit in ZE should conform to the image of other orbits, which generally
consist of 3 distinct points. Since a function cannot map a single point to 3 points, an encoder f
ensuring disentanglement for all images cannot exist.
Using similar topological arguments, we formally prove the following theorem (App. C.1),
generalizing the observation above to a large family of transformations including translations,
rotations and scalings.
Theorem 1: Disentanglement into subspaces by a continuous encoder is impossible for any
finite group acting on Euclidean space RN.
3.2	Practical Examples of Topological Defects
The formal theorem of the previous section does not tell us how hard it would be to approximate
disentanglement in practice. We show next that a disentangling encoder f would need to be discon-
tinuous around all images that present a symmetry with respect to the transformation, which makes
this function very discontinuous in practice. As an example, we consider the image of an equilateral
triangle undergoing rotation (Fig. 2B, color changes are for visualisation purposes). Due to the sym-
metries of the triangle, a rotation of 120° of this image returns the image itself. Now We consider the
same image with an infinitesimal perturbation on one corner of the triangle, breaking the symmetry
4
Under review as a conference paper at ICLR 2021
of the image. A rotation of 120o of this perturbed image returns an image that is infinitesimally
close to the original image. And yet the equivariant part of the encoder f⅛ (i.e. the projection of f
onto the equivariant subspace ZE) should map these two images to disjoint points in the equivariant
subspace ZE, in order to properly encode the rotation transformation. Generalizing this argument to
all symmetric images, we see that a disentangling encoder would be discontinuous in the neighbor-
hood of all images that present a symmetry with respect to the transformation to disentangle. This
is incompatible with most deep learning frameworks, where the encoder is usually a neural network
implementing a continuous function. We provide a formal proof of the discontinuity of /e in App.
C.2.
The invariant encoder f1 (i.e. the projection of f onto the invariant subspace ZI) also presents
topological defects around symmetric images. We provide both a visual proof and a formal proof of
these defects in App. C.2.
3.3	A more More Flexible Definition OF Disentanglement
^^^^^^
An underlying assumption behind the traditional definition of disentanglement is that the data is
naturally acted upon by a set of transformations that are orthogonal to each other, and that modify
well-separated aspects of the data samples. However, in many cases this separation between fac-
tors of variation of the data is not possible (as also noted by Higgins et al. (2018)). We notice that
the current definition of disentanglement unnecessarily conflates the objective of isolating factors of
variation with the algorithm of mapping these factors into distinct subspaces of the internal repre-
sentation. in order to build a model that respects the structure of the data and its transformations,
the latent space should instead preserve the entanglement between factors of variation that are not
independent. Thus, We
A model equipped with a latent operator is equivariant to a transformation if encodinga sample then
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^W^^^^^ʤ^^^^^
applying the latent operator is equivalent to transforming the sample first then encoding. Formally,
W^^^W^^W^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^W^^^^^^^^^^^^^^^^∙^^^^^^^^^,
x ∈ X
WWWWWW
f(φk(x)) = Ψk(f (x)),Vk ∈ K,
WWWWWWWWWWWWWWWWWWWWWWWWW
(1)
we turn to a definition of disentanglement in which the transformations are modelled as distributed
WWW
operators (i.e. not restricted to a subspace) in the latent space.
DefinitiOn 1:
corresponds to the action of a single transformation and the resulting model is equivariant.
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW
These operators are 八 representation is Said to be disentangled With respect to a PartiCUIar
WWWWWWWWWWWWWWWWWW
decomposition ofa Symmetry group into subgroups, ifthere is afamily OfknOWn OPeratOrS acting on
this representation, POtentiaUy distributed across the full latent, Where each OPeratOr is equivariant
to the action ofa SingIe SUbgroup. controllable ThiS definitionis in the sense that they have an explicit
WWWWWWWWW	WWWWWWWWWWWWWWWWWWWWWWWWWWWWW
form, thus allowing the user to manipulate the latent representation by applying the operator. This
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW
definition, more flexible than traditional disentanglement in the choice of the latent operators, while
WWWWWWWWW
SerVing the Same ObjeCtiVe as traditional disentanglement (isolation and identification of QbWWJP
the same desiderata of identification and isolation of the factors of variations present in the data)..
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW	WWWWWW	W
3.4 The Shift Operator for Affine Transformations
models. Transformations in data often have additional structure describing how to (1) undo a
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW
¢35^^0^11555^3^^^121^10^^15^01^1(^1^^^5^(31^^^^1^^
for the order transformations are applied (associativity). A collection of transformations with a rule
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW
for combining two transformations into another satisfying these three simple requirements has a
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW
group structure (see App. A for a formal definition). With a group structure, we can decompose
WWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWWW
5
Under review as a conference paper at ICLR 2021
flexible disentangled models.
^^^^^^^^^^^^^^^^^^^^^^^
Using classical results from the linear representation of finite groups (Scott & Serre, 1996),
we show in App. C.3ʃnow that a carefully chosen distributed operator in latent space, —the shift
operator ψk (shown in Fig. 3A)——is linearly isomorphic to specific transformations that include
■^^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^
integer pixel translations and rotations. With this operator, we can learn a latent space equivariant
zzzZzzZzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzZzzzzzZzzzzzzzzzzzzzzzzzzZzzzzzZzzzzzz
Equation 1 rewrites as follows:
Z^ZZZZZZZZZZZZZZZZZZZZZZZ
W φk(x) = ψk(Wx) ∀x ∈ X(= RN), ∀k ∈ K	(2)
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
Vk ∈ K,φk = WT ψ W
ZZZZZZZZZZZZZZZZZZZZZ
(3)
We consider additional properties on φ corresponding to the assumptions (i) that G is cyclic
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
of order K with generator g∩ and (ii) φ is isomorphic to the regular representation of G (see
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
Scott & Serre (1996)). These properties are respected by all cyclic linear transformations of finite
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
order K ofthe images (see App. A for definitions). SPeCiaI CaSeS of these transformations include all
discrete and CyCIiC affine transformations, such as integer pixel translation with periodic boundary
conditions, or rotations. A PraCtiCaI COnSeqUenCe of this proof is that it is POSSibIe to learn a Iatent
SPaCe equivariant to any affine transformation With this OPerator, USing a SimPIe Unear autoencoder.
DefinitiOn and PrOPertieS of the Shift OPeratOr For a transformation gk ∈ G SUCh that gk = g∩,
Where g∩ is the generator of Given that the encoder and decoder are linear and invertible, the two
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
representations φ and ψ must be isomorphic. Two representations are isomorphic if and only if they
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
have the same character (Scott & Serre, 1996, Theorem 4, Corollary 2) (see App. A for a definition
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
of characters). We thus want to choose ψ such that it preserves the character of the representation
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
φjZZJZPonZngJoJheJKZiOZOLG , the COrreSPOnding Shift OPeratOr ψk is the Unitary Shift matriX
exponentiated by k.—Note that this OPeratOr is distributed as it acts on the dataset of images.
ZZZZZZZZZZZZZZZZZZZZZ
Importantly, we will see that our proposed operator needs to be distributed in the sense that it
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ZZZZZZZZZZZZZZZ
should act on the full latent space, COntrary to conventional disentanglement models.Importantly,
ZZZZZZZZZ
the proposed Shift OPeratOr is flexible in the SenSe that it does not require SPeCifiC knowledge of
WhiCh transformation groupis applied to the data (i. e. for example rotations or translations), but
only the order of the group and the Charaeter of its representation (App. A). FUrthermore, in App.
code.
ZZZZ
Let us consider the matrix of order K = IGI that corresponds to a shift of elements in
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
a K-dimensional vector by k positions. We construct from Mk the shift operator as a
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ZZZ
representation of the group's action on the latent space. For each gk ∈ G its corresponding
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
shift operator is the block diagonal matrix of order N composed of N repetition of Mk.
KmKmmmmmammmmmmmmKmmKaKaKmammm
k
Mk :=
"0
1
0
Mk
Mk
(5)
0
0
1
0
1
0
⑷
0
0
Mk
1
ZZZZZZZZZZZZZZZZZZZZZ
using this shift operator ensures that an equivariant invertible linear mapping W exists between
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
propose a complex VerSiOn of the Shift operatorshow that we can also replace this shift operator by
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
6
Under review as a conference paper at ICLR 2021
a complex diagonal operator, which is more computationally efficient. AlthOUgh these OPeratOrS
*^Z^z^Z^Z^^^Z^Z^Z^Z^z^Z^∙^Z^Z^Z^Z^Z^∙^Z^Z^Z^Z^Z^
CannOt theoretically be mapped to COntinUOUS transforms, We note that any COntinUOUS transform
Can be approximated USing finite groups. In the remainder of the paper, We denote ψt,k,N the Shift
OPeratOr that COrreSPOndS to t ∈ T Where T is a CyCliC group of transformations (e. g. rotations)of
order N With generator to and order K, SUCh that t = t0.
In the next SeCtion, We show how these theoretical results Can lead to PraCtiCal and effective
disentangling models for affine transfOrmatiOnSSUCh as rotations and translations. to multiply with
ZzzzzzzZZzzzz
the latent.
zzzzzzzzz
y). The role of the enCoder is to ConstrUCt a latent spaCe where transformations Can be represented
ZZzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzZzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzZzzzzzz
supervised version of the shift operator. These assumptions allow us toguarantee a linear equivariant
z^^^^zzzzzzzzzzzzzzzzzzzzzZzzzzzzzzzzzzzzzzZzzzzzzzzzzzzzZZzzzzzzzzzzzzzzZzzzzzz
model Can be learned with pairs of examples (see oUr training objeCtives in App. B.2). Note that the
zzzzzzzzzzzzzzzzzzzzzZzzzzzzzzzzZzzzZzzzzzzzzzzzZzzZzzzzzzzzzZZzzzzZzzzzzzzzzzz
taCkle Continuous transformations, a disCretisation step Could be added and we leave the exploration
"Z^Z^Z∕Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^^^^^Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z/Z^Z^Z^Z∕Z^Z^^
of this extension for future work.
ZZZZZZZZZZZZZZZZZZZZZZZZZZZ
4 Distributed Disentanglement in Practice
Our empirical (Sec. 2) and theoretical (Sec. 3) findings converge to show the difficulties of dis-
entangling even simple affine transformations into distinct subspaces. Here we show that, using
distributed equivariant operators instead, it is practically possible to learn to disentangle these affine
transformations, according to our more flexible definition of disentanglement.
4.1	the Supervised Shift Operator Model
Guided by our theoretical results, we train a supervised non-variational autoencoder using pairs
of samples and their transformed version (with a known transformation indexed by k) using the
distributed shift operator from Sec. 3.4 (shown in Fig. 3A) instead of the disentangled operator
from Sec. 2. We feed the original sample x to a linear invertible 1 (or quasi-invertible. see App.
ZZZZZZZZZZZZZZZZZZZZZZZ
BZZlenCOder that produces a latent representation. The latent representation is then multiplied by
the shift operator matrix parametrized by k. The transformed latent is then decoded and L2 loss
between the two reconstructions (x and its transformed version) and their respective ground-truth
images is back-propagated.
As predicted by character theory, our proposed model is able to correctly structure the latent
space, such that applying the shift operator to the latent code at test time emulates the learned
transformation (see Fig. 3Aand, test MSE reported in Table 2). Also COnSiStentWithq and the LSBD
Z	ZZZZZZZZZZZZ
dZiZsZeZnZtaZnZgZlZeZmZeZnZtZmZZeZaZsZuZreZZfZroZmZZZAZnZoZnZyZmZZoZuZsZ(Z2Z0Z2Z1Z)ZaZnZdZZreZpZoZrZteZdZZiZnZZAZpZpZ.ZZEZZ.1Z.Z1Z).ZZiZnZtZeZreZsZtiZnZgZlZy,
test MSE for rotations are lower than for translations. We believe this is due to the fact that in
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
striking than with rotations. Consistent with the theory, the same linear autoencoder equipped with
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
a disentangled operator fails at learning the transformation (Fig. 3B). We reproduce these results
for translation in App. E. We thus show that, unlike prior approaches, our theoretically motivated
model is able to learn to disentangle affine transformations from examples.
4.2	the Weakly Supervised Shift Operator Model
Here we show that our method can also learn to disentangle transformations in a weakly supervised
setting where the model is not given the transformation parameter between pairs of transformed
images (e.g. rotation angle) during training. We consider the case of a single transformation for
simplicity. We encode samples by pairs χ1,χ2 (with x2 a transformed version of xi) into zι and
z2 respectively, and use a phase correlation technique to identify the shift between zi and z2, as in
1or quasi-invertible, See App. B.2
7
Under review as a conference paper at ICLR 2021
Ddbhbhqqhhd
Figure 3: Success and flexibility of proposed distributed shift operator models: A. Proposed
shift operator model successfully learns rotation on simple shapes. B. Disentangled operator fails
to learn rotation. C. Weakly supervised shift operator model, using the complex version of the
shift operator, successfully rotates simple shapes. Note that the model maps ground-truth counter
clockwise rotations to clockwise rotations, while respecting the cyclic structure of the group. D.
Stacked shift operator model succeeds on conjunction of translations. E. Stacked shift operator
model succeeds on conjunction of translations and rotations. Numbers above plots indicate rotation
angle and/or translation in x and y respectively.
Reddy & Chatterji (1996). An L2 loss is applied on reconstructed samples, with the original sam-
ple transformed according to all possible k and weighted according to soft-max scores given by the
cross-correlation method (see App. B.3 for details). Here and in the remainder of the experiments,
we use the complex version of the shift operator for computational efficiency (shown in Fig. 3C).
This weakly supervised version of the model has an extra free parameter which is the number of
latent transformations, not known a priori. Let us denote KL this number, which can be different
than the ground-truth order of the group K. We explore the effect of different KL in App. B.3. The
results of this model (with 10 latent transformations KL) are shown in Fig. 3C. The weakly super-
vised shift operator model works almost as well as its supervised counterpart, and this is confirmed
by test MSE (see Table 2). The same model can successfully be trained on MNIST digits (Fig. 14).
4.3	Multiple transformations: Stacking Shift Operators
So far, we have only considered the case of a single type of transformation at a time. When working
with real images, there are more than one type of transformation jointly acting on the data, for
example a rotation followed by a translation. Here we show how we can adapt our proposed shift
operator model to the case of multiple transformations.
Stacked shift operator model In the case of multiple transformations, a group element is a com-
position of consecutive single transformations. For example, elements of the Special Euclidean
group (i.e. translations and rotations) are a composition of the form ay ax h, where ax is an ele-
ment of the x-translation subgroup, ay of the y-translation subgroup, and h of the image rotation
subgroup. Our theory in C.3 ensures that each of these subgroup’s action in image space is linearly
isomorphic to the repeated regular representation ψ . We can thus match the structure of the learning
problem by simply stacking linear layers and operators. We build a stacked version of our shift
operator model, in which we use one complex shift operator for each type of transformation, and
apply these operators to the latent code in a sequential manner, akin to Tai et al. (2019). Specifically,
8
Under review as a conference paper at ICLR 2021
consider an image x ∈ X that encounters a consecutive set of transformations gn, gn-1, . . . g1, with
gi ∈ Gi ∀i. We encode x into z with a linear invertible encoder z = W x. We then apply the
operator on the latent space, ψ1(g1), corresponding to the representation ofg1 on the latent space Z.
We then apply a linear layer L1 before using the operator corresponding to G2 . The resulting latent
code after all operators have been applied is z0 = ψn(gn)Ln-1 . . . ψ2(g2)L1ψ1z. The transformed
latent code z0 is fed to the linear decoder, in order to produce a reconstruction that will be compared
with the ground-truth transformed image x0 , as in the single transformation case.
Translations in X and Y Consider the conjunction of translations in x and y axes of the image.
This is a finite group of 2D translations. This group is a direct product of two cyclic groups, and it is
abelian (i.e. commutative). We refer the interested reader to App. D.1 for details on direct products.
To tackle this case with the stacked shift operator model, we first use the shift operator ψx,k,N
corresponding to the translation in x, then apply a linear layer denoted L1, before using the operator
ψy,k0,N corresponding to the translation in y: z0 = ψy,k0,N L1ψx,k,N z. We train this stacked model
on translated shapes with 5 integer translations in both x and y axes (i.e. the group order is 25).
Results reported in Fig. 3D show that the stacked shift operator model is able to correctly handle the
group of 2D translations.
Translations and rotations We consider a discrete and finite version of the Special Euclidean
group, where A is a finite group of 2D translations presented in the previous section and H a finite
cyclic group of rotations. This group has a semi-direct product structure (see App. D.3 for details)
and is non-commutative, contrary to the 2D translations case. With the stacked shift operators model,
we first use the operator ψh,j,N corresponding to the rotation h, then the one for translation in x, then
the one for y-translation. The resulting transformed latent code is z0 = ψy,k0,N L2ψx,k,N L1ψh,j,N z.
We train this stacked model on discrete rotations followed by integer translations and discrete ro-
tations, using 5 integer translations in both x and y axes and 4 rotations. Results reported in Fig.
3E and MSE Table 2 show that the model is perfectly able to structure the latent space such that
the group structure is respected. Additionally, Figures 17 and 18 show pairs of samples and the
*^Z^z^Z^Z^Z^z^Z^Z^^z^Z^Z^∙^z^Z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^∙^Z^Z^Z^Z^Z^Z^Z^Z^
reconstructions by the stacked shift operator model in the cases of (i) translation in both x and y
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
axes and (ii) rotations and translations in both axes. In appendix Figure 16 we also explore the
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
case where the order of the group of rotations is 5, breaking the semi-direct product structure (see
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
note in Appendix D.4.1) and show that the stacked shift operator nonetheless performs with great
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
performance.
ZZZZZZZZZZZ
Insight from representation theory on the structure of hidden layers When dealing with mul-
tiple transformations (e.g. rotations and translations), we know the form of the operator for every
subgroup (shift operator), but we do not know a priori the form of the resulting operator for the
entire group. In App. D.1 and D.4 we derive from representation theory the operator for the entire
group in the 2D translation and Special Euclidean group cases and show that they can be built from
each subgroup’s operator in a non-trivial way. Importantly, we show that the resulting operator for
the discrete finite Special Euclidean case has a block matrix form representation based on represen-
tations of both translations and rotations. This is expected: this group is non-commutative, so the
correct operator cannot be diagonal otherwise two operators corresponding to two elements would
commute. Equipped with this theory we can derive insights about the form that intermediate layers
should take after training. In particular, we show (App. D.2) that the layer L1 should be a block
diagonal matrix consisting of repetitions of a permutation matrix that reorders elements in ψx,k,N z.
Similarly, in the case of translations and rotations together, L2 must reorder ψy,k0,N, and L1 be the
product of two matrices L1 = PQ, where Q is a N by N block diagonal matrix and P is reordering
the rows of the vector Qψh,j,N z (see App. D.5). In future work, we plan to explore the use of these
insights to regularize internal layers of stacked shift operator models.
5	Discussion
Finding representations that are equivariant to transformations present in data is a daunting problem
with no single solution. A large body of work (Cohen et al., 2020; 2018; Esteves et al., 2018; Grey-
danus et al., 2019; Romero et al., 2020; Finzi et al., 2020; Tai et al., 2019) proposes to hard-code
equivariances in the neural architecture, which requires a priori knowledge of the transformations
present in the data. In another line of work, Falorsi et al. (2018); Davidson et al. (2018); Falorsi et al.
9
Under review as a conference paper at ICLR 2021
(2019) show that the topology of the data manifold should be preserved by the latent representation,
but these studies do not address the problem of disentanglement. Higgins et al. (2018) have proposed
the framework of disentanglement as equivariance that we build upon here. Our work extends their
original contribution in multiple ways. First, we show that traditional disentanglement introduces
topological defects (i.e. discontinuities in the encoder), even in the case of simple affine transfor-
mations. Second, we conceptually reframe disentanglement, allowing equivariant operators to act
on the entire latent space, so as to resolve these topological defects. Finally, we show that models
equipped with such operators successfully learn to disentangle simple affine transformations.
An important direction for future work will be to expand the reach of the theory to a broader family
of transformations. In particular, it is unclear how the proposed approach should be adapted to
learn transformations which are not affine or linear in image space, such as local deformations,
compositional transformations (acting on different objects present in an image), and out-of plane
rotations of objects in images (but see Dupont et al. (2020) for an empirical success using a variant
of the shift operator). Another important direction would be to extend the theory and proposed
models to continous Lie groups. Moreover, our current implementation of disentanglement relies
on some supervision, by including pairs of transformed images(with or WithoUt knowledge of the
Parameter of the transformation Occurring between them). It ,jmoggvej[ɪwould be important
to understand how disentangled representations can be learned without such pairs of transformed
images (see Anselmi et al. (2019); Zhou et al. (2020) for relevant work).
Finally, our work lays a theoretical foundation for the recent success of a new family of
methods that —instead of enforcing disentangled representations to be restricted to distinct
subspaces— use operators (hard-coded or learned) acting on the entire latent space (Connor &
Rozell, 2020; Connor et al., 2020; Dupont et al., 2020; Giannone et al., 2020; Quessard et al.,
2020) (see also (Memisevic & Hinton, 2010; Cohen & Welling, 2014; Sohl-Dickstein et al., 2017)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^J^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^J^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
for precursor methods). These methods work well where traditional disentanglement methods fail:
JJJJJJJJJJJJJJJJJJ
for instance by learning to generate full 360° in-plane rotations OfMNIST digits (Connor & Rozell,
2020), and even out-of-plane rotations of 3D objects (Dupont et al., 2020). UnIike our SimPIe
theoretically-derived models WhiCh are SUited only for affine transformations, these These methods
JJJJJJ
use distributed operators in combination with non-linear autoencoder architectures, an interesting di-
rection for future theoretical investigations. Moreover, in the case where the latent operators cannot
JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ
be determined in advance like in the affine case, these operators could be learned like in Connor et
JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ
al. A benefit of this approach is that multiple operators can be learned in the same subspace, instead
JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ
of the stacking strategy that we needed to use in the case of hard-coded shift operators.
JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJ
References
Anonymous. Quantifying and learning disentangled representations with limited supervision. In
Submitted to International Conference on Learning Representations, 2021. URL https://
openreview.net/forum?id=YZ-NHPj6c6O. under review.
Fabio Anselmi, Georgios Evangelopoulos, Lorenzo Rosasco, and Tomaso Poggio. Symmetry-
adapted representation learning. Pattern Recognition, 86:201-208, February 2019. ISSN 0031-
3203. doi: 10.1016/j.patcog.2018.07.025. URL http://www.sciencedirect.com/
science/article/pii/S0031320318302620.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013. Publisher: IEEE.
R. Berndt. Representations of Linear Groups: An Introduction Based on Examples from Physics
and Number Theory. Vieweg+Teubner Verlag, 2007. ISBN 9783834894014. URL https:
//books.google.fr/books?id=tIGl77fdspAC.
Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in $\beta$-VAE. arXiv:1804.03599 [cs,
stat], April 2018. URL http://arxiv.org/abs/1804.03599. arXiv: 1804.03599.
Hugo Caselles-Dupre, Michael Garcia Ortiz, and David Filliat. Symmetry-Based Disentangled
Representation Learning requires Interaction with Environments. In H. Wallach, H. Larochelle,
10
Under review as a conference paper at ICLR 2021
A.	Beygelzimer, F. d\textquotesingle Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32, pp. 4606-4615. Curran Associates, Inc., 2019.
Taco Cohen and Max Welling. Learning the Irreducible Representations of Commutative Lie
Groups. arXiv:1402.4437 [cs], May 2014. URL http://arxiv.org/abs/1402.4437.
arXiv: 1402.4437.
Taco Cohen, Mario Geiger, and Maurice Weiler. A General Theory of Equivariant CNNs on Ho-
mogeneous Spaces. arXiv:1811.02017 [cs, stat], January 2020. URL http://arxiv.org/
abs/1811.02017. arXiv: 1811.02017.
Taco S. Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical cnns. arXiv preprint
arXiv:1801.10130, 2018.
Marissa Connor and Christopher Rozell. Representing Closed Transformation Paths in Encoded
Network Latent Space. In AAAI, pp. 3666-3675, 2020.
Marissa C. Connor, Gregory H. Canal, and Christopher J. Rozell. Variational Autoencoder with
Learned Latent Structure. arXiv:2006.10597 [cs, stat], June 2020. URL http://arxiv.
org/abs/2006.10597. arXiv: 2006.10597.
Tim R. Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M. Tomczak. Hy-
perspherical Variational Auto-Encoders. arXiv:1804.00891 [cs, stat], September 2018. URL
http://arxiv.org/abs/1804.00891. arXiv: 1804.00891.
Emilien Dupont, Miguel Angel Bautista, Alex Colburn, Aditya Sankar, Carlos Guestrin, Josh
Susskind, and Qi Shan. Equivariant Neural Rendering. arXiv preprint arXiv:2006.07630, 2020.
Carlos Esteves, Christine Allen-Blanchette, Xiaowei Zhou, and Kostas Daniilidis. Polar Transformer
Networks. arXiv:1709.01889 [cs], February 2018. URL http://arxiv.org/abs/1709.
01889. arXiv: 1709.01889.
Pavel Etingof, Oleg Golberg, Sebastian Hensel, Tiankai Liu, Alex Schwendner, Dmitry Vaintrob,
and Elena Yudovina. Introduction to representation theory, 2009. URL http://arxiv.org/
abs/0901.0827. cite arxiv:0901.0827.
Luca Falorsi, Pim de Haan, Tim R. Davidson, Nicola De Cao, Maurice Weiler, Patrick Forre, and
Taco S. Cohen. Explorations in Homeomorphic Variational Auto-Encoding. arXiv:1807.04689
[cs, stat], July 2018. URL http://arxiv.org/abs/1807.04689. arXiv: 1807.04689.
Luca Falorsi, Pim de Haan, Tim R. Davidson, and Patrick Forre. Reparameterizing Distributions
on Lie Groups. arXiv:1903.02958 [cs, math, stat], March 2019. URL http://arxiv.org/
abs/1903.02958. arXiv: 1903.02958.
M. Finzi, S. Stanton, P. Izmailov, and A. G. Wilson. Generalizing Convolutional Networks for
Equivariance to Lie Groups on Arbitrary Continuous Data. In Proceedings of the International
Conference on Machine Vision and Machine Learning, 2020.
Giorgio Giannone, Saeed Saremi, Jonathan Masci, and Christian Osendorfer. No Representation
without Transformation. arXiv:1912.03845 [cs, stat], April 2020. URL http://arxiv.org/
abs/1912.03845. arXiv: 1912.03845.
Samuel Greydanus, Misko Dzamba, and Jason Yosinski. Hamiltonian neural networks. In Advances
in Neural Information Processing Systems, pp. 15379-15389, 2019.
S0ren Hauberg. Only Bayes should learn a manifold (on the estimation of differential geometric
structure from data). arXiv:1806.04994 [cs, stat], September 2019. URL http://arxiv.
org/abs/1806.04994. arXiv: 1806.04994.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning Basic Visual Concepts with a
Constrained Variational Framework. November 2016. URL https://openreview.net/
forum?id=Sy2fzU9gl.
11
Under review as a conference paper at ICLR 2021
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a Definition of Disentangled Representations. arXiv:1812.02230
[cs, stat], December 2018. URL http://arxiv.org/abs/1812.02230. arXiv:
1812.02230.
Ilyes Khemakhem, Diederik P. Kingma, Ricardo Pio Monti, and AaPo Hyvarinen. Variational AU-
toencoders and Nonlinear ICA: A Unifying Framework. arXiv:1907.04809 [cs, stat], February
2020. URL http://arxiv.org/abs/1907.04809. arXiv: 1907.04809.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P. Kingma and Max Welling. AUto-Encoding Variational Bayes. arXiv:1312.6114 [cs,
stat], May 2014. URL http://arxiv.org/abs/1312.6114. arXiv: 1312.6114.
Brenden M. Lake, Tomer D. Ullman, JoshUa B. TenenbaUm, and SamUel J. Gershman. BUilding
machines that learn and think like PeoPle. Behavioral and brain sciences, 40, 2017. PUblisher:
Cambridge University Press.
Yann LeCUn, Corinna Cortes, and CJ BUrges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
Francesco Locatello, Gabriele Abbati, Thomas Rainforth, Stefan Bauer, Bernhard Scholkopf, and
Olivier Bachem. On the fairness of disentangled rePresentations. In Advances in Neural Informa-
tion Processing Systems, pp. 14611-14624, 2019a.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging Common Assumptions in the Unsupervised Learn-
ing of Disentangled Representations. arXiv:1811.12359 [cs, stat], June 2019b. URL http:
//arxiv.org/abs/1811.12359. arXiv: 1811.12359.
Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael
Tschannen. Weakly-Supervised Disentanglement Without Compromises. arXiv:2002.02886 [cs,
stat], June 2020. URL http://arxiv.org/abs/2002.02886. arXiv: 2002.02886.
Roland Memisevic and Geoffrey E. Hinton. Learning to Represent Spatial Transformations with
Factored Higher-Order Boltzmann Machines. Neural Computation, 22(6):1473-1492, June
2010. ISSN 0899-7667, 1530-888X. doi: 10.1162/neco.2010.01-09-953. URL https:
//www.mitpressjournals.org/doi/abs/10.1162/neco.2010.01-09-953.
James Raymond Munkres. Topology. Pearson, Harlow, 2. ed edition, 2014. ISBN 978-1-292-02362-
5. OCLC: 904316202.
Emmy Noether. The finiteness theorem for invariants of a finite group (translation of Emmy
Noether’s ”Der Endlichkeitsatz der Invarianten endlicher Gruppen” by c. mclarty in 2015).
arXiv:1503.07849 [math], March 1915. URL http://arxiv.org/abs/1503.07849.
arXiv: 1503.07849.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d AlcheBuc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran
Associates, Inc., 2019.
Robin Quessard, Thomas D. Barrett, and William R. Clements. Learning Group Structure and
Disentangled Representations of Dynamical Environments. arXiv preprint arXiv:2002.06991,
2020.
B.	S. Reddy and B. N. Chatterji. An fft-based technique for translation, rotation, and scale-invariant
image registration. IEEE Transactions on Image Processing, 5(8):1266-1271, 1996.
12
Under review as a conference paper at ICLR 2021
David W. Romero, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Attentive Group
Equivariant Convolutional Networks. arXiv preprint arXiv:2002.03830, 2020.
L.L. Scott and J.P. Serre. Linear Representations of Finite Groups. Graduate Texts in Mathematics.
Springer New York, 1996. ISBN 9780387901909. URL https://books.google.fr/
books?id=NCfZgr54TJ4C.
Rui Shu, Yining Chen, Abhishek Kumar, Stefano Ermon, and Ben Poole. Weakly Supervised
Disentanglement with Guarantees. arXiv:1910.09772 [cs, stat], April 2020. URL http:
//arxiv.org/abs/1910.09772. arXiv: 1910.09772.
Jascha Sohl-Dickstein, Ching Ming Wang, and Bruno A. Olshausen. An Unsupervised Algorithm
For Learning Lie Group Transformations. arXiv:1001.1027 [cs], June 2017. URL http://
arxiv.org/abs/1001.1027. arXiv: 1001.1027.
Kai Sheng Tai, Peter Bailis, and Gregory Valiant. Equivariant Transformer Networks. In Interna-
tional Conference on Machine Learning, 2019.
Stefan Van Der Walt, S Chris Colbert, and Gael Varoquaux. The numpy array: a structure for
efficient numerical computation. Computing in Science & Engineering, 13(2):22, 2011.
Stefan van der Walt, Johannes L. Schonberger, Juan Nunez-Iglesias, Francois Boulogne, Joshua D.
Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-
image: image processing in Python. PeerJ, 2:e453, 6 2014. ISSN 2167-8359. doi: 10.7717/peerj.
453. URL https://doi.org/10.7717/peerj.453.
Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization.
arXiv preprint arXiv:2007.02933, 2020.
13
Under review as a conference paper at ICLR 2021
Appendix
Table of Contents
A	Prerequisites in Group Theory and Representation Theory	14
B	Experimental Details	15
B.1	Dataset generation ........................................................ 15
B.2	Model architectures and training .......................................... 16
B.3	Weakly supervised shift operator training procedure ....................... 17
B.4	Hyper-parameters .......................................................... 18
C	Reframing Disentanglement: Formal Proofs	18
C.1 Topological proof against disentanglement ................................... 18
C.2 Topological defects arise in the neighborhood of symmetric images ........... 19
C.3 Character Theory of the disentanglement of finite discrete linear transformations 21
D The Case of Multiple Transformations: Formal Derivations	23
D.1 Translations in both axes as a direct product .............................. 23
D.2 Insights on the stacked shift operators model for the 2D translation group . 24
D.3 Semi-direct product of two groups .......................................... 25
D.4 Representations of the (discrete finite) Special Euclidean group ........... 26
D.5 Insights on the stacked shift operators model for the discrete finite Special Eu-
clidean group ............................................................. 31
D.6	Details of Section D.4 .................................................... 35
E Additional Results	36
E.1	Quantifications ........................................................... 36
E.2	Additional analyses ....................................................... 36
A	Prerequisites in Group Theory and Representation Theory
Definition of a group A group is a set G together with an operation ◦ : G × G → G such that it
respects the following axioms:
•	Associativity: (gk ◦ gk0) ◦ gk00 = gk ◦ (gk0 ◦ gk00) with gk, gk0, gk00 ∈ G.
•	Identity element: there exists an element eG ∈ G such that, for every gk ∈ G,
gk ◦ eG = eG ◦ gk = eGgk2gG二eg£9kj=JgS, and eG is unique.
•	Inverse element: for each gk ∈ G, there exists an element gk0 ∈ G, denoted gk-1 such that
gk ◦ gk-1 = gk-1 ◦ gk = eG.
In the paper, for clarity we do not write explicitly the operation ◦ unless needed.
Finite cyclic groups We will be interested in finite groups, composed of a finite number of
elements (i.e. the order of G).
A cyclic group is a special type of group that is generated by a single element, called the
generator g0, such that each element can be obtained by repeatedly applying the group operation ◦
to g0 or its inverse. Every element of a cyclic group can thus be written as gk = g0k. Note that every
cyclic group is abelian (i.e. its elements commute).
A group G that is both finite and cyclic has a finite order K such that g0K = eG .
14
Under review as a conference paper at ICLR 2021
Representation and equivariance Informally, for a model to be equivariant to a group of
transformations means that if we encode a sample, and transform the code, we get the same result as
encoding the transformed sample. (Higgins et al., 2018) show that disentanglement can be viewed
as a special case of equivariance, where the transformation of the code is restricted to a subspace.
We provide a formal definition of equivariance below after introducing notations.
In the framework of group theory, we consider φ a linear representation of the group G
acting on X (Scott & Serre, 1996): φ : G → GL(X). Each element of the group gk ∈ G is
represented by a matrix φ(gk) = φk, and φk is a matrix with specific properties:
1.	φ is a homomorphism: φ(gk gk0) = φ(gk)φ(gk0), gk, gk0 ∈ G.
2.	φk is invertible and φk-1 = φ(gk-1) as φ(gk-1)φ(gk) = φ(gkgk-1) = φ(eG) = I. where I is
the identity matrix in GL(X).
The set of matrices φk form a linear representation of G and they multiply with the vectors in X as
follows:
φk : X(= RN) → X s.t. ∀x ∈ X, φk(x) ∈ X.	(6)
The character of φ is the function χφ such that for each gk ∈ G, it returns the trace of φ(gk) = φk,
i.e. χφ(gk) = Tr(φk). Importantly, the character of a representation completely characterizes
the representation up to a linear isomorphism (i.e. change of basis) (Scott & Serre, 1996). The
character table of a representation is composed of the values of the character evaluated at each
element of the group.
Similarly, we denote a linear representation of the action of G onto the latent space Z by
ψ : G → GL(Z) such that ∀k, ψ(gk) = ψk. While corresponding to the action of the same group
element gk ∈ G, ψk does not have to be the same as φk, as it represents the action of G on Z and
not on X.
ψk : Z(= CN) → Z s.t. ∀z ∈ Z, ψk(z) ∈ Z.	(7)
Note that we consider a complex latent space. With these notions, formally the model f : X → Z is
equivariant to the group of transformations G that acts on the data if, for all elements of the group
gk and its action φk and ψk on the spaces X and Z respectively, we have:
∀x ∈ X, ∀k ∈ K, f (φk (x)) = ψk (f (x))	(8)
Finally, a group action on image space RN is said to be affine if it affects the image through an
affine change of cooordinates.
B	Experimental Details
B.1	Dataset generation
Simple shapes We construct a dataset of randomly generated simple shapes, consisting of 5 ran-
domly chosen points connected by straight lines with 28x28 pixels. We normalize pixel values to
ensure they lie within [0, 1]. For all experiments, we use a dataset with 2000 shapes. For each shape,
We apply 10 counterclockwise rotations by {0。, 36。, 72。，...，324。} using Scikit-image's rotation
functionality (see van der Walt et al. (2014)). For translations along the x-axis or y-axis we apply
10 translations using numpy.roll (see Van Der Walt et al. (2011)). This ensures periodic boundary
conditions such that once a pixels is shifted beyond one edge of the frame, it reappears on the other
edge. For experiments with supervision involving pairs, we construct every possible combination
of pairs, x1, x2 and apply every transformation to both x1 and x2. For datasets containing multiple
transformations, we first rotate, then translate along the x-axis, then translate along the y-axis. We
use 50% train-test split then further split the training set into 20%-80% for validation and training.
MNIST (LeCun et al., 2010) Similar to simple shapes, we construct rotated and translated ver-
sions of MNIST by applying the same transformations to the original MNIST digits. We normalize
15
Under review as a conference paper at ICLR 2021
pixel values to lie between [0, 1]. For supervised experiments, we similarly construct every com-
bination of pairs x1 , x2 and apply transformations to both x1 and x2 . Since constructing every
combination of transformed pairs would lead to many multiples the size of the original MNIST, we
randomly sample from the training set to match the original number of samples. We use the full test
set augmented with transformations for reporting losses.
B.2	Model architectures and training
We implement all models using PyTorch with the Adam optimizer (Paszke et al., 2019; Kingma &
Ba, 2014).
Variational Autoencoder and Variants We implement existing state-of-the-art disentanglement
methods β-VAE and CCI-VAE, which aim to learn factorized representations corresponding to fac-
tors of variation in the data. In our case the factors of variation are the image content and the
transformation used (rotation or translations). We use the loss from CCI-VAE made up of a recon-
StrUction mean squared error and a KUllback-Leibler divergence scaled by β:
m
L = X(Xi- fD(f (xι))2∕m + β∣KL(q(z∣x),p(z)) - C|	(9)
where m is the number of samples and Kullback-Leibler divergence is estimated as
KL(q(z∣x),p(z) = -0.5 * Pd(I + ln(σ2) - μ2 - σi2), where d is the latent dimension (see
Kingma & Welling (2014)). For a standard VAE, we use C = 0 and β = 1.0. For β-VAE, we sweep
over choices of β with C = 0. For CCI-VAE, we sweep over choices of β and linearly increase C
throughout training from C = 0 to C = 36.0.
We use the encoder/decoder architectures from Burgess et al. (2018) comprised of 4 convolutional
layers, each with 28 channels, 4x4 kernels, and a stride of 2. This is followed by 2 fully 256-unit
connected layers. We apply ReLU activation after each layer. The latent distribution is generated
from 30 units: 15 units for the mean and 15 units for the log-variance of a Gaussian distribution.
The decoder is comprised of the same transposed architecture as the encoder with a final sigmoid
activation.
Autoencoder with latent operators For the standard autoencoder and autoencoders with the
shift/disentangled latent operators, we use supervised training with pairs (x1, x2) and a transfor-
mation parameter k corresponding to the transformation between x1 and x2. The loss is the sum of
reconstruction losses for x1 and x2 :
mm
L = X(x1,i - fD(f(x1,i))2∕m + X(x2,i - fD(ψk(f(x1,i))))2∕m	(10)
where m is the number of samples and ψ is the disentangled or shift latent operator. For a standard
autoencoder, only the first reconstruction term is present in the loss function.
For the non-linear autoencoders, we use the same architecture above based on CCI-VAE. In the
linear case, we use a single fully 28x28 connected layer and an 800 dimensional latent space. We
use 800 dimensions to approximate the number of pixels (28x28) and ensure that K=10 divides
the latent dimension. This is an approximation of the correct theoretical operator (which should be
invertible) that works well in practice.
Weakly supervised shift operator model We use linear encoders and decoders with a 784 di-
mensional latent space to match the number of pixels. Indeed the weakly supervised shift operator
uses the complex version of the shift operator, so we can perfectly match the size of the image.
Training is done with L2 loss on all possible reconstructions of x1 (of each training pair i), weighted
by scores αi,k. Appendix B.3 below gives a detailed explanation of the computation of the scores
αi,k:
m	m KL
L = X(x1,i - fD(f(x1,i))2∕m + X X αi,k(x2,i - fD(ψk(f(x1,i,k))))2∕m	(11)
i=1	i=1 k=1
16
Under review as a conference paper at ICLR 2021
where m is the number of samples. At test-time, we use the transformation with maximum score
αi,k.
Stacked shift operator model We use linear encoders and decoders with a 784 dimensional latent
space to match pixel size, as we use in the stacked model the complex version of the shift operator.
Intermediate layers Li are invertible linear layers of size 784 as well. Training is done with L2 loss
on reconstructed samples as in the autoencoder with shift latent operator (see Equation 10.)
B.3	Weakly supervised shift operator training procedure
Method We experimentally show in Section 4.2 that the proposed operator ψk works well
in practice. Additionally, we developed a method for inferring the parameter of the trans-
formation k that needs to act on the sample. We encode samples by pairs x1 , x2 (with x2 a
transformed version of x1) into z1 and z2 respectively, and use a classical phase correlation
technique (Reddy & Chatterji, 1996) to identify the shift between z1 and z2, described below. Then,
we use the complex diagonal shift operator parametrized by the inferred transformation parameter k.
Importantly, in the weakly supervised version of the shift operator, the model has an extra
free parameter which is the number of latent transformations, not known a priori. Let us denote KL
this number, which can be different than the ground-truth order of the group K.
To infer the latent transformation that appear between z1 and z2, we compute the cross-
power spectrum between the two codes z1 and z2, that are both complex vectors of size N and
obtain a complex vector of size N . We repeat KL times this vector, obtaining a KL x N matrix,
of which we compute the inverse Fourier transform. The resulting matrix should have rows that
are approximately 0, except at the row k corresponding to the shift between the two images, see
Reddy & Chatterji (1996). Thus, we compute the mean of the frequencies of the real part of the
inverse Fourier result (i.e. the mean over the N values in the second dimension). This gives us a
KL-dimensional vector, which we use as a vector of scores of each k to be the correct shift between
z1 and z2 . During training, we compute the soft-max of these scores with a temperature parameter
k, this gives us KL weights αk. We transform z1 with all K possible shift operators, decode into
K reconstructions xk, and weight the mean square error between x2 and each xk by αk before
back-propagating. This results, for each samples pair (x1,i , x2,i), in the loss:
m	m KL
L = X(x1,i - fD(f(x1,i))2/m + X X αi,k(x2,i - fD(ψk(f (xι,i,k))))2∕m	(12)
i=1	i=1 k=1
where m is the number of samples and αi,k the scores for the pair (x1,i, x2,i). At test-time, we use
the transformation with maximum score αi,k.
Dataset	KL	
	10	21
Shapes (10,0,0)	0.001 ± 0.0013	0.0005 ± 0.0001
Shapes (0,10,0)	0.0097 ± 0.0052	0.0038 ± 0.0013
Shapes (0,0,10)	0.0115 ± 0.0049	0.005 ± 0.0033
MNIST (10,0,0)	0.0035 ± 0.0039	0.0074 ± 0.0083
Table 1: Comparing test mean square error (MSE) ± standard deviation of the mean over random
seeds for different KL. Numbers in () refer to the number of rotations, the number of translations
on the x-axis, and the number of translations on the y-axis respectively.
Effect of the number of latent transformations In the weakly supervised shift operator model,
the number of latent transformations KL is a free parameter. Interestingly, when using rotations
the best cross-validated number of transformations is 10, which matches the ground-truth order of
the group. For translations (either on the x or the y axis), best results are obtained using KL = 21
which is larger than the ground-truth order of the group K . Table 1 compares test MSE for both
values of KL . We think that in the case of translations, changes in the image induced by each
shift (each transformation) are less visually striking than with rotations, and a larger KL gives extra
17
Under review as a conference paper at ICLR 2021
flexibility to the model to identify the group elements and respects the group structure (namely its
cyclic aspect).
B.4 Hyper-parameters
General hyper-parameters We sweep across several sets of hyper-parameters for our experi-
ments. We report results for the model with the lowest validation test loss. To avoid over-fitting,
results for any given model are also stored during training for the parameters yielding the lowest
validation loss.
For experiments with simple shapes we sweep across combinations of
•	5 seeds: 0, 10, 20, 30, 40
•	4 batch sizes: 4,8,16,32
•	2 learning rates: 0.0005, 0.001
For MNIST, we sweep across combinations of
•	5 seeds: 0, 10, 20, 30, 40
•	4 batch sizes: 8,16,32, 64
•	2 learning rates: 0.0005, 0.001
In addition to these general parameters, we also sweep across choices of β, {4, 10, 100, 1000} and
latent dimension {10, 30} for the variational autoencoder models and variants. We repeat all exper-
iments across four seeds used to initialize random number generation and weight initialization.
Weakly supervised shift operator hyper-parameters We perform a sweep over hyper-
parameters as described above. Additionally for the weakly supervised model, we sweep over
temperature τ of the soft-max that shapes the scores of each transformation over values τ =
{0.01, 0.1, 1.0}, and the number of transformations composing the operator family (i.e. the order of
the group) over values 10 and 21, where 10 is the ground-truth order of the group.
Stacked shift operator model hyper-parameters We perform a sweep over hyper-parameters as
described above. The only exception is that for the case of the Special Euclidean group, we train
only for 5 epochs, and try batch sizes {32, 64} for MNIST and {16, 32} for simple shapes, as the
number of generated samples is high. Similarly, for the case of 2D translations, we use batch sizes
{16, 32} for simple shapes.
C Reframing Disentanglement: Formal Proofs
We consider the action of a finite group on image space RN . Using tools from topology, we show
that it is impossible to learn a representation which disentangles the action of this group with a
continuous encoder f.
C.1 Topological proof against disentanglement
We consider a finite group G of cardinal |G| that acts on RN. Given an image x ∈ RN, an orbit
containing that image is given by {g1x, g2x, ..., gK x}.
We consider an encoder f : RN → M that disentangles the group action. The image of f is
composed of an equivariant subspace and an invariant subspace. We define fE : RN → ME the
projection of f on its equivariant subspace and fI : RN → MI the projection of f on its invariant
subspace.
fE is equivariant to the group action. In equations:
∀x, ∀g, fE (gx) = gfE(x)	(13)
18
Under review as a conference paper at ICLR 2021
For the disentanglement to be complete, fE should not contain any information about the identity
of the orbit the image belongs to. If O1 and O2 are two distinct orbits, we thus have:
∀x1 ∈ O1,∀x2 ∈ O2,∃g ∈ G,fE(x1) =gfE(x2)	(14)
fI is invariant to the group action:
∀x, ∀g, fI (gx) = fI (x)	(15)
We also assume that the representation contains all the information needed to reconstruct the input
image:
∀x1 ∈O1,∀x2 ∈ O2,fI(x1) 6=fI(x2)	(16)
This last assumption corresponds to assuming that every image can be perfectly identified from its
latent representation by a decoder (i.e. perfect autoencoder). We also assume that both the encoder
and decoder are continuous functions. This is a reasonable assumption as most deep network ar-
chitectures are differentiable and thus continuous. In the language of topology, the encoder f is a
homeomorphism, a continuous invertible function whose inverse is also a continuous function. Also
called a topological isomorphism, an homeomorphism is a function that preserves all topological
properties of its input space (see Munkres (2014) p.105). Here we prove that f cannot preserve the
topology of RN while disentangling the group action of G.
Consider fE|O the restriction of fE to a single orbit of an image x without any particular symmetry.
fE|O inherits continuity from f, and it can easily be shown that fE|O is invertible (otherwise, infor-
mation about the transformation on this orbit is irremediably lost, and f can thus not be invertible).
fE |O is thus also a homeomorphism, and so it preserves the topology of the orbit O in image space,
which is a set of |G| disconnected points.
By equation 14, we know that restrictions of fE to all other orbits have an image contained in the
same topological space. As a consequence, the image of fE itself is a set of |G| disconnected points.
Since fE is a projection of f, the image of f should at least be composed of |G| disconnected
parts (this follows from the fact that the projection of a connected space cannot be disconnected).
However, this is impossible because the domain of f is RN, which is connected, and f is an home-
omorphism, thus preserving the connectedness of RN.
In summary, we have shown that, for topological reasons, a continuous invertible encoder cannot
possibly disentangle the action of a finite group acting on image space RN. In the next section, we
show that topological defects arise in the neighborhood of all images presenting a symmetry with
respect to the transformation.
C.2 Topological defects arise in the neighborhood of symmetric images
We proved in the previous section that it is impossible to map a finite group acting on RN to a disen-
tangled representation with a continuous invertible encoder. In this section, in order to gain intuition
of why disentanglement is impossible, we show that topological defects appear in the neighborhood
of images that present a symmetry with respect to the group action.
C.2.1	fE IS NOT CONTINUOUS ABOUT SYMMETRIC IMAGES: FORMAL PROOF
Let’s consider an image xs that presents a symmetry with respect to the group action:
∃g ∈ G, g 6= eG, gxs = xs	(17)
Let’s further assume that an infinitesimal perturbation to this image along a direction u breaks the
symmetry of the image:
∀0 < < E, x0 := xs + u, gx0 6= x0	(18)
Since fE preserves the information about the transformation,
|fE(gx0)-fE(x0)| >C 6=0	(19)
19
Under review as a conference paper at ICLR 2021
where C is the smallest distance between two disconnected points ofME. We assume that the group
action is continuous:
gx0 = gxs + O() = xs + O()	(20)
We can rewrite equation 19 as:
|fE(xs+O()) -fE(xs+O())| > C 6=0	(21)
which is in contradiction with the continuity hypothesis on the encoder. We have thus shown that
the equivariant part of the encoder fE presents some discontinuities around all images that present
a symmetry. Note that for both rotations and translations, the uniform image is an example of
symmetric image with respect to these transformations.
C.2.2 fI IS NOT DIFFERENTIABLE ABOUT SYMMETRIC IMAGES: VISUAL PROOF
As an example, we consider an equilateral triangle which is either perturbed at its top corner, left
corner, or both corners (Fig. 4). When perturbed on either one of its corner, the perturbation moves
the image to the same orbit, because the triangle perturbed on its right corner is a rotated version of
the triangle perturbed on its top corner. The gradient of fI along these two directions at the equilat-
eral triangle image should thus be the same (so as not to leak information about the transformation
in the invariant subspace ZI). The simultaneous perturbation along the two corners moves the image
to a different orbit, so the gradient of fI along this direction should not be aligned with the previous
gradients (so as to preserve all information about identity of the orbit). And yet, if the function fI
was differentiable everywhere, this gradient should be a linear combination of the former gradients,
and thus all three gradients should be collinear. The function fI can thus not be differentiable ev-
erywhere. This imperative is incompatible with many deep learning frameworks, where the encoder
is implemented by a neural network that is differentiable everywhere (with a notable exception for
networks equipped with relu nonlinearities which are differentiable almost everywhere).
Figure 4: Visual proof that the invariant part of the encoder fI cannot be differentiable about sym-
metric figures. We assume fI is differentiable and show a contradiction. We consider an equilateral
triangle which is perturbed at its top corner, left corner, or both corners. When perturbed either one
of its corner, the perturbation brings the image to the same orbit, because of the symmetry. In latent
space, the perturbation should thus move the latent representation in the same direction. The pertur-
bation along the two corners simultaneously brings to image to a different orbit, and yet, since the
perturbation is a simple linear combination of the single-corner perturbations, it can only be colinear
to these perturbations. This collinearity leads to the encoder not being injective, and thus loosing
information about the identity of the image.
C.2.3 fI IS NOT DIFFERENTIABLE ABOUT SYMMETRIC IMAGES: FORMAL PROOF
Next, we show that, if we add an extra assumption on the encoder, namely that it is differentiable
everywhere, it is also impossible to achieve the invariant part of the encoder fI . Note that this extra
assumption if true for networks equipped with differentiable non-linearities, such as tanh or sigmoid,
but not for networks equipped with relus.
20
Under review as a conference paper at ICLR 2021
Let’s consider an image xs presenting a symmetry w.r.t the group action. We consider perturbations
of that image along two distinct directions u and gu.
By symmetry, it is easy to see that:
f
∂u
xs
∂fι
∂gu
xs
(22)
As a consequence, a perturbation by U = (u+gu), is equal to the perturbation along one or the other
direction:
f
∂u0
_ 1	∂fι
*
X 2 Idu
xs
X + f
xs
ʌ = f
x ) du
Xs
(23)
xs
In the general case, x0 = xs + u0 does not belong to the same orbit as x = xs + u. fI is thus losing
information about which orbit the perturbed image x0 belongs to, which is in contradiction with the
assumption shown in Equation 16.
C.3 Character Theory of the disentanglement of finite discrete linear
TRANSFORMATIONS
C.3. 1 The Shift Operator
Datasets are structured by group transformations that act on the data samples. Our goal is for
the model to reflect that structure. Specifically, we want our model to be equivariant to the
transformations that act on the data. We defined group equivariance in Section A.
We show here that a carefully chosen distributed operator in latent space, —the shift opera-
tor— is linearly isomorphic to all cyclic linear transformations of finite order K of images (i.e. the
group G of transformations is cyclic and finite and it acts linearly on image space). The practical
consequences of this fact is that it is possible to learn an equivariant mapping to every affine
transformation using this operator, using linear invertible encoder and decoder architectures.
Consider a linear encoder model f = W . If we want W to be an equivariant invertible
linear mapping W between X and Z, Equation 8 rewrites as follows:
W φk (x) = ψk (W x) ∀x ∈ X(= RN), ∀k ∈ K
(24)
where φk and ψk are the representations of gk ∈ G on the image and latent space respectively, as
defined in A.
For W to be equivariant, Equation 24 must be true for every image x. As W is invertible,
Equation 24 is true if and only if, ∀k ∈ K, the two representations ψk and φk are isomorphic:
∀k ∈ K, φk = W -1 ψk W
(25)
We consider additional properties on φ corresponding to the assumptions (i) that G is cyclic of order
K with generator g0 and (ii) φ is isomorphic to the regular representation of G (see Scott & Serre
(1996)):
1.	φ is cyclic, i.e. such that φk = φ0k where φ0 = φ(g0) and thus φ0K = I.
2.	The character of φ is s.t. χφ (e) = N and χφ (gk ) = 0 for gk 6= e.
The second property might seem counter-intuitive, but it just means that the transformation leaves
no pixel unchanged (i.e. permutes all the pixel). In the case of rotations, this is approximately true
since only the rotation origin remains in place. The character table of φ, given the second property,
is
χφ
g0
0
-1
0
e
N
0
0
21
Under review as a conference paper at ICLR 2021
We have just seen that if the encoder and decoder are linear and invertible, the two representations
φ and ψ must be isomorphic. Two representations are isomorphic if and only if they have the same
character (Scott & Serre, 1996, Theorem 4, Corollary 2). We thus want to choose ψ such that it
preserves the character of the representation φ corresponding to the action of G on the dataset of
images. Importantly, we will see that our proposed operator needs to be distributed in the sense that
it should act on the full latent code.
Let us consider the matrix of order K = |G| that corresponds to a shift of elements in a K-
dimensional vector by k positions. It is the permutation corresponding to a shift of 1, exponentiated
by k. We construct from Mk the shift operator as a representation of the group’s action on the latent
space. For each gk ∈ G such that gk = g0k, its corresponding shift operator is the block diagonal
matrix of order N composed of K repetition of Mk.
(26)	ψk :=	Mk ...
M k
(27)
Let us compute the character table of this representation. First, for the identity, is it trivial to see that
χψ(e) = N. Second, for any gk 6= e, we have
χψ(gk) = Tr(ψk) = 0
(28)
since all diagonal elements of gk will be 0. Therefore, the character table of the shift operator is the
same as χφ :
χψ
e	go	go	...	gK-1
N	0	0	0	0
Using this shift operator ensures that an equivariant invertible linear mapping W exists between
image space and the latent space equipped with the shift operator. Note that the character of the
disentangled operator does not match the character table of φ, and so we verify once again in
this linear autoencoder setting that the disentangled operator is unfit to be equivariant to affine
transformations of images.
When K does not divide N, we use a latent code dimension that is slightly different than N
but divisible by K . This is an approximation of the correct theoretical operator and we verify that it
works well in practice.
In the next Section C.3.2, we show that we can also replace this shift operator by a complex diagonal
operator, which is more computationally efficient to multiply with the latent.
C.3.2 Complex Diagonal Shift Operator
In order to optimise computational time, we can also consider for Mk the following diagonal com-
plex matrix:
k
0
1
0
0
ω
Mk :
(29)
0
22
Under review as a conference paper at ICLR 2021
2iπ
With ω = eɪ. The shift operator in this case is a diagonal matrix, as follows:
ψk,N :
ωK-1
(30)
ωK-1
Let us compute the character table of this representation. First, for the identity, is it trivial to see that
χψ(e) = N. Second, for any gk 6= e, we have
N-1
χψ(gk) = Tr(ψk) = X(ωk)n
n=0
(31)
as ωk = e KN = 1 since we're assuming N can be divided by K. Again, the character table
of the shift operator is the same as χφ . Using this operator fastens computation since it requires
only multiplying the diagonal values (a vector of size N) with the latent code (a vector of size
N as well) instead of doing a matrix multiplication. Note that when using this complex version
of the shift operator, the encoding and decoding layers of the autoencoder should be complex as well.
When K does not divide N, we still use a latent code of size N and the operator ψk is a
diagonal matrix of order N, but the last cycle 1, ω, . . . is unfinished (does not go until ωK-1). The
character of this representation is no longer equal to 0 for non-identity elements gk 6= e, but equals
a small value << N and this approximation works well in practice.
D The Case of Multiple Transformations: Formal Derivations
D. 1 Translations in both axes as a direct product
To cover the case of 2D translations (acting on both x and y axes of the image), we consider
an abelian group G that is the direct product of two subgroups G = Ax × Ay . Both Ax and
Ay are normal in G because every subgroup of an abelian group is normal. Moreover, we
consider that Ax and Ay are both cyclic of order K and K0 respectively, which is the case for
integer translation ofan image using periodic boundary condition. We denote ax,0 and ay,0 the gen-
erators ofAx and Ay, and write each translation as a = (ax,k, ay,k0) with ax,k = akx,0, ay,k0 = ayk,00.
We show that the shift operator can handle this case, with differences. We assume for sim-
plicity that both K and K0 divide N, and KK0 divides N. Following Scott & Serre (1996), we
write group elements g ∈ G as g(k,k0) = (ax,k, ay,k0). The order of the group is K = KK0. If we
consider the regular representation of 2D translations over the image space, as in Section C.3, its
character table is
χφ
e	g(0,i)	g(0 ⑵
N 0	0
...	g(K-1,K0-1)
0	0
We consider two representations over the latent space: ψx : Ax → GL(CK ) is a linear represen-
tation of Ax and ψy : Ay → GL(CK0) a linear representation of Ay, each of the matrix form
described in Section C.3.2. Group theory (Scott & Serre, 1996) tells us that ψ is the tensor product
ofψx and ψy, i.e. for g = (k, k0) ∈ G:
ψk = φ(g) = φ((aχ,k, ay,k0)) = (ψx 0 ψy )(aχ,k ,ay,k0) = ψx(aχ,k )於 Ψy (ay,ko)	(32)
Let us consider the two shift operators:
ψx,k := Diag(1, ω1, ω12..., ω1K-1)k	(33)
ψy,k0 := Diag(1, ω2, ω22, . . .ω2K0-1)k0	(34)
23
Under review as a conference paper at ICLR 2021
where ωι = e2iπ and ω2 = eK0. Then the tensor product ψχ,k 0 ψy,ko writes as a diagonal matrix
of order KK0 :
2k0
ω2
ψx,k0ψy,k0 :
k0(K0-1)
ω2
k
ω1
(35)
k(K-1) k0(K0-1)
ω1	ω2
The character of ψχ,k 0 ψy,ko is
If k 6= 0, PnK=-01 ω1kn
K0-1
χψχ,k 3ψy,k = E ωnk0
n=0
K-1
× X ω1nk
n=0
(36)
1-ωkK
1-ωk
0 since ω1kK = e2iπk
ω2	if k0	6=	0).	Hence, for	(k,	k0)	6=	(0, 0),	PnK=0-0 1 ω2nk0	×
(k, k0) = (0,0), χψx,fc0ψy k = KK0. Thus, the character table of ψχ 0 ψy is
= 1 (and similarly for
PnK=-01 ω1nk = 0. For
χψ
e
KK0
g(0,1)	g(0,2)
g(K-1,K0-1)
0
0
0
0
We will use for 2D translations a diagonal operator that is the repetition of KK times ψχ,k 0 ψy,ko
(assuming KK0 divides N), denoted ψ, with is for gk,k0 = (ax,k, ay,k0)
ψk,k0,N :
Thus, the character table of ψ is
(37)
e
N
g(0,1)	g(0,2)
χψ
g(K-1,K0-1)
0
0
0
0
We see that ψ has the same character table as ψ the representation in image space, hence is a suited
operator to use for this case.
D.2 INSIGHTS ON THE STACKED SHIFT OPERATORS MODEL FOR THE 2D TRANSLATION
GROUP
For the case of2D translations where G = Ax × Ay, we first use the operator corresponding to ax,k,
the translation in x, then intersect a linear layer denoted L1 before using the operator corresponding
24
Under review as a conference paper at ICLR 2021
to ay,k0, the translation in y. When we operate on the latent code, we perform
z0 = ψy,k0,N L1ψx,k,N z	(38)
where ψx,k,N is the operator representing the translation in x. It is a matrix of order N, where ψx,k
is repeated K times.
ψx,k,N :
(39)
Similarly, the operator corresponding to ay,ko is a matrix of order N, where ψy,ko is repeated KT
times
ψy,k0,N :
(40)
We see that in order for the product ψy,k0,N L1ψx,k,N when applied to z, to match the result of
ψk,k0,N applied to z, we need a permutation matrix P that operates on a matrix of order KK0 made
of blocks of K0 times ψx,k and returns a matrix of order KK0 with:
•	1 at the first K0 rows and columns c = 1, (K + 1), . . . KK0 - K + 1
•	ω1k at rows from K0 + 1 to 2K and columns c = 2, K + 2, . . . KK0 - K + 2 etc.
•	until ω1k(K -1) at rows from KK0 - K0 to KK0 and columns c = K, 2K, KK0
And the layer L1 would be a matrix of order N made of this permutation matrix P, repeated in block
diagonal form KNKT times, such that (ψy,ko,NLιΨx,k,N) Z = Ψk,k0,N z.
D.3 Semi-direct product of two groups
D.3.1 Definition of a semi-direct product
A semi-direct product G = A o H of two groups A and H is a group such that:
•	A is normal in G.
25
Under review as a conference paper at ICLR 2021
•	There is an homomorphism f : H → Aut(A) where Aut(A) is the group of automorphism
of A. For a ∈ A, denote f (h)a by h(a). In other words, f(h) represents how H acts on A.
•	The semi-direct product G = A o H is defined to be the product A × H with multiplication
law
(a1, h1)(a2, h2) = (a1h1(a2), h1h2)	(41)
Note that this enforces that h1(a1) = h1a1h1-1.
D.3.2 Irreducible representation of a semi-direct product
In this section, we also assume A is abelian. One can derive the irreducible representations of G in
this case, as explained in Scott & Serre (1996) Section 8.2.
First, consider the irreducible characters of A, they are of degree 1 since A is abelian and
form the character group X = Hom(A, C*). We use X to match Scott & Serre (1996) notation,
but note that X does not denote image space here, but the group of characters of A. H acts on this
group by:
hχ(a) = χ(h-1ah)	(42)
Second, consider a system of representative of the orbits of H in X. Each element of this system is
a χi, i ∈ G/H. For a given χi, denote Hi the stabilizer ofχi, i.e. hχi = χi. This means
hχi (a) = χi(h-1ah) = χi(a),∀a ∈ A	(43)
Third, extend the representations of A to a representation of Gi = A o Hi by setting
χi(ah) = χi (a), h ∈ Hi, a ∈ A	(44)
The χi are also characters of degree 1 of Gi .
Fourth, consider the irreducible representations of Hi. Scott & Serre (1996) propose to use
the irreducible representation ρ of Hi . Combining with the canonical projection Gi → Hi we get
irreducible representations P of G%. Irreducible representations of Gi are obtained by taking the
tensor product Xi 0 p.
Finally, the irreducible representations of G are computed by taking the representation induced by
Xi 0 P.
Etingof et al. (2009) show that the character of the induced representation is
XIndG (χr0ρ)(a, h) = ∣H∣	X	Xr 仿⑷卜。W-%〃)	(45)
i h0 ∈H s.t.h0-1 hh0 ∈Hi
D.4 Representations of the (discrete finite) Special Euclidean group
In this section, we focus on the specific case of the semi-direct product G = A o H where A =
Ax × Ay is the group of 2D translations and H the group of rotations. Hence, A is abelian and
H is a cyclic group. We will derive the irreducible representations of this group using the method
presented in the previous section D.3.2.
D.4.1 A note on the discrete finite special Euclidean group
While the Special Euclidean group respects the structure of a semi-direct product in the continuous
transformations case, we will consider its discrete and finite version. That is, we consider a finite
number of translations and rotations. With integer valued translation, which is of interest when
working with images (considering translations of a finite number of pixels), we cannot consider all
rotations. Indeed for example rotations of 2∏ break the normal aspect of the subgroup of transla-
tions.
Proof. Take the translation element a = (1, 1) (one pixel translation in x and y respectively), and the
rotation h of angle 2∏. Consider the composition hah-1, applied to a point of coordinates i,j this
gives hah-1(i,j) = h(h-1 (i,j) + (1,1)) = (i,j) + h(1,1) = (i,j) + (0, √2) and the translation
(0, √2) is not an integer translation. Thus, A is not normal in G in this case.	□
26
Under review as a conference paper at ICLR 2021
In what follows, we consider rotations that preserve the normality of the group of integer 2D trans-
Iations of the image. Namely, these are rotations of 2∏, ∏,答 and identity and their multiples.
Nonetheless, we think the approach is insightful and approximate solutions could be found with this
method. Furthermore, to ease derivations we consider that both K ≥ 2 and K0 ≥ 2 are odd (and
thus the product KK0 is odd), such that stabilizers of the character group of A are either the entire
H or only the identity. We leave the exploration of even K and K0 for future work.
D.4.2 Finding the orbits
As we consider integer translations with periodic boundary conditions, characters of A are 2D com-
plex numbers and are function of Z2 elements of the 2D discrete and finite translation group of
the form a = (x, y ) with x = x0k and y = y0k0 . The characters of this group are evaluated as
x1	y1 0
X(χι,yι)(a) = eπ K +K
respectively.
, where x1 , y1 are of the form with x1 ∈ 1 . . . K and y1 ∈ 1 . . . K0 ,
We consider two cases:
1.	χ0,0.
2.	χ(x1,y1) with x1 6= 0 ory1 6= 0.
Note that the total number of orbits of H in X is H PXS∈χ ∣Hχ∣ where Hx is the stabilizer of χχ.
Either χx = eX is the identity element of X and |Hx| = |H| or χx 6= eX and the only stabilizer is
eH (as shown in Section D.6.1) thus |Hx| = 1. Thus, we have that:
H XXIHxI = ⅛α×lHl +(IAI-I)X 1) = 1 + JAHl	(46)
χx inX
Hence, there are l%1 orbits in case 2 (the total number of orbits minus the orbit considered in the
first case, χ(0,0)).
D.4.3 ACTION OF H ON THE CHARACTERS OFA
Elements of H acts on characters of A as:
hχ(x1,y1) (a) = χ(x1,y1)(h-1ah)	(47)
= χ(x1,y1) (h-1(a))	(48)
=gi2∏( K (cos(-θ)x-sin(-θ)y)+ K (sin(-θ)x+cos(-θ)y))	(49)
=ei2π((K Cos⑻+K Sin(-S))X+(患(-sin(-θ))+K cos(-θ))y)	(50)
=ei2π((K Cos⑻-K Sin(S))x+(K sin(θ)+K cos(θ))y)	(51)
= χ(x1 cos(θ)-y1 sin(θ),x1 sin(θ)+y1 cos(S))(a)	(52)
= χh((x1,y1))(a)	(53)
where θ is the angle of rotation of h.
D.4.4 CASE 1 χ0,0 (ORBIT OF THE ORIGIN)
A representative is χ0,0 = 1. The stabilizer group Hi is the entire H, and the irreducible
representations of H are of the form ei2π两 for n ∈ 1,..., ∣H∣ where ∣H∣ is the total number of
rotations. Thus we use the tensor product θn (0, φ) = 1 氧 ei2π 尚 as a representation of G. There
are IHI of such irreducible representations ofG, all of degree 1, since the group of rotation is abelian.
The resulting representations, corresponding to the irreducible we get from combining this
27
Under review as a conference paper at ICLR 2021
orbit and each one of the irreducible of H can be represented in matrix form:
j
0
ρ(a, h) :
1
0
(54)
0
ei2π 1HH11
where j is such that h = hj0 and h0 is the generator of the group of rotations.
D.4.5 CASE 2 X(χι,y1) (1A-1 OF THEM)
A representative can be taken to be χr = χ(x1,y1) (x, y), and x1, y1 are now fixed. Its stabilizer
group Hi is only {eH } (see (Berndt, 2007) and proof in Section D.6.1).
We now select ρ an irreducible representation of {eH }, and we select the trivial representa-
tion 1 and combining with the canonical projection step 1 will also be a representation of Gi . We
then take the tensor product Xr 0 1 as a representation of Gi = A o {e∏}. Let Us now derive the
representation of the entire group A o G = IndGW(Xr 0 1).
Induced representation I ndGG (χr 0 1) First, we need a set of representatives of the left coset of
G in G/Gk = A o eH . The left cosets are defined as (a, h)Gk = {(a, h)(ak, eH ) ∀ak ∈ A}. There
are |H| cosets (one per h), which we can denote (eA , h)Gk. We take a representative gi for each
coset Gk. We take as representative (eA , hi) = (eA, hi)(eA , eH ) ∈ (eA, h)Gk 1, so the hi are now
fixed.
The representation (ρ, W) is induced by (Xr 0 1, V ) if
|H |
W = M ρ(gi)V	(55)
i=1
with gi = (eA, hi) and ρ(gi) is described below.
For each g and each gi, there is j(i) ∈ {1, . . . |H|} and fj ∈ Gk such that we have ggi = gj(i)fj.
Indeed:
ggi = (a, h)(eA , hi)	(56)
= (a, hhi)	(57)
= (eA,hhi)((hhi)-1(a),eH )	(58)
so gj(i) = (eA, hhi). In other words, the action of g = (a, h) on an element w ∈ W permutes the
representatives:
|H |	|H |
ρ(g)w = ρ((a, h))	ρ(gi)vi =	ρ(gj(i))X((hhi)-1(a))vi	(59)
i=1	i=1
Now, we need to find for each element of G, the resulting permutation of the coset representatives.
G is generated by a ∈ A, h ∈ H: any element (a, h) ∈ G can be written as (a, eH )(eA , h). For
(a, eH ), we get gj(i) = (eA , hi) = gi. The induced representations are thus:
-Xr (h-1(a))	二 0	0	-
ρ(a, eH ) =	0	Xr(h2-1(a)) ...	0	(60)
0	...	0 Xr (h|H | (a))
1An element (a, h) is in the same coset as (eA, h) as (a, h)-1 (eA, h) = (h-1(a-1), h-1)(eA, h) =
(h-1(a-1), eH) ∈Gk
28
Under review as a conference paper at ICLR 2021
with no permutation. For (eA, h) we get gj(i) = (eA, hhi). The resulting induced representations
are:
0	0	...	χr((hhj-1(1))-1(eA))
ρ(eA, h) = 0 χr ((hhj -1 (2))	(eA))	...
0	...	Xr ((hhjT(|H|))-1(eA))	0
(61)
i.e.
0	0	...	1
ρ(eA ,	h)	=	Ph	=	0	1	...
0	...	1	0
(62)
Where the permutation matrix Ph above represents how h acts on its own group. If hhi = hj
then Ph has a 1 at the column j of its i-th row. For example, if hh|H| = h1 then j(|H|) = 1,
j-1(1) = |H| and so for the row 1, there is χr((hhH)-1(eA)) = 1 at the |H |-th column as above.
Let us denote for clarity χr,i = χr(hi-1(a)). The representations of (a, h) will be
0
0
0
χr,1	...	0
ρ(a, h) = ρ(a, eH)ρ(eA, h) =	0	χr,2	...
0	...	0
0
0	Ph
χr,∣H∣-
0	...
χr,2	...
...	χr,∣H∣
χr,1
0
(63)
For each orbit, we get one of these (of degree H).
D.4.6 Resulting representation
The resulting representation will be a block diagonal matrix of size |A||H | of the form
H-1
ei2π -∣hγ j
ρ(a, h) :=
Mr1 (a)Ph
1
(64)
The first |H | elements on the diagonal correspond to the irreducible representation of h = hj0 , this
is the shift operator we have been considering in the single transformation case. Second, each matrix
product Mr(a, h)Ph, representative r, is repeated |H| times as it is of degree |H| (see calculation of
degree in the Section D.6.2) and corresponds to:
-Xr (h-1(a))	...	0	0
Mr(a) =	0	χr(h2-1(a))	...	0
0	...	0	Xr (h∣H ∣ (a))
(65)
29
Under review as a conference paper at ICLR 2021
If We assume |A||H | divides N, We can use an operator ψa,h,N that is the repetition of ANHI times
ρ(a, h) into a matrix of order N :
ψa,h,N :
(66)
Its character table is:
χψ
(eA , eH)
N
(a1,eH)	(a2,eH)	...	(a∣A∣, h∣H∣)
0	0	0	0
and is the same character as the operator acting on image space, as shoWn With the computation of
the character of ρ(a, h) in Section D.4.7.
D.4.7 Character table
For the identity element (eA, eH), We get for ρ(a, h) the identity matrix, and its trace is |A||H|.
When h = eH, Ph has no diagonal element and PkH=0-1 ei2πIHTk = 0, hence the trace of ρ(a, h) =
0. When a 6= eA, if h 6= eH We are in the previous case. If h = eH, PH is the identity matrix and
the representation Will be
一1
1
ρ(a,eH) =
1
Mr1 (a)I
Mr1 (a)I
MR(a)I
The trace of ρ(a, eH ) is
R
T r(ρ(a, eH)) = |H| + |H| X X χr(hi-1(a))
hi∈H r=r1
(67)
(68)
as each block-diagonal matrix is repeated |H | times. If We interchange the order of summation We
get:
R
T r(ρ(a, eH)) = |H| + |H| X X χr(hi-1(a)))	(69)
r=r1 hi∈H
R
= |H| + |H| X X hi,χr(a))	(70)
r=r1 hi∈H
Where hi,χr represents the action of hi on χr. The action of every hi on χr gives all the elements in
the orbit of χr, so the double sum results in the sum over all characters χ ∈ X ofA, apart from the
orbit of χ0,0 Which contains only χ0,0 .
Tr(ρ(a, eH)) = |H| + |H| X	χ(a)	(71)
χ∈X,χ6=X0,0
(72)
30
Under review as a conference paper at ICLR 2021
The sum of the all the irreducible characters, for a 6= eA is 0 (Scott & Serre, 1996, Corollary 2):
Pχ∈X χ(a) = 0. Hence,
χ(a) = 0 = X0,0 (a) +	χ(a)
χ∈X	χ∈X,χ6=X0,0
= 1+ X χ(a)
χ∈X,χ6=X0,0
→ X	χ(a) = -1.
χ∈X,χ6=X0,0
Hence,
Tr(ρ(a, eH)) = |H| + |H |(-1) = 0.
(73)
(74)
(75)
(76)
Consequently, the character table of ρ is the following:
(eA, eH)
XP ∣A∣∣H∣
(aι,eH)	(a2,eH) ...	(a∣A∣,h∣H∣)
0	0	0	0
The character of ψ is PNHψ times the character of χρ, i.e.
χψ
(eA, eH)
N
(a1, eH)	(a2, eH)	...	(a|A|, h|H|)
0	0	0	0
D.5 Insights on the stacked shift operators model for the discrete finite
Special Euclidean group
Similarly to 2D translation case, we can use the theoretical form of the representation to use to gain
insight on what the intersected layers Li should be for the case of 2D translations in conjunction
with rotations. Elements of this group are (a, h) where a is the 2D translation, composed of ax,k the
translation on the x-axis, ay,k0 translation on the y-axis, and h the rotation. Using the stacked shift
operators model, we first use the operator corresponding to the rotation, then the one for translation
in x, then the one for y-translation. The resulting transformed latent code is
z = ψy,k0,N L2 ψx,k,N L1ψh,j,N z
(77)
with ay,k0	=	ayk,00,	ax,k =	akx,0,	h =	hj0.	The operators for translations	ψx,k,N ,	ψy,k0,N	are
described in Equations 39 and 40. We repeat them here for clarity.
k,N :=
(78)
31
Under review as a conference paper at ICLR 2021
ψy,k0,N :=
(79)
where ωι = eK and ω2
声 times, as follows,
2iπ
e K0. And ψh,j,N is the repetition of the shift operator for the rotation
ψh,j,N :=
(80)
The representations for each transformation ψx,k,N , ψy,k0 ,N , ψh,j,N are linked with the represen-
tation that the theory gives us in Equation 66.
First, recall that elements of H acts on a character χr = χ(x1,y1):
χ(x1,y1) (hi (a)) = χhi((x1,y1))(a)
(81)
Thus when hi spans H in a given matrix Mr, we obtain |H| distinct characters built from χr, i.e.
Mr (a)
χh1(r)(a))	...	0
0	χh2 (r) (a)	...
0	...	0
0
0
Xh∣H∣(r) (G
(82)
For another matrix Mr0 , we obtain again |H | distinct orbits, distinct from the one present in Mr
(otherwise they would be in the same orbit). We have AH-I representatives (excluding x(o,o)),
thus |A| - 1 characters of A evaluated at a are obtained by considering all the matrices Mr . The
remaining character is χ(0,0) = 1. Hence, the diagonal matrix ρ(a, eH ) of order |A||H | contains all
the characters of A evaluated at the element a, each one repeated |H | times since each matrix Mr is
repeated |H| times, and the first diagonal elements of ρ(a, eH ) are 1.
So we see that what we need are |H| times each character ofA. As explained in Section D.4.2, these
characters are of the form:
i(2π--k+2∏--k0)	i(2π---k) i(2π ——k0)	I i√
X(x1,y1)(a) = e( K K = e(K) e (	K0 ) = ωf1kωyι k
(83)
32
Under review as a conference paper at ICLR 2021
The characters of each translation group are χx1 = ω1x1 for the x-translation and χy1 = ω2y1 for the
y-translation, thus:
χ(x1,y1) (a) = χ(x1,y1) (ax,k, ay,k0) = ω1x1 ω2y1 = χx1 (ax,k)χy1 (ay,k0)	(84)
Since ψχ,k is represented N times in ψχ,k,N and ψy,k0 is represented KK0 times in Ψy,k0,N, We can
take L2 to reorder ψy,k0,N into blocks of K0 repeated diagonal elements such that when multiplied
With ψx,k,N by doing ψy,ko,NL2Ψx,k,N, we obtain KNKT = ∣N∣ blocks of size |A|, each containing
every character of A once. Furthermore, We can see that the resulting matrix ψy,k0,N L2ψx,k,N
is composed of IANH ∣ blocks of size |A||H | containing every character of A, but repeated |H | times.
This resulting matrix is then multiplied with the matrix L1 ψh,j,N. So let us now turn to the
representation of the rotation h, that is ψh,j,N. It is composed of the elements that appear in the
upper-left diagonal of ρ(a, h). But we also need to make the permutation matrices Ph appear.
Recall that diagonal matrix of order |H | that corresponds to the shift representation of h = hj0 (not
repeated 声 times) is
1
i2∏ j
ei2π ∣h∣
ψh,j =	ei2π ⅛
(85)
i2∏ j(∣H∣-1)
ei2π	∣h∣
If we right-multiply ψh,j by
-1	1
1	ei2π ⅛
r	i2π (∣H∣-1)
1	ei2π	|H|
1
i2∏ | H|-1
ei2π ∣h∣
i2	(∣H∣-1)(∣H∣-1)
i2π	|H|
(86)
and left-multiply it by
1
-i2∏ (∣H∣-1)
e i2π	∣H∣
1
B
e-i2π IHHII
(87)
1
. 2	(∣H∣-1)(∣H∣-1)
e-i2π	∣H∣
we obtain a matrix that is filled with 0 except for each row r, one time the value |H | at the column
c where c such that h0-rhhc0 = eH, i.e. hhc0 = hr0, this is exactly what Ph represents, as we defined
it such that if hhi = hj then Ph has a 1 at the column j of its i-th row. Thus,
Ph= 1H| Bψh,jC
(88)
Consider a block diagonal matrix M where the first |H | elements are all ones (on the diagonal), and
then the matrix B is repeated on the block diagonal |A| - 1 times.
(89)
33
Under review as a conference paper at ICLR 2021
Consider M0 that is the repetition of |A| times ψh,j
(90)
(91)
which is of order |A||H |. Assuming the encoder takes care of the right-multiplication by C and the
scaling by |H | (which it can learn to do), we do not write the scaling and right-multiplication by C .
The operator ψh,j,N is 备 repetitions of ψh,j, hence it can be seen as repetitions of
M0. Thus, let us denote Q a N by N block diagonal matrix form of PNHψ repetitions of M. When
multiplying Q with ψh,j,N we can get MM0 repeated |A||H| times:
Qψh,j,N
(92)
This is the matrix that is multiplied with ψy,k0 ,N L2ψx,k,N. However, the order of the rows in the
resulting does not correspond to the ordering of the characters in ψy,k0,N L2ψx,k,N. If we want to
match the operator in Equation 66 such that
ψa,h,N z = (ψy,k0,N L2ψx,k,N L1ψh,j,N)z	(93)
We need L1 be the product of two matrices P and Q, such that P reorders the rows of the vector
Qψh,j,N z to match the ordering of the characters in ψy,k0,N L2ψx,k,N. The result of
(ψy,k0,N L2ψx,k,N)P Qψh,j,N z	(94)
will be a vector that is a permuted version of the vector we would get with ψa,h,N z, and a linear
decoder can learn to reorder it if we want to exactly match ψa,h,N z.
34
Under review as a conference paper at ICLR 2021
D.6 Details of Section D.4
D.6.1 Stabilizers of orbits Case 2
We show here that for χr = χ(x1,y1) (x, y) with x1 6= 0 or y1 6= 0 the stabilizer group Hi is only
{eH}.
Proof. Let us consider the stabilizer group of χ(x1,y1). It is composed of the elements h ∈ H
such that for a ∈ A, hχ(x1,y1)(a) = χh((x1,y1))(a) = χ(x1,y1)(a). In other words, we must have
that the character of the rotated vector h((x1, y1)) is the same as the character corresponding to
(x1, y1), for any a. Seeing characters as 2D complex numbers, we must have (x1, y1) = (cos θx1 -
sin θy1, sin θx1 + cos θy1), i.e. the rotated vector corresponding to the character is equal to itself.
As we employ boundary conditions, this is for example the case for θ = π or θ = -π for x1 =
KK,yι = K, since the inverse of (K, K) is itself. But We restrict ourselves and We consider only
odd K and K0, such that there is no angle π such that the result of the rotation of (x1, y1) gives the
same vector in complex space.	□
D.6.2 Degrees of the representations
Case 1 In Case 1, We have |H| representations of degree 1.
Case 2 Recall that in Case 2, Hi = {eH} and We use ρ = 1, thus the character of the induced
representation is
XIndG(Xr01) (a,h) =	E	Xr(h(吟Xl(eH)	(95)
h0 ∈H s.t.h0-1 hh0 =eH
h0-1hh0 = eH means h0-1 h = h0-1. If h = eH, h0 Will span the entire H and We have
XIndGH (Xr ®1)(a, eH) = Σ Xr(a)Xl(eH) = |H|Xr(a)	(96)
h0∈H
If h 6= eH then there is no element h0 for Which this is true. Indeed, otherWise it means h =
h0h0-1 = eH leading to a contradiction. Thus:
|H |Xr (a) ifh = eH
0 if h 6= eH
To obtain the degree of the representation, We calculate the character of the identity element.
XIndGjXr 01)(eA,eH) =	E	Xr (eA)Xl (eH)	(97)
h0 ∈H s.t.h0-1 eH h0 =eH
= X Xr(eA)X1(eH) = |H|Xr(eA)X1(eH) = |H|X1(eH) = |H|	(98)
h0∈H
Where We use the fact Xr (eA) = 1 (A is abelian hence its irreducible characters are of degree 1)
and that h0-1eHh0 = eH is true for all h0 ∈ H. Hence, the degree of the induced representation is
|H|X1(eH) = |H| since X1(eH) = |{eH}| = 1.
Thus in case 2 each orbit induces a unique induced representation of degree |H |, and We
have 1AH-1 orbits in the second case. This shows we derived all the irreducible representations of
G, as if We do the sum of the degree squared of irreducible representations We have
|H| + 1A-I |H ∣2 =附 + |A||H|-|H| = |A||H|	(99)
|H|
which correctly equals the order of G, see Scott & Serre (1996, Corollary 2).
35
Under review as a conference paper at ICLR 2021
E Additional Results
E.1 Quantifications
Test Mean Squared Error Table 2 reports test MSE for disentangled, supervised shift and weakly
supervised shift operators.
Table 2: Test mean square error (MSE) ± standard deviation of the mean over random seeds. Num-
bers in () refer to the number of rotations, the number of translations on the x-axis, and the number
of translations on the y-axis in this order. The case of multiple transformation does not apply to the
weakly supervised shift operator, and we did not experiment on translated MNiST with the weakly
shift operator as its performance on translated simple shapes and rotated MNiST were showing its
relevance already.
Dataset	Model		
	Disentangled	Shift	Weak. sup. shift (KL = 10)
Shapes (10,0,0)	0.0208 土 5.2e-6	0.0002 土 1.8e-6	0.001 ± 1.3e-3
Shapes (0,10,0)	0.0352 土 9.0e-6	0.0052 土 9.6e-6	0.0097 土 5.2e-3
Shapes (0,0,10)	0.0353 土 7.6e-6	0.0052 ± 1.5e-5	0.0115 土 4.9e-3
Shapes (0,5,5)	n/a	0.0047 土 1.9e-5	n/a
Shapes (4,5,5)	n/a	0.0049 土 7.7e-6	n/a
Shapes (5,5,5)	n/a	0.0021 土 5.9e-6	n/a
MNiST (10,0,0)	0.0660 土 8.2e-5	0.0004 土 5.6e-6	0.0035 土 3.9e-3
MNiST (0,10,0)	0.0838 土 3.2e-5	0.0079 ± 5.1e-5	n/a
MNiST (0,0,10)	0.0857 土 5.1e-5	0.0062 土 3.2e-5	n/a
MNiST (4,5,5)	n/a	0.004 ± 4.0e-5	n/a
LSBD Disentanglement Measure We compute the LSBD disentanglement metric to quantify
*^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^ *^Z^Z^z^Z^Z^z^^Z^Z^Z^z^Z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^^^z^Z^Z^Z^z^Z^Z^Z^z^Z^Z^Z^z^Z^z^Z^z^Z^Z^Z^
how well the shift operator captures the factors of variation compared to the disentangled operator
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
(see Anonymous (2021)). Note traditional disentanglement metrics are not appropriate as they
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
describe how well factors of variation are restricted to subspaces in contrast to our proposed
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
framework using distributed latent operators. LSBD, on the other hand, measures how well latent
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
operators capture each factor variation to quantify disentanglement even for distributed operators.
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
Using LSBD, we quantify the advantage of the shift operator with LSBD of 0.0020 versus the
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
disentangled operator with LSBD of 0.0106 for the models in Fig. 3.A and 3.B.
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
E.2 Additional analyses
Latent Variance and PCA Analysis The analysis in Fig. 1.C and 1.D quantitatively measures
disentanglement in the latent representation for VAE and variants on rotated MNIST. We first apply
every transformation to a given shape and compute the variance of the latent representation as we
vary the transformation. The final figure shows the average variance across all test samples. For
the PCA analysis, we seek to determine whether the transformation acts on a subspace of the latent
representations. We first compute the ranked eigenvalues of the latent representations of each shape
with all transformations applied to the input. We then normalize the ranked eigenvalues by the sum
of all eigenvalues to obtain the proportion of variance explained each latent dimension. Finally, we
plot the average of the normalized ranked eigenvalues across all test samples.
Additional latent traversals Figures 5 and 6 show all latent traversals for the model with the best
validation loss. We see the success of disentanglement in the case of a single digit and the failure of
a latent to capture rotation in the case of multiple digits. We show a more granular traversal with 50
ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ
p⅛ZLRerJa⅛n!fLeasZh⅛∣seliSe⅛2Jn Figures 9, 10 we show comparable results for the case of
translations along the x-axis and y-axis.
Additional results for the distributed operator models Fig. 14 shows additional results on
MNIST. Fig. 14a shows that the weakly supervised shift operator performs well on Rotated MNIST,
and in Fig. 14b we see the stacked shift operator model is able to correctly encode the multiple
36
Under review as a conference paper at ICLR 2021
transformations case on MNIST. Fig. 15 shows performance of the weakly supervised shift operator
on translated simple shapes (either x or y translations). The effect of the number of latent trans-
formations is explored in App. B.3. It shows the best model is in the case of translation obtained
with 21 transformations. Nonetheless, to be able to feed to the model the correct 10 transformations
needed in these plots, the reported plots are for the model with 10 latent transformations. Fig. 16
shows example results for the case of the Special Euclidean group (i.e. rotations in conjunction with
translations) in a setting where the semi-direct product structure is broken (see Appendix D.4.1).
Here, we used a rotation group of order 5. We see the stacked shift operator model performs well
nonetheless.
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■I
buπbbqbbbbbeiud□.buπbbdbπbbbbbdei
□□□□□□□□□□□□□,^3□□□□□□□□□ŋ□ŋ□□□
Ddqddqddddddddddddddqqdddddddd
ŋŋŋŋBŋ□。切。Q□。D。Qŋŋ□D□□□。。。□
□□□昭S□□□□。□□□□□□□□□□□□□□□□。。□
QQŋQ・M□□□□Q□□ŋ□ŋ□Q□[!]□□□□□□□Q□
^y□□□□□□□□□□□。Q□□□□□□Q□□□□□□□□。
/qqqhhhhhhhd-hdhbbhbbhbhbqqhhhh
DDnDDQnDnnnnDnHDnnHnDDQnnQDDDD
DnBBB□BDBnn01ElnQQnBIHnBBIBnDlEI□nDn
BnBBBQBBBDBDBDEIlaBBHBBαnDnEIDnBn
DDDDDDODnnnnnnOOnnHnnnDnnQDDn-U
3333333333333□□□3 3 3□□KStSNBSNBSUNa
nnnπuiSnUnnnnnnBIB is n⅞3unu¾nl3Hil 13口
nBoBDHBBBBBBBB□sBB□BBBαBBHBD□n
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
□ □ □ ŋ □ □ □
H H Q O El El Q
DQDDDDD
n D □ □ □ E3 □
□ □ □ □ □ □ □
ŋ □ ŋ □ □ □ □
□ □ □ □ □ □ ŋ
□ ŋ □ □ □ □ □
Q H P D O B B
DDDDDDD
ddddddd
ddddddd
qqqqqqd
dqqqqqq
DDDDDHH
B Q Q D □ □ E3
DDDDDDD
D H B S H D D
□ □ Q Q □ □ □
H H Q Q Q Q Q
ŋ □ ŋ □ □ Q □
DHDDDDQ
□ □ □ □ □ □ □
DQQQQDD
□ □ □ □ □ □ □
Ξ E3 □ □ Q □ □
Q Q □ Q □ Q □
D D D D D □ □
□ Q D □ □ □ □
Figure 5: Single Rotated MNIST Digit Label: Latent traversals for VAE (left), β-VAE (middle),
CCI-VAE (right) trained on a single rotated MNIST digit (10 rotations). Latent traversal spans the
range [-6, 6] for each latent dimension.
37
Under review as a conference paper at ICLR 2021
QaQaBDDBDDBlHBaBDDBaBDHnDDnDBD
H B H Pl Bl El ∏
QS□QHQH
H H Q D ES Q Q
Q □ El El El El El
H Q Dl Dl Q □ Ξ
EO BI Q Q D El EB
H S D D Ql O B
D D Q D El D El
D D D D El H Q
EB EB Q D EO El BI
D D E9 D EO EB B
EB EO E9 EO EO D EO
D D E9 E9 EO EO El
Q Q EB S B B Q
H H H El En D D
EB EB EO El EO EO EO
O D En D D El El
Q D Q Q Q Q EO
Q □ Q □ Q Q Q
H S Ξ □ El D E3
Q □ El □ El El El
El El El El El Q D
H Q Q □ S S S
El El El El El El El
B B □ Q Ξ Ξ Ξ
Q Q Q Q El Q EB
Q EO D El EO D E9
ð ð EO EO EO El EB
El Q El El El El El
S Q Q El □ Bl D
Figure 6: Rotated MNIST: Latent traversals for VAE (left), β-VAE (middle), CCI-VAE (right)
trained on all rotated MNIST digits (10 rotations). Latent traversal spans the range [-6, 6] for
each latent dimension. Note in this case, the best validation model for VAE contained 30 latent
dimensions whereas β-VAE and CCI-VAE contain 10.
38
Under review as a conference paper at ICLR 2021
(a) Granular latent traversals for VAE trained on all rotated MNIST digits (10 rotations).
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
,d"" Q EaEaEassEaQQQQQQQQsssssaaaHQEaDDDQQHHSQiaaQQQQaQQQQQQ
□	□□□□□□□□□□□□□□□□□□□QD0000D0nS000iBQBBH
HHHHHHHHHElEiBESQElISQEiDDDQEnnnDDDDDDQDQQDDD
QQHQBBElQQQQQQQDQQQEIQQQElQDQBiQigigOQlEaQQEDEaQ
血血血血血血血血血血血血而血血血血而血血血血血血血血Ei血血血血血血血血血血血
S□□□EiE]EiEiEiElEi□EiEiQEiEiElEIQQ□DlDDDD□□□□□□□□□QQ
QE3E3EIQQEαE3
Q
D
BBiiiiiiiiBiHHHHSHHaiommaEaQisaQDDDQ
□	□□□□□□□QQQQBnnoiD∏niBiiiiaBBBiiiiiiiii
BBBBDaaiiiiiiniiiiBiiDnnDDEinDoooQDHQQBQEIEIEiQEi

ΞΞΞΞΞΞΞΞ
Qrl Q QBBBBSSSSSSSSSSSSSSQ□□□QOOOB∣mmmmQliaaE]QQQiaQQQQQQ□□
dimension. Note in this case, the best validation model for VAE contained 30 latent dimensions
whereas β-VAE and CCI-VAE contain 10
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
39
Under review as a conference paper at ICLR 2021
original 00 36° 72° 108° 144o 180° 2160 252o 288o 324o
Original 0。 36。 72。 108C 144。 180° 216° 252° 288° 324°
Hqqhbbbiihbh
(a) Supervised disentangled operator on Rotated
Shapes (10 rotations)
Figure 8: Non-linear disentangled operator with latent rotations
(b) Supervised disentangled operator on Rotated
MNIST (10 rotations).
Q□Q□□□□Q
E3BH□□□□E3
DDD□Q□Q□
QQQQQQQ□
QQQ□DQQ9
Q□D□QQ□□
QQQQDQDq
Q□Q□□□QQ
QDQQQQQQ
BQQ□D□SS
hqqdddqd
QHQQ□□□□
QQQ□QQQ□
Q□□QonaE∣
Q□□□□QQQ
Q□Q□□QQ□
Q□□□□QQQ
Q□□□□□□□
QQQQ□Q□Q
QECEO□□□
Q□BQQQK30
Q□□□□QQQ
QBD□□□3E1
Q□D□□D□□
Q□□□Q□QQ
Danannnn
DDDDΠΠDB
E3E330QK9DE1
snnnnnnn
QDunnnDD
QDnnDnnn
DDnDDDDD
QD□DDDDD
DDDDDODn
Qnnnnnnn
BlnDnnnnn
Eonnnnnnn
QEJElQDIiliE
Qnnnnaan
Qnnadnan
Q□□□□□□□
SDnnnDnn
QnnDDnnn
Dnnnnnnn
Bnnnnnnn
ESDDDDDnn
QnnDDDnD
Qnunnnnn
Qnnnnnnn
DDDDonnn
QQDQQQQE1
QQQQQQQ□
B□DQD□□□
DQQQE1E1E10
Q□□QQ□□□
Q□Q□Q□□□
QQQ□□Q□□
Q□QQ□QQ□
Q□QQQQ□□
BEDDQEICia
bdddddqq
Q□Q□Q□□□
D□QQQ□Q□
Q□QD□QQ□
Q□□□O□□□
QQQQQQQQ
Q□□□□□□□
Bqqqqqqq
Q□□□□□□□
Q□QQDQQ□
E3E1□□QQ□□
B∣QQQQQQQ



Figure 9:	Single MNIST Digit Label Translated along X-Axis: Latent traversals for VAE (left),
β-VAE (middle), CCI-VAE (right) trained on a single MNIST digit translated along the x-axis (10
translations). Latent traversal spans the range [-6, 6] for each latent dimension.
40
Under review as a conference paper at ICLR 2021
QQQQQQQQmUQQQHQQQQQQQQQQQQQQQSQ
^mqddddddddddddddddqdhdddddqdhd
KIQ□Q□Q.^IQQ.QQ.DQQ.QQQQQQnQ.^JDQQQQQ-D
HDQDDDDDQDDDDDDDQDDDnDDDDDDDDD
Ddddddddddddddddddddddddddbddq
Qdddsddddddddddddddqddddddhddd
□QDQHQQQD□□□~^3Q□QQ□□QQQQQQQHQ
□QQαQQQQQQQ□QQQ□.^3□DQQQQH,^3Q
qqqqbqqqqqq⅝3bq一QaQQQ一Q≡QDnQ-QQQπQ
QQQQQQQ
qqqqqqq
qqqqqqq
qqqqqqq
qqqqqqq
Q □ Q Q H B H
□ □ D D H D D
QQQQQQQ
HHqqqqq
QQ□□QQQ
HQQ□HQQ
q□qqqqq
qqq□qqq
qqqqqqq
HHqqqqq
qqqqqqq
Q Q Q Q Q Q QQ
QQQQQQQ
qqqqqqq
QQQQQQQ
QQQQQQQ
QQQQQQQ
□ □ □ □ □ □ D
QQQQQQQ
□□□□HDD
QQQQQQQ
QQQQQQQ
QQQQQQQ
QQQQQQQQ
QQQQQQQQ
QQQQQQQH
Qhhhhhhq
Bqqqqqqq
bqqqqqqq
sddbbddb
QQQQQQQQ
QDQEIEIQQ□
qddddbdd
QQQQQQQQ
qdhddddb
hdddddqs
E∣DQDDQDB
QQ□□QHK∣H
QQQQQQQQ
QQDDDQDB
QQQQQDQQ
QDDBQQQQ
Bqqhhqqq
QQQQQQ□□
QDDQBHBB
Bqqqqqqq
QQQQQQQQ
Bhqqqhqq
Bqqqqqqq
QMMIflBQQQ
QQQQQQQQ
QBBQQQQQ
qhqqhhdh
Figure 10:	Single MNIST Digit Label Translated along Y-Axis: Latent traversals for VAE (left),
β-VAE (middle), CCI-VAE (right) trained on a single MNIST digit translated along the y-axis (10
translations). Latent traversal spans the range [-6, 6] for each latent dimension.
41
Under review as a conference paper at ICLR 2021
Figure 11: MNIST Translated along X-Axis: Latent traversals for VAE (left), β-VAE (middle),
CCI-VAE (right) trained on all MNIST digits translated along the x-axis (10 translations). Latent
traversal spans the range [-6, 6] for each latent dimension.
42
Under review as a conference paper at ICLR 2021
DDDDDDDD
QUΠQQBg]E]
qddddddd
QDDDDDDQ
QDDDDDDQ
Oqqqdeiqq
QDDDDDDD
DQDDDDDD
DQQDQQQQ
QDDDDQDD
DS□SDQDQ
QQQQQIigQH
UDDDDQDQ
QQQQQQQQ
QQQQQQQQ
QDQHHn∏B
QQQQQQQQ
≡ħħħd□□□
QHQQQQaQ
QDDQDaaa
QDDDDDDD
QDDQI1DHD
g⅛ι⅛⅝s⅛M⅛ιm⅛ι
QHHQDQ□Q
bqqqπħhħ
IiQQDQDQQ
QDDDDDUD
H□□□H□QH
QQQQQQQQ
≡UBQQQ□□
BQHHSSHS
ISHSaiafflBi
QQQQQQQQ
QQQQQQQQ
≡nn∏QQEVBi
≡QQ□SQHB
Oqbhhhbq
QQQQQQQQ
QQHHQQQQ
QHHQHHHQ
≡HHSHQSH
M囱目倒口∙名剧
QQQQQQQQ
Iqqhhqhq
HSSSHHHH
Iqqqhhqh
QQQQQQQQ
QQQQQQQQ
H∣QQQQQQQ
H∣SSHHSHH
HQQQQHQQ
∣9SSSSSSS
≡SSSSSSQ
≡QQHHHHH
Bqqqbqqh
QQQQQQQQ
Figure 12:	MNIST Translated along Y-Axis: Latent traversals for VAE (left), β-VAE (middle),
CCI-VAE (right) trained on all MNIST digits translated along the y-axis (10 translations). Latent
traversal spans the range [-6, 6] for each latent dimension.
(a) Supervised disentangled operator on translated
shapes along the x-axis (10 translations)
Original _
■■■■■■■■■■■
original
HHQHHHHMIiaH
original
BHHHHHHaaEia
Original
Hqqhhhhhhoei
original
BnaBHHHHHIIB
(b) Supervised disentangled operator on translated
shapes along y-axis (10 translations).
Figure 13:	Non-linear disentangled operator with latent translations
43
Under review as a conference paper at ICLR 2021
270o 270o O0 180o 180o 90o 270o 90o 270o 270o 90o
Original 12,18 12,18 12,12 6,18 18,0 12,24 12,12 24,6 12,18 6,6 12,12
QnnHHiiHffianKK
90o 270o 180o Oo O0 270o 180o 270o 90o 180o 270o
Original 0,0 0,12 24,12 6,24 24,0 6,0 18,24 0,24 24,12 12,24 6,12
HBNHHaDBBHHH
270o	270o	90o	90o	O0	O0	O0	90o	90o	180o	O0
Original 24,24	12,18	24,24	0,24	18,24	18,24	12,24	12,24	6,12	18,0	6,24
ΞaκaΞ≡≡κ≡H≡E
180o	90o	90o	180o	O0	90o	270o	180o	90o	90o	O0
original 6,12	6,12	6,6	24,24	24,18	12,0	0,0	6,0	0,18	6,24	0,0
ΠHIEΠHBHBiBBU
(a)	Weakly supervised shift operator on Rotated
MNIST (10 rotations).
(b)	Supervised shift operator on Rotated-
Translated MNIST (4 rotations, 5 x-translations
and 5 y-translations).
Figure 14: MNIST additional experiments.
(a)	Weakly supervised shift operator on x-
translations (10 translations).
(b)	Weakly supervised shift operator on y-
translations (10 translations).
Figure 15: Simple shapes additional experiments.
Figure 16: Supervised shift operator on Rotated-Translated simple shapes when the semi-direct
product structure is not respected as rotations angles are j ∏, j = 1,... 5 (5 rotations, 5 X-
translations and 5 y-translations).
44
Under review as a conference paper at ICLR 2021
lest Reconstruction
x2 from
Figure 17: Pairs of test samples and their reconstructions for the stacked shift model with 5
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
translations in both x and y.
45
Under review as a conference paper at ICLR 2021
lest Reconstruction
x2 from
Figure 18: Pairs of test samples and their reconstructions for the stacked shift model with 4 rotations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
and 5 translations in both x and y.
46