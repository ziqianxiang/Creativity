Under review as a conference paper at ICLR 2021
Adaptive Hierarchical Hyper-gradient De-
SCENT
Anonymous authors
Paper under double-blind review
Ab stract
In this study, we investigate learning rate adaption at different levels based on the
hyper-gradient descent framework and propose a method that adaptively learns the
optimizer parameters by combining different levels of adaptations. Meanwhile, we
show the relationship between regularizing over-parameterized learning rates and
building combinations of adaptive learning rates at different levels. The experi-
ments on several network architectures, including feed-forward networks, LeNet-5
and ResNet-18/34, show that the proposed multi-level adaptive approach can
significantly outperforms baseline adaptive methods in a variety of circumstances.
1	Introduction
The basic optimization algorithm for training deep neural networks is the gradient descent method
(GD), which includes stochastic gradient descent (SGD), mini-batch gradient descent, and batch
gradient descent. The model parameters are updated according to the first-order gradients of the
empirical risks with respect to the parameters being optimized, while back-propagation is imple-
mented for calculating the gradients of parameters (Ruder, 2016). Naive gradient descent methods
apply fixed learning rates without any adaptation mechanisms. However, considering the change
of available information during the learning process, SGD with fixed learning rates can result in
inefficiency and requires a large amount of computing resources in hyper-parameter searching. One
solution is to introduce a learning rate adaptation. This idea can be traced back to the work on gain
adaptation for connectionist learning methods (Sutton, 1992) and related extensions for non-linear
cases (Schraudolph, 1999; Yu et al., 2006). In recent years, optimizers with adaptive updating rules
were developed in the context of deep learning, while the learning rates are still fixed in training. The
proposed methods include AdaGrad (Duchi et al., 2011), RMSProp (Tieleman and Hinton, 2012), and
Adam (Kingma and Ba, 2015). In addition, there are optimizers aiming to address the convergence
issue in Adam (Reddi et al., 2018; Luo et al., 2018) and to rectify the variance of the adaptive learning
rate (Liu et al., 2019). Other techniques, such as Lookahead, can also achieve variance reduction and
stability improvement with negligible extra computational cost (Zhang et al., 2019).
Even though the adaptive optimizers with fixed learning rates can converge faster than SGD in a
wide range of tasks, the updating rules are designed manually while more hyper-parameters are
introduced. Another idea is to use objective function information and update the learning rates
as trainable parameters. These methods were introduced as automatic differentiation, where the
hyper-parameters can be optimized with backpropagation (Maclaurin et al., 2015; Baydin et al.,
2018). As gradient-based hyper-parameter optimization methods, they can be implemented as an
online approach (Franceschi et al., 2017). With the idea of auto-differentiation, learning rates can be
updated in real-time with the corresponding derivatives of the empirical risk (Almeida et al., 1998),
which can be generated to all types of optimizers for deep neural networks (Baydin et al., 2017).
Another step size adaptation approach called “L4”, is based on the linearized expansion of the loss
functions, which rescales the gradient to make fixed predicted progress on the loss (Rolinek and
Martius, 2018). Furthermore, by addressing the issue of poor generalization performance of adaptive
methods, dynamically bound for gradient methods was introduced to build a gradual transition
between adaptive approach and SGD (Luo et al., 2018).
Another set of approaches train an RNN (recurrent neural network) agent to generate the optimal
learning rates in the next step given the historical training information, known as “learning to learn”
1
Under review as a conference paper at ICLR 2021
(Andrychowicz et al., 2016). This approach empirically outperforms hand-designed optimizers in a
variety of learning tasks, but another study has shown that it may not be effective for long horizons
(Lv et al., 2017). The generalization ability of this approach can be improved by using meta training
samples and hierarchical LSTMs (long short-term memory) (Wichrowska et al., 2017).
Beyond the adaptive learning rate, learning rate schedules can also improve the convergence of
optimizers, including time-based decay, step decay, exponential decay (Li and Arora, 2019). The
most fundamental and widely applied one is a piece-wise step-decay learning rate schedule, which
could vastly improve the convergence of SGD and even adaptive optimizers(Luo et al., 2018; Liu
et al., 2019). It can be further improved by introducing a statistical test to determine when to apply
step-decay (Lang et al., 2019; Zhang et al., 2020). Also, there are works on warm-restart (O’donoghue
and Candes, 2015; Loshchilov and Hutter, 2017), which could improve the performance of SGD
anytime when training deep neural networks.
We find that the existing gradient or model-based learning rate adaptation methods including hyper-
gradient descent, L4 and learning to learn only focus on global adaptation, which could be further
extended to multi-level cases. That focus aims to introduce locally shared adaptive learning rates such
as the layer-wise learning rate and parameter-wise learning rate and considers all levels’ information
in determining the updating step-size for each parameter. The main contribution of our study can be
summarized as follows:
•	We introduce hierarchical learning rate structures for neural networks and apply hyper-
gradient descent to obtain adaptive learning rates at different levels.
•	We introduce a set of regularization techniques for learning rates to address the balance of
global and local adaptations and show the relationship with weighted combinations.
•	We propose an algorithm implementing the combination of adaptive learning rates at multiple
levels for model parameter updating.
2	Multi-level Adaptation Methods
2.1	Layer-wise, Unit-wise and Parameter-wise Adaptation
In the paper on hyper-descent (Baydin et al., 2017), the learning rate is set to be a scalar. However, to
make the most of learning rate adaptation, in this study, we introduce layer-wise or even parameter-
wise updating rules, where the learning rate αt in each iteration time step is considered to be a
vector (layer-wise) or even a list of matrices (parameter-wise). For the sake of simplicity, we collect
all the learning rates in a vector: αt = (α1,t, ..., αN,t)T. Correspondingly, the objective f(θ) is a
function of θ = (θ1, θ2, ..., θN)T, collecting all the model parameters. In this case, the derivative of
the objective function f with respect to each learning rate can be written as
∂f(θt-l)
∂αi,t-i
df (θ1,t-1,…,θN,t-1) = df (θ1,t-1,…,θN,t-1) dθj,t-1
∂αi,t-i	^=	∂θj,t-i	∂ai,t-i
(1)
where N is the total number of all the model parameters. Eq. (1) can be generalized to group-
wise updating, where we associate a learning rate with a special group of parameters, and each
parameter group is updated according to its only learning rate. Notice that although there is a
dependency between at-ι and θt-2 with: αt-ι = at4 - βVf (θt-2), where β is the updating
rate of hyper-gradient descent, we consider that αt-1 is calculated after θt-2 and thus a change
of αt-1 will not result in a change of θt-2. Assume θt = u(Θt-1, α) is the updating rule, where
Θt = {θs }ts=0 and α is the learning rate, then the basic gradient descent method for each group i
gives θi,t = u(Θt-1, αi,t-1) = θi,t-1 - αi,t-1Vθif(θt-1). Hence for gradient descent
∂f(θt-ι)
∂ai,t-i
Vθif(θt-1)TVαi,t-1u(Θt-1,αi,t-1)=-Vθif(θt-1)TVθif(θt-2).
(2)
Here αi,t-1 is a scalar with index i at time step t - 1, corresponding to the learning rate of the
ith group, while the shape of Vθif(θ) is the same as the shape of θi. We particularly consider
two special cases: (1) In layer-wise adaptation, θi is the weight matrix of ith layer, and αi is the
particular learning rate for this layer. (2) In parameter-wise adaptation, θi corresponds to a certain
parameter involved in the model, which can be an element of the weight matrix in a certain layer.
2
Under review as a conference paper at ICLR 2021
2.2	Regularization on Adaptive Learning Rates
The selection of adaptation level should depend on a case-by-case basis. Global or parameter-wise
adaptation is usually not the optimal choice across all circumstances. Recall that for deep neural
networks, we typically use a relatively large architecture with regularization. This idea can also
be applied to learning rate space with parameter structure. To address over-parameterization in
implementing lower-level learning rate adaptation, we introduce regularization on learning rates to
control the flexibility. First, for layer-wise adaptation, we can add the following regularization term
to the loss function
Llr_reg_layer = λlayer	(αl-αg)2,	(3)
where l is the indices for each layer, λlayer is the layer-wise regularization coefficient, αl and αg are
the layer-wise and global-wise adaptive learning rates. A larger λlayer can push each layer’s learning
rate towards the global learning rate across all the layers. Given a particular αg,t, the gradient of the
loss function with respect to the learning rate αl in layer l can be written as
∂Lfuiι(θ,α) _ ∂Lmodei(θ,α) + ∂Liueg(θ,α)
∂αl,t
∂αl,t
∂αl,t
(4)
=Vθ"(θt-i)TVαι,t-1 u(Θt-2, αt-i) + 2λiayer(αι,t - αg,t).
Notice that the time step index of layer-wise regularization term is t rather than t - 1, which ensures
that we push the layer-wise learning rates towards the corresponding global learning rates of the
current step t. Denoting by hl,t-1 = -Vθlf(θt-1)TVθlu(Θt-2, αl,t-1), then the updating rule for
learning rates can be written as
αl,t = αl,t-1 - β----，~[ = αl,t-1 - β(-hl,t-1 + 2λlayer(αl,t - αg,t)).	(5)
∂αl,t
Eq. (5) has a close form solution but only applicable in the two-levels case. However, there is an
extra hyper-parameter λlayer to be tuned. In addition, when there are more levels, components of
learning rates at different levels can be interdependent. To construct a workable updating scheme
for Eq. (5), we replace αl,t and αg,t with their relevant approximations. We take the strategy of
using their updated version without considering regularization, i.e., αι,t = αι,t-ι + βhι,t-ι and
α^g,t = αg,t-ι + βhg,t-ι, where hg,,t-ι = -Vθf (θt-ι)TTVɑg,t-u(Θt-2, αg,t-ι) is the global h
for all parameters. Here we regard aι,t and &g,t as the “virtual” layer-wise and global-wise learning
rates for time step t and taking them into the right-hand side of Eq. (5) gives the new updating rule as
follows
αl,t = αl,t-1 + βhl,t-1 - 2βλlayer(αl,t - αg,t) = (1 - 2βλlayer)αl,t + 2βλlayerαg,t.	(6)
Notice that in Eq. (6), the two terms are actually a weighted average of the layer-wise learning rate
αι,t and global learning rate ag,t at the current time step. Since we hope to push the layer-wise
learning rates towards the global one, the parameters should meet the constraint: 0 < 2βλlayer < 1,
and thus they can be optimized using hyper-parameter searching within a bounded interval as well
as gradient-based hyper-parameter optimizations. We can also consider the case where three levels
of learning rate adaptations are involved, including global-wise, layer-wise, and parameter-wise
adaptation. If we introduce two more regularization terms to control the variation of parameter-wise
learning rate with respect to layer-wise learning rate and global learning rates, the regularization loss
can be written as
Llr_reg_para
λlayer (αl
l
-ɑg)2 + λpmlayer E E(αp, - αι)2 + λpara E £①, - α,)2,
lp	lp
where αpl is the learning rate for the p-th parameter inside layer l. The second and third terms push
each parameter-wise learning rate towards the layer-wise learning rate and the global learning rates,
respectively. Like the two-level case, the updating rule with this three-level regularization can be
approximated by the weighted combination of three components under “virtual approximation”. The
detail of the updating rule for the three-levels case is given by Algorithm 1 in Section 2.3. We also
provided a discussion on the bias of implementing “virtual approximation” in Appendix A.1.
In general, we can organize all the learning rates in a tree structure. For example, in the three-level
case above, αg will be the root node, while {αl} are the children node at level 1 of the tree and {αlp}
3
Under review as a conference paper at ICLR 2021
are the children node of αl as leaf nodes at level three of the tree. In a general case, we assume there
are L levels in the tree. Denote the set of all the paths from the root node to each of leave nodes
as P and a path is denoted by p = {α1, α2, ..., αL} where α1 is the root node, and αL is the left
node on the path. On this path, denote ancestors(i) all the ancestor nodes of αi along the path, i.e.,
ancestors(i) = {α1, ..., αi-1}. We will construct a regularizer to push αi towards each of its parents.
Then the regularization can be written as
Llr_reg =	λij(αi-αj)2.	(7)
p∈P αi ∈p αj ∈ancestor(i)
Under this pair-wise L2 regularization, the updating rule for any leave node learning rate αL can be
given by the following theorem whose proof is provided in Appendix A.2.
Theorem 1. Under virtual approximation, the effect of applying pair-wise L2 regularization Eq. (7)
results in performing a weighted linear combination of virtual learning rates at different levels
αL = PL=I Yj α^j with PL=I Yj = 1, where each component aj is calculated by assuming no
regularization.
Remarks: Theorem 1 actually suggests that a similar updating rule can be obtained for the learning
rate at any level on the path. All these have been demonstrated in Algorithm 1 for the three-level case.
2.3	Prospective of Adaptive Learning Rate Combination
Motivated by the analytical derivation in Section 2.2, we can consider combining adaptive learning
rates at different levels as a substitute and approximation of regularization on the differences in
learning rates. This makes the effect of learning rate regularization trainable with gradient-based
methods. In a general form, assume that we have L levels, which could include global-level,
layer-level, unit-level and parameter-level, etc, Theorem 1 suggests the following updating rule:
αt = pL=ι Yjaj,t. In a more general form, We can implement non-linear models such as neural
networks to model the final adaptive learning rates with respect to the learning rates at different
levels:at = g(a^ι,t, a2,t...aL,t; θ), where θ is the vector of parameters of the non-linear model. We
can treat the combination Weights {Y1, ..., YL} as trainable parameters, Which can also be globally
shared or parameter/layer-specific. In this study, we consider the globally shared combination
weights. We only need these different levels of learning rate to have a hierarchical relationship to
apply this method. For example, we can further introduce “filter level” to replace layer-level for the
convolutional neural network if there is no clear layer structure, where the parameters in each filter
will share the same learning rate.
As the real learning rates implemented in model parameter updating are weighted combina-
tions, the corresponding gradient matrices cannot be directly used for learning rate updating. In this
case, we first break down the gradient by the combined learning rate to three levels, use each of
them to update the learning rate at each level, and then calculate the combination by the updated
learning rates. Especially, hp,t, hl,t and hg,t are calculated by the gradients of model losses without
regularization, as is shown in Eq. (8)1.
hp,t = df (θ,α) = -Vθ f(θt-i, a)|p ∙ Vɑu(Θτ ,a)∣p
∂ap,t
hl,t = 'f(", a) = -tr(Vef(®t-La)ITVau(8t-2,a)|l)	⑻
∂al,t
hg,t = df∂θ,a) = — XX tr(Ve f(θt-ι, a)|lT Vαu(Θt-2, a)l)
∂at	l=1
where ht = Pl hl,t = Pp hp,t and hl,t = Pp∈lth layer hp and f(θ, a) corresponds to the model loss
Lmodel(θ, a) in Section 2.2. Algorithm 1 is the full updating rules for the newly proposed optimizer
with three levels, which can be denoted as combined adaptive multi-level hyper-gradient descent
(CAM-HD). In Algorithm 1, we introduce the general form of gradient descent based optimizers
(Reddi et al., 2018; Luo et al., 2018): for SGD, φt(g1, ...gt) = gt and ψt(g1, ...gt) = 1, while
1Here we use trace form to represent the sum of the element-wise product but compute in the simplest way.
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Updating the rule of three-level CAM-HD
input: α0, β, δ, T
initialization: θo, γ1,0, γ2,0, γ3,0, αp,0, α1,0, ɑ0, a；,。= γ1,0αp,0 + γ2,0a1,0 + γ3,0α0
for t ∈ 1, 2, ..., T do
gt = Vθ f (θ,a)
Update hp,t, hl,t and hg,t by Eq. (8).
aP,t = ap,t-1 - Bp fα*t 1)∂αp,t-1 = aP,t-1 - βpγ1,t-1hp,t
p,t-1	p,t-1
al,t = al,t-1 - Bl Pp f(θt-1) da；：-： = al,t-1 - βγ2,t-1 Pp hp,t = al,t-1 - Blγ2,t-1hl,t
at = at-ι - β Pi Pp ∂a*77∂⅛-1 = at-ι - βgγ3,t-1hg,t
p,t-1	t-1
ap,t = γi,t-1ap,t + Y2,t-1al,t + Y3,t-1at
∂L	∂L	∂αp*,t-1	∂L
γ1,t =	γ1,t-1	-	δ ∂Y1t-	=	γ1,t-1	—	δ	TP ∂ap,t-1 ∂γ1,t-1	=	γ1,t-1	- δap,t-1 TP ∂ap,t-1
γ2,t =	γ2,t-1	-	δ ∂γdLL-ι	=	γ2,t-1	-	δ	PP 谭-ι fe-f	=	γ1,t-1	- δal,t-1 PP ∂a*Lt-7
Y3,t =	Y3,t-1	-	δ∂YdL-1	=	Y3,t-1	-	δ	PpS⅛11⅛1-	=	Y3,t-1	- δat-1 Pp3⅛1
Yl = Yl/(Y1	+	Y2 + Y3),	Y2	= Yl/(Y1	+ Y2 + Y3), Y = Yl/(Yl + Y2 + Y3)
mt = φt(g1, ...gt)
Vt = ψt(gl,...gt)
θt = θt-1 - ap,tmt/√Vt
end
return θT, γ1,T, γ2,T, γ3,T, αp,T, αl,T, αT
for Adam, φt(g1, ...gt) = (1 - B1)Σit=1B1t-1gi and ψt(g1, ...gt) = (1 - B2)diag(Σit=1B2t-1gi2).
Meanwhile, the corresponding u(Θt-2, α) should be changed accordingly. Notice that in each
updating time step of Algorithm 1, we re-normalize the combination weights γ1 , γ2 and γ3 to ensure
that their summation is always 1 even after updating with stochastic gradient-based methods. An
alternative way of doing this is to implement Softmax function. In addition, the training of γs can
also be extended to multi-level cases, which means we can have different combination weights in
different layers. For the updating rates Bp , Bl and Bg of the learning rates at different levels, we set:
Bp = npB = B, Bl = nlB, Bg = nB, where B is a shared parameter. This setting will make the
updating steps of learning rates at different levels be on the same scale considering their difference
in the number of parameters. An alternative way is to take the average based on the number of
parameters in Eq. (8) at first.
2.4	Convergence Analysis and Algorithm Complexity
The proposed CAM-HD is not an independent optimization method, which can be applied in any
gradient-based updating rules. Its convergence properties highly depend on the base optimizer that
is applied. By referring to the discussion on convergence in (Baydin et al., 2017), we introduce
κp,t = T(t)ap,t + (1 - T(t))a∞, where the function T(t) is selected to satisfy tτ(t) → 0 as t → ∞,
and α∞ is a chosen constant value. Then we can demonstrate the convergence analysis for the
three-level case in the following theorem.
Theorem 2 (Convergence under mild assumptions about f). Suppose that f is convex and L-Lipschitz
smooth with ∣∣Vpf (θ)k < Mp, ∣∣Mf(θ)k < Mi, kVg f(θ)k < Mg for some fixed Mp, Mi, Mg and
all θ. Then θt → θ; if a∞ < 1/L where L is the Lipschitz constant for all the gradients and
t ∙ τ(t) → 0 as t → ∞, where the θt are generated according to (non-stochastic) gradient descent.
In the above theorem, Vp is the gradient of target function w.r.t. a model parameter with index p, Vl
is the average gradient of target function w.r.t. parameters in a layer with index l, and Vg is the global
average gradient of target function w.r.t. all model parameters. The proof of this theorem is given in
Appendix A.3. Notice that when we introduce κp,t instead of ap,t in Algorithm 1, the corresponding
gradients ∂0*(θ)^ will also be replaced by 热⑼ 凌％~1 = ∂KL- (θ) T(t).
p,t-1	p,t-1	p,t-1	p,t-1
5
Under review as a conference paper at ICLR 2021
We also made an analysis on the number of parameters and algorithm complexity (See Appendix A.4).
Our method will not increase the number of model parameters but requires extra space complexity ∆T
during training. Also it requires an extra time complexity but at least one-order smaller than training
with baseline setting, while the absolute ratio is smaller than the inverse of batch size ∆T << \/m&
3	Experiments
We use the feed-forward neural network models and different types of convolutions neural networks
on multiple benchmark datasets to compare with existing baseline optimizers.For each learning task,
the following optimizers will be applied: (a) standard baseline optimizers such as Adam and SGD;
(b) hyper-gradient descent in (Baydin et al., 2017); (c) L4 stepsize adaptation for standard optimizers
(Rolinek and Martius, 2018); (d) Adabound optimizer (Luo et al., 2018); (e) RAdam optimizer (Liu
et al., 2019); and (f) the proposed adaptive combination of different levels of hyper-descent. The
implementation of (b) is based on the code provided with the original paper. For each experiment, we
provide the average curve and standard error bar in each time step with ten runs.
3.1	Hyper-parameter Tuning
To compare the effect of CAM-HD with baseline optimizers, we first do hyperparameter tuning for
the model training process with baseline optimizers by referring to related papers (Kingma and Ba,
2015; Baydin et al., 2017; Rolinek and Martius, 2018; Luo et al., 2018) as well as implementing
an independent grid search. We mainly consider the hyper-parameters of batch size, learning rate,
and momentum for models with different architecture. The search space for batch size is the set of
{2n}n=3,...,9, while the search space for learning rate, hyper-gradient updating rate and combination
weight updating rate (CAM-HD-lr) are {10-1, ..., 10-4}, {10-1, ..., 10-10} and {10-1, ..., 10-4}
respectively. The selection criterion is the 5-fold cross-validation loss by early-stopping at the
patience of 3. The optimized hyper-parameters for the tasks in this paper are given in Table 1. For the
learning tasks with recommended learning rate schedules, we will apply these schedules as well.
Table 1: Hyperparameter Settings for Experiments
Architecture	Dataset	Batch size	lr (SGD/SGDN)	lr (Adam)	Hyper-grad lr (SGD/SGDN)	Hyper-grad lr (Adam)	CAM-HD-lr
MLP 1		32	-	0.0003	-	1.00E-07	0.01
MLP 2	MNIST	64	-	0.001	-	1.00E-07	0.01
MLP 3		128	-	0.001	-	1.00E-07	0.01
	MNIST	256	-	0.001	1.00E-03	1.00E-08	0.03
LeNet-5	CIFAR10	256	-	0.001	1.00E-03	1.00E-08	0.03
	SVHN	128	-	0.001	1.00E-03	1.00E-08	0.03
ResNet-18		256	0.1	0.001	1.00E-06	1.00E-08	0.001
ResNet-34	CIFAR10	256	0.1	0.001	1.00E-06	1.00E-08	0.001
3.2	Combination Ratio and Model Performances
First, we perform a study on the combination of different level learning rates. The simulations are
based on image classification tasks on MNIST and CIFAR10 (LeCun et al., 1998; Krizhevsky and
Hinton, 2012). One feed-forward neural network with three hidden layers of size [100, 100, 100] and
two convolutional network models, including LeNet-5 (LeCun et al., 2015) and ResNet-18 (He et al.,
2016), are implemented. We use full training sets of MNIST and CIFAR10 for training and full test
sets for validation. In each case, two levels of learning rates are considered, which are the global and
layer-wise adaptation for FFNN, and global and filter-wise adaptation for CNNs. Adam-CAM-HD
optimizer is implemented in all three simulations. We change the combination weights of two levels
in each case to see the change of model performance in terms of test classification accuracy at epoch
10, with the corresponding updating rate δ = 0. Another hyper-parameter setting follows Table 1.
We conduct ten runs at each combination weights with different parameter initializations for all three
simulations and draw the error bars for standard errors. The result is given in Figure 1. We can see
that in all three cases, the optimal performance is neither at full global level nor full layer/filter level,
but a combination of two levels of adaptive learning rates. Still, the differences between the endpoints
6
Under review as a conference paper at ICLR 2021
98.0
o
5
9 97.9
<
己 97.8
97.7
0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0
(a) Gamma (Global, FFNN, MNIST) (b) Gamma (Global, LeNet-5, CIFAR10)	(C) Gamma (Global, ReSNet-18, OFAR10)
Figure 1: The diagram of model performances (at epoch 10) trained by Adam-CAM-HD with
different fixed combination ratios in the case of two-level learning rates adaptation.
(a) Time SteP (Hidden size [100,100])
(b) Time Step (Hidden size [1000,100)
(c) Time Step (Hidden size [1000,1000])
Figure 2:	The comparison of learning curves of FFNN on MNIST with different adaptive optimizers.
and the optimal combination in terms of model performance have some statistical significance level.
This supports our analysis in Section 2.2. Also, in real training processes, it is possible that the
learning in favor of different combination weights in various stages and this requires the online
updating of the combination weights.
3.3	Feed Forward Neural Network for Image Classification
This experiment is conducted with feed-forward neural networks for image classification on MNIST,
including 60,000 training examples and 10,000 test examples. We use the full training set for training
and the full test set for validation. Three FFNN with three different hidden layer configurations are
implemented, including [100, 100], [1000, 100], and [1000, 1000]. Adaptive optimizers including
Adam, Adam-HD with two hyper-gradient updating rates, and proposed Adam-CAM-HD are applied.
For Adam-CAM-HD, we apply three-level parameter-layer-global adaptation with initialization
of γ1 = γ2 = 0.3 and γ3 = 0.4, and two-level layer-global adaptation with γ1 = γ2 = 0.5.
Figure 2 shows the validation accuracy for different optimizers during the training process of 30
epochs. We can learn that both the two-level and three-level Adam-CAM-HD outperform the baseline
Adam optimizer with optimized hyper-parameters significantly. For Adam-HD, we find that the
default hyper-gradient updating rate (10-7) for Adam applied in (Baydin et al., 2017) is not optimal
in our experiments, while an optimized one of 10-9 can outperform Adam but still worse than
Adam-CAM-HD with default hyper-gradient updating rate (10-7).
3.4	Lenet-5 for Image Classification
The second experiment is done with LeNet-5, an early-year convolutional neural network without
involving many building and training tricks. We compare a set of adaptive Adam optimizers including
Adam, Adam-HD, Adam-CAM-HD, and L4 for the image classification learning task of MNIST,
CIFAR10 and SVHN (Netzer et al., 2011). For Adam-CAM-HD, we apply a two-level setting with
filter-wise and global learning rates adaptation and initialize γ1 = 0.2, γ2 = 0.8. We also implement
an exponential decay function τ(t) = exp(-rt) as was discussed in Section 2.4 with rate r = 0.002,
while t is the number of iterations. For L4, we implement the recommended L4 learning rate of
0.15. For Adabound and RAdam, we also apply the recommended hyper-parameters in the original
7
Under review as a conference paper at ICLR 2021
papers. The other hyper-parameter settings are optimized in Table 1. As we can see in Figure 3,
(a) Epoch (MNIST)
(C) Epoch (SVHN)
Figure 3:	The comparison of learning curves of training LeNet-5 with different adaptive optimizers.
Adam-CAM-HD again shows the advantage over other methods in all the three sub-experiments,
except MNIST L4 that could perform better in a later stage. The experiment on CIFAR10 and
SVHN indicates that the recommended hyper-parameters for Adabound, RAdam and L4 could fail
in some cases with unstable accuracy curves. On the other hand, Adam-HD can not significantly
outperform Adam with the recommended and optimized hyper-gradient updating rate shared with
Adam-CAM-HD. The corresponding summary of test performance is given in Table 2, in which
the test accuracy of Adam-CAM-HD outperform other optimizers on both CIFAR10 and SVHN.
Especially, it gives significantly better results than Adam and Adam-HD for all the three datasets.
Table 2: Summary of test performances with LeNet-5
MNIST	CIFAR10	SVHN
	Test acc	Test S.E	Test acc	Test S.E	Test acc	Test S.E
Adam-CAM-HD	98.93	0.07	65.55	0.18	87.58	0.37
Adam-HD	98.83	0.05	63.3	0.66	86.94	0.13
Adam-L4	99.19	0.05	63.76	0.26	85.44	0.42
Adabound	99.11	0.05	59.79	0.70	87.22	0.14
RAdam	98.94	0.06	61.17	0.79	87.31	0.41
Adam	98.89	0.05	63.88	0.45	86.82	0.16
3.5 ResNet for Image Classification
In the third experiment, we apply ResNets for image classification task on CIFAR10. We compare
Adam and its adaptive optimizers, as well as SGD with Nestorov momentum (SGDN) and corre-
sponding adaptive optimizers for training both ResNet-18 and ResNet-34. For SGDN methods, we
apply a learning rate schedule, in which the learning rate is initialized to a default value of 0.1 and
reduced to 0.01 or 10% (for SGDN-CAM-HD) after epoch 150. The momentum is set to be 0.9 for
all SGDN methods. For Adam-CAM-HD SGDN-CAM-HD, we apply two-level CAM-HD with the
same setting as the second experiment. In addition, we apply an exponential decay function with a
decay rate r = 0.001. The validation accuracy results, training loss, and validation loss are shown
in Figure 4. We can see that the validation accuracy of Adam-CAM-HD reaches about 90% in 40
epochs and consistently outperforms Adam, L4 and Adam-HD optimizers in a later stage. The L4
optimizer with recommended hyper-parameter and an optimized weight-decay rate of 0.0005 (instead
of 1e-4 applied in other Adam-based optimizers) can outperform baseline Adam for both ResNet-18
and ResNet-34, while its training loss outperforms all other methods but with potential over-fitting.
Adam-HD achieves better training loss than Adam after epoch 100. However, we find that the
validation performance is not good with a default hyper-gradient coefficient of 10-8 (shared with
Adam-CAM-HD). Instead, an optimized coefficient of 10-9 can make a safe but small improvement
from Adam. RAdam performs slightly better than Adam-CAM-HD in terms of validation accuracy,
but the validation cross-entropy of both RAdam and Adabound are worse than our method. Also, we
find that in training ResNet-18/34, the validation accuracy and validation loss of SGDN-CAM-HD
slightly outperform SGDN in most epochs even after the resetting of the learning rate at epoch 150.
8
Under review as a conference paper at ICLR 2021
寸
T 90
80
70
6 60
αi 40
—J- RAdam
—J- Adam
-I- SGDN-CAM-HD
-I- SGDN
0	50	100	150	200
(a) Time Step (Validation Accuracy)
0	50	100	150	200
(d)	Time Step (Validation Accuracy)
sso-jAdotu0,sso,o
-f- Adam-CAM-HD
—J- Adam-HD
-J- Adam-L4
—J- Adabound
-√E- RAdam
—J- Adam
-f- SGDN-CAM-HD
SGDN
丁..丁
0	50	100	150	200
(b) Time Step (Training Loss)
-P Adam-CAM-HD
—J- Adam-HD
-J- Adam-L4
-∈- Adabound
—J- RAdam
—J- Adam
-J- SGDN-CAM-HD
-P SGDN
-Ξ- Adam-CAM-HD
—J- Adam-HD
-J- Adam-L4
—J- Adabound
—J- RAdam
—J- Adam
-Ξ- SGDN-CAM-HD
-J- SGDN
0	50	100	150	200
(e)	Time Step (Training Loss)
-I- Adam-CAM-HD
—J- Adam-HD
A Adam-L4
—J- Adabound
RAdam
—J- Adam
-I- SGDN-CAM-HD
0	50	100	150	200
(C) Time Step (Validation Loss)
-Ξ- Adam-CAM-HD
1.6
÷
1.4
÷
u 0.6
0.4-
0.2
Adam-HD
Adam-L4
Adabound
H2
R 1.0
—J- RAdam
—J- Adam
-I- SGDN-CAM-HD
SGDN
(f) Time Step (Validation Loss)
SSol Ado.nuxso」。
5 50
Figure 4:	The learning curves of training ResNet on CIFAR10 with adaptive optimizers.
The test performances of dif- ferent optimizers for ResNet-18	Table 3: Summary of test performances with ResNet-18/34				
and ResNet-34 after 200 epoch		ReSNet-18		ReSNet-34	
of training are shown in Ta-				—	
ble 3. Notice that the results		Test acc	TeSt S.E	TeSt acc	TeSt S.E
of SGDN and SGDN-CAM-HD	Adam	86.94	0.13	87.87	0.2
are achieved with a piece-wise	Adam-HD	87.26	0.35	88.48	0.48
constant learning rates schedule,	Adam-L4	87.81	0.22	88.02	0.15
while the results of Adam-based	Adabound	90.29	0.15	90.15	0.30
optimizers are achieved without	RAdam	91.54	0.17	91.76	0.28
a learning rate schedule. We	Adam-CAM-HD	90.10	0.23	90.18	0.06
can learn that the proposed CAM-	SGDN	93.04	0.23	92.93	0.19
HD method can improve the	SGDN-CAM-HD	93.2	0.24	93.47	0.23
corresponding baseline method					
(Adam, Adam-HD and SGDN)					
with statistical significance in almost every case. Adam-CAM-HD performs a bit worse than RAdam
but comparable to Adabound in terms of the average test accuracy for both ResNet-18 and ResNet-34.
As a higher-level adaptation method, CAM-HD can be applied on top of RAdam/Adabound/L4 for
further improvement.
4 Conclusion
In this study, we propose a gradient-based learning rate adaptation strategy by introducing hierarchical
multiple-level learning rates in deep neural networks. By considering the relationship between regu-
larization and the combination of adaptive learning rate at different levels, we further propose a joint
algorithm for adaptively learning each level’s combination weight. Experiments on FFNN, LeNet-5,
and ResNet-18/34 indicate that the proposed methods can outperform the standard ADAM/SGDN and
other baseline methods with statistical significance. Although the advantage is not fully guaranteed,
our method achieves a higher adaptation level and can be continuously reduced to baseline methods
under a specific set of hyper-parameters. This could bring more thoughts and further study on
implementing a hierarchical learning rate system for deep neural networks.
9
Under review as a conference paper at ICLR 2021
References
L.	B. Almeida, T. Langlois, J. D. Amaral, and A. Plakhov. Parameter adaptation in stochastic
optimization. On-Line Learning in Neural Networks, Publications of the Newton Institute, pages
111-134,1998.
M.	Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, B. Shillingford, and
N. De Freitas. Learning to learn by gradient descent by gradient descent. In NeurIPS, pages
3981-3989, 2016.
A. G. Baydin, R. Cornish, D. M. Rubio, M. Schmidt, and F. Wood. Online learning rate adaptation
with hypergradient descent. ICLR, 2017.
A. G. Baydin, B. A. Pearlmutter, A. A. Radul, and J. M. Siskind. Automatic differentiation in
machine learning: a survey. JMLR, 18(153), 2018.
J.	Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. JMLR, 12:2121-2159, 2011.
L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperpa-
rameter optimization. In ICML, pages 1165-1173. JMLR. org, 2017.
K.	He and J. Sun. Convolutional neural networks at constrained time cost. In CVPR, pages 5353-5360,
2015.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages
770-778, 2016.
H. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-gradient methods
under the Polyak-IojasieWicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 795-811. Springer, 2016.
D. P. Kingma and J. Ba. Adam: A method for stochastic oPtimization. ICLR, 2015.
A. Krizhevsky and G. Hinton. Learning multiPle layers of features from tiny images. University of
Toronto, 2012.
H. Lang, L. Xiao, and P. Zhang. Using statistics to automate stochastic oPtimization. In Advances in
Neural Information Processing Systems, Pages 9540-9550, 2019.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning aPPlied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Y. LeCun et al. Lenet-5, convolutional neural netWorks. URL: http://yann. lecun. com/exdb/lenet, 20:
5, 2015.
Z. Li and S. Arora. An exPonential learning rate schedule for deeP learning. In International
Conference on Learning Representations, 2019.
L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han. On the variance of the adaPtive learning
rate and beyond. In ICLR, 2019.
I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent With Warm restarts. 2017.
L. Luo, Y. Xiong, Y. Liu, and X. Sun. AdaPtive gradient methods With dynamic bound of learning
rate. In ICLR, 2018.
K. Lv, S. Jiang, and J. Li. Learning gradient descent: Better generalization and longer horizons. In
ICML, Pages 2247-2255. JMLR. org, 2017.
D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyPerParameter oPtimization through
reversible learning. In ICML, Pages 2113-2122, 2015.
10
Under review as a conference paper at ICLR 2021
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images
with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised
Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/housenumbers/
nips2011_housenumbers.pdf.
B. O’donoghue and E. Candes. Adaptive restart for accelerated gradient schemes. Foundations of
computational mathematics,15(3):715-732, 2015.
S. J. Reddi, S. Kale, and S. Kumar. On the convergence of adam and beyond. In ICLR, 2018.
M.	Rolinek and G. Martius. L4: Practical loss-based stepsize adaptation for deep learning. In
NeurIPS, pages 6433-6443, 2018.
S. Ruder. An overview of gradient descent optimization algorithms. arXiv:1609.04747, 2016.
N.	N. Schraudolph. Local gain adaptation in stochastic gradient descent. In 1999 Ninth International
Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), volume 2, pages
569-574 vol.2, 1999.
R. Sun. Optimization for deep learning: theory and algorithms. arXiv:1912.08957, 2019.
R. S. Sutton. Gain adaptation beats least squares. In Proceedings of the 7th Yale workshop on
adaptive and learning systems, volume 161168, 1992.
T. Tieleman and G. Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. coursera: Neural networks for machine learning. Tech. Rep., Technical report, page 31,
2012.
O. Wichrowska, N. Maheswaranathan, M. W. Hoffman, S. G. Colmenarejo, M. Denil, N. de Freitas,
and J. Sohl-Dickstein. Learned optimizers that scale and generalize. In ICML, pages 3751-3760.
JMLR. org, 2017.
J. Yu, D. Aberdeen, and N. N. Schraudolph. Fast online policy gradient learning with smd gain vector
adaptation. In NeurIPS, pages 1185-1192, 2006.
M. Zhang, J. Lucas, J. Ba, and G. E. Hinton. Lookahead optimizer: k steps forward, 1 step back. In
NeurIPS, pages 9593-9604, 2019.
P. Zhang, H. Lang, Q. Liu, and L. Xiao. Statistical adaptive stochastic gradient methods.
arXiv:2002.10597, 2020.
11
Under review as a conference paper at ICLR 2021
A Appendix for Paper: Adaptive Multi-level Hyper-gradient
Descent
A.1	Analysis about Virtual Approximation
Consider the difference between Eq. (5) and Eq. (6) in the paper:
αl,t - al,t = -2βλlayer((αl,t - αg,t) - (αl,t - αg,t)).	(9)
Based on the setting of multi-level adaptation, on the right-hand side of Eq. (9), global learning rate
is updated without regularization ɑg,t = αg,t. For the layer-wise learning rates, the difference is
given by aι,t - αι,t = 2βλlayer(αι,t - αg,t), which corresponds to the gradient with respect to the
regularization term. Thus, Eq. (9) can be rewritten as:
αl,t - al,t = -2βλlayer(2βλlayer(αl,t - αg,t)) = -4β2λ2(I - Og^)αl,t	(IO)
which is the error of the virtual approximation introduced in Eq. (6) in the paper. If 4β2λι2 << 1 or
ααg,t → 1, this approximation becomes accurate. Another way for handling Eq. (4) in the paper is to
implement the previous-step learning rates in the regularization term.
αl,t ≈ αl,t-1 - β(-hl,t-1 + 2λlayer(αl,t-1 - αg,t-1)).	(11)
Since we have αl,t = α^l,t - 2βλlayer(αl,t - αg,t) and o^l,t = αl,t-ι + βhl,t-ι, using the learning rates
in the last step for regularization will introduce a higher variation from term βhl,t-1, with respect
to the true learning rates in the current step. Thus, we consider the proposed virtual approximation
works better than last-step approximation.
A.2 Proof of Theorem 1:
Proof. Consider the learning regularizer
Llueg(α) = XX X	λij (a,-.3 )2.	(12)
p∈P αi ∈p αj ∈parents(i)
To apply hyper-gradient descent method to update the learning rate αL at level L, we need to work out
the derivative of Llr_reg with respect to αL. The terms in Eq. (12) involving αL are only (αi - αj)2
where αj is an ancestor on the path from the root to the leave node αL . Hence
∂Lfull (θ, a)
∂θL,t
dLmodel(θ, a)
∂θL,t
+ dLlr reg (O)
∂θL,t
-VθLf (θt-i)τVθLu(Θt-2,at-i)+	E	2λLj(aL,t- Oj,t).
αj ∈acenstors(L)
(13)
As there are exactly L - 1 ancestors on the path, we can simply use the index j = 1, 2, ..., L - 1.
The corresponding updating function for an,t is:
L-1
aL,t = an,t-1 - β(hL +	2λLj (aL,t - aj,t))
j=1
L-1	L-1
≈ 0L,t(1 - 2β X λLjOn,t) + X(2βλLjOj,t))
j=1	j=1
L
=EYj aj,t.
j=1
where
L-1
γL = 1 - 2β X λLj ,
j=1
γj = 2βλLj,	forj = 1,2,...,L- 1.
This form satisfies aL = Pjι Yj Oj with Pjι Yj = 1. This completes the proof.
(14)
(15)
□
12
Under review as a conference paper at ICLR 2021
A.3 Proof of Theorem 2:
Proof. We take three-level’s case discussed in Section 2 for example, which includes global level,
layer-level and parameter-level. Suppose that the target function f is convex, L-Lipschitz smooth at
all levels, which means for all θ1 and θ2 :
||Vpf(θι) -Vpf(Θ2)∣∣≤ Lp∣∣θι- Θ2∣∣
∣∣Wf(θ1)-Wf(θ2)∣∣≤ Ll∣∣θι- Θ2∣∣
IIVgf(θι)-Vgf(θ2)∣∣≤ Lg∣∣Θ1-Θ2∣∣
L = max{Lp,Ll,Lg}
(16)
and its gradient with respect to parameter-wise, layer-wise, global-wise parameter groups satisfy
kVp f (θ)k < Mp, kVlf (θ)k < Ml, kVg f (θ)k < Mg for some fixed Mp, Ml, Mg and all θ. Then
the effective combined learning rate for each parameter satisfies:
|ap,t| = Iγp,t-1αp,t + γl,t-1αl,t + γg,t-1αt∣
t-1
≤ (γp,t-1 + γl,t-1 + γg,t-1)α0 + β	γp,t-1npmax{IVf(θp,i+1)TVf(θp,i)I}
i=0	p
+γl,t-1nlmax{IVf(θl,i+1)TVf(θl,i)I}+γg,t-1IVf(θg,i+1)TVf(θg,i)I
t-1
≤ α0 + β	γp,t-1npmax{kVf(θp,i+1)kkVf(θp,i)k}
i=0	p
+γl,t-1nlmlax{kVf(θl,i+1)kkVf(θl,i)k}+γg,t-1kVf(θg,i+1)kkVf(θg,i)k
≤ α0 + tβ(npMp2 + nl Ml2 + Mg2 )
(17)
where θp,i refers to the value of parameter indexed by p at time step i, θl,i refers to the set/vector of
parameters in layer with index l at time step i, and θg,i refers to the whole set of model parameters at
time step i. In addition, np and nl are the total number of parameters and number of the layers, and
we have applied 0 < γp, γl, γg < 1. This gives an upper bound for the learning rate in each particular
time step, which is O(t) as t → ∞. By introducing κp,t = T(t)αp,t + (1 一 T(t))α∞, where the
function τ(t) is selected to satisfy tτ (t) → 0 as t → ∞, so we have κp,t → α∞ as t → ∞. If
α∞ < L, for larger enough t, We have 1/(L +1) < κp,t < 1/L, and the algorithm converges when
the corresponding gradient-based optimizer converges for such a learning rate under our assumptions
about f. This follows the discussion in (Karimi et al., 2016; Sun, 2019).	□
A.4 Algorithm Complexity
A.4. 1 Number of parameters and space complexity
The proposed adaptive optimizer is for efficiently updating the model parameters, while the final
model parameters will not be increase by introducing CAM-HD optimizer. However, during the
training process, several extra intermediate variables are introduced. For example, in the discussed
three-level’s case for feed-forward neural network with nlayer layers, we need to restore hp,t, hl,t and
hg,t, which have the sizes of S(hp,t) = Pln=lay1er-1(nl + 1)nl+1, S(hl,t) = nlayer and S(hg,t) = 1,
respectively, where ni is the number of units in ith layer. Also, learning rates αp,t, αl,t, αg,t and
take the sizes of S(ap,t ) = Pl=y1	(nl + 1)nl+1 , S(al,t) = nlayer, S(ag,t) = 1, S(ag,t ) = 1, and
S(ap,t) = Pn=IerT(nl + 1)nl+1, respectively. Also we need a small set of scalar parameters to
restore γ1, γ2 and γ3 and other coefficients.
Consider the fact that in training the baseline models, we need to restore model parameters,
corresponding gradients, as well as the intermediate gradients during the implementation of chain
rule, CAM-HD will take twice of the space for storing intermediate variables in the worst case.
For two-level learning rate adaptation considering global and layer-wise learning rates, the extra
space complexity by CAM-HD will be one to two orders’ smaller than that of baseline model during
training.
13
Under review as a conference paper at ICLR 2021
A.4.2 Time Complexity
In CAM-HD, we need to calculate gradient of loss with respect to the learning rates at each level,
which are hp,t, hl,t and hg,t in three-level’s case. However, the gradient of each parameter is already
known during normal model training, the extra computational cost comes from taking summations
and updating the lowest-level learning rates. In general, this cost is in linear relation with the number
of differentiable parameters in the original models. Here we discuss the case of feed-forward
networks and convolutional networks.
Recall that for feed-forward neural network the whole computational complexity is:
nlayer
T(n) = O(m ∙ niter ∙ Enl ∙ ni-1 ∙ ni-2)
l=2
(18)
where m is the number of training examples, niter is the iterations of training, and nl is the number of
units in the l-th layer. On the other hand, when using three-level CAM-HD with, where the lowest
level is parameter-wise, we need nlayer element products to calculate hp,t for all layers, one nlayer
matrix element summations to calculate hl,t for all layers, as well as a list summation to calculate
hg,t. In addition, two element-wise summations will also be implemented for calculating αp,t and
αp. Therefore, the extra computational cost of using CAM-HD is ∆T(n) = O(n ∙ n^r Pnn=^ (nl ∙
nl-1 + nl)), where nb is the number of mini-batches for training. Notice that mb = m/nb is the
batch size, which is usually larger than 100. This extra cost is more than one-order smaller than
the computational complexity of training a model without learning rate adaptation. For the cases
when the lowest level is layer-wise, only one element-wise matrix product is needed in each layer to
calculate hl,t. For convolutional neural networks, we have learned that the total time complexity of
all convolutional layers is (He and Sun, 2015):
nconv_layer
O(nb ∙ niter ∙ E (nl-1 ∙ s2 ∙ nl ∙ m2))
l=1
(19)
where l is the index of a convolutional layer, and nconv_layer is the depth (number of convolutional
layers). nl is the number of filters in the l-th layer, while nl-1 is known as the number of input
channels of the l-th layer. sl is the spatial size of the filter. ml is the spatial size of the output feature
map. If we consider convolutional filters as layers, the extra computational cost for CAM-HD in this
case is ∆T(n) = O(n ∙ n^ PncInv_layer ((nl-ι ∙ s2 + 1) ∙ nl)), which is still more than one order
smaller than the cost of model without learning rate adaptation.
Therefore, for large networks, applying CAM-HD will not significantly increase the computational
cost from the theoretical prospective.
14
Under review as a conference paper at ICLR 2021
A.5 Supplementary Experimental Results
A.5.1 Learning of combination weights
The following figures including Figure 5, Figure 6, Figure 7 and Figure 8 give the learning curves
of combination weights with respect to the number of training iterations in each experiments, in
which each curve is averaged by 5 trials with error bars. Through these figures, we can compare
the updating curves with different models, different datasets and different CAM-HD optimizers.
Figure 5: Learning curves of γs for FFNN on MNIST with Adam.
0.7-
0.6-
0.5-
0.4-
0.3-
(b) Iteration (LeNet-5, SGDN, MNIST)
0	200	400	600	800
(c) Iteration (LeNet-5, Adam, MNIST)
Figure 6: Learning curves of γs for LeNet-5 on MNIST with SGD, SGDN and Adam.
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1000	1500	2000
(a) Iteration (LeNet-5, Adam, CIFAR10)
Figure 7: Learning curves of γs for LeNet-5 on CIFAR10 and SVHN.
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0
1000
2000
(b) Iteration (LeNet-5, Adam, SVHN)
Figure 5 corresponds to the experiment of FFNN on MNIST in Section 3.3, which is a three-level
case. We can see that for different FFNN architecture, the learning behaviors of γs also show different
patterns, although trained on a same dataset. Meanwhile, the standard errors for multiple trials are
much smaller relative to the changes of the average combination weight values.
Figure 6 corresponds to the learning curves of γs in the experiments of LeNet-5 for MNIST image
classification with SGD, SGDN and Adam, which are trained on 10% of original training dataset.
15
Under review as a conference paper at ICLR 2021
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0	2000	4000	6000	8000	0	2000	4000	6000	8000
(a) Iteration (OFAR10, ReSNet-18, SGDN) (b) Iteration (OFAR10, ReSNet-18, Adam)
Figure 8: Learning curves of γs for ResNet-18.
In addition, Figure 7 corresponds to the learning curves of γs in the experiments of LeNet-5 for
CIFAR10 and SVHN image classification with Adam-CAM-HD.
As is shown in Figure 6, for SGD-CAM-HD, SGDN-CAM-HD and Adam-CAM-HD, the equilibrium
values of combination weights are different from each other. Although the initialization γ1 = 0.2,
γ2 = 0.8 and the updating rate δ = 0.03 are set to be the same for the three optimizers, the values
of γ1 and γ2 only change in a small proportion when training with Adam-CAM-HD, while the
change is much more significant towards larger filter/layer-wise adaptation when SGD-CAM-HD
or SGDN-CAM-HD is implemented. The numerical results show that for SGDN-CAM-HD, the
average value of weight for layer-wise adaptation γ1 jumps from 0.2 to 0.336 in the first epoch, then
drop back to 0.324 before keeping increasing till about 0.388. For Adam-CAM-HD, the average
γ1 moves from 0.20 to 0.211 with about 5% change. In Figure 7, both the two subplots are models
trained with Adam-CAM-HD. For the updating curves in Figure 7(a), which is trained on CIFAR10
with Adam-CAM-HD, the combination weight for filter-wise adaptation moves from 0.20 to 0.188.
Meanwhile, for the updating curves in Figure 7(b), which is trained on SVHN, the combination
weight for filter-wise adaptation moves from 0.20 to 0.195.
The similar effect can also be observed from the learning curves of γs for ResNet-18, which is given
in Figure 8 and we only take the first 8,000 iterations. Again, we find that in training ResNet-18
on CIFAR10, the combination weights of SGD/SGDN-CAM-HD change much faster than that of
Adam-CAM-HD. There are several reasons for this effect: First, in the cases when γs do not move
significantly, we apply Adam-CAM-HD, where the main learning rate (1e-3) is only about 1%-6% of
the learning rate of SGD or SGDN (1e-1). In Algorithm 1, we can see that the updating rate of γs
is in proportion of alpha given other terms unchanged. Thus, for the same tasks, if the same value
of updating rate δ is applied, the updating scale of γs for Adam-CAM-HD can be much smaller
than that for SGDN-CAM-HD. Second, this does not mean that if we apply a much larger δ for
Adam-CAM-HD, the combination weights will still not change significantly or the performance will
not be improved. It simply means that using a small δ can also achieve good performance due to the
goodness of initialisation points. Third, it is possible that Adam requires lower level of combination
ratio adaptation for the same network architecture compared with SGD/SGDN due to the fact that
Adam itself involves stronger adaptiveness.
A.5.2 Other Experimental Results
In Figure 2, Figure 3 and Figure 4 of the paper, we have shown the curves of validation accuracies
to compare different adaptive optimizers in a variety of learning tasks. Here we further provide the
training and validation cross-entropy loss curves for corresponding methods in these tasks. Figure 8
is the full results of FFNNs, and Figure 9 is the results of LeNet-5.
16
Under review as a conference paper at ICLR 2021
1.62 -
1.60
1.58
1.56
1.54
1.52 -
1.50
1.48
0	5	10	15	20	25	30
Adam-CAM-HD (3-level)
Adam-CAM-HD (2-level)
Adam-HD (hp-grad 1e-9)
Adam-HD (hp-grad 1e-7)
Adam
98-
§97-
§ 96-
0)
« 95-
0)
9 94-
%3-
0	5	10	15	20	25	30
(a) Time Step (Validation Accuracy)
(d) Time Step (Validation Accuracy)
一壬壬"t-
I、
Adam-CAM-HD (3-level)
Adam-CAM-HD (2-level)
Adam-HD (hp-grad 1e-9)
Adam-HD (hp-grad 1e-7)
Adam
5	10	15	20	25	30
(g) Time Step (Validation Accuracy)
1.54
1.52 -
1.50
1.48
(h) Time Step (Training Loss)
1.53
1.52
1.51
1.50
1.49
1.48
1.510
1.505
1.500
1.495
1.490
1.485
1.480
1.510
1.505
1.500
1.495
1.490
1.485
1.480
(c) Time Step (Validation Loss)
⅛-⅛-4u
Adam-CAM-HD (3-level)
Adam-CAM-HD (2-level)
Adam-HD (hp-grad 1e-9)
Adam-HD (hp-grad 1e-7)
Adam
I ʃ I	IiVT
0	5	10	15	20	25	30
(f) Time Step (Validation Loss)
Figure 9: The comparison of learning curves of FFNN on MNIST with different adaptive optimizers.
17
Under review as a conference paper at ICLR 2021
(a) Epoch (Validation Accuracy)
65
60
I55
O 50
45
40
z∙^
壬壬 壬 壬
Adam-CAM-HD
Adam-HD (2-level)
Adam-L4
Adabound
RAdam
Adam
2	4	6	8	10
(e) Epoch (Training Loss)
Adam-CAM-HD
Adam-HD (2-level)
Adam-L4
Adabound
RAdam
Adam
2	4	6	8	10
(d) Epoch (Validation Accuracy)
2	4	6	8	10
(f) Epoch (Validation Loss)
壬 Adam-CAM-HD
-⅛- Adam-HD (2-level)
-⅛- Adam-L4
一壬-Adabound
-壬-RAdam
一壬-Adam
-⅜--⅞-壬-⅜-
Adam-CAM-HD
Adam-HD (2-level)
Adam-L4
Adabound
RAdam
Adam
2	4	6	8
(i) Epoch (Validation Loss)
2	4	6	8	10
(g) Epoch (Validation Accuracy)
2	4	6	8	10
(h) Epoch (Training Loss)
Figure 10: The comparison of learning curves of training LeNet-5 on MNIST, CIFAR10 and SVHN
with different adaptive optimizers.
18