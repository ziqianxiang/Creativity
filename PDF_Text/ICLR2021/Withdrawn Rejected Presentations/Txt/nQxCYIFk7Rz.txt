Under review as a conference paper at ICLR 2021
Multiple Descent: Design Your Own General-
ization Curve
Anonymous authors
Paper under double-blind review
Ab stract
This paper explores the generalization loss of linear regression in variably parame-
terized families of models, both under-parameterized and over-parameterized. We
show that the generalization curve can have an arbitrary number of peaks, and
moreover, locations of those peaks can be explicitly controlled. Our results high-
light the fact that both classical U-shaped generalization curve and the recently
observed double descent curve are not intrinsic properties of the model family. In-
stead, their emergence is due to the interaction between the properties of the data
and the inductive biases of learning algorithms.
1	Introduction
The main goal of machine learning methods is to provide an accurate out-of-sample prediction,
known as generalization. For a fixed family of models, a common way to select a model from this
family is through empirical risk minimization, i.e., algorithmically selecting models that minimize
the risk on the training dataset. Given a variably parameterized family of models, the statistical
learning theory aims to identify the dependence between model complexity and model performance.
The empirical risk usually decreases monotonically as the model complexity increases, and achieves
its minimum when the model is rich enough to interpolate the training data, resulting in zero (or near-
zero) training error. In contrast, the behaviour of the test error as a function of model complexity
is far more complicated. Indeed, in this paper we show how to construct a model family for which
the generalization curve can be fully controlled (away from the interpolation threshold) in both
under-parameterized and over-parameterized regimes. Classical statistical learning theory supports
a U-shaped curve of generalization versus model complexity (Geman et al., 1992; Hastie et al.,
2009). Under such a framework, the best model is found at the bottom of the U-shaped curve,
which corresponds to appropriately balancing under-fitting and over-fitting the training data. From
the view of the bias-variance trade-off, a higher model complexity increases the variance while
decreasing the bias. A good choice of model complexity achieves a relatively low bias while still
keeping the variance under control. On the other hand, a model that interpolates the training data is
deemed to over-fit and tends to worsen the generalization performance due to the soaring variance.
Although classical statistical theory suggests a pattern of behavior for the generalization curve up to
the interpolation threshold, it does not describe what happens beyond the interpolation threshold,
commonly referred to as the over-parameterized regime. This is the exact regime where many
modern machine learning models, especially deep neural networks, achieved remarkable success.
Indeed, neural networks generalize well even when the models are so complex that they have the
potential to interpolate all the training data points (Zhang et al., 2017; Belkin et al., 2018b; Ghorbani
et al., 2019; Hastie et al., 2019).
Modern practitioners commonly deploy deep neural networks with hundreds of millions or even
billions of parameters. It has become widely accepted that large models achieve performance supe-
rior to small models that may be suggested by the classical U-shaped generalization curve (Bengio
et al., 2003; Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016; Huang et al., 2019). This
indicates that the test error decreases again once model complexity grows beyond the interpolation
threshold, resulting in the so called double-descent phenomenon described in (Belkin et al., 2018a),
which has been broadly supported by empirical evidence (Neyshabur et al., 2015; Neal et al., 2018;
Geiger et al., 2019; 2020) and confirmed empirically on modern neural architectures by Nakkiran
et al. (2019). On the theoretical side, this phenomenon has been recently addressed by several works
1
Under review as a conference paper at ICLR 2021
on various model settings. In particular, Belkin et al. (2019a) proved the existence of double-descent
phenomenon for linear regression with random feature selection and analyzed the random Fourier
feature model (Rahimi & Recht, 2008). Mei & Montanari (2019) also studied the Fourier model and
computed the asymptotic test error which captures the double-descent phenomenon. Bartlett et al.
(2020); Tsigler & Bartlett (2020) analyzed and gave explicit conditions for “benign overfitting” in
linear and ridge regression, respectively. In a recent work, Caron & Chretien (2020) provided a
finite sample analysis of the nonlinear function estimation and showed that the parameter learned
through empirical risk minimization converges to the true parameter with high probability as the
model complexity tends to infinity, implying the existence of double descent.
Among all the aforementioned efforts, one particularly interesting question is whether one can ob-
serve more than two descents in the generalization curve. In a recent work, d’Ascoli et al. (2020)
empirically showed a sample-wise triple-descent phenomenon under the random Fourier feature
model. Similar triple-descent was also observed for linear regression (Nakkiran et al., 2020). More
rigorously, Liang et al. (2020) presented an upper bound on the risk of the minimum-norm inter-
polation versus the data dimension in Reproducing Kernel Hilbert Spaces (RKHS), which exhibits
multiple descent. However, a multiple-descent upper bound without a properly matching lower
bound does not imply the existence of a multiple-descent generalization curve. In this work, we
study the multiple descent phenomenon by addressing the following questions:
•	Can the existence of a multiple descent generalization curve be rigorously proven?
•	Can an arbitrary number of descents occur?
•	Can the generalization curve and the locations of descents be designed?
In this paper, we show that the answer to all three of these questions is yes. Further related work is
presented in Appendix A.
Our Contribution. We consider the linear regression model and analyze how the risk changes as
the dimension of the data grows. In the linear regression setting, the data dimension is equal to the
dimension of the parameter space, which reflects the model complexity. We rigorously show that
the multiple descent generalization curve exists under this setting. To our best knowledge, this is the
first work proving a multiple descent phenomenon for any learning model.
Our analysis considers both the underparameterized and overparameterized regimes. In the over-
parameterized regime, we show that one can control where a descent or an ascent occurs in the
generalization curve. This is realized through our algorithmic construction of a feature-revealing
process. To be more specific, we assume that the data is in RD , where D can be arbitrarily large
or even essentially infinite. We view each dimension of the data as a feature. We consider a linear
regression problem restricted on the first d features, where d < D. New features are revealed by
increasing the dimension of the data. We then show that by specifying the distribution of the newly
revealed feature to be either a standard Gaussian or a Gaussian mixture, one can determine where an
ascent or a descent occurs. In order to create an ascent when a new feature is revealed, it is sufficient
that the feature follows a Gaussian mixture distribution. In order to have a descent, it is sufficient
that the new feature follows a standard Gaussian distribution. Therefore, in the overparameterized
regime, we can fully control the occurrence of a descent and an ascent. As a comparison, in the
underparameterized regime, the generalization loss always increases regardless of the feature distri-
bution. We also consider a dimension-normalized version of the generalization loss, under which
we show that the generalization curve exhibits multiple descent in the underparameterized regime.
Generally speaking, we show that we are able to design the generalization curve.
On the one hand, we show theoretically that the generalization curve is malleable and can be con-
structed in an arbitrary fashion. On the other hand, we rarely observe complex generalization curves
in practice, besides carefully curated constructions. Putting these facts together, we arrive at the
conclusion that realistic generalization curves arise from specific interactions between properties of
typical data and the inductive biases of algorithms. We should highlight that the nature of these
interactions is far from being understood and should be an area of further investigations.
2	Preliminaries and Problem Formulation
Notation. For x ∈ RD and d ≤ D, we let x[1 : d] ∈ Rd denote a d-dimensional vector with x[1 :
d]i = xi for all 1 ≤ i ≤ d. For a matrix A ∈ Rn×d, we denote its Moore-Penrose pseudoinverse by
2
Under review as a conference paper at ICLR 2021
A+ ∈ Rd×n . We use the big O notation O and write variables in the subscript of O if the implicit
constant depends on them. For example, On,d,σ(1) is a constant that only depends on n, d, and σ. If
f (σ) and g(σ) are functions of σ, write f (σ)〜g(σ) if lim f(σ) = 1. It Will be given in the context
how we take the limit.
Distributions. Let N(μ, σ2) (μ, σ ∈ R) and N(μ, Σ) (μ ∈ Rn, Σ ∈ Rn×n) denote the univariate
and multivariate Gaussian distributions, respectively, where μ ∈ Rn and Σ ∈ Rn×n is a positive
semi-definite matrix. We define a family of trimodal Gaussian mixture distributions as follows
Nmix，3N(0,σ2) + 3N(-μ,σ2) + 1 N(μ,σ2).
For an illustration, please see Fig. 1.
0.8
0.7
0.6
0.5
X 0.4
0.3
0.2
0.0
X
(a) N(0, 1) feature	(b) Nσm,i1x feature, (σ = 0.3)	(c) Nσm,i1x feature, (σ = 0.2)
Figure 1: Density functions oftheN(0, 1) and Nσm,1ix feature. A new entry is independently sampled
from the 1-dimensional distribution being either a standard Gaussian or trimodal Gaussian mixture.
Smaller σ leads to higher concentration around each modes.
Let χ2 (k, λ) denote the noncentral chi-squared distribution with k degrees of freedom and the non-
centrality parameter λ. For example, if Xi 〜N(μi, 1) (for i = 1,2,...,k) are independent Gaus-
sian random variables, we have Pk=I XIi 〜χ2(k,λ), where λ = Pk=I μ2∙ We also denote
by χ2 (k) the (central) chi-squared distribution with k degrees and the F -distribution by F (d1, d2)
where d1 and d2 are the degrees of freedom.
Problem Setup. Let x1 , . . . , xn ∈ RD be column vectors that represent the training data of size
n and let xtest ∈ RD be a column vector that represents the test data. We assume that they are all
independently drawn from a distribution
iid
Xl,...,Xn,Xtest 〜D ∙
Let us consider a linear regression problem on the first d features, where d ≤ D for some arbitrary
large D. Here, d can be viewed as the number of features revealed. The design matrix A equals
[xι[1 : d],...,xn[1 : d]]> ∈ Rn×d. The true linear model is β* ∈ Rd. The noise ε ∈ Rn follows
the multivariate standard Gaussian distribution N(0, η2In). Let x = xtest[1 : d] denote the first d
features of the test data.
For the underparameterized regime where d < n, the least square solution on the training data is
A+ (Ae* + ε). For the overparameterized regime where d > n, A+ (Aβ* + ε) is the minimum-norm
solution. In both regimes we consider the solution β，A+(Aβ* + ε). The excess generalization
loss on the test data is then given by
Ld，E [(y - χ>β)2 -(y - χ>β*)2
=E 卜(β — β*))]
=E [(x> ((A+A - I)β* + A+ε))2i
=E(x>(A+A-I)β*)2 + E (x>A+ε)2
= E (x>(A+A -I)β*)2 + η2E (A>)+x2 ,
3
Under review as a conference paper at ICLR 2021
where y = χ>β* + εtest and εtest 〜N(0, η2). We call the term E [(χ>(A+A - I)β*)2] the bias
and call the term η2 E (A> )+ x2 the variance.
In this paper, We assume β* = 0 and the noise level η = 1. In this settings, We get
Ld = Ek(A>)+xk2.
Remark 1. In the underparametrized regime, if D is a continous distribution (our construction
presented later satisfies this condition), the matrix A has independent column almost surely. In this
case, We have A+A = I and therefore the bias E (x>(A+A - I)β*)2 vanishes irrespective of the
true linear model β*. In other Words, in the underparametrized regime, Ld equals η2Ek(A>)+xk2
for all β * .
We Would like to study the change in the loss caused by the groWth in the number of features re-
vealed. Note that the product (A+)>x sums over d dimensions. Once We reveal aneW feature, Which
>	>	A> + x
is equivalent to adding a neW roW b> to A> and a neW component y to x, the product b> y
sums over d + 1 dimensions. As a result, to compare quantities of different dimensions, We need to
normalize the generalization loss by the dimension. We define the dimension-normalized general-
ization loss L0d as folloWs
Ld，E d(A>)+χ
=⅛ Ld.
Local Maximum and Multiple Descent. We say that a local maximum occurs at a dimension d ≥ 1
if L0d-1 < L0d and L0d > L0d+1. Intuitively, a local maximum occurs if there is an increasing stage
of the generalization loss, folloWed by a decreasing stage, as the dimension d groWs. Additionally,
We define L00 , -∞. If the generalization loss exhibits a single descent, based on our definition, a
unique local maximum occurs atd = 1. For a double-descent generalization curve, a local maximum
occurs at tWo different dimensions. In general, if We observe a local maximum at K different
dimensions We call it a K-descent.
3 Underparameterized Regime
First, We present our main theorem for the underparametrized regime beloW, Whose proof is deferred
to the end of Section 3. It states that the un-normalized generalization loss Ld is alWays non-
decreasing as d groWs. Moreover, it is possible to have an arbitrarily large ascent, i.e., Ld+1 - Ld >
C for any C > 0.
Theorem 1 (Proof in Appendix B.1). If d < n, we have Ld+1 ≥ Ld irrespective of the data
distribution. Moreover, for any C > 0, there exists a distribution D such that Ld+1 - Ld > C.
For the dimension-normalized generalization loss L0d, there can be both ascents and descents. And
it is possible to specify Where the local peaks in the generalization curve occur.
Theorem 2 (Underparameterized regime). Let D + 2 < √2n. For any 1 < di < d2 < •…< dκ <
D where dj+1 - dj ≥ 2, there exists a distribution D such that a local maximum of the L0d curve
occurs at dj.
Note that the assumption dj+1 -dj ≥ 2 is necessary because tWo local maxima may not be adjacent.
We present an example in Fig. 2.
Remark 2 (D can be a product distribution). As Will be clear later in the proof of Theorem 2, the
distribution D can be made as simple as a product distribution D = Di X …XDD such that
Xi,j % Dj for all 1 ≤ i ≤ n, where Dj is either sampled from N(0,1) or a Gaussian mixture NmiX
for some σj > 0. As a consequence, by permuting the order of Di’s in the product distribution, We
can change the order of revealing the features.
Remark 3 (Kernel regression on Gaussian data). In light of Remark 2, D can be chosen to be
a product distribution that consists of only N(0, 1) and Nσmix. Note that one can simulate Nσm,iix
4
Under review as a conference paper at ICLR 2021
Figure 2: Illustration of multiple descent for the dimension-normalized generalization loss L0d as a
function of the dimension d. A local maximum occurs at d if L0d-1 < L0d > L0d+1. The triplet
L0d-1, L0d, L0d+1 then form an ascent/descent, which is marked by the shaded area. Local maxima
are marked by the dotted lines. Adding a new feature with a Gaussian mixture distribution increases
the loss, while adding one with a univariate Gaussian distribution decreases the loss. Therefore, a
Gaussian mixture feature followed by a Gaussian feature creates one ascent/descent.
with N (0, 1) through the inverse transform sampling. To see this, let FN (0,1) and FN mix be the
Cdf of N(0,1) and Nmj, respectively. If X 〜 N(0,1), We have FN(o,i)(X)〜Unif((0,1))
and therefore 夕σ(X)，FNmX(FN(0J)(X))〜Nmx. In fact, we can use a multivariate Gaussian
D0 = N(0,Id×d) and a sequence of non-linear kernels k[1:d] (x, y)，h0[1:d] (x),φ[1d (y)i, where
the feature map is 0[1:d](x) ，[Φι(χι), Φ2(χ2),..., φd(xd)]> ∈ Rd. Here is a simple rule for
defining φj: If Dj = N(0,1), we set φj to the identity function. If Dj = NmIx, we set φj to 中力.
Thus, the problem becomes a kernel regression problem on the standard Gaussian data.
Getting back Theorem 2, let us discuss how we will construct such a distribution D inductively.
We fix d. Again, denote the first d features of xtest by x , xtest [1 : d]. Let us consider adding
an additional component to the training data x1 [1 : d], . . . , xn [1 : d] and test data x so that we
increment the dimension d by 1. Let bi ∈ R denote the additional component that we add to the
vector xi (so that the new vector is [xi [1 : d]> , bi]> . Similarly, let y ∈ R denote the additional
component that we add to the vector x. We form the column vector b = [b1, . . . , bn]> ∈ Rn that
collects all additional components that we add to the training data.
We consider the change in (dimension-normalized) generalization loss as follows
Ld+1-Ld = E U A> + X	T∣(A+)>x∣∣2
Ld+ι-Ld=J 片[AT [x ]∣2-1 d(A
(1)
Note that the components b1 , . . . , bn , y are i.i.d. Lemma 3 relates the pseudo-inverse of [A, b]> to
that of A> .
Lemma 3 (Proof in Appendix B.2). Let A ∈ Rn×d and 0 6= b ∈ Rn×1, where n ≥ d + 1.
Additionally, let P = AA+ and Q = bb+ =,and define Z，b [I—P)b. If z = 0 and the
columnwise partitioned matrix [A, b] has linearly independent columns, we have
A>
b>
ΑΑ+bb>	、(A + )>	(I-AA+)b i
∣∣b∣∣2-b>AA+ b) (	)	，∣∣b∣∣2-b>AA+司
[(I - Q)(I + 1—tPPQ) )(A+ )>, b⅞-⅛]
h(I-Q)(I + PQ)(A+)>, b⅞¾i .
5
Under review as a conference paper at ICLR 2021
In our construction ofD, the components Dj are all continuous distributions. The matrix I -P is an
orthogonal projection matrix and therefore rank(I - P) = n - d. As a result, it holds almost surely
that b 6= 0, z 6= 0, and [A, b] has linearly independent columns. Thus the assumptions of Lemma 3
are satisfied almost surely. In the sequel, we assume that these assumptions are always fulfilled.
Lemma 4 (Proof in Appendix B.3). Assume d, n > d + 2 and P are fixed, where P ∈ Rn×n is an
orthogonal projection matrix whose rank is d. Define Z，b [I-P), where b = [bi,..., bn]> ∈ Rn.
If y, bi,…，bn iid Nmix, we have E[1∕z] = On,d,σ ⑴ and E[y2∕b>(I - P)b] = On,d,σ ⑴.
Theorem 5 provides an upper bound for the following quantity
Eb,y
d + 1 b>
+
x
y
(A+ )> xlll A, x
1
if bi, . . . , bn, y are i.i.d. according to N(0, 1) or Nσm,iix. This quantiry is similar to the difference
between the dimension-normalized generalization loss L0d+i - L0d but with expectation only over b
and y.
Theorem 5 (Proof in Appendix B.4). Conditioned on A and x, the following statements hold:
(a)	If d + 2 < √2n and bι,...,bn,y iid N (0,1), we have
Eb,y
d +1 b>
+
x
y
d-k(A+)>xk2(2n -(d + 2)2)
d(d + 1)2(n — d — 2)
1
(b)	If d + 2 < n and bi,..., bn, y iiid Nmix, we have
2
≤
Ebi d+ι IAT
d(A+)>x
2
+ On,d,σ (1) ,
+
x
y
where On,d,σ (1) is a universal constant that only depends on n, d, and σ.
Corollary 6. Assume d + 2 < √2n. If either bi,... ,bn,y iiid N (0,1) or bι,...,bn,y iiid Nmiix,
and by taking expectation over all random variables, we have
El d+1 [AA>
2
= On,d,σ
E 1(A+)>x
ld
+
x
y
2
We will use Theorem 5 in two different ways. The first way is presented in Corollary 6. We would
like to show inductively (on d) that L0d is finite for every d. Provided that we are able to guarantee
finite L0i, Corollary 6 implies that L0d is finite for every difthe components are always sampled from
N(0, 1) or Nσm,iix.
Alternatively, we can use Theorem 5 to create a descent, i.e., make L0d+i < L0d. In light of (2), to
make the left-hand side negative, we need
d - k(A+)Txk2(2n - (d + 2)2) <0,
which is equivalent to
ll 1	+ T ll2	1
d(A ) X > d(2n - (d + 2))2 .
One we take expectation over A and x, we need the above equation to hold in expectation in order
to create a descent, i.e.,
Ll >---------1--------M .
d > d(2n - (d +2))2
Provided that L0d can be made sufficiently large, letting L0D satisfy the above inequality and then
adding an additional N(0, 1) entry will lead to L0d+i < L0d. Making a large L0d, in turn, can be
achieved by adding an entry sampled from Nσm,iix when the data dimension increases from d - 1 to
d in the previous step. Indeed, Theorem 7 shows that adding a Nσm,iix feature can increase the loss by
arbitrary amount.
6
Under review as a conference paper at ICLR 2021
Theorem 7 (Proof in Appendix B.5).For any C > 0 and E (A+)>x2 < +∞, there exists a
iid
σ > 0 such that if bι,...,bn,y 〜NmiIx, we have
We are now ready to prove Theorem 2.
Proof of Theorem 2. We construct D inductively. Let D1 = N(0, 1). When d = 1, we have
A = [x1[1 : d], . . . ,xn[1 : d]]> = [x1,1, . . . ,xn,1]> ∈ Rn,
which is a column vector. Therefore, A+ = kA>2. As a result, We get
LI = EIl(A+)>x『 =E 祢=n⅛,
where X 〜N(0,1) and ∣∣Ak2 〜χ2(n).
Since we will set Dj (for j ≥ 2) to either N(0, 1) or Nσm,i1x, by Corollary 6, we have L0j+1
On,j,σj+1 (L0j). By induction, we obtain that L0j is finite for all 1 ≤ j ≤ D.
We define d0 , 0. Assume that we have determined distributions D1, . . . , Ddj+1, where 0 ≤ j <
K. We set Ddj+2, . . . , Ddj+1 -1 to N(0, 1). For Ddj+1, by Theorem 7, we pick σdj+1 such that if
Ddj+1 = Nσmix , we have
j+1	σdj+1
Ld	> max ] Ld	_i,-----------1--------- ∖ .	(3)
dj+1	I dj+1	dj+1 (2n - (dj+1 +2)2)/
Next, we set Ddj+1+1 = N(0, 1). Taking the expectation of (2) in Theorem 5 over all random
variables, we have
L	_ L ≤ dj+1 - d2+iLdj+i(2n - (dj+1 + 2)2) < o
dj + 1 + 1	dj + 1 —	dj + 1 (dj+1 + I)2(n - dj + 1 - 2)	，
where the last inequality is due to (3). So far we have constructed a local maximum at dj+1. By
induction, we conclude that a local maximum occurs at every dj.	□
Remark 4. From Remark 2 and the proof of Theorem 2 it is clear that D = D1 × …× Dd is
a product distribution. The construction in the proof also shows that the generalization curve is
actually determined by the specific choice of the Di ’s. Note that permuting the order of Di ’s is
equivalent to changing the order by which the features are being revealed (i.e., permuting the entries
of the data Xi's). Therefore, given the same data points X1, ∙∙∙ ,Xn ∈ RD, we can create many
different generalization curves simply by changing the order of the feature-revealing process.
4 Overparameterized Regime
In this section, we study the multiple decent phenomenon in the overparameterized regime. Note
that as stated in Section 2, we consider the minimum-norm solution here. As stated in the following
theorem, we require d ≥ n+ 8, which means d starts at roughly the same order as n. In other words,
the result covers almost the entire spectrum of the overparameterized regime.
Theorem 8 (Overparameterized regime). Let n < D - 9. Given any sequence ∆n+8, ∆n+9, . . . ,
∆ D-1 where ∆d ∈ {↑, J} ,there exists a distribution D such thatfor every n + 8 ≤ d ≤ D 一 1, we
have
Ld+1
> Ld ,	if ∆d = ↑
< Ld,	if δm = J
L0	> L0d ,
Ld+1	< L0d,
if ∆d = ↑
if ∆d = J
7
Under review as a conference paper at ICLR 2021
n+8	n+9	n+10	n+11	n+12	n+13	n+14
Dimension of data d
Figure 3: Illustration of the multiple descent phenomenon for the generalization loss Ld (or the
dimension-normalized generalization loss L0d) versus the dimension of data d in the overparameter-
ized regime starting from d = n + 8. One can fully control the generalization curve to increase or
decrease as specified by the sequence ∆ = {，↑,，，↑,，...}. Adding a new feature with Gaus-
sian mixture distribution increases the loss, while adding one with Gaussian distribution decreases
the loss.
In Theorem 8, the sequence ∆n+8, ∆n+9, •一，Δd-i is just used to specify the increas-
ing/decreasing behavior of the L0d sequence for d > n + 8. Compared to Theorem 2 for the un-
derparameterized regime, where one is able to fully control the ascents but only partially control
the descents, Theorem 8 indicates that one is able to fully control both ascents and descents in the
overparameterized regime by placing an ascent/descent wherever one desires. Fig. 3 illustrates an
example.
Lemma 9 gives the pseudo-inverse of A when d > n.
Lemma 9 (Proof in Appendix C.1). Let A ∈ Rn×d and b ∈ Rn×1, where n ≤ d. Assume that
matrix A and the columnwise partitioned matrix B , [A, b] have linearly independent rows. Let
G，(AA>)-1 ∈ Rn×n and U，1+⅛ ∈ R1×n. Wehave
A>
b>
+
= (I-bu)>(A+)>,u> .
Lemma 10 establishes finite expectation for several random variables. These finite expectation re-
sults are necessary for Theorem 11 and Theorem 12 to hold. Technically, they are the dominating
random variables needed in Lebesgue’s dominated convergence theorem. Lemma 10 indicates that
to guarantee these finite expectations, it suffices to set the first n + 8 distributions to the standard
normal distribution and then set Dn+8 , . . . , DD to either a Gaussian or a Gaussian mixture distribu-
tion. In fact, in Theorem 11 and Theorem 12, we always add a Gaussian distribution or a Gaussian
mixture.
Lemma 10 (Proof in Appendix C.2). Let D = Di ×∙∙∙× Dd be a product distribution where
(a)	Dd = N(0, 1) if d = 1, . . . , n + 8; and
(b)	Dd is either N (0, σ2) or Nmix*壮 for d> n + 8.
Let D [i：d[ denote Di ×∙∙∙× Dd. Assume that every row of A ∈ Rn×d and X ∈ Rd×1 are i.i.d. and
follow D[1:d]. For any d such that n + 8 ≤ d ≤ D, all of the followings hold:
E[k(A+)>xk2] < +∞,
E[λ2max((AA>)-i)] < +∞,	(4)
E[λmax((AA>)-i)k(A+)>xk2] < +∞,
E[λ2max((AA>)-i)k(A+)>xk2] < +∞.
Theorem 11 shows that in order to have Ld+i < Ld and L0d+i < L0d, it suffices to add a Gaussian
feature.
8
Under review as a conference paper at ICLR 2021
Theorem 11 (Appendix C.3). If E[k(A>A)+xk2] > 0 and all equations in (4) hold, there exists
σ > 0 such that if y, bι,...,bniid N (0, σ2), we have
2
A> +	2
Ld+1-Ld=E	Ab>	xy	-E(A+)>x2 <0,
1 A>	+ x	2	1	2
Ld+1 - Ld = E	A>	X- E j(A+)>χ	< 0 .
d+1	d d +	1 b> y	d
Theorem 12 shows that adding a Gaussian mixture feature can make Ld+1 > Ld and L0d+1 > L0d.
Theorem 12 (Proof in Appendix C.4). Assume Ek(A+)>xk2 < +∞. For any C > 0, there exist
iid mix
μ, σ > 0 such that if y, bι,...,bn 〜Nmix, we have
2
A> +	2
Ld+1-Ld=E Ab>	xy	-E(A+)>x2 >C,
1	A>	+ x	2	1	2
Ld+1 - Ld = E	A>	X- E j(A+)>χ	> C.
d+1	d d + 1	b> y	d
The proof of Theorem 8 immediately follows from Theorem 11 and Theorem 12.
Proof of Theorem 8. We construct the product distribution D = QdD=1 Dd. We set Dd = N(0, 1)
for d = 1,...,n + 8. For n + 8 < d ≤ D, Dd is either N(0, σ2) or Nmμ depending on ∆d being
either J or ↑.
First we show that for each step d, the assumption E[k(A>A)+Xk2] > 0 of Theorem 11 is sat-
isfied. If E[k(A>A)+Xk2] = 0, we know that (A>A)+X = 0 almost surely. Since D is a con-
tinuous distribution, the matrix A has full row rank almost surely. Therefore, rank((A>A)+) =
rank(A>A) = n almost surely. Thus dimker(A>A)+ = d - n ≤ d - 1 almost surely, which
implies X ∈/ ker(A>A)+. In other words, (A>A)+X 6= 0 almost surely. We reach a contradiction.
Moreover, by Lemma 10, the assumption Ek(A+)>Xk2 < +∞ of Theorem 12 is also satisfied.
If ∆d-1 = J, by Theorem 11, there exists σd > 0 such that ifDd = N(0, σd2), then Ld < Ld-1 and
Ld < L'd-∖. Similarly if ∆d-ι = ↑, by Theorem 12, there exists ◎& and μd such that Dd = Nmμ
guarantees Ld > Ld-1 and L0d > L0d-1.
□
5 Conclusion
Our work proves that the expected risk of linear regression can manifest multiple descents when
the number of features increases and sample size is fixed. This is carried out through an algorith-
mic construction of a feature-revealing process where the newly revealed feature follows either a
Gaussian distribution or a Gaussian mixture distribution. Notably, the construction also enables us
to control local maxima in the underparameterized regime and control ascents/descents freely in
the overparameterized regime. Overall, this allows us to design the generalization curve away from
the interpolation threshold. We conjecture that the same multiple-descent generalization curve can
occur in non-linear neural networks and we humbly suggest that entities with infinite computational
powers investigate this phenomenon.
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of gener-
alization error in neural networks. Neural Networks,132:428-446, 2020.
9
Under review as a conference paper at ICLR 2021
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pp. 8141-8150, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In ICML, pp.
477-502, 2019b.
Sanjeev Arora, Simon S Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and Dingli Yu.
Harnessing the power of infinitely wide deep nets on small-data tasks. In International Conference
on Learning Representations, 2019c.
Jerzy K Baksalary and Oskar Maria Baksalary. Particular formulae for the moore-penrose inverse
of a columnwise partitioned matrix. Linear algebra and its applications, 421(1):16-23, 2007.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. stat, 1050:28, 2018a.
Mikhail Belkin, Siyuan Ma, and Soumik Mandal. To understand deep learning we need to under-
stand kernel learning. In International Conference on Machine Learning, pp. 541-549, 2018b.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019a.
Mikhail Belkin, Alexander Rakhlin, and Alexandre B Tsybakov. Does data interpolation contra-
dict statistical optimality? In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 1611-1619, 2019b.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic
language model. Journal of machine learning research, 3(Feb):1137-1155, 2003.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10836-10846,
2019.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics, 7(3):331-368, 2007.
Emmanuel Caron and Stephane Chretien. A finite sample analysis of the double descent phe-
nomenon for ridge function estimation. arXiv preprint arXiv:2007.12882, 2020.
Lin Chen and Sheng Xu. Deep neural tangent kernel and laplace kernel have the same rkhs. arXiv
preprint arXiv:2009.10683, 2020.
Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization
gap between adversarially robust and standard models. In ICML, 2020.
Yehuda Dar, Paul Mayer, Lorenzo Luzi, and Richard G Baraniuk. Subspace fitting meets regression:
The effects of supervision and orthonormality constraints on double descent of generalization
errors. In ICML, 2020.
Stephane d,Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.
Ernesto De Vito, Andrea Caponnetto, and Lorenzo Rosasco. Model selection for regularized least-
squares algorithm in learning theory. Foundations of Computational Mathematics, 5(1):59-85,
2005.
10
Under review as a conference paper at ICLR 2021
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Mario Geiger, Stefano Spigler, Stephane d'Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019.
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, StePhane d'Ascoli,
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with
number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment,
2020(2):023401, 2020.
Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural networks and the bias/variance dilemma.
Neural computation, 4(1):1-58, 1992.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension. arXiv preprint arXiv:1904.12191, 2019.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong
Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural
networks using pipeline parallelism. In Advances in neural information processing systems, pp.
103-112, 2019.
Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. Precise tradeoffs in adversarial training
for linear regression. In Conference on Learning Theory, 2020.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeles” regression can general-
ize. Annals of Statistics, pp. to appear, 2019.
Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. In COLT, 2020.
Marco Loog, Tom Viering, and Alexander Mey. Minimizers of the empirical risk and risk mono-
tonicity. In Advances in Neural Information Processing Systems, pp. 7478-7487, 2019.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More
data can help, double descend, or hurt generalization. arXiv preprint arXiv:2002.11080, 2020.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292,
2019.
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. arXiv preprint arXiv:2003.01897, 2020.
11
Under review as a conference paper at ICLR 2021
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-
Julien, and Ioannis Mitliagkas. A modern take on the bias-variance tradeoff in neural networks.
arXiv preprint arXiv:1810.08591, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. In ICLR (Workshop), 2015.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
neural information processing Systems, pp.1177-1184, 2008.
Alexander Rakhlin and Xiyu Zhai. Consistency of interpolation with laplace kernels is a high-
dimensional phenomenon. In Conference on Learning Theory, pp. 2595-2623, 2019.
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco. Asymptotics of ridge (less) regression
under general source condition. arXiv preprint arXiv:2006.06386, 2020.
Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features.
In Advances in Neural Information Processing Systems, pp. 3215-3225, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.
Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
arXiv:2009.14286, 2020.
Dietrich von Rosen. Moments for the inverted wishart distribution. Scandinavian Journal of Statis-
tics, pp. 97-109, 1988.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems, pp. 9712-9724, 2019.
Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of ad-
aboost and random forests as interpolating classifiers. The Journal of Machine Learning Research,
18(1):1558-1590, 2017.
Ji Xu and Daniel J Hsu. On the number of variables to use in principal component regression. In
Advances in Neural Information Processing Systems, pp. 5094-5103, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In ICLR, 2017.
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-
parameterized deep relu networks. Machine Learning, 109(3):467-492, 2020.
12
Under review as a conference paper at ICLR 2021
A	Further Related Work
Our work is directly related to the recent line of research in the theoretical understanding of the
double descent (Belkin et al., 2019a; Hastie et al., 2019; Xu & Hsu, 2019; Mei & Montanari, 2019)
and the multiple descent phenomenon (Liang et al., 2020). Here we briefly discuss some other work
that is closely related to this paper.
Least Square Regression. In this paper we focus on the least square linear regression with no
regularization. For the regularized least square regression, De Vito et al. (2005) proposed a selection
procedure for the regularization parameter. Advani & Saxe (2017) analyzed the generalization of
neural networks with mean squared error under the asymptotic regime where both the sample size
and model complexity tend to infinity. Richards et al. (2020) proved for least square regression in
the asymptotic regime that as the dimension-to-sample-size ratio d/n grows, an additional peak can
occur in both the variance and bias due to the covariance structure of the features. As a comparison,
in this paper the sample size is fixed and the model complexity increases. Rudi & Rosasco (2017)
studied kernel ridge regression and gave an upper bound on the number of the random features to
reach certain risk level. Our result shows that there exists a natural setting where by manipulating
the random features one can control the risk curve.
Over-Parameterization and Interpolation. The double descent occurs when the model com-
plexity reaches and increases beyond the interpolation threshold. Most previous works focused on
proving an upper bound or optimal rate for the risk. Caponnetto & De Vito (2007) gave the optimal
rate for least square ridge regression via careful selection of the regularization parameter. Belkin
et al. (2019b) showed that the optimal rate for risk can be achieved by a model that interpolates
the training data. In a series of work on kernel regression with regularization parameter tending
to zero (a.k.a. kernel ridgeless regression), Rakhlin & Zhai (2019) showed that the risk is bounded
away from zero when the data dimension is fixed with respect to the sample size. Liang & Rakhlin
(2019) then considered the case when d n and proved a risk upper bound that can be small given
favorable data and kernel assumptions. Instead of giving a bound, our paper presents an exact com-
putation of risk in the cases of underparameterized and overparameterized linear regression, and
proves the existence of the multiple descent phenomenon. Wyner et al. (2017) analyzed AdaBoost
and Random Forest from the perspective of interpolation. There has also been a line of work on
wide neural networks (Arora et al., 2019a;b;c; Du et al., 2019; Allen-Zhu et al., 2019; Wei et al.,
2019; Cao & Gu, 2019; Advani et al., 2020; Chen & Xu, 2020; Zou et al., 2020).
Sample-wise Double Descent and Non-monotonicity. There has also been recent development
beyond the model-complexity double-descent phenomenon. For example, regarding sample-wise
non-monotonicity, Nakkiran et al. (2019) empirically observed the epoch-wise double-descent and
sample-wise non-monotonicity for neural networks. Chen et al. (2020) and Min et al. (2020) iden-
tified and proved the sample-wise double descent under the adversarial training setting, and Javan-
mard et al. (2020) discovered double-descent under adversarially robust linear regression. Loog
et al. (2019) showed that empirical risk minimization can lead to sample-wise non-monotonicity in
the standard linear model setting under various loss functions including the absolute loss and the
squared loss, which covers the range from classification to regression. We also refer the reader to
their discussion of the earlier work on non-monotonicity of generalization curves. Dar et al. (2020)
demonstrated the double descent curve of the generalization errors of subspace fitting problems.
B Proofs for Underparametrized Regime
B.1 Proof of Theorem 1
Proof. We follow the notation convention in (1):
2
A> +	2
Ld+1 - Ld = E	b>	y - Il(A Vxll	.
13
Under review as a conference paper at ICLR 2021
Recall d < n and the matrix B0
fat matrices. As a result, if x0 ,
Ab> is of size (d + 1) × n. Both matrices B0 and B , A>
are
kB0+x0k2
, we have
min
z:B0z=x0
kzk2,
kB+xk2 = min kzk2.
z:Bz=x
Since {z | B0z = x0} ⊆ {z | Bz = x}, we get kB0+x0k2 ≥ kB+xk2. Therefore, we obtain
Ld+1 ≥ Ld.
The second part of the theorem, which says that for any C > 0 there exists a distribution such that
Ld+ι - Ld > C, follows directly from Theorem 7.	□
B.2 Proof of Lemma 3
Proof. By (Baksalary & Baksalary, 2007, Theorem 1), we have
A>
b>
[(I - Q)A(A>(I - Q)A)-1, b>觉Pbb).]
Define r , A>b ∈ Rd. Since A has linearly independent columns, the Gram matrix G = A>A is
non-singular. The Sherman-Morrison formula gives
(A>(I	O)AL	_ (a>a rr>YlGτ+ GTrr>G-1	_G-1,	GTrbT(A+)>
(A (I-	Q)A)	= (AA-kbk2)	= G + ∣∣b∣∣2-r>G-ir =	G	+	||b||2-r>G-ir	,
where we use the facts r = A>b and AG-1 = (A+)> in the last equality. Therefore, we deduce
A(A>(I - Q)A)-1 =AG-1+
(A+)> +
AG-1rb>(A+ )>
IIbIl2 - r>G-1r
AG-1A>bb>(A+)>
IlbIl2 - r>G-1r
AA+bb>
+ ∣∣b∣∣2 - r>G-1r
I+rjr‰	(a+)>
、	1	kbk2~ )
Observe that
1 - r>G-1 r = 1 -
kbk2
b>A(A>A)-1A>b
kbk2
b>Pb
1------= Z
网2
+
Therefore, we obtain the desired expression.
□
B.3 Proof of Lemma 4
Lemma 13 shows that a noncentral χ2 distribution first-order stochastically dominates a central χ2
distribution of the same degree of freedom. It will be needed in the proof of Lemma 4.
Lemma 13. Assume that random variables X 〜χ2(k, λ) and Y 〜χ2(k), where λ > 0. For any
c > 0, we have
P(X ≥ c) > P(Y ≥ c).
In other words, the random variable X (first-order) stochastically dominates Y.
Proof. Let Y1, X2,...,Xk iid N(0,1) and Xi 〜 N(√λ, 1) and all these random variables are
jointly independent. Then X0，Pk=I Xii 〜χ2(k, λ) and Y0，Y2 + Pk=2 Xil 〜χ2(k).
14
Under review as a conference paper at ICLR 2021
It suffices to show that P(X0 ≥ C)	>	P(Y0	≥	c), or equivalently, P(|N(μ, 1)| ≥ C) >
P(|N(0,1)∣ ≥ C) for all c > 0 and μ，√λ >	0.	Denote Fc(t) = P(|N(μ, 1)| ≥ c) and	We
have
Fc(μ) = 1
c
exp
c
—
(x — μ)2∖ ,	1	f--μ
-------I dx = 1 - ―/	/ exp
2	)	√2Π J-c-μ
dx,
and thus
T = W F (-F) - exp (-⅛f 小 0.
This shows P(|N(μ, 1)| ≥ c) > P(|N(0,1)| ≥ c) and we are done.
□
ProofofLemma 4. Since b iid Nσm,1ix,
we can rewrite b = U + W where W 〜 N(0, σ2In ) and the
entries of U satisfy Ui iid Unif({ -1,0,1}). Furthermore, U and W are independent. Note that for
any fixed n and d, the support of U is finite and its cardinality only depends on n. Therefore, we
only need to show that conditioning on U, the expectation over W is On,d,σ(1). In other words, for
any fixed u, we want to show Ew [1∕z | u] = Ond,σ(1) and Ew [b>(y-P)b IUI = Ond,σ(1).
Note that since y2∕σ2 is first-order stochastically dominated by χ2(1, 1), we have
E[y2 |U] = E[y2] ≤ σ2E[χ2(1, 1)] = 2σ2.
Therefore, it remains to show Ew [1∕z | u] = On,d,σ(1) and Ew [b>(I-p)b IUI = On,d,σ(1).
Note that
b>Ib
b (I — P )b = 1 +
(U + W)> P(U + W)
(u + w)>(I — P)(u + w).
1
z
Since P is an orthogonal projection, there exists an orthogonal transformation O depending only on
P such that
(U + W)>P(U + W) = [O(U + W)]>Dd[O(U + W)]
where Dd = diag([1, . . . , 1, 0 . . . , 0]) with d diagonal entries equal to 1 and the others equal to 0.
We denote u = O(u), which is fixed (as U and O are fixed), and W = O(W)〜 N(0,σ2In). It
follows that
1 = 1+	(U + W)>Dd(U + W) =ι +
Z	(U + W)>(I — Dd)(U + W)
Observe that
Pd=I(Ui + Wi)2
pn=d+ι(Ui+W,)2
1+
Pd=ι(Ui + Wi)2∕σ2
Pn=d+ι(Ui + Wi)2∕σ2 .
d	ud
X(Ui + Gi)/σ 〜χ2 ( d, tχU2
nn
X (Ui + wi)2/b2 〜χ21 n ― d, ʌ I X u21 ,
i=d+1	i=d+1
and that these two quantities are independent. It follows that
E
i + Wi)2∕σJU = d + JXU2 .
i=1	i=1
By Lemma 13, the denominator PNd+ι(Ui + W,)2∕σ2 first-order stochastically dominates χ2(n —
d). Therefore, we have
E
1
1
- Pn=d+1(Ui + w∖) /σ
u≤E
χ2(n - d)
1
n — d — 2
15
Under review as a conference paper at ICLR 2021
Putting the numerator and denominator together yields
d+
≤1+
1 u2
n-d-2
≤ 1 + nd⅛ =On,d,σ ⑴.
Similarly, we have
E [b>(⅛ N =
JO(U + w)]>(I - Dd)[O(u + w)]
LPi=d+ι(Ui + Wi)2∕σ2
σ2 χ2(n - d)
1	1
σ2 n — d — 2
On,d,σ(1) .
B.4 Proof of Theorem 5
Proof. First, we rewrite the expression as follows
(d + 1)2 卜1 - Q)(I + PQ/z)(A+)Tx + b>(i- P)by	- d2k(A+ )τxk2,
(5)
where P, Q, z are defined in Lemma 3. Since y has mean 0 and is independent of other random
variables, so that the cross term vanishes under expectation over b and y:
Eb,y ((I — Q)(I + PQ∕z)(A+)τx,
J，) ]=0,
where〈•，•〉denotes the inner product. Therefore taking the expectation of (5) over b and y yields
Eb,y [| d+1 [AτHx]	- d(A+)τx[]	⑹
Eb,y "(d +1)2 k(I - Q)(I + PQ/z)(A+)Txk2 - d2k(A+)Txk2 + (d +1)2 bτ(I- pP)by∣ #
(7)
(dɪpEb,y k(I - Q)(I + PQ∕z)(A+)τxk2 - (1 + d)2k(A+)τxk2 + |bT1f-^，|[ ∙
(8)
We simplify the third term. Recall that I - P = I - AA+ is an orthogonal projection matrix and
thus idempotent
22
(bτ(I - P)b)2 k(I - P)bk2 = bτ(I - P)b ∙	⑼
(I- P)b
bτ(I - P)by
E
1
z
u
≤
□
16
Under review as a conference paper at ICLR 2021
Thus we have
Eb,y [| d+ι [AT [χ]∣2-1 d(A+)τxl2j	(10)
1	1	y2
7-7-E^Eb,y k(I- Q)(I + PQ∕z)(A+)τxk2-(1 +夕2k(A+ )τxk2 + 八> ；PM .
(d+1)	d	b (I - P )b
(11)
We consider the first and second terms. We write V = (A+)τx and define Z = b kI-P). The sum
of the first and second terms equals
k(I - Q)(I + PQ∕z)vk2 - (1 + d)2 kvk2 = -vτ (M + δI)v,	(12)
where δ = d + 击 and
M，Q - PQ + QP + (2 - g) QPQ + QPQPQ.
z	z z2	z2
The rank of M is at most 2. To see this, we re-write M in the following way
M = Q (-P + (2 - ⅛) PQ + PQPQ)_ + [-PQ] , M1 + M2.
Notice that rank(M1 ) ≤ rank(Q), rank(M2) ≤ rank(Q), and rank(Q) = 1. It follows that
rank(M) ≤ rank(M1) + rank(M2) = 2. The matrix M has at least n - 2 zero eigenvalues. We
claim that M has two non-zero eigenvalues and they are 1 - 1∕z < 0 and 1.
Since
rank(P Q) ≤ rank(Q) = 1
and
bτPb
tr(PQ) = JbkT = 1 - z,
thus PQ has a unique non-zero eigenvalue 1 - z . Let u 6= 0 denote the corresponding eigenvector
such that PQu = (1 - z)u. Since u ∈ imP and P is a projection, we have Pu = u. Therefore we
can verify that
Mu = (1 — ɪ)u .
To show that the other non-zero eigenvalue of M is 1, we compute the trace of M
tr(M) =	tr(Q)-…+ (2	- =) tr(PQ) +	t^	=2 - 1,
z	z z2	z2	z
where we use the fact that tr(Q) = 1, tr(P Q) = 1 - z,
2	PbbτPbbτ	(bτPb)(bτPb)	2
tr((PQB = tr () = tr (l kb⅛4	) =(I-Z)2 .
We have shown that M has eigenvalue 1 - 1∕z and M has at most two non-zero eigenvalues.
Therefore, the other non-zero eigenvalue is tr(M) - (1 - 1∕Z) = 1.
We are now in a position to upper bound (12) as follows:
-vτ(M + δI)v ≤ -(1 - 1∕Z + δ)kvk2 < -(1 - 1∕Z + 2∕d)kvk2 .
Putting all three terms of the change in the dimension-normalized generalization loss yields
1
d + 1
2
-(1 - 1/z + 2/d)kvk2 + bτ(I - P)b
Eb,y
≤ (d +1)2 Eb,y
17
Under review as a conference paper at ICLR 2021
For bι,...,bn,y iiid N (0,1), We have E[y2] = 1. Moreover, b>(I - P )b follows χ2(n - d) a dis-
tribution. Thus b>(i-p)b follows an inverse-chi-squared distribution with mean 冷—―.Therefore
y2	1	d
the expectation E[ b>j-p)b ] = n-(. Notice that 1/z follows a 1 + n-d F (d, n - d) distribution
and thus E[1∕z] = 1 + n-d-. As a result, we obtain
d-IIvll2 (2n-(d +2)2)
d(d + 1)2(n — d — 2)
For bι,...,bn ,y iiid Nm1x, Lemma 4 implies that
Eb,y [1/z] < On,d,σ(1),
and
y2
Eb,y[b>(i - P)b] < On,d,σ ⑴.
Therefore, we conclude that
1	A> + x 2	1	2
% d"+Γ b> y ≤ d(A+) X + On,d,σ(1) ∙
□
B.5 Proof of Theorem 7
Proof. We start from (11). Taking expectation over all random variables gives
1	1	y2
=——2E k(I - Q)(I + PQ∕z)(A+)>xk2 - (1 + -)2k(A+)>x∣2 + EZy
(d+ 1)2	d	b>(I - P)b
≥ 77‰ (-(1 + 1)^13)7112 + E [p⅛D .
(d+ 1)2	d	i=1 bi2
Our strategy is to choose σ so that E [Py^^ ] is sufficiently large. This is indeed possible as we
immediately show. Define independent random variables U 〜 Unif({-1,0,1}) and W 〜 N(0, σ2).
Since y has the same distribution as u + w, we have
E[y2] = E[(u + w)2] = E[u2] + E[w2] ≥ ∣.
On the other hand,
E
-1	-
P:^
1
≥ P(max |bi | ≤ σ) E
[P(|b1| ≤ σ)]n E
≥ --
3√2
1
!πσ2
L∑Lι b2
1
[PCT
max |bi | ≤ σ
i
max |bi | ≤ σ
i
Z-σ exp J 点)dt
n ɪ
nσ2
1
- 5nnσ2 .
18
Under review as a conference paper at ICLR 2021
Together we have
一 y2 一
^∑CT
〉	1
—5n+1nσ2 .
Since
AT + X
bτ y
2
-∣∣(A+ )τχ∣∣2
we have
TXi 2
+∞,
E
E
which completes the proof.
□
C Proofs for Overparametrized Regime
C.1 Proof of Lemma 9
Proof. Since A and B have full row rank, (AAT)-I and (BBT)-I exist. Therefore we have
B+ = Bτ(BB T)-1.
The Sherman-Morrison formula gives
TT	TTT	GbbT G
(BBT)T = (aat + bbT)-1 = G--------T— = G - Gbu = G(I - bu).
1 + bT Gb
Hence, we deduce
B+ = [A,b]τG(I - bu)
AτG(I - bu)
bτG(I - bu)
'A+(I - bu)
bτG(I - bu)
A+ (I - bu)
u
Transposing the above equation yields to the promised equation.
□
C.2 Proof of Lemma 10
Proof. Let us first denote
v , (A+)τx
and
G，(AAT)T ∈ Rn×n.
First note that by Cauchy-Schwarz inequality, it suffices to show there exists D such that
E[λmax(G)] < +∞ and EkVk4 < +∞.
We define Ad ∈ Rn×d to be the submatrix of A that consists of all n rows and first d columns.
Denote
Gd，(AdAT)-1 ∈ Rn×n.
We will prove E[λmax(G)] < +∞ by induction.
The base step is d = n + 8. Recall D[i-n+β] = N(0,In+β). We first show E[λmax(Gn+s)]4 < +∞.
Note that since Gn+s is almost surely positive definite,
E[λm1ax(Gn+8)] = E[λmax(Gn+8)] ≤ Etr(G，8) = Etr((A„+8A>+8)-4) = tr(E[(An+8AT+8)-4]).
19
Under review as a conference paper at ICLR 2021
By our choice of D[1:n+8] , the matrix (An+8An>+8)-1 is an inverse Wishart matrix of size n × n
with (n + 8) degrees of freedom, and thus has finite fourth moment (see, for example, Theorem 4.1
in (von Rosen, 1988)). It then follows that
E[λ4max(Gn+8)] ≤ tr(E[(An+8An>+8)-4]) < +∞ .
For the inductive step, assume E[λmax(Gd)]4 < +∞ for some d ≥ n + 8. We claim that
λmax(Gd+1) ≤ λmax(Gd) ,
or equivalently,
λmin(AdAd>) ≤ λmin(Ad+1Ad>+1) .
Indeed, this follows from the fact that
AdAd> 4 AdAd> + bb> = Ad+1Ad>+1,
under the Loewner order, where b ∈ Rn×1 is the (d + 1)-th column of A. Therefore, we have
E[λ4max(Gd+1)] ≤ E[λ4max(Gd)]
and by induction, we conclude that E[λ4max (G)] < +∞ for all d ≥ n + 8.
Now we proceed to show Ekvk4 < +∞. We have
kvk4 = k(AA>)-1Axk4 ≤ k(AA>)-1AkOp ∙kχk4 ,
where ∣∣ ∙ ∣∣θp denotes the '2 → '2 operator norm. Note that
k(AA>)-1Ak4p = λmιaχ (((AA>)-1A)> (AA>)-1A)
=λmιaχ (A>(AA>)-2A)
=λmaχ ((A>(AA>)-2A)2),
where the last equality uses the fact that A> (AA> )-2A is positive semidefinite. Moreover, we
deduce
k(AA>)-1A∣0p = λmαχ (A>(AA>)-3A)
≤ tr (A>(AA>)-3A)
=tr ((AA>)-3AA>)
=tr ((AA>)-2).
Using the fact that AdAd> 4 Ad+1 Ad>+1 established above, induction gives
(AA> )-2 4 (An+8An>+8)-2 .
It follows that
E [k(AA>)-1Ak0p] ≤ E [tr ((An+sA*)—2)] = tr (E [(An+sA*)-2]) < +∞ , (13)
where again we use that fact that inverse Wishart matrix (An+8A>+s) 1 has finite second moment.
Next, we demonstrate E∣x∣4 < +∞. Recall that every Di is either a Gaussian or a Gaussian
mixture distribution. Therefore, every entry of x has a subgaussian tail, and thus E∣x∣4 < +∞.
Together with (13) and the fact that x and A are independent, we conclude that
EH4 ≤ E [k(AA>)-1A∣0p] ∙ E [kxk4] < +∞ .
□
20
Under review as a conference paper at ICLR 2021
C.3 Proof of Theorem 11
Proof. The randomness comes from A, x, y and b. We first condition on A and x being fixed.
Let G，(AA>)-1 ∈ Rn×n and U，1+⅞⅛b ∈ R1×n. Define
v, (A+)>x,	r, 1+b>Gb, H,bb>.
We compute the left-hand side but take the expectation over only y for the moment
+ 2
Ey	Ab>	xy	- (A+)>x2
= Ey (I - bu)>v + u>y2 - kvk2
= k(I - bu)>vk2 + Eyku>yk2 - kvk2	(E[y] = 0)
=k(I- bu)>vk2 + Ey [y2 ] kGf--kvk2.
Let us first consider the first and third terms of the above equation:
k(I — bu)>v∣∣2 — |同|2 = v> ((I — bu)(I — bu)> — I) V
= —v> (bu + u>b> — buu>b>) V
> (HG + GH
r
HG-H
v.
—
Write G = VΛV>, where Λ = diag(λ1, . . . , λn) ∈ Rn×n is a diagonal matrix (λi > 0) and
V ∈ Rn×n is an orthogonal matrix. Recall b 〜N(0, σ2In). Therefore W，V>b 〜N(0, σ2In).
Taking the expectation over b, we have
Eb
HG + GH
r
Eb V
V>bb>VΛ+ΛV>bb>V
1 + b> V ΛV > b
V Ew
ww> Λ + Λww>
1 + w>Λw
TafRA "∏r WW>Λ+Λww>
Let R E Ew [ -1+w>Λw-
. We have
2λiwi
σ2EV 〜N (0,In)
2λiν2
_1+ σ2 Pi=I λiν- _
>0
and if i 6= j ,
Rij = Ew
Notice that for any w and j, it has the same distribution if we replace wj by -wj . As a result,
Rij = Ew
(λi + λj )Wi(-Wj )
1 + Pi=I λiW-
—Rij .
Thus the matrix R is a diagonal matrix and
R
2 2 Adiag(V产
σ 1 + σ2ν>Λν .
Thus we get
HG + GH 2	GV diag(ν)2V>
EbA	r	= 2σ EV〜N(0,1n),A	1 + σ-ν>Λν
Moreover, by the monotone convergence theorem, we deduce
GV diag(ν)2V>
σlim+ EV〜N(0,in),A,χ -v 1+ σ2V>Λν V = EV〜N(0,in),A,x [-v GVdiag(ν)2V>v]
E[—V>GV].
21
Under review as a conference paper at ICLR 2021
It follows that as σ → 0+,
τ HG + GH
r
E
〜-2σ2E[vτGv] = -2σ2E IVT(AAT)T。] = -2σ2E[k(ATA)+x『].
Moreover, by (4), we have
E[vτ(AAT)Tv] ≤ E[λmax ((AAT)-1) k(A+)τxk2] < +∞ .
Next, we study the term HG2H∕r2:
Eb,A
^ HG2H 一
r2
Eb,A
V τbbτ V Λ2V τbbτ V
(1 + bτV ΛV Tb)2
EW 〜"(0,σ2In),A
σ4EV 〜N (0,In),A
V
wwτΛ2wwτ .
(1 + wτΛw)2
νν τΛ2 νν T
(1 + σ2ν τΛν )2
V
Again, by the monotone convergence theorem, we have
Iim EV 〜N (0,In),A,X
σ→0+
一 TV VVτΛ2νντ vτ ■
V V (1+ σ2ντΛν)2 V V
EV〜N(0,in),A,x [vTVVVTΛ2νντVτv]
Ea,x
=E [vτ
It follows that as σ → 0+,
vτV (2Λ2 +In Eλ2) Vτv
(2G2 +tr(G2)In) v].
Eb,A,x
^ HG2 H -
r2
〜σ
σ
-4E [vτ (2G2 +tr(G2)In) v]
∙4E[2∣∣(AAT)TVk2 +tr((AAT厂2)∏2].
Moreover, by (4), we have
E [2k(AAT)TVk2 + tr((AAT)-2)kvk2] ≤ (n + 2)E 2/54>尸)|"+)%『]< +∞ .
We apply a similar method to the term
kGbk2
kG2k . We deduce
bτ G2 b
bτ V Λ2V τb
r2	— (1 + bτGb)2 — (1 + bτVΛVTb)2 .
It follows that
E
-W -
r2
EW ~N (0,σ2In),A
wτΛ2w
(1 + wτΛw)2
σ2EV 〜N (0,In),A
ντΛ2ν ^
(1 + σ2ντΛν)2
The monotone convergence theorem implies
σ→m+ EV 〜N (0,In),A
Thus we get as σ → 0+
-ντΛ2ν
(1 + σ2ντΛν)2
E[ντΛ2ν] = E[tr(G2)].
Ey [y2]≡
〜σ4E[tr(G2)],
where E[tr(G2)] ≤ nE[λjmax((AAτ)-1)] < + ∞.
Putting all three terms together, we have as σ → 0+
Ld+1 - Ld 〜-2σ2E[k(AτA)+x∣∣2].
Therefore, there exists σ > 0 such that Ld+ι - Ld < 0. Furthermore, we deduce
Ld+1 - Ld =*(Ld+1 - Ld) < 0 .
□
22
Under review as a conference paper at ICLR 2021
C.4 Proof of Theorem 12
Proof. Again we first condition on A and x being fixed. Let G , (AA>)-1 ∈ Rn×n and u
ι+b>GGb ∈ R1×n as defined in Lemma 9. We also define the following variables:
v , (A+)>x , r , 1 + b> Gb.
We compute L0d+1 - L0d but take the expectation over only y for the moment
(d+1)2 (Ey ∣∣(I - bu)>v + u>y∣∣2 - (1 + 1∕d)2kvk2)
7d4n2 (k(I - bu)>vk2 + Eyku>yk2 - (1 + 1∕d)2kvk2)	(E[y] = 0)
(d + 1)
(d+1)2 (k(I-bu)>vk2 + Ey[y2]陪-(1 + 1∕d)2H2) .	(14)
Our strategy is to make E[y2 kGbk ] arbitrarily large. To this end, by the independence of y and b We
have
E J 2 kGbk2] — E「2]E 口3门
Ey,b y -r2- = Ey[y IEb _r- .
By definition of NmiX, With probability 2/3, y is sampled from either N(μ, σ2) or N(-μ, σ2),
which implies E[y2] ≥ 1 μ2. For each bi, we have
P(IbiI ∈ [σ, 2Q ≥ 3 X 4.
Also note that G is positive definite. It follows that
Eb
IIGbII2 -
r2
Eb
IIGbII2
(1 + b>Gb)2
≥ E (λmin(G)IIbII)2	≥ (1Y
≥ Eb (1 + λmax(G)IIbII2)2 ≥ 112 J
λmin(G)nσ2
(1 +4λmax(G)nσ2)2
Altogether we have
E Γ 2 kGbk2 ] ≥	1 nSin(G)μ2σ2
Ey,b y r2 ≥ ≥ 3 ∙ 12n (1 + 4nλmax(G)σ2)2 .
Let μ = 1∕σ2 and we have
2 kGbk2
σl→m+E y21	≥ σl→m+ EA"%b
EA,xEy,b lim
σ→0+
1________nλmin (G)
3 ∙ 12n σ2(1 + 4nλmax(G)σ2)2
1________nλmin (G)
3 ∙ 12n σ2(1 + 4nλmax(G)σ2)2
+∞ ,
where we switch the order of expectation and limit using the monotone convergence theorem. Taking
full expectation over A, x, b and y of (14) and using the assumption that Ekv k2 < +∞ we have
Ld+1 -Ld = 7-7‰2 (EA,χ,bk(I - bu)>vk2 + E [y2kGk2] - (1 + 1∕d)2EA,χkv∣∣2) → +∞
(d+ 1)2	r2
as σ → 0+ . In addition, we have as σ → 0+ ,
Ld+1 - Ld ≥ d2(L0d+1 - L0d) → +∞ .
□
23