Under review as a conference paper at ICLR 2021
Sparse Uncertainty Representation in Deep
Learning with Inducing Weights
Anonymous authors
Paper under double-blind review
Ab stract
Bayesian neural networks and deep ensembles represent two modern paradigms
of uncertainty quantification in deep learning. Yet these approaches struggle to
scale mainly due to memory inefficiency issues, since they require parameter stor-
age several times higher than their deterministic counterparts. To address this,
we augment the weight matrix of each layer with a small number of inducing
weights, thereby projecting the uncertainty quantification into such low dimen-
sional spaces. We further extend Matheron’s conditional Gaussian sampling rule
to enable fast weight sampling, which enables our inference method to main-
tain reasonable run-time as compared with ensembles. Importantly, our approach
achieves competitive performance to the state-of-the-art in prediction and uncer-
tainty estimation tasks with fully connected neural networks and ResNets, while
reducing the parameter size to ≤ 47.9% of that of a Single neural network.
1	Introduction
Deep learning models are becoming deeper and wider than ever before. From image recognition
models such as ResNet-101 (He et al., 2016a) and DenseNet (Huang et al., 2017) to BERT (Xu
et al., 2019) and GPT-3 (Brown et al., 2020) for language modelling, deep neural networks have
found consistent success in fitting large-scale data. As these models are increasingly deployed in
real-world applications, calibrated uncertainty estimates for their predictions become crucial, espe-
cially in safety-critical areas such as healthcare. In this regard, Bayesian neural networks (BNNs)
(MacKay, 1995; Blundell et al., 2015; Gal & Ghahramani, 2016; Zhang et al., 2020) and deep
ensembles (Lakshminarayanan et al., 2017) represent two popular paradigms for estimating un-
certainty, which have shown promising results in applications such as (medical) image processing
(Kendall & Gal, 2017; Tanno et al., 2017) and out-of-distribution detection (Ovadia et al., 2019).
Though progress has been made, one major obstacle to scaling up BNNs and deep ensembles is the
computation cost in both time and space complexities. Especially for the latter, both approaches re-
quire the number of parameters to be several times higher than their deterministic counterparts.
Recent efforts have been made to improve their memory efficiency (Louizos & Welling, 2017;
Swiatkowski et al., 2020; Wen et al., 2020; Dusenberry et al., 2020). Still, these approaches re-
quire storage memory that is higher than storing a deterministic neural network.
Perhaps surprisingly, when taking the width of the network layers to the infinite limit, the resulting
neural network becomes “parameter efficient”. Indeed, an infinitely wide BNN becomes a Gaussian
process (GP) that is known for good uncertainty estimates (Neal, 1995; Matthews et al., 2018; Lee
et al., 2018). Effectively, the “parameters” of a GP are the datapoints, which have a considerably
smaller memory footprint. To further reduce the computational burden, sparse posterior approxima-
tions with a small number of inducing points are widely used (Snelson & Ghahramani, 2006; Titsias,
2009), rendering sparse GPS more memory efficient than their neural network counterparts.
Can We bring the advantages of sparse approximations in GPs - which are infinitely-wide neural
networks - to finite width deep learning models? We provide an affirmative answer regarding mem-
ory efficiency, by proposing an uncertainty quantification framework based on SParSe uncertainty
representations. We present our approach in BNN context, but the proposed approach is applicable
to deep ensembles as well. In details, our contributions are as follows:
1
Under review as a conference paper at ICLR 2021
• We introduce inducing Weights as auxiliary variables for uncertainty estimation in deep
neural networks with efficient (approximate) posterior sampling. Specifically:
-	We introduce inducing weights — lower dimensional counterparts to the actual weight
matrices — for variational inference in BNNs, as well as a memory efficient parame-
terisation and an extension to ensemble methods (Section 3.1).
-	We extend Matheron’s rule to facilitate efficient posterior sampling (Section 3.2).
-	We show the connection to sparse (deep) GPs, in that inducing weights can be viewed
as PrOjeCted noisy inducing OUtPUtS in pre-activation output space (Section 3.3).
-	We provide an in-depth computation complexity analysis (Section 3.4), showing the
significant advantage in terms of parameter efficiency.
• We apply the proposed approach to both BNNs and deep ensembles. Experiments in clas-
sification, model robustness and out-of-distribution detection tasks show that our inducing
weight approaches achieve competitive performance to their counterparts in the original
weight space on modern deep architectures for image classification, while reducing the
parameter count to less than half of that of a single neural network.
2 Variational inference with inducing variables
This section lays out the basics on variational inference and inducing variables for posterior approx-
imations, which serve as foundation and inspiration for this work. Given observations D = {X, Y}
with X = [x1, ..., xN], Y = [y1, ..., yN], we would like to fit a neural network p(y|x, W1:L) with
weights W1:L to the data. BNNs posit a prior distribution p(W1:L) over the weights, and construct an
approximate posterior q(Wi：L)to the intractable exact posterior p(Wi：lID) 8 p(D|Wi：l)p(Wi：l),
wherep(D|W1:L) = p(Y|X, W1:L) =QnN=1p(yn|xn,W1:L).
Variational inference Variational inference (Jordan et al., 1999; Zhang et al., 2018a) constructs an
approximation q(θ) to the posterior p(θ∣D) 8 p(θ)p(D∣θ) by maximising a variational lower-bound:
logP(D) ≥ L(q(θ)):= Eq(θ) [logp(D∣θ)] - KL [q(θ)l∣p(θ)].	(1)
For BNNs, θ = {W1:L}, and a simple choice of q is a fully factorised Gaussian (FFG):
q(W1:L) = QlL=1 Qid=lou1t Qjd=lin1 N (ml(i,j), vl(i,j)), with ml(i,j), vl(i,j) the mean and variance of Wl(i,j)
and dlin, dlout the respective number of inputs and outputs to layer l. The variational parameters are
then φ = {ml(i,j) , vl(i,j)}lL=1.
Gradients of (1) w.r.t. φ can be estimated with mini-batches of data
(Hoffman et al., 2013) and with Monte Carlo sampling from the q distribution (Titsias & LazarO-
Gredilla, 2014; Kingma & Welling, 2014). By setting q to an FFG, a variational BNN can be trained
with similar computational requirements as a deterministic network (Blundell et al., 2015).
Improved posterior approximation with inducing variables Auxiliary variable approaches
(Agakov & Barber, 2004; Salimans et al., 2015; Ranganath et al., 2016) construct the q(θ) dis-
tribution with an auxiliary variable a: q(θ) = / q(θ∣a)q(a)da, with the hope that a potentially richer
mixture distribution q(θ) can achieve better approximations. As then q(θ) becomes intractable, an
auxiliary variational lower-bound is used to optimise q(θ, a):
logP(D) ≥ L(q(θ, a)) = Eq(θ,a)[logp(D∣θ)] + Eq(θ,a)
^log p(θ)r(ɑθ).
.q(θla)q(a).
(2)
Here r(a∣θ) is an auxiliary distribution that needs to be specified, where existing approaches often
use a “reverse model” for r(a∣θ). Instead, we define r(θ∣a) in a generative manner: r(a∣θ) is the
“posterior” of the following “generative model”, whose “evidence” is exactly the prior of θ:
r(a∣θ) = p(a∣θ) H p(a)p(θ∣a), such thatp(θ) := Rp(a)p(θ∣a)da = p(θ).	(3)
Plugging in (3) to (2) immediately leads to:
L(q(θ, a)) = Eq(θ) [logP(D|。)] - Eq(a) [KL[q(0|a) ||p(0|a)]] — KL[q(a)||p(a)].	(4)
This approach returns an efficient approximate inference algorithm, translating the complexity of
inference in θ to a, if dim(a) < dim(θ) and q(θ, a) = q(θ∣a)q(a) has the following properties:
2
Under review as a conference paper at ICLR 2021
1.	A “pseudo prior” p(a)p(θ∣a) is defined such that Jp(a)p(θ∣a)da = p(θ);
2.	The conditional distributions q(θ∣a) and jρ(θ∖a) are in the same parametric family, so that
they can share parameters;
3.	Both sampling θ 〜q(θ) and computing KL[q(θ∖a)∖∖p(θ∖a)] can be done efficiently;
4.	The designs of q(a) and p(a) can potentially provide extra advantages (in time and space
complexities and/or optimisation easiness).
We call a the inducing VariabIe of θ, which is inspired by varationally sparse GP (SVGP) with in-
ducing points (Snelson & Ghahramani, 2006; Titsias, 2009). Indeed SVGP is a special case: θ = f,
a = u, the GP prior is p(f ∖X) = GP(0, Kχχ), P(U) = GP(0, Kzz), p(f, U) = p(u)p(f ∖X, u),
q(f ∖u) = p(f ∖X, u), q(f, u) = p(f ∖X, u)q(u), and Z are the optimisable inducing inputs. The vari-
ational lower-bound is L(q(f, U)) = Eq(f) [log p(Y∖f)]-KL[q(U)∖∖p(U)], and the variational param-
eters are φ = {Z, distribution parameters of q(U)}. SVGP satisfies the marginalisation constraint
(3) by definition, and it has KL[q(f ∖u) ∖∖p(f ∖u)] = 0. Also by using small M = dim(u) and exploit-
ing the q distribution design, SVGP reduces run-time from O(N3) to O(NM2) where N is the num-
ber of inputs in X, meanwhile it also makes storing a full Gaussian q(U) affordable. Lastly, U can
be whitened, leading to the “pseudo prior” p(f, V) = p(f ∖X, U = K；/2V)P(V),p(v) = N(v; 0, I)
which could bring potential benefits in optimisation.
In the rest of the paper We assume the “pseudo prior” p(θ, a) satisfies the marginalisation constraint
(3), allowing us to write p(θ, a) := p(θ, a). It might seem unclear how to design p(θ, a) for an
arbitrary probabilistic model, however, for a Gaussian prior on θ the rules for computing conditional
Gaussian distributions can be used to construct p. In section 3 we exploit these rules to develop an
efficient approximate inference method for Bayesian neural networks with inducing weights.
3 Sparse uncertainty representation with inducing weights
3.1	Inducing weights for neural network parameters
Following the design principles of inducing variables, we introduce to each network layer l a smaller
inducing weight matrix Ui, and construct joint approximate posterior distributions for inference. In
the rest of the paper we assume a factorised prior across layers p(W1:L) = Ql p(Wl ), and for
notation ease we drop the l indices when the context is clear.
Augmenting network layers with inducing weights Suppose the weight W ∈ Rdout ×din has
a Gaussian prior p(W) = p(vec(W)) = N(0, σ2I) where vec(W) concatenates the columns
of the weight matrix into a vector. A first attempt to augment p(vec(W )) with an inducing
weight variable U ∈ RMout ×Min may be to construct a multivariate Gaussian p(vec(W), vec(U)),
such that p(vec(W), vec(U))dU = N(0, σ2I). This means for the joint covariance matrix of
(vec(W ), vec(U)), it requires the block corresponding to the covariance of vec(W) to match the
prior covariance σ2 I. We are then free to parameterise the rest of the entries in the joint covari-
ance matrix, as long as this full matrix remains positive definite. Now the conditional distribution
p(W ∖U) is a function of these parameters, and the conditional sampling from p(W ∖U) is further
discussed in Appendix A.1. Unfortunately, as dim(vec(W)) is typically large (e.g. of the order of
107), using a full covariance Gaussian for p(vec(W), vec(U)) becomes computationally intractable.
This issue can be addressed using matrix normal distributions (Gupta & Nagar, 2018). Notice that
the prior p(vec(W )) = N(0, σ2I) has an equivalent matrix normal distribution form as p(W) =
MN (0, σr2I, σc2I), with σr, σc > 0 the row and column standard deviations satisfying σ = σrσc.
Now we introduce the inducing variables in matrix space, in addition to U we pad in two auxiliary
variables Ur ∈ RMout ×din, Uc ∈ Rdout ×Min, so that the full augmented prior is:
with Lr
and Lc
WUc)〜p(W, Uc, Ur , U) := MN(0, Σr, Σc)
s.t. Σc = LcLc>
σr2I
σc
ZcZc>
Zr Zr>
(5)
3
Under review as a conference paper at ICLR 2021
(a) inducing weight
augmentation
U
(b) Matheron's rule (for vectors)
-1
×(
(c) Extended Matheron's rule (for matrices)
(ours)
Figure 1: Visualisation of (a) the inducing weight augmentation, and compare (b) the original Math-
eron’s rule to (c) our extended version. The white blocks represent random noises from the joint.
)
See Figure 1(a) for a visualisation. Matrix normal distributions have similar marginalization and
conditioning properties as multivariate Gaussian distributions. Therefore the marginalisation con-
straint (3) is satisfied for any Zc , Zr , Dc and Dr . The marginal distribution of the inducing weight
is p(U) = MN (0, Ψr , Ψc) with Ψr = Zr Zr> + Dr2 and Ψc = ZcZc> + Dc2 . In the experiments
We use Whitened inducing weights which transforms U so thatP(U) = MN(0, I, I) (Appendix E),
but for clarity, we continue using the formulas presented above in the main text.
The matrix normal parameterisation introduces two additional variables Ur , Uc without providing
additional expressiveness. Hence it is desirable to integrate them out, leading to a joint multivariate
normal with Khatri-Rao product structure for the covariance:
σ210 σ2I	QcZ> 0 QrZ>∖∖
P(Vec(W), Vec(U))= N P (σcZc 0 QrZr	Ψcc 0 Ψr	))-	⑹
As the dominating memory complexity here is O(doutMout+dinMin) which comes from storing Zr
and Zc, we see that the matrix normal parameterisation of the augmented prior is memory efficient.
Posterior approximation in the joint space We construct a factorised posterior approximation
across the layers: q(W1:L, U1:L) = Ql q(Wl|Ul)q(Ul).
The simplest option for q(W|U) is q(W|U) = P(Vec(W)| Vec(U)) = N(μw∣u, ∑w∣u), similar to
sparse GPs. A slightly more flexible variant rescales the covariance matrix while keeping the mean
tied, i.e. q(W|U) = q(vec(W)| Vec(U)) = N(μw∣u, λ2∑w∣u), which still allows for the KL term
to be calculated efficiently (see Appendix B):
R(λ) := KL [q(W |U)||P(W |U)] = dindout(0.5λ2 - logλ - 0.5), W ∈ Rdout×din.	(7)
Plugging θ = {W1:L }, a = {U1:L} into (4) results in the following variational lower-bound
L(q(W1:L, U1:L)) = Eq(W1:L)[log P(D|W1:L)] - PlL=1(R(λl) + KL[q(Ul)||P(Ul)]),	(8)
with λl the associated scaling parameter for q(Wl |Ul). Therefore the variational parameters are now
φ = {Zc, Zr, Dc, Dr, λ, dist. params. of q(U)} for each network layer.
Two choices of q(U) A simple choice is FFG q(Vec(U)) = N(mu, diag(vu)), which performs
mean-field inference in U space (c.f. Blundell et al., 2015), and in this case KL[q(U)||P(U)] has
a closed-form solution. Another choice is a “mixture of delta measures” q(U)= 表 PK=I δ(U =
U(k)). In other words, we keep K distinct sets of parameters {U1(:kL)}kK=1 in inducing space that are
projected back into the original parameter space via the shared conditional distributions q(Wl |Ul) to
obtain the weights. This approach can be viewed as constructing “deep ensembles” in U space, and
we follow ensemble methods (e.g. Lakshminarayanan et al., 2017) to drop KL[q(U)||P(U)] in (8).
Often the inducing weight U is chosen to have significantly lower dimensions compared to W.
Combining with the fact that q(W|U) and P(W|U) only differ in the covariance scaling constant,
we see that U can be regarded as a SParSe represent如ion of UnCertainty for the network layer, as the
major updates in (approximate) posterior belief is quantified by q(U).
3.2	Efficient sampling with extended Matheron’ s rule
Computing the variational lower-bound (8) requires samples from q(W), which asks for an efficient
sampling procedure for the conditional q(W|U). Unfortunately, q(W|U) derived from (6) with
4
Under review as a conference paper at ICLR 2021
noisy proj.
X
noisy proj.
ReLU
X
ReLU
K	hl
X
h2
X
Figure 2: Showing the U variables in pre-activation spaces. To simplify we set σc = 1 w.l.o.g.
covariance rescaling is not a matrix normal, so direct sampling remains prohibitively expensive. To
address this challenge, We extend Matheron,s rule (JoUmeI & Huijbregts, 1978; Hoffman & Ribak,
1991; Doucet, 2010) to efficiently sample from q(W |U). The idea is that one can sample from a
conditional Gaussian by transforming a sample from the joint distribution. In detail, We derive in
Appendix C the extended Matheron,s rule to sample W 〜q(W|U):
W = λW + σZ> Ψ-1(U - λU)Ψ-1Zc, W ,U 〜P(W, Uc, Ur, U) = MN (0, ∑r, Σ°).	(9)
Here W ,tJ 〜P(W, Uc, Ur, U) means we sample W, Uc, Ur, U from the joint and drop Uc, Ur. In
fact Uc, Ur are never computed: as shown in Appendix C, the samples W, U can be obtained by:
W = σEι, U = ZrEιZ> + LrE2Dc + DrE3L> + DrE4Dc, El 〜MN(0,Idout,Idin),
E2,E3,E4 〜MN(0,lMout,lMin), Lr = Cholesky(ZrZ>), L° = Cholesky(Z°Z>)「
Therefore the major extra cost to pay is O(2Mo3ut + 2Mi3n + doutMoutMin + Mindoutdin) required
by inverting Ψr , Ψc, computing Lr, Lc, and the matrix multiplications. The extended Matheron,s
rule is visualised in Figure 1 with a comparison to the original Matheron,s rule for sampling from
q(vec(W)| vec(U)). This clearly shows that our recipe avoids computing big matrix inverses and
multiplications, resulting in a significant speed-up for conditional sampling.
3.3	Understanding inducing weights: a function-space perspective
We present the proposed approach again but from a function-space inference perspective. Assume a
network layer computes the following transformation of the input X = [x1, ..., xN], xi ∈ Rdin×1:
F = WX, H = g(F), W ∈ Rdout×din, X ∈ Rdin×N,g(∙) is the non-linearity.	(11)
As W has a Gaussian priorP(vec(W)) = N(0, σ2I), each of the rows in F = [f1, ..., fdout]>, fi ∈
RN×1 has a Gaussian process form with linear kernel: fi|X 〜 GP(0, KXX), KXX(m, n) =
σ2x>mxn. Inference on F directly has O(N3 + doutN2) cost, so a sparse approximation is needed.
Slightly different from the usual approach, we introduce auxiliary variables Uc = [uc1, ..., ucd ]> ∈
Rdout×Min as follows, using shared “inducing inputs” Zc> ∈ Rdin ×Min:
p(fi, Ui|X) = GP (O, K[X,Z>],[X,Z>]) , P(UcIui)= N (Ui∕σc,σrD2) ,	(12)
By marginalising out the “noiseless inducing outputs” {Ui} (derivations in Appendix D), we can
compute the marginal distributions as P(Uc) := P({Uic}) = MN (0, σr2I, Ψc) and
p(F∣X, UC) = MN (UcΨ-1σcZcX, σrI, Xτσ∣(I - Z>Ψ-1Zc)X) .	(13)
Note that p(W∣Uc) = MN (UcΨ-1σcZc, σrI, σ2(I 一 Z>Ψ-1Zc)) (see Appendix A.2). Since
W 〜MN (M, ∑ι, ∑2) leads to W X 〜MN (MX, ∑ι, Xτ∑2X), this means p(F∣X, UC) is the
push-forward distribution ofP(W|Uc) for the operation F = WX:
F 〜p(F∣X, Uc)	⇔ W 〜p(W∣Uc), F = WX.
As {呜} are the “noisy” versions of {Ui} in f space, UC can thus be viewed as “scaled noisy inducing
outputs” in function space (see the red bars in the 2nd column of Figure 2).
So far the inducing variables UC is used as a compact representation of the correlation between
columns of F only. In other words the output dimension d°u for each f (and uc) remains large
(e.g. > 1000 in a fully connected layer). Therefore dimensionality reduction can be applied to the
5
Under review as a conference paper at ICLR 2021
Table 1: Computational complexity for a single layer. We assume W ∈ Rdout×din, U ∈
RMout ×Min, and κ forward passes are made for each of the N inputs. (*It uses a parallel Com-
puting friendly vectorisation technique (Wen et al., 2020) for further speed-up.)
Method	Time complexity	Memory complexity
Deterministic-W	O(Ndindout)	O(dindout)
FFG-W Ensemble-W Matrix-normal-W k-tied FFG-W rank-1 BNN	O(NKdindout)	O(2dindout) O(NKdindout)	O(Kdindout) O(NKdindout)	O(dindout + din + dout) O(NKdindout)	O(dindout + k(din + dout)) O(NKdindout¥	O(dindout + 2(din + dout))
FFG-U	O(NKdindout + 2M3n + 2 MOut	O(dinMin + d out Mout + 2 MinMout) +K (dout Mout Min + Mindoutdin))
Ensemble-U	same as above	O(dinMin + doutMout + KMinMout)
ColUmn VectorS of Uc and F. In Appendix D we present a generative approach to do so by extending
probabilistic PCA (Tipping & Bishop, 1999) to matrix normals: P(Uc) = Rp(Uc∣U)p(U)dU, where
the projection’s parameters are {Zr, Dr}, andp(Uc, U) matches the marginals of (5). This means U
can be viewed as the “noisy projected inducing output” of the GP whose corresponding “inducing
inputs” are Zc> (see the red bar in the 1st column of Figure 2). Similarly the column vectors in UrX
can be viewed as the noisy projections of the column vectors in F.
In Appendix D we further show that the push-forward distribution q(F|X, U) of q(W |U) only
differs from p(F|X, U) in the covariance matrices up to the same scaling constant λ. Therefore the
resulting function-space variational objective is almost identical to (8), except for scaling coefficients
that are added to the R(λl) terms to account for the change in dimensionality from vec(W) to
vec(F). This result nicely connects posterior inference in weight- and function-space.
3.4	Computational complexities
In Table 1 we report the computational complexity figures for two types of inducing weight ap-
proaches: FFG q(U) (FFG-U) and Delta mixture q(U) (Ensemble-U). Baseline approaches in-
clude: Deterministic-W, variational inference with FFG q(W) (FFG-W, Blundell et al., 2015), deep
ensemble in W (Ensemble-W, Lakshminarayanan et al., 2017), as well as parameter efficient ap-
proaches such as matrix-normal q(W) (Matrix-normal-W, Louizos & Welling (2017)), variational
inference with k-tied FFG q(W) (k-tied FFG-W, Swiatkowski et al. (2020)), and rank-1 BNN
(Dusenberry et al., 2020). The gain in memory is significant for the inducing weight approaches, in
fact with Min < din and Mout < dout the parameter storage requirement is smaller than a single
deterministic neural network. The major overhead in run-time comes from the extended Matheron’s
rule for sampling q(W |U). Some of the computations there are performed only once, and in our
experiments we show that by using a relatively low-dimensional U, the overhead is acceptable.
4	Experiments
We evaluate the inducing weight approaches on regression, classification and related uncertainty
estimation tasks. The goal is to demonstrate competitive performance to popular W -space uncer-
tainty estimation methods while using significantly fewer parameters. The evaluation baselines are:
(1) variational inference with FFG q(W) (FFG-W, Blundell et al., 2015) v.s. FFG q(U) (FFG-U,
ours); (2) ensemble methods in W space (Ensemble-W Lakshminarayanan et al., 2017) v.s. ensem-
ble in U space (Ensemble-U, ours). Another baseline is training a deterministic neural network with
maximum likelihood. Details and additional results can be found in Appendix F and G.
4.1	Synthetic 1-D regression
We follow Foong et al. (2019) to construct a synthetic regression task, by sampling two clusters of
inputs xι ~ U [-1, -0.7], x2 ~ U [0.5,1], and targets y ~ N(cos(4x + 0.8), 0.01). As ground
truth we show the exact posterior results using the NUTS sampler (Hoffman & Gelman, 2014). The
results are visualised in Figure 3 with the noiseless function in black, predictive mean in blue, and
6
Under review as a conference paper at ICLR 2021
(b) FFG-U
(a) FFG-W
(c) FCG-W
Figure 3: Toy regression results, with observations in red dots and the ground truth function in black.
speed
model size
Table 2: CIFAR in-distribution metrics (in %).
Method	CIFAR10		CIFAR100	
	Acc. ↑	ECE J	Acc. ↑	ECE J
Deterministic-W	93.02	5.23	72.68	19.41
Ensemble-W	94.94	1.25	76.61	6.25
FFG-W	93.22	0.55	73.44	5.49
FFG-U	91.52	1.31	75.69	5.20
Ensemble-U	92.20	0.80	76.10	2.49
- - -
Ooo
3 2 1
E 一Uru≥.!
FFG-W (N = IOO)
FFG-W (N=500)
FFG-U (M=128f N=100}
FFG-U (M=128, N=500}
FFG-U (M=256, N=100)
FFG-U (M=256, N=500}
FFG-W
deterministic
3264 128	256
M
12 4	8	16	32
number of samples (K)
xxx
ΦN-ω ∙luajed e>4-∂J
Figure 4: Resnet-18 run-times and model sizes.
up to three standard deviations as shaded area. Similar to prior results in the literature, FFG-W
fails to represent the increased uncertainty away from the data and in between clusters. While un-
derestimating predictive uncertainty overall, FFG-U show a small increase in predictive uncertainty
away from the data. In contrast, a per-layer full covariance Gaussian in both weight (FCG-W) and
inducing space (FCG-U) as well as Ensemble-U better captures the increased predictive variance,
although the mean function is more similar to that of FFG-W.
4.2	Classification and in-distribution calibration
As the core empirical evaluation, we train Resnet-18 models (He et al., 2016b) on CIFAR-10 and
CIFAR-100 (Krizhevsky et al., 2009). To avoid underfitting issues with FFG-W, a useful trick is to
set an upper limit σm2 ax on the variance of q(W) (Louizos & Welling, 2017). This trick is similarly
applied to the U -space methods, where we cap λ ≤ λmax for q(W |U), and for FFG-U we also set
σm2 ax for the variance of q(U). In convolution layers, we treat the 4D weight tensor W of shape
(cout, cin, h, w) as a cout × cinhw matrix. We use U matrices of shape 128 × 128 for all layers
(i.e. M = Min = Mout = 128), except that for CIFAR-10 we set Mout = 10 for the last layer.
In Table 2 we report test accuracy and test expected calibration error (ECE) (Guo et al., 2017) as a
first evaluation of the uncertainty estimates. Overall, Ensemble-W achieves the highest accuracy,
but is not as well-calibrated as variational methods. For the inducing weight approaches, Ensemble-
U outperforms FFG-U on both datasets. It is overall the best performing approach on the more
challenging CIFAR-100 dataset (close-to-Ensemble-W accuracy and lowest ECE).
In Figure 4 we show prediction run-times on trained models, relative to those of an ensemble of
deterministic networks, as well as relative parameter sizes to a single ResNet-18. The extra run-time
costs for the inducing methods come from computing the extended Matheron’s rule. However, as
they can be calculated once and then cached when drawing multiple samples, the overhead reduces
to a small factor when using larger number of samples K and large batch-size N . More impor-
tantly, when compared to a deterministic ResNet-18 network, the inducing weight models reduce
the parameter count by over 50% (5, 352, 853 vs. 11, 173, 962, 47.9%) even for a large M = 128.
Hyper-parameter choices We visualise in Figure 5 the accuracy and ECE results for the inducing
weight models with different hyper-parameters. It is clear from the right-most panels that perfor-
mances in both metrics improve as the U matrix size M is increased, and the results for M = 64 and
M = 128 are fairly similar. Also setting proper values for λmax , σmax is key to the improved per-
formances. The left-most panels show that with fixed σmax values (or with ensemble in U space),
the preferred conditional variance cap values λmax are fairly small (but still larger than 0 which
corresponds to a point estimate for W given U). For σmax which controls variance in U space, we
see from the top middle panel that the accuracy metric is fairly robust to σmax as long as λmax is
not too large. But for ECE, a careful selection of σmax is required (bottom middle panel).
7
Under review as a conference paper at ICLR 2021
■ FFG-U1 GmaX=O.10
■ FFG-U, σmax=0.30
—Ensemble-U
O 5
9 8
(求)Aue-I muπ
→- FFG-UJmaX=O∙03
→- FFG-UMmax=O-IO
→- FFG-UMmax= 0.30
O 5
9 8
(％)Aue,Jr□ura
0	0.1	0.3
O O
9 8
(％)AueJnuue
FFG-U, OmaX=O.1，λmax=0.03
Ensemble-U, Λmax=0.03
Ensemble-U1 Λmaκ=0.1
I16
I32
Figure 5: Averaged CIFAR-10 accuracy (↑) and ECE Q) results for the inducing weight methods
with different hyper-parameters. Models reported in the first two-columns uses M = 128 for U
dimensions. For λmax = 0 (and σmax = 0) we use point estimates for the corresponding variables.
Ensemble-W	FFG-W
CIFAR-10
806040
Oooo
8 6 4 2
(2)AuB-Jmuv
0	12	3	4
Skew intensity
Figure 6: Accuracy (↑) and ECE ⑷ on corrupted CIFAR. We show the mean and two standard
errors for each metric on the 19 perturbations provided in (Hendrycks & Dietterich, 2019).
O Oooo
3 2 1
(％)um (％)um
FFG-U Ensemble-U
CIFAR-10
0	12	3	4
CIFAR-100
0	12	3	4
Skew intensity
4.3	Model robustness and Out-of-distribution detection
To investigate the inducing weight’s robustness to dataset shift, we compute predictions on corrupted
CIFAR datasets (Hendrycks & Dietterich, 2019) after training on clean data. Figure 6 shows accu-
racy and ECE results. Ensemble-W is the most accurate model across skew intensities, while FFG-
W, though performing well on clean data, returns the worst accuracy under perturbation. The induc-
ing weight methods perform competitively to Ensemble-W, although FFG-U surprisingly maintains
slightly higher accuracy on CIFAR-100 than Ensemble-U despite being less accurate on the clean
data. In terms of ECE, the inducing weight methods again perform competitively to Ensemble-W,
with Ensemble-U sometimes being the best among the three. Interestingly, while the accuracy of
FFG-W decays quickly as the data is perturbed more strongly, its ECE remains roughly constant.
We further present in Table 3 the utility of the maximum predicted probability for out-of-distribution
(OOD) detection when presented with both the in-distribution data (CIFAR10 and CIFAR100 test
sets) and an OOD dataset (CIFAR100/SVHN and CIFAR10/SVHN). The metrics are the area under
the receiver operator characteristic (AUROC) and the area under the precision-recall curve (AUPR).
Again Ensemble-W performs the best in most settings, but more importantly, the inducing weight
methods achieve very close results despite using the smallest number of parameters.
5	Related Work
Parameter-efficient uncertainty quantification methods Recent research has proposed Gaus-
sian posterior approximations for BNNs with efficient covariance structure (Ritter et al., 2018; Zhang
et al., 2018b; Mishkin et al., 2018). The inducing weight approach differs from these in introducing
structure via a hierarchical posterior with low-dimensional auxiliary variables. Another line of work
reduces the memory overhead via efficient parameter sharing (Louizos & Welling, 2017; Wen et al.,
8
Under review as a conference paper at ICLR 2021
Table 3: OOD detection metrics for Resnet-18 trained on CIFAR10/100.
In-dist. OOD Method / Metric	CIFAR10				CIFAR100			
	CIFAR100		SVHN		CIFAR10		SVHN	
	AUROC	AUPR	AUROC	AUPR	AUROC	AUPR	AUROC	AUPR
Deterministic-W	.87±.0θ	.86±.00	.92±.01	.88±.02	.73±.00	.76±.00	.80±.00	.72±.01
Ensemble-W	.89	.91	.95	.94	.77	.80	.85	.77
FFG-W	.87±.0θ	.89±.00	.89±.01	.86±.01	.75±.00	.78±.00	.79±.02	.67±.04
FFG-U	.86±.0θ	.88±.00	.90±.00	.87±.01	.77±.00	.79±.00	.84±.01	.76±.01
Ensemble-U	.86±.0θ	.88±.00	.89±.01	.84±.02	.77±.00	.80±.00	.83±.00	.74±.01
2020; Swiatkowski et al., 2020; Dusenberry et al., 2020). They all maintain a “mean parameter”
for the weights, making the memory footprint at least that of storing a deterministic neural network.
Instead, our approach shares parameters via the augmented prior with efficient low-rank structure,
reducing the memory use compared to a deterministic network. In a similar spirit to our approach, Iz-
mailov et al. (2019) perform inference in a d-dimensional sub-space obtained from PCA on weights
collected from an SGD trajectory. However, this approach does not leverage the layer-structure of
neural networks and requires d× memory of a single network.
Sparse GP and function-space inference As BNNs and GPs are closely related (Neal, 1995;
Matthews et al., 2018; Lee et al., 2018), recent efforts have introduced GP-inspired techniques to
BNNs (Ma et al., 2019; Sun et al., 2019; Khan et al., 2019; Ober & Aitchison, 2020). Compared to
weight-space inference, function-space inference is appealing since its uncertainty is more directly
relevant for many predictive uncertainty estimation tasks. While the inducing weight approach per-
forms computations in weight-space, Section 3.3 establishes the connection to function-space pos-
teriors. Our approach is related to sparse deep GP methods with Uc having similar interpretations
as inducing outputs in e.g. Salimbeni & Deisenroth (2017). The major difference is that U lies in a
low-dimensional space, projected from the pre-activation output space ofa network layer.
Priors on neural network weights Hierarchical priors for weights has also been explored
(Louizos et al., 2017; Krueger et al., 2017; Atanov et al., 2019; Ghosh et al., 2019; Karaletsos & Bui,
2020). However, We emphasise that p(W, U) is a pseudo prior that is constructed to assist POSteriOr
inference rather than to improve model design. Indeed, parameters associated with the inducing
weights are optimisable for improving posterior approximations. Our approach can be adapted to
other priors, e.g. for a Horseshoe prior p(θ, V) = p(θ∣ν)p(ν) = N(θ; 0, V2)C +(ν; 0,1), the pseudo
prior can be defined as p(θ, ν, a) = p(θ∣ν, a)p(a)p(ν) such that Jp(θ∣ν, a)p(a)da = p(θ∣ν). In
general, pseudo priors have found broader success in Bayesian computation (Carlin & Chib, 1995).
6	Conclusion
We have proposed a parameter-efficient uncertainty quantification framework for neural networks.
It augments each of the network layer weights with a small matrix of inducing weight, and by ex-
tending Matheron’s rule to matrix-normal related distributions, maintains a relatively small run-time
overhead as compared with ensemble methods. Critically, experiments on prediction and uncertainty
estimation tasks demonstrate the competence of the inducing weight methods to the state-of-the-art,
while reducing the parameter count to less than half of a deterministic ResNet-18.
Several directions are to be explored in the future. First, modelling correlations across layers might
further improve the inference quality. We outline an initial approach leveraging inducing variables
in Appendix E. Second, based on the function-space interpretation of inducing weights, better ini-
tialisation techniques can be inspired from the sparse GP and dimension reduction literature. Lastly,
the small run-time overhead of our approach can be mitigated by a better design of the inducing
weight structure as well as vectorisation techniques amenable to parallelised computation.
References
Felix V Agakov and David Barber. An auxiliary variational method. In International COnferenCe on
NeUral InfOrmatiOn PrOCeSsing, pp. 561-566. Springer, 2004.
9
Under review as a conference paper at ICLR 2021
Andrei Atanov, Arsenii Ashukha, Kirill Struminsky, Dmitriy Vetrov, and Max Welling. The deep
weight prior. In International Conference on Learning Representations, 2019.
Eli Bingham, Jonathan P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis
Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D Goodman. Pyro: Deep universal
probabilistic programming. The JoUrnal OfMachine Learning Research, 20(1):973-978, 2019.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
in neural networks. In Proceedings of the 32nd International Conference on International
Conference on Machine Learning, pp. 1613-1622, 2015.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv PrePrint arXiv:2005.14165, 2020.
Bradley P Carlin and Siddhartha Chib. Bayesian model choice via markov chain monte carlo meth-
ods. JoUrnal of the Royal Statistical Society: Series B (Methodological), 57(3):473^84, 1995.
Arnaud Doucet. A note on efficient conditional simulation of gaussian distributions. Technical
report, University of British Columbia, 2010.
Michael W Dusenberry, Ghassen Jerfel, Yeming Wen, Yi-an Ma, Jasper Snoek, Katherine Heller,
Balaji Lakshminarayanan, and Dustin Tran. Efficient and scalable Bayesian neural nets with
rank-1 factors. In Proceedings of the 37th IntemationaI Conference on InternationaI Conference
on Machine Learning, pp. 9823-9833, 2020.
Andrew YK Foong, Yingzhen Li, Jose Miguel Hernandez-Lobato, and Richard E Turner. ,in-
between' uncertainty in Bayesian neural networks. arXiv PrePrint arXiv:1906.11537, 2019.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine Iearning, pp. 1050-1059,
2016.
Soumya Ghosh, Jiayu Yao, and Finale Doshi-Velez. Model selection in Bayesian neural networks
via horseshoe priors. JoUrnaI of Machine Learning Research, 20(182):1^6, 2019.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321-1330, 2017.
Arjun K Gupta and Daya K Nagar. Matrix Variate distributions, volume 104. CRC Press, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer Vision and Pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In EUroPean conference on computer vision, pp. 630-645. Springer, 2016b.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. arXiv PrePrint arXiv:1903.12261, 2019.
Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths
in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593-1623, 2014.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. The JoUrnal ofMachine Learning Research, 14(1):1303-1347, 2013.
Yehuda Hoffman and Erez Ribak. Constrained realizations of gaussian fields-a simple algorithm.
The AstroPhysical Journal, 380:L5-L8, 1991.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer Vision and Pattern
recognition, pp. 47004708, 2017.
10
Under review as a conference paper at ICLR 2021
Pavel Izmailov, Wesley J Maddox, Polina Kirichenko, Timur Garipov, Dmitry Vetrov, and An-
drew Gordon Wilson. SUbsPace inference for Bayesian deep learning. In Uncertainty in ArtificiaI
Intelligence, 2019.
Michael I Jordan, ZoUbin Ghahramani, Tommi S Jaakkola, and Lawrence K SaUl. An introdUction
to variational methods for graphical models. Machine Iearning, 37(2):183-233, 1999.
Andre G Journel and Charles J Huijbregts. Mining geostatistics. Academic press London, 1978.
Theofanis Karaletsos and Thang D BUi. Hierarchical gaUssian process priors for Bayesian neUral
network weights. arXiv PrePrint arXiv:2002.04033, 2020.
Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for computer
vision? In AdvanceS in neural information PrOceSSing systems, pp. 5574—5584, 2017.
Mohammad Emtiyaz E Khan, Alexander Immer, Ehsan Abedi, and Maciej Korzepa. Approxi-
mate inference turns deep networks into gaussian processes. In AdvanceS in neural information
processing systems, pp. 3094-3104, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv PrePrint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International COnference
on Learning RePreSentations, 2014.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. URl:
https:〃www. cs.toronto. edu/kriz/cifar. html, 6:1, 2009.
David Krueger, Chin-Wei Huang, Riashat Islam, Ryan Turner, Alexandre Lacoste, and Aaron
Courville. Bayesian hypernetworks. arXiv PrePrint arXiv:1710.04759, 2017.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In AdVanceS in neural information PrOceSSing
SyStems, pp. 6402-6413, 2017.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International COnference on
Learning RePreSentations, 2018.
Christos Louizos and Max Welling. Multiplicative normalizing flows for variational Bayesian neural
networks. In PrOceedingS of the 34th International COnference on Machine Learning-Volume 70,
pp. 2218-2227, 2017.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In
AdVanceS in neural information PrOceSSing systems, pp. 3288-3298, 2017.
Chao Ma, Yingzhen Li, and Jose Miguel Hernandez-Lobato. Variational implicit processes. In
International COnference on Machine Learning, pp. 4222T233, 2019.
David JC MacKay. Bayesian neural networks and density networks. NUcIear InStrUmentS and
MethOdS in PhySicS ReSearch SectiOn A: AcceIerators, Spectrometers, DetectOrS and ASSOciated
EqUiPment, 354(1):73-80, 1995.
Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahra-
mani. Gaussian process behaviour in wide deep neural networks. In International COnference on
Learning RePreSentations, 2018.
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and Mohammad Emtiyaz Khan.
Slang: Fast structured covariance approximations for Bayesian deep learning with natural gradi-
ent. In AdVanceS in NeuraI InfOrmatiOn PrOceSSing Systems, pp. 6245-6255, 2018.
Radford M Neal. BayeSian Learning for NeUraI Networks. PhD thesis, University of Toronto, 1995.
Sebastian W. Ober and Laurence Aitchison. Global inducing point variational posteriors for
Bayesian neural networks and deep gaussian processes. arXiv PrePrint arXiv:2005.08140, 2020.
11
Under review as a conference paper at ICLR 2021
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dil-
lon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? eval-
Uating predictive uncertainty under dataset shift. In Advances in NeUral Information Processing
Systems, pp.13991-14002, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In ProceedingS of
the 33rd IntemationaI Conference on Machine Learning, pp. 324-333, 2016.
Hippolyt Ritter, Aleksandar Botev, and David Barber. A scalable laplace approximation for neural
networks. In International ConferenCe on Learning RePreSentations, 2018.
Tim Salimans, Diederik Kingma, and Max Welling. Markov chain Monte Carlo and variational
inference: Bridging the gap. In ProCeedingS of the 32nd International Conference on MaChine
Learning, pp. 1218-1226, 2015.
Hugh Salimbeni and Marc Deisenroth. Doubly stochastic variational inference for deep gaussian
processes. In AdvanCeS in NeUral Information ProCeSSing Systems, pp. 4588T599, 2017.
Edward Snelson and Zoubin Ghahramani. Sparse gaussian processes using pseudo-inputs. In
AdvanCeS in neural information ProCeSSing systems, pp. 1257-1264, 2006.
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse. Functional variational Bayesian
neural networks. In International ConferenCe on Learning RePreSentations, 2019.
Jakub Swiatkowski, Kevin Roth, Bastiaan S Veeling, Linh Tran, Joshua V Dillon, Stephan Mandt,
Jasper Snoek, Tim Salimans, Rodolphe Jenatton, and Sebastian Nowozin. The k-tied normal
distribution: A compact parameterization of gaussian mean field posteriors in Bayesian neural
networks. In ProCeedingS of the 37th International Conference on IntemationaI ConferenCe on
MaChine Learning, pp. 6631-6641, 2020.
Ryutaro Tanno, Daniel E Worrall, Aurobrata Ghosh, Enrico Kaden, Stamatios N Sotiropoulos, An-
tonio Criminisi, and Daniel C Alexander. Bayesian image quality transfer with Cnns: exploring
uncertainty in dmri super-resolution. In International ConferenCe on MediCal Image ComPUting
and ComPUter-ASSiSted Intervention, pp. 611-619. Springer, 2017.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. JoUrnal
of the Royal StatiStiCaI Society: SerieS B (StatiStiCaI Methodology), 61(3):611-622, 1999.
Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In ArtifiCial
Intelligence and StatiStics, pp. 567-574, 2009.
Michalis Titsias and Miguel Lazaro-Gredilla. Doubly stochastic variational bayes for non-conjugate
inference. In InternationaI ConferenCe on machine Iearning, pp. 1971-1979, 2014.
Yeming Wen, Dustin Tran, and Jimmy Ba. Batchensemble: an alternative approach to efficient
ensemble and lifelong learning. In International ConferenCe on Learning RePreSentations, 2020.
Hu Xu, Bing Liu, Lei Shu, and Philip Yu. BERT post-training for review reading comprehension and
aspect-based sentiment analysis. In ProCeedingS of the 2019 ConferenCe of the North AmeriCan
ChaPter of the ASSoCiation for ComPUtationaI LingUiStics: HUman LangUage Technologies,
VQIUme 1 (Long and Short Papers). Association for Computational Linguistics, 2019.
Cheng Zhang, Judith BUtepage, Hedvig Kjellstrom, and Stephan Mandt. Advances in variational
inference. IEEE transactions on Pattern analysis and machine intelligence, 41(8):2008-2026,
2018a.
Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger Grosse. Noisy natural gradient as
variational inference. In IntemationaI ConferenCe on MaChine Learning, pp. 5852-5861, 2018b.
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew Gordon Wilson. Cyclical
stochastic gradient mcmc for Bayesian deep learning. In International ConferenCe on Learning
RePreSentations, 2020.
12