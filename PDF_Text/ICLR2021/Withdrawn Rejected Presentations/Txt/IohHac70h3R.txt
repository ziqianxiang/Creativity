Under review as a conference paper at ICLR 2021
On the Marginal Regret Bound Minimization
of Adaptive Methods
Anonymous authors
Paper under double-blind review
Ab stract
Numerous adaptive algorithms such as AMSGrad and Radam have been proposed
and applied to deep learning recently. However, these modifications do not improve
the convergence rate of adaptive algorithms and whether a better algorithm exists
still remains an open question. In this work, we propose a new motivation for
designing the proximal function of adaptive algorithms, named as marginal regret
bound minimization. Based on such an idea, we propose a new class of adaptive
algorithms that not only achieves marginal optimality, but can also potentially
converge much faster than any existing adaptive algorithms in the long term. We
show the superiority of the new class of adaptive algorithms both theoretically and
empirically using experiments in deep learning.
1	Introduction
Accelerating the convergence speed of optimization algorithms is one main concern of the machine
learning community. After stochastic gradient descent (SGD) was introduced, quite a few variants of
SGD have become popular, such as momentum (Polyak, 1964) and AdaGrad (Duchi et al., 2011).
Instead of directly moving parameters in the negative direction of the gradient, AdaGrad proposed
to scale the gradient by a matrix, which was the matrix in the proximal function of the composite
mirror descent rule (Duchi et al., 2011). The diagonal version of AdaGrad designed this matrix to be
the square root of the global average of the squared gradients. Duchi et al. (2011) proved that this
algorithm could be faster than SGD when the gradients were sparse.
However, AdaGrad’s performance is known to deteriorate when the gradients are dense, especially in
high dimensional problems such as deep learning (Reddi et al., 2018). To tackle this issue, many new
algorithms were proposed to boost the performances of AdaGrad. Most of these algorithms focused
on changing the design of the matrix in the proximal function. For example, RMSProp (Tieleman
& Hinton, 2012) and Adam (Kingma & Ba, 2015) changed the global average design in AdaGrad
to the exponential moving average. However, Reddi et al. (2018) proved that such a modification
had convergence issues in the presence of high frequency noises and added a max operation to the
matrix of Adam, leading to the AMSGrad algorithm. Other modifications, such as Padam (Chen
& Gu, 2018), AdaShift (Zhou et al., 2019), NosAdam (Huang et al., 2019), and Radam (Liu et al.,
2019), were based on various designs of this matrix as well. However, all aforementioned works did
not improve the convergence rate of AdaGrad and simply supported their designs using experiments
and synthetic examples. A theoretical foundation for the design of this matrix that improves the
convergence and guides future adaptive algorithms is very much needed.
In this work, we bring new insights to the design of the matrix in the proximal function. In particular,
our major contributions in this paper are listed as follows
•	We propose a new motivation for designing the proximal function in adaptive algorithms.
Specifically, we have found a marginally optimal design, which is the best matrix at each
time step through minimizing the marginal increment of the regret bound.
•	Based on our proposal of marginal regret bound minimization, we create a new class of
adaptive algorithms, named as AMX. We prove theoretically that AMX can converge with a
regret bound of Size O(√τ), where T is smaller than T. Such a regret bound is potentially
much smaller than those of common adaptive algorithms and can make AMX converge
1
Under review as a conference paper at ICLR 2021
much faster than any existing adaptive algorithms, depending on τ . In the worst case, we
show it is at least as fast as AMSGrad and AdaGrad under the same assumptions
•	We evaluate AMX’s empirical performance on different tasks in deep learning. All experi-
ments show our algorithm can converge fast and achieve good testing performances.
2	Background
Notation: We denote the set of all positive definite matrices in Rd×d by Sd+ . For any two vectors
a, b ∈ Rd, We use √a for element-wise square root, a2 for element-wise square, |a| for element-wise
absolute value, a/b for element-wise division, and max(a, b) for element-wise maximum between a
and b. We also frequently use the notation gi：T,i = [gι,i, g2,i,…,gτ,i], i.e. the vector of all the i-th
elements of vectors g1,g2,…gτ. For a vector a, we use diag(a) to represent the diagonal matrix
whose diagonal entries are a. For two functions f (t), g(t), f(t) = o(g(t)) means f (t)/g(t) → 0 as
t goes to infinity. We use O(∙) to omit logarithm terms in big-O notations. We say a space X has a
bounded diameter D∞ if kx - yk∞ ≤ D∞, ∀x, y ∈ X.
Online Learning Framework. We choose the online learning framework to analyze all the algo-
rithms in this paper. In this framework, an algorithm picks a new xt ∈ X according to its update rule
at each iteration t, where X ⊆ Rd is the set of feasible values of xt . The composite loss function
ft + φ is then revealed, where φ is the regularization function that controls the complexity of x and
ft can be considered as the instantaneous loss at t. In the convex setting, ft and φ are both convex
functions. The regularized regret function is defined with respect to an optimal predictor x* as
T
R(T) = X ft(xt) - ft(x*) + φ(xt) - φ(x*).
t=1
Our goal is to find algorithms that ensures a sub-linear regret, i.e. R(T) = o(T), which means that
the average regret converges to zero. For example, online gradient descent is proved to have a regret
of O(√dT) (Zinkevich, 2003), where d is the dimension size of X. Note that stochastic optimization
and online learning are basically interchangeable (Cesa-Bianchi et al., 2004). Therefore, we will refer
to online algorithms and their stochastic counterparts using the same names. For example, we will
use stochastic gradient descent (SGD) to represent online gradient descent as it is more well-known.
Composite Mirror Descent Setup. In this paper, we will revisit the general composite mirror
descent method (Duchi et al., 2010b) used in the creation of the first adaptive algorithm, AdaGrad, to
bring new insights into adaptive methods. Such a general framework is preferred because it covers a
wide range of algorithms, including both SGD and all the adaptive methods, and thus simplifies the
discussions. The composite mirror descent rule at the time step t + 1 is to solve for
xt+1 = argminx∈X{αthgt, xi + αtφ(x) + Bψt (x, xt)},	(1)
where gt is the gradient, φ(x) is the regularization function in the dual space, and αt is the step
size. Also, ψt is a strongly convex and differentiable function, named as the proximal function and
Bψt (x, xt) is the Bregman divergence associated with ψt defined as
Bψt(x,y) = ψt(x) - ψt(y) - Nψt(y),x - yi.
The general update rule (1) is mostly determined by the function ψt. We first observe that it becomes
the projected SGD algorithm when ψt(x) = xTx and φ(x) = 0.
xt+1 = argminx∈X {αt hgt, xi + kx - xtk22} = ΠX (xt - αtgt),	(SGD)
where ΠX (x) = argminy∈X kx - yk2 is the projection operation that ensures the updated parameter
is in the original space. On the other hand, adaptive algorithms choose different proximal functions
ψt(x) = hx, Htxi, where Ht can be any full or diagonal symmetric positive definite matrix.
xt+1 = argminx∈X{αthgt, xi + αtφ(x) + hx - xt, Ht(x - xt)i},	(Adaptive)
Another popular representation of adaptive algorithms is the generalized projection rule xt+1 =
ΠX,Ht (xt - αtHt-1gt), where ΠX,Ht (x) = argminy∈X kHt1/2(x - y)k2, which is used in alotof
2
Under review as a conference paper at ICLR 2021
recent literature such as Reddi et al. (2018); Huang et al. (2019). We show that these two rules are
actually equivalent when φ(x) = 0 in the Appendix A.1, so that the regret bounds found in different
representations can be generalized. A few recent works have shown that adaptive algorithms work
well with special designs of the step sizes at (Choi et al., 2019; VasWani et al., 2020). In this work, We
choose the more standard at = a/ʌ/t as it is used in most analysis. Also, we restrain our discussions
to diagonal matrix proximal functions in the main text, i.e. Ht = diag(ht), ht ∈ Rd. Discussions on
extending our results to full matrix proximal functions are provided in Appendix B.
Different Designs of the Proximal Function. Recently, researchers have proposed numerous
designs of Ht = diag(ht), such as AdaGrad (Duchi et al., 2011), Adam (Kingma & Ba, 2015),
AMSGrad (Reddi et al., 2018) and NosAdam (Huang et al., 2019), to name a few. It’s impossible to
go over all the proposed designs in this section so we choose the two most famous designs to review.
The first adaptive algorithm, AdaGrad, used the square root of the average of past gradient squares as
the diagonal ht of the matrix in the proximal function (Duchi et al., 2011), i.e.
ht = (M=ιg )1/2	(AdaGrad)
Normally, a small constant is added to ht at each iteration. Some recent work have shown that
tuning this constant can benefit the performance of adaptive algorithms (Zaheer et al., 2018; Savarese
et al., 2019). However, in this work, we assume it is small and fixed for simplicity, as it is originally
designed to compute the pseudo-inverse, or equivalently, avoid division by zero in the generalized
projected descent. Kingma & Ba (2015) proposed Adam to replace the simple average by exponential
moving average in AdaGrad, but Reddi et al. (2018) showed that there was a mistake in Adam’s
convergence analysis, which lead to divergence of Adam even in simple convex problems. They
therefore proposed the following simple modification of Adam to ensure its convergence.
ht = maxt{ Pt=1(1 -β2)β2-ig2 )1/2}	(AMSGrad)
where β2 ∈ (0, 1) is a constant. We propose the following theorem that generalizes the regret bounds
for most of the designs of the diagonal matrix proximal function.
Theorem 2.1 Let the Sequence {xt} be defined by the update rule (1) and for any x*, denote
D2,∞ = Ilxt 一 x*k∞ ∙ When ψt(x) =〈x, Htxi, where Ht = diag (ht,1, ht,2,…，ht,d) ,assume
without loss of generality that φ(x1) = 0, H0 = 0, if (ht,i/at) ≥ (ht-1,i/at-1), then
Td	Td 2
R(T) ≤ X X(*-片 )D2∞+X X ≡.
(2)
The proof is relegated to Appendix A.3. The above regret bound is suitable for any designs of ht that
satisfy the constraint condition (ht,i/at) ≥ (ht-1,i/at-1). Such a condition is crucial because if
it is unsatisfied, the regret R(T) might diverge. In fact, the divergence of Adam in simple convex
problems results from not satisfying this constraint, which is proved by Reddi et al. (2018). With
at = a/ʌ/t in Theorem 2.1, most of recent adaptive algorithms have a regret bound of the following
form (Duchi et al., 2011; Reddi et al., 2018; Huang et al., 2019; Luo et al., 2019).
d	d
R(T) ≤ C1√TX hτ,i + f (T) X kg1：T,ik2 + C2	(3)
i=1	i=1
where C1, C2 are constants and f (T) = o(√T). 1 These algorithms are supposed to converge faster
than SGD when the gradients are sparse or small, i.e. when Pd=1 hτ,i《 √d and Pd=1 ||g1：T,i||2《
√dT. However, all existing regret bounds are still O(√T), which makes it hard to compare different
proximal functions. Whether the best proximal function exists and whether the O(√T) regret bound
can be further improved still remain open questions.
1The regret bound of AdaGrad given in the original paper was O(kg1:T,i k2) because Duchi et al. (2011)
used a constant learning rate αt = α and hence ht = Pit=1 gi2 . When changed into the same setting where
at = a/ʌ/t, ht becomes P；=i g2/t and the regret also has the form of (3), shown in Reddi et al. (2018).
3
Under review as a conference paper at ICLR 2021
3 The Motivation-Marginal Regret B ound Minimization
In this section, we introduce the motivation behind our new class of algorithms. Although we find it
difficult to determine the optimal proximal function globally, we show that it is possible to find the
best proximal function at each iteration through marginal regret bound minimization. Denote R(T)
to be the regret upper bound (the right hand side of inequality (2)) in Theorem 2.1. At time step T,
we define the marginal regret bound increment ∆R(T ) as follows.
△凤T) := R(T) - R(T -1) = XX(hT,i - hT-1i)DT,∞ + XX OTgTi.
αT	αT-1	,	2hT,i
i=1	i=1	,
As shown in the definition, △R(T) will be the increment in the regret bound R(T) after hT is
determined. An important observation here is that hT-1 is a given constant at T, so △R(T) is only a
function of hT . Therefore, the best design of hT we can find at this moment is the one that minimizes
△R(T) and satisfy the constraint in Theorem 2.1. Consider the minimization problem
minhτ∆R(T), s.t.hT,i ≥ hT-1i ≥ 0,	(4)
αT	αT-1
We propose the following proposition that solves the problem above
Proposition 3.1 With αt = ɑ/ʌ/t,the minimum ofproblem (4) is obtained at
hT
max
rT--h hT-ι, (2T⅛∞)1/2IgT |}.
(5)
To see this, set the function L with the Lagrangian multiplier μ as follows
L(h, α
—
d2
J)dt ,∞+X ατgτi-hhτ
αT-1	,	i=1 2hT,i	αT
hτ -1
αT-1
,μi.
Take partial derivatives with respect to each hτ,i, We can see that DT ∞ = (αT gTi)∕2hTi + μi.
By the complementary slackness conditions, either μi = 0 or hγ,i = (ot∕ot-ι)hr-ι,i. When
μi = 0, hτ,i = (αT/2DT,∞)"2∣gτI and the constraint condition hτ,i ≥ (ατ∕ατ-ι)hτ-ι,i needs
to be satisfied. Hence, by setting aτ = α∕√T, we can get the solution in (5).
Solution (5) is the best diagonal matrix proximal function in terms of regret bound increment at time
T. Therefore, if we replace T by t in the subscripts, we can obtain a greedy choice of the proximal
function ht that minimizes the marginal regret bound increment at each time step. Intuitively, the
reason this solution achieves the minimum is that it balances the two terms of △R(T). On each
dimension (i), it makes the first term of △R(T) zero when the derivative is small 2, i.e. when we
don’t need to have a even larger hτ,i to slow down. When the derivative is too large, hτ,i adapts to
the size of the derivative so that the second term of △R(T) is constrained.
However, similar to the other greedy algorithms, solution (5) is only suboptimal as it minimizes the
regret bound marginally instead of globally. Moreover, the parameter Dt,∞ is often unknown during
the optimization process because x* is usually unknown. Therefore, stronger theoretical motivation
is needed to trust that solution (5) or similar algorithms can be useful and beneficial.
4 A new class of adaptive algorithms - AMX
Now, motivated by the greedy choice of ht in section 3, we focus on a more general design of the
diagonal matrix in the proximal functions that has the following form and show why such greedy
designs can be beneficial in the long term. Consider
)
ht
max
ht-1,ctIgtI
(AMX)
2Because (ht,i∕αt) - (ht-ι,i∕ɑt-ι) = 0 when ht,i = (αt∕ɑt-1)ht-1,i
4
Under review as a conference paper at ICLR 2021
Algorithm 1 AMX Algorithm (Diagonal, Composite Mirror Descent Form)
1:	Input: X ∈ F, at = α/yfi,{ct}T=ι, Φ(x),e
2:	Initialize h0 = 0, H0 = 0
3:	for t = 1 to T do
4：	gt = Vft(xt)
5：	ht = maχ( Jt-I ht-i, ct|gt|)+ E
6:	xt+1 = argminx∈X{αthgt, xi + αtφ(x) + hx - xt, diag(ht)(x - xt)i}
7:	end for
where ct is an arbitary function of t, for example solution (5) or as simple as ct = 1. The correspond-
ing new class of adaptive algorithms is given in Algorithm 1, which we name as AMX. Note that the
diagonal proximal function performs all operations coordinate-wisely, therefore we start our analysis
from one dimension (i). Denote (i)-th dimension component of the regret bound as
T	T2
R(i)(T ):= X 与-T )D2∞+X 就.
(6)
For a sequence of gradients g1,g2,∙∙∙ ,gτ, denote the time steps t when ht,i = Ct∣gt,i∣ as
(i) (i)	(i)	(i)
ri，，τ2 , ∙…τmi. Note that τj ’ S may be different across different dimensions, so the analysis
apply to each dimension independently. These are the time steps when the gradient term ct|gt|
dominates h on the (i)-th dimension and ht,i = ct∣gt,i| when t = τ(i),∀j = 1,…，mi. Since ht,i
is equal to q∕t-1 ht-ι,i between Tf)’s, which is exactly the same as in section 3, the increment in
the first term of the right hand side of (6) is always 0. Therefore, we have the following proposition
Proposition 4.1 For any τ ∈ (τj* * * * * * * * (i), τj(+i)1), the regret bound increment on the (i)-th dimension is
τ2
R⑴(T) - R叫Tr)= X /.
t=τj(i)+1	t,i
The above proposition indicates that the regret increments between the τ(i),s are only related to the
second term of R(i) (T). Note that the first term of R(i) (T) is a major reason why the regret bound
is O(√T) because there is a 1∕ατ term in the summation. Therefore, such designs of ht,i try to
constrain the regret increments between τ(i),s and hence can potentially make the regret bound small.
More importantly, Proposition 4.1 is true for the time steps between the last τm(ii) and T + 1. Denote
D∞ to be the bounded diameter of the parameter space X, because the regret bound increment is
only related to the second term after the last τm(ii), the bound in equation (6) becomes
τ	τ 2	2	(i)	τ
R7τ) ≤	x( R	- d )d∞	+ χ	*	≤ D∞≠mi hτ (i) i + X 齐 ∣gt,i∣	⑺
t=1 αt αt-1	t=1	2ht,i	α τmi,i t=1 2 tct
Since R(T) = pid=1 R⑺(T), so the total regret upper bound across all the dimensions is
d	d D2 . ΓA)) d T
R(T) ≤ XR⑴(T) ≤ X ~D∞0τmhτm),i + XX 2⅛∣gt,i∣	⑻
i=1	i=1 α	i=1 t=12 tct
The first term of the right hand side can be considered as better than the O(√T) regret bound of
(i)
SGD or common adaptive algorithms when some or all of the τmi are much smaller than T and ht,i ’s
are bounded. Therefore if we can ensure the second term also increases much slower than O(√T),
which is decided by the design of ct, then the AMX class of algorithms is potentially much faster
than SGD and the other adaptive algorithms. Note that we need ht,i to be bounded in the above
arguments, therefore ct can be at most a constant. Fortunately, one simple yet very effective design
that we have found is ct = 1. We formalize the above statements in the following theorem.
5
Under review as a conference paper at ICLR 2021
Theorem 4.1 Let {xt} and {ht} be the sequences obtainedfrom Algorithm 1, αt = ɑ/ʌ/t, Ct = 1.
Let {τm(ii)}id=1 be the largest time steps t on each dimension when ht,i = ct|gt,i|. Assume that F has
bounded diameter kx - yk∞ ≤ D∞, ∀x, y ∈ F. Then we have the following bound on the regret.
d Γ)2 ∕zj-(i)	d	d
R(T) ≤ X ——— hτ (i) i+^X，1+log Tmi iig1：T (i) ik2 + ^X VZTm^ log( 三 )|gT (i) i1,
α α α	Imi ,i	2	1: τ mi ,i	2	(i) Imi ,i
i=1	i=1	i=1	Tmi
(9)
An important remark here is that using a different constant ct = c in Theorem 4.1 is equivalent to
tuning the step size by 1/c, as it magnifies all |gt| at the same time in the algorithm. Using a decaying
Ct can further improve the first term in Theorem 4.1, but it also enlarges the second and the third term
so the regret may not be O(√T) anymore. We will focus on Ct = 1 to prove AMX can potentially
converge faster in the rest of this paper, but a detailed discussion about the possible choices of Ct for
future designs of the AMX algorithm is provided in Appendix A.5, which proves that Ct cannot be
O(1/√7) if we do not impose any further assumptions.
Now, since most of the bound in Theorem 4.1 depends on the time steps {Tm(ii)}id=1 instead of T
(only a log term), the specific AMX algorithm can be potentially much faster than common adaptive
algorithms. To further make our argument clearer, we propose the following corollary.
Corollary 4.1 Let T = maxi{Tm(ii)} in Theorem 4.1, under the same assumptions as AdaGrad and
AMSGrad, Algorithm 1 converges with regret bound
T	.…
O(max(√τ, √τlog —))	(10)
T
r-ɪ-ɪ-l	1 1	∙ 1 ∙ .	,1 ,,1	, 1	1 ∙	∙	. 1 l' ∙	√^1 / I ∖ ∙ Γ∙	∙ , ,1 1
The corollary indicates that the regret bound is approximately of Size O(√τ) if we omit the log
terms. As far as we are aware, this is the first algorithm that generates a regret bound that is not
asymptotically O(√T), so AMX can be potentially much faster than any existing algorithms. The
time step T can be understood as “the time when the gradients start to converge” and whether it makes
the convergence faster depends on the distribution of the gradients. For example, if T = √T, the
regret bound is only of size O(T 1/4 logT). We emphasize that a small T is not an assumption on the
gradient distribution, but rather a condition that once satisfied, the regret bound will only increase
logarithmically and hence the algorithm converges very fast. Moreover, the regret bounds of the other
adaptive algorithms go to O(√T) even under such conditions, because their regret bound increments
are not minimized. We use a rather simple example to illustrate why AMX has this unique advantage.
Example. Suppose that the domain is a hyper-cube X = {ixi∞ ≤
1}, then D∞ = 2. Assume that on each dimension, the gradient
decreases as |gt,i| = (1∕√7)∣gι,/, and ∣gι,/《 1,∀i. Note that
this is one example where adaptive algorithms should work well
since ∣∣grτ,i∣∣2 ≤ ∣gι,i | √(1 + log T) ≪ TT. A very important
property for AMX in this case is that T is the first time step, so its
regret bound only increases logarithmically. However, the regret
bounds of the other algorithms still goes to O(√T). We plot the
regret bounds of AMX, AMSGrad and AdaGrad in Figure 4. Note
that the regret bound of AMX increases much slower than AdaGrad
and AMSGrad, hence it is much faster than these algorithms in this
example. One may argue that the example is extreme since T = 1
rarely happens in real situations. However, the regret in this example
can be understood as the regret increment after T in real training
processes, i.e. before T, AMX is only asymptotically as fast as the
Figure 1: The regret bounds of
AMX, AdaGrad, AMSGrad in
the example.
other adaptive algorithms, but after T, since the regret increment of AMX is very small, it converges
very fast. More details of this example can be found in Appendix D.1.
Besides, since the term √τ log(T∕τ) in Corollary 4.1 is at most O(√T)3, the AMX algorithm is
at least as fast as AdaGrad and AMSGrad under the same assumptions. We propose the following
theorem that corresponds to the general results in section 2 to prove our claim:
3This claim can be easily proved by taking derivative of T and finding the maximum of √τ log(T∕τ)
6
Under review as a conference paper at ICLR 2021
Theorem 4.2 Let {xt} and {ht} be the sequences obtainedfrom Algorithm 1, αt = α∕√t, Ct = 1.
Assume that F has bounded diameter kx - yk∞ ≤ D∞ , ∀x, y ∈ F. Then we have the following
bound on the regret.
R(T) ≤ d∞√t XX hτ,i + 2pi + logTXX kgi：T,ik2,	(11)
α i=1	2	i=1
The above bound can be considered as being better than the regret of SGD, i.e., O(√dT), When
Pd=ι hτ,i《√d and Pd=1 ∣∣gLτ,i∣∣2《√dT (Duchi et al., 2011). Therefore, AMX can be at least
much faster than SGD When the gradients are sparse or small, and it can be potentially even faster. To
keep up With the current popular adaptive algorithms such as Adam, We also provide the detailed
implementation of adding first order momentum into AMX and include some discussions on its
convergence properties in Appendix C. Similar to Algorithm 1, the AMX With momentum algorithm
has a regret bound that (mostly) depends on τ instead of T and hence enjoys the acceleration.
5 Experiments
In this section, We evaluate the effectiveness of the specific AMX algorithm proposed in Section 4
(i.e. ct = 1) on different deep learning tasks. We relegate more details of parameter tuning and step
size decay strategies to Appendix D.2-D.5. Moreover, an empirical analysis for different designs of
{ct}tT=1that shoW different {ct}tT=1’s generate different performances is provided in Appendix D.6.
TraInTop-I Accuracy
1∞∞ 200∞ 300∞ 400∞ 500∞ 60000
iterations
90e5807570«5«055
Λ3snMV Woj. L
AdaGrad
AdamW
AMSGrad
SGDM
AdaGrad
—AdamW
—AMSGrad
SGDM
—AMX
10∞0 200∞ 300∞ 400∞ 500∞ WoOo
Iterations
50 IOO 150
Epochs
I -------- 1
Λ3enMV τi0l-181-
50 IOO
Epochs
150
(a) CIFAR-10 Training Acc.(b) CIFAR-10 Testing Acc.(c) CIFAR-100TrainingAcc(.d) CIFAR-100 Testing Acc.
Figure 2: Training and Testing Top-1 accuracy on CIFAR-10 and CIFAR-100.
Table 1:	Testing Top-1 accuracy on the CIFAR-10,
CIFAR-100 datasets and testing IoU on the VOC2012
Segmentation dataset. The results were averaged over 5
independent runs. Our results were shown in bold.
Optimizer	CIFAR-10 CoAR-100	VOC2012
Table 2:	Validation perplexity on the charac-
ter Penn Tree Bank (PTB) dataset and BLEU
score on the IWSLT’14 DE-EN dataset. The
results were averaged over three independent
runs. Our results were shown in bold.
SGDM
AdaGrad
Adam
AMSGRAD
AMX
92.40 ± 0.06
85.60 ± 0.14
91.85± 0.03
91.97 ± 0.07
92.42± 0.08
77.80 ± 0.08
72.84± 0.06
74.51± 0.05
75.75 ± 0.04
77.65± 0.10
76.10 ± 0.09 OPTIMIZER	CHAR-PTB IWSLT’14
71.28 ± 0.18
73.32± 0.21
73.66 ± 0.13
76.04± 0.16
AdaGrad
Adam
AMSGRAD
AMX
2.63 ± 0.04
2.48 ± 0.08
2.46 ± 0.05
2.32 ± 0.04
25.56 ± 0.05
28.01 ± 0.07
28.15 ± 0.06
28.29 ± 0.03
We compared our AMX algorithm With SGD With momentum (SGDM), Adam, AdaGrad and
AMSGrad on different tasks in deep learning. The hyper-parameters in AMX Were set to be ct = 1 in
this subsection. For the language modeling and the neural machine translation tasks, because SGDM
typically performs much Worse than adaptive algorithms, We did not include it in the comparisons.
FolloWing Loshchilov & Hutter (2019), We used decoupled Weight decay in all the algorithms.
Image Classification. We first conducted some experiments Where τ Was possibly very large and
AMX Was only as fast as the other adaptive algorithms, but it still achieved better testing performances.
The image classification task Was performed on the CIFAR (Krizhevsky et al., 2009) datasets. We
used the publicly available code by Li et al. (2020) and DeVries & Taylor (2017) to train ResNet-20
and ResNet-18 (He et al., 2016) on CIFAR-10 and CIFAR-100 respectively using batch size of 128.
7
Under review as a conference paper at ICLR 2021
We summarized the performances of different algorithms in Figure 2 and Table 1. As observed, AMX
started slightly slowly, but it quickly caught up with the other adaptive algorithms and converged
much faster than SGDM. This was possibly because the time when gradients start to converge (the τ
in section 4) was large in image classification tasks, and AMX could only converge asymptotically
as fast as the other adaptive algorithms, corresponding to Theorem 4.1. However, its final testing
performance was as good as SGDM, so it converged both fast and well at the same time. The other
adaptive algorithms such as Adam and AMSGrad had faster training performances in the beginning,
but they ended up with much worse final accuracy than SGDM and AMX.
Image Segmentation. Next, more experiments proved our claim that AMX could be potentially
much faster and generate even better testing performances. For the segmentation task, we used the
publicly released implementation of the Deeplab model (Chen et al., 2016) by Kazuto1011 (2016)
and evaluated the performances of different algorithms on the PASCAL VOC2012 Segmentation
dataset (Everingham et al., 2014). We used a small batch size of 4 and a polynomially decaying step
size in 20k iterations. The trained models were evaluated at the 5k, 10k, 15k and 20k iterations and
we used mean Intersection over Union (IoU) as the evaluation metric. The results were provided in
Figure 3(a), 3(b) and Table 1. As shown in the figures and the table, AMX was not only the fastest
adaptive algorithm but also achieved the best IoU score, which was comparable to that of SGDM.
The other algorithms were not able to perform similarly.
♦ ♦ — J
S3 πc-c-SF
0.75
0.74
0.73
0.72
0.71
0.70
0.69
AdaGrad
AdamW
AMSGrad
SGDM
AMX
AdaGrad
—AdamW
一 AMSGrad
—AMX
uqsie>
w1∙1∙1∙lill>
Xllxeleeu uoln-eΛ3
AdaGrad
—AdamW
一 AMSGrad
—AMX
(a) VOC Training Loss Curve (b) VOC Testing IoU (c) PTB Test Perplexity (d) IWSLT’14 Test Perplexity
Figure 3: (a), (b). Training Loss and Testing IoU curves on the VOC2012 Segmentation dataset. (c).
Validation perplexity curve on the Penn Tree Bank (PTB) dataset. (d). Validation Perplexity curve on
the IWSLT’14 DE-EN machine translation dataset.
Language Modeling. We trained three-layer LSTMs (Hochreiter & Schmidhuber, 1997) on the
character level Penn Tree Bank (PTB) dataset. The general setup in Merity et al. (2017) was adopted
in our experiments. Specifically, we trained the model for 500 epochs with batch size 128. The
validation perplexity curve and the final validation perplexity were shown in Figure 3(c) and Table 2.
It can be observed that AMX was the fastest algorithm and achieved the lowest perplexity among all
the adaptive algorithms, which proved our claim that AMX was potentially much faster.
Neural Machine Translation. We utilized the publicly released code by pcyin (2018) and trained
the basic attentional neural machine translation models (Luong et al., 2015) on the IWSLT’14 DE-EN
(Ranzato et al., 2015) dataset. We used 64 as the batch size and decreased the step size by 2 every 5
iterations. The validation perplexity curve and the final BLEU score were reported in Figure 3(d) and
Table 2. AMX not only had a much smoother validation perplexity curve, but also achieved the best
BLEU score among all the adaptive algorithms, showing that AMX was indeed a better choice.
6 Conclusion
In this paper, we propose our design of the best proximal functions at each time step based on marginal
regret bound minimization. We then show that a more general class of adaptive algorithms can not
only achieve marginal optimality in some sense, but also converge much faster than any existing
adaptive algorithms, depending on the distribution of the gradients. We evaluate one particular case of
our new class of algorithms on different tasks in deep learning and show its effectiveness. This work
provides a new framework for adaptive algorithms and can hopefully prevent the random searching
process for better designs of the proximal function. Future researchers can concentrate on finding
better choices of the sequence {ct}tT=1 to find better algorithms.
8
Under review as a conference paper at ICLR 2021
References
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, USA,
2004. ISBN 0521833787.
Nicolo Cesa-Bianchi, Alex Conconi, and ClaUdio Gentile. On the generalization ability of on-line
learning algorithms. IEEE Transacitons on Information Theory, 2004.
JinghUi Chen and QUanqUan GU. Closing the generalization gap of adaptive gradient methods in
training deep neUral networks. arXiv preprint arXiv:1806.06763, 2018.
Liang-Chieh Chen, George PapandreoU, Iasonas Kokkinos, Kevin MUrphy, and Alan L. YUille.
Deeplab: Semantic image segmentation with deep convolUtional nets, atroUs convolUtion, and fUlly
connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:834-848,
2016.
Dami Choi, Christopher J. ShallUe, Zachary Nado, Jaehoon Lee, Chris J. Maddison, and George E.
Dahl. On empirical comparisons of optimizers for deep learning, 2019.
Terrance DeVries and Graham W. Taylor. Improved regUlarization of convolUtional neUral networks
with cUtoUt, 2017.
John DUchi, Shai Shalev-Shwartz, Yoram Singer, and AmbUj Tewari. Composite objective mirror
descent. In Proceedings of the Twenty Third Annual Conference on Computational Learning
Theory, 2010b.
John DUchi, Elad Hazan, and Yoram Singer. Adaptive sUbgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research (JMLR), pp. 12:2121-2159, 2011.
Mark Everingham, S. M. Ali Eslami, LUc Van Gool, Christopher K. I. Williams, John Winn, and
Andrew Zisserman. The pascal visUal object classes challenge: A retrospective. International
Journal of Computer Vision(IJCV), 2014.
Kaiming He, XiangyU Zhang, Shaoqing Ren, and Jian SUn. Deep residUal learning for image
recognition. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.
SePP Hochreiter and JUrgen SChmidhuber. Long short-term memory. Neural computation, pp.
9(8):1735-1780, 1997.
Haiwen Huang, Chang Wang, and Bin Dong. Nostalgic adam: Weighting more of the past gradients
when designing the adaptive learning rate. arXiv preprint arXiv: 1805.07557, 2019.
Kazuto1011. Deeplab with pytorch. https://github.com/kazuto1011/
deeplab-pytorch, 2016.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. Proceedings of
the 3rd International Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced
research). 2009.
Wenjie Li, Zhaoyang Zhang, Xinjiang Wang, and Ping Luo. Adax: Adaptive gradient descent with
exponential long term memory. arXiv preprint arXiv:2004.09740, 2020.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO:
common objects in context. CoRR, abs/1405.0312, 2014.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. Proceedings of 7th
International Conference on Learning Representations (ICLR), 2019.
9
Under review as a conference paper at ICLR 2021
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. Proceedings of 7th International Conference on Learning Representations,
2019.
Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-
based neural machine translation, 2015.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm
language models. arXiv preprint arXiv:1708.02182, 2017.
pcyin. Basic pytorch implementation of attentional neural machine translation. https://github.
com/pcyin/pytorch_basic_nmt, 2018.
Boris Polyak. Some methods of speeding up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, pp. 4(5):1-17, 1964.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training
with recurrent neural networks, 2015.
Sashank J. Reddi, Stayen Kale, and Sanjiv Kumar. On the convergence of adam and beyond.
Proceedings of the 6th International Conference on Learning Representations (ICLR), 2018.
Pedro Savarese, David McAllester, Sudarshan Babu, and Michael Maire. Domain-independent
dominance of adaptive methods, 2019.
Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural networks for machine learning, pp. 4(2):26-31, 2012.
Sharan Vaswani, Frederik Kunstner, Issam Laradji, Si Yi Meng, Mark Schmidt, and Simon Lacoste-
Julien. Adaptive gradient methods converge faster with over-parameterization (and you can do a
line-search), 2020.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. Advances in Neural Information Processing Systems 31, 2018.
Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift:
Decorrelation and convergence of adaptive learning rate methods. Proceedings of 7th International
Conference on Learning Representations (ICLR), 2019.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. Inter-
national Conference on Machine Learning (ICML), 2003.
10
Under review as a conference paper at ICLR 2021
A Proofs of Results in the Main Text
In this appendix, we provide proofs of all the results and theorems in the main text, except for
Theorem 4.2, which is proved in section C as it is the same as β1 = 0 in the momentum design. An
important remark here is that adding to ht does not affect the proof here because when ht,i is placed
on the denominator, 1/ht,i ≥ 1/(ht,i + ). When ht,i is placed on the numerator, we generally use
ht,i instead of giving it a specific form. Also, we set to be very small in real experiments (1e-8).
Further Notations. Except for the notations mentioned in the main text, the following notations are
needed for this appendix. Given a norm k ∙ k, its dual norm is defined to be ky∣∣* = SUPkxk≤ι{(χ, y)}.
For example, the dual norm of the Mahalanobis norm k ∙ k a = 'h, A∖, A * 0 is the Mahalabonis
norm k ∙ k a-i . A function f is said to be 1-strongly convex with respect to the norm k ∙ k if
f(y) ≥ f(x) + hVf(x),y — x) + 2∣∣x — yk
A. 1 Equivalence between Composite Mirror Descent and Generalized
Projected Algorithms
Begining with the generalized projected adaptive algorithm, we can find that
xt+1 = ΠXHt (xt — αtHt-1gt) := argminx∈X kx — (xt — αt Ht-1 gt)k2Ht
argminx∈X kH1/2(x - (xt - αtH-Igt)) k2
argminx∈x kH1∕2(x — xt) + αtH-12gtk2
argminx∈X {2αt hgt, x — xt) + hx — xt, Ht(x — xt))}
argminx∈X{2αthgt,x) + hx — xt, Ht(x — xt))}
(12)
Given ψt(x) = hx, Htx), it can be shown directly (also in A.3) that Bψt (x, xt) = hx—xt, Ht(x—xt)).
Therefore comparing equation 12 with equation 1, we know that they are equivalent when the
regularization term φ(x) = 0, except that we need to double the step sizes αt.
A.2 Proof of Lemma A.1
Lemma A.1 Let {xt} be a sequence of outputs from the update rule (1) and assume that {ψt} are
strongly convex functions with respect to the norm k ∙ ∣∣ψt. Let k ∙ ∣∣ψ* be the corresponding dual
norm. Also without loss of generality let Bψ0 (x*, xQ) = 0. Thenfor any x*, we have
Xft(xt) — ft(x*) + φ(xt) — φ(x*) ≤ X Bψt(x*,Xt)
Bψt-1 (x*, Xt)
αt-i
(13)
Proof. Since xt+1 satisfies equation (1), for all x ∈ X and φ0(xt+1) ∈ ∂φ(xt+1).
hx — xt+1, αtgt + αtφ0(xt+1) + Vψt(xt+1) — Vψt(xt)) ≥ 0	(14)
Also since ft’s are convex functions, we know that ft(x) ≥ ft(xt) + hf0(xt), x — xt) and likewise
for φ(xt+1). We therefore have the following
αt(ft(xt) — ft(x*)) + αt(φ(xt+1) — φ(x*))
≤ αt hft0(xt), xt — x*) + αthφ0t(xt+1), xt+1 — x*)
αthft0(xt),xt+1 — x*) + αthφ0t(xt+1), xt+1 — x*) + αthft0(xt),xt — xt+1)
hx* — xt+1, Vψt(xt+1) — Vψt(xt) — αtft0(xt) — αtφ0t(xt+1))
(15)
+ hx* — xt+1, Vψt(xt) — Vψt(xt+1)) + αthft0(xt), xt — xt+1)
≤ hx* — xt+1, Vψt(xt) — Vψt(xt+1)) + αthft0(xt),xt — xt+1)
where the first inequality is due to the convexity of φt and ft . The second inequality is due to the
positiveness in equation 14. Since by definition
11
Under review as a conference paper at ICLR 2021
Bψt(x*,xt) = ψt(x") - ψt(xt) - hVψt(xt),x* - Xti
Bψt(χ*,χt+ι) = ψt(χ*) - Ψt(χt+ι) - hVψt(χt+ι),χ* - xt+ii	(16)
Bψt(xt+1,xt) = ψt(xt+1) - ψt(xt) - hVψt(xt),xt+1 -xti
Therefore
at(ft(Xt) - ft(x*)) + αt(φ(xt+ι) - φ(x*))
≤ Bψt(x*,xt) - Bψt(x*,xt+ι) - Bψt (xt+ι,xt) + αthf0(xt),xt - Xt+ii
≤ Bψt(χ*,Xt)- Bψt(χ*,χt+1)- Bψt(χt+ι,Xt) + 2IIXt- xt+ιkψt + ~2llfi°(χt)kψ*	(17)
2
≤ Bψt (X*,Xt)- Bψt (X*,Xt+1) + j kft (Xt)I 岛
where the second inequality is due to the Fenche1's inequality on the conjugate functions ɪ ∣∣T∣ψ= and
1 k ∙ kψ*. The last inequality is due to the strong convexity of Bψt (∙, ∙). Therefore the lemma can be
proved by dividing αT on both sides and take the summation.
A.3 Proof of Theorem 2.1
Proof. When ψt(X) =(x, HtXi, the dual norm ∣∣ ∙ ∣∣ψ* is the Mahalanobis norm ∣∣ ∙ ∣∣h-i
Bψt (X, y) = ψt(X) -ψt(y) - hVψt(y), X -yi
= hX, HtXi - hy, Htyi - 2hHty, X - yi	(18)
= h(X - y),Ht(X - y)i
Therefore by Lemma A.1,
T
X ft(xt) - ft(x*) + φ(xt) - φ(x*)
t=1
≤ X Bψt(X*,xt) - Bψt (x*,xt+l) + α kf 0(χ古)∣2*
t=1 αt	αt	2 t	ψt
P kHt1/2(Xt-X*)k2 - kHt1∕2(xt+ι-x*)k2 + akf0(x)k2*
L	at	at	2 t t ψt
≤
kH72(xι-x*)k2 + X kHt1/2(Xt-X*)k2
ɑ	⅛	at
kH1-1(xt-x*)k2
at-1
T
+ X mkft'(xt)kψ*
t=1
—
d	Td
≤ X ⅛ (XIi)2+X X(*-空)(Xti
T
t=1
*
t
Td	Td
X X(⅛- W)2+X X
2ht,i
Td
≤ XX( + -
t=1 i=1	t
ht-1,i
at-1
)D2,∞ + X X 上「
t=1 i=1	t,i
(19)
2
where the second inequality is by re-arranging the sum and deleting the last negative term.
12
Under review as a conference paper at ICLR 2021
A.4 Proof of Theorem 4.1
Proof. Based on Theorem 2.1 and Proposition 4.1, take ht to be the design in Algorithm 1 with
ct = 1. Then after the final τm(i)i (which means the last gradient that is ”large”) for each dimension i,
the first term doesn’t increase and the second term can be bounded as follows.
Td
R(T) ≤ XX(H -
t=1 i=1	t
ht-1,i
at-i
Td
)Dt2,∞+XX
αtg2,i
2ht,i
d ≤X i=1	h (i)	T d U⅛i d∞+XX α>,i∣ αTm(ii)	t=1 i=1 2
d =X i=1	(i) h (i)	Tmi d	d	T Ud∞+XXα2t∣gt,i∣+X X	α2t∣gt,i∣ Tmi	t=1 i=1	i=1 t=Tm(ii) +1
d
≤X
i=1
D∞ + XX∣gt,iI + X X 2r^iWm),i∣
dT
τm(i)i	d
h	(i) i	τm(ii)
τmi,
α
t=1 i=1
i=1 t=τm(i)i+1
d h (i) Tm Tm)	Tmi d	d	/--- T
≤ X	D∞ + XX ItIgt,i∣ + X 2 q⅛gτm) ,i∣iog( W)
i=1	t=1 i=1	i=1	τmi
≤ X '∞ OTmi hτm) ,i+X 2 q1+log τm kgι:Tmi ,ik2+X 2 qm∖9τm)』log( W)
i=1	i=1	i=1	τmi
(20)
where the first inequality is by Theorem 2.1 and the second one is by Proposition 4.1. The third
inequality is by the fact that ∣gti∣ ≤ ʌ/Tmih ⑸.=ʌ/Tmi ∣g ⑸.∣ for all t ≥ Tmi + 1. The fourth
,	t	Tmi,i	t	Tmi,i
inequality is by P二 ⑸ 11/t ≤ RT (i 1/t = log(%).The last inequality is by the fact that
t=Tmi +1	t=Tmi	Tmi
Tmi d	d
XX O^|gt,i| ≤ Xkgi：Tm),i
t=1 i=1	i=1
(21)
d ,------------------
≤ X 2V 1+log Tm)kgl：Tmii ,ik2
i=1
where the first inequality is by the Cauchy-Schwarz Inequality.
A.5 DISCUSSIONS ABOUT THE POSSIBLE DESIGNS OF ct
We first show our claim that given a design of ct, scaling it by a constant a is the same as scaling
the step size αt by 1/a. Note that the maximum operation can be commuted with constant scaling,
therefore by unrolling the maximum operation
13
Under review as a conference paper at ICLR 2021
a ∙ max
ht = max
ht-1,act|gt|
t-1 ht-1 ,ct |gt|)
t-ɪ	r rt-ɪZI	, III
a ∙ max	—mmax < √ t—ɪht-2,act-1∣gt-1∣ 卜 ct|gt| ；
t -2	r t — 1	1	1	∣∣[
a ∙ max	~~t~ht-2, V -ɪet-ilgt-il,etlgtl >
(22)
)
a ∙ max
ct-j |gt-j |
t-1
j=0
Therefore we know that it is equivalent to scaling αt by 1/a since it magnifies all ct|gt| at the same
time. Now given the discussions in section 4, We know that Ct can be at most a constant, which is
because it is strictly Ω(1), then the regret is strictly Ω(√T). Now we can possibly use a decaying
ct. However, if Ct is too small, then the regret may also be strictly Ω(√T) because the second term
in Theorem 2.1 can be very large. We show that Ct = Ω(1∕√t) by giving a counter example of
the sequence of gt such that the regret becomes very large. Suppose |gt| = (1/t)1/4|g1 |, |g1,i| >
0, ∀i, Ct = 1/√7, then since the size of the gradients keep decreasing, hence
ht
max
t-1
j=0
max
Ct-j |gt-j |
|gt-j |
√tmax {|gt-j l}j=0
√t |g1|
t-1
j=0
(23)
The specific distribution of gradients meets the requirement for adaptive algorithms to work well
because
kgLT,i∣∣2 = ∣g1,i∣ JPT=I √t
= O(T 1/4). Now, despite the first term in the regret bound in
Theorem 2.1 becomes a constant, the second term becomes very large because
Td	Td 2
R(T)≤ XX(等 - 廿)D2,∞ + XX/
t=1 i=1 t	t-1	t=1 i=1	t,i
d	Td	2
=X "£ d2 + XX	αg1,i
=⅛ m ∞ + t=ι ⅛
d	dT
=X 弋 D∞+XX %
i=1	i=1 t=1
(24)
Now notice that
)
)
14
Under review as a conference paper at ICLR 2021
T I
Tt ≤ E√ ≤ 2Vt — ι
t=1 t
(25)
Therefore the regret upper bound there is already Θ(√T). Changing the gradients to be further larger,
forexample, |gt| = (1/t)1/8|gi|, will still satisfy the ∣∣g±T,i∣∣2《 √T condition, but also make the
regret bound even larger (Θ(T3/4)). Therefore using Ct = O(1/√t) is unacceptable in terms of the
regret bound. Note that we can of course change the initial time step, the argument still holds if
the first few c/s are not smaller than Θ(1/√t) in order. Next, we show the regret bound is indeed
achievable by the regret. We propose the following theorem:
TheoremA.1 If Ct = O(1∕√t) in Algorithm 1, then there exists an online convex optimization
problem where ∣∣gLτ,i∣∣2《√T and AMX has a regret of size Ω( √T)
Proof. We recall the terms in the proof of Lemma A.1.
αt(ft(xt) 一 ft(x*)) + αt(φ(xt+1) 一 φ(x*))
≤ αt hft0 (xt), xt 一 x*i + αthφ0t(xt+1), xt+1 一 x*i	(26)
= αthft0(xt),xt+1 一 x*i + αthφ0t(xt+1), xt+1 一 x*i + αthft0(xt),xt 一 xt+1i
If we set φ(x) = 0 in the above inequality and let ft(x) = gtx be a linear loss function, then the first
inequality becomes an equality. Moreover, it's possible to make the term athft(xt), xt+ι 一 x*) > 0
for all t, as long as We change the loss function to be positive when xt+ι > x* and negative when
xt+i < x*, then we know the regret is
T
R(T ) = X ft(χt)- ft(χ*)
t=1
T
hft0(xt),xt+1 一 x*i + hft0(xt),xt 一 xt+1i
t=1
T
≥	hft0(xt), xt 一 xt+1i
t=1
(27)
T
Xhgt, OtHtTgti
Td
Xt=1Xi=1
2
αtgf,i
^h?
Notice what we have claimed before Theorem A.1 are all about this term being larger than O(√T),
therefore we prove our claim.
B Full Matrix Proximal Function
When Ht is not a diagonal matrix but a full matrix, we can similarly prove a general regret bound for
any matrix in Theorem B.1 below.
Theorem B.1 Let the sequence {xt} be defined by the update rule (1) and for any x*, denote
Dt2,2 = kxt 一 x* k22. When ψt(x) = hx, Htxi, where Ht ∈ Sd+ is a general matrix, assume without
loss of generality that φ(xι) = 0, Ho = 0, iftr (H) ≥ tr (H-I), then
R(T)≤
—
Ht-i
αt-1
T
t=i
(28)
15
Under review as a conference paper at ICLR 2021
Proof. When Ht is a full matrix, similar to Theorem 2.1, we can get the regret bound
R(T) ≤ X Bψj - Bψj++α hgt，HTgti	(29)
Let λmax(M) denote the largest eigenvalue of a matrix M, then
Bψt (X* ,xt) - Bψt-1 (X*,xt) = (χ* - Xt (Ht - Ht-I )(χ* -
αt	αt-1	αt	αt-1
≤ kx* - Xtk2λmaχ(Ht - Ht-1) ≤ kx* - Xtk2tr(Ht - Ht-1)
αt αt-1	αt	αt-1
(30)
Hence we prove the regret bound shown in Theorem B.1. Moreover, the marginal regret bound
minimization problem in this case corresponding to Theorem B.1 and the constraint is
minHt D22tr( -t------^) + ~trtr(H-IgtgT), s.t.Ht » 0, tr( —t) ≥ tr( —— )	(31)
t , αt	αt-1	2	αt	αt-1
where Dt,2 = kXt - X* k2. Now, we propose a proposition that shows the optimal full matrix solution
is the one that reaches the infimum of the problem.
Proposition B.1 The following matrix
Ht*
max
1 ^^gT^
2
(2d2 gtgt )1/2,	(Full)
t,2
(32)
and its Moore-Penrose pseudoinverse Ht*- gives an infimum to the problem (31). i.e.
H*
Dm ^0T ) + T MHt gtgt ) = infHt>0,tr( H )≥tr( H-)
Ot)+Oa tr (H-ιgtgT)}
(33)
Proof. Now similarly, we can first construct the Lagrangian for the marginal regret bound minimiza-
tion problem. Let θ ≥ 0 denote a Lagrangian parameter for the trace constraint, and Z 0 for the
positive definiteness constraint. Then the Lagrangian problem is
L(Hte Z) = D2,2tr(Ht) + Ottr(H-1gtgt) - θtr((Ht - H-)) - tr(HtZ)	(34)
, αt 2	αt	αt-1
Take derivative with respect to Ht we can get
D21 - Ot H-IgtgT H-1-θI - Z = 0	(35)
Ot	2
where I is the identity matrix. If an invertible Ht can be found, then the generalized complementarity
condition (Boyd & Vandenberghe, 2004) implies that Z = 0 and either θ = 0 or tr(H) = tr(H-I).
If θ = 0
Ht =(9n2 gtgT)1/2 andtr( —t) ≥tr( t 1)	(36)
2Dt2,2	Ot	Ot-1
However, this is not an acceptable solution since it is not invertible.
If θ 6= 0, note that we need θ to be real, hence there is no solution because gtgtT has rank at most 1,
and even if θ = Dt2,2/Ot, there does not exist a matrix that makes the term Ht-1gtgtTHt-1 zero.
Instead, we propose the following matrix reaches the infimum of problem 31.
16
Under review as a conference paper at ICLR 2021
Ht
max
tr(尸 HT)	ι](上 gtgT 产
tr((元gtgT)1/2),2D2,2
(37)
It can be understood as a special maximum operation that makes sure
tr(Htt) = max 卜(J t-ɪ Ht-1),
√fc tr((gtgT ,"I
Now, a very important remark here is that the solution is not invertible, because gt gtT has rank at
most . Therefore, the equation above is not acceptable as a direct solution to problem (31). However,
setting Ht to be as in the above equation gives a solution to the infimum problem in Proposition B.1.
Let gt gtT be diagonally decomposed by
gtgtT = Q v0 00 QT,v = gtT gt = X gt2,i
(38)
and define the matrices Ht(δ) that can be written as,
0
δI
QT,
(39)
Therefore we know that limδ→0 Ht(δ) = Htt and that
D2,2tr( ')) + Wtr(Ht(δ)TgtgT)
,	αt	2
=D>ax (tr(HG √fc 卜 ɪ- { 20^^, EDs + ⑹卜(QK O^
=D22max (tr(√t≡Ht-1),	) + min / 1(√ +^以, √2Dt,2(√v + 峭]
, I α	√2Dt,2 J	[ 2tr(尸Ht-ι)	2
(40)
Hence
QT)
lim Dt,2tr( Q ) + ^7t柱(Ht(δ)-1gtgt)
δ→0	,	αt	2
=D2,2max (tr(口Ht-I) √√v~ ) + min /	—, √2D⅞≡ ]
, I α	√2Dt,2 J 1 2tr(尸Ht-1)	2
t
=D2,2tr( ~t) + Vtr(Ht-gtgT)	(41)
αt	2
D2 2tr( (	Ht-I)T------T==----- if tr( qt--1 Ht-I) is larger
_	' α	2tr(J t-1 Ht-I
√2Dt,2√V	if Aat	tr((gtgT)1/2) is larger
2Dt,2
17
Under review as a conference paper at ICLR 2021
D2
Now, let g(θ) = infHt (L(Ht,θ,Z (θ))) be the dual of problem (31), where when -O^
> θ we define
the matrices Z(θ) and Ht(θ, δ) as
Z(θ)
0
0
,Ht(θ,δ)=	/ T Q [√0v
y2(Dt,2 - θαt)	[
0
δI
(42)
Then from the derivative with respect to Ht, we know that
(D -THtG δ)-1gtgTHtG δ)-1 - Z⑹=0
(43)
Therefore Ht(θ, δ) achieves the minimum in the dual, moreover
g(θ) = D2>(H^) WIr(HtG δ尸gtgT…("- *)) - tr(Ht(θ, δ)ZG))
D22	(√V + δn)	+ J2v(D2,2 - θαt) - θ(	(√V + δn)	- tr(H-I))- (n - 1)δ(D2,2 - θαt)
t,2,2(D2,2 - θαt)	2	,2(D2,2 - θαt)	αt-1	√2(D2,2 - θαt)
(Dt2,2 -θ)
(√v + δn)
,2(D2,2 - θαt)
+
J2V(D2,2 - θ3t)
2
+ θtr( H-)
αt-1
—
(n - 1)δ JDt,2 - Oat
√2
(44)
Notice that take θ = 0, limδ→o g(θ) = √2Dt,t√v, and take θ to be
J2v(D2,2 - θαt)
at(√v + nδ)
tr( ɪ Ht-ι)
(45)
then
g(θ) = D2,2tr( 占 Ht-I) +
(46)
Note that they are equal to the two cases respectively, therefore the duality gap in this problem is zero
and Ht is indeed the infimum solution.
Similarly, Ht is always better than any other full matrix proximal function at t in terms of regret. A
unified algorithm is provided in Algorithm 2, where the maxt operation is a special operation that
shows how the diagonal AMX algorithm is similar to the full matrix one, in the sense that it tries to
find the maximum trace between the two matrices, i.e.
tr(Ht) = max |tr( jt-ɪHt-1), tr((Ctgtgt)”2)}
C Discussions on AMX algorithm with Momentum
We provide the detailed implementation of the diagonal AMX algorithm with momentum and
decoupled weight decay here. The momentum term is implemented in line 5 in Algorithm 3, where
mt = β1tmt-1+(1-β1t)gt and {β1t}tT=1 are called the momentum parameters. This implementation
is exactly the same as the implementation of modern adaptive algorithms such as Kingma & Ba
(2015); Reddi et al. (2018); Huang et al. (2019) and Li et al. (2020).
18
Under review as a conference paper at ICLR 2021
Algorithm 2 AMX Algorithm (Composite Mirror Descent Form)
1:	Input: x ∈ F, {αt}tT=1,{ct}tT=1,φ(x)
2:	Initialize h0 = 0, H0 = 0
3:	for t = 1 to T do
4：	gt = Pft(Xt)
5：	if Ht is a diagonal matrix then
6：	ht = max(Jɪ-1 ht-1, (ctg2)1/2)
7：	Ht = diag(ht ) +
8： else	____
9：	Ht = max*(ʌ/t-ɪHt-I (CtgtgT)1/2) + eI
10： end if
11：	xt+1 = argminx∈X {αt hgt, xi + αtφ(x) + hx - xt, Ht(x - xt)i}
12： end for
Algorithm 3 AMX Algorithm with Momentum (Diagonal)
1 2 3 4 5 6 7 8 9	： Input: x ∈ F, {αt }t=1 , {β1t }t=1 , Ct = T, = Te - 8, λ = 5e - 2 ： Initialize m0 = 0, h0 = 0 ： for t = T to T do ：	gt = Pft(xt) mt = β1tmt-1 + (1 — β1t)g t ht = Jmax( t-1 h2-1, CtgT) + E Ht = diag(ht,1, ht,2,…，ht,d) ：	xt+1 = ΠF,Ht (xt - αtmt∕ht - λαtxt) ： end for
The following theorem provides a regret bound for algorithm 3 and shows its convergence.
Theorem C.1 Let {xt} and {ht} be the sequences obtained from Algorithm 3, a = α∕√7, Ct =
1, β1,1 = β1, β1,t ≤ β1, for all t ∈ [T]. Assume that F has bounded diameter kx - yk∞ ≤
D∞, ∀x, y ∈ F and kPft(x)k ≤ G∞ for all t ∈ [T] and x ∈ F. Then for xt generated using
Algorithm 3, we have the following bound on the regret.
Dτ√ V D∞ √T XX 卜 ,	D∞ XX XX β1tht,i α√T+TθgT XX I， H Z47X
R(T) ≤ 20(T-而 g hT,i+ 2(T-β1) t=ι g Fr + (i-βι)3 ⅛ kg1:T，ik2,(47)
Corollary C.1 follows immediately from the above theorem.
Corollary C.1 Setting β1t = β1λt-1 in Theorem C.1, then we have
R(T、V D∞√T XXa I	DTG∞	α√τ÷TogΓ XX	U	(4G
R(T) ≤ 2α(1-βι) N hT，i + 2(1-βι)(1- λ2) + (1-βι)3 ⅛ kg1:T，ik2	(48)
Similarly, the bound above can be considered as better than the regret of SGD when Pd=i hτ,i《√d
and Pd=Ikgi：T,ik2《√dT (Duchi et al., 2011), which is the same as the claims We make in
Theorem 4.2. Another important observation is that Theorem 4.1 also holds here with similar
arguments (because the first term is almost the same as the first term without momentum, the second
term is a constant, and the third term is almost the same as well. We provide a similar theorem here.
Theorem C.2 Let {xt} and {ht} be the sequences obtained from Algorithm 3, αt = α∕√7, Ct =
1, βι,ι = βι,βιt = βιλt-1, for all t ∈ [T]. Assume that F has bounded diameter ∣∣x — yk∞ ≤
T , ∀x, y ∈ F and kPft(x)k ≤ GT for all t ∈ [T] and x ∈ F. Let {τm(ii)}id=1 be the largest time
steps t on each dimension when ht,i = |gt,i|. Then for xt generated using Algorithm 3, we have the
19
Under review as a conference paper at ICLR 2021
following bound on the regret.
D2 二 K D2 G一	α 二 /	777
R(T) ≤ 2α(1 - βι) X hτ(i),i V τmi + 2(1 - βι)(1 - λ2) + (1 - βι)3 X '1 + logTmikgLTm),ik2,
+ (⅛y X qm)log( T) )lgτmi) ,i|
1 i=1	τmi
(49)
The above regret bound can be considered as significantly better than O(√T) because it mostly
depends on the time steps τm(ii) instead of the number of time steps T. Given a distribution of gradients,
it is possible that Tmi,s are much smaller than T, and hence the regret is much smaller than common
adaptive optimizers. The regret is at most O(√T), which is the same as AdaGrad and AMSGrad.
We also conducted experiments to examine the effect of momentum on the convergence and per-
formance of our algorithm on CIFAR-10. As shown in figure 4, the choice of β1 did not affect the
convergence rate or the performance of AMX too much.
Aue,Jnu3v'do.L U 亘 j.
Figure 4: Training and testing Top-1 accuracy curve on CIFAR-10 with different momentum parame-
ters β1
90
85
›
百80
S 7。
60
55
50	10000 20000 30000 40000 50000 60000
Iterations
(b) Testing Top-1 Accuracy
C.1 Proof of Regret B ound of AMX with Momentum
C.1.1 Proof of Theorem 4.2 and Theorem C.1
Proof. Following the proof by Reddi et al. (2018), we provide the proof of regret bound in Theorem
4.2 and Theorem C.1. Note that Algorithm 1 is a special case of algorithm 1. Therefore we only need
to prove Theorem C.1 and Theorem 4.2 can be directly inferred by taking β1t = 0 and change α by
α∕2. By the definition of the projection operation ∏f,h , We know that
xt+1 = ΠF,Ht (xt - αtHt-1mt) = argminx∈F kHt1/2 (x - (xt - αtHt-1mt))k (50)
Using Lemma 4 in Reddi et al. (2018) with a direct substitution of z1 = (xt - αtHt-1mt), Q = Ht
and z2 = x* for x* ∈ F, the following inequality holds:
kH1∕2(uι - u2)k2 = IlH1/2(xt+i - x*)k2 ≤ kH1∕2(xt - αtH-1mt- x*)∣∣2
=∣∣H1∕2(xt — x*)k2 + α2∣∣H-1∕2mt∣∣2 - 2αthmt, (Xt- x*)i
= IHt / (xt - x*)I2 + αt2 IHt- / mt I2 - 2αt hβ1tmt-1 + (1 - β1t)gt, (xt - x*)i
(51)
20
Under review as a conference paper at ICLR 2021
where the first equality is due to ∏f,√v(x*) = x*. Rearrange the last inequality, We obtain
(1 - β1t)hgt, (χt - x*)i≤ W
[kH1/2(xt - x*)k2 -kHt1/2(xt+i - x*)k2] + αtkH-1∕2mtk2
一 β1thmt-1, (xt - x* )i
≤ 2α hkHt1/2(Xt-X*)k2-kHt1∕2(Xt+ι-
X*)k2] + 2 kH-1/2mtk2
+ 竽 kH-1∕2mt-ik2 + 2βα kHt1/2(Xt-X*)k2
(52)
The second inequality is from applying the Cauchy-SchWarz and Young’s inequality. By the convexity
of ft and Lemma C.1 and Lemma C.2, We have
Eft(Xt)- ft(x*) ≤ Ehgt, (Xt-X*)i
t=1
T
≤X
t=1
t=1
2O⅛ [闾/2(々 - x*)H2 -闾/2(” - 门月 + 2(⅛阳-1/2向2
≤	D∞2
β1tαt
2(1 - BIt)
d
kHt-1/2mt-1k2 +
β1t
2αt(1 - β1t)
kH1/2(Xt-X*)k2
2αT (1 - β1)
D∞2
i=1
d
hT,i +
t=1
T
β1t
2αt(1 - β1)
kHt1/2(Xt - x*)k2 + a,-+：： X kgrτ,ik2
2αT (1 - β1)
hT,i +
≤	D∞2
2αT (1 - β1)
≤ —D∞
2αT (1 - β1)
i=1
d
XhT,i+
i=1
d
XhT,i+
i=1
t=1
d
Σβ∖t(Xti - x*)2hti +
“ ι 一 I '	, i) t,i +
D∞
2(1 - βι)
D∞
2(1 - βι)
Td
XX
t=1 i=1
Td
XX
t=1 i=1
β1tht,i	a√1 + log T
at +	(1 - βι)3
β1tht,i + α√1 + log T
at +	(1 - βι)3
α√1 + log T	I， H
(1 - βι )3	2^kgl：T,ik2
d
X kg1:T,ik2
i=1
d
X kg1:T,ik2
i=1
(53)
To obtain the regret bound in 4.2, We can take βι = 0 and α = α∕2 by equivalence in A.1 in the
above bound, and the regret bound follows
RT ≤ d∞^-t
X hT,i + α P1 + log T X kgl：T,i k2,
i=1
i=1
(54)
T
T
+
T
α
d
d
C.1.2 Auxillary Lemmas
Lemma C.1 For the parameter settings and conditions assumed in Theorem C.1, we have
T	C	J .,——d
β~t	Bltat	Ii tt—1/2	∣∣2 / aV1 + log T∣∣
之 2IFkHt	mtk ≤ (1-βι)2 3 k"k2
where C is a constant.
Proof: We first analyze with the following process directly from the update rules, note that
mT,i =	(1 - β1j)Πk=1jβ1(T-k+1)gj,i
j=1
hT,i = max(-T-hT-1,i, gT,i)
(55)
(56)
T
21
Under review as a conference paper at ICLR 2021
T
X atkH-1/2mtk2
t=1
T -1
X αt kHt-1/2mt k
t=1
d m2
2	mT,i
+αTE 而;
i=1	T,i
T-1	d
X atkH-1/2mtk2 + a X
(PT=ι(1-βj )∏T-jβi(τ-k+i)gj,i )2
Tm max( t-1 hτ-ι,i, gτ ,i)
T-1	d
≤ X αtkH-1∕2mtk2 + a X
(PT=I ∏T=jβl(T-k+1))(PT=l(1 - β j)2∏T=jβl(T-k + 1)gj,i)
Tm max( T-1 hT -1,i,gT ,i)
T-1	d
≤ X αtkH-1∕2mtk2 + a X
(PT=I βT-j)(PT=ι β∕g2,i)
Tm maχ( TT-1 hτ-ι,i ,gT ,i)
T-1
≤ X atkH-1/2mt|
t=1
2+
α X	PT=I βT-g2,i
(I - βI) i=ι qT maχ( T-1 hT-ι,i ,gT ,i)
T-1
≤ X αt |Ht-1/2mt |
2+
t=1
α
(I- βI)
dT
XX
i=1 j=1
T-1
≤ X αt |Ht-1/2mt |
t=1
2 + α X X βT j|gj,i|
+ 1而 L j=ι ^j
(57)
where the first inequality is from the Cauchy-Schwarz inequality. The second inequality is due to
the fact that β1t ≤ β1, ∀t. The third inequality follows from PjT=1 β1T -j ≤ 1/(1 - β1) and that
1 — βij ≤ 1. The fourth one comes from the fact that h2,i ≥ t-1 h2-ι,i and that h2,i ≥ g2,『 Therefore
T	T	d t	t-j
X "mtk2 ≤ X O⅛ X X T
t=	t=	i=1j =1
α
(I- βl)
dTt
XXX
i=1 t=1j =1
βt-∣gj,i∣
√
(1 - β1)
dT	T
XX % X βj-t ≤
i=1 t=1 t j =t
(1 - β1)2
X X |gt,i|
i=1 ⅛1 √
(58)
d
≤ (1 -β1)2 X kg1：T,ik2
T1
X t ≤
t=1
α √1+log m
(1 - βι)2
d
|g1:T,i |2
i=1
α
α
The second equality is from rearranging the order of summation. The third inequality comes from
the fact that PjT=t β1j-t ≤ 1/(1 - β1). The second last inequality is due to the Cauchy-Schwarz
inequality.
Lemma C.2 For the parameter settings and conditions assumed in Theorem C.1, we have
T 1	D2 d
X — [kH1/2(xt - x*)k2 -kH1/2(xt+i - x*)k2] ≤ D∞ X hT,i	(59)
t=1 αt	2αT i=1
22
Under review as a conference paper at ICLR 2021
Proof: By the definition of L2 norm, since hti ≥ ht-1,i by the conditions in the problem (4)
T1
X - [kH1/2(Xt - X*)k2 -kH1/2(Xt+i - X*)k2]
t=1 αt
1T
≤ SkH1/2(X! - x*)k2 + ∑
d	Td
=α1 X hι,i(xι,i-x*)2 + XX
d	Td
=α1 X hι,i(χι,i-χ*)2 + XX
i=1	t=2 i=1
D2	d
≤ D∞ X hτi
—2ατ Nγ T,i
i=1
kHt1/2(Xt - X*)k2	kHt1-/21(Xt - X*)k2
αt
αt-1
ht,i	* 2
(Xt,i - xi ) -
αt
ht-1,i	* 2
(Xt,i - Xi)
αt-1	i
(60)
ht,i
αt
ht-1,i
αt-1
(Xt,i- xi)2
—
—
where the first inequality is deleting the last negative term in the summation. The last inequality is
from the telescopic summation and the bounded diameter that k X - x* k ≤ D∞
D	Experiment Details and More Experiments
For all the algorithms, we used the default momentum hyper-parameters, that is γ = 0.9 for SGDM,
(0.9, 0.999) for Adam and AMSGrad, and (β1, ct) = (0.9, 1) for AMX in algorithm 3. The small
number is set to be 1e-8 to avoid division by zero.
D.1 Synthetic Example
For the synthetic example in section 4, We used ∣gt,i | = √1t ∣gι,i |, ∀i, and ∣gι,i | = 0.01 across all the
dimensions. The dimension size is set to be d = 3, and the step sizes are set to be α = 0.5, at = α∕√7
for all the algorithms. The design of step sizes is much larger than What We usually use in real
applications, because it makes the increment in the regret bound of AMX much more visible. The
increment in the first term of AMX is already zero, and the increment in the second term is very
small. If We use a smaller step size α, We can barely see any increment in the regret bound of AMX
in the figures. The hyper-parameter of AMSGrad is set to be β2 = 0.999.
D.2 Image Classification
For CIFAR-10 and CIFAR-100, the 32 × 32 images Were zero-padded on the edges With 4 pixels on
each margin and randomly cropped and horizontally flipped to generate the input. The input images
Were also normalized using the dataset mean and standard deviation. For the step sizes on CIFAR-10,
We searched over {1e-4, 5e-4, 1e-3, 2e-3, 3e-3, 5e-3, 1e-2, 2e-2} for all the adaptive methods and
found that 5e-3 Works the best for AMX. The best step sizes for AdaGrad, Adam, AMSGrad Were
1e-2, 1e-3, 1e-3 respectively. For SGDM, the best step size Was 1e-1. We also grid-searched the
Weight decay on {1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4}. On CIFAR-10, 1e-1 Weight decay Worked
the best for Adam and AMSGrad and 5e-2 Worked the best for AMX. 5e-4 Worked the best for
AdaGrad and SGD. On CIFAR-100, all adaptive algotithms Worked the best When the Weight decay
Was 1e-1, but SGD still needed the 5e-4 Weight decay. The increments in the first term and the second
term of the regret bounds are plotted in Figure 5(a) and 5(b). On CIFAR-10, the step sizes of different
algorithms Were decreased by a factor of 0.1 at the the 100th and 150th epoch. On CIFAR-100, the
step sizes Were decreased by a factor of 0.2 at the 60th, 120th, and 160th epoch.
D.3 Image Segmentation
The Deeplab-ASPP model implemented by Kazuto1011 (2016) Was used in this task. We folloWed
their settings and reported the mean IoU values averaged over three independent runs. The model
23
Under review as a conference paper at ICLR 2021
Figure 5: The first and the second term of the regret bound in equation (2) with the diameter Dt,∞
replaced by D∞ = 2. As can be observed, the first term of AMX stops increasing after τ , which
is the first time step. The other algorithms do not have such a nice property. Therefore, even if the
second term of AMX is slightly larger than the second term of AdaGrad, the overall regret of AMX is
much smaller. That means AMX converge much faster than AdaGrad and AMSGrad in the example.
was pretrained on the MS-COCO dataset (Lin et al., 2014). We did not use the CRF post-processing
technique. We tried the initial step sizes {5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 3e-5, 1e-5, 5e-6} for all the
optimizers and we found that 1e-6 worked the best for Adam and AMSGrad. 5e-5 was the best for
AMX and 1e-3 was the best for SGD. Similar to the experiments on CIFAR-10, 0.1 weight decay
was applied to Adam and AMSGrad. 5e-2 weight decay was applied to AMX and 5e-4 weight decay
was applied to SGDM.
D.4 Language Modeling
We trained three layer LSTMs using the instructions provided by Kazuto1011 (2016). Specifically,
the LSTMs consisted of 200 embedding size and 1k hidden units. For the dropout probabilities
and the batch size, we followed the default values. A 1.2e-6 weight decay was applied to all the
algorithms and we tuned over {1e-4, 5e-4, 1e-3, 2e-3, 3e-3, 4e-3, 5e-3, 1e-2, 5e-2} for the initial step
sizes. We found that the algorithms were not very sensitive to the initial step sizes, and we reported
the results of 2e-3 for Adam and AMSGrad, 3e-3 for AMX, and 1e-2 for AdaGrad, which were the
best results among all the possible step sizes. We decayed the step size by a factor of 0.1 at the 300th
and 400th epochs.
D.5 Neural Machine Translation
We used the basic implementation of the attentional neural machine translation model by pcyin (2018)
and followed their settings. The hidden size of the LSTM was 256 and the embedding size was 256.
Label smoothing was set to be 0.1 and the drop out probability was 0.2. We tuned over {5e-2, 1e-2
5e-3, 1e-3, 5e-4, 1e-4} step sizes for all the adaptive optimizers and found that similar to CIFAR-10,
1e-3 worked the best for Adam and AMSGrad and 5e-3, 1e-2 were the best step sizes for AMX and
AdaGrad respectively. We averaged the BLEU scores on the IWSLT’14 German to English dataset
(Ranzato et al., 2015) over three independent runs and reported them in table 2.
D.6 Empirical study of the hyper-parameter
The hyper-parameter ct plays an important role in our class of AMX algorithms. We conducted an
empirical study on some of the designs of Ct and compared the performance of the corresponding
algorithms. We compared the designs Ct = 1,0.5,0.1, l/ʌ/t, 1/t on CIFAR-10 and reported their
top-1 accuracy in Figure 6 and Table 3. All the algorithms were trained using the same settings as
Appendix D.2 with the same initial step size 5e-4. As observed, changing the constant Ct to 0.1 and
24
Under review as a conference paper at ICLR 2021
0.5 did not affect the convergence speed or the final accuracy too much, which meant the performance
was not sensitive to the value of ct. However, Ct = 1∕√t and Ct = 1/t resulted in slower convergence
and worse performance, meaning that these designs were not preferred. These observations also
correspond to our claims in section A.5, i.e. despite that these designs make the first term in Theorem
2.1 even smaller, they also make the second term much larger so that the algorithm becomes slower.
0 5 0 5 0 5
0 9 9 8 8 7
A》EnMV I∙doJ. C-EH
IOOOO 20000 30000 40000 50000 60000
Iterations
(a) Training Top-1 Accuracy
Figure 6: Training and testing Top-1
different designs of ct
05050505
98877665
A》EmdV rtA-OH ttWH
50
10000 20000 30000 40000 50000 60000
Iterations
(b) Testing Top-1 Accuracy
Table 3: Testing Top-1 Accu-
racy on CIFAR-10 with differ-
ent designs of ct . The results
were averaged over 3 indepen-
dent runs.
Ct	Acc.	
Ct	1	92.42 ± 0.06
Ct	0.5	92.35 ± 0.08
Ct	二 0.1	92.46 ± 0.05
Ct	二 1∕√t	92.15 ± 0.06
Ct	1/t	91.73 ± 0.13
accuracy curve on CIFAR-10 with
25