Under review as a conference paper at ICLR 2021
Predicting the Outputs of Finite Networks
Trained with Noisy Gradients
Anonymous authors
Paper under double-blind review
Ab stract
A recent line of works studied wide deep neural networks (DNNs) by approximating
them as Gaussian Processes (GPs). A DNN trained with gradient flow was shown
to map to a GP governed by the Neural Tangent Kernel (NTK), whereas earlier
works showed that a DNN with an i.i.d. prior over its weights maps to the so-
called Neural Network Gaussian Process (NNGP). Here we consider a DNN
training protocol, involving noise, weight decay and finite width, whose outcome
corresponds to a certain non-Gaussian stochastic process. An analytical framework
is then introduced to analyze this non-Gaussian process, whose deviation from
a GP is controlled by the finite width. Our contribution is three-fold: (i) In the
infinite width limit, we establish a correspondence between DNNs trained with
noisy gradients and the NNGP, not the NTK. (ii) We provide a general analytical
form for the finite width correction (FWC) for DNNs with arbitrary activation
functions and depth and use it to predict the outputs of empirical finite networks
with high accuracy. Analyzing the FWC behavior as a function of n, the training set
size, we find that it is negligible for both the very small n regime, and, surprisingly,
for the large n regime (where the GP error scales as O(1/n)). (iii) We flesh-out
algebraically how these FWCs can improve the performance of finite convolutional
neural networks (CNNs) relative to their GP counterparts on image classification
tasks.
1	Introduction
Deep neural networks (DNNs) have been rapidly advancing the state-of-the-art in machine learning,
yet a complete analytic theory remains elusive. Recently, several exact results were obtained in the
highly over-parameterized regime (N → ∞ where N denotes the width or number of channels for
fully connected networks (FCNs) and convolutional neural networks (CNNs), respectively) (Daniely
et al., 2016). This facilitated the derivation of an exact correspondence with Gaussian Processes
(GPs) known as the Neural Tangent Kernel (NTK) (Jacot et al., 2018). The latter holds when highly
over-parameterized DNNs are trained by gradient flow, namely with vanishing learning rate and
involving no stochasticity.
The NTK result has provided the first example of a DNN to GP correspondence valid after end-to-end
DNN training. This theoretical breakthrough allows one to think of DNNs as inference problems with
underlying GPs (Rasmussen & Williams, 2005). For instance, it provides a quantitative description
of the generalization properties (Cohen et al., 2019; Rahaman et al., 2018) and training dynamics
(Jacot et al., 2018; Basri et al., 2019) of DNNs. Roughly speaking, highly over-parameterized DNNs
generalize well because they have a strong implicit bias to simple functions, and train well because
low-error solutions in weight space can be reached by making a small change to the random values of
the weights at initialization.
Despite its novelty and importance, the NTK correspondence suffers from a few shortcomings: (a) Its
deterministic training is qualitatively different from the stochastic one used in practice, which may
lead to poorer performance when combined with a small learning rate (Keskar et al., 2016). (b) It
under-performs, often by a large margin, convolutional neural networks (CNNs) trained with SGD
(Arora et al., 2019). (c) Deriving explicit finite width corrections (FWCs) is challenging, as it requires
solving a set of coupled ODEs (Dyer & Gur-Ari, 2020; Huang & Yau, 2019). Thus, there is a need
for an extended theory for end-to-end trained deep networks which is valid for finite width DNNs.
1
Under review as a conference paper at ICLR 2021
Our contribution is three-fold. First, we prove a correspondence between a DNN trained with noisy
gradients and a Stochastic Process (SP) which at N → ∞ tends to the Neural Network Gaussian
Process (NNGP) (Lee et al., 2018; Matthews et al., 2018). In these works, the NNGP kernel is
determined by the distribution of the DNN weights at initialization which are i.i.d. random variables,
whereas in our correspondence the weights are sampled across the stochastic training dynamics,
drifting far away from their initial values. We call ours the NNSP correspondence, and show that it
holds when the training dynamics in output space exhibit ergodicity.
Second, we predict the outputs of trained finite-width DNNs, significantly improving upon the
corresponding GP predictions. This is done by deriving leading FWCs which are found to scale
with width as 1/N. The accuracy at which we can predict the empirical DNNs’ outputs serves as a
strong verification for our aforementioned ergodicity assumption. In the regime where the GP RMSE
error scales as 1/n, we find that the leading FWC are a decaying function of n, and thus overall
negligible. In the small n regime we find that the FWC is small and grows with n. We thus conclude
that finite-width corrections are important for intermediate values of n (Fig. 1).
Third, we propose an explanation for why finite CNNs trained on image classification tasks can
outperform their infinite-width counterparts, as observed by Novak et al. (2018). The key difference
is that in finite CNNs weight sharing is beneficial. Our theory, which accounts for the finite width,
quantifies this difference (§4.2).
Overall, the NNSP correspondence provides a rich analytical and numerical framework for exploring
the theory of deep learning, unique in its ability to incorporate finite over-parameterization, stochas-
ticity, and depth. We note that there are several factors that make finite SGD-trained DNNs used
in practice different from their GP counterparts, e.g. large learning rates, early stopping etc. (Lee
et al., 2020). Importantly, our framework quantifies the contribution of finite-width effects to this
difference, distilling it from the contribution of these other factors.
1.1	Related work
The idea of leveraging the dynamics of the gradient descent algorithm for approximating Bayesian
inference has been considered in various works (Welling & Teh, 2011; Mandt et al., 2017; Teh et al.,
2016; Maddox et al., 2019; Ye et al., 2017). However, to the best of our knowledge, a correspondence
with a concrete SP or a non-parametric model was not established nor was a comparison made of the
DNN’s outputs with analytical predictions.
Finite width corrections were studied recently in the context of the NTK correspondence by several
authors. Hanin & Nica (2019) study the NTK of finite DNNs, but where the depth scales together with
width, whereas we keep the depth fixed. Dyer & Gur-Ari (2020) obtained a finite N correction to the
linear integral equation governing the evolution of the predictions on the training set. Our work differs
in several aspects: (a) We describe a different correspondence under different a training protocol
with qualitatively different behavior. (b) We derive relatively simple formulae for the outputs which
become entirely explicit at large n. (c) We account for all sources of finite N corrections whereas
finite N NTK randomness remained an empirical source of corrections not accounted for by Dyer
& Gur-Ari (2020). (d) Our formalism differs considerably: its statistical mechanical nature enables
one to import various standard tools for treating randomness, ergodicity breaking, and taking into
account non-perturbative effects. (e) We have no smoothness limitation on our activation functions
and provide FWCs on a generic data point and not just on the training set.
Another recent paper (Yaida, 2020) studied Bayesian inference with weakly non-Gaussian priors
induced by finite-N DNNs. Unlike here, there was no attempt to establish a correspondence with
trained DNNs. The formulation presented here has the conceptual advantage of representing a
distribution over function space for arbitrary training and test data, rather than over specific draws of
data sets. This is useful for studying the large n behavior of learning curves, where analytical insights
into generalization can be gained (Cohen et al., 2019).
A somewhat related line of work studied the mean field regime of shallow NNs (Mei et al., 2018;
Chen et al., 2020; Tzen & Raginsky, 2020). We point out the main differences from our work:
(a) The NN output is scaled differently with width. (b) In the mean field regime one is interested
in the dynamics (finite t) of the distribution over the NN parameters in the form of a PDE of the
Fokker-Planck type. In contrast, in our framework we are interested in the distribution over function
2
Under review as a conference paper at ICLR 2021
space at equilibrium, i.e. for t → ∞. (c) It seems that the mean field analysis is tailored for two-layer
fully-connected NNs and is hard to generalize to deeper nets or to CNNs. In contrast, our formalism
generalizes to deeper fully-connected NNs and to CNNs as well, as we showed in section 4.2.
2	The NNSP correspondence
In this section we show that finite-width DNNs, trained in a specific manner, correspond to Bayesian
inference using a non-parametric model which tends to the NNGP as N → ∞. We first give a
short review of Langevin dynamics in weight space as described by Neal et al. (2011), Welling &
Teh (2011), which we use to generate samples from the posterior over weights. We then shift our
perspective and consider the corresponding distribution over functions induced by the DNN, which
characterizes the non-parametric model.
Recap of Langevin-type dynamics - Consider a DNN trained with full-batch gradient descent while
injecting white Gaussian noise and including a weight decay term, so that the discrete time dynamics
of the weights read
∆wt := wt+1 - Wt = -(Ywt + VwL (Zw)) dt + v2Tdtξt
(1)
where wt is the vector of all network weights at time step t, γ is the strength of the weight decay,
L(zw) is the loss as a function of the output zw, T is the temperature (the magnitude of noise), dt
is the learning rate and ξt 〜N(0, I). As dt → 0 these discrete-time dynamics converge to the
continuous-time Langevin equation given by w (t) = -Vw (2||w(t)||2 + L (Zw)) + √2Tξ (t) with
hξi(t)ξj(t0)i = δijδ (t - t0), so that as t → ∞ the weights will be sampled from the equilibrium
distribution in weight space, given by (Risken & Frank, 1996)
P (w) H exp
||w||2+L(Zw)
exp
llwll2 + 2σ2 L (Zw)))
(2)
The above equality holds since the equilibrium distribution of the Langevin dynamics is also the
posterior distribution of a Bayesian neural network (BNN) with an i.i.d. Gaussian prior on the
weights w 〜N(0, σwI). Thus we can map the hyper-parameters of the training to those of the
BNN: σw = T∕γ and σ2 = T/2. Notice that a sensible scaling for the weight variance at layer ' is
σw,'〜OQ/N'-i),thus the weight decay needs to scale as γ' 〜O(N'-ι).
A transition from weight space to function space - We aim to move from a distribution over weight
space Eq. 2 to a one over function space. Namely, we consider the distribution of Zw (x) implied by
the above P(w) where for concreteness we consider a DNN with a single scalar output Zw (x) ∈ R
on a regression task with data {(xα, yα)}nα=1 ⊂ Rd × R. Denoting by P[f] the induced measure on
function space we formally write
P[f] =
dwδ[f - Zw]P (w)
(X e-2σ2L[f] / dwe-21W llwll2δ[f - zw]
(3)
where dw denotes an integral over all weights and we denote by δ[f - Zw] a delta-function in
function-space. As common in path-integrals or field-theory formalism (Schulman, 2012), such a
delta function is understood as a limit procedure where one chooses a suitable basis for function
space, trims it to a finite subset, treats δ[f - Zw] as a product of regular delta-functions, and at the
end of the computation takes the size of the subset to infinity.
To proceed We decompose the posterior over functions Eq. 3 as P[f] X e-212L[f]P0 [f] where the
prior over functions is P0[f] X R dwe 2σW llwl1 δ[f - Zw]. The integration over weights now obtains
a clear meaning: it yields the distribution over functions induced by a DNN with i.i.d. random weights
—12- ∣∣w∣∣2
chosen according to the prior P0 (w) X e 2σw2	. Thus, we can relate any correlation function in
function space and weight space, for instance (Df is the integration measure over function space)
DfP0 [f]f(x)f(x0) =	Df
dwP0(w)δ[f -Zw
]f(x)f(x0) =
dwP0 (w)Zw (x)Zw (x0) (4)
As noted by Cho & Saul (2009), for highly over-parameterized DNNs the r.h.s. of4 equals the kernel
of the NNGP associated with this DNN, K(x, x0). Moreover P0[f] tends to a Gaussian and can be
3
Under review as a conference paper at ICLR 2021
written as
Po[f] H exp (-1 ∕dμ(x)dμ(x0)f(x)K-1(x,x0)f (x0)) + O (1/N)	(5)
where μ(χ) is the measure of the input space, and the O(1∕N) scaling of the finite-N correction
will be explained in §3. If we now plug 5 in 3, take the loss to be the total square error1 L[f] =
Pnα=1 (yα - f (xα))2, and take N → ∞ we have that the posterior P[f] is that of a GP. Assuming
ergodicity, one finds that training-time averaged output of the DNN is given by the posterior mean of
a GP, with measurement noise2 equal to σ2 = T /2 and a kernel given by the NNGP of that DNN.
We refer to the above expressions for P0[f] and P[f] describing the distribution of outputs of a DNN
trained according to our protocol - the NNSP correspondence. Unlike the NTK correspondence, the
kernel which appears here is different and no additional initialization dependent terms appear (as
should be the case since we assumed ergodicity). Furthermore, given knowledge of P0 [f] at finite N,
one can predict the DNN’s outputs at finite N. Henceforth, we refer to P0[f] as the prior distribution,
as it is the prior distribution of a DNN with random weights drawn from P0(w).
Evidence supporting ergodicity - Our derivation relies on the ergodicity of the dynamics. Ergodicity
is in general hard to prove rigorously in non-convex settings, and thus we must revert to heuristics.
The most robust evidence of ergodicity in function space is the high level of accuracy of our analytical
expressions w.r.t. to our numerical results. This is a self-consistency argument: we assume ergodicity
in order to derive our analytical results and then indeed find that they agree very well with the
experiment, thus validating our original assumption.
Another indicator of ergodicity is a small auto-correlation time (ACT) of the dynamics. Although
short ACT does not logically imply ergodicity (in fact, the converse is true: exponentially long
ACT implies non-ergodic dynamics). However, the empirical ACT gives a lower bound on the
true correlation time of the dynamics. In our framework, it is sufficient that the dynamics of the
outputs zw be ergodic, even if the dynamics of the weights converge much slower to an equilibrium
distribution. Indeed, we have found that the ACTs of the outputs are considerably smaller than those
of the weights (see Fig. 2b). Full ergodicity may be too strong of a condition and we don’t really
need it for our purposes, since we are mainly interested in collecting statistics that will allow us to
accurately compute the posterior mean of the distribution in function space. Thus, a weaker condition
which is sufficient here is ergodicity in the mean (see App. F), and we believe our self-consistent
argument above demonstrates that it holds.
In a related manner, optimizing the train loss can be seen as an attempt to find a solution to n
constraints using far more variables (roughly M ∙ N2 where M is the number of layers). From a
different angle, in a statistical mechanical description of satisfiability problems, one typically expects
ergodic behavior when the ratio of the number of variables to the number of constraints becomes
much larger than one (Gardner & Derrida, 1988).
3	Inference on the resulting NNSP
Having mapped the time-averaged outputs of a DNN to inference on the above NNSP, we turn to
analyze the predictions of this NNSP in the case where N is large but finite, such that the NNSP is
only weakly non-Gaussian (i.e. its deviation from a GP is O (1/N)). The main result of this section
is a derivation of leading FWCs to the standard GP regression results for the posterior mean，gp (x*)
and variance ∑gp(x*) on an unseen test point x*, given a training set {(χα, yα)}α=ι ⊂ Rd X R,
namely (Rasmussen & Williams, 2005)
fGp(χ*) = X yaK αβκ ；	∑gp(x*) = κ **- X Ka Kɑβκ	⑹
α,β	α,β
where Kaβ ：= K(xa,xβ) + σ2δaβ; Ka ：= K(x*,xa);	K** ：= K(x*,x*).
1We take the total error, i.e. we don’t divide by n so that L[f] becomes more dominant for larger n.
2Here σ2 is a property of the training protocol and not of the data itself, or our prior on it.
4
Under review as a conference paper at ICLR 2021
3.1	Edgeworth expansion and perturbation theory
Our first task is to find how P [f] changes compared to the Gaussian (	→ ∞) scenario. As the
data-dependent part e-L[fP2σ is independent of the DNNs, this amounts to obtaining finite width
corrections to the prior P0 [f]. One way to characterize this is to perform an Edgeworth expan-
sion of P0 [f] (Mccullagh, 2017; Sellentin et al., 2017). We give a short recap of the Edgeworth
expansion to elucidate our derivation, beginning with a scalar valued RV. Consider continuous iid
RVs {Zi}	and assume WLOG	hZii	=	0,	Zi2	= 1, with higher cumulants κrZ	for r ≥	3.	Now
consider their normalized sum YN
Y	NκrZ
we have κr≥2 ：= κr≥2 = (√N)r
√N PN=I Zi. From additivity and homogeneity of cumulants
二 NKI2-ι. Now, let 6(y) := (2π)-1/2e-y2/2. The Charac-
.	_ _ _ . 0 ,.	一一 一 ，
tenstic function of Y is f(t) := F[f (y)]
Taking the inverse Fourier transform F-1
exp Pr∞=1 κr
∞
exp	r=3 κr
has the effect of mapping it 7→ -∂y thus we get
6⑴.
f(y) = exp Pr∞=3 κr
r!
ψ(y) = 6(y)(1 + P∞=3 K!Hr(y)) where Hr(y) is the rth Her
mite polynomial, e.g. H4(y) = y4 - 6y2 + 3. Ifwe were to consider vector-valued RVs, then the r’th
order cumulant becomes a tensor with r indices, and the Hermite polynomials become multi-variate
polynomials. In our case, we are considering random functions defined by our stochastic process (the
NNSP), thus the cumulants are functional tensors, i.e. are continuously indexed by the inputs x
α.
This is especially convenient here since for all DNNs with the last layer being fully-connected, all odd
cumulants vanish and the 2rth cumulant scales as 1/N r-1. Consequently, at large N we can char-
acterize P0[f] up to O(N -2) by its second and fourth cumulants, K(x1, x2) and U(x1, x2, x3, x4),
respectively. Thus the leading order correction to P0 [f] reads
Po [f] Y e-SGp[f]
1 - NNSu[f]) + O(1/N2)
(7)
where the GP action SGP and the first FWC action SU are given by
SGP[f]
2 / dμi.2fxι K-IIx2 fX2 ；
SU[f]
1
-4! I dμL4Uxl ,x2,x3,x4 Hxι,x2,x3,x4 [f]	(8)
Here, H is the 4th functional Hermite polynomial (see App. A), U is the 4th order functional
cumulant of the NN output3, which depends on the choice of the activation function φ
Ux1,x2,x3,x4 = ςa4 (hφαφβφγφδi - hφαφβi hφγφδi) +2 other perms. of (α, β, γ, δ) ∈ {1, . . . ,4}
(9)
where φα := φ(z^i-1(χa)') and the pre-activations are z'(χ) = b' + PN= 1 Wjφ(z'-1(χ)). Here
we distinguished between the scaled and non-scaled weight variances: σ∖ = ς2∕N, where a are the
weights of the last layer. Our shorthand notation for the integration measure over inputs means e.g.
dμ1∙,4 := dμ(xι)…dμ(x4).
Using perturbation theory, in App. B we compute the leading FWC to the posterior mean f (x*) and
variance (δf(x*))2 on a test point x*
f(x*) = fGp(x*) + N TfU (x*) + O(N-2)
(δf(x*))2 = ΣGP(x*) + N-1ΣU(x*) + O(N-2)
with ∑u(x*) =〈(f(x*))2)U — 2fcp(x*)fu(x*) and
fU (X*) = 1 Ua1a2a3 (yaι ya2 ya3 — 3K-1la2 ya3
〈(f(x*))2〉U = 2Ua*a2 (yaιya2 - K-a?)
(10)
(11)
where all repeating indices are summed over the training set (i.e. range over {1, . . . , n}), denoting:
yα ：= K-β1yβ, and defining
~ ,
U *	:
α1 α2α3
~ ..
U**	:
α1α2 :
rr**
Uα1 α2
-Uα1α2α3α4 K-41βK
(UaIa2a3+ UaIa2a3) K-31βKβ
(12)
Uai
—
3Here we take U 〜O(1) to emphasize the scaling with N in Eqs. 7, 10.
5
Under review as a conference paper at ICLR 2021
Equations 11, 12 are one of our key analytical results, which are qualitatively different from the
corresponding GP expressions Eq. 6. The correction to the predictive mean fu (x*) has a linear
term in y , which can be viewed as a correction to the GP kernel, but also a cubic term in y, unlike
∕gp (x*) which is purely linear. The correction to the predictive variance ∑u(x*) has quartic and
quadratic terms in y, unlike ∑gp(x* ) which is y-independent. Ualα2α3 has a clear interpretation in
terms of GP regression: if we consider the indices ɑι, α2,α3 as fixed, then UAiazaa Can be thought
of as the ground truth value of a target function (analogous to y*), and the second term on the r.h.s.
Uα1α2α3α4K-1βKe is then the GP prediction of 以皿@3 with the kernel K, where a4 runs on the
training set (compare to ∕gp (x*) in Eq. 6). Thus Ua1a2a3 is the discrepancy in predicting Uai a2a3a4
using a GP with kernel K. In §3.2 we study the behavior of fu (x*) as a function of n.
The posterior variance Σ(x)
(δf(x))2 has a clear interpretation in our correspondence: it is
a measure of how much we can decrease the test loss by averaging. Our procedure for generating
empirical network outputs involves time-averaging over the training dynamics after reaching equilib-
rium and also over different realizations of noise and initial conditions (see App. F). This allows for
a reliable comparison with our FWC theory for the mean. In principle, one could use the network
outputs at the end of training without this averaging, in which case there will be fluctuations that will
scale with Σ(xa). Following this, one finds that the expected MSE test loss after training saturates is
n-1 Pα=1	(Xa) - y (Xa)) 2〉+ ς(Xa))
where n* is the size of the test set.
3.2	Finite width corrections for small and large data sets
The expressions in Eqs. 6, 11 for the GP prediction and the leading FWC are explicit but only up to a
potentially large matrix inversion, K-1. These matrices also have a random component related to the
largely arbitrary choice of the particular n training points used to characterize the target function. An
insightful tool, used in the context of GPs, which solves both these issues is the Equivalent Kernel
(EK) (Rasmussen & Williams, 2005; Sollich & Williams, 2004). The EK approximates the GP
predictions at large n, after averaging on all draws of (roughly) n training points representing the
target function. Even if one is interested in a particular dataset, the EK result captures the behavior of
specific dataset up to small corrections. Essentially, the discrete sums over the training set appearing
in Eq. 6 are replaced by integrals over all input space, which together with a spectral decomposition
of the kernel function K(X, X0) = Pi λiψi(X)ψi(X0) yields the well known result
fGK(x*) = Z dμ(x0) X "F?Tx g(x0)	(13)
J L	λi + σ2∕n
Here we develop an extension of Eq. 13 for the NNSPs we find at large but finite N . In particular,
we find the leading non-linear correction to the EK result, i.e. the "EK analogue" of Eq. 11. To this
end, we consider the average predictions of an NNSP trained on an ensemble of data sets of size n0,
corresponding to n0 independent draws from a distribution μ(x) over all possible inputs x. Following
the steps in App. J we find
fUκ(x*)
6δχ*χιuχ1,χ2,χ3,χ4 ]σ6δχ2χ2g(χ2)δχ3χ3g(χ3)δχ4χ4g(χ4)	σ^δχ2,χ3δχ4,χ4g(χ4)
(14)
where an integral / dμ(x) is implicit for every pair of repeated X coordinates. We introduced the
discrepancy operator δχχo which acts on some function φ as J dμ(x )δχχo夕(x0) := δχχo夕(x0)=
夕(x) - fEK(x)∙ Essentially, Eq. 14 is derived from Eq. 11 by replacing each KT by (n∕σ2)δ and
noticing that in this regime U*2,χ3,χ4 in Eq. 12 becomes 鼠*χι Uχ1,χ2,χ3,χ4. Interestingly, fEκ(x*) is
written explicitly in terms of meaningful quantities: δxx0 g(X0) and δx*xi Uxi,x2,x3,x4.
Equations 13, 14 are valid for any weakly non-Gaussian process, including ones related to CNNs
(where N corresponds to the number of channels). It can also be systematically extended to smaller
values of n by taking into account higher terms in 1∕n, as in Cohen et al. (2019). At N → ∞, we
obtain the standard EK result, Eq. 13. It is basically a high-pass linear filter which filters out features
of g that have support on eigenfunctions ψi associated with eigenvalues λi that are small relative
to σ2∕n. We stress that the ψi, λi,s are independent of any particular size n dataset but rather are
6
Under review as a conference paper at ICLR 2021
a property of the average dataset. In particular, no computationally costly data dependent matrix
inversion is needed to evaluate Eq. 13.
Turning to our FWC result, Eq. 14, it depends on g(x) only via the discrepancy operator δxx0. Thus
these FWCs would be proportional to the error of the DNN, at N → ∞. In particular, perfect
performance at N → ∞, implies no FWC. Second, the DNN’s average predictions act as a linear
transformation on the target function combined with a cubic non-linearity. Third, for g(x) having
support only on some finite set of eigenfunctions ψi of K, δxxo g (x0) would scale as σ2/n at very
large n. Thus the above cubic term would lose its explicit dependence on n. The scaling with n
of this second term is less obvious, but numerical results suggest that δx2x3 also scales as σ2 /n, so
that the whole expression in the {∙ ∙ ∙ } has no scaling with n. In addition, some decreasing behavior
with n is expected due to the Sx*x1 Ux1 ,x2,x3,x4 factor which can be viewed as the discrepancy in
predicting Ux^2 ,x3 ,x4, at fixed X2, X3 ,X4, based on n random samples (Xa's) of Uxa ,x2 ,x3 ,x4. In Fig.
1 we illustrate this behavior at large n and also find that for small n the FWC is small but increasing
with n, implying that at large N FWCs are only important at intermediate values of n.
21	22	23	24	25	26	27
train set size
21	22	23	24	25	26	27
train set size
21	22	23	24	25	26	27
train set size
(a) FWC vs. n for d = 8
(b) FWC vs. n for d = 4
(c) GP RMSE vs. nford = 4
Figure 1: Leading FWC to the mean IfU(x*)| (Eq. 11) and GP discrepancy
(RMSE) as a function of train set size n for varying training noise σ2. The
target is quadratic g(x) = XTAx = O(1) with X ∈ Sd-I(Vzd) so the number
of parameters to be learnt is d(d + 1)/2 (vertical grey dashed line). The GP
discrepancy is monotonically decreasing with n whereas IfU (x*)| increases
linearly for small n (dashed-dotted lines in (a)) before it decays (best illustrated
for larger d and σ2). For sufficiently large n, both the GP discrepancy and
IfU (x*)| scale as 1/n (diagonal dashed black lines in (b),(c)). This verifies our
prediction for the scaling of FWCs with n, Eq. 14 in the large n regime . Notably,
it implies that at large N FWCs are only important at intermediate values of n.
4	Numerical experiments
In this section we numerically test our analytical results. We first demonstrate that in the limit
N → ∞ the outputs of a FCN trained in the regime of the NNSP correspondence converge to a GP
with a known kernel, and that the MSE between them scales as 〜 1/N2 which is the scaling of the
leading FWC squared. Second, we show that introducing the leading FWC term NT fU (x*), Eq. 11,
further reduces this MSE by more than an order of magnitude. Third, we study the performance gap
between finite CNNs and their corresponding NNGPs on CIFAR-10.
4.1	Toy example: fully connected networks on synthetic data
We trained a 2-layer FCN f (x) = PN=I aiφ(w⑴∙ x) on a quadratic target y (x) = XT Ax where the
x’s are sampled with a uniform measure from the hyper-sphere Sd-1 (Vd), see App. G.1 for more
details. Our settings are such that there are not enough training points to fully learn the target: Fig. 2a
shows that the time averaged outputs (after reaching equilibrium) fDNN (x*) is much closer to the GP
prediction fGP (x*) than to the ground truth y*. Otherwise, the convergence of the network output
to the corresponding NNGP as N grows (shown in Fig. 2c) would be trivial, since all reasonable
estimators would be close to the target and hence close to each other.
7
Under review as a conference paper at ICLR 2021
In Fig. 2c We plot in log-log Scale (with base 10) the MSE (normalized by (fDNN)2) between the
predictions of the network fDNN and the corresponding GP and FWC predictions for quadratic
and ReLU activations. We find that indeed for sufficiently large widths (N & 500) the slope of
the GP-DNN MSE approaches -2.0 (for both ReLU and quadratic), which is expected from our
theory, since the leading FWC scales as 1/N. For smaller widths, higher order terms (in 1/N) in the
Edgeworth series Eq. 7 come into play. For quadratic activation, we find that our FWC result further
reduces the MSE by more than an order of magnitude relative to the GP theory. We recognize a
regime where the GP and FWC MSEs intersect at N . 100, below which our FWC actually increases
the MSE, which suggests a scale of how large N needs to be for our leading FWC theory to hold.
(a) Predictions
(b) Auto-correlation functions
Figure 2: Fully connected 2-layer network trained on a regression task. (a)
Network outputs on a test point f (x*, t) vs. normalized time: the time-averaged
DNN output f(x*) (dashed line) is much closer to the GP prediction fGP(x*)
(dotted line) than to the ground truth y* (dashed-dotted line). (b) ACFs of the
time series of the 1st and 2nd layer weights, and of the outputs: the output
converges to equilibrium faster than the weights. (c) Relative MSE between the
network outputs and the labels y (triangles), GP predictions f GP (x*) Eq. 6 (dots),
and FWC predictions Eq. 10 (x’s), shown vs. width for quadratic (blue) and
ReLU (red) activations. For sufficiently large widths (N & 500) the slope of the
GP-DNN MSE approaches -2.0 and the FWC-DNN MSE is further improved
by more than an order of magnitude.
(c) MSE scaling
4.2	Performance gap between finite CNNs and their NNGP
Several papers have shown that the performance on image classification tasks of SGD-trained finite
CNNs can surpass that of the corresponding GPs, be it NTK (Arora et al., 2019) or NNGP (Novak
et al., 2018). More recently, Lee et al. (2020) emphasized that this performance gap depends on
the procedure used to collapse the spatial dimensions of image-shaped data before the final readout
layer: flattening the image into a one-dimensional vector (CNN-VEC) or applying global average
pooling to the spatial dimensions (CNN-GAP). It was observed that while infinite FCN and CNN-
VEC outperform their respective finite networks, infinite CNN-GAP networks under-perform their
finite-width counterparts, i.e. there exists a finite optimal width.
One notable margin, of about 15% accuracy on CIFAR10, was shown in Novak et al. (2018) for
the case of CNN-GAP. It was further pointed out there, that the NNGPs associated with CNN-VEC,
coincide with those of the corresponding Locally Connected Networks (LCNs), namely CNNs
without weight sharing between spatial locations. Furthermore, the performance of SGD-trained
LCNs was found to be on par with that of their NNGPs. We argue that our framework can account
for this observation. The priors P0 [f] of a LCN and CNN-VEC agree on their second cumulant (the
covariance), which is the only one not vanishing as N → ∞, but they need not agree on their higher
order cumulants, which come into play at finite N. In App. I we show that U appearing in our leading
FWC, already differentiates between CNNs and LCNs. Common practice strongly suggests that the
prior over functions induced by CNNs is better suited than that of LCNs for classification of natural
images. As a result we expect that the test loss of a finite-width CNN trained using our protocol
will initially decrease with N but then increase beyond some optimal width Nopt , tending towards
the loss of the corresponding GP as N → ∞. This is in contrast to SGD behavior reported in some
works where the CNN performance seems to saturate as a function of N , to some value better than
8
Under review as a conference paper at ICLR 2021
the NNGP (Novak et al., 2018; Neyshabur et al., 2018). Notably those works used maximum over
architecture scans, high learning rates, and early stopping, all of which are absent from our training
protocol.
To test the above conjecture we trained, according
to our protocol, a CNN with six convolutional lay-
ers and two fully connected layers on CIFAR10, and
used CNN-VEC for the readout. We used MSE loss
with a one-hot encoding into a 10 dimensional vector
of the categorical label; further details and additional
settings are given in App. G. Fig. 3 demonstrates that,
using our training protocol, a finite CNN can outper-
form its corresponding GP and approaches its GP as
the number of channels increases. This phenomenon
was observed in previous studies under realistic train-
ing settings (Novak et al., 2018), and here we show
that it appears also under our training protocol. We
note that a similar yet more pronounced trend in per-
formance appears here also when one considers the
averaged MSE loss rather the the MSE loss of the
average outputs.
Figure 3: DNN-GP MSE demonstrates con-
vergence to a slope of -2.0, validating the
theoretically expected scaling. DNN-ground
truth (Y) MSE shows finite CNN can outper-
form corresponding GP.
5	Conclusion
In this work we presented a correspondence between
finite-width DNNs trained using Langevin dynamics (i.e. using small learning rates, weight-decay
and noisy gradients) and inference on a stochastic-process (the NNSP), which approaches the
NNGP as N → ∞. We derived finite width corrections, that improve upon the accuracy of the
NNGP approximation for predicting the DNN outputs on unseen test points, as well as the expected
fluctuations around these. In the limit of a large number of training points n → ∞, explicit
expressions for the DNNs’ outputs were given, involving no costly matrix inversions. In this regime,
the FWC can be written in terms of the discrepancy of GP predictions, so that when GP has a small
test error the FWC will be small, and vice versa. In the small n regime, the FWC is small but
grows with n, which implies that at large N , FWCs are only important at intermediate values of n.
For no-pooling CNNs, we build on an observation made by Novak et al. (2018) that finite CNNs
outperform their corresponding NNGPs, and show that this is because the leading FWCs reflect the
weight-sharing property of CNNs which is ignored at the level of the NNGP. This constitutes one
real-world example where the FWC is well suited to the structure of the data distribution, and thus
improves performance relative to the corresponding GP. In a future study, it would be very interesting
to consider well controlled toy models that can elucidate under what conditions on the architecture
and data distribution does the FWC improve performance relative to GP.
9
Under review as a conference paper at ICLR 2021
References
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
Exact Computation with an Infinitely Wide Neural Net. arXiv e-prints, art. arXiv:1904.11955, Apr
2019.
Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman. The Convergence Rate of Neural
Networks for Learned Functions of Different Frequencies. arXiv e-prints, art. arXiv:1906.00425,
Jun 2019.
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. Mean-field analysis of two-layer neural
networks: Non-asymptotic rates and generalization bounds. arXiv preprint arXiv:2002.04026,
2020.
Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Proceedings of the
22Nd International Conference on Neural Information Processing Systems, NIPS'09,pp. 342-350,
USA, 2009. Curran Associates Inc. ISBN 978-1-61567-911-9. URL http://dl.acm.org/
citation.cfm?id=2984093.2984132.
Omry Cohen, Or Malka, and Zohar Ringel. Learning Curves for Deep Neural Networks: A Gaussian
Field Theory Perspective. arXiv e-prints, art. arXiv:1906.05301, Jun 2019.
A. Daniely, R. Frostig, and Y. Singer. Toward Deeper Understanding of Neural Networks: The Power
of Initialization and a Dual View on Expressivity. ArXiv e-prints, February 2016.
Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=S1gFvANKDS.
E. Gardner and B. Derrida. Optimal storage properties of neural network models. Journal of Physics
A Mathematical General, 21:271-284, January 1988. doi: 10.1088/0305-4470/21/1/031.
Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. arXiv
preprint arXiv:1909.05989, 2019.
Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy.
arXiv preprint arXiv:1909.08156, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
Generalization in Neural Networks. arXiv e-prints, art. arXiv:1806.07572, Jun 2018.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.
Jaehoon Lee, Samuel S Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak,
and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. arXiv
preprint arXiv:2007.15801, 2020.
Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson. A
Simple Baseline for Bayesian Uncertainty in Deep Learning. arXiv e-prints, art. arXiv:1902.02476,
Feb 2019.
Stephan Mandt, Matthew D. Hoffman, and David M. Blei. Stochastic Gradient Descent as Approxi-
mate Bayesian Inference. arXiv e-prints, art. arXiv:1704.04289, Apr 2017.
Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. arXiv preprint arXiv:1804.11271, 2018.
Peter Mccullagh. Tensor Methods in Statistics. Dover Books on Mathematics, 2017.
10
Under review as a conference paper at ICLR 2021
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings ofthe National Academy of Sciences, 115(33):E7665-E7671,
2018. ISSN 0027-8424. doi: 10.1073/pnas.1806579115. URL https://www.pnas.org/
content/115/33/E7665.
PAP Moran. Rank Correlation and Product-Moment Correlation. Biometrika, 35(1):203-206, 1948.
Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,
2(11):2, 2011.
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. Towards
Understanding the Role of Over-Parametrization in Generalization of Neural Networks. arXiv
e-prints, art. arXiv:1805.12076, May 2018.
Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-Dickstein. Bayesian Deep Convolutional Networks with Many
Channels are Gaussian Processes. arXiv e-prints, art. arXiv:1810.05148, October 2018.
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht,
Yoshua Bengio, and Aaron Courville. On the Spectral Bias of Neural Networks. arXiv e-prints, art.
arXiv:1806.08734, Jun 2018.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning
(Adaptive Computation and Machine Learning). The MIT Press, 2005. ISBN 026218253X.
H. Risken and T. Frank. The Fokker-Planck Equation: Methods of Solution and Applications.
Springer Series in Synergetics. Springer Berlin Heidelberg, 1996. ISBN 9783540615309. URL
https://books.google.co.il/books?id=MG2V9vTgSgEC.
Lawrence S Schulman. Techniques and applications of path integration. Courier Corporation, 2012.
Elena Sellentin, Andrew H Jaffe, and Alan F Heavens. On the use of the edgeworth expansion in
cosmology i: how to foresee and evade its pitfalls. arXiv preprint arXiv:1709.03452, 2017.
Peter Sollich and Christopher KI Williams. Understanding gaussian process regression using the
equivalent kernel. In International Workshop on Deterministic and Statistical Methods in Machine
Learning, pp. 211-228. Springer, 2004.
Yee Whye Teh, Alexandre H. Thiery, and Sebastian J. Vollmer. Consistency and fluctuations for
stochastic gradient langevin dynamics. J. Mach. Learn. Res., 17(1):193-225, January 2016. ISSN
1532-4435.
Belinda Tzen and Maxim Raginsky. A mean-field theory of lazy training in two-layer neural nets:
entropic regularization and controlled mckean-vlasov dynamics. arXiv preprint arXiv:2002.01987,
2020.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, ICML’11, pp. 681-688, USA, 2011. Omnipress. ISBN 978-1-4503-0619-5. URL
http://dl.acm.org/citation.cfm?id=3104482.3104568.
Sho Yaida. Non-gaussian processes and neural networks at finite widths. In Mathematical and
Scientific Machine Learning, pp. 165-192. PMLR, 2020.
N. Ye, Z. Zhu, and R. K. Mantiuk. Langevin Dynamics with Continuous Tempering for Training
Deep Neural Networks. ArXiv e-prints, March 2017.
A Zee. Quantum Field Theory in a Nutshell. Nutshell handbook. Princeton Univ. Press, Princeton,
NJ, 2003. URL https://cds.cern.ch/record/706825.
11
Under review as a conference paper at ICLR 2021
A	Edgeworth series
The Central Limit Theorem (CLT) tells us that the distribution of a sum of N independent RVs
will tend to a Gaussian as N → ∞. Its relevancy for wide fully-connected DNNs (or CNNs with
many channels) comes from the fact that every pre-activation averages over N uncorrelated random
variables thereby generating a Gaussian distribution at large N (Cho & Saul, 2009), augmented by
higher order cumulants which decay as 1/N r/2-1, where r is the order of the cumulant. When higher
order cumulants are small, an Edgeworth series (see e.g. Mccullagh (2017); Sellentin et al. (2017))
is a useful practical tool for obtaining the probability distribution from these cumulants. Having
the probability distribution and interpreting its logarithm as our action, places us closer to standard
field-theory formalism.
For simplicity we focus on a 2-layer network, but the derivation generalizes straightforwardly
to networks of any depth. We are interested in the finite N corrections to the prior distribution
Po[f], i.e. the distribution of the DNN output f (x) = PN=I aiφ(wTx), with a% 〜N(0, ςa) and
Wi 〜N(0, ςwI). Because a has zero mean and a variance that scales as 1/N, all odd cumulants are
zero and the 2r’th cumulant scales as 1/N r-1. This holds true for any DNN having a fully-connected
last layer with variance scaling as 1/N. The derivation of the multivariate Edgeworth series can be
found in e.g. Mccullagh (2017); Sellentin et al. (2017), and our case is similar where instead of a
vector-valued RV we have the functional RV f (x), so the cumulants become "functional tensors" i.e.
multivariate functions of the input x. Thus, the leading FWC to the prior P0 [f] is
11
Po [f ] = Ze S [f] 1 + 4!	dμ (xι)…dμ (x4) U (xι,x2,x3,x4) H [f ； x1,x2,x3,x4] +O(1∕N2)
(A.1)
where SGP [f] is as in the main text Eq. 8 and the 4th Hermite functional tensor is
H [f] = ∕dμ (x1)…dμ (x4) K-1 (xi,x；)…KT (x4,x4) f (x1) ∙∙∙ f (x4)
-KT (xɑ, xβ) / dμ (xμ) dμ (XV) K-1 (xμ, xμ)KT (XV, XV)f (xμ)f (XV) [6] (AZ
+ K	(xα, Xe) K	(Xμ, XV) [3]
where by the integers in [∙] we mean all possible combinations of this form, e.g.
K-βK-V = K-2lK34l + K13lK24l + K14lK23l	(A3
H[f] is the functional analogue of the fourth Hermite polynomial: H4 (X) = X4 - 6X2 + 3, which
appears in the scalar Edgeworth series expanded about a standard Gaussian.
B First order correction to posterior mean and variance
B.1	Posterior mean
The posterior mean with the leading FWC action is given by
hf (X*)i = R DfD-fe-f) + O(1∕N 2)	(B.1)
J J
where
1n
S [f]= Sgp [f ]	+ SData [f ]	+ SU [f ]；	SData [f]	=	2σ2 £(f(Xa)—	yα)	(B.2)
α=1
where the O(1∕N 2) implies that we only treat the first order Taylor expansion of S[f], and where
SGP [f], SU[f] are as in the main text Eq. 8. The general strategy is to bring the path integral Df
to the front, so that we will get just correlation functions w.r.t. the Gaussian theory (including
the data term SData[f])(…〉0, namely the well known results (Rasmussen & Williams, 2005) for
12
Under review as a conference paper at ICLR 2021
∕gp(x*) = hf (χ*)io and ∑gp(x*) = <(δf (x*))2)。，and then finally perform the integrals over
input space. Expanding both the numerator and the denominator of Eq. B.1, the leading finite width
correction for the posterior mean reads
fu (x*) = J (/ dμι∕U (x1,x2,x3,x4) hf (x*) H [f]io -hf (x*)io / dμι∕U (x1,x2,x3,x4)(H [f])。
(B.3)
This, as standard in field theory, amounts to omitting all terms corresponding to bubble diagrams,
namely We keep only terms with a factor of (f (x*) f (x、))。and ignore terms with a factor of
hf (x*)i。 , since these will cancel out. This is a standard result in perturbative field theory (see e.g.
Zee (2003)).
We now write down the contributions of the quartic, quadratic and constant terms in H[f]:
1.	For the quartic term in H [f], we have
hf (x*) f (x01) f (x02) f (x03) f (x04))。 - hf (x*))。 hf (x01) f (x02) f (x03) f (x04))。
=ς(x*, Xa) ς (Xe ,xγ)f (xδ)[12]+ς(x*, Xa) /(xβ)/(xγ)f (xδ) [4] (B⑷
We dub these terms by fΣΣ* and fff∑* to be referenced shortly. We mention here that
they are the source of the linear and cubic terms in the target y appearing in Eq. 11 in the
main text.
2.	For the quadratic term in H [f], we have
f (x*)f (xμ)f (XV)〉0- hf ox*))。f (xμ)f (XV))0 = ς (χ*,xμ) /(xν) [2]	(B.5)
we note in passing that these cancel out exactly together with similar but opposite sign
terms/diagrams in the quartic contribution, which is a reflection of measure invariance. This
is elaborated on in §B.3.
3.	For the constant terms in H [f], we will be left only with bubble diagram terms 8
Df f (X* ) which will cancel out in the leading order of 1/N .
B.2	Posterior variance
The posterior variance is given by
∑(X*) = hf (x*) f (x*))-产
=hf (x*) f (x*))o + hf (x*) f (x*))u - fGp - 2fGpfu + O(1∕N2)	(B.6)
=∑gp(x*) + hf (x*) f (x*))u - 2fGpfu + O(1∕N2)
Following similar steps as for the posterior mean, the leading finite width correction for the posterior
second moment at X* reads
hf (X*) f (X*))U
4 (/ dμ1∙∙4U (X1,X2,X3,X4) hf (x*) f (x*) H [f])o -
hf (x*) f (x*))o / dμι∕U (X1,X2,X3,X4) (H [f])o)
(B.7)
As for the posterior mean, the constant terms in H[f] cancel out and the contributions of the quartic
and quadratic terms are
quartic terms = Σ*α∑*βfγ fδ [12] + ∑*a∑*β∑γδ [12]	(B.8)
and
quadratic terms = Σ*μΣ*ν [2]
(B.9)
13
Under review as a conference paper at ICLR 2021
B.3	Measure invariance of the result
The expressions derived above may seem formidable, since they contain many terms and involve
integrals over input space which seemingly depend on the measure μ(x). Here We show how they
may in fact be simplified to the compact expressions in the main text Eq. 11 which involve only
discrete sums over the training set and no integrals, and are thus manifestly measure-invariant.
For simplicity, we show here the derivation for the FWC of the mean fu (x*), and a similar derivation
can be done for ∑u(x*). In the following, we carry out the X integrals, by plugging in the expressions
from Eq. 6 and coupling them to U. As in the main text, we use the Einstein summation notation, i.e.
repeated indices are summed over the training set. The contribution of the quadratic terms is
Aɑι,*KK-ι1βι yβl - Aαιɑ2 KK-' KK彘 2 期 βι Kβ2,*	(B∙10)
where we defined
A (x3, x4)
JJ dμ(xi)dμ(x2)U (X1,X2,X3,X4) K-1 (xι, X2)
(B.11)
Fortunately, this seemingly measure-dependent expression will cancel out with one of the terms
coming from the fΣΣ* contribution of the quartic terms in H [f ]. This is not a coincidence and is a
general feature of the Hermite polynomials appearing in the Edgeworth series, thus for any order in
1/N in the Edgeworth series we will always be left only with measure invariant terms. Collecting all
terms that survive we have
4! {4uα1α2α3 K-1lβ1 K-21β2 K -31β3 yβl yβ2 yβ3 - 12Ua"3K-21β2 K-1lβ1 夕产1 }	(B.12)
where we defined
fτ*	.一 TT*	— TT	充—1 N*
Uα1 α2α3 := Uα1α2α3 - Uα1 α2α3α4Kα4β4Kβ4
This is a more explicit form of the result reported in the main text, Eq. 11.
(B.13)
C Finite width corrections for more than one hidden layer
For simplicity, consider a fully connected network with two hidden layers both of width N , and no
biases, thus the pre-activations h (x) and output z (x) are given by
h (X) = √w2 W⑵φ (σ√1 W(I)X
Z (X) = √σ= aTφ (h⑵(X))
(C.1)
We want to find the 2nd and 4th cumulants of z (X). Recall that we found that the leading order
Edgeworth expansion for the functional distribution of h is
Pk,U [h] 8 e— 2h(x1)K-1(x1，x2)h(x2) (1 + 1LU ㈤,立,x3, x[) H [h; x∖x2,x3, x4])	(C.2)
where K—1 (X01, X02) and U (X01, X02, X03, X04) are known from the previous layer. So we are looking for
two maps:
Kφ(K,U)(X,X0)= hφ (h (X)) φ (h (X0))iPK,U [h]
Uφ (K, U) (X1, X2,X3,X4) = hφ (h (X1)) φ(h (X2)) φ (h (X3)) φ (h (X4))iPK,U[h]
(C.3)
so that the mapping between the first two cumulants K and U of two consequent layers is (assuming
no biases)
K('+1) (x,x0)
σ
σw('+i)
Kφ (K⑹,U(')) (x,x0)
U('+1) (X1,X2,X3,X4)
σ4
σw('+i)
Uφ (K⑶,U(')) (X1,X2,X3,X4)
(C.4)
Xα2)Kφ
Xα4) [3]
14
Under review as a conference paper at ICLR 2021
where the starting point is the first layer (N (0) ≡ d)
2
K⑴(x, x0) = N((CI) X ∙ x0	U⑴(xi, x2,x3, x4) = 0
(C.5)
The important point to note, is that these functional integrals can be reduced to ordinary finite
dimensional integrals. For example, for the second layer, denote
we find for K(2)
h:
h1
h2
K(1)
1) (x1, x1)	K(1) (x1,x2)
1) (x1, x2)	K(1) (x2, x2)
(C.6)
K(2) (x1, x2)
■72-
σw(2)
J dhe-2h K(1)hφ (hi) φ (h2)
(C.7)
and for U (2) we denote
h:
h1
h2
h3
h4
K(1)
KK⑴(xι, xι)
K⑴(X1,X2)
K⑴(X1,X3)
K(1) (x1, x4)
K(1) (x1, x2)
K(1) (x2, x2)
K(1) (x2, x3)
K(1) (x2, x4)
K(1) (x1,x3)
K(1) (x2, x3)
K(1) (x3, x3)
K(1) (x3, x4)
K⑴(xι, X4)、
K⑴(X2, X4)
K⑴(x3,x4)
K(1) (x4, x4)
(C.8)
/
∖
so that
Uφ K(1), U(1) (x1,x2, x3, x4) =
dhe-1h K(1)hφ (hi) φ (h2) φ (h3) φ (h4)
(C.9)
This iterative process can be repeated for an arbitrary number of layers.
D Fourth cumulant for threshold power-law activation
FUNCTIONS
D. 1 Fourth cumulant for ReLU activation function
The U’s appearing in our FWC results can be derived for several activations functions, and in our
numerical experiments we use a quadratic activation φ(z) = z2 and ReLU. Here we give the result
for ReLU, which is similar for any other threshold power law activation (see derivation in App. D.2),
and give the result for quadratic activation in App. E. For simplicity, in this section we focus on
the case of a 2-layer FCN with no biases, input dimension d and N neurons in the hidden layer,
SUch that φα := φ(w(i) ∙ Xa) is the activation at the ith hidden unit with input Xa sampled with a
uniform measure from Sd-ι(√d), where w(i) is a vector of weights of the first layer. This can be
generalized to the more realistic settings of deeper nets and un-normalized inputs, where in the former
the linear kernel L is replaced by the kernel of the layer preceding the output, and the latter amounts
to introducing some scaling factors.
For φ = ReLU, (Cho & Saul, 2009) give a closed form expression for the kernel which corresponds
to the GP. Here we find U corresponding to the leading FWC by first finding the fourth moment of
the hidden layer μ4 := hφ1φ2φ3φ4) (see Eq. 9), taking for simplicity ςW = 1
μ4
_________ ∞
VZdet(L-I) Z dze- 1 ZTLTZz1z2z3z4
(2π)2	C	i 2 3 4
(D.1)
where L-i above corresponds to the matrix inverse of the 4 × 4 matrix with elements Laβ =
(Xa ∙ Xe)/d which is the kernel of the previous layer (the linear kernel in the 2-layer case) evaluated
on two random points. In App. D.2 we follow the derivation in Moran (1948), which yields (with a
slight modification noted therein) the following series in the off-diagonal elements of the matrix L
∞
μ4 =	A'mnpqr L12 L13 Ln4 Lp3 L24 L34
',m,n,p,q,r=0
(D.2)
15
Under review as a conference paper at ICLR 2021
where the coefficients A'mnpqr are
(-)
'+m+n+p+q+r
G'+m+nG'+p+q Gm+p+r Gn+q+r
'∖m∖n∖,p∖q∖r∖
(D.3)
For ReLU activation, these G’s read
ReLU
s
s=0
s=1
s ≥ 3 and odd
s=2k+2 k=0,1,2,...
G
(D.4)
and similar expressions can be derived for other threshold power-law activations of the form φ(z) =
Θ(z)zν. The series Eq. D.2 is expected to converge for sufficiently large input dimension d since
the overlap between random normalized inputs scales as O(1∕√d) and consequently L(x, x0)〜
O(1∕√d) for two random points from the data sets. However, when we sum over 1兀、…α4 we also
have terms with repeating indices and so Lae 's are equal to 1. The above Taylor expansion diverges
whenever the 4 × 4 matrix Lαβ - δαβ has eigenvalues larger than 1. Notably this divergence does
not reflect a true divergence of U, but rather the failure of representing it using the above expansion.
Therefore at large n, one can opt to neglect elements of U with repeating indices, since there are
much fewer of these. Alternatively this can be dealt with by a re-parameterization of the z’s leading
to a similar but slightly more involved Taylor series.
D.2 Derivation of the previous subsection
In this section we derive the expression for the fourth moment hf1f2f3f4i of a two-layer fully
connected network with threshold-power law activations with exponent ν: φ(z) = Θ(z)zν; ν = 0
corresponds to a step function, ν = 1 corresponds to ReLU, ν = 2 corresponds to ReQU (rectified
quadratic unit) and so forth.
When the inputs are normalized to lie on the hypersphere, the matrix L is
L
/ 1	L12	L13	L14
L12	1	L23	L24
L13	L23	1	L34
L14	L24	L34	1
(D.5)
where the off diagonal elements here have Lae = O (l∕√d). We follow the derivation in Ref.
Moran (1948), which computes the probability mass of the positive orthant for a quadrivariate
Gaussian distribution with covariance matrix L:
______ ∞
P= pd2∏P ∕dze-2 ZTLTz
(D.6)
The characteristic function (Fourier transform) of this distribution is
ψ (t1,t2,t3,t4)
= exp (-1 tTLt)
=exp ( - 2 X ta ) exP I - X Laetatβ
a=1	a<e
exp (-1X ta) X
a=1	',m,n,p,q,r=0
'+m+n+p+q+r ' m n p q r
(-)	L12 L13 L14 L23 L24 L34
'∖m∖n∖ιp∖q∖ιr∖
'+m+n '+p+q m+p+r n+q+r
t1	t2	t3	t4
Performing an inverse Fourier transform, we may now write the positive orthant probability as
(D.7)
16
Under review as a conference paper at ICLR 2021
∞
X
',m,n,p,q,r=0
(-)
'+m+n+p+q+r
` mn p q r
L12 L13 L14 L23 L24 L34
'∖m∖n∖,p∖q∖r∖
χ...
×」[dz[ dt ePa = l(-2 Jzata
(2π)	R4+	R4
m+n.'+p+q.m+p+r .n+q+r
t2	t3	t4
(D.8)
∞
X	A'mnpqr L12 Lm Ln4 L23 L24L34
',m,n,p,q,r=0
where the coefficients A'mnpqr are
A'mnpqr
(-)'+m+n+p+q+r G'+m+nG'+p+qGm+p+rGn+q+r
'∖m∖n∖ιp∖q∖ιr∖
(D.9)
and the one dimensional integral is
∞∞
GsV=O) = ； Jdz / ts exp (— ɪt2 — itz) dt	(D.10)
0	-∞
We can evaluate the integral over t to get
G(sν=0)
1
(—i)S (2n)1/2
(D.11)
and performing the integral over z yields
GSfY
I (2k)!
i(2π)1∕22kk!
s=0
s even and s ≥ 2
s = 2k + 1 k = 0, 1, 2, ...
(D.12)
We can now obtain the result for any integer ν by inserting z ν inside the z integral:
∞∞	∞
G(V) = ɪ Z dzzν Z ts exp ——ɪt2 — itz∖ dt =----7- Z zν (-) ^zz /2dz
s	2π J	J pk 2	)	(—i) (2π)1∕2J ∖dz)
0	-∞	0
(D.13)
Using integration by parts we arrive at the result Eq. D.4 reported in the main text
(√2∏
2
0
(-)k(2k)!
√2∏2k k!
s=0
s=1
s ≥ 3 and odd
s = 2k + 2 k = 0, 1, 2, ...
(D.14)
Similar expressions can be derived for other threshold power-law activations of the form φ(z) =
Θ(z)zV for arbitrary integer ν. In a more realistic setting, the inputs x may not be perfectly normalized,
in which case the diagonal elements of L are not unity. It amounts to introducing a scaling factor for
each of the four z’s and makes the expressions a little less neat but poses no real obstacle.
17
Under review as a conference paper at ICLR 2021
E Fourth cumulant for quadratic activation function
For a two-layer network, we may write U, the 4th cumulant of the output f(x) = PiN=1 aiφ(wiTx),
with ai ~ N(0, ς02∕N) and Wi ~ N(0, (ςW/d)I) for a general activation function φ as
ς4
Uα1,α2,α3,α4 = N (%αι ,α2),(α3,α4) + v(α1,α3),(α2,α4) + %α1,α4),(α2,α3))
(E.1)
with
V(α1,α2),(α3,α4)=hφα1φα2φα3φα4iw -hφα1φα2iw hφα3φα4iw	(E.2)
For the case ofa quadratic activation function φ(z) = z2 theV ’s read
V(α1,α2),(α3,α4)	= 2	L11 L33	(L24)2	+ L11L44	(L23)2	+ L22L33	(L14)2	+ L22 L44	(L13)2	+...
4 (L13)2 (L24)2 + (L14)2 (L23)2 +8 (L11L23L34L24 + L22L34L14L13 + L33L12L14L24 + L44 L12 L13 L23)+...
16 (L1 L13 L4 L34 + L1 L14 L3 L34 + L13L14L3L4)	(E.3)
where the linear kernel from the first layer is L(x, x0) = ςwX ∙ x0. Notice that we distinguish between
the scaled and non-scaled variances:
σ2 = ςɑ ；
a N;
厅 2 = ςW
°w = d
(E.4)
These formulae were used when comparing the outputs of the empirical two-layer network with our
FWC theory Eq. 11. One can generalize them straightforwardly to a network with M layers by
recursively computing K(M-1) the kernel in the (M - 1)th layer (see e.g. Cho & Saul (2009)), and
replacing L with K(M-1).
F Auto-correlation time and ergodicity
As mentioned in the main text, the network outputs ∕dnn(x*) are a result of averaging across many
realizations (seeds) of initial conditions and the noisy training dynamics, and across time (epochs)
after the training loss levels off. Our NNSP correspondence relies on the fact that our stochastic
training dynamics are ergodic, namely that averages across time equal ensemble averages. Actually,
for our purposes it suffices that the dynamics are ergodic in the mean, namely that the time-average
estimate of the mean obtained from a single sample realization of the process converges in both the
mean and in the mean-square sense to the ensemble mean:
lim EKfDNN(x*;t)〉T - μ(x*)] =0
T→∞	T
看m E [(〈fDNN(X*;t)〉T -μ(x*))[
(F.1)
0
where μ(χ*) is the ensemble mean on the test point x* and the time-average estimate of the mean
over a time window T is
1 TT	1 tj=T
〈fDNN(x*; t)〉T :=而	f DNN(X*; t)dt ≈ 而 X f DNN(X*; tj)
T 0	T tj=0
(F.2)
This is hard to prove rigorously but we can do a numerical consistency check using the following
procedure: Consider the time series of the network output on the test point X* for the i’th realization
as a row vector and stack these row vectors for all different realizations into a matrix F, such
that Fij = fiDNN(X*; tj). (1) Divide the time series data in the matrix F into non-overlapping
sub-matrices, each of dimension nseeds × nepochs. (2) For each of these sub-matrices, find f(X*)
i.e. the empirical dynamical average across that time window and across the chosen seeds; (2) Find
18
Under review as a conference paper at ICLR 2021
the empirical variance σ2mp(x*) across these f (x*); (4) Repeat (1)-(3) for other combinations of
nepochs, nseeds. If ergodicity holds, we should expect to see the following relation
σemp (X J = σm
τ
nepochs nseeds
(F.3)
where τ is the auto-correlation time of the outputs and σm2 is the macroscopic variance. The results
of this procedure are shown in Fig. F.1, where we plot on a log-log scale the empirical variance σe2mp
vs. the number of epochs nepochs used for time averaging in each set (and using all 500 seeds in this
case). Performing a linear fit on the average across test points (black x’s in the figure) yields a slope
of approximately -1, which is strong evidence for ergodic dynamics.
Figure F.1: Ergodidty check. Empirical variance σ2mp(x*) vs. the number of
epochs used for time averaging on a (base 10) log-log scale, with dt = 0.003 and
N = 200. The colored circles represent different test points x* and the black x's
are averages across these.
G Numerical experiment details
G.1 FCN experiment details
We trained a 2-layer FCN on a quadratic target y(x) = xTAx where the x’s are sampled with a
uniform measure from the hyper-sphere Sd-ι (Vd), with d = 16 and the matrix elements are sampled
as Aij 〜N(0,1) and fixed for all x's. For both activation functions, We used a training noise level
of σ2 = 0.2, training set of size n = 110 and a weight decay of the first layer γw = 0.05. Notice
that for any activation φ, K scales linearly with ς2 = σ2N = (T∕γɑ) ∙ N, thus in order to keep K
constant as we vary N we need to scale the weight decay of the last layer as Ya 〜O (N). This is
done in order to keep the prior distribution in accord with the typical values of the target as N varies,
so that the comparison is fair.
We ran each experiment for 2 ∙ 106 epochs, which includes the time it takes for the training loss to level
off, which is usually on the order of 104 epochs. In the main text we showed GP and FWC results for a
learning rate ofdt = 0.001. Here we report in Fig. G.1 the results using dt ∈ {0.003, 0.001, 0.0005}.
For a learning rate of dt = 0.003 and width N ≥ 1000 the dynamics become unstable and strongly
oscillate, thus the general trend is broken, as seen in the blue markers in Fig. G.1. The dynamics with
the smaller learning rates are stable, and we see that there is a convergence to very similar values up
to an expected statistical error.
19
Under review as a conference paper at ICLR 2021
GP-net & FWC-net MSEs vs width: seed averaged
GSW ⅞⊂.05n- PUe4θu,ds6o-
× ■		(							■
						.	g		
			)3 GP )3 FWC )1 GP )1 FWC )05 GP )05 FWC	-×-		W		I	
	■	∣[	=	U.UL ×	Ir	=	0.0C ・	Ir	=	0.0C ×	Ir	=	0.0C ・	Ir	=	0.0C ×	Ir	=	0.0C 						ɪ			-∙
									×-
1.8	2.0	2.2	2.4	2.6	2.8	3.0	3.2
log(width)
Figure G.1: Regression task with fully connected network: (un-normalized)
MSE vs. width on log-log scale (base 10) for quadratic activation and dif-
ferent leaning rates. The learning rates dt = 0.001, 0.0005 converge to very
similar values (recall this is a log scale), demonstrating that the learning rate is
sufficiently small so that the discrete-time dynamics is a good approximation
of the continuous-time dynamics. For a learning rate of dt = 0.003 (blue) and
width N ≥ 1000 the dynamics become unstable, thus the general trend is broken,
so one cannot take the dt to be too large.
G.2 CNN experiment details and additional settings
The CNN experiment reported in the main text was carried as follows.
Dataset: In the main text Fig. 3 we used a random sample of 10 train-points and 2000 test points
from the CIFAR10 dataset, and in App. H we report results on 1000 train-points and 1000 test points,
balanced in terms of labels. To use MSE loss, the ten categorical labels were one-hot encoded into
vector of zeros and one.
Architecture: we used 6 convolutional layers with ReLU non-linearity, kernel of size 5 × 5, stride
of 1, no-padding, no-pooling. The number of input channels was 3 for the input layer and C for the
subsequent 5 CNN layers. We then vectorized the outputs of the final layer and fed it into an ReLU
activated fully-connected layer with 25C outputs, which were fed into a linear layer with 10 outputs
corresponding to the ten categories. The loss we used was MSE loss.
Training: Training WaS carried using full-batch SGD (GD) at varying learning-rates around 5 ∙ 10-4,
Gaussian white noise was added to the gradients to generate σ2 = 0.2 in the NNGP-correspondence,
layer-dependant weight decay and bias decay which implies a (normalized by width) weight variance
and bias variance of σw2 = 2 and σb2 = 1 respectively, when trained with no-data. During training
we saved, every 1000 epochs, the outputs of the CNN on every test point. We note in passing
that the standard deviation of the test outputs around their training-time-averaged value was about
0.1 per CNN output. Training was carried for around half a million epochs which enabled us to
reach a statistical error of about 2 ∙ 10-4, in estimating the Mean-Squared-Discrepancy between
the training-time-averaged CNN outputs and our NNGP predictions. Notably our best agreement
between the DNN and GP occurred at 112 channels where the MSE was about 7 ∙ 10-3. Notably the
variance of the CNN (the average of its outputs squared) with no data, was about 25.
Statistics. To train our CNN within the regime of the NNSP correspondence, sufficient training time
(namely, epochs) was needed to get estimates of the average outputs fE (Xa) = f(xα) + δfα since
20
Under review as a conference paper at ICLR 2021
the estimators’ fluctuations, δfα , scale as (τ /ttraining)-1/2, where τ is an auto-correlation time scale.
Notably, apart from just random noise when estimating the relative MSE between the averaged CNN
outputs and the GP, a bias term appears equal to the variance of δfɑ averaged over all α's as indeed
ntest	ntest	ntest	ntest
X (fE (Xa)-fGP (xɑ))2 = X (f(Xa)-fGP (Xa ))2-2 X f (Xa)-fGP (Xɑ))δfα+ X (δfɑ)2
α=1	α=1	α=1	α=1
(G.1)
In all our experiments this bias was the dominant source of statistical error. One can estimate it
roughly given the number of uncorrelated samples taken into % (Xa) and correct the estimator. We
did not do so in the main text to make the data analysis more transparent. Since the relative MSEs
go down to 7 ∙ 10-3 and the fluctuations of the outputs quantified by ∑α = (δfa)2 are of the order
0.12, the amount of uncorrelated samples of CNN outputs we require should be much larger than
0.12/(7 ∙ 10-3) ≈ 1.43. To estimate this bias in practice we repeated the experiment with 3-7
different initialization seeds and deduced the bias from the variance of the results. For comparison
with NNGP (our DNN - GP plots) the error bars were proportional to the variance of δfa . For
comparison with the target, we took much larger error bars equal to the uncertainty in estimating the
expected loss from a test set of size 1000. These latter error bars where estimated empirically by
measuring the variance across ten smaller test sets of size 100.
Lastly we discarded the initial “burn-in" epochs, where the network has not yet reached equilibrium.
We took this burn-in time to be the time it takes the train-loss to reach within 5% of its stationary
value at large times. We estimated the stationary values by waiting until the DNNs train loss remained
constant (up to trends much smaller than the fluctuations) for about 5 ∙ 105 epochs. This also coincided
well with having more or less stationary test loss.
Learning rate. To be in the regime of the NNSP correspondence, the learning rate must be taken
small enough such that discrepancy resulting from having discretization correction to the continuum
Langevin dynamics falls well below those coming from finite-width. We find that higher C require
lower learning rates, potentially due to the weight decay term being large at large width. In Fig. G.2.
we report the relative MSE between the NNGP and CNN at learning rates of 0.002, 0.001, 0.0005
and C = 48 showing good convergence already at 0.001. Following this we used learning rates of
0.0005 for C ≤ 48 and 0.00025 for C > 48, in the main figure.
Figure G.2: MSE between our CNN with C = 48 and its NNGP as a function
of three learning rates.
Comparison with the NNGP. Following Novak et al. (2018), we obtained the Kernel of our CNN.
Notably, since we did not have pooling layers this can be done straightforwardly without any
approximations. The NNGP predictions were then obtained in a standard manner (Rasmussen &
Williams, 2005).
H	Further numerical results on CNNs
Here we report two additional numerical results following the CNN experiment we carried (for details
see App. G). Fig. H.3b is the same as Fig. H.3a apart from the fact that we subtracted our estimate of
the statistical bias of our MSE estimator described in App. G.
21
Under review as a conference paper at ICLR 2021
(a) n = 1000	(b) subtract bias of MSE estimator
Figure H.3: CNNs trained on CIFAR10 in the regime of the NNSP corre-
spondence compared with NNGPs MSE test loss normalized by target variance
of a deep CNN (solid green) and its associated NNGP (dashed green) along with
the MSE between the NNGP’s predictions and CNN outputs normalized by the
NNGP’s MSE test loss (solid blue, and on a different scale). We used balanced
training and test sets of size 1000 each. For the largest number of channels we
reached, the slope of the discrepancy between the CNN’s GP and the trained
DNN on the log-log scale was -1.77, placing us close to the perturbartive regime
where a slope of -2.0 is expected. Error bars here reflect statistical errors related
only to output averaging and not due to the random choice of a test-set. The
performance deteriorates at large N = #Channels as the NNSP associated with
the CNN approaches an NNGP.
Concerning the experiment with 10 training points. Here we used the same CNN as in the previous
experiment. The noise level was again the same and led to an effective σ2 = 0.1 for the GP. The
weight decay on the biases was taken to be ten times larger leading to σb2 = 0.1 instead of σb = 1.0
as before. For C ≤ 80 We used a learning rate of dt = 5 ∙ 10-5 after verifying that reducing it further
had no appreciable effect. For C ≤ 80 we used dt = 2.5 ∙ 10-5. For C ≤ 8θ we used 6 ∙ 10+5 training
epochs and We averaged over 4 different initialization seeds. For C > 80 We used betWeen 10 - 16
different initialization seeds. We reduced the aforementioned statistical bias in estimating the MSE
from all our MSEs. This bias, equal to the variance of the averaged outputs, was estimated based on
our different seeds. The error bars equal this estimated variance which was the dominant source of
error.
I	The fourth cumulant can differentiate CNNs from LCNs
Here we show that while the NNGP kernel K of a CNN without pooling cannot distinguish a CNN
from an LCN, the fourth cumulant, U, can. For simplicity let us consider the simplest CNN without
pooling consisting of the following parts: (1) A 1D image with one color/channel (Xi) as input
i ∈ {0, . . . , L - 1}; (2) A single convolutional layer with some activation φ acting with stride 1
and no-padding using the conv-kernel Txc where c ∈ {1, . . . , C} is a channel number index and
x ∈ {0, . . . , 2l} is the relative position in the image. Notably, in an LCN this conv-kernel will
receive an additional dependence on x, the location on Xi on which the kernel acts. (3) A vectorizing
operation taking the C outputs of each convolutional around a point X ∈ {l,...,L - l}, into a
single index y ∈ {0,...,C(L — 2l)}. (4) A linear fully connected layer with weights Wcx where
o ∈ {0, . . . , #outputs} are the output indices.
Consider first the NNGP of such a random DNN with weights chosen according to some iid Gaussian
distribution Po(w), with w including both Wcx and Tx. Denoting by zo(χ) the o,th output of the
CNN, for an input X we have (where we denote in this section(…):=<…)p。(W))
Koo0 (x,x0) ≡ hzo(X)Zo (x0)i = δoo0 X hWxsW°i0ihΦ(Tx(χ)Xχ+z-i)φ(Tx0 (X0)XXS-，)〉
c,x ,X,X'
(I.1)
22
Under review as a conference paper at ICLR 2021
The NNGP kernel of an LCN is the same as that of a CNN. This stems from the fact that hWCXW0 近0 i
yields a Kronecker delta function on the x, X0 indices. Consequently, the difference between LCN
and CNN, which amounts to whether Tx(X) is the same (CNN) or a different (LCN) random variable
than Txo=x(X0), becomes irrelevant as the these two are never averaged together.
For simplicity, we turn to the fourth cumulant of the same output, given by
hzθ(xi) ∙ ∙ ∙ Z0(x4)i-hz0(Xα )zo(xβ )ihzo (XY )zo(xδ )i[3] = hzo(xi) ∙ ∙ ∙ Z0(x4)i-K (Xα, Xe )K (XY, Xδ )[3]
(I.2)
with the second term on the LHS implying all pair-wise averages of zo(X1)..zo(X4). Note that the
first term on the LHS is not directly related to the kernel, thus it has a chance of differentiating a
CNN from an LCN. Explicitly, it reads
X	hWx1X1 …WxIx4ihΦ(Tx1 (Xι)Xχι+xι-ι)…φ(TX4(X4)Xχ4+词-ι)i	(I.3)
Cl ..C4Xl .. x4
The average over the four W,s yields non-zero terms of the type Wcx Wcx WJxO WJxO with either
X = X0 (type 1), X = X0 and C= C (type 2), or X = X0 and C = C (type 3).
The type 1 contribution cannot differentiate an LCN form a CNN since, as in the NNGP case, they
always involve only one X. The type 2 contribution also cannot differentiate since it yields
X	hWxx WxxihWc⅛ Wx0x0ihΦ(Tc(X)XX+x-ι)φ(Tc(X)Xx+x-ι )φ(Tc0 (X0)Xxo+≡o-ι)φ(Tcθ (X,)Xx0+x0-1 )i
c=c0 ;3c=3c0
(I.4)
Examining the average involving the four T,s, one finds that since TC(X) is uncorrelated with Tc (X0)
for both LCNs and CNNs, it splits into
X	hW⅛ WcxihWOxO W√≡0 ihΦ(T=(X)Xx+≡-l)φ(T=(X)Xx+≡-l )ihΦ(Tc0 (X0)XxO+谈-l)φ(Tc0 (X0)Xx，+x，-l)i
c=c0 ;3c=3co
(I.5)
where as in the NNGP, two T's with different X are never averaged together and we only get a
contribution proportional to products of two K,s. We note in passing that these type 2 terms yield a
contribution that largely cancels that of K(Xα, Xβ)K(XY, Xδ)[3], apart from a “diagonal" contribution
(X = X0).
We turn our attention to the type 3 term given by
X hWOxWxXXW^Wcxoih。(Tc(X)XX+x-1)Φ(Tc(X)Xx+x-1 )φ(毒(X0)Xxθ+χo-ι)φ(TcO(X0)Xx，+x，-i)i
c;xx6=xxO
(I.6)
Examining the average involving the four T,s, one now finds a sharp difference between an LCN
and a CnN. For an LCN, this average would split into a product of two K's since Tc(X) would be
uncorrelated with Tac(X0). For a CNN however, Tc(X) is the same random variable as Tc(X0) and
therefore the average does not split giving rise to a distinct contribution that differentiates a CNN
from an LCN. Notably, it is small by a factor of 1/C owing to the fact that it contains a redundant
summation over one C-index while the averages over the four W,s contain a 1/C2 factor when
properly normalized.
J	Corrections to EK
Here we derive finite-N correction to the Equivalent Kernel result. Using the tools developed by
Cohen et al. (2019), the replicated partition function relevant for estimating the predictions of the
network (f (x*)) averaged (〈••• in) over all draws of datasets of size n0 with n0 taken from a Poisson
distribution with mean n is given by
Zn = ʃDfe-SGP[f]-2n2 Rdμx(f(x)-y(x))2(ι + SU[f]) + O(1∕N2)	(J.1)
with SGP [f] and SU [f] given in Eq. 8. We comment that the above expression is only valid for
obtaining the leading order asymptotics in n. Enabling generic n requires introducing replicas
explicitly (see Cohen et al. (2019)). Notably, the above expression coincides with that used for
23
Under review as a conference paper at ICLR 2021
a finite dataset, with two main differences: all the sums over the training set have been replaced
by integrals with respect to the measure, μχ, from which data points are drawn. Furthermore σ2
is now accompanied by n. Following this, all the diagrammatic and combinatorial aspects shown
in the derivation for a finite dataset hold here as well. For instance, let us examine a specific
contribution coming from the quartic term in H [f]: UX1..X4K-IxI …K-x4f(x1)…f (x4),and
from the diagram/Wick-contraction where we take the expectation value of 3 out of the 4 f’s in this
quartic term, to arrive at an expression which is ultimately cubic in the targets y
uχ1,χ2,χ3,χ4 KxIxOhf(XI)i∞κx2X0 hf (X2 )i∞Kx3X0hf (X3) i ∞ K-X0，g(x4, x*)	(Jz
1	1	22	33	44
1
where we recall that hf (X)i∞ = Kxx0 Kx-01x00 y(X00) and Σ∞(X1, X2) = Kx1,x2 -
Kx1,x0 Kx-01,x00 Kx00,x2 being the posterior covariance in the EK limit, where Kxx0f(X0) = Kxx0 f (X0)+
(σ2 /n)f (X). Using the fact that Kx-x10 Kx0x00 gives a delta function w.r.t. the measure, the integrals
against Kx-1x0 can be easily carried out yielding
(U	u U	K-1	K 0 ʌ	K-1	K-1	K-1	(.(T0 )°,(ʃr0	)il(T0 )	(J3)
(ux1,χ2,x3,x* Ux1,χ2,x3,x4Kx4,x4Kx4,x* J Kxι,x1Kx2,x2 Kx3,x3 y (X 1 ) y (X2 ) y (X3 )	(J.3)
Introducing the discrepancy operator δxx" := 6xx〃 - Kxx，K-x〃 = σ2K-1ll, we can write a more
compact expression
(σ2) δx*,x4UxI,x2,x3,x4δx1,x1 δx2,x2δx3,x3y(XI)(X2)y(X3)	(J.4)
This with the additional 1/4! factor times the combinatorial factor of 4 related to choosing the
"partner" of f(X*) in the Wick contraction, yields an overall factor of 1/6 as in the main text, Eq.
14. The other term therein, which is linear in y, is a result of following similar steps with the fΣΣ*
contributions that do not get canceled by the quadratic part in H [f].
24