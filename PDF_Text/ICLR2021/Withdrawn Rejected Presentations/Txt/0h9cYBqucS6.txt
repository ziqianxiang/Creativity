Under review as a conference paper at ICLR 2021
Communication-Computation Efficient
Secure Aggregation for Federated Learning
Anonymous authors
Paper under double-blind review
Ab stract
Federated learning has been spotlighted as a way to train neural network models
using data distributed over multiple clients without a need to share private data.
Unfortunately, however, it has been shown that data privacy could not be fully
guaranteed as adversaries may be able to extract certain information on local data
from the model parameters transmitted during federated learning. A recent solu-
tion based on the secure aggregation primitive enables privacy-preserving feder-
ated learning, but at the expense of significant extra communication/computational
resources. In this paper, we propose communication-computation efficient se-
cure aggregation which reduces the amount of CommUnication/computational re-
sources at least by a factor of ʌ/n/ log n relative to the existing secure solution
without sacrificing data privacy, where n is the number of clients. The key idea
behind the suggested scheme is to design the topology of the secret-sharing nodes
(denoted by the assignment graph G) as sparse random graphs instead of the com-
plete graph corresponding to the existing solution. We first obtain a sufficient
condition on G to guarantee reliable and private federated learning. Afterwards,
We suggest using the ErdOS-Renyi graph as G, and provide theoretical guarantees
on the reliability/privacy of the proposed scheme. Through extensive real-world
experiments, we demonstrate that our scheme, using only 50% of the resources
required in the conventional scheme, maintains virtually the same levels of relia-
bility and data privacy in practical federated learning systems.
1 Introduction
Federated learning (McMahan et al., 2017) has been considered as a promising framework for train-
ing models in a decentralized manner without explicitly sharing the local private data. This frame-
work is especially useful in various predictive models which learn from private distributed data, e.g.,
healthcare services based on medical data distributed over multiple organizations (Brisimi et al.,
2018; Xu & Wang, 2019) and text prediction based on the messages of distributed clients (Yang
et al., 2018; Ramaswamy et al., 2019). In the federated learning (FL) setup, each device contributes
to the global model update by transmitting its local model only; the private data is not shared across
the network, which makes FL highly attractive (Kairouz et al., 2019; Yang et al., 2019).
Unfortunately, however, FL could still be vulnerable against the adversarial attacks on the data
leakage. Specifically, the local model transmitted from a device contains extensive information on
the training data, and an eavesdropper can estimate the data owned by the target device (Fredrikson
et al., 2015; Shokri et al., 2017; Melis et al., 2019). Motivated by this issue, the authors of (Bonawitz
et al., 2017) suggested secure aggregation (SA), which integrates cryptographic primitives into the
FL framework to protect data privacy. However, SA requires significant amounts of additional re-
sources on communication and computing for guaranteeing privacy. Especially, the communication
and computation burden of SA increases as a quadratic function of the number of clients, which
limits the scalability of SA.
Contributions We propose communication-computation efficient secure aggregation (CCESA),
which maintains the reliability and data privacy in federated learning, with reduced resources on
communication and computation compared to conventional SA. Our basic idea is illustrated in Fig. 1
1
Under review as a conference paper at ICLR 2021
Figure 1: Conventional secure aggregation (SA) (Bonawitz et al., 2017) versus the suggested communication-
computation efficient secure aggregation (CCESA). Via selective secret sharing across only a subset of client
pairs, the proposed algorithm reduces the communication cost (for exchanging public keys and secret shares
among clients) and computational cost (for generating secret shares and pseudo-random values, and performing
key agreements), compared to the existing fully-shared method. CCESA still maintains virtually the same levels
of reliability and privacy, as proven by the theoretic analysis of Section 4.
1
f)ι = θι+ PRGSI)
+ PRG&3)+PRG(S1,4)
(b) Suggested algorithm (CCESA)
Training Set image	(α)	(b')	(C)
Figure 2: A training image and the reconstructed images using model inversion attacks with (a) the proposed
scheme (CCESA), (b) existing secure aggregation (SA) (Bonawitz et al., 2017), (C) federated averaging with no
security measures (McMahan et al., 2017). The federated averaging scheme leaks private data from the trans-
mitted model, while SA and the proposed CCESA do not. Note that the required communication/computational
resources of CCESA are only 40% of those of SA. Additional examples are given in Supplementary Materials.
for n = 5 clients. Compared to the existing scheme (Bonawitz et al., 2017), which applies secret
sharing for all client pairs, we suggest sharing secrets for a subset of pairs in a way that data privacy
is preserved. Using theoretic analysis, we provide a sufficient condition on the graph topology for
private and reliable federated learning. As summarized in Table 1, the proposed CCESA algorithm
maintains both reliability and privacy against an eavesdropper who can access to any information
transmitted between every client and the server, while the required amount of resources reduces
by a factor of at least O(ʌ/n/ log n) compared to SA. Notably, the reduction factor gets bigger
with increasing n, suggesting that the proposed scheme is a scalable solution for privacy-preserving
federated learning. Our mathematical results are also confirmed in experiments on two real datasets
of AT&T face database and CIFAR-10. Especially, under the model inversion attack on the face
dataset, the results in Fig. 2 show that the suggested scheme achieves perfect data privacy by using
less amount of resources than SA, while federated averaging without security measures (McMahan
et al., 2017) significantly compromises the privacy of the data.
Related work Focusing on the collaborative learning setup with multiple clients and a server,
previous works have suggested solutions to prevent the information leakage in the communication
links between the server and clients. One major approach utilizes the concept of differential privacy
(DP) (Dwork et al., 2014) by adding artificial random noise to the transmitted models (Wei et al.,
2020; Geyer et al., 2017; Truex et al., 2020) or gradients (Shokri & Shmatikov, 2015; Abadi et al.,
2016; Balcan et al., 2012). Depending on the noise distribution, DP-based collaborative learning
generally exhibits a trade-off between the privacy level and the convergence of the global model.
Another popular approach is deploying secure multiparty computation (MPC) (Ben-Or et al., 1988;
Damgard et al., 2012; Aono et al., 2017; Lindell et al., 2015; Bonawitz et al., 2017; Zhang et al.,
2018; Tjell & Wisniewski, 2019; Shen et al., 2020; So et al., 2020) based on the cryptographic
primitives including secret sharing and the homomorphic encryption (Leontiadis et al., 2014; 2015;
Shi et al., 2011; Halevi et al., 2011). Although these schemes guarantee privacy, they suffer from
high communication burdens while reconstructing the secret information distributed over multiple
clients. A notable work (Bonawitz et al., 2017) suggested secure aggregation (SA), which tolerates
multiple client failures by applying pairwise additive masking (Acs & Castelluccia, 2011; Elahi
et al., 2014; Jansen & Johnson, 2016; Goryczka & Xiong, 2015). A recent work (So et al., 2020)
suggested Turbo-aggregate, which partitions n computing devices into L groups and updates the
global model by utilizing the circular aggregation topology. However, each client in Turbo-aggregate
requires a communication cost of at least 4mnR∕L bits, which is much larger than that of our
scheme (CCESA) requiring a communication cost of √n log n(2aκ + 5as) + mR1. For example,
1Here, m is the number of model parameters where each parameter is represented in R bits. aK and aS are
the number of bits required for exchanging public keys and the number of bits in a secret share, respectively.
2
Under review as a conference paper at ICLR 2021
			CCESA		SA	FedAvg
Communication cost	Client Server	O(7n log n + m) θ(nʌ/n log n + mn)	O(n + m) O(n2 + mn)	O(m) O(mn)
Computation cost	Client Server	O(n log n + mʌ/n log n) O(mn log n)	O(n2 + mn) O(mn2)	0 O(mn)
Reliability		≥ 1 - O(ne-√nlog n)一	≥ 1 - O(ne-n)	1
Privacy		≥ 1 - O(n-C),for C > 0	1	0
Table 1: Communication and computation cost of the proposed CCESA algorithm, Secure Aggregation (SA)
in (Bonawitz et al., 2017), federated averaging (FedAvg) in (McMahan et al., 2017). Detailed expressions and
derivations are in Section 4 and Supplementary Materials, respectively.
in a practical scenario with m = 106, R = 32, n = 100, L = 10 and aK = aS = 256, our scheme
requires only 3% of the communication bandwidth used in Turbo-aggregate.
The idea of replacing the complete graph with a low-degree graph for communication efficiency
has been studied in the areas of distributed learning (Charles et al., 2017; Sohn et al., 2020) and
multi-party computation (Fitzi et al., 2007; Harnik et al., 2007). A very recent paper (Bell et al.,
2020) proposed new protocols using k-regular graphs for communication in the secure aggrega-
tion (Bonawitz et al., 2017) framework, which are robust against semi-honest and semi-malicious
threat models, respectively. However, the results in (Bell et al., 2020) is based on a strong assump-
tion on the number of clients dropped out of the protocol, while our work does not assume anything
on the number of dropouts.
2 Background
Federated learning Consider a scenario with one server and n clients. Each client i has its local
training dataset Di = {(xi,k,yi,k)}kN=i1 where xi,k and yi,k are the feature vector and the label of
the k-th training sample, respectively. For each round t, the server first selects a set St of cn clients
(0 < c ≤ 1) and sends the current global model θg(lto)bal to those selected clients. Then, each client
i ∈ St updates the received model by using the local data Di and sends the updated model θi(t+1)
to the server. Finally, the server updates the global model by aggregating local updates from the
selected clients, i.e., θgt+1ι) J Pi∈St NNθ(t+1) where N = Pi∈St Ni.
Cryptographic primitives for preserving privacy Here we review three cryptographic tools used
in SA (Bonawitz et al., 2017). First, t-out-of-n secret sharing (Shamir, 1979) is splitting a secret s
into n shares, in a way that any t shares can reconstruct s, while any t - 1 shares provides absolutely
no information on s. We denote t-out-of-n secret sharing by s -(-t,-n→) (sk)k∈[n] , where sk indicates
the kth share of secret S and [n] represents the index set {1, 2, •一,n}. Second, the Diffie-Hellman
key agreement is used to generate a secret si,j that is only shared by two target clients i, j ∈ [n]. The
key agreement scheme designs public-private key pairs (suPK, suSK) for clients u ∈ [n] in a way that
si,j = f (siPK, sjSK) = f (sjPK, siSK) holds for all i, j ∈ [n] for some key agreement function f.
The secret si,j is unknown when neither siSK nor sjSK is provided. Third, symmetric authenticated
encryption is encrypting/decrypting message m using a key k shared by two target clients. This
guarantees the integrity of the messages communicated by the two clients.
Secure aggregation (Bonawitz et al., 2017) For privacy-preserving federated learning, SA has
been proposed based on the cryptographic primitives of Shamir’s secret sharing, key agreement and
symmetric authenticated encryption. The protocol consists of four steps: Step 0 Advertise Keys,
Step 1 Share Keys, Step 2 Masked Input Collection, and Step 3 Unmasking.
Consider a server with n clients where client i ∈ [n] has its private local model θi . Denote the client
index set as V0 = V = [n]. The objective of the server is to obtain the sum of models Pi θi without
getting any other information on private local models. In Step 0, client i ∈ V0 generates key pairs
(siPK, siSK) and (ciP K , ciSK) by using a key agreement scheme. Then, client i advertises its public
keys (siPK, ciPK) to the server. The server collects the public keys from a client set V1 ⊂ V0, and
broadcasts {(i, siPK, ciP K)}i∈V1 to all clients in V1 . In Step 1, client i generates a random element
bi and applies t-out-of-n secret sharing to generate n shares of bi and siSK, i.e., bi
and siSK
-(-t,-n→) (siS,jK)j∈[n]. By using the symmetric authenticated encryption, client i computes the
3
Under review as a conference paper at ICLR 2021
ciphertext ei,j for all j ∈ V1\{i}, by taking bi,j and siS,jK as messages and ci,j = f(cjPK, ciSK)
as a key, where f is the key agreement function. Finally, client i sends {(i, j, ei,j)}j∈v'ι∖{i} to the
server. The server collects the message from at least t clients (denote this set of client as V2 ⊂ V1)
and sends {(i, j, ei,j)}i∈V2 to each client j ∈ V2. In Step 2, client i computes the shared secret
si,j = f(sjPK,siSK) for all j ∈ V2\{i}. Then, client i computes the masked private vector
θi = θi + PRG(bi)+	X PRG(Sij) - X PRG(Sij),	⑴
j∈V2 ;i<j	j∈V2 ;i>j
and sends 瓦 to the server, where PRG(x) indicates a pseudorandom generator with seed X out-
putting a vector having the dimension identical to %. Note that the masked vector θi gives no
information on private vector θi unless both SSK and b are revealed. The server collects θi from at
least t clients (denote this set as V3 ⊂ V2), and sends V3 to each client i ∈ V3 . In Step 3, client j
decrypts the ciphertext {ei,j }i∈v2∖{j} by using the key ci,j = f(cPK, CSK) to obtain {bi,j }i∈v2∖{j}
and {sSK}i∈V2∖{j}. Each client j sends a set of shares {bi,j}i∈V3 ∪ {s^K}i∈V2∖V3 to the server.
The server collects the responds from at least t clients (denote this set of clients as V4 ⊂ V3). For
each client i ∈ V3, the server reconstructs bi from {bi,j}i∈V4 and computes PRG(bi). Similarly, for
each client i ∈ V2\V3, the server reconstructs siSK from {siS,jK}i∈V4 and computes PRG(si,j ) for
all j ∈ V2. Using this information, the server obtains the sum of private local models by computing
X θi = X θi - X PRG(bi) -	X	PRG(Sij) + X	PRG(Sij).⑵
i∈V3	i∈V3	i∈V3	j∈V2 ,i∈V2 \V3 ;i<j	j∈V2 ,i∈V2 \V3 ;i>j
3 Suggested algorithm
In the secure aggregation (Bonawitz et al., 2017), the public keys (ciPK, siPK) and the shares of
secrets (siSK, bi) are transmitted between clients and the server, which requires additional commu-
nication/computational resources compared with the vanilla federated learning. Specifically, since
each client i needs to receive information from all other clients j 6= i, the required amount of
resources increases as a quadratic function of the number of clients n.
In this paper, we suggest a variant of secure aggregation, dubbed as communication-computation ef-
ficient secure aggregation (CCESA), which enables to provide a more scalable solution for privacy-
preserving federated learning by improving the communication/computational efficiency. The basic
idea of the proposed algorithm is to allow each client to share its public keys and secret shares to
a subset of other clients, instead of sharing them with all other clients. By doing so, compared
with SA, the suggested scheme achieves two advantages in resource efficiency, without losing the
reliability of learning algorithms and the data privacy. The first advantage is the reduction of the
communication cost, since each node shares its public keys and secrets with less clients. The second
advantage is a reduction of the computational cost of each client, since a smaller number of masks
are used while computing its masked private vector.
The proposed algorithm is specified by the assignment graph which represents how public keys
and secret shares are assigned to the other clients. Given n clients, the assignment graph G =
(V, E) consists of n vertices where the vertex and the edge set of G are represented by V and E,
respectively. We set V = [n] where each index i ∈ V represents client i, and the edge {i, j} ∈ E
connecting vertices i and j indicates that client i and j exchange their public keys and secret shares.
For vertex i ∈ [n], we define Adj(i) := j; {i, j} ∈ E as the index set of vertices adjacent to
vertex i. In our algorithm, public keys and secrets of client i are shared with clients j ∈ Adj(i).
Now, using the assignment graph notation, we formally define the suggested algorithm. Due to the
space limitation, we put the algorithm in Supplementary Materials A; here we only describe what
differs from SA. In Step 0, instead of broadcasting the public keys (cjPK, sjPK) for client j to all
other clients, the server sends the public keys only to the client i satisfying j ∈ Adj(i) ∩ V1. In
Step 1, each client i ∈ V1 uses ti-out-of-(|Adj(i)| + 1) secret sharing scheme to generate shares
of SSK and " i.e., SSK -ti,1Aj(i)l+1→ (SSK)共4的《”{讣 and b (ti,1Atj(i)l + → (bi,j)j∈Adj(i)∪{i}
and sends the encrypted SiS,jK and bi,j to client j through the server. In Step 2, client i computes the
masked private model
θi = θi + PRG(bi) + E PRG(si,j) - E	PRG(si,j),
j∈V2∩Adj(i);i<j	j∈V2∩Adj(i)[i>j
(3)
4
Under review as a conference paper at ICLR 2021
and transmits θi to the server. In Step 3, client i sends bj,i to the server for all j ∈ V3 ∩ Adj(i), and
sends sjS,Ki to the server for all j ∈ (V2\V3) ∩ Adj(i). After reconstructing secrets from shares, the
server obtains the sum of the local models θi as
X θi = X θi - X PRG(bi) - X PRG(Sij) +	X PRG(Sij).	(4)
i∈V3	i∈V3	i∈V3	i∈V2∖V3 ,j∈Adj(i)∩V⅛;i>j i∈V2∖V3 ,j∈Adj(i)∩V⅛[i<j
Note that the suggested protocol with n-complete assignment graph G reduces to SA.
Here we define several notations representing the evolution of the assignment graph G as some of
the nodes may drop out of the system in each step. Recall that V0 = V and Vi+1 is defined as
the set of survived nodes in Step i ∈ {0,…，3}. Let us define Gi as the induced subgraph of G
whose vertex set is Vi, i.e., Gi := G - (V\Vi). Then, Gi+1 represents how the nodes survived
until Step i are connected. We define the evolution of assignment graph during the protocol as
G =(GO, G1, ∙∙∙ , G4).
In Fig. 1, we illustrate an example of the suggested algorithm with n = 5 clients. Fig. 1a corresponds
to SA (Bonawitz et al., 2017), while Fig. 1b depicts the proposed scheme. Here, we focus on the
required communication/computational resources of client 1. Note that each client exchanges public
keys and secret shares with its adjacent clients. For example, client 1 exchanges the data with four
other clients in the conventional scheme, while client 1 exchanges the data with clients 3 and 5 in the
suggested scheme. Thus, the proposed CCESA requires only half of the bandwidth compared to the
conventional scheme. In addition, CCESA requires less computational resources than conventional
scheme, since each client generates less secret shares and pseudo-random values, and performs less
key agreements.
4	Theoretical analysis
4.1	Performance metrics
The proposed CCESA algorithm aims at developing private, reliable and resource-efficient solutions
for federated learning. Here, we define key performance metrics for federated learning systems
including federated averaging (McMahan et al., 2017), SA (Bonawitz et al., 2017) and CCESA.
Recall that the server receives (masked) model parameters θi from clients i ∈ V3 , and wants to
update the global model as the sum of the unmasked model parameters, i.e., θglobal J Pi∈v⅛ θi∙
The condition for successful (or reliable) global model update is stated as follows.
Definition 1. A system is called reliable if the server successfully obtains the sum of the model
parameters	i∈V θi aggregated over the distributed clients.
Now, we define private federated learning. In our analysis, we focus on a passive eavesdropper
who can access to any information transmitted between any client and the server throughout running
CCESA algorithm, namely, public keys of clients, secret shares, masked local models and the indices
of survived clients V3 2. Once an eavesdropper gets the local model θi of client i, it can reconstruct
the private data (e.g., face image or medical records) of the client or can identify the client which
contains the target data. In general, if an eavesdropper obtains the sum of local models occupied by
a subset T of clients, a similar privacy attack is possible for the subset of clients. Thus, to preserve
the data privacy, it is safe to protect the information on the partial sum of model parameters against
the eavesdropper; we formalize this.
Definition 2. A system is called private if H (E记7 3。= H (E记7 θi |E) holdsfor all T satisfying
T ⊂ V3 and T ∈ {0,V3}. Here, H is the entropy function, and E is the information accessible to
the eavesdropper.
When both reliability and privacy conditions hold, the server successfully updates the global model,
while an eavesdropper cannot extract data in the information-theoretic sense. We define Pe(r) and
Pe(p) as the probabilities that reliability and privacy conditions do not hold, respectively.
2Our threat model is equivalent to the “server-only” honest-but-curious adversary, which is weaker than
“client-server collusion” adversary considered in secure aggregation (Bonawitz et al., 2017).
5
Under review as a conference paper at ICLR 2021
4.2	RESULTS FOR GENERAL ASSIGNMENT GRAPH G
Recall that our proposed scheme is specified by the assignment graph G. We here provide mathe-
matical analysis on the performance metrics of reliability and privacy, in terms of the graph G. To
be specific, the theorems below provide the necessary and sufficient conditions on the assignment
graph G to enable reliable/private federated learning, where the reliability and privacy are defined
in Definitions 1 and 2. Before going into the details, we first define informative nodes as below.
Definition 3. A node i ∈ V0 is informative if |(Adj(i) ∪ {i}) ∩ V4| ≥ ti holds.
Note that node i is called informative when the server can reconstruct the secrets (bi or siSK) of node
i in Step 3 of the algorithm. Using this definition, we state the condition on graph G for enabling
reliable systems as below.
Theorem 1.	The system is reliable if and only if node i in informative for all i ∈ V3+, where
V3+ = V3 ∪ {i ∈ V2 : Adj(i) ∩ V3 = 0} is the union of V3 and the neighborhoods of V3 within V2.
Proof. The full proof is given in Supplementary Materials; here we provide a sketch for the proof.
Recall that the server receives the sum of masked models i∈V θi, while the system is said to be
reliable if the server obtains the sum of unmasked models Pi∈V θi. Thus, the reliability condition
holds if and only if the server can cancel out the random terms in (4), which is possible when either
siSK or bi is recovered for all i ∈ V3+ . Since a secret is recovered if and only if at least ti shares are
gathered from adjacent nodes, We need ∣(Adj(i) ∪{i}) ∩ V4| ≥ t, which completes the proof. □
Now, before moving on to the next theorem, we define some sets of graph evolutions as below:
Gc = {G	=	(Go,	Gι,…，G4)	:	G3 is connected },
Gd = {G	=	(Go,	Gι,…，G4)	:	G3 is not connected },
GNI = {G ∈ GD : ∀l ∈ [κ], ∃i ∈ Cl+ such that node i is not informative}.
Here, when G3 is a disconnected graph with κ ≥ 2 components, Cl is defined as the vertex set of
the lth component, and Cl+ := Cl ∪ {i ∈ V2 : Adj(i) ∩ Cl 6= 0}. Using this definition, we state a
sufficient condition on the assignment graph to enable private federated learning.
Lemma 1. The system is private ifG ∈ GC.
Proof. Again we just provide a sketch of the proof here; the full proof is in Supplementary Mate-
rials. Note that G3 is the induced subgraph of G whose vertex set is V3 . Suppose an eavesdropper
has access to the masked local models {θi}i∈T of a subset T ⊂ V3 of nodes. Now, the question
is whether this eavesdropper can recover the sum of the unmasked models Pi∈T θi. If G3 is con-
nected, there exists an edge e = {p, q} such that P ∈ T and q ∈ V3\T. Note that Pi∈τ θi contains
the PRG(sp,q) term, while sp,q is not accessible by the eavesdropper since p, q ∈ V3. Thus, from
(4), the eavesdropper cannot obtain Pi∈τ θi, which completes the proof.	□
Based on the Lemma above, we state the necessary and sufficient condition for private system as
below, the proof of which is given in the Supplementary Materials.
Theorem 2.	The system is private if and only if G ∈ GC ∪ GNI.
The theorems above provide guidelines on how to construct the assignment graph G to enable re-
liable and private federated learning. These guidelines can be further specified when we use the
ErdOS-Renyi graph as the assignment graph G. In the next section, we explore how the ErdOS-Renyi
graph can be used for reliable and private federated learning.
4.3 Results for ERDOS-RENYI assignment graph G
The ErdOS-Renyi graph G ∈ G(n,p) is a random graph of n nodes where each edge connecting
two arbitrary nodes is connected with probability p. Define CCEsA(n, p) as the proposed scheme
using the assignment graph of G ∈ G(n, p). According to the analysis provided in this section,
CCEsA(n, p) almost surely achieves both reliability and privacy conditions, provided that the con-
nection probability p is chosen appropriately. Throughout the analysis below, we assume that each
client independently drops out with probability q at each step (from step 0 to step 3), and the secret
sharing parameter ti is set to t for all i ∈ [n].
6
Under review as a conference paper at ICLR 2021
4.3.1	FOR ASYMPTOTICALLY LARGE n
We start with the analysis on CCESA(n, p) when n is asymptotically large. The following two
theorems provide lower bounds on p to satisfy reliability/privacy conditions. The proofs are provided
in Supplementary Materials.
Theorem 3.	CCESA(n,p) is asymptotically almost surely reliable if P > 3^(2-(211-(^4-1)1.
Theorem 4.	CCESA(n, p) is asymptotically almost surely private ifp >
dn(1-q)3-√n log n ∣
From these theorems, the condition for achieving both reliability and privacy is obtained as follows.
Remark 1. Let
?二	Jog(「n(1 - q)3 - √n logn]) 3√(n - I)IOgCn - I) - 1 .
P =max{mi-斤二√nθgn∏, (n - 1)(2(1 - q)4 - 1) }.
(5)
Ifp > p?, then CCESA(n, p) is asymptotically almost surely (a.a.s.) reliable and private. Note that
the threshold connection probability p? is a decreasing function of n. Thus, the proposed algorithm
is getting more resource efficient than SA as n grows, improving the scalability of the system.
In the remarks below, we compare SA and the proposed CCESA, in terms of the required amount
of communication/computational resources to achieve both reliability and privacy. These results are
summarized in Table 1.
Remark 2. Let B be the amount of additional communication bandwidth used at each client, com-
pared to that of federated averaging (McMahan et al., 2017). Since the bandwidth is proportional
to np, we have BCCESA(n,p)〜O('n log n) and BSA 〜O(n). Thus, the suggested CCESA pro-
tocol utilizes a much smaller bandwidth compared to SA in (Bonawitz et al., 2017). The detailed
comparison is given in Section D.1 of the Supplementary Materials.
Remark 3. Compared to SA, the proposed CCESA algorithm generates a smaller number of secret
shares and pseudo-random values, and performs less key agreements. Thus, the computational
burden at the server and the clients reduces by a factor of at least O(，n/ log n). The detailed
comparison is given in Section D.2 of the Supplementary Materials.
4.3.2 FOR FINITE n
We now discuss the performance of the suggested scheme for finite n. Let Pe(p) be the error prob-
ability that CCESA(n, p) does not satisfy the the privacy condition, and define Pe(p) as the error
probability that CCESA(n, p) is not reliable. Below we provide upper bounds on Pe(p) and Pe(r).
Theorem 5.	For arbitrary n, p, q and t, the error probability for reliability Pe(r) is bounded by
Per) ≤ ne-(n-1)DκL(n-1 llp(1-q) ), where DKL is the Kullback-Leibler (KL) divergence.
Theorem 6.	For arbitrary n, p and q, the error probability for privacy Pe(p) is bounded by
n	bm/2c
Pe(p) ≤ X mn (1 - q)3m(1 - (1 - q)3)(n-m) X	mk (1 - p)k(m-k).
m=0	k=1
Fig. 3 illustrates the upper bounds on Pe(p) and Pe(r) obtained in Theorems 5 and 6, when p = p? .
Here, qtotal := 1 - (1 -q)4 is defined as the dropout probability of the entire protocol (from Step 0 to
Step 3). Note that the upper bounds in Theorems 5 and 6 are decreasing functions of p. Therefore,
the plotted values in Fig. 3 are indeed upper bounds on the error probabilities for arbitrary p > p? .
It is shown that a system with the suggested algorithm is private and reliable with high probability
for an arbitrary chosen p > p?. The error probability for the privacy Pe(p) is below 10-40, which
is negligible even for small n. The error probability for the reliability Pe(r) is below 10-2, which
means that in at most one round out of 100 federated learning rounds, the (masked) models {θi}i∈V3
received by the server cannot be converted to the sum of (unmasked) local models Pi∈V θi. Even
in this round when the server cannot obtain the sum of (unmasked) local models, the server is aware
of the fact that the current round is not reliable, and may maintain the global model used in the
previous round. This does not harm the accuracy of our scheme, as shown in the experimental
results of Section 5.
7
Under review as a conference paper at ICLR 2021
Figure 3: Upper bounds on the error probabilities Pe(r) and Pe(p) in Theorems 5 and 6 for p = p?, where p? is
the threshold connection probability for achieving both reliability and privacy as in (5). Note that for arbitrary
p > p? , the error probabilities are lower than the upper bounds marked in the figure. One can confirm that the
suggested CCESA algorithm is private and reliable with a high probability, provided that p > p? .
	n	qtotal	t	p	Client				Server
					Step 0	Step 1	Step 2	Step 3	
	100	0	51	1	6	3572	3537	8^	13-
	100	0.1	51	1	6	3540	3365	80	9847
SA	300	0	151	1	6	11044	10867	269	82
	300	0.1	151	1	6	10502	10453	255	72847
	500	0	251	1	6	19097	18196	449	198
	500	0.1	251	1	6	18107	17315	432	329645
	100	0	43	0.6362	6	2216	2171	56-	16-
	100	0.1	51	0.7953	6	2715	2648	66	8067
CCESA	300	0	83	0.4109	6	4435	4354	110	54
	300	0.1	98	0.5136	6	5382	5300	113	37082
	500	0	112	0.3327	6	5954	5846	145	132
	500	0.1	133	0.4159	6	7634	7403	184	141511
Table 2: Running time (unit: ms) of SA (Bonawitz et al., 2017) and suggested CCESA
5 Experiments
Here we provide experimental results on the proposed CCESA algorithm. We compare CCESA
and secure aggregation (SA) of (Bonawitz et al., 2017) in terms of time complexity (running time),
reliability, and privacy. We tested both schemes on two real datasets, AT&T Laboratories Cambridge
database of faces (https://www.kaggle.com/kasikrit/att-database-of-faces) and CIFAR-10. For the
AT&T face dataset containing images of 40 individuals, we considered a federated learning setup
where each of n = 40 clients uses its own images for local training. All algorithms are implemented
in python and PyTorch (Paszke et al., 2017). Codes will be made available to the public.
5.1	Running time
In Table 2, we tested the running time of our CCESA and existing SA for various n and qtotal.
Similar to the setup used in (Bonawitz et al., 2017), we assumed that each node has its local model
θ with dimension m = 10000, where each element of the model is chosen from the field F216 .
Here, t is selected by following the guideline in Supplementary Materials, and P is chosen as p?〜
O(ʌ/logn/n) defined in (5) which is proven to meet both reliability and privacy conditions. For
every n, qtotal setup, the proposed CCESA(n, p) requires p times less running time compared with the
conventional SA of (Bonawitz et al., 2017). This is because each client generates p times less number
of secret shares and pseudo-random values, and performs p times less number of key agreements.
This result is consistent with our analysis on the computational complexity in Table 1.
5.2	Reliability
Recall that a system is reliable if the server obtains the sum of the local models i∈V θi. Fig. 4
shows the reliability of CCESA in CIFAR-10 dataset. We plotted the test accuracies of SA and the
suggested CCESA(n, p) for various p. Here, we included the result when p = p? = 0.3106, where
p? is the provably minimum connection probability for achieving both the reliability and privacy
according to Remark 1. One can confirm that CCESA with p = p? achieves the performance of
SA in both i.i.d. and non-i.i.d. data settings, coinciding with our theoretical result in Theorem 3.
Moreover, in both settings, selecting p = 0.25 is sufficient to achieve the test accuracy performance
of SA when the system is trained for 200 rounds. Thus, the required communication/computational
resources for guaranteeing the reliability, which is proportional to np, can be reduced to 50% of the
conventional wisdom in federated learning. Similar behaviors are observed from the experiments on
AT&T Face dataset, as in Fig. B.1 of the Supplementary Materials.
5.3	Privacy
We first consider a privacy threat called model inversion attack (Fredrikson et al., 2015). The basic
setup is as follows: the attacker eavesdrops the masked model θi sent from client i to the server, and
reconstructs the face image ofa target client. Under this setting, we compared how the eavesdropped
model reveals the information on the raw data for various schemes. As in Fig. 2 and Fig. B.2 in the
8
Under review as a conference paper at ICLR 2021
SA -p=0.3106	p=0.25	p=0.2 — p=0.15
O 20	40	60	80	100	120	140	160	180 200
Federated Learning Round
Uooo
0 6 4 2
O 20	40	60	80	100	120	140	160	180	200
Federated Learning Round
(a) i.i.d. data across clients	(b) non-i.i.d. data across clients
Figure 4: Test accuracies of SA versus proposed CCESA(n, p) with various connection probability p, for
federated learning using CIFAR-10 dataset. Here, we set n = 1000 and qtotal = 0.1. The suggested CCESA
achieves the ideal test accuracy by using only 75% of the communication/computational resources used in the
conventional SA. This shows the reliability of CCESA in real-world scenarios.
Schemes \ Number of training data (ntrain)	∣	5000 I	10000 I	15000 I	50000
Federated Averaging (McMahan et al., 2017)	72.49%	70.72%	72.80%	66.47%
Secure Aggregation (SA) (BonaWitZ et al., 2017)	49.67%	49.96%	49.85%	49.33%
CCESA (Suggested)	49.29%	50.14%	49.02%	50.00%
Table 3: Accuracy of the membership inference attack on local models trained on CIFAR-10. The scheme with
a higher attack accuracy is more vulnerable to the inference attack. In order to maximize the uncertainty of
the membership inference, the test set for the attack model consists of 5000 members (training data points)
and 5000 non-members (evaluation data points). For the proposed CCESA, the attacker is no better than the
random guess with accuracy = 50%, showing the privacy-preserving ability of CCESA.
Supplementary Materials, the vanilla federated averaging (McMahan et al., 2017) with no privacy-
preserving techniques reveals the characteristics of individual’s face, compromising the privacy of
clients. On the other hand, both SA and CCESA do not allow any clue on the client’s face; these
schemes are resilient to the model inversion attack and preserve the privacy of clients. This observa-
tion is consistent with our theoretical results that the proposed CCESA guarantees data privacy. We
also considered another privacy threat called the membership inference attack (Shokri et al., 2017).
Here, the attacker eavesdrops the masked model θi and guesses whether a target data is a member of
the training dataset. Table 3 summarizes the accuracy of the inference attack for CIFAR-10 dataset,
under the federated learning setup where ntrain training data is equally distributed into n = 10 clients.
The attack accuracy reaches near 70% for federated averaging, while SA and CCESA have the at-
tack accuracy of near 50%, similar to the performance of the random guess. This shows that both
SA and CCESA do not reveal any clue on the local training set, which secures data privacy.
6 Conclusion
We devised communication-computation efficient secure aggregation (CCESA) which successfully
preserves the data privacy of federated learning in a highly resource-efficient manner. Based on
graph-theoretic analysis, we showed that using O(n log n) resources is sufficient for guaranteeing
reliability and privacy of the proposed system with n clients, which is much smaller than O(n2)
resources used in the conventional wisdom. Our experiments on real datasets, measuring the test
accuracy and the privacy leakage, show that CCESA requires only 50% of resources than the con-
ventional wisdom, to achieve the same level of reliability and privacy.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Gergely Acs and Claude Castelluccia. I have a dream!(differentially private smart metering). In
International Workshop on Information Hiding, pp. 118-132. Springer, 2011.
Yoshinori Aono, Takuya Hayashi, Lihua Wang, Shiho Moriai, et al. Privacy-preserving deep learn-
ing via additively homomorphic encryption. IEEE Transactions on Information Forensics and
Security, 13(5):1333-1345, 2017.
Maria Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, commu-
nication complexity and privacy. In Conference on Learning Theory, pp. 26-1, 2012.
James Henry Bell, Kallista A Bonawitz, Adria Gascon, Tancrede Lepoint, and Mariana Raykova.
Secure single-server aggregation with (poly) logarithmic overhead. In Proceedings of the 2020
ACM SIGSAC Conference on Computer and Communications Security, pp. 1253-1269, 2020.
Michael Ben-Or, Shafi Goldwasser, and Avi Wigderson. Completeness theorems for non-
cryptographic fault-tolerant distributed computation. In Proceedings of the twentieth annual ACM
symposium on Theory of computing, pp. 1-10. 1988.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch Paschalidis, and Wei
Shi. Federated learning of predictive models from federated electronic health records. Interna-
tional journal of medical informatics, 112:59-67, 2018.
Zachary Charles, Dimitris Papailiopoulos, and Jordan Ellenberg. Approximate gradient coding via
sparse random graphs. arXiv preprint arXiv:1711.06771, 2017.
Ivan Damgard, Valerio Pastro, Nigel Smart, and Sarah Zakarias. Multiparty computation from
somewhat homomorphic encryption. In Annual Cryptology Conference, pp. 643-662. Springer,
2012.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Tariq Elahi, George Danezis, and Ian Goldberg. Privex: Private collection of traffic statistics for
anonymous communication networks. In Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, pp. 1068-1079, 2014.
Matthias Fitzi, Matthew Franklin, Juan Garay, and S Harsha Vardhan. Towards optimal and efficient
perfectly secure message transmission. In Theory of Cryptography Conference, pp. 311-322.
Springer, 2007.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322-1333, 2015.
Robin C Geyer, Tassilo Klein, and Moin Nabi. Differentially private federated learning: A client
level perspective. arXiv preprint arXiv:1712.07557, 2017.
Slawomir Goryczka and Li Xiong. A comprehensive comparison of multiparty secure additions
with differential privacy. IEEE transactions on dependable and secure computing, 14(5):463-
477, 2015.
Shai Halevi, Yehuda Lindell, and Benny Pinkas. Secure computation on the web: Computing with-
out simultaneous interaction. In Annual Cryptology Conference, pp. 132-150. Springer, 2011.
10
Under review as a conference paper at ICLR 2021
Danny Harnik, Yuval Ishai, and Eyal Kushilevitz. How many oblivious transfers are needed for
secure multiparty computation? In Annual International Cryptology Conference, pp. 284-302.
Springer, 2007.
Rob Jansen and Aaron Johnson. Safely measuring tor. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 1553-1567, 2016.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AUrelien BelleL Mehdi Bennis, Arjun Nitin
Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances
and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.
Iraklis Leontiadis, Kaoutar Elkhiyaoui, and Refik Molva. Private and dynamic time-series data ag-
gregation with trust relaxation. In International Conference on Cryptology and Network Security,
pp. 305-320. Springer, 2014.
Iraklis Leontiadis, Kaoutar Elkhiyaoui, Melek Onen, and Refik Molva. Puda-privacy and unforge-
ability for data aggregation. In International Conference on Cryptology and Network Security,
pp. 3-18. Springer, 2015.
Yehuda Lindell, Benny Pinkas, Nigel P Smart, and Avishay Yanai. Efficient constant round multi-
party computation combining bmr and spdz. In Annual Cryptology Conference, pp. 319-338.
Springer, 2015.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended
feature leakage in collaborative learning. In 2019 IEEE Symposium on Security and Privacy (SP),
pp. 691-706. IEEE, 2019.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Francoise Beaufays. Federated learning
for emoji prediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019.
Adi Shamir. How to share a secret. Communications of the ACM, 22(11):612-613, 1979.
Sheng Shen, Tianqing Zhu, Di Wu, Wei Wang, and Wanlei Zhou. From distributed machine learning
to federated learning: In the view of data privacy and security. Concurrency and Computation:
Practice and Experience, 2020.
Elaine Shi, TH Hubert Chan, Eleanor Rieffel, Richard Chow, and Dawn Song. Privacy-preserving
aggregation of time-series data. In Proc. NDSS, volume 2, pp. 1-17. Citeseer, 2011.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd
ACM SIGSAC conference on computer and communications security, pp. 1310-1321, 2015.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP),
pp. 3-18. IEEE, 2017.
Jinhyun So, Basak Guler, and A Salman Avestimehr. Turbo-aggregate: Breaking the quadratic
aggregation barrier in secure federated learning. arXiv preprint arXiv:2002.04156, 2020.
Jy-yong Sohn, Dong-Jun Han, Beongjun Choi, and Jaekyun Moon. Election coding for distributed
learning: Protecting signsgd against byzantine attacks. Advances in Neural Information Process-
ing Systems, 33, 2020.
Katrine Tjell and Rafael Wisniewski. Privacy preservation in distributed optimization via dual de-
composition and admm. In 2019 IEEE 58th Conference on Decision and Control (CDC), pp.
7203-7208. IEEE, 2019.
11
Under review as a conference paper at ICLR 2021
Stacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei. Ldp-fed: federated
learning with local differential privacy. In Proceedings of the Third ACM International Workshop
on Edge Systems, Analytics and Networking, pp. 61-66, 2020.
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin, Tony QS Quek,
and H Vincent Poor. Federated learning with differential privacy: Algorithms and performance
analysis. IEEE Transactions on Information Forensics and Security, 2020.
Jie Xu and Fei Wang. Federated learning for healthcare informatics. arXiv preprint
arXiv:1911.06270, 2019.
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept
and applications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):1-19,
2019.
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ra-
mage, and Francoise Beaufays. Applied federated learning: Improving google keyboard query
suggestions. arXiv preprint arXiv:1812.02903, 2018.
Chunlei Zhang, Muaz Ahmad, and Yongqiang Wang. Admm based privacy-preserving decentralized
optimization. IEEE Transactions on Information Forensics and Security, 14(3):565-580, 2018.
12
Under review as a conference paper at ICLR 2021
A Detailed description of the CCESA protocol
Algorithm 1: Communication-Computation Efficient Secure Aggregation (CCESA) Protocol
Input: Number of clients n, assignment graph G, privacy thresholds ti of all clients i ∈ [n],
local models θi of all clients i ∈ [n], Diffie-Hellman key pairs (ciPK, ciSK), (siPK, siSK)
of all clients i ∈ [n] and corresponding key agreement function f, pseudo-random
generator PRG
Step 0. Advertise Keys
Client i:
Sends (i, ciPK, siPK) to the server
Server:
Collects the messages from clients (denote this set of clients as V1)
_ Sends {(i, CPK, SPK)}i∈Adj(j)∩Vι to all Clientsj ∈ V1;
Step 1. Share Keys
Client i:
Generates a random element bi
Applies ti-out-of-(|Adj(i)| + 1) secret sharing schemes to bi and SiSK
(ti ,|Adj(i)|+1)	SK (ti,|Adj(i)|+1)	SK
bi -----------→ (bi,j)j∈(Adj(i))∪{i} , Si -----------→ (Si,j )j∈Adj(i)∪{i}
Encrypts [bi,j, SSK] to [bi,j, MSK] using the authenticated encryption with key
f(CjPK, CiSK)
Sends {(i, j, Mbi,j, SMiS,jK)}j∈Adj(i)∩V1 to the server
Server:
Collects the messages from clients (denote this set of clients as V2)
_ Sends {(i,j, %,SSj)}i∈Aj(j)∩% to all clients j ∈ ½
Step 2. Masked Input Collection
Client i:
Computes Si,j = f (SjPK, SiSK) and
~	_. ,	_, , 、 _____ _., 、
θi = θi + PRG(bi) + Pj∈V2∩Adj(i)χj PRG(Sij) - Pj∈V2∩Adj(i)*>j PRG(Sij)
Sends (i, θi) to the server
Server:
Collects the messages from clients (denote this set of clients as V3)
Sends V3 to all clients j in V3
Step 3. Unmasking
Client i:
Decrypts Sbi,j with key f(CjPK, CiSK) to obtain bi,j for all j ∈ Adj(i) ∩ V3
Decrypts SSiS,jK with key f(CjPK, CiSK) to obtain SiS,jK for all j ∈ Adj(i) ∩ (V2\V3)
SendS {bi,j}j∈Ad7-(i)∩V3, {SSKb∈Aj(i)∩(V2∖V3) to the server
Server:
Collects the messages from clients
Reconstructs bi from {bi,j}j∈Adj(i)∩V3 for all i ∈ V3
Reconstructs SiSK from {SSK}j∈Ady∙(i)∩(½∖V3) for all i ∈ V2∖V3
Computes Si,j = f(SjPK, SiSK) for all j ∈ Adj(i) ∩ V3
Computes the aggregated sum of local models
Pi∈V3 θi = Pi∈V3 θi - Pi∈V3 PRG(bi) - P i∈V2∖V3 ,j ∈ Adj (i) ∩V3 ；i>j PRG(Sij )
_	+ 5∑i∈V2∖V3 j∈Adj(i)∩V3[i<j PRG(Sij)
13
Under review as a conference paper at ICLR 2021
(a) qtotal = 0	(b) qtotal = 0.1
Figure B.1: Test accuracies of SA versus proposed CCESA(n,p) with various connection probability p, for
federated learning using the AT&T face dataset. Here, we set n = 40 and t = 21. The suggested CCESA
achieves the ideal test accuracy by using only 70% of the COmmUniCation/computational resources used in the
conventional SA.
Schemes \ Number of training data (ntrain)	5000	10000	15000	50000
Federated Averaging (McMahan et al., 2017)	70.41%	65.82%	65.89%	60.62%
Secure Aggregation (SA) (Bonawitz et al., 2017)	49.78%	49.97%	49.91%	49.10%
CCESA (Suggested)	49.48%	50.07%	49.16%	50.00%
Table B.1: Precision of the membership inference attack on local models trained on CIFAR-10. The scheme
with a higher attack precision is more vulnerable to the inference attack. For the proposed CCESA, the attacker
is no better than the random guess with precision = 50%, showing the privacy-preserving ability of CCESA.
B Additional experimental results
B.1	Reliability
In Fig. 4 of the main paper, we provided the experimental results on the reliaiblity of CCESA on
CIFAR-10 dataset. Similarly, Fig. B.1 shows the reliability of CCESA in AT&T Face dataset, where
the model is trained over n = 40 clients. We plotted the test accuracies of SA and the suggested
CCESA(n, p) for various p. In both settings of qtotal, selecting p = 0.7 is sufficient to achieve
the test accuracy performance of SA when the system is trained for 50 rounds. Thus, the required
communication/computational resources for guaranteeing the reliability, which is proportional to
np, can be reduced to 70% of the conventional wisdom in federated learning.
B.2	Privacy
In Section 5.3 and Fig. 2 of the main paper, we provided the experimental results on the AT&T Face
dataset, under the model inversion attack. In Fig. B.2, we provide additional experimental results on
the same dataset for different participants. Similar to the result in Fig. 2, the model inversion attack
successfully reveals the individuals identity in federated averaging (McMahan et al., 2017), while
the privacy attack is not effective in both SA and the suggested CCESA.
In the main manuscript, we have also considered another type of privacy threat called membership
inference attack (Shokri et al., 2017), where the attacker observes masked local model θi sent from
client i to the server, and guesses whether a particular data is the member of the training set. We
measured three types of performance metrics of the attacker: accuracy (the fraction of the records
correctly estimated the membership), precision (the fraction of the responses inferred as members
of the training dataset that are indeed members) and recall (the fraction of the training data that the
attacker can correctly infer as members). Table 3 in the main manuscript summarizes the attack
accuracy result, while Table B.1 shows the attack precision for CIFAR-10 dataset. We also observed
that recall is close to 1 for all schemes. Similar to the results on the attack accuracy, Table B.1 shows
that the attack precision of federated averaging reaches near 70%, while that of SA and CCESA
remain around the baseline performance of random guess. This shows that both SA and CCESA do
not reveal any clue on the training set.
14
Under review as a conference paper at ICLR 2021
(Q)	(b)	(C)	(d)
Figure B.2: The result of model inversion attack to three schemes, (b) the suggested scheme (CCESA), (C)
SA (BonaWitz et al., 2017) and (d) federated averaging (McMahan et al., 2017), for AT&T Face dataset. The
original training images at (a) can be successfully reconstructed by the attack only in federated averaging setup,
i.e., both SA and CCESA achieve the same level of privacy.
15
Under review as a conference paper at ICLR 2021
C	Proofs
C.1 Proof of Theorem 1
Proof. Note that the sum of masked local models obtained by the server is expressed as
X θi = X θi + X PRG(bi)+ Z
i∈V3	i∈V3	i∈V3
where
Z= X X	PRG(si,j)-X X	PRG(si,j).
i∈V3 j∈V2∩Adj(i);i<j	i∈V3 j∈V2∩Adj(i);i>j
Here, Z can be rewritten as
Z=X X PRG(si,j)-X X PRG(si,j)
i∈V3 j ∈V2 ∩ Adj (i);i<j	i∈V3 j ∈V2∩ Adj (i);i>j
=X X PRG(si,j)-X X PRG(si,j)
i∈V3 j三V2∩Adj(i)4<j	j∈V2 i三V3∩Adj(j)∙,i>j
(=a) X X PRG(si,j)-X X PRG(si,j)
i∈V3 j∈V2∩Adj(i)yi<j	i∈V2 j∈V3∩Adj(i)yi<j
=X X	PRG(si,j)+X	X	PRG(si,j)
i∈V3 j∈V3∩Adj(i')-;i<j	i∈V3 j∈(V2∖V3)∩Adj(i)Y<j
- X X PRG(si,j) - X X	PRG(si,j)
i∈V3 j三V3 ∩Adj(i);i<j	i∈V2∖V3 j∈V3 ∩Adj(i);i<j
= X	X	PRG(si,j) - X	X	PRG(si,j )
i∈V3 j∈(V2∖V3)∩Adj(i);i<j	j∈V3 i∈(V2∖V3)∩Adj(j);i<j
(=b) X	X	PRG(si,j ) - X	X	PRG(si,j )
i∈V3 j ∈(V2∖V3)∩Adj (i);i<j	i∈V3 j ∈ (V2∖V3)∩ Adj (i);i>j
= X	X	PRG(si,j) - X	X	PRG(si,j)
i∈V3 j ∈(V2∖V3)∩Adj (i);i<j	i∈V3 j ∈ (V2∖V3)∩ Adj (i);i>j
=X	X	PRG(si,j)-X	X	PRG(si,j),
i∈V3 j∈(V3+\V3)∩Adj(i);i<j	i∈V3 j∈(V：+\V3)∩Adj(i);i>j
where (a) and (b) come from si,j = sj,i. In order to obtain the sum of unmasked local models
i∈V θi from the sum of masked local models i∈V θi, the server should cancel out all the ran-
dom terms in Pi∈V PRG(bi) + Z. In other words, the server should reconstruct bi for all i ∈ V3
and sjSK for all j ∈ V3+\V3. Since the server can obtain |(Adj(i) ∪ {i}) ∩ V4| secret shares of client
i in Step 3, |(Adj(i) ∪ {i}) ∩ V4| ≥ ti for all i ∈ V3+ is a sufficient condition for reliability.
Now we prove the converse part by contrapositive. Suppose there exists i ∈ V3+ such that |(Adj(i) ∪
{i}) ∩ V4| < ti. In this case, note that the server cannot reconstruct both siSK and bi from the shares.
If i ∈ V3, the server cannot subtract PRG(bi) from i∈V θi. As a result, the server cannot obtain
Pi∈V θi. If i ∈ V3+\V3, the server cannot subtract PRG(si,j) for all j ∈ V3 since the server does
not have any knowledge of neither siSK nor sjSK. Therefore, the server cannot compute Pi∈V θi,
which completes the proof.
□
C.2 Proof of Lemma 1
Proof. Let T ⊂ V3 be an arbitrary set of clients satisfying T ∈ {0,V3}. It is sufficient to prove the
following statement: given a connected graph G3 , an eavesdropper cannot obtain the partial sum of
16
Under review as a conference paper at ICLR 2021
local models £记丁 θi from the sum of masked models £记丁 θi. More formally, We need to prove
H(X θi∣ X θi) = H(X θi).
i∈T i∈T	i∈T
ʌ T , , 1 ,,1	C	1 11	1	Fl X~ʌ	X	∙1 1 , ,1	F	∙	F
Note that the sum of masked local models	i∈T θi accessible to the eavesdropper is expressed as
X θi = X θi + X PRG(bi) + z,	(6)
i∈T i∈T	i∈T
Where
z=X X PRG(si,j)-X X PRG(si,j)
i∈T j ∈V2 ∩ Adj(i);i<j	i∈T j三V2 ∩ Adj(i);i>j
=X	X	PRG(si,j)- X	X	PRG(si,j)
i∈T j∈V2∩Adj(i)∙,i<j	j∈V2 i三T∩Adj(j),i>j
(=a) X	X	PRG(si,j)- X	X	PRG(si,j)
i∈T j∈V2∩Adj(i);i<j	i∈V2 j∈T∩Adj(i);i<j
=X X PRG(si,j)+X	X PRG(si,j)
i∈T j∈T∩Adj(i)γi<j	i∈T j∈(V2∖T )∩Adj(i)∖i<j
-X X	PRG(si,j)- X X	PRG(si,j)
i∈T j∈T∩Adj(i')γi<j	i∈V2∖T j∈T∩Adj(i)yi<j
=X	X	PRG(si,j)-X	X	PRG(si,j)
i∈T j ∈(V2∖T)∩Adj (i);i<j	j∈T i∈(V2∖T )C\Adj(j);i<j
(=b) X	X	PRG(si,j) - X	X	PRG(si,j)
i∈T j ∈(V2∖T)∩ Adj (i);i<j	i∈T j∈(V2∖T )∩Adj(i)∖i>j
={X	X	PRG(Sij) - X	X	PRG(Sij)}
i∈T j∈(V2∖V3)∩Aj(i)Y<j	i∈T j三(V2∖V3)∩Adj(iXi>j
+{X	X	PRG(Si,j) - X	X	PRG(Si,j)}.
i∈T j ∈(V3∖T)∩ Adj (i);i<j	i∈T j ∈(V3∖T)∩ Adj (i);i>j
Here, (a) and (b) come from Si,j = Sj,i. If G3 = (V3, E3) is connected, there exists an edge
e = {p, q} such that p ∈ T and q ∈ (V3\T). As a consequence, pseudorandom term PRG(Sp,q)
is included in z, and its coefficient cp,q is determined as 1 (ifp < q), or -1 (ifp > q). Note that
equation (6) can be reWritten as
X瓦=Xθi + cp,q ∙ PRG(Sp,q) + r,
i∈T	i∈T
Where r is the sum of pseudorandom terms Which do not include PRG(Sp,q). In order to unmask
PRG(Sp,q), the eavesdropper need to knoW at least one of the secret keys of clients p and q. HoWever,
the eavesdropper cannot obtain any shares of the secret keys since the server do not request the
shares of SpSK and SqSK in step 3. Therefore, due to the randomness of pseudorandom generator,
Tr八 I	X ∖	∕λ ∖ ι ι ι ι ∙ ι	ι . .ι	C	ι~ι
H(Σ2i∈T °i I fi∈T θi) = H(Σi∈T θi) holds, which completes the proof.	□
C.3 Proof of Theorem 2
Proof. We first prove that the system is private if G ∈ GC ∪ GNI . When G ∈ GC, the statement
holds directly from Lemma 1. Thus, below we only prove for the case of G ∈ GNI . Note that it is
sufficient to prove the following statement: given a graph evolution G = (Go, Gι,…，G4) ∈ GNI,
an eavesdropper cannot obtain the partial sum of local models Pi∈T θi from the sum of masked
models £记丁 θi for every T ⊂ V3 satisfying T ∈ {V3, 0}. More formally, we need to prove
H(X θi∣ X θi) = H(X θi).	(7)
i∈T i∈T	i∈T
17
Under review as a conference paper at ICLR 2021
When T = Cl for some l ∈ [κ], there exists i? ∈ Cl+ such that node i is not informative, according
to the definition of GNI . Thus, the server (as well as eavesdroppers) cannot reconstruct both bi and
siSK . Note that the sum of masked models is
X 瓦=X θi + X PRG(bi) + X X (-1)1j>i PRG(Sj,i),	(8)
i∈T	i∈T i∈T	j∈T i∈V2∪Adj(j)
where 1A is the indicator function which is value 1 when the statement A is true, and 0 otherwise.
When i? ∈ Cl = T, we cannot unmask PRG(bi?) in this equation. When i? ∈ Cl+\Cl, there exists
j ∈ Cl such that {i?, j} ∈ E. Note that the eavesdropper needs to know either SjSK or SiS?K, in order
to compute PRG(Sj,i? ). Since i? is not informative, the eavesdropper cannot get SiS?K. Moreover,
since the server has already requested the shares of bj , the eavesdropper cannot access SjSK . Thus,
the eavesdropper cannot unmask PRG(Sj,i?) from (8). All in all, the eavesdropper cannot unmask
at least one pseudorandom term in i∈T θi, proving (7).
When T 6= Cl ∀l ∈ [κ], there exists an edge e = {p, q} such that p ∈ T and q ∈ (V3\T). Thus,
We cannot unmask PRG(Sp,q) from Pi∈τ θ%. Following the steps in Section C.2, We have (7).
Now, we prove the converse by contrapositive: if G = (G0, Gι, •…，G4) ∈ Gd ∩ GNI, then the
system is not private. In other Words, We need to prove the folloWing statement: ifG3 is disconnected
and there exists a component Cl such that all nodes in Cl are informative, then the system is not
private. Let T = Cl. Then, the eavesdropper obtains
X θi = X θi + X PRG(bi) + z,
i∈T i∈T	i∈T
where
z=X X	(-1)1i>jPRG(Si,j) (=a)XX(-1)1i>jPRG(Si,j) (=b) 0.
i∈Tj∈V2∩Adj(i)	i∈T j∈T
Note that (a) holds since T is a component, (b) holds from Si,j = Sj,i. Moreover, the eavesdropper
can reconstruct bi for all i ∈ T in Step 3 of the algorithm. Thus, the eavesdropper can successfully
unmask random terms in £记丁 θi and obtain 52i∈τ θi. ThiS completes the proof.	□
C.4 Proof of Theorem 3
Proof. Consider ErdOS-Renyi assignment graph G ∈ G(n,p). Let Ni := ∣Adj(i)| be the degree of
node i, and Xi := |Adj(i) ∩ V4| be the number of clients (except client i) that successfully send the
shares of client i to the server in Step 3. Then, Ni and Xi follow the binomial distributions
Ni 〜B(n - 1,p), Xi 〜B(Ni,(1 - q)4) = B(n - 1,p(1 - q)4),
respectively. By applying Hoeffding’s inequality on random variable Xi, we obtain
P(Xi < (n - 1)p(1 - q)4 - Pn - 1)log(n - 1)) ≤ 1/(n - 1)2.
Let E be the event that the system is not reliable, i.e., the sum of local models Pi∈V θi is not
reconstructed by the server, and Ei be the event {|(Adj(i) ∪ {i}) ∩ V4| < t}, i.e., a secret of client
i is not reconstructed by the server. For a given P > '+g--；)；-：：=-1), we obtain
P(E) (=a)P(∪i∈V3+Ei) ≤P(∪i∈V3+{Xi <t}) ≤ X P(Xi <t)
i∈V3+
≤ X P(Xi < t) = nP (X1 < t)
i∈[n]
≤nP(Xi < (n - 1)p(1 - q)4 - p(n - 1)log(n - 1)) ≤ -,~~n—2 n→∞> 0,
(n- 1)2
18
Under review as a conference paper at ICLR 2021
where (a) comes from Theorem 1. Therefore, we conclude that CCESA(n, p) is asymptotically
almost surely (a.a.s.) reliable if p >
t+V(n-1) log(n-1)
(n-1)(1-q)4
Furthermore, based on the parameter
selection rule of t in Section F, we obtain a lower bound on p as
t +，(n - 1)log(n - 1) ≥ (…"{-1)^”-1+1 +，(n - 1)log(n - 1) - 1
p> (n - 1)(1 - q)4	≥	(n -1)(1- q)4
Rearranging the above inequality with respect to p yields
p>
3√(n - 1)log(n - 1) - 1
(n - 1)(2(1-q)4-1)
□
C.5 Proof of Theorem 4
Proof. Let X := |V3 | be the number of clients sending its masked local model in Step 2. Then, X
follows Binomial random variable B(n, (1 - q)3). Given assignment graph G of CCESA(n, p), note
that the induced subgraph G3 = G - V∖V3 is an Erdos-Renyi graph G(X,p).
First, we prove
P(G3 is ConnecteduX - n(1 - q)3∣ ≤ √nlnn) n→∞> 1,
if p> p? = ln(dn(1-q) -√√nnne)(1 + e). The left hand side of (9) can be rewritten as
dn(1-q)3- n lnne
P(G3 is Connectedl |X — n(1 — q)31 ≤ nn ln n)
_ pι∈[n(i-q)3-√nτnn,n(i-q)3+√n^] P(X = I)P(G(I,p) is ConneCted)
(9)
Σ>l∈ [n(1-q)3-√n ln n,n(1-q)3 + √n ln n] P(X = I)
Here, We use a well-known property of Erdos-Renyi graph: G(l,p) is asymptotically almost surely
(a.a.s.) connected if p > (1+)ln l for some e > 0. Since lnl is a decreasing function, G(l, P) is a.a.s.
connected for all l ∈ [n(1 — q)3 — √n ln n, n(1 — q)3 + √n ln n] when P > ln(ddn(1-q3 -√√n：nne’.
Thus, for given p > p?, we can conclude
P(G3 is connected∣|X — n(1 — q)3∣ ≤ √nlnn) n→∞→ 1.
Now, we will prove that CCESA(n, p) is a.a.s. private when p > p?. The probability that
CCESA(n, p) is private is lower bounded by
(a)
P(CCESA(n, p) is private) ≥ P(G3 is ConneCted)
=P(|X — n(1 — q)3∣ ≤ √nlnn)P(G3 is connected∣|X — n(1 — q)3∣ ≤ √nlnn)
+ P(|X — n(1 — q)3∣ > Vznlnn)P(G3 is connected∣ |X — n(1 — q)3∣ > Vnlnn)
(b)	,_____ 一
≥ (1 - 2∕n2)P(G3 is connected∣ |X - n(1 - q)3∣ ≤ ∖!nlnn) -~- 1,
where (a) comes from Lemma 1 and (b) comes from Hoeffding’s inequality
P(|X — n(1 — q)3∣ ≤ √nlnn) ≥ 1 — 2/n2,
which completes the proof.
□
C.6 Proof of Theorem 5
Proof. Consider an ErdoS-Renyi assignment graph G ∈ G(n,p). Let Ni := ∣Aj(i)∣ be the degree
of node i, and Xi := |Adj(i) ∩ V4| be the number of clients (except client i) that successfully send
the shares of client i to the server in Step 3. Then, Ni and Xi follow the binomial distributions
Ni 〜B(n — 1,p), Xi 〜B(Ni, (1 — q)4) = B(n — 1,p(1 — q)4),
19
Under review as a conference paper at ICLR 2021
respectively. Let Ei be an event {|(Adj(i) ∪ {i}) ∩ V4| < t}, i.e., a secret of client i is not recon-
structed by the server. We obtain an upper bound on P(Ei) as
t-1	1
P(Ei)	≤P(Xi	<t)	= n-i	1	(p(1	-	q)4)i(1 - p(1 - q)4)(n-1-i)
=)e-(n-1)D(⅛-⅛ ∣∣p(1-q)4)
where (a) comes from Chernoff bound on the binomial distribution. Thus, Pe(r) is upper bounded
by
Pe(r) =P(∪i∈V3+Ei) ≤ P (∪i∈V3+ {Xi < t}) ≤ X P(Xi < t)
i∈V3+
≤ X P(Xi <t) = nP(Xι <t) = ne-(nT)D(⅛-⅛Up(I-q)4),
i∈[n]
where (b) comes from Theorem 1.
□
C.7 Proof of Theorem 6
Proof. Let Pdc(n,p) be the probability of an event that Erdos-Renyi graph G ∈ G(n,p) is discon-
nected. Then, Pdc(n,p) is upper bounded as follows.
Pdc(n,p) = P (G(n, p) is disconnected)
=P(∪k=ι2c {there exists a subset of k nodes that is disconnected})
bn/2c
≤	P (there exists a subset of k nodes that is disconnected)
k=1
bn/2c n
≤	P(a specific subset of k nodes is disconnected)
k=1	k
bn/2c
= X	nk (1 - p)k(n-k)
k=1
Therefore, Pe(p) is upper bounded by
(a)
Pe(p) ≤ P(G3 = G-V\V3 is disconnected)
n
=	P(G3 has m vertices)Pdc(m, p)
m=0
Xn	mn
m=0
Xn	mn
m=0
(1 - q)3m(1 - (1 - q)3)(n-m) ∙ Pdc(m,p)
bm/2c
(1 - q)3m(1 - (1 - q)3)(n-m) X m (1 - p)k(m-k),
where (a) comes from Lemma 1.
□
D	Required resources of CCESA
D.1 Communication cost
Here, we derive the additional communication bandwidth BCCESA used at each client for running
CCESA, compared to the bandwidth used for running federated averaging (McMahan et al., 2017).
20
Under review as a conference paper at ICLR 2021
We consider the worst-case scenario having the maximum additional bandwidth, where no client
fails during the operation.
The required communication bandwidth of each client is composed of four parts. First, in Step
0, each client i sends two public keys to the server, and receives 2|Adj(i)| public keys from other
clients. Second, in Step 1, each client i sends encrypted 2|Adj(i)| shares to other nodes, and re-
ceives 2|Adj (i)| shares from other nodes through the server. Third, in Step 2, each client i sends a
masked data yi of mR bits. Here, m is the dimension of model parameters where each parameter
is represented in R bits. Fourth, in Step 3, each client i sends |Adj(i)| + 1 shares to the server.
Therefore, total communication bandwidth of client i can be expressed as
(total communication bandwidth) = 2(|Adj(i)| + 1)aK + (5|Adj(i)| + 1)aS + mR,
where aK and aS are the number of bits required for exchanging public keys and the number of bits
in a secret share. Since each client i requires mR bits to send the private vector θi in the federated
averaging (McMahan et al., 2017), we have
BCCESA = 2(|Adj(i)| + 1)aK + (5|Adj(i)| + 1)aS.
If We Choose the connection probability P =(1 + e)p? for a small e > 0, we have BCCESA =
O(√nlog n), where p? is defined in (5). Note that the additional bandwidth BSA required for SA
can be similarly obtained as BSA = 2naK + (5n - 4)aS having BSA = O(n). Thus, we have
BCCESA
BSA
→0
as n increases, showing the scalability of CCESA. These results are summarized in Table 1 in the
main manuscript.
D.2 Computational cost
We evaluate the computational cost of CCESA. Here we do not count the cost for computing the
signatures since it is negligible. First, we derive the computational cost of each client. Given the
number of model parameters m and the number of clients n, the computational cost of client i is
composed of three parts: (1) computing 2|Adj(i)| key agreements, which takes O(|Adj(i)|) time,
(2) generating shares of ti-out-of-|Adj(i)| secret shares of siSK and bi, which takes O(|Adj(i)|2)
time, and (3) generating masked local model θi, which requires O(m|Adj(i)|) time. Thus, the total
computational cost of each client is obtained as O(|Adj(i)|2 + m|Adj(i)|). Second, the server’s
computational cost is composed of two parts: (1) reconstructing ti-out-of-|Adj(i)| secrets from
shares for all clients i ∈ [n], which requires O(|Adj(i)|2) time, and (2) removing masks from
n
masked sum of local models i=1 θi, which requires O(m|Adj(i)|2) time in the worst case. As a
result, the total computational cost of the server is O(m∣Adj(i)|2). If we choose P = (1 + e)p? for
small e > 0, the total computational cost per each client is O(n log n + m√n log n), while the total
computation cost of the server is O(mn log n). The computational cost of SA can be obtained in a
similar manner, by setting Adj(i) = n - 1; each client requires O(n2 + mn) time while the server
requires O(mn2) time. These results are summarized in Table 1 in the main manuscript.
E Reliability and privacy of CCESA
Here, we analyze the asymptotic behavior of probability that a system is reliable/private. In our
analysis, we assume that the connection probability is set to P? and the parameter t used in the
secret sharing is selected based on the rule in Section F. First, we prove that a system is reliable with
probability ≥ 1 - O(ne-n log n). Using Theorem 5, the probability that a system is reliable can be
directly derived as
P (A system is reliable) =1 - Pe(r)
≥1 - ne-(n-1')DκL(n-1||p?(I-q)4)
- ne	.
Using the fact that Kullback-Leibler divergence term satisfies
t — 1	t — 1	t-1	+ — 1	1 — t-1
Dkl (T||P?(1 - q)4) = T log () +。- T) log (I-f^)
n - 1	n - 1 P?(1 - q)4	n - 1	1 - P?(1 - q)4
=Θ(√log n/n),
21
Under review as a conference paper at ICLR 2021
We conclude that CCESA(n,p?) is reliable with Probablilty ≥ 1 - O(ne-√n log n).
Now we prove that a system is private with probability ≥ 1 - O(n-C) for an arbitrary C > 0. Using
Theorem 6, the Probability that a system is Private can be obtained as
P(A system is Private) =1 - Pe(p)
n
≥1-	ambm,
m=0
where am =	*)(1 -	q)3m(1 - (1 -	q)3)(n-m)	and	bm	=	£"2」*)(1 -	p*)k(m-k).
Note that the summation term Pnm=0 am bm can be broken uP into two Parts: Pmmt=h0 ambm and
Pnm=m +1 ambm, where mth = bn(1 - q)3/2c. In the rest of the Proof, we will Prove two lem-
mas, showing that Pmmt=h0 ambm = O(e-n) andPnm=mth+1 ambm = O(n-C), resPectively.
Lemma 2.
mth
ambm = O(e-n)
m=0
Proof. Since bm ≤ 1 for all m, we have
mth
ambm ≤
m=0
mth
am.
m=0
Note that am = P(X = m) holds for binomial random variable X = B(n, (1 - q)3). By utilizing
Hoeffding’s inequality, we have
mth
X am =P(X ≤ mth) ≤ e-2(n(1-q)3-mth)2 ≤ e-n(1-q)6/2.
m=0
Therefore, we conclude that Pmmt=h0 ambm = O(e-n).
Lemma 3.
n
ambm = O(n-C)
m=mth+1
□
Proof. Since am ≤ 1 for all m, we have
n
ambm ≤
m=mth +1
n
bm.
m=mth+1
Let C > 0 be given. Then, the uPPer bound on bm can be obtained as
bm/2c	bm/2c
bm= X mk (1 - p?)k(m-k) ≤ X	mk e-k(m-k)p?
k=1	k=1
bm/2c
=X (m)m-lk(m-k)/m = Cm + dm
k=1 k
where λ = p?n/log n, Cm = (m) Pk=1 m-λMm-k)∕m, dm, = (m) Pkm/2+1 m-λMm-k)∕m, and
k? = bm(1 - C+2 )C for some C > 0. The first part of summation is upper bounded by
cm
k?
Xmk
k=1
m—λk(m-k)/m ≤ ^m-kbλ(m-k)∕m-1C
k=1
m-bNm-k*"m-1c
≤ 1 - m-bNm-k*"mTc
≤ fm-kbλ(m-k* )∕m-lC
m-(C+1)
1 - m-(C+1).
22
Under review as a conference paper at ICLR 2021
The second part of summation, We will use the bound (Jn) ≤ (竿)k. Using this bound, dm is upper
bounded by
bm/2c
dm = X m m-λkg-k"m
k=k?+1
bm∕2	em1-λ(m-k) / m
≤ X (em-^—)k
k=k?+1
bm/2c
X(
k=k?+1
em1-λ(m-k)∕m
k? +1
)k
≤
≤
bm2c em-Nm-k”m
( em_______)k
L (1 - λ-1(C + 2))
k=k?+1
bm/2c
X(
k=k?+1
em-λ/2	、k
1 - λ-1(C + 2)).
≤
For sufficiently large λ, we have em-λ∕2∕(1 - λ-1(C +2)) < δ for some δ < 1. Therefore, dm, is
upper bounded by
∞	k?
dm ≤ X δk = — = O(δmC0 )
1-δ
k=k?+1
where C0 = (1 - λ-1(C + 2)) > 0. Combining upper bounds on cm and dm, we obtain bm =
O(m-(C+1)). Since bm is a decreasing function of m,
nn
bm ≤	bmth+1 = (n - mth)bmth+1 (=a) O(n-C)
m=mth+1	m=mth+1
holds where (a) comes from mth = bn(1 - q)3∕2c.
□
Combining the above two lemmas, we conclude that CCESA(n, p?) is private with probability ≥
1 - O(n-C) for arbitrary C > 0. These results on the reliability and the privacy are summarized in
Table 1 of the main manuscript.
F DESIGNING THE PARAMETER t FOR THE SECRET SHARING
Here we provide a rule for selecting parameter t used in the secret sharing. In general, setting t to
a smaller number is better for tolerating dropout scenarios. However, when t is excessively small,
the system is vulnerable to the unmasking attack of adversarial server; the server may request shares
of bi and siSK to disjoint sets of remaining clients simultaneously, which reveals the local model
θi to the server. The following proposition provides a rule of designing parameter t to avoid such
unmasking attack.
Proposition 1 (Lower bound on t). For CCESA(n,p), let t > m-1)p+Λ∕(n-1)log(nF+1 be given.
Then, the system is asymptotically almost surely secure against the unmasking attack.
Proof. Let E be the event that at least one of local models are revealed to the server, and Ei be the
event that ith local model θi is revealed to the server. Note that θi is revealed to the server if t clients
send the shares of bi and other t clients send the shares of siSK in Step 3. Therefore,
P(Ei) ≤ P(∣(Adj(i) ∪{i}) ∩ V4∣ ≥ 2t)
≤ P(|(Adj(i) ∪ {i})| ≥2t) =P(|(Adj(i)| ≥2t-1)
1_________________ (a)	1
≤ P(|Aj(i)| > (n - 1)p + √(n - 1)log(n - 1)) ≤ (n - 1产,
where (a) comes from Hoeffding’s inequality of binomial random variable. As a result, we obtain
P(E) = P(∪i∈[n]Ei) ≤ X P(Ei) = nP(E1 ) = -^n-ɪ -→-→ 0,
i∈[n]	(n- 1)
which completes the proof.	□
As stated above, setting t to a smaller number is better to tolerate the dropout of multiple clients.
Thus, as in the following remark, we set t to be the minimum value avoiding the unmasking attack.
23
Under review as a conference paper at ICLR 2021
Remark 4 (Design rule for t). Throughout the paper, we set t = d(n——p+,(n ? °g n + ] for
CCESA(n, p), in order to secure a system against the unmasking attack and provide the maximum
tolerance against dropout scenarios.
G Detailed experimental setup
G.1 AT & T face dataset
AT&T Face dataset contains images of 40 members. We allocated the data to n = 40 clients
participating in the federated learning, where each client contains the images of a specific member.
This experimental setup is suitable for the practical federated learning scenario where each client has
its own image and the central server aggregates the local models for face recognition. Following the
previous work (Fredrikson et al., 2015) on the model inversion, we used softmax regression for the
classification. Both the number of local training epochs and the number of global aggregation rounds
are set to Elocal = Eglobal = 50, and we used the SGD optimizer with learning rate γ = 0.05.
G.2 CIFAR- 1 0 dataset
G.2.1 Reliability experiment in Fig. 4
We ran experiments under the federated learning setup where 50000 training images are allocated
to n = 1000 clients. Here, we considered two scenarios for data allocation: one is partitioning the
data in the i.i.d. manner (i.e., each client randomly obtains 50 images), while the other is non-i.i.d.
allocation scenario. For the non-i.i.d. scenario, we followed the procedure of (McMahan et al.,
2017). Specifically, the data is first sorted by its category, and then the sorted data is divided into
2000 shards. Each client randomly chooses 2 shards for its local training data. Since each client
has access to at most 2 classes, the test accuracy performance is degraded compared with the i.i.d.
setup. For training the classifier, we used VGG-11 network and the SGD optimizer with learning
rate γ = 0.1 and momentum β = 0.5. The local training epoch is set to Elocal = 3.
G.2.2 Privacy experiments in Table 3 and Table B.1
We conducted experiments under the federated learning setup where ntrain training images are as-
signed to n = 10 clients. We considered i.i.d. data allocation setup where each client randomly
obtains ntrain/10 training images. The network architecture, the optimizer, and the number of local
training epochs are set to the options used in Sec. G.2.1.
G.3 Connection probability setup in Fig. 3
In Fig. 3, we select different connection probabilities p = p?(n, qtotal) for various n and qtotal, where
p? is defined in (5). The detailed values of connection probability p are provided in Table G.2.
qtotal∖n	100	200	300	400	500	600	700	800	900	1000
0	0.636	0.484	0.411	0.365	0.333	0.308	0.289	0.273	0.260	0.248
0.01	0.649	0.494	0.419	0.373	0.340	0.315	0.295	0.280	0.265	0.254
0.05	0.707	0.538	0.457	0.406	0.370	0.344	0.321	0.304	0.289	0.276
0.1	0.795	0.605	0.513	0.456	0.416	0.385	0.361	0.341	0.325	0.311
Table G.2: Connection probability p = p? in Fig. 3
G.4 Running time experiment in Table 2
We implemented CCESA algorithm in python. For symmetric authenticated encryption, we use
AES-GCM with 128-bit keys in Crypto.Cipher package. For the pseudorandom generator,
we use randint function (input: random seed, output: random integer in the field of size 216)
in numpy.random package. For key agreement, we use Elliptic-Curve Diffie-Hellman over the
NIST SP800-56 curve composed with a SHA-256 hash function. For secret sharing, we use standard
t-out-of-n secret sharing (Shamir, 1979).
24