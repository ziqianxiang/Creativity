Under review as a conference paper at ICLR 2021
Fundamental Limits and Tradeoffs
in Invariant Representation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Many machine learning applications involve learning representations that achieve
two competing goals: To maximize information or accuracy with respect to a
target while simultaneously maximizing invariance or independence with respect
to a subset of features. Typical examples include privacy-preserving learning,
domain adaptation, and algorithmic fairness, just to name a few. In fact, all of
the above problems admit a common minimax game-theoretic formulation, whose
equilibrium represents a fundamental tradeoff between accuracy and invariance.
In this paper, we provide an information theoretic analysis of this general and
important problem under both classification and regression settings. In both cases,
we analyze the inherent tradeoffs between accuracy and invariance by providing a
geometric characterization of the feasible region in the information plane, where
we connect the geometric properties of this feasible region to the fundamental
limitations of the tradeoff problem. In the regression setting, we also derive a
tight lower bound on the Lagrangian objective that quantifies the tradeoff between
accuracy and invariance. Our results shed new light on this fundamental problem
by providing insights on the interplay between accuracy and invariance. These
results deepen our understanding of this fundamental problem and may be useful
in guiding the design of adversarial representation learning algorithms.
1	Introduction
One of the fundamental tasks in both supervised and unsupervised learning is to learn proper
representations of data for various downstream tasks. Due to the recent advances in deep learning,
there has been a surge of interest in learning so-called invariant representations. Roughly speaking,
the underlying problem of invariant representation learning is to find a feature transformation of the
data that balances two goals simultaneously. First, the features should preserve enough information
with respect to the target task of interest, e.g., good predictive accuracy. On the other hand, the
representations should be invariant to the change of a pre-defined attribute, e.g., in visual perceptions
the representations should be invariant to the change of perspective or lighting conditions, etc. Clearly,
in general there is often a tension between these two competing goals of error minimization and
invariance maximization. Understanding the fundamental limits and tradeoffs therein remains an
important open problem.
In practice, the problem of learning invariant representations is often formulated as solving a minimax
sequential game between two agents, a feature encoder and an adversary. Under this framework,
the goal of the feature encoder is to learn representations that could confuse a worst-case adversary
in discriminating the pre-defined attribute. Meanwhile, the representations given by the feature
encoder should be amenable for a follow-up predictor of target task. In this paper, we consider
the situation where both the adversary and the predictor have infinity capacity, so that the tradeoff
between accuracy and invariance solely depends on the representations given by the feature encoder.
In particular, our results shed light on the best possible tradeoff attainable by any algorithm. This
leads to a Lagrangian objective with a tradeoff parameter between these two competing goals, and we
study the fundamental limitations of this tradeoff by analyzing the extremal values of this Lagrangian
in both classification and regression settings. Our results shed new light on the fundamental tradeoff
between accuracy and invariance, and give a crisp characterization of how the dependence between
the target task and the pre-defined attribute affects the limits of representation learning.
1
Under review as a conference paper at ICLR 2021
Contributions We geometrically characterize the tradeoff between accuracy and invariance via the
information plane (Shwartz-Ziv & Tishby, 2017) analysis under both classification and regression
settings, where each feature transformation correspond to a point on the information plane. For the
classification setting, we provide a fundamental characterization of the feasible region in the informa-
tion plane, including its boundedness, convexity, and extremal vertices. For the regression setting,
we provide an analogous characterization of the feasible region by replacing mutual information
with conditional variances. Finally, in the regression setting, we prove a tight information-theoretic
lower bound on a Lagrangian objective that trades off accuracy and invariance. The proof relies on an
interesting SDP relaxation, which may be of independent interest.
Related Work There are abundant applications of learning invariant representations in various
downstream tasks, including domain adaptation (Ben-David et al., 2007; 2010; Ganin et al., 2016;
Zhao et al., 2018), algorithmic fairness (Edwards & Storkey, 2015; Zemel et al., 2013; Zhang et al.,
2018; Zhao et al., 2019b), privacy-preserving learning (Hamm, 2015; 2017; Coavoux et al., 2018;
Xiao et al., 2019), invariant visual representations (Quiroga et al., 2005; Gens & Domingos, 2014;
Bouvrie et al., 2009; Mallat, 2012; Anselmi et al., 2016), and causal inference (Johansson et al., 2016;
Shalit et al., 2017; Johansson et al., 2020), just to name a few. To the best of our knowledge, no
previous work studies the particular tradeoff problem in this paper. Closest to our work are results in
domain adaptation (Zhao et al., 2019a) and algorithmic fairness (Menon & Williamson, 2018; Zhao
& Gordon, 2019), showing a lower bound on the classification accuracy on two groups, e.g., source
vs. target in domain adaptation and majority vs. minority in algorithmic fairness. Compared to these
previous results, our work directly characterizes the tradeoff between accuracy and invariance using
information-theoretic concepts in both classification and regression settings. Furthermore, we also
give an approximation to the Pareto frontier between accuracy and invariance in both cases.
2	Background and Preliminaries
Notation We adopt the usual setup given (X, Y ) ∈ X × Y, where Y is the response, X ∈ Rp
represents the input vector, and we seek a classification/regression function f (X) that minimizes
E'(f (X), Y) where ' : Y×Y → R is some loss function depending on the context of the underlying
problem. In this paper, We consider two typical choices of ':(1) the cross entropy loss, i.e. '(y, y0)=
-y log(y0) - (1 -y) log(1-y0), which is typically used when Y is a discrete variable in classification;
(2) the squared loss, i.e. '(y, y0) = (y - y0)2, which is suitable for Y continuous, as in regression.
Throughout the paper, we will assume that all random variables have finite second-order moments.
Problem Setup Apart from the input/output pairs, in our setting there is a third variable A, which
corresponds to a variable that a predictor should be invariant to. Depending on the particular
application, A could correspond to potential protected attributes in algorithmic fairness, e.g., the
ethnicity or gender of an individual; or A could be the identity of domain index in domain adaptation,
etc. In general, we assume that there is a joint distribution D over the triple (X, A, Y), from which
our observational data are sampled from. Upon receiving the data, the goal of the learner has two
folds. On one hand, the learner aims to accurately predict the target Y. On the other hand, it also
tries to be insensitive to variation in A. To achieve this dual goal, one standard approach in the
literature (Zemel et al., 2013; Edwards & Storkey, 2015; Hamm, 2015; Ganin et al., 2016; Zhao
et al., 2018) is through the lens of representation learning. Specifically, let Z = g(X) where g(∙) is
a (possibly randomized) transformation function that takes X as input and gives the corresponding
feature encoding Z. The hope is that, by learning the transformation function g(∙), Z contains as
much information as possible about the target Y while at the same time filtering out information
related to A. This problem is often phrased as an adversarial game:
min max	ED ['(f ◦ g(X), Y)] - λ ∙ ED ['(f0 ◦ g(X),A)],	(1)
f,g f0
where the two competing agents are the feature transformation g and the adversary f0, and λ > 0 is a
tradeoff hyperparameter between the task variable Y and the attribute A. For example, the adversary
f0 could be understood as a domain discriminator in applications related to domain adaptation,
or an auditor of sensitive attribute in algorithmic fairness. In the above minimax game, the first
term corresponds to the accuracy of the target task, and the second term is the loss incurred by the
adversary. It is worth pointing out that the minimax problem in (1) is separable for any fixed feature
transformation g, in the sense that once g has been fixed, the optimization of f and f0 are independent
of each other. Formally, define RY(g) := inff ED'(f (g(X)), Y) to be the optimal risk in predicting
2
Under review as a conference paper at ICLR 2021
Y using Z = g(X) under loss', and similarly define RA (g). The separation structure of the problem
leads to the following compact form:
OPT(λ) := min RY(g) - λ ∙ RA(g).	⑵
g
The minimization here is taken over a family of (possibly randomized) transformations g . Intuitively,
(2) characterizes the situation where for a given transformation Z = g(X), both f and f0 play their
optimal responses. Hence this objective function characterizes a fundamental limit of what the best
possible representation we can hope to achieve for a fixed value λ. In general, with 0 < λ < ∞,
there is an inherent tension between the minimization of RY(g) and the maximization of RA(g) and
a choice of the tradeoff hyperparameter λ essentially corresponds to a realization of such tradeoff.
Motivating Examples We discuss several examples to which the above framework is applicable.
Example 2.1 (Privacy-Preservation). In privacy applications, the goal is to make it difficult to predict
sensitive data, represented by the attribute A, while retaining information about Y (Hamm, 2015;
2017; Coavoux et al., 2018; Xiao et al., 2019). A way to achieve this is to pass information through
Z, the “privatized” or “sanitized” data.
Example 2.2 (Algorithmic Fairness). In fairness applications, we seek to make predictions about the
response Y without discriminating based on the information contained in the protected attributes A.
For example, A may represent a protected class of individuals defined by, e.g. race or gender. This
definition of fairness is also known as statistical parity in the literature, and has received increasing
attention recently from an information-theoretic perspective (McNamara et al., 2019; Zhao & Gordon,
2019; Dutta et al., 2019).
Example 2.3 (Domain Adaptation). In domain adaptation, our goal is to train a predictor using
labeled data from the source domain that generalizes to the target domain. In this case, A corresponds
to the identity of domains, and the hope here is to learn a domain-invariant representation Z that is
informative about the target Y (Ben-David et al., 2007; 2010; Ganin et al., 2016; Zhao et al., 2018).
Example 2.4 (Group Invariance). In many applications in computer vision, it is desirable to learn
predictors that are invariant to the action of a group G on the input space. Typical examples include
rotation, translation, and scale. By considering random variables A that take their values in G, one
approach to this problem is to learn a representation Z that “ignores” changes in A (Quiroga et al.,
2005; Gens & Domingos, 2014; Bouvrie et al., 2009; Mallat, 2012; Anselmi et al., 2016).
Example 2.5 (Information bottleneck). The information bottleneck (Tishby et al., 2000) is the
problem of finding a representation Z that minimizes the objective I(Z; Y) - λI(Z; X) in an
unsupervised manner. This is closely related to, but not the same as the problem we study, owing to
the invariant attribute A.
3	Feasible Region on the Information Plane
We begin by defining the feasible region associated with the adversarial game (1) and discussing its
relevance to the problem we study. Formally, we define the information plane to be the 2D coordinate
plane with axes -RY (g) and -RA(g), respectively. The feasible region then corresponds to the
two-dimensional region defined by the pairs (-Rγ(g), -RA(g)) over all possible representations
Z = g(X) on the information plane. More concretely, in the classification and regression settings,
these pairs can be given a more intuitive interpretation in terms of mutual information and conditional
variances, respectively. In particular, it is easy to show the following:
1. (Classification) Under cross-entropy loss, using standard information-theoretic identities the
adversarial game (2) can be rewritten as
min	H (Y | Z) - λ∙ H (A | Z) o max	I (Y; Z) - λ ∙ I (A; Z).	(3)
Z=g(X)	Z=g(X)
2. (Regression) Under the least-squares loss, by the law of total variance, the adversarial game (2)
can be rewritten as
min E[Var(Y | Z)] -λ∙E[Var(A	|	Z)]	o max	VarE[Y	|	Z] -λ∙Var E[A	|	Z].	(4)
Z=g(X)	Z=g(X)
These equivalences motivate the following definitions:
(Classification): RCE := {(I(Y; Z), I(A; Z)) ∈ R1 2},
(Regression): RLS := {(Var E[Y | Z], Var E[A | Z]) ∈ R2}.
3
Under review as a conference paper at ICLR 2021
(a) Information plane of classification.
Figure 1: 2D information plane. The shaded area correspond to the feasible region.
We call both RCE and RLS the feasible region for the classification and regression settings, respectively.
See Fig. 1 for an illustration of the information planes and the feasible regions. At this point, it
may not be immediately clear what the relevance of the feasible region is. To see this, recall that
our high-level goal is to find representations Z that maximize accuracy (i.e. ED [`(f (Z), Y )]) while
simultaneously maximizing invariance (i.e. ED['(f 0(Z),A)]), and consider the four vertices (not
necessarily a part of the feasible region) in Fig. 1. These four corners have intuitive interpretations:
•	(Red) The so-called “informationless” regime, in which all of the information regarding
both Y and A is destroyed. This is achieved by choosing a constant representation Z ≡ c.
•	(Yellow) Here, we retain all of the information in A while removing all of the information
about Y . This is not a particularly interesting regime for the aforementioned applications.
•	(Blue) The full information regime, where Z = X and no information is lost. This is the
“classical” setting, wherein information about A is allowed to leak into Y .
•	(Green) This is the ideal representation that we would like to attain: we preserve all the
relevant information about Y while simultaneously removing all the information about A.
Unfortunately, in general, the ideal representation may not be attainable due to the potential correlation
between Y and A. As a result, we are interested in characterizing how “close” we can get to attaining
this ideal transformation given the distribution over (X, A, Y ). More precisely, we can describe the
various extremal points on the boundary of the feasible region as follows:
•	EY: This point corresponds to a representation Z that maximizes accuracy subject to a hard
constraint on the invariance (cf. (5),(10)), i.e. there is no leakage of information about A into
the representation Z. In classification, we enforce this via the mutual information constraint
I(A; Z) = 0 and in regression via the conditional variance constraint Var E[A | Z] = 0.
•	EA: This point corresponds to a representation Z that maximizes invariance subject to a
hard constraint on the accuracy (cf. (8), (12)), i.e. there is no loss of information about Y in
the representation Z. In classification, we enforce this via the mutual information constraint
I(Y ; Z) = H(Y ) and in regression we enforce this via the conditional variance constraint
Var E[Y | Z] = Var(Y ).
•	As We vary λ ∈ (0, ∞), We carve out a path OPT(λ) between EY and EA that corresponds
to the optimal values of (1). This is the Pareto frontier of the accuracy-invariance tradeoff,
and represents the best possible tradeoff attainable for a given λ.
Due to the symmetry betWeen Y and A in (2), the feasible regions in both cases are symmetric With
respect to the diagonal of the bounding rectangle. With the feasible region more clearly exposed, We
can noW concretely outline our objective: To analytically characterize the solutions to the extremal
problems corresponding to the loWer and upper right points on the boundaries, and to provide loWer
bounds on the objective OPT(λ). Due to the page limit, We defer all the detailed proofs to appendix
and mainly focus on providing interpretations and insights of our results in the main paper.
4	Classification
In order to understand the tradeoff betWeen these tWo competing goals, it is the most interesting to
study the case Where the original input X contains full information to predict both Y and A, so that
4
Under review as a conference paper at ICLR 2021
any loss of accuracy is not due to the noninformative input X . To this end, our following analysis
focuses on the noiseless setting1:
Assumption 4.1. There exist functions fγ(∙) and fA(∙), such that Y = fY(X) and A = fA(X).
In order to characterize the feasible region RCE, first note that from the data processing inequality,
the following inequalities hold:
0 ≤ I (Y; Z) ≤ I (Y; X) = H(Y),	0 ≤ I (A； Z) ≤ I (A； X) = H(A),
which means that for any transformation Z = g(X), the point (I(Y; Z), I(A; Z)) must lie within a
rectangle shown in Fig. 2a. The following lemma shows that the feasible region RCE is convex:
Lemma 4.1. RCE is convex.
Here, the convexity of RCE is guaranteed by a construction of randomized feature transformation. As
we briefly discussed before, we know that two vertices of the bounding rectangle are attainable, i.e.,
the “informationless” origin and the “full information” diagonal vertex. Now with Lemma 4.1, it is
clear that all the points on the diagonal of the bounding rectangle are also attainable.
4.1	Maximal Mutual Information under the Independence Constraint
In this section We explore the extremal point EY. This means that We would like to maximize the
mutual information of Z w.r.t Y and simultaneously being independent ofA:
mZax I(Y; Z),	subject to I(A; Z) = 0.
(5)
First of all, realize that the optimal solution of (5) clearly depends on the coupling betWeen A and Y.
To see this, consider the folloWing tWo extreme cases:
Example 4.1.	If A = Y almost surely, then I(A; Z) = 0 directly implies I(Y; Z) = 0, hence
maxZ I(Y; Z) = 0 under the constraint that I(A; Z) = 0.
Example 4.2.	If A ⊥ Y, then Z = fY (X) = Y satisfies the constraint that I (A; Z) = I (A; Y) = 0.
Furthermore, I(Y; Z) = I(Y; Y) = H(Y) ≥ I(Y; Z0), ∀Z0 6= Y. Hence maxZ I(Y; Z) = H(Y).
The above tWo examples shoW that the optimal solution of (5), if exists analytically, must include a
quantity that characterizes the dependency betWeen A and Y. We first define such a quantity:
Definition 4.1. Define ∆γ∣A := | PrD(Y = 1 | A = 0) 一 PrD(Y = 1 | A = 1)|.
It is easy to verify that the following claims hold about Δy∣a:
0 ≤ ∆γ।a ≤ 1, and ∆γ∣A = 0 ^⇒ A ⊥ Y, and ∆γ∣a = 1 ^⇒ A = Y or A = 1 一 Y. (6)
With this introduced notation, the following theorem gives an analytic solution to (5).
Theorem 4.1.	The optimal solution of optimization problem (5) is
maχ I (Y; Z) = H (Y) - ∆γ∣A ∙ H (A).
Z,I (A;Z)=0
(7)
Let us have a sanity check of this result: First, if A ⊥ Y, then ∆γ |A = 0, and in this case
the optimal solution given by Theorem 4.1 reduces to H (Y) 一 0 ∙ H (A) = H (Y), which is
consistent with Example 4.2. Next, consider the other extreme case where A = Y. In this case
∆γ ∣a = 1 and H (Y) = H (A), therefore the optimal solution given by Theorem 4.1 becomes
H (Y) 一 1 ∙ H (A) = H (Y) 一 H (Y) = 0. This is consistent with Example 4.1.
Moreover, due to the symmetry between A and Y, we can now characterize the locations of the two
extremal points on the lower and left boundaries. The updated figure is plotted in Fig. 2b. In Fig. 2b
Δa∣ γ is defined analogously as ∆γ∣A by swapping Y and A.
4.2 Minimum Mutual Information under the Sufficient Statistics Constraint
Next, we characterize the other extremal point, i.e. EA Again, by the symmetry between A and Y, it
suffices to solve the following optimization problem, whose optimal solution is EA.
mZin I(A; Z), subject to I(Y; Z) = H(Y)
(8)
1Extensions to the general noisy setting are feasible, but the results are less interpretable. Hence we mainly
focus on the noiseless setting in this paper.
5
Under review as a conference paper at ICLR 2021
I(A; Z)
(a) Rectangle bounding box.
I(A; Z)
I(A;Z)
Figure 2: Information plane in classification. Shaded area corresponds to the known feasible region.
(b) Maximal I(∙, ∙) under the independence constraint.
I(A; Z)
(d) The convex polygon characterization of RCE.
Theorem 4.2.	The optimal solution of optimization problem (8) is
min
Z,I(Y;Z)=H(Y)
I(A;Z)=I(A;Y).
(9)
Clearly, if A and Y are independent, then the gap I (A; Y ) = 0, meaning that we can simultaneously
preserve all the target related information and filter out all the information related to A. With the
above result, we can now characterize the locations of the remaining two extremal points on the top
and right boundaries of bounding rectangle. The updated figure is shown in Fig. 2c.
4.3 THE INFORMATION PLANE IN LEARNING REPRESENTATIONS Z
To get the full picture, we combine our results in Section 4.1 and Section 4.2 and use the fact that
RCE must be convex (Lemma 4.1). This allows us to complete the analysis by connecting the black
dots on the four boundaries of the bounding rectangle, as shown in Fig. 2d. The feasible region RCE
is a convex polygon. Furthermore, both the constrained accuracy optimal solution and the constrained
invariance optimal solution can be readily read from Fig. 2d as well.
As we mentioned before, ideally we would like to find a representation Z that attains the green
vertex of the bounding rectangle. Unfortunately, due to the potential coupling between Y and A, this
solution is not always feasible. Nevertheless, it is instructive to see the gaps between the optimal
solutions we could hope to achieve and the ideal one:
•	Maximal information: The gap is given by ∆γ∣A ∙ H (A). On one hand, if A ⊥ Y, then
∆γ ∣a = 0 sothe gap is 0. On the other hand, if A = Y ,then ∆γ∣A = 1 and H (A) = H (Y),
so the gap achieves the maximum value H(Y).
•	Maximal invariance: The gap is given by I(A; Y). On one hand, if A ⊥ Y, then I(A; Y) =
0 so the gap is 0. On the other hand, if A = Y, then I(A; Y) = H (A), so again, the gap
achieves the maximum value of H(A).
One open question that we do not answer here is whether the feasible region RCE is strictly convex
or not. That is, whether the Pareto-frontier between EY and EA is strictly convex or not? On the
other hand, for each value of λ, the line segment connecting EY and EA forms a lower bound of
6
Under review as a conference paper at ICLR 2021
Var E[A | Z]
Var E [Y | Z]
Var(Y)
(a)	Rectangle bounding box.
Var E[A | Z]
Var(A)
Var(A).〈y㈤
TaaF
Var(Y )<a,y>2
Tyyp
→ Var E [Y | Z]
Var(Y)
r
Va
Var E[A | Z]
73 W
∕2<y,Σa> _
(^aar -
Var(A)∙
〈a,a
〈,a,y〉
Var E[Y | Z]
% a,y〉Var(Y)
(b)	Maximal Variance.
Var E[A | Z]
Var(A)S
Var(A) ・〈ya>
⅞3
•H A∙≡>
I 〈»•»)
I Gq-L .
Var(Y )<a,y>2
3) (a,y)Var(Y)
Z ≡ C I 2<y,Σa> - Var(A)∙
∖ S,a>	〈a,a；
Tyyp
→ Var E [Y | Z]
Z ≡ C
Z = X
"W
a∙3∙( A)Je >
Z
2
TaaT
2
Z = X
(c) Minimum Variance.	(d) The convex polygon characterization of RLS.
Figure 3: Information plane in regression. Shaded area corresponds to the known feasible region.
the Lagrangian. For the aforementioned applications, our approximation of the frontier is critical
in order to be able to certify that a given model is not optimal. For example, given some practically
computed representation Z and by using known optimal estimators of the mutual information, it is
possible to estimate I(A; Y ), I(Y ; Z), I(A; Z) in order to directly bound the (sub)-optimality of Z
using Fig. 2d, i.e., how far away the point (I (Y; Z), I (A; Z)) is from the line segment between EY
and EA. This distance lower bounds the distance to the optimal representations on the Pareto frontier.
5 Regression
Similar to what we have before, in regression we assume a noiseless setting for better interpretability
of our results. The generalization to the noisy setting is included in the appendix. Let H be an RKHS.
Assumption 5.1. There exist functions fγ, fA ∈ H, such that Y = fY(X) and A = fA(X).
Let〈•，∙i be the canonical inner product in RKHS H. Under this assumption, there exists a feature
map 夕(X) and a = 0,y = 0, such that Y = fYY(X)=(夕(X), y〉and A = fA(X)=(夕(X), a).
This feature map does not have to be finite-dimensional, and our analysis works for the case where
fY, fA are infinite-dimensional. Next, by the law of total variance, the following inequalities hold:
0 ≤ Var E[Y | Z] ≤ Var E[Y | X] = Var(Y),	0 ≤ Var E[A | Z] ≤ Var E[A | X] = Var(A),
which means that for any transformation Z = g(X), the point (Var E[Y | Z], Var E[A | Z]) must
lie within a rectangle shown in Fig. 3a. To simplify the notation, we define Σ := Cov(夕(X),夕(X))
to be the covariance operator of 夕(X).
Again, if we consider all the possible feature transformations Z = g(X), then the points (Var E[Y |
Z], Var E[A | Z]) will form a feasible region RLS. Similar to what we have in classification, the
following lemma shows that the feasible region RLS is convex:
Lemma 5.1. RLS is convex.
The convexity of RLS is guaranteed by a construction of randomized feature transformation. Similarly,
both the “informationless” origin and the “full information” diagonal vertex are attainable.
7
Under review as a conference paper at ICLR 2021
5.1	The B ounding Vertices on the Plane
In this section We explore the extremal point EY and EA for regression. For EY, this means that We
would like to maximize the variance of Z w.r.t Y and simultaneously minimizing that of A:
mZax	Var E[Y	|	Z],	subject to	Var E[A	|	Z]	=	0.	(10)
It is clear that the optimal solution of (10) depends on the coupling betWeen A and Y , and the
folloWing theorem precisely characterizes this relationship:
Theorem 5.1.	The optimal solution of optimization problem (10) is upper bounded by
max	VarE[Y | Z] ≤ Var(Y) — (2夕,乎—Var(A) T yi ) (%夕〉.	(11)
Z,Var E[A∣Z]=0	L ∣ 」一 ' '卜(a,a)	{a, αi2	) C	6 7
Again, let us sanity check this result: First, if ais orthogonal to y, i.e., ha, yi = 0, then the gap
is 0, and the optimal solution becomes Var(Y). Next, consider the other extreme case Where a is
parallel to y. In this case it can be readily verified that the optimal solution reduces to 0. With these
tWo results, We can noW characterize the locations of the tWo extremal points on the bottom and left
boundaries of bounding rectangle. The updated figure is plotted in Fig. 3b.
Similarly, for EA, it suffices to solve the following problem, whose optimal solution is EA:
min Var E[A | Z],	subject to Var E[Y | Z] = Var(Y)
Theorem 5.2.	The optimal solution of optimization problem (12) is lower bounded by
min
Z,Var E[Y∣Z]≥Var(Y)
、丁 Var . r7.. Var(Y) ∙ ha, y)2
VarE[A | Z] ≥ ——屋川.
hy, yi2
(12)
(13)
Again, if a is orthogonal to y, then the optimal solution is 0, meaning that we can simultaneously
preserve all the target variance and filter out all the variance related to A. On the other hand, if a is
parallel to y, then Var(Y) ∙ ha, y)2/hy, y)2 = Var(A). The updated plot is shown in Fig. 3c.
5.2 A Spectral Lower Bound of the Lagrangian
Combining our results in Thm. 5.1 and Thm. 5.2, along with the fact that RLS must be convex, we
plot the full picture about the feasible region in the regression setting in Fig. 3d. Both the constrained
accuracy optimal solution and the constrained invariance optimal solution can be readily read from
Fig. 3d as well. In fact, in the regression setting, we can say even more: We can derive a tight lower
bound to the Lagrangian problem OPT(λ) := minz=g(x)E[Var(Y | Z)] 一 λ ∙ E[Var(A | Z)].
Theorem 5.3. The optimal solution of the Lagrangian has the following lower bound:
OPT(λ) ≥ 2 {Var(Y) 一 λ ∙ Var(A) 一 P(Var(Y) + λ∙ Var(A))2 - 4λ(y, Σa)2 } .	(14)
Evidently, the key quantity in the lower bound (14) is the quadratic term hy, Σai, which effectively
measures the dependence between the Y and A under the feature covariance Σ.
The proof of Thm. 5.1, 5.2 and 5.3 rely on a finite-dimensional SDP relaxation, and constructs an
explicit optimal solution to this relaxation. We re-formulate the objective as a linear functional of
V := Cov(E[夕(X) | Z], E[夕(X) | Z]), which satisfies the semi-definite constraint 0 W V W Σ =
Cov(夕(X),夕(X)). Therefore, the optimal value of the SDP is an upper/lower bound of the objective.
Furthermore, we show that under certain regularity conditions, the SDP relaxation is exact. One
particularly interesting setting where the regularity condition holds is when 夕(X) follows a Gaussian
distribution. More discussions about the tightness of our bounds are presented in appendix C.5.
6 Conclusion
We provide an information plane analysis to study the general and important problem for learning
invariant representations in both classification and regression settings. In both cases, we analyze the
inherent tradeoffs between accuracy and invariance by providing a geometric characterization of the
feasible region on the information plane, in terms of its boundedness, convexity, as well as its its
extremal vertices. Furthermore, in the regression setting, we also derive a tight lower bound that for the
Lagrangian form of accuracy and invariance. Given the wide applications of invariant representations
in machine learning, we believe our theoretical results could contribute to better understandings
of the fundamental tradeoffs between accuracy and invariance under various settings, e.g., domain
adaptation, algorithmic fairness, invariant visual representations, and privacy-preservation learning.
8
Under review as a conference paper at ICLR 2021
References
Fabio Anselmi, Lorenzo Rosasco, and Tomaso Poggio. On invariance and selectivity in representation
learning. Information and Inference: A Journal ofthe IMA, 5(2):134-158, 2016.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175, 2010.
Jake Bouvrie, Lorenzo Rosasco, and Tomaso Poggio. On invariance in hierarchical models. In
Advances in Neural Information Processing Systems, pp. 162-170, 2009.
Maximin Coavoux, Shashi Narayan, and Shay B Cohen. Privacy-preserving neural representations
of text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pp. 1-10, 2018.
Sanghamitra Dutta, Dennis Wei, Hazar Yueksel, Pin-Yu Chen, Sijia Liu, and Kush R Varshney. An
information-theoretic perspective on the relationship between fairness and accuracy. arXiv preprint
arXiv:1910.07870, 2019.
Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Frangois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Robert Gens and Pedro M Domingos. Deep symmetry networks. In Advances in neural information
processing systems, pp. 2537-2545, 2014.
Jihun Hamm. Preserving privacy of continuous high-dimensional data with minimax filters. In
Artificial Intelligence and Statistics, pp. 324-332, 2015.
Jihun Hamm. Minimax filter: learning to preserve privacy from inference attacks. The Journal of
Machine Learning Research, 18(1):4704-4734, 2017.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In International conference on machine learning, pp. 3020-3029, 2016.
Fredrik D Johansson, Uri Shalit, Nathan Kallus, and David Sontag. Generalization bounds and
representation learning for estimation of potential outcomes and causal effects. arXiv preprint
arXiv:2001.07426, 2020.
Stephane Mallat. Group invariant scattering. Communications on Pure and Applied Mathematics, 65
(10):1331-1398, 2012.
Daniel McNamara, Cheng Soon Ong, and Robert C Williamson. Costs and benefits of fair representa-
tion learning. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp.
263-270, 2019.
Aditya Krishna Menon and Robert C Williamson. The cost of fairness in binary classification. In
Conference on Fairness, Accountability and Transparency, pp. 107-118, 2018.
R Quian Quiroga, Leila Reddy, Gabriel Kreiman, Christof Koch, and Itzhak Fried. Invariant visual
representation by single neurons in the human brain. Nature, 435(7045):1102-1107, 2005.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: general-
ization bounds and algorithms. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 3076-3085. JMLR. org, 2017.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.
arXiv preprint arXiv:1703.00810, 2017.
9
Under review as a conference paper at ICLR 2021
Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 2000.
Taihong Xiao, Yi-Hsuan Tsai, Kihyuk Sohn, Manmohan Chandraker, and Ming-Hsuan Yang.
Adversarial learning of privacy-preserving and task-oriented representations. arXiv preprint
arXiv:1911.10143, 2019.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 335-
340, 2018.
Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. In Advances in
neural information processing systems, pp. 15675-15685, 2019.
Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J
Gordon. Adversarial multiple source domain adaptation. In Advances in neural information
processing systems, pp. 8559-8570, 2018.
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. arXiv preprint arXiv:1901.09453, 2019a.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. arXiv preprint arXiv:1910.07162, 2019b.
10
Under review as a conference paper at ICLR 2021
A	Proofs for Claims in Section 3
In this section we give detailed arguments to derive the objective functions of Eq. (3) and (4)
respectively from the original minimax formulation in Eq. (1). First, let us consider the classification
setting.
Classification Given a fixed feature map Z = g(X), due to the symmetry between Y and A in
Eq.(1), it suffices for Us to consider the case of finding f that minimizes ED['(f ◦ g(X), Y)], and
analogous result follows for the case of finding the optimal f0 that minimizes ED [`(f 0 ◦ g(X), A)]
similarly. By definition of the cross-entropy loss, we have:
ED['(f ◦ g(X), Y)] = -ED [I(Y = 0) log(1 - f (g(X))) + I(Y = 1) log(g(f (X)))]
= -ED [I(Y = 0) log(1 - f(Z)) + I(Y = 1) log(f(Z))]
= -EZEY [I(Y=0)log(1-f(Z))+I(Y= 1)log(f(Z)) | Z]
=-EZ[Pr(Y=0 | Z)log(1-f(Z))+Pr(Y=1 | Z) log(f(Z))]
= EZ [DKL(Pr(Y | Z) kf(Z))]+H(Y| Z)
≥ H (Y | Z).
It is also clear from the above proof that the minimum value of the cross-entropy loss is achieved
when f(Z) is a randomized classifier such that E[f (Z)] = Pr(Y = 1 | Z). This shows that
mfin ED ['(f ◦ g(X ),Y)] = H (Y | Z), and mfin ED ['(f0 ◦ g(X ),A)] = H (A | Z).
To see the second part of Eq. (3), simply use the identity that H(Y | Z) = H(Y) - I(Y; Z) and
H(A | Z) = H(A) - I(A; Z) with the fact that both H(Y) and H(A) are constants that only depend
on the joint distribution D.
Regression Again, given a fixed feature map Z = g(X), because of the symmetry between Y and
A let us focus on the analysis of finding f that minimizes ED ['(f ◦ g(X), Y)]. In this case since
'(∙, ∙) is the mean squared error, it follows that
Ed['(f ◦ g(X), Y)] = Ed [(f ◦ g(X) — Y)2]
= ED (f(Z) - Y)2
=EZ [(f(Z) - E[Y | Z])2] +EZ[EY[(Y-E[Y|Z])2]]
≥EZ [EY[(Y-E[Y|Z])2]]
= E[Var(Y | Z)],
where the third equality is due to the Pythagorean theorem. Furthermore, it is clear that the optimal
mean-squared error is obtained by the conditional mean f(Z) = E[Y | Z]. This shows that
mfinED['(f ◦ g(X), Y)] = E[Var(Y | Z)], and mfi0n ED ['(f 0 ◦ g(X), A)] = E[Var(A | Z)].
For the second part, use the law of total variance Var(Y) = E[Var(Y | Z)] + Var(E[Y | Z]) and
Var(A) = E[Var(A | Z)] + Var(E[A | Z]). Realizing that both Var(Y) and Var(A) are constants
that only depend on the joint distribution D, we finish the proof.
B Missing Proofs in Classification (Section 4)
In what follows we first restate the propositions, lemmas and theorems in the main text, and then
provide the corresponding proofs.
B.1	CONVEXITY OF RCE
Lemma 4.1. RCE is convex.
Proof. Let Zi = gi(X) for i ∈ {0, 1} with corresponding points (I(Y; Zi), I(A; Zi)) ∈ RCE.
Then we only need to prove that for ∀u ∈ [0, 1], (uI(Y; Z0) + (1 - u)I(Y; Z1), uI(A; Z0) + (1 -
11
Under review as a conference paper at ICLR 2021
U)I(A; Zι)) ∈ RCE as well. For any U ∈ [0,1], let S 〜U(0,1), the uniform distribution over (0,1),
such that S ⊥ (Y, A). Consider the following randomized transformation Z:
Z0 If S ≤ U,
Z1 otherwise.
(15)
To compute I(Y ; Z), we have:
I(Y;Z)=E[I(Y;Z|S)]
=Pr(S ≤ U) ∙ I (Y; Zo) + Pr(S > U) ∙ I (Y; Zι)
=UI(Y;Z0)+(1 -U)I(Y;Z1).
Similar argument could be used to show that I(A; Z) = UI(A; Z0) + (1 - U)I(A; Z1). So by
construction we now find a randomized transformation Z = g(X) such that (UI(Y; Z0) + (1 -
U)I(Y;Z1),UI(A;Z0)+(1-U)I(A;Z1)) ∈ RCE.
B.2	Proof of Theorem 4.1
We proceed to provide the proof that the optimal value of (5) is the one given by Theorem 4.1.
Theorem 4.1. The optimal solution of optimization problem (5) is
maχ I (Y; Z) = H (Y) - ∆γ∣A ∙ H (A).	⑺
Z,I (A;Z)=0
Proof. For a joint distribution D over (X, A, Y) and a function g : X → Z, in what follows we use
g]D to denote the induced distribution of D under g over (Z, A, Y). We first make the following
claim: without loss of generality, for any joint distribution g]D over (Z, A, Y), we could find
(Zo, A0, Y0)〜g]D and a deterministic function f, such that Y0 = f (A0, Zo, S) where S 〜 U(0,1),
S ⊥ (A0, Z0) and I(Y0; Z0) ≥ I(Y; Z) with Z0 = (Z0, S). To see this, consider the following
construction:
A0,Zo 〜D(A,Z), S 〜U(0,1).
Let (a, z, s) be the sample of the above sampling process and construct
Y0 = 1 If s ≤ E[Y | A = a, Z = z],
0 Otherwise.
Now it is easy to verify that (Z0, A0, Y0) 〜 g]D and Pr(Y 0 = 1 | A0 = a, Z0 = z) = E[Y | A =
a, Z = z]. To see the last claim, we have the following inequality hold:
I (Y0; Z 0) = I (Y0; Zo, S) ≥ I (Y; Zo) = I (Y; Z).
Now to upper bound I(Y; Z), we have
I(Y;Z)=H(Y)-H(Y|Z),
hence it suffices to lower bound H(Y | Z). To this end, define
Do ：= {z,ε ∈ (0,1) | f(0,z,ε) = 1},
D1 := {z,ε ∈ (0, 1) | f(1, z, ε) = 1}.
Then,
Pr((z, ε) ∈ D0) = Pr(f (0, z, ε) = 1)
=Pr(f(0,z,ε)=1 |A=0)
=Pr(f(A,z,ε)=1 |A=0)
= Pr(Y = 1 | A = 0).
Analogously, the following equation also holds:
Pr((z, ε) ∈ D1) = Pr(Y = 1 | A= 1).
12
Under review as a conference paper at ICLR 2021
Without loss of generality, assume that Pr(Y = 1 | A = 1) ≥ Pr(Y = 1 | A = 0), then
Pr((z, ε) ∈ D1\D0) ≥ Pr((z, ε) ∈ D1) - Pr((z, ε) ∈D0)
= | Pr(Y = 1 | A = 1) - Pr(Y = 1 | A = 0)|.
But on the other hand, we know that if (z, ε) ∈ D1\D0, then f(1, z, ε) = 1 and f(0, z, ε) = 0, and
this implies that Y = A, hence:
H(Y | Z) ≥ H(Y | Z,S)
=Pr((z, ε)	∈ Dι∖Do)	∙ H (Y	|	(z, ε) ∈ D1∖D0)	+ Pr((z, ε)	∈ D1∖D0)	∙ H (Y	| (z, ε)	∈ D1∖D0)
≥ Pr((z, ε)	∈ Dι∖Do)	∙ H (Y	|	(z, ε) ∈ D1∖D0)
= Pr((Z,ε) ∈ Dι∖Do) ∙ H(A)
≥ | Pr(Y =1 | A = 1) - Pr(Y =1 | A = 0)| ∙ H(A),
which implies that
I (Y; Z) ≤ H (Y) - | Pr(Y = 1 | A =1) - Pr(Y = 1 | A = 0)∣∙ H (A) = H (Y) - ∆γ ∣a ∙ H (A).
To see that the upper bound could be attained, let us consider the following construction. Denote
a := Pr(Y = 1 | A = 0) and β := Pr(Y = 1 | A = 1). Construct a uniformly random Z 〜 U(0,1)
and then sample A independently from Z according to the corresponding marginal distribution of A
in D. Next, define:
Y= 1 ifZ ≤ α∧A=0orZ ≤β∧A= 1,
0 otherwise.
It is easy to see that Z ⊥ A by construction. Furthermore, by the construction of Y, we also
have A, Y 〜D(A, Y) hold. Since I (Y; Z) = H (Y) - H (Y | Z), We only need to verify
H (Y | Z) = ∆γ ∣a ∙ H (A) in this case. Assume without loss of generality α ≤ β, there are three
different cases depending on the value of Z :
•	Z ≤ α: In this case no matter what the value of A, we always have Y = 1.
•	Z > β: In this case no matter what the value of A, we always have Y = 0.
•	α < Z ≤ β: In this case Y = A, hence the conditional distribution of Y given Z ∈ (α, β]
is equal to the conditional distribution of A given Z ∈ (α, β]. But by our construction, A
is independent of Z, which means that in this case the conditional distribution of A given
Z ∈ (α, β] is just the distribution of A.
Combine all the above three cases, we have:
H (Y | Z) = Pr(Z ≤ α) ∙ H (Y | Z ≤ α) + Pr(Z > β) ∙ H (Y | Z >β)+ Pr(α < Z ≤ β) ∙ H (Y | α<Z ≤ β)
= 0 + 0+ |e — a| ∙ H (A | α < Z ≤ β)
=| Pr(Y = 1 | A = 1) - Pr(Y = 1 | A = 0)| ∙ H (A)
= δy∣a ∙ H(A),
which completes the proof.
B.3 Proof of Theorem 4.2
Theorem 4.2. The optimal solution of optimization problem (8) is
min	I(A; Z) = I(A; Y).
Z,I(Y;Z)=H(Y)
(9)
Proof. First, realize that H(Z) ≥ I(Y; Z) = H(Y) by our constraint. Furthermore, we also know
that 0 ≤ H(Y | Z,A) ≤ H(Y | Z) = H(Y) - I(Y; Z) = 0, which means H(Y | Z,A) = 0. With
these two observations, we have:
I(A; Z) = H(Z) -H(Z | A)
≥ H(Y) - H(Z | A)
≥ H(Y) -H(Y,Z | A)
= H(Y) - H(Y | A) - H(Y | Z, A)
= H(Y) - H(Y | A)
=I(A;Y).
To attain the equality, simply set Z = fγ (X) = Y. Specifically, this implies that 1-bit is sufficient to
encode all the information for the optimal solution, which completes the proof.
13
Under review as a conference paper at ICLR 2021
C Missing Proofs in Regression (Section 5)
C.1 CONVEXITY OF RLS
Analogous to the classification setting, here we first show that the feasible region RLS is convex:
Lemma 5.1. RLS is convex.
Proof. Let Zi = gi(X) for i ∈ {0, 1} with corresponding points (Var E[Y | Zi], Var E[A | Zi]) ∈
RLS. Then it suffices if we could show for ∀u ∈ [0, 1], (u Var E[Y | Z0] + (1 - u) Var E[Y |
Z1], u Var E[A | Z0]) + (1 - u) Var E[A | Z1]) ∈ RLS as well.
We give a constructive proof. Due to the symmetry between A and Y , we will only prove the result for
Y, and the same analysis could be directly applied to A as well. For any U ∈ [0,1], let U 〜 U(0,1),
the uniform distribution over (0, 1), such that U ⊥ (Y, A). Consider the following randomized
transformation Z :
Z0 If U ≤ u,
Z1 otherwise.
(16)
To compute Var E[Y | Z], define K := E[Y | Z], then by the law of total variance, we have:
Var E[Y | Z] = Var(K) = E[Var(K | U)] + Var E[K | U].
We first compute Var E[K | U]:
Var E[K | U] = Var E[E[Y | Z] | U]
= Var E[Y | U]	(The law of total expectation)
= Var E[Y]	(Y ⊥ U)
= 0.
On the other hand, for E[Var(K | U)], we have:
E[Var(K | U)] = Pr(U = 0) ∙ Var(K | U = 0) + Pr(U = 1) ∙ Var(K | U = 1)
=u ∙ Var(K | U = 0) + (1 - U) ∙ Var(K | U =1)
=u ∙ VarE[Y | Zo] + (1 - u) ∙ VarE[Y | Zι].
Combining both equations above yields:
Var E[Y | Z] = U ∙ Var E[Y | Z0] + (1 - u) ∙ Var E[Y | ZJ
Similar argument could be used to show that Var E[A | Z] = U ∙ Var E[A | Zo] + (1 - u) ∙
Var E[A | Z1]. So by construction we now find a randomized transformation Z = g(X) such that
(u ∙ Var E[Y | Zo] + (1 - u) ∙ Var E[Y | Zι],u ∙ Var E[A | Zo] + (1 - u) ∙ Var E[A | Zι]) ∈ Rls,
which completes the proof.
C.2 Proof of Theorem 5.1 and Theorem 5.2
In this section, we will prove Theorem 5.1 and Theorem 5.2. We will provide proofs to both
theorems in a generalized noisy setting, i.e., we no longer assume the noiseless condition so that the
corresponding theorems in the noiseless setting follow as a special case. To this end, we first re-define
fY(X) := E[Y | X]	(17)
fA(X) := E[A | X]	(18)
and fY, fA ∈ H. We reuse the notations a, y to denote
fY (X )= E[Y X ] = hy"(X )i	(19)
fA (X )= E[A∣X ] = ha"(X )i.	(20)
It is easy to see that the noiseless setting is indeed a special case where Y = E[Y|X], A = E[A|X]
almost surely.
For readers’ convenience, we restate the Theorem 5.1 below:
14
Under review as a conference paper at ICLR 2021
Theorem 5.1. The optimal solution of optimization problem (10) is upper bounded by
max	Var E[Y | Z] ≤ Var(Y) — (2夕,乎—Var(A) ：ha, yi )8喻.	(11)
Z,Var E[A|Z]=0	ha, ai	ha, ai2
The following theorem is the generalized version of Theorem 5.1 in noisy setting:
Theorem C.1. The optimal solution of optimization problem (10) is upper bounded by
max	Var E[Y | Z] ≤ Var(E[Y|X]) -
Z,VarE[A|Z]=0
(2y, Σai _ Var(E[A|X]) ∙(a,y> A /	、
Vha,ai	ha,ai2	J ha,yi
(21)
It is easy to see Theorem 5.1 is an immediate corollary of this result: under the noiseless assumption,
we have Var(E[Y|X]) = Var(Y) and Var(E[A|X]) = Var(A).
Proof. Using the law of total expectation,
E[Y ∣ Z] = E [E[Y ∣ X] ∣ Z] = J E[Y ∣ X, Z] ∙ P(X ∣ Z) dX.
Since Z = g(X) is a function of X, We have Z ⊥ Y ∣ X ,so E[Y ∣ X, Z ] = E[Y ∣ X ] = fY (X).
Therefore,
E[Y∣Z] = J E[Y ∣ X, Z] ∙ p(X ∣ Z) dX
=JX fY (X) ∙ p(X ∣ Z) dX
E[fY(X) I Z].
Hence,
Var(E[Y | Z]) = Var(E[fY(X) | Z]).
(22)
Therefore,
Var E[Y ∣ Z] = Var(E[fY(X) ∣ Z])
= VarEKy“(X)i I Z]
=Var(y, E[夕(X) ∣ ZD	(Linearity of Expectation)
=(y, Cov(E[g(X) ∣ Z], Eb(X) ∣ Z])y).
Similarly, for A =〈a,夕(X)i,we have:
Var E[A ∣ Z] =〈a, Cov(E[夕(X) ∣ Z], E[夕(X) ∣ Z])a).
To simplify the notation, define V := Cov(E[夕(X) ∣ Z], E[夕(X) ∣ Z]). Then again, by the law
of total variance, it is easy to verify that 0 W V W Σ = Cov(夕(X),夕(X)). Hence the original
maximization problem could be relaxed as follows:
mZax 〈y, Vyi,	subject to 0 W V W Σ, 〈a, V ai = 0.
To proceed, we first decompose y orthogonally w.r.t. a:
y=y⊥a + yka,
where y⊥a is the component of y that is perpendicular to aand yka is the parallel component of yto
a. Using this orthogonal decomposition, we have ∀V:
(y,Vyi = h(y⊥a + yka),V(y⊥a + yka)i
=hy⊥a,Vy⊥ai	(V 1/2yka = 0)
≤ hy⊥a, ∑y⊥ai	(V W ∑),
where the equality above can be attained by choosing V so that the corresponding eigenvalues of V
along the direction of y⊥a coincide with those of ∑. Note that this is also feasible since the constraint
15
Under review as a conference paper at ICLR 2021
of eigenvalues being 0 only applies to the direction yka, which is orthogonal to y⊥a. To complete the
proof, realize that the vector y⊥a could be constructed as follows:
y⊥a = (I - a0a0T)y,
where a0 = a/kak is the unit vector of a. The last step is to simplify the above equation as:
hy⊥a, Σy⊥a, i = h(I - a0a0T)y,Σ(I - a0a0T)yi
(2hy Σai	Var(E[A | X]) ∙(a心
=Var(E[Y | XD-(iaa------------------E------------) ha,yi,
by using the fact that Var(E[Y | X]) = hy, Σyi and Var(E[A | X]) = ha, Σai.
To ShoW When the equality is attained, let V * be the optimal solution of (??), which could be Con-
structed by first eigendecomposing Σ and then set all the eigenvalues of Σ to 0 whose corresponding
eigenvectors are not orthogonal to a. It is Worth pointing out that V* is positive semidefinite but not
necessarily invertible. Nevertheless, We could still define the projection matrix of V* that projects to
the column space of V* as folloWs:
PV* := V*(V*TV*)tV*T,
where Qt denotes the Moore-Penrose pseudoinverse of matrix Q. With PV*, it is easy to verify that
the optimal transformation is given by Z such that
E[g(X) | Z] = PV* 夕(X).
To see this, we have:
Cov(E[夕(X) | Z], Eb(X) | Z]) =Var E[g(X) | Z]
= Var(PV * 夕(X))
=PV* VarQ(XXPV*
= PV * ΣPVT*
= V*,
completing the proof.
Next, we will prove Theorem 5.2, restated below:
Theorem 5.2. The optimal solution of optimization problem (12) is lower bounded by
min
Z,Var E[Υ∣Z]≥Var(Υ)
、丁 τrπr . . r7.. Var(Y) ∙ (a, y)2
Var E [A | Z] ≥ —一：;即.
(y, yi2
(13)
The following theorem is the generalized version of Theorem 5.2 in noisy setting:
Theorem C.2. The optimal solution of optimization problem (12) is
min
Z,VarE[Y|Z]=Var(Y)
Var E[A | Z]
Var(E[Y∣X]) ∙ (α,y)2
hy,yi2
(23)
It is easy to see Theorem 5.2 is an immediate corollary of this result: under the noiseless assumption,
we have Var(E[Y|X]) = Var(Y) and Var(E[A|X]) = Var(A).
Proof. Due to the symmetry between Y and A, here we only prove the first part of the theorem. As
in the proof of Theorem 5.1, we have the following identities hold:
Var E[A | Z] = (a, Cov(E[夕(X) | Z], Eb(X) | Z])a},
VarE[Y [ Z] = (y, Cov(E[^(X) | Z], E[g(X) | Z])y).
Again, let V := Cov(E[夕(X) | Z], E[夕(X) | Z]) So that we can relax the optimization problem as
follows:
mZin (a, Vai,	subject to 0 V	Σ, (y, Vyi = Var(Y) = (y, Σyi.
16
Under review as a conference paper at ICLR 2021
To proceed, we first decompose a orthogonally w.r.t. y:
a = a⊥y + aky,
where a⊥y is the component of a that is perpendicular to y and aky is the parallel component of a to
y. Using this orthogonal decomposition, we have ∀V :
ha,Vai = h(a⊥y + aky), V (a⊥y + aky)i
≥haky,Vakyi,	(V	0),
where the equality could be attained by choosing V such that V1/2a⊥y = 0. On the other hand, it is
clear that
aky = ha,yoi ∙ yo,
where yo = y∕∣∣y∣∣ is the unit vector of y. Plug aky =〈a, y。)∙ yo into haky, Vaky〉with the fact that
hy, Vyi = Var(E[Y |X]) = hy, Σyi, we get
haky,Vakyi = Var(E[YX"ha，")2 .
hy, yi2
Again, to attain the equality, we should first construct the optimal V * matrix by eigendecomposing
Σ. Specifically, this time we set all the eigenvalues of Σ whose corresponding eigenvectors are
perpendicular to y to 0. Similar to what we argue in the proof of Theorem 5.1, V* is positive
semidefinite but not necessarily invertible. Nevertheless, we could still define the projection matrix
of V* that projects to the column space of V* as follows:
PV* := V*(V*TV*)tV*T,
where Qt denotes the Moore-Penrose pseudoinverse of matrix Q. With PV*, it is easy to verify that
the optimal transformation is given by Z such that
E[2(X) | Z] = PV*夕(X).
To see this, we have:
Cov(E[夕(X) | Z], Eb(X) | Z]) =Var Eb(X) | Z]
= Var(PV * 夕(X))
=PV* VarQ(XXPV*
= PV * ΣPVT*
= V*,
completing the proof.
C.3 Proof of Theorem 5.3
To prove Theorem 5.3, we first introduce the following decompositions of the loss functions:
The following lemma is a more refined version of the Data-Processing Inequality, which gives an
exact characterization of the Bayes optimality gap for a given Z. Recall that the Bayes error is
EX[Var[Y|X]].
Lemma C.1 (L2 Error Decomposition).
EZ[Var[Y|Z]] - EX[Var[Y|X]] = EZVar(E[Y|X] |Z) ≥ 0.	(24)
Similarly,
EZ[Var[A|Z]] - EX[Var[A|X]] = EZVar(E[A|X] |Z) ≥ 0.	(25)
Proof. Since Z = g(X) is a function ofX, we havep(y|x) = p(y|x, z), or equivalently, (Y ⊥ Z)|X
By law of total variance,
Var(Y|Z) = EX [Var(Y|X,Z)|Z] +Var(E[Y|X,Z] |Z)
= EX [Var(Y|X)|Z] +Var(E[Y|X] |Z)
17
Under review as a conference paper at ICLR 2021
Taking expectation over Z,
EZ Var(Y |Z)
= EZEX [Var(Y|X)|Z] +EZVar(E[Y|X] |Z)
= EXEZ [Var(Y|X)|Z] +EZVar(E[Y|X] |Z)
= EX Var(Y |X) +EZVar(E[Y |X] |Z) ,
where the last equality is due to the law of total expectation.
The following lemma is a direct consequence of the law of total variance.
Lemma C.2 (L2 Invariance Decomposition).
Var(A) - EZ Var(A|Z) = Var(E[A|Z]) ≥ 0.
We will prove a generalized version of Theorem 5.3 without noiseless assumption, stated below:
Theorem C.3. The optimal solution of the Lagrangian has the following lower bound:
OPT(λ) ≥ 1 {λ Var(E[A|X])+Var(E[YX]) - P(λ Var(E[A∣X]) +Var(E[YX]))2 - 4λ(a, Σy>2}
+ (E[Var(Y|X)] - λ Var(A)).
When the noiseless assumption holds, we have Var(E[A|X]) = Var(A), Var(E[Y|X]) = Var(Y),
and E[Var(Y|X)] = 0, hence the bound above simplifies to:
2n Var(Y) - λ Var(A) - P(λ Var(A) + Var(Y))2 - 4λ(a, Σy>2}.
which is exactly Theorem 5.3.
Proof of Theorem 5.3. By Lemma C.1 and Lemma C.2, we can decompose the objective as:
E[Var(Y|Z)] - λE[Var(A∣Z)]
= (E[Var(Y|Z)] - E[Var(Y|X)]) + λ(Var(A) - E[Var(A|Z)]) + (E[Var(Y|X)] - λ Var(A))
= EZVar(E[Y|X] |Z) + Var(E[A|Z]) + (E[Var(Y|X)] - λ Var(A))
Since E[Var(Y|X)] - λ Var(A) does not depend onZ, we will focus on the first two terms:
min	EZ Var(E[Y|X] |Z) + λ Var(E[A∣Z]).	(26)
Z=g(X)
Recall that for the squared loss,
fY (X ) = E [Y |X ],	fA (X ) = E[A∣X ].
We will first simplify the objective in (26). We have
E Var (E [Y|X] |Z) = E Var(fY(X)|Z),
and using the law of total expectation,
(27)
E(A|Z) = E(A|X, Z)p(X|Z)dX.
Since Z = g(X) is a function of X, We have Z ⊥ A|X, so E(A∣X, Z) = E(A∣X) = fA(X).
Therefore,
E[A|Z] = E[A|X,Z]p(X|Z)dX
=JfA(X )p(X∣Z )dX = E[fA(X )|Z ]
Hence,
Var(E(AIZ)) = Var(E[fA (X )|Z ])
(28)
18
Under review as a conference paper at ICLR 2021
Now we substitute (27), (28) into (26), which gives the following equivalent form of (26):
min {EVar(fY(X)|Z) + λVar(E[fA(X)∣Z])}
Z=g(X)	Y
In this case, the objective (29) becomes:
E Var(fY (X )|Z) + λ Var(E[fA (X )|Z ])	(29)
=E Var(hy3(X ))|Z) + λ Var(EKa"(X )i∣Z ])	(30)
=hy, E CovQ(X )”(X )∣Z)yi+	(31)
λ(a, Cov(E[g(X)|Z], E[g(X)[Z])a〉	(32)
By the law of total covariance,
ECovQ(X),2(X)|Z) + Cov(E3(X)|Z], Eb(X)|Z])
=Cov(P(X ),^(X)) = ς
Let V = Cov(E[夕(X)|Z],E[夕(X)|Z]) , which satisfies Σ 占 V 占 0. Then, finding the feature
transform Z = g(X) that minimizes (32) is equivalent to:
min
V=CoV(ES(X)∣ Z],ES(X) |Z])
hy,(Σ - V)yi +λha,Vai
The key technique of our lower bound is to relax the constraint V = Cov(E[P(X)|Z], E[P(X)|Z])
by the semi-definite constraint Σ V	0.
V ∙miν*0hy, (* —V )yi+λha, Vαi
This is an SDP whose optimal solution lower bounds the objective (26). Moreover, we can show that
there is a simplified form for the SDP optimal solution using eigenvalues and eigenvectors:
hy, (Σ - V)yi + λhα, Vαi
=hy, Σyi + hV, λααT - yyT i
= hy, Σyi + h∑-"VΣ-1/2, Σ"(λαατ - yyT)Σ叫.
Note that I 占 Q := Σ-" VΣ-1/2 占 0, and R := Σ1/2 (λαατ 一 yyT)Σ1/2 is a matrix with rank at
most 2.
When the matrix R is positive definite or negative definite, the minimum is achieved at Q = 0 or
I. Otherwise, the only possibility is that R is a rank-2 matrix with one positive eigenvalue and one
negative eigenvalue. By Von-Neumann’s trace inequality,
d
hQ, Ri ≥ X σi(R)σd-i+1(Q).
i=1
Since σ1(R) > 0 = σ2(R) = ... = σd-1(R) = 0 > σd(R) and 0 ≤ σd(Q) ≤ 1, we have
hQ,Ri ≥ σd(R) = σd(∑”(λαατ - yyT)Σ1/2)
The minimizer is
Q = wwT, V = Σ1/2wwT Σ1/2
where w is the unit eigenvector of R with eigenvalue σd(R). By Lemma C.3,
σd(R) = 1 {λ(α, Σαi 一 (y, Σy) — P(λ(α, Σα) +〈y, Σy>)2 — 4λ(α, Σy>2}
Therefore,
OPT(λ) = hy, (Σ 一 V)yi + λhα, V αi + (E[Var(Y |X)] 一 λ Var(A))
≥ hy, Σyi + σd(R) + (E[Var(Y |X)] 一 λ Var(A))
≥ 2{λ(α, Σαi + hy, Σyi — P(λ<α, Σα) +〈y, Σy>)2 — 4λ(α, Σy>2}
+ (E[Var(Y |X)] 一 λ Var(A))
=2{λ Var(E[A|X])+Var(E[Y|X]) — P(λ Var(E[A∣X]) + Var(E[Y|X]))2 — 4λ<α, Σy>2}
+ (E[Var(Y|X)] — λ Var(A))
Hence we have completed the proof.
19
Under review as a conference paper at ICLR 2021
C.4 Explicit formula for eigenvalues
The following lemma is used the in the last step of the proof of Theorem 5.3 to simplify the expression
involving σd(R) = σd(∑V2(λaaT — yyT)Σ1/2).
Lemma C.3. Let R = ∑"(λaaT - yyT)Σ1/2, then the eigenvalues of R are
σι(R) = 1 {λ(a,	Σa)	-	hy,	Σyi	+	∙√(λ(a, Σa) +〈y,	Σy))2 - 4λ(a, Σy>2},
σd(R) = 1 {λ(a,	Σa)	-〈y,	Σyi	-	√(λ(a, Σa) +〈y,	Σy))2 - 4λ(a, Σy>2}
σ2(R)=…=σd-i(R) = 0
Proof. Since rank(R) ≤ rank(λaaT - yyT) ≤ 2, R has at most two non-zero eigenvalues σ1 (R)
and σd(R). Notice that
d
tr(R) = X σi(R) = σ1(R) +σd(R),
i=1
d
tr(R2) = X σi2(R) = σ12(R) + σd2(R)
i=1
We can write tr(R) and tr(R2 ) explicitly:
tr(R) = λ tr(∑V2aaTΣ1/2) - tr(Σ1SyyTΣ1/2) = λ(a, Σa) -〈y, Σy
tr(R2) = tr(∑"(λaaT — yyT )Σ(λaaT — yyT )Σ1/2)
=tr(Σ1/2 (λ2(aTΣa)aaT — λατΣy(aΣyT) — λyTΣa(yΣaT) + (aTΣa)aaT) Σ1/2)
= λ2(aT Σa)2 - 2λ(aTΣy)2 + (yTΣy)2
Therefore,
σι(R) + σd(R) = λ<a, Σa) -〈y, Σy)
σι(R)σd(R) = 1 (⑸(R) + σd(R))2 - (σ2(R) + σ2(R)))
=λ(a, Σyi2 - λ(a, Σaihy, Σy>
Thus σ1(R) and σd(R) are the roots of the quadratic equation:
x2 — (λ<a, Σai —〈y, Σyi)x + λ(a, Σy>2 — λ(a, Σaihy, Σy = 0
We complete the proof by solving this quadratic equation.
C.5 Achievability of lower bound
In the proof of Theorem 5.3, we showed a lower bound on the tradeoff via an SDP relaxation.
Therefore, the lower bound is achievable whenever the SDP relaxation is tight. We state this as a
regularity condition on (X,夕).
Definition C.1. (X,夕)is called regular, if for any positive Semidefinite matrix M: Σ 占 M 占 0,
there exists Z = g(X), such that
Cov(E[q(X )|Z ], E[q(X )|Z ]) = M.
Theorem C.4. When (X,夕)is regular, the lower bound in Theorem 5.3 is achievable.
Proof. From the proof of Theorem 5.3, we can see that if there exists Z = g(X), such that
CoV(Ek(X )∣Z], Ek(X )|Z ]) = £1/2WwT Σ1/2,
where w is the unit eigenvector of R with eigenvalue σd(R), then the equality is achievable. It is easy
to see that
Σ 占 Σ1∕2wwτΣ1∕2 占 0.
Therefore, choosing M = Σ1/2wwτΣ1/2 in the definition of regularity guarantees the existence of
Z . Hence we have completed the proof.
20
Under review as a conference paper at ICLR 2021
A sufficient condition on the regularity of (X,夕)is the Gaussianity of 夕(X), in which case choosing
g(X ) as a linear transform is sufficient:
Theorem C.5. (X,夕)is regular if 夕(X) follows Gaussian distribution.
Proof. Note that when 夕(X) is Gaussian,(夕(X), L夕(X)) is jointly Gaussian for any L ∈ Rk×d.
Let Z = L夕(X), then the conditional distribution 夕(X)|Z is Gaussian, with mean and covariance
E[q(X )|Z ] = ΣLt (LΣLt )-1Z, Cov[q(X ),q(X )|Z ]
= ΣLT(LΣLT)-1LΣ.
Hence,
E Cov[^(X ),^(X )|Z ] = ΣLt (LΣLt )-1LΣ.
We will prove that for any Σ M	0, there exists a linear transform L, such that M =
ΣLT(LΣLT)-1LΣ.
Consider the eigenvalue decomposition of Σ-"MΣ-1/2 = UTDU, k = rank(M), U ∈
Rk×d, D ∈ Rk×k, D is invertible. Then, let L = D-1/2UΣ-1/2, we have
LΣLT = D-1,
LΣ = D—1/2U Σ1/2,
ΣLT(LΣLT)-1LΣ = M.
Therefore we have completed the proof.
We conjecture that this regularity condition holds for more general distributions beyond Gaussian.
21