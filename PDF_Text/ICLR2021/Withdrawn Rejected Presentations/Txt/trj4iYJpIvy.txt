Under review as a conference paper at ICLR 2021
Approximation Algorithms for Sparse Princi-
pal Component Analysis
Anonymous authors
Paper under double-blind review
Ab stract
Principal component analysis (PCA) is a widely used dimension reduction tech-
nique in machine learning and multivariate statistics. To improve the interpretability
of PCA, various approaches to obtain sparse principal direction loadings have been
proposed, which are termed Sparse Principal Component Analysis (SPCA). In
this paper, we present three provably accurate, polynomial time, approximation
algorithms for the SPCA problem, without imposing any restrictive assumptions
on the input covariance matrix. The first algorithm is based on randomized matrix
multiplication; the second algorithm is based on a novel deterministic thresholding
scheme; and the third algorithm is based on a semidefinite programming relaxation
of SPCA. All algorithms come with provable guarantees and run in low-degree
polynomial time. Our empirical evaluations confirm our theoretical findings.
1 Introduction
Principal Component Analysis (PCA) and the related Singular Value Decomposition (SVD) are
fundamental data analysis and dimension reduction tools in a wide range of areas including machine
learning, multivariate statistics and many others. They return a set of orthogonal vectors of decreasing
importance that are often interpreted as fundamental latent factors that underlie the observed data.
Even though the vectors returned by PCA and SVD have strong optimality properties, they are
notoriously difficult to interpret in terms of the underlying processes generating the data (Mahoney &
Drineas, 2009), since they are linear combinations of all available data points or all available features.
The concept of Sparse Principal Components Analysis (SPCA) was introduced in the seminal work
of (d’Aspremont et al., 2007), where sparsity constraints were enforced on the singular vectors in
order to improve interpretability. A prominent example where sparsity improves interpretability
is document analysis, where sparse principal components can be mapped to specific topics by
inspecting the (few) keywords in their support (d’Aspremont et al., 2007; Mahoney & Drineas, 2009;
Papailiopoulos et al., 2013).
Formally, given a positive semidefinite (PSD) matrix A ∈ Rn×n, SPCA can be defined as follows:1
Z* = maXχ∈Rn, kxk2≤1 x>Ax, subject to ∣∣xko ≤ k.	(1)
In the above formulation, A is a covariance matrix representing, for example, all pairwise feature
or object similarities for an underlying data matrix. Therefore, SPCA can be applied for either the
object or feature space of the data matrix, while the parameter k controls the sparsity of the resulting
vector and is part of the input. Let x* denote a vector that achieves the optimal value Z * in the above
formulation. Then intuitively, the optimization problem of eqn. (1) seeks a sparse, unit norm vector
x* that maximizes the data variance.
It is well-known that solving the above optimization problem is NP-hard (Moghaddam et al., 2006a)
and that its hardness is due to the sparsity constraint. Indeed, if the sparsity constraint was removed,
then the resulting optimization problem can be easily solved by computing the top left or right
singular vector of A and its maximal value Z* is equal to the top singular value of A.
Notation. We use bold letters to denote matrices and vectors. For a matrix A ∈ Rn×n , we
denote its (i, j)-th entry by Ai,j ; its i-th row by Ai* and its j-th column by A*j ; its 2-norm by 1
1Recall that the p-th power of the `p norm of a vector x ∈ Rn is defined as kxkpp = Pin=1 |xi |p for
0 < p < ∞. For p = 0, kxk0 is a semi-norm denoting the number of non-zero entries of x.
1
Under review as a conference paper at ICLR 2021
kAk2 = maxx∈Rn, kxk2=1 kAxk2; and its (squared) Frobenius norm by kAk2F = Pi,j Ai2,j. We
use the notation A 0 to denote that the matrix A is symmetric positive semidefinite (PSD) and
Tr(A) = Pi Ai,i to denote its trace, which is also equal to the sum of its singular values. Given a
PSD matrix A ∈ Rn×n, its Singular Value Decomposition is given by A = UΣUT , where U is the
matrix of left/right singular vectors and Σ is the diagonal matrix of singular values.
1.1	Our Contributions
We present three algorithms for SPCA and associated quality-of-approximation results (Theo-
rems 2.2, 3.1, and 4.1). All three algorithms are simple, intuitive, and run in O n3.5 or less
time. They return a vector that is provably sparse and, when applied to the input covariance matrix
A, ProVably captures a fraction of the optimal solution Z*. We note that in all three algorithms, the
output vector has a sparsity that depends on k (the target sparsity of the original SPCA problem of
eqn. (1)) and (an accuracy parameter between zero and one).
The first algorithm is based on randomized, approximate matrix multiplication: it randomly (but
non-uniformly) selects a subset of O k/2 columns of A1/2 (the square root of the PSD matrix
A) and computes its top right singular Vector. The output of this algorithm is precisely this singular
Vector, padded with zeros to become a Vector in Rn . It turns out that this simple algorithm, which,
surprisingly has not been analyzed in prior work, returns an O k/2 sparse Vector y ∈ Rn that
satisfies (with constant probability that can be amplified as desired, see Section 2 for details):
y>Ay ≥ 1Z * -e √Z* ∙
Notice that the aboVe bound depends on both Z* and it square root and therefore is not a relatiVe
error bound. The second term scales as a function of the trace of A diVided by k, which depends on
the properties of the matrix A and the target sparsity.
The second algorithm is a deterministic thresholding scheme. It computes a small number of the top
singular Vectors of the matrix A and then applies a deterministic thresholding scheme on those singular
Vectors to (eVentually) construct a sparse Vector z ∈ Rn that satisfies z>Az ≥ (1/2)Z*-(3/2)e Tr(A).
Our analysis proVides unconditional guarantees for the accuracy of the solution of this simple
thresholding scheme. To the best of our knowledge, no such analyses haVe appeared in prior work
(see Section 1.2 for details). The error bound of the second algorithm is weaker than the one proVided
by the first algorithm, but the second algorithm is deterministic and does not need to compute the
square root (i.e., all singular Vectors and singular Values) of the matrix A.
Our third algorithm proVides noVel bounds for the following standard conVex relaxation of the
problem of eqn. (1).
max
Z∈Rn×n, Z0
Tr(AZ) s.t. Tr(Z) ≤ 1 and	|Zi,j | ≤ k.
(2)
It is well-known that the optimal solution to eqn. (2) is at least the optimal solution to eqn. (1). We
present a noVel, two-step rounding scheme that conVerts the optimal solution matrix Z ∈ Rn×n to a
vector Z ∈ Rn that has expected sparsity2 O (k2∕e2) and satisfies z>Az ≥ YZ (1 一 e) ∙ Z* 一 e. Here,
γZ is a constant that precisely depends on the top singular Value of Z, the condition number of Z,
and the extent to which the SDP relaxation of eqn. (2) is able to capture the original problem (see
Theorem 4.1 and the following discussion for details). To the best of our knowledge, this is the first
analysis of a rounding scheme for the convex relaxation of eqn. (2) that does not assume a specific
model for the covariance matrix A.
Applications to Sparse Kernel PCA. Our algorithms have immediate applications to sparse kernel
PCA (SKPCA), where the input matrix A ∈ Rn×n is instead implicitly given as a kernel matrix
whose entry (i, j) is the value k(i, j) := hφ(Xi*), φ(Xj*)i for some kernel function φ that implicitly
maps an observation vector into some high-dimensional feature space. Although A is not explicit,
2For simplicity of presentation and following the lines of (Fountoulakis et al., 2017), we assume that the rows
and columns of the matrix A have unit norm; this assumption was not necessary for the previous two algorithms
and can be removed as in (Fountoulakis et al., 2017). We are also hiding a poly-logarithmic factor for simplicity,
hence the O(∙) notation. See Theorem 4.1 for a detailed statement.
2
Under review as a conference paper at ICLR 2021
we can query all O n2 entries of A using O n2 time assuming an oracle that computes the kernel
function k. We can then subsequently apply our SPCA algorithms and achieve polynomial time
runtime with the same approximation guarantees.
1.2	Prior work
SPCA was formally introduced by (d’Aspremont et al., 2007); however, previously studied PCA
approaches based on rotating (Jolliffe, 1995) or thresholding (Cadima & Jolliffe, 1995) the top
singular vector of the input matrix seemed to work well, at least in practice, given sparsity constraints.
Following (d’Aspremont et al., 2007), there has been an abundance of interest in SPCA. (Jolliffe
et al., 2003) considered LASSO (SCoTLASS) on an `1 relaxation of the problem, while (Zou &
Hastie, 2005) considered a non-convex regression-type approximation, penalized similar to LASSO.
Additional heuristics based on LASSO (Ando et al., 2009) and non-convex `1 regularizations (Zou
& Hastie, 2005; Zou et al., 2006; Sriperumbudur et al., 2007; Shen & Huang, 2008) have also been
explored. Random sampling approaches based on non-convex `1 relaxations (Fountoulakis et al.,
2017) have also been studied; we highlight that unlike our approach, (Fountoulakis et al., 2017) solved
a non-convex relaxation of the SPCA problem and thus perhaps relied on locally optimal solutions.
Additionally, (Moghaddam et al., 2006b) considered a branch-and-bound heuristic motivated by
greedy spectral ideas. (JoUmee et al., 2010; PaPailioPoUlos et al., 2013; Kuleshov, 2013; Yuan &
Zhang, 2013) further explored other spectral approaches based on iterative methods similar to the
Power method. (Yuan & Zhang, 2013) sPecifically designed a sParse PCA algorithm with early
stoPPing for the Power method, based on the target sParsity.
Another line of work focused on using semidefinite Programming (SDP) relaxations (d’AsPremont
et al., 2007; d’AsPremont et al., 2008; Amini & Wainwright, 2009). Notably, (Amini & Wain-
wright, 2009) achieved Provable theoretical guarantees regarding the SDP and thresholding aPProach
of (d’AsPremont et al., 2007) in a specific, high-dimensional sPiked covariance model, in which a
base matrix is Perturbed by adding a sParse maximal eigenvector. In other words, the inPut matrix is
the identity matrix Plus a “sPike”, i.e., a sParse rank-one matrix.
DesPite the variety of heuristic-based sParse PCA aPProaches, very few theoretical guarantees have
been Provided for SPCA; this is Partially exPlained by a line of hardness-of-aPProximation results.
The sParse PCA Problem is well-known to be NP-Hard (Moghaddam et al., 2006a). (Magdon-Ismail,
2017) shows that if the inPut matrix is not PSD, then even the sign of the oPtimal value cannot
be determined in Polynomial time unless P = NP, ruling out any multiPlicative aPProximation
algorithm. In the case where the inPut matrix is PSD, (Chan et al., 2016) shows that it is NP-hard
to aPProximate the oPtimal value uP to multiPlicative (1 + ) error, ruling out any Polynomial-
time aPProximation scheme (PTAS). Moreover, they show Small-Set ExPansion hardness for any
Polynomial-time constant factor aPProximation algorithm and also that the standard SDP relaxation
might have an exPonential gaP.
We conclude by summarizing Prior work that offers Provable guarantees (beyond the work of (Amini
& Wainwright, 2009)), tyPically given some assumptions about the input matrix. (d’AsPremont et al.,
2014) showed that the SDP relaxation can be used to find Provable bounds when the covariance inPut
matrix is formed by a number of data Points samPled from Gaussian models with a single sParse
singular vector. (PaPailioPoulos et al., 2013) Presented a combinatorial algorithm that analyzed a
sPecific set of vectors in a low-dimensional eigensPace of the inPut matrix and Presented relative
error guarantees for the oPtimal objective, given the assumPtion that the inPut covariance matrix has
a decaying sPectrum. (Asteris et al., 2011) gave a Polynomial-time algorithm that solves sParse
PCA exactly for inPut matrices of constant rank. (Chan et al., 2016) showed that sParse PCA can be
aPProximated in Polynomial time within a factor of n-1/3 and also highlighted an additive PTAS
of (Asteris et al., 2015) based on the idea of finding multiPle disjoint comPonents and solving
biPartite maximum weight matching Problems. This PTAS needs time npoly(1/), whereas all of our
algorithms have running times that are a low-degree Polynomial in n.
2 Sparse PCA via Randomized Matrix Multiplication
Our first algorithm for SPCA leverages Primitives and ideas from randomized matrix multiPlica-
tion (Drineas & Kannan, 2001; Drineas et al., 2006; Drineas & Mahoney, 2016; 2018; Woodruff,
2014). Let P ∈ Rm×n and Q ∈ Rn×p and recall that their product PQ equals PQ = P>ι P*iQi*.
3
Under review as a conference paper at ICLR 2021
Recall that P*i denotes the i-th column of P and Qi羊 the i-th row of Q. A well-known approach
to approximate the product PQ is to sample a subset of columns of P (we will do this without
replacement) and the corresponding rows of Q (Drineas & Mahoney, 2018). Formally, let the
random variables Zi 七 BemoUlli(pi), i = 1... n, denote whether the i-th column of P and the
i-th row of Q are sampled. Define the diagonal sampling-and-rescaling matrix S ∈ Rn×n by
S，diag {Zι∕√ρι,..., Zn∕√ρn}. The sampling probabilities {pi}n=ι do not have to sum up to
one. The number of sampled column/row pairs, denoted by s, satisfies E [s] = Pin=1 pi. (See
Algorithm 1 for details.) The next lemma (see Appendix A.1 for its proof) presents accuracy bounds
Algorithm 1 Construct SamPling-and-rescaling matrix S
Input: Probabilities pi, i = 1.. .n and integer S《 n.
Output: Diagonal sampling-and-rescaling matrix S ∈ Rn×n.
1: for i = 1 to n do
°. s	∫1∕√Pi, with probability Pi =min{spi, 1}
ii	0,	otherwise
when Algorithm 1 is used to approximate matrix multiplication.
Lemma 2.1 Given matrices P ∈ Rm×n and Q ∈ Rn×p, let S ∈ Rn×n be constructed using
Algorithm 1 with Pi = IIP*ik2∕kPkF , for i = 1.. .n. Then,
E [kPS2Q - PQkF] ≤ S HPkFkQkF .
(3)
Our SPCA algorithm uses the above primitive to approximate the product of the (square root) of the
input matrix A and its top right singular vector v. Thus, the proposed SPCA algorithm sparsifies
the top right singular vector of v of A without losing too much of the variance that is captured
by v. Interestingly, this conceptually simple algorithm has not been formally analyzed in prior
work. Algorithm 2 details our approach.
Algorithm 2 SPCA via randomized matrix multiplication
Input: A ∈ Rn×n, sparsity parameter k, accuracy parameter ∈ (0, 1).
Output: y ∈ Rn satisfying E [kyk2] ≤ 1 and E [kyk0] ≤ k/2.
1.	X 一 A1/2;
2.	Use Algorithm 1 to construct S ∈ Rn×n with Pi = kx*ik2∕kxkF and S = 4k∕e2;
3.	Let v ∈ Rn be the top right singular vector of XS;
4.	y — Sv;
Theorem 2.2 Let k be the sparsity parameter and ∈ (0, 1] be the accuracy parameter. Let
S ∈ Rn×n be the sampling matrix of Lemma 2.1 with S = 4k/2. Then, Algorithm 2 returns a vector
y with expected sparsity at most S (i.e., E [kyk0] ≤ S) and expected two norm at most one (i.e.,
E kyk22 ≤ 1) such that with probability at least 1/4, we have
y>Ay ≥ 1/2Z* — e √Z* ∙ √Tr(A)∕k.
(4)
See Appendix A.1 for a proof of the above theorem. We note that the success probability of Algo-
rithm 2 can be trivially amplified by repeating the algorithm t times and keeping the vector y that
maximizes y>Ay. Then, the failure probability of the overall approach diminishes exponentially
fast as a function of t to at most (3/4)t. Finally, the running time of Algorithm 2 is dominated by
the computation of a square root of the matrix A in the first step, which takes O n3 time via the
computation of the SVD of A.
3 SPCA via thresholding
Our second algorithm is based on a thresholding scheme using the top ` right singular vectors of
the PSD matrix A. Given A and an accuracy parameter e, our approach first computes Σ' ∈ R'×'
4
Under review as a conference paper at ICLR 2021
(the diagonal matrix of the top ' singular values of A) and u` ∈ Rn ×' (the matrix of the top ' right
singular vectors of A), for ` = 1/. Then, it deterministically selects a subset of O k/2 columns
of Σ1∕2U> using a simple thresholding scheme based on the norms of the columns of Σ1∕2U].
(Recall that k is the sparsity parameter of the SPCA problem.) In the last step, it returns the top right
singular vector of the matrix consisting of the chosen columns of Σ1∕2U>. Notice that this right
singular vector is an O k/2 -dimensional vector, which is finally expanded to a vector in Rn by
appropriate padding with zeros. This sparse vector is our approximate solution to the SPCA problem
of eqn. (1).
This simple algorithm is somewhat reminiscent of prior thresholding approaches for SPCA. However,
to the best of our knowledge, no provable a priori bounds were known for such algorithms without
strong assumptions on the input matrix. This might be due to the fact that prior approaches focused
on thresholding only the top right singular vector of A, whereas our approach thresholds the top
` = 1/ right singular vectors of A. This slight relaxation allows us to present provable bounds for
the proposed algorithm.
In more detail, let the SVD of A be A = UΣUT. Let Σ' ∈ R'×' be the diagonal matrix of the top
' singular values and let u` ∈ Rn×' be the matrix of the top ' right (or left) singular vectors. Let
R = {iι,... ,i∣R∣} be the set of indices of rows of u` that have squared norms at least e/k and let
R be its complement. Here |R| denotes the cardinality of the set R and R ∪ R= {1,...,n}. Let
R ∈ Rn×lRl be a sampling matrix that selects3 the columns of u` whose indices are in the set R.
Given this notation, we are now ready to state Algorithm 3.
Algorithm 3 SPCA via thresholding
Input: A ∈ Rn×n , sparsity k, error parameter e > 0.
Output: y ∈ Rn such that kyk2 = 1 and kyk0 = k/e2 .
1:	' J 1/e;
2:	Compute u` ∈ Rn×' (top ' left singular vectors of A) and Σ' ∈ R'×' (square roots of the top '
singular values of A);
3:	Let R = {iι,..., i∣R∣} be the set of rows of u` with squared norms at least e/k and let
R ∈ Rn×lRl be the associated sampling matrix (see text for details);
4:	y ∈ R|R| J argmaxkxk 2 = 1 ∣∣Σ'U>Rx∣∣2;
5:	return z = Ry ∈ Rn ;
Notice that Ry satisfies kRyk2 = kyk2 = 1 (since R has orthogonal columns) and kRyk0 = |R|.
Since R is the set of rows of U' with squared norms at least e/k and ∣∣U'∣F = ' = 1/e, it follows
that |R| ≤ k/e2. Thus, the vector returned by Algorithm 3 has k/e2 sparsity and unit norm.
Theorem 3.1 Let k be the sparsity parameter and e ∈ (0, 1] be the accuracy parameter. Then, the
vector z ∈ Rn (the output of Algorithm 3) has sparsity k/e2, unit norm, and satisfies z> Az ≥
(1/2)Z * — (3/2)e Tr(A).
We defer the proof of Theorem 3.1 to Appendix A.2. The running time of Algorithm 3 is dominated
by the computation of the top ` singular vectors and singular values of the matrix A. In practice, any
iterative method, such as subspace iteration using a random initial subspace or the Krylov subspace
of the matrix, can be used towards this end. However, our current analysis does not account for the
inevitable approximation error incurred by such methods, which run in O (nnz(A)') time. One could
always use the SVD of the full matrix A (O n3 time) to compute the top ` singular vectors and
singular values of A. Finally, we highlight that, as an intermediate step in the proof of Theorem 3.1,
we need to prove the following lemma (see Appendix A.2 for its proof):
Lemma 3.2 Let A ∈ Rn×n be a PSD matrix and Σ ∈ Rn×n (respectively, Σ' ∈ R'×') be
the diagonal matrix of all (respectively, top `) singular values and let U ∈ Rn×n (respectively,
U' ∈ Rn×') be the matrix of all (respectively, top ') singular vectors. Then, for all unit vectors
x ∈ Rn,
愕2u>x∣∣2 ≥ IIς"u> x∣∣2-e IY(A).
3Each column of R has a single non-zero entry (set to one), corresponding to one of the |R| selected columns.
Formally, Rit,t = 1 for t = 1, . . . , |R|; all other entries of R are set to zero.
5
Under review as a conference paper at ICLR 2021
The above simple lemma is very much at the heart of our proof of Theorem 3.1 and, unlike prior
work, allows us to provide provably accurate bounds for the thresholding Algorithm 3.
Using an approximate SVD solution. The guarantees of Theorem 3.1 in Algorithm 3 uses an
exact SVD computation, which could take time O n3 . We can further improve the running time
by using an approximate SVD algorithm such as the randomized block Krylov method of Musco
& Musco (2015), which runs in nearly input sparsity runtime. Our analysis uses the relationships
∣∣∑1m∣ ≤ Tr(A) and σ1(Σ') ≤ Tr(A). The randomized block Krylov method ofMusco & Musco
(2015) recovers these guarantees up to a multiplicative (1 + e), using O (logn ∙ nnz( A)) runtime.
Thus by rescaling , we recover the same guarantees of Theorem 3.1 by using an approximate SVD
with nearly input sparsity runtime.
4 SPCA via a Semidefinite Programming Relaxation
Our third algorithm is based on the SDP relaxation of eqn. (2). Recall that solving eqn. (2) returns a
PSD matrix Z ∈ Rn×n that, by the definition of the semidefinite programming relaxation, satisfies
Tr(AZ*) ≥ Z*, where Z* is the true optimal solution of SPCA in eqn. (2).
We now need to convert the matrix Z* ∈ Rn×n into a sparse vector that will be the output of our
approximate SPCA algorithm and will satisfy certain accuracy guarantees. Towards that end, we
employ a novel two-step rounding procedure. First, a critical observation is that generating a random
Gaussian vector g ∈ Rn and computing the vector Z*g ∈ Rn results in an unbiased estimator for the
trace of (Z*)> AZ* in the following sense: E g>(Z*)>AZ*g = Tr((Z*)> AZ*).
Using von Neumann’s trace inequality, we can prove that
E [g>(Z*)>AZ*g] = Tr((Z*)>AZ*) ≥ γz* ∙ Tr(AZ*) ≥ YZ ∙ Z*.	(5)
Here γz* is a constant that precisely depends on the top singular value of Z*, the condition number
of Z*, and the extent to which the SDP relaxation of eqn. (2) is able to capture the original problem
(see Theorem 4.1 for the exact expression of γZ*). The above inequality implies that, at least
in expectation, we could use the vector Z*g as a “rounding” of the output of the semidefinite
programming relaxation. However, there is absolutely no guarantee that the vector Z*g is sparse.
Thus, in order to sparsify Z*g, we employ a separate sparsification procedure, where each entry of
Z*g is kept (and rescaled) with probability proportional to its magnitude. This procedure is similar
to the one proposed in (Fountoulakis et al., 2017) and guarantees that larger entries of Z*g are more
likely to be kept, while smaller entries of Z*g are more likely to be set to zero, without too much
loss in accuracy. We also note that to ensure a sufficiently high probability of success for the overall
approach, we generate multiple Gaussian vectors and keep the one that maximizes the quantity
g>(Z*)>AZ*g. See Algorithm 4 and Algorithm 5 for a detailed presentation of our approach.
Algorithm 4 SPARSIFY
Input: y ∈ Rn and sparsity parameter s.
Output: z ∈ Rn with E [kzk0] ≤ s.
1:	for i = 1 . . . n do
2:	zi :
Pyi, with probability Pi = min {1, SyyiI},
0	otherwise.
The running time of the algorithm is dominated by the time needed to solve the semidefinite pro-
gramming relaxation of eqn. (2), which, in our setting, is O n3.5 (Alizadeh, 1995). We do note
that SDP solvers such as the one in (Alizadeh, 1995) return an additive error approximation to the
optimal solution. However, the running time dependence of SDP solvers on the additive error γ is
logarithimic in 1 /γ and thus highly accurate approximations can be derived without a significant
increase in the number of iterations of the solver. Thus, for the sake of clarity, we initially omit this
additive error from the analysis and address the approximate solution at the end.
Our main quality-of-approximation result for Algorithm 5 is Theorem 4.1. For simplicity of presen-
tation, and following the lines of (Fountoulakis et al., 2017), we assume that all rows and columns
6
Under review as a conference paper at ICLR 2021
Algorithm 5 Rounding-based SPCA
Input: PSD matrix A ∈ Rn×n, error tolerance > 0, and sparsity parameter k.
Output: x ∈ Rn with E [kxk0] = s.
1:	Let Z be the optimal solution to the relaxed SPCA problem of eqn.(2);
2:	M - 8%2 and S — O (k log； (1/4);	.See Theorem A.13 for the exact value of s.
3:	Generate M random Gaussian vectors g1, . . . , gM in Rn;
4:	y — Z*gj, where j — argmaxi=ι…M g>(Z*)>AZ*g/
5:	Z — SPARSIFY(y, s);
of A have been normalized to have unit norms. This assumption can be relaxed as in (Fountoulakis
et al., 2017). In the statement of the theorem, we will use the notation Z1 to denote the best rank-one
approximation to the matrix Z.
Theorem 4.1 Given a PSD matrix A ∈ Rn×n, a sparsity parameter k, and an error tolerance > 0,
let Z be an optimal solution to the relaxed SPCA problem of eqn. (2). Assume that
Tr(AZ) ≤ αTr(AZ1)	(6)
for some constant α ≥ 1. Then, Algorithm 5 outputs a vector z ∈ Rn that, with probability at least
5/8, satisfies 6[|同图=O (k2 log5/2(1/e)/e2),1同卜=O (Plog 1/e), and
z>Az ≥ YZ(1 — e) ∙ Z* — e.
Here YZ =(1 一(1 一 K(Z))(1 一 1)) σι(Z) with σι(Z) and K(Z) being the top singular value
and condition number of Z respectively.
Similar to Theorem 2.2, the probability of success can be boosted to 1 一 δ by repeating the algorithm
O (1) times in parallel. Moreover by using Markov,s inequality, we can also guarantee a vector Z
with sparsity O k2 log5/2(1/)/2 with probability 1 一 δ, rather than just in expectation.
We now discuss the condition of eqn. (6) and the constant YZ . Our assumption simply says that much
of the trace of the matrix AZ should be captured by the trace of AZ1 , as quantified by the constant
α. For example, if Z were a rank-one matrix, then the assumption would hold with α = 1. As the
trace of AZ1 fails to approximate the trace of AZ (which intuitively implies that the SDP relaxation
of eqn. (2) did not sufficiently capture the original problem) the constant α increases and the quality
of the approximation decreases. More precisely, first notice that the constant YZ is upper bounded by
one, because σ1(Z) ≤ 1 by the SDP relaxation. Second, the quality of the approximation increases as
YZ approaches one. This happens if either the condition number of Z is close to one or if the constant
α is close to one; at the same time, σ1(Z) also needs to be close to one. Clearly, these conditions are
satisfied if Z is well approximated by Z1 . In our experiments, we indeed observed that α is close
to one and that the top singular value of Z is close to one, which imply that YZ is also close to one
(Appendix, Table 6). The proof of Theorem 4.1 is delegated to Appendix A.3 (as Theorem A.13), but
we outline here a summary of statements that lead to the final bound.
Lemma 4.2 Let y and Z be defined as in Algorithm 5. If kyk1 ≤ α and kyk2 ≤ β, then
|y>Ay 一 Z>AZ| ≤ 2|y>A(y 一 Z)| + |(y 一 Z)>A(y 一 Z)|.
Moreover, with probability at least 7/8, we have |y> A(y — z)| ≤ 4αβ∕√s and |(y — z)> A(y 一 z)| ≤
[ 64α4 + 96aβ3
Lemma 4.3 Let M = 8%2, α = k(1 + 2√IogM), and β = 2√logM. Ifthe sparsity parameter S
is set to s = 450α2β3∕e2, then with probability at least 3/4, we have y> Ay ≤ z> Az + e.
Letting y = Z*g, we now conclude the proof by combining eqn. (5) with the above lemma to bound
(at least in expectation) the accuracy of Algorithm 5. To get a high probability bound, we leverage a
result by (Avron & Toledo, 2011) on estimating the trace of PSD matrices. This approach allows us
7
Under review as a conference paper at ICLR 2021
to properly analyze step 4 of Algorithm 5, which uses multiple random Gaussian vectors to achieve
measure concentration (see Appendix A.3 for details).
Finally, we can bound the `2 norm of the vector z of Algorithm 5 by proving that, with probability
at least 3∕4, ∣∣z∣∣2 = O (Plog 1/e). Notice that this slightly relaxes the requirement that Z has unit
norm; however, even for accuracy e close to machine precision, ʌ/log 1∕e is a small constant.
Using approximate SDP solution. The guarantees of Theorem 4.1 in Algorithm 5 uses an optimal
solution Z* to the SDP relaxation in eqn. (2). In practice, we will only obtain an approximate solution
Z to eqn. (2) using any standard SDP solver, e.g. (AIizadeh, 1995), such that Tr(AZ) ≥ Tr(AZ*) - e
after O (log ɪ) iterations. Since our analysis only uses the relationship (x*)> Ax* ≤ Tr(AZ*), then
the additive e guarantee can be absorbed into the other e factors in the guarantees of Theorem 4.1.
Thus, we recover the same guarantees of Theorem 4.1 by using an approximate solution to the SDP
relaxation in eqn. (2).
5	Experiments
(a) CHR 1, n = 37, 493	(b) CHR 2, n = 40, 844	(c) Gene expression data
Fig. 1: Experimental results on real data: f (y) vs. sparsity.
spca-d (Algorithm 3)
spca-r (Algorithm 2)
spca-sdp (Algorithm 5)
dec (Yuan et al., 2019)
cwpca (Beck & Vaisbourd, 2016)
spca-lowrank (Papailiopoulos et al., 2013)
spca (Zou et al., 2006)
dspca (d’Aspremont et al., 2007)
topdiam
-0.420
-0.448
-0.424
-0.423
-0.423
-0.427
-0.477
-0.491
length moist testsg ovensg
-	0.422	0
-	0.445	0
-	0.430	0
-	0.430	0
-	0.430	0
-	0.432	0
-	0.476	0
-	0.507	0
ringtop ringbut bowmax bowdist whorls clear knots diaknot PVE Z *
00
00
00
00
00
00
0	0.177
00
-0.296	-0.416	-0.305	-0.371	-0.394	0	0	0	30.71%	3.993
-0.379	-0.212	-0.179	-0.398	-0.471	0	0	0	28.65%	3.725
-0.268	-0.403	-0.313	-0.379	-0.399	0	0	0	30.74%	3.996
-0.268	-0.403	-0.313	-0.379	-0.399	0	0	0	30.74%	3.996
-0.268	-0.403	-0.313	-0.379	-0.399	0	0	0	30.74%	3.996
-0.249	-0.390	-0.326	-0.383	-0.403	0	0	0	30.72%	3.994
0	-0.250	-0.344	-0.416	-0.400	0	0	0	28.01%	3.641
-0.067	-0.357	-0.234	-0.387	-0.409	0	0	0	29.39%	3.821
Table 1: Loadings, % of variance explained (PVE), and the objective function value for the first pc of the Pit
Props data.
We compare the outputs our algorithms with that of the state-of-the-art sparse PCA solvers such as
the coordinate-wise optimization algorithm of Beck & Vaisbourd (2016) (cwpca) and the block
decomposition algorithm of Yuan et al. (2019) (dec), along with other standard methods such
as Papailiopoulos et al. (2013) (spca-lowrank), d’Aspremont et al. (2007) (dspca), and Zou
et al. (2006) (spca). For the implementation of dec, we use coordinate descent method with
workset: (6, 6) and for cwpca, we use greedy coordinate wise (GCW) method.
First, in order to explore the sparsity patterns of the outputs of our algorithms and how close they
are as compared to standard methods, we first apply our methods on the pit props data which was
introduced in (Jeffers, 1967) and is a benchmark example used to test sparse PCA. It is a 13 × 13
correlation matrix, originally calculated from 180 observations with 13 explanatory variables. While
the existing algorithms aimed to extract top 6 principal components, we restrict only to the top
principal component. In particular, we are interested to apply our algorithms on the Pit Props matrix
in a view to extract the top pc having a sparsity pattern similar to that of Beck & Vaisbourd (2016),
Yuan et al. (2019), (Zou et al., 2006), Papailiopoulos et al. (2013), and (d’Aspremont et al., 2007). It
is known that the decomposition method of Yuan et al. (2019) can find the global optimal solution.
We take k = 7 and Table 1 shows that while spca-d (Algorithm 3) and spca-r (Algorithm 2)
perform very similar to spca or that of dspca with % of variace explained (PVE) uniformly better
than spca, our SDP-based method spca-sdp (Algorithm 5) exactly recovers the optimal solution
and the output matches with both dec and cwpca and very close to spca-lowrank. We also
8
Under review as a conference paper at ICLR 2021
apply our algorithms on another benchmark artificial example from (Zou et al., 2006), please see
Appendix B.3 for a detailed discussion.
Tightness of our bounds. Now, we also verify the tight-
ness of the theoretical lower bounds of our results with the
guarantee of (Papailiopoulos et al., 2013) on the pit props
data. We take = 0.1 and found that the lower bound of
our spca-sdp (Algorithm 5) (dashed red line on the left)
is indeed very close to that of Papailiopoulos et al. (2013)
with d=3 (dashed grey line on the left) . Nevertheless, the
accuracy parameter of Papailiopoulos et al. (2013) typi-
cally relies on the spectrum of A i.e., for a highly accurate
output, can be much smaller depending on the structure of
A, in which case the difference between the lower bounds
of our Algorithm 5 and Papailiopoulos et al. (2013) becomes even smaller.
Next, we further demonstrate the empirical performance of our algorithms on larger real-world
datasets as well as on a synthetic dataset, similar to (Fountoulakis et al., 2017) (see Appendix B).
We use human genetic data from the HGDP (Consortium, 2007) and HAPMAP (Li et al., 2008) (22
matrices, one for each chromosome). In addition, we also use a lung cancer gene expression dataset
(107× 22, 215) from (Landi et al., 2008) and a sparse document-term matrix (2, 858 × 12, 427) created
using the Text-to-Matrix Generator (TMG) (Zeimpekis & Gallopoulos, 2006) (see Appendix B).
Comparisons and metrics. We compare our Algorithm 2 (spca-r), Algorithm 3 (spca-d),
and Algorithm 5 (spca-sdp) with the solutions returned by spca (Zou et al., 2006), as well as the
simple MaxComp heuristic (Cadima & Jolliffe, 1995). We define the quantity f(y) = y>Ay/kAk2
to measure the quality of an approximate solution y ∈ Rn to the SPCA problem. Notice that
0 ≤ f(y) ≤ 1 for all y with kyk2 ≤ 1. As f(y) gets closer to one, the vector y captures more of
the variance of the matrix A that corresponds to its top singular value and corresponding singular
vector. Our goal is to identify a sparse vector y with f(y) ≈ 1. Since the outputs of our Algorithm 2
and Algorithm 5 may have norms that slightly exceed one and in order to have a fair comparison
between different methods, we normalize our outputs in the same way as in (Fountoulakis et al.,
2017) (Appendix B).
Results. In our experiments, for spca-d, spca-r, and spca-sdp, we fix the sparsity s to be
equal to k, so that all algorithms return the same number of non-zero elements. In Figures 1a-1c we
evaluate the performance of the different SPCA algorithms by plotting f(y) against kyk0, i.e., the
sparsity of the output vector, on data from chromosome 1, chromosome 2, and the gene expression
data. Note that performance of our SDP-based method (spca-sdp) is indeed comparable with the
state-of-the-art dec, cwpca, and spca-lowrank, while both spca-d and spca-r are better
than or at least comparable to both spca and maxcomp. However, in practice, the running time of
the SDP relaxation is substantially higher than our other methods, which highlights the interesting
trade-offs between the accuracy and computation discussed in (Amini & Wainwright, 2009). See
Apeendix B for more experimental results.
6 Conclusion and Open Problems
We present three provably accurate, polynomial time, approximation algorithms for SPCA, without
imposing restrictive assumptions on the input covariance matrix. Future directions include: (i) extend
the proposed algorithms to handle more than one sparse singular vector by deflation or other strategies
and (ii) explore matching lower bounds and/or improve the guarantees of Theorems 2.2, 3.1, and 4.1.
9
Under review as a conference paper at ICLR 2021
References
Farid Alizadeh. Interior Point Methods in Semidefinite Programming with Applications to Combina-
torial Optimization. SIAM Journal on Optimization, 5(1):13-51,1995. 6, 8
Arash A. Amini and Martin J. Wainwright. High-dimensional Analysis of Semidefinite Relaxations
for Sparse Principal Components. Annals of Statistics, 37:2877-2921, 2009. 3, 9
Ei Ando, Toshio Nakata, and Masafumi Yamashita. Approximating the Longest Path Length of a
Stochastic DAG by a Normal Distribution in Linear Time. Journal of Discrete Algorithms, 7(4):
420-438, 2009. 3
Megasthenis Asteris, Dimitris Papailiopoulos, and George N Karystinos. Sparse Principal Component
of a Rank-deficient Matrix. In 2011 IEEE International Symposium on Information Theory
Proceedings, pp. 673-677, 2011. 3
Megasthenis Asteris, Dimitris Papailiopoulos, Anastasios Kyrillidis, and Alexandros G Dimakis.
Sparse PCA via Bipartite Matchings. In Advances in Neural Information Processing Systems, pp.
766-774, 2015. 3
Haim Avron and Sivan Toledo. Randomized Algorithms for Estimating the Trace of an Implicit
Symmetric Positive Semi-definite Matrix. Journal of the ACM, 58(2):8:1-8:34, 2011. 7, 21
Amir Beck and Yakov Vaisbourd. The Sparse Principal Component Analysis Problem: Optimality
Conditions and Algorithms. Journal of Optimization Theory and Applications, 170(1):119-143,
2016. 8,23
Jorge Cadima and Ian T. Jolliffe. Loading and Correlations in the Interpretation of Principal
Components. Journal of Applied Statistics, 22(2):203-214, 1995. 3, 9
Siu On Chan, Dimitris Papailliopoulos, and Aviad Rubinstein. On the Approximability of Sparse
PCA. In Proceedings of the 29th Conference on Learning Theory, pp. 623-646, 2016. 3
International HapMap Consortium. A Second Generation Human Haplotype Map of over 3.1 Million
SNPs. Nature, 449(7164):851, 2007. 9, 22
Alexandre d’Aspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanckriet. A Direct
Formulation for Sparse PCA using Semidefinite Programming. SIAM Review, 49(3):434-448,
2007. 1,3,8,23,24
Petros Drineas and Ravi Kannan. Fast Monte-Carlo Algorithms for Approximate Matrix Multipli-
cation. In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science, pp.
452-461, 2001. 3
Petros Drineas and Michael W. Mahoney. RandNLA: Randomized Numerical Linear Algebra.
Communications of the ACM, 59(6):80-90, 2016. 3
Petros Drineas and Michael W. Mahoney. Lectures on Randomized Numerical Linear Algebra,
volume 25 of The Mathematics of Data, IAS/Park City Mathematics Series. American Mathematical
Society, 2018. 3, 4
Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast Monte Carlo Algorithms for Matrices
II: Computing a Low-Rank Approximation to a Matrix. SIAM Journal on Computing, 36(1):
158-183, 2006. 3
Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Optimal Solutions for Sparse
Principal Component Analysis. Journal of Machine Learning Research, 9(Jul):1269-1294, 2008.
3
Alexandre d’Aspremont, Francis Bach, and Laurent El Ghaoui. Approximation Bounds for Sparse
Principal Component Analysis. Mathematical Programming, 148(1-2):89-110, 2014. 3
Kimon Fountoulakis, Abhisek Kundu, Eugenia-Maria Kontopoulou, and Petros Drineas. A Random-
ized Rounding Algorithm for Sparse PCA. ACM Transactions on Knowledge Discovery from Data,
11(3):38:1-38:26, 2017. 2, 3, 6,7, 9, 16, 17, 23
10
Under review as a conference paper at ICLR 2021
John NR Jeffers. Two case studies in the application of principal component analysis. Journal of the
Royal Statistical Society: Series C (Applied Statistics), 16(3):225-236, 1967. 8
Ian T. Jolliffe. Rotation of principal components: Choice of Normalization Constraints. Journal of
Applied Statistics, 22(1):29-35, 1995. 3
Ian T. Jolliffe, Nickolay T. Trendafilov, and Mudassir Uddin. A Modified Principal Component
Technique Based on the LASSO. Journal of Computational and Graphical Statistics, 12(3):
531-547, 2003. 3
Michel Journee, Yurii Nesterov, Peter Richtarik, and RodolPhe Sepulchre. Generalized Power Method
for Sparse Principal Component Analysis. Journal of Machine Learning Research, 11(2), 2010. 3
Volodymyr Kuleshov. Fast Algorithms for Sparse Principal Component Analysis Based on Rayleigh
Quotient Iteration. In Proceedings of the 30th International Conference on Machine Learning, pp.
1418-1425, 2013. 3
Maria Teresa Landi, Tatiana Dracheva, Melissa Rotunno, Jonine D. Figueroa, Huaitian Liu, Abhijit
Dasgupta, Felecia E. Mann, Junya Fukuoka, Megan Hames, Andrew W. Bergen, et al. Gene
Expression Signature of Cigarette Smoking and Its Role in Lung Adenocarcinoma Development
and Survival. PloS one, 3(2), 2008. 9, 22
Jun Z. Li, Devin M. Absher, Hua Tang, Audrey M. Southwick, Amanda M. Casto, Sohini Ra-
machandran, Howard M. Cann, Gregory S. Barsh, Marcus Feldman, Luigi L. Cavalli-Sforza, et al.
Worldwide Human Relationships Inferred from Genome-Wide Patterns of Variation. Science, 319
(5866):1100-1104, 2008. 9, 22
Malik Magdon-Ismail. NP-Hardness and Inapproximability of Sparse PCA. Information Processing
Letters, 126:35-38, 2017. 3
Michael W. Mahoney and P. Drineas. CUR Matrix Decompositions for Improved Data Analysis. In
Proceedings of the National Academy of Sciences, pp. 697-702, 106 (3), 2009. 1
Baback Moghaddam, Yair Weiss, and Shai Avidan. Generalized Spectral Bounds for Sparse LDA. In
Proceedings of the 23rd International Conference on Machine learning, pp. 641-648, 2006a. 1, 3
Baback Moghaddam, Yair Weiss, and Shai Avidan. Spectral Bounds for Sparse PCA: Exact and
Greedy Algorithms. In Advances in Neural Information Processing Systems, pp. 915-922, 2006b.
3
Cameron Musco and Christopher Musco. Randomized block krylov methods for stronger and faster
approximate singular value decomposition. In Advances in Neural Information Processing Systems
28: Annual Conference on Neural Information Processing Systems, pp. 1396-1404, 2015. 6
Dimitris Papailiopoulos, Alexandros Dimakis, and Stavros Korokythakis. Sparse PCA through
Low-rank Approximations. In Proceedings of the 30th International Conference on Machine
Learning, pp. 747-755, 2013. 1, 3, 8, 9, 23
Haipeng Shen and Jianhua Z. Huang. Sparse Principal Component Analysis via Regularized Low
Rank Matrix Approximation. Journal of Multivariate Analysis, 99(6):1015-1034, 2008. 3
Bharath K. Sriperumbudur, David A. Torres, and Gert R.G. Lanckriet. Sparse Eigen Methods by
D.C. Programming. In Proceedings of the 24th International Conference on Machine Learning,
pp. 831-838, 2007. 3
Martin J. Wainwright. Lecture Notes, 2015. https://www.stat.berkeley.edu/
~mjwain/stat210b/Chap2_TailBounds_Jan22_2 015.pdf. 19
David P. Woodruff. Sketching as a Tool for Numerical Linear Algebra. Foundations and Trends in
Theoretical Computer Science, 10(1-2), 2014. 3
Ganzhao Yuan, Li Shen, and Wei-Shi Zheng. A Decomposition Algorithm for the Sparse Generalized
Eigenvalue Problem. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2019. 8, 23
11
Under review as a conference paper at ICLR 2021
Xiao-Tong Yuan and Tong Zhang. Truncated Power Method for Sparse Eigenvalue Problems. Journal
ofMachine Learning Research, 14(APr):899-925, 2013. 3
Dimitrios Zeimpekis and Efstratios Gallopoulos. TMG: A MATLAB Toolbox for Generating Term-
Document Matrices from Text Collections. In Grouping Multidimensional Data, PP. 187-210.
SPringer, 2006. 9, 23
Hui Zou and Trevor Hastie. Regularization and Variable Selection via the Elastic Net. Journal of the
Royal Statistical Society: Series B, 67(2):301-320, 2005. 3
Hui Zou, Trevor Hastie, and Robert Tibshirani. SParse PrinciPal ComPonent Analysis. Journal of
Computational and Graphical Statistics, 15(2):265-286, 2006. 3, 8, 9, 23, 24, 25
12
Under review as a conference paper at ICLR 2021
Appendix to
Approximation Algorithms for Sparse Principal
Component Analysis
A Appendix
A. 1 SPCA via randomized matrix multiplication: Proofs
First, we prove two lemmas that are crucial in proving Lemma 2.1.
Lemma A.1 Given matrices P ∈ Rm×n and Q ∈ Rn×p, let S ∈ Rn×n be constructed using
Algorithm 1. Then,
E (PS2Q)ij = (PQ)ij
n P2 Q2 n
Var(PS2Q)ij = X 上 kj - X P2Qj
k=1 pk	k=1
for any indices i, j ∈ {1, . . . , n}.
Proof : For any i, j ∈ {1, . . . , n}, we have that
E [(PS2Q)ij]	= E X PikSkkQkj	= E X Pik (Z2)	Qkj = X	(PkQkj) E	第]
k=1	k=1 pk	k=1 pk
n
= XPikQkj = (PQ)ij ,
k=1
since Zk2 =d Zk 〜Ber(P) and thus E [Z2] = E [Zk] = Pk, where = denotes equality in distribution.
By the independence of the Zk’s, and noting that Var (Zk) = pk(1 - pk), we have that
Var(PS2Q)j = VarXPik (Zk) Qkj
X ( ɪ )2 var Zk=X ( Tp ) Pg
n
X
k=1
P2k Qkj
Pk
n
- XPi2kQ2kj .
k=1
□
Lemma A.2 Given matrices P ∈ Rm×n and Q ∈ Rn×p, let S ∈ Rn×n be constructed using
Algorithm 1. Then,
E [kPS2Q -PQkF] = X kP*ik2 ∙kQi*k2 - X kP*ik2 kQi*k2.	⑺
i=1	Pi	i=1
Here P*i and Qi* are the i-th column of P and i-th row of Q respectively.
Proof : Using Lemma A.1, we have that
mp	mp
E [kPQ - PS2QkF] = XXE [((PQ)ij - (PS2Q)ij)2] = XXVar(PS2Q)j
mp
XX
i=1 j=1
13
Under review as a conference paper at ICLR 2021
X=X (Pk-1)(X P2)(XQj
X 以IqI - X …I Si.
k=1	pk	k=1
□
Proof of Lemma 2.1: From Lemma A.2,
E [kPSiQ -PQkF] = X WkQ*ki - X kPMi kQi*ki
i=1	pi	i=1
= X	"kis[Q*ki + X	HP/ ∙kQ,*ki - XkP/i kQ,*ki
{∙⅛i≤ι∕s}	Sg	{i：pi>1/s}	i=ι
≤ X	kP*,ki ∙kQ,*ki ≤ X kP*,ki ∙kQ,*ki
一 {,上 ∕s}	Sp	一台	Sp	.
We conclude the proof by setting Pi = k P*a k 2/k p k F.	■
Proof of Theorem 2.2. Proof: In Lemma 2.1, let P = X and Q = x* to get
E [∣∣XSix* - Xx*∣∣i] ≤ 1 ∣∣X∣∣F ∙ ∣∣x*∣∣i ≤ S ∣∣X∣∣F .	⑻
The last inequality follows from kx* ki ≤ 1. Moreover, by Markov’s inequality, with probability at
least 3/4,
4	i
kXsix*- Xx*ki ≤ 7∣X∣∣F = - ∣∣X∣∣F.	⑼
Sk
Let X = Sx*. Taking square roots of both sides of the above inequality, and applying the triangle
inequality on the left hand side of the above inequality, we get:
∣kXx*ki -kXSxki∣ ≤ √k ∣∣X∣∣F
⇒kXSxki ≥√Z*-√√k ∣∣X∣∣F
a	2r
⇒ kXsxki ≥ Z* + k HXkF-√k √Z? •冈F.	(10)
Note that Z* = kXx* kii. Ignoring the non-negative term in eqn. (10), we conclude
kXsxki ≥z*-√√k√Z*∙kXkF.	(11)
Next, using sub-multiplicativity on the left hand side of eqn. (11),
kXsxki ≤kXskikxki = kXSvki kx ki,	(12)
where v ∈ Rn is the top right singular vector of Xs. Letting xi* be the i-th entry of x*, we have,
E(kxki) = E
X x*iPi
h pi
kx* kii = 1 ,
since E Zii = Pi . Using Markov’s inequality, with probability at least 1/i,
kxki ≤ 2.
Conditioning on this event, we can rewrite eqn. (12) as follows:
kXsxki ≤ 2kXSvki = 2(Sv)>X>X(Sv) = 2y>X>Xy.
(13)
(14)
(15)
14
Under review as a conference paper at ICLR 2021
Combining eqns. (11) and (15), we conclude
y>X>Xy ≥ 2Z* -√k √> ∙∣∣X∣∣f .
UsingX>X = A and Tr(A) = kXk2F concludes the proof of eqn. (4). Finally, following the lines
of eqn. (13), we can prove
E(kyk22) = E(kSvk22) = 1.
To conclude the proof of the theorem, notice that the failure probability is at most 1/4 + 1/2 = 3/4
from a union bound on the failure probabilities of eqns.(9) and (14).	□
A.2 SPCA via thresholding: Proofs
We will use the notation of Section 3. For notational convenience, let σ1, . . . , σn be the diagonal
entries of the matrix Σ ∈ Rn×n, i.e., the singular values of A.
Proof of Lemma 3.2: Let U',⊥ ∈ Rn×(n-') be a matrix whose columns form a basis for the subspace
perpendicular to the subspace spanned by the columns of u`. Similarly, let Σ',⊥ ∈ R(n-')×(n-')
be the diagonal matrix of the bottom n - ' singular values of A. Notice that U = [u` U',⊥] and
∑ = [∑' 0； 0 ∑',⊥]; thus,	'
U£1/2U> = U'∑y2U> + U',⊥∑1,U>⊥.
By the Pythagorean theorem,
IlUς1∕2u>x∣∣2 = ||u'W/2u>x|[2 + ∣∣u',⊥ς1∕⊥u>⊥x].
Using invariance properties of the vector two-norm and sub-multiplicativity, we get
忖2uH2 ≥忖/2屋||2-回”陀>回2.
We conclude the proof by noting that ∣∣Σ1∕2U>x∣∣2 = x>UΣU>x = x>Ax and
Il V'1∕2∣∣2 - rj- V 1 Xrr _ Tr(A)
Iς',⊥∣2 = σ'+1 ≤ '工 σi = "ɪ.
i=1
The inequality above follows since σι ≥ σ? ≥ .. ” ≥ σ'+ι ≥ ... ≥ σ% We conclude the proof by
setting ` = 1/.
Proof of Theorem 3.1. Let R = {iι,..., i∣R∣} be the set of indices of rows of u` (columns of U>)
that have squared norms at least e/k and let R be its complement. Here |R| denotes the cardinality of
the set R and R ∪ R = {1,...,n}. Let R ∈ Rn×lRl be the sampling matrix that selects the columns
of u` whose indices are in the set R and let R⊥ ∈ Rn×STRI) be the sampling matrix that selects
the columns of u` whose indices are in the set R. Thus, each column of R (respectively R⊥) has
a single non-zero entry, equal to one, corresponding to one of the |R| (respectively |R|) selected
columns. Formally, Rit,t = 1 for all t = 1, . . . , |R|, while all other entries of R (respectively
R⊥ ) are set to zero; R⊥ can be defined analogously. The following properties are easy to prove:
RR> + R⊥R⊥> = In; R>R = I; R⊥>R⊥ = I; R>⊥R = 0. Recall that x* is the optimal solution
to the SPCA problem from eqn. (1). We proceed as follows:
∣∣Σ1∕2U>x∣∣2 = ∣∣Σ1∕2U>(RR> + R⊥R>)x∣∣2
≤ 2∣∣Σ1∕2U>RR>x*∣∣2 + 2∣∣Σ1∕2U>R⊥R⊥x*∣∣2
≤ 2∣∣∑y2U>RR>x*∣∣2 + 2σι ∣∣U>R⊥R>x*∣∣2 .	(16)
The above inequalities follow from the Pythagorean theorem and sub-multiplicativity. We now bound
the second term in the right-hand side of the above inequality.
n
∣∣U>r⊥r⊥x* ∣∣2 = k X(U>R⊥)*i(R>χ*)ik2
i=1
15
Under review as a conference paper at ICLR 2021
n	I— n
≤ X k(u>R⊥)*ik2 ∙∣(R⊥χ*)i∣ ≤ G Xl(R⊥χ*)i∣
i=1	i=1
≤ rTk ∣∣Riχ*kι ≤ rTk √k=√^.	(17)
In the above derivations We use standard properties of norms and the fact that the columns of U>
that have indices in the set R have squared norm at most e/k. The last inequality follows from
∣R⊥χ*kι ≤ ∣∣x*∣ι ≤ √k, since x* has at most k non-zero entries and Euclidean norm at most one.
Recall that the vector y of Algorithm 3 maximizes k∑1∕2U>Rx∣2 over all vectors X of appropriate
dimensions (including Rχ*) and thus
k∑J2UIRyk2 ≥ 忖2U>RR1x*( .	(18)
Combining eqns. (16), (17), and (18), we get
2∣∣∑y2Ulx*∣∣2 ≤ k∑72U>zk2+ eTr(A).	(19)
In the above we used z = Ry (as in Algorithm 3) and σ1 ≤ Tr(A). Notice that
U'∑y2U> Z + U',⊥∑1∕⊥U>⊥z = U∑"2U>z,
and use the Pythagorean theorem to get
kU'∑y2u>zk2 + ∣U',⊥∑1∕⊥u>⊥z∣2 = ku∑"u1z∣∣2.
Using the unitary invariance of the two norm and dropping a non-negative term, we get the bound
k∑72U>zk2 ≤k∑"u1z∣2.	(20)
Combining eqns. (20) and (19), we conclude
2∣∣∑y2U>x*∣∣2 ≤ k∑"U1z∣∣2+ eTr(A).	(21)
We now apply Lemma 3.2 to the optimal vector x* to get
∣∑1/2 u>x*∣∣2-e Tr(A) ≤∣∣∑y2u>x*[.
Combining with eqn. (21) we get
z>Az ≥ 1Z* - 3eTr(A).
2	2
In the above we used k∑"2U>zk2 = z>Az and ∣∣∑"U>x* ||； = (x*)>Ax* = Z*.	■
A.3 SPCA via a Semidefinite Programming Relaxation: Proofs
We start with a lemma arguing that the sparsification procedure of Algorithm 5 does not significantly
distort the `2 norm of the input/output vectors.
Lemma A.3 Let y and z be defined as in Algorithm 5. If ∣y∣1 ≤ α, then, with probability at least
15/16,
16α2
∣z - y∣22 ≤
s
Proof : Notice that
n	n2	n	2
e h∣z-y∣2i= Xɑ-1) y2 ≤ X ⅛ ≤∣y∣ι XI =呼，
i=1	i=1	i=1
which is at most α2 from our assumptions. The lemma follows by Markov,s inequality.
□
To prove Lemma 4.3, we start with the following consequence of the triangle inequality, e.g., Lemma
3 from (Fountoulakis et al., 2017).
16
Under review as a conference paper at ICLR 2021
Lemma A.4 Let y and z be defined as in Algorithm 5. Then
|y>Ay - z>Az| ≤ 2|y>A(y - z)| + |(y - z)>A(y - z)|.
Proof: This is Lemma 3 from (FoUntoUlakis et al., 2017).	□
We now proceed to upper bound the two terms in the above lemma separately. The following lemma
boUnds the first term.
Lemma A.5 Let y and z be defined as in Algorithm 5. If kyk1 ≤ α and kyk2 ≤ β, then, with
probability at least 15∕i6, ∣y>A(y — z)| ≤ 4αβ∕√s.
Proof : Recall that we set zi = yi/pi with probability pi and zero otherwise, for all i = 1 . . . n.
Then,
E [(y>A(y - Z))2] = X (- - 1) y2(Ai*y产
i=1 pi
Using |Ai*y| ≤ kAi/b ∣∣yk2 ≤ β (from our assumption on the '2 norm of y as well as our
assUmption that the rows/colUmns of A have Unit norm), it follows that
E[(y>A(y-z))2] ≤ β 2 X y2 ≤ " ≤ 空
i=1 pi	s	s
The lemma follows from Markov’s inequality.
□
The next lemma provides an upper bound for the second term in the right hand side of Lemma A.4.
Lemma A.6 Let y and z be defined as in Algorithm 5. If kyk1 ≤ α and kyk2 ≤ β, then, with
probability at least 15/16,
Ky - ZFAy - Z) | ≤ r 6^ + 96αβ3.
Proof: Let Zi = ɪ with probability Pi and zero otherwise for all i = 1 ...n. Then,
pi
E h((y ― Z)TA(Z ― y)) i = ^X Aa,cAb,dyaybycyd ∙ E [(I- Za)(I- Zb)(I- Zc)(I- Zd)] .
a,b,c,d
We immediately have E [1 - Zi] = 0. Thus, if any of the indices a, b, c, d appears only once in the
above summation, then
E[(1-Za)(1-Zb)(1-Zc)(1-Zd)] =0.
Let
B1 =XA2a,bya2yb2E[(1-Za)2(1-Zb)2 ,
a6=b
B2 =XAa,aAb,bya2yb2E[(1-Za)2(1-Zb)2 ,
a6=b
B3=XAa,bAb,aya2yb2E[(1-Za)2(1-Zb)2 ,
a6=b
n
B4 =XA2a,aya4E[(1-Za)4 .
a=1
It now follows that
4
E [((y - Z)TA(Z- y))?] = X Bi.
i=1
Using |Ai,j| ≤ 1 for all i,j, we can bound B1, B2, and B3by
nn
i=m1a2x3{Bi} ≤ Xya2E [(1 - Za)2 Xyb2E [(1 - Zb)2 .
, ,	a=1	b=1
17
Under review as a conference paper at ICLR 2021
Using E [(1 — Zi)2] = A — 1 for all i, We get
imιaχX3{Bi}) ≤ (X C-1) ya! ≤ (呼! ≤ α2,
Where the inequality folloWs by kyk1 ≤ α. To bound B4 , use kyk1 ≤ αand kyk2 ≤ β , to get
n	n	n 146
B4=X Aa，ayaE [(1-Za) ] ≤ X yaE[(I-Za) ]=X ya (磅—若+K—4+pa
n
≤ X ya4
a=1
1 6∣∣y∣∣ι
|ya|s
Ykyk4	工	6 ∣∣y∣∣3 ∣∣y∣∣3 α4	6αβ3
≤	T	+	-s —	≤ /	+ 丁
The last inequality folloWs from properties of norms, namely that kyk33 ≤ kyk23 . Thus,
E [((y-z)>A(z-y))2i≤ α4 + 3α4 + 6αβ3
Using Markov’s inequality, We conclude
< 4a4	6ɑβ3
s2	s
Pr
|(z - y)>A(z - y)| ≥ 4
6αβ 3
s
≤ ɪ.
一 16
口
Proof of Lemma 4.2: Observe that Lemma 4.2 folloWs immediately from Lemma A.4, Lemma A.5,
Lemma A.6, and a union bound.	口
The next tWo lemmas bound the `2 and `1 norms of the vectors yi for all i = 1 . . . M . We Will bound
the norm of a single vector yi (We Will drop the index) and then apply a union bound on all M vectors.
Lemma A.7 Let y be defined as in Algorithm 5. Then,
Pr hkyk2 ≥ 2PlogMi ≤ M12.
Proof : Let Z = UΣV> be the singular value decomposition of Z and let σi = Σi,i, for i = 1 . . . n,
be the singular values of Z. Since Tr(Z) = 1, it folloWs that Pin=1 σi = 1 and also σi ≤ 1 for all
i = 1 . . . n. Additionally,
nn
X σi2 ≤ X σi ≤ 1.	(22)
i=1	i=1
Then,
kyk22 = kZgk22 = g>Z>Zg = g>VΣ2V>g = ΣV>g22.
The rotational invariance of the Gaussian distribution implies that y 〜h2, where h is a random
vector whose i-th entry h satisfies h 〜N(0, σ2). Hence,
n
E kyk22 =E khk22 =Xσi2 ≤1.
i=1
Now, from Markov’s inequality, for any C > 0,
Pr ]l∣yk2 ≥ t + IotM 1 = Pr heCkyk2 ≥ eCt+ClogM/ti ≤ ECeMC；]
e
Then,
E eCkyk2	=E eCkhk2
18
Under review as a conference paper at ICLR 2021
n∞
≤ Y -72-	ec%-x2∕2σ2 dχ
i=1 2πσi 0
n
Y
i=1
C2 σi2∕2
∞
0

n	∞n	n
Y (——dt)=Y L/2=exP (X C 小
Using eqn. (22), we get
Setting C = 2t and ≤ 1, we get
E eCkyk2 ≤eC2∕2
Pr
eC2∕2	1
≤ ---=-----.
一eCtM Ct	M2
Setting t = √log M concludes the proof.
□
Prior to bounding the `1 norm of y, we present a measure concentration result that will be useful in
our proof. First, recall the definition of L-Lipschitz functions.
Definition A.8 Let f : Rn → R be any function. If kf (x) - f (y)k2 ≤ L kx - yk2 for all x, y ∈
Rn, then f is L-Lipschitz.
Theorem A.9 (Gaussian Lipschitz Concentration) (Wainwright, 2015) Let f be an L-Lipschitz
function and let g ∈ Rn be a vector of i.i.d. Gaussians. Then f(x) is sub-Gaussian with variance L2
and, for all t ≥ 0,
Pr[|f(x)-E[f(x)]| ≥t] ≤2e-t2∕2L2.
Lemma A.10 Let y be defined as in Algorithm 5. Then,
Pr [kykι ≥ k(1 + 2kPl0gM)i ≤ 击.
Proof: Since gj 〜N(0,1) for all j = 1... n, the 2-stability of the Gaussian distribution implies
that
nn
E [kZgk1] =XXZi,jgj
i=1 j=1
Let f(x) = kZxk1. The triangle inequality implies that
nn
| kZxkι- kZyki | ≤ XZ*χ- Zi*y| = X|Zi*(X -y)|.
i=1	i=1
Thus, by Cauchy-Schwarz,
n
IkZxki -kzyki l≤ X kZi*k2 kχ -yk2,
i=i
and f(x) is kZki,2-Lipschitz4. Using Theorem A.9,
Pr	kyki- r∏ kZki,2 ≥ t ≤ 2e-t2/2kZk2,2,
4Reeallthatthe Lp,q normof A is IIAllp,q = (Pn=I (Pn=1 |Ai,j|q) q) P,e.g., IIAlIF =IIAk2,2
19
Under review as a conference paper at ICLR 2021
for all t ≥ 0. Setting t = 2√log M and noting that ∣∣Zk 12 ≤ ∣∣Z∣∣ 11 ≤ k, we get
Pr [kykι ≥ k(1 + 2pl0gM)i ≤ MM2.
□
Proof of Lemma 4.3: Using Lemma A.7 and Lemma A.10, we conclude that kyk1 ≤ α and
IlyIl2 ≤ β both hold with probability at least 1 - M. Using Lemma A.4, We get
|y>Ay - z>Az| ≤ 2|y>A(y - z)| + |(y - z)>A(y - z)|.
Since ∣y>A(y - z)| ≤ 4√β with probability at least 15 (by Lemma A.5) and
>	64ɑ4	96αβ3
|(y-z)>A(y-z)| ≤ V二 +
with probability at least 15 (by Lemma A.6), setting S = 450Ofe', we get
y>Ay ≤ z>Az + ,
with probability at least 14/16 - 2/M ≥ 3/4, since M ≥ 16.	□
We now prove an inequality that was used in eqn. (5) to compare y>Ay and Tr(AZ).
Lemma A.11 Let Z, A ∈ Rn×n be PSD matrices and Tr(AZ) ≤ α Tr(AZ1) for some α ≥ 1,
where Z1 is the best rank-1 approximation of Z. Then,
Tr(ZAZ) ≥ γZ . Tr(AZ) .
Here YZ =(1 —(1 — K(Z))(1 — 1)) σι(Z) with σ∖(Z) and K(Z) being the top singular value and
condition number of Z respectively.
Proof : For simplicity of exposition, assume that rank(Z) = n. Let Z = UΣU be the SVD of Z.
Suppose U = (U1 U1,⊥) and Σ = Σ01 Σ0 such that we have Z1 = U1Σ1U1> and Z1,⊥ =
U1,⊥Σ1,⊥U1>,⊥. As Z1 is the best rank-1 approximation of Z, we have Z1Z1,⊥ = Z1,⊥Z1 = 0.
Using this, we rewrite Tr(ZAZ) as the following
Tr(ZAZ) = Tr ((Z1 + Z1,⊥)A(Z1 + Z1,⊥))
= Tr(Z1AZ1) + Tr(Z1,⊥AZ1,⊥) + Tr(Z1AZ1,⊥) + Tr(Z1,⊥AZ1)
= Tr(Z1AZ1) + Tr(Z1,⊥AZ1,⊥) + Tr(AZ1,⊥Z1) + Tr(AZ1Z1,⊥)
= Tr(Z1AZ1) + Tr(Z1,⊥AZ1,⊥) ,	(23)
where the third equality follows from the invariance of matrix trace under cyclic permutations and
the last step is due to Z1Z1,⊥ = Z1,⊥Z1 = 0.
Next, we rewrite Tr(AZ) as
Tr(AZ) = Tr(AZ1) + Tr(AZ1,⊥) = Tr(AZ1Z1Z1) + Tr(AZ1,⊥)
=Tr(ZIZ1AZ1)+Tr(AZ1,⊥)
≤ σ1(Z1) ∙ σ1(Z1AZ1)+Tr(AZ1,⊥)	(24)
where Z〔 is the pseudo-inverse of Z1 and we have used the fact Z1z1Z1 = Z1 and the last inequality
follows from the von Neumann's trace inequality. Now, noting that σ1(Z1) = σ⅛) along with the
fact that σ1(Z1AZ1) ≤ Tr(Z1AZ1) applying eqn. (23), we have
Tr(AZ) ≤ -ɪ ( Tr(ZAZ) - Tr(Z1,⊥ AZ1,⊥)) + Tr(AZj)	(25)
σ1 (Z)
20
Under review as a conference paper at ICLR 2021
Next, we will show that Tr(Zι,⊥ AZι,⊥) ≥ σn(Z)∙Tr(AZι,⊥). First, note that ∑ι,⊥ < σn(Z)∙In-ι,
as σn(Z) ≤ σi(Z) for all i = 2,..., n. Therefore, Pre- and post-multiplying both sides by ∑,⊥, We
further have Σ21,⊥ < Qn(Z) ∙ ∑ι,⊥. Again, pre- and post-multiplying both sides by Uι,⊥ and U>⊥,
we have:
Z2,⊥ = Uι,⊥∑1,⊥U>⊥ < Qn(Z) ∙ U1,⊥∑1,⊥U>⊥ = Qn(Z) ∙ Zι,⊥ ∙	(26)
As the matrix A is also PSD, it has a PSD square-root A1/2 such that A = A1/2 ∙ A1/2. Now, pre-
and post-multiplying both sides of eqn. (26) by A1/2 , we have
A1∕2Z2,⊥A1∕2 < Qn(Z) ∙ A1∕2Zι,⊥A1∕2	(27)
Next, we rewrite Tr(Z1,⊥AZ1,⊥ ) as follows
n
Tr(Z1,⊥AZ1,⊥) = Tr(AZ2,⊥) = Tr(A1∕2Z2,⊥A1/2) = X e>A1∕2Z2,⊥A1∕2ei
i=1
n
≥ Qn(Z) ∙ Xe>A1∕2Zι,⊥A1∕2ei = Qn(Z) ∙ Tr(A1/2Zι,⊥A1∕2)
i=1
=Qn(Z) ∙ Tr(AZι,⊥) .	(28)
In the above, e1, e2, . . . , en ∈ Rn are the canonical basis vectors and we have used the invariance
property of matrix trace under cyclic permutations. Finally, the inequality in eqn. (28) directly follows
from eqn. (27), as eqn. (27) boils down to x> A1∕2Z2,⊥ A1/2X ≥ Qn(Z) ∙ x> A1∕2Zι,⊥ A1∕2x for any
vector x 6= 0.
Next, we combine eqns. (25) and (28) and replacing κ(Z) = ∕1(Z) to get
IY(AZ) ≤ TQZAZ2 + (1- κ⅛) aAZa
= TrQZAr + (1 - K(Z)卜aAZ)-IY(AZI))
≤ TQZAZ) + (I- K(Z) j (I-1)Tr(AZ),	(29)
where the equality is holds as Tr(AZ) = Tr(AZ1) + Tr(AZ1,⊥) and the last inequality is due to
our assumption that Tr(AZ) ≤ α Tr(AZι). This concludes the proof.	□
To finalize our proof, we use the following result of (Avron & Toledo, 2011) for estimating the trace
of PSD matrices.
Theorem A.12 (Avron & Toledo, 2011) Given a PSD matrix A ∈ Rn×n, let M = 80/2. Let gi (for
i = 1 . . . M) be standard Gaussian random vectors. Then with probability at least 7/8,
1 M
Tr(A)- M ∑g>Agi ≤ e∙ Tr(A).
We are now ready to prove the correctness of Algorithm 5 by establishing Theorem A.13.
Theorem A.13 Let Z be an optimal solution to the relaxed SPCA problem of eqn. (2) and Assume
that Tr(AZ) ≤ α Tr(AZ1) for some constant α ≥ 1, where Z1 is the best rank-1 approximation of
Z. Then, there exists an algorithm that takes as input a PSD matrix A ∈ Rn×n, an approximation
parameter e > 0, and a parameter k, and outputs a vector z such that with probability at least 5/8,
450α2β3
E[kzko] ≤ S = e2β
Ilzll2 ≤ β + √s,	z>Az ≥ YZ(1 - e) ∙ Z* - e.
In the above, α =	k(1	+ 2√logM),	β =	2√logM,	and M =	8%2,	and	YZ	=
(1 一(1 一 K(z))(1 一 1)) Qi(Z) with Qi(Z) and κ(Z) being the top singular value and condition
number of Z respectively.
21
Under review as a conference paper at ICLR 2021
Proof: Consider Algorithm 5 and let Z* be an optimal solution to the SPCA Semidefinite relaxation
of eqn. (2). Then, as already discussed, (x*)> Ax* ≤ Tr(AZ*), where x* is the optimal solution to
the SPCA problem of eqn. (1). Then, using Lemma A.11, it follows that
γz* Tr(AZ*) ≤ Tr ((Z*)>AZ*).
Applying Theorem A.12 to the matrix (Z*)>AZ* and using our choice of y in Algorithm 4, we get
1M
y>Ay ≥ MM ∑g>(Z*)>AZ*gi ≥ (1 - e)Tr ((Z*)>AZ*),
M i=1
with probability at least 7/8. By Lemma 4.3, we have y>Ay ≤ z>Az + with probability at least
3/4. Thus, with probability at least 5/8,
(1 — e)γz* ∙Z* = (1 — e)γz* ∙ (x*)>Ax* ≤ z>Az + e.
To conclude the proof, We need to bound the '2 norm of the solution vector z. Let E be the event
that IlZgikI ≤ k(1 + 2√logM) and IlZgik2 ≤ 2√logM for all i = 1... M. From Lemma A.7
and Lemma A.10 and the union bound, we have Pr [E] ≥ 1 一 M. Conditioned on E, Lemma A.3
implies that, with probability at least 15/16,
ky-zk2 ≤ 16k2(1 + 2√IogM)2.
Therefore, with probability at least 15 一 M ≥ 4 (since M ≥ 16), an application of the triangle
inequality gets
kzk2 ≤ kyk2 + kz — yk2 ≤ 2PlogM +4k(1 + 2√ogM.
s
Using our chosen values for a and β concludes the proof.	□
B	Additional Notes on Experiments
In addition, we normalize the outputs of Algorithm 2 and Algorithm 5 by keeping the rows and
the columns of A corresponding to the nonzero elements of the output vectors and then getting the
top singular vector of the induced matrix and padding it with zeros. The above two considerations
make our comparisons fair in terms of function f(y) (see Section 5 for the definition of f (y)).
For Algorithm 3, we fix the threshold parameter ` to 30 for human genetic data, as well as for the text
data; we set ` = 10 for the gene expression data. Finally, for Algorithm 5, we fix M (the number
of random Gaussian vectors) to 300 and we use Python’s cvxpy package to solve eqn. (2). All the
experiments were implemented on a single-core Intel(R) Xeon(R) Gold 6126 CPU @ 2.60GHz.
B.1	Real Data
Population genetics data. We use population genetics data from the Human Genome Diversity
Panel (Consortium, 2007) and the HAPMAP (Li et al., 2008). In particular, we use the 22 matrices
(one for each chromosome) that encode all autosomal genotypes. Each matrix contains 2,240 rows
and a varying number of columns that is equal to the number of single nucleotide polymorphisms
(SNPs, well-known biallelic loci of genetic variation across the human genome) in the respective
chromosome. The columns of each matrix were mean-centered as a preprocessing step. See Table 4
for summary statistics.
Gene expression data. We also use a lung cancer gene expression dataset (GSE10072) from from
the NCBI Gene Expression Omnibus database (Landi et al., 2008). This dataset contains 107 samples
(58 cases and 49 controls) and 22,215 features. Both the population genetics and the gene expression
datasets are interesting in the context of sparse PCA beyond numerical evaluations, since the sparse
components can be directly interpreted to identify small sets of SNPs or genes that capture the data
variance.
22
Under review as a conference paper at ICLR 2021
Text classification data. We also evaluate our algorithms on a text classification dataset used
in (Fountoulakis et al., 2017). This consists of two publicly available standard test collections for ad
hoc information retrieval system evaluation: the Cranfield collection that contains 1, 398 abstracts
of aerodynamics journal articles and the CISI (Centre for Inventions and Scientific Information)
data that contains 1, 460 information science abstracts. Finally, using these two collections, a sparse,
2, 858 × 12, 427 document-term matrix was created using the Text-to-Matrix Generator (TMG)
(Zeimpekis & Gallopoulos, 2006), with the entries representing the weight of each term in the
corresponding document. See Table 5 for summary statistics.
B.2	Synthetic Data
We also use a synthetic dataset generated using the same mechanism as in (Fountoulakis et al., 2017).
Specifically, we construct the m × n matrix X such that X = UΣV> + Eσ . Here, Eσ is a noise
matrix, containing i.i.d. Gaussian elements with zero mean and we set σ = 10-3; U ∈ Rm×m
-	-	..	..	_	z ~	一、	一	-	-	~	一
is a Hadamard matrix With normalized columns; Σ = (∑ 0) ∈ Rm×n such that Σ ∈ Rm×m
一. . .. ~ .. _ _ ~ √ . . ______________________________________________ _______ 、… _
is a diagonal matrix with Σ11 = 100 and Σ讥 =e for i = 2,...,m; V ∈ Rn×n such that
V = Gn(θ)V, where V ∈ Rn×n is also a Hadamard matrix with normalized columns and
Gn(θ) = G(ii,ii + 1,θ) G(i2, i2 + 1,θ) ... G(in∕4, - + 1, θ),
is a composition of 4 GiVenS rotation matrices with ik = n+ 2k — 1 for k = 1, 2,..., 4. Here
G(i, j, θ) ∈ Rn×n be a GiVens rotation matrix, which rotates the plane i - j by an angle θ. For
θ ≈ 0.27∏ and n = 212, the matrix Gn(θ) rotates the bottom n components of the columns of V,
making half of them almost zero and the rest half larger. Figure 2 shows the absolute Values of the
elements of the first column of the matrices V and V.
B.3 Additional Experiments
	Xi	X2	X3	X4	X5	X6	X7	X8	X9	Xi0	PVE	Z*
spca-r (Algorithm 2)	-0-	0	-0-	0	0.51	0.51	0.50	0	0	0.48	39.6%	1164.2
spca-d (Algorithm 3)	0.50	0.50	0.50	0.50	0	0	0	0	0	0	39.5%	1161.0
SPca-SdP (Algorithm 5)	0	0	0	0	0.50	0.50	0.50	0.50	0	0	40.9%	1201.0
dec (Yuan et al., 2019)	0	0	0	0	0.50	0.50	0.50	0.50	0	0	40.9%	1201.0
cwpca (Beck & Vaisbourd, 2016)	0.50	0.50	0.50	0.50	0	0	0	0	0	0	39.5%	1161.0
spca-lowrank (Papailiopoulos et al., 2013)	0	0	0	0	0.50	0.50	0.50	0.50	0	0	40.9%	1200.9
SPca (Zou et al., 2006)	0	0	0	0	0.50	0.50	0.50	0.50	0	0	40.9%	1201.0
dspca (d’Aspremont et al., 2007)	0	0	0	0	0.50	0.50	0.50	0.50	0	0	40.9%	1201.0
Table 2: Loadings, % Variance explained (PVE), and the objectiVe function Value for the first principal
component of the artificial data.
Additionally, in order to further explore the sparsity patterns of the outputs of our algorithms and
how close they are as compared to standard methods, we further apply our methods on a simulation
example proposed by (Zou et al., 2006). We describe them below:
Artificial Data of (Zou et al., 2006). In this example, three hidden factors V1 , V2 , and V3 are created
in the following way:
Vi 〜N(0, 290), V2 〜N(0, 300),
V3 = —0.3 Vi + 0.925 V2 + ε , ε 〜N(0,1) and
V1, V2, and ε are independent.
Next, we create 10 obserVable Variables X1,	X2, . . . ,	X10 in	the following way:
Xi = V1	+ εi1	,	εi1	〜 N(0,	1)	,	i	=	1,	2, 3, 4	,
Xi = V2	+ εi2	,	εi2	〜 N(0,	1)	,	i	=	5,	6, 7, 8	,
Xi = V3	+ εi3	,	εi3	〜 N(0,	1)	,	i	=	9,	10 ,
εij are independent
i= 1,2,...,10; j= 1,2,3.
23
Under review as a conference paper at ICLR 2021
We take A to be the exact covariance matrix of (X1 X2 . . . X10) to compute the top principal
component. As the first two factors i.e., V1 and V2 are associated with four variables while the last one
i.e., V3 is associated with only two variables and noting that all the three factors V1, V2, and V3 roughly
have the same variance, V1 and V2 are almost equally important, and they are both significantly more
important than V3 . Therefore, for the first sparse principal component, the ideal solution would be to
use either (X1, X2, X3, X4) or (X5, X6, X7, X8). Using the true covariance matrix and the oracle
knowledge that the ideal sparsity is k = 4, we apply our algorithms and compare it with spca of
(Zou et al., 2006) as well as the SDP-based algorithm of (d’Aspremont et al., 2007). We found that
while two of our methods, namely, spca-d (Algorithm 3) and spca-sdp (Algorithm 5) are able to
identify the correct sparsity pattern of the optimal solution, spca-r (Algorithm 2) wrongly includes
the variable X10 instead of X8, possibly due to high correlation between V2 and V3 (see Table 2 for
details). However, the output of the spca-d is much more interpretable, even though it has slightly
lower PVE than spca-r.
In our additional experiments on the large datasets, Figure 2b shows the performance of various
SPCA algorithms on the synthetic data. Notice that the performance of the maxcomp heuristic is
worse than spca as well as our algorithms. This is quite evident from the way we constructed the
n
synthetic data. In particular, turning the bottom n elements of V into large values guarantees that
these would not be good elements to retain in the construction of the output vector in maxcomp, as
they fail to capture the right sparsity pattern. On the other hand, our algorithms perform better than or
comparable to spca. Similar to the real data, performance of spca-sdp closely matches with that
of dec, cwpca, and spca-lowrank. In Figure 3, we demonstrate how our algorithms perform on
Chr 3 and Chr 4 of the population genetics data. We see a similar behavior as observed for Chr 1
and CHR 2 in Figures 1a-1b. In Table 3, we report the variance f(y) captured by the output vectors
of different methods for the text data, which again validates the accuracy of our algorithms.
(a)	(b)
Fig. 2: Experimental results on synthetic data with m = 27 and n = 212: (a) the red and the blue lines
are the sorted absolute values of the elements of the first column of matrices V and V respectively.
(b) f (y) vs. sparsity ratio.
-k-	Pca	Spca	maxcomp	spca-d	spca-r	Spca-Sdp	cwpca	dec	spca-lowrank
-100-	0.4597	0.0774	-00309-	0.1385	0.0247	-0∏644-	0.1519	0.1379	0TΓ556
-500-	0.4597	0.1874	-02140-	0.2948	0.1683	-03266-	0.3542	0.3378	035∏
1,000	0.4597	0.2737	-03625-	0.3650	0.2892	-0.4128-	0.4331	0.4214	04306
2,000	0.4597	0.4056	-07380-	0.4334	0.3935	-04396-	0.4554	0.4408	0.4545
5,000	0.4597	0.4257	-0.4441-	0.4422	0.4276	-0.4413-	0.4558	0.4521	04560
10,000	0.4597	0.4462	0.4512	0.4505	0.4412	0.4485 一	0.4559	0.4543	0.4571
Table 3: Text data: f(y) vs. the sparsity parameter k for various SPCA algorithms.
24
Under review as a conference paper at ICLR 2021
(a) CHR 3, n = 34, 258
(b) CHR 4, n = 30, 328
Fig. 3: Experimental results on real data: f (y) vs. sparsity ratio.
Table 4: Statistics of the population genetics data.
Dataset	# Rows (m)	# Columns (n)	Density
Chr 1	2,240	37,493	0.986
Chr 2	2,240	40,844	0.987
Chr 3	2,240	34,258	0.986
Chr 4	2,240	30,328	0.986
Table 5: Statistics of gene expression and text data.
Dataset	# Rows (m)	# Columns (n)	Density
Gene expression	107	22,215	0.999
Text classification	2,858	12,427	0.004
Table 6: Values of σ1(Z), κ(Z), α, and γZ for various datasets.
Dataset	σι (Z)	κ(Z)	α	γZ
Pit Props with k = 7	0.9999	3.95 × 1010	1.0000	0.9999
Data of Zou et al. (2006) with k = 4	0.9999	5.14 × 1010	1.0000	0.9999
CHR 1 with k = 10, 000	0.9985	7.12 × 106	1.0017	0.9968
CHR2withk = 10, 000	0.9996	6.94 × 106	1.0003	0.9982
CHR3withk = 10, 000	0.9995	9.47 × 107	1.0005	0.9989
CHR4withk = 10, 000	0.9998	1.27 × 107	1.0002	0.9991
Gene expression with k = 5, 000	0.9913	2.05 × 106	1.0001	0.9914
Text Classification with k = 5, 000	0.9997	5.78 × 106	1.0001	0.9987
25