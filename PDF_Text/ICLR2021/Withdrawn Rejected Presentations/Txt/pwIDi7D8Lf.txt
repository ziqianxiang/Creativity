Under review as a conference paper at ICLR 2021
MCMC-Interactive Variational Inference
Anonymous authors
Paper under double-blind review
Ab stract
Leveraging well-established MCMC strategies, we propose MCMC-interactive
variational inference (MIVI) to not only estimate the posterior in a time constrained
manner, but also facilitate the design of MCMC transitions. Constructing a varia-
tional distribution followed by a short Markov chain that has parameters to learn,
MIVI takes advantage of the complementary properties of variational inference
and MCMC to encourage mutual improvement. On one hand, with the variational
distribution locating high posterior density regions, the Markov chain is optimized
within the variational inference framework to efficiently target the posterior despite
a small number of transitions. On the other hand, the optimized Markov chain with
considerable flexibility guides the variational distribution towards the posterior and
alleviates its underestimation of uncertainty. Furthermore, we prove the optimized
Markov chain in MIVI admits extrapolation, which means its marginal distribution
gets closer to the true posterior as the chain grows. Therefore, the Markov chain
can be used separately as an efficient MCMC scheme. Experiments show that MIVI
not only accurately and efficiently approximates the posteriors but also facilitates
designs of stochastic gradient MCMC and Gibbs sampling transitions.
1	Introduction
Markov chain Monte Carlo (MCMC) has become a reference method for Bayesian inference, espe-
cially for tasks requiring high-quality uncertainty estimation. However, its applications to modern
machine learning problems are challenged by complex models and big data. A primary reason is that
MCMC is often restricted to reversible ergodic chains, like Metropolis-Hastings (MH) (Metropolis
et al., 1953; Hastings, 1970) and Gibbs sampling (Geman & Geman, 1984), which require evaluating
the likelihood over the whole data set. A number of MCMC schemes escaping reversibility with
theoretical and/or empirical supports (Bierkens et al., 2019; Chen & Hwang, 2013; Neal, 1998) bring
about considerable advantages such as accelerated mixing and enhanced adaptability to non-conjugate
models, but their designs often demand significant efforts to achieve both efficacy and efficiency.
Stochastic gradient MCMCs (SG-MCMCs) (Welling & Teh, 2011; Ding et al., 2014; Ma et al.,
2015; Li et al., 2016), which exploit the gradient information and neglect MH rejection steps,
have been widely adopted for big data applications. Starting from arbitrary initial samples, SG-
MCMCs move towards the stationary distribution via a random walk with step sizes annealed to zero.
Thus it may either need labor-intensive tuning of the step-size annealing schedule, or easily suffer
from slow mixing or high approximation errors. Variational inference (VI) approximates posterior
p(z | x) with variational distribution q(z) by minimizing KL(q(z) ||p(z | x)), the Kullback-Leibler
(KL) divergence from p(z | x) to q(z) (Jordan et al., 1999; Blei et al., 2017). Though q(z) may
underestimate uncertainty if its presumed distribution family (e.g., diagonal Gaussian) is not flexible
enough, VI is often much faster in finding a high posterior density region than MCMC which explores
the whole parameter space by random jumps based on local information (Robert et al., 2018).
Inspired by the advantages of MCMC and VI that overcome each other’s limitations, we start a
Markov chain with initial values drawn from an optimized variational distribution q(z) so that the
convergence can be expedited. If marginal distributions of this q(z)-mixed Markov chain are more
flexible than the variational distribution family of q, there emerge interesting research questions:
Can the framework of VI curb such a Markov chain from running wild as well as drive it towards
the posterior? If yes, how can we design such a Markov chain that is (richly) parameterized and
jointly optimized with q(z) to deliver posterior approximations as good as valid MCMCs? Therefore,
we are motivated to propose MCMC-interactive variational inference (MIVI) for efficient and high-
1
Under review as a conference paper at ICLR 2021
quality uncertainty estimation. MIVI admits stochastic-gradient optimizations with a small number of
MCMC updates of q(z) and allows fast posterior sampling without keeping track of MCMC iterations.
Furthermore, leveraging MCMCs that converge to the true posterior, we provide the parameterized
Markov chain with an appropriate but adequate amount of flexibility to ease its optimization.
We encounter two-way difficulties when MCMC interacts with VI for mutual improvement. First,
given an MCMC scheme, it is nontrivial to minimize the KL divergence from the posterior to the
marginal distribution of the chain, because the density of the latter is often implicitly-defined by
MCMC transitions. Second, even if the KL divergence is computable, it can be arduous to design a
Markov chain that moderately improves q without worrying about mode collapse or overdispersion.
Our proposed MIVI has well addressed these challenges. To avoid calculating the KL divergence, we
use a discriminator to estimate a log density ratio (Mescheder et al., 2017). To design a Markov chain
that effectively improves q, MIVI borrows the idea of MCMC and (semi-)implicit VIs (Ranganath
et al., 2016; Tran et al., 2017; Yin & Zhou, 2018; Molchanov et al., 2019; Titsias & Ruiz, 2018) and
strikes a balance between flexibility and convergence to the true posterior. Concretely, we replace
unfavorable components of a valid MCMC scheme by (richly) parameterized functions that is to
be learned in the VI framework; we learn step sizes of a SG-MCMC for general-purpose inference
and design model-specific Gibbs-sampling-like Markov chains for more accurate estimations at
lower computing cost. More importantly, the optimized chain in MIVI can used separately as a
valid MCMC. To the best our knowledge, MIVI is the first VI algorithm to utilize Gibbs sampling
transitions and to facilitate their potential inspirition-driven designs.
2	Method description
MIVI is constructed by a variational distribution qφ mixed with a Markov chain, where qφ parameter-
ized by φ is used to initialize T ∈ Z+ transitions of the chain. We use the marginal distribution of the
chain at time T as a refined variational distribution, written as (Z) = R h^τ) (Z | zo)qφ(zo)dzo
where h(ηT ) parameterized by η is the kernel of T transitions of the chain. We show how to optimize
φ and η in the framework of VI given valid formulations of h(ηt), as well as how to formulate such
hηt for monotonically non-increasing KL 喏 φ(z) || P(Z | x)) as t grows. With theoretical support
provided, the short Markov chain admits extrapolation and fast posterior simulation. We defer all the
proofs to Appendix. When there is no ambiguity, we omit the superscript (T) and denote for brevity
the marginal distribution by q%φ and the transition by hη.
We first focus on optimizing φ and η given a valid hη. Supposepθ(x, Z) = pθ(x | Z)p(Z) is the joint
likelihood of data x given Z and priorp(Z). We optimize θ, φ, and η to maximize the ELBO:
max Eη,φ(z) log p⅛⅛ =max %η,φ(z) log ⅛)Q - KL@3(Z) || 9。(Z)).	⑴
θ,φ,η	,	θ,φ,η
The first term on the right-hand side of (1) is simple to estimate if the transition hη is reparameterizable.
Difficulty lies in KL(6*φ(z) || Qψ(z)) because marginal distribution 6%φ(z) is not always in closed
form. To circumvent the difficulty we use a discriminator to estimate log Qq：(；) which only requires
to draw random samples from the two distributions (Mescheder et al., 2017). Specifically, with
fixed 9η,φ(Z) and qφ(z'), an optimal discriminator that is able to distinguish samples from the two
distributions will be D*(z) = log 6%φ(z) - log 9φ(z) that solves
maxD EqQη,φ(z) log σ(D(Z)) + Eqφ(z) log(1 - σ(D(Z))),	(2)
where σ(∙) is the sigmoid function. Consequently, (1) turns out to be
maxθ,φ,η Eqq,φ(z) [logPθ(x, Z) - log 9φ(z) - D*(z)] .	(3)
2.1	Optimization
Theoretically, the ELBO (3) can be maximized if the discriminator is flexible enough. In practice,
however, the saturation of the sigmoid function in the cross-entropy loss of (2) undermines the power
of D to distinguish samples from q。and qrι^. Concretely, if q。is far from qrι^, the optimization
procedure encourages large D, driving σ(D) to approach value 1 which is a saturation region of the
sigmoid function, and consequently, the diminished gradient significantly slows down D from getting
bigger. Meanwhile, when maximizing the ELBO of (3) with an under-optimized discriminator D
2
Under review as a conference paper at ICLR 2021
for KL(⅞η,φ(z) || qφ(z)), a small increase of D cannot compensate for a much larger increase of the
cross entropy -E*,φ log qφ(Z) if qφ and qη,φ are too far from each other. In short, a big discrepancy
between q© and 6,@ impedes optimizing the discriminator and a poor discriminator further spaces
the two distributions. This vicious circle often makes (3) fail to increase 旧彳不力色)logpθ(x, Z) and
hence brings about poor estimations of qη,φ and q© that drift apart from each other.
Even if the discriminator is so flexible that it is unaffected by the vicious circle, optimizing (3) by
gradient ascent with respect to φ can be intractable because D* itself, found by (2), depends on φ.
The problem of calculating this gradient cannot be solved by the strategy of Mescheder et al. (2017)
after the Markov chain is introduced. To circumvent the two aforementioned difficulties when using
the discriminator, MIVI reformulates the objective by maximizing a lower bound of (3) with respect
to θ and η given optimal D* and φ* that are obtained by two auxiliary optimization problems. This
lower bound and the two auxiliary optimization problems are expressed as
maxθ,η E*,φ*(z) [logPθ(x, Z) - log qφ*(z) - D*(z)],
D* = argmaXD E^,φ*(z) logσ(D(z)) + Eqφ* (Z) log(1 - σ(D(z))),
Φ* = arg min© -E(⅛φ(z) log q© (z).
(4)
(5)
(6)
It is straightforward to take the gradient of (4) and (6) with respect to θ and φ, respectively. More
importantly, the following property overcomes the difficulty in taking the gradient of D* with respect
to η when maximizing (4) under the assumption of reparameterizable Markov chain transitions.
Property 1. Suppose hη is reparameterizable, which means there exists a deterministic vector-valued
function f and a random vector ε such that Z(T) 〜qη,φ is equivalent to Z(T) = f (Z((O),ε) where
z(0)〜q©(Z). The gradient of (4) with respect to η is equal to
Eε[Vη log pθ (χ, fη (z(0),ε)) - Vη log qφ* (fη (z(0),ε)) - (Vη 加2(0),£))( ^DzzL ∣ z = % (z⑼,ε) )] ∙
2.2	Formulation of Markov chain transitions
We have discussed the optimization of θ, φ and η in MIVI. But MIVI makes sense only if q%©
is a better posterior approximation than q© . Yet to be determined is the formulation of a valid
transition hη that keeps pushing q%©(Z) closer to P(Z | x) and thus enables extrapolation of the short
Markov chain. We utilize stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as a
general-purpose solution and Gibbs sampling for model-specific but more efficient inference.
SGLD So far hη being reparameterizable is the only assumption of MIVI on the Markov chain.
Consequently, SGLD can be incorporated in MIVI and universally applied, as it approximates
posteriors with stochastic gradient descent and injected Gaussian noise. Concretely, for a mini batch
x of size n from training data of size N, a variable z at discrete time t of SGLD is updated by
z(t) = z(t-1) + η2t[Vz logp(z(tT)) + 与Vz logp(x | z(t-1))] + q, q 〜N(0, nt)	⑺
where ηt is the step size at time t. With a long run and diminishing ηt , SGLD proceeds through two
phases (Welling & Teh, 2011): the first is the phase of stochastic optimization in which p(x, Z) is
being maximized, and the second is the phase of Langevin dynamics in which a random walk is
approximating the posterior sampling. With standard assumptions (Khasminskii, 2011; Vollmer et al.,
2016) to guarantee ergodicity and diminishing step sizes, SGLD converges to a stationary distribution
that well approximates the true posterior. Particularly, Teh et al. (2016) provide conditions under
which SGLD converges to the posterior and find consistent posterior estimators with asymptotic
normality. The following lemma validates the use of any T steps of SGLD transitions in MIVI and
an extrapolation (i.e., more than T steps of transitions).
Lemma 1 (Page 81 of Cover & Thomas (2006)). Suppose Z are variables on a Markov chain M with
the stationary distribution π(z). Let μ(t) be any distribution on the state space of M at t and μ(t+1)
be the marginal distribution after one transitionfrom μ(t). Let q denote the mass/densityfunction of
variables Z(t)〜μ(t) or Z(t+1) 〜μ(t+1). We have KL(q(Z(t)) || π(z)) ≥ KL(q(Z(t+1)) || π(z)).
Specifically, Lemma 1 sheds light on SGLD's continuously refined q；© in terms of its KL divergence
from the stationary distribution, for not only t ≤ T in MIVI, but also t > T in extrapolation as long
as the step sizes appropriately anneal. The guaranteed superiority of q；© over q©, however, is not
enough; running a finite number of transitions, people also seek a balance of fast convergence (low
3
Under review as a conference paper at ICLR 2021
variance) and small discretization errors (low bias) by a good selection of step sizes which may need
to be tuned labor-intensively. To this end, MIVI incorporates T transitions of SGLD, sets η of hη as
step sizes {η1, . . . , ηT}, and optimizes η by (4) for a good bias-variance tradeoff. Moreover, with
qφ of MIVI locating high posterior density regions, the T transitions start from the second phase of
SGLD and the optimized step sizes can be leveraged to extrapolate the markov chain to any length t,
T < t < ∞ for even better posterior estimations.
Gibbs sampling In addition to SGLD, Lemmas 2 and 3 show MIVI can utilize reparameterizable
Gibbs sampling transitions to keep improving 或，@(Z) as t increases and, more significantly, facilitate
an efficient design of MCMC alternative to Gibbs sampling. Specifically, it is implied that for a
Markov chain of variables z of interest and (auxiliary) variables w, using a valid Gibbs sampling
transition for z will keep pushing its marginal distribution closer to the posterior as long as the
Markov chain’s transition for w is good enough.
Lemma 2. Suppose the transition of a Markov chain M of (w, z ) at time t + 1 is r such
that r(w(t+1), z(t+1) | W⑴，Z⑶)=r(z(t+1) | w(t))r(w(t+1) | z(t+1)). Let μ⑴ be any distribu-
tion on the state space of M at t and μ(t+1) be the marginal distribution after one transition
from μ(t). Let q denote the joint mass/density function and thus q(w(t), Z(t), w(t+1), z(t+1)) =
q(w(t), Z(t))r(w(t+1), Z(t+1) | w(t), Z(t)). If r(Z | w) is the conditional distribution ofZ given w
and hence a valid transition of Z in a Gibbs sampler G that converges to the posterior π(w, Z), then
KL(q(w(t), Z(t)) || π(w, Z)) ≥ KL(q(w(t), Z(t+1)) || π(w, Z)).
Lemma 3. With all the assumptions in Lemma 2, suppose μ0(t) is the posterior with
density π(w, Z) and is at time t of M. q 0 denotes the joint mass/density of vari-
ables from μ0(t) and μ0(t+1). If r(w | Z) at time t is close to π(w∣z) in the sense
that Eq(z(t)) KL(r(w | Z(t)) || π(w | Z(t))) ≤ Eq(z(t+1)) KL(q(w(t) | Z(t+1)) || q0 (w(t) | Z(t+1))),
then KL(q(Z(t)) || π(Z)) ≥ KL(q(Z(t+1)) || π(Z)).
As shown by Lemma 2 and 3, if (w, Z) are variables of interest and the full conditional distribution
p(Z | w, x) is reparameterizable, we can use p(Z | w, x) as the transition of Z in the Markov chain.
Furthermore, a (richly) parameterized transition function h(η1) (w | Z, x) is learned by MIVI to well
approximate the full conditional distribution p(w | Z, x) so that, by Lemma 3, Z(t) approaches to
the true posterior p(Z | x) as t increases, not only within the T transitions of MIVI, but also for
t > T when the extrapolated chain serves as an MCMC scheme. This is especially useful when we
care about posterior estimates of Z more than w. Moreover, iterating p(w | Z, x) and h(η1)(w | Z, x)
is an efficient MCMC scheme with fast mixing because the optimized h(η1)(w | Z, x) by MIVI has
located high density regions of p(w | Z, x). We provide in Section 4 specific applications where w
are auxiliary variables enabling a closed-form reparameterizable full conditional distribution of Z .
2.3	MIVI implementation
Instead of keeping the discriminator D and φ optimal in every epoch when optimizing θ and η, we
regard the problem as a three-player game analogous to the two-player game of Mescheder et al.
(2017) in order to reduce the computing cost: 1) Given D and φ, we optimize η and θ to maximize
ELBO(4). 2) Given η, We optimize φ to reduce the discrepancy between q$ and q%φ measured by
the cross entropy (6). 3) The discriminator D tries to differentiate samples from q%φ and q©. Note
that η and φ are learned adversarially and the game terminates at a saddle point that is a maximum
of (4) with respect to n's strategy and a minimum of (6) with respect to φ,s strategy. The ELBO of
MIVL Eqn φ(z) log pθ (X(Z), is bounded above as in Property 2.
PrOPerty2. E*,。⑶ log 注 ≤ 旧μ⑶ log %.
The upper bound together with saturation of σ(D) provides a fast pre-training strategy. Concretely,
given φ* and D* we assume σ(D*) saturates such that Eqn 6*(z)D*(z) ≤ C for some positive constant
C (that may depend on η and φ*). Consequently, (4) is bounded between (Eqn **(之)log pθ(XZ)) - C)
and Eqn φ*(Z)log pθ(XZ)) which, instead, can be optimized to avoid potentially the most time-
consuming training of D. We summarize the implementation of MIVI as Algorithm 1 in Appendix.
We find that MIVI is numerically stable and converges fast as shown in Section 4.
4
Under review as a conference paper at ICLR 2021
3	Related work and contribution
Using a discriminator to approximate a hard-to-compute KL divergence was first introduced by
Mescheder et al. (2017) that enable an arbitrarily flexible variational distribution. It is also adopted
by Li et al. (2017) where the variational posterior is supervised by SG-MCMC. But in their training
procedure the discriminator and variational parameters are entangled in a way that makes it difficult to
rigorously calculate the gradient of the objective function. By contrast, we reformulate the objective
with auxiliary optimization problems and provide rigorously derived gradients. Learning step sizes of
SGLD by VI has been explored by Gallego & Insua (2019) and Nijkamp et al. (2020). The former
utilizes the Gaussianity of SGLD transitions, and the latter regard SGLD as a normalizing flow that
assumes a volume-preserving invertible transformation. Both methods depend on the good properties
of SGLD. Comparatively, the reformulated optimizations of MIVI make it well adapted to different
kinds of MCMCs with reparameterizable transitions, so that many SG-MCMCs, like Hamiltonian
and Langevin dynamics, and Gibbs sampling schemes can be incorporated.
While VI and MCMC have complementary properties, existing works combining the two have
primarily studied one-way improvement. As for utilizing MCMC to facilitate VI, a common practice
is using the refined MCMC marginal distribution to guide and improve the variational distribution.
Ruiz & Titsias (2019) minimize the discrepancy between the variational and a marginal distribution
of Hamilton Monte Carlo (HMC) using the contrastive divergence without explicitly computing
the KL divergence. Titsias (2017) implicitly augments the variational distribution by MCMC and
a model-based reparameterization. Salimans et al. (2015) incorporate in VI finite steps of MCMC
and the MCMC samples are inferred as auxiliary variables; HMC is adopted to illustrate this idea
and is related to normalizing flow. Generally, Rezende & Mohamed (2015) write Hamiltonian and
Langevin dynamics as infinitesimal flows; both flows can be used in VI for a tighter ELBO and
the inference requires volume-preserving invertible transformations. Zhang et al. (2020) construct
measure preserving flows and utilize distribution preservation of Hamilton Monte Carlo. Chen et al.
(2017) propose the use of Langevin dynamics as a way to transit from one latent variable to the next
to improve variational autoencoders (VAEs).
On the other hand, research of using VI to facilitate MCMC includes de Freitas et al. (2001) that
use a variational distribution as the MH proposal to alleviate the poor scaling with dimension of the
independent Metropolis algorithm. Habib & Barber (2019) learn a lower-dimensional embedding
of the parameters of interest by VI to accelerate MCMC mixing. Several works share the idea of
providing MCMC proposals with more flexibility by introducing auxiliary variables (Maddison et al.,
2017; Naesseth et al., 2018; Le et al., 2018). In comparison, we fulfill mutual improvement of VI and
MCMC by MIVI. Being a marginal distribution of valid MCMCs, the variational distribution of MIVI
gets closer to the posterior. MIVI replaces unfavorable parts (like unknown, non-reparameterizable or
manually tuned transitions, see Section 4) of MCMCs by (richly) parameterized functions and learns
them in the framework of VI. In this way, MIVI facilitates designs of MCMCs. More importantly,
with theoretical support, the chain in VI can be extrapolated and used as an efficient alternative to
well established MCMCs. In addition, to the best of our knowledge, MIVI is the first method to
combine VI and Gibbs sampling.
4	Experiments
We first use toy data (deferred to Appendix) and a negative binomial (NB) model to illustrate the
flexibility of MIVI incorprating a few SGLD transitions. Next, we use both Bayesian logistic and
bridge regression to show MIVI and Gibbs sampling facilitate each other when some of the Gibbs
sampling transitions are unknown or not reparameterizable. In addition, we provide experiments of
variational autoencoders (VAEs) (Kingma & Welling, 2014) by MIVI and demonstrate its remarkable
performance compared to existing state-of-the-art algorithms. We use Adam (Kingma & Ba, 2014) to
otpimize θ, φ, η and D with the learning rate as 0.001. Throughout this section unless specified, the
prior p(z) used in Gibbs sampling, mean-field VI (MFVI), and MIVI is N(0, I) for real-valued z and
the variational distribution qφ(z) is a diagonal Gaussian whose mean and log of variances constitute
φ. We set η as the step sizes of SGLD if incorporated in MIVI, and for simplicity, a time-invariant
step size is set and learned. The learned step size can initialize an appropriate decay, like the one
suggested by Teh et al. (2016). Also see Vollmer et al. (2016) for theoretical analysis of SGLD with a
fixed step size. More experiment settings are deferred to Appendix.
5
Under review as a conference paper at ICLR 2021
23-
22 ∙
2.1 -
>-to
1.9-
1Λ∙
1.7 J-
>-to
1.9
14
1.7
OWE
«4« OCT «.«9 0-70 0-71 0.72 0.73 0.74
P
(a) NB: Gibbs and MFVL
23
22
—GibbS
MFVI
(b) NB: MIVI.
Figure 1: Estimated posterior densities. (a) and (b) are the estimated posteriors of r and p for the negative
binomial model by Gibbs sampling (red), MFVI (gray) and MIVI (orange for qφ and blue for q。,.)，respectively.
(c) and (d) are the estimated posteriors of the logistic regression coefficient β and the auxiliary Polya gamma
random variable ω by Gibbs sampling (red) and qη,φ of MIVI (blue).
4.1	Negative binomial model
We draw 1,000 random samples from negative binomial (NB) distribution NB(x | r = 2,p = 0.7)
whose probability p(x) = r(x；；)px(1 - p)r for X ∈ 0 ∪ Z+. We use r 〜Gamma(0.1,0.1) and
P 〜Beta(0.1,0.1) as the prior. Posteriors of r and P under the NB model are estimated by Gibbs
sampling (Zhou et al., 2012), MFVI, and MIVI. We set z = (log(r), logit(p)) in MFVI and MIVI
that incorporates T = 10 SGLD transitions. Shown in Figure 1 (a) are the estimated density contour
plots of (r,p) by Gibbs sampling and the one transformed from (log(r), logit(p))〜q@ by MFVI.
Analogously plotted in Figure 1 (b) are the densities of (r,p) resulting from q© and qη,ψ by MIVI. The
negative correlation in the posterior of r and P as shown by Gibbs sampling has been well recovered
by Qη,ψ in MIVI. Furthermore, the diagonal Gaussian q© in MFVI has underestimated the parameter
uncertainty whereas qφ of the same family in MIVI gives much better variance estimations because
MIVI restrains the discrepancy between q© and qη,ψ.
Next, we accentuate MIVI that uses Gibbs sampling transitions for variables z and replaces unknown
or non-reparameterizable Gibbs transitions for variables w by (richly) parameterized functions.
Compared to SGLD, MIVI needs fewer Gibbs-sampling-like transitions without sacrificing capacity,
and the inferred q∏φ gives comparable posterior estimates to Gibbs sampling. Examples include
Bayesian logistic and bridge regression using auxiliary variables that are difficult to find or sample.
4.2	Bayesian logistic regression
One of the most well-known data augmentation schemes is the Polya gamma (PG) augmentation
for logistic regression (Polson et al., 2013), making the regression coefficients have Gaussian con-
ditional distributions. Specifically, given a unique Xi, i = 1,… ,n, and yi 〜BernOUlli(σ(xiβ)),
p(yi |	Xi,β) =	eyix；	=	e(yi	22)xi	R0∞ e-ωi(xiβ)2∕2p(ωi)dωi	where	p(ωi)	is the density of
1+e i
PG(1, 0) prior on ωi. The conditional posterior of ωi is PG(1, x0iβ) and that of β is a Gaussian
distribution. Iterating the samplings from both distributions defines a valid Gibbs sampler (see
Appendix for details). However, PG distributions are not reparameterizable. Therefore, in the Markov
chain of MIVI we use the Gaussian full conditional distribution as the transition for β and a neural
network gη parameterized by η for local variables ωi ’s. Specifically, concatenating x0iβ and an
independent Gaussian random vector i as the input of gη , the Markov chain in MIVI proceeds
by iterating ωi = gη(χiβ, ej and (β | -)〜N (∑β(X0κ +1), ∑β), where ∑β = (X0ΩX + I)-1,
Ω = diag(ωι,..., ωn), and K = (yι — 0.5, ∙∙∙ ,yn — 0.5).
We synthesize a data set of 1,000 four-dimenstional, correlated Xi 〜N(0, Σ) where the elements
of ∑ are σv,v = 1, v = 1, 2, 3, 4, σ1,2 = σ2,1 = -0.8, σ3,4 = σ4,3 = 0.9 and other σv,v0 = 0. True
β = (βι, β2,β3, β4) is set to be (-2, -1,1, 2) and yi 〜 BemOUlli(σ(x,β)). Good estimations of βι
and β2 should be positively correlated and β3 and β4 negatively correlated. We run only one transition
of the Markov chain in MIVI (i.e., T = 1) and compare q∏φ with Gibbs sampling. Shown in Figure 1
(c) and (d), respectively, are the estimated posterior of β and of ω averaged over data which is
P p(ω | X)P(X)dx. As a result, qd@ of MIVI gives rise to comparable posterior estimations to Gibbs
sampling. We plot in Appendix the estimated posterior ωi for some randomly selected i which are
also similar to those from Gibbs sampling. Additionally, logistic regression of binary MNIST (3
v.s. 5) by MIVI achieves a testing accuracy of 95.79% that matches 95.64% from the MLE of a
well-tuned L2-penalized logistic regression. Also provided in Appendix are estimated distributions
of ωi from q∏φ associated to randomly selected MNIST training images. Therefore, having well
approximated the non-reparameterizable Gibbs sampling transition of ωi ’s by a neural network, MIVI
delivers posterior estimations on par with Gibbs sampling and preserves the classification capacity.
6
Under review as a conference paper at ICLR 2021
-eugo。
AfrequentistBGibbs* MIVI*OLS
*frequentist>extrapolation∙ MIVI *OLS
-e-豆。。
-e-豆。。
*frequentist>extrapolation∙ MIVI *OLS
-500	500	-500	500	-500	500
(a) α = 1 (Lasso).	(b) α = 0.5.	(C) α = 1.5.
Figure 2: Bridge regression of diabetes data. (a) is results of α = 1 (Lasso), (b) α = 0.5 and (c) α = 1.5,
inCluding point estimates of β by a frequentist approaCh (green triangle) minimizing the loss funCtion, Gibbs
sampling or the extrapolated Markov chain (red square), ∣qη,φ of MIVI (blue dot) and OLS (yellow diamond)
and the 95% CIs by Gibbs (or the extrapolated Chain) and MIVI.
4.3	Bayesian bridge regression
Next we show MIVI not only well approximates posteriors but also helps to design valid Gibbs-
sampling-like MCMC when some Gibbs sampler transitions are unknown in analytic expressions.
1p
Bridge regression tries to find β = (βι,...,βp) that minimizes 2 ||y - Xβ∣∣2 + ψ Ev=I ∖βv ∣a
given the choice of α ∈ (0, 2) and ψ > 0. From a Bayesian perspective, a hirarchical model
for bridge regression is p(y ∖ X,β,σ) = N(y ∖ Xβ,σ2I), p(1∕σ2) = Gamma(1∕σ2 ∖ r,c), and
p(βv ∖ ɑ, ρ, σ) a e-ρβv/σ'° for V = 1,...,p, where r and C are the gamma shape and rate pa-
rameter, respectively, and ρ is a hyper-parameter regularizing the Lα norm of β . A data augmen-
tation that writes p(βv ∖ α, ρ, σ) as a scale mixture of normals enables conjugacy. Specifically,
—c∣β /c∣α	∞ — λvβv p2/a /
e ρlβv/σl = J0 e 2σ2 P g(λv)dλv, where g(λv) is proportional to the density of a positive
stable distribution with index of stability a/2 (WeSt, 1987; Polson et al., 2014). While both the prior
and full conditional of β are Gaussian, neither the posterior nor the full conditional distribution of λv
is known in closed form, which impedes an efficient Gibbs sampler under this data augmentation.
To circumvent the unknown conditional distribution of global variables λv ’s, we use a flexible
reparameterizable distribution to approximate their marginal distribution which serves as a time-
invariant transition of λv 's in the Markov chain of MIVI. For simplicity, We adopt Weibull distributions
as λv 〜WeibUll(av, bv), which is equivalent to λv = a。elog(- logu)/bv, U 〜Uniform(0,1), but
other flexible distributions on R+, like a neural network with random noise as input, also work as long
as they are reparameterizable. Given λv's, β and σ2 are updated according to their full conditional
distributions. Concretely, the Markov chain of MIVI proceeds by iterating
(λv ∖-)〜WeibUll(av ,b), V = 1,...,p,	(β ∖ -)〜N (ΣX 0y,σ2Σ),
(1∕σ2 ∖-)〜Gamma(r + n+p ,c + 1 ∖∖y - Xβ∖∖2 + 1 Pv=1 P2βV),	(8)
where Σ = (X0X + ρ22αΛ)-1, A = diag(λ1,…，λp), andn is the number of observations. With
η = (log a1, log b1, . . . , log ap, log bp) optimized by MIVI, the Markov chain can be extrapolated to
approximate a collapsed Gibbs sampler whose transition of λv's are their marginal distributions. Note
that bridge regression is reduced to Lasso if α = 1 and a Gibbs sampler is feasible by imposing a
Laplacian prior on β (Park & Casella, 2008). When α 6= 1, Polson et al. (2014) has proposed a Gibbs
sampler that requires truncated multivariate distributions for parameter updates, which may require
inefficient rejection sampling. Additionally, the data augmentation by the positive stable distributed
variables that results in full conditional distributions of β and σ2 in (8) is different from the one in
Polson et al. (2014) and cannot be reduced to the one in Park & Casella (2008) when α = 1.
With α = 0.5, 1, and 1.5 we showcase MIVI for bridge regression on diabetes data (Efron et al.,
2004) and the validity of the extrapolated Markov chain (8) as an MCMC scheme. Since choosing
the hyper-parameter ρ is outside our scope of research, for α = 1 we run the Gibbs sampling (Park &
Casella, 2008) with the suggested value of ρ, followed by MIVI (T = 3) and frequentist Lasso that
approximately match the L1 norm of β. For α = 0.5 and 1.5, we first use 4-fold cross-validation
to select the value of the hyper-parameter, and then run MIVI (T = 3) with ρ chosen to match the
Lα norm. In addition, we use the optimized Weibull distribution to extrapolate the Markov chain
from random initial values. In Figure 2 we provide the point estimates of β resulting from MIVI,
7
Under review as a conference paper at ICLR 2021
Table 1: Comparison of VAE algorithms on MNIST and fMNIST (z ∈ R40).
	Vanilla	SIVI	DSIVI	UIVI	VCD	VIS-5-5	MIVI-5-0	MIVI-5-5
MNIST	-86.48	-84.71	-83.79	-83.47	-81.01	-83.82	-84.39	-83.09
fMNIST	-121.95	-118.69	-112.02	-109.97	-109.90	-106.96	-108.86	-102.51
Gibbs sampling (α = 1) or the extrapolated Markov chain (8) (α = 0.5 and 1.5) where MCMC
samples from the last 1,000 of a total of 5,000 iterations are collected for inference, and the frequentist
bridge regression along with ordinary least squares (OLS). Also reported are the 95% confidence
intervals (CIs) by MIVI and the Gibbs sampling or the extrapolated chain. While estimation of β
by Qη,φ of MIVI is not sparse in the exact sense, the point estimates (and CIs) coincide with those
by frequentist Lasso (and Gibbs sampling for α = 1). Moreover, for α = 0.5 and 1.5, frequentist
estimates lie around the center of the CIs by MIVI and the extrapolated chains. For α = 1 we also run
an extrapolated chain of MIVI and the CIs are similar to MIVI. Together with Sections 4.2, the results
endorse MIVI as an alternative to Gibbs sampling but in a way of simplicity and high efficiency.
4.4	Variational autoencoders
We consider MIVI of latent variables in VAEs on two data sets. One is stochastically binarized
MNIST (Salakhutdinov & Murray, 2008) consisting of 50,000 training and 10,000 testing images of
hand written digits. The other is fashion MNIST (fMNIST) (Xiao et al., 2017) consisting of 60,000
training and 10,000 testing images of clothing items, where the pixels are binarized at threshold 0.5.
The variational distribution qφ(z | x) of the latent code z is diagonal Gaussian whose mean and log of
variances are parameterized by two separate fully connected neural networks with two hidden layers
of 200 units and ReLU activation functions. The same network structure is used for the Bernoulli
probability of decoder pθ(x | z), except for a sigmoid transformation of the output. T = 5 SGLD
transitions are incorporated in MIVI, with η as the parameter of a neural network whose input is xi
and output is the time-invariant step size of the SGLD for t = 1, . . . , T. For comparison, vanilla
VAE (Kingma & Welling, 2014) and five recently proposed algorithms are used as benchmarks:
semi-implicit VI (SIVI) (Yin & Zhou, 2018), doubly semi-implicit VI (DSIVI) (Molchanov et al.,
2019), unbiased implicit VI (UIVI) (Titsias & Ruiz, 2018), variational contrastive divergence (VCD)
(Ruiz & Titsias, 2019), and variationally inferred sampling (VIS) (Gallego & Insua, 2019). SIVI,
DSIVI and UIVI use implicit distributions as the variational distribution to provide a high degree of
flexibility. VCD and VIS have been discussed in Section 3. We reproduce these approaches with the
same configuration and neural network structures as of MIVI. Note that VCD has been reported to
outperform Hoffman (2017), and the latter outperforms Salimans et al. (2015) that uses Hamiltonian
flow (see Ruiz & Titsias (2019) and Hoffman (2017)).
We evaluate the performance via the average marginal log-likelihood calculated by importance
T
sampling, written as logp(x) ≈ log } Pj=ι pθ t jp) j for reasonably large J. See Appendix for
detailed settings and discussion. For MIVI with T = 5, we run 0 or 5 SGLD transitions with the
optimized step sizes on testing images, denoted respectively by MIVI-5-0 that uses qφ and MIVI-5-5
(5)
that uses q； φ for testing. VIS also has 5 steps of SGLD for training and 5 for testing (denoted by
VIS-5-5). Provided in Table 1 is the performance comparison of the VAE algorithms for z ∈ R40.
MIVI slightly outperforms other algorithms except VCD on MNIST and outperforms all the others
on fMNIST. MIVI can be better than VIS because we use a neural network whose input is xi to learn
the SGLD step size of zi for each i, whereas VIS learns (or pre-specifies) an equal step size for all
zi ’s. Additional results on VAEs are provided in Appendix.
5 Conclusion
The proposed MIVI incorporating a short Markov chain encourages VI and MCMC to overcome each
other’s limitations and to achieve mutual improvement. We establish MIVI by auxiliary optimizations
so that all the gradients can be rigorously computed and the training becomes stable. We formulate the
Markov chain by transition functions that are partly adopted from valid MCMC and partly optimized
in the framework of VI. Moreover, we prove the short chain in MIVI can be extrapolated and serve as
an efficient MCMC that approaches towards the posterior, and consequently, MIVI facilitates designs
of MCMC transitions. Therefore, capable of posterior approximation and simulation without keeping
track of an MCMC trajectory, MIVI is an overall solution to effective and efficient point estimation
and uncertainty quantification.
8
Under review as a conference paper at ICLR 2021
References
Joris Bierkens, Paul Fearnhead, Gareth Roberts, et al. The Zig-Zag process and super-efficient
sampling for Bayesian analysis of big data. The Annals ofStatistics, 47(3):1288-1320, 2019.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112(518):859-877, 2017.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
preprint arXiv:1509.00519, 2015.
Changyou Chen, Chunyuan Li, Liqun Chen, Wenlin Wang, Yunchen Pu, and Lawrence
Carin. Continuous-time flows for efficient inference and density estimation. arXiv preprint
arXiv:1709.01179, 2017.
Ting-Li Chen and Chii-Ruey Hwang. Accelerating reversible Markov chains. Statistics & Probability
Letters, 83(9):1956-1962, 2013.
Thomas M Cover and Joy A Thomas. Elements of information theory 2nd edition (Wiley series in
Telecommunications and Signal Processing), 2006.
Nando de Freitas, Pedro H0jen-S0rensen, Michael I Jordan, and Stuart Russell. Variational MCMC.
In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, UAI’01, pp.
120-127, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1558608001.
Nan Ding, Youhan Fang, Ryan Babbush, Changyou Chen, Robert D Skeel, and Hartmut Neven.
Bayesian sampling using stochastic gradient thermostats. In Advances in Neural Information
Processing Systems, pp. 3203-3211, 2014.
Bradley Efron, Trevor Hastie, Iain Johnstone, Robert Tibshirani, et al. Least angle regression. The
Annals of Statistics, 32(2):407-499, 2004.
Victor Gallego and David R^os Insua. Variationally inferred sampling through a refined bound for
probabilistic programs. arXiv preprint arXiv:1908.09744, 2019.
Stuart Geman and Donald Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:
721-741, 1984.
Raza Habib and David Barber. Auxiliary variational MCMC. the International Conference on
Learning Representations (ICLR), 2019.
WK Hastings. Monte Carlo sampling methods using markov chains and their applications. Biometrika,
57(1):97-109, 1970.
Matthew D Hoffman. Learning deep latent gaussian models with markov chain monte carlo. In
International Conference on Machine Learning, pp. 1510-1519, 2017.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine Learning, 37(2):183-233, 1999.
Rafail Khasminskii. Stochastic stability of differential equations, volume 66. Springer Science &
Business Media, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In the International
Conference on Learning Representations (ICLR), 2014.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and Frank Wood. Auto-encoding sequential
Monte Carlo. In International Conference on Learning Representations (ICLR), 2018.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic
gradient Langevin dynamics for deep neural networks. In 30th AAAI Conference on Artificial
Intelligence, 2016.
9
Under review as a conference paper at ICLR 2021
Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised MCMC.
arXiv preprint arXiv:1702.08343, 2017.
Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient MCMC. In
Advances in Neural Information Processing Systems,pp. 2917-2925, 2015.
Chris J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi, Andriy Mnih,
Arnaud Doucet, and Yee Teh. Filtering variational objectives. In Advances in Neural Information
Processing Systems, pp. 6573-6583, 2017.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational Bayes: Unify-
ing variational autoencoders and generative adversarial networks. In Proceedings of the 34th
International Conference on Machine Learning, pp. 2391-2400, 2017.
Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward
Teller. Equation of state calculations by fast computing machines. The Journal of Chemical
Physics, 21(6):1087-1092, 1953.
Dmitry Molchanov, Valery Kharitonov, Artem Sobolev, and Dmitry Vetrov. Doubly semi-implicit
variational inference. International Conference on Artificial Intelligence and Statistics, 2019.
Christian Naesseth, Scott Linderman, Rajesh Ranganath, and David Blei. Variational sequential
Monte Carlo. In International Conference on Artificial Intelligence and Statistics, pp. 968-977,
2018.
Radford M Neal. Suppressing random walks in Markov Chain Monte Carlo using ordered overrelax-
ation. In Learning in Graphical Models, pp. 205-228. Springer, 1998.
Erik Nijkamp, Bo Pang, Tian Han, Linqi Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning
multi-layer latent variable model via variational optimization of short run MCMC for approximate
inference. arXiv:1912.01909, 2020.
Art B Owen. Importance sampling. Monte Carlo Theory, methods and examples.: http://statweb.
Stanford. edu∕~ owen/mc/Ch-var-is. pdf, 2009.
Trevor Park and George Casella. The Bayesian lasso. Journal of the American Statistical Association,
103(482):681-686, 2008.
Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models
using P6lya-gamma latent variables. Journal ofthe American statistical Association, 108(504):
1339-1349, 2013.
Nicholas G Polson, James G Scott, and Jesse Windle. The Bayesian bridge. Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 76(4):713-733, 2014.
Rajesh Ranganath, Dustin Tran, and David Blei. Hierarchical variational models. In International
Conference on Machine Learning, pp. 324-333, 2016.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. volume 37 of
Proceedings of Machine Learning Research, pp. 1530-1538, 2015.
Christian P Robert, Victor Elvira, Nick Tawn, and Changye Wu. Accelerating MCMC algorithms.
Wiley Interdisciplinary Reviews: Computational Statistics, 10(5):e1435, 2018.
Francisco JR Ruiz and Michalis K Titsias. A contrastive divergence for combining variational
inference and MCMC. In Proceedings of the 28th International Conference on Machine Learning
(ICML-19), 2019.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of deep belief networks. In
Proceedings of the 25th International Conference on Machine Learning, pp. 872-879, 2008.
Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218-1226,
2015.
10
Under review as a conference paper at ICLR 2021
Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer. Consistency and fluctuations for
stochastic gradient Iangevin dynamics. The Journal ofMachine Learning Research, 17(1):193-225,
2016.
Michalis K Titsias. Learning model reparametrizations: Implicit variational inference by fitting
MCMC distributions. arXiv preprint arXiv:1708.01529, 2017.
Michalis K Titsias and Francisco JR Ruiz. Unbiased implicit variational inference. arXiv preprint
arXiv:1808.02078, 2018.
Dustin Tran, Rajesh Ranganath, and David Blei. Hierarchical implicit models and likelihood-free
variational inference. In Advances in Neural Information Processing Systems, pp. 5523-5533,
2017.
Sebastian J Vollmer, Konstantinos C Zygalakis, and Yee Whye Teh. Exploration of the (non-)
asymptotic bias and variance of stochastic gradient langevin dynamics. The Journal of Machine
Learning Research, 17(1):5504-5548, 2016.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 681-688,
2011.
Mike West. On scale mixtures of normal distributions. Biometrika, 74(3):646-648, 1987.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: A novel image dataset for bench-
marking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In Proceedings of the 28th
International Conference on Machine Learning (ICML-18), 2018.
Yichuan Zhang, Jos6 Miguel Herndndez-Lobato, and Zoubin Ghahramani. Ergodic measure PreserV-
ing flows. the International Conference on Learning Representations (ICLR), 2020.
Mingyuan Zhou, Lingbo Li, David Dunson, and Lawrence Carin. Lognormal and gamma mixed
negative binomial regression. In Proceedings of the International Conference on Machine Learning,
volume 2012, PP. 1343, 2012.
11
Under review as a conference paper at ICLR 2021
MCMC-Interactive Variational Inference: Appendix
A	Proofs
Proofof Property 1. Calculating Nn旧内力*(2)[logpθ(x, Z) - log qφ*(z)] is straightforward and
hence We only need to derive NnEg6*(z)D*(x,z). Given φ*, D*(x,Z) = log。黑；Zz), and
the fact that the expectation of a score function is 0, we have
%η,φ*(z)Nnd*(X, Z)= Eη,φ* (Z)Nn log qn,Φ*lz) = 0∙
Consequently, with a reparameterizable qη,φ We have
Eε h(NnD*)(x,%(Z(O),ε))] =0.
Therefore, taking the gradient of Eqn **(z)D*(x, Z) with respect to η we get
Nn Eqn,φ* (z)Dt(x, Z) = Nn EεD*(x,% (Z(O) ,ε))
=Eε [NnD*(x,f (Z(O),ε))]
=Eε (NnD*)(x,fn(Z(O),ε)) + (Nn f (Z(O),ε))("D：；, Z) ∣ z=%氏⑼⑶)
=Eε (Nnfn(Z⑼，ε))( "D*,；, Z) ∣ z=fn(z(0),ε)).
□
Proof of Property 2.
Eqqη,φ (z) log
p(x, Z)
qn,Φ(Z)
Eqqη,φ (z) log
p(x, Z)
qφ(Z)
-KL伍n,φ(Z) || qφ(Z)) ≤ %n,φ(z) log
p(x, Z)
qφ(Z)
□
Lemma 4 (Chain rule of KL divergence, page 25 of Cover & Thomas (2006)).
KL(q(w,z) || q0(w, z)) = KL(q(w) || q0(w)) + Eq(w)KL(q(z |w) || q0(z | w)).
ProofofLemma 2. Let μ0(t) be an arbitrary distribution on the state space of M at t
with the joint mass/density function denoted by q0 and thus q0(w(t), Z(t), Z(t+1)) =
q0 (w(t), Z(t))r(Z(t+1) | w(t), Z(t)). By Lemma 4,
KL(q(w(t), Z(t), Z(t+1) || q0(w(t), Z(t), Z(t+1)))
=KL(q(w(t), Z(t)) || q0(w(t), Z(t))) + Eq(w(t),z(t))KL(q(Z(t+1) | w(t), Z(t)) || q0(Z(t+1) | w(t), Z(t)))
=KL(q(w(t), Z(t)) || q0 (w(t), Z(t))) + Eq(w(t),z(t))KL(r(Z(t+1) | w(t)) || r(Z(t+1) | w(t)))
=KL(q(w(t), Z(t)) || q0(w(t), Z(t)))
Again, by Lemma 4,
KL(q(w(t), Z(t), Z(t+1)) || q0(w(t), Z(t), Z(t+1)))
=KL(q(w(t), Z(t+1)) || q0(w(t), Z(t+1))) + Eq(w(t),z(t+1))KL(q(Z(t) | w(t), Z(t+1)) || q0(Z(t) | w(t), Z(t+1)))
≥KL(q(w(t), Z(t+1)) || q0(w(t), Z(t+1)))
So	KL(q(w(t),	Z(t))	||	q0(w(t),	Z(t)))	≥ KL(q(w(t),	Z(t+1)) || q0(w(t), Z(t+1))).	Let
q0(w(t), Z(t)) = π(w, z), i.e., μt is the posterior distribution to which the Gibbs sampler G
converges. Since M’s transition r(Z|w) is the conditional distribution as well as the transition of G,
q0 (w(t), Z(t+1)) = q0(w(t))r(Z(t+1) | w(t)) = π(w, Z). Therefore,
KL(q(w(t), Z(t))) || π(w, Z)) ≥ KL(q(w(t), Z(t+1)) || π(w, Z)).
□
12
Under review as a conference paper at ICLR 2021
ProofofLemma 3. Since μ(t) can be any distribution, We assume that q(w(t), z(t)) =
q(z(t))r(w(t) | z(t)) for arbitrary q(z(t)). By the proof of Lemma 2,
KL(q(w(t), z(t)) || q0(w(t), z(t))) ≥ KL(q(w(t), z(t+1)) || q0(w(t), z(t+1))).
By Lemma 4, The left-hand side of this inequality is equal to
KL(q(z(t)) || π(z)) + Eq(z(t))KL(r(w | z(t)) || π(w | z(t))),
and the right-hand side is equal to
KL(q(z(t+1)) || q0(z(t+1))) + Eq(z(t+1))KL(q(w(t) | z(t+1)) || q0(w(t) | z(t+1))).
Since Eq(z(t))KL(r(w | z(t)) || π(w | z(t))) ≤ Eq(z(t+1))KL(q(w(t) | z(t+1)) || q0(w(t) | z(t+1))),
We have
KL(q(z(t)) || π(z)) ≥ KL(q(z(t+1)) || q0(z(t+1))).
Considering q0(z(t+1)) = r(z(t+1) | w(t))q0(w(t))dw(t) and r(z | w) is the conditional distribu-
tion of z given w, We have q0(z(t+1)) = q0(z(t)) = π(z). Therefore,
KL(q(z(t)) || π(z)) ≥ KL(q(z(t+1)) || π(z)).
□
B Full algorithm and detailed implementation
We summarize the implementation of MIVI as Algorithm 1. The neural netWork structure of
discriminator D depends on the dimension of z and does not have to be complex because, anyWay, the
sigmoid function saturates when q© and 6,@ are far from each other at an early stage of training. To
avoid a potentially time-consuming optimization of D, We simply omit it at the early stage of training
according to the analysis of Property 2, and start training D after first M epochs when q© and qη,φ get
closer. In this way, a reasonably flexible D is good enough. Since a cross-entropy loss is used to train
D, D works well if the loss drops from a large value towards 0, which have been observed in our
experiments. Furthermore, we find that the algorithm converges faster if we stop the gradient of zj(t)
with respect to φ in line 10 of Algorithm 1; in PyTorch, we use the command .detach() on zj(t).
Essentially, stopping the gradient is only optional and does not change parameter estimations in our
experiments. We minimize (6) where the expectation is approximated by sampling z(t) from qη,φ to
let q© and qη,φ get close to each other; stopping the gradient of z(t) with respect to φ can be regarded
as fixing qη,φ. In this way, we let q© approach to qη,φ that has been well learned by optimizing (4) so
faster convergence can be achieved. Note that qη,φ is less and less dependent on φ as the number of
transitions increases. So there is no need to stop the gradient if T is large.
C	Bayesian logistic regression and Polya gamma distribution
For a unique Xi, i = 1,… ,n and yi ∈ {0,1},the hierarchical model for Bayesian logistic regression
can be expressed as
yi 〜BernOUlli(1∕(1 + e-xiβ)),
β 〜N(b,B).
As in Polson et al. (2013), under the Polya-gamma (PG) distribution based data augmentation, the
full conditional distributions can be expressed as
(ωi | -) 〜 PG(1, x0iβ), i = 1, . . . ,n,
(β∣-)〜N (∑(X0κ + B-1b), ∑),
where Σ = (X0ΩX + B)-1, Ω = diag(ωι,…，ωn) and K = (yi — 2,…，yn — 2). Note that
ωi 〜PG(1, xiβ) is equivalent to ωi=2∏ Pk=I ^^分产+彳吗β∕2π)2 where Yk 〜d Gamma(1,1). To
13
Under review as a conference paper at ICLR 2021
Algorithm 1 MCMC-interactive variational inference
Input: Data x, model pθ (x | z), prior p(z), reparameterizable variational family qφ and Markov
chain updating function fη implied by reparameterizable hη
Output: θ, φ, η
1:	Epoch J 0.
2:	while not converge do
3:	Draw mini-batch x of size n from training data of Size N (for MIVI with SGLD)
4:	Sample zj0) iid qφ(z), j = 1,...,J.
5:	# Begin Markov chain transitions:
6:	for t = 1,…，T, say T = 3 and j = 1,...,J do
7:	zj(t) = fη(zj(t-1), εj(t)) with some independent random vector ε(jt).
8:	end for
9:	# Begin optimization:
10:	Update φ by descending the gradient
-v° j⅛t X log q。(Zjt))
j,t
11:	if Epoch < M, say M = 100 then
12:	Update θ and η by ascending the gradient
1	pθ(x | zj(t))p(zj(t))
vθη J × T 工 og	( (t)x
J× T j,t	qφ(zj )
13:	else
14:	Update θ and η by ascending the gradient
1 V11	Pθ(X | Zjt))P(Zjt))	(6
vθ,η J×T E log—K)------------------------D(Zj )
j,t	qφ(Zj )
15:	Update D by maximizing
j×T X log σ(D(Zjt))) + J X log (1 -σ(D(ZjO))))
j,t	j
16:	end if
17:	Epoch J Epoch + 1
18:	end while
14
Under review as a conference paper at ICLR 2021
generate PG random variables, Polson et al. (2013) use rejection sampling with finite truncations of
this expression as the proposal and Zhou et al. (2012) uses finite truncations together with matching
the first- and second-order moments. Neither solution, however, can be used as a Markov chain
transition in MIVI due to the lack of reparameterization.
We plot the estimated posteriors of ω∕s associated to the synthesized data by Gibbs sampling and
MIVI in Figure 3 and those of binary MNIST by MIVI in Figure 4 for eight randomly selected i in
training data.
Figure 3: PG auxiliary variable ωi by Gibbs sampling (red) and MIVI (blue) for eight randomly selected
samples of the synthesized data in Section 4.2.
10 ʧ
klLl
0.0	0.2 0.0	0.2	0.0	0.2	0.0	0.2
5
0
Figure 4: PG auxiliary variable ωi by MIVI for eight randomly selected training images of the binary MNIST
data in Section 4.2.
15
Under review as a conference paper at ICLR 2021
D	Experiment settings
D.1 General settings
With the definitions of J and M in Algorithm 1, we run 1,000 epochs with J = 200, T = 5, and
M = 100 in the toy experiment of Section E.1 and 2,000 epochs with J = 1000 and M = 0 for the
negative binomial model in Section 4.1. For the Bayesian logistic in Section 4.2 we run 1,000 epochs
with J = 200 and M = 0. For the Bayesian bridge regression in Section 4.3 we run 1,000 epochs
with J = 100 and M = 0. For the VAE by MIVI in Section 4.4 we run 2,500 epochs with J = 10
and M = 200.
For experiments of VAE on MNIST and FashionMNIST, we follow the original partition to split the
data as 50,000/10,000/10,000 for training/validation/test. The MNIST data is dynamically binarized,
and the FashionMNIST data is binarized with 0.5 as a threshold for each pixel. The dimension of
the latent variable z is set as 40. To ensure the fairness of comparison, we use the same network
architecture to build up the VAE on UIVI and VCD and use the same experiment configuration as in
Titsias & Ruiz (2018) and Ruiz & Titsias (2019). We apply a 2-hidden-layer network with 200 hidden
units for both encoder and decoder and choose ReLU as the activation function. Then we optimize
the model using the initial learning rate as 0.001 with a 10% decay for every 15,000 iterations, and
choose the best model with validation set for testing. Specifically for SIVI-VAE and DSIVI-VAE, the
dimension of ψ is set as 500. For MIVI we run 2, 500 epochs with the initial Adam learning rate as
0.001 (with a 12% decay for every 100 epochs) for MNIST and 0.0001 (with a 10% decay for every
200 epochs) for fMNIST.
D.2 Performance evaluation of MIVI on VAEs
In Section 4.4 we evaluate MIVI for VAEs by estimating the average marginal log-likelihood,
J	J
log P(X) ≈ log J X pθ(xjj=log J X pθ≡⅛π e-Dgj)
The correctness of right-hand side of this equation depends on an optimal discriminator D*, which
can be hard to verify. Therefore, we use the Gaussianity of SGLD and a Monte Carlo method to
evaluate 6必 Specifically, the updating function of SGLD is f (z, E) such that
z(t) = fηt (z(t-1), t)
=z(t-1) + η Θ [Vz logp(z(t-1)) + NVz logP(X | z(t-1))] + q
2n
where Et 〜N(0, diag(ηt)) and Θ stands for element-wise multiplication. So we have Z(T) 〜
N(μ(z(T-1),ητ), ητ) where μ(z, η) = Z + 2 Θ [Vz logp(z) + Nn Vz logP(X | z)] and consequently,
Z(TT 〜N (μ(z(T-1),ητ), diag(ητ))
=N (μ(fnτ-ι (Z(T-2),Eτ-i),ητ-i), diag(ητ))
=N (μ(fnτ-ι (fnτ-2(Z((T-3), Et-2), ET-1), ηT-1), diag(ηr))
=N (μ(fnτ-ι (fητ-2 (... (fηι (Z(O), E1), E2)...), ET -i), diag(ητ)).
Therefore, the marginal distribution qrιφ is equal to
/ . . . / N (μ(fητ-ι (fητ-2 (. . . (f∏ (Z(O) ,E1),E2) ...), ET-1)), diag(ητ)) dP (El) ...dP(Eτ-ι)qφ (Z(O))dz(0)
1 K	(O)
≈K XN (μfrτ-ι (ZrT-2(... (frι (zk , Eι,k), E2,k).. ∙), ET-ι,k)), diag(ητ))	⑼
16
Under review as a conference paper at ICLR 2021
where ZkO) iid qφ, 6t,k innd N(0, diag(ηt)) for k = 1,..., K and t = 1,..., T - 1. We evaluate the
performance of MIVI for VAEs by
j
1	/-ʌ	1	1 LPθ(X | Zj)p(zj)
log p(χ) ≈ log-X -1~~(∖~
J j=1	qη,φ(Zj)
(10)
Where Zj =为式于丁叮一 (...(fη (zj°),eιj ),62,j). ∙ ∙),eτj), z(0 嗯 qφ and
qη,Φ(Zj) = K+1N (zj | μ(fητ-ι (fητ-2(. ∙ ∙ (fηι (ZjO),eι,j), e2,j).. J, eT-i,j)), ητ) 十
1K
K + 1)： N (ZjI μ(fητ-ι (fητ-2 (. . . (fηι (Zk ,e1,k ), e2,k ) .∙∙), eT -1,k )), ητ) (II)
K +1 k=1
analogous to Yin & Zhou (2018).
We set J = 1000 and K = 50 for the evaluation by the importance sampling. Note what we are
estimating in (10) is in fact a lower bound of logp(X) (Burda et al., 2015). Its quality depends on both
the decoder pθ (x | Z) and the encoder which is used as the importance distribution; fixingpθ(x | Z), a
poor importance distribution may give rise to a loose bound. The estimation OfMIVI-5-5 (using GnT)
as the importance distribution) is better than that of MIVI-5-0 (using qφ as the importance distribution)
because pθ (x | Z) is trained based on qGη(T,φ). Moreover, in case of multimodality of p(Z | x) which is
very probable for VAE models, qφ can be lighter-tailed than qGη,φ and may result in larger variance
of the importance sampling estimation. In addition, we need be careful about extrapolation when
conducting the importance sampling based estimation as in (10), which is only valid under the
assumption that the importance distribution q satisfies q(Z) > 0 when p(x | Z)p(Z) 6= 0 (Owen,
2009). Concretely, though we have observed that the value obtained by (10) for MIVI-5-t increases
as t grows, that value may no longer reflect the true performance of the model, since qGη(t,)φ may no
longer maintain non-negligible density on the regions where the joint likelihood has non-negligible
values. So we only compare MIVI-5-5 so that the number of transitions are the same in training and
testing.
E S upplimentary experimental results
E.1 Toy experiments
Table 2: Target bivariate distributions.
Correlated Gaussian	Banana	Gaussian mixture
a√∣0∣	1 ~0.8 I ʌ~~~z2 八,门~~~1. ∕∣-1∣	1	-0.5∣∖ , 1 Ar ∕∣1.3∣	1 ~03
N(Iθ],∣0.8	1	N (z1; 五,1)N (z2;0, 4)	2 N(J-II , [-0.5	1 D+ 2 N(J1.3],∣0.3	1
(b) Banana.
Figure 5: Target distributions (red) and fitted 备啰(blue) of MIVI.
Ground truth
To show the validity and flexibility of qGφ,η of SGLD in MIVI, we fit synthetic bivariate distributions
listed in Table 2. Figure 5 shows the contour plots of the synthetic bivariate distributions (red) along
with the fitted qGη,φ(Z) (blue). In all cases, qGη,φ(Z) has well recovered the target distribution and
17
Under review as a conference paper at ICLR 2021
-3	-2	-1	0	1	2	3	-4	-2	0	2	4
(a) Correlated Gaussian.	(b) Banana.
Figure 6: Target distributions (red) and fitted qφ (orange) of MIVI.
captured the bivariate correlation, dependence, and multimodality, respectively, despite the small
number of SGLD updates. In addition, Figure 6 shows that qφ of MIVI has captured the large
varianace of each dimension of z .
E.2 Additional results of VAE
Table 3: Comparison of VAE algorithms on MNIST and fMNIST (z ∈ R10).
	Vanilla	SIVI	DSIVI	UIVI	VCD	VIS-5-5	MIVI-5-0	MIVI-5-5
MNIST	-97.82	-96.78	-89.96	-94.09	-95.86	-87.65	-92.04	-88.50
fMNIST	-124.73	-121.42	-121.39	-110.72	-117.65	-116.27	-117.74	-113.17
We try a lower dimensional z, set z ∈ R10 in all models, keep other settings the same as in z ∈ R40,
and report the VAE model comparison in Table 3 where we cite the results of UIVI and VCD for
z ∈ R10 from Titsias & Ruiz (2018) and Ruiz & Titsias (2019), respectively. It is shown that MIVI-
5-5 is as good as VIS-5-5 which also uses SGLD for a refined encoder, and outperforms implicit
VI approaches (except UIVI on fMNIST) because MIVI’s encoder as in (11) is not only flexible
but also less complex in parameterization and hence easy to optimize. We show reconstructions of
randomly selected binarized MNIST testing images by MIVI in Figure 7 panel (a) and some of the
most improved ones in panel (b). The first column is the testing image, the second column is the
reconstruction using Z 〜qψ, and the third to the twelfth columns use Z from 成tφ for t = 1,..., 10,
respectively, with fine-tuned step sizes. Overall, the reconstructions are good enough by Z 〜qφ and
can be further improved by 成tφ as t increases.
18
Under review as a conference paper at ICLR 2021
7乙
7乙
7乙
7乙
7乙
7乙
7乙
7乙
72
7i
72
7乙
dəodododoooo
$	$	3	，3	3	3	3	5	3	3	3
*SS8&33@33@4
平耳4444444444
SSSSS33S323S
/。q /7A <
/ O q /7aw
/。q /⅛,A C
/。q /7At
/。q /7∙ C
/。q /7a∙c
/。q /7∙<
/。q /7A C
/。q /7qc
/。“ /7AC
/ O q /7AC
/ 白 q / £ %√
?o to
7oto
7。to
7。fo
70 0
7。S
700
70
7ofo
7。S
7。S
70
L b U Cfi Lp UJ (j> Lp 0> (j> (j> o>
T 7
J≠÷孑 FJ
(b) Most improved.
777777777
oo0。Ooooo
JlJJJJJJJ
555555555
999999999
777777777
¾¾¾¾¾‰%%⅞
q
o
I
5
q
7
¾
4
q
o
I
5
9
7
¾
q
7OJ597⅛4
(a) Randomly selected.
Figure 7: VAE reconstructions of binarized MNIST testing images by MIVI (z ∈ R10).
19