Under review as a conference paper at ICLR 2021
Apollo: An Adaptive Parameter-wise Diag-
onal Quasi-Newton Method for Nonconvex
Stochastic Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we introduce Apollo, a quasi-Newton method for nonconvex
stochastic optimization, which dynamically incorporates the curvature of the loss
function by approximating the Hessian via a diagonal matrix. Importantly, the up-
date and storage of the diagonal approximation of Hessian is as efficient as adaptive
first-order optimization methods with linear complexity for both time and memory.
To handle nonconvexity, we replace the Hessian with its rectified absolute value,
which is guaranteed to be positive-definite. Experiments on three tasks of vision
and language show that Apollo achieves significant improvements over other
stochastic optimization methods, including SGD and variants of Adam, in terms of
both convergence speed and generalization performance. The implementation of
the algorithm is available at anonymous link.
1	Introduction
Nonconvex stochastic optimization is of core practical importance in many fields of machine learning,
in particular for training deep neural networks (DNNs). First-order gradient-based optimization
algorithms, conceptually attractive due to their linear efficiency on both the time and memory
complexity, have led to tremendous progress and impressive successes. A number of advanced
first-order algorithms have emerged over the years to pursue fast and stable convergence, among
which stochastic gradient descent (SGD) (Robbins & Monro, 1951; LeCun et al., 1998), equipped
with momentum (Rumelhart et al., 1985; Qian, 1999; Bottou & Bousquet, 2008), has stood out
for its simplicity and effectiveness across a wide range of applications (Hinton & Salakhutdinov,
2006; Hinton et al., 2012; Graves, 2013). However, one disadvantage of SGD is that the gradients
in different directions are scaled uniformly, resulting in limited convergence speed and sensitive
choice of the learning rate, and thus has spawned a lot of recent interest in accelerating SGD from the
algorithmic and practical perspectives.
Recently, many adaptive first-order optimization methods have been proposed to achieve rapid
training progress with element-wise scaled learning rates, and we can only mention a few here due
to space limits. In their pioneering work, Duchi et al. (2011) proposed AdaGrad, which scales the
gradient by the square root of the accumulative square gradients from the first iteration. While
AdaGrad works well for sparse settings, its performance significantly degrades for dense settings,
primarily due to the monotonic increase of the accumulation. Subsequently, several methods have
been proposed with the intuition to limit the accumulation to a small window of past iterations, and
in particular exponentially reduce the weight of earlier iterations. Notable works incorporating this
method are RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma &
Ba, 2015), among which Adam has become the default optimization algorithm across many deep
learning applications because of its fast convergence speed and relatively consistent selections of
hyper-parameters (Ruder, 2016; Zhang et al., 2020). However, it has been observed that these adaptive
optimization methods may converge to bad/suspicious local optima, resulting in worse generalization
ability than their non-adaptive counterparts (Wilson et al., 2017), or fail to converge due to unstable
and extreme learning rates (Luo et al., 2019).
Quasi-Newton methods have been widely used in solving convex optimization problems, due to their
efficient computation and fast convergence rate (Broyden, 1967; Dennis & Mora 1977). However,
the stochastic, high-dimensional and nonconvex nature of many machine learning tasks, such as
1
Under review as a conference paper at ICLR 2021
training deep neural networks, has rendered many classical quasi-Newton methods ineffective and/or
inefficient (Keskar & Berahas, 2016; Wang et al., 2017; Yao et al., 2020). Indeed, in many natural
language processing (NLP) and computer vision (CV) tasks (He et al., 2016; Ma & Hovy, 2016;
Luo et al., 2019), SGD (with momentum) is chosen as the optimizer, benefiting from its stable and
efficient training and outstanding generalization.
In this work, we develop Apollo, a quasi-Newton method for nonconvex stochastic optimization
to simultaneously tackle the aforementioned challenges of stochastic variance, nonconvexity and
inefficiency. Algorithmically, Apollo dynamically incorporates the curvature of the objective
function with diagonally approximated Hessian. It only requires first-order gradients and updates
the approximation of the Hessian diagonally so that it satisfies a parameter-wise version of the
weak secant condition (Wolfe, 1959). To handle nonconvexity, we replace the Hessian with its
rectified absolute value, the computation of which is also efficient under our diagonal approximation,
yielding an efficient optimization algorithm with linear complexity for both time and memory (§3).
Experimentally, through three tasks on CV and NLP with popular deep neural networks, including
ResNets (He et al., 2016), LSTMs (Hochreiter & Schmidhuber, 1997) and Transformers (Vaswani
et al., 2017), we demonstrate that Apollo significantly outperforms SGD and variants of Adam, in
terms of both convergence speed and generalization performance (§4).
2	Backgrounds
In this section, we set up the notations on nonconvex stochastic optimization, briefly review the
(quasi-) Newton methods, and discuss the problems of applying quasi-Newton methods to nonconvex
stochastic optimization that we attempt to study in the rest of the paper.
2.1	Nonconvex Stochastic Optimization
In this paper, we consider the following stochastic optimization problem:
min f(θ) = E[l(θ; Γ)]
(1)
where l : Rd × Rn → R is a continuously differentiable (and possible nonconvex) function, θ ∈ Rd
denotes the parameter to be optimized, Γ ∈ Rn denotes a random variable with distribution function
P, and E[∙] denotes the expectation w.r.t Γ. Intuitively, Γ incorporates noises in f, leading to a
stochastic objective function. A special case of (1) that arises frequently in machine learning is the
empirical risk minimization problem:
min f (θ) = ɪ li(θ)
θ∈Rd 八'Ng -
(2)
where li : Rd → R is the loss function corresponds to the i-th data, and N is the number of data
samples that is assumed to be extremely large. Objective functions may also have other sources of
noise than data subsampling, such as dropout (Srivastava et al., 2014) in deep neural networks.
Decoupled Parameters. In this work, we consider a setting of decoupled parameters: θ =
{θ(l), l = 1, . . . , L}. Intuitively, under this setting the parameter θ is decoupled into a sequence of
parameters serving different functionalities. For example, in neural network training the parameters
of a neural network can be naturally decoupled into the parameters of different layers or modules.
2.2	Newton and quasi-Newton Methods
Newton’s method usually employs the following updates to solve (1):
θt+1 = θt - Ht-1gt	(3)
where gt = Vf (θt) is the gradient at θt and Ht = V2f (θt) is the Hessian matrix. The convergence
rate of Newton’s method is quadratic under standard assumptions (Nocedal & Wright, 2006). How-
ever, major challenges with this method are i) the expensive computation of the inverse Hessian at
every iteration and the corresponding quadratic memory complexity; and ii) the limitation to convex
functions (nonconvexity results in negative curvature of Ht and misleads the update directions).
2
Under review as a conference paper at ICLR 2021
A standard alternative to Newton’s method is a class of quasi-Newton methods, which have been
widely used in solving convex deterministic optimization:
θt+1 = θt - ηtBt-1gt
(4)
where η is the stepsize (a.k.a learning rate), Bt is an approximation to the Hessian matrix V2f (θt)
at θt , which is updated based on the well-known secant equation:
Bt+1 = argmin kB - Btk
B
s.t. Bt+1st = yt (secant equation)
(5)
where st = θt+1 -θt and yt = gt+1 -gt. Bt+1 is, in the sense of some matrix norm, the closest to Bt
among all symmetric matrices that satisfy the secant equation. Each choice of the matrix norm results
in a different update formula, such as DFP (Davidon, 1991; Fletcher, 1987) and BFGS (Broyden,
1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970). The popularity of this method is due to the
fact that only the gradient of the objective function is required at each iteration. Since no second
derivatives (Hessian) are required, quasi-Newton method is sometimes more efficient than Newton’s
method, especially when the computation of Hessian is expensive. To further reduce memory cost,
one seminal work is the limited memory BFGS (L-BFGS) (Liu & Nocedal, 1989; Byrd et al., 1995)
that achieves desirable linear computational and memory complexity by approximating the Hessian
as a series of sum of first order information from previous iterations.
2.3 Problems of quasi-Newton Methods
Despite their impressive successes on convex deterministic optimization, quasi-Newton methods
suffer from their own problems in the scenario of large-scale nonconvex stochastic optimization.
Stochastic Variance. One main challenge that prevents quasi-Newton methods from being applied
to nonconvex stochastic optimization (1) is the variance introduced by the stochastic nature of the
problem. At each iteration, only the stochastic gradient gt is available, which is an unbiased estimation
of the gradient Vf(θt) and may lead to an erroneous approximation of Hessian (Byrd et al., 2011).
Nonconvexity. Another key challenge in designing stochastic quasi-Newton methods for noncon-
vex problem lies in the difficulty of preserving the positive-definiteness of Bt in (5), due to the
nonconvexity of the objective function. What is worse is that performing line search is infeasible in
the stochastic setting, due to the presence of noise in the stochastic gradients (Wang et al., 2017).
Computational and Memory Efficiency. Even though quasi-Newton methods are more efficient
than Newton’s method, the time and memory complexities are still relatively large compared with
adaptive first-order methods. For instance, L-BFGS requires to store first-order information from m
previous iterations with commonly m ≥ 5, which is still too expensive for deep neural networks con-
taining millions of parameters. Moreover, adapting quasi-Newton methods to nonconvex stochastic
optimization probably introduces additional computation, further slowing down these methods.
Due to the aforementioned problems, no quasi-Newton methods (to our best knowledge) designed for
nonconvex optimization consistently outperform adaptive first-order algorithms w.r.t convergence
speed and generalization performance. The main goal of this work is to algorithmically design and
experimentally demonstrate a novel quasi-Newton method, in hope of improving the convergence
speed and generalization performance of nonconvex stochastic optimization eventually.
3 Adaptive Parameter-Wise Diagonal Quasi-Newton
With the end goal of designing an efficient quasi-Newton method to solve the problem in (1) in mind,
we first propose to approximate the Hessian with a diagonal matrix, whose elements are determined
by the variational approach subject to the parameter-wise weak secant equation (§3.1). Then, we
explain our stepsize bias correction technique to reduce the stochastic variance in §3.2. To handle
nonconvexity, we directly use the rectified absolute value of the diagonally approximated Hessian
as the preconditioning of the gradient (§3.3). The initialization technique of Apollo allows us
to eliminate one hyper-parameter (§3.4). At last, we provide a theoretical analysis of Apollo’s
convergence in online convex programming (§3.5). The pseudo-code is shown in Algorithm 1.
3
Under review as a conference paper at ICLR 2021
3.1 Quasi-Newton Methods with Diagonal Hessian Approximation
As discussed in Bordes et al. (2009), designing an efficient stochastic quasi-Newton algorithm
involves a careful trade-off between the sparsity of the approximation matrix Bt and the quality of its
approximation of the Hessian Ht, and diagonal approximation is a reasonable choice (Becker et al.,
1988). If B is chosen to be a diagonal matrix to satisfy the secant equation in (5), one can obtain an
updating formula similar to the SGD-QN algorithm (Bordes et al., 2009).
An alternative of the secant equation in the updating formula (5), as first introduced by Nazareth
(1995), is the weak secant equation (Dennis & Wolkowicz, 1993):
Bt+1 = argmin kB - Btk
B
s.t. stTBt+1st = stT yt (weak secant equation)
(6)
The motivation of using the weak secant condition in diagonal quasi-Newton method is straight-
forward: the standard mean-value theorem might not necessarily hold for vector-valued functions
expressed in the secant equation, Bt+ιst = yt ≈ V2f (θt)st. Thus, We do not know whether there
exists a vector θ ∈ Rd such that yt = V2f (θ)st (Dennis & Mora 1977). On the other hand, the
Taylor theorem ensures that there exists such θ that stT yt = stT V2f(θ)st, leading to the reasonable
assumption of the weak secant condition (6).
Based on the variational technique (Zhu et al., 1999), the solution of (6) with Frobenius norm is:
Λ，Bt+ι - Bt = STyt— STBtst Diag(S2)	⑺
kstk4
where St2 is the element-wise square vector of St, Diag(St2) is the diagonal matrix with diagonal
elements from vector s2, and k」4 is the 4-norm of a vector.
Parameter-Wise Weak Secant Condition. However, in optimization problems with high-
dimensional parameter space, such as training deep neural networks with millions of parameters, the
weak secant condition might be too flexible to produce a good Hessian approximation. In the setting
of decoupled parameters (§2.1), we propose a parameter-wise version of the weak secant equation
to achieve a trade-off between the secant and weak secant conditions: for each parameter θ(l) ∈ θ,
we update B corresponding to θ(l) by solving (6) individually. Remarkably, the secant condition
restricts B with an equation of a d-dimensional vector, while the weak secant condition relaxes it
with a 1-dimensional scalar. The parameter-wise weak secant condition expresses the restriction as a
l-dimension vector (1 < l < d), resulting in a reasonable trade-off. The updating formula is the same
as (7) for each parameter-wise B .
3.2	Stepsize Bias Correction
To mitigate the stochastic variance problem in stochastic quasi-Newton methods, Apollo utilizes
stepsize bias correction on the stochastic gradients at each step t. Concretely, comparing the parameter
updating formulas of Newton’s method (3) and quasi-Newton methods (4), we observe that the optimal
stepsize ηt equals to 1 if the Hessian approximation Bt and the stochastic gradient gt are close to the
exact Hessian Ht and gradient Vf (θt), respectively. Inspired by this, we correct the stepsize bias
in the stochastic gradient gt by replacing it with a corrected gradient gt0 = ηtgt . Together with the
corresponding corrected yt0 = gt0+1 - gt0 = ηtyt, we correct the updating term Λ of Bt in (7) by:
Λ0= sTy0- STBtst Diag(S2)
kStk44	t	(8)
=- ddyt + dTBtdt Diag(d2)
kdt k4
where dt = -St∕ηt = B-Igt is the update direction. Based on previous studies, incorporating
exponential moving averages (EMVs) for the stochastic gradients significantly reduces the vari-
ance (Kingma & Ba, 2015; Yao et al., 2020). We follow these work and apply exponential moving
average to gt, together with the initialization bias correction:
β(I- βt)	1 - β	zɑʌ
mt+1 = 1 - βt+1 mt + 1- βt+1 gt+1	⑼
4
Under review as a conference paper at ICLR 2021
Algorithm 1: APOLLO, our proposed algorithm for nonconvex stochastic optimization. All
operations on vector are element-wise. Good default settings are β = 0.9 and = 1e-4.
Initial: m0, d°, B0 J 0,0,0
while t ∈ {0, . . . , T } do
for θ ∈ {θ1, . . . , θL} do
gt+ι J Vft(θt)
β(1-βt)	1-β
mt+1 j---ι-βt+ι mt + ι-β⅛+ι gt+1
C / dT(mt+ι-mt) + dTBtdt
α V	(kdtk4+e)4
Bt+1 J Bt- α ∙ Diag(d2)
Dt+1 J rectify(Bt+1, 1)
dt+1 J Dt-+11 mt+1
θt+1 J θt - ηt+1 dt+1
end
end
return θT
// Initialize m0, d0, B0 to zero
// Calculate gradient at step t
// Update bias-corrected moving
// Calculate coefficient of B update
// Update diagonal Hessian
// Handle nonconvexity
// Calculate update direction
// Update parameters
where 0 < β < 1 is the decay rate of EMV and yt in (8) is written as mt+1 - mt . Note that we do
not apply moving average methods to the approximated Hessian, though the diagonal matrix is easier
to be explicitly formed to average than full matrices. Further improving Apollo by investigating the
moving average of the diagonal matrix Bt might be a interesting direction of future work.
3.3	Rectified Absolute Value of Hessian for Nonconvexity
To guarantee convergence, quasi-Newton methods require the approximated Hessian matrix Bt to
be positive definite at each step. The common strategy in previous studies is to solve the updating
formula in (5) by restricting the candidate matrix B to be symmetric positive definite. It is known
that the BFGS update preserves the positive-definiteness of Bt+1 as long as the curvature condition
stT yt > 0 holds, which can be guaranteed for strongly convex problem. For nonconvex problem, the
curvature condition can be satisfied by performing a line search, which is, however, expensive or
even infeasible in stochastic setting, because the exact function values and gradient information are
unavailable. Wang et al. (2017) proposed the stochastic damped L-BFGS (SdLBFGS) method that
implicitly generates a positive definite matrix without line search. However, it usually requires large
history size (m ≥ 100) to guarantee convergence, which is infeasible for large-scale optimization.
To handle nonconvexity, we adopt a different strategy that does not require the solution of Bt in (5)
to be positive definite. Intuitively, we search for Bt that is a good approximation of the real Hessian,
which is not necessarily positive definite in nonconvex problem. When we use Bt as preconditioning
to calculate the update direction, we use its absolute value:
|Bt| = qBTBt	(10)
where √ is the positive definite square root of a matrix. The motivation of absolute value is Straight-
forward: for dimensions with large absolute values of curvature, the objective function could be
very sharp and we would prefer to take relatively smaller steps than those flatter dimensions. Since
APOLLO formulate Bt as a diagonal matrix, the cost of computing |Bt | is marginal.
Rectified Absolute Value of Bt For nonconvex objective functions, there exist inflection points
whose curvatures are zero. To prevent the steps from becoming arbitrarily large, we rectify the
absolute value of Bt with a convexity hyper-parameter σ:
Dt = rectify(Bt, σ) = max(|Bt|, σ)	(11)
where the rectify(∙, σ) function is similar to the rectified linear unit (ReLU) (Nair & Hinton, 2010)
with threshold set to σ. The update direction in (8) is then dt = Dt-1mt.
AdaHessian (Yao et al., 2020) used an idea similar to the absolute values of Bt to handle nonconvexity,
where the root mean square averaging is applied to compute the Hessian diagonal. Different from
Apollo, AdaHessian requires second-order information to compute the Hessian matvec oracle and
approximate the Hessian diagonal using Hutchinson’s method, which is significantly more costly.
5
Under review as a conference paper at ICLR 2021
3.4	Initialization
The rectified Dt in (11) introduces one more hyper-parameter σ, limiting the application of APOLLO
in practice. In this section, we show that the zero initialization approach in Apollo, which initializes
the moving average of gradient m0 , the parameter update direction d0 and the diagonal approximation
of Hessian B0 as (vector of) zeros, leads to coupled stepsize η and convexity σ, allowing us to
eliminate one hyper-parameter of η or σ.
Coupled Stepsize η and Convexity σ.	With the zero initialization of m0 , d0 , and B0, we have
the following theorem to illustrate the relation between η and σ (details in Appendix A):
Theorem 1. Given zero initialization of m0 , d0 , and B0 and a fixed parameter intialization θ0.
Suppose that we have two sets ofhyper-parameters η, σ and η0, σ0 with the same ratio: σ = σ. Then
the convergence trajectories of these two sets of hyper-parameters are exactly the same:
θt = θt0, ∀t ∈ {1, . . . , T}.	(12)
where θt and θt0 are the parameters of (η, σ) and (η0, σ0) at iteration t, respectively.
From Theorem 1, we observe that η and σ are coupled with each other and in practice we only need
to tune one of them, leaving the other fixed. Therefore, in our experiments (§4), we fix σ = 1 and
tune η on different problems.
Learning Rate Warmup for Apollo As discussed in Kingma & Ba (2015), zero initialization
leads to estimations biased towards zero in the initial iterations. For the moving average mt , this
bias can be corrected by dividing the bias-correction term (9). For dt and Bt, however, we cannot
derive such bias correction terms. Fortunately, a simple linear warmup heuristic of η at the beginning
iterations achieves remarkablely stable training in our experiments.
3.5	Convergence Analysis
We analyze the convergence of Apollo using the online learning framework (Zinkevich, 2003) for a
sequence of convex cost functions f1 (θ), f2(θ), . . . , fTθ.
Theorem 2. Let {θt} be the sequence obtained from Algorithm 1, η = √, βt ≤ β ≤ 1, ∀t ∈ [T ].
Suppose kgtk2 ≤ G, kθt - θt0 k2 ≤ D, ∀t, t0 ∈ [T]. For θt generated with the APOLLO algorithm,
we have the following bound on the regret:
RT ≤ D2	XXβt2	+ 3ηG2 (2√T -	1)	(13)
—	2(1- β)	t=1 ηt 2(1 - β>	J	' '
The following result falls as an immediate corollary of the above result.
Corollary 2.1. Suppose βt = βλt-1,0'λ'1 in Theorem 2, we have
RT ≤ 2η(1 -Dβ)(1 - λ2)2 + ⅛⅛(2√t -1)	(14)
For the convex case, Theorem 2 implies the regret of APOLLO is upper bounded by O(√T) (proof in
Appendix B). The conditions for Corollary 2.1, as in Reddi et al. (2018), can be relaxed to βt = β∕t
and still ensures a regret of O(√T).
4	Experiments
To evaluate Apollo, we conduct experiments on four benchmark datasets across three tasks of CV
and NLP that are commonly used to evaluate optimization algorithms: CIFAR-10 (Krizhevsky &
Hinton, 2009) and ImageNet (Deng et al., 2009) for image classification; One Billion Words (Chelba
et al., 2013) for language modeling; and WMT 2014 English-German for neural machine translation.
The four baseline methods we compare with are SGD with momentum (Bottou & Bousquet, 2008),
Adam (Kingma & Ba, 2015), Rectified Adam (RAdam) (Liu et al., 2020), and AdaHessian (Yao et al.,
2020). Following Loshchilov & Hutter (2019), we decouple weight decays in Adam, RAdam and
AdaHessian in all the experiments. For each experiment, we report the average over 5 runs. More
detailed descriptions, results and analysis of the conducted experiments are provided in Appendix C.
6
Under review as a conference paper at ICLR 2021
Figure 1: Training loss and test accuracy of ResNet-110 on CIFAR-10 and ResNeXt-50 on ImageNet,
with two schedule strategies of learning rate decay.
Figure 2: SGD, Adam, RAdam and Apollo with different learning rates on CIFAR-10.
4.1	Image Classification
We begin our experiments with an evaluation
of the convergence and generalization perfor-
mance on image classification. We use ResNet-
1101 for CIFAR-10 and ResNext-50 (Xie et al.,
2017) for ImageNet, respectively (details in Ap-
pendix C.1). The results on CIFAR-10 and Im-
ageNet are presented in Figure 1 and Table 1,
together with the four baselines. For each opti-
mizer, we use two scheduling strategies of learn-
ing rate decay: i) milestone that decays the learn-
ing rate at the end of some predefined epochs;
and ii) cosine annealing schedule proposed in
Loshchilov & Hutter (2017).
Table 1: Test Acc. on CIFAR-10 and ImageNet.
Method	CIFAR-10		ImageNet	
	milestone	cosine	milestone	cosine
SGD	I^^93.91 ^^	94.53	I^^77.19^^	78.17
Adam	91.41 ^^	91.56	71.72^^	71.19
RAdam	91.80	91.88	72.37	71.64
Adam-adj	93.74^^	94.24	76.86^^	77.54
RAdam-adj	93.88	94.38	76.91	77.68
AdaHessian	I^^93.97	94.48	I -	一
Apollo	I^^94.20^^	94.60	I^^77.90	78.54
Though Loshchilov & Hutter (2019) claimed that the optimal settings of the learning rate and weight
decay factor in Adam with decoupled weight decay is more independent than the original Adam,
we observed that the strength of weight decay regularization is still co-related with the learning rate.
Since the strength of regularization has significant effect on both the performance of convergence
and generalization, we adjust the weight decay rates for Adam and RAdam, named Adam-adj and
RAdam-adj , so that the they have similar strength of regularization for fair comparison. More
detailed analysis on the effect of weight decay is provided in Appendix D.2.
1ResNet-110 is a modified (small) version of ResNet-18 to adapt the image size 32 × 32 in CIFAR-10.
7
Under review as a conference paper at ICLR 2021
From Figure 1 and Table 1, we first see that Adam and RAdam, without adjusting the weight decay
rates, converge much faster than other optimization methods, while obtaining significantly worse
classification accuracy. After adjusting the weight decay rates, the test accuracy of Adam-adj and
RAdam-adj remarkably improves, with rapid decline of convergence speed. This suggests that the
fast convergence speed of Adam and RAdam results from relatively weak regularization. Thus, the
effect of regularization strength needs to be considered when we analyze the performance of different
optimization methods. Second, Apollo outperforms the three first-order methods (SGD, Adam and
RAdam) on both the convergence speed and classification accuracy, demonstrating its effectiveness
on training the ResNet architectures based on convolutional neural networks (CNNs) (LeCun et al.,
1989). Comparing with AdaHessian, Apollo obtains better test accuracy with similar convergence
speed. Note that AdaHessian requires second-order information and is significantly more costly
(detailed comparison in Appendix D.3), and we omit it from the following experiments.
Robustness to Learning Rate Change. Besides performance improvements, we also investigate
the robustness of different optimization methods to the change of learning rate. For each optimizer,
we use the learning rate in the previous experiment (Table 1) as the base, i.e. 0.1 for SGD, 0.001 for
Adam and RAdam, and 0.5 for APOLLO. Then, we explore different learning rates that are α times
of the base learning rate, with α ∈ {0.2, 1.0, 2.0, 10.0}. To eliminate the impact of weight decay,
we adjust the weight decay rates according to the factor α. Experimental results with ResNet-110
on CIFAR-10 are summarized in Figure 2. After correcting the impact of weight decay, all the
optimization methods, except SGD with α = 10.0, achieves consistent model performance (highly
overlapped training and test curves with each other), while Apollo slightly improves the robustness
of model training over the three baseline methods.
4.2	Language Modeling
To evaluate Apollo on Recurrent Neural Networks (RNNs) that are applied in a wide range of NLP
tasks (Graves, 2013), we conduct experiments on the One Billion Words dataset, using a two-layer
LSTM network for language modeling (details in Appendix C.2).
Figure 3 and Table 2 illustrate the perplexity (PPL) of training and test for each optimization method.
As shown in Figure 3, although Apollo is slower than Adam and RAdam in the first few updates, its
convergence is much faster after that. On generalization performance, Apollo achieves significant
improvements (more than 4.0 PPL points on test data) over Adam and RAdam. This demonstrates
the effectiveness of Apollo on training LSTM-based neural architectures.
Training Stability. From the middle plot Figure 3 we see that the training losses of Adam and
RAdam may suddenly increase. This occurred in all the runs of experiments using Adam and RAdam,
and we selected these successfully converged — the loss went back to normal after some updates, and
discarded those failed to converged — the model crashed due to loss numerical overflow. The models
optimized with Apollo never suffered from this issue, demonstrating the convergence stability of it.
4.3	Neural Machine Translation
To evaluate Apollo on Attention-based Transformer architec-
ture (Vaswani et al., 2017), we train the Transformer-base model
on the WMT2014 English-German (EN-DE) dataset (around 4.5M
sentence pairs). We use the same data preprocessing steps as in
Ma et al. (2019) (details in Appendix C.3). From Table 3, Apollo
obtains improvements over all the baseline methods. For each
experiment, we report the mean and standard variance over 5 runs.
Table 3: BLEU on WMT-14.
Method	I BLEU
SGD	26.59±0.07
Adam	27.84±0.12
RAdam	28.15±0.15
Apollo	I 28.34±0.10
8
Under review as a conference paper at ICLR 2021
5	Related Work
Stochastic Quasi-Newton Methods. In the literature of (nonconvex) stochastic quasi-Newton
methods, several algorithms have been developed recently for large-scale machine learning problems:
oLBFGS (Schraudolph et al., 2007; Mokhtari & Ribeiro, 2015), RES (Mokhtari & Ribeiro, 2014),
SFO (Sohl-Dickstein et al., 2014), SQN (Byrd et al., 2016), SdLBFGS (Wang et al., 2017), and
AdaQN (Keskar & Berahas, 2016), among which only SdLBFGS and AdaQN are designed to
solve nonconvex optimization problems. The SdLBFGS algorithm carefully controls the quality
of modified BFGS updates to preserve the positive-definiteness of Bt in (5) without line search.
AdaQN shares a similar idea but is specifically designed for RNNs by refining the initial L-BFGS
scaling, step acceptance control and choice of curvature information matrix, and adopting the SQN
framework (Byrd et al., 2016). Different from these two methods, APOLLO does not require Bt in
(5) to be positive definite, but replacing it with its rectified absolute value to handle nonconvexity.
Moreover, both SdLBFGS and AdaQN use the updating formula similar to L-BFGS and require even
larger history size (commonly ≥ 100) to guarantee convergence, preventing them from being applied
to large-scale optimization. For comprehensive comparison, we conducted experiments with small
toy CNN models to compare SdLBFGS with Apollo (details in Appendix E).
Adaptive First-Order Methods. From the diagonal approximation of Hessian, APOLLO is also
related to those diagonally-scaled first-order algorithms, such as AdaGrad (Duchi et al., 2011),
RMSProp (Tieleman & Hinton, 2012), AdaDelta (Zeiler, 2012), and Adam (Kingma & Ba, 2015).
Subsequently, a number of techniques have emerged to theoretically justify and algorithmically im-
prove Adam, including AMSGrad (Reddi et al., 2018), AdaBound (Luo et al., 2019) and RAdam (Liu
et al., 2020). The main difference is that the diagonal preconditioning in Apollo is directly derived
from the quasi-Newton updating formula (6). In terms of memory efficiency, Anil et al. (2019) and
Chen et al. (2020) further reduces the memory cost adaptive methods, and Agarwal et al. (2019)
proposed an efficient method for full-matrix adaptive regularization.
Stochastic Second-Order Hessian-Free Methods. Stochastic Second-Order Hessian-Free meth-
ods (Martens, 2010; Martens & Sutskever, 2011) implicitly solve quadratic models using matrix-vector
products. Dauphin et al. (2014) argued the existence of saddle points and proposed a method to
rapidly escape them. K-FAC (Martens & Grosse, 2015) computes a second-order step by constructing
an invertible approximation of the Fisher information matrix in an online fashion. Shampoo (Gupta
et al., 2018) approximates the Fisher information matrix using low-rank decomposition. Recently,
Yao et al. (2020) proposed AdaHessian, which approximates the Hessian diagonal using Hutchinson’s
method. These second-order methods differ from Apollo mainly in the request of second-order
information of the objective function at each iteration.
6	Conclusion and Extensions
We have introduced Apollo, a simple and computationally efficient quasi-Newton algorithm for
nonconvex stochastic optimization. This method is aimed towards large-scale optimization problems
in the sense of large datasets and/or high-dimensional parameter spaces.
We briefly outline a few extensions to Apollo that we want to explore in future work.
Parameter-Wise Gradient Clipping. The standard gradient clipping method (Pascanu et al., 2013)
is to clip the gradients based on the norm computed over gradients of all the parameters together.
A modification of gradient clipping to properly apply it to Apollo is to clip the gradient of each
parameter individually based on its own norm. Preliminary results are provided in Appendix D.4.
Decoupled Weight Decay in Apollo. (Loshchilov & Hutter, 2019) demonstrated that L2 regu-
larization is not identical to weight decay for adaptive gradient methods and proposed Adam with
decoupled weight decay (AdamW). The application of decoupled weight decay to Apollo is slightly
different from AdamW as APOLLO memorizes the update direction of the last iteration dt to update
the diagonal Hessian. The algorithm of Apollo with decoupled weight decay is in Appendix F.
Apollo is Scale-Variant. An important advantage of adaptive optimization methods, including
Adam and its variants, is that they are inherently scale-invariant — invariant with the scale of the
objective function. The property of scale-invariance yields more consistent hyper-parameters of these
adaptive methods than SGD across different machine learning tasks. Unfortunately, Apollo does
not hold the property of scale-invariance. Developing a scale-invariant version of Apollo might be
a potential direction for future work.
9
Under review as a conference paper at ICLR 2021
References
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang, and Yi Zhang.
Efficient full-matrix adaptive regularization. In International Conference on Machine Learning,
pp.102-110, 2019.
Rohan Anil, Vineet Gupta, Tomer Koren, and Yoram Singer. Memory efficient adaptive optimization.
In Advances in Neural Information Processing Systems, pp. 9749-9758, 2019.
Sue Becker, Yann Le Cun, et al. Improving the convergence of back-propagation learning with second
order methods. In Proceedings of the 1988 connectionist models summer school, pp. 29-37, 1988.
Antoine Bordes, Leon Bottou, and Patrick Gallinari. SGD-QN: Careful quasi-newton stochastic
gradient descent. The Journal of Machine Learning Research, 10:1737-1754, 2009.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural
information processing systems, pp. 161-168, 2008.
Charles G Broyden. Quasi-newton methods and their application to function minimisation. Mathe-
matics of Computation, 21(99):368-381, 1967.
Charles George Broyden. The convergence of a class of double-rank minimization algorithms. IMA
Journal of Applied Mathematics, 6(1):76-90, 1970.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for
bound constrained optimization. SIAM Journal on scientific computing, 16(5):1190-1208, 1995.
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of stochastic hessian
information in optimization methods for machine learning. SIAM Journal on Optimization, 21(3):
977-995, 2011.
Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008-1031, 2016.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.
Xinyi Chen, Naman Agarwal, Elad Hazan, Cyril Zhang, and Yi Zhang. Extreme tensoring for
low-memory preconditioning. In International Conference on Learning Representations, 2020.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua
Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In Advances in neural information processing systems, pp. 2933-2941, 2014.
William C Davidon. Variable metric method for minimization. SIAM Journal on Optimization, 1(1):
1-17, 1991.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
John E Dennis, Jr and Jorge J More. Quasi-newton methods, motivation and theory. SIAM review, 19
(1):46-89, 1977.
John E Dennis, Jr and Henry Wolkowicz. Sizing and least-change secant methods. SIAM Journal on
Numerical Analysis, 30(5):1291-1314, 1993.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Roger Fletcher. A new approach to variable metric algorithms. The computer journal, 13(3), 1970.
Roger Fletcher. Practical methods of optimization. John Wiley & Sons, 1987.
10
Under review as a conference paper at ICLR 2021
D Goldfarb. A family of variable metric updates derived by variational means. Mathematics of
Computation, 24(109):23-26,1970.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint:1308.0850, 2013.
Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimiza-
tion. arXiv preprint arXiv:1802.09568, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. NeUral computation, 9(8):
1735-1780, 1997.
Nitish Shirish Keskar and Albert S Berahas. AdaQN: An adaPtive quasi-newton algorithm for training
rnns. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
PP. 1-16. SPringer, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic oPtimization. In International
Conference on Learning Representations, 2015.
Alex Krizhevsky and Geoffrey Hinton. Learning multiPle layers of features from tiny images.
Technical rePort, Citeseer, 2009.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. BackProPagation aPPlied to handwritten ziP code recognition.
Neural computation, 1(4):541-551, 1989.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yingkai Li and Huidong Liu. Implementation of stochastic quasi-newton’s method in pytorch. arXiv
preprint arXiv:1805.02338, 2018.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In International Conference on
Learning Representations, 2020.
Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In
International Conference on Learning Representations, 2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. In International Conference on Learning Representations, 2019.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 1064-1074, Berlin, Germany, August 2016.
11
Under review as a conference paper at ICLR 2021
Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and Eduard Hovy. Flowseq: Non-
autoregressive conditional sequence generation with generative flow. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing, pp. 4282-4292, Hong
Kong, November 2019.
James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International
Conference on International Conference on Machine Learning, pp. 735-742, 2010.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. In International conference on machine learning, pp. 2408-2417, 2015.
James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization.
In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 1033-
1040. Citeseer, 2011.
Aryan Mokhtari and Alejandro Ribeiro. Res: Regularized stochastic bfgs algorithm. IEEE Transac-
tions on Signal Processing, 62(23):6089-6104, 2014.
Aryan Mokhtari and Alejandro Ribeiro. Global convergence of online limited memory bfgs. The
Journal of Machine Learning Research, 16(1):3151-3181, 2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In
ICML, 2010.
JL Nazareth. If quasi-newton then why not quasi-cauchy. SIAG/Opt Views-and-news, 6:11-14, 1995.
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media,
2006.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and
Michael Auli. FairSeq: A fast, extensible toolkit for sequence modeling. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics
(Demonstrations), pp. 48-53, 2019.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. In International conference on machine learning, pp. 1310-1318, 2013.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):
145-151, 1999.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
International Conference on Learning Representations, 2018.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747, 2016.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by
error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,
1985.
Nicol N Schraudolph, Jin Yu, and Simon Gunter. A stochastic quasi-newton method for online convex
optimization. In Artificial intelligence and statistics, pp. 436-443, 2007.
David F Shanno. Conditioning of quasi-newton methods for function minimization. Mathematics of
computation, 24(111):647-656, 1970.
Jascha Sohl-Dickstein, Ben Poole, and Surya Ganguli. Fast large-scale optimization by unifying
stochastic gradient and quasi-newton methods. In International Conference on Machine Learning,
pp. 604-612, 2014.
12
Under review as a conference paper at ICLR 2021
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818-2826, 2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2), 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Xiao Wang, Shiqian Ma, Donald Goldfarb, and Wei Liu. Stochastic quasi-newton methods for
nonconvex stochastic optimization. SIAM Journal on Optimization, 27(2):927-956, 2017.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in neural information
processing systems, pp. 4148-4158, 2017.
Philip Wolfe. The secant method for simultaneous nonlinear equations. Communications of the ACM,
2(12):12-13, 1959.
Saining Xie, Ross Girshick, Piotr Doll念 Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1492-1500, 2017.
Zhewei Yao, Amir Gholami, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. ADAHESSIAN:
An adaptive second order optimizer for machine learning. arXiv preprint arXiv:2006.00719, 2020.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint:1212.5701, 2012.
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv
Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural
Information Processing Systems, 33, 2020.
Mingfa Zhu, John Lawrence Nazareth, and Henry Wolkowicz. The quasi-cauchy relation and diagonal
updating. SIAM Journal on Optimization, 9(4):1192-1204, 1999.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In
Proceedings of the 20th international conference on machine learning (icml-03), pp. 928-936,
2003.
13
Under review as a conference paper at ICLR 2021
Appendix: Apollo: An Adaptive Parameter-wise Diagonal
Quasi-Newton Method for Nonconvex Stochastic Optimization
A Coupled Stepsize and Convexity
Before proving Theorem 1, we first define the notations.
Let α = n = σσ0 be the ratio of the two sets of learning rates. Let m[, d； and Bt be the corresponding
terms of parameter θt0 at step t for (η0, σ0).
Proof of Theorem 1
Proof. Induction on the step of updates t, we attempt to prove that at each step t:
mt = m0t,	Bt0 = αBt, and θt = θt0, ∀t	(15)
Initial step: when t = 1, since θ0 = θ00 , we have m1 = m01. With d0 = d00 = 0 and (8), we have
B1 = B10 = 0 and:
D1 = recify(B1, σ) = σ
D10 = recify(B10 , σ0) = σ0
Then, D10 = αD1 and
θ1 = θ0 — η0Dl-1m1 = θo — ηα(D-1 /α)mι = θo — ηD-1 mi = θι.
Thus, the statement (15) is true.
Induction on step t:	Suppose that the statement (15) is true for all the previous t steps. Now we
prove the case t + 1. From the inductive assumption and (9), we have,
B0 = αBt,	d； = Ldt and m；+i = mt+1.
t	t α	t+1
From (8),
B0+1 = Bt - 滥y't∣+od4TB0dt Diag(d02)
kdt k4
R	( 1 dt)Tyt+(1 dt)T(αBt)(αdt) TY //17、2、
=αBt--------------k(W---------------Diag(( adt))
=αBt - αdt yt + dt4Btdt Diag(d2)
kdt k44	t
= αBt+1.
Then,
Dtt+1 = recify(Btt+1, σt)
= recify(αBt+1, ασ)
= αrecify(Bt+1, σ)
= αDt+1
and we have θtt+1 = θt+1.
Finally, to sum up with the induction, we have proven Theorem 1.
□
14
Under review as a conference paper at ICLR 2021
B Convergence Analysis
Proof of Theorem 2
T
Proof. Let θ* = argmin P ft(θ), where F is the feasible set of θ. From the Algorithm 1, We have
θ∈F t=1
θt+ι - θ* = θt - θ* - ηtD-1 mt. Then, we have the following:
∣∣D∕(θt+ι - θ*)k2 = ∣∣D/(% - θ*) - ntD-1/2mtk2
=kD1/2(θt - θ*)k2 + kntD-1/2mtk2 - ηtmT(θt - θ*)
=kD1/2(θt - θ*)k2 + kηtD-1∕2mtk2 - ηt(βtmt-i + (1- βt)gt)T(θt - θ*)
Then, we have
gT(θt -θ*) = 2η(II-β) [kD∕(θt -θ*)k2 -kD1∕2(θt+ι - θ*)k2]
+ 2(⅛) kD-1/2mtk2 - i⅛mT-ι0-θ*)
≤ 2η(II- β) [kD∕(θt - θ*)k2 -kD1∕2(θt+ι - θ*)k2]
+ 2(⅛) kD-1/2mtk2 + 2(⅛) kmt-lk2 + 2ηt (β- βt) kθt - θ*k2
Using the standard approach of bounding the regret at each step with convexity of the functions
T
{ft}t=ι, we have the following bound of RT = P ft(θt) - ft(θ*):
t=1
TT
Xft(θt) - ft(θ*) ≤ XgT(θt-θ*)
T
≤ X 2η(I- β) [kD1∕2(θt - θ*)k2 -kD1∕2(θt+ι - θ*)k2]
T
+ X ；kD-"mtk2 + ；kmt-ik2
T	β2
+ X	I®-θ*k2
From the triangle inequality, we have
kD∕(θt - θ*)k2 - kD1/2(Θt+1 - θ*)k2 ≤ kD^(θt - θt+ι)k2 = n2kD-1/2mtk2
To sum up with (16) and (17), we have
T
RT ≤ X	η2kD-1/2mtk2
T
+ X 2(⅛) kD72mtk2 + ;kmt-ik2
+ X Al I®-θ*k2
T	T2
=X -η-kD—1/2mtk2 + , ηt	、kmt-ik2 + X —7βt--∣∣θt - θ*∣∣2
=1 -	βt11 t	tl 12	2(1-βt)"	t	1112	=	2ηt(1-βt)" t 112
(16)
(17)
(18)
15
Under review as a conference paper at ICLR 2021
Since the element of Dt is rectified by 1, i.e. Dt,i ≥ 1, and kmtk2 ≤ G, βt < β < 1, we have
T
ηt	-1/2	2	ηt
H 1 - βt ""t	mt"2 + 2(1 - βt)
T
Ilmt-ιk2 ≤ X Yn^ kmtk2 +
t=1 1 - βt
T
≤ 2(1 - βt) XXX ηt
_	3ηG2 XX ɪ
2(I- βt) t=ι √t
ηt	2
2(1 - βt)k t-1k2
3G2
(19)
≤ ∙Λ (2√T -ι)
The last inequality is due to the following upper bound:
X √ ≤ ZT √ = 2√T - 1
t=ι √t	Jt=ι Vi
In addition, as ∣∣θt 一 θ*∣2 ≤ D and βt < β < 1, we have
T	β 2
X	kθt-θ*k2
Finally, to sum up with (18), (19) and (20), we have
D2
≤ 2(1- β)
X β2
ηt
t=1 ηt
(20)
RT ≤
D2
2(1 - β)
T2
X βt +
t=1 ηt
3ηG2 (2√t - 1)
2(1 - β)"	)
□
Proof of Corollary 2.1
Proof. Since βt = βλt-1, by sum of arithmetico-geometric series we have
TT
Xλ2(t-1)√t ≤ Xλ2(t-1)t ≤
1
(1 - λ2)2
(21)
Plugging (21) into (20), we have
R ≤
---D2β2——+ 3ηG2 (2√T - 1)
2η(1 - β)(1 - λ2)2 + 2(1 - β)(	)
□
C Experimental Details
C.1 Image Classification
For CIFAR-10 dataset, we use the ResNet-110 architecture in the public implementation2. Note that
ResNet-110 is a modified version of ResNet-18 (He et al., 2016) to adapt the small image size 32 × 32
in CIFAR-10, and is much smaller than ResNet-18. The number of parameters for ResNet-110 and
ResNet-18 are 1.73 M and 11.69 M, respectively. For ImageNet, we used the neural architecture of
ResNeXt-50 (Xie et al., 2017). Batch size is 128 for CIFAR-10 and 256 for ImageNet. For each
optimizer, we used two learning rate decay strategies. First, we train the model on CIFAR-10 for
164 epochs and decay the learning rate at the end of 80-th and 120-th epochs by 0.1. For ImageNet,
we train the model for 120 epochs and decay the learning rate at at the end of 40-th and 80-th
2https://github.com/bearpaw/pytorch-classification
16
Under review as a conference paper at ICLR 2021
epochs by 0.1. Second, we also used the cosine annealing schedule (Loshchilov & Hutter, 2017).
For the cosine annealing schedule, we train a CIFAR-10 model for 200 epochs and a ImageNet
model for 120 epochs. The start learning rate is 0.1 for SGD, 0.001 for Adam and RAdam, 0.15
for AdaHessian and 0.5 for APOLLO. The weight decay rates for SGD and APOLLO are 5e-4 on
CIFAR-10 and 2e-4 on ImageNet. For AdaHessian, the weight decay rate is 1e-4 on CIFAR-10 and
2e-4 on ImageNet. For Adam and RAdam, we adjusted the weight decay rates based on the ratio
of the learning rate, i.e. multiplying 500 to the original decay rates3. For APOLLO learning rates
are warmed up linearly from 0.01 to 0.5 in the first 100 updates. For AdaHessian, we performed
grid search for hyper-parameters with η ∈ {0.1, 0.15, 0.2}, ∈ {1e-1 , 1e-2, 1e-4, 1e-6} and
weight decay ∈ {5e-4, 1e-3, 2e-3, 3e-3, 4e-3}, and selected the optimal hyper-parameters based
on the generalization accuracy (η = 0.15,	= 1e-2, weight decay = 1e-3). In addition, we
observed that warmup is also crucial for AdaHessian. Thus, we linearly warmed up the learning rates
from 0.001 to 0.1 in the first 100 updates, similar to APOLLO. The implementation of AdaHessian is
based on the public implementation4. Random cropping and random horizontal flipping are applied
to training data. For each experiment, we conduct training on one NVIDIA Tesla V100 GPU for
CIFAR-10 and eight NVIDIA Tesla V100 GPUs for ImageNet.
C.2 Language Modeling
One Billion Words dataset (Chelba et al., 2013) is a publicly available benchmark for measuring
progress of language modeling. It contains about 0.8 billion tokens with a vocabulary of 793,471
words, including sentence boundary markers. Different from Liu et al. (2020) which shrinks the
vocabulary to about 0.64 million words, we used the standard vocabulary5. For the language model,
we used two-layer LSTM with 2048 hidden states with adaptive softmax and 300-dimensional word
embeddings as input. The cut-offs of the adaptive softmax are set to [60000, 100000, 640000], which
is different from Liu et al. (2020). Dropout (Srivastava et al., 2014) is applied to each layer with
drop rate of 0.1. The start learning rate is set to 0.001 for Adam and RAdam, and 10.0 for APOLLO.
Gradient clips with 1.0 are applied to all the optimization methods. Each model is trained for 20
epochs, and the learning rate decays at the end of the 12-th and 18-th epochs by decay rate 0.1.
LSTMs are unrolled for 20 steps without resetting the LSTM states and the batch size is set to 128.
Every models is trained on one NVIDIA Titan RTX GPU.
C.3 Neural Machine Translation
Our experiments on WMT 2014 English-German are based on the Transformer-base model (Vaswani
et al., 2017), with implementation from the FairSeq package (Ott et al., 2019). This dataset contains
4.5M parallel sentence pairs for training. We following the standard setting (Vaswani et al., 2017),
using Newstest2013 as the validation set and Newstest2014 as the test set. The dataset is pre-processed
following (Ma et al., 2019), using the scripts from FairSeq package6. Specifically, we use word
embedding with 512 dimension and 6-layer encoder/decoder with 8 multi-head attention and 2048
feed-forward dimensions. We apply 0.1 label smoothing (Szegedy et al., 2016), and perform totally
500, 000 updates to train each model. For Adam and RAdam, we use start learning rate 0.0005. For
Adam we set β = (0.9, 0.98), while for RAdam we set β = (0.9, 0.999). For SGD and APOLLO,
the start learning rates is 0.1 and 10.0, respectively. The momentum of SGD is 0.9. For learning rate
scheduling, we applied linear warm up the learning rate for SGD, Adam APOLLO — 4000 updates
for Adam and 1000 updates for SGD and APOLLO. For RAdam, we did not apply warm up because
RAdam is inherently designed to avoid it. After learning rate warming up, we applied the inverse
square root decay (Vaswani et al., 2017) to Adam. For SGD, RAdam and Apollo, we decayed
the learning rate at the 250, 000 and 450, 000 updates by decay rate 0.1. Gradient clips with 1.0 are
applied to all the optimization methods, and the dropout ratio are set to 0.1. Weight decay rates are
1e-4 for Adam and RAdam, 1e-6 for SGD, and 1e-8 for APOLLO. The decoding beam size is set to
5, and the checkpoints of the last 10 epochs are averaged before evaluation. For each experiment, we
conducted distributed training across eight NVIDIA Tesla V100 GPUs with maximum batch size as
8192 tokens per GPU (totally 8192 × 8 tokens per batch).
3Weight decay adjustment: Yadam * Hadam = YapollO * ηapoiio, where Y is the weight decay factor.
4https://github.com/davda54/ada-hessian
5https://github.com/rafaljozefowicz/lm/blob/master/1b_word_vocab.txt
6https://github.com/pytorch/fairseq
17
Under review as a conference paper at ICLR 2021
Table 4: Classification accuracy on CIFAR-10 and ImageNet. For each experiment, we report the
mean and standard variance over 5 runs.
Method	CIFAR-10		ImageNet	
	milestone decay	cosine annealing	milestone decay	cosine annealing
SGD	I	93.91±0.07	94.53±0.27 I	77.19±0.07	78.17±0.06
Adam	91.41±0.30	91.56±0.19	71.72±0.13	71.19±0.10
RAdam	91.80±0.04	91.88±0.15	72.37±0.08	71.64±0.14
Adam-adj	93.74±0.15	94.24±0.09	76.86±0.06	77.54±0.16
RAdam-adj	93.88±0.11	94.38±0.25	76.91±0.07	77.68±0.08
AdaHessian	I	93.97±0.22	94.48±0.17 J	-	一
Apollo	I	94∙20±0.12	94.60±0.06	I	77.90±0.06	78.54±0.09
Figure 4: Training loss and test accuracy of ResNet-110 on CIFAR-10 with various rates of weight
decay, with two schedule strategies of learning rate decay.
D Detailed Experimental Results
In this section, we report the detailed experimental results in Section 4, and the results the investigation
of the effect of weight decay.
D.1 Detailed Results on Image Classification
Table 4 illustrates the details of the experimental results on Image Classification. For each experiment,
we report the mean values with corresponding standard deviations over 5 runs.
D.2 Effect of Weight Decay Rate on Optimization
To further investigate the effect of weight decay rate on converge speed and generalization perfor-
mance for different optimization methods, we conduct experiments on CIFAR-10 of ResNet-110
with a range of weight decay rates from 1e-4 to 9e-4. As discussed before, for Adam and RAdam,
we adjust the weight decay rate based on the ratio of the learning rate such that the they have similar
strength of regularization for fair comparison:
Yadam * ηadam = YaPollo * ιηapollo	(22)
where γ is the weight decay rate and η is the learning rate.
Figure 4 shows the convergence of different optimization methods with various rates of weight decay,
together with the classification accuracy. Apollo achieves improvements over all the three baselines
on convergence speed with different rates of weight decay. For classification accuracy, Apollo
obtains the best accuracy when the weight decay rate is larger than 4e-4. When the weight decay
rate is decreasing, SGD obtains the best accuracy, while Apollo achieves comparable performance.
18
Under review as a conference paper at ICLR 2021
D.3 Comparison on Training Speed and Memory Cost
In this section, we compare the training speed and memory between SGD, Adam, AdaHessian
and Apollo. Table 5 summarizes the comparison of cost of a single iteration of update. For fair
comparison, experiments of CIFAR-10 and One Billion Words are conduced on a single NVIDIA
TITAN RTX GPU, while experiments of ImageNet and WMT are performed with distributed training
on 8 NVIDIA Tesla V100 GPUs.
Table 5: Comparison between different optimization methods on training speed and memory cost.
Cost (×SGD)	CIFAR-10		ImageNet		1BW		WMT-14	
	Speed	Memory	Speed	Memory	Speed	Memory	Speed	Memory
SGD	1.00	1.00	1.00	1.00	1.00	1.00	1.00	1.00
Adam	1.16	1.01	1.01	1.03	1.19	1.34	1.13	1.04
Apollo	1.42	1.01	1.23	1.05	1.49	1.62	1.19	1.06
AdaHessian	5.76	2.12	11.78	2.51	3.51	2.78	8.46	2.47
From Table 5, we see that the second-order AdaHessian requires much more computational resource
than first-order methods on both time and memory. In addition, the slow-down of AdaHessian
becomes more significant for larger-scale models with distributed training across multiple GPUS,
such as ResNext-50 on ImageNet and Transformer on WMT.
D.4 Experiments on Parameter-Wise Gradient Clipping
In this section, we provide some preliminary results on parameter-wise gradient clipping, a modifica-
tion of the standard gradient clipping that is inherently proper to Apollo. Parameter-wise gradient
clipping is to clip the gradient of each parameter individually based on its own norm. It can be
regarded as a trade-off between gradient clipping by global norm and by each value.
We conducted two groups of experiments to compare with the standard gradient clipping method —
language modeling and neural machine translation. The experimental settings for standard gradient
clipping are exactly the same as in section 4, where we clipped the gradient by global norm 1.0 for
each model. For parameter-wise gradient clipping, we clipped each parameter by 0.5 for the LSTM
model in language modeling, and 0.1 for the Transformer-base model in NMT.
Table 6: Comparison between Apollo with standard and parameter-wise gradient clipping on One
Billion Words and WMT-14. We report the mean and standard variance over 5 runs.
	1BW	WMT-14
	 Standard	31.94±0.09	28.34±0.10
Parameter-wise	31.75±0.10	28.39±0.11
Table 6 lists the preliminary results. On both the two groups of experiments, parameter-wise gradient
clipping slightly outperforms the standard one.
E Experiments with Small Toy CNN Models
In this section, we provide the comparison between SdLBFGS (Wang et al., 2017) and Apollo
on CIFAR-10 dataset with a small toy CNN model7. The implementation of SdLBFGS is based on
the public PyTorch release8, which includes two important modifications to the original SdLBFGS
algorithm: identity matrix initialization and direction normalization (Li & Liu, 2018). For each
optimizer, we train the CNN model for 50 epochs with batch size equals to 64. After each epoch, the
learning rate is decayed by the rate 0.95. For the start learning rate for each optimizer, we performed
7https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html
8https://github.com/harryliew/SdLBFGS
19
Under review as a conference paper at ICLR 2021
search in a wide range: η ∈ {1.0, 0.5, 0.2, 0.1, 0.05, 0.01, 0.005}, and select the one obtains the
optimal performance. The final start learning rate for both SdLBFGS and APOLLO is 0.1. Following
Li & Liu (2018), the memory size of SdLBFGS is set to 100. For APOLLO, we linearly warmed up
the learning rate from 0.01 in the first 10 updates. For other hyper-parameters of each optimizer, we
choose the default value.
Figure 5: Training loss and test accuracy of SdLBFGS and Apollo on CIFAR-10 with the small toy
CNN model.
From Figure 5, we see that Apollo convergences faster than SdLBFGS and obtains comparable test
accuracy. Note that Apollo is much faster (more than 10 times for one iteration) than SdLBFGS
and consumes much less memory (SdLBFGS stores 100 previous update directions).
F Apollo with Decoupled Weight Decay
Algorithm 2: APOLLO with weight decay (L2/Decoupled)
Initial: m0,d0, B0 J 0, 0,0
while t ∈ {0, . . . , T } do
for θ ∈ {θ1 , . . . , θL} do
gt+ι J Vft (θt )+λθt
mt+1 J β1-β+ mt + 1--t+ι gt+1
,dT (mt+ι-mt )+dTBtdt
α J	(kdt ∣∣4+e)4
Bt+1 J Bt - α ∙ Diag(d2)
Dt+1 J rectify(Bt+1, 1)
dt+1 J Dt-+11mt+1+λθt
θt+1 J θt - ηt+1dt+1
end
end
return θT
// Initialize m0, d0, B0 to zero
// Calculate gradient at step t
// Update bias-corrected moving
// Calculate coefficient of B update
// Update diagonal Hessian
// Handle nonconvexity
// Calculate update direction
// Update parameters
20