Under review as a conference paper at ICLR 2021
Differentiable Learning of Graph-like Logi-
cal Rules from Knowledge Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Logical rules inside a knowledge graph (KG) are essential for reasoning, logical
inference, and rule mining. However, existing works can only handle simple, i.e.,
chain-like and tree-like, rules and cannot capture KG’s complex semantics, which
can be better captured by graph-like rules. Besides, learning graph-like rules is
very difficult because the graph structure exhibits a huge discrete search space.
To address these issues, observing that the plausibility of logical rules can be
explained by how frequently it appears in a KG, we propose a score function that
represents graph-like rules with learnable parameters. The score also helps relax
the discrete space into a continuous one and can be uniformly transformed into
matrix form by the Einstein summation convention. Thus, it allows us to learn
graph-like rules in an efficient, differentiable, and end-to-end training manner by
optimizing the normalized score. We conduct extensive experiments on real-world
datasets to show that our method outperforms previous works due to logical rules’
better expressive ability. Furthermore, we demonstrate that our method can learn
high-quality and interpretable graph-like logical rules.
1	Introduction
Knowledge graph (KG) refers to a special type of directed graphs including various entities as nodes
and relations as directed edges representing a large number of facts (Auer et al., 2007; Bollacker
et al., 2008). In KG, logical rules are a set of compositional logical relations within a specific struc-
ture, which are important for reasoning (Cohen et al., 2019; Zhang et al., 2019a; Qu & Tang, 2019),
logical inference (Dhingra et al., 2020; Das et al., 2018; Xiong et al., 2017), rule mining (Sadeghian
et al., 2019; Yang et al., 2017; Yang & Song, 2020), theorem proving (Rocktaschel & Riedel, 2017;
Minervini et al., 2018; 2020), etc.
Learning logical rules (Galarraga et al., 2015; Chen et al., 2016), as an important task, aims to
infer a structural logical rule for logical query or relation, which can support logical query or link
prediction while providing interpretable logical rules. The structure of logical queries can be various
with very different semantics, as shown in Figure 1, including chain-like, tree-like and graph-like
rules. Learning the logical rules, especially the graph-like rules, are very difficult because both the
logical structure and the relations assigned on each edge are unknown requiring to be inferred from
input-output pairs, which compose a huge discrete searching space.
In this paper, we dive into the problem of learning graph-like logical rules, including both the logical
structure representing how logic connects and the relations assigned on different edges. Recently,
a series of works on learning logical rule (Yang et al., 2017; Sadeghian et al., 2019; Yang & Song,
2020) has been proposed, which not only can support tasks including logical query and link pre-
diction, but as a side effect, can also provide the mined logical rules with high interpretability. As
shown in Figure 1, all these works are limited to learning chain-like rules (the left case) (Yang
et al., 2017; Sadeghian et al., 2019) or tree-like rules (the middle case) (Hamilton et al., 2018; Ren
et al., 2020; Yang & Song, 2020). However, there are widely-existed graph-like logical rules, which
the existing works cannot handle due to their limited expressive ability about logical rules. Learn-
ing graph-like logical rules is very important in many scenarios such as recommendation systems,
question-answering system and KG completion, while learning such complex rules is still an open
and challenging problem.
1
Under review as a conference paper at ICLR 2021
Chain-like rule
Semantic
Questions
Who is X's friend's supervisor?
Q. Input Entity
Logical Structures
G = {eι(%M),e2(%,%)一∙}
Vi,V{e{{Xj}^{Zk}}
Tree-like rule
What is the address of the university
that both the students Xiand X2
study at?
Graph-like rule
Which book has two common
readers with the book X while the
two readers are friends?
Structural
Logical Rules
study at address of
Knowledge Graphs
and Scores
ST({占}.y)
friend
sr: 4
ead(inv)
sr： 2
^:iι⅞ → /√pQ}.y)
Figure 1: Three examples of chain-like, tree-like, graph-like rules (three columns) and their corre-
sponding semantic questions, logical structures, structural logical rules, KGs and scores (four rows).
We propose a novel method that can explicitly learn the structural logical rules, including a logical
structure and the relations assigned on each edge, and we can use the inferred logical rules for
conducting inductive logical query with unseen entities and graphs. All the structural logical rules
construct a discrete search space to explore, and searching for that is an NP-hard problem. To
tackle with this problem, our method constructs a continuous space including both the structural
information and the relational information to learn, which allows us to train our model in an end-
to-end differentiable manner. Specifically, as shown in Figure 1, we take the frequency of a logical
rule in KG as its score to estimate how likely a logical rule stands. After optimizing w.r.t. the
normalized score, our model yields interpretable logical rules of high quality, and support inductive
logical query and link prediction, which has been demonstrated by our extensive experiments on
real-world datasets.
Our contributions can be summarized as following three aspects,
•	We first propose the problem of learning graph-like rules and design an end-to-end differentiable
model that can learn graph-like logical rules instead of only chain-like or tree-like rules, modeling
both the logical structure describing how the logic connects and relations assigned on edges.
•	We provide a uniform expression by Einsum to represent the score of all graph-like logical rules,
including the ones that cannot be represented by a combination of matrix/element-wise addi-
tion/product, which is elegant for expression and convenient for implementation.
•	We conduct extensive experiments to demonstrate that our model has better expressive ability
for graph-like logical rules and show our model can mine high-quality logical rules with high
interpretability.
2	Problem Formulation
Here, we formally introduce the definition of logical score, and based on that, we further introduce
our model’s main focus, relation inference (Yang et al., 2017; Sadeghian et al., 2019) and structural
rule learning, and our evaluation task, logical query (Hamilton et al., 2018; Ren et al., 2020).
2
Under review as a conference paper at ICLR 2021
Definition 1 (Logical Score) Logical rule is formulated by ∧in=1Ri → Rcpx : sr where sr is the
Scorefor ∧n=∖Ri, and R is a relation R = Ri(Vi, Vi0), Vi, Vi0 ∈ {{Xj}, Y, {Zk}} for i = 1,…,n
and Rcpx is a relation Rcpx ({Xj}, Y), {Xj} are input nodes, {Zk} are free-variable nodes, Y is
output node.
For strict logical query, for any Rcpx({Xj}, Y), there exists (Zι, ∙∙∙ , ZK) that make ∧n=ιRi be
true, we can draw the conclusion ∧in=1Ri → Rcpx. However, because KG is usually noisy and
incomplete, for learning logical rules, our key insight is to design the score as the number of free-
variable tuples (Zι,…，ZK) that make ∧n=ιR be true, which can capture the correlation between
logical rules and the input-output pairs of a logical query. For example, for the case in the middle of
Figure 1, Rstudyat(X1,Z) ∧ Rstudyat(X2, Z) ∧ Raddress of(Z, Y) → Rcpx(X1,X2,Y); for the case in
the right of Figure 1, we have Rread(X, Z1) ∧ Rread(X, Z2) ∧ Rfriend(Z1, Z2) ∧ Rread(inv)(Z1,Y) ∧
Rread(inv) (Z2, Y) → Rcpx(X, Y). Note that, Rcpx can both be a relation that exists in the KG and
the human-defined logic rule for a query, which tends to be more complex. The score sr serves as
two roles: (i). when input-output pairs are given, it can measure how likely a logical rule is, which
corresponds to the scenario of Task 1 and Task 2; (ii). when logical rules for query and inputs are
given, it can measure how much a output node fits the query, which corresponds to Task 3.
Task 1 (Relation Inference) Given Rcpx({Xj}, Y) is satisfied and a logical structure composed by
G = {eι(V1,V10), e2(V2,V2), ∙∙∙}，we need to infer how to assign relation Ri on each edge ei to
form a logical rule ∧in=1Ri(Vi, Vi0) that will make the score sr ofRcpx({Xj}, Y) high.
For this task, previous relation inference works (Yang et al., 2017; Sadeghian et al., 2019) can also
conduct this task but they limit the G to be a chain-like. We model the relation between the input-
output pairs behind the query as Rcpx and infer its graph-like logical rule.
Task 2 (Structural Rule Learning) Given Rcpx ({Xj}, Y) is satisfied and the possible max nodes
number n ≥ n, where n is the size of {{Xj},Y}, we need to infer What structure G 二
{e1(V1,V10), e2(V2,V2), ∙∙∙ , en(Vn, Vn)} where n ≤ n ≤ n and the relations assigned on edges
∧in=1 Ri that will make the score sr high.
For this task, logical structures in previous works (Yang et al., 2017; Sadeghian et al., 2019; Yang &
Song, 2020) are limited to chains or trees, and the number of input entities are limited to 1. However,
we can infer both the logical structure and the relations assigned on edges for graph-like rules.
Task 3 (Logical Query) Given input nodes {Xj } and the query relation, the target nodes of this
query can be represented by q = {Y|Rcpx({Xj}, Y)}.
Note that, in previous works (Hamilton et al., 2018; Ren et al., 2020), the logical rule Rcpx = ∧in=1Ri
is given, different from those works, we need to infer the ∧in=1Ri for logic query. Our model targets
at the inference of complex logical rules, and use the inferred logic rules to conduct logical query
as evaluation task. For evaluation, we regard Task 3 as the main task and the other two tasks as side
products.
3	Related Works
3.1	Logical Query from Knowledge Graphs
Logical rules learning (Teru et al., 2020; Evans & Grefenstette, 2018; Manhaeve et al., 2018; Wang
et al., 2019; Ho et al., 2018) is to learn logical rules (Task 1) for logical query (Task 3) in an inductive
setting. Neural-LP (Yang et al., 2017) design an end-to-end differentiable framework to learn the
probability of different logical rules. Furthermore, DRUM (Sadeghian et al., 2019) improve Neural-
LP (Yang et al., 2017) by introducing the low-rank matrix decomposition. However, these two works
can only tackle chain-like logical rules. Different from our model, they mainly focus on relatively
simple logical rules such as chain-like or tree-like rules. To the best of our knowledge, our model is
the first one that can learn to infer graph-like complex logical rule including structure and relations
assigned on different edges.
Logical queries (Serge et al., 1995) aims to learn how to accurately query an entity (Task 3) ac-
cording to given input entities and relations representing the logical rules in a transductive setting.
3
Under review as a conference paper at ICLR 2021
Table 1: Comparison between our method and existing methods.
Methods	∣	GQE I	Q2B	I GraIL ∣	NeuralLP ∣	DRUM Il	Ours
G’s structure	∣	tree ∣	tree	I chain ∣	chain ∣	chain ∣∣	graph
I	#{Xj} I	multiple ∣	multiple	I single ∣	single ∣	single ∣∣	multiple
Variable ∣ Z's position ∣	middle ∣	middle	I middle ∣	middle ∣	middle ∣∣	arbitrary
I Y's position ∣	end I	end	I end I	end I	end Il	arbitrary
Tasks	I	3 I	3	I 3 I	1,3 I	1,3 Il	1,2,3
Inductive	∣	× I	×	I √ I	√ I	√ Il	√
Interpretable	∣	× I	×	I × I	√ I	√ Il	√
According to Task 3, the logic rules representing the semantics of query explicitly given at both
training and testing stages in this branch of works, but in our paper, the logic rules require to be
inferred in the training stage. For most of these works, the main idea is to project entities into the
embedding space (Bordes et al., 2013; Trouillon et al., 2016; SUn et al., 2018; Balazevic et al., 2019)
and transform the relations into a type of manipulation in embedding space, such as a linear pro-
jection. Hamilton et al. (2018) first proposes an embedding-based method for conduct query with
tree-like logical rules. Ren et al. (2020) further improves Hamilton et al. (2018) by modeling the
entities as box embedding rather than vector embedding, which is more natural for the manipulation
for the conjunction of sets. Different from our model, these methods require explicit given logical
structures with given relations on edges.
3.2	Differentiable Inductive Logic Programming
Inductive Logic Programming (ILP) (Shapiro, 1981) aims to conduct inductive logic reasoning or
theorem proving based on entities, predicates and formulas. Predicates refer to a type of func-
tion projecting one or more entities to 0 or 1. For example, isM ale(X) return whether X is
a person (1) or not (0), isF atherOf (Y, X) returns whether Y is the father of X (1) or not
(0), where X and Y are variables which we can instantiate them as entities. Then, we can de-
fine formulas by combining predicates by logic operations including and/or/not. For examples,
isM ale(X) ∧ isF atherOf (Y, X) → isSonOf (X, Y ) is a logic entailment, which composes of a
body formula isM ale(X) ∧ isF atherOf (Y, X) and a head formula isSonOf (X, Y ). The physi-
cal meaning of this entailment means if the body formula is satisfied (equals to 1), then we can draw
the conclusion in the head formula. ILP is an NP-hard problem (Zhang et al., 2019b) and tradi-
tional methods relying on hard matching (Galarraga et al., 2015) for ILP are of high computational
complexity due to the very large search space.
Markov Logic Networks(MLN) (Richardson & Domingos, 2006; Kok & Domingos, 2005) elegantly
combine ILP problem and probabilistic models, which define potential functions in Markov random
field with formulas as nodes. Different from that, we consider the logical rule as a graph with
entities as nodes. In recent years, a lot of works on differentiable ILP (ROCktaSCheI & Riedel, 2017;
Minervini et al., 2018; 2020) have been proposed. The most recent work is NLIL (Yang & Song,
2020), which targets learning the logic rules based on unary and binary predicates efficiently. When
we only focus on the binary predicate, then we can naturally conduct ILP based on KG because the
relations in KG can be naturally regarded as a binary predicate on two entities (exist: 1; not exist:
0). If we only use NLIL (Yang & Song, 2020) for binary predicates (relations in KG), then it can
only tackle chain-like rules (same as Neural-LP (Yang et al., 2017) and DRUM (Sadeghian et al.,
2019)) instead of graph-like rules. To the best of our knowledge, differentiable ILP methods cannot
handle graph-like rules.
4	The Proposed Method
Logical graph structure and relations assigned on edges construct a huge discrete space, which
is extremely hard to search (Task 1 and Task 2). The key idea of our method is to design a score
function modeling both the logical structure and the relation assigned on edges, by optimizing which
we can obtain complex rules and conduct logical query (Task 3).
4
Under review as a conference paper at ICLR 2021
•	First, we introduce how to represent the score sr in when given the logical structure, to estimate
how likely a logical rule is, by maximizing which we can infer relations (Task 1).
•	Second, we introduce how to merge the structural information into the score to uniformly obtain
the logical structure and relations on edges by optimizing one score (Task 2).
•	Finally, we provide a uniform and elegant expression of the score sr in a matrix form by Einsum
for all graph-like logical rules and exploit cross-entropy loss for optimizing our model, which can
further support logical query (Task 3).
4.1	Relation Inference for Graph-like Logical Rules
Given input node {Xj}, target node Y and a structure G as shown in Figure 1, we use the number of
the tuples ({Zk}) that satisfy ∧in=1Ri as the score to evaluate the plausibility of a logical rule. We
denote the adjacency matrix of relation Ri as Ai , which is a soft choice among adjacency matrices
Ak corresponding to all relations R in KG. Then, the score, can be represented as
SrHXj }, Y) = XZV1=1 ∙∙∙ XZVKK = 1 Υn=ι Ai [Vi, Vn	(1)
where Vi, Vi0 ∈ {{Xj}, Y, {Zk}}, Ai[Vi, Vi0] denotes the (Vi, Vi0)-indexed entry in Ai, Ai is defined
as
Ai = XRl Ak ∙ exp(βik)/X? exp(βik0),	⑵
k=1	k0=1
where Ak represents the k-th relation in the set of relations in KG, by which We can use the Co-
efficiencts {βik} on different relations as learnable parameters to learn which relation should be
assigned on each edge given the logical structure. For the right case in Figure 1, Eq. (1) becomes
sr({Xj},Y) = X|ZV|=1 X|ZV|=1 A1[X, Z1]A2[X, Z2]A3[Z1, Z2]A4[Z1, Y]A5[Z2, Y],	(3)
Intuitively, in Eq. (3), after assigning an entity for each free variable Z, A1 [X, Z1] = 1 means there
is relation R1 between X and Z1 , the product of such terms equals to 1 only if all of them equal to
1, which means all free variable Z1 and Z2 together with X and Y satisfies the logical rule ∧in=1Ri.
We sum w.r.t. Z1, . . . , ZK to calculate the count of such tuples (Z1, . . . , ZK) in the full KG that
satisfied the logical rule to measure its plausibility.
4.2	Structural Rule Learning
In realistic logical inference scenarios (Yang & Song, 2020; Sadeghian et al., 2019), besides the
relations, we also do not know the structure of the logical rule G = {eι(H, V0), e2(V2, V0),…}.
Thus, we need to infer the logical structure as well as the relations. To achieve this goal, we add two
special auxiliary relations: “removing” relation represented by full-one matrix 1 (all the entries are
1) and “merging” relation represented by identity matrix (the diagonal entries are 1 and the others are
0) into the adjacency matrices. The physical meaning of identity matrix I is merging two connected
nodes as one node in the logical rule, because if I[Vi, Vi0] = 1, then Vi = Vi0, i.e., they are the same
entity in KG; the full-one matrix 1 removes the edge in the logical rule, because for any Vi and Vi0,
we have 1[Vi, Vi0] = 1, i.e., there is no relation requirement between Vi and Vi0. We expand the
parameters {βjk} for these two matrices and update the equation of softly selected adjacency matrix
Ai in Eq. (1) as follows,
Ai = XTI? A+ ∙ exP(βik)/XRR7 exP(βik0 ),	⑷
k=1	k0=1
where Aj represents the k-th relation in the augmented set of relations including the original rela-
tions in KG and two auxiliary relations. By merging nodes and removing edges, which correspond
to learning large coefficients on one of these two relations, we can obtain any graph structure from
the complete graph whose nodes number is no less than the ground truth graph.
Theorem 4.1 Given adjacency matrices consisting of the original adjacency matrices and two aux-
iliary adjacency matrices: identity matrix I and full-one matrix L we assume there are m ≤ m
points constructing the complete graph, then for any logical rule ∧in=1Ri, there exists a suite of
parameters {βjk} that make Eq. (1) equal to the number of (Z1, . . . , ZK) that satisfies the logical
rule.
5
Under review as a conference paper at ICLR 2021
4.3	Training Algorithm
For chain-like and tree-like rules, Eq. (1) can be efficiently computed with matrix and element-
wise product (see Section 4.4). But for more general graph-like rules, Eq. (1) cannot be directly
computed in such a compact way. Here, we introduce Einsum (see Appendix A for more details)
to make this possible. Einsum is a flexible convention of matrix calculation that unifies matrix
multiplication, element-wise multiplication and some more complex matrix/tensor calculation that
cannot be expressed by these ordinary operators (e.g., calculating the scores for graph-like rules).
Specially, we express Eq. (2) in a Einsum format as follows,
Sr({Xj},Y) = einsum (’Xi,…，匕匕，…，匕匕,Y,,vχι,…,Ai,..., An,vγ),	(5)
where sr ({Xj}, Y) denotes the score of the pair ({Xj}, Y), vX and vY are the one-hot vectors of
the input and output entities.
Such a convention has two advantages: (i). we can uniformly represent all graph-like rules in a
matrix form; (ii). well-engineered libraries such as Numpy1 and Pytorch2 can be exploited for fast
computation (Daniel et al., 2018).
Finally, we need a loss function that can not only encourage the positive samples but also can provide
penalty for negative samples, which allows our model to be learn logical rules accurately. Thus, we
first calculate Sr({Xj},Y) = Sr({Xj},Y)/PYo∈v Sr ({Xj},Y0), i.e., the normalized score for
each entity, then optimize the objective function composed by cross-entropy loss as follows,
argmin{βjk} X({Xj},Y)∈D XY,∈V -I[Y0 = Y]l°g®({Xj}，Y0)),	⑹
where IH is an indicator function, which returns 1 if the statement is true and otherwise returns 0.
We optimize this loss, which is back-propagated through the calculation of Einsum, to learn the
parameters {βjk } with high interpretability in an end-to-end differentiable manner. The training
process is summarized in Algorithm 1.
Algorithm 1 The training process of our model for logical inference.
Require: A set D of training data {({Xj}, Rcpx, Y)}, KG, max nodes number n ≥ ne, max_step;
1:	initialize βik , i = 1, . . . , n, k = 1, . . . , |R|, step = 0;
2:	while step < max_step do
3:	sample a mini-batch Dbatch batch ⊆ D;
4:	for each ({Xj }, Rcpx, Y) ∈ Sbatch do
5:	update parameters {βik} based on the loss function Eq. (6);
6:	end for
7:	step J step+1
8:	end while
9:	return logical rules ∧=∖Ri,n ≤ n ≤ n.
Computational Complexity. Naturally, the search space of structural logical rules is very large and
searching them is an NP-hard problem (Galarraga et al., 2015). Our method constructs a continuous
space to estimate the logical rules to optimize it in a differentiable manner, which significantly
reduces the complexity to O(|V |K+i), where |V| is the number of entities in KG, K is the number
of free-variable nodes.
4.4 Case Study: Comparison with existing methods
We take three real-world cases in Figure 1 to further show how our graph-like rules learning method
generalizes previous chain-/tree-like one. To the best of our knowledge, our model is the first that
can infer the graph-like rules.
•	Chain-like rule. Left in Figure 1 can be represented by Sr ({Xj}, Y) = einsum(’i, jk, kl, l’, vX,
Afriend, Asupervisor, vY). This will degenerate to the form of matrix multiplication (Yang et al.,
1https://numpy.org/doc/stable/reference/generated/numpy.einsum.html
2https://pytorch.org/docs/stable/generated/torch.einsum.html
6
Under review as a conference paper at ICLR 2021
2017; Sadeghian et al., 2019) as follows, sr ({Xj}, Y ) = vY>AsupervisorAfriendvX, where vX and
vY are the one-hot vectors of the input entity and output entity.
•	Tree-like rule. Middle in Figure 1 can be represented by sr ({Xj}, Y ) = einsum( ’i, j, ik, jk, kl, l’,
vX1,vX2,Astudyat, Astudyat, Aaddress of, vY). This will degenerate to the form of a combination of
matrix multiplication and element-wise product. For example, the case in the middle of Figure 1
as follows, sr ({Xj}, Y) = vY>Aaddress of(Astudy atvX2 Astudy atvX1), where	denotes the element-
wise product, vX1 and vX2 are one-hot vectors of two inputs, and vY is the one-hot vector of the
tail. EQB (Hamilton et al., 2018) and Query2box (Ren et al., 2020) require Y to be in the end of
the logical rules and {Zk} in the middle of logical rules, because they transform the logical rules
into a computational flow. Our method does not have requirements on the number of input nodes
and the position of Y and {Zk }.
•	Graph-like rule. Right in Figure 1 can be represented by sr ({Xj}, Y ) = einsum( ’i, ij, ik, jk, jl,
kl, l ’,vX,Aread,Aread,Afriend,Aread(inv),Aread(inv),vY). This cannot be simplified by only using
a combination of matrix multiplication/element-wise addition/product. All the graph-like logical
rules can be expressed by Einsum uniformly.
5 Experiments
We conduct extensive experiments on real-world datasets to compare our performance on logical
query (Task 3) in Section 5.2. Furthermore, we also demonstrate that our model is able to infer the
relations (Task 1) and learn the structural logical rules (Task 2) with high quality in Section 5.3 and
Section 5.4.
5.1	Experiment Setup
We implement our model in Python using Pytorch libraryand optimize all the models by Adam
optimizer (Kingma & Ba, 2015).
Datasets. We use the Kinship, Family and Unified Medical Language System (UMLS)
datasets (Kok & Domingos, 2007) to evaluate our model’s ability to learn some representative log-
ical rules for logical queries. Furthermore, we use Douban and Yelp to evaluate our model’s ability
to learn complex graph-like logical rules for logical queries. We report more details in Section C.1.
Query Generation. We care-
fully design five representa-
tive query structures as shown
in Figure 2 including the right
example in Figure 1, which do
exist in real-world datasets.
<^⅛B≡3ES1Λ
(2c) 2-chain (2i)2-intersection (2iw) 2^intersection (tri) triangle	(2cb) 2 chains
without an input
with a bridge
Figure 2: Five representative query structures.
We choose 2-chain (2c) because it is a basic chain rule, a comparison will help understand how our
model works on the most common cases. We choose 2-intersection (2i) because it is a typical tree-
like logical rule, while many of the existing works cannot handle it because most of them only allow
one input or are limited to chain-like rules. Only complex logical query methods (Ren et al., 2020)
can tackle with this case. We choose 2-intersection-without-an-input (2iw) because it is a special
type of tree-like logical rules, the positions of the output and free-variable node are very special. To
test more complex graph-like rule, we choose a triangle (tri) structure, which cannot be modeled as
chain/tree-like rules. At last, we choose a structure of 2 chains with a bridge (2cb) the same as the
right case in Figure 1. All these query structures have corresponding realistic semantics. For query
structure 2c, 2i, 2iw, we extract the top 5 frequent query types (∧in=1Ri) and then follow the logic
rules to randomly generate 1000 input-output pairs for each query structure. For query structure tri,
we use the query type represented by ”Who is X’s friend and meanwhile has a co-reading book with
X?”. For query structure 2cb, we use the query semantics ”Which book has two common readers
with the book X while the two readers are friends” as shown in Figure 1. We split the datasets
(queries) into training, validation, testing datasets according to the ratio 2 : 1 : 1. We use the
training dataset for learning the parameters in our model, use the validation set to decide when to
conduct early-stopping, and finally use the testing dataset for evaluating our model.
7
Under review as a conference paper at ICLR 2021
Comparing Methods. We compare our method on logical query with a series of rule mining meth-
ods Neural-LP (Yang et al., 2017) and DRUM(Sadeghian et al., 2019) with high interpretability
and the state-of-the-art embedding-based logical query method (Ren et al., 2020) that can handle
tree-like logical rules. To fairly compare the expressive ability about logical rules, we remove the
embedding information and the corresponding neural network generating coefficients in rule mining
baselines (Yang et al., 2017; Sadeghian et al., 2019), instead, we set the coefficients on relations as
learnable parameters.
Evaluation Metrics. We use Mean Reciprocal Rank (MRR) and Hit Rate atk (k= 1, 3) as evaluation
metrics (see Section C.2 for more details).
5.2 Performance Comparison
Table 2: Performance comparison in terms of different query structures between different methods
on three real-world datasets. The notation - means the method in the row cannot be used for the
query structure in the column.
2cl 2i I 2iw
Dataset	Method	MRR	Hit Rate		MRR	Hit Rate		MRR	Hit Rate	
						@3	@1		@3	@1
			@3	@1						
	Query2box	.23	.22	.20	.25	.17	.09	-	-	-
Kinship	Neural-LP	.38	.38	.26	-	-	-	.43	.61	.19
	DRUM	.55	.59	.44	-	-	-	.33	.45	.15
	Ours	.63	.69	.52	.63	.81	.42	.53	.77	.24
	Query2box	.13	.08	.07	.24	.28	.10	-	-	-
TTN4T Q	Neural-LP	.74	.82	.61	-	-	-	.28	.38	.06
UMLS	DRUM	.66	.65	.64	-	-	-	.18	.14	.04
	Ours	.66	.71	.58	.43	.53	.21	.32	.48	.12
	Query2box	.41	.56	.21	.23	.26	.00	-	-	-
Family	Neural-LP	.73	.86	.60	-	-	-	.67	.84	.45
	DRUM	.75	.90	.60	-	-	-	.65	.87	.41
	Ours	.72	.85	.58	.67	.86	.47	.69	.88	.48
Table 3: Performance comparison of complex logical query on real-world datasets.
Dataset	Method	tri			2cb		
		MRR	Hit Rate		MRR	Hit Rate	
			@3	@1		@3	@1
	Neural-LP	.91	1.00	.83	.61	.65	.49
Douban	DRUM	1.00	1.00	1.00	.61	.65	.49
	Ours	1.00	1.00	1.00	1.00	1.00	1.00
	Neural-LP	.79	.90	.70	.74	.82	.92
Yelp	DRUM	.65	.62	.53	.64	.61	.58
	Ours	.89	.85	.85	.94	.94	.94
We compare our model with other methods on three real-world datasets in terms of the three rela-
tively simple but representative query structures (2c,2i,2iw) as shown in Table 2. Our method is the
only one that can tackle all these three query structures. We can observe that, for query structure
2c, Our model achieves better or comparable performance with Neural-LP and DRUM, because
these two methods are specifically designed for chain-like rules. However, they cannot handle query
structure 2i because they only allow single-input query. Furthermore, they cannot learn the 2iw
correctly, because they require the output entity to be in the end of chain rule. Their learned chain
rules for 2iw are inaccurate or totally wrong, so our model improve the performance of 2iw query
type with a large margin compared to them. Query2box is designed for handling missing relations
in complex queries, which performs poorly on these datasets. Furthermore, Query2box relies on the
embeddings of entities so it cannot handle unseen entities but ours can.
For more complex logic rules (tri,2cb), we conduct the experiment on two real-world datasets in
recommendation system domains, Douban and Yelp, whose information is reported in Table 6. The
8
Under review as a conference paper at ICLR 2021
performances of Neural-LP and DRUM are poor because their learned chain rules are far from the
the graph-like rules as shown in Figure 3. Here, we do not compare with Query2box because it
cannot work on such graph-like logical rules.
The results reported in Table 3 show that our
model has the best ability on learning such graph-
like rules, which cannot be accurately modeled by
methods for chain-like rules, such as Neural-LP
or DRUM. As we will discuss in Section 5.3, our
model learns totally correct logical rules so it can
achieve such a good performance. The running
time results are shown in Table 4, we can observe
that our running time is comparable to Neural-LP
and DRUM.
Method	Kinship	UMLS	Family	Douban	Yelp
Neural-LP	.76	.91	3.37	-.85-	.83
DRUM	-.23-	.43	1.65	-7-	.71
Ours	.99	1.08	6.76	.76	.72
Table 4: Running times (mins) of our model
and other models on different datasets.
5.3	Case Study
Furthermore, we check whether the learned rules are the
same as ground-truth rules. As mentioned, we learn a set
of weights represented by {βik } for different relations as-
signed on edges, while the ’merging’ relation correspond-
ing to matrix 1 means merging two connected nodes and
Ground Truth
3: The
Learned Confidence (all correct)
the 'removing' relation corresponding to matrix I means re-
moving the edge (no rule requirement). So we visualize Figure
the weights representing the learned logic rules as shown rules learned by
in Figure 3. From that, we can observe that most of the rela- show the learned k-th relation (k =
tions are learned correctly with very high confidence, and it arg maxk βik) assigned on each i-
learns high confidence for removing one edge (the auxiliary th edge wiPth the highest confidence
relation represented by full-one matrix), which means our exp(βik)/	k0 exp(βik0 ). Note that,
model has the ability to both infer the relations and learn the full-one matrix, i.e., 1, removes the
logic structures.	corresponding edge from the logical
rule.
graph-like logical
our model. We
5.4	Ablation Study
To demonstrate the effectiveness of auxiliary matrices I and 1, we conduct the experiments that we
train the model with both matrices, one of them or none of them, respectively.
As shown in Table 5, we can observe that for case 2c, the
model with both matrices achieves the best performance,
which suggests the effectiveness of these two matrices. For
case 2i, the model with the matrix 1 achieves the best perfor-
mance because the model with 1 has the expressive ability to
model the case 2i, more matrices will lead to more parame-
ters and difficulty for learning. Due to the same reason, for
case 2iw, the model with the matrix 1 and the model with
both matrices achieve similar performance. For most complex
cases, we need both auxiliary matrices to accurately express
the complex logical rules.
Table 5: Performance comparison
of hit@10 among the variants of
our method with or without matri-
ces I and 1 on Family dataset.
Variants	2c	2i	2iw
none	0.08	0.69	0.20
only 1	0.43	0.94	0.58
only I	0.31	0.77	0.50
both	0.89	0.86	0.58
6 Conclusion
We propose a uniform score to not only unify the existing logical query and inference works but
also tackle more complex graph-like logical rules. Furthermore, we exploit Einsum to elegantly
express this score function and optimize our model in an end-to-end differentiable manner, which
can learn both the logic structure and the relations assigned on edges. At last, we conduct extensive
experiments on real-world datasets datasets to demonstrate the effectiveness of our model on logical
query and show that our model can yield high-quality complex logical rules with interpretability.
9
Under review as a conference paper at ICLR 2021
References
Krister Ahlander. Einstein summation for multidimensional arrays. Computers & Mathematics with
Applications, 44(8-9):1007-1017, 2002.
Soren Auer, Christian Bizer, Georgi Kobilarov, JenS Lehmann, Richard Cyganiak, and Zachary Ives.
Dbpedia: A nucleus for a web of open data. In The semantic web, pp. 722-735. Springer, 2007.
Ivana Balazevic, Carl Allen, and Timothy M Hospedales. Tucker: Tensor factorization for knowl-
edge graph completion. arXiv preprint arXiv:1901.09590, 2019.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collab-
oratively created graph database for structuring human knowledge. In Proceedings of the 2008
ACM SIGMOD international conference on Management of data, pp. 1247-1250, 2008.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In Advances in Neural Information
Processing Systems, pp. 2787-2795, 2013.
Yang Chen, Sean Goldberg, Daisy Zhe Wang, and Soumitra Siddharth Johri. Ontological pathfind-
ing. In International Conference on Management of Data, pp. 835-846, 2016.
William W Cohen, Haitian Sun, R Alex Hofer, and Matthew Siegler. Scalable neural methods for
reasoning with a symbolic knowledge base. In International Conference on Learning Represen-
tations, 2019.
G Daniel, Johnnie Gray, et al. opt_einsum-a python package for optimizing contraction order for
einsum-like expressions. Journal of Open Source Software, 3(26):753, 2018.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishna-
murthy, Alex Smola, and Andrew McCallum. Go for a walk and arrive at the answer: Reasoning
over paths in knowledge bases using reinforcement learning. In International Conference on
Learning Representations, 2018.
Bhuwan Dhingra, Manzil Zaheer, Vidhisha Balachandran, Graham Neubig, Ruslan Salakhutdinov,
and William W Cohen. Differentiable reasoning over a virtual knowledge base. In International
Conference on Learning Representations, 2020.
Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. Journal of
Artificial Intelligence Research, 61:1-64, 2018.
Luis Galarraga, Christina Teflioudi, Katja Hose, and Fabian M Suchanek. Fast rule mining in on-
tological knowledge bases with AMIE+. International Journal on Very Large Databases, 24(6):
707-730, 2015.
Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding logical
queries on knowledge graphs. In Advances in Neural Information Processing Systems, pp. 2026-
2037, 2018.
Vinh Thinh Ho, Daria Stepanova, Mohamed H Gad-Elrab, Evgeny Kharlamov, and Gerhard
Weikum. Rule learning from knowledge graphs guided by embedding models. In International
Semantic Web Conference, pp. 72-90. Springer, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Stanley Kok and Pedro Domingos. Learning the structure of markov logic networks. In International
Conference on Machine Learning, pp. 441-448, 2005.
Stanley Kok and Pedro Domingos. Statistical predicate invention. In International Conference on
Machine Learning, pp. 433-440, 2007.
Robin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt.
Deepproblog: Neural probabilistic logic programming. In Advances in Neural Information Pro-
cessing Systems, pp. 3749-3759, 2018.
10
Under review as a conference paper at ICLR 2021
Pasquale Minervini, Matko Bosnjak, Tim Rocktaschel, and Sebastian Riedel. Towards neural theo-
rem proving at scale. arXiv preprint arXiv:1807.08204, 2018.
Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, and Tim Rocktschel.
Learning reasoning strategies in end-to-end differentiable proving, 2020.
Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. In Advances in Neural
Information Processing Systems,pp. 7712-7722, 2019.
Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in
vector space using box embeddings. In International Conference on Learning Representations,
2020.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(1-2):
107-136, 2006.
Tim Rocktaschel and Sebastian Riedel. End-to-end differentiable proving. In Advances in Neural
Information Processing Systems, pp. 3788-3800, 2017.
Ali Sadeghian, Mohammadreza Armandpour, Patrick Ding, and Daisy Zhe Wang. Drum: End-to-
end differentiable rule mining on knowledge graphs. In Advances in Neural Information Process-
ing Systems, pp. 15321-15331, 2019.
Abiteboul Serge, Hull Richard, and Vianu Victor. Foundations of databases: the logical level, 1995.
Ehud Y Shapiro. Inductive inference of theories from facts. Yale University, Department of Com-
puter Science, 1981.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by
relational rotation in complex space. In International Conference on Learning Representations,
2018.
Komal K. Teru, Etienne Denis, and William L. Hamilton. Inductive relation prediction by subgraph
reasoning. arXiv: Learning, 2020.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, EnC GaUSSier, and Guillaume Bouchard. Com-
plex embeddings for simple link prediction. International Conference on Machine Learning,
2016.
Po-Wei Wang, Daria Stepanova, Csaba Domokos, and J Zico Kolter. Differentiable learning of
numerical rules in knowledge graphs. In International Conference on Learning Representations,
2019.
Wenhan Xiong, Thien Hoang, and William Yang Wang. Deeppath: A reinforcement learning method
for knowledge graph reasoning. In Proceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing, pp. 564-573, 2017.
Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge
base reasoning. In Advances in Neural Information Processing Systems, pp. 2319-2328, 2017.
Yuan Yang and Le Song. Learn to explain efficiently via neural logic inductive learning. In Interna-
tional Conference on Learning Representations, 2020.
Wen Zhang, Bibek Paudel, Liang Wang, Jiaoyan Chen, Hai Zhu, Wei Zhang, Abraham Bernstein,
and Huajun Chen. Iteratively learning embeddings and rules for knowledge graph reasoning. In
The World Wide Web Conference, pp. 2366-2377, 2019a.
Yuyu Zhang, Xinshi Chen, Yuan Yang, Arun Ramamurthy, Bo Li, Yuan Qi, and Le Song. Effi-
cient probabilistic logic reasoning with graph neural networks. In International Conference on
Learning Representations, 2019b.
11
Under review as a conference paper at ICLR 2021
A	Einsum
EinsUm (Ahlander, 2002) is a mathematical notational convention that is elegant and convenient
for expressing summation and multiplication on vectors, matrices, and tensors. It can represent
some calcUlation of mUltiple matrices that cannot be represented by matrix prodUct or element-wise
prodUct. Specifically, it not only makes the expression of complex logic rUles very simple and
elegant bUt also make the implementation of this calcUlation simpler and more efficient. Here, we
introdUce the implicit mode EinsUm becaUse it can tackle more cases than the classical one, and this
fUnction has been implemented in many widely-Used libraries sUch as NUmpy and Pytorch. It takes
a string representing the eqUation and a series of tensors as inpUt and provides a tensor as oUtpUt.
The rUles of EinsUm are as follows,
•	the inpUt string is comma-separated inclUding a series of labels, where each term separated by
commas represent a tensor (or matrix, vector) and each letter is corresponding to a dimension;
•	the terms in the string before '→' means input variables, and the term after '→' means output
variables;
•	the same label in multiple inputs means element-wise multiplication on the dimensions of differ-
ent input tensors;
•	the dimensions that occur in the inputs but not in the outputs mean that dimension will be summed,
while others will remain in the output. When there is no dimension after '→',it means all dimen-
sions should be summed to get a scalar, and in such case '→' can be omitted.
Here we provide several simple examples to help understand:
•	einsum (’i, i → ’, a, b) or einsum (’i, i’, a, b) represents the inner product of two vectors a>b;
•	einsum (’i, j → ij’, a, b) represents the outer product of two vectors ab>;
•	einsum (’ij, ij → ij’, A, B) represents the element-wise product of two matrices A B;
•	einsum (’ij, jk → ik’, A, B) represents the matrix product of two matrices AB.
B Proof of Theorem 4.1
Proof 1 Here, we provide a strategy to construct the coefficients {βjk} which make Eq. (1) equal
to the number of (Z1, . . . , ZK) that satisfies the logical rule. First, without loss of generality, we
assume that the ground-truth logical rules is ∧in=1Ri(Vi, Vi0) including m free-variable nodes. Our
strategy is divided into two stages: merging redundant nodes and removing useless edges.
Stage 1: Merging redundant nodes. If m > m, we can set the coefficient of identity matrix of a
complete graph on all edges between node Zm, Zm+ι, ∙ ∙ ∙ , Zm as 1, which means all ofthese edges
are merged. And then for any Zj that j < m and Zk,Zl that k, l > m, we force the coefficient on the
edge between Zj and Zk should be the same as the edge between Zj and Zl, i.e., the edge connected
to these binding nodes should be the same. After the merging stage, we get a complete graph with
m nodes.
Stage 2: Removing useless edges. Then for any V and V 0 if there is no edge between these two
nodes, we set the coefficient of ’removing’ relation as 1 and others as 0, which means we remove
those edges from the logical rules. Finally, for Ri (Vi , Vi0) in the ground-truth logical rule, we set
the coefficient of the correct relation on the corresponding edge as 1 and others as 0.
After all these manipulations, we can get that
sr({Xj}, Y) = XZV=I …XZVITYi=c1 Ai [Vi, Vn	⑺
1=	K=	=
where
Ai = Xk=II 卜Xp(eik"XkR= 1eXP(eikO)) A+,
becomes	Sr({Xj }, Y) = XZV=I -XZLIYnT Ai M,百，	(8)
1=	K=	=
12
Under review as a conference paper at ICLR 2021
where
Ai=XR+2 (exp® "X7+2 eχpG‰O)) A,
k=1	k0=1
which means the Eq. (7) becomes the number of (Z1, . . . , ZK) that satisfies the logical rule. Note
that, this is not the only way to construct the parameters to achieve this goal.
C Experiments
C.1 Dataset
The statistics of five real-world datasets are reported in Table 6.
Table 6: Datasets statistics for learning complex logical rules.
	#TriPlets	#ReIationS |R|	#Entities |V|
KinShiP	9587	25	104
UMLS	5960	46	135
Family	28356	12	3007
Douban	310	3	100
-Yelp-	283	3	100
C.2 Evaluation Metrics.
For each query Rcpx({Xj}, Y ), we can calculate scores sr for all entities by our model. We sort the
entities in a descending order of score and denote the rank of a right entity Y as rank(Y ). Then, for
each entity Y we calculate MRR as follows,
MRR = TT=;------j- X、	---77τ7r
|Dtest|	({Xj},Y)∈Dtest rank(Y )
and Hit Rate at k as follows,
hit@k = -ɪ- X	I [rank(Y) ≤ k],
∣Dtest∣ 乙({Xj},Y)∈Dtest L ' , - l,
where IH is an indicator function, which returns 1 if the statement is true and otherwise returns 0.
C.3 Link Prediction
We conduct the experiment of link prediction on three datasets Kinship, UMLS and Family follow-
ing the setting in Sadeghian et al. (2019). To be fair, we remove the embedding information in
logical rule mining methods to purely compare the expressive ability about logic rules. We test all
the methods for chain-like rules(Neural-LP (Yang et al., 2017), DRUM (Sadeghian et al., 2019) for
more complex rule learning with only one input and one output, although they may learn inaccurate
or wrong rules. We further add GraIL (Teru et al., 2020), the state-of-the-art method designed for
inductive link prediction, as a baseline. As shown in Table 7, we can observe that our model achieves
comparable performance compared with existing works.
Table 7: Performance comparison of link prediction on three real-world datasets.
Method	Kinship				UMLS				Family			
	MRR	Hit Rate			MRR	Hit Rate			MRR	Hit Rate		
		@10	@3	@1		@10	@3	@1		@10	@3	@1
DRUM	.52	.83	.60	~3	.80	.92	783	^T73	.71	.90	布	.58
GraIL	.55	.85	.63	IT	.72	.91	783	T59	.85	.95	792	.76
Ours	.55	.72	.61	744	.70	.79	T72	T65	.82	.94	布	.78
13