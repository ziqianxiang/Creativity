Under review as a conference paper at ICLR 2021
FSPN: A New Class of Probabilistic Graphical
Model
Anonymous authors
Paper under double-blind review
Ab stract
We introduce factorize-sum-split-product networks (FSPNs), a new class of prob-
abilistic graphical models (pGms). FSPNs are designed to overcome the draw-
backs of existing PGMs in terms of estimation accuracy and inference efficiency.
Specifically, Bayesian networks (BNs) have low inference speed and performance
of tree-structured sum-product networks(SPNs) significantly degrades in presence
of highly correlated variables. FSPNs absorb their advantages by adaptively mod-
eling the joint distribution of variables according to their dependence degree, so
that one can simultaneously attain the two desirable goals—high estimation ac-
curacy and fast inference speed. We present efficient probability inference and
structure learning algorithms for FSPNs, along with a theoretical analysis and ex-
tensive evaluation evidence. Our experimental results on synthetic and benchmark
datasets indicate the superiority of FSPN over other PGMs.
1	Introduction
Probabilistic graphical models (PGMs), a rich framework for representing probability distributions,
have commonly been used for building accurate and tractable models for high-dimensional complex
data. In comparison with deep generative models containing many hidden layers, which are “black-
box” approximators, PGMs without latent variables tend to be much more interpretable and faster in
inference. They are more appliable in lots of tasks such as online causal inference (Pearl et al., 2009)
and query selectivity estimation (Getoor & Taskar, 2001), which have strict requirements on both
inference accuracy and speed of the deployed models. Therefore, PGMs have recently re-attracted
considerable research interests in the ML community (Zheng et al., 2018; Scanagatta et al., 2019;
Paris et al., 2020). Much efforts (Rooshenas & Lowd, 2014; Vergari et al., 2015; Desana & Schnorr,
2020; Rahman & Gogate, 2016; Shao et al., 2019; Sharir & Shashua, 2018; Choi et al.; Rahman
et al., 2014; Darwiche, 2009; Boutilier et al., 2013) have been devoted to improving the accuracy
and tractability (inference speed) of PGMs.
Challenges of PGMs. In the most well-known class of PGMs—Bayesian networks (BNs), a set
of random variables is modeled as a directed acyclic graph (DAG) where each variable (node) is
conditionally independent of others given its parents. BNs can accurately and compactly model data
distributions. However, although sometimes tractable, marginal probability inference on BNs is
generally intractable. Even worse, structure learning ofBN is NP-hard (Chickering, 1996). Another
class of PGMs—Markov random fields (MRFs), model the joint probability density function (PDF)
as an undirected graph. However, marginal probability inference on MRFs is also difficult (Koller &
Friedman, 2009; Murphy, 2012). Even computing the likelihood of a single data point is intractable.
To improve tractability, (Poon & Domingos, 2011) proposed a new class of PGMs, the sum-product
networks (SPNs). SPNs are recursively defined as weighted sums or products of smaller SPNs
on simpler distributions. The most widely used SPN has a tree-structure, whose inference time
is linear w.r.t. the number of nodes. They have high expressive efficiency on weakly correlated
variables (Martens & Medabalimi, 2014). However, for highly correlated variables, their joint PDF
is difficult to split into smaller ones where variables are locally independent. The learned SPN has
large size and poor generality. Prior works have tried to extend SPNs by incorporating BNs or MRFs
into SPNs (Rooshenas & Lowd, 2014; Vergari et al., 2015; Desana & Schnorr, 2020) or learning
directed acyclic graph (DAG)-structured SPNs (Rahman & Gogate, 2016; Dennis & Ventura, 2015).
However, these models either slow down the inference speed or hard to learn. Some works (Shao
et al., 2019; Sharir & Shashua, 2018) designs variations of SPNs to model conditional PDFs, but
they are not suitable for evidence or marginal probability computation.
1
Under review as a conference paper at ICLR 2021
In summary, existing PGMs still have some drawbacks in terms of estimation accuracy or inference
speed. Designing highly accurate and tractable PGMs remains a challenging task.
Our Contributions. The key reason of existing PGMs’ drawbacks arise from that they only utilize
a single approach to decompose the joint PDF. BNs rely on conditional factorization, which is ac-
curate but difficult for inference. Tree-structured SPNs use locally independent factorization, which
cannot work well in presence of highly correlated variables. This naturally leads us to the following
question: if we could combine the strength of the two factorization approaches, is it possible to de-
sign a new type of PGM that is both accurate in estimation and fast for inference? To this end, we
propose factorize-sum-split-product networks (FSPNs), a versatile PGM aiming at this goal.
The main idea of FSPN is to adaptively decompose the joint PDF of variables based on their depen-
dence degree. Specifically, FSPN separates the set of highly correlated variables from the rest by
conditional factorization without loss of precision and processes each part accordingly. For highly
correlated variables, their values are interdependent, so a multivariate PDF using some dimension
reduction techniques (McGee & Carleton, 1970; Wang, 2015) is more suitable to model them. For
the remaining weakly correlated variables, local independence commonly exists, so that their joint
PDF can be split into small regions where they are mutually independent. FSPN recursively applies
the above operations to model the joint PDF in a compact tree structure. We show that, FSPN is a
very general PGM, which subsumes tree-structured SPNs and discrete BNs as special cases.
FSPNs absorb existing PGMs’ advantages while overcoming their drawbacks. First, expressive
efficiency of FSPNs is high, as two factorization approaches are used for variables with different
dependence degree in an adaptive manner. Second, FSPN models are tractable, as their inference
time is near linear w.r.t. its number of nodes. Third, structure learning of FSPNs is efficient. A lo-
cally optimal structure can be easily and efficiently obtained. In our evaluation, FSPNs consistently
outperform other models on datasets with varied variable dependence degree in terms of estimation
accuracy, inference speed, model size and training cost. On a series of PGM benchmark datasets,
FSPNs also achieve comparable performance w.r.t. the state-of-the-art models. In summary, our
contributions are as follows:
•	We propose FSPNs, a novel and general class of PGMs that simultaneously attain high esti-
mation accuracy and fast inference speed (Section 3).
•	We design an efficient inference algorithm for FSPNs, which runs in near linear time w.r.t. its
node size (Section 4).
•	We devise an efficient structure learning algorithm for FSPNs, which returns a locally maxi-
mum likelihood structure (Section 5).
•	We conduct extensive experiments to demonstrate the superiority of FSPNs on both synthetic
and benchmark datasets (Section 6).
2	Background and Related Work
In this section, we briefly review some background knowledge and related work. Let X =
{X1 , . . . , Xm } be a set of m random variables and D ∈ Rn×m be a training data matrix sam-
pled from Pr(X). A PGM aims at building a compact generative model PrD(X) on X such that: 1)
PrD (X) can be efficiently learned to accurately approximate the joint PDF Pr(X); and 2) marginal
probabilities can be inferred efficiently using the model on PrD (X). Building a PGM that fulfills
these two goals remains non-trivial. We review two well-known classes of PGMs as follows.
Bayesian Networks (BNs) represent the joint PDF Pr(X) of X as a DAG based on the condi-
tional independence assumption. Such a representation is exact when the BN structure accurately
captures the causal relations among variables. Therefore, it is an expressive and explainable PGM.
However, BNs have significant drawbacks in terms of inference and structure learning efficiency.
First, marginal probability inference for BNs has a high time complexity and is sometimes even
intractable. Exact inference methods, such as variable elimination and belief propagation (Koller
& Friedman, 2009), have exponential time complexity w.r.t. its node size. Approximate inference
methods, such as sampling (Gelfand, 2000; Andrieu et al., 2003) and loopy belief propagation (Mur-
phy et al., 2013), reduce the time cost but sacrifice estimation accuracy. Second, the structure learn-
ing problem for BN is NP-hard. Exact methods rely on either combinatorial search (Chickering
& Heckerman, 1997) in a super-exponential space of all DAGs or numerical optimization (Zheng
2
Under review as a conference paper at ICLR 2021
et al., 2018; Yu et al., 2019) with high time complexity on computing the DAG constraints. Some
approximate methods speed up the learning process by heuristic rules (Scanagatta et al., 2015; Ram-
sey et al., 2017), which may make the learned BN structure inaccurate for probability estimation.
Some prior works have focused on learning tractable BNs by compiling into a circuit (Lowd &
Domingos, 2012) or bounding tree width (Scanagatta et al., 2016). On a Chow-Liu tree, inference
time is O(ndh+1) where n, d and h represents the number of nodes, domain size and tree width,
respectively. In our evaluation, the inference time on such BNs is still slow even when h = 1.
Unlike BNs, Markov random field (MRFs) model the joint PDF of variables as an undirected graph.
The marginal probability inference on MRFs involve computing the normalizing factor of the poten-
tial functions, the complexity of which is exponential w.r.t. the tree width of the underlying graph.
Therefore, MRFs are not suitable for computing marginal probabilities and are thus mainly used for
data generation and pattern recognition (Li, 2009). Another tractable PGM related to BNs are cutset
networks (Rahman et al., 2014), which can be regarded as an ensemble of tractable BNs.
Sum-Product Networks (SPNs) model the joint PDF by recursively applying two operations,
namely sum and product, to split the joint PDF into simpler PDFs to capture contextual indepen-
dence. Specifically, a sum node represents a weighted sum of mixture models as PrD0 (X0) =
Pj wj PrD0 (X0) where Dj0 and wj are the data and weight of the i-th child. A product node
partitions variables X0 into mutually independent subsets S1, S2, . . . , Sd such that PrD0 (X0) =
Qi PrD0 (Si). A leaf node commonly maintains a univariate distribution PrD0 (X0) of a single vari-
able X0 . The marginal probability can be computed by a bottom-up traversal on SPNs, whose time
cost is linear w.r.t. the number of nodes.
In the literature, tree-structured SPNs can be efficiently learned by a number of methods (Gens &
Domingos, 2012; 2013; Peharz et al., 2013; Trapp et al., 2019). Thus, they are most widely used.
However, tree-structured SPNs can not work well in presence of highly correlated variables in X .
In this situation, a single product node is unable to split these variables and SPN would repeatedly
add sum nodes to split D0 into very small volumes, i.e., |D0| = 1 in extreme. This large structure has
low inference speed and poor generality, which degrades its estimation quality.
In order to overcome the drawbacks of tree-structured SPNs, a number of attempts have been made
to incorporate BNs or MRFs into SPNs. (Desana & Schnorr, 2020) designs SPGMs, a hybrid model
of BNs and SPNs, where sum and product nodes are applied and their children may be BNs modeling
partial variables in X. However, inference time for SPGMs is quadratic w.r.t. the maximum domain
size of variables in X with a large factor. (Rooshenas & Lowd, 2014) and (Vergari et al., 2015)
proposed SPN-BTB and ID-SPN, which apply BNs and MRFs to model PrD0 (X0) as multivariate
leaf nodes where variables in X0 can not be easily modeled by sum and product operations. They are
more compact than tree-structured SPNs. However, the embedded BNs or MRFs may slow down
the inference process. In our evaluation, their inference time is longer than tree-structured SPNs.
More general form of SPN structures are DAGs. DAG-structured SPNs tend to be much more com-
pact and efficient for inference than tree-structured SPNs, as they merge redundant sub-structures
into a singleton unit in DAGs. However, learning an optimal DAG-structured SPN is NP-Hard (Rah-
man & Gogate, 2016). Existing solutions obtains the DAG structure by greedy search or heuristic
rules (Rahman & Gogate, 2016; Dennis & Ventura, 2015) over tree-structured SPNs, whose perfor-
mance gain may be limited.
SPNs can be extended to model conditional PDFs Pr(Y |X). (Shao et al., 2019) proposed con-
ditional SPNs (CSPNs). Each leaf node in CSPNs models Pr(Yi|X) on a singleton variable Yi.
CSPNs are mainly used for point data and cannot be directly used to compute marginal probabilities
on X. (Sharir & Shashua, 2018) proposed sum-product-quotient networks (SPQNs), where the quo-
tient operation can model the conditional PDF by dividing the PDFs of its children. However, until
now, SPQNs are only a theoretical model, and no structure learning methods have been proposed.
There also exist other types of PGMs, such as probabilistic sentential decision diagrams (PS-
DDs) (Kisa et al., 2014). PSDDs can model the PDF in presence of massive, logical constraints
but only work on binary variables.
Summary. To model a joint PDF, existing PGMs apply two factorization approaches. BNs and some
types of SPNs use conditional factorization to capture conditional independence, which is accurate
but inefficient for inference and structure learning. Tree-structured SPNs use independent factoriza-
3
Under review as a conference paper at ICLR 2021
tion to capture contextual independence, which enables fast inference but has poor performance in
presence of highly correlated variables. None of them can comprehensively fulfill the desired goals
of PGMs. To resolve these drawbacks, we combine their advantages to design a new type of PGMs,
which attains both estimation accuracy and inference efficiency at the same time.
3	THE FSPN MODEL
In this section, We propose the factorize-split-sum-product network (FSPN), a new class of PGM.
Main Idea. FSPN adaptively applies both conditional and independent factorization approaches to
decompose variables X with different dependence degree. Specifically, let H be a set of highly
correlated variables in X, e.g., whose pairwise correlation values are above certain threshold. A
tree-structured SPN can not accurately and efficiently model Pr(X) in presence of H. Therefore,
we first decompose Pr(X) as Pr(X) = Pr(W) ∙ Pr(H|W) where W = X \ H by the conditional
factorization approach and model each distribution accordingly.
For Pr(H |W), each value x of W specifies a conditional PDF Pr(H|x). To compactly model
Pr(H |W), we partition W into multiple ranges R1, R2, . . . , Rt such that for any x, x0 in the same
Ri, Pr(H |x) = Pr(H |x0) roughly holds. Then, we only need to maintain one PDF Pri(H) for
each Ri. As variables in H are highly correlated, we can model Pri(H) as a multivariate PDF using
sparse distribution modeling (Wang, 2015) and dimension reduction techniques such as PCA/ICA
and piece-wise regression (McGee & Carleton, 1970). For Pr(W), we can recursively separate each
set of highly correlated variables from W using the above method until the remaining variables
W0 ⊆ W are weakly correlated. As local contextual independence highly likely holds on W0 ,
we can apply the independent factorization approach to further decompose Pr(W0). Specifically,
Pr(W0) is split into multiple small regions where variables in W0 are locally independent. Then, for
each region, we maintain a univariate distribution Pr(Xj) for each Xj ∈ W0.
Formal Definition. Given a set of variables X and training data matrix D, let F denote the FSPN
modeling the joint PDF PrD(X). Each node N in F either represents a PDF PrD0 (S) of a variable
subset S ⊆ X on a data subset D0 ⊆ D; or a conditional PDF PrD0 (S|C) of S conditioned on vari-
ables C ⊆ X\S on D0. The root node ofF with S = X, D0 = D exactly represents PrD(X). Each
node N decomposes the distribution PrD0 (S) or PrD0 (S|C) according to the following operations:
•	Factorize: Given the PDF PrD0 (S), let H ⊆ S be a set of highly correlated attributes and
W = S\H. We have P^，(S) = P^，(W) ∙ P^，(H| W). The factorize node generates the left child
and right child to process the PDF PrD0 (W) and conditional PDF PrD0 (H |W), respectively.
•	Sum: Given the PDF PrD， (S), we can partition the data D0 into subsets D10 , D20 , . . . , Dk0 . The
sum node creates the i-th child to process the PDF PrD， (S) for each 1 ≤ i ≤ k. We can regard
PrD， (S) as mixture models, i.e., PrD， (S) =Pi wi PrD， (S) where wi is the weight of the i-th child.
•	Product: Given the PDF PrD， (S), assume that we can partition S into mutually independent
subsets of variables S1, S2, . . . , Sd on D0, i.e., PrD， (S) = Qj PrD， (Sj). Then, the product node
creates the j-th child to process PDF PrD， (Sj) for each 1 ≤ j ≤ d.
•	Uni-Leaf: Given the PDF PrD， (S), if |S| = 1, we create a leaf node with the univariate
distribution PrD， (S).
•	Split: Given the conditional PDF PrD， (S|C), we partition the data D0 into regions
D10 , D20 , . . . , Dt0 in a grid manner according to the domain space of C . The `-th child models the
conditional PDF PrD， (S|C). For each value c of C, PrD， (S|c) is modeled on exactly one child as
PrD， (S|c). Note that the different semantic of split and sum nodes. A split node separately models
PrD， (S|C) for different values of variables in C while a sum node decomposes the large model
PrD， (S) into smaller models on S.
•	Multi-Leaf: Given the conditional PDF PrD， (S|C), ifS is independent of C on D0, PrD， (S|c)
stays the same for any value c of C . At this time, we create a leaf node to represent the multivariate
distribution PrD， (S).
The above operations are recursively applied in order to model the joint PDF. In this paper, we fo-
cus on tree-structured FSPNs. However, a more general structure for FSPNs could be a DAG. We
leave the exploration of DAG-structured FSPN for future work. Factorize, sum, product and uni-leaf
nodes represent PDFs while split and multi-leaf nodes represent conditional PDFs. Figure 1 illus-
trates an FSPN example of the four variables in the data table. The two highly correlated attributes
4
Under review as a conference paper at ICLR 2021
Xi ■	X2	X3 ■	X4 ■
2	5.1	3.1	21
4	8.5	5.6	1.6
9	17.6	8	7.3
• ∙ ∙ 1	• • • 2.8	• • • 3.7	• • • 6.1
0	0.8	6.1	0.9
Figure 1: An data table of four variables and its corresponding FSPN.
X1,X2 are first separated from X3,X4 on node Ni. Pr(X3,X4) are decomposed by sum node (N2)
and product nodes (N4, N5). The uni-leaf nodes L1,L3 and L2,L4 models Pr(X3) and Pr(X4),
respectively. Pr(X1,X2∣X3,X4) are split into two regions by the value of X3 on node N3. On each
region, X1,X2 are independent of X3, X4 and modeled as multi-leaf nodes L5 and Lg .
Comparison with Other PGMs. For the remainder of this section, We compare our tree-structured
FSPNs with other PGMs. First, we show that FSPNs subsume tree-structured SPNs and discrete
BNs. Clearly, if we disable the factorize operation, an FSPN degenerates to the tree-structured
SPN model; if we only apply the factorize, product, split, uni-leaf and multi-leaf operations, an
FSPN could equally represent a discrete BN model. Due to space limits, we put the details on
how ajoint PDF represented by BN or tree-structured SP can be equivalently modeled by a FSPN in
Appendix A. Based on the transformation process, we obtain the following proposition, which shows
that the model size of an equivalent FSPN is bounded above by the model size of a tree-structured
SPN or BN, so the expressive efficiency of FSPN is no worse than them.
Proposition 1 Given a set of random variables X and data matrix D, if PrD(X) can be represented
by an SPN or a BN with model size M, then there exists an equivalent FSPN modeling PrD(X) with
model size no larger than M.
Second, we show that FSPNs resolve the drawbacks of BNs and tree-structured SPNs. FSPNs
combine the strengths of both conditional and independent factorization methods. Unlike tree-
structured SPNs, FSPNs separately model highly correlated variables together as multi-leaf nodes,
so an FSPN’s structure tends to be more compact and allows more accurate probability estimates.
Unlike the situation for BNs, the structure of FSPNs can be accurately and efficiently learned, and
the probability inference on an FSPN is near linear time w.r.t. its number of nodes. We discuss the
details of probability inference and structure learning for FSPNs in Section 4 and 5, respectively.
Third, unlike the case of SPGMs (Desana & Schnorr, 2020), FSPNs do not require an inference
process on any BN sub-structures, thus more efficient. Unlike SPN-BTBs (Rooshenas & Lowd,
2014) and ID-SPNs (Vergari et al., 2015) with multi-leaf nodes on any possible subsets X0 ⊆ X,
the multi-leaf nodes in an FSPN only model highly correlated variables S. Notice that values in the
joint PDF of S are very concentrated, so Pr(S) can be easily compressed and modeled in a lower
dimension space. Whereas, for Pr(X0), the storage cost for the exact PDF grows w.r.t. |X0|. Hence,
SPN-BTBs and ID-SPNs use BNs or MRFs with low tree-width, which may degrade the estima-
tion accuracy and inference speed. Furthermore, DAG-SPNs (Rahman & Gogate, 2016; Dennis &
Ventura, 2015) could be subsumed by DAG-structured FSPNs.
Fourth, comparing FSPNs with CSPNs (Shao et al., 2019), we note the different goals behind mod-
eling the conditional PDFs Pr(Y |X). CSPNs try to find local conditional independence between
variables in Y and model Pr(Yi|X) for each Yi ∈ Y . Whereas, FSPNs try to find local inde-
pendence between Y and X and model Pr(Y ) = Pr(Y |X) as multi-leaf nodes together. Unlike
SPQNs (Sharir & Shashua, 2018), FSPNs use the factorize nodes to simplify the representation of
joint PDF Pr(X, Y ) using two simpler distributions Pr(X) and Pr(Y |X). Whereas, SPQNs model
Pr(Y |X) using SPNs on Pr(X, Y) and Pr(X).
4 Probability Inference on FSPN
In this section, we describe the probability inference algorithm FSPN-Infer. In general, FSPN-
Infer works in a recursive manner. It starts from the root node of the FSPN and computes the
probability on different types of nodes accordingly. Specifically, for sum or product nodes, we
accumulate the probabilities from children by weighted sum or multiplication, in a way similar to
tree-structured SPNs. For each factorize node, we apply a divide-and-conquer process. Notice that,
5
Under review as a conference paper at ICLR 2021
Algorithm FSPN-INFER(F, E)
1:	let N be the root node of F
2:	if N is uni-leaf node then
3:	return PrD (E) by the univariate PDF on N
4:	else if N is a sum node then
5:	let N1, N2, . . . , Nt be the children of N with weights w1, w2, . . . , wt
6:	Pi - FSPN-INFER(FNi, E) for each 1 ≤ i ≤ t
7:	return Pit=1 wipi
8:	else if N is a product node then
9:	let N1 , N2 , . . . , Nt be the children of N
10:	Pi - FSPN-INFER(FNi, E) for each 1 ≤ i ≤ t
11:	return Qit=1 pi
12:	else
13:	let NL be the left child on variables W and NR be the right child
14:	let L1 , L2, . . . , Lt be the multi-leaf nodes of NR
15:	split E into E1, E2, . . . , Et by regions of L1, L2, . . . , Lt
16:	get Pi of Ei on variables X \W from the multivariate PDF on Li for each 1 ≤ i ≤ t
17:	Pi - FSPN-INFER(Fnl , Ei) for each 1 ≤ i ≤ t
18:	return Pt=ι Pi ∙ qi
each multi-leaf node in the right child of the factorize node specifies a range, within which the highly
correlated variables are locally independent of the others. Hence, we first divide the range of the
computed event into several parts by multi-leaf nodes. Then, for each part, the the probability of all
highly correlated variables can be obtained directly from the multi-leaf node, and the probability of
other variables could be recursively computed from the FSPN rooted at the left child of the factorize
node. Finally, we multiply and sum them together to obtain the result.
Next, we formally describe the FSPN-INFER algorithm. Given the FSPN F modeling the PDF
PrD (X), we can easily compute the marginal probability of an event of X. We can represent E in
a canonical form as a hyper-rectangle: X1 ∈ [L1, U1], X2 ∈ [L2, U2], . . . , Xm ∈ [Lm, Um], where
each side of the closed interval can also be open. Li and Ui represent the lower and upper bound
of variable Xi, respectively. We have Li = -∞ or Ui = ∞ if E has no constraint on left or right
side of Xi . If the constraint of E on a variable contains discontinuous intervals, we may split it into
several events satisfying the above form. In this paper, we do not consider events with ranges that
are not axis-aligned. FSPN-INFER takes the FSPN F and an event E as inputs, and outputs the
probability PrD (E) = PrD(X ∈ E). Let N be the root node of F (line 1). For any node N0 in F,
let FN0 denote the FSPN rooted at N0. FSPN-INFER recursively computes PrD (E) by the following
rules:
•	In the base case (lines 2-3) where N is a uni-leaf node, We directly return the probability of
E on the univariate PDF.
•	IfN is a sum (lines 4-7) or product node (lines 8-11), let N1, N2, . . . , Nt be all of its children.
We can further call FSPN-INFER on FNi and E for each 1 ≤ i ≤ t to obtain the probability on the
PDF represented by each child. Then, node N computes a weighted sum (for sum node) or multiplies
(for product node) these probabilities together to obtain PrD (E).
•	If N is a factorize node (lines 12-18), let NL and NR be its left child and right child. Assume
NL and NR of N representing the PDF PrD(W) and the conditional PDF PrD(H|W), respectively.
We have PrD(E) = Pee∈E PrD(ew) ∙ PrD(eH∣ew), where ew and &h represent the values of e on vari-
ables W and H, respectively. Let L1, L2, . . . , Lt be all multi-leaf nodes of NR. Each Li is defined
on a sub-range of W and maintains the PDF Pri(H) = PrD(H|w), which stays the same for all w
in this sub-range. Each sub-range also forms a hyper-rectangle, which is ensured by our structure
learning algorithm described in the next section. For different ew, PrD(H|ew) is represented by
different PDFs on Li. Therefore, we need to partition the range of E into E1, E2, . . . , Et in terms
of W to compute PrD(E). Ei represents the sub-range of E whose values of W fall in Li, which
could also be interpreted as a valid event since its range is also a hyper-rectangle. Then, we have
tt
PrD (E) = X PrD (ew)∙PrD (eH |ew) =X X PrD (eH |ew)∙PrD (ew) =X X Pri (eH )∙ X PrD (ew)
e∈E	i=1 e∈Ei	i=1 eH ∈Ei	ew∈Ei
(1)
The probability pi = Pe∈E Pri (eH) of Ei on H could be directly obtained from the multi-leaf Li.
The probability qi = Pe∈E PrD(ew) of Ei on W can be obtained by calling FSPN-INFER on FNL,
6
Under review as a conference paper at ICLR 2021
the FSPN rooted at the left child of N, and Ei. Then We sum all Pi ∙ q% together to get PrD(E).
Similarly, for the evidence probability inference PrD(Q|E = e) where Q, E are disjoint subsets of
X. We can obtain PrD(Q, E=e) and PrD(E=e) on the FSPN to compute PrD (Q|E = e).
We present a comprehensive example in Appendix B, Which describes the probability inference
process on an FSPN step by step. We noW analyze the complexity of our inference algorithm. Com-
puting the probability of any range on each leaf node can be done in O(1) time (Gens & Domingos,
2013). Let f and l be the number of factorize and multi-leaf nodes in F . The maximum number of
ranges to be computed on each node is O(lf), so the inference time of FSPN is O(lf n). Actually,
f tends to be a very small number (near O(1)) and the probability of multiple ranges could be com-
puted in parallel. Therefore, We have the folloWing proposition regarding the inference time cost of
FSPN. In our evaluations, inference on FSPN is 1-3 orders of magnitude faster than BN and SPN.
We reserve designing the FSPNs With theoretical bounds on l and f in the future Work.
Proposition 2 Given a FSPN F representing PrD (X) with n nodes and any event E of X, the
marginal probability PrD(E) = PrD(X ∈ E) can be obtained in near O(n) time on F.
5	Structure Learning of FSPN
In this section, We discuss the structure learning algorithm Learn-FS PN of FSPN. Learn-FSPN
takes as inputs a data matrix D, tWo sets X and C of variables, and outputs the FSPN for PrD(X|C).
Initially, we can call LEARN-FSPN(D, X, 0) to build the FSPN on PrD(X). LEARN-FSPN gener-
ally Works in a top-doWn manner by recursively identifying different operations to decompose the
joint PDF into small and tractable PDFs. Due to space limits, we put the pseudocode of Learn-
FSPN in Appendix C. The main steps of the algorithm are described as follows:
1.	Separating highly and weakly correlated variables: when C = 0, LEARN-FSPN detects whether
there exists a set H of highly correlated attributes since the principle of FSPN is to separate them
with others as early as possible. We find H by examining pairwise correlations and then group
variables whose correlation value is larger than a threshold τ . If H 6= 0, we add a factorize node
to split PrD(X). The left child and right child recursively call LEARN-FSPN to model PrD(X\H)
and PrD(H|X\H), respectively.
2.	Modeling weakly correlated variables: when C = 0 and there do not exist highly correlated vari-
ables in X, we try to split PrD(X) into small regions where variables in X are locally independent.
Specifically, if X contains only one variable, we model the univariate distribution PrD (X) using
maximum likelihood estimation (MLE) of parameters. This can be done by applying a multinomial
Dirichlet distribution for discrete variables or a Gaussian mixture model for continuous ones. Other-
wise, we first try to partition X into mutually independent subsets using an independence test oracle.
If X can be partitioned as S1 , S2, . . . , Sk, we add a product node and call LEARN-FSPN to model
PrD(Si) on each child. If not, we apply an EM algorithm, such as k-means, to cluster instances D
into D1, D2, . . . , Dt, add a sum node and call LEARN-FSPN to model PrDi (X) on each child.
3.	Modeling conditional PDFs: when C 6= 0, it tries to model the conditional PDF PrD(X|C). At
this time, variables in X must be highly correlated. First, we test if X is independent of C on D
by the oracle. If so, we can learn the MLE parameters of multivariate distribution PrD (X), such
as the multi-dimensional Dirichlet distribution or Gaussian mixture model. If not, we use an EM
algorithm, such as grid approximation of k-means, to partition the domain space of variables in
C into multiple grids. Based on this, instances in D are split into D10 , D20 , . . . , Dt0 where each Di0
represents all data points in a grid. Due to space limits, we put the details of the partition methods in
Appendix C. Then, we add a split node and call LEARN-FSPN to model PrD0 (X|C) on each child.
Note that, Learn-FSPN should be viewed as a framework rather than a specific algorithm, since
we can choose different independent test oracles and clustering algorithms such as (Neal & Hinton,
1998). Moreover, any structure learning optimization (Bueff et al., 2018; Jaini et al., 2018; Kalra
et al., 2018; Trapp et al., 2019) for SPNs can be also incorporated into Learn-FSPN.
Next, we show that Learn-FSPN returns an FSPN which locally maximizes the likelihood. The
analysis proceeds in a bottom-up manner. First, on both uni-leaf and multi-leaf nodes, the parameters
of distribution are learned using MLE. Second, the independence test oracle used in the product node
factorize the joint PDF into a product of independent ones, which causes no likelihood loss. Third,
7
Under review as a conference paper at ICLR 2021
Average RDC Score
Figure 2: Evaluation results on synthetic data.
-H- BN-DAG T- BN-Tree - O - FSPN
)sm( emiT ecnerefnI
0	0.2	0.4	0.6	0.8	1
Average RDC Score
for the EM methods used in sum and split nodes, the nodes can locally maximize the likelihood if
all children locally maximize the likelihood (Gens & Domingos, 2013). Fourth, the factorize node
uses the exact probability conditional factorization, which causes no likelihood loss. Therefore, we
have the following proposition.
Proposition 3 Given a set of random variables X and data D, LEARN-FSPN can return a local
MLE FSPN F of PrD(X) with independence test oracles and EM algorithms.
6	Evaluation Results
In this section, we report the evaluation results on both synthetic and real-world benchmark data.
Results on Synthetic Data. First, we demonstrate the superiority of FSPNs over existing PGMs on
six synthetic datasets with varied degree of dependence between variables. The dependence degree
is evaluated as the average RDC score (Lopez-Paz et al., 2013) between variables, which ranges
from 0.07 to 0.95. Each dataset contains 108 rows on 20 discrete variables with different domain
size ranging from 5 to 100. We sampled 105 rows for training PGMs and 104 rows for tuning
hyper-parameters. During testing, we randomly generate 102 evidence queries Pr(Q|E = e). We
uniformly select 2-4 variables as Q, 6-10 variables as E and assign a value e of E. To measure
inference accuracy we use the KL-divergence between the true distribution and the estimated one by
the learned PGM.
We compare our FSPN with a variety of PGMs: MSPN is a tree-structured SPN learned using
the method in (Molina et al., 2018) on mixed domains, which is shown to be better than the SPN
learned by (Gens & Domingos, 2013). SPN-BTB (Rooshenas & Lowd, 2014) is a SPN structure
with embedded BN as multivariate leaf nodes. The embedded BN is implemented by Chow-Liu
tree whose tree width is bounded by 1. SPGM (Desana & Schnorr, 2020) is the model integrating
BN and SPN. BN-DAG is a plain BN structure. BN-Tree is a BN implemented by Chow-Liu tree
with bounded width of 1. The structure of BN-DAG and BN-Tree are learned by the Pomegranate
package (Schreiber, 2018). Notice that, we do not find any open-source implementation of DAG-
SPN (Rahman & Gogate, 2016). Other start-of-the-art variations of BNs and SPNs, described in
Table 1 either work only on binary domains or do not support probability inference given evidence.
Hence, we can not compare with them in our experiments.
For fairness, we apply an exhaustive grid search over the hyper-parameters of each model and report
the best result. Figure 2 reports the evaluation results in terms of the two criteria: KL-divergence for
inference accuracy and the inference time for efficiency. We clearly observe that:
•	The estimation accuracy of FSPN is consistently better than all other models. In comparison
with MSPN and BN-Tree, the KL-divergence decreases by up to 144× and 82×, respectively. This
is because the tree-structured SPN can not work well in presence of highly correlated variables, and
the tree-structured BN sacrifices the model accuracy to improve its inference speed. In comparison
with SPN-BTB, SPGM and BN-DAG, FSPN also decreases the KL-divergence. This verifies that
FSPN can model the joint PDF more accurately.
•	The inference time of FSPN is 1-3 orders of magnitude faster than others. In comparison with
BN-DAG, SPN-BTB and BN-Tree, it improves the inference speed by up to 680×, 261× and 206×,
respectively. This verifies that the BN inference process is very time costly. Although bounding the
8
Under review as a conference paper at ICLR 2021
Table 1: Average test log-likelihoods on discrete datasets.
Dataset	# of vars	FSPN (ours)	LearnSPN	BayesSPN	SPGM	SPN-BTB	ID-SPN	MT	WinMine	ECNet	EM-PSDD
NLTCS	16	-6.05	-6.11	-6.02	-5.99	-6.01	-6.00	-6.01	-6.03	-6.00	-6.03
MSNBC	17	-6.01	-6.11	-6.03	-6.03	-6.03	-6.06	-6.07	-6.04	-6.04	-6.04
KDD	65	-2.14	-2.18	-2.13	-2.13	-2.12	-2.12	-2.13	-2.18	-2.12	-2.12
Plants	69	-12.00	-12.98	-12.94	-12.71	-12.09	-12.68	-12.95	-12.65	-12.78	-13.79
Audio	100	-40.02	-40.50	-39.79	-39.90	-39.62	-39.77	-40.08	-40.50	-39.73	-41.98
Jester	100	-52.39	-53.48	-52.86	-52.83	-53.60	-52.42	-53.08	-51.07	-52.57	-53.47
Netflix	100	-57.12	-57.33	-56.80	-56.42	-56.37	-56.36	-56.74	-57.02	-56.32	-58.41
Accidents	111	-26.99	-30.04	-33.89	-26.89	-28.35	-26.98	-29.63	-26.32	-29.96	-33.64
Retail	135	-10.83	-11.04	-10.83	-10.83	-10.86	-10.88	-10.83	-10.87	-10.82	-10.81
Pumsb-star	163	-22.04	-24.78	-31.96	-22.15	-22.66	-22.40	-23.71	-22.72	-24.18	-33.67
DNA	180	-80.97	-82.52	-92.84	-79.88	-80.07	-81.21	-85.14	-80.65	-85.82	-92.67
Kosarak	190	-10.66	-10.99	-10.77	-10.57	-10.58	-10.60	-10.62	-10.83	-10.58	-10.81
MSWeb	294	-9.60	-10.25	-9.89	-9.81	-9.61	-9.73	-9.85	-9.70	-9.79	-9.97
Book	500	-33.81	-35.89	-34.34	-34.18	-33.82	-34.14	-34.63	-36.41	-33.96	-34.97
EachMovie	500	-50.69	-52.49	-50.94	-54.08	-50.41	-51.51	-54.60	-54.37	-51.39	-58.01
WebKB	839	-149.72	-158.20	-157.33	-154.55	-149.85	-151.84	-156.86	-157.43	-153.22	-161.09
Reuters-52	889	-81.62	-85.07	-84.44	-85.24	-81.59	-83.35	-85.90	-87.55	-86.11	-89.61
20 Newsgrp	910	-155.30	-155.93	-151.95	-153.69	—	-151.47	-154.24	-158.95	-151.29	-161.09
BBC	1, 058	-252.81	-250.69	-254.69	-255.22	-226.56	-248.93	-261.84	-257.86	-250.58	-253.19
AD	1, 556	-15.46	-19.73	-63.80	-14.30	-13.60	-19.00	-16.02	-18.35	-16.68	-31.78
tree width ofBN could speed up the inference, it is still much slower than FSPN. FSPN is also faster
than MSPN since its structure is more compact.
We also evaluate the model size and training time of all models. Due to space limits, we put the
results and analysis in Appendix D. In a nutshell, model size of learned FSPNs are up to two orders
of magnitude smaller than others, and FSPNs’ training time is several times faster than others except
BN-Tree. In summary, this set of experiments validates the design choices of FSPNs, which provide
both accurate results and fast inference.
Benchmark Testing. Next, we compare FSPN with the current state-of-the-art methods on 20 real-
world benchmark datasets used in the literature (Gens & Domingos, 2013). Table 1 reports the test
log-likelihood scores of FSPN and other PGMs. Specifically, LearnSPN is the tree-structured SPN
from (Gens & Domingos, 2013). BayesSPN (Trapp et al., 2019) is an SPN with Bayesian structure
learning. ID-SPN (Rooshenas & Lowd, 2014) use embedded MRF as leaf nodes to enhance the
performance of SPN. MT stands for the mixture of tree models (Meila & Jordan, 2000). WinMine is
one of the most sophisticated BN learning package (Chickering, 2002). ECNet (Rahman et al., 2014)
and EM-PSDD (Liang et al., 2017) are the cutset network and PSDD with the best performance,
respectively. To avoid the effects of hyper-parameters, we quote results of other PGMs from their
original paper. We find that:
•	Overall, FSPN outperforms LearnSPN, BayesSPN, MT, WinMine, ECNet and EM-PSDD.
This is because LearnSPN and BayesSPN can not accurately model the joint PDF in presence of
highly correlated variables. The expressiveness of MT is inherently low since its model complexity
is not as high as others.
•	The performance FSPN is comparable to SPGM, SPN-BTB and ID-SPN on the whole. It
is slightly better than SPGM and ID-SPN but slightly worse than SPN-BTB. These models use
embedded BNs or MRFs in their structure, so they are more accurate than other SPN models.
7	Conclusions
In this paper we propose FSPNs, a novel class of PGMs aiming at overcoming the drawbacks of
existing PGMs. FSPN can adaptively model the joint distribution of variables with different depen-
dence degree. It achieves high estimation accuracy and tractability at the same time. We design a
near linear time marginal probability inference algorithm and a local MLE structure learning algo-
rithm for FSPNs. Our extensive evaluation results on synthetic and benchmark datasets demonstrate
that FSPNs attain superior performance. Based on these promising results, we affirmatively believe
in that FSPNs may be a better alternative to existing PGMs in a wide range of ML applications.
Moreover, we believe that there are many possible extensions of FSPNs worth researching in the
future, such as supporting maximum a posterior inference, latent variable interpretation, DAG and
Bayesian structure leaning of FSPNs and bounding their tree-width for tractability.
9
Under review as a conference paper at ICLR 2021
References
Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to
mcmc for machine learning. Machine Learning, 50(1-2):5-43, 2003.
Craig Boutilier, Nir Friedman, Moises Goldszmidt, and Daphne Koller. Context-specific indepen-
dence in bayesian networks. arXiv preprint arXiv:1302.3562, 2013.
Andreas Bueff, Stefanie Speichert, and Vaishak Belle. Tractable querying and learning in hybrid
domains via sum-product networks. arXiv preprint arXiv:1807.05464, 2018.
David Maxwell Chickering. Learning bayesian networks is np-complete. In Learning from Data,
pp. 121-130. Springer, 1996.
David Maxwell Chickering. The winmine toolkit. Technical report, Technical Report MSR-TR-
2002-103, Microsoft, Redmond, WA, 2002.
David Maxwell Chickering and David Heckerman. Efficient approximations for the marginal like-
lihood of bayesian networks with hidden variables. Machine Learning, 29(2-3):181-212, 1997.
YooJung Choi, Antonio Vergari, and Guy Van den Broeck. Probabilistic circuits: A unifying frame-
work for tractable probabilistic models.
Adnan Darwiche. Modeling and reasoning with Bayesian networks. Cambridge university press,
2009.
Aaron Dennis and Dan Ventura. Greedy structure search for sum-product networks. In Twenty-
Fourth International Joint Conference on Artificial Intelligence, 2015.
Mattia Desana and Christoph Schnorr. Sum-product graphical models. Machine Learning, 109(1):
135-173, 2020.
Alan E Gelfand. Gibbs sampling. Journal of the American statistical Association, 95(452):1300-
1304, 2000.
Robert Gens and Pedro Domingos. Discriminative learning of sum-product networks. In Advances
in Neural Information Processing Systems, pp. 3239-3247, 2012.
Robert Gens and Pedro Domingos. Learning the structure of sum-product networks. In International
Conference on Machine Learning, pp. 873-880, 2013.
Lise Getoor and Daphne Taskar, Ben andKoller. Selectivity estimation using probabilistic models.
In Proceedings of the 2001 ACM SIGMOD international conference on Management of data, pp.
461-472, 2001.
Priyank Jaini, Amur Ghose, and Pascal Poupart. Prometheus: directly learning acyclic directed
graph structures for sum-product networks. In International Conference on Probabilistic Graph-
ical Models, pp. 181-192, 2018.
Agastya Kalra, Abdullah Rashwan, Wei-Shou Hsu, Pascal Poupart, Prashant Doshi, and Georgios
Trimponias. Online structure learning for feed-forward and recurrent sum-product networks. In
Advances in Neural Information Processing Systems, pp. 6944-6954, 2018.
D Kisa, G Van den Broeck, and A Choi. Probabilistic sentential decision diagrams. In International
Conference on Principles of Knowledge Representation and Reasoning, pp. 1-10, 2014.
Daphne Koller and Nir Friedman. Probabilistic graphical models principles and techniques. MIT
Press, 2009.
Stan Z Li. Markov random field modeling in image analysis. Springer Science & Business Media,
2009.
Yitao Liang, Jessa Bekker, and Guy Van den Broeck. Learning the structure of probabilistic sen-
tential decision diagrams. In Proceedings of the 33rd Conference on Uncertainty in Artificial
Intelligence, 2017.
10
Under review as a conference paper at ICLR 2021
David LoPez-Paz, PhiliPP Hennig, and Bernhard Scholkopf. The randomized dependence coeffi-
cient. In Advances in Neural Information Processing Systems, pp. 1-9, 2013.
Daniel Lowd and Pedro Domingos. Learning arithmetic circuits. arXiv preprint arXiv:1206.3271,
2012.
James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks.
arXiv preprint arXiv:1411.7717, 2014.
Victor E. McGee and Willard T. Carleton. Piecewise regression. Regression, Journal of the American
Statistical Association, pp. 1109-1124, 1970.
Marina Meila and Michael I Jordan. Learning with mixtures of trees. Journal of Machine Learning
Research, 1(Oct):1-48, 2000.
Alejandro Molina, Antonio Vergari, Nicola Di Mauro, Sriraam Natarajan, Floriana Esposito, and
Kristian Kersting. Mixed sum-product networks: A deep architecture for hybrid domains. In
Thirty-second AAAI conference on artificial intelligence, 2018.
Kevin Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate infer-
ence: An empirical study. arXiv preprint arXiv:1301.6725, 2013.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Radford M Neal and Geoffrey E Hinton. A view of the em algorithm that justifies incremental,
sparse, and other variants. In Learning in Graphical Models, pp. 355-368. Springer, 1998.
Iago Paris, Raquel Sanchez-Cauce, and Francisco Javier Diez. Sum-product networks: a survey.
arXiv preprint arXiv:2004.01167, 2020.
Judea Pearl et al. Causal inference in statistics: An overview. Statistics Surveys, 3:96-146, 2009.
Robert Peharz, Bernhard C. Geiger, and Franz Pernkopf. Greedy part-wise learning of sum-product
networks. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pp. 612-627, 2013.
Hoifung Poon and Pedro Domingos. Sum-product networks: a new deep architecture. In IEEE
International Conference on Computer Vision Workshops, pp. 689-690, 2011.
Tahrima Rahman and Vibhav Gogate. Merging strategies for sum-product networks: from trees to
graphs. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence,
2016.
Tahrima Rahman, Prasanna Kothalkar, and Vibhav Gogate. Cutset networks: A simple, tractable,
and scalable approach for improving the accuracy of chow-liu trees. In Joint European Conference
on Machine Learning and Knowledge Discovery in Databases, 2014.
Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour. A million vari-
ables and more: the fast greedy equivalence search algorithm for learning high-dimensional
graphical causal models, with an application to functional magnetic resonance images. Inter-
national Journal of Data Science and Analytics, 3(2):121-129, 2017.
Amirmohammad Rooshenas and Daniel Lowd. Learning sum-product networks with direct and
indirect variable interactions. In International Conference on Machine Learning, 2014.
Mauro Scanagatta, Cassio P de Campos, Giorgio Corani, and Marco Zaffalon. Learning bayesian
networks with thousands of variables. In Advances in Neural Information Processing Systems,
pp. 1864-1872, 2015.
Mauro Scanagatta, Giorgio Corani, Cassio P De Campos, and Marco Zaffalon. Learning treewidth-
bounded bayesian networks with thousands of variables. In Advances in Neural Information
Processing Systems, pp. 1462-1470, 2016.
Mauro Scanagatta, Antonio Salmeron, and Fabio Stella. A survey on bayesian network structure
learning from data. Progress in Artificial Intelligence, pp. 1-15, 2019.
11
Under review as a conference paper at ICLR 2021
Jacob Schreiber. Pomegranate: fast and flexible probabilistic modeling in python. Journal of Ma-
chine Learning Research,18(164):1-6, 2018.
Xiaoting Shao, Alejandro Molina, Antonio Vergari, Karl Stelzner, Robert Peharz, Thomas Liebig,
and Kristian Kersting. Conditional sum-product networks: Imposing structure on deep proba-
bilistic architectures. arXiv preprint arXiv:1905.08550, 2019.
Or Sharir and Amnon Shashua. Sum-product-quotient networks. In International Conference on
Artificial Intelligence and Statistics, pp. 529-537. PMLR, 2018.
Martin Trapp, Robert Peharz, Hong Ge, Franz Pernkopf, and Zoubin Ghahramani. Bayesian learning
of sum-product networks. In Advances in Neural Information Processing Systems, pp. 6347-6358,
2019.
Antonio Vergari, Nicola Di Mauro, and Floriana Esposito. Simplifying, regularizing and strengthen-
ing sum-product network structure learning. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pp. 343-358. Springer, 2015.
Junhui Wang. Joint estimation of sparse multivariate regression and conditional graphical models.
Statistica Sinica, pp. 831-851, 2015.
Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: dag structure learning with graph neural net-
works. In International Conference on Machine Learning, pp. 7154-7163, 2019.
Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: continuous
optimization for structure learning. In Advances in Neural Information Processing Systems, pp.
9472-9483, 2018.
12
Under review as a conference paper at ICLR 2021
A Generality of FSPN
We present the details on how FSPNs subsume tree-structured SPNs, as well as discrete BNs, where
all variables are discretized and all (conditional) probability distributions are stored in a tabular form.
• Given a set of variables X and data D, if PrD (X) could be represented by a tree-structured
SPN S, we can easily construct an FSPN F that equally represent PrD (X). Specifically, we disable
the factorize operation in FSPN by setting the factorization threshold to ∞, and follow the same
steps of S to construct F . Then, the FSPN F is exactly the same of S. Obviously, their model size
is the same.
Algorithm BN-TO-FSPN(B, N)
1:	if B contains more than one connected component B1 , B2 , . . . , Bt then
2:	set N to be a product node with children N1 , N2 , . . . , Nt
3:	call BN-TO-FSPN(Bi, Ni) for each i
4:	else
5:	let Xi be a node in B containing no out-neighbor
6:	if Xi has no in-neighbor in B then
7:	set N to be a uni-leaf node representing PrD (Xi)
8:	else
9:	set N to be a factorize node with left child NL and right child NR
10:	set NR to be a split node
11:	for each value y of Xpa(i) in the CPT of Xi do
12:	add a multi-leaf node Ny as child of NR
13:	let Dy 一 {d ∈ D|Xpa(i)of d is y}
14:	let Ny represent PrDy (Xi)
15:	remove Xi from B to be B0
16:	call BN-TO-FSPN(B0, NL)
• Given a set of variables X and data D, if PrD (X) can be represented by a discrete BN B, we
can also build an FSPN F that equally represent PrD (X). Without ambiguity, we also use B to refer
to its DAG structure. We present the procedures in the BN-to-FSPN algorithm. It takes as inputs
a discrete BN B and a node N in F and outputs FN representing the PDF of B. We initialize F with
a root node N . Then, BN-TO-FSPN works in a recursive manner by executing the following steps:
①(lines 1-3) If B contains more than one connected component Bi, B?,..., Bt, the vari-
ables in each are mutually independent. Therefore, We set N to be a product node with children
N1, N2, . . . , Nt into F, and call BN-TO-FSPN on Bi and node Ni for each i.
②(lines 5-7) If B contains only one connected component, let Xi be a node (variable) in B that
has no out-neighbor. If Xi also has no in-neighbor (parent) in B, it maintains the PDF PrD(Xi). At
that time, we set N to be a uni-leaf representing the univariate distribution PrD(Xi).
(3) (IineS 9-16) If the parent set Xpa(i)of Xi is not empty, Xi has a conditional probability table
(CPT) defining PrD(Xi|Xpa(i)) = PrD(Xi |X \ {Xi}). At this time, we set N tobea factorize node
with the left child representing PrD(X \ {Xi}) and right child NR representing PrD (Xi |X \ {Xi}).
For the right child NR, we set it tobea split node. For each entry y of Xpa(i) in the CPTofXi, we add
a leaf Ly ofNR containing all data Dy in D whose value on Xpa(i) equals y. On each leafLy, by the
first-order Markov property of BN, Xi is conditionally independent of variables X \ {Xi } \ Xpa(i)
given its parents Xpa(i). Therefore, we can simplify the PDF represented by Ly as PrD(Xi|y) =
PrDy (Xi). Therefore, NR characterizes the CPT of PrD(Xi|Xpa(i)) = PrD(Xi|X \ {Xi}).
Later, we remove the node Xi from B to be B0, which represents the PDF PrD (X \ {Xi}). We
call BN-TO-FSPN on B0 and node NL, the left child of N to further model the PDF.
Finally, we obtain the FSPN F representing the same PDF ofB. Next, we analyze the model size of
B and F. The storage cost of each node Xi in B is the number of entries in CPT of PrD(Xi|Xpa(i)).
The FSPN F represents PrD(Xi) in step (2 when Xpa(i) is empty and PrD(Xi|y) for each value y
of Xpa(i) in step (3 . In the simplest case, if F also represents the distribution in a tabular form, the
storage cost is the same as B. Therefore, the model size ofF can not be larger than that ofB.
Consequently, proposition 1 holds.
13
Under review as a conference paper at ICLR 2021
Figure 3: An example of probability inference on FSPN.
Ranges and Probabilties of Events
Events	Xi ■	X2 I	X3	X4 I	Probability
E	[1,7]	[0, 20]	[3, 6]	[0, 20]	0.171
E	[1,7]	[0, 20]	[3, 5]	[0, 20]	0.051
E2	[1,7]	[0, 20]	(5, 6]	[0, 20]	0.12
B Example of Probability Inference on FSPN
We show an example of probability inference on the FSPN in Figure 1. This FSPN models the
joint PDF on four variables X1,X2,X3,X4. The two highly correlated attributes X1,X2 are first
separated from X3, X4 on node Ni. Pr(X3, X4) are decomposed by sum node (N2) and product
nodes (N4, N5). The uni-leaf nodes L1,L3 and L2,L4 models Pr(X3) and Pr(X4), respectively.
Pr(X1,X2∣X3,X4) are split into two regions by the value of X3 on node N3. On each region,
X1,X2 are independent of X3, X4 and modeled as multi-leaf nodes L5 and L6.
We assume that the domain of each variable is [0, 20]. We consider the example event E : Xi ∈
[1, 7],X3 ∈ [3,6], whose canonical form is E : Xi ∈ [1, 7],X2 ∈ [0,20], X3 ∈ [3, 6], X4 ∈ [0, 20].
We then obtain its probability on the FSPN by following the procedures step by step in Figure 3.
First, we consider the factorize root node Ni. The right child N3 splits the domain on whether X3
is greater than 5 to the two multi-leaf nodes L5 and L6 . The domains on L5 and L6 could be repre-
sented as Xi ∈ [0, 20], X2 ∈ [0, 20], X3 ∈ [0, 5], X4 ∈ [0, 20] and Xi ∈ [0, 20], X2 ∈ [0, 20], X3 ∈
(5, 20], X4 ∈ [0, 20], respectively. We divide the range of E into two parts by intersecting with the
domains of L5 and L6 as Ei : Xi ∈ [1, 7], X2 ∈ [0, 20], X3 ∈ [3, 5], X4 ∈ [0, 20] and E2 : Xi ∈
[1, 7], X2 ∈ [0, 20], X3 ∈ (5, 6], X4 ∈ [0, 20]. obviously, we have Pr(E) = Pr(Ei) +Pr(E2).
Second, we consider how to compute Pr(Ei) and Pr(E2). Within the range of Ei, the variables
X3, X4 is locally independent of the highly correlated variables Xi, X2, so we have Pr(Ei) =
Pr(X1,X2) ∙ Pr(X3, X4). Thejoint PDF of highly correlated variables X1,X2 is modeled together
by the multivariate leaf node L5, so we get the probability Pr(Xi ∈ [1, 7], X2 ∈ [0, 20]) = 0.3
from L5 . The joint PDF of variables X3 , X4 is modeled by the FSPN rooted at node N2, the left
child of Ni . We can obtain the probability in a similar way of SPN. Specifically, N2 is a sum
node, so we have Pr^ (X3 ∈ [3, 5],X4 ∈ [0,20]) = 0.3 ∙ Pr。(X3 ∈ [3,5], X4 ∈ [0, 20]) 十
0.7 ∙ PrN5 (X3 ∈ [3,5],X4 ∈ [0,20]). N4 is a product node, so we have P3(X3 ∈ [3, 5],X4 ∈
[0, 20]) = PrL1 (X3 ∈ [3,5])∙ P% (X4 ∈ [0,20]).Theprobability that PrL1 (X3 ∈ [3, 5])=0.1 and
PrL2 (X4 ∈ [0, 20]) = 1 could be obtained from the univariate leaf nodes Li and L2, respectively.
Hence, we get PrN4 (X3 ∈ [3, 5], X4 ∈ [0, 20]) = 0.1. The probability PrN5 (X3 ∈ [3, 5], X4 ∈
[0, 20]) = 0.2 could be obtained from leaf nodes L3 and L4 in the same way. As a result, we have
PrN2 (X3 ∈ [3, 5],X4 ∈ [0, 20]) =0.3 * 0.1+0.7 * 0.2 = 0.17 and Pr(EI) = 0.3 * 0.17 = 0.051.
Third, the probability of E2 could be computed in the same way as Ei . For E2, the probability
Pr(Xi ∈ [1, 7], X2 ∈ [0, 20]) = 0.4 is obtained from the multivariate leaf node L6, and the proba-
bility PrN2 (X3 ∈ (5, 6], X4 ∈ [0, 20]) = 0.3 is also computed by the FSPN rooted at node N2. We
have Pr(E2) = 0.4 * 0.3 = 0.12.
Finally, we obtain the probability of E as Pr(E) = Pr(Ei) + Pr(E2) = 0.171.
C Details of the Learn-FSPN Algorithm
In our implementation of the Learn-FSPN algorithm, we use the RDC score (Lopez-Paz et al.,
2013) as the independence test oracle since it can capture dependencies between variables of hybrid
domains. Two variables are identified to be independent and highly correlated if their RDC score
is lower than a threshold τL or larger than a threshold τH , respectively. In our experiment, we set
τL = 0.3 and τH = 0.7, respectively.
For the clustering method in line 17 for sum nodes, we use k-means, an EM method. For the partition
method in line 26 for split nodes, we design two methods as follows:
14
Under review as a conference paper at ICLR 2021

Algorithm LEARN-FSPN(D, X, C) 1:	if C = 0 then 2:	test the correlations cij for each pair of attributes Xi , Xj ∈ X 3:	H ―{Xi,Xj∣cij ≥ T} 4:	recursively enlarge H - H ∪ {Xk∣Cik ≥ τ,Xi ∈ H, Xk ∈ X \ H} 5:	if H 6= 0 then 6:	set N to be a factorize node 7:	call LEARN-FSPN(D, X \ H,0) 8:	call LEARN-FSPN(D, H, X \ H) 9:	else if |X | = 1 then 10:	set N to be a uni-leaf node 11:	model the univariate distribution PrD (X) 12:	else if subsets S1 , S2 , . . . , Sk are mutually indepedent by the independence oracle then 13:	set N to be a product node 14:	call LEARN-FSPN(D, Si, 0) for each 1 ≤ i ≤ k 15:	else 16:	set N to be a sum node 17:	let D1 , D2, . . . , Dt be the generated cluster of data with weights w1 , w2, . . . , wt 18:	Wi — ∣Ti∖∕∣TN| for all 1 ≤ i ≤ n 19:	call LEARN-FSPN(Di, X, 0) for each 1 ≤ i ≤ t 20:	else 21:	if X is independent of C on D by the independence oracle then 22:	set N to be a multi-leaf node 23:	model the multivariate distribution PrD (X) 24:	else 25:	set N to be a split node 26:	let D10 , D20 , . . . , Dt0 be the generated partition of data 27:	call LEARN-FSPN(Di0, X, C) for each 1 ≤ i ≤ t
•	Grid approximation of k-means: At first, we use the k-means method to cluster all data into
two clusters. By the properties of k-means, each clustering forms a hyper-spheroid in the space.
Let c1 and c2 be the central points and r1 and r2 by the radius of the two clusters, respectively.
Certainly, on the straight line across c1 and c2 in the space, there must exist two boundary points b1
and b2 of the two clusters. Let b be the mid-point of b1 and b2, we can split all data into two parts by
one dimension of b’s value. Some data points x in one cluster would be divided into the other part.
Hence, we choose the dimension of b’s value with minimal |x| as the splitting point.
•	Greedy splitting: Let c ∈ C be the variable that maximizes the pairwise correlations between
variables in X \ C and C. Intuitively, dividing the space by c would largely break the correlations
between X \ C and C. Then, we randomly choose d values c1 , c2, . . . , cd in the range of variable
c. For each value ci , we could divide all data into two parts. We compute the pairwise correlations
between variables in X \ C and C in each part. The value ci minimizes this value is chosen as the
splitting point.
Notice that, Learn-FSPN is a framework support different independent test oracles, clustering and
partition algorithms. The problem to design best plug-ins of these methods for a specific application
is still open. In our experiments, we have also tried some other kinds of independent test and
clustering methods. We find that using RDC scores, k-means clustering and greedy splitting methods
attains the best performance on our datasets.
D Additional Experimental Results
We present additional experimental results on the model size and training time of PGMs. Figure 4
shows the model size and training time of each PGM on the synthetic datasets. Table 2 gives the
detailed number of nodes in each model.
In terms of the model and number of nodes, we observe that:
•	The model and number of nodes of FSPNs are consistently much smaller than SPN-BTBs
and MSPNs by up to two orders of magnitude. In particular, the FSPNs’ model size is up to 27×
15
Under review as a conference paper at ICLR 2021
Average RDC Score
Figure 4: Model size and training time on synthetic data.
-B- BN-DAG T- BN-Tree - o - FSPN
f
O
I
.s
U
∙χ
二
Average RDC Score
Table 2: Number of nodes on synthetic data.
RDC Score of Dataset	# of nodes in MSPN	# of nodes in SPN-BTB	# of nodes in SPGM	# of nodes in BN-DAG/Tree	# of nodes in FSPN
0.07	21	21	21	20	21
0.21	765	562	321	20	168
0.37	1, 482	1, 175	472	20	375
0.65	3, 520	1, 626	481	20	437
0.83	3, 847	2, 753	718	20	661
0.95	4, 173	2, 398	563	20	33
and 116× smaller than MSPNs and SPN-BTBs, respectively. The FSPNs’ number of nodes is up to
126× and 73× smaller than MSPNs and SPN-BTBs, respectively. This is because tree-structured
SPNs may generate a large number of nodes in presence of highly correlated variables. SPN-BTBs
have lots of leaf nodes, and for each leaf node, they create a Chow-Liu tree. As a result, the space
cost of SPN-BTBs is the highest among all models.
•	The model size of FSPNs is also smaller than SPGMs and BN-DAGs. This is because they
require to store the CPTs over multiple variables, while FSPNs store the lightweight univariate distri-
butions and multivariate distributions on only highly correlated variables. In terms of the number of
nodes, FSPNs are also much smaller than SPGMs by up to 17×. The number of nodes in BN-DAGs
and BN-Trees always equals to the variable number so it is not informative to make a comparison.
•	The model size of BN-Tree is the smallest since the tree-width is only one for the learned
Chow-Liu tree.
In terms of the training time, we find that FSPNs are several times faster to train than other mod-
els, except BN-Trees. Whereas for other models (BN-DAGs, SPN-BTBs, SPGMs) related to BNs,
the structure learning process is time costly. MSPNs also require a longer training time since the
structure learning algorithm repeatedly splits nodes in presence of highly correlated variables.
In summary, these results verify our claims in Section 2. The results show that the design choices
underlying FSPNs, i.e. separating highly correlated variables from others and modeling them adap-
tively may represent the joint PDF in a more compact form and learn it efficiently.
16