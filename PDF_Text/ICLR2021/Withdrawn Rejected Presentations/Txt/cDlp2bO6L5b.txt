Under review as a conference paper at ICLR 2021
Double Double Descent: On Generalization
Errors in Transfer Learning between Linear
Regression Tasks
Anonymous authors
Paper under double-blind review
Ab stract
We study the transfer learning process between two linear regression problems.
An important and timely special case is when the regressors are overparameter-
ized and perfectly interpolate their training data. We examine a parameter trans-
fer mechanism whereby a subset of the parameters of the target task solution are
constrained to the values learned for a related source task. We analytically char-
acterize the generalization error of the target task in terms of the salient factors in
the transfer learning architecture, i.e., the number of examples available, the num-
ber of (free) parameters in each of the tasks, the number of parameters transferred
from the source to target task, and the correlation between the two tasks. Our non-
asymptotic analysis shows that the generalization error of the target task follows a
two-dimensional double descent trend (with respect to the number of free param-
eters in each of the tasks) that is controlled by the transfer learning factors. Our
analysis points to specific cases where the transfer of parameters is beneficial.
1 Introduction
Transfer learning (Pan & Yang, 2009) is a prominent strategy to address a machine learning task
of interest using information and parameters already learned and/or available for a related task.
Such designs significantly aid training of overparameterized models like deep neural networks (e.g.,
(Bengio, 2012; Shin et al., 2016; Long et al., 2017)), which are inherently challenging due to the vast
number of parameters compared to the number of training data examples. There are various ways
to integrate the previously-learned information from the source task in the learning process of the
target task; often this is done by taking subsets of parameters (e.g., layers in neural networks) learned
for the source task and plugging them in the target task model as parameter subsets that can be set
fixed, finely tuned, or serve as non-random initialization for a thorough learning process. Obviously,
transfer learning is useful only if the source and target tasks are sufficiently related with respect to
the transfer mechanism utilized, e.g., (Rosenstein et al., 2005; Zamir et al., 2018; Kornblith et al.,
2019). The importance of transfer learning in the contemporary common-practice should motivate
corresponding fundamental understanding of its main aspects via analytical frameworks that may
consider linear structures, e.g., (Lampinen & Ganguli, 2019).
In general, the impressive success of overparameterized architectures for supervised learning have
raised fundamental questions regarding the classical role of the bias-variance tradeoff that guided
the traditional designs towards seemingly-optimal underparameterized models (Breiman & Freed-
man, 1983). Recent empirical studies (Spigler et al., 2018; Geiger et al., 2019; Belkin et al., 2019a)
have demonstrated the phenomenon that overparameterized supervised learning corresponds to a
generalization error curve with a double descent trend (with respect to the number of parameters in
the learned model). This double descent shape means that the generalization error peaks when the
learned model starts to interpolate the training data (i.e., to achieve zero training error), but then the
error continuously decreases as the overparmeterization increases, often arriving to a global min-
imum that outperforms the best underparameterized solution. This phenomenon has been studied
theoretically from the linear regression perspective in an extensive series of papers, e.g., (Belkin
et al., 2019b; Hastie et al., 2019; Xu & Hsu, 2019; Mei & Montanari, 2019). The next stage is
to provide corresponding fundamental understanding to learning problems beyond a single fully-
1
Under review as a conference paper at ICLR 2021
supervised regression problem (see, e.g., the study of overparameterized linear subspace fitting in
unsupervised and semi-supervised settings (Dar et al., 2020)).
In this paper we study the fundamentals of the natural meeting point between overparameterized
models and the transfer learning concept. Our analytical framework is based on the least squares
solutions to two related linear regression problems: the first is a source task whose solution has
been found independently, and the second is a target task that is addressed using the solution already
available for the source task. Specifically, the target task is carried out while keeping a subset of
its parameters fixed to values transferred from the source task solution. Accordingly, the target task
includes three types of parameters: free to-be-optimized parameters, transferred parameters set fixed
to values from the source task, and parameters fixed to zeros (which in our case correspond to the
elimination of input features). The mixture of the parameter types defines the parameterization level
(i.e., the relation between the number of free parameters and the number of examples given) and the
transfer-learning level (namely, the portion of transferred parameters among the solution layout).
We conduct a non-asymptotic statistical analysis of the generalization errors in this transfer learning
structure. Clearly, since the source task is solved independently, its generalization error follows a
regular (one-dimensional) double descent shape with respect to the number of examples and free
parameters available in the source task. Hence, our main contribution and interest are in the charac-
terization of the generalization error of the target task that is carried out using the transfer learning
approach described above. We show that the generalization error of the target task follows a dou-
ble descent trend that depends on the double descent shape of the source task and on the transfer
learning factors such as the number of parameters transferred and the correlation between the source
and target tasks. We also examine the generalization error of the target task as a function of two
quantities: the number of free parameters in the source task and the number of free parameters in the
target task. This interpretation presents the generalization error of the target task as having a two-
dimensional double descent trend that clarifies the fundamental factors affecting the performance
of the overall transfer learning approach. Specifically, we show how the generalization error of the
target task is affected by the interaction among the source-target task relation, the form of the true
solution, and the specific set of transferred parameters. Moreover, we analytically characterize the
cases where transfer of parameters is preferred over constraining them to be zero valued (where the
latter is equivalent to omitting features).
This paper is organized as follows. In Section 2 we define the transfer learning architecture exam-
ined in this paper. In Sections 3-4 we present the analytic and empirical results that characterize
the generalization errors of the target task, and outline the cases where transfer of parameters is
beneficial. Note that Section 3 studies a simplified setting where transferred parameters are chosen
uniformly at random, whereas Section 4 examines transfer of specific (i.e., not random) sets of pa-
rameters. Section 5 concludes the paper. The Appendix includes all of the proofs and mathematical
developments as well as additional details and results for the empirical part of the paper.
2	Transfer Learning between Linear Regression Tasks: Problem
Definition
2.1	Source Task: Data Model and Solution Form
We start with the source data model, where a d-dimensional Gaussian input vector Z 〜 N (0, Id) is
connected to a response value v ∈ R via the noisy linear model
v = zTθ+ξ,
(1)
where ξ 〜N(0, σg) is a Gaussian noise component independent of z, and θ ∈ Rd is an unknown
vector. The data user is unfamiliar with the distribution of (z, v), however gets a dataset of ne inde-
ne
pendent and identically distributed (i.i.d.) draws of (z, V) pairs denoted as De，{(z(i),v(i))}
i=1
The ne data samples can be rearranged as Z , [z(1), . . . , z(en)]T and v , [v(1), . . . , v(en)]T that sat-
isfy the relation v = Zθ + ξ where ξ , [ξ(1), . . . , ξ(ne)]T is an unknown noise vector that its ith
component ξ(i) participates in the relation v(i) = z(i),T θ + ξ(i) underlying the ith data sample.
2
Under review as a conference paper at ICLR 2021
The source task is defined for a new (out of sample) data pair z(test) , v(test) drawn from the
distribution induced by (1) independently of the ne examples in De. For a given z(test) , the source
task is to estimate the response value v(test) by the value vb that minimizes the corresponding out-of-
sample squared error (i.e., the generalization error of the source task)
Eeout , Envb - v(test)2o = σξ2 + Enθb - θ2o	(2)
where the second equality stems from the data model in (1) and the corresponding linear form of
vb = z(test),T θb where θb estimates θ based on De.
To address the source task based on the ne examples, one should choose the number of free param-
eters in the estimate θb ∈ Rd. Consider a predetermined layout where pe out of the d components
of θ are free to be optimized, whereas the remaining d - pe components are constrained to zero
values. The coordinates of the free parameters are specified in the set S , {s1, . . . , spe} where
1 ≤ si < … < sp ≤ d and the complementary set SC，{1,... ,d}∖S contains the coordinates
constrained to be zero valued. We define the ∣S∣×d matrix QS as the linear operator that extracts
from a d-dimensional vector its |S |-dimensional subvector of components residing at the coordi-
nates specified in S. Specifically, the values of the (k, sk) components (k = 1, . . . , |S|) of QS are
ones and the other components of QS are zeros. The definition given here for QS can be adapted
also to other sets of coordinates (e.g., QSc for Sc) as denoted by the subscript of Q. We now turn
to formulate the source task using the linear regression form of
2
θ = arg min kv - Zrk2 subject to QScr = 0	(3)
r∈Rd
that its min-norm solution (see details in Appendix A.1) is
θb= QTSZS+v	(4)
where ZS+ is the pseudoinverse of ZS , ZQTS. Note that ZS is a ne × pe matrix that its ith row is
formed by the pe components of z(i) specified by the coordinates in S, namely, only pe out of the
d features of the input data vectors are utilized. Moreover, θ is a d-dimensional vector that may
have nonzero values only in the pe coordinates specified in S (this can be easily observed by noting
that for an arbitrary w ∈ R|S| , the vector u = QTSw is a d-dimensional vector that its components
satisfy usk = wk for k = 1, ..., |S| and uj = 0 for j ∈/ S). While the specific optimization form in
(3) was not explicit in previous studies of non-asymptotic settings such as (Breiman & Freedman,
1983; Belkin et al., 2019b), the solution in (4) coincides with theirs and, thus, the formulation of
the generalization error of our source task (which is a linear regression problem that, by itself, does
not have any transfer learning aspect) is available from (Breiman & Freedman, 1983; Belkin et al.,
2019b) and provided in Appendix A.2 in our notations for completeness of presentation.
2.2	Target Task: Data Model and Solution using Transfer Learning
A second data class, which is our main interest, is modeled by (x, y) ∈ Rd × R that satisfy
y = xTβ+	(5)
where X 〜N (0, Id) is a Gaussian input vector including d features, E 〜N(0, σ2) is a Gaussian
noise component independent of x, and β ∈ Rd is an unknown vector related to the θ from (1) via
θ = Hβ + η
(6)
where H ∈ Rd×d is a deterministic matrix and η 〜N(0, σ%Id) is a Gaussian noise vector. Here
η, X, E, z and ξ are independent. The data user does not know the distribution of (X, y) but re-
ceives a small dataset of n i.i.d. draws of (X, y) pairs denoted as D
{(χ(i),y(i)) }：=1.
The n
data samples can be organized in a n × d matrix of input variables X , [χ(1), . . . , χ(n)]T and a
n × 1 vector of responses y , [y(1), . . . , y(n)]T that together satisfy the relation y = Xβ + where
, [E(1), . . . , E(n)]T is an unknown noise vector that its ith component E(i) is involved in the con-
nection y(i) = χ(i),T β + E(i) underlying the ith example pair.
3
Under review as a conference paper at ICLR 2021
The target task considers a new (out of sample) data pair x(test), y(test) drawn from the model in
(5) independently of the training examples in D. Given x(test), the goal is to establish an estimate yb
of the response value y(test) such that the out-of-sample squared error, i.e., the generalization error
of the target task,
Eout,Enyb-y(test)2o = σ2 + Enβb - β2o	(7)
is minimized, where yb = x(test),T βb and the second equality stems from the data model in (5).
The target task is addressed via linear regression that seeks for an estimate βb ∈ Rd with a layout
including three disjoint sets of coordinates F, T, Z that satisfy F ∪ T ∪ Z = {1, . . . , d} and corre-
spond to three types of parameters:
•	p parameters are free to be optimized and their coordinates are specified in F.
•	t parameters are transferred from the co-located coordinates of the estimate θ already
formed for the source task. Only the free parameters of the source task are relevant to
be transferred to the target task and, therefore, T ⊂ S and t ∈ {0, . . . , pe}. The transferred
parameters are taken as is from θ and set fixed in the corresponding coordinates of β, i.e.,
th
for k ∈ T, βk = θk where βk and θk are the kth components of β and θ, respectively.
•	` parameters are set to zeros. Their coordinates are included in Z and effectively correspond
to ignoring features in the same coordinates of the input vectors.
Clearly, the layout should satisfy p + t + ` = d. Then, the constrained linear regression problem for
the target task is formulated as
βb = arg min ky - Xbk2	(8)
b∈Rd
subject to QT b = QT θ
QZb = 0
where QT and QZ are the linear operators extracting the subvectors corresponding to the coor-
dinates in T and Z, respectively, from d-dimensional vectors. Here bθ ∈ Rd is the precomputed
estimate for the source task and considered a constant vector for the purpose of the target task. The
examined transfer learning structure includes a single computation of the source task (3), followed
by a single computation of the target task (8) that produces the eventual estimate of interest β using
the given θ. The min-norm solution of the target task in (8) is (see details in Appendix A.3)
βb=QTFXF+ y-XTθbT + QTT θbT	(9)
where θbT , QT θb, XT , XQTT , and XF+ is the pseudoinverse of XF , XQTF. Note that the
desired layout is indeed implemented by the β form in (9): the components corresponding to Z
are zeros, the components corresponding to T are taken as is from θ, and only the p coordinates
specified in F are adjusted for the purpose of minimizing the in-sample error in the optimization
cost of (8) while considering the transferred parameters. In this paper we study the generalization
ability of overparameterized solutions (i.e., when P > n) to the target task formulated in (8)-(9).
3	Transfer of Random Sets of Parameters
To analytically study the generalization error of the target task we consider, in this section, the
overall layout of coordinate subsets L , {S, F, T, Z} as a random structure. The simplified set-
tings of this section provide useful insights towards Section 4 where we analyze transfer of specific,
non-random sets of parameters.
Definition 3.1. A coordinate subset layout L = {S, F, T, Z} that is {pe, p, t}-uniformly distributed,
for pe ∈ {1, . . . , d} and (p, t) ∈ {0, . . . , d} × {0, . . . ,pe} such that p + t ≤ d, satisfies: S is uni-
formly chosen at random from all the subsets of pe unique coordinates of {1, . . . , d}. Given S, the
target-task coordinate layout {F, T, Z} is uniformly chosen at random from all the layouts where
F, T, and Z are three disjoint sets of coordinates that satisfy F ∪ T ∪ Z = {1, . . . , d} such that
|F|= p, |T|= t and T ⊂ S , and |Z |= d - p - t.
4
Under review as a conference paper at ICLR 2021
Recall the relation between the two tasks as provided in (6) and let us denote β(H) , Hβ. The
following definitions emphasize crucial aspects in the examined transfer learning framework. The
signal-to-noise ratio is defined for the source task as Γsrc，kβ!22+dση, and for the target task as
Γtgt，kβk2. The normalized task correlation between the two tasks is P，1忸(£；2；； ?. The task
energy ratio between the two tasks is ω，kβ Jj+d*.
The following characterizes the expected out-of-sample error of the target task with respect to trans-
fer of uniformly-distributed sets of parameters 1.
Theorem 3.1. Let L = {S, F, T, Z} be a coordinate subset layout that is {pe, p, t}-uniformly dis-
tributed. Then, the expected out-of-sample error of the target task has the form of
kβ"2 ( n---ι	(d	- P + d ∙ r-gt	+ t ∙	δEtransfer)	for P ≤ n -	2,
EL {Eout} =	-d^2 ∙ ∖ p-n-ι	(d	- P + d ∙ r-gt	+ t ∙	δEtransfer)	+ P - n for P ≥ n +	2,	(IO)
(∞	otherwise.
where
(I- 2ρ + d-e⅛Γr1 ɪ	for P ≤ n - 2,
∆Etransfer = 3 × I ∣(1 - 2ρ + d-⅞-Γ-r1 ) for P ≥ n +2,	(11)
(∞	otherwise.
is the expected error difference introduced by each constrained parameter that is transferred from
the source task instead of being set to zero.
The last theorem is proved using non-asymptotic properties of Wishart matrices (see Appendix B).
Figure 1 presents the curves
of EL Eout
with respect to the number of free parameters P in the
target task, whereas the source task has Pe = d free parameters. In Fig. 1, the solid-line curves
correspond to analytic values induced by Theorem 3.1 and the respective empirically computed
values are denoted by circles (all the presented results are for d = 80, ne = 50, kβk22 = d, σξ2 =
0.025 ∙ d. See additional details in Appendix D). The number of free parameters P is upper bounded
by d - t that gets smaller for a larger number of transferred parameters t (see, in Fig. 1, the earlier
stopping of the curves when t is larger). Observe that the generalization error peaks at P = n and,
then, decreases as P grows in the overparameterized range ofP > n + 1. We identify this behavior
as a double descent phenomenon, but without the first descent in the underparameterized range (this
was also the case in settings examined in (Belkin et al., 2019b; Dar et al., 2020)).
By considering the generalization error formula from Theorem 3.1 as a function ofPeandP (i.e., the
number of free parameters in the source and target tasks, respectively) we receive a two-dimensional
double descent behavior as presented in Fig. 2 and its extended version Fig. 5 where each subfigure is
for a different pair oft and ση2. The results show a double descent trend along theP axis (with a peak
atP = n) and also, when parameter transfer is applied (i.e., t > 0), a double descent trend along the Pe
axis (with apeakatPe= ne). Definition 3.1 implies that Pe ∈ {t, . . . , d} andP ∈ {0, . . . , d-t}, hence,
a larger number of transferred parameters t eliminates a larger portion of the underparameterized
range of the source task and also eliminates a larger portion of the overparameterized range of the
target task (see in Fig. 5 the white eliminated regions that grow with t). When t is high, the wide
elimination of portions from the (Pe, P)-plane hinders the complete form of the two-dimensional
double descent phenomenon (see, e.g., Fig. 2d).
An increased transfer of parameters limits the level of overparameterization applicable in the target
task and, in turn, the potential gains from the transfer learning are also limited. Yet, when the
source task is sufficiently related to the target task (see, e.g., Figures 1a, 1b), the parameter transfer
1Our expectation over uniformly random choice of coordinates (together with a deterministic β) can be
interpreted instead as considering fixed indices and β modeled as isotropic random vector and H = αId
for some real value α. This alternative interpretation relates to settings used for linear regression analysis in
Section 4 of Hastie et al. (2019) and in Xu & Hsu (2019). Also, note that in our Section 4 we formulate the
generalization error (without expectation over L) for a specific L and a specific choice of parameters.
5
Under review as a conference paper at ICLR 2021
250
4 200
P
1 150
ω
R 100
H
50
250
4 200
P
1 150
ω
向100
H
50
20	40	60	80	20	40	60	80
Number of parameters (P) Number of parameters (P)
(a)	(b)
20	40	60	80
Number of parameters (p)
(d)
Figure 1: The expected generalization error of the target task, EL Eout , with respect to the num-
ber of free parameters (in the target task). The analytical values, induced from Theorem 3.1, are
presented using solid-line curves, and the respective empirical results obtained from averaging over
250 experiments are denoted by circle markers. Each subfigure considers a different case of the
relation (6) between the source and target tasks in the form of a different ση2 value, whereas H = Id
for all. Each curve color refer to a different number of transferred parameters.
60
Id 40
20
Analytical:蟾=0.5 t =0
80
400d 40
200 20
60
(a)
600 60
20	40
P
0
80
800 80
800
600
400
200
60
80
60
80
60
(b)
(d)
(c)
20	40
P
20	40
P
20	40
P
0
80
Figure 2: Analytical evaluation of the expected generalization error of the target task, EL Eout ,
with respect to the number of free parameters pe and p (in the source and target tasks, respectively).
Each subfigure considers a different number of transferred parameters t. The white regions corre-
spond to (pe, p) settings eliminated by the value of t in the specific subfigure. The yellow-colored
areas correspond to values greater or equal to 800. All subfigures are for ση2 = 0.5 and H = Id. See
Fig. 5 for settings with different values of ση2 . See Fig. 6 for the corresponding empirical evaluation.
compensates, at least partially, for an insufficient number of free parameters (in the target task). The
last claim is evident in Figures 1a, 1b where, for p > n + 1, there is a range of generalization error
values that is achievable by several settings of (p, t) pairs (i.e., specific error levels can be attained by
curves of different colors in the same subfigure). E.g., for ση2 = 0.5 (Fig. 1b), the error achieved by
p = 60 free parameters and no parameter transfer can be also achieved using p = 48 free parameters
and t = 32 parameters transferred from the source task. In Appendix C we elaborate on the two
special cases that are induced by setting t = 0 or p = 0 in the general result of Theorem 3.1.
Let us return to the general case of Theorem 3.1 and examine the expected generalization error for a
given number of free parameters p in the target task. We form analytical conditions on the required
number of free parameters pe in the source task to get a useful parameter transfer that reduces the
generalization error of the target task compared to the option of setting parameters to zero.
Corollary 3.1. Let p ∈	{0, .	. .	, d}	and ne	be given quantities. Consider	pe	∈	{1,	. . .	, d}.	Then,
the term ∆Etransfer, which quantifies the expected error difference due to each parameter being
transferred instead of set to zero, satisfies ∆Etransfer < 0 (i.e., parameter transfer is beneficial) if
the source task is sufficiently overparameterized such that
Pe > n + ι + Id — n + d ∙ r-rC — ι) /2p
or sufficiently underparameterized such that
p < n—1—
d — n + d ∙ ΓsrC + 1
2 (P -I)
or
ne — 1 —
d — n + d ∙ Γ-rC + 1
2 (P -I)
< pn ≤ d
or 1 ≤ pn ≤ d
for	P	> 0,	nn <	d	—	1,	(12)
for	P> 1,	n ≤	d	(1	+	Γsr1)	+	1,	(13)
for	0	≤ ρ < 1,	n	>	d(1 + ΓsT1) + 1,	(14)
for	ρ	≥ 1,	n >	d(1	+	Γsr1)	+	1.	(15)
6
Under review as a conference paper at ICLR 2021
AnalytiCal ʌ^transfer ∙ H = Id
说
(a)	(b)	(c)
Figure 3: The analytical values of ∆Etransfer defined in Theorem 3.1 for a random coordinate layout
(namely, the expected error difference due to transfer of a parameter from the source to target task)
as a function of pe and ση2 . The positive and negative values of ∆Etransfer appear in color scales
of red and blue, respectively. The regions of negative values correspond to beneficial transfer of
parameters. The positive values were truncated at 2 for the clarity of visualization. The solid black
lines denote the analytical thresholds for useful transfer learning as implied by Corollary 3.1. Each
subfigure refers to another task relation model induced by H = 11d, H = Id, H = 3 Id.
Otherwise, ∆Etransfer ≥ 0 (i.e., parameter transfer is not beneficial).
The proof of the last corollary is provided in Appendix E.1. The analytical thresholds for useful
transfer learning are demonstrated by the black lines in Fig. 3. Importantly, Figures 7d-7f show
that the analytical thresholds excellently match the regions where the empirical settings yield useful
parameter transfer (i.e., where ∆Etransfer < 0 is empirically satisfied). The details on the empirical
computation of ∆Etransfer are available in Appendix E.2. Fig. 3 considers settings where ne < d - 1
and, thus, refers to the analytical conditions formulated in (12)-(13). See Appendix E.3 for empirical
results for settings where ne > d that correspond to the conditions in (14)-(15).
4 Transfer of a Specific Set of Parameters
Equipped with the fundamental analytical understanding of the key principles formulated in the
former section for the case of uniformly-distributed coordinate layout, we now proceed to the
analysis of settings that consider a specific non-random layout of coordinates L = {S, F, T, Z}.
The following theorem (see proof in Appendix F) formulates the generalization error of the tar-
get task that is solved using a specific set of transferred parameters indicated by the coordi-
nates in T. First, we define the next quantities with respect to the specific coordinate lay-
out used: The possibly-transferred to actually-transferred energy ratio of the source task is
ψτ, (P 旭) 12+tση) / (IBF112+tση).
The zeroed-to-transferred energy ratio of the
source task φT
+ tση2 / IIIβ(TH)III22 + tση2
. The constrained-coordinates SNR
is defined for the source task as Γsrc,Sc , IIIβ(SHc) III + (d - pe)ση2 /σξ2, and for the target
task as Γtgt,FC，∣∣βFC ∣∣2/σ2. The normalized task correlation in T between the two tasks
isρT,hβ(TH),βTi/ IIIβ(TH)III22 + tση2
ωT , IIIβ(TH) III22 + tση2 /∣βFC ∣22.
The task energy ratio in T between the two tasks is
Theorem 4.1. Let L = {S , F, T, Z} be a specific, non-random coordinate subset layout. Then,
the out-of-sample error of the target task has the form of
n-1	1	+	Γ-1	+ ∆E(T，S)	kβ< k2	for n	≤	n	—	2
n-p-1	11	+ γ tgt,Fc	+AEtransfer	kβFc ∣∣2	forP	≤	n	2，
p⅛	(1	+ Γgt,Fc+∆Et(TnSfejkeFC k2	+	p-n	kβ F k2	for P	≥	n	+	2,	(16)
∞
otherwise.
7
Under review as a conference paper at ICLR 2021
Number of parameters (p)	Number of parameters (p)
(a) Sparse β
(b) Sparse β
Figure 4: Analytical (solid lines) and empirical (circle markers) values of Eo(uLt) for specific, non-
random coordinate layouts. All subfigures use the same sequential evolution of L with p. Here
ση2 = 0.5. See Figures 10-11 for the complete set of results.
Number of parameters (p)
(c) Linear-shape β
Number of parameters (P)
(d) Linear-shape β
where
{1 ɔ l φt(d-e)(ι+r-.ι Sc)	£ 一〜 o
1 - 2ρτ +n-p-ι ,S	forP ≤ n - 2,
n ( Qe2-nρ)ψτ+ne-1	2 m + φτ 5-pX1+r-∙1,sc) ʌ	foe + 2
p I	p2-1	- 2ρτ +	p-n-ι	forP ≥ n + 2,
∞	otherwise.
is the error difference introduced by transferring from the source task the parameters specific in T
instead of setting them to zero.
Figures 4,10,11 show the curves of Eo(uLt), for specific coordinate layouts L that evolve with respect
to the number of free parameters P in the target task. The excellent fit of the analytical results to the
empirical values is evident. The effect of the specific coordinate layout utilized is clearly visible by
the less-smooth curves (compared to the on-average results for random coordinate layouts in 1). We
examine two different cases for the true β (a linearly-increasing and a sparse layout of values, with
the same norm, see Appendix G) and the resulting error curves significantly differ despite the use of
the same sequential construction of the coordinate layouts with respect to P (see, e.g., Figs. 4a,4c).
The linear operator H in the task relation model greatly affects the generalization error curves as
evident from comparing our results for different types ofH: an identity, local averaging, and discrete
derivative operators (see, e.g., Fig. 4a vs. Fig. 4b, Fig. 4c vs. Fig. 4d, and also the complete set of
results in Figs. 10-11). Theorem 4.1 yields the following rule on the benefits from transfer learning.
Corollary 4.1. Let F and S be given. Then, the parameter transfer induced by a specific
T ⊂ {FC ∩ S} is beneficial if ∆Et(rTan,Ssf)er < 0. Otherwise, this specific parameter transfer is not
beneficial over zeroing the parameters (i.e., omitting the input features) corresponding to T.
The last corollary is an extension of the simplified case for uniformly-distributed coordinate layouts
as formulated in Corollary 3.1. Our results also exhibit that a specific set T of t transferred parame-
ters can be the best setting for a given set F ofP free parameters but not necessarily for an extended
set F0 ⊃ F of P0 > P free parameters (see, e.g., Fig. 4b where the orange and red colored curves do
not consistently maintain their relative vertical order at the overparameterized range of solutions).
5 Conclusions
In this work we have established an analytical framework for the fundamental study of transfer
learning in conjunction with overparameterized models. We used least squares solutions to linear
regression problems for shedding clarifying light on the generalization performance induced for a
target task addressed using parameters transferred from an already completed source task. We for-
mulated the generalization error of the target task and presented its two-dimensional double descent
shape as a function of the number of free parameters individually available in the source and tar-
get tasks. We characterized the conditions for a beneficial transfer of parameters and demonstrated
the effects of various crucial aspects such as the source-target task relation, the specific choice of
transferred parameters and its interaction with the form of the true solution. We believe that our
work opens a new research direction for the fundamental understanding of the generalization ability
of transfer learning designs. Future work may study additional transfer learning layouts (like fine
tuning of the transferred parameters) and their implications to practical neural network architectures.
8
Under review as a conference paper at ICLR 2021
References
M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the
classical bias-variance trade-off. Proceedings of the National Academy of Sciences, 116(32):
15849-15854, 2019a.
M. Belkin, D. Hsu, and J. Xu. Two models of double descent for weak features. arXiv preprint
arXiv:1903.07571, 2019b.
Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In ICML work-
shop on unsupervised and transfer learning, pp. 17-36, 2012.
L.	Breiman and D. Freedman. How many variables should be entered in a regression equation?
Journal of the American Statistical Association, 78(381):131-136, 1983.
Y. Dar, P. Mayer, L. Luzi, and R. G. Baraniuk. Subspace fitting meets regression: The effects of
supervision and orthonormality constraints on double descent of generalization errors. In Inter-
national Conference on Machine Learning (ICML), 2020.
M.	Geiger, A. Jacot, S. Spigler, F. Gabriel, L. Sagun, S. d’Ascoli, G. Biroli, C. Hongler, and
M. Wyart. Scaling description of generalization with number of parameters in deep learning.
arXiv preprint arXiv:1901.01608, 2019.
T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless
least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
F. Hiai and D. Petz. Asymptotic freeness almost everywhere for random matrices. Acta Sci. Math.
Szeged, 66:801-826, 2000.
S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In IEEE conference
on computer vision and pattern recognition (CVPR), pp. 2661-2671, 2019.
A. K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning
in deep linear networks. In International Conference on Learning Representations (ICLR), 2019.
M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep transfer learning with joint adaptation networks.
In International Conference on Machine Learning (ICML), pp. 2208-2217, 2017.
S. Mei and A. Montanari. The generalization error of random features regression: Precise asymp-
totics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
S. J. Pan and Q. Yang. A survey on transfer learning. IEEE transactions on knowledge and data
engineering, 22(10):1345-1359, 2009.
M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G. Dietterich. To transfer or not to transfer. In
NIPS workshop on transfer learning, 2005.
H.-C. Shin, H. R. Roth, M. Gao, L. Lu, Z. Xu, I. Nogues, J. Yao, D. Mollura, and R. M. Summers.
Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset
characteristics and transfer learning. IEEE transactions on medical imaging, 35(5):1285-1298,
2016.
S. Spigler, M. Geiger, S. d’Ascoli, L. Sagun, G. Biroli, and M. Wyart. A jamming transition
from under-to over-parametrization affects loss landscape and generalization. arXiv preprint
arXiv:1810.09665, 2018.
A. M. TUlino and S. Verdu. Random matrix theory and wireless communications. Now Publishers
Inc, 2004.
J. Xu and D. J. Hsu. On the number of variables to use in principal component regression. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 5095-5104, 2019.
A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling
task transfer learning. In IEEE conference on computer vision and pattern recognition (CVPR),
pp. 3712-3722, 2018.
9
Under review as a conference paper at ICLR 2021
Appendices
The following appendices supports the main paper as follows. Appendix A provides additional
details on the mathematical developments leading to the results in Section 2 of the main paper. In
Appendix B we present the proof of Theorem 3.1 that formulates the expected generalization error
of the target task in the setting of uniformly-distributed coordinate layouts. Appendix C includes
the explicit formulations of two special cases of Theorem 3.1 that were discussed in Section 3 of
the main paper. Appendix D provides additional empirical results and details for Section 3 of the
main paper. In Appendix E we provide the proof of Corollary 3.1 as well as related analytical and
empirical evaluations and details. In Appendices F-G we refer to the setting of transfer of a specific
set of parameters from Section 4 of the main text. Specifically, in Appendix F we prove Theorem
4.1 and in Appendix G we provide additional analytical and empirical evaluations.
A	Mathematical Developments for Section 2
，一 —一 ^ „
A.1 THE ESTIMATE θ IN EQ. (4)
Let us solve the optimization problem provided in (3). Using the relation QTS QS + QTScQSc = Id
we can rewrite (3) as
^	_	_	_	_	..9
θb = arg min kv - ZSQSr - ZScQScrk2	(17)
r∈Rd
subject to QScr = 0
where ZS , ZQTS and ZSc , ZQTSc . By setting the equality constraint in the optimization cost,
the problem in (17) becomes
^	_	_	..9	. . _ .
θb = arg min kv - ZS QS rk2	(18)
r∈Rd
subject to QScr = 0.
Without the equality constraint, (18) is just an unconstrained least squares problem that its solution
is
θb=QTSZS+v	(19)
where ZS+ is the Moore-Penrose pseudoinverse of ZS. Note that θb in (19) satisfies the equality
constraint in (17) and, therefore, (19) is also the solution for the constrained optimization problems
in (17), (18), and (3).
A.2 The Double Descent Formulation for the Generalization Error of the
Source Task
~ J(1+
Eout =	∞
lfl +
The generalization error ofa single linear regression problem (that includes noise) in non-asymptotic
settings is provided in (Belkin et al., 2019b) for a given coordinate subset (i.e., deterministic S in
our terms). The result of (Belkin et al., 2019b) can be written in our notations as
e-e-1 )(kθsc k2 + σξ)	fore ≤ n - 2,
for pe ∈ {ne - 1, ne, ne + 1},	(20)
e⅛1) (kθsck2 + σξ) + kθsk2(1- e)	fore ≥ e + 2.
The analysis in our work considers the coordinate subset S to be uniformly chosen at random
from all the subsets of ee ∈ {1, . . . , d} unique coordinates of {1, . . . , d}. Then, we get that
ES{k°sk20 = e ∣∣θ∣∣2 and ES{∣∣θsc∣∣2} = d-e I∣θk2∙ Accordingly, the expectation over S of
the generalization error of the source task leads to the following result
〜	f(l + ⅛) (d - e + d ∙ esr1)	for e ≤ e - 2,
ESnEeoutO = d kθk21 ∞	fore∈{e - ι,n,n +1},
[(l + e-e-1) (d - e + d ∙ esrc-1) + e - e for e ≥ n + 2,
(21)
10
Under review as a conference paper at ICLR 2021
kθk2
where Γsrc = ɪ^2. Note that the expectation of Γsrc With respect to the noise vector η yields the
σξ
7'∙	. ∙ . .1	1 t' IFC EI	C <	, 1	∙	.
Γsrc quantity that was defined before Theorem 3.1 in the main text.
._ _	_	^	_ . . .
A.3 THE ESTIMATE β IN EQ. (10)
The optimization problem in (8), given for the target task, can be addressed using the relation
QTF QF + QTT QT + QTZ QZ = Id and rewritten as
βb = arg min ky - XFQFb - XT QTb - XZ QZ bk2	(22)
b∈Rd
subject to QT b = QTθ
QZb=0
where XF , XQTF, XT , XQTT , and XZ , XQTZ. By setting the equality constraints of (22) in
its optimization cost, the problem (22) can be translated into the form of
βb = arg min y - XT QT bθ - XF QF b	(23)
b∈Rd	2
..	ci	C 公
subject to QT b = QT θ
QZb = 0.
The last optimization is a restricted least squares problem that can be solved using the method of
Lagrange multipliers to show that
βb=QTFXF+ y-XTθbT + QTT θbT	(24)
where θbT , QT θb and XF+ is the Moore-Penrose pseudoinverse of XF.
B Proof of Theorem 3.1
This section is organized as follows. Section B.1 presents auxiliary results on uniformly-distributed
coordinate subsets. Section B.2 provides results on non-asymptotic properties of Gaussian and
Wishart matrices. The auxiliary results are utilized in Section B.3 to prove the formulations given
in Theorem 3.1 for the generalization error of the target task in the setting of uniformly-distributed
coordinate subset layout.
B.1	Auxiliary Results on the Uniformly-Distributed Coordinate Subset
Layout
Recall Definition 3.1 in the main text that characterizes a coordinate subset layout L = {S, F, T, Z}
that is {pe, p, t}-uniformly distributed, for pe ∈ {1, . . . , d} and (p, t) ∈ {0, . . . , d} × {0, . . . ,pe} such
that p + t ≤ d. Here we provide several auxiliary results that are induced by this random structure
and utilized in the proof of Theorem 3.1.
For S that is uniformly chosen at random from all the subsets of pe unique coordinates of {1, . . . , d},
we get that the mean of the projection operator QTS QS is
(¢-1)	〜
El { QT Qs } = ES {QT Qs } = ⅛p Id =三 Id	(25)
pe
where we used the structure of QTs Qs that is a d×d diagonal matrix with its jth diagonal component
equals 1 if j ∈ S and 0 otherwise.
Definition 3.1 also specifies that, given S, the target-task coordinate layout {F, T, Z} is uniformly
chosen at random from all the layouts where F , T, and Z are three disjoint sets of coordinates
that satisfy F ∪ T ∪ Z = {1, . . . ,d} such that |F|= p, |T|= t and T ⊂ S, and |Z|= d - p- t.
Accordingly,
(e-i)	t	t
El{ QT Qt} = Es{ El∣s {qT Qt}} = ɪ Es{ QT Qs} = ⅛{ QT Qs} = - Id, (26)
pt	p	d
11
Under review as a conference paper at ICLR 2021
and similarly
El{qF Qf O = d Id,	(27)
El{qZQzO = d - P - tId.	(28)
Another useful auxiliary result, based on the relation QS QTS = Ipe (carefully note the transpose
appearance), is provided by
El{Qs QT Qt QT O = ES {Qs El∣s {qT Qt }qT } = ； ES {Qs QT QS QT }=；片.(29)
The results in (26)-(28) imply that
Ec{kβτk2}	= βTEc{QTQt}β	=	d kβk2,	(30)
Ec{kβFk2}	= βTEc{QFQf}β	=	d kβk2,	(31)
Ec{kβzk2}	= βTEc{QZQz}β	=	d -d - t	kβk2,	(32)
where βT , QTβ, βF , QFβ, and βz , Qzβ. Note that the expressions in (30)-(32) hold also
for d-dimensional deterministic vectors other than β, e.g., (30)-(32) hold for β(H) , Hβ.
B.2 Auxiliary Results using Non-Asymptotic Properties of Gaussian and
Wishart Matrices
The random matrix XF , XQTF is of size n × d and all its components are i.i.d. standard Gaussian
variables. Then, almost surely,
EnXFXFo=Ip × (P ford >n,	⑸)
where XF+XF is the d × d projection operator onto the range of XF. Accordingly, let a ∈ Rp be a
random vector independent of the matrix XF and, then,
EnIlXFXFa俏o=Enkak2} × (n ford>n,	⑦
Since the components of XF are i.i.d. standard Gaussian variables then XFXF 〜WP (Ip, n) is a
d X d Wishart matrix with n degrees of freedom, and XF XF 〜 Wn (In,d) is a n X n Wishart matrix
with d degrees of freedom. The pseudoinverse of the n × n Wishart matrix (almost surely) satisfies
{n⅛ ∙ P
∞
p—n—1
for d ≤ n - 2,
for d ∈ {n - 1, n, n + 1},	(35)
for d ≥ n + 2,
where the result for d ≥ n + 2 corresponds to the common case of inverse Wishart matrix with
more degrees of freedom than its dimension, and the result for d ≤ n - 2 is based on constructions
provided in the proof of Theorem 1.3 in Breiman and Freedman (1983).
Following (35), let u ∈ Rn be a random vector independent of XF. Then,
EnHXF u∣∣2}=n EnkUk2} ×
P
n—p—1
∞
n
p—n—1
for d ≤ n - 2,
for d ∈ {n - 1, n, n + 1},
for d ≥ n + 2,
(36)
that specifically for u = XFc βFc becomes
P
E IIXF+XFcβFcII22	= E kβFck22	×
n—p— 1
∞
n
p—n—1
for d ≤ n - 2,
for d ∈ {n - 1, n, n + 1},
for d ≥ n + 2.
(37)
12
Under review as a conference paper at ICLR 2021
The results in (33)-(37) are presented using notions of the target task, specifically, using the data
matrix X and the coordinate subset T. One can obtain the corresponding results for the source task
by updating (33)-(37) by replacing X, T, n andp with Z, S, ne and pe, respectively. For example, the
result corresponding to (33) is
叩+zs o=Ie × (⅛ for e >e,	(38)
where ZS+ZS is the pe × pe projection operator onto the range of ZS.
B.3 Proof Outline of Theorem 3.1
The generalization error Eout of the target task was formulated in Eq. (7) for a specific coordinate
subset layout L = {S, F, T, Z}. Here we prove the formulation given in Theorem 3.1 for the
expecation of Eout with respect to a {pe, p, t}-uniformly distributed layout L (see Definition 3.1).
While in the main text the expectation with respect to L is explicitly denoted as EL{∙}, in the
developments below We use the notation of E{∙} to refer to the expectation with respect to all the
random elements (that may also include L) of the expression it is applied on. The developments
start with
ELnEouto = σ2 + Enβb - β2o
=σ2+EnkβZk22o+EnXF+	y-XTθbT	- βF 2o +	EnθbT	- βT 2o, (39)
where the last decomposition shows the expected squared error EL Eout as the sum of the irre-
ducible error σ2 and the expected errors corresponding to each of the three subvectors induced by
the coordinate subsets Z, F, T.
Using the expression for the estimate θ given in (4) and the relation y = Xβ + , the expected error
in the subvector induced by F, i.e., the third term in (39), can be developed into
e{ IXF (y - XTθτ) - βF∣∣2} = e{ IIXF (Xe + e - XTQTQTZ+v)-万产俏}
= EnXF+ (XFcβFc + )22o + EnXF+XT QT QTSZS+v22o
+ E{∣∣(IP - XFXF) βF∣∣2} - 2E{βT (XFXT)T XFXTQTQTZ+v}.	(40)
The four terms in the decomposition in (40) are further developed as follows. The first term in (40)
can be computed using the results in (31), (36) and (37) to receive the form of
p -P-
n-p-1
× J ∞
Ip—n—1
for p ≤ n - 2,
for p ∈ {n - 1, n, n + 1},
for p ≥ n + 2.
(41)
The second term in (40) is developed next using the relations v = Zθ+ξ andθ = Hβ+η, as well as
the results in (29), (36), (38) and the forms of Γsrc andω that were defined before Theorem 3.1 in the
main text. Also, note that XTXT 〜Wt (It, n) is a Wishart matrix with mean e{XTXt} = nIt.
13
Under review as a conference paper at ICLR 2021
Then,
1 e{ I I XTQtQTZ (ZHe + Zn + ξ) I I 2} ×
n e{ I I z+ (ZHe + Zn + ξ) I I 2}×
e{ IlXF XT Qt QT Z+v∣∣2} = e{ ∣ ∣ X+XT QT QT Z+ (ZHe + Zn + ξ)∣∣j}
n⅛ for P ≤ n - 2,
∞ forp ∈ {n — 1,n,n + 1},
UZl for P ≥ n + 2,
n⅛ for p ≤ n - 2,
∞ for p ∈ {n — 1, n, n + 1},
=Z1 for P ≥ n + 2,
d ι∣βk2 ×t × ω×
∞	for p ∈ {n — 1, n, n + 1} or P ∈ {e — 1, e, e + 1},
n-p-1 (1 + d-ep⅞rc)	for P	≤	n	— 2	and p	≤ n —	2,
n⅛ ∙ f (1+ d-e⅛1Γ1)	for P	≤	n	- 2	and p	≥ n +	2,	(42)
p⅛ (1+ d-¾-* )	for P	≥	n	+ 2	and p	≤ P —	2,
p-n-1 ∙ n (1 + d-en-1sr1)	for P	≥	n	+ 2	and p	≥ P +	2.
In the last development we also used the next result
e{IIz+ (ZHe + Zn + ξ)II2}=
e{ I I z+ZS (eSH) + ns)I I 2} + e{ I I z+ (z5ce黑) + ZSCnSc + ξ)I I 2}
for p ≤ P,
for p > n,
+ P e{ I I zsc eSH)+ zsc nsc+ξ I IJ ×
for pP ≤ nP,
for p > n,
/p p
I e-p-1
< ∞
I n
∖ le-n-ɪ
for pP ≤ nP — 2,
for pP ∈ {nP — 1, nP, nP + 1},
for pP ≥ nP + 2,
/ J p
+ (⅛ ∙ ( i i e(H)l + dση )+ σl)× ∣∞ :
∖ Ip-e-ɪ
for pP ≤ nP — 2,
for pP ∈ {nP — 1, nP, nP + 1},
for pP ≥ nP + 2,
d ι∣ekl × ω×
(
∖
for pP ≤ nP,
for p > n,
+ (d—P+d ∙ rsr1)
×
(
∖
n—p— 1
∞
n
p—n— 1
for pP ≤ nP — 2,
for p ∈ {n — 1, n,
for P ≥ P + 2,
P + 1},
1	IIpP 1 +
d ι∣ekl ×ω × < ∞
[p(1 +
d-p⅛d∙rsr1)
n—p—1	)
d-p⅛d∙rsr1)
p—n—1	)
for pP ≤ nP — 2,
for pP ∈ {nP — 1, nP, nP + 1},
for pP ≥ nP + 2.
(43)
14
Under review as a conference paper at ICLR 2021
Next, the third term of (40) can be developed using (31) and (34) into
Enll(Ip-XFXF) βF∣∣2}=E{∣∣βF∣∣2} × {0 - n forp ≤ ；
=1 kβk2 × ʃ0 forP ≤ n,
d 2 p - n for p > n.
(44)
∞
p
+	n-p-1 〜
d hβ(H), βi× n⅛ ∙ e
d	n
p-n-1	〜
n n
、p—n—1	P
The fourth term of (40) is developed next using the relation v = ZHβ + Zη + ξ and the indepen-
dence of η and ξ with the other random variables. Using (35) and (38) one can get
E{βT (XFXT)T XFXTQtQTZ+v} = E{βTXTXF,TXFXTQtQTZ+ZsβSH)}
for P ∈ {n - 1, n, n + 1} or Pe ∈ {ne - 1, ne, ne + 1},
for P ≤	n -	2 and Pe	≤	ne -	2,
for P ≤	n -	2 and Pe	≥	ne +	2,	(45)
for P ≥	n +	2 and Pe	≤	ne -	2,
for P ≥	n +	2 and Pe	≥	ne +	2,
where the last expression can be rewritten using the relation hβ(H), β = ∣∣βk2 ∙ ω ∙ P that stems
from the definitions provided before Theorem 3.1 in the main text for the normalized task correlation
and the task energy ratio. At this intermediate stage, one can use the results provided in (40)-(45) to
formulate the expected error in the subvector induced by F, i.e., the third term in (39).
We now turn to develop an explicit formula for the expected error in the subvector induced by T,
i.e., the fourth term in (39)
EnlllθbT-βTlll2o = EnllQT QTS ZS+v - βT ll22o =EnllQTQTSZS+(ZHβ+Zη+ξ)-βTll22o
=E llQT QTS ZS+	(ZHβ	+ Zη +	ξ)ll22	+E ∣βT ∣22	-2E	βTT QTQTSZS+ZHβ
Using (43), the first term in (46) can be developed into the form of
e{ ∣∣Qt QT Z+ (ZHβ + Zη + ξ)∣∣2 O = "e{ ∣∣z+ (ZHe + Zn + ξ) ∣∣2}=
I	[1 + d-Pp-吴	for P ≤ n - 2,
=-kβ∣2 X t X ω × < ∞	for Pn ∈ {n 一 1,n,n + 1},
I n (1 + d-p+d早)for P ≥ n + 2.
(46)
(47)
The second term in (46) is given in (30) as
E{kβτk2} = d kβk2.	(48)
The third term in (46) can be developed using (38) to obtain the expression
E{eTQtQTZ+ZHβO = E{βTQtQTZ+ZsβSH)O =匕hβ(H),βi X (P f：：n≤ n, (49)
d	d	P for P > /",
where the last expression can be also written differently using the relation hβ(H), β = ∣∣βk2 ∙ ω ∙ ρ.
Setting the results from (32) and (40)-(49) into (39), and noting that σ∣ = ∣∣β∣2 Γ-gt, leads to the
formula given in Theorem 3.1, namely,
ELnEouto = d kβk2 X
(1 + n-P-1 ) (d 一 P + d ∙ rtgt + t ∙ δEtransfer)	for P ≤ n 一 2,
∞	for P ∈ {n - 1, n, n + 1},	(50)
(1 + p-n-1 ) (d 一 P + d ∙ rtgt + t ∙ δEtransfer) + P 一 n for P ≥ n + 2,
15
Under review as a conference paper at ICLR 2021
where
卜一2ρ + d-e+--r1	for P ≤ n — 2,
∆Etransfer = ω ×	∞	forpe∈ {ne - 1,ne,ne+ 1},
[I(I-2ρ + 号+早)for p ≥ e + 2.
(51)
C S pecial Cases of Theorem 3.1
Let us emphasize two special cases of the general result in Theorem 3.1. The first case considers the
solution of the target task without transfer learning, i.e., t = 0. The corresponding generalization
error is formulated in the following corollary that shows that our general formula from Theorem
3.1 reduces for t = 0 into the expectation (over L) of the standard double descent formula (in non-
asymptotic settings such as in (Belkin et al., 2019b)). The red-colored curves in the subfigures of
Fig. 1 are identical and correspond to the formula of this case.
Corollary C.1 (No transfer learning). Fort = 0, i.e., no parameters are transferred from the source
task to the target task. Then, for P ∈ {0, . . . , d},
1	{ (1	+ n⅛)	(d - p + d ∙ rtgt)	forP ≤ n - 2,
EL {E0U=0)} =	d kβk2	× ∞ ∞	for P ∈ {n — 1,n,n	+ 1},
[(1+ p-nn-ι) (d—p+d ∙ rtgt) + p—n forp ≥ n+2.
(52)
The second special case corresponds to transferring parameters from the source task to the target task
without applying any additional learning. This case is induced by setting p = 0 in the formula of
Theorem 3.1, as explicitly formulated in the following corollary. The corresponding generalization
errors are demonstrated in the left-most vertical slices of each of the subfigures in Fig. 5 that are
clearly affected by the relation between the source and target tasks and the number of transferred
parameters.
Corollary C.2 (Parameter transfer without additional learning in target problem). For p = 0, i.e.,
no parameters are learned in the target problem using D. The target estimate β includes only
t ∈ {0, . . . ,pe} parameters transferred from the source task, and the remaining components are set
to zeros.
ELnEou=0)} = kβk2+σ+1 × (d∣∣β叫 2+ση) ×
[1 - 2ρ + d-¾d%	for P ≤ n — 2,
×	∞	for pe ∈ {ne — 1, ne, ne + 1},	(53)
[e (1 - 2ρ + d-p+竿)for P ≥ e + 2.
D Empirical Results for Section 3: Additional Details and
Demonstrations
In Fig. 6 we present the empirically computed values of the out-of-sample squared error of the
target task, EL Eout , with respect to the number of free parameters Pe and P (in the source and
target tasks, respectively). The empirical values in Fig. 6 (and also the values denoted by circle
markers in Fig. 1) were obtained by averaging over 250 experiments where each experiment was
carried out based on new realizations of the data matrices, noise components, and the sequential
order of adding coordinates to subsets (such as S) for the gradual increase of Pe and P within each
experiment. Each single evaluation of the expectation of the squared error for an out-of-sample
data pair x(test) , y(test) was empirically carried out by averaging over a set of 1000 out-of-sample
realizations of data pairs. The deterministic β ∈ Rd used in the experiments has an increasing
linear form that starts at zero and satisfies kβk22 = d. Since we consider averaging over uniformly
distributed layout of coordinate subsets, the results depend only on kβk22 and not on the shape of the
sequence of values in β.
16
Under review as a conference paper at ICLR 2021
80
60
Id 40
20
800
600
400
200
0
20	40	60	80
20	40	60	80
P
(C)
20	40	60	80
P
(b)
20	40	60	80
P
(a)
60
40 40
20
Analytical: σ3 =0.5 i =0
80
600
200
0
20	40	60	80
P
(e)
800
80
60
400ft 40
20
Analytical: σ3 =0.5 t =16
(d)
800
80
600
60
400 I B. 40
200
20
(h)
(g)
80
60
Id 40
20
20	40	60	80
P
(f)
800
600
400
200
0
20	40	60	80
P
(1)
20	40	60	80
P
(k)
20	40	60	80
P
(i)
20	40	60	80
P
(j)
80
60
⅛ 40
20
(m)
(n)
Analytical: =2 t =32
600
0
20	40	60	80
P
(o)
800
60
400 Id 40
200	20
Analytical: W =2 t =48
80
600
400
200
(p)
0
20	40	60	80
800

0
Figure 5:	Analytical evaluation of the out-of-sample squared error of the target task, EL Eout ,
with respect to the number of free parameters pe and p (in the source and target tasks, respectively).
Each row of subfigures considers a different case of the relation (6) between the source and target
tasks in the form ofa different noise variance ση2 whereas H = Id for all. Each column of subfigures
considers a different number of transferred parameters t. Here d = 80, ne = 50, n = 20, kβk22 = d,
σ2 = 0.05 ∙ d,屋=0.025 ∙ d. The white regions correspond to (pe,p) settings eliminated by the
value of t in the specific subfigure. The yellow-colored areas correspond to values greater or equal
to 800. See Fig. 6 for the corresponding empirical evaluation.
One can observe the excellent match between the empirical results in Fig. 6 and the analytical results
provided in Fig. 5. This establishes further the formulation given in Theorem 3.1.
E Proofs and Additional Details on Parameter Transfer
Usefulness in the Setting of Uniformly-Distributed
Coordinate Layouts
E.1 Proof of Corollary 3.1
Recall that Eq. (11) in Theorem 3.1 formulates ∆Etransfer and defines it as the expected error dif-
ference introduced by each constrained parameter that is transferred from the source task instead of
17
Under review as a conference paper at ICLR 2021
60
Id 40
20
EmPiriCaI Results for=0 i =0
40Od 40
200
20
600 60
0
20	40	60	80
P
(a)
800 80
60
40 40
20
Empirical Results for =0.5 t =0
200
600 60
20	40	60
P
(e)
0
80
60
Id 40
20
ElnPiriCaI Results for —1
t=0
800
600
60
40Od 40
200
20
20	40	60
P
(i)
0
80
60
⅛ 40
20
Empirical Results for =2
¢=0
800
600
60
400ift 40
200
20
0
20	40	60	80
P
(m)
Empirical Results for σ? =O t =16
800
40Od 40
20
σ2 =0*5 t =16
I 800
Empirical Results for
80「
20	40	60	80
P
(b)
600
200
0
20	40	60	80
P
(f)
80
I 800
400 i 4 40
200
20
600	60
Empirical Results for b； =0 t =32
200
80
20	40	60
P
(c)
60
400 i a. 40
20
Empirical Results for σζ =0.5
600
200
80
20	40	60
P
(g)
800
i=32
1800
Empirical Results for σ? =1 i =16
20	40	60	80
P
(j)
i=32
I 800
800
Empirical Results for σ∣ =2 i =16
80
600
60
400 I B. 40
200
20
Empirical Results for 吊=2
600
200
20	40	60	80
P
(o)
20	40	60	80
P
(n)
400 1d40
20
600	60
Empirical Results for σ∏ =0 t =48
80	-
600
400
200
80
(d)
20	40	60
60
400 ia. 40
20
Empirical Results for 吊=0.5
600
400
200
(h)
20	40	60
0
80
I 800
i =48
1800
¢=48
60
20
Id 40
Empirical Results for σ≡ —1
600
400
200
80
20	40	60
P
(1)
1800
¢=48
60
400 Id 40
20
Empirical Results for σ∣ =2
600
400
200
(p)
20	40	60	80
800



0
0
0
0
0
0
0
0
Figure 6:	Empirical evaluation of the out-of-sample squared error of the target task, EL Eout ,
with respect to the number of free parameters pe and p (in the source and target tasks, respectively).
The presented values obtained by averaging over 250 experiments. Each row of subfigures considers
a different case of the relation (6) between the source and target tasks in the form of a different noise
variance ση2 whereas H = Id for all. Each column of subfigures considers a different number of
transferred parameters t. Here d = 80, e = 50, n = 20, kβk2 = d, σ2 = 0.05 ∙ d, σξ = 0.025 ∙ d.
The white regions correspond to (pe, p) settings eliminated by the value oft in the specific subfigure.
The yellow-colored areas correspond to values greater or equal to 800.
being set to zero. Accordingly, Corollary 3.1 presents the conditions on the number of free param-
eters pe in the source task, such that the parameter transfer is useful, namely, ∆Etransfer < 0. The
conditions provided in Corollary 3.1 separately refer to the case where the source task is overparam-
eterized (i.e., pe > ne) and the case where the source task is underparameterized (i.e., pe < ne).
E.1.1 Proof for The Overparameterized Case
Let us start by proving the condition in Eq. (12) that refers to an overparameterized source task, i.e.,
pe > ne. Then, according to (11), ∆Etransfer < 0 is possible only when pe ≥ ne + 2 and
d — p + d ∙ Γ-1
src
〜〜	1
p-n- 1
1 -2ρ+
< 0.
(54)
18
Under review as a conference paper at ICLR 2021
For ρ > 0, the last inequality can be rewritten as
~■、 ~ 1 1 .
p>n+1+
d - e + d ∙ Γ-1 一 1
2ρ
(55)
The overparameterization condition of pe ≥ ne + 2 implies d ≥ pe ≥ ne + 2 and, thus, d 一 ne 一 1 ≥
1. Then, together with ρ > 0 (and because Γs-rc1 is always non-negative) we get that the term
d-n+d∙rsrc-1 in(55)is positive valued and, hence, the lower bound of p in (55) is at least n +1 and
2ρ
when the SNR of the source task or the task correlation are lower then a greater overparameterization
is required in the source task (i.e., larger pe) for having a useful transfer of parameters. At this
intermediate stage we finished to prove the condition given in Eq. (12) of Corollary 3.1 and now we
turn to prove that this is the only overparameterized case that enables ∆Etransfer < 0.
For P = 0, the condition in (54) induces the requirement of e > d + d ∙ Γ^1 一 1 and, because Γ-C
is non-negative by its definition, this requirement also implies that en > d 一 1 that contradicts the
basic overparameterization relations of d ≥ pe ≥ ne + 2. Hence, one cannot obtain ∆Etransfer < 0
when pe > en and P = 0.
For P < 0, the condition in (54) means that
~	. ~	1	1	.
p<n+1+
d — n + d ∙ r-rC — 1
2ρ
(56)
where d — n + d ∙「£1 — 1 > 0 again due to p ≥ n + 2. However, here ρ < 0 makes (56)
to imply that pe < ne + 1 that clearly contradicts the overparameterized case of pe ≥ ne + 2. Ac-
cordingly, ∆Etransfer < 0 is impossible when pn > nn and P < 0. This completes the proof for the
overparameterized part of Corollary 3.1.
E.1.2 Proof for the Underparameterized Case
We now turn to prove the conditions in Eq. (13)-(15) that refer to underparameterized settings of the
source task, i.e., pn < nn. Then, by (11), ∆Etransfer < 0 is possible only when pn ≤ nn — 2 and
1 —2P+
d — p + d ∙ ΓsT1
n—p— 1
< 0.
(57)
For P > 1, the last inequality can be also formulated as
p<n—1—
d — n + d ∙ ΓsrC + 1
2 (P -I)
(58)
which, due to the required intersection with pn ≤ nn — 2, remains in its form of (58) only for
n ≤ d + d ∙ Γtsr1 + 1. For n > d + d ∙ Γ-1 + 1, the condition in (58) becomes P ≤ d (because
then the right hand side of (58) is at least nn — 1 whereas pn ≤ d < nn — 1).
For ρ = 1, the condition in (57) can be developed into n > d + d ∙ Γ^1 + 1 that naturally conforms
with the underparameterization condition pn ≤ nn — 2 (this is because always pn ≤ d and Γs-r1c ≥ 0 by
their definitions). Then, for P = 1 and n > d + d ∙「£1 + 1 we get 1 ≤ P ≤ d.
For 0 ≤ P < 1, the condition in (57) can be rewritten as
p>n—1—
d — n + d ∙ Γ^1 + 1
2 (P -I)
(59)
If n ≤ d + d ∙ Γ^1 + 1 then (59) implies p > n — 1, which contradicts the underparameterized case
of P ≤ n — 2. Hence, for 0 ≤ ρ < 1 and n ≤ d + d ∙ Γ-1 + 1 it is impossible to get ∆Etransfer < 0.
If n > d + d ∙ Γsr1 + 1 then the right hand side of (59) is lower than nn — 1 and, thus, the condition
n
—1—
d — n + d ∙ Γsr1 + 1
2(P -I)
< pn ≤ d
(60)
can be feasible (for n > d + d ∙ Γ-1 + 1) in underparameterized settings.
For P < 0, the condition in (57) is equivalent to
1 + d — P + d . rsr1
1	~	~	1
n—p— 1
<0
(61)
19
Under review as a conference paper at ICLR 2021
which implies
pe > 2 (ee + d + d ∙ rsrC - 1) .	(62)
Then, for e > d + d ∙「£1 + 1, the inequality in (62) leads to
P > d + d ∙ ΓsT1 > d	(63)
that clearly contradicts the basic demand p ≤ d in our general settings. If e ≤ d + d ∙「£1 + 1
then the form of (59) is also relevant for ρ < 0 and yields that pe > ne - 1, which contradicts the
underparameterized case of pe ≤ ne - 2. Hence, we showed that for ρ < 0 and any ne it is impossible
to get ∆Etransfer < 0. This completes the proof of all the conditions in Corollary 3.1.
E.2 DETAILS ON THE EMPIRICAL EVALUATION OF ∆Etransfer
The analytical formula for ∆Etransfer, given in Theorem 3.1, measures the expected difference in
the generalization error (of the target task) due to each parameter that is transferred instead of being
set to zero. Accordingly, the empirical evaluation of ∆Etransfer for a given pe can be computed by
∆ Etransfer=占 X
p=1,...,n-2,n+2,...,d
EbL Eo(upe,tp,t=m)	-EbL Eo(pue,tp,t=0)
m ∙ α(p)
(64)
where
α(p)	, 1	kβ∣∣2	X ∫1 +	n-p-ɪ	forP ≤ n - 2,
⑼，d	kβk2	11 +	p-n-ɪ	for P ≥ n + 2,
(65)
is a normalization factor required for the accurate correspondence to the analytical definition of
∆Etransfer provided in Theorem 3.1 in a form independent of p. Here EbL Eo(pu,tp,t=m) is the
out-of-sample error of the target task that is empirically computed for m transferred parameters,
p free parameters in the target task, and pe free parameters in the source task. Correspondingly,
EbL Eo(pu,tp,t=0) is the empirically computed error induced by avoiding parameter transfer. There-
fore, the formula in (64) empirically measures the average error difference for a single transferred
parameter by averaging over the various settings induced by different values of p while pe is kept
fixed. To obtain a good numerical accuracy with averaging over a moderate number of experiments
we use the value m = 5.
Each empirical evaluation of EbL Eo(pue,tp,t) , for a specific set of values pe, p, t corresponds to aver-
aging over 500 experiments where each experiment was conducted for new realizations of the data
matrices, noise components, and the sequential order of adding coordinates to subsets. Each single
evaluation of the expectation of the squared error for an out-of-sample data pair x(test) , y(test)
was empirically computed by averaging over 1000 out-of-sample realizations of data pairs.
E.3 Empirical Results for Parameter Transfer Usefulness in the Setting of
Uniformly-Distributed Coordinate Layouts
We start by providing the full set of analytical and empirical results for the case of ne < d - 1. The
analytical results in Figures 7a-7c are the same as in Figure 3 and provided here again for easier
comparison with their corresponding empirical results in Figures 7d-7f.
In Fig. 3 we present analytical and empirical values of ∆Etransfer induced by settings where ne < d,
which naturally enable the corresponding overparameterized (i.e., ne < pe < d) and underparameter-
ized (i.e., pe < ne < d) settings of the source task.
Here we provide in Fig. 8 the analytical and empirical evaluations of ∆Etransfer that correspond to
settings where ne > d. Note that ne > d implies that, by the definition of pe, the corresponding settings
(of the source task) are underparameterized with pe ≤ d < en. Like in Fig. 3, the results in Fig. 8 show
the excellent match between the analytical and empirical results and, specifically, the accuracy of
the analytical thresholds (from Corollary 3.1) in determining the empirical settings where parameter
transfer is beneficial.
20
Under review as a conference paper at ICLR 2021
(a)	(b)	(c)
(d)
吊
(f)
*
(e)
Figure 7: The analytical and empirical values of ∆Etransfer defined in Theorem 3.1 (namely, the
expected error difference due to transfer of a parameter from the source to target task) as a func-
tion of pe and ση2 . The positive and negative values of ∆Etransfer appear in color scales of red and
blue, respectively. The regions of negative values (appear in shades of blue) correspond to beneficial
transfer of parameters. The positive values were truncated in the value of 2 for the clarity of visual-
ization. The solid black lines (in all subfigures) denote the analytical thresholds for useful transfer
learning as implied by Corollary 3.1. Each subfigure corresponds to a different task relation model
induced by the definitions of H as H = 11d, H = Id, and H = 2Id. For all the subfigures, d = 80,
e = 50, kβ∣∣2 = d, σξ = 0.025 ∙ d.
F Transfer of Specific Sets of Parameters: Proof of Theorem 4.1
In this section we outline the proof of Theorem 4.1 for the generalization error of the target task in the
setting where a specific coordinate subset layout L determines the transferred set of parameters. The
proof of Theorem 4.1 resembles the one of Theorem 3.1 given in Appendix B with the important
difference that now we cannot use the simplified constructions that were provided in Section B.1
for uniformlly-distributed coordinate subsets. Hence, we start in Section F.1 by providing auxiliary
results that use non-asymptotic properties of random orthonormal matrices (that in our case originate
in SVD of the random matrices Z and X that have i.i.d. Gaussian components) in order to formulate
quantities important for the setting of specific coordinate subset layouts. We also use our results
that were provided in B.2 based on non-asymptotic properties of Gaussian and Wishart matrices
and without any necessary aspect of random coordinate subset layouts. Then, in Section F.2, we
prove Theorem 4.1. This proof has a similar general structure as the proof given for 3.1, but with
several important modifications. Therefore, it is recommended to read first the proof provided for
Theorem 3.1 in Section B before getting into the details of the following proof for the case of a
specific coordinate layout.
F.1 Auxiliary Results for the Specific Coordinate Subset Layout Setting
Here the coordinate subset layout L = {S, F, T, Z} is specific, i.e., non random, and therefore the
induced operators such as QS , QF , QT , QZ are also fixed and do not have any random aspect.
Recall that QTS QS is a d × d diagonal matrix with its jth diagonal component equals 1 if j ∈ S
and 0 otherwise. Similarly holds for the other coordinate subsets. Accordingly, here the norms of
vector forms such as βT , QT β, βF , QFβ, and βZ , QZβ, are directly referred to as ∣βT ∣22,
∣βF ∣22, ∣βZ ∣22, respectively.
21
Under review as a conference paper at ICLR 2021
(b)
(a)
(c)
4	瑞	瑞
(d)	(e)	⑴
Figure 8: The analytical (top row of subfigures) and empirical (bottom row of subfigures) values
of ∆Etransfer defined in Theorem 3.1 (namely, the expected error difference due to transfer of a pa-
rameter from the source to target task) as a function of pe and ση2. The positive and negative values
of ∆Etransfer appear in color scales of red and blue, respectively. The regions of negative values
(appear in shades of blue) correspond to beneficial transfer of parameters. The positive values were
truncated in the value of 2 for the clarity of visualization. The solid black lines (in all subfigures)
denote the analytical thresholds for useful transfer learning as implied by Corollary 3.1. Each col-
umn of subfigures correspond to a different task relation model induced by the definitions of H as
H = 2Id, H = Id, and H = 2Id. For all the subfigures, d = 80, e = 50, kβ∣∣2 = d, σξ = 0.025 ∙d.
Recall that T ⊂ S . Then, for a deterministic vector w ∈ Rd,
E QTQTSZS+ZSQSw22
∣wT ∣22
e⅛((e +
ne
p-1
kwτ III+ (1 - n-1) tkws III)
for pe ≤ ne,
for p > n.
(66)
EnllQT QT z+ ZS CQS C w∣∣∣ O = e IlwS C ∣∣∣
p
ne-pe-1
×∞
n
pn-nn -1
for pe ≤ ne - 2,
for pe ∈ {ne - 1, ne, ne + 1},
for pe ≥ ne + 2.
(67)

For two deterministic vectors w, a ∈ Rd,
E	QTa,QTQTSZS+ZSQSw
1	for pe ≤ ne ,
= haT , wTi ×
∖ T T 1 n for P > e.
(68)
For a deterministic vector r ∈ Rnn ,
p p
C	c、	+	n—p—1
E{∣∣QτQTZ+r，} = npe kr∣2 X (∞五
pn—nn—1
for Pe ≤ ne - 2,
for Pe ∈ {ne - 1, ne, ne + 1},
for Pe ≥ ne + 2.
(69)
In our case we have the ne × Pe matrix ZS that its components are i.i.d. standard Gaussian variables,
thus, ZS can be decomposed into a form that involve an independent Haar-distributed matrix, i.e., a
random orthonormal matrix that is uniformly distributed over the set of orthonormal matrices of the
relevant size. This let us to prove the results in (66)-(69) using some algebra and the non-asymptotic
properties of random Haar-distributed matrices, see examples for such properties in Lemma 2.5 in
(Tulino & VerdU, 2004) and also in Proposition 1.2 in (Hiai & Petz, 2000).
22
Under review as a conference paper at ICLR 2021
F.2 Proof Outline of Theorem 4.1
The generalization error Eout of the target task was expressed in its basic form in Eq. (7) for a specific
coordinate subset layout L = {S, F, T, Z}. Please note that, unlike in the proof of Theorem 3.1
in Section B, the expectations below do not include the expectation with respect to L, which is
non-random here.
We start with the relevant decomposition of the error expression, namely,
Eout = σ2 + Enβb - β2o
=σ2+kβZk22+EnXF+	y-XTθbT	-	βF	2o + EnθbT	- βT	2o.	(70)
Then, we use the expression for the estimate θ given in (4) and the relation y = Xβ + , to decom-
pose the third term in (70) as follows
e{ IXF (y - XTθτ) - βF∣∣2} = e{ IIXF (Xe + e - XTQTQTZ+v)-万产俏}
= EnXF+ (XFcβFc + )22o + EnXF+XT QT QTSZS+v22o
+ E{∣∣(lp - XFXF) βF∣∣2} - 2E{βT (XFXT)T XFXTQTQSZ+v}.	(71)
The four terms in (71) are further developed as follows. The first term in (71) can be computed using
the results in (36) and (37) to receive the form of
E ∣∣XF+(XFcβFc+)∣∣22	=	kβFck22+σ2
(-P—
I n—p—1
×∞
I —n—
p—n—1
for p ≤ n - 2,
for p ∈ {n - 1, n, n + 1},
for p ≥ n + 2.
(72)
The second term in (71) is developed next using the relations v = Zθ + ξ and θ = Hβ + η, as well
as the results in (36), (66)-(67), (69) and the forms of Γsrc,Sc and ωT that were defined in the main
text before Theorem 4.1, respectively. Also, note that XTXT 〜Wt (L, n) is a Wishart matrix with
mean E XTT XT = nIt . Then,
E ∣∣XF+XT QT QST ZS+v∣∣22	=E ∣∣XF+ XT QT QTS ZS+ (ZHβ + Zη + ξ)∣∣22
e{∣∣QtQTz+ (ZHe + Zn + ξ)∣∣20 × 1∞p 1
Ip—n—1
keFck22 × ωT×
for p ≤ n - 2,
for p ∈ {n - 1, n, n + 1},
for p ≥ n + 2,
(73)
P
n—p— 1
P
n—p— 1
n
p—n— 1
n
p—n— 1
'1 , φτ(d-e)(1+r-r1,sc )∖
1 +	n—p—1	I
e ((p2-np)ΨT+np-1	φτ(d-p)(1+r^C,sc)
e I	p2-1	1	p—n—1
1 , φτ(d-e)(1+r-r1,sc) ʌ
1 +	n—e—1	I
e ((e2-ne)ψτ+ne-1	φτ(d-p)(1+r^C,sc)
e [	p2-1	1	p—n—1
for p ∈ {n - 1, n, n + 1}
or pe ∈ {ne - 1, ne, ne + 1},
for p ≤ n	- 2	and pe ≤	ne	- 2,
for p ≤ n	- 2	and pe ≥	ne	+ 2,	(74)
for p ≥ n	+ 2	and pe ≤	ne	- 2,
for p ≥ n	+ 2	and pe ≥	ne	+ 2.
In the last calculations we also used the following result
En∣∣QT QTS ZS+ (ZHe + Zη + ξ)∣∣22o
(75)
En∣∣∣QTQTSZS+ZSe(SH)+ηS∣∣∣22o
+En∣∣∣QTQTSZS+ZSc e(SHc) + ηSc ∣∣∣2o + En∣∣QT QTS ZS+ξ∣∣22o
∞
23
Under review as a conference paper at ICLR 2021
that using (66)-(67), (69) leads to
E	QT QTS ZS+ (ZHβ + Zη + ξ)22 =
(76)
t ∖ l∣βS1)ll2 + peσ2	for P ≤ n
∖jeP+i) Ie+e-ι) (∣∣βτι)∣∣:+tσ2) + (I-n-ι)t(∣∣βsυ∣∣2+κ)) fore>e,
+pe(∣∣βH )∣∣2+(d- p)σ2+σ2) × ([∞p
∖ j P-n-1
for Pe ≤ ne - 2,
for Pe ∈ {ne -1, ne, ne +1},
for Pe ≥ ne + 2,
2	1	for Pe ≤ ne,
kβFc Il 2 × ωT × I I e	(e-ep)ψτ+np-1	ʌ,〜)
∖l,p ∙	e2-1	forP >n√
/ ( e
/	∖ I e-eT
+ kβFc k2 × ωT × φT × Id- p) (1 +r-1,Sc) × I ∞
∖ Ie-e-1
for Pe ≤ ne - 2,
for Pe ∈ {ne -1, ne, ne +1},
for Pe ≥ ne + 2,
{1 , φt(d-ρ)(1+r-rC,sc)
1 +	e-e-1
∞
e ( (p2-np)ψτ+ee-1 ι
e [	P2-1	+
φτ (d-e)(1+r-r1,sc)
p-n-1
for Pe ≤ ne - 2,
for Pe ∈ {ne - 1, ne, ne + 1},
for Pe ≥ ne + 2.
where ψT
专 IeSH) I
IeIH
2
2+tσ
2
2+tση2
2
η , φT
tση2
+tση2
and Γsrc,Sc
∣∣eSH)∣∣:+(d-e)σ2
σξ
were already
defined before Theorem 4.1 in the main text.
Now, the third term of (71) can be developed using (34) into
E1∣∣(lp-XFXF) βF∣∣20 = kβFk2 × (0	n forp ≤ n,	(77)
[∣∣∖p F F∕LF∣∣2J	IlLFII2	11 - n for p>n,	∖ /
∞
P
n-p-1
hβT), βτi ×	n-p-ι
n
p—n— 1
n
、p—n—1
The fourth term of (71) is developed next using the relation v = ZHβ + Zη + ξ and the indepen-
dence of η and ξ with the other random variables. Using (35) and (68) we get
E{βT (XFXT)T XFXTQtQsZ+v} = E{βTXTXF,TXFXTQtQTZrZSβSH)}
for p ∈	{n - 1, n, n	+	1} or pe ∈	{ne - 1,	ne,	ne +	1},
for p ≤	n -	2 and pe	≤	ne -	2,
•	e for p ≤	n 一	2 and p	≥	e +	2,	(78)
for p ≥	n +	2 and pe	≤	ne -	2,
~ . ..
•	e for P ≥ n + 2 and P ≥ n + 2,
and the last expression can be reformulated using the relation hβ(TH), βTi = l∣βFc ∣∣2 • ωT • PTthat
relies on the definitions given before Theorem 4.1 in the main text. At this intermediate stage,
one can use the results provided in (71)-(78) to formulate the error in the subvector induced by the
specific F of interest (namely, this part of the error is represented by the third term in (70).
We proceed to the formulation of the error in the subvector induced by the specific T of interest,
i.e., the fourth error term in (70)
En∣∣∣θbT-βT∣∣∣2o = En∣∣QT QTS ZS+v - βT ∣∣22o =En∣∣QTQTSZS+(ZHβ+Zη+ξ)-βT∣∣22o
= En∣∣QT QTS ZS+ (ZHβ + Zη + ξ)∣∣22o + kβT k22 - 2EnβTT QT QTS ZS+ZHβo	(79)
Note that the first term in (79) was already developed in (76) into an explicit form. The second term
in (79) remains as it is because T is fixed in the current setting.
24
Under review as a conference paper at ICLR 2021
The third term in (79) can be developed using (68) into
EnβTT QT QTS ZS+ZHβo =EnβTTQTQTSZS+ZSβ(SH)o
=hβτ, βτi × (n	for pe ≤ e,	(80)
T T 1 e for P > n,
where the last expression can be also written differently using the relation
hβTH), βτi = IIeFC ∣∣2 ∙ ωτ ∙ ρτ that was already mentioned above.
Setting the results from (71)-(80) into (70), and noting that σ2 = kβFc k22 Γt-g1t,F c, yields the formula
given in Theorem 4.1, i.e,
f n---1(1 +Γ-gt,F C + ∆E(Tf}	kβF C	k2	for P ≤ n - 2,
EoLt)	= p ∕-1T	(l+Γ-t FC +∆EtTS)ej	kβF C	k2	+ p-n	kβF k2	for P ≥ n + 2,⑻)
p-n-1	tgt,	transer	2 p	2
[∞	otherwise.
where
{1 C l φt(d-e)(ι+r-riSC)	「~∕~ C
1 — 2ρτ +-------ee-p-1src,S '	for P ≤ n 一 2,
n ((p2-np)ψτ+np-1	2	, φT(d-p)(1+r-r1,sC)∖ f -> ~ , 2
P I -----铲-ι-----------2Pτ H------p-ne-ι------ forP ≥ n + 2,
∞	otherwise.
G Transfer of S pecific Sets of Parameters: Additional
Analytical and Empirical Evaluations of Generalization
Error Curves
The following results are for two different forms of the true solution β: the first is a form with
linearly increasing values (Fig. 9a), the second is a form with sparse value where only 25% of
coordinates have non-zero value (Fig. 9b). Note that both forms satisfy ∣β∣22 = d.
The three types of linear operator H in our evaluations are as follows. First, H = Id that is the iden-
tity operator. Second, is the circulant matrix H that corresponds to a shift-invariant local averaging
operator that uniformly considers 11-coordinates neighborhood around the computed coordinate.
Third, is the circulant matrix H that corresponds to discrete derivative operator based on the convo-
lution kernel [-0.5, 0.5].
Figures 10- 11 present the analytical and empirical values of the generalization error of the target
task with respect to specific coordinate layouts L that evolve with respect to the value of P (this
evolution of L is the same in each of the subfigures and it is not particularly designed to any of
the combinations of the true β, H, and ση2). It is clear from Figures 10- 11 that the increase in
ση2 , which by its definition corresponds to less related source and target tasks, reduces the benefits
or even increases the harm due to transfer of parameters (one can observe that in Figs. 10- 11 by
comparing the error curves among subfigures in the same row).
The effect of H with respect to the true β is also evident. First, the identity operator H = Id
does not reduce the relation between the source and target tasks and therefore does not degrade the
parameter transfer performance by itself (i.e., for H = Id, only the additive noise level ση2) can
reduce the relation between the tasks). Second, when H is a local averaging operator it does not
reduce the benefits from transfer learning (e.g., compare second to first row of subfigures in 10- 11)
in the case of linearly-increasing β shape (because local averaging does not affect a linear function,
except to the few first and last coordinates where the periodic averaging is applied), in contrast, the
local averaging operator significantly degrades the parameter transfer performance in the case of the
sparse β form. Lastly, when H is a discrete derivative operator it renders transfer learning harmful
in the case of linearly-increasing β shape (e.g., compare third to first row of subfigures in 10). In
the case of the sparse β form the discrete derivative reduces the potential benefits of the parameter
transfer but does not eliminte them completely in the case these benefits exist for H = Id (e.g.,
compare third to first row of subfigures in 11).
25
Under review as a conference paper at ICLR 2021
0	20	40	60	80
Coordinate
β: sparse structure of values
(b)
Figure 9: The two types of true solution β used for the analytical and empirical evaluations.
Number of parameters (p)
250
200
J150
100
50
20	40	60	80
Number of parameters (p)
Number of parameters (p)
Number of parameters (P)
(d)
(a)	(b)	(c)
Number of parameters (p)
Number of parameters (p)
(f)
Number of parameters (p)
H
=2
Number of parameters (p)
(h)
(e)
Number of parameters (p)
Number of parameters (p)
(g)
Number of parameters (p)
Number of parameters (p)
(i)	(j)	(k)	(l)
Figure 10: Analytical (solid lines) and empirical (circle markers) values of Eo(uLt) for specific, non-
random coordinate layouts. The true solution β has linearly-increasing values. All subfigures use
the same sequential evolution of L with p. Each subfigure considers a different case of the relation
(6) between the source and target tasks: each column of subfigures has a different ση2 value, and each
row of subfigures corresponds to a different linear operator H. The analytical values, induced from
Theorem 4.1, are presented using solid-line curves, and the respective empirical results obtained
from averaging over 750 experiments are denoted by circle markers. Each curve color refer to a
different number of transferred parameters.
26
Under review as a conference paper at ICLR 2021
250
Number of parameters (p)
(a)
H = Id,σ≡=0.5
200
150
100
50
20	40	60	80
Number of parameters (p)
(b)
250
H = Id, σj =1
200
150
100
50
20	40	60	80
Number of parameters (p)
(c)
250
H = Id,σ≡=2
200
150
100
50
20	40	60	80
Number of parameters (P)
(d)
Number of parameters (p)
(e)
200
150
100
H locally averages, =0.5
50
20	40	60	80
NUmber of parameters (P)
(f)
200
150
100
H locally averages,尺
50
20	40	60	80
Number of parameters (p)
(g)
Number of parameters (p)
(h)
200
150
100
H is discrete derivative, =0
50
20	40	60	80
Number of parameters (p)
(i)
200
150
100
H is discrete derivative, =0.5
50
20	40	60	80
Number of parameters (p)
(j)
200
150
100
H is discrete derivative,
250
50
20	40	60	80
Number of parameters (P)
(k)
200
150
100
H is discrete derivative, =2
50
20	40	60	80
Number of parameters (p)
(1)
Figure 11: Analytical (solid lines) and empirical (circle markers) values of Eo(uLt) for specific, non-
random coordinate layouts. The true solution β has a sparse form of values. All subfigures use
the same sequential evolution of L with p. Each subfigure considers a different case of the relation
(6) between the source and target tasks: each column of subfigures has a different ση2 value, and each
row of subfigures corresponds to a different linear operator H. The analytical values, induced from
Theorem 4.1, are presented using solid-line curves, and the respective empirical results obtained
from averaging over 750 experiments are denoted by circle markers. Each curve color refer to a
different number of transferred parameters.
27