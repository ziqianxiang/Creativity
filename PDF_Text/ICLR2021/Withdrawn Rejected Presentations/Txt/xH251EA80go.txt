Under review as a conference paper at ICLR 2021
A Simple Sparse Denoising Layer for Robust
Deep Learning
Anonymous authors
Paper under double-blind review
Ab stract
Deep models have achieved great success in many applications. However, vanilla
deep models are not well-designed against the input perturbation. In this work,
we take an initial step to design a simple robust layer as a lightweight plug-in
for vanilla deep models. To achieve this goal, we first propose a fast sparse cod-
ing and dictionary learning algorithm for sparse coding problem with an exact
k-sparse constraint or l0 norm regularization. Our method comes with a closed-
form approximation for the sparse coding phase by taking advantage of a novel
structured dictionary. With this handy approximation, we propose a simple sparse
denoising layer (SDL) as a lightweight robust plug-in. Extensive experiments on
both classification and reinforcement learning tasks manifest the effectiveness of
our methods.
1	Introduction
Deep neural networks have obtained a great success in many applications, including computer vi-
sion, reinforcement learning (RL) and natural language processing, etc. However, vanilla deep
models are not robust to noise perturbations of the input. Even a small perturbation of input data
would dramatically harm the prediction performance (Goodfellow et al., 2015).
To address this issue, there are three mainstreams of strategies: data argumentation based learning
methods (Zheng et al., 2016; Ratner et al., 2017; Madry et al., 2018; Cubuk et al., 2020), loss func-
tions/regularization techniques (Elsayed et al., 2018; Zhang et al., 2019), and importance weighting
of network architecture against noisy input perturbation. Su et al. (2018) empirically investigated
18 deep classification models. Their studies found that model architecture is a more critical factor
to robustness than the model size. Most recently, Guo et al. (2020) employed a neural architecture
search (NAS) method to investigate the robust architectures. However, the NAS-based methods are
still very computationally expensive. Furthermore, their resultant model cannot be easily adopted
as a plug-in for other vanilla deep models. A handy robust plug-in for backbone models remains
highly demanding.
In this work, we take an initial step to design a simple robust layer as a lightweight plug-in for
the vanilla deep models. To achieve this goal, we first propose a novel fast sparse coding and
dictionary learning algorithm. Our algorithm has a closed-form approximation for the sparse coding
phase, which is cheap to compute compared with iterative methods in the literature. The closed-
form update is handy for the situation that needs fast computation, especially in the deep learning.
Based on this, we design a very simple sparse denoising layer for deep models. Our SDL is very
flexible, and it enables an end-to-end training. Our SDL can be used as a lightweight plug-in for
many modern architecture of deep models (e.g., ResNet and DenseDet for classification and deep
PPO models for RL). Our contributions are summarized as follows:
•	We propose simple sparse coding and dictionary learning algorithms for both k-sparse con-
strained sparse coding problem and l0-norm regularized problem. Our algorithms have
simple approximation form for the sparse coding phase.
•	We introduce a simple sparse denoising layer (SDL) based on our handy update. Our SDL
involves simple operations only, which is a fast plug-in layer for end-to-end training.
•	Extensive experiments on both classification tasks and reinforcement learning tasks show
the effectiveness of our SDL.
1
Under review as a conference paper at ICLR 2021
2	Related Works
Sparse Coding and Dictionary Learning: Sparse coding and dictionary learning are widely stud-
ied in computer vision and image processing. One related popular method is K-SVD (Elad &
Aharon, 2006; Rubinstein et al., 2008), it jointly learns an over-complete dictionary and the sparse
representations by minimizing a l0-norm regularized reconstruction problem. Specifically, K-SVD
alternatively iterates between the sparse coding phase and dictionary updating phase. The both steps
are based on heuristic greedy methods. Despite its good performance, K-SVD is very computation-
ally demanding. Moreover, as pointed out by Bao et al. (2013), both the sparse coding phase and
dictonary updating of K-SVD use some greedy approaches that lack rigorous theoretical guarantee
on its optimality and convergence. Bao et al. (2013) proposed to learn an orthogonal dictionary
instead of the over-complete one. The idea is to concatenate the free parameters with predefined
filters to form an orthogonal dictionary. This trick reduces the time complexity compared with K-
SVD. However, their algorithm relies on the predefined filters. Furthermore, the alternative descent
method heavily relies on SVD, which is not easy to extend to deep models.
In contrast, our method learns a structured over-complete dictionary, which has a simple form as a
layer for deep learning. Recently, some works (Venkatakrishnan et al., 2013) employed deep neural
networks to approximate alternating direction method of multipliers (ADMM) or other proximal
algorithms for image denoising tasks. In (Wei et al., 2020), reinforcement learning is used to learn
the hyperparameters of these deep iterative models. However, this kind of method itself needs to
train a complex deep model. Thus, they are computationally expensive, which is too heavy or
inflexible as a plug-in layer for backbone models in other tasks instead of image denoising tasks, e.g.,
reinforcement learning and multi-class classification, etc. An illustration of number of parameters
of SDL, DnCNN (Zhang et al., 2017) and PnP (Wei et al., 2020) are shown in Table 1. SDL has
much less parameters and simpler structure compared with DnCNN and PnP, and it can serve as a
lightweight plug-in for other tasks, e.g., RL.
Table 1: Number of parameters of different models
SDL^^DnCNN (Zhang et al., 2017)^^PnP (Wei et al., 2020)
parameters	630	667,008	296,640
Robust Deep Learning: In the literature of robust deep learning, several robust losses have been
studied. To achieve better generalization ability, Elsayed et al. (2018) proposed a loss function to
impose a large margin of any chosen layers of a deep network. Barron (2019) proposed a general
loss with a shape parameter to cover several robust losses as special cases. For the problems with
noisy input perturbation, several data argumentation-based algorithms and regularization techniques
are proposed (Zheng et al., 2016; Ratner et al., 2017; Cubuk et al., 2020; Elsayed et al., 2018; Zhang
et al., 2019). However, the network architecture remains less explored to address the robustness of
the input perturbation. Guo et al. (2020) employed NAS methods to search the robust architectures.
However, the searching-based method is very computationally expensive. The resultant architectures
cannot be easily used as a plug-in for other popular networks. In contrast, our SDL is based on a
closed-form of sparse coding, which can be used as a handy plug-in for many backbone models.
3	Fast Sparse Coding and dictionary learning
In this section, we present our fast sparse coding and dictionary learning algorithm for the k-sparse
problem and the l0-norm regularized problem in Section 3.1 and Section 3.2, respectively. Both
algorithms belong to the alternative descent optimization framework.
3.1	k-Sparse Coding
We first introduce the optimization problem for sparse coding with a k-sparse constraint. Mathe-
matically, we aim at optimizing the following objective
Ym,iDn kX - DY k2F
SUbjeCttokyik0 ≤ k,∀i ∈ {1,…，N}	(1)
μ(D) ≤ λ
kdjk2 = 1,∀j ∈{1,…，M},
2
Under review as a conference paper at ICLR 2021
where D ∈ Rd×M is the dictionary, and di denotes the ith column of matrix D . yi denotes the ith
column of the matrix Y ∈ RM×N, and μ(∙) denotes the mutual coherence that is defined as
μ(D) = max Jdi Idj l∣∣ .
i6=j kdik2kdjk2
(2)
The optimization problem (1) is discrete and non-convex, which is very difficult to optimize. To
alleviate this problem, we employ a structured dictionary as
D = R>B.
(3)
We require that R> R = RR> = Id and BB > = Id, and each column vector of matrix B has
a constant l2-norm, i.e., kbik2 = c. The benefit of the structured dictionary is that it enables a fast
update algorithm with a closed-form approximation for the sparse coding phase.
3.1.1	CONSTRUCTION OF STRUCTURED MATRIX B
Now, we show how to design a structured matrix B that satisfies the requirements. First, we con-
struct B by concatenating the real and imaginary parts of rows of a discrete Fourier matrix. The
proof of the following theorems regarding the properties of B can be found in Appendix.
Without loss of generality, we assume that d = 2m, M = 2n. Let F ∈ Cn×n be an n × n
discrete Fourier matrix. Fkj = e2πnkj is the (k,j)thentry of F, where i = √-1. Let Λ =
{k1, k2, ..., km} ⊂ {1, ..., n-1} be a subset of indexes.
The structured matrix B can be constructed as Eq.(4).
ReFΛ
ImFΛ
-ImFΛ
ReFΛ
∈ Rd×N
(4)
where Re and Im denote the real and imaginary parts of a complex number, and FΛ in Eq. (5) is the
matrix constructed by m rows of F
2πik1 1
e n
FΛ=	.
.
2πikm 1
_ e n
2πik1 n
2πikmn
e n
∈ Cm×n
(5)
e n
Proposition 1. Suppose d = 2m, M = 2n. Construct matrix B as in Eq.(4). Then BB> = Id
and kbj k2 = Pf，∀j ∈{1,…，M}.
Theorem 1 shows that the structured construction B satisfies the orthogonal constraint and constant
norm constraint. One thing remaining is how to construct B to achieve a small mutual coherence.
To achieve this goal, we can leverage the coordinate descent method in (Lyu, 2017) to construct the
index set Λ. For a prime number n such that m divides n-1, i.e., m|(n - 1), we can employ a
closed-form construction. Let g denote a primitive root modulo n. We construct the index Λ =
{k1,k2, ...,km} as
A	0	0 n—1	2(n-1)	(m-1)(n-I)
Λ = {g,g m ,g m ,…，g m } mod n.	(6)
The resulted structured matrix B has a bounded mutual coherence, which is shown in Theorem 1.
Theorem 1. Suppose d = 2m, M = 2n, and n is a prime such that m|(n - 1). Construct matrix
B as in Eq.(4) with index set Λ as Eq.(6). Let mutual coherence μ(B) := maxi=j %|：；乳口2 ∙ Then
μ(B) ≤ √.	,一
Remark: The bound of mutual coherence in Theorem 1 is non-trivial when n < m2 . For the
case n ≥ m2, we can use the coordinate descent method in (Lyu, 2017) to minimize the mutual
coherence.
Now, we show that the structured dictionary D = R>B satisfies the constant norm constraint and
has a bounded mutual coherence. The results are summarized in Theorem 1.
3
Under review as a conference paper at ICLR 2021
Corollary 1. Let D = R>B with R>R = RR> = Id. Construct matrix B as in Eq.(4) with
index set Λ as Eq.(6). Then μ(D) = μ(B) ≤ √n and ∣∣dj ∣∣2 = ∣∣bj∣∣2 = Pn, ∀j ∈ {1,…，M}.
Corollary 1 shows that, for any orthogonal matrix R, each column vector of the structured dictionary
D has a constant l2-norm. Moreover, it remains a constant mutual coherence μ(D) = μ(B).
Thus, given a fixed matrix B, we only need to learn matrix R for the dictionary learning without
undermining the low mutual coherence property.
3.1.2	Joint Optimization for Dictionary Learning and Sparse Coding
With the structured matrix B, we can jointly optimize R and Y for the optimization problem (7).
Ym,iRn∣X-R>BY∣2F
subject to ∣∣yiko ≤ k, ∀i ∈ {1,…，N}
R>R = RR> = Id
(7)
This problem can be solved by the alternative descent method. For a fixed R, we show the sparse
representation Y has a closed-form approximation thanks to the structured dictionary. For the fixed
sparse codes Y , dictionary parameter R has a closed-form solution.
Fix R, optimize Y : Since the constraints of Y is column separable, i.e., ∣yi ∣0 ≤ k, and the
objective (8) is also decomposable,
N
∣X - R>BY ∣2F =X∣xi-R>Byi∣2F.	(8)
i=1
It is sufficient to optimize the sparse code yi ∈ RM for each point xi ∈ Rd separately.
Without loss of generality, for any input x ∈ Rd, we aim at finding the optimal sparse code y ∈ RM
such that ∣y∣0 ≤ k. Since R>R = RR>=IdandBB> = Id , we have
∣x - R>By∣22 = ∣Rx - RR>By∣22
= ∣Rx - By∣22
= ∣BB>Rx - By∣22
= ∣B(B>Rx - y)∣22
= ∣B(h - y)∣22.	(9)
where h = B>Rx is a dense code. Case 1: When m = n (the columns of B are orthogonal), we
can rewrite Eq.(9) into a summation form as
M
∣B(h-y)∣22=X(hj-yj)2∣bj∣22.	(10)
j=1
Case 2: When m < n, we have an error-bounded approximation using R.H.S. in Eq.(10). Let
z = h - y, we have
M	MM
∣Bz∣22-Xzj2∣bj∣22=X X zizjbi>bj	(11)
j=1	i=1 j=1,j6=i
MM
≤ XX ∣ZiZj∣kbik2kbjk2μ(B)	(12)
i=1 j=1,j6=i
MM
=XX |zizj | ∙ n ∙ μ(B)	(13)
i=1 j=1,j6=i
4
Under review as a conference paper at ICLR 2021
It is worth to note that the error bound is small when the mutual coherence μ(B) is small. When We
employ the structural matrix in Theorem 1. It follows that
M	MM	厂
IkBzk2 - Xz2kbjk2∣≤ X X ∣ZiZ小三∙min*,1)	(14)
j=1	i=1 j=1,j6=i
MM
= CX X |zizj|	(15)
i=1 j=1,j6=i
MM
= CX X |hi - yi||hj - yj|	(16)
i=1 j=1,j6=i
where C = min(√1n, mm). In Eq.(14), we use μ(B) ≤ √n from Theorem 1.
Considering the sparse constraint ky k0 ≤ k, the error bound is minimized when all the non-zero
term yj = hj to get |yj - hj | = 0. Let S denote the set of index of non-zero element yj of y . Now
the problem is to find the index set S to minimize
MM
X X |hi-yi||hj-yj|=X X |hi||hj|	(17)
i=1 j=1,j6=i	i∈Sc j∈Sc,j6=i
where Sc denotes the complement set of S. We can see that Eq.(17) is minimized when S consists
of the index of the k largest (in absolute value) elements of h.
Now, we consider PM=Izj kbj k2. Note that kbj ∣∣2 = m, it follows that
MM
X zjkbj k2 = : X (hj-yj )2.	(18)
j =1	j=1
Because each term (hj - yj)2 ≥ 0 is minimized when yj = hj, we know that Eq.(18) under sparse
constraints is minimized when all the non-zero term setting as yj = hj . Otherwise we can set a
non-zero term yj to yj = hj to further reduce term (hj - yj )2 to zero.
Now, the problem is to find the index set of the non-zero term to minimize Eq.(19).
MM
X(hj-yj)2=Xhj2- X	hi2	(19)
j=1	j=1	i∈S,∣S∣≤k
where S := {j|yj 6= 0}. We can see that Eq.(19) is minimized when S consists of the index of the
k largest (in absolute value ) elements of h.
Remark: Both the approximation PjM=1 zj2 kbjk22 and the error bound is minimized by the same
solution.
Fix Y , Optimize R : For a fixed Y , we know that
kX - R>BY k2F = kXk2F + kBY k2F -2tr(R>BYX>)	(20)
This is the nearest orthogonal matrix problem, which has a closed-form solution as shown
in (Schonemann, 1966; Gong et al., 2012). Let BYX> = UΓV> obtained by singular value
decomposition (SVD), where U, V are orthgonal matrix. Then, Eq.(20) is minimized by
R= UV>	(21)
3.2	l0 -NORM REGULARIZATION
We employ the structured dictionary D = R>B same as in Section 3.1. The optimization problem
with l0 -norm regularization is defined as
Ym,iRn kX - R>BY k2F + λkY k0
subject to R>R = RR> = Id	(22)
5
Under review as a conference paper at ICLR 2021
Figure 1: Illustration of the SDL Plug-in
This problem can be solved by the alternative descent method. For a fixed R, we show Y has
a closed-form approximation thanks to the structured dictionary. For fixed the sparse codes Y ,
dictionary parameter R also has a closed-form solution.
Fix R, optimize Y : Since the objective can be rewritten as Eq.(23)
N
kX - R>BY k2F +λkYk0=Xkxi-R>Byik2F +λkyik0.	(23)
It is sufficient to optimize Yi for each point Xi separately. Without loss of generality, for any input
x ∈ Rd, we aim at finding an optimal sparse code y ∈ RM. Since R>R = RR> = Id and
BB> = Id , when m = n, following the derivation in Section 3.1.2, we have
kx-R>Byk2F + λkyk0 = kB(h - y)k2F + λkyk0,
(24)
where h = B>Rx is a dense code. Note that ∣∣bj k2 = m, together with Eq.(24), it follows that
kB(h - y)kF + λkyko = — (X (hj- yj)2+	1[yj = 0] j.
n	j=1	m
where 1[∙] is an indicator function which is 1 if its argument is true and 0 otherwise.
This problem is separable for each variable yj , and each term is minimized by setting
y. = / hj	if h2 ≥ nλ
j 0 otherwise
(25)
(26)
Fix Y , update R: For a fixed Y , minimizing the objective leads to the same nearest orthogonal
matrix problem as shown in Section 3.1.2. Let BY X> = UΓV > obtained by SVD, where U, V
are orthogonal matrix. Then, the reconstruction problem is minimized by R = UV > .
Remark: Problems with other separable regularization terms can be solved in a similar way. The
key difference is how to achieve sparse codes y. For example, for l1-norm regularized problems, y
can be obtained by a soft thresholding function, i.e., y = sign(y) © max(0, |y| — nλ∕(2m)).
4 Sparse Denoising Layer
One benefit of our fast sparse coding algorithm is that it enables a simple closed-form reconstruction,
which can be used as a plug-in layer for deep neural networks. Specifically, given an orthogonal
matrix R and input vector x, the optimal reconstruction of our method can be expressed as
X = R> B f (B> Rx)
(27)
where f (∙) is a non-linear mapping function. For the k-sparse constrained problem, f (∙) is a k-max
pooling function (w.r.t the absolute value) as Eq.(28)
f(h )	hj	if |hj | is one of the k-highest values of |h| ∈ RM
j	0	otherwise
(28)
6
Under review as a conference paper at ICLR 2021
For the lo-norm regularization problem, f (∙) is a hard thresholding function as Eq.(29)
f(hj) = ( hj	if ∣hj∣≥qnλ .
0 otherwise
(29)
For the lι-norm regularization problem, f (∙) is a soft thresholding function as Eq.(30)
f(hj) = sign(hj)
× max
(30)
where sign(∙) denotes the Sign function.
The reconstruction in Eq.(27) can be used as a simple plug-in layer for deep networks, we named it
as sparse denoising layer (SDL). It is worth noting that only the orthogonal matrix R is needed to
learn. The structured matrix B is constructed as in Section 3.1.1 and fixed.
The orthogonal matrix R can be parameterized by exponential mapping or Cayley mapping (Hel-
frich et al., 2018) of a skew-symmetric matrix. In this work, we employ the Cayley mapping to
enable gradient update using deep learning tools. Specifically, the orthogonal matrix R can be
obtained by the Cayley mapping of a skew-symmetric matrix as
R=(I+W)(I-W)-1,	(31)
where W is a skew-symmetric matrix, i.e., W = -W> ∈ Rd×d. For a skew-symmetric matrix
W , only the upper triangular matrix (without main diagonal) are free parameters. Thus, the number
of free parameters of SDL is d(d - 1)/2, which is much smaller compared with the number of
parameters of backbone deep networks.
For training a network with a SDL, we add a reconstruction loss term as a regularization. The
optimization problem is defined as
min	'(X; W, Θ) + β∣∣Z - Z∣∣F,	(32)
W∈Rd×d,Θ
where W is a skew-symmetric matrix parameter of SDL. Θ is the parameter of the backbone net-
work. Z is the reconstruction of the latent representation Z via SDL (Eq.(27)). An illustration of the
SDL plug-in is shown in Figure 1. When SDL is used as the first layer, then Z = X, and Z = X.
In this case, Z is the reconstruction of the input data X.
It is worth noting that the shape of the input and output of SDL are same. Thus, SDL can be used as
plug-in for any backbone models without changing the input/output shape of different layers in the
backbone network. With the simple SDL plug-in, backbone models can be trained from scratches.
Figure 2: Mean test accuracy ± std over 5 independent runs on CIFAR10/CIFAR100 dataset under
FGSM adversarial attack for Densenet and Resnet with or without SDL
5 Experiments
We evaluate the performance of our SDL on both classification tasks and RL tasks. For classification,
we employ both DenseNet-100 (Huang et al., 2017) and ResNet-34 (He et al., 2016) as backbone.
For RL tasks, we employ deep PPO models1 (Schulman et al., 2017) as backbone. For all the tasks,
we test the performance of backbone models with and without our SDL when adding Gaussian noise
or Laplace noise. In all the experiments, we plug SDL as the first layer of deep models. We set the
1https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/
7
Under review as a conference paper at ICLR 2021
Figure 3: Mean test accuracy ± std over 5 independent runs on CIFAR10 dataset with Gaussian
noise for Densenet and Resnet with or without SDL
Figure 4: Mean test accuracy ± std over 5 independent runs on CIFAR100 dataset with Gaussian
noise for Densenet and Resnet with or without SDL
standard deviation of input noise as {0, 0.1, 0.2, 0.3}, respectively. (The input noise is added after
input normalization) . We keep all the hyperparameters of the backbone models same, the only
difference is whether plugging SDL. The parameter β for the reconstruction loss is fixed as β = 100
in all the experiments.
Classification Tasks: We test SDL on CIFAR10 and CIFAR100 dataset. We construct the struc-
tured matrix B ∈ R12×14 by Eq.(4). In this setting, the orthogonal matrix R corresponds to the
convolution parameter of Conv2d。) with kernelsize=2 X 2. We set the sparse parameter of our
k-sparse SDL as k = 3 in all the classification experiments. The average test accuracy over five
independent runs on CIFAR10 and CIFAR100 with Gaussian noise are shown in Fig. 3 and Fig. 4,
respectively. We can observe that models with SDL obtain a similar performance compared with
vanilla model on the clean input case. With an increasing variance of input noise, models with SDL
outperform the vanilla models more and more significantly. The experimental results with Laplace
noise are presented in Fig. 9 and Fig. 10 in the supplementary. The results on Laplace noise cases
show the similar trends with Gaussian noise cases. We further test the performance of SDL under
the fast gradient sign method (FGSM) attack (Goodfellow et al., 2015). The perturbation parame-
ter epsilon is set to 8/256. Experimental results are shown in Fig. 2. We can observe that adding
SDL plug-in can improve the adversarial robustness of the backbone models. More experiments on
tiny-Imagenet dataset can be found in Appendix G.
RL Tasks: We test deep PPO model with SDL on Atari games: KungFuMaster, Tennis and
Seaquest. The deep PPO model concatenates four frames as the input state. The size of input
8
Under review as a conference paper at ICLR 2021
(a) kungfumaster-Clean
(b) kungfumaster-Gaussian-0.1 (c) kungfumaster-Gaussian-0.2 (d) kungfumaster-Gaussian-0.3
Updates
(e) Tennis-Clean	(f) Tennis-Gaussian-0.1 (g) Tennis-Gaussian-0.2 (h) Tennis-Gaussian-0.3
Figure 5: Average Return ± std over 5 independent runs on KungfuMaster and Tennis game with
Gaussian noise with or without SDL
Updates
Updates
(c) KungfuMaster-Laplace-0.2 (d) KungfuMaster-Laplace-0.3
(b) KungfuMaster-Laplace-0.1
(a) KungfuMaster-Clean
(e) Tennis-Clean
Updates
(f) Tennis-Laplace-0.1
Updates
(g) Tennis-Laplace-0.2
(h) Tennis-Laplace-0.3

Figure 6: Average Return ± std over 5 independent runs on KungfuMaster and Tennis game with
Laplace noise with or without SDL
state is 84 × 84 × 4. We construct the structured matrix B ∈ R36×38 by Eq.(4). In this setting, the
orthogonal matrix R corresponds to the convolution parameter of Conv2d(∙) with kernelsize=3 × 3.
The number of free parameters is 630. We set the sparse parameter of our k-sparse SDL as k = 4 in
all the RL experiments.
The return of one episode is the sum of rewards over all steps during the whole episode. We present
the average return over five independent runs on KungFuMaster and Tennis game with Gaussian
noise and Laplace noise in Fig. 5 and Fig. 6, respectively. Results on Seaquest game are shown in
Fig. 16 in the supplement due to the space limitation. We can see that models with SDL achieve a
competitive average return on the clean cases. Moreover, models with SDL obtain higher average
return than vanilla models when the input state is perturbed with noise.
6 Conclusion
We proposed fast sparse coding algorithms for both k-sparse problem and l0 -norm regularization
problems. Our algorithms have a simple closed-form update. We proposed a sparse denoising layer
as a lightweight plug-in for backbone models against noisy input perturbation based on this handy
closed-form. Experiments on both ResNet/DenseNet classification model and deep PPO RL model
showed the effeteness of our SDL against noisy input perturbation and adversarial perturbation.
9
Under review as a conference paper at ICLR 2021
References
Chenglong Bao, Jian-Feng Cai, and Hui Ji. Fast sparsity-based orthogonal dictionary learning for
image restoration. In Proceedings of the IEEE International Conference on Computer Vision, pp.
3384-3391,2013.
Jonathan T Barron. A general and adaptive robust loss function. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 4331-4339, 2019.
Jean Bourgain, Alexey A Glibichuk, and SERGEI VLADIMIROVICH KONYAGIN. Estimates for
the number of sums and products and for exponential sums in fields of prime order. Journal of
the London Mathematical Society, 73(2):380-398, 2006.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated
data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.
Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over
learned dictionaries. IEEE Transactions on Image processing, 15(12):3736-3745, 2006.
Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large
margin deep networks for classification. In Advances in neural information processing systems,
pp. 842-852, 2018.
Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. Iterative quantization: A
procrustean approach to learning binary codes for large-scale image retrieval. IEEE transactions
on pattern analysis and machine intelligence, 35(12):2916-2929, 2012.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. ICLR, 2015.
Minghao Guo, Yuzhe Yang, Rui Xu, Ziwei Liu, and Dahua Lin. When nas meets robustness: In
search of robust architectures against adversarial attacks. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 631-640, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In International Conference on Machine Learning, pp. 1969-1978. PMLR,
2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Yueming Lyu. Spherical structured feature maps for kernel approximation. In International Confer-
ence on Machine Learning, pp. 2256-2264, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ICLR, 2018.
Alexander J Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher Re. Learn-
ing to compose domain-specific transformations for data augmentation. In Advances in neural
information processing systems, pp. 3236-3246, 2017.
Ron Rubinstein, Michael Zibulevsky, and Michael Elad. Efficient implementation of the k-svd algo-
rithm using batch orthogonal matching pursuit. Technical report, Computer Science Department,
Technion, 2008.
Peter H Schonemann. A generalized solution of the orthogonal ProcrUSteS problem. Psychometrika,
31(1):1-10, 1966.
10
Under review as a conference paper at ICLR 2021
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness
the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification
models. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631-648,
2018.
Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. Plug-and-play priors
for model based reconstruction. In 2013 IEEE Global Conference on Signal and Information
Processing, pp. 945-948. IEEE, 2013.
Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Carola-Bibiane Schnlieb, and Hua
Huang. Tuning-free plug-and-play proximal algorithm for inverse imaging problems. ICML,
2020.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. ICML, 2019.
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:
Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing, 26
(7):3142-3155, 2017.
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep
neural networks via stability training. In Proceedings of the ieee conference on computer vision
and pattern recognition, pp. 4480-4488, 2016.
11
Under review as a conference paper at ICLR 2021
A Proof of Proposition 1
Proof. Let ci ∈ C1×n be the ith row of matrix FΛ ∈ Cm×n in Eq.(5). Let vi ∈ R1×2n be the ith
row of matrix B ∈ R2m×2n in Eq.(4). For 1 ≤ i, j ≤ m, i 6= j, we know that
vivi>+m = 0,	(33)
Vi+mV>+m = ViV> = Re(Cic；),	(34)
Vi+mV> = -Vivj+m = Im(Cic；),	(35)
where * denotes the complex conjugate, Re(∙) and Im(∙) denote the real and imaginary parts of the
input complex number.
For a discrete Fourier matrix F, we know that
；
CiCj；
n-1
1X
n
k=0
2π (i — j)ki
e n
1,
0,
ifi=j
otherwise
(36)
When i 6= j, from Eq.(36), we know CiCj； = 0. Thus, we have
vi+mvj+m = vivj = Re(CiCj ) = 0,
vi+mvj = -vivj+m = Im(CiCj ) = 0,
When i = j, we know that vi+mvi>+m = vivi> = Ci Ci； = 1.
Put two cases together, also note that d = 2m, we have BB> = Id .
The l2-norm of the column vector of B is given as
kbj k2 = 1 X (sin2 2∏kij +cos2 2∏kij) = m
n	n	nn
i=1
Thus, We have ∣∣bj∙ ∣∣2 = Pn for j ∈ {1,…，M}
(37)
(38)
(39)
□
B Proof of Theorem 1
Proof. Let Ci ∈ Cm×1 be the ith column of matrix FΛ ∈ Cm×n in Eq.(5). Let bi ∈ R2m×1 be the
ith column of matrix B ∈ R2m×2n in Eq.(4). For1 ≤ i, j ≤ n, i 6= j, we know that
bi> bi+n = 0,	(40)
bi+nbj+n= bi bj = Re(Ci； Cj),	(41)
bi>+nbj = -bi>bj+n= Im(Ci；Cj),	(42)
where * denotes the complex conjugate, Re(∙) and Im(∙) denote the real and imaginary parts of the
input complex number.
It follows that
μ(B) ≤=	max	|b>brl≤ ..max.,.∖cicji| = μ(FA)	(43)
1≤k,r≤2n,k6=r	1≤i,j ≤n,i6=j
From the definition of FΛ in Eq.(5), we know that
1	∖ 、	2πiz(j—i)
μ(FΛ) = 1‹.max. z」c；cj∖ = 1‹.maχ. z - Te-n-	(44)
1≤i,j≤n,i6=j	1≤i,j≤n,i6=j m
1
max —
1≤k≤n-1 m
∑2πizk
e ~
z∈Λ
(45)
12
Under review as a conference paper at ICLR 2021
0 n-1 2(nT)	(m-i)(n-i)
Because A = {g0, g m , g m , ∙ ∙ ∙ , g m } mod n , We know that A is a subgroup of the
multiplicative group {g0 ,g1, ∙ ∙ ∙ , gn-2 } mod n. From BoUrgain et al. (2006), We know that
max
1≤k≤n-1
∑2∏izk
e n
z∈Λ
≤ √n
(46)
Finally, we know that
μ(B) ≤ μ(FΛ) ≤ 且.
m
(47)
□
C Proof of Corollary 1
Proof. Since R>R = RR> = Id and D = R>B, We know that IldjI12 = k bj k 2. From Theorem 1,
we know that ∣∣bj∙ ∣2 = Pm, ∀j ∈ {1,∙∙∙,M}. It follows that ∣∣dj∣∣2 = Ilbjll2 = Pm for
∀j ∈ {1,∙∙∙ ,M}.
From the definition of mutual coherence μ(∙), we know it is rotation invariant. Since D = R>B
with R>R = RR> = Id, we know μ(D) = μ(B). From Theorem 1, we have μ(B) ≤ √n. Thus,
we obtain μ(D) = μ(B) ≤ √.
□
D	Empirical convergence of objective functions
We test our fast dictionary learning algorithms on Lena with image patches (size 12×12). We present
the empirical convergence result of our fast algorithms in Figure 7. It shows that the objective tends
to converge less than fifty iterations.
(a) k-sparse constrained objective
(b) l0-norm regularized objective
Figure 7: Decreasing of the objective functions
(c) li -norm regularized objective
13
Under review as a conference paper at ICLR 2021
E	Demo of Denoised Images
We show the denoised results of our fast sparse coding algorithm on some widely used testing im-
ages. The input images are perturbed by Gaussian noise with std σ = 100. The denoised results are
presented in Figure 8. It shows that our algorithms can reduce the influence of the noisy perturbation
of the images.
(a) Noisy Lena	(b) Noisy peppers	(c) Noisy boat	(d) Noisy barbara
(i) l0 -norm Denoised Lena
(j) l0-norm Denoised peppers
(k) l0-norm Denoised boat
(l) l0-norm Denoised barbara
(m) l1 -norm Denoised Lena
(n) l1-norm Denoised peppers
(o) l1 -norm Denoised boat
(p) l1-norm Denoised barbara
Figure 8: Demo of denoised results of our fast sparse coding algorithm
14
Under review as a conference paper at ICLR 2021
F Experimental results on Classification with Laplace Noise
Figure 9: Mean test accuracy ± std over 5 independent runs on CIFAR10 dataset with Laplace
noise for Densenet and Resnet with or without SDL
(e) Resnet-Clean
(f) Resnet-Laplace-0.1
(g) Resnet-Laplace-0.2
(h) Resnet-Laplace-0.3
Figure 10:	Mean test accuracy ± std over 5 independent runs on CIFAR100 dataset with Laplace
noise for Densenet and Resnet with or without SDL
15
Under review as a conference paper at ICLR 2021
G Experimental results on Tiny-Imagenet dataset
(a) Densenet-Clean
(b) Densenet-Gaussian-0.1
(c) Densenet-Gaussian-0.2
(d) Densenet-Gaussian-0.3
(e) Densenet-Gaussian-0.4
Eooch
(f) Densenet-Gaussian-0.5
Figure 11:	Mean test accuracy ± std over 5 independent runs on Tiny-Imagenet dataset with
Gaussian noise for Densenet with or without SDL
(a) Resnet-Clean
5040302010
>U2⊃UU<-kəi-
(b) Resnet-Gaussian-0.1
(c) Resnet-Gaussian-0.2
(d) Resnet-Gaussian-0.3
Figure 12: Mean test accuracy
Gaussian noise for Resnet with or
(e) Resnet-Gaussian-0.4
(f) Resnet-Gaussian-0.5
± std over 5 independent runs
without SDL
on
Tiny-Imagenet dataset with
16
Under review as a conference paper at ICLR 2021
(a) Densenet-Clean
(c) Densenet-Laplace-0.2
(d) Densenet-Laplace-0.3
(e) Densenet-Laplace-0.4
Figure 13: Mean test accuracy ± std over 5 independent runs on Tiny-Imagenet dataset with Laplace
noise for Densenet with or without SDL

(f) Densenet-Laplace-0.5
(a) Resnet-Clean
(b) Resnet-Laplace-0.1
(c) Resnet-Laplace-0.2
Figure 14: Mean test accuracy ± std over 5 independent runs on Tiny-Imagenet dataset with Laplace
noise for Resnet with or without SDL
17
Under review as a conference paper at ICLR 2021
(a) Densenet-FGSM
201510
>ws⊃uu<⅛①H
Eooch
(b) Resnet-FGSM
Figure 15:	Mean test accuracy ± std over 5 independent runs on Tiny-Imagenet dataset under
FGSM attack for Densenet and Resnet with or without SDL
H Results of RL on Seaquest Game
(a) Clean
(b) Gaussian-0.1	(c) Gaussian-0.2
Updates
(f) Laplace-0.1
(g) Laplace-0.2
(d) Gaussian-0.3
Updates
(h) Laplace-0.3
Figure 16:	Average Return ± std over 5 independent runs on Seaquest game with Gaussian/Laplace
noise with or without SDL
18
Under review as a conference paper at ICLR 2021
I PyTorch Implementation of the SDL Layer
class SparseDenOisingLayer(nn.Module):
def __init__(self, sparseK, B,n):
SuPer(SparseDenOisingLayer, self).___init__()
self.ksize = 2	# kernelsize of Conv2d
self.channel = 3	# channel of input
Outplanes = self.channel*self.ksize*self.ksize
self.B = torch.from_numpy(B).float().cuda()
self.n = n
self.outplanes = Outplanes
self.sparseK = sparseK
self.register_parameter(name='U’, param=torch.nn.Parameter(torch.
randn(outplanes,outplanes).
cuda()))
def forward(self, x):
#	Cayley Mapping to compute orthogonal matrix R
KA = torch.triu(self.U,diagonal=1 )
tmpA = KA- KA.t()
tmpB = torch.eye(self.outplanes,self.outplanes).cuda()-tmpA
KU = torch.mm( (torch.eye(self.outplanes,self.outplanes).cuda()+
tmpA ) , torch.inverse( tmpB
) )	# orthogonal matrix R
weight = KU.view(self.outplanes,self.channel,self.ksize,self.
ksize) #ReshaPe into the
Conv2d parameter
out = F.conv2d(x,weight, stride=1, padding = self.ksize-1)
out = out.permute(0,2,3,1)
out = torch.matmul(out,self.B)
#	(function f) k-max pooling w.r.t the absolute value
index = torch.abs(out).topk(self.sparseK, dim = 3)
mask = torch.zeros(out.shape).cuda()
mask.scatter_(3, index[1], 1.)
out = out* mask
#	#
out = torch.matmul(out,torch.transpose(self.B, 0, 1))
out = out.permute(0,3,1,2)
out = F.conv_transpose2d(out,weight, stride=1, padding = self.
ksize-1 )/(self.ksize*self.
ksize)
return out # reconstruction of the input
19