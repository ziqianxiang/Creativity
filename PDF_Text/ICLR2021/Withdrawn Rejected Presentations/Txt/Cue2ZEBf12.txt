Under review as a conference paper at ICLR 2021
Towards Adversarial Robustness of Bayesian
Neural Network through Hierarchical Vari-
ational Inference
Anonymous authors
Paper under double-blind review
Ab stract
Recent works have applied Bayesian Neural Network (BNN) to adversarial train-
ing, and shown the improvement of adversarial robustness via the BNN’s strength
of stochastic gradient defense. However, we have found that in general, the BNN
loses its stochasticity after its training with the BNN’s posterior. As a result, the
lack of the stochasticity leads to weak regularization effect to the BNN, which
increases KL divergence in ELBO from variational inference. In this paper, we
propose an enhanced Bayesian regularizer through hierarchical variational infer-
ence in order to boost adversarial robustness against gradient-based attack. Fur-
thermore, we also prove that the proposed method allows the BNN’s stochasticity
to be elevated with the reduced KL divergence. Exhaustive experiment results
demonstrate the effectiveness of the proposed method by showing the improve-
ment of adversarial robustness, compared with adversarial training (Madry et al.,
2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD
attack to the L∞ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.
1	Introduction
Deep neural networks have achieved impressive performance in a wide variety of machine learning
tasks. Despite the breakthrough outcomes, deep neural networks are easily deceived from adver-
sarial attack with the carefully crafted perturbations (Szegedy et al., 2014; Goodfellow et al., 2015;
Chen et al., 2017; Carlini & Wagner, 2017; Papernot et al., 2017; Eykholt et al., 2018; Madry et al.,
2018). Injecting these perturbations into clean inputs (i.e., adversarial examples), which are imper-
ceptible to the human eyes, fools the estimators in the deep neural networks. Weak reliability due to
the invisible perturbations has affected security problems in deep learning applications (Apruzzese
et al., 2019; Wang et al., 2019b; Sagduyu et al., 2019; Rosenberg et al., 2020).
To defend such adversarial examples, many algorithms have been studied to improve adversarial
robustness so far. Adversarial training, where deep neural networks are trained on adversarial ex-
amples, is one of the few defense strategies against strong adversarial attacks (Huang et al., 2015;
Zantedeschi et al., 2017; Kurakin et al., 2017; Madry et al., 2018; Athalye et al., 2018a; Liu et al.,
2018). Among them, Madry et al. (2018) has shown that adversarially trained networks can be ro-
bust to white-box attacks with the knowledge of the network parameters. Besides, most of the above
studies have agreed with that adversarial training shows an effective adversarial robustness against
several white-box attacks.
Meanwhile, adversarial training and BNN have been combined to improve adversarial robustness
with stochastic approach through variational inference. In fact, the variational inference maximizes
Evidence Lower Bound (ELBO) to find an approximate posterior closely following the true poste-
rior for the machine learning tasks (Graves, 2011; Kingma & Welling, 2014; Blundell et al., 2015;
Hemandez-Lobato & Adams, 2015). Based on the variational inference, adversarial training with
BNN has accomplished the achievement of adversarial robustness by implicitly using the approxi-
mate posterior against the adversarial perturbations (Mescheder et al., 2017; Ye & Zhu, 2018). They
have focused on training the network parameters or spaces itself on the maximum ELBO without
obtaining the approximate posterior directly.
1
Under review as a conference paper at ICLR 2021
Contrary to the above studies, Liu et al. (2019) presents an adversarial training with BNN, called
“adversarial-BNN” to deal with the approximate posterior explicitly. They straightforwardly learn
Gaussian parameters (e.g., mean and variance) of the approximate posterior as follows: W 〜
N(μ, σ2), instead of the weight parameters. Alternatively, the weight parameters are sampled by
the approximate posterior, such that W = μ + σe, where the stochastic sampler E is gererated from
E 〜N(0,1). The stochastic sampler E provides the change of the weight parameters with the learned
Gaussian parameters. The variation of them creates stochastic gradient, helping the improvement of
adversarial robustness (Carbone et al., 2020).
However, we find that in general, the approximate posterior’s variance converges to zero-like small
value as follows: W 〜N(μ, σ2 ≈ 0) after training the BNN with its posterior. Although the
stochastic sampler helps the weight parameters to change, they become fixed-like parameters, such
that W = μ + σ(≈ 0)E. The lack of their stochasticity causes the BNN’s stochasticity to be vanished
so that the BNN cannot easily respond to slightly different inputs within the same class. In other
words, the vanished stochasticity breaks the regularization effect in the BNN, which increases KL
divergence in the ELBO. The broken BNN regularizer produces an ill-posed posterior, thus resulting
in weak adversarial robustness. This is because the broken regularizer hinders the maximum ELBO
from approximating the true posterior against the adversarial perturbations. Therefore, an enhanced
BNN regularizer is required to better approximate the true posterior for adversarial robustness.
In this paper, we present the enhanced Bayesian regularizer through hierarchical variational infer-
ence in order to boost adversarial robustness compared to the BNN regularizer from variational
inference. Furthermore, we also prove that the proposed method significantly intensifies the BNN’s
stochasticity by introducing a closed form approximation of conjugate prior for the true posterior. In
the end, we validate the effectiveness of the proposed method by showing the improvement of adver-
sarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu
et al., 2019) under PGD attack as well as EOT-PGD attack to the L∞ perturbation on CIFAR-10/100,
STL-10, and Tiny-ImageNet.
Our contributions of this paper can be summarized into two-fold as follows.
•	We newly design an enhanced Bayesian regularizer through hierarchical variational infer-
ence built with a concept of the conjugate prior, and verify that the proposed method further
strengthens the BNN’s stochasticity, compared to the BNN regularizer based on variational
inference.
•	We conduct exhaustive experiments to validate the effectiveness of the proposed method
by adversarial robustness, and exhibit the outstanding performance compared with adver-
sarial training and adversarial-BNN under both PGD attack and EOT-PGD attack on four
benchmark datasets: CIFAR-10/100, STL-10, and Tiny-ImageNet.
2	Preliminary
In this section, we specify the notations used in our paper at first and summarize the related works
on adversarial attack/defense, and adversarial training with BNN.
Notations. Let x denote the clean image from a given dataset, and y denote the class label corre-
sponding to the clean image. Let D and Dadv indicate each clean dataset and adversarial dataset,
such that (χ,y)〜D and (Xadv ,y)〜Dadv. A deep neural network f parameterized by weight
parameters W is denoted by fw (x). Adversarial examples are represented by xadv = x + δ, where
δ denotes the adversarial perturbations. In order to align the experiments in the previous works, we
use the cross-entropy loss J (fw (x), y) for image classification. Moreover, we regard δ as the L∞
perturbation within Y-ball, such that kδ∣∣∞ ≤ γ. Here, k∙k∞ describes the L∞.
2.1	Adversarial attack/defense
Adversarial Attacks. The goal of adversarial attacks is generating the adversarial examples to
deceive the prediction of the deep neural networks. Most of them produce the adversarial examples
by the gradient of the loss function over the input. Goodfellow et al. (2015) introduces a single-step
attack called Fast Gradient Sign Method (FGSM). Kurakin et al. (2017) proposes iterative-FGSM
with multiple-step attack. Further, Carlini & Wagner (2017) presents C&W attack to overcome
2
Under review as a conference paper at ICLR 2021
defensive distillation (Papernot et al., 2016), encompassing a range of attacks cast from the same
optimization framework. On the other hand, Athalye et al. (2018a) analyzes the effectiveness of
Projected Gradient Descent (PGD) method (Madry et al., 2018) to perform adversarial attack to the
L∞ perturbation. PGD attack is an iterative algorithm by computing the gradient of the loss in the
direction of the highest loss and projecting it back to the L∞ perturbation around the clean image.
We use the PGD attack for the experiments of adversarial robustness, which can be written as:
xa+vι = Yhxadv + η ∙ sign (VxJ(fw(x),y)∣x=xadv i,	(1)
x,γ
where Qx,γ is the function to project its argument to the surface of x’s γ-neighbor ball
x∣ xadv - x∞ ≤ γ , and η denotes step size. For the adversarial defense utilizing random-
ized classifiers, an adaptive attack such as Expectation over Transformation (EOT) is known to be
effective because it allows an attacker to compute the actual gradient over the expected transfor-
mation to the stochastic classifiers (Athalye et al., 2018a;b). Inspired from the EOT attack, we can
modify Eq. (1) and adjust more efficient PGD attack (Zimmermann, 2019).
xa+v = Y hxadv + η ∙ sign (E [vxJ(fw(x),y)∣x
=xadvii,	(2)
x,γ
Zimmermann (2019) applies the averaged-gradient over the multiple weight parameters to generate
the adversarial examples, and shows that this efficient PGD attack degrades the BNN’s adversarial
robustness. This is because the averaged-gradient can attack all of the possibly sampled weight
parameters in the BNN. In other words, the averaged-gradient attack weakens the BNN’s advantage
of stochastic gradient defense. In this paper, we call the efficient PGD attack, namely EOT-PGD
attack, and take it to validate the effectiveness of the proposed method.
Adversarial Defense. The aim of adversarial defense is to secure deep learning networks against
adversarial attacks. There are generally three main categories of defense strategies (Silva & Najafi-
rad, 2020; Hao-Chen et al., 2020). (1) Gradient masking/obfuscation: a defender intentionally hides
the gradient information of deep neural networks in order to confuse adversaries, because most at-
tack algorithms are based on the classifier’s gradient information (Papernot et al., 2016; Buckman
et al., 2018; Guo et al., 2018; Dhillon et al., 2018). (2) Adversarial detections: they focus on dis-
tinguishing whether the input is benign or adversarial (Metzen et al., 2017; Grosse et al., 2017; Xu
et al., 2017). When they succeed to detect the adversarial attack in the input, the classifiers stop mak-
ing a decision. (3) Robust optimization: it is a well-known paradigm that aims to obtain solutions
under bounded feasible regions. Especially, its main interest from an adversarial perspective, is im-
proving the classifier’s robustness by changing the learning scheme of deep neural networks (Cisse
et al., 2017; Hein & Andriushchenko, 2017; Raghunathan et al., 2018; Wong & Kolter, 2018).
Adversarial training (Goodfellow et al., 2015; Kurakin et al., 2017; Madry et al., 2018) is one of the
famous robust optimization methods in deep learning fields, which allows the deep neural network
to learn robust parameters against the adversarial examples. Madry et al.(2018) tries to find the
optimal weight parameters against adversarial dataset Dadv to improve adversarial robustness under
PGD attack to the L∞ perturbation, by minimizing the pre-defined loss L for the machine learning
tasks. The formulation can be written as:
w* = argmin	E	JL (fw(xadv),y)] ,	(3)
W	(xadv,y)〜Dadv
where xadv is generated from the PGD attack as described in Eq. (1). The optimal weight parameters
w* get the capacity of adversarial defense to confront the adversarial perturbations.
2.2	Adversarial training with BNN
Many studies have mingled with adversarial training and BNN through variational inference. Here,
variational inference imposes the probabilistic approach on the network parameters or spaces to ob-
tain a closed form approximation of the true posterior based on the maximum ELBO, thus leading to
well-posed inference (Welling & Teh, 2011; Paisley et al., 2012; Hoffman et al., 2013; Salimans &
Knowles, 2013; Kingma & Welling, 2014; Rezende et al., 2014). Likewise, finding the closed form
from an adversarial perspective can lead to well-posed inference for adversarial robustness. Thus,
3
Under review as a conference paper at ICLR 2021
(a) adversarial-BNN
Figure 1: Diagrams that describe the methods of sampling the weight parameters for (a) with
adversarial-BNN and (b) the proposed method. The weight parameters in (a) are sampled from
the Gaussian distribution q(w) as an approximate posterior through variational inference, and those
in (b) are sampled from the Normal-Inverse Gamma distribution q(μ, σ2) as an approximate conju-
gate prior through hierarchical variational inference. Red circles denote the learning parameters in
the BNN, black circles denote the random variables sampled from the described probability distri-
butions, and the black arrows indicate the sampling order.
adversarial training with BNN tries to approximate a true posterior against adversarial perturbations:
p(w | Dadv ) to indirectly utilize it to conduct the well-posed inference for adversarial robustness.
Fundamentally, the reason we should approximate the true posterior is attributed by its intractability
because of a high dimensional dependency of adversarial dataset p(Dadv). Thus, variational infer-
ence for adversarial robustness introduces an approximate posterior in the closed form of q(w) as a
practical estimator to approximate the true posterior, where the formulation can be written as:
log p(y |xadv) = DKL(q(w) || p(w | Dadv))
+ E	log p(y | xadv,w) - DKL (q(w) || p(w)) .
W 〜q(w)
X---------------------------------------------}
(4)
^z∖^^^
ELBO
We should minimize KL divergence DKL q(w) || p(w | Dadv) above to make the probability-
distance close between the two probability distributions. But, this KL divergence is still intractable
measurement, and we can alternatively maximize the ELBO in Eq. (4), which has the same effect
to minimize this KL divergence. This is because the log-probability log p(y | xadv) in Eq. (4) is
consistent with the change of the weight parameters w. The formulation of variational inference
also can be represented to lower bound as: log p(y | xadv ) ≥ ELBO.
Some of the studies (Mescheder et al., 2017; Ye & Zhu, 2018; Wang et al., 2019a; Carbone et al.,
2020) are to improve adversarial robustness by maximizing the ELBO. Others (Feinman et al., 2017;
Smith & Gal, 2018; Wang et al., 2018) are to detect the adversarial examples by predicting Bayesian
uncertainty (Gal & Ghahramani, 2016; Kendall & Gal, 2017; Pearce et al., 2020), which are also
based on the maximum ELBO. These previous studies have implicitly examined the approximate
posterior to better learn the network parameters or spaces, instead of directly shaping the approxi-
mate posterior.
Unlike the previous studies, Liu et al. (2019) proposes “adversarial-BNN” which combines adver-
sarial training and the BNN to improve adversarial robustness by shaping the approximate posterior
explicitly. Liu et al. designs its posterior q(w) = N(μ, σ2) and considers Gaussian parameters (e.g.,
mean, variance) of it as learning parameters. In this work, the first term in the ELBO represents the
expected log-likelihood calculated from the cross-entropy as follows:方 PN=I J (/仅⑸(Xadv), y)
by Monte-Carlo sampling, where N denotes the sampling number. As shown in Fig. 1(a), the
weight parameter w(i) is sampled by the approximate posterior,s mean and variance μ, σ2, such that
w(i) = μ + σe(iL Here, e(i) is the stochastic sampler, such that e(i)〜N(0,1).
The second term in the ELBO normally represents a regularization term for an approximate posterior
as the BNN’s posterior. This term can be computed from the KL divergence between the approxi-
mate posterior q(w) and a prior p(w) which is usually set to N(0, 1) for variational inference. In
other words, this KL divergence has a role ofa BNN regularizer in ELBO, and this regularizer is an
4
Under review as a conference paper at ICLR 2021
important factor for adversarial robustness. This is because once the BNN regularizer is increased
unexpectedly, the minimum ELBO produces an ill-posed posterior, creating weak adversarial robust-
ness. In contrast, as the BNN regularizer is reduced, the maximum ELBO allows for the well-posed
inference due to strong regularization effect bringing in better approximating the true posterior. In
addition to the aspect of the ELBO, there are many studies showing that high regularization perfor-
mance improves adversarial robustness (Saito et al., 2018; Yu et al., 2019; Zhang & Liang, 2019;
SUn et al., 2019; Terjek, 2020; Zhang et al., 2020).
3	Proposed Method
Adversarial training with BNN has expanded generic probability to adversarial probability, and has
shown the improvement of adversarial robUstness. However, as mentioned in the Section 1, the van-
ished stochasticity declines adversarial defense fUnctionality of the BNN. To investigate the BNN’s
stochasticity, we introdUce a concept of the conjUgate prior, and develop a new ELBO motivated by
the conjUgate prior. The conjUgate prior can provide a key to how the GaUssian parameters of the
approximate posterior q(w) are trained dUring adversarial training. Based on oUr findings in the con-
jUgate view, we will design an enhanced Bayesian regUlarizer that solves the stochasticity problem
caUsing weak adversarial robUstness. Before we explain the proposed method, we firstly introdUce
two parts: (1) the conjUgate prior of the GaUssian posterior, (2) hierarchical variational inference for
bUilding a new ELBO with this concept of conjUgate prior.
Gaussian Conjugate Prior. The conjUgate prior of the GaUssian posterior is called the GaUssian
conjUgate prior. In the conjUgate view, the trUe posterior p(w | D) is no longer posterior, bUt is the
likelihood P(W | D, μ, σ2) given the Gaussian parameters μ, σ2 when assuming the true posterior
is Gaussian. Here, Gaussian conjugate prior is the probability distribution p(μ,σ2 | D) for the
Gaussian parameters of the true posterior. The conjugate prior takes an benefit to approximate the
true posterior by modeling such Gaussian parameters, thus guiding well-posed posterior. But, the
conjugate prior is also intractable to deal with, similar to the true posterior because they are same
distribution family. Therefore, We introduce an approximate conjugate prior q(μ, σ2), in the shape
of Normal-Inverse Gamma distribution, to approximate the conjugate prior of the true posterior. The
formulation of the approximate conjugate prior can be written as follows:
q(μ, σ2) = N (μ | μo, σ0) IG (σ2 | ν, ψ) .	(5)
It can be factorized into the Normal distribution q(μ) = N (μ | μo, σ0) and the Inverse Gamma dis-
tribution q(σ2 ) = IG σ2 | ν, ψ independently by mean-field approximation (Kingma & Welling,
2014). In the Normal distribution, μo denotes the mean prior knowledge of the approximate poste-
rior q(w), and σ02 denotes the variance of the approximate posterior’s mean. In addition, ν, in the
Inverse Gamma distribution, denotes the degree of freedom on positive real set which determines the
shape of this distribution, and ψ denotes the variance prior knowledge of the approximate posterior.
Here, the approximate conjugate prior can sample each of the Gaussian parameters independently.
The Normal distribution samples the approximate posterior’s mean, and the Inverse Gamma distribu-
tion samples its variance. As illustratedinFig. 1(b), tractable parameters μo, σ0, ψ, and V in q(μ, σ2)
can generate the mean μ and the variance σ2 of the approximate posterior, where μ = μo + σ0%,
and σ2 =25,such that e* 〜N(0,1),吃 〜IG(ν, 1). In addition, the generated mean and the
variance are both used to sample the weight parameters: W = μo + σ0Eμ + (少% )1/2U We call
it Normal-Inverse Gamma sampling. More importantly, the approximate conjugate prior has more
number of the tractable parameters than the approximate posterior q(W), which significantly help
sampling the weight parameters for complex representation.
Hierarchical Variational Inference. Next, we tackle the way the approximate conjugate prior
makes a closed form approximation of the conjugate prior as a same family of true posterior. We
modify the ELBO in Eq. (4) to build up a new ELBO with the approximate conjugate prior, namely
hierarchical-ELBO, which can be written as:
E	[log P(y | XadV, μ + σ6)] - DKL(q(μ,σ2) || p(μ,σ2)).	(6)
(μ,σ2)-q(μ,σ2 )
The first term stands for the classification performance taken from the cross-entropy. The sec-
ond term represents the BNN regularizer in hierarchical-ELBO equivalent to that in the ELBO as
5
Under review as a conference paper at ICLR 2021
depicted in Eq. (4). The second term can be computed by the KL divergence between the approx-
imate conjugate prior q(μ, σ2) and a hierarchical prior p(μ, σ2). Note that the hierarchical prior
is factorized into the mean prior p(μ | σ2) and the variance prior p(σ2). Then, the above KL di-
vergence can be decomposed into two equations: (1) Eσ2〜q92) [DκL(q(μ) || p(μ | σ2))] and (2)
DKL (q(σ2) || p(σ2)) (see Appendix A). In order to make two equations tractable, both the mean
prior and the variance prior should be set, similar to the setting of the prior p(w) = N (0, 1) in the
ELBO from variational inference. Since the prior is Gaussian, the hierarchical prior can be formed
to Gaussian conjugate prior, Normal-Inverse Gamma, where we keep the prior knowledge of the
mean and the variance with zero and one, respectively. We set the mean prior to N 0, λ-1σ2
for (1), where λ denotes inverse scale factor to the variance σ2 . In addition, the variance prior is
set to IG(ν, 1) for (2). Here, ν is set to the same parameters both in q(σ2) and p(σ2) to reduce
computational complexity in calculating (2).
On the above settings for the hierarchical prior p(μ, σ2), the second term can be tractably expanded
with the summation of the two equations (see Appendix B), which can be written as:
2νj + 1
DκL(q(μ,σ2) || p(μ,σ2))=需 P=MjHψj+ +log jjɪ - j1 - 2 G(Vj)],⑺
where M denotes the number of weight parameters in the deep neural networks, and j represents
the j th parameters for the j th approximate conjugate prior. The function G has the formulation of
Digamma function, such that G(Vj) = Γ0 (Vj)∕Γ(Vj).
ELBO vs Hierarchical-ELBO. Until we control the parameters λ, V of the hierarchical prior
p(μ,σ2), the ELBO is essentially same to the hierarchical-ELBO. Also, the BNN regularizer in
the ELBO is identical to that in the hierarchical-ELBO. This is because it is nothing but we just
have replaced the weight parameters in the ELBO with the Gaussian parameters for the extension,
as described in Eq. (6). Different from the fixed prior p(w) = N(0, 1) in the ELBO, controllable
parameters λ, V of the hierarchical prior p(μ, σ2) can allow the BNN regularizer in the hierarchical-
ELBO to change adaptively. As a consequence, properly chosen parameters λ and V can reduce
the KL divergence, helping to well-posed posterior with the maximum hierarchical-ELBO. The
formulation of the hierarchical-ELBO can be also represented to lower bound with the ELBO as
follows: log p(y | xadv) ≥ hierarchical-ELBO ≥ ELBO, where the hierarchical-ELBO better
approximates the true posterior than the ELBO.
Enhanced Bayesian Regularizer. Now, we focus on the optimization of the BNN regularizer
in the hierarchical-ELBO to reduce the KL divergence. By gradient results over the controllable
parameters: 衾DKL = 0 and 昼DKL = 0, We can easily obtain the optimal value λ* = ν92+^),
but the optimal value V* is intractable to acquire, since the gradient result is intractable as follows:
2 ddVG(v)-*=ψ + log ψ - 1. We should indirectly find the optimal value V* by applying the
above gradient results to the KL divergence, which can be written as:
DκL(q(μ,σ2) Il p(μ,σ2))=吉PM=ι[2log (1 + σ,j) + 2jdVG(Vj)- 2G(Vj)+ 2logVj- 2],⑻
、	-_z-	J
H(Vj)
where the function H(Vj) is decreasing and convergent function (see Appendix C). We examine this
function is converged to zero, as Vj gets large enough. With the optimal value V * infinite, the KL
divergence in the hierarchical-ELBO can be definitely reduced.
Finally, we propose the enhanced Bayesian regularizer as follows:
焉 ΣM=ι[1 log (1 + σ2^)]. The
proposed method provides more tight lower bound than the ELBO, such that the formulation can be
described as follows: log p(y | xadv) ≥ hierarchical-ELBO ELBO. Based on the tight lower
bound, the enhanced Bayesian regularizer aids to the acquisition of the well-posed posterior to boost
adversarial robustness. Consequently, we have verified that the proposed method considerably inten-
sifies the BNN regularizer by designing the hierarchical-ELBO in the conjugate view. In practical,
we can compute the proposed method for each training iteration, such as Expectation-Maximization
(EM) step. We describe the algorithm for the proposed method below.
6
Under review as a conference paper at ICLR 2021
Algorithm 1 Code for adversarial training in the BNN with the enhanced Bayesian regularizer
1	Require: α (learning rate), μo and σ0 (learning parameters)	
2	: for (x, y) in D do	. Training procedure per epoch
3	w - μo + σo6, E 〜N(0,1)	. Sample the weight parameters
4	Xadv — attack(x, y, W)	. PGD attack to generate adversarial examples
5	y 一 fw(χadv)	. Predict the outputs given the adversarial examples
6	Li 一 J(y,y)	. Calculate the cross-entropy loss
7	L - pM=ι2 iog(1+σ0j)	. Calculate the proposed method
8	L-Li + L2	",3	. Calculate the hierarchical-ELBO in Eq. (6)
9 10	(μ0, σO) J (μ0 - α∂L, σ0 - α舞) : end for	. Update the learning parameters
Theoretical Analysis. On the above optimization with infinite V*, the Inverse Gamma dis-
tributions are converged to delta probability distribution, such that lim“*→∞ q(σ2) = δ(σ2),
limν*→∞p(σ2) = δ(σ2). Hence, We can discover that the BNN from the ELBO generally loses its
stochasticity after its training with the approximate posterior q(w) via the zero convergence variance.
For the hierarchical-ELBO, the tractable parameter ψ is not required anymore during sampling the
weight parameters in Fig. 1(b). This is because σ2 is sampled to zero from delta distribution such
that ∈σ2 〜 δ(∈σ2) = limν*→∞IG(ν, 1). Thus, only the Normal distribution q(μ) = N(μo,σ0)
is involved in sampling the weight parameters as follows: W = μo + σo6. Here, according to the
decomposition of the second term in Eq. (6), q(μ) is learned to follow the mean prior p(μ | σ2)
with the optimal parameter λ*, such that limλ→λ* p(μ | σ2) = N(0, k(μ2 + σ2)), where k is
statistically proven to be above one (see Appendix d). Then, the variance σ02 of q(μ) follows the
variance k(μ2 + σ2) of the mean prior, which is definitely larger than the variance σ0. In short,
training the BNN with the approximate conjugate prior leads to increase the change in the weight
parameters, such that W = μo + σ0(↑)e. Therefore, we verify that the proposed method allows for
the enhancement of the BNN’s stochasticity through hierarchical-ELBO.
4 Experiments
In this section, we validate adversarial robustness of the proposed method compared to strong base-
lines in various datasets. Since our method is inspired by adversarial training (Madry et al., 2018),
BNN (Blundell et al., 2015), and adversarial-BNN (Liu et al., 2019), these three comparison meth-
ods are natural baseline methods.
Implementation Details. We conduct exhaustive experiments on not only a standard low dimen-
sional dataset (CIFAR-10), but also more challenging datasets such as CIFAR-100, STL-10 (Coates
et al., 2011), and Tiny-ImageNet (Le & Yang, 2015). STL-10 has 5,000 training images and 8,000
testing images with a size of 96×96 pixels. Tiny-ImageNet is a small subset of ImageNet dataset,
containing 100,000 training images, 10,000 validation images, and 10,000 testing images separated
in 200 different classes, whose dimensions are 64×64 pixels. Two vanilla networks, VGG and
Model A1, are adopted as our baseline networks. Model A is trained for STL-10 while the others are
trained via VGG. Each baseline network is trained with L∞ 0.03 perturbation magnitude during ad-
versarial training stage for comparing baseline methods and Ours. For validation, we apply the PGD
attack and EOT-PGD attack to the test set, and adjust L∞ perturbation magnitude of the adversarial
perturbation δ from 0 to 0.03 within the perturbation magnitude used in adversarial training.
Computational complexity. According to Theoretical Analysis above, the number of learning
parameters in the proposed method is same number of those in the adversarial-BNN. Thus, there is
no excessive computational complexity to implement the proposed method.
Hyper-parameter. To fairly validate adversarial robustness, we equally set hyper-parameter set-
tings that learning rate equals to 0.01 with Adam (Kingma & Ba, 2015), and the learning parameters
(mean and variance) are initialized to each U(-1∕√C, 1∕√c) (c : channel number) and 0.1 (what
1Publicly available at https://github.com/aaron-xichen/pytorch-playground/tree/
master/stl10
7
Under review as a conference paper at ICLR 2021
Table 1:	The statistical summaries for the weight parameters compared with adversarial-BNN and
Ours. We include mean, variance, and KL divergence (KLD). We describe the average of KLD
for all layers in deep neural networks. Outperforming statistical results are marked in bold. The
proposed method provides non-zero mean, higher variance (see Appendix F), and lower KLD.
CIFAR-10	STL-10	CIFAR-100	Tiny-ImageNet
Mean	Variance	KLD	Mean	Variance	KLD	Mean	Variance	KLD	Mean	Variance	KLD
adv-BNN 0	0.073	6.25	0	0.085	1.28	0	0.073	8.87	0	0.069	15.53
adv-Ours	0.004	0.141	1.12	0.003	0.101	0.96	0.003	0.118	1.01	0.015	0.226	1.42
Perturtatlon Magnitude
Perturtatlon Magnitude
Perturbation Magnitude
Perturbation Magnitude
Figure 2: Comparison of classification accuracy under PGD attack (first row) and EOT-PGD attack
(second row) to the L∞ perturbation magnitude on different datasets: CIFAR-10, STL-10, CIFAR-
100, and Tiny-ImageNet.
we are interested in after training). For PGD attack, we set the number of steps T to 10 in adversarial
training stage and 30 in test stage, and set the step size η to 竽 δ. For EOT-PGD attack, we equally
use the hyper-parameters of PGD attack and set the number of samples to 10.
Stochasticity and Regularization. Tab. 1 shows comparison of statistical summaries for the weight
parameters such as mean, variance and KL divergence in adversarial training with BNN and Ours.
The summaries shows that naively putting BNN to adversarial training reduces the variance, and
even fails to maintain the initial variance 0.1. In addition, the decreased variance causes higher KL
divergence. This phenomenon shows an empirical evidence on the recall of the vanished stochas-
ticity from adversarial training with BNN, where the broken BNN regularizer increases the KL
divergence. Conversely, the proposed method shows comparatively higher variance (more increased
variance than the initial variance) and lower KL divergence (6.7 times lower on average across all
datasets). Therefore, we expect the proposed method to have a strong regularization effect so that
the robustness discrepancy between adversarial-BNN and Ours becomes greater in large datasets
with more classes such as CIFAR-100 and Tiny-ImageNet.
Robustness under white-box PGD and EOT-PGD attack. First, we compare adversarially trained
baseline networks with baseline methods and Ours by adversarial robustness under the white-box
PGD attack (first row in the Fig. 2) and EOT-PGD (second row in the Fig. 2) to the L∞ perturbation.
As illustrated in this figure, adversarial-BNN has greater performance on STL-10 than naive adver-
sarial training, but it has lower performances on the other datasets. While, the proposed method
outperforms baseline methods to all datasets.
Ablation studies. We conduct three ablation studies: (1) attacking with higher perturbation mag-
nitude (δ > 0.03) than the perturbation magnitude of adversarial training, (2) adversarial training
with large perturbation magnitude (δ = 0.06) on WideResNet (Zagoruyko & Komodakis, 2016),
and (3) comparison of clean training and adversarial training with BNN and Ours. The two abla-
tion studies (1) and (2) are represented in Fig. 2 where the proposed method generally surpasses
baseline methods, although it converges to the performances of baseline methods in much higher
perturbation magnitude. For the ablation study (3), the statistical summaries of clean training on
8
Under review as a conference paper at ICLR 2021
Table 2:	The statistical summaries between clean training without adversarial examples and adver-
sarial training on WideResNet. The descriptions in this table are the same as Tab. 1.
Clean training (δ = 0)	Adversarial training (δ = 0.06)
CIFAR-10	CIFAR-100	CIFAR-10	CIFAR-100
Mean	Variance	KLD	Mean	Variance	KLD	Mean	Variance	KLD	Mean	Variance	KLD
BNN	0	0.077	5.82	0	0.065	23.59	adv-BNN	0	0.077	5.82	0	0.075	7.48
Ours	0.044	0.178	1.87	0.052	0.122	2.24	adv-Ours	0.015	0.192	1.01	0.031	0.146	1.42

0.02	0.04	0.06	0.08 0
Perturbation Magnitude
0.02	0.04	0.06	0.0β
Perturbation Magnitude
0.03	0.04	0.05	0.06	0-07	0.08 0.03	0.04	0.05	0.06	0.07	0.08
Perturbation Magnitude	Perturbation Magnitude
AoBJnOOVWQJ.
(a) higher perturbation magnitude on Baseline networks (6 = 0.03)
(b) adversarial training on WideReSNet (6 = 0.06)
Figure 3: Comparison of classification accuracy for two ablation studies: (a) attacking with higher
perturbation magnitude (δ > 0.03) than the maximum magnitude described in Fig. 2 on baseline
networks (VGG, Model A) for CIFAR-10 and STL-10, (b) adversarial training with large perturba-
tion magnitude (δ = 0.06) on WideResNet for CIFAR-10 and CIFAR-100. These graphs are under
PGD attack (first row) and EOT-PGD attack (second row) to the L∞ perturbation magnitude.
baseline networks (VGG, Model A) are illustrated in Appendix E. Further, Tab. 2 compares those
of clean training and adversarial training with BNN and Ours on WideResNet. They show that both
variances of BNN are lower than those of Ours. That is, this vanished stochasticity also happens in
normal BNN when clean training. Here, we can analyze that the superior robustness of Ours does
not come from the improvement of classification accuracy by itself.
Discussion. We tackle why the proposed method is effective to EOT-Iike attack. In general, a ran-
domized network provides a stochastic tolerance obfuscating the gradient-based attack, which uses
a single sample of the randomness. Especially, the BNN,s stochasticity makes an attacker confused
to incorrectly estimate the true gradient. But, EOT-like attack easily bypasses this stochasticity by
taking the expectation over multiple weight parameters. However, our re-birth of BNN’s stochastic-
ity against EOT-like attack can be mathematically represented by Taylor series (see Appendix G).
Now that it is possible to demonstrate the effectiveness of the proposed method against EOT-like
attack, we can insist that the proposed method is not just obfuscating the gradient, but helping the
deep neural networks to have strong adversarial robustness.
5 Conclusion
As mentioned in Theoretical Analysis at the Section 3, we have found that the BNN generally loses
its stochasticity after its training with the BNN’s posterior via the zero convergence variance. To
solve it, we newly design the enhanced Bayesian regularizer with improved stochasticity through
the hierarchical-ELBO under a concept of the conjugate prior. In addition, we have analyzed the
improved stochasticity is effective to EOT-like attack. We believe the hierarchical-ELBO better
approximates the true posterior against adversarial perturbations by the tight lower bound more than
the ELBO, thus boosting the adversarial robustness. Further progress can be made by investigating
the hierarchical-ELBO to explore generative models, such as Variational Auto-Encoder (VAE) and
Generative Adversarial Network (GAN) for scalable machine learning tasks.
9
Under review as a conference paper at ICLR 2021
References
G. Apruzzese, M. Colajanni, L. Ferretti, and M. Marchetti. Addressing adversarial attacks against
security systems based on machine learning. In 2019 11th International Conference on Cyber
Conflict (CyCon), volume 900, pp. 1-18, 2019.
Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In ICML, pp. 274-283, 2018a. URL
http://proceedings.mlr.press/v80/athalye18a.html.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversar-
ial examples. volume 80 of Proceedings of Machine Learning Research, pp. 284-293, Stock-
holmsmassan, Stockholm Sweden, 10-15 JUl 2018b. PMLR. URL http://Proceedings.
mlr.press/v80/athalye18b.html.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. In Proceedings of the 32nd International Conference on International Confer-
ence on Machine Learning - Volume 37, ICML’15, pp. 1613-1622. JMLR.org, 2015.
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot
way to resist adversarial examples. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=S18Su--CW.
Ginevra Carbone, Matthew Wicker, Luca Laurenti, Andrea Patane, Luca Bortolussi, and Guido
Sanguinetti. Robustness of Bayesian Neural Networks to Gradient-Based Attacks. arXiv e-prints,
art. arXiv:2002.04359, February 2020.
N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pp. 39-57, 2017.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypass-
ing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelli-
gence and Security, AISec ’17, pp. 3-14, New York, NY, USA, 2017. Association for Com-
puting Machinery. ISBN 9781450352024. doi: 10.1145/3128572.3140444. URL https:
//doi.org/10.1145/3128572.3140444.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order op-
timization based black-box attacks to deep neural networks without training substitute models.
Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, 2017.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. arXiv preprint arXiv:1704.08847, 2017.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna,
Zachary C. Lipton, and Animashree Anandkumar. Stochastic activation pruning for robust
adversarial defense. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=H1uR4GZRZ.
K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and
D. Song. Robust physical-world attacks on deep learning visual classification. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 1625-1634, 2018.
Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. Detecting adversarial
samples from artifacts. ArXiv, abs/1703.00410, 2017.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model un-
certainty in deep learning. In Proceedings of the 33rd International Conference on International
Conference on Machine Learning - Volume 48, ICML’16, pp. 1050-1059. JMLR.org, 2016.
10
Under review as a conference paper at ICLR 2021
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015. URL http://
arxiv.org/abs/1412.6572.
Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R. S. Zemel,
P. L. Bartlett, F. Pereira, and K. Q. Weinberger (eds.), Advances in Neural Information Processing
Systems24, pp. 2348-2356. Curran Associates, Inc., 2011. URL http://papers.nips.cc/
paper/4329- practical- variational- inference- for- neural- networks.
pdf.
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. On
the (statistical) detection of adversarial examples. arXiv preprint arXiv:1702.06280, 2017.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=SyJ7ClWCb.
Han Xu Yao Ma Hao-Chen, Liu Debayan Deb, Hui Liu Ji-Liang Tang Anil, and K Jain. Adversarial
attacks and defenses in images, graphs and text: A review. International Journal of Automation
and Computing, 17(2):151-178, 2020.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier
against adversarial manipulation. In Advances in Neural Information Processing Systems, pp.
2266-2276, 2017.
Jose MigUel Hernandez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning
of bayesian neural networks. In ICML, 2015.
Matthew D. Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational
inference. Journal of Machine Learning Research, 14(4):1303-1347, 2013. URL http:
//jmlr.org/papers/v14/hoffman13a.html.
Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvari. Learning with a strong adver-
sary. http://arxiv.org/abs/1511.03034, 11 2015.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for
computer vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 5574-5584. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7141- what- uncertainties- do- we- need- in- bayesian- deep- learning- for- computer- vision.
pdf.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Confer-
ence on Learning Representations, 2014. URL https://openreview.net/forum?id=
33X9fd2-9FyZd.
Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. 2017.
URL https://arxiv.org/abs/1611.01236.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7, 2015.
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks
via random self-ensemble. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss (eds.), Computer Vision - ECCV 2018, pp. 381-397, Cham, 2018. Springer International
Publishing. ISBN 978-3-030-01234-2.
Xuanqing Liu, Yao Li, Chongruo Wu, and Cho-Jui Hsieh. Adv-BNN: Improved adversarial defense
through robust bayesian neural network. In International Conference on Learning Representa-
tions, 2019. URL https://openreview.net/forum?id=rk4Qso0cKm.
11
Under review as a conference paper at ICLR 2021
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying
variational autoencoders and generative adversarial networks. In Proceedings of the 34th Inter-
national Conference on Machine Learning - Volume 70, ICML’17, pp. 2391-2400. JMLR.org,
2017.
Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. On detecting adversarial
perturbations. arXiv preprint arXiv:1702.04267, 2017.
John Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochas-
tic search. In Proceedings of the 29th International Coference on International Conference on
Machine Learning, ICML’12, pp. 1363-1370, Madison, WI, USA, 2012. Omnipress. ISBN
9781450312851.
Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation
as a defense to adversarial perturbations against deep neural networks. 2016 IEEE Symposium on
Security and Privacy (SP), pp. 582-597, 2016.
Nicolas Papernot, P. McDaniel, Ian J. Goodfellow, S. Jha, Z. Y. Celik, and A. Swami. Practical
black-box attacks against machine learning. Proceedings of the 2017 ACM on Asia Conference
on Computer and Communications Security, 2017.
Tim Pearce, Felix Leibfried, and Alexandra Brintrup. Uncertainty in neural networks: Approxi-
mately bayesian ensembling. volume 108 of Proceedings of Machine Learning Research, pp.
234-244, Online, 26-28 Aug 2020. PMLR. URL http://proceedings.mlr.press/
v108/pearce20a.html.
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial exam-
ples. arXiv preprint arXiv:1801.09344, 2018.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. volume 32 of Proceedings of Machine
Learning Research, pp. 1278-1286, Bejing, China, 22-24 Jun 2014. PMLR. URL http:
//proceedings.mlr.press/v32/rezende14.html.
Ihai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach. Adversarial Learning in the Cyber
Security Domain. arXiv e-prints, art. arXiv:2007.02407, July 2020.
Y. E. Sagduyu, Y. Shi, and T. Erpek. Iot network security from the perspective of adversarial deep
learning. In 2019 16th Annual IEEE International Conference on Sensing, Communication, and
Networking (SECON), pp. 1-9, 2019.
Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Adversarial dropout reg-
ularization. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=HJIoJWZCZ.
Tim Salimans and David A. Knowles. Fixed-form variational posterior approximation through
stochastic linear regression. Bayesian Anal., 8(4):837-882, 12 2013. doi: 10.1214/13-BA858.
URL https://doi.org/10.1214/13-BA858.
Samuel Henrique Silva and Peyman Najafirad. Opportunities and challenges in deep learning ad-
versarial robustness: A survey. arXiv preprint arXiv:2007.00753, 2020.
Lewis Smith and Yarin Gal. Understanding measures of uncertainty for adversarial example detec-
tion. In Amir Globerson and Ricardo Silva (eds.), Proceedings of the Thirty-Fourth Conference on
Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018,
pp. 560-569. AUAI Press, 2018. URL http://auai.org/uai2018/proceedings/
papers/207.pdf.
12
Under review as a conference paper at ICLR 2021
S. Sun, P. Guo, L. Xie, and M. Hwang. Adversarial regularization for attention based end-to-end
robust speech recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing,
27(11):1826-1838,2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.
David Terjek. Adversarial IiPschitz regularization. In International Conference on Learning RePre-
sentations, 2020. URL https://openreview.net/forum?id=Bke_DertPB.
Kang Wang, Rui Zhao, Hui Su, and Qiang Ji. Generalizing eye tracking with bayesian adversarial
learning. In Proceedings of the IEEE/CVF Conference on ComPuter Vision and Pattern Recogni-
tion (CVPR), June 2019a.
Kuan-Chieh Wang, Paul Vicol, James Lucas, Li Gu, Roger B. Grosse, and Richard S. Zemel.
Adversarial distillation of bayesian neural network Posteriors. In Jennifer G. Dy and Andreas
Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, ICML
2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of
Machine Learning Research, PP. 5177-5186. PMLR, 2018. URL http://proceedings.
mlr.press/v80/wang18i.html.
Xianmin Wang, Jing Li, Xiaohui Kuang, Yu an Tan, and Jin Li. The security of machine learning
in an adversarial setting: A survey. Journal of Parallel and Distributed ComPuting, 130:12 -
23, 2019b. ISSN 0743-7315. doi: httPs://doi.org/10.1016/j.jPdc.2019.03.003. URL http:
//www.sciencedirect.com/science/article/pii/S0743731518309183.
Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics.
In Proceedings of the 28th International Conference on International Conference on Machine
Learning, ICML’11, PP. 681-688, Madison, WI, USA, 2011. OmniPress. ISBN 9781450306195.
Eric Wong and Zico Kolter. Provable defenses against adversarial examPles via the convex outer
adversarial PolytoPe. In International Conference on Machine Learning, PP. 5286-5295. PMLR,
2018.
Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examPles in deeP
neural networks. arXiv PrePrint arXiv:1704.01155, 2017.
Nanyang Ye and Zhanxing Zhu. Bayesian adversarial learning. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural In-
formation Processing Systems 31, PP. 6892-6901. Curran Associates, Inc., 2018. URL http:
//papers.nips.cc/paper/7921-bayesian-adversarial-learning.pdf.
Bing Yu, Jingfeng Wu, Jinwen Ma, and Zhanxing Zhu. Tangent-normal adversarial regularization
for semi-suPervised learning. In Proceedings of the IEEE/CVF Conference on ComPuter Vision
and Pattern Recognition (CVPR), June 2019.
Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. arXiv e-Prints, art.
arXiv:1605.07146, May 2016.
Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. Efficient defenses against adver-
sarial attacks. Proceedings of the 10th ACM WorkshoP on Artificial Intelligence and Security,
2017.
Han Zhang, Zizhao Zhang, Augustus Odena, and Honglak Lee. Consistency regularization for
generative adversarial networks. In International Conference on Learning RePresentations, 2020.
URL https://openreview.net/forum?id=S1lxKlSKPH.
Yuchen Zhang and Percy Liang. Defending against whitebox adversarial attacks via randomized
discretization. In AISTATS, 2019.
Roland S. Zimmermann. Comment on “Adv-BNN: ImProved Adversarial Defense through Robust
Bayesian Neural Network”. arXiv e-Prints, art. arXiv:1907.00895, July 2019.
13
Under review as a conference paper at ICLR 2021
A Decomposition of KL divergence in Hierarchical-ELBO
The approximate conjugate prior q(μ,σ2) can be independently factorized into the Normal dis-
tribution q(μ) and the Inverse Gamma distribution q(σ2) by the mean-field approximation. The
hierarchical prior p(μ, σ2) is factorized into the mean prior p(μ | σ2) and the variance prior p(σ2).
The KL divergence in the hierarchical-ELBO can be decomposed as follows:
DKL(q(μ, σ2) || p(μ, σ2))
Il
q(μ,σ2)log Hdμdσ2
=ZZ q(Mqb)Iog p(μqη )σC()σ2) dμdσ2
=ZZ q(μ)q(σ2)log J(μ) 2∖ dμdσ2 + ZZ q(μ)q(σ2)log q(σ2)dμdσ2
p(μ | σ2 )	p(σ2 )
=Z q(σ2) Zq(μ)log双案2)dμ dσ2 + Z q(σ2)logp(fɪ)dσ2
=Eσ2〜q(σ2) [Dkl3〃)|| p(μ | σ2))] + Dκz(q(σ2) || p(σ2)).
B Full expansion of KL divergence in Hierarchical-ELB O
The first term in the KL divergence at the hierarchical-ELBO can be written as:
(9)
Eσ2〜q(σ2) [DKL(q(μ) || P(〃 | σ2川
M
fEσj~q(σj) [DklS^) Il p(μj I σ2))] (by Lemma B.4)
j=1
M
EEσj 〜q(σj )
j=1
σ2,j+ μ0,j
.2λ-1σ2
+2log
λ-1σ2
σ2,j
(by Lemma B.1)
(10)
—
1
2
M
X
j=1
σ02,j + μ20,j	1	λj-1ψj	1 Γ0 (νj)	1
Vj 1λ-1ψ- + 2logk-2E - 2
(by Lemma B.2)
where
q(μj) = N (μj	I	μ0,j, σ0,j),	p(μj	I	σj )	= N (μj	I 0,	λi-	σj ).	(11)
14
Under review as a conference paper at ICLR 2021
The second term in the KL divergence at the hierarchical-ELBO can be written as:
DKL(q(σ2) || p(σ2))
M
X DKL(q(σj2) || p(σj2)) (by Lemma B.4)
j=1	(12)
X [νj log Ψj-Vj j-1
j=1	ψj
(by Lemma B.3)
where
q(σj2) = IG(σj2 | νj , ψj), p(σj2) = IG(σj2 | νj ,1).	(13)
Full expansion of the KL divergence can be computed by the summation of the above two terms
Eq. (10) and Eq. (12). The formulation can be written as:
τ. 2 2	2∖ Ii 2	2∖∖	1 ∖~^λ Xj (σ0,j + μ0,j) + 2,1
DKL(q(μ,σ ) || p(μ,σ )) = M X[νj-------工❷. -------+log
2νj + 1
ψ7 - 2νj + 1 - 1Γ0 (νj) ]
(λiσ2,j)1	2	2Γ(νj)]
(14)
Lemma B.1. KL divergence between two Gaussian distributions.
DKL(q(x)Mx))=σq+(μσp μp) +1log σf - 2,
where
q(x) = N(x | μq,σ2), p(x) = N(x | μp,σp).
Lemma B.2. The expected Inverse Gamma distribution
where
Ex〜q(x) X = ψ, Ex〜q(x) [log x]=log ψ - r(ν)，
ψν
q(x) = IG(x | ν,ψ) = Γ(V) x( +1) exp (-ψx )
15
Under review as a conference paper at ICLR 2021
Lemma B.3. KL divergence between two Inverse Gamma distributions
Q	[ ( Γ(	品X-(V+1)exp (-ψqx-1L
DκL(q(x)∣∣p(x)) = / q(x)log -ψν-------------------dx
J	ΓV)X-(V+1) exp(-ψpx-1)
/
ν
ψq
q(x)log -V exp
ψp
-(ψq - ψp)x-1 dx
/
q(x) V log ψq - (ψq - ψp)x-1
ψp
dx
V log ψq - (ψq - ψp)
ψp
dx
where
V log ψq -V(% - ψp),	(by Lemma B.2)
ψp	ψq
q(x) = IG(x | ν,ψq), p(x) = IG(x | ν, ψp).
Lemma B.4. KL divergence with independent random variables
Dkl(q(x)∣∣p(x))= 〃 …/ q(X)log pp(X)dX1dX2 …dXM
M
where
〃…/Eog Y筌
dxιdx2 …dxM
M	q	(xj)
X : / / • • • / q(x) log -—^dxIdx2 ∙ ∙ ∙ dxM
j=1	p(xj)
鄂 / / x1	q(Xj) /
〉」J q(Xj)logP(XJydxj X
j=1S----------{----}----}
DKL(q(xj)||p(xj))
M
DKL(q (xj)||p(xj)),
j=1
M
X = (χ1,χ2,…，xm), q(χ) = Y[q(χi),
j=1
M
q	(xi)dxi
i=1
i6=j
X-------{-------}
1
M
p(x) =	p(xi).
j=1
j
1
16
Under review as a conference paper at ICLR 2021
C Function Analysis
(a) The function H
Figure 4: Function analysis for (a) the function H, and (b) the derivative of the function H,
(b) The derivative of the function H
where
H (Vj) = VV 片 G(Vj) -
2dVj
1 G(Vj)+1 log Vj - 2
(15)
and
where
and
∞
1	X _Vj_
2	k=0 (Vj+ k)2
-Γ0j + 1log Vj - 1 > 0
Γ(νj) + 2 g j 2
lim H(Vj ) = 0+.
νj →∞
H 0(Vj) = V M G(νi) + —
(j)	2 dν2 (i)+2νj
∞
XVj	1
—行E +药<0
lim H0(Vj) = 0-.
νj →∞
(16)
(17)
(18)
D PROOF OF k VALUE
The Normal distribution q(μ) = N(μo,σ2) is learned to follow the mean prior p(μ | σ2) =
N(0, (λ*)-1σ2), with the optimal value λ* = "(.，2)∙ The mean prior,s variance equals to
νψσ2(μ2 + σ0). Here, e。? = σψ2 is sampled from IG(v, 1), as depicted in the Fig. 1(b). Thus, the
variance is written as: Vβσ2(μ2 + σ2). Now, We can statistically estimate Vβσ2 by the expectation
over eσ2 〜IG(v, 1). Then, it satisfies Eeb? [v%] = v—ɪ > 1 Hence, the mean prior p(μ | σ2)
can be designed to N(0, k(μ2 + σ0)), where k > 1.
17
Under review as a conference paper at ICLR 2021
E Clean training without adversarial examples
Table 3: On the clean training, the statistical summaries for the weight parameters compared with
BNN and Ours. We include mean, variance, and KL divergence (KLD). We describe the average of
KLD for all layers in deep neural networks. Outperforming statistical results are marked in bold.
	CIFAR-10			STL-10			CIFAR-100			Tiny-ImageNet		
	Mean	Variance	KLD	Mean	Variance	KLD	Mean	Variance	KLD	Mean	Variance	KLD
BNN	0	0.071	7.7	0	0.051	25	0	0.068	15.4	0	0.064	26.7
Ours	0.003	0.129	1.0	0.002	0.092	1.0	0.007	0.109	1.4	0.028	0.211	1.8
Figure 5: Comparison of classification accuracy without adversarial training. These graphs are
under PGD attack (first row) and EOT-PGD attack (second row) to the L∞ perturbation magnitude
on different datasets: CIFAR-10, STL-10, CIFAR-100, and Tiny-ImageNet.
F	Non-zero Mean and Higher Variance
Non-Zero Mean. According to the experiment results in Tab. 1-3, the mean learned from adver-
sarial training with BNN follows zero, while the mean from the proposed method follows non-zero.
Analyzing KL divergences between ELBO and hierarchical-ELBO can provide a clue to this phe-
nomenon. The relationship of their KL divergences can be written as follows:
DKL(q(μ,σ2') ||p(μ,σ2)) < DKLg(W) ||P(W)).
×----------------V----------------} X---------------V------------}
for hierarchical-ELBO	for ELBO
(19)
The left KL divergence above represents the proposed method to better approximate the true poste-
rior by the tight lower bound of hierarchical-ELBO, and the right one denotes the BNN regularizer
in ELBO during adversarial training with BNN. Through the optimization described in the Section
3, the proposed method can be much smaller than the BNN regularizer. Based on this relationship of
Eq. (19), it is not required that hierarchical-ELBO concentrates on learning its KL divergence due to
already optimized strong regularization. In contrast, ELBO more tries to reduce its KL divergence
to improve regularization effect through the zero mean, the purpose of which is addressing weak
regularization effect attributed by the vanished stochasticity. At this time, the proposed method rel-
atively more focuses to learn the expected log-likelihood in the hierarchical-ELBO, where the mean
does not have to be zero. Instead, the mean is adaptively learned according to the characteristics of
the datasets.
18
Under review as a conference paper at ICLR 2021
Higher Variance. As also shown in Tab. 1-3, the proposed method promotes higher variance
compared to the initial variance 0.1. While, adversarial training with BNN has lower variance
causing the lack of its stochasticity. We contemplate whether the stochasticity and higher variance
purely affects the adversarial robustness of the deep neural networks. To examine this impact, we
handle the magnitude of the derivative to the log-likelihood over the weight parameters, which can
be written as follows:
dW log p(y |Xadd，W)
1 d log p(y | xαdv ,μ+σe)
(20)
where W = μ + σe, such that E 〜N(0,1). Note that for mathematical convenience, the denoted μ
and σ 2 correspond to the mean and the variance respectively in both adversarial training with BNN
and Ours.
Eq. (20) explicates the magnitude of the change in the cross-entropy as the weight parameters change
from the variation of E, which can be represented to: dW = σdE. Once the variance gets much
smaller due to the vanished stochasticity, the magnitude becomes larger. That is, prediction results
of the deep neural networks differ significantly from the slight change of the weight parameters,
such that w = μ + σ(≈ 0)e. The large change in cross-entropy makes ELBO inconsistent, and it
loses adversarial robustness. In contrast, once the variance from the proposed method gets much
larger, the prediction results are robust to the large change of the weight parameters, where the mag-
nitude becomes smaller. Accordingly, Eq. (20) guides the proposed method to keep the maximum
hierarchical-ELBO consistent by the higher variance, thus better obtaining well-posed inference to
boost adversarial robustness. Consequently, we verify that the higher variance has a positive impact
on adversarial robustness by itself with large BNN’s stochasticity.
G Robustness of proposed method against EOT-like attack
Let T (W) = NxJ (fw (x), y) where w denotes the weight parameters w = μ + σe, such that E 〜
N(0,1). Note that for mathematical convenience, the denoted μ and σ2 correspond to the mean and
the variance respectively in both adversarial training with BNN and Ours.
Expectation over Transformation (EOT) attack is well-known adaptive attack targeting randomized
neural networks. The EOT-like attack easily bypasses the stochasticity of randomized classifiers
by taking the expectation over the multiple weight parameters to compute the actual gradient. The
formulation of the expected T over the weight parameters can be written as follows:
Ew [T (w)] = EJT (μ + σe)].
According to Taylor series, the above equation can be written as follows:
Ee[T (μ + σe)] = Ee [T (μ) + σeT0(μ) + ʧ T00(μ) + …]
3+吼[5〃)]+ 吼 ⅛2 T 00(μ)] + ….
actual gradient
For naive adversarial training (σ is not used), the expected T can be written as:
Ee [T (μ)]= T (μ).
(21)
(22)
(23)
For adversarial training with BNN, the variance σ2 of the approximate posterior q(w) becomes to
be reduced, so Taylor series ends up with first-order approximation. Eq. (22) can be written as:
19
Under review as a conference paper at ICLR 2021
Ee [T (μ + σe)] = Ee [T (μ)]+ Ee [σeT 0(μ)] = T (μ),	(24)
which equals to the result of the expected T from naive adversarial training in Eq. (23). On the other
hand, the proposed method can enhance the BNN’s stochasticity with the large enough variance σ2.
Hence, the remaining higher order terms of Taylor series in Eq. (22) still survive at even degree,
which can be written as:
Ee[T(μ + σe)] = Ee[T(μ) + σeT0(μ) + ʧT〃(〃)+ …]
2
T (M)	+ ^2 t 00 (M)+	.
|{z}	、2	、- J
actualgradient	Stochastictolerantterms
(25)
The remaining terms of Taylor series make a difference from the actual gradient as opposed to
adversarial training and adversarial-BNN. Here, these terms retain stochastic tolerance against EOT-
like attack, thus bringing in the improvement of adversarial robustness as the effectiveness of the
proposed method.
20